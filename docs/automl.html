<!DOCTYPE html><html><head><title>Help for package automl</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {automl}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#automl_predict'><p>automl_predict</p></a></li>
<li><a href='#automl_train'><p>automl_train</p></a></li>
<li><a href='#automl_train_manual'><p>automl_train_manual</p></a></li>
<li><a href='#autopar'><p>parameters for automatic hyperparameters optimization</p></a></li>
<li><a href='#hpar'><p>Deep Neural Net parameters and hyperparameters</p></a></li>
<li><a href='#pso'><p>PSO parameters and hyperparameters</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Deep Learning with Metaheuristic</td>
</tr>
<tr>
<td>Version:</td>
<td>1.3.2</td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/aboulaboul/automl/issues">https://github.com/aboulaboul/automl/issues</a></td>
</tr>
<tr>
<td>Description:</td>
<td>Fits from simple regression to highly customizable deep neural networks 
    either with gradient descent or metaheuristic, using automatic hyper parameters 
    tuning and custom cost function.
    A mix inspired by the common tricks on Deep Learning and Particle Swarm Optimization.</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://aboulaboul.github.io/automl">https://aboulaboul.github.io/automl</a>
<a href="https://github.com/aboulaboul/automl">https://github.com/aboulaboul/automl</a></td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GNU General Public License]</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>TRUE</td>
</tr>
<tr>
<td>Imports:</td>
<td>stats, utils, parallel</td>
</tr>
<tr>
<td>Suggests:</td>
<td>datasets</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>6.1.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2020-01-16 11:25:44 UTC; aboul</td>
</tr>
<tr>
<td>Author:</td>
<td>Alex Boulangé [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Alex Boulangé &lt;aboul@free.fr&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2020-01-16 11:40:09 UTC</td>
</tr>
</table>
<hr>
<h2 id='automl_predict'>automl_predict</h2><span id='topic+automl_predict'></span>

<h3>Description</h3>

<p>Predictions function, to apply a trained model on datas
</p>


<h3>Usage</h3>

<pre><code class='language-R'>automl_predict(model, X, layoutputnum)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="automl_predict_+3A_model">model</code></td>
<td>
<p> model trained previously with <a href="#topic+automl_train">automl_train</a> or <a href="#topic+automl_train_manual">automl_train_manual</a></p>
</td></tr>
<tr><td><code id="automl_predict_+3A_x">X</code></td>
<td>
<p> inputs matrix or data.frame (containing numerical values only)</p>
</td></tr>
<tr><td><code id="automl_predict_+3A_layoutputnum">layoutputnum</code></td>
<td>
<p> which layer number to output especially for auto encoding (default 0: no particular layer, the last one)</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>##REGRESSION (predict Sepal.Length given other parameters)
data(iris)
xmat &lt;- as.matrix(cbind(iris[,2:4], as.numeric(iris$Species)))
ymat &lt;- iris[,1]
amlmodel &lt;- automl_train_manual(Xref = xmat, Yref = ymat,
 hpar = list(modexec = 'trainwpso', verbose = FALSE))
res &lt;- cbind(ymat, automl_predict(model = amlmodel, X = xmat))
colnames(res) &lt;- c('actual', 'predict')
head(res)
#
## Not run: 
##CLASSIFICATION (predict Species given other Iris parameters)
data(iris)
xmat = iris[,1:4]
lab2pred &lt;- levels(iris$Species)
lghlab &lt;- length(lab2pred)
iris$Species &lt;- as.numeric(iris$Species)
ymat &lt;- matrix(seq(from = 1, to = lghlab, by = 1), nrow(xmat),
 lghlab, byrow = TRUE)
ymat &lt;- (ymat == as.numeric(iris$Species)) + 0
amlmodel &lt;- automl_train_manual(Xref = xmat, Yref = ymat,
 hpar = list(modexec = 'trainwpso', verbose = FALSE))
res &lt;- cbind(ymat, round(automl_predict(model = amlmodel, X = xmat)))
colnames(res) &lt;- c(paste('act',lab2pred, sep = '_'),
 paste('pred',lab2pred, sep = '_'))
head(res)

## End(Not run)
</code></pre>

<hr>
<h2 id='automl_train'>automl_train</h2><span id='topic+automl_train'></span>

<h3>Description</h3>

<p>The multi deep neural network automatic train function (several deep neural networks are trained with automatic hyperparameters tuning, best model is kept)<br />
This function launches the <a href="#topic+automl_train_manual">automl_train_manual</a> function by passing it parameters
for each particle at each converging step
</p>


<h3>Usage</h3>

<pre><code class='language-R'>automl_train(Xref, Yref, autopar = list(), hpar = list(), mdlref = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="automl_train_+3A_xref">Xref</code></td>
<td>
<p> inputs matrix or data.frame (containing numerical values only)
</p>
</td></tr>
<tr><td><code id="automl_train_+3A_yref">Yref</code></td>
<td>
<p> target matrix or data.frame (containing numerical values only)
</p>
</td></tr>
<tr><td><code id="automl_train_+3A_autopar">autopar</code></td>
<td>
<p> list of parameters for hyperparameters optimization, see <a href="#topic+autopar">autopar</a> section<br />
Not mandatory (the list is preset and all arguments are initialized with default value) but it is advisable to adjust some important arguments for performance reasons (including processing time)
</p>
</td></tr>
<tr><td><code id="automl_train_+3A_hpar">hpar</code></td>
<td>
<p> list of parameters and hyperparameters for Deep Neural Network, see <a href="#topic+hpar">hpar</a> section<br />
Not mandatory (the list is preset and all arguments are initialized with default value) but it is advisable to adjust some important arguments for performance reasons (including processing time)
</p>
</td></tr>
<tr><td><code id="automl_train_+3A_mdlref">mdlref</code></td>
<td>
<p> model trained with <a href="#topic+automl_train">automl_train</a> to start training with saved <a href="#topic+hpar">hpar</a> and <a href="#topic+autopar">autopar</a>
(not the model)<br />
nb: manually entered parameters above override loaded ones</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
##REGRESSION (predict Sepal.Length given other Iris parameters)
data(iris)
xmat &lt;- cbind(iris[,2:4], as.numeric(iris$Species))
ymat &lt;- iris[,1]
amlmodel &lt;- automl_train(Xref = xmat, Yref = ymat)

## End(Not run)
##CLASSIFICATION (predict Species given other Iris parameters)
data(iris)
xmat = iris[,1:4]
lab2pred &lt;- levels(iris$Species)
lghlab &lt;- length(lab2pred)
iris$Species &lt;- as.numeric(iris$Species)
ymat &lt;- matrix(seq(from = 1, to = lghlab, by = 1), nrow(xmat), lghlab, byrow = TRUE)
ymat &lt;- (ymat == as.numeric(iris$Species)) + 0
#with gradient descent and random hyperparameters sets
amlmodel &lt;- automl_train(Xref = xmat, Yref = ymat,
                          autopar = list(numiterations = 1, psopartpopsize = 1, seed = 11),
                          hpar = list(numiterations = 10))
</code></pre>

<hr>
<h2 id='automl_train_manual'>automl_train_manual</h2><span id='topic+automl_train_manual'></span>

<h3>Description</h3>

<p>The base deep neural network train function (one deep neural network trained without automatic hyperparameters tuning)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>automl_train_manual(Xref, Yref, hpar = list(), mdlref = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="automl_train_manual_+3A_xref">Xref</code></td>
<td>
<p> inputs matrix or data.frame (containing numerical values only)</p>
</td></tr>
<tr><td><code id="automl_train_manual_+3A_yref">Yref</code></td>
<td>
<p> target matrix or data.frame (containing numerical values only)</p>
</td></tr>
<tr><td><code id="automl_train_manual_+3A_hpar">hpar</code></td>
<td>
<p> list of parameters and hyperparameters for Deep Neural Network, see <a href="#topic+hpar">hpar</a> section<br />
Not mandatory (the list is preset and all arguments are initialized with default value) but it is advisable to adjust some important arguments for performance reasons (including processing time)</p>
</td></tr>
<tr><td><code id="automl_train_manual_+3A_mdlref">mdlref</code></td>
<td>
<p> model trained with <a href="#topic+automl_train">automl_train</a> or <a href="#topic+automl_train_manual">automl_train_manual</a> to start training from a saved model (shape,
weights...) for fine tuning<br />
nb: manually entered parameters above override loaded ones</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>##REGRESSION (predict Sepal.Length given other Iris parameters)
data(iris)
xmat &lt;- cbind(iris[,2:4], as.numeric(iris$Species))
ymat &lt;- iris[,1]
#with gradient descent
amlmodel &lt;- automl_train_manual(Xref = xmat, Yref = ymat,
                                hpar = list(learningrate = 0.01,
                                            numiterations = 30,
                                            minibatchsize = 2^2))
## Not run: 
#with PSO
amlmodel &lt;- automl_train_manual(Xref = xmat, Yref = ymat,
                                hpar = list(modexec = 'trainwpso',
                                            numiterations = 30,
                                            psopartpopsize = 50))
#with PSO and custom cost function
f &lt;- 'J=abs((y-yhat)/y)'
f &lt;- c(f, 'J=sum(J[!is.infinite(J)],na.rm=TRUE)')
f &lt;- c(f, 'J=(J/length(y))')
f &lt;- paste(f, collapse = ';')
amlmodel &lt;- automl_train_manual(Xref = xmat, Yref = ymat,
                                hpar = list(modexec = 'trainwpso',
                                            numiterations = 30,
                                            psopartpopsize = 50,
                                            costcustformul = f))

##CLASSIFICATION (predict Species given other Iris parameters)
data(iris)
xmat = iris[,1:4]
lab2pred &lt;- levels(iris$Species)
lghlab &lt;- length(lab2pred)
iris$Species &lt;- as.numeric(iris$Species)
ymat &lt;- matrix(seq(from = 1, to = lghlab, by = 1), nrow(xmat), lghlab, byrow = TRUE)
ymat &lt;- (ymat == as.numeric(iris$Species)) + 0
#with gradient descent and 2 hidden layers
amlmodel &lt;- automl_train_manual(Xref = xmat, Yref = ymat,
                                hpar = list(layersshape = c(10, 10, 0),
                                            layersacttype = c('tanh', 'relu', 'sigmoid'),
                                            layersdropoprob = c(0, 0, 0)))
#with gradient descent and no hidden layer (logistic regression)
amlmodel &lt;- automl_train_manual(Xref = xmat, Yref = ymat,
                                hpar = list(layersshape = c(0),
                                            layersacttype = c('sigmoid'),
                                            layersdropoprob = c(0)))
#with PSO and softmax
amlmodel &lt;- automl_train_manual(Xref = xmat, Yref = ymat,
                                hpar = list(modexec = 'trainwpso',
                                            layersshape = c(10, 0),
                                            layersacttype = c('relu', 'softmax'),
                                            layersdropoprob = c(0, 0),
                                            numiterations = 50,
                                            psopartpopsize = 50))

## End(Not run)
</code></pre>

<hr>
<h2 id='autopar'>parameters for automatic hyperparameters optimization</h2><span id='topic+autopar'></span>

<h3>Description</h3>

<p>List of parameters to allow multi deep neural network automatic hyperparameters tuning with Particle Swarm Optimization<br />
Not mandatory (the list is preset and all arguments are initialized with default value) but it is advisable to adjust some important arguments for performance reasons (including processing time)
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="autopar_+3A_psopartpopsize">psopartpopsize</code></td>
<td>
<p> number of particles in swarm, the main argument that should be tuned (default value 8, which is quite low)<br />
#tuning priority 1</p>
</td></tr>
<tr><td><code id="autopar_+3A_psoxxx">psoxxx</code></td>
<td>
<p> see <a href="#topic+pso">pso</a> for other PSO specific arguments details</p>
</td></tr>
<tr><td><code id="autopar_+3A_numiterations">numiterations</code></td>
<td>
<p> number of convergence steps between particles (hyperparameters), default value 3)<br />
#tuning priority 1</p>
</td></tr>
<tr><td><code id="autopar_+3A_auto_modexec">auto_modexec</code></td>
<td>
<p> if &lsquo;TRUE&rsquo; the type of Neural Net optimization will be randomly choosen between &lsquo;trainwgrad&rsquo; and &lsquo;trainwpso&rsquo; for each particle<br />
default value is &lsquo;FALSE&rsquo; (so default value of argument &lsquo;modexec&rsquo; in <a href="#topic+automl_train_manual">automl_train_manual</a> function, actually &lsquo;trainwgrad&rsquo; as default is more suited to large data volume)<br />
the value can be forced if defined in <a href="#topic+hpar">hpar</a> list</p>
</td></tr>
<tr><td><code id="autopar_+3A_auto_runtype">auto_runtype</code></td>
<td>
<p> if &lsquo;2steps&rsquo; the 2 following steps will be run automatically (default value is &lsquo;normal&rsquo;):<br />
1st overfitting, the goal is performance<br />
2nd regularization, the goal is generalization <br />
nb: &lsquo;overfitting&rsquo; or &lsquo;regularization&rsquo; may be directly specified to avoid the 2 steps<br /></p>
</td></tr>
<tr><td><code id="autopar_+3A_auto_minibatchsize">auto_minibatchsize</code></td>
<td>
<p> see below</p>
</td></tr>
<tr><td><code id="autopar_+3A_auto_minibatchsize_min">auto_minibatchsize_min</code></td>
<td>
<p> see below</p>
</td></tr>
<tr><td><code id="autopar_+3A_auto_minibatchsize_max">auto_minibatchsize_max</code></td>
<td>
 <p>&lsquo;auto_minibatch&rsquo; default value &lsquo;TRUE&rsquo; for automatic adjustment of &lsquo;minibatchsize&rsquo; argument in <a href="#topic+automl_train_manual">automl_train_manual</a> function<br />
the minimum and maximum value for &lsquo;minibatchsize&rsquo; corespond to 2 to the power value (default 0 for &lsquo;auto_minibatchsize_min&rsquo; and 9 for &lsquo;auto_minibatchsize_max&rsquo;)</p>
</td></tr>
<tr><td><code id="autopar_+3A_auto_learningrate">auto_learningrate</code></td>
<td>
<p> see below</p>
</td></tr>
<tr><td><code id="autopar_+3A_auto_learningrate_min">auto_learningrate_min</code></td>
<td>
<p> see below</p>
</td></tr>
<tr><td><code id="autopar_+3A_auto_learningrate_max">auto_learningrate_max</code></td>
<td>
 <p>&lsquo;auto_learningrate&rsquo; default value &lsquo;TRUE&rsquo; for automatic adjustment of &lsquo;learningrate&rsquo; argument in <a href="#topic+automl_train_manual">automl_train_manual</a> function<br />
the minimum and maximum value for &lsquo;learningrate&rsquo; correspond to 10 to the power negative value (default -5 for &lsquo;auto_learningrate_min&rsquo; and -2 for &lsquo;auto_learningrate_max&rsquo;)</p>
</td></tr>
<tr><td><code id="autopar_+3A_auto_beta1">auto_beta1</code></td>
<td>
<p> see below</p>
</td></tr>
<tr><td><code id="autopar_+3A_auto_beta2">auto_beta2</code></td>
<td>
 <p>&lsquo;auto_beta1&rsquo; and &lsquo;auto_beta2&rsquo; default value &lsquo;TRUE&rsquo; for automatic adjustment of &lsquo;beta1&rsquo; and &lsquo;beta2&rsquo; argument in <a href="#topic+automl_train_manual">automl_train_manual</a> function</p>
</td></tr>
<tr><td><code id="autopar_+3A_auto_psopartpopsize">auto_psopartpopsize</code></td>
<td>
<p> see below</p>
</td></tr>
<tr><td><code id="autopar_+3A_auto_psopartpopsize_min">auto_psopartpopsize_min</code></td>
<td>
<p> see below</p>
</td></tr>
<tr><td><code id="autopar_+3A_auto_psopartpopsize_max">auto_psopartpopsize_max</code></td>
<td>
 <p>&lsquo;auto_psopartpopsize&rsquo; default value &lsquo;TRUE&rsquo; for automatic adjustment of &lsquo;psopartpopsize&rsquo; argument in <a href="#topic+automl_train_manual">automl_train_manual</a> function (concern only &lsquo;modexec&rsquo; set to &lsquo;trainwpso&rsquo;)<br />
the minimum and maximum value for &lsquo;psopartpopsize&rsquo; ; default 2 for &lsquo;auto_psopartpopsize_min&rsquo; and 50 for &lsquo;auto_psopartpopsize_max&rsquo;)</p>
</td></tr>
<tr><td><code id="autopar_+3A_auto_lambda">auto_lambda</code></td>
<td>
<p> see below</p>
</td></tr>
<tr><td><code id="autopar_+3A_auto_lambda_min">auto_lambda_min</code></td>
<td>
<p> see below</p>
</td></tr>
<tr><td><code id="autopar_+3A_auto_lambda_max">auto_lambda_max</code></td>
<td>
 <p>&lsquo;auto_lambda&rsquo; default value &lsquo;FALSE&rsquo; for automatic adjustment of &lsquo;lambda&rsquo; regularization argument in <a href="#topic+automl_train_manual">automl_train_manual</a> function<br />
the minimum and maximum value for &lsquo;lambda&rsquo; correspond to 10 to the power value  (default -2) for &lsquo;auto_lambda_min&rsquo; and (default 4) for &lsquo;auto_lambda_max&rsquo;)</p>
</td></tr>
<tr><td><code id="autopar_+3A_auto_psovelocitymaxratio">auto_psovelocitymaxratio</code></td>
<td>
<p> see below</p>
</td></tr>
<tr><td><code id="autopar_+3A_auto_psovelocitymaxratio_min">auto_psovelocitymaxratio_min</code></td>
<td>
<p> see below</p>
</td></tr>
<tr><td><code id="autopar_+3A_auto_psovelocitymaxratio_max">auto_psovelocitymaxratio_max</code></td>
<td>
 <p>&lsquo;auto_psovelocitymaxratio&rsquo; default value &lsquo;TRUE&rsquo; for automatic adjustment of &lsquo;psovelocitymaxratio&rsquo; PSO velocity max ratio argument in <a href="#topic+automl_train_manual">automl_train_manual</a> function<br />
the minimum and maximum value for &lsquo;psovelocitymaxratio&rsquo;; default 0.01 for &lsquo;auto_psovelocitymaxratio_min&rsquo; and 0.5 for &lsquo;auto_psovelocitymaxratio_max&rsquo;</p>
</td></tr>
<tr><td><code id="autopar_+3A_auto_layers">auto_layers</code></td>
<td>
<p> see below (&lsquo;auto_layers&rsquo; default value &lsquo;TRUE&rsquo; for automatic adjustment of layers shape in <a href="#topic+automl_train_manual">automl_train_manual</a> function)</p>
</td></tr>
<tr><td><code id="autopar_+3A_auto_layers_min">auto_layers_min</code></td>
<td>
<p> (linked to &lsquo;auto_layers&rsquo; above, set <a href="#topic+hpar">hpar</a> &lsquo;layersshape&rsquo; and &lsquo;layersacttype&rsquo;) the minimum number of hidden layers (default 1 no hidden layer)</p>
</td></tr>
<tr><td><code id="autopar_+3A_auto_layers_max">auto_layers_max</code></td>
<td>
<p> (linked to &lsquo;auto_layers&rsquo; above, set <a href="#topic+hpar">hpar</a> &lsquo;layersshape&rsquo; and &lsquo;layersacttype&rsquo;) the maximum number of hidden layers (default 2)</p>
</td></tr>
<tr><td><code id="autopar_+3A_auto_layersnodes_min">auto_layersnodes_min</code></td>
<td>
<p> (linked to &lsquo;auto_layers&rsquo; above, set <a href="#topic+hpar">hpar</a> &lsquo;layersshape&rsquo; and &lsquo;layersacttype&rsquo;) the minimum number of nodes per layer (default 3)</p>
</td></tr>
<tr><td><code id="autopar_+3A_auto_layersnodes_max">auto_layersnodes_max</code></td>
<td>
<p> (linked to &lsquo;auto_layers&rsquo; above, set <a href="#topic+hpar">hpar</a> &lsquo;layersshape&rsquo; and &lsquo;layersacttype&rsquo;) the maximum number of nodes per layer (default 33)</p>
</td></tr>
<tr><td><code id="autopar_+3A_auto_layersdropo">auto_layersdropo</code></td>
<td>
<p> see below</p>
</td></tr>
<tr><td><code id="autopar_+3A_auto_layersdropoprob_min">auto_layersdropoprob_min</code></td>
<td>
<p> see below</p>
</td></tr>
<tr><td><code id="autopar_+3A_auto_layersdropoprob_max">auto_layersdropoprob_max</code></td>
<td>
 <p>&lsquo;auto_layersdropo&rsquo; default value &lsquo;FALSE&rsquo; for automatic adjustment of <a href="#topic+hpar">hpar</a> &lsquo;layersdropoprob&rsquo; in <a href="#topic+automl_train_manual">automl_train_manual</a> function)<br />
the minimum and maximum value for &lsquo;layersdropoprob&rsquo;; default 0.05 for &lsquo;auto_layersdropoprob_min&rsquo; and 0.75 for &lsquo;auto_layersdropoprob_max&rsquo;</p>
</td></tr>
<tr><td><code id="autopar_+3A_seed">seed</code></td>
<td>
<p> seed for reproductibility (default 4)</p>
</td></tr>
<tr><td><code id="autopar_+3A_nbcores">nbcores</code></td>
<td>
<p> number of cores used to parallelize particles optimization, not available on Windows (default 1, automatically reduced if not enough cores)</p>
</td></tr>
<tr><td><code id="autopar_+3A_verbose">verbose</code></td>
<td>
<p> to display or not the costs at each iteration for each particle (default TRUE)</p>
</td></tr></table>
<p><br />
</p>
<table>
<tr><td><code id="autopar_+3A_subtimelimit">subtimelimit</code></td>
<td>
<p> time limit in seconds for sub modelizations to avoid waiting too long for a specific particle to finish its modelization (default 3600)</p>
</td></tr></table>
<p><br />
</p>
<p><em>back to <a href="#topic+automl_train">automl_train</a></em>
</p>

<hr>
<h2 id='hpar'>Deep Neural Net parameters and hyperparameters</h2><span id='topic+hpar'></span>

<h3>Description</h3>

<p>List of Neural Network parameters and hyperparameters to train with gradient descent or particle swarm optimization<br />
Not mandatory (the list is preset and all arguments are initialized with default value) but it is advisable to adjust some important arguments for performance reasons (including processing time)
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="hpar_+3A_modexec">modexec</code></td>
<td>
 <p>&lsquo;trainwgrad&rsquo; (the default value) to train with gradient descent (suitable for all volume of data)<br />
&lsquo;trainwpso&rsquo; to train using Particle Swarm Optimization, each particle represents a set of neural network weights (CAUTION: suitable for low volume of data, time consuming for medium to large volume of data)</p>
</td></tr></table>
<p><br />
</p>
<p><em>Below specific arguments to &lsquo;trainwgrad&rsquo; execution mode</em>
</p>
<table>
<tr><td><code id="hpar_+3A_learningrate">learningrate</code></td>
<td>
<p> learningrate alpha (default value 0.001)<br />
#tuning priority 1</p>
</td></tr>
<tr><td><code id="hpar_+3A_beta1">beta1</code></td>
<td>
<p> see below</p>
</td></tr>
<tr><td><code id="hpar_+3A_beta2">beta2</code></td>
<td>
 <p>&lsquo;Momentum&rsquo; if beta1 different from 0 and beta2 equal 0 )<br />
&lsquo;RMSprop&rsquo; if beta1 equal 0 and beta2 different from 0<br />
&lsquo;adam optimization&rsquo; if beta1 different from 0 and beta2 different from 0 (default)<br />
(default value beta1 equal 0.9 and beta2 equal 0.999)<br />
#tuning priority 2</p>
</td></tr>
<tr><td><code id="hpar_+3A_lrdecayrate">lrdecayrate</code></td>
<td>
<p> learning rate decay value (default value 0, no learning rate decay, 1e-6 should be a good value to start with)<br />
#tuning priority 4</p>
</td></tr>
<tr><td><code id="hpar_+3A_chkgradevery">chkgradevery</code></td>
<td>
<p> epoch interval to run gradient check function (default value 0, for debug only)</p>
</td></tr>
<tr><td><code id="hpar_+3A_chkgradepsilon">chkgradepsilon</code></td>
<td>
<p> epsilon value for derivative calculations and threshold test in gradient check function (default 0.0000001)</p>
</td></tr></table>
<p><br />
</p>
<p><em>Below specific arguments to &lsquo;trainwpso&rsquo; execution mode</em>
</p>
<table>
<tr><td><code id="hpar_+3A_psoxxx">psoxxx</code></td>
<td>
<p> see <a href="#topic+pso">pso</a> for PSO specific arguments details</p>
</td></tr>
<tr><td><code id="hpar_+3A_costcustformul">costcustformul</code></td>
<td>
<p> custom cost formula (default &lsquo;&rsquo;, no custom cost function)<br />
standard input variables: yhat (prediction), y (target actual value)<br />
custom input variables: any variable declared in hpar may be used via alias mydl (ie: hpar(list = (foo = 1.5)) will be used in custom cost formula as mydl$foo))<br />
result: J<br />
see &lsquo;automl_train_manual&rsquo; example using Mean Average Percentage Error cost function<br />
nb: X and Y matrices used as input into automl_train_manual or automl_train_manual functions are transposed (features in rows and cases in columns)</p>
</td></tr></table>
<p><br />
</p>
<p><em>Below arguments for both execution modes</em>
</p>
<table>
<tr><td><code id="hpar_+3A_numiterations">numiterations</code></td>
<td>
<p> number of training epochs (default value 50))<br />
#tuning priority 1</p>
</td></tr>
<tr><td><code id="hpar_+3A_seed">seed</code></td>
<td>
<p> seed for reproductibility (default 4)</p>
</td></tr>
<tr><td><code id="hpar_+3A_minibatchsize">minibatchsize</code></td>
<td>
<p> mini batch size, 2 to the power 0 for stochastic gradient descent (default 2 to the power 5)
#tuning priority 3</p>
</td></tr>
<tr><td><code id="hpar_+3A_layersshape">layersshape</code></td>
<td>
<p> number of nodes per layer, each nodes number initialize a hidden layer<br />
output layer nodes number, may be left to 0 it will be automatically set by Y matrix shape<br />
default value one hidden layer with 10 nodes: c(10, 0)<br />
#tuning priority 4</p>
</td></tr>
<tr><td><code id="hpar_+3A_layersacttype">layersacttype</code></td>
<td>
<p> activation function for each layer; &lsquo;linear&rsquo; for no activation or &lsquo;sigmoid&rsquo;, &lsquo;relu&rsquo; or &lsquo;reluleaky&rsquo; or &lsquo;tanh&rsquo; or &lsquo;softmax&rsquo; (softmax for output layer only supported in trainwpso exec mode)<br />
output layer activation function may be left to &lsquo;&rsquo;, default value &lsquo;linear&rsquo; for regression, &lsquo;sigmoid&rsquo; for classification<br />
nb: layersacttype parameter vector must have same length as layersshape parameter vector<br />
default value c(&lsquo;relu&rsquo;, &lsquo;&rsquo;)<br />
#tuning priority 4</p>
</td></tr>
<tr><td><code id="hpar_+3A_layersdropoprob">layersdropoprob</code></td>
<td>
<p> drop out probability for each layer, continuous value from 0 to less than 1 (give the percentage of matrix weight values to drop out randomly)<br />
nb: layersdropoprob parameter vector must have same length as layersshape parameter vector<br />
default value no drop out: c(0, 0)<br />
#tuning priority for regularization</p>
</td></tr>
<tr><td><code id="hpar_+3A_printcostevery">printcostevery</code></td>
<td>
<p> epoch interval to test and print costs (train and cross validation cost: default value 10, for 1 test every 10 epochs)</p>
</td></tr>
<tr><td><code id="hpar_+3A_testcvsize">testcvsize</code></td>
<td>
<p> size of cross validation sample, 0 for no cross validation sample (default 10, for 10 percent)</p>
</td></tr>
<tr><td><code id="hpar_+3A_testgainunder">testgainunder</code></td>
<td>
<p> threshold to stop the training if the gain between last train or cross validation cost is smaller than the threshold, 0 for no stop test (default 0.000001)</p>
</td></tr>
<tr><td><code id="hpar_+3A_costtype">costtype</code></td>
<td>
<p> cost type function name &lsquo;mse&rsquo; or &lsquo;crossentropy&rsquo; or &lsquo;custom&rsquo;<br />
&lsquo;mse&rsquo; for Mean Squared Error, set automatically for continuous target type (&lsquo;mape&rsquo; Mean Absolute Percentage Error may be specified)<br />
&lsquo;crossentropy&rsquo; set automatically for binary target type<br />
&lsquo;custom&rsquo; set automatically if &lsquo;costcustformul&rsquo; different from &lsquo;&rsquo;
</p>
</td></tr>
<tr><td><code id="hpar_+3A_lambda">lambda</code></td>
<td>
<p> regularization term added to cost function (default value 0, no regularization)
</p>
</td></tr>
<tr><td><code id="hpar_+3A_batchnor_mom">batchnor_mom</code></td>
<td>
<p> batch normalization momentum for j and B (default 0, no batch normalization, may be set to 0.9 for deep neural net)
</p>
</td></tr>
<tr><td><code id="hpar_+3A_epsil">epsil</code></td>
<td>
<p> epsilon the low value to avoid dividing by 0 or log(0) in cost function, etc ... (default value 1e-12)<br />
</p>
</td></tr>
<tr><td><code id="hpar_+3A_verbose">verbose</code></td>
<td>
<p> to display or not the costs and the shapes (default TRUE)</p>
</td></tr></table>
<p><br />
</p>
<p><em>back to  <a href="#topic+automl_train">automl_train</a>, <a href="#topic+automl_train_manual">automl_train_manual</a></em>
</p>


<h3>See Also</h3>

<p>Deep Learning specialization from Andrew NG on Coursera
</p>

<hr>
<h2 id='pso'>PSO parameters and hyperparameters</h2><span id='topic+pso'></span>

<h3>Description</h3>

<p>List of parameters and hyperparameters for Particle Swarm Optimization
</p>


<h3>Arguments</h3>

<p>All PSO parameters and hyperparameters are preset with default value<br />
</p>
<table>
<tr><td><code id="pso_+3A_psopartpopsize">psopartpopsize</code></td>
<td>
<p> number of particles in swarm (discrete value)<br />
(&lsquo;autopar&rsquo; context: default value 8, which means that 8 different neural net hyperparameters sets will be tested<br />
(&lsquo;hpar&rsquo; context: default value 50, which means that 50 neural net weights sets will be tested<br />
#tuning priority 1</p>
</td></tr></table>
<p> (impact on memory consumption)<br />
</p>
<p><em>CAUTION: you should only change the values below if you know what you are doing</em>
</p>
<table>
<tr><td><code id="pso_+3A_psovarvalmin">psovarvalmin</code></td>
<td>
<p> Minimum value for particles positions (default value -10)</p>
</td></tr>
<tr><td><code id="pso_+3A_psovarvalmax">psovarvalmax</code></td>
<td>
<p> maximum value for particles positions (default value 10)</p>
</td></tr>
<tr><td><code id="pso_+3A_psovelocitymaxratio">psovelocitymaxratio</code></td>
<td>
<p> ratio applied to limit velocities (continuous value between 0 and 1, default value 0.2)</p>
</td></tr>
<tr><td><code id="pso_+3A_psoinertiadampratio">psoinertiadampratio</code></td>
<td>
<p> inertia damp ratio (continuous value between 0 and 1, default value 1 equivalent to OFF)</p>
</td></tr>
<tr><td><code id="pso_+3A_psokappa">psokappa</code></td>
<td>
<p> kappa (default value 1)</p>
</td></tr>
<tr><td><code id="pso_+3A_psophi1">psophi1</code></td>
<td>
<p> Phi 1 (default value 2.05)</p>
</td></tr>
<tr><td><code id="pso_+3A_psophi2">psophi2</code></td>
<td>
<p> Phi 2 (default value 2.05)</p>
</td></tr></table>
<p><br />
</p>
<p><em>back to <a href="#topic+autopar">autopar</a>, <a href="#topic+hpar">hpar</a></em>
</p>


<h3>See Also</h3>

<p>PSO video tutorial from Yarpiz
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
