<!DOCTYPE html><html><head><title>Help for package Lavash</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {Lavash}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#Lavash'><p>Lava Estimation for the Sum of Sparse and Dense Signals</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Lava Estimation for the Sum of Sparse and Dense Signals</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0</td>
</tr>
<tr>
<td>Date:</td>
<td>2018-02-09</td>
</tr>
<tr>
<td>Author:</td>
<td>Victor Chernozhukov [aut, cre], Christian Hansen [aut, cre], Yuan Liao [aut, cre], Jaeheon Jung [ctb, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Jaeheon Jung &lt;jaeheonj81@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>The lava estimation is a new technique to recover signals that is the sum of a sparse and dense signals. The post-lava method corrects the shrinkage bias of lava. For more information on the lava estimation, see Chernozhukov, Hansen, and Liao (2017) &lt;<a href="https://doi.org/10.1214%2F16-AOS1434">doi:10.1214/16-AOS1434</a>&gt;.</td>
</tr>
<tr>
<td>Depends:</td>
<td>MASS, glmnet, pracma</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>6.0.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2018-02-09 19:45:14 UTC; Jung</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2018-02-12 09:25:12 UTC</td>
</tr>
</table>
<hr>
<h2 id='Lavash'>Lava Estimation for the Sum of Sparse and Dense Signals</h2><span id='topic+Lavash'></span>

<h3>Description</h3>

<p>The lava estimation of linear regression models, which are suitable for estimating coefficients that can be represented by the sum of sparse and dense signals. The post-lava method corrects the shrinkage bias of lava. The model is Y=X*b+error, where b can be decomposed into b=dense+sparse. The method of lava solves the following problem: min_[b2,b1] 1/n*|Y-X*(b1+b2)|_2^2+lamda2*|b2|_2^2+lambda1*|b1|_1. The estimator is b2+b1. Both tuning parameters are chosen using the K-fold cross-validation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Lavash(X, Y, K, Lambda1, Lambda2, method = c("profile", "iteration"), Maxiter = 50)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Lavash_+3A_x">X</code></td>
<td>

<p>n by p data matrix, where n and p respectively denot the sample size and the number of regressors.
</p>
</td></tr>
<tr><td><code id="Lavash_+3A_y">Y</code></td>
<td>

<p>n by 1 matrix of outcome.
</p>
</td></tr>
<tr><td><code id="Lavash_+3A_k">K</code></td>
<td>

<p>the K fold cross validation.
</p>
</td></tr>
<tr><td><code id="Lavash_+3A_lambda1">Lambda1</code></td>
<td>

<p>a vector of candidate values to be evaluated for lambda1, in the cross validation.
</p>
</td></tr>
<tr><td><code id="Lavash_+3A_lambda2">Lambda2</code></td>
<td>

<p>a vector of candidate values to be evaluated for lambda2, in the cross validation.
</p>
</td></tr>
<tr><td><code id="Lavash_+3A_method">method</code></td>
<td>

<p>choose between 'profile' and 'iteration'. 'profile' computes using the profiled lasso method. 'iteration' computes using iterating lasso and ridge.
</p>
</td></tr>
<tr><td><code id="Lavash_+3A_maxiter">Maxiter</code></td>
<td>

<p>the maximum number of iterations. the default value is 50. Only used when 'iteration' is selected.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>We recommend using a relatively long vector of Lambda1 (e.g., 50 or 100 values), but a short vector of Lambda2 (e.g., within 10). Higher dimensions of Lambda2 substantially increase the computational time because a 'for' loop is called for lambda2.
Two algorithms are used: (i) the profiled lasso method, building on the singular value decomposition of the design matrix, and runs fast. (whose computational cost is nearly the same as that of lasso) (ii) the iteration between lasso and ridge.
</p>


<h3>Value</h3>

<table>
<tr><td><code>lava_dense</code></td>
<td>
<p>parameter estimate of the dense component using lava</p>
</td></tr>
<tr><td><code>lava_sparse</code></td>
<td>
<p>parameter estimate of the sparse component using lava</p>
</td></tr>
<tr><td><code>lava_estimate</code></td>
<td>
<p>equals lava_dense+lave_estimate: parameter estimate using lava</p>
</td></tr>
<tr><td><code>postlava_dense</code></td>
<td>
<p>parameter estimate of the dense component using post-lava</p>
</td></tr>
<tr><td><code>postlava_sparse</code></td>
<td>
<p>parameter estimate of the sparse component using post-lava</p>
</td></tr>
<tr><td><code>post_lava</code></td>
<td>
<p>equals postlava_dense+postlava_sparse: parameter estimate using post-lava</p>
</td></tr>
<tr><td><code>LAMBDA</code></td>
<td>
<p>[lambda1lava,lambda2lava, lambda1post, lambda2post]: These are the CV-chosen lambda1 and lambda2 tunings for lava and post-lava.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Victor Chernozhukov, Christian Hansen, Yuan Liao, Jaeheon Jung
</p>


<h3>References</h3>

<p>Chernozhukov, V., Hansen, C., and Liao, Y. (2017) &quot;A lava attack on the recovery of sums of dense and sparse signals&quot;, Annals of Statistics, 45, 39-76
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 10; p &lt;- 5
b &lt;- matrix(0,p,1); b[1,1] &lt;- 3; b[2:p,1] &lt;- 0.1
X &lt;- randn(n,p) 
Y &lt;- X%*%b+randn(n,1)
K&lt;-5
iter&lt;-50
Lambda2&lt;-c(0.01, 0.07, 0.2, 0.7, 3,10,60,1000,2000)
Lambda1&lt;-seq(0.01,6,6/50)

result&lt;-Lavash(X,Y,K,Lambda1,Lambda2,method="profile", Maxiter = iter)

result$lava_dense
result$lava_sparse
result$lava_estimate
result$postlava_dense
result$postlava_sparse
result$post_lava
result$LAMBDA
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
