<!DOCTYPE html><html><head><title>Help for package deepregression</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {deepregression}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#+25+26gt+3B+25'><p>Pipe operator</p></a></li>
<li><a href='#check_and_install'><p>Function to check python environment and install necessary packages</p></a></li>
<li><a href='#coef.drEnsemble'><p>Method for extracting ensemble coefficient estimates</p></a></li>
<li><a href='#combine_penalties'><p>Function to combine two penalties</p></a></li>
<li><a href='#create_family'><p>Function to create (custom) family</p></a></li>
<li><a href='#create_penalty'><p>Function to create mgcv-type penalty</p></a></li>
<li><a href='#cv'><p>Generic cv function</p></a></li>
<li><a href='#deepregression'><p>Fitting Semi-Structured Deep Distributional Regression</p></a></li>
<li><a href='#distfun_to_dist'><p>Function to define output distribution based on dist_fun</p></a></li>
<li><a href='#ensemble'><p>Generic deep ensemble function</p></a></li>
<li><a href='#ensemble.deepregression'><p>Ensemblind deepregression models</p></a></li>
<li><a href='#extract_pure_gam_part'><p>Extract the smooth term from a deepregression term specification</p></a></li>
<li><a href='#extract_S'><p>Convenience function to extract penalty matrix and value</p></a></li>
<li><a href='#extractval'><p>Formula helpers</p></a></li>
<li><a href='#extractvar'><p>Extract variable from term</p></a></li>
<li><a href='#family_to_tfd'><p>Character-tfd mapping function</p></a></li>
<li><a href='#family_to_trafo'><p>Character-to-transformation mapping function</p></a></li>
<li><a href='#fitted.drEnsemble'><p>Method for extracting the fitted values of an ensemble</p></a></li>
<li><a href='#form_control'><p>Options for formula parsing</p></a></li>
<li><a href='#from_dist_to_loss'><p>Function to transform a distritbution layer output into a loss function</p></a></li>
<li><a href='#from_preds_to_dist'><p>Define Predictor of a Deep Distributional Regression Model</p></a></li>
<li><a href='#gam_plot_data'><p>used by gam_processor</p></a></li>
<li><a href='#get_distribution'><p>Function to return the fitted distribution</p></a></li>
<li><a href='#get_ensemble_distribution'><p>Obtain the conditional ensemble distribution</p></a></li>
<li><a href='#get_gam_part'><p>Extract gam part from wrapped term</p></a></li>
<li><a href='#get_gamdata'><p>Extract property of gamdata</p></a></li>
<li><a href='#get_gamdata_reduced_nr'><p>Extract number in matching table of reduced gam term</p></a></li>
<li><a href='#get_layer_by_opname'><p>Function to return layer given model and name</p></a></li>
<li><a href='#get_layernr_by_opname'><p>Function to return layer number given model and name</p></a></li>
<li><a href='#get_layernr_trainable'><p>Function to return layer numbers with trainable weights</p></a></li>
<li><a href='#get_names_pfc'><p>Extract term names from the parsed formula content</p></a></li>
<li><a href='#get_partial_effect'><p>Return partial effect of one smooth term</p></a></li>
<li><a href='#get_processor_name'><p>Extract processor name from term</p></a></li>
<li><a href='#get_special'><p>Extract terms defined by specials in formula</p></a></li>
<li><a href='#get_type_pfc'><p>Function to subset parsed formulas</p></a></li>
<li><a href='#get_weight_by_name'><p>Function to retrieve the weights of a structured layer</p></a></li>
<li><a href='#get_weight_by_opname'><p>Function to return weight given model and name</p></a></li>
<li><a href='#handle_gam_term'><p>Function to define smoothness and call mgcv's smooth constructor</p></a></li>
<li><a href='#keras_dr'><p>Compile a Deep Distributional Regression Model</p></a></li>
<li><a href='#layer_add_identity'><p>Convenience layer function</p></a></li>
<li><a href='#layer_generator'><p>Function that creates layer for each processor</p></a></li>
<li><a href='#layer_sparse_conv_2d'><p>Sparse 2D Convolutional layer</p></a></li>
<li><a href='#layer_spline'><p>Function to define spline as TensorFlow layer</p></a></li>
<li><a href='#log_score'><p>Function to return the log_score</p></a></li>
<li><a href='#loop_through_pfc_and_call_trafo'><p>Function to loop through parsed formulas and apply data trafo</p></a></li>
<li><a href='#make_folds'><p>Generate folds for CV out of one hot encoded matrix</p></a></li>
<li><a href='#make_generator'><p>creates a generator for training</p></a></li>
<li><a href='#make_generator_from_matrix'><p>Make a DataGenerator from a data.frame or matrix</p></a></li>
<li><a href='#make_tfd_dist'><p>Families for deepregression</p></a></li>
<li><a href='#makeInputs'><p>Convenience layer function</p></a></li>
<li><a href='#makelayername'><p>Function that takes term and create layer name</p></a></li>
<li><a href='#multioptimizer'><p>Function to define an optimizer combining multiple optimizers</p></a></li>
<li><a href='#names_families'><p>Returns the parameter names for a given family</p></a></li>
<li><a href='#orthog_control'><p>Options for orthogonalization</p></a></li>
<li><a href='#orthog_P'><p>Function to compute adjusted penalty when orthogonalizing</p></a></li>
<li><a href='#orthog_post_fitting'><p>Orthogonalize a Semi-Structured Model Post-hoc</p></a></li>
<li><a href='#orthog_structured_smooths_Z'><p>Orthogonalize structured term by another matrix</p></a></li>
<li><a href='#penalty_control'><p>Options for penalty setup in the pre-processing</p></a></li>
<li><a href='#plot_cv'><p>Plot CV results from deepregression</p></a></li>
<li><a href='#plot.deepregression'><p>Generic functions for deepregression models</p></a></li>
<li><a href='#precalc_gam'><p>Pre-calculate all gam parts from the list of formulas</p></a></li>
<li><a href='#predict_gen'><p>Generator function for deepregression objects</p></a></li>
<li><a href='#prepare_data'><p>Function to prepare data based on parsed formulas</p></a></li>
<li><a href='#prepare_newdata'><p>Function to prepare new data based on parsed formulas</p></a></li>
<li><a href='#process_terms'><p>Control function to define the processor for terms in the formula</p></a></li>
<li><a href='#quant'><p>Generic quantile function</p></a></li>
<li><a href='#reinit_weights'><p>Genereic function to re-intialize model weights</p></a></li>
<li><a href='#reinit_weights.deepregression'><p>Method to re-initialize weights of a <code>"deepregression"</code> model</p></a></li>
<li><a href='#separate_define_relation'><p>Function to define orthogonalization connections in the formula</p></a></li>
<li><a href='#stddev'><p>Generic sd function</p></a></li>
<li><a href='#stop_iter_cv_result'><p>Function to get the stoppting iteration from CV</p></a></li>
<li><a href='#subnetwork_init'><p>Initializes a Subnetwork based on the Processed Additive Predictor</p></a></li>
<li><a href='#tf_repeat'><p>TensorFlow repeat function which is not available for TF 2.0</p></a></li>
<li><a href='#tf_row_tensor'><p>Row-wise tensor product using TensorFlow</p></a></li>
<li><a href='#tf_split_multiple'><p>Split tensor in multiple parts</p></a></li>
<li><a href='#tf_stride_cols'><p>Function to index tensors columns</p></a></li>
<li><a href='#tf_stride_last_dim_tensor'><p>Function to index tensors last dimension</p></a></li>
<li><a href='#tfd_mse'><p>For using mean squared error via TFP</p></a></li>
<li><a href='#tfd_zinb'><p>Implementation of a zero-inflated negbinom distribution for TFP</p></a></li>
<li><a href='#tfd_zip'><p>Implementation of a zero-inflated poisson distribution for TFP</p></a></li>
<li><a href='#tib_layer'><p>Hadamard-type layers</p></a></li>
<li><a href='#update_miniconda_deepregression'><p>Function to update miniconda and packages</p></a></li>
<li><a href='#weight_control'><p>Options for weights of layers</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Fitting Deep Distributional Regression</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.0</td>
</tr>
<tr>
<td>Description:</td>
<td>
    Allows for the specification of semi-structured deep distributional regression models which are fitted in a neural network as 
    proposed by Ruegamer et al. (2023) &lt;<a href="https://doi.org/10.18637%2Fjss.v105.i02">doi:10.18637/jss.v105.i02</a>&gt;.
    Predictors can be modeled using structured (penalized) linear effects, structured non-linear effects or using an unstructured deep network model.</td>
</tr>
<tr>
<td>Config/reticulate:</td>
<td>list( packages = list( list(package = "six", pip =
TRUE), list(package = "tensorflow", version = "2.10.0", pip =
TRUE), list(package = "tensorflow_probability", version =
"0.16", pip = TRUE), list(package = "keras", version =
"2.10.0", pip = TRUE)) )</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.0.0), tensorflow (&ge; 2.2.0), tfprobability, keras (&ge;
2.2.0)</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat, knitr, covr</td>
</tr>
<tr>
<td>Imports:</td>
<td>mgcv, dplyr, R6, reticulate (&ge; 1.14), Matrix, magrittr,
tfruns, methods</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.2</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-01-17 22:28:56 UTC; david</td>
</tr>
<tr>
<td>Author:</td>
<td>David Ruegamer [aut, cre],
  Florian Pfisterer [ctb],
  Philipp Baumann [ctb],
  Chris Kolb [ctb],
  Lucas Kook [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>David Ruegamer &lt;david.ruegamer@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-01-17 22:50:08 UTC</td>
</tr>
</table>
<hr>
<h2 id='+25+26gt+3B+25'>Pipe operator</h2><span id='topic++25+3E+25'></span>

<h3>Description</h3>

<p>See <code>magrittr::<a href="magrittr.html#topic+pipe">%&gt;%</a></code> for details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lhs %&gt;% rhs
</code></pre>

<hr>
<h2 id='check_and_install'>Function to check python environment and install necessary packages</h2><span id='topic+check_and_install'></span>

<h3>Description</h3>

<p>If you encounter problems with installing the required python modules
please make sure, that a correct python version is configured using
<code>py_discover_config</code> and change the python version if required.
Internally uses <code>keras::install_keras</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_and_install(force = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_and_install_+3A_force">force</code></td>
<td>
<p>if TRUE, forces the installations</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Function that checks if a Python environment is available
and contains TensorFlow. If not the recommended version is installed.
</p>

<hr>
<h2 id='coef.drEnsemble'>Method for extracting ensemble coefficient estimates</h2><span id='topic+coef.drEnsemble'></span>

<h3>Description</h3>

<p>Method for extracting ensemble coefficient estimates
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'drEnsemble'
coef(object, which_param = 1, type = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="coef.drEnsemble_+3A_object">object</code></td>
<td>
<p>object of class <code>"drEnsemble"</code></p>
</td></tr>
<tr><td><code id="coef.drEnsemble_+3A_which_param">which_param</code></td>
<td>
<p>integer, indicating for which distribution parameter
coefficients should be returned (default is first parameter)</p>
</td></tr>
<tr><td><code id="coef.drEnsemble_+3A_type">type</code></td>
<td>
<p>either NULL (all types of coefficients are returned),
&quot;linear&quot; for linear coefficients or &quot;smooth&quot; for coefficients of 
smooth terms</p>
</td></tr>
<tr><td><code id="coef.drEnsemble_+3A_...">...</code></td>
<td>
<p>further arguments supplied to <code>coef.deepregression</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>list of coefficient estimates of all ensemble members
</p>

<hr>
<h2 id='combine_penalties'>Function to combine two penalties</h2><span id='topic+combine_penalties'></span>

<h3>Description</h3>

<p>Function to combine two penalties
</p>


<h3>Usage</h3>

<pre><code class='language-R'>combine_penalties(penalties, dims)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="combine_penalties_+3A_penalties">penalties</code></td>
<td>
<p>a list of penalties</p>
</td></tr>
<tr><td><code id="combine_penalties_+3A_dims">dims</code></td>
<td>
<p>dimensions of the parameters to penalize</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a TensorFlow penalty combining the two penalties
</p>

<hr>
<h2 id='create_family'>Function to create (custom) family</h2><span id='topic+create_family'></span>

<h3>Description</h3>

<p>Function to create (custom) family
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_family(tfd_dist, trafo_list, output_dim = 1L)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="create_family_+3A_tfd_dist">tfd_dist</code></td>
<td>
<p>a tensorflow probability distribution</p>
</td></tr>
<tr><td><code id="create_family_+3A_trafo_list">trafo_list</code></td>
<td>
<p>list of transformations h for each parameter 
(e.g, <code>exp</code> for a variance parameter)</p>
</td></tr>
<tr><td><code id="create_family_+3A_output_dim">output_dim</code></td>
<td>
<p>integer defining the size of the response</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a function that can be used by 
<code>tfp$layers$DistributionLambda</code> to create a new 
distribuional layer
</p>

<hr>
<h2 id='create_penalty'>Function to create mgcv-type penalty</h2><span id='topic+create_penalty'></span>

<h3>Description</h3>

<p>Function to create mgcv-type penalty
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_penalty(evaluated_gam_term, df, controls, Z = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="create_penalty_+3A_evaluated_gam_term">evaluated_gam_term</code></td>
<td>
<p>a list resulting from a smoothConstruct call</p>
</td></tr>
<tr><td><code id="create_penalty_+3A_df">df</code></td>
<td>
<p>integer; specified degrees-of-freedom for the gam term</p>
</td></tr>
<tr><td><code id="create_penalty_+3A_controls">controls</code></td>
<td>
<p>list; further arguments defining the smooth</p>
</td></tr>
<tr><td><code id="create_penalty_+3A_z">Z</code></td>
<td>
<p>matrix; matrix for constraint(s)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list with penalty parameter and penalty matrix
</p>

<hr>
<h2 id='cv'>Generic cv function</h2><span id='topic+cv'></span>

<h3>Description</h3>

<p>Generic cv function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cv(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cv_+3A_x">x</code></td>
<td>
<p>model to do cv on</p>
</td></tr>
<tr><td><code id="cv_+3A_...">...</code></td>
<td>
<p>further arguments passed to the class-specific function</p>
</td></tr>
</table>

<hr>
<h2 id='deepregression'>Fitting Semi-Structured Deep Distributional Regression</h2><span id='topic+deepregression'></span>

<h3>Description</h3>

<p>Fitting Semi-Structured Deep Distributional Regression
</p>


<h3>Usage</h3>

<pre><code class='language-R'>deepregression(
  y,
  list_of_formulas,
  list_of_deep_models = NULL,
  family = "normal",
  data,
  tf_seed = as.integer(1991 - 5 - 4),
  return_prepoc = FALSE,
  subnetwork_builder = subnetwork_init,
  model_builder = keras_dr,
  fitting_function = utils::getFromNamespace("fit.keras.engine.training.Model",
    "keras"),
  additional_processors = list(),
  penalty_options = penalty_control(),
  orthog_options = orthog_control(),
  weight_options = weight_control(),
  formula_options = form_control(),
  output_dim = 1L,
  verbose = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="deepregression_+3A_y">y</code></td>
<td>
<p>response variable</p>
</td></tr>
<tr><td><code id="deepregression_+3A_list_of_formulas">list_of_formulas</code></td>
<td>
<p>a named list of right hand side formulas,
one for each parameter of the distribution specified in <code>family</code>;
set to <code>~ 1</code> if the parameter should be treated as constant.
Use the <code>s()</code>-notation from <code>mgcv</code> for specification of
non-linear structured effects and <code>d(...)</code> for
deep learning predictors (predictors in brackets are separated by commas),
where <code>d</code> can be replaced by an name name of the names in
<code>list_of_deep_models</code>, e.g., <code>~ 1 + s(x) + my_deep_mod(a,b,c)</code>,
where my_deep_mod is the name of the neural net specified in
<code>list_of_deep_models</code> and <code>a,b,c</code> are features modeled via
this network.</p>
</td></tr>
<tr><td><code id="deepregression_+3A_list_of_deep_models">list_of_deep_models</code></td>
<td>
<p>a named list of functions specifying a keras model.
See the examples for more details.</p>
</td></tr>
<tr><td><code id="deepregression_+3A_family">family</code></td>
<td>
<p>a character specifying the distribution. For information on
possible distribution and parameters, see <code><a href="#topic+make_tfd_dist">make_tfd_dist</a></code>. Can also
be a custom distribution.</p>
</td></tr>
<tr><td><code id="deepregression_+3A_data">data</code></td>
<td>
<p>data.frame or named list with input features</p>
</td></tr>
<tr><td><code id="deepregression_+3A_tf_seed">tf_seed</code></td>
<td>
<p>a seed for TensorFlow (only works with R version &gt;= 2.2.0)</p>
</td></tr>
<tr><td><code id="deepregression_+3A_return_prepoc">return_prepoc</code></td>
<td>
<p>logical; if TRUE only the pre-processed data and layers are returned 
(default FALSE).</p>
</td></tr>
<tr><td><code id="deepregression_+3A_subnetwork_builder">subnetwork_builder</code></td>
<td>
<p>function to build each subnetwork (network for each distribution parameter;
per default <code>subnetwork_init</code>). Can also be a list of the same size as
<code>list_of_formulas</code>.</p>
</td></tr>
<tr><td><code id="deepregression_+3A_model_builder">model_builder</code></td>
<td>
<p>function to build the model based on additive predictors 
(per default <code>keras_dr</code>). In order to work with the methods defined for the class 
<code>deepregression</code>, the model should behave like a keras model</p>
</td></tr>
<tr><td><code id="deepregression_+3A_fitting_function">fitting_function</code></td>
<td>
<p>function to fit the instantiated model when calling <code>fit</code>. Per default
the keras <code>fit</code> function.</p>
</td></tr>
<tr><td><code id="deepregression_+3A_additional_processors">additional_processors</code></td>
<td>
<p>a named list with additional processors to convert the formula(s).
Can have an attribute <code>"controls"</code> to pass additional controls</p>
</td></tr>
<tr><td><code id="deepregression_+3A_penalty_options">penalty_options</code></td>
<td>
<p>options for smoothing and penalty terms defined by <code><a href="#topic+penalty_control">penalty_control</a></code></p>
</td></tr>
<tr><td><code id="deepregression_+3A_orthog_options">orthog_options</code></td>
<td>
<p>options for the orthgonalization defined by <code><a href="#topic+orthog_control">orthog_control</a></code></p>
</td></tr>
<tr><td><code id="deepregression_+3A_weight_options">weight_options</code></td>
<td>
<p>options for layer weights defined by <code><a href="#topic+weight_control">weight_control</a></code></p>
</td></tr>
<tr><td><code id="deepregression_+3A_formula_options">formula_options</code></td>
<td>
<p>options for formula parsing (mainly used to make calculation more efficiently)</p>
</td></tr>
<tr><td><code id="deepregression_+3A_output_dim">output_dim</code></td>
<td>
<p>dimension of the output, per default 1L</p>
</td></tr>
<tr><td><code id="deepregression_+3A_verbose">verbose</code></td>
<td>
<p>logical; whether to print progress of model initialization to console</p>
</td></tr>
<tr><td><code id="deepregression_+3A_...">...</code></td>
<td>
<p>further arguments passed to the <code>model_builder</code> function</p>
</td></tr>
</table>


<h3>References</h3>

<p>Ruegamer, D. et al. (2023):
deepregression: a Flexible Neural Network Framework for Semi-Structured Deep Distributional Regression.
<a href="https://doi.org/10.18637/jss.v105.i02">doi:10.18637/jss.v105.i02</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(deepregression)

n &lt;- 1000
data = data.frame(matrix(rnorm(4*n), c(n,4)))
colnames(data) &lt;- c("x1","x2","x3","xa")
formula &lt;- ~ 1 + deep_model(x1,x2,x3) + s(xa) + x1

deep_model &lt;- function(x) x %&gt;%
layer_dense(units = 32, activation = "relu", use_bias = FALSE) %&gt;%
layer_dropout(rate = 0.2) %&gt;%
layer_dense(units = 8, activation = "relu") %&gt;%
layer_dense(units = 1, activation = "linear")

y &lt;- rnorm(n) + data$xa^2 + data$x1

mod &lt;- deepregression(
  list_of_formulas = list(loc = formula, scale = ~ 1),
  data = data, y = y,
  list_of_deep_models = list(deep_model = deep_model)
)

if(!is.null(mod)){

# train for more than 10 epochs to get a better model
mod %&gt;% fit(epochs = 10, early_stopping = TRUE)
mod %&gt;% fitted() %&gt;% head()
cvres &lt;- mod %&gt;% cv()
mod %&gt;% get_partial_effect(name = "s(xa)")
mod %&gt;% coef()
mod %&gt;% plot()

}

mod &lt;- deepregression(
  list_of_formulas = list(loc = ~ 1 + s(xa) + x1, scale = ~ 1,
                          dummy = ~ -1 + deep_model(x1,x2,x3) %OZ% 1),
  data = data, y = y,
  list_of_deep_models = list(deep_model = deep_model),
  mapping = list(1,2,1:2)
)

</code></pre>

<hr>
<h2 id='distfun_to_dist'>Function to define output distribution based on dist_fun</h2><span id='topic+distfun_to_dist'></span>

<h3>Description</h3>

<p>Function to define output distribution based on dist_fun
</p>


<h3>Usage</h3>

<pre><code class='language-R'>distfun_to_dist(dist_fun, preds)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="distfun_to_dist_+3A_dist_fun">dist_fun</code></td>
<td>
<p>a distribution function as defined by <code>make_tfd_dist</code></p>
</td></tr>
<tr><td><code id="distfun_to_dist_+3A_preds">preds</code></td>
<td>
<p>tensors with predictions</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a symbolic tfp distribution
</p>

<hr>
<h2 id='ensemble'>Generic deep ensemble function</h2><span id='topic+ensemble'></span>

<h3>Description</h3>

<p>Generic deep ensemble function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ensemble(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ensemble_+3A_x">x</code></td>
<td>
<p>model to ensemble</p>
</td></tr>
<tr><td><code id="ensemble_+3A_...">...</code></td>
<td>
<p>further arguments passed to the class-specific function</p>
</td></tr>
</table>

<hr>
<h2 id='ensemble.deepregression'>Ensemblind deepregression models</h2><span id='topic+ensemble.deepregression'></span>

<h3>Description</h3>

<p>Ensemblind deepregression models
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'deepregression'
ensemble(
  x,
  n_ensemble = 5,
  reinitialize = TRUE,
  mylapply = lapply,
  verbose = FALSE,
  patience = 20,
  plot = TRUE,
  print_members = TRUE,
  stop_if_nan = TRUE,
  save_weights = TRUE,
  callbacks = list(),
  save_fun = NULL,
  seed = seq_len(n_ensemble),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ensemble.deepregression_+3A_x">x</code></td>
<td>
<p>object of class <code>"deepregression"</code> to ensemble</p>
</td></tr>
<tr><td><code id="ensemble.deepregression_+3A_n_ensemble">n_ensemble</code></td>
<td>
<p>numeric; number of ensemble members to fit</p>
</td></tr>
<tr><td><code id="ensemble.deepregression_+3A_reinitialize">reinitialize</code></td>
<td>
<p>logical; if <code>TRUE</code> (default), model weights are
initialized randomly prior to fitting each member. Fixed weights are
not affected</p>
</td></tr>
<tr><td><code id="ensemble.deepregression_+3A_mylapply">mylapply</code></td>
<td>
<p>lapply function to be used; defaults to <code>lapply</code></p>
</td></tr>
<tr><td><code id="ensemble.deepregression_+3A_verbose">verbose</code></td>
<td>
<p>whether to print training in each fold</p>
</td></tr>
<tr><td><code id="ensemble.deepregression_+3A_patience">patience</code></td>
<td>
<p>number of patience for early stopping</p>
</td></tr>
<tr><td><code id="ensemble.deepregression_+3A_plot">plot</code></td>
<td>
<p>whether to plot the resulting losses in each fold</p>
</td></tr>
<tr><td><code id="ensemble.deepregression_+3A_print_members">print_members</code></td>
<td>
<p>logical; print results for each member</p>
</td></tr>
<tr><td><code id="ensemble.deepregression_+3A_stop_if_nan">stop_if_nan</code></td>
<td>
<p>logical; whether to stop CV if NaN values occur</p>
</td></tr>
<tr><td><code id="ensemble.deepregression_+3A_save_weights">save_weights</code></td>
<td>
<p>whether to save final weights of each ensemble member;
defaults to <code>TRUE</code></p>
</td></tr>
<tr><td><code id="ensemble.deepregression_+3A_callbacks">callbacks</code></td>
<td>
<p>a list of callbacks used for fitting</p>
</td></tr>
<tr><td><code id="ensemble.deepregression_+3A_save_fun">save_fun</code></td>
<td>
<p>function applied to the model in each fold to be stored in
the final result</p>
</td></tr>
<tr><td><code id="ensemble.deepregression_+3A_seed">seed</code></td>
<td>
<p>seed for reproducibility</p>
</td></tr>
<tr><td><code id="ensemble.deepregression_+3A_...">...</code></td>
<td>
<p>further arguments passed to <code>object$fit_fun</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>object of class <code>"drEnsemble"</code>, containing the original
<code>"deepregression"</code> model together with a list of ensembling
results (training history and, if <code>save_weights</code> is <code>TRUE</code>,
the trained weights of each ensemble member)
</p>

<hr>
<h2 id='extract_pure_gam_part'>Extract the smooth term from a deepregression term specification</h2><span id='topic+extract_pure_gam_part'></span>

<h3>Description</h3>

<p>Extract the smooth term from a deepregression term specification
</p>


<h3>Usage</h3>

<pre><code class='language-R'>extract_pure_gam_part(term, remove_other_options = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="extract_pure_gam_part_+3A_term">term</code></td>
<td>
<p>term specified in a formula</p>
</td></tr>
<tr><td><code id="extract_pure_gam_part_+3A_remove_other_options">remove_other_options</code></td>
<td>
<p>logical; whether to remove other options
withing the smooth term</p>
</td></tr>
</table>


<h3>Value</h3>

<p>pure gam part of term
</p>

<hr>
<h2 id='extract_S'>Convenience function to extract penalty matrix and value</h2><span id='topic+extract_S'></span>

<h3>Description</h3>

<p>Convenience function to extract penalty matrix and value
</p>


<h3>Usage</h3>

<pre><code class='language-R'>extract_S(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="extract_S_+3A_x">x</code></td>
<td>
<p>evaluated smooth term object</p>
</td></tr>
</table>

<hr>
<h2 id='extractval'>Formula helpers</h2><span id='topic+extractval'></span><span id='topic+extractlen'></span><span id='topic+form2text'></span>

<h3>Description</h3>

<p>Formula helpers
</p>


<h3>Usage</h3>

<pre><code class='language-R'>extractval(term, name, default_for_missing = FALSE, default = NULL)

extractlen(term, data)

form2text(form)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="extractval_+3A_term">term</code></td>
<td>
<p>formula term</p>
</td></tr>
<tr><td><code id="extractval_+3A_name">name</code></td>
<td>
<p>character; the value to extract</p>
</td></tr>
<tr><td><code id="extractval_+3A_default_for_missing">default_for_missing</code></td>
<td>
<p>logical; if TRUE, returns <code>default</code> if argument is missing</p>
</td></tr>
<tr><td><code id="extractval_+3A_default">default</code></td>
<td>
<p>value returned when missing</p>
</td></tr>
<tr><td><code id="extractval_+3A_data">data</code></td>
<td>
<p>a data.frame or list</p>
</td></tr>
<tr><td><code id="extractval_+3A_form">form</code></td>
<td>
<p>formula that is converted to a character string</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the value used for <code>name</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>extractval("s(a, la = 2)", "la")

</code></pre>

<hr>
<h2 id='extractvar'>Extract variable from term</h2><span id='topic+extractvar'></span>

<h3>Description</h3>

<p>Extract variable from term
</p>


<h3>Usage</h3>

<pre><code class='language-R'>extractvar(term, allow_ia = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="extractvar_+3A_term">term</code></td>
<td>
<p>term specified in formula</p>
</td></tr>
<tr><td><code id="extractvar_+3A_allow_ia">allow_ia</code></td>
<td>
<p>logical; whether to allow interaction of terms
using the <code>:</code> notation</p>
</td></tr>
</table>


<h3>Value</h3>

<p>variable as string
</p>

<hr>
<h2 id='family_to_tfd'>Character-tfd mapping function</h2><span id='topic+family_to_tfd'></span>

<h3>Description</h3>

<p>Character-tfd mapping function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>family_to_tfd(family)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="family_to_tfd_+3A_family">family</code></td>
<td>
<p>character defining the distribution</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a tfp distribution
</p>

<hr>
<h2 id='family_to_trafo'>Character-to-transformation mapping function</h2><span id='topic+family_to_trafo'></span>

<h3>Description</h3>

<p>Character-to-transformation mapping function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>family_to_trafo(family, add_const = 1e-08)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="family_to_trafo_+3A_family">family</code></td>
<td>
<p>character defining the distribution</p>
</td></tr>
<tr><td><code id="family_to_trafo_+3A_add_const">add_const</code></td>
<td>
<p>see <code><a href="#topic+make_tfd_dist">make_tfd_dist</a></code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list of transformation for each distribution parameter
</p>

<hr>
<h2 id='fitted.drEnsemble'>Method for extracting the fitted values of an ensemble</h2><span id='topic+fitted.drEnsemble'></span>

<h3>Description</h3>

<p>Method for extracting the fitted values of an ensemble
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'drEnsemble'
fitted(object, apply_fun = tfd_mean, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fitted.drEnsemble_+3A_object">object</code></td>
<td>
<p>a deepregression model</p>
</td></tr>
<tr><td><code id="fitted.drEnsemble_+3A_apply_fun">apply_fun</code></td>
<td>
<p>function applied to fitted distribution,
per default <code>tfd_mean</code></p>
</td></tr>
<tr><td><code id="fitted.drEnsemble_+3A_...">...</code></td>
<td>
<p>arguments passed to the <code>predict</code> function</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list of fitted values for each ensemble member
</p>

<hr>
<h2 id='form_control'>Options for formula parsing</h2><span id='topic+form_control'></span>

<h3>Description</h3>

<p>Options for formula parsing
</p>


<h3>Usage</h3>

<pre><code class='language-R'>form_control(precalculate_gamparts = TRUE, check_form = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="form_control_+3A_precalculate_gamparts">precalculate_gamparts</code></td>
<td>
<p>logical; if TRUE (default), additive parts are pre-calculated
and can later be used more efficiently. Set to FALSE only if no smooth effects are in the 
formula(s) and a formula is very large so that extracting all terms takes long or might fail</p>
</td></tr>
<tr><td><code id="form_control_+3A_check_form">check_form</code></td>
<td>
<p>logical; if TRUE (default), the formula is checked in <code>process_terms</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a list with options
</p>

<hr>
<h2 id='from_dist_to_loss'>Function to transform a distritbution layer output into a loss function</h2><span id='topic+from_dist_to_loss'></span>

<h3>Description</h3>

<p>Function to transform a distritbution layer output into a loss function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>from_dist_to_loss(
  family,
  ind_fun = function(x) tfd_independent(x),
  weights = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="from_dist_to_loss_+3A_family">family</code></td>
<td>
<p>see <code>?deepregression</code></p>
</td></tr>
<tr><td><code id="from_dist_to_loss_+3A_ind_fun">ind_fun</code></td>
<td>
<p>function applied to the model output before calculating the
log-likelihood. Per default independence is assumed by applying <code>tfd_independent</code>.</p>
</td></tr>
<tr><td><code id="from_dist_to_loss_+3A_weights">weights</code></td>
<td>
<p>sample weights</p>
</td></tr>
</table>


<h3>Value</h3>

<p>loss function
</p>

<hr>
<h2 id='from_preds_to_dist'>Define Predictor of a Deep Distributional Regression Model</h2><span id='topic+from_preds_to_dist'></span>

<h3>Description</h3>

<p>Define Predictor of a Deep Distributional Regression Model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>from_preds_to_dist(
  list_pred_param,
  family = NULL,
  output_dim = 1L,
  mapping = NULL,
  from_family_to_distfun = make_tfd_dist,
  from_distfun_to_dist = distfun_to_dist,
  add_layer_shared_pred = function(x, units) layer_dense(x, units = units, use_bias =
    FALSE),
  trafo_list = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="from_preds_to_dist_+3A_list_pred_param">list_pred_param</code></td>
<td>
<p>list of input-output(-lists) generated from
<code>subnetwork_init</code></p>
</td></tr>
<tr><td><code id="from_preds_to_dist_+3A_family">family</code></td>
<td>
<p>see <code>?deepregression</code>; if NULL, concatenated
<code>list_pred_param</code> entries are returned (after applying mapping if provided)</p>
</td></tr>
<tr><td><code id="from_preds_to_dist_+3A_output_dim">output_dim</code></td>
<td>
<p>dimension of the output</p>
</td></tr>
<tr><td><code id="from_preds_to_dist_+3A_mapping">mapping</code></td>
<td>
<p>a list of integers. The i-th list item defines which element
elements of <code>list_pred_param</code> are used for the i-th parameter.
For example, <code>mapping = list(1,2,1:2)</code> means that <code>list_pred_param[[1]]</code>
is used for the first distribution parameter, <code>list_pred_param[[2]]</code> for
the second distribution parameter and  <code>list_pred_param[[3]]</code> for both
distribution parameters (and then added once to <code>list_pred_param[[1]]</code> and
once to <code>list_pred_param[[2]]</code>)</p>
</td></tr>
<tr><td><code id="from_preds_to_dist_+3A_from_family_to_distfun">from_family_to_distfun</code></td>
<td>
<p>function to create a <code>dist_fun</code> 
(see <code>?distfun_to_dist</code>) from the given character <code>family</code></p>
</td></tr>
<tr><td><code id="from_preds_to_dist_+3A_from_distfun_to_dist">from_distfun_to_dist</code></td>
<td>
<p>function creating a tfp distribution based on the
prediction tensors and <code>dist_fun</code>. See <code>?distfun_to_dist</code></p>
</td></tr>
<tr><td><code id="from_preds_to_dist_+3A_add_layer_shared_pred">add_layer_shared_pred</code></td>
<td>
<p>layer to extend shared layers defined in <code>mapping</code></p>
</td></tr>
<tr><td><code id="from_preds_to_dist_+3A_trafo_list">trafo_list</code></td>
<td>
<p>a list of transformation function to convert the scale of the
additive predictors to the respective distribution parameter</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list with input tensors and output tensors that can be passed
to, e.g., <code>keras_model</code>
</p>

<hr>
<h2 id='gam_plot_data'>used by gam_processor</h2><span id='topic+gam_plot_data'></span>

<h3>Description</h3>

<p>used by gam_processor
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gam_plot_data(pp, weights, grid_length = 40, pe_fun = pe_gen)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gam_plot_data_+3A_pp">pp</code></td>
<td>
<p>processed term</p>
</td></tr>
<tr><td><code id="gam_plot_data_+3A_weights">weights</code></td>
<td>
<p>layer weights</p>
</td></tr>
<tr><td><code id="gam_plot_data_+3A_grid_length">grid_length</code></td>
<td>
<p>length for grid for evaluating basis</p>
</td></tr>
<tr><td><code id="gam_plot_data_+3A_pe_fun">pe_fun</code></td>
<td>
<p>function used to generate partial effects</p>
</td></tr>
</table>

<hr>
<h2 id='get_distribution'>Function to return the fitted distribution</h2><span id='topic+get_distribution'></span>

<h3>Description</h3>

<p>Function to return the fitted distribution
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_distribution(x, data = NULL, force_float = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_distribution_+3A_x">x</code></td>
<td>
<p>the fitted deepregression object</p>
</td></tr>
<tr><td><code id="get_distribution_+3A_data">data</code></td>
<td>
<p>an optional data set</p>
</td></tr>
<tr><td><code id="get_distribution_+3A_force_float">force_float</code></td>
<td>
<p>forces conversion into float tensors</p>
</td></tr>
</table>

<hr>
<h2 id='get_ensemble_distribution'>Obtain the conditional ensemble distribution</h2><span id='topic+get_ensemble_distribution'></span>

<h3>Description</h3>

<p>Obtain the conditional ensemble distribution
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_ensemble_distribution(object, data = NULL, topK = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_ensemble_distribution_+3A_object">object</code></td>
<td>
<p>object of class <code>"drEnsemble"</code></p>
</td></tr>
<tr><td><code id="get_ensemble_distribution_+3A_data">data</code></td>
<td>
<p>data for which to return the fitted distribution</p>
</td></tr>
<tr><td><code id="get_ensemble_distribution_+3A_topk">topK</code></td>
<td>
<p>not implemented yet</p>
</td></tr>
<tr><td><code id="get_ensemble_distribution_+3A_...">...</code></td>
<td>
<p>further arguments currently ignored</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>tfd_distribution</code> of the ensemble, i.e., a mixture of the
ensemble member's predicted distributions conditional on <code>data</code>
</p>

<hr>
<h2 id='get_gam_part'>Extract gam part from wrapped term</h2><span id='topic+get_gam_part'></span>

<h3>Description</h3>

<p>Extract gam part from wrapped term
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_gam_part(term, wrapper = "vc")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_gam_part_+3A_term">term</code></td>
<td>
<p>character; gam model term</p>
</td></tr>
<tr><td><code id="get_gam_part_+3A_wrapper">wrapper</code></td>
<td>
<p>character; function name that is wrapped around the gam part</p>
</td></tr>
</table>

<hr>
<h2 id='get_gamdata'>Extract property of gamdata</h2><span id='topic+get_gamdata'></span>

<h3>Description</h3>

<p>Extract property of gamdata
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_gamdata(
  term,
  param_nr,
  gamdata,
  what = c("data_trafo", "predict_trafo", "input_dim", "partial_effect", "sp_and_S",
    "df")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_gamdata_+3A_term">term</code></td>
<td>
<p>term in formula</p>
</td></tr>
<tr><td><code id="get_gamdata_+3A_param_nr">param_nr</code></td>
<td>
<p>integer; number of the distribution parameter</p>
</td></tr>
<tr><td><code id="get_gamdata_+3A_gamdata">gamdata</code></td>
<td>
<p>list as returned by <code>precalc_gam</code></p>
</td></tr>
<tr><td><code id="get_gamdata_+3A_what">what</code></td>
<td>
<p>string specifying what to return</p>
</td></tr>
</table>


<h3>Value</h3>

<p>property of the gamdata object as defined by <code>what</code>
</p>

<hr>
<h2 id='get_gamdata_reduced_nr'>Extract number in matching table of reduced gam term</h2><span id='topic+get_gamdata_reduced_nr'></span>

<h3>Description</h3>

<p>Extract number in matching table of reduced gam term
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_gamdata_reduced_nr(term, param_nr, gamdata)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_gamdata_reduced_nr_+3A_term">term</code></td>
<td>
<p>term in formula</p>
</td></tr>
<tr><td><code id="get_gamdata_reduced_nr_+3A_param_nr">param_nr</code></td>
<td>
<p>integer; number of the distribution parameter</p>
</td></tr>
<tr><td><code id="get_gamdata_reduced_nr_+3A_gamdata">gamdata</code></td>
<td>
<p>list as returned by <code>precalc_gam</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>integer with number of gam term in matching table
</p>

<hr>
<h2 id='get_layer_by_opname'>Function to return layer given model and name</h2><span id='topic+get_layer_by_opname'></span>

<h3>Description</h3>

<p>Function to return layer given model and name
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_layer_by_opname(mod, name, partial_match = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_layer_by_opname_+3A_mod">mod</code></td>
<td>
<p>deepregression model</p>
</td></tr>
<tr><td><code id="get_layer_by_opname_+3A_name">name</code></td>
<td>
<p>character</p>
</td></tr>
<tr><td><code id="get_layer_by_opname_+3A_partial_match">partial_match</code></td>
<td>
<p>logical; whether to also check for a partial match</p>
</td></tr>
</table>

<hr>
<h2 id='get_layernr_by_opname'>Function to return layer number given model and name</h2><span id='topic+get_layernr_by_opname'></span>

<h3>Description</h3>

<p>Function to return layer number given model and name
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_layernr_by_opname(mod, name, partial_match = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_layernr_by_opname_+3A_mod">mod</code></td>
<td>
<p>deepregression model</p>
</td></tr>
<tr><td><code id="get_layernr_by_opname_+3A_name">name</code></td>
<td>
<p>character</p>
</td></tr>
<tr><td><code id="get_layernr_by_opname_+3A_partial_match">partial_match</code></td>
<td>
<p>logical; whether to also check for a partial match</p>
</td></tr>
</table>

<hr>
<h2 id='get_layernr_trainable'>Function to return layer numbers with trainable weights</h2><span id='topic+get_layernr_trainable'></span>

<h3>Description</h3>

<p>Function to return layer numbers with trainable weights
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_layernr_trainable(mod, logic = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_layernr_trainable_+3A_mod">mod</code></td>
<td>
<p>deepregression model</p>
</td></tr>
<tr><td><code id="get_layernr_trainable_+3A_logic">logic</code></td>
<td>
<p>logical; TRUE: return logical vector; FALSE (default) index</p>
</td></tr>
</table>

<hr>
<h2 id='get_names_pfc'>Extract term names from the parsed formula content</h2><span id='topic+get_names_pfc'></span>

<h3>Description</h3>

<p>Extract term names from the parsed formula content
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_names_pfc(pfc)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_names_pfc_+3A_pfc">pfc</code></td>
<td>
<p>parsed formula content</p>
</td></tr>
</table>


<h3>Value</h3>

<p>vector of term names
</p>

<hr>
<h2 id='get_partial_effect'>Return partial effect of one smooth term</h2><span id='topic+get_partial_effect'></span>

<h3>Description</h3>

<p>Return partial effect of one smooth term
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_partial_effect(
  object,
  names = NULL,
  return_matrix = FALSE,
  which_param = 1,
  newdata = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_partial_effect_+3A_object">object</code></td>
<td>
<p>deepregression object</p>
</td></tr>
<tr><td><code id="get_partial_effect_+3A_names">names</code></td>
<td>
<p>string; for partial match with smooth term</p>
</td></tr>
<tr><td><code id="get_partial_effect_+3A_return_matrix">return_matrix</code></td>
<td>
<p>logical; whether to return the design matrix or</p>
</td></tr>
<tr><td><code id="get_partial_effect_+3A_which_param">which_param</code></td>
<td>
<p>integer; which distribution parameter
the partial effect (<code>FALSE</code>, default)</p>
</td></tr>
<tr><td><code id="get_partial_effect_+3A_newdata">newdata</code></td>
<td>
<p>data.frame; new data (optional)</p>
</td></tr>
<tr><td><code id="get_partial_effect_+3A_...">...</code></td>
<td>
<p>arguments passed to <code>get_weight_by_name</code></p>
</td></tr>
</table>

<hr>
<h2 id='get_processor_name'>Extract processor name from term</h2><span id='topic+get_processor_name'></span>

<h3>Description</h3>

<p>Extract processor name from term
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_processor_name(term)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_processor_name_+3A_term">term</code></td>
<td>
<p>term in formula</p>
</td></tr>
</table>


<h3>Value</h3>

<p>processor name as string
</p>

<hr>
<h2 id='get_special'>Extract terms defined by specials in formula</h2><span id='topic+get_special'></span>

<h3>Description</h3>

<p>Extract terms defined by specials in formula
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_special(term, specials, simplify = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_special_+3A_term">term</code></td>
<td>
<p>term in formula</p>
</td></tr>
<tr><td><code id="get_special_+3A_specials">specials</code></td>
<td>
<p>string(s); special name(s)</p>
</td></tr>
<tr><td><code id="get_special_+3A_simplify">simplify</code></td>
<td>
<p>logical; shortcut for returning only
the name of the special in term</p>
</td></tr>
</table>


<h3>Value</h3>

<p>specials in formula
</p>

<hr>
<h2 id='get_type_pfc'>Function to subset parsed formulas</h2><span id='topic+get_type_pfc'></span>

<h3>Description</h3>

<p>Function to subset parsed formulas
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_type_pfc(pfc, type = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_type_pfc_+3A_pfc">pfc</code></td>
<td>
<p>list of parsed formulas</p>
</td></tr>
<tr><td><code id="get_type_pfc_+3A_type">type</code></td>
<td>
<p>either NULL (all types of coefficients are returned),
&quot;linear&quot; for linear coefficients or &quot;smooth&quot; for coefficients of</p>
</td></tr>
</table>

<hr>
<h2 id='get_weight_by_name'>Function to retrieve the weights of a structured layer</h2><span id='topic+get_weight_by_name'></span>

<h3>Description</h3>

<p>Function to retrieve the weights of a structured layer
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_weight_by_name(mod, name, param_nr = 1, postfixes = "")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_weight_by_name_+3A_mod">mod</code></td>
<td>
<p>fitted deepregression object</p>
</td></tr>
<tr><td><code id="get_weight_by_name_+3A_name">name</code></td>
<td>
<p>name of partial effect</p>
</td></tr>
<tr><td><code id="get_weight_by_name_+3A_param_nr">param_nr</code></td>
<td>
<p>distribution parameter number</p>
</td></tr>
<tr><td><code id="get_weight_by_name_+3A_postfixes">postfixes</code></td>
<td>
<p>character (vector) appended to layer name</p>
</td></tr>
</table>


<h3>Value</h3>

<p>weight matrix
</p>

<hr>
<h2 id='get_weight_by_opname'>Function to return weight given model and name</h2><span id='topic+get_weight_by_opname'></span>

<h3>Description</h3>

<p>Function to return weight given model and name
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_weight_by_opname(mod, name, partial_match = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_weight_by_opname_+3A_mod">mod</code></td>
<td>
<p>deepregression model</p>
</td></tr>
<tr><td><code id="get_weight_by_opname_+3A_name">name</code></td>
<td>
<p>character</p>
</td></tr>
<tr><td><code id="get_weight_by_opname_+3A_partial_match">partial_match</code></td>
<td>
<p>logical; whether to also check for a partial match</p>
</td></tr>
</table>

<hr>
<h2 id='handle_gam_term'>Function to define smoothness and call mgcv's smooth constructor</h2><span id='topic+handle_gam_term'></span>

<h3>Description</h3>

<p>Function to define smoothness and call mgcv's smooth constructor
</p>


<h3>Usage</h3>

<pre><code class='language-R'>handle_gam_term(object, data, controls)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="handle_gam_term_+3A_object">object</code></td>
<td>
<p>character defining the model term</p>
</td></tr>
<tr><td><code id="handle_gam_term_+3A_data">data</code></td>
<td>
<p>data.frame or list</p>
</td></tr>
<tr><td><code id="handle_gam_term_+3A_controls">controls</code></td>
<td>
<p>controls for penalization</p>
</td></tr>
</table>


<h3>Value</h3>

<p>constructed smooth term
</p>

<hr>
<h2 id='keras_dr'>Compile a Deep Distributional Regression Model</h2><span id='topic+keras_dr'></span>

<h3>Description</h3>

<p>Compile a Deep Distributional Regression Model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>keras_dr(
  list_pred_param,
  weights = NULL,
  optimizer = tf$keras$optimizers$Adam(),
  model_fun = keras_model,
  monitor_metrics = list(),
  from_preds_to_output = from_preds_to_dist,
  loss = from_dist_to_loss(family = list(...)$family, weights = weights),
  additional_penalty = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="keras_dr_+3A_list_pred_param">list_pred_param</code></td>
<td>
<p>list of input-output(-lists) generated from
<code>subnetwork_init</code></p>
</td></tr>
<tr><td><code id="keras_dr_+3A_weights">weights</code></td>
<td>
<p>vector of positive values; optional (default = 1 for all observations)</p>
</td></tr>
<tr><td><code id="keras_dr_+3A_optimizer">optimizer</code></td>
<td>
<p>optimizer used. Per default Adam</p>
</td></tr>
<tr><td><code id="keras_dr_+3A_model_fun">model_fun</code></td>
<td>
<p>which function to use for model building (default <code>keras_model</code>)</p>
</td></tr>
<tr><td><code id="keras_dr_+3A_monitor_metrics">monitor_metrics</code></td>
<td>
<p>Further metrics to monitor</p>
</td></tr>
<tr><td><code id="keras_dr_+3A_from_preds_to_output">from_preds_to_output</code></td>
<td>
<p>function taking the list_pred_param outputs
and transforms it into a single network output</p>
</td></tr>
<tr><td><code id="keras_dr_+3A_loss">loss</code></td>
<td>
<p>the model's loss function; per default evaluated based on
the arguments <code>family</code> and <code>weights</code> using <code>from_dist_to_loss</code></p>
</td></tr>
<tr><td><code id="keras_dr_+3A_additional_penalty">additional_penalty</code></td>
<td>
<p>a penalty that is added to the negative log-likelihood;
must be a function of model$trainable_weights with suitable subsetting</p>
</td></tr>
<tr><td><code id="keras_dr_+3A_...">...</code></td>
<td>
<p>arguments passed to <code>from_preds_to_output</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list with input tensors and output tensors that can be passed
to, e.g., <code>keras_model</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(24)
n &lt;- 500
x &lt;- runif(n) %&gt;% as.matrix()
z &lt;- runif(n) %&gt;% as.matrix()

y &lt;- x - z
data &lt;- data.frame(x = x, z = z, y = y)

# change loss to mse and adapt
# \code{from_preds_to_output} to work
# only on the first output column
mod &lt;- deepregression(
 y = y,
 data = data,
 list_of_formulas = list(loc = ~ 1 + x + z, scale = ~ 1),
 list_of_deep_models = NULL,
 family = "normal",
 from_preds_to_output = function(x, ...) x[[1]],
 loss = "mse"
)


</code></pre>

<hr>
<h2 id='layer_add_identity'>Convenience layer function</h2><span id='topic+layer_add_identity'></span><span id='topic+layer_concatenate_identity'></span>

<h3>Description</h3>

<p>Convenience layer function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>layer_add_identity(inputs)

layer_concatenate_identity(inputs)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="layer_add_identity_+3A_inputs">inputs</code></td>
<td>
<p>list of tensors</p>
</td></tr>
</table>


<h3>Details</h3>

<p>convenience layers to work with list of inputs where <code>inputs</code>
can also have length one
</p>


<h3>Value</h3>

<p>tensor
</p>

<hr>
<h2 id='layer_generator'>Function that creates layer for each processor</h2><span id='topic+layer_generator'></span><span id='topic+int_processor'></span><span id='topic+lin_processor'></span><span id='topic+gam_processor'></span>

<h3>Description</h3>

<p>Function that creates layer for each processor
</p>


<h3>Usage</h3>

<pre><code class='language-R'>layer_generator(
  term,
  output_dim,
  param_nr,
  controls,
  layer_class = tf$keras$layers$Dense,
  without_layer = tf$identity,
  name = makelayername(term, param_nr),
  further_layer_args = NULL,
  layer_args_names = NULL,
  units = as.integer(output_dim),
  ...
)

int_processor(term, data, output_dim, param_nr, controls)

lin_processor(term, data, output_dim, param_nr, controls)

gam_processor(term, data, output_dim, param_nr, controls)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="layer_generator_+3A_term">term</code></td>
<td>
<p>character; term in the formula</p>
</td></tr>
<tr><td><code id="layer_generator_+3A_output_dim">output_dim</code></td>
<td>
<p>integer; number of units in the layer</p>
</td></tr>
<tr><td><code id="layer_generator_+3A_param_nr">param_nr</code></td>
<td>
<p>integer; identifier for models with more 
than one additive predictor</p>
</td></tr>
<tr><td><code id="layer_generator_+3A_controls">controls</code></td>
<td>
<p>list; control arguments which allow
to pass further information</p>
</td></tr>
<tr><td><code id="layer_generator_+3A_layer_class">layer_class</code></td>
<td>
<p>a tf or keras layer function</p>
</td></tr>
<tr><td><code id="layer_generator_+3A_without_layer">without_layer</code></td>
<td>
<p>function to be used as 
layer if <code>controls$with_layer</code> is FALSE</p>
</td></tr>
<tr><td><code id="layer_generator_+3A_name">name</code></td>
<td>
<p>character; name of layer. 
if NULL, <code>makelayername</code> will be used to create layer name</p>
</td></tr>
<tr><td><code id="layer_generator_+3A_further_layer_args">further_layer_args</code></td>
<td>
<p>named list; further arguments passed to
the layer</p>
</td></tr>
<tr><td><code id="layer_generator_+3A_layer_args_names">layer_args_names</code></td>
<td>
<p>character vector; if NULL, default
layer args will be used. Needs to be set for layers that do not
provide the arguments of a default Dense layer.</p>
</td></tr>
<tr><td><code id="layer_generator_+3A_units">units</code></td>
<td>
<p>integer; number of units for layer</p>
</td></tr>
<tr><td><code id="layer_generator_+3A_...">...</code></td>
<td>
<p>other keras layer parameters</p>
</td></tr>
<tr><td><code id="layer_generator_+3A_data">data</code></td>
<td>
<p>data frame; the data used in processors</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a basic processor list structure
</p>

<hr>
<h2 id='layer_sparse_conv_2d'>Sparse 2D Convolutional layer</h2><span id='topic+layer_sparse_conv_2d'></span>

<h3>Description</h3>

<p>Sparse 2D Convolutional layer
</p>


<h3>Usage</h3>

<pre><code class='language-R'>layer_sparse_conv_2d(filters, kernel_size, lam = NULL, depth = 2, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="layer_sparse_conv_2d_+3A_filters">filters</code></td>
<td>
<p>number of filters</p>
</td></tr>
<tr><td><code id="layer_sparse_conv_2d_+3A_kernel_size">kernel_size</code></td>
<td>
<p>size of convolutional filter</p>
</td></tr>
<tr><td><code id="layer_sparse_conv_2d_+3A_lam">lam</code></td>
<td>
<p>regularization strength</p>
</td></tr>
<tr><td><code id="layer_sparse_conv_2d_+3A_depth">depth</code></td>
<td>
<p>depth of weight factorization</p>
</td></tr>
<tr><td><code id="layer_sparse_conv_2d_+3A_...">...</code></td>
<td>
<p>arguments passed to TensorFlow layer</p>
</td></tr>
</table>


<h3>Value</h3>

<p>layer object
</p>

<hr>
<h2 id='layer_spline'>Function to define spline as TensorFlow layer</h2><span id='topic+layer_spline'></span>

<h3>Description</h3>

<p>Function to define spline as TensorFlow layer
</p>


<h3>Usage</h3>

<pre><code class='language-R'>layer_spline(
  units = 1L,
  P,
  name,
  trainable = TRUE,
  kernel_initializer = "glorot_uniform"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="layer_spline_+3A_units">units</code></td>
<td>
<p>integer; number of output units</p>
</td></tr>
<tr><td><code id="layer_spline_+3A_p">P</code></td>
<td>
<p>matrix; penalty matrix</p>
</td></tr>
<tr><td><code id="layer_spline_+3A_name">name</code></td>
<td>
<p>string; string defining the layer's name</p>
</td></tr>
<tr><td><code id="layer_spline_+3A_trainable">trainable</code></td>
<td>
<p>logical; whether layer is trainable</p>
</td></tr>
<tr><td><code id="layer_spline_+3A_kernel_initializer">kernel_initializer</code></td>
<td>
<p>initializer; for basis coefficients</p>
</td></tr>
</table>


<h3>Value</h3>

<p>TensorFlow layer
</p>

<hr>
<h2 id='log_score'>Function to return the log_score</h2><span id='topic+log_score'></span>

<h3>Description</h3>

<p>Function to return the log_score
</p>


<h3>Usage</h3>

<pre><code class='language-R'>log_score(
  x,
  data = NULL,
  this_y = NULL,
  ind_fun = function(x) tfd_independent(x),
  convert_fun = as.matrix,
  summary_fun = function(x) x
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="log_score_+3A_x">x</code></td>
<td>
<p>the fitted deepregression object</p>
</td></tr>
<tr><td><code id="log_score_+3A_data">data</code></td>
<td>
<p>an optional data set</p>
</td></tr>
<tr><td><code id="log_score_+3A_this_y">this_y</code></td>
<td>
<p>new y for optional data</p>
</td></tr>
<tr><td><code id="log_score_+3A_ind_fun">ind_fun</code></td>
<td>
<p>function indicating the dependency; per default (iid assumption)
<code>tfd_independent</code> is used.</p>
</td></tr>
<tr><td><code id="log_score_+3A_convert_fun">convert_fun</code></td>
<td>
<p>function that converts Tensor; per default <code>as.matrix</code></p>
</td></tr>
<tr><td><code id="log_score_+3A_summary_fun">summary_fun</code></td>
<td>
<p>function summarizing the output; per default the identity</p>
</td></tr>
</table>

<hr>
<h2 id='loop_through_pfc_and_call_trafo'>Function to loop through parsed formulas and apply data trafo</h2><span id='topic+loop_through_pfc_and_call_trafo'></span>

<h3>Description</h3>

<p>Function to loop through parsed formulas and apply data trafo
</p>


<h3>Usage</h3>

<pre><code class='language-R'>loop_through_pfc_and_call_trafo(pfc, newdata = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="loop_through_pfc_and_call_trafo_+3A_pfc">pfc</code></td>
<td>
<p>list of processor transformed formulas</p>
</td></tr>
<tr><td><code id="loop_through_pfc_and_call_trafo_+3A_newdata">newdata</code></td>
<td>
<p>list in the same format as the original data</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list of matrices or arrays
</p>

<hr>
<h2 id='make_folds'>Generate folds for CV out of one hot encoded matrix</h2><span id='topic+make_folds'></span>

<h3>Description</h3>

<p>Generate folds for CV out of one hot encoded matrix
</p>


<h3>Usage</h3>

<pre><code class='language-R'>make_folds(mat, val_train = 0, val_test = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="make_folds_+3A_mat">mat</code></td>
<td>
<p>matrix with columns corresponding to folds
and entries corresponding to a one hot encoding</p>
</td></tr>
<tr><td><code id="make_folds_+3A_val_train">val_train</code></td>
<td>
<p>the value corresponding to train, per default 0</p>
</td></tr>
<tr><td><code id="make_folds_+3A_val_test">val_test</code></td>
<td>
<p>the value corresponding to test, per default 1</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>val_train</code> and <code>val_test</code> can both be a set of value
</p>

<hr>
<h2 id='make_generator'>creates a generator for training</h2><span id='topic+make_generator'></span>

<h3>Description</h3>

<p>creates a generator for training
</p>


<h3>Usage</h3>

<pre><code class='language-R'>make_generator(
  input_x,
  input_y = NULL,
  batch_size,
  sizes,
  shuffle = TRUE,
  seed = 42L
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="make_generator_+3A_input_x">input_x</code></td>
<td>
<p>list of matrices</p>
</td></tr>
<tr><td><code id="make_generator_+3A_input_y">input_y</code></td>
<td>
<p>list of matrix</p>
</td></tr>
<tr><td><code id="make_generator_+3A_batch_size">batch_size</code></td>
<td>
<p>integer</p>
</td></tr>
<tr><td><code id="make_generator_+3A_sizes">sizes</code></td>
<td>
<p>sizes of the image including colour channel</p>
</td></tr>
<tr><td><code id="make_generator_+3A_shuffle">shuffle</code></td>
<td>
<p>logical for shuffling data</p>
</td></tr>
<tr><td><code id="make_generator_+3A_seed">seed</code></td>
<td>
<p>seed for shuffling in generators</p>
</td></tr>
</table>


<h3>Value</h3>

<p>generator for all x and y
</p>

<hr>
<h2 id='make_generator_from_matrix'>Make a DataGenerator from a data.frame or matrix</h2><span id='topic+make_generator_from_matrix'></span>

<h3>Description</h3>

<p>Creates a Python Class that internally iterates over the data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>make_generator_from_matrix(
  x,
  y = NULL,
  generator = image_data_generator(),
  batch_size = 32L,
  shuffle = TRUE,
  seed = 1L
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="make_generator_from_matrix_+3A_x">x</code></td>
<td>
<p>matrix;</p>
</td></tr>
<tr><td><code id="make_generator_from_matrix_+3A_y">y</code></td>
<td>
<p>vector;</p>
</td></tr>
<tr><td><code id="make_generator_from_matrix_+3A_generator">generator</code></td>
<td>
<p>generator as e.g. obtained from 'keras::image_data_generator'.
Used for consistent train-test splits.</p>
</td></tr>
<tr><td><code id="make_generator_from_matrix_+3A_batch_size">batch_size</code></td>
<td>
<p>integer</p>
</td></tr>
<tr><td><code id="make_generator_from_matrix_+3A_shuffle">shuffle</code></td>
<td>
<p>logical; Should data be shuffled?</p>
</td></tr>
<tr><td><code id="make_generator_from_matrix_+3A_seed">seed</code></td>
<td>
<p>integer; seed for shuffling data.</p>
</td></tr>
</table>

<hr>
<h2 id='make_tfd_dist'>Families for deepregression</h2><span id='topic+make_tfd_dist'></span>

<h3>Description</h3>

<p>Families for deepregression
</p>


<h3>Usage</h3>

<pre><code class='language-R'>make_tfd_dist(family, add_const = 1e-08, output_dim = 1L, trafo_list = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="make_tfd_dist_+3A_family">family</code></td>
<td>
<p>character vector</p>
</td></tr>
<tr><td><code id="make_tfd_dist_+3A_add_const">add_const</code></td>
<td>
<p>small positive constant to stabilize calculations</p>
</td></tr>
<tr><td><code id="make_tfd_dist_+3A_output_dim">output_dim</code></td>
<td>
<p>number of output dimensions of the response (larger 1 for
multivariate case)</p>
</td></tr>
<tr><td><code id="make_tfd_dist_+3A_trafo_list">trafo_list</code></td>
<td>
<p>list of transformations for each distribution parameter.
Per default the transformation listed in details is applied.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>To specify a custom distribution, define the a function as follows
<code>
function(x) do.call(your_tfd_dist, lapply(1:ncol(x)[[1]],
                                    function(i)
                                     your_trafo_list_on_inputs[[i]](
                                       x[,i,drop=FALSE])))
</code>
and pass it to <code>deepregression</code> via the <code>dist_fun</code> argument.
Currently the following distributions are supported
with parameters (and corresponding inverse link function in brackets):
</p>

<ul>
<li><p>&quot;normal&quot;: normal distribution with location (identity), scale (exp)
</p>
</li>
<li><p>&quot;bernoulli&quot;: bernoulli distribution with logits (identity)
</p>
</li>
<li><p>&quot;bernoulli_prob&quot;: bernoulli distribution with probabilities (sigmoid)
</p>
</li>
<li><p>&quot;beta&quot;: beta with concentration 1 = alpha (exp) and concentration
0 = beta (exp)
</p>
</li>
<li><p>&quot;betar&quot;: beta with mean (sigmoid) and scale (sigmoid)
</p>
</li>
<li><p>&quot;cauchy&quot;: location (identity), scale (exp)
</p>
</li>
<li><p>&quot;chi2&quot;: cauchy with df (exp)
</p>
</li>
<li><p>&quot;chi&quot;: cauchy with df (exp)
</p>
</li>
<li><p>&quot;exponential&quot;: exponential with lambda (exp)
</p>
</li>
<li><p>&quot;gamma&quot;: gamma with concentration (exp) and rate (exp)
</p>
</li>
<li><p>&quot;gammar&quot;: gamma with location (exp) and scale (exp), following
<code>gamlss.dist::GA</code>, which implies that the expectation is the location, 
and the variance of the distribution is the <code>location^2 scale^2</code>
</p>
</li>
<li><p>&quot;gumbel&quot;: gumbel with location (identity), scale (exp)
</p>
</li>
<li><p>&quot;half_cauchy&quot;: half cauchy with location (identity), scale (exp)
</p>
</li>
<li><p>&quot;half_normal&quot;: half normal with scale (exp)
</p>
</li>
<li><p>&quot;horseshoe&quot;: horseshoe with scale (exp)
</p>
</li>
<li><p>&quot;inverse_gamma&quot;: inverse gamma with concentation (exp) and rate (exp)
</p>
</li>
<li><p>&quot;inverse_gamma_ls&quot;: inverse gamma with location (exp) and variance (1/exp)
</p>
</li>
<li><p>&quot;inverse_gaussian&quot;: inverse Gaussian with location (exp) and concentation
(exp)
</p>
</li>
<li><p>&quot;laplace&quot;: Laplace with location (identity) and scale (exp)
</p>
</li>
<li><p>&quot;log_normal&quot;: Log-normal with location (identity) and scale (exp) of
underlying normal distribution
</p>
</li>
<li><p>&quot;logistic&quot;: logistic with location (identity) and scale (exp)
</p>
</li>
<li><p>&quot;negbinom&quot;: neg. binomial with count (exp) and prob (sigmoid)
</p>
</li>
<li><p>&quot;negbinom_ls&quot;: neg. binomail with mean (exp) and clutter factor (exp)
</p>
</li>
<li><p>&quot;pareto&quot;: Pareto with concentration (exp) and scale (1/exp) 
</p>
</li>
<li><p>&quot;pareto_ls&quot;: Pareto location scale version with mean (exp) 
and scale (exp), which corresponds to a Pareto distribution with parameters scale = mean
and concentration = 1/sigma, where sigma is the scale in the pareto_ls version
</p>
</li>
<li><p>&quot;poisson&quot;: poisson with rate (exp)
</p>
</li>
<li><p>&quot;poisson_lograte&quot;: poisson with lograte (identity))
</p>
</li>
<li><p>&quot;student_t&quot;: Student's t with df (exp)
</p>
</li>
<li><p>&quot;student_t_ls&quot;: Student's t with df (exp), location (identity) and
scale (exp)
</p>
</li>
<li><p>&quot;uniform&quot;: uniform with upper and lower (both identity)
</p>
</li>
<li><p>&quot;zinb&quot;: Zero-inflated negative binomial with mean (exp), 
variance (exp) and prob (sigmoid)
</p>
</li>
<li><p>&quot;zip&quot;:  Zero-inflated poisson distribution with mean (exp) and prob (sigmoid)
</p>
</li></ul>


<hr>
<h2 id='makeInputs'>Convenience layer function</h2><span id='topic+makeInputs'></span>

<h3>Description</h3>

<p>Convenience layer function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>makeInputs(pp, param_nr)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="makeInputs_+3A_pp">pp</code></td>
<td>
<p>processed predictors</p>
</td></tr>
<tr><td><code id="makeInputs_+3A_param_nr">param_nr</code></td>
<td>
<p>integer for the parameter</p>
</td></tr>
</table>


<h3>Value</h3>

<p>input tensors with appropriate names
</p>

<hr>
<h2 id='makelayername'>Function that takes term and create layer name</h2><span id='topic+makelayername'></span>

<h3>Description</h3>

<p>Function that takes term and create layer name
</p>


<h3>Usage</h3>

<pre><code class='language-R'>makelayername(term, param_nr, truncate = 60)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="makelayername_+3A_term">term</code></td>
<td>
<p>term in formula</p>
</td></tr>
<tr><td><code id="makelayername_+3A_param_nr">param_nr</code></td>
<td>
<p>integer; defining number of the distribution's parameter</p>
</td></tr>
<tr><td><code id="makelayername_+3A_truncate">truncate</code></td>
<td>
<p>integer; value from which on names are truncated</p>
</td></tr>
</table>


<h3>Value</h3>

<p>name (string) for layer
</p>

<hr>
<h2 id='multioptimizer'>Function to define an optimizer combining multiple optimizers</h2><span id='topic+multioptimizer'></span>

<h3>Description</h3>

<p>Function to define an optimizer combining multiple optimizers
</p>


<h3>Usage</h3>

<pre><code class='language-R'>multioptimizer(optimizers_and_layers)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="multioptimizer_+3A_optimizers_and_layers">optimizers_and_layers</code></td>
<td>
<p>a list if <code>tuple</code>s of optimizer
and respective layers</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an optimizer
</p>

<hr>
<h2 id='names_families'>Returns the parameter names for a given family</h2><span id='topic+names_families'></span>

<h3>Description</h3>

<p>Returns the parameter names for a given family
</p>


<h3>Usage</h3>

<pre><code class='language-R'>names_families(family)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="names_families_+3A_family">family</code></td>
<td>
<p>character specifying the family as defined by <code>deepregression</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>vector of parameter names
</p>

<hr>
<h2 id='orthog_control'>Options for orthogonalization</h2><span id='topic+orthog_control'></span>

<h3>Description</h3>

<p>Options for orthogonalization
</p>


<h3>Usage</h3>

<pre><code class='language-R'>orthog_control(
  split_fun = split_model,
  orthog_type = c("tf", "manual"),
  orthogonalize = options()$orthogonalize,
  identify_intercept = options()$identify_intercept,
  deep_top = NULL,
  orthog_fun = NULL,
  deactivate_oz_at_test = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="orthog_control_+3A_split_fun">split_fun</code></td>
<td>
<p>a function separating the deep neural network in two parts
so that the orthogonalization can be applied to the first part before
applying the second network part; per default, the function <code>split_model</code> is
used which assumes a dense layer as penultimate layer and separates the network
into a first part without this last layer and a second part only consisting of a
single dense layer that is fed into the output layer</p>
</td></tr>
<tr><td><code id="orthog_control_+3A_orthog_type">orthog_type</code></td>
<td>
<p>one of two options; If <code>"manual"</code>, 
the QR decomposition is calculated before model fitting, 
otherwise (<code>"tf"</code>) a QR is calculated in each batch iteration via TF.
The first only works well for larger batch sizes or ideally batch_size == NROW(y).</p>
</td></tr>
<tr><td><code id="orthog_control_+3A_orthogonalize">orthogonalize</code></td>
<td>
<p>logical; if set to <code>TRUE</code>, automatic orthogonalization is activated</p>
</td></tr>
<tr><td><code id="orthog_control_+3A_identify_intercept">identify_intercept</code></td>
<td>
<p>whether to orthogonalize the deep network w.r.t. the intercept
to make the intercept identifiable</p>
</td></tr>
<tr><td><code id="orthog_control_+3A_deep_top">deep_top</code></td>
<td>
<p>function; optional function to put on top of the deep network instead
of splitting the function using <code>split_fun</code></p>
</td></tr>
<tr><td><code id="orthog_control_+3A_orthog_fun">orthog_fun</code></td>
<td>
<p>function; for custom orthogonaliuation. if NULL, <code>orthog_type</code> 
is used to define the function that computes the orthogonalization</p>
</td></tr>
<tr><td><code id="orthog_control_+3A_deactivate_oz_at_test">deactivate_oz_at_test</code></td>
<td>
<p>logical; whether to deactive the orthogonalization cell
at test time when using <code>orthog_tf</code> for <code>orthog_fun</code> (the default).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a list with options
</p>

<hr>
<h2 id='orthog_P'>Function to compute adjusted penalty when orthogonalizing</h2><span id='topic+orthog_P'></span>

<h3>Description</h3>

<p>Function to compute adjusted penalty when orthogonalizing
</p>


<h3>Usage</h3>

<pre><code class='language-R'>orthog_P(P, Z)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="orthog_P_+3A_p">P</code></td>
<td>
<p>matrix; original penalty matrix</p>
</td></tr>
<tr><td><code id="orthog_P_+3A_z">Z</code></td>
<td>
<p>matrix; constraint matrix</p>
</td></tr>
</table>


<h3>Value</h3>

<p>adjusted penalty matrix
</p>

<hr>
<h2 id='orthog_post_fitting'>Orthogonalize a Semi-Structured Model Post-hoc</h2><span id='topic+orthog_post_fitting'></span>

<h3>Description</h3>

<p>Orthogonalize a Semi-Structured Model Post-hoc
</p>


<h3>Usage</h3>

<pre><code class='language-R'>orthog_post_fitting(mod, name_penult, param_nr = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="orthog_post_fitting_+3A_mod">mod</code></td>
<td>
<p>deepregression model</p>
</td></tr>
<tr><td><code id="orthog_post_fitting_+3A_name_penult">name_penult</code></td>
<td>
<p>character name of the penultimate layer 
of the deep part part</p>
</td></tr>
<tr><td><code id="orthog_post_fitting_+3A_param_nr">param_nr</code></td>
<td>
<p>integer; number of the parameter to be returned</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>deepregression</code> object with weights frozen and
deep part specified by <code>name_penult</code> orthogonalized
</p>

<hr>
<h2 id='orthog_structured_smooths_Z'>Orthogonalize structured term by another matrix</h2><span id='topic+orthog_structured_smooths_Z'></span>

<h3>Description</h3>

<p>Orthogonalize structured term by another matrix
</p>


<h3>Usage</h3>

<pre><code class='language-R'>orthog_structured_smooths_Z(S, L)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="orthog_structured_smooths_Z_+3A_s">S</code></td>
<td>
<p>matrix; matrix to orthogonalize</p>
</td></tr>
<tr><td><code id="orthog_structured_smooths_Z_+3A_l">L</code></td>
<td>
<p>matrix; matrix which defines the projection
and its orthogonal complement, in which <code>S</code> is projected</p>
</td></tr>
</table>


<h3>Value</h3>

<p>constraint matrix
</p>

<hr>
<h2 id='penalty_control'>Options for penalty setup in the pre-processing</h2><span id='topic+penalty_control'></span>

<h3>Description</h3>

<p>Options for penalty setup in the pre-processing
</p>


<h3>Usage</h3>

<pre><code class='language-R'>penalty_control(
  defaultSmoothing = NULL,
  df = 10,
  null_space_penalty = FALSE,
  absorb_cons = FALSE,
  anisotropic = TRUE,
  zero_constraint_for_smooths = TRUE,
  no_linear_trend_for_smooths = FALSE,
  hat1 = FALSE,
  sp_scale = function(x) ifelse(is.list(x) | is.data.frame(x), 1/NROW(x[[1]]),
    1/NROW(x))
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="penalty_control_+3A_defaultsmoothing">defaultSmoothing</code></td>
<td>
<p>function applied to all s-terms, per default (NULL)
the minimum df of all possible terms is used. Must be a function the smooth term
from mgcv's smoothCon and an argument <code>df</code>.</p>
</td></tr>
<tr><td><code id="penalty_control_+3A_df">df</code></td>
<td>
<p>degrees of freedom for all non-linear structural terms (default = 7);
either one common value or a list of the same length as number of parameters;
if different df values need to be assigned to different smooth terms,
use df as an argument for <code>s()</code>, <code>te()</code> or <code>ti()</code></p>
</td></tr>
<tr><td><code id="penalty_control_+3A_null_space_penalty">null_space_penalty</code></td>
<td>
<p>logical value;
if TRUE, the null space will also be penalized for smooth effects.
Per default, this is equal to the value give in <code>variational</code>.</p>
</td></tr>
<tr><td><code id="penalty_control_+3A_absorb_cons">absorb_cons</code></td>
<td>
<p>logical; adds identifiability constraint to the basis.
See <code>?mgcv::smoothCon</code> for more details.</p>
</td></tr>
<tr><td><code id="penalty_control_+3A_anisotropic">anisotropic</code></td>
<td>
<p>whether or not use anisotropic smoothing (default is TRUE)</p>
</td></tr>
<tr><td><code id="penalty_control_+3A_zero_constraint_for_smooths">zero_constraint_for_smooths</code></td>
<td>
<p>logical; the same as absorb_cons,
but done explicitly. If true a constraint is put on each smooth to have zero mean. Can
be a vector of <code>length(list_of_formulas)</code> for each distribution parameter.</p>
</td></tr>
<tr><td><code id="penalty_control_+3A_no_linear_trend_for_smooths">no_linear_trend_for_smooths</code></td>
<td>
<p>logical; see <code>zero_constraint_for_smooths</code>, but
this removes the linear trend from splines</p>
</td></tr>
<tr><td><code id="penalty_control_+3A_hat1">hat1</code></td>
<td>
<p>logical; if TRUE, the smoothing parameter is defined by the trace of the hat
matrix sum(diag(H)), else sum(diag(2*H-HH))</p>
</td></tr>
<tr><td><code id="penalty_control_+3A_sp_scale">sp_scale</code></td>
<td>
<p>function of response; for scaling the penalty (1/n per default)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a list with options
</p>

<hr>
<h2 id='plot_cv'>Plot CV results from deepregression</h2><span id='topic+plot_cv'></span>

<h3>Description</h3>

<p>Plot CV results from deepregression
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_cv(x, what = c("loss", "weight"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_cv_+3A_x">x</code></td>
<td>
<p><code>drCV</code> object returned by <code>cv.deepregression</code></p>
</td></tr>
<tr><td><code id="plot_cv_+3A_what">what</code></td>
<td>
<p>character indicating what to plot (currently supported 'loss'
or 'weights')</p>
</td></tr>
<tr><td><code id="plot_cv_+3A_...">...</code></td>
<td>
<p>further arguments passed to <code>matplot</code></p>
</td></tr>
</table>

<hr>
<h2 id='plot.deepregression'>Generic functions for deepregression models</h2><span id='topic+plot.deepregression'></span><span id='topic+predict.deepregression'></span><span id='topic+fitted.deepregression'></span><span id='topic+fit.deepregression'></span><span id='topic+coef.deepregression'></span><span id='topic+print.deepregression'></span><span id='topic+cv.deepregression'></span><span id='topic+mean.deepregression'></span><span id='topic+stddev.deepregression'></span><span id='topic+quant.deepregression'></span>

<h3>Description</h3>

<p>Generic functions for deepregression models
</p>
<p>Predict based on a deepregression object
</p>
<p>Function to extract fitted distribution
</p>
<p>Fit a deepregression model (pendant to fit for keras)
</p>
<p>Extract layer weights / coefficients from model
</p>
<p>Print function for deepregression model
</p>
<p>Cross-validation for deepgression objects
</p>
<p>mean of model fit
</p>
<p>Standard deviation of fit distribution
</p>
<p>Calculate the distribution quantiles
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'deepregression'
plot(
  x,
  which = NULL,
  which_param = 1,
  only_data = FALSE,
  grid_length = 40,
  main_multiple = NULL,
  type = "b",
  get_weight_fun = get_weight_by_name,
  ...
)

## S3 method for class 'deepregression'
predict(
  object,
  newdata = NULL,
  batch_size = NULL,
  apply_fun = tfd_mean,
  convert_fun = as.matrix,
  ...
)

## S3 method for class 'deepregression'
fitted(object, apply_fun = tfd_mean, ...)

## S3 method for class 'deepregression'
fit(
  object,
  batch_size = 32,
  epochs = 10,
  early_stopping = FALSE,
  early_stopping_metric = "val_loss",
  verbose = TRUE,
  view_metrics = FALSE,
  patience = 20,
  save_weights = FALSE,
  validation_data = NULL,
  validation_split = ifelse(is.null(validation_data), 0.1, 0),
  callbacks = list(),
  convertfun = function(x) tf$constant(x, dtype = "float32"),
  ...
)

## S3 method for class 'deepregression'
coef(object, which_param = 1, type = NULL, ...)

## S3 method for class 'deepregression'
print(x, ...)

## S3 method for class 'deepregression'
cv(
  x,
  verbose = FALSE,
  patience = 20,
  plot = TRUE,
  print_folds = TRUE,
  cv_folds = 5,
  stop_if_nan = TRUE,
  mylapply = lapply,
  save_weights = FALSE,
  callbacks = list(),
  save_fun = NULL,
  ...
)

## S3 method for class 'deepregression'
mean(x, data = NULL, ...)

## S3 method for class 'deepregression'
stddev(x, data = NULL, ...)

## S3 method for class 'deepregression'
quant(x, data = NULL, probs, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.deepregression_+3A_x">x</code></td>
<td>
<p>a deepregression object</p>
</td></tr>
<tr><td><code id="plot.deepregression_+3A_which">which</code></td>
<td>
<p>character vector or number(s) identifying the effect to plot; 
default plots all effects</p>
</td></tr>
<tr><td><code id="plot.deepregression_+3A_which_param">which_param</code></td>
<td>
<p>integer, indicating for which distribution parameter
coefficients should be returned (default is first parameter)</p>
</td></tr>
<tr><td><code id="plot.deepregression_+3A_only_data">only_data</code></td>
<td>
<p>logical, if TRUE, only the data for plotting is returned</p>
</td></tr>
<tr><td><code id="plot.deepregression_+3A_grid_length">grid_length</code></td>
<td>
<p>the length of an equidistant grid at which a two-dimensional function
is evaluated for plotting.</p>
</td></tr>
<tr><td><code id="plot.deepregression_+3A_main_multiple">main_multiple</code></td>
<td>
<p>vector of strings; plot main titles if multiple plots are selected</p>
</td></tr>
<tr><td><code id="plot.deepregression_+3A_type">type</code></td>
<td>
<p>either NULL (all types of coefficients are returned),
&quot;linear&quot; for linear coefficients or &quot;smooth&quot; for coefficients of 
smooth terms</p>
</td></tr>
<tr><td><code id="plot.deepregression_+3A_get_weight_fun">get_weight_fun</code></td>
<td>
<p>function to extract weight from model given <code>x</code>,
a <code>name</code> and <code>param_nr</code></p>
</td></tr>
<tr><td><code id="plot.deepregression_+3A_...">...</code></td>
<td>
<p>arguments passed to the <code>predict</code> function</p>
</td></tr>
<tr><td><code id="plot.deepregression_+3A_object">object</code></td>
<td>
<p>a deepregression model</p>
</td></tr>
<tr><td><code id="plot.deepregression_+3A_newdata">newdata</code></td>
<td>
<p>optional new data, either data.frame or list</p>
</td></tr>
<tr><td><code id="plot.deepregression_+3A_batch_size">batch_size</code></td>
<td>
<p>integer, the batch size used for mini-batch training</p>
</td></tr>
<tr><td><code id="plot.deepregression_+3A_apply_fun">apply_fun</code></td>
<td>
<p>function applied to fitted distribution,
per default <code>tfd_mean</code></p>
</td></tr>
<tr><td><code id="plot.deepregression_+3A_convert_fun">convert_fun</code></td>
<td>
<p>how should the resulting tensor be converted,
per default <code>as.matrix</code></p>
</td></tr>
<tr><td><code id="plot.deepregression_+3A_epochs">epochs</code></td>
<td>
<p>integer, the number of epochs to fit the model</p>
</td></tr>
<tr><td><code id="plot.deepregression_+3A_early_stopping">early_stopping</code></td>
<td>
<p>logical, whether early stopping should be user.</p>
</td></tr>
<tr><td><code id="plot.deepregression_+3A_early_stopping_metric">early_stopping_metric</code></td>
<td>
<p>character, based on which metric should
early stopping be trigged (default: &quot;val_loss&quot;)</p>
</td></tr>
<tr><td><code id="plot.deepregression_+3A_verbose">verbose</code></td>
<td>
<p>whether to print training in each fold</p>
</td></tr>
<tr><td><code id="plot.deepregression_+3A_view_metrics">view_metrics</code></td>
<td>
<p>logical, whether to trigger the Viewer in RStudio / Browser.</p>
</td></tr>
<tr><td><code id="plot.deepregression_+3A_patience">patience</code></td>
<td>
<p>number of patience for early stopping</p>
</td></tr>
<tr><td><code id="plot.deepregression_+3A_save_weights">save_weights</code></td>
<td>
<p>logical, whether to save weights in each epoch.</p>
</td></tr>
<tr><td><code id="plot.deepregression_+3A_validation_data">validation_data</code></td>
<td>
<p>optional specified validation data</p>
</td></tr>
<tr><td><code id="plot.deepregression_+3A_validation_split">validation_split</code></td>
<td>
<p>float in [0,1] defining the amount of data used for validation</p>
</td></tr>
<tr><td><code id="plot.deepregression_+3A_callbacks">callbacks</code></td>
<td>
<p>a list of callbacks used for fitting</p>
</td></tr>
<tr><td><code id="plot.deepregression_+3A_convertfun">convertfun</code></td>
<td>
<p>function to convert R into Tensor object</p>
</td></tr>
<tr><td><code id="plot.deepregression_+3A_plot">plot</code></td>
<td>
<p>whether to plot the resulting losses in each fold</p>
</td></tr>
<tr><td><code id="plot.deepregression_+3A_print_folds">print_folds</code></td>
<td>
<p>whether to print the current fold</p>
</td></tr>
<tr><td><code id="plot.deepregression_+3A_cv_folds">cv_folds</code></td>
<td>
<p>an integer; can also be a list of lists 
with train and test data sets per fold</p>
</td></tr>
<tr><td><code id="plot.deepregression_+3A_stop_if_nan">stop_if_nan</code></td>
<td>
<p>logical; whether to stop CV if NaN values occur</p>
</td></tr>
<tr><td><code id="plot.deepregression_+3A_mylapply">mylapply</code></td>
<td>
<p>lapply function to be used; defaults to <code>lapply</code></p>
</td></tr>
<tr><td><code id="plot.deepregression_+3A_save_fun">save_fun</code></td>
<td>
<p>function applied to the model in each fold to be stored in
the final result</p>
</td></tr>
<tr><td><code id="plot.deepregression_+3A_data">data</code></td>
<td>
<p>either <code>NULL</code> or a new data set</p>
</td></tr>
<tr><td><code id="plot.deepregression_+3A_probs">probs</code></td>
<td>
<p>the quantile value(s)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns an object <code>drCV</code>, a list, one list element for each fold
containing the model fit and the <code>weighthistory</code>.
</p>

<hr>
<h2 id='precalc_gam'>Pre-calculate all gam parts from the list of formulas</h2><span id='topic+precalc_gam'></span>

<h3>Description</h3>

<p>Pre-calculate all gam parts from the list of formulas
</p>


<h3>Usage</h3>

<pre><code class='language-R'>precalc_gam(lof, data, controls)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="precalc_gam_+3A_lof">lof</code></td>
<td>
<p>list of formulas</p>
</td></tr>
<tr><td><code id="precalc_gam_+3A_data">data</code></td>
<td>
<p>the data list</p>
</td></tr>
<tr><td><code id="precalc_gam_+3A_controls">controls</code></td>
<td>
<p>controls from deepregression</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list of length 2 with a matching table to
link every unique gam term to formula entries and the respective
data transformation functions
</p>

<hr>
<h2 id='predict_gen'>Generator function for deepregression objects</h2><span id='topic+predict_gen'></span>

<h3>Description</h3>

<p>Generator function for deepregression objects
</p>


<h3>Usage</h3>

<pre><code class='language-R'>predict_gen(
  object,
  newdata = NULL,
  batch_size = NULL,
  apply_fun = tfd_mean,
  convert_fun = as.matrix,
  ret_dist = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict_gen_+3A_object">object</code></td>
<td>
<p>deepregression model;</p>
</td></tr>
<tr><td><code id="predict_gen_+3A_newdata">newdata</code></td>
<td>
<p>data.frame or list; for (optional) new data</p>
</td></tr>
<tr><td><code id="predict_gen_+3A_batch_size">batch_size</code></td>
<td>
<p>integer; <code>NULL</code> will use the default (20)</p>
</td></tr>
<tr><td><code id="predict_gen_+3A_apply_fun">apply_fun</code></td>
<td>
<p>see <code>?predict.deepregression</code></p>
</td></tr>
<tr><td><code id="predict_gen_+3A_convert_fun">convert_fun</code></td>
<td>
<p>see <code>?predict.deepregression</code></p>
</td></tr>
<tr><td><code id="predict_gen_+3A_ret_dist">ret_dist</code></td>
<td>
<p>logical; whether to return the whole distribution or
only the (mean) prediction</p>
</td></tr>
</table>


<h3>Value</h3>

<p>matrix or list of distributions
</p>

<hr>
<h2 id='prepare_data'>Function to prepare data based on parsed formulas</h2><span id='topic+prepare_data'></span>

<h3>Description</h3>

<p>Function to prepare data based on parsed formulas
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prepare_data(pfc, gamdata = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prepare_data_+3A_pfc">pfc</code></td>
<td>
<p>list of processor transformed formulas</p>
</td></tr>
<tr><td><code id="prepare_data_+3A_gamdata">gamdata</code></td>
<td>
<p>processor for gam part</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list of matrices or arrays
</p>

<hr>
<h2 id='prepare_newdata'>Function to prepare new data based on parsed formulas</h2><span id='topic+prepare_newdata'></span>

<h3>Description</h3>

<p>Function to prepare new data based on parsed formulas
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prepare_newdata(pfc, newdata, gamdata = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prepare_newdata_+3A_pfc">pfc</code></td>
<td>
<p>list of processor transformed formulas</p>
</td></tr>
<tr><td><code id="prepare_newdata_+3A_newdata">newdata</code></td>
<td>
<p>list in the same format as the original data</p>
</td></tr>
<tr><td><code id="prepare_newdata_+3A_gamdata">gamdata</code></td>
<td>
<p>processor for gam part</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list of matrices or arrays
</p>

<hr>
<h2 id='process_terms'>Control function to define the processor for terms in the formula</h2><span id='topic+process_terms'></span>

<h3>Description</h3>

<p>Control function to define the processor for terms in the formula
</p>


<h3>Usage</h3>

<pre><code class='language-R'>process_terms(
  form,
  data,
  controls,
  output_dim,
  param_nr,
  parsing_options,
  specials_to_oz = c(),
  automatic_oz_check = TRUE,
  identify_intercept = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="process_terms_+3A_form">form</code></td>
<td>
<p>the formula to be processed</p>
</td></tr>
<tr><td><code id="process_terms_+3A_data">data</code></td>
<td>
<p>the data for the terms in the formula</p>
</td></tr>
<tr><td><code id="process_terms_+3A_controls">controls</code></td>
<td>
<p>controls for gam terms</p>
</td></tr>
<tr><td><code id="process_terms_+3A_output_dim">output_dim</code></td>
<td>
<p>the output dimension of the response</p>
</td></tr>
<tr><td><code id="process_terms_+3A_param_nr">param_nr</code></td>
<td>
<p>integer; identifier for the distribution parameter</p>
</td></tr>
<tr><td><code id="process_terms_+3A_parsing_options">parsing_options</code></td>
<td>
<p>options</p>
</td></tr>
<tr><td><code id="process_terms_+3A_specials_to_oz">specials_to_oz</code></td>
<td>
<p>specials that should be automatically checked for</p>
</td></tr>
<tr><td><code id="process_terms_+3A_automatic_oz_check">automatic_oz_check</code></td>
<td>
<p>logical; whether to automatically check for DNNs to be orthogonalized</p>
</td></tr>
<tr><td><code id="process_terms_+3A_identify_intercept">identify_intercept</code></td>
<td>
<p>logical; whether to make the intercept automatically identifiable</p>
</td></tr>
<tr><td><code id="process_terms_+3A_...">...</code></td>
<td>
<p>further processors</p>
</td></tr>
</table>


<h3>Value</h3>

<p>returns a processor function
</p>

<hr>
<h2 id='quant'>Generic quantile function</h2><span id='topic+quant'></span>

<h3>Description</h3>

<p>Generic quantile function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>quant(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="quant_+3A_x">x</code></td>
<td>
<p>object</p>
</td></tr>
<tr><td><code id="quant_+3A_...">...</code></td>
<td>
<p>further arguments passed to the class-specific function</p>
</td></tr>
</table>

<hr>
<h2 id='reinit_weights'>Genereic function to re-intialize model weights</h2><span id='topic+reinit_weights'></span>

<h3>Description</h3>

<p>Genereic function to re-intialize model weights
</p>


<h3>Usage</h3>

<pre><code class='language-R'>reinit_weights(object, seed)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="reinit_weights_+3A_object">object</code></td>
<td>
<p>model to re-initialize</p>
</td></tr>
<tr><td><code id="reinit_weights_+3A_seed">seed</code></td>
<td>
<p>seed for reproducibility</p>
</td></tr>
</table>

<hr>
<h2 id='reinit_weights.deepregression'>Method to re-initialize weights of a <code>"deepregression"</code> model</h2><span id='topic+reinit_weights.deepregression'></span>

<h3>Description</h3>

<p>Method to re-initialize weights of a <code>"deepregression"</code> model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'deepregression'
reinit_weights(object, seed)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="reinit_weights.deepregression_+3A_object">object</code></td>
<td>
<p>object of class <code>"deepregression"</code></p>
</td></tr>
<tr><td><code id="reinit_weights.deepregression_+3A_seed">seed</code></td>
<td>
<p>seed for reproducibility</p>
</td></tr>
</table>


<h3>Value</h3>

<p>invisible <code>NULL</code>
</p>

<hr>
<h2 id='separate_define_relation'>Function to define orthogonalization connections in the formula</h2><span id='topic+separate_define_relation'></span>

<h3>Description</h3>

<p>Function to define orthogonalization connections in the formula
</p>


<h3>Usage</h3>

<pre><code class='language-R'>separate_define_relation(
  form,
  specials,
  specials_to_oz,
  automatic_oz_check = TRUE,
  identify_intercept = FALSE,
  simplify = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="separate_define_relation_+3A_form">form</code></td>
<td>
<p>a formula for one distribution parameter</p>
</td></tr>
<tr><td><code id="separate_define_relation_+3A_specials">specials</code></td>
<td>
<p>specials in formula to handle separately</p>
</td></tr>
<tr><td><code id="separate_define_relation_+3A_specials_to_oz">specials_to_oz</code></td>
<td>
<p>parts of the formula to orthogonalize</p>
</td></tr>
<tr><td><code id="separate_define_relation_+3A_automatic_oz_check">automatic_oz_check</code></td>
<td>
<p>logical; automatically check if terms must be orthogonalized</p>
</td></tr>
<tr><td><code id="separate_define_relation_+3A_identify_intercept">identify_intercept</code></td>
<td>
<p>logical; whether to make the intercept identifiable</p>
</td></tr>
<tr><td><code id="separate_define_relation_+3A_simplify">simplify</code></td>
<td>
<p>logical; if FALSE, formulas are parsed more carefully.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a list of formula components with ids and 
assignments for orthogonalization
</p>

<hr>
<h2 id='stddev'>Generic sd function</h2><span id='topic+stddev'></span>

<h3>Description</h3>

<p>Generic sd function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stddev(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stddev_+3A_x">x</code></td>
<td>
<p>object</p>
</td></tr>
<tr><td><code id="stddev_+3A_...">...</code></td>
<td>
<p>further arguments passed to the class-specific function</p>
</td></tr>
</table>

<hr>
<h2 id='stop_iter_cv_result'>Function to get the stoppting iteration from CV</h2><span id='topic+stop_iter_cv_result'></span>

<h3>Description</h3>

<p>Function to get the stoppting iteration from CV
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stop_iter_cv_result(
  res,
  thisFUN = mean,
  loss = "validloss",
  whichFUN = which.min
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stop_iter_cv_result_+3A_res">res</code></td>
<td>
<p>result of cv call</p>
</td></tr>
<tr><td><code id="stop_iter_cv_result_+3A_thisfun">thisFUN</code></td>
<td>
<p>aggregating function applied over folds</p>
</td></tr>
<tr><td><code id="stop_iter_cv_result_+3A_loss">loss</code></td>
<td>
<p>which loss to use for decision</p>
</td></tr>
<tr><td><code id="stop_iter_cv_result_+3A_whichfun">whichFUN</code></td>
<td>
<p>which function to use for decision</p>
</td></tr>
</table>

<hr>
<h2 id='subnetwork_init'>Initializes a Subnetwork based on the Processed Additive Predictor</h2><span id='topic+subnetwork_init'></span>

<h3>Description</h3>

<p>Initializes a Subnetwork based on the Processed Additive Predictor
</p>


<h3>Usage</h3>

<pre><code class='language-R'>subnetwork_init(
  pp,
  deep_top = NULL,
  orthog_fun = orthog_tf,
  split_fun = split_model,
  shared_layers = NULL,
  param_nr = 1,
  selectfun_in = function(pp) pp[[param_nr]],
  selectfun_lay = function(pp) pp[[param_nr]],
  gaminputs,
  summary_layer = layer_add_identity
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="subnetwork_init_+3A_pp">pp</code></td>
<td>
<p>list of processed predictor lists from <code>processor</code></p>
</td></tr>
<tr><td><code id="subnetwork_init_+3A_deep_top">deep_top</code></td>
<td>
<p>keras layer if the top part of the deep network after orthogonalization
is different to the one extracted from the provided network</p>
</td></tr>
<tr><td><code id="subnetwork_init_+3A_orthog_fun">orthog_fun</code></td>
<td>
<p>function used for orthogonalization</p>
</td></tr>
<tr><td><code id="subnetwork_init_+3A_split_fun">split_fun</code></td>
<td>
<p>function to split the network to extract head</p>
</td></tr>
<tr><td><code id="subnetwork_init_+3A_shared_layers">shared_layers</code></td>
<td>
<p>list defining shared weights within one predictor;
each list item is a vector of characters of terms as given in the parameter formula</p>
</td></tr>
<tr><td><code id="subnetwork_init_+3A_param_nr">param_nr</code></td>
<td>
<p>integer number for the distribution parameter</p>
</td></tr>
<tr><td><code id="subnetwork_init_+3A_selectfun_in">selectfun_in</code>, <code id="subnetwork_init_+3A_selectfun_lay">selectfun_lay</code></td>
<td>
<p>functions defining which subset of pp to
take as inputs and layers for this subnetwork; per default the <code>param_nr</code>'s entry</p>
</td></tr>
<tr><td><code id="subnetwork_init_+3A_gaminputs">gaminputs</code></td>
<td>
<p>input tensors for gam terms</p>
</td></tr>
<tr><td><code id="subnetwork_init_+3A_summary_layer">summary_layer</code></td>
<td>
<p>keras layer that combines inputs (typically adding or concatenating)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>returns a list of input and output for this additive predictor
</p>

<hr>
<h2 id='tf_repeat'>TensorFlow repeat function which is not available for TF 2.0</h2><span id='topic+tf_repeat'></span>

<h3>Description</h3>

<p>TensorFlow repeat function which is not available for TF 2.0
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tf_repeat(a, dim)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tf_repeat_+3A_a">a</code></td>
<td>
<p>tensor</p>
</td></tr>
<tr><td><code id="tf_repeat_+3A_dim">dim</code></td>
<td>
<p>dimension for repeating</p>
</td></tr>
</table>

<hr>
<h2 id='tf_row_tensor'>Row-wise tensor product using TensorFlow</h2><span id='topic+tf_row_tensor'></span>

<h3>Description</h3>

<p>Row-wise tensor product using TensorFlow
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tf_row_tensor(a, b, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tf_row_tensor_+3A_a">a</code>, <code id="tf_row_tensor_+3A_b">b</code></td>
<td>
<p>tensor</p>
</td></tr>
<tr><td><code id="tf_row_tensor_+3A_...">...</code></td>
<td>
<p>arguments passed to TensorFlow layer</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a TensorFlow layer
</p>

<hr>
<h2 id='tf_split_multiple'>Split tensor in multiple parts</h2><span id='topic+tf_split_multiple'></span>

<h3>Description</h3>

<p>Split tensor in multiple parts
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tf_split_multiple(A, len)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tf_split_multiple_+3A_a">A</code></td>
<td>
<p>tensor</p>
</td></tr>
<tr><td><code id="tf_split_multiple_+3A_len">len</code></td>
<td>
<p>integer; defines the split lengths</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list of tensors
</p>

<hr>
<h2 id='tf_stride_cols'>Function to index tensors columns</h2><span id='topic+tf_stride_cols'></span>

<h3>Description</h3>

<p>Function to index tensors columns
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tf_stride_cols(A, start, end = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tf_stride_cols_+3A_a">A</code></td>
<td>
<p>tensor</p>
</td></tr>
<tr><td><code id="tf_stride_cols_+3A_start">start</code></td>
<td>
<p>first index</p>
</td></tr>
<tr><td><code id="tf_stride_cols_+3A_end">end</code></td>
<td>
<p>last index (equals start index if NULL)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>sliced tensor
</p>

<hr>
<h2 id='tf_stride_last_dim_tensor'>Function to index tensors last dimension</h2><span id='topic+tf_stride_last_dim_tensor'></span>

<h3>Description</h3>

<p>Function to index tensors last dimension
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tf_stride_last_dim_tensor(A, start, end = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tf_stride_last_dim_tensor_+3A_a">A</code></td>
<td>
<p>tensor</p>
</td></tr>
<tr><td><code id="tf_stride_last_dim_tensor_+3A_start">start</code></td>
<td>
<p>first index</p>
</td></tr>
<tr><td><code id="tf_stride_last_dim_tensor_+3A_end">end</code></td>
<td>
<p>last index (equals start index if NULL)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>sliced tensor
</p>

<hr>
<h2 id='tfd_mse'>For using mean squared error via TFP</h2><span id='topic+tfd_mse'></span>

<h3>Description</h3>

<p>For using mean squared error via TFP
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_mse(mean)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_mse_+3A_mean">mean</code></td>
<td>
<p>parameter for the mean</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>deepregression</code> allows to train based on the
MSE by using <code>loss = "mse"</code> as argument to <code>deepregression</code>.
This tfd function just provides a dummy <code>family</code>
</p>


<h3>Value</h3>

<p>a TFP distribution
</p>

<hr>
<h2 id='tfd_zinb'>Implementation of a zero-inflated negbinom distribution for TFP</h2><span id='topic+tfd_zinb'></span>

<h3>Description</h3>

<p>Implementation of a zero-inflated negbinom distribution for TFP
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_zinb(mu, r, probs)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_zinb_+3A_mu">mu</code>, <code id="tfd_zinb_+3A_r">r</code></td>
<td>
<p>parameter of the negbin_ls distribution</p>
</td></tr>
<tr><td><code id="tfd_zinb_+3A_probs">probs</code></td>
<td>
<p>vector of probabilites of length 2 (probability for poisson and
probability for 0s)</p>
</td></tr>
</table>

<hr>
<h2 id='tfd_zip'>Implementation of a zero-inflated poisson distribution for TFP</h2><span id='topic+tfd_zip'></span>

<h3>Description</h3>

<p>Implementation of a zero-inflated poisson distribution for TFP
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_zip(lambda, probs)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_zip_+3A_lambda">lambda</code></td>
<td>
<p>scalar value for rate of poisson distribution</p>
</td></tr>
<tr><td><code id="tfd_zip_+3A_probs">probs</code></td>
<td>
<p>vector of probabilites of length 2 (probability for poisson and
probability for 0s)</p>
</td></tr>
</table>

<hr>
<h2 id='tib_layer'>Hadamard-type layers</h2><span id='topic+tib_layer'></span><span id='topic+simplyconnected_layer'></span><span id='topic+inverse_group_lasso_pen'></span><span id='topic+regularizer_group_lasso'></span><span id='topic+tibgroup_layer'></span><span id='topic+layer_hadamard'></span><span id='topic+layer_group_hadamard'></span><span id='topic+layer_hadamard_diff'></span>

<h3>Description</h3>

<p>Hadamard-type layers
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tib_layer(units, la, ...)

simplyconnected_layer(la, ...)

inverse_group_lasso_pen(la)

regularizer_group_lasso(la, group_idx)

tibgroup_layer(units, group_idx, la, ...)

layer_hadamard(units = 1, la = 0, depth = 3, ...)

layer_group_hadamard(units, la, group_idx, depth, ...)

layer_hadamard_diff(
  units,
  la,
  initu = "glorot_uniform",
  initv = "glorot_uniform",
  ...
)

layer_hadamard(units = 1, la = 0, depth = 3, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tib_layer_+3A_units">units</code></td>
<td>
<p>integer; number of units</p>
</td></tr>
<tr><td><code id="tib_layer_+3A_la">la</code></td>
<td>
<p>numeric; regularization value (&gt; 0)</p>
</td></tr>
<tr><td><code id="tib_layer_+3A_...">...</code></td>
<td>
<p>arguments passed to TensorFlow layer</p>
</td></tr>
<tr><td><code id="tib_layer_+3A_group_idx">group_idx</code></td>
<td>
<p>list of group indices</p>
</td></tr>
<tr><td><code id="tib_layer_+3A_depth">depth</code></td>
<td>
<p>integer; depth of weight factorization</p>
</td></tr>
<tr><td><code id="tib_layer_+3A_initu">initu</code>, <code id="tib_layer_+3A_initv">initv</code></td>
<td>
<p>initializers for parameters</p>
</td></tr>
</table>


<h3>Value</h3>

<p>layer object
</p>

<hr>
<h2 id='update_miniconda_deepregression'>Function to update miniconda and packages</h2><span id='topic+update_miniconda_deepregression'></span>

<h3>Description</h3>

<p>Function to update miniconda and packages
</p>


<h3>Usage</h3>

<pre><code class='language-R'>update_miniconda_deepregression(
  python = VERSIONPY,
  uninstall = TRUE,
  also_packages = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="update_miniconda_deepregression_+3A_python">python</code></td>
<td>
<p>string; version of python</p>
</td></tr>
<tr><td><code id="update_miniconda_deepregression_+3A_uninstall">uninstall</code></td>
<td>
<p>logical; whether to uninstall previous conda env</p>
</td></tr>
<tr><td><code id="update_miniconda_deepregression_+3A_also_packages">also_packages</code></td>
<td>
<p>logical; whether to install also all required packages</p>
</td></tr>
</table>

<hr>
<h2 id='weight_control'>Options for weights of layers</h2><span id='topic+weight_control'></span>

<h3>Description</h3>

<p>Options for weights of layers
</p>


<h3>Usage</h3>

<pre><code class='language-R'>weight_control(
  specific_weight_options = NULL,
  general_weight_options = list(activation = NULL, use_bias = FALSE, trainable = TRUE,
    kernel_initializer = "glorot_uniform", bias_initializer = "zeros", kernel_regularizer
    = NULL, bias_regularizer = NULL, activity_regularizer = NULL, kernel_constraint =
    NULL, bias_constraint = NULL),
  warmstart_weights = NULL,
  shared_layers = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="weight_control_+3A_specific_weight_options">specific_weight_options</code></td>
<td>
<p>specific options for certain
weight terms; must be a list of length <code>length(list_of_formulas)</code> and
each element in turn a named list (names are term names as in the formula)
with specific options in a list</p>
</td></tr>
<tr><td><code id="weight_control_+3A_general_weight_options">general_weight_options</code></td>
<td>
<p>default options for layers</p>
</td></tr>
<tr><td><code id="weight_control_+3A_warmstart_weights">warmstart_weights</code></td>
<td>
<p>While all keras layer options are availabe,
the user can further specify a list for each distribution parameter
with list elements corresponding to term names with values as vectors
corresponding to start weights of the respective weights</p>
</td></tr>
<tr><td><code id="weight_control_+3A_shared_layers">shared_layers</code></td>
<td>
<p>list for each distribution parameter;
each list item can be again a list of character vectors specifying terms
which share layers</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a list with options
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
