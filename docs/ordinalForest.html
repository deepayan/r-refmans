<!DOCTYPE html><html><head><title>Help for package ordinalForest</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {ordinalForest}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#ordinalForest-package'><p>Ordinal Forests: Prediction and Variable Ranking with Ordinal Target Variables</p></a></li>
<li><a href='#hearth'><p>Data on Coronary Artery Disease</p></a></li>
<li><a href='#ordfor'><p>Ordinal forests</p></a></li>
<li><a href='#perff'><p>Performance functions based on Youden's J statistic</p></a></li>
<li><a href='#predict.ordfor'><p>Prediction using ordinal forest objects</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Ordinal Forests: Prediction and Variable Ranking with Ordinal
Target Variables</td>
</tr>
<tr>
<td>Version:</td>
<td>2.4-3</td>
</tr>
<tr>
<td>Date:</td>
<td>2022-11-29</td>
</tr>
<tr>
<td>Author:</td>
<td>Roman Hornung</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Roman Hornung &lt;hornung@ibe.med.uni-muenchen.de&gt;</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp (&ge; 0.11.2), combinat, nnet, verification</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp</td>
</tr>
<tr>
<td>Description:</td>
<td>The ordinal forest (OF) method allows ordinal regression with high-dimensional
  and low-dimensional data. After having constructed an OF prediction rule using a training dataset, 
  it can be used to predict the values of the ordinal target variable for new observations.
  Moreover, by means of the (permutation-based) variable importance measure of OF, it is also
  possible to rank the covariates with respect to their importance in the prediction of the 
  values of the ordinal target variable.
  OF is presented in Hornung (2020).
  NOTE: Starting with package version 2.4, it is also possible to obtain class probability 
  predictions in addition to the class point predictions. Moreover, the variable importance values
  can also be based on the class probability predictions. Preliminary results indicate that
  this might lead to a better discrimination between influential and non-influential covariates.
  The main functions of the package are: ordfor() (construction of OF) and predict.ordfor() 
  (prediction of the target variable values of new observations).
  References:
  Hornung R. (2020) Ordinal Forests. Journal of Classification 37, 4–17. 
  &lt;<a href="https://doi.org/10.1007%2Fs00357-018-9302-x">doi:10.1007/s00357-018-9302-x</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.2</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-11-30 12:29:13 UTC; hornung</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-11-30 13:50:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='ordinalForest-package'>Ordinal Forests: Prediction and Variable Ranking with Ordinal Target Variables</h2><span id='topic+ordinalForest-package'></span><span id='topic+ordinalForest'></span>

<h3>Description</h3>

<p>The ordinal forest (OF) method allows ordinal regression with high-dimensional
and low-dimensional data. After having constructed an OF prediction rule using a training dataset, 
it can be used to predict the values of the ordinal target variable for new observations.
Moreover, by means of the (permutation-based) variable importance measure of OF, it is also possible to rank the covariates 
with respect to their importances in the prediction of the values of the ordinal target 
variable. <br />
OF is presented in Hornung (2020).
</p>


<h3>Details</h3>

<p>Starting with package version 2.4, it is also possible to obtain class probability 
predictions in addition to the class point predictions and variable importance values
based on the class probabilities through using the (negative) ranked probability score (Epstein, 1969)
as performance function (<code>perffunction="probability"</code>, new default). Using the ranked probability score in the variable importance can be expected to deliver more stable variable rankings, because the ranked probability score accounts for the ordinal scale of the dependent variable.  In situations in which there is no need for predicting class probabilities, but simply
class predictions are sufficient, other performance functions may be more suitable. See the documentation of the <code><a href="#topic+ordfor">ordfor</a></code> function for further details.
</p>
<p>For a brief, practice-orientated introduction to OF see: <code><a href="#topic+ordfor">ordfor</a></code>
</p>
<p>The main functions are: <code><a href="#topic+ordfor">ordfor</a></code> (construction of OF prediction rules) and
<code><a href="#topic+predict.ordfor">predict.ordfor</a></code> (prediction of the values of the target variable values of new observations).
</p>
<p>NOTE: <span class="pkg">ordinalForest</span> uses R code and C++ code from the R package <span class="pkg">ranger</span> for the involved regression forests.
<span class="pkg">ordinalForest</span> does, however, not depend on <span class="pkg">ranger</span> or import <span class="pkg">ranger</span>, because it was necessary to
copy the C++ code and parts of the R code from <span class="pkg">ranger</span> to <span class="pkg">ordinalForest</span> instead. The reason for this
is that <span class="pkg">ranger</span>'s C++ code had to be altered in part in order to implement ordinal forest.
</p>


<h3>References</h3>


<ul>
<li><p> Hornung R. (2020) Ordinal Forests. Journal of Classification 37, 4–17. &lt;doi: <a href="https://doi.org/10.1007/s00357-018-9302-x">10.1007/s00357-018-9302-x</a>&gt;.
</p>
</li>
<li><p> Epstein E.S. (1969) A scoring system for probability forecasts of ranked categories, Journal of Applied Meteorology. 8(6), 985-987.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Illustration of the key functionalities of the package:
##########################################################

# Load example dataset:

data(hearth)

# Inspect the data:
table(hearth$Class)
dim(hearth)

head(hearth) 


# Split into training dataset and test dataset:

set.seed(123)
trainind &lt;- sort(sample(1:nrow(hearth), size=floor(nrow(hearth)*(2/3))))
testind &lt;- setdiff(1:nrow(hearth), trainind)

datatrain &lt;- hearth[trainind,]
datatest &lt;- hearth[testind,]


# Construct OF prediction rule using the training dataset (default 
# perffunction = "probability" corresponding to the 
# (negative) ranked probability score as performance function):

ordforres &lt;- ordfor(depvar="Class", data=datatrain, nsets=1000, ntreeperdiv=100, 
  ntreefinal=5000, perffunction = "equal")
ordforres

# Study variable importance values:
sort(ordforres$varimp, decreasing=TRUE)

# Take a closer look at the top variables:
boxplot(datatrain$oldpeak ~ datatrain$Class, horizontal=TRUE)
fisher.test(table(datatrain$exang, datatrain$Class))

# Predict values of the ordinal target variable in the test dataset:

preds &lt;- predict(ordforres, newdata=datatest)
preds

# Compare predicted values with true values:
table(data.frame(true_values=datatest$Class, predictions=preds$ypred))

## End(Not run) 

</code></pre>

<hr>
<h2 id='hearth'>Data on Coronary Artery Disease</h2><span id='topic+hearth'></span>

<h3>Description</h3>

<p>This data includes 294 patients undergoing angiography at the Hungarian Institute of Cardiology in Budapest between 1983 and 1987.
</p>


<h3>Format</h3>

<p>A data frame with 294 observations, ten covariates and one ordinal target variable
</p>


<h3>Details</h3>

<p>The variables are as follows:
</p>

<ul>
<li> <p><code>age</code>. numeric. Age in years 
</p>
</li>
<li> <p><code>sex</code>. factor. Sex (1 = male; 0 = female)
</p>
</li>
<li> <p><code>chest_pain</code>. factor. Chest pain type (1 = typical angina; 2 = atypical angina; 3 = non-anginal pain; 4 = asymptomatic)
</p>
</li>
<li> <p><code>trestbps</code>. numeric. Resting blood pressure (in mm Hg on admission to the hospital)
</p>
</li>
<li> <p><code>chol</code>. numeric. Serum cholestoral in mg/dl
</p>
</li>
<li> <p><code>fbs</code>. factor. Fasting blood sugar &gt; 120 mg/dl (1 = true; 0 = false)
</p>
</li>
<li> <p><code>restecg</code>. factor. Resting electrocardiographic results (1 = having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of &gt; 0.05 mV); 0 = normal)
</p>
</li>
<li> <p><code>thalach</code>. numeric. Maximum heart rate achieved 
</p>
</li>
<li> <p><code>exang</code>. factor. Exercise induced angina (1 = yes; 0 = no)
</p>
</li>
<li> <p><code>oldpeak</code>. numeric. ST depression induced by exercise relative to rest
</p>
</li>
<li> <p><code>Class</code>. factor. Ordinal target variable - severity of coronary artery disease (determined using angiograms) (1 = no disease; 2 = degree 1; 3 = degree 2; 4 = degree 3; 5 = degree 4)
</p>
</li></ul>

<p><code style="white-space: pre;">&#8288; &#8288;</code><br />
The original openML dataset was pre-processed in the following way: <br />
</p>
<p>1. The variables were re-named according to the description given on openML.
</p>
<p>2. The missing values which were coded as &quot;-9&quot; were replaced by NA values.
</p>
<p>3. The variables <code>slope</code>, <code>ca</code>, and <code>thal</code> were excluded, because these featured
too many missing values.
</p>
<p>4. The categorical covariates were transformed into factors.
</p>
<p>5. There were 6 <code>restecg</code> values of &quot;2&quot; which were replaced by &quot;1&quot;.
</p>
<p>6. The missing values were imputed: The missing values of the numerical covariates were replaced by the means
of the corresponding non-missing values. The missing values of the categorical covariates were replaced by
the modes of the corresponding non-missing values.
</p>


<h3>Source</h3>

<p>OpenML: data.name: heart-h, data.id: 1565, link: <a href="https://www.openml.org/d/1565/">https://www.openml.org/d/1565/</a>
</p>


<h3>References</h3>


<ul>
<li><p> Detrano, R., Janosi, A., Steinbrunn, W., Pfisterer, M., Schmid, J.-J., Sandhu, S., Guppy, K. H., Lee, S., Froelicher, V. (1989) International application of a new probability algorithm for the diagnosis of coronary artery disease. The American Journal Of Cardiology, 64, 304&ndash;310.
</p>
</li>
<li><p> Vanschoren, J., van Rijn, J. N., Bischl, B., Torgo, L. (2013) OpenML: networked science in machine learning. SIGKDD Explorations, 15(2), 49&ndash;60.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>data(hearth)

table(hearth$Class)
dim(hearth)

head(hearth) 

</code></pre>

<hr>
<h2 id='ordfor'>Ordinal forests</h2><span id='topic+ordfor'></span>

<h3>Description</h3>

<p>Constructs prediction rules using the ordinal forest (OF) method presented in Hornung (2020). <br />
The following tasks can be performed using OF: 1) Predicting the values of an ordinal target variable for new observations based on covariate values (see <code><a href="#topic+predict.ordfor">predict.ordfor</a></code>);
2) Ranking the importances of the covariates with respect to predicting the values of the ordinal target variable. <br />
The default values for the hyperparameters <code>nsets</code>, <code>ntreeperdiv</code>, <code>ntreefinal</code>, <code>npermtrial</code>, and <code>nbest</code>
were found to be in a reasonable range in Hornung (2020) and it should not be necessary to alter these values in most situations. <br />
For details on OFs see the 'Details' section below. <br />
NOTE: Starting with package version 2.4, it is also possible to obtain class probability 
predictions in addition to the class point predictions and variable importance values
based on the class probabilities through using the (negative) ranked probability score (Epstein, 1969)
as performance function (<code>perffunction="probability"</code>). Using the ranked probability score in the variable importance can be expected to deliver more stable variable rankings, because the ranked probability score accounts for the ordinal scale of the dependent variable.  In situations in which there is no need for predicting class probabilities, but simply
class predictions are sufficient, other performance functions may be more suitable. See the subsection &quot;Performance functions&quot; in the &quot;Details&quot; section below for further details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ordfor(
  depvar,
  data,
  nsets = 1000,
  ntreeperdiv = 100,
  ntreefinal = 5000,
  importance = c("rps", "accuracy"),
  perffunction = c("equal", "probability", "proportional", "oneclass", "custom"),
  classimp,
  classweights,
  nbest = 10,
  naive = FALSE,
  num.threads = NULL,
  npermtrial = 500,
  permperdefault = FALSE,
  mtry = NULL,
  min.node.size = NULL,
  replace = TRUE,
  sample.fraction = ifelse(replace, 1, 0.632),
  always.split.variables = NULL,
  keep.inbag = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ordfor_+3A_depvar">depvar</code></td>
<td>
<p>character. Name of the dependent variable in <code>data</code>.</p>
</td></tr>
<tr><td><code id="ordfor_+3A_data">data</code></td>
<td>
<p>data.frame. Data frame containing the covariates and a factor-valued ordinal target variable. The order of the levels of the latter
has to correspond to the order of the ordinal classes of the target variable.</p>
</td></tr>
<tr><td><code id="ordfor_+3A_nsets">nsets</code></td>
<td>
<p>integer. Number of score sets tried prior to the approximation
of the optimal score set.</p>
</td></tr>
<tr><td><code id="ordfor_+3A_ntreeperdiv">ntreeperdiv</code></td>
<td>
<p>integer. Number of trees in the smaller regression forests constructed 
for each of the <code>nsets</code> different score sets tried.</p>
</td></tr>
<tr><td><code id="ordfor_+3A_ntreefinal">ntreefinal</code></td>
<td>
<p>integer. Number of trees in the larger regression forest 
constructed using the optimized score set (i.e., the OF).</p>
</td></tr>
<tr><td><code id="ordfor_+3A_importance">importance</code></td>
<td>
<p>character. The type of variable importance measure to use. The default <code>"rps"</code> uses the ranked probability score as an error measure. 
If set to <code>"accuracy"</code>, the importance measure is based on the accuracy. The latter choice corresponds to the default importance measure of random forests
and does not take the ordinal scale of the target variable into account. NOTE: If the ranked probability score is used as performance function (<code>perffunction="probability"</code>),
<code>importance</code> is set to <code>"rps"</code> automatically. Preliminary results indicate that the option <code>"rps"</code> might lead to a better discrimination between
influential and non-influential covariates.</p>
</td></tr>
<tr><td><code id="ordfor_+3A_perffunction">perffunction</code></td>
<td>
<p>character. Performance function. The default is <code>"equal"</code>. See 'Details', subsection 'Performance functions' below and <code><a href="#topic+perff">perff</a></code>.</p>
</td></tr>
<tr><td><code id="ordfor_+3A_classimp">classimp</code></td>
<td>
<p>character. Class to prioritize if <code>perffunction="oneclass"</code>.</p>
</td></tr>
<tr><td><code id="ordfor_+3A_classweights">classweights</code></td>
<td>
<p>numeric. Needed if <code>perffunction="custom"</code>: vector of length equal to the number of classes. Class weights - the
higher the weight w_j assigned to class j is chosen, the higher the accuracy of the OF with respect to discerning observations in class j from observations
not in class j will tend to be.</p>
</td></tr>
<tr><td><code id="ordfor_+3A_nbest">nbest</code></td>
<td>
<p>integer. Number of best score sets used to calculate the optimized score set.</p>
</td></tr>
<tr><td><code id="ordfor_+3A_naive">naive</code></td>
<td>
<p>boolean. If set to <code>TRUE</code>, a naive ordinal forest is constructed, that is, the score set used for the 
classes of the target variable is not optimized, but instead the following (naive) scores
are used: 1,2,3,... Note that it is strongly recommended to set <code>naive=FALSE</code> (default). The only advantage of choosing <code>naive=TRUE</code> is
that the computational burden is reduced. However, the precision of the predictions of a prediction rule obtained using 
naive ordinal forest can be considerably worse than that of a corresponding prediction rule obtained using ordinal forest.</p>
</td></tr>
<tr><td><code id="ordfor_+3A_num.threads">num.threads</code></td>
<td>
<p>integer. Number of threads. Default is number of CPUs available (passed to the modified <code>ranger</code> code).</p>
</td></tr>
<tr><td><code id="ordfor_+3A_npermtrial">npermtrial</code></td>
<td>
<p>integer. Number of permutations of the class width ordering to try for
the second to the <code>nsets</code>th score set tried prior to the calculation of
the optimized score set.</p>
</td></tr>
<tr><td><code id="ordfor_+3A_permperdefault">permperdefault</code></td>
<td>
<p>boolean. If set to <code>TRUE</code>, <code>npermtrial</code> different permutations will per default 
be tried for the 2th to the <code>nsets</code>th score set used during the optimization - also for J! &lt; <code>nsets</code>. Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="ordfor_+3A_mtry">mtry</code></td>
<td>
<p>integer. Number of variables to sample as candidate variables for each split. Default is the (rounded down) square root of the number of variables.</p>
</td></tr>
<tr><td><code id="ordfor_+3A_min.node.size">min.node.size</code></td>
<td>
<p>integer. Minimal node size. Default is 5, except if <code>perffunction="probability"</code>, in which case the default is 10.</p>
</td></tr>
<tr><td><code id="ordfor_+3A_replace">replace</code></td>
<td>
<p>boolean. Sample with replacement. Default is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="ordfor_+3A_sample.fraction">sample.fraction</code></td>
<td>
<p>numeric. Fraction of observations to sample. Default is 1 for sampling with replacement and 0.632 for sampling without replacement.</p>
</td></tr>
<tr><td><code id="ordfor_+3A_always.split.variables">always.split.variables</code></td>
<td>
<p>character. Character vector with variable names to be always selected in addition to the <code>mtry</code> variables tried for splitting.</p>
</td></tr>
<tr><td><code id="ordfor_+3A_keep.inbag">keep.inbag</code></td>
<td>
<p>boolean. Save how often observations are in-bag in each tree. Default is <code>FALSE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>



<h4>Introduction</h4>

<p>The ordinal forest (OF) method allows ordinal regression with high-dimensional and low-dimensional 
data. After having constructed an OF prediction rule using a training dataset, it can be used to predict the values of the ordinal target variable for new observations.
Moreover, by means of the (permutation-based) variable importance measure of OF, it is also 
possible to rank the covariates with respect to their importance in the prediction of the values
of the ordinal target variable. <br />
OF is presented in Hornung (2020). See the latter publication for details on the method. In the
following, a brief, practice-orientated introduction to OF is provided.
</p>



<h4>Methods</h4>

<p>The concept of OF is based on the following assumption: There exists a (possibly latent) refined continuous 
variable y* underlying the observed ordinal target variable y (y in {1,...,J}, J number of classes), where y* determines the
values of y. The functional relationship between y* and y takes the form of a monotonically increasing
step function. Depending on which of J intervals ]c_1,<code style="white-space: pre;">&#8288; &#8288;</code>c_2], <code style="white-space: pre;">&#8288; &#8288;</code>]c_2,<code style="white-space: pre;">&#8288; &#8288;</code>c_3], <code style="white-space: pre;">&#8288; &#8288;</code> ..., <code style="white-space: pre;">&#8288; &#8288;</code> ]c_J,<code style="white-space: pre;">&#8288; &#8288;</code>c_{J+1}[
contains the value of y*, the ordinal target variable y takes a different value.
</p>
<p>In situations in which the values of the continuous target variable y* are known, they can be used in regression techniques for continuous response variables.
The OF method is, however, concerned with settings in which only the values of the classes of the ordinal target variable are given.
The main idea of OF is to optimize score values s_1,...,s_J to be used in place of the class values 1,...,J of the ordinal target variable
in standard regression forests by maximizing the out-of-bag (OOB) prediction performance measured by a performance function g (see section 
&quot;Performance functions&quot;).
</p>
<p>The approximation of the optimal score set consists of two steps:<br />
1) Construct a large number of regression forests (b in 1,...,<code>nsets</code>) featuring limited numbers 
of trees, where each of these uses as the values of the target variable a randomly generated score set 
s_{b,1},...,s_{b,J}. For each forest constructed, calculate the value of the performance function g 
using the OOB estimated predictions of the values of the ordinal target variable and the corresponding 
true values.<br />
2) Calculate the approximated optimal score set s_1,...,s_J as a summary over the <code>nbest</code> best score sets generated in 1),
that is, those <code>nbest</code> score sets that were associated with the highest values of the performance function g.
</p>
<p>After calculating the optimized score set, a larger regression forest is constructed using this optimized score set
s_1,...,s_J for the class values 1,...,J of the target variable. This regression forest is the OF prediction rule.
</p>
<p>Except in the case of using the (negative) ranked probabilty score as performance function, prediction is performed by majority voting of the predictions of the individual trees in the OF.
If the (negative) ranked probabilty score is used as performance function, both class predictions and predicted class probabilities are provided: The class probabilities
are obtained by averaging over the class probabilities predicted by the individual trees and the class predictions are obtained as the classes with maximum class probabilites.
</p>
<p>OF features a permutation variable importance measure that, if <code>importance</code> is set to <code>"rps"</code> (default), uses the ranked probability
score as error measure and the misclassification error else (<code>importance="accuracy"</code>).
</p>



<h4>Hyperparameters</h4>

<p>There are several hyperparameters, which do, however, not have to be optimized by the user in general, because the default values
used for these hyperparameters were seen to be in a reasonable range and the results seem to be quite robust with respect to the 
choices of the hyperparameter values.
</p>
<p>These hyperparameters are described in the following:
</p>

<ul>
<li> <p><code>nsets</code> <code style="white-space: pre;">&#8288;   &#8288;</code> Default value: 1000. The default value of the number of considered score sets in the approximation of the optimal score set 
is quite large. A large number of considered score sets is necessary to attain a high chance that some of the score sets are close enough to the optimal score set,
that is, the score set that leads to the optimal OOB prediction performance with respect to the considered performance function (provided with the argument <code>perffunction</code>).
</p>
</li>
<li> <p><code>ntreeperdiv</code> <code style="white-space: pre;">&#8288;   &#8288;</code> Default value: 100. A very small number of trees considered per tried
score set might lead to a too strong variability in the assessments of the performances achieved for the individual score sets. For ultra-high dimensional
covariate data it might be necessary to choose a higher value for <code>ntreeperdiv</code> than the default value 100.
</p>
</li>
<li> <p><code>ntreefinal</code> <code style="white-space: pre;">&#8288;   &#8288;</code> Default value: 5000. The number of trees <code>ntreefinal</code>
plays the same role as in conventional regression forests.
</p>
</li>
<li> <p><code>npermtrial</code> <code style="white-space: pre;">&#8288;   &#8288;</code> Default value: 500. As stated above it is necessary to consider a large number of tried score sets <code>nsets</code> in the
optimization in order to increase the chance that the best of the considered score sets are close to the optimal score set.
To further increase this chance, it is in addition necessary that the collection of score sets tried is heterogeneous enough across 
the iterations. OF uses a particular algorithm for sampling the score sets tried that leads to a strongly heterogeneous collection of sets.
This algorithm features the hyperparameter <code>npermtrial</code>, where it has been seen in Hornung (2020) that the results are quite 
robust with respect to the choice of the value of this parameter.
</p>
</li>
<li> <p><code>nbest</code> <code style="white-space: pre;">&#8288;   &#8288;</code> Default value: 10. In the case of a relatively small value of <code>nsets</code>, it is important that the 
number <code>nbest</code> of best score sets used to calculate the optimized score set is not strongly misspecified. A too large value of <code>nbest</code> 
leads to including suboptimal score sets into the calculation of the optimized score set that are too distinct from the optimal score set.
Conversely, a too small value of <code>nbest</code> leads to a high variance of the optimized score set. The combination <code>nsets=1000</code> and <code>nbest=10</code>
should lead to a good trade-off between the heterogeneity of the considered score sets and the variance in the estimation.
In Hornung (2020) this combination delivered good results and it was seen that using a very large value of <code>nbest</code> can lead to worse results.
</p>
</li></ul>




<h4>Performance functions</h4>

<p>As noted above, the different score sets tried during the estimation of the optimal score set are assessed with respect to their OOB prediction performance.
The choice of the specific performance function used in these assessments determines the specific kind of performance the ordinal forest should feature:
</p>

<ul>
<li> <p><code>perffunction="equal"</code> <code style="white-space: pre;">&#8288;   &#8288;</code> This choice should be made if it is of interest to classify observations from each class with the same accuracy independent of the class sizes. 
Youden's J statistic is calculated with respect to each class (&quot;observation/prediction in class j&quot; vs. &quot;observation/prediction NOT in class j&quot; (j=1,...,J))
and the simple average of the J results taken.
</p>
</li>
<li> <p><code>perffunction="probability"</code> <code style="white-space: pre;">&#8288;   &#8288;</code> This choice should be made if it is of interest to predict class probabilties for the observations.
The ranked probability score is calculated between the predicted probabilities for the J classes and the observed class values. Because smaller values
of the ranked probability score correspond to a better prediction, the negative ranked probability score is considered as performance functions.
</p>
</li>
<li> <p><code>perffunction="proportional"</code> <code style="white-space: pre;">&#8288;   &#8288;</code> This choice should be made if the main goal is to classify
correctly as many observations as possible. The latter is associated with a preference for larger classes at the 
expense of a lower classification accuracy with respect to smaller classes.
Youden's J statistic is calculated with respect to each class and subsequently a weighted average of these values is taken - with weights 
proportional to the number of observations representing the respective classes in the training data.
</p>
</li>
<li> <p><code>perffunction="oneclass"</code> <code style="white-space: pre;">&#8288;   &#8288;</code> This choice should be made if it is merely relevant that observations 
in class <code>categ</code> can be distinguished as reliably as possible from observations not in class <code>categ</code>.
Class <code>categ</code> must be passed to <code>ordfor</code> via the argument <code>categ</code>.
Youden's J statistic is calculated with respect to class <code>categ</code>.
</p>
</li>
<li> <p><code>perffunction="custom"</code> <code style="white-space: pre;">&#8288;   &#8288;</code> This choice should be made if there is a particular ranking of the classes with respect to their importance. 
Youden's J statistic is calculated with respect to each class. Subsequently, a weighted average
with user-specified weights (provided via the argument <code>classweights</code>) is taken. In this way, classes with 
higher weights are prioritized by the OF algorithm over classes with smaller weights.
</p>
</li></ul>




<h3>Value</h3>

<p><code>ordfor</code> returns an object of class <code>ordfor</code>.
An object of class &quot;<code>ordfor</code>&quot; is a list containing the following components: 
</p>
<table>
<tr><td><code>forestfinal</code></td>
<td>
<p> object of class <code>"ranger"</code>. Regression forest constructed using the optimized score set (i.e., the OF). Required by <code><a href="#topic+predict.ordfor">predict.ordfor</a></code>.  </p>
</td></tr>
<tr><td><code>bordersbest</code></td>
<td>
<p> vector of length J+1. Average over the <code>nbest</code> best partitions of [0,1]. Required by <code><a href="#topic+predict.ordfor">predict.ordfor</a></code>. </p>
</td></tr>
<tr><td><code>forests</code></td>
<td>
<p> list of length <code>nsets</code>. The regression forests constructed for the <code>nsets</code> different score sets tried prior to the approximation of the optimal score set. </p>
</td></tr>
<tr><td><code>perffunctionvalues</code></td>
<td>
<p> vector of length <code>nsets</code>. Performance function values for all score sets tried prior to the approximation of the optimal score set. </p>
</td></tr>
<tr><td><code>bordersb</code></td>
<td>
<p> matrix of dimension <code>nsets</code> x (J+1). All <code>nsets</code> partitions of [0,1] considered. </p>
</td></tr>
<tr><td><code>classes</code></td>
<td>
<p> character vector of length J. Classes of the target variable. </p>
</td></tr>
<tr><td><code>nsets</code></td>
<td>
<p> integer. Number of score sets tried prior to the approximation of the optimal score set. </p>
</td></tr>
<tr><td><code>ntreeperdiv</code></td>
<td>
<p> integer. Number of trees per score set considered. </p>
</td></tr>
<tr><td><code>ntreefinal</code></td>
<td>
<p> integer. Number of trees of the OF prediction rule. </p>
</td></tr>
<tr><td><code>perffunction</code></td>
<td>
<p> character. Performance function used.  </p>
</td></tr>
<tr><td><code>classimp</code></td>
<td>
<p> character. If <code>perffunction="oneclass"</code>: class to priorize, NA else. </p>
</td></tr>
<tr><td><code>nbest</code></td>
<td>
<p> integer. Number of best score sets used to approximate the optimal score set. </p>
</td></tr>
<tr><td><code>classfreq</code></td>
<td>
<p> table. Class frequencies. </p>
</td></tr>
<tr><td><code>varimp</code></td>
<td>
<p> vector of length p. Permutation variable importance for each covariate. If <code>perffunction="probability"</code>, the ranked probability score is used as error measure in the variable importance. For all other choices of the performance function, the misclassification error is used. </p>
</td></tr>
</table>


<h3>References</h3>


<ul>
<li><p> Hornung R. (2020) Ordinal Forests. Journal of Classification 37, 4–17. &lt;doi: <a href="https://doi.org/10.1007/s00357-018-9302-x">10.1007/s00357-018-9302-x</a>&gt;.
</p>
</li>
<li><p> Epstein E.S. (1969) A scoring system for probability forecasts of ranked categories, Journal of Applied Meteorology. 8(6), 985-987.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
data(hearth)

set.seed(123)
hearthsubset &lt;- hearth[sort(sample(1:nrow(hearth), size=floor(nrow(hearth)*(1/2)))),]
ordforres &lt;- ordfor(depvar="Class", data=hearthsubset, nsets=50, nbest=5, ntreeperdiv=100, 
  ntreefinal=1000)
# NOTE: nsets=50 is not enough, because the prediction performance of the resulting 
# ordinal forest will be suboptimal!! In practice, nsets=1000 (default value) or a 
# larger number should be used.

ordforres

sort(ordforres$varimp, decreasing=TRUE)

## End(Not run)

</code></pre>

<hr>
<h2 id='perff'>Performance functions based on Youden's J statistic</h2><span id='topic+perff'></span><span id='topic+perff_equal'></span><span id='topic+perff_proportional'></span><span id='topic+perff_oneclass'></span><span id='topic+perff_custom'></span>

<h3>Description</h3>

<p>In <code><a href="#topic+ordfor">ordfor</a></code> so-called performance functions are used to measure the performance of the 
smaller regression forests constructed prior to the approximation of the optimal score set.
Except for one, which uses the ranked probability score (enabling class probability estimation), all of these performance functions are based on Youden's J statistic. 
These functions may, however, also be used to measure the precision of
predictions on new data or the precision of OOB predictions. Note that the performance function using the
ranked probability score is not covered in this help page. The function <code>rps</code> from the package 
<code>verification</code> (version 1.42) can be used to calculate the ranked probability score.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>perff_equal(ytest, ytestpred, categ, classweights)

perff_proportional(ytest, ytestpred, categ, classweights)

perff_oneclass(ytest, ytestpred, categ, classweights)

perff_custom(ytest, ytestpred, categ, classweights)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="perff_+3A_ytest">ytest</code></td>
<td>
<p>factor. True values of the target variable.</p>
</td></tr>
<tr><td><code id="perff_+3A_ytestpred">ytestpred</code></td>
<td>
<p>factor. Predicted values of the target variable.</p>
</td></tr>
<tr><td><code id="perff_+3A_categ">categ</code></td>
<td>
<p>character. Needed in the case of <code>perff_oneclass</code>: Class to prioiritize.</p>
</td></tr>
<tr><td><code id="perff_+3A_classweights">classweights</code></td>
<td>
<p>numeric. Needed in the case of <code>perff_custom</code>: Vector of length equal to the number of classes. Class weights - classes with 
higher weights are to be prioiritzed over those with smaller weights.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>perff_equal</code> should be used if it is of interest to classify observations from each class with the same accuracy independent of the class sizes. 
Youden's J statistic is calculated with respect to each class (&quot;observation/prediction in class j&quot; vs. &quot;observation/prediction NOT in class j&quot; (j=1,...,J))
and the simple average of the J results taken.
</p>
<p><code>perff_proportional</code> should be used if the main goal is to classify
correctly as many observations as possible. The latter is associated with a preference for larger classes at the 
expense of a lower classification accuracy with respect to smaller classes.
Youden's J statistic is calculated with respect to each class and subsequently a weighted average of these values is taken - with weights 
proportional to the number of observations representing the respective classes in the training data.
</p>
<p><code>perff_oneclass</code> should be used if it is merely relevant that observations 
in class <code>categ</code> can be distinguished as reliably as possible from observations not in class <code>categ</code>.
Class <code>categ</code> must be passed to <code>perff_oneclass</code> via the argument <code>categ</code>.
Youden's J statistic is calculated with respect to class <code>categ</code>.
</p>
<p><code>perff_custom</code> should be used if there is a particular ranking of the classes with respect to their importance. 
Youden's J statistic is calculated with respect to each class. Subsequently, a weighted average
with user-specified weights (provided via the argument <code>classweights</code>) is taken. In this way, classes with 
higher weights are prioritized by the OF algorithm over classes with smaller weights.
</p>


<h3>References</h3>


<ul>
<li><p> Hornung R. (2020) Ordinal Forests. Journal of Classification 37, 4–17. &lt;doi: <a href="https://doi.org/10.1007/s00357-018-9302-x">10.1007/s00357-018-9302-x</a>&gt;.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
data(hearth)

set.seed(123)
trainind &lt;- sort(sample(1:nrow(hearth), size=floor(nrow(hearth)*(1/2))))
testind &lt;- sort(sample(setdiff(1:nrow(hearth), trainind), size=20))

datatrain &lt;- hearth[trainind,]
datatest &lt;- hearth[testind,]

ordforres &lt;- ordfor(depvar="Class", data=datatrain, nsets=50, nbest=5, ntreeperdiv=100, 
  ntreefinal=1000)
# NOTE: nsets=50 is not enough, because the prediction performance of the resulting 
# ordinal forest will be suboptimal!! In practice, nsets=1000 (default value) or a larger
# number should be used.

preds &lt;- predict(ordforres, newdata=datatest)

table('true'=datatest$Class, 'predicted'=preds$ypred)


perff_equal(ytest=datatest$Class, ytestpred=preds$ypred)
 
perff_proportional(ytest=datatest$Class, ytestpred=preds$ypred)
 
perff_oneclass(ytest=datatest$Class, ytestpred=preds$ypred, categ="1")
perff_oneclass(ytest=datatest$Class, ytestpred=preds$ypred, categ="2")
perff_oneclass(ytest=datatest$Class, ytestpred=preds$ypred, categ="3")
perff_oneclass(ytest=datatest$Class, ytestpred=preds$ypred, categ="4")
perff_oneclass(ytest=datatest$Class, ytestpred=preds$ypred, categ="5")


perff_custom(ytest=datatest$Class, ytestpred=preds$ypred, classweights=c(1,2,1,1,1))


# perff_equal, perff_proportional, and perff_oneclass are special cases of perff_custom:

perff_custom(ytest=datatest$Class, ytestpred=preds$ypred, classweights=c(1,1,1,1,1))
perff_equal(ytest=datatest$Class, ytestpred=preds$ypred)

perff_custom(ytest=datatest$Class, ytestpred=preds$ypred, classweights=table(datatest$Class))
perff_proportional(ytest=datatest$Class, ytestpred=preds$ypred)

perff_custom(ytest=datatest$Class, ytestpred=preds$ypred, classweights=c(0,0,0,1,0))
perff_oneclass(ytest=datatest$Class, ytestpred=preds$ypred, categ="4")

## End(Not run) 

</code></pre>

<hr>
<h2 id='predict.ordfor'>Prediction using ordinal forest objects</h2><span id='topic+predict.ordfor'></span>

<h3>Description</h3>

<p>Prediction of test data using ordinal forest.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ordfor'
predict(object, newdata, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.ordfor_+3A_object">object</code></td>
<td>
<p>object of class <code>ordfor</code>. See function <code><a href="#topic+ordfor">ordfor</a></code>.</p>
</td></tr>
<tr><td><code id="predict.ordfor_+3A_newdata">newdata</code></td>
<td>
<p>data.frame. Data frame containing new data.</p>
</td></tr>
<tr><td><code id="predict.ordfor_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>predict.ordfor</code> returns an object of class <code>ordforpred</code>.
An object of class &quot;<code>ordforpred</code>&quot; is a list containing the following components: 
</p>
<table>
<tr><td><code>ypred</code></td>
<td>
<p> vector of length <code>nrow(newdata)</code>. Factor-valued test data predictions. </p>
</td></tr>
<tr><td><code>classprobs</code></td>
<td>
<p> predicted class probabilities. Only provided, if the performance function based on the ranked probability score was used, while training the ordinal forest (see <code><a href="#topic+ordfor">ordfor</a></code>).
Matrix of dimension <code>nrow(newdata)</code> x J (NA, if the ranked probability was not used while training). The value in the j-th column of the i-th row contains the predicted probability that test observation i is of class j. </p>
</td></tr>
</table>


<h3>References</h3>


<ul>
<li><p> Hornung R. (2020) Ordinal Forests. Journal of Classification 37, 4–17. &lt;doi: <a href="https://doi.org/10.1007/s00357-018-9302-x">10.1007/s00357-018-9302-x</a>&gt;.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
data(hearth)

set.seed(123)
trainind &lt;- sort(sample(1:nrow(hearth), size=floor(nrow(hearth)*(1/2))))
testind &lt;- sort(sample(setdiff(1:nrow(hearth), trainind), size=20))

datatrain &lt;- hearth[trainind,]
datatest &lt;- hearth[testind,]

ordforres &lt;- ordfor(depvar="Class", data=datatrain, perffunction = "probability", nsets=50,
  nbest=5, ntreeperdiv=100, ntreefinal=1000)
# NOTE: nsets=50 is not enough, because the prediction performance of the resulting 
# ordinal forest will be suboptimal!! In practice, nsets=1000 (default value) or a larger
# number should be used.

preds &lt;- predict(ordforres, newdata=datatest)
preds
 
table(data.frame(true_values=datatest$Class, predictions=preds$ypred))

head(preds$classprobs)

## End(Not run)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
