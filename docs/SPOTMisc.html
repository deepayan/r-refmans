<!DOCTYPE html><html><head><title>Help for package SPOTMisc</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {SPOTMisc}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#active2All'><p>Active to all</p></a></li>
<li><a href='#dataCensusFull'><p>Full census data set</p></a></li>
<li><a href='#evalKerasGeneric'><p>evalKerasGeneric model building and compile</p></a></li>
<li><a href='#evalKerasMnist'><p>evalKerasMnist</p></a></li>
<li><a href='#evalKerasMnist_0'><p>evalKerasMnist_0</p></a></li>
<li><a href='#evalKerasTransferLearning'><p>evalKerasTransferLearning</p></a></li>
<li><a href='#evalParamCensus'><p>evaluate hyperparameter config on census data</p></a></li>
<li><a href='#funBBOBCall'><p>funBBOBCall</p></a></li>
<li><a href='#funKerasGeneric'><p>funKerasGeneric</p></a></li>
<li><a href='#funKerasMnist'><p>funKerasMnist</p></a></li>
<li><a href='#funKerasMnist_0'><p>funKerasMnist_0</p></a></li>
<li><a href='#funKerasTransferLearning'><p>funKerasTransferLearning</p></a></li>
<li><a href='#genCatsDogsData'><p>generate Cats Dogs Data</p></a></li>
<li><a href='#genericDataPrep'><p>Create an input pipeline using tfdatasets</p></a></li>
<li><a href='#getDataCensus'><p>Get Census KDD data set (+variation)</p></a></li>
<li><a href='#getExplan'><p>Get experimental design</p></a></li>
<li><a href='#getGenericTrainValTestData'><p>getGenericTrainValTestData</p></a></li>
<li><a href='#getIndices'><p>Get indices (positions) of variable names</p></a></li>
<li><a href='#getKerasConf'><p>Get keras configuration parameter list</p></a></li>
<li><a href='#getMlConfig'><p>get ml config for keras on census</p></a></li>
<li><a href='#getMlrResample'><p>Generate a fixed holdout instance for resampling</p></a></li>
<li><a href='#getMlrTask'><p>Generate an mlr task from Census KDD data set (+variation)</p></a></li>
<li><a href='#getMnistData'><p>getMnistData</p></a></li>
<li><a href='#getModelConf'><p>Get model configuration</p></a></li>
<li><a href='#getObjf'><p>Get objective function for mlr</p></a></li>
<li><a href='#getPredf'><p>Get predictions from mlr</p></a></li>
<li><a href='#getSimpleKerasModel'><p>getSimpleKerasModel</p></a></li>
<li><a href='#getVarNames'><p>Get variable names or subsets of variable names</p></a></li>
<li><a href='#ggparcoordPrepare'><p>Build data frame for ggparcoord (parallel plot)</p></a></li>
<li><a href='#ggplotProgress'><p>simple progress plot</p></a></li>
<li><a href='#int2fact'><p>Helper function: transform integer to factor</p></a></li>
<li><a href='#kerasBuildCompile'><p>evalKerasGeneric model building and compile</p></a></li>
<li><a href='#kerasCompileResult'><p>Generate result from keras run</p></a></li>
<li><a href='#kerasEvalPrediction'><p>Evaluate keras prediction</p></a></li>
<li><a href='#kerasFit'><p>kerasFit fit</p></a></li>
<li><a href='#kerasReturnDummy'><p>Return dummy values</p></a></li>
<li><a href='#makeLearnerFromHyperparameters'><p>Make mlr learner from conf and hyperparameter vector</p></a></li>
<li><a href='#mapX2FLAGS'><p>Map x parameters to a list of named values</p></a></li>
<li><a href='#MSE'><p>mean squared errors</p></a></li>
<li><a href='#optimizer_adadelta'><p>Adadelta optimizer.</p></a></li>
<li><a href='#optimizer_adagrad'><p>Adagrad optimizer</p></a></li>
<li><a href='#optimizer_adam'><p>Adam optimizer</p></a></li>
<li><a href='#optimizer_adamax'><p>Adamax optimizer</p></a></li>
<li><a href='#optimizer_nadam'><p>Nesterov Adam optimizer</p></a></li>
<li><a href='#optimizer_rmsprop'><p>RMSProp optimizer</p></a></li>
<li><a href='#optimizer_sgd'><p>Stochastic gradient descent (SGD) optimizer</p></a></li>
<li><a href='#plot_function_surface'><p>Surface plot</p></a></li>
<li><a href='#plot_parallel'><p>Parallel coordinate plot of a data set</p></a></li>
<li><a href='#plot_sensitivity'><p>Sensitivity plot of a model</p></a></li>
<li><a href='#plot_surface'><p>Surface plot of a model</p></a></li>
<li><a href='#plotnice.spotTreeModel'><p>Plot a nice rpart tree model</p></a></li>
<li><a href='#plotParallel'><p>Parallel coordinate plot of a data set</p></a></li>
<li><a href='#plotSensitivity'><p>Sensitivity ggplot of a model</p></a></li>
<li><a href='#predDlCensus'><p>Predict deep learning models on Census data</p></a></li>
<li><a href='#predMlCensus'><p>Predict machine learning models on Census data</p></a></li>
<li><a href='#prepare_data_plot'><p>Prepare data for plots</p></a></li>
<li><a href='#prepare_spot_result_plot'><p>Prepare data (results from a tuning run) for plots</p></a></li>
<li><a href='#prepareComparisonPlot'><p>prepare data frame for comparisons (boxplots, violin plots)</p></a></li>
<li><a href='#prepareProgressPlot'><p>prepare data frame for progress plot</p></a></li>
<li><a href='#printf'><p>formatted output</p></a></li>
<li><a href='#printFLAGS'><p>Print parameter values from FLAG list</p></a></li>
<li><a href='#resDl100'><p>Results from the spot() run dl100</p></a></li>
<li><a href='#RMSE'><p>root mean squared errors</p></a></li>
<li><a href='#scorePredictions'><p>Score results from pred</p></a></li>
<li><a href='#selectKerasActivation'><p>Select keras activation function</p></a></li>
<li><a href='#selectKerasOptimizer'><p>Select keras optimizer</p></a></li>
<li><a href='#selectTarget'><p>Select target variable in a data frame</p></a></li>
<li><a href='#sequentialBifurcation'><p>Sequential Bifurcation</p></a></li>
<li><a href='#spotKeras'><p>spotKEras</p></a></li>
<li><a href='#spotPlot'><p>spot plot (generic function)</p></a></li>
<li><a href='#SSE'><p>sum of squared errors</p></a></li>
<li><a href='#startCensusRun'><p>Start hyperparameter optimization runs with spot based on US census data</p></a></li>
<li><a href='#startMnistRun'><p>Start hyperparameter optimization runs with spot based on MNIST data</p></a></li>
<li><a href='#startXGBCensusRun'><p>Start hyperparameter optimization runs with spot based on US census data</p></a></li>
<li><a href='#subgroups'><p>Return effects for each subgroup</p></a></li>
<li><a href='#trans_10pow'><p>10 power x transformation</p></a></li>
<li><a href='#trans_10pow_round'><p>10 power x transformation with round</p></a></li>
<li><a href='#trans_1minus10pow'><p>10 power x transformation</p></a></li>
<li><a href='#trans_2pow'><p>2 power x transformation</p></a></li>
<li><a href='#trans_2pow_round'><p>2 power x transformation with round</p></a></li>
<li><a href='#trans_id'><p>Identity transformation</p></a></li>
<li><a href='#trans_mult2_round'><p>Mult 2 transformation</p></a></li>
<li><a href='#trans_odd_round'><p>odd transformation</p></a></li>
<li><a href='#translate_levels'><p>Helper function: translate levels</p></a></li>
<li><a href='#valid_inputs'><p>Check the validity of input parameters.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Misc Extensions for the 'SPOT' Package</td>
</tr>
<tr>
<td>Version:</td>
<td>1.19.52</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Thomas Bartz-Beielstein &lt;tbb@bartzundbartz.de&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Implements additional models, simulation tools, and interfaces as extensions to 'SPOT'.
    It provides tools for hyperparameter tuning via 'keras/tensorflow', interfacing 'mlr', for performing Markov chain simulations, 
    and for sensitivity analysis based on sequential bifurcation methods as described in Bettonvil and Kleijnen (1996). 
    Furthermore, additional plotting functions for output from 'SPOT' runs are implemented.
    Bartz-Beielstein T, Lasarczyk C W G, Preuss M (2005) &lt;<a href="https://doi.org/10.1109%2FCEC.2005.1554761">doi:10.1109/CEC.2005.1554761</a>&gt;.
    Bartz-Beielstein T, Zaefferer M, Rehbach F (2021) &lt;<a href="https://arxiv.org/abs/1712.04076">arXiv:1712.04076</a>&gt;.
    Bartz-Beielstein T, Rehbach F, Sen A, Zaefferer M &lt;<a href="https://arxiv.org/abs/2105.14625">arXiv:2105.14625</a>&gt;.
    Bettonvil, B, Kleijnen JPC (1996) &lt;<a href="https://doi.org/10.1016%2FS0377-2217%2896%2900156-7">doi:10.1016/S0377-2217(96)00156-7</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>yes</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>LazyDataCompression:</td>
<td>xz</td>
</tr>
<tr>
<td>Date:</td>
<td>2022-08-31</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.0.0)</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.1</td>
</tr>
<tr>
<td>Imports:</td>
<td>callr, dplyr, ggplot2, GGally, graphics, grDevices, keras,
magrittr, mlr, Metrics, plotly, RColorBrewer, reticulate,
rlang, rpart.plot, rsample, sensitivity, smoof, SPOT, stats,
tensorflow, tfdatasets, utils</td>
</tr>
<tr>
<td>Suggests:</td>
<td>farff, knitr, rmarkdown, rpart, testthat, xgboost</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://www.spotseven.de">https://www.spotseven.de</a></td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-08-31 18:55:18 UTC; bartz</td>
</tr>
<tr>
<td>Author:</td>
<td>Thomas Bartz-Beielstein
    <a href="https://orcid.org/0000-0002-5938-5158"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut, cre],
  Martin Zaefferer <a href="https://orcid.org/0000-0003-2372-2092"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut],
  Frederik Rehbach <a href="https://orcid.org/0000-0003-0922-8629"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-09-05 15:10:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='active2All'>Active to all</h2><span id='topic+active2All'></span>

<h3>Description</h3>

<p>Recreates the full set of parameters from the subset of active ones.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>active2All(x, a, model)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="active2All_+3A_x">x</code></td>
<td>
<p>subset of parameters</p>
</td></tr>
<tr><td><code id="active2All_+3A_a">a</code></td>
<td>
<p>names of the active parameters</p>
</td></tr>
<tr><td><code id="active2All_+3A_model">model</code></td>
<td>
<p>model (char)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>y full parameters
</p>


<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- "dl"
# indices of active variables
i &lt;- c(1,3)
# names of active variables
a &lt;- getVarNames(model=model,i=i)
x &lt;- getModelConf(model=model)$defaults
# now matrix x has only active variables 1 and 3:
x &lt;- x[1, getIndices(model=model, a=a), drop=FALSE]
# reconstruct the full set of parameters
active2All(x, a, model)
# 2nd example: new values to x (dropout=0.1, units=11):
x &lt;- matrix(c(0.1,11), nrow=1)
a &lt;- c("dropout", "units")
# reconstruct the full set of parameters
active2All(x, a, model)
# matrix
x &lt;- rbind(x,2*x)
active2All(x, a, model)


</code></pre>

<hr>
<h2 id='dataCensusFull'>Full census data set</h2><span id='topic+dataCensusFull'></span>

<h3>Description</h3>

<p>Census KDD Dataset (OpenML ID: 4535). Data frame with 299285 obs. of  42 variables, obtained via
<code>OpenML::getOMLDataSet(data.id=4535, cache.only=cache.only)$data)</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dataCensusFull
</code></pre>


<h3>Format</h3>

<p>A data frame with 42 variables.
</p>

<hr>
<h2 id='evalKerasGeneric'>evalKerasGeneric model building and compile</h2><span id='topic+evalKerasGeneric'></span>

<h3>Description</h3>

<p>Hyperparameter Tuning: Keras Generic Classification Function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>evalKerasGeneric(x = NULL, kerasConf = NULL, specList = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="evalKerasGeneric_+3A_x">x</code></td>
<td>
<p>matrix of transformed hyperparameter values to evaluate with the function.
If <code>NULL</code>, a simple keras model will be build, which is considered default
(see <code><a href="#topic+getSimpleKerasModel">getSimpleKerasModel</a></code>).</p>
</td></tr>
<tr><td><code id="evalKerasGeneric_+3A_kerasconf">kerasConf</code></td>
<td>
<p>List of additional parameters passed to keras as described in
<code><a href="#topic+getKerasConf">getKerasConf</a></code>. If no value is specified, stop() is called.</p>
</td></tr>
<tr><td><code id="evalKerasGeneric_+3A_speclist">specList</code></td>
<td>
<p>prepared data. See <code><a href="#topic+genericDataPrep">genericDataPrep</a></code>.
See <code><a href="#topic+getGenericTrainValTestData">getGenericTrainValTestData</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Trains a simple deep NN on a generic data set.
Standard Code from <a href="https://tensorflow.rstudio.com/">https://tensorflow.rstudio.com/</a>.
Modified by T. Bartz-Beielstein.
</p>


<h3>Value</h3>

<p>list with function values (training, validation, and test loss/accuracy,
and keras model information)
</p>


<h3>See Also</h3>

<p><code><a href="#topic+getKerasConf">getKerasConf</a></code>
</p>
<p><code><a href="#topic+funKerasGeneric">funKerasGeneric</a></code>
</p>
<p><code><a href="keras.html#topic+fit">fit</a></code>
</p>

<hr>
<h2 id='evalKerasMnist'>evalKerasMnist</h2><span id='topic+evalKerasMnist'></span>

<h3>Description</h3>

<p>Hyperparameter Tuning: Keras MNIST Classification Test Function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>evalKerasMnist(x, kerasConf, data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="evalKerasMnist_+3A_x">x</code></td>
<td>
<p>matrix of hyperparameter values to evaluate with the function.
Rows for points and columns for dimension.</p>
</td></tr>
<tr><td><code id="evalKerasMnist_+3A_kerasconf">kerasConf</code></td>
<td>
<p>List of additional parameters passed to keras as described in <code><a href="#topic+getKerasConf">getKerasConf</a></code>.
Default: <code>kerasConf = getKerasConf()</code>.</p>
</td></tr>
<tr><td><code id="evalKerasMnist_+3A_data">data</code></td>
<td>
<p>mnist data set. Default: <code><a href="#topic+getMnistData">getMnistData</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Trains a simple deep NN on the MNIST dataset.
Standard Code from https://tensorflow.rstudio.com/
Modified by T. Bartz-Beielstein (tbb@bartzundbartz.de)
</p>


<h3>Value</h3>

<p>list with function values (training, validation, and test loss/accuracy,
and keras model information)
</p>


<h3>See Also</h3>

<p><code><a href="#topic+getKerasConf">getKerasConf</a></code>
</p>
<p><code><a href="#topic+funKerasMnist">funKerasMnist</a></code>
</p>
<p><code><a href="keras.html#topic+fit">fit</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
### These examples require an activated Python environment as described in
### Bartz-Beielstein, T., Rehbach, F., Sen, A., and Zaefferer, M.:
### Surrogate Model Based Hyperparameter Tuning for Deep Learning with SPOT,
### June 2021. http://arxiv.org/abs/2105.14625.
PYTHON_RETICULATE &lt;- FALSE
if(PYTHON_RETICULATE){

library("SPOTMisc")
kerasConf &lt;- getKerasConf()
kerasConf$verbose &lt;- 1
kerasConf$model &lt;- "dl"
cfg &lt;-  getModelConf(kerasConf)
x &lt;- matrix(cfg$default, nrow=1)
if (length(cfg$transformations) &gt; 0) {  x &lt;- transformX(xNat=x, fn=cfg$transformations)}
res &lt;- evalKerasMnist(x, kerasConf, data = getMnistData(kerasConf))
#
kerasConf$model &lt;- "cnn"
kerasConf$encoding &lt;- "tensor"
cfg &lt;-  getModelConf(kerasConf)
x &lt;- matrix(cfg$default, nrow=1)
if (length(cfg$transformations) &gt; 0) {  x &lt;- transformX(xNat=x, fn=cfg$transformations)}
res &lt;- evalKerasMnist(x, kerasConf, data = getMnistData(kerasConf))
}

</code></pre>

<hr>
<h2 id='evalKerasMnist_0'>evalKerasMnist_0</h2><span id='topic+evalKerasMnist_0'></span>

<h3>Description</h3>

<p>Hyperparameter Tuning: Keras MNIST Classification Test Function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>evalKerasMnist_0(x, kerasConf = getKerasConf(), data = getMnistData())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="evalKerasMnist_0_+3A_x">x</code></td>
<td>
<p>matrix of hyperparameter values to evaluate with the function.
Rows for points and columns for dimension.</p>
</td></tr>
<tr><td><code id="evalKerasMnist_0_+3A_kerasconf">kerasConf</code></td>
<td>
<p>List of additional parameters passed to keras as described in <code><a href="#topic+getKerasConf">getKerasConf</a></code>.
Default: <code>kerasConf = getKerasConf()</code>.</p>
</td></tr>
<tr><td><code id="evalKerasMnist_0_+3A_data">data</code></td>
<td>
<p>mnist data set. Default: <code><a href="#topic+getMnistData">getMnistData</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Trains a simple deep NN on the MNIST dataset.
Standard Code from https://tensorflow.rstudio.com/
Modified by T. Bartz-Beielstein (tbb@bartzundbartz.de)
</p>


<h3>Value</h3>

<p>list with function values (training, validation, and test loss/accuracy,
and keras model information)
</p>


<h3>See Also</h3>

<p><code><a href="#topic+getKerasConf">getKerasConf</a></code>
</p>
<p><code><a href="#topic+funKerasMnist">funKerasMnist</a></code>
</p>
<p><code><a href="keras.html#topic+fit">fit</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
### These examples require an activated Python environment as described in
### Bartz-Beielstein, T., Rehbach, F., Sen, A., and Zaefferer, M.:
### Surrogate Model Based Hyperparameter Tuning for Deep Learning with SPOT,
### June 2021. http://arxiv.org/abs/2105.14625.
PYTHON_RETICULATE &lt;- FALSE
if(PYTHON_RETICULATE){

library("SPOTMisc")
kerasConf &lt;- getKerasConf()
kerasConf$verbose &lt;- 1
lower &lt;- c(1e-6, 1e-6, 16,0.6, 1e-9, 10, 6,0.4,0.99,1,1e-8)
upper &lt;- c(0.5, 0.5, 512, 1.5, 1e-2, 50, 10,0.999,0.999,10,6e-8)
types &lt;- c("numeric",  "numeric",  "integer",  "numeric",  "numeric",
           "integer",  "integer",  "numeric",  "numeric",  "integer",
           "numeric")

x &lt;- matrix(lower, 1,)
res &lt;- evalKerasMnist(x, kerasConf)
str(res)
### The number of units for all layers can be listed as follows:
res$modelConf$config$layers[,2]$units
}

</code></pre>

<hr>
<h2 id='evalKerasTransferLearning'>evalKerasTransferLearning</h2><span id='topic+evalKerasTransferLearning'></span>

<h3>Description</h3>

<p>Hyperparameter Tuning: Keras TransferLearning Test Function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>evalKerasTransferLearning(x, kerasConf = getKerasConf(), data = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="evalKerasTransferLearning_+3A_x">x</code></td>
<td>
<p>matrix of hyperparameter values to evaluate with the function.
Rows for points and columns for dimension.
<code>"dropout" =  x[1]</code>,
<code>"learning_rate" =  x[2]</code>,
<code>"epochs" = x[3]</code>,
<code>"beta_1" =  x[4]</code>,
<code>"beta_2" =  x[5]</code>,
<code>"epsilon" = x[6]</code>, and
<code>"optimizer" = x[7]</code> (type: factor).</p>
</td></tr>
<tr><td><code id="evalKerasTransferLearning_+3A_kerasconf">kerasConf</code></td>
<td>
<p>List of additional parameters passed to keras as described in <code><a href="#topic+getKerasConf">getKerasConf</a></code>.
Default: <code>kerasConf = getKerasConf()</code>.</p>
</td></tr>
<tr><td><code id="evalKerasTransferLearning_+3A_data">data</code></td>
<td>
<p>data</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Trains a transfer learning model.
Standard Code from https://tensorflow.rstudio.com/
Modified by T. Bartz-Beielstein (tbb@bartzundbartz.de)
</p>


<h3>Value</h3>

<p>list with function values (training, validation, and test loss/accuracy,
and keras model information)
</p>


<h3>See Also</h3>

<p><code><a href="#topic+getKerasConf">getKerasConf</a></code>
</p>
<p><code><a href="#topic+funKerasTransferLearning">funKerasTransferLearning</a></code>
</p>
<p><code><a href="#topic+funKerasMnist">funKerasMnist</a></code>
</p>
<p><code><a href="keras.html#topic+fit">fit</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
### These examples require an activated Python environment as described in
### Bartz-Beielstein, T., Rehbach, F., Sen, A., and Zaefferer, M.:
### Surrogate Model Based Hyperparameter Tuning for Deep Learning with SPOT,
### June 2021. http://arxiv.org/abs/2105.14625.
PYTHON_RETICULATE &lt;- FALSE
if(PYTHON_RETICULATE){

library("SPOTMisc")
lower &lt;- c(1e-6, 1e-6, 1, 0.6, 0.99, 1e-9, 1)
x &lt;- matrix(lower, 1,)
res &lt;- evalKerasTransferLearning(x,
                                 kerasConf = getKerasConf()
                                 )
str(res)
### The number of units for all layers can be listed as follows:
res$modelConf$config$layers[,2]$units
}

</code></pre>

<hr>
<h2 id='evalParamCensus'>evaluate hyperparameter config on census data</h2><span id='topic+evalParamCensus'></span>

<h3>Description</h3>

<p>evaluate hyperparameter config on census data
</p>


<h3>Usage</h3>

<pre><code class='language-R'>evalParamCensus(
  runNr = "00",
  model = "dl",
  xbest = "xBestOcba",
  k = 30,
  directory = "data",
  target = "age",
  cachedir = "oml.cache",
  task.type = "classif",
  nobs = 10000,
  nfactors = "high",
  nnumericals = "high",
  cardinality = "high",
  prop = 2/3,
  verbosity = 0
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="evalParamCensus_+3A_runnr">runNr</code></td>
<td>
<p>run number (character)</p>
</td></tr>
<tr><td><code id="evalParamCensus_+3A_model">model</code></td>
<td>
<p>ml/dl model (character)</p>
</td></tr>
<tr><td><code id="evalParamCensus_+3A_xbest">xbest</code></td>
<td>
<p>best value, e.g., &quot;xBestOcba&quot; or &quot;xbest&quot;</p>
</td></tr>
<tr><td><code id="evalParamCensus_+3A_k">k</code></td>
<td>
<p>number of repeats (integer)</p>
</td></tr>
<tr><td><code id="evalParamCensus_+3A_directory">directory</code></td>
<td>
<p>location of the (non-default, e.g., tuned) parameter file</p>
</td></tr>
<tr><td><code id="evalParamCensus_+3A_target">target</code></td>
<td>
<p>&quot;age&quot; or &quot;income_class&quot;</p>
</td></tr>
<tr><td><code id="evalParamCensus_+3A_cachedir">cachedir</code></td>
<td>
<p>cache dir</p>
</td></tr>
<tr><td><code id="evalParamCensus_+3A_task.type">task.type</code></td>
<td>
<p>task type: &quot;classif&quot; or &quot;regression&quot;</p>
</td></tr>
<tr><td><code id="evalParamCensus_+3A_nobs">nobs</code></td>
<td>
<p>number of observations</p>
</td></tr>
<tr><td><code id="evalParamCensus_+3A_nfactors">nfactors</code></td>
<td>
<p>factors, e.g., &quot;high&quot;</p>
</td></tr>
<tr><td><code id="evalParamCensus_+3A_nnumericals">nnumericals</code></td>
<td>
<p>numericals</p>
</td></tr>
<tr><td><code id="evalParamCensus_+3A_cardinality">cardinality</code></td>
<td>
<p>cardinality</p>
</td></tr>
<tr><td><code id="evalParamCensus_+3A_prop">prop</code></td>
<td>
<p>proportion. Default: <code>2/3</code></p>
</td></tr>
<tr><td><code id="evalParamCensus_+3A_verbosity">verbosity</code></td>
<td>
<p>verbosity level (0 or 1)</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
### These examples require an activated Python environment as described in
### Bartz-Beielstein, T., Rehbach, F., Sen, A., and Zaefferer, M.:
### Surrogate Model Based Hyperparameter Tuning for Deep Learning with SPOT,
### June 2021. http://arxiv.org/abs/2105.14625.
PYTHON_RETICULATE &lt;- FALSE
if(PYTHON_RETICULATE){
## The following code was used to evaluate the results in the book
## "Hyperparameter Tuning for Machine and Deep Learning with R - A Practical Guide"
## by Bartz, Bartz-Beielstein, Zaefferer, Mersmann:
##
modelList &lt;- list("dl", "cvglmnet",  "kknn", "ranger", "rpart" , "svm", "xgboost")
runNr &lt;- list("100", "Default")
directory &lt;- "../book/data"
for (model in modelList){
  for (run in runNr){    score &lt;- evalParamCensus(model = model,
                               runNr = run,
                               directory = directory,
                               prop=2/3,
                               k=30)
fileName &lt;- paste0(directory, "/", model, run, "Evaluation.RData")
save(score, file = fileName)
 }}
}
</code></pre>

<hr>
<h2 id='funBBOBCall'>funBBOBCall</h2><span id='topic+funBBOBCall'></span>

<h3>Description</h3>

<p>Call (external) BBOB Function.
Call the generator <code><a href="smoof.html#topic+makeBBOBFunction">makeBBOBFunction</a></code> for the
noiseless function set of the real-parameter
Black-Box Optimization Benchmarking (BBOB).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>funBBOBCall(x, opt = list(), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="funBBOBCall_+3A_x">x</code></td>
<td>
<p>matrix of points to evaluate with the function.
Rows for points and columns for dimension.</p>
</td></tr>
<tr><td><code id="funBBOBCall_+3A_opt">opt</code></td>
<td>
<p>list with the following entries
</p>

<dl>
<dt><code>dimensions</code></dt><dd><p>[integer(1)] Problem dimension. Integer value between 2 and 40.</p>
</dd>
<dt><code>fid</code></dt><dd><p>[integer(1)] Function identifier. Integer value between 1 and 24.</p>
</dd>
<dt><code>iid</code></dt><dd><p>[integer(1)] Instance identifier. Integer value greater than or equal 1.</p>
</dd>
</dl>
</td></tr>
<tr><td><code id="funBBOBCall_+3A_...">...</code></td>
<td>
<p>further arguments</p>
</td></tr>
</table>


<h3>Value</h3>

<p>1-column matrix with resulting function values
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Call the first instance of the 2D Sphere function
require("smoof")
require("SPOT")
set.seed(123)
x &lt;- matrix(c(1,2),1,2)
funBBOBCall(x, opt = list(dimensions = 2L, fid = 1L, iid =1L))
spot(x=NULL, funBBOBCall,
       lower = c(-2,-3), upper = c(1,2),
       control=list(funEvals=15),
       opt = list(dimensions = 2L, fid = 1L, iid = 1L ))
       
</code></pre>

<hr>
<h2 id='funKerasGeneric'>funKerasGeneric</h2><span id='topic+funKerasGeneric'></span>

<h3>Description</h3>

<p>Hyperparameter Tuning: Generic Classification Objective Function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>funKerasGeneric(x, kerasConf = NULL, specList = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="funKerasGeneric_+3A_x">x</code></td>
<td>
<p>matrix of hyperparameter values to evaluate with the function.
Rows for points and columns for dimension.</p>
</td></tr>
<tr><td><code id="funKerasGeneric_+3A_kerasconf">kerasConf</code></td>
<td>
<p>List of additional parameters passed to keras as described in <code><a href="#topic+getKerasConf">getKerasConf</a></code>.
Default: <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="funKerasGeneric_+3A_speclist">specList</code></td>
<td>
<p>prepared data. See <code><a href="#topic+genericDataPrep">genericDataPrep</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Trains a simple deep NN on arbitrary data sets.
Provides a template that can be used for other networks as well.
Standard Code from <a href="https://tensorflow.rstudio.com/">https://tensorflow.rstudio.com/</a>
Modified by T. Bartz-Beielstein (tbb@bartzundbartz.de)
</p>
<p>Note: The WARNING &quot;tensorflow:Layers in a Sequential model should only have a single input tensor.
Consider rewriting this model with the Functional API&quot;
can be safely ignored:
in general, Keras encourages its users to use functional models
for multi-input layers, but there is nothing wrong with doing so.
See: <a href="https://github.com/tensorflow/recommenders/issues/188">https://github.com/tensorflow/recommenders/issues/188</a>.
</p>


<h3>Value</h3>

<p>1-column matrix with resulting function values (test loss)
</p>


<h3>See Also</h3>

<p><code><a href="#topic+getKerasConf">getKerasConf</a></code>
</p>
<p><code><a href="#topic+evalKerasGeneric">evalKerasGeneric</a></code>
</p>
<p><code><a href="#topic+evalKerasGeneric">evalKerasGeneric</a></code>
</p>
<p><code><a href="keras.html#topic+fit">fit</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
### These examples require an activated Python environment as described in
### Bartz-Beielstein, T., Rehbach, F., Sen, A., and Zaefferer, M.:
### Surrogate Model Based Hyperparameter Tuning for Deep Learning with SPOT,
### June 2021. http://arxiv.org/abs/2105.14625.
PYTHON_RETICULATE &lt;- FALSE
if(PYTHON_RETICULATE){

## data preparation
target &lt;- "age"
batch_size &lt;- 32
prop &lt;- 2/3
dfGeneric &lt;- getDataCensus(target = target, nobs = 1000)
data &lt;- getGenericTrainValTestData(dfGeneric = dfGeneric, prop = prop)
specList &lt;- genericDataPrep(data=data, batch_size = batch_size)

## model configuration:
cfg &lt;-  getModelConf(list(model="dl"))
x &lt;- matrix(cfg$default, nrow=1)
transformFun &lt;- cfg$transformations
types &lt;- cfg$type
lower &lt;- cfg$lower
upper &lt;- cfg$upper

kerasConf &lt;- getKerasConf()

### First example: simple function call:
message("objectiveFunctionEvaluation(): x before transformX().")
print(x)
if (length(transformFun) &gt; 0) {  x &lt;- transformX(xNat=x, fn=transformFun)}
message("objectiveFunctionEvaluation(): x after transformX().")
print(x)
funKerasGeneric(x, kerasConf = kerasConf, specList = specList)

### Second example: evaluation of several (three) hyperparameter settings:
xxx &lt;- rbind(x,x,x)
funKerasGeneric(xxx, kerasConf = kerasConf, specList)

### Third example: spot call with extended verbosity:
res &lt;- spot(x = NULL,
            fun = funKerasGeneric,
            lower = lower,
            upper = upper,
            control = list(funEvals=50,
                         handleNAsMethod = handleNAsMean,
                         noise = TRUE,
                         types = types,
                         plots = TRUE,
                         progress = TRUE,
                         seedFun = 1,
                         seedSPOT = 1,
                         transformFun=transformFun),
                         kerasConf = kerasConf,
                         specList = specList)
  }


</code></pre>

<hr>
<h2 id='funKerasMnist'>funKerasMnist</h2><span id='topic+funKerasMnist'></span>

<h3>Description</h3>

<p>Hyperparameter Tuning: Keras MNIST Classification Test Function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>funKerasMnist(x, kerasConf, data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="funKerasMnist_+3A_x">x</code></td>
<td>
<p>matrix of hyperparameter values to evaluate with the function.
Rows for points and columns for dimension.</p>
</td></tr>
<tr><td><code id="funKerasMnist_+3A_kerasconf">kerasConf</code></td>
<td>
<p>List of additional parameters passed to keras as described in <code><a href="#topic+getKerasConf">getKerasConf</a></code>.
Default: <code>kerasConf = getKerasConf()</code>.</p>
</td></tr>
<tr><td><code id="funKerasMnist_+3A_data">data</code></td>
<td>
<p>mnist data set. Default: <code><a href="#topic+getMnistData">getMnistData</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Trains a simple deep NN on the MNIST dataset.
Provides a template that can be used for other networks as well.
Standard Code from https://tensorflow.rstudio.com/
Modified by T. Bartz-Beielstein (tbb@bartzundbartz.de)
</p>


<h3>Value</h3>

<p>1-column matrix with resulting function values (test loss)
</p>


<h3>See Also</h3>

<p><code><a href="#topic+getKerasConf">getKerasConf</a></code>
</p>
<p><code><a href="#topic+evalKerasMnist">evalKerasMnist</a></code>
</p>
<p><code><a href="keras.html#topic+fit">fit</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
### These examples require an activated Python environment as described in
### Bartz-Beielstein, T., Rehbach, F., Sen, A., and Zaefferer, M.:
### Surrogate Model Based Hyperparameter Tuning for Deep Learning with SPOT,
### June 2021. http://arxiv.org/abs/2105.14625.
PYTHON_RETICULATE &lt;- FALSE
if(PYTHON_RETICULATE){

library("SPOTMisc")
library("SPOT")
kerasConf &lt;- getKerasConf()
## The following two settings are default:
kerasConf$encoding = "oneHot"
kerasConf$model = "dl"
## get the data with the correct encoding
mnist &lt;- getMnistData(kerasConf)
## get the model
cfg &lt;-  getModelConf(kerasConf)

### First example: simple function call:
x &lt;- matrix(cfg$default, nrow=1)
if (length(cfg$transformations) &gt; 0) {  x &lt;- transformX(xNat=x, fn=cfg$transformations)}
funKerasMnist(x, kerasConf = kerasConf, data = mnist)
### Use convnet:
kerasConf &lt;- getKerasConf()
kerasConf$encoding &lt;- "tensor"
kerasConf$model &lt;- "cnn"
mnist &lt;- getMnistData(kerasConf)
cfg &lt;-  getModelConf(kerasConf)
x &lt;- matrix(cfg$default, nrow=1)
if (length(cfg$transformations) &gt; 0) {  x &lt;- transformX(xNat=x, fn=cfg$transformations)}
funKerasMnist(x, kerasConf = kerasConf, data=mnist)

### Second example: evaluation of several (three) hyperparameter settings:
x &lt;- matrix(cfg$default, nrow=1)
if (length(cfg$transformations) &gt; 0) {  x &lt;- transformX(xNat=x, fn=cfg$transformations)}
xxx &lt;- rbind(x,x,x)
funKerasMnist(xxx, kerasConf = kerasConf, data=mnist)

### Third example: spot call (dense network):
kerasConf &lt;- getKerasConf()
kerasConf$verbose &lt;- 0
kerasConf$encoding = "oneHot"
kerasConf$model = "dl"
## get the data with the correct encoding
mnist &lt;- getMnistData(kerasConf)
## get the model
cfg &lt;-  getModelConf(kerasConf)
## max 32 training epochs
cfg$upper[6] &lt;- 5
resDl &lt;- spot(x = NULL,
            fun = funKerasMnist,
            lower = cfg$lower,
            upper = cfg$upper,
            control = list(funEvals=15,
                         transformFun = cfg$transformations,
                         types = cfg$type,
                         noise = TRUE,
                         plots = TRUE,
                         progress = TRUE,
                         seedFun = 1,
                         seedSPOT = 1),
                         kerasConf = kerasConf,
                         data = mnist)

### Fourth example: spot call (convnet):
kerasConf &lt;- getKerasConf()
kerasConf$verbose &lt;- 1
kerasConf$encoding &lt;- "tensor"
kerasConf$model &lt;- "cnn"
## get the data with the correct encoding
mnist &lt;- getMnistData(kerasConf)
## get the model
cfg &lt;-  getModelConf(kerasConf)
## max 32 training epochs
cfg$upper[6] &lt;- 5
resCnn &lt;- spot(x = NULL,
            fun = funKerasMnist,
            lower = cfg$lower,
            upper = cfg$upper,
            control = list(funEvals=15,
                         transformFun = cfg$transformations,
                         types = cfg$type,
                         noise = TRUE,
                         plots = TRUE,
                         progress = TRUE,
                         seedFun = 1,
                         seedSPOT = 1),
                         kerasConf = kerasConf,
                         data = mnist)
}


</code></pre>

<hr>
<h2 id='funKerasMnist_0'>funKerasMnist_0</h2><span id='topic+funKerasMnist_0'></span>

<h3>Description</h3>

<p>Hyperparameter Tuning: Keras MNIST Classification Test Function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>funKerasMnist_0(x, kerasConf, data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="funKerasMnist_0_+3A_x">x</code></td>
<td>
<p>matrix of hyperparameter values to evaluate with the function.
Rows for points and columns for dimension.</p>
</td></tr>
<tr><td><code id="funKerasMnist_0_+3A_kerasconf">kerasConf</code></td>
<td>
<p>List of additional parameters passed to keras as described in <code><a href="#topic+getKerasConf">getKerasConf</a></code>.
Default: <code>kerasConf = getKerasConf()</code>.</p>
</td></tr>
<tr><td><code id="funKerasMnist_0_+3A_data">data</code></td>
<td>
<p>mnist data set. Default: <code><a href="#topic+getMnistData">getMnistData</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Trains a simple deep NN on the MNIST dataset.
Provides a template that can be used for other networks as well.
Standard Code from https://tensorflow.rstudio.com/
Modified by T. Bartz-Beielstein (tbb@bartzundbartz.de)
</p>


<h3>Value</h3>

<p>1-column matrix with resulting function values (test loss)
</p>


<h3>See Also</h3>

<p><code><a href="#topic+getKerasConf">getKerasConf</a></code>
</p>
<p><code><a href="#topic+evalKerasMnist">evalKerasMnist</a></code>
</p>
<p><code><a href="keras.html#topic+fit">fit</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
### These examples require an activated Python environment as described in
### Bartz-Beielstein, T., Rehbach, F., Sen, A., and Zaefferer, M.:
### Surrogate Model Based Hyperparameter Tuning for Deep Learning with SPOT,
### June 2021. http://arxiv.org/abs/2105.14625.
PYTHON_RETICULATE &lt;- FALSE
if(PYTHON_RETICULATE){

library("SPOTMisc")
library("SPOT")
kerasConf &lt;- getKerasConf()
## The following two settings are default:
kerasConf$encoding = "oneHot"
kerasConf$model = "dl"
cfg &lt;-  getModelConf(kerasConf$model)
x &lt;- matrix(cfg$default, nrow=1)
transformFun &lt;- cfg$transformations
types &lt;- cfg$type
lower &lt;- cfg$lower
upper &lt;- cfg$upper

### First example: simple function call:
x &lt;- matrix(lower, 1,)
funKerasMnist(x, kerasConf = kerasConf)
### Use convnet:
kerasConf$encoding &lt;- "tensor"
kerasConf$model &lt;- "cnn"
funKerasMnist(x, kerasConf = kerasConf)

### Second example: evaluation of several (three) hyperparameter settings:
xxx &lt;- rbind(x,x,x)
funKerasMnist(xxx, kerasConf = kerasConf)

### Third example: spot call (dense network):
kerasConf$verbose &lt;- 1
data &lt;- getMnistData()
res &lt;- spot(x = NULL,
            fun = funKerasMnist,
            lower = lower,
            upper = upper,
            control = list(funEvals=15,
                         noise = TRUE,
                         types = types,
                         plots = TRUE,
                         progress = TRUE,
                         seedFun = 1,
                         seedSPOT = 1),
                         kerasConf = kerasConf,
                         data = data)

### Fourth example: spot call (convnet):
kerasConf$verbose &lt;- 1
kerasConf$encoding &lt;- "tensor"
kerasConf$model &lt;- "cnn"
data &lt;- getMnistData(kerasConf)
res &lt;- spot(x = NULL,
            fun = funKerasMnist,
            lower = lower,
            upper = upper,
            control = list(funEvals=15,
                         noise = TRUE,
                         types = types,
                         plots = TRUE,
                         progress = TRUE,
                         seedFun = 1,
                         seedSPOT = 1),
                         kerasConf = kerasConf,
                         data = data)
  }


</code></pre>

<hr>
<h2 id='funKerasTransferLearning'>funKerasTransferLearning</h2><span id='topic+funKerasTransferLearning'></span>

<h3>Description</h3>

<p>Hyperparameter Tuning: Keras TransfewrLearning Test Function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>funKerasTransferLearning(x, kerasConf = getKerasConf(), data = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="funKerasTransferLearning_+3A_x">x</code></td>
<td>
<p>matrix of hyperparameter values to evaluate with the function.
Rows for points and columns for dimension.</p>
</td></tr>
<tr><td><code id="funKerasTransferLearning_+3A_kerasconf">kerasConf</code></td>
<td>
<p>List of additional parameters passed to keras as described in <code><a href="#topic+getKerasConf">getKerasConf</a></code>.
Default: <code>kerasConf = getKerasConf()</code>.</p>
</td></tr>
<tr><td><code id="funKerasTransferLearning_+3A_data">data</code></td>
<td>
<p>data</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Trains a simple deep NN on the MNIST dataset.
Provides a template that can be used for other networks as well.
Standard Code from https://tensorflow.rstudio.com/
Modified by T. Bartz-Beielstein (tbb@bartzundbartz.de)
</p>


<h3>Value</h3>

<p>1-column matrix with resulting function values (test loss).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+getKerasConf">getKerasConf</a></code>
</p>
<p><code><a href="#topic+evalKerasTransferLearning">evalKerasTransferLearning</a></code>
</p>
<p><code><a href="#topic+evalKerasMnist">evalKerasMnist</a></code>
</p>
<p><code><a href="keras.html#topic+fit">fit</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
### These examples require an activated Python environment as described in
### Bartz-Beielstein, T., Rehbach, F., Sen, A., and Zaefferer, M.:
### Surrogate Model Based Hyperparameter Tuning for Deep Learning with SPOT,
### June 2021. http://arxiv.org/abs/2105.14625.

PYTHON_RETICULATE &lt;- FALSE
if(PYTHON_RETICULATE){
library("SPOTMisc")
library("SPOT")
kerasConf &lt;- getKerasConf()

 # Hyperparameters:
 # "dropout" =  x[1],
 # "learning_rate" =  x[2],
 # "epochs" = x[3],
 # "beta_1" =  x[4],
 # "beta_2" =  x[5],
 # "epsilon" = x[6],
 # "optimizer" = x[7]

lower &lt;- c(1e-6, 1e-6, 2, 0.8, 0.8, 1e-9, 1)
upper &lt;- c(0.2, 1e-2, 5, 0.99, 0.9999, 1e-3, 2)
types &lt;- c("numeric",  "numeric",  "integer",  "numeric",  "numeric",
           "integer",  "factor")

## First Example: spot call with extended verbosity. Default objective
## "validationLoss", i.e., validation loss, is used. only 20 function
## evaluations (for testing).
kerasConf$verbose &lt;- 1
res &lt;- spot(x = NULL,
            fun = funKerasTransferLearning,
            lower = lower,
            upper = upper,
            control = list(funEvals=20,
                           model=buildKriging,
                           noise = TRUE,
                           types = types,
                           optimizer=optimDE,
                           plots = TRUE,
                           progress = TRUE,
                           seedFun = 1,
                           seedSPOT = 1,
                           kerasConf = kerasConf)
                           )
 save(res, file = paste0("resKerasTransferLearning", as.numeric(Sys.time()),".RData"))


 ## Example: resKerasTransferLearning04
 ## Default objective function "validationLoss", i.e.,
 ## training loss
library("SPOTMisc")
library("SPOT")
kerasConf &lt;- getKerasConf()

# Hyperparameters:
# "dropout" =  x[1],
# "learning_rate" =  x[2],
# "epochs" = x[3],
# "beta_1" =  x[4],
# "beta_2" =  x[5],
# "epsilon" = x[6],
# "optimizer" = x[7]

lower &lt;- c(1e-6, 1e-6, 2, 0.8, 0.8, 1e-9, 1)
upper &lt;- c(0.2, 1e-2, 5, 0.99, 0.9999, 1e-3, 2)
types &lt;- c("numeric",  "numeric",  "integer",  "numeric",  "numeric",
           "integer",  "factor")

res &lt;- spot(x = NULL,
            fun = funKerasTransferLearning,
            lower = lower,
            upper = upper,
            control = list(funEvals=100,
                           model=buildKriging,
                           noise = TRUE,
                           types = types,
                           optimizer=optimDE,
                           plots = FALSE,
                           progress = TRUE,
                           seedFun = 1,
                           seedSPOT = 1,
                           kerasConf = kerasConf))
save(res,file = paste0("resKerasTransferLearningValidationLoss04",
as.numeric(Sys.time()),".RData"))



 ## Example: resKerasTransferLearning05
 ## objective function "negValidationAccuracy", i.e.,
 ## negative validation accuracy
library("SPOTMisc")
library("SPOT")
kerasConf &lt;- getKerasConf()

# Hyperparameters:
# "dropout" =  x[1],
# "learning_rate" =  x[2],
# "epochs" = x[3],
# "beta_1" =  x[4],
# "beta_2" =  x[5],
# "epsilon" = x[6],
# "optimizer" = x[7]

lower &lt;- c(1e-6, 1e-6, 2, 0.8, 0.8, 1e-9, 1)
upper &lt;- c(0.2, 1e-2, 5, 0.99, 0.9999, 1e-3, 2)
types &lt;- c("numeric",  "numeric",  "integer",  "numeric",  "numeric",
           "integer",  "factor")

kerasConf$returnValue &lt;- "negValidationAccuracy"
res &lt;- spot(x = NULL,
            fun = funKerasTransferLearning,
            lower = lower,
            upper = upper,
            control = list(funEvals=100,
                           model=buildKriging,
                           noise = TRUE,
                           types = types,
                           optimizer=optimDE,
                           plots = FALSE,
                           progress = TRUE,
                           seedFun = 1,
                           seedSPOT = 1,
                           kerasConf = kerasConf))
save(res,file = paste0("resKerasTransferLearningNegValidationAccuracy05",
as.numeric(Sys.time()),".RData"))


 ## Example: resKerasTransferLearning06
 ## objective function "trainingLoss", i.e.,
 ## training loss

library("SPOTMisc")
library("SPOT")
kerasConf &lt;- getKerasConf()

# Hyperparameters:
# "dropout" =  x[1],
# "learning_rate" =  x[2],
# "epochs" = x[3],
# "beta_1" =  x[4],
# "beta_2" =  x[5],
# "epsilon" = x[6],
# "optimizer" = x[7]

lower &lt;- c(1e-6, 1e-6, 2, 0.8, 0.8, 1e-9, 1)
upper &lt;- c(0.2, 1e-2, 5, 0.99, 0.9999, 1e-3, 2)
types &lt;- c("numeric",  "numeric",  "integer",  "numeric",  "numeric",
           "integer",  "factor")

kerasConf$returnValue &lt;- "trainingLoss"
res &lt;- spot(x = NULL,
            fun = funKerasTransferLearning,
            lower = lower,
            upper = upper,
            control = list(funEvals=100,
                           model=buildKriging,
                           noise = TRUE,
                           types = types,
                           optimizer=optimDE,
                           plots = FALSE,
                           progress = TRUE,
                           seedFun = 1,
                           seedSPOT = 1,
                           kerasConf = kerasConf)
)
save(res, file = paste0("resKerasTransferLearningTrainingLoss06",
as.numeric(Sys.time()),".RData"))
 }


</code></pre>

<hr>
<h2 id='genCatsDogsData'>generate Cats Dogs Data</h2><span id='topic+genCatsDogsData'></span>

<h3>Description</h3>

<p>Generate data for <code><a href="#topic+funKerasTransferLearning">funKerasTransferLearning</a></code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>genCatsDogsData(kerasConf = getKerasConf())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="genCatsDogsData_+3A_kerasconf">kerasConf</code></td>
<td>
<p>keras configuration. Default: <code>kerasConf = <a href="#topic+getKerasConf">getKerasConf</a></code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Standard Data from https://tensorflow.rstudio.com/
Modified by T. Bartz-Beielstein (tbb@bartzundbartz.de)
</p>


<h3>Value</h3>

<p>list with test, validation, and test data
</p>

<hr>
<h2 id='genericDataPrep'>Create an input pipeline using tfdatasets</h2><span id='topic+genericDataPrep'></span>

<h3>Description</h3>

<p>Create an input pipeline using tfdatasets
</p>


<h3>Usage</h3>

<pre><code class='language-R'>genericDataPrep(
  data,
  batch_size = 32,
  minLevelSizeEmbedding = 100,
  embeddingDim = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="genericDataPrep_+3A_data">data</code></td>
<td>
<p>data. List, e.g., df$trainCensus, df$testGeneric, and df$valCensus data)</p>
</td></tr>
<tr><td><code id="genericDataPrep_+3A_batch_size">batch_size</code></td>
<td>
<p>batch size. Default: 32</p>
</td></tr>
<tr><td><code id="genericDataPrep_+3A_minlevelsizeembedding">minLevelSizeEmbedding</code></td>
<td>
<p>integer. Embedding will be used for
factor variables with more than <code>minLevelSizeEmbedding</code> levels. Default: <code>100</code>.</p>
</td></tr>
<tr><td><code id="genericDataPrep_+3A_embeddingdim">embeddingDim</code></td>
<td>
<p>integer. Dimension used for embedding. Default: <code>floor(log(minLevelSizeEmbedding))</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a fitted <code>FeatureSpec</code> object and the hold-out testGeneric (=data$testGeneric).
This is returned as the follwoing list.
</p>

<dl>
<dt><code>train_ds_generic</code></dt><dd><p>train</p>
</dd>
<dt><code>val_ds_generic</code></dt><dd><p>validation</p>
</dd>
<dt><code>test_ds_generic</code></dt><dd><p>test</p>
</dd>
<dt><code>specGeneric_prep</code></dt><dd><p>feature spec object</p>
</dd>
<dt><code>testGeneric</code></dt><dd><p>data$testGeneric</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>
### These examples require an activated Python environment as described in
### Bartz-Beielstein, T., Rehbach, F., Sen, A., and Zaefferer, M.:
### Surrogate Model Based Hyperparameter Tuning for Deep Learning with SPOT,
### June 2021. http://arxiv.org/abs/2105.14625.
PYTHON_RETICULATE &lt;- FALSE
if(PYTHON_RETICULATE){
target &lt;- "age"
batch_size &lt;- 32
prop &lt;- 2/3
cachedir &lt;- "oml.cache"
dfCensus &lt;- getDataCensus(target = target,
nobs = 1000, cachedir = cachedir, cache.only=FALSE)
data &lt;- getGenericTrainValTestData(dfGeneric = dfCensus,
prop = prop)
specList &lt;- genericDataPrep(data=data, batch_size = batch_size)
## Call iterator:
require(magrittr)
specList$train_ds_generic %&gt;%
  reticulate::as_iterator() %&gt;%
   reticulate::iter_next()
}


</code></pre>

<hr>
<h2 id='getDataCensus'>Get Census KDD data set (+variation)</h2><span id='topic+getDataCensus'></span>

<h3>Description</h3>

<p>This function downloads (or loads from cache folder) the Census
KDD Dataset (OpenML ID: 4535).
If requested, data set is changed w.r.t the number of observations, number of
numerical/categorical feature,
the cardinality of the categorical features, and the task type (regr. or classif).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getDataCensus(
  task.type = "classif",
  nobs = 50000,
  nfactors = "high",
  nnumericals = "high",
  cardinality = "high",
  data.seed = 1,
  cachedir = "oml.cache",
  target = NULL,
  cache.only = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="getDataCensus_+3A_task.type">task.type</code></td>
<td>
<p>character, either &quot;classif&quot; or &quot;regr&quot;.</p>
</td></tr>
<tr><td><code id="getDataCensus_+3A_nobs">nobs</code></td>
<td>
<p>integer, number of observations uniformly sampled from the full data set.</p>
</td></tr>
<tr><td><code id="getDataCensus_+3A_nfactors">nfactors</code></td>
<td>
<p>character, controls the number of factors (categorical features) to use.
Can be &quot;low&quot;, &quot;med&quot;, &quot;high&quot;, or &quot;full&quot; (full corresponds to original data set).</p>
</td></tr>
<tr><td><code id="getDataCensus_+3A_nnumericals">nnumericals</code></td>
<td>
<p>character, controls the number of numerical features to use.
Can be &quot;low&quot;, &quot;med&quot;, &quot;high&quot;, or &quot;full&quot; (full corresponds to original data set).</p>
</td></tr>
<tr><td><code id="getDataCensus_+3A_cardinality">cardinality</code></td>
<td>
<p>character, controls the number of factor levels (categories)
for the categorical features. Can be &quot;low&quot;, &quot;med&quot;, &quot;high&quot; (high corresponds to original data set).</p>
</td></tr>
<tr><td><code id="getDataCensus_+3A_data.seed">data.seed</code></td>
<td>
<p>integer, this will be used via set.seed() to make the random subsampling reproducible.
Will not have an effect if all observations are used.</p>
</td></tr>
<tr><td><code id="getDataCensus_+3A_cachedir">cachedir</code></td>
<td>
<p>character. The cache directory, e.g., <code>"oml.cache"</code>.
Default: <code>"oml.cache"</code>.</p>
</td></tr>
<tr><td><code id="getDataCensus_+3A_target">target</code></td>
<td>
<p>character &quot;age&quot; or &quot;income_class&quot;. If <code>target = age</code>, the
numerical varible <code>age</code> is converted to a factor:
<code>age&lt;-as.factor(age&lt;40)</code></p>
</td></tr>
<tr><td><code id="getDataCensus_+3A_cache.only">cache.only</code></td>
<td>
<p>logical. Only try to retrieve the object from cache.
Will result in error if the object is not found. Default is TRUE.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>census data set
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Example downloads OpenML data, might take some time:
task.type &lt;- "classif"
nobs &lt;- 1e4 # max: 229285
data.seed &lt;- 1
nfactors &lt;- "full"
nnumericals &lt;- "low"
cardinality &lt;- "med"
censusData &lt;- getDataCensus(
  task.type = task.type,
  nobs = nobs,
  nfactors = nfactors,
  nnumericals = nnumericals,
  cardinality = cardinality,
  data.seed = data.seed,
  cachedir = "oml.cache",
  target="age")
  

</code></pre>

<hr>
<h2 id='getExplan'>Get experimental design</h2><span id='topic+getExplan'></span>

<h3>Description</h3>

<p>Based on <code><a href="base.html#topic+expand.grid">expand.grid</a></code> an experimental design
is generated. Factors are: nfactors, nnumericals, cardinality, data.seed etc.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getExplan()
</code></pre>


<h3>Value</h3>

<p>explan
</p>

<hr>
<h2 id='getGenericTrainValTestData'>getGenericTrainValTestData</h2><span id='topic+getGenericTrainValTestData'></span>

<h3>Description</h3>

<p>getGenericTrainValTestData
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getGenericTrainValTestData(dfGeneric = NULL, prop = 0.5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="getGenericTrainValTestData_+3A_dfgeneric">dfGeneric</code></td>
<td>
<p>data, e.g.,  obtained with <code><a href="#topic+getDataCensus">getDataCensus</a></code>. Default: <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="getGenericTrainValTestData_+3A_prop">prop</code></td>
<td>
<p>vector. proportion between train / test and train/val. Default: <code>2/3</code>. If one
value is given, the same proportion will be used for both split. Otherwise, the first
entry is used for the test/training split and the second value for the training/validation
split. If the second value is 1, the validation set is empty.
Given <code>prop = (p1,p2)</code>, the data will be partitioned as shown in the following two steps:
</p>

<dl>
<dt>Step 1:</dt><dd><p><code>train1 = p1*data</code> and <code>test = )(1-p1)*data</code></p>
</dd>
<dt>Step 2:</dt><dd><p><code>train2 = p2*train1 = p2*p1*data</code> and <code>val = )(1-p2)*train1 = (1-p2)*p1*data</code></p>
</dd>
</dl>
</td></tr>
</table>


<h3>Value</h3>

<p>list with training, validation and test data: trainCensus, valCensus, testCensus.
</p>


<h3>Note</h3>

<p>If <code>p2=1</code>, no validation data will be generated.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+getKerasConf">getKerasConf</a></code>
</p>
<p><code><a href="#topic+funKerasGeneric">funKerasGeneric</a></code>
</p>
<p><code><a href="#topic+getDataCensus">getDataCensus</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
### These examples require an activated Python environment as described in
### Bartz-Beielstein, T., Rehbach, F., Sen, A., and Zaefferer, M.:
### Surrogate Model Based Hyperparameter Tuning for Deep Learning with SPOT,
### June 2021. http://arxiv.org/abs/2105.14625.
PYTHON_RETICULATE &lt;- FALSE
if(PYTHON_RETICULATE){
task.type &lt;- "classif"
nobs &lt;- 1e4
nfactors &lt;- "high"
nnumericals &lt;- "high"
cardinality &lt;- "high"
data.seed &lt;- 1
cachedir &lt;- "oml.cache"
target = "age"
prop &lt;- 2 / 3
dfCensus &lt;- getDataCensus(task.type = task.type,
nobs = nobs, nfactors = nfactors,
nnumericals = nnumericals, cardinality = cardinality,
data.seed = data.seed, cachedir = cachedir,
target = target)
census &lt;- getGenericTrainValTestData(dfGeneric=dfCensus,
prop = prop)
## train data size is 2/3*2/3*10000:
dim(census$trainGeneric)
}

</code></pre>

<hr>
<h2 id='getIndices'>Get indices (positions) of variable names</h2><span id='topic+getIndices'></span>

<h3>Description</h3>

<p>Get indices (positions) of variable names
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getIndices(model, a)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="getIndices_+3A_model">model</code></td>
<td>
<p>from <code><a href="#topic+getModelConf">getModelConf</a></code>, e.g., <code>"dl"</code>.</p>
</td></tr>
<tr><td><code id="getIndices_+3A_a">a</code></td>
<td>
<p>name of variables</p>
</td></tr>
</table>


<h3>Value</h3>

<p>indices of variable names.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>getIndices(model="dl",
           a = c("dropout", "units"))

</code></pre>

<hr>
<h2 id='getKerasConf'>Get keras configuration parameter list</h2><span id='topic+getKerasConf'></span>

<h3>Description</h3>

<p>Configuration list for <code>keras</code>'s <code><a href="keras.html#topic+fit">fit</a></code> function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getKerasConf()
</code></pre>


<h3>Details</h3>

<p>Additional parameters passed to <code>keras</code>, e.g.,
</p>

<dl>
<dt><code>activation:</code></dt><dd><p>character. Activation function in the last layer. Default: <code>"sigmoid"</code>.</p>
</dd>
<dt><code>active:</code></dt><dd><p>vector of active varaibles, e.g., c(1,10) specifies that the first and tenth variable will be considerer by spot.</p>
</dd>
<dt><code>callbacks:</code></dt><dd><p>List of callbacks to be called during training. Default: <code>list()</code>.</p>
</dd>
<dt><code>clearSession:</code></dt><dd><p>logical. Whether to call <code><a href="keras.html#topic+k_clear_session">k_clear_session</a></code> or not at the end of keras modelling. Default: <code>FALSE</code>.</p>
</dd>
<dt><code>encoding:</code></dt><dd><p>character. Encoding used during data preparation, e.g., by <code><a href="#topic+getMnistData">getMnistData</a></code>. Default: <code>"oneHot"</code>.</p>
</dd>
<dt><code>loss:</code></dt><dd><p>character. Loss function for compile. Default: <code>"loss_binary_crossentropy"</code>.</p>
</dd>
<dt><code>metrics:</code></dt><dd><p>character. Metrics function for compile. Default: <code>"binary_accuracy"</code>.</p>
</dd>
<dt><code>model:</code></dt><dd><p>model specified via <code><a href="#topic+getModelConf">getModelConf</a></code>. Default: <code>"dl"</code>.</p>
</dd>
<dt><code>nClasses:</code></dt><dd><p>Number of classes in (multi-class) classification. Specifies the number of units in the last layer (before softmax).
Default: <code>1</code> (binary classification).</p>
</dd>
<dt><code>resDummy:</code></dt><dd><p>logical. If <code>TRUE</code>, generate dummy (mock up) result for testing. If <code>FALSE</code>, run keras and tf evaluations.
Default: <code>FALSE</code>.</p>
</dd>
<dt><code>returnValue:</code></dt><dd><p>Return value. Can be one of <code>"trainingLoss"</code>, <code>"negTrainingAccuracy"</code>,
<code>"validationLoss"</code>, <code>"negValidationAccuracy"</code>, <code>"testLoss"</code>, or <code>"negTestAccuracy"</code>.</p>
</dd>
<dt><code>returnObject:</code></dt><dd><p>Return object. Can be one of <code>"evaluation"</code>, <code>"model"</code>,
<code>"pred"</code>.	Default: <code>"evaluation"</code>.</p>
</dd>
<dt><code>shuffle:</code></dt><dd><p>Logical (whether to shuffle the training data before each epoch) or string (for &quot;batch&quot;).
&quot;batch&quot; is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks.
Has no effect when steps_per_epoch is not NULL. Default: <code>FALSE</code>.</p>
</dd>
<dt><code>testData:</code></dt><dd><p>Test Data on which to evaluate the loss and any model metrics at the end of the optimization using evaluate().</p>
</dd>
<dt><code>tfDevice:</code></dt><dd><p>Tensorflow device. CPU/GPU allocation. Passed to <code>tensorflow</code> via <code>tf$device(kerasConf$tfDevice)</code>. Default: <code>"/cpu:0"</code> (use CPU only).</p>
</dd>
<dt><code>trainData:</code></dt><dd><p>Train Data on which to evaluate the loss and any model metrics at the end of each epoch.</p>
</dd>
<dt><code>validationData:</code></dt><dd><p>Validation Data on which to evaluate the loss and any model metrics at the end of each epoch.</p>
</dd>
<dt><code>validation_data (deprecated, see validationData):</code></dt><dd><p>Data on which to evaluate the loss and any model metrics at the end of each epoch.
The model will not be trained on this data. This could be a list (x_val, y_val) or a list (x_val, y_val, val_sample_weights).
validation_data will override validation_split. Default: <code>NULL</code>.</p>
</dd>
<dt><code>validation_split:</code></dt><dd><p>Float between 0 and 1. Fraction of the training data to be
used as validation data. The model will set apart this fraction of the training data,
will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch.
The validation data is selected from the last samples in the x and y data provided, before shuffling. Default: <code>0.2</code>.</p>
</dd>
<dt><code>verbose:</code></dt><dd><p>Verbosity mode (0 = silent, 1 = progress bar, 2 = one line per epoch). Default: <code>0</code>.</p>
</dd>
</dl>



<h3>Value</h3>

<p>kerasConf <code>list</code> with configuration parameters.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+evalKerasMnist">evalKerasMnist</a></code>
</p>
<p><code><a href="#topic+funKerasMnist">funKerasMnist</a></code>
</p>
<p><code><a href="keras.html#topic+fit">fit</a></code>
</p>

<hr>
<h2 id='getMlConfig'>get ml config for keras on census</h2><span id='topic+getMlConfig'></span>

<h3>Description</h3>

<p>get ml config for keras on census
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getMlConfig(
  target,
  model,
  data,
  task.type,
  nobs,
  nfactors,
  nnumericals,
  cardinality,
  data.seed,
  prop
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="getMlConfig_+3A_target">target</code></td>
<td>
<p>character <code>"age"</code> or <code>"income_class"</code></p>
</td></tr>
<tr><td><code id="getMlConfig_+3A_model">model</code></td>
<td>
<p>character model name, e.g., <code>"dl"</code></p>
</td></tr>
<tr><td><code id="getMlConfig_+3A_data">data</code></td>
<td>
<p>data, e.g., from <code><a href="#topic+getDataCensus">getDataCensus</a></code></p>
</td></tr>
<tr><td><code id="getMlConfig_+3A_task.type">task.type</code></td>
<td>
<p><code>"classif"</code>  (character)</p>
</td></tr>
<tr><td><code id="getMlConfig_+3A_nobs">nobs</code></td>
<td>
<p>number of observations (numerical), max <code>229285</code>. Default: <code>1e4</code></p>
</td></tr>
<tr><td><code id="getMlConfig_+3A_nfactors">nfactors</code></td>
<td>
<p>(character), e.g., <code>"high"</code></p>
</td></tr>
<tr><td><code id="getMlConfig_+3A_nnumericals">nnumericals</code></td>
<td>
<p>(character), e.g., <code>"high"</code></p>
</td></tr>
<tr><td><code id="getMlConfig_+3A_cardinality">cardinality</code></td>
<td>
<p>(character), e.g., <code>"high"</code></p>
</td></tr>
<tr><td><code id="getMlConfig_+3A_data.seed">data.seed</code></td>
<td>
<p>(numerical) seed</p>
</td></tr>
<tr><td><code id="getMlConfig_+3A_prop">prop</code></td>
<td>
<p>(numerical) split proportion (train, vals,test)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>cfg (list)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
### These examples require an activated Python environment as described in
### Bartz-Beielstein, T., Rehbach, F., Sen, A., and Zaefferer, M.:
### Surrogate Model Based Hyperparameter Tuning for Deep Learning with SPOT,
### June 2021. http://arxiv.org/abs/2105.14625.
PYTHON_RETICULATE &lt;- FALSE
if(PYTHON_RETICULATE){
target &lt;- "age"
task.type &lt;- "classif"
nobs &lt;- 1e2
nfactors &lt;- "high"
nnumericals &lt;- "high"
cardinality &lt;- "high"
data.seed &lt;- 1
cachedir &lt;- "oml.cache"
model &lt;- "ranger"

dfCensus &lt;- getDataCensus(
task.type = task.type,
nobs = nobs,
nfactors = nfactors,
nnumericals = nnumericals,
cardinality = cardinality,
data.seed = data.seed,
cachedir = cachedir,
target = target)

cfg &lt;- getMlConfig(
target = target,
model = model,
data = dfCensus,
task.type = task.type,
nobs = nobs,
nfactors = nfactors,
nnumericals = nnumericals,
cardinality = cardinality,
data.seed = data.seed,
prop= 2/3)
}

</code></pre>

<hr>
<h2 id='getMlrResample'>Generate a fixed holdout instance for resampling</h2><span id='topic+getMlrResample'></span>

<h3>Description</h3>

<p>Determines test/train split and applies
<code><a href="mlr.html#topic+makeFixedHoldoutInstance">makeFixedHoldoutInstance</a></code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getMlrResample(task, dataset, data.seed = 1, prop = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="getMlrResample_+3A_task">task</code></td>
<td>
<p>mlr task</p>
</td></tr>
<tr><td><code id="getMlrResample_+3A_dataset">dataset</code></td>
<td>
<p>e.g., census data set</p>
</td></tr>
<tr><td><code id="getMlrResample_+3A_data.seed">data.seed</code></td>
<td>
<p>seed</p>
</td></tr>
<tr><td><code id="getMlrResample_+3A_prop">prop</code></td>
<td>
<p>proportion, e.g., 2/3 take 2/3 of the data for training and 1/3
for test</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list: an mlr resample generated
with <code><a href="mlr.html#topic+makeFixedHoldoutInstance">makeFixedHoldoutInstance</a></code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+getMlrTask">getMlrTask</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Example downloads OpenML data, might take some time:
dataset &lt;- getDataCensus(
task.type="classif",
nobs = 1e3,
nfactors = "high",
nnumericals = "high",
cardinality = "high",
data.seed=1,
cachedir= "oml.cache",
target = "age")

taskdata &lt;- getMlrTask(dataset,
task.type = "classif",
data.seed = 1)

rsmpl &lt;- getMlrResample(task=taskdata,
dataset = dataset,
data.seed = 1,
prop = 2/3)

</code></pre>

<hr>
<h2 id='getMlrTask'>Generate an mlr task from Census KDD data set (+variation)</h2><span id='topic+getMlrTask'></span>

<h3>Description</h3>

<p>Prepares the Census data set for mlr.
Performs imputation via: <code>factor = imputeMode()</code>,
<code>integer = imputeMedian()</code>,
<code>numeric = imputeMean()</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getMlrTask(dataset, task.type = "classif", data.seed = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="getMlrTask_+3A_dataset">dataset</code></td>
<td>
<p>census data set</p>
</td></tr>
<tr><td><code id="getMlrTask_+3A_task.type">task.type</code></td>
<td>
<p>character, either &quot;classif&quot; or &quot;regr&quot;.</p>
</td></tr>
<tr><td><code id="getMlrTask_+3A_data.seed">data.seed</code></td>
<td>
<p>seed</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an mlr task with the respective data set. Generated with
<code><a href="mlr.html#topic+makeClassifTask">makeClassifTask</a></code> or
<code><a href="mlr.html#topic+makeRegrTask">makeRegrTask</a></code> for
classification and regression repectively.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+getDataCensus">getDataCensus</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Example downloads OpenML data, might take some time:
x &lt;- getDataCensus(
task.type="classif",
nobs = 1e3,
nfactors = "high",
nnumericals = "high",
cardinality = "high",
data.seed=1,
cachedir= "oml.cache",
target = "age")

taskdata &lt;- getMlrTask(
dataset = x,
task.type = "classif",
data.seed = 1)

</code></pre>

<hr>
<h2 id='getMnistData'>getMnistData</h2><span id='topic+getMnistData'></span>

<h3>Description</h3>

<p>Based on the setting <code>kerasConf$encoding</code> either one-hot encoded data or
tensor-shaped data are returned.The labels are converted to binary class matrices using
the function <code><a href="keras.html#topic+to_categorical">to_categorical</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getMnistData(kerasConf)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="getMnistData_+3A_kerasconf">kerasConf</code></td>
<td>
<p>List of additional parameters passed to keras as described
in <code><a href="#topic+getKerasConf">getKerasConf</a></code>. Default: <code>NULL</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list with training and test data, i.e.,
<code>list(x_train, x_test, y_train, y_test)</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+getKerasConf">getKerasConf</a></code>
</p>
<p><code><a href="#topic+funKerasMnist">funKerasMnist</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
### These examples require an activated Python environment as described in
### Bartz-Beielstein, T., Rehbach, F., Sen, A., and Zaefferer, M.:
### Surrogate Model Based Hyperparameter Tuning for Deep Learning with SPOT,
### June 2021. http://arxiv.org/abs/2105.14625.
PYTHON_RETICULATE &lt;- FALSE
if(PYTHON_RETICULATE){

library("SPOTMisc")
kerasConf &lt;- getKerasConf()
kerasConf$encoding &lt;- "oneHot" # default
mnist &lt;- getMnistData(kerasConf)
# lots of zeros, but there are also some nonzero (greyscale) values, e.g.:
mnist$x_train[1,150:160]
str(mnist$x_train[1,])
# y-labels are one-hot encoded. The first entry represents "5"
mnist$y_train[1,]
##
kerasConf$encoding &lt;- "tensor"
mnist &lt;- getMnistData(kerasConf)
## 28x28:
str(mnist$x_train[1,,,])
mnist$y_train[1,]
}

</code></pre>

<hr>
<h2 id='getModelConf'>Get model configuration</h2><span id='topic+getModelConf'></span>

<h3>Description</h3>

<p>Configure machine and deep learning models
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getModelConf(
  modelArgs = NULL,
  model,
  task.type = NULL,
  nFeatures = NULL,
  active = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="getModelConf_+3A_modelargs">modelArgs</code></td>
<td>
<p>list with information about model, active variables etc. Note:
<code>argList</code> will replace the other arguments. Use <code>argList$model</code> instead
of <code>model</code> etc.</p>
</td></tr>
<tr><td><code id="getModelConf_+3A_model">model</code></td>
<td>
<p>machine or deep learning model (character). One of the following:
</p>

<dl>
<dt><code>"cvglmnet"</code></dt><dd><p>glm net.</p>
</dd>
<dt><code>"kknn"</code></dt><dd><p>nearest neighbour.</p>
</dd>
<dt><code>"ranger"</code></dt><dd><p>random forest.</p>
</dd>
<dt><code>"rpart"</code></dt><dd><p>recursive partitioning and  regression trees, <code><a href="rpart.html#topic+rpart">rpart</a></code></p>
</dd>
<dt><code>"svm"</code></dt><dd><p>support vector machines.</p>
</dd>
<dt><code>"xgboost"</code></dt><dd><p>gradient boosting, <code><a href="xgboost.html#topic+xgb.train">xgb.train</a></code>.</p>
</dd>
<dt><code>"dl"</code></dt><dd><p>deep learning: dense network.</p>
</dd>
<dt><code>"cnn"</code></dt><dd><p>deep learning: convolutionary network</p>
</dd></dl>
<p>.</p>
</td></tr>
<tr><td><code id="getModelConf_+3A_task.type">task.type</code></td>
<td>
<p>character, either <code>"classif"</code> or <code>"regr"</code>.</p>
</td></tr>
<tr><td><code id="getModelConf_+3A_nfeatures">nFeatures</code></td>
<td>
<p>number of features, e.g., <code>sum(task$task.desc$n.feat)</code></p>
</td></tr>
<tr><td><code id="getModelConf_+3A_active">active</code></td>
<td>
<p>vector of activated tunepars, e.g., <code>c("minsplit", "maxdepth")</code>
for model <code>"rpart"</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns returns a list of the
machine learning model configuration and corresponding hyperparameters:
</p>

<dl>
<dt><code>learner</code></dt><dd><p>character: combination of task.type and model name.</p>
</dd>
<dt><code>lower</code></dt><dd><p>vector of lower bounds.</p>
</dd>
<dt><code>upper</code></dt><dd><p>vector of upper bounds.</p>
</dd>
<dt><code>fixpars</code></dt><dd><p>list of fixed parameters.</p>
</dd>
<dt><code>factorlevels</code></dt><dd><p>list of factor levels.</p>
</dd>
<dt><code>transformations</code></dt><dd><p>vector of transformations.</p>
</dd>
<dt><code>dummy</code></dt><dd><p>logical. Use dummy encoding, e.g., <code><a href="xgboost.html#topic+xgb.train">xgb.train</a></code></p>
</dd>
<dt><code>relpars</code></dt><dd><p>list of relative hyperparameters.</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'># Get hyperparameter names and their defaults for fitting a
# (recursive partitioning and  regression trees) model:
modelArgs &lt;- list(model = "rpart")
cfg &lt;- getModelConf(modelArgs)
cfg$tunepars
cfg$defaults
## do not use anymore:
cfg &lt;- getModelConf(model="rpart")
cfg$tunepars
cfg$defaults
modelArgs &lt;- list(model="rpart", active = c("minsplit", "maxdepth"))
cfgAct &lt;- getModelConf(modelArgs)
cfgAct$tunepars
cfgAct$defaults

</code></pre>

<hr>
<h2 id='getObjf'>Get objective function for mlr</h2><span id='topic+getObjf'></span>

<h3>Description</h3>

<p>mlrTools
This function receives a configuration for a tuning experiment,
and returns an objective function to be tuned via SPOT.
It basically provides the result from a call to <code><a href="mlr.html#topic+resample">resample</a></code>:
<code>resample(lrn, task, resample, measures = measures, show.info = FALSE)</code>,
with measures defined as <code>mmce</code> for classification and <code>rmse</code> for regression, <code>timeboth</code>, <code>timetrain</code>, and
<code>timepredict</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getObjf(config, timeout = 3600)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="getObjf_+3A_config">config</code></td>
<td>
<p>list</p>
</td></tr>
<tr><td><code id="getObjf_+3A_timeout">timeout</code></td>
<td>
<p>integer, time in seconds after which a model (learner) evaluation will be aborted.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Parameter names are set, parameters are transformed
to the actual parameter scale, integer levels are converted
to factor levels for categorical parameters, and parameters
are set in relation to other parameters.
</p>


<h3>Value</h3>

<p>an objective function that can be optimized via <code><a href="SPOT.html#topic+spot">spot</a></code>.
</p>

<hr>
<h2 id='getPredf'>Get predictions from mlr</h2><span id='topic+getPredf'></span>

<h3>Description</h3>

<p>mlrTools
This function receives a configuration for a tuning experiment,
and returns predicted values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getPredf(config, timeout = 3600)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="getPredf_+3A_config">config</code></td>
<td>
<p>list</p>
</td></tr>
<tr><td><code id="getPredf_+3A_timeout">timeout</code></td>
<td>
<p>integer, time in seconds after which a model (learner) evaluation will be aborted.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an prediction function that can be called via <code><a href="SPOT.html#topic+spot">spot</a></code>.
It basically provides the result from a call to <code><a href="mlr.html#topic+resample">resample</a></code>:
<code>resample(lrn, task, resample, measures = measures, show.info = FALSE)</code>,
</p>

<hr>
<h2 id='getSimpleKerasModel'>getSimpleKerasModel</h2><span id='topic+getSimpleKerasModel'></span>

<h3>Description</h3>

<p>build, compile, and train a simple model  (for testing)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getSimpleKerasModel(specList, kerasConf = getKerasConf())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="getSimpleKerasModel_+3A_speclist">specList</code></td>
<td>
<p>spec</p>
</td></tr>
<tr><td><code id="getSimpleKerasModel_+3A_kerasconf">kerasConf</code></td>
<td>
<p>keras configuration. Default: return value from <code><a href="#topic+getKerasConf">getKerasConf</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>model. Fitted keras model
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
### These examples require an activated Python environment as described in
### Bartz-Beielstein, T., Rehbach, F., Sen, A., and Zaefferer, M.:
### Surrogate Model Based Hyperparameter Tuning for Deep Learning with SPOT,
### June 2021. http://arxiv.org/abs/2105.14625.
PYTHON_RETICULATE &lt;- FALSE
if(PYTHON_RETICULATE){
target &lt;- "age"
nobs &lt;- 1000
batch_size &lt;- 32
prop &lt;- 2/3
dfCensus &lt;- getDataCensus(target = target,
nobs = nobs)
data &lt;- getGenericTrainValTestData(dfGeneric = dfCensus,
prop = prop)
specList &lt;- genericDataPrep(data=data, batch_size = batch_size)
kerasConf &lt;- getKerasConf()
simpleModel &lt;- getSimpleKerasModel(specList = specList,
               kerasConf = kerasConf)
}

</code></pre>

<hr>
<h2 id='getVarNames'>Get variable names or subsets of variable names</h2><span id='topic+getVarNames'></span>

<h3>Description</h3>

<p>Get variable names or subsets of variable names
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getVarNames(model, i = "all")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="getVarNames_+3A_model">model</code></td>
<td>
<p>from <code><a href="#topic+getModelConf">getModelConf</a></code>, e.g., <code>"dl"</code>.</p>
</td></tr>
<tr><td><code id="getVarNames_+3A_i">i</code></td>
<td>
<p>index for selecting subsets. Default is <code>"all"</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>vector of variable names. Returns <code>NA</code> if wrong indices are selected.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Default is return all:
getVarNames(model="dl")
getVarNames(model="dl",i=3)
getVarNames(model="dl",i=c(1,3,5))
# var name does not exits, so return NA
getVarNames(model="dl",i=c(100))


</code></pre>

<hr>
<h2 id='ggparcoordPrepare'>Build data frame for ggparcoord (parallel plot)</h2><span id='topic+ggparcoordPrepare'></span>

<h3>Description</h3>

<p>Build data frame for ggparcoord (parallel plot)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ggparcoordPrepare(
  x,
  y,
  xlab = NULL,
  ylab = NULL,
  probs = seq(0.25, 0.75, 0.25),
  yrange = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ggparcoordPrepare_+3A_x">x</code></td>
<td>
<p>elements <code>x</code>, e.g., result from a <code><a href="SPOT.html#topic+spot">spot</a></code> run.</p>
</td></tr>
<tr><td><code id="ggparcoordPrepare_+3A_y">y</code></td>
<td>
<p>associated function values</p>
</td></tr>
<tr><td><code id="ggparcoordPrepare_+3A_xlab">xlab</code></td>
<td>
<p>character, the value of the independent variable</p>
</td></tr>
<tr><td><code id="ggparcoordPrepare_+3A_ylab">ylab</code></td>
<td>
<p>character, the value of the dependent variable predicted by the corresponding model.</p>
</td></tr>
<tr><td><code id="ggparcoordPrepare_+3A_probs">probs</code></td>
<td>
<p>quantile probabilities. Default:  <code>seq(0, 1, 0.25)</code></p>
</td></tr>
<tr><td><code id="ggparcoordPrepare_+3A_yrange">yrange</code></td>
<td>
<p>y interval</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data frame for <code><a href="GGally.html#topic+ggparcoord">ggparcoord</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(SPOT)
require(GGally)
n &lt;- 4 # param
k &lt;- 50 # samples
x &lt;- designUniformRandom(,rep(0,n), rep(1,n),control=list(size=k))
y &lt;- matrix(0, nrow=k,ncol=1)
y &lt;- funSphere(x)
result &lt;- list(x=x, y=y)
df &lt;- ggparcoordPrepare(x=result$x,
                        y=result$y,
                        xlab=result$control$parNames,
                        probs = c(0.25, 0.5, 0.75))
                        #probs = c(0.1, 0.9))  # c(0.9,0.95) )#seq(0.25, 1, 0.25))
m &lt;- ncol(df)
splineFactor &lt;- max(1, floor(2*m))
ggparcoord(data=df, columns = 1:(m-2), groupColumn = m,
scale = "uniminmax", boxplot = FALSE, alphaLines = 0.2,showPoints = TRUE)
##

require(SPOT)
require(GGally)
result &lt;- spot(x=NULL,
              fun=funSphere,
              lower=rep(-1,3),
              upper= rep(1,3),
 control=list(funEvals=20,
             model=buildKriging,
             modelControl=list(target="y")))

df &lt;- ggparcoordPrepare(x=result$x,
                        y=result$y,
                        xlab=result$control$parNames,
                        probs = c(0.25, 0.5, 0.75))  # c(0.9,0.95) )#seq(0.25, 1, 0.25))
m &lt;- ncol(df)
splineFactor &lt;- max(1, floor(2*m))
ggparcoord(data=df, columns = 1:(m-2), groupColumn = m,
splineFactor = splineFactor, scale = "uniminmax",
boxplot = FALSE, alphaLines = 0.2,showPoints = TRUE, scaleSummary = "median")


</code></pre>

<hr>
<h2 id='ggplotProgress'>simple progress plot</h2><span id='topic+ggplotProgress'></span>

<h3>Description</h3>

<p>simple progress plot
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ggplotProgress(
  dfRun,
  xlabel = "function evaluations",
  ylabel = "MMCE",
  aspectRatio = 2,
  scalesFreeFixed = "free_y",
  nColumns = 3
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ggplotProgress_+3A_dfrun">dfRun</code></td>
<td>
<p>data frame, e.g., result from <code><a href="#topic+prepareProgressPlot">prepareProgressPlot</a></code>.</p>
</td></tr>
<tr><td><code id="ggplotProgress_+3A_xlabel">xlabel</code></td>
<td>
<p>x label</p>
</td></tr>
<tr><td><code id="ggplotProgress_+3A_ylabel">ylabel</code></td>
<td>
<p>y label</p>
</td></tr>
<tr><td><code id="ggplotProgress_+3A_aspectratio">aspectRatio</code></td>
<td>
<p>aspect.ratio</p>
</td></tr>
<tr><td><code id="ggplotProgress_+3A_scalesfreefixed">scalesFreeFixed</code></td>
<td>
<p>&quot;free_x&quot;, &quot;free_y&quot; or &quot;fixed&quot;</p>
</td></tr>
<tr><td><code id="ggplotProgress_+3A_ncolumns">nColumns</code></td>
<td>
<p>number of columns</p>
</td></tr>
</table>


<h3>Value</h3>

<p>p ggplot
</p>

<hr>
<h2 id='int2fact'>Helper function: transform integer to factor</h2><span id='topic+int2fact'></span>

<h3>Description</h3>

<p>This function re-codes a factor with pre-specified factor levels,
using an integer encoding as input.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>int2fact(x, lvls)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="int2fact_+3A_x">x</code></td>
<td>
<p>an integer vector (that represents factor vector) to be transformed</p>
</td></tr>
<tr><td><code id="int2fact_+3A_lvls">lvls</code></td>
<td>
<p>the original factor levels used</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the same factor, now coded with the original levels
</p>

<hr>
<h2 id='kerasBuildCompile'>evalKerasGeneric model building and compile</h2><span id='topic+kerasBuildCompile'></span>

<h3>Description</h3>

<p>Hyperparameter Tuning: Keras Generic Classification Function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kerasBuildCompile(FLAGS, kerasConf, specList)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kerasBuildCompile_+3A_flags">FLAGS</code></td>
<td>
<p>flags. list of hyperparameter values.
If <code>NULL</code>, a simple keras model will be build, which is considered default
(see <code><a href="#topic+getSimpleKerasModel">getSimpleKerasModel</a></code>).</p>
</td></tr>
<tr><td><code id="kerasBuildCompile_+3A_kerasconf">kerasConf</code></td>
<td>
<p>List of additional parameters passed to keras as described in <code><a href="#topic+getKerasConf">getKerasConf</a></code>.
Default: <code>kerasConf = getKerasConf()</code>.</p>
</td></tr>
<tr><td><code id="kerasBuildCompile_+3A_speclist">specList</code></td>
<td>
<p>prepared data. See <code><a href="#topic+genericDataPrep">genericDataPrep</a></code>.
See <code><a href="#topic+getGenericTrainValTestData">getGenericTrainValTestData</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Trains a simple deep NN on a generic data set.
Standard Code from <a href="https://tensorflow.rstudio.com/">https://tensorflow.rstudio.com/</a>
Modified by T. Bartz-Beielstein (tbb@bartzundbartz.de)
</p>


<h3>Value</h3>

<p>list with function values (training, validation, and test loss/accuracy,
and keras model information)
</p>


<h3>See Also</h3>

<p><code><a href="#topic+getKerasConf">getKerasConf</a></code>
</p>
<p><code><a href="#topic+funKerasGeneric">funKerasGeneric</a></code>
</p>
<p><code><a href="keras.html#topic+fit">fit</a></code>
</p>

<hr>
<h2 id='kerasCompileResult'>Generate result from keras run</h2><span id='topic+kerasCompileResult'></span>

<h3>Description</h3>

<p>Compile a matrix with training, validation, and test results
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kerasCompileResult(y, kerasConf)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kerasCompileResult_+3A_y">y</code></td>
<td>
<p>(1x6)-dim matrix with the following entries: <code>trainingLoss</code>,
<code>negTrainingAccuracy</code>, <code>validationLoss</code>, <code>negValidationAccuracy</code>,
<code>testLoss</code>,and <code>negTestAccuracy</code>.</p>
</td></tr>
<tr><td><code id="kerasCompileResult_+3A_kerasconf">kerasConf</code></td>
<td>
<p>keras configuration generated with <code><a href="#topic+getKerasConf">getKerasConf</a></code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>All values should be minimized: accuracies will be negative.
The (1x7)-dim result matrix has the following entries
</p>

<dl>
<dt><code>returnValue:</code></dt><dd><p>Metric used for optimization. Default: <code>"validationLoss"</code>.</p>
</dd>
<dt><code>trainingLoss:</code></dt><dd><p>training loss.</p>
</dd>
<dt><code>negTrainingAccuracy:</code></dt><dd><p>negative training accuracy.</p>
</dd>
<dt><code>validationLoss:</code></dt><dd><p>validation  loss.</p>
</dd>
<dt><code>negValidationAccuracy:</code></dt><dd><p>negative validation accuracy.</p>
</dd>
<dt><code>testLoss:</code></dt><dd><p>test loss.</p>
</dd>
<dt><code>negTestAccuracy:</code></dt><dd><p>negative test accuracy.</p>
</dd>
</dl>



<h3>Value</h3>

<p>result matrix
</p>


<h3>See Also</h3>

<p><code><a href="#topic+evalKerasMnist">evalKerasMnist</a></code>
</p>
<p><code><a href="#topic+funKerasMnist">funKerasMnist</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- 1
testLoss &lt;-  x
negTestAccuracy &lt;- 1-x
validationLoss &lt;- x/2
negValidationAccuracy &lt;- 1-x/2
trainingLoss &lt;- x/3
negTrainingAccuracy &lt;- 1-x/3
y &lt;- matrix(c(trainingLoss, negTrainingAccuracy,
validationLoss, negValidationAccuracy,
testLoss, negTestAccuracy), 1,6)
kerasConf &lt;- list()
kerasConf$returnValue &lt;-   "testLoss"
sum(kerasCompileResult(y, kerasConf)) == 4
kerasConf$returnValue &lt;-  "negTestAccuracy"
sum(kerasCompileResult(y, kerasConf)) == 3
kerasConf$returnValue &lt;-   "validationLoss"
sum(kerasCompileResult(y, kerasConf))*2 == 7
kerasConf$returnValue &lt;-   "negValidationAccuracy"
sum(kerasCompileResult(y, kerasConf))*2 == 7
kerasConf$returnValue &lt;-     "trainingLoss"
sum(kerasCompileResult(y, kerasConf))*3 == 10
kerasConf$returnValue &lt;-   "negTrainingAccuracy"
sum(kerasCompileResult(y, kerasConf))*3 == 11

</code></pre>

<hr>
<h2 id='kerasEvalPrediction'>Evaluate keras prediction</h2><span id='topic+kerasEvalPrediction'></span>

<h3>Description</h3>

<p>Evaluates prediction from keras model using several
metrics based on training, validation and test data
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kerasEvalPrediction(pred, testScore = c(NA, NA), specList, metrics, kerasConf)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kerasEvalPrediction_+3A_pred">pred</code></td>
<td>
<p>prediction from keras predict</p>
</td></tr>
<tr><td><code id="kerasEvalPrediction_+3A_testscore">testScore</code></td>
<td>
<p>additional score values</p>
</td></tr>
<tr><td><code id="kerasEvalPrediction_+3A_speclist">specList</code></td>
<td>
<p>spec with target</p>
</td></tr>
<tr><td><code id="kerasEvalPrediction_+3A_metrics">metrics</code></td>
<td>
<p>keras metrics (history)</p>
</td></tr>
<tr><td><code id="kerasEvalPrediction_+3A_kerasconf">kerasConf</code></td>
<td>
<p>keras config</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
### These examples require an activated Python environment as described in
### Bartz-Beielstein, T., Rehbach, F., Sen, A., and Zaefferer, M.:
### Surrogate Model Based Hyperparameter Tuning for Deep Learning with SPOT,
### June 2021. http://arxiv.org/abs/2105.14625.
PYTHON_RETICULATE &lt;- FALSE
if(PYTHON_RETICULATE){
library(tfdatasets)
library(keras)
target &lt;- "age"
batch_size &lt;- 32
prop &lt;- 2/3
dfCensus &lt;- getDataCensus(nobs=1000,
                          target = target)
data &lt;- getGenericTrainValTestData(dfGeneric = dfCensus,
prop = prop)
specList &lt;- genericDataPrep(data=data, batch_size = batch_size)
## spec test data has 334 elements:
str(specList$testGeneric$target)
## simulate test:
pred &lt;- runif(length(specList$testGeneric$target))
kerasConf &lt;- getKerasConf()
simpleModel &lt;- getSimpleKerasModel(specList=specList,
                    kerasConf=kerasConf)
FLAGS &lt;- list(epochs=16)
y &lt;- kerasFit(model=simpleModel,
               specList = specList,
               FLAGS = FLAGS,
               kerasConf = kerasConf)
 simpeModel &lt;- y$model
 history &lt;- y$history
# evaluate on test data
pred &lt;- predict(simpleModel, specList$testGeneric)
## in use keras evaluation (test error):
testScore &lt;-
 keras::evaluate(simpleModel,
         tfdatasets::dataset_use_spec(dataset=specList$test_ds_generic,
         spec=specList$specGeneric_prep),
         verbose = kerasConf$verbose)
 kerasEvalPrediction(pred=pred,
                     testScore = testScore,
                    specList = specList,
                    metrics = history$metrics,
                    kerasConf = kerasConf
                    )
}

</code></pre>

<hr>
<h2 id='kerasFit'>kerasFit fit</h2><span id='topic+kerasFit'></span>

<h3>Description</h3>

<p>Hyperparameter Tuning: Keras Generic Classification Function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kerasFit(model, specList, FLAGS, kerasConf)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kerasFit_+3A_model">model</code></td>
<td>
<p>model
If <code>NULL</code>, a simple keras model will be build, which is considered default
(see <code><a href="#topic+getSimpleKerasModel">getSimpleKerasModel</a></code>).</p>
</td></tr>
<tr><td><code id="kerasFit_+3A_speclist">specList</code></td>
<td>
<p>prepared data. See <code><a href="#topic+genericDataPrep">genericDataPrep</a></code>.
See <code><a href="#topic+getGenericTrainValTestData">getGenericTrainValTestData</a></code>.</p>
</td></tr>
<tr><td><code id="kerasFit_+3A_flags">FLAGS</code></td>
<td>
<p>flags, see also <code><a href="#topic+mapX2FLAGS">mapX2FLAGS</a></code></p>
</td></tr>
<tr><td><code id="kerasFit_+3A_kerasconf">kerasConf</code></td>
<td>
<p>List of additional parameters passed to keras as described in <code><a href="#topic+getKerasConf">getKerasConf</a></code>.
Default: <code>kerasConf = getKerasConf()</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Trains a simple deep NN on a generic data set.
Standard Code from <a href="https://tensorflow.rstudio.com/">https://tensorflow.rstudio.com/</a>
Modified by T. Bartz-Beielstein (tbb@bartzundbartz.de)
</p>


<h3>Value</h3>

<p>list with function values (training, validation, and test loss/accuracy,
and keras model information)
</p>


<h3>See Also</h3>

<p><code><a href="#topic+getKerasConf">getKerasConf</a></code>
</p>
<p><code><a href="#topic+funKerasGeneric">funKerasGeneric</a></code>
</p>
<p><code><a href="keras.html#topic+fit">fit</a></code>
</p>

<hr>
<h2 id='kerasReturnDummy'>Return dummy values</h2><span id='topic+kerasReturnDummy'></span>

<h3>Description</h3>

<p>Return dummy values
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kerasReturnDummy(kerasConf)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kerasReturnDummy_+3A_kerasconf">kerasConf</code></td>
<td>
<p>keras configuration list</p>
</td></tr>
</table>


<h3>Value</h3>

<p>y row matrix of random (uniformly distributed) return values
</p>


<h3>Examples</h3>

<pre><code class='language-R'>kerasConf &lt;- getKerasConf()
kerasReturnDummy(kerasConf)

</code></pre>

<hr>
<h2 id='makeLearnerFromHyperparameters'>Make mlr learner from conf and hyperparameter vector</h2><span id='topic+makeLearnerFromHyperparameters'></span>

<h3>Description</h3>

<p>calls makelearner
</p>


<h3>Usage</h3>

<pre><code class='language-R'>makeLearnerFromHyperparameters(x = NULL, cfg = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="makeLearnerFromHyperparameters_+3A_x">x</code></td>
<td>
<p>hyperparameter vector</p>
</td></tr>
<tr><td><code id="makeLearnerFromHyperparameters_+3A_cfg">cfg</code></td>
<td>
<p>configuration list</p>
</td></tr>
</table>


<h3>Value</h3>

<p>mlr learner
</p>

<hr>
<h2 id='mapX2FLAGS'>Map x parameters to a list of named values</h2><span id='topic+mapX2FLAGS'></span>

<h3>Description</h3>

<p>numerical parameters are mapped to their meanings, e.g.,
<code>x[1]</code> to <code>"dropout rate"</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mapX2FLAGS(x, model = "dl")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mapX2FLAGS_+3A_x">x</code></td>
<td>
<p>matrix input values.</p>
</td></tr>
<tr><td><code id="mapX2FLAGS_+3A_model">model</code></td>
<td>
<p>(char) network type, e.g., <code>"cnn"</code> or <code>"dl"</code>. Default: <code>"dl"</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>For a <code>"dl"</code> network, the parameter vector <code>x</code> is mapped
to the following <code>FLAGS</code>:
</p>

<dl>
<dt><code>x[1]: dropout</code></dt><dd><p>dropout rate first layer.</p>
</dd>
<dt><code>x[2]: dropoutfac</code></dt><dd><p>dropout factor (multiplier).</p>
</dd>
<dt><code>x[3]: units</code></dt><dd><p>number of units in the first layer.</p>
</dd>
<dt><code>x[4]: unitsfact</code></dt><dd><p>units factor (multiplier).</p>
</dd>
<dt><code>x[5]: learning_rate</code></dt><dd><p>learning rate for optimizer. See, e.g.: <code>link{optimizer_sgd}</code></p>
</dd>
<dt><code>x[6]: epochs</code></dt><dd><p>number of training epochs.</p>
</dd>
<dt><code>x[7]: beta_1</code></dt><dd><p>The exponential decay rate for the 1st moment estimates. float, 0 &lt; beta &lt; 1. Generally close to 1.</p>
</dd>
<dt><code>x[8]: beta_2</code></dt><dd><p>The exponential decay rate for the 2nd moment estimates. float, 0 &lt; beta &lt; 1. Generally close to 1.</p>
</dd>
<dt><code>x[9]: layers</code></dt><dd><p>number of layers.</p>
</dd>
<dt><code>x[10]: epsilon</code></dt><dd><p>float &gt;= 0. Fuzz factor. If NULL, defaults to k_epsilon().</p>
</dd>
<dt><code>x[11]: optimizer</code></dt><dd><p>integer. Specifies optimizer.</p>
</dd>
</dl>



<h3>Value</h3>

<p>FLAGS named list (parameter names as specified in <code><a href="#topic+getModelConf">getModelConf</a></code>), e.g.,
for &quot;dl&quot;: dropout, dropoutfac, units, unitsfact, learning_rate,
epochs, beta_1, beta_2, layers, epsilon, optimizer
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## First example: dense neural net
x &lt;- getModelConf(list(model="dl"))$defaults
mapX2FLAGS(x=x, model = "dl")
## Second example: convnet
x &lt;- getModelConf(list(model="cnn"))$defaults
mapX2FLAGS(x=x, model = "cnn")

</code></pre>

<hr>
<h2 id='MSE'>mean squared errors</h2><span id='topic+MSE'></span>

<h3>Description</h3>

<p>mean squared errors
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MSE(y, yhat)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MSE_+3A_y">y</code></td>
<td>
<p>actual value</p>
</td></tr>
<tr><td><code id="MSE_+3A_yhat">yhat</code></td>
<td>
<p>predicted value</p>
</td></tr>
</table>


<h3>Value</h3>

<p>mean squared errors
</p>

<hr>
<h2 id='optimizer_adadelta'>Adadelta optimizer.</h2><span id='topic+optimizer_adadelta'></span>

<h3>Description</h3>

<p>Adadelta optimizer as described in [ADADELTA: An Adaptive Learning Rate
Method](https://arxiv.org/abs/1212.5701).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optimizer_adadelta(
  learning_rate = 0,
  rho = 0.95,
  epsilon = NULL,
  decay = 0,
  clipnorm = NULL,
  clipvalue = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="optimizer_adadelta_+3A_learning_rate">learning_rate</code></td>
<td>
<p>float &gt;= 0. Learning rate.</p>
</td></tr>
<tr><td><code id="optimizer_adadelta_+3A_rho">rho</code></td>
<td>
<p>float &gt;= 0. Decay factor.</p>
</td></tr>
<tr><td><code id="optimizer_adadelta_+3A_epsilon">epsilon</code></td>
<td>
<p>float &gt;= 0. Fuzz factor. If 'NULL', defaults to 'k_epsilon()'.</p>
</td></tr>
<tr><td><code id="optimizer_adadelta_+3A_decay">decay</code></td>
<td>
<p>float &gt;= 0. Learning rate decay over each update.</p>
</td></tr>
<tr><td><code id="optimizer_adadelta_+3A_clipnorm">clipnorm</code></td>
<td>
<p>Gradients will be clipped when their L2 norm exceeds this
value.</p>
</td></tr>
<tr><td><code id="optimizer_adadelta_+3A_clipvalue">clipvalue</code></td>
<td>
<p>Gradients will be clipped when their absolute value exceeds
this value.</p>
</td></tr>
<tr><td><code id="optimizer_adadelta_+3A_...">...</code></td>
<td>
<p>Unused, present only for backwards compatability</p>
</td></tr>
</table>


<h3>Note</h3>

<p>To enbale compatibility with the ranges of the learning rates
of the other optimizers, the learning rate <code>learning_rate</code>
is internally mapped to <code>1- learning_rate</code>. That is,
a learning rat of 0 will be mapped to 1 (which is the default.)
It is recommended to leave the parameters of this optimizer at their
default values.
</p>


<h3>See Also</h3>

<p>Other optimizers: 
<code><a href="#topic+optimizer_adagrad">optimizer_adagrad</a>()</code>,
<code><a href="#topic+optimizer_adamax">optimizer_adamax</a>()</code>,
<code><a href="#topic+optimizer_adam">optimizer_adam</a>()</code>,
<code><a href="#topic+optimizer_nadam">optimizer_nadam</a>()</code>,
<code><a href="#topic+optimizer_rmsprop">optimizer_rmsprop</a>()</code>,
<code><a href="#topic+optimizer_sgd">optimizer_sgd</a>()</code>
</p>

<hr>
<h2 id='optimizer_adagrad'>Adagrad optimizer</h2><span id='topic+optimizer_adagrad'></span>

<h3>Description</h3>

<p>Adagrad optimizer as described in [Adaptive Subgradient Methods for Online
Learning and Stochastic Optimization](https://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optimizer_adagrad(
  learning_rate = 0.01,
  epsilon = NULL,
  decay = 0,
  clipnorm = NULL,
  clipvalue = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="optimizer_adagrad_+3A_learning_rate">learning_rate</code></td>
<td>
<p>float &gt;= 0. Learning rate.</p>
</td></tr>
<tr><td><code id="optimizer_adagrad_+3A_epsilon">epsilon</code></td>
<td>
<p>float &gt;= 0. Fuzz factor. If 'NULL', defaults to 'k_epsilon()'.</p>
</td></tr>
<tr><td><code id="optimizer_adagrad_+3A_decay">decay</code></td>
<td>
<p>float &gt;= 0. Learning rate decay over each update.</p>
</td></tr>
<tr><td><code id="optimizer_adagrad_+3A_clipnorm">clipnorm</code></td>
<td>
<p>Gradients will be clipped when their L2 norm exceeds this
value.</p>
</td></tr>
<tr><td><code id="optimizer_adagrad_+3A_clipvalue">clipvalue</code></td>
<td>
<p>Gradients will be clipped when their absolute value exceeds
this value.</p>
</td></tr>
<tr><td><code id="optimizer_adagrad_+3A_...">...</code></td>
<td>
<p>Unused, present only for backwards compatability</p>
</td></tr>
</table>


<h3>Note</h3>

<p>To enable compatibility with the ranges of the learning rates
of the other optimizers, the learning rate <code>learning_rate</code>
is internally mapped to <code>10 * learning_rate</code>. That is,
a learning rat of 0.001 will be mapped to 0.01 (which is the default.)
</p>


<h3>See Also</h3>

<p>Other optimizers: 
<code><a href="#topic+optimizer_adadelta">optimizer_adadelta</a>()</code>,
<code><a href="#topic+optimizer_adamax">optimizer_adamax</a>()</code>,
<code><a href="#topic+optimizer_adam">optimizer_adam</a>()</code>,
<code><a href="#topic+optimizer_nadam">optimizer_nadam</a>()</code>,
<code><a href="#topic+optimizer_rmsprop">optimizer_rmsprop</a>()</code>,
<code><a href="#topic+optimizer_sgd">optimizer_sgd</a>()</code>
</p>

<hr>
<h2 id='optimizer_adam'>Adam optimizer</h2><span id='topic+optimizer_adam'></span>

<h3>Description</h3>

<p>Adam optimizer as described in [Adam - A Method for Stochastic
Optimization](https://arxiv.org/abs/1412.6980v8).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optimizer_adam(
  learning_rate = 0.001,
  beta_1 = 0.9,
  beta_2 = 0.999,
  epsilon = NULL,
  decay = 0,
  amsgrad = FALSE,
  clipnorm = NULL,
  clipvalue = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="optimizer_adam_+3A_learning_rate">learning_rate</code></td>
<td>
<p>float &gt;= 0. Learning rate.</p>
</td></tr>
<tr><td><code id="optimizer_adam_+3A_beta_1">beta_1</code></td>
<td>
<p>The exponential decay rate for the 1st moment estimates. float,
0 &lt; beta &lt; 1. Generally close to 1.</p>
</td></tr>
<tr><td><code id="optimizer_adam_+3A_beta_2">beta_2</code></td>
<td>
<p>The exponential decay rate for the 2nd moment estimates. float,
0 &lt; beta &lt; 1. Generally close to 1.</p>
</td></tr>
<tr><td><code id="optimizer_adam_+3A_epsilon">epsilon</code></td>
<td>
<p>float &gt;= 0. Fuzz factor. If 'NULL', defaults to 'k_epsilon()'.</p>
</td></tr>
<tr><td><code id="optimizer_adam_+3A_decay">decay</code></td>
<td>
<p>float &gt;= 0. Learning rate decay over each update.</p>
</td></tr>
<tr><td><code id="optimizer_adam_+3A_amsgrad">amsgrad</code></td>
<td>
<p>Whether to apply the AMSGrad variant of this algorithm from
the paper &quot;On the Convergence of Adam and Beyond&quot;.</p>
</td></tr>
<tr><td><code id="optimizer_adam_+3A_clipnorm">clipnorm</code></td>
<td>
<p>Gradients will be clipped when their L2 norm exceeds this
value.</p>
</td></tr>
<tr><td><code id="optimizer_adam_+3A_clipvalue">clipvalue</code></td>
<td>
<p>Gradients will be clipped when their absolute value exceeds
this value.</p>
</td></tr>
<tr><td><code id="optimizer_adam_+3A_...">...</code></td>
<td>
<p>Unused, present only for backwards compatability</p>
</td></tr>
</table>


<h3>References</h3>

<p>- [Adam - A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980v8)
- [On the Convergence of Adam and Beyond](https://openreview.net/forum?id=ryQu7f-RZ)
</p>


<h3>Note</h3>

<p>Default parameters follow those provided in the original paper.
</p>


<h3>See Also</h3>

<p>Other optimizers: 
<code><a href="#topic+optimizer_adadelta">optimizer_adadelta</a>()</code>,
<code><a href="#topic+optimizer_adagrad">optimizer_adagrad</a>()</code>,
<code><a href="#topic+optimizer_adamax">optimizer_adamax</a>()</code>,
<code><a href="#topic+optimizer_nadam">optimizer_nadam</a>()</code>,
<code><a href="#topic+optimizer_rmsprop">optimizer_rmsprop</a>()</code>,
<code><a href="#topic+optimizer_sgd">optimizer_sgd</a>()</code>
</p>

<hr>
<h2 id='optimizer_adamax'>Adamax optimizer</h2><span id='topic+optimizer_adamax'></span>

<h3>Description</h3>

<p>Adamax optimizer from Section 7 of the [Adam paper](https://arxiv.org/abs/1412.6980v8).
It is a variant of Adam based on the infinity norm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optimizer_adamax(
  learning_rate = 0.002,
  beta_1 = 0.9,
  beta_2 = 0.999,
  epsilon = NULL,
  decay = 0,
  clipnorm = NULL,
  clipvalue = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="optimizer_adamax_+3A_learning_rate">learning_rate</code></td>
<td>
<p>float &gt;= 0. Learning rate.</p>
</td></tr>
<tr><td><code id="optimizer_adamax_+3A_beta_1">beta_1</code></td>
<td>
<p>The exponential decay rate for the 1st moment estimates. float,
0 &lt; beta &lt; 1. Generally close to 1.</p>
</td></tr>
<tr><td><code id="optimizer_adamax_+3A_beta_2">beta_2</code></td>
<td>
<p>The exponential decay rate for the 2nd moment estimates. float,
0 &lt; beta &lt; 1. Generally close to 1.</p>
</td></tr>
<tr><td><code id="optimizer_adamax_+3A_epsilon">epsilon</code></td>
<td>
<p>float &gt;= 0. Fuzz factor. If 'NULL', defaults to 'k_epsilon()'.</p>
</td></tr>
<tr><td><code id="optimizer_adamax_+3A_decay">decay</code></td>
<td>
<p>float &gt;= 0. Learning rate decay over each update.</p>
</td></tr>
<tr><td><code id="optimizer_adamax_+3A_clipnorm">clipnorm</code></td>
<td>
<p>Gradients will be clipped when their L2 norm exceeds this
value.</p>
</td></tr>
<tr><td><code id="optimizer_adamax_+3A_clipvalue">clipvalue</code></td>
<td>
<p>Gradients will be clipped when their absolute value exceeds
this value.</p>
</td></tr>
<tr><td><code id="optimizer_adamax_+3A_...">...</code></td>
<td>
<p>Unused, present only for backwards compatability</p>
</td></tr>
</table>


<h3>Note</h3>

<p>To enable compatibility with the ranges of the learning rates
of the other optimizers, the learning rate <code>learning_rate</code>
is internally mapped to <code>2 * learning_rate</code>. That is,
a learning rat of 0.001 will be mapped to 0.002 (which is the default.)
</p>


<h3>See Also</h3>

<p>Other optimizers: 
<code><a href="#topic+optimizer_adadelta">optimizer_adadelta</a>()</code>,
<code><a href="#topic+optimizer_adagrad">optimizer_adagrad</a>()</code>,
<code><a href="#topic+optimizer_adam">optimizer_adam</a>()</code>,
<code><a href="#topic+optimizer_nadam">optimizer_nadam</a>()</code>,
<code><a href="#topic+optimizer_rmsprop">optimizer_rmsprop</a>()</code>,
<code><a href="#topic+optimizer_sgd">optimizer_sgd</a>()</code>
</p>

<hr>
<h2 id='optimizer_nadam'>Nesterov Adam optimizer</h2><span id='topic+optimizer_nadam'></span>

<h3>Description</h3>

<p>Much like Adam is essentially RMSprop with momentum, Nadam is Adam RMSprop
with Nesterov momentum.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optimizer_nadam(
  learning_rate = 0.002,
  beta_1 = 0.9,
  beta_2 = 0.999,
  epsilon = NULL,
  schedule_decay = 0.004,
  clipnorm = NULL,
  clipvalue = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="optimizer_nadam_+3A_learning_rate">learning_rate</code></td>
<td>
<p>float &gt;= 0. Learning rate.</p>
</td></tr>
<tr><td><code id="optimizer_nadam_+3A_beta_1">beta_1</code></td>
<td>
<p>The exponential decay rate for the 1st moment estimates. float,
0 &lt; beta &lt; 1. Generally close to 1.</p>
</td></tr>
<tr><td><code id="optimizer_nadam_+3A_beta_2">beta_2</code></td>
<td>
<p>The exponential decay rate for the 2nd moment estimates. float,
0 &lt; beta &lt; 1. Generally close to 1.</p>
</td></tr>
<tr><td><code id="optimizer_nadam_+3A_epsilon">epsilon</code></td>
<td>
<p>float &gt;= 0. Fuzz factor. If 'NULL', defaults to 'k_epsilon()'.</p>
</td></tr>
<tr><td><code id="optimizer_nadam_+3A_schedule_decay">schedule_decay</code></td>
<td>
<p>Schedule deacy.</p>
</td></tr>
<tr><td><code id="optimizer_nadam_+3A_clipnorm">clipnorm</code></td>
<td>
<p>Gradients will be clipped when their L2 norm exceeds this
value.</p>
</td></tr>
<tr><td><code id="optimizer_nadam_+3A_clipvalue">clipvalue</code></td>
<td>
<p>Gradients will be clipped when their absolute value exceeds
this value.</p>
</td></tr>
<tr><td><code id="optimizer_nadam_+3A_...">...</code></td>
<td>
<p>Unused, present only for backwards compatability</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Default parameters follow those provided in the paper.
</p>


<h3>Note</h3>

<p>To enable compatibility with the ranges of the learning rates
of the other optimizers, the learning rate <code>learning_rate</code>
is internally mapped to <code>2 * learning_rate</code>. That is,
a learning rat of 0.001 will be mapped to 0.002 (which is the default.)
</p>


<h3>See Also</h3>

<p>[On the importance of initialization and momentum in deep
learning](https://www.cs.toronto.edu/~fritz/absps/momentum.pdf).
</p>
<p>Other optimizers: 
<code><a href="#topic+optimizer_adadelta">optimizer_adadelta</a>()</code>,
<code><a href="#topic+optimizer_adagrad">optimizer_adagrad</a>()</code>,
<code><a href="#topic+optimizer_adamax">optimizer_adamax</a>()</code>,
<code><a href="#topic+optimizer_adam">optimizer_adam</a>()</code>,
<code><a href="#topic+optimizer_rmsprop">optimizer_rmsprop</a>()</code>,
<code><a href="#topic+optimizer_sgd">optimizer_sgd</a>()</code>
</p>

<hr>
<h2 id='optimizer_rmsprop'>RMSProp optimizer</h2><span id='topic+optimizer_rmsprop'></span>

<h3>Description</h3>

<p>RMSProp optimizer
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optimizer_rmsprop(
  learning_rate = 0.001,
  rho = 0.9,
  epsilon = NULL,
  decay = 0,
  clipnorm = NULL,
  clipvalue = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="optimizer_rmsprop_+3A_learning_rate">learning_rate</code></td>
<td>
<p>float &gt;= 0. Learning rate.</p>
</td></tr>
<tr><td><code id="optimizer_rmsprop_+3A_rho">rho</code></td>
<td>
<p>float &gt;= 0. Decay factor.</p>
</td></tr>
<tr><td><code id="optimizer_rmsprop_+3A_epsilon">epsilon</code></td>
<td>
<p>float &gt;= 0. Fuzz factor. If 'NULL', defaults to 'k_epsilon()'.</p>
</td></tr>
<tr><td><code id="optimizer_rmsprop_+3A_decay">decay</code></td>
<td>
<p>float &gt;= 0. Learning rate decay over each update.</p>
</td></tr>
<tr><td><code id="optimizer_rmsprop_+3A_clipnorm">clipnorm</code></td>
<td>
<p>Gradients will be clipped when their L2 norm exceeds this
value.</p>
</td></tr>
<tr><td><code id="optimizer_rmsprop_+3A_clipvalue">clipvalue</code></td>
<td>
<p>Gradients will be clipped when their absolute value exceeds
this value.</p>
</td></tr>
<tr><td><code id="optimizer_rmsprop_+3A_...">...</code></td>
<td>
<p>Unused, present only for backwards compatability</p>
</td></tr>
</table>


<h3>Note</h3>

<p>This optimizer is usually a good choice for recurrent neural networks.
</p>


<h3>See Also</h3>

<p>Other optimizers: 
<code><a href="#topic+optimizer_adadelta">optimizer_adadelta</a>()</code>,
<code><a href="#topic+optimizer_adagrad">optimizer_adagrad</a>()</code>,
<code><a href="#topic+optimizer_adamax">optimizer_adamax</a>()</code>,
<code><a href="#topic+optimizer_adam">optimizer_adam</a>()</code>,
<code><a href="#topic+optimizer_nadam">optimizer_nadam</a>()</code>,
<code><a href="#topic+optimizer_sgd">optimizer_sgd</a>()</code>
</p>

<hr>
<h2 id='optimizer_sgd'>Stochastic gradient descent (SGD) optimizer</h2><span id='topic+optimizer_sgd'></span>

<h3>Description</h3>

<p>Stochastic gradient descent optimizer with support for momentum, learning
rate decay, and Nesterov momentum.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optimizer_sgd(
  learning_rate = 0.01,
  momentum = 0,
  decay = 0,
  nesterov = FALSE,
  clipnorm = NULL,
  clipvalue = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="optimizer_sgd_+3A_learning_rate">learning_rate</code></td>
<td>
<p>float &gt;= 0. Learning rate.</p>
</td></tr>
<tr><td><code id="optimizer_sgd_+3A_momentum">momentum</code></td>
<td>
<p>float &gt;= 0. Parameter that accelerates SGD in the relevant
direction and dampens oscillations.</p>
</td></tr>
<tr><td><code id="optimizer_sgd_+3A_decay">decay</code></td>
<td>
<p>float &gt;= 0. Learning rate decay over each update.</p>
</td></tr>
<tr><td><code id="optimizer_sgd_+3A_nesterov">nesterov</code></td>
<td>
<p>boolean. Whether to apply Nesterov momentum.</p>
</td></tr>
<tr><td><code id="optimizer_sgd_+3A_clipnorm">clipnorm</code></td>
<td>
<p>Gradients will be clipped when their L2 norm exceeds this
value.</p>
</td></tr>
<tr><td><code id="optimizer_sgd_+3A_clipvalue">clipvalue</code></td>
<td>
<p>Gradients will be clipped when their absolute value exceeds
this value.</p>
</td></tr>
<tr><td><code id="optimizer_sgd_+3A_...">...</code></td>
<td>
<p>Unused, present only for backwards compatability</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Based on:
[keras/R/optimizers.R](https://github.com/rstudio/keras/blob/main/R/optimizers.R).
The following code is commented:
<code>backcompat_fix_rename_lr_to_learning_rate(...)</code>
</p>


<h3>Value</h3>

<p>Optimizer for use with <code>compile.keras.engine.training.Model</code>.
</p>


<h3>Note</h3>

<p>To enable compatibility with the ranges of the learning rates
of the other optimizers, the learning rate <code>learning_rate</code>
is internally mapped to <code>10 * learning_rate</code>. That is,
a learning rat of 0.001 will be mapped to 0.01 (which is the default.)
</p>


<h3>See Also</h3>

<p>Other optimizers: 
<code><a href="#topic+optimizer_adadelta">optimizer_adadelta</a>()</code>,
<code><a href="#topic+optimizer_adagrad">optimizer_adagrad</a>()</code>,
<code><a href="#topic+optimizer_adamax">optimizer_adamax</a>()</code>,
<code><a href="#topic+optimizer_adam">optimizer_adam</a>()</code>,
<code><a href="#topic+optimizer_nadam">optimizer_nadam</a>()</code>,
<code><a href="#topic+optimizer_rmsprop">optimizer_rmsprop</a>()</code>
</p>

<hr>
<h2 id='plot_function_surface'>Surface plot</h2><span id='topic+plot_function_surface'></span>

<h3>Description</h3>

<p>A (filled) contour plot or perspective plot of a function, interactive via plotly.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_function_surface(
  f = function(x) {
     rowSums(x^2)
 },
  lower = c(0, 0),
  upper = c(1, 1),
  type = "filled.contour",
  s = 100,
  xlab = "x1",
  ylab = "x2",
  zlab = "y",
  color.palette = terrain.colors,
  title = " ",
  levels = NULL,
  points1,
  points2,
  pch1 = 20,
  pch2 = 8,
  lwd1 = 1,
  lwd2 = 1,
  cex1 = 1,
  cex2 = 1,
  col1 = "blue",
  col2 = "red",
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_function_surface_+3A_f">f</code></td>
<td>
<p>function to be plotted. The function should either be able to take two vectors or one matrix specifying sample locations. i.e. <code>z=f(X)</code> or <code>z=f(x2,x1)</code> where Z is a two column matrix containing the sample locations <code>x1</code> and <code>x2</code>.</p>
</td></tr>
<tr><td><code id="plot_function_surface_+3A_lower">lower</code></td>
<td>
<p>boundary for x1 and x2 (defaults to <code>c(0,0)</code>).</p>
</td></tr>
<tr><td><code id="plot_function_surface_+3A_upper">upper</code></td>
<td>
<p>boundary (defaults to <code>c(1,1)</code>).</p>
</td></tr>
<tr><td><code id="plot_function_surface_+3A_type">type</code></td>
<td>
<p>string describing the type of the plot:  <code>"filled.contour"</code> (default), <code>"contour"</code>,
<code>"persp"</code> (perspective), or <code>"persp3d"</code> plot.
Note that &quot;persp3d&quot; is based on the plotly package and will work in RStudio, but not in the standard RGui.</p>
</td></tr>
<tr><td><code id="plot_function_surface_+3A_s">s</code></td>
<td>
<p>number of samples along each dimension. e.g. <code>f</code> will be evaluated <code>s^2</code> times.</p>
</td></tr>
<tr><td><code id="plot_function_surface_+3A_xlab">xlab</code></td>
<td>
<p>lable of first axis</p>
</td></tr>
<tr><td><code id="plot_function_surface_+3A_ylab">ylab</code></td>
<td>
<p>lable of second axis</p>
</td></tr>
<tr><td><code id="plot_function_surface_+3A_zlab">zlab</code></td>
<td>
<p>lable of third axis</p>
</td></tr>
<tr><td><code id="plot_function_surface_+3A_color.palette">color.palette</code></td>
<td>
<p>colors used, default is <code>terrain.color</code></p>
</td></tr>
<tr><td><code id="plot_function_surface_+3A_title">title</code></td>
<td>
<p>of the plot</p>
</td></tr>
<tr><td><code id="plot_function_surface_+3A_levels">levels</code></td>
<td>
<p>number of levels for the plotted function value. Will be set automatically with default NULL.. (contour plots  only)</p>
</td></tr>
<tr><td><code id="plot_function_surface_+3A_points1">points1</code></td>
<td>
<p>can be omitted, but if given the points in this matrix are added to the plot in form of dots. Contour plots and persp3d only. Contour plots expect matrix with two columns for coordinates. 3Dperspective expects matrix with three columns, third column giving the corresponding observed value of the plotted function.</p>
</td></tr>
<tr><td><code id="plot_function_surface_+3A_points2">points2</code></td>
<td>
<p>can be omitted, but if given the points in this matrix are added to the plot in form of crosses. Contour plots and persp3d only.  Contour plots expect matrix with two columns for coordinates. 3Dperspective expects matrix with three columns, third column giving the corresponding observed value of the plotted function.</p>
</td></tr>
<tr><td><code id="plot_function_surface_+3A_pch1">pch1</code></td>
<td>
<p>pch (symbol) setting for points1 (default: 20). (contour plots only)</p>
</td></tr>
<tr><td><code id="plot_function_surface_+3A_pch2">pch2</code></td>
<td>
<p>pch (symbol) setting for points2 (default: 8). (contour plots only)</p>
</td></tr>
<tr><td><code id="plot_function_surface_+3A_lwd1">lwd1</code></td>
<td>
<p>line width for points1 (default: 1). (contour plots only)</p>
</td></tr>
<tr><td><code id="plot_function_surface_+3A_lwd2">lwd2</code></td>
<td>
<p>line width for points2 (default: 1). (contour plots only)</p>
</td></tr>
<tr><td><code id="plot_function_surface_+3A_cex1">cex1</code></td>
<td>
<p>cex for points1 (default: 1). (contour plots only)</p>
</td></tr>
<tr><td><code id="plot_function_surface_+3A_cex2">cex2</code></td>
<td>
<p>cex for points2 (default: 1). (contour plots only)</p>
</td></tr>
<tr><td><code id="plot_function_surface_+3A_col1">col1</code></td>
<td>
<p>color for points1 (default: &quot;black&quot;). (contour plots only)</p>
</td></tr>
<tr><td><code id="plot_function_surface_+3A_col2">col2</code></td>
<td>
<p>color for points2 (default: &quot;black&quot;). (contour plots only)</p>
</td></tr>
<tr><td><code id="plot_function_surface_+3A_...">...</code></td>
<td>
<p>additional parameters passed to <code>contour</code> or <code>filled.contour</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>plotly visualization (based on <code><a href="plotly.html#topic+plot_ly">plot_ly</a></code>)
</p>

<hr>
<h2 id='plot_parallel'>Parallel coordinate plot of a data set</h2><span id='topic+plot_parallel'></span>

<h3>Description</h3>

<p>mlrTools
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_parallel(
  object,
  yrange = NULL,
  yvar = 1,
  xlab = paste("x", 1:ncol(x), sep = ""),
  ylab = "y",
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_parallel_+3A_object">object</code></td>
<td>
<p>the result list returned by <code><a href="SPOT.html#topic+spot">spot</a></code>, importantly including a <code>modelFit</code>, and the data <code>x</code>, <code>y</code>.</p>
</td></tr>
<tr><td><code id="plot_parallel_+3A_yrange">yrange</code></td>
<td>
<p>a two-element vector that specifies the range of y values to consider (data outside of that range will be excluded).</p>
</td></tr>
<tr><td><code id="plot_parallel_+3A_yvar">yvar</code></td>
<td>
<p>integer which specifies the variable that is displayed on the color scale. yvar==1 (default) means that the y-variable is shown (tuned measure). Larger integers mean that respective columns from logInfo are used (i.e., yvar specifies the respective column number, starting with 2 for the first logged value).</p>
</td></tr>
<tr><td><code id="plot_parallel_+3A_xlab">xlab</code></td>
<td>
<p>a vector of characters, giving the labels for each of the two independent variables.</p>
</td></tr>
<tr><td><code id="plot_parallel_+3A_ylab">ylab</code></td>
<td>
<p>character, the value of the dependent variable predicted by the corresponding model.</p>
</td></tr>
<tr><td><code id="plot_parallel_+3A_...">...</code></td>
<td>
<p>additional parameters (currently unused).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>plotly parallel coordinate plot ('parcoords') visualization (based on <code><a href="plotly.html#topic+plot_ly">plot_ly</a></code>)
</p>


<h3>See Also</h3>

<p><code><a href="SPOT.html#topic+plotFunction">plotFunction</a></code>, <code><a href="SPOT.html#topic+plotData">plotData</a></code>
</p>

<hr>
<h2 id='plot_sensitivity'>Sensitivity plot of a model</h2><span id='topic+plot_sensitivity'></span>

<h3>Description</h3>

<p>mlrTools
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_sensitivity(
  object,
  s = 100,
  xlab = paste("x", 1:ncol(object$x), sep = ""),
  ylab = "y",
  type = "best",
  agg.sample = 100,
  agg.fun = mean,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_sensitivity_+3A_object">object</code></td>
<td>
<p>the result list returned by <code><a href="SPOT.html#topic+spot">spot</a></code>, importantly including a <code>modelFit</code>, and the data <code>x</code>, <code>y</code>.</p>
</td></tr>
<tr><td><code id="plot_sensitivity_+3A_s">s</code></td>
<td>
<p>number of samples along each dimension.</p>
</td></tr>
<tr><td><code id="plot_sensitivity_+3A_xlab">xlab</code></td>
<td>
<p>a vector of characters, giving the labels for each of the two independent variables.</p>
</td></tr>
<tr><td><code id="plot_sensitivity_+3A_ylab">ylab</code></td>
<td>
<p>character, the value of the dependent variable predicted by the corresponding model.</p>
</td></tr>
<tr><td><code id="plot_sensitivity_+3A_type">type</code></td>
<td>
<p>string describing the type of the plot:  <code>"best"</code> (default) shows sensitivity around optimum, <code>"contour"</code>,
<code>"persp"</code> (perspective), or <code>"persp3d"</code> plot.
Note that &quot;persp3d&quot; is based on the plotly package and will work in RStudio, but not in the standard RGui.</p>
</td></tr>
<tr><td><code id="plot_sensitivity_+3A_agg.sample">agg.sample</code></td>
<td>
<p>number of samples for aggregation type (type=&quot;agg&quot;).</p>
</td></tr>
<tr><td><code id="plot_sensitivity_+3A_agg.fun">agg.fun</code></td>
<td>
<p>function for aggregation (type=&quot;agg&quot;).</p>
</td></tr>
<tr><td><code id="plot_sensitivity_+3A_...">...</code></td>
<td>
<p>additional parameters (currently unused).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>plotly visualization (based on <code><a href="plotly.html#topic+plot_ly">plot_ly</a></code>)
</p>


<h3>See Also</h3>

<p><code><a href="SPOT.html#topic+plotFunction">plotFunction</a></code>, <code><a href="SPOT.html#topic+plotData">plotData</a></code>
</p>

<hr>
<h2 id='plot_surface'>Surface plot of a model</h2><span id='topic+plot_surface'></span>

<h3>Description</h3>

<p>A (filled) contour or perspective plot of a fitted model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_surface(
  object,
  which = if (ncol(object$x) &gt; 1 &amp; tolower(type) != "singledim") {
     1:2
 } else {
   
     1
 },
  constant = object$x[which.min(unlist(object$y)), ],
  xlab = paste("x", which, sep = ""),
  ylab = "y",
  type = "filled.contour",
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_surface_+3A_object">object</code></td>
<td>
<p>the result list returned by <code><a href="SPOT.html#topic+spot">spot</a></code>, importantly including a <code>modelFit</code>, and the data <code>x</code>, <code>y</code>.</p>
</td></tr>
<tr><td><code id="plot_surface_+3A_which">which</code></td>
<td>
<p>a vector with two elements, each an integer giving the two independent variables of the plot
(the integers are indices of the respective data set).</p>
</td></tr>
<tr><td><code id="plot_surface_+3A_constant">constant</code></td>
<td>
<p>a numeric vector that states for each variable a constant value that it will take on
if it is not varied in the plot. This affects the parameters not selected by the <code>which</code> parameter.
By default, this will be fixed to the best known solution, i.e., the one with minimal y-value, according
to <code>which.min(object$y)</code>. The length of this numeric vector should be the same as the number of columns in <code>object$x</code></p>
</td></tr>
<tr><td><code id="plot_surface_+3A_xlab">xlab</code></td>
<td>
<p>a vector of characters, giving the labels for each of the two independent variables.</p>
</td></tr>
<tr><td><code id="plot_surface_+3A_ylab">ylab</code></td>
<td>
<p>character, the value of the dependent variable predicted by the corresponding model.</p>
</td></tr>
<tr><td><code id="plot_surface_+3A_type">type</code></td>
<td>
<p>string describing the type of the plot:  <code>"filled.contour"</code> (default), <code>"contour"</code>,
<code>"persp"</code> (perspective), or <code>"persp3d"</code> plot.
Note that &quot;persp3d&quot; is based on the plotly package and will work in RStudio, but not in the standard RGui.</p>
</td></tr>
<tr><td><code id="plot_surface_+3A_...">...</code></td>
<td>
<p>additional parameters passed to the <code>contour</code> or <code>filled.contour</code> function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>plotly visualization (based on <code><a href="plotly.html#topic+plot_ly">plot_ly</a></code>)
</p>

<hr>
<h2 id='plotnice.spotTreeModel'>Plot a nice rpart tree model</h2><span id='topic+plotnice.spotTreeModel'></span>

<h3>Description</h3>

<p>Plot model produced by <code><a href="SPOT.html#topic+buildTreeModel">buildTreeModel</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotnice.spotTreeModel(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plotnice.spotTreeModel_+3A_x">x</code></td>
<td>
<p>tree model (settings and parameters) of class <code>spotTreeModel</code>.</p>
</td></tr>
<tr><td><code id="plotnice.spotTreeModel_+3A_...">...</code></td>
<td>
<p>parameters passed to rpart.plot plotting function</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The returned value is identical to that of <code><a href="rpart.plot.html#topic+prp">prp</a></code>.
</p>

<hr>
<h2 id='plotParallel'>Parallel coordinate plot of a data set</h2><span id='topic+plotParallel'></span>

<h3>Description</h3>

<p>Parallel plot based on <code><a href="GGally.html#topic+ggparcoord">ggparcoord</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotParallel(
  result,
  xlab = NULL,
  ylab = NULL,
  yrange = NULL,
  splineFactor = 1,
  colorOption = "A",
  scale = "uniminmax",
  boxplot = FALSE,
  alphaLines = 0.1,
  showPoints = TRUE,
  title = "",
  probs = seq(0.25, 0.75, 0.25),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plotParallel_+3A_result">result</code></td>
<td>
<p>the result list returned by <code><a href="SPOT.html#topic+spot">spot</a></code>,
importantly including the data <code>x</code>, <code>y</code>.</p>
</td></tr>
<tr><td><code id="plotParallel_+3A_xlab">xlab</code></td>
<td>
<p>character, the value of the independent variable</p>
</td></tr>
<tr><td><code id="plotParallel_+3A_ylab">ylab</code></td>
<td>
<p>character, the value of the dependent variable predicted by the corresponding model.</p>
</td></tr>
<tr><td><code id="plotParallel_+3A_yrange">yrange</code></td>
<td>
<p>a two-element vector that specifies the range of y values to consider (data outside of that range will be excluded).</p>
</td></tr>
<tr><td><code id="plotParallel_+3A_splinefactor">splineFactor</code></td>
<td>
<p>logical or numeric operator indicating whether spline interpolation should be used.
Numeric values will multiplied by the number of columns, TRUE will default to cubic interpolation,
AsIs to set the knot count directly and 0, FALSE, or non-numeric values will not use spline interpolation. See
<code><a href="GGally.html#topic+ggparcoord">ggparcoord</a></code>. Default: <code>"A"</code>.</p>
</td></tr>
<tr><td><code id="plotParallel_+3A_coloroption">colorOption</code></td>
<td>
<p>A character string indicating the colormap
option to use. Four options are available:
&quot;magma&quot; (or &quot;A&quot;),
&quot;inferno&quot; (or &quot;B&quot;),
&quot;plasma&quot; (or &quot;C&quot;),
&quot;viridis&quot; (or &quot;D&quot;, the default option) and
&quot;cividis&quot; (or &quot;E&quot;). See <code><a href="ggplot2.html#topic+scale_colour_viridis_d">scale_colour_viridis_d</a></code></p>
</td></tr>
<tr><td><code id="plotParallel_+3A_scale">scale</code></td>
<td>
<p>method used to scale the variables. Default: <code>"uniminmax"</code>.</p>
</td></tr>
<tr><td><code id="plotParallel_+3A_boxplot">boxplot</code></td>
<td>
<p>logical operator indicating whether or not boxplots should
underlay the distribution of each variable</p>
</td></tr>
<tr><td><code id="plotParallel_+3A_alphalines">alphaLines</code></td>
<td>
<p>value of alpha scaler for the lines of the parcoord plot or
a column name of the data. Default: 0.1</p>
</td></tr>
<tr><td><code id="plotParallel_+3A_showpoints">showPoints</code></td>
<td>
<p>logical operator indicating whether points
should be plotted or not. Default: TRUE</p>
</td></tr>
<tr><td><code id="plotParallel_+3A_title">title</code></td>
<td>
<p>character string denoting the title of the plot. Default:
<code>""</code>.</p>
</td></tr>
<tr><td><code id="plotParallel_+3A_probs">probs</code></td>
<td>
<p>quantile probabilities. Default:  <code>seq(0, 1, 0.25)</code></p>
</td></tr>
<tr><td><code id="plotParallel_+3A_...">...</code></td>
<td>
<p>additional parameters to be passed to
<code><a href="GGally.html#topic+ggparcoord">ggparcoord</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>plotly parallel coordinate plot ('parcoords') visualization (based on
<code><a href="plotly.html#topic+plot_ly">plot_ly</a></code>)
</p>


<h3>See Also</h3>

<p><code><a href="SPOT.html#topic+plotFunction">plotFunction</a></code>, <code><a href="SPOT.html#topic+plotData">plotData</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
require("SPOT")
res &lt;- spot(x=NULL,
             funSphere,
             lower=rep(-1,3),
             upper=rep(1,3),
             control=list(funEvals=25))
plotParallel(res, scale="std")

</code></pre>

<hr>
<h2 id='plotSensitivity'>Sensitivity ggplot of a model</h2><span id='topic+plotSensitivity'></span>

<h3>Description</h3>

<p>Generates a sensitivity plot.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotSensitivity(
  object,
  s = 100,
  xlab = paste("x", 1:ncol(object$x), sep = ""),
  ylab = "y",
  type = "best",
  agg.sample = 100,
  agg.fun = mean,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plotSensitivity_+3A_object">object</code></td>
<td>
<p>the result list returned by
<code><a href="SPOT.html#topic+spot">spot</a></code>, importantly including a <code>modelFit</code>, and the data <code>x</code>, <code>y</code>.</p>
</td></tr>
<tr><td><code id="plotSensitivity_+3A_s">s</code></td>
<td>
<p>number of samples along each dimension.</p>
</td></tr>
<tr><td><code id="plotSensitivity_+3A_xlab">xlab</code></td>
<td>
<p>a vector of characters, giving the labels for each of the two independent variables.</p>
</td></tr>
<tr><td><code id="plotSensitivity_+3A_ylab">ylab</code></td>
<td>
<p>character, the value of the dependent variable predicted by the corresponding model.</p>
</td></tr>
<tr><td><code id="plotSensitivity_+3A_type">type</code></td>
<td>
<p>string describing the type of the plot:
<code>"best"</code> (default) shows sensitivity around optimum,
<code>"contour"</code>,</p>
</td></tr>
<tr><td><code id="plotSensitivity_+3A_agg.sample">agg.sample</code></td>
<td>
<p>number of samples for aggregation type
(type=&quot;agg&quot;).</p>
</td></tr>
<tr><td><code id="plotSensitivity_+3A_agg.fun">agg.fun</code></td>
<td>
<p>function for aggregation (type=&quot;agg&quot;).</p>
</td></tr>
<tr><td><code id="plotSensitivity_+3A_...">...</code></td>
<td>
<p>additional parameters (currently unused).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>ggplot2 visualization
</p>


<h3>See Also</h3>

<p><code><a href="SPOT.html#topic+plotFunction">plotFunction</a></code>,
<code><a href="SPOT.html#topic+plotData">plotData</a></code>
</p>

<hr>
<h2 id='predDlCensus'>Predict deep learning models on Census data</h2><span id='topic+predDlCensus'></span>

<h3>Description</h3>

<p>Predict deep learning models on Census data
</p>


<h3>Usage</h3>

<pre><code class='language-R'>predDlCensus(
  x = NULL,
  target = "age",
  task.type = "classif",
  nobs = 10000,
  nfactors = "high",
  nnumericals = "high",
  cardinality = "high",
  cachedir = "oml.cache",
  k = 1,
  prop = 2/3,
  batch_size = 32,
  verbosity = 0
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predDlCensus_+3A_x">x</code></td>
<td>
<p>matrix with untransformed hyperparameters, e.g., result from <code><a href="SPOT.html#topic+spot">spot</a></code>.
Hyperparameters will be transformed in predDlCensus with <code><a href="SPOT.html#topic+transformX">transformX</a></code>
and transformations defined in <code><a href="#topic+getModelConf">getModelConf</a></code>.</p>
</td></tr>
<tr><td><code id="predDlCensus_+3A_target">target</code></td>
<td>
<p>target</p>
</td></tr>
<tr><td><code id="predDlCensus_+3A_task.type">task.type</code></td>
<td>
<p>class/reg</p>
</td></tr>
<tr><td><code id="predDlCensus_+3A_nobs">nobs</code></td>
<td>
<p>number of obsvervations, max: 229,285</p>
</td></tr>
<tr><td><code id="predDlCensus_+3A_nfactors">nfactors</code></td>
<td>
<p>(character) number of factor variables</p>
</td></tr>
<tr><td><code id="predDlCensus_+3A_nnumericals">nnumericals</code></td>
<td>
<p>(character) number of numerical variables</p>
</td></tr>
<tr><td><code id="predDlCensus_+3A_cardinality">cardinality</code></td>
<td>
<p>(character) cardinality</p>
</td></tr>
<tr><td><code id="predDlCensus_+3A_cachedir">cachedir</code></td>
<td>
<p>cache directory</p>
</td></tr>
<tr><td><code id="predDlCensus_+3A_k">k</code></td>
<td>
<p>number of repeats</p>
</td></tr>
<tr><td><code id="predDlCensus_+3A_prop">prop</code></td>
<td>
<p>vector. proportion between train / test and train/val. Default: <code>2/3</code>. If one
value is given, the same proportion will be used for both splits. Otherwise, the first
entry is used for the test/training split and the second value for the training/validation
split. If the second value is 1, the validation set is empty.
Given <code>prop = (p1,p2)</code>, the data will be partitioned as shown in the following two steps:
</p>

<dl>
<dt>Step 1:</dt><dd><p><code>train1 = p1*data</code> and <code>test = )(1-p1)*data</code></p>
</dd>
<dt>Step 2:</dt><dd><p><code>train2 = p2*train1 = p2*p1*data</code> and <code>val = )(1-p2)*train1 = (1-p2)*p1*data</code></p>
</dd>
</dl>

<p>Note: If <code>p2=1</code>, no validation data will be generated.</p>
</td></tr>
<tr><td><code id="predDlCensus_+3A_batch_size">batch_size</code></td>
<td>
<p>batch_size. Default: <code>32</code>.</p>
</td></tr>
<tr><td><code id="predDlCensus_+3A_verbosity">verbosity</code></td>
<td>
<p>verbosity. Default: 0</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list of matrices with true and predicted values.
</p>

<dl>
<dt><code>trueY</code></dt><dd><p>true values</p>
</dd>
<dt><code>hatY</code></dt><dd><p>predicted values</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>
### These examples require an activated Python environment as described in
### Bartz-Beielstein, T., Rehbach, F., Sen, A., and Zaefferer, M.:
### Surrogate Model Based Hyperparameter Tuning for Deep Learning with SPOT,
### June 2021. http://arxiv.org/abs/2105.14625.
PYTHON_RETICULATE &lt;- FALSE
if(PYTHON_RETICULATE){
cfg &lt;- getModelConf(list(model="dl"))
x &lt;- matrix(cfg$defaults, nrow=1)
res &lt;- predDlCensus(x=x, k=2)
}

</code></pre>

<hr>
<h2 id='predMlCensus'>Predict machine learning models on Census data</h2><span id='topic+predMlCensus'></span>

<h3>Description</h3>

<p>Predict machine learning models on Census data
</p>


<h3>Usage</h3>

<pre><code class='language-R'>predMlCensus(
  x = NULL,
  model = NULL,
  target = "age",
  task.type = "classif",
  nobs = 10000,
  nfactors = "high",
  nnumericals = "high",
  cardinality = "high",
  cachedir = "oml.cache",
  k = 1,
  prop = 2/3,
  verbosity = 0
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predMlCensus_+3A_x">x</code></td>
<td>
<p>matrix hyperparameter, e.g., result from <code><a href="SPOT.html#topic+spot">spot</a></code>
Load result data for ml model to get the hyperparamater vector x, e.g.,
<code>load("data/resdl11.RData")</code> and
<code>x &lt;- result$xbest</code> or use default.</p>
</td></tr>
<tr><td><code id="predMlCensus_+3A_model">model</code></td>
<td>
<p>character ml model, e.g., <code>"kknn"</code>
run: <code>result$xbest</code>. If <code>NULL</code>, default parameters will be used.
Default: <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="predMlCensus_+3A_target">target</code></td>
<td>
<p>target</p>
</td></tr>
<tr><td><code id="predMlCensus_+3A_task.type">task.type</code></td>
<td>
<p>class/reg</p>
</td></tr>
<tr><td><code id="predMlCensus_+3A_nobs">nobs</code></td>
<td>
<p>number of obsvervations, max: 229,285</p>
</td></tr>
<tr><td><code id="predMlCensus_+3A_nfactors">nfactors</code></td>
<td>
<p>(character) number of factor variables</p>
</td></tr>
<tr><td><code id="predMlCensus_+3A_nnumericals">nnumericals</code></td>
<td>
<p>(character) number of numerical variables</p>
</td></tr>
<tr><td><code id="predMlCensus_+3A_cardinality">cardinality</code></td>
<td>
<p>(character) cardinality</p>
</td></tr>
<tr><td><code id="predMlCensus_+3A_cachedir">cachedir</code></td>
<td>
<p>cachedir</p>
</td></tr>
<tr><td><code id="predMlCensus_+3A_k">k</code></td>
<td>
<p>number of repeats</p>
</td></tr>
<tr><td><code id="predMlCensus_+3A_prop">prop</code></td>
<td>
<p>split proportion. Default: <code>c(3/5,1)</code>.</p>
</td></tr>
<tr><td><code id="predMlCensus_+3A_verbosity">verbosity</code></td>
<td>
<p>verbsity. Default: 0</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list of matrices with predictions and true values
</p>

<hr>
<h2 id='prepare_data_plot'>Prepare data for plots</h2><span id='topic+prepare_data_plot'></span>

<h3>Description</h3>

<p>Data preparation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prepare_data_plot(
  model = buildRanger,
  modelControl = list(),
  x,
  namesx = paste("x", 1:ncol(x), sep = ""),
  y,
  namesy = "y",
  log = NULL,
  nameslog = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prepare_data_plot_+3A_model">model</code></td>
<td>
<p>a function that can be used to build a model based on the data,
e.g. : <code>buildRanger</code> or <code>buildKriging</code>. Default is <code>buildRanger</code>, since it is fast and robust.</p>
</td></tr>
<tr><td><code id="prepare_data_plot_+3A_modelcontrol">modelControl</code></td>
<td>
<p>a list of control settings for the respective model.
Default is an empty list (use default model controls).</p>
</td></tr>
<tr><td><code id="prepare_data_plot_+3A_x">x</code></td>
<td>
<p>a matrix of x-values to be plotted
(i.e., columns are the independent variables, rows are samples). Should have same number of rows as y and log.</p>
</td></tr>
<tr><td><code id="prepare_data_plot_+3A_namesx">namesx</code></td>
<td>
<p>character vector, printable names for the x data. Should have same length as x has columns. Default is x1, x2, ...</p>
</td></tr>
<tr><td><code id="prepare_data_plot_+3A_y">y</code></td>
<td>
<p>a one-column matrix of y-values to be plotted (dependent variable). Should have same number of rows as x and log.</p>
</td></tr>
<tr><td><code id="prepare_data_plot_+3A_namesy">namesy</code></td>
<td>
<p>character, giving a printable name for y. Default is &quot;y&quot;.</p>
</td></tr>
<tr><td><code id="prepare_data_plot_+3A_log">log</code></td>
<td>
<p>matrix, a data set providing (optional) additional dependent variables (but these are not modeled). Should have same number of rows as y and x.</p>
</td></tr>
<tr><td><code id="prepare_data_plot_+3A_nameslog">nameslog</code></td>
<td>
<p>character vector, printable names for the log data. Should have same length as log has columns. Default is NULL (no names).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list with plotting data and information
</p>

<hr>
<h2 id='prepare_spot_result_plot'>Prepare data (results from a tuning run) for plots</h2><span id='topic+prepare_spot_result_plot'></span>

<h3>Description</h3>

<p>Preparation of the list elements used for plotting.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prepare_spot_result_plot(data, model = buildRanger, modelControl = list(), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prepare_spot_result_plot_+3A_data">data</code></td>
<td>
<p>a list containing the various data, e.g., as produced by a <code><a href="SPOT.html#topic+spot">spot</a></code> call.</p>
</td></tr>
<tr><td><code id="prepare_spot_result_plot_+3A_model">model</code></td>
<td>
<p>a function that can be used to build a model based on the data, e.g. : <code>buildRanger</code>
or <code>buildKriging</code>. Default is <code>buildRanger</code>, since it is fast and robust.</p>
</td></tr>
<tr><td><code id="prepare_spot_result_plot_+3A_modelcontrol">modelControl</code></td>
<td>
<p>a list of control settings for the respective model.
Default is an empty list (use default model controls).</p>
</td></tr>
<tr><td><code id="prepare_spot_result_plot_+3A_...">...</code></td>
<td>
<p>additional parameters passed to <code><a href="#topic+prepare_data_plot">prepare_data_plot</a></code>:
<code>namesx</code>, <code>namesy</code>, <code>nameslog</code> character vectors providing names for x, y and logInfo data.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list with plotting data and information generated with <code><a href="#topic+prepare_data_plot">prepare_data_plot</a></code>
</p>

<hr>
<h2 id='prepareComparisonPlot'>prepare data frame for comparisons (boxplots, violin plots)</h2><span id='topic+prepareComparisonPlot'></span>

<h3>Description</h3>

<p>converts <code>result</code> from a <code><a href="SPOT.html#topic+spot">spot</a></code> run
into the long format for ggplot.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prepareComparisonPlot(
  runNrMl,
  runNrDl,
  directory,
  defaultModelList = list("dl", "cvglmnet", "kknn", "ranger", "rpart", "svm", "xgboost"),
  tunedModelList = list("dl", "cvglmnet", "kknn", "ranger", "rpart", "svm", "xgboost")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prepareComparisonPlot_+3A_runnrml">runNrMl</code></td>
<td>
<p>run number (character) of ml models</p>
</td></tr>
<tr><td><code id="prepareComparisonPlot_+3A_runnrdl">runNrDl</code></td>
<td>
<p>run number (character) of dl models</p>
</td></tr>
<tr><td><code id="prepareComparisonPlot_+3A_directory">directory</code></td>
<td>
<p>location of the (non-default, e.g., tuned) parameter file</p>
</td></tr>
<tr><td><code id="prepareComparisonPlot_+3A_defaultmodellist">defaultModelList</code></td>
<td>
<p>default model list. Default: <code>list("dl", "cvglmnet", "kknn",
"ranger", "rpart" ,  "svm", "xgboost")</code></p>
</td></tr>
<tr><td><code id="prepareComparisonPlot_+3A_tunedmodellist">tunedModelList</code></td>
<td>
<p>tuned model list. Default: <code>list("dl", "cvglmnet", "kknn",
"ranger", "rpart" ,  "svm", "xgboost")</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>data frame with results:
</p>

<dl>
<dt><code>x</code></dt><dd><p>integer representing step</p>
</dd>
<dt><code>y</code></dt><dd><p>corresponding function value at step x.</p>
</dd>
<dt><code>name</code></dt><dd><p>ml/dl model name, e.g., ranger</p>
</dd>
<dt><code>size</code></dt><dd><p>initial design size.</p>
</dd>
<dt><code>yInitMin</code></dt><dd><p>min y value before SMBO is started, based on the
initial design only.</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>
### These examples require an activated Python environment as described in
### Bartz-Beielstein, T., Rehbach, F., Sen, A., and Zaefferer, M.:
### Surrogate Model Based Hyperparameter Tuning for Deep Learning with SPOT,
### June 2021. http://arxiv.org/abs/2105.14625.
PYTHON_RETICULATE &lt;- FALSE
if(PYTHON_RETICULATE){
runNrMl &lt;- list("15")
runNrDl &lt;- list("28")
directory &lt;- "../book/data"
prepareComparisonPlot(runNrMl,
                    runNrDl,
                    directory)
}

</code></pre>

<hr>
<h2 id='prepareProgressPlot'>prepare data frame for progress plot</h2><span id='topic+prepareProgressPlot'></span>

<h3>Description</h3>

<p>converts <code>result</code> from a <code><a href="SPOT.html#topic+spot">spot</a></code> run into the long format.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prepareProgressPlot(modelList, runNr, directory = NULL, maxY = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prepareProgressPlot_+3A_modellist">modelList</code></td>
<td>
<p>ml/dl model (character)</p>
</td></tr>
<tr><td><code id="prepareProgressPlot_+3A_runnr">runNr</code></td>
<td>
<p>run number (character)</p>
</td></tr>
<tr><td><code id="prepareProgressPlot_+3A_directory">directory</code></td>
<td>
<p>location of the (non-default, e.g., tuned) parameter file. Note:
load result only when directory is specified, otherwise use (only one!) result from the workspace.</p>
</td></tr>
<tr><td><code id="prepareProgressPlot_+3A_maxy">maxY</code></td>
<td>
<p>max number of y values. If <code>NULL</code> then all y values are used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data frame with results:
</p>

<dl>
<dt><code>x</code></dt><dd><p>integer representing step</p>
</dd>
<dt><code>y</code></dt><dd><p>corresponding function value at step x.</p>
</dd>
<dt><code>name</code></dt><dd><p>ml/dl model name, e.g., ranger</p>
</dd>
<dt><code>size</code></dt><dd><p>initial design size.</p>
</dd>
<dt><code>yInitMin</code></dt><dd><p>min y value before SMBO is started, based on the
initial design only.</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>
modelList &lt;- list("resDl")
runNr &lt;- list("100")
result &lt;- resDl100
directory &lt;- NULL
prepareProgressPlot(modelList,
                    runNr,
                    directory)

</code></pre>

<hr>
<h2 id='printf'>formatted output</h2><span id='topic+printf'></span>

<h3>Description</h3>

<p>Combine <code><a href="base.html#topic+sprintf">sprintf</a></code> and <code><a href="base.html#topic+writeLines">writeLines</a></code> to
generate formatted output
</p>


<h3>Usage</h3>

<pre><code class='language-R'>printf(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="printf_+3A_...">...</code></td>
<td>
<p>output to be printed</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
x &lt;- 123
printf("x value: %d", x)

</code></pre>

<hr>
<h2 id='printFLAGS'>Print parameter values from FLAG list</h2><span id='topic+printFLAGS'></span>

<h3>Description</h3>

<p>Simple print method for FLAG list.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>printFLAGS(FLAGS)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="printFLAGS_+3A_flags">FLAGS</code></td>
<td>
<p>list of parameters, see <code><a href="#topic+mapX2FLAGS">mapX2FLAGS</a></code></p>
</td></tr>
</table>

<hr>
<h2 id='resDl100'>Results from the spot() run dl100</h2><span id='topic+resDl100'></span>

<h3>Description</h3>

<p>Details and the corresponding R code to generate the data
can be found in the package vignette <code>SPOTMiscVignette.Rmd</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>resDl100
</code></pre>


<h3>Format</h3>

<p>A list of 9 values
</p>

<dl>
<dt>xbest</dt><dd><p>num:</p>
</dd>
<dt>ybest</dt><dd><p>num:</p>
</dd>
<dt>x</dt><dd><p>num</p>
</dd>
<dt>y</dt><dd><p>num</p>
</dd>
<dt>logInfo</dt><dd><p>logi</p>
</dd>
<dt>count</dt><dd><p>int</p>
</dd>
<dt>msg</dt><dd><p>chr</p>
</dd>
<dt>model fit</dt><dd><p>List of 13</p>
</dd>
<dt>ybestVec</dt><dd><p>num</p>
</dd>
</dl>


<hr>
<h2 id='RMSE'>root mean squared errors</h2><span id='topic+RMSE'></span>

<h3>Description</h3>

<p>root mean squared errors
</p>


<h3>Usage</h3>

<pre><code class='language-R'>RMSE(y, yhat)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="RMSE_+3A_y">y</code></td>
<td>
<p>actual value</p>
</td></tr>
<tr><td><code id="RMSE_+3A_yhat">yhat</code></td>
<td>
<p>predicted value</p>
</td></tr>
</table>


<h3>Value</h3>

<p>root mean squared errors
</p>

<hr>
<h2 id='scorePredictions'>Score results from pred</h2><span id='topic+scorePredictions'></span>

<h3>Description</h3>

<p>errors for <code>(actual, predicted)</code> values.
Based on package <code>Metrics</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>scorePredictions(val)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="scorePredictions_+3A_val">val</code></td>
<td>
<p>list of matrices with true and
predicted values, e.g., output from
<code><a href="#topic+predMlCensus">predMlCensus</a></code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>matrix with scores
</p>

<hr>
<h2 id='selectKerasActivation'>Select keras activation function</h2><span id='topic+selectKerasActivation'></span>

<h3>Description</h3>

<p>Select keras activation function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>selectKerasActivation(activation)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="selectKerasActivation_+3A_activation">activation</code></td>
<td>
<p>integer specifying the activation function. Can be one of the following:
<code>1=NULL</code>, <code>2=RELU</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>activation function use with <code><a href="#topic+funKerasMnist">funKerasMnist</a></code>.
</p>

<hr>
<h2 id='selectKerasOptimizer'>Select keras optimizer</h2><span id='topic+selectKerasOptimizer'></span>

<h3>Description</h3>

<p>Select one of the following optimizers:
&quot;SDG&quot;, &quot;RMSPROP&quot;, &quot;ADAGRAD&quot;, &quot;ADADELTA&quot;, &quot;ADAM&quot;, &quot;ADAMAX&quot;, &quot;NADAM&quot;.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>selectKerasOptimizer(
  optimizer,
  learning_rate = 0.01,
  momentum = 0,
  decay = 0,
  nesterov = FALSE,
  clipnorm = NULL,
  clipvalue = NULL,
  rho = 0.9,
  epsilon = NULL,
  beta_1 = 0.9,
  beta_2 = 0.999,
  amsgrad = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="selectKerasOptimizer_+3A_optimizer">optimizer</code></td>
<td>
<p>integer specifying the algorithm. Can be one of the following:
<code>1=SDG</code>, <code>2=RMSPROP</code>, <code>3=ADAGRAD</code>, <code>4=ADADELTA</code>,
<code>5=ADAM</code>, <code>6=ADAMAX</code>, or <code>7=NADAM</code>.
</p>
<p>## SGD:</p>
</td></tr>
<tr><td><code id="selectKerasOptimizer_+3A_learning_rate">learning_rate</code></td>
<td>
<p>float &gt;= 0. Learning rate.</p>
</td></tr>
<tr><td><code id="selectKerasOptimizer_+3A_momentum">momentum</code></td>
<td>
<p>float &gt;= 0. Parameter that accelerates SGD in the relevant
direction and dampens oscillations.</p>
</td></tr>
<tr><td><code id="selectKerasOptimizer_+3A_decay">decay</code></td>
<td>
<p>float &gt;= 0. Learning rate decay over each update.</p>
</td></tr>
<tr><td><code id="selectKerasOptimizer_+3A_nesterov">nesterov</code></td>
<td>
<p>boolean. Whether to apply Nesterov momentum.</p>
</td></tr>
<tr><td><code id="selectKerasOptimizer_+3A_clipnorm">clipnorm</code></td>
<td>
<p>Gradients will be clipped when their L2 norm exceeds this
value.</p>
</td></tr>
<tr><td><code id="selectKerasOptimizer_+3A_clipvalue">clipvalue</code></td>
<td>
<p>Gradients will be clipped when their absolute value exceeds
this value.
</p>
<p>### RMS:</p>
</td></tr>
<tr><td><code id="selectKerasOptimizer_+3A_rho">rho</code></td>
<td>
<p>float &gt;= 0. Decay factor.</p>
</td></tr>
<tr><td><code id="selectKerasOptimizer_+3A_epsilon">epsilon</code></td>
<td>
<p>float &gt;= 0. Fuzz factor. If 'NULL', defaults to 'k_epsilon()'.
</p>
<p>### ADAM:</p>
</td></tr>
<tr><td><code id="selectKerasOptimizer_+3A_beta_1">beta_1</code></td>
<td>
<p>The exponential decay rate for the 1st moment estimates. float,
0 &lt; beta &lt; 1. Generally close to 1.</p>
</td></tr>
<tr><td><code id="selectKerasOptimizer_+3A_beta_2">beta_2</code></td>
<td>
<p>The exponential decay rate for the 2nd moment estimates. float,
0 &lt; beta &lt; 1. Generally close to 1.</p>
</td></tr>
<tr><td><code id="selectKerasOptimizer_+3A_amsgrad">amsgrad</code></td>
<td>
<p>Whether to apply the AMSGrad variant of this algorithm from
the paper &quot;On the Convergence of Adam and Beyond&quot;.</p>
</td></tr>
<tr><td><code id="selectKerasOptimizer_+3A_...">...</code></td>
<td>
<p>Unused, present only for backwards compatability</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Optimizer for use with <code>compile.keras.engine.training.Model</code>.
</p>

<hr>
<h2 id='selectTarget'>Select target variable in a data frame</h2><span id='topic+selectTarget'></span>

<h3>Description</h3>

<p>Select target variable in a data frame
</p>


<h3>Usage</h3>

<pre><code class='language-R'>selectTarget(df, target)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="selectTarget_+3A_df">df</code></td>
<td>
<p>data frame</p>
</td></tr>
<tr><td><code id="selectTarget_+3A_target">target</code></td>
<td>
<p>character specification of the target variable</p>
</td></tr>
</table>


<h3>Value</h3>

<p>df with entry target
</p>


<h3>Examples</h3>

<pre><code class='language-R'>df &lt;- data.frame(cbind(x=1:2,
                 y=3:4))
df &lt;- selectTarget(df=df, target="y")

</code></pre>

<hr>
<h2 id='sequentialBifurcation'>Sequential Bifurcation</h2><span id='topic+sequentialBifurcation'></span>

<h3>Description</h3>

<p><code>sequentialBifurcation</code> is a wrapper function to
<code><a href="sensitivity.html#topic+sb">sb</a></code> from the <code><a href="caret.html#topic+sensitivity">sensitivity</a></code> package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sequentialBifurcation(
  fun,
  lower,
  upper,
  k,
  interaction = FALSE,
  verbosity = 0,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sequentialBifurcation_+3A_fun">fun</code></td>
<td>
<p>function</p>
</td></tr>
<tr><td><code id="sequentialBifurcation_+3A_lower">lower</code></td>
<td>
<p>bound of natural variables. Determines the number of parameters (variables).</p>
</td></tr>
<tr><td><code id="sequentialBifurcation_+3A_upper">upper</code></td>
<td>
<p>bound of natural variables</p>
</td></tr>
<tr><td><code id="sequentialBifurcation_+3A_k">k</code></td>
<td>
<p><code>integer</code>  bifurcations. Must be smaller than the number of parameters.</p>
</td></tr>
<tr><td><code id="sequentialBifurcation_+3A_interaction">interaction</code></td>
<td>
<p><code>logical</code> TRUE if two-factor interactions should be considered. Default is
<code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="sequentialBifurcation_+3A_verbosity">verbosity</code></td>
<td>
<p><code>integer</code>. If larger than zero, the designs are shown.</p>
</td></tr>
<tr><td><code id="sequentialBifurcation_+3A_...">...</code></td>
<td>
<p>optional parameters passed to fun</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The model without interaction is
<code>Y = beta_0 + sum_{i=1}^p beta_i X_i</code>,
while the model with two-factor interactions is
<code>Y = beta_0 + sum_{i=1}^p beta_i X_i + sum_{1 &lt;= i &lt; j &lt;= p} gamma_{ij} X_i X_j</code>.
In both cases, the factors are assumed to be uniformly distributed on [-1,1].
This is a difference with Bettonvil et al. where the factors vary across [0,1] in the former case, while [-1,1] in the latter.
Another difference with Bettonvil et al. is that in the current implementation, the groups are splitted right in the middle.
</p>


<h3>Value</h3>

<p>sa <code>list</code> with sensitivity information (effects) for subgroups.
</p>


<h3>References</h3>

<p>B. Bettonvil and J. P. C. Kleijnen, 1996,
Searching for important factors in simulation models with many factors:
sequential bifurcations,
European Journal of Operational Research, 96, 180194.
</p>

<hr>
<h2 id='spotKeras'>spotKEras</h2><span id='topic+spotKeras'></span>

<h3>Description</h3>

<p>A wrapper that calls SPOT when optimizing a keras model with data
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spotKeras(x = NULL, fun, lower, upper, control, kerasConf, kerasData, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="spotKeras_+3A_x">x</code></td>
<td>
<p>is an optional start point (or set of start points), specified as a matrix.
One row for each point, and one column for each optimized parameter.</p>
</td></tr>
<tr><td><code id="spotKeras_+3A_fun">fun</code></td>
<td>
<p>is the objective function. It should receive a matrix x and return a matrix y.
In case the function uses external code and is noisy, an additional seed parameter may be used, see the <code>control$seedFun</code> argument below for details.
Mostly, fun must have format y = f(x, ...). If a noisy function requires some specific seed handling, e.g., in some other non-R code,
a seed can be passed to fun. For that purpose, the user must specify <code>control$noise = TRUE</code> and fun should be <code>fun(x, seed, ...)</code></p>
</td></tr>
<tr><td><code id="spotKeras_+3A_lower">lower</code></td>
<td>
<p>is a vector that defines the lower boundary of search space.
This determines also the dimensionality of the problem.</p>
</td></tr>
<tr><td><code id="spotKeras_+3A_upper">upper</code></td>
<td>
<p>is a vector that defines the upper boundary of search space.</p>
</td></tr>
<tr><td><code id="spotKeras_+3A_control">control</code></td>
<td>
<p>is a list with control settings for spot. See <code>spotControl</code>.</p>
</td></tr>
<tr><td><code id="spotKeras_+3A_kerasconf">kerasConf</code></td>
<td>
<p>List of additional parameters passed to keras as described in <code><a href="#topic+getKerasConf">getKerasConf</a></code>.</p>
</td></tr>
<tr><td><code id="spotKeras_+3A_kerasdata">kerasData</code></td>
<td>
<p>dataset to use</p>
</td></tr>
<tr><td><code id="spotKeras_+3A_...">...</code></td>
<td>
<p>additional parameters passed to <code>fun</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>This function returns a result list.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
### These examples require an activated Python environment as described in
### Bartz-Beielstein, T., Rehbach, F., Sen, A., and Zaefferer, M.:
### Surrogate Model Based Hyperparameter Tuning for Deep Learning with SPOT,
### June 2021. http://arxiv.org/abs/2105.14625.
PYTHON_RETICULATE &lt;- FALSE
if(PYTHON_RETICULATE){

model &lt;- "dl"
activeVars &lt;- c("layers", "units", "epochs")
kerasConf &lt;- getKerasConf()
kerasConf$active &lt;-  activeVars
cfg &lt;-  getModelConf("dl", active = activeVars)
lower &lt;- cfg$lower
upper &lt;- cfg$upper
types &lt;- cfg$type
result &lt;- spotKeras(x = NULL,
                  fun = funKerasMnist,
                        lower = lower,
                        upper = upper,
                        control = list(funEvals = 2,
                                       noise = TRUE,
                                       types = types,
                                       plots = FALSE,
                                       progress = TRUE,
                                       seedFun = 1,
                                       seedSPOT = 1,
                                       designControl = list(size = 1)),
                         kerasConf = kerasConf,
                         kerasData = getMnistData(kerasConf))
# The result does contain the active parameters only. To get the full vector, use
active2All(x=result$xbest, a=activeVars, model=model)
}

</code></pre>

<hr>
<h2 id='spotPlot'>spot plot (generic function)</h2><span id='topic+spotPlot'></span>

<h3>Description</h3>

<p>A wrapper function for available plotting options in SPOT and SPOTMisc.
Plotting functions from SPOT -&gt; plotdata, plotModel, plotFunction.
Plotting functions from SPOTMisc -&gt; plot_parallel, plot_sensitivity.
</p>
<p>spotPlot provides a higher level of abstraction and the users can use every plotting
function only by calling spotPlot.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spotPlot(plotType, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="spotPlot_+3A_plottype">plotType</code></td>
<td>
<p>function type to be called. It should be given as either
&quot;data&quot;, &quot;model&quot;, &quot;fun&quot;, &quot;parallel&quot; or &quot;sensitivity&quot;.
Otherwise the function returns an error message.</p>
</td></tr>
<tr><td><code id="spotPlot_+3A_...">...</code></td>
<td>
<p>additional parameters passed to <code>plotData</code> or <code>plotModel</code>,
<code>plotFunction</code>, <code>plot_parallel</code> or <code>plot_sensitivity</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Alpar Guer <a href="mailto:alpar.guer@smail.th-koeln.de">alpar.guer@smail.th-koeln.de</a>
</p>


<h3>See Also</h3>

<p><code><a href="SPOT.html#topic+plotData">plotData</a></code>
</p>
<p><code><a href="SPOT.html#topic+plotModel">plotModel</a></code>
</p>
<p><code><a href="SPOT.html#topic+plotFunction">plotFunction</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library("SPOT")
set.seed(1)                              # seed
k &lt;- 30                                  # sample number
x &lt;- matrix( cbind(runif(k)*10, runif(k)*10), k, 2)    # create data
y &lt;- funSphere(x)      # generate random test data
fit &lt;- buildLM(x,y)                      # create a model
result &lt;- spot(x=NULL, funSphere, c(-5, -5), c(5, 5))

spotPlot(plotType="data", x, y, type="filled.contour")
spotPlot(plotType="model", object=fit, type="contour")
spotPlot(plotType="fun", f=function(x){rowSums(x^2)},
   lower=c(-10,0), upper=c(15,10), type="filled.contour")
spotPlot(plotType = "parallel", object=fit)
spotPlot(plotType = "sensitivity", object=result)

</code></pre>

<hr>
<h2 id='SSE'>sum of squared errors</h2><span id='topic+SSE'></span>

<h3>Description</h3>

<p>sum of squared errors
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SSE(y, yhat)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SSE_+3A_y">y</code></td>
<td>
<p>actual value</p>
</td></tr>
<tr><td><code id="SSE_+3A_yhat">yhat</code></td>
<td>
<p>predicted value</p>
</td></tr>
</table>


<h3>Value</h3>

<p>sum of squared errors
</p>

<hr>
<h2 id='startCensusRun'>Start hyperparameter optimization runs with spot based on US census data</h2><span id='topic+startCensusRun'></span>

<h3>Description</h3>

<p>Runs to compare standard machine learning and deep learning models
</p>


<h3>Usage</h3>

<pre><code class='language-R'>startCensusRun(
  modelList = list("dl", "cvglmnet", "kknn", "ranger", "rpart", "svm", "xgboost"),
  runNr = "000",
  SPOTVersion = "2.10.12",
  SPOTMiscVersion = "1.19.2",
  timebudget = 3600,
  target = "age",
  cachedir = "oml.cache",
  task.type = "classif",
  nobs = 10000,
  nfactors = "high",
  nnumericals = "high",
  cardinality = "high",
  data.seed = 1,
  prop = 2/3,
  batch_size = 32,
  tuner.seed = 1,
  returnValue = "validationLoss",
  initSizeFactor = 2,
  spotModel = buildKriging,
  spotOptim = optimDE,
  lower = NULL,
  upper = NULL,
  noise = TRUE,
  OCBA = TRUE,
  OCBABudget = 3,
  multiStart = 2,
  multFun = 200,
  handleNAsMethod = handleNAsMean,
  imputeCriteriaFuns = list(is.infinite, is.na, is.nan),
  krigingTarget = "ei",
  krigingUseLambda = TRUE,
  krigingReinterpolate = FALSE,
  defaultAsStartingPoint = TRUE,
  plots = FALSE,
  Rinit = 2,
  replicates = 2,
  resDummy = FALSE,
  verbosity = 0
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="startCensusRun_+3A_modellist">modelList</code></td>
<td>
<p>list of models. Default:
<code>list("dl", "cvglmnet",  "kknn",   "ranger", "rpart" ,  "svm", "xgboost")</code></p>
</td></tr>
<tr><td><code id="startCensusRun_+3A_runnr">runNr</code></td>
<td>
<p>character, specifies the run number. Default: <code>"000"</code></p>
</td></tr>
<tr><td><code id="startCensusRun_+3A_spotversion">SPOTVersion</code></td>
<td>
<p>smallest package version number</p>
</td></tr>
<tr><td><code id="startCensusRun_+3A_spotmiscversion">SPOTMiscVersion</code></td>
<td>
<p>smallest package version number</p>
</td></tr>
<tr><td><code id="startCensusRun_+3A_timebudget">timebudget</code></td>
<td>
<p>time budget Default:  <code>3600</code> (secs)</p>
</td></tr>
<tr><td><code id="startCensusRun_+3A_target">target</code></td>
<td>
<p>target &quot;age&quot;</p>
</td></tr>
<tr><td><code id="startCensusRun_+3A_cachedir">cachedir</code></td>
<td>
<p>cache dir &quot;oml.cache&quot;</p>
</td></tr>
<tr><td><code id="startCensusRun_+3A_task.type">task.type</code></td>
<td>
<p>task type &quot;classif&quot;</p>
</td></tr>
<tr><td><code id="startCensusRun_+3A_nobs">nobs</code></td>
<td>
<p>number of observations 1e4</p>
</td></tr>
<tr><td><code id="startCensusRun_+3A_nfactors">nfactors</code></td>
<td>
<p>number of factorial variables &quot;high&quot;</p>
</td></tr>
<tr><td><code id="startCensusRun_+3A_nnumericals">nnumericals</code></td>
<td>
<p>number of numerical variables &quot;high&quot;</p>
</td></tr>
<tr><td><code id="startCensusRun_+3A_cardinality">cardinality</code></td>
<td>
<p>cardinality &quot;high&quot;</p>
</td></tr>
<tr><td><code id="startCensusRun_+3A_data.seed">data.seed</code></td>
<td>
<p>1</p>
</td></tr>
<tr><td><code id="startCensusRun_+3A_prop">prop</code></td>
<td>
<p>proportion 2 / 3</p>
</td></tr>
<tr><td><code id="startCensusRun_+3A_batch_size">batch_size</code></td>
<td>
<p>batch size (for dl) 32</p>
</td></tr>
<tr><td><code id="startCensusRun_+3A_tuner.seed">tuner.seed</code></td>
<td>
<p>seed for SPOT  1</p>
</td></tr>
<tr><td><code id="startCensusRun_+3A_returnvalue">returnValue</code></td>
<td>
<p>&quot;validationLoss&quot;</p>
</td></tr>
<tr><td><code id="startCensusRun_+3A_initsizefactor">initSizeFactor</code></td>
<td>
<p>multiplier for the initial design size 2</p>
</td></tr>
<tr><td><code id="startCensusRun_+3A_spotmodel">spotModel</code></td>
<td>
<p>buildKriging</p>
</td></tr>
<tr><td><code id="startCensusRun_+3A_spotoptim">spotOptim</code></td>
<td>
<p>optimDE</p>
</td></tr>
<tr><td><code id="startCensusRun_+3A_lower">lower</code></td>
<td>
<p>NULL</p>
</td></tr>
<tr><td><code id="startCensusRun_+3A_upper">upper</code></td>
<td>
<p>NULL</p>
</td></tr>
<tr><td><code id="startCensusRun_+3A_noise">noise</code></td>
<td>
<p>TRUE</p>
</td></tr>
<tr><td><code id="startCensusRun_+3A_ocba">OCBA</code></td>
<td>
<p>TRUE</p>
</td></tr>
<tr><td><code id="startCensusRun_+3A_ocbabudget">OCBABudget</code></td>
<td>
<p>3</p>
</td></tr>
<tr><td><code id="startCensusRun_+3A_multistart">multiStart</code></td>
<td>
<p>2</p>
</td></tr>
<tr><td><code id="startCensusRun_+3A_multfun">multFun</code></td>
<td>
<p>200</p>
</td></tr>
<tr><td><code id="startCensusRun_+3A_handlenasmethod">handleNAsMethod</code></td>
<td>
<p>handleNAsMean</p>
</td></tr>
<tr><td><code id="startCensusRun_+3A_imputecriteriafuns">imputeCriteriaFuns</code></td>
<td>
<p>list(is.infinite, is.na, is.nan)</p>
</td></tr>
<tr><td><code id="startCensusRun_+3A_krigingtarget">krigingTarget</code></td>
<td>
<p>&quot;ei&quot;</p>
</td></tr>
<tr><td><code id="startCensusRun_+3A_kriginguselambda">krigingUseLambda</code></td>
<td>
<p>TRUE</p>
</td></tr>
<tr><td><code id="startCensusRun_+3A_krigingreinterpolate">krigingReinterpolate</code></td>
<td>
<p>FALSE</p>
</td></tr>
<tr><td><code id="startCensusRun_+3A_defaultasstartingpoint">defaultAsStartingPoint</code></td>
<td>
<p>TRUE</p>
</td></tr>
<tr><td><code id="startCensusRun_+3A_plots">plots</code></td>
<td>
<p>FALSE</p>
</td></tr>
<tr><td><code id="startCensusRun_+3A_rinit">Rinit</code></td>
<td>
<p>2</p>
</td></tr>
<tr><td><code id="startCensusRun_+3A_replicates">replicates</code></td>
<td>
<p>2</p>
</td></tr>
<tr><td><code id="startCensusRun_+3A_resdummy">resDummy</code></td>
<td>
<p>FALSE</p>
</td></tr>
<tr><td><code id="startCensusRun_+3A_verbosity">verbosity</code></td>
<td>
<p>0</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
### These examples require an activated Python environment as described in
### Bartz-Beielstein, T., Rehbach, F., Sen, A., and Zaefferer, M.:
### Surrogate Model Based Hyperparameter Tuning for Deep Learning with SPOT,
### June 2021. http://arxiv.org/abs/2105.14625.
PYTHON_RETICULATE &lt;- FALSE
if(PYTHON_RETICULATE){
library("dplyr")
library("farff")
library("GGally")
library("keras")
library("tensorflow")
library("Metrics")
library("mlr")
library("reticulate")
library("rpart")
library("rpart.plot")
library("SPOT")
library("SPOTMisc")
library("tfdatasets")
library("rsample")
startCensusRun(modelList=list("ranger", timebudget=60))
}


</code></pre>

<hr>
<h2 id='startMnistRun'>Start hyperparameter optimization runs with spot based on MNIST data</h2><span id='topic+startMnistRun'></span>

<h3>Description</h3>

<p>Runs to compare deep learning models. Note: Number of epochs is
limited: <code>model &lt;- "dl"; cfg &lt;- getModelConf(model = model); cfg$upper[6] &lt;- 5</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>startMnistRun(
  runNr = "000",
  SPOTVersion = "2.11.4",
  SPOTMiscVersion = "1.19.6",
  encoding = "tensor",
  network = "cnn",
  timebudget = 60,
  data.seed = 1,
  prop = 2/3,
  batch_size = 32,
  tuner.seed = 1,
  returnValue = "validationLoss",
  initSizeFactor = 1,
  spotModel = buildKriging,
  spotOptim = optimDE,
  lower = NULL,
  upper = NULL,
  noise = TRUE,
  OCBA = FALSE,
  OCBABudget = 0,
  multiStart = 2,
  multFun = 200,
  handleNAsMethod = handleNAsMean,
  imputeCriteriaFuns = list(is.infinite, is.na, is.nan),
  krigingTarget = "ei",
  krigingUseLambda = TRUE,
  krigingReinterpolate = TRUE,
  defaultAsStartingPoint = TRUE,
  plots = FALSE,
  Rinit = 1,
  replicates = 1,
  resDummy = FALSE,
  verbosity = 0
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="startMnistRun_+3A_runnr">runNr</code></td>
<td>
<p>character, specifies the run number. Default: <code>"000"</code></p>
</td></tr>
<tr><td><code id="startMnistRun_+3A_spotversion">SPOTVersion</code></td>
<td>
<p>smallest package version number</p>
</td></tr>
<tr><td><code id="startMnistRun_+3A_spotmiscversion">SPOTMiscVersion</code></td>
<td>
<p>smallest package version number</p>
</td></tr>
<tr><td><code id="startMnistRun_+3A_encoding">encoding</code></td>
<td>
<p>encoding: <code>"oneHot"</code> od <code>"tensor"</code>. Default: <code>"tensor"</code></p>
</td></tr>
<tr><td><code id="startMnistRun_+3A_network">network</code></td>
<td>
<p>network: <code>"dl"</code> odr <code>"cnn"</code>. Default: <code>"cnn"</code></p>
</td></tr>
<tr><td><code id="startMnistRun_+3A_timebudget">timebudget</code></td>
<td>
<p>time budget Default:  <code>3600</code> (secs)</p>
</td></tr>
<tr><td><code id="startMnistRun_+3A_data.seed">data.seed</code></td>
<td>
<p>1</p>
</td></tr>
<tr><td><code id="startMnistRun_+3A_prop">prop</code></td>
<td>
<p>proportion 2 / 3</p>
</td></tr>
<tr><td><code id="startMnistRun_+3A_batch_size">batch_size</code></td>
<td>
<p>batch size (for dl) 32</p>
</td></tr>
<tr><td><code id="startMnistRun_+3A_tuner.seed">tuner.seed</code></td>
<td>
<p>seed for SPOT  1</p>
</td></tr>
<tr><td><code id="startMnistRun_+3A_returnvalue">returnValue</code></td>
<td>
<p>&quot;validationLoss&quot;</p>
</td></tr>
<tr><td><code id="startMnistRun_+3A_initsizefactor">initSizeFactor</code></td>
<td>
<p>multiplier for the initial design size 2</p>
</td></tr>
<tr><td><code id="startMnistRun_+3A_spotmodel">spotModel</code></td>
<td>
<p>buildKriging</p>
</td></tr>
<tr><td><code id="startMnistRun_+3A_spotoptim">spotOptim</code></td>
<td>
<p>optimDE</p>
</td></tr>
<tr><td><code id="startMnistRun_+3A_lower">lower</code></td>
<td>
<p>NULL</p>
</td></tr>
<tr><td><code id="startMnistRun_+3A_upper">upper</code></td>
<td>
<p>NULL</p>
</td></tr>
<tr><td><code id="startMnistRun_+3A_noise">noise</code></td>
<td>
<p>TRUE</p>
</td></tr>
<tr><td><code id="startMnistRun_+3A_ocba">OCBA</code></td>
<td>
<p>TRUE</p>
</td></tr>
<tr><td><code id="startMnistRun_+3A_ocbabudget">OCBABudget</code></td>
<td>
<p>3</p>
</td></tr>
<tr><td><code id="startMnistRun_+3A_multistart">multiStart</code></td>
<td>
<p>2</p>
</td></tr>
<tr><td><code id="startMnistRun_+3A_multfun">multFun</code></td>
<td>
<p>200</p>
</td></tr>
<tr><td><code id="startMnistRun_+3A_handlenasmethod">handleNAsMethod</code></td>
<td>
<p>handleNAsMean</p>
</td></tr>
<tr><td><code id="startMnistRun_+3A_imputecriteriafuns">imputeCriteriaFuns</code></td>
<td>
<p>list(is.infinite, is.na, is.nan)</p>
</td></tr>
<tr><td><code id="startMnistRun_+3A_krigingtarget">krigingTarget</code></td>
<td>
<p>&quot;ei&quot;</p>
</td></tr>
<tr><td><code id="startMnistRun_+3A_kriginguselambda">krigingUseLambda</code></td>
<td>
<p>TRUE</p>
</td></tr>
<tr><td><code id="startMnistRun_+3A_krigingreinterpolate">krigingReinterpolate</code></td>
<td>
<p>FALSE</p>
</td></tr>
<tr><td><code id="startMnistRun_+3A_defaultasstartingpoint">defaultAsStartingPoint</code></td>
<td>
<p>FALSE</p>
</td></tr>
<tr><td><code id="startMnistRun_+3A_plots">plots</code></td>
<td>
<p>FALSE</p>
</td></tr>
<tr><td><code id="startMnistRun_+3A_rinit">Rinit</code></td>
<td>
<p>2</p>
</td></tr>
<tr><td><code id="startMnistRun_+3A_replicates">replicates</code></td>
<td>
<p>2</p>
</td></tr>
<tr><td><code id="startMnistRun_+3A_resdummy">resDummy</code></td>
<td>
<p>FALSE</p>
</td></tr>
<tr><td><code id="startMnistRun_+3A_verbosity">verbosity</code></td>
<td>
<p>0</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
### These examples require an activated Python environment as described in
### Bartz-Beielstein, T., Rehbach, F., Sen, A., and Zaefferer, M.:
### Surrogate Model Based Hyperparameter Tuning for Deep Learning with SPOT,
### June 2021. http://arxiv.org/abs/2105.14625.
PYTHON_RETICULATE &lt;- FALSE
if(PYTHON_RETICULATE){
library("dplyr")
library("farff")
library("GGally")
library("keras")
library("tensorflow")
library("Metrics")
library("mlr")
library("reticulate")
library("rpart")
library("rpart.plot")
library("SPOT")
library("SPOTMisc")
library("tfdatasets")
library("rsample")
startMnistRun(timebudget=60, initSizeFactor = 1, verbosity = 1)
startMnistRun(timebudget=60, encoding="tensor", network="cnn")
}


</code></pre>

<hr>
<h2 id='startXGBCensusRun'>Start hyperparameter optimization runs with spot based on US census data</h2><span id='topic+startXGBCensusRun'></span>

<h3>Description</h3>

<p>Runs to compare standard machine learning and deep learning models
</p>


<h3>Usage</h3>

<pre><code class='language-R'>startXGBCensusRun(
  modelList = list("xgboost"),
  runNr = "000",
  SPOTVersion = "2.11.14",
  SPOTMiscVersion = "1.19.28",
  timebudget = 3600,
  target = "age",
  cachedir = "oml.cache",
  task.type = "classif",
  nobs = 10000,
  nfactors = "high",
  nnumericals = "high",
  cardinality = "high",
  data.seed = 1,
  prop = 2/3,
  batch_size = 32,
  tuner.seed = 1,
  returnValue = "validationLoss",
  initSizeFactor = 2,
  spotModel = buildKriging,
  spotOptim = optimDE,
  lower = NULL,
  upper = NULL,
  noise = TRUE,
  OCBA = TRUE,
  OCBABudget = 3,
  multiStart = 2,
  multFun = 200,
  handleNAsMethod = handleNAsMean,
  imputeCriteriaFuns = list(is.infinite, is.na, is.nan),
  krigingTarget = "ei",
  krigingUseLambda = TRUE,
  krigingReinterpolate = FALSE,
  defaultAsStartingPoint = TRUE,
  plots = FALSE,
  Rinit = 2,
  replicates = 2,
  resDummy = FALSE,
  verbosity = 0
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="startXGBCensusRun_+3A_modellist">modelList</code></td>
<td>
<p>list of models. Default:
<code>list("xgboost")</code></p>
</td></tr>
<tr><td><code id="startXGBCensusRun_+3A_runnr">runNr</code></td>
<td>
<p>character, specifies the run number. Default: <code>"000"</code></p>
</td></tr>
<tr><td><code id="startXGBCensusRun_+3A_spotversion">SPOTVersion</code></td>
<td>
<p>smallest package version number</p>
</td></tr>
<tr><td><code id="startXGBCensusRun_+3A_spotmiscversion">SPOTMiscVersion</code></td>
<td>
<p>smallest package version number</p>
</td></tr>
<tr><td><code id="startXGBCensusRun_+3A_timebudget">timebudget</code></td>
<td>
<p>time budget Default:  <code>3600</code> (secs)</p>
</td></tr>
<tr><td><code id="startXGBCensusRun_+3A_target">target</code></td>
<td>
<p>target &quot;age&quot;</p>
</td></tr>
<tr><td><code id="startXGBCensusRun_+3A_cachedir">cachedir</code></td>
<td>
<p>cache dir &quot;oml.cache&quot;</p>
</td></tr>
<tr><td><code id="startXGBCensusRun_+3A_task.type">task.type</code></td>
<td>
<p>task type &quot;classif&quot;</p>
</td></tr>
<tr><td><code id="startXGBCensusRun_+3A_nobs">nobs</code></td>
<td>
<p>number of observations 1e4</p>
</td></tr>
<tr><td><code id="startXGBCensusRun_+3A_nfactors">nfactors</code></td>
<td>
<p>number of factorial variables &quot;high&quot;</p>
</td></tr>
<tr><td><code id="startXGBCensusRun_+3A_nnumericals">nnumericals</code></td>
<td>
<p>number of numerical variables &quot;high&quot;</p>
</td></tr>
<tr><td><code id="startXGBCensusRun_+3A_cardinality">cardinality</code></td>
<td>
<p>cardinality &quot;high&quot;</p>
</td></tr>
<tr><td><code id="startXGBCensusRun_+3A_data.seed">data.seed</code></td>
<td>
<p>1</p>
</td></tr>
<tr><td><code id="startXGBCensusRun_+3A_prop">prop</code></td>
<td>
<p>proportion 2 / 3</p>
</td></tr>
<tr><td><code id="startXGBCensusRun_+3A_batch_size">batch_size</code></td>
<td>
<p>batch size (for dl) 32</p>
</td></tr>
<tr><td><code id="startXGBCensusRun_+3A_tuner.seed">tuner.seed</code></td>
<td>
<p>seed for SPOT  1</p>
</td></tr>
<tr><td><code id="startXGBCensusRun_+3A_returnvalue">returnValue</code></td>
<td>
<p>&quot;validationLoss&quot;</p>
</td></tr>
<tr><td><code id="startXGBCensusRun_+3A_initsizefactor">initSizeFactor</code></td>
<td>
<p>multiplier for the initial design size 2</p>
</td></tr>
<tr><td><code id="startXGBCensusRun_+3A_spotmodel">spotModel</code></td>
<td>
<p>buildKriging</p>
</td></tr>
<tr><td><code id="startXGBCensusRun_+3A_spotoptim">spotOptim</code></td>
<td>
<p>optimDE</p>
</td></tr>
<tr><td><code id="startXGBCensusRun_+3A_lower">lower</code></td>
<td>
<p>NULL</p>
</td></tr>
<tr><td><code id="startXGBCensusRun_+3A_upper">upper</code></td>
<td>
<p>NULL</p>
</td></tr>
<tr><td><code id="startXGBCensusRun_+3A_noise">noise</code></td>
<td>
<p>TRUE</p>
</td></tr>
<tr><td><code id="startXGBCensusRun_+3A_ocba">OCBA</code></td>
<td>
<p>TRUE</p>
</td></tr>
<tr><td><code id="startXGBCensusRun_+3A_ocbabudget">OCBABudget</code></td>
<td>
<p>3</p>
</td></tr>
<tr><td><code id="startXGBCensusRun_+3A_multistart">multiStart</code></td>
<td>
<p>2</p>
</td></tr>
<tr><td><code id="startXGBCensusRun_+3A_multfun">multFun</code></td>
<td>
<p>200</p>
</td></tr>
<tr><td><code id="startXGBCensusRun_+3A_handlenasmethod">handleNAsMethod</code></td>
<td>
<p>handleNAsMean</p>
</td></tr>
<tr><td><code id="startXGBCensusRun_+3A_imputecriteriafuns">imputeCriteriaFuns</code></td>
<td>
<p>list(is.infinite, is.na, is.nan)</p>
</td></tr>
<tr><td><code id="startXGBCensusRun_+3A_krigingtarget">krigingTarget</code></td>
<td>
<p>&quot;ei&quot;</p>
</td></tr>
<tr><td><code id="startXGBCensusRun_+3A_kriginguselambda">krigingUseLambda</code></td>
<td>
<p>TRUE</p>
</td></tr>
<tr><td><code id="startXGBCensusRun_+3A_krigingreinterpolate">krigingReinterpolate</code></td>
<td>
<p>FALSE</p>
</td></tr>
<tr><td><code id="startXGBCensusRun_+3A_defaultasstartingpoint">defaultAsStartingPoint</code></td>
<td>
<p>FALSE</p>
</td></tr>
<tr><td><code id="startXGBCensusRun_+3A_plots">plots</code></td>
<td>
<p>FALSE</p>
</td></tr>
<tr><td><code id="startXGBCensusRun_+3A_rinit">Rinit</code></td>
<td>
<p>2</p>
</td></tr>
<tr><td><code id="startXGBCensusRun_+3A_replicates">replicates</code></td>
<td>
<p>2</p>
</td></tr>
<tr><td><code id="startXGBCensusRun_+3A_resdummy">resDummy</code></td>
<td>
<p>FALSE</p>
</td></tr>
<tr><td><code id="startXGBCensusRun_+3A_verbosity">verbosity</code></td>
<td>
<p>0</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
### These examples require an activated Python environment as described in
### Bartz-Beielstein, T., Rehbach, F., Sen, A., and Zaefferer, M.:
### Surrogate Model Based Hyperparameter Tuning for Deep Learning with SPOT,
### June 2021. http://arxiv.org/abs/2105.14625.
PYTHON_RETICULATE &lt;- FALSE
if(PYTHON_RETICULATE){
library("dplyr")
library("farff")
library("GGally")
library("keras")
library("tensorflow")
library("Metrics")
library("mlr")
library("reticulate")
library("rpart")
library("rpart.plot")
library("SPOT")
library("SPOTMisc")
library("tfdatasets")
library("rsample")
startXGBCensusRun(modelList=list("xgboost"), timebudget=60, plots=TRUE)
}


</code></pre>

<hr>
<h2 id='subgroups'>Return effects for each subgroup</h2><span id='topic+subgroups'></span>

<h3>Description</h3>

<p>subgroups: returns the table the effects per groups.
Code based on the <code>sbgroups</code> function written by Gilles Pujol for
the function <code><a href="sensitivity.html#topic+sb">sb</a></code> in the <code>sensitivity</code> package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>subgroups(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="subgroups_+3A_x">x</code></td>
<td>
<p>data</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data frame with group names and effects
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
require("SPOT")
require("RColorBrewer")
set.seed(2)
# Interesting for larger n:
n &lt;- 2
lower &lt;- c(-0.1, rep(-10,n))
upper &lt;- c(0.1, rep(10,n))

# Model-based optimization
res &lt;- spot(,funSphere,
             lower, upper,
             control=list(funEvals=30,
                          optimizer = optimNLOPTR))

# Use the surrogate model for prediction
predictFunKriging &lt;- function(x){
      predict(object = res$modelFit, x)
      }

# Determine sensitivity
sens &lt;- sequentialBifurcation(predictFunKriging,
                              lower, upper,
                              k=n+1, interaction = TRUE, verbosity = 0)

# Extract group information (variable effects) from sensitivity analysis
ps &lt;- subgroups(sens)
colors &lt;- brewer.pal(12, "Set3")
barplot(ps$effect, names.arg=ps$group, col= colors)


</code></pre>

<hr>
<h2 id='trans_10pow'>10 power x transformation</h2><span id='topic+trans_10pow'></span>

<h3>Description</h3>

<p>Parameter values can be translated,
e.g., to base 10.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>trans_10pow(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="trans_10pow_+3A_x">x</code></td>
<td>
<p>input</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>10^x</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>f2 &lt;- function(x){2^x}
fn &lt;- c("identity", "exp", "f2")
xNat &lt;- diag(3)
SPOT::transformX(xNat, fn)

</code></pre>

<hr>
<h2 id='trans_10pow_round'>10 power x transformation with round</h2><span id='topic+trans_10pow_round'></span>

<h3>Description</h3>

<p>Parameter values can be translated,
e.g., to base 10 as implemented in <code><a href="#topic+trans_10pow">trans_10pow</a></code>.
<code>trans_10pow_round</code> implements the transformation x -&gt; round(2^x).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>trans_10pow_round(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="trans_10pow_round_+3A_x">x</code></td>
<td>
<p>input</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>round(10^x)</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>f2 &lt;- function(x){2^x}
fn &lt;- c("identity", "exp", "f2")
xNat &lt;- diag(3)
SPOT::transformX(xNat, fn)

</code></pre>

<hr>
<h2 id='trans_1minus10pow'>10 power x transformation</h2><span id='topic+trans_1minus10pow'></span>

<h3>Description</h3>

<p>Parameter values x are transformed to <code>1-10^x</code>.
This is helpful for parameters that are likely to be set very close to (but below) a value of 1,
such as discount factors in reinforcement learning.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>trans_1minus10pow(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="trans_1minus10pow_+3A_x">x</code></td>
<td>
<p>input</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>1-10^x</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>f2 &lt;- function(x){2^x}
fn &lt;- c("identity", "exp", "f2")
xNat &lt;- diag(3)
SPOT::transformX(xNat, fn)

</code></pre>

<hr>
<h2 id='trans_2pow'>2 power x transformation</h2><span id='topic+trans_2pow'></span>

<h3>Description</h3>

<p>Parameter values can be translated,
e.g., to base 10 as implemented in <code><a href="#topic+trans_10pow">trans_10pow</a></code>.
<code>trans_2pow</code> implements the transformation x -&gt; 2^x.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>trans_2pow(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="trans_2pow_+3A_x">x</code></td>
<td>
<p>input</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>2^x</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>f2 &lt;- function(x){2^x}
fn &lt;- c("identity", "exp", "f2")
xNat &lt;- diag(3)
SPOT::transformX(xNat, fn)

</code></pre>

<hr>
<h2 id='trans_2pow_round'>2 power x transformation with round</h2><span id='topic+trans_2pow_round'></span>

<h3>Description</h3>

<p>Parameter values can be translated,
e.g., to base 10 as implemented in <code><a href="#topic+trans_10pow">trans_10pow</a></code>.
<code>trans_2pow_round</code> implements the transformation x -&gt; round(2^x).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>trans_2pow_round(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="trans_2pow_round_+3A_x">x</code></td>
<td>
<p>input</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>round(2^x)</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>f2 &lt;- function(x){2^x}
fn &lt;- c("identity", "exp", "f2")
xNat &lt;- diag(3)
SPOT::transformX(xNat, fn)

</code></pre>

<hr>
<h2 id='trans_id'>Identity transformation</h2><span id='topic+trans_id'></span>

<h3>Description</h3>

<p>Parameter values can be translated,
e.g., to base 10 as implemented in <code><a href="#topic+trans_10pow">trans_10pow</a></code>.
<code>trans_id</code> implements the identity (transformation), i.e., x is mapped to x.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>trans_id(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="trans_id_+3A_x">x</code></td>
<td>
<p>input</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>x</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>f2 &lt;- function(x){2^x}
fn &lt;- c("identity", "exp", "f2")
xNat &lt;- diag(3)
SPOT::transformX(xNat, fn)

</code></pre>

<hr>
<h2 id='trans_mult2_round'>Mult 2 transformation</h2><span id='topic+trans_mult2_round'></span>

<h3>Description</h3>

<p>Parameter values can be translated,
implements the multiplication (transformation), i.e., x is mapped
to round(2x).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>trans_mult2_round(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="trans_mult2_round_+3A_x">x</code></td>
<td>
<p>input</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>x</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>f2 &lt;- function(x){2^x}
fn &lt;- c("identity", "exp", "f2")
xNat &lt;- diag(3)
SPOT::transformX(xNat, fn)

</code></pre>

<hr>
<h2 id='trans_odd_round'>odd transformation</h2><span id='topic+trans_odd_round'></span>

<h3>Description</h3>

<p>Generate odd numbers, i.e., <code>x -&gt; 2x-1</code> for x &gt; 0.
Return values are rounded using <code>round</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>trans_odd_round(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="trans_odd_round_+3A_x">x</code></td>
<td>
<p>input</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>x</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>f2 &lt;- function(x){2^x}
fn &lt;- c("trans_odd_round", "exp", "f2")
xNat &lt;- diag(3)
SPOT::transformX(xNat, fn)

</code></pre>

<hr>
<h2 id='translate_levels'>Helper function: translate levels</h2><span id='topic+translate_levels'></span>

<h3>Description</h3>

<p>Translate existing levels of a factor into new levels.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>translate_levels(x, translations)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="translate_levels_+3A_x">x</code></td>
<td>
<p>a factor vector to be translated</p>
</td></tr>
<tr><td><code id="translate_levels_+3A_translations">translations</code></td>
<td>
<p>a named list that specifies the translation: <code>list(newlevel=c(oldlevel1,oldlevel2,etc))</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>translated factor
</p>

<hr>
<h2 id='valid_inputs'>Check the validity of input parameters.</h2><span id='topic+valid_inputs'></span>

<h3>Description</h3>

<p>Helper function. Check correct parameter names.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>valid_inputs(chars)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="valid_inputs_+3A_chars">chars</code></td>
<td>
<p>character</p>
</td></tr>
</table>


<h3>Value</h3>

<p>correct characters
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
