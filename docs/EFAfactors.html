<!DOCTYPE html><html lang="en"><head><title>Help for package EFAfactors</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {EFAfactors}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#af.softmax'><p>An Activation Function: Softmax</p></a></li>
<li><a href='#CD'><p>the Comparison Data (CD) Approach</p></a></li>
<li><a href='#CDF'><p>the  Comparison Data Forest (CDF) Approach</p></a></li>
<li><a href='#check_python_libraries'><p>Check and Install Python Libraries (numpy and onnxruntime)</p></a></li>
<li><a href='#data.bfi'><p>25 Personality Items Representing 5 Factors</p></a></li>
<li><a href='#data.datasets'><p>Subset Dataset for Training the Pre-Trained Deep Neural Network (DNN)</p></a></li>
<li><a href='#data.scaler'><p>the Scaler for the Pre-Trained Deep Neural Network (DNN)</p></a></li>
<li><a href='#DNN_predictor'><p>A Pre-Trained Deep Neural Network (DNN) for Determining the Number of Factors</p></a></li>
<li><a href='#EFAhclust'><p>Hierarchical Clustering for EFA</p></a></li>
<li><a href='#EFAindex'><p>Various Indeces in EFA</p></a></li>
<li><a href='#EFAkmeans'><p>K-means for EFA</p></a></li>
<li><a href='#EFAscreet'><p>Scree Plot</p></a></li>
<li><a href='#EFAsim.data'><p>Simulate Data that Conforms to the theory of Exploratory Factor Analysis.</p></a></li>
<li><a href='#EFAvote'><p>Voting Method for Number of Factors in EFA</p></a></li>
<li><a href='#EKC'><p>Empirical Kaiser Criterion</p></a></li>
<li><a href='#extractor.feature.DNN'><p>Extracting features for the Pre-Trained Deep Neural Network (DNN)</p></a></li>
<li><a href='#extractor.feature.FF'><p>Extracting features According to Goretzko &amp; Buhner (2020)</p></a></li>
<li><a href='#factor.analysis'><p>Factor Analysis by Principal Axis Factoring</p></a></li>
<li><a href='#FF'><p>Factor Forest (FF) Powered by An Tuned XGBoost Model for Determining the Number of Factors</p></a></li>
<li><a href='#GenData'><p>Simulating Data Following John Ruscio's RGenData</p></a></li>
<li><a href='#Hull'><p>the Hull Approach</p></a></li>
<li><a href='#KGC'><p>Kaiser-Guttman Criterion</p></a></li>
<li><a href='#load_DNN'><p>Load the Trained Deep Neural Network (DNN)</p></a></li>
<li><a href='#load_scaler'><p>Load the Scaler for the Pre-Trained Deep Neural Network (DNN)</p></a></li>
<li><a href='#load_xgb'><p>Load the Tuned XGBoost Model</p></a></li>
<li><a href='#model.xgb'><p>the Tuned XGBoost Model for Determining the Number of Facotrs</p></a></li>
<li><a href='#normalizor'><p>Feature Normalization</p></a></li>
<li><a href='#PA'><p>Parallel Analysis</p></a></li>
<li><a href='#plot.CD'><p>Plot Comparison Data for Factor Analysis</p></a></li>
<li><a href='#plot.CDF'><p>Plot Comparison Data Forest (CDF) Classification Probability Distribution</p></a></li>
<li><a href='#plot.DNN_predictor'><p>Plot DNN Predictor Classification Probability Distribution</p></a></li>
<li><a href='#plot.EFAhclust'><p>Plot Hierarchical Cluster Analysis Dendrogram</p></a></li>
<li><a href='#plot.EFAkmeans'><p>Plot EFA K-means Clustering Results</p></a></li>
<li><a href='#plot.EFAscreet'><p>Plots the Scree Plot</p></a></li>
<li><a href='#plot.EFAvote'><p>Plot Voting Results for Number of Factors</p></a></li>
<li><a href='#plot.EKC'><p>Plot Empirical Kaiser Criterion (EKC) Plot</p></a></li>
<li><a href='#plot.FF'><p>Plot Factor Forest (FF) Classification Probability Distribution</p></a></li>
<li><a href='#plot.Hull'><p>Plot Hull Plot for Factor Analysis</p></a></li>
<li><a href='#plot.KGC'><p>Plot Kaiser-Guttman Criterion (KGC) Plot</p></a></li>
<li><a href='#plot.PA'><p>Plot Parallel Analysis Scree Plot</p></a></li>
<li><a href='#predictLearner.classif.xgboost.earlystop'><p>Prediction Function for the Tuned XGBoost Model with Early Stopping</p></a></li>
<li><a href='#print.CD'><p>Print Comparison Data Method Results</p></a></li>
<li><a href='#print.CDF'><p>Print Comparison Data Forest (CDF) Results</p></a></li>
<li><a href='#print.DNN_predictor'><p>Print DNN Predictor Method Results</p></a></li>
<li><a href='#print.EFAdata'><p>Print the EFAsim.data</p></a></li>
<li><a href='#print.EFAhclust'><p>Print EFAhclust Method Results</p></a></li>
<li><a href='#print.EFAkmeans'><p>Print EFAkmeans Method Results</p></a></li>
<li><a href='#print.EFAscreet'><p>Print the Scree Plot</p></a></li>
<li><a href='#print.EFAvote'><p>Print Voting Method Results</p></a></li>
<li><a href='#print.EKC'><p>Print Empirical Kaiser Criterion Results</p></a></li>
<li><a href='#print.FF'><p>Print Factor Forest (FF) Results</p></a></li>
<li><a href='#print.Hull'><p>Print Hull Method Results</p></a></li>
<li><a href='#print.KGC'><p>Print Kaiser-Guttman Criterion Results</p></a></li>
<li><a href='#print.PA'><p>Print Parallel Analysis Method Results</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Determining the Number of Factors in Exploratory Factor Analysis</td>
</tr>
<tr>
<td>Version:</td>
<td>1.2.1</td>
</tr>
<tr>
<td>Date:</td>
<td>2025-02-15</td>
</tr>
<tr>
<td>Author:</td>
<td>Haijiang Qin <a href="https://orcid.org/0009-0000-6721-5653"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre, cph],
  Lei Guo <a href="https://orcid.org/0000-0002-8273-3587"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut,
    cph]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Haijiang Qin &lt;haijiang133@outlook.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Provides a collection of standard factor retention methods in Exploratory Factor Analysis (EFA), making it easier to determine the number of factors. Traditional methods such as the scree plot by Cattell (1966) &lt;<a href="https://doi.org/10.1207%2Fs15327906mbr0102_10">doi:10.1207/s15327906mbr0102_10</a>&gt;, Kaiser-Guttman Criterion (KGC) by Guttman (1954) &lt;<a href="https://doi.org/10.1007%2FBF02289162">doi:10.1007/BF02289162</a>&gt; and Kaiser (1960) &lt;<a href="https://doi.org/10.1177%2F001316446002000116">doi:10.1177/001316446002000116</a>&gt;, and flexible Parallel Analysis (PA) by Horn (1965) &lt;<a href="https://doi.org/10.1007%2FBF02289447">doi:10.1007/BF02289447</a>&gt; based on eigenvalues form PCA or EFA are readily available. This package also implements several newer methods, such as the Empirical Kaiser Criterion (EKC) by Braeken and van Assen (2017) &lt;<a href="https://doi.org/10.1037%2Fmet0000074">doi:10.1037/met0000074</a>&gt;, Comparison Data (CD) by Ruscio and Roche (2012) &lt;<a href="https://doi.org/10.1037%2Fa0025697">doi:10.1037/a0025697</a>&gt;, and Hull method by Lorenzo-Seva et al. (2011) &lt;<a href="https://doi.org/10.1080%2F00273171.2011.564527">doi:10.1080/00273171.2011.564527</a>&gt;, as well as some AI-based methods like Comparison Data Forest (CDF) by Goretzko and Ruscio (2024) &lt;<a href="https://doi.org/10.3758%2Fs13428-023-02122-4">doi:10.3758/s13428-023-02122-4</a>&gt; and Factor Forest (FF) by Goretzko and Buhner (2020) &lt;<a href="https://doi.org/10.1037%2Fmet0000262">doi:10.1037/met0000262</a>&gt;. Additionally, it includes a deep neural network (DNN) trained on large-scale datasets that can efficiently and reliably determine the number of factors.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.1.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>BBmisc, checkmate, ddpcr, ineq, MASS, Matrix, mlr, proxy,
psych, ranger, reticulate, Rcpp, RcppArmadillo, SimCorMultRes,
xgboost</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.2</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Collate:</td>
<td>'CD.R' 'CDF.R' 'check_python_libraries.R' 'data.bfi.R'
'data.datasets.R' 'data.scaler.R' 'DNN_predictor.R'
'EFAhclust.R' 'EFAindex.R' 'EFAkmeans.R' 'EFAvote.R' 'EKC.R'
'EFAscreet.R' 'EFAsim.data.R' 'extractor.feature.DNN.R'
'extractor.feature.FF.R' 'factor.analysis.R' 'FF.R' 'GenData.R'
'get.runs.R' 'Hull.R' 'KGC.R' 'load.R' 'model.xgb.R'
'normalizor.R' 'PA.R' 'ParamHelpers.R' 'plot.R' 'print.R'
'RcppExports.R' 'af.softmax.R' 'utils.R' 'zzz.R'</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://haijiangqin.com/EFAfactors/">https://haijiangqin.com/EFAfactors/</a></td>
</tr>
<tr>
<td>Packaged:</td>
<td>2025-02-17 04:05:34 UTC; Haiji</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2025-02-17 04:30:07 UTC</td>
</tr>
</table>
<hr>
<h2 id='af.softmax'>An Activation Function: Softmax</h2><span id='topic+af.softmax'></span>

<h3>Description</h3>

<p>This function computes the softmax of a numeric vector. The softmax function
transforms a vector of real values into a probability distribution, where each element
is between 0 and 1 and the sum of all elements is 1. @seealso <a href="#topic+DNN_predictor">DNN_predictor</a>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>af.softmax(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="af.softmax_+3A_x">x</code></td>
<td>
<p>A numeric vector for which the softmax transformation is to be computed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The softmax function is calculated as:
</p>
<p style="text-align: center;"><code class="reqn">softmax(x_i) = \frac{exp(x_i)}{\sum_{j} exp(x_j)}</code>
</p>

<p>In the case of overflow (i.e., when <code>exp(x_i)</code> is too large), this function handles
<code>Inf</code> values by assigning <code>1</code> to the corresponding positions and <code>0</code> to the
others before Softmax. @seealso <a href="#topic+DNN_predictor">DNN_predictor</a>
</p>


<h3>Value</h3>

<p>A numeric vector representing the softmax-transformed values of <code>x</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- c(1, 2, 3)
af.softmax(x)

</code></pre>

<hr>
<h2 id='CD'>the Comparison Data (CD) Approach</h2><span id='topic+CD'></span>

<h3>Description</h3>

<p>This function runs the comparison data (CD) approach of Ruscio &amp; Roche (2012).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CD(
  response,
  nfact.max = 10,
  N.pop = 10000,
  N.Samples = 500,
  Alpha = 0.3,
  cor.type = "pearson",
  use = "pairwise.complete.obs",
  vis = TRUE,
  plot = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="CD_+3A_response">response</code></td>
<td>
<p>A required <code>N</code> × <code>I</code> matrix or data.frame consisting of the responses of <code>N</code> individuals
to × <code>I</code> items.</p>
</td></tr>
<tr><td><code id="CD_+3A_nfact.max">nfact.max</code></td>
<td>
<p>The maximum number of factors discussed by CD approach. (default = 10)</p>
</td></tr>
<tr><td><code id="CD_+3A_n.pop">N.pop</code></td>
<td>
<p>Size of finite populations of simulating.. (default = 10,000)</p>
</td></tr>
<tr><td><code id="CD_+3A_n.samples">N.Samples</code></td>
<td>
<p>Number of samples drawn from each population. (default = 500)</p>
</td></tr>
<tr><td><code id="CD_+3A_alpha">Alpha</code></td>
<td>
<p>Alpha level when testing statistical significance (Wilcoxon Rank Sum and Signed Rank Tests) of
improvement with additional factor. (default = .30)</p>
</td></tr>
<tr><td><code id="CD_+3A_cor.type">cor.type</code></td>
<td>
<p>A character string indicating which correlation coefficient (or covariance) is to be computed. One of &quot;pearson&quot; (default),
&quot;kendall&quot;, or &quot;spearman&quot;. @seealso <a href="stats.html#topic+cor">cor</a>.</p>
</td></tr>
<tr><td><code id="CD_+3A_use">use</code></td>
<td>
<p>an optional character string giving a method for computing covariances in the presence of missing values. This
must be one of the strings &quot;everything&quot;, &quot;all.obs&quot;, &quot;complete.obs&quot;, &quot;na.or.complete&quot;, or &quot;pairwise.complete.obs&quot; (default).
@seealso <a href="stats.html#topic+cor">cor</a>.</p>
</td></tr>
<tr><td><code id="CD_+3A_vis">vis</code></td>
<td>
<p>A Boolean variable that will print the factor retention results when set to TRUE, and will not print
when set to FALSE. (default = TRUE)</p>
</td></tr>
<tr><td><code id="CD_+3A_plot">plot</code></td>
<td>
<p>A Boolean variable that will print the CD plot when set to TRUE, and will not print it when set to
FALSE. @seealso <a href="#topic+plot.CD">plot.CD</a>. (Default = TRUE)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Ruscio and Roche (2012) proposed a method for determining the number of factors through comparison data (CD).
This method identifies the appropriate number of factors by finding the solution that best reproduces the pattern
of eigenvalues. CD employs an iterative procedure when generating comparison data with a known factor structure,
taking into account previous factors. Initially, CD compares whether the simulated comparison data with one
latent factor (j=1) reproduces the empirical eigenvalue pattern significantly worse than the two-factor solution (j+1).
If so, CD increases the value of j until further improvements are no longer significant or a preset maximum number of
factors is reached. Specifically, CD involves five steps:
</p>
<p>1. Generate random data with either j or j+1 latent factors and calculate the eigenvalues of the respective correlation matrices.
</p>
<p>2. Compute the root mean square error (RMSE) of the difference between the empirical and simulated eigenvalues using the formula
</p>
<p style="text-align: center;"><code class="reqn">
   RMSE = \sqrt{\sum_{i=1}^{p} (\lambda_{emp,i} - \lambda_{sim,i})^2}
</code>
</p>

<p>, where:
</p>

<ul>
<li> <p><code class="reqn">\lambda_{emp,i}</code>: The i-th empirical eigenvalue.
</p>
</li>
<li> <p><code class="reqn">\lambda_{sim,i}</code>: The i-th simulated eigenvalue.
</p>
</li>
<li> <p><code class="reqn">p</code>: The number of items or eigenvalues.
</p>
</li></ul>

<p>. This step produces two RMSEs, corresponding to the different numbers of latent factors.
</p>
<p>3. Repeat steps 1 and 2, 500 times ( default in the Package ).
</p>
<p>4. Use a one-sided Wilcoxon test (alpha = 0.30) to assess whether the RMSE is significantly reduced under the two-factor condition.
</p>
<p>5. If the difference in RMSE is not significant, CD suggests selecting j factors. Otherwise, j is increased by 1, and steps 1 to 4 are repeated.
</p>
<p>The code is implemented based on the resources available at:
</p>

<ul>
<li> <p><a href="https://ruscio.pages.tcnj.edu/quantitative-methods-program-code/">https://ruscio.pages.tcnj.edu/quantitative-methods-program-code/</a>
</p>
</li>
<li> <p><a href="https://osf.io/gqma2/?view_only=d03efba1fd0f4c849a87db82e6705668">https://osf.io/gqma2/?view_only=d03efba1fd0f4c849a87db82e6705668</a>
</p>
</li>
<li> <p><a href="https://osf.io/mvrau/">https://osf.io/mvrau/</a>
</p>
</li></ul>

<p>Since the CD approach requires extensive data simulation and computation, C++ code is used to speed up the process.
</p>


<h3>Value</h3>

<p>An object of class <code>CD</code> is a <code>list</code> containing the following components:
</p>
<table role = "presentation">
<tr><td><code>nfact</code></td>
<td>
<p>The number of factors to be retained.</p>
</td></tr>
<tr><td><code>RMSE.Eigs</code></td>
<td>
<p>A matrix containing the root mean square error (RMSE) of the eigenvalues
produced by each simulation for every discussed number of factors.</p>
</td></tr>
<tr><td><code>Sig</code></td>
<td>
<p>A boolean variable indicating whether the significance level of the Wilcoxon
Rank Sum and Signed Rank Tests has reached Alpha.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Haijiang Qin &lt;Haijiang133@outlook.com&gt;
</p>


<h3>References</h3>

<p>Auerswald, M., &amp; Moshagen, M. (2019). How to determine the number of factors to retain in exploratory factor analysis: A comparison of extraction methods under realistic conditions. Psychological methods, 24(4), 468-491. https://doi.org/https://doi.org/10.1037/met0000200.
</p>
<p>Goretzko, D., &amp; Buhner, M. (2020). One model to rule them all? Using machine learning algorithms to determine the number of factors in exploratory factor analysis. Psychol Methods, 25(6), 776-786. https://doi.org/10.1037/met0000262.
</p>
<p>Ruscio, J., &amp; Roche, B. (2012). Determining the number of factors to retain in an exploratory factor analysis using comparison data of known factorial structure. Psychological Assessment, 24, 282–292. http://dx.doi.org/10.1037/a0025697.
</p>


<h3>See Also</h3>

<p><a href="#topic+GenData">GenData</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(EFAfactors)
set.seed(123)

##Take the data.bfi dataset as an example.
data(data.bfi)

response &lt;- as.matrix(data.bfi[, 1:25]) ## loading data
response &lt;- na.omit(response) ## Remove samples with NA/missing values

## Transform the scores of reverse-scored items to normal scoring
response[, c(1, 9, 10, 11, 12, 22, 25)] &lt;- 6 - response[, c(1, 9, 10, 11, 12, 22, 25)] + 1


## Run CD function with default parameters.

CD.obj &lt;- CD(response)

print(CD.obj)

## CD plot
plot(CD.obj)

## Get the RMSE.Eigs and nfact results.
RMSE.Eigs &lt;- CD.obj$RMSE.Eigs
nfact &lt;- CD.obj$nfact

head(RMSE.Eigs)
print(nfact)



## Limit the maximum number of factors to 8, with populations set to 5000.

CD.obj &lt;- CD(response, nfact.max=8, N.pop = 5000)

print(CD.obj)

## CD plot
plot(CD.obj)

## Get the RMSE.Eigs and nfact results.
RMSE.Eigs &lt;- CD.obj$RMSE.Eigs
nfact &lt;- CD.obj$nfact

head(RMSE.Eigs)
print(nfact)






</code></pre>

<hr>
<h2 id='CDF'>the  Comparison Data Forest (CDF) Approach</h2><span id='topic+CDF'></span>

<h3>Description</h3>

<p>The Comparison Data Forest (CDF; Goretzko &amp; Ruscio, 2019) approach is a combination of Random Forest with the comparison data (CD) approach.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CDF(
  response,
  num.trees = 500,
  mtry = "sqrt",
  nfact.max = 10,
  N.pop = 10000,
  N.Samples = 500,
  cor.type = "pearson",
  use = "pairwise.complete.obs",
  vis = TRUE,
  plot = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="CDF_+3A_response">response</code></td>
<td>
<p>A required <code>N</code> × <code>I</code> matrix or data.frame consisting of the responses of <code>N</code> individuals
to × <code>I</code> items.</p>
</td></tr>
<tr><td><code id="CDF_+3A_num.trees">num.trees</code></td>
<td>
<p>the number of trees in the Random Forest. (default = 500) See details.</p>
</td></tr>
<tr><td><code id="CDF_+3A_mtry">mtry</code></td>
<td>
<p>the maximum depth for each tree, can be a number or a character (<code>"sqrt"</code>).
When <code>mtry = "sqrt"</code>, it means that the maximum depth of each tree will be determined by the square root of
the number of available features (converted to an integer by <a href="base.html#topic+round">round</a>).default = <code>"sqrt"</code>. See details.</p>
</td></tr>
<tr><td><code id="CDF_+3A_nfact.max">nfact.max</code></td>
<td>
<p>The maximum number of factors discussed by CD approach. (default = 10)</p>
</td></tr>
<tr><td><code id="CDF_+3A_n.pop">N.pop</code></td>
<td>
<p>Size of finite populations of simulating.. (default = 10,000)</p>
</td></tr>
<tr><td><code id="CDF_+3A_n.samples">N.Samples</code></td>
<td>
<p>Number of samples drawn from each population. (default = 500)</p>
</td></tr>
<tr><td><code id="CDF_+3A_cor.type">cor.type</code></td>
<td>
<p>A character string indicating which correlation coefficient (or covariance) is to be computed. One of &quot;pearson&quot; (default),
&quot;kendall&quot;, or &quot;spearman&quot;. @seealso <a href="stats.html#topic+cor">cor</a>.</p>
</td></tr>
<tr><td><code id="CDF_+3A_use">use</code></td>
<td>
<p>an optional character string giving a method for computing covariances in the presence of missing values. This
must be one of the strings &quot;everything&quot;, &quot;all.obs&quot;, &quot;complete.obs&quot;, &quot;na.or.complete&quot;, or &quot;pairwise.complete.obs&quot; (default).
@seealso <a href="stats.html#topic+cor">cor</a>.</p>
</td></tr>
<tr><td><code id="CDF_+3A_vis">vis</code></td>
<td>
<p>A Boolean variable that will print the factor retention results when set to TRUE, and will not print
when set to FALSE. (default = TRUE)</p>
</td></tr>
<tr><td><code id="CDF_+3A_plot">plot</code></td>
<td>
<p>A Boolean variable that will print the CDF plot when set to TRUE, and will not print it when set to
FALSE. @seealso <a href="#topic+plot.CDF">plot.CDF</a>. (Default = TRUE)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Comparison Data Forest (CDF; Goretzko &amp; Ruscio, 2019) Approach is a combination of
random forest with the comparison data (CD) approach. Its basic steps involve using the method
of Ruscio &amp; Roche (2012) to simulate data with different factor counts, then extracting features
from this data to train a random forest model. Once the model is trained, it can be used to
predict the number of factors in empirical data. The algorithm consists of the following steps:
</p>
<p>1. **Simulation Data:**
</p>

<dl>
<dt>(1)</dt><dd><p>For each value of <code class="reqn">nfact</code> in the range from 1 to <code class="reqn">nfact_{max}</code>,
generate a population data using the <a href="#topic+GenData">GenData</a> function.</p>
</dd>
<dt>(2)</dt><dd><p>Each population is based on <code class="reqn">nfact</code> factors and consists of <code class="reqn">N_{pop}</code> observations.</p>
</dd>
<dt>(3)</dt><dd><p>For each generated population, repeat the following for <code class="reqn">N_{rep}</code> times, For the <code class="reqn">j</code>-th in <code class="reqn">N_{rep}</code>:
a. Draw a sample <code class="reqn">N_{sam}</code> from the population that matches the size of the empirical data;
b. Compute a feature set <code class="reqn">\mathbf{fea}_{nfact,j}</code> from each <code class="reqn">N_{sam}</code>.</p>
</dd>
<dt>(4)</dt><dd><p>Combine all the generated feature sets <code class="reqn">\mathbf{fea}_{nfact,j}</code>
into a data frame as <code class="reqn">\mathbf{data}_{train, nfact}</code>.</p>
</dd>
<dt>(5)</dt><dd><p>Combine all <code class="reqn">\mathbf{data}_{train, nfact}</code> into a final data frame as the training dataset <code class="reqn">\mathbf{data}_{train}</code>.</p>
</dd>
</dl>

<p>2. **Training RF:**
</p>
<p>Train a Random Forest model <code class="reqn">RF</code> using the combined <code class="reqn">\mathbf{data}_{train}</code>.
</p>
<p>3. **Prediction the Empirical Data:**
</p>

<dl>
<dt>(1)</dt><dd><p>Calculate the feature set <code class="reqn">\mathbf{fea}_{emp}</code>for the empirical data.</p>
</dd>
<dt>(2)</dt><dd><p>Use the trained Random Forest model <code class="reqn">RF</code> to predict the number of factors <code class="reqn">nfact_{emp}</code> for the empirical data:
</p>
<p style="text-align: center;"><code class="reqn">nfact_{emp} = RF(\mathbf{fea}_{emp})</code>
</p>
</dd>
</dl>

<p>According to Goretzko &amp; Ruscio (2024) and Breiman (2001), the number of
trees in the Random Forest <code>num.trees</code> is recommended to be 500.
The Random Forest in CDF performs a classification task, so the recommended maximum
depth for each tree <code>mtry</code> is <code class="reqn">\sqrt{q}</code> (where <code class="reqn">q</code> is the number of features),
which results in <code class="reqn">m_{try}=\sqrt{181}=13</code>.
</p>
<p>Since the CDF approach requires extensive data simulation and computation, which is much more time consuming
than the <a href="#topic+CD">CD</a> Approach, C++ code is used to speed up the process.
</p>


<h3>Value</h3>

<p>An object of class <code>CDF</code> is a <code>list</code> containing the following components:
</p>
<table role = "presentation">
<tr><td><code>nfact</code></td>
<td>
<p>The number of factors to be retained.</p>
</td></tr>
<tr><td><code>RF</code></td>
<td>
<p>the trained Random Forest model</p>
</td></tr>
<tr><td><code>probability</code></td>
<td>
<p>A matrix containing the probabilities for factor numbers ranging from 1
to nfact.max (1xnfact.max), where the number in the f-th column represents the probability
that the number of factors for the response is f.</p>
</td></tr>
<tr><td><code>features</code></td>
<td>
<p>A matrix (1×181) containing all the features for determining the number of
factors. @seealso <a href="#topic+extractor.feature.FF">extractor.feature.FF</a></p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Haijiang Qin &lt;Haijiang133@outlook.com&gt;
</p>


<h3>References</h3>

<p>Breiman, L. (2001). Random Forests. Machine Learning, 45(1), 5-32. https://doi.org/10.1023/A:1010933404324
</p>
<p>Goretzko, D., &amp; Ruscio, J. (2024). The comparison data forest: A new comparison data approach to determine the number of factors in exploratory factor analysis. Behavior Research Methods, 56(3), 1838-1851. https://doi.org/10.3758/s13428-023-02122-4
</p>
<p>Ruscio, J., &amp; Roche, B. (2012). Determining the number of factors to retain in an exploratory factor analysis using comparison data of known factorial structure. Psychological Assessment, 24, 282–292. http://dx.doi.org/10.1037/a0025697.
</p>


<h3>See Also</h3>

<p><a href="#topic+GenData">GenData</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(EFAfactors)
set.seed(123)

##Take the data.bfi dataset as an example.
data(data.bfi)

response &lt;- as.matrix(data.bfi[, 1:25]) ## loading data
response &lt;- na.omit(response) ## Remove samples with NA/missing values

## Transform the scores of reverse-scored items to normal scoring
response[, c(1, 9, 10, 11, 12, 22, 25)] &lt;- 6 - response[, c(1, 9, 10, 11, 12, 22, 25)] + 1

## Run CDF function with default parameters.

CDF.obj &lt;- CDF(response)

print(CDF.obj)

## CDF plot
plot(CDF.obj)

## Get the nfact results.
nfact &lt;- CDF.obj$nfact
print(nfact)



## Limit the maximum number of factors to 8, with populations set to 5000.

CDF.obj &lt;- CDF(response, nfact.max=8, N.pop = 5000)

print(CDF.obj)

## CDF plot
plot(CDF.obj)

## Get the nfact results.
nfact &lt;- CDF.obj$nfact
print(nfact)





</code></pre>

<hr>
<h2 id='check_python_libraries'>Check and Install Python Libraries (numpy and onnxruntime)</h2><span id='topic+check_python_libraries'></span>

<h3>Description</h3>

<p>This function checks whether the Python libraries 'numpy' and 'onnxruntime' are installed. If not, it will prompt
the user to decide whether to install them. If the user chooses 'y', the required library will be installed using
the &lsquo;reticulate' package. If the user chooses &rsquo;n', the installation will be skipped. @seealso <a href="#topic+DNN_predictor">DNN_predictor</a>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_python_libraries()
</code></pre>


<h3>Value</h3>

<p>A list indicating whether 'numpy' and 'onnxruntime' are installed.
The list contains the following logical elements:
</p>
<table role = "presentation">
<tr><td><code>numpy_installed</code></td>
<td>
<p>TRUE if 'numpy' is installed, FALSE otherwise.</p>
</td></tr>
<tr><td><code>onnxruntime_installed</code></td>
<td>
<p>TRUE if 'onnxruntime' is installed, FALSE otherwise.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'># Check and install necessary Python libraries
check_python_libraries()

</code></pre>

<hr>
<h2 id='data.bfi'>25 Personality Items Representing 5 Factors</h2><span id='topic+data.bfi'></span>

<h3>Description</h3>

<p>This dataset includes 25 self-report personality items sourced from the International Personality Item Pool
(ipip.ori.org) as part of the Synthetic Aperture Personality Assessment (SAPA) web-based personality
assessment project. The dataset contains responses from 2,800 examinees. Additionally, three
demographic variables (sex, education, and age) are included.
</p>


<h3>Format</h3>

<p>A data frame with 2,800 observations on 28 variables. The variables include:
</p>

<ul>
<li> <p><code>A1</code> - Am indifferent to the feelings of others. (q_146)
</p>
</li>
<li> <p><code>A2</code> - Inquire about others’ well-being. (q_1162)
</p>
</li>
<li> <p><code>A3</code> - Know how to comfort others. (g_1206)
</p>
</li>
<li> <p><code>A4</code> - Love children. (g_1364)
</p>
</li>
<li> <p><code>A5</code> - Make people feel at ease. (q_1419)
</p>
</li>
<li> <p><code>C1</code> - Am exacting in my work. (q_124)
</p>
</li>
<li> <p><code>C2</code> - Continue until everything is perfect. (q_530)
</p>
</li>
<li> <p><code>C3</code> - Do things according to a plan. (q_619)
</p>
</li>
<li> <p><code>C4</code> - Do things in a half-way manner. (g_626)
</p>
</li>
<li> <p><code>C5</code> - Waste my time. (g_1949)
</p>
</li>
<li> <p><code>E1</code> - Don't talk a lot. (q_712)
</p>
</li>
<li> <p><code>E2</code> - Find it difficult to approach others. (q_901)
</p>
</li>
<li> <p><code>E3</code> - Know how to captivate people. (q_1205)
</p>
</li>
<li> <p><code>E4</code> - Make friends easily. (q_1410)
</p>
</li>
<li> <p><code>E5</code> - Take charge. (g_1768)
</p>
</li>
<li> <p><code>N1</code> - Get angry easily. (q_952)
</p>
</li>
<li> <p><code>N2</code> - Get irritated easily. (q_974)
</p>
</li>
<li> <p><code>N3</code> - Have frequent mood swings. (q_1099)
</p>
</li>
<li> <p><code>N4</code> - Often feel blue. (g_1479)
</p>
</li>
<li> <p><code>N5</code> - Panic easily. (q_1505)
</p>
</li>
<li> <p><code>O1</code> - Am full of ideas. (q_128)
</p>
</li>
<li> <p><code>O2</code> - Avoid difficult reading material. (g_316)
</p>
</li>
<li> <p><code>O3</code> - Carry the conversation to a higher level. (q_492)
</p>
</li>
<li> <p><code>O4</code> - Spend time reflecting on things. (g_1738)
</p>
</li>
<li> <p><code>O5</code> - Will not probe deeply into a subject. (q_1964)
</p>
</li>
<li> <p><code>gender</code> - Gender: Males = 1, Females = 2
</p>
</li>
<li> <p><code>education</code> - Education level: 1 = High School, 2 = Finished High School,
3 = Some College, 4 = College Graduate, 5 = Graduate Degree
</p>
</li>
<li> <p><code>age</code> - Age in years
</p>
</li></ul>



<h3>Details</h3>

<p>The 25 items are organized by five factors: Agreeableness, Conscientiousness, Extraversion,
Neuroticism, and Openness. The scoring key is created using <code>make.keys</code>, and scores are
calculated using <code>score.items</code>. These factors are useful for IRT-based latent factor analysis
of the polychoric correlation matrix. Endorsement plots and item information functions reveal
variations in item quality. Responses were collected on a 6-point scale:
1 = Very Inaccurate, 2 = Moderately Inaccurate, 3 = Slightly Inaccurate, 4 = Slightly Accurate,
5 = Moderately Accurate, 6 = Very Accurate, as part of the Synthetic Aperture Personality Assessment (SAPA)
project (https://www.sapa-project.org/). For examples of data collection techniques, visit
https://www.sapa-project.org/ or the International Cognitive Ability Resource at
https://icar-project.org. The items were sampled from the International Personality Item Pool of
Lewis Goldberg using SAPA sampling techniques. This dataset is a sample from the larger SAPA data bank.
</p>


<h3>Note</h3>

<p>The data.bfi data set and items should not be confused with the BFI (Big Five Inventory) of Oliver Johnand colleagues (John, O. P,
Donahue, E. M., &amp; Kentle, R.L. (1991). The Big Five Inventory Versions 4a and 54. Berkeley, CA: University of California,
Berkeley, Institute of Personality and Social Research.)
</p>


<h3>Source</h3>

<p>The items are from the ipip (Goldberg, 1999). The data are from the SAPA project (Revelle, Wiltand Rosenthal, 2010),
collected Spring, 2010(https://www.sapa-project.org/).
</p>


<h3>References</h3>

<p>Goldberg, L.R. (1999). A broad-bandwidth, public domain, personality inventory measuring the lower-level facets of several five-factor models. In Mervielde, I., Deary, I., De Fruyt, F., &amp; Ostendorf, F. (Eds.), Personality psychology in Europe (Vol. 7, pp. 7-28). Tilburg University Press.
</p>
<p>Revelle, W., Wilt, J., &amp; Rosenthal, A. (2010). Individual Differences in Cognition: New Methods for Examining the Personality-Cognition Link. In Gruszka, A., Matthews, G., &amp; Szymura, B. (Eds.), Handbook of Individual Differences in Cognition: Attention, Memory and Executive Control (pp. 117-144). Springer.
</p>
<p>Revelle, W., Condon, D., Wilt, J., French, J.A., Brown, A., &amp; Elleman, L.G. (2016). Web and phone-based data collection using planned missing designs. In Fielding, N.G., Lee, R.M., &amp; Blank, G. (Eds.), SAGE Handbook of Online Research Methods (2nd ed., pp. 100-116). Sage Publications.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(data.bfi)
head(data.bfi)

</code></pre>

<hr>
<h2 id='data.datasets'>Subset Dataset for Training the Pre-Trained Deep Neural Network (DNN)</h2><span id='topic+data.datasets'></span>

<h3>Description</h3>

<p>This dataset is a subset of the full datasets, consisting of 1,000 samples
from the original 10,000,000-sample datasets.
</p>


<h3>Format</h3>

<p>A 1,000×55 matrix, where the first 54 columns represent feature values and
the last column represents the labels, which correspond to the number of factors associated with the features.
</p>


<h3>Note</h3>

<p>Methods for generating and extracting features from the dataset can be found in <a href="#topic+DNN_predictor">DNN_predictor</a>.
</p>


<h3>See Also</h3>

<p><a href="#topic+DNN_predictor">DNN_predictor</a>, <a href="#topic+load_scaler">load_scaler</a>, <a href="#topic+data.scaler">data.scaler</a>, <a href="#topic+normalizor">normalizor</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(data.datasets)
head(data.datasets)

</code></pre>

<hr>
<h2 id='data.scaler'>the Scaler for the Pre-Trained Deep Neural Network (DNN)</h2><span id='topic+data.scaler'></span>

<h3>Description</h3>

<p>This dataset contains the means and standard deviations of the 10,000,000 datasets for
training Pre-Trained Deep Neural Network (DNN), which can be used to determine the number of factors.
</p>


<h3>Format</h3>

<p>A <code>list</code> containing two <code>vector</code>s, each of length 54:
</p>

<dl>
<dt>means</dt><dd><p>A numeric vector representing the means of the 54
features extracted from the 10,000,000 datasets.</p>
</dd>
<dt>sds</dt><dd><p>A numeric vector representing the standard deviations of
the 54 features extracted from the 10,000,000 datasets.</p>
</dd>
</dl>



<h3>See Also</h3>

<p><a href="#topic+DNN_predictor">DNN_predictor</a>, <a href="#topic+load_scaler">load_scaler</a>, <a href="#topic+data.datasets">data.datasets</a>, <a href="#topic+normalizor">normalizor</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(data.scaler)
print(data.scaler)

data.scaler &lt;- load_scaler()
print(data.scaler)

</code></pre>

<hr>
<h2 id='DNN_predictor'>A Pre-Trained Deep Neural Network (DNN) for Determining the Number of Factors</h2><span id='topic+DNN_predictor'></span>

<h3>Description</h3>

<p>This function will invoke a pre-trained deep neural network that can reliably
perform the task of determining the number of factors. The maximum number of
factors that the network can discuss is 10. The DNN model is implemented in Python
and trained on PyTorch (https://pytorch.org/) with
CUDA 11.8 for acceleration. After training, the DNN was saved as a <code>DNN.onnx</code>
file. The <em>DNN_predictor</em> function performs inference by loading the <code>DNN.onnx</code>
file in both Python and R environments. Therefore, please note that Python (suggested &gt;= 3.10) and the
libraries <code>numpy</code> and <code>onnxruntime</code> are required. @seealso <a href="#topic+check_python_libraries">check_python_libraries</a>
</p>
<p>To run this function, Python is required, along with the installation of <code>numpy</code> and <code>onnxruntime</code>. See more in Details and Note.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>DNN_predictor(
  response,
  cor.type = "pearson",
  use = "pairwise.complete.obs",
  vis = TRUE,
  plot = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="DNN_predictor_+3A_response">response</code></td>
<td>
<p>A required <code>N</code> × <code>I</code> matrix or data.frame consisting of the responses of <code>N</code> individuals
to <code>I</code> items.</p>
</td></tr>
<tr><td><code id="DNN_predictor_+3A_cor.type">cor.type</code></td>
<td>
<p>A character string indicating which correlation coefficient (or covariance) is to be computed. One of &quot;pearson&quot; (default),
&quot;kendall&quot;, or &quot;spearman&quot;. @seealso <a href="stats.html#topic+cor">cor</a>.</p>
</td></tr>
<tr><td><code id="DNN_predictor_+3A_use">use</code></td>
<td>
<p>An optional character string giving a method for computing covariances in the presence of missing values. This
must be one of the strings &quot;everything&quot;, &quot;all.obs&quot;, &quot;complete.obs&quot;, &quot;na.or.complete&quot;, or &quot;pairwise.complete.obs&quot; (default).
@seealso <a href="stats.html#topic+cor">cor</a>.</p>
</td></tr>
<tr><td><code id="DNN_predictor_+3A_vis">vis</code></td>
<td>
<p>A Boolean variable that will print the factor retention results when set to TRUE, and will not print
when set to FALSE. (default = TRUE)</p>
</td></tr>
<tr><td><code id="DNN_predictor_+3A_plot">plot</code></td>
<td>
<p>A Boolean variable that will print the DNN_predictor plot when set to TRUE, and will not print it when set to
FALSE. @seealso <a href="#topic+plot.DNN_predictor">plot.DNN_predictor</a>. (Default = TRUE)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Due to the improved performance of deep learning models with larger datasets (Chen et al., 2017),
a total of 10,000,000 datasets (<a href="#topic+data.datasets">data.datasets</a>) were simulated
to extract features for training deep learning neural networks.
Each dataset was generated following the methods described by Auerswald &amp; Moshagen (2019) and Goretzko &amp; Buhner (2020),
with the following specifications:
</p>

<ul>
<li><p> Factor number: <em>F</em> ~ U[1,10]
</p>
</li>
<li><p> Sample size: <em>N</em> ~ U[100,1000]
</p>
</li>
<li><p> Number of variables per factor: <em>vpf</em> ~ [3,20]
</p>
</li>
<li><p> Factor correlation: <em>fc</em> ~ U[0.0,0.4]
</p>
</li>
<li><p> Primary loadings: <em>pl</em> ~ U[0.35,0.80]
</p>
</li>
<li><p> Cross-loadings: <em>cl</em> ~ U[-0.2,0.2]
</p>
</li></ul>

<p>A population correlation matrix was created for each data set based on the following decomposition:
</p>
<p style="text-align: center;"><code class="reqn">\mathbf{\Sigma} = \mathbf{\Lambda} \mathbf{\Phi} \mathbf{\Lambda}^T + \mathbf{\Delta}</code>
</p>

<p>where <code class="reqn">\mathbf{\Lambda}</code> is the loading matrix, <code class="reqn">\mathbf{\Phi}</code> is the factor correlation
matrix, and <code class="reqn">\mathbf{\Delta}</code> is a diagonal matrix,
with <code class="reqn">\mathbf{\Delta} = 1 - \text{diag}(\mathbf{\Lambda} \mathbf{\Phi} \mathbf{\Lambda}^T)</code>.
The purpose of <code class="reqn">\mathbf{\Delta}</code> is to ensure that the diagonal elements of <code class="reqn">\mathbf{\Sigma} </code> are 1.
</p>
<p>The response data for each subject was simulated using the following formula:
</p>
<p style="text-align: center;"><code class="reqn">X_i = L_i + \epsilon_i, \quad 1 \leq i \leq I</code>
</p>

<p>where <code class="reqn">L_i</code> follows a normal distribution <code class="reqn">N(0, \sigma)</code>, representing the contribution of latent factors,
and <code class="reqn">\epsilon_i</code> is the residual term following a standard normal distribution. <code class="reqn">L_i</code> and <code class="reqn">\epsilon_i</code>
are uncorrelated, and <code class="reqn">\epsilon_i</code> and <code class="reqn">\epsilon_j</code> are also uncorrelated.
</p>
<p>For each simulated dataset, a total of 6 types of features (which can be
classified into 2 types; @seealso <a href="#topic+extractor.feature.DNN">extractor.feature.DNN</a>)
are extracted and compiled into a feature vector, consisting of 54 features: 8 + 8 + 8 + 10 + 10 + 10.
These features are as follows:
</p>
<p>1. Clustering-Based Features
</p>

<dl>
<dt>(1)</dt><dd><p>Hierarchical clustering is performed with correlation coefficients as dissimilarity.
The top 9 tree node heights are calculated, and all heights are divided by the maximum
height. The heights from the 2nd to 9th nodes are used as features. @seealso <a href="#topic+EFAhclust">EFAhclust</a></p>
</dd>
<dt>(2)</dt><dd><p>Hierarchical clustering with Euclidean distance as dissimilarity is performed. The top 9
tree node heights are calculated, and all heights are divided by the maximum height. The
heights from the 2nd to 9th nodes are used as features. @seealso <a href="#topic+EFAhclust">EFAhclust</a></p>
</dd>
<dt>(3)</dt><dd><p>K-means clustering is applied with the number of clusters ranging from 1 to 9. The
within-cluster sum of squares (WSS) for clusters 2 to 9 are divided by the WSS for a single
cluster. @seealso <a href="#topic+EFAkmeans">EFAkmeans</a></p>
</dd>
</dl>

<p>These three features are based on clustering algorithms. The purpose of division is to normalize the
data. These clustering metrics often contain information unrelated to the number of factors, such as
the number of items and the number of respondents, which can be avoided by normalization. The reason
for using the 2nd to 9th data is that only the top F-1 data are needed to determine the number of factors F.
The first data point is fixed at 1 after the division operations, so it is excluded. This approach
helps in model simplification.
</p>
<p>2. Traditional Exploratory Factor Analysis Features (Eigenvalues)
</p>

<dl>
<dt>(4)</dt><dd><p>The top 10 largest eigenvalues.</p>
</dd>
<dt>(5)</dt><dd><p>The ratio of the top 10 largest eigenvalues to the corresponding reference eigenvalues from
Empirical Kaiser Criterion (EKC; Braeken &amp; van Assen, 2017). @seealso <a href="#topic+EKC">EKC</a></p>
</dd>
<dt>(6)</dt><dd><p>The cumulative variance proportion of the top 10 largest eigenvalues.</p>
</dd>
</dl>

<p>Only the top 10 elements are used to simplify the model.
</p>
<p>The DNN model is implemented in Python and trained on PyTorch (https://download.pytorch.org/whl/cu118) with
CUDA 11.8 for acceleration. After training, the DNN was saved as a <code>DNN.onnx</code> file. The <em>DNN_predictor</em> function
performs inference by loading the <code>DNN.onnx</code> file in both Python and R environments.
</p>


<h3>Value</h3>

<p>An object of class <code>DNN_predictor</code> is a <code>list</code> containing the following components:
</p>
<table role = "presentation">
<tr><td><code>nfact</code></td>
<td>
<p>The number of factors to be retained.</p>
</td></tr>
<tr><td><code>features</code></td>
<td>
<p>A matrix (1×54) containing all the features for determining the number of
factors by the DNN.</p>
</td></tr>
<tr><td><code>probability</code></td>
<td>
<p>A matrix containing the probabilities for factor numbers ranging from 1
to 10 (1x10), where the number in the f-th column represents the probability
that the number of factors for the response is f.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Note that Python and the libraries <code>numpy</code> and <code>onnxruntime</code> are required.
</p>
<p>First, please ensure that Python is installed on your computer and that Python is
included in the system's PATH environment variable. If not,
please download and install it from the official website (https://www.python.org/).
</p>
<p>If you encounter an error when running this function stating that the <code>numpy</code> and <code>onnxruntime</code>
modules are missing:
</p>
<p><code>Error in py_module_import(module, convert = convert) :</code>
</p>
<p><code>ModuleNotFoundError: No module named 'numpy'</code>
</p>
<p>or
</p>
<p><code>Error in py_module_import(module, convert = convert) :</code>
</p>
<p><code>ModuleNotFoundError: No module named 'onnxruntime'</code>
</p>
<p>this means that the <code>numpy</code> or <code>onnxruntime</code> library is missing from your Python environment.
The <a href="#topic+check_python_libraries">check_python_libraries</a> function can help you install these two dependency libraries. @seealso <a href="#topic+check_python_libraries">check_python_libraries</a>
</p>
<p>Of course, you can also choose not to use the <a href="#topic+check_python_libraries">check_python_libraries</a> function. You can
directly install the <code>numpy</code> or <code>onnxruntime</code> library using the appropriate commands.
If you are using Windows or macOS, please run the command <code>pip install numpy</code> or <code>pip install onnxruntime</code>
in Command Prompt or Windows PowerShell (Windows), or Terminal (macOS). If you are using Linux, please ensure that
<code>pip</code> is installed and use the command <code>pip install numpy</code> or <code>pip install onnxruntime</code> to install
the missing libraries.
</p>


<h3>Author(s)</h3>

<p>Haijiang Qin &lt;Haijiang133@outlook.com&gt;
</p>


<h3>References</h3>

<p>Auerswald, M., &amp; Moshagen, M. (2019). How to determine the number of factors to retain in exploratory factor analysis: A comparison of extraction methods under realistic conditions. Psychological methods, 24(4), 468-491. https://doi.org/10.1037/met0000200.
</p>
<p>Braeken, J., &amp; van Assen, M. A. L. M. (2017). An empirical Kaiser criterion. Psychological methods, 22(3), 450-466. https://doi.org/10.1037/met0000074.
</p>
<p>Goretzko, D., &amp; Buhner, M. (2020). One model to rule them all? Using machine learning algorithms to determine the number of factors in exploratory factor analysis. Psychol Methods, 25(6), 776-786. https://doi.org/10.1037/met0000262.
</p>

<hr>
<h2 id='EFAhclust'>Hierarchical Clustering for EFA</h2><span id='topic+EFAhclust'></span>

<h3>Description</h3>

<p>A function performs clustering on items by calling <a href="stats.html#topic+hclust">hclust</a>.
Hierarchical cluster analysis on a set of dissimilarities and methods for analyzing it.
The items will be continuously clustered in pairs until all items are grouped
into a single cluster, at which point the process will stop.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>EFAhclust(
  response,
  dissimilarity.type = "R",
  method = "ward.D",
  cor.type = "pearson",
  use = "pairwise.complete.obs",
  nfact.max = 10,
  plot = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="EFAhclust_+3A_response">response</code></td>
<td>
<p>A required <code>N</code> × <code>I</code> matrix or data.frame consisting of the responses of <code>N</code> individuals
to <code>I</code> items.</p>
</td></tr>
<tr><td><code id="EFAhclust_+3A_dissimilarity.type">dissimilarity.type</code></td>
<td>
<p>A character indicating which kind of dissimilarity is to be computed. One of &quot;R&quot; or &quot;E&quot; (default)
for the correlation coefficient or Euclidean distance.</p>
</td></tr>
<tr><td><code id="EFAhclust_+3A_method">method</code></td>
<td>
<p>the agglomeration method to be used. This should be (an unambiguous abbreviation of) one of &quot;ward.D&quot;,
&quot;ward.D2&quot;, &quot;single&quot;, &quot;complete&quot;, &quot;average&quot; (= UPGMA), &quot;mcquitty&quot; (= WPGMA), &quot;median&quot; (= WPGMC) or &quot;centroid&quot; (= UPGMC).
(default = &quot;ward.D&quot;)
@seealso <code><a href="stats.html#topic+hclust">hclust</a></code></p>
</td></tr>
<tr><td><code id="EFAhclust_+3A_cor.type">cor.type</code></td>
<td>
<p>A character string indicating which correlation coefficient (or covariance) is to be computed. One of &quot;pearson&quot; (default),
&quot;kendall&quot;, or &quot;spearman&quot;. @seealso <a href="stats.html#topic+cor">cor</a>.</p>
</td></tr>
<tr><td><code id="EFAhclust_+3A_use">use</code></td>
<td>
<p>An optional character string giving a method for computing covariances in the presence of missing values. This
must be one of the strings &quot;everything&quot;, &quot;all.obs&quot;, &quot;complete.obs&quot;, &quot;na.or.complete&quot;, or &quot;pairwise.complete.obs&quot; (default).
@seealso <a href="stats.html#topic+cor">cor</a>.</p>
</td></tr>
<tr><td><code id="EFAhclust_+3A_nfact.max">nfact.max</code></td>
<td>
<p>The maximum number of factors discussed by the Second-Order Difference (SOD) approach. (default = 10)</p>
</td></tr>
<tr><td><code id="EFAhclust_+3A_plot">plot</code></td>
<td>
<p>A Boolean variable that will print the EFAhclust plot when set to TRUE, and will not print it when set to
FALSE. @seealso <a href="#topic+plot.EFAhclust">plot.EFAhclust</a>. (Default = TRUE)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Hierarchical cluster analysis always merges the two nodes with the smallest dissimilarity,
forming a new node in the process. This continues until all nodes are merged into one
large node, at which point the algorithm terminates. This method undoubtedly creates a hierarchical
structure by the end of the process, which encompasses the relationships between all items:
items with high correlation have short connecting lines between them, while items with low correlation
have longer lines. This hierarchical structure is well-suited to be represented as a binary tree.
In this representation, the dissimilarity between two nodes can be indicated by the height of the
tree nodes; the greater the difference between nodes, the higher the height of the tree nodes connecting
them (the longer the line). Researchers can decide whether two nodes belong to the same cluster
based on the height differences between nodes, which, in exploratory factor analysis, represents
whether these two nodes belong to the same latent factor.
</p>
<p>The Second-Order Difference (SOD) approach is a commonly used method for finding the &quot;elbow&quot;
(the point of greatest slope change). According to the principles of exploratory factor analysis,
items belonging to different latent factors have lower correlations, while items under the same
factor are more highly correlated. In hierarchical clustering, this is reflected in the height of
the nodes in the dendrogram, with differences in node heights representing the relationships between items.
By sorting all node heights in descending order and applying the SOD method to locate the elbow,
the number of factors can be determined. @seealso <a href="#topic+EFAkmeans">EFAkmeans</a>
</p>


<h3>Value</h3>

<p>An object of class <code>EFAhclust</code> is a <code>list</code> containing the following components:
</p>
<table role = "presentation">
<tr><td><code>hc</code></td>
<td>
<p>An object of class <code>hclust</code> that describes the tree produced by the clustering process. @seealso <a href="stats.html#topic+hclust">hclust</a></p>
</td></tr>
<tr><td><code>cor.response</code></td>
<td>
<p>A matrix of dimension <code>I × I</code> containing all the correlation coefficients of items.</p>
</td></tr>
<tr><td><code>clusters</code></td>
<td>
<p>A list containing all the clusters.</p>
</td></tr>
<tr><td><code>heights</code></td>
<td>
<p>A vector containing all the heights of the cluster tree. The heights are arranged in descending order.</p>
</td></tr>
<tr><td><code>nfact.SOD</code></td>
<td>
<p>The number of factors to be retained by the Second-Order Difference (SOD) approach.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Batagelj, V. (1988). Generalized Ward and Related Clustering Problems. In H. H. Bock, Classification and Related Methods of Data Analysis the First Conference of the International Federation of Classification Societies (IFCS), Amsterdam.
</p>
<p>Murtagh, F., &amp; Legendre, P. (2014). Ward’s Hierarchical Agglomerative Clustering Method: Which Algorithms Implement Ward’s Criterion? Journal of Classification, 31(3), 274-295. https://doi.org/10.1007/s00357-014-9161-z.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(EFAfactors)
set.seed(123)

##Take the data.bfi dataset as an example.
data(data.bfi)

response &lt;- as.matrix(data.bfi[, 1:25]) ## loading data
response &lt;- na.omit(response) ## Remove samples with NA/missing values

## Transform the scores of reverse-scored items to normal scoring
response[, c(1, 9, 10, 11, 12, 22, 25)] &lt;- 6 - response[, c(1, 9, 10, 11, 12, 22, 25)] + 1


## Run EFAhclust function with default parameters.

EFAhclust.obj &lt;- EFAhclust(response)

plot(EFAhclust.obj)

## Get the heights.
heights &lt;- EFAhclust.obj$heights
print(heights)

## Get the nfact retained by SOD
nfact.SOD &lt;- EFAhclust.obj$nfact.SOD
print(nfact.SOD)






</code></pre>

<hr>
<h2 id='EFAindex'>Various Indeces in EFA</h2><span id='topic+EFAindex'></span>

<h3>Description</h3>

<p>A function performs clustering on items by calling <a href="psych.html#topic+VSS">VSS</a> and <a href="psych.html#topic+fa">fa</a>.
Apply the Very Simple Structure (VSS), Comparative Fit Index (CFI), MAP, and other
criteria to determine the appropriate number of factors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>EFAindex(
  response,
  nfact.max = 10,
  cor.type = "cor",
  use = "pairwise.complete.obs"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="EFAindex_+3A_response">response</code></td>
<td>
<p>A required <code>N</code> × <code>I</code> matrix or data.frame consisting of the responses of <code>N</code> individuals
to <code>I</code> items.</p>
</td></tr>
<tr><td><code id="EFAindex_+3A_nfact.max">nfact.max</code></td>
<td>
<p>The maximum number of factors discussed by CD approach. (default = 10)</p>
</td></tr>
<tr><td><code id="EFAindex_+3A_cor.type">cor.type</code></td>
<td>
<p>How to find the correlations: &quot;cor&quot; is Pearson&quot;, &quot;cov&quot; is covariance, &quot;tet&quot; is tetrachoric, &quot;poly&quot; is polychoric,
&quot;mixed&quot; uses mixed cor for a mixture of tetrachorics, polychorics, Pearsons, biserials, and
polyserials, Yuleb is Yulebonett, Yuleq and YuleY are the obvious Yule coefficients as appropriate.</p>
</td></tr>
<tr><td><code id="EFAindex_+3A_use">use</code></td>
<td>
<p>an optional character string giving a method for computing covariances in the presence of missing values. This
must be one of the strings &quot;everything&quot;, &quot;all.obs&quot;, &quot;complete.obs&quot;, &quot;na.or.complete&quot;, or &quot;pairwise.complete.obs&quot; (default).
@seealso <a href="stats.html#topic+cor">cor</a>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>matrix</code> with the following components:
</p>

<dl>
<dt>CFI</dt><dd><p>the Comparative Fit Index</p>
</dd>
<dt>RMSEA</dt><dd><p>Root Mean Square Error of Approximation (RMSEA) for each number of factors.</p>
</dd>
<dt>SRMR</dt><dd><p>Standardized Root Mean Square Residual.</p>
</dd>
<dt>MAP</dt><dd><p>Velicer's MAP values (lower values are better).</p>
</dd>
<dt>BIC</dt><dd><p>Bayesian Information Criterion (BIC) for each number of factors.</p>
</dd>
<dt>SABIC</dt><dd><p>Sample-size Adjusted Bayesian Information Criterion (SABIC) for each number of factors.</p>
</dd>
<dt>chisq</dt><dd><p>Chi-square statistic from the factor analysis output.</p>
</dd>
<dt>df</dt><dd><p>Degrees of freedom.</p>
</dd>
<dt>prob</dt><dd><p>Probability that the residual matrix is greater than 0.</p>
</dd>
<dt>eChiSq</dt><dd><p>Empirically found chi-square statistic.</p>
</dd>
<dt>eCRMS</dt><dd><p>Empirically found mean residual corrected for degrees of freedom.</p>
</dd>
<dt>eBIC</dt><dd><p>Empirically found BIC based on the empirically found chi-square statistic.</p>
</dd>
<dt>vss</dt><dd><p>VSS fit with complexity 1.</p>
</dd>
<dt>sqresid</dt><dd><p>Squared residual correlations.</p>
</dd>
<dt>fit</dt><dd><p>Factor fit of the complete model.</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>library(EFAfactors)
set.seed(123)

##Take the data.bfi dataset as an example.
data(data.bfi)

response &lt;- as.matrix(data.bfi[, 1:25]) ## loading data
response &lt;- na.omit(response) ## Remove samples with NA/missing values

## Transform the scores of reverse-scored items to normal scoring
response[, c(1, 9, 10, 11, 12, 22, 25)] &lt;- 6 - response[, c(1, 9, 10, 11, 12, 22, 25)] + 1


## Run EFAindex function with default parameters.

EFAindex.matrix &lt;- EFAindex(response)

print(EFAindex.matrix)





</code></pre>

<hr>
<h2 id='EFAkmeans'>K-means for EFA</h2><span id='topic+EFAkmeans'></span>

<h3>Description</h3>

<p>A function performs K-means algorithm on items by calling <a href="stats.html#topic+kmeans">kmeans</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>EFAkmeans(response, nfact.max = 10, plot = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="EFAkmeans_+3A_response">response</code></td>
<td>
<p>A required <code>N</code> × <code>I</code> matrix or data.frame consisting of the responses of <code>N</code> individuals
to <code>I</code> items.</p>
</td></tr>
<tr><td><code id="EFAkmeans_+3A_nfact.max">nfact.max</code></td>
<td>
<p>The maximum number of factors discussed by EFAkmeans approach. (default = 10)</p>
</td></tr>
<tr><td><code id="EFAkmeans_+3A_plot">plot</code></td>
<td>
<p>A Boolean variable that will print the EFAkmeans plot when set to TRUE, and will not print it when set to
FALSE. @seealso <code><a href="#topic+plot.EFAkmeans">plot.EFAkmeans</a></code>. (Default = TRUE)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>K-means is a well-established and widely used classical clustering algorithm.
It is an unsupervised machine learning algorithm that requires the number of clusters K to be specified in advance.
After K-means terminates, the total within-cluster sum of squares (WSS) can be calculated to represent the goodness
of fit of the clustering:
</p>
<p style="text-align: center;"><code class="reqn">WSS = \sum_{\mathbf{C}_k \in \mathbf{C}} \sum_{i \in \mathbf{C}_k} \|i - \mu_k\|^2</code>
</p>

<p>where
<code class="reqn">\mathbf{C}</code> is the set of all clusters.
<code class="reqn">\mathbf{C}_k</code> is the k-th cluster.
<code class="reqn">i</code> represents each item in the cluster <code class="reqn">\mathbf{C}_k</code>.
<code class="reqn">\mu_k</code> is the centroid of cluster <code class="reqn">\mathbf{C}_k</code>.
</p>
<p>Similar to the scree plot where eigenvalues decrease as the number of factors increases,
WSS also decreases as K increases. A &quot;significant reduction&quot; in WSS at a particular K may suggest that K is the
most appropriate number of clusters, which in exploratory factor analysis implies that the number of factors is K.
The &quot;significant reduction&quot; can be identified using the Second-Order Difference (SOD) approach. @seealso <code><a href="#topic+EFAkmeans">EFAkmeans</a></code>
</p>


<h3>Value</h3>

<p>An object of class <code>EFAkmeans</code> is a <code>list</code> containing the following components:
</p>
<table role = "presentation">
<tr><td><code>wss</code></td>
<td>
<p>A vector containing all within-cluster sum of squares (WSS).</p>
</td></tr>
<tr><td><code>nfact.SOD</code></td>
<td>
<p>The number of factors to be retained by the Second-Order Difference (SOD) approach.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>library(EFAfactors)
set.seed(123)

##Take the data.bfi dataset as an example.
data(data.bfi)

response &lt;- as.matrix(data.bfi[, 1:25]) ## loading data
response &lt;- na.omit(response) ## Remove samples with NA/missing values

## Transform the scores of reverse-scored items to normal scoring
response[, c(1, 9, 10, 11, 12, 22, 25)] &lt;- 6 - response[, c(1, 9, 10, 11, 12, 22, 25)] + 1


## Run EFAkmeans function with default parameters.

EFAkmeans.obj &lt;- EFAkmeans(response)

plot(EFAkmeans.obj)

## Get the heights.
wss &lt;- EFAkmeans.obj$wss
print(wss)

## Get the nfact retained by SOD
nfact.SOD &lt;- EFAkmeans.obj$nfact.SOD
print(nfact.SOD)





</code></pre>

<hr>
<h2 id='EFAscreet'>Scree Plot</h2><span id='topic+EFAscreet'></span>

<h3>Description</h3>

<p>This function generates a scree plot to display the eigenvalues of the correlation matrix
computed from the given response data. The scree plot helps in determining the number of
factors to retain in exploratory factor analysis by examining the point at which the
eigenvalues start to level off, indicating less variance explained by additional factors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>EFAscreet(
  response,
  fa = "pc",
  nfact = 1,
  cor.type = "pearson",
  use = "pairwise.complete.obs"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="EFAscreet_+3A_response">response</code></td>
<td>
<p>A required <code>N</code> × <code>I</code> matrix or data.frame consisting of the responses of <code>N</code> individuals
to <code>I</code> items.</p>
</td></tr>
<tr><td><code id="EFAscreet_+3A_fa">fa</code></td>
<td>
<p>A string that determines the method used to obtain eigenvalues. If 'pc', it represents
Principal Component Analysis (PCA); if 'fa', it represents Principal Axis Factoring (a widely
used Factor Analysis method; @seealso <code><a href="#topic+factor.analysis">factor.analysis</a></code>;
Auerswald &amp; Moshagen, 2019). (Default = 'pc')</p>
</td></tr>
<tr><td><code id="EFAscreet_+3A_nfact">nfact</code></td>
<td>
<p>A numeric value that specifies the number of factors to extract, only effective when <code>fa = 'fa'</code>. (Default = 1)</p>
</td></tr>
<tr><td><code id="EFAscreet_+3A_cor.type">cor.type</code></td>
<td>
<p>A character string indicating which correlation coefficient (or covariance) is to be computed. One of &quot;pearson&quot; (default),
&quot;kendall&quot;, or &quot;spearman&quot;. @seealso <a href="stats.html#topic+cor">cor</a>.</p>
</td></tr>
<tr><td><code id="EFAscreet_+3A_use">use</code></td>
<td>
<p>An optional character string giving a method for computing covariances in the presence of missing values. This
must be one of the strings &quot;everything&quot;, &quot;all.obs&quot;, &quot;complete.obs&quot;, &quot;na.or.complete&quot;, or &quot;pairwise.complete.obs&quot; (default).
@seealso <a href="stats.html#topic+cor">cor</a>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>EFAscreet</code> is a <code>list</code> containing the following components:
</p>
<table role = "presentation">
<tr><td><code>eigen.value</code></td>
<td>
<p>A vector containing the empirical eigenvalues</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><a href="#topic+plot.EFAscreet">plot.EFAscreet</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(EFAfactors)
set.seed(123)

##Take the data.bfi dataset as an example.
data(data.bfi)

response &lt;- as.matrix(data.bfi[, 1:25]) ## loading data
response &lt;- na.omit(response) ## Remove samples with NA/missing values

## Transform the scores of reverse-scored items to normal scoring
response[, c(1, 9, 10, 11, 12, 22, 25)] &lt;- 6 - response[, c(1, 9, 10, 11, 12, 22, 25)] + 1


## Run EFAscreet function with default parameters.

EFAscreet.obj &lt;- EFAscreet(response)

plot(EFAscreet.obj)




</code></pre>

<hr>
<h2 id='EFAsim.data'>Simulate Data that Conforms to the theory of Exploratory Factor Analysis.</h2><span id='topic+EFAsim.data'></span>

<h3>Description</h3>

<p>This function is used to simulate data that conforms to the theory of exploratory factor analysis,
with a high degree of customization for the variables involved.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>EFAsim.data(
  nfact,
  vpf,
  N = 500,
  distri = "normal",
  fc = "R",
  pl = "R",
  cl = "R",
  low.vpf = 5,
  up.vpf = 15,
  a = NULL,
  b = NULL,
  vis = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="EFAsim.data_+3A_nfact">nfact</code></td>
<td>
<p>A numeric value specifying the number of factors to simulate.</p>
</td></tr>
<tr><td><code id="EFAsim.data_+3A_vpf">vpf</code></td>
<td>
<p>A numeric or character value specifying the number of items under each factor.
If a numeric value is provided, the numeric must be larger than 2,
and the number of items under each factor will be fixed
to this value. If a character value is provided, it must be one of 'S', 'M', 'L', or 'R'.
These represent random selection of items under each factor from <code class="reqn">U(5, 10)</code>, <code class="reqn">U(5, 15)</code>,
<code class="reqn">U(5, 20)</code>, or <code class="reqn">U(low.vpf up.vpf)</code>, respectively.</p>
</td></tr>
<tr><td><code id="EFAsim.data_+3A_n">N</code></td>
<td>
<p>A numeric value specifying the number of examinees to simulate.</p>
</td></tr>
<tr><td><code id="EFAsim.data_+3A_distri">distri</code></td>
<td>
<p>A character, either 'normal' or 'beta', indicating whether the simulated data
will follow a standard multivariate normal distribution or a multivariate beta distribution.</p>
</td></tr>
<tr><td><code id="EFAsim.data_+3A_fc">fc</code></td>
<td>
<p>A numeric or character value specifying the degree of correlation between factors.
If a numeric value is provided, it must be within the range of 0 to 0.75, and the correlation
between all factors will be fixed at this value. If a character value is provided, it must be 'R',
and the correlations between factors will be randomly selected from <code class="reqn">U(0.0, 0.5)</code>.</p>
</td></tr>
<tr><td><code id="EFAsim.data_+3A_pl">pl</code></td>
<td>
<p>A numeric or character value specifying the size of the primary factor loadings.
If a numeric value is provided, it must be within the range of 0 to 1, and all primary factor
loadings in the loading matrix will be fixed at this value. If a character value is provided,
it must be one of 'L', 'M', 'H', or 'R', representing <code class="reqn">pl~U(0.35, 0.50)</code>, <code class="reqn">pl~U(0.50, 0.65)</code>,
<code class="reqn">pl~U(0.65, 0.80)</code>, or <code class="reqn">pl~U(0.35, 0.80)</code>, respectively, consistent with the settings in Goretzko &amp; Buhner (2020).</p>
</td></tr>
<tr><td><code id="EFAsim.data_+3A_cl">cl</code></td>
<td>
<p>A numeric or character value specifying the size of cross-loadings.
If a numeric value is provided, it must be within the range of 0 to 0.5, and all cross-loadings
in the loading matrix will be fixed at this value. If a character value is provided, it must be
one of 'L', 'H', 'None', or 'R', representing <code class="reqn">cl~U(-0.1, 0.1)</code>, <code class="reqn">cl~U(-0.2, -0.1) \cup U(0.1, 0.2)</code>,
<code class="reqn">cl = 0</code>, or <code class="reqn">cl~U(-0.2, 0.2)</code>, respectively, consistent with the settings in Auerswald &amp; Moshagen (2019).</p>
</td></tr>
<tr><td><code id="EFAsim.data_+3A_low.vpf">low.vpf</code></td>
<td>
<p>A numeric value specifying the minimum number of items per factor, must be larger than 2, effective only when <code>vpf</code> is 'R'. (default = 5)</p>
</td></tr>
<tr><td><code id="EFAsim.data_+3A_up.vpf">up.vpf</code></td>
<td>
<p>A numeric value specifying the maximum number of items per factor, effective only when <code>vpf</code> is 'R'. (default = 15)</p>
</td></tr>
<tr><td><code id="EFAsim.data_+3A_a">a</code></td>
<td>
<p>A numeric or NULL specifying the 'a' parameter of the beta distribution, effective only when <code>distri = 'beta'</code>.
If a numeric value is provided, it will be used as the 'a' parameter of the beta distribution.
If NULL, a random integer between 1 and 10 will be used. (default = NULL)</p>
</td></tr>
<tr><td><code id="EFAsim.data_+3A_b">b</code></td>
<td>
<p>A numeric or NULL specifying the 'b' parameter of the beta distribution, effective only when <code>distri = 'beta'</code>.
If a numeric value is provided, it will be used as the 'b' parameter of the beta distribution.
If NULL, a random integer between 1 and 10 will be used. (default = NULL)</p>
</td></tr>
<tr><td><code id="EFAsim.data_+3A_vis">vis</code></td>
<td>
<p>A logical value indicating whether to print process information. (default = TRUE)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A population correlation matrix was created for each data set based on the following decomposition:
</p>
<p style="text-align: center;"><code class="reqn">\mathbf{\Sigma} = \mathbf{\Lambda} \mathbf{\Phi} \mathbf{\Lambda}^T + \mathbf{\Delta}</code>
</p>

<p>where <code class="reqn">\mathbf{\Lambda}</code> is the loading matrix, <code class="reqn">\mathbf{\Phi}</code> is the factor correlation
matrix, and <code class="reqn">\mathbf{\Delta}</code> is a diagonal matrix,
with <code class="reqn">\mathbf{\Delta} = 1 - \text{diag}(\mathbf{\Lambda} \mathbf{\Phi} \mathbf{\Lambda}^T)</code>.
The purpose of <code class="reqn">\mathbf{\Delta}</code> is to ensure that the diagonal elements of <code class="reqn">\mathbf{\Sigma} </code> are 1.
</p>
<p>The response data for each subject was simulated using the following formula:
</p>
<p style="text-align: center;"><code class="reqn">X_i = L_i + \epsilon_i, \quad 1 \leq i \leq I</code>
</p>

<p>where <code class="reqn">L_i</code> follows a a standard normal distribution (<code>distri = 'normal'</code>) or a beta
distribution (<code>distri = 'beta'</code>), representing the contribution of latent factors.
And <code class="reqn">\epsilon_i</code> is the residual term following a standard normal distribution
(<code>distri = 'normal'</code>) or a beta distribution (<code>distri = 'beta'</code>) . <code class="reqn">L_i</code> and <code class="reqn">\epsilon_i</code>
are uncorrelated, and <code class="reqn">\epsilon_i</code> and <code class="reqn">\epsilon_j</code> are also uncorrelated.
</p>


<h3>Value</h3>

<p>An object of class <code>EFAdata</code> is a <code>list</code> containing the following components:
</p>
<table role = "presentation">
<tr><td><code>loadings</code></td>
<td>
<p>A simulated loading matrix.</p>
</td></tr>
<tr><td><code>items</code></td>
<td>
<p>A <code>list</code> containing all factors and the item indices under each factor.</p>
</td></tr>
<tr><td><code>cor.factors</code></td>
<td>
<p>A simulated factor correlation matrix.</p>
</td></tr>
<tr><td><code>cor.items</code></td>
<td>
<p>A simulated item correlation matrix.</p>
</td></tr>
<tr><td><code>response</code></td>
<td>
<p>A simulated response data matrix.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Goretzko, D., &amp; Buhner, M. (2020). One model to rule them all? Using machine learning algorithms to determine the number of factors in exploratory factor analysis. Psychological Methods, 25(6), 776-786. https://doi.org/10.1037/met0000262.
</p>
<p>Auerswald, M., &amp; Moshagen, M. (2019). How to determine the number of factors to retain in exploratory factor analysis: A comparison of extraction methods under realistic conditions. Psychological methods, 24(4), 468-491. https://doi.org/https://doi.org/10.1037/met0000200
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(EFAfactors)

## Run EFAsim.data function with default parameters.
data.obj &lt;- EFAsim.data(nfact = 3, vpf = 5, N=500, distri="normal", fc="R", pl="R", cl="R",
                        low.vpf = 5, up.vpf = 15, a = NULL, b = NULL, vis = TRUE)

head(data.obj$loadings)




</code></pre>

<hr>
<h2 id='EFAvote'>Voting Method for Number of Factors in EFA</h2><span id='topic+EFAvote'></span>

<h3>Description</h3>

<p>This function implements a voting method to determine the most appropriate number of factors
in exploratory factor analysis (EFA). The function accepts a vector of votes, where each value
represents the number of factors suggested by different EFA approaches. If there is a clear
winner (a single number of factors with the most votes), that number is returned. In case of
a tie, the function returns the first value among the tied results and outputs a message. The
result is returned as an object of class <code>vote</code>, which can be printed and plotted.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>EFAvote(votes, vis = TRUE, plot = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="EFAvote_+3A_votes">votes</code></td>
<td>
<p>A vector of integers, where each element corresponds to the number of factors suggested
by an EFA method.</p>
</td></tr>
<tr><td><code id="EFAvote_+3A_vis">vis</code></td>
<td>
<p>Logical, whether to print the results of the voting. Defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="EFAvote_+3A_plot">plot</code></td>
<td>
<p>Logical, whether to display a pie chart of the voting results. Defaults to <code>TRUE</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>EFAvote</code>, which is a list containing:
</p>
<table role = "presentation">
<tr><td><code>nfact</code></td>
<td>
<p>The number of factors with the most votes. If there is a tie, the first one in the order is returned.</p>
</td></tr>
<tr><td><code>votes</code></td>
<td>
<p>The original vector of votes.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><a href="#topic+plot.EFAvote">plot.EFAvote</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(EFAfactors)

nfacts &lt;- c(5, 5, 5, 6, 6, 4)
names(nfacts) &lt;- c("Hull", "CD", "PA", "EKC", "FF", "DNN")

EFAvote.obj &lt;- EFAvote(votes = nfacts)

# Visualize the voting results
plot(EFAvote.obj)

</code></pre>

<hr>
<h2 id='EKC'>Empirical Kaiser Criterion</h2><span id='topic+EKC'></span>

<h3>Description</h3>

<p>This function will apply the Empirical Kaiser Criterion (Braeken &amp; van Assen, 2017) method to
determine the number of factors. The method assumes that the distribution of eigenvalues
asymptotically follows a Marcenko-Pastur distribution (Marcenko &amp; Pastur, 1967). It calculates
the reference eigenvalues based on this distribution and determines whether to retain a factor
by comparing the size of the empirical eigenvalues to the reference eigenvalues.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>EKC(
  response,
  cor.type = "pearson",
  use = "pairwise.complete.obs",
  vis = TRUE,
  plot = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="EKC_+3A_response">response</code></td>
<td>
<p>A required <code>N</code> × <code>I</code> matrix or data.frame consisting of the responses of <code>N</code> individuals
to <code>I</code> items.</p>
</td></tr>
<tr><td><code id="EKC_+3A_cor.type">cor.type</code></td>
<td>
<p>A character string indicating which correlation coefficient (or covariance) is to be computed. One of &quot;pearson&quot; (default),
&quot;kendall&quot;, or &quot;spearman&quot;. @seealso <a href="stats.html#topic+cor">cor</a>.</p>
</td></tr>
<tr><td><code id="EKC_+3A_use">use</code></td>
<td>
<p>An optional character string giving a method for computing covariances in the presence of missing values. This
must be one of the strings &quot;everything&quot;, &quot;all.obs&quot;, &quot;complete.obs&quot;, &quot;na.or.complete&quot;, or &quot;pairwise.complete.obs&quot; (default).
@seealso <a href="stats.html#topic+cor">cor</a>.</p>
</td></tr>
<tr><td><code id="EKC_+3A_vis">vis</code></td>
<td>
<p>A Boolean variable that will print the factor retention results when set to TRUE, and will not print
when set to FALSE. (default = TRUE)</p>
</td></tr>
<tr><td><code id="EKC_+3A_plot">plot</code></td>
<td>
<p>A Boolean variable that will print the EKC plot when set to TRUE, and will not print it when set to
FALSE. @seealso <a href="#topic+plot.EKC">plot.EKC</a>. (Default = TRUE)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Empirical Kaiser Criterion (EKC; Auerswald &amp; Moshagen, 2019; Braeken &amp; van Assen, 2017)
refines Kaiser-Guttman Criterion
by accounting for random sample variations in eigenvalues. At the population level, the EKC is
equivalent to the original Kaiser-Guttman Criterion, extracting all factors whose eigenvalues
from the correlation matrix are greater than one. However, at the sample level, it adjusts for
the distribution of eigenvalues in normally distributed data. Under the null model, the eigenvalue
distribution follows the Marčenko-Pastur distribution (Marčenko &amp; Pastur, 1967) asymptotically.
The upper bound of this distribution serves as the reference eigenvalue for the first eigenvalue <code class="reqn">\lambda</code>, so
</p>
<p style="text-align: center;"><code class="reqn">\lambda_{1,ref} = \left( 1 + \sqrt{\frac{I}{N}} \right)^2</code>
</p>

<p>, which is determined by N individuals and I items. For subsequent eigenvalues, adjustments are
made based on the variance explained by previous factors. The j-th reference eigenvalue is:
</p>
<p style="text-align: center;"><code class="reqn">\lambda_{j,ref} = \max \left[ \frac{I - \sum_{i=0}^{j-1} \lambda_i}{I - j + 1} \left( 1 + \sqrt{\frac{I}{N}} \right)^2, 1 \right]</code>
</p>

<p>The j-th reference eigenvalue is reduced according to the magnitude of earlier eigenvalues
since higher previous values mean less unexplained variance remains. As in the original
Kaiser-Guttman Criterion, the reference eigenvalue cannot drop below one.
</p>
<p style="text-align: center;"><code class="reqn">F = \sum_{i=1}^{I} I(\lambda_i &gt; \lambda_{i,ref})</code>
</p>

<p>Here, \( F \) represents the number of factors determined by the EKC, and <code class="reqn">I(\cdot)</code> is the
indicator function, which equals 1 when the condition is true, and 0 otherwise.
</p>


<h3>Value</h3>

<p>An object of class <code>EKC</code> is a <code>list</code> containing the following components:
</p>
<table role = "presentation">
<tr><td><code>nfact</code></td>
<td>
<p>The number of factors to be retained.</p>
</td></tr>
<tr><td><code>eigen.value</code></td>
<td>
<p>A vector containing the empirical eigenvalues</p>
</td></tr>
<tr><td><code>eigen.ref</code></td>
<td>
<p>A vector containing the reference eigenvalues</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Haijiang Qin &lt;Haijiang133@outlook.com&gt;
</p>


<h3>References</h3>

<p>Auerswald, M., &amp; Moshagen, M. (2019). How to determine the number of factors to retain in exploratory factor analysis: A comparison of extraction methods under realistic conditions. Psychological methods, 24(4), 468-491. https://doi.org/10.1037/met0000200.
</p>
<p>Braeken, J., &amp; van Assen, M. A. L. M. (2017). An empirical Kaiser criterion. Psychological methods, 22(3), 450-466. https://doi.org/10.1037/met0000074.
</p>
<p>Marcˇenko, V. A., &amp; Pastur, L. A. (1967). Distribution of eigenvalues for some sets of random matrices. Mathematics of the USSR-Sbornik, 1, 457–483. http://dx.doi.org/10.1070/SM1967v001n04ABEH001994
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(EFAfactors)
set.seed(123)

##Take the data.bfi dataset as an example.
data(data.bfi)

response &lt;- as.matrix(data.bfi[, 1:25]) ## loading data
response &lt;- na.omit(response) ## Remove samples with NA/missing values

## Transform the scores of reverse-scored items to normal scoring
response[, c(1, 9, 10, 11, 12, 22, 25)] &lt;- 6 - response[, c(1, 9, 10, 11, 12, 22, 25)] + 1


## Run EKC function with default parameters.

EKC.obj &lt;- EKC(response)

print(EKC.obj)

plot(EKC.obj)

## Get the eigen.value, eigen.ref and  nfact results.
eigen.value &lt;- EKC.obj$eigen.value
eigen.ref &lt;- EKC.obj$eigen.ref
nfact &lt;- EKC.obj$nfact

print(eigen.value)
print(eigen.ref)
print(nfact)




</code></pre>

<hr>
<h2 id='extractor.feature.DNN'>Extracting features for the Pre-Trained Deep Neural Network (DNN)</h2><span id='topic+extractor.feature.DNN'></span>

<h3>Description</h3>

<p>This function is used to extract the features required by the Pre-Trained Deep
Neural Network (DNN).  @seealso <a href="#topic+DNN_predictor">DNN_predictor</a>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>extractor.feature.DNN(
  response,
  cor.type = "pearson",
  use = "pairwise.complete.obs"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="extractor.feature.DNN_+3A_response">response</code></td>
<td>
<p>A required <code>N</code> × <code>I</code> matrix or data.frame consisting of the responses of <code>N</code> individuals
to <code>I</code> items.</p>
</td></tr>
<tr><td><code id="extractor.feature.DNN_+3A_cor.type">cor.type</code></td>
<td>
<p>A character string indicating which correlation coefficient (or covariance) is to be computed. One of &quot;pearson&quot; (default),
&quot;kendall&quot;, or &quot;spearman&quot;. @seealso <a href="stats.html#topic+cor">cor</a>.</p>
</td></tr>
<tr><td><code id="extractor.feature.DNN_+3A_use">use</code></td>
<td>
<p>An optional character string giving a method for computing covariances in the presence of missing values. This
must be one of the strings &quot;everything&quot;, &quot;all.obs&quot;, &quot;complete.obs&quot;, &quot;na.or.complete&quot;, or &quot;pairwise.complete.obs&quot; (default).
@seealso <a href="stats.html#topic+cor">cor</a>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A total of two types of features (6 kinds, making up 54 features in total) will be extracted, and they are as follows:
1. Clustering-Based Features
</p>

<dl>
<dt>(1)</dt><dd><p>Hierarchical clustering is performed with correlation coefficients as dissimilarity.
The top 9 tree node heights are calculated, and all heights are divided by the maximum
height. The heights from the 2nd to 9th nodes are used as features. @seealso <a href="#topic+EFAhclust">EFAhclust</a></p>
</dd>
<dt>(2)</dt><dd><p>Hierarchical clustering with Euclidean distance as dissimilarity is performed. The top 9
tree node heights are calculated, and all heights are divided by the maximum height. The
heights from the 2nd to 9th nodes are used as features. @seealso <a href="#topic+EFAhclust">EFAhclust</a></p>
</dd>
<dt>(3)</dt><dd><p>K-means clustering is applied with the number of clusters ranging from 1 to 9. The
within-cluster sum of squares (WSS) for clusters 2 to 9 are divided by the WSS for a single
cluster. @seealso <a href="#topic+EFAkmeans">EFAkmeans</a></p>
</dd>
</dl>

<p>These three features are based on clustering algorithms. The purpose of division is to normalize the
data. These clustering metrics often contain information unrelated to the number of factors, such as
the number of items and the number of respondents, which can be avoided by normalization. The reason
for using the 2nd to 9th data is that only the top F-1 data are needed to determine the number of factors F.
The first data point is fixed at 1 after the division operations, so it is excluded. This approach
helps in model simplification.
</p>
<p>2. Traditional Exploratory Factor Analysis Features (Eigenvalues)
</p>

<dl>
<dt>(4)</dt><dd><p>The top 10 largest eigenvalues.</p>
</dd>
<dt>(5)</dt><dd><p>The ratio of the top 10 largest eigenvalues to the corresponding reference eigenvalues from
Empirical Kaiser Criterion (EKC; Braeken &amp; van Assen, 2017). @seealso <a href="#topic+EKC">EKC</a></p>
</dd>
<dt>(6)</dt><dd><p>The cumulative variance proportion of the top 10 largest eigenvalues.</p>
</dd>
</dl>

<p>Only the top 10 elements are used to simplify the model.
</p>


<h3>Value</h3>

<p>A matrix (1×54) containing all the features for determining the number of
factors by the DNN.
</p>


<h3>Author(s)</h3>

<p>Haijiang Qin &lt;Haijiang133@outlook.com&gt;
</p>


<h3>See Also</h3>

<p><a href="#topic+DNN_predictor">DNN_predictor</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(EFAfactors)
set.seed(123)

##Take the data.bfi dataset as an example.
data(data.bfi)

response &lt;- as.matrix(data.bfi[, 1:25]) ## loading data
response &lt;- na.omit(response) ## Remove samples with NA/missing values

## Transform the scores of reverse-scored items to normal scoring
response[, c(1, 9, 10, 11, 12, 22, 25)] &lt;- 6 - response[, c(1, 9, 10, 11, 12, 22, 25)] + 1


## Run extractor.feature.DNN function with default parameters.

features &lt;- extractor.feature.DNN(response)

print(features)






</code></pre>

<hr>
<h2 id='extractor.feature.FF'>Extracting features According to Goretzko &amp; Buhner (2020)</h2><span id='topic+extractor.feature.FF'></span>

<h3>Description</h3>

<p>This function will extract 181 features from the data according to the method by Goretzko &amp; Buhner (2020).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>extractor.feature.FF(
  response,
  cor.type = "pearson",
  use = "pairwise.complete.obs"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="extractor.feature.FF_+3A_response">response</code></td>
<td>
<p>A required <code>N</code> × <code>I</code> matrix or data.frame consisting of the responses of <code>N</code> individuals
to <code>I</code> items.</p>
</td></tr>
<tr><td><code id="extractor.feature.FF_+3A_cor.type">cor.type</code></td>
<td>
<p>A character string indicating which correlation coefficient (or covariance) is to be computed. One of &quot;pearson&quot; (default),
&quot;kendall&quot;, or &quot;spearman&quot;. @seealso <a href="stats.html#topic+cor">cor</a>.</p>
</td></tr>
<tr><td><code id="extractor.feature.FF_+3A_use">use</code></td>
<td>
<p>An optional character string giving a method for computing covariances in the presence of missing values. This
must be one of the strings &quot;everything&quot;, &quot;all.obs&quot;, &quot;complete.obs&quot;, &quot;na.or.complete&quot;, or &quot;pairwise.complete.obs&quot; (default).
@seealso <a href="stats.html#topic+cor">cor</a>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The code for the <code>extractor.feature.FF</code> function is implemented based on the publicly available code by Goretzko &amp; Buhner (2020) (https://osf.io/mvrau/).
The extracted features are completely consistent with the 181 features described in the original text by Goretzko &amp; Buhner (2020).
These features include:
</p>

<ul>
<li> <p><code>1.</code> - Number of examinees
</p>
</li>
<li> <p><code>2.</code> - Number of items
</p>
</li>
<li> <p><code>3.</code> - Number of eigenvalues greater than 1
</p>
</li>
<li> <p><code>4.</code> - Proportion of variance explained by the 1st eigenvalue
</p>
</li>
<li> <p><code>5.</code> - Proportion of variance explained by the 2nd eigenvalue
</p>
</li>
<li> <p><code>6.</code> - Proportion of variance explained by the 3rd eigenvalue
</p>
</li>
<li> <p><code>7.</code> - Number of eigenvalues greater than 0.7
</p>
</li>
<li> <p><code>8.</code> - Standard deviation of the eigenvalues
</p>
</li>
<li> <p><code>9.</code> - Number of eigenvalues accounting for 50
</p>
</li>
<li> <p><code>10.</code> - Number of eigenvalues accounting for 75
</p>
</li>
<li> <p><code>11.</code> - L1-norm of the correlation matrix
</p>
</li>
<li> <p><code>12.</code> - Frobenius-norm of the correlation matrix
</p>
</li>
<li> <p><code>13.</code> - Maximum-norm of the correlation matrix
</p>
</li>
<li> <p><code>14.</code> - Average of the off-diagonal correlations
</p>
</li>
<li> <p><code>15.</code> - Spectral-norm of the correlation matrix
</p>
</li>
<li> <p><code>16.</code> - Number of correlations smaller or equal to 0.1
</p>
</li>
<li> <p><code>17.</code> - Average of the initial communality estimates
</p>
</li>
<li> <p><code>18.</code> - Determinant of the correlation matrix
</p>
</li>
<li> <p><code>19.</code> - Measure of sampling adequacy (MSA after Kaiser, 1970)
</p>
</li>
<li> <p><code>20.</code> - Gini coefficient (Gini, 1921) of the correlation matrix
</p>
</li>
<li> <p><code>21.</code> - Kolm measure of inequality (Kolm, 1999) of the correlation matrix
</p>
</li>
<li> <p><code>22-101.</code> - Eigenvalues from Principal Component Analysis (PCA), padded with -1000 if insufficient
</p>
</li>
<li> <p><code>102-181.</code> - Eigenvalues from Factor Analysis (FA), fixed at 1 factor, padded with -1000 if insufficient
</p>
</li></ul>



<h3>Value</h3>

<p>A matrix (1×181) containing all the 181 features (Goretzko &amp; Buhner, 2020).
</p>


<h3>References</h3>

<p>Goretzko, D., &amp; Buhner, M. (2020). One model to rule them all? Using machine learning algorithms to determine the number of factors in exploratory factor analysis. Psychol Methods, 25(6), 776-786. https://doi.org/10.1037/met0000262.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(EFAfactors)
set.seed(123)

##Take the data.bfi dataset as an example.
data(data.bfi)

response &lt;- as.matrix(data.bfi[, 1:25]) ## loading data
response &lt;- na.omit(response) ## Remove samples with NA/missing values

## Transform the scores of reverse-scored items to normal scoring
response[, c(1, 9, 10, 11, 12, 22, 25)] &lt;- 6 - response[, c(1, 9, 10, 11, 12, 22, 25)] + 1


## Run extractor.feature.FF function with default parameters.

features &lt;- extractor.feature.FF(response)

print(features)





</code></pre>

<hr>
<h2 id='factor.analysis'>Factor Analysis by Principal Axis Factoring</h2><span id='topic+factor.analysis'></span>

<h3>Description</h3>

<p>This function performs factor analysis using the Principal Axis Factoring (PAF) method.
The process involves extracting factors from an initial correlation matrix and iteratively
refining the factor estimates until convergence is achieved.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>factor.analysis(
  data,
  nfact = 1,
  iter.max = 1000,
  criterion = 0.001,
  cor.type = "pearson",
  use = "pairwise.complete.obs"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="factor.analysis_+3A_data">data</code></td>
<td>
<p>A data.frame or matrix of response If the matrix is square, it is assumed to
be a correlation matrix. Otherwise, correlations (with pairwise deletion) will be computed.</p>
</td></tr>
<tr><td><code id="factor.analysis_+3A_nfact">nfact</code></td>
<td>
<p>The number of factors to extract. (default = 1)</p>
</td></tr>
<tr><td><code id="factor.analysis_+3A_iter.max">iter.max</code></td>
<td>
<p>The maximum number of iterations for the factor extraction process. Default is 1000.</p>
</td></tr>
<tr><td><code id="factor.analysis_+3A_criterion">criterion</code></td>
<td>
<p>The convergence criterion for the iterative process. The extraction process will
stop when the change in communalities is less than this value. Default is 0.001</p>
</td></tr>
<tr><td><code id="factor.analysis_+3A_cor.type">cor.type</code></td>
<td>
<p>A character string indicating which correlation coefficient (or covariance) is to be computed.
One of &quot;pearson&quot; (default), &quot;kendall&quot;, or &quot;spearman&quot;. @seealso <a href="stats.html#topic+cor">cor</a>.</p>
</td></tr>
<tr><td><code id="factor.analysis_+3A_use">use</code></td>
<td>
<p>An optional character string giving a method for computing covariances in the presence of missing values.
This must be one of the strings &quot;everything&quot;, &quot;all.obs&quot;, &quot;complete.obs&quot;, &quot;na.or.complete&quot;, or &quot;pairwise.complete.obs&quot; (default).
@seealso <a href="stats.html#topic+cor">cor</a>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Principal Axis Factoring (PAF) method involves the following steps:
</p>
<p>Step 1. **Basic Principle**:
The core principle of factor analysis using Principal Axis Factoring (PAF) is expressed as:
</p>
<p style="text-align: center;"><code class="reqn">\mathbf{R} = \mathbf{\Lambda} \mathbf{\Lambda}^T + \mathbf{\Phi}</code>
</p>

<p style="text-align: center;"><code class="reqn">R_{ii} = H_i^2 + \Phi_{ii}</code>
</p>

<p>where <code class="reqn">\mathbf{\Lambda}</code> is the matrix of factor loadings, and <code class="reqn">\mathbf{\Phi}</code> is the diagonal
matrix of unique variances. Here, <code class="reqn">H_i^2</code> represents the portion of the i-th item's variance explained by the factor model.
<code class="reqn">\mathbf{H}^2</code> reflects the amount of total variance in the variable accounted for by the factors in the model, indicating the
explanatory power of the factor model for that variable.
</p>
<p>Step 2. **Factor Extraction by Iteratoin**:
</p>
<p>- Initial Communalities:
Compute the initial communalities as the squared multiple correlations:
</p>
<p style="text-align: center;"><code class="reqn">H_{i(t)}^2 = R_{ii(t)}</code>
</p>

<p>where <code class="reqn">H_{i(t)}^2</code> is the communality of i-th item in the <code class="reqn">t</code>-th iteration, and <code class="reqn">R_{ii(t)}</code> is the i-th
diagonal element of the correlation matrix in the <code class="reqn">t</code>-th iteration.
</p>
<p>- Extract Factors and Update Communalities:
</p>
<p style="text-align: center;"><code class="reqn">\Lambda_{ij} = \sqrt{\lambda_j} \times v_{ij}</code>
</p>

<p style="text-align: center;"><code class="reqn">H_{i(t+1)}^2 = \sum_j \Lambda_{ij}^2</code>
</p>

<p style="text-align: center;"><code class="reqn">R_{ii(t+1)} = H_{i(t+1)}^2</code>
</p>

<p>where <code class="reqn">\Lambda_{ij}</code> represents the j-th factor loading for the i-th item, <code class="reqn">\lambda_j</code> is the j-th
eigenvalue, <code class="reqn">H_{i(t+1)}^2</code> is the communality of i-th item in the <code class="reqn">t+1</code>-th iteration, and <code class="reqn">v_{ij}</code> is
the j-th value of the i-th item in the eigen vector matrix <code class="reqn">\mathbf{v}</code>.
</p>
<p>Step 3. **Iterative Refinement**:
</p>
<p>- Calculate the Change between <code class="reqn">\mathbf{H}_{t}^2</code> and <code class="reqn">\mathbf{H}_{t+1}^2</code>:
</p>
<p style="text-align: center;"><code class="reqn">\Delta H_i^2 = \lvert H_{i(t+1)}^2 - H_{i(t)}^2 \lvert</code>
</p>

<p>where <code class="reqn">\Delta H_i^2</code> represents the change in communalities between iterations <code class="reqn">t</code> and <code class="reqn">t+1</code>.
</p>
<p>- Convergence Criterion:
Continue iterating until the change in communalities is less than the specified criterion <code class="reqn">criterion</code>:
</p>
<p style="text-align: center;"><code class="reqn">\sum_i \Delta H_i^2 &lt; criterion</code>
</p>

<p>The iterative process is implemented using C++ code to ensure computational speed.
</p>


<h3>Value</h3>

<p>A list containing:
</p>
<table role = "presentation">
<tr><td><code>loadings</code></td>
<td>
<p>The extracted factor loadings.</p>
</td></tr>
<tr><td><code>eigen.value</code></td>
<td>
<p>The eigenvalues of the correlation matrix.</p>
</td></tr>
<tr><td><code>H2</code></td>
<td>
<p>A vector that contains the explanatory power of the factor model for all items.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Haijiang Qin &lt;Haijiang133@outlook.com&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(EFAfactors)
set.seed(123)

##Take the data.bfi dataset as an example.
data(data.bfi)

response &lt;- as.matrix(data.bfi[, 1:25]) ## loading data
response &lt;- na.omit(response) ## Remove samples with NA/missing values

## Transform the scores of reverse-scored items to normal scoring
response[, c(1, 9, 10, 11, 12, 22, 25)] &lt;- 6 - response[, c(1, 9, 10, 11, 12, 22, 25)] + 1


## Run factor.analysis function to extract 5 factors

PAF.obj &lt;- factor.analysis(response, nfact = 5)


## Get the loadings, eigen.value and  H2 results.
loadings &lt;- PAF.obj$loadings
eigen.value &lt;- PAF.obj$eigen.value
H2 &lt;- PAF.obj$H2

print(loadings)
print(eigen.value)
print(H2)





</code></pre>

<hr>
<h2 id='FF'>Factor Forest (FF) Powered by An Tuned XGBoost Model for Determining the Number of Factors</h2><span id='topic+FF'></span>

<h3>Description</h3>

<p>This function will invoke a tuned XGBoost model (Goretzko &amp; Buhner, 2020; Goretzko, 2022; Goretzko &amp; Ruscio, 2024) that can reliably
perform the task of determining the number of factors. The maximum number of factors that the network can discuss is 8.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>FF(
  response,
  cor.type = "pearson",
  use = "pairwise.complete.obs",
  vis = TRUE,
  plot = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="FF_+3A_response">response</code></td>
<td>
<p>A required <code>N</code> × <code>I</code> matrix or data.frame consisting of the responses of <code>N</code> individuals
to <code>I</code> items.</p>
</td></tr>
<tr><td><code id="FF_+3A_cor.type">cor.type</code></td>
<td>
<p>A character string indicating which correlation coefficient (or covariance) is to be computed. One of &quot;pearson&quot; (default),
&quot;kendall&quot;, or &quot;spearman&quot;. @seealso <a href="stats.html#topic+cor">cor</a>.</p>
</td></tr>
<tr><td><code id="FF_+3A_use">use</code></td>
<td>
<p>An optional character string giving a method for computing covariances in the presence of missing values. This
must be one of the strings &quot;everything&quot;, &quot;all.obs&quot;, &quot;complete.obs&quot;, &quot;na.or.complete&quot;, or &quot;pairwise.complete.obs&quot; (default).
@seealso <a href="stats.html#topic+cor">cor</a>.</p>
</td></tr>
<tr><td><code id="FF_+3A_vis">vis</code></td>
<td>
<p>A Boolean variable that will print the factor retention results when set to TRUE, and will not print
when set to FALSE. (default = TRUE)</p>
</td></tr>
<tr><td><code id="FF_+3A_plot">plot</code></td>
<td>
<p>A Boolean variable that will print the FF plot when set to TRUE, and will not print it when set to
FALSE. @seealso <a href="#topic+plot.FF">plot.FF</a>. (Default = TRUE)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A total of 500,000 datasets were simulated to extract features for training the tuned XGBoost
model (Goretzko &amp; Buhner, 2020; Goretzko, 2022).
Each dataset was generated according to the following specifications:
</p>

<ul>
<li><p> Factor number: <em>F</em> ~ U[1,8]
</p>
</li>
<li><p> Sample size: <em>N</em> ~ U[200,1000]
</p>
</li>
<li><p> Number of variables per factor: <em>vpf</em> ~ U[3,10]
</p>
</li>
<li><p> Factor correlation: <em>fc</em> ~ U[0.0,0.4]
</p>
</li>
<li><p> Primary loadings: <em>pl</em> ~ U[0.35,0.80]
</p>
</li>
<li><p> Cross-loadings: <em>cl</em> ~ U[0.0,0.2]
</p>
</li></ul>

<p>A population correlation matrix was created for each data set based on the following decomposition:
</p>
<p style="text-align: center;"><code class="reqn">\mathbf{\Sigma} = \mathbf{\Lambda} \mathbf{\Phi} \mathbf{\Lambda}^T + \mathbf{\Delta}</code>
</p>

<p>where <code class="reqn">\mathbf{\Lambda}</code> is the loading matrix, <code class="reqn">\mathbf{\Phi}</code> is the factor correlation
matrix, and <code class="reqn">\mathbf{\Delta}</code> is a diagonal matrix,
with <code class="reqn">\mathbf{\Delta} = 1 - \text{diag}(\mathbf{\Lambda} \mathbf{\Phi} \mathbf{\Lambda}^T)</code>.
The purpose of <code class="reqn">\mathbf{\Delta}</code> is to ensure that the diagonal elements of <code class="reqn">\mathbf{\Sigma} </code> are 1.
</p>
<p>The response data for each subject were simulated using the following formula:
</p>
<p style="text-align: center;"><code class="reqn">X_i = L_i + \epsilon_i, \quad 1 \leq i \leq I</code>
</p>

<p>where <code class="reqn">L_i</code> follows a normal distribution <code class="reqn">N(0, \sigma)</code>, representing the contribution of latent factors,
and <code class="reqn">\epsilon_i</code> is the residual term following a standard normal distribution. <code class="reqn">L_i</code> and <code class="reqn">\epsilon_i</code>
are uncorrelated, and <code class="reqn">\epsilon_i</code> and <code class="reqn">\epsilon_j</code> are also uncorrelated.
</p>
<p>For each simulated dataset, a total of 184 features are extracted and compiled into a feature vector.
These features include:
</p>

<ul>
<li> <p><code>1.</code> - Number of examinees
</p>
</li>
<li> <p><code>2.</code> - Number of items
</p>
</li>
<li> <p><code>3.</code> - Number of eigenvalues greater than 1
</p>
</li>
<li> <p><code>4.</code> - Proportion of variance explained by the 1st eigenvalue
</p>
</li>
<li> <p><code>5.</code> - Proportion of variance explained by the 2nd eigenvalue
</p>
</li>
<li> <p><code>6.</code> - Proportion of variance explained by the 3rd eigenvalue
</p>
</li>
<li> <p><code>7.</code> - Number of eigenvalues greater than 0.7
</p>
</li>
<li> <p><code>8.</code> - Standard deviation of the eigenvalues
</p>
</li>
<li> <p><code>9.</code> - Number of eigenvalues accounting for 50
</p>
</li>
<li> <p><code>10.</code> - Number of eigenvalues accounting for 75
</p>
</li>
<li> <p><code>11.</code> - L1-norm of the correlation matrix
</p>
</li>
<li> <p><code>12.</code> - Frobenius-norm of the correlation matrix
</p>
</li>
<li> <p><code>13.</code> - Maximum-norm of the correlation matrix
</p>
</li>
<li> <p><code>14.</code> - Average of the off-diagonal correlations
</p>
</li>
<li> <p><code>15.</code> - Spectral-norm of the correlation matrix
</p>
</li>
<li> <p><code>16.</code> - Number of correlations smaller or equal to 0.1
</p>
</li>
<li> <p><code>17.</code> - Average of the initial communality estimates
</p>
</li>
<li> <p><code>18.</code> - Determinant of the correlation matrix
</p>
</li>
<li> <p><code>19.</code> - Measure of sampling adequacy (MSA after Kaiser, 1970)
</p>
</li>
<li> <p><code>20.</code> - Gini coefficient (Gini, 1921) of the correlation matrix
</p>
</li>
<li> <p><code>21.</code> - Kolm measure of inequality (Kolm, 1999) of the correlation matrix
</p>
</li>
<li> <p><code>21.</code> - Number of factors retained by the PA method @seealso <a href="#topic+PA">PA</a>
</p>
</li>
<li> <p><code>23.</code> - Number of factors retained by the EKC method @seealso <a href="#topic+EKC">EKC</a>
</p>
</li>
<li> <p><code>24.</code> - Number of factors retained by the CD method @seealso <a href="#topic+CD">CD</a>
</p>
</li>
<li> <p><code>25-104.</code> - Eigenvalues from Principal Component Analysis (PCA), padded with -1000 if insufficient
</p>
</li>
<li> <p><code>105-184.</code> - Eigenvalues from Factor Analysis (FA), fixed at 1 factor, padded with -1000 if insufficient
</p>
</li></ul>

<p>The code for the <code>FF</code> function is implemented based on the publicly available code by Goretzko &amp; Buhner (2020) (https://osf.io/mvrau/).
The Tuned XGBoost Model is also obtained from this site. However, to meet the requirements for a streamlined R package, we can only
save the core components of the Tuned XGBoost Model. Although these non-core parts do not affect performance, they include a lot of information
about the model itself, such as the number of features, subsets of samples, and data from the training process, among others.
For the complete Tuned XGBoost Model, please download it from https://osf.io/mvrau/.
</p>


<h3>Value</h3>

<p>An object of class <code>FF</code> is a <code>list</code> containing the following components:
</p>
<table role = "presentation">
<tr><td><code>nfact</code></td>
<td>
<p>The number of factors to be retained.</p>
</td></tr>
<tr><td><code>probability</code></td>
<td>
<p>A matrix containing the probabilities for factor numbers ranging from 1
to 8 (1x8), where the number in the f-th column represents the probability
that the number of factors for the response is f.</p>
</td></tr>
<tr><td><code>features</code></td>
<td>
<p>A matrix (1×184) containing all the features for determining the number of
factors by the tuned XGBoost Model.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Goretzko, D., &amp; Buhner, M. (2020). One model to rule them all? Using machine learning algorithms to determine the number of factors in exploratory factor analysis. Psychol Methods, 25(6), 776-786. https://doi.org/10.1037/met0000262.
</p>
<p>Goretzko, D. (2022). Factor Retention in Exploratory Factor Analysis With Missing Data. Educ Psychol Meas, 82(3), 444-464. https://doi.org/10.1177/00131644211022031.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(EFAfactors)
set.seed(123)

##Take the data.bfi dataset as an example.
data(data.bfi)

response &lt;- as.matrix(data.bfi[, 1:25]) ## loading data
response &lt;- na.omit(response) ## Remove samples with NA/missing values

## Transform the scores of reverse-scored items to normal scoring
response[, c(1, 9, 10, 11, 12, 22, 25)] &lt;- 6 - response[, c(1, 9, 10, 11, 12, 22, 25)] + 1


## Run FF function with default parameters.

FF.obj &lt;- FF(response)

print(FF.obj)

plot(FF.obj)

## Get the probability and nfact results.
probability &lt;- FF.obj$probability
nfact &lt;- FF.obj$nfact

print(probability)
print(nfact)




</code></pre>

<hr>
<h2 id='GenData'>Simulating Data Following John Ruscio's RGenData</h2><span id='topic+GenData'></span>

<h3>Description</h3>

<p>This function simulates data with <code class="reqn">nfact</code> factors based on empirical data.
It represents the simulation data part of the <a href="#topic+CD">CD</a> function
and the <a href="#topic+CDF">CDF</a> function. This function improves upon
<a href="RGenData.html#topic+GenDataPopulation">GenDataPopulation</a> by utilizing C++ code to achieve faster data simulation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>GenData(
  response,
  nfact = 1,
  N.pop = 10000,
  Max.Trials = 5,
  lr = 1,
  cor.type = "pearson",
  use = "pairwise.complete.obs",
  isSort = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="GenData_+3A_response">response</code></td>
<td>
<p>A required <code>N</code> × <code>I</code> matrix or data.frame consisting of the responses of <code>N</code> individuals
to <code>I</code> items.</p>
</td></tr>
<tr><td><code id="GenData_+3A_nfact">nfact</code></td>
<td>
<p>The number of factors to extract in factor analysis. (default = 1)</p>
</td></tr>
<tr><td><code id="GenData_+3A_n.pop">N.pop</code></td>
<td>
<p>Size of finite populations for simulating. (default = 10,000)</p>
</td></tr>
<tr><td><code id="GenData_+3A_max.trials">Max.Trials</code></td>
<td>
<p>The maximum number of consecutive trials without obtaining a lower RMSR. (default = 5)</p>
</td></tr>
<tr><td><code id="GenData_+3A_lr">lr</code></td>
<td>
<p>The learning rate for updating the correlation matrix during iteration. (default = 1)</p>
</td></tr>
<tr><td><code id="GenData_+3A_cor.type">cor.type</code></td>
<td>
<p>A character string indicating which correlation coefficient (or covariance) is to be computed. One of &quot;pearson&quot; (default),
&quot;kendall&quot;, or &quot;spearman&quot;. @seealso <a href="stats.html#topic+cor">cor</a>.</p>
</td></tr>
<tr><td><code id="GenData_+3A_use">use</code></td>
<td>
<p>An optional character string specifying a method for computing covariances in the presence of missing values. This
must be one of the strings &quot;everything&quot;, &quot;all.obs&quot;, &quot;complete.obs&quot;, &quot;na.or.complete&quot;, or &quot;pairwise.complete.obs&quot; (default).
@seealso <a href="stats.html#topic+cor">cor</a>.</p>
</td></tr>
<tr><td><code id="GenData_+3A_issort">isSort</code></td>
<td>
<p>Logical, determines whether the simulated data needs to be sorted in descending order. (default = FALSE)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The core idea of <code>GenData</code> is to start with the empirical data's correlation matrix
and iteratively approach data with <code>nfact</code> factors. Any value in the simulated data must come
from the empirical data. The specific steps of <code>GenData</code> are as follows:
</p>

<dl>
<dt>(1)</dt><dd><p>Use the empirical data (<code class="reqn">\mathbf{Y}_{emp}</code>) correlation matrix as the target, <code class="reqn">\mathbf{R}_{targ}</code>.</p>
</dd>
<dt>(2)</dt><dd><p>Simulate scores for <code class="reqn">N.pop</code> examinees on <code class="reqn">nfact</code> factors using a multivariate standard normal distribution:
</p>
<p style="text-align: center;"><code class="reqn">\mathbf{S}_{(N.pop \times nfact)} \sim \mathcal{N}(0, 1)</code>
</p>

<p>Simulate noise for <code class="reqn">N.pop</code> examinees on <code class="reqn">I</code> items:
</p>
<p style="text-align: center;"><code class="reqn">\mathbf{U}_{(N.pop \times I)} \sim \mathcal{N}(0, 1)</code>
</p>
</dd>
<dt>(3)</dt><dd><p>Initialize <code class="reqn">\mathbf{R}_{temp} = \mathbf{R}_{targ}</code>, and set the minimum Root
Mean Square Residual <code class="reqn">RMSR_{min} = \text{Inf}</code>. Start the iteration process.</p>
</dd>
<dt>(4)</dt><dd><p>Extract <code>nfact</code> factors from <code class="reqn">\mathbf{R}_{temp}</code>, and obtain the factor
loadings matrix <code class="reqn">\mathbf{L}_{shar}</code>. Ensure that the first element of
<code class="reqn">\mathbf{L}_{share}</code> is positive to standardize the direction.</p>
</dd>
<dt>(5)</dt><dd><p>Calculate the unique factor matrix <code class="reqn">\mathbf{L}_{uniq, (I \times 1)}</code>:
</p>
<p style="text-align: center;"><code class="reqn">L_{uniq,i} = \sqrt{1 - \sum_{j=1}^{nfact} L_{share, i, j}^2}</code>
</p>
</dd>
<dt>(6)</dt><dd><p>Calculate the simulated data <code class="reqn">\mathbf{Y}_{sim}</code>:
</p>
<p style="text-align: center;"><code class="reqn">Y_{sim, i, j} = \mathbf{S}_{i} \mathbf{L}_{shar, j}^T + U_{i, j} L_{uniq,i}</code>
</p>
</dd>
<dt>(7)</dt><dd><p>Compute the correlation matrix of the simulated data, <code class="reqn">\mathbf{R}_{simu}</code>.</p>
</dd>
<dt>(8)</dt><dd><p>Calculate the residual correlation matrix <code class="reqn">\mathbf{R}_{resi}</code> between the
target matrix <code class="reqn">\mathbf{R}_{targ}</code> and the simulated data's correlation matrix <code class="reqn">\mathbf{R}_{simu}</code>:
</p>
<p style="text-align: center;"><code class="reqn">\mathbf{R}_{resi} = \mathbf{R}_{targ} - \mathbf{R}_{simu}</code>
</p>
</dd>
<dt>(9)</dt><dd><p>Calculate the current RMSR:
</p>
<p style="text-align: center;"><code class="reqn">RMSR_{cur} = \sqrt{\frac{\sum_{i &lt; j} \mathbf{R}_{resi, i, j}^2}{0.5 \times (I^2 - I)}}</code>
</p>
</dd>
<dt>(10)</dt><dd><p>If <code class="reqn">RMSR_{cur} &lt; RMSR_{min}</code>, update <code class="reqn">\mathbf{R}_{temp} = \mathbf{R}_{temp} +
              lr \times \mathbf{R}_{resi}</code>, <code class="reqn">RMSR_{min} = RMSR_{cur}</code>, set <code class="reqn">\mathbf{R}_{min, resi} = \mathbf{R}_{resi}</code>,
and reset the count of consecutive trials without improvement <code class="reqn">cou = 0</code>.
If <code class="reqn">RMSR_{cur} \geq RMSR_{min}</code>, update <code class="reqn">\mathbf{R}_{temp} = \mathbf{R}_{temp} +
              0.5 \times cou \times lr \times \mathbf{R}_{min, resi}</code> and increment <code class="reqn">cou = cou + 1</code>.</p>
</dd>
<dt>(11)</dt><dd><p>Repeat steps (4) through (10) until <code class="reqn">cou \geq Max.Trials</code>.</p>
</dd>
</dl>

<p>Of course C++ code is used to speed up.
</p>


<h3>Value</h3>

<p>A <code>N.pop</code> * <code>I</code> matrix containing the simulated data.
</p>


<h3>References</h3>

<p>Ruscio, J., &amp; Roche, B. (2012). Determining the number of factors to retain in an exploratory factor analysis using comparison data of known factorial structure. Psychological Assessment, 24, 282–292. http://dx.doi.org/10.1037/a0025697.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(EFAfactors)
set.seed(123)

##Take the data.bfi dataset as an example.
data(data.bfi)

response &lt;- as.matrix(data.bfi[, 1:25]) ## loading data
response &lt;- na.omit(response) ## Remove samples with NA/missing values

## Transform the scores of reverse-scored items to normal scoring
response[, c(1, 9, 10, 11, 12, 22, 25)] &lt;- 6 - response[, c(1, 9, 10, 11, 12, 22, 25)] + 1

data.simulated &lt;- GenData(response, nfact = 1, N.pop = 10000)
head(data.simulated)




</code></pre>

<hr>
<h2 id='Hull'>the Hull Approach</h2><span id='topic+Hull'></span>

<h3>Description</h3>

<p>The Hull method is a heuristic approach used to determine the optimal number of common factors
in factor analysis. It evaluates models with increasing numbers of factors and uses goodness-of-fit
indices relative to the model degrees of freedom to select the best-fitting model. The method is known
for its effectiveness and reliability compared to other methods like the scree plot.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Hull(
  response,
  fa = "pc",
  nfact = 1,
  cor.type = "pearson",
  use = "pairwise.complete.obs",
  vis = TRUE,
  plot = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Hull_+3A_response">response</code></td>
<td>
<p>A required <code>N</code> × <code>I</code> matrix or data.frame consisting of the responses of <code>N</code> individuals
to × <code>I</code> items.</p>
</td></tr>
<tr><td><code id="Hull_+3A_fa">fa</code></td>
<td>
<p>A string that determines the method used to obtain eigenvalues in PA. If 'pc', it represents
Principal Component Analysis (PCA); if 'fa', it represents Principal Axis Factoring (a widely
used Factor Analysis method; @seealso <code><a href="#topic+factor.analysis">factor.analysis</a></code>;
Auerswald &amp; Moshagen, 2019). (Default = 'pc')</p>
</td></tr>
<tr><td><code id="Hull_+3A_nfact">nfact</code></td>
<td>
<p>A numeric value that specifies the number of factors to extract, only effective when <code>fa = 'fa'</code>. (Default = 1)</p>
</td></tr>
<tr><td><code id="Hull_+3A_cor.type">cor.type</code></td>
<td>
<p>A character string indicating which correlation coefficient (or covariance) is to be computed. One of &quot;pearson&quot; (default),
&quot;kendall&quot;, or &quot;spearman&quot;. @seealso <a href="stats.html#topic+cor">cor</a>.</p>
</td></tr>
<tr><td><code id="Hull_+3A_use">use</code></td>
<td>
<p>an optional character string giving a method for computing covariances in the presence of missing values. This
must be one of the strings &quot;everything&quot;, &quot;all.obs&quot;, &quot;complete.obs&quot;, &quot;na.or.complete&quot;, or &quot;pairwise.complete.obs&quot; (default).
@seealso <a href="stats.html#topic+cor">cor</a>.</p>
</td></tr>
<tr><td><code id="Hull_+3A_vis">vis</code></td>
<td>
<p>A Boolean variable that will print the factor retention results when set to TRUE, and will not print
when set to FALSE. (default = TRUE)</p>
</td></tr>
<tr><td><code id="Hull_+3A_plot">plot</code></td>
<td>
<p>A Boolean variable that will print the CD plot when set to TRUE, and will not print it when set to
FALSE. @seealso <a href="#topic+plot.Hull">plot.Hull</a>. (Default = TRUE)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Hull method (Lorenzo-Seva &amp; Timmerman, 2011) is a heuristic approach used to determ
ine the number of common factors in factor analysis. This method is similar to
non-graphical variants of Cattell's scree plot but relies on goodness-of-fit indices
relative to the model degrees of freedom. The Hull method finds the optimal number of
factors by following these steps:
</p>

<ol>
<li><p> Calculate the goodness-of-fit index (CFI)
and model degrees of freedom (df; Lorenzo-Seva &amp; Timmerman, 2011; <code class="reqn">df = I × F - 0.5 × F * (F - 1)</code>,
<code class="reqn">I</code> is the number of items, and <code class="reqn">F</code> is the number of factors)
for models with an increasing number of factors, up to a prespecified maximum,
which is equal to the </p>
</li>
<li><p>nfact of <a href="#topic+PA">PA</a> method. the GOF will always be
Comparative Fit Index (CFI), for it performs best under various conditions than other GOF (Auerswald &amp; Moshagen, 2019;
Lorenzo-Seva &amp; Timmerman, 2011), such as RMSEA and SRMR. @seealso <a href="#topic+EFAindex">EFAindex</a>
</p>
</li>
<li><p> Identify and exclude solutions that are less complex (with fewer factors)
but have a higher fit index.
</p>
</li>
<li><p> Further exclude solutions if their fit indices fall below the line connecting
adjacent viable solutions.
</p>
</li>
<li><p> Determine the number of factors where the ratio of the difference in
goodness-of-fit indices to the difference in degrees of freedom is maximized.
</p>
</li></ol>



<h3>Value</h3>

<p>A list with the following components:
</p>
<table role = "presentation">
<tr><td><code>nfact</code></td>
<td>
<p>The optimal number of factors according to the Hull method.</p>
</td></tr>
<tr><td><code>CFI</code></td>
<td>
<p>A numeric vector of CFI values for each number of factors considered.</p>
</td></tr>
<tr><td><code>df</code></td>
<td>
<p>A numeric vector of model degrees of freedom for each number of factors considered.</p>
</td></tr>
<tr><td><code>Hull.CFI</code></td>
<td>
<p>A numeric vector of CFI values with points below the convex Hull curve removed.</p>
</td></tr>
<tr><td><code>Hull.df</code></td>
<td>
<p>A numeric vector of model degrees of freedom with points below the convex Hull curve removed.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Haijiang Qin &lt;Haijiang133@outlook.com&gt;
</p>


<h3>References</h3>

<p>Auerswald, M., &amp; Moshagen, M. (2019). How to determine the number of factors to retain in exploratory factor analysis: A comparison of extraction methods under realistic conditions. Psychological methods, 24(4), 468-491. https://doi.org/https://doi.org/10.1037/met0000200.
</p>
<p>Lorenzo-Seva, U., Timmerman, M. E., &amp; Kiers, H. A. L. (2011). The Hull Method for Selecting the Number of Common Factors. Multivariate Behavioral Research, 46(2), 340-364. https://doi.org/10.1080/00273171.2011.564527.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(EFAfactors)
set.seed(123)

##Take the data.bfi dataset as an example.
data(data.bfi)

response &lt;- as.matrix(data.bfi[, 1:25]) ## loading data
response &lt;- na.omit(response) ## Remove samples with NA/missing values

## Transform the scores of reverse-scored items to normal scoring
response[, c(1, 9, 10, 11, 12, 22, 25)] &lt;- 6 - response[, c(1, 9, 10, 11, 12, 22, 25)] + 1


## Run EKC function with default parameters.

Hull.obj &lt;- Hull(response)

print(Hull.obj)

plot(Hull.obj)

## Get the CFI, df and  nfact results.
CFI &lt;- Hull.obj$CFI
df &lt;- Hull.obj$df
nfact &lt;- Hull.obj$nfact

print(CFI)
print(df)
print(nfact)




</code></pre>

<hr>
<h2 id='KGC'>Kaiser-Guttman Criterion</h2><span id='topic+KGC'></span>

<h3>Description</h3>

<p>This function implements the Kaiser-Guttman criterion (Guttman, 1954; Kaiser, 1960) for determining the number of factors to retain in factor analysis.
It is based on the eigenvalues of the correlation matrix of the responses. According to the criterion, factors are retained
if their corresponding eigenvalues are greater than 1.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>KGC(
  response,
  fa = "pc",
  nfact = 1,
  cor.type = "pearson",
  use = "pairwise.complete.obs",
  vis = TRUE,
  plot = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="KGC_+3A_response">response</code></td>
<td>
<p>A required <code>N</code> × <code>I</code> matrix or data.frame consisting of the responses of <code>N</code> individuals
to <code>I</code> items.</p>
</td></tr>
<tr><td><code id="KGC_+3A_fa">fa</code></td>
<td>
<p>A string that determines the method used to obtain eigenvalues. If 'pc', it represents
Principal Component Analysis (PCA); if 'fa', it represents Principal Axis Factoring (a widely
used Factor Analysis method; @seealso <code><a href="#topic+factor.analysis">factor.analysis</a></code>;
Auerswald &amp; Moshagen, 2019). (Default = 'pc')</p>
</td></tr>
<tr><td><code id="KGC_+3A_nfact">nfact</code></td>
<td>
<p>A numeric value that specifies the number of factors to extract, only effective when <code>fa = 'fa'</code>. (Default = 1)</p>
</td></tr>
<tr><td><code id="KGC_+3A_cor.type">cor.type</code></td>
<td>
<p>A character string indicating which correlation coefficient (or covariance) is to be computed. One of &quot;pearson&quot; (default),
&quot;kendall&quot;, or &quot;spearman&quot;. @seealso <a href="stats.html#topic+cor">cor</a>.</p>
</td></tr>
<tr><td><code id="KGC_+3A_use">use</code></td>
<td>
<p>An optional character string giving a method for computing covariances in the presence of missing values. This
must be one of the strings &quot;everything&quot;, &quot;all.obs&quot;, &quot;complete.obs&quot;, &quot;na.or.complete&quot;, or &quot;pairwise.complete.obs&quot; (default).
@seealso <a href="stats.html#topic+cor">cor</a>.</p>
</td></tr>
<tr><td><code id="KGC_+3A_vis">vis</code></td>
<td>
<p>A Boolean variable that will print the factor retention results when set to TRUE, and will not print
when set to FALSE. (default = TRUE)</p>
</td></tr>
<tr><td><code id="KGC_+3A_plot">plot</code></td>
<td>
<p>A Boolean variable that will print the KGC plot when set to TRUE, and will not print it when set to
FALSE. @seealso <a href="#topic+plot.KGC">plot.KGC</a>. (Default = TRUE)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>KGC</code> is a <code>list</code> containing the following components:
</p>
<table role = "presentation">
<tr><td><code>nfact</code></td>
<td>
<p>The number of factors to be retained.</p>
</td></tr>
<tr><td><code>eigen.value</code></td>
<td>
<p>A vector containing the empirical eigenvalues</p>
</td></tr>
</table>


<h3>References</h3>

<p>Guttman, L. (1954). Some necessary conditions for common-factor analysis. Psychometrika, 19, 149–161. http://dx.doi.org/10.1007/BF02289162.
</p>
<p>Kaiser, H. F. (1960). The application of electronic computers to factor analysis. Educational and Psychological Measurement, 20, 141–151. http://dx.doi.org/10.1177/001316446002000116.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(EFAfactors)
set.seed(123)

##Take the data.bfi dataset as an example.
data(data.bfi)

response &lt;- as.matrix(data.bfi[, 1:25]) ## loading data
response &lt;- na.omit(response) ## Remove samples with NA/missing values

## Transform the scores of reverse-scored items to normal scoring
response[, c(1, 9, 10, 11, 12, 22, 25)] &lt;- 6 - response[, c(1, 9, 10, 11, 12, 22, 25)] + 1


## Run KGC function with default parameters.

KGC.obj &lt;- KGC(response)

print(KGC.obj)

plot(KGC.obj)

## Get the eigen.value, eigen.ref and  nfact results.
eigen.value &lt;- KGC.obj$eigen.value
nfact &lt;- KGC.obj$nfact

print(eigen.value)
print(nfact)




</code></pre>

<hr>
<h2 id='load_DNN'>Load the Trained Deep Neural Network (DNN)</h2><span id='topic+load_DNN'></span>

<h3>Description</h3>

<p>Loads the Pre-Trained Deep Neural Network (DNN) from the <code>DNN.onnx</code>.
The function uses the <code>reticulate</code> package to import the <code>onnxruntime</code> Python library
and create an inference session for the model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>load_DNN()
</code></pre>


<h3>Value</h3>

<p>An ONNX runtime inference session object for the DNN model.
</p>


<h3>Note</h3>

<p>Note that Python and the libraries <code>numpy</code> and <code>onnxruntime</code> are required.
</p>
<p>First, please ensure that Python is installed on your computer and that Python is
included in the system's PATH environment variable. If not,
please download and install it from the official website (https://www.python.org/).
</p>
<p>If you encounter an error when running this function stating that the <code>numpy</code> and <code>onnxruntime</code>
modules are missing:
</p>
<p><code>Error in py_module_import(module, convert = convert) :</code>
</p>
<p><code>ModuleNotFoundError: No module named 'numpy'</code>
</p>
<p>or
</p>
<p><code>Error in py_module_import(module, convert = convert) :</code>
</p>
<p><code>ModuleNotFoundError: No module named 'onnxruntime'</code>
</p>
<p>this means that the <code>numpy</code> or <code>onnxruntime</code> library is missing from your Python environment. If you are using Windows or macOS,
please run the command <code>pip install numpy</code> or <code>pip install onnxruntime</code> in Command Prompt or Windows PowerShell (Windows), or Terminal (macOS).
If you are using Linux, please ensure that <code>pip</code> is installed and use the command <code>pip install numpy</code> or
<code>pip install onnxruntime</code> to install the missing libraries.
</p>


<h3>See Also</h3>

<p><a href="#topic+DNN_predictor">DNN_predictor</a>
</p>

<hr>
<h2 id='load_scaler'>Load the Scaler for the Pre-Trained Deep Neural Network (DNN)</h2><span id='topic+load_scaler'></span>

<h3>Description</h3>

<p>Loads the scaler object within the <code>EFAfactors</code> package. This object is a <code>list</code> containing a mean vector and
a standard deviation vector, which were computed from the 10,000,000 datasets <a href="#topic+data.datasets">data.datasets</a>
used for training the Pre-Trained Deep Neural Network (DNN). It serves as a tool for normalizing features in
<a href="#topic+DNN_predictor">DNN_predictor</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>load_scaler()
</code></pre>


<h3>Value</h3>

<p>scaler objective.
</p>


<h3>See Also</h3>

<p><a href="#topic+DNN_predictor">DNN_predictor</a>, <a href="#topic+normalizor">normalizor</a>, <a href="#topic+data.datasets">data.datasets</a>, <a href="#topic+data.scaler">data.scaler</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(EFAfactors)

scaler &lt;- load_scaler()
print(scaler)


</code></pre>

<hr>
<h2 id='load_xgb'>Load the Tuned XGBoost Model</h2><span id='topic+load_xgb'></span>

<h3>Description</h3>

<p>Loads the tuned XGBoost model object within the <code>EFAfactors</code> package
into the global environment and retrieves it for use. Only the core model is retained to reduce the size.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>load_xgb()
</code></pre>


<h3>Value</h3>

<p>The tuned XGBoost model object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(EFAfactors)

xgb_model &lt;- load_xgb()
print(xgb_model)


</code></pre>

<hr>
<h2 id='model.xgb'>the Tuned XGBoost Model for Determining the Number of Facotrs</h2><span id='topic+model.xgb'></span>

<h3>Description</h3>

<p>the Tuned XGBoost Model for Determining the Number of Facotrs
</p>


<h3>Format</h3>

<p>An object of class <code>TuneModel</code> is the Tuned XGBoost Model for Determining the Number of Facotrs
</p>


<h3>See Also</h3>

<p><a href="#topic+FF">FF</a>, <a href="#topic+load_xgb">load_xgb</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(model.xgb)
print(model.xgb)

model.xgb &lt;- load_xgb()
print(model.xgb)

</code></pre>

<hr>
<h2 id='normalizor'>Feature Normalization</h2><span id='topic+normalizor'></span>

<h3>Description</h3>

<p>This function normalizes a matrix of features using precomputed means and standard deviations.
The function automatically runs <a href="#topic+load_scaler">load_scaler</a> to read the standard deviations and means of the features,
which are organized into a <code>list</code> object named <code>scaler</code>. These means and standard deviations are computed from
the 10,000,000 datasets <code><a href="#topic+data.datasets">data.datasets</a></code> for training the Pre-Trained Deep Neural Network (DNN).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>normalizor(features)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="normalizor_+3A_features">features</code></td>
<td>
<p>A numeric matrix where each row represents an observation and each column represents a feature.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function applies z-score normalization to each element in the <code>features</code> matrix. It uses
the <code>scaler</code> object, which is expected to contain precomputed means and standard deviations for each feature.
The normalized value for each element is computed as:
</p>
<p style="text-align: center;"><code class="reqn">z = \frac{x - \mu}{\sigma}</code>
</p>

<p>where <code class="reqn">x</code> is the original value, <code class="reqn">\mu</code> is the mean, and <code class="reqn">\sigma</code> is the standard deviation.
</p>


<h3>Value</h3>

<p>A matrix of the same dimensions as <code>features</code>, where each feature has been normalized.
</p>


<h3>See Also</h3>

<p><a href="#topic+DNN_predictor">DNN_predictor</a>, <a href="#topic+load_scaler">load_scaler</a>, <a href="#topic+data.datasets">data.datasets</a>, <a href="#topic+data.scaler">data.scaler</a>
</p>

<hr>
<h2 id='PA'>Parallel Analysis</h2><span id='topic+PA'></span>

<h3>Description</h3>

<p>This function performs Parallel Analysis (PA), which is a method used to determine the number of
factors to retain in exploratory factor analysis. It compares the empirical eigenvalues with those obtained
from simulated random data to identify the point where the observed eigenvalues are larger than those expected by chance.
The number of empirical eigenvalues that are greater than the corresponding reference eigenvalues is the number
of factors recommended to be retained by the PA method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PA(
  response,
  fa = "pc",
  n.iter = 100,
  type = "quant",
  nfact = 1,
  quant = 0.95,
  cor.type = "pearson",
  use = "pairwise.complete.obs",
  vis = TRUE,
  plot = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="PA_+3A_response">response</code></td>
<td>
<p>A required <code>N</code> × <code>I</code> matrix or data.frame consisting of the responses
of <code>N</code> individuals to <code>I</code> items.</p>
</td></tr>
<tr><td><code id="PA_+3A_fa">fa</code></td>
<td>
<p>A string that determines the method used to obtain eigenvalues in PA. If 'pc', it represents
Principal Component Analysis (PCA); if 'fa', it represents Principal Axis Factoring (a widely
used Factor Analysis method; @seealso <code><a href="#topic+factor.analysis">factor.analysis</a></code>;
Auerswald &amp; Moshagen, 2019). (Default = 'pc')</p>
</td></tr>
<tr><td><code id="PA_+3A_n.iter">n.iter</code></td>
<td>
<p>A numeric value that determines the number of simulations for the random data. (Default = 100)</p>
</td></tr>
<tr><td><code id="PA_+3A_type">type</code></td>
<td>
<p>A string that determines the method used to calculate the reference eigenvalues from the simulated data.
If 'mean', the reference eigenvalue (<code>eigen.ref</code>) is the mean of the simulated eigenvalues (<code>eigen.sim</code>);
if 'quant', the reference eigenvalue is the <code>quant</code> percentile of <code>eigen.sim</code>. (Default = 'quant')</p>
</td></tr>
<tr><td><code id="PA_+3A_nfact">nfact</code></td>
<td>
<p>A numeric value that specifies the number of factors to extract, only effective when <code>fa = 'fa'</code>. (Default = 1)</p>
</td></tr>
<tr><td><code id="PA_+3A_quant">quant</code></td>
<td>
<p>A numeric value between 0 and 1, representing the quantile to be used for the reference
eigenvalues calculation when <code>type = 'quant'</code>. (Default = 0.95)</p>
</td></tr>
<tr><td><code id="PA_+3A_cor.type">cor.type</code></td>
<td>
<p>A character string indicating the correlation coefficient (or covariance) to
be computed. One of &quot;pearson&quot; (default), &quot;kendall&quot;, or &quot;spearman&quot;. @seealso <a href="stats.html#topic+cor">cor</a>.</p>
</td></tr>
<tr><td><code id="PA_+3A_use">use</code></td>
<td>
<p>An optional character string specifying the method for computing covariances when
there are missing values. This must be one of &quot;everything&quot;, &quot;all.obs&quot;, &quot;complete.obs&quot;,
&quot;na.or.complete&quot;, or &quot;pairwise.complete.obs&quot; (default). @seealso <a href="stats.html#topic+cor">cor</a>.</p>
</td></tr>
<tr><td><code id="PA_+3A_vis">vis</code></td>
<td>
<p>A Boolean that determines whether to print the factor retention results. Set to <code>TRUE</code>
to print, or <code>FALSE</code> to suppress output. (Default = TRUE)</p>
</td></tr>
<tr><td><code id="PA_+3A_plot">plot</code></td>
<td>
<p>A Boolean that determines whether to display the PA plot. Set to <code>TRUE</code> to show the plot,
or <code>FALSE</code> to suppress it. @seealso <a href="#topic+plot.PA">plot.PA</a>. (Default = TRUE)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function performs Parallel Analysis (PA; Horn, 1965; Auerswald &amp; Moshagen, 2019) to determine the number of factors to retain.
PA is a widely used method and is considered the &quot;gold standard&quot; for factor retention due to its high accuracy and stability,
although it may underperform compared to methods like CD or EKC under certain conditions.
The core idea of PA is to simulate random data multiple times, for example, 100 times, and compute the eigenvalues from each simulation.
These simulated eigenvalues are then processed using either the mean or a quantile method to obtain the reference eigenvalues,
such as the i-th reference eigenvalue <code class="reqn">\lambda_{i,ref}</code>.
The relationship between the i-th empirical eigenvalue <code class="reqn">\lambda_{i}</code> and <code class="reqn">\lambda_{i,ref}</code> indicates whether the i-th factor should be retained.
If <code class="reqn">\lambda_{i} &gt; \lambda_{i,ref}</code>, it suggests that the explanatory power of the i-th factor from the original data is stronger than that of the i-th factor from the random data,
and therefore the factor should be retained. Conversely, if <code class="reqn">\lambda_{i} &lt;= \lambda_{i,ref}</code>,
it indicates that the explanatory power of the i-th factor from the original data is weaker or equal to that of the random data,
making it indistinguishable from noise, and thus the factor should not be retained. So,
</p>
<p style="text-align: center;"><code class="reqn">F = \sum_{i=1}^{I} I(\lambda_i &gt; \lambda_{i,ref})</code>
</p>

<p>Here, \( F \) represents the number of factors determined by the EKC, and <code class="reqn">I(\cdot)</code> is the
indicator function, which equals 1 when the condition is true, and 0 otherwise.
</p>
<p>Auerswald &amp; Moshagen (2019) found that the most accurate results for PA were obtained when
using PCA to extract eigenvalues and using the 95th percentile of the simulated
eigenvalues to calculate the reference eigenvalues. Therefore,
the recommended settings for this function are <code>fa = 'pc'</code>, <code>type = 'quant'</code>, and <code>quant = 0.95</code>.
</p>


<h3>Value</h3>

<p>An object of class <code>PA</code>, which is a <code>list</code> containing the following components:
</p>
<table role = "presentation">
<tr><td><code>nfact</code></td>
<td>
<p>The number of factors to retain.</p>
</td></tr>
<tr><td><code>fa</code></td>
<td>
<p>Indicates the method used to obtain eigenvalues in PA. 'pc' represents Principal Component Analysis, and 'fa' represents Principal Axis Factoring.</p>
</td></tr>
<tr><td><code>type</code></td>
<td>
<p>Indicates the method used to calculate <code>eigen.ref</code>. If 'mean', <code>eigen.ref</code> is the mean of <code>eigen.sim</code>; if 'quant', <code>eigen.ref</code> is the <code>quant</code> percentile of <code>eigen.sim</code>.</p>
</td></tr>
<tr><td><code>eigen.value</code></td>
<td>
<p>A vector containing the empirical eigenvalues.</p>
</td></tr>
<tr><td><code>eigen.ref</code></td>
<td>
<p>A vector containing the reference eigenvalues, which depend on <code>type</code>.</p>
</td></tr>
<tr><td><code>eigen.sim</code></td>
<td>
<p>A matrix containing the simulated eigenvalues for all iterations.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Haijiang Qin &lt;Haijiang133@outlook.com&gt;
</p>


<h3>References</h3>

<p>Auerswald, M., &amp; Moshagen, M. (2019). How to determine the number of factors to retain in exploratory factor analysis: A comparison of extraction methods under realistic conditions. Psychological methods, 24(4), 468-491. https://doi.org/10.1037/met0000200.
</p>
<p>Horn, J. L. (1965). A rationale and test for the number of factors in factor analysis. Psychometrika, 30, 179–185. http://dx.doi.org/10.1007/BF02289447.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(EFAfactors)
set.seed(123)

##Take the data.bfi dataset as an example.
data(data.bfi)

response &lt;- as.matrix(data.bfi[, 1:25]) ## loading data
response &lt;- na.omit(response) ## Remove samples with NA/missing values

## Transform the scores of reverse-scored items to normal scoring
response[, c(1, 9, 10, 11, 12, 22, 25)] &lt;- 6 - response[, c(1, 9, 10, 11, 12, 22, 25)] + 1


## Run PA function with default parameters.

PA.obj &lt;- PA(response)

print(PA.obj)

plot(PA.obj)

## Get the eigen.value, eigen.ref and  nfact results.
eigen.value &lt;- PA.obj$eigen.value
eigen.ref &lt;- PA.obj$eigen.ref
nfact &lt;- PA.obj$nfact

print(eigen.value)
print(eigen.ref)
print(nfact)



</code></pre>

<hr>
<h2 id='plot.CD'>Plot Comparison Data for Factor Analysis</h2><span id='topic+plot.CD'></span>

<h3>Description</h3>

<p>This function generates a Comparison Data plot to visualize the Root Mean Square Error (RMSE) of
eigenvalues for various numbers of factors. This plot helps in evaluating the fit of different
factor models and identifying the optimal number of factors based on RMSE values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'CD'
plot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.CD_+3A_x">x</code></td>
<td>
<p>An object of class <code>CD</code>, representing the results to be plotted.</p>
</td></tr>
<tr><td><code id="plot.CD_+3A_...">...</code></td>
<td>
<p>Additional arguments to be passed to the plotting function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None. This function is used for side effects (plotting).
</p>


<h3>See Also</h3>

<p><a href="#topic+CD">CD</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(EFAfactors)
set.seed(123)

##Take the data.bfi dataset as an example.
data(data.bfi)

response &lt;- as.matrix(data.bfi[, 1:25]) ## loading data
response &lt;- na.omit(response) ## Remove samples with NA/missing values

## Transform the scores of reverse-scored items to normal scoring
response[, c(1, 9, 10, 11, 12, 22, 25)] &lt;- 6 - response[, c(1, 9, 10, 11, 12, 22, 25)] + 1


CD.obj &lt;- CD(response)

## CD plot
plot(CD.obj)



</code></pre>

<hr>
<h2 id='plot.CDF'>Plot Comparison Data Forest (CDF) Classification Probability Distribution</h2><span id='topic+plot.CDF'></span>

<h3>Description</h3>

<p>This function generates a bar plot of the classification probabilities predicted by the Comparison Data Forest
for determining the number of factors. The plot displays the probability distribution across different numbers of factors,
with each bar representing the probability for a specific number of factors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'CDF'
plot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.CDF_+3A_x">x</code></td>
<td>
<p>An object of class <code>CDF</code>, representing the results to be plotted.</p>
</td></tr>
<tr><td><code id="plot.CDF_+3A_...">...</code></td>
<td>
<p>Additional arguments to be passed to the plotting function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None. This function is used for side effects (plotting).
</p>


<h3>See Also</h3>

<p><a href="#topic+CDF">CDF</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(EFAfactors)
set.seed(123)

## Take the data.bfi dataset as an example.
data(data.bfi)

response &lt;- as.matrix(data.bfi[, 1:25]) ## Load data
response &lt;- na.omit(response) ## Remove samples with NA/missing values

## Transform the scores of reverse-scored items to normal scoring
response[, c(1, 9, 10, 11, 12, 22, 25)] &lt;- 6 - response[, c(1, 9, 10, 11, 12, 22, 25)] + 1


CDF.obj &lt;- CDF(response)

## Plot the CDF probabilities
plot(CDF.obj)



</code></pre>

<hr>
<h2 id='plot.DNN_predictor'>Plot DNN Predictor Classification Probability Distribution</h2><span id='topic+plot.DNN_predictor'></span>

<h3>Description</h3>

<p>This function generates a bar plot of the classification probabilities predicted by the pre-trained deep neural network
for determining the number of factors. The plot displays the probability distribution across different numbers of factors,
with each bar representing the probability for a specific number of factors. The maximum number of factors that the network
can evaluate is 10. The function also annotates each bar with its probability value.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'DNN_predictor'
plot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.DNN_predictor_+3A_x">x</code></td>
<td>
<p>An object of class <code>DNN_predictor</code>, representing the results to be plotted.</p>
</td></tr>
<tr><td><code id="plot.DNN_predictor_+3A_...">...</code></td>
<td>
<p>Additional arguments to be passed to the plotting function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None. This function is used for side effects (plotting).
</p>


<h3>See Also</h3>

<p><a href="#topic+DNN_predictor">DNN_predictor</a>
</p>

<hr>
<h2 id='plot.EFAhclust'>Plot Hierarchical Cluster Analysis Dendrogram</h2><span id='topic+plot.EFAhclust'></span>

<h3>Description</h3>

<p>This function generates a dendrogram from hierarchical cluster analysis results. The hierarchical clustering method
merges the two nodes with the smallest dissimilarity at each step, forming a new node until all nodes are combined
into a single hierarchical structure. The resulting dendrogram represents the hierarchical relationships between items,
where items with high correlation are connected by shorter lines, and items with low correlation are connected by longer lines.
The height of the tree nodes indicates the dissimilarity between nodes: a greater height reflects a larger difference.
Researchers can use this representation to determine if two nodes belong to the same cluster, which in exploratory factor analysis,
helps identify whether items belong to the same latent factor.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'EFAhclust'
plot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.EFAhclust_+3A_x">x</code></td>
<td>
<p>An object of class <code>EFAhclust</code>, representing the results to be plotted.</p>
</td></tr>
<tr><td><code id="plot.EFAhclust_+3A_...">...</code></td>
<td>
<p>Additional arguments to be passed to the plotting function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None. This function is used for side effects (plotting).
</p>


<h3>See Also</h3>

<p><a href="#topic+EFAhclust">EFAhclust</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(EFAfactors)
set.seed(123)

## Take the data.bfi dataset as an example.
data(data.bfi)

response &lt;- as.matrix(data.bfi[, 1:25]) ## Load data
response &lt;- na.omit(response) ## Remove samples with NA/missing values

## Transform the scores of reverse-scored items to normal scoring
response[, c(1, 9, 10, 11, 12, 22, 25)] &lt;- 6 - response[, c(1, 9, 10, 11, 12, 22, 25)] + 1


  EFAhclust.obj &lt;- EFAhclust(response)

  ## Plot the hierarchical clustering dendrogram
  plot(EFAhclust.obj)



</code></pre>

<hr>
<h2 id='plot.EFAkmeans'>Plot EFA K-means Clustering Results</h2><span id='topic+plot.EFAkmeans'></span>

<h3>Description</h3>

<p>This function creates a plot to visualize the Within-cluster Sum of Squares (WSS) for different numbers of clusters (K)
in the context of exploratory factor analysis. The plot helps identify the most appropriate number of factors by showing
how WSS decreases as the number of factors (or clusters) increases.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'EFAkmeans'
plot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.EFAkmeans_+3A_x">x</code></td>
<td>
<p>An object of class <code>EFAkmeans</code>, representing the results to be plotted.</p>
</td></tr>
<tr><td><code id="plot.EFAkmeans_+3A_...">...</code></td>
<td>
<p>Additional arguments to be passed to the plotting function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None. This function is used for side effects (plotting).
</p>


<h3>See Also</h3>

<p><a href="#topic+EFAkmeans">EFAkmeans</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(EFAfactors)
set.seed(123)

## Take the data.bfi dataset as an example.
data(data.bfi)

response &lt;- as.matrix(data.bfi[, 1:25]) ## Load data
response &lt;- na.omit(response) ## Remove samples with NA/missing values

## Transform the scores of reverse-scored items to normal scoring
response[, c(1, 9, 10, 11, 12, 22, 25)] &lt;- 6 - response[, c(1, 9, 10, 11, 12, 22, 25)] + 1


  EFAkmeans.obj &lt;- EFAkmeans(response)

  ## Plot the EFA K-means clustering results
  plot(EFAkmeans.obj)


</code></pre>

<hr>
<h2 id='plot.EFAscreet'>Plots the Scree Plot</h2><span id='topic+plot.EFAscreet'></span>

<h3>Description</h3>

<p>Plots the Scree Plot from an object of class <code>EFAscreet</code>. The scree plot visualizes the eigenvalues
of the correlation matrix in descending order and helps in identifying the optimal number of factors
by showing where the eigenvalues start to plateau.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'EFAscreet'
plot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.EFAscreet_+3A_x">x</code></td>
<td>
<p>An object of class <code>EFAscreet</code>, which contains the eigenvalues from the factor analysis.</p>
</td></tr>
<tr><td><code id="plot.EFAscreet_+3A_...">...</code></td>
<td>
<p>Additional arguments to be passed to the <code>plot</code> function (not used).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The scree plot is a graphical tool used in exploratory factor analysis. It shows the eigenvalues
corresponding to the factors. The number of factors is typically determined by finding the point
where the plot levels off (&quot;elbow&quot; point).
</p>


<h3>Value</h3>

<p>A scree plot displaying the eigenvalues against the number of factors.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(EFAfactors)
set.seed(123)

##Take the data.bfi dataset as an example.
data(data.bfi)

response &lt;- as.matrix(data.bfi[, 1:25]) ## loading data
response &lt;- na.omit(response) ## Remove samples with NA/missing values

## Transform the scores of reverse-scored items to normal scoring
response[, c(1, 9, 10, 11, 12, 22, 25)] &lt;- 6 - response[, c(1, 9, 10, 11, 12, 22, 25)] + 1


## Run EFAscreet function with default parameters.

 EFAscreet.obj &lt;- EFAscreet(response)

 plot(EFAscreet.obj)



</code></pre>

<hr>
<h2 id='plot.EFAvote'>Plot Voting Results for Number of Factors</h2><span id='topic+plot.EFAvote'></span>

<h3>Description</h3>

<p>This function creates a pie chart to visualize the results of a voting method used to
determine the number of factors in exploratory factor analysis (EFA). The voting method combines
the results from multiple EFA techniques, and the pie chart displays the proportions of votes
each number of factors received. Each slice of the pie represents the percentage of votes
for a specific number of factors, providing a clear visual representation of the most
commonly suggested number of factors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'EFAvote'
plot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.EFAvote_+3A_x">x</code></td>
<td>
<p>An object of class <code>EFAvote</code>, representing the results to be plotted.</p>
</td></tr>
<tr><td><code id="plot.EFAvote_+3A_...">...</code></td>
<td>
<p>Additional arguments to be passed to the plotting function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None. This function is used for side effects (plotting).
</p>


<h3>See Also</h3>

<p><a href="#topic+EFAvote">EFAvote</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(EFAfactors)


nfacts &lt;- c(5, 5, 5, 6, 6, 4)
names(nfacts) &lt;- c("Hull", "CD", "PA", "EKC", "XGB","DNN")

EFAvote.obj &lt;- EFAvote(votes = nfacts)
plot(EFAvote.obj)



</code></pre>

<hr>
<h2 id='plot.EKC'>Plot Empirical Kaiser Criterion (EKC) Plot</h2><span id='topic+plot.EKC'></span>

<h3>Description</h3>

<p>This function generates an Empirical Kaiser Criterion (EKC) plot to visualize the eigenvalues
of the actual data. The EKC method helps in determining the number of factors to retain by
identifying the point where the eigenvalues exceed the reference eigenvalue.
The plot provides a graphical representation to assist in factor selection.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'EKC'
plot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.EKC_+3A_x">x</code></td>
<td>
<p>An object of class <code>EKC</code>, representing the results to be plotted.</p>
</td></tr>
<tr><td><code id="plot.EKC_+3A_...">...</code></td>
<td>
<p>Additional arguments to be passed to the plotting function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None. This function is used for side effects (plotting).
</p>


<h3>See Also</h3>

<p><a href="#topic+EKC">EKC</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(EFAfactors)
set.seed(123)

##Take the data.bfi dataset as an example.
data(data.bfi)

response &lt;- as.matrix(data.bfi[, 1:25]) ## loading data
response &lt;- na.omit(response) ## Remove samples with NA/missing values

## Transform the scores of reverse-scored items to normal scoring
response[, c(1, 9, 10, 11, 12, 22, 25)] &lt;- 6 - response[, c(1, 9, 10, 11, 12, 22, 25)] + 1


 EKC.obj &lt;- EKC(response)

 ## EKC plot
 plot(EKC.obj)



</code></pre>

<hr>
<h2 id='plot.FF'>Plot Factor Forest (FF) Classification Probability Distribution</h2><span id='topic+plot.FF'></span>

<h3>Description</h3>

<p>This function generates a bar plot of the classification probabilities predicted by the Factor Forest
for determining the number of factors. The plot displays the probability distribution across different numbers of factors,
with each bar representing the probability for a specific number of factors. Unlike the deep neural network (DNN) model,
the Factor Forest can evaluate up to a maximum of 8 factors. The function also annotates each bar with its probability value.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'FF'
plot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.FF_+3A_x">x</code></td>
<td>
<p>An object of class <code>FF</code>, representing the results to be plotted.</p>
</td></tr>
<tr><td><code id="plot.FF_+3A_...">...</code></td>
<td>
<p>Additional arguments to be passed to the plotting function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None. This function is used for side effects (plotting).
</p>


<h3>See Also</h3>

<p><a href="#topic+FF">FF</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(EFAfactors)
set.seed(123)

## Take the data.bfi dataset as an example.
data(data.bfi)

response &lt;- as.matrix(data.bfi[, 1:25]) ## Load data
response &lt;- na.omit(response) ## Remove samples with NA/missing values

## Transform the scores of reverse-scored items to normal scoring
response[, c(1, 9, 10, 11, 12, 22, 25)] &lt;- 6 - response[, c(1, 9, 10, 11, 12, 22, 25)] + 1


FF.obj &lt;- FF(response)

## Plot the FF probabilities
plot(FF.obj)



</code></pre>

<hr>
<h2 id='plot.Hull'>Plot Hull Plot for Factor Analysis</h2><span id='topic+plot.Hull'></span>

<h3>Description</h3>

<p>This function creates a Hull plot to visualize the relationship between
the Comparative Fit Index (CFI) and the degrees of freedom (df) for a
range of models with different numbers of factors. The Hull plot helps
in assessing model fit and identifying optimal models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'Hull'
plot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.Hull_+3A_x">x</code></td>
<td>
<p>An object of class <code>Hull</code>, representing the results to be plotted.</p>
</td></tr>
<tr><td><code id="plot.Hull_+3A_...">...</code></td>
<td>
<p>Additional arguments to be passed to the plotting function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None. This function is used for side effects (plotting).
</p>


<h3>See Also</h3>

<p><a href="#topic+Hull">Hull</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(EFAfactors)
set.seed(123)

##Take the data.bfi dataset as an example.
data(data.bfi)

response &lt;- as.matrix(data.bfi[, 1:25]) ## loading data
response &lt;- na.omit(response) ## Remove samples with NA/missing values

## Transform the scores of reverse-scored items to normal scoring
response[, c(1, 9, 10, 11, 12, 22, 25)] &lt;- 6 - response[, c(1, 9, 10, 11, 12, 22, 25)] + 1


 Hull.obj &lt;- CD(response)

 ## Hull plot
 plot(Hull.obj)



</code></pre>

<hr>
<h2 id='plot.KGC'>Plot Kaiser-Guttman Criterion (KGC) Plot</h2><span id='topic+plot.KGC'></span>

<h3>Description</h3>

<p>This function generates a Kaiser-Guttman Criterion (KGC) plot to visualize the eigenvalues
of the actual data. The Kaiser-Guttman Criterion, also known as the Kaiser criterion,
suggests retaining factors with eigenvalues greater than 1. The plot shows the eigenvalues
and includes a reference line at 1 to indicate the threshold for factor retention.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'KGC'
plot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.KGC_+3A_x">x</code></td>
<td>
<p>An object of class <code>KGC</code>, representing the results to be plotted.</p>
</td></tr>
<tr><td><code id="plot.KGC_+3A_...">...</code></td>
<td>
<p>Additional arguments to be passed to the plotting function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None. This function is used for side effects (plotting).
</p>


<h3>See Also</h3>

<p><a href="#topic+KGC">KGC</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(EFAfactors)
set.seed(123)

## Take the data.bfi dataset as an example.
data(data.bfi)

response &lt;- as.matrix(data.bfi[, 1:25]) ## Load data
response &lt;- na.omit(response) ## Remove samples with NA/missing values

## Transform the scores of reverse-scored items to normal scoring
response[, c(1, 9, 10, 11, 12, 22, 25)] &lt;- 6 - response[, c(1, 9, 10, 11, 12, 22, 25)] + 1


  KGC.obj &lt;- KGC(response)

  ## Plot the Kaiser-Guttman Criterion
  plot(KGC.obj)


</code></pre>

<hr>
<h2 id='plot.PA'>Plot Parallel Analysis Scree Plot</h2><span id='topic+plot.PA'></span>

<h3>Description</h3>

<p>This function creates a Parallel Analysis (PA) scree plot to compare the eigenvalues of the actual
data with the eigenvalues from simulated data. The plot helps in determining the number of factors
by visualizing where the eigenvalues of the actual data intersect with those from simulated data.
It provides a graphical representation of the results from a parallel analysis to aid in factor selection.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'PA'
plot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.PA_+3A_x">x</code></td>
<td>
<p>An object of class <code>PA</code>, representing the results to be plotted.</p>
</td></tr>
<tr><td><code id="plot.PA_+3A_...">...</code></td>
<td>
<p>Additional arguments to be passed to the plotting function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None. This function is used for side effects (plotting).
</p>


<h3>See Also</h3>

<p><a href="#topic+PA">PA</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(EFAfactors)
set.seed(123)

##Take the data.bfi dataset as an example.
data(data.bfi)

response &lt;- as.matrix(data.bfi[, 1:25]) ## loading data
response &lt;- na.omit(response) ## Remove samples with NA/missing values

## Transform the scores of reverse-scored items to normal scoring
response[, c(1, 9, 10, 11, 12, 22, 25)] &lt;- 6 - response[, c(1, 9, 10, 11, 12, 22, 25)] + 1


 PA.obj &lt;- PA(response)

 ## PA plot
 plot(PA.obj)




</code></pre>

<hr>
<h2 id='predictLearner.classif.xgboost.earlystop'>Prediction Function for the Tuned XGBoost Model with Early Stopping</h2><span id='topic+predictLearner.classif.xgboost.earlystop'></span>

<h3>Description</h3>

<p>This function performs predictions using a trained XGBoost model with early stopping.
The function itself does not have any specific purpose; its existence is solely to
ensure the proper operation of <a href="#topic+FF">FF</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'classif.xgboost.earlystop'
predictLearner(.learner, .model, .newdata, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predictLearner.classif.xgboost.earlystop_+3A_.learner">.learner</code></td>
<td>
<p>An object representing the learner.</p>
</td></tr>
<tr><td><code id="predictLearner.classif.xgboost.earlystop_+3A_.model">.model</code></td>
<td>
<p>The trained XGBoost model used to make predictions.</p>
</td></tr>
<tr><td><code id="predictLearner.classif.xgboost.earlystop_+3A_.newdata">.newdata</code></td>
<td>
<p>A data frame or matrix containing new observations for which predictions are to be made.</p>
</td></tr>
<tr><td><code id="predictLearner.classif.xgboost.earlystop_+3A_...">...</code></td>
<td>
<p>Additional parameters passed to the <code>predict</code> function in XGBoost.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of predicted class labels or a matrix of predicted probabilities.
</p>

<hr>
<h2 id='print.CD'>Print Comparison Data Method Results</h2><span id='topic+print.CD'></span>

<h3>Description</h3>

<p>This function prints the number of factors suggested by the Comparison Data (CD) method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'CD'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.CD_+3A_x">x</code></td>
<td>
<p>An object of class <code>CD</code>, representing the results to be printed.</p>
</td></tr>
<tr><td><code id="print.CD_+3A_...">...</code></td>
<td>
<p>Additional arguments to be passed to the plotting function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None. This function is used for side effects (printing).
</p>


<h3>See Also</h3>

<p><a href="#topic+CD">CD</a>
</p>

<hr>
<h2 id='print.CDF'>Print Comparison Data Forest (CDF) Results</h2><span id='topic+print.CDF'></span>

<h3>Description</h3>

<p>This function prints the number of factors suggested by the Comparison Data Forest.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'CDF'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.CDF_+3A_x">x</code></td>
<td>
<p>An object of class <code>CDF</code>, representing the results to be printed.</p>
</td></tr>
<tr><td><code id="print.CDF_+3A_...">...</code></td>
<td>
<p>Additional arguments to be passed to the plotting function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None. This function is used for side effects (printing).
</p>


<h3>See Also</h3>

<p><a href="#topic+CDF">CDF</a>
</p>

<hr>
<h2 id='print.DNN_predictor'>Print DNN Predictor Method Results</h2><span id='topic+print.DNN_predictor'></span>

<h3>Description</h3>

<p>This function prints the number of factors suggested by the deep neural network (DNN) predictor.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'DNN_predictor'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.DNN_predictor_+3A_x">x</code></td>
<td>
<p>An object of class <code>DNN_predictor</code>, representing the results to be printed.</p>
</td></tr>
<tr><td><code id="print.DNN_predictor_+3A_...">...</code></td>
<td>
<p>Additional arguments to be passed to the plotting function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None. This function is used for side effects (printing).
</p>


<h3>See Also</h3>

<p><a href="#topic+DNN_predictor">DNN_predictor</a>
</p>

<hr>
<h2 id='print.EFAdata'>Print the EFAsim.data</h2><span id='topic+print.EFAdata'></span>

<h3>Description</h3>

<p>This function prints the items in factors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'EFAdata'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.EFAdata_+3A_x">x</code></td>
<td>
<p>An object of class <code>EFA.data</code>, representing the results to be printed.</p>
</td></tr>
<tr><td><code id="print.EFAdata_+3A_...">...</code></td>
<td>
<p>Additional arguments to be passed to the plotting function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None. This function is used for side effects (printing).
</p>


<h3>See Also</h3>

<p><a href="#topic+EFAsim.data">EFAsim.data</a>
</p>

<hr>
<h2 id='print.EFAhclust'>Print EFAhclust Method Results</h2><span id='topic+print.EFAhclust'></span>

<h3>Description</h3>

<p>This function prints the number of factors suggested by the EFAhclust method using the Second-Order Difference (SOD) approach.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'EFAhclust'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.EFAhclust_+3A_x">x</code></td>
<td>
<p>An object of class <code>EFAhclust</code>, representing the results to be printed.</p>
</td></tr>
<tr><td><code id="print.EFAhclust_+3A_...">...</code></td>
<td>
<p>Additional arguments to be passed to the plotting function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None. This function is used for side effects (printing).
</p>


<h3>See Also</h3>

<p><a href="#topic+EFAhclust">EFAhclust</a>
</p>

<hr>
<h2 id='print.EFAkmeans'>Print EFAkmeans Method Results</h2><span id='topic+print.EFAkmeans'></span>

<h3>Description</h3>

<p>This function prints the number of factors suggested by the EFAkmeans method using the Second-Order Difference (SOD) approach.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'EFAkmeans'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.EFAkmeans_+3A_x">x</code></td>
<td>
<p>An object of class <code>EFAkmeans</code>, representing the results to be printed.</p>
</td></tr>
<tr><td><code id="print.EFAkmeans_+3A_...">...</code></td>
<td>
<p>Additional arguments to be passed to the plotting function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None. This function is used for side effects (printing).
</p>


<h3>See Also</h3>

<p><a href="#topic+EFAkmeans">EFAkmeans</a>
</p>

<hr>
<h2 id='print.EFAscreet'>Print the Scree Plot</h2><span id='topic+print.EFAscreet'></span>

<h3>Description</h3>

<p>Prints a brief summary of an object of class <code>EFAscreet</code>. This function will display the scree plot
of the eigenvalues when called, providing a visual representation of the factor analysis results.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'EFAscreet'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.EFAscreet_+3A_x">x</code></td>
<td>
<p>An object of class <code>EFAscreet</code>, which contains the eigenvalues from the factor analysis.</p>
</td></tr>
<tr><td><code id="print.EFAscreet_+3A_...">...</code></td>
<td>
<p>Additional arguments to be passed to the plotting function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None. This function is used for side effects (printing).
</p>

<hr>
<h2 id='print.EFAvote'>Print Voting Method Results</h2><span id='topic+print.EFAvote'></span>

<h3>Description</h3>

<p>This function prints the number of factors suggested by the voting.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'EFAvote'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.EFAvote_+3A_x">x</code></td>
<td>
<p>An object of class <code>EFAvote</code>, representing the results to be printed.</p>
</td></tr>
<tr><td><code id="print.EFAvote_+3A_...">...</code></td>
<td>
<p>Additional arguments to be passed to the plotting function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None. This function is used for side effects (printing).
</p>


<h3>See Also</h3>

<p><a href="#topic+EFAvote">EFAvote</a>
</p>

<hr>
<h2 id='print.EKC'>Print Empirical Kaiser Criterion Results</h2><span id='topic+print.EKC'></span>

<h3>Description</h3>

<p>This function prints the number of factors suggested by the Empirical Kaiser Criterion (EKC).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'EKC'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.EKC_+3A_x">x</code></td>
<td>
<p>An object of class <code>EKC</code>, representing the results to be printed.</p>
</td></tr>
<tr><td><code id="print.EKC_+3A_...">...</code></td>
<td>
<p>Additional arguments to be passed to the plotting function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None. This function is used for side effects (printing).
</p>


<h3>See Also</h3>

<p><a href="#topic+EKC">EKC</a>
</p>

<hr>
<h2 id='print.FF'>Print Factor Forest (FF) Results</h2><span id='topic+print.FF'></span>

<h3>Description</h3>

<p>This function prints the number of factors suggested by the Factor Forest.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'FF'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.FF_+3A_x">x</code></td>
<td>
<p>An object of class <code>FF</code>, representing the results to be printed.</p>
</td></tr>
<tr><td><code id="print.FF_+3A_...">...</code></td>
<td>
<p>Additional arguments to be passed to the plotting function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None. This function is used for side effects (printing).
</p>


<h3>See Also</h3>

<p><a href="#topic+FF">FF</a>
</p>

<hr>
<h2 id='print.Hull'>Print Hull Method Results</h2><span id='topic+print.Hull'></span>

<h3>Description</h3>

<p>This function prints the number of factors suggested by the Hull method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'Hull'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.Hull_+3A_x">x</code></td>
<td>
<p>An object of class <code>Hull</code>, representing the results to be printed.</p>
</td></tr>
<tr><td><code id="print.Hull_+3A_...">...</code></td>
<td>
<p>Additional arguments to be passed to the print function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None. This function is used for side effects (printing).
</p>


<h3>See Also</h3>

<p><a href="#topic+Hull">Hull</a>
</p>

<hr>
<h2 id='print.KGC'>Print Kaiser-Guttman Criterion Results</h2><span id='topic+print.KGC'></span>

<h3>Description</h3>

<p>This function prints the number of factors suggested by the Kaiser-Guttman Criterion (KGC).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'KGC'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.KGC_+3A_x">x</code></td>
<td>
<p>An object of class <code>KGC</code>, representing the results to be printed.</p>
</td></tr>
<tr><td><code id="print.KGC_+3A_...">...</code></td>
<td>
<p>Additional arguments to be passed to the plotting function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None. This function is used for side effects (printing).
</p>


<h3>See Also</h3>

<p><a href="#topic+KGC">KGC</a>
</p>

<hr>
<h2 id='print.PA'>Print Parallel Analysis Method Results</h2><span id='topic+print.PA'></span>

<h3>Description</h3>

<p>This function prints the number of factors suggested by the Parallel Analysis (PA) method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'PA'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.PA_+3A_x">x</code></td>
<td>
<p>An object of class <code>PA</code>, representing the results to be printed.</p>
</td></tr>
<tr><td><code id="print.PA_+3A_...">...</code></td>
<td>
<p>Additional arguments to be passed to the plotting function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None. This function is used for side effects (printing).
</p>


<h3>See Also</h3>

<p><a href="#topic+PA">PA</a>
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
