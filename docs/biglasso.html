<!DOCTYPE html><html><head><title>Help for package biglasso</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {biglasso}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#biglasso-package'><p>Extending Lasso Model Fitting to Big Data</p></a></li>
<li><a href='#biglasso'><p>Fit lasso penalized regression path for big data</p></a></li>
<li><a href='#colon'><p>Gene expression data from colon-cancer patients</p></a></li>
<li><a href='#cv.biglasso'><p>Cross-validation for biglasso</p></a></li>
<li><a href='#loss.biglasso'><p>Internal biglasso functions</p></a></li>
<li><a href='#plot.biglasso'><p>Plot coefficients from a &quot;biglasso&quot; object</p></a></li>
<li><a href='#plot.cv.biglasso'><p>Plots the cross-validation curve from a &quot;cv.biglasso&quot; object</p></a></li>
<li><a href='#plot.mbiglasso'><p>Plot coefficients from a &quot;mbiglasso&quot; object</p></a></li>
<li><a href='#predict.biglasso'><p>Model predictions based on a fitted <code>biglasso</code> object</p></a></li>
<li><a href='#predict.cv.biglasso'><p>Model predictions based on a fitted <code>cv.biglasso</code> object</p></a></li>
<li><a href='#setupX'><p>Set up design matrix X by reading data from big data file</p></a></li>
<li><a href='#summary.cv.biglasso'><p>Summarizing inferences based on cross-validation</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Version:</td>
<td>1.5.2</td>
</tr>
<tr>
<td>Date:</td>
<td>2022-10-05</td>
</tr>
<tr>
<td>Title:</td>
<td>Extending Lasso Model Fitting to Big Data</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Patrick Breheny &lt;patrick-breheny@uiowa.edu&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Extend lasso and elastic-net model fitting for ultra
    high-dimensional, multi-gigabyte data sets that cannot be loaded into
    memory. Designed to be more memory- and computation-efficient than existing
    lasso-fitting packages like 'glmnet' and 'ncvreg', thus allowing
    the user to analyze big data analysis even on an ordinary laptop.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://yaohuizeng.github.io/biglasso/index.html">https://yaohuizeng.github.io/biglasso/index.html</a>,
<a href="https://github.com/YaohuiZeng/biglasso">https://github.com/YaohuiZeng/biglasso</a>,
<a href="https://arxiv.org/abs/1701.05936">https://arxiv.org/abs/1701.05936</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/YaohuiZeng/biglasso/issues">https://github.com/YaohuiZeng/biglasso/issues</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.2.0), bigmemory (&ge; 4.5.0), Matrix, ncvreg</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp (&ge; 0.12.1), methods</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo (&ge; 0.8.600), bigmemory, BH</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Suggests:</td>
<td>parallel, testthat, glmnet, survival, knitr, rmarkdown</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.1</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-10-05 21:42:37 UTC; pbreheny</td>
</tr>
<tr>
<td>Author:</td>
<td>Yaohui Zeng [aut],
  Chuyi Wang [aut],
  Patrick Breheny <a href="https://orcid.org/0000-0002-0650-1119"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-10-05 22:40:05 UTC</td>
</tr>
</table>
<hr>
<h2 id='biglasso-package'>Extending Lasso Model Fitting to Big Data</h2><span id='topic+biglasso-package'></span>

<h3>Description</h3>

<p>Extend lasso and elastic-net linear, logistic and cox regression models for
ultrahigh-dimensional, multi-gigabyte data sets that cannot be loaded into
available RAM. This package utilizes memory-mapped files to store the
massive data on the disk and only read those into memory whenever necessary
during model fitting. Moreover, some advanced feature screening rules are
proposed and implemented to accelerate the model fitting. As a result, this
package is much more memory- and computation-efficient and highly scalable
as compared to existing lasso-fitting packages such as
<a href="https://CRAN.R-project.org/package=glmnet">glmnet</a> and
<a href="https://CRAN.R-project.org/package=ncvreg">ncvreg</a>, thus allowing for
powerful big data analysis even with only an ordinary laptop.
</p>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;"> Package: </td><td style="text-align: left;"> biglasso</td>
</tr>
<tr>
 <td style="text-align: left;"> Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;"> Version: </td><td style="text-align: left;">
1.4-1</td>
</tr>
<tr>
 <td style="text-align: left;"> Date: </td><td style="text-align: left;"> 2021-01-29</td>
</tr>
<tr>
 <td style="text-align: left;"> License: </td><td style="text-align: left;"> GPL-3</td>
</tr>

</table>

<p>Penalized regression models, in particular the lasso, have been extensively
applied to analyzing high-dimensional data sets. However, due to the memory
limit, existing R packages are not capable of fitting lasso models for
ultrahigh-dimensional, multi-gigabyte data sets which have been increasingly
seen in many areas such as genetics, biomedical imaging, genome sequencing
and high-frequency finance.
</p>
<p>This package aims to fill the gap by extending lasso model fitting to Big
Data in R. Version &gt;= 1.2-3 represents a major redesign where the source
code is converted into C++ (previously in C), and new feature screening
rules, as well as OpenMP parallel computing, are implemented. Some key
features of <code>biglasso</code> are summarized as below: </p>
 <ol>
<li><p> it
utilizes memory-mapped files to store the massive data on the disk, only
loading data into memory when necessary during model fitting. Consequently,
it's able to seamlessly data-larger-than-RAM cases. </p>
</li>
<li><p> it is built upon
pathwise coordinate descent algorithm with warm start, active set cycling,
and feature screening strategies, which has been proven to be one of fastest
lasso solvers. </p>
</li>
<li><p> in incorporates our newly developed hybrid and adaptive
screening that outperform state-of-the-art screening rules such as
the sequential strong rule (SSR) and the sequential EDPP rule (SEDPP) with
additional 1.5x to 4x speedup. </p>
</li>
<li><p> the implementation is designed to be as
memory-efficient as possible by eliminating extra copies of the data created
by other R packages, making it at least 2x more memory-efficient than
<code>glmnet</code>. </p>
</li>
<li><p> the underlying computation is implemented in C++, and
parallel computing with OpenMP is also supported. </p>
</li></ol>

<p><strong>For more information:</strong> </p>
 <ul>
<li><p> Benchmarking results:
<a href="https://github.com/YaohuiZeng/biglasso">https://github.com/YaohuiZeng/biglasso</a>.
</p>
</li>
<li><p> Tutorial:
<a href="http://yaohuizeng.github.io/biglasso/articles/biglasso.html">http://yaohuizeng.github.io/biglasso/articles/biglasso.html</a>
</p>
</li>
<li><p> Technical paper:
<a href="https://arxiv.org/abs/1701.05936">https://arxiv.org/abs/1701.05936</a> </p>
</li></ul>



<h3>Note</h3>

<p>The input design matrix X must be a <code><a href="bigmemory.html#topic+big.matrix">big.matrix</a></code> object. 
This can be created by the function <code>as.big.matrix</code> in the R package 
<a href="https://CRAN.R-project.org//package=bigmemory">bigmemory</a>. 
If the data (design matrix) is very large (e.g. 10 GB) and stored in an external 
file, which is often the case for big data, X can be created by calling the
function <code><a href="#topic+setupX">setupX</a></code>.
<strong>In this case, there are several restrictions about the data file:</strong>
</p>
 <ol>
<li><p> the data file must be a well-formated ASCII-file, with
each row corresponding to an observation and each column a variable; </p>
</li>
<li>
<p>the data file must contain only one single type. Current version only
supports <code>double</code> type; </p>
</li>
<li><p> the data file must contain only numeric
variables. If there are categorical variables, the user needs to create
dummy variables for each categorical varable (by adding additional columns).</p>
</li></ol>

<p>Future versions will try to address these restrictions.
</p>
<p>Denote the number of observations and variables be, respectively, <code>n</code>
and <code>p</code>. It's worth noting that the package is more suitable for wide
data (ultrahigh-dimensional, <code>p &gt;&gt; n</code>) as compared to long data
(<code>n &gt;&gt; p</code>). This is because the model fitting algorithm takes advantage
of sparsity assumption of high-dimensional data. To just give the user some
ideas, below are some benchmarking results of the total computing time (in
seconds) for solving lasso-penalized linear regression along a sequence of
100 values of the tuning parameter. In all cases, assume 20 non-zero
coefficients equal +/- 2 in the true model. (Based on Version 1.2-3,
screening rule &quot;SSR-BEDPP&quot; is used)
</p>
 <ul>
<li><p> For wide data case (<code>p &gt; n</code>), <code>n = 1,000</code>:
</p>

<table>
<tr>
 <td style="text-align: center;"> <code>p</code> </td><td style="text-align: center;"> 1,000 </td><td style="text-align: center;"> 10,000 </td><td style="text-align: center;"> 100,000 </td><td style="text-align: center;"> 1,000,000
</td>
</tr>
<tr>
 <td style="text-align: center;"> Size of <code>X</code> </td><td style="text-align: center;"> 9.5 MB </td><td style="text-align: center;"> 95 MB </td><td style="text-align: center;"> 950 MB </td><td style="text-align: center;"> 9.5 GB </td>
</tr>
<tr>
 <td style="text-align: center;">
Elapsed time (s) </td><td style="text-align: center;"> 0.11 </td><td style="text-align: center;"> 0.83 </td><td style="text-align: center;"> 8.47 </td><td style="text-align: center;"> 85.50 </td>
</tr>
<tr>
 <td style="text-align: center;"> </td>
</tr>

</table>

</li></ul>



<h3>Author(s)</h3>

<p>Yaohui Zeng, Chuyi Wang and Patrick Breheny
</p>
<p>Maintainer: Yaohui Zeng &lt;yaohui.zeng@gmail.com&gt; and Chuyi Wang &lt;wwaa0208@gmail.com&gt;
</p>


<h3>References</h3>

 <ul>
<li><p> Zeng, Y., and Breheny, P. (2017). The biglasso
Package: A Memory- and Computation-Efficient Solver for Lasso Model Fitting
with Big Data in R. <a href="https://arxiv.org/abs/1701.05936">https://arxiv.org/abs/1701.05936</a>.  </p>
</li>
<li>
<p>Tibshirani, R., Bien, J., Friedman, J., Hastie, T., Simon, N., Taylor, J.,
and Tibshirani, R. J. (2012). Strong rules for discarding predictors in
lasso-type problems. <em>Journal of the Royal Statistical Society: Series
B (Statistical Methodology)</em>, <strong>74</strong>(2), 245-266.  </p>
</li>
<li><p> Wang, J.,
Zhou, J., Wonka, P., and Ye, J. (2013). Lasso screening rules via dual
polytope projection. <em>In Advances in Neural Information Processing
Systems</em>, pp. 1070-1078.  </p>
</li>
<li><p> Xiang, Z. J., and Ramadge, P. J. (2012).
Fast lasso screening tests based on correlations. <em>In Acoustics, Speech
and Signal Processing (ICASSP), 2012 IEEE International Conference on</em> (pp.
2137-2140). IEEE.  </p>
</li>
<li><p> Wang, J., Zhou, J., Liu, J., Wonka, P., and Ye, J.
(2014). A safe screening rule for sparse logistic regression. <em>In
Advances in Neural Information Processing Systems</em>, pp. 1053-1061.  </p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## Example of reading data from external big data file, fit lasso model, 
## and run cross validation in parallel

# simulated design matrix, 1000 observations, 500,000 variables, ~ 5GB
# there are 10 true variables with non-zero coefficient 2.
xfname &lt;- 'x_e3_5e5.txt' 
yfname &lt;- 'y_e3_5e5.txt' # response vector
time &lt;- system.time(
  X &lt;- setupX(xfname, sep = '\t') # create backing files (.bin, .desc)
)
print(time) # ~ 7 minutes; this is just one-time operation
dim(X)

# the big.matrix then can be retrieved by its descriptor file (.desc) in any new R session. 
rm(X)
xdesc &lt;- 'x_e3_5e5.desc' 
X &lt;- attach.big.matrix(xdesc)
dim(X)

y &lt;- as.matrix(read.table(yfname, header = F))
time.fit &lt;- system.time(
  fit &lt;- biglasso(X, y, family = 'gaussian', screen = 'Hybrid')
)
print(time.fit) # ~ 44 seconds for fitting a lasso model along the entire solution path

# cross validation in parallel
seed &lt;- 1234
time.cvfit &lt;- system.time(
  cvfit &lt;- cv.biglasso(X, y, family = 'gaussian', screen = 'Hybrid', 
                       seed = seed, ncores = 4, nfolds = 10)
)
print(time.cvfit) # ~ 3 minutes for 10-fold cross validation
plot(cvfit)
summary(cvfit)

## End(Not run)

</code></pre>

<hr>
<h2 id='biglasso'>Fit lasso penalized regression path for big data</h2><span id='topic+biglasso'></span>

<h3>Description</h3>

<p>Extend lasso model fitting to big data that cannot be loaded into memory.
Fit solution paths for linear, logistic or Cox regression models penalized by
lasso, ridge, or elastic-net over a grid of values for the regularization
parameter lambda.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>biglasso(
  X,
  y,
  row.idx = 1:nrow(X),
  penalty = c("lasso", "ridge", "enet"),
  family = c("gaussian", "binomial", "cox", "mgaussian"),
  alg.logistic = c("Newton", "MM"),
  screen = c("Adaptive", "SSR", "Hybrid", "None"),
  safe.thresh = 0,
  update.thresh = 1,
  ncores = 1,
  alpha = 1,
  lambda.min = ifelse(nrow(X) &gt; ncol(X), 0.001, 0.05),
  nlambda = 100,
  lambda.log.scale = TRUE,
  lambda,
  eps = 1e-07,
  max.iter = 1000,
  dfmax = ncol(X) + 1,
  penalty.factor = rep(1, ncol(X)),
  warn = TRUE,
  output.time = FALSE,
  return.time = TRUE,
  verbose = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="biglasso_+3A_x">X</code></td>
<td>
<p>The design matrix, without an intercept. It must be a
double type <code><a href="bigmemory.html#topic+big.matrix">big.matrix</a></code> object. The function
standardizes the data and includes an intercept internally by default during
the model fitting.</p>
</td></tr>
<tr><td><code id="biglasso_+3A_y">y</code></td>
<td>
<p>The response vector for <code>family="gaussian"</code> or <code>family="binomial"</code>.
For <code>family="cox"</code>, <code>y</code> should be a two-column matrix with columns
'time' and 'status'. The latter is a binary variable, with '1' indicating death,
and '0' indicating right censored. For <code>family="mgaussin"</code>, <code>y</code>
should be a n*m matrix where n is the sample size and m is the number of
responses.</p>
</td></tr>
<tr><td><code id="biglasso_+3A_row.idx">row.idx</code></td>
<td>
<p>The integer vector of row indices of <code>X</code> that used for
fitting the model. <code>1:nrow(X)</code> by default.</p>
</td></tr>
<tr><td><code id="biglasso_+3A_penalty">penalty</code></td>
<td>
<p>The penalty to be applied to the model. Either <code>"lasso"</code>
(the default), <code>"ridge"</code>, or <code>"enet"</code> (elastic net).</p>
</td></tr>
<tr><td><code id="biglasso_+3A_family">family</code></td>
<td>
<p>Either <code>"gaussian"</code>, <code>"binomial"</code>, <code>"cox"</code> or
<code>"mgaussian"</code> depending on the response.</p>
</td></tr>
<tr><td><code id="biglasso_+3A_alg.logistic">alg.logistic</code></td>
<td>
<p>The algorithm used in logistic regression. If &quot;Newton&quot;
then the exact hessian is used (default); if &quot;MM&quot; then a
majorization-minimization algorithm is used to set an upper-bound on the
hessian matrix. This can be faster, particularly in data-larger-than-RAM
case.</p>
</td></tr>
<tr><td><code id="biglasso_+3A_screen">screen</code></td>
<td>
<p>The feature screening rule used at each <code>lambda</code> that
discards features to speed up computation: <code>"SSR"</code> (default if
<code>penalty="ridge"</code> or <code>penalty="enet"</code> )is the sequential strong rule;
<code>"Hybrid"</code> is our newly proposed hybrid screening rules which combine the
strong rule with a safe rule. <code>"Adaptive"</code> (default for <code>penalty="lasso"</code>
without <code>penalty.factor</code>) is our newly proposed adaptive rules which
reuse screening reference for multiple lambda values. <strong>Note that:</strong>
(1) for linear regression with elastic net penalty, both <code>"SSR"</code> and
<code>"Hybrid"</code> are applicable since version 1.3-0;  (2) only <code>"SSR"</code> is
applicable to elastic-net-penalized logistic regression or cox regression;
(3) active set cycling strategy is incorporated with these screening rules.</p>
</td></tr>
<tr><td><code id="biglasso_+3A_safe.thresh">safe.thresh</code></td>
<td>
<p>the threshold value between 0 and 1 that controls when to
stop safe test. For example, 0.01 means to stop safe test at next lambda 
iteration if the number of features rejected by safe test at current lambda
iteration is not larger than 1% of the total number of features. So 1 means
to always turn off safe test, whereas 0 (default) means to turn off safe test
if the number of features rejected by safe test is 0 at current lambda.</p>
</td></tr>
<tr><td><code id="biglasso_+3A_update.thresh">update.thresh</code></td>
<td>
<p>the non negative threshold value that controls how often to
update the reference of safe rules for &quot;Adaptive&quot; methods. Smaller value means
updating more often.</p>
</td></tr>
<tr><td><code id="biglasso_+3A_ncores">ncores</code></td>
<td>
<p>The number of OpenMP threads used for parallel computing.</p>
</td></tr>
<tr><td><code id="biglasso_+3A_alpha">alpha</code></td>
<td>
<p>The elastic-net mixing parameter that controls the relative
contribution from the lasso (l1) and the ridge (l2) penalty. The penalty is
defined as </p>
<p style="text-align: center;"><code class="reqn"> \alpha||\beta||_1 + (1-\alpha)/2||\beta||_2^2.</code>
</p>

<p><code>alpha=1</code> is the lasso penalty, <code>alpha=0</code> the ridge penalty,
<code>alpha</code> in between 0 and 1 is the elastic-net (&quot;enet&quot;) penalty.</p>
</td></tr>
<tr><td><code id="biglasso_+3A_lambda.min">lambda.min</code></td>
<td>
<p>The smallest value for lambda, as a fraction of
lambda.max.  Default is .001 if the number of observations is larger than
the number of covariates and .05 otherwise.</p>
</td></tr>
<tr><td><code id="biglasso_+3A_nlambda">nlambda</code></td>
<td>
<p>The number of lambda values.  Default is 100.</p>
</td></tr>
<tr><td><code id="biglasso_+3A_lambda.log.scale">lambda.log.scale</code></td>
<td>
<p>Whether compute the grid values of lambda on log
scale (default) or linear scale.</p>
</td></tr>
<tr><td><code id="biglasso_+3A_lambda">lambda</code></td>
<td>
<p>A user-specified sequence of lambda values.  By default, a
sequence of values of length <code>nlambda</code> is computed, equally spaced on
the log scale.</p>
</td></tr>
<tr><td><code id="biglasso_+3A_eps">eps</code></td>
<td>
<p>Convergence threshold for inner coordinate descent.  The
algorithm iterates until the maximum change in the objective after any
coefficient update is less than <code>eps</code> times the null deviance. Default
value is <code>1e-7</code>.</p>
</td></tr>
<tr><td><code id="biglasso_+3A_max.iter">max.iter</code></td>
<td>
<p>Maximum number of iterations.  Default is 1000.</p>
</td></tr>
<tr><td><code id="biglasso_+3A_dfmax">dfmax</code></td>
<td>
<p>Upper bound for the number of nonzero coefficients.  Default is
no upper bound.  However, for large data sets, computational burden may be
heavy for models with a large number of nonzero coefficients.</p>
</td></tr>
<tr><td><code id="biglasso_+3A_penalty.factor">penalty.factor</code></td>
<td>
<p>A multiplicative factor for the penalty applied to
each coefficient. If supplied, <code>penalty.factor</code> must be a numeric
vector of length equal to the number of columns of <code>X</code>.  The purpose of
<code>penalty.factor</code> is to apply differential penalization if some
coefficients are thought to be more likely than others to be in the model.
Current package doesn't allow unpenalized coefficients. That
is<code>penalty.factor</code> cannot be 0. <code>penalty.factor</code> is only supported
for &quot;SSR&quot; screen.</p>
</td></tr>
<tr><td><code id="biglasso_+3A_warn">warn</code></td>
<td>
<p>Return warning messages for failures to converge and model
saturation?  Default is TRUE.</p>
</td></tr>
<tr><td><code id="biglasso_+3A_output.time">output.time</code></td>
<td>
<p>Whether to print out the start and end time of the model
fitting. Default is FALSE.</p>
</td></tr>
<tr><td><code id="biglasso_+3A_return.time">return.time</code></td>
<td>
<p>Whether to return the computing time of the model
fitting. Default is TRUE.</p>
</td></tr>
<tr><td><code id="biglasso_+3A_verbose">verbose</code></td>
<td>
<p>Whether to output the timing of each lambda iteration.
Default is FALSE.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The objective function for linear regression or multiple responses linear regression 
(<code>family = "gaussian"</code> or <code>family = "mgaussian"</code>) is
</p>
<p style="text-align: center;"><code class="reqn">\frac{1}{2n}\textrm{RSS} + \lambda*\textrm{penalty},</code>
</p>

<p>where for <code>family = "mgaussian"</code>), a group-lasso type penalty is applied.
For logistic regression
(<code>family = "binomial"</code>) it is </p>
<p style="text-align: center;"><code class="reqn">-\frac{1}{n} loglike +
\lambda*\textrm{penalty},</code>
</p>
<p>, for cox regression,
breslow approximation for ties is applied.
</p>
<p>Several advanced feature screening rules are implemented. For
lasso-penalized linear regression, all the options of <code>screen</code> are
applicable. Our proposal adaptive rule - <code>"Adaptive"</code> - achieves highest speedup
so it's the recommended one, especially for ultrahigh-dimensional large-scale
data sets. For cox regression and/or the elastic net penalty, only
<code>"SSR"</code> is applicable for now. More efficient rules are under development.
</p>


<h3>Value</h3>

<p>An object with S3 class <code>"biglasso"</code> for
<code>"gaussian", "binomial", "cox"</code> families, or an object with S3 class
<code>"mbiglasso"</code> for <code>"mgaussian"</code> family,  with following variables.
</p>
<table>
<tr><td><code>beta</code></td>
<td>
<p>The fitted matrix of coefficients, store in sparse matrix
representation. The number of rows is equal to the number of coefficients,
whereas the number of columns is equal to <code>nlambda</code>. For <code>"mgaussian"</code>
family with m responses, it is a list of m such matrices.</p>
</td></tr> 
<tr><td><code>iter</code></td>
<td>
<p>A vector of length <code>nlambda</code> containing the number of 
iterations until convergence at each value of <code>lambda</code>.</p>
</td></tr> 
<tr><td><code>lambda</code></td>
<td>
<p>The sequence of regularization parameter values in the path.</p>
</td></tr>
<tr><td><code>penalty</code></td>
<td>
<p>Same as above.</p>
</td></tr>
<tr><td><code>family</code></td>
<td>
<p>Same as above.</p>
</td></tr>
<tr><td><code>alpha</code></td>
<td>
<p>Same as above.</p>
</td></tr> 
<tr><td><code>loss</code></td>
<td>
<p>A vector containing either the residual sum of squares 
(for <code>"gaussian", "mgaussian"</code>) or negative log-likelihood
(for <code>"binomial", "cox"</code>) of the fitted model at each value of <code>lambda</code>.</p>
</td></tr>
<tr><td><code>penalty.factor</code></td>
<td>
<p>Same as above.</p>
</td></tr>
<tr><td><code>n</code></td>
<td>
<p>The number of observations used in the model fitting. It's equal to
<code>length(row.idx)</code>.</p>
</td></tr> 
<tr><td><code>center</code></td>
<td>
<p>The sample mean vector of the variables, i.e., column mean of
the sub-matrix of <code>X</code> used for model fitting.</p>
</td></tr> 
<tr><td><code>scale</code></td>
<td>
<p>The sample standard deviation of the variables, i.e., column
standard deviation of the sub-matrix of <code>X</code> used for model fitting.</p>
</td></tr> 
<tr><td><code>y</code></td>
<td>
<p>The response vector used in the model fitting. Depending on
<code>row.idx</code>, it could be a subset of the raw input of the response vector y.</p>
</td></tr>
<tr><td><code>screen</code></td>
<td>
<p>Same as above.</p>
</td></tr> 
<tr><td><code>col.idx</code></td>
<td>
<p>The indices of features that have 'scale' value greater than
1e-6. Features with 'scale' less than 1e-6 are removed from model fitting.</p>
</td></tr> 
<tr><td><code>rejections</code></td>
<td>
<p>The number of features rejected at each value of <code>lambda</code>.</p>
</td></tr>
<tr><td><code>safe_rejections</code></td>
<td>
<p>The number of features rejected by safe rules at each
value of <code>lambda</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Yaohui Zeng, Chuyi Wang and Patrick Breheny
</p>
<p>Maintainer: Yaohui Zeng &lt;yaohui.zeng@gmail.com&gt; and Chuyi Wang &lt;wwaa0208@gmail.com&gt;
</p>


<h3>See Also</h3>

<p><code><a href="#topic+biglasso-package">biglasso-package</a></code>, <code><a href="#topic+setupX">setupX</a></code>,
<code><a href="#topic+cv.biglasso">cv.biglasso</a></code>, <code><a href="#topic+plot.biglasso">plot.biglasso</a></code>,
<code><a href="ncvreg.html#topic+ncvreg">ncvreg</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Linear regression
data(colon)
X &lt;- colon$X
y &lt;- colon$y
X.bm &lt;- as.big.matrix(X)
# lasso, default
par(mfrow=c(1,2))
fit.lasso &lt;- biglasso(X.bm, y, family = 'gaussian')
plot(fit.lasso, log.l = TRUE, main = 'lasso')
# elastic net
fit.enet &lt;- biglasso(X.bm, y, penalty = 'enet', alpha = 0.5, family = 'gaussian')
plot(fit.enet, log.l = TRUE, main = 'elastic net, alpha = 0.5')

## Logistic regression
data(colon)
X &lt;- colon$X
y &lt;- colon$y
X.bm &lt;- as.big.matrix(X)
# lasso, default
par(mfrow = c(1, 2))
fit.bin.lasso &lt;- biglasso(X.bm, y, penalty = 'lasso', family = "binomial")
plot(fit.bin.lasso, log.l = TRUE, main = 'lasso')
# elastic net
fit.bin.enet &lt;- biglasso(X.bm, y, penalty = 'enet', alpha = 0.5, family = "binomial")
plot(fit.bin.enet, log.l = TRUE, main = 'elastic net, alpha = 0.5')

## Cox regression
set.seed(10101)
N &lt;- 1000; p &lt;- 30; nzc &lt;- p/3
X &lt;- matrix(rnorm(N * p), N, p)
beta &lt;- rnorm(nzc)
fx &lt;- X[, seq(nzc)] %*% beta/3
hx &lt;- exp(fx)
ty &lt;- rexp(N, hx)
tcens &lt;- rbinom(n = N, prob = 0.3, size = 1)  # censoring indicator
y &lt;- cbind(time = ty, status = 1 - tcens)  # y &lt;- Surv(ty, 1 - tcens) with library(survival)
X.bm &lt;- as.big.matrix(X)
fit &lt;- biglasso(X.bm, y, family = "cox")
plot(fit, main = "cox")

## Multiple responses linear regression
set.seed(10101)
n=300; p=300; m=5; s=10; b=1
x = matrix(rnorm(n * p), n, p)
beta = matrix(seq(from=-b,to=b,length.out=s*m),s,m)
y = x[,1:s] %*% beta + matrix(rnorm(n*m,0,1),n,m)
x.bm = as.big.matrix(x)
fit = biglasso(x.bm, y, family = "mgaussian")
plot(fit, main = "mgaussian")

</code></pre>

<hr>
<h2 id='colon'>Gene expression data from colon-cancer patients</h2><span id='topic+colon'></span>

<h3>Description</h3>

<p>The data file contains gene expression data of 62 samples (40 tumor samples,
22 normal samples) from colon-cancer patients analyzed with an Affymetrix
oligonucleotide Hum6000 array.
</p>


<h3>Format</h3>

<p>A list of 2 variables included in <code>colon</code>: </p>

<ul>
<li><p><code>X</code>: a 62-by-2000 matrix that records the gene expression data.
Used as design matrix.
</p>
</li>
<li><p><code>y</code>: a binary vector of length 62 recording the sample status: 1 =
tumor; 0 = normal. Used as response vector.
</p>
</li></ul>



<h3>Source</h3>

<p>The raw data can be found on Bioconductor:
<a href="https://bioconductor.org/packages/release/data/experiment/html/colonCA.html">https://bioconductor.org/packages/release/data/experiment/html/colonCA.html</a>.
</p>


<h3>References</h3>

 <ul>
<li><p> U. Alon et al. (1999): Broad patterns of gene
expression revealed by clustering analysis of tumor and normal colon tissue
probed by oligonucleotide arrays. <em>Proc. Natl. Acad. Sci. USA</em>
<strong>96</strong>, 6745-6750. <a href="https://www.pnas.org/doi/abs/10.1073/pnas.96.12.6745">https://www.pnas.org/doi/abs/10.1073/pnas.96.12.6745</a>.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>data(colon)
X &lt;- colon$X
y &lt;- colon$y
str(X)
dim(X)
X.bm &lt;- as.big.matrix(X, backingfile = "") # convert to big.matrix object
str(X.bm)
dim(X.bm)
</code></pre>

<hr>
<h2 id='cv.biglasso'>Cross-validation for biglasso</h2><span id='topic+cv.biglasso'></span>

<h3>Description</h3>

<p>Perform k-fold cross validation for penalized regression models over a grid
of values for the regularization parameter lambda.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cv.biglasso(
  X,
  y,
  row.idx = 1:nrow(X),
  family = c("gaussian", "binomial", "cox", "mgaussian"),
  eval.metric = c("default", "MAPE", "auc", "class"),
  ncores = parallel::detectCores(),
  ...,
  nfolds = 5,
  seed,
  cv.ind,
  trace = FALSE,
  grouped = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cv.biglasso_+3A_x">X</code></td>
<td>
<p>The design matrix, without an intercept, as in
<code><a href="#topic+biglasso">biglasso</a></code>.</p>
</td></tr>
<tr><td><code id="cv.biglasso_+3A_y">y</code></td>
<td>
<p>The response vector, as in <code>biglasso</code>.</p>
</td></tr>
<tr><td><code id="cv.biglasso_+3A_row.idx">row.idx</code></td>
<td>
<p>The integer vector of row indices of <code>X</code> that used for
fitting the model. as in <code>biglasso</code>.</p>
</td></tr>
<tr><td><code id="cv.biglasso_+3A_family">family</code></td>
<td>
<p>Either <code>"gaussian"</code>, <code>"binomial"</code>, <code>"cox"</code> or
<code>"mgaussian"</code> depending on the response. <code>"cox"</code> and <code>"mgaussian"</code>
are not supported yet.</p>
</td></tr>
<tr><td><code id="cv.biglasso_+3A_eval.metric">eval.metric</code></td>
<td>
<p>The evaluation metric for the cross-validated error and
for choosing optimal <code>lambda</code>. &quot;default&quot; for linear regression is MSE
(mean squared error), for logistic regression is binomial deviance.
&quot;MAPE&quot;, for linear regression only, is the Mean Absolute Percentage Error.
&quot;auc&quot;, for binary classification, is the area under the receiver operating
characteristic curve (ROC).
&quot;class&quot;, for binary classification, gives the misclassification error.</p>
</td></tr>
<tr><td><code id="cv.biglasso_+3A_ncores">ncores</code></td>
<td>
<p>The number of cores to use for parallel execution of the
cross-validation folds, run on a cluster created by the <code>parallel</code>
package. (This is also supplied to the <code>ncores</code> argument in
<code><a href="#topic+biglasso">biglasso</a></code>, which is the number of OpenMP threads, but only for
the first call of <code><a href="#topic+biglasso">biglasso</a></code> that is  run on the entire data. The
individual calls of <code><a href="#topic+biglasso">biglasso</a></code> for the CV folds are run without
the <code>ncores</code> argument.)</p>
</td></tr>
<tr><td><code id="cv.biglasso_+3A_...">...</code></td>
<td>
<p>Additional arguments to <code>biglasso</code>.</p>
</td></tr>
<tr><td><code id="cv.biglasso_+3A_nfolds">nfolds</code></td>
<td>
<p>The number of cross-validation folds.  Default is 5.</p>
</td></tr>
<tr><td><code id="cv.biglasso_+3A_seed">seed</code></td>
<td>
<p>The seed of the random number generator in order to obtain
reproducible results.</p>
</td></tr>
<tr><td><code id="cv.biglasso_+3A_cv.ind">cv.ind</code></td>
<td>
<p>Which fold each observation belongs to.  By default the
observations are randomly assigned by <code>cv.biglasso</code>.</p>
</td></tr>
<tr><td><code id="cv.biglasso_+3A_trace">trace</code></td>
<td>
<p>If set to TRUE, cv.biglasso will inform the user of its
progress by announcing the beginning of each CV fold.  Default is FALSE.</p>
</td></tr>
<tr><td><code id="cv.biglasso_+3A_grouped">grouped</code></td>
<td>
<p>Whether to calculate CV standard error (<code>cvse</code>) over
CV folds (<code>TRUE</code>), or over all cross-validated predictions. Ignored
when <code>eval.metric</code> is 'auc'.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function calls <code>biglasso</code> <code>nfolds</code> times, each time leaving
out 1/<code>nfolds</code> of the data.  The cross-validation error is based on the
residual sum of squares when <code>family="gaussian"</code> and the binomial
deviance when <code>family="binomial"</code>.<br /> <br /> The S3 class object
<code>cv.biglasso</code> inherits class <code><a href="ncvreg.html#topic+cv.ncvreg">cv.ncvreg</a></code>.  So S3
functions such as <code>"summary", "plot"</code> can be directly applied to the
<code>cv.biglasso</code> object.
</p>


<h3>Value</h3>

<p>An object with S3 class <code>"cv.biglasso"</code> which inherits from
class <code>"cv.ncvreg"</code>.  The following variables are contained in the
class (adopted from <code><a href="ncvreg.html#topic+cv.ncvreg">cv.ncvreg</a></code>).  </p>
<table>
<tr><td><code>cve</code></td>
<td>
<p>The error
for each value of <code>lambda</code>, averaged across the cross-validation
folds.</p>
</td></tr> <tr><td><code>cvse</code></td>
<td>
<p>The estimated standard error associated with each value
of for <code>cve</code>.</p>
</td></tr> <tr><td><code>lambda</code></td>
<td>
<p>The sequence of regularization parameter
values along which the cross-validation error was calculated.</p>
</td></tr>
<tr><td><code>fit</code></td>
<td>
<p>The fitted <code>biglasso</code> object for the whole data.</p>
</td></tr>
<tr><td><code>min</code></td>
<td>
<p>The index of <code>lambda</code> corresponding to <code>lambda.min</code>.</p>
</td></tr>
<tr><td><code>lambda.min</code></td>
<td>
<p>The value of <code>lambda</code> with the minimum
cross-validation error.</p>
</td></tr> <tr><td><code>lambda.1se</code></td>
<td>
<p>The largest value of <code>lambda</code>
for which the cross-validation error is at most one standard error larger
than the minimum cross-validation error.</p>
</td></tr> <tr><td><code>null.dev</code></td>
<td>
<p>The deviance for
the intercept-only model.</p>
</td></tr> <tr><td><code>pe</code></td>
<td>
<p>If <code>family="binomial"</code>, the
cross-validation prediction error for each value of <code>lambda</code>.</p>
</td></tr>
<tr><td><code>cv.ind</code></td>
<td>
<p>Same as above.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Yaohui Zeng and Patrick Breheny
</p>
<p>Maintainer: Yaohui Zeng &lt;yaohui.zeng@gmail.com&gt;
</p>


<h3>See Also</h3>

<p><code><a href="#topic+biglasso">biglasso</a></code>, <code><a href="#topic+plot.cv.biglasso">plot.cv.biglasso</a></code>,
<code><a href="#topic+summary.cv.biglasso">summary.cv.biglasso</a></code>, <code><a href="#topic+setupX">setupX</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## cv.biglasso
data(colon)
X &lt;- colon$X
y &lt;- colon$y
X.bm &lt;- as.big.matrix(X)

## logistic regression
cvfit &lt;- cv.biglasso(X.bm, y, family = 'binomial', seed = 1234, ncores = 2)
par(mfrow = c(2, 2))
plot(cvfit, type = 'all')
summary(cvfit)

## End(Not run)

</code></pre>

<hr>
<h2 id='loss.biglasso'>Internal biglasso functions</h2><span id='topic+loss.biglasso'></span>

<h3>Description</h3>

<p>Internal biglasso functions
</p>


<h3>Usage</h3>

<pre><code class='language-R'>loss.biglasso(y, yhat, family, eval.metric, grouped = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="loss.biglasso_+3A_y">y</code></td>
<td>
<p>The observed response vector.</p>
</td></tr>
<tr><td><code id="loss.biglasso_+3A_yhat">yhat</code></td>
<td>
<p>The predicted response vector.</p>
</td></tr>
<tr><td><code id="loss.biglasso_+3A_family">family</code></td>
<td>
<p>Either &quot;gaussian&quot; or &quot;binomial&quot;, depending on the response.</p>
</td></tr>
<tr><td><code id="loss.biglasso_+3A_eval.metric">eval.metric</code></td>
<td>
<p>The evaluation metric for the cross-validated error and
for choosing optimal <code>lambda</code>. &quot;default&quot; for linear regression is MSE
(mean squared error), for logistic regression is misclassification error.
&quot;MAPE&quot;, for linear regression only, is the Mean Absolute Percentage Error.
&quot;auc&quot;, for logistic regression, is the area under the receiver operating
characteristic curve (ROC).</p>
</td></tr>
<tr><td><code id="loss.biglasso_+3A_grouped">grouped</code></td>
<td>
<p>Whether to calculate loss for the entire CV fold
(<code>TRUE</code>), or for predictions individually. Must be <code>TRUE</code> when
<code>eval.metric</code> is 'auc'.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These are not intended for use by users.<code>loss.biglasso</code> calculates the
value of the loss function for the given predictions (used for cross-validation).
</p>


<h3>Author(s)</h3>

<p>Yaohui Zeng and Patrick Breheny
</p>
<p>Maintainer: Yaohui Zeng &lt;yaohui.zeng@gmail.com&gt;
</p>

<hr>
<h2 id='plot.biglasso'>Plot coefficients from a &quot;biglasso&quot; object</h2><span id='topic+plot.biglasso'></span>

<h3>Description</h3>

<p>Produce a plot of the coefficient paths for a fitted <code><a href="#topic+biglasso">biglasso</a></code>
object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'biglasso'
plot(x, alpha = 1, log.l = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.biglasso_+3A_x">x</code></td>
<td>
<p>Fitted <code>"biglasso"</code> model.</p>
</td></tr>
<tr><td><code id="plot.biglasso_+3A_alpha">alpha</code></td>
<td>
<p>Controls alpha-blending, helpful when the number of covariates
is large.  Default is alpha=1.</p>
</td></tr>
<tr><td><code id="plot.biglasso_+3A_log.l">log.l</code></td>
<td>
<p>Should horizontal axis be on the log scale?  Default is TRUE.</p>
</td></tr>
<tr><td><code id="plot.biglasso_+3A_...">...</code></td>
<td>
<p>Other graphical parameters to <code>plot</code></p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Yaohui Zeng and Patrick Breheny
</p>
<p>Maintainer: Yaohui Zeng &lt;yaohui.zeng@gmail.com&gt;
</p>


<h3>See Also</h3>

<p><code><a href="#topic+biglasso">biglasso</a></code>, <code><a href="#topic+cv.biglasso">cv.biglasso</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## See examples in "biglasso"

</code></pre>

<hr>
<h2 id='plot.cv.biglasso'>Plots the cross-validation curve from a &quot;cv.biglasso&quot; object</h2><span id='topic+plot.cv.biglasso'></span>

<h3>Description</h3>

<p>Plot the cross-validation curve from a <code><a href="#topic+cv.biglasso">cv.biglasso</a></code> object,
along with standard error bars.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'cv.biglasso'
plot(
  x,
  log.l = TRUE,
  type = c("cve", "rsq", "scale", "snr", "pred", "all"),
  selected = TRUE,
  vertical.line = TRUE,
  col = "red",
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.cv.biglasso_+3A_x">x</code></td>
<td>
<p>A <code>"cv.biglasso"</code> object.</p>
</td></tr>
<tr><td><code id="plot.cv.biglasso_+3A_log.l">log.l</code></td>
<td>
<p>Should horizontal axis be on the log scale?  Default is TRUE.</p>
</td></tr>
<tr><td><code id="plot.cv.biglasso_+3A_type">type</code></td>
<td>
<p>What to plot on the vertical axis.  <code>cve</code> plots the
cross-validation error (deviance); <code>rsq</code> plots an estimate of the
fraction of the deviance explained by the model (R-squared); <code>snr</code>
plots an estimate of the signal-to-noise ratio; <code>scale</code> plots, for
<code>family="gaussian"</code>, an estimate of the scale parameter (standard
deviation); <code>pred</code> plots, for <code>family="binomial"</code>, the estimated
prediction error; <code>all</code> produces all of the above.</p>
</td></tr>
<tr><td><code id="plot.cv.biglasso_+3A_selected">selected</code></td>
<td>
<p>If <code>TRUE</code> (the default), places an axis on top of the
plot denoting the number of variables in the model (i.e., that have a
nonzero regression coefficient) at that value of <code>lambda</code>.</p>
</td></tr>
<tr><td><code id="plot.cv.biglasso_+3A_vertical.line">vertical.line</code></td>
<td>
<p>If <code>TRUE</code> (the default), draws a vertical line at
the value where cross-validaton error is minimized.</p>
</td></tr>
<tr><td><code id="plot.cv.biglasso_+3A_col">col</code></td>
<td>
<p>Controls the color of the dots (CV estimates).</p>
</td></tr>
<tr><td><code id="plot.cv.biglasso_+3A_...">...</code></td>
<td>
<p>Other graphical parameters to <code>plot</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Error bars representing approximate 68% confidence intervals are plotted
along with the estimates at value of <code>lambda</code>.  For <code>rsq</code> and
<code>snr</code>, these confidence intervals are quite crude, especially near.
</p>


<h3>Author(s)</h3>

<p>Yaohui Zeng and Patrick Breheny
</p>
<p>Maintainer: Yaohui Zeng &lt;yaohui.zeng@gmail.com&gt;
</p>


<h3>See Also</h3>

<p><code><a href="#topic+biglasso">biglasso</a></code>, <code><a href="#topic+cv.biglasso">cv.biglasso</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## See examples in "cv.biglasso"

</code></pre>

<hr>
<h2 id='plot.mbiglasso'>Plot coefficients from a &quot;mbiglasso&quot; object</h2><span id='topic+plot.mbiglasso'></span>

<h3>Description</h3>

<p>Produce a plot of the coefficient paths for a fitted multiple responses
<code>mbiglasso</code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'mbiglasso'
plot(x, alpha = 1, log.l = TRUE, norm.beta = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.mbiglasso_+3A_x">x</code></td>
<td>
<p>Fitted <code>"mbiglasso"</code> model.</p>
</td></tr>
<tr><td><code id="plot.mbiglasso_+3A_alpha">alpha</code></td>
<td>
<p>Controls alpha-blending, helpful when the number of covariates
is large.  Default is alpha=1.</p>
</td></tr>
<tr><td><code id="plot.mbiglasso_+3A_log.l">log.l</code></td>
<td>
<p>Should horizontal axis be on the log scale?  Default is TRUE.</p>
</td></tr>
<tr><td><code id="plot.mbiglasso_+3A_norm.beta">norm.beta</code></td>
<td>
<p>Should the vertical axis be the l2 norm of coefficients for each variable?
Default is TRUE. If False, the vertical axis is the coefficients.</p>
</td></tr>
<tr><td><code id="plot.mbiglasso_+3A_...">...</code></td>
<td>
<p>Other graphical parameters to <code>plot</code></p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Chuyi Wang
</p>
<p>Maintainer: Chuyi Wang &lt;wwaa0208@gmail.com&gt;
</p>


<h3>See Also</h3>

<p><code><a href="#topic+biglasso">biglasso</a></code>,
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## See examples in "biglasso"

</code></pre>

<hr>
<h2 id='predict.biglasso'>Model predictions based on a fitted <code>biglasso</code> object</h2><span id='topic+predict.biglasso'></span><span id='topic+predict.mbiglasso'></span><span id='topic+coef.biglasso'></span><span id='topic+coef.mbiglasso'></span>

<h3>Description</h3>

<p>Extract predictions (fitted reponse, coefficients, etc.) from a 
fitted <code><a href="#topic+biglasso">biglasso</a></code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'biglasso'
predict(
  object,
  X,
  row.idx = 1:nrow(X),
  type = c("link", "response", "class", "coefficients", "vars", "nvars"),
  lambda,
  which = 1:length(object$lambda),
  ...
)

## S3 method for class 'mbiglasso'
predict(
  object,
  X,
  row.idx = 1:nrow(X),
  type = c("link", "response", "coefficients", "vars", "nvars"),
  lambda,
  which = 1:length(object$lambda),
  k = 1,
  ...
)

## S3 method for class 'biglasso'
coef(object, lambda, which = 1:length(object$lambda), drop = TRUE, ...)

## S3 method for class 'mbiglasso'
coef(object, lambda, which = 1:length(object$lambda), intercept = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.biglasso_+3A_object">object</code></td>
<td>
<p>A fitted <code>"biglasso"</code> model object.</p>
</td></tr>
<tr><td><code id="predict.biglasso_+3A_x">X</code></td>
<td>
<p>Matrix of values at which predictions are to be made. It must be a
<code><a href="bigmemory.html#topic+big.matrix">big.matrix</a></code> object. Not used for
<code>type="coefficients"</code>.</p>
</td></tr>
<tr><td><code id="predict.biglasso_+3A_row.idx">row.idx</code></td>
<td>
<p>Similar to that in <code><a href="#topic+biglasso">biglasso</a></code>, it's a
vector of the row indices of <code>X</code> that used for the prediction.
<code>1:nrow(X)</code> by default.</p>
</td></tr>
<tr><td><code id="predict.biglasso_+3A_type">type</code></td>
<td>
<p>Type of prediction: <code>"link"</code> returns the linear predictors;
<code>"response"</code> gives the fitted values; <code>"class"</code> returns the
binomial outcome with the highest probability; <code>"coefficients"</code> returns
the coefficients; <code>"vars"</code> returns a list containing the indices and
names of the nonzero variables at each value of <code>lambda</code>;
<code>"nvars"</code> returns the number of nonzero coefficients at each value of
<code>lambda</code>.</p>
</td></tr>
<tr><td><code id="predict.biglasso_+3A_lambda">lambda</code></td>
<td>
<p>Values of the regularization parameter <code>lambda</code> at which
predictions are requested.  Linear interpolation is used for values of
<code>lambda</code> not in the sequence of lambda values in the fitted models.</p>
</td></tr>
<tr><td><code id="predict.biglasso_+3A_which">which</code></td>
<td>
<p>Indices of the penalty parameter <code>lambda</code> at which
predictions are required.  By default, all indices are returned.  If
<code>lambda</code> is specified, this will override <code>which</code>.</p>
</td></tr>
<tr><td><code id="predict.biglasso_+3A_...">...</code></td>
<td>
<p>Not used.</p>
</td></tr>
<tr><td><code id="predict.biglasso_+3A_k">k</code></td>
<td>
<p>Index of the response to predict in multiple responses regression (
<code>family="mgaussian"</code>).</p>
</td></tr>
<tr><td><code id="predict.biglasso_+3A_drop">drop</code></td>
<td>
<p>If coefficients for a single value of <code>lambda</code> are to be
returned, reduce dimensions to a vector?  Setting <code>drop=FALSE</code> returns
a 1-column matrix.</p>
</td></tr>
<tr><td><code id="predict.biglasso_+3A_intercept">intercept</code></td>
<td>
<p>Whether the intercept should be included in the returned
coefficients. For <code>family="mgaussian"</code> only.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The object returned depends on <code>type</code>.
</p>


<h3>Author(s)</h3>

<p>Yaohui Zeng and Patrick Breheny
</p>
<p>Maintainer: Yaohui Zeng &lt;yaohui.zeng@gmail.com&gt;
</p>


<h3>See Also</h3>

<p><code><a href="#topic+biglasso">biglasso</a></code>, <code><a href="#topic+cv.biglasso">cv.biglasso</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Logistic regression
data(colon)
X &lt;- colon$X
y &lt;- colon$y
X.bm &lt;- as.big.matrix(X, backingfile = "")
fit &lt;- biglasso(X.bm, y, penalty = 'lasso', family = "binomial")
coef &lt;- coef(fit, lambda=0.05, drop = TRUE)
coef[which(coef != 0)]
predict(fit, X.bm, type="link", lambda=0.05)[1:10]
predict(fit, X.bm, type="response", lambda=0.05)[1:10]
predict(fit, X.bm, type="class", lambda=0.1)[1:10]
predict(fit, type="vars", lambda=c(0.05, 0.1))
predict(fit, type="nvars", lambda=c(0.05, 0.1))
</code></pre>

<hr>
<h2 id='predict.cv.biglasso'>Model predictions based on a fitted <code><a href="#topic+cv.biglasso">cv.biglasso</a></code> object</h2><span id='topic+predict.cv.biglasso'></span><span id='topic+coef.cv.biglasso'></span>

<h3>Description</h3>

<p>Extract predictions from a fitted <code><a href="#topic+cv.biglasso">cv.biglasso</a></code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'cv.biglasso'
predict(
  object,
  X,
  row.idx = 1:nrow(X),
  type = c("link", "response", "class", "coefficients", "vars", "nvars"),
  lambda = object$lambda.min,
  which = object$min,
  ...
)

## S3 method for class 'cv.biglasso'
coef(object, lambda = object$lambda.min, which = object$min, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.cv.biglasso_+3A_object">object</code></td>
<td>
<p>A fitted <code>"cv.biglasso"</code> model object.</p>
</td></tr>
<tr><td><code id="predict.cv.biglasso_+3A_x">X</code></td>
<td>
<p>Matrix of values at which predictions are to be made. It must be a
<code><a href="bigmemory.html#topic+big.matrix">big.matrix</a></code> object. Not used for
<code>type="coefficients"</code>.</p>
</td></tr>
<tr><td><code id="predict.cv.biglasso_+3A_row.idx">row.idx</code></td>
<td>
<p>Similar to that in <code><a href="#topic+biglasso">biglasso</a></code>, it's a
vector of the row indices of <code>X</code> that used for the prediction.
<code>1:nrow(X)</code> by default.</p>
</td></tr>
<tr><td><code id="predict.cv.biglasso_+3A_type">type</code></td>
<td>
<p>Type of prediction: <code>"link"</code> returns the linear predictors;
<code>"response"</code> gives the fitted values; <code>"class"</code> returns the
binomial outcome with the highest probability; <code>"coefficients"</code> returns
the coefficients; <code>"vars"</code> returns a list containing the indices and
names of the nonzero variables at each value of <code>lambda</code>;
<code>"nvars"</code> returns the number of nonzero coefficients at each value of
<code>lambda</code>.</p>
</td></tr>
<tr><td><code id="predict.cv.biglasso_+3A_lambda">lambda</code></td>
<td>
<p>Values of the regularization parameter <code>lambda</code> at which
predictions are requested.  The default value is the one corresponding to
the minimum cross-validation error. Accepted values are also the strings
&quot;lambda.min&quot; (<code>lambda</code> of minimum cross-validation error) and
&quot;lambda.1se&quot; (Largest value of <code>lambda</code> for which the cross-validation
error was at most one standard error larger than the minimum.).</p>
</td></tr>
<tr><td><code id="predict.cv.biglasso_+3A_which">which</code></td>
<td>
<p>Indices of the penalty parameter <code>lambda</code> at which
predictions are requested. The default value is the index of lambda
corresponding to lambda.min.  Note: this is overridden if <code>lambda</code> is
specified.</p>
</td></tr>
<tr><td><code id="predict.cv.biglasso_+3A_...">...</code></td>
<td>
<p>Not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The object returned depends on <code>type</code>.
</p>


<h3>Author(s)</h3>

<p>Yaohui Zeng and Patrick Breheny
</p>
<p>Maintainer: Yaohui Zeng &lt;yaohui.zeng@gmail.com&gt;
</p>


<h3>See Also</h3>

<p><code><a href="#topic+biglasso">biglasso</a></code>, <code><a href="#topic+cv.biglasso">cv.biglasso</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## predict.cv.biglasso
data(colon)
X &lt;- colon$X
y &lt;- colon$y
X.bm &lt;- as.big.matrix(X, backingfile = "")
fit &lt;- biglasso(X.bm, y, penalty = 'lasso', family = "binomial")
cvfit &lt;- cv.biglasso(X.bm, y, penalty = 'lasso', family = "binomial", seed = 1234, ncores = 2)
coef &lt;- coef(cvfit)
coef[which(coef != 0)]
predict(cvfit, X.bm, type = "response")
predict(cvfit, X.bm, type = "link")
predict(cvfit, X.bm, type = "class")
predict(cvfit, X.bm, lambda = "lambda.1se")

## End(Not run)
</code></pre>

<hr>
<h2 id='setupX'>Set up design matrix X by reading data from big data file</h2><span id='topic+setupX'></span>

<h3>Description</h3>

<p>Set up the design matrix X as a <code>big.matrix</code> object based on external
massive data file stored on disk that cannot be fullly loaded into memory.
The data file must be a well-formated ASCII-file, and contains only one
single type. Current version only supports <code>double</code> type. Other
restrictions about the data file are described in
<code><a href="#topic+biglasso-package">biglasso-package</a></code>. This function reads the massive data, and
creates a <code>big.matrix</code> object. By default, the resulting
<code>big.matrix</code> is file-backed, and can be shared across processors or
nodes of a cluster.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>setupX(
  filename,
  dir = getwd(),
  sep = ",",
  backingfile = paste0(unlist(strsplit(filename, split = "\\."))[1], ".bin"),
  descriptorfile = paste0(unlist(strsplit(filename, split = "\\."))[1], ".desc"),
  type = "double",
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="setupX_+3A_filename">filename</code></td>
<td>
<p>The name of the data file. For example, &quot;dat.txt&quot;.</p>
</td></tr>
<tr><td><code id="setupX_+3A_dir">dir</code></td>
<td>
<p>The directory used to store the binary and descriptor files
associated with the <code>big.matrix</code>. The default is current working
directory.</p>
</td></tr>
<tr><td><code id="setupX_+3A_sep">sep</code></td>
<td>
<p>The field separator character. For example, &quot;,&quot; for
comma-delimited files (the default); &quot;\t&quot; for tab-delimited files.</p>
</td></tr>
<tr><td><code id="setupX_+3A_backingfile">backingfile</code></td>
<td>
<p>The binary file associated with the file-backed
<code>big.matrix</code>. By default, its name is the same as <code>filename</code> with
the extension replaced by &quot;.bin&quot;.</p>
</td></tr>
<tr><td><code id="setupX_+3A_descriptorfile">descriptorfile</code></td>
<td>
<p>The descriptor file used for the description of the
file-backed <code>big.matrix</code>. By default, its name is the same as
<code>filename</code> with the extension replaced by &quot;.desc&quot;.</p>
</td></tr>
<tr><td><code id="setupX_+3A_type">type</code></td>
<td>
<p>The data type. Only &quot;double&quot; is supported for now.</p>
</td></tr>
<tr><td><code id="setupX_+3A_...">...</code></td>
<td>
<p>Additional arguments that can be passed into function
<code><a href="bigmemory.html#topic+read.big.matrix">read.big.matrix</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For a data set, this function needs to be called only one time to set up the
<code>big.matrix</code> object with two backing files (.bin, .desc) created in
current working directory. Once set up, the data can be &quot;loaded&quot; into any
(new) R session by calling <code>attach.big.matrix(discriptorfile)</code>.
</p>
<p>This function is a simple wrapper of
<code><a href="bigmemory.html#topic+read.big.matrix">read.big.matrix</a></code>. See
<code><a href="bigmemory.html#topic+read.big.matrix">read.big.matrix</a></code> and the package
<a href="https://CRAN.R-project.org/package=bigmemory">bigmemory</a> for more
details.
</p>


<h3>Value</h3>

<p>A <code>big.matrix</code> object corresponding to a file-backed
<code>big.matrix</code>. It's ready to be used as the design matrix <code>X</code> in
<code><a href="#topic+biglasso">biglasso</a></code> and <code><a href="#topic+cv.biglasso">cv.biglasso</a></code>.
</p>


<h3>Author(s)</h3>

<p>Yaohui Zeng and Patrick Breheny
</p>
<p>Maintainer: Yaohui Zeng &lt;yaohui.zeng@gmail.com&gt;
</p>


<h3>See Also</h3>

<p><code><a href="#topic+biglasso">biglasso</a></code>, <code><a href="ncvreg.html#topic+cv.ncvreg">cv.ncvreg</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## see the example in "biglasso-package"

</code></pre>

<hr>
<h2 id='summary.cv.biglasso'>Summarizing inferences based on cross-validation</h2><span id='topic+summary.cv.biglasso'></span><span id='topic+print.summary.cv.biglasso'></span>

<h3>Description</h3>

<p>Summary method for <code>cv.biglasso</code> objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'cv.biglasso'
summary(object, ...)

## S3 method for class 'summary.cv.biglasso'
print(x, digits, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.cv.biglasso_+3A_object">object</code></td>
<td>
<p>A <code>cv.biglasso</code> object.</p>
</td></tr>
<tr><td><code id="summary.cv.biglasso_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.</p>
</td></tr>
<tr><td><code id="summary.cv.biglasso_+3A_x">x</code></td>
<td>
<p>A <code>"summary.cv.biglasso"</code> object.</p>
</td></tr>
<tr><td><code id="summary.cv.biglasso_+3A_digits">digits</code></td>
<td>
<p>Number of digits past the decimal point to print out.  Can be
a vector specifying different display digits for each of the five
non-integer printed values.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>summary.cv.biglasso</code> produces an object with S3 class
<code>"summary.cv.biglasso"</code>.  The class has its own print method and contains
the following list elements: </p>
<table>
<tr><td><code>penalty</code></td>
<td>
<p>The penalty used by
<code>biglasso</code>.</p>
</td></tr> <tr><td><code>model</code></td>
<td>
<p>Either <code>"linear"</code> or <code>"logistic"</code>,
depending on the <code>family</code> option in <code>biglasso</code>.</p>
</td></tr> <tr><td><code>n</code></td>
<td>
<p>Number
of observations</p>
</td></tr> <tr><td><code>p</code></td>
<td>
<p>Number of regression coefficients (not including
the intercept).</p>
</td></tr> <tr><td><code>min</code></td>
<td>
<p>The index of <code>lambda</code> with the smallest
cross-validation error.</p>
</td></tr> <tr><td><code>lambda</code></td>
<td>
<p>The sequence of <code>lambda</code> values
used by <code>cv.biglasso</code>.</p>
</td></tr> <tr><td><code>cve</code></td>
<td>
<p>Cross-validation error (deviance).</p>
</td></tr>
<tr><td><code>r.squared</code></td>
<td>
<p>Proportion of variance explained by the model, as estimated
by cross-validation.</p>
</td></tr> <tr><td><code>snr</code></td>
<td>
<p>Signal to noise ratio, as estimated by
cross-validation.</p>
</td></tr> <tr><td><code>sigma</code></td>
<td>
<p>For linear regression models, the scale
parameter estimate.</p>
</td></tr> <tr><td><code>pe</code></td>
<td>
<p>For logistic regression models, the
prediction error (misclassification error).</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Yaohui Zeng and Patrick Breheny
</p>
<p>Maintainer: Yaohui Zeng &lt;yaohui.zeng@gmail.com&gt;
</p>


<h3>See Also</h3>

<p><code><a href="#topic+biglasso">biglasso</a></code>, <code><a href="#topic+cv.biglasso">cv.biglasso</a></code>,
<code><a href="#topic+plot.cv.biglasso">plot.cv.biglasso</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## See examples in "cv.biglasso" and "biglasso-package"
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
