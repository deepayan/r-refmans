<!DOCTYPE html><html lang="en-US"><head><title>Help for package VecDep</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {VecDep}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#betakernelestimator'><p>betakernelestimator</p></a></li>
<li><a href='#bwd1'><p>bwd1</p></a></li>
<li><a href='#bwd1asR0'><p>bwd1asR0</p></a></li>
<li><a href='#bwd1avar'><p>bwd1avar</p></a></li>
<li><a href='#bwd2'><p>bwd2</p></a></li>
<li><a href='#bwd2asR0'><p>bwd2asR0</p></a></li>
<li><a href='#bwd2avar'><p>bwd2avar</p></a></li>
<li><a href='#covgpenal'><p>covgpenal</p></a></li>
<li><a href='#createR0'><p>createR0</p></a></li>
<li><a href='#cvomega'><p>cvomega</p></a></li>
<li><a href='#ellcopest'><p>ellcopest</p></a></li>
<li><a href='#elldistrest'><p>elldistrest</p></a></li>
<li><a href='#elliptselect'><p>elliptselect</p></a></li>
<li><a href='#estphi'><p>estphi</p></a></li>
<li><a href='#estR'><p>estR</p></a></li>
<li><a href='#gethac'><p>gethac</p></a></li>
<li><a href='#grouplasso'><p>grouplasso</p></a></li>
<li><a href='#hamse'><p>hamse</p></a></li>
<li><a href='#Helhac'><p>Helhac</p></a></li>
<li><a href='#Helnormal'><p>Helnormal</p></a></li>
<li><a href='#Helnormalavar'><p>Helnormalavar</p></a></li>
<li><a href='#Icluster'><p>Icluster</p></a></li>
<li><a href='#install_tensorflow'><p>install_tensorflow</p></a></li>
<li><a href='#minormal'><p>minormal</p></a></li>
<li><a href='#minormalavar'><p>minormalavar</p></a></li>
<li><a href='#miStudent'><p>miStudent</p></a></li>
<li><a href='#mlehac'><p>mlehac</p></a></li>
<li><a href='#otsort'><p>otsort</p></a></li>
<li><a href='#phiellip'><p>phiellip</p></a></li>
<li><a href='#phihac'><p>phihac</p></a></li>
<li><a href='#phinp'><p>phinp</p></a></li>
<li><a href='#transformationestimator'><p>transformationestimator</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Measuring Copula-Based Dependence Between Random Vectors</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.3</td>
</tr>
<tr>
<td>Description:</td>
<td>Provides functions for estimation (parametric, semi-parametric and non-parametric)
             of copula-based dependence coefficients between a finite collection of random vectors,
             including phi-dependence measures and Bures-Wasserstein dependence measures. 
             An algorithm for agglomerative hierarchical variable clustering is also implemented.
             Following the articles De Keyser &amp; Gijbels (2024) &lt;<a href="https://doi.org/10.1016%2Fj.jmva.2024.105336">doi:10.1016/j.jmva.2024.105336</a>&gt;,
             De Keyser &amp; Gijbels (2024) &lt;<a href="https://doi.org/10.1016%2Fj.ijar.2023.109090">doi:10.1016/j.ijar.2023.109090</a>&gt;, and De Keyser &amp; Gijbels (2024)
             &lt;<a href="https://doi.org/10.48550%2FarXiv.2404.07141">doi:10.48550/arXiv.2404.07141</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.2</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/StevenDeKeyser98/VecDep">https://github.com/StevenDeKeyser98/VecDep</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/StevenDeKeyser98/VecDep/issues">https://github.com/StevenDeKeyser98/VecDep/issues</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.4.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>ElliptCopulas (&ge; 0.1.4.1), HAC (&ge; 1.1-1), hash (&ge; 2.2.6.3),
sets (&ge; 1.0-25), covglasso (&ge; 1.0.3), expm (&ge; 1.0-0), magic
(&ge; 1.6-1), pbapply (&ge; 1.7-2), Rmpfr (&ge; 0.9-5), reticulate
(&ge; 1.39.0), gtools (&ge; 3.9.5)</td>
</tr>
<tr>
<td>Suggests:</td>
<td>mvtnorm, ggplot2, extraDistr, fossil, dendextend, copula,
knitr, rmarkdown, testthat (&ge; 3.0.0)</td>
</tr>
<tr>
<td>Language:</td>
<td>en-US</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-11-13 20:10:33 UTC; u0149189</td>
</tr>
<tr>
<td>Author:</td>
<td>Steven De Keyser <a href="https://orcid.org/0000-0002-3469-8692"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre],
  Ir√®ne Gijbels <a href="https://orcid.org/0000-0002-4443-9803"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Steven De Keyser &lt;steven.dekeyser@kuleuven.be&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-11-14 13:50:19 UTC</td>
</tr>
</table>
<hr>
<h2 id='betakernelestimator'>betakernelestimator</h2><span id='topic+betakernelestimator'></span>

<h3>Description</h3>

<p>This function computes the non-parametric beta kernel copula density estimator.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>betakernelestimator(input, h, pseudos)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="betakernelestimator_+3A_input">input</code></td>
<td>
<p>The copula argument at which the density estimate is to be computed.</p>
</td></tr>
<tr><td><code id="betakernelestimator_+3A_h">h</code></td>
<td>
<p>The bandwidth to be used in the beta kernel.</p>
</td></tr>
<tr><td><code id="betakernelestimator_+3A_pseudos">pseudos</code></td>
<td>
<p>The (estimated) copula observations from a <code class="reqn">q</code>-dimensional random vector <code class="reqn">\mathbf{X}</code> (<code class="reqn">n \times q</code> matrix with observations in rows, variables in columns).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Given a <code class="reqn">q</code>-dimensional random vector <code class="reqn">\mathbf{X} = (\mathbf{X}_{1}, \dots, \mathbf{X}_{k})</code> with <code class="reqn">\mathbf{X}_{i} = (X_{i1}, \dots, X_{id_{i}})</code>,
and samples <code class="reqn">X_{ij}^{(1)}, \dots, X_{ij}^{(n)}</code> from <code class="reqn">X_{ij}</code> for <code class="reqn">i = 1, \dots, k</code> and <code class="reqn">j = 1, \dots, d_{i}</code>,
the beta kernel estimator for the copula density of <code class="reqn">\mathbf{X}</code> equals, at <code class="reqn">\mathbf{u} = (u_{11}, \dots, u_{kd_{k}}) \in \mathbb{R}^{q}</code>,
</p>
<p style="text-align: center;"><code class="reqn">\widehat{c}_{\text{B}}(\mathbf{u}) = \frac{1}{n} \sum_{\ell = 1}^{n} \prod_{i = 1}^{k} \prod_{j = 1}^{d_{i}} k_{\text{B}} \left (\widehat{U}_{ij}^{(\ell)},\frac{u_{ij}}{h_{n}} + 1, \frac{1-u_{ij}}{h_{n}} + 1 \right ),</code>
</p>

<p>where <code class="reqn">h_{n} &gt; 0</code> is a bandwidth parameter, <code class="reqn">\widehat{U}_{ij}^{(\ell)} = \widehat{F}_{ij} (X_{ij}^{(\ell)})</code> with </p>
<p style="text-align: center;"><code class="reqn">\widehat{F}_{ij}(x_{ij}) = \frac{1}{n+1} \sum_{\ell = 1}^{n} 1 \left (X_{ij}^{(\ell)} \leq x_{ij} \right )</code>
</p>
<p> the (rescaled) empirical cdf of <code class="reqn">X_{ij}</code>, and
</p>
<p style="text-align: center;"><code class="reqn">k_{\text{B}}(u,\alpha,\beta) = \frac{u^{\alpha - 1} (1-u)^{\beta-1}}{B(\alpha,\beta)},</code>
</p>
<p> with <code class="reqn">B</code> the beta function.
</p>


<h3>Value</h3>

<p>The beta kernel copula density estimator evaluated at the input.
</p>


<h3>References</h3>

<p>De Keyser, S. &amp; Gijbels, I. (2024).
Hierarchical variable clustering via copula-based divergence measures between random vectors.
International Journal of Approximate Reasoning 165:109090.
doi: https://doi.org/10.1016/j.ijar.2023.109090.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+transformationestimator">transformationestimator</a></code> for the computation of the Gaussian transformation kernel copula density estimator,
<code><a href="#topic+hamse">hamse</a></code> for local bandwidth selection for the beta kernel or Gaussian transformation kernel copula density estimator,
<code><a href="#topic+phinp">phinp</a></code> for fully non-parametric estimation of the <code class="reqn">\Phi</code>-dependence between <code class="reqn">k</code> random vectors.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>q = 3
n = 100

# Sample from multivariate normal distribution with identity covariance matrix
sample = mvtnorm::rmvnorm(n,rep(0,q),diag(3),method = "chol")

# Copula pseudo-observations
pseudos = matrix(0,n,q)
for(j in 1:q){pseudos[,j] = (n/(n+1)) * ecdf(sample[,j])(sample[,j])}

# Argument at which to estimate the density
input = rep(0.5,q)

# Local bandwidth selection
h = hamse(input,pseudos = pseudos,n = n,estimator = "beta",bw_method = 1)

# Beta kernel estimator
est_dens = betakernelestimator(input,h,pseudos)

# True density
true = copula::dCopula(input, copula::normalCopula(0, dim = q))

</code></pre>

<hr>
<h2 id='bwd1'>bwd1</h2><span id='topic+bwd1'></span>

<h3>Description</h3>

<p>Given a <code class="reqn">q</code>-dimensional random vector <code class="reqn">\mathbf{X} = (\mathbf{X}_{1},...,\mathbf{X}_{k})</code> with <code class="reqn">\mathbf{X}_{i}</code> a <code class="reqn">d_{i}</code>-dimensional random vector, i.e., <code class="reqn">q = d_{1} + ... + d_{k}</code>,
this function computes the correlation-based Bures-Wasserstein coefficient <code class="reqn">\mathcal{D}_{1}</code> between <code class="reqn">\mathbf{X}_{1},...,\mathbf{X}_{k}</code> given the entire correlation matrix <code class="reqn">\mathbf{R}</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bwd1(R, dim)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="bwd1_+3A_r">R</code></td>
<td>
<p>The correlation matrix of <code class="reqn">\mathbf{X}</code>.</p>
</td></tr>
<tr><td><code id="bwd1_+3A_dim">dim</code></td>
<td>
<p>The vector of dimensions <code class="reqn">(d_{1},...,d_{k})</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Given a correlation matrix </p>
<p style="text-align: center;"><code class="reqn">\mathbf{R} = \begin{pmatrix} \mathbf{R}_{11} &amp; \mathbf{R}_{12} &amp; \cdots &amp; \mathbf{R}_{1k} \\
                                                             \mathbf{R}_{12}^{\text{T}} &amp; \mathbf{R}_{22} &amp; \cdots &amp; \mathbf{R}_{2k} \\
                                                             \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
                                                             \mathbf{R}_{1k}^{\text{T}} &amp; \mathbf{R}_{2k}^{\text{T}} &amp; \cdots &amp; \mathbf{R}_{kk} \end{pmatrix},</code>
</p>

<p>the coefficient <code class="reqn">\mathcal{D}_{1}</code> equals </p>
<p style="text-align: center;"><code class="reqn">\mathcal{D}_{1}(\mathbf{R}) =
\frac{d_{W}^{2}(\mathbf{R},\mathbf{I}_{q}) - \sum_{i=1}^{k}d_{W}^{2}(\mathbf{R}_{ii},\mathbf{I}_{d_{i}})}{\text{sup}_{\mathbf{A} \in \Gamma(\mathbf{R}_{11}, \dots, \mathbf{R}_{kk})}d_{W}^{2}(\mathbf{A},\mathbf{I}_{q}) - \sum_{i=1}^{k}d_{W}^{2}(\mathbf{R}_{ii},\mathbf{I}_{d_{i}})},</code>
</p>

<p>where <code class="reqn">d_{W}</code> stands for the Bures-Wasserstein distance, <code class="reqn">\Gamma(\mathbf{R}_{11}, \dots, \mathbf{R}_{kk})</code> denotes the set of all correlation matrices
with diagonal blocks <code class="reqn">\mathbf{R}_{ii}</code> for <code class="reqn">i = 1, \dots, k</code>, and <code class="reqn">\mathbf{I}_{q}</code> is the identity matrix.
The underlying assumption is that the copula of <code class="reqn">\mathbf{X}</code> is Gaussian.
</p>


<h3>Value</h3>

<p>The first Bures-Wasserstein dependence coefficient <code class="reqn">\mathcal{D}_{1}</code> between <code class="reqn">\mathbf{X}_{1},...,\mathbf{X}_{k}</code>.
</p>


<h3>References</h3>

<p>De Keyser, S. &amp; Gijbels, I. (2024).
High-dimensional copula-based Wasserstein dependence.
doi: https://doi.org/10.48550/arXiv.2404.07141.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+bwd2">bwd2</a></code> for the computation of the second Bures-Wasserstein dependence coefficient <code class="reqn">\mathcal{D}_{2}</code>,
<code><a href="#topic+bwd1avar">bwd1avar</a></code> for the computation of the asymptotic variance of the plug-in estimator for <code class="reqn">\mathcal{D}_{1}</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>q = 10
dim = c(1,2,3,4)

# AR(1) correlation matrix with correlation 0.5
R = 0.5^(abs(matrix(1:q-1,nrow = q, ncol = q, byrow = TRUE) - (1:q-1)))

bwd1(R,dim)
</code></pre>

<hr>
<h2 id='bwd1asR0'>bwd1asR0</h2><span id='topic+bwd1asR0'></span>

<h3>Description</h3>

<p>Given a <code class="reqn">q</code>-dimensional random vector <code class="reqn">\mathbf{X} = (\mathbf{X}_{1},...,\mathbf{X}_{k})</code> with <code class="reqn">\mathbf{X}_{i}</code> a <code class="reqn">d_{i}</code>-dimensional random vector, i.e., <code class="reqn">q = d_{1} + ... + d_{k}</code>,
this function simulates a sample from the asymptotic distribution of the plug-in estimator for the correlation-based Bures-Wasserstein coefficient <code class="reqn">\mathcal{D}_{1}</code>
between <code class="reqn">\mathbf{X}_{1},...,\mathbf{X}_{k}</code> given that the entire correlation matrix <code class="reqn">\mathbf{R}</code> is equal to <code class="reqn">\mathbf{R}_{0}</code> (correlation matrix under independence of <code class="reqn">\mathbf{X}_{1},...,\mathbf{X}_{k}</code>).
The argument dim should be in ascending order.
This function requires importation of the python modules &quot;numpy&quot; and &quot;scipy&quot;.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bwd1asR0(R, dim, M)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="bwd1asR0_+3A_r">R</code></td>
<td>
<p>The correlation matrix of <code class="reqn">\mathbf{X}</code>.</p>
</td></tr>
<tr><td><code id="bwd1asR0_+3A_dim">dim</code></td>
<td>
<p>The vector of dimensions <code class="reqn">(d_{1},...,d_{k})</code>, in ascending order.</p>
</td></tr>
<tr><td><code id="bwd1asR0_+3A_m">M</code></td>
<td>
<p>The sample size.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A sample of size M is drawn from the asymptotic distribution of the plug-in estimator <code class="reqn">\mathcal{D}_{1}(\widehat{\mathbf{R}}_{n})</code> at <code class="reqn">\mathbf{R}_{0} = \text{diag}(\mathbf{R}_{11}, \dots, \mathbf{R}_{kk})</code>,
where <code class="reqn">\widehat{\mathbf{R}}_{n}</code> is the sample matrix of normal scores rank correlations.
The underlying assumption is that the copula of <code class="reqn">\mathbf{X}</code> is Gaussian.
</p>
<p>To create a Python virtual environment with &quot;numpy&quot; and &quot;scipy&quot;,
run:
</p>
<p>install_tensorflow()
</p>
<p>reticulate::use_virtualenv(&quot;r-tensorflow&quot;, required = FALSE)
</p>
<p>reticulate::py_install(&quot;numpy&quot;)
</p>
<p>reticulate::py_install(&quot;scipy&quot;)
</p>


<h3>Value</h3>

<p>A sample of size M from the asymptotic distribution of the plug-in estimator for the first Bures-Wasserstein dependence coefficient <code class="reqn">\mathcal{D}_{1}</code> under independence of <code class="reqn">\mathbf{X}_{1},...,\mathbf{X}_{k}</code>.
</p>


<h3>References</h3>

<p>De Keyser, S. &amp; Gijbels, I. (2024).
High-dimensional copula-based Wasserstein dependence.
doi: https://doi.org/10.48550/arXiv.2404.07141.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+bwd1">bwd1</a></code> for the computation of the first Bures-Wasserstein dependence coefficient <code class="reqn">\mathcal{D}_{1}</code>,
<code><a href="#topic+bwd2">bwd2</a></code> for the computation of the second Bures-Wasserstein dependence coefficient <code class="reqn">\mathcal{D}_{2}</code>,
<code><a href="#topic+bwd1avar">bwd1avar</a></code> for the computation of the asymptotic variance of the plug-in estimator for <code class="reqn">\mathcal{D}_{1}</code>,
<code><a href="#topic+bwd2avar">bwd2avar</a></code> for the computation of the asymptotic variance of the plug-in estimator for <code class="reqn">\mathcal{D}_{2}</code>,
<code><a href="#topic+bwd2asR0">bwd2asR0</a></code> for sampling from the asymptotic distribution of the plug-in estimator for <code class="reqn">\mathcal{D}_{2}</code> under the hypothesis of independence between <code class="reqn">\mathbf{X}_{1},\dots,\mathbf{X}_{k}</code>,
<code><a href="#topic+estR">estR</a></code> for the computation of the sample matrix of normal scores rank correlations,
<code><a href="#topic+otsort">otsort</a></code> for rearranging the columns of sample such that dim is in ascending order.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
q = 5
dim = c(2,3)

# AR(1) correlation matrix with correlation 0.5
R = 0.5^(abs(matrix(1:q-1,nrow = q, ncol = q, byrow = TRUE) - (1:q-1)))

R0 = createR0(R,dim)

# Check whether scipy module is available (see details)
have_scipy = reticulate::py_module_available("scipy")

if(have_scipy){

sample = bwd1asR0(R0,dim,1000)

}

</code></pre>

<hr>
<h2 id='bwd1avar'>bwd1avar</h2><span id='topic+bwd1avar'></span>

<h3>Description</h3>

<p>Given a <code class="reqn">q</code>-dimensional random vector <code class="reqn">\mathbf{X} = (\mathbf{X}_{1},...,\mathbf{X}_{k})</code> with <code class="reqn">\mathbf{X}_{i}</code> a <code class="reqn">d_{i}</code>-dimensional random vector, i.e., <code class="reqn">q = d_{1} + ... + d_{k}</code>,
this function computes the asymptotic variance of the plug-in estimator for the correlation-based Bures-Wasserstein coefficient <code class="reqn">\mathcal{D}_{1}</code>
between <code class="reqn">\mathbf{X}_{1},...,\mathbf{X}_{k}</code> given the entire correlation matrix <code class="reqn">\mathbf{R}</code>.
The argument dim should be in ascending order.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bwd1avar(R, dim)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="bwd1avar_+3A_r">R</code></td>
<td>
<p>The correlation matrix of <code class="reqn">\mathbf{X}</code>.</p>
</td></tr>
<tr><td><code id="bwd1avar_+3A_dim">dim</code></td>
<td>
<p>The vector of dimensions <code class="reqn">(d_{1},...,d_{k})</code>, in ascending order.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The asymptotic variance of the plug-in estimator <code class="reqn">\mathcal{D}_{1}(\widehat{\mathbf{R}}_{n})</code> is computed at <code class="reqn">\mathbf{R}</code>,
where <code class="reqn">\widehat{\mathbf{R}}_{n}</code> is the sample matrix of normal scores rank correlations.
The underlying assumption is that the copula of <code class="reqn">\mathbf{X}</code> is Gaussian.
</p>


<h3>Value</h3>

<p>The asymptotic variance of the plug-in estimator for the first Bures-Wasserstein dependence coefficient <code class="reqn">\mathcal{D}_{1}</code> between <code class="reqn">\mathbf{X}_{1},...,\mathbf{X}_{k}</code>.
</p>


<h3>References</h3>

<p>De Keyser, S. &amp; Gijbels, I. (2024).
High-dimensional copula-based Wasserstein dependence.
doi: https://doi.org/10.48550/arXiv.2404.07141.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+bwd1">bwd1</a></code> for the computation of the first Bures-Wasserstein dependence coefficient <code class="reqn">\mathcal{D}_{1}</code>,
<code><a href="#topic+bwd2">bwd2</a></code> for the computation of the second Bures-Wasserstein dependence coefficient <code class="reqn">\mathcal{D}_{2}</code>,
<code><a href="#topic+bwd2avar">bwd2avar</a></code> for the computation of the asymptotic variance of the plug-in estimator for <code class="reqn">\mathcal{D}_{2}</code>,
<code><a href="#topic+bwd1asR0">bwd1asR0</a></code> for sampling from the asymptotic distribution of the plug-in estimator for <code class="reqn">\mathcal{D}_{1}</code> under the hypothesis of independence between <code class="reqn">\mathbf{X}_{1},\dots,\mathbf{X}_{k}</code>,
<code><a href="#topic+bwd2asR0">bwd2asR0</a></code> for sampling from the asymptotic distribution of the plug-in estimator for <code class="reqn">\mathcal{D}_{2}</code> under the hypothesis of independence between <code class="reqn">\mathbf{X}_{1},\dots,\mathbf{X}_{k}</code>,
<code><a href="#topic+estR">estR</a></code> for the computation of the sample matrix of normal scores rank correlations,
<code><a href="#topic+otsort">otsort</a></code> for rearranging the columns of sample such that dim is in ascending order.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>q = 10
dim = c(1,2,3,4)

# AR(1) correlation matrix with correlation 0.5
R = 0.5^(abs(matrix(1:q-1,nrow = q, ncol = q, byrow = TRUE) - (1:q-1)))

bwd1avar(R,dim)
</code></pre>

<hr>
<h2 id='bwd2'>bwd2</h2><span id='topic+bwd2'></span>

<h3>Description</h3>

<p>Given a <code class="reqn">q</code>-dimensional random vector <code class="reqn">\mathbf{X} = (\mathbf{X}_{1},...,\mathbf{X}_{k})</code> with <code class="reqn">\mathbf{X}_{i}</code> a <code class="reqn">d_{i}</code>-dimensional random vector, i.e., <code class="reqn">q = d_{1} + ... + d_{k}</code>,
this function computes the correlation-based Bures-Wasserstein coefficient <code class="reqn">\mathcal{D}_{2}</code> between <code class="reqn">\mathbf{X}_{1},...,\mathbf{X}_{k}</code> given the entire correlation matrix <code class="reqn">\mathbf{R}</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bwd2(R, dim)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="bwd2_+3A_r">R</code></td>
<td>
<p>The correlation matrix of <code class="reqn">\mathbf{X}</code>.</p>
</td></tr>
<tr><td><code id="bwd2_+3A_dim">dim</code></td>
<td>
<p>The vector of dimensions <code class="reqn">(d_{1},...,d_{k})</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Given a correlation matrix </p>
<p style="text-align: center;"><code class="reqn">\mathbf{R} = \begin{pmatrix} \mathbf{R}_{11} &amp; \mathbf{R}_{12} &amp; \cdots &amp; \mathbf{R}_{1k} \\
                                                             \mathbf{R}_{12}^{\text{T}} &amp; \mathbf{R}_{22} &amp; \cdots &amp; \mathbf{R}_{2k} \\
                                                             \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
                                                             \mathbf{R}_{1k}^{\text{T}} &amp; \mathbf{R}_{2k}^{\text{T}} &amp; \cdots &amp; \mathbf{R}_{kk} \end{pmatrix},</code>
</p>

<p>the coefficient <code class="reqn">\mathcal{D}_{2}</code> equals </p>
<p style="text-align: center;"><code class="reqn">\mathcal{D}_{2}(\mathbf{R}) =
\frac{d_{W}^{2}(\mathbf{R},\mathbf{R}_{0})}{\sup_{\mathbf{A} \in \Gamma(\mathbf{R}_{11}, \dots, \mathbf{R}_{kk})} d_{W}^{2}(\mathbf{A},\mathbf{R}_{0})},</code>
</p>

<p>where <code class="reqn">d_{W}</code> stands for the Bures-Wasserstein distance, <code class="reqn">\Gamma(\mathbf{R}_{11}, \dots, \mathbf{R}_{kk})</code> denotes the set of all correlation matrices
with diagonal blocks <code class="reqn">\mathbf{R}_{ii}</code> for <code class="reqn">i = 1, \dots, k</code>, and the matrix <code class="reqn">\mathbf{R}_{0} = \text{diag}(\mathbf{R}_{11},\dots,\mathbf{R}_{kk})</code> is the correlation matrix under independence.
The underlying assumption is that the copula of <code class="reqn">\mathbf{X}</code> is Gaussian.
</p>


<h3>Value</h3>

<p>The second Bures-Wasserstein dependence coefficient <code class="reqn">\mathcal{D}_{2}</code> between <code class="reqn">\mathbf{X}_{1},...,\mathbf{X}_{k}</code>.
</p>


<h3>References</h3>

<p>De Keyser, S. &amp; Gijbels, I. (2024).
High-dimensional copula-based Wasserstein dependence.
doi: https://doi.org/10.48550/arXiv.2404.07141.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+bwd1">bwd1</a></code> for the computation of the first Bures-Wasserstein dependence coefficient <code class="reqn">\mathcal{D}_{1}</code>,
<code><a href="#topic+bwd2avar">bwd2avar</a></code> for the computation of the asymptotic variance of the plug-in estimator for <code class="reqn">\mathcal{D}_{2}</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>q = 10
dim = c(1,2,3,4)

# AR(1) correlation matrix with correlation 0.5
R = 0.5^(abs(matrix(1:q-1,nrow = q, ncol = q, byrow = TRUE) - (1:q-1)))

bwd2(R,dim)
</code></pre>

<hr>
<h2 id='bwd2asR0'>bwd2asR0</h2><span id='topic+bwd2asR0'></span>

<h3>Description</h3>

<p>Given a <code class="reqn">q</code>-dimensional random vector <code class="reqn">\mathbf{X} = (\mathbf{X}_{1},...,\mathbf{X}_{k})</code> with <code class="reqn">\mathbf{X}_{i}</code> a <code class="reqn">d_{i}</code>-dimensional random vector, i.e., <code class="reqn">q = d_{1} + ... + d_{k}</code>,
this function simulates a sample from the asymptotic distribution of the plug-in estimator for the correlation-based Bures-Wasserstein coefficient <code class="reqn">\mathcal{D}_{2}</code>
between <code class="reqn">\mathbf{X}_{1},...,\mathbf{X}_{k}</code> given that the entire correlation matrix <code class="reqn">\mathbf{R}</code> is equal to <code class="reqn">\mathbf{R}_{0}</code> (correlation matrix under independence of <code class="reqn">\mathbf{X}_{1},...,\mathbf{X}_{k}</code>).
The argument dim should be in ascending order.
This function requires importation of the python modules &quot;numpy&quot; and &quot;scipy&quot;.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bwd2asR0(R, dim, M)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="bwd2asR0_+3A_r">R</code></td>
<td>
<p>The correlation matrix of <code class="reqn">\mathbf{X}</code>.</p>
</td></tr>
<tr><td><code id="bwd2asR0_+3A_dim">dim</code></td>
<td>
<p>The vector of dimensions <code class="reqn">(d_{1},...,d_{k})</code>, in ascending order.</p>
</td></tr>
<tr><td><code id="bwd2asR0_+3A_m">M</code></td>
<td>
<p>The sample size.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A sample of size M is drawn from the asymptotic distribution of the plug-in estimator <code class="reqn">\mathcal{D}_{2}(\widehat{\mathbf{R}}_{n})</code> at <code class="reqn">\mathbf{R}_{0} = \text{diag}(\mathbf{R}_{11}, \dots, \mathbf{R}_{kk})</code>,
where <code class="reqn">\widehat{\mathbf{R}}_{n}</code> is the sample matrix of normal scores rank correlations.
The underlying assumption is that the copula of <code class="reqn">\mathbf{X}</code> is Gaussian.
</p>
<p>To create a Python virtual environment with &quot;numpy&quot; and &quot;scipy&quot;,
run:
</p>
<p>install_tensorflow()
</p>
<p>reticulate::use_virtualenv(&quot;r-tensorflow&quot;, required = FALSE)
</p>
<p>reticulate::py_install(&quot;numpy&quot;)
</p>
<p>reticulate::py_install(&quot;scipy&quot;)
</p>


<h3>Value</h3>

<p>A sample of size M from the asymptotic distribution of the plug-in estimator for the second Bures-Wasserstein dependence coefficient <code class="reqn">\mathcal{D}_{2}</code> under independence of <code class="reqn">\mathbf{X}_{1},...,\mathbf{X}_{k}</code>.
</p>


<h3>References</h3>

<p>De Keyser, S. &amp; Gijbels, I. (2024).
High-dimensional copula-based Wasserstein dependence.
doi: https://doi.org/10.48550/arXiv.2404.07141.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+bwd1">bwd1</a></code> for the computation of the first Bures-Wasserstein dependence coefficient <code class="reqn">\mathcal{D}_{1}</code>,
<code><a href="#topic+bwd2">bwd2</a></code> for the computation of the second Bures-Wasserstein dependence coefficient <code class="reqn">\mathcal{D}_{2}</code>,
<code><a href="#topic+bwd1avar">bwd1avar</a></code> for the computation of the asymptotic variance of the plug-in estimator for <code class="reqn">\mathcal{D}_{1}</code>,
<code><a href="#topic+bwd2avar">bwd2avar</a></code> for the computation of the asymptotic variance of the plug-in estimator for <code class="reqn">\mathcal{D}_{2}</code>,
<code><a href="#topic+bwd1asR0">bwd1asR0</a></code> for sampling from the asymptotic distribution of the plug-in estimator for <code class="reqn">\mathcal{D}_{1}</code> under the hypothesis of independence between <code class="reqn">\mathbf{X}_{1},\dots,\mathbf{X}_{k}</code>,
<code><a href="#topic+estR">estR</a></code> for the computation of the sample matrix of normal scores rank correlations,
<code><a href="#topic+otsort">otsort</a></code> for rearranging the columns of sample such that dim is in ascending order.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
q = 5
dim = c(2,3)

# AR(1) correlation matrix with correlation 0.5
R = 0.5^(abs(matrix(1:q-1,nrow = q, ncol = q, byrow = TRUE) - (1:q-1)))

R0 = createR0(R,dim)

# Check whether scipy module is available (see details)
have_scipy = reticulate::py_module_available("scipy")

if(have_scipy){

sample = bwd2asR0(R0,dim,1000)

}

</code></pre>

<hr>
<h2 id='bwd2avar'>bwd2avar</h2><span id='topic+bwd2avar'></span>

<h3>Description</h3>

<p>Given a <code class="reqn">q</code>-dimensional random vector <code class="reqn">\mathbf{X} = (\mathbf{X}_{1},...,\mathbf{X}_{k})</code> with <code class="reqn">\mathbf{X}_{i}</code> a <code class="reqn">d_{i}</code>-dimensional random vector, i.e., <code class="reqn">q = d_{1} + ... + d_{k}</code>,
this function computes the asymptotic variance of the plug-in estimator for the correlation-based Bures-Wasserstein coefficient <code class="reqn">\mathcal{D}_{2}</code>
between <code class="reqn">\mathbf{X}_{1},...,\mathbf{X}_{k}</code> given the entire correlation matrix <code class="reqn">\mathbf{R}</code>.
The argument dim should be in ascending order.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bwd2avar(R, dim)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="bwd2avar_+3A_r">R</code></td>
<td>
<p>The correlation matrix of <code class="reqn">\mathbf{X}</code>.</p>
</td></tr>
<tr><td><code id="bwd2avar_+3A_dim">dim</code></td>
<td>
<p>The vector of dimensions <code class="reqn">(d_{1},...,d_{k})</code>, in ascending order.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The asymptotic variance of the plug-in estimator <code class="reqn">\mathcal{D}_{2}(\widehat{\mathbf{R}}_{n})</code> is computed at <code class="reqn">\mathbf{R}</code>,
where <code class="reqn">\widehat{\mathbf{R}}_{n}</code> is the sample matrix of normal scores rank correlations.
The underlying assumption is that the copula of <code class="reqn">\mathbf{X}</code> is Gaussian.
</p>


<h3>Value</h3>

<p>The asymptotic variance of the plug-in estimator for the second Bures-Wasserstein dependence coefficient <code class="reqn">\mathcal{D}_{2}</code> between <code class="reqn">\mathbf{X}_{1},...,\mathbf{X}_{k}</code>.
</p>


<h3>References</h3>

<p>De Keyser, S. &amp; Gijbels, I. (2024).
High-dimensional copula-based Wasserstein dependence.
doi: https://doi.org/10.48550/arXiv.2404.07141.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+bwd1">bwd1</a></code> for the computation of the first Bures-Wasserstein dependence coefficient <code class="reqn">\mathcal{D}_{1}</code>,
<code><a href="#topic+bwd2">bwd2</a></code> for the computation of the second Bures-Wasserstein dependence coefficient <code class="reqn">\mathcal{D}_{2}</code>,
<code><a href="#topic+bwd1avar">bwd1avar</a></code> for the computation of the asymptotic variance of the plug-in estimator for <code class="reqn">\mathcal{D}_{1}</code>,
<code><a href="#topic+bwd1asR0">bwd1asR0</a></code> for sampling from the asymptotic distribution of the plug-in estimator for <code class="reqn">\mathcal{D}_{1}</code> under the hypothesis of independence between <code class="reqn">\mathbf{X}_{1},\dots,\mathbf{X}_{k}</code>,
<code><a href="#topic+bwd2asR0">bwd2asR0</a></code> for sampling from the asymptotic distribution of the plug-in estimator for <code class="reqn">\mathcal{D}_{2}</code> under the hypothesis of independence between <code class="reqn">\mathbf{X}_{1},\dots,\mathbf{X}_{k}</code>,
<code><a href="#topic+estR">estR</a></code> for the computation of the sample matrix of normal scores rank correlations,
<code><a href="#topic+otsort">otsort</a></code> for rearranging the columns of sample such that dim is in ascending order.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>q = 10
dim = c(1,2,3,4)

# AR(1) correlation matrix with correlation 0.5
R = 0.5^(abs(matrix(1:q-1,nrow = q, ncol = q, byrow = TRUE) - (1:q-1)))

bwd2avar(R,dim)
</code></pre>

<hr>
<h2 id='covgpenal'>covgpenal</h2><span id='topic+covgpenal'></span>

<h3>Description</h3>

<p>This function computes the empirical penalized Gaussian copula covariance matrix with the Gaussian log-likelihood
plus a lasso-type penalty as objective function.
Model selection is done by choosing omega such that BIC is maximal.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>covgpenal(
  S,
  n,
  omegas,
  derpenal = function(t, omega) {
     derSCAD(t, omega, 3.7)
 },
  nsteps = 1
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="covgpenal_+3A_s">S</code></td>
<td>
<p>The sample matrix of normal scores covariances.</p>
</td></tr>
<tr><td><code id="covgpenal_+3A_n">n</code></td>
<td>
<p>The sample size.</p>
</td></tr>
<tr><td><code id="covgpenal_+3A_omegas">omegas</code></td>
<td>
<p>The candidate values for the tuning parameter in <code class="reqn">[0,\infty)</code>.</p>
</td></tr>
<tr><td><code id="covgpenal_+3A_derpenal">derpenal</code></td>
<td>
<p>The derivative of the penalty function to be used (default = scad with parameter <code class="reqn">a = 3.7</code>).</p>
</td></tr>
<tr><td><code id="covgpenal_+3A_nsteps">nsteps</code></td>
<td>
<p>The number of weighted covariance graphical lasso iterations (default = 1).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The aim is to solve/compute </p>
<p style="text-align: center;"><code class="reqn">\widehat{\boldsymbol{\Sigma}}_{\text{LT},n} \in \text{arg min}_{\boldsymbol{\Sigma} &gt; 0} \left \{
\ln \left | \boldsymbol{\Sigma} \right | + \text{tr} \left (\boldsymbol{\Sigma}^{-1} \widehat{\boldsymbol{\Sigma}}_{n} \right ) + P_{\text{LT}}\left (\boldsymbol{\Sigma},\omega_{n} \right ) \right \},</code>
</p>

<p>where the penalty function <code class="reqn">P_{\text{LT}}</code> is of lasso-type:
</p>
<p style="text-align: center;"><code class="reqn">P_{\text{LT}} \left (\boldsymbol{\Sigma},\omega_{n} \right ) = \sum_{ij} p_{\omega_{n}} \left (\Delta_{ij} \left |\sigma_{ij} \right | \right ),</code>
</p>

<p>for a certain penalty function <code class="reqn">p_{\omega_{n}}</code> with penalty parameter <code class="reqn">\omega_{n}</code>, and <code class="reqn">\sigma_{ij}</code> the
<code class="reqn">(i,j)</code>'th entry of <code class="reqn">\boldsymbol{\Sigma}</code> with <code class="reqn">\Delta_{ij} = 1</code> if <code class="reqn">i \neq j</code> and <code class="reqn">\Delta_{ij} = 0</code> if <code class="reqn">i = j</code> (in order to not shrink the variances).
The matrix <code class="reqn">\widehat{\boldsymbol{\Sigma}}_{n}</code> is the matrix of sample normal scores covariances.
</p>
<p>In case <code class="reqn">p_{\omega_{n}}(t) = \omega_{n} t</code> is the lasso penalty, the implementation for the
(weighted) covariance graphical lasso is available in the R package &lsquo;covglasso&rsquo; (see the manual for further explanations). For general penalty functions,
we perform a local linear approximation to the penalty function and iteratively do (nsteps, default = 1) weighted covariance graphical lasso optimizations.
</p>
<p>The default for the penalty function is the scad (derpenal = derivative of scad penalty), i.e.,
</p>
<p style="text-align: center;"><code class="reqn">p_{\omega_{n},\text{scad}}^{\prime}(t) = \omega_{n} \left [1 \left (t \leq \omega_{n} \right ) + \frac{\max \left (a \omega_{n} - t,0 \right )}{\omega_{n} (a-1)} 1 \left (t &gt; \omega_{n} \right ) \right ],</code>
</p>

<p>with <code class="reqn">a = 3.7</code> by default.
</p>
<p>For tuning <code class="reqn">\omega_{n}</code>, we maximize (over a grid of candidate values) the BIC criterion
</p>
<p style="text-align: center;"><code class="reqn">\text{BIC} \left (\widehat{\boldsymbol{\Sigma}}_{\omega_{n}} \right ) = -n \left [\ln \left |\widehat{\boldsymbol{\Sigma}}_{\omega_{n}} \right | + \text{tr} \left (\widehat{\boldsymbol{\Sigma}}_{\omega_{n}}^{-1} \widehat{\boldsymbol{\Sigma}}_{n} \right ) \right ] - \ln(n) \text{df} \left (\widehat{\boldsymbol{\Sigma}}_{\omega_{n}} \right ),</code>
</p>

<p>where <code class="reqn">\widehat{\boldsymbol{\Sigma}}_{\omega_{n}}</code> is the estimated candidate covariance matrix using <code class="reqn">\omega_{n}</code>
and df (degrees of freedom) equals the number of non-zero entries in <code class="reqn">\widehat{\boldsymbol{\Sigma}}_{\omega_{n}}</code>, not taking the elements under the diagonal into account.
</p>


<h3>Value</h3>

<p>A list with elements &quot;est&quot; containing the (lasso-type) penalized matrix of sample normal scores rank correlations (output as provided by the function &ldquo;covglasso.R&rdquo;), and &quot;omega&quot; containing the optimal tuning parameter.
</p>


<h3>References</h3>

<p>De Keyser, S. &amp; Gijbels, I. (2024).
High-dimensional copula-based Wasserstein dependence.
doi: https://doi.org/10.48550/arXiv.2404.07141.
</p>
<p>Fop, M. (2021).
covglasso: sparse covariance matrix estimation, R package version 1.0.3.
url: https://CRAN.R-project.org/package=covglasso.
</p>
<p>Wang, H. (2014).
Coordinate descent algorithm for covariance graphical lasso.
Statistics and Computing 24:521-529.
doi: https://doi.org/10.1007/s11222-013-9385-5.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+grouplasso">grouplasso</a></code> for group lasso estimation of the normal scores rank correlation matrix.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>q = 10
dim = c(5,5)
n = 100

# AR(1) correlation matrix with correlation 0.5
R = 0.5^(abs(matrix(1:q-1,nrow = q, ncol = q, byrow = TRUE) - (1:q-1)))

# Sparsity on off-diagonal blocks
R0 = createR0(R,dim)

# Sample from multivariate normal distribution
sample = mvtnorm::rmvnorm(n,rep(0,q),R0,method = "chol")

# Normal scores
scores = matrix(0,n,q)
for(j in 1:q){scores[,j] = qnorm((n/(n+1)) * ecdf(sample[,j])(sample[,j]))}

# Sample matrix of normal scores covariances
Sigma_est = cov(scores) * ((n-1)/n)

# Candidate tuning parameters
omega = seq(0.01, 0.6, length = 50)

Sigma_est_penal = covgpenal(Sigma_est,n,omega)

</code></pre>

<hr>
<h2 id='createR0'>createR0</h2><span id='topic+createR0'></span>

<h3>Description</h3>

<p>Given a <code class="reqn">q</code>-dimensional random vector <code class="reqn">\mathbf{X} = (\mathbf{X}_{1},...,\mathbf{X}_{k})</code> with <code class="reqn">\mathbf{X}_{i}</code> a <code class="reqn">d_{i}</code>-dimensional random vector, i.e., <code class="reqn">q = d_{1} + ... + d_{k}</code>,
this function constructs the correlation matrix under independence of <code class="reqn">\mathbf{X}_{1},...,\mathbf{X}_{k}</code>, given the entire correlation matrix <code class="reqn">\mathbf{R}</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>createR0(R, dim)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="createR0_+3A_r">R</code></td>
<td>
<p>The correlation matrix of <code class="reqn">\mathbf{X}</code>.</p>
</td></tr>
<tr><td><code id="createR0_+3A_dim">dim</code></td>
<td>
<p>The vector of dimensions <code class="reqn">(d_{1},...,d_{k})</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Given a correlation matrix </p>
<p style="text-align: center;"><code class="reqn">\mathbf{R} = \begin{pmatrix} \mathbf{R}_{11} &amp; \mathbf{R}_{12} &amp; \cdots &amp; \mathbf{R}_{1k} \\
                                                             \mathbf{R}_{12}^{\text{T}} &amp; \mathbf{R}_{22} &amp; \cdots &amp; \mathbf{R}_{2k} \\
                                                             \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
                                                             \mathbf{R}_{1k}^{\text{T}} &amp; \mathbf{R}_{2k}^{\text{T}} &amp; \cdots &amp; \mathbf{R}_{kk} \end{pmatrix},</code>
</p>

<p>the matrix <code class="reqn">\mathbf{R}_{0} = \text{diag}(\mathbf{R}_{11}, \dots, \mathbf{R}_{kk})</code>, being the correlation matrix
under independence of <code class="reqn">\mathbf{X}_{1}, \dots, \mathbf{X}_{k}</code>, is returned.
</p>


<h3>Value</h3>

<p>The correlation matrix under independence of <code class="reqn">\mathbf{X}_{1}, \dots, \mathbf{X}_{n}</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>q = 10
dim = c(1,2,3,4)

# AR(1) correlation matrix with correlation 0.5
R = 0.5^(abs(matrix(1:q-1,nrow = q, ncol = q, byrow = TRUE) - (1:q-1)))

createR0(R,dim)
</code></pre>

<hr>
<h2 id='cvomega'>cvomega</h2><span id='topic+cvomega'></span>

<h3>Description</h3>

<p>This functions selects the omega tuning parameter for ridge penalization of the empirical Gaussian copula correlation matrix via cross-validation.
The objective function is the Gaussian log-likelihood, and a grid search is performed using K folds.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cvomega(sample, omegas, K)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cvomega_+3A_sample">sample</code></td>
<td>
<p>A sample from a <code class="reqn">q</code>-dimensional random vector <code class="reqn">\mathbf{X}</code> (<code class="reqn">n \times q</code> matrix with observations in rows, variables in columns).</p>
</td></tr>
<tr><td><code id="cvomega_+3A_omegas">omegas</code></td>
<td>
<p>A grid of candidate penalty parameters in <code class="reqn">[0,1]</code>.</p>
</td></tr>
<tr><td><code id="cvomega_+3A_k">K</code></td>
<td>
<p>The number of folds to be used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The loss function is the Gaussian log-likelihood, i.e., given an estimated (penalized)
Gaussian copula correlation matrix (normal scores rank correlation matrix) <code class="reqn">\widehat{\mathbf{R}}_{n}^{(-j)}</code> computed on a training set leaving out fold j, and
<code class="reqn">\widehat{\mathbf{R}}_{n}^{(j)}</code> the empirical (non-penalized)
Gaussian copula correlation matrix computed on test fold j, we search for the tuning parameter that minimizes
</p>
<p style="text-align: center;"><code class="reqn">\sum_{j = 1}^{K} \left [\ln \left ( \left | \widehat{\mathbf{R}}_{n}^{(-j)} \right | \right ) + \text{tr} \left \{\widehat{\mathbf{R}}_{n}^{(j)} \left (\widehat{\mathbf{R}}_{n}^{(-j)} \right )^{-1} \right \} \right ].</code>
</p>

<p>The underlying assumption is that the copula of <code class="reqn">\mathbf{X}</code> is Gaussian.
</p>


<h3>Value</h3>

<p>The optimal ridge penalty parameter minimizing the cross-validation error.
</p>


<h3>References</h3>

<p>De Keyser, S. &amp; Gijbels, I. (2024).
High-dimensional copula-based Wasserstein dependence.
doi: https://doi.org/10.48550/arXiv.2404.07141.
</p>
<p>Warton, D.I. (2008).
Penalized normal likelihood and ridge regularization of correlation and covariance matrices.
Journal of the American Statistical Association 103(481):340-349. <br />
doi: https://doi.org/10.1198/016214508000000021.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+estR">estR</a></code> for computing the (Ridge penalized) empirical Gaussian copula correlation matrix.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>q = 10
n = 50

# AR(1) correlation matrix with correlation 0.5
R = 0.5^(abs(matrix(1:q-1,nrow = q, ncol = q, byrow = TRUE) - (1:q-1)))

# Sample from multivariate normal distribution
sample = mvtnorm::rmvnorm(n,rep(0,q),R,method = "chol")

# 5-fold cross-validation with Gaussian likelihood as loss for selecting omega
omega = cvomega(sample = sample,omegas = seq(0.01,0.999,len = 50),K = 5)

R_est = estR(sample,omega = omega)
</code></pre>

<hr>
<h2 id='ellcopest'>ellcopest</h2><span id='topic+ellcopest'></span>

<h3>Description</h3>

<p>This functions performs improved kernel density estimation of the generator of a meta-elliptical
copula by using Liebscher's algorithm, combined with a shrinkage function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ellcopest(
  dataU,
  Sigma_m1,
  h,
  grid,
  niter = 10,
  a,
  Kernel = "epanechnikov",
  shrink,
  verbose = 1,
  startPoint = "identity",
  prenormalization = FALSE,
  normalize = 1
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ellcopest_+3A_datau">dataU</code></td>
<td>
<p>The (estimated) copula observations from a <code class="reqn">q</code>-dimensional random vector <code class="reqn">\mathbf{X}</code> (<code class="reqn">n \times q</code> matrix with observations in rows, variables in columns).</p>
</td></tr>
<tr><td><code id="ellcopest_+3A_sigma_m1">Sigma_m1</code></td>
<td>
<p>The (estimated) inverse of the scale matrix of the meta-elliptical copula.</p>
</td></tr>
<tr><td><code id="ellcopest_+3A_h">h</code></td>
<td>
<p>The bandwidth of the kernel.</p>
</td></tr>
<tr><td><code id="ellcopest_+3A_grid">grid</code></td>
<td>
<p>The grid of values on which to estimate the density generator.</p>
</td></tr>
<tr><td><code id="ellcopest_+3A_niter">niter</code></td>
<td>
<p>The number of iterations used in the MECIP (default = 10).</p>
</td></tr>
<tr><td><code id="ellcopest_+3A_a">a</code></td>
<td>
<p>The tuning parameter to improve the performance at <code class="reqn">0</code>.</p>
</td></tr>
<tr><td><code id="ellcopest_+3A_kernel">Kernel</code></td>
<td>
<p>The kernel used for the smoothing (default = &quot;epanechnikov&quot;).</p>
</td></tr>
<tr><td><code id="ellcopest_+3A_shrink">shrink</code></td>
<td>
<p>The shrinkage function to further improve the performance at <code class="reqn">0</code> and guarantee the existence of the AMISE bandwidth.</p>
</td></tr>
<tr><td><code id="ellcopest_+3A_verbose">verbose</code></td>
<td>
<p>See the &ldquo;EllDistrEst.R&rdquo; function of the R package &lsquo;ElliptCopulas&rsquo;.</p>
</td></tr>
<tr><td><code id="ellcopest_+3A_startpoint">startPoint</code></td>
<td>
<p>See the &ldquo;EllDistrEst.R&rdquo; function of the R package &lsquo;ElliptCopulas&rsquo;.</p>
</td></tr>
<tr><td><code id="ellcopest_+3A_prenormalization">prenormalization</code></td>
<td>
<p>See the &ldquo;EllDistrEst.R&rdquo; function of the R package &lsquo;ElliptCopulas&rsquo;.</p>
</td></tr>
<tr><td><code id="ellcopest_+3A_normalize">normalize</code></td>
<td>
<p>A value in <code class="reqn">\{1,2\}</code> indicating the normalization procedure that is applied to the estimated generator (default = 1).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The context is the one of a <code class="reqn">q</code>-dimensional random vector <code class="reqn">\mathbf{X} = (\mathbf{X}_{1}, \dots, \mathbf{X}_{k})</code>,
</p>
<p>with <code class="reqn">\mathbf{X}_{i} = (X_{i1}, \dots, X_{id_{i}})</code> for <code class="reqn">i = 1, \dots, k</code>, having a meta-elliptical copula.
This means that there exists a generator <code class="reqn">g_{\mathcal{R}} : (0,\infty) \rightarrow \mathbb{R}</code> and a quantile function <code class="reqn">Q</code>, such that the random vector <code class="reqn">\mathbf{Z} = (\mathbf{Z}_{1}, \dots, \mathbf{Z}_{k})</code> with
</p>
<p style="text-align: center;"><code class="reqn">\mathbf{Z}_{i} = (Z_{i1}, \dots, Z_{id_{i}}) = \left(\left (Q \circ F_{i1} \right ) \left (X_{i1} \right ), \dots, \left (Q \circ F_{id_{i}} \right ) \left (X_{id_{i}} \right )  \right )</code>
</p>
<p> for <code class="reqn">i = 1, \dots, k</code>,
where <code class="reqn">F_{ij}</code> is the cdf of <code class="reqn">X_{ij}</code>, has a multivariate elliptical distribution.
Denoting <code class="reqn">\widehat{F}_{ij}(x_{ij}) = \frac{1}{n+1} \sum_{\ell = 1}^{n} 1 \left (X_{ij}^{(\ell)} \leq x_{ij} \right )</code> for the (rescaled) empirical cdf of <code class="reqn">X_{ij}</code> based on a sample <code class="reqn">X_{ij}^{(1)}, \dots, X_{ij}^{(n)}</code> for <code class="reqn">i = 1, \dots, k</code> and <code class="reqn">j = 1, \dots, d_{i}</code>,
and <code class="reqn">\widehat{\mathbf{R}}</code> for an estimator of the scale matrix <code class="reqn">\mathbf{R}</code>, this function estimates <code class="reqn">g_{\mathcal{R}}</code> by using the MECIP (Meta-Elliptical Copula Iterative Procedure) of Derumigny &amp; Fermanian (2022).
</p>
<p>This means that we start from an initial guess <code class="reqn">\widehat{g}_{\mathcal{R},0}</code> for the generator <code class="reqn">g_{\mathcal{R}}</code>, based on which we obtain an estimated
sample from <code class="reqn">\mathbf{Z}</code> through the quantile function corresponding to <code class="reqn">\widehat{g}_{\mathcal{R},0}</code>.
Based on this estimated sample, we then obtain an estimator <code class="reqn">\widehat{g}_{\mathcal{R},1}</code> using the function
<code><a href="#topic+elldistrest">elldistrest</a></code>, performing improved kernel estimation with shrinkage function.
This procedure is repeated for a certain amount (niter) of iterations to obtain a final estimate for <code class="reqn">g_{\mathcal{R}}</code>.
</p>
<p>The estimator without the shrinkage function <code class="reqn">\alpha</code> is implemented in the R package &lsquo;ElliptCopulas&rsquo;.
We use this implementation and bring in the shrinkage function.
</p>
<p>In order to make <code class="reqn">g_{\mathcal{R}}</code> identifiable, an extra normalization procedure is implemented
in line with an extra constraint on <code class="reqn">g_{\mathcal{R}}</code>.
When normalize = 1, this corresponds to <code class="reqn">\mathbf{R}</code> being the correlation matrix of <code class="reqn">\mathbf{Z}</code>.
When normalize = 2, this corresponds to the identifiability condition of Derumigny &amp; Fermanian (2022).
</p>


<h3>Value</h3>

<p>The estimates for <code class="reqn">g_{\mathcal{R}}</code> at the grid points.
</p>


<h3>References</h3>

<p>Derumigny, A., Fermanian, J.-D., Ryan, V., van der Spek, R. (2024).
ElliptCopulas, R package version 0.1.4.1.
url: https://CRAN.R-project.org/package=ElliptCopulas.
</p>
<p>Derumigny, A. &amp; Fermanian, J.-D. (2022).
Identifiability and estimation of meta-elliptical copula generators.
Journal of Multivariate Analysis 190:104962. <br />
doi: https://doi.org/10.1016/j.jmva.2022.104962.
</p>
<p>De Keyser, S. &amp; Gijbels, I. (2024).
Hierarchical variable clustering via copula-based divergence measures between random vectors.
International Journal of Approximate Reasoning 165:109090.
doi: https://doi.org/10.1016/j.ijar.2023.109090.
</p>
<p>Liebscher, E. (2005).
A semiparametric density estimator based on elliptical distributions.
Journal of Multivariate Analysis 92(1):205-225.
doi: https://doi.org/10.1016/j.jmva.2003.09.007.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+elldistrest">elldistrest</a></code> for improved kernel estimation of the elliptical generator of an elliptical distribution,
<code><a href="#topic+elliptselect">elliptselect</a></code> for selecting optimal tuning parameters for the improved kernel estimator of the elliptical generator,
<code><a href="#topic+phiellip">phiellip</a></code> for estimating the <code class="reqn">\Phi</code>-dependence between <code class="reqn">k</code> random vectors having a meta-elliptical copula.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
q = 4

# Sample size
n = 1000

# Grid on which to evaluate the elliptical generator
grid = seq(0.005,100,by = 0.005)

# Degrees of freedom
nu = 7

# Student-t generator with 7 degrees of freedom
g_q = ((nu/(nu-2))^(q/2))*(gamma((q+nu)/2)/(((pi*nu)^(q/2))*gamma(nu/2))) *
      ((1+(grid/(nu-2)))^(-(q+nu)/2))

# Density of squared radius
R2 = function(t,q){(gamma((q+nu)/2)/(((nu-2)^(q/2))*gamma(nu/2)*gamma(q/2))) *
                   (t^((q/2)-1)) * ((1+(t/(nu-2)))^(-(q+nu)/2))}

# Sample from 4-dimensional Student-t distribution with 7 degrees of freedom
# and identity covariance matrix
sample = ElliptCopulas::EllDistrSim(n,q,diag(q),density_R2 = function(t){R2(t,q)})

# Copula pseudo-observations
pseudos = matrix(0,n,q)
for(j in 1:q){pseudos[,j] = (n/(n+1)) * ecdf(sample[,j])(sample[,j])}

# Shrinkage function
shrinkage = function(t,p){1-(1/((t^p) + 1))}

# Tuning parameter selection
opt_parameters = elliptselect(n,q,seq((3/4)-(1/q)+0.01,1-0.01,len = 200),
                                  seq(0.01,2,len = 200))

# Optimal tuning parameters
a = opt_parameters$Opta ; p = opt_parameters$Optp ; h = opt_parameters$Opth

# Estimated elliptical generator
g_est = ellcopest(dataU = pseudos,Sigma_m1 = diag(q),h = h,grid = grid,a = a,
                  shrink = function(t){shrinkage(t,p)})

plot(grid,g_est,type = "l", xlim = c(0,8))
lines(grid,g_q,col = "green")


</code></pre>

<hr>
<h2 id='elldistrest'>elldistrest</h2><span id='topic+elldistrest'></span>

<h3>Description</h3>

<p>This functions performs improved kernel density estimation of the generator of an elliptical
distribution by using Liebscher's algorithm, combined with a shrinkage function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>elldistrest(
  Z,
  mu = 0,
  Sigma_m1,
  grid,
  h,
  Kernel = "epanechnikov",
  a,
  shrink,
  mpfr = FALSE,
  precBits = 100,
  dopb = FALSE,
  normalize = 1
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="elldistrest_+3A_z">Z</code></td>
<td>
<p>A sample from a <code class="reqn">q</code>-dimensional random vector <code class="reqn">\mathbf{Z}</code> (<code class="reqn">n \times q</code> matrix with observations in rows, variables in columns).</p>
</td></tr>
<tr><td><code id="elldistrest_+3A_mu">mu</code></td>
<td>
<p>The (estimated) mean of <code class="reqn">\mathbf{Z}</code> (default <code class="reqn">= 0</code>).</p>
</td></tr>
<tr><td><code id="elldistrest_+3A_sigma_m1">Sigma_m1</code></td>
<td>
<p>The (estimated) inverse of the scale matrix of <code class="reqn">\mathbf{Z}</code>.</p>
</td></tr>
<tr><td><code id="elldistrest_+3A_grid">grid</code></td>
<td>
<p>The grid of values on which to estimate the density generator.</p>
</td></tr>
<tr><td><code id="elldistrest_+3A_h">h</code></td>
<td>
<p>The bandwidth of the kernel.</p>
</td></tr>
<tr><td><code id="elldistrest_+3A_kernel">Kernel</code></td>
<td>
<p>The kernel used for the smoothing (default = &quot;epanechnikov&quot;).</p>
</td></tr>
<tr><td><code id="elldistrest_+3A_a">a</code></td>
<td>
<p>The tuning parameter to improve the performance at <code class="reqn">0</code>.</p>
</td></tr>
<tr><td><code id="elldistrest_+3A_shrink">shrink</code></td>
<td>
<p>The shrinkage function to further improve the performance at <code class="reqn">0</code> and guarantee the existence of the AMISE bandwidth.</p>
</td></tr>
<tr><td><code id="elldistrest_+3A_mpfr">mpfr</code></td>
<td>
<p>See the &ldquo;EllDistrEst.R&rdquo; function of the R package &lsquo;ElliptCopulas&rsquo;.</p>
</td></tr>
<tr><td><code id="elldistrest_+3A_precbits">precBits</code></td>
<td>
<p>See the &ldquo;EllDistrEst.R&rdquo; function of the R package &lsquo;ElliptCopulas&rsquo;.</p>
</td></tr>
<tr><td><code id="elldistrest_+3A_dopb">dopb</code></td>
<td>
<p>See the &ldquo;EllDistrEst.R&rdquo; function of the R package &lsquo;ElliptCopulas&rsquo;.</p>
</td></tr>
<tr><td><code id="elldistrest_+3A_normalize">normalize</code></td>
<td>
<p>A value in <code class="reqn">\{1,2\}</code> indicating the normalization procedure that is applied to the estimated generator (default = 1).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The context is the one of a <code class="reqn">q</code>-dimensional random vector <code class="reqn">\mathbf{Z}</code> following an elliptical distribution with
generator <code class="reqn">g_{\mathcal{R}} : (0,\infty) \rightarrow \mathbb{R}</code> and scale matrix <code class="reqn">\mathbf{R}</code> such that the density of <code class="reqn">\mathbf{Z}</code> is given by
</p>
<p style="text-align: center;"><code class="reqn">h(\mathbf{z}) = \left |\mathbf{R} \right |^{-1/2} g_{\mathcal{R}} \left (\mathbf{z}^{\text{T}} \mathbf{R}^{-1} \mathbf{z} \right ),</code>
</p>

<p>for <code class="reqn">\mathbf{z} \in \mathbb{R}^{q}</code>. Suppose that a sample <code class="reqn">\mathbf{Z}^{(1)}, \dots, \mathbf{Z}^{(n)}</code> from <code class="reqn">\mathbf{Z}</code> is given, and let
<code class="reqn">\widehat{\mathbf{R}}</code> be an estimator for the scale matrix <code class="reqn">\mathbf{R}</code>. Then, when defining
</p>
<p style="text-align: center;"><code class="reqn">\widehat{\mathbf{Y}}^{(\ell)} = \widehat{\mathbf{R}}^{-1/2} \mathbf{Z}^{(\ell)}</code>
</p>
<p> for <code class="reqn">\ell = 1, \dots, n</code>,
this function computes the estimator <code class="reqn">\widehat{g}_{\mathcal{R}}^{\text{I}}</code> for <code class="reqn">g_{\mathcal{R}}</code> given by
</p>
<p style="text-align: center;"><code class="reqn">\widehat{g}_{\mathcal{R}}^{\text{I}}(t) = c^{\text{I}}(t) \sum_{\ell = 1}^{n} \left \{k \left (\frac{\psi(t) - \psi \left (\left | \left |\widehat{\mathbf{Y}}^{(\ell)} \right | \right |^{2} \right )}{h_{n} \alpha \left (\psi(t) \right )} \right ) + k \left (\frac{\psi(t) + \psi \left (\left | \left |\widehat{\mathbf{Y}}^{(\ell)} \right | \right |^{2} \right )}{h_{n} \alpha \left (\psi(t) \right )} \right ) \right \},</code>
</p>

<p>where <code class="reqn">c^{\text{I}}(t) = [\Gamma(q/2)/(\pi^{q/2} n h_{n} \alpha(\psi(t)))] t^{-q/2 + 1} \psi^{\prime}(t)</code>, with <code class="reqn">k</code> the kernel and <code class="reqn">h_{n}</code> the bandwidth.
The function </p>
<p style="text-align: center;"><code class="reqn">\psi(t) = -a +  \left (a^{q/2} + t^{q/2} \right )^{2/q},</code>
</p>

<p>with <code class="reqn">a &gt; 0</code> a tuning parameter was introduced by Liebscher (2005), and the shrinkage function
<code class="reqn">\alpha(t)</code> yields further estimation improvement. We suggest to take (for <code class="reqn">q &gt; 2</code>)
</p>
<p style="text-align: center;"><code class="reqn">\alpha(t) = 1 - \frac{1}{t^{\delta} + 1},</code>
</p>

<p>where <code class="reqn">\delta \in (3/4 - 1/q, 1)</code> is another tuning parameter. When <code class="reqn">q = 2</code>, one can just take <code class="reqn">\alpha(t) = 1</code>, and the value of <code class="reqn">a</code> does not matter.
</p>
<p>The estimator without the shrinkage function <code class="reqn">\alpha</code> is implemented in the R package &lsquo;ElliptCopulas&rsquo;.
We use this implementation and bring in the shrinkage function.
</p>
<p>In order to make <code class="reqn">g_{\mathcal{R}}</code> identifiable, an extra normalization procedure is implemented
in line with an extra constraint on <code class="reqn">g_{\mathcal{R}}</code>.
When normalize = 1, this corresponds to <code class="reqn">\mathbf{R}</code> being the correlation matrix of <code class="reqn">\mathbf{Z}</code>.
When normalize = 2, this corresponds to the identifiability condition of Derumigny &amp; Fermanian (2022).
</p>


<h3>Value</h3>

<p>The estimates for <code class="reqn">g_{\mathcal{R}}</code> at the grid points.
</p>


<h3>References</h3>

<p>Derumigny, A., Fermanian, J.-D., Ryan, V., van der Spek, R. (2024).
ElliptCopulas, R package version 0.1.4.1.
url: https://CRAN.R-project.org/package=ElliptCopulas.
</p>
<p>Derumigny, A. &amp; Fermanian, J.-D. (2022).
Identifiability and estimation of meta-elliptical copula generators.
Journal of Multivariate Analysis 190:104962. <br />
doi: https://doi.org/10.1016/j.jmva.2022.104962.
</p>
<p>De Keyser, S. &amp; Gijbels, I. (2024).
Hierarchical variable clustering via copula-based divergence measures between random vectors.
International Journal of Approximate Reasoning 165:109090.
doi: https://doi.org/10.1016/j.ijar.2023.109090.
</p>
<p>Liebscher, E. (2005).
A semiparametric density estimator based on elliptical distributions.
Journal of Multivariate Analysis 92(1):205-225.
doi: https://doi.org/10.1016/j.jmva.2003.09.007.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ellcopest">ellcopest</a></code> for improved kernel estimation of the elliptical generator of a meta-elliptical copula,
<code><a href="#topic+elliptselect">elliptselect</a></code> for selecting optimal tuning parameters for the improved kernel estimator of the elliptical generator,
<code><a href="#topic+phiellip">phiellip</a></code> for estimating the <code class="reqn">\Phi</code>-dependence between <code class="reqn">k</code> random vectors having a meta-elliptical copula.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
q = 4

# Sample size
n = 1000

# Grid on which to evaluate the elliptical generator
grid = seq(0.005,100,by = 0.005)

# Degrees of freedom
nu = 7

# Student-t generator with 7 degrees of freedom
g_q = ((nu/(nu-2))^(q/2))*(gamma((q+nu)/2)/(((pi*nu)^(q/2))*gamma(nu/2))) *
      ((1+(grid/(nu-2)))^(-(q+nu)/2))

# Density of squared radius
R2 = function(t,q){(gamma((q+nu)/2)/(((nu-2)^(q/2))*gamma(nu/2)*gamma(q/2))) *
                   (t^((q/2)-1)) * ((1+(t/(nu-2)))^(-(q+nu)/2))}

# Sample from 4-dimensional Student-t distribution with 7 degrees of freedom
# and identity covariance matrix
sample = ElliptCopulas::EllDistrSim(n,q,diag(q),density_R2 = function(t){R2(t,q)})

# Shrinkage function
shrinkage = function(t,p){1-(1/((t^p) + 1))}

# Tuning parameter selection
opt_parameters = elliptselect(n,q,seq((3/4)-(1/q)+0.01,1-0.01,len = 200),
                                  seq(0.01,2,len = 200))

# Optimal tuning parameters
a = opt_parameters$Opta ; p = opt_parameters$Optp ; h = opt_parameters$Opth

# Estimated elliptical generator
g_est = elldistrest(Z = sample, Sigma_m1 = diag(q), grid = grid, h = h, a = a,
                    shrink = function(t){shrinkage(t,p)})

plot(grid,g_est,type = "l", xlim = c(0,8))
lines(grid,g_q,col = "green")


</code></pre>

<hr>
<h2 id='elliptselect'>elliptselect</h2><span id='topic+elliptselect'></span>

<h3>Description</h3>

<p>This functions selects optimal tuning parameters for improved kernel estimation of the generator of an elliptical distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>elliptselect(n, q, pseq, aseq)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="elliptselect_+3A_n">n</code></td>
<td>
<p>The sample size.</p>
</td></tr>
<tr><td><code id="elliptselect_+3A_q">q</code></td>
<td>
<p>The total dimension.</p>
</td></tr>
<tr><td><code id="elliptselect_+3A_pseq">pseq</code></td>
<td>
<p>Candidate values for the <code class="reqn">\delta</code> parameter of the shrinkage function.</p>
</td></tr>
<tr><td><code id="elliptselect_+3A_aseq">aseq</code></td>
<td>
<p>Candidate values for the <code class="reqn">a</code> parameter of the Liebscher function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When using the function <code><a href="#topic+elldistrest">elldistrest</a></code> for estimating an elliptical generator <code class="reqn">g_{\mathcal{R}}</code>
based on a kernel <code class="reqn">k</code> with bandwidth <code class="reqn">h_{n}</code>, the function
</p>
<p style="text-align: center;"><code class="reqn">\psi(t) = -a +  \left (a^{q/2} + t^{q/2} \right )^{2/q},</code>
</p>
<p> and the shrinkage function (for <code class="reqn">q &gt; 3</code>)
</p>
<p style="text-align: center;"><code class="reqn">\alpha(t) = 1 - \frac{1}{t^{\delta} + 1},</code>
</p>
<p> this function selects <code class="reqn">h_{n}, \delta</code> and <code class="reqn">a</code> in the following way.
</p>
<p>Use the normal generator <code class="reqn">g_{\mathcal{R}}(t) = e^{-t/2}/(2 \pi)^{q/2}</code> as reference generator, and define
</p>
<p style="text-align: center;"><code class="reqn">\Psi(t) = \frac{\pi^{q/2}}{\Gamma(q/2)} \left (\psi^{-1}(t) \right )^{\prime} \left (\psi^{-1}(t) \right )^{q/2 - 1} g_{\mathcal{R}} \left (\psi^{-1}(t) \right ),</code>
</p>
<p> as well as
</p>
<p style="text-align: center;"><code class="reqn">h_{n}^{\text{opt}} = \left \{\frac{\left (\int_{-1}^{1} k^{2}(t) dt \right ) \left (\int_{0}^{\infty} \alpha(t)^{-1} \Psi(t) dt \right )}{\left (\int_{-1}^{1} t^{2} k(t) dt \right )^{2} \left (\int_{0}^{\infty} \left (\alpha(t)^{2} \Psi^{\prime \prime}(t) \right )^{2} dt \right )} \right \}^{1/5} n^{-1/5}.</code>
</p>

<p>When <code class="reqn">q = 2</code>, take <code class="reqn">\alpha(t) = 1</code> (there is no need for shrinkage), and take <code class="reqn">h_{n}^{\text{opt}}</code>. The value of <code class="reqn">a</code> does not matter.
</p>
<p>When <code class="reqn">q &gt; 2</code>, specify a grid of candidate <code class="reqn">\delta</code>-values in <code class="reqn">(3/4 - 1/q,1)</code> and a grid of <code class="reqn">a</code>-values in <code class="reqn">(0, \infty)</code>.
For each of these candidate values, compute the corresponding optimal (AMISE) bandwidth <code class="reqn">h_{n}^{\text{opt}}</code>.
Take the combination of parameters that minimizes (a numerical approximation of) the (normal reference) AMISE given in equation (20) of De Keyser &amp; Gijbels (2024).
</p>


<h3>Value</h3>

<p>A list with elements &quot;Opta&quot; containing the optimal <code class="reqn">a</code>, &quot;Optp&quot; containing the optimal <code class="reqn">\delta</code>, and &quot;Opth&quot; containing the optimal <code class="reqn">h_{n}</code>.
</p>


<h3>References</h3>

<p>De Keyser, S. &amp; Gijbels, I. (2024).
Hierarchical variable clustering via copula-based divergence measures between random vectors.
International Journal of Approximate Reasoning 165:109090.
doi: https://doi.org/10.1016/j.ijar.2023.109090.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+elldistrest">elldistrest</a></code> for improved kernel estimation of the elliptical generator of an elliptical distribution,
<code><a href="#topic+ellcopest">ellcopest</a></code> for improved kernel estimation of the elliptical generator of a meta-elliptical copula,
<code><a href="#topic+phiellip">phiellip</a></code> for estimating the <code class="reqn">\Phi</code>-dependence between <code class="reqn">k</code> random vectors having a meta-elliptical copula.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
q = 4
n = 1000
opt_parameters = elliptselect(n,q,seq((3/4)-(1/q)+0.01,1-0.01,len = 200),
                                  seq(0.01,2,len = 200))

</code></pre>

<hr>
<h2 id='estphi'>estphi</h2><span id='topic+estphi'></span>

<h3>Description</h3>

<p>Given a <code class="reqn">q</code>-dimensional random vector <code class="reqn">\mathbf{X} = (\mathbf{X}_{1},...,\mathbf{X}_{k})</code> with <code class="reqn">\mathbf{X}_{i}</code> a <code class="reqn">d_{i}</code>-dimensional random vector, i.e., <code class="reqn">q = d_{1} + ... + d_{k}</code>,
this function estimates the <code class="reqn">\Phi</code>-dependence between <code class="reqn">\mathbf{X}_{1},...,\mathbf{X}_{k}</code> by estimating the joint and marginal
copula densities.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>estphi(sample, dim, est_method, phi)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="estphi_+3A_sample">sample</code></td>
<td>
<p>A sample from a <code class="reqn">q</code>-dimensional random vector <code class="reqn">\mathbf{X}</code> (<code class="reqn">n \times q</code> matrix with observations in rows, variables in columns).</p>
</td></tr>
<tr><td><code id="estphi_+3A_dim">dim</code></td>
<td>
<p>The vector of dimensions <code class="reqn">(d_{1},...,d_{k})</code>.</p>
</td></tr>
<tr><td><code id="estphi_+3A_est_method">est_method</code></td>
<td>
<p>The method used for estimating the <code class="reqn">\Phi</code>-dependence.</p>
</td></tr>
<tr><td><code id="estphi_+3A_phi">phi</code></td>
<td>
<p>The function <code class="reqn">\Phi</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When <code class="reqn">\mathbf{X}</code> has copula density <code class="reqn">c</code> with marginal copula densities <code class="reqn">c_{i}</code> of <code class="reqn">\mathbf{X}_{i}</code> for <code class="reqn">i = 1, \dots, k</code>,
the <code class="reqn">\Phi</code>-dependence between <code class="reqn">\mathbf{X}_{1}, \dots, \mathbf{X}_{k}</code> equals
</p>
<p style="text-align: center;"><code class="reqn">\mathcal{D}_{\Phi} \left (\mathbf{X}_{1}, \dots, \mathbf{X}_{k} \right ) = \mathbb{E} \left \{ \frac{\prod_{i = 1}^{k} c_{i}(\mathbf{U}_{i})}{c \left ( \mathbf{U} \right )} \Phi \left (\frac{c(\mathbf{U})}{\prod_{i = 1}^{k}c_{i}(\mathbf{U}_{i})} \right ) \right \},</code>
</p>

<p>for a certain continuous, convex function <code class="reqn">\Phi : (0,\infty) \rightarrow \mathbb{R}</code>, and with <code class="reqn">\mathbf{U} = (\mathbf{U}_{1}, \dots, \mathbf{U}_{k}) \sim c</code>.
</p>
<p>This functions allows to estimate <code class="reqn">\mathcal{D}_{\Phi}</code> in several ways (options for est_method)
</p>
 <ul>
<li><p> list(&quot;hac&quot;, type = type, M = M) for parametric estimation by fitting a hierarchical Archimedean copula (hac) via pseudo-maximum likelihood estimation,
using a generator of type = type and a simulated Monte Carlo sample of size <code class="reqn">M</code> in order to approximate the expectation, see also the functions <code><a href="#topic+mlehac">mlehac</a></code> and <code><a href="#topic+phihac">phihac</a></code>,
</p>
</li>
<li><p> list(&quot;nphac&quot;, estimator = estimator, type = type) for fully non-parametric estimation using the beta kernel estimator
or Gaussian transformation kernel estimator using a fitted hac (via pseudo-maximum likelihood estimation) of type = type to find locally optimal bandwidths, see also the function <code><a href="#topic+phinp">phinp</a></code>,
</p>
</li>
<li><p> list(&quot;np&quot;, estimator = estimator, bw_method = bw_method) for fully non-parametric estimation using the beta kernel estimator or
Gaussian transformation kernel estimator, see <code><a href="#topic+phinp">phinp</a></code> for different bw_method arguments (either 1 or 2, for performing local bandwidth selection),
</p>
</li>
<li><p> list(&quot;ellip&quot;, grid = grid) for semi-parametric estimation through meta-elliptical copulas, with bandwidths determined by the <code><a href="#topic+elliptselect">elliptselect</a></code> function,
see also the function <code><a href="#topic+phiellip">phiellip</a></code>.</p>
</li></ul>



<h3>Value</h3>

<p>The estimated <code class="reqn">\Phi</code>-dependence between <code class="reqn">\mathbf{X}_{1}, \dots, \mathbf{X}_{k}</code>.
</p>


<h3>References</h3>

<p>De Keyser, S. &amp; Gijbels, I. (2024).
Hierarchical variable clustering via copula-based divergence measures between random vectors.
International Journal of Approximate Reasoning 165:109090.
doi: https://doi.org/10.1016/j.ijar.2023.109090.
</p>
<p>De Keyser, S. &amp; Gijbels, I. (2024).
Parametric dependence between random vectors via copula-based divergence measures.
Journal of Multivariate Analysis 203:105336. <br />
doi: https://doi.org/10.1016/j.jmva.2024.105336.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+phihac">phihac</a></code> for computing the <code class="reqn">\Phi</code>-dependence between all the child copulas of a hac object with two nesting levels,
<code><a href="#topic+phinp">phinp</a></code> for fully non-parametric estimation of the <code class="reqn">\Phi</code>-dependence between <code class="reqn">k</code> random vectors,
<code><a href="#topic+phiellip">phiellip</a></code> for estimating the <code class="reqn">\Phi</code>-dependence between <code class="reqn">k</code> random vectors having a meta-elliptical copula.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

# Hierarchical Archimedean copula setting
q = 4
dim = c(2,2)

# Sample size
n = 1000

# Four dimensional hierarchical Gumbel copula
# with parameters (theta_0,theta_1,theta_2) = (2,3,4)
hac = gethac(dim,c(2,3,4),type = 1)

# Sample
sample =  suppressWarnings(HAC::rHAC(n,hac))

# Several estimators for the mutual information between two random vectors of size 2

est_phi_1 = estphi(sample,dim,list("hac",type = 1,M = 10000),function(t){t * log(t)})
est_phi_2 = estphi(sample,dim,list("nphac",estimator = "beta",type = 1),
                                   function(t){t * log(t)})
est_phi_3 = estphi(sample,dim,list("nphac",estimator = "trans",type = 1),
                                   function(t){t * log(t)})
est_phi_4 = estphi(sample,dim,list("np",estimator = "beta",bw_method = 1),
                                   function(t){t * log(t)})
est_phi_5 = estphi(sample,dim,list("np",estimator = "trans",bw_method = 1),
                                   function(t){t * log(t)})
est_phi_6 = estphi(sample,dim,list("np",estimator = "beta",bw_method = 2),
                                   function(t){t * log(t)})
est_phi_7 = estphi(sample,dim,list("np",estimator = "trans",bw_method = 2),
                                   function(t){t * log(t)})

true_phi = phihac(hac,dim,10000,function(t){t * log(t)})

# Gaussian copula setting

q = 4
dim = c(2,2)

# Sample size
n = 1000

# AR(1) correlation matrix with correlation 0.5
R = 0.5^(abs(matrix(1:q-1,nrow = q, ncol = q, byrow = TRUE) - (1:q-1)))

# Sample from 4-dimensional normal distribution
sample = mvtnorm::rmvnorm(n,rep(0,q),R,method = "chol")

# Estimate mutual information via MECIP procedure
est_phi = estphi(sample,dim,list("ellip",grid = seq(0.005,100,by = 0.005)),
                                 function(t){t * log(t)})

true_phi = minormal(R,dim)


</code></pre>

<hr>
<h2 id='estR'>estR</h2><span id='topic+estR'></span>

<h3>Description</h3>

<p>This function computes the sample <code class="reqn">Q</code>-scores rank correlation matrix.
A ridge penalization is possible.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>estR(
  sample,
  omega = 1,
  Q = function(t) {
     stats::qnorm(t)
 }
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="estR_+3A_sample">sample</code></td>
<td>
<p>A sample from a <code class="reqn">q</code>-dimensional random vector <code class="reqn">\mathbf{X}</code> (<code class="reqn">n \times q</code> matrix with observations in rows, variables in columns).</p>
</td></tr>
<tr><td><code id="estR_+3A_omega">omega</code></td>
<td>
<p>The penalty parameter for ridge penalization (default = 1, meaning no penalization).</p>
</td></tr>
<tr><td><code id="estR_+3A_q">Q</code></td>
<td>
<p>The quantile function to be applied to the copula pseudo-observations (default = qnorm()).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Given a <code class="reqn">q</code>-dimensional random vector <code class="reqn">\mathbf{X} = (\mathbf{X}_{1},...,\mathbf{X}_{k})</code> with <code class="reqn">\mathbf{X}_{i} = (X_{i1}, \dots, X_{id_{i}})</code> a <code class="reqn">d_{i}</code> dimensional random vector, i.e., <code class="reqn">q = d_{1} + ... + d_{k}</code>, the sample <code class="reqn">Q</code>-scores rank correlation matrix is given as </p>
<p style="text-align: center;"><code class="reqn">\widehat{\mathbf{R}}_{n} = \begin{pmatrix}
\widehat{\mathbf{R}}_{11} &amp; \widehat{\mathbf{R}}_{12} &amp; \cdots &amp; \widehat{\mathbf{R}}_{1k} \\ \widehat{\mathbf{R}}_{12}^{\text{T}} &amp; \widehat{\mathbf{R}}_{22} &amp; \cdots &amp; \widehat{\mathbf{R}}_{2k} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \widehat{\mathbf{R}}_{1k}^{\text{T}} &amp; \widehat{\mathbf{R}}_{2k}^{\text{T}} &amp; \cdots &amp; \widehat{\mathbf{R}}_{kk}
\end{pmatrix} \hspace{0.2cm} \text{with} \hspace{0.2cm} \left (\widehat{\mathbf{R}}_{im} \right )_{jt} = \widehat{\rho}_{ij,mt} = \frac{\frac{1}{n} \sum_{\ell = 1}^{n} \widehat{Z}_{ij}^{(\ell)} \widehat{Z}_{mt}^{(\ell)}}{\frac{1}{n} \sum_{\ell = 1}^{n} \left [Q \left (\frac{\ell}{n+1} \right ) \right ]^{2}},</code>
</p>

<p>for <code class="reqn">i,m = 1, \dots, k</code>, <code class="reqn">j = 1, \dots, d_{i}</code>, and <code class="reqn">t = 1, \dots, d_{m}</code>, based on the observed <code class="reqn">Q</code>-scores
</p>
<p style="text-align: center;"><code class="reqn">\widehat{Z}_{ij}^{(\ell)} = Q \left (\frac{n}{n+1} \widehat{F}_{ij} \left (X_{ij}^{(\ell)} \right )\right ) = Q \left (\frac{1}{n+1} \sum_{t = 1}^{n} 1 \left \{X_{ij}^{(t)} \leq X_{ij}^{(\ell)} \right \} \right ),</code>
</p>

<p>for <code class="reqn">\ell = 1, \dots, n</code>, where <code class="reqn">\widehat{F}_{ij}</code> is the empirical cdf of the sample <code class="reqn">X_{ij}^{(1)},\dots,X_{ij}^{(n)}</code> for <code class="reqn">i = 1, \dots, k</code> and <code class="reqn">j = 1, \dots, d_{i}</code>.
The underlying assumption is that the copula of <code class="reqn">\mathbf{X}</code> is meta-elliptical.
The default for <code class="reqn">Q</code> is the standard normal quantile function (corresponding to the assumption of a Gaussian copula).
Ridge penalization (especially in the Gaussian copula setting) with penalty parameter omega = <code class="reqn">\omega</code> boils down to computing
</p>
<p style="text-align: center;"><code class="reqn">\omega \widehat{\mathbf{R}}_{n} + (1-\omega) \mathbf{I}_{q},</code>
</p>
<p> where <code class="reqn">\mathbf{I}_{q}</code> stands for the identity matrix.
</p>


<h3>Value</h3>

<p>The (ridge penalized) sample <code class="reqn">Q</code>-scores rank correlation matrix.
</p>


<h3>References</h3>

<p>De Keyser, S. &amp; Gijbels, I. (2024).
Some new tests for independence among continuous random vectors.
</p>
<p>Warton, D.I. (2008).
Penalized normal likelihood and ridge regularization of correlation and covariance matrices.
Journal of the American Statistical Association 103(481):340-349. <br />
doi: https://doi.org/10.1198/016214508000000021.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cvomega">cvomega</a></code> for selecting omega using K-fold cross-validation in case of a Gaussian copula.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Multivariate normal copula setting

q = 10
n = 50

# AR(1) correlation matrix with correlation 0.5
R = 0.5^(abs(matrix(1:q-1,nrow = q, ncol = q, byrow = TRUE) - (1:q-1)))

# Sample from multivariate normal distribution
sample = mvtnorm::rmvnorm(n,rep(0,q),R,method = "chol")

# 5-fold cross-validation with Gaussian likelihood as loss for selecting omega
omega = cvomega(sample = sample,omegas = seq(0.01,0.999,len = 50),K = 5)

R_est = estR(sample,omega = omega)

# Multivariate Student-t copula setting

q = 10
n = 500

# Degrees of freedom
nu = 7

# Density of R^2, with R the radius of the elliptical distribution
# Identifiability contraint is that R is the correlation matrix
R2 = function(t,q){(gamma((q+nu)/2)/(((nu-2)^(q/2)) * gamma(nu/2) * gamma(q/2))) *
                   (t^((q/2)-1)) * ((1+(t/(nu-2)))^(-(q+nu)/2))}

# Univariate quantile function, with unit variance
Q = function(t){extraDistr::qlst(t,nu,0,sqrt((nu-2)/nu))}

# AR(1) correlation matrix with correlation 0.5
R = 0.5^(abs(matrix(1:q-1,nrow = q, ncol = q, byrow = TRUE) - (1:q-1)))

# Sample from multivariate Student-t distribution
# with correlation matrix R and nu degrees of freedom
sample = ElliptCopulas::EllDistrSim(n,q,t(chol(R)),density_R2 = function(t){R2(t,q)})

R_est = estR(sample,Q = Q)
</code></pre>

<hr>
<h2 id='gethac'>gethac</h2><span id='topic+gethac'></span>

<h3>Description</h3>

<p>Given a <code class="reqn">q</code>-dimensional random vector <code class="reqn">\mathbf{X} = (\mathbf{X}_{1},...,\mathbf{X}_{k})</code> with <code class="reqn">\mathbf{X}_{i}</code> a <code class="reqn">d_{i}</code>-dimensional random vector, i.e., <code class="reqn">q = d_{1} + ... + d_{k}</code>,
this function construct a hac object (hierarchical Archimedean copula) with two nesting levels given
the specified dimensions and parameters of the root and <code class="reqn">k</code> child copulas.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gethac(dim, thetas, type)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="gethac_+3A_dim">dim</code></td>
<td>
<p>The vector of dimensions <code class="reqn">(d_{1},...,d_{k})</code>.</p>
</td></tr>
<tr><td><code id="gethac_+3A_thetas">thetas</code></td>
<td>
<p>The parameters <code class="reqn">(\theta_{0}, \theta_{1}, \dots, \theta_{k})</code>.</p>
</td></tr>
<tr><td><code id="gethac_+3A_type">type</code></td>
<td>
<p>The type of Archimedean copula.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A hierarchical (or nested) Archimedean copula <code class="reqn">C</code> with two nesting levels and <code class="reqn">k</code> child copulas is given by
</p>
<p style="text-align: center;"><code class="reqn">C(\mathbf{u}) = C_{0} \left (C_{1}(\mathbf{u}_{1}), \dots, C_{k}(\mathbf{u}_{k}) \right ),</code>
</p>

<p>where <code class="reqn">\mathbf{u} = (\mathbf{u}_{1}, \dots, \mathbf{u}_{k}) \in \mathbb{R}^{q}</code> with <code class="reqn">\mathbf{u}_{i} \in \mathbb{R}^{d_{i}}</code> for <code class="reqn">i = 1, \dots, k</code>.
The (<code class="reqn">k</code>-dimensional) copula <code class="reqn">C_{0}</code> is called the root copula, and the (<code class="reqn">d_{i}</code>-dimensional) copulas <code class="reqn">C_{i}</code> are the child copulas.
</p>
<p>They all belong to the class of Archimedean copulas, and we denote <code class="reqn">\theta_{i}</code> for the parameter of <code class="reqn">C_{i}</code> for <code class="reqn">i = 0,1,\dots,k</code>.
A sufficient condition to guarantee that <code class="reqn">C</code> indeed is a copula, is that <code class="reqn">C_{0},C_{1}, \dots, C_{k}</code> are all a particular member of this class of Archimedean copulas (e.g., Clayton),
and such that <code class="reqn">\theta_{0} \leq \theta_{i}</code> for <code class="reqn">i = 1, \dots, k</code> (sufficient nesting condition).
</p>
<p>When a certain child copula <code class="reqn">C_{i}</code> is one dimensional (<code class="reqn">\mathbf{X}_{i}</code> is one dimensional), <code class="reqn">\theta_{i}</code> can be any number.
It must hold that length(thetas) <code class="reqn"> =  k + 1</code>.
</p>
<p>Many functions for working with nested Archimedean copulas are developed in the R package &lsquo;HAC&rsquo;,
and the function <code><a href="#topic+gethac">gethac</a></code> utilizes these functions to quickly construct a hac object that is useful for modelling
the dependence between <code class="reqn">\mathbf{X}_{1}, \dots, \mathbf{X}_{k}</code>.
See also the R package &lsquo;HAC&rsquo; for the different possibilities of type (specified by a number in <code class="reqn">\{1,\dots,10\}</code>).
</p>


<h3>Value</h3>

<p>A hac object with two nesting levels and <code class="reqn">k</code> child copulas.
</p>


<h3>References</h3>

<p>De Keyser, S. &amp; Gijbels, I. (2024).
Parametric dependence between random vectors via copula-based divergence measures.
Journal of Multivariate Analysis 203:105336. <br />
doi: https://doi.org/10.1016/j.jmva.2024.105336.
</p>
<p>Okhrin, O., Ristig, A. &amp; Chen, G. (2024).
HAC: estimation, simulation and visualization of hierarchical Archimedean copulae (HAC), R package version 1.1-1. <br />
url: https://CRAN.R-project.org/package=HAC.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+phihac">phihac</a></code> for computing the <code class="reqn">\Phi</code>-dependence between all the child copulas of a hac object with two nesting levels,
<code><a href="#topic+Helhac">Helhac</a></code> for computing the Hellinger distance between all the child copulas of a hac object with two nesting levels,
<code><a href="#topic+mlehac">mlehac</a></code> for maximum pseudo-likelihood estimation of the parameters of a hac object with two nesting levels.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dim = c(3,5,1,2)
thetas = c(2,2,3,1,4)

# 11 dimensional nested Gumbel copula with
# (theta_0,theta_1,theta_2,theta_3,theta_4) = (2,2,3,1,4),
# where the value of theta_3 could be anything,
# because the third random vector is one dimensional

HAC = gethac(dim,thetas,type = 1)

</code></pre>

<hr>
<h2 id='grouplasso'>grouplasso</h2><span id='topic+grouplasso'></span>

<h3>Description</h3>

<p>Given a <code class="reqn">q</code>-dimensional random vector <code class="reqn">\mathbf{X} = (\mathbf{X}_{1},...,\mathbf{X}_{k})</code> with <code class="reqn">\mathbf{X}_{i}</code> a <code class="reqn">d_{i}</code>-dimensional random vector, i.e., <code class="reqn">q = d_{1} + ... + d_{k}</code>,
this function computes the empirical penalized Gaussian copula covariance matrix with the Gaussian log-likelihood
plus the grouped lasso penalty as objective function, where the groups are the diagonal and off-diagonal blocks corresponding to the different
random vectors.
Model selection is done by choosing omega such that BIC is maximal.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>grouplasso(Sigma, S, n, omegas, dim, step.size = 100, trace = 0)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="grouplasso_+3A_sigma">Sigma</code></td>
<td>
<p>An initial guess for the covariance matrix (typically equal to S).</p>
</td></tr>
<tr><td><code id="grouplasso_+3A_s">S</code></td>
<td>
<p>The sample matrix of normal scores covariances.</p>
</td></tr>
<tr><td><code id="grouplasso_+3A_n">n</code></td>
<td>
<p>The sample size.</p>
</td></tr>
<tr><td><code id="grouplasso_+3A_omegas">omegas</code></td>
<td>
<p>The candidate values for the tuning parameter in <code class="reqn">[0,\infty)</code>.</p>
</td></tr>
<tr><td><code id="grouplasso_+3A_dim">dim</code></td>
<td>
<p>The vector of dimensions <code class="reqn">(d_{1},...,d_{k})</code>.</p>
</td></tr>
<tr><td><code id="grouplasso_+3A_step.size">step.size</code></td>
<td>
<p>The step size used in the generalized gradient descent, affects the speed of the algorithm (default = 100).</p>
</td></tr>
<tr><td><code id="grouplasso_+3A_trace">trace</code></td>
<td>
<p>Controls how verbose output should be (default = 0, meaning no verbose output).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Given a covariance matrix </p>
<p style="text-align: center;"><code class="reqn">\boldsymbol{\Sigma} = \begin{pmatrix} \boldsymbol{\Sigma}_{11} &amp; \boldsymbol{\Sigma}_{12} &amp; \cdots &amp; \boldsymbol{\Sigma}_{1k} \\
                                                             \boldsymbol{\Sigma}_{12}^{\text{T}} &amp; \boldsymbol{\Sigma}_{22} &amp; \cdots &amp; \boldsymbol{\Sigma}_{2k} \\
                                                             \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
                                                             \boldsymbol{\Sigma}_{1k}^{\text{T}} &amp; \boldsymbol{\Sigma}_{2k}^{\text{T}} &amp; \cdots &amp; \boldsymbol{\Sigma}_{kk} \end{pmatrix},</code>
</p>

<p>the aim is to solve/compute </p>
<p style="text-align: center;"><code class="reqn">\widehat{\boldsymbol{\Sigma}}_{\text{GLT},n} \in \text{arg min}_{\boldsymbol{\Sigma} &gt; 0} \left \{
\ln \left | \boldsymbol{\Sigma} \right | + \text{tr} \left (\boldsymbol{\Sigma}^{-1} \widehat{\boldsymbol{\Sigma}}_{n} \right ) + P_{\text{GLT}}\left (\boldsymbol{\Sigma},\omega_{n} \right ) \right \},</code>
</p>

<p>where the penalty function <code class="reqn">P_{\text{GLT}}</code> is of group lasso-type:
</p>
<p style="text-align: center;"><code class="reqn">P_{\text{GLT}} \left (\boldsymbol{\Sigma},\omega_{n} \right ) = 2 \sum_{i,j = 1, j &gt; i}^{k} p_{\omega_{n}} \left (\sqrt{d_{i}d_{j}} \left | \left |\boldsymbol{\Sigma}_{ij} \right | \right |_{\text{F}} \right )
+ \sum_{i = 1}^{k} p_{\omega_{n}} \left (\sqrt{d_{i}(d_{i}-1)} \left | \left | \boldsymbol{\Delta}_{i} * \boldsymbol{\Sigma}_{ii} \right | \right |_{\text{F}} \right ), </code>
</p>

<p>for a certain penalty function <code class="reqn">p_{\omega_{n}}</code> with penalty parameter <code class="reqn">\omega_{n}</code>, and
<code class="reqn">\boldsymbol{\Delta}_{i} \in \mathbb{R}^{d_{i} \times d_{i}}</code> a matrix with ones as off-diagonal elements and zeroes
on the diagonal (in order to avoid shrinking the variances, the operator <code class="reqn">*</code> stands for elementwise multiplication).
</p>
<p>For now, the only possibility in this function for <code class="reqn">p_{\omega_{n}}</code> is the lasso penalty <code class="reqn">p_{\omega_{n}}(t) = \omega_{n} t</code>.
For other penalties (e.g., scad), one can do a local linear approximation to the penalty function and iteratively perform weighted group lasso optimizations (similar to what is done in the function <code><a href="#topic+covgpenal">covgpenal</a></code>).
</p>
<p>Regarding the implementation, we used the code available in the R package &lsquo;spcov&rsquo; (see the manual for further explanations),
but altered it to the context of a group-lasso penalty.
</p>
<p>For tuning <code class="reqn">\omega_{n}</code>, we maximize (over a grid of candidate values) the BIC criterion
</p>
<p style="text-align: center;"><code class="reqn">\text{BIC} \left (\widehat{\boldsymbol{\Sigma}}_{\omega_{n}} \right ) = -n \left [\ln \left |\widehat{\boldsymbol{\Sigma}}_{\omega_{n}} \right | + \text{tr} \left (\widehat{\boldsymbol{\Sigma}}_{\omega_{n}}^{-1} \widehat{\boldsymbol{\Sigma}}_{n} \right ) \right ] - \ln(n) \text{df} \left (\widehat{\boldsymbol{\Sigma}}_{\omega_{n}} \right ),</code>
</p>

<p>where <code class="reqn">\widehat{\boldsymbol{\Sigma}}_{\omega_{n}}</code> is the estimated candidate covariance matrix using <code class="reqn">\omega_{n}</code>
and df (degrees of freedom) equals
</p>
<p style="text-align: center;"><code class="reqn"> \hspace{-3cm} \text{df} \left (\widehat{\boldsymbol{\Sigma}}_{\omega_{n}} \right ) = \sum_{i,j = 1, j &gt; i}^{k} 1 \left (\left | \left | \widehat{\boldsymbol{\Sigma}}_{\omega_{n},ij} \right | \right |_{\text{F}} &gt; 0 \right ) \left (1 + \frac{\left | \left | \widehat{\boldsymbol{\Sigma}}_{\omega_{n},ij} \right | \right |_{\text{F}}}{\left | \left | \widehat{\boldsymbol{\Sigma}}_{n,ij} \right | \right |_{\text{F}}} \left (d_{i}d_{j} - 1 \right ) \right )</code>
</p>

<p style="text-align: center;"><code class="reqn"> \hspace{2cm} + \sum_{i = 1}^{k} 1 \left (\left | \left |\boldsymbol{\Delta}_{i} * \widehat{\boldsymbol{\Sigma}}_{\omega_{n},ii} \right | \right |_{\text{F}} &gt; 0  \right )
\left (1 + \frac{\left | \left |\boldsymbol{\Delta}_{i} * \widehat{\boldsymbol{\Sigma}}_{\omega_{n},ii} \right | \right |_{\text{F}}}{\left | \left |\boldsymbol{\Delta}_{i} * \widehat{\boldsymbol{\Sigma}}_{n,ii} \right | \right |_{\text{F}}} \left (\frac{d_{i} \left ( d_{i} - 1 \right )}{2} - 1 \right )  \right ) + q,</code>
</p>

<p>with <code class="reqn">\widehat{\boldsymbol{\Sigma}}_{\omega_{n},ij}</code> the <code class="reqn">(i,j)</code>'th block of <code class="reqn">\widehat{\boldsymbol{\Sigma}}_{\omega_{n}}</code>, similarly for <code class="reqn">\widehat{\boldsymbol{\Sigma}}_{n,ij}</code>.
</p>


<h3>Value</h3>

<p>A list with elements &quot;est&quot; containing the (group lasso) penalized matrix of sample normal scores rank correlations (output as provided by the function &ldquo;spcov.R&rdquo;), and &quot;omega&quot; containing the optimal tuning parameter.
</p>


<h3>References</h3>

<p>De Keyser, S. &amp; Gijbels, I. (2024).
High-dimensional copula-based Wasserstein dependence.
doi: https://doi.org/10.48550/arXiv.2404.07141.
</p>
<p>Bien, J. &amp; Tibshirani, R. (2022).
spcov: sparse estimation of a covariance matrix, R package version 1.3.
url: https://CRAN.R-project.org/package=spcov.
</p>
<p>Bien, J. &amp; Tibshirani, R. (2011).
Sparse Estimation of a Covariance Matrix.
Biometrika 98(4):807-820.
doi: https://doi.org/10.1093/biomet/asr054.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+covgpenal">covgpenal</a></code> for (elementwise) lasso-type estimation of the normal scores rank correlation matrix.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
q = 10
dim = c(5,5)
n = 100

# AR(1) correlation matrix with correlation 0.5
R = 0.5^(abs(matrix(1:q-1,nrow = q, ncol = q, byrow = TRUE) - (1:q-1)))

# Sparsity on off-diagonal blocks
R0 = createR0(R,dim)

# Sample from multivariate normal distribution
sample = mvtnorm::rmvnorm(n,rep(0,q),R0,method = "chol")

# Normal scores
scores = matrix(0,n,q)
for(j in 1:q){scores[,j] = qnorm((n/(n+1)) * ecdf(sample[,j])(sample[,j]))}

# Sample matrix of normal scores covariances
Sigma_est = cov(scores) * ((n-1)/n)

# Candidate tuning parameters
omega = seq(0.01, 0.6, length = 50)

Sigma_est_penal = grouplasso(Sigma_est, Sigma_est, n, omega, dim)

</code></pre>

<hr>
<h2 id='hamse'>hamse</h2><span id='topic+hamse'></span>

<h3>Description</h3>

<p>This function performs local bandwidth selection based on the amse (asymptotic mean squared error)
for the beta kernel or Gaussian transformation kernel copula density estimator.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hamse(input, cop = NULL, pseudos = NULL, n, estimator, bw_method)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="hamse_+3A_input">input</code></td>
<td>
<p>The copula argument at which the optimal local bandwidth is to be computed.</p>
</td></tr>
<tr><td><code id="hamse_+3A_cop">cop</code></td>
<td>
<p>A fitted reference hac object, in case bw_method = 0 (default = NULL).</p>
</td></tr>
<tr><td><code id="hamse_+3A_pseudos">pseudos</code></td>
<td>
<p>The (estimated) copula observations from a <code class="reqn">q</code>-dimensional random vector <code class="reqn">\mathbf{X}</code> (<code class="reqn">n \times q</code> matrix with observations in rows, variables in columns), in case bw_method = 1 (default = NULL).</p>
</td></tr>
<tr><td><code id="hamse_+3A_n">n</code></td>
<td>
<p>The sample size.</p>
</td></tr>
<tr><td><code id="hamse_+3A_estimator">estimator</code></td>
<td>
<p>Either &quot;beta&quot; or &quot;trans&quot; for the beta kernel or the Gaussian transformation kernel copula density estimator.</p>
</td></tr>
<tr><td><code id="hamse_+3A_bw_method">bw_method</code></td>
<td>
<p>A number in <code class="reqn">\{0,1\}</code> specifying the method used for computing the bandwidth.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When estimator = &quot;beta&quot;, this function computes, at a certain input, a numerical approximation of the optimal local bandwidth (for the beta kernel copula density estimator) in terms of the amse
(asymptotic mean squared error) given in equation (27) of De Keyser &amp; Gijbels (2024).
When estimator = &quot;trans&quot; (for the Gaussian transformation kernel copula density estimator), this optimal bandwidth is given
at the end of Section 5.2 in De Keyser &amp; Gijbels (2024).
</p>
<p>Of course, these optimal bandwidths depend upon the true unknown copula.
If bw_method = 0, then the given fitted (e.g., via MLE using <code><a href="#topic+mlehac">mlehac</a></code>) hac object (hierarchical Archimedean copula) cop is used as reference copula.
If bw_method = 1, then a non-parametric (beta or Gaussian transformation) kernel copula density estimator based on the pseudos as pivot is used. This pivot is computed
using the big O bandwidth (i.e., <code class="reqn">n^{-2/(q+4)}</code> in case of the beta estimator, and <code class="reqn">n^{-1/(q+4)}</code> for the transformation estimator, with <code class="reqn">q</code> the total dimension).
</p>


<h3>Value</h3>

<p>The optimal local bandwidth (in terms of amse).
</p>


<h3>References</h3>

<p>De Keyser, S. &amp; Gijbels, I. (2024).
Hierarchical variable clustering via copula-based divergence measures between random vectors.
International Journal of Approximate Reasoning 165:109090.
doi: https://doi.org/10.1016/j.ijar.2023.109090.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+betakernelestimator">betakernelestimator</a></code> for the computation of the beta kernel copula density estimator, <br />
<code><a href="#topic+transformationestimator">transformationestimator</a></code> for the computation of the Gaussian transformation kernel copula density estimator,
<code><a href="#topic+phinp">phinp</a></code> for fully non-parametric estimation of the <code class="reqn">\Phi</code>-dependence between <code class="reqn">k</code> random vectors.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>q = 4
dim = c(2,2)

# Sample size
n = 1000

# Four dimensional hierarchical Gumbel copula
# with parameters (theta_0,theta_1,theta_2) = (2,3,4)
HAC = gethac(dim,c(2,3,4),type = 1)

# Sample
sample = suppressWarnings(HAC::rHAC(n,HAC))

# Copula pseudo-observations
pseudos = matrix(0,n,q)
for(j in 1:q){pseudos[,j] = (n/(n+1)) * ecdf(sample[,j])(sample[,j])}

# Maximum pseudo-likelihood estimator to be used
# as reference copula for bw_method = 0
est_cop = mlehac(sample,dim,1,c(2,3,4))

h_1 = hamse(rep(0.5,q),cop = est_cop,n = n,estimator = "beta",bw_method = 0)
h_2 = hamse(rep(0.5,q),cop = est_cop,n = n,estimator = "trans",bw_method = 0)
h_3 = hamse(rep(0.5,q),pseudos = pseudos,n = n,estimator = "beta",bw_method = 1)
h_4 = hamse(rep(0.5,q),pseudos = pseudos,n = n,estimator = "trans",bw_method = 1)

est_dens_1 = betakernelestimator(rep(0.5,q),h_1,pseudos)
est_dens_2 = transformationestimator(rep(0.5,q),h_2,pseudos)
est_dens_3 = betakernelestimator(rep(0.5,q),h_3,pseudos)
est_dens_4 = transformationestimator(rep(0.5,q),h_4,pseudos)

true = HAC::dHAC(c("X1" = 0.5, "X2" = 0.5, "X3" = 0.5, "X4" = 0.5), HAC)

</code></pre>

<hr>
<h2 id='Helhac'>Helhac</h2><span id='topic+Helhac'></span>

<h3>Description</h3>

<p>This function computes the Hellinger distance between all the child copulas of a hac object obtained by the function <code><a href="#topic+gethac">gethac</a></code>, i.e.,
given a <code class="reqn">q</code>-dimensional random vector <code class="reqn">\mathbf{X} = (\mathbf{X_{1}},...,\mathbf{X}_{k})</code> with <code class="reqn">\mathbf{X}_{i}</code> a <code class="reqn">d_{i}</code>-dimensional random vector, i.e., <code class="reqn">q = d_{1} + ... + d_{k}</code>,
where <code class="reqn">\mathbf{X}_{1},...,\mathbf{X}_{k}</code> are connected via a hierarchical Archimedean copula with two nesting levels, <code><a href="#topic+Helhac">Helhac</a></code> computes the Hellinger distance
between <code class="reqn">\mathbf{X}_{1},...,\mathbf{X}_{k}</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Helhac(cop, dim, M)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Helhac_+3A_cop">cop</code></td>
<td>
<p>A hac object as provided by the function <code><a href="#topic+gethac">gethac</a></code>.</p>
</td></tr>
<tr><td><code id="Helhac_+3A_dim">dim</code></td>
<td>
<p>The vector of dimensions <code class="reqn">(d_{1},...,d_{k})</code>.</p>
</td></tr>
<tr><td><code id="Helhac_+3A_m">M</code></td>
<td>
<p>The size of the Monte Carlo sample used for approximating the integral of the Hellinger distance.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When <code class="reqn">\mathbf{X}</code> has copula density <code class="reqn">c</code> with marginal copula densities <code class="reqn">c_{i}</code> of <code class="reqn">\mathbf{X}_{i}</code> for <code class="reqn">i = 1, \dots, k</code>,
the <code class="reqn">\Phi</code>-dependence between <code class="reqn">\mathbf{X}_{1}, \dots, \mathbf{X}_{k}</code> equals
</p>
<p style="text-align: center;"><code class="reqn">\mathcal{D}_{\Phi} \left (\mathbf{X}_{1}, \dots, \mathbf{X}_{k} \right ) = \int_{[0,1]^{q}} \prod_{i = 1}^{k} c_{i}(\mathbf{u}_{i}) \Phi \left (\frac{c(\mathbf{u})}{\prod_{i = 1}^{k}c_{i}(\mathbf{u}_{i})} \right ),</code>
</p>

<p>for a certain continuous, convex function <code class="reqn">\Phi : (0,\infty) \rightarrow \mathbb{R}</code>.
The Hellinger distance corresponds to <code class="reqn">\Phi(t) = (\sqrt{t}-1)^{2}</code>, and <code class="reqn">\mathcal{D}_{(\sqrt{t}-1)^{2}}</code> could be approximated by
<code class="reqn">\widehat{\mathcal{D}}_{(\sqrt{t}-1)^{2}}</code> as implemented in the function <code><a href="#topic+phihac">phihac</a></code>.
Yet, for this specific choice of <code class="reqn">\Phi</code>, it is better to first simplify <code class="reqn">\mathcal{D}_{(\sqrt{t}-1)^{2}}</code> to
</p>
<p style="text-align: center;"><code class="reqn">\mathcal{D}_{(\sqrt{t}-1)^{2}} \left (\mathbf{X}_{1}, \dots, \mathbf{X}_{k} \right ) = 2 - 2 \int_{[0,1]^{q}} \sqrt{c(\mathbf{u}) \prod_{i = 1}^{k} c_{i}(\mathbf{u}_{i})} d \mathbf{u},</code>
</p>

<p>and then, by drawing a sample of size <code class="reqn">M</code> from <code class="reqn">c</code>, say <code class="reqn">\mathbf{U}^{(1)}, \dots, \mathbf{U}^{(M)}</code>, with <code class="reqn">\mathbf{U}^{(\ell)} = (\mathbf{U}_{1}^{(\ell)}, \dots, \mathbf{U}_{k}^{(\ell)})</code>, approximate it by
</p>
<p style="text-align: center;"><code class="reqn">\widetilde{D}_{(\sqrt{t}-1)^{2}} = 2 - \frac{2}{M} \sum_{\ell = 1}^{M} \sqrt{\frac{\prod_{i = 1}^{k} c_{i} \left (\mathbf{U}_{i}^{(\ell)} \right )}{c \left ( \mathbf{U}^{(\ell)} \right )}}.</code>
</p>

<p>The function <code><a href="#topic+Helhac">Helhac</a></code> computes <code class="reqn">\widetilde{\mathcal{D}}_{(\sqrt{t}-1)^{2}}</code> when <code class="reqn">c</code> is a hierarchical Archimedean copula with two nesting levels,
as produced by the function <code><a href="#topic+gethac">gethac</a></code>.
</p>


<h3>Value</h3>

<p>The Hellinger distance between <code class="reqn">\mathbf{X}_{1}, \dots, \mathbf{X}_{k}</code> (i.e., between all the child copulas of the hac object).
</p>


<h3>References</h3>

<p>De Keyser, S. &amp; Gijbels, I. (2024).
Parametric dependence between random vectors via copula-based divergence measures.
Journal of Multivariate Analysis 203:105336. <br />
doi: https://doi.org/10.1016/j.jmva.2024.105336.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gethac">gethac</a></code> for creating a hac object with two nesting levels,
<code><a href="#topic+phihac">phihac</a></code> for computing the <code class="reqn">\Phi</code>-dependence between all the child copulas of a hac object with two nesting levels,
<code><a href="#topic+mlehac">mlehac</a></code> for maximum pseudo-likelihood estimation of the parameters of a hac object with two nesting levels.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
dim = c(2,2)
thetas = c(2,3,4)

# 4 dimensional nested Gumbel copula with (theta_0,theta_1,theta_2) = (2,3,4)
HAC = gethac(dim,thetas,type = 1)

# Hellinger distance based on Monte Carlo sample of size 10000
Hel = Helhac(HAC,dim,10000)


</code></pre>

<hr>
<h2 id='Helnormal'>Helnormal</h2><span id='topic+Helnormal'></span>

<h3>Description</h3>

<p>Given a <code class="reqn">q</code>-dimensional random vector <code class="reqn">\mathbf{X} = (\mathbf{X}_{1},...,\mathbf{X}_{k})</code> with <code class="reqn">\mathbf{X}_{i}</code> a <code class="reqn">d_{i}</code>-dimensional random vector, i.e., <code class="reqn">q = d_{1} + ... + d_{k}</code>,
this function computes the correlation-based Hellinger distance between <code class="reqn">\mathbf{X}_{1},...,\mathbf{X}_{k}</code> given the entire correlation matrix <code class="reqn">\mathbf{R}</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Helnormal(R, dim)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Helnormal_+3A_r">R</code></td>
<td>
<p>The correlation matrix of <code class="reqn">\mathbf{X}</code>.</p>
</td></tr>
<tr><td><code id="Helnormal_+3A_dim">dim</code></td>
<td>
<p>The vector of dimensions <code class="reqn">(d_{1},...,d_{k})</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Given a correlation matrix </p>
<p style="text-align: center;"><code class="reqn">\mathbf{R} = \begin{pmatrix} \mathbf{R}_{11} &amp; \mathbf{R}_{12} &amp; \cdots &amp; \mathbf{R}_{1k} \\
                                                             \mathbf{R}_{12}^{\text{T}} &amp; \mathbf{R}_{22} &amp; \cdots &amp; \mathbf{R}_{2k} \\
                                                             \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
                                                             \mathbf{R}_{1k}^{\text{T}} &amp; \mathbf{R}_{2k}^{\text{T}} &amp; \cdots &amp; \mathbf{R}_{kk} \end{pmatrix},</code>
</p>

<p>the Hellinger distance equals </p>
<p style="text-align: center;"><code class="reqn">\mathcal{D}_{(\sqrt{t}-1)^{2}}^{\mathcal{N}}(\mathbf{R}) = 2 - 2 \frac{2^{q/2} |\mathbf{R}|^{1/4}}{\left |\mathbf{I}_{q} + \mathbf{R}_{0}^{-1} \mathbf{R} \right |^{1/2} \prod_{i = 1}^{k} \left | \mathbf{R}_{ii} \right |^{1/4}},</code>
</p>

<p>where <code class="reqn">\mathbf{I}_{q}</code> denotes the identity matrix, and <code class="reqn">\mathbf{R}_{0} = \text{diag}(\mathbf{R}_{11},\dots,\mathbf{R}_{kk})</code> is the correlation matrix under independence of <code class="reqn">\mathbf{X}_{1}, \dots, \mathbf{X}_{k}</code>.
The underlying assumption is that the copula of <code class="reqn">\mathbf{X}</code> is Gaussian.
</p>


<h3>Value</h3>

<p>The correlation-based Hellinger distance between <code class="reqn">\mathbf{X}_{1},...,\mathbf{X}_{k}</code>.
</p>


<h3>References</h3>

<p>De Keyser, S. &amp; Gijbels, I. (2024).
Parametric dependence between random vectors via copula-based divergence measures.
Journal of Multivariate Analysis 203:105336. <br />
doi: https://doi.org/10.1016/j.jmva.2024.105336.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+minormal">minormal</a></code> for the computation of the Gaussian copula mutual information,
<code><a href="#topic+Helnormalavar">Helnormalavar</a></code> for the computation of the asymptotic variance of the plug-in estimator for the Gaussian copula Hellinger distance.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>q = 10
dim = c(1,2,3,4)

# AR(1) correlation matrix with correlation 0.5
R = 0.5^(abs(matrix(1:q-1,nrow = q, ncol = q, byrow = TRUE) - (1:q-1)))

Helnormal(R,dim)
</code></pre>

<hr>
<h2 id='Helnormalavar'>Helnormalavar</h2><span id='topic+Helnormalavar'></span>

<h3>Description</h3>

<p>Given a <code class="reqn">q</code>-dimensional random vector <code class="reqn">\mathbf{X} = (\mathbf{X}_{1},...,\mathbf{X}_{k})</code> with <code class="reqn">\mathbf{X}_{i}</code> a <code class="reqn">d_{i}</code>-dimensional random vector, i.e., <code class="reqn">q = d_{1} + ... + d_{k}</code>,
this function computes the asymptotic variance of the plug-in estimator for the correlation-based Hellinger distance
between <code class="reqn">\mathbf{X}_{1},...,\mathbf{X}_{k}</code> given the entire correlation matrix <code class="reqn">\mathbf{R}</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Helnormalavar(R, dim)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Helnormalavar_+3A_r">R</code></td>
<td>
<p>The correlation matrix of <code class="reqn">\mathbf{X}</code>.</p>
</td></tr>
<tr><td><code id="Helnormalavar_+3A_dim">dim</code></td>
<td>
<p>The vector of dimensions <code class="reqn">(d_{1},...,d_{k})</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The asymptotic variance of the plug-in estimator <code class="reqn">\mathcal{D}_{(\sqrt{t}-1)^{2}}(\widehat{\mathbf{R}}_{n})</code> is computed at <code class="reqn">\mathbf{R}</code>,
where <code class="reqn">\widehat{\mathbf{R}}_{n}</code> is the sample matrix of normal scores rank correlations.
The underlying assumption is that the copula of <code class="reqn">\mathbf{X}</code> is Gaussian.
</p>


<h3>Value</h3>

<p>The asymptotic variance of the correlation-based Hellinger distance between <code class="reqn">\mathbf{X}_{1},...,\mathbf{X}_{k}</code>.
</p>


<h3>References</h3>

<p>De Keyser, S. &amp; Gijbels, I. (2024).
Parametric dependence between random vectors via copula-based divergence measures.
Journal of Multivariate Analysis 203:105336. <br />
doi: https://doi.org/10.1016/j.jmva.2024.105336.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+minormal">minormal</a></code> for the computation of the mutual information,
<code><a href="#topic+Helnormal">Helnormal</a></code> for the computation of the Hellinger distance,
<code><a href="#topic+minormalavar">minormalavar</a></code> for the computation of the asymptotic variance of the plug-in estimator for the mutual information,
<code><a href="#topic+estR">estR</a></code> for the computation of the sample matrix of normal scores rank correlations.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>q = 10
dim = c(1,2,3,4)

# AR(1) correlation matrix with correlation 0.5
R = 0.5^(abs(matrix(1:q-1,nrow = q, ncol = q, byrow = TRUE) - (1:q-1)))

Helnormalavar(R,dim)
</code></pre>

<hr>
<h2 id='Icluster'>Icluster</h2><span id='topic+Icluster'></span>

<h3>Description</h3>

<p>This function clusters the columns (variables) of a dataset via agglomerative hierarchical variable clustering using estimated multivariate similarities
(dependence coefficients) between random vectors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Icluster(
  data,
  est_method,
  max_dim = Inf,
  norm = NULL,
  link = "average",
  trace = 1
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Icluster_+3A_data">data</code></td>
<td>
<p>The dataset (<code class="reqn">n \times q</code> matrix with observations in rows, variables in columns) whose columns need to be clustered.</p>
</td></tr>
<tr><td><code id="Icluster_+3A_est_method">est_method</code></td>
<td>
<p>The method for estimating the similarity between two clusters of variables.</p>
</td></tr>
<tr><td><code id="Icluster_+3A_max_dim">max_dim</code></td>
<td>
<p>The maximum dimension of the random vectors for which no link function is used when computing the similarity (default = Inf).</p>
</td></tr>
<tr><td><code id="Icluster_+3A_norm">norm</code></td>
<td>
<p>A possible normalization function applied to the dependence measure (default = NULL, meaning no normalization).</p>
</td></tr>
<tr><td><code id="Icluster_+3A_link">link</code></td>
<td>
<p>The link function to be used when max_dim is exceeded (default = &quot;average&quot;).</p>
</td></tr>
<tr><td><code id="Icluster_+3A_trace">trace</code></td>
<td>
<p>Controls how verbose output should be (default = 1, showing the progress).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Suppose that the <code class="reqn">q</code> variables (of which we have <code class="reqn">n</code> observations in data) are <code class="reqn">\mathcal{S} = \{X_{1}, \dots, X_{q}\}</code>.
Then, most important in hierarchical variable clustering, is computing the similarity
</p>
<p style="text-align: center;"><code class="reqn">\mathcal{D}(\mathbb{X},\mathbb{Y})</code>
</p>
<p> between two disjoint subsets of variables <code class="reqn">\mathbb{X}, \mathbb{Y} \subset \mathcal{S}</code>.
In particular, the main algorithm is as follows:
</p>

<ul>
<li><p> Each object <code class="reqn">\{X_{i}\}</code> forms a separate cluster, i.e., <code class="reqn">\aleph_{1} = \{\{X_{1}\},\dots,\{X_{q}\}\}</code> is the initial feature partition.</p>
</li></ul>

<p>For <code class="reqn">i = 1,2,\dots,q-1</code>:
</p>

<ul>
<li><p> For each pair of disjoint clusters <code class="reqn">\mathbb{X},\mathbb{Y} \in \aleph_{i}</code>, compute the similarity <code class="reqn">\mathcal{D}(\mathbb{X},\mathbb{Y})</code>.
</p>
</li>
<li><p> Define <code class="reqn">\aleph_{i+1} = (\aleph_{i} \setminus \{\widetilde{\mathbb{X}},\widetilde{\mathbb{Y}}\}) \cup \{\widetilde{\mathbb{X}} \cup \widetilde{\mathbb{Y}} \}</code>, where <code class="reqn">\widetilde{\mathbb{X}},\widetilde{\mathbb{Y}}</code> are the clusters having maximal similarity according to the previous step.
</p>
</li>
<li><p> The algorithm stops with <code class="reqn">\aleph_{q} = \{\{X_{1},\dots,X_{q}\}\}</code>.</p>
</li></ul>

<p>We call <code class="reqn">\{\aleph_{1}, \dots, \aleph_{q}\}</code> the hierarchy constructed throughout the algorithm, and define, for <code class="reqn">i \in \{1, \dots, q\}</code>,
<code class="reqn">\text{Adiam}(\aleph_{i}) = |\aleph_{i}|^{-1} \sum_{\mathbb{X} \in \aleph_{i}} \text{diam}(\mathbb{X})</code>, with
</p>
<p style="text-align: center;"><code class="reqn">\text{diam}(\mathbb{X}) = \begin{cases} \underset{\{X,Y\} \subset \mathbb{X} }{\min} \mathcal{D}(X,Y)  &amp; \mbox{if } |\mathbb{X}| &gt; 1 \\ 1 &amp; \mbox{if } |\mathbb{X}| = 1, \end{cases}</code>
</p>

<p>and <code class="reqn">\text{Msplit}(\aleph_{i}) = \max_{\mathbb{X} \in \aleph_{i}} \text{split}(\mathbb{X})</code>, with
</p>
<p style="text-align: center;"><code class="reqn">\text{split}(\mathbb{X}) =  \underset{Y \in \aleph_{i} \setminus \mathbb{X}}{\underset{X \in \mathbb{X}}{\max}} \mathcal{D}(X,Y) \hspace{0.2cm} \text{for} \hspace{0.2cm} \{\mathbb{X}\} \neq \aleph_{i}.</code>
</p>

<p>Adiam stands for the average diameter of a partition (measuring the homogeneity, which we want to be large), while
Msplit stands for the maximum split of a partition (measuring the non-separation, which we want to be small).
</p>
<p>For measuring the similarity <code class="reqn">\mathcal{D}(\mathbb{X},\mathbb{Y})</code>, we approach <code class="reqn">\mathbb{X}</code> and <code class="reqn">\mathbb{Y}</code> as being two random vectors
(let's say of dimensions <code class="reqn">d_{1}</code> and <code class="reqn">d_{2}</code> respectively). For <code class="reqn">\mathcal{D}</code>, we take an estimated dependence measure between (two) random vectors. The following options are possible:
</p>

<ul>
<li><p>  list(&quot;phi&quot;, &quot;mi&quot;, &quot;Gauss&quot;, omegas = omegas) for the estimated Gaussian copula mutual information. Use omegas = 1 for no penalty, or a sequence of omegas for a ridge penalty tuned via 5-fold cross-validation,
see also the functions <code><a href="#topic+minormal">minormal</a></code>, <code><a href="#topic+estR">estR</a></code>, and <code><a href="#topic+cvomega">cvomega</a></code>,
</p>
</li>
<li><p> list(&quot;phi&quot;, &quot;Hel&quot;, &quot;Gauss&quot;, omegas = omegas) for the estimated Gaussian copula Hellinger distance. Use omegas = 1 for no penalty, or a sequence of omegas for a ridge penalty tuned via 5-fold cross-validation,
see also the functions <code><a href="#topic+Helnormal">Helnormal</a></code>, <code><a href="#topic+estR">estR</a></code>, and <code><a href="#topic+cvomega">cvomega</a></code>,
</p>
</li>
<li><p> list(&quot;phi&quot;, phi(t), &quot;hac&quot;, type = type, M = M) for general <code class="reqn">\Phi</code>-dependence with specified function phi(t) = <code class="reqn">\Phi(t)</code>,
estimated by fitting (via pseudo maximum likelihood estimation) a hierarchical Archimedean copula of given type = type,
and computed based on a Monte Carlo sample of size <code class="reqn">M</code> in order to approximate the expectation, see also the functions <code><a href="#topic+mlehac">mlehac</a></code>, <code><a href="#topic+phihac">phihac</a></code> and <code><a href="#topic+estphi">estphi</a></code>,
</p>
</li>
<li><p> list(&quot;phi&quot;, phi(t), &quot;nphac&quot;, estimator = estimator, type = type) for general
<code class="reqn">\Phi</code>-dependence with specified function phi(t) = <code class="reqn">\Phi(t)</code>,
estimated via non-parametric beta kernel estimation or Gaussian transformation kernel estimation,
and local bandwidth selection, by using a fitted (via pseudo maximum likelihood) hierarchical Archimedean copula
as reference copula, see also the functions <code><a href="#topic+phinp">phinp</a></code> and <code><a href="#topic+estphi">estphi</a></code>,
</p>
</li>
<li><p> list(&quot;phi&quot;, phi(t), &quot;np&quot;, estimator = estimator, bw_method = bw_method) for general <br />
<code class="reqn">\Phi</code>-dependence with specified function phi(t) = <code class="reqn">\Phi(t)</code>,
estimated via non-parametric beta kernel estimation or Gaussian transformation kernel estimation,
and local bandwidth selection, either by using a non-parametric kernel estimator as reference copula if bw_method = 1,
or by using a big O bandwidth rule if bw_method = 2, see also the functions <code><a href="#topic+phinp">phinp</a></code> and <code><a href="#topic+estphi">estphi</a></code>,
</p>
</li>
<li><p> list(&quot;phi&quot;, phi(t), &quot;ellip&quot;, grid = grid) for general <code class="reqn">\Phi</code>-dependence with specified function phi(t) = <code class="reqn">\Phi(t)</code>,
estimated via the improved MECIP procedure on the specified grid, and parameter selection done via
the function <code><a href="#topic+elliptselect">elliptselect</a></code> using the Gaussian generator as reference generator, see also the functions <code><a href="#topic+phiellip">phiellip</a></code> and <code><a href="#topic+estphi">estphi</a></code>,
</p>
</li>
<li><p> list(&quot;ot&quot;, coef = coef, omegas = omegas) for Gaussian copula Bures-Wasserstein dependence measures, either coefficient <code class="reqn">\mathcal{D}_{1}</code> (coef = 1) or coefficient <code class="reqn">\mathcal{D}_{2}</code> (coef = 2).
Use omegas = 1 for no penalty, or a sequence of omegas for a ridge penalty tuned via 5-fold cross-validation,
see also the functions <code><a href="#topic+bwd1">bwd1</a></code>, <code><a href="#topic+bwd2">bwd2</a></code>, <code><a href="#topic+estR">estR</a></code>, and <code><a href="#topic+cvomega">cvomega</a></code>.</p>
</li></ul>

<p>When <code class="reqn">d_{1} + d_{2} &gt;</code> max_dim, the specified link function (say <code class="reqn">L</code>) is used
for computing the similarity between <code class="reqn">\mathbb{X}</code> and <code class="reqn">\mathbb{Y}</code>, i.e.,
</p>
<p style="text-align: center;"><code class="reqn">\mathcal{D} \left ( \mathbb{X}, \mathbb{Y} \right ) = L \left (\left \{\mathcal{D}(X,Y) : X \in \mathbb{X}, Y \in \mathbb{Y} \right \} \right ),</code>
</p>

<p>which by default is the average of all inter-pairwise similarities. Other options are &quot;single&quot; for the minimum, and &quot;complete&quot; for the maximum.
</p>
<p>The function norm (say <code class="reqn">N</code>) is a possible normalization applied to the similarity measure, i.e., instead of
computing <code class="reqn">\mathcal{D}</code> (using the method specified by est_method), the similarity becomes <code class="reqn">N \circ \mathcal{D}</code>.
The default is <code class="reqn">N(t) = t</code>, meaning that no normalization is applied.
</p>


<h3>Value</h3>

<p>A list with elements &quot;hierarchy&quot; containing the hierarchy constructed throughout the algorithm (a hash object), &quot;all&quot; containing all similarities that were computed throughout the algorithm (a hash object),
&quot;diam&quot; containing the average diameters of all partitions created throughout the algorithm (a vector), and &quot;split&quot; containing the maximum splits of all partitions created throughout the algorithm (a vector).
</p>


<h3>References</h3>

<p>De Keyser, S. &amp; Gijbels, I. (2024).
Hierarchical variable clustering via copula-based divergence measures between random vectors.
International Journal of Approximate Reasoning 165:109090.
doi: https://doi.org/10.1016/j.ijar.2023.109090.
</p>
<p>De Keyser, S. &amp; Gijbels, I. (2024).
Parametric dependence between random vectors via copula-based divergence measures.
Journal of Multivariate Analysis 203:105336. <br />
doi: https://doi.org/10.1016/j.jmva.2024.105336.
</p>
<p>De Keyser, S. &amp; Gijbels, I. (2024).
High-dimensional copula-based Wasserstein dependence.
doi: https://doi.org/10.48550/arXiv.2404.07141.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+minormal">minormal</a></code> for the computation of the Gaussian copula mutual information,
<code><a href="#topic+Helnormal">Helnormal</a></code> for the computation of the Gaussian copula Hellinger distance,
<code><a href="#topic+estphi">estphi</a></code> for several approach to estimating the <code class="reqn">\Phi</code>-dependence between <code class="reqn">k</code> random vectors,
<code><a href="#topic+bwd1">bwd1</a></code> for the computation of the first Bures-Wasserstein dependence coefficient <code class="reqn">\mathcal{D}_{1}</code>,
<code><a href="#topic+bwd2">bwd2</a></code> for the computation of the second Bures-Wasserstein dependence coefficient <code class="reqn">\mathcal{D}_{2}</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
q = 20

# We will impose a clustering
# {{X1,X2},{X3,X4,X5},{X6,X7,X8},{X9,X10,X11,X12,X13},{X14,X15,X16,X17,X18,X19,X20}}
dim = c(2,3,3,5,7)

# Sample size
n = 200

# Twenty dimensional hierarchical Gumbel copula with parameters
# (theta_0,theta_1,theta_2,theta_3,theta_4,theta_5) = (2,3,4,5,6,7)
hac = gethac(dim,c(2,3,4,5,6,7),type = 1)

# So, strongest cluster is {X14,X15,X16,X17,X18,X19,X20}, then {X9,X10,X11,X12,X13},
# then {X6,X7,X8}, then {X3,X4,X5}, and finally {X1,X2}

# Sample
sample =  suppressWarnings(HAC::rHAC(n,hac))

# Cluster using different methods

# Gaussian copula based methods

Clustering1 =  Icluster(data = sample,
                        est_method = list("phi", "mi", "Gauss", omegas = 1))

# 5-cluster partition
Clustering1$hierarchy$Aleph_16

Clustering2 =  Icluster(data = sample,
                        est_method = list("phi", "mi", "Gauss",
                                          omegas = seq(0.01,0.999,len = 50)))

# 5-cluster partition
Clustering2$hierarchy$Aleph_16

Clustering3 =  Icluster(data = sample,
                        est_method = list("phi", "mi", "Gauss", omegas = 1),
                        max_dim = 2)

# 5-cluster partition
Clustering3$hierarchy$Aleph_16

Clustering4 =  Icluster(data = sample,
                        est_method = list("phi", "Hel", "Gauss", omegas = 1))

# 5-cluster partition
Clustering4$hierarchy$Aleph_16

Clustering5 =  Icluster(data = sample,
                        est_method = list("ot", coef = 1, omegas = 1))

# 5-cluster partition
Clustering5$hierarchy$Aleph_16

Clustering6 =  Icluster(data = sample,
                        est_method = list("ot", coef = 2, omegas = 1))

# 5-cluster partition
Clustering6$hierarchy$Aleph_16

Clustering7 =  Icluster(data = sample,
                        est_method = list("ot", coef = 2, omegas = 1), max_dim = 4)

# 5-cluster partition
Clustering7$hierarchy$Aleph_16

# Parametric hierarchical Archimedean copula approach

Clustering8 = Icluster(data = sample,
                       est_method = list("phi", function(t){t * log(t)}, "hac",
                                         type = 1, M = 1000), max_dim = 4)

# 5-cluster partition
Clustering8$hierarchy$Aleph_16

Clustering9 = Icluster(data = sample,
                       est_method = list("phi", function(t){(sqrt(t)-1)^2}, "hac",
                                         type = 1, M = 1000), max_dim = 2)

# 5-cluster partition
Clustering9$hierarchy$Aleph_16

# Non-parametric approaches

Clustering10 = Icluster(data = sample,
                        est_method = list("phi", function(t){t * log(t)}, "nphac",
                                       estimator = "beta", type = 1), max_dim = 3)

# 5-cluster partition
Clustering10$hierarchy$Aleph_16

Clustering11 = Icluster(data = sample,
                        est_method = list("phi", function(t){t * log(t)}, "nphac",
                                     estimator = "trans", type = 1), max_dim = 3)

# 5-cluster partition
Clustering11$hierarchy$Aleph_16

Clustering12 = Icluster(data = sample,
                        est_method = list("phi", function(t){t * log(t)}, "np",
                                     estimator = "beta", bw_method = 1), max_dim = 3)

# 5-cluster partition
Clustering12$hierarchy$Aleph_16

Clustering13 = Icluster(data = sample,
                        est_method = list("phi", function(t){t * log(t)}, "np",
                                     estimator = "trans", bw_method = 2), max_dim = 3)

# 5-cluster partition
Clustering13$hierarchy$Aleph_16

Clustering14 = Icluster(data = sample,
                        est_method = list("phi", function(t){(sqrt(t)-1)^2}, "np",
                                      estimator = "trans", bw_method = 1), max_dim = 2)

# 5-cluster partition
Clustering14$hierarchy$Aleph_16

# Semi-parametric meta-elliptical copula approach
# Uncomment to run (takes a long time)

# Clustering15 = Icluster(data = sample,
#                        est_method = list("phi", function(t){t * log(t)}, "ellip",
#                                     grid = seq(0.005,100,by = 0.005)), max_dim = 2)

# 5-cluster partition
# Clustering15$hierarchy$Aleph_16



</code></pre>

<hr>
<h2 id='install_tensorflow'>install_tensorflow</h2><span id='topic+install_tensorflow'></span>

<h3>Description</h3>

<p>This function installs a python virtual environment.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>install_tensorflow(envname = "r-tensorflow")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="install_tensorflow_+3A_envname">envname</code></td>
<td>
<p>Name of the environment.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>No return value, used for creating a python virtual environment.
</p>

<hr>
<h2 id='minormal'>minormal</h2><span id='topic+minormal'></span>

<h3>Description</h3>

<p>Given a <code class="reqn">q</code>-dimensional random vector <code class="reqn">\mathbf{X} = (\mathbf{X}_{1},...,\mathbf{X}_{k})</code> with <code class="reqn">\mathbf{X}_{i}</code> a <code class="reqn">d_{i}</code>-dimensional random vector, i.e., <code class="reqn">q = d_{1} + ... + d_{k}</code>,
this function computes the correlation-based mutual information between <code class="reqn">\mathbf{X}_{1},...,\mathbf{X}_{k}</code> given the entire correlation matrix <code class="reqn">\mathbf{R}</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>minormal(R, dim)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="minormal_+3A_r">R</code></td>
<td>
<p>The correlation matrix of <code class="reqn">\mathbf{X}</code>.</p>
</td></tr>
<tr><td><code id="minormal_+3A_dim">dim</code></td>
<td>
<p>The vector of dimensions <code class="reqn">(d_{1},...,d_{k})</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Given a correlation matrix </p>
<p style="text-align: center;"><code class="reqn">\mathbf{R} = \begin{pmatrix} \mathbf{R}_{11} &amp; \mathbf{R}_{12} &amp; \cdots &amp; \mathbf{R}_{1k} \\
                                                             \mathbf{R}_{12}^{\text{T}} &amp; \mathbf{R}_{22} &amp; \cdots &amp; \mathbf{R}_{2k} \\
                                                             \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
                                                             \mathbf{R}_{1k}^{\text{T}} &amp; \mathbf{R}_{2k}^{\text{T}} &amp; \cdots &amp; \mathbf{R}_{kk} \end{pmatrix},</code>
</p>

<p>the mutual information equals </p>
<p style="text-align: center;"><code class="reqn">\mathcal{D}_{t \ln(t)}^{\mathcal{N}}(\mathbf{R}) = - \frac{1}{2} \ln \left (\frac{|\mathbf{R}|}{\prod_{i = 1}^{k} \left |\mathbf{R}_{ii} \right |} \right ).</code>
</p>

<p>The underlying assumption is that the copula of <code class="reqn">\mathbf{X}</code> is Gaussian.
</p>


<h3>Value</h3>

<p>The correlation-based mutual information between <code class="reqn">\mathbf{X}_{1},...,\mathbf{X}_{k}</code>.
</p>


<h3>References</h3>

<p>De Keyser, S. &amp; Gijbels, I. (2024).
Parametric dependence between random vectors via copula-based divergence measures.
Journal of Multivariate Analysis 203:105336. <br />
doi: https://doi.org/10.1016/j.jmva.2024.105336.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Helnormal">Helnormal</a></code> for the computation of the Gaussian copula Hellinger distance,
<code><a href="#topic+minormalavar">minormalavar</a></code> for the computation of the asymptotic variance of the plug-in estimator for the Gaussian copula mutual information,
<code><a href="#topic+miStudent">miStudent</a></code> for the computation of the Student-t mutual information.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>q = 10
dim = c(1,2,3,4)
# AR(1) correlation matrix with correlation 0.5

R = 0.5^(abs(matrix(1:q-1,nrow = q, ncol = q, byrow = TRUE) - (1:q-1)))

minormal(R,dim)
</code></pre>

<hr>
<h2 id='minormalavar'>minormalavar</h2><span id='topic+minormalavar'></span>

<h3>Description</h3>

<p>Given a <code class="reqn">q</code>-dimensional random vector <code class="reqn">\mathbf{X} = (\mathbf{X}_{1},...,\mathbf{X}_{k})</code> with <code class="reqn">\mathbf{X}_{i}</code> a <code class="reqn">d_{i}</code>-dimensional random vector, i.e., <code class="reqn">q = d_{1} + ... + d_{k}</code>,
this function computes the asymptotic variance of the plug-in estimator for the correlation-based mutual information
between <code class="reqn">\mathbf{X}_{1},...,\mathbf{X}_{k}</code> given the entire correlation matrix <code class="reqn">\mathbf{R}</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>minormalavar(R, dim)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="minormalavar_+3A_r">R</code></td>
<td>
<p>The correlation matrix of <code class="reqn">\mathbf{X}</code>.</p>
</td></tr>
<tr><td><code id="minormalavar_+3A_dim">dim</code></td>
<td>
<p>The vector of dimensions <code class="reqn">(d_{1},...,d_{k})</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The asymptotic variance of the plug-in estimator <code class="reqn">\mathcal{D}_{t \ln(t)}(\widehat{\mathbf{R}}_{n})</code> is computed at <code class="reqn">\mathbf{R}</code>,
where <code class="reqn">\widehat{\mathbf{R}}_{n}</code> is the sample matrix of normal scores rank correlations.
The underlying assumption is that the copula of <code class="reqn">\mathbf{X}</code> is Gaussian.
</p>


<h3>Value</h3>

<p>The asymptotic variance of the correlation-based mutual information between <code class="reqn">\mathbf{X}_{1},...,\mathbf{X}_{k}</code>.
</p>


<h3>References</h3>

<p>De Keyser, S. &amp; Gijbels, I. (2024).
Parametric dependence between random vectors via copula-based divergence measures.
Journal of Multivariate Analysis 203:105336. <br />
doi: https://doi.org/10.1016/j.jmva.2024.105336.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+minormal">minormal</a></code> for the computation of the mutual information,
<code><a href="#topic+Helnormal">Helnormal</a></code> for the computation of the Hellinger distance,
<code><a href="#topic+Helnormalavar">Helnormalavar</a></code> for the computation of the asymptotic variance of the plug-in estimator for the Hellinger distance,
<code><a href="#topic+estR">estR</a></code> for the computation of the sample matrix of normal scores rank correlations.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>q = 10
dim = c(1,2,3,4)

# AR(1) correlation matrix with correlation 0.5
R = 0.5^(abs(matrix(1:q-1,nrow = q, ncol = q, byrow = TRUE) - (1:q-1)))

minormalavar(R,dim)
</code></pre>

<hr>
<h2 id='miStudent'>miStudent</h2><span id='topic+miStudent'></span>

<h3>Description</h3>

<p>Given a <code class="reqn">q</code>-dimensional random vector <code class="reqn">\mathbf{X} = (\mathbf{X}_{1},...,\mathbf{X}_{k})</code> with <code class="reqn">\mathbf{X}_{i}</code> a <code class="reqn">d_{i}</code>-dimensional random vector, i.e., <code class="reqn">q = d_{1} + ... + d_{k}</code>,
this function computes the Student-t mutual information between <code class="reqn">\mathbf{X}_{1},...,\mathbf{X}_{k}</code> given the entire correlation matrix <code class="reqn">\mathbf{R}</code> and the degrees of freedom nu.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>miStudent(R, dim, nu)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="miStudent_+3A_r">R</code></td>
<td>
<p>The correlation matrix of <code class="reqn">\mathbf{X}</code>.</p>
</td></tr>
<tr><td><code id="miStudent_+3A_dim">dim</code></td>
<td>
<p>The vector of dimensions <code class="reqn">(d_{1},...,d_{k})</code>.</p>
</td></tr>
<tr><td><code id="miStudent_+3A_nu">nu</code></td>
<td>
<p>The degrees of freedom.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Given a correlation matrix </p>
<p style="text-align: center;"><code class="reqn">\mathbf{R} = \begin{pmatrix} \mathbf{R}_{11} &amp; \mathbf{R}_{12} &amp; \cdots &amp; \mathbf{R}_{1k} \\
                                                             \mathbf{R}_{12}^{\text{T}} &amp; \mathbf{R}_{22} &amp; \cdots &amp; \mathbf{R}_{2k} \\
                                                             \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
                                                             \mathbf{R}_{1k}^{\text{T}} &amp; \mathbf{R}_{2k}^{\text{T}} &amp; \cdots &amp; \mathbf{R}_{kk} \end{pmatrix},</code>
</p>

<p>and a certain amount of degrees of freedom <code class="reqn">\nu &gt; 0</code>,
the Student-t mutual information equals </p>
<p style="text-align: center;"><code class="reqn">\mathcal{D}_{t \ln(t)}^{\text{S}}(\mathbf{R},\nu) = - \frac{1}{2} \ln \left (\frac{|\mathbf{R}|}{\prod_{i = 1}^{k} \left |\mathbf{R}_{ii} \right |} \right ) + K(\nu),</code>
</p>

<p>where
</p>
<p style="text-align: center;"><code class="reqn">\hspace{-2cm} K(\nu) = \ln \left (\frac{\Gamma((q+\nu)/2) \Gamma(\nu/2)^{k-1}}{\prod_{i = 1}^{k} \Gamma((d_{i} + \nu)/2)} \right ) + \sum_{i = 1}^{k} \left [\frac{d_{i} + \nu}{2} \psi((d_{i} + \nu)/2) \right ]</code>
</p>

<p style="text-align: center;"><code class="reqn">\hspace{5.4cm} - \frac{q + \nu}{2} \psi((q + \nu)/2) - \frac{\nu}{2}(k-1)\psi(\nu/2),</code>
</p>

<p>with <code class="reqn">\Gamma</code> the gamma function and <code class="reqn">\psi</code> the digamma function.
The underlying assumption is that the copula of <code class="reqn">\mathbf{X}</code> is Student-t.
</p>


<h3>Value</h3>

<p>The Student-t mutual information between <code class="reqn">\mathbf{X}_{1},...,\mathbf{X}_{k}</code>.
</p>


<h3>References</h3>

<p>De Keyser, S. &amp; Gijbels, I. (2024).
Hierarchical variable clustering via copula-based divergence measures between random vectors.
International Journal of Approximate Reasoning 165:109090.
doi: https://doi.org/10.1016/j.ijar.2023.109090.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+minormal">minormal</a></code> for the computation of the Gaussian copula mutual information.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>q = 10
dim = c(1,2,3,4)

# AR(1) correlation matrix with correlation 0.5
R = 0.5^(abs(matrix(1:q-1,nrow = q, ncol = q, byrow = TRUE) - (1:q-1)))

# Degrees of freedom
nu = 7

miStudent(R,dim,nu)
</code></pre>

<hr>
<h2 id='mlehac'>mlehac</h2><span id='topic+mlehac'></span>

<h3>Description</h3>

<p>Given a <code class="reqn">q</code>-dimensional random vector <code class="reqn">\mathbf{X} = (\mathbf{X}_{1},...,\mathbf{X}_{k})</code> with <code class="reqn">\mathbf{X}_{i}</code> a <code class="reqn">d_{i}</code>-dimensional random vector, i.e., <code class="reqn">q = d_{1} + ... + d_{k}</code>,
this function performs maximum pseudo-likelihood estimation for the parameters of
a hierarchical Archimedean copula with two nesting levels of a specific type, used for modelling the dependence between <code class="reqn">\mathbf{X}_{1}, \dots, \mathbf{X}_{k}</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mlehac(sample, dim, type, start_val = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mlehac_+3A_sample">sample</code></td>
<td>
<p>A sample from a <code class="reqn">q</code>-dimensional random vector <code class="reqn">\mathbf{X}</code> (<code class="reqn">n \times q</code> matrix with observations in rows, variables in columns).</p>
</td></tr>
<tr><td><code id="mlehac_+3A_dim">dim</code></td>
<td>
<p>The vector of dimensions <code class="reqn">(d_{1},...,d_{k})</code>.</p>
</td></tr>
<tr><td><code id="mlehac_+3A_type">type</code></td>
<td>
<p>The type of Archimedean copula.</p>
</td></tr>
<tr><td><code id="mlehac_+3A_start_val">start_val</code></td>
<td>
<p>The starting values for the parameters <code class="reqn">(\theta_{0},\theta_{1},...,\theta_{k})</code> of the <br />
hierarchical Archimedean copula.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Under the assumption that <code class="reqn">\mathbf{X} = (\mathbf{X}_{1}, \dots, \mathbf{X}_{k})</code> has a hierarchical Archimedean copula with two nesting levels, i.e.,
</p>
<p style="text-align: center;"><code class="reqn">C(\mathbf{u}) = C_{0} \left (C_{1}(\mathbf{u}_{1}), \dots, C_{k}(\mathbf{u}_{k}) \right ),</code>
</p>

<p>where <code class="reqn">\mathbf{u} = (\mathbf{u}_{1}, \dots, \mathbf{u}_{k}) \in \mathbb{R}^{q}</code> with <code class="reqn">\mathbf{u}_{i} \in \mathbb{R}^{d_{i}}</code> for <code class="reqn">i = 1, \dots, k</code>,
and with <code class="reqn">\theta_{i}</code> the parameter of <code class="reqn">C_{i}</code> for <code class="reqn">i = 0,1, \dots, k</code> (see the function <code><a href="#topic+gethac">gethac</a></code>), this functions performs maximum pseudo-likelihood estimation for
<code class="reqn">\boldsymbol{\theta}_{C} = (\theta_{0}, \theta_{1}, \dots, \theta_{k})</code>. This means that for <code class="reqn">\widehat{F}_{ij}(x_{ij}) = \frac{1}{n+1} \sum_{\ell = 1}^{n} 1 \left (X_{ij}^{(\ell)} \leq x_{ij} \right )</code> the (rescaled) empirical cdf of <code class="reqn">X_{ij}</code> based on a sample <code class="reqn">X_{ij}^{(1)}, \dots, X_{ij}^{(n)}</code>
for <code class="reqn">i = 1, \dots, k</code> and <code class="reqn">j = 1, \dots, d_{i}</code> (recall that <code class="reqn">\mathbf{X}_{i} = (X_{i1}, \dots, X_{id_{i}})</code>),
we look for </p>
<p style="text-align: center;"><code class="reqn">\widehat{\boldsymbol{\theta}}_{C,n}^{\text{NP}} = \text{arg max}_{\boldsymbol{\theta}_{C}} \sum_{\ell = 1}^{n} \ln \left \{c \left ( \widehat{F}_{11} \left (X_{11}^{(\ell)} \right ), \dots, \widehat{F}_{kd_{k}} \left (X_{kd_{k}}^{(\ell)} \right ) ; \boldsymbol{\theta}_{C} \right ) \right \},</code>
</p>

<p>where <code class="reqn">c( \cdot ; \boldsymbol{\theta}_{C})</code> is the copula density of the hierarchical Archimedean copula.
</p>
<p>We assume that <code class="reqn">C_{i}</code> belongs to the same family of Archimedean copulas (e.g., Clayton) for <code class="reqn">i = 0, \dots, k</code>,
and make use of the R package &lsquo;HAC&rsquo;.
</p>
<p>In case the starting values (start_val) are not specified, the starting value for <code class="reqn">\theta_{0}</code> is put equal to 1.9
and the starting values for <code class="reqn">\theta_{i}</code> with <code class="reqn">i \in \{1, \dots, k \}</code> are determined by performing
maximum pseudo-likelihood estimation to the <code class="reqn">d_{i}</code>-dimensional marginals with starting value <code class="reqn">2</code>.
</p>


<h3>Value</h3>

<p>The maximum pseudo-likelihood estimates for <code class="reqn">(\theta_{0},\theta_{1}, \dots, \theta_{k})</code>.
</p>


<h3>References</h3>

<p>De Keyser, S. &amp; Gijbels, I. (2024).
Parametric dependence between random vectors via copula-based divergence measures.
Journal of Multivariate Analysis 203:105336. <br />
doi: https://doi.org/10.1016/j.jmva.2024.105336.
</p>
<p>Okhrin, O., Ristig, A. &amp; Chen, G. (2024).
HAC: estimation, simulation and visualization of hierarchical Archimedean copulae (HAC), R package version 1.1-1. <br />
url:  https://CRAN.R-project.org/package=HAC.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gethac">gethac</a></code> for creating a hac object with two nesting levels,
<code><a href="#topic+phihac">phihac</a></code> for computing the <code class="reqn">\Phi</code>-dependence between all the child copulas of a hac object with two nesting levels,
<code><a href="#topic+Helhac">Helhac</a></code> for computing the Hellinger distance between all the child copulas of a hac object with two nesting levels.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dim = c(2,2)
thetas = c(2,3,4)

# Sample size
n = 1000

# 4 dimensional nested Gumbel copula with (theta_0,theta_1,theta_2) = (2,3,4)
HAC = gethac(dim,thetas,type = 1)

# Sample
sample = suppressWarnings(HAC::rHAC(n,HAC))

# Maximum pseudo-likelihood estimator with starting values equal to thetas
HAC_est_1 = mlehac(sample,dim,1,thetas)

# Maximum pseudo-likelihood estimator with starting values
# theta_0 = 1.9, and theta_1, theta_2 determined by maximum
# pseudo-likelihood estimation for marginal child copulas

HAC_est_2 = mlehac(sample,dim,1)


</code></pre>

<hr>
<h2 id='otsort'>otsort</h2><span id='topic+otsort'></span>

<h3>Description</h3>

<p>Given a <code class="reqn">q</code>-dimensional random vector <code class="reqn">\mathbf{X} = (\mathbf{X}_{1},...,\mathbf{X}_{k})</code> with <code class="reqn">\mathbf{X}_{i}</code> a <code class="reqn">d_{i}</code>-dimensional random vector, i.e., <code class="reqn">q = d_{1} + ... + d_{k}</code>,
this function sorts the columns (variables) of a sample of <code class="reqn">\mathbf{X}</code> such that the dimensions are in ascending order.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>otsort(sample, dim)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="otsort_+3A_sample">sample</code></td>
<td>
<p>A sample from a <code class="reqn">q</code>-dimensional random vector <code class="reqn">\mathbf{X}</code> (<code class="reqn">n \times q</code> matrix with observations in rows, variables in columns).</p>
</td></tr>
<tr><td><code id="otsort_+3A_dim">dim</code></td>
<td>
<p>The vector of dimensions <code class="reqn">(d_{1},...,d_{k})</code>, in order as given in sample.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The columns of sample are rearranged such that the data corresponding to the random vector <code class="reqn">\mathbf{X}_{i}</code>
having the smallest dimension <code class="reqn">d_{i}</code> comes first, then the random vector with second smallest dimension, and so on.
</p>


<h3>Value</h3>

<p>A list with elements &quot;sample&quot; containing the ordered sample, and &quot;dim&quot; containing the ordered dimensions.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>q = 10
n = 50
dim = c(2,3,1,4)

# Sample from multivariate normal distribution
sample = mvtnorm::rmvnorm(n,rep(0,q),diag(q), method = "chol")

ordered = otsort(sample,dim)
</code></pre>

<hr>
<h2 id='phiellip'>phiellip</h2><span id='topic+phiellip'></span>

<h3>Description</h3>

<p>Given a <code class="reqn">q</code>-dimensional random vector <code class="reqn">\mathbf{X} = (\mathbf{X}_{1},...,\mathbf{X}_{k})</code> with <code class="reqn">\mathbf{X}_{i}</code> a <code class="reqn">d_{i}</code>-dimensional random vector, i.e., <code class="reqn">q = d_{1} + ... + d_{k}</code>,
this function estimates the <code class="reqn">\Phi</code>-dependence between <code class="reqn">\mathbf{X}_{1},...,\mathbf{X}_{k}</code> by estimating the joint and marginal
meta-elliptical copula generators via the MECIP.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>phiellip(sample, dim, phi, grid, params, normalize = 1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="phiellip_+3A_sample">sample</code></td>
<td>
<p>A sample from a <code class="reqn">q</code>-dimensional random vector <code class="reqn">\mathbf{X}</code> (<code class="reqn">n \times q</code> matrix with observations in rows, variables in columns).</p>
</td></tr>
<tr><td><code id="phiellip_+3A_dim">dim</code></td>
<td>
<p>The vector of dimensions <code class="reqn">(d_{1},...,d_{k})</code>.</p>
</td></tr>
<tr><td><code id="phiellip_+3A_phi">phi</code></td>
<td>
<p>The function <code class="reqn">\Phi</code>.</p>
</td></tr>
<tr><td><code id="phiellip_+3A_grid">grid</code></td>
<td>
<p>The grid of values on which to estimate the density generators.</p>
</td></tr>
<tr><td><code id="phiellip_+3A_params">params</code></td>
<td>
<p>The tuning parameters to be used when estimating the density generators.</p>
</td></tr>
<tr><td><code id="phiellip_+3A_normalize">normalize</code></td>
<td>
<p>A value in <code class="reqn">\{1,2\}</code> indicating the normalization procedure that is applied to the estimated generator (default = 1).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When <code class="reqn">\mathbf{X} = (\mathbf{X}_{1}, \dots, \mathbf{X}_{k})</code> has a meta-elliptical copula with generator <code class="reqn">g_{\mathcal{R}}</code>, marginal generators <code class="reqn">g_{\mathcal{R}_{i}}</code> of <code class="reqn">\mathbf{X}_{i}</code> for <code class="reqn">i = 1, \dots, k</code>, and scale matrix </p>
<p style="text-align: center;"><code class="reqn">\mathbf{R} = \begin{pmatrix} \mathbf{R}_{11} &amp; \mathbf{R}_{12} &amp; \cdots &amp; \mathbf{R}_{1k} \\
                                                             \mathbf{R}_{12}^{\text{T}} &amp; \mathbf{R}_{22} &amp; \cdots &amp; \mathbf{R}_{2k} \\
                                                             \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
                                                             \mathbf{R}_{1k}^{\text{T}} &amp; \mathbf{R}_{2k}^{\text{T}} &amp; \cdots &amp; \mathbf{R}_{kk} \end{pmatrix},</code>
</p>
<p> the <code class="reqn">\Phi</code>-dependence between <code class="reqn">\mathbf{X}_{1}, \dots, \mathbf{X}_{k}</code> equals
</p>
<p style="text-align: center;"><code class="reqn">\mathcal{D}_{\Phi}\left (\mathbf{X}_{1}, \dots, \mathbf{X}_{k} \right ) = \mathbb{E} \left \{\frac{\prod_{i = 1}^{k} g_{\mathcal{R}_{i}}\left (\mathbf{Z}_{i}^{\text{T}} \mathbf{R}_{ii}^{-1} \mathbf{Z}_{i} \right ) \left | \mathbf{R} \right |^{1/2}}{g_{\mathcal{R}}\left (\mathbf{Z}^{\text{T}} \mathbf{R}^{-1} \mathbf{Z} \right ) \prod_{i = 1}^{k} \left | \mathbf{R}_{ii} \right |^{1/2}} \Phi \left (\frac{g_{\mathcal{R}} \left (\mathbf{Z}^{\text{T}} \mathbf{R}^{-1} \mathbf{Z} \right ) \prod_{i = 1}^{k} \left |\mathbf{R}_{ii} \right |^{1/2}}{\prod_{i = 1}^{k} g_{\mathcal{R}_{i}} \left (\mathbf{Z}_{i}^{\text{T}}\mathbf{R}_{ii}^{-1} \mathbf{Z}_{i} \right ) \left |\mathbf{R} \right |^{1/2} } \right )\right \},</code>
</p>

<p>where (recall that <code class="reqn">\mathbf{X}_{i} = (X_{i1}, \dots, X_{id_{i}})</code> for <code class="reqn">i = 1, \dots, k</code>)
</p>
<p style="text-align: center;"><code class="reqn">\mathbf{Z}_{i} = (Z_{i1}, \dots, Z_{id_{i}}) = \left(\left (Q \circ F_{i1} \right ) \left (X_{i1} \right ), \dots, \left (Q \circ F_{id_{i}} \right ) \left (X_{id_{i}} \right )  \right ),</code>
</p>

<p>and <code class="reqn">\mathbf{Z} = (\mathbf{Z}_{1}, \dots, \mathbf{Z}_{k})</code>, with <code class="reqn">Q</code> the quantile function corresponding to <code class="reqn">g_{\mathcal{R}}</code>.
</p>
<p>The expectation <code class="reqn">\mathbb{E}</code> is replaced by the empirical mean using the estimated sample <code class="reqn">\widehat{\mathbf{Z}}^{(1)}, \dots, \widehat{\mathbf{Z}}^{(n)}</code> with <code class="reqn">\widehat{\mathbf{Z}}^{(\ell)} = (\widehat{\mathbf{Z}}_{1}^{(\ell)}, \dots, \widehat{\mathbf{Z}}_{k}^{(\ell)})</code> for <code class="reqn">\ell = 1, \dots, n</code>, where
</p>
<p style="text-align: center;"><code class="reqn">\widehat{\mathbf{Z}}_{i}^{(\ell)} = \left (\widehat{Z}_{i1}^{(\ell)}, \dots, \widehat{Z}_{id_{i}}^{(\ell)} \right ) = \left ( \left (\widehat{Q} \circ \widehat{F}_{i1} \right ) \left (X_{i1}^{(\ell)} \right ), \dots, \left (\widehat{Q} \circ \widehat{F}_{id_{i}} \right ) \left (X_{id_{i}}^{(\ell)} \right ) \right ),</code>
</p>
<p> for <code class="reqn">i = 1, \dots, k</code>.
Here, <code class="reqn">\widehat{Q}</code> will be the quantile function corresponding to the final estimator for <code class="reqn">g_{\mathcal{R}}</code>, and </p>
<p style="text-align: center;"><code class="reqn">\widehat{F}_{ij}(x_{ij}) = \frac{1}{n+1} \sum_{\ell = 1}^{n} 1 \left (X_{ij}^{(\ell)} \leq x_{ij} \right )</code>
</p>
<p> is the (rescaled) empirical cdf of <code class="reqn">X_{ij}</code> based on a sample <code class="reqn">X_{ij}^{(1)}, \dots, X_{ij}^{(n)}</code> for <code class="reqn">i = 1, \dots, k</code> and <code class="reqn">j = 1, \dots, d_{i}</code>.
</p>
<p>The estimation of <code class="reqn">\mathbf{R}</code> is done via its relation with the Kendall's tau matrix, see the function &ldquo;KTMatrixEst.R&rdquo; in
the R package &lsquo;ElliptCopulas&rsquo; of Derumigny et al. (2024).
</p>
<p>For estimating <code class="reqn">g_{\mathcal{R}}</code> and <code class="reqn">g_{\mathcal{R}_{i}}</code> for <code class="reqn">i = 1, \dots, k</code>, the function <code><a href="#topic+ellcopest">ellcopest</a></code> is used.
This function requires certain tuning parameters (a bandwidth <code class="reqn">h</code>, a parameter <code class="reqn">a</code>, and a parameter <code class="reqn">\delta</code> for the shrinkage function). Suppose that there are
<code class="reqn">m</code> marginal random vectors (among <code class="reqn">\mathbf{X}_{1}, \dots, \mathbf{X}_{k}</code>) that are of dimension strictly larger than one.
Then, all tuning parameters should be given as
</p>
<p style="text-align: center;"><code class="reqn">\text{params} = \text{list}(\text{"h"} = (h,h_{1},\dots,h_{m}), \text{"a"} = (a,a_{1}, \dots, a_{m}), \text{"p"} = (\delta, \delta_{1}, \dots, \delta_{m})),</code>
</p>

<p>i.e., <code class="reqn">(h,a,\delta)</code> will be used for estimating <code class="reqn">g_{\mathcal{R}}</code>, and <code class="reqn">(h_{i},a_{i},\delta_{i})</code> will be used for estimating <code class="reqn">g_{\mathcal{R}_{i}}</code> for <code class="reqn">i = 1, \dots, k</code>.
</p>
<p>When <code class="reqn">d_{i} = 1</code> for a certain <code class="reqn">i \in \{1, \dots, k \}</code>, the function &ldquo;Convert_gd_To_g1.R&rdquo; from the R package &lsquo;ElliptCopulas&rsquo; is used to estimate <code class="reqn">g_{\mathcal{R}_{i}}</code>.
</p>
<p>In order to make <code class="reqn">g_{\mathcal{R}}</code> identifiable, an extra normalization procedure is implemented
in line with an extra constraint on <code class="reqn">g_{\mathcal{R}}</code>.
When normalize = 1, this corresponds to <code class="reqn">\mathbf{R}</code> being the correlation matrix of <code class="reqn">\mathbf{Z}</code>.
When normalize = 2, this corresponds to the identifiability condition of Derumigny &amp; Fermanian (2022).
</p>


<h3>Value</h3>

<p>The estimated <code class="reqn">\Phi</code>-dependence between <code class="reqn">\mathbf{X}_{1}, \dots, \mathbf{X}_{k}</code>.
</p>


<h3>References</h3>

<p>Derumigny, A., Fermanian, J.-D., Ryan, V., van der Spek, R. (2024).
ElliptCopulas, R package version 0.1.4.1.
url: https://CRAN.R-project.org/package=ElliptCopulas.
</p>
<p>De Keyser, S. &amp; Gijbels, I. (2024).
Hierarchical variable clustering via copula-based divergence measures between random vectors.
International Journal of Approximate Reasoning 165:109090.
doi: https://doi.org/10.1016/j.ijar.2023.109090.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+elldistrest">elldistrest</a></code> for improved kernel estimation of the elliptical generator of an elliptical distribution,
<code><a href="#topic+ellcopest">ellcopest</a></code> for improved kernel estimation of the elliptical generator of a meta-elliptical copula,
<code><a href="#topic+elliptselect">elliptselect</a></code> for selecting optimal tuning parameters for the improved kernel estimator of the elliptical generator.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
q = 4
dim = c(2,2)

# Sample size
n = 1000

# Grid on which to evaluate the elliptical generator
grid = seq(0.005,100,by = 0.005)

# Degrees of freedom
nu = 7

# Student-t generator with 7 degrees of freedom
g_q = ((nu/(nu-2))^(q/2))*(gamma((q+nu)/2)/(((pi*nu)^(q/2))*gamma(nu/2))) *
                          ((1+(grid/(nu-2)))^(-(q+nu)/2))

# Density of squared radius
R2 = function(t,q){(gamma((q+nu)/2)/(((nu-2)^(q/2))*gamma(nu/2)*gamma(q/2))) *
                   (t^((q/2)-1)) * ((1+(t/(nu-2)))^(-(q+nu)/2))}

# Sample from 4-dimensional Student-t distribution with 7 degrees of freedom
# and identity covariance matrix
sample = ElliptCopulas::EllDistrSim(n,q,diag(q),density_R2 = function(t){R2(t,q)})

# Tuning parameter selection for g_R
opt_parameters_joint = elliptselect(n,q,seq((3/4)-(1/q)+0.01,1-0.01,len = 200),
                                        seq(0.01,2,len = 200))

# Optimal tuning parameters for g_R
a = opt_parameters_joint$Opta ; p = opt_parameters_joint$Optp ;
                                h = opt_parameters_joint$Opth

# Tuning parameter selection for g_R_1 (same for g_R_2)
opt_parameters_marg = elliptselect(n,2,seq((3/4)-(1/2)+0.01,1-0.01,len = 200),
                                       seq(0.01,2,len = 200))

# Optimal tuning parameters for g_R_1 (same for g_R_2)
a1 = opt_parameters_marg$Opta ; p1 = opt_parameters_marg$Optp ;
                                h1 = opt_parameters_marg$Opth

a2 = a1 ; p2 = p1 ; h2 = h1
params = list("h" = c(h,h1,h2), "a" = c(a,a1,a2), "p" = c(p,p1,p2))

# Mutual information between two random vectors of size 2
est_phi = phiellip(sample, dim, function(t){t * log(t)}, grid, params)


</code></pre>

<hr>
<h2 id='phihac'>phihac</h2><span id='topic+phihac'></span>

<h3>Description</h3>

<p>This function computes the <code class="reqn">\Phi</code>-dependence between all the child copulas of a hac object obtained by the function <code><a href="#topic+gethac">gethac</a></code>, i.e.,
given a <code class="reqn">q</code>-dimensional random vector <code class="reqn">\mathbf{X} = (\mathbf{X}_{1},...,\mathbf{X}_{k})</code> with <code class="reqn">\mathbf{X}_{i}</code> a <code class="reqn">d_{i}</code>-dimensional random vector, i.e., <code class="reqn">q = d_{1} + ... + d_{k}</code>,
where <code class="reqn">\mathbf{X}_{1},...,\mathbf{X}_{k}</code> are connected via a hierarchical Archimedean copula with two nesting levels, phihac computes the <code class="reqn">\Phi</code>-dependence
between <code class="reqn">\mathbf{X}_{1},...,\mathbf{X}_{k}</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>phihac(cop, dim, M, phi)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="phihac_+3A_cop">cop</code></td>
<td>
<p>A hac object as provided by the function <code><a href="#topic+gethac">gethac</a></code>.</p>
</td></tr>
<tr><td><code id="phihac_+3A_dim">dim</code></td>
<td>
<p>The vector of dimensions <code class="reqn">(d_{1},...,d_{k})</code>.</p>
</td></tr>
<tr><td><code id="phihac_+3A_m">M</code></td>
<td>
<p>The size of the Monte Carlo sample used for approximating the integral of the <code class="reqn">\Phi</code>-dependence.</p>
</td></tr>
<tr><td><code id="phihac_+3A_phi">phi</code></td>
<td>
<p>The function <code class="reqn">\Phi</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When <code class="reqn">\mathbf{X}</code> has copula density <code class="reqn">c</code> with marginal copula densities <code class="reqn">c_{i}</code> of <code class="reqn">\mathbf{X}_{i}</code> for <code class="reqn">i = 1, \dots, k</code>,
the <code class="reqn">\Phi</code>-dependence between <code class="reqn">\mathbf{X}_{1}, \dots, \mathbf{X}_{k}</code> equals
</p>
<p style="text-align: center;"><code class="reqn">\mathcal{D}_{\Phi} \left (\mathbf{X}_{1}, \dots, \mathbf{X}_{k} \right ) = \int_{[0,1]^{q}} \prod_{i = 1}^{k} c_{i}(\mathbf{u}_{i}) \Phi \left (\frac{c(\mathbf{u})}{\prod_{i = 1}^{k}c_{i}(\mathbf{u}_{i})} \right ),</code>
</p>

<p>for a certain continuous, convex function <code class="reqn">\Phi : (0,\infty) \rightarrow \mathbb{R}</code>.
By drawing a sample of size <code class="reqn">M</code> from <code class="reqn">c</code>, say <code class="reqn">\mathbf{U}^{(1)}, \dots, \mathbf{U}^{(M)}</code>, with <code class="reqn">\mathbf{U}^{(\ell)} = (\mathbf{U}_{1}^{(\ell)}, \dots, \mathbf{U}_{k}^{(\ell)})</code>, we can approximate <code class="reqn">\mathcal{D}_{\Phi}</code> by
</p>
<p style="text-align: center;"><code class="reqn">\widehat{\mathcal{D}}_{\Phi} = \frac{1}{M} \sum_{\ell = 1}^{M} \frac{\prod_{i = 1}^{k} c_{i} \left (\mathbf{U}_{i}^{(\ell)} \right )}{c \left (\mathbf{U}^{(\ell)} \right )} \Phi \left (\frac{c \left (\mathbf{U}^{(\ell)} \right )}{\prod_{i = 1}^{k} c_{i} \left (\mathbf{U}_{i}^{(\ell)} \right )} \right ).</code>
</p>

<p>The function <code><a href="#topic+phihac">phihac</a></code> computes <code class="reqn">\widehat{\mathcal{D}}_{\Phi}</code> when <code class="reqn">c</code> is a hierarchical Archimedean copula with two nesting levels,
as produced by the function <code><a href="#topic+gethac">gethac</a></code>.
</p>


<h3>Value</h3>

<p>The <code class="reqn">\Phi</code>-dependence between <code class="reqn">\mathbf{X}_{1}, \dots, \mathbf{X}_{k}</code> (i.e., between all the child copulas of the hac object).
</p>


<h3>References</h3>

<p>De Keyser, S. &amp; Gijbels, I. (2024).
Parametric dependence between random vectors via copula-based divergence measures.
Journal of Multivariate Analysis 203:105336. <br />
doi: https://doi.org/10.1016/j.jmva.2024.105336.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gethac">gethac</a></code> for creating a hac object with two nesting levels,
<code><a href="#topic+Helhac">Helhac</a></code> for computing the Hellinger distance between all the child copulas of a hac object with two nesting levels,
<code><a href="#topic+mlehac">mlehac</a></code> for maximum pseudo-likelihood estimation of the parameters of a hac object with two nesting levels.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
dim = c(2,2)
thetas = c(2,3,4)

# 4 dimensional nested Gumbel copula with (theta_0,theta_1,theta_2) = (2,3,4)
HAC = gethac(dim,thetas,type = 1)

# Mutual information based on Monte Carlo sample of size 10000
Phi_dep = phihac(HAC,dim,10000,function(t){t * log(t)})


</code></pre>

<hr>
<h2 id='phinp'>phinp</h2><span id='topic+phinp'></span>

<h3>Description</h3>

<p>Given a <code class="reqn">q</code>-dimensional random vector <code class="reqn">\mathbf{X} = (\mathbf{X}_{1},...,\mathbf{X}_{k})</code> with <code class="reqn">\mathbf{X}_{i}</code> a <code class="reqn">d_{i}</code>-dimensional random vector, i.e., <code class="reqn">q = d_{1} + ... + d_{k}</code>,
this function estimates the <code class="reqn">\Phi</code>-dependence between <code class="reqn">\mathbf{X}_{1},...,\mathbf{X}_{k}</code> by estimating the joint and marginal
copula densities via fully non-parametric copula kernel density estimation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>phinp(sample, cop = NULL, dim, phi, estimator, bw_method)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="phinp_+3A_sample">sample</code></td>
<td>
<p>A sample from a <code class="reqn">q</code>-dimensional random vector <code class="reqn">\mathbf{X}</code> (<code class="reqn">n \times q</code> matrix with observations in rows, variables in columns).</p>
</td></tr>
<tr><td><code id="phinp_+3A_cop">cop</code></td>
<td>
<p>A fitted reference hac object, in case bw_method = 0 (default = NULL).</p>
</td></tr>
<tr><td><code id="phinp_+3A_dim">dim</code></td>
<td>
<p>The vector of dimensions <code class="reqn">(d_{1},...,d_{k})</code>.</p>
</td></tr>
<tr><td><code id="phinp_+3A_phi">phi</code></td>
<td>
<p>The function <code class="reqn">\Phi</code>.</p>
</td></tr>
<tr><td><code id="phinp_+3A_estimator">estimator</code></td>
<td>
<p>Either &quot;beta&quot; or &quot;trans&quot; for the beta kernel or the Gaussian transformation kernel copula density estimator.</p>
</td></tr>
<tr><td><code id="phinp_+3A_bw_method">bw_method</code></td>
<td>
<p>A number in <code class="reqn">\{0,1,2\}</code> specifying the method used for computing optimal local bandwidths.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When <code class="reqn">\mathbf{X}</code> has copula density <code class="reqn">c</code> with marginal copula densities <code class="reqn">c_{i}</code> of <code class="reqn">\mathbf{X}_{i}</code> for <code class="reqn">i = 1, \dots, k</code>,
the <code class="reqn">\Phi</code>-dependence between <code class="reqn">\mathbf{X}_{1}, \dots, \mathbf{X}_{k}</code> equals
</p>
<p style="text-align: center;"><code class="reqn">\mathcal{D}_{\Phi} \left (\mathbf{X}_{1}, \dots, \mathbf{X}_{k} \right ) = \mathbb{E} \left \{ \frac{\prod_{i = 1}^{k} c_{i}(\mathbf{U}_{i})}{c \left ( \mathbf{U} \right )} \Phi \left (\frac{c(\mathbf{U})}{\prod_{i = 1}^{k}c_{i}(\mathbf{U}_{i})} \right ) \right \},</code>
</p>

<p>for a certain continuous, convex function <code class="reqn">\Phi : (0,\infty) \rightarrow \mathbb{R}</code>, and with <code class="reqn">\mathbf{U} = (\mathbf{U}_{1}, \dots, \mathbf{U}_{k}) \sim c</code>.
</p>
<p>The expectation <code class="reqn">\mathbb{E}</code> is replaced by the empirical mean using the estimated copula sample <code class="reqn">\widehat{\mathbf{U}}^{(1)}, \dots, \widehat{\mathbf{U}}^{(n)}</code> with <code class="reqn">\widehat{\mathbf{U}}^{(\ell)} = (\widehat{\mathbf{U}}_{1}^{(\ell)}, \dots, \widehat{\mathbf{U}}_{k}^{(\ell)})</code> for <code class="reqn">\ell = 1, \dots, n</code>, where (recall that <code class="reqn">\mathbf{X}_{i} = (X_{i1}, \dots, X_{id_{i}})</code> for <code class="reqn">i = 1, \dots, k</code>)
</p>
<p style="text-align: center;"><code class="reqn">\widehat{\mathbf{U}}_{i}^{(\ell)} = \left (\widehat{U}_{i1}^{(\ell)}, \dots, \widehat{U}_{id_{i}}^{(\ell)} \right ) = \left (\widehat{F}_{i1} \left (X_{i1}^{(\ell)} \right ), \dots, \widehat{F}_{id_{i}} \left (X_{id_{i}}^{(\ell)} \right )  \right ).</code>
</p>

<p>Hereby, <code class="reqn">\widehat{F}_{ij}(x_{ij}) = \frac{1}{n+1} \sum_{\ell = 1}^{n} 1 \left (X_{ij}^{(\ell)} \leq x_{ij} \right )</code> is the (rescaled) empirical cdf of <code class="reqn">X_{ij}</code> based on a sample <code class="reqn">X_{ij}^{(1)}, \dots, X_{ij}^{(n)}</code> for <code class="reqn">i = 1, \dots, k</code> and <code class="reqn">j = 1, \dots, d_{i}</code>.
</p>
<p>The joint copula density <code class="reqn">c</code> and marginal copula densities <code class="reqn">c_{i}</code> for <code class="reqn">i = 1, \dots, k</code> are estimated via fully non-parametric copula kernel density estimation.
When estimator = &quot;beta&quot;, the beta kernel copula density estimator is used.
When estimator = &quot;trans&quot;, the Gaussian transformation kernel copula density estimator is used.
</p>
<p>Bandwidth selection is done locally by using the function <code><a href="#topic+hamse">hamse</a></code>.
When bw_method = 0, then the given fitted (e.g., via MLE using <code><a href="#topic+mlehac">mlehac</a></code>) hac object (hierarchical Archimedean copula) cop is used as reference copula.
When bw_method = 1, then a non-parametric (beta or Gaussian transformation) kernel copula density estimator based on the pseudos as pivot is used. This pivot is computed
using the big O bandwidth (i.e., <code class="reqn">n^{-2/(q+4)}</code> in case of the beta estimator, and <code class="reqn">n^{-1/(q+4)}</code> for the transformation estimator, with <code class="reqn">q</code> the total dimension).
When bw_method = 2, the big O bandwidths are taken.
</p>


<h3>Value</h3>

<p>The estimated <code class="reqn">\Phi</code>-dependence between <code class="reqn">\mathbf{X}_{1}, \dots, \mathbf{X}_{k}</code>.
</p>


<h3>References</h3>

<p>De Keyser, S. &amp; Gijbels, I. (2024).
Hierarchical variable clustering via copula-based divergence measures between random vectors.
International Journal of Approximate Reasoning 165:109090.
doi: https://doi.org/10.1016/j.ijar.2023.109090.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+betakernelestimator">betakernelestimator</a></code> for the computation of the beta kernel copula density estimator, <br />
<code><a href="#topic+transformationestimator">transformationestimator</a></code> for the computation of the Gaussian transformation kernel copula density estimator,
<code><a href="#topic+hamse">hamse</a></code> for local bandwidth selection for the beta kernel or Gaussian transformation kernel copula density estimator.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
q = 4
dim = c(2,2)

# Sample size
n = 500

# Four dimensional hierarchical Gumbel copula
# with parameters (theta_0,theta_1,theta_2) = (2,3,4)
HAC = gethac(dim,c(2,3,4),type = 1)

# Sample
sample =  suppressWarnings(HAC::rHAC(n,HAC))

# Maximum pseudo-likelihood estimator to be used as reference copula for bw_method = 0
est_cop = mlehac(sample,dim,1,c(2,3,4))

# Estimate mutual information between two random vectors of size 2 in different ways

est_phi_1 = phinp(sample,cop = est_cop,dim = dim,phi = function(t){t * log(t)},
                  estimator = "beta",bw_method = 0)
est_phi_2 = phinp(sample,cop = est_cop,dim = dim,phi = function(t){t * log(t)},
                  estimator = "trans",bw_method = 0)
est_phi_3 = phinp(sample,dim = dim,phi = function(t){t * log(t)},
                  estimator = "beta",bw_method = 1)
est_phi_4 = phinp(sample,dim = dim,phi = function(t){t * log(t)},
                  estimator = "trans",bw_method = 1)
est_phi_5 = phinp(sample,dim = dim,phi = function(t){t * log(t)},
                  estimator = "beta",bw_method = 2)
est_phi_6 = phinp(sample,dim = dim,phi = function(t){t * log(t)},
                  estimator = "trans",bw_method = 2)


</code></pre>

<hr>
<h2 id='transformationestimator'>transformationestimator</h2><span id='topic+transformationestimator'></span>

<h3>Description</h3>

<p>This function computes the non-parametric Gaussian transformation kernel copula density estimator.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>transformationestimator(input, h, pseudos)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="transformationestimator_+3A_input">input</code></td>
<td>
<p>The copula argument at which the density estimate is to be computed.</p>
</td></tr>
<tr><td><code id="transformationestimator_+3A_h">h</code></td>
<td>
<p>The bandwidth to be used in the Gaussian kernel.</p>
</td></tr>
<tr><td><code id="transformationestimator_+3A_pseudos">pseudos</code></td>
<td>
<p>The (estimated) copula observations from a <code class="reqn">q</code>-dimensional random vector <code class="reqn">\mathbf{X}</code> (<code class="reqn">n \times q</code> matrix with observations in rows, variables in columns).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Given a <code class="reqn">q</code>-dimensional random vector <code class="reqn">\mathbf{X} = (\mathbf{X}_{1}, \dots, \mathbf{X}_{k})</code> with <code class="reqn">\mathbf{X}_{i} = (X_{i1}, \dots, X_{id_{i}})</code>,
and samples <code class="reqn">X_{ij}^{(1)}, \dots, X_{ij}^{(n)}</code> from <code class="reqn">X_{ij}</code> for <code class="reqn">i = 1, \dots, k</code> and <code class="reqn">j = 1, \dots, d_{i}</code>,
the Gaussian transformation kernel estimator for the copula density of <code class="reqn">\mathbf{X}</code> equals, at <code class="reqn">\mathbf{u} = (u_{11}, \dots, u_{kd_{k}}) \in \mathbb{R}^{q}</code>,
</p>
<p style="text-align: center;"><code class="reqn">\widehat{c}_{\text{T}}(\mathbf{u}) = \frac{1}{n h_{n}^{q} \prod_{i = 1}^{k} \prod_{j = 1}^{d_{i}} \phi \left (\Phi^{-1} \left (u_{ij} \right ) \right )} \sum_{\ell = 1}^{n} \prod_{i = 1}^{k} \prod_{j = 1}^{d_{i}} \phi \left (\frac{\Phi^{-1}(u_{ij}) - \Phi^{-1} \left (\widehat{U}_{ij}^{(\ell)} \right )}{h_{n}} \right ),</code>
</p>

<p>where <code class="reqn">h_{n} &gt; 0</code> is a bandwidth parameter, <code class="reqn">\widehat{U}_{ij}^{(\ell)} = \widehat{F}_{ij} (X_{ij}^{(\ell)})</code> with </p>
<p style="text-align: center;"><code class="reqn">\widehat{F}_{ij}(x_{ij}) = \frac{1}{n+1} \sum_{\ell = 1}^{n} 1 \left (X_{ij}^{(\ell)} \leq x_{ij} \right )</code>
</p>
<p> the (rescaled) empirical cdf of <code class="reqn">X_{ij}</code>, and
<code class="reqn">\Phi</code> the standard normal distribution function with corresponding quantile function <code class="reqn">\Phi^{-1}</code> and density function <code class="reqn">\phi</code>.
</p>


<h3>Value</h3>

<p>The Gaussian transformation kernel copula density estimator evaluated at the input.
</p>


<h3>References</h3>

<p>De Keyser, S. &amp; Gijbels, I. (2024).
Hierarchical variable clustering via copula-based divergence measures between random vectors.
International Journal of Approximate Reasoning 165:109090.
doi: https://doi.org/10.1016/j.ijar.2023.109090.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+betakernelestimator">betakernelestimator</a></code> for the computation of the beta kernel copula density estimator,
<code><a href="#topic+hamse">hamse</a></code> for local bandwidth selection for the beta kernel or Gaussian transformation kernel copula density estimator,
<code><a href="#topic+phinp">phinp</a></code> for fully non-parametric estimation of the <code class="reqn">\Phi</code>-dependence between <code class="reqn">k</code> random vectors.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>q = 3
n = 100

# Sample from multivariate normal distribution with identity covariance matrix
sample = mvtnorm::rmvnorm(n,rep(0,q),diag(3),method = "chol")

# Copula pseudo-observations
pseudos = matrix(0,n,q)
for(j in 1:q){pseudos[,j] = (n/(n+1)) * ecdf(sample[,j])(sample[,j])}

# Argument at which to estimate the density
input = rep(0.5,q)

# Local bandwidth selection
h = hamse(input,pseudos = pseudos,n = n,estimator = "trans",bw_method = 1)

# Gaussian transformation kernel estimator
est_dens = transformationestimator(input,h,pseudos)

# True density
true = copula::dCopula(rep(0.5,q), copula::normalCopula(0, dim = q))

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
