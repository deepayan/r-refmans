<!DOCTYPE html><html lang="en"><head><title>Help for package torchvisionlib</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {torchvisionlib}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#ops_deform_conv2d'><p>Performs Deformable Convolution v2,</p></a></li>
<li><a href='#ops_nms'><p>Performs non-maximum suppression (NMS) on the boxes</p></a></li>
<li><a href='#ops_ps_roi_align'><p>Performs Position-Sensitive Region of Interest (RoI) Align operator</p></a></li>
<li><a href='#torchvisionlib_is_installed'><p>Checks if an installation of torchvisionlib was found.</p></a></li>
<li><a href='#vision_read_jpeg'><p>Read JPEG's directly into torch tensors</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>Additional Operators for Image Models</td>
</tr>
<tr>
<td>Version:</td>
<td>0.5.0</td>
</tr>
<tr>
<td>Description:</td>
<td>Implements additional operators for computer vision models, including
    operators necessary for image segmentation and object detection deep learning
    models.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.6)</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, torch</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp, torch (&ge; 0.9.0), rlang, glue, withr</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat (&ge; 3.0.0)</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/mlverse/torchvisionlib">https://github.com/mlverse/torchvisionlib</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/mlverse/torchvisionlib/issues">https://github.com/mlverse/torchvisionlib/issues</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-02-15 18:37:30 UTC; dfalbel</td>
</tr>
<tr>
<td>Author:</td>
<td>Daniel Falbel [aut, cre],
  RStudio [cph]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Daniel Falbel &lt;daniel@rstudio.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-02-15 19:20:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='ops_deform_conv2d'>Performs Deformable Convolution v2,</h2><span id='topic+ops_deform_conv2d'></span>

<h3>Description</h3>

<p>Ddescribed in <a href="https://arxiv.org/abs/1811.11168">Deformable ConvNets v2: More Deformable, Better Results</a>
if <code>mask</code> is not <code>NULL</code> and performs Deformable Convolution, described in
<a href="https://arxiv.org/abs/1703.06211">Deformable Convolutional Networks</a>
if <code>mask</code> is <code>NULL</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ops_deform_conv2d(
  input,
  offset,
  weight,
  bias = NULL,
  stride = c(1, 1),
  padding = c(0, 0),
  dilation = c(1, 1),
  mask = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ops_deform_conv2d_+3A_input">input</code></td>
<td>
<p>(<code>Tensor[batch_size, in_channels, in_height, in_width]</code>): input tensor</p>
</td></tr>
<tr><td><code id="ops_deform_conv2d_+3A_offset">offset</code></td>
<td>
<p>(<code>Tensor[batch_size, 2 * offset_groups * kernel_height * kernel_width, out_height, out_width]</code>):
offsets to be applied for each position in the convolution kernel.</p>
</td></tr>
<tr><td><code id="ops_deform_conv2d_+3A_weight">weight</code></td>
<td>
<p>(<code style="white-space: pre;">&#8288;Tensor[out_channels, in_channels // groups, kernel_height, kernel_width]&#8288;</code>): convolution weights,
split into groups of size (in_channels // groups)</p>
</td></tr>
<tr><td><code id="ops_deform_conv2d_+3A_bias">bias</code></td>
<td>
<p>(<code>Tensor[out_channels]</code>): optional bias of shape (out_channels,). Default: <code>NULL</code></p>
</td></tr>
<tr><td><code id="ops_deform_conv2d_+3A_stride">stride</code></td>
<td>
<p>(int or <code>Tuple[int, int]</code>): distance between convolution centers. Default: 1</p>
</td></tr>
<tr><td><code id="ops_deform_conv2d_+3A_padding">padding</code></td>
<td>
<p>(int or <code>Tuple[int, int]</code>): height/width of padding of zeroes around
each image. Default: 0</p>
</td></tr>
<tr><td><code id="ops_deform_conv2d_+3A_dilation">dilation</code></td>
<td>
<p>(int or <code>Tuple[int, int]</code>): the spacing between kernel elements. Default: 1</p>
</td></tr>
<tr><td><code id="ops_deform_conv2d_+3A_mask">mask</code></td>
<td>
<p>(<code>Tensor[batch_size, offset_groups * kernel_height * kernel_width, out_height, out_width]</code>):
masks to be applied for each position in the convolution kernel. Default: <code>NULL</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>Tensor[batch_sz, out_channels, out_h, out_w]</code>: result of convolution
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torchvisionlib_is_installed()) {
  library(torch)
  input &lt;- torch_rand(4, 3, 10, 10)
  kh &lt;- kw &lt;- 3
  weight &lt;- torch_rand(5, 3, kh, kw)
  # offset and mask should have the same spatial size as the output
  # of the convolution. In this case, for an input of 10, stride of 1
  # and kernel size of 3, without padding, the output size is 8
  offset &lt;- torch_rand(4, 2 * kh * kw, 8, 8)
  mask &lt;- torch_rand(4, kh * kw, 8, 8)
  out &lt;- ops_deform_conv2d(input, offset, weight, mask = mask)
  print(out$shape)
}
</code></pre>

<hr>
<h2 id='ops_nms'>Performs non-maximum suppression (NMS) on the boxes</h2><span id='topic+ops_nms'></span>

<h3>Description</h3>

<p>Performs non-maximum suppression (NMS) on the boxes according to their
intersection-over-union (IoU).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ops_nms(boxes, scores, iou_threshold)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ops_nms_+3A_boxes">boxes</code></td>
<td>
<p><code>Tensor[N,4]</code> boxes to perform NMS on. They are expected to be
in <code style="white-space: pre;">&#8288;(x1, y1, x2, y2)&#8288;</code> format with <code style="white-space: pre;">&#8288;0 &lt;= x1 &lt; x2&#8288;</code> and <code style="white-space: pre;">&#8288;0 &lt;= y1 &lt; y2&#8288;</code>.</p>
</td></tr>
<tr><td><code id="ops_nms_+3A_scores">scores</code></td>
<td>
<p><code>Tensor[N]</code> scores for each one of the boxes.</p>
</td></tr>
<tr><td><code id="ops_nms_+3A_iou_threshold">iou_threshold</code></td>
<td>
<p><code>float</code> discards all overlapping boxes with <code>IoU &gt; iou_threshold</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>NMS iteratively removes lower scoring boxes which have an IoU greater than
<code>iou_threshold</code> with another (higher scoring) box.
</p>
<p>If multiple boxes have the exact same score and satisfy the IoU criterion with
respect to a reference box, the selected box is not guaranteed to be the same
between CPU and GPU. This is similar to the behavior of argsort in PyTorch
when repeated values are present.
</p>


<h3>Value</h3>

<p>int64 tensor with the indices of the elements that have been kept by NMS,
sorted in decreasing order of scores
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torchvisionlib_is_installed()) {
  ops_nms(torch::torch_rand(3, 4), torch::torch_rand(3), 0.5)
}
</code></pre>

<hr>
<h2 id='ops_ps_roi_align'>Performs Position-Sensitive Region of Interest (RoI) Align operator</h2><span id='topic+ops_ps_roi_align'></span><span id='topic+nn_ps_roi_align'></span>

<h3>Description</h3>

<p>The (RoI) Align operator is mentioned in <a href="https://arxiv.org/abs/1711.07264">Light-Head R-CNN</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ops_ps_roi_align(
  input,
  boxes,
  output_size,
  spatial_scale = 1,
  sampling_ratio = -1
)

nn_ps_roi_align(output_size, spatial_scale = 1, sampling_ratio = -1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ops_ps_roi_align_+3A_input">input</code></td>
<td>
<p>(<code>Tensor[N, C, H, W]</code>): The input tensor, i.e. a batch with <code>N</code> elements. Each element
contains <code>C</code> feature maps of dimensions <code style="white-space: pre;">&#8288;H x W&#8288;</code>.</p>
</td></tr>
<tr><td><code id="ops_ps_roi_align_+3A_boxes">boxes</code></td>
<td>
<p>(<code>Tensor[K, 5]</code> or <code>List[Tensor[L, 4]]</code>): the box coordinates in (x1, y1, x2, y2)
format where the regions will be taken from.
The coordinate must satisfy <code style="white-space: pre;">&#8288;0 &lt;= x1 &lt; x2&#8288;</code> and <code style="white-space: pre;">&#8288;0 &lt;= y1 &lt; y2&#8288;</code>.
If a single Tensor is passed, then the first column should
contain the index of the corresponding element in the batch, i.e. a number in <code style="white-space: pre;">&#8288;[1, N]&#8288;</code>.
If a list of Tensors is passed, then each Tensor will correspond to the boxes for an element i
in the batch.</p>
</td></tr>
<tr><td><code id="ops_ps_roi_align_+3A_output_size">output_size</code></td>
<td>
<p>(int or <code>Tuple[int, int]</code>): the size of the output (in bins or pixels) after the pooling
is performed, as (height, width).</p>
</td></tr>
<tr><td><code id="ops_ps_roi_align_+3A_spatial_scale">spatial_scale</code></td>
<td>
<p>(float): a scaling factor that maps the box coordinates to
the input coordinates. For example, if your boxes are defined on the scale
of a 224x224 image and your input is a 112x112 feature map (resulting from a 0.5x scaling of
the original image), you'll want to set this to 0.5. Default: 1.0</p>
</td></tr>
<tr><td><code id="ops_ps_roi_align_+3A_sampling_ratio">sampling_ratio</code></td>
<td>
<p>(int): number of sampling points in the interpolation grid
used to compute the output value of each pooled output bin. If &gt; 0,
then exactly <code style="white-space: pre;">&#8288;sampling_ratio x sampling_ratio&#8288;</code> sampling points per bin are used. If
&lt;= 0, then an adaptive number of grid points are used (computed as
<code>ceil(roi_width / output_width)</code>, and likewise for height). Default: -1</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>Tensor[K, C / (output_size[1] * output_size[2]), output_size[1], output_size[2]]</code>:
The pooled RoIs
</p>


<h3>Functions</h3>


<ul>
<li> <p><code>nn_ps_roi_align()</code>: The <code><a href="torch.html#topic+nn_module">torch::nn_module()</a></code> wrapper for <code><a href="#topic+ops_ps_roi_align">ops_ps_roi_align()</a></code>.
</p>
</li></ul>


<h3>Examples</h3>

<pre><code class='language-R'>if (torchvisionlib_is_installed()) {
library(torch)
library(torchvisionlib)
input &lt;- torch_randn(1, 3, 28, 28)
boxes &lt;- list(torch_tensor(matrix(c(1,1,5,5), ncol = 4)))
roi &lt;- nn_ps_roi_align(output_size = c(1, 1))
roi(input, boxes)
}

</code></pre>

<hr>
<h2 id='torchvisionlib_is_installed'>Checks if an installation of torchvisionlib was found.</h2><span id='topic+torchvisionlib_is_installed'></span><span id='topic+install_torchvisionlib'></span>

<h3>Description</h3>

<p>Checks if an installation of torchvisionlib was found.
</p>
<p>Install additional libraries
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torchvisionlib_is_installed()

install_torchvisionlib(url = Sys.getenv("TORCHVISIONLIB_URL", unset = NA))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="torchvisionlib_is_installed_+3A_url">url</code></td>
<td>
<p>Url for the binaries. Can also be the file path to the binaries.</p>
</td></tr>
</table>

<hr>
<h2 id='vision_read_jpeg'>Read JPEG's directly into torch tensors</h2><span id='topic+vision_read_jpeg'></span>

<h3>Description</h3>

<p>Read JPEG's directly into torch tensors
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vision_read_jpeg(path)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="vision_read_jpeg_+3A_path">path</code></td>
<td>
<p>path to JPEG file</p>
</td></tr>
</table>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
