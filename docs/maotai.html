<!DOCTYPE html><html><head><title>Help for package maotai</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {maotai}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#bmds'><p>Bayesian Multidimensional Scaling</p></a></li>
<li><a href='#boot.mblock'><p>Generate Index for Moving Block Bootstrapping</p></a></li>
<li><a href='#boot.stationary'><p>Generate Index for Stationary Bootstrapping</p></a></li>
<li><a href='#cayleymenger'><p>Cayley-Menger Determinant</p></a></li>
<li><a href='#checkdist'><p>Check for Distance Matrix</p></a></li>
<li><a href='#checkmetric'><p>Check for Metric Matrix</p></a></li>
<li><a href='#cmds'><p>Classical Multidimensional Scaling</p></a></li>
<li><a href='#cov2corr'><p>Convert Covariance into Correlation Matrix</p></a></li>
<li><a href='#cov2pcorr'><p>Convert Covariance into Partial Correlation Matrix</p></a></li>
<li><a href='#dpmeans'><p>DP-means Algorithm for Clustering Euclidean Data</p></a></li>
<li><a href='#ecdfdist'><p>Distance Measures between Multiple Empirical Cumulative Distribution Functions</p></a></li>
<li><a href='#ecdfdist2'><p>Pairwise Measures for Two Sets of Empirical CDFs</p></a></li>
<li><a href='#epmeans'><p>EP-means Algorithm for Clustering Empirical Distributions</p></a></li>
<li><a href='#kmeanspp'><p>K-Means++ Clustering Algorithm</p></a></li>
<li><a href='#lgpa'><p>Large-scale Generalized Procrustes Analysis</p></a></li>
<li><a href='#lyapunov'><p>Solve Lyapunov Equation</p></a></li>
<li><a href='#matderiv'><p>Numerical Approximation to Gradient of a Function with Matrix Argument</p></a></li>
<li><a href='#metricdepth'><p>Metric Depth</p></a></li>
<li><a href='#mmd2test'><p>Kernel Two-sample Test with Maximum Mean Discrepancy</p></a></li>
<li><a href='#nef'><p>Negative Eigenfraction</p></a></li>
<li><a href='#nem'><p>Negative Eigenvalue Magnitude</p></a></li>
<li><a href='#pdeterminant'><p>Calculate the Pseudo-Determinant of a Matrix</p></a></li>
<li><a href='#rotationS2'><p>Compute a Rotation on the 2-dimensional Sphere</p></a></li>
<li><a href='#shortestpath'><p>Find Shortest Path using Floyd-Warshall Algorithm</p></a></li>
<li><a href='#sylvester'><p>Solve Sylvester Equation</p></a></li>
<li><a href='#trio'><p>Trace Ratio Optimation</p></a></li>
<li><a href='#tsne'><p>t-SNE Embedding</p></a></li>
<li><a href='#weiszfeld'><p>Weiszfeld Algorithm for Computing L1-median</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Tools for Matrix Algebra, Optimization and Inference</td>
</tr>
<tr>
<td>Version:</td>
<td>0.2.5</td>
</tr>
<tr>
<td>Description:</td>
<td>Matrix is an universal and sometimes primary object/unit in applied mathematics and statistics. We provide a number of algorithms for selected problems in optimization and statistical inference. For general exposition to the topic with focus on statistical context, see the book by Banerjee and Roy (2014, ISBN:9781420095388).</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Suggests:</td>
<td>covr, igraph, testthat (&ge; 3.0.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>Matrix, Rcpp, Rdpack, RSpectra, Rtsne, RANN, cluster, labdsv,
shapes, stats, utils, fastcluster, dbscan, pracma</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo, RcppDist</td>
</tr>
<tr>
<td>RdMacros:</td>
<td>Rdpack</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/kisungyou/maotai">https://github.com/kisungyou/maotai</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/kisungyou/maotai/issues">https://github.com/kisungyou/maotai/issues</a></td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-03-29 21:29:17 UTC; kisung</td>
</tr>
<tr>
<td>Author:</td>
<td>Kisung You <a href="https://orcid.org/0000-0002-8584-459X"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut,
    cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Kisung You &lt;kisungyou@outlook.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-03-29 22:40:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='bmds'>Bayesian Multidimensional Scaling</h2><span id='topic+bmds'></span>

<h3>Description</h3>

<p>A Bayesian formulation of classical Multidimensional Scaling is presented.
Even though this method is based on MCMC sampling, we only return maximum a posterior (MAP) estimate
that maximizes the posterior distribution. Due to its nature without any special tuning,
increasing <code>mc.iter</code> requires much computation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bmds(
  data,
  ndim = 2,
  par.a = 5,
  par.alpha = 0.5,
  par.step = 1,
  mc.iter = 8128,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bmds_+3A_data">data</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix whose rows are observations.</p>
</td></tr>
<tr><td><code id="bmds_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="bmds_+3A_par.a">par.a</code></td>
<td>
<p>hyperparameter for conjugate prior on variance term, i.e., <code class="reqn">\sigma^2 \sim IG(a,b)</code>. Note that <code class="reqn">b</code> is chosen appropriately as in paper.</p>
</td></tr>
<tr><td><code id="bmds_+3A_par.alpha">par.alpha</code></td>
<td>
<p>hyperparameter for conjugate prior on diagonal term, i.e., <code class="reqn">\lambda_j \sim IG(\alpha, \beta_j)</code>. Note that <code class="reqn">\beta_j</code> is chosen appropriately as in paper.</p>
</td></tr>
<tr><td><code id="bmds_+3A_par.step">par.step</code></td>
<td>
<p>stepsize for random-walk, which is standard deviation of Gaussian proposal.</p>
</td></tr>
<tr><td><code id="bmds_+3A_mc.iter">mc.iter</code></td>
<td>
<p>the number of MCMC iterations.</p>
</td></tr>
<tr><td><code id="bmds_+3A_verbose">verbose</code></td>
<td>
<p>a logical; <code>TRUE</code> to show iterations, <code>FALSE</code> otherwise.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>embed</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>stress</dt><dd><p>discrepancy between embedded and origianl data as a measure of error.</p>
</dd>
</dl>



<h3>References</h3>

<p>Oh M, Raftery AE (2001).
&ldquo;Bayesian Multidimensional Scaling and Choice of Dimension.&rdquo;
<em>Journal of the American Statistical Association</em>, <b>96</b>(455), 1031&ndash;1044.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## use simple example of iris dataset
data(iris) 
idata = as.matrix(iris[,1:4])

## run Bayesian MDS
#  let's run 10 iterations only.
iris.cmds = cmds(idata, ndim=2)
iris.bmds = bmds(idata, ndim=2, mc.iter=5, par.step=(2.38^2)) 

## extract coordinates and class information
cx = iris.cmds$embed # embedded coordinates of CMDS
bx = iris.bmds$embed #                         BMDS
icol = iris[,5]      # class information

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(2,1))
mc = paste0("CMDS with STRESS=",round(iris.cmds$stress,4))
mb = paste0("BMDS with STRESS=",round(iris.bmds$stress,4))
plot(cx, col=icol,pch=19,main=mc)
plot(bx, col=icol,pch=19,main=mb)
par(opar)


</code></pre>

<hr>
<h2 id='boot.mblock'>Generate Index for Moving Block Bootstrapping</h2><span id='topic+boot.mblock'></span>

<h3>Description</h3>

<p>Assuming data being dependent with cardinality <code>N</code>, <code>boot.mblock</code> returns 
a vector of index that is used for moving block bootstrapping.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>boot.mblock(N, b = max(2, round(N/10)))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="boot.mblock_+3A_n">N</code></td>
<td>
<p>the number of observations.</p>
</td></tr>
<tr><td><code id="boot.mblock_+3A_b">b</code></td>
<td>
<p>the size of a block to be drawn.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a vector of length <code>N</code> for moving block bootstrap sampling.
</p>


<h3>References</h3>

<p>Kunsch HR (1989).
&ldquo;The Jackknife and the Bootstrap for General Stationary Observations.&rdquo;
<em>The Annals of Statistics</em>, <b>17</b>(3), 1217&ndash;1241.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## example : bootstrap confidence interval of mean and variances
vec.x = seq(from=0,to=10,length.out=100)
vec.y = sin(1.21*vec.x) + 2*cos(3.14*vec.x) + rnorm(100,sd=1.5)
data.mu  = mean(vec.y)
data.var = var(vec.y)

## apply moving block bootstrapping
nreps   = 50
vec.mu  = rep(0,nreps)
vec.var = rep(0,nreps)
for (i in 1:nreps){
   sample.id = boot.mblock(100, b=10)
   sample.y  = vec.y[sample.id]
   vec.mu[i]  = mean(sample.y)
   vec.var[i] = var(sample.y)
   print(paste("iteration ",i,"/",nreps," complete.", sep=""))
}

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3), pty="s")
plot(vec.x, vec.y, type="l", main="1d signal")  # 1d signal
hist(vec.mu, main="mean CI", xlab="mu")         # mean
abline(v=data.mu, col="red", lwd=4)
hist(vec.var, main="variance CI", xlab="sigma") # variance
abline(v=data.var, col="blue", lwd=4)
par(opar)


</code></pre>

<hr>
<h2 id='boot.stationary'>Generate Index for Stationary Bootstrapping</h2><span id='topic+boot.stationary'></span>

<h3>Description</h3>

<p>Assuming data being dependent with cardinality <code>N</code>, <code>boot.stationary</code> returns 
a vector of index that is used for stationary bootstrapping. To describe, starting points 
are drawn from uniform distribution over <code>1:N</code> and the size of each block is 
determined from geometric distribution with parameter <code class="reqn">p</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>boot.stationary(N, p = 0.25)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="boot.stationary_+3A_n">N</code></td>
<td>
<p>the number of observations.</p>
</td></tr>
<tr><td><code id="boot.stationary_+3A_p">p</code></td>
<td>
<p>parameter for geometric distribution with the size of each block.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a vector of length <code>N</code> for moving block bootstrap sampling.
</p>


<h3>References</h3>

<p>Politis DN, Romano JP (1994).
&ldquo;The Stationary Bootstrap.&rdquo;
<em>Journal of the American Statistical Association</em>, <b>89</b>(428), 1303.
ISSN 01621459.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## example : bootstrap confidence interval of mean and variances
vec.x = seq(from=0,to=10,length.out=100)
vec.y = sin(1.21*vec.x) + 2*cos(3.14*vec.x) + rnorm(100,sd=1.5)
data.mu  = mean(vec.y)
data.var = var(vec.y)

## apply stationary bootstrapping
nreps   = 50
vec.mu  = rep(0,nreps)
vec.var = rep(0,nreps)
for (i in 1:nreps){
   sample.id = boot.stationary(100)
   sample.y  = vec.y[sample.id]
   vec.mu[i]  = mean(sample.y)
   vec.var[i] = var(sample.y)
   print(paste("iteration ",i,"/",nreps," complete.", sep=""))
}

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3), pty="s")
plot(vec.x, vec.y, type="l", main="1d signal")  # 1d signal
hist(vec.mu, main="mean CI", xlab="mu")         # mean
abline(v=data.mu, col="red", lwd=4)
hist(vec.var, main="variance CI", xlab="sigma") # variance
abline(v=data.var, col="blue", lwd=4)
par(opar)


</code></pre>

<hr>
<h2 id='cayleymenger'>Cayley-Menger Determinant</h2><span id='topic+cayleymenger'></span>

<h3>Description</h3>

<p>Cayley-Menger determinant is a formula of a <code class="reqn">n</code>-dimensional simplex 
with respect to the squares of all pairwise distances of its vertices.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cayleymenger(data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cayleymenger_+3A_data">data</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix of row-stacked observations.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list containing</p>

<dl>
<dt>det</dt><dd><p>determinant value.</p>
</dd>
<dt>vol</dt><dd><p>volume attained from the determinant.</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>## USE 'IRIS' DATASET
data(iris)
X = as.matrix(iris[,1:4])

## COMPUTE CAYLEY-MENGER DETERMINANT
#  since k=4 &lt; n=149, it should be zero.
cayleymenger(X)

</code></pre>

<hr>
<h2 id='checkdist'>Check for Distance Matrix</h2><span id='topic+checkdist'></span>

<h3>Description</h3>

<p>This function checks whether the distance matrix <code class="reqn">D:=d_{ij} = d(x_i, x_j)</code> satisfies 
three axioms to make itself a semimetric, which are (1) <code class="reqn">d_{ii} = 0</code>, (2) <code class="reqn">d_{ij} &gt; 0</code> for <code class="reqn">i\neq j</code>, and 
(3) <code class="reqn">d_{ij} = d_{ji}</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>checkdist(d)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="checkdist_+3A_d">d</code></td>
<td>
<p><code>"dist"</code> object or <code class="reqn">(N\times N)</code> matrix of pairwise distances.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a logical; <code>TRUE</code> if it satisfies metric property, <code>FALSE</code> otherwise.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+checkmetric">checkmetric</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Let's use L2 distance matrix of iris dataset
data(iris)
dx = as.matrix(stats::dist(iris[,1:4]))

# perturb d(i,j) 
dy = dx  
dy[1,2] &lt;- dy[2,1] &lt;- 10

# run the algorithm
checkdist(dx)
checkdist(dy)

</code></pre>

<hr>
<h2 id='checkmetric'>Check for Metric Matrix</h2><span id='topic+checkmetric'></span>

<h3>Description</h3>

<p>This function checks whether the distance matrix <code class="reqn">D:=d_{ij} = d(x_i, x_j)</code> satisfies 
four axioms to make itself a semimetric, which are (1) <code class="reqn">d_{ii} = 0</code>, (2) <code class="reqn">d_{ij} &gt; 0</code> for <code class="reqn">i\neq j</code>, 
(3) <code class="reqn">d_{ij} = d_{ji}</code>, and (4) <code class="reqn">d_{ij} \leq d_{ik} + d_{kj}</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>checkmetric(d)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="checkmetric_+3A_d">d</code></td>
<td>
<p><code>"dist"</code> object or <code class="reqn">(N\times N)</code> matrix of pairwise distances.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a logical; <code>TRUE</code> if it satisfies metric property, <code>FALSE</code> otherwise.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+checkdist">checkdist</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Let's use L2 distance matrix of iris dataset
data(iris)
dx = as.matrix(stats::dist(iris[,1:4]))

# perturb d(i,j) 
dy = dx  
dy[1,2] &lt;- dy[2,1] &lt;- 10

# run the algorithm
checkmetric(dx)
checkmetric(dy)

</code></pre>

<hr>
<h2 id='cmds'>Classical Multidimensional Scaling</h2><span id='topic+cmds'></span>

<h3>Description</h3>

<p>Classical multidimensional scaling aims at finding low-dimensional structure 
by preserving pairwise distances of data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cmds(data, ndim = 2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cmds_+3A_data">data</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix whose rows are observations.</p>
</td></tr>
<tr><td><code id="cmds_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>embed</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>stress</dt><dd><p>discrepancy between embedded and origianl data as a measure of error.</p>
</dd>
</dl>



<h3>References</h3>

<p>Torgerson WS (1952).
&ldquo;Multidimensional Scaling: I. Theory and Method.&rdquo;
<em>Psychometrika</em>, <b>17</b>(4), 401&ndash;419.
ISSN 0033-3123, 1860-0980.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## use simple example of iris dataset
data(iris) 
idata = as.matrix(iris[,1:4])
icol  = as.factor(iris[,5])   # class information

## run Classical MDS
iris.cmds = cmds(idata, ndim=2)

## visualize
opar &lt;- par(no.readonly=TRUE)
plot(iris.cmds$embed, col=icol, 
     main=paste0("STRESS=",round(iris.cmds$stress,4)))
par(opar)

</code></pre>

<hr>
<h2 id='cov2corr'>Convert Covariance into Correlation Matrix</h2><span id='topic+cov2corr'></span>

<h3>Description</h3>

<p>Given a covariance matrix, return a correlation matrix that has unit diagonals. 
We strictly impose (and check) whether the given input is a symmetric matrix 
of full-rank.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cov2corr(mat)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cov2corr_+3A_mat">mat</code></td>
<td>
<p>a <code class="reqn">(p\times p)</code> covariance matrix.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code class="reqn">(p\times p)</code> correlation matrix.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# generate an empirical covariance scaled
prep_mat = stats::cov(matrix(rnorm(100*10),ncol=10))
prep_vec = diag(as.vector(stats::runif(10, max=5)))
prep_cov = prep_vec%*%prep_mat%*%prep_vec

# compute correlation matrix
prep_cor = cov2corr(prep_cov)

# visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,2), pty="s")
image(prep_cov, axes=FALSE, main="covariance")
image(prep_cor, axes=FALSE, main="correlation")
par(opar)


</code></pre>

<hr>
<h2 id='cov2pcorr'>Convert Covariance into Partial Correlation Matrix</h2><span id='topic+cov2pcorr'></span>

<h3>Description</h3>

<p>Given a covariance matrix, return a partial correlation matrix that has unit diagonals. 
We strictly impose (and check) whether the given input is a symmetric matrix 
of full-rank.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cov2pcorr(mat)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cov2pcorr_+3A_mat">mat</code></td>
<td>
<p>a <code class="reqn">(p\times p)</code> covariance matrix.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code class="reqn">(p\times p)</code> partial correlation matrix.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# generate an empirical covariance scaled
prep_mat = stats::cov(matrix(rnorm(100*10),ncol=10))
prep_vec = diag(as.vector(stats::runif(10, max=5)))
prep_cov = prep_vec%*%prep_mat%*%prep_vec

# compute correlation and partial correlation matrices
prep_cor = cov2corr(prep_cov)
prep_par = cov2pcorr(prep_cov)

# visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3), pty="s")
image(prep_cov, axes=FALSE, main="covariance")
image(prep_cor, axes=FALSE, main="correlation")
image(prep_par, axes=FALSE, main="partial correlation")
par(opar)


</code></pre>

<hr>
<h2 id='dpmeans'>DP-means Algorithm for Clustering Euclidean Data</h2><span id='topic+dpmeans'></span>

<h3>Description</h3>

<p>DP-means is a nonparametric clustering method motivated by DP mixture model in that 
the number of clusters is determined by a parameter <code class="reqn">\lambda</code>. The larger 
the <code class="reqn">\lambda</code> value is, the smaller the number of clusters is attained. 
In addition to the original paper, we added an option to randomly permute 
an order of updating for each observation's membership as a common 
heuristic in the literature of cluster analysis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dpmeans(
  data,
  lambda = 1,
  maxiter = 1234,
  abstol = 1e-06,
  permute.order = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dpmeans_+3A_data">data</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> data matrix for each row being an observation.</p>
</td></tr>
<tr><td><code id="dpmeans_+3A_lambda">lambda</code></td>
<td>
<p>a threshold to define a new cluster.</p>
</td></tr>
<tr><td><code id="dpmeans_+3A_maxiter">maxiter</code></td>
<td>
<p>maximum number of iterations.</p>
</td></tr>
<tr><td><code id="dpmeans_+3A_abstol">abstol</code></td>
<td>
<p>stopping criterion</p>
</td></tr>
<tr><td><code id="dpmeans_+3A_permute.order">permute.order</code></td>
<td>
<p>a logical; <code>TRUE</code> if random order for permutation is used, <code>FALSE</code> otherwise.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>cluster</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>centers</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
</dl>



<h3>References</h3>

<p>Kulis B, Jordan MI (2012).
&ldquo;Revisiting K-Means: New Algorithms via Bayesian Nonparametrics.&rdquo;
In <em>Proceedings of the 29th International Coference on International Conference on Machine Learning</em>,  ICML'12, 1131&ndash;1138.
ISBN 978-1-4503-1285-1.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## define data matrix of two clusters
x1  = matrix(rnorm(50*3,mean= 2), ncol=3)
x2  = matrix(rnorm(50*3,mean=-2), ncol=3)
X   = rbind(x1,x2)
lab = c(rep(1,50),rep(2,50))

## run dpmeans with several lambda values
solA &lt;- dpmeans(X, lambda= 5)$cluster
solB &lt;- dpmeans(X, lambda=10)$cluster
solC &lt;- dpmeans(X, lambda=20)$cluster

## visualize the results
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,4), pty="s")
plot(X,col=lab,  pch=19, cex=.8, main="True", xlab="x", ylab="y")
plot(X,col=solA, pch=19, cex=.8, main="dpmeans lbd=5", xlab="x", ylab="y")
plot(X,col=solB, pch=19, cex=.8, main="dpmeans lbd=10", xlab="x", ylab="y")
plot(X,col=solC, pch=19, cex=.8, main="dpmeans lbd=20", xlab="x", ylab="y")
par(opar)


## let's find variations by permuting orders of update
## used setting : lambda=20, we will 8 runs
sol8 &lt;- list()
for (i in 1:8){
  sol8[[i]] = dpmeans(X, lambda=20, permute.order=TRUE)$cluster
}

## let's visualize
vpar &lt;- par(no.readonly=TRUE)
par(mfrow=c(2,4), pty="s")
for (i in 1:8){
  pm = paste("permute no.",i,sep="")
  plot(X,col=sol8[[i]], pch=19, cex=.8, main=pm, xlab="x", ylab="y")
}
par(vpar)


</code></pre>

<hr>
<h2 id='ecdfdist'>Distance Measures between Multiple Empirical Cumulative Distribution Functions</h2><span id='topic+ecdfdist'></span>

<h3>Description</h3>

<p>We measure distance between two empirical cumulative distribution functions (ECDF). For 
simplicity, we only take an input of <code><a href="stats.html#topic+ecdf">ecdf</a></code> objects from <span class="pkg">stats</span> package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ecdfdist(elist, method = c("KS", "Lp", "Wasserstein"), p = 2, as.dist = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ecdfdist_+3A_elist">elist</code></td>
<td>
<p>a length <code class="reqn">N</code> list of <code>ecdf</code> objects.</p>
</td></tr>
<tr><td><code id="ecdfdist_+3A_method">method</code></td>
<td>
<p>name of the distance/dissimilarity measure. Case insensitive.</p>
</td></tr>
<tr><td><code id="ecdfdist_+3A_p">p</code></td>
<td>
<p>exponent for <code>Lp</code> or <code>Wasserstein</code> distance.</p>
</td></tr>
<tr><td><code id="ecdfdist_+3A_as.dist">as.dist</code></td>
<td>
<p>a logical; <code>TRUE</code> to return <code>dist</code> object, <code>FALSE</code> to return an <code class="reqn">(N\times N)</code> symmetric matrix of pairwise distances.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>either <code>dist</code> object of an <code class="reqn">(N\times N)</code> symmetric matrix of pairwise distances by <code>as.dist</code> argument.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+ecdf">ecdf</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## toy example : 10 of random and uniform distributions
mylist = list()
for (i in 1:10){
  mylist[[i]] = stats::ecdf(stats::rnorm(50, sd=2))
}
for (i in 11:20){
  mylist[[i]] = stats::ecdf(stats::runif(50, min=-5))
}

## compute Kolmogorov-Smirnov distance
dm = ecdfdist(mylist, method="KS")

## visualize
mks  =" KS distances of 2 Types"
opar = par(no.readonly=TRUE)
par(pty="s")
image(dm[,nrow(dm):1], axes=FALSE, main=mks)
par(opar)


</code></pre>

<hr>
<h2 id='ecdfdist2'>Pairwise Measures for Two Sets of Empirical CDFs</h2><span id='topic+ecdfdist2'></span>

<h3>Description</h3>

<p>We measure distance between two sets of empirical cumulative distribution functions (ECDF). For 
simplicity, we only take an input of <code><a href="stats.html#topic+ecdf">ecdf</a></code> objects from <span class="pkg">stats</span> package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ecdfdist2(elist1, elist2, method = c("KS", "Lp", "Wasserstein"), p = 2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ecdfdist2_+3A_elist1">elist1</code></td>
<td>
<p>a length <code class="reqn">M</code> list of <code>ecdf</code> objects.</p>
</td></tr>
<tr><td><code id="ecdfdist2_+3A_elist2">elist2</code></td>
<td>
<p>a length <code class="reqn">N</code> list of <code>ecdf</code> objects.</p>
</td></tr>
<tr><td><code id="ecdfdist2_+3A_method">method</code></td>
<td>
<p>name of the distance/dissimilarity measure. Case insensitive.</p>
</td></tr>
<tr><td><code id="ecdfdist2_+3A_p">p</code></td>
<td>
<p>exponent for <code>Lp</code> or <code>Wasserstein</code> distance.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an <code class="reqn">(M\times N)</code> matrix of pairwise distances.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+ecdf">ecdf</a></code> <code><a href="#topic+ecdfdist">ecdfdist</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## toy example
#  first list : 10 of random and uniform distributions
mylist1 = list()
for (i in 1:10){ mylist1[[i]] = stats::ecdf(stats::rnorm(50, sd=2))}
for (i in 11:20){mylist1[[i]] = stats::ecdf(stats::runif(50, min=-5))}

#  second list : 15 uniform and random distributions
mylist2 = list()
for (i in 1:15){ mylist2[[i]] = stats::ecdf(stats::runif(50, min=-5))}
for (i in 16:30){mylist2[[i]] = stats::ecdf(stats::rnorm(50, sd=2))}

## compute Kolmogorov-Smirnov distance
dm2ks = ecdfdist2(mylist1, mylist2, method="KS")
dm2lp = ecdfdist2(mylist1, mylist2, method="lp")
dm2wa = ecdfdist2(mylist1, mylist2, method="wasserstein")
nrs   = nrow(dm2ks)

## visualize
opar = par(no.readonly=TRUE)
par(mfrow=c(1,3), pty="s")
image(dm2ks[,nrs:1], axes=FALSE, main="Kolmogorov-Smirnov")
image(dm2lp[,nrs:1], axes=FALSE, main="L2")
image(dm2wa[,nrs:1], axes=FALSE, main="Wasserstein")
par(opar)


</code></pre>

<hr>
<h2 id='epmeans'>EP-means Algorithm for Clustering Empirical Distributions</h2><span id='topic+epmeans'></span>

<h3>Description</h3>

<p>EP-means is a variant of k-means algorithm adapted to cluster 
multiple empirical cumulative distribution functions under metric structure 
induced by Earth Mover's Distance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>epmeans(elist, k = 2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="epmeans_+3A_elist">elist</code></td>
<td>
<p>a length <code class="reqn">N</code> list of either vector or <code>ecdf</code> objects.</p>
</td></tr>
<tr><td><code id="epmeans_+3A_k">k</code></td>
<td>
<p>the number of clusters.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing </p>

<dl>
<dt>cluster</dt><dd><p>an integer vector indicating the cluster to which each <code>ecdf</code> is allocated.</p>
</dd>
<dt>centers</dt><dd><p>a length <code class="reqn">k</code> list of centroid <code>ecdf</code> objects.</p>
</dd>
</dl>



<h3>References</h3>

<p>Henderson K, Gallagher B, Eliassi-Rad T (2015).
&ldquo;EP-MEANS: An Efficient Nonparametric Clustering of Empirical Probability Distributions.&rdquo;
In <em>Proceedings of the 30th Annual ACM Symposium on Applied Computing - SAC '15</em>, 893&ndash;900.
ISBN 978-1-4503-3196-8.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## two sets of 1d samples, 10 each and add some noise
#    set 1 : mixture of two gaussians
#    set 2 : single gamma distribution

# generate data
elist = list()
for (i in 1:10){
   elist[[i]] = stats::ecdf(c(rnorm(100, mean=-2), rnorm(50, mean=2)))
}
for (j in 11:20){
   elist[[j]] = stats::ecdf(rgamma(100,1) + rnorm(100, sd=sqrt(0.5)))
}

# run EP-means with k clusters 
# change the value below to see different settings
myk   = 2
epout = epmeans(elist, k=myk)

# visualize
opar = par(no.readonly=TRUE)
par(mfrow=c(1,myk))
for (k in 1:myk){
  idk = which(epout$cluster==k)
  for (i in 1:length(idk)){
    if (i&lt;2){
      pm = paste("class ",k," (size=",length(idk),")",sep="")
      plot(elist[[idk[i]]], verticals=TRUE, lwd=0.25, do.points=FALSE, main=pm)
    } else {
      plot(elist[[idk[i]]], add=TRUE, verticals=TRUE, lwd=0.25, do.points=FALSE)
    }
    plot(epout$centers[[k]], add=TRUE, verticals=TRUE, lwd=2, col="red", do.points=FALSE)
  }
}
par(opar)


</code></pre>

<hr>
<h2 id='kmeanspp'>K-Means++ Clustering Algorithm</h2><span id='topic+kmeanspp'></span>

<h3>Description</h3>

<p><code class="reqn">k</code>-means++ algorithm is known to be a smart, careful initialization 
technique. It is originally intended to return a set of <code class="reqn">k</code> points 
as initial centers though it can still be used as a rough clustering algorithm 
by assigning points to the nearest points.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kmeanspp(data, k = 2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kmeanspp_+3A_data">data</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix whose rows are observations.</p>
</td></tr>
<tr><td><code id="kmeanspp_+3A_k">k</code></td>
<td>
<p>the number of clusters.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a length-<code class="reqn">n</code> vector of class labels.
</p>


<h3>References</h3>

<p>Arthur D, Vassilvitskii S (2007).
&ldquo;K-Means++: The Advantages of Careful Seeding.&rdquo;
In <em>In Proceedings of the 18th Annual ACM-SIAM Symposium on Discrete Algorithms</em>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## use simple example of iris dataset
data(iris) 
mydata = as.matrix(iris[,1:4])
mycol  = as.factor(iris[,5])

## find the low-dimensional embedding for visualization
my2d = cmds(mydata, ndim=2)$embed

## apply 'kmeanspp' with different numbers of k's.
k2 = kmeanspp(mydata, k=2)
k3 = kmeanspp(mydata, k=3)
k4 = kmeanspp(mydata, k=4)
k5 = kmeanspp(mydata, k=5)
k6 = kmeanspp(mydata, k=6)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(2,3))
plot(my2d, col=k2, main="k=2", pch=19, cex=0.5)
plot(my2d, col=k3, main="k=3", pch=19, cex=0.5)
plot(my2d, col=k4, main="k=4", pch=19, cex=0.5)
plot(my2d, col=k5, main="k=5", pch=19, cex=0.5)
plot(my2d, col=k6, main="k=6", pch=19, cex=0.5)
plot(my2d, col=mycol, main="true cluster", pch=19, cex=0.5)
par(opar)

</code></pre>

<hr>
<h2 id='lgpa'>Large-scale Generalized Procrustes Analysis</h2><span id='topic+lgpa'></span>

<h3>Description</h3>

<p>We modify generalized Procrustes analysis for large-scale data by 
first setting a subset of anchor points and applying the attained transformation 
to the rest data. If <code>sub.id</code> is a vector <code>1:dim(x)[1]</code>, it uses all 
observations as anchor points, reducing to the conventional generalized Procrustes analysis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lgpa(x, sub.id = 1:(dim(x)[1]), scale = TRUE, reflect = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lgpa_+3A_x">x</code></td>
<td>
<p>a <code class="reqn">(k\times m\times n)</code> 3d array, where <code class="reqn">k</code> is the number of points, <code class="reqn">m</code> the number of dimensions, and <code class="reqn">n</code> the number of samples.</p>
</td></tr>
<tr><td><code id="lgpa_+3A_sub.id">sub.id</code></td>
<td>
<p>a vector of indices for defining anchor points.</p>
</td></tr>
<tr><td><code id="lgpa_+3A_scale">scale</code></td>
<td>
<p>a logical; <code>TRUE</code> if scaling is applied, <code>FALSE</code> otherwise.</p>
</td></tr>
<tr><td><code id="lgpa_+3A_reflect">reflect</code></td>
<td>
<p>a logical; <code>TRUE</code> if reflection is required, <code>FALSE</code> otherwise.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code class="reqn">(k\times m\times n)</code> 3d array of aligned samples.
</p>


<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Goodall C (1991).
&ldquo;Procrustes Methods in the Statistical Analysis of Shape.&rdquo;
<em>Journal of the Royal Statistical Society. Series B (Methodological)</em>, <b>53</b>(2), 285&ndash;339.
ISSN 00359246.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## This should be run if you have 'shapes' package installed.
library(shapes)
data(gorf.dat)

## apply anchor-based method and original procGPA
out.proc = shapes::procGPA(gorf.dat, scale=TRUE)$rotated # procGPA from shapes package
out.anc4 = lgpa(gorf.dat, sub.id=c(1,4,9,7), scale=TRUE) # use 4 points 
out.anc7 = lgpa(gorf.dat, sub.id=1:7, scale=TRUE)        # use all but 1 point as anchors

## visualize
opar = par(no.readonly=TRUE)
par(mfrow=c(3,4), pty="s")
plot(out.proc[,,1], main="procGPA No.1", pch=18)
plot(out.proc[,,2], main="procGPA No.2", pch=18)
plot(out.proc[,,3], main="procGPA No.3", pch=18)
plot(out.proc[,,4], main="procGPA No.4", pch=18)
plot(out.anc4[,,1], main="4 Anchors No.1", pch=18, col="blue")
plot(out.anc4[,,2], main="4 Anchors No.2", pch=18, col="blue")
plot(out.anc4[,,3], main="4 Anchors No.3", pch=18, col="blue")
plot(out.anc4[,,4], main="4 Anchors No.4", pch=18, col="blue")
plot(out.anc7[,,1], main="7 Anchors No.1", pch=18, col="red")
plot(out.anc7[,,2], main="7 Anchors No.2", pch=18, col="red")
plot(out.anc7[,,3], main="7 Anchors No.3", pch=18, col="red")
plot(out.anc7[,,4], main="7 Anchors No.4", pch=18, col="red")
par(opar)

## End(Not run)

</code></pre>

<hr>
<h2 id='lyapunov'>Solve Lyapunov Equation</h2><span id='topic+lyapunov'></span>

<h3>Description</h3>

<p>The Lyapunov equation is of form
</p>
<p style="text-align: center;"><code class="reqn">AX + XA^\top = Q</code>
</p>

<p>where <code class="reqn">A</code> and <code class="reqn">Q</code> are square matrices of same size. Above form is also known as <em>continuous</em> form. 
This is a wrapper of <code>armadillo</code>'s <code>sylvester</code> function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lyapunov(A, Q)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lyapunov_+3A_a">A</code></td>
<td>
<p>a <code class="reqn">(p\times p)</code> matrix as above.</p>
</td></tr>
<tr><td><code id="lyapunov_+3A_q">Q</code></td>
<td>
<p>a <code class="reqn">(p\times p)</code> matrix as above.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a solution matrix <code class="reqn">X</code> of size <code class="reqn">(p\times p)</code>.
</p>


<h3>References</h3>

<p>Sanderson C, Curtin R (2016).
&ldquo;Armadillo: A Template-Based C++ Library for Linear Algebra.&rdquo;
<em>The Journal of Open Source Software</em>, <b>1</b>(2), 26.
</p>
<p>Eddelbuettel D, Sanderson C (2014).
&ldquo;RcppArmadillo: Accelerating R with High-Performance C++ Linear Algebra.&rdquo;
<em>Computational Statistics and Data Analysis</em>, <b>71</b>, 1054&ndash;1063.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## simulated example
#  generate square matrices
A = matrix(rnorm(25),nrow=5)
X = matrix(rnorm(25),nrow=5)
Q = A%*%X + X%*%t(A)

#  solve using 'lyapunov' function
solX = lyapunov(A,Q)
## Not run: 
pm1 = "* Experiment with Lyapunov Solver"
pm2 = paste("* Absolute Error  : ",norm(solX-X,"f"),sep="")
pm3 = paste("* Relative Error  : ",norm(solX-X,"f")/norm(X,"f"),sep="")
cat(paste(pm1,"\n",pm2,"\n",pm3,sep=""))

## End(Not run)

</code></pre>

<hr>
<h2 id='matderiv'>Numerical Approximation to Gradient of a Function with Matrix Argument</h2><span id='topic+matderiv'></span>

<h3>Description</h3>

<p>For a given function <code class="reqn">f:\mathbf{R}^{n\times p} \rightarrow \mathbf{R}</code>, 
we use finite difference scheme that approximates a gradient at a given point <code class="reqn">x</code>. 
In Riemannian optimization, this can be used as a proxy for 
ambient gradient. Use with care since it may accumulate numerical error.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>matderiv(fn, x, h = 0.001)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="matderiv_+3A_fn">fn</code></td>
<td>
<p>a function that takes a matrix of size <code class="reqn">(n\times p)</code> and returns a scalar value.</p>
</td></tr>
<tr><td><code id="matderiv_+3A_x">x</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix where the gradient is to be computed.</p>
</td></tr>
<tr><td><code id="matderiv_+3A_h">h</code></td>
<td>
<p>step size for centered difference scheme.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an approximate numerical gradient matrix of size <code class="reqn">(n\times p)</code>.
</p>


<h3>References</h3>

<p>Kincaid D, Cheney EW (2009).
<em>Numerical Analysis: Mathematics of Scientific Computing</em>,  number 2 in Pure and Applied Undergraduate Texts, 3. ed edition.
American Mathematical Society, Providence, RI.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## function f(X) = &lt;a,Xb&gt; for two vectors 'a' and 'b'
#  derivative w.r.t X is ab'
#  take an example of (5x5) symmetric positive definite matrix

#  problem settings
a   &lt;- rnorm(5)
b   &lt;- rnorm(5)
ftn &lt;- function(X){
  return(sum(as.vector(X%*%b)*a))
}       # function to be taken derivative
myX &lt;- matrix(rnorm(25),nrow=5)  # point where derivative is evaluated
myX &lt;- myX%*%t(myX)

# main computation
sol.true &lt;- base::outer(a,b)
sol.num1 &lt;- matderiv(ftn, myX, h=1e-1) # step size : 1e-1
sol.num2 &lt;- matderiv(ftn, myX, h=1e-5) #             1e-3
sol.num3 &lt;- matderiv(ftn, myX, h=1e-9) #             1e-5

## visualize/print the results
expar = par(no.readonly=TRUE)
par(mfrow=c(2,2),pty="s")
image(sol.true, main="true solution")
image(sol.num1, main="h=1e-1")
image(sol.num2, main="h=1e-5")
image(sol.num3, main="h=1e-9")
par(expar)


ntrue = norm(sol.true,"f")
cat('* Relative Errors in Frobenius Norm ')
cat(paste("*  h=1e-1   : ",norm(sol.true-sol.num1,"f")/ntrue,sep=""))
cat(paste("*  h=1e-5   : ",norm(sol.true-sol.num2,"f")/ntrue,sep=""))
cat(paste("*  h=1e-9   : ",norm(sol.true-sol.num3,"f")/ntrue,sep=""))


</code></pre>

<hr>
<h2 id='metricdepth'>Metric Depth</h2><span id='topic+metricdepth'></span>

<h3>Description</h3>

<p>Compute the metric depth proposed by Geenens et al. (2023), which is 
one generalization of statistical depth function onto the arbitrary metric space. Our implementation assumes that 
given the multivariate data it computes the (empirical) depth for all observations using under the Euclidean regime.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>metricdepth(data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="metricdepth_+3A_data">data</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix whose rows are observations.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a length-<code class="reqn">n</code> vector of empirical metric depth values.
</p>


<h3>References</h3>

<p>Geenens G, Nieto-Reyes A, Francisci G (2023).
&ldquo;Statistical Depth in Abstract Metric Spaces.&rdquo;
<em>Statistics and Computing</em>, <b>33</b>(2), 46.
ISSN 0960-3174, 1573-1375.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## use simple example of iris dataset
data(iris) 
X &lt;- as.matrix(iris[,1:4])
y &lt;- as.factor(iris[,5])

## compute the metric depth
mdX &lt;- metricdepth(X)

## visualize
#  2-d embedding for plotting by MDS
X2d &lt;- maotai::cmds(X, ndim=2)$embed

# get a color code for the metric depth
pal = colorRampPalette(c("yellow","red"))

# draw
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,2), pty="s")
plot(X2d, pch=19, main="by class", xlab="", ylab="", col=y)
plot(X2d, pch=19, main="by depth", xlab="", ylab="", col=pal(150)[order(mdX)])
legend("bottomright", col=pal(2), pch=19, legend=round(range(mdX), 2))
par(opar)

## End(Not run)

</code></pre>

<hr>
<h2 id='mmd2test'>Kernel Two-sample Test with Maximum Mean Discrepancy</h2><span id='topic+mmd2test'></span>

<h3>Description</h3>

<p>Maximum Mean Discrepancy (MMD) as a measure of discrepancy between 
samples is employed as a test statistic for two-sample hypothesis test 
of equal distributions. Kernel matrix <code class="reqn">K</code> is a symmetric square matrix 
that is positive semidefinite.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mmd2test(K, label, method = c("b", "u"), mc.iter = 999)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mmd2test_+3A_k">K</code></td>
<td>
<p>kernel matrix or an object of <code>kernelMatrix</code> class from <span class="pkg">kernlab</span> package.</p>
</td></tr>
<tr><td><code id="mmd2test_+3A_label">label</code></td>
<td>
<p>label vector of class indices.</p>
</td></tr>
<tr><td><code id="mmd2test_+3A_method">method</code></td>
<td>
<p>type of estimator to be used. <code>"b"</code> for biased and <code>"u"</code> for unbiased estimator of MMD.</p>
</td></tr>
<tr><td><code id="mmd2test_+3A_mc.iter">mc.iter</code></td>
<td>
<p>the number of Monte Carlo resampling iterations.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a (list) object of <code>S3</code> class <code>htest</code> containing: </p>

<dl>
<dt>statistic</dt><dd><p>a test statistic.</p>
</dd>
<dt>p.value</dt><dd><p><code class="reqn">p</code>-value under <code class="reqn">H_0</code>.</p>
</dd>
<dt>alternative</dt><dd><p>alternative hypothesis.</p>
</dd>
<dt>method</dt><dd><p>name of the test.</p>
</dd>
<dt>data.name</dt><dd><p>name(s) of provided kernel matrix.</p>
</dd>
</dl>



<h3>References</h3>

<p>Gretton A, Borgwardt KM, Rasch MJ, Schölkopf B, Smola A (2012).
&ldquo;A Kernel Two-Sample Test.&rdquo;
<em>J. Mach. Learn. Res.</em>, <b>13</b>, 723&ndash;773.
ISSN 1532-4435.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## small test for CRAN submission
dat1 &lt;- matrix(rnorm(60, mean= 1), ncol=2) # group 1 : 30 obs of mean  1
dat2 &lt;- matrix(rnorm(50, mean=-1), ncol=2) # group 2 : 25 obs of mean -1

dmat &lt;- as.matrix(dist(rbind(dat1, dat2)))  # Euclidean distance matrix
kmat &lt;- exp(-(dmat^2))                      # build a gaussian kernel matrix
lab  &lt;- c(rep(1,30), rep(2,25))             # corresponding label

mmd2test(kmat, lab)                         # run the code !

## Not run: 
## WARNING: computationally heavy. 
#  Let's compute empirical Type 1 error at alpha=0.05
niter = 496  
pvals1 = rep(0,niter)
pvals2 = rep(0,niter)
for (i in 1:niter){
  dat = matrix(rnorm(200),ncol=2)
  lab = c(rep(1,50), rep(2,50))
  lbd = 0.1
  kmat = exp(-lbd*(as.matrix(dist(dat))^2))
  pvals1[i] = mmd2test(kmat, lab, method="b")$p.value
  pvals2[i] = mmd2test(kmat, lab, method="u")$p.value
  print(paste("iteration ",i," complete..",sep=""))
}

#  Visualize the above at multiple significance levels
alphas = seq(from=0.001, to=0.999, length.out=100)
errors1 = rep(0,100)
errors2 = rep(0,100)
for (i in 1:100){
   errors1[i] = sum(pvals1&lt;=alphas[i])/niter
   errors2[i] = sum(pvals2&lt;=alphas[i])/niter
}

opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,2), pty="s")
plot(alphas, errors1, "b", main="Biased Estimator Error", 
     xlab="alpha", ylab="error", cex=0.5)
abline(a=0,b=1, lwd=1.5, col="red")
plot(alphas, errors2, "b", main="Unbiased Estimator Error", 
     xlab="alpha", ylab="error", cex=0.5)
abline(a=0,b=1, lwd=1.5, col="blue")
par(opar)

## End(Not run)

</code></pre>

<hr>
<h2 id='nef'>Negative Eigenfraction</h2><span id='topic+nef'></span>

<h3>Description</h3>

<p>Negative Eigenfraction (NEF) is a measure of distortion for the data 
whether they are lying in Euclidean manner or not. When the value is exactly 0, it means 
the data is Euclidean. On the other hand, when NEF is far away from 0, it means not Euclidean. 
The concept of NEF is closely related to the definiteness of a Gram matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nef(data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nef_+3A_data">data</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix whose rows are observations.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a nonnegative NEF value.
</p>


<h3>References</h3>

<p>Pękalska E, Harol A, Duin RPW, Spillmann B, Bunke H (2006).
&ldquo;Non-Euclidean or Non-Metric Measures Can Be Informative.&rdquo;
In Yeung D, Kwok JT, Fred A, Roli F, de Ridder D (eds.), <em>Structural, Syntactic, and Statistical Pattern Recognition</em>, 871&ndash;880.
ISBN 978-3-540-37241-7.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## use simple example of iris dataset 
data(iris) 
mydat = as.matrix(iris[,1:4])

## calculate NEF
nef(mydat)

</code></pre>

<hr>
<h2 id='nem'>Negative Eigenvalue Magnitude</h2><span id='topic+nem'></span>

<h3>Description</h3>

<p>Negative Eigenvalue Magnitude (NEM) is a measure of distortion for the data 
whether they are lying in Euclidean manner or not. When the value is exactly 0, it means 
the data is Euclidean. On the other hand, when NEM is far away from 0, it means not Euclidean. 
The concept of NEM is closely related to the definiteness of a Gram matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nem(data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nem_+3A_data">data</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix whose rows are observations.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a nonnegative NEM value.
</p>


<h3>References</h3>

<p>Pękalska E, Harol A, Duin RPW, Spillmann B, Bunke H (2006).
&ldquo;Non-Euclidean or Non-Metric Measures Can Be Informative.&rdquo;
In Yeung D, Kwok JT, Fred A, Roli F, de Ridder D (eds.), <em>Structural, Syntactic, and Statistical Pattern Recognition</em>, 871&ndash;880.
ISBN 978-3-540-37241-7.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## use simple example of iris dataset 
data(iris) 
mydat = as.matrix(iris[,1:4])

## calculate NEM
nem(mydat)

</code></pre>

<hr>
<h2 id='pdeterminant'>Calculate the Pseudo-Determinant of a Matrix</h2><span id='topic+pdeterminant'></span>

<h3>Description</h3>

<p>When a given square matrix <code class="reqn">A</code> is rank deficient, determinant is zero. 
Still, we can compute the pseudo-determinant by multiplying all non-zero 
eigenvalues. Since thresholding to determine near-zero eigenvalues is subjective, 
we implemented the function as of original limit problem. When matrix is 
non-singular, it coincides with traditional determinant.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pdeterminant(A)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pdeterminant_+3A_a">A</code></td>
<td>
<p>a square matrix whose pseudo-determinant be computed.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a scalar value for computed pseudo-determinant.
</p>


<h3>References</h3>

<p>Holbrook A (2018).
&ldquo;Differentiating the Pseudo Determinant.&rdquo;
<em>Linear Algebra and its Applications</em>, <b>548</b>, 293&ndash;304.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## show the convergence of pseudo-determinant
#  settings
n = 10
A = cov(matrix(rnorm(5*n),ncol=n))   # (n x n) matrix
k = as.double(Matrix::rankMatrix(A)) # rank of A

# iterative computation
ntry = 11
del.vec = exp(-(1:ntry))
det.vec = rep(0,ntry)
for (i in 1:ntry){
  del = del.vec[i]
  det.vec[i] = det(A+del*diag(n))/(del^(n-k))
}

# visualize the results
opar &lt;- par(no.readonly=TRUE)
plot(1:ntry, det.vec, main=paste("true rank is ",k," out of ",n,sep=""),"b", xlab="iterations")
abline(h=pdeterminant(A),col="red",lwd=1.2)
par(opar)

</code></pre>

<hr>
<h2 id='rotationS2'>Compute a Rotation on the 2-dimensional Sphere</h2><span id='topic+rotationS2'></span>

<h3>Description</h3>

<p>A vector of unit norm is an element on the hypersphere. When two unit-norm 
vectors <code class="reqn">x</code> and <code class="reqn">y</code> in 3-dimensional space are given, this function 
computes a rotation matrix <code class="reqn">Q</code> on the 2-dimensional sphere such that
</p>
<p style="text-align: center;"><code class="reqn">y=Qx</code>
</p>
<p>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rotationS2(x, y)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rotationS2_+3A_x">x</code></td>
<td>
<p>a length-<code class="reqn">3</code> vector. If <code class="reqn">\|x\|\neq 1</code>, normalization is applied.</p>
</td></tr>
<tr><td><code id="rotationS2_+3A_y">y</code></td>
<td>
<p>a length-<code class="reqn">3</code> vector. If <code class="reqn">\|y\|\neq 1</code>, normalization is applied.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code class="reqn">(3\times 3)</code> rotation matrix.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## generate two data points
#  one randomly and another on the north pole
x = stats::rnorm(3)
x = x/sqrt(sum(x^2))
y = c(0,0,1)

## compute the rotation
Q = rotationS2(x,y)

## compare 
Qx = as.vector(Q%*%x)

## print
printmat = rbind(Qx, y)
rownames(printmat) = c("rotated:", "target:")
print(printmat)


</code></pre>

<hr>
<h2 id='shortestpath'>Find Shortest Path using Floyd-Warshall Algorithm</h2><span id='topic+shortestpath'></span>

<h3>Description</h3>

<p>This is a fast implementation of Floyd-Warshall algorithm to find the
shortest path in a pairwise sense using <code>RcppArmadillo</code>. A logical input
is also accepted. The given matrix should contain pairwise distance values <code class="reqn">d_{i,j}</code> where 
<code class="reqn">0</code> means there exists no path for node <code class="reqn">i</code> and j.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>shortestpath(dist)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="shortestpath_+3A_dist">dist</code></td>
<td>
<p>either an <code class="reqn">(n\times n)</code> matrix or a <code>dist</code> class object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an <code class="reqn">(n\times n)</code> matrix containing pairwise shortest path length.
</p>


<h3>References</h3>

<p>Floyd RW (1962).
&ldquo;Algorithm 97: Shortest Path.&rdquo;
<em>Communications of the ACM</em>, <b>5</b>(6), 345.
</p>
<p>Warshall S (1962).
&ldquo;A Theorem on Boolean Matrices.&rdquo;
<em>Journal of the ACM</em>, <b>9</b>(1), 11&ndash;12.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## simple example : a ring graph
#  edges exist for pairs
A = array(0,c(10,10))
for (i in 1:9){
  A[i,i+1] = 1
  A[i+1,i] = 1
}
A[10,1] &lt;- A[1,10] &lt;- 1

# compute shortest-path and show the matrix
sdA &lt;- shortestpath(A)

# visualize
opar &lt;- par(no.readonly=TRUE)
par(pty="s")
image(sdA, main="shortest path length for a ring graph")
par(opar)

</code></pre>

<hr>
<h2 id='sylvester'>Solve Sylvester Equation</h2><span id='topic+sylvester'></span>

<h3>Description</h3>

<p>The Sylvester equation is of form
</p>
<p style="text-align: center;"><code class="reqn">AX + XB = C</code>
</p>

<p>where <code class="reqn">X</code> is the unknown and others are given. Though it's possible to have non-square <code class="reqn">A</code> and <code class="reqn">B</code> matrices, 
we currently support square matrices only. This is a wrapper of <code>armadillo</code>'s <code>sylvester</code> function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sylvester(A, B, C)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sylvester_+3A_a">A</code></td>
<td>
<p>a <code class="reqn">(p\times p)</code> matrix as above.</p>
</td></tr>
<tr><td><code id="sylvester_+3A_b">B</code></td>
<td>
<p>a <code class="reqn">(p\times p)</code> matrix as above.</p>
</td></tr>
<tr><td><code id="sylvester_+3A_c">C</code></td>
<td>
<p>a <code class="reqn">(p\times p)</code> matrix as above.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a solution matrix <code class="reqn">X</code> of size <code class="reqn">(p\times p)</code>.
</p>


<h3>References</h3>

<p>Sanderson C, Curtin R (2016).
&ldquo;Armadillo: A Template-Based C++ Library for Linear Algebra.&rdquo;
<em>The Journal of Open Source Software</em>, <b>1</b>(2), 26.
</p>
<p>Eddelbuettel D, Sanderson C (2014).
&ldquo;RcppArmadillo: Accelerating R with High-Performance C++ Linear Algebra.&rdquo;
<em>Computational Statistics and Data Analysis</em>, <b>71</b>, 1054&ndash;1063.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## simulated example
#  generate square matrices
A = matrix(rnorm(25),nrow=5)
X = matrix(rnorm(25),nrow=5)
B = matrix(rnorm(25),nrow=5)
C = A%*%X + X%*%B

#  solve using 'sylvester' function
solX = sylvester(A,B,C)
pm1 = "* Experiment with Sylvester Solver"
pm2 = paste("* Absolute Error  : ",norm(solX-X,"f"),sep="")
pm3 = paste("* Relative Error  : ",norm(solX-X,"f")/norm(X,"f"),sep="")
cat(paste(pm1,"\n",pm2,"\n",pm3,sep=""))


</code></pre>

<hr>
<h2 id='trio'>Trace Ratio Optimation</h2><span id='topic+trio'></span>

<h3>Description</h3>

<p>This function provides several algorithms to solve the following problem
</p>
<p style="text-align: center;"><code class="reqn">\textrm{max} \frac{tr(V^\top A V)}{tr(V^\top B V)} \textrm{ such that } V^\top C V = I</code>
</p>

<p>where <code class="reqn">V</code> is a projection matrix, i.e., <code class="reqn">V^\top V = I</code>. Trace ratio optimization 
is pertained to various linear dimension reduction methods. It should be noted that 
when <code class="reqn">C = I</code>, the above problem is often reformulated as a generalized eigenvalue problem 
since it's an easier proxy with faster computation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>trio(
  A,
  B,
  C,
  dim = 2,
  method = c("2003Guo", "2007Wang", "2009Jia", "2012Ngo"),
  maxiter = 1000,
  eps = 1e-10
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="trio_+3A_a">A</code></td>
<td>
<p>a <code class="reqn">(p\times p)</code> symmetric matrix in the numerator term.</p>
</td></tr>
<tr><td><code id="trio_+3A_b">B</code></td>
<td>
<p>a <code class="reqn">(p\times p)</code> symmetric matrix in the denomiator term.</p>
</td></tr>
<tr><td><code id="trio_+3A_c">C</code></td>
<td>
<p>a <code class="reqn">(p\times p)</code> symmetric constraint matrix. If not provided, it is set as identical matrix automatically.</p>
</td></tr>
<tr><td><code id="trio_+3A_dim">dim</code></td>
<td>
<p>an integer for target dimension. It can be considered as the number of loadings.</p>
</td></tr>
<tr><td><code id="trio_+3A_method">method</code></td>
<td>
<p>the name of algorithm to be used. Default is <code>2003Guo</code>.</p>
</td></tr>
<tr><td><code id="trio_+3A_maxiter">maxiter</code></td>
<td>
<p>maximum number of iterations to be performed.</p>
</td></tr>
<tr><td><code id="trio_+3A_eps">eps</code></td>
<td>
<p>stopping criterion for iterative algorithms.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>V</dt><dd><p>a <code class="reqn">(p\times dim)</code> projection matrix.</p>
</dd>
<dt>tr.val</dt><dd><p>an attained maximum scalar value.</p>
</dd>
</dl>



<h3>References</h3>

<p>Guo Y, Li S, Yang J, Shu T, Wu L (2003).
&ldquo;A Generalized Foley–Sammon Transform Based on Generalized Fisher Discriminant Criterion and Its Application to Face Recognition.&rdquo;
<em>Pattern Recognition Letters</em>, <b>24</b>(1-3), 147&ndash;158.
</p>
<p>Wang H, Yan S, Xu D, Tang X, Huang T (2007).
&ldquo;Trace Ratio vs. Ratio Trace for Dimensionality Reduction.&rdquo;
In <em>2007 IEEE Conference on Computer Vision and Pattern Recognition</em>, 1&ndash;8.
</p>
<p>Yangqing Jia, Feiping Nie, Changshui Zhang (2009).
&ldquo;Trace Ratio Problem Revisited.&rdquo;
<em>IEEE Transactions on Neural Networks</em>, <b>20</b>(4), 729&ndash;735.
</p>
<p>Ngo TT, Bellalij M, Saad Y (2012).
&ldquo;The Trace Ratio Optimization Problem.&rdquo;
<em>SIAM Review</em>, <b>54</b>(3), 545&ndash;569.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## simple test
#  problem setting
p = 5
mydim = 2
A = matrix(rnorm(p^2),nrow=p); A=A%*%t(A)
B = matrix(runif(p^2),nrow=p); B=B%*%t(B)
C = diag(p)

#  approximate solution via determinant ratio problem formulation
eigAB  = eigen(solve(B,A)) 
V      = eigAB$vectors[,1:mydim]
eigval = sum(diag(t(V)%*%A%*%V))/sum(diag(t(V)%*%B%*%V))

#  solve using 4 algorithms
m12 = trio(A,B,dim=mydim, method="2012Ngo")
m09 = trio(A,B,dim=mydim, method="2009Jia")
m07 = trio(A,B,dim=mydim, method="2007Wang")
m03 = trio(A,B,dim=mydim, method="2003Guo")

#  print the results
line1 = '* Evaluation of the cost function'
line2 = paste("* approx. via determinant : ",eigval,sep="")
line3 = paste("* trio by 2012Ngo         : ",m12$tr.val, sep="")
line4 = paste("* trio by 2009Jia         : ",m09$tr.val, sep="")
line5 = paste("* trio by 2007Wang        : ",m07$tr.val, sep="")
line6 = paste("* trio by 2003Guo         : ",m03$tr.val, sep="")
cat(line1,"\n",line2,"\n",line3,"\n",line4,"\n",line5,"\n",line6)

</code></pre>

<hr>
<h2 id='tsne'>t-SNE Embedding</h2><span id='topic+tsne'></span>

<h3>Description</h3>

<p>This function is a simple wrapper of <code><a href="Rtsne.html#topic+Rtsne">Rtsne</a></code> function for 
t-Stochastic Neighbor Embedding for finding low-dimensional structure of 
the data embedded in the high-dimensional space.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tsne(data, ndim = 2, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tsne_+3A_data">data</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix whose rows are observations.</p>
</td></tr>
<tr><td><code id="tsne_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="tsne_+3A_...">...</code></td>
<td>
<p>extra parameters to be used in <code><a href="Rtsne.html#topic+Rtsne">Rtsne</a></code> function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>embed</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>stress</dt><dd><p>discrepancy between embedded and origianl data as a measure of error.</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>
## use simple example of iris dataset 
data(iris) 
mydat = as.matrix(iris[,1:4])
mylab = as.factor(iris[,5])

## run t-SNE and MDS for comparison
iris.cmds = cmds(mydat, ndim=2)
iris.tsne = tsne(mydat, ndim=2)

## extract coordinates and class information
cx = iris.cmds$embed # embedded coordinates of CMDS
tx = iris.tsne$embed #                         t-SNE

## visualize
#  main title
mc = paste("CMDS with STRESS=",round(iris.cmds$stress,4),sep="")
mt = paste("tSNE with STRESS=",round(iris.tsne$stress,4),sep="")

#  draw a figure
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,2))
plot(cx, col=mylab, pch=19, main=mc)
plot(tx, col=mylab, pch=19, main=mt)
par(opar)


</code></pre>

<hr>
<h2 id='weiszfeld'>Weiszfeld Algorithm for Computing L1-median</h2><span id='topic+weiszfeld'></span>

<h3>Description</h3>

<p>Geometric median, also known as L1-median, is a solution to the following problem
</p>
<p style="text-align: center;"><code class="reqn">\textrm{argmin} \sum_{i=1}^n \| x_i - y \|_2 </code>
</p>

<p>for a given data <code class="reqn">x_1,x_2,\ldots,x_n \in R^p</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>weiszfeld(X, weights = NULL, maxiter = 496, abstol = 1e-06)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="weiszfeld_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix for <code class="reqn">p</code>-dimensional signal. If vector is given, it is assumed that <code class="reqn">p=1</code>.</p>
</td></tr>
<tr><td><code id="weiszfeld_+3A_weights">weights</code></td>
<td>
<p><code>NULL</code> for equal weight <code>rep(1/n,n)</code>; otherwise, it has to be a vector of length <code class="reqn">n</code>.</p>
</td></tr>
<tr><td><code id="weiszfeld_+3A_maxiter">maxiter</code></td>
<td>
<p>maximum number of iterations.</p>
</td></tr>
<tr><td><code id="weiszfeld_+3A_abstol">abstol</code></td>
<td>
<p>stopping criterion</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## generate sin(x) data with noise for 100 replicates
set.seed(496)
t = seq(from=0,to=10,length.out=20)
X = array(0,c(100,20))
for (i in 1:100){
   X[i,] = sin(t) + stats::rnorm(20, sd=0.5)
}

## compute L1-median and L2-mean
vecL2 = base::colMeans(X)
vecL1 = weiszfeld(X)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3), pty="s")
matplot(t(X[1:5,]), type="l", main="5 generated data", ylim=c(-2,2))
plot(t, vecL2, type="l", col="blue", main="L2-mean",   ylim=c(-2,2))
plot(t, vecL1, type="l", col="red",  main="L1-median", ylim=c(-2,2))
par(opar)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
