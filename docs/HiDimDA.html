<!DOCTYPE html><html><head><title>Help for package HiDimDA</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {HiDimDA}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#HiDimDA-package'><p>High Dimensional Discriminant Analysis</p></a></li>
<li><a href='#AlonDS'><p>Alon Colon Cancer Data Set</p></a></li>
<li><a href='#canldaRes'><p>Class object used for storing the results of a canonical high-dimensional linear discriminant analysis.</p></a></li>
<li><a href='#clldaRes'><p>Class object used for storing the results of a  high-dimensional linear discriminant analysis routine</p>
(with &lsquo;ldafun&rsquo; argument set to &ldquo;classification&rdquo;).</a></li>
<li><a href='#CovE'><p>Generic methods for extracting covariance and inverse covariance matrices from objects storing the results of a Linear Discriminant Analysis</p></a></li>
<li><a href='#DACrossVal'><p>Cross Validation for Discriminant Analysis Classification Algorithms</p></a></li>
<li><a href='#Dlda'><p>Diagonal Linear Discriminant Analysis.</p></a></li>
<li><a href='#DMat'><p>DMat objects: diagonal matrices</p></a></li>
<li><a href='#FrobSigAp'><p>Approximation of Covariance Matrices from q-factor models</p></a></li>
<li><a href='#HiDimDA-internal'><p>Internal HiDimDA Functions</p></a></li>
<li><a href='#MatMult'><p>MatMult: Specialized matrix multiplication of &lsquo;DMat&rsquo;, &lsquo;ShrnkMat&rsquo;, &lsquo;ShrnkMatInv&rsquo;, &lsquo;SigFq&rsquo; and &lsquo;SigFqInv&rsquo; objects.</p></a></li>
<li><a href='#Mlda'><p>Maximum uncertainty Linear Discriminant Analysis.</p></a></li>
<li><a href='#MldaInvE'><p>Maximum uncertainty Linear Discriminant Analysis inverse matrix estimator.</p></a></li>
<li><a href='#RFlda'><p> High-Dimensional Factor-based Linear Discriminant Analysis.</p></a></li>
<li><a href='#SelectV'><p>Variable Selection for High-Dimensional Supervised Classification.</p></a></li>
<li><a href='#ShrnkMat'><p>ShrnkMat objects: shrunken matrix estimates of a covariance</p></a></li>
<li><a href='#ShrnkMatInv'><p>ShrnkMatInv objects: precision (inverse of covariance) matrices associated with shrunken estimates of a covariance</p></a></li>
<li><a href='#ShrnkSigE'><p>Shrunken Covariance Estimate.</p></a></li>
<li><a href='#SigFq'><p>SigFq objects: covariance matrices associated with a q-factor model</p></a></li>
<li><a href='#SigFqInv'><p>SigFqInv objects: precision (inverse of covariance) matrices associated with a q-factor model</p></a></li>
<li><a href='#Slda'><p>Shrunken Linear Discriminant Analysis.</p></a></li>
<li><a href='#solve'><p>Solve methods for &lsquo;DMat&rsquo;, &lsquo;ShrnkMat&rsquo;, &lsquo;ShrnkMatInv&rsquo;, &lsquo;SigFq&rsquo; and &lsquo;SigFqInv&rsquo; objects.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>High Dimensional Discriminant Analysis</td>
</tr>
<tr>
<td>Version:</td>
<td>0.2-6</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-02-25</td>
</tr>
<tr>
<td>Author:</td>
<td>Antonio Pedro Duarte Silva &lt;psilva@ucp.pt&gt;</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Antonio Pedro Duarte Silva &lt;psilva@ucp.pt&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>splines</td>
</tr>
<tr>
<td>Suggests:</td>
<td>MASS</td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>yes</td>
</tr>
<tr>
<td>LazyData:</td>
<td>yes</td>
</tr>
<tr>
<td>Description:</td>
<td>Performs linear discriminant analysis in high dimensional
        problems based on reliable covariance estimators for problems
        with (many) more variables than observations. Includes routines
        for classifier training, prediction, cross-validation and
        variable selection.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-02-25 19:31:26 UTC; antonio</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-02-25 23:20:03 UTC</td>
</tr>
</table>
<hr>
<h2 id='HiDimDA-package'>High Dimensional Discriminant Analysis</h2><span id='topic+HiDimDA-package'></span><span id='topic+HiDimDA'></span>

<h3>Description</h3>

<p>Performs Linear Discriminant Analysis in High Dimensional problems based on reliable covariance estimators for problems with
(many) more variables than observations. Includes routines for classifier training, prediction, cross-validation and variable selection.</p>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> HiDimDA</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 0.2-6</td>
</tr>
<tr>
 <td style="text-align: left;">
Date: </td><td style="text-align: left;"> 2024-02-25</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL-3</td>
</tr>
<tr>
 <td style="text-align: left;">
LazyLoad: </td><td style="text-align: left;"> yes</td>
</tr>
<tr>
 <td style="text-align: left;">
LazyData: </td><td style="text-align: left;"> yes</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p>HiDimDA is a package for High-Dimensional Discriminant Analysis aimed at problems with many variables, possibly much more
than the number of available observations. Its core consists of the four Linear Discriminant Analyis routines:
</p>

<table>
<tr>
 <td style="text-align: left;">
 Dlda: </td><td style="text-align: left;"> Diagonal Linear Discriminant Analysis</td>
</tr>
<tr>
 <td style="text-align: left;">
Slda: </td><td style="text-align: left;"> Shrunken Linear Discriminant Analysis</td>
</tr>
<tr>
 <td style="text-align: left;">
Mlda: </td><td style="text-align: left;"> Maximum-uncertainty Linear Discriminant Analysis</td>
</tr>
<tr>
 <td style="text-align: left;">
RFlda: </td><td style="text-align: left;"> Factor-model  Linear Discriminant Analysis</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p>and the variable selection routine:
</p>

<table>
<tr>
 <td style="text-align: left;">
SelectV: </td><td style="text-align: left;"> High-Dimensional variable selection for supervised classification</td>
</tr>
<tr>
 <td style="text-align: left;"> 
</td>
</tr>

</table>

<p>that selects variables to be used in a Discriminant classification rule by
ranking them according to two-sample t-scores (problems with two-groups),
or ANOVA F-scores (problems wih more that two groups), and discarding those
with scores below a threshold defined by the Higher Criticism (HC) approach 
of Donoho and Jin (2008), the Expanded Higher Criticism scheme 
proposed by Duarte Silva (2011), False Discovery Rate (Fdr) control as suggested by 
Benjamini and Hochberg (1995), the FAIR approach of Fan and Fan (2008), or simply by 
fixing the number of retained variables to some pre-defined constant.
</p>
<p>All four discriminant routines, &lsquo;Dlda&rsquo;, &lsquo;Slda&rsquo;, &lsquo;Mlda&rsquo; and &lsquo;RFlda&rsquo;, compute Linear
Discriminant Functions, by default after a preliminary variable selection step, based on alternative estimators of 
a within-groups covariance matrix that leads to reliable allocation rules in problems where the number of selected
variables is close to, or larger than, the number of available observations.  
</p>
<p>Consider a Discriminant Analysis problem with <code class="reqn">k</code> groups, <code class="reqn">p</code> selected variables, a training sample consisting
of <code class="reqn">N = \sum_{g=1}^{k}n_g</code> observations with group and overall means, 
<code class="reqn">\bar{X}_g</code> and <code class="reqn">\bar{X}_.</code>, and a between-groups scatter (scaled by degrees of freedom) 
matrix, <code class="reqn">S_B = \frac{1}{N-k} \sum_{g=1}^{k} n_g (\bar{X}_g -\bar{X}_.)(\bar{X}_g -\bar{X}_.)^T </code>
</p>
<p>Following the two main classical approaches to Linear Discrimant Analysis, the Discriminant Functions returned by HiDimDA discriminant
routines are either based on the canonical linear discriminants given by the normalized eigenvectors
</p>
<p style="text-align: center;"><code class="reqn">LD_j = Egvct_j (S_B \hat{\Sigma}_W^{-1})</code>
</p>
  
<p style="text-align: center;"><code class="reqn">j = 1,...,r=min(p,k-1)</code>
</p>

<p style="text-align: center;"><code class="reqn">[LD_1, ..., LD_r]^T \hat{\Sigma}_W [LD_1, ..., LD_r] = I_r </code>
</p>
 
<p>or the classification functions 
</p>
<p style="text-align: center;"><code class="reqn">CF_g = (\bar{X}_g - \bar{X}_1) \hat{\Sigma}_W^{-1}</code>
</p>
 
<p style="text-align: center;"><code class="reqn">g = 2,...,k</code>
</p>
 
<p>where <code class="reqn">\hat{\Sigma}_W^{-1}</code> is an estimate of  the inverse within-groups covariance. 
</p>
<p>It is well known that these two approaches are equivalent, in the sense that classification rules that assign new observations to
the group with the closest (according to the Euclidean distance) centroid in the space of the canonical variates, 
<code class="reqn">Z = [LD_1 ... LD_r]^T X </code>, give the same results as the rule that assigns a new observation to group 1 if all classification scores, 
<code class="reqn">Clscr_g = CF_g^T X - CF_g^T  \frac{(\bar{X}_1 + \bar{X}_g)}{2} </code>, are negative, and to the group with the highest classification 
score otherwise.
</p>
<p>The discriminant routines of HiDimDA compute canonical linear discriminant functions by default, and classification functions when
the argument &lsquo;ldafun&rsquo; is set to &ldquo;classification&rdquo;. However, unlike traditional linear discriminant analysis where
<code class="reqn">\Sigma_W^{-1}</code> is estimated by the inverse of the sample covariance, 
which is not well-defined when <code class="reqn">p \geq N-k</code> and is unreliable if <code class="reqn">p</code> is close to <code class="reqn">N-k</code>, the routines of HiDimDA use 
four alternative well-conditioned estimators of <code class="reqn">\Sigma_W^{-1}</code> that lead to reliable classification rules if <code class="reqn">p</code> is larger than, 
or close to, <code class="reqn">N-k</code>.      
</p>
<p>In particular, &lsquo;Dlda&rsquo; estimates <code class="reqn">\Sigma_W^{-1}</code> by the diagonal matrix of inverse sample variances, &lsquo;Slda&rsquo; by
the inverse of an optimally shrunken Ledoit and Wolf's (2004) covariance estimate with the targets and optimal
target intensity estimators proposed by Fisher and Sun (2011), &lsquo;Mlda&rsquo; uses a regularized inverse
covariance that deemphasizes the importance given to the last eigenvectors of the sample covariance (see Thomaz, Kitani
and Gillies (2006) for details), and &lsquo;RFlda&rsquo; uses a factor model estimate of the true inverse correlation (or covariance)
matrix based on the approach of Duarte Silva (2011).
</p>
<p>The HiDimDA package also includes predict methods for all discriminant routines implemented, a routine (&lsquo;DACrossVal&rsquo;) for asssessing
the quality of the classification results by kfold cross-validation, and utilities for storing, extracting and efficiently handling  specialized high-dimensional covariance and inverse covariance matrix estimates. 
</p>


<h3>Author(s)</h3>

<p>Antonio Pedro Duarte Silva  &lt;psilva@porto.ucp.pt&gt;
</p>
<p>Maintainer: Antonio Pedro Duarte Silva  &lt;psilva@porto.ucp.pt&gt;
</p>


<h3>References</h3>

<p>Benjamini, Y. and Hochberg, Y. (1995) &ldquo;Controling the false discovery rate: A practical and powerful
approach to multiple testing&rdquo;, <em>Journal of the Royal Statistical Society</em> B, 57, 289-300.
</p>
<p>Donoho, D. and Jin, J. (2008) &ldquo;Higher criticism thresholding: Optimal feature selection when useful
features are rare and weak&rdquo;, In: <em>Proceedings National Academy of Sciences</em>, USA 105, 14790-14795.
</p>
<p>Fan, J. and Fan, Y. (2008) &ldquo;High-dimensional classification using features annealed independence rules&rdquo;,
<em>Annals of Statistics</em>, 36 (6), 2605-2637.
</p>
<p>Fisher, T.J. and Sun, X. (2011) &ldquo;Improved Stein-type shrinkage estimators for the high-dimensional multivariate normal covariance matrix&rdquo;, <em>Computational Statistics and Data Analysis</em>, 55 (1), 1909-1918. 
</p>
<p>Ledoit, O. and Wolf, M. (2004) &ldquo;A well-conditioned estimator for large-dimensional covariance matrices.&rdquo;, <em>Journal of Multivariate Analysis</em>, 88 (2), 365-411. 
</p>
<p>Pedro Duarte Silva, A. (2011) &ldquo;Two Group Classification with High-Dimensional Correlated Data: A Factor Model Approach&rdquo;, 
<em>Computational Statistics and Data Analysis</em>, 55 (1), 2975-2990.
</p>
<p>Thomaz, C.E. Kitani, E.C. and Gillies, D.F. (2006) &ldquo;A maximum uncertainty LDA-based approach for limited sample size problems - with application to face recognition&rdquo;, <em>Journal of the Brazilian Computer Society</em>, 12 (2), 7-18
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Dlda">Dlda</a></code>, <code><a href="#topic+Mlda">Mlda</a></code>, <code><a href="#topic+Slda">Slda</a></code>,<code><a href="#topic+RFlda">RFlda</a></code>, <code><a href="#topic+predict.canldaRes">predict.canldaRes</a></code>, <code><a href="#topic+predict.clldaRes">predict.clldaRes</a></code>, <code><a href="#topic+AlonDS">AlonDS</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>
# train the four main classifiers with their default setings 
# on Alon's colon data set (after a logarithmic transformation), 
# selecting genes by the Expanded HC scheme 

# Pre-process and select the genes to be used in the classifiers

log10genes &lt;- log10(AlonDS[,-1]) 
SelectionRes &lt;- SelectV(log10genes,AlonDS$grouping)
genesused &lt;- log10genes[SelectionRes$vkpt]

# Train classifiers

DiaglldaRule &lt;- Dlda(genesused,AlonDS$grouping)     
FactldaRule &lt;- RFlda(genesused,AlonDS$grouping)     
MaxUldaRule &lt;- Mlda(genesused,AlonDS$grouping)     
ShrkldaRule &lt;- Slda(genesused,AlonDS$grouping)     

# Get in-sample classification results

predict(DiaglldaRule,genesused,grpcodes=levels(AlonDS$grouping))$class         	       
predict(FactldaRule,genesused,grpcodes=levels(AlonDS$grouping))$class         	       
predict(MaxUldaRule,genesused,grpcodes=levels(AlonDS$grouping))$class         	       
predict(ShrkldaRule,genesused,grpcodes=levels(AlonDS$grouping))$class         	       

# Compare classifications with true assignments

cat("Original classes:\n")
print(AlonDS$grouping)             		 

# Show set of selected genes

cat("Genes kept in discrimination rule:\n")
print(colnames(genesused))             		 
cat("Number of selected genes =",SelectionRes$nvkpt,"\n")
</code></pre>

<hr>
<h2 id='AlonDS'>Alon Colon Cancer Data Set</h2><span id='topic+AlonDS'></span>

<h3>Description</h3>

<p>This data set was collected by Alon <em>et. al.</em> and consists of 2000 genes measured on 62 patients: 40 diagnosed with colon cancer and 22 healthy patients. The patient status is described by the factor &lsquo;grouping&rsquo; and the gene values are given by the numeric variables &lsquo;genes.1&rsquo; through &lsquo;genes.2000&rsquo;.</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(AlonDS)</code></pre>


<h3>Format</h3>

<p>A data frame containing 62 observations on one factor and 2000 numeric variables.</p>


<h3>Source</h3>

<p> Alon, U., Barkai, N., Notterman, D.A., Gish, K., Ybarra, S., Mack, D. and Levine, A.J. (1999) &ldquo;Broad patterns of gene expression revealed by clustering analysis of tumor and normal colon tissues probed by oligonucleotide arrays&rdquo;, In: <em>Proceedings National Academy of Sciences</em> USA 96, 6745-6750. The data set is available at http://microarray.princeton.edu/oncology.</p>

<hr>
<h2 id='canldaRes'>Class object used for storing the results of a canonical high-dimensional linear discriminant analysis.</h2><span id='topic+canldaRes'></span><span id='topic+predict.canldaRes'></span><span id='topic+print.canldaRes'></span><span id='topic+coef.canldaRes'></span><span id='topic+CovE.canldaRes'></span><span id='topic+ICovE.canldaRes'></span>

<h3>Description</h3>

<p>&lsquo;predict.canldaRes&rsquo; classifies multivariate observations in conjunction with a &lsquo;canldaRes&rsquo; object.
</p>
<p>&lsquo;print.canldaRes&rsquo; is the S3 print method for &lsquo;canldaRes&rsquo; objects.
</p>
<p>&lsquo;coef.canldaRes&rsquo; is the S3 coef method for &lsquo;canldaRes&rsquo; objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'canldaRes'
predict(object, newdata, prior=object$prior, grpcodes=NULL, 
nbvrs=ncol(object$scaling), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="canldaRes_+3A_object">object</code></td>
<td>
<p>An object of class &lsquo;canldaRes&rsquo;.</p>
</td></tr>
<tr><td><code id="canldaRes_+3A_newdata">newdata</code></td>
<td>
<p>Matrix of cases to be classified.</p>
</td></tr>
<tr><td><code id="canldaRes_+3A_prior">prior</code></td>
<td>
<p>The prior probabilities used.</p>
</td></tr> 
<tr><td><code id="canldaRes_+3A_grpcodes">grpcodes</code></td>
<td>
<p>Factor with the class codes. Set to 0:k-1 (k being the number of different classes) by default.</p>
</td></tr>
<tr><td><code id="canldaRes_+3A_nbvrs">nbvrs</code></td>
<td>
<p>Number of canonical discriminant variables used for prediction.</p>
</td></tr>
<tr><td><code id="canldaRes_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with components
</p>
<table>
<tr><td><code>class</code></td>
<td>
<p>The MAP classification (a factor)</p>
</td></tr>
<tr><td><code>ZsqDistances</code></td>
<td>
<p>A matrix with the squared Euclidean distance, in the discriminant space, of each new observation to the group centroids.</p>
</td></tr>
<tr><td><code>prior</code></td>
<td>
<p>The prior probabilities used.</p>
</td></tr> 
<tr><td><code>ZsqDprioradj</code></td>
<td>
<p>The adjustments to squared Euclidean distance, induced by the chosen (or estimated) prior probabilities.</p>
</td></tr>
<tr><td><code>Z</code></td>
<td>
<p>A matrix with the values of the canonical discriminant variates.</p>
</td></tr>
<tr><td><code>Zmeans</code></td>
<td>
<p>A matrix with the group centroids in the canonical space.</p>
</td></tr>
</table>


<h3>See Also</h3>

 <p><code><a href="#topic+Dlda">Dlda</a></code>, <code><a href="#topic+Mlda">Mlda</a></code>, <code><a href="#topic+Slda">Slda</a></code>, <code><a href="#topic+RFlda">RFlda</a></code>,  <code><a href="base.html#topic+print">print</a></code>, <code><a href="stats.html#topic+coef">coef</a></code> </p>

<hr>
<h2 id='clldaRes'>Class object used for storing the results of a  high-dimensional linear discriminant analysis routine 
(with &lsquo;ldafun&rsquo; argument set to &ldquo;classification&rdquo;).</h2><span id='topic+clldaRes'></span><span id='topic+predict.clldaRes'></span><span id='topic+print.clldaRes'></span><span id='topic+coef.clldaRes'></span><span id='topic+CovE.clldaRes'></span><span id='topic+ICovE.clldaRes'></span>

<h3>Description</h3>

<p>&lsquo;predict.clldaRes&rsquo; Classifies multivariate observations in conjunction with a &lsquo;clldaRes&rsquo; object.
</p>
<p>&lsquo;print.clldaRes&rsquo; is the S3 print method for &lsquo;clldaRes&rsquo; objects.
</p>
<p>&lsquo;coef.clldaRes&rsquo; is the S3 coef method for &lsquo;clldaRes&rsquo; objects. </p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'clldaRes'
predict(object, newdata, prior=object$prior, grpcodes=NULL,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="clldaRes_+3A_object">object</code></td>
<td>
<p>An object of class &lsquo;clldaRes&rsquo;.</p>
</td></tr>
<tr><td><code id="clldaRes_+3A_newdata">newdata</code></td>
<td>
<p>Matrix of cases to be classified.</p>
</td></tr>
<tr><td><code id="clldaRes_+3A_prior">prior</code></td>
<td>
<p>The prior probabilities used.</p>
</td></tr> 
<tr><td><code id="clldaRes_+3A_grpcodes">grpcodes</code></td>
<td>
<p>Factor with the class codes. Set to 0:k-1 (k being the number of different classes) by default.</p>
</td></tr>
<tr><td><code id="clldaRes_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with components
</p>
<table>
<tr><td><code>class</code></td>
<td>
<p>The MAP classification (a factor)</p>
</td></tr>
<tr><td><code>x</code></td>
<td>
<p>The classification scores of the test cases.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+Dlda">Dlda</a></code>, <code><a href="#topic+Mlda">Mlda</a></code>, <code><a href="#topic+Slda">Slda</a></code>, <code><a href="#topic+RFlda">RFlda</a></code>, <code><a href="base.html#topic+print">print</a></code>, <code><a href="stats.html#topic+coef">coef</a></code> </p>

<hr>
<h2 id='CovE'>Generic methods for extracting covariance and inverse covariance matrices from objects storing the results of a Linear Discriminant Analysis</h2><span id='topic+CovE'></span><span id='topic+ICovE'></span><span id='topic+CovE.Scanlda'></span><span id='topic+ICovE.Scanlda'></span><span id='topic+CovE.RFcanlda'></span><span id='topic+ICovE.RFcanlda'></span><span id='topic+CovE.Scllda'></span><span id='topic+ICovE.Scllda'></span><span id='topic+CovE.RFcllda'></span><span id='topic+ICovE.RFcllda'></span>

<h3>Description</h3>

<p>&lsquo;CovE&rsquo; Extracts an object with an appropriate representation of a whithin groups covariance matrix from a &lsquo;Scanlda&rsquo;, &lsquo;Scllda&rsquo;, &lsquo;RFcanlda&rsquo; or a &lsquo;RFcllda&rsquo; object.
</p>
<p>&lsquo;ICovE&rsquo; Extracts an object with an appropriate representation of a whithin groups inverse covariance matrix from a &lsquo;Scanlda&rsquo;, &lsquo;Scllda&rsquo;, &lsquo;RFcanlda&rsquo; or a &lsquo;RFcllda&rsquo; object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'Scanlda'
CovE(object)
## S3 method for class 'Scanlda'
ICovE(object)
## S3 method for class 'RFcanlda'
CovE(object)
## S3 method for class 'RFcanlda'
ICovE(object)
## S3 method for class 'Scllda'
CovE(object)
## S3 method for class 'Scllda'
ICovE(object)
## S3 method for class 'RFcllda'
CovE(object)
## S3 method for class 'RFcllda'
ICovE(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CovE_+3A_object">object</code></td>
<td>
<p>An object of class &lsquo;Scanlda&rsquo;, &lsquo;Scllda&rsquo;, &lsquo;RFcanlda&rsquo; or &lsquo;RFcllda&rsquo;.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object with an appropriate representation of the matrix extracted.</p>


<h3>See Also</h3>

<p><code><a href="#topic+Slda">Slda</a></code>, <code><a href="#topic+RFlda">RFlda</a></code>, <code><a href="#topic+canldaRes">canldaRes</a></code>, <code><a href="#topic+clldaRes">clldaRes</a></code> </p>

<hr>
<h2 id='DACrossVal'>Cross Validation for Discriminant Analysis Classification Algorithms</h2><span id='topic+DACrossVal'></span>

<h3>Description</h3>

<p>&lsquo;DACrossVal&rsquo; evaluates the performance of a Discriminant Analysis training algorithm by kfold 
Cross-Validation.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>DACrossVal(data, grouping, TrainAlg, EvalAlg=EvalClrule, 
Strfolds=TRUE, kfold=10, CVrep=20, prior="proportions", ...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="DACrossVal_+3A_data">data</code></td>
<td>
<p>Matrix or data frame of observations.</p>
</td></tr>
<tr><td><code id="DACrossVal_+3A_grouping">grouping</code></td>
<td>
<p>Factor specifying the class for each observation.</p>
</td></tr>
<tr><td><code id="DACrossVal_+3A_trainalg">TrainAlg</code></td>
<td>
<p>A function with the training algorithm. It should return an object that can be used as input to the argument of &lsquo;EValAlg&rsquo;.</p>
</td></tr>
<tr><td><code id="DACrossVal_+3A_evalalg">EvalAlg</code></td>
<td>
<p>A function with the evaluation algorithm. By default set to &lsquo;EvalClrule&rsquo; which returns a list with components &ldquo;err&rdquo; (estimates of error rates by class) and &ldquo;Ng&rdquo; (number of out-sample observations by class). This default can be used for all &lsquo;TrainAlg&rsquo; 
arguments that return an object with a predict method returning a list with a &lsquo;class&rsquo; component (a factor) containing the classification results.</p>
</td></tr>
<tr><td><code id="DACrossVal_+3A_strfolds">Strfolds</code></td>
<td>
<p>Boolean flag indicating if the folds should be stratified according to the original class
proportions (default), or randomly generated from the whole training sample, ignoring class membership.</p>
</td></tr>
<tr><td><code id="DACrossVal_+3A_kfold">kfold</code></td>
<td>
<p>Number of training sample folds to be created in each replication.</p>
</td></tr>
<tr><td><code id="DACrossVal_+3A_cvrep">CVrep</code></td>
<td>
<p>Number of replications to be performed.</p>
</td></tr>
<tr><td><code id="DACrossVal_+3A_prior">prior</code></td>
<td>
<p>The prior probabilities of class membership.  If unspecified, the class proportions for the training set are used.  If present, the probabilities should be specified in the order of the factor levels.</p>
</td></tr>
<tr><td><code id="DACrossVal_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed to &lsquo;TrainAlg&rsquo; and &lsquo;EvalAlg&rsquo;.</p>
</td></tr> 
</table>


<h3>Value</h3>

<p>A three dimensional array with the number of holdout observations, and estimated classification errors for each combination of fold and replication tried. The array dimensions are defined as follows:<br /> <br />
The first dimension runs through the different fold-replication combinations.<br /> <br />
The second dimension represents the classes. <br /> <br />
The third dimension has two named levels representing respectively the number of holdout observations (&ldquo;Ng&rdquo;), and the estimated classification errors (&ldquo;Clerr&rdquo;).</p>


<h3>Author(s)</h3>

<p>A. Pedro Duarte Silva</p>


<h3>See Also</h3>

<p><code><a href="MASS.html#topic+lda">lda</a></code>, <code><a href="#topic+RFlda">RFlda</a></code>, <code><a href="MASS.html#topic+predict.lda">predict.lda</a></code>, <code><a href="#topic+predict.clldaRes">predict.clldaRes</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Evaluate the performance of traditional (Fisher's) linear discriminant
# analysis on the iris data set, by ten-fold cross-validation replicated 
# three times.

library(MASS)
CrosValRes1 &lt;- DACrossVal(iris[1:4],iris$Species,TrainAlg=lda,CVrep=3)
summary(CrosValRes1[,,"Clerr"])

# Evaluate the performance on Alon's Colon Cancer Data set 
# (with a logarithmic transformation), of a one-factor 
# linear discriminant rule with the best fifty genes, 
# by four-fold cross-validation.

## Not run: 

CrosValRes2 &lt;- DACrossVal(log10(AlonDS[,-1]),AlonDS$grouping,TrainAlg=RFlda,
ldafun="classification",Selmethod="fixedp",maxp=50,kfold=4,CVrep=1)
summary(CrosValRes2[,,"Clerr"])


## End(Not run)

</code></pre>

<hr>
<h2 id='Dlda'>Diagonal Linear Discriminant Analysis.</h2><span id='topic+Dlda'></span><span id='topic+Dlda.default'></span><span id='topic+Dlda.data.frame'></span><span id='topic+is.Dlda'></span>

<h3>Description</h3>

<p>&lsquo;Dlda&rsquo; finds the coefficients of a linear discriminant rule based on a Diagonal covariance matrix estimator.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
## Default S3 method:
Dlda(data, grouping, prior = "proportions", VSelfunct = SelectV, 
ldafun=c("canonical","classification"), ...)

## S3 method for class 'data.frame'
Dlda(data, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Dlda_+3A_data">data</code></td>
<td>
<p>Matrix or data frame of observations.</p>
</td></tr>
<tr><td><code id="Dlda_+3A_grouping">grouping</code></td>
<td>
<p>Factor specifying the class for each observation.</p>
</td></tr>
<tr><td><code id="Dlda_+3A_prior">prior</code></td>
<td>
<p>The prior probabilities of class membership.  If unspecified, the class proportions for the training set are used.  If
present, the probabilities should be specified in the order of the factor levels.</p>
</td></tr>
<tr><td><code id="Dlda_+3A_vselfunct">VSelfunct</code></td>
<td>
<p>Variable selection function. Either the string &ldquo;none&rdquo; (no selection is to be performed) or a function that takes &lsquo;data&rsquo; and &lsquo;grouping&rsquo; as its first two arguments and returns a list with two components: (i) &lsquo;nvkpt&rsquo; - the number of variables to be used in the Discriminant rule; and (ii) &lsquo;vkptInd&rsquo; - the indices of the variables to be used in the Discriminant rule. The default is the &lsquo;SelectV&rsquo; function that, by default, selects variables by the Expanded HC scheme described in Duarte Silva (2011).</p>
</td></tr> 
<tr><td><code id="Dlda_+3A_ldafun">ldafun</code></td>
<td>
<p>Type of discriminant linear functions computed. The alternatives are &ldquo;canonical&rdquo; for maximum-discrimination canonical linear
functions and  &ldquo;classification&rdquo; for direct-classification linear functions.</p>
</td></tr>
<tr><td><code id="Dlda_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>If algument &lsquo;ldafun&rsquo; is set to &ldquo;canonical&rdquo; an object of class &lsquo;canldaRes&rsquo; with the following components:
</p>
<table>
<tr><td><code>prior</code></td>
<td>
<p>The prior probabilities used.</p>
</td></tr> 
<tr><td><code>means</code></td>
<td>
<p>The class means.</p>
</td></tr> 
<tr><td><code>scaling</code></td>
<td>
<p>A matrix which transforms observations to discriminant functions, normalized so that the within groups covariance matrix is spherical.</p>
</td></tr>
<tr><td><code>svd</code></td>
<td>
<p>The singular values, which give the ratio of the between- and within-group standard deviations on the linear discriminant variables.  Their squares are the canonical F-statistics.</p>
</td></tr>
<tr><td><code>vkpt</code></td>
<td>
<p>A vector with the indices of the variables kept in the discriminant rule if the number of variables kept is less than &lsquo;ncol(data)&rsquo;. NULL otherwise.</p>
</td></tr> 
<tr><td><code>nvkpt</code></td>
<td>
<p>The number of variables kept in the discriminant rule if this number is less than&lsquo;ncol(data)&rsquo;. NULL otherwise.</p>
</td></tr> 
<tr><td><code>N</code></td>
<td>
<p>The number of observations used.</p>
</td></tr> 
<tr><td><code>call</code></td>
<td>
<p>The (matched) function call.</p>
</td></tr> 
</table>
<p>If algument &lsquo;ldafun&rsquo; is set to &ldquo;classification&rdquo; an object of class &lsquo;clldaRes&rsquo; with the following components:
</p>
<table>
<tr><td><code>prior</code></td>
<td>
<p>The prior probabilities used.</p>
</td></tr> 
<tr><td><code>means</code></td>
<td>
<p>The class means.</p>
</td></tr> 
<tr><td><code>coef</code></td>
<td>
<p>A matrix with the coefficients of the k-1 classification functions.</p>
</td></tr>
<tr><td><code>cnst</code></td>
<td>
<p>A vector with the thresholds (2nd members of linear classification rules) used in classification rules that assume equal priors.</p>
</td></tr>
<tr><td><code>vkpt</code></td>
<td>
<p>A vector with the indices of the variables kept in the discriminant rule if the number of variables kept is less than &lsquo;ncol(data)&rsquo;. NULL otherwise.</p>
</td></tr> 
<tr><td><code>nvkpt</code></td>
<td>
<p>The number of variables kept in the discriminant rule if this number is less than&lsquo;ncol(data)&rsquo;. NULL otherwise.</p>
</td></tr> 
<tr><td><code>N</code></td>
<td>
<p>The number of observations used.</p>
</td></tr> 
<tr><td><code>call</code></td>
<td>
<p>The (matched) function call.</p>
</td></tr> 
</table>


<h3>Author(s)</h3>

<p>A. Pedro Duarte Silva</p>


<h3>See Also</h3>

<p><code><a href="#topic+SelectV">SelectV</a></code>, <code><a href="#topic+DMat">DMat</a></code>, <code><a href="#topic+predict.canldaRes">predict.canldaRes</a></code>, <code><a href="#topic+predict.clldaRes">predict.clldaRes</a></code>, <code><a href="#topic+AlonDS">AlonDS</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>
# train classifier on Alon's Colon Cancer Data Set 
#(after a logarithmic transformation). 

log10genes &lt;- log10(AlonDS[,-1])


ldarule &lt;- Dlda(log10genes,AlonDS$grouping)     

# show classification rule

print(ldarule)

# get in-sample classification results

predict(ldarule,log10genes,grpcodes=levels(AlonDS$grouping))$class           	       

# compare classifications with true assignments

cat("Original classes:\n")
print(AlonDS$grouping)             		 

# Estimate error rates by four-fold cross-validation.
# Note: In cross-validation analysis it is recommended to set 
# the argument 'ldafun' to "classification", in order to speed up 
# computations by avoiding unecessary eigen-decompositions 

## Not run: 

CrosValRes &lt;- DACrossVal(log10genes,AlonDS$grouping,TrainAlg=Dlda,
ldafun="classification",kfold=4,CVrep=1)
summary(CrosValRes[,,"Clerr"])


## End(Not run)
         	       
</code></pre>

<hr>
<h2 id='DMat'>DMat objects: diagonal matrices</h2><span id='topic+DMat'></span><span id='topic+is.DMat'></span><span id='topic+as.matrix.DMat'></span><span id='topic+print.DMat'></span><span id='topic++2B.DMat'></span><span id='topic+-.DMat'></span><span id='topic++2A.DMat'></span><span id='topic++2F.DMat'></span>

<h3>Description</h3>

<p>Creates a &lsquo;DMat&rsquo; object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>DMat(D)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="DMat_+3A_d">D</code></td>
<td>
<p>A vector with the diagonal elements of the matrix.</p>
</td></tr>
</table>


<h3>Value</h3>

<p> An object of class &lsquo;DMat&rsquo; for which the generic method &lsquo;as.matrix&rsquo; (converting to a traditional numeric matrix), as well as specialized methods for matrix inversion, multiplication, and element-wise arithmetic operations, are available.</p>


<h3>See Also</h3>

<p><code><a href="#topic+solve.DMat">solve.DMat</a></code>, <code><a href="#topic+LeftMult.DMat">LeftMult.DMat</a></code>, <code><a href="#topic+RightMult.DMat">RightMult.DMat</a></code></p>

<hr>
<h2 id='FrobSigAp'>Approximation of Covariance Matrices from q-factor models</h2><span id='topic+FrobSigAp'></span><span id='topic+FrobSigAp1'></span>

<h3>Description</h3>

<p>&lsquo;FrobSigAp&rsquo; finds the parameters of a q-factor model whose covariance is closest to the matrix argument &lsquo;Sigma&rsquo;, according to the Frobenius norm.
</p>
<p>&lsquo;FrobSigAp1&rsquo; finds the parameters of a q-factor model whose covariance is closest to the matrix square <code>SigmaSr %*% t(SigmaSr)</code> of the argument &lsquo;SigmaSr&rsquo;, according to the Frobenius norm. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
FrobSigAp(Sigma, q, nstarts = 1, k0 = NULL, penF = NULL, 
atol = 1e-20, rtol = sqrt(.Machine$double.eps))

FrobSigAp1(SigmaSr, SigmaRank, q, nstarts = 1, k0=NULL, penF=NULL, 
atol = 1e-20, rtol = 100 * sqrt(.Machine$double.eps))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="FrobSigAp_+3A_sigma">Sigma</code></td>
<td>
<p>Square, symmetric and positive-definite matrix to be approximated</p>
</td></tr>
<tr><td><code id="FrobSigAp_+3A_q">q</code></td>
<td>
<p>Number of factors in the assumed factor model.</p>
</td></tr>
<tr><td><code id="FrobSigAp_+3A_nstarts">nstarts</code></td>
<td>
<p>Number of different randomly generated starting points used in the optimization.</p>
</td></tr>
<tr><td><code id="FrobSigAp_+3A_k0">k0</code></td>
<td>
<p>Lower bound for the elements of the specific variances. When NULL (default), k0 is set to 0.01 times the minimum diagonal element of &lsquo;Sigma&rsquo;.</p>
</td></tr>
<tr><td><code id="FrobSigAp_+3A_penf">penF</code></td>
<td>
<p>Penalty factor, used to forbid specific variances below the k0 bound. When set to NULL (default), a penalty equal to 100 times the maximum diagonal element of &lsquo;Sigma&rsquo; is used.</p>
</td></tr>
<tr><td><code id="FrobSigAp_+3A_atol">atol</code></td>
<td>
<p>The absolute convergence tolerance of the local optimizer.</p>
</td></tr>
<tr><td><code id="FrobSigAp_+3A_rtol">rtol</code></td>
<td>
<p>The relative convergence tolerance of the local optimizer. The local optimizer stops if it is unable to reduce the approximation error (err) by a factor of &lsquo;reltol *(abs(err) + reltol)&rsquo; at a step.</p>
</td></tr>
<tr><td><code id="FrobSigAp_+3A_sigmasr">SigmaSr</code></td>
<td>
<p>Matrix square root of the covariance to be approximated.</p>
</td></tr>
<tr><td><code id="FrobSigAp_+3A_sigmarank">SigmaRank</code></td>
<td>
<p>Rank of the covariance matrix to be approximated.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The minimization of the error Frobenius norm is performed by the &lsquo;nlminb&rsquo; PORT optimization routine. The actual computations of the errors, and their analytical gradients and hessians, are coded in C in order to speed up the algorithm.<br /> The minimization procedure takes the loadings (B) of the factor model as arguments, computes the optimal specific variances by the analytical formula <code>D = diag(Sigma) - diag(B %*% t(B))</code>, and uses the first q eigenvectors of &lsquo;Sigma&rsquo; as starting points of the optimization.<br />
For small values of q (1 or 2), this procedure seems to quickly converge to the global minimum of the approximation error. 
For larger values of q, the computational time can be much higher and multiple random starting points (that can be specified by the argument &lsquo;nstarts&rsquo;) may be required in order to escape local optima.
</p>


<h3>Value</h3>

<p> An object of class &lsquo;SigFq&rsquo; representing the covariance assumed by the closest q-factor model.
&lsquo;SigFq&rsquo; objects have specialized methods for matrix inversion, multiplication, and element-wise arithmetic operations.</p>


<h3>Author(s)</h3>

<p>A. Pedro Duarte Silva</p>


<h3>References</h3>

<p>Pedro Duarte Silva, A. (2011) &ldquo;Two Group Classification with High-Dimensional Correlated Data: A Factor Model Approach&rdquo;, 
<em>Computational Statistics and Data Analysis</em>, 55 (1), 2975-2990.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+RFlda">RFlda</a></code>, <code><a href="#topic+SigFq">SigFq</a></code>, <code><a href="#topic+SigFqInv">SigFqInv</a></code>, <code><a href="#topic+solve.SigFq">solve.SigFq</a></code>, <code><a href="#topic+solve.SigFqInv">solve.SigFqInv</a></code></p>

<hr>
<h2 id='HiDimDA-internal'>Internal HiDimDA Functions</h2><span id='topic+ForbSigap'></span><span id='topic+ForbSigap1'></span><span id='topic+RepLOptim'></span><span id='topic+buildB'></span><span id='topic+f'></span><span id='topic+f1'></span><span id='topic+fgrad'></span><span id='topic+fgrad1'></span><span id='topic+fhess'></span><span id='topic+fhess1'></span><span id='topic+grpmeans'></span><span id='topic+grpvar'></span><span id='topic+l2vnorm'></span><span id='topic+scalebygrps'></span><span id='topic+EvalClrule'></span><span id='topic+mylocmle'></span><span id='topic+mylocfdr'></span><span id='topic+loccov'></span><span id='topic+loccov2'></span><span id='topic+locfdrpval'></span><span id='topic+HC'></span><span id='topic+tscores'></span><span id='topic+fscores'></span><span id='topic+Genlda'></span><span id='topic+rghtsngv'></span>

<h3>Description</h3>

<p>Internal HiDimDA functions.
</p>


<h3>Note</h3>

<p>These are not to be called by the user (or in some cases are just
waiting for proper documentation to be written).
</p>

<hr>
<h2 id='MatMult'>MatMult: Specialized matrix multiplication of &lsquo;DMat&rsquo;, &lsquo;ShrnkMat&rsquo;, &lsquo;ShrnkMatInv&rsquo;, &lsquo;SigFq&rsquo; and &lsquo;SigFqInv&rsquo; objects.</h2><span id='topic+LeftMult'></span><span id='topic+RightMult'></span><span id='topic+LeftMult.matrix'></span><span id='topic+RightMult.matrix'></span><span id='topic+LeftMult.DMat'></span><span id='topic+RightMult.DMat'></span><span id='topic+LeftMult.ShrnkMat'></span><span id='topic+RightMult.ShrnkMat'></span><span id='topic+LeftMult.ShrnkMatInv'></span><span id='topic+RightMult.ShrnkMatInv'></span><span id='topic+LeftMult.SigFq'></span><span id='topic+RightMult.SigFq'></span><span id='topic+LeftMult.SigFqInv'></span><span id='topic+RightMult.SigFqInv'></span>

<h3>Description</h3>

<p>&lsquo;LeftMult&rsquo; multiplies, on the left, a vector or matrix of compatible dimensions, by a  &lsquo;DMat&rsquo;, &lsquo;ShrnkMat&rsquo;, &lsquo;ShrnkMatInv&rsquo;, &lsquo;SigFq&rsquo; or &lsquo;SigFqInv&rsquo; object.
</p>
<p>&lsquo;RightMult&rsquo; multiplies, on the right, a vector or matrix of compatible dimensions, by a  &lsquo;DMat&rsquo;, &lsquo;ShrnkMat&rsquo;, &lsquo;ShrnkMatInv&rsquo;, &lsquo;SigFq&rsquo; or &lsquo;SigFqInv&rsquo; object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'DMat'
LeftMult(x, a)
## S3 method for class 'DMat'
RightMult(x, a)
## S3 method for class 'ShrnkMat'
LeftMult(x, a)
## S3 method for class 'ShrnkMat'
RightMult(x, a)
## S3 method for class 'ShrnkMatInv'
LeftMult(x, a)
## S3 method for class 'ShrnkMatInv'
RightMult(x, a)
## S3 method for class 'SigFq'
LeftMult(x, a)
## S3 method for class 'SigFq'
RightMult(x, a)
## S3 method for class 'SigFqInv'
LeftMult(x, a)
## S3 method for class 'SigFqInv'
RightMult(x, a)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MatMult_+3A_x">x</code></td>
<td>
<p>An object of class &lsquo;DMat&rsquo;, &lsquo;ShrnkMat&rsquo;, &lsquo;ShrnkMatInv&rsquo;, &lsquo;SigFq&rsquo; or class &lsquo;SigFqInv&rsquo;, with a compact representation of a specialized square symmetric matrix.</p>
</td></tr> 
<tr><td><code id="MatMult_+3A_a">a</code></td>
<td>
<p>A vector, or matrix, by which &lsquo;x&rsquo; is to be multiplied.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector or a (traditional numeric) matrix with the result of the matrix product.</p>


<h3>See Also</h3>

<p><code><a href="#topic+DMat">DMat</a></code>, <code><a href="#topic+FrobSigAp">FrobSigAp</a></code>, <code><a href="#topic+ShrnkMat">ShrnkMat</a></code>, <code><a href="#topic+ShrnkMatInv">ShrnkMatInv</a></code>, <code><a href="#topic+SigFq">SigFq</a></code>, <code><a href="#topic+SigFqInv">SigFqInv</a></code>, <code><a href="#topic+solve.DMat">solve.DMat</a></code>, 
</p>
<p><code><a href="#topic+solve.ShrnkMat">solve.ShrnkMat</a></code>, <code><a href="#topic+solve.ShrnkMatInv">solve.ShrnkMatInv</a></code>, <code><a href="#topic+solve.SigFq">solve.SigFq</a></code>, <code><a href="#topic+solve.SigFqInv">solve.SigFqInv</a></code>
</p>

<hr>
<h2 id='Mlda'>Maximum uncertainty Linear Discriminant Analysis.</h2><span id='topic+Mlda'></span><span id='topic+Mlda.default'></span><span id='topic+Mlda.data.frame'></span><span id='topic+is.Mlda'></span>

<h3>Description</h3>

<p>&lsquo;Mlda&rsquo; finds the coefficients of a linear discriminant rule based on the &ldquo;Maximum uncertainty Linear Discriminant Analysis&rdquo; approach of Thomaz, Kitani and Gillies (2006).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
## Default S3 method:
Mlda(data, grouping, prior = "proportions", StddzData=TRUE, 
VSelfunct = SelectV, ldafun=c("canonical","classification"), 
PCAstep=FALSE, ...)

## S3 method for class 'data.frame'
Mlda(data, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Mlda_+3A_data">data</code></td>
<td>
<p>Matrix or data frame of observations.</p>
</td></tr>
<tr><td><code id="Mlda_+3A_grouping">grouping</code></td>
<td>
<p>Factor specifying the class for each observation.</p>
</td></tr>
<tr><td><code id="Mlda_+3A_prior">prior</code></td>
<td>
<p>The prior probabilities of class membership.  If unspecified, the class proportions for the training set are used.  If
present, the probabilities should be specified in the order of the factor levels.</p>
</td></tr>
<tr><td><code id="Mlda_+3A_stddzdata">StddzData</code></td>
<td>
<p>A boolean flag indicating whether the data should be standardized first (default) or used in their original scales.</p>
</td></tr>
<tr><td><code id="Mlda_+3A_vselfunct">VSelfunct</code></td>
<td>
<p>Variable selection function. Either the string &ldquo;none&rdquo; (no selection is to be performed) or a function that takes &lsquo;data&rsquo; and &lsquo;grouping&rsquo; as its first two arguments and returns a list with two components: (i) &lsquo;nvkpt&rsquo; - the number of variables to be used in the Discriminant rule; and (ii) &lsquo;vkptInd&rsquo; - the indices of the variables to be used in the Discriminant rule. The default is the &lsquo;SelectV&rsquo; function that, by default, selects variables by the Expanded HC scheme described in Duarte Silva (2011).</p>
</td></tr> 
<tr><td><code id="Mlda_+3A_ldafun">ldafun</code></td>
<td>
<p>Type of discriminant linear functions computed. The alternatives are &ldquo;canonical&rdquo; for maximum-discrimination canonical
linear functions and  &ldquo;classification&rdquo; for direct-classification linear functions.</p>
</td></tr>
<tr><td><code id="Mlda_+3A_pcastep">PCAstep</code></td>
<td>
<p>A flag indicating if data should be first projected into the space spanned by its first nrow(data)-1 Principal Components in problems where nrow(data)-1 is less than the number of selected variables. In applications with a very large number of useful variables seting PCAstep to TRUE avoids many potential memory problems and tends to substantially increase the size of the data sets that can be analyzed by Mlda.</p>
</td></tr>
<tr><td><code id="Mlda_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>If algument &lsquo;ldafun&rsquo; is set to &ldquo;canonical&rdquo; an object of class &lsquo;canldaRes&rsquo; with the following components:
</p>
<table>
<tr><td><code>prior</code></td>
<td>
<p>The prior probabilities used.</p>
</td></tr> 
<tr><td><code>means</code></td>
<td>
<p>The class means.</p>
</td></tr> 
<tr><td><code>scaling</code></td>
<td>
<p>A matrix which transforms observations to discriminant functions, normalized so that the within groups covariance matrix is spherical.</p>
</td></tr>
<tr><td><code>svd</code></td>
<td>
<p>The singular values, which give the ratio of the between- and within-group standard deviations on the linear discriminant variables.  Their squares are the canonical F-statistics.</p>
</td></tr>
<tr><td><code>vkpt</code></td>
<td>
<p>A vector with the indices of the variables kept in the discriminant rule if the number of variables kept is less than &lsquo;ncol(data)&rsquo;. NULL otherwise.</p>
</td></tr> 
<tr><td><code>nvkpt</code></td>
<td>
<p>The number of variables kept in the discriminant rule if this number is less than&lsquo;ncol(data)&rsquo;. NULL otherwise.</p>
</td></tr> 
<tr><td><code>N</code></td>
<td>
<p>The number of observations used.</p>
</td></tr> 
<tr><td><code>call</code></td>
<td>
<p>The (matched) function call.</p>
</td></tr> 
</table>
<p>If algument &lsquo;ldafun&rsquo; is set to &ldquo;classification&rdquo; an object of class &lsquo;clldaRes&rsquo; with the following components:
</p>
<table>
<tr><td><code>prior</code></td>
<td>
<p>The prior probabilities used.</p>
</td></tr> 
<tr><td><code>means</code></td>
<td>
<p>The class means.</p>
</td></tr> 
<tr><td><code>coef</code></td>
<td>
<p>A matrix with the coefficients of the k-1 classification functions.</p>
</td></tr>
<tr><td><code>cnst</code></td>
<td>
<p>A vector with the thresholds (2nd members of linear classification rules) used in classification rules that assume equal priors.</p>
</td></tr>
<tr><td><code>vkpt</code></td>
<td>
<p>A vector with the indices of the variables kept in the discriminant rule if the number of variables kept is less than &lsquo;ncol(data)&rsquo;. NULL otherwise.</p>
</td></tr> 
<tr><td><code>nvkpt</code></td>
<td>
<p>The number of variables kept in the discriminant rule if this number is less than&lsquo;ncol(data)&rsquo;. NULL, otherwise.</p>
</td></tr> 
<tr><td><code>N</code></td>
<td>
<p>The number of observations used.</p>
</td></tr> 
<tr><td><code>call</code></td>
<td>
<p>The (matched) function call.</p>
</td></tr> 
</table>


<h3>Author(s)</h3>

<p>A. Pedro Duarte Silva</p>


<h3>References</h3>

<p>Pedro Duarte Silva, A. (2011) &ldquo;Two Group Classification with High-Dimensional Correlated Data: A Factor Model Approach&rdquo;, 
<em>Computational Statistics and Data Analysis</em>, 55 (1), 2975-2990.
</p>
<p>Thomaz, Kitani and Gillies (2006) &ldquo;A maximum uncertainty LDA-based approach for limited sample size problems - with application to face recognition&rdquo;, <em>Journal of the Brazilian Computer Society</em>, 12 (2), 7-18.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+SelectV">SelectV</a></code>, <code><a href="#topic+MldaInvE">MldaInvE</a></code>, <code><a href="#topic+predict.canldaRes">predict.canldaRes</a></code>, <code><a href="#topic+predict.clldaRes">predict.clldaRes</a></code>, <code><a href="#topic+AlonDS">AlonDS</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>
# train classifier on Alon's Colon Cancer Data Set 
# (after a logarithmic transformation). 

log10genes &lt;- log10(AlonDS[,-1])

ldarule &lt;- Mlda(log10genes,AlonDS$grouping)     

# show classification rule

print(ldarule)

# get in-sample classification results

predict(ldarule,log10genes,grpcodes=levels(AlonDS$grouping))$class           	       

# compare classifications with true assignments

cat("Original classes:\n")
print(AlonDS$grouping)             		 

# Estimate error rates by four-fold cross-validation.
# Note: In cross-validation analysis it is recommended to set 
# the argument 'ldafun' to "classification", in order to speed up 
# computations by avoiding unecessary eigen-decompositions 

## Not run: 

CrosValRes &lt;- DACrossVal(log10genes,AlonDS$grouping,TrainAlg=Mlda,
ldafun="classification",kfold=4,CVrep=1)
summary(CrosValRes[,,"Clerr"])
 

## End(Not run)       	       
</code></pre>

<hr>
<h2 id='MldaInvE'>Maximum uncertainty Linear Discriminant Analysis inverse matrix estimator.</h2><span id='topic+MldaInvE'></span>

<h3>Description</h3>

<p>Builds a well-conditioned estimator for the inverse of a symmetric positive definite matrix, with a 
bad-conditioned or singular estimate, based on the &ldquo;Maximum Uncertainty Linear Discriminant Analysis&rdquo; 
approach of Thomaz, Kitani and Gillies (2006).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  
MldaInvE(M, check=TRUE, onlyMinv=TRUE, 
numtol=sqrt(.Machine$double.eps))  
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MldaInvE_+3A_m">M</code></td>
<td>
<p>Singular or bad-conditioned estimate of the matrix for which a well-conditioned inverse estimate is sought.</p>
</td></tr>
<tr><td><code id="MldaInvE_+3A_check">check</code></td>
<td>
<p>Boolean flag indicating if the symmetry of M and the sign of its eigenvalues should be check upfront.</p>
</td></tr>
<tr><td><code id="MldaInvE_+3A_onlyminv">onlyMinv</code></td>
<td>
<p>Boolean flag indicating if only an estimate of the matrix inverse is sought, or if a well-conditioned 
approximation to the matrix that M estimates should be returned as well.</p>
</td></tr>
<tr><td><code id="MldaInvE_+3A_numtol">numtol</code></td>
<td>
<p>Numerical tolerance.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>If onlyMinv is set to true a matrix with the inverse estimate sought. Otherwise a list with components ME and MInvE,
with a well-conditioned approximation to the matrix that M estimates and its inverse.</p>


<h3>Author(s)</h3>

<p>A. Pedro Duarte Silva</p>


<h3>References</h3>

<p>Thomaz, Kitani and Gillies (2006) &ldquo;A maximum uncertainty LDA-based approach for limited sample size problems - with application to face recognition&rdquo;, <em>Journal of the Brazilian Computer Society</em>, 12 (2), 7-18</p>


<h3>See Also</h3>

<p><code><a href="#topic+Mlda">Mlda</a></code></p>

<hr>
<h2 id='RFlda'> High-Dimensional Factor-based Linear Discriminant Analysis.</h2><span id='topic+RFlda'></span><span id='topic+RFlda.default'></span><span id='topic+RFlda.data.frame'></span><span id='topic+is.RFlda'></span>

<h3>Description</h3>

<p>&lsquo;RFlda&rsquo; finds the coefficients of a linear discriminant rule based on a correlation (or covariance)
matrix estimator that tries to approximate the true correlation (covariance) by the closest (according to 
a Frobenius norm) correlation (covariance) compatible with a q-factor model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
## Default S3 method:
RFlda(data, grouping, q = 1, prior = "proportions", 
CorrAp = TRUE, maxq=5, VSelfunct = SelectV,
ldafun=c("canonical","classification"), nstarts = 1, 
CVqtrials=1:3, CVqfolds=3, CVqrep=1, CVqStrt=TRUE, ...)

## S3 method for class 'data.frame'
RFlda(data, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="RFlda_+3A_data">data</code></td>
<td>
<p>Matrix or data frame of observations.</p>
</td></tr>
<tr><td><code id="RFlda_+3A_grouping">grouping</code></td>
<td>
<p>Factor specifying the class for each observation.</p>
</td></tr>
<tr><td><code id="RFlda_+3A_q">q</code></td>
<td>
<p>Number of factors assumed by the model. This argument can be set to a fixed number between 1 and the argument of &lsquo;maxq&rsquo;, or to the string
&ldquo;CVq&rdquo;. In the latter case the number of factors is chosen amongst the values of the argument &lsquo;CVqtrials&rsquo;, by minimizing a cross-validated estimate of the error rate.</p>
</td></tr>
<tr><td><code id="RFlda_+3A_prior">prior</code></td>
<td>
<p>The prior probabilities of class membership.  If unspecified, the class proportions for the training set are used.  If
present, the probabilities should be specified in the order of the factor levels.</p>
</td></tr>
<tr><td><code id="RFlda_+3A_corrap">CorrAp</code></td>
<td>
<p>A boolean flag indicating whether the approximation error of the correlation (default), or of the covariance matrix , should be minimized.</p>
</td></tr>
<tr><td><code id="RFlda_+3A_maxq">maxq</code></td>
<td>
<p>Upper limit on the values allowed for argument &lsquo;q&rsquo;.</p>
</td></tr>
<tr><td><code id="RFlda_+3A_vselfunct">VSelfunct</code></td>
<td>
<p>Variable selection function. Either the string &ldquo;none&rdquo; (no selection is to be performed) or a function that takes &lsquo;data&rsquo; and &lsquo;grouping&rsquo; as its first two arguments and returns a list with two components: (i) &lsquo;nvkpt&rsquo; - the number of variables to be used in the Discriminant rule; and (ii) &lsquo;vkptInd&rsquo; - the indices of the variables to be used in the Discriminant rule. The default is the &lsquo;SelectV&rsquo; function that, by default, selects variables by the Expanded HC scheme described in Duarte Silva (2011).</p>
</td></tr> 
<tr><td><code id="RFlda_+3A_ldafun">ldafun</code></td>
<td>
<p>Type of discriminant linear functions computed. The alternatives are &ldquo;canonical&rdquo; for maximum-discrimination canonical
linear functions and  &ldquo;classification&rdquo; for direct-classification linear functions.</p>
</td></tr>
<tr><td><code id="RFlda_+3A_nstarts">nstarts</code></td>
<td>
<p>Number of different randomly generated starting points used in the minimization of the Frobenius norm of the correlation (or covariance) matrix approximation.</p>
</td></tr>
<tr><td><code id="RFlda_+3A_cvqtrials">CVqtrials</code></td>
<td>
<p>Vector of values to be tried for the number of factors assumed by the model, when argument &lsquo;q&rsquo; is set to &ldquo;CVq&rdquo;.</p>
</td></tr>
<tr><td><code id="RFlda_+3A_cvqfolds">CVqfolds</code></td>
<td>
<p>Number of training sample folds to be created in each replication of the cross-validation procedure for choosing the number of factors, when argument &lsquo;q&rsquo; is set to &ldquo;CVq&rdquo;.</p>
</td></tr>
<tr><td><code id="RFlda_+3A_cvqrep">CVqrep</code></td>
<td>
<p>Number of replications to be performed in the cross-validation procedure for choosing the number of factors, when argument &lsquo;q&rsquo; is set to &ldquo;CVq&rdquo;.</p>
</td></tr>
<tr><td><code id="RFlda_+3A_cvqstrt">CVqStrt</code></td>
<td>
<p>Boolean flag indicating if, in the cross-validation procedure for choosing the number of factors when argument &lsquo;q&rsquo; is set to &ldquo;CVq&rdquo;, the folds should be stratified according to the original class proportions (default), or randomly generated from the whole training sample ignoring class membership.</p>
</td></tr>
<tr><td><code id="RFlda_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>If algument &lsquo;ldafun&rsquo; is set to &ldquo;canonical&rdquo; an object of class &lsquo;RFcanlda&rsquo;, which extends class &lsquo;canldaRes&rsquo;, with the following components:
</p>
<table>
<tr><td><code>prior</code></td>
<td>
<p>The prior probabilities used.</p>
</td></tr> 
<tr><td><code>means</code></td>
<td>
<p>The class means.</p>
</td></tr> 
<tr><td><code>scaling</code></td>
<td>
<p>A matrix which transforms observations to discriminant functions, normalized so that the within groups covariance matrix is spherical.</p>
</td></tr>
<tr><td><code>svd</code></td>
<td>
<p>The singular values, which give the ratio of the between- and within-group standard deviations on the linear discriminant variables.  Their squares are the canonical F-statistics.</p>
</td></tr>
<tr><td><code>vkpt</code></td>
<td>
<p>A vector with the indices of the variables kept in the discriminant rule if the number of variables kept is less than &lsquo;ncol(data)&rsquo;. NULL otherwise.</p>
</td></tr> 
<tr><td><code>nvkpt</code></td>
<td>
<p>The number of variables kept in the discriminant rule if this number is less than&lsquo;ncol(data)&rsquo;. NULL otherwise.</p>
</td></tr> 
<tr><td><code>q</code></td>
<td>
<p>The number of o factors used in the factor model chosen.</p>
</td></tr> 
<tr><td><code>SigFq</code></td>
<td>
<p>An object of class &lsquo;SigFq&rsquo; with the q-factor model approximation to the within groups covariance matrix. &lsquo;SigFq&rsquo; objects have specialized methods for matrix inversion, multiplication, and element-wise arithmetic operations.</p>
</td></tr>
<tr><td><code>SigFqInv</code></td>
<td>
<p>An object of class &lsquo;SigFqInv&rsquo; with the q-factor model approximation to the within groups precision (inverse covariance) matrix. &lsquo;SigFqInv&rsquo; objects have specialized methods for matrix inversion, multiplication, and element-wise arithmetic operations.</p>
</td></tr>
<tr><td><code>N</code></td>
<td>
<p>The number of observations used.</p>
</td></tr> 
<tr><td><code>call</code></td>
<td>
<p>The (matched) function call.</p>
</td></tr> 
</table>
<p>If algument &lsquo;ldafun&rsquo; is set to &ldquo;classification&rdquo; an object of class &lsquo;RFcllda&rsquo;, which extends class &lsquo;clldaRes&rsquo;, with the following components:
</p>
<table>
<tr><td><code>prior</code></td>
<td>
<p>The prior probabilities used.</p>
</td></tr> 
<tr><td><code>means</code></td>
<td>
<p>The class means.</p>
</td></tr> 
<tr><td><code>coef</code></td>
<td>
<p>A matrix with the coefficients of the k-1 classification functions.</p>
</td></tr>
<tr><td><code>cnst</code></td>
<td>
<p>A vector with the thresholds (2nd members of linear classification rules) used in classification rules that assume equal priors.</p>
</td></tr>
<tr><td><code>vkpt</code></td>
<td>
<p>A vector with the indices of the variables kept in the discriminant rule if the number of variables kept is less than &lsquo;ncol(data)&rsquo;. NULL otherwise.</p>
</td></tr> 
<tr><td><code>nvkpt</code></td>
<td>
<p>The number of variables kept in the discriminant rule if this number is less than&lsquo;ncol(data)&rsquo;. NULL, otherwise.</p>
</td></tr> 
<tr><td><code>q</code></td>
<td>
<p>The number of o factors used in the factor model chosen.</p>
</td></tr> 
<tr><td><code>SigFq</code></td>
<td>
<p>An object of class &lsquo;SigFq&rsquo; with the q-factor model approximation to the within groups covariance matrix. &lsquo;SigFq&rsquo; objects have specialized methods for matrix inversion, multiplication, and element-wise arithmetic operations.</p>
</td></tr>
<tr><td><code>SigFqInv</code></td>
<td>
<p>An object of class &lsquo;SigFqInv&rsquo; with the q-factor model approximation to the within groups precision (inverse covariance) matrix. &lsquo;SigFqInv&rsquo; objects have specialized methods for matrix inversion, multiplication, and element-wise arithmetic operations.</p>
</td></tr>
<tr><td><code>N</code></td>
<td>
<p>The number of observations used.</p>
</td></tr> 
<tr><td><code>call</code></td>
<td>
<p>The (matched) function call.</p>
</td></tr> 
</table>


<h3>Author(s)</h3>

<p>A. Pedro Duarte Silva</p>


<h3>References</h3>

<p>Pedro Duarte Silva, A. (2011) &ldquo;Two Group Classification with High-Dimensional Correlated Data: A Factor Model Approach&rdquo;, 
<em>Computational Statistics and Data Analysis</em>, 55 (1), 2975-2990.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+FrobSigAp">FrobSigAp</a></code>, <code><a href="#topic+SelectV">SelectV</a></code>, <code><a href="#topic+SigFq">SigFq</a></code>, <code><a href="#topic+SigFqInv">SigFqInv</a></code>, <code><a href="#topic+predict.canldaRes">predict.canldaRes</a></code>, <code><a href="#topic+predict.clldaRes">predict.clldaRes</a></code>,
<code><a href="#topic+AlonDS">AlonDS</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>
#train classifier with 10 genes (after a logarithmic transformation) 
# on Alon's Colon Cancer Data set. 

log10genes &lt;- log10(AlonDS[,-1])

ldarule1 &lt;- RFlda(log10genes,AlonDS$grouping,Selmethod="fixedp",maxp=10)     

# get in-sample classification results

predict(ldarule1,log10genes,grpcodes=levels(AlonDS$grouping))$class           	       

# compare classifications with true assignments

cat("Original classes:\n")
print(AlonDS$grouping)             		 

# Estimate error rates by four-fold cross-validation.
# Note: In cross-validation analysis it is recommended to set 
# the argument 'ldafun' to "classification", in order to speed up 
# computations by avoiding unecessary eigen-decompositions 

## Not run: 

CrosValRes1 &lt;- DACrossVal(log10genes,AlonDS$grouping,TrainAlg=RFlda,
Selmethod="fixedp",ldafun="classification",maxp=10,kfold=4,CVrep=1)
summary(CrosValRes1[,,"Clerr"])

# Find the best factor model amongst the choices q=1 or 2

ldarule2 &lt;- RFlda(log10genes,AlonDS$grouping,q="CVq",CVqtrials=1:2,
Selmethod="fixedp",ldafun="classification",maxp=10)
cat("Best error rate estimate found with q =",ldarule2$q,"\n")

# Perform the analysis finding the number of selected genes 
# by the Expanded HC scheme 

ldarule3 &lt;- RFlda(log10genes,AlonDS$grouping,q=ldarule2$q)     
cat("Number of selected genes =",ldarule3$nvkpt,"\n")

# get classification results

predict(ldarule3,log10genes,grpcodes=levels(AlonDS$grouping))$class           	       


## End(Not run)

</code></pre>

<hr>
<h2 id='SelectV'>Variable Selection for High-Dimensional Supervised Classification.</h2><span id='topic+SelectV'></span>

<h3>Description</h3>

<p>Selects variables to be used in a Discriminant Analysis classification rule.</p>


<h3>Usage</h3>

<pre><code class='language-R'>SelectV(data, grouping, 
Selmethod=c("ExpHC","HC","Fdr","Fair","fixedp"),
NullDist=c("locfdr","Theoretical"), uselocfdr=c("onlyHC","always"), 
minlocfdrp=200, comvar=TRUE, Fdralpha=0.5, 
ExpHCalpha=0.5, HCalpha0=0.1, maxp=ncol(data), tol=1E-12, ...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SelectV_+3A_data">data</code></td>
<td>
<p>Matrix or data frame of observations.</p>
</td></tr>
<tr><td><code id="SelectV_+3A_grouping">grouping</code></td>
<td>
<p>Factor specifying the class for each observation.</p>
</td></tr>
<tr><td><code id="SelectV_+3A_selmethod">Selmethod</code></td>
<td>
<p>The method used to choose the number of variables selected. Current alternatives are:
</p>
<p>&lsquo;ExpHC&rsquo; (default) for the Expanded Higher Criticism scheme of Duarte Silva (2011)
</p>
<p>&lsquo;HC&rsquo; for the Higher Criticism (HC) approach of Donoho and Jin (2004, 2008)
</p>
<p>&lsquo;Fdr&rsquo; for the False Discovery Rate control approach of Benjamini and Hochberg (1995)
</p>
<p>&lsquo;Fair&rsquo; for the FAIR (Features Annealed Independence Rules) approach of Fan and Fan (2008). This option is only available for two-group classification problems. 
</p>
<p>&lsquo;fixedp&rsquo; for a constant chosen by the user.
</p>
</td></tr>
<tr><td><code id="SelectV_+3A_nulldist">NullDist</code></td>
<td>
<p>The Null distribution used to compute pvalues from t-scores or F-scores. Current alternatives are &ldquo;Theoretical&rdquo; for the corresponding theoretical distributions, and &ldquo;locfdr&rdquo; for an empirical Null of z-scores estimated by the maximum likelihood approach of Efron (2004).</p>
</td></tr>
<tr><td><code id="SelectV_+3A_uselocfdr">uselocfdr</code></td>
<td>
<p>Flag indicating the statistics for which the Null empirical distribution estimated by the locfdr approach should be used. Current alternatives are &ldquo;onlyHC&rdquo; (default) and &ldquo;always&rdquo;.</p>
</td></tr>
<tr><td><code id="SelectV_+3A_minlocfdrp">minlocfdrp</code></td>
<td>
<p>Minimum number of variables required to estimate empirical Null distributions by the locfdr method. When the number of variables is below &lsquo;minlocfdrp&rsquo;, theoretical Nulls are always employed.</p>
</td></tr> 
<tr><td><code id="SelectV_+3A_comvar">comvar</code></td>
<td>
<p>Boolean flag indicating if a common group variance is to be assumed (default) in the computation of the t-scores used for problems with two groups.</p>
</td></tr>
<tr><td><code id="SelectV_+3A_fdralpha">Fdralpha</code></td>
<td>
<p>Control level for variable selection based on False Discovery Rate Control (see Benjamini and Hochberg (1995)).</p>
</td></tr>
<tr><td><code id="SelectV_+3A_exphcalpha">ExpHCalpha</code></td>
<td>
<p>Control level for the first step of the Extended Higher Criticism scheme (see Duarte Silva (2011)).</p>
</td></tr>
<tr><td><code id="SelectV_+3A_hcalpha0">HCalpha0</code></td>
<td>
<p>Proportion of pvalues used to compute the HC statistic (see Donoho and Jin (2004, 2008)).</p>
</td></tr>
<tr><td><code id="SelectV_+3A_maxp">maxp</code></td>
<td>
<p>Maximum number of variables to be used in the discriminant rule.</p>
</td></tr> 
<tr><td><code id="SelectV_+3A_tol">tol</code></td>
<td>
<p>Numerical precision for distinguishing pvalues from 0 and 1. Computed pvalues below &lsquo;tol&rsquo; are set to &lsquo;tol&rsquo;, and those above 1-&lsquo;tol&rsquo; are set to 1-&lsquo;tol&rsquo;.</p>
</td></tr>
<tr><td><code id="SelectV_+3A_...">...</code></td>
<td>
<p>Arguments passed from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function &lsquo;SelectV&rsquo; selects variables to be used in a Discriminant classification rule by the 
Higher Criticism (HC) approach of Donoho and Jin (2004, 2008), the Expanded Higher Criticism scheme 
proposed by Duarte Silva (2011), False Discovery Rate (Fdr) control as suggested by 
Benjamini and Hochberg (1995), the FAIR (Features Annealed Independence Rules) approach of Fan and Fan (2008),
or simply by fixing the number of selected variables to some pre-defined constant.
</p>
<p>The Fdr method is, by default, based on simple p-values  derived from t-scores (problems with two groups) or ANOVA F-scores 
(problems with more than two groups). 
When the argument &lsquo;NullDist&rsquo; is set to &ldquo;Theoretical&rdquo; these values are also used in the HC method. Otherwise, 
the HC p-values are derived from an empirical Null of z-scores estimated by the maximum likelihood 
approach of Efron (2004). 
</p>
<p>The variable rankings are based on absolute-value t-scores or ANOVA F-scores.
</p>


<h3>Value</h3>

<p>A list with two components: 
</p>
<table>
<tr><td><code>nvkpt</code></td>
<td>
<p>the number of variables to be used in the Discriminant rule</p>
</td></tr> 
<tr><td><code>vkptInd</code></td>
<td>
<p> the indices of the variables to be used in the Discriminant rule</p>
</td></tr> 
</table>


<h3>Author(s)</h3>

<p>A. Pedro Duarte Silva</p>


<h3>References</h3>

<p>Benjamini, Y. and Hochberg, Y. (1995) &ldquo;Controling the false discovery rate: A practical and powerful
approach to multiple testing&rdquo;, <em>Journal of the Royal Statistical Society</em> B, 57, 289-300.
</p>
<p>Donoho, D. and Jin, J. (2004) &ldquo;Higher criticism for detecting sparse heterogeneous mixtures&rdquo;,
<em>Annals of Statistics</em> 32, 962-964.
</p>
<p>Donoho, D. and Jin, J. (2008) &ldquo;Higher criticism thresholding: Optimal feature selection when useful
features are rare and weak&rdquo;, In: <em>Proceedings National Academy of Sciences</em>, USA 105, 14790-14795.
</p>
<p>Efron, B. (2004) &ldquo;Large-scale simultaneous hypothesis testing: the choice of a null hypothesis&rdquo;,
<em>Journal of the American Statistical Association</em> 99, 96-104.
</p>
<p>Fan, J. and Fan, Y. (2008) &ldquo;High-dimensional classification using features annealed independence rules&rdquo;,
<em>Annals of Statistics</em>, 36 (6), 2605-2637.
</p>
<p>Pedro Duarte Silva, A. (2011) &ldquo;Two Group Classification with High-Dimensional Correlated Data: A Factor Model Approach&rdquo;, 
<em>Computational Statistics and Data Analysis</em>, 55 (1), 2975-2990.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Dlda">Dlda</a></code>, <code><a href="#topic+Mlda">Mlda</a></code>, <code><a href="#topic+Slda">Slda</a></code>, <code><a href="#topic+RFlda">RFlda</a></code>, <code><a href="#topic+AlonDS">AlonDS</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 

# Compare the number of variables selected by the four methods 
# currently available  on Alon's Colon Cancer Data set 
# after a logarithmic transformation. 

log10genes &lt;- log10(AlonDS[,-1])

Res &lt;- array(dim=4)
names(Res) &lt;- c("ExpHC","HC","Fdr","Fair")
Res[1] &lt;- SelectV(log10genes,AlonDS[,1])$nvkpt
Res[2] &lt;- SelectV(log10genes,AlonDS[,1],Selmethod="HC")$nvkpt
Res[3] &lt;- SelectV(log10genes,AlonDS[,1],Selmethod="Fdr")$nvkpt
Res[4] &lt;- SelectV(log10genes,AlonDS[,1],Selmethod="Fair")$nvkpt

print(Res)

## End(Not run)

</code></pre>

<hr>
<h2 id='ShrnkMat'>ShrnkMat objects: shrunken matrix estimates of a covariance</h2><span id='topic+ShrnkMat'></span><span id='topic+is.ShrnkMat'></span><span id='topic+as.matrix.ShrnkMat'></span><span id='topic+print.ShrnkMat'></span><span id='topic++2B.ShrnkMat'></span><span id='topic+-.ShrnkMat'></span><span id='topic++2A.ShrnkMat'></span><span id='topic++2F.ShrnkMat'></span>

<h3>Description</h3>

<p>Creates a &lsquo;ShrnkMat&rsquo; object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ShrnkMat(U, D, p, q, Intst, Trgt="Idntty")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ShrnkMat_+3A_u">U</code></td>
<td>
<p>A p by q matrix with the orthonomal eigenvectors of the original (unshrunken) covariance estimate.</p>
</td></tr>
<tr><td><code id="ShrnkMat_+3A_d">D</code></td>
<td>
<p>A p-dimensional vector with the eigenvalues of the original (unshrunken) covariance estimate.</p>
</td></tr>
<tr><td><code id="ShrnkMat_+3A_p">p</code></td>
<td>
<p>The dimension of the covariance matrix.</p>
</td></tr>
<tr><td><code id="ShrnkMat_+3A_q">q</code></td>
<td>
<p>The rank of the original (unshrunken) covariance estimate.</p>
</td></tr> 
<tr><td><code id="ShrnkMat_+3A_intst">Intst</code></td>
<td>
<p>The target intensity used by the shunk estimator.</p>
</td></tr>
<tr><td><code id="ShrnkMat_+3A_trgt">Trgt</code></td>
<td>
<p>The target used by the shunk estimator. If set to the string &ldquo;Idntty&rdquo; (default)
a p-dimensional matrix identity target will be assumed. Otherwise a matrix-type object representing a symmetric matrix target.</p>
</td></tr>
</table>


<h3>Value</h3>

<p> An object of class &lsquo;ShrnkMat&rsquo; for which the generic method &lsquo;as.matrix&rsquo; (converting to a traditional numeric matrix), as well as specialized methods for matrix inversion, multiplication, and element-wise arithmetic operations, are available.</p>


<h3>See Also</h3>

 <p><code><a href="#topic+solve.ShrnkMat">solve.ShrnkMat</a></code>, <code><a href="#topic+LeftMult.ShrnkMat">LeftMult.ShrnkMat</a></code>, <code><a href="#topic+RightMult.ShrnkMat">RightMult.ShrnkMat</a></code>, <code><a href="#topic+ShrnkMatInv">ShrnkMatInv</a></code> </p>

<hr>
<h2 id='ShrnkMatInv'>ShrnkMatInv objects: precision (inverse of covariance) matrices associated with shrunken estimates of a covariance</h2><span id='topic+ShrnkMatInv'></span><span id='topic+is.ShrnkMatInv'></span><span id='topic+as.matrix.ShrnkMatInv'></span><span id='topic+print.ShrnkMatInv'></span><span id='topic++2B.ShrnkMatInv'></span><span id='topic+-.ShrnkMatInv'></span><span id='topic++2A.ShrnkMatInv'></span><span id='topic++2F.ShrnkMatInv'></span>

<h3>Description</h3>

<p>Creates a &lsquo;ShrnkMatInv&rsquo; object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ShrnkMatInv(U, D, p, q, Intst, Trgt="Idntty")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ShrnkMatInv_+3A_u">U</code></td>
<td>
<p>A p by q matrix with the orthonomal eigenvectors of the original (unshrunken) covariance estimate.</p>
</td></tr>
<tr><td><code id="ShrnkMatInv_+3A_d">D</code></td>
<td>
<p>A p-dimensional vector with the eigenvalues of the original (unshrunken) covariance estimate.</p>
</td></tr>
<tr><td><code id="ShrnkMatInv_+3A_p">p</code></td>
<td>
<p>The dimension of the covariance matrix.</p>
</td></tr>
<tr><td><code id="ShrnkMatInv_+3A_q">q</code></td>
<td>
<p>The rank of the original (unshrunken) covariance estimate.</p>
</td></tr> 
<tr><td><code id="ShrnkMatInv_+3A_intst">Intst</code></td>
<td>
<p>The target intensity used by the shunk estimator.</p>
</td></tr>
<tr><td><code id="ShrnkMatInv_+3A_trgt">Trgt</code></td>
<td>
<p>The target used by the shunk estimator. If set to the string &ldquo;Idntty&rdquo; (default)
a p-dimensional matrix identity target will be assumed. Otherwise a matrix-type object representing a symmetric matrix target.</p>
</td></tr>
</table>


<h3>Value</h3>

<p> An object of class &lsquo;ShrnkMatInv&rsquo; for which the generic method &lsquo;as.matrix&rsquo; (converting to a traditional numeric matrix), as well as specialized methods for matrix inversion, multiplication, and element-wise arithmetic operations, are available.</p>


<h3>See Also</h3>

<p><code><a href="#topic+ShrnkMat">ShrnkMat</a></code>, <code><a href="#topic+solve.ShrnkMatInv">solve.ShrnkMatInv</a></code>, <code><a href="#topic+LeftMult.ShrnkMatInv">LeftMult.ShrnkMatInv</a></code>, 
</p>
<p><code><a href="#topic+RightMult.ShrnkMatInv">RightMult.ShrnkMatInv</a></code>
</p>

<hr>
<h2 id='ShrnkSigE'>Shrunken Covariance Estimate.</h2><span id='topic+ShrnkSigE'></span>

<h3>Description</h3>

<p>Builds a well-conditioned shrunken estimate of a covariance matrix based on Fisher and Sun's (2011) estimates
and generalizations of Ledoit and Wolf's (2004) optimal optimal shrunken covariance matrices.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  
ShrnkSigE(df, p , SigmaRank, Sigma=NULL, SigmaSr=NULL, check=TRUE, 
Trgt, minp=20, numtol=sqrt(.Machine$double.eps), ...)  
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ShrnkSigE_+3A_df">df</code></td>
<td>
<p>Degrees of freedom of the original (unshrunken) covariance estimate.</p>
</td></tr>
<tr><td><code id="ShrnkSigE_+3A_p">p</code></td>
<td>
<p>Dimension of the covariance matrix.</p>
</td></tr>
<tr><td><code id="ShrnkSigE_+3A_sigmarank">SigmaRank</code></td>
<td>
<p>Rank of the original (unshrunken) covariance estimate.</p>
</td></tr>
<tr><td><code id="ShrnkSigE_+3A_sigma">Sigma</code></td>
<td>
<p>Original (unshrunken) covariance estimate.</p>
</td></tr>
<tr><td><code id="ShrnkSigE_+3A_sigmasr">SigmaSr</code></td>
<td>
<p>Matrix square-root of the original (unshrunken) covariance estimate, i.e. a matrix, SigmaSr, such that SigmaSr^T SigmaSr  = Sigma, where Sigma is the original unshrunken covariance estimate.</p>
</td></tr>
<tr><td><code id="ShrnkSigE_+3A_trgt">Trgt</code></td>
<td>
<p>A string code with the target type used by the shrunken estimator. The alternatives are &ldquo;CnstDiag&rdquo; for a Ledoit-Wolf constant diagonal target, &ldquo;Idntty&rdquo; for a p-dimensional identity, and &ldquo;VarDiag&rdquo; for a diagonal target of empirical variances.</p>
</td></tr> 
<tr><td><code id="ShrnkSigE_+3A_check">check</code></td>
<td>
<p>Boolean flag indicating if the symmetry and the sign of the Sigma eigenvalues should be check upfront.</p>
</td></tr>
<tr><td><code id="ShrnkSigE_+3A_minp">minp</code></td>
<td>
<p>Minimum number of variables required for the estimation of the target intensity to be considered reliable. If the dimension of Sigma is below pmin, no shrunken estimate is computed and the original sample covariance is returned.</p>
</td></tr>
<tr><td><code id="ShrnkSigE_+3A_numtol">numtol</code></td>
<td>
<p>Numerical tolerance. All computed eigenvalues below numtol are considered equal to zero, and the rank of original shrunken estimate is adjusted acordingly.</p>
</td></tr>
<tr><td><code id="ShrnkSigE_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>ShrnkSigE can take as input an original unshrunken estimate of the covariance matrix or, in alternative, one
matrix square-root, SigmaSr (e.g. the original, centered and scaled, data matrix), such that 
<code class="reqn">SigmaSr^T SigmaSr = Sigma</code>. 
In problems with more variables than observations it is preferable to use a matrix square-root for reasons of memory and 
computational efficiency.</p>


<h3>Value</h3>

<p>An object of class &lsquo;ShrnkMat&rsquo; with a compact representation of the shrunken covariance estimator.</p>


<h3>Author(s)</h3>

<p>A. Pedro Duarte Silva</p>


<h3>References</h3>

<p>Ledoit, O. and Wolf, M. (2004) &ldquo;A well-conditioned estimator for large-dimensional covariance matrices.&rdquo;, <em>Journal of Multivariate Analysis</em>, 88 (2), 365-411. 
</p>
<p>Fisher, T.J. and Sun, X. (2011) &ldquo;Improved Stein-type shrinkage estimators for the high-dimensional multivariate normal covariance matrix&rdquo;, <em>Computational Statistics and Data Analysis</em>, 55 (1), 1909-1918. 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ShrnkMat">ShrnkMat</a></code>, <code><a href="#topic+Slda">Slda</a></code></p>

<hr>
<h2 id='SigFq'>SigFq objects: covariance matrices associated with a q-factor model</h2><span id='topic+SigFq'></span><span id='topic+is.SigFq'></span><span id='topic+as.matrix.SigFq'></span><span id='topic+print.SigFq'></span><span id='topic++2B.SigFq'></span><span id='topic+-.SigFq'></span><span id='topic++2A.SigFq'></span><span id='topic++2F.SigFq'></span>

<h3>Description</h3>

<p>Creates a &lsquo;SigFq&rsquo; object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SigFq(D, B, p, q, optres=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SigFq_+3A_d">D</code></td>
<td>
<p>A p-dimensional vector with the specific variances of the assumed factor model.</p>
</td></tr>
<tr><td><code id="SigFq_+3A_b">B</code></td>
<td>
<p>A p by q matrix with the loadings of assumed factor model.</p>
</td></tr>
<tr><td><code id="SigFq_+3A_p">p</code></td>
<td>
<p>The full dimensionality (i.e., the number of original variables) of the process being modelled.</p>
</td></tr>
<tr><td><code id="SigFq_+3A_q">q</code></td>
<td>
<p>The number of factors in the assumed factor model.</p>
</td></tr> 
<tr><td><code id="SigFq_+3A_optres">optres</code></td>
<td>
<p>The full optimization results provided by the error minimization algorithm used to find SigFq parameters.</p>
</td></tr>
</table>


<h3>Value</h3>

<p> An object of class &lsquo;SigFq&rsquo; for which the generic method &lsquo;as.matrix&rsquo; (converting to a traditional numeric matrix), as well as specialized methods for matrix inversion, multiplication, and element-wise arithmetic operations, are available.</p>


<h3>See Also</h3>

<p><code><a href="#topic+FrobSigAp">FrobSigAp</a></code>, <code><a href="#topic+solve.SigFq">solve.SigFq</a></code>, <code><a href="#topic+LeftMult.SigFq">LeftMult.SigFq</a></code>, <code><a href="#topic+RightMult.SigFq">RightMult.SigFq</a></code>, <code><a href="#topic+SigFqInv">SigFqInv</a></code></p>

<hr>
<h2 id='SigFqInv'>SigFqInv objects: precision (inverse of covariance) matrices associated with a q-factor model</h2><span id='topic+SigFqInv'></span><span id='topic+is.SigFqInv'></span><span id='topic+as.matrix.SigFqInv'></span><span id='topic+print.SigFqInv'></span><span id='topic++2B.SigFqInv'></span><span id='topic+-.SigFqInv'></span><span id='topic++2A.SigFqInv'></span><span id='topic++2F.SigFqInv'></span>

<h3>Description</h3>

<p>Creates a &lsquo;SigFqInv&rsquo; object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SigFqInv(D, B, p, q, optres=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SigFqInv_+3A_d">D</code></td>
<td>
<p>A p-dimensional vector with the specific variances of the assumed factor model.</p>
</td></tr>
<tr><td><code id="SigFqInv_+3A_b">B</code></td>
<td>
<p>A p by q matrix with the loadings of assumed factor model.</p>
</td></tr>
<tr><td><code id="SigFqInv_+3A_p">p</code></td>
<td>
<p>The full dimensionality (i.e., the number of original variables) of the process being modelled.</p>
</td></tr>
<tr><td><code id="SigFqInv_+3A_q">q</code></td>
<td>
<p>The number of factors in the assumed factor model.</p>
</td></tr> 
<tr><td><code id="SigFqInv_+3A_optres">optres</code></td>
<td>
<p>The full optimization results provided by the error minimization algorithm used to find SigFq parameters.</p>
</td></tr>
</table>


<h3>Value</h3>

<p> An object of class &lsquo;SigFqInv&rsquo; for which the generic method &lsquo;as.matrix&rsquo; (converting to a traditional numeric matrix), as well as specialized methods for matrix inversion, multiplication, and element-wise arithmetic operations, are available.</p>


<h3>See Also</h3>

<p><code><a href="#topic+FrobSigAp">FrobSigAp</a></code>, <code><a href="#topic+solve.SigFqInv">solve.SigFqInv</a></code>, <code><a href="#topic+LeftMult.SigFqInv">LeftMult.SigFqInv</a></code>, <code><a href="#topic+RightMult.SigFqInv">RightMult.SigFqInv</a></code>, <code><a href="#topic+SigFq">SigFq</a></code></p>

<hr>
<h2 id='Slda'>Shrunken Linear Discriminant Analysis.</h2><span id='topic+Slda'></span><span id='topic+Slda.default'></span><span id='topic+Slda.data.frame'></span><span id='topic+is.Slda'></span>

<h3>Description</h3>

<p>&lsquo;Slda&rsquo; finds the coefficients of a linear discriminant rule based on Fisher and Sun's (2011) estimate and generalizations of Ledoit and Wolf's (2004) optimal shrunken covariance matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
## Default S3 method:
Slda(data, grouping, prior = "proportions", StddzData=TRUE, 
VSelfunct = SelectV, Trgt=c("CnstDiag","Idntty","VarDiag"), 
minp=20, ldafun=c("canonical","classification"), ...)

## S3 method for class 'data.frame'
Slda(data, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Slda_+3A_data">data</code></td>
<td>
<p>Matrix or data frame of observations.</p>
</td></tr>
<tr><td><code id="Slda_+3A_grouping">grouping</code></td>
<td>
<p>Factor specifying the class for each observation.</p>
</td></tr>
<tr><td><code id="Slda_+3A_prior">prior</code></td>
<td>
<p>The prior probabilities of class membership.  If unspecified, the class proportions for the training set are used.  If
present, the probabilities should be specified in the order of the factor levels.</p>
</td></tr>
<tr><td><code id="Slda_+3A_stddzdata">StddzData</code></td>
<td>
<p>A boolean flag indicating whether the data should be standardized first (default) or used in their original scales.</p>
</td></tr>
<tr><td><code id="Slda_+3A_vselfunct">VSelfunct</code></td>
<td>
<p>Variable selection function. Either the string &ldquo;none&rdquo; (no selection is to be performed) or a function that takes &lsquo;data&rsquo; and &lsquo;grouping&rsquo; as its first two arguments and returns a list with two components: (i) &lsquo;nvkpt&rsquo; - the number of variables to be used in the Discriminant rule; and (ii) &lsquo;vkptInd&rsquo; - the indices of the variables to be used in the Discriminant rule. The default is the &lsquo;SelectV&rsquo; function that, by default, selects variables by the Expanded HC scheme described in Duarte Silva (2011).</p>
</td></tr> 
<tr><td><code id="Slda_+3A_trgt">Trgt</code></td>
<td>
<p>A string code with the target type used by the shrunken estimator. The alternatives are &ldquo;CnstDiag&rdquo; for a Ledoit-Wolf constant diagonal target, &ldquo;Idntty&rdquo; for a p-dimensional identity, and &ldquo;VarDiag&rdquo; for a diagonal target of empirical variances.</p>
</td></tr> 
<tr><td><code id="Slda_+3A_minp">minp</code></td>
<td>
<p>Minimum number of variables required for the estimation of the target intensity to be considered reliable. If the dimension of Sigma is below pmin, no shrunken estimate is computed and the original sample covariance is employed.</p>
</td></tr>
<tr><td><code id="Slda_+3A_ldafun">ldafun</code></td>
<td>
<p>Type of discriminant linear functions computed. The alternatives are &ldquo;canonical&rdquo; for maximum-discrimination canonical
linear functions and &ldquo;classification&rdquo; for direct-classification linear functions.</p>
</td></tr>
<tr><td><code id="Slda_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>If algument &lsquo;ldafun&rsquo; is set to &ldquo;canonical&rdquo; an object of class &lsquo;Scanlda&rsquo;, which extends class &lsquo;canldaRes&rsquo;, with the following components:
</p>
<table>
<tr><td><code>prior</code></td>
<td>
<p>The prior probabilities used.</p>
</td></tr> 
<tr><td><code>means</code></td>
<td>
<p>The class means.</p>
</td></tr> 
<tr><td><code>scaling</code></td>
<td>
<p>A matrix which transforms observations to discriminant functions, normalized so that the within groups covariance matrix is spherical.</p>
</td></tr>
<tr><td><code>svd</code></td>
<td>
<p>The singular values, which give the ratio of the between- and within-group standard deviations on the linear discriminant variables.  Their squares are the canonical F-statistics.</p>
</td></tr>
<tr><td><code>vkpt</code></td>
<td>
<p>A vector with the indices of the variables kept in the discriminant rule if the number of variables kept is less than &lsquo;ncol(data)&rsquo;. NULL otherwise.</p>
</td></tr> 
<tr><td><code>nvkpt</code></td>
<td>
<p>The number of variables kept in the discriminant rule if this number is less than&lsquo;ncol(data)&rsquo;. NULL otherwise.</p>
</td></tr> 
<tr><td><code>SSig</code></td>
<td>
<p>An object of class &lsquo;ShrnkMat&rsquo; with a compact representation of the within groups covariance matrix.
&lsquo;ShrnkMat&rsquo; objects have specialized methods for matrix inversion, multiplication, and element-wise arithmetic operations.</p>
</td></tr>
<tr><td><code>SSigInv</code></td>
<td>
<p>An object of class &lsquo;ShrnkMatInv&rsquo; with a compact representation of the within groups precision (inverse covariance) matrix. &lsquo;ShrnkMatInv&rsquo; objects have specialized methods for matrix inversion, multiplication, and element-wise arithmetic operations.</p>
</td></tr>
<tr><td><code>N</code></td>
<td>
<p>The number of observations used.</p>
</td></tr> 
<tr><td><code>call</code></td>
<td>
<p>The (matched) function call.</p>
</td></tr> 
</table>
<p>If algument &lsquo;ldafun&rsquo; is set to &ldquo;classification&rdquo; an object of class &lsquo;Scllda&rsquo;, which extends class &lsquo;clldaRes&rsquo;, with the following components:
</p>
<table>
<tr><td><code>prior</code></td>
<td>
<p>The prior probabilities used.</p>
</td></tr> 
<tr><td><code>means</code></td>
<td>
<p>The class means.</p>
</td></tr> 
<tr><td><code>coef</code></td>
<td>
<p>A matrix with the coefficients of the k-1 classification functions.</p>
</td></tr>
<tr><td><code>cnst</code></td>
<td>
<p>A vector with the thresholds (2nd members of linear classification rules) used in classification rules that assume equal priors.</p>
</td></tr>
<tr><td><code>vkpt</code></td>
<td>
<p>A vector with the indices of the variables kept in the discriminant rule if the number of variables kept is less than &lsquo;ncol(data)&rsquo;. NULL otherwise.</p>
</td></tr> 
<tr><td><code>nvkpt</code></td>
<td>
<p>The number of variables kept in the discriminant rule if this number is less than&lsquo;ncol(data)&rsquo;. NULL, otherwise.</p>
</td></tr> 
<tr><td><code>SSig</code></td>
<td>
<p>An object of class &lsquo;ShrnkMat&rsquo; with a compact representation of the within groups covariance matrix.
&lsquo;ShrnkMat&rsquo; objects have specialized methods for matrix inversion, multiplication, and element-wise arithmetic operations.</p>
</td></tr>
<tr><td><code>SSigInv</code></td>
<td>
<p>An object of class &lsquo;ShrnkMatInv&rsquo; with a compact representation of the within groups precision (inverse covariance) matrix. &lsquo;ShrnkMatInv&rsquo; objects have specialized methods for matrix inversion, multiplication, and element-wise arithmetic operations.</p>
</td></tr>
<tr><td><code>N</code></td>
<td>
<p>The number of observations used.</p>
</td></tr> 
<tr><td><code>call</code></td>
<td>
<p>The (matched) function call.</p>
</td></tr> 
</table>


<h3>Author(s)</h3>

<p>A. Pedro Duarte Silva</p>


<h3>References</h3>

<p>Ledoit, O. and Wolf, M. (2004) &ldquo;A well-conditioned estimator for large-dimensional covariance matrices.&rdquo;, <em>Journal of Multivariate Analysis</em>, 88 (2), 365-411. 
</p>
<p>Fisher, T.J. and Sun, X. (2011) &ldquo;Improved Stein-type shrinkage estimators for the high-dimensional multivariate normal covariance matrix&rdquo;, <em>Computational Statistics and Data Analysis</em>, 55 (1), 1909-1918. 
</p>
<p>Pedro Duarte Silva, A. (2011) &ldquo;Two Group Classification with High-Dimensional Correlated Data: A Factor Model Approach&rdquo;, 
<em>Computational Statistics and Data Analysis</em>, 55 (1), 2975-2990.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+SelectV">SelectV</a></code>, <code><a href="#topic+ShrnkSigE">ShrnkSigE</a></code>, <code><a href="#topic+ShrnkMat">ShrnkMat</a></code>, <code><a href="#topic+ShrnkMatInv">ShrnkMatInv</a></code>, 
</p>
<p><code><a href="#topic+predict.canldaRes">predict.canldaRes</a></code>, <code><a href="#topic+predict.clldaRes">predict.clldaRes</a></code>, <code><a href="#topic+AlonDS">AlonDS</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# train classifier on Alon's Colon Cancer Data set 
# after a logarithmic transformation 
# (selecting genes by the Expanded HC scheme). 

ldarule &lt;- Slda(log10(AlonDS[,-1]),AlonDS$grouping)     

# show classification rule

print(ldarule)

# get in-sample classification results

predict(ldarule,log10(AlonDS[,-1]),grpcodes=levels(AlonDS$grouping))$class           	       

# compare classifications with true assignments

cat("Original classes:\n")
print(AlonDS[,1])             		 


# Estimate error rates by four-fold cross-validation.
# Note: In cross-validation analysis it is recommended to set 
# the argument 'ldafun' to "classification", in order to speed up 
# computations by avoiding unecessary eigen-decompositions 

## Not run: 

CrosValRes &lt;- DACrossVal(log10(AlonDS[,-1]),AlonDS$grouping,TrainAlg=Slda,
ldafun="classification",kfold=4,CVrep=1)
summary(CrosValRes[,,"Clerr"])


## End(Not run)
         	       
</code></pre>

<hr>
<h2 id='solve'>Solve methods for &lsquo;DMat&rsquo;, &lsquo;ShrnkMat&rsquo;, &lsquo;ShrnkMatInv&rsquo;, &lsquo;SigFq&rsquo; and &lsquo;SigFqInv&rsquo; objects.</h2><span id='topic+solve.DMat'></span><span id='topic+solve.ShrnkMat'></span><span id='topic+solve.ShrnkMatInv'></span><span id='topic+solve.SigFq'></span><span id='topic+solve.SigFqInv'></span>

<h3>Description</h3>

<p>These methods solve the equation <code>a %*% x = b</code> for &lsquo;x&rsquo;,
where &lsquo;a&rsquo; is a specialized square symmetric matrix represented by a 
&lsquo;DMat&rsquo;, &lsquo;ShrnkMat&rsquo;, &lsquo;ShrnkMatInv&rsquo;, &lsquo;SigFq&rsquo; 
or &lsquo;SigFqInv&rsquo; object, and &lsquo;b&rsquo; can be either a vector or a matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'DMat'
solve(a, b = NULL, ...)
## S3 method for class 'ShrnkMat'
solve(a, b = NULL, ...)
## S3 method for class 'ShrnkMatInv'
solve(a, b = NULL, ...)
## S3 method for class 'SigFq'
solve(a, b = NULL, ...)
## S3 method for class 'SigFqInv'
solve(a, b = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="solve_+3A_a">a</code></td>
<td>
<p>An object of type &lsquo;DMat&rsquo;, &lsquo;ShrnkMat&rsquo;, &lsquo;ShrnkMatInv&rsquo;, &lsquo;SigFq&rsquo; or &lsquo;SigFqInv&rsquo; representing a specialized symmetric square matrix.</p>
</td></tr>
<tr><td><code id="solve_+3A_b">b</code></td>
<td>
<p>A numeric vector or matrix giving the right-hand side(s) of the linear system. If missing, &lsquo;b&rsquo; is taken to be an identity matrix and solve will return an object representing the inverse of the matrix associated with &lsquo;a&rsquo;.</p>
</td></tr>
<tr><td><code id="solve_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The result returned depends on the values of the arguments. When &lsquo;b&rsquo; is not NULL, both functions return a numeric vector or matrix with the solution of the system. When &lsquo;b&rsquo; is NULL, &lsquo;solve.DMat&rsquo; returns a &lsquo;DMat&rsquo; object, &lsquo;solve.ShrnkMat&rsquo; a &lsquo;ShrnkMatInv&rsquo; object, &lsquo;solve.ShrnkMatInv&rsquo; a &lsquo;ShrnkMat&rsquo; object, &lsquo;solve.SigFq&rsquo; a &lsquo;SigFqInv&rsquo; object and &lsquo;solve.SigFqInv&rsquo; returns a &lsquo;SigFq&rsquo; object.</p>


<h3>See Also</h3>

<p><code><a href="#topic+DMat">DMat</a></code>, <code><a href="#topic+ShrnkMat">ShrnkMat</a></code>, <code><a href="#topic+ShrnkMatInv">ShrnkMatInv</a></code>, <code><a href="#topic+SigFq">SigFq</a></code>, <code><a href="#topic+SigFqInv">SigFqInv</a></code>, <code><a href="#topic+FrobSigAp">FrobSigAp</a></code>, <code><a href="base.html#topic+solve">solve</a></code></p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
