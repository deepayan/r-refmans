<!DOCTYPE html><html><head><title>Help for package hetGP</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {hetGP}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#hetGP-package'><p>Package hetGP</p></a></li>
<li><a href='#allocate_mult'><p>Allocation of replicates on existing designs</p></a></li>
<li><a href='#ato'><p> Assemble To Order (ATO) Data and Fits</p></a></li>
<li><a href='#bfs'><p> Bayes Factor Data</p></a></li>
<li><a href='#compareGP'><p>Likelihood-based comparison of models</p></a></li>
<li><a href='#cov_gen'><p>Correlation function of selected type, supporting both isotropic and product forms</p></a></li>
<li><a href='#crit_cSUR'><p>Contour Stepwise Uncertainty Reduction criterion</p></a></li>
<li><a href='#crit_EI'><p>Expected Improvement criterion</p></a></li>
<li><a href='#crit_ICU'><p>Integrated Contour Uncertainty criterion</p></a></li>
<li><a href='#crit_IMSPE'><p>Sequential IMSPE criterion</p></a></li>
<li><a href='#crit_MCU'><p>Maximum Contour Uncertainty criterion</p></a></li>
<li><a href='#crit_MEE'><p>Maximum Empirical Error criterion</p></a></li>
<li><a href='#crit_optim'><p>Criterion optimization</p></a></li>
<li><a href='#crit_qEI'><p>Parallel Expected improvement</p></a></li>
<li><a href='#crit_tMSE'><p>t-MSE criterion</p></a></li>
<li><a href='#deriv_crit_EI'><p>Derivative of EI criterion for GP models</p></a></li>
<li><a href='#deriv_crit_IMSPE'><p>Derivative of crit_IMSPE</p></a></li>
<li><a href='#f1d'><p>1d test function (1)</p></a></li>
<li><a href='#f1d_n'><p>Noisy 1d test function (1)</p>
Add Gaussian noise with variance r(x) = scale * (1.1 + sin(2 pi x))^2 to <code>f1d</code></a></li>
<li><a href='#f1d2'><p>1d test function (2)</p></a></li>
<li><a href='#f1d2_n'><p>Noisy 1d test function (2)</p>
Add Gaussian noise with variance r(x) = scale * (exp(sin(2 pi x)))^2 to <code>f1d2</code></a></li>
<li><a href='#find_reps'><p>Data preprocessing</p></a></li>
<li><a href='#horizon'><p>Adapt horizon</p></a></li>
<li><a href='#IMSPE'><p>Integrated Mean Square Prediction Error</p></a></li>
<li><a href='#IMSPE_optim'><p>IMSPE optimization</p></a></li>
<li><a href='#logLikH'><p>Generic Log-likelihood function</p>
This function can be used to compute loglikelihood for homGP/hetGP models</a></li>
<li><a href='#LOO_preds'><p>Leave one out predictions</p></a></li>
<li><a href='#mleCRNGP'><p>Gaussian process modeling with correlated noise</p></a></li>
<li><a href='#mleHetGP'><p>Gaussian process modeling with heteroskedastic noise</p></a></li>
<li><a href='#mleHetTP'><p>Student-t process modeling with heteroskedastic noise</p></a></li>
<li><a href='#mleHomGP'><p>Gaussian process modeling with homoskedastic noise</p></a></li>
<li><a href='#mleHomTP'><p>Student-T process modeling with homoskedastic noise</p></a></li>
<li><a href='#pred_noisy_input'><p>Gaussian process prediction prediction at a noisy input <code>x</code>, with centered Gaussian noise of variance <code>sigma_x</code>.</p>
Several options are available, with different efficiency/accuracy tradeoffs.</a></li>
<li><a href='#predict.CRNGP'><p>Gaussian process predictions using a GP object for correlated noise (of class <code>CRNGP</code>)</p></a></li>
<li><a href='#predict.hetGP'><p>Gaussian process predictions using a heterogeneous noise GP object (of class <code>hetGP</code>)</p></a></li>
<li><a href='#predict.hetTP'><p>Student-t process predictions using a heterogeneous noise TP object (of class <code>hetTP</code>)</p></a></li>
<li><a href='#predict.homGP'><p>Gaussian process predictions using a homoskedastic noise GP object (of class <code>homGP</code>)</p></a></li>
<li><a href='#predict.homTP'><p>Student-t process predictions using a homoskedastic noise GP object (of class <code>homGP</code>)</p></a></li>
<li><a href='#rebuild'><p>Import and export of hetGP objects</p></a></li>
<li><a href='#scores'><p>Score and RMSE function</p>
To asses the performance of the prediction, this function computes the root mean squared error and proper score function (also known as negative log-probability density).</a></li>
<li><a href='#simul'><p>Conditional simulation for CRNGP</p></a></li>
<li><a href='#simul.CRNGP'><p>Fast conditional simulation for a CRNGP model</p></a></li>
<li><a href='#sirEval'><p>SIR test problem</p></a></li>
<li><a href='#update.hetGP'><p>Update <code>"hetGP"</code>-class model fit with new observations</p></a></li>
<li><a href='#update.hetTP'><p>Update <code>"hetTP"</code>-class model fit with new observations</p></a></li>
<li><a href='#update.homGP'><p>Fast <code>homGP</code>-update</p></a></li>
<li><a href='#update.homTP'><p>Fast <code>homTP</code>-update</p></a></li>
<li><a href='#Wij'><p>Compute double integral of the covariance kernel over a [0,1]^d domain</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Heteroskedastic Gaussian Process Modeling and Design under
Replication</td>
</tr>
<tr>
<td>Version:</td>
<td>1.1.6</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-09-29</td>
</tr>
<tr>
<td>Author:</td>
<td>Mickael Binois, Robert B. Gramacy</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Mickael Binois &lt;mickael.binois@inria.fr&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Performs Gaussian process regression with heteroskedastic noise following the model by Binois, M., Gramacy, R., Ludkovski, M. (2016) &lt;<a href="https://doi.org/10.48550/arXiv.1611.05902">doi:10.48550/arXiv.1611.05902</a>&gt;, with implementation details in Binois, M. &amp; Gramacy, R. B. (2021) &lt;<a href="https://doi.org/10.18637%2Fjss.v098.i13">doi:10.18637/jss.v098.i13</a>&gt;. The input dependent noise is modeled as another Gaussian process. Replicated observations are encouraged as they yield computational savings. Sequential design procedures based on the integrated mean square prediction error and lookahead heuristics are provided, and notably fast update functions when adding new observations.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/LGPL-2">LGPL-2</a> | <a href="https://www.r-project.org/Licenses/LGPL-2.1">LGPL-2.1</a> | <a href="https://www.r-project.org/Licenses/LGPL-3">LGPL-3</a> [expanded from: LGPL]</td>
</tr>
<tr>
<td>LazyData:</td>
<td>FALSE</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10),</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp (&ge; 0.12.3), MASS, methods, DiceDesign</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, monomvn, lhs, colorspace</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-10-02 11:43:12 UTC; mbinois</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-10-02 21:40:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='hetGP-package'>Package hetGP</h2><span id='topic+hetGP-package'></span>

<h3>Description</h3>

<p>Performs Gaussian process regression with heteroskedastic noise following 
Binois, M., Gramacy, R., Ludkovski, M. (2016) &lt;arXiv:1611.05902&gt;. 
The input dependent noise is modeled as another Gaussian process. 
Replicated observations are encouraged as they yield computational savings. 
Sequential design procedures based on the integrated mean square prediction error and lookahead heuristics are provided,
and notably fast update functions when adding new observations.
</p>


<h3>Details</h3>

<p>Important functions: <br />
<code><a href="#topic+mleHetGP">mleHetGP</a></code> as the main function to build a model. <br />
<code><a href="#topic+mleHomGP">mleHomGP</a></code> the equivalent for homoskedastic modeling. <br />
<code><a href="#topic+crit_IMSPE">crit_IMSPE</a></code> for adding a new design based on the Integrated Mean Square Prediction Error. <br />
<code><a href="#topic+IMSPE_optim">IMSPE_optim</a></code> for augmenting a design, possibly based on a lookahead heuristic to bias the search toward replication. <br />
<code><a href="#topic+crit_optim">crit_optim</a></code> is similar to <code>IMSPE_optim</code> but for the optimization or contour finding criterion available.
</p>


<h3>Note</h3>

<p>The authors are grateful for support from National Science Foundation grant DMS-1521702 and DMS-1521743.
</p>


<h3>Author(s)</h3>

<p>Mickael Binois, Robert B. Gramacy
</p>


<h3>References</h3>

<p>M. Binois, Robert B. Gramacy, M. Ludkovski (2018), Practical heteroskedastic Gaussian process modeling for large simulation experiments,
Journal of Computational and Graphical Statistics, 27(4), 808&ndash;821.<br /> 
Preprint available on arXiv:1611.05902. <br /> <br />
</p>
<p>M. Binois, J. Huang, R. B. Gramacy, M. Ludkovski (2019), 
Replication or exploration? Sequential design for stochastic simulation experiments,
Technometrics, 61(1), 7&ndash;23.<br /> 
Preprint available on arXiv:1710.03206. <br /> <br />
</p>
<p>M. Chung, M. Binois, R. B. Gramacy, DJ Moquin, AP Smith, AM Smith (2019). 
Parameter and Uncertainty Estimation for Dynamical Systems Using Surrogate Stochastic Processes.
SIAM Journal on Scientific Computing, 41(4), 2212&ndash;2238.<br />
Preprint available on arXiv:1802.00852.<br /> <br />
</p>
<p>M. Binois, R. B. Gramacy (2021).
hetGP: Heteroskedastic Gaussian Process Modeling and Sequential Design in R.
Journal of Statistical Software, 98(13), 1&ndash;44.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>##------------------------------------------------------------
## Example 1: Heteroskedastic GP modeling on the motorcycle data
##------------------------------------------------------------
set.seed(32)

## motorcycle data
library(MASS)
X &lt;- matrix(mcycle$times, ncol = 1)
Z &lt;- mcycle$accel
nvar &lt;- 1
plot(X, Z, ylim = c(-160, 90), ylab = 'acceleration', xlab = "time")

## Model fitting
settings &lt;- list(return.hom = TRUE) # To keep homoskedastic model used for training
model &lt;- mleHetGP(X = X, Z = Z, lower = rep(0.1, nvar), upper = rep(50, nvar),
                  covtype = "Matern5_2", settings = settings)

## A quick view of the fit                  
summary(model)

## Create a prediction grid and obtain predictions
xgrid &lt;- matrix(seq(0, 60, length.out = 301), ncol = 1) 
predictions &lt;- predict(x = xgrid, object =  model)

## Display averaged observations
points(model$X0, model$Z0, pch = 20)

## Display mean predictive surface
lines(xgrid, predictions$mean, col = 'red', lwd = 2)
## Display 95% confidence intervals
lines(xgrid, qnorm(0.05, predictions$mean, sqrt(predictions$sd2)), col = 2, lty = 2)
lines(xgrid, qnorm(0.95, predictions$mean, sqrt(predictions$sd2)), col = 2, lty = 2)
## Display 95% prediction intervals
lines(xgrid, qnorm(0.05, predictions$mean, sqrt(predictions$sd2 + predictions$nugs)), 
  col = 3, lty = 2)
lines(xgrid, qnorm(0.95, predictions$mean, sqrt(predictions$sd2 + predictions$nugs)), 
  col = 3, lty = 2)
  
## Comparison with homoskedastic fit
predictions2 &lt;- predict(x = xgrid, object = model$modHom)
lines(xgrid, predictions2$mean, col = 4, lty = 2, lwd = 2)
lines(xgrid, qnorm(0.05, predictions2$mean, sqrt(predictions2$sd2)), col = 4, lty = 3)
lines(xgrid, qnorm(0.95, predictions2$mean, sqrt(predictions2$sd2)), col = 4, lty = 3)


##------------------------------------------------------------
## Example 2: Sequential design
##------------------------------------------------------------
## Not run: 
library(DiceDesign)

## Design configuration / Parameter settings
N_tot &lt;- 500 # total number of points
n_init &lt;- 10 # number of unique designs

## HetGP options
nvar &lt;- 1 # number of variables
lower &lt;- rep(0.001, nvar)
upper &lt;- rep(1, nvar)

### Problem definition

## Mean function
forrester &lt;- function(x){
 return(((x*6-2)^2)*sin((x*6-2)*2))
}

## Noise field via standard deviation
noiseFun &lt;- function(x, coef = 1.1, scale = 1){
 if(is.null(nrow(x)))
   x &lt;- matrix(x, nrow = 1)
 return(scale*(coef + sin(x * 2 * pi)))
}

### Test function defined in [0,1]
ftest &lt;- function(x){
 if(is.null(nrow(x)))
   x &lt;- matrix(x, ncol = 1)
 return(forrester(x) + rnorm(nrow(x), mean = 0, sd = noiseFun(x)))
}

## Predictive grid
ngrid &lt;- 51
xgrid &lt;- seq(0,1, length.out = ngrid)
Xgrid &lt;- matrix(xgrid, ncol = 1)

par(mar = c(3,3,2,3)+0.1)
plot(xgrid, forrester(xgrid), type = 'l', lwd = 1, col = "blue", lty = 3,
    xlab = '', ylab = '', ylim = c(-8,16))

set.seed(42)

# Initial design
X &lt;- maximinSA_LHS(lhsDesign(n_init, nvar, seed = 42)$design)$design
Z &lt;- apply(X, 1, ftest)

points(X, Z)

model &lt;- model_init &lt;- mleHetGP(X = X, Z = Z, lower = lower, upper = upper)

for(ii in 1:(N_tot - n_init)){
 ##Precalculations
 Wijs &lt;- Wij(mu1 = model$X0, theta = model$theta, type = model$covtype)
 
 ## Adapt the horizon based on the training rmspe/score
   current_horizon &lt;- horizon(model = model, Wijs = Wijs)

 if(current_horizon == -1){
   opt &lt;- IMSPE_optim(model = model, h = 0, Wijs = Wijs)
 }else{
   opt &lt;- IMSPE_optim(model = model, h = current_horizon, Wijs = Wijs)
 }
 
 Xnew &lt;- opt$par
 Znew &lt;- apply(Xnew, 1, ftest)
 X &lt;- rbind(X, Xnew)
 Z &lt;- c(Z, Znew)
 points(Xnew, Znew)
 
 ## Update of the model
 model &lt;- update(object = model, Xnew = Xnew, Znew = Znew, lower = lower, upper = upper)
 if(ii %% 25 == 0 || ii == (N_tot - n_init)){
   model_test &lt;- mleHetGP(X = list(X0 = model$X0, Z0 = model$Z0, mult = model$mult),
    Z = model$Z, lower = lower, upper = upper, maxit = 1000)
   model &lt;- compareGP(model, model_test)
 }
}
### Plot result
preds &lt;- predict(x = Xgrid, model)
lines(Xgrid, preds$mean, col = 'red', lwd = 2)
lines(Xgrid, qnorm(0.05, preds$mean, sqrt(preds$sd2)), col = 2, lty = 2)
lines(Xgrid, qnorm(0.95, preds$mean, sqrt(preds$sd2)), col = 2, lty = 2)
lines(Xgrid, qnorm(0.05, preds$mean, sqrt(preds$sd2 + preds$nugs)), col = 3, lty = 2)
lines(Xgrid, qnorm(0.95, preds$mean, sqrt(preds$sd2 + preds$nugs)), col = 3, lty = 2)
par(new = TRUE)
plot(NA,NA, xlim = c(0, 1), ylim = c(0,max(model$mult)), axes = FALSE, ylab = "", xlab = "")
segments(x0 = model$X, x1 = model$X, y0 = rep(0, nrow(model$X)), y1 = model$mult, col = 'grey')
axis(side = 4)
mtext(side = 4, line = 2, expression(a[i]), cex = 0.8)
mtext(side = 2, line = 2, expression(f(x)), cex = 0.8)
mtext(side = 1, line = 2, 'x', cex = 0.8)

## End(Not run)
</code></pre>

<hr>
<h2 id='allocate_mult'>Allocation of replicates on existing designs</h2><span id='topic+allocate_mult'></span>

<h3>Description</h3>

<p>Allocation of replicates on existing design locations, based on (29) from (Ankenman et al, 2010)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>allocate_mult(model, N, Wijs = NULL, use.Ki = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="allocate_mult_+3A_model">model</code></td>
<td>
<p><code>hetGP</code> model</p>
</td></tr>
<tr><td><code id="allocate_mult_+3A_n">N</code></td>
<td>
<p>total budget of replication to allocate</p>
</td></tr>
<tr><td><code id="allocate_mult_+3A_wijs">Wijs</code></td>
<td>
<p>optional previously computed matrix of <code>Wijs</code>, see <code><a href="#topic+Wij">Wij</a></code></p>
</td></tr>
<tr><td><code id="allocate_mult_+3A_use.ki">use.Ki</code></td>
<td>
<p>should <code>Ki</code> from <code>model</code> be used? 
Using the inverse of C (covariance matrix only, without noise, using <code><a href="MASS.html#topic+ginv">ginv</a></code>) is also possible</p>
</td></tr>
</table>


<h3>Value</h3>

<p>vector with approximated best number of replicates per design
</p>


<h3>References</h3>

<p>B. Ankenman, B. Nelson, J. Staum (2010), Stochastic kriging for simulation metamodeling, Operations research, pp. 371&ndash;382, 58
</p>


<h3>Examples</h3>

<pre><code class='language-R'>##------------------------------------------------------------
## Example: Heteroskedastic GP modeling on the motorcycle data
##------------------------------------------------------------
set.seed(32)

## motorcycle data
library(MASS)
X &lt;- matrix(mcycle$times, ncol = 1)
Z &lt;- mcycle$accel
nvar &lt;- 1

data_m &lt;- find_reps(X, Z, rescale = TRUE)

plot(rep(data_m$X0, data_m$mult), data_m$Z, ylim = c(-160, 90),
     ylab = 'acceleration', xlab = "time")


## Model fitting
model &lt;- mleHetGP(X = list(X0 = data_m$X0, Z0 = data_m$Z0, mult = data_m$mult),
                  Z = Z, lower = rep(0.1, nvar), upper = rep(5, nvar),
                  covtype = "Matern5_2")
## Compute best allocation                  
A &lt;- allocate_mult(model, N = 1000)

## Create a prediction grid and obtain predictions
xgrid &lt;- matrix(seq(0, 1, length.out = 301), ncol = 1) 
predictions &lt;- predict(x = xgrid, object =  model)

## Display mean predictive surface
lines(xgrid, predictions$mean, col = 'red', lwd = 2)
## Display 95% confidence intervals
lines(xgrid, qnorm(0.05, predictions$mean, sqrt(predictions$sd2)), col = 2, lty = 2)
lines(xgrid, qnorm(0.95, predictions$mean, sqrt(predictions$sd2)), col = 2, lty = 2)
## Display 95% prediction intervals
lines(xgrid, qnorm(0.05, predictions$mean, sqrt(predictions$sd2 + predictions$nugs)), 
col = 3, lty = 2)
lines(xgrid, qnorm(0.95, predictions$mean, sqrt(predictions$sd2 + predictions$nugs)), 
col = 3, lty = 2)

par(new = TRUE)
plot(NA,NA, xlim = c(0,1), ylim = c(0,max(A)), axes = FALSE, ylab = "", xlab = "")
segments(x0 = model$X0, x1 = model$X0, 
y0 = rep(0, nrow(model$X)), y1 = A, col = 'grey')
axis(side = 4)
mtext(side = 4, line = 2, expression(a[i]), cex = 0.8)       
</code></pre>

<hr>
<h2 id='ato'> Assemble To Order (ATO) Data and Fits </h2><span id='topic+ato'></span><span id='topic+X'></span><span id='topic+Z'></span><span id='topic+Xa'></span><span id='topic+Xtrain'></span><span id='topic+Xtrain.out'></span><span id='topic+Ztrain'></span><span id='topic+Ztrain.out'></span><span id='topic+train'></span><span id='topic+mult'></span><span id='topic+reps'></span><span id='topic+kill'></span><span id='topic+Zm'></span><span id='topic+Zv'></span><span id='topic+out'></span><span id='topic+Xtest'></span><span id='topic+Ztest'></span><span id='topic+nc'></span><span id='topic+ato.a'></span><span id='topic+Xa'></span><span id='topic+Za'></span><span id='topic+out.a'></span>

<h3>Description</h3>

<p>A batch design-evaluated ATO data set, random partition into training and
testing, and fitted <span class="pkg">hetGP</span> model; similarly a sequentially designed
adaptive horizon data set, and associated fitted <span class="pkg">hetGP</span> model 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(ato)</code></pre>


<h3>Format</h3>

<p>Calling <code>data(ato)</code> causes the following objects to be loaded into the namespace.
</p>

<dl>
<dt><code>X</code></dt><dd><p> 2000x8 <code>matrix</code> of inputs coded from 1,...,20 to the unit 8-cube; original inputs can be recreated as <code>X*19 + 1</code> </p>
</dd>
<dt><code>Z</code></dt><dd><p> 2000x10 <code>matrix</code> of normalized outputs obtained in ten replicates at each of the 2000 inputs <code>X</code>.  Original outputs can be obtained as <code>Z*sqrt(Zv) + Zm</code> </p>
</dd>
<dt><code>Zm</code></dt><dd><p> scalar mean used to normalize <code>Z</code> </p>
</dd>
<dt><code>Zv</code></dt><dd><p> scalar variance used to normalize <code>Z</code> </p>
</dd>
<dt><code>train</code></dt><dd><p> vector of 1000 rows of <code>X</code> and <code>Z</code> selected for training </p>
</dd>
<dt><code>Xtrain</code></dt><dd><p> 1000x8 <code>matrix</code> obtained as a random partition of <code>X</code> </p>
</dd>
<dt><code>Ztrain</code></dt><dd><p> length 1000 list of vectors containing the selected (replicated) observations at each row of <code>Xtrain</code> </p>
</dd>
<dt><code>mult</code></dt><dd><p> the length of each entry of <code>Ztrain</code>; same as <code>unlist(lapply(Ztrain, length))</code> </p>
</dd>
<dt><code>kill</code></dt><dd><p> a <code>logical</code> vector indicating which rows of <code>Xtrain</code> for which all replicates of <code>Z</code> are selected for <code>Ztrain</code>;  same as <code>mult == 10</code></p>
</dd>
<dt><code>Xtrain.out</code></dt><dd><p> 897x8 <code>matrix</code> comprised of the subset of <code>X</code> where not all replicates are selected for training; i.e., those for which <code>kill == FALSE</code> </p>
</dd>
<dt><code>Ztrain.out</code></dt><dd> <p><code>list</code> of length 897 containing the replicates of <code>Z</code> not selected for <code>Ztrain</code> </p>
</dd>
<dt><code>nc</code></dt><dd> <p><code>noiseControl</code> argument for <code>mleHetGP</code> call </p>
</dd>
<dt><code>out</code></dt><dd> <p><code>mleHetGP</code> model based on <code>Xtrain</code> and <code>Ztrain</code> using <code>noiseControl=nc</code> </p>
</dd>
<dt><code>Xtest</code></dt><dd><p> 1000x8 <code>matrix</code> containing the other partition of <code>X</code> of locations not selected for training </p>
</dd>
<dt><code>Ztest</code></dt><dd><p> 1000x10 <code>matrix</code> of responses from the partition of <code>Z</code> not selected for training </p>
</dd>
<dt><code>ato.a</code></dt><dd><p> 2000x9 <code>matrix</code> of sequentially designed inputs (8) and outputs (1) obtained under an adaptive horizon scheme </p>
</dd>
<dt><code>Xa</code></dt><dd><p> 2000x8 matrix of coded inputs from <code>ato.a</code> as <code>(ato.a[,1:8]-1)/19</code> </p>
</dd>
<dt><code>Za</code></dt><dd><p> length 2000 vector of outputs from <code>ato.a</code> as <code>(ato.a[,9] - Zm)/sqrt(Zv)</code> </p>
</dd>
<dt><code>out.a</code></dt><dd> <p><code>mleHetGP</code> model based on <code>Xa</code> and <code>Za</code> using <code>noiseControl=nc</code> </p>
</dd>
</dl>



<h3>Details</h3>

<p>The assemble to order (ATO) simulator  (Hong, Nelson, 2006) is a queuing
simulation targeting inventory management scenarios.  The setup is as follows.
A company manufactures <code class="reqn">m</code> products.  Products are built from base parts
called items, some of which are &ldquo;key&rdquo; in that the product cannot be built
without them.  If a random request comes in for a product that is missing a
key item, a replenishment order is executed, and is filled after a random
period.  Holding items in inventory is expensive, so there is a balance
between inventory costs and revenue. Hong &amp; Nelson built a
<code>Matlab</code> simulator for this setup, which was subsequently
reimplemented by Xie, et al., (2012).  
</p>
<p>Binois, et al (2018a) describe an out-of-sample experiment based on this
latter implementation in its default (Hong &amp; Nelson) setting, specifying
item cost structure, product makeup (their items) and revenue, distribution
of demand and replenishment time, under target stock vector inputs <code class="reqn">b \in
  \{1,\dots,20\}^8</code> for eight items.  They worked with 2000
random uniform input locations (<code>X</code>), and ten replicate responses at
each location (<code>Z</code>). The partition of 1000 training data points
(<code>Xtrain</code> and
<code>Ztrain</code>) and 1000 testing (<code>Xtest</code> and <code>Ztest</code>) sets
provided here is an example of one that was used for the Monte Carlo
experiment in that paper.  The elements <code>Xtrain.out</code> and
<code>Ztrain.out</code> comprise of replicates from the training inputs which were
not used in training, so may be used for out-of-sample testing.  For more
details on how the partitions were build, see the code in the examples
section below.
</p>
<p>Binois, et al (2018b) describe an adaptive lookahead horizon scheme for
building a sequential design (<code>Xa</code>, <code>Za</code>) of size 2000 whose
predictive performance, via proper scores, is almost as good as the
approximately 5000 training data sites in each of the Monte Carlo
repetitions described above.  The example code below demonstrates this via
out-of-sample predictions on <code>Xtest</code> (measured against <code>Ztest</code>)
when <code>Xtrain</code> and <code>Ztrain</code> are used compared to those from
<code>Xa</code> and <code>Za</code>.
</p>


<h3>Note</h3>

<p> The <code>mleHetGP</code> output objects were build with
<code>return.matrices=FALSE</code> for more compact storage.  Before these objects
can be used for calculations, e.g., prediction or design, these covariance
matrices need to be rebuilt with <code><a href="#topic+rebuild">rebuild</a></code>.  The generic
<code>predict</code> method will call <code><a href="#topic+rebuild">rebuild</a></code> automatically, 
however, some of the other methods will not, and it is often more
efficient to call <code><a href="#topic+rebuild">rebuild</a></code> once at the outset, rather
than for every subsequent <code>predict</code> call </p>


<h3>Author(s)</h3>

 
<p>Mickael Binois, <a href="mailto:mbinois@mcs.anl.gov">mbinois@mcs.anl.gov</a>, and
Robert B. Gramacy, <a href="mailto:rbg@vt.edu">rbg@vt.edu</a>
</p>


<h3>References</h3>

<p>Hong L., Nelson B. (2006), Discrete optimization via simulation using COMPASS. Operations Research, 54(1), 115-129.
</p>
<p>Xie J., Frazier P., Chick S. (2012). Assemble to Order Simulator. <a href="http://simopt.org/wiki/index.php?title=Assemble_to_Order&amp;oldid=447">http://simopt.org/wiki/index.php?title=Assemble_to_Order&amp;oldid=447</a>.
</p>
<p>M. Binois, J. Huang, R. Gramacy, M. Ludkovski (2018a), Replication or exploration? Sequential design for stochastic simulation experiments,
arXiv preprint arXiv:1710.03206.
</p>
<p>M. Binois, Robert B. Gramacy, M. Ludkovski (2018b), Practical heteroskedastic Gaussian process modeling for large simulation experiments,
arXiv preprint arXiv:1611.05902.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+bfs">bfs</a></code>, <code><a href="#topic+sirEval">sirEval</a></code>, <code>link{rebuild}</code>, 
<code><a href="#topic+horizon">horizon</a></code>, <code><a href="#topic+IMSPE_optim">IMSPE_optim</a></code>, <code><a href="#topic+mleHetGP">mleHetGP</a></code>, 
<code>vignette("hetGP")</code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(ato)

## Not run: 
##
## the code below was used to create the random partition 
##

## recover the data in its original form
X &lt;- X*19+1
Z &lt;- Z*sqrt(Zv) + Zm

## code the inputs and outputs; i.e., undo the transformation 
## above
X &lt;- (X-1)/19
Zm &lt;- mean(Z)
Zv &lt;- var(as.vector(Z))
Z &lt;- (Z - Zm)/sqrt(Zv)

## random training and testing partition
train &lt;- sample(1:nrow(X), 1000)
Xtrain &lt;- X[train,]
Xtest &lt;- X[-train,]
Ztest &lt;- as.list(as.data.frame(t(Z[-train,])))
Ztrain &lt;- Ztrain.out &lt;- list()
mult &lt;- rep(NA, nrow(Xtrain))
kill &lt;- rep(FALSE, nrow(Xtrain))
for(i in 1:length(train)) {
  reps &lt;- sample(1:ncol(Z), 1)
  w &lt;- sample(1:ncol(Z), reps)
  Ztrain[[i]] &lt;- Z[train[i],w]
  if(reps &lt; 10) Ztrain.out[[i]] &lt;- Z[train[i],-w]
  else kill[i] &lt;- TRUE
  mult[i] &lt;- reps
}

## calculate training locations and outputs for replicates not
## included in Ztrain
Xtrain.out &lt;- Xtrain[!kill,]
Ztrain.out &lt;- Ztrain[which(!kill)]

## fit hetGP model
out &lt;- mleHetGP(X=list(X0=Xtrain, Z0=sapply(Ztrain, mean), mult=mult),
  Z=unlist(Ztrain), lower=rep(0.01, ncol(X)), upper=rep(30, ncol(X)),
  covtype="Matern5_2", noiseControl=nc, known=list(beta0=0), 
  maxit=100000, settings=list(return.matrices=FALSE))

##
## the adaptive lookahead design is read in and fit as 
## follows
##
Xa &lt;- (ato.a[,1:8]-1)/19
Za &lt;- ato.a[,9]
Za &lt;- (Za - Zm)/sqrt(Zv)

## uses nc defined above
out.a &lt;- mleHetGP(Xa, Za, lower=rep(0.01, ncol(X)), 
  upper=rep(30, ncol(X)), covtype="Matern5_2", known=list(beta0=0), 
  noiseControl=nc, maxit=100000, settings=list(return.matrices=FALSE))

## End(Not run)

##
## the following code duplicates a predictive comparison in
## the package vignette
##

## first using the model fit to the train partition (out)
out &lt;- rebuild(out)

## predicting out-of-sample at the test sights
phet &lt;- predict(out, Xtest)
phets2 &lt;- phet$sd2 + phet$nugs
mhet &lt;- as.numeric(t(matrix(rep(phet$mean, 10), ncol=10)))
s2het &lt;- as.numeric(t(matrix(rep(phets2, 10), ncol=10)))
sehet &lt;- (unlist(t(Ztest)) - mhet)^2
sc &lt;- - sehet/s2het - log(s2het)
mean(sc)

## predicting at the held-out training replicates
phet.out &lt;- predict(out, Xtrain.out)
phets2.out &lt;- phet.out$sd2 + phet.out$nugs
s2het.out &lt;- mhet.out &lt;- Ztrain.out
for(i in 1:length(mhet.out)) {
  mhet.out[[i]] &lt;- rep(phet.out$mean[i], length(mhet.out[[i]]))
  s2het.out[[i]] &lt;- rep(phets2.out[i], length(s2het.out[[i]]))
}
mhet.out &lt;- unlist(t(mhet.out))
s2het.out &lt;- unlist(t(s2het.out))
sehet.out &lt;- (unlist(t(Ztrain.out)) - mhet.out)^2
sc.out &lt;- - sehet.out/s2het.out - log(s2het.out)
mean(sc.out)

## Not run: 
## then using the model trained from the "adaptive" 
## sequential design, with comparison from the "batch" 
## one above, using the scores function
out.a &lt;- rebuild(out.a)
sc.a &lt;- scores(out.a, Xtest = Xtest, Ztest = Ztest)
c(batch=mean(sc), adaptive=sc.a)

## an example of one iteration of sequential design

  Wijs &lt;- Wij(out.a$X0, theta=out.a$theta, type=out.a$covtype)
  h &lt;- horizon(out.a, Wijs=Wijs)
  control = list(tol_dist=1e-4, tol_diff=1e-4, multi.start=30, maxit=100)
  opt &lt;- IMSPE_optim(out.a, h, Wijs=Wijs, control=control)
  opt$par

## End(Not run)
</code></pre>

<hr>
<h2 id='bfs'> Bayes Factor Data </h2><span id='topic+bfs'></span><span id='topic+bfs.exp'></span><span id='topic+bfs.gamma'></span>

<h3>Description</h3>

<p>Data from a Bayes factor MCMC-based simulation experiment comparing Student-t to Gaussian errors in an RJ-based Laplace prior Bayesian linear regession setting
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(ato)</code></pre>


<h3>Format</h3>

<p>Calling <code>data(bfs)</code> causes the following objects to be loaded into the namespace.
</p>

<dl>
<dt><code>bfs.exp</code></dt><dd><p> 20x11 <code>data.frame</code> whose first column is <code class="reqn">\theta</code>, indicating the mean parameter of an exponential distribution encoding the prior of the Student-t degrees of freedom parameter <code class="reqn">\nu</code>.  The remaining ten
columns comprise of Bayes factor evaluations under that setting  </p>
</dd>
<dt><code>bfs.gamma</code></dt><dd><p> 80x7 <code>data.frame</code> whose first two columns are <code class="reqn">\beta</code> and <code class="reqn">\alpha</code>, indicating the second and first parameters to a 
Gamma distribution encoding the prior of the Student-t degrees of freedom parameters <code class="reqn">\nu</code>.  The remaining five columns comprise of Bayes factor evaluations under those settings </p>
</dd>
</dl>



<h3>Details</h3>

<p>Gramacy &amp; Pantaleo (2010), Sections 3-3-3.4, describe an experiment
involving Bayes factor (BF)  calculations to determine if data are
leptokurtic (Student-t errors) or not (simply Gaussian) as a function of the
prior parameterization on the Student-t degrees of freedom parameter
<code class="reqn">\nu</code>. Franck &amp; Gramacy (2018) created a grid of hyperparameter
values in <code class="reqn">\theta</code> describing the mean of an Exponential
distribution, evenly spaced in <code class="reqn">\log_{10}</code> space from
<code>10^(-3)</code> to <code>10^6</code> spanning &ldquo;solidly Student-t&rdquo; (even
Cauchy) to &ldquo;essentially Gaussian&rdquo; in terms of the mean of the prior
over <code class="reqn">\nu</code>.  For each <code class="reqn">\theta</code> setting on the grid they
ran the Reversible Jump (RJ) MCMC to approximate the BF of Student-t over Gaussian 
by feeding in sample likelihood evaluations provided by <span class="pkg">monomvn</span>'s
<code><a href="monomvn.html#topic+blasso">blasso</a></code> to compute the BF. In order to understand the
Monte Carlo variability in those calculations, ten replicates of the BFs
under each hyperparameter setting were collected.  These data are provided
in <code>bfs.exp</code>.
</p>
<p>A similar, larger experiment was provided with <code class="reqn">\nu</code> under a Gamma
prior with parameters <code class="reqn">\alpha</code> and <code class="reqn">\beta \equiv \theta</code>.  In this higher dimensional space, a Latin hypercube sample 
of size eighty was created, and five replicates of BFs were recorded.  
These data are provided in <code>bfs.gamma</code>.
</p>
<p>The examples below involve <code><a href="#topic+mleHetTP">mleHetTP</a></code> fits (Chung, et al., 2018)
to these data and a visualization of the predictive surfaces thus obtained.
The code here follows an example provided, with more detail, in
<code>vignette("hetGP")</code> 
</p>


<h3>Note</h3>

<p> For code showing how these BFs were calculated, see supplementary material
from Franck &amp; Gramacy (2018) </p>


<h3>Author(s)</h3>

 
<p>Mickael Binois, <a href="mailto:mbinois@mcs.anl.gov">mbinois@mcs.anl.gov</a>, and
Robert B. Gramacy, <a href="mailto:rbg@vt.edu">rbg@vt.edu</a>
</p>


<h3>References</h3>

 
<p>Franck CT, Gramacy RB (2018). Assessing Bayes factor surfaces using
interactive visualization and computer surrogate modeling. Preprint
available on arXiv:1809.05580.
</p>
<p>Gramacy RB (2017). <span class="pkg">monomvn</span>: Estimation for Multivariate Normal
and Student-t Data with Monotone Missingness. R package version 1.9-7,
<a href="https://CRAN.R-project.org/package=monomvn">https://CRAN.R-project.org/package=monomvn</a>. 
</p>
<p>R.B. Gramacy and E. Pantaleo (2010). Shrinkage regression for multivariate
inference with missing data, and an application to portfolio balancing.
Bayesian Analysis 5(2), 237-262. Preprint available on arXiv:0907.2135
</p>
<p>Chung M, Binois M, Gramacy RB, Moquin DJ, Smith AP, Smith AM (2018).
Parameter and Uncertainty Estimation for Dynamical Systems Using Surrogate
Stochastic Processes. SIAM Journal on Scientific Computing, 41(4), 2212-2238.
Preprint available on arXiv:1802.00852. 
</p>


<h3>See Also</h3>

 
<p><code><a href="#topic+ato">ato</a></code>, <code><a href="#topic+sirEval">sirEval</a></code>, <code><a href="#topic+mleHetTP">mleHetTP</a></code>, 
<code>vignette("hetGP")</code>, <code><a href="monomvn.html#topic+blasso">blasso</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(bfs)

##
## Exponential version first
##

thetas &lt;- matrix(bfs.exp$theta, ncol=1)
bfs &lt;- as.matrix(t(bfs.exp[,-1]))

## the data are heavy tailed, so t-errors help
bfs1 &lt;- mleHetTP(X=list(X0=log10(thetas), Z0=colMeans(log(bfs)),
  mult=rep(nrow(bfs), ncol(bfs))), Z=log(as.numeric(bfs)), lower=10^(-4), 
  upper=5, covtype="Matern5_2")

## predictions on a grid in 1d
dx &lt;- seq(0,1,length=100)
dx &lt;- 10^(dx*4 - 3)
p &lt;- predict(bfs1, matrix(log10(dx), ncol=1))

## visualization
matplot(log10(thetas), t(log(bfs)), col=1, pch=21, ylab="log(bf)", 
  main="Bayes factor surface")
lines(log10(dx), p$mean, lwd=2, col=2)
lines(log10(dx), p$mean + 2*sqrt(p$sd2 + p$nugs), col=2, lty=2, lwd=2)
lines(log10(dx), p$mean - 2*sqrt(p$sd2 + p$nugs), col=2, lty=2, lwd=2)
legend("topleft", c("hetTP mean", "hetTP interval"), lwd=2, lty=1:2, col=2)

##
## Now Gamma version
##

D &lt;- as.matrix(bfs.gamma[,1:2])
bfs &lt;- as.matrix(t(bfs.gamma[,-(1:2)]))

## fitting in 2fd
bfs2 &lt;- mleHetTP(X=list(X0=log10(D), Z0=colMeans(log(bfs)), 
  mult=rep(nrow(bfs), ncol(bfs))), Z = log(as.numeric(bfs)), 
  lower = rep(10^(-4), 2), upper = rep(5, 2), covtype = "Matern5_2", 
  maxit=100000)

## predictions on a grid in 2d
dx &lt;- seq(0,1,length=100)
dx &lt;- 10^(dx*4 - 3)
DD &lt;- as.matrix(expand.grid(dx, dx))
p &lt;- predict(bfs2, log10(DD))

## visualization via image-contour plots
par(mfrow=c(1,2))
mbfs &lt;- colMeans(bfs)
image(log10(dx), log10(dx), t(matrix(p$mean, ncol=length(dx))),  
  col=heat.colors(128), xlab="log10 alpha", ylab="log10 beta", 
  main="mean log BF")
text(log10(D[,2]), log10(D[,1]), signif(log(mbfs), 2), cex=0.5)
contour(log10(dx), log10(dx),t(matrix(p$mean, ncol=length(dx))),
  levels=c(-5,-3,-1,0,1,3,5), add=TRUE, col=4)
image(log10(dx), log10(dx), t(matrix(sqrt(p$sd2 + p$nugs), 
  ncol=length(dx))),  col=heat.colors(128), xlab="log10 alpha", 
  ylab="log10 beta", main="sd log BF")
text(log10(D[,2]), log10(D[,1]), signif(apply(log(bfs), 2, sd), 2), 
  cex=0.5)
</code></pre>

<hr>
<h2 id='compareGP'>Likelihood-based comparison of models</h2><span id='topic+compareGP'></span>

<h3>Description</h3>

<p>Compare two models based on the log-likelihood for <code>hetGP</code> and <code>homGP</code> models
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compareGP(model1, model2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="compareGP_+3A_model1">model1</code>, <code id="compareGP_+3A_model2">model2</code></td>
<td>
<p><code>hetGP</code> or <code>homGP</code> models</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Best model based on the likelihood, first one in case of a tie
</p>


<h3>Note</h3>

<p>If comparing homoskedastic and heteroskedastic models, the un-penalised likelihood is used for the later, see e.g., (Binois et al. 2017+).
</p>


<h3>References</h3>

<p>M. Binois, Robert B. Gramacy, M. Ludkovski (2018), Practical heteroskedastic Gaussian process modeling for large simulation experiments,
Journal of Computational and Graphical Statistics, 27(4), 808&ndash;821.<br /> 
Preprint available on arXiv:1611.05902.
</p>

<hr>
<h2 id='cov_gen'>Correlation function of selected type, supporting both isotropic and product forms</h2><span id='topic+cov_gen'></span>

<h3>Description</h3>

<p>Correlation function of selected type, supporting both isotropic and product forms
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cov_gen(X1, X2 = NULL, theta, type = c("Gaussian", "Matern5_2", "Matern3_2"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cov_gen_+3A_x1">X1</code></td>
<td>
<p>matrix of design locations, one point per row</p>
</td></tr>
<tr><td><code id="cov_gen_+3A_x2">X2</code></td>
<td>
<p>matrix of design locations if correlation is calculated between <code>X1</code> and <code>X2</code> (otherwise calculated between <code>X1</code> and itself)</p>
</td></tr>
<tr><td><code id="cov_gen_+3A_theta">theta</code></td>
<td>
<p>vector of lengthscale parameters (either of size one if isotropic or of size d if anisotropic)</p>
</td></tr>
<tr><td><code id="cov_gen_+3A_type">type</code></td>
<td>
<p>one of &quot;<code>Gaussian</code>&quot;, &quot;<code>Matern5_2</code>&quot;, &quot;<code>Matern3_2</code>&quot;</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Definition of univariate correlation function and hyperparameters:
</p>

<ul>
<li><p> &quot;<code>Gaussian</code>&quot;: <code class="reqn">c(x, y) = exp(-(x-y)^2/theta)</code>
</p>
</li>
<li><p> &quot;<code>Matern5_2</code>&quot;: <code class="reqn">c(x, y) = (1+sqrt(5)/theta * abs(x-y) + 5/(3*theta^2)(x-y)^2) * exp(-sqrt(5)*abs(x-y)/theta)</code>
</p>
</li>
<li><p> &quot;<code>Matern3_2</code>&quot;: <code class="reqn">c(x, y) = (1+sqrt(3)/theta * abs(x-y)) * exp(-sqrt(3)*abs(x-y)/theta)</code>
</p>
</li></ul>

<p>Multivariate correlations are product of univariate ones.
</p>

<hr>
<h2 id='crit_cSUR'>Contour Stepwise Uncertainty Reduction criterion</h2><span id='topic+crit_cSUR'></span>

<h3>Description</h3>

<p>Computes cSUR infill criterion
</p>


<h3>Usage</h3>

<pre><code class='language-R'>crit_cSUR(x, model, thres = 0, preds = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="crit_cSUR_+3A_x">x</code></td>
<td>
<p>matrix of new designs, one point per row (size n x d)</p>
</td></tr>
<tr><td><code id="crit_cSUR_+3A_model">model</code></td>
<td>
<p><code>homGP</code> or <code>hetGP</code> model, including inverse matrices</p>
</td></tr>
<tr><td><code id="crit_cSUR_+3A_thres">thres</code></td>
<td>
<p>for contour finding</p>
</td></tr>
<tr><td><code id="crit_cSUR_+3A_preds">preds</code></td>
<td>
<p>optional predictions at <code>x</code> to avoid recomputing if already done (must contain <code>cov</code>)</p>
</td></tr>
</table>


<h3>References</h3>

<p>Lyu, X., Binois, M. &amp; Ludkovski, M. (2018+). Evaluating Gaussian Process Metamodels and Sequential Designs for Noisy Level Set Estimation. arXiv:1807.06712. <br />
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Infill criterion example
set.seed(42)
branin &lt;- function(x){
  m &lt;- 54.8104; s &lt;- 51.9496
  if(is.null(dim(x))) x &lt;- matrix(x, nrow = 1)
  xx &lt;- 15 * x[,1] - 5; y &lt;- 15 * x[,2]
  f &lt;- (y - 5.1 * xx^2/(4 * pi^2) + 5 * xx/pi - 6)^2 + 10 * (1 - 1/(8 * pi)) * cos(xx) + 10
  f &lt;- (f - m)/s
  return(f)
}

ftest &lt;- function(x, sd = 0.1){
  if(is.null(dim(x))) x &lt;- matrix(x, nrow = 1)
  return(apply(x, 1, branin) + rnorm(nrow(x), sd = sd))
}

ngrid &lt;- 101; xgrid &lt;- seq(0, 1, length.out = ngrid)
Xgrid &lt;- as.matrix(expand.grid(xgrid, xgrid))
Zgrid &lt;- ftest(Xgrid)

n &lt;- 20
N &lt;- 500
X &lt;- Xgrid[sample(1:nrow(Xgrid), n),]
X &lt;- X[sample(1:n, N, replace = TRUE),]
Z &lt;- ftest(X)
model &lt;- mleHetGP(X, Z, lower = rep(0.001,2), upper = rep(1,2))

critgrid &lt;- apply(Xgrid, 1, crit_cSUR, model = model)

filled.contour(matrix(critgrid, ngrid), color.palette = terrain.colors, main = "cSUR criterion")

</code></pre>

<hr>
<h2 id='crit_EI'>Expected Improvement criterion</h2><span id='topic+crit_EI'></span>

<h3>Description</h3>

<p>Computes EI for minimization
</p>


<h3>Usage</h3>

<pre><code class='language-R'>crit_EI(x, model, cst = NULL, preds = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="crit_EI_+3A_x">x</code></td>
<td>
<p>matrix of new designs, one point per row (size n x d)</p>
</td></tr>
<tr><td><code id="crit_EI_+3A_model">model</code></td>
<td>
<p><code>homGP</code> or <code>hetGP</code> model, or their TP equivalents, including inverse matrices</p>
</td></tr>
<tr><td><code id="crit_EI_+3A_cst">cst</code></td>
<td>
<p>optional plugin value used in the EI, see details</p>
</td></tr>
<tr><td><code id="crit_EI_+3A_preds">preds</code></td>
<td>
<p>optional predictions at <code>x</code> to avoid recomputing if already done</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>cst</code> is classically the observed minimum in the deterministic case. 
In the noisy case, the min of the predictive mean works fine.
</p>


<h3>Note</h3>

<p>This is a beta version at this point.
</p>


<h3>References</h3>

<p>Mockus, J.; Tiesis, V. &amp; Zilinskas, A. (1978).
The application of Bayesian methods for seeking the extremum Towards Global Optimization, Amsterdam: Elsevier, 2, 2.<br /> <br />
</p>
<p>Vazquez E, Villemonteix J, Sidorkiewicz M, Walter E (2008). 
Global Optimization Based on Noisy Evaluations: An Empirical Study of Two Statistical Approaches, 
Journal of Physics: Conference Series, 135, IOP Publishing.<br /> <br />
</p>
<p>A. Shah, A. Wilson, Z. Ghahramani (2014), Student-t processes as alternatives to Gaussian processes, Artificial Intelligence and Statistics, 877&ndash;885.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Optimization example
set.seed(42)


## Noise field via standard deviation
noiseFun &lt;- function(x, coef = 1.1, scale = 1){
if(is.null(nrow(x)))
 x &lt;- matrix(x, nrow = 1)
   return(scale*(coef + cos(x * 2 * pi)))
}

## Test function defined in [0,1]
ftest &lt;- function(x){
if(is.null(nrow(x)))
x &lt;- matrix(x, ncol = 1)
return(f1d(x) + rnorm(nrow(x), mean = 0, sd = noiseFun(x)))
}

n_init &lt;- 10 # number of unique designs
N_init &lt;- 100 # total number of points
X &lt;- seq(0, 1, length.out = n_init)
X &lt;- matrix(X[sample(1:n_init, N_init, replace = TRUE)], ncol = 1)
Z &lt;- ftest(X)

## Predictive grid
ngrid &lt;- 51
xgrid &lt;- seq(0,1, length.out = ngrid)
Xgrid &lt;- matrix(xgrid, ncol = 1)

model &lt;- mleHetGP(X = X, Z = Z, lower = 0.001, upper = 1)

EIgrid &lt;- crit_EI(Xgrid, model)
preds &lt;- predict(x = Xgrid, model)

par(mar = c(3,3,2,3)+0.1)
plot(xgrid, f1d(xgrid), type = 'l', lwd = 1, col = "blue", lty = 3,
xlab = '', ylab = '', ylim = c(-8,16))
points(X, Z)
lines(Xgrid, preds$mean, col = 'red', lwd = 2)
lines(Xgrid, qnorm(0.05, preds$mean, sqrt(preds$sd2)), col = 2, lty = 2)
lines(Xgrid, qnorm(0.95, preds$mean, sqrt(preds$sd2)), col = 2, lty = 2)
lines(Xgrid, qnorm(0.05, preds$mean, sqrt(preds$sd2 + preds$nugs)), col = 3, lty = 2)
lines(Xgrid, qnorm(0.95, preds$mean, sqrt(preds$sd2 + preds$nugs)), col = 3, lty = 2)
par(new = TRUE)
plot(NA, NA, xlim = c(0, 1), ylim = c(0,max(EIgrid)), axes = FALSE, ylab = "", xlab = "")
lines(xgrid, EIgrid, lwd = 2, col = 'cyan')
axis(side = 4)
mtext(side = 4, line = 2, expression(EI(x)), cex = 0.8)
mtext(side = 2, line = 2, expression(f(x)), cex = 0.8)
</code></pre>

<hr>
<h2 id='crit_ICU'>Integrated Contour Uncertainty criterion</h2><span id='topic+crit_ICU'></span>

<h3>Description</h3>

<p>Computes ICU infill criterion
</p>


<h3>Usage</h3>

<pre><code class='language-R'>crit_ICU(x, model, thres = 0, Xref, w = NULL, preds = NULL, kxprime = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="crit_ICU_+3A_x">x</code></td>
<td>
<p>matrix of new designs, one point per row (size n x d)</p>
</td></tr>
<tr><td><code id="crit_ICU_+3A_model">model</code></td>
<td>
<p><code>homGP</code> or <code>hetGP</code> model, including inverse matrices</p>
</td></tr>
<tr><td><code id="crit_ICU_+3A_thres">thres</code></td>
<td>
<p>for contour finding</p>
</td></tr>
<tr><td><code id="crit_ICU_+3A_xref">Xref</code></td>
<td>
<p>matrix of input locations to approximate the integral by a sum</p>
</td></tr>
<tr><td><code id="crit_ICU_+3A_w">w</code></td>
<td>
<p>optional weights vector of weights for <code>Xref</code> locations</p>
</td></tr>
<tr><td><code id="crit_ICU_+3A_preds">preds</code></td>
<td>
<p>optional predictions at <code>Xref</code> to avoid recomputing if already done</p>
</td></tr>
<tr><td><code id="crit_ICU_+3A_kxprime">kxprime</code></td>
<td>
<p>optional covariance matrix between <code>model$X0</code> and <code>Xref</code> to avoid its recomputation</p>
</td></tr>
</table>


<h3>References</h3>

<p>Lyu, X., Binois, M. &amp; Ludkovski, M. (2018+). Evaluating Gaussian Process Metamodels and Sequential Designs for Noisy Level Set Estimation. arXiv:1807.06712. <br />
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Infill criterion example
set.seed(42)
branin &lt;- function(x){
  m &lt;- 54.8104; s &lt;- 51.9496
  if(is.null(dim(x))) x &lt;- matrix(x, nrow = 1)
  xx &lt;- 15 * x[,1] - 5; y &lt;- 15 * x[,2]
  f &lt;- (y - 5.1 * xx^2/(4 * pi^2) + 5 * xx/pi - 6)^2 + 10 * (1 - 1/(8 * pi)) * cos(xx) + 10
  f &lt;- (f - m)/s
  return(f)
}

ftest &lt;- function(x, sd = 0.1){
  if(is.null(dim(x))) x &lt;- matrix(x, nrow = 1)
  return(apply(x, 1, branin) + rnorm(nrow(x), sd = sd))
}

ngrid &lt;- 51; xgrid &lt;- seq(0, 1, length.out = ngrid)
Xgrid &lt;- as.matrix(expand.grid(xgrid, xgrid))
Zgrid &lt;- ftest(Xgrid)

n &lt;- 20
N &lt;- 500
X &lt;- Xgrid[sample(1:nrow(Xgrid), n),]
X &lt;- X[sample(1:n, N, replace = TRUE),]
Z &lt;- ftest(X)
model &lt;- mleHetGP(X, Z, lower = rep(0.001,2), upper = rep(1,2))

# Precalculations for speedup
preds &lt;- predict(model, x = Xgrid)
kxprime &lt;- cov_gen(X1 = model$X0, X2 = Xgrid, theta = model$theta, type = model$covtype)
 
critgrid &lt;- apply(Xgrid, 1, crit_ICU, model = model, Xref = Xgrid,
                  preds = preds, kxprime = kxprime)

filled.contour(matrix(critgrid, ngrid), color.palette = terrain.colors, main = "ICU criterion")

</code></pre>

<hr>
<h2 id='crit_IMSPE'>Sequential IMSPE criterion</h2><span id='topic+crit_IMSPE'></span>

<h3>Description</h3>

<p>Compute the integrated mean square prediction error after adding a new design
</p>


<h3>Usage</h3>

<pre><code class='language-R'>crit_IMSPE(x, model, id = NULL, Wijs = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="crit_IMSPE_+3A_x">x</code></td>
<td>
<p>matrix for the new design (size 1 x d)</p>
</td></tr>
<tr><td><code id="crit_IMSPE_+3A_model">model</code></td>
<td>
<p><code>homGP</code> or <code>hetGP</code> model, including inverse matrices</p>
</td></tr>
<tr><td><code id="crit_IMSPE_+3A_id">id</code></td>
<td>
<p>instead of providing <code>x</code>, one can provide the index of a considered existing design</p>
</td></tr>
<tr><td><code id="crit_IMSPE_+3A_wijs">Wijs</code></td>
<td>
<p>optional previously computed matrix of Wijs, to avoid recomputing it; see <code><a href="#topic+Wij">Wij</a></code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>The computations are scale free, i.e., values are not multiplied by <code>nu_hat</code> from <code>homGP</code> or <code>hetGP</code>.
Currently this function ignores the extra terms related to the estimation of the mean.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+deriv_crit_IMSPE">deriv_crit_IMSPE</a></code> for the derivative
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## One-d toy example

set.seed(42)
ftest &lt;- function(x, coef = 0.1) return(sin(2*pi*x) + rnorm(1, sd = coef))

n &lt;- 9
designs &lt;- matrix(seq(0.1, 0.9, length.out = n), ncol = 1)
X &lt;- matrix(designs[rep(1:n, sample(1:10, n, replace = TRUE)),])
Z &lt;- apply(X, 1, ftest)

prdata &lt;- find_reps(X, Z, inputBounds = matrix(c(0,1), nrow = 2, ncol = 1))
Z &lt;- prdata$Z
plot(prdata$X0[rep(1:n, times = prdata$mult),], prdata$Z, xlab = "x", ylab = "Y")

model &lt;- mleHetGP(X = list(X0 = prdata$X0, Z0 = prdata$Z0, mult = prdata$mult),
                  Z = Z, lower = 0.1, upper = 5)

ngrid &lt;- 501
xgrid &lt;- matrix(seq(0,1, length.out = ngrid), ncol = 1)

## Precalculations
Wijs &lt;- Wij(mu1 = model$X0, theta = model$theta, type = model$covtype)


t0 &lt;- Sys.time()

IMSPE_grid &lt;- apply(xgrid, 1, crit_IMSPE, Wijs = Wijs, model = model)

t1 &lt;- Sys.time()
print(t1 - t0)

plot(xgrid, IMSPE_grid * model$nu_hat, xlab = "x", ylab = "crit_IMSPE values")
abline(v = designs)

###############################################################################
## Bi-variate case

nvar &lt;- 2 

set.seed(2)
ftest &lt;- function(x, coef = 0.1) return(sin(2*pi*sum(x)) + rnorm(1, sd = coef))

n &lt;- 16 # must be a square
xgrid0 &lt;- seq(0.1, 0.9, length.out = sqrt(n))
designs &lt;- as.matrix(expand.grid(xgrid0, xgrid0))
X &lt;- designs[rep(1:n, sample(1:10, n, replace = TRUE)),]
Z &lt;- apply(X, 1, ftest)

prdata &lt;- find_reps(X, Z, inputBounds = matrix(c(0,1), nrow = 2, ncol = 1))
Z &lt;- prdata$Z

model &lt;- mleHetGP(X = list(X0 = prdata$X0, Z0 = prdata$Z0, mult = prdata$mult), Z = Z, 
 lower = rep(0.1, nvar), upper = rep(1, nvar))
ngrid &lt;- 51
xgrid &lt;- seq(0,1, length.out = ngrid)
Xgrid &lt;- as.matrix(expand.grid(xgrid, xgrid))
## Precalculations
Wijs &lt;- Wij(mu1 = model$X0, theta = model$theta, type = model$covtype)
t0 &lt;- Sys.time()

IMSPE_grid &lt;- apply(Xgrid, 1, crit_IMSPE, Wijs = Wijs, model = model)
filled.contour(x = xgrid, y = xgrid, matrix(IMSPE_grid, ngrid),
               nlevels = 20, color.palette = terrain.colors,
               main = "Sequential IMSPE values")
</code></pre>

<hr>
<h2 id='crit_MCU'>Maximum Contour Uncertainty criterion</h2><span id='topic+crit_MCU'></span>

<h3>Description</h3>

<p>Computes MCU infill criterion
</p>


<h3>Usage</h3>

<pre><code class='language-R'>crit_MCU(x, model, thres = 0, gamma = 2, preds = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="crit_MCU_+3A_x">x</code></td>
<td>
<p>matrix of new designs, one point per row (size n x d)</p>
</td></tr>
<tr><td><code id="crit_MCU_+3A_model">model</code></td>
<td>
<p><code>homGP</code> or <code>hetGP</code> model, including inverse matrices</p>
</td></tr>
<tr><td><code id="crit_MCU_+3A_thres">thres</code></td>
<td>
<p>for contour finding</p>
</td></tr>
<tr><td><code id="crit_MCU_+3A_gamma">gamma</code></td>
<td>
<p>optional weight in -|f(x) - thres| + gamma * s(x). Default to 2.</p>
</td></tr>
<tr><td><code id="crit_MCU_+3A_preds">preds</code></td>
<td>
<p>optional predictions at <code>x</code> to avoid recomputing if already done</p>
</td></tr>
</table>


<h3>References</h3>

<p>Srinivas, N., Krause, A., Kakade, S, &amp; Seeger, M. (2012). 
Information-theoretic regret bounds for Gaussian process optimization 
in the bandit setting, IEEE Transactions on Information Theory, 58, pp. 3250-3265.<br /> <br />
</p>
<p>Bogunovic, J., Scarlett, J., Krause, A. &amp; Cevher, V. (2016). 
Truncated variance reduction: A unified approach to Bayesian optimization and level-set estimation,
in Advances in neural information processing systems, pp. 1507-1515. <br /> <br />
</p>
<p>Lyu, X., Binois, M. &amp; Ludkovski, M. (2018+). 
Evaluating Gaussian Process Metamodels and Sequential Designs for Noisy Level Set Estimation. arXiv:1807.06712. <br />
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Infill criterion example
set.seed(42)
branin &lt;- function(x){
  m &lt;- 54.8104; s &lt;- 51.9496
  if(is.null(dim(x))) x &lt;- matrix(x, nrow = 1)
  xx &lt;- 15 * x[,1] - 5; y &lt;- 15 * x[,2]
  f &lt;- (y - 5.1 * xx^2/(4 * pi^2) + 5 * xx/pi - 6)^2 + 10 * (1 - 1/(8 * pi)) * cos(xx) + 10
  f &lt;- (f - m)/s
  return(f)
}

ftest &lt;- function(x, sd = 0.1){
  if(is.null(dim(x))) x &lt;- matrix(x, nrow = 1)
  return(apply(x, 1, branin) + rnorm(nrow(x), sd = sd))
}

ngrid &lt;- 101; xgrid &lt;- seq(0, 1, length.out = ngrid)
Xgrid &lt;- as.matrix(expand.grid(xgrid, xgrid))
Zgrid &lt;- ftest(Xgrid)

n &lt;- 20
N &lt;- 500
X &lt;- Xgrid[sample(1:nrow(Xgrid), n),]
X &lt;- X[sample(1:n, N, replace = TRUE),]
Z &lt;- ftest(X)
model &lt;- mleHetGP(X, Z, lower = rep(0.001,2), upper = rep(1,2))

critgrid &lt;- apply(Xgrid, 1, crit_MCU, model = model)

filled.contour(matrix(critgrid, ngrid), color.palette = terrain.colors, main = "MEE criterion")

</code></pre>

<hr>
<h2 id='crit_MEE'>Maximum Empirical Error criterion</h2><span id='topic+crit_MEE'></span>

<h3>Description</h3>

<p>Computes MEE infill criterion
</p>


<h3>Usage</h3>

<pre><code class='language-R'>crit_MEE(x, model, thres = 0, preds = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="crit_MEE_+3A_x">x</code></td>
<td>
<p>matrix of new designs, one point per row (size n x d)</p>
</td></tr>
<tr><td><code id="crit_MEE_+3A_model">model</code></td>
<td>
<p><code>homGP</code> or <code>hetGP</code> model, including inverse matrices</p>
</td></tr>
<tr><td><code id="crit_MEE_+3A_thres">thres</code></td>
<td>
<p>for contour finding</p>
</td></tr>
<tr><td><code id="crit_MEE_+3A_preds">preds</code></td>
<td>
<p>optional predictions at <code>x</code> to avoid recomputing if already done</p>
</td></tr>
</table>


<h3>References</h3>

<p>Ranjan, P., Bingham, D. &amp; Michailidis, G (2008). 
Sequential experiment design for contour estimation from complex computer codes, 
Technometrics, 50, pp. 527-541. <br /> <br />
</p>
<p>Bichon, B., Eldred, M., Swiler, L., Mahadevan, S. &amp; McFarland, J. (2008).
Efficient global  reliability  analysis  for  nonlinear  implicit  performance  functions, 
AIAA Journal, 46, pp. 2459-2468. <br /> <br />
</p>
<p>Lyu, X., Binois, M. &amp; Ludkovski, M. (2018+). Evaluating Gaussian Process Metamodels and Sequential Designs for Noisy Level Set Estimation. arXiv:1807.06712. <br />
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Infill criterion example
set.seed(42)
branin &lt;- function(x){
  m &lt;- 54.8104; s &lt;- 51.9496
  if(is.null(dim(x))) x &lt;- matrix(x, nrow = 1)
  xx &lt;- 15 * x[,1] - 5; y &lt;- 15 * x[,2]
  f &lt;- (y - 5.1 * xx^2/(4 * pi^2) + 5 * xx/pi - 6)^2 + 10 * (1 - 1/(8 * pi)) * cos(xx) + 10
  f &lt;- (f - m)/s
  return(f)
}

ftest &lt;- function(x, sd = 0.1){
  if(is.null(dim(x))) x &lt;- matrix(x, nrow = 1)
  return(apply(x, 1, branin) + rnorm(nrow(x), sd = sd))
}

ngrid &lt;- 101; xgrid &lt;- seq(0, 1, length.out = ngrid)
Xgrid &lt;- as.matrix(expand.grid(xgrid, xgrid))
Zgrid &lt;- ftest(Xgrid)

n &lt;- 20
N &lt;- 500
X &lt;- Xgrid[sample(1:nrow(Xgrid), n),]
X &lt;- X[sample(1:n, N, replace = TRUE),]
Z &lt;- ftest(X)
model &lt;- mleHetGP(X, Z, lower = rep(0.001,2), upper = rep(1,2))

critgrid &lt;- apply(Xgrid, 1, crit_MEE, model = model)

filled.contour(matrix(critgrid, ngrid), color.palette = terrain.colors, main = "MEE criterion")

</code></pre>

<hr>
<h2 id='crit_optim'>Criterion optimization</h2><span id='topic+crit_optim'></span>

<h3>Description</h3>

<p>Search for the best value of available criterion, possibly using a h-steps lookahead strategy to favor designs with replication
</p>


<h3>Usage</h3>

<pre><code class='language-R'>crit_optim(
  model,
  crit,
  ...,
  h = 2,
  Xcand = NULL,
  control = list(multi.start = 10, maxit = 100),
  seed = NULL,
  ncores = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="crit_optim_+3A_model">model</code></td>
<td>
<p><code>homGP</code> or <code>hetGP</code> model</p>
</td></tr>
<tr><td><code id="crit_optim_+3A_crit">crit</code></td>
<td>
<p>considered criterion, one of <code>"crit_cSUR"</code>, <code>"crit_EI"</code>, <code>"crit_ICU"</code>,
<code>"crit_MCU"</code> and <code>"crit_tMSE"</code>. Note that <code>crit_IMSPE</code> has its dedicated method, see <code><a href="#topic+IMSPE_optim">IMSPE_optim</a></code>.</p>
</td></tr>
<tr><td><code id="crit_optim_+3A_...">...</code></td>
<td>
<p>additional parameters of the criterion</p>
</td></tr>
<tr><td><code id="crit_optim_+3A_h">h</code></td>
<td>
<p>horizon for multi-step ahead framework.
The decision is made between:
</p>

<ul>
<li><p> sequential crit search starting by a new design (optimized first) then adding <code>h</code> replicates
</p>
</li>
<li><p> sequential crit searches starting by <code>1</code> to <code>h</code> replicates before adding a new point
</p>
</li></ul>

<p>Use <code>h = 0</code> for the myopic criterion, i.e., not looking ahead.</p>
</td></tr>
<tr><td><code id="crit_optim_+3A_xcand">Xcand</code></td>
<td>
<p>optional discrete set of candidates (otherwise a maximin LHS is used to initialize continuous search)</p>
</td></tr>
<tr><td><code id="crit_optim_+3A_control">control</code></td>
<td>
<p>list in case <code>Xcand == NULL</code>, with elements <code>multi.start</code>,
to perform a multi-start optimization based on <code><a href="stats.html#topic+optim">optim</a></code>, with <code>maxit</code> iterations each.
Also, <code>tol_dist</code> defines the minimum distance to an existing design for a new point to be added, otherwise the closest existing design is chosen.
In a similar fashion, <code>tol_dist</code> is the minimum relative change of crit for adding a new design.</p>
</td></tr>
<tr><td><code id="crit_optim_+3A_seed">seed</code></td>
<td>
<p>optional seed for the generation of LHS designs with <code><a href="DiceDesign.html#topic+maximinSA_LHS">maximinSA_LHS</a></code></p>
</td></tr>
<tr><td><code id="crit_optim_+3A_ncores">ncores</code></td>
<td>
<p>number of CPU available (&gt; 1 mean parallel TRUE), see <code><a href="parallel.html#topic+mclapply">mclapply</a></code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>When looking ahead, the kriging believer heuristic is used,
meaning that the non-observed value is replaced by the mean prediction in the update.
</p>


<h3>Value</h3>

<p>list with elements:
</p>

<ul>
<li> <p><code>par</code>: best first design,
</p>
</li>
<li> <p><code>value</code>: criterion h-steps ahead starting from adding <code>par</code>,
</p>
</li>
<li> <p><code>path</code>: list of elements list(<code>par</code>, <code>value</code>, <code>new</code>) at each step <code>h</code>
</p>
</li></ul>



<h3>References</h3>

<p>M. Binois, J. Huang, R. B. Gramacy, M. Ludkovski (2019), 
Replication or exploration? Sequential design for stochastic simulation experiments,
Technometrics, 61(1), 7-23.<br /> 
Preprint available on arXiv:1710.03206.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>###############################################################################
## Bi-variate example (myopic version)
###############################################################################

nvar &lt;- 2 

set.seed(42)
ftest &lt;- function(x, coef = 0.1) return(sin(2*pi*sum(x)) + rnorm(1, sd = coef))

n &lt;- 25 # must be a square
xgrid0 &lt;- seq(0.1, 0.9, length.out = sqrt(n))
designs &lt;- as.matrix(expand.grid(xgrid0, xgrid0))
X &lt;- designs[rep(1:n, sample(1:10, n, replace = TRUE)),]
Z &lt;- apply(X, 1, ftest)

model &lt;- mleHomGP(X, Z, lower = rep(0.1, nvar), upper = rep(1, nvar))

ngrid &lt;- 51
xgrid &lt;- seq(0,1, length.out = ngrid)
Xgrid &lt;- as.matrix(expand.grid(xgrid, xgrid))

preds &lt;- predict(x = Xgrid, object =  model)

## Initial plots
contour(x = xgrid,  y = xgrid, z = matrix(preds$mean, ngrid),
        main = "Predicted mean", nlevels = 20)
points(model$X0, col = 'blue', pch = 20)

crit &lt;- "crit_EI"
crit_grid &lt;- apply(Xgrid, 1, crit, model = model)
filled.contour(x = xgrid, y = xgrid, matrix(crit_grid, ngrid),
               nlevels = 20, color.palette = terrain.colors, 
               main = "Initial criterion landscape",
plot.axes = {axis(1); axis(2); points(model$X0, pch = 20)})

## Sequential crit search
nsteps &lt;- 1 # Increase for better results

for(i in 1:nsteps){
  res &lt;- crit_optim(model, crit = crit, h = 0, control = list(multi.start = 50, maxit = 30))
  newX &lt;- res$par
  newZ &lt;- ftest(newX)
  model &lt;- update(object = model, Xnew = newX, Znew = newZ)
}

## Final plots
contour(x = xgrid,  y = xgrid, z = matrix(preds$mean, ngrid),
        main = "Predicted mean", nlevels = 20)
points(model$X0, col = 'blue', pch = 20)

crit_grid &lt;- apply(Xgrid, 1, crit, model = model)
filled.contour(x = xgrid, y = xgrid, matrix(crit_grid, ngrid),
               nlevels = 20, color.palette = terrain.colors, 
               main = "Final criterion landscape",
plot.axes = {axis(1); axis(2); points(model$X0, pch = 20)})

###############################################################################
## Bi-variate example (look-ahead version)
###############################################################################
## Not run:   
nvar &lt;- 2 

set.seed(42)
ftest &lt;- function(x, coef = 0.1) return(sin(2*pi*sum(x)) + rnorm(1, sd = coef))

n &lt;- 25 # must be a square
xgrid0 &lt;- seq(0.1, 0.9, length.out = sqrt(n))
designs &lt;- as.matrix(expand.grid(xgrid0, xgrid0))
X &lt;- designs[rep(1:n, sample(1:10, n, replace = TRUE)),]
Z &lt;- apply(X, 1, ftest)

model &lt;- mleHomGP(X, Z, lower = rep(0.1, nvar), upper = rep(1, nvar))

ngrid &lt;- 51
xgrid &lt;- seq(0,1, length.out = ngrid)
Xgrid &lt;- as.matrix(expand.grid(xgrid, xgrid))

nsteps &lt;- 5 # Increase for more steps
crit &lt;- "crit_EI"

# To use parallel computation (turn off on Windows)
library(parallel)
parallel &lt;- FALSE #TRUE #
if(parallel) ncores &lt;- detectCores() else ncores &lt;- 1

for(i in 1:nsteps){
  res &lt;- crit_optim(model, h = 3, crit = crit, ncores = ncores,
                    control = list(multi.start = 100, maxit = 50))
  
  # If a replicate is selected
  if(!res$path[[1]]$new) print("Add replicate")
  
  newX &lt;- res$par
  newZ &lt;- ftest(newX)
  model &lt;- update(object = model, Xnew = newX, Znew = newZ)
  
  ## Plots 
  preds &lt;- predict(x = Xgrid, object =  model)
  contour(x = xgrid,  y = xgrid, z = matrix(preds$mean, ngrid),
          main = "Predicted mean", nlevels = 20)
  points(model$X0, col = 'blue', pch = 20)
  points(newX, col = "red", pch = 20)
  
  crit_grid &lt;- apply(Xgrid, 1, crit, model = model)
  filled.contour(x = xgrid, y = xgrid, matrix(crit_grid, ngrid),
                 nlevels = 20, color.palette = terrain.colors,
  plot.axes = {axis(1); axis(2); points(model$X0, pch = 20)})
}

## End(Not run)
</code></pre>

<hr>
<h2 id='crit_qEI'>Parallel Expected improvement</h2><span id='topic+crit_qEI'></span>

<h3>Description</h3>

<p>Fast approximated batch-Expected Improvement criterion (for minimization)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>crit_qEI(x, model, cst = NULL, preds = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="crit_qEI_+3A_x">x</code></td>
<td>
<p>matrix of new designs representing the batch of q points,
one point per row (size q x d)</p>
</td></tr>
<tr><td><code id="crit_qEI_+3A_model">model</code></td>
<td>
<p><code>homGP</code> or <code>hetGP</code> model, including inverse matrices.</p>
</td></tr>
<tr><td><code id="crit_qEI_+3A_cst">cst</code></td>
<td>
<p>optional plugin value used in the EI, see details</p>
</td></tr>
<tr><td><code id="crit_qEI_+3A_preds">preds</code></td>
<td>
<p>optional predictions at <code>x</code> to avoid recomputing if already done (must include the predictive covariance, i.e., the <code>cov</code> slot)</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>cst</code> is classically the observed minimum in the deterministic case. 
In the noisy case, the min of the predictive mean works fine.
</p>


<h3>Note</h3>

<p>This is a beta version at this point. It may work for for TP models as well.
</p>


<h3>References</h3>

<p>M. Binois (2015), Uncertainty quantification on Pareto fronts and high-dimensional strategies in Bayesian optimization, with applications in multi-objective automotive design.
Ecole Nationale Superieure des Mines de Saint-Etienne, PhD thesis.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Optimization example (noiseless)
set.seed(42)

## Test function defined in [0,1]
ftest &lt;- f1d

n_init &lt;- 5 # number of unique designs
X &lt;- seq(0, 1, length.out = n_init)
X &lt;- matrix(X, ncol = 1)
Z &lt;- ftest(X)

## Predictive grid
ngrid &lt;- 51
xgrid &lt;- seq(0,1, length.out = ngrid)
Xgrid &lt;- matrix(xgrid, ncol = 1)

model &lt;- mleHomGP(X = X, Z = Z, lower = 0.01, upper = 1, known = list(g = 2e-8))

# Regular EI function
cst &lt;- min(model$Z0)
EIgrid &lt;- crit_EI(Xgrid, model, cst = cst)
plot(xgrid, EIgrid, type = "l")
abline(v = X, lty = 2) # observations

# Create batch (based on regular EI peaks)
xbatch &lt;- matrix(c(0.37, 0.17, 0.7), 3, 1)
abline(v = xbatch, col = "red")
fqEI &lt;- crit_qEI(xbatch, model, cst = cst)

# Compare with Monte Carlo qEI
preds &lt;- predict(model, xbatch, xprime = xbatch)
nsim &lt;- 1e4
simus &lt;- matrix(rnorm(3 * nsim), nsim) %*% chol(preds$cov)
simus &lt;- simus + matrix(preds$mean, nrow = nsim, ncol = 3, byrow = TRUE)
MCqEI &lt;- mean(apply(cst - simus, 1, function(x) max(c(x, 0))))

</code></pre>

<hr>
<h2 id='crit_tMSE'>t-MSE criterion</h2><span id='topic+crit_tMSE'></span>

<h3>Description</h3>

<p>Computes targeted mean squared error infill criterion
</p>


<h3>Usage</h3>

<pre><code class='language-R'>crit_tMSE(x, model, thres = 0, preds = NULL, seps = 0.05)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="crit_tMSE_+3A_x">x</code></td>
<td>
<p>matrix of new designs, one point per row (size n x d)</p>
</td></tr>
<tr><td><code id="crit_tMSE_+3A_model">model</code></td>
<td>
<p><code>homGP</code> or <code>hetGP</code> model, including inverse matrices</p>
</td></tr>
<tr><td><code id="crit_tMSE_+3A_thres">thres</code></td>
<td>
<p>for contour finding</p>
</td></tr>
<tr><td><code id="crit_tMSE_+3A_preds">preds</code></td>
<td>
<p>optional predictions at <code>x</code> to avoid recomputing if already done (must contain <code>cov</code>)</p>
</td></tr>
<tr><td><code id="crit_tMSE_+3A_seps">seps</code></td>
<td>
<p>parameter for the target window</p>
</td></tr>
</table>


<h3>References</h3>

<p>Picheny, V., Ginsbourger, D., Roustant, O., Haftka, R., Kim, N. (2010).
Adaptive designs of experiments for accurate approximation of a target region,
Journal of Mechanical Design (132), p. 071008.<br /> <br />
</p>
<p>Lyu, X., Binois, M. &amp; Ludkovski, M. (2018+). 
Evaluating Gaussian Process Metamodels and Sequential Designs for Noisy Level Set Estimation. arXiv:1807.06712. <br />
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Infill criterion example
set.seed(42)
branin &lt;- function(x){
  m &lt;- 54.8104; s &lt;- 51.9496
  if(is.null(dim(x))) x &lt;- matrix(x, nrow = 1)
  xx &lt;- 15 * x[,1] - 5; y &lt;- 15 * x[,2]
  f &lt;- (y - 5.1 * xx^2/(4 * pi^2) + 5 * xx/pi - 6)^2 + 10 * (1 - 1/(8 * pi)) * cos(xx) + 10
  f &lt;- (f - m)/s
  return(f)
}

ftest &lt;- function(x, sd = 0.1){
  if(is.null(dim(x))) x &lt;- matrix(x, nrow = 1)
  return(apply(x, 1, branin) + rnorm(nrow(x), sd = sd))
}

ngrid &lt;- 101; xgrid &lt;- seq(0, 1, length.out = ngrid)
Xgrid &lt;- as.matrix(expand.grid(xgrid, xgrid))
Zgrid &lt;- ftest(Xgrid)

n &lt;- 20
N &lt;- 500
X &lt;- Xgrid[sample(1:nrow(Xgrid), n),]
X &lt;- X[sample(1:n, N, replace = TRUE),]
Z &lt;- ftest(X)
model &lt;- mleHetGP(X, Z, lower = rep(0.001,2), upper = rep(1,2))

critgrid &lt;- apply(Xgrid, 1, crit_tMSE, model = model)

filled.contour(matrix(critgrid, ngrid), color.palette = terrain.colors, main = "tMSE criterion")

</code></pre>

<hr>
<h2 id='deriv_crit_EI'>Derivative of EI criterion for GP models</h2><span id='topic+deriv_crit_EI'></span>

<h3>Description</h3>

<p>Derivative of EI criterion for GP models
</p>


<h3>Usage</h3>

<pre><code class='language-R'>deriv_crit_EI(x, model, cst = NULL, preds = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="deriv_crit_EI_+3A_x">x</code></td>
<td>
<p>matrix for the new design (size 1 x d)</p>
</td></tr>
<tr><td><code id="deriv_crit_EI_+3A_model">model</code></td>
<td>
<p><code>homGP</code> or <code>hetGP</code> model</p>
</td></tr>
<tr><td><code id="deriv_crit_EI_+3A_cst">cst</code></td>
<td>
<p>threshold for contour criteria</p>
</td></tr>
<tr><td><code id="deriv_crit_EI_+3A_preds">preds</code></td>
<td>
<p>pre-computed preds for contour criteria</p>
</td></tr>
</table>


<h3>References</h3>

<p>Ginsbourger, D. Multiples metamodeles pour l'approximation et l'optimisation de fonctions numeriques multivariables Ecole Nationale Superieure des Mines de Saint-Etienne, Ecole Nationale Superieure des Mines de Saint-Etienne, 2009 <br /> <br />
Roustant, O., Ginsbourger, D., DiceKriging, DiceOptim: Two R packages for the analysis of computer experiments by kriging-based metamodeling and optimization, Journal of Statistical Software, 2012
</p>


<h3>See Also</h3>

<p><code><a href="#topic+crit_EI">crit_EI</a></code> for the criterion
</p>

<hr>
<h2 id='deriv_crit_IMSPE'>Derivative of crit_IMSPE</h2><span id='topic+deriv_crit_IMSPE'></span>

<h3>Description</h3>

<p>Derivative of crit_IMSPE
</p>


<h3>Usage</h3>

<pre><code class='language-R'>deriv_crit_IMSPE(x, model, Wijs = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="deriv_crit_IMSPE_+3A_x">x</code></td>
<td>
<p>matrix for the new design (size 1 x d)</p>
</td></tr>
<tr><td><code id="deriv_crit_IMSPE_+3A_model">model</code></td>
<td>
<p><code>homGP</code> or <code>hetGP</code> model</p>
</td></tr>
<tr><td><code id="deriv_crit_IMSPE_+3A_wijs">Wijs</code></td>
<td>
<p>optional previously computed matrix of Wijs, see <code><a href="#topic+Wij">Wij</a></code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>Derivative of the sequential IMSPE with respect to <code>x</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+crit_IMSPE">crit_IMSPE</a></code> for the criterion
</p>

<hr>
<h2 id='f1d'>1d test function (1)</h2><span id='topic+f1d'></span>

<h3>Description</h3>

<p>1d test function (1)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>f1d(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="f1d_+3A_x">x</code></td>
<td>
<p>scalar or matrix (size n x 1) in [0,1]</p>
</td></tr>
</table>


<h3>References</h3>

<p>A. Forrester, A. Sobester, A. Keane (2008), Engineering design via surrogate modelling: a practical guide, John Wiley &amp; Sons
</p>


<h3>Examples</h3>

<pre><code class='language-R'>plot(f1d)
</code></pre>

<hr>
<h2 id='f1d_n'>Noisy 1d test function (1)
Add Gaussian noise with variance r(x) = scale * (1.1 + sin(2 pi x))^2 to <code><a href="#topic+f1d">f1d</a></code></h2><span id='topic+f1d_n'></span>

<h3>Description</h3>

<p>Noisy 1d test function (1)
Add Gaussian noise with variance r(x) = scale * (1.1 + sin(2 pi x))^2 to <code><a href="#topic+f1d">f1d</a></code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>f1d_n(x, scale = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="f1d_n_+3A_x">x</code></td>
<td>
<p>scalar or matrix (size n x 1) in [0,1]</p>
</td></tr>
<tr><td><code id="f1d_n_+3A_scale">scale</code></td>
<td>
<p>scalar in [0, Inf] to control the signal to noise ratio</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>X &lt;- matrix(seq(0, 1, length.out = 101), ncol = 1)
Xr &lt;- X[sort(sample(x = 1:101, size = 500, replace = TRUE)),, drop = FALSE]
plot(Xr, f1d_n(Xr))
lines(X, f1d(X), col = "red", lwd = 2)
</code></pre>

<hr>
<h2 id='f1d2'>1d test function (2)</h2><span id='topic+f1d2'></span>

<h3>Description</h3>

<p>1d test function (2)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>f1d2(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="f1d2_+3A_x">x</code></td>
<td>
<p>scalar or matrix (size n x 1) in [0,1]</p>
</td></tr>
</table>


<h3>References</h3>

<p>A. Boukouvalas, and D. Cornford (2009), Learning heteroscedastic Gaussian processes for complex datasets, Technical report. <br /> <br />
M. Yuan, and  G. Wahba (2004), Doubly penalized likelihood estimator in heteroscedastic regression, Statistics and Probability Letters 69, 11-20.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>plot(f1d2)
</code></pre>

<hr>
<h2 id='f1d2_n'>Noisy 1d test function (2)
Add Gaussian noise with variance r(x) = scale * (exp(sin(2 pi x)))^2 to <code><a href="#topic+f1d2">f1d2</a></code></h2><span id='topic+f1d2_n'></span>

<h3>Description</h3>

<p>Noisy 1d test function (2)
Add Gaussian noise with variance r(x) = scale * (exp(sin(2 pi x)))^2 to <code><a href="#topic+f1d2">f1d2</a></code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>f1d2_n(x, scale = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="f1d2_n_+3A_x">x</code></td>
<td>
<p>scalar or matrix (size n x 1) in [0,1]</p>
</td></tr>
<tr><td><code id="f1d2_n_+3A_scale">scale</code></td>
<td>
<p>scalar in [0, Inf] to control the signal to noise ratio</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>X &lt;- matrix(seq(0, 1, length.out = 101), ncol = 1)
Xr &lt;- X[sort(sample(x = 1:101, size = 500, replace = TRUE)),, drop = FALSE]
plot(Xr, f1d2_n(Xr))
lines(X, f1d2(X), col = "red", lwd = 2)
</code></pre>

<hr>
<h2 id='find_reps'>Data preprocessing</h2><span id='topic+find_reps'></span>

<h3>Description</h3>

<p>Prepare data for use with <code><a href="#topic+mleHetGP">mleHetGP</a></code>, in particular to find replicated observations
</p>


<h3>Usage</h3>

<pre><code class='language-R'>find_reps(
  X,
  Z,
  return.Zlist = TRUE,
  rescale = FALSE,
  normalize = FALSE,
  inputBounds = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="find_reps_+3A_x">X</code></td>
<td>
<p>matrix of design locations, one point per row</p>
</td></tr>
<tr><td><code id="find_reps_+3A_z">Z</code></td>
<td>
<p>vector of observations at <code>X</code></p>
</td></tr>
<tr><td><code id="find_reps_+3A_return.zlist">return.Zlist</code></td>
<td>
<p>to return <code>Zlist</code>, see below</p>
</td></tr>
<tr><td><code id="find_reps_+3A_rescale">rescale</code></td>
<td>
<p>if <code>TRUE</code>, the inputs are rescaled to the unit hypercube</p>
</td></tr>
<tr><td><code id="find_reps_+3A_normalize">normalize</code></td>
<td>
<p>if <code>TRUE</code>, the outputs are centered and normalized</p>
</td></tr>
<tr><td><code id="find_reps_+3A_inputbounds">inputBounds</code></td>
<td>
<p>optional matrix of known boundaries in original input space, of size 2 times <code>ncol(X)</code>. 
If not provided, and <code>rescale == TRUE</code>, it is estimated from the data.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Replicates are searched based on character representation, using <code><a href="base.html#topic+unique">unique</a></code>.
</p>


<h3>Value</h3>

<p>A list with the following elements that can be passed to the main fitting functions, e.g., <code><a href="#topic+mleHetGP">mleHetGP</a></code> and <code><a href="#topic+mleHomGP">mleHomGP</a></code>
</p>

<ul>
<li> <p><code>X0</code> matrix with unique designs locations, one point per row,
</p>
</li>
<li> <p><code>Z0</code> vector of averaged observations at <code>X0</code>,
</p>
</li>
<li> <p><code>mult</code> number of replicates at <code>X0</code>,
</p>
</li>
<li> <p><code>Z</code> vector with all observations, sorted according to <code>X0</code>,
</p>
</li>
<li> <p><code>Zlist</code> optional list, each element corresponds to observations at a design in <code>X0</code>,
</p>
</li>
<li> <p><code>inputBounds</code> optional matrix, to rescale back to the original input space,
</p>
</li>
<li> <p><code>outputStats</code> optional vector, with mean and variance of the original outputs.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>##------------------------------------------------------------
## Find replicates on the motorcycle data
##------------------------------------------------------------
## motorcycle data
library(MASS)
X &lt;- matrix(mcycle$times, ncol = 1)
Z &lt;- mcycle$accel

data_m &lt;- find_reps(X, Z)

# Initial data
plot(X, Z, ylim = c(-160, 90), ylab = 'acceleration', xlab = "time")
# Display mean values
points(data_m$X0, data_m$Z0, pch = 20)
</code></pre>

<hr>
<h2 id='horizon'>Adapt horizon</h2><span id='topic+horizon'></span>

<h3>Description</h3>

<p>Adapt the look-ahead horizon depending on the replicate allocation or a target ratio
</p>


<h3>Usage</h3>

<pre><code class='language-R'>horizon(
  model,
  current_horizon = NULL,
  previous_ratio = NULL,
  target = NULL,
  Wijs = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="horizon_+3A_model">model</code></td>
<td>
<p><code>hetGP</code> or <code>homGP</code> model</p>
</td></tr>
<tr><td><code id="horizon_+3A_current_horizon">current_horizon</code></td>
<td>
<p>horizon used for the previous iteration, see details</p>
</td></tr>
<tr><td><code id="horizon_+3A_previous_ratio">previous_ratio</code></td>
<td>
<p>ratio before adding the previous new design</p>
</td></tr>
<tr><td><code id="horizon_+3A_target">target</code></td>
<td>
<p>scalar in ]0,1] for desired n/N</p>
</td></tr>
<tr><td><code id="horizon_+3A_wijs">Wijs</code></td>
<td>
<p>optional previously computed matrix of Wijs, see <code><a href="#topic+Wij">Wij</a></code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>target</code> is provided, along with <code>previous_ratio</code> and <code>current_horizon</code>:
</p>

<ul>
<li><p> the horizon is increased by one if more replicates are needed but a new ppint has been added at the previous iteration,
</p>
</li>
<li><p> the horizon is decreased by one if new points are needed but a replicate has been added at the previous iteration,
</p>
</li>
<li><p> otherwise it is unchanged.
</p>
</li></ul>

<p>If no <code>target</code> is provided, <code><a href="#topic+allocate_mult">allocate_mult</a></code> is used to obtain the best allocation of the existing replicates,
then the new horizon is sampled from the difference between the actual allocation and the best one, bounded below by 0.
See (Binois et al. 2017).
</p>


<h3>Value</h3>

<p>randomly selected horizon for next iteration (adpative) if no <code>target</code> is provided, 
otherwise returns the update horizon value.
</p>


<h3>References</h3>

<p>M. Binois, J. Huang, R. B. Gramacy, M. Ludkovski (2019), 
Replication or exploration? Sequential design for stochastic simulation experiments,
Technometrics, 61(1), 7-23.<br /> 
Preprint available on arXiv:1710.03206.
</p>

<hr>
<h2 id='IMSPE'>Integrated Mean Square Prediction Error</h2><span id='topic+IMSPE'></span>

<h3>Description</h3>

<p>IMSPE of a given design
</p>


<h3>Usage</h3>

<pre><code class='language-R'>IMSPE(
  X,
  theta = NULL,
  Lambda = NULL,
  mult = NULL,
  covtype = NULL,
  nu = NULL,
  eps = sqrt(.Machine$double.eps)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="IMSPE_+3A_x">X</code></td>
<td>
<p><code>hetGP</code> or <code>homGP</code> model. Alternatively, one can provide a matrix of unique designs considered</p>
</td></tr>
<tr><td><code id="IMSPE_+3A_theta">theta</code></td>
<td>
<p>lengthscales</p>
</td></tr>
<tr><td><code id="IMSPE_+3A_lambda">Lambda</code></td>
<td>
<p>diagonal matrix for the noise</p>
</td></tr>
<tr><td><code id="IMSPE_+3A_mult">mult</code></td>
<td>
<p>number of replicates at each design</p>
</td></tr>
<tr><td><code id="IMSPE_+3A_covtype">covtype</code></td>
<td>
<p>either &quot;Gaussian&quot;, &quot;Matern3_2&quot; or &quot;Matern5_2&quot;</p>
</td></tr>
<tr><td><code id="IMSPE_+3A_nu">nu</code></td>
<td>
<p>variance parameter</p>
</td></tr>
<tr><td><code id="IMSPE_+3A_eps">eps</code></td>
<td>
<p>numerical nugget</p>
</td></tr>
</table>


<h3>Details</h3>

<p>One can provide directly a model of class <code>hetGP</code> or <code>homGP</code>, or provide <code>X</code> and all other arguments
</p>

<hr>
<h2 id='IMSPE_optim'>IMSPE optimization</h2><span id='topic+IMSPE_optim'></span>

<h3>Description</h3>

<p>Search for the best value of the IMSPE criterion, possibly using a h-steps lookahead strategy to favor designs with replication
</p>


<h3>Usage</h3>

<pre><code class='language-R'>IMSPE_optim(
  model,
  h = 2,
  Xcand = NULL,
  control = list(tol_dist = 1e-06, tol_diff = 1e-06, multi.start = 20, maxit = 100),
  Wijs = NULL,
  seed = NULL,
  ncores = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="IMSPE_optim_+3A_model">model</code></td>
<td>
<p><code>homGP</code> or <code>hetGP</code> model</p>
</td></tr>
<tr><td><code id="IMSPE_optim_+3A_h">h</code></td>
<td>
<p>horizon for multi-step ahead framework.
The decision is made between:
</p>

<ul>
<li><p> sequential crit search starting by a new design (optimized first) then adding <code>h</code> replicates
</p>
</li>
<li><p> sequential crit searches starting by <code>1</code> to <code>h</code> replicates before adding a new point
</p>
</li></ul>

<p>Use <code>h = 0</code> for the myopic criterion, i.e., not looking ahead.</p>
</td></tr>
<tr><td><code id="IMSPE_optim_+3A_xcand">Xcand</code></td>
<td>
<p>optional discrete set of candidates (otherwise a maximin LHS is used to initialise continuous search)</p>
</td></tr>
<tr><td><code id="IMSPE_optim_+3A_control">control</code></td>
<td>
<p>list in case <code>Xcand == NULL</code>, with elements <code>multi.start</code>,
to perform a multi-start optimization based on <code><a href="stats.html#topic+optim">optim</a></code>, with <code>maxit</code> iterations each.
Also, <code>tol_dist</code> defines the minimum distance to an existing design for a new point to be added, otherwise the closest existing design is chosen.
In a similar fashion, <code>tol_dist</code> is the minimum relative change of IMSPE for adding a new design.</p>
</td></tr>
<tr><td><code id="IMSPE_optim_+3A_wijs">Wijs</code></td>
<td>
<p>optional previously computed matrix of Wijs, see <code><a href="#topic+Wij">Wij</a></code></p>
</td></tr>
<tr><td><code id="IMSPE_optim_+3A_seed">seed</code></td>
<td>
<p>optional seed for the generation of designs with <code><a href="DiceDesign.html#topic+maximinSA_LHS">maximinSA_LHS</a></code></p>
</td></tr>
<tr><td><code id="IMSPE_optim_+3A_ncores">ncores</code></td>
<td>
<p>number of CPU available (&gt; 1 mean parallel TRUE), see <code><a href="parallel.html#topic+mclapply">mclapply</a></code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>The domain needs to be [0, 1]^d for now.
</p>


<h3>Value</h3>

<p>list with elements:
</p>

<ul>
<li> <p><code>par</code>: best first design,
</p>
</li>
<li> <p><code>value</code>: IMSPE h-steps ahead starting from adding <code>par</code>,
</p>
</li>
<li> <p><code>path</code>: list of elements list(<code>par</code>, <code>value</code>, <code>new</code>) at each step <code>h</code>
</p>
</li></ul>



<h3>References</h3>

<p>M. Binois, J. Huang, R. B. Gramacy, M. Ludkovski (2019), 
Replication or exploration? Sequential design for stochastic simulation experiments,
Technometrics, 61(1), 7-23.<br /> 
Preprint available on arXiv:1710.03206.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>###############################################################################
## Bi-variate example (myopic version)
###############################################################################

nvar &lt;- 2 

set.seed(42)
ftest &lt;- function(x, coef = 0.1) return(sin(2*pi*sum(x)) + rnorm(1, sd = coef))

n &lt;- 25 # must be a square
xgrid0 &lt;- seq(0.1, 0.9, length.out = sqrt(n))
designs &lt;- as.matrix(expand.grid(xgrid0, xgrid0))
X &lt;- designs[rep(1:n, sample(1:10, n, replace = TRUE)),]
Z &lt;- apply(X, 1, ftest)

model &lt;- mleHomGP(X, Z, lower = rep(0.1, nvar), upper = rep(1, nvar))

ngrid &lt;- 51
xgrid &lt;- seq(0,1, length.out = ngrid)
Xgrid &lt;- as.matrix(expand.grid(xgrid, xgrid))

preds &lt;- predict(x = Xgrid, object =  model)

## Initial plots
contour(x = xgrid,  y = xgrid, z = matrix(preds$mean, ngrid),
        main = "Predicted mean", nlevels = 20)
points(model$X0, col = 'blue', pch = 20)

IMSPE_grid &lt;- apply(Xgrid, 1, crit_IMSPE, model = model)
filled.contour(x = xgrid, y = xgrid, matrix(IMSPE_grid, ngrid),
               nlevels = 20, color.palette = terrain.colors, 
               main = "Initial IMSPE criterion landscape",
plot.axes = {axis(1); axis(2); points(model$X0, pch = 20)})

## Sequential IMSPE search
nsteps &lt;- 1 # Increase for better results

for(i in 1:nsteps){
  res &lt;- IMSPE_optim(model, control = list(multi.start = 30, maxit = 30))
  newX &lt;- res$par
  newZ &lt;- ftest(newX)
  model &lt;- update(object = model, Xnew = newX, Znew = newZ)
}

## Final plots
contour(x = xgrid,  y = xgrid, z = matrix(preds$mean, ngrid),
        main = "Predicted mean", nlevels = 20)
points(model$X0, col = 'blue', pch = 20)

IMSPE_grid &lt;- apply(Xgrid, 1, crit_IMSPE, model = model)
filled.contour(x = xgrid, y = xgrid, matrix(IMSPE_grid, ngrid),
               nlevels = 20, color.palette = terrain.colors, 
               main = "Final IMSPE criterion landscape",
plot.axes = {axis(1); axis(2); points(model$X0, pch = 20)})

###############################################################################
## Bi-variate example (look-ahead version)
###############################################################################
## Not run:  
nvar &lt;- 2 

set.seed(42)
ftest &lt;- function(x, coef = 0.1) return(sin(2*pi*sum(x)) + rnorm(1, sd = coef))

n &lt;- 25 # must be a square
xgrid0 &lt;- seq(0.1, 0.9, length.out = sqrt(n))
designs &lt;- as.matrix(expand.grid(xgrid0, xgrid0))
X &lt;- designs[rep(1:n, sample(1:10, n, replace = TRUE)),]
Z &lt;- apply(X, 1, ftest)

model &lt;- mleHomGP(X, Z, lower = rep(0.1, nvar), upper = rep(1, nvar))

ngrid &lt;- 51
xgrid &lt;- seq(0,1, length.out = ngrid)
Xgrid &lt;- as.matrix(expand.grid(xgrid, xgrid))

nsteps &lt;- 5 # Increase for more steps

# To use parallel computation (turn off on Windows)
library(parallel)
parallel &lt;- FALSE #TRUE #
if(parallel) ncores &lt;- detectCores() else ncores &lt;- 1

for(i in 1:nsteps){
  res &lt;- IMSPE_optim(model, h = 3, control = list(multi.start = 100, maxit = 50),
   ncores = ncores)
  
  # If a replicate is selected
  if(!res$path[[1]]$new) print("Add replicate")
  
  newX &lt;- res$par
  newZ &lt;- ftest(newX)
  model &lt;- update(object = model, Xnew = newX, Znew = newZ)
  
  ## Plots 
  preds &lt;- predict(x = Xgrid, object =  model)
  contour(x = xgrid,  y = xgrid, z = matrix(preds$mean, ngrid),
          main = "Predicted mean", nlevels = 20)
  points(model$X0, col = 'blue', pch = 20)
  points(newX, col = "red", pch = 20)
  
  ## Precalculations
  Wijs &lt;- Wij(mu1 = model$X0, theta = model$theta, type = model$covtype)
  
  IMSPE_grid &lt;- apply(Xgrid, 1, crit_IMSPE, Wijs = Wijs, model = model)
  filled.contour(x = xgrid, y = xgrid, matrix(IMSPE_grid, ngrid),
                 nlevels = 20, color.palette = terrain.colors,
  plot.axes = {axis(1); axis(2); points(model$X0, pch = 20)})
}

## End(Not run)
</code></pre>

<hr>
<h2 id='logLikH'>Generic Log-likelihood function
This function can be used to compute loglikelihood for homGP/hetGP models</h2><span id='topic+logLikH'></span>

<h3>Description</h3>

<p>Generic Log-likelihood function
This function can be used to compute loglikelihood for homGP/hetGP models
</p>


<h3>Usage</h3>

<pre><code class='language-R'>logLikH(
  X0,
  Z0,
  Z,
  mult,
  theta,
  g,
  Delta = NULL,
  k_theta_g = NULL,
  theta_g = NULL,
  logN = FALSE,
  beta0 = NULL,
  eps = sqrt(.Machine$double.eps),
  covtype = "Gaussian"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="logLikH_+3A_x0">X0</code></td>
<td>
<p>unique designs</p>
</td></tr>
<tr><td><code id="logLikH_+3A_z0">Z0</code></td>
<td>
<p>averaged observations</p>
</td></tr>
<tr><td><code id="logLikH_+3A_z">Z</code></td>
<td>
<p>replicated observations (sorted with respect to X0)</p>
</td></tr>
<tr><td><code id="logLikH_+3A_mult">mult</code></td>
<td>
<p>number of replicates at each Xi</p>
</td></tr>
<tr><td><code id="logLikH_+3A_theta">theta</code></td>
<td>
<p>scale parameter for the mean process, either one value (isotropic) or a vector (anistropic)</p>
</td></tr>
<tr><td><code id="logLikH_+3A_g">g</code></td>
<td>
<p>nugget of the nugget process</p>
</td></tr>
<tr><td><code id="logLikH_+3A_delta">Delta</code></td>
<td>
<p>vector of nuggets corresponding to each X0i or pXi, that are smoothed to give Lambda</p>
</td></tr>
<tr><td><code id="logLikH_+3A_k_theta_g">k_theta_g</code></td>
<td>
<p>constant used for linking nuggets lengthscale to mean process lengthscale, i.e., theta_g[k] = k_theta_g * theta[k], alternatively theta_g can be used</p>
</td></tr>
<tr><td><code id="logLikH_+3A_theta_g">theta_g</code></td>
<td>
<p>either one value (isotropic) or a vector (anistropic), alternative to using k_theta_g</p>
</td></tr>
<tr><td><code id="logLikH_+3A_logn">logN</code></td>
<td>
<p>should exponentiated variance be used</p>
</td></tr>
<tr><td><code id="logLikH_+3A_beta0">beta0</code></td>
<td>
<p>mean, if not provided, the MLE estimator is used</p>
</td></tr>
<tr><td><code id="logLikH_+3A_eps">eps</code></td>
<td>
<p>minimal value of elements of Lambda</p>
</td></tr>
<tr><td><code id="logLikH_+3A_covtype">covtype</code></td>
<td>
<p>covariance kernel type</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For hetGP, this is not the joint log-likelihood, only the likelihood of the mean process.
</p>

<hr>
<h2 id='LOO_preds'>Leave one out predictions</h2><span id='topic+LOO_preds'></span>

<h3>Description</h3>

<p>Provide leave one out predictions, e.g., for model testing and diagnostics. 
This is used in the method plot available on GP and TP models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>LOO_preds(model, ids = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="LOO_preds_+3A_model">model</code></td>
<td>
<p><code>homGP</code> or <code>hetGP</code> model, TP version is not considered at this point</p>
</td></tr>
<tr><td><code id="LOO_preds_+3A_ids">ids</code></td>
<td>
<p>vector of indices of the unique design point considered (default to all)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list with mean and variance predictions at x_i assuming this point has not been evaluated
</p>


<h3>Note</h3>

<p>For TP models, <code>psi</code> is considered fixed.
</p>


<h3>References</h3>

<p>O. Dubrule (1983), Cross validation of Kriging in a unique neighborhood, Mathematical Geology 15, 687&ndash;699. <br /> <br />
</p>
<p>F. Bachoc (2013), Cross Validation and Maximum Likelihood estimations of hyper-parameters of Gaussian processes 
with model misspecification, Computational Statistics &amp; Data Analysis, 55&ndash;69.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(32)
## motorcycle data
library(MASS)
X &lt;- matrix(mcycle$times, ncol = 1)
Z &lt;- mcycle$accel
nvar &lt;- 1

## Model fitting
model &lt;- mleHomGP(X = X, Z = Z, lower = rep(0.1, nvar), upper = rep(10, nvar),
                  covtype = "Matern5_2", known = list(beta0 = 0))
LOO_p &lt;- LOO_preds(model)
 
# model minus observation(s) at x_i
d_mot &lt;- find_reps(X, Z)

LOO_ref &lt;- matrix(NA, nrow(d_mot$X0), 2)
for(i in 1:nrow(d_mot$X0)){
 model_i &lt;- mleHomGP(X = list(X0 = d_mot$X0[-i,, drop = FALSE], Z0 = d_mot$Z0[-i],
                     mult = d_mot$mult[-i]), Z = unlist(d_mot$Zlist[-i]),
                     lower = rep(0.1, nvar), upper = rep(50, nvar), covtype = "Matern5_2",
                     known = list(theta = model$theta, k_theta_g = model$k_theta_g, g = model$g,
                                  beta0 = 0))
 model_i$nu_hat &lt;- model$nu_hat
 p_i &lt;- predict(model_i, d_mot$X0[i,,drop = FALSE])
 LOO_ref[i,] &lt;- c(p_i$mean, p_i$sd2)
}

# Compare results

range(LOO_ref[,1] - LOO_p$mean)
range(LOO_ref[,2] - LOO_p$sd2)

# Use of LOO for diagnostics
plot(model)
</code></pre>

<hr>
<h2 id='mleCRNGP'>Gaussian process modeling with correlated noise</h2><span id='topic+mleCRNGP'></span>

<h3>Description</h3>

<p>Gaussian process regression when seed (or trajectory) information is provided, based on maximum likelihood estimation of the 
hyperparameters. Trajectory handling involves observing all times for any given seed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mleCRNGP(
  X,
  Z,
  T0 = NULL,
  stype = c("none", "XS"),
  lower = NULL,
  upper = NULL,
  known = NULL,
  noiseControl = list(g_bounds = c(sqrt(.Machine$double.eps) * 10, 100), rho_bounds =
    c(0.001, 0.9)),
  init = NULL,
  covtype = c("Gaussian", "Matern5_2", "Matern3_2"),
  maxit = 100,
  eps = sqrt(.Machine$double.eps),
  settings = list(return.Ki = TRUE, factr = 1e+07)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mleCRNGP_+3A_x">X</code></td>
<td>
<p>matrix of all designs, one per row. The last column is assumed to contain the integer seed value.</p>
</td></tr>
<tr><td><code id="mleCRNGP_+3A_z">Z</code></td>
<td>
<p>vector of all observations. If <code>ts</code> is provided, the <code>Z</code> is a matrix of size <code>nrow(X) x length(ts)</code>.</p>
</td></tr>
<tr><td><code id="mleCRNGP_+3A_t0">T0</code></td>
<td>
<p>optional vector of times (same for all <code>X</code>s)</p>
</td></tr>
<tr><td><code id="mleCRNGP_+3A_stype">stype</code></td>
<td>
<p>structural assumptions, options include:
</p>

<ul>
<li> <p><code>none</code>: no structure, regular matrix inversion is used (only when no time is present);
</p>
</li></ul>

<p>When time is present, the Kronecker structure is always used (the alternative is to provide times as an extra variable in <code>X</code>)
Using the Kronecker structure becomes efficient when the product (nx x ns) x nt becomes large.</p>
</td></tr>
<tr><td><code id="mleCRNGP_+3A_lower">lower</code>, <code id="mleCRNGP_+3A_upper">upper</code></td>
<td>
<p>optional bounds for the <code>theta</code> parameter (see <code><a href="#topic+cov_gen">cov_gen</a></code> for the exact parameterization).
In the multivariate case, it is possible to give vectors for bounds (resp. scalars) for anisotropy (resp. isotropy)</p>
</td></tr>
<tr><td><code id="mleCRNGP_+3A_known">known</code></td>
<td>
<p>optional list of known parameters, e.g., <code>beta0</code>, <code>theta</code>, <code>g</code> or <code>rho</code>.</p>
</td></tr>
<tr><td><code id="mleCRNGP_+3A_noisecontrol">noiseControl</code></td>
<td>
<p>list with element, 
</p>

<ul>
<li> <p><code>g_bounds</code>, vector providing minimal and maximal noise to signal ratio;
</p>
</li>
<li> <p><code>rho_bounds</code>, vector providing minimal and maximal correlation between seed values, in [0,1];
</p>
</li></ul>
</td></tr>
<tr><td><code id="mleCRNGP_+3A_init">init</code></td>
<td>
<p>optional list specifying starting values for MLE optimization, with elements:
</p>

<ul>
<li> <p><code>theta_init</code> initial value of the theta parameters to be optimized over (default to 10% of the range determined with <code>lower</code> and <code>upper</code>)
</p>
</li>
<li> <p><code>g_init</code> initial value of the nugget parameter to be optimized over.
</p>
</li>
<li> <p><code>rho_init</code> initial value of the seed correlation parameter.
</p>
</li></ul>
</td></tr>
<tr><td><code id="mleCRNGP_+3A_covtype">covtype</code></td>
<td>
<p>covariance kernel type, either 'Gaussian', 'Matern5_2' or 'Matern3_2', see <code><a href="#topic+cov_gen">cov_gen</a></code></p>
</td></tr>
<tr><td><code id="mleCRNGP_+3A_maxit">maxit</code></td>
<td>
<p>maximum number of iteration for L-BFGS-B of <code><a href="stats.html#topic+optim">optim</a></code></p>
</td></tr>
<tr><td><code id="mleCRNGP_+3A_eps">eps</code></td>
<td>
<p>jitter used in the inversion of the covariance matrix for numerical stability</p>
</td></tr>
<tr><td><code id="mleCRNGP_+3A_settings">settings</code></td>
<td>
<p>list with argument <code>return.Ki</code>, to include the inverse covariance matrix in the object for further use (e.g., prediction).
Arguments <code>factr</code> (default to 1e9) and <code>pgtol</code> are available to be passed to <code>control</code> for L-BFGS-B in <code><a href="stats.html#topic+optim">optim</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The global covariance matrix of the model is parameterized as <code>nu_hat * (Cx + g Id) * Cs = nu_hat * K</code>,
with <code>Cx</code> the spatial correlation matrix between unique designs, depending on the family of kernel used (see <code><a href="#topic+cov_gen">cov_gen</a></code> for available choices) and values of lengthscale parameters.
<code>Cs</code> is the correlation matrix between seed values, equal to 1 if the seeds are equal, <code>rho</code> otherwise.
<code>nu_hat</code> is the plugin estimator of the variance of the process.
</p>
<p>Compared to <code><a href="#topic+mleHomGP">mleHomGP</a></code>, here the replications have a specific identifier, i.e., the seed.
</p>


<h3>Value</h3>

<p>a list which is given the S3 class &quot;<code>CRNGP</code>&quot;, with elements:
</p>

<ul>
<li> <p><code>theta</code>: maximum likelihood estimate of the lengthscale parameter(s),
</p>
</li>
<li> <p><code>g</code>: maximum likelihood estimate of the nugget variance,
</p>
</li>
<li> <p><code>rho</code>: maximum likelihood estimate of the seed correlation parameter,
</p>
</li>
<li> <p><code>trendtype</code>: either &quot;<code>SK</code>&quot; if <code>beta0</code> is given, else &quot;<code>OK</code>&quot; 
</p>
</li>
<li> <p><code>beta0</code>: estimated trend unless given in input,
</p>
</li>
<li> <p><code>nu_hat</code>: plugin estimator of the variance,
</p>
</li>
<li> <p><code>ll</code>: log-likelihood value,
</p>
</li>
<li> <p><code>X0</code>, <code>S0</code>, <code>T0</code>: values for the spatial, seed and time designs 
</p>
</li>
<li> <p><code>Z</code>, <code>eps</code>, <code>covtype</code>, <code>stype</code>,: values given in input,
</p>
</li>
<li> <p><code>call</code>: user call of the function
</p>
</li>
<li> <p><code>used_args</code>: list with arguments provided in the call
</p>
</li>
<li> <p><code>nit_opt</code>, <code>msg</code>: <code>counts</code> and <code>msg</code> returned by <code><a href="stats.html#topic+optim">optim</a></code>
</p>
</li>
<li> <p><code>Ki</code>: inverse covariance matrix (not scaled by <code>nu_hat</code>) (if <code>return.Ki</code> is <code>TRUE</code> in <code>settings</code>)
</p>
</li>
<li> <p><code>Ct</code>: if time is used, corresponding covariance matrix.
</p>
</li>
<li> <p><code>time</code>: time to train the model, in seconds.
</p>
</li></ul>



<h3>Note</h3>

<p>This function is experimental at this time and could evolve in the future.
</p>


<h3>References</h3>

<p>Xi Chen, Bruce E Ankenman, and Barry L Nelson. The effects of common random numbers on stochastic kriging metamodels. ACM Transactions on Modeling and Computer Simulation (TOMACS), 22(2):1-20, 2012.<br /> <br />
</p>
<p>Michael Pearce, Matthias Poloczek, and Juergen Branke. Bayesian simulation optimization with common random numbers. In 2019 Winter Simulation Conference (WSC), pages 3492-3503. IEEE, 2019. <br /> <br />
</p>
<p>A Fadikar, M Binois, N Collier, A Stevens, KB Toh, J Ozik. Trajectory-oriented optimization of stochastic epidemiological models. arXiv preprint arXiv:2305.03926
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.CRNGP">predict.CRNGP</a></code> for predictions, <code><a href="#topic+simul.CRNGP">simul.CRNGP</a></code> for generating conditional simulation on a Kronecker grid.
<code>summary</code> and <code>plot</code> functions are available as well.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>##------------------------------------------------------------
## Example 1: CRN GP modeling on 1d sims
##------------------------------------------------------------
#' set.seed(42)
nx &lt;- 50
ns &lt;- 5
x &lt;- matrix(seq(0,1, length.out = nx), nx)
s &lt;- matrix(seq(1, ns, length.out = ns))
g &lt;- 1e-3
theta &lt;- 0.01
KX &lt;- cov_gen(x, theta = theta)
rho &lt;- 0.3
KS &lt;- matrix(rho, ns, ns)
diag(KS) &lt;- 1
YY &lt;- MASS::mvrnorm(n = 1, mu = rep(0, nx*ns), Sigma = kronecker(KX, KS) + g * diag(nx*ns))
YYmat &lt;- matrix(YY, ns, nx)
matplot(x, t(YYmat), pch = 1, type = "b", lty = 3)

Xgrid &lt;- as.matrix(expand.grid(s, x))
Xgrid &lt;- cbind(Xgrid[,2], Xgrid[,1])
ids &lt;- sample(1:nrow(Xgrid), 20)
X0 &lt;- Xgrid[ids,]
Y0 &lt;-  YY[ids]
points(X0[,1], Y0, pch = 20, col = 1 + ((X0[,2] - 1) %% 6))

model &lt;- mleCRNGP(X0, Y0, known = list(theta = 0.01, g = 1e-3, rho = 0.3))

preds &lt;- predict(model, x = Xgrid, xprime = Xgrid)
matlines(x, t(matrix(preds$mean, ns, nx)), lty = 1)
# prediction on new seed (i.e., average prediction)
xs1 &lt;- cbind(x, ns+1)
predsm &lt;- predict(model, x = xs1)
lines(x, predsm$mean, col = "orange", lwd = 3)
lines(x, predsm$mean + 2 * sqrt(predsm$sd2), col = "orange", lwd = 2, lty = 3)
lines(x, predsm$mean - 2 * sqrt(predsm$sd2), col = "orange", lwd = 2, lty = 3)

# Conditional realizations
sims &lt;- MASS::mvrnorm(n = 1, mu = preds$mean, Sigma = 1/2 * (preds$cov + t(preds$cov)))
plot(Xgrid[,1], sims, col = 1 + ((Xgrid[,2] - 1) %% 6))
points(X0[,1], Y0, pch = 20, col = 1 + ((X0[,2] - 1) %% 6))
## Not run: 
##------------------------------------------------------------
## Example 2: Homoskedastic GP modeling on 2d sims
##------------------------------------------------------------
set.seed(2)
nx &lt;- 31
ns &lt;- 5
d &lt;- 2
x &lt;- as.matrix(expand.grid(seq(0,1, length.out = nx), seq(0,1, length.out = nx)))
s &lt;- matrix(seq(1, ns, length.out = ns))
Xgrid &lt;- as.matrix(expand.grid(seq(1, ns, length.out = ns), seq(0,1, length.out = nx), 
                               seq(0,1, length.out = nx)))
Xgrid &lt;- Xgrid[,c(2, 3, 1)]
g &lt;- 1e-3
theta &lt;- c(0.02, 0.05)
KX &lt;- cov_gen(x, theta = theta)
rho &lt;- 0.33
KS &lt;- matrix(rho, ns, ns)
diag(KS) &lt;- 1
YY &lt;- MASS::mvrnorm(n = 1, mu = rep(0, nx*nx*ns), Sigma = kronecker(KX, KS) + g * diag(nx*nx*ns))
YYmat &lt;- matrix(YY, ns, nx*nx)
filled.contour(matrix(YYmat[1,], nx))
filled.contour(matrix(YYmat[2,], nx))

ids &lt;- sample(1:nrow(Xgrid), 80)
X0 &lt;- Xgrid[ids,]
Y0 &lt;-  YY[ids]

## Uncomment below for For 3D visualisation
# library(rgl)
# plot3d(Xgrid[,1], Xgrid[,2], YY, col = 1 + (Xgrid[,3] - 1) %% 6)
# points3d(X0[,1], X0[,2], Y0, size = 10, col = 1 + ((X0[,3] - 1) %% 6))

model &lt;- mleCRNGP(X0, Y0, know = list(beta0 = 0))

preds &lt;- predict(model, x = Xgrid, xprime = Xgrid)
# surface3d(unique(Xgrid[1:nx^2,1]),unique(Xgrid[,2]), matrix(YY[Xgrid[,3]==1], nx), 
#   front = "lines", back = "lines")
# aspect3d(1, 1, 1)
# surface3d(unique(Xgrid[1:nx^2,1]),unique(Xgrid[,2]), matrix(preds$mean[Xgrid[,3]==1], nx), 
#   front = "lines", back = "lines", col = "red")
plot(preds$mean, YY)

# prediction on new seed (i.e., average prediction)
xs1 &lt;- cbind(x, ns+1)
predsm &lt;- predict(model, x = xs1)
# surface3d(unique(x[,1]), unique(x[,2]), matrix(predsm$mean, nx), col = "orange", 
#   front = "lines", back = "lines")

# Conditional realizations
sims &lt;- MASS::mvrnorm(n = 1, mu = preds$mean, Sigma = 1/2 * (preds$cov + t(preds$cov)))
# plot3d(X0[,1], X0[,2], Y0, size = 10, col = 1 + ((X0[,3] - 1) %% 6))
# surface3d(unique(x[,1]), unique(x[,2]), matrix(sims[Xgrid[,3] == 1], nx), col = 1, 
#   front = "lines", back = "lines")
# surface3d(unique(x[,1]), unique(x[,2]), matrix(sims[Xgrid[,3] == 2], nx), col = 2, 
#   front = "lines", back = "lines")

# Faster alternative for conditional realizations 
# (note: here the design points are part of the simulation points)
Xgrid0 &lt;- unique(Xgrid[, -(d + 1), drop = FALSE])
sims2 &lt;- simul(object = model,Xgrid = Xgrid, ids = ids, nsim = 5, check = TRUE) 

##------------------------------------------------------------
## Example 3: Homoskedastic GP modeling on 1d trajectories (with time)
##------------------------------------------------------------
set.seed(42)
nx &lt;- 11
nt &lt;- 9
ns &lt;- 7
x &lt;- matrix(sort(seq(0,1, length.out = nx)), nx)
s &lt;- matrix(sort(seq(1, ns, length.out = ns)))
t &lt;- matrix(sort(seq(0, 1, length.out = nt)), nt)
covtype &lt;- "Matern5_2"
g &lt;- 1e-3
theta &lt;- c(0.3, 0.5)
KX &lt;- cov_gen(x, theta = theta[1], type = covtype)
KT &lt;- cov_gen(t, theta = theta[2], type = covtype)
rho &lt;- 0.3
KS &lt;- matrix(rho, ns, ns)
diag(KS) &lt;- 1
XST &lt;- as.matrix(expand.grid(x, s, t))

Kmc &lt;- kronecker(chol(KT), kronecker(chol(KS), chol(KX)))
YY &lt;- t(Kmc) %*% rnorm(nrow(Kmc))

ninit &lt;- 50
XS &lt;- as.matrix(expand.grid(x, s))
ids &lt;- sort(sample(1:nrow(XS), ninit))
XST0 &lt;- cbind(XS[ids[rep(1:ninit, each = nt)],], rep(t[,1], times = ninit))
X0 &lt;- XST[which(duplicated(rbind(XST, XST0), fromLast = TRUE)),]
Y0 &lt;-  YY[which(duplicated(rbind(XST, XST0), fromLast = TRUE))]

# tmp &lt;- hetGP:::find_reps(X = X0[,-3], Y0)
model &lt;- mleCRNGP(X = XS[ids,], T0=t, Z = matrix(Y0, ncol = nt), covtype = covtype)

preds &lt;- predict(model, x = XS, xprime = XS)

# compare with regular CRN GP
mref &lt;- mleCRNGP(X = X0[, c(1, 3, 2)], Z = Y0, covtype = covtype)
pref &lt;- predict(mref, x = XST[, c(1, 3, 2)], xprime = XST[, c(1, 3, 2)])

print(model$time) # Use Kronecker structure for time
print(mref$time)

plot(as.vector(preds$mean), YY)
plot(pref$mean, YY) 


## End(Not run)
</code></pre>

<hr>
<h2 id='mleHetGP'>Gaussian process modeling with heteroskedastic noise</h2><span id='topic+mleHetGP'></span>

<h3>Description</h3>

<p>Gaussian process regression under input dependent noise based on maximum likelihood estimation of the hyperparameters. 
A second GP is used to model latent (log-) variances. 
This function is enhanced to deal with replicated observations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mleHetGP(
  X,
  Z,
  lower = NULL,
  upper = NULL,
  noiseControl = list(k_theta_g_bounds = c(1, 100), g_max = 100, g_bounds = c(1e-06, 1)),
  settings = list(linkThetas = "joint", logN = TRUE, initStrategy = "residuals", checkHom
    = TRUE, penalty = TRUE, trace = 0, return.matrices = TRUE, return.hom = FALSE, factr
    = 1e+09),
  covtype = c("Gaussian", "Matern5_2", "Matern3_2"),
  maxit = 100,
  known = NULL,
  init = NULL,
  eps = sqrt(.Machine$double.eps)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mleHetGP_+3A_x">X</code></td>
<td>
<p>matrix of all designs, one per row, or list with elements:
</p>

<ul>
<li> <p><code>X0</code> matrix of unique design locations, one point per row
</p>
</li>
<li> <p><code>Z0</code> vector of averaged observations, of length <code>nrow(X0)</code>
</p>
</li>
<li> <p><code>mult</code> number of replicates at designs in <code>X0</code>, of length <code>nrow(X0)</code>
</p>
</li></ul>
</td></tr>
<tr><td><code id="mleHetGP_+3A_z">Z</code></td>
<td>
<p>vector of all observations. If using a list with <code>X</code>, <code>Z</code> has to be ordered with respect to <code>X0</code>, and of length <code>sum(mult)</code></p>
</td></tr>
<tr><td><code id="mleHetGP_+3A_lower">lower</code>, <code id="mleHetGP_+3A_upper">upper</code></td>
<td>
<p>optional bounds for the <code>theta</code> parameter (see <code><a href="#topic+cov_gen">cov_gen</a></code> for the exact parameterization).
In the multivariate case, it is possible to give vectors for bounds (resp. scalars) for anisotropy (resp. isotropy)</p>
</td></tr>
<tr><td><code id="mleHetGP_+3A_noisecontrol">noiseControl</code></td>
<td>
<p>list with elements related to optimization of the noise process parameters:
</p>

<ul>
<li> <p><code>g_min</code>, <code>g_max</code> minimal and maximal noise to signal ratio (of the mean process)
</p>
</li>
<li> <p><code>lowerDelta</code>, <code>upperDelta</code> optional vectors (or scalars) of bounds on <code>Delta</code>, of length <code>nrow(X0)</code> (default to <code>rep(eps, nrow(X0))</code> and <code>rep(noiseControl$g_max, nrow(X0))</code> resp., or their <code>log</code>) 
</p>
</li>
<li> <p><code>lowerTheta_g</code>, <code>upperTheta_g</code> optional vectors of bounds for the lengthscales of the noise process if <code>linkThetas == 'none'</code>.
Same as for <code>theta</code> if not provided.
</p>
</li>
<li> <p><code>k_theta_g_bounds</code> if <code>linkThetas == 'joint'</code>, vector with minimal and maximal values for <code>k_theta_g</code> (default to <code>c(1, 100)</code>). See Details.
</p>
</li>
<li> <p><code>g_bounds</code> vector for minimal and maximal noise to signal ratios for the noise of the noise process, i.e., the smoothing parameter for the noise process.
(default to <code>c(1e-6, 1)</code>).
</p>
</li></ul>
</td></tr>
<tr><td><code id="mleHetGP_+3A_settings">settings</code></td>
<td>
<p>list for options about the general modeling procedure, with elements:
</p>

<ul>
<li> <p><code>linkThetas</code> defines the relation between lengthscales of the mean and noise processes.
Either <code>'none'</code>, <code>'joint'</code>(default) or <code>'constr'</code>, see Details.
</p>
</li>
<li> <p><code>logN</code>, when <code>TRUE</code> (default), the log-noise process is modeled.
</p>
</li>
<li> <p><code>initStrategy</code> one of <code>'simple'</code>, <code>'residuals'</code> (default) and <code>'smoothed'</code> to obtain starting values for <code>Delta</code>, see Details
</p>
</li>
<li> <p><code>penalty</code> when <code>TRUE</code>, the penalized version of the likelihood is used (i.e., the sum of the log-likelihoods of the mean and variance processes, see References).
</p>
</li>
<li> <p><code>checkHom</code> when <code>TRUE</code>, if the log-likelihood with a homoskedastic model is better, then return it.
</p>
</li>
<li> <p><code>trace</code> optional scalar (default to <code>0</code>). If positive, tracing information on the fitting process.
If <code>1</code>, information is given about the result of the heterogeneous model optimization.
Level <code>2</code> gives more details. Level 3 additionaly displays all details about initialization of hyperparameters.
</p>
</li>
<li> <p><code>return.matrices</code> boolean to include the inverse covariance matrix in the object for further use (e.g., prediction).
</p>
</li>
<li> <p><code>return.hom</code> boolean to include homoskedastic GP models used for initialization (i.e., <code>modHom</code> and <code>modNugs</code>).
</p>
</li>
<li> <p><code>factr</code> (default to 1e9) and <code>pgtol</code> are available to be passed to <code>control</code> for L-BFGS-B in <code><a href="stats.html#topic+optim">optim</a></code>.   
</p>
</li></ul>
</td></tr>
<tr><td><code id="mleHetGP_+3A_covtype">covtype</code></td>
<td>
<p>covariance kernel type, either <code>'Gaussian'</code>, <code>'Matern5_2'</code> or <code>'Matern3_2'</code>, see <code><a href="#topic+cov_gen">cov_gen</a></code></p>
</td></tr>
<tr><td><code id="mleHetGP_+3A_maxit">maxit</code></td>
<td>
<p>maximum number of iterations for <code>L-BFGS-B</code> of <code><a href="stats.html#topic+optim">optim</a></code> dedicated to maximum likelihood optimization</p>
</td></tr>
<tr><td><code id="mleHetGP_+3A_init">init</code>, <code id="mleHetGP_+3A_known">known</code></td>
<td>
<p>optional lists of starting values for mle optimization or that should not be optimized over, respectively.
Values in <code>known</code> are not modified, while it can happen to these of <code>init</code>, see Details. 
One can set one or several of the following:
</p>

<ul>
<li> <p><code>theta</code> lengthscale parameter(s) for the mean process either one value (isotropic) or a vector (anistropic)
</p>
</li>
<li> <p><code>Delta</code> vector of nuggets corresponding to each design in <code>X0</code>, that are smoothed to give <code>Lambda</code>
(as the global covariance matrix depend on <code>Delta</code> and <code>nu_hat</code>, it is recommended to also pass values for <code>theta</code>)
</p>
</li>
<li> <p><code>beta0</code> constant trend of the mean process
</p>
</li>
<li> <p><code>k_theta_g</code> constant used for link mean and noise processes lengthscales, when <code>settings$linkThetas == 'joint'</code>
</p>
</li>
<li> <p><code>theta_g</code> either one value (isotropic) or a vector (anistropic) for lengthscale parameter(s) of the noise process, when <code>settings$linkThetas != 'joint'</code>
</p>
</li>
<li> <p><code>g</code> scalar nugget of the noise process
</p>
</li>
<li> <p><code>g_H</code> scalar homoskedastic nugget for the initialisation with a <code><a href="#topic+mleHomGP">mleHomGP</a></code>. See Details.
</p>
</li></ul>
</td></tr>
<tr><td><code id="mleHetGP_+3A_eps">eps</code></td>
<td>
<p>jitter used in the inversion of the covariance matrix for numerical stability</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The global covariance matrix of the model is parameterized as <code>nu_hat * (C + Lambda * diag(1/mult)) = nu_hat * K</code>,
with <code>C</code> the correlation matrix between unique designs, depending on the family of kernel used (see <code><a href="#topic+cov_gen">cov_gen</a></code> for available choices) and values of lengthscale parameters.
<code>nu_hat</code> is the plugin estimator of the variance of the process.
<code>Lambda</code> is the prediction on the noise level given by a second (homoskedastic) GP: <br />
</p>
<p style="text-align: center;"><code class="reqn">\Lambda = C_g(C_g + \mathrm{diag}(g/\mathrm{mult}))^{-1} \Delta</code>
</p>
 <p><br />
with <code>C_g</code> the correlation matrix between unique designs for this second GP, with lengthscales hyperparameters <code>theta_g</code> and nugget <code>g</code>
and <code>Delta</code> the variance level at <code>X0</code> that are estimated.
</p>
<p>It is generally recommended to use <code><a href="#topic+find_reps">find_reps</a></code> to pre-process the data, to rescale the inputs to the unit cube and to normalize the outputs.
</p>
<p>The noise process lengthscales can be set in several ways:
</p>

<ul>
<li><p> using <code>k_theta_g</code> (<code>settings$linkThetas == 'joint'</code>), supposed to be greater than one by default. 
In this case lengthscales of the noise process are multiples of those of the mean process.
</p>
</li>
<li><p> if <code>settings$linkThetas == 'constr'</code>, then the lower bound on <code>theta_g</code> correspond to estimated values of an homoskedastic GP fit.
</p>
</li>
<li><p> else lengthscales between the mean and noise process are independent (both either anisotropic or not).
</p>
</li></ul>

<p>When no starting nor fixed parameter values are provided with <code>init</code> or <code>known</code>, 
the initialization process consists of fitting first an homoskedastic model of the data, called <code>modHom</code>.
Unless provided with <code>init$theta</code>, initial lengthscales are taken at 10% of the range determined with <code>lower</code> and <code>upper</code>,
while <code>init$g_H</code> may be use to pass an initial nugget value.
The resulting lengthscales provide initial values for <code>theta</code> (or update them if given in <code>init</code>). <br /> <br />
If necessary, a second homoskedastic model, <code>modNugs</code>, is fitted to the empirical residual variance between the prediction
given by <code>modHom</code> at <code>X0</code> and <code>Z</code> (up to <code>modHom$nu_hat</code>).
Note that when specifying <code>settings$linkThetas == 'joint'</code>, then this second homoskedastic model has fixed lengthscale parameters.
Starting values for <code>theta_g</code> and <code>g</code> are extracted from <code>modNugs</code>.<br /> <br />
Finally, three initialization schemes for <code>Delta</code> are available with <code>settings$initStrategy</code>: 
</p>

<ul>
<li><p> for <code>settings$initStrategy == 'simple'</code>, <code>Delta</code> is simply initialized to the estimated <code>g</code> value of <code>modHom</code>. 
Note that this procedure may fail when <code>settings$penalty == TRUE</code>.
</p>
</li>
<li><p> for <code>settings$initStrategy == 'residuals'</code>, <code>Delta</code> is initialized to the estimated residual variance from the homoskedastic mean prediction.
</p>
</li>
<li><p> for <code>settings$initStrategy == 'smoothed'</code>, <code>Delta</code> takes the values predicted by <code>modNugs</code> at <code>X0</code>.
</p>
</li></ul>

<p>Notice that <code>lower</code> and <code>upper</code> bounds cannot be equal for <code><a href="stats.html#topic+optim">optim</a></code>.
</p>


<h3>Value</h3>

<p>a list which is given the S3 class <code>"hetGP"</code>, with elements:
</p>

<ul>
<li> <p><code>theta</code>: unless given, maximum likelihood estimate (mle) of the lengthscale parameter(s),
</p>
</li>
<li> <p><code>Delta</code>: unless given, mle of the nugget vector (non-smoothed),
</p>
</li>
<li> <p><code>Lambda</code>: predicted input noise variance at <code>X0</code>, 
</p>
</li>
<li> <p><code>nu_hat</code>: plugin estimator of the variance,
</p>
</li>
<li> <p><code>theta_g</code>: unless given, mle of the lengthscale(s) of the noise/log-noise process,
</p>
</li>
<li> <p><code>k_theta_g</code>: if <code>settings$linkThetas == 'joint'</code>, mle for the constant by which lengthscale parameters of <code>theta</code> are multiplied to get <code>theta_g</code>,
</p>
</li>
<li> <p><code>g</code>: unless given, mle of the nugget of the noise/log-noise process,
</p>
</li>
<li> <p><code>trendtype</code>: either &quot;<code>SK</code>&quot; if <code>beta0</code> is provided, else &quot;<code>OK</code>&quot;,
</p>
</li>
<li> <p><code>beta0</code> constant trend of the mean process, plugin-estimator unless given,
</p>
</li>
<li> <p><code>nmean</code>: plugin estimator for the constant noise/log-noise process mean,
</p>
</li>
<li> <p><code>ll</code>: log-likelihood value, (<code>ll_non_pen</code>) is the value without the penalty,
</p>
</li>
<li> <p><code>nit_opt</code>, <code>msg</code>: <code>counts</code> and <code>message</code> returned by <code><a href="stats.html#topic+optim">optim</a></code>
</p>
</li>
<li> <p><code>modHom</code>: homoskedastic GP model of class <code>homGP</code> used for initialization of the mean process,
</p>
</li>
<li> <p><code>modNugs</code>: homoskedastic GP model of class <code>homGP</code> used for initialization of the noise/log-noise process,
</p>
</li>
<li> <p><code>nu_hat_var</code>: variance of the noise process,
</p>
</li>
<li> <p><code>used_args</code>: list with arguments provided in the call to the function, which is saved in <code>call</code>,
</p>
</li>
<li> <p><code>Ki</code>, <code>Kgi</code>: inverse of the covariance matrices of the mean and noise processes (not scaled by <code>nu_hat</code> and <code>nu_hat_var</code>),  
</p>
</li>
<li> <p><code>X0</code>, <code>Z0</code>, <code>Z</code>, <code>eps</code>, <code>logN</code>, <code>covtype</code>: values given in input,
</p>
</li>
<li> <p><code>time</code>: time to train the model, in seconds.
</p>
</li></ul>



<h3>References</h3>

<p>M. Binois, Robert B. Gramacy, M. Ludkovski (2018), Practical heteroskedastic Gaussian process modeling for large simulation experiments,
Journal of Computational and Graphical Statistics, 27(4), 808&ndash;821.<br /> 
Preprint available on arXiv:1611.05902. <br /> <br />
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.hetGP">predict.hetGP</a></code> for predictions, <code><a href="#topic+update.hetGP">update.hetGP</a></code> for updating an existing model.
<code>summary</code> and <code>plot</code> functions are available as well.
<code><a href="#topic+mleHetTP">mleHetTP</a></code> provide a Student-t equivalent.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>##------------------------------------------------------------
## Example 1: Heteroskedastic GP modeling on the motorcycle data
##------------------------------------------------------------
set.seed(32)

## motorcycle data
library(MASS)
X &lt;- matrix(mcycle$times, ncol = 1)
Z &lt;- mcycle$accel
nvar &lt;- 1
plot(X, Z, ylim = c(-160, 90), ylab = 'acceleration', xlab = "time")


## Model fitting
model &lt;- mleHetGP(X = X, Z = Z, lower = rep(0.1, nvar), upper = rep(50, nvar),
                  covtype = "Matern5_2")
            
## Display averaged observations
points(model$X0, model$Z0, pch = 20)

## A quick view of the fit                  
summary(model)

## Create a prediction grid and obtain predictions
xgrid &lt;- matrix(seq(0, 60, length.out = 301), ncol = 1) 
predictions &lt;- predict(x = xgrid, object =  model)

## Display mean predictive surface
lines(xgrid, predictions$mean, col = 'red', lwd = 2)
## Display 95% confidence intervals
lines(xgrid, qnorm(0.05, predictions$mean, sqrt(predictions$sd2)), col = 2, lty = 2)
lines(xgrid, qnorm(0.95, predictions$mean, sqrt(predictions$sd2)), col = 2, lty = 2)
## Display 95% prediction intervals
lines(xgrid, qnorm(0.05, predictions$mean, sqrt(predictions$sd2 + predictions$nugs)), 
  col = 3, lty = 2)
lines(xgrid, qnorm(0.95, predictions$mean, sqrt(predictions$sd2 + predictions$nugs)), 
  col = 3, lty = 2)

##------------------------------------------------------------
## Example 2: 2D Heteroskedastic GP modeling
##------------------------------------------------------------
set.seed(1)
nvar &lt;- 2
  
## Branin redefined in [0,1]^2
branin &lt;- function(x){
  if(is.null(nrow(x)))
    x &lt;- matrix(x, nrow = 1)
    x1 &lt;- x[,1] * 15 - 5
    x2 &lt;- x[,2] * 15
    (x2 - 5/(4 * pi^2) * (x1^2) + 5/pi * x1 - 6)^2 + 10 * (1 - 1/(8 * pi)) * cos(x1) + 10
}

## Noise field via standard deviation
noiseFun &lt;- function(x){
  if(is.null(nrow(x)))
    x &lt;- matrix(x, nrow = 1)
  return(1/5*(3*(2 + 2*sin(x[,1]*pi)*cos(x[,2]*3*pi) + 5*rowSums(x^2))))
}

## data generating function combining mean and noise fields
ftest &lt;- function(x){
  return(branin(x) + rnorm(nrow(x), mean = 0, sd = noiseFun(x)))
}

## Grid of predictive locations
ngrid &lt;- 51
xgrid &lt;- matrix(seq(0, 1, length.out = ngrid), ncol = 1) 
Xgrid &lt;- as.matrix(expand.grid(xgrid, xgrid))

## Unique (randomly chosen) design locations
n &lt;- 50
Xu &lt;- matrix(runif(n * 2), n)

## Select replication sites randomly
X &lt;- Xu[sample(1:n, 20*n, replace = TRUE),]

## obtain training data response at design locations X
Z &lt;- ftest(X)

## Formating of data for model creation (find replicated observations) 
prdata &lt;- find_reps(X, Z, rescale = FALSE, normalize = FALSE)

## Model fitting
model &lt;- mleHetGP(X = list(X0 = prdata$X0, Z0 = prdata$Z0, mult = prdata$mult), Z = prdata$Z,
                  lower = rep(0.01, nvar), upper = rep(10, nvar),
                  covtype = "Matern5_2")

## a quick view into the data stored in the "hetGP"-class object
summary(model)                  
             
## prediction from the fit on the grid     
predictions &lt;- predict(x = Xgrid, object =  model)

## Visualization of the predictive surface
par(mfrow = c(2, 2))
contour(x = xgrid,  y = xgrid, z = matrix(branin(Xgrid), ngrid), 
  main = "Branin function", nlevels = 20)
points(X, col = 'blue', pch = 20)
contour(x = xgrid,  y = xgrid, z = matrix(predictions$mean, ngrid), 
  main = "Predicted mean", nlevels = 20)
points(Xu, col = 'blue', pch = 20)
contour(x = xgrid,  y = xgrid, z = matrix(noiseFun(Xgrid), ngrid), 
  main = "Noise standard deviation function", nlevels = 20)
points(Xu, col = 'blue', pch = 20)
contour(x = xgrid,  y= xgrid, z = matrix(sqrt(predictions$nugs), ngrid), 
  main = "Predicted noise values", nlevels = 20)
points(Xu, col = 'blue', pch = 20)
par(mfrow = c(1, 1))
</code></pre>

<hr>
<h2 id='mleHetTP'>Student-t process modeling with heteroskedastic noise</h2><span id='topic+mleHetTP'></span>

<h3>Description</h3>

<p>Student-t process regression under input dependent noise based on maximum likelihood estimation of the hyperparameters.
A GP is used to model latent (log-) variances.
This function is enhanced to deal with replicated observations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mleHetTP(
  X,
  Z,
  lower = NULL,
  upper = NULL,
  noiseControl = list(k_theta_g_bounds = c(1, 100), g_max = 10000, g_bounds = c(1e-06,
    0.1), nu_bounds = c(2 + 0.001, 30), sigma2_bounds = c(sqrt(.Machine$double.eps),
    10000)),
  settings = list(linkThetas = "joint", logN = TRUE, initStrategy = "residuals", checkHom
    = TRUE, penalty = TRUE, trace = 0, return.matrices = TRUE, return.hom = FALSE, factr
    = 1e+09),
  covtype = c("Gaussian", "Matern5_2", "Matern3_2"),
  maxit = 100,
  known = list(beta0 = 0),
  init = list(nu = 3),
  eps = sqrt(.Machine$double.eps)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mleHetTP_+3A_x">X</code></td>
<td>
<p>matrix of all designs, one per row, or list with elements:
</p>

<ul>
<li> <p><code>X0</code> matrix of unique design locations, one point per row
</p>
</li>
<li> <p><code>Z0</code> vector of averaged observations, of length <code>nrow(X0)</code>
</p>
</li>
<li> <p><code>mult</code> number of replicates at designs in <code>X0</code>, of length <code>nrow(X0)</code>
</p>
</li></ul>
</td></tr>
<tr><td><code id="mleHetTP_+3A_z">Z</code></td>
<td>
<p>vector of all observations. If using a list with <code>X</code>, <code>Z</code> has to be ordered with respect to <code>X0</code>, and of length <code>sum(mult)</code></p>
</td></tr>
<tr><td><code id="mleHetTP_+3A_lower">lower</code>, <code id="mleHetTP_+3A_upper">upper</code></td>
<td>
<p>bounds for the <code>theta</code> parameter (see <code><a href="#topic+cov_gen">cov_gen</a></code> for the exact parameterization).
In the multivariate case, it is possible to give vectors for bounds (resp. scalars) for anisotropy (resp. isotropy)</p>
</td></tr>
<tr><td><code id="mleHetTP_+3A_noisecontrol">noiseControl</code></td>
<td>
<p>list with elements related to optimization of the noise process parameters:
</p>

<ul>
<li> <p><code>g_min</code>, <code>g_max</code> minimal and maximal noise to signal ratio (of the mean process)
</p>
</li>
<li> <p><code>lowerDelta</code>, <code>upperDelta</code> optional vectors (or scalars) of bounds on <code>Delta</code>, of length <code>nrow(X0)</code> (default to <code>rep(eps, nrow(X0))</code> and <code>rep(noiseControl$g_max, nrow(X0))</code> resp., or their <code>log</code>) 
</p>
</li>
<li> <p><code>lowerTheta_g</code>, <code>upperTheta_g</code> optional vectors of bounds for the lengthscales of the noise process if <code>linkThetas == 'none'</code>.
Same as for <code>theta</code> if not provided.
</p>
</li>
<li> <p><code>k_theta_g_bounds</code> if <code>linkThetas == 'joint'</code>, vector with minimal and maximal values for <code>k_theta_g</code> (default to <code>c(1, 100)</code>). See Details.
</p>
</li>
<li> <p><code>g_bounds</code> vector for minimal and maximal noise to signal ratios for the noise of the noise process, i.e., the smoothing parameter for the noise process.
(default to <code>c(1e-6, 1)</code>).
</p>
</li>
<li> <p><code>sigma2_bounds</code>, vector providing minimal and maximal signal variance.
</p>
</li>
<li> <p><code>nu_bounds</code>, vector providing minimal and maximal values for the degrees of freedom. 
</p>
</li></ul>
</td></tr>
<tr><td><code id="mleHetTP_+3A_settings">settings</code></td>
<td>
<p>list for options about the general modeling procedure, with elements:
</p>

<ul>
<li> <p><code>linkThetas</code> defines the relation between lengthscales of the mean and noise processes.
Either <code>'none'</code>, <code>'joint'</code>(default) or <code>'constr'</code>, see Details.
</p>
</li>
<li> <p><code>logN</code>, when <code>TRUE</code> (default), the log-noise process is modeled.
</p>
</li>
<li> <p><code>initStrategy</code> one of <code>'simple'</code>, <code>'residuals'</code> (default) and <code>'smoothed'</code> to obtain starting values for <code>Delta</code>, see Details
</p>
</li>
<li> <p><code>penalty</code> when <code>TRUE</code>, the penalized version of the likelihood is used (i.e., the sum of the log-likelihoods of the mean and variance processes, see References).
</p>
</li>
<li> <p><code>checkHom</code> when <code>TRUE</code>, if the log-likelihood with a homoskedastic model is better, then return it.
</p>
</li>
<li> <p><code>trace</code> optional scalar (default to <code>0</code>). If positive, tracing information on the fitting process.
If <code>1</code>, information is given about the result of the heterogeneous model optimization.
Level <code>2</code> gives more details. Level 3 additionaly displays all details about initialization of hyperparameters.
</p>
</li>
<li> <p><code>return.matrices</code> boolean too include the inverse covariance matrix in the object for further use (e.g., prediction).
</p>
</li>
<li><p> Arguments <code>factr</code> (default to 1e9) and <code>pgtol</code> are available to be passed to <code>control</code> for L-BFGS-B in <code><a href="stats.html#topic+optim">optim</a></code>.   
</p>
</li></ul>
</td></tr>
<tr><td><code id="mleHetTP_+3A_covtype">covtype</code></td>
<td>
<p>covariance kernel type, either <code>'Gaussian'</code>, <code>'Matern5_2'</code> or <code>'Matern3_2'</code>, see <code><a href="#topic+cov_gen">cov_gen</a></code></p>
</td></tr>
<tr><td><code id="mleHetTP_+3A_maxit">maxit</code></td>
<td>
<p>maximum number of iterations for <code>L-BFGS-B</code> of <code><a href="stats.html#topic+optim">optim</a></code> dedicated to maximum likelihood optimization</p>
</td></tr>
<tr><td><code id="mleHetTP_+3A_init">init</code>, <code id="mleHetTP_+3A_known">known</code></td>
<td>
<p>optional lists of starting values for mle optimization or that should not be optimized over, respectively.
Values in <code>known</code> are not modified, while it can happen to those of <code>init</code>, see Details. 
One can set one or several of the following:
</p>

<ul>
<li> <p><code>theta</code> lengthscale parameter(s) for the mean process either one value (isotropic) or a vector (anistropic)
</p>
</li>
<li> <p><code>Delta</code> vector of nuggets corresponding to each design in <code>X0</code>, that are smoothed to give <code>Lambda</code>
(as the global covariance matrix depend on <code>Delta</code> and <code>nu_hat</code>, it is recommended to also pass values for <code>theta</code>)
</p>
</li>
<li> <p><code>beta0</code> constant trend of the mean process
</p>
</li>
<li> <p><code>k_theta_g</code> constant used for link mean and noise processes lengthscales, when <code>settings$linkThetas == 'joint'</code>
</p>
</li>
<li> <p><code>theta_g</code> either one value (isotropic) or a vector (anistropic) for lengthscale parameter(s) of the noise process, when <code>settings$linkThetas != 'joint'</code>
</p>
</li>
<li> <p><code>g</code> scalar nugget of the noise process
</p>
</li>
<li> <p><code>nu</code> degree of freedom parameter
</p>
</li>
<li> <p><code>sigma2</code> scale variance
</p>
</li>
<li> <p><code>g_H</code> scalar homoskedastic nugget for the initialisation with a <code><a href="#topic+mleHomGP">mleHomGP</a></code>. See Details.
</p>
</li></ul>
</td></tr>
<tr><td><code id="mleHetTP_+3A_eps">eps</code></td>
<td>
<p>jitter used in the inversion of the covariance matrix for numerical stability</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The global covariance matrix of the model is parameterized as <code>K = sigma2 * C + Lambda * diag(1/mult)</code>,
with <code>C</code> the correlation matrix between unique designs, depending on the family of kernel used (see <code><a href="#topic+cov_gen">cov_gen</a></code> for available choices).
<code>Lambda</code> is the prediction on the noise level given by a (homoskedastic) GP: <br />
</p>
<p style="text-align: center;"><code class="reqn">\Lambda = C_g(C_g + \mathrm{diag}(g/\mathrm{mult}))^{-1} \Delta</code>
</p>
 <p><br />
with <code>C_g</code> the correlation matrix between unique designs for this second GP, with lengthscales hyperparameters <code>theta_g</code> and nugget <code>g</code>
and <code>Delta</code> the variance level at <code>X0</code> that are estimated.
</p>
<p>It is generally recommended to use <code><a href="#topic+find_reps">find_reps</a></code> to pre-process the data, to rescale the inputs to the unit cube and to normalize the outputs.
</p>
<p>The noise process lengthscales can be set in several ways:
</p>

<ul>
<li><p> using <code>k_theta_g</code> (<code>settings$linkThetas == 'joint'</code>), supposed to be greater than one by default. 
In this case lengthscales of the noise process are multiples of those of the mean process.
</p>
</li>
<li><p> if <code>settings$linkThetas == 'constr'</code>, then the lower bound on <code>theta_g</code> correspond to estimated values of an homoskedastic GP fit.
</p>
</li>
<li><p> else lengthscales between the mean and noise process are independent (both either anisotropic or not).
</p>
</li></ul>

<p>When no starting nor fixed parameter values are provided with <code>init</code> or <code>known</code>, 
the initialization process consists of fitting first an homoskedastic model of the data, called <code>modHom</code>.
Unless provided with <code>init$theta</code>, initial lengthscales are taken at 10% of the range determined with <code>lower</code> and <code>upper</code>,
while <code>init$g_H</code> may be use to pass an initial nugget value.
The resulting lengthscales provide initial values for <code>theta</code> (or update them if given in <code>init</code>). <br /> <br />
If necessary, a second homoskedastic model, <code>modNugs</code>, is fitted to the empirical residual variance between the prediction
given by <code>modHom</code> at <code>X0</code> and <code>Z</code> (up to <code>modHom$nu_hat</code>).
Note that when specifying <code>settings$linkThetas == 'joint'</code>, then this second homoskedastic model has fixed lengthscale parameters.
Starting values for <code>theta_g</code> and <code>g</code> are extracted from <code>modNugs</code>.<br /> <br />
Finally, three initialization schemes for <code>Delta</code> are available with <code>settings$initStrategy</code>: 
</p>

<ul>
<li><p> for <code>settings$initStrategy == 'simple'</code>, <code>Delta</code> is simply initialized to the estimated <code>g</code> value of <code>modHom</code>. 
Note that this procedure may fail when <code>settings$penalty == TRUE</code>.
</p>
</li>
<li><p> for <code>settings$initStrategy == 'residuals'</code>, <code>Delta</code> is initialized to the estimated residual variance from the homoskedastic mean prediction.
</p>
</li>
<li><p> for <code>settings$initStrategy == 'smoothed'</code>, <code>Delta</code> takes the values predicted by <code>modNugs</code> at <code>X0</code>.
</p>
</li></ul>

<p>Notice that <code>lower</code> and <code>upper</code> bounds cannot be equal for <code><a href="stats.html#topic+optim">optim</a></code>.
</p>


<h3>Value</h3>

<p>a list which is given the S3 class <code>"hetTP"</code>, with elements:
</p>

<ul>
<li> <p><code>theta</code>: unless given, maximum likelihood estimate (mle) of the lengthscale parameter(s),
</p>
</li>
<li> <p><code>Delta</code>: unless given, mle of the nugget vector (non-smoothed),
</p>
</li>
<li> <p><code>Lambda</code>: predicted input noise variance at <code>X0</code>, 
</p>
</li>
<li> <p><code>sigma2</code>: plugin estimator of the variance,
</p>
</li>
<li> <p><code>theta_g</code>: unless given, mle of the lengthscale(s) of the noise/log-noise process,
</p>
</li>
<li> <p><code>k_theta_g</code>: if <code>settings$linkThetas == 'joint'</code>, mle for the constant by which lengthscale parameters of <code>theta</code> are multiplied to get <code>theta_g</code>,
</p>
</li>
<li> <p><code>g</code>: unless given, mle of the nugget of the noise/log-noise process,
</p>
</li>
<li> <p><code>trendtype</code>: either &quot;<code>SK</code>&quot; if <code>beta0</code> is provided, else &quot;<code>OK</code>&quot;,
</p>
</li>
<li> <p><code>beta0</code> constant trend of the mean process, plugin-estimator unless given,
</p>
</li>
<li> <p><code>nmean</code>: plugin estimator for the constant noise/log-noise process mean,
</p>
</li>
<li> <p><code>ll</code>: log-likelihood value, (<code>ll_non_pen</code>) is the value without the penalty,
</p>
</li>
<li> <p><code>nit_opt</code>, <code>msg</code>: <code>counts</code> and <code>message</code> returned by <code><a href="stats.html#topic+optim">optim</a></code>
</p>
</li>
<li> <p><code>modHom</code>: homoskedastic GP model of class <code>homGP</code> used for initialization of the mean process,
</p>
</li>
<li> <p><code>modNugs</code>: homoskedastic GP model of class <code>homGP</code> used for initialization of the noise/log-noise process,
</p>
</li>
<li> <p><code>nu_hat_var</code>: variance of the noise process,
</p>
</li>
<li> <p><code>used_args</code>: list with arguments provided in the call to the function, which is saved in <code>call</code>,
</p>
</li>
<li> <p><code>X0</code>, <code>Z0</code>, <code>Z</code>, <code>eps</code>, <code>logN</code>, <code>covtype</code>: values given in input,
</p>
</li>
<li> <p><code>time</code>: time to train the model, in seconds.
</p>
</li></ul>



<h3>References</h3>

<p>M. Binois, Robert B. Gramacy, M. Ludkovski (2018), Practical heteroskedastic Gaussian process modeling for large simulation experiments,
Journal of Computational and Graphical Statistics, 27(4), 808&ndash;821.<br /> 
Preprint available on arXiv:1611.05902.<br /> <br />
</p>
<p>A. Shah, A. Wilson, Z. Ghahramani (2014), Student-t processes as alternatives to Gaussian processes, Artificial Intelligence and Statistics, 877&ndash;885.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.hetTP">predict.hetTP</a></code> for predictions. 
<code>summary</code> and <code>plot</code> functions are available as well.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>##------------------------------------------------------------
## Example 1: Heteroskedastic TP modeling on the motorcycle data
##------------------------------------------------------------
set.seed(32)

## motorcycle data
library(MASS)
X &lt;- matrix(mcycle$times, ncol = 1)
Z &lt;- mcycle$accel
nvar &lt;- 1
plot(X, Z, ylim = c(-160, 90), ylab = 'acceleration', xlab = "time")


## Model fitting
model &lt;- mleHetTP(X = X, Z = Z, lower = rep(0.1, nvar), upper = rep(50, nvar),
                  covtype = "Matern5_2")
            
## Display averaged observations
points(model$X0, model$Z0, pch = 20)

## A quick view of the fit                  
summary(model)

## Create a prediction grid and obtain predictions
xgrid &lt;- matrix(seq(0, 60, length.out = 301), ncol = 1) 
preds &lt;- predict(x = xgrid, object =  model)

## Display mean predictive surface
lines(xgrid, preds$mean, col = 'red', lwd = 2)
## Display 95% confidence intervals
lines(xgrid, preds$mean + sqrt(preds$sd2) * qt(0.05, df = model$nu + nrow(X)), col = 2, lty = 2)
lines(xgrid, preds$mean + sqrt(preds$sd2) * qt(0.95, df = model$nu + nrow(X)), col = 2, lty = 2)
## Display 95% prediction intervals
lines(xgrid, preds$mean + sqrt(preds$sd2 + preds$nugs) * qt(0.05, df = model$nu + nrow(X)),
  col = 3, lty = 2)
lines(xgrid, preds$mean + sqrt(preds$sd2 + preds$nugs) * qt(0.95, df = model$nu + nrow(X)), 
  col = 3, lty = 2)

##------------------------------------------------------------
## Example 2: 2D Heteroskedastic TP modeling
##------------------------------------------------------------
set.seed(1)
nvar &lt;- 2
  
## Branin redefined in [0,1]^2
branin &lt;- function(x){
  if(is.null(nrow(x)))
    x &lt;- matrix(x, nrow = 1)
    x1 &lt;- x[,1] * 15 - 5
    x2 &lt;- x[,2] * 15
    (x2 - 5/(4 * pi^2) * (x1^2) + 5/pi * x1 - 6)^2 + 10 * (1 - 1/(8 * pi)) * cos(x1) + 10
}

## Noise field via standard deviation
noiseFun &lt;- function(x){
  if(is.null(nrow(x)))
    x &lt;- matrix(x, nrow = 1)
  return(1/5*(3*(2 + 2*sin(x[,1]*pi)*cos(x[,2]*3*pi) + 5*rowSums(x^2))))
}

## data generating function combining mean and noise fields
ftest &lt;- function(x){
  return(branin(x) + rnorm(nrow(x), mean = 0, sd = noiseFun(x)))
}

## Grid of predictive locations
ngrid &lt;- 51
xgrid &lt;- matrix(seq(0, 1, length.out = ngrid), ncol = 1) 
Xgrid &lt;- as.matrix(expand.grid(xgrid, xgrid))

## Unique (randomly chosen) design locations
n &lt;- 100
Xu &lt;- matrix(runif(n * 2), n)

## Select replication sites randomly
X &lt;- Xu[sample(1:n, 20*n, replace = TRUE),]

## obtain training data response at design locations X
Z &lt;- ftest(X)

## Formating of data for model creation (find replicated observations) 
prdata &lt;- find_reps(X, Z, rescale = FALSE, normalize = FALSE)

## Model fitting
model &lt;- mleHetTP(X = list(X0 = prdata$X0, Z0 = prdata$Z0, mult = prdata$mult), Z = prdata$Z, ,
                  lower = rep(0.01, nvar), upper = rep(10, nvar),
                  covtype = "Matern5_2")

## a quick view into the data stored in the "hetTP"-class object
summary(model)                  
             
## prediction from the fit on the grid     
preds &lt;- predict(x = Xgrid, object =  model)

## Visualization of the predictive surface
par(mfrow = c(2, 2))
contour(x = xgrid,  y = xgrid, z = matrix(branin(Xgrid), ngrid), 
  main = "Branin function", nlevels = 20)
points(X, col = 'blue', pch = 20)
contour(x = xgrid,  y = xgrid, z = matrix(preds$mean, ngrid), 
  main = "Predicted mean", nlevels = 20)
points(X, col = 'blue', pch = 20)
contour(x = xgrid,  y = xgrid, z = matrix(noiseFun(Xgrid), ngrid), 
  main = "Noise standard deviation function", nlevels = 20)
points(X, col = 'blue', pch = 20)
contour(x = xgrid,  y= xgrid, z = matrix(sqrt(preds$nugs), ngrid), 
  main = "Predicted noise values", nlevels = 20)
points(X, col = 'blue', pch = 20)
par(mfrow = c(1, 1))
</code></pre>

<hr>
<h2 id='mleHomGP'>Gaussian process modeling with homoskedastic noise</h2><span id='topic+mleHomGP'></span>

<h3>Description</h3>

<p>Gaussian process regression under homoskedastic noise based on maximum likelihood estimation of the 
hyperparameters. This function is enhanced to deal with replicated observations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mleHomGP(
  X,
  Z,
  lower = NULL,
  upper = NULL,
  known = NULL,
  noiseControl = list(g_bounds = c(sqrt(.Machine$double.eps), 100)),
  init = NULL,
  covtype = c("Gaussian", "Matern5_2", "Matern3_2"),
  maxit = 100,
  eps = sqrt(.Machine$double.eps),
  settings = list(return.Ki = TRUE, factr = 1e+07)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mleHomGP_+3A_x">X</code></td>
<td>
<p>matrix of all designs, one per row, or list with elements:
</p>

<ul>
<li> <p><code>X0</code> matrix of unique design locations, one point per row
</p>
</li>
<li> <p><code>Z0</code> vector of averaged observations, of length <code>nrow(X0)</code>
</p>
</li>
<li> <p><code>mult</code> number of replicates at designs in <code>X0</code>, of length <code>nrow(X0)</code>
</p>
</li></ul>
</td></tr>
<tr><td><code id="mleHomGP_+3A_z">Z</code></td>
<td>
<p>vector of all observations. If using a list with <code>X</code>, <code>Z</code> has to be ordered with respect to <code>X0</code>, and of length <code>sum(mult)</code></p>
</td></tr>
<tr><td><code id="mleHomGP_+3A_lower">lower</code>, <code id="mleHomGP_+3A_upper">upper</code></td>
<td>
<p>optional bounds for the <code>theta</code> parameter (see <code><a href="#topic+cov_gen">cov_gen</a></code> for the exact parameterization).
In the multivariate case, it is possible to give vectors for bounds (resp. scalars) for anisotropy (resp. isotropy)</p>
</td></tr>
<tr><td><code id="mleHomGP_+3A_known">known</code></td>
<td>
<p>optional list of known parameters, e.g., <code>beta0</code>, <code>theta</code> or <code>g</code></p>
</td></tr>
<tr><td><code id="mleHomGP_+3A_noisecontrol">noiseControl</code></td>
<td>
<p>list with element , 
</p>

<ul>
<li> <p><code>g_bounds</code>, vector providing minimal and maximal noise to signal ratio
</p>
</li></ul>
</td></tr>
<tr><td><code id="mleHomGP_+3A_init">init</code></td>
<td>
<p>optional list specifying starting values for MLE optimization, with elements:
</p>

<ul>
<li> <p><code>theta_init</code> initial value of the theta parameters to be optimized over (default to 10% of the range determined with <code>lower</code> and <code>upper</code>)
</p>
</li>
<li> <p><code>g_init</code> initial value of the nugget parameter to be optimized over (based on the variance at replicates if there are any, else <code>0.1</code>)
</p>
</li></ul>
</td></tr>
<tr><td><code id="mleHomGP_+3A_covtype">covtype</code></td>
<td>
<p>covariance kernel type, either 'Gaussian', 'Matern5_2' or 'Matern3_2', see <code><a href="#topic+cov_gen">cov_gen</a></code></p>
</td></tr>
<tr><td><code id="mleHomGP_+3A_maxit">maxit</code></td>
<td>
<p>maximum number of iteration for L-BFGS-B of <code><a href="stats.html#topic+optim">optim</a></code></p>
</td></tr>
<tr><td><code id="mleHomGP_+3A_eps">eps</code></td>
<td>
<p>jitter used in the inversion of the covariance matrix for numerical stability</p>
</td></tr>
<tr><td><code id="mleHomGP_+3A_settings">settings</code></td>
<td>
<p>list with argument <code>return.Ki</code>, to include the inverse covariance matrix in the object for further use (e.g., prediction).
Arguments <code>factr</code> (default to 1e9) and <code>pgtol</code> are available to be passed to <code>control</code> for L-BFGS-B in <code><a href="stats.html#topic+optim">optim</a></code> (for the joint likelihood only).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The global covariance matrix of the model is parameterized as <code>nu_hat * (C + g * diag(1/mult)) = nu_hat * K</code>,
with <code>C</code> the correlation matrix between unique designs, depending on the family of kernel used (see <code><a href="#topic+cov_gen">cov_gen</a></code> for available choices) and values of lengthscale parameters.
<code>nu_hat</code> is the plugin estimator of the variance of the process.
</p>
<p>It is generally recommended to use <code><a href="#topic+find_reps">find_reps</a></code> to pre-process the data, to rescale the inputs to the unit cube and to normalize the outputs.
</p>


<h3>Value</h3>

<p>a list which is given the S3 class &quot;<code>homGP</code>&quot;, with elements:
</p>

<ul>
<li> <p><code>theta</code>: maximum likelihood estimate of the lengthscale parameter(s),
</p>
</li>
<li> <p><code>g</code>: maximum likelihood estimate of the nugget variance,
</p>
</li>
<li> <p><code>trendtype</code>: either &quot;<code>SK</code>&quot; if <code>beta0</code> is given, else &quot;<code>OK</code>&quot; 
</p>
</li>
<li> <p><code>beta0</code>: estimated trend unless given in input,
</p>
</li>
<li> <p><code>nu_hat</code>: plugin estimator of the variance,
</p>
</li>
<li> <p><code>ll</code>: log-likelihood value,
</p>
</li>
<li> <p><code>X0</code>, <code>Z0</code>, <code>Z</code>, <code>mult</code>, <code>eps</code>, <code>covtype</code>: values given in input,
</p>
</li>
<li> <p><code>call</code>: user call of the function
</p>
</li>
<li> <p><code>used_args</code>: list with arguments provided in the call
</p>
</li>
<li> <p><code>nit_opt</code>, <code>msg</code>: <code>counts</code> and <code>msg</code> returned by <code><a href="stats.html#topic+optim">optim</a></code>
</p>
</li>
<li> <p><code>Ki</code>: inverse covariance matrix (not scaled by <code>nu_hat</code>) (if <code>return.Ki</code> is <code>TRUE</code> in <code>settings</code>)
</p>
</li>
<li> <p><code>time</code>: time to train the model, in seconds.
</p>
</li></ul>



<h3>References</h3>

<p>M. Binois, Robert B. Gramacy, M. Ludkovski (2018), Practical heteroskedastic Gaussian process modeling for large simulation experiments,
Journal of Computational and Graphical Statistics, 27(4), 808&ndash;821.<br /> 
Preprint available on arXiv:1611.05902. <br /> <br />
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.homGP">predict.homGP</a></code> for predictions, <code><a href="#topic+update.homGP">update.homGP</a></code> for updating an existing model. 
<code>summary</code> and <code>plot</code> functions are available as well. 
<code><a href="#topic+mleHomTP">mleHomTP</a></code> provide a Student-t equivalent.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>##------------------------------------------------------------
## Example 1: Homoskedastic GP modeling on the motorcycle data
##------------------------------------------------------------
set.seed(32)

## motorcycle data
library(MASS)
X &lt;- matrix(mcycle$times, ncol = 1)
Z &lt;- mcycle$accel
plot(X, Z, ylim = c(-160, 90), ylab = 'acceleration', xlab = "time")

 
model &lt;- mleHomGP(X = X, Z = Z, lower = 0.01, upper = 100)
  
## Display averaged observations
points(model$X0, model$Z0, pch = 20) 
xgrid &lt;- matrix(seq(0, 60, length.out = 301), ncol = 1) 
predictions &lt;- predict(x = xgrid, object =  model)

## Display mean prediction
lines(xgrid, predictions$mean, col = 'red', lwd = 2)
## Display 95% confidence intervals
lines(xgrid, qnorm(0.05, predictions$mean, sqrt(predictions$sd2)), col = 2, lty = 2)
lines(xgrid, qnorm(0.95, predictions$mean, sqrt(predictions$sd2)), col = 2, lty = 2)
## Display 95% prediction intervals
lines(xgrid, qnorm(0.05, predictions$mean, sqrt(predictions$sd2 + predictions$nugs)), 
  col = 3, lty = 2)
lines(xgrid, qnorm(0.95, predictions$mean, sqrt(predictions$sd2 + predictions$nugs)), 
  col = 3, lty = 2)
</code></pre>

<hr>
<h2 id='mleHomTP'>Student-T process modeling with homoskedastic noise</h2><span id='topic+mleHomTP'></span>

<h3>Description</h3>

<p>Student-t process regression under homoskedastic noise based on maximum likelihood estimation of the 
hyperparameters. This function is enhanced to deal with replicated observations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mleHomTP(
  X,
  Z,
  lower = NULL,
  upper = NULL,
  known = list(beta0 = 0),
  noiseControl = list(g_bounds = c(sqrt(.Machine$double.eps), 10000), nu_bounds = c(2 +
    0.001, 30), sigma2_bounds = c(sqrt(.Machine$double.eps), 10000)),
  init = list(nu = 3),
  covtype = c("Gaussian", "Matern5_2", "Matern3_2"),
  maxit = 100,
  eps = sqrt(.Machine$double.eps),
  settings = list(return.Ki = TRUE, factr = 1e+09)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mleHomTP_+3A_x">X</code></td>
<td>
<p>matrix of all designs, one per row, or list with elements:
</p>

<ul>
<li> <p><code>X0</code> matrix of unique design locations, one point per row
</p>
</li>
<li> <p><code>Z0</code> vector of averaged observations, of length <code>nrow(X0)</code>
</p>
</li>
<li> <p><code>mult</code> number of replicates at designs in <code>X0</code>, of length <code>nrow(X0)</code>
</p>
</li></ul>
</td></tr>
<tr><td><code id="mleHomTP_+3A_z">Z</code></td>
<td>
<p>vector of all observations. If using a list with <code>X</code>, <code>Z</code> has to be ordered with respect to <code>X0</code>, and of length <code>sum(mult)</code></p>
</td></tr>
<tr><td><code id="mleHomTP_+3A_lower">lower</code>, <code id="mleHomTP_+3A_upper">upper</code></td>
<td>
<p>bounds for the <code>theta</code> parameter (see <code><a href="#topic+cov_gen">cov_gen</a></code> for the exact parameterization).
In the multivariate case, it is possible to give vectors for bounds (resp. scalars) for anisotropy (resp. isotropy)</p>
</td></tr>
<tr><td><code id="mleHomTP_+3A_known">known</code></td>
<td>
<p>optional list of known parameters, e.g., <code>beta0</code> (default to <code>0</code>), <code>theta</code>, <code>g</code>, <code>sigma2</code> or <code>nu</code></p>
</td></tr>
<tr><td><code id="mleHomTP_+3A_noisecontrol">noiseControl</code></td>
<td>
<p>list with element,
</p>

<ul>
<li> <p><code>g_bound</code>, vector providing minimal and maximal noise variance
</p>
</li>
<li> <p><code>sigma2_bounds</code>, vector providing minimal and maximal signal variance
</p>
</li>
<li> <p><code>nu_bounds</code>, vector providing minimal and maximal values for the degrees of freedom. 
The mininal value has to be stricly greater than 2. If the mle optimization gives a large value, e.g., 30,
considering a GP with <code><a href="#topic+mleHomGP">mleHomGP</a></code> may be better. 
</p>
</li></ul>
</td></tr>
<tr><td><code id="mleHomTP_+3A_init">init</code></td>
<td>
<p>list specifying starting values for MLE optimization, with elements:
</p>

<ul>
<li> <p><code>theta_init</code> initial value of the theta parameters to be optimized over (default to 10% of the range determined with <code>lower</code> and <code>upper</code>)
</p>
</li>
<li> <p><code>g_init</code> initial value of the nugget parameter to be optimized over (based on the variance at replicates if there are any, else 10% of the variance)
</p>
</li>
<li> <p><code>sigma2</code> initial value of the variance paramter (default to <code>1</code>)
</p>
</li>
<li> <p><code>nu</code> initial value of the degrees of freedom parameter (default to <code>3</code>)
</p>
</li></ul>
</td></tr>
<tr><td><code id="mleHomTP_+3A_covtype">covtype</code></td>
<td>
<p>covariance kernel type, either 'Gaussian', 'Matern5_2' or 'Matern3_2', see <code><a href="#topic+cov_gen">cov_gen</a></code></p>
</td></tr>
<tr><td><code id="mleHomTP_+3A_maxit">maxit</code></td>
<td>
<p>maximum number of iteration for L-BFGS-B of <code><a href="stats.html#topic+optim">optim</a></code></p>
</td></tr>
<tr><td><code id="mleHomTP_+3A_eps">eps</code></td>
<td>
<p>jitter used in the inversion of the covariance matrix for numerical stability</p>
</td></tr>
<tr><td><code id="mleHomTP_+3A_settings">settings</code></td>
<td>
<p>list with argument <code>return.Ki</code>, to include the inverse covariance matrix in the object for further use (e.g., prediction).
Arguments <code>factr</code> (default to 1e9) and <code>pgtol</code> are available to be passed to <code>control</code> for L-BFGS-B in <code><a href="stats.html#topic+optim">optim</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The global covariance matrix of the model is parameterized as <code>K = sigma2 * C + g * diag(1/mult)</code>,
with <code>C</code> the correlation matrix between unique designs, depending on the family of kernel used (see <code><a href="#topic+cov_gen">cov_gen</a></code> for available choices).
</p>
<p>It is generally recommended to use <code><a href="#topic+find_reps">find_reps</a></code> to pre-process the data, to rescale the inputs to the unit cube and to normalize the outputs.
</p>


<h3>Value</h3>

<p>a list which is given the S3 class &quot;<code>homGP</code>&quot;, with elements:
</p>

<ul>
<li> <p><code>theta</code>: maximum likelihood estimate of the lengthscale parameter(s),
</p>
</li>
<li> <p><code>g</code>: maximum likelihood estimate of the nugget variance,
</p>
</li>
<li> <p><code>trendtype</code>: either &quot;<code>SK</code>&quot; if <code>beta0</code> is given, else &quot;<code>OK</code>&quot; 
</p>
</li>
<li> <p><code>beta0</code>: estimated trend unless given in input,
</p>
</li>
<li> <p><code>sigma2</code>:  maximum likelihood estimate of the scale variance,
</p>
</li>
<li> <p><code>nu2</code>:  maximum likelihood estimate of the degrees of freedom parameter,
</p>
</li>
<li> <p><code>ll</code>: log-likelihood value,
</p>
</li>
<li> <p><code>X0</code>, <code>Z0</code>, <code>Z</code>, <code>mult</code>, <code>eps</code>, <code>covtype</code>: values given in input,
</p>
</li>
<li> <p><code>call</code>: user call of the function
</p>
</li>
<li> <p><code>used_args</code>: list with arguments provided in the call
</p>
</li>
<li> <p><code>nit_opt</code>, <code>msg</code>: <code>counts</code> and msg returned by <code><a href="stats.html#topic+optim">optim</a></code>
</p>
</li>
<li> <p><code>Ki</code>, inverse covariance matrix (if <code>return.Ki</code> is <code>TRUE</code> in <code>settings</code>)
</p>
</li>
<li> <p><code>time</code>: time to train the model, in seconds.
</p>
</li></ul>



<h3>References</h3>

<p>M. Binois, Robert B. Gramacy, M. Ludkovski (2018), Practical heteroskedastic Gaussian process modeling for large simulation experiments,
Journal of Computational and Graphical Statistics, 27(4), 808&ndash;821.<br /> 
Preprint available on arXiv:1611.05902.<br /> <br />
</p>
<p>A. Shah, A. Wilson, Z. Ghahramani (2014), Student-t processes as alternatives to Gaussian processes, Artificial Intelligence and Statistics, 877&ndash;885. <br /> <br />
</p>
<p>M. Chung, M. Binois, RB Gramacy, DJ Moquin, AP Smith, AM Smith (2019). 
Parameter and Uncertainty Estimation for Dynamical Systems Using Surrogate Stochastic Processes.
SIAM Journal on Scientific Computing, 41(4), 2212-2238.<br />
Preprint available on arXiv:1802.00852.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.homTP">predict.homTP</a></code> for predictions.
<code>summary</code> and <code>plot</code> functions are available as well.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>##------------------------------------------------------------
## Example 1: Homoskedastic Student-t modeling on the motorcycle data
##------------------------------------------------------------
set.seed(32)

## motorcycle data
library(MASS)
X &lt;- matrix(mcycle$times, ncol = 1)
Z &lt;- mcycle$accel
plot(X, Z, ylim = c(-160, 90), ylab = 'acceleration', xlab = "time")

noiseControl = list(g_bounds = c(1e-3, 1e4))
model &lt;- mleHomTP(X = X, Z = Z, lower = 0.01, upper = 100, noiseControl = noiseControl)
summary(model)
  
## Display averaged observations
points(model$X0, model$Z0, pch = 20) 
xgrid &lt;- matrix(seq(0, 60, length.out = 301), ncol = 1) 
preds &lt;- predict(x = xgrid, object =  model)

## Display mean prediction
lines(xgrid, preds$mean, col = 'red', lwd = 2)
## Display 95% confidence intervals
lines(xgrid, preds$mean + sqrt(preds$sd2) * qt(0.05, df = model$nu + nrow(X)), col = 2, lty = 2)
lines(xgrid, preds$mean + sqrt(preds$sd2) * qt(0.95, df = model$nu + nrow(X)), col = 2, lty = 2)
## Display 95% prediction intervals
lines(xgrid, preds$mean + sqrt(preds$sd2 + preds$nugs) * qt(0.05, df = model$nu + nrow(X)), 
  col = 3, lty = 2)
lines(xgrid, preds$mean + sqrt(preds$sd2 + preds$nugs) * qt(0.95, df = model$nu + nrow(X)), 
  col = 3, lty = 2)
</code></pre>

<hr>
<h2 id='pred_noisy_input'>Gaussian process prediction prediction at a noisy input <code>x</code>, with centered Gaussian noise of variance <code>sigma_x</code>. 
Several options are available, with different efficiency/accuracy tradeoffs.</h2><span id='topic+pred_noisy_input'></span>

<h3>Description</h3>

<p>Gaussian process prediction prediction at a noisy input <code>x</code>, with centered Gaussian noise of variance <code>sigma_x</code>. 
Several options are available, with different efficiency/accuracy tradeoffs.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pred_noisy_input(x, model, sigma_x, type = c("simple", "taylor", "exact"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pred_noisy_input_+3A_x">x</code></td>
<td>
<p>design considered</p>
</td></tr>
<tr><td><code id="pred_noisy_input_+3A_model">model</code></td>
<td>
<p>GP</p>
</td></tr>
<tr><td><code id="pred_noisy_input_+3A_sigma_x">sigma_x</code></td>
<td>
<p>input variance</p>
</td></tr>
<tr><td><code id="pred_noisy_input_+3A_type">type</code></td>
<td>
<p>available options include
</p>

<ul>
<li> <p><code>simple</code> relying on a corrective term, see (McHutchon2011);
</p>
</li>
<li> <p><code>taylor</code> based on a Taylor expansion, see, e.g., (Girard2003);
</p>
</li>
<li> <p><code>exact</code> for exact moments (only for the <code>Gaussian</code> covariance).
</p>
</li></ul>
</td></tr>
</table>


<h3>Note</h3>

<p>Beta version.
</p>


<h3>References</h3>

<p>A. McHutchon and C.E. Rasmussen (2011), 
Gaussian process training with input noise, 
Advances in Neural Information Processing Systems, 1341-1349. <br />
</p>
<p>A. Girard, C.E. Rasmussen, J.Q. Candela and R. Murray-Smith (2003), 
Gaussian process priors with uncertain inputs application to multiple-step ahead time series forecasting,
Advances in Neural Information Processing Systems, 545-552.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>################################################################################
### Illustration of prediction with input noise
################################################################################

## noise std deviation function defined in [0,1]
noiseFun &lt;- function(x, coef = 1.1, scale = 0.25){
  if(is.null(nrow(x))) x &lt;- matrix(x, nrow = 1)
  return(scale*(coef + sin(x * 2 * pi)))
}

## data generating function combining mean and noise fields
ftest &lt;- function(x, scale = 0.25){
if(is.null(nrow(x))) x &lt;- matrix(x, ncol = 1)
  return(f1d(x) + rnorm(nrow(x), mean = 0, sd = noiseFun(x, scale = scale)))
}

ntest &lt;- 101; xgrid &lt;- seq(0,1, length.out = ntest); Xgrid &lt;- matrix(xgrid, ncol = 1)
set.seed(42)
Xpred &lt;- Xgrid[rep(1:ntest, each = 100),,drop = FALSE]
Zpred &lt;- matrix(ftest(Xpred), byrow = TRUE, nrow = ntest)
n &lt;- 10
N &lt;- 20
X &lt;- matrix(seq(0, 1, length.out = n))
if(N &gt; n) X &lt;- rbind(X, X[sample(1:n, N-n, replace = TRUE),,drop = FALSE])
X &lt;- X[order(X[,1]),,drop = FALSE]

Z &lt;- apply(X, 1, ftest)
par(mfrow = c(1, 2))
plot(X, Z, ylim = c(-10,15), xlim = c(-0.1,1.1))
lines(xgrid, f1d(xgrid))
lines(xgrid, drop(f1d(xgrid)) + 2*noiseFun(xgrid), lty = 3)
lines(xgrid, drop(f1d(xgrid)) - 2*noiseFun(xgrid), lty = 3)
model &lt;- mleHomGP(X, Z, known = list(beta0 = 0))
preds &lt;- predict(model, Xgrid)
lines(xgrid, preds$mean, col = "red", lwd = 2)
lines(xgrid, preds$mean - 2*sqrt(preds$sd2), col = "blue")
lines(xgrid, preds$mean + 2*sqrt(preds$sd2), col = "blue")
lines(xgrid, preds$mean - 2*sqrt(preds$sd2 + preds$nugs), col = "blue", lty = 2)
lines(xgrid, preds$mean + 2*sqrt(preds$sd2 + preds$nugs), col = "blue", lty = 2)

sigmax &lt;- 0.1
X1 &lt;- matrix(0.5)

lines(xgrid, dnorm(xgrid, X1, sigmax) - 10, col = "darkgreen")

# MC experiment
nmc &lt;- 1000
XX &lt;- matrix(rnorm(nmc, X1, sigmax))
pxx &lt;- predict(model, XX)
YXX &lt;- rnorm(nmc, mean = pxx$mean, sd = sqrt(pxx$sd2 + pxx$nugs))
points(XX, YXX, pch = '.')

hh &lt;- hist(YXX, breaks = 51, plot = FALSE)
dd &lt;- density(YXX)
plot(hh$density, hh$mids, ylim = c(-10, 15))
lines(dd$y, dd$x)

# GP predictions
pin1 &lt;- pred_noisy_input(X1, model, sigmax^2, type = "exact")
pin2 &lt;- pred_noisy_input(X1, model, sigmax^2, type = "taylor")
pin3 &lt;- pred_noisy_input(X1, model, sigmax^2, type = "simple")
ygrid &lt;- seq(-10, 15,, ntest)
lines(dnorm(ygrid, pin1$mean, sqrt(pin1$sd2)), ygrid, lty = 2, col = "orange")
lines(dnorm(ygrid, pin2$mean, sqrt(pin2$sd2)), ygrid, lty = 2, col = "violet")
lines(dnorm(ygrid, pin3$mean, sqrt(pin3$sd2)), ygrid, lty = 2, col = "grey")
abline(h = mean(YXX), col = "red") # empirical mean

par(mfrow = c(1, 1))
</code></pre>

<hr>
<h2 id='predict.CRNGP'>Gaussian process predictions using a GP object for correlated noise (of class <code>CRNGP</code>)</h2><span id='topic+predict.CRNGP'></span>

<h3>Description</h3>

<p>Gaussian process predictions using a GP object for correlated noise (of class <code>CRNGP</code>)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'CRNGP'
predict(object, x, xprime = NULL, t0 = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.CRNGP_+3A_object">object</code></td>
<td>
<p>an object of class <code>CRNGP</code>; e.g., as returned by <code><a href="#topic+mleCRNGP">mleCRNGP</a></code></p>
</td></tr>
<tr><td><code id="predict.CRNGP_+3A_x">x</code></td>
<td>
<p>matrix of designs locations to predict at (one point per row). Last column is for the integer valued seed. 
If trajectories are considered, i.e., with time, the prediction will occur at the same times as the training data unless <code>t0</code> is provided.</p>
</td></tr>
<tr><td><code id="predict.CRNGP_+3A_xprime">xprime</code></td>
<td>
<p>optional second matrix of predictive locations to obtain the predictive covariance matrix between <code>x</code> and <code>xprime</code></p>
</td></tr>
<tr><td><code id="predict.CRNGP_+3A_t0">t0</code></td>
<td>
<p>single column matrix of times to predict at, if trajectories are considered. By default the prediction is at the same times as the training data.</p>
</td></tr>
<tr><td><code id="predict.CRNGP_+3A_...">...</code></td>
<td>
<p>no other argument for this method</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The full predictive variance corresponds to the sum of <code>sd2</code> and <code>nugs</code>. See <code><a href="#topic+mleHomGP">mleHomGP</a></code> for examples.
</p>


<h3>Value</h3>

<p>list with elements
</p>

<ul>
<li> <p><code>mean</code>: kriging mean;
</p>
</li>
<li> <p><code>sd2</code>: kriging variance (filtered, e.g. without the nugget value)
</p>
</li>
<li> <p><code>cov</code>: predictive covariance matrix between <code>x</code> and <code>xprime</code>
</p>
</li>
<li> <p><code>nugs</code>: nugget value at each prediction location, for consistency with <code><a href="#topic+mleHomGP">mleHomGP</a></code>.
</p>
</li></ul>


<hr>
<h2 id='predict.hetGP'>Gaussian process predictions using a heterogeneous noise GP object (of class <code>hetGP</code>)</h2><span id='topic+predict.hetGP'></span>

<h3>Description</h3>

<p>Gaussian process predictions using a heterogeneous noise GP object (of class <code>hetGP</code>)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'hetGP'
predict(object, x, noise.var = FALSE, xprime = NULL, nugs.only = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.hetGP_+3A_object">object</code></td>
<td>
<p>an object of class <code>hetGP</code>; e.g., as returned by <code><a href="#topic+mleHetGP">mleHetGP</a></code></p>
</td></tr>
<tr><td><code id="predict.hetGP_+3A_x">x</code></td>
<td>
<p>matrix of designs locations to predict at (one point per row)</p>
</td></tr>
<tr><td><code id="predict.hetGP_+3A_noise.var">noise.var</code></td>
<td>
<p>should the variance of the latent variance process be returned?</p>
</td></tr>
<tr><td><code id="predict.hetGP_+3A_xprime">xprime</code></td>
<td>
<p>optional second matrix of predictive locations to obtain the predictive covariance matrix between <code>x</code> and <code>xprime</code></p>
</td></tr>
<tr><td><code id="predict.hetGP_+3A_nugs.only">nugs.only</code></td>
<td>
<p>if <code>TRUE</code>, only return noise variance prediction</p>
</td></tr>
<tr><td><code id="predict.hetGP_+3A_...">...</code></td>
<td>
<p>no other argument for this method.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The full predictive variance corresponds to the sum of <code>sd2</code> and <code>nugs</code>.
See <code><a href="#topic+mleHetGP">mleHetGP</a></code> for examples.
</p>


<h3>Value</h3>

<p>list with elements
</p>

<ul>
<li> <p><code>mean</code>: kriging mean;
</p>
</li>
<li> <p><code>sd2</code>: kriging variance (filtered, e.g. without the nugget values)
</p>
</li>
<li> <p><code>nugs</code>: noise variance prediction
</p>
</li>
<li> <p><code>sd2_var</code>: (returned if <code>noise.var = TRUE</code>) kriging variance of the noise process (i.e., on log-variances if <code>logN = TRUE</code>)
</p>
</li>
<li> <p><code>cov</code>: (returned if <code>xprime</code> is given) predictive covariance matrix between <code>x</code> and <code>xprime</code>
</p>
</li></ul>


<hr>
<h2 id='predict.hetTP'>Student-t process predictions using a heterogeneous noise TP object (of class <code>hetTP</code>)</h2><span id='topic+predict.hetTP'></span>

<h3>Description</h3>

<p>Student-t process predictions using a heterogeneous noise TP object (of class <code>hetTP</code>)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'hetTP'
predict(object, x, noise.var = FALSE, xprime = NULL, nugs.only = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.hetTP_+3A_object">object</code></td>
<td>
<p>an object of class <code>hetTP</code>; e.g., as returned by <code><a href="#topic+mleHetTP">mleHetTP</a></code></p>
</td></tr>
<tr><td><code id="predict.hetTP_+3A_x">x</code></td>
<td>
<p>matrix of designs locations to predict at</p>
</td></tr>
<tr><td><code id="predict.hetTP_+3A_noise.var">noise.var</code></td>
<td>
<p>should the variance of the latent variance process be returned?</p>
</td></tr>
<tr><td><code id="predict.hetTP_+3A_xprime">xprime</code></td>
<td>
<p>optional second matrix of predictive locations to obtain the predictive covariance matrix between <code>x</code> and <code>xprime</code></p>
</td></tr>
<tr><td><code id="predict.hetTP_+3A_nugs.only">nugs.only</code></td>
<td>
<p>if TRUE, only return noise variance prediction</p>
</td></tr>
<tr><td><code id="predict.hetTP_+3A_...">...</code></td>
<td>
<p>no other argument for this method.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The full predictive variance corresponds to the sum of <code>sd2</code> and <code>nugs</code>.
</p>


<h3>Value</h3>

<p>list with elements
</p>

<ul>
<li> <p><code>mean</code>: kriging mean;
</p>
</li>
<li> <p><code>sd2</code>: kriging variance (filtered, e.g. without the nugget values)
</p>
</li>
<li> <p><code>nugs</code>: noise variance
</p>
</li>
<li> <p><code>sd2_var</code>: (optional) kriging variance of the noise process (i.e., on log-variances if <code>logN = TRUE</code>)
</p>
</li>
<li> <p><code>cov</code>: (optional) predictive covariance matrix between x and xprime
</p>
</li></ul>


<hr>
<h2 id='predict.homGP'>Gaussian process predictions using a homoskedastic noise GP object (of class <code>homGP</code>)</h2><span id='topic+predict.homGP'></span>

<h3>Description</h3>

<p>Gaussian process predictions using a homoskedastic noise GP object (of class <code>homGP</code>)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'homGP'
predict(object, x, xprime = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.homGP_+3A_object">object</code></td>
<td>
<p>an object of class <code>homGP</code>; e.g., as returned by <code><a href="#topic+mleHomGP">mleHomGP</a></code></p>
</td></tr>
<tr><td><code id="predict.homGP_+3A_x">x</code></td>
<td>
<p>matrix of designs locations to predict at (one point per row)</p>
</td></tr>
<tr><td><code id="predict.homGP_+3A_xprime">xprime</code></td>
<td>
<p>optional second matrix of predictive locations to obtain the predictive covariance matrix between <code>x</code> and <code>xprime</code></p>
</td></tr>
<tr><td><code id="predict.homGP_+3A_...">...</code></td>
<td>
<p>no other argument for this method</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The full predictive variance corresponds to the sum of <code>sd2</code> and <code>nugs</code>. See <code><a href="#topic+mleHomGP">mleHomGP</a></code> for examples.
</p>


<h3>Value</h3>

<p>list with elements
</p>

<ul>
<li> <p><code>mean</code>: kriging mean;
</p>
</li>
<li> <p><code>sd2</code>: kriging variance (filtered, e.g. without the nugget value)
</p>
</li>
<li> <p><code>cov</code>: predictive covariance matrix between <code>x</code> and <code>xprime</code>
</p>
</li>
<li> <p><code>nugs</code>: nugget value at each prediction location, for consistency with <code><a href="#topic+mleHomGP">mleHomGP</a></code>.
</p>
</li></ul>


<hr>
<h2 id='predict.homTP'>Student-t process predictions using a homoskedastic noise GP object (of class <code>homGP</code>)</h2><span id='topic+predict.homTP'></span>

<h3>Description</h3>

<p>Student-t process predictions using a homoskedastic noise GP object (of class <code>homGP</code>)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'homTP'
predict(object, x, xprime = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.homTP_+3A_object">object</code></td>
<td>
<p>an object of class <code>homGP</code>; e.g., as returned by <code><a href="#topic+mleHomTP">mleHomTP</a></code></p>
</td></tr>
<tr><td><code id="predict.homTP_+3A_x">x</code></td>
<td>
<p>matrix of designs locations to predict at</p>
</td></tr>
<tr><td><code id="predict.homTP_+3A_xprime">xprime</code></td>
<td>
<p>optional second matrix of predictive locations to obtain the predictive covariance matrix between <code>x</code> and <code>xprime</code></p>
</td></tr>
<tr><td><code id="predict.homTP_+3A_...">...</code></td>
<td>
<p>no other argument for this method</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The full predictive variance corresponds to the sum of <code>sd2</code> and <code>nugs</code>.
</p>


<h3>Value</h3>

<p>list with elements
</p>

<ul>
<li> <p><code>mean</code>: kriging mean;
</p>
</li>
<li> <p><code>sd2</code>: kriging variance (filtered, e.g. without the nugget value)
</p>
</li>
<li> <p><code>cov</code>: predictive covariance matrix between <code>x</code> and <code>xprime</code>
</p>
</li>
<li> <p><code>nugs</code>: nugget value at each prediction location
</p>
</li></ul>


<hr>
<h2 id='rebuild'>Import and export of hetGP objects</h2><span id='topic+rebuild'></span><span id='topic+rebuild.homGP'></span><span id='topic+strip'></span><span id='topic+rebuild.hetGP'></span><span id='topic+rebuild.homTP'></span><span id='topic+rebuild.hetTP'></span>

<h3>Description</h3>

<p>Functions to make <code>hetGP</code> objects lighter before exporting them, and to reverse this after import.
The <code>rebuild</code> function may also be used to obtain more robust inverse of covariance matrices using <code><a href="MASS.html#topic+ginv">ginv</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rebuild(object, robust)

## S3 method for class 'homGP'
rebuild(object, robust = FALSE)

strip(object)

## S3 method for class 'hetGP'
rebuild(object, robust = FALSE)

## S3 method for class 'homTP'
rebuild(object, robust = FALSE)

## S3 method for class 'hetTP'
rebuild(object, robust = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rebuild_+3A_object">object</code></td>
<td>
<p><code>homGP</code> or <code>homTP</code> model without slot <code>Ki</code> (inverse covariance matrix),
or <code>hetGP</code> or <code>hetTP</code> model without slot <code>Ki</code> or <code>Kgi</code></p>
</td></tr>
<tr><td><code id="rebuild_+3A_robust">robust</code></td>
<td>
<p>if <code>TRUE</code> <code><a href="MASS.html#topic+ginv">ginv</a></code> is used for matrix inversion, otherwise it is done via Cholesky.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>object</code> with additional or removed slots.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(32)
## motorcycle data
library(MASS)
X &lt;- matrix(mcycle$times, ncol = 1)
Z &lt;- mcycle$accel
## Model fitting
model &lt;- mleHetGP(X = X, Z = Z, lower = 0.1, upper = 50)

# Check size
object.size(model)

# Remove internal elements, e.g., to save it
model &lt;- strip(model)

# Check new size
object.size(model)

# Now rebuild model, and use ginv instead
model &lt;- rebuild(model, robust = TRUE)
object.size(model)

</code></pre>

<hr>
<h2 id='scores'>Score and RMSE function
To asses the performance of the prediction, this function computes the root mean squared error and proper score function (also known as negative log-probability density).</h2><span id='topic+scores'></span>

<h3>Description</h3>

<p>Score and RMSE function
To asses the performance of the prediction, this function computes the root mean squared error and proper score function (also known as negative log-probability density).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>scores(model, Xtest, Ztest, return.rmse = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="scores_+3A_model">model</code></td>
<td>
<p><code>homGP</code> or <code>hetGP</code> model, including inverse matrices</p>
</td></tr>
<tr><td><code id="scores_+3A_xtest">Xtest</code></td>
<td>
<p>matrix of new design locations</p>
</td></tr>
<tr><td><code id="scores_+3A_ztest">Ztest</code></td>
<td>
<p>corresponding vector of observations, or alternatively, 
a matrix of size [nrow(Xtest) x number of replicates], a list of size nrow(Xtest) with a least one value per element</p>
</td></tr>
<tr><td><code id="scores_+3A_return.rmse">return.rmse</code></td>
<td>
<p>if <code>TRUE</code>, return the root mean squared error</p>
</td></tr>
</table>


<h3>References</h3>

<p>T. Gneiting, and A. Raftery (2007). Strictly Proper Scoring Rules, Prediction, and Estimation, Journal of the American Statistical Association, 102(477), 359-378.
</p>

<hr>
<h2 id='simul'>Conditional simulation for CRNGP</h2><span id='topic+simul'></span>

<h3>Description</h3>

<p>Conditional simulation for CRNGP
</p>


<h3>Usage</h3>

<pre><code class='language-R'>simul(object, Xgrid, ids, nsim, eps, seqseeds, check)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="simul_+3A_object">object</code></td>
<td>
<p><code>CRNGP</code> model</p>
</td></tr>
<tr><td><code id="simul_+3A_xgrid">Xgrid</code></td>
<td>
<p>matrix of (x, seed) locations where the simulation is performed. 
Where all design locations are matched with all seed values. In particular, it is assumed that each unique x values is matched with all seeds before going to the next x value.
The last column MUST correspond to seeds values. <code>Xgrid</code> must also contain the evaluated designs (e.g., in model$X0)</p>
</td></tr>
<tr><td><code id="simul_+3A_ids">ids</code></td>
<td>
<p>vector of indices corresponding to observed values in <code>Xgrid</code></p>
</td></tr>
<tr><td><code id="simul_+3A_nsim">nsim</code></td>
<td>
<p>number of simulations to return</p>
</td></tr>
<tr><td><code id="simul_+3A_eps">eps</code></td>
<td>
<p>jitter used in the Cholesky decomposition of the covariance matrix for numerical stability</p>
</td></tr>
<tr><td><code id="simul_+3A_seqseeds">seqseeds</code></td>
<td>
<p>is the seed sequence repeated (e.g., 1 2 3 1 2 3), else it is assumed to be ordered (e.g., 1 1 2 2 3 3)</p>
</td></tr>
<tr><td><code id="simul_+3A_check">check</code></td>
<td>
<p>if <code>TRUE</code>, check that Xgrid has the proper structure (slower)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Conditional simulation matrix.
</p>

<hr>
<h2 id='simul.CRNGP'>Fast conditional simulation for a CRNGP model</h2><span id='topic+simul.CRNGP'></span>

<h3>Description</h3>

<p>Fast conditional simulation for a CRNGP model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'CRNGP'
simul(
  object,
  Xgrid,
  ids = NULL,
  nsim = 1,
  eps = sqrt(.Machine$double.eps),
  seqseeds = TRUE,
  check = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="simul.CRNGP_+3A_object">object</code></td>
<td>
<p>a <code>CRNGP</code> model obtained with <code><a href="#topic+mleCRNGP">mleCRNGP</a></code></p>
</td></tr>
<tr><td><code id="simul.CRNGP_+3A_xgrid">Xgrid</code></td>
<td>
<p>matrix of (x, seed) locations where the simulation is performed.
The last column MUST correspond to seeds values. <code>Xgrid</code> must also contain the evaluated designs (e.g., in <code>object$X0</code>).
All design locations are matched with all seed values, either by increasing seed values or repeating the seed sequence.</p>
</td></tr>
<tr><td><code id="simul.CRNGP_+3A_ids">ids</code></td>
<td>
<p>vector of indices corresponding to observed values in <code>Xgrid</code></p>
</td></tr>
<tr><td><code id="simul.CRNGP_+3A_nsim">nsim</code></td>
<td>
<p>number of simulations to return</p>
</td></tr>
<tr><td><code id="simul.CRNGP_+3A_eps">eps</code></td>
<td>
<p>jitter used in the Cholesky decomposition of the covariance matrix for numerical stability</p>
</td></tr>
<tr><td><code id="simul.CRNGP_+3A_seqseeds">seqseeds</code></td>
<td>
<p>is the seed sequence repeated (e.g., 1 2 3 1 2 3), else it is assumed to be ordered (e.g., 1 1 2 2 3 3)</p>
</td></tr>
<tr><td><code id="simul.CRNGP_+3A_check">check</code></td>
<td>
<p>if <code>TRUE</code>, check that <code>Xgrid</code> has the proper structure (slower)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix of size <code>nrow(Xgrid) x nsim</code>.
</p>


<h3>References</h3>

<p>Chiles, J. P., &amp; Delfiner, P. (2012). Geostatistics: modeling spatial uncertainty (Vol. 713). John Wiley &amp; Sons. <br /> <br />
</p>
<p>Chevalier, C.; Emery, X.; Ginsbourger, D.
Fast Update of Conditional Simulation Ensembles
Mathematical Geosciences, 2014
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
##------------------------------------------------------------
## Example: Homoskedastic GP modeling on 2d sims
##------------------------------------------------------------
set.seed(2)
nx &lt;- 31
ns &lt;- 5
d &lt;- 2
x &lt;- as.matrix(expand.grid(seq(0,1, length.out = nx), seq(0,1, length.out = nx)))
s &lt;- matrix(seq(1, ns, length.out = ns))
Xgrid &lt;- as.matrix(expand.grid(seq(1, ns, length.out = ns), seq(0,1, length.out = nx), 
                               seq(0,1, length.out = nx)))
Xgrid &lt;- Xgrid[,c(2, 3, 1)]
g &lt;- 1e-6
theta &lt;- c(0.2, 0.5)
KX &lt;- cov_gen(x, theta = theta)
rho &lt;- 0.33
KS &lt;- matrix(rho, ns, ns)
diag(KS) &lt;- 1

YY &lt;- MASS::mvrnorm(n = 1, mu = rep(0, nx*nx*ns), Sigma = kronecker(KX, KS) + g * diag(nx*nx*ns))
YYmat &lt;- matrix(YY, ns, nx*nx)
filled.contour(matrix(YYmat[1,], nx))
filled.contour(matrix(YYmat[2,], nx))

ids &lt;- sample(1:nrow(Xgrid), 80)
X0 &lt;- Xgrid[ids,]
Y0 &lt;-  YY[ids]

# For 3d visualization
# library(rgl)
# plot3d(Xgrid[,1], Xgrid[,2], YY, col = 1 + (Xgrid[,3] - 1) %% 6)
# points3d(X0[,1], X0[,2], Y0, size = 10, col = 1 + ((X0[,3] - 1) %% 6))

model &lt;- mleCRNGP(X0, Y0, known = list(g = 1e-6))

preds &lt;- predict(model, x = Xgrid, xprime = Xgrid)
# surface3d(unique(Xgrid[1:nx^2,1]),unique(Xgrid[,2]), matrix(YY[Xgrid[,3]==1], nx), 
#   front = "lines", back = "lines")
# aspect3d(1, 1, 1)
# surface3d(unique(Xgrid[1:nx^2,1]),unique(Xgrid[,2]), matrix(preds$mean[Xgrid[,3]==1], nx), 
#   front = "lines", back = "lines", col = "red")

# Conditional realizations (classical way)
set.seed(2)
t0 &lt;- Sys.time()
SigmaCond &lt;- 1/2 * (preds$cov + t(preds$cov))
sims &lt;- t(chol(SigmaCond + diag(sqrt(.Machine$double.eps), nrow(Xgrid)))) %*% rnorm(nrow(Xgrid))
sims &lt;- sims + preds$mean
print(difftime(Sys.time(), t0))
# sims &lt;- MASS::mvrnorm(n = 1, mu = preds$mean, Sigma = 1/2 * (preds$cov + t(preds$cov)))
# plot3d(X0[,1], X0[,2], Y0, size = 10, col = 1 + ((X0[,3] - 1) %% 6))
# surface3d(unique(x[,1]), unique(x[,2]), matrix(sims[Xgrid[,3] == 1], nx), col = 1, 
#   front = "lines", back = "lines")
# surface3d(unique(x[,1]), unique(x[,2]), matrix(sims[Xgrid[,3] == 2], nx), col = 2, 
#   front = "lines", back = "lines")

# Alternative for conditional realizations 
# (note: here the design points are part of the simulation points)
set.seed(2)
t0 &lt;- Sys.time()
condreas &lt;- simul(model, Xgrid, ids = ids)
print(difftime(Sys.time(), t0))
# plot3d(X0[,1], X0[,2], Y0, size = 10, col = 1 + ((X0[,3] - 1) %% 6))
# surface3d(unique(x[,1]), unique(x[,2]), matrix(condreas[Xgrid[,3] == 1], nx), col = 1, 
#   front = "lines", back = "lines")
# surface3d(unique(x[,1]), unique(x[,2]), matrix(condreas[Xgrid[,3] == 2], nx), col = 2, 
#   front = "lines", back = "lines")

# Alternative using ordered seeds:
Xgrid2 &lt;- as.matrix(expand.grid(seq(0,1, length.out = nx), 
  seq(0,1, length.out = nx), seq(1, ns, length.out = ns)))
condreas2 &lt;- simul(model, Xgrid2, ids = ids, seqseeds = FALSE)

## Check that values at X0 are coherent:
# condreas[ids,1] - Y0
# sims[ids,1] - Y0

## Check that the empirical mean/covariance is correct
condreas2 &lt;- simul(model, Xgrid, ids = ids, nsim = 1000)
print(range(rowMeans(condreas2) - preds$mean))
print(range(cov(t(condreas2)) - preds$cov))

## End(Not run)
</code></pre>

<hr>
<h2 id='sirEval'>SIR test problem</h2><span id='topic+sirEval'></span><span id='topic+sirSimulate'></span>

<h3>Description</h3>

<p>Epidemiology problem, initial and rescaled to [0,1]^2 versions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sirEval(x)

sirSimulate(S0 = 1990, I0 = 10, M = S0 + I0, beta = 0.75, gamma = 0.5, imm = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sirEval_+3A_x">x</code></td>
<td>
<p>vector of size two</p>
</td></tr>
<tr><td><code id="sirEval_+3A_s0">S0</code></td>
<td>
<p>initial nunber of susceptibles</p>
</td></tr>
<tr><td><code id="sirEval_+3A_i0">I0</code></td>
<td>
<p>initial number of infected</p>
</td></tr>
<tr><td><code id="sirEval_+3A_m">M</code></td>
<td>
<p>total population</p>
</td></tr>
<tr><td><code id="sirEval_+3A_beta">beta</code>, <code id="sirEval_+3A_gamma">gamma</code>, <code id="sirEval_+3A_imm">imm</code></td>
<td>
<p>control rates</p>
</td></tr>
</table>


<h3>References</h3>

<p>R. Hu, M. Ludkovski (2017), Sequential Design for Ranking Response Surfaces, SIAM/ASA Journal on Uncertainty Quantification, 5(1), 212-239.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## SIR test problem illustration
ngrid &lt;- 10 # increase
xgrid &lt;- seq(0, 1, length.out = ngrid)
Xgrid &lt;- as.matrix(expand.grid(xgrid, xgrid))

nrep &lt;- 5 # increase
X &lt;- Xgrid[rep(1:nrow(Xgrid), nrep),]
Y &lt;- apply(X, 1, sirEval)
dataSIR &lt;- find_reps(X, Y)
filled.contour(xgrid, xgrid, matrix(lapply(dataSIR$Zlist, sd), ngrid),
               xlab = "Susceptibles", ylab = "Infecteds", color.palette = terrain.colors)

</code></pre>

<hr>
<h2 id='update.hetGP'>Update <code>"hetGP"</code>-class model fit with new observations</h2><span id='topic+update.hetGP'></span>

<h3>Description</h3>

<p>Fast update of existing <code>hetGP</code> model with new observations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'hetGP'
update(
  object,
  Xnew,
  Znew,
  ginit = 0.01,
  lower = NULL,
  upper = NULL,
  noiseControl = NULL,
  settings = NULL,
  known = NULL,
  maxit = 100,
  method = "quick",
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="update.hetGP_+3A_object">object</code></td>
<td>
<p>previously fit <code>"hetGP"</code>-class model</p>
</td></tr>
<tr><td><code id="update.hetGP_+3A_xnew">Xnew</code></td>
<td>
<p>matrix of new design locations; <code>ncol(Xnew)</code> must match the input dimension encoded in <code>object</code></p>
</td></tr>
<tr><td><code id="update.hetGP_+3A_znew">Znew</code></td>
<td>
<p>vector new observations at those design locations, of length <code>nrow(X)</code>. <code>NA</code>s can be passed, see Details</p>
</td></tr>
<tr><td><code id="update.hetGP_+3A_ginit">ginit</code></td>
<td>
<p>minimal value of the smoothing parameter (i.e., nugget of the noise process) for optimization initialization.
It is compared to the <code>g</code> hyperparameter in the object.</p>
</td></tr>
<tr><td><code id="update.hetGP_+3A_lower">lower</code>, <code id="update.hetGP_+3A_upper">upper</code>, <code id="update.hetGP_+3A_noisecontrol">noiseControl</code>, <code id="update.hetGP_+3A_settings">settings</code>, <code id="update.hetGP_+3A_known">known</code></td>
<td>
<p>optional bounds for mle optimization, see <code><a href="#topic+mleHetGP">mleHetGP</a></code>. 
If not provided, they are extracted from the existing model</p>
</td></tr>
<tr><td><code id="update.hetGP_+3A_maxit">maxit</code></td>
<td>
<p>maximum number of iterations for the internal L-BFGS-B optimization method; see <code><a href="stats.html#topic+optim">optim</a></code> for more details</p>
</td></tr>
<tr><td><code id="update.hetGP_+3A_method">method</code></td>
<td>
<p>one of <code>"quick"</code>, <code>"mixed"</code> see Details.</p>
</td></tr>
<tr><td><code id="update.hetGP_+3A_...">...</code></td>
<td>
<p>no other argument for this method.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The update can be performed with or without re-estimating hyperparameter.
In the first case, <code><a href="#topic+mleHetGP">mleHetGP</a></code> is called, based on previous values for initialization. 
The only missing values are the latent variables at the new points, that are initialized based on two possible update schemes in <code>method</code>:
</p>

<ul>
<li> <p><code>"quick"</code> the new delta value is the predicted nugs value from the previous noise model;
</p>
</li>
<li> <p><code>"mixed"</code> new values are taken as the barycenter between prediction given by the noise process and empirical variance. 
</p>
</li></ul>

<p>The subsequent number of MLE computations can be controlled with <code>maxit</code>.
</p>
<p>In case hyperparameters need not be updated, <code>maxit</code> can be set to <code>0</code>. 
In this case it is possible to pass <code>NA</code>s in <code>Znew</code>, then the model can still be used to provide updated variance predictions.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>##------------------------------------------------------------
## Sequential update example
##------------------------------------------------------------
set.seed(42)

## Spatially varying noise function
noisefun &lt;- function(x, coef = 1){
  return(coef * (0.05 + sqrt(abs(x)*20/(2*pi))/10))
}

## Initial data set
nvar &lt;- 1
n &lt;- 20
X &lt;- matrix(seq(0, 2 * pi, length=n), ncol = 1)
mult &lt;- sample(1:10, n, replace = TRUE)
X &lt;- rep(X, mult)
Z &lt;- sin(X) + rnorm(length(X), sd = noisefun(X))

## Initial fit
testpts &lt;- matrix(seq(0, 2*pi, length = 10*n), ncol = 1)
model &lt;- model_init &lt;- mleHetGP(X = X, Z = Z, lower = rep(0.1, nvar), 
  upper = rep(50, nvar), maxit = 1000)

## Visualizing initial predictive surface
preds &lt;- predict(x = testpts, model_init) 
plot(X, Z)
lines(testpts, preds$mean, col = "red")

## 10 fast update steps
nsteps &lt;- 5
npersteps &lt;- 10
for(i in 1:nsteps){
  newIds &lt;- sort(sample(1:(10*n), npersteps))
  
  newX &lt;- testpts[newIds, drop = FALSE] 
  newZ &lt;- sin(newX) + rnorm(length(newX), sd = noisefun(newX))
  points(newX, newZ, col = "blue", pch = 20)
  model &lt;- update(object = model, Xnew = newX, Znew = newZ)
  X &lt;- c(X, newX)
  Z &lt;- c(Z, newZ)
  plot(X, Z)
  print(model$nit_opt)
}

## Final predictions after 10 updates
p_fin &lt;- predict(x=testpts, model) 

## Visualizing the result by augmenting earlier plot
lines(testpts, p_fin$mean, col = "blue")
lines(testpts, qnorm(0.05, p_fin$mean, sqrt(p_fin$sd2)), col = "blue", lty = 2)
lines(testpts, qnorm(0.95, p_fin$mean, sqrt(p_fin$sd2)), col = "blue", lty = 2)
lines(testpts, qnorm(0.05, p_fin$mean, sqrt(p_fin$sd2 + p_fin$nugs)), 
  col = "blue", lty = 3)
lines(testpts, qnorm(0.95, p_fin$mean, sqrt(p_fin$sd2 + p_fin$nugs)), 
  col = "blue", lty = 3)

## Now compare to what you would get if you did a full batch fit instead
model_direct &lt;-  mleHetGP(X = X, Z = Z, maxit = 1000,
                          lower = rep(0.1, nvar), upper = rep(50, nvar),
                          init = list(theta = model_init$theta, k_theta_g = model_init$k_theta_g))
p_dir &lt;- predict(x = testpts, model_direct)
print(model_direct$nit_opt)
lines(testpts, p_dir$mean, col = "green")
lines(testpts, qnorm(0.05, p_dir$mean, sqrt(p_dir$sd2)), col = "green", 
  lty = 2)
lines(testpts, qnorm(0.95, p_dir$mean, sqrt(p_dir$sd2)), col = "green", 
  lty = 2)
lines(testpts, qnorm(0.05, p_dir$mean, sqrt(p_dir$sd2 + p_dir$nugs)), 
  col = "green", lty = 3)
lines(testpts, qnorm(0.95, p_dir$mean, sqrt(p_dir$sd2 + p_dir$nugs)), 
  col = "green", lty = 3)
lines(testpts, sin(testpts), col = "red", lty = 2)

## Compare outputs
summary(model_init)
summary(model)
summary(model_direct)

</code></pre>

<hr>
<h2 id='update.hetTP'>Update <code>"hetTP"</code>-class model fit with new observations</h2><span id='topic+update.hetTP'></span>

<h3>Description</h3>

<p>Fast update of existing <code>hetTP</code> model with new observations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'hetTP'
update(
  object,
  Xnew,
  Znew,
  ginit = 0.01,
  lower = NULL,
  upper = NULL,
  noiseControl = NULL,
  settings = NULL,
  known = NULL,
  maxit = 100,
  method = "quick",
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="update.hetTP_+3A_object">object</code></td>
<td>
<p>previously fit <code>"hetTP"</code>-class model</p>
</td></tr>
<tr><td><code id="update.hetTP_+3A_xnew">Xnew</code></td>
<td>
<p>matrix of new design locations; <code>ncol(Xnew)</code> must match the input dimension encoded in <code>object</code></p>
</td></tr>
<tr><td><code id="update.hetTP_+3A_znew">Znew</code></td>
<td>
<p>vector new observations at those design locations, of length <code>nrow(X)</code>. <code>NA</code>s can be passed, see Details</p>
</td></tr>
<tr><td><code id="update.hetTP_+3A_ginit">ginit</code></td>
<td>
<p>minimal value of the smoothing parameter (i.e., nugget of the noise process) for optimization initialisation.
It is compared to the <code>g</code> hyperparameter in the object.</p>
</td></tr>
<tr><td><code id="update.hetTP_+3A_lower">lower</code>, <code id="update.hetTP_+3A_upper">upper</code>, <code id="update.hetTP_+3A_noisecontrol">noiseControl</code>, <code id="update.hetTP_+3A_settings">settings</code>, <code id="update.hetTP_+3A_known">known</code></td>
<td>
<p>optional bounds for mle optimization, see <code><a href="#topic+mleHetTP">mleHetTP</a></code>. 
If not provided, they are extracted from the existing model</p>
</td></tr>
<tr><td><code id="update.hetTP_+3A_maxit">maxit</code></td>
<td>
<p>maximum number of iterations for the internal L-BFGS-B optimization method; see <code><a href="stats.html#topic+optim">optim</a></code> for more details</p>
</td></tr>
<tr><td><code id="update.hetTP_+3A_method">method</code></td>
<td>
<p>one of <code>"quick"</code>, <code>"mixed"</code> see Details.</p>
</td></tr>
<tr><td><code id="update.hetTP_+3A_...">...</code></td>
<td>
<p>no other argument for this method.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The update can be performed with or without re-estimating hyperparameter.
In the first case, <code><a href="#topic+mleHetTP">mleHetTP</a></code> is called, based on previous values for initialization. 
The only missing values are the latent variables at the new points, that are initialized based on two possible update schemes in <code>method</code>:
</p>

<ul>
<li> <p><code>"quick"</code> the new delta value is the predicted nugs value from the previous noise model;
</p>
</li>
<li> <p><code>"mixed"</code> new values are taken as the barycenter between prediction given by the noise process and empirical variance. 
</p>
</li></ul>

<p>The subsequent number of MLE computations can be controlled with <code>maxit</code>.
</p>
<p>In case hyperparameters need not be updated, <code>maxit</code> can be set to <code>0</code>. 
In this case it is possible to pass <code>NA</code>s in <code>Znew</code>, then the model can still be used to provide updated variance predictions.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>##------------------------------------------------------------
## Sequential update example
##------------------------------------------------------------
set.seed(42)

## Spatially varying noise function
noisefun &lt;- function(x, coef = 1){
  return(coef * (0.05 + sqrt(abs(x)*20/(2*pi))/10))
}

## Initial data set
nvar &lt;- 1
n &lt;- 20
X &lt;- matrix(seq(0, 2 * pi, length=n), ncol = 1)
mult &lt;- sample(1:10, n, replace = TRUE)
X &lt;- rep(X, mult)
Z &lt;- sin(X) + noisefun(X) * rt(length(X), df = 10)

## Initial fit
testpts &lt;- matrix(seq(0, 2*pi, length = 10*n), ncol = 1)
model &lt;- model_init &lt;- mleHetTP(X = X, Z = Z, lower = rep(0.1, nvar), 
  upper = rep(50, nvar), maxit = 1000)

## Visualizing initial predictive surface
preds &lt;- predict(x = testpts, model_init) 
plot(X, Z)
lines(testpts, preds$mean, col = "red")

## 10 fast update steps
nsteps &lt;- 5
npersteps &lt;- 10
for(i in 1:nsteps){
  newIds &lt;- sort(sample(1:(10*n), npersteps))
  
  newX &lt;- testpts[newIds, drop = FALSE] 
  newZ &lt;- sin(newX) + noisefun(newX) * rt(length(newX), df = 10)
  points(newX, newZ, col = "blue", pch = 20)
  model &lt;- update(object = model, Xnew = newX, Znew = newZ)
  X &lt;- c(X, newX)
  Z &lt;- c(Z, newZ)
  plot(X, Z)
  print(model$nit_opt)
}

## Final predictions after 10 updates
p_fin &lt;- predict(x=testpts, model) 

## Visualizing the result by augmenting earlier plot
lines(testpts, p_fin$mean, col = "blue")
lines(testpts, qnorm(0.05, p_fin$mean, sqrt(p_fin$sd2)), col = "blue", lty = 2)
lines(testpts, qnorm(0.95, p_fin$mean, sqrt(p_fin$sd2)), col = "blue", lty = 2)
lines(testpts, qnorm(0.05, p_fin$mean, sqrt(p_fin$sd2 + p_fin$nugs)), 
  col = "blue", lty = 3)
lines(testpts, qnorm(0.95, p_fin$mean, sqrt(p_fin$sd2 + p_fin$nugs)), 
  col = "blue", lty = 3)

## Now compare to what you would get if you did a full batch fit instead
model_direct &lt;-  mleHetTP(X = X, Z = Z, maxit = 1000,
                          lower = rep(0.1, nvar), upper = rep(50, nvar),
                          init = list(theta = model_init$theta, k_theta_g = model_init$k_theta_g))
p_dir &lt;- predict(x = testpts, model_direct)
print(model_direct$nit_opt)
lines(testpts, p_dir$mean, col = "green")
lines(testpts, qnorm(0.05, p_dir$mean, sqrt(p_dir$sd2)), col = "green", 
  lty = 2)
lines(testpts, qnorm(0.95, p_dir$mean, sqrt(p_dir$sd2)), col = "green", 
  lty = 2)
lines(testpts, qnorm(0.05, p_dir$mean, sqrt(p_dir$sd2 + p_dir$nugs)), 
  col = "green", lty = 3)
lines(testpts, qnorm(0.95, p_dir$mean, sqrt(p_dir$sd2 + p_dir$nugs)), 
  col = "green", lty = 3)
lines(testpts, sin(testpts), col = "red", lty = 2)

## Compare outputs
summary(model_init)
summary(model)
summary(model_direct)

</code></pre>

<hr>
<h2 id='update.homGP'>Fast <code>homGP</code>-update</h2><span id='topic+update.homGP'></span>

<h3>Description</h3>

<p>Update existing <code>homGP</code> model with new observations
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'homGP'
update(
  object,
  Xnew,
  Znew = NULL,
  lower = NULL,
  upper = NULL,
  noiseControl = NULL,
  known = NULL,
  maxit = 100,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="update.homGP_+3A_object">object</code></td>
<td>
<p>initial model of class <code>homGP</code></p>
</td></tr>
<tr><td><code id="update.homGP_+3A_xnew">Xnew</code></td>
<td>
<p>matrix of new design locations; <code>ncol(Xnew)</code> must match the input dimension encoded in object</p>
</td></tr>
<tr><td><code id="update.homGP_+3A_znew">Znew</code></td>
<td>
<p>vector new observations at those new design locations, of length <code>nrow(X)</code>. <code>NA</code>s can be passed, see Details</p>
</td></tr>
<tr><td><code id="update.homGP_+3A_lower">lower</code>, <code id="update.homGP_+3A_upper">upper</code>, <code id="update.homGP_+3A_noisecontrol">noiseControl</code>, <code id="update.homGP_+3A_known">known</code></td>
<td>
<p>optional bounds for MLE optimization, see <code><a href="#topic+mleHomGP">mleHomGP</a></code>.
If not provided, they are extracted from the existing model</p>
</td></tr>
<tr><td><code id="update.homGP_+3A_maxit">maxit</code></td>
<td>
<p>maximum number of iterations for the internal L-BFGS-B optimization method; see <code><a href="stats.html#topic+optim">optim</a></code> for more details</p>
</td></tr>
<tr><td><code id="update.homGP_+3A_...">...</code></td>
<td>
<p>no other argument for this method.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In case hyperparameters need not be updated, <code>maxit</code> can be set to <code>0</code>. 
In this case it is possible to pass <code>NA</code>s in <code>Znew</code>, then the model can still be used to provide updated variance predictions.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
##------------------------------------------------------------
## Example : Sequential Homoskedastic GP modeling 
##------------------------------------------------------------
set.seed(42)

## Spatially varying noise function
noisefun &lt;- function(x, coef = 1){
  return(coef * (0.05 + sqrt(abs(x)*20/(2*pi))/10))
}

nvar &lt;- 1
n &lt;- 10
X &lt;- matrix(seq(0, 2 * pi, length=n), ncol = 1)
mult &lt;- sample(1:10, n)
X &lt;- rep(X, mult)
Z &lt;- sin(X) + rnorm(length(X), sd = noisefun(X))

testpts &lt;- matrix(seq(0, 2*pi, length = 10*n), ncol = 1)
model &lt;- model_init &lt;- mleHomGP(X = X, Z = Z,
                                lower = rep(0.1, nvar), upper = rep(50, nvar))
preds &lt;- predict(x = testpts, object = model_init) 
plot(X, Z)
lines(testpts, preds$mean, col = "red")


nsteps &lt;- 10
for(i in 1:nsteps){
  newIds &lt;- sort(sample(1:(10*n), 10))
  
  newX &lt;- testpts[newIds, drop = FALSE] 
  newZ &lt;- sin(newX) + rnorm(length(newX), sd = noisefun(newX))
  points(newX, newZ, col = "blue", pch = 20)
  model &lt;- update(object = model, newX, newZ)
  X &lt;- c(X, newX)
  Z &lt;- c(Z, newZ)
  plot(X, Z)
  print(model$nit_opt)
}
p_fin &lt;- predict(x = testpts, object = model) 
lines(testpts, p_fin$mean, col = "blue")
lines(testpts, qnorm(0.05, p_fin$mean, sqrt(p_fin$sd2)), col = "blue", lty = 2)
lines(testpts, qnorm(0.95, p_fin$mean, sqrt(p_fin$sd2)), col = "blue", lty = 2)
lines(testpts, qnorm(0.05, p_fin$mean, sqrt(p_fin$sd2 + p_fin$nugs)),
      col = "blue", lty = 3)
lines(testpts, qnorm(0.95, p_fin$mean, sqrt(p_fin$sd2 + p_fin$nugs)),
      col = "blue", lty = 3)

model_direct &lt;-  mleHomGP(X = X, Z = Z, lower = rep(0.1, nvar), upper = rep(50, nvar))
p_dir &lt;- predict(x = testpts, object = model_direct)
print(model_direct$nit_opt)
lines(testpts, p_dir$mean, col = "green")
lines(testpts, qnorm(0.05, p_dir$mean, sqrt(p_dir$sd2)), col = "green", lty = 2)
lines(testpts, qnorm(0.95, p_dir$mean, sqrt(p_dir$sd2)), col = "green", lty = 2)
lines(testpts, qnorm(0.05, p_dir$mean, sqrt(p_dir$sd2 + p_dir$nugs)),
      col = "green", lty = 3)
lines(testpts, qnorm(0.95, p_dir$mean, sqrt(p_dir$sd2 + p_dir$nugs)),
      col = "green", lty = 3)

lines(testpts, sin(testpts), col = "red", lty = 2)

## Compare outputs
summary(model_init)
summary(model)
summary(model_direct)



## End(Not run)
</code></pre>

<hr>
<h2 id='update.homTP'>Fast <code>homTP</code>-update</h2><span id='topic+update.homTP'></span>

<h3>Description</h3>

<p>Update existing <code>homTP</code> model with new observations
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'homTP'
update(
  object,
  Xnew,
  Znew = NULL,
  lower = NULL,
  upper = NULL,
  noiseControl = NULL,
  known = NULL,
  maxit = 100,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="update.homTP_+3A_object">object</code></td>
<td>
<p>initial model of class <code>homTP</code></p>
</td></tr>
<tr><td><code id="update.homTP_+3A_xnew">Xnew</code></td>
<td>
<p>matrix of new design locations; <code>ncol(Xnew)</code> must match the input dimension encoded in object</p>
</td></tr>
<tr><td><code id="update.homTP_+3A_znew">Znew</code></td>
<td>
<p>vector new observations at those new design locations, of length <code>nrow(X)</code>. <code>NA</code>s can be passed, see Details</p>
</td></tr>
<tr><td><code id="update.homTP_+3A_lower">lower</code>, <code id="update.homTP_+3A_upper">upper</code>, <code id="update.homTP_+3A_noisecontrol">noiseControl</code>, <code id="update.homTP_+3A_known">known</code></td>
<td>
<p>optional bounds for MLE optimization, see <code><a href="#topic+mleHomTP">mleHomTP</a></code>.
If not provided, they are extracted from the existing model</p>
</td></tr>
<tr><td><code id="update.homTP_+3A_maxit">maxit</code></td>
<td>
<p>maximum number of iterations for the internal L-BFGS-B optimization method; see <code><a href="stats.html#topic+optim">optim</a></code> for more details</p>
</td></tr>
<tr><td><code id="update.homTP_+3A_...">...</code></td>
<td>
<p>no other argument for this method.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In case hyperparameters need not be updated, <code>maxit</code> can be set to <code>0</code>. 
In this case it is possible to pass <code>NA</code>s in <code>Znew</code>, then the model can still be used to provide updated variance predictions.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
##------------------------------------------------------------
## Example : Sequential Homoskedastic TP moding 
##------------------------------------------------------------
set.seed(42)

## Spatially varying noise function
noisefun &lt;- function(x, coef = 1){
  return(coef * (0.05 + sqrt(abs(x)*20/(2*pi))/10))
}

df_noise &lt;- 3
nvar &lt;- 1
n &lt;- 10
X &lt;- matrix(seq(0, 2 * pi, length=n), ncol = 1)
mult &lt;- sample(1:50, n, replace = TRUE)
X &lt;- rep(X, mult)
Z &lt;- sin(X) + noisefun(X) * rt(length(X), df = df_noise)

testpts &lt;- matrix(seq(0, 2*pi, length = 10*n), ncol = 1)
mod &lt;- mod_init &lt;- mleHomTP(X = X, Z = Z, covtype = "Matern5_2",
                                lower = rep(0.1, nvar), upper = rep(50, nvar))
preds &lt;- predict(x = testpts, object = mod_init) 
plot(X, Z)
lines(testpts, preds$mean, col = "red")


nsteps &lt;- 10
for(i in 1:nsteps){
  newIds &lt;- sort(sample(1:(10*n), 5))
  
  newX &lt;- testpts[rep(newIds, times = sample(1:50, length(newIds), replace = TRUE)), drop = FALSE] 
  newZ &lt;- sin(newX) + noisefun(newX) * rt(length(newX), df = df_noise)
  points(newX, newZ, col = "blue", pch = 20)
  mod &lt;- update(object = mod, newX, newZ)
  X &lt;- c(X, newX)
  Z &lt;- c(Z, newZ)
  plot(X, Z)
  print(mod$nit_opt)
}
p_fin &lt;- predict(x = testpts, object = mod) 
lines(testpts, p_fin$mean, col = "blue")
lines(testpts, p_fin$mean + sqrt(p_fin$sd2) * qt(0.05, df = mod$nu + length(Z)),
      col = "blue", lty = 2)
lines(testpts, p_fin$mean + sqrt(p_fin$sd2) * qt(0.95, df = mod$nu + length(Z)),
      col = "blue", lty = 2)
lines(testpts, p_fin$mean + sqrt(p_fin$sd2 + p_fin$nugs) * qt(0.05, df = mod$nu + length(Z)),
      col = "blue", lty = 3)
lines(testpts, p_fin$mean + sqrt(p_fin$sd2 + p_fin$nugs) * qt(0.95, df = mod$nu + length(Z)),
      col = "blue", lty = 3)

mod_dir &lt;-  mleHomTP(X = X, Z = Z, covtype = "Matern5_2",
                          lower = rep(0.1, nvar), upper = rep(50, nvar))
p_dir &lt;- predict(x = testpts, object = mod_dir)
print(mod_dir$nit_opt)
lines(testpts, p_dir$mean, col = "green")
lines(testpts, p_dir$mean + sqrt(p_dir$sd2) * qt(0.05, df = mod_dir$nu + length(Z)),
      col = "green", lty = 2)
lines(testpts, p_dir$mean + sqrt(p_dir$sd2) * qt(0.95, df = mod_dir$nu + length(Z)),
      col = "green", lty = 2)
lines(testpts, p_dir$mean + sqrt(p_dir$sd2 + p_dir$nugs) * qt(0.05, df = mod_dir$nu + length(Z)),
      col = "green", lty = 3)
lines(testpts, p_dir$mean + sqrt(p_dir$sd2 + p_dir$nugs) * qt(0.95, df = mod_dir$nu + length(Z)),
      col = "green", lty = 3)

lines(testpts, sin(testpts), col = "red", lty = 2)

## Compare outputs
summary(mod_init)
summary(mod)
summary(mod_dir)



## End(Not run)
</code></pre>

<hr>
<h2 id='Wij'>Compute double integral of the covariance kernel over a [0,1]^d domain</h2><span id='topic+Wij'></span>

<h3>Description</h3>

<p>Compute double integral of the covariance kernel over a [0,1]^d domain
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Wij(mu1, mu2 = NULL, theta, type)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Wij_+3A_mu1">mu1</code>, <code id="Wij_+3A_mu2">mu2</code></td>
<td>
<p>input locations considered</p>
</td></tr>
<tr><td><code id="Wij_+3A_theta">theta</code></td>
<td>
<p>lengthscale hyperparameter of the kernel</p>
</td></tr>
<tr><td><code id="Wij_+3A_type">type</code></td>
<td>
<p>kernel type, one of &quot;<code>Gaussian</code>&quot;, &quot;<code>Matern5_2</code>&quot; or &quot;<code>Matern3_2</code>&quot;, see <code><a href="#topic+cov_gen">cov_gen</a></code></p>
</td></tr>
</table>


<h3>References</h3>

<p>M. Binois, J. Huang, R. B. Gramacy, M. Ludkovski (2019), 
Replication or exploration? Sequential design for stochastic simulation experiments,
Technometrics, 61(1), 7-23.<br /> 
Preprint available on arXiv:1710.03206.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
