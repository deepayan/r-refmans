<!DOCTYPE html><html><head><title>Help for package rMIDAS</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {rMIDAS}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#add_bin_labels'><p>Reverse numeric conversion of binary vector</p></a></li>
<li><a href='#add_missingness'><p>Apply MAR missingness to data</p></a></li>
<li><a href='#coalesce_one_hot'><p>Coalesce one-hot encoding back to a single variable</p></a></li>
<li><a href='#col_minmax'><p>Scale numeric vector between 0 and 1</p></a></li>
<li><a href='#combine'><p>Estimate and combine regression models from multiply-imputed data</p></a></li>
<li><a href='#complete'><p>Impute missing values using imputation model</p></a></li>
<li><a href='#convert'><p>Pre-process data for Midas imputation</p></a></li>
<li><a href='#delete_rMIDAS_env'><p>Delete the rMIDAS Environment and Configuration</p></a></li>
<li><a href='#import_midas'><p>Instantiate Midas class</p></a></li>
<li><a href='#mid_py_setup'><p>Configure python for MIDAS imputation</p></a></li>
<li><a href='#midas_setup'><p>Manually set up Python connection</p></a></li>
<li><a href='#na_to_nan'><p>Replace NA missing values with NaN</p></a></li>
<li><a href='#overimpute'><p>Perform overimputation diagnostic test</p></a></li>
<li><a href='#python_configured'><p>Check whether Python is capable of executing example code</p></a></li>
<li><a href='#python_init'><p>Initialise connection to Python</p></a></li>
<li><a href='#reset_rMIDAS_env'><p>Reset the rMIDAS Environment Configuration</p></a></li>
<li><a href='#set_python_env'><p>Manually select python binary</p></a></li>
<li><a href='#skip_if_no_numpy'><p>Skip test where 'numpy' not available.</p></a></li>
<li><a href='#train'><p>Train an imputation model using Midas</p></a></li>
<li><a href='#undo_minmax'><p>Reverse minmax scaling of numeric vector</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Multiple Imputation with Denoising Autoencoders</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.0</td>
</tr>
<tr>
<td>Description:</td>
<td>A tool for multiply imputing missing data using 'MIDAS', a deep learning method based on denoising autoencoder neural networks (see Lall and Robinson, 2022; &lt;<a href="https://doi.org/10.1017%2Fpan.2020.49">doi:10.1017/pan.2020.49</a>&gt;). This algorithm offers significant accuracy and efficiency advantages over other multiple imputation strategies, particularly when applied to large datasets with complex features. Alongside interfacing with 'Python' to run the core algorithm, this package contains functions for processing data before and after model training, running imputation model diagnostics, generating multiple completed datasets, and estimating regression models on these datasets. For more information see Lall and Robinson (2023) &lt;<a href="https://doi.org/10.18637%2Fjss.v107.i09">doi:10.18637/jss.v107.i09</a>&gt;.</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.6.0), data.table, mltools, reticulate</td>
</tr>
<tr>
<td>Imports:</td>
<td>rappdirs, Rdpack</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat, knitr, rmarkdown</td>
</tr>
<tr>
<td>RdMacros:</td>
<td>Rdpack</td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>Python (&gt;= 3.6.0)</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.apache.org/licenses/LICENSE-2.0">Apache License (&ge; 2.0)</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/MIDASverse/rMIDAS">https://github.com/MIDASverse/rMIDAS</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/MIDASverse/rMIDAS/issues">https://github.com/MIDASverse/rMIDAS/issues</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-10-11 15:21:12 UTC; tsr</td>
</tr>
<tr>
<td>Author:</td>
<td>Thomas Robinson <a href="https://orcid.org/0000-0001-7097-1599"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre, cph],
  Ranjit Lall <a href="https://orcid.org/0000-0003-1455-3506"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut,
    cph],
  Alex Stenlake [ctb, cph],
  Elviss Dvinskis [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Thomas Robinson &lt;ts.robinson1994@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-10-11 15:50:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='add_bin_labels'>Reverse numeric conversion of binary vector</h2><span id='topic+add_bin_labels'></span>

<h3>Description</h3>

<p>Helper function to re-apply binary variable labels post-imputation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>add_bin_labels(x, one, zero, fast = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="add_bin_labels_+3A_x">x</code></td>
<td>
<p>A numeric vector or column, scaled between 0 and 1</p>
</td></tr>
<tr><td><code id="add_bin_labels_+3A_one">one</code></td>
<td>
<p>A character string, the label associated with binary value 1</p>
</td></tr>
<tr><td><code id="add_bin_labels_+3A_zero">zero</code></td>
<td>
<p>A character string, the label associated with binary value 0</p>
</td></tr>
<tr><td><code id="add_bin_labels_+3A_fast">fast</code></td>
<td>
<p>Boolean indicating whether to return binary value 1 if predicted probability &gt;= 0.5  (TRUE), or take random draw using predicted probability as weighting.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Vector of character strings corresponding to binary values
</p>


<h3>Examples</h3>

<pre><code class='language-R'>ex_bin &lt;- c(1,0,0,1,1,0,0,1,0)
cat &lt;- "cat"
dog &lt;- "dog"

add_bin_labels(x = ex_bin, one = cat, zero = dog)
</code></pre>

<hr>
<h2 id='add_missingness'>Apply MAR missingness to data</h2><span id='topic+add_missingness'></span>

<h3>Description</h3>

<p>Helper function to add missing values to data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>add_missingness(X, prop, cols = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="add_missingness_+3A_x">X</code></td>
<td>
<p>A data.frame or similar</p>
</td></tr>
<tr><td><code id="add_missingness_+3A_prop">prop</code></td>
<td>
<p>Numeric value between 0 and 1; the proportion of observations set to missing</p>
</td></tr>
<tr><td><code id="add_missingness_+3A_cols">cols</code></td>
<td>
<p>A vector of column names to be corrupted; if NULL, all columns are used</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Data with missing values
</p>


<h3>Examples</h3>

<pre><code class='language-R'>whole_data &lt;- data.frame(a = rnorm(1000),
                        b = rnorm(1000))

missing_data &lt;- add_missingness(whole_data, 0.1)
</code></pre>

<hr>
<h2 id='coalesce_one_hot'>Coalesce one-hot encoding back to a single variable</h2><span id='topic+coalesce_one_hot'></span>

<h3>Description</h3>

<p>Helper function to reverse one-hot encoding post-imputation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>coalesce_one_hot(X, var_name, fast = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="coalesce_one_hot_+3A_x">X</code></td>
<td>
<p>A data.frame, data.table or matrix, for a single variable</p>
</td></tr>
<tr><td><code id="coalesce_one_hot_+3A_var_name">var_name</code></td>
<td>
<p>A character string, with the original variable label</p>
</td></tr>
<tr><td><code id="coalesce_one_hot_+3A_fast">fast</code></td>
<td>
<p>Boolean, indicating whether to choose category with highest predicted probability (TRUE), or use predicted probabilities as weights in draw from random distribution</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of length equal to <code>nrow(X)</code>, containing categorical labels corresponding to the columns of <code>X</code>
</p>

<hr>
<h2 id='col_minmax'>Scale numeric vector between 0 and 1</h2><span id='topic+col_minmax'></span>

<h3>Description</h3>

<p>Helper function to scale numeric variables. Aids convergence of Midas model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>col_minmax(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="col_minmax_+3A_x">x</code></td>
<td>
<p>A numeric vector or column.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Vector scaled between 0 and 1
</p>


<h3>Examples</h3>

<pre><code class='language-R'>ex_num &lt;- runif(100,1,10)
scaled &lt;- col_minmax(ex_num)
</code></pre>

<hr>
<h2 id='combine'>Estimate and combine regression models from multiply-imputed data</h2><span id='topic+combine'></span>

<h3>Description</h3>

<p><code>combine()</code> calculates <em>m</em> individual regression models, then applies &quot;Rubin's Rules&quot; to produce a single, combined estimate of the regression parameters and uncertainty.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>combine(formula, df_list, dof_adjust = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="combine_+3A_formula">formula</code></td>
<td>
<p>A formula, or character string coercible to a formula</p>
</td></tr>
<tr><td><code id="combine_+3A_df_list">df_list</code></td>
<td>
<p>A list, containing data.frames or objects coercible to data.frames</p>
</td></tr>
<tr><td><code id="combine_+3A_dof_adjust">dof_adjust</code></td>
<td>
<p>Boolean, indicating whether or not to apply the Rubin and Barnard (1999) degrees of freedom adjustment for small-samples</p>
</td></tr>
<tr><td><code id="combine_+3A_...">...</code></td>
<td>
<p>Further arguments passed onto <code>glm()</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>Data.frame of combined model results.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(89)
test_dfs &lt;- lapply(1:5, function (x) data.frame(a = rnorm(1000),
                                                b = runif(1000),
                                                c = 2*rnorm(1000)))

midas_res &lt;- combine("a ~ b + c", df_list = test_dfs)
</code></pre>

<hr>
<h2 id='complete'>Impute missing values using imputation model</h2><span id='topic+complete'></span>

<h3>Description</h3>

<p>Having trained an imputation model, complete() produces <code>m</code> completed datasets, saved as a list.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>complete(
  mid_obj,
  m = 10L,
  unscale = TRUE,
  bin_label = TRUE,
  cat_coalesce = TRUE,
  fast = FALSE,
  file = NULL,
  file_root = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="complete_+3A_mid_obj">mid_obj</code></td>
<td>
<p>Object of class <code>midas</code>, the result of running <code>rMIDAS::train()</code></p>
</td></tr>
<tr><td><code id="complete_+3A_m">m</code></td>
<td>
<p>An integer, the number of completed datasets required</p>
</td></tr>
<tr><td><code id="complete_+3A_unscale">unscale</code></td>
<td>
<p>Boolean, indicating whether to unscale any columns that were previously minmax scaled between 0 and 1</p>
</td></tr>
<tr><td><code id="complete_+3A_bin_label">bin_label</code></td>
<td>
<p>Boolean, indicating whether to add back labels for binary columns</p>
</td></tr>
<tr><td><code id="complete_+3A_cat_coalesce">cat_coalesce</code></td>
<td>
<p>Boolean, indicating whether to decode the one-hot encoded categorical variables</p>
</td></tr>
<tr><td><code id="complete_+3A_fast">fast</code></td>
<td>
<p>Boolean, indicating whether to impute category with highest predicted probability (TRUE), or to use predicted probabilities to make weighted sample of category levels (FALSE)</p>
</td></tr>
<tr><td><code id="complete_+3A_file">file</code></td>
<td>
<p>Path to save completed datasets. If <code>NULL</code>, completed datasets are only loaded into memory.</p>
</td></tr>
<tr><td><code id="complete_+3A_file_root">file_root</code></td>
<td>
<p>A character string, used as the root for all filenames when saving completed datasets if a <code>filepath</code> is supplied. If no file_root is provided, completed datasets will be saved as &quot;file/midas_impute_yymmdd_hhmmss_m.csv&quot;</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For more information, see Lall and Robinson (2023): <a href="doi:10.18637/jss.v107.i09">doi:10.18637/jss.v107.i09</a>.
</p>


<h3>Value</h3>

<p>List of length <code>m</code>, each element of which is a completed data.frame (i.e. no missing values)
</p>


<h3>References</h3>

<p>Lall R, Robinson T (2023).
&ldquo;Efficient Multiple Imputation for Diverse Data in Python and R: MIDASpy and rMIDAS.&rdquo;
<em>Journal of Statistical Software</em>, <b>107</b>(9), 1&ndash;38.
<a href="https://doi.org/10.18637/jss.v107.i09">doi:10.18637/jss.v107.i09</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Generate raw data, with numeric, binary, and categorical variables
## Not run: 
# Run where Python available and configured correctly
if (python_configured()) {
set.seed(89)
n_obs &lt;- 10000
raw_data &lt;- data.table(a = sample(c("red","yellow","blue",NA),n_obs, replace = TRUE),
                       b = 1:n_obs,
                       c = sample(c("YES","NO",NA),n_obs,replace=TRUE),
                       d = runif(n_obs,1,10),
                       e = sample(c("YES","NO"), n_obs, replace = TRUE),
                       f = sample(c("male","female","trans","other",NA), n_obs, replace = TRUE))

# Names of bin./cat. variables
test_bin &lt;- c("c","e")
test_cat &lt;- c("a","f")

# Pre-process data
test_data &lt;- convert(raw_data,
                     bin_cols = test_bin,
                     cat_cols = test_cat,
                     minmax_scale = TRUE)

# Run imputations
test_imp &lt;- train(test_data)

# Generate datasets
complete_datasets &lt;- complete(test_imp, m = 5, fast = FALSE)

# Use Rubin's rules to combine m regression models
midas_pool &lt;- combine(formula = d~a+c+e+f,
                      complete_datasets)
}

## End(Not run)

</code></pre>

<hr>
<h2 id='convert'>Pre-process data for Midas imputation</h2><span id='topic+convert'></span>

<h3>Description</h3>

<p><code>convert</code> pre-processes datasets to enable user-friendly interface with the main <code>train()</code> function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>convert(data, bin_cols = NULL, cat_cols = NULL, minmax_scale = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="convert_+3A_data">data</code></td>
<td>
<p>Either an object of class <code>data.frame</code>, <code>data.table</code>, or a path to a regular, delimited file</p>
</td></tr>
<tr><td><code id="convert_+3A_bin_cols">bin_cols</code>, <code id="convert_+3A_cat_cols">cat_cols</code></td>
<td>
<p>A vector, column names corresponding to binary and categorical variables respectively</p>
</td></tr>
<tr><td><code id="convert_+3A_minmax_scale">minmax_scale</code></td>
<td>
<p>Boolean, indicating whether to scale all numeric columns between 0 and 1, to improve model convergence</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function has two advantages over manual pre-processing:
</p>

<ol>
<li><p> Utilises data.table for fast read-in and processing of large datasets
</p>
</li>
<li><p> Outputs an object that can be passed directly to <code>train()</code> without re-specifying column names etc.
</p>
</li></ol>

<p>For more information, see Lall and Robinson (2023): <a href="doi:10.18637/jss.v107.i09">doi:10.18637/jss.v107.i09</a>.
</p>


<h3>Value</h3>

<p>Returns custom S3 object of class &lsquo;midas_preproc&rsquo; containing:
</p>

<ul>
<li> <p><code>data</code> &ndash; processed version of input data,
</p>
</li>
<li> <p><code>bin_list</code> &ndash; vector of binary variable names
</p>
</li>
<li> <p><code>cat_lists</code> &ndash; embedded list of one-hot encoded categorical variable names
</p>
</li>
<li> <p><code>minmax_params</code> &ndash; list of min. and max. values for each numeric object scaled
</p>
</li></ul>

<p>List containing converted data, categorical and binary labels to be imported into the imputation model, and scaling parameters for post-imputation transformations.
</p>


<h3>References</h3>

<p>Lall R, Robinson T (2023).
&ldquo;Efficient Multiple Imputation for Diverse Data in Python and R: MIDASpy and rMIDAS.&rdquo;
<em>Journal of Statistical Software</em>, <b>107</b>(9), 1&ndash;38.
<a href="https://doi.org/10.18637/jss.v107.i09">doi:10.18637/jss.v107.i09</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data = data.frame(a = sample(c("red","yellow","blue",NA),100, replace = TRUE),
                  b = 1:100,
                  c = sample(c("YES","NO",NA),100,replace = TRUE),
                  d = runif(100),
                  e = sample(c("YES","NO"), 100, replace = TRUE),
                  f = sample(c("male","female","trans","other",NA), 100, replace = TRUE),
                  stringsAsFactors = FALSE)

bin &lt;- c("c","e")
cat &lt;- c("a","f")

convert(data, bin_cols = bin, cat_cols = cat)
</code></pre>

<hr>
<h2 id='delete_rMIDAS_env'>Delete the rMIDAS Environment and Configuration</h2><span id='topic+delete_rMIDAS_env'></span>

<h3>Description</h3>

<p>Deletes both the virtual environment and the configuration file for the rMIDAS package.
After deletion, it is necessary to restart the R session and then load the rMIDAS package once more.
This will trigger the setup process again.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>delete_rMIDAS_env()
</code></pre>


<h3>Value</h3>

<p>A message indicating the completion of the deletion process.
</p>

<hr>
<h2 id='import_midas'>Instantiate Midas class</h2><span id='topic+import_midas'></span>

<h3>Description</h3>

<p>Import Midas class into R environment, and instantiates passed parameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>import_midas(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="import_midas_+3A_...">...</code></td>
<td>
<p>Arguments passed to the MIDAS class for instantiating network</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Object of class 'midas'
</p>

<hr>
<h2 id='mid_py_setup'>Configure python for MIDAS imputation</h2><span id='topic+mid_py_setup'></span>

<h3>Description</h3>

<p>This helper function checks if the required Python dependencies are installed, and if not, checks with user before installing them.
Users should not call this function directly. Users can set a custom python install using <code>set_python_env()</code> so long as this is done prior to the first call to <code>train()</code> or <code>complete()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mid_py_setup()
</code></pre>

<hr>
<h2 id='midas_setup'>Manually set up Python connection</h2><span id='topic+midas_setup'></span>

<h3>Description</h3>

<p>This function allows users to initialise a custom Python configuration to run MIDAS, having manually set a Python version using <code>reticulate::use_python</code>, <code>reticulate::use_virtualenv</code>, <code>reticulate::use_condaenv</code>, or <code>reticulate::use_miniconda</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>midas_setup()
</code></pre>


<h3>Note</h3>

<p>This function is primarily for users who wish to have complete control over configuring Python versions and environments.
</p>
<p>This function call is <strong>not</strong> required if users either use the <code>rMIDAS::set_python_env()</code> function or leave settings at their default.
</p>
<p>If users set a custom binary/environment, this must be completed prior to the first call to either <code>train()</code> or <code>complete()</code>.
</p>

<hr>
<h2 id='na_to_nan'>Replace NA missing values with NaN</h2><span id='topic+na_to_nan'></span>

<h3>Description</h3>

<p>Helper function to convert <code>NA</code> values in a data.frame to <code>NaN</code>. This ensures the correct conversion of missing values when reticulate converts R objects to their Python equivalent. See the reticulate package documentation on type conversions for more information.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>na_to_nan(df)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="na_to_nan_+3A_df">df</code></td>
<td>
<p>Data frame, or object coercible to one.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Data frame with <code>NA</code> values replaced with <code>NaN</code> values.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>na_to_nan(data.frame(a = c(1,NA,0,0,NA,NA)))
</code></pre>

<hr>
<h2 id='overimpute'>Perform overimputation diagnostic test</h2><span id='topic+overimpute'></span>

<h3>Description</h3>

<p><code>overimpute()</code> spikes additional missingness into the input data and reports imputation accuracy at training intervals specified by the user.
<code>overimpute()</code> works like <code>train()</code> &ndash; users must specify input data, binary and categorical columns (if data is not generated via <code>convert()</code>, model parameters for the neural network, and then overimputation parameters (see below for full details).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>overimpute(
  data,
  binary_columns = NULL,
  softmax_columns = NULL,
  spikein = 0.3,
  training_epochs,
  report_ival = 35,
  plot_vars = FALSE,
  skip_plot = FALSE,
  spike_seed = NULL,
  save_path = "",
  layer_structure = c(256, 256, 256),
  learn_rate = 4e-04,
  input_drop = 0.8,
  seed = 123L,
  train_batch = 16L,
  latent_space_size = 4,
  cont_adj = 1,
  binary_adj = 1,
  softmax_adj = 1,
  dropout_level = 0.5,
  vae_layer = FALSE,
  vae_alpha = 1,
  vae_sample_var = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="overimpute_+3A_data">data</code></td>
<td>
<p>A data.frame (or coercible) object, or an object of class <code>midas_pre</code> created from rMIDAS::convert()</p>
</td></tr>
<tr><td><code id="overimpute_+3A_binary_columns">binary_columns</code></td>
<td>
<p>A vector of column names, containing binary variables. NOTE: if <code>data</code> is a <code>midas_pre</code> object, this argument will be overwritten.</p>
</td></tr>
<tr><td><code id="overimpute_+3A_softmax_columns">softmax_columns</code></td>
<td>
<p>A list of lists, each internal list corresponding to a single categorical variable and containing names of the one-hot encoded variable names. NOTE: if <code>data</code> is a <code>midas_pre</code> object, this argument will be overwritten.</p>
</td></tr>
<tr><td><code id="overimpute_+3A_spikein">spikein</code></td>
<td>
<p>A numeric between 0 and 1; the proportion of observed values in the input dataset to be randomly removed.</p>
</td></tr>
<tr><td><code id="overimpute_+3A_training_epochs">training_epochs</code></td>
<td>
<p>An integer, specifying the number of overimputation training epochs.</p>
</td></tr>
<tr><td><code id="overimpute_+3A_report_ival">report_ival</code></td>
<td>
<p>An integer, specifying the number of overimputation training epochs between calculations of loss. Shorter intervals provide a more granular view of model performance but slow down the overimputation process.</p>
</td></tr>
<tr><td><code id="overimpute_+3A_plot_vars">plot_vars</code></td>
<td>
<p>Boolean, specifies whether to plot the distribution of original versus overimputed values. This takes the form of a density plot for continuous variables and a barplot for categorical variables (showing proportions of each class).</p>
</td></tr>
<tr><td><code id="overimpute_+3A_skip_plot">skip_plot</code></td>
<td>
<p>Boolean, specifies whether to suppress the main graphical output. This may be desirable when users are conducting a series of overimputation exercises and are primarily interested in the console output. <strong>Note</strong>, when <code>skip_plot = FALSE</code>, users must manually close the resulting pyplot window before the code will terminate.</p>
</td></tr>
<tr><td><code id="overimpute_+3A_spike_seed">spike_seed</code>, <code id="overimpute_+3A_seed">seed</code></td>
<td>
<p>An integer, to initialize the pseudo-random number generators. Separate seeds can be provided for the spiked-in missingness and imputation, otherwise <code>spike_seed</code> is set to <code>seed</code> (default = 123L).</p>
</td></tr>
<tr><td><code id="overimpute_+3A_save_path">save_path</code></td>
<td>
<p>String, indicating path to directory to save overimputation figures. Users should include a trailing &quot;/&quot; at the end of the path i.e. save_path = &quot;path/to/figures/&quot;.</p>
</td></tr>
<tr><td><code id="overimpute_+3A_layer_structure">layer_structure</code></td>
<td>
<p>A vector of integers, The number of nodes in each layer of the network (default = <code>c(256, 256, 256)</code>, denoting a three-layer network with 256 nodes per layer). Larger networks can learn more complex data structures but require longer training and are more prone to overfitting.</p>
</td></tr>
<tr><td><code id="overimpute_+3A_learn_rate">learn_rate</code></td>
<td>
<p>A number, the learning rate <code class="reqn">\gamma</code> (default = 0.0001), which controls the size of the weight adjustment in each training epoch. In general, higher values reduce training time at the expense of less accurate results.</p>
</td></tr>
<tr><td><code id="overimpute_+3A_input_drop">input_drop</code></td>
<td>
<p>A number between 0 and 1. The probability of corruption for input columns in training mini-batches (default = 0.8). Higher values increase training time but reduce the risk of overfitting. In our experience, values between 0.7 and 0.95 deliver the best performance.</p>
</td></tr>
<tr><td><code id="overimpute_+3A_train_batch">train_batch</code></td>
<td>
<p>An integer, the number of observations in training mini-batches (default = 16).</p>
</td></tr>
<tr><td><code id="overimpute_+3A_latent_space_size">latent_space_size</code></td>
<td>
<p>An integer, the number of normal dimensions used to parameterize the latent space.</p>
</td></tr>
<tr><td><code id="overimpute_+3A_cont_adj">cont_adj</code></td>
<td>
<p>A number, weights the importance of continuous variables in the loss function</p>
</td></tr>
<tr><td><code id="overimpute_+3A_binary_adj">binary_adj</code></td>
<td>
<p>A number, weights the importance of binary variables in the loss function</p>
</td></tr>
<tr><td><code id="overimpute_+3A_softmax_adj">softmax_adj</code></td>
<td>
<p>A number, weights the importance of categorical variables in the loss function</p>
</td></tr>
<tr><td><code id="overimpute_+3A_dropout_level">dropout_level</code></td>
<td>
<p>A number between 0 and 1, determines the number of nodes dropped to &quot;thin&quot; the network</p>
</td></tr>
<tr><td><code id="overimpute_+3A_vae_layer">vae_layer</code></td>
<td>
<p>Boolean, specifies whether to include a variational autoencoder layer in the network</p>
</td></tr>
<tr><td><code id="overimpute_+3A_vae_alpha">vae_alpha</code></td>
<td>
<p>A number, the strength of the prior imposed on the Kullback-Leibler divergence term in the variational autoencoder loss functions.</p>
</td></tr>
<tr><td><code id="overimpute_+3A_vae_sample_var">vae_sample_var</code></td>
<td>
<p>A number, the sampling variance of the normal distributions used to parameterize the latent space.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Accuracy is measured as the RMSE of imputed values versus actual values for continuous variables and classification error for categorical variables (i.e., the fraction of correctly predicted classes subtracted from 1).
Both metrics are reported in two forms:
</p>

<ol>
<li><p> their summed value over all Monte Carlo samples from the estimated missing-data posterior &ndash; &quot;Aggregated RMSE&quot; and &quot;Aggregated softmax error&rdquo;;
</p>
</li>
<li><p> their aggregated value divided by the number of such samples &ndash; &quot;Individual RMSE&quot; and &quot;Individual softmax error&quot;.
</p>
</li></ol>

<p>In the final model, we recommend selecting the number of training epochs that minimizes the average value of these metrics &mdash; weighted by the proportion (or substantive importance) of continuous and categorical variables &mdash; in the overimputation exercise.  This &ldquo;early stopping&rdquo; rule reduces the risk of overtraining and thus, in effect, serves as an extra layer of regularization in the network.
</p>
<p>For more information, see Lall and Robinson (2023): <a href="doi:10.18637/jss.v107.i09">doi:10.18637/jss.v107.i09</a>.
</p>


<h3>Value</h3>

<p>Object of class <code>midas</code>, and outputs both overimputation loss values to the console and generates overimputation graphs.
</p>


<h3>References</h3>

<p>Lall R, Robinson T (2023).
&ldquo;Efficient Multiple Imputation for Diverse Data in Python and R: MIDASpy and rMIDAS.&rdquo;
<em>Journal of Statistical Software</em>, <b>107</b>(9), 1&ndash;38.
<a href="https://doi.org/10.18637/jss.v107.i09">doi:10.18637/jss.v107.i09</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+train">train</a></code> for the main imputation function.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Run where Python initialised and configured correctly
if (python_configured()) {

raw_data &lt;- data.table(a = sample(c("red","yellow","blue",NA),1000, replace = TRUE),
                         b = 1:1000,
                         c = sample(c("YES","NO",NA),1000,replace=TRUE),
                         d = runif(1000,1,10),
                         e = sample(c("YES","NO"), 1000, replace = TRUE),
                         f = sample(c("male","female","trans","other",NA), 1000, replace = TRUE))

# Names of bin./cat. variables
test_bin &lt;- c("c","e")
test_cat &lt;- c("a","f")

# Pre-process data
test_data &lt;- convert(raw_data,
                       bin_cols = test_bin,
                       cat_cols = test_cat,
                       minmax_scale = TRUE)

# Overimpute - without plots
test_imp &lt;- overimpute(test_data,
                       spikein = 0.3,
                       plot_vars = FALSE,
                       skip_plot = TRUE,
                       training_epochs = 10,
                       report_ival = 5)
}

## End(Not run)
</code></pre>

<hr>
<h2 id='python_configured'>Check whether Python is capable of executing example code</h2><span id='topic+python_configured'></span>

<h3>Description</h3>

<p>Checks if each Python dependency is available.
This function is called within some examples to ensure code executes properly.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>python_configured()
</code></pre>


<h3>Value</h3>

<p><code>NULL</code>
</p>

<hr>
<h2 id='python_init'>Initialise connection to Python</h2><span id='topic+python_init'></span>

<h3>Description</h3>

<p>Internal function. Checks if Python has already been initialised, and if not, completes the required setup to run the MIDAS algorithm.
This function is called automatically, and users should not call it directly.
To configure which Python install/environment/conda is used, see documentation for <code>set_python_env()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>python_init()
</code></pre>

<hr>
<h2 id='reset_rMIDAS_env'>Reset the rMIDAS Environment Configuration</h2><span id='topic+reset_rMIDAS_env'></span>

<h3>Description</h3>

<p>Resets the configuration for the rMIDAS package by deleting the configuration file.
Once the configuration is reset, it is necessary to restart the R session
and then load the rMIDAS package once more.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>reset_rMIDAS_env()
</code></pre>


<h3>Value</h3>

<p>A message indicating the completion of the reset process.
</p>

<hr>
<h2 id='set_python_env'>Manually select python binary</h2><span id='topic+set_python_env'></span>

<h3>Description</h3>

<p>This function allows users to set a custom python binary, virtualenv or conda environment, from which the MIDAS algorithm is run.
Users comfortable with reticulate can configure Python manually using <code>reticulate::use_</code>.
Note: If users wish to set a custom binary/environment, this must be completed prior to the first call to either <code>train()</code> or <code>complete()</code>. The same is true if users use the reticulate package directly.
If users wish to switch to a different Python binaries, R must be restarted prior to calling this function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>set_python_env(x, type = "auto", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="set_python_env_+3A_x">x</code></td>
<td>
<p>Character string, path to python binary, or directory of virtualenv, or name of conda environment</p>
</td></tr>
<tr><td><code id="set_python_env_+3A_type">type</code></td>
<td>
<p>Character string, specifies whether to set a python binary (&quot;auto&quot;), &quot;virtualenv&quot;, or &quot;conda&quot;</p>
</td></tr>
<tr><td><code id="set_python_env_+3A_...">...</code></td>
<td>
<p>Further arguments passed to <code>reticulate::use_condaenv()</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>Boolean indicating whether the custom python environment was activated.
</p>

<hr>
<h2 id='skip_if_no_numpy'>Skip test where 'numpy' not available.</h2><span id='topic+skip_if_no_numpy'></span>

<h3>Description</h3>

<p>Check if Python's numpy is available, and skip test if not.
This function is called within some tests to ensure server tests involving <code>reticulate</code> calls execute properly.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>skip_if_no_numpy()
</code></pre>


<h3>Value</h3>

<p><code>NULL</code>
</p>

<hr>
<h2 id='train'>Train an imputation model using Midas</h2><span id='topic+train'></span>

<h3>Description</h3>

<p>Build and run a MIDAS neural network on the supplied missing data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>train(
  data,
  binary_columns = NULL,
  softmax_columns = NULL,
  training_epochs = 10L,
  layer_structure = c(256, 256, 256),
  learn_rate = 4e-04,
  input_drop = 0.8,
  seed = 123L,
  train_batch = 16L,
  latent_space_size = 4,
  cont_adj = 1,
  binary_adj = 1,
  softmax_adj = 1,
  dropout_level = 0.5,
  vae_layer = FALSE,
  vae_alpha = 1,
  vae_sample_var = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="train_+3A_data">data</code></td>
<td>
<p>A data.frame (or coercible) object, or an object of class <code>midas_pre</code> created from rMIDAS::convert()</p>
</td></tr>
<tr><td><code id="train_+3A_binary_columns">binary_columns</code></td>
<td>
<p>A vector of column names, containing binary variables. NOTE: if <code>data</code> is a <code>midas_pre</code> object, this argument will be overwritten.</p>
</td></tr>
<tr><td><code id="train_+3A_softmax_columns">softmax_columns</code></td>
<td>
<p>A list of lists, each internal list corresponding to a single categorical variable and containing names of the one-hot encoded variable names. NOTE: if <code>data</code> is a <code>midas_pre</code> object, this argument will be overwritten.</p>
</td></tr>
<tr><td><code id="train_+3A_training_epochs">training_epochs</code></td>
<td>
<p>An integer, indicating the number of forward passes to conduct when running the model.</p>
</td></tr>
<tr><td><code id="train_+3A_layer_structure">layer_structure</code></td>
<td>
<p>A vector of integers, The number of nodes in each layer of the network (default = <code>c(256, 256, 256)</code>, denoting a three-layer network with 256 nodes per layer). Larger networks can learn more complex data structures but require longer training and are more prone to overfitting.</p>
</td></tr>
<tr><td><code id="train_+3A_learn_rate">learn_rate</code></td>
<td>
<p>A number, the learning rate <code class="reqn">\gamma</code> (default = 0.0001), which controls the size of the weight adjustment in each training epoch. In general, higher values reduce training time at the expense of less accurate results.</p>
</td></tr>
<tr><td><code id="train_+3A_input_drop">input_drop</code></td>
<td>
<p>A number between 0 and 1. The probability of corruption for input columns in training mini-batches (default = 0.8). Higher values increase training time but reduce the risk of overfitting. In our experience, values between 0.7 and 0.95 deliver the best performance.</p>
</td></tr>
<tr><td><code id="train_+3A_seed">seed</code></td>
<td>
<p>An integer, the value to which Python's pseudo-random number generator is initialized. This enables users to ensure that data shuffling, weight and bias initialization, and missingness indicator vectors are reproducible.</p>
</td></tr>
<tr><td><code id="train_+3A_train_batch">train_batch</code></td>
<td>
<p>An integer, the number of observations in training mini-batches (default = 16).</p>
</td></tr>
<tr><td><code id="train_+3A_latent_space_size">latent_space_size</code></td>
<td>
<p>An integer, the number of normal dimensions used to parameterize the latent space.</p>
</td></tr>
<tr><td><code id="train_+3A_cont_adj">cont_adj</code></td>
<td>
<p>A number, weights the importance of continuous variables in the loss function</p>
</td></tr>
<tr><td><code id="train_+3A_binary_adj">binary_adj</code></td>
<td>
<p>A number, weights the importance of binary variables in the loss function</p>
</td></tr>
<tr><td><code id="train_+3A_softmax_adj">softmax_adj</code></td>
<td>
<p>A number, weights the importance of categorical variables in the loss function</p>
</td></tr>
<tr><td><code id="train_+3A_dropout_level">dropout_level</code></td>
<td>
<p>A number between 0 and 1, determines the number of nodes dropped to &quot;thin&quot; the network</p>
</td></tr>
<tr><td><code id="train_+3A_vae_layer">vae_layer</code></td>
<td>
<p>Boolean, specifies whether to include a variational autoencoder layer in the network</p>
</td></tr>
<tr><td><code id="train_+3A_vae_alpha">vae_alpha</code></td>
<td>
<p>A number, the strength of the prior imposed on the Kullback-Leibler divergence term in the variational autoencoder loss functions.</p>
</td></tr>
<tr><td><code id="train_+3A_vae_sample_var">vae_sample_var</code></td>
<td>
<p>A number, the sampling variance of the normal distributions used to parameterize the latent space.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For more information, see Lall and Robinson (2023): <a href="doi:10.18637/jss.v107.i09">doi:10.18637/jss.v107.i09</a>.
</p>


<h3>Value</h3>

<p>Object of class <code>midas</code> from which completed datasets can be drawn, using <code>rMIDAS::complete()</code>
</p>


<h3>References</h3>

<p>Lall R, Robinson T (2023).
&ldquo;Efficient Multiple Imputation for Diverse Data in Python and R: MIDASpy and rMIDAS.&rdquo;
<em>Journal of Statistical Software</em>, <b>107</b>(9), 1&ndash;38.
<a href="https://doi.org/10.18637/jss.v107.i09">doi:10.18637/jss.v107.i09</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Generate raw data, with numeric, binary, and categorical variables
## Not run: 
# Run where Python available and configured correctly
if (python_configured()) {
set.seed(89)
n_obs &lt;- 10000
raw_data &lt;- data.table(a = sample(c("red","yellow","blue",NA),n_obs, replace = TRUE),
                       b = 1:n_obs,
                       c = sample(c("YES","NO",NA),n_obs,replace=TRUE),
                       d = runif(n_obs,1,10),
                       e = sample(c("YES","NO"), n_obs, replace = TRUE),
                       f = sample(c("male","female","trans","other",NA), n_obs, replace = TRUE))

# Names of bin./cat. variables
test_bin &lt;- c("c","e")
test_cat &lt;- c("a","f")

# Pre-process data
test_data &lt;- convert(raw_data,
                     bin_cols = test_bin,
                     cat_cols = test_cat,
                     minmax_scale = TRUE)

# Run imputations
test_imp &lt;- train(test_data)

# Generate datasets
complete_datasets &lt;- complete(test_imp, m = 5, fast = FALSE)

# Use Rubin's rules to combine m regression models
midas_pool &lt;- combine(formula = d~a+c+e+f,
                      complete_datasets)
}

## End(Not run)

</code></pre>

<hr>
<h2 id='undo_minmax'>Reverse minmax scaling of numeric vector</h2><span id='topic+undo_minmax'></span>

<h3>Description</h3>

<p>Helper function to reverse minmax scaling applied in the pre-processing step.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>undo_minmax(s, s_min, s_max)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="undo_minmax_+3A_s">s</code></td>
<td>
<p>A numeric vector or column, scaled between 0 and 1.</p>
</td></tr>
<tr><td><code id="undo_minmax_+3A_s_min">s_min</code></td>
<td>
<p>A numeric value, the minimum of the unscaled vector</p>
</td></tr>
<tr><td><code id="undo_minmax_+3A_s_max">s_max</code></td>
<td>
<p>A numeric value, the maximum of the unscaled vector</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Vector re-scaled using original parameters <code>s_min</code> and <code>s_max</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>ex_num &lt;- runif(100,1,10)
scaled &lt;- col_minmax(ex_num)
undo_scale &lt;- undo_minmax(scaled, s_min = min(ex_num), s_max = max(ex_num))

# Prove two are identical
all.equal(ex_num, undo_scale)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
