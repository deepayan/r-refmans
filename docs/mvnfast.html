<!DOCTYPE html><html lang="en"><head><title>Help for package mvnfast</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {mvnfast}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#dmixn'><p>Fast density computation for mixture of multivariate normal distributions.</p></a></li>
<li><a href='#dmixt'><p>Fast density computation for mixture of multivariate Student's t distributions.</p></a></li>
<li><a href='#dmvn'><p>Fast computation of the multivariate normal density.</p></a></li>
<li><a href='#dmvt'><p>Fast computation of the multivariate Student's t density.</p></a></li>
<li><a href='#maha'><p>Fast computation of squared mahalanobis distance between all rows of <code>X</code> and the vector <code>mu</code> with respect to sigma.</p></a></li>
<li><a href='#ms'><p>Mean-shift mode seeking algorithm</p></a></li>
<li><a href='#rmixn'><p>Fast simulation of r.v.s from a mixture of multivariate normal densities</p></a></li>
<li><a href='#rmixt'><p>Fast simulation of r.v.s from a mixture of multivariate Student's t densities</p></a></li>
<li><a href='#rmvn'><p>Fast simulation of multivariate normal random variables</p></a></li>
<li><a href='#rmvt'><p>Fast simulation of multivariate Student's t random variables</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Fast Multivariate Normal and Student's t Methods</td>
</tr>
<tr>
<td>Version:</td>
<td>0.2.8</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-02-20</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Matteo Fasiolo &lt;matteo.fasiolo@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Provides computationally efficient tools related to the
    multivariate normal and Student's t distributions. The main functionalities
    are: simulating multivariate random vectors, evaluating multivariate normal or
    Student's t densities and Mahalanobis distances. These tools are very efficient
    thanks to the use of C++ code and of the OpenMP API.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2.0)]</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/mfasiolo/mvnfast/">https://github.com/mfasiolo/mvnfast/</a></td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown, testthat, mvtnorm, microbenchmark, MASS,
plyr, RhpcBLASctl</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo, BH</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.2</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-02-23 11:38:10 UTC; mf15002</td>
</tr>
<tr>
<td>Author:</td>
<td>Matteo Fasiolo [aut, cre],
  Thijs van den Berg [ctb]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-02-23 12:40:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='dmixn'>Fast density computation for mixture of multivariate normal distributions.</h2><span id='topic+dmixn'></span>

<h3>Description</h3>

<p>Fast density computation for mixture of multivariate normal distributions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dmixn(X, mu, sigma, w, log = FALSE, ncores = 1, isChol = FALSE, A = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="dmixn_+3A_x">X</code></td>
<td>
<p>matrix n by d where each row is a d dimensional random vector. Alternatively <code>X</code> can be a d-dimensional vector.</p>
</td></tr>
<tr><td><code id="dmixn_+3A_mu">mu</code></td>
<td>
<p>an (m x d) matrix, where m is the number of mixture components.</p>
</td></tr>
<tr><td><code id="dmixn_+3A_sigma">sigma</code></td>
<td>
<p>as list of m covariance matrices (d x d) on for each mixture component. 
Alternatively it can be a list of m cholesky decomposition of the covariance. 
In that case <code>isChol</code> should be set to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="dmixn_+3A_w">w</code></td>
<td>
<p>vector of length m, containing the weights of the mixture components.</p>
</td></tr>
<tr><td><code id="dmixn_+3A_log">log</code></td>
<td>
<p>boolean set to true the logarithm of the pdf is required.</p>
</td></tr>
<tr><td><code id="dmixn_+3A_ncores">ncores</code></td>
<td>
<p>Number of cores used. The parallelization will take place only if OpenMP is supported.</p>
</td></tr>
<tr><td><code id="dmixn_+3A_ischol">isChol</code></td>
<td>
<p>boolean set to true is <code>sigma</code> is the cholesky decomposition of the covariance matrix.</p>
</td></tr>
<tr><td><code id="dmixn_+3A_a">A</code></td>
<td>
<p>an (optional) numeric matrix of dimension (m x d), which will be used to store the evaluations of each mixture
density over each mixture component. It is useful when m and n are large and one wants to call <code>dmixt()</code> 
several times, without reallocating memory for the whole matrix each time. NB1: <code>A</code> will be modified, 
not copied! NB2: the element of <code>A</code> must be of class &quot;numeric&quot;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>NB: at the moment the parallelization does not work properly on Solaris OS when <code>ncores&gt;1</code>. Hence, <code>dmixt()</code> checks if the OS 
is Solaris and, if this the case, it imposes <code>ncores==1</code>.
</p>


<h3>Value</h3>

<p>A vector of length n where the i-the entry contains the pdf of the i-th random vector (i.e. the i-th row of <code>X</code>).
</p>


<h3>Author(s)</h3>

<p>Matteo Fasiolo &lt;matteo.fasiolo@gmail.com&gt;.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#### 1) Example use
# Set up mixture density
mu &lt;- matrix(c(1, 2, 10, 20), 2, 2, byrow = TRUE)
sigma &lt;- list(diag(c(1, 10)), matrix(c(1, -0.9, -0.9, 1), 2, 2))
w &lt;- c(0.1, 0.9)

# Simulate
X &lt;- rmixn(1e4, mu, sigma, w)

# Evaluate density
ds &lt;- dmixn(X, mu, sigma, w = w)
head(ds)

##### 2) More complicated example
# Define mixture
set.seed(5135)
N &lt;- 10000
d &lt;- 2
w &lt;- rep(1, 2) / 2
mu &lt;- matrix(c(0, 0, 2, 3), 2, 2, byrow = TRUE) 
sigma &lt;- list(matrix(c(1, 0, 0, 2), 2, 2), matrix(c(1, -0.9, -0.9, 1), 2, 2)) 

# Simulate random variables
X &lt;- rmixn(N, mu, sigma, w = w, retInd = TRUE)

# Plot mixture density
np &lt;- 100
xvals &lt;- seq(min(X[ , 1]), max(X[ , 1]), length.out = np)
yvals &lt;- seq(min(X[ , 2]), max(X[ , 2]), length.out = np)
theGrid &lt;- expand.grid(xvals, yvals) 
theGrid &lt;- as.matrix(theGrid)
dens &lt;- dmixn(theGrid, mu, sigma, w = w)
plot(X, pch = '.', col = attr(X, "index")+1)
contour(x = xvals, y = yvals, z = matrix(dens, np, np),
        levels = c(0.002, 0.01, 0.02, 0.04, 0.08, 0.15 ), add = TRUE, lwd = 2)

</code></pre>

<hr>
<h2 id='dmixt'>Fast density computation for mixture of multivariate Student's t distributions.</h2><span id='topic+dmixt'></span>

<h3>Description</h3>

<p>Fast density computation for mixture of multivariate Student's t distributions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dmixt(X, mu, sigma, df, w, log = FALSE, ncores = 1, isChol = FALSE, A = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="dmixt_+3A_x">X</code></td>
<td>
<p>matrix n by d where each row is a d dimensional random vector. Alternatively <code>X</code> can be a d-dimensional vector.</p>
</td></tr>
<tr><td><code id="dmixt_+3A_mu">mu</code></td>
<td>
<p>an (m x d) matrix, where m is the number of mixture components.</p>
</td></tr>
<tr><td><code id="dmixt_+3A_sigma">sigma</code></td>
<td>
<p>as list of m covariance matrices (d x d) on for each mixture component. 
Alternatively it can be a list of m cholesky decomposition of the covariance. 
In that case <code>isChol</code> should be set to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="dmixt_+3A_df">df</code></td>
<td>
<p>a positive scalar representing the degrees of freedom. All the densities in the mixture have the same <code>df</code>.</p>
</td></tr>
<tr><td><code id="dmixt_+3A_w">w</code></td>
<td>
<p>vector of length m, containing the weights of the mixture components.</p>
</td></tr>
<tr><td><code id="dmixt_+3A_log">log</code></td>
<td>
<p>boolean set to true the logarithm of the pdf is required.</p>
</td></tr>
<tr><td><code id="dmixt_+3A_ncores">ncores</code></td>
<td>
<p>Number of cores used. The parallelization will take place only if OpenMP is supported.</p>
</td></tr>
<tr><td><code id="dmixt_+3A_ischol">isChol</code></td>
<td>
<p>boolean set to true is <code>sigma</code> is the cholesky decomposition of the covariance matrix.</p>
</td></tr>
<tr><td><code id="dmixt_+3A_a">A</code></td>
<td>
<p>an (optional) numeric matrix of dimension (m x d), which will be used to store the evaluations of each mixture
density over each mixture component. It is useful when m and n are large and one wants to call <code>dmixt()</code> 
several times, without reallocating memory for the whole matrix each time. NB1: <code>A</code> will be modified, 
not copied! NB2: the element of <code>A</code> must be of class &quot;numeric&quot;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>There are many candidates for the multivariate generalization of Student's t-distribution, here we use
the parametrization described here <a href="https://en.wikipedia.org/wiki/Multivariate_t-distribution">https://en.wikipedia.org/wiki/Multivariate_t-distribution</a>. NB: at the moment 
the parallelization does not work properly on Solaris OS when <code>ncores&gt;1</code>. Hence, <code>dmixt()</code> checks if the OS 
is Solaris and, if this the case, it imposes <code>ncores==1</code>.
</p>


<h3>Value</h3>

<p>A vector of length n where the i-the entry contains the pdf of the i-th random vector (i.e. the i-th row of <code>X</code>).
</p>


<h3>Author(s)</h3>

<p>Matteo Fasiolo &lt;matteo.fasiolo@gmail.com&gt;.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#### 1) Example use
# Set up mixture density
df &lt;- 6
mu &lt;- matrix(c(1, 2, 10, 20), 2, 2, byrow = TRUE)
sigma &lt;- list(diag(c(1, 10)), matrix(c(1, -0.9, -0.9, 1), 2, 2))
w &lt;- c(0.1, 0.9)

# Simulate
X &lt;- rmixt(1e4, mu, sigma, df, w)

# Evaluate density
ds &lt;- dmixt(X, mu, sigma, w = w, df = df)
head(ds)

##### 2) More complicated example
# Define mixture
set.seed(5135)
N &lt;- 10000
d &lt;- 2
df = 10
w &lt;- rep(1, 2) / 2
mu &lt;- matrix(c(0, 0, 2, 3), 2, 2, byrow = TRUE) 
sigma &lt;- list(matrix(c(1, 0, 0, 2), 2, 2), matrix(c(1, -0.9, -0.9, 1), 2, 2)) 

# Simulate random variables
X &lt;- rmixt(N, mu, sigma, w = w, df = df, retInd = TRUE)

# Plot mixture density
np &lt;- 100
xvals &lt;- seq(min(X[ , 1]), max(X[ , 1]), length.out = np)
yvals &lt;- seq(min(X[ , 2]), max(X[ , 2]), length.out = np)
theGrid &lt;- expand.grid(xvals, yvals) 
theGrid &lt;- as.matrix(theGrid)
dens &lt;- dmixt(theGrid, mu, sigma, w = w, df = df)
plot(X, pch = '.', col = attr(X, "index")+1)
contour(x = xvals, y = yvals, z = matrix(dens, np, np),
        levels = c(0.002, 0.01, 0.02, 0.04, 0.08, 0.15 ), add = TRUE, lwd = 2)

</code></pre>

<hr>
<h2 id='dmvn'>Fast computation of the multivariate normal density.</h2><span id='topic+dmvn'></span>

<h3>Description</h3>

<p>Fast computation of the multivariate normal density.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dmvn(X, mu, sigma, log = FALSE, ncores = 1, isChol = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="dmvn_+3A_x">X</code></td>
<td>
<p>matrix n by d where each row is a d dimensional random vector. Alternatively <code>X</code> can be a d-dimensional vector.</p>
</td></tr>
<tr><td><code id="dmvn_+3A_mu">mu</code></td>
<td>
<p>vector of length d, representing the mean of the distribution.</p>
</td></tr>
<tr><td><code id="dmvn_+3A_sigma">sigma</code></td>
<td>
<p>covariance matrix (d x d). Alternatively it can be the cholesky decomposition
of the covariance. In that case isChol should be set to TRUE.</p>
</td></tr>
<tr><td><code id="dmvn_+3A_log">log</code></td>
<td>
<p>boolean set to true the logarithm of the pdf is required.</p>
</td></tr>
<tr><td><code id="dmvn_+3A_ncores">ncores</code></td>
<td>
<p>Number of cores used. The parallelization will take place only if OpenMP is supported.</p>
</td></tr>
<tr><td><code id="dmvn_+3A_ischol">isChol</code></td>
<td>
<p>boolean set to true is <code>sigma</code> is the cholesky decomposition of the covariance matrix.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of length n where the i-the entry contains the pdf of the i-th random vector.
</p>


<h3>Author(s)</h3>

<p>Matteo Fasiolo &lt;matteo.fasiolo@gmail.com&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>N &lt;- 100
d &lt;- 5
mu &lt;- 1:d
X &lt;- t(t(matrix(rnorm(N*d), N, d)) + mu)
tmp &lt;- matrix(rnorm(d^2), d, d)
mcov &lt;- tcrossprod(tmp, tmp)  + diag(0.5, d)
myChol &lt;- chol(mcov)

head(dmvn(X, mu, mcov), 10)
head(dmvn(X, mu, myChol, isChol = TRUE), 10)

## Not run: 
# Performance comparison: microbenchmark does not work on all
# platforms, hence we need to check whether it is installed
if( "microbenchmark" %in% rownames(installed.packages()) ){
library(mvtnorm)
library(microbenchmark)

a &lt;- cbind(
      dmvn(X, mu, mcov),
      dmvn(X, mu, myChol, isChol = TRUE),
      dmvnorm(X, mu, mcov))
      
# Check if we get the same output as dmvnorm()
a[ , 1] / a[, 3]
a[ , 2] / a[, 3]

microbenchmark(dmvn(X, mu, myChol, isChol = TRUE), 
               dmvn(X, mu, mcov), 
               dmvnorm(X, mu, mcov))
               
detach("package:mvtnorm", unload=TRUE)
}

## End(Not run)

</code></pre>

<hr>
<h2 id='dmvt'>Fast computation of the multivariate Student's t density.</h2><span id='topic+dmvt'></span>

<h3>Description</h3>

<p>Fast computation of the multivariate Student's t density.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dmvt(X, mu, sigma, df, log = FALSE, ncores = 1, isChol = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="dmvt_+3A_x">X</code></td>
<td>
<p>matrix n by d where each row is a d dimensional random vector. Alternatively <code>X</code> can be a d-dimensional vector.</p>
</td></tr>
<tr><td><code id="dmvt_+3A_mu">mu</code></td>
<td>
<p>vector of length d, representing the mean of the distribution.</p>
</td></tr>
<tr><td><code id="dmvt_+3A_sigma">sigma</code></td>
<td>
<p>scale matrix (d x d). Alternatively it can be the cholesky decomposition
of the scale matrix. In that case isChol should be set to TRUE. Notice that ff the degrees of 
freedom (the argument <code>df</code>) is larger than 2, the <code>Cov(X)=sigma*df/(df-2)</code>.</p>
</td></tr>
<tr><td><code id="dmvt_+3A_df">df</code></td>
<td>
<p>a positive scalar representing the degrees of freedom.</p>
</td></tr>
<tr><td><code id="dmvt_+3A_log">log</code></td>
<td>
<p>boolean set to true the logarithm of the pdf is required.</p>
</td></tr>
<tr><td><code id="dmvt_+3A_ncores">ncores</code></td>
<td>
<p>Number of cores used. The parallelization will take place only if OpenMP is supported.</p>
</td></tr>
<tr><td><code id="dmvt_+3A_ischol">isChol</code></td>
<td>
<p>boolean set to true is <code>sigma</code> is the cholesky decomposition of the covariance matrix.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>There are many candidates for the multivariate generalization of Student's t-distribution, here we use
the parametrization described here <a href="https://en.wikipedia.org/wiki/Multivariate_t-distribution">https://en.wikipedia.org/wiki/Multivariate_t-distribution</a>. NB: at the moment 
the parallelization does not work properly on Solaris OS when <code>ncores&gt;1</code>. Hence, <code>dmvt()</code> checks if the OS 
is Solaris and, if this the case, it imposes <code>ncores==1</code>.
</p>


<h3>Value</h3>

<p>A vector of length n where the i-the entry contains the pdf of the i-th random vector.
</p>


<h3>Author(s)</h3>

<p>Matteo Fasiolo &lt;matteo.fasiolo@gmail.com&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>N &lt;- 100
d &lt;- 5
mu &lt;- 1:d
df &lt;- 4
X &lt;- t(t(matrix(rnorm(N*d), N, d)) + mu)
tmp &lt;- matrix(rnorm(d^2), d, d)
mcov &lt;- tcrossprod(tmp, tmp)  + diag(0.5, d)
myChol &lt;- chol(mcov)

head(dmvt(X, mu, mcov, df = df), 10)
head(dmvt(X, mu, myChol, df = df, isChol = TRUE), 10)

</code></pre>

<hr>
<h2 id='maha'>Fast computation of squared mahalanobis distance between all rows of <code>X</code> and the vector <code>mu</code> with respect to sigma.</h2><span id='topic+maha'></span>

<h3>Description</h3>

<p>Fast computation of squared mahalanobis distance between all rows of <code>X</code> and the vector <code>mu</code> with respect to sigma.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>maha(X, mu, sigma, ncores = 1, isChol = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="maha_+3A_x">X</code></td>
<td>
<p>matrix n by d where each row is a d dimensional random vector. Alternatively <code>X</code> can be a d-dimensional vector.</p>
</td></tr>
<tr><td><code id="maha_+3A_mu">mu</code></td>
<td>
<p>vector of length d, representing the central position.</p>
</td></tr>
<tr><td><code id="maha_+3A_sigma">sigma</code></td>
<td>
<p>covariance matrix (d x d). Alternatively is can be the cholesky decomposition
of the covariance. In that case <code>isChol</code> should be set to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="maha_+3A_ncores">ncores</code></td>
<td>
<p>Number of cores used. The parallelization will take place only if OpenMP is supported.</p>
</td></tr>
<tr><td><code id="maha_+3A_ischol">isChol</code></td>
<td>
<p>boolean set to true is <code>sigma</code> is the cholesky decomposition of the covariance matrix.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a vector of length n where the i-the entry contains the square mahalanobis distance i-th random vector.
</p>


<h3>Author(s)</h3>

<p>Matteo Fasiolo &lt;matteo.fasiolo@gmail.com&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>N &lt;- 100
d &lt;- 5
mu &lt;- 1:d
X &lt;- t(t(matrix(rnorm(N*d), N, d)) + mu)
tmp &lt;- matrix(rnorm(d^2), d, d)
mcov &lt;- tcrossprod(tmp, tmp)
myChol &lt;- chol(mcov)

rbind(head(maha(X, mu, mcov), 10),
      head(maha(X, mu, myChol, isChol = TRUE), 10),
      head(mahalanobis(X, mu, mcov), 10))

## Not run: 
# Performance comparison: microbenchmark does not work on all
# platforms, hence we need to check whether it is installed
if( "microbenchmark" %in% rownames(installed.packages()) ){
library(microbenchmark)

a &lt;- cbind(
  maha(X, mu, mcov),
  maha(X, mu, myChol, isChol = TRUE),
  mahalanobis(X, mu, mcov))
  
# Same output as mahalanobis
a[ , 1] / a[, 3]
a[ , 2] / a[, 3]

microbenchmark(maha(X, mu, mcov),
               maha(X, mu, myChol, isChol = TRUE),
               mahalanobis(X, mu, mcov))
}

## End(Not run)
</code></pre>

<hr>
<h2 id='ms'>Mean-shift mode seeking algorithm</h2><span id='topic+ms'></span>

<h3>Description</h3>

<p>Given a sample from a d-dimensional distribution, an initialization point and a bandwidth
the algorithm finds the nearest mode of the corresponding Gaussian kernel density.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ms(X, init, H, tol = 1e-06, ncores = 1, store = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ms_+3A_x">X</code></td>
<td>
<p>n by d matrix containing the data.</p>
</td></tr>
<tr><td><code id="ms_+3A_init">init</code></td>
<td>
<p>d-dimensional vector containing the initial point for the optimization.</p>
</td></tr>
<tr><td><code id="ms_+3A_h">H</code></td>
<td>
<p>Positive definite bandwidth matrix representing the covariance of each component of the Gaussian kernel density.</p>
</td></tr>
<tr><td><code id="ms_+3A_tol">tol</code></td>
<td>
<p>Tolerance used to assess the convergence of the algorithm, which is stopped if the absolute values
of increments along all the dimensions are smaller then tol at any iteration. Default value is 1e-6.</p>
</td></tr>
<tr><td><code id="ms_+3A_ncores">ncores</code></td>
<td>
<p>Number of cores used. The parallelization will take place only if OpenMP is supported.</p>
</td></tr>
<tr><td><code id="ms_+3A_store">store</code></td>
<td>
<p>If <code>FALSE</code> only the latest iteration is returned, if <code>TRUE</code> the function will return a matrix where
the i-th row is the position of the algorithms at the i-th iteration.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list where <code>estim</code> is a d-dimensional vector containing the last position of the algorithm, while <code>traj</code> 
is a matrix with d-colums representing the trajectory of the algorithm along each dimension. If <code>store == FALSE</code> the whole trajectory
is not stored and <code>traj = NULL</code>.
</p>


<h3>Author(s)</h3>

<p>Matteo Fasiolo &lt;matteo.fasiolo@gmail.com&gt;.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(434)

# Simulating multivariate normal data
N &lt;- 1000
mu &lt;- c(1, 2)
sigma &lt;- matrix(c(1, 0.5, 0.5, 1), 2, 2)
X &lt;- rmvn(N, mu = mu, sigma = sigma)

# Plotting the true density function
steps &lt;- 100
range1 &lt;- seq(min(X[ , 1]), max(X[ , 1]), length.out = steps)
range2 &lt;- seq(min(X[ , 2]), max(X[ , 2]), length.out = steps)
grid &lt;- expand.grid(range1, range2)
vals &lt;- dmvn(as.matrix(grid), mu, sigma)

contour(z = matrix(vals, steps, steps),  x = range1, y = range2, xlab = "X1", ylab = "X2")
points(X[ , 1], X[ , 2], pch = '.')
 
# Estimating the mode from "nrep" starting points
nrep &lt;- 10
index &lt;- sample(1:N, nrep)
for(ii in 1:nrep) {
  start &lt;- X[index[ii], ]
  out &lt;- ms(X, init = start, H = 0.1 * sigma, store = TRUE)
  lines(out$traj[ , 1], out$traj[ , 2], col = 2, lwd = 2) 
  points(out$final[1], out$final[2], col = 4, pch = 3, lwd = 3) # Estimated mode (blue)
  points(start[1], start[2], col = 2, pch = 3, lwd = 3)         # ii-th starting value 
}
</code></pre>

<hr>
<h2 id='rmixn'>Fast simulation of r.v.s from a mixture of multivariate normal densities</h2><span id='topic+rmixn'></span>

<h3>Description</h3>

<p>Fast simulation of r.v.s from a mixture of multivariate normal densities
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rmixn(
  n,
  mu,
  sigma,
  w,
  ncores = 1,
  isChol = FALSE,
  retInd = FALSE,
  A = NULL,
  kpnames = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="rmixn_+3A_n">n</code></td>
<td>
<p>number of random vectors to be simulated.</p>
</td></tr>
<tr><td><code id="rmixn_+3A_mu">mu</code></td>
<td>
<p>an (m x d) matrix, where m is the number of mixture components.</p>
</td></tr>
<tr><td><code id="rmixn_+3A_sigma">sigma</code></td>
<td>
<p>as list of m covariance matrices (d x d) on for each mixture component. 
Alternatively it can be a list of m cholesky decomposition of the covariance. 
In that case <code>isChol</code> should be set to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="rmixn_+3A_w">w</code></td>
<td>
<p>vector of length m, containing the weights of the mixture components.</p>
</td></tr>
<tr><td><code id="rmixn_+3A_ncores">ncores</code></td>
<td>
<p>Number of cores used. The parallelization will take place only if OpenMP is supported.</p>
</td></tr>
<tr><td><code id="rmixn_+3A_ischol">isChol</code></td>
<td>
<p>boolean set to true is <code>sigma</code> is the cholesky decomposition of the covariance matrix.</p>
</td></tr>
<tr><td><code id="rmixn_+3A_retind">retInd</code></td>
<td>
<p>when set to <code>TRUE</code> an attribute called &quot;index&quot; will be added to the output matrix of random variables.
This is a vector specifying to which mixture components each random vector belongs. <code>FALSE</code> by default.</p>
</td></tr>
<tr><td><code id="rmixn_+3A_a">A</code></td>
<td>
<p>an (optional) numeric matrix of dimension (n x d), which will be used to store the output random variables.
It is useful when n and d are large and one wants to call <code>rmvn()</code> several times, without reallocating memory
for the whole matrix each time. NB: the element of <code>A</code> must be of class &quot;numeric&quot;.</p>
</td></tr>
<tr><td><code id="rmixn_+3A_kpnames">kpnames</code></td>
<td>
<p>if <code>TRUE</code> the dimensions' names are preserved. That is, the i-th column of the output
has the same name as the i-th entry of <code>mu</code> or the i-th column of <code>sigma</code>. 
<code>kpnames==FALSE</code> by default.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Notice that this function does not use one of the Random Number Generators (RNGs) provided by R, but one 
of the parallel cryptographic RNGs described in (Salmon et al., 2011). It is important to point out that this
RNG can safely be used in parallel, without risk of collisions between parallel sequence of random numbers.
The initialization of the RNG depends on R's seed, hence the <code>set.seed()</code> function can be used to 
obtain reproducible results. Notice though that changing <code>ncores</code> causes most of the generated numbers
to be different even if R's seed is the same (see example below). NB: at the moment the RNG does not work
properly on Solaris OS when <code>ncores&gt;1</code>. Hence, <code>rmixn()</code> checks if the OS is Solaris and, if this the case, 
it imposes <code>ncores==1</code>.
</p>


<h3>Value</h3>

<p>If <code>A==NULL</code> (default) the output is an (n x d) matrix where the i-th row is the i-th simulated vector.
If <code>A!=NULL</code> then the random vector are store in <code>A</code>, which is provided by the user, and the function
returns <code>NULL</code>. Notice that if <code>retInd==TRUE</code> an attribute called &quot;index&quot; will be added to A.
This is a vector specifying to which mixture components each random vector belongs.
</p>


<h3>Author(s)</h3>

<p>Matteo Fasiolo &lt;matteo.fasiolo@gmail.com&gt;, C++ RNG engine by Thijs van den Berg &lt;http://sitmo.com/&gt;.
</p>


<h3>References</h3>

<p>John K. Salmon, Mark A. Moraes, Ron O. Dror, and David E. Shaw (2011). Parallel Random Numbers: As Easy as 1, 2, 3.
D. E. Shaw Research, New York, NY 10036, USA.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Create mixture of two components
mu &lt;- matrix(c(1, 2, 10, 20), 2, 2, byrow = TRUE)
sigma &lt;- list(diag(c(1, 10)), matrix(c(1, -0.9, -0.9, 1), 2, 2))
w &lt;- c(0.1, 0.9)

# Simulate
X &lt;- rmixn(1e4, mu, sigma, w, retInd = TRUE)
plot(X, pch = '.', col = attr(X, "index"))

# Simulate with fixed seed
set.seed(414)
rmixn(4, mu, sigma, w)

set.seed(414)
rmixn(4, mu, sigma, w)

set.seed(414)  
rmixn(4, mu, sigma, w, ncores = 2) # r.v. generated on the second core are different

###### Here we create the matrix that will hold the simulated random variables upfront.
A &lt;- matrix(NA, 4, 2)
class(A) &lt;- "numeric" # This is important. We need the elements of A to be of class "numeric". 

set.seed(414)
rmixn(4, mu, sigma, w, ncores = 2, A = A) # This returns NULL ...
A                                         # ... but the result is here

</code></pre>

<hr>
<h2 id='rmixt'>Fast simulation of r.v.s from a mixture of multivariate Student's t densities</h2><span id='topic+rmixt'></span>

<h3>Description</h3>

<p>Fast simulation of r.v.s from a mixture of multivariate Student's t densities
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rmixt(
  n,
  mu,
  sigma,
  df,
  w,
  ncores = 1,
  isChol = FALSE,
  retInd = FALSE,
  A = NULL,
  kpnames = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="rmixt_+3A_n">n</code></td>
<td>
<p>number of random vectors to be simulated.</p>
</td></tr>
<tr><td><code id="rmixt_+3A_mu">mu</code></td>
<td>
<p>an (m x d) matrix, where m is the number of mixture components.</p>
</td></tr>
<tr><td><code id="rmixt_+3A_sigma">sigma</code></td>
<td>
<p>as list of m covariance matrices (d x d) on for each mixture component. 
Alternatively it can be a list of m cholesky decomposition of the covariance. 
In that case <code>isChol</code> should be set to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="rmixt_+3A_df">df</code></td>
<td>
<p>a positive scalar representing the degrees of freedom. All the densities in the mixture have the same <code>df</code>.</p>
</td></tr>
<tr><td><code id="rmixt_+3A_w">w</code></td>
<td>
<p>vector of length m, containing the weights of the mixture components.</p>
</td></tr>
<tr><td><code id="rmixt_+3A_ncores">ncores</code></td>
<td>
<p>Number of cores used. The parallelization will take place only if OpenMP is supported.</p>
</td></tr>
<tr><td><code id="rmixt_+3A_ischol">isChol</code></td>
<td>
<p>boolean set to true is <code>sigma</code> is the cholesky decomposition of the covariance matrix.</p>
</td></tr>
<tr><td><code id="rmixt_+3A_retind">retInd</code></td>
<td>
<p>when set to <code>TRUE</code> an attribute called &quot;index&quot; will be added to the output matrix of random variables.
This is a vector specifying to which mixture components each random vector belongs. <code>FALSE</code> by default.</p>
</td></tr>
<tr><td><code id="rmixt_+3A_a">A</code></td>
<td>
<p>an (optional) numeric matrix of dimension (n x d), which will be used to store the output random variables.
It is useful when n and d are large and one wants to call <code>rmvn()</code> several times, without reallocating memory
for the whole matrix each time. NB: the element of <code>A</code> must be of class &quot;numeric&quot;.</p>
</td></tr>
<tr><td><code id="rmixt_+3A_kpnames">kpnames</code></td>
<td>
<p>if <code>TRUE</code> the dimensions' names are preserved. That is, the i-th column of the output
has the same name as the i-th entry of <code>mu</code> or the i-th column of <code>sigma</code>. 
<code>kpnames==FALSE</code> by default.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>There are many candidates for the multivariate generalization of Student's t-distribution, here we use
the parametrization described here <a href="https://en.wikipedia.org/wiki/Multivariate_t-distribution">https://en.wikipedia.org/wiki/Multivariate_t-distribution</a>. 
</p>
<p>Notice that this function does not use one of the Random Number Generators (RNGs) provided by R, but one 
of the parallel cryptographic RNGs described in (Salmon et al., 2011). It is important to point out that this
RNG can safely be used in parallel, without risk of collisions between parallel sequence of random numbers.
The initialization of the RNG depends on R's seed, hence the <code>set.seed()</code> function can be used to 
obtain reproducible results. Notice though that changing <code>ncores</code> causes most of the generated numbers
to be different even if R's seed is the same (see example below). NB: at the moment 
the parallelization does not work properly on Solaris OS when <code>ncores&gt;1</code>. Hence, <code>rmixt()</code> checks if the OS 
is Solaris and, if this the case, it imposes <code>ncores==1</code>
</p>


<h3>Value</h3>

<p>If <code>A==NULL</code> (default) the output is an (n x d) matrix where the i-th row is the i-th simulated vector.
If <code>A!=NULL</code> then the random vector are store in <code>A</code>, which is provided by the user, and the function
returns <code>NULL</code>. Notice that if <code>retInd==TRUE</code> an attribute called &quot;index&quot; will be added to A.
This is a vector specifying to which mixture components each random vector belongs.
</p>


<h3>Author(s)</h3>

<p>Matteo Fasiolo &lt;matteo.fasiolo@gmail.com&gt;, C++ RNG engine by Thijs van den Berg &lt;http://sitmo.com/&gt;.
</p>


<h3>References</h3>

<p>John K. Salmon, Mark A. Moraes, Ron O. Dror, and David E. Shaw (2011). Parallel Random Numbers: As Easy as 1, 2, 3.
D. E. Shaw Research, New York, NY 10036, USA.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Create mixture of two components
df &lt;- 6
mu &lt;- matrix(c(1, 2, 10, 20), 2, 2, byrow = TRUE)
sigma &lt;- list(diag(c(1, 10)), matrix(c(1, -0.9, -0.9, 1), 2, 2))
w &lt;- c(0.1, 0.9)

# Simulate
X &lt;- rmixt(1e4, mu, sigma, df, w, retInd = TRUE)
plot(X, pch = '.', col = attr(X, "index"))

# Simulate with fixed seed
set.seed(414)
rmixt(4, mu, sigma, df, w)

set.seed(414)
rmixt(4, mu, sigma, df, w)

set.seed(414)  
rmixt(4, mu, sigma, df, w, ncores = 2) # r.v. generated on the second core are different

###### Here we create the matrix that will hold the simulated random variables upfront.
A &lt;- matrix(NA, 4, 2)
class(A) &lt;- "numeric" # This is important. We need the elements of A to be of class "numeric". 

set.seed(414)
rmixt(4, mu, sigma, df, w, ncores = 2, A = A) # This returns NULL ...
A                                             # ... but the result is here

</code></pre>

<hr>
<h2 id='rmvn'>Fast simulation of multivariate normal random variables</h2><span id='topic+rmvn'></span>

<h3>Description</h3>

<p>Fast simulation of multivariate normal random variables
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rmvn(n, mu, sigma, ncores = 1, isChol = FALSE, A = NULL, kpnames = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="rmvn_+3A_n">n</code></td>
<td>
<p>number of random vectors to be simulated.</p>
</td></tr>
<tr><td><code id="rmvn_+3A_mu">mu</code></td>
<td>
<p>vector of length d, representing the mean.</p>
</td></tr>
<tr><td><code id="rmvn_+3A_sigma">sigma</code></td>
<td>
<p>covariance matrix (d x d). Alternatively is can be the cholesky decomposition
of the covariance. In that case <code>isChol</code> should be set to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="rmvn_+3A_ncores">ncores</code></td>
<td>
<p>Number of cores used. The parallelization will take place only if OpenMP is supported.</p>
</td></tr>
<tr><td><code id="rmvn_+3A_ischol">isChol</code></td>
<td>
<p>boolean set to true is <code>sigma</code> is the cholesky decomposition of the covariance matrix.</p>
</td></tr>
<tr><td><code id="rmvn_+3A_a">A</code></td>
<td>
<p>an (optional) numeric matrix of dimension (n x d), which will be used to store the output random variables.
It is useful when n and d are large and one wants to call <code>rmvn()</code> several times, without reallocating memory
for the whole matrix each time. NB: the element of <code>A</code> must be of class &quot;numeric&quot;.</p>
</td></tr>
<tr><td><code id="rmvn_+3A_kpnames">kpnames</code></td>
<td>
<p>if <code>TRUE</code> the dimensions' names are preserved. That is, the i-th column of the output
has the same name as the i-th entry of <code>mu</code> or the i-th column of <code>sigma</code>. 
<code>kpnames==FALSE</code> by default.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Notice that this function does not use one of the Random Number Generators (RNGs) provided by R, but one 
of the parallel cryptographic RNGs described in (Salmon et al., 2011). It is important to point out that this
RNG can safely be used in parallel, without risk of collisions between parallel sequence of random numbers.
The initialization of the RNG depends on R's seed, hence the <code>set.seed()</code> function can be used to 
obtain reproducible results. Notice though that changing <code>ncores</code> causes most of the generated numbers
to be different even if R's seed is the same (see example below). NB: at the moment the RNG does not work
properly on Solaris OS when <code>ncores&gt;1</code>. Hence, <code>rmvn()</code> checks if the OS is Solaris and, if this the case, 
it imposes <code>ncores==1</code>.
</p>


<h3>Value</h3>

<p>If <code>A==NULL</code> (default) the output is an (n x d) matrix where the i-th row is the i-th simulated vector.
If <code>A!=NULL</code> then the random vector are store in <code>A</code>, which is provided by the user, and the function
returns <code>NULL</code>.
</p>


<h3>Author(s)</h3>

<p>Matteo Fasiolo &lt;matteo.fasiolo@gmail.com&gt;, C++ RNG engine by Thijs van den Berg &lt;http://sitmo.com/&gt;.
</p>


<h3>References</h3>

<p>John K. Salmon, Mark A. Moraes, Ron O. Dror, and David E. Shaw (2011). Parallel Random Numbers: As Easy as 1, 2, 3.
D. E. Shaw Research, New York, NY 10036, USA.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>d &lt;- 5
mu &lt;- 1:d

# Creating covariance matrix
tmp &lt;- matrix(rnorm(d^2), d, d)
mcov &lt;- tcrossprod(tmp, tmp)

set.seed(414)
rmvn(4, 1:d, mcov)

set.seed(414)
rmvn(4, 1:d, mcov)

set.seed(414)  
rmvn(4, 1:d, mcov, ncores = 2) # r.v. generated on the second core are different

###### Here we create the matrix that will hold the simulated random variables upfront.
A &lt;- matrix(NA, 4, d)
class(A) &lt;- "numeric" # This is important. We need the elements of A to be of class "numeric". 

set.seed(414)
rmvn(4, 1:d, mcov, ncores = 2, A = A) # This returns NULL ...
A                                     # ... but the result is here

</code></pre>

<hr>
<h2 id='rmvt'>Fast simulation of multivariate Student's t random variables</h2><span id='topic+rmvt'></span>

<h3>Description</h3>

<p>Fast simulation of multivariate Student's t random variables
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rmvt(n, mu, sigma, df, ncores = 1, isChol = FALSE, A = NULL, kpnames = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="rmvt_+3A_n">n</code></td>
<td>
<p>number of random vectors to be simulated.</p>
</td></tr>
<tr><td><code id="rmvt_+3A_mu">mu</code></td>
<td>
<p>vector of length d, representing the mean of the distribution.</p>
</td></tr>
<tr><td><code id="rmvt_+3A_sigma">sigma</code></td>
<td>
<p>scale matrix (d x d). Alternatively it can be the cholesky decomposition
of the scale matrix. In that case isChol should be set to TRUE. Notice that ff the degrees of 
freedom (the argument <code>df</code>) is larger than 2, the <code>Cov(X)=sigma*df/(df-2)</code>.</p>
</td></tr>
<tr><td><code id="rmvt_+3A_df">df</code></td>
<td>
<p>a positive scalar representing the degrees of freedom.</p>
</td></tr>
<tr><td><code id="rmvt_+3A_ncores">ncores</code></td>
<td>
<p>Number of cores used. The parallelization will take place only if OpenMP is supported.</p>
</td></tr>
<tr><td><code id="rmvt_+3A_ischol">isChol</code></td>
<td>
<p>boolean set to true is <code>sigma</code> is the cholesky decomposition of the covariance matrix.</p>
</td></tr>
<tr><td><code id="rmvt_+3A_a">A</code></td>
<td>
<p>an (optional) numeric matrix of dimension (n x d), which will be used to store the output random variables.
It is useful when n and d are large and one wants to call <code>rmvn()</code> several times, without reallocating memory
for the whole matrix each time. NB: the element of <code>A</code> must be of class &quot;numeric&quot;.</p>
</td></tr>
<tr><td><code id="rmvt_+3A_kpnames">kpnames</code></td>
<td>
<p>if <code>TRUE</code> the dimensions' names are preserved. That is, the i-th column of the output
has the same name as the i-th entry of <code>mu</code> or the i-th column of <code>sigma</code>. 
<code>kpnames==FALSE</code> by default.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>There are in fact many candidates for the multivariate generalization of Student's t-distribution, here we use
the parametrization described here <a href="https://en.wikipedia.org/wiki/Multivariate_t-distribution">https://en.wikipedia.org/wiki/Multivariate_t-distribution</a>.
</p>
<p>Notice that <code>rmvt()</code> does not use one of the Random Number Generators (RNGs) provided by R, but one 
of the parallel cryptographic RNGs described in (Salmon et al., 2011). It is important to point out that this
RNG can safely be used in parallel, without risk of collisions between parallel sequence of random numbers.
The initialization of the RNG depends on R's seed, hence the <code>set.seed()</code> function can be used to 
obtain reproducible results. Notice though that changing <code>ncores</code> causes most of the generated numbers
to be different even if R's seed is the same (see example below). NB: at the moment the RNG does not work
properly on Solaris OS when <code>ncores&gt;1</code>. Hence, <code>rmvt()</code> checks if the OS is Solaris and, if this the case, 
it imposes <code>ncores==1</code>.
</p>


<h3>Value</h3>

<p>If <code>A==NULL</code> (default) the output is an (n x d) matrix where the i-th row is the i-th simulated vector.
If <code>A!=NULL</code> then the random vector are store in <code>A</code>, which is provided by the user, and the function
returns <code>NULL</code>.
</p>


<h3>Author(s)</h3>

<p>Matteo Fasiolo &lt;matteo.fasiolo@gmail.com&gt;, C++ RNG engine by Thijs van den Berg &lt;http://sitmo.com/&gt;.
</p>


<h3>References</h3>

<p>John K. Salmon, Mark A. Moraes, Ron O. Dror, and David E. Shaw (2011). Parallel Random Numbers: As Easy as 1, 2, 3.
D. E. Shaw Research, New York, NY 10036, USA.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>d &lt;- 5
mu &lt;- 1:d
df &lt;- 4

# Creating covariance matrix
tmp &lt;- matrix(rnorm(d^2), d, d)
mcov &lt;- tcrossprod(tmp, tmp) + diag(0.5, d)

set.seed(414)
rmvt(4, 1:d, mcov, df = df)

set.seed(414)
rmvt(4, 1:d, mcov, df = df)

set.seed(414)  
rmvt(4, 1:d, mcov, df = df, ncores = 2) # These will not match the r.v. generated on a single core.

###### Here we create the matrix that will hold the simulated random variables upfront.
A &lt;- matrix(NA, 4, d)
class(A) &lt;- "numeric" # This is important. We need the elements of A to be of class "numeric". 

set.seed(414)
rmvt(4, 1:d, mcov, df = df, ncores = 2, A = A) # This returns NULL ...
A                                     # ... but the result is here

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
