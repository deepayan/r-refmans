<!DOCTYPE html><html lang="en"><head><title>Help for package EIX</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {EIX}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#EIX-package'><p>EIX package</p></a></li>
<li><a href='#calculateGain'><p>calculateGain</p></a></li>
<li><a href='#HR_data'><p>Why are our best and most experienced employees leaving prematurely?</p></a></li>
<li><a href='#importance'><p>Importance of variables and interactions in the model</p></a></li>
<li><a href='#interactions'><p>Importance of interactions and pairs in the model</p></a></li>
<li><a href='#lollipop'><p>Tables needed for lollipop plot</p></a></li>
<li><a href='#plot.importance'><p>Plot importance measures</p></a></li>
<li><a href='#plot.interactions'><p>Plot importance of interactions or pairs</p></a></li>
<li><a href='#plot.lollipop'><p>Visualiation of the model</p></a></li>
<li><a href='#tableOfTrees'><p>tableOfTrees</p></a></li>
<li><a href='#titanic_data'><p>Passengers and Crew on the RMS Titanic</p></a></li>
<li><a href='#waterfall'><p>Explain prediction of a single observation</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>Explain Interactions in 'XGBoost'</td>
</tr>
<tr>
<td>Version:</td>
<td>1.2.0</td>
</tr>
<tr>
<td>Description:</td>
<td>Structure mining from 'XGBoost' and 'LightGBM' models.
    Key functionalities of this package cover: visualisation of tree-based ensembles models,
    identification of interactions, measuring of variable importance,
    measuring of interaction importance, explanation of single prediction 
    with break down plots (based on 'xgboostExplainer' and 'iBreakDown' packages). 
    To download the 'LightGBM' use the following link: <a href="https://github.com/Microsoft/LightGBM">https://github.com/Microsoft/LightGBM</a>.
    'EIX' is a part of the 'DrWhy.AI' universe.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>Imports:</td>
<td>MASS, ggplot2, data.table, purrr, xgboost, DALEX, ggrepel,
ggiraphExtra, iBreakDown, tidyr, scales</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.1</td>
</tr>
<tr>
<td>Suggests:</td>
<td>Matrix, knitr, rmarkdown, lightgbm</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/ModelOriented/EIX">https://github.com/ModelOriented/EIX</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/ModelOriented/EIX/issues">https://github.com/ModelOriented/EIX/issues</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2021-03-18 23:13:27 UTC; 01131304</td>
</tr>
<tr>
<td>Author:</td>
<td>Szymon Maksymiuk [aut, cre],
  Ewelina Karbowiak [aut],
  Przemyslaw Biecek [aut, ths]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Szymon Maksymiuk &lt;sz.maksymiuk@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2021-03-23 08:10:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='EIX-package'>EIX package</h2><span id='topic+EIX-package'></span>

<h3>Description</h3>

<p>Structure mining from 'XGBoost' and 'LightGBM' models.
Key functionalities of this package cover: visualisation of tree-based ensembles models,
identification of interactions, measuring of variable importance,
measuring of interaction importance, explanation of single prediction
with break down plots (based on 'xgboostExplainer' and 'iBreakDown' packages).
To download the 'LightGBM' use the following link: &lt;https://github.com/Microsoft/LightGBM&gt;.
EIX' is a part of the 'DrWhy.AI' universe.
</p>

<hr>
<h2 id='calculateGain'>calculateGain</h2><span id='topic+calculateGain'></span>

<h3>Description</h3>

<p>List of trees with pairs of variable and other needed fields
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calculateGain(xgb.model, data)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="calculateGain_+3A_xgb.model">xgb.model</code></td>
<td>
<p>a xgboost or lightgbm model</p>
</td></tr>
<tr><td><code id="calculateGain_+3A_data">data</code></td>
<td>
<p>a data table with data used to train the model</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list
</p>

<hr>
<h2 id='HR_data'>Why are our best and most experienced employees leaving prematurely?</h2><span id='topic+HR_data'></span>

<h3>Description</h3>

<p>A dataset from Kaggle competition Human Resources Analytics.
https://www.kaggle.com/ludobenistant/hr-analytics/data
</p>


<h3>Format</h3>

<p>A data table with 14999 rows and 10 variables
</p>


<h3>Details</h3>

<p>The description of the dataset was copied from the <code>breakDown</code> package.
</p>

<ul>
<li><p>  satisfaction_level Level of satisfaction (0-1)
</p>
</li>
<li><p>  last_evaluation Time since last performance evaluation (in Years)
</p>
</li>
<li><p>  number_project  Number of projects completed while at work
</p>
</li>
<li><p>  average_montly_hours  Average monthly hours at workplace
</p>
</li>
<li><p>  time_spend_company  Number of years spent in the company
</p>
</li>
<li><p>  Work_accident  Whether the employee had a workplace accident
</p>
</li>
<li><p>  left  Whether the employee left the workplace or not (1 or 0) Factor
</p>
</li>
<li><p>  promotion_last_5years  Whether the employee was promoted in the last five years
</p>
</li>
<li><p>  sales  Department in which they work for
</p>
</li>
<li><p>  salary  Relative level of salary (high)
</p>
</li></ul>



<h3>Source</h3>

<p>https://www.kaggle.com/ludobenistant/hr-analytics/data, <a href="https://cran.r-project.org/package=breakDown">https://cran.r-project.org/package=breakDown</a>
</p>

<hr>
<h2 id='importance'>Importance of variables and interactions in the model</h2><span id='topic+importance'></span>

<h3>Description</h3>

<p>This functions calculates a table with selected measures of importance
for variables and interactions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>importance(xgb_model, data, option = "both", digits = 4)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="importance_+3A_xgb_model">xgb_model</code></td>
<td>
<p>a xgboost or lightgbm model.</p>
</td></tr>
<tr><td><code id="importance_+3A_data">data</code></td>
<td>
<p>a data table with data used to train the model.</p>
</td></tr>
<tr><td><code id="importance_+3A_option">option</code></td>
<td>
<p>if &quot;variables&quot; then table includes only single variables,
if &quot;interactions&quot;, then only interactions
if &quot;both&quot;, then both single variable and interactions.
Default &quot;both&quot;.</p>
</td></tr>
<tr><td><code id="importance_+3A_digits">digits</code></td>
<td>
<p>number of significant digits that shall be returned. Will be passed to the signif() functions.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Available measures:
</p>

<ul>
<li><p> &quot;sumGain&quot; - sum of Gain value in all nodes, in which given variable occurs,
</p>
</li>
<li><p> &quot;sumCover&quot; - sum of Cover value in all nodes, in which given variable occurs; for LightGBM models: number of observation, which pass through the node,
</p>
</li>
<li><p> &quot;mean5Gain&quot; - mean gain from 5 occurrences of given variable with the highest gain,
</p>
</li>
<li><p> &quot;meanGain&quot; - mean Gain value in all nodes, in which given variable occurs,
</p>
</li>
<li><p> &quot;meanCover&quot; - mean Cover value in all nodes, in which given variable occurs; for LightGBM models: mean number of observation, which pass through the node,
</p>
</li>
<li><p> &quot;freqency&quot; - number of occurrences in the nodes for given variable.
</p>
</li></ul>

<p>Additionally for table with single variables:
</p>

<ul>
<li><p> &quot;meanDepth&quot;  - mean depth weighted by gain,
</p>
</li>
<li><p> &quot;numberOfRoots&quot; - number of occurrences in the root,
</p>
</li>
<li><p> &quot;weightedRoot&quot; - mean number of occurrences in the root, which is weighted by gain.
</p>
</li></ul>



<h3>Value</h3>

<p>a data table
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library("EIX")
library("Matrix")
sm &lt;- sparse.model.matrix(left ~ . - 1,  data = HR_data)

library("xgboost")
param &lt;- list(objective = "binary:logistic", max_depth = 2)
xgb_model &lt;- xgboost(sm, params = param, label = HR_data[, left] == 1, nrounds = 25, verbose=0)

imp &lt;- importance(xgb_model, sm, option = "both")
imp
plot(imp,  top = 10)

imp &lt;- importance(xgb_model, sm, option = "variables")
imp
plot(imp,  top = nrow(imp))

 imp &lt;- importance(xgb_model, sm, option = "interactions")
 imp
plot(imp,  top =  nrow(imp))

 imp &lt;- importance(xgb_model, sm, option = "variables")
 imp
plot(imp, top = NULL, radar = FALSE, xmeasure = "sumCover", ymeasure = "sumGain")


</code></pre>

<hr>
<h2 id='interactions'>Importance of interactions and pairs in the model</h2><span id='topic+interactions'></span>

<h3>Description</h3>

<p>This function calculates a table with two measures of importance for interactions and pairs in the model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>interactions(xgb_model, data, option = "interactions")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="interactions_+3A_xgb_model">xgb_model</code></td>
<td>
<p>a xgboost or lightgbm model.</p>
</td></tr>
<tr><td><code id="interactions_+3A_data">data</code></td>
<td>
<p>a data table with data used to train the model.</p>
</td></tr>
<tr><td><code id="interactions_+3A_option">option</code></td>
<td>
<p>if &quot;interactions&quot;, the table contains interactions,
if &quot;pairs&quot;, this table contains all the pairs in the model.
Default &quot;interactions&quot;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Available measures:
</p>

<ul>
<li><p> &quot;sumGain&quot; - sum of Gain value in all nodes, in which given variable occurs,
</p>
</li>
<li><p> &quot;freqency&quot; - number of occurrences in the nodes for given variable.
</p>
</li></ul>

<p>NOTE: Be careful use of this function with <code>option="pairs"</code> parameter,
because high gain of pair can be a result of high gain of child variable.
As strong interactions should be considered only these pairs of variables,
where variable on the bottom (child) has higher gain than variable on the top (parent).
</p>


<h3>Value</h3>

<p>a data table
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library("EIX")
library("Matrix")
sm &lt;- sparse.model.matrix(left ~ . - 1,  data = HR_data)

library("xgboost")
param &lt;- list(objective = "binary:logistic", max_depth = 2)
xgb_model &lt;- xgboost(sm, params = param, label = HR_data[, left] == 1, nrounds = 25, verbose=0)

inter &lt;- interactions(xgb_model, sm, option = "interactions")
inter
plot(inter)

inter &lt;- interactions(xgb_model, sm, option = "pairs")
inter
plot(inter)


library(lightgbm)
train_data &lt;- lgb.Dataset(sm, label =  HR_data[, left] == 1)
params &lt;- list(objective = "binary", max_depth = 2)
lgb_model &lt;- lgb.train(params, train_data, 25)

inter &lt;- interactions(lgb_model, sm, option = "interactions")
inter
plot(inter)

inter &lt;- interactions(lgb_model, sm, option = "pairs")
inter
plot(inter)


</code></pre>

<hr>
<h2 id='lollipop'>Tables needed for lollipop plot</h2><span id='topic+lollipop'></span>

<h3>Description</h3>

<p>This function calculates two tables needed to generate lollipop plot, which visualise the model.
The first table contains information about all nodes in the trees forming a model.
It includes gain value, depth and ID of each nodes.
The second table contains similarly information about roots in the trees.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lollipop(xgb_model, data)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="lollipop_+3A_xgb_model">xgb_model</code></td>
<td>
<p>a xgboost or lightgbm model.</p>
</td></tr>
<tr><td><code id="lollipop_+3A_data">data</code></td>
<td>
<p>a data table with data used to train the model.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an object of the lollipop class
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library("EIX")
library("Matrix")
sm &lt;- sparse.model.matrix(left ~ . - 1,  data = HR_data)

library("xgboost")
param &lt;- list(objective = "binary:logistic", max_depth = 2)
xgb_model &lt;- xgboost(sm, params = param, label = HR_data[, left] == 1, nrounds = 25, verbose = 0)

lolli &lt;- lollipop(xgb_model, sm)
plot(lolli, labels = "topAll", log_scale = TRUE)


library(lightgbm)
train_data &lt;- lgb.Dataset(sm, label =  HR_data[, left] == 1)
params &lt;- list(objective = "binary", max_depth = 2)
lgb_model &lt;- lgb.train(params, train_data, 25)

lolli &lt;- lollipop(lgb_model, sm)
plot(lolli, labels = "topAll", log_scale = TRUE)



</code></pre>

<hr>
<h2 id='plot.importance'>Plot importance measures</h2><span id='topic+plot.importance'></span>

<h3>Description</h3>

<p>This functions plots selected measures of importance for variables and interactions.
It is possible to visualise importance table in two ways: radar plot with six measures
and scatter plot with two choosen measures.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'importance'
plot(
  x,
  ...,
  top = 10,
  radar = TRUE,
  text_start_point = 0.5,
  text_size = 3.5,
  xmeasure = "sumCover",
  ymeasure = "sumGain"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.importance_+3A_x">x</code></td>
<td>
<p>a result from the <code>importance</code> function.</p>
</td></tr>
<tr><td><code id="plot.importance_+3A_...">...</code></td>
<td>
<p>other parameters.</p>
</td></tr>
<tr><td><code id="plot.importance_+3A_top">top</code></td>
<td>
<p>number of positions on the plot or NULL for all variable. Default 10.</p>
</td></tr>
<tr><td><code id="plot.importance_+3A_radar">radar</code></td>
<td>
<p>TRUE/FALSE. If TRUE the plot shows
six measures of variables' or interactions' importance in the model.
If FALSE the plot containing two chosen measures
of variables' or interactions' importance in the model.</p>
</td></tr>
<tr><td><code id="plot.importance_+3A_text_start_point">text_start_point</code></td>
<td>
<p>place, where the names of the particular feature start. Available for 'radar=TRUE'. Range from 0 to 1. Default 0.5.</p>
</td></tr>
<tr><td><code id="plot.importance_+3A_text_size">text_size</code></td>
<td>
<p>size of the text on the plot. Default 3.5.</p>
</td></tr>
<tr><td><code id="plot.importance_+3A_xmeasure">xmeasure</code></td>
<td>
<p>measure on the x-axis.Available for 'radar=FALSE'. Default &quot;sumCover&quot;.</p>
</td></tr>
<tr><td><code id="plot.importance_+3A_ymeasure">ymeasure</code></td>
<td>
<p>measure on the y-axis. Available for 'radar=FALSE'. Default &quot;sumGain&quot;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Available measures:
</p>

<ul>
<li><p> &quot;sumGain&quot; - sum of Gain value in all nodes, in which given variable occurs,
</p>
</li>
<li><p> &quot;sumCover&quot; - sum of Cover value in all nodes, in which given variable occurs; for LightGBM models: number of observation, which pass through the node,
</p>
</li>
<li><p> &quot;mean5Gain&quot; - mean gain from 5 occurrences of given variable with the highest gain,
</p>
</li>
<li><p> &quot;meanGain&quot; - mean Gain value in all nodes, in which given variable occurs,
</p>
</li>
<li><p> &quot;meanCover&quot; - mean Cover value in all nodes, in which given variable occurs; for LightGBM models: mean number of observation, which pass through the node,
</p>
</li>
<li><p> &quot;freqency&quot; - number of occurrences in the nodes for given variable.
</p>
</li></ul>

<p>Additionally for plots with single variables:
</p>

<ul>
<li><p> &quot;meanDepth&quot;  - mean depth weighted by gain,
</p>
</li>
<li><p> &quot;numberOfRoots&quot; - number of occurrences in the root,
</p>
</li>
<li><p> &quot;weightedRoot&quot; - mean number of occurrences in the root, which is weighted by gain.
</p>
</li></ul>



<h3>Value</h3>

<p>a ggplot object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library("EIX")
library("Matrix")
sm &lt;- sparse.model.matrix(left ~ . - 1,  data = HR_data)

library("xgboost")
param &lt;- list(objective = "binary:logistic", max_depth = 2)
xgb_model &lt;- xgboost(sm, params = param, label = HR_data[, left] == 1, nrounds = 25, verbose=0)

imp &lt;- importance(xgb_model, sm, option = "both")
imp
plot(imp,  top = 10)

imp &lt;- importance(xgb_model, sm, option = "variables")
imp
plot(imp,  top = nrow(imp))

 imp &lt;- importance(xgb_model, sm, option = "interactions")
 imp
plot(imp,  top =  nrow(imp))

 imp &lt;- importance(xgb_model, sm, option = "variables")
 imp
plot(imp, top = NULL, radar = FALSE, xmeasure = "sumCover", ymeasure = "sumGain")


library(lightgbm)
train_data &lt;- lgb.Dataset(sm, label =  HR_data[, left] == 1)
params &lt;- list(objective = "binary", max_depth = 2)
lgb_model &lt;- lgb.train(params, train_data, 25)

imp &lt;- importance(lgb_model, sm, option = "both")
imp
plot(imp,  top = nrow(imp))

imp &lt;- importance(lgb_model, sm, option = "variables")
imp
plot(imp, top = NULL, radar = FALSE, xmeasure = "sumCover", ymeasure = "sumGain")



</code></pre>

<hr>
<h2 id='plot.interactions'>Plot importance of interactions or pairs</h2><span id='topic+plot.interactions'></span>

<h3>Description</h3>

<p>This function plots the importance ranking of interactions and pairs in the model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'interactions'
plot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.interactions_+3A_x">x</code></td>
<td>
<p>a result from the <code>interactions</code> function.</p>
</td></tr>
<tr><td><code id="plot.interactions_+3A_...">...</code></td>
<td>
<p>other parameters.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>NOTE: Be careful use of this function with <code>option="pairs"</code> parameter,
because high gain of pair can be a result of high gain of child variable.
As strong interactions should be considered only these pairs of variables,
where variable on the bottom (child) has higher gain than variable on the top (parent).
</p>


<h3>Value</h3>

<p>a ggplot object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library("EIX")
library("Matrix")
sm &lt;- sparse.model.matrix(left ~ . - 1,  data = HR_data)

library("xgboost")
param &lt;- list(objective = "binary:logistic", max_depth = 2)
xgb_model &lt;- xgboost(sm, params = param, label = HR_data[, left] == 1, nrounds = 25, verbose=0)

inter &lt;- interactions(xgb_model, sm,		option = "interactions")
inter
plot(inter)

inter &lt;- interactions(xgb_model, sm,		option = "pairs")
inter
plot(inter)


library(lightgbm)
train_data &lt;- lgb.Dataset(sm, label =  HR_data[, left] == 1)
params &lt;- list(objective = "binary", max_depth = 2)
lgb_model &lt;- lgb.train(params, train_data, 25)

inter &lt;- interactions(lgb_model, sm,		option = "interactions")
inter
plot(inter)

inter &lt;- interactions(lgb_model, sm,		option = "pairs")
inter
plot(inter)


</code></pre>

<hr>
<h2 id='plot.lollipop'>Visualiation of the model</h2><span id='topic+plot.lollipop'></span>

<h3>Description</h3>

<p>The lollipop plots the model with the most important interactions and variables in the roots.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'lollipop'
plot(x, ..., labels = "topAll", log_scale = TRUE, threshold = 0.1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.lollipop_+3A_x">x</code></td>
<td>
<p>a result from the <code>lollipop</code> function.</p>
</td></tr>
<tr><td><code id="plot.lollipop_+3A_...">...</code></td>
<td>
<p>other parameters.</p>
</td></tr>
<tr><td><code id="plot.lollipop_+3A_labels">labels</code></td>
<td>
<p>if &quot;topAll&quot; then labels for the most important interactions (vertical label)
and variables in the roots (horizontal label) will be displayed,
if &quot;interactions&quot; then labels for all interactions,
if &quot;roots&quot; then labels for all variables in the root.</p>
</td></tr>
<tr><td><code id="plot.lollipop_+3A_log_scale">log_scale</code></td>
<td>
<p>TRUE/FALSE logarithmic scale on the plot. Default TRUE.</p>
</td></tr>
<tr><td><code id="plot.lollipop_+3A_threshold">threshold</code></td>
<td>
<p>on the plot will occur only labels with Gain higher than 'threshold' of the max Gain value in the model.
The lower threshold, the more labels on the plot. Range from 0 to 1. Default 0.1.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a ggplot object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library("EIX")
library("Matrix")
sm &lt;- sparse.model.matrix(left ~ . - 1,  data = HR_data)

library("xgboost")
param &lt;- list(objective = "binary:logistic", max_depth = 2)
xgb_model &lt;- xgboost(sm, params = param, label = HR_data[, left] == 1, nrounds = 25, verbose = 0)

lolli &lt;- lollipop(xgb_model, sm)
plot(lolli, labels = "topAll", log_scale = TRUE)


library(lightgbm)
train_data &lt;- lgb.Dataset(sm, label =  HR_data[, left] == 1)
params &lt;- list(objective = "binary", max_depth = 3)
lgb_model &lt;- lgb.train(params, train_data, 25)

lolli &lt;- lollipop(lgb_model, sm)
plot(lolli, labels = "topAll", log_scale = TRUE)


</code></pre>

<hr>
<h2 id='tableOfTrees'>tableOfTrees</h2><span id='topic+tableOfTrees'></span>

<h3>Description</h3>

<p>tableOfTrees
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tableOfTrees(model, data)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tableOfTrees_+3A_model">model</code></td>
<td>
<p>a xgboost or lightgbm model</p>
</td></tr>
<tr><td><code id="tableOfTrees_+3A_data">data</code></td>
<td>
<p>a data table with data used to train the model</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a data table
</p>

<hr>
<h2 id='titanic_data'>Passengers and Crew on the RMS Titanic</h2><span id='topic+titanic_data'></span>

<h3>Description</h3>

<p>The <code>titanic</code> data is a complete list of passengers and crew members on  the RMS Titanic.
It includes a variable indicating whether a person did  survive the sinking of the RMS
Titanic on April 15, 1912.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(titanic_data)
</code></pre>


<h3>Format</h3>

<p>a data frame with 2207 rows and 11 columns
</p>


<h3>Details</h3>

<p>The description of the dataset was copied from the <code>DALEX</code> package.
</p>
<p>This dataset was copied from the <code>stablelearner</code> package and went through few variable
transformations. Levels in <code>embarked</code> was replaced with full names, <code>sibsp</code>, <code>parch</code> and <code>fare</code>
were converted to numerical variables and values for crew were replaced with 0.
If you use this dataset please cite the original package.
</p>
<p>From <code>stablelearner</code>: The website <a href="https://www.encyclopedia-titanica.org">https://www.encyclopedia-titanica.org</a> offers detailed  information about passengers and crew
members on the RMS Titanic. According to the website 1317 passengers and 890 crew member were abord.
8 musicians and 9 employees of the shipyard company are listed as passengers, but travelled with a
free ticket, which is why they have <code>NA</code> values in <code>fare</code>. In addition to that, <code>fare</code>
is truely missing for a few regular passengers.
</p>

<ul>
<li><p> gender a factor with levels <code>male</code> and <code>female</code>.
</p>
</li>
<li><p> age a numeric value with the persons age on the day of the sinking.
</p>
</li>
<li><p> class a factor specifying the class for passengers or the type of service aboard for crew members.
</p>
</li>
<li><p> embarked a factor with the persons place of of embarkment (Belfast/Cherbourg/Queenstown/Southampton).
</p>
</li>
<li><p> country a factor with the persons home country.
</p>
</li>
<li><p> fare a numeric value with the ticket price (<code>0</code> for crew members, musicians and employees of the shipyard company).
</p>
</li>
<li><p> sibsp an ordered factor specifying the number if siblings/spouses aboard; adopted from Vanderbild data set (see below).
</p>
</li>
<li><p> parch an ordered factor specifying the number of parents/children aboard; adopted from Vanderbild data set (see below).
</p>
</li>
<li><p> survived a factor with two levels (<code>no</code> and <code>yes</code>) specifying whether the person has survived the sinking.
</p>
</li></ul>



<h3>Source</h3>

<p>The description of dataset was copied from the <code>DALEX</code> package.
This dataset was copied from the <code>stablelearner</code> package and went through few variable
transformations. The complete list of persons on the RMS titanic was downloaded from
<a href="https://www.encyclopedia-titanica.org">https://www.encyclopedia-titanica.org</a> on April 5, 2016.
</p>


<h3>References</h3>

<p><a href="https://www.encyclopedia-titanica.org">https://www.encyclopedia-titanica.org</a>,
<a href="https://CRAN.R-project.org/package=stablelearner">https://CRAN.R-project.org/package=stablelearner</a>, <a href="https://cran.r-project.org/package=DALEX">https://cran.r-project.org/package=DALEX</a>.
</p>

<hr>
<h2 id='waterfall'>Explain prediction of a single observation</h2><span id='topic+waterfall'></span>

<h3>Description</h3>

<p>This function calculates a table with influence of variables and interactions
on the prediction of a given observation. It supports only xgboost models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>waterfall(
  xgb_model,
  new_observation,
  data,
  type = "binary",
  option = "interactions",
  baseline = 0
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="waterfall_+3A_xgb_model">xgb_model</code></td>
<td>
<p>a xgboost model.</p>
</td></tr>
<tr><td><code id="waterfall_+3A_new_observation">new_observation</code></td>
<td>
<p>a new observation.</p>
</td></tr>
<tr><td><code id="waterfall_+3A_data">data</code></td>
<td>
<p>row from the original dataset with the new observation to explain (not one-hot-encoded).
The param above has to be set to merge categorical features.
If you dont wont to merge categorical features, set this parameter the same as <code>new_observation</code>.</p>
</td></tr>
<tr><td><code id="waterfall_+3A_type">type</code></td>
<td>
<p>the learning task of the model. Available tasks: &quot;binary&quot; for binary classification  or &quot;regression&quot; for linear regression.</p>
</td></tr>
<tr><td><code id="waterfall_+3A_option">option</code></td>
<td>
<p>if &quot;variables&quot;, the plot includes only single variables,
if &quot;interactions&quot;, then only interactions.
Default &quot;interaction&quot;.</p>
</td></tr>
<tr><td><code id="waterfall_+3A_baseline">baseline</code></td>
<td>
<p>a number or a character &quot;Intercept&quot; (for model intercept).
The baseline for the plot, where the rectangles should start.
Default 0.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function contains code or pieces of code
from <code>breakDown</code> code created by Przemysław Biecek
and <code>xgboostExplainer</code> code created by David Foster.
</p>


<h3>Value</h3>

<p>an object of the broken class
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

library("EIX")
library("Matrix")
sm &lt;- sparse.model.matrix(left ~ . - 1,  data = HR_data)

library("xgboost")
param &lt;- list(objective = "binary:logistic", max_depth = 2)
xgb_model &lt;- xgboost(sm, params = param, label = HR_data[, left] == 1, nrounds = 25, verbose=0)

data &lt;- HR_data[9,-7]
new_observation &lt;- sm[9,]

wf &lt;- waterfall(xgb_model, new_observation, data,  option = "interactions")
wf

plot(wf)


</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
