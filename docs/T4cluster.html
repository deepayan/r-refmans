<!DOCTYPE html><html><head><title>Help for package T4cluster</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {T4cluster}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#compare.adjrand'><p>(+) Adjusted Rand Index</p></a></li>
<li><a href='#compare.rand'><p>(+) Rand Index</p></a></li>
<li><a href='#dpmeans'><p>DP-Means Clustering</p></a></li>
<li><a href='#EKSS'><p>Ensembles of K-Subspaces</p></a></li>
<li><a href='#funhclust'><p>Functional Hierarchical Clustering</p></a></li>
<li><a href='#funkmeans03A'><p>Functional K-Means Clustering by Abraham et al. (2003)</p></a></li>
<li><a href='#gen3S'><p>Generate from Three 5-dimensional Subspaces in 200-dimensional space.</p></a></li>
<li><a href='#genDONUTS'><p>Generate Nested Donuts</p></a></li>
<li><a href='#genLP'><p>Generate Line and Plane Example with Fixed Number of Components</p></a></li>
<li><a href='#genSMILEY'><p>Generate SMILEY Data</p></a></li>
<li><a href='#gmm'><p>Finite Gaussian Mixture Model</p></a></li>
<li><a href='#gmm03F'><p>Ensemble of Gaussian Mixtures with Random Projection</p></a></li>
<li><a href='#gmm11R'><p>Regularized GMM by Ruan et al. (2011)</p></a></li>
<li><a href='#gmm16G'><p>Weighted GMM by Gebru et al. (2016)</p></a></li>
<li><a href='#gskmeans'><p>Geodesic Spherical K-Means</p></a></li>
<li><a href='#household'><p>Load 'household' data</p></a></li>
<li><a href='#kmeans'><p>K-Means Clustering</p></a></li>
<li><a href='#kmeans18B'><p>K-Means Clustering with Lightweight Coreset</p></a></li>
<li><a href='#kmeanspp'><p>K-Means++ Clustering</p></a></li>
<li><a href='#LRR'><p>Low-Rank Representation</p></a></li>
<li><a href='#LRSC'><p>Low-Rank Subspace Clustering</p></a></li>
<li><a href='#LSR'><p>Least Squares Regression</p></a></li>
<li><a href='#MSM'><p>Bayesian Mixture of Subspaces of Different Dimensions</p></a></li>
<li><a href='#pcm'><p>Compute Pairwise Co-occurrence Matrix</p></a></li>
<li><a href='#predict.MSM'><p>S3 method to predict class label of new data with 'MSM' object</p></a></li>
<li><a href='#psm'><p>Compute Posterior Similarity Matrix</p></a></li>
<li><a href='#sc05Z'><p>Spectral Clustering by Zelnik-Manor and Perona (2005)</p></a></li>
<li><a href='#sc09G'><p>Spectral Clustering by Gu and Wang (2009)</p></a></li>
<li><a href='#sc10Z'><p>Spectral Clustering by Zhang et al. (2010)</p></a></li>
<li><a href='#sc11Y'><p>Spectral Clustering by Yang et al. (2011)</p></a></li>
<li><a href='#sc12L'><p>Spectral Clustering by Li and Guo (2012)</p></a></li>
<li><a href='#scNJW'><p>Spectral Clustering by Ng, Jordan, and Weiss (2002)</p></a></li>
<li><a href='#scSM'><p>Spectral Clustering by Shi and Malik (2000)</p></a></li>
<li><a href='#scUL'><p>Spectral Clustering with Unnormalized Laplacian</p></a></li>
<li><a href='#spkmeans'><p>Spherical K-Means Clustering</p></a></li>
<li><a href='#SSC'><p>Sparse Subspace Clustering</p></a></li>
<li><a href='#SSQP'><p>Subspace Segmentation via Quadratic Programming</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Tools for Cluster Analysis</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.2</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Description:</td>
<td>Cluster analysis is one of the most fundamental problems in data science. We provide a variety of algorithms from clustering to the learning on the space of partitions. See Hennig, Meila, and Rocci (2016, ISBN:9781466551886) for general exposition to cluster analysis.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp (&ge; 1.0.5), Rdpack, Rdimtools, ADMM, MASS, fda, ggplot2,
lpSolve, maotai, mclustcomp, rstiefel, scatterplot3d, stats,
utils</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://kisungyou.com/T4cluster/">https://kisungyou.com/T4cluster/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/kisungyou/T4cluster/issues">https://github.com/kisungyou/T4cluster/issues</a></td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo</td>
</tr>
<tr>
<td>RdMacros:</td>
<td>Rdpack</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.1</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10)</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2021-08-14 02:57:45 UTC; kisung</td>
</tr>
<tr>
<td>Author:</td>
<td>Kisung You <a href="https://orcid.org/0000-0002-8584-459X"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut,
    cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Kisung You &lt;kisungyou@outlook.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2021-08-16 07:20:06 UTC</td>
</tr>
</table>
<hr>
<h2 id='compare.adjrand'>(+) Adjusted Rand Index</h2><span id='topic+compare.adjrand'></span>

<h3>Description</h3>

<p>Compute Adjusted Rand index between two clusterings. Please note that the 
value can yield negative value.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compare.adjrand(x, y)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="compare.adjrand_+3A_x">x</code></td>
<td>
<p>1st cluster label vector of length-<code class="reqn">n</code>.</p>
</td></tr>
<tr><td><code id="compare.adjrand_+3A_y">y</code></td>
<td>
<p>2nd cluster label vector of length-<code class="reqn">n</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Adjusted Rand Index value.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+compare.rand">compare.rand</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# -------------------------------------------------------------
#         true label vs. clustering with 'iris' dataset 
# -------------------------------------------------------------
## PREPARE
data(iris)
X   = as.matrix(iris[,1:4])
lab = as.integer(as.factor(iris[,5]))

## CLUSTERING WITH DIFFERENT K VALUES
vec_k  = 2:7
vec_cl = list()
for (i in 1:length(vec_k)){
  vec_cl[[i]] = T4cluster::kmeans(X, k=round(vec_k[i]))$cluster
}

## COMPUTE COMPARISON INDICES
vec_comp = rep(0, length(vec_k))
for (i in 1:length(vec_k)){
  vec_comp[i] = compare.adjrand(vec_cl[[i]], lab)
}

## VISUALIZE
opar &lt;- par(no.readonly=TRUE)
plot(vec_k, vec_comp, type="b", lty=2, xlab="number of clusters", 
     ylab="comparison index", main="Adjusted Rand Index with true k=3")
abline(v=3, lwd=2, col="red")
par(opar)


</code></pre>

<hr>
<h2 id='compare.rand'>(+) Rand Index</h2><span id='topic+compare.rand'></span>

<h3>Description</h3>

<p>Compute Rand index between two clusterings. It has a value between 0 and 1 
where 0 indicates two clusterings do not agree and 1 exactly the same.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compare.rand(x, y)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="compare.rand_+3A_x">x</code></td>
<td>
<p>1st cluster label vector of length-<code class="reqn">n</code>.</p>
</td></tr>
<tr><td><code id="compare.rand_+3A_y">y</code></td>
<td>
<p>2nd cluster label vector of length-<code class="reqn">n</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Rand Index value.
</p>


<h3>References</h3>

<p>Rand WM (1971).
&ldquo;Objective Criteria for the Evaluation of Clustering Methods.&rdquo;
<em>Journal of the American Statistical Association</em>, <b>66</b>(336), 846.
ISSN 01621459.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# -------------------------------------------------------------
#         true label vs. clustering with 'iris' dataset 
# -------------------------------------------------------------
## PREPARE
data(iris)
X   = as.matrix(iris[,1:4])
lab = as.integer(as.factor(iris[,5]))

## CLUSTERING WITH DIFFERENT K VALUES
vec_k  = 2:7
vec_cl = list()
for (i in 1:length(vec_k)){
  vec_cl[[i]] = T4cluster::kmeans(X, k=round(vec_k[i]))$cluster
}

## COMPUTE COMPARISON INDICES
vec_comp = rep(0, length(vec_k))
for (i in 1:length(vec_k)){
  vec_comp[i] = compare.rand(vec_cl[[i]], lab)
}

## VISUALIZE
opar &lt;- par(no.readonly=TRUE)
plot(vec_k, vec_comp, type="b", lty=2, xlab="number of clusters", 
     ylab="comparison index", main="Rand Index with true k=3")
abline(v=3, lwd=2, col="red")
par(opar)


</code></pre>

<hr>
<h2 id='dpmeans'>DP-Means Clustering</h2><span id='topic+dpmeans'></span>

<h3>Description</h3>

<p>DP-means is a non-parametric clustering method motivated by DP mixture model in that 
the number of clusters is determined by a parameter <code class="reqn">\lambda</code>. The larger 
the <code class="reqn">\lambda</code> value is, the smaller the number of clusters is attained. 
In addition to the original paper, we added an option to randomly permute 
an order of updating for each observation's membership as a common 
heuristic in the literature of cluster analysis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dpmeans(data, lambda = 0.1, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dpmeans_+3A_data">data</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix of row-stacked observations.</p>
</td></tr>
<tr><td><code id="dpmeans_+3A_lambda">lambda</code></td>
<td>
<p>a threshold to define a new cluster (default: 0.1).</p>
</td></tr>
<tr><td><code id="dpmeans_+3A_...">...</code></td>
<td>
<p>extra parameters including </p>

<dl>
<dt>maxiter</dt><dd><p>the maximum number of iterations (default: 10).</p>
</dd>
<dt>eps</dt><dd><p>the stopping criterion for iterations (default: 1e-5).</p>
</dd>
<dt>permute</dt><dd><p>a logical; <code>TRUE</code> if random order for update is used, <code>FALSE</code> otherwise (default).</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list of S3 class <code>T4cluster</code> containing 
</p>

<dl>
<dt>cluster</dt><dd><p>a length-<code class="reqn">n</code> vector of class labels (from <code class="reqn">1:k</code>).</p>
</dd> 
<dt>algorithm</dt><dd><p>name of the algorithm.</p>
</dd>
</dl>



<h3>References</h3>

<p>Kulis B, Jordan MI (2012).
&ldquo;Revisiting K-Means: New Algorithms via Bayesian Nonparametrics.&rdquo;
In <em>Proceedings of the 29th International Coference on International Conference on Machine Learning</em>,  ICML'12, 1131&ndash;1138.
ISBN 978-1-4503-1285-1.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># -------------------------------------------------------------
#            clustering with 'iris' dataset
# -------------------------------------------------------------
## PREPARE
data(iris)
X   = as.matrix(iris[,1:4])
lab = as.integer(as.factor(iris[,5]))

## EMBEDDING WITH PCA
X2d = Rdimtools::do.pca(X, ndim=2)$Y

## CLUSTERING WITH DIFFERENT LAMBDA VALUES
dpm1 = dpmeans(X, lambda=1)$cluster
dpm2 = dpmeans(X, lambda=5)$cluster
dpm3 = dpmeans(X, lambda=25)$cluster

## VISUALIZATION
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,4), pty="s")
plot(X2d, col=lab, pch=19, main="true label")
plot(X2d, col=dpm1, pch=19, main="dpmeans: lambda=1")
plot(X2d, col=dpm2, pch=19, main="dpmeans: lambda=5")
plot(X2d, col=dpm3, pch=19, main="dpmeans: lambda=25")
par(opar)

</code></pre>

<hr>
<h2 id='EKSS'>Ensembles of K-Subspaces</h2><span id='topic+EKSS'></span>

<h3>Description</h3>

<p>Ensembles of K-Subspaces method exploits multiple runs of K-Subspace Clustering and 
uses consensus framework to aggregate multiple clustering results 
to mitigate the effect of random initializations. When the results are merged, 
it zeros out <code class="reqn">n-q</code> number of values in a co-occurrence matrix. The paper 
suggests to use large number of runs (<code>B</code>) where each run may not require 
large number of iterations (<code>iter</code>) since the main assumption of the 
algorithm is to utilize multiple partially-correct information. At the extreme case, 
iteration <code>iter</code> may be set to 0 for which the paper denotes it as EKSS-0.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>EKSS(data, k = 2, d = 2, q = floor(nrow(data) * 0.75), B = 500, iter = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="EKSS_+3A_data">data</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix of row-stacked observations.</p>
</td></tr>
<tr><td><code id="EKSS_+3A_k">k</code></td>
<td>
<p>the number of clusters (default: 2).</p>
</td></tr>
<tr><td><code id="EKSS_+3A_d">d</code></td>
<td>
<p>candidate dimension for each subspace (default: 2).</p>
</td></tr>
<tr><td><code id="EKSS_+3A_q">q</code></td>
<td>
<p>threshold; the number of smaller values to be zeroed out (default: 0.75*<code class="reqn">n</code>).</p>
</td></tr>
<tr><td><code id="EKSS_+3A_b">B</code></td>
<td>
<p>the number of ensembles/runs (default: 500).</p>
</td></tr>
<tr><td><code id="EKSS_+3A_iter">iter</code></td>
<td>
<p>the number of iteration for each run (default: 0).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list of S3 class <code>T4cluster</code> containing 
</p>

<dl>
<dt>cluster</dt><dd><p>a length-<code class="reqn">n</code> vector of class labels (from <code class="reqn">1:k</code>).</p>
</dd> 
<dt>algorithm</dt><dd><p>name of the algorithm.</p>
</dd>
</dl>



<h3>References</h3>

<p>Lipor J, Hong D, Tan YS, Balzano L (2021). “Subspace Clustering Using Ensembles of <code class="reqn">K</code>-Subspaces.” arXiv:1709.04744.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## generate a toy example
set.seed(10)
tester = genLP(n=100, nl=2, np=1, iso.var=0.1)
data   = tester$data
label  = tester$class

## do PCA for data reduction
proj = base::eigen(stats::cov(data))$vectors[,1:2]
dat2 = data%*%proj

## run EKSS algorithm with k=2,3,4 with EKSS-0 and 5 iterations
out2zero = EKSS(data, k=2)
out3zero = EKSS(data, k=3)
out4zero = EKSS(data, k=4)

out2iter = EKSS(data, k=2, iter=5)
out3iter = EKSS(data, k=3, iter=5)
out4iter = EKSS(data, k=4, iter=5)

## extract label information
lab2zero = out2zero$cluster
lab3zero = out3zero$cluster
lab4zero = out4zero$cluster

lab2iter = out2iter$cluster
lab3iter = out3iter$cluster
lab4iter = out4iter$cluster

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(2,3))
plot(dat2, pch=19, cex=0.9, col=lab2zero, main="EKSS-0:K=2")
plot(dat2, pch=19, cex=0.9, col=lab3zero, main="EKSS-0:K=3")
plot(dat2, pch=19, cex=0.9, col=lab4zero, main="EKSS-0:K=4")
plot(dat2, pch=19, cex=0.9, col=lab2iter, main="EKSS iter:K=2")
plot(dat2, pch=19, cex=0.9, col=lab3iter, main="EKSS iter:K=3")
plot(dat2, pch=19, cex=0.9, col=lab4iter, main="EKSS iter:K=4")
par(opar)


</code></pre>

<hr>
<h2 id='funhclust'>Functional Hierarchical Clustering</h2><span id='topic+funhclust'></span>

<h3>Description</h3>

<p>Given <code class="reqn">N</code> curves <code class="reqn">\gamma_1 (t), \gamma_2 (t), \ldots, \gamma_N (t) : I \rightarrow \mathbf{R}</code>, 
perform hierarchical agglomerative clustering with <span class="pkg">fastcluster</span> package's implementation of 
the algorithm. Dissimilarity for curves is measured by <code class="reqn">L_p</code> metric.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>funhclust(
  fdobj,
  p = 2,
  method = c("single", "complete", "average", "mcquitty", "ward.D", "ward.D2",
    "centroid", "median"),
  members = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="funhclust_+3A_fdobj">fdobj</code></td>
<td>
<p>a <code>'fd'</code> functional data object of <code class="reqn">N</code> curves by the <span class="pkg">fda</span> package.</p>
</td></tr>
<tr><td><code id="funhclust_+3A_p">p</code></td>
<td>
<p>an exponent in <code class="reqn">L_p</code> formalism (default: 2).</p>
</td></tr>
<tr><td><code id="funhclust_+3A_method">method</code></td>
<td>
<p>agglomeration method to be used. This must be one of <code>"single"</code>, <code>"complete"</code>, <code>"average"</code>, <code>"mcquitty"</code>, <code>"ward.D"</code>, <code>"ward.D2"</code>, <code>"centroid"</code> or <code>"median"</code>.</p>
</td></tr>
<tr><td><code id="funhclust_+3A_members">members</code></td>
<td>
<p><code>NULL</code> or a vector whose length equals the number of observations. See <code><a href="stats.html#topic+hclust">hclust</a></code> for details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an object of class <code>hclust</code>. See <code><a href="stats.html#topic+hclust">hclust</a></code> for details.
</p>


<h3>References</h3>

<p>Ferreira L, Hitchcock DB (2009).
&ldquo;A Comparison of Hierarchical Methods for Clustering Functional Data.&rdquo;
<em>Communications in Statistics - Simulation and Computation</em>, <b>38</b>(9), 1925&ndash;1949.
ISSN 0361-0918, 1532-4141.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># -------------------------------------------------------------
#                     two types of curves
#
# type 1 : sin(x) + perturbation; 20 OF THESE ON [0, 2*PI]
# type 2 : cos(x) + perturbation; 20 OF THESE ON [0, 2*PI]
# -------------------------------------------------------------
## PREPARE : USE 'fda' PACKAGE
#  Generate Raw Data
datx = seq(from=0, to=2*pi, length.out=100)
daty = array(0,c(100, 40))
for (i in 1:20){
  daty[,i]    = sin(datx) + rnorm(100, sd=0.1)
  daty[,i+20] = cos(datx) + rnorm(100, sd=0.1)
}
#  Wrap as 'fd' object
mybasis &lt;- fda::create.bspline.basis(c(0,2*pi), nbasis=10)
myfdobj &lt;- fda::smooth.basis(datx, daty, mybasis)$fd

## RUN THE ALGORITHM 
hcsingle = funhclust(myfdobj, method="single")

## VISUALIZE
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
matplot(datx, daty[,1:20],  type="l", main="Curves Type 1")
matplot(datx, daty[,21:40], type="l", main="Curves Type 2")
plot(hcsingle, main="hclust with 'single' linkage")
par(opar)


</code></pre>

<hr>
<h2 id='funkmeans03A'>Functional K-Means Clustering by Abraham et al. (2003)</h2><span id='topic+funkmeans03A'></span>

<h3>Description</h3>

<p>Given <code class="reqn">N</code> curves <code class="reqn">\gamma_1 (t), \gamma_2 (t), \ldots, \gamma_N (t) : I \rightarrow \mathbf{R}</code>, 
perform <code class="reqn">k</code>-means clustering on the coefficients from the functional data expanded by 
B-spline basis. Note that in the original paper, authors used B-splines as the choice of basis 
due to nice properties. However, we allow other types of basis as well for convenience.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>funkmeans03A(fdobj, k = 2, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="funkmeans03A_+3A_fdobj">fdobj</code></td>
<td>
<p>a <code>'fd'</code> functional data object of <code class="reqn">N</code> curves by the <span class="pkg">fda</span> package.</p>
</td></tr>
<tr><td><code id="funkmeans03A_+3A_k">k</code></td>
<td>
<p>the number of clusters (default: 2).</p>
</td></tr>
<tr><td><code id="funkmeans03A_+3A_...">...</code></td>
<td>
<p>extra parameters including </p>

<dl>
<dt>maxiter</dt><dd><p>the maximum number of iterations (default: 10).</p>
</dd>
<dt>nstart</dt><dd><p>the number of random initializations (default: 5).</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list of S3 class <code>T4cluster</code> containing 
</p>

<dl>
<dt>cluster</dt><dd><p>a length-<code class="reqn">N</code> vector of class labels (from <code class="reqn">1:k</code>).</p>
</dd> 
<dt>mean</dt><dd><p>a <code>'fd'</code> object of <code class="reqn">k</code> mean curves.</p>
</dd>
<dt>algorithm</dt><dd><p>name of the algorithm.</p>
</dd>
</dl>



<h3>References</h3>

<p>Abraham C, Cornillon PA, Matzner-Lober E, Molinari N (2003).
&ldquo;Unsupervised Curve Clustering Using B-Splines.&rdquo;
<em>Scandinavian Journal of Statistics</em>, <b>30</b>(3), 581&ndash;595.
ISSN 0303-6898, 1467-9469.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># -------------------------------------------------------------
#                     two types of curves
#
# type 1 : sin(x) + perturbation; 20 OF THESE ON [0, 2*PI]
# type 2 : cos(x) + perturbation; 20 OF THESE ON [0, 2*PI]
# type 3 : sin(x) + cos(0.5x)   ; 20 OF THESE ON [0, 2*PI]
# -------------------------------------------------------------
## PREPARE : USE 'fda' PACKAGE
#  Generate Raw Data
datx = seq(from=0, to=2*pi, length.out=100)
daty = array(0,c(100, 60))
for (i in 1:20){
  daty[,i]    = sin(datx) + rnorm(100, sd=0.5)
  daty[,i+20] = cos(datx) + rnorm(100, sd=0.5)
  daty[,i+40] = sin(datx) + cos(0.5*datx) + rnorm(100, sd=0.5)
}
#  Wrap as 'fd' object
mybasis &lt;- fda::create.bspline.basis(c(0,2*pi), nbasis=10)
myfdobj &lt;- fda::smooth.basis(datx, daty, mybasis)$fd

## RUN THE ALGORITHM WITH K=2,3,4
fk2 = funkmeans03A(myfdobj, k=2)
fk3 = funkmeans03A(myfdobj, k=3)
fk4 = funkmeans03A(myfdobj, k=4)

## FUNCTIONAL PCA FOR VISUALIZATION
embed = fda::pca.fd(myfdobj, nharm=2)$score

## VISUALIZE
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(embed, col=fk2$cluster, pch=19, main="K=2")
plot(embed, col=fk3$cluster, pch=19, main="K=3")
plot(embed, col=fk4$cluster, pch=19, main="K=4")
par(opar)

</code></pre>

<hr>
<h2 id='gen3S'>Generate from Three 5-dimensional Subspaces in 200-dimensional space.</h2><span id='topic+gen3S'></span>

<h3>Description</h3>

<p>Generate from Three 5-dimensional Subspaces in 200-dimensional space.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gen3S(n = 50, var = 0.3)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gen3S_+3A_n">n</code></td>
<td>
<p>the number of data points sampled from each subspace (default: 50).</p>
</td></tr>
<tr><td><code id="gen3S_+3A_var">var</code></td>
<td>
<p>degree of Gaussian noise (default: 0.3).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing with :</p>

<dl>
<dt>data</dt><dd><p>an <code class="reqn">(3*n\times 3)</code> data matrix.</p>
</dd>
<dt>class</dt><dd><p>length-<code class="reqn">3*n</code> vector for class label.</p>
</dd>
</dl>



<h3>References</h3>

<p>Wang S, Yuan X, Yao T, Yan S, Shen J (2011).
&ldquo;Efficient Subspace Segmentation via Quadratic Programming.&rdquo;
In <em>Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence</em>,  AAAI'11, 519&ndash;524.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## a toy example
tester = gen3S(n=100)
data   = tester$data
label  = tester$class


</code></pre>

<hr>
<h2 id='genDONUTS'>Generate Nested Donuts</h2><span id='topic+genDONUTS'></span>

<h3>Description</h3>

<p>It generates nested <em>donuts</em>, which are just hollow circles. For flexible 
testing, the parameter <code>k</code> controls the number of circles of varying 
radii where <code>n</code> controls the number of observations for each circle.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>genDONUTS(n = 50, k = 2, sd = 0.1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="genDONUTS_+3A_n">n</code></td>
<td>
<p>the number of data points for each hollow circle (default: 50).</p>
</td></tr>
<tr><td><code id="genDONUTS_+3A_k">k</code></td>
<td>
<p>the number of circles (default: 2).</p>
</td></tr>
<tr><td><code id="genDONUTS_+3A_sd">sd</code></td>
<td>
<p>magnitude of white noise (default: 0.1).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing with <code class="reqn">m = nk</code>:</p>

<dl>
<dt>data</dt><dd><p>an <code class="reqn">(m\times 2)</code> data matrix.</p>
</dd>
<dt>label</dt><dd><p>a length-<code class="reqn">m</code> vector(factor) for class labels.</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>
## generate data
donut2 = genDONUTS(k=2)
donut3 = genDONUTS(k=3)
donut4 = genDONUTS(k=4)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3), pty="s")
plot(donut2$data, col=donut2$label, pch=19, main="k=2")
plot(donut3$data, col=donut3$label, pch=19, main="k=3")
plot(donut4$data, col=donut4$label, pch=19, main="k=4")
par(opar)


</code></pre>

<hr>
<h2 id='genLP'>Generate Line and Plane Example with Fixed Number of Components</h2><span id='topic+genLP'></span>

<h3>Description</h3>

<p>This function generates a toy example of 'line and plane' data in <code class="reqn">R^3</code> that 
data are generated from a mixture of lines (one-dimensional) planes (two-dimensional).
The number of line- and plane-components are explicitly set by the user for flexible testing.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>genLP(n = 100, nl = 1, np = 1, iso.var = 0.1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="genLP_+3A_n">n</code></td>
<td>
<p>the number of data points for each line and plane.</p>
</td></tr>
<tr><td><code id="genLP_+3A_nl">nl</code></td>
<td>
<p>the number of line components.</p>
</td></tr>
<tr><td><code id="genLP_+3A_np">np</code></td>
<td>
<p>the number of plane components.</p>
</td></tr>
<tr><td><code id="genLP_+3A_iso.var">iso.var</code></td>
<td>
<p>degree of isotropic variance.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing with <code class="reqn">m = n\times(nl+np)</code>:</p>

<dl>
<dt>data</dt><dd><p>an <code class="reqn">(m\times 3)</code> data matrix.</p>
</dd>
<dt>class</dt><dd><p>length-<code class="reqn">m</code> vector for class label.</p>
</dd>
<dt>dimension</dt><dd><p>length-<code class="reqn">m</code> vector of corresponding dimension from which an observation is created.</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>## test for visualization
set.seed(10)
tester = genLP(n=100, nl=1, np=2, iso.var=0.1)
data   = tester$data
label  = tester$class

## do PCA for data reduction
proj = base::eigen(stats::cov(data))$vectors[,1:2]
dat2 = data%*%proj

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(2,2), pty="s")
plot(dat2[,1],dat2[,2],pch=19,cex=0.5,col=label,main="PCA")
plot(data[,1],data[,2],pch=19,cex=0.5,col=label,main="Axis 1 vs 2")
plot(data[,1],data[,3],pch=19,cex=0.5,col=label,main="Axis 1 vs 3")
plot(data[,2],data[,3],pch=19,cex=0.5,col=label,main="Axis 2 vs 3")
par(opar)

## Not run: 
## visualize in 3d
x11()
scatterplot3d::scatterplot3d(x=data, pch=19, cex.symbols=0.5, color=label)

## End(Not run)

</code></pre>

<hr>
<h2 id='genSMILEY'>Generate SMILEY Data</h2><span id='topic+genSMILEY'></span>

<h3>Description</h3>

<p>Creates a smiley-face data in <code class="reqn">\mathbf{R}^2</code>. This function is a modification 
of <span class="pkg">mlbench</span>'s <code>mlbench.smiley</code> function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>genSMILEY(n = 496, sd = 0.1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="genSMILEY_+3A_n">n</code></td>
<td>
<p>number of samples to be generated.</p>
</td></tr>
<tr><td><code id="genSMILEY_+3A_sd">sd</code></td>
<td>
<p>additive Gaussian noise level.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list containing</p>

<dl>
<dt>data</dt><dd><p>an <code class="reqn">(n\times 2)</code> data matrix.</p>
</dd>
<dt>label</dt><dd><p>a length-<code class="reqn">n</code> vector(factor) for class labels.</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>
## Generate SMILEY Data with Difference Noise Levels
s10 = genSMILEY(200, sd=0.1)
s25 = genSMILEY(200, sd=0.25)
s50 = genSMILEY(200, sd=0.5)

## Visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3), pty="s")
plot(s10$data, col=s10$label, pch=19, main="sd=0.10")
plot(s25$data, col=s25$label, pch=19, main="sd=0.25")
plot(s50$data, col=s50$label, pch=19, main="sd=0.50")
par(opar)


</code></pre>

<hr>
<h2 id='gmm'>Finite Gaussian Mixture Model</h2><span id='topic+gmm'></span>

<h3>Description</h3>

<p>Finite Gaussian Mixture Model (GMM) is a well-known probabilistic clustering algorithm by fitting the following distribution to the data
</p>
<p style="text-align: center;"><code class="reqn">f(x; \left\lbrace \mu_k, \Sigma_k \right\rbrace_{k=1}^K) = \sum_{k=1}^K w_k N(x; \mu_k, \Sigma_k)</code>
</p>

<p>with parameters <code class="reqn">w_k</code>'s for cluster weights, <code class="reqn">\mu_k</code>'s for class means, and <code class="reqn">\Sigma_k</code>'s for class covariances. 
This function is a wrapper for <span class="pkg">Armadillo</span>'s GMM function, which supports two types of covariance models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gmm(data, k = 2, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gmm_+3A_data">data</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix of row-stacked observations.</p>
</td></tr>
<tr><td><code id="gmm_+3A_k">k</code></td>
<td>
<p>the number of clusters (default: 2).</p>
</td></tr>
<tr><td><code id="gmm_+3A_...">...</code></td>
<td>
<p>extra parameters including </p>

<dl>
<dt>maxiter</dt><dd><p>the maximum number of iterations (default: 10).</p>
</dd>
<dt>usediag</dt><dd><p>a logical; covariances are diagonal if <code>TRUE</code>, or full covariances are returned for <code>FALSE</code> (default: <code>FALSE</code>).</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list of S3 class <code>T4cluster</code> containing 
</p>

<dl>
<dt>cluster</dt><dd><p>a length-<code class="reqn">n</code> vector of class labels (from <code class="reqn">1:k</code>).</p>
</dd>
<dt>mean</dt><dd><p>a <code class="reqn">(k\times p)</code> matrix where each row is a class mean.</p>
</dd>
<dt>variance</dt><dd><p>a <code class="reqn">(p\times p\times k)</code> array where each slice is a class covariance.</p>
</dd>
<dt>weight</dt><dd><p>a length-<code class="reqn">k</code> vector of class weights that sum to 1.</p>
</dd>
<dt>loglkd</dt><dd><p>log-likelihood of the data for the fitted model.</p>
</dd>
<dt>algorithm</dt><dd><p>name of the algorithm.</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'># -------------------------------------------------------------
#            clustering with 'iris' dataset
# -------------------------------------------------------------
## PREPARE
data(iris)
X   = as.matrix(iris[,1:4])
lab = as.integer(as.factor(iris[,5]))

## EMBEDDING WITH PCA
X2d = Rdimtools::do.pca(X, ndim=2)$Y  

## CLUSTERING WITH DIFFERENT K VALUES
cl2 = gmm(X, k=2)$cluster
cl3 = gmm(X, k=3)$cluster
cl4 = gmm(X, k=4)$cluster

## VISUALIZATION
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,4), pty="s")
plot(X2d, col=lab, pch=19, main="true label")
plot(X2d, col=cl2, pch=19, main="gmm: k=2")
plot(X2d, col=cl3, pch=19, main="gmm: k=3")
plot(X2d, col=cl4, pch=19, main="gmm: k=4")
par(opar)

</code></pre>

<hr>
<h2 id='gmm03F'>Ensemble of Gaussian Mixtures with Random Projection</h2><span id='topic+gmm03F'></span>

<h3>Description</h3>

<p>When the data lies in a high-dimensional Euclidean space, fitting a model-based 
clustering algorithm is troublesome. This function implements an algorithm 
from the reference, which uses an aggregate information from an ensemble of 
Gaussian mixtures in combination with random projection.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gmm03F(data, k = 2, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gmm03F_+3A_data">data</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix of row-stacked observations.</p>
</td></tr>
<tr><td><code id="gmm03F_+3A_k">k</code></td>
<td>
<p>the number of clusters (default: 2).</p>
</td></tr>
<tr><td><code id="gmm03F_+3A_...">...</code></td>
<td>
<p>extra parameters including </p>

<dl>
<dt>nruns</dt><dd><p>the number of projections (default: 20).</p>
</dd>
<dt>lowdim</dt><dd><p>target dimension for random projection (default: 5).</p>
</dd>
<dt>maxiter</dt><dd><p>the maximum number of iterations (default: 10).</p>
</dd>
<dt>usediag</dt><dd><p>a logical; covariances are diagonal if <code>TRUE</code>, or full covariances are returned for <code>FALSE</code> (default: <code>FALSE</code>).</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list of S3 class <code>T4cluster</code> containing 
</p>

<dl>
<dt>cluster</dt><dd><p>a length-<code class="reqn">n</code> vector of class labels (from <code class="reqn">1:k</code>).</p>
</dd>
<dt>algorithm</dt><dd><p>name of the algorithm.</p>
</dd>
</dl>



<h3>References</h3>

<p>Fern XZ, Brodley CE (2003).
&ldquo;Random Projection for High Dimensional Data Clustering: A Cluster Ensemble Approach.&rdquo;
In <em>Proceedings of the Twentieth International Conference on International Conference on Machine Learning</em>,  ICML'03, 186–193.
ISBN 1577351894.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# -------------------------------------------------------------
#            clustering with 'iris' dataset
# -------------------------------------------------------------
## PREPARE
data(iris)
X   = as.matrix(iris[,1:4])
lab = as.integer(as.factor(iris[,5]))

## EMBEDDING WITH PCA
X2d = Rdimtools::do.pca(X, ndim=2)$Y  

## CLUSTERING WITH DIFFERENT K VALUES
cl2 = gmm03F(X, k=2)$cluster
cl3 = gmm03F(X, k=3)$cluster
cl4 = gmm03F(X, k=4)$cluster

## VISUALIZATION
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(2,2), pty="s")
plot(X2d, col=lab, pch=19, main="true label")
plot(X2d, col=cl2, pch=19, main="gmm03F: k=2")
plot(X2d, col=cl3, pch=19, main="gmm03F: k=3")
plot(X2d, col=cl4, pch=19, main="gmm03F: k=4")
par(opar)


</code></pre>

<hr>
<h2 id='gmm11R'>Regularized GMM by Ruan et al. (2011)</h2><span id='topic+gmm11R'></span>

<h3>Description</h3>

<p>Ruan et al. (2011) proposed a regularized covariance estimation by 
graphical lasso to cope with high-dimensional scenario where conventional 
GMM might incur singular covariance components. Authors proposed to use 
<code class="reqn">\lambda</code> as a regularization parameter as normally used in 
sparse covariance/precision estimation problems and suggested to use the 
model with the smallest BIC values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gmm11R(data, k = 2, lambda = 1, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gmm11R_+3A_data">data</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix of row-stacked observations.</p>
</td></tr>
<tr><td><code id="gmm11R_+3A_k">k</code></td>
<td>
<p>the number of clusters (default: 2).</p>
</td></tr>
<tr><td><code id="gmm11R_+3A_lambda">lambda</code></td>
<td>
<p>regularization parameter for graphical lasso (default: 1).</p>
</td></tr>
<tr><td><code id="gmm11R_+3A_...">...</code></td>
<td>
<p>extra parameters including </p>

<dl>
<dt>maxiter</dt><dd><p>the maximum number of iterations (default: 10).</p>
</dd>
<dt>nstart</dt><dd><p>the number of random initializations (default: 5).</p>
</dd>
<dt>usediag</dt><dd><p>a logical; covariances are diagonal if <code>TRUE</code>, or full covariances are returned for <code>FALSE</code> (default: <code>FALSE</code>).</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list of S3 class <code>T4cluster</code> containing 
</p>

<dl>
<dt>cluster</dt><dd><p>a length-<code class="reqn">n</code> vector of class labels (from <code class="reqn">1:k</code>).</p>
</dd>
<dt>mean</dt><dd><p>a <code class="reqn">(k\times p)</code> matrix where each row is a class mean.</p>
</dd>
<dt>variance</dt><dd><p>a <code class="reqn">(p\times p\times k)</code> array where each slice is a class covariance.</p>
</dd>
<dt>weight</dt><dd><p>a length-<code class="reqn">k</code> vector of class weights that sum to 1.</p>
</dd>
<dt>loglkd</dt><dd><p>log-likelihood of the data for the fitted model.</p>
</dd>
<dt>algorithm</dt><dd><p>name of the algorithm.</p>
</dd>
</dl>



<h3>References</h3>

<p>Ruan L, Yuan M, Zou H (2011).
&ldquo;Regularized Parameter Estimation in High-Dimensional Gaussian Mixture Models.&rdquo;
<em>Neural Computation</em>, <b>23</b>(6), 1605&ndash;1622.
ISSN 0899-7667, 1530-888X.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># -------------------------------------------------------------
#            clustering with 'iris' dataset
# -------------------------------------------------------------
## PREPARE
data(iris)
X   = as.matrix(iris[,1:4])
lab = as.integer(as.factor(iris[,5]))

## EMBEDDING WITH PCA
X2d = Rdimtools::do.pca(X, ndim=2)$Y  

## COMPARE WITH STANDARD GMM
cl.gmm = gmm(X, k=3)$cluster
cl.11Rf = gmm11R(X, k=3)$cluster
cl.11Rd = gmm11R(X, k=3, usediag=TRUE)$cluster

## VISUALIZATION
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3), pty="s")
plot(X2d, col=cl.gmm,  pch=19, main="standard GMM")
plot(X2d, col=cl.11Rf, pch=19, main="gmm11R: full covs")
plot(X2d, col=cl.11Rd, pch=19, main="gmm11R: diagonal covs")
par(opar)

</code></pre>

<hr>
<h2 id='gmm16G'>Weighted GMM by Gebru et al. (2016)</h2><span id='topic+gmm16G'></span>

<h3>Description</h3>

<p>When each observation <code class="reqn">x_i</code> is associated with a weight <code class="reqn">w_i &gt; 0</code>, 
modifying the GMM formulation is required. Gebru et al. (2016) proposed a method 
to use scaled covariance based on an observation that
</p>
<p style="text-align: center;"><code class="reqn">\mathcal{N}\left(x\vert \mu, \Sigma\right)^w \propto \mathcal{N}\left(x\vert \mu, \frac{\Sigma}{w}\right)</code>
</p>
 
<p>by considering the positive weight as a role of precision. Currently, 
we provide a method with fixed weight case only while the paper also considers 
a Bayesian formalism on the weight using Gamma distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gmm16G(data, k = 2, weight = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gmm16G_+3A_data">data</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix of row-stacked observations.</p>
</td></tr>
<tr><td><code id="gmm16G_+3A_k">k</code></td>
<td>
<p>the number of clusters (default: 2).</p>
</td></tr>
<tr><td><code id="gmm16G_+3A_weight">weight</code></td>
<td>
<p>a positive weight vector of length <code class="reqn">n</code>. If <code>NULL</code> (default), uniform weight is set.</p>
</td></tr>
<tr><td><code id="gmm16G_+3A_...">...</code></td>
<td>
<p>extra parameters including </p>

<dl>
<dt>maxiter</dt><dd><p>the maximum number of iterations (default: 10).</p>
</dd>
<dt>usediag</dt><dd><p>a logical; covariances are diagonal if <code>TRUE</code>, or full covariances are returned for <code>FALSE</code> (default: <code>FALSE</code>).</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list of S3 class <code>T4cluster</code> containing 
</p>

<dl>
<dt>cluster</dt><dd><p>a length-<code class="reqn">n</code> vector of class labels (from <code class="reqn">1:k</code>).</p>
</dd>
<dt>mean</dt><dd><p>a <code class="reqn">(k\times p)</code> matrix where each row is a class mean.</p>
</dd>
<dt>variance</dt><dd><p>a <code class="reqn">(p\times p\times k)</code> array where each slice is a class covariance.</p>
</dd>
<dt>weight</dt><dd><p>a length-<code class="reqn">k</code> vector of class weights that sum to 1.</p>
</dd>
<dt>loglkd</dt><dd><p>log-likelihood of the data for the fitted model.</p>
</dd>
<dt>algorithm</dt><dd><p>name of the algorithm.</p>
</dd>
</dl>



<h3>References</h3>

<p>Gebru ID, Alameda-Pineda X, Forbes F, Horaud R (2016).
&ldquo;EM Algorithms for Weighted-Data Clustering with Application to Audio-Visual Scene Analysis.&rdquo;
<em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, <b>38</b>(12), 2402&ndash;2415.
ISSN 0162-8828, 2160-9292.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># -------------------------------------------------------------
#            clustering with 'iris' dataset
# -------------------------------------------------------------
## PREPARE
data(iris)
X   = as.matrix(iris[,1:4])
lab = as.integer(as.factor(iris[,5]))

## EMBEDDING WITH PCA
X2d = Rdimtools::do.pca(X, ndim=2)$Y  

## CLUSTERING WITH DIFFERENT K VALUES
cl2 = gmm16G(X, k=2)$cluster
cl3 = gmm16G(X, k=3)$cluster
cl4 = gmm16G(X, k=4)$cluster

## VISUALIZATION
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,4), pty="s")
plot(X2d, col=lab, pch=19, main="true label")
plot(X2d, col=cl2, pch=19, main="gmm16G: k=2")
plot(X2d, col=cl3, pch=19, main="gmm16G: k=3")
plot(X2d, col=cl4, pch=19, main="gmm16G: k=4")
par(opar)

</code></pre>

<hr>
<h2 id='gskmeans'>Geodesic Spherical K-Means</h2><span id='topic+gskmeans'></span>

<h3>Description</h3>

<p>Geodesic spherical <code class="reqn">k</code>-means algorithm is an counterpart of the spherical <code class="reqn">k</code>-means 
algorithm by replacing the cosine similarity with the squared geodesic distance, 
which is the great-circle distance under the intrinsic geometry regime 
on the unit hypersphere.  If the data is not 
normalized, it performs the normalization and proceeds thereafter.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gskmeans(data, k = 2, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gskmeans_+3A_data">data</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix of row-stacked observations. If not row-stochastic, each row is normalized to be unit norm.</p>
</td></tr>
<tr><td><code id="gskmeans_+3A_k">k</code></td>
<td>
<p>the number of clusters (default: 2).</p>
</td></tr>
<tr><td><code id="gskmeans_+3A_...">...</code></td>
<td>
<p>extra parameters including </p>

<dl>
<dt>init</dt><dd><p>initialization method; either <code>"kmeans"</code> or <code>"gmm"</code> (default: <code>"kmeans"</code>).</p>
</dd>
<dt>maxiter</dt><dd><p>the maximum number of iterations (default: 10).</p>
</dd>
<dt>abstol</dt><dd><p>stopping criterion to stop the algorithm (default: <code class="reqn">10^{-8}</code>).</p>
</dd>
<dt>verbose</dt><dd><p>a logical; <code>TRUE</code> to show iteration history or <code>FALSE</code> to quiet.</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list of S3 class <code>T4cluster</code> containing 
</p>

<dl>
<dt>cluster</dt><dd><p>a length-<code class="reqn">n</code> vector of class labels (from <code class="reqn">1:k</code>).</p>
</dd> 
<dt>cost</dt><dd><p>a value of the cost function.</p>
</dd>
<dt>means</dt><dd><p>an <code class="reqn">(k\times p)</code> matrix where each row is a unit-norm class mean. </p>
</dd>
<dt>algorithm</dt><dd><p>name of the algorithm.</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>
# -------------------------------------------------------------
#            clustering with 'household' dataset
# -------------------------------------------------------------
## PREPARE
data(household, package="T4cluster")
X   = household$data
lab = as.integer(household$gender)

## EXECUTE GSKMEANS WITH VARYING K's
vec.rand = rep(0, 9)
for (i in 1:9){
  clust_i = gskmeans(X, k=(i+1))$cluster
  vec.rand[i] = compare.rand(clust_i, lab)
}

## VISUALIZE THE RAND INDEX
opar &lt;- par(no.readonly=TRUE)
plot(2:10, vec.rand, type="b", pch=19, ylim=c(0.5, 1),
     ylab="Rand index",xlab="number of clusters",
     main="clustering quality index over varying k's.")
par(opar)


</code></pre>

<hr>
<h2 id='household'>Load 'household' data</h2><span id='topic+household'></span>

<h3>Description</h3>

<p>The data is taken from <span class="pkg">HSAUR3</span> package's <code>household</code> data. We use 
housing, service, and food variables and normalize them to be unit-norm so 
that each observation is projected onto the 2-dimensional sphere. The data 
consists of 20 males and 20 females and has been used for clustering 
on the unit hypersphere.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(household)
</code></pre>


<h3>Format</h3>

<p>a named list containing</p>

<dl>
<dt>data</dt><dd><p>an <code class="reqn">(n\times 3)</code> data matrix whose rows are unit-norm.</p>
</dd>
<dt>gender</dt><dd><p>a length-<code class="reqn">n</code> factor for class label.</p>
</dd>
</dl>



<h3>See Also</h3>

<p><code><a href="HSAUR3.html#topic+household">household</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Load the data
data(household, package="T4cluster")

## Visualize the data in pairs
opar &lt;- par(no.readonly=TRUE)
scatterplot3d::scatterplot3d(household$data, color=rep(c("red","blue"), each=20), 
              pch=19, main="household expenditure on the 2-dimensional sphere",
              xlim=c(0,1.2), ylim=c(0,1.2), zlim=c(0,1.2), angle=45)
par(opar)


</code></pre>

<hr>
<h2 id='kmeans'>K-Means Clustering</h2><span id='topic+kmeans'></span>

<h3>Description</h3>

<p><code class="reqn">K</code>-means algorithm we provide is a wrapper to the <span class="pkg">Armadillo</span>'s k-means routine.
Two types of initialization schemes are employed. Please see the parameters section for more details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kmeans(data, k = 2, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kmeans_+3A_data">data</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix of row-stacked observations.</p>
</td></tr>
<tr><td><code id="kmeans_+3A_k">k</code></td>
<td>
<p>the number of clusters (default: 2).</p>
</td></tr>
<tr><td><code id="kmeans_+3A_...">...</code></td>
<td>
<p>extra parameters including </p>

<dl>
<dt>init</dt><dd><p>initialization method; either <code>"random"</code> for random initialization, or <code>"plus"</code> for k-means++ starting.</p>
</dd>
<dt>maxiter</dt><dd><p>the maximum number of iterations (default: 10).</p>
</dd>
<dt>nstart</dt><dd><p>the number of random initializations (default: 5).</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list of S3 class <code>T4cluster</code> containing 
</p>

<dl>
<dt>cluster</dt><dd><p>a length-<code class="reqn">n</code> vector of class labels (from <code class="reqn">1:k</code>).</p>
</dd> 
<dt>mean</dt><dd><p>a <code class="reqn">(k\times p)</code> matrix where each row is a class mean.</p>
</dd>
<dt>wcss</dt><dd><p>within-cluster sum of squares (WCSS).</p>
</dd>
<dt>algorithm</dt><dd><p>name of the algorithm.</p>
</dd>
</dl>



<h3>References</h3>

<p>Sanderson C, Curtin R (2016).
&ldquo;Armadillo: A Template-Based C++ Library for Linear Algebra.&rdquo;
<em>The Journal of Open Source Software</em>, <b>1</b>(2), 26.
ISSN 2475-9066.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># -------------------------------------------------------------
#            clustering with 'iris' dataset
# -------------------------------------------------------------
## PREPARE
data(iris)
X   = as.matrix(iris[,1:4])
lab = as.integer(as.factor(iris[,5]))

## EMBEDDING WITH PCA
X2d = Rdimtools::do.pca(X, ndim=2)$Y

## CLUSTERING WITH DIFFERENT K VALUES
cl2 = kmeans(X, k=2)$cluster
cl3 = kmeans(X, k=3)$cluster
cl4 = kmeans(X, k=4)$cluster

## VISUALIZATION
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,4), pty="s")
plot(X2d, col=lab, pch=19, main="true label")
plot(X2d, col=cl2, pch=19, main="k-means: k=2")
plot(X2d, col=cl3, pch=19, main="k-means: k=3")
plot(X2d, col=cl4, pch=19, main="k-means: k=4")
par(opar)

</code></pre>

<hr>
<h2 id='kmeans18B'>K-Means Clustering with Lightweight Coreset</h2><span id='topic+kmeans18B'></span>

<h3>Description</h3>

<p>Apply <code class="reqn">k</code>-means clustering algorithm on top of the lightweight coreset 
as proposed in the paper. 
The smaller the set is, the faster the execution becomes with potentially larger quantization errors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kmeans18B(data, k = 2, m = round(nrow(data)/2), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kmeans18B_+3A_data">data</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix of row-stacked observations.</p>
</td></tr>
<tr><td><code id="kmeans18B_+3A_k">k</code></td>
<td>
<p>the number of clusters (default: 2).</p>
</td></tr>
<tr><td><code id="kmeans18B_+3A_m">m</code></td>
<td>
<p>the size of coreset (default: <code class="reqn">n/2</code>).</p>
</td></tr>
<tr><td><code id="kmeans18B_+3A_...">...</code></td>
<td>
<p>extra parameters including </p>

<dl>
<dt>maxiter</dt><dd><p>the maximum number of iterations (default: 10).</p>
</dd>
<dt>nstart</dt><dd><p>the number of random initializations (default: 5).</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list of S3 class <code>T4cluster</code> containing 
</p>

<dl>
<dt>cluster</dt><dd><p>a length-<code class="reqn">n</code> vector of class labels (from <code class="reqn">1:k</code>).</p>
</dd> 
<dt>mean</dt><dd><p>a <code class="reqn">(k\times p)</code> matrix where each row is a class mean.</p>
</dd>
<dt>wcss</dt><dd><p>within-cluster sum of squares (WCSS).</p>
</dd>
<dt>algorithm</dt><dd><p>name of the algorithm.</p>
</dd>
</dl>



<h3>References</h3>

<p>Bachem O, Lucic M, Krause A (2018).
&ldquo;Scalable k -Means Clustering via Lightweight Coresets.&rdquo;
In <em>Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining</em>, 1119&ndash;1127.
ISBN 978-1-4503-5552-0.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># -------------------------------------------------------------
#            clustering with 'iris' dataset
# -------------------------------------------------------------
## PREPARE
data(iris)
X   = as.matrix(iris[,1:4])
lab = as.integer(as.factor(iris[,5]))

## EMBEDDING WITH PCA
X2d = Rdimtools::do.pca(X, ndim=2)$Y

## CLUSTERING WITH DIFFERENT CORESET SIZES WITH K=3
core1 = kmeans18B(X, k=3, m=25)$cluster
core2 = kmeans18B(X, k=3, m=50)$cluster
core3 = kmeans18B(X, k=3, m=100)$cluster

## VISUALIZATION
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,4), pty="s")
plot(X2d, col=lab, pch=19, main="true label")
plot(X2d, col=core1, pch=19, main="kmeans18B: m=25")
plot(X2d, col=core2, pch=19, main="kmeans18B: m=50")
plot(X2d, col=core3, pch=19, main="kmeans18B: m=100")
par(opar)

</code></pre>

<hr>
<h2 id='kmeanspp'>K-Means++ Clustering</h2><span id='topic+kmeanspp'></span>

<h3>Description</h3>

<p><code class="reqn">K</code>-means++ algorithm is usually used as a fast initialization scheme, though 
it can still be used as a standalone clustering algorithms by first choosing the 
centroids and assign points to the nearest centroids.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kmeanspp(data, k = 2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kmeanspp_+3A_data">data</code></td>
<td>
<p>an <code class="reqn">(n \times p)</code> matrix of row-stacked observations.</p>
</td></tr>
<tr><td><code id="kmeanspp_+3A_k">k</code></td>
<td>
<p>the number of clusters (default: 2).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list of S3 class <code>T4cluster</code> containing 
</p>

<dl>
<dt>cluster</dt><dd><p>a length-<code class="reqn">n</code> vector of class labels (from <code class="reqn">1:k</code>).</p>
</dd>
<dt>algorithm</dt><dd><p>name of the algorithm.</p>
</dd>
</dl>



<h3>References</h3>

<p>Arthur D, Vassilvitskii S (2007).
&ldquo;K-Means++: The Advantages of Careful Seeding.&rdquo;
In <em>Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms</em>,  SODA '07, 1027&ndash;1035.
ISBN 978-0-89871-624-5.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># -------------------------------------------------------------
#            clustering with 'iris' dataset
# -------------------------------------------------------------
## PREPARE
data(iris)
X   = as.matrix(iris[,1:4])
lab = as.integer(as.factor(iris[,5]))

## EMBEDDING WITH PCA
X2d = Rdimtools::do.pca(X, ndim=2)$Y

## CLUSTERING WITH DIFFERENT K VALUES
cl2 = kmeanspp(X, k=2)$cluster
cl3 = kmeanspp(X, k=3)$cluster
cl4 = kmeanspp(X, k=4)$cluster

## VISUALIZATION
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,4), pty="s")
plot(X2d, col=lab, pch=19, main="true label")
plot(X2d, col=cl2, pch=19, main="k-means++: k=2")
plot(X2d, col=cl3, pch=19, main="k-means++: k=3")
plot(X2d, col=cl4, pch=19, main="k-means++: k=4")
par(opar)

</code></pre>

<hr>
<h2 id='LRR'>Low-Rank Representation</h2><span id='topic+LRR'></span>

<h3>Description</h3>

<p>Low-Rank Representation (LRR) constructs the connectivity of the data by 
solving 
</p>
<p style="text-align: center;"><code class="reqn">\textrm{min}_C \|C\|_*\quad\textrm{such that}\quad D=DC</code>
</p>

<p>for column-stacked data matrix <code class="reqn">D</code> and <code class="reqn">\|\cdot \|_*</code> is the 
nuclear norm which is relaxation of the rank condition. If you are interested in 
full implementation of the algorithm with sparse outliers and noise, please 
contact the maintainer.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>LRR(data, k = 2, rank = 2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="LRR_+3A_data">data</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix of row-stacked observations.</p>
</td></tr>
<tr><td><code id="LRR_+3A_k">k</code></td>
<td>
<p>the number of clusters (default: 2).</p>
</td></tr>
<tr><td><code id="LRR_+3A_rank">rank</code></td>
<td>
<p>sum of dimensions for all <code class="reqn">k</code> subspaces (default: 2).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list of S3 class <code>T4cluster</code> containing 
</p>

<dl>
<dt>cluster</dt><dd><p>a length-<code class="reqn">n</code> vector of class labels (from <code class="reqn">1:k</code>).</p>
</dd> 
<dt>algorithm</dt><dd><p>name of the algorithm.</p>
</dd>
</dl>



<h3>References</h3>

<p>Liu G, Lin Z, Yu Y (2010).
&ldquo;Robust Subspace Segmentation by Low-Rank Representation.&rdquo;
In <em>Proceedings of the 27th International Conference on International Conference on Machine Learning</em>,  ICML'10, 663&ndash;670.
ISBN 978-1-60558-907-7.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## generate a toy example
set.seed(10)
tester = genLP(n=100, nl=2, np=1, iso.var=0.1)
data   = tester$data
label  = tester$class

## do PCA for data reduction
proj = base::eigen(stats::cov(data))$vectors[,1:2]
dat2 = data%*%proj

## run LRR algorithm with k=2, 3, and 4 with rank=4
output2 = LRR(data, k=2, rank=4)
output3 = LRR(data, k=3, rank=4)
output4 = LRR(data, k=4, rank=4)

## extract label information
lab2 = output2$cluster
lab3 = output3$cluster
lab4 = output4$cluster

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(dat2, pch=19, cex=0.9, col=lab2, main="LRR:K=2")
plot(dat2, pch=19, cex=0.9, col=lab3, main="LRR:K=3")
plot(dat2, pch=19, cex=0.9, col=lab4, main="LRR:K=4")
par(opar)


</code></pre>

<hr>
<h2 id='LRSC'>Low-Rank Subspace Clustering</h2><span id='topic+LRSC'></span>

<h3>Description</h3>

<p>Low-Rank Subspace Clustering (LRSC) constructs the connectivity of the data by solving 
</p>
<p style="text-align: center;"><code class="reqn">\textrm{min}_C \|C\|_*\quad\textrm{such that}\quad A=AC,~C=C^\top</code>
</p>
 
<p>for the uncorrupted data scenario where <code class="reqn">A</code> is a column-stacked 
data matrix. In the current implementation, the first equality constraint 
for reconstructiveness of the data can be relaxed by solving
</p>
<p style="text-align: center;"><code class="reqn">\textrm{min}_C \|C\|_* + \frac{\tau}{2} \|A-AC\|_F^2  \quad\textrm{such that}\quad C=C^\top</code>
</p>
 
<p>controlled by the regularization parameter <code class="reqn">\tau</code>. If you are interested in 
enabling a more general class of the problem suggested by authors, 
please contact maintainer of the package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>LRSC(data, k = 2, type = c("relaxed", "exact"), tau = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="LRSC_+3A_data">data</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix of row-stacked observations.</p>
</td></tr>
<tr><td><code id="LRSC_+3A_k">k</code></td>
<td>
<p>the number of clusters (default: 2).</p>
</td></tr>
<tr><td><code id="LRSC_+3A_type">type</code></td>
<td>
<p>type of the problem to be solved.</p>
</td></tr>
<tr><td><code id="LRSC_+3A_tau">tau</code></td>
<td>
<p>regularization parameter for relaxed-constraint problem.</p>
</td></tr>
</table>


<h3>Details</h3>

<p style="text-align: center;"><code class="reqn">\textrm{min}_C \|C\|_*\quad\textrm{such that}\quad D=DC</code>
</p>

<p>for column-stacked data matrix <code class="reqn">D</code> and <code class="reqn">\|\cdot \|_*</code> is the 
nuclear norm which is relaxation of the rank condition. If you are interested in 
full implementation of the algorithm with sparse outliers and noise, please 
contact the maintainer.
</p>


<h3>Value</h3>

<p>a named list of S3 class <code>T4cluster</code> containing 
</p>

<dl>
<dt>cluster</dt><dd><p>a length-<code class="reqn">n</code> vector of class labels (from <code class="reqn">1:k</code>).</p>
</dd> 
<dt>algorithm</dt><dd><p>name of the algorithm.</p>
</dd>
</dl>



<h3>References</h3>

<p>Vidal R, Favaro P (2014).
&ldquo;Low Rank Subspace Clustering (LRSC).&rdquo;
<em>Pattern Recognition Letters</em>, <b>43</b>, 47&ndash;61.
ISSN 01678655.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## generate a toy example
set.seed(10)
tester = genLP(n=100, nl=2, np=1, iso.var=0.1)
data   = tester$data
label  = tester$class

## do PCA for data reduction
proj = base::eigen(stats::cov(data))$vectors[,1:2]
dat2 = data%*%proj

## run LRSC algorithm with k=2,3,4 with relaxed/exact solvers
out2rel = LRSC(data, k=2, type="relaxed")
out3rel = LRSC(data, k=3, type="relaxed")
out4rel = LRSC(data, k=4, type="relaxed")

out2exc = LRSC(data, k=2, type="exact")
out3exc = LRSC(data, k=3, type="exact")
out4exc = LRSC(data, k=4, type="exact")

## extract label information
lab2rel = out2rel$cluster
lab3rel = out3rel$cluster
lab4rel = out4rel$cluster

lab2exc = out2exc$cluster
lab3exc = out3exc$cluster
lab4exc = out4exc$cluster

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(2,3))
plot(dat2, pch=19, cex=0.9, col=lab2rel, main="LRSC Relaxed:K=2")
plot(dat2, pch=19, cex=0.9, col=lab3rel, main="LRSC Relaxed:K=3")
plot(dat2, pch=19, cex=0.9, col=lab4rel, main="LRSC Relaxed:K=4")
plot(dat2, pch=19, cex=0.9, col=lab2exc, main="LRSC Exact:K=2")
plot(dat2, pch=19, cex=0.9, col=lab3exc, main="LRSC Exact:K=3")
plot(dat2, pch=19, cex=0.9, col=lab4exc, main="LRSC Exact:K=4")
par(opar)


</code></pre>

<hr>
<h2 id='LSR'>Least Squares Regression</h2><span id='topic+LSR'></span>

<h3>Description</h3>

<p>For the subspace clustering, traditional method of least squares regression 
is used to build coefficient matrix that reconstructs the data point by solving 
</p>
<p style="text-align: center;"><code class="reqn">\textrm{min}_Z \|X-XZ\|_F^2 + \lambda \|Z\|_F \textrm{ such that }diag(Z)=0</code>
</p>

<p>where <code class="reqn">X\in\mathbf{R}^{p\times n}</code> is a column-stacked data matrix. 
As seen from the equation, we use a denoising version controlled by <code class="reqn">\lambda</code> and 
provide an option to abide by the constraint <code class="reqn">diag(Z)=0</code> by <code>zerodiag</code> parameter.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>LSR(data, k = 2, lambda = 1e-05, zerodiag = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="LSR_+3A_data">data</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix of row-stacked observations.</p>
</td></tr>
<tr><td><code id="LSR_+3A_k">k</code></td>
<td>
<p>the number of clusters (default: 2).</p>
</td></tr>
<tr><td><code id="LSR_+3A_lambda">lambda</code></td>
<td>
<p>regularization parameter (default: 1e-5).</p>
</td></tr>
<tr><td><code id="LSR_+3A_zerodiag">zerodiag</code></td>
<td>
<p>a logical; <code>TRUE</code> (default) to use the problem formulation with zero 
diagonal entries or <code>FALSE</code> otherwise.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list of S3 class <code>T4cluster</code> containing 
</p>

<dl>
<dt>cluster</dt><dd><p>a length-<code class="reqn">n</code> vector of class labels (from <code class="reqn">1:k</code>).</p>
</dd> 
<dt>algorithm</dt><dd><p>name of the algorithm.</p>
</dd>
</dl>



<h3>References</h3>

<p>Lu C, Min H, Zhao Z, Zhu L, Huang D, Yan S (2012).
&ldquo;Robust and Efficient Subspace Segmentation via Least Squares Regression.&rdquo;
In Hutchison D, Kanade T, Kittler J, Kleinberg JM, Mattern F, Mitchell JC, Naor M, Nierstrasz O, Pandu Rangan C, Steffen B, Sudan M, Terzopoulos D, Tygar D, Vardi MY, Weikum G, Fitzgibbon A, Lazebnik S, Perona P, Sato Y, Schmid C (eds.), <em>Computer Vision -ECCV 2012</em>, volume 7578, 347&ndash;360.
Springer Berlin Heidelberg, Berlin, Heidelberg.
ISBN 978-3-642-33785-7 978-3-642-33786-4.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## generate a toy example
set.seed(10)
tester = genLP(n=100, nl=2, np=1, iso.var=0.1)
data   = tester$data
label  = tester$class

## do PCA for data reduction
proj = base::eigen(stats::cov(data))$vectors[,1:2]
dat2 = data%*%proj

## run LSR for k=3 with different lambda values
out1 = LSR(data, k=3, lambda=1e-2)
out2 = LSR(data, k=3, lambda=1)
out3 = LSR(data, k=3, lambda=1e+2)

## extract label information
lab1 = out1$cluster
lab2 = out2$cluster
lab3 = out3$cluster

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(dat2, pch=19, cex=0.9, col=lab1, main="LSR:lambda=1e-2")
plot(dat2, pch=19, cex=0.9, col=lab2, main="LSR:lambda=1")
plot(dat2, pch=19, cex=0.9, col=lab3, main="LSR:lambda=1e+2")
par(opar)


</code></pre>

<hr>
<h2 id='MSM'>Bayesian Mixture of Subspaces of Different Dimensions</h2><span id='topic+MSM'></span>

<h3>Description</h3>

<p><code>MSM</code> is a Bayesian model inferring mixtures of subspaces that are of possibly different dimensions. 
For simplicity, this function returns only a handful of information that are most important in 
representing the mixture model, including projection, location, and hard assignment parameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MSM(data, k = 2, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MSM_+3A_data">data</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix of row-stacked observations.</p>
</td></tr>
<tr><td><code id="MSM_+3A_k">k</code></td>
<td>
<p>the number of mixtures.</p>
</td></tr>
<tr><td><code id="MSM_+3A_...">...</code></td>
<td>
<p>extra parameters including </p>

<dl>
<dt>temperature</dt><dd><p>temperature value for Gibbs posterior (default: 1e-6).</p>
</dd>
<dt>prop.var</dt><dd><p>proposal variance parameter (default: 1.0).</p>
</dd>
<dt>iter</dt><dd><p>the number of MCMC runs (default: 496).</p>
</dd>
<dt>burn.in</dt><dd><p>burn-in for MCMC runs (default: iter/2).</p>
</dd>
<dt>thin</dt><dd><p>interval for recording MCMC runs (default: 10).</p>
</dd>
<dt>print.progress</dt><dd><p>a logical; <code>TRUE</code> to show completion of iterations by 10, <code>FALSE</code> otherwise (default: <code>FALSE</code>).</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Value</h3>

<p>a list whose elements are S3 class <code>"MSM"</code> instances, which are also lists of following elements: </p>

<dl>
<dt>P</dt><dd><p>length-<code>k</code> list of projection matrices.</p>
</dd>
<dt>U</dt><dd><p>length-<code>k</code> list of orthonormal basis.</p>
</dd>
<dt>theta</dt><dd><p>length-<code>k</code> list of center locations of each mixture.</p>
</dd>
<dt>cluster</dt><dd><p>length-<code>n</code> vector of cluster label.</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>
## generate a toy example
set.seed(10)
tester = genLP(n=100, nl=2, np=1, iso.var=0.1)
data   = tester$data
label  = tester$class

## do PCA for data reduction
proj = base::eigen(stats::cov(data))$vectors[,1:2]
dat2 = data%*%proj

## run MSM algorithm with k=2, 3, and 4
maxiter = 500
output2 = MSM(data, k=2, iter=maxiter)
output3 = MSM(data, k=3, iter=maxiter)
output4 = MSM(data, k=4, iter=maxiter)

## extract final clustering information
nrec  = length(output2)
finc2 = output2[[nrec]]$cluster
finc3 = output3[[nrec]]$cluster
finc4 = output4[[nrec]]$cluster

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(3,4))
plot(dat2[,1],dat2[,2],pch=19,cex=0.3,col=finc2+1,main="K=2:PCA")
plot(data[,1],data[,2],pch=19,cex=0.3,col=finc2+1,main="K=2:Axis(1,2)")
plot(data[,1],data[,3],pch=19,cex=0.3,col=finc2+1,main="K=2:Axis(1,3)")
plot(data[,2],data[,3],pch=19,cex=0.3,col=finc2+1,main="K=2:Axis(2,3)")

plot(dat2[,1],dat2[,2],pch=19,cex=0.3,col=finc3+1,main="K=3:PCA")
plot(data[,1],data[,2],pch=19,cex=0.3,col=finc3+1,main="K=3:Axis(1,2)")
plot(data[,1],data[,3],pch=19,cex=0.3,col=finc3+1,main="K=3:Axis(1,3)")
plot(data[,2],data[,3],pch=19,cex=0.3,col=finc3+1,main="K=3:Axis(2,3)")

plot(dat2[,1],dat2[,2],pch=19,cex=0.3,col=finc4+1,main="K=4:PCA")
plot(data[,1],data[,2],pch=19,cex=0.3,col=finc4+1,main="K=4:Axis(1,2)")
plot(data[,1],data[,3],pch=19,cex=0.3,col=finc4+1,main="K=4:Axis(1,3)")
plot(data[,2],data[,3],pch=19,cex=0.3,col=finc4+1,main="K=4:Axis(2,3)")
par(opar)


</code></pre>

<hr>
<h2 id='pcm'>Compute Pairwise Co-occurrence Matrix</h2><span id='topic+pcm'></span>

<h3>Description</h3>

<p>Let <em>clustering</em> be a label from data of <code class="reqn">N</code> observations and suppose 
we are given <code class="reqn">M</code> such labels. Co-occurrent matrix counts the number of events 
where two observations <code class="reqn">X_i</code> and <code class="reqn">X_j</code> belong to the same category/class. 
<em>PCM</em> serves as a measure of uncertainty embedded in any algorithms with non-deterministic components.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pcm(partitions)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pcm_+3A_partitions">partitions</code></td>
<td>
<p>partitions can be provided in either (1) an <code class="reqn">(M\times N)</code> matrix 
where each row is a clustering for <code class="reqn">N</code> objects, or (2) a length-<code class="reqn">M</code> list of 
length-<code class="reqn">N</code> clustering labels.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an <code class="reqn">(N\times N)</code> matrix, whose elements <code class="reqn">(i,j)</code> are counts for 
how many times observations <code class="reqn">i</code> and <code class="reqn">j</code> belong to the same cluster, ranging from <code class="reqn">0</code> to <code class="reqn">M</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+psm">psm</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># -------------------------------------------------------------
#               PSM with 'iris' dataset + k-means++
# -------------------------------------------------------------
## PREPARE WITH SUBSET OF DATA
data(iris)
X     = as.matrix(iris[,1:4])
lab   = as.integer(as.factor(iris[,5]))

## EMBEDDING WITH PCA
X2d = Rdimtools::do.pca(X, ndim=2)$Y

## RUN K-MEANS++ 100 TIMES
partitions = list()
for (i in 1:100){
  partitions[[i]] = kmeanspp(X)$cluster
}

## COMPUTE PCM
iris.pcm = pcm(partitions)

## VISUALIZATION
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,2), pty="s")
plot(X2d, col=lab, pch=19, main="true label")
image(iris.pcm[,150:1], axes=FALSE, main="PCM")
par(opar)

</code></pre>

<hr>
<h2 id='predict.MSM'>S3 method to predict class label of new data with 'MSM' object</h2><span id='topic+predict.MSM'></span>

<h3>Description</h3>

<p>Given an instance of <code>MSM</code> class from <code><a href="#topic+MSM">MSM</a></code> function, predict 
class label of a new data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'MSM'
predict(object, newdata, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.MSM_+3A_object">object</code></td>
<td>
<p>an <code>'MSM'</code> object from <code><a href="#topic+MSM">MSM</a></code> function.</p>
</td></tr>
<tr><td><code id="predict.MSM_+3A_newdata">newdata</code></td>
<td>
<p>an <code class="reqn">(m\times p)</code> matrix of row-stacked observations.</p>
</td></tr>
<tr><td><code id="predict.MSM_+3A_...">...</code></td>
<td>
<p>extra parameters (not necessary).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a length-<code class="reqn">m</code> vector of class labels.
</p>

<hr>
<h2 id='psm'>Compute Posterior Similarity Matrix</h2><span id='topic+psm'></span>

<h3>Description</h3>

<p>Let <em>clustering</em> be a label from data of <code class="reqn">N</code> observations and suppose 
we are given <code class="reqn">M</code> such labels. Posterior similarity matrix, as its name suggests, 
computes posterior probability for a pair of observations to belong to the same cluster, i.e., 
</p>
<p style="text-align: center;"><code class="reqn">P_{ij} = P(\textrm{label}(X_i) = \textrm{label}(X_j))</code>
</p>

<p>under the scenario where multiple clusterings are samples drawn from a posterior distribution within 
the Bayesian framework. However, it can also be used for non-Bayesian settings as 
<code>psm</code> is a measure of uncertainty embedded in any algorithms with non-deterministic components.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>psm(partitions)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="psm_+3A_partitions">partitions</code></td>
<td>
<p>partitions can be provided in either (1) an <code class="reqn">(M\times N)</code> matrix 
where each row is a clustering for <code class="reqn">N</code> objects, or (2) a length-<code class="reqn">M</code> list of 
length-<code class="reqn">N</code> clustering labels.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an <code class="reqn">(N\times N)</code> matrix, whose elements <code class="reqn">(i,j)</code> are posterior probability 
for an observation <code class="reqn">i</code> and <code class="reqn">j</code> belong to the same cluster.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pcm">pcm</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># -------------------------------------------------------------
#               PSM with 'iris' dataset + k-means++
# -------------------------------------------------------------
## PREPARE WITH SUBSET OF DATA
data(iris)
X     = as.matrix(iris[,1:4])
lab   = as.integer(as.factor(iris[,5]))

## EMBEDDING WITH PCA
X2d = Rdimtools::do.pca(X, ndim=2)$Y

## RUN K-MEANS++ 100 TIMES
partitions = list()
for (i in 1:100){
  partitions[[i]] = kmeanspp(X)$cluster
}

## COMPUTE PSM
iris.psm = psm(partitions)

## VISUALIZATION
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,2), pty="s")
plot(X2d, col=lab, pch=19, main="true label")
image(iris.psm[,150:1], axes=FALSE, main="PSM")
par(opar)

</code></pre>

<hr>
<h2 id='sc05Z'>Spectral Clustering by Zelnik-Manor and Perona (2005)</h2><span id='topic+sc05Z'></span>

<h3>Description</h3>

<p>Zelnik-Manor and Perona proposed a method to define a set of data-driven 
bandwidth parameters where <code class="reqn">\sigma_i</code> is the distance from a point <code class="reqn">x_i</code> to its <code>nnbd</code>-th 
nearest neighbor. Then the affinity matrix is defined as
</p>
<p style="text-align: center;"><code class="reqn">A_{ij} = \exp(-d(x_i, d_j)^2 / \sigma_i \sigma_j)</code>
</p>
<p> and the standard 
spectral clustering of Ng, Jordan, and Weiss (<code><a href="#topic+scNJW">scNJW</a></code>) is applied.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sc05Z(data, k = 2, nnbd = 7, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sc05Z_+3A_data">data</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix of row-stacked observations or S3 <code>dist</code> object of <code class="reqn">n</code> observations.</p>
</td></tr>
<tr><td><code id="sc05Z_+3A_k">k</code></td>
<td>
<p>the number of clusters (default: 2).</p>
</td></tr>
<tr><td><code id="sc05Z_+3A_nnbd">nnbd</code></td>
<td>
<p>neighborhood size to define data-driven bandwidth parameter (default: 7).</p>
</td></tr>
<tr><td><code id="sc05Z_+3A_...">...</code></td>
<td>
<p>extra parameters including </p>

<dl>
<dt>algclust</dt><dd><p>method to perform clustering on embedded data; either <code>"kmeans"</code> (default) or <code>"GMM"</code>.</p>
</dd>
<dt>maxiter</dt><dd><p>the maximum number of iterations (default: 10).</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list of S3 class <code>T4cluster</code> containing 
</p>

<dl>
<dt>cluster</dt><dd><p>a length-<code class="reqn">n</code> vector of class labels (from <code class="reqn">1:k</code>).</p>
</dd> 
<dt>eigval</dt><dd><p>eigenvalues of the graph laplacian's spectral decomposition.</p>
</dd>
<dt>embeds</dt><dd><p>an <code class="reqn">(n\times k)</code> low-dimensional embedding.</p>
</dd>
<dt>algorithm</dt><dd><p>name of the algorithm.</p>
</dd>
</dl>



<h3>References</h3>

<p>Zelnik-manor L, Perona P (2005).
&ldquo;Self-Tuning Spectral Clustering.&rdquo;
In Saul LK, Weiss Y, Bottou L (eds.), <em>Advances in Neural Information Processing Systems 17</em>, 1601&ndash;1608.
MIT Press.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># -------------------------------------------------------------
#            clustering with 'iris' dataset
# -------------------------------------------------------------
## PREPARE
data(iris)
X   = as.matrix(iris[,1:4])
lab = as.integer(as.factor(iris[,5]))

## EMBEDDING WITH PCA
X2d = Rdimtools::do.pca(X, ndim=2)$Y

## CLUSTERING WITH DIFFERENT K VALUES
cl2 = sc05Z(X, k=2)$cluster
cl3 = sc05Z(X, k=3)$cluster
cl4 = sc05Z(X, k=4)$cluster

## VISUALIZATION
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,4), pty="s")
plot(X2d, col=lab, pch=19, main="true label")
plot(X2d, col=cl2, pch=19, main="sc05Z: k=2")
plot(X2d, col=cl3, pch=19, main="sc05Z: k=3")
plot(X2d, col=cl4, pch=19, main="sc05Z: k=4")
par(opar)

</code></pre>

<hr>
<h2 id='sc09G'>Spectral Clustering by Gu and Wang (2009)</h2><span id='topic+sc09G'></span>

<h3>Description</h3>

<p>The algorithm defines a set of data-driven 
bandwidth parameters where <code class="reqn">\sigma_i</code> is the average distance from a point <code class="reqn">x_i</code> to its <code>nnbd</code>-th 
nearest neighbor. Then the affinity matrix is defined as
</p>
<p style="text-align: center;"><code class="reqn">A_{ij} = \exp(-d(x_i, d_j)^2 / \sigma_i \sigma_j)</code>
</p>
<p> and the standard 
spectral clustering of Ng, Jordan, and Weiss (<code><a href="#topic+scNJW">scNJW</a></code>) is applied.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sc09G(data, k = 2, nnbd = 7, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sc09G_+3A_data">data</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix of row-stacked observations or S3 <code>dist</code> object of <code class="reqn">n</code> observations.</p>
</td></tr>
<tr><td><code id="sc09G_+3A_k">k</code></td>
<td>
<p>the number of clusters (default: 2).</p>
</td></tr>
<tr><td><code id="sc09G_+3A_nnbd">nnbd</code></td>
<td>
<p>neighborhood size to define data-driven bandwidth parameter (default: 7).</p>
</td></tr>
<tr><td><code id="sc09G_+3A_...">...</code></td>
<td>
<p>extra parameters including </p>

<dl>
<dt>algclust</dt><dd><p>method to perform clustering on embedded data; either <code>"kmeans"</code> (default) or <code>"GMM"</code>.</p>
</dd>
<dt>maxiter</dt><dd><p>the maximum number of iterations (default: 10).</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list of S3 class <code>T4cluster</code> containing 
</p>

<dl>
<dt>cluster</dt><dd><p>a length-<code class="reqn">n</code> vector of class labels (from <code class="reqn">1:k</code>).</p>
</dd> 
<dt>eigval</dt><dd><p>eigenvalues of the graph laplacian's spectral decomposition.</p>
</dd>
<dt>embeds</dt><dd><p>an <code class="reqn">(n\times k)</code> low-dimensional embedding.</p>
</dd>
<dt>algorithm</dt><dd><p>name of the algorithm.</p>
</dd>
</dl>



<h3>References</h3>

<p>Gu R, Wang J (2009).
&ldquo;An Improved Spectral Clustering Algorithm Based on Neighbour Adaptive Scale.&rdquo;
In <em>2009 International Conference on Business Intelligence and Financial Engineering</em>, 233&ndash;236.
ISBN 978-0-7695-3705-4.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># -------------------------------------------------------------
#            clustering with 'iris' dataset
# -------------------------------------------------------------
## PREPARE
data(iris)
X   = as.matrix(iris[,1:4])
lab = as.integer(as.factor(iris[,5]))

## EMBEDDING WITH PCA
X2d = Rdimtools::do.pca(X, ndim=2)$Y

## CLUSTERING WITH DIFFERENT K VALUES
cl2 = sc09G(X, k=2)$cluster
cl3 = sc09G(X, k=3)$cluster
cl4 = sc09G(X, k=4)$cluster

## VISUALIZATION
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,4), pty="s")
plot(X2d, col=lab, pch=19, main="true label")
plot(X2d, col=cl2, pch=19, main="sc09G: k=2")
plot(X2d, col=cl3, pch=19, main="sc09G: k=3")
plot(X2d, col=cl4, pch=19, main="sc09G: k=4")
par(opar)

</code></pre>

<hr>
<h2 id='sc10Z'>Spectral Clustering by Zhang et al. (2010)</h2><span id='topic+sc10Z'></span>

<h3>Description</h3>

<p>The algorithm defines a set of data-driven 
bandwidth parameters <code class="reqn">p_{ij}</code> by constructing a similarity matrix. 
Then the affinity matrix is defined as </p>
<p style="text-align: center;"><code class="reqn">A_{ij} = \exp(-d(x_i, d_j)^2 / 2 p_{ij})</code>
</p>
 
<p>and the standard spectral clustering of Ng, Jordan, and Weiss (<code><a href="#topic+scNJW">scNJW</a></code>) is applied.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sc10Z(data, k = 2, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sc10Z_+3A_data">data</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix of row-stacked observations or S3 <code>dist</code> object of <code class="reqn">n</code> observations.</p>
</td></tr>
<tr><td><code id="sc10Z_+3A_k">k</code></td>
<td>
<p>the number of clusters (default: 2).</p>
</td></tr>
<tr><td><code id="sc10Z_+3A_...">...</code></td>
<td>
<p>extra parameters including </p>

<dl>
<dt>algclust</dt><dd><p>method to perform clustering on embedded data; either <code>"kmeans"</code> (default) or <code>"GMM"</code>.</p>
</dd>
<dt>maxiter</dt><dd><p>the maximum number of iterations (default: 10).</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list of S3 class <code>T4cluster</code> containing 
</p>

<dl>
<dt>cluster</dt><dd><p>a length-<code class="reqn">n</code> vector of class labels (from <code class="reqn">1:k</code>).</p>
</dd> 
<dt>eigval</dt><dd><p>eigenvalues of the graph laplacian's spectral decomposition.</p>
</dd>
<dt>embeds</dt><dd><p>an <code class="reqn">(n\times k)</code> low-dimensional embedding.</p>
</dd>
<dt>algorithm</dt><dd><p>name of the algorithm.</p>
</dd>
</dl>



<h3>References</h3>

<p>Zhang Y, Zhou J, Fu Y (2010).
&ldquo;Spectral Clustering Algorithm Based on Adaptive Neighbor Distance Sort Order.&rdquo;
In <em>The 3rd International Conference on Information Sciences and Interaction Sciences</em>, 444&ndash;447.
ISBN 978-1-4244-7384-7.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># -------------------------------------------------------------
#            clustering with 'iris' dataset
# -------------------------------------------------------------
## PREPARE
data(iris)
X   = as.matrix(iris[,1:4])
lab = as.integer(as.factor(iris[,5]))

## EMBEDDING WITH PCA
X2d = Rdimtools::do.pca(X, ndim=2)$Y

## CLUSTERING WITH DIFFERENT K VALUES
cl2 = sc10Z(X, k=2)$cluster
cl3 = sc10Z(X, k=3)$cluster
cl4 = sc10Z(X, k=4)$cluster

## VISUALIZATION
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,4), pty="s")
plot(X2d, col=lab, pch=19, main="true label")
plot(X2d, col=cl2, pch=19, main="sc10Z: k=2")
plot(X2d, col=cl3, pch=19, main="sc10Z: k=3")
plot(X2d, col=cl4, pch=19, main="sc10Z: k=4")
par(opar)

</code></pre>

<hr>
<h2 id='sc11Y'>Spectral Clustering by Yang et al. (2011)</h2><span id='topic+sc11Y'></span>

<h3>Description</h3>

<p>As a data-driven method, the algorithm recovers geodesic distance from a k-nearest 
neighbor graph scaled by an (exponential) parameter <code class="reqn">\rho</code> and applies 
random-walk spectral clustering. Authors referred their method as 
density sensitive similarity function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sc11Y(data, k = 2, nnbd = 7, rho = 2, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sc11Y_+3A_data">data</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix of row-stacked observations or S3 <code>dist</code> object of <code class="reqn">n</code> observations.</p>
</td></tr>
<tr><td><code id="sc11Y_+3A_k">k</code></td>
<td>
<p>the number of clusters (default: 2).</p>
</td></tr>
<tr><td><code id="sc11Y_+3A_nnbd">nnbd</code></td>
<td>
<p>neighborhood size to define data-driven bandwidth parameter (default: 7).</p>
</td></tr>
<tr><td><code id="sc11Y_+3A_rho">rho</code></td>
<td>
<p>exponent scaling parameter (default: 2).</p>
</td></tr>
<tr><td><code id="sc11Y_+3A_...">...</code></td>
<td>
<p>extra parameters including </p>

<dl>
<dt>algclust</dt><dd><p>method to perform clustering on embedded data; either <code>"kmeans"</code> (default) or <code>"GMM"</code>.</p>
</dd>
<dt>maxiter</dt><dd><p>the maximum number of iterations (default: 10).</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list of S3 class <code>T4cluster</code> containing 
</p>

<dl>
<dt>cluster</dt><dd><p>a length-<code class="reqn">n</code> vector of class labels (from <code class="reqn">1:k</code>).</p>
</dd> 
<dt>eigval</dt><dd><p>eigenvalues of the graph laplacian's spectral decomposition.</p>
</dd>
<dt>embeds</dt><dd><p>an <code class="reqn">(n\times k)</code> low-dimensional embedding.</p>
</dd>
<dt>algorithm</dt><dd><p>name of the algorithm.</p>
</dd>
</dl>



<h3>References</h3>

<p>Yang P, Zhu Q, Huang B (2011).
&ldquo;Spectral Clustering with Density Sensitive Similarity Function.&rdquo;
<em>Knowledge-Based Systems</em>, <b>24</b>(5), 621&ndash;628.
ISSN 09507051.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># -------------------------------------------------------------
#            clustering with 'iris' dataset
# -------------------------------------------------------------
## PREPARE
data(iris)
X   = as.matrix(iris[,1:4])
lab = as.integer(as.factor(iris[,5]))

## EMBEDDING WITH PCA
X2d = Rdimtools::do.pca(X, ndim=2)$Y

## CLUSTERING WITH DIFFERENT K VALUES
cl2 = sc11Y(X, k=2)$cluster
cl3 = sc11Y(X, k=3)$cluster
cl4 = sc11Y(X, k=4)$cluster

## VISUALIZATION
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,4), pty="s")
plot(X2d, col=lab, pch=19, main="true label")
plot(X2d, col=cl2, pch=19, main="sc11Y: k=2")
plot(X2d, col=cl3, pch=19, main="sc11Y: k=3")
plot(X2d, col=cl4, pch=19, main="sc11Y: k=4")
par(opar)

</code></pre>

<hr>
<h2 id='sc12L'>Spectral Clustering by Li and Guo (2012)</h2><span id='topic+sc12L'></span>

<h3>Description</h3>

<p>Li and Guo proposed to construct an affinity matrix
</p>
<p style="text-align: center;"><code class="reqn">A_{ij} = \exp(-d(x_i, d_j)^2 / 2 \sigma^2)</code>
</p>
<p> and adjust the matrix 
by neighbor propagation. Then, standard spectral clustering from 
the symmetric, normalized graph laplacian is applied.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sc12L(data, k = 2, sigma = 1, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sc12L_+3A_data">data</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix of row-stacked observations or S3 <code>dist</code> object of <code class="reqn">n</code> observations.</p>
</td></tr>
<tr><td><code id="sc12L_+3A_k">k</code></td>
<td>
<p>the number of clusters (default: 2).</p>
</td></tr>
<tr><td><code id="sc12L_+3A_sigma">sigma</code></td>
<td>
<p>common bandwidth parameter (default: 1).</p>
</td></tr>
<tr><td><code id="sc12L_+3A_...">...</code></td>
<td>
<p>extra parameters including </p>

<dl>
<dt>algclust</dt><dd><p>method to perform clustering on embedded data; either <code>"kmeans"</code> (default) or <code>"GMM"</code>.</p>
</dd>
<dt>maxiter</dt><dd><p>the maximum number of iterations (default: 10).</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list of S3 class <code>T4cluster</code> containing 
</p>

<dl>
<dt>cluster</dt><dd><p>a length-<code class="reqn">n</code> vector of class labels (from <code class="reqn">1:k</code>).</p>
</dd> 
<dt>eigval</dt><dd><p>eigenvalues of the graph laplacian's spectral decomposition.</p>
</dd>
<dt>embeds</dt><dd><p>an <code class="reqn">(n\times k)</code> low-dimensional embedding.</p>
</dd>
<dt>algorithm</dt><dd><p>name of the algorithm.</p>
</dd>
</dl>



<h3>References</h3>

<p>Li X, Guo L (2012).
&ldquo;Constructing Affinity Matrix in Spectral Clustering Based on Neighbor Propagation.&rdquo;
<em>Neurocomputing</em>, <b>97</b>, 125&ndash;130.
ISSN 09252312.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+scNJW">scNJW</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># -------------------------------------------------------------
#            clustering with 'iris' dataset
# -------------------------------------------------------------
## PREPARE
data(iris)
X   = as.matrix(iris[,1:4])
lab = as.integer(as.factor(iris[,5]))

## EMBEDDING WITH PCA
X2d = Rdimtools::do.pca(X, ndim=2)$Y

## CLUSTERING WITH DIFFERENT K VALUES
cl2 = sc12L(X, k=2)$cluster
cl3 = sc12L(X, k=3)$cluster
cl4 = sc12L(X, k=4)$cluster

## VISUALIZATION
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,4), pty="s")
plot(X2d, col=lab, pch=19, main="true label")
plot(X2d, col=cl2, pch=19, main="sc12L: k=2")
plot(X2d, col=cl3, pch=19, main="sc12L: k=3")
plot(X2d, col=cl4, pch=19, main="sc12L: k=4")
par(opar)

</code></pre>

<hr>
<h2 id='scNJW'>Spectral Clustering by Ng, Jordan, and Weiss (2002)</h2><span id='topic+scNJW'></span>

<h3>Description</h3>

<p>The version of Ng, Jordan, and Weiss first constructs the affinity matrix
</p>
<p style="text-align: center;"><code class="reqn">A_{ij} = \exp(-d(x_i, d_j)^2 / \sigma^2)</code>
</p>

<p>where <code class="reqn">\sigma</code> is a common bandwidth parameter and performs k-means (or possibly, GMM) clustering on 
the row-space of eigenvectors for the symmetric graph laplacian matrix
</p>
<p style="text-align: center;"><code class="reqn">L=D^{-1/2}(D-A)D^{-1/2}</code>
</p>
<p>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>scNJW(data, k = 2, sigma = 1, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="scNJW_+3A_data">data</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix of row-stacked observations or S3 <code>dist</code> object of <code class="reqn">n</code> observations.</p>
</td></tr>
<tr><td><code id="scNJW_+3A_k">k</code></td>
<td>
<p>the number of clusters (default: 2).</p>
</td></tr>
<tr><td><code id="scNJW_+3A_sigma">sigma</code></td>
<td>
<p>bandwidth parameter (default: 1).</p>
</td></tr>
<tr><td><code id="scNJW_+3A_...">...</code></td>
<td>
<p>extra parameters including </p>

<dl>
<dt>algclust</dt><dd><p>method to perform clustering on embedded data; either <code>"kmeans"</code> (default) or <code>"GMM"</code>.</p>
</dd>
<dt>maxiter</dt><dd><p>the maximum number of iterations (default: 10).</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list of S3 class <code>T4cluster</code> containing 
</p>

<dl>
<dt>cluster</dt><dd><p>a length-<code class="reqn">n</code> vector of class labels (from <code class="reqn">1:k</code>).</p>
</dd> 
<dt>eigval</dt><dd><p>eigenvalues of the graph laplacian's spectral decomposition.</p>
</dd>
<dt>embeds</dt><dd><p>an <code class="reqn">(n\times k)</code> low-dimensional embedding.</p>
</dd>
<dt>algorithm</dt><dd><p>name of the algorithm.</p>
</dd>
</dl>



<h3>References</h3>

<p>Ng AY, Jordan MI, Weiss Y (2002).
&ldquo;On Spectral Clustering: Analysis and an Algorithm.&rdquo;
In Dietterich TG, Becker S, Ghahramani Z (eds.), <em>Advances in Neural Information Processing Systems 14</em>, 849&ndash;856.
MIT Press.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># -------------------------------------------------------------
#            clustering with 'iris' dataset
# -------------------------------------------------------------
## PREPARE
data(iris)
X   = as.matrix(iris[,1:4])
lab = as.integer(as.factor(iris[,5]))

## EMBEDDING WITH PCA
X2d = Rdimtools::do.pca(X, ndim=2)$Y

## CLUSTERING WITH DIFFERENT K VALUES
cl2 = scNJW(X, k=2)$cluster
cl3 = scNJW(X, k=3)$cluster
cl4 = scNJW(X, k=4)$cluster

## VISUALIZATION
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,4), pty="s")
plot(X2d, col=lab, pch=19, main="true label")
plot(X2d, col=cl2, pch=19, main="scNJW: k=2")
plot(X2d, col=cl3, pch=19, main="scNJW: k=3")
plot(X2d, col=cl4, pch=19, main="scNJW: k=4")
par(opar)

</code></pre>

<hr>
<h2 id='scSM'>Spectral Clustering by Shi and Malik (2000)</h2><span id='topic+scSM'></span>

<h3>Description</h3>

<p>The version of Shi and Malik first constructs the affinity matrix
</p>
<p style="text-align: center;"><code class="reqn">A_{ij} = \exp(-d(x_i, d_j)^2 / \sigma^2)</code>
</p>

<p>where <code class="reqn">\sigma</code> is a common bandwidth parameter and performs k-means (or possibly, GMM) clustering on 
the row-space of eigenvectors for the random-walk graph laplacian matrix
</p>
<p style="text-align: center;"><code class="reqn">L=D^{-1}(D-A)</code>
</p>
<p>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>scSM(data, k = 2, sigma = 1, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="scSM_+3A_data">data</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix of row-stacked observations or S3 <code>dist</code> object of <code class="reqn">n</code> observations.</p>
</td></tr>
<tr><td><code id="scSM_+3A_k">k</code></td>
<td>
<p>the number of clusters (default: 2).</p>
</td></tr>
<tr><td><code id="scSM_+3A_sigma">sigma</code></td>
<td>
<p>bandwidth parameter (default: 1).</p>
</td></tr>
<tr><td><code id="scSM_+3A_...">...</code></td>
<td>
<p>extra parameters including </p>

<dl>
<dt>algclust</dt><dd><p>method to perform clustering on embedded data; either <code>"kmeans"</code> (default) or <code>"GMM"</code>.</p>
</dd>
<dt>maxiter</dt><dd><p>the maximum number of iterations (default: 10).</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list of S3 class <code>T4cluster</code> containing 
</p>

<dl>
<dt>cluster</dt><dd><p>a length-<code class="reqn">n</code> vector of class labels (from <code class="reqn">1:k</code>).</p>
</dd> 
<dt>eigval</dt><dd><p>eigenvalues of the graph laplacian's spectral decomposition.</p>
</dd>
<dt>embeds</dt><dd><p>an <code class="reqn">(n\times k)</code> low-dimensional embedding.</p>
</dd>
<dt>algorithm</dt><dd><p>name of the algorithm.</p>
</dd>
</dl>



<h3>References</h3>

<p>Shi J, Malik J (Aug./2000).
&ldquo;Normalized Cuts and Image Segmentation.&rdquo;
<em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, <b>22</b>(8), 888&ndash;905.
ISSN 01628828.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># -------------------------------------------------------------
#            clustering with 'iris' dataset
# -------------------------------------------------------------
## PREPARE WITH SUBSET OF DATA
data(iris)
sid = sample(1:150, 50)
X   = as.matrix(iris[sid,1:4])
lab = as.integer(as.factor(iris[sid,5]))

## EMBEDDING WITH PCA
X2d = Rdimtools::do.pca(X, ndim=2)$Y

## CLUSTERING WITH DIFFERENT K VALUES
cl2 = scSM(X, k=2)$cluster
cl3 = scSM(X, k=3)$cluster
cl4 = scSM(X, k=4)$cluster

## VISUALIZATION
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,4), pty="s")
plot(X2d, col=lab, pch=19, main="true label")
plot(X2d, col=cl2, pch=19, main="scSM: k=2")
plot(X2d, col=cl3, pch=19, main="scSM: k=3")
plot(X2d, col=cl4, pch=19, main="scSM: k=4")
par(opar)

</code></pre>

<hr>
<h2 id='scUL'>Spectral Clustering with Unnormalized Laplacian</h2><span id='topic+scUL'></span>

<h3>Description</h3>

<p>The version of Shi and Malik first constructs the affinity matrix
</p>
<p style="text-align: center;"><code class="reqn">A_{ij} = \exp(-d(x_i, d_j)^2 / \sigma^2)</code>
</p>

<p>where <code class="reqn">\sigma</code> is a common bandwidth parameter and performs k-means (or possibly, GMM) clustering on 
the row-space of eigenvectors for the unnormalized graph laplacian matrix
</p>
<p style="text-align: center;"><code class="reqn">L=D-A</code>
</p>
<p>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>scUL(data, k = 2, sigma = 1, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="scUL_+3A_data">data</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix of row-stacked observations or S3 <code>dist</code> object of <code class="reqn">n</code> observations.</p>
</td></tr>
<tr><td><code id="scUL_+3A_k">k</code></td>
<td>
<p>the number of clusters (default: 2).</p>
</td></tr>
<tr><td><code id="scUL_+3A_sigma">sigma</code></td>
<td>
<p>bandwidth parameter (default: 1).</p>
</td></tr>
<tr><td><code id="scUL_+3A_...">...</code></td>
<td>
<p>extra parameters including </p>

<dl>
<dt>algclust</dt><dd><p>method to perform clustering on embedded data; either <code>"kmeans"</code> (default) or <code>"GMM"</code>.</p>
</dd>
<dt>maxiter</dt><dd><p>the maximum number of iterations (default: 10).</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list of S3 class <code>T4cluster</code> containing 
</p>

<dl>
<dt>cluster</dt><dd><p>a length-<code class="reqn">n</code> vector of class labels (from <code class="reqn">1:k</code>).</p>
</dd> 
<dt>eigval</dt><dd><p>eigenvalues of the graph laplacian's spectral decomposition.</p>
</dd>
<dt>embeds</dt><dd><p>an <code class="reqn">(n\times k)</code> low-dimensional embedding.</p>
</dd>
<dt>algorithm</dt><dd><p>name of the algorithm.</p>
</dd>
</dl>



<h3>References</h3>

<p>von Luxburg U (2007).
&ldquo;A Tutorial on Spectral Clustering.&rdquo;
<em>Statistics and Computing</em>, <b>17</b>(4), 395&ndash;416.
ISSN 0960-3174, 1573-1375.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># -------------------------------------------------------------
#            clustering with 'iris' dataset
# -------------------------------------------------------------
## PREPARE
data(iris)
X   = as.matrix(iris[,1:4])
lab = as.integer(as.factor(iris[,5]))

## EMBEDDING WITH PCA
X2d = Rdimtools::do.pca(X, ndim=2)$Y

## CLUSTERING WITH DIFFERENT K VALUES
cl2 = scUL(X, k=2)$cluster
cl3 = scUL(X, k=3)$cluster
cl4 = scUL(X, k=4)$cluster

## VISUALIZATION
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,4), pty="s")
plot(X2d, col=lab, pch=19, main="true label")
plot(X2d, col=cl2, pch=19, main="scUL: k=2")
plot(X2d, col=cl3, pch=19, main="scUL: k=3")
plot(X2d, col=cl4, pch=19, main="scUL: k=4")
par(opar)

</code></pre>

<hr>
<h2 id='spkmeans'>Spherical K-Means Clustering</h2><span id='topic+spkmeans'></span>

<h3>Description</h3>

<p>Spherical <code class="reqn">k</code>-means algorithm performs clustering for the data residing 
on the unit hypersphere with the cosine similarity. If the data is not 
normalized, it performs the normalization and proceeds thereafter.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spkmeans(data, k = 2, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="spkmeans_+3A_data">data</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix of row-stacked observations. If not row-stochastic, each row is normalized to be unit norm.</p>
</td></tr>
<tr><td><code id="spkmeans_+3A_k">k</code></td>
<td>
<p>the number of clusters (default: 2).</p>
</td></tr>
<tr><td><code id="spkmeans_+3A_...">...</code></td>
<td>
<p>extra parameters including </p>

<dl>
<dt>init</dt><dd><p>initialization method; either <code>"kmeans"</code> or <code>"gmm"</code> (default: <code>"kmeans"</code>).</p>
</dd>
<dt>maxiter</dt><dd><p>the maximum number of iterations (default: 10).</p>
</dd>
<dt>abstol</dt><dd><p>stopping criterion to stop the algorithm (default: <code class="reqn">10^{-8}</code>).</p>
</dd>
<dt>verbose</dt><dd><p>a logical; <code>TRUE</code> to show iteration history or <code>FALSE</code> to quiet.</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list of S3 class <code>T4cluster</code> containing 
</p>

<dl>
<dt>cluster</dt><dd><p>a length-<code class="reqn">n</code> vector of class labels (from <code class="reqn">1:k</code>).</p>
</dd> 
<dt>cost</dt><dd><p>a value of the cost function.</p>
</dd>
<dt>means</dt><dd><p>an <code class="reqn">(k\times p)</code> matrix where each row is a unit-norm class mean. </p>
</dd>
<dt>algorithm</dt><dd><p>name of the algorithm.</p>
</dd>
</dl>



<h3>References</h3>

<p>I. S. Dhillon and D. S. Modha (2001). &quot;Concept decompositions for large sparse text data using clustering.&quot; <em>Machine Learning</em>, <strong>42</strong>:143–175.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# -------------------------------------------------------------
#            clustering with 'household' dataset
# -------------------------------------------------------------
## PREPARE
data(household, package="T4cluster")
X   = household$data
lab = as.integer(household$gender)

## EXECUTE SPKMEANS WITH VARYING K's
vec.rand = rep(0, 9)
for (i in 1:9){
  clust_i = spkmeans(X, k=(i+1))$cluster
  vec.rand[i] = compare.rand(clust_i, lab)
}

## VISUALIZE THE RAND INDEX
opar &lt;- par(no.readonly=TRUE)
plot(2:10, vec.rand, type="b", pch=19, ylim=c(0.5, 1),
     ylab="Rand index",xlab="number of clusters",
     main="clustering quality index over varying k's.")
par(opar)


</code></pre>

<hr>
<h2 id='SSC'>Sparse Subspace Clustering</h2><span id='topic+SSC'></span>

<h3>Description</h3>

<p>Sparse Subspace Clustering (SSC) assumes that the data points lie in 
a union of low-dimensional subspaces. The algorithm constructs local 
connectivity and uses the information for spectral clustering. <code>SSC</code> is 
an implementation based on basis pursuit for sparse reconstruction for the 
model without systematic noise, which solves
</p>
<p style="text-align: center;"><code class="reqn">\textrm{min}_C \|C\|_1\quad\textrm{such that}\quad diag(C)=0,~D=DC</code>
</p>

<p>for column-stacked data matrix <code class="reqn">D</code>. If you are interested in 
full implementation of the algorithm with sparse outliers and noise, please 
contact the maintainer.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SSC(data, k = 2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SSC_+3A_data">data</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix of row-stacked observations.</p>
</td></tr>
<tr><td><code id="SSC_+3A_k">k</code></td>
<td>
<p>the number of clusters (default: 2).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list of S3 class <code>T4cluster</code> containing 
</p>

<dl>
<dt>cluster</dt><dd><p>a length-<code class="reqn">n</code> vector of class labels (from <code class="reqn">1:k</code>).</p>
</dd> 
<dt>algorithm</dt><dd><p>name of the algorithm.</p>
</dd>
</dl>



<h3>References</h3>

<p>Elhamifar E, Vidal R (2009).
&ldquo;Sparse Subspace Clustering.&rdquo;
In <em>2009 IEEE Conference on Computer Vision and Pattern Recognition</em>, 2790&ndash;2797.
ISBN 978-1-4244-3992-8.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## generate a toy example
set.seed(10)
tester = genLP(n=100, nl=2, np=1, iso.var=0.1)
data   = tester$data
label  = tester$class

## do PCA for data reduction
proj = base::eigen(stats::cov(data))$vectors[,1:2]
dat2 = data%*%proj

## run SSC algorithm with k=2, 3, and 4
output2 = SSC(data, k=2)
output3 = SSC(data, k=3)
output4 = SSC(data, k=4)

## extract label information
lab2 = output2$cluster
lab3 = output3$cluster
lab4 = output4$cluster

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(3,4))
plot(dat2[,1],dat2[,2],pch=19,cex=0.3,col=lab2,main="K=2:PCA")
plot(data[,1],data[,2],pch=19,cex=0.3,col=lab2,main="K=2:Axis(1,2)")
plot(data[,1],data[,3],pch=19,cex=0.3,col=lab2,main="K=2:Axis(1,3)")
plot(data[,2],data[,3],pch=19,cex=0.3,col=lab2,main="K=2:Axis(2,3)")

plot(dat2[,1],dat2[,2],pch=19,cex=0.3,col=lab3,main="K=3:PCA")
plot(data[,1],data[,2],pch=19,cex=0.3,col=lab3,main="K=3:Axis(1,2)")
plot(data[,1],data[,3],pch=19,cex=0.3,col=lab3,main="K=3:Axis(1,3)")
plot(data[,2],data[,3],pch=19,cex=0.3,col=lab3,main="K=3:Axis(2,3)")

plot(dat2[,1],dat2[,2],pch=19,cex=0.3,col=lab4,main="K=4:PCA")
plot(data[,1],data[,2],pch=19,cex=0.3,col=lab4,main="K=4:Axis(1,2)")
plot(data[,1],data[,3],pch=19,cex=0.3,col=lab4,main="K=4:Axis(1,3)")
plot(data[,2],data[,3],pch=19,cex=0.3,col=lab4,main="K=4:Axis(2,3)")
par(opar)


</code></pre>

<hr>
<h2 id='SSQP'>Subspace Segmentation via Quadratic Programming</h2><span id='topic+SSQP'></span>

<h3>Description</h3>

<p>Subspace Segmentation via Quadratic Programming (SSQP) solves the following problem
</p>
<p style="text-align: center;"><code class="reqn">\textrm{min}_Z \|X-XZ\|_F^2 + \lambda \|Z^\top Z\|_1 \textrm{ such that }diag(Z)=0,~Z\leq 0</code>
</p>

<p>where <code class="reqn">X\in\mathbf{R}^{p\times n}</code> is a column-stacked data matrix. The computed <code class="reqn">Z^*</code> is 
used as an affinity matrix for spectral clustering.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SSQP(data, k = 2, lambda = 1e-05, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SSQP_+3A_data">data</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix of row-stacked observations.</p>
</td></tr>
<tr><td><code id="SSQP_+3A_k">k</code></td>
<td>
<p>the number of clusters (default: 2).</p>
</td></tr>
<tr><td><code id="SSQP_+3A_lambda">lambda</code></td>
<td>
<p>regularization parameter (default: 1e-5).</p>
</td></tr>
<tr><td><code id="SSQP_+3A_...">...</code></td>
<td>
<p>extra parameters for the gradient descent algorithm including </p>

<dl>
<dt>maxiter</dt><dd><p>maximum number of iterations (default: 100).</p>
</dd>
<dt>abstol</dt><dd><p>tolerance level to stop (default: 1e-7).</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list of S3 class <code>T4cluster</code> containing 
</p>

<dl>
<dt>cluster</dt><dd><p>a length-<code class="reqn">n</code> vector of class labels (from <code class="reqn">1:k</code>).</p>
</dd> 
<dt>algorithm</dt><dd><p>name of the algorithm.</p>
</dd>
</dl>



<h3>References</h3>

<p>Wang S, Yuan X, Yao T, Yan S, Shen J (2011).
&ldquo;Efficient Subspace Segmentation via Quadratic Programming.&rdquo;
In <em>Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence</em>,  AAAI'11, 519&ndash;524.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## generate a toy example
set.seed(10)
tester = genLP(n=100, nl=2, np=1, iso.var=0.1)
data   = tester$data
label  = tester$class

## do PCA for data reduction
proj = base::eigen(stats::cov(data))$vectors[,1:2]
dat2 = data%*%proj

## run SSQP for k=3 with different lambda values
out1 = SSQP(data, k=3, lambda=1e-2)
out2 = SSQP(data, k=3, lambda=1)
out3 = SSQP(data, k=3, lambda=1e+2)

## extract label information
lab1 = out1$cluster
lab2 = out2$cluster
lab3 = out3$cluster

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(dat2, pch=19, cex=0.9, col=lab1, main="SSQP:lambda=1e-2")
plot(dat2, pch=19, cex=0.9, col=lab2, main="SSQP:lambda=1")
plot(dat2, pch=19, cex=0.9, col=lab3, main="SSQP:lambda=1e+2")
par(opar)


</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
