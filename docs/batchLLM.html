<!DOCTYPE html><html lang="en"><head><title>Help for package batchLLM</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {batchLLM}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#batchLLM'><p>Batch Process LLM Text Completions Using a Data Frame</p></a></li>
<li><a href='#batchLLM_shiny'><p>Interact with batchLLM via a Shiny Gadget</p></a></li>
<li><a href='#beliefs'><p>Beliefs Dataset</p></a></li>
<li><a href='#claudeR'><p>Interact with Anthropic's Claude API</p></a></li>
<li><a href='#get_batches'><p>Get Batches</p></a></li>
<li><a href='#scrape_metadata'><p>Scrape Metadata</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Batch Process LLM Text Completions Using a Data Frame</td>
</tr>
<tr>
<td>Version:</td>
<td>0.2.0</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Dylan Pieper &lt;dylanpieper@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Batch process large language model (LLM) text completions using data frame rows, with support for OpenAI's 'GPT' (<a href="https://chat.openai.com">https://chat.openai.com</a>), Anthropic's 'Claude' (<a href="https://claude.ai">https://claude.ai</a>), and Google's 'Gemini' (<a href="https://gemini.google.com">https://gemini.google.com</a>). Includes features such as local storage, metadata logging, API rate limiting delays, and a 'shiny' app addin.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.2</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/dylanpieper/batchLLM">https://github.com/dylanpieper/batchLLM</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/dylanpieper/batchLLM/issues">https://github.com/dylanpieper/batchLLM/issues</a></td>
</tr>
<tr>
<td>Imports:</td>
<td>openai, gemini.R, rlang, stats, digest, dplyr, shiny,
shinyWidgets, shinydashboard, DT, httr, jsonlite, spsComps,
shinyjs, readr, readxl</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10)</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-10-09 18:36:10 UTC; dylan</td>
</tr>
<tr>
<td>Author:</td>
<td>Dylan Pieper [aut, cre, cph]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-10-14 09:30:06 UTC</td>
</tr>
</table>
<hr>
<h2 id='batchLLM'>Batch Process LLM Text Completions Using a Data Frame</h2><span id='topic+batchLLM'></span>

<h3>Description</h3>

<p>Batch process large language model (LLM) text completions by looping across the rows of a data frame column.
The package currently supports OpenAI's GPT, Anthropic's Claude, and Google's Gemini models, with built-in delays for API rate limiting.
The package provides advanced text processing features, including automatic logging of batches and metadata to local files, side-by-side comparison of outputs from different LLMs, and integration of a user-friendly Shiny App Addin.
Use cases include natural language processing tasks such as sentiment analysis, thematic analysis, classification, labeling or tagging, and language translation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>batchLLM(
  df,
  df_name = NULL,
  col,
  prompt,
  LLM = "openai",
  model = "gpt-4o-mini",
  temperature = 0.5,
  max_tokens = 500,
  batch_delay = "random",
  batch_size = 10,
  case_convert = NULL,
  sanitize = FALSE,
  attempts = 1,
  log_name = "batchLLM-log",
  hash_algo = "crc32c",
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="batchLLM_+3A_df">df</code></td>
<td>
<p>A data frame that contains the input data.</p>
</td></tr>
<tr><td><code id="batchLLM_+3A_df_name">df_name</code></td>
<td>
<p>An optional string specifying the name of the data frame to log. This is particularly useful in Shiny applications or when the data frame is passed programmatically rather than explicitly. Default is NULL.</p>
</td></tr>
<tr><td><code id="batchLLM_+3A_col">col</code></td>
<td>
<p>The name of the column in the data frame to process.</p>
</td></tr>
<tr><td><code id="batchLLM_+3A_prompt">prompt</code></td>
<td>
<p>A system prompt for the LLM model.</p>
</td></tr>
<tr><td><code id="batchLLM_+3A_llm">LLM</code></td>
<td>
<p>A string for the name of the LLM with the options: &quot;openai&quot;, &quot;anthropic&quot;, and &quot;google&quot;. Default is &quot;openai&quot;.</p>
</td></tr>
<tr><td><code id="batchLLM_+3A_model">model</code></td>
<td>
<p>A string for the name of the model from the LLM. Default is &quot;gpt-4o-mini&quot;.</p>
</td></tr>
<tr><td><code id="batchLLM_+3A_temperature">temperature</code></td>
<td>
<p>A temperature for the LLM model. Default is .5.</p>
</td></tr>
<tr><td><code id="batchLLM_+3A_max_tokens">max_tokens</code></td>
<td>
<p>A maximum number of tokens to generate before stopping. Default is 500.</p>
</td></tr>
<tr><td><code id="batchLLM_+3A_batch_delay">batch_delay</code></td>
<td>
<p>A string for the batch delay with the options: &quot;random&quot;, &quot;min&quot;, and &quot;sec&quot;. Numeric examples include &quot;1min&quot; and &quot;30sec&quot;. Default is &quot;random&quot; which is an average of 10.86 seconds (n = 1,000 simulations).</p>
</td></tr>
<tr><td><code id="batchLLM_+3A_batch_size">batch_size</code></td>
<td>
<p>The number of rows to process in each batch. Default is 10.</p>
</td></tr>
<tr><td><code id="batchLLM_+3A_case_convert">case_convert</code></td>
<td>
<p>A string for the case conversion of the output with the options: &quot;upper&quot;, &quot;lower&quot;, or NULL (no change). Default is NULL.</p>
</td></tr>
<tr><td><code id="batchLLM_+3A_sanitize">sanitize</code></td>
<td>
<p>Extract the LLM text completion from the model's response by returning only content in <code>&lt;result&gt;</code> XML tags. Additionally, remove all punctuation. This feature prevents unwanted text (e.g., preamble) or punctuation from being included in the model's output. Default is FALSE.</p>
</td></tr>
<tr><td><code id="batchLLM_+3A_attempts">attempts</code></td>
<td>
<p>The maximum number of loop retry attempts. Default is 1.</p>
</td></tr>
<tr><td><code id="batchLLM_+3A_log_name">log_name</code></td>
<td>
<p>A string for the name of the log without the <code>.rds</code> file extension. Default is &quot;batchLLM-log&quot;.</p>
</td></tr>
<tr><td><code id="batchLLM_+3A_hash_algo">hash_algo</code></td>
<td>
<p>A string for a hashing algorithm from the 'digest' package. Default is <code>crc32c</code>.</p>
</td></tr>
<tr><td><code id="batchLLM_+3A_...">...</code></td>
<td>
<p>Additional arguments to pass on to the LLM API function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns the input data frame with an additional column containing the text completion output.
The function also writes the output and metadata to the log file after each batch in a nested list format.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(batchLLM)

# Set API keys
Sys.setenv(OPENAI_API_KEY = "your_openai_api_key")
Sys.setenv(ANTHROPIC_API_KEY = "your_anthropic_api_key")
Sys.setenv(GEMINI_API_KEY = "your_gemini_api_key")

# Define LLM configurations
llm_configs &lt;- list(
  list(LLM = "openai", model = "gpt-4o-mini"),
  list(LLM = "anthropic", model = "claude-3-haiku-20240307"),
  list(LLM = "google", model = "1.5-flash")
)

# Apply batchLLM function to each configuration
beliefs &lt;- lapply(llm_configs, function(config) {
  batchLLM(
    df = beliefs,
    col = statement,
    prompt = "classify as a fact or misinformation in one word",
    LLM = config$LLM,
    model = config$model,
    batch_size = 10,
    batch_delay = "1min",
    case_convert = "lower"
  )
})[[length(llm_configs)]]

# Print the updated data frame
print(beliefs)

## End(Not run)
</code></pre>

<hr>
<h2 id='batchLLM_shiny'>Interact with batchLLM via a Shiny Gadget</h2><span id='topic+batchLLM_shiny'></span>

<h3>Description</h3>

<p>This function provides a user interface using Shiny to interact with
the <code>batchLLM</code> package. It allows users to configure and execute batch processing
through an interactive dashboard.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>batchLLM_shiny()
</code></pre>


<h3>Value</h3>

<p>No return value. Launches a Shiny Gadget that allows users to interact with the <code>batchLLM</code> package.
</p>

<hr>
<h2 id='beliefs'>Beliefs Dataset</h2><span id='topic+beliefs'></span>

<h3>Description</h3>

<p>The beliefs dataset consists of 20 statements representing opposing views on various scientific, environmental, and societal topics.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>beliefs
</code></pre>


<h3>Format</h3>

<p>A data frame with 20 rows and 1 variable:
</p>

<dl>
<dt>statement</dt><dd><p>A character string with a statement representing a belief.</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>head(beliefs)
</code></pre>

<hr>
<h2 id='claudeR'>Interact with Anthropic's Claude API</h2><span id='topic+claudeR'></span>

<h3>Description</h3>

<p>This function provides an interface to interact with Claude AI models via Anthropic's API, allowing for flexible text generation based on user inputs.
This function was adapted from the <a href="https://github.com/yrvelez/claudeR">claudeR</a> repository by <a href="https://github.com/yrvelez">yrvelez</a> on GitHub (MIT License).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>claudeR(
  prompt,
  model = "claude-3-5-sonnet-20240620",
  max_tokens = 500,
  stop_sequences = NULL,
  temperature = 0.7,
  top_k = -1,
  top_p = -1,
  api_key = NULL,
  system_prompt = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="claudeR_+3A_prompt">prompt</code></td>
<td>
<p>A string vector for Claude-2, or a list for Claude-3 specifying the input for the model.</p>
</td></tr>
<tr><td><code id="claudeR_+3A_model">model</code></td>
<td>
<p>The model to use for the request. Default is the latest Claude-3 model.</p>
</td></tr>
<tr><td><code id="claudeR_+3A_max_tokens">max_tokens</code></td>
<td>
<p>A maximum number of tokens to generate before stopping.</p>
</td></tr>
<tr><td><code id="claudeR_+3A_stop_sequences">stop_sequences</code></td>
<td>
<p>Optional. A list of strings upon which to stop generating.</p>
</td></tr>
<tr><td><code id="claudeR_+3A_temperature">temperature</code></td>
<td>
<p>Optional. Amount of randomness injected into the response.</p>
</td></tr>
<tr><td><code id="claudeR_+3A_top_k">top_k</code></td>
<td>
<p>Optional. Only sample from the top K options for each subsequent token.</p>
</td></tr>
<tr><td><code id="claudeR_+3A_top_p">top_p</code></td>
<td>
<p>Optional. Does nucleus sampling.</p>
</td></tr>
<tr><td><code id="claudeR_+3A_api_key">api_key</code></td>
<td>
<p>Your API key for authentication.</p>
</td></tr>
<tr><td><code id="claudeR_+3A_system_prompt">system_prompt</code></td>
<td>
<p>Optional. An optional system role specification.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The resulting completion up to and excluding the stop sequences.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(batchLLM)

# Set API in the env or use api_key parameter in the claudeR call
Sys.setenv(ANTHROPIC_API_KEY = "your_anthropic_api_key")

# Using Claude-2
response &lt;- claudeR(
  prompt = "What is the capital of France?",
  model = "claude-2.1",
  max_tokens = 50
)
cat(response)

# Using Claude-3
response &lt;- claudeR(
  prompt = list(
    list(role = "user", content = "What is the capital of France?")
  ),
  model = "claude-3-5-sonnet-20240620",
  max_tokens = 50,
  temperature = 0.8
)
cat(response)

# Using a system prompt
response &lt;- claudeR(
  prompt = list(
    list(role = "user", content = "Summarize the history of France in one paragraph.")
  ),
  system_prompt = "You are a concise summarization assistant.",
  max_tokens = 500
)
cat(response)

## End(Not run)
</code></pre>

<hr>
<h2 id='get_batches'>Get Batches</h2><span id='topic+get_batches'></span>

<h3>Description</h3>

<p>Get batches of generated output in a single data frame from the <code>.rds</code> log file.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_batches(df_name = NULL, log_name = "batchLLM-log")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_batches_+3A_df_name">df_name</code></td>
<td>
<p>A string to match the name of a processed data frame.</p>
</td></tr>
<tr><td><code id="get_batches_+3A_log_name">log_name</code></td>
<td>
<p>A string specifying the name of the log without the <code>.rds</code> file extension. Default is &quot;batchLLM-log&quot;.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame containing the generated output.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(batchLLM)

# Assuming you have a log file with data for "beliefs_40a3012b" (see batchLLM example)
batches &lt;- get_batches("beliefs_40a3012b")
head(batches)

# Using a custom log file name
custom_batches &lt;- get_batches("beliefs_40a3012b", log_name = "custom-log.rds")
head(custom_batches)

## End(Not run)
</code></pre>

<hr>
<h2 id='scrape_metadata'>Scrape Metadata</h2><span id='topic+scrape_metadata'></span>

<h3>Description</h3>

<p>Scrape metadata from the <code>.rds</code> log file.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>scrape_metadata(df_name = NULL, log_name = "batchLLM-log")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="scrape_metadata_+3A_df_name">df_name</code></td>
<td>
<p>Optional. A string to match the name of a processed data frame.</p>
</td></tr>
<tr><td><code id="scrape_metadata_+3A_log_name">log_name</code></td>
<td>
<p>A string specifying the name of the log file without the extension. Default is &quot;batchLLM-log&quot;.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame containing metadata.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(batchLLM)

# Scrape metadata for all data frames in the default log file
all_metadata &lt;- scrape_metadata()
head(all_metadata)

# Scrape metadata for a specific data frame
specific_metadata &lt;- scrape_metadata("beliefs_40a3012b")
head(specific_metadata)

# Use a custom log file name
custom_metadata &lt;- scrape_metadata(log_name = "custom-log")
head(custom_metadata)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
