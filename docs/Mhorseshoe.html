<!DOCTYPE html><html><head><title>Help for package Mhorseshoe</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {Mhorseshoe}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#Mhorseshoe'><p>Mhorseshoe: Approximate Algorithm for Horseshoe Prior</p></a></li>
<li><a href='#approx_horseshoe'><p>Run approximate MCMC algorithm for horseshoe prior</p></a></li>
<li><a href='#exact_horseshoe'><p>Run exact MCMC algorithm for horseshoe prior</p></a></li>
<li><a href='#rejection_sampler'><p>rejection sampler to update local shrinkage parameters lambda.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Approximate Algorithm for Horseshoe Prior</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.2</td>
</tr>
<tr>
<td>Description:</td>
<td>Provides an approximate algorithm for the horseshoe estimator
    used in Bayesian linear models. By implementing a sampler with high
    computational cost in the 'Rcpp' package and using an approximate algorithm
    that reduces matrix calculation complexity, parameter estimation speed for
    high-dimensional sparse data is faster. The approximate algorithm is
    described in Johndrow et al. (2020)
    <a href="https://www.jmlr.org/papers/v21/19-536.html">https://www.jmlr.org/papers/v21/19-536.html</a>.</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Imports:</td>
<td>stats, Rcpp (&ge; 1.0.11)</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown, ggplot2, horseshoe</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-11-23 04:23:29 UTC; rkdal</td>
</tr>
<tr>
<td>Author:</td>
<td>Kang Mingi [aut, cre],
  Lee Kyoungjae [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Kang Mingi &lt;leehuimin115@g.skku.edu&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-11-24 09:20:10 UTC</td>
</tr>
</table>
<hr>
<h2 id='Mhorseshoe'>Mhorseshoe: Approximate Algorithm for Horseshoe Prior</h2><span id='topic+Mhorseshoe'></span><span id='topic+Mhorseshoe-package'></span>

<h3>Description</h3>

<p>Mhorseshoe is a package for a high-dimensional Bayesian linear
modeling algorithm using a horseshoe prior. This package provides two
different algorithm functions : <code><a href="#topic+exact_horseshoe">exact_horseshoe</a></code>,
<code><a href="#topic+approx_horseshoe">approx_horseshoe</a></code>. approx_horseshoe is version that can lower
the computational cost than the existing horseshoe estimator through the
approximate MCMC algorithm in the case of <code class="reqn">p &gt;&gt; N</code> for <code class="reqn">p</code>
predictors and <code class="reqn">N</code> observations. You can see examples of the use of the
two algorithms through the vignette, <code>browseVignettes("Mhorseshoe")</code>.
</p>


<h3>About the horseshoe estimator in this package</h3>

<p>The Bayesian horseshoe estimator(Carvalho et al., 2010) has good properties
such as being robust to high-dimensional sparse data and outliers, and has
good computational efficiency, making it suitable for high-dimensional
Bayesian inference. The likelihood function of the Gaussian linear model to
be considered in this package is as follows:
</p>
<p style="text-align: center;"><code class="reqn">L(y\ |\ x, \beta, \sigma^2) = (\frac{1}{\sqrt{2\pi}\sigma})^{-N/2}exp
\{ -\frac{1}{2\sigma^2}(y-X\beta)^T(y-X\beta)\}</code>
</p>

<p>And the form of the hierarchical model applying the horseshoe prior to
<code class="reqn">\beta</code> is expressed as follows:
</p>
<p style="text-align: center;"><code class="reqn">\beta_{j}\ |\ \sigma^{2}, \tau^{2}, \lambda_{j}^{2}
\overset{i.i.d}{\sim} N\left(0, \sigma^{2}\tau^{2}\lambda_{j}^{2} \right),
\quad \lambda_{j}\overset{i.i.d}{\sim}C^{+}(0,1),\quad j=1,2,...,p, \\ \tau
\overset{i.i.d}{\sim}C^{+}(0,1),\quad p(\sigma^2)\propto gamma\left(
w/2, w/2\right).</code>
</p>

<p>Where <code class="reqn">C^{+}(0,1)</code> is a half-Cauchy distribution, <code class="reqn">\lambda_{1}, ...,
\lambda_{p}</code> are local shrinkage parameters, and <code class="reqn">\tau</code> is global
shrinkage parameter. In general, the disadvantage of the horseshoe estimator
is that the computational cost is large for high-dimensional data where
<code class="reqn">p&gt;&gt;n</code>, and the computation of the algorithm is very slow. To overcome
these limitations, Johndrow et al. (2020) proposed a scalable approximate
MCMC algorithm that can reduce computational costs by introducing a
thresholding method while applying the horseshoe prior. This approximate
algorithm is implemented in <code><a href="#topic+approx_horseshoe">approx_horseshoe</a></code> of this package.
</p>


<h3>Exact algorithm process</h3>

<p>The exact algorithm process is as follows.
</p>
<p style="text-align: center;"><code class="reqn">\xi = \tau^{-2},\quad \eta_{j} = \lambda_{j}^{-2},\quad
j = 1,2,...,p, \\ D = diag\left(\eta_{1}^{-1},..., \eta_{p}^{-1} \right),
\quad M_{\xi} = I_{N} + \xi^{-1}XDX^{T}, \\ p(\xi | y, X, \eta) \propto
|M_{\xi}|^{-1/2}\{(w + y^{T}M_{\xi}^{-1}y) / 2 \}^{-(w + N) / 2}/
\{\sqrt{\xi}(1+\xi)\}.</code>
</p>


<ol>
<li><p> Sample <code class="reqn">\lambda_{j}^{-2} = \eta_{j},\ j=1,2,...,p</code>, using the
following posterior of <code class="reqn">\eta_{j}</code> for rejection sampling.
</p>
<p style="text-align: center;"><code class="reqn">p(\eta_{j} | \xi, \beta_{j}, \sigma^{2}) \propto \frac{1}{1 +
\eta_{j}}exp\{-\frac{\beta_{j}^{2}\xi \eta_{j}}{2\sigma^{2}} \}.</code>
</p>

</li>
<li><p> Sample <code class="reqn">\tau^{-2} = \xi</code> using the proposed MH algorithm as follows.
</p>
<p style="text-align: center;"><code class="reqn">log(\xi^{\star}) \sim N(log(\xi), s), accept\ \xi \ w.p.\
\{p(\xi^{\star} | y, X, \eta)\xi^{\star}\}/\{p(\xi | y, X, \eta)\xi \}.</code>
</p>

</li>
<li><p> Sample <code class="reqn">\sigma^{2}</code>, </p>
<p style="text-align: center;"><code class="reqn">\sigma^{2} | y, X, \eta, \xi \sim
InvGamma\{(w + N) / 2, (w + y^{T}M_{\xi}^{-1}y) / 2 \}.</code>
</p>

</li>
<li><p> Sample <code class="reqn">\beta</code>, </p>
<p style="text-align: center;"><code class="reqn">\beta | y, X, \eta, \xi, \sigma \sim
N\left(\left(X^{T}X + \left(\xi^{-1} D \right)^{-1}\right)^{-1}X^{T}y,
\ \sigma^{2}\left(X^{T}X + \left(\xi^{-1} D \right)^{-1} \right)^{-1}
\right).</code>
</p>

</li></ol>

<p>Since the algorithm of this package considers the case of <code class="reqn">p &gt;&gt; N</code>,
the computational cost can be lowered by sampling <code class="reqn">\beta</code> using
fast sampling(Bhattacharya et al.,2016) in the step 4.
</p>
<p style="text-align: center;"><code class="reqn">u \sim N_{p}(0, \xi^{-1}D),\quad f \sim N_{N}(0, I_{N}), \\
v = Xu + f,\quad v^{\star} = M_{\xi}^{-1}(y/\sigma - v), \\
\beta = \sigma(u + \xi^{-1}DX^{T}v^{\star}).</code>
</p>

<p>The exact algorithm is implemented in <code><a href="#topic+exact_horseshoe">exact_horseshoe</a></code> of this
package.
</p>


<h3>About approximate MCMC algorithm</h3>

<p>In order to reduce the computational cost of the exact algorithm, the
matrix <code class="reqn">D</code> to which the thresholding method is applied is called
<code class="reqn">D_{\delta}</code> as follows.
</p>
<p style="text-align: center;"><code class="reqn">D_{\delta} = diag\left(\eta_{j}^{-1}1\left(\xi^{-1}\eta_{j}^{-1}
&gt; \delta,\ j=1,2,...,p. \right) \right),</code>
</p>

<p>If a <code class="reqn">D_{\delta}</code> matrix is used instead of a <code class="reqn">D</code> matrix, rows and
columns with diagonal components of 0 can be eliminated, thus reducing the
computational cost for matrix multiplication. This is called the
approximate MCMC algorithm and is implemented in
<code><a href="#topic+approx_horseshoe">approx_horseshoe</a></code>.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Kang Mingi <a href="mailto:leehuimin115@g.skku.edu">leehuimin115@g.skku.edu</a>
</p>
<p>Authors:
</p>

<ul>
<li><p> Lee Kyoungjae <a href="mailto:leekjstat@gmail.com">leekjstat@gmail.com</a>
</p>
</li></ul>



<h3>References</h3>

<p>Bhattacharya, A., Chakraborty, A., &amp; Mallick, B. K. (2016).
Fast sampling with Gaussian scale mixture priors in high-dimensional
regression. Biometrika, asw042.
</p>
<p>Carvalho, C. M., Polson, N. G., &amp; Scott, J. G. (2010). The
horseshoe estimator for sparse signals. Biometrika, 97(2), 465â€“480.
</p>
<p>Johndrow, J., Orenstein, P., &amp; Bhattacharya, A. (2020). Scalable Approximate
MCMC Algorithms for the Horseshoe Prior. In Journal of Machine Learning
Research (Vol. 21).
</p>

<hr>
<h2 id='approx_horseshoe'>Run approximate MCMC algorithm for horseshoe prior</h2><span id='topic+approx_horseshoe'></span>

<h3>Description</h3>

<p>In this function, The algorithm introduced in Section 2.2 of Johndrow et al.
(2020) is implemented, and is a horseshoe estimator that generally considers
the case where <code class="reqn">p&gt;&gt;N</code>. The assumptions and notations for the model are
the same as those in <code><a href="#topic+Mhorseshoe">Mhorseshoe</a></code>. This algorithm introduces a
threshold and uses only a portion of the total <code class="reqn">p</code> columns for matrix
multiplication, lowering the computational cost compared to the existing
horseshoe estimator. According to Section 3.2 of Johndrow et al. (2020), the
approximate MCMC algorithm applying the methodology constructs an
approximate Markov chain <code class="reqn">P_{\epsilon}</code> that can converge to an exact
Markov chain <code class="reqn">P</code>, and acceptable results were confirmed through
empirical analysis of simulation and real data. The &quot;auto.threshold&quot;
argument in this function is an adaptive probability algorithm for threshold
developed in this package, which is an algorithm that estimates and updates
a new threshold through updated shrinkage parameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>approx_horseshoe(
  X,
  y,
  burn = 1000,
  iter = 5000,
  auto.threshold = TRUE,
  threshold = 0,
  tau = 1,
  s = 0.8,
  sigma2 = 1,
  w = 1,
  alpha = 0.05,
  a = 0.2,
  b = 10,
  t = 10,
  adapt_p0 = 0,
  adapt_p1 = -4.6 * 10^(-4)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="approx_horseshoe_+3A_x">X</code></td>
<td>
<p>Design matrix, <code class="reqn">X \in \mathbb{R}^{N \times p}</code>.</p>
</td></tr>
<tr><td><code id="approx_horseshoe_+3A_y">y</code></td>
<td>
<p>Response vector, <code class="reqn">y \in \mathbb{R}^{N}</code>.</p>
</td></tr>
<tr><td><code id="approx_horseshoe_+3A_burn">burn</code></td>
<td>
<p>Number of burn-in samples. Default is 1000.</p>
</td></tr>
<tr><td><code id="approx_horseshoe_+3A_iter">iter</code></td>
<td>
<p>Number of samples to be drawn from the posterior. Default is
5000.</p>
</td></tr>
<tr><td><code id="approx_horseshoe_+3A_auto.threshold">auto.threshold</code></td>
<td>
<p>Argument for setting whether to use an algorithm that
automatically updates the threshold using adaptive probability.</p>
</td></tr>
<tr><td><code id="approx_horseshoe_+3A_threshold">threshold</code></td>
<td>
<p>Threshold to be used in the approximate MCMC algorithm.
If you select auto.threshold = FALSE, and threshold = 0(This is the default
value for the threshold argument), the threshold is set according to the
sizes of N and p. if <code class="reqn">p &lt; N</code>, <code class="reqn">\delta = 1/\sqrt{Np}</code>, else
<code class="reqn">\delta = 1/p</code>. Or, you can set your custom value directly through this
argument. For more information about <code class="reqn">\delta</code>, see
<code><a href="#topic+Mhorseshoe">Mhorseshoe</a></code> and 4.1 of Johndrow et al. (2020).</p>
</td></tr>
<tr><td><code id="approx_horseshoe_+3A_tau">tau</code></td>
<td>
<p>Initial value of the global shrinkage parameter <code class="reqn">\tau</code> when
starting the algorithm. Default is 1.</p>
</td></tr>
<tr><td><code id="approx_horseshoe_+3A_s">s</code></td>
<td>
<p><code class="reqn">s^{2}</code> is the variance of tau's MH proposal distribution.
0.8 is a good default. If set to 0, the algorithm proceeds by
fixing the global shrinkage parameter <code class="reqn">\tau</code> to the initial setting
value.</p>
</td></tr>
<tr><td><code id="approx_horseshoe_+3A_sigma2">sigma2</code></td>
<td>
<p>error variance <code class="reqn">\sigma^{2}</code>. Default is 1.</p>
</td></tr>
<tr><td><code id="approx_horseshoe_+3A_w">w</code></td>
<td>
<p>Parameter of gamma prior for <code class="reqn">\sigma^{2}</code>. Default is 1.</p>
</td></tr>
<tr><td><code id="approx_horseshoe_+3A_alpha">alpha</code></td>
<td>
<p><code class="reqn">100(1-\alpha)\%</code> credible interval setting argument.</p>
</td></tr>
<tr><td><code id="approx_horseshoe_+3A_a">a</code></td>
<td>
<p>Parameter of the rejection sampler, and it is recommended to leave
it at the default value, <code class="reqn">a = 1/5</code>.</p>
</td></tr>
<tr><td><code id="approx_horseshoe_+3A_b">b</code></td>
<td>
<p>Parameter of the rejection sampler, and it is recommended to leave
it at the default value, <code class="reqn">b = 10</code>.</p>
</td></tr>
<tr><td><code id="approx_horseshoe_+3A_t">t</code></td>
<td>
<p>Threshold update cycle for adaptive probability algorithm when
auto.threshold is set to TRUE. default is 10.</p>
</td></tr>
<tr><td><code id="approx_horseshoe_+3A_adapt_p0">adapt_p0</code></td>
<td>
<p>Parameter <code class="reqn">p_{0}</code> of adaptive probability,
<code class="reqn">p(t) = exp[p_{0} + p_{1}t]</code>. default is <code class="reqn">0</code>.</p>
</td></tr>
<tr><td><code id="approx_horseshoe_+3A_adapt_p1">adapt_p1</code></td>
<td>
<p>Parameter <code class="reqn">a_{1}</code> of adaptive probability,
<code class="reqn">p(t) = exp[p_{0} + p_{1}t]</code>. default is <code class="reqn">-4.6 \times 10^{-4}</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>BetaHat</code></td>
<td>
<p>Posterior mean of <code class="reqn">\beta</code>.</p>
</td></tr>
<tr><td><code>LeftCI</code></td>
<td>
<p>Lower bound of <code class="reqn">100(1-\alpha)\%</code> credible interval for
<code class="reqn">\beta</code>.</p>
</td></tr>
<tr><td><code>RightCI</code></td>
<td>
<p>Upper bound of <code class="reqn">100(1-\alpha)\%</code> credible interval for
<code class="reqn">\beta</code>.</p>
</td></tr>
<tr><td><code>Sigma2Hat</code></td>
<td>
<p>Posterior mean of <code class="reqn">\sigma^{2}</code>.</p>
</td></tr>
<tr><td><code>TauHat</code></td>
<td>
<p>Posterior mean of <code class="reqn">\tau</code>.</p>
</td></tr>
<tr><td><code>LambdaHat</code></td>
<td>
<p>Posterior mean of <code class="reqn">\lambda_{j},\ j=1,2,...p.</code>.</p>
</td></tr>
<tr><td><code>ActiveMean</code></td>
<td>
<p>Average number of elements in the active set per iteration
in this algorithm.</p>
</td></tr>
<tr><td><code>BetaSamples</code></td>
<td>
<p>Samples from the posterior of <code class="reqn">\beta</code>.</p>
</td></tr>
<tr><td><code>LambdaSamples</code></td>
<td>
<p>Lambda samples through rejection sampling.</p>
</td></tr>
<tr><td><code>TauSamples</code></td>
<td>
<p>Tau samples through MH algorithm.</p>
</td></tr>
<tr><td><code>Sigma2Samples</code></td>
<td>
<p>Samples from the posterior of the parameter
<code class="reqn">sigma^{2}</code>.</p>
</td></tr>
<tr><td><code>ActiveSet</code></td>
<td>
<p>Matrix indicating active elements as 1 and non-active
elements as 0 per iteration of the MCMC algorithm.</p>
</td></tr>
</table>


<h3>Approximate algorithm details</h3>

<p>Approximate algorithm has the following changes:
</p>
<p style="text-align: center;"><code class="reqn">D_{\delta} = diag\left(\eta_{j}^{-1}1\left(\xi^{-1}\eta_{j}^{-1}
&gt; \delta,\ j=1,2,...,p. \right) \right),</code>
</p>

<p style="text-align: center;"><code class="reqn">M_{\xi} \approx M_{\xi, \delta} = I_{N} + \xi^{-1}XD_{\delta}X^{T},</code>
</p>

<p>Where <code class="reqn">\eta_{j} = \lambda_{j}^{-2}</code>, <code class="reqn">\lambda_{j}</code>
are local shrinkage parameters, <code class="reqn">\xi = \tau^{-2}</code>, <code class="reqn">\tau</code> is a
global shrinkage parameter, <code class="reqn">1(\cdot)</code> is an indicator function that
returns <code class="reqn">1</code> if the conditions in the parentheses are satisfied, and
<code class="reqn">0</code> otherwise, and <code class="reqn">\delta</code> is the threshold. The set of
X's columns: <code class="reqn">\{x_{j}\ :\ \xi^{-1}\eta_{j}^{-1} &gt; \delta,\ j = 1,2,...,
p \}</code> is defined as the active set, and let's define <code class="reqn">S</code> as the
index set of the active set:
</p>
<p style="text-align: center;"><code class="reqn">S = \{j\ |\ \xi^{-1}\eta_{j}^{-1} &gt; \delta,\ j=1,2,...,p. \}.</code>
</p>

<p>Recalling the posterior distribution for <code class="reqn">\beta</code>, it is as follows:
</p>
<p style="text-align: center;"><code class="reqn">\beta | y, X, \eta, \xi, \sigma \sim
N\left(\left(X^{T}X + \left(\xi^{-1} D \right)^{-1}\right)^{-1}X^{T}y,
\ \sigma^{2}\left(X^{T}X + \left(\xi^{-1} D \right)^{-1} \right)^{-1}
\right).</code>
</p>

<p>If <code class="reqn">\xi^{-1} \eta_{j}^{-1}</code> is very small, the posterior of <code class="reqn">\beta</code>
will have a mean and variance close to 0. Therefore, let's set
<code class="reqn">\xi^{-1}\eta_{j}^{-1}</code> smaller than <code class="reqn">\delta</code> to 0 and the size of
inverse <code class="reqn">M_{\xi, \delta}</code> matrix is reduced as follows.
</p>
<p style="text-align: center;"><code class="reqn">length(S)=s_{\delta} \le p, \\ X_{S} \in R^{N \times s_{\delta}},
\quad D_{S} \in R^{s_{\delta} \times s_{\delta}}, \\ M_{\xi, \delta}^{-1} =
\left(I_{N} + \xi^{-1}X_{S}D_{S}X_{S}^{T} \right)^{-1}.</code>
</p>

<p><code class="reqn">M_{\xi, \delta}^{-1}</code> can be expressed using the Woodbury identity
as follows.
</p>
<p style="text-align: center;"><code class="reqn">M_{\xi, \delta}^{-1} = I_{N} - X_{S}\left(\xi D_{S}^{-1} +
X_{S}^{T}X_{S} \right)^{-1}X_{S}^{T}.</code>
</p>

<p><code class="reqn">M_{\xi, \delta}^{-1}</code>, which reduces the computational cost, is
applied to all parts of this algorithm, <code class="reqn">\beta</code> samples are extracted
from the posterior using fast sampling(Bhattacharya et al.,2016) as follows.
</p>
<p style="text-align: center;"><code class="reqn">u \sim N_{p}(0, \xi^{-1}D),\quad f \sim N_{N}(0, I_{N}), \\
v = Xu + f,\quad v^{\star} = M_{\xi, \delta}^{-1}(y/\sigma - v), \\
\beta = \sigma(u + \xi^{-1}D_{\delta}X^{T}v^{\star}).</code>
</p>



<h3>Adaptive probability algorithm for threshold update</h3>

<p>If the auto.threshold argument is set to TRUE, this algorithm operates
every <code class="reqn">t</code> iteration to estimate the threshold and decide whether to
update. In this algorithm, the process of updating a new threshold is added
by applying the properties of the shrinkage weight <code class="reqn">k_{j},\ j=1,2,...,p</code>
proposed by Piironen and Vehtari (2017). In the prior of <code class="reqn">\beta_{j} \sim
N(0, \sigma^{2}\tau^{2}\lambda_{j}^{2}) = N(0, \sigma^{2}\xi^{-1}
\eta_{j}^{-1})</code>, the variable <code class="reqn">m_{eff}</code> is defined as follows.
</p>
<p style="text-align: center;"><code class="reqn">k_{j} = 1/\left(1+n\xi^{-1}s_{j}^{2}\eta_{j}^{-1} \right),
\quad j=1,2,...,p, \\ m_{eff} = \sum_{j=1}^{p}{\left(1-k_{j} \right)}.</code>
</p>

<p>The assumptions and notations for the model are the same as those in
<code><a href="#topic+Mhorseshoe">Mhorseshoe</a></code>, and <code class="reqn">s_{j},\ j=1,2,...,p</code> are the diagonal
components of <code class="reqn">X^{T}X</code>. For the zero components of <code class="reqn">\beta</code>,
<code class="reqn">k_{j}</code> is derived close to 1, and nonzero's <code class="reqn">k_{j}</code> is derived
close to 0, so the variable <code class="reqn">m_{eff}</code>, the sum of <code class="reqn">1-k_{j}</code>, is
called the effective number of nonzero coefficients. In this algorithm, the
threshold <code class="reqn">\delta</code> is updated to set
<code class="reqn">s_{\delta} = ceiling(m_{eff})</code>.
</p>
<p>Adaptive probability is defined to satisfy
Theorem 5(diminishing adaptation condition) of Roberts and Rosenthal (2007).
at <code class="reqn">T</code>th iteration,
</p>
<p style="text-align: center;"><code class="reqn">p(T) = exp[p_{0} + p_{1}T],\quad p_{1} &lt; 0,
\quad u \sim U(0, 1), \\ if\ u &lt; p(T),\ update\ \delta\ so\ that\
s_{\delta} = ceiling(m_{eff}).</code>
</p>

<p>The default is <code class="reqn">p_{0} = 0</code>, <code class="reqn">p_{1} = -4.6 \times 10^{-4}</code>, and
under this condition, <code class="reqn">p(10000) &lt; 0.01</code> is satisfied.
</p>


<h3>References</h3>

<p>Bhattacharya, A., Chakraborty, A., &amp; Mallick, B. K. (2016).
Fast sampling with Gaussian scale mixture priors in high-dimensional
regression. Biometrika, asw042.
</p>
<p>Johndrow, J., Orenstein, P., &amp; Bhattacharya, A. (2020).
Scalable Approximate MCMC Algorithms for the Horseshoe Prior. In Journal
of Machine Learning Research (Vol. 21).
</p>
<p>Piironen, J., &amp; Vehtari, A. (2017). Sparsity information and regularization
in the horseshoe and other shrinkage priors. Electronic Journal of
Statistics, 11, 5018-5051.
</p>
<p>Roberts G, Rosenthal J. Coupling and ergodicity of adaptive Markov chain
Monte Carlo algorithms. J Appl Prob. 2007;44:458â€“475.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Making simulation data.
set.seed(123)
N &lt;- 200
p &lt;- 100
true_beta &lt;- c(rep(1, 10), rep(0, 90))

X &lt;- matrix(1, nrow = N, ncol = p) # Design matrix X.
for (i in 1:p) {
  X[, i] &lt;- stats::rnorm(N, mean = 0, sd = 1)
}

y &lt;- vector(mode = "numeric", length = N) # Response variable y.
e &lt;- rnorm(N, mean = 0, sd = 2) # error term e.
for (i in 1:10) {
  y &lt;- y + true_beta[i] * X[, i]
}
y &lt;- y + e

# Run with auto.threshold option
result1 &lt;- approx_horseshoe(X, y, burn = 0, iter = 100)

# Run with fixed custom threshold
result2 &lt;- approx_horseshoe(X, y, burn = 0, iter = 100,
                            auto.threshold = FALSE, threshold = 1/(5 * p))

# posterior mean
betahat &lt;- result1$BetaHat

# Lower bound of the 95% credible interval
leftCI &lt;- result1$LeftCI

# Upper bound of the 95% credible interval
RightCI &lt;- result1$RightCI

</code></pre>

<hr>
<h2 id='exact_horseshoe'>Run exact MCMC algorithm for horseshoe prior</h2><span id='topic+exact_horseshoe'></span>

<h3>Description</h3>

<p>The exact MCMC algorithm introduced in Section 2.1 of Johndrow et al. (2020)
was implemented in this function. This algorithm is the horseshoe estimator
that updates the global shrinkage parameter <code class="reqn">\tau</code> using
Metropolis-Hastings algorithm, and uses blocked-Gibbs sampling for
<code class="reqn">(\tau, \beta, \sigma)</code>. The local shrinkage parameter
<code class="reqn">\lambda_{j},\ j = 1,2,...,p</code> is updated by the rejection sampler.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>exact_horseshoe(
  X,
  y,
  burn = 1000,
  iter = 5000,
  a = 1/5,
  b = 10,
  s = 0.8,
  tau = 1,
  sigma2 = 1,
  w = 1,
  alpha = 0.05
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="exact_horseshoe_+3A_x">X</code></td>
<td>
<p>Design matrix, <code class="reqn">X \in \mathbb{R}^{N \times p}</code>.</p>
</td></tr>
<tr><td><code id="exact_horseshoe_+3A_y">y</code></td>
<td>
<p>Response vector, <code class="reqn">y \in \mathbb{R}^{N}</code>.</p>
</td></tr>
<tr><td><code id="exact_horseshoe_+3A_burn">burn</code></td>
<td>
<p>Number of burn-in samples. Default is 1000.</p>
</td></tr>
<tr><td><code id="exact_horseshoe_+3A_iter">iter</code></td>
<td>
<p>Number of samples to be drawn from the posterior. Default is
5000.</p>
</td></tr>
<tr><td><code id="exact_horseshoe_+3A_a">a</code></td>
<td>
<p>Parameter of the rejection sampler, and it is recommended to leave
it at the default value, <code class="reqn">a = 1/5</code>.</p>
</td></tr>
<tr><td><code id="exact_horseshoe_+3A_b">b</code></td>
<td>
<p>Parameter of the rejection sampler, and it is recommended to leave
it at the default value, <code class="reqn">b = 10</code>.</p>
</td></tr>
<tr><td><code id="exact_horseshoe_+3A_s">s</code></td>
<td>
<p><code class="reqn">s^{2}</code> is the variance of tau's MH proposal distribution.
0.8 is a good default. If set to 0, the algorithm proceeds by
fixing the global shrinkage parameter <code class="reqn">\tau</code> to the initial setting
value.</p>
</td></tr>
<tr><td><code id="exact_horseshoe_+3A_tau">tau</code></td>
<td>
<p>Initial value of the global shrinkage parameter <code class="reqn">\tau</code> when
starting the algorithm. Default is 1.</p>
</td></tr>
<tr><td><code id="exact_horseshoe_+3A_sigma2">sigma2</code></td>
<td>
<p>error variance <code class="reqn">\sigma^{2}</code>. Default is 1.</p>
</td></tr>
<tr><td><code id="exact_horseshoe_+3A_w">w</code></td>
<td>
<p>Parameter of gamma prior for <code class="reqn">\sigma^{2}</code>. Default is 1.</p>
</td></tr>
<tr><td><code id="exact_horseshoe_+3A_alpha">alpha</code></td>
<td>
<p><code class="reqn">100(1-\alpha)\%</code> credible interval setting argument.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See <code><a href="#topic+Mhorseshoe">Mhorseshoe</a></code> or browseVignettes(&quot;Mhorseshoe&quot;).
</p>


<h3>Value</h3>

<table>
<tr><td><code>BetaHat</code></td>
<td>
<p>Posterior mean of <code class="reqn">\beta</code>.</p>
</td></tr>
<tr><td><code>LeftCI</code></td>
<td>
<p>Lower bound of <code class="reqn">100(1-\alpha)\%</code> credible interval for
<code class="reqn">\beta</code>.</p>
</td></tr>
<tr><td><code>RightCI</code></td>
<td>
<p>Upper bound of <code class="reqn">100(1-\alpha)\%</code> credible interval for
<code class="reqn">\beta</code>.</p>
</td></tr>
<tr><td><code>Sigma2Hat</code></td>
<td>
<p>Posterior mean of <code class="reqn">\sigma^{2}</code>.</p>
</td></tr>
<tr><td><code>TauHat</code></td>
<td>
<p>Posterior mean of <code class="reqn">\tau</code>.</p>
</td></tr>
<tr><td><code>LambdaHat</code></td>
<td>
<p>Posterior mean of <code class="reqn">\lambda_{j},\ j=1,2,...p.</code>.</p>
</td></tr>
<tr><td><code>BetaSamples</code></td>
<td>
<p>Samples from the posterior of <code class="reqn">\beta</code>.</p>
</td></tr>
<tr><td><code>LambdaSamples</code></td>
<td>
<p>Lambda samples through rejection sampling.</p>
</td></tr>
<tr><td><code>TauSamples</code></td>
<td>
<p>Tau samples through MH algorithm.</p>
</td></tr>
<tr><td><code>Sigma2Samples</code></td>
<td>
<p>Samples from the posterior of the parameter
<code class="reqn">sigma^{2}</code>.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Johndrow, J., Orenstein, P., &amp; Bhattacharya, A. (2020). Scalable
Approximate MCMC Algorithms for the Horseshoe Prior. In Journal of Machine
Learning Research (Vol. 21).
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Making simulation data.
set.seed(123)
N &lt;- 50
p &lt;- 100
true_beta &lt;- c(rep(1, 10), rep(0, 90))

X &lt;- matrix(1, nrow = N, ncol = p) # Design matrix X.
for (i in 1:p) {
  X[, i] &lt;- stats::rnorm(N, mean = 0, sd = 1)
}

y &lt;- vector(mode = "numeric", length = N) # Response variable y.
e &lt;- rnorm(N, mean = 0, sd = 2) # error term e.
for (i in 1:10) {
  y &lt;- y + true_beta[i] * X[, i]
}
y &lt;- y + e

# Run
result &lt;- exact_horseshoe(X, y, burn = 0, iter = 100)

# posterior mean
betahat &lt;- result$BetaHat

# Lower bound of the 95% credible interval
leftCI &lt;- result$LeftCI

# Upper bound of the 95% credible interval
RightCI &lt;- result$RightCI

</code></pre>

<hr>
<h2 id='rejection_sampler'>rejection sampler to update local shrinkage parameters lambda.</h2><span id='topic+rejection_sampler'></span>

<h3>Description</h3>

<p>rejection sampler to update local shrinkage parameters lambda.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rejection_sampler(eps, a, b)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
