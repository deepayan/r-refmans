<!DOCTYPE html><html><head><title>Help for package chattr</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {chattr}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#ch_history'><p>Displays the current session' chat history</p></a></li>
<li><a href='#ch_submit'><p>Method to easily integrate to new LLM API's</p></a></li>
<li><a href='#chattr'><p>Submits prompt to LLM</p></a></li>
<li><a href='#chattr_app'><p>Starts a Shiny app interface to the LLM</p></a></li>
<li><a href='#chattr_defaults'><p>Default arguments to use when making requests to the LLM</p></a></li>
<li><a href='#chattr_defaults_save'><p>Saves the current defaults in a yaml file that is compatible with</p>
the config package</a></li>
<li><a href='#chattr_test'><p>Confirms connectivity to LLM interface</p></a></li>
<li><a href='#chattr_use'><p>Sets the LLM model to use in your session</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Interact with Large Language Models in 'RStudio'</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.0</td>
</tr>
<tr>
<td>Description:</td>
<td>Enables user interactivity with large-language models ('LLM') inside
    the 'RStudio' integrated development environment (IDE). The user can
    interact with the model using the 'shiny' app included in this package, or
    directly in the 'R' console. It comes with back-ends for 'OpenAI', 'GitHub'
    'Copilot', and 'LlamaGPT'. </td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/mlverse/chattr">https://github.com/mlverse/chattr</a>,
<a href="https://mlverse.github.io/chattr/">https://mlverse.github.io/chattr/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/mlverse/chattr/issues">https://github.com/mlverse/chattr/issues</a></td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.1</td>
</tr>
<tr>
<td>Imports:</td>
<td>rstudioapi, lifecycle, processx, jsonlite, config, httr2 (&ge;
1.0.1), purrr, rlang, bslib, shiny, clipr, callr, yaml, glue,
cli, fs</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10)</td>
</tr>
<tr>
<td>Suggests:</td>
<td>covr, knitr, rmarkdown, testthat (&ge; 3.0.0), shinytest2,
withr</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-04-25 19:01:27 UTC; edgar</td>
</tr>
<tr>
<td>Author:</td>
<td>Edgar Ruiz [aut, cre],
  Posit Software, PBC [cph, fnd]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Edgar Ruiz &lt;edgar@posit.co&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-04-27 18:10:12 UTC</td>
</tr>
</table>
<hr>
<h2 id='ch_history'>Displays the current session' chat history</h2><span id='topic+ch_history'></span>

<h3>Description</h3>

<p>Displays the current session' chat history
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ch_history(x = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ch_history_+3A_x">x</code></td>
<td>
<p>An list object that contains chat history. Use this argument to
override the current history.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list object with the current chat history
</p>

<hr>
<h2 id='ch_submit'>Method to easily integrate to new LLM API's</h2><span id='topic+ch_submit'></span>

<h3>Description</h3>

<p>Method to easily integrate to new LLM API's
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ch_submit(
  defaults,
  prompt = NULL,
  stream = NULL,
  prompt_build = TRUE,
  preview = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ch_submit_+3A_defaults">defaults</code></td>
<td>
<p>Defaults object, generally puled from <code>chattr_defaults()</code></p>
</td></tr>
<tr><td><code id="ch_submit_+3A_prompt">prompt</code></td>
<td>
<p>The prompt to send to the LLM</p>
</td></tr>
<tr><td><code id="ch_submit_+3A_stream">stream</code></td>
<td>
<p>To output the response from the LLM as it happens, or wait until
the response is complete. Defaults to TRUE.</p>
</td></tr>
<tr><td><code id="ch_submit_+3A_prompt_build">prompt_build</code></td>
<td>
<p>Include the context and additional prompt as part of the
request</p>
</td></tr>
<tr><td><code id="ch_submit_+3A_preview">preview</code></td>
<td>
<p>Primarily used for debugging. It indicates if it should send
the prompt to the LLM (FALSE), or if it should print out the resulting
prompt (TRUE)</p>
</td></tr>
<tr><td><code id="ch_submit_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Use this function to integrate your own LLM API. It has a few
requirements to get it to work properly:
</p>

<ul>
<li><p> The output of the function needs to be the parsed response from the LLM
</p>
</li>
<li><p> For those that support streaming, make sure to use the <code>cat()</code> function to
output the response of the LLM API as it is happening.
</p>
</li>
<li><p> If <code>preview</code> is set to TRUE, do not send to the LLM API. Simply return the
resulting prompt.
</p>
</li></ul>

<p>The <code>defaults</code> argument controls which method to use. You can use the
<code>chattr_defaults()</code> function, and set the provider. The <code>provider</code> value
is what creates the R class name. It will pre-pend <code>cl_</code> to the class name.
See the examples for more clarity.
</p>


<h3>Value</h3>

<p>The output from the model currently in use.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(chattr)
ch_submit.ch_my_llm &lt;- function(defaults,
                                prompt = NULL,
                                stream = NULL,
                                prompt_build = TRUE,
                                preview = FALSE,
                                ...) {
  # Use `prompt_build` to append the prompts you with to append
  if (prompt_build) prompt &lt;- paste0("Use the tidyverse\n", prompt)
  # If `preview` is true, return the resulting prompt back
  if (preview) {
    return(prompt)
  }
  llm_response &lt;- paste0("You said this: \n", prompt)
  if (stream) {
    cat("streaming:\n")
    for (i in seq_len(nchar(llm_response))) {
      # If `stream` is true, make sure to `cat()` the current output
      cat(substr(llm_response, i, i))
      Sys.sleep(0.1)
    }
  }
  # Make sure to return the entire output from the LLM at the end
  llm_response
}

chattr_defaults("console", provider = "my llm")
chattr("hello")
chattr("hello", stream = FALSE)
chattr("hello", prompt_build = FALSE)
chattr("hello", preview = TRUE)

## End(Not run)
</code></pre>

<hr>
<h2 id='chattr'>Submits prompt to LLM</h2><span id='topic+chattr'></span>

<h3>Description</h3>

<p>Submits prompt to LLM
</p>


<h3>Usage</h3>

<pre><code class='language-R'>chattr(prompt = NULL, preview = FALSE, prompt_build = TRUE, stream = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="chattr_+3A_prompt">prompt</code></td>
<td>
<p>Request to send to LLM. Defaults to NULL</p>
</td></tr>
<tr><td><code id="chattr_+3A_preview">preview</code></td>
<td>
<p>Primarily used for debugging. It indicates if it should send
the prompt to the LLM (FALSE), or if it should print out the resulting
prompt (TRUE)</p>
</td></tr>
<tr><td><code id="chattr_+3A_prompt_build">prompt_build</code></td>
<td>
<p>Include the context and additional prompt as part of the
request</p>
</td></tr>
<tr><td><code id="chattr_+3A_stream">stream</code></td>
<td>
<p>To output the response from the LLM as it happens, or wait until
the response is complete. Defaults to TRUE.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The output of the LLM to the console, document or script.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(chattr)
chattr_use("test")
chattr("hello")
chattr("hello", preview = TRUE)

</code></pre>

<hr>
<h2 id='chattr_app'>Starts a Shiny app interface to the LLM</h2><span id='topic+chattr_app'></span>

<h3>Description</h3>

<p>Starts a Shiny app interface to the LLM
</p>


<h3>Usage</h3>

<pre><code class='language-R'>chattr_app(
  viewer = c("viewer", "dialog"),
  as_job = getOption("chattr.as_job", FALSE),
  as_job_port = getOption("shiny.port", 7788),
  as_job_host = getOption("shiny.host", "127.0.0.1")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="chattr_app_+3A_viewer">viewer</code></td>
<td>
<p>Specifies where the Shiny app is going to display</p>
</td></tr>
<tr><td><code id="chattr_app_+3A_as_job">as_job</code></td>
<td>
<p>App runs as an RStudio IDE Job. Defaults to FALSE. If set to
TRUE, the Shiny app will not be able to transfer the code blocks directly to
the document, or console, in the IDE.</p>
</td></tr>
<tr><td><code id="chattr_app_+3A_as_job_port">as_job_port</code></td>
<td>
<p>Port to use for the Shiny app. Applicable only if <code>as_job</code>
is set to TRUE.</p>
</td></tr>
<tr><td><code id="chattr_app_+3A_as_job_host">as_job_host</code></td>
<td>
<p>Host IP to use for the Shiny app. Applicable only if
<code>as_job</code> is set to TRUE.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A chat interface inside the 'RStudio' IDE
</p>

<hr>
<h2 id='chattr_defaults'>Default arguments to use when making requests to the LLM</h2><span id='topic+chattr_defaults'></span>

<h3>Description</h3>

<p>Default arguments to use when making requests to the LLM
</p>


<h3>Usage</h3>

<pre><code class='language-R'>chattr_defaults(
  type = "default",
  prompt = NULL,
  max_data_files = NULL,
  max_data_frames = NULL,
  include_doc_contents = NULL,
  include_history = NULL,
  provider = NULL,
  path = NULL,
  model = NULL,
  model_arguments = NULL,
  system_msg = NULL,
  yaml_file = "chattr.yml",
  force = FALSE,
  label = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="chattr_defaults_+3A_type">type</code></td>
<td>
<p>Entry point to interact with the model. Accepted values:
'notebook', chat'</p>
</td></tr>
<tr><td><code id="chattr_defaults_+3A_prompt">prompt</code></td>
<td>
<p>Request to send to LLM. Defaults to NULL</p>
</td></tr>
<tr><td><code id="chattr_defaults_+3A_max_data_files">max_data_files</code></td>
<td>
<p>Sets the maximum number of data files to send to the
model. It defaults to 20. To send all, set to NULL</p>
</td></tr>
<tr><td><code id="chattr_defaults_+3A_max_data_frames">max_data_frames</code></td>
<td>
<p>Sets the maximum number of data frames loaded in the
current R session to send to the model. It defaults to 20. To send all,
set to NULL</p>
</td></tr>
<tr><td><code id="chattr_defaults_+3A_include_doc_contents">include_doc_contents</code></td>
<td>
<p>Send the current code in the document</p>
</td></tr>
<tr><td><code id="chattr_defaults_+3A_include_history">include_history</code></td>
<td>
<p>Indicates whether to include the chat history when
every time a new prompt is submitted</p>
</td></tr>
<tr><td><code id="chattr_defaults_+3A_provider">provider</code></td>
<td>
<p>The name of the provider of the LLM. Today, only &quot;openai&quot; is
is available</p>
</td></tr>
<tr><td><code id="chattr_defaults_+3A_path">path</code></td>
<td>
<p>The location of the model. It could be an URL or a file path.</p>
</td></tr>
<tr><td><code id="chattr_defaults_+3A_model">model</code></td>
<td>
<p>The name or path to the model to use.</p>
</td></tr>
<tr><td><code id="chattr_defaults_+3A_model_arguments">model_arguments</code></td>
<td>
<p>Additional arguments to pass to the model as part of
the request, it requires a list. Examples of arguments: temperature, top_p,
max_tokens</p>
</td></tr>
<tr><td><code id="chattr_defaults_+3A_system_msg">system_msg</code></td>
<td>
<p>For OpenAI GPT 3.5 or above, the system message to send as
part of the request</p>
</td></tr>
<tr><td><code id="chattr_defaults_+3A_yaml_file">yaml_file</code></td>
<td>
<p>The path to a valid <code>config</code> YAML file that contains the
defaults to use in a session</p>
</td></tr>
<tr><td><code id="chattr_defaults_+3A_force">force</code></td>
<td>
<p>Re-process the base and any work space level file defaults</p>
</td></tr>
<tr><td><code id="chattr_defaults_+3A_label">label</code></td>
<td>
<p>Label to display in the Shiny app, and other locations</p>
</td></tr>
<tr><td><code id="chattr_defaults_+3A_...">...</code></td>
<td>
<p>Additional model arguments that are not standard for all
models/backends</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The idea is that because we will use addin shortcut to execute the
request, all of the other arguments can be controlled via this function. By
default, it will try to load defaults from a <code>config</code> YAML file, if none are
found, then the defaults for GPT 3.5 will be used. The defaults can be
modified by calling this function, even after the interactive session has
started.
</p>


<h3>Value</h3>

<p>An 'ch_model' object that contains the current defaults that will be
used to communicate with the LLM.
</p>

<hr>
<h2 id='chattr_defaults_save'>Saves the current defaults in a yaml file that is compatible with
the config package</h2><span id='topic+chattr_defaults_save'></span>

<h3>Description</h3>

<p>Saves the current defaults in a yaml file that is compatible with
the config package
</p>


<h3>Usage</h3>

<pre><code class='language-R'>chattr_defaults_save(path = "chattr.yml", overwrite = FALSE, type = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="chattr_defaults_save_+3A_path">path</code></td>
<td>
<p>Path to the file to save the configuration to</p>
</td></tr>
<tr><td><code id="chattr_defaults_save_+3A_overwrite">overwrite</code></td>
<td>
<p>Indicates to replace the file if it exists</p>
</td></tr>
<tr><td><code id="chattr_defaults_save_+3A_type">type</code></td>
<td>
<p>The type of UI to save the defaults for. It defaults to NULL
which will save whatever types had been used during the current R session</p>
</td></tr>
</table>


<h3>Value</h3>

<p>It creates a YAML file with the defaults set in the current R
session.
</p>

<hr>
<h2 id='chattr_test'>Confirms connectivity to LLM interface</h2><span id='topic+chattr_test'></span><span id='topic+ch_test'></span>

<h3>Description</h3>

<p>Confirms connectivity to LLM interface
</p>


<h3>Usage</h3>

<pre><code class='language-R'>chattr_test(defaults = NULL)

ch_test(defaults = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="chattr_test_+3A_defaults">defaults</code></td>
<td>
<p>Defaults object, generally puled from <code>chattr_defaults()</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>It returns console massages with the status of the test.
</p>

<hr>
<h2 id='chattr_use'>Sets the LLM model to use in your session</h2><span id='topic+chattr_use'></span>

<h3>Description</h3>

<p>Sets the LLM model to use in your session
</p>


<h3>Usage</h3>

<pre><code class='language-R'>chattr_use(x = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="chattr_use_+3A_x">x</code></td>
<td>
<p>The label of the LLM model to use, or the path of a valid YAML
default file . Valid values are 'copilot', 'gpt4', 'gpt35', and 'llamagpt'.
The value 'test' is also acceptable, but it is meant for package examples,
and internal testing.</p>
</td></tr>
<tr><td><code id="chattr_use_+3A_...">...</code></td>
<td>
<p>Default values to modify.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If the error &quot;No model setup found&quot; was returned, that is because none of the
expected setup for Copilot, OpenAI or LLama was automatically detected. Here
is how to setup a model:
</p>

<ul>
<li><p> OpenAI - The main thing <code>chattr</code> checks is the presence of the R user's
OpenAI PAT (Personal Access Token). It looks for it in the 'OPENAI_API_KEY'
environment variable. Get a PAT from the OpenAI website, and save it to that
environment variable. Then restart R, and try again.
</p>
</li>
<li><p> GitHub Copilot - Setup GitHub Copilot in your RStudio IDE, and restart
R. <code>chattr</code> will look for the default location where RStudio saves the
Copilot authentication information.
</p>
</li></ul>

<p>Use the 'CHATTR_MODEL' environment variable to set it for the
R session, or create a YAML file named 'chattr.yml' in your working directory
to control the model, and the defaults it will use to communicate with such
model.
</p>


<h3>Value</h3>

<p>It returns console messages to allow the user select the model to
use.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
