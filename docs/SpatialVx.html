<!DOCTYPE html><html><head><title>Help for package SpatialVx</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {SpatialVx}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#SpatialVx-package'>
<p>Spatial Forecast Verification</p></a></li>
<li><a href='#abserrloss'>
<p>Loss functions for the spatial prediction comparison test (SPCT)</p></a></li>
<li><a href='#Aindex'>
<p>Area Index</p></a></li>
<li><a href='#bearing'>
<p>Bearing from One Spatial Location to Another</p></a></li>
<li><a href='#binarizer'>
<p>Create Binary Fields</p></a></li>
<li><a href='#calculate_dFSS'>
<p>Calculate dFSS Score Value</p></a></li>
<li><a href='#calculate_FSSvector_from_binary_fields'>
<p>Calculate FSS Values</p></a></li>
<li><a href='#calculate_FSSwind'>
<p>Calculate FSSwind Score Value</p></a></li>
<li><a href='#censqdelta'>
<p>Centered on Square Domain Baddeley's Delta Metric</p></a></li>
<li><a href='#centdist'>
<p>Centroid Distance Between Two Identified Objects</p></a></li>
<li><a href='#Cindex'>
<p>Connectivity Index</p></a></li>
<li><a href='#CircleHistogram'>
<p>Circular Histogram</p></a></li>
<li><a href='#clusterer'>
<p>Cluster Analysis Verification</p></a></li>
<li><a href='#combiner'>
<p>Combine Features/Matched Objects</p></a></li>
<li><a href='#compositer'>
<p>Create Composite Features</p></a></li>
<li><a href='#CSIsamples'>
<p>Forecast Verification with Cluster Analysis: The Variation</p></a></li>
<li><a href='#deltamm'>
<p>Merge and/or Match Identified Features Within Two Fields</p></a></li>
<li><a href='#disjointer'>
<p>Identify Disjoint Sets of Connected Components</p></a></li>
<li><a href='#EBS'>
<p>Elmore, Baldwin and Schultz Method for Field Significance for Spatial Bias Errors</p></a></li>
<li><a href='#ExampleSpatialVxSet'>
<p>Simulated Spatial Verification Set</p></a></li>
<li><a href='#expvg'>
<p>Exponential Variogram</p></a></li>
<li><a href='#expvgram'>
<p>Exponential Variogram</p></a></li>
<li><a href='#FeatureAxis'>
<p>Major and Minor Axes of a Feature</p></a></li>
<li><a href='#FeatureFinder'>
<p>Threshold-based Feature Finder</p></a></li>
<li><a href='#FeatureMatchAnalyzer'>
<p>Analyze Features of a Verification Set</p></a></li>
<li><a href='#FeatureProps'>
<p>Single Feature Properties</p></a></li>
<li><a href='#FeatureTable'>
<p>Feature-Based Contingency Table</p></a></li>
<li><a href='#Fint2d'>
<p>2-d Interpolation</p></a></li>
<li><a href='#FQI'>
<p>Forecast Quality Index</p></a></li>
<li><a href='#fss2dfun'><p>Various Verification Statistics on Possibly Neighborhood-Smoothed Fields.</p></a></li>
<li><a href='#fss2dPlot'>
<p>Create Several Graphics for List Objects Returned from hoods2d</p></a></li>
<li><a href='#Gbeta'>
<p>Spatial-Alignment Summary Measures</p></a></li>
<li><a href='#GeoBoxPlot'>
<p>Geographic Box Plot</p></a></li>
<li><a href='#GFSNAMfcstEx'>
<p>Example Verification Set</p></a></li>
<li><a href='#gmm2d'>
<p>2-d Gaussian Mixture Models Verification</p></a></li>
<li><a href='#griddedVgram'>
<p>Variograms for a Gridded Verification Set</p></a></li>
<li><a href='#hiw'>
<p>Spatial Forecast Verification Shape Analysis</p></a></li>
<li><a href='#hoods2d'>
<p>Neighborhood Verification Statistics for a Gridded Verification Set.</p></a></li>
<li><a href='#hoods2dPlot'><p>Quilt Plot and a Matrix Plot.</p></a></li>
<li><a href='#hump'>
<p>Simulated Forecast and Verification Fields</p></a></li>
<li><a href='#imomenter'>
<p>Image Moments</p></a></li>
<li><a href='#interester'>
<p>Feature Interest</p></a></li>
<li><a href='#iwarper'>
<p>Image Warping By Hand</p></a></li>
<li><a href='#locmeasures2d'>
<p>Binary Image Measures</p></a></li>
<li><a href='#locperf'>
<p>Localization Performance Measures</p></a></li>
<li><a href='#LocSig'>
<p>Temporal Block Bootstrap Keeping Locations in Space Constant</p></a></li>
<li><a href='#lossdiff'>
<p>Test for Equal Predictive Ability on Average Over a Regularly Gridded Space</p></a></li>
<li><a href='#make.SpatialVx'>
<p>Spatial Verification Sets &ndash; SpatialVx Object</p></a></li>
<li><a href='#MCdof'>
<p>Monte Carlo Degrees of Freedom</p></a></li>
<li><a href='#MergeForce'>
<p>Force Merges in Matched Feature Objects</p></a></li>
<li><a href='#metrV'>
<p>Binary Location Metric Proposed in Zhu et al. (2011)</p></a></li>
<li><a href='#Mij'>
<p>Raw Image Moments.</p></a></li>
<li><a href='#minboundmatch'>
<p>Minimum Boundary Separation Feature Matching</p></a></li>
<li><a href='#obs0601'>
<p>Spatial Forecast Verification Methods Inter-Comparison Project (ICP) Test Cases and other example verification sets</p></a></li>
<li><a href='#OF'>
<p>Optical Flow Verification</p></a></li>
<li><a href='#optflow'>
<p>Optical Flow</p></a></li>
<li><a href='#pphindcast2d'>
<p>Practically Perfect Hindcast Neighborhood Verification Method</p></a></li>
<li><a href='#rigider'>
<p>Rigid Transformation</p>
</p></a></li>
<li><a href='#S1'>
<p>S1 Score, Anomaly Correlation</p></a></li>
<li><a href='#saller'>
<p>Feature-based Analysis of a Field (Image)</p></a></li>
<li><a href='#Sindex'>
<p>Shape Index</p></a></li>
<li><a href='#spatbiasFS'>
<p>Field Significance Method of Elmore et al. (2006)</p></a></li>
<li><a href='#SpatialVx-internal'>
<p>Internal functions for the SpatialVx package</p></a></li>
<li><a href='#spct'>
<p>Spatial Prediction Comparison Test</p></a></li>
<li><a href='#structurogram'>
<p>Structure Function for Non-Gridded Spatial Fields.</p></a></li>
<li><a href='#structurogram.matrix'>
<p>Structure Function for Gridded Fields</p></a></li>
<li><a href='#surrogater2d'>
<p>Create Surrogate Fields</p></a></li>
<li><a href='#TheBigG'>
<p>The Spatial Alignment Summary Measure Called G</p></a></li>
<li><a href='#thresholder'>
<p>Apply a Threshold to a Field</p></a></li>
<li><a href='#UKobs6'>
<p>Example Precipitation Rate Verification Set (NIMROD)</p></a></li>
<li><a href='#upscale2d'>
<p>Upscaling Neighborhood Verification on a 2-d Verification Set</p></a></li>
<li><a href='#variographier'>
<p>Variography Score</p></a></li>
<li><a href='#vxstats'>
<p>Some Common Traditional Forecast Verification Statistics.</p></a></li>
<li><a href='#warper'>
<p>Image Warp</p></a></li>
<li><a href='#waveIS'>
<p>Intensity Scale (IS) Verification</p></a></li>
<li><a href='#wavePurifyVx'>
<p>Apply Traditional Forecast Verification After Wavelet Denoising</p></a></li>
<li><a href='#waverify2d'>
<p>High-Resolution Gridded Forecast Verification Using Discrete Wavelet Decomposition</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Version:</td>
<td>1.0-2</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-02-01</td>
</tr>
<tr>
<td>Title:</td>
<td>Spatial Forecast Verification</td>
</tr>
<tr>
<td>Author:</td>
<td>Eric Gilleland [aut, cre],
  Kim Elmore [ctb],
  Caren Marzban [ctb],
  Matt Pocernich [ctb],
  Gregor Skok [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Eric Gilleland &lt;EricG@ucar.edu&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.1.0), spatstat (&ge; 2.0-0), fields (&ge; 6.8), smoothie,
smatr, turboEM</td>
</tr>
<tr>
<td>Imports:</td>
<td>spatstat.geom, spatstat.linnet, spatstat.model, distillery,
maps, methods, boot, CircStats, fastcluster, waveslim</td>
</tr>
<tr>
<td>Description:</td>
<td>Spatial forecast verification refers to verifying weather forecasts when the verification set (forecast and observations) is on a spatial field, usually a high-resolution gridded spatial field.  Most of the functions here require the forecast and observed fields to be gridded and on the same grid. For a thorough review of most of the methods in this package, please see Gilleland et al. (2009) &lt;<a href="https://doi.org/10.1175%2F2009WAF2222269.1">doi:10.1175/2009WAF2222269.1</a>&gt; and for a tutorial on some of the main functions available here, see Gilleland (2022) &lt;<a href="https://doi.org/10.5065%2F4px3-5a05">doi:10.5065/4px3-5a05</a>&gt;. </td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://projects.ral.ucar.edu/icp/SpatialVx/">https://projects.ral.ucar.edu/icp/SpatialVx/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://staff.ral.ucar.edu/ericg/">https://staff.ral.ucar.edu/ericg/</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-02-01 20:20:29 UTC; gille</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-02-01 21:20:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='SpatialVx-package'>
Spatial Forecast Verification
</h2><span id='topic+SpatialVx-package'></span><span id='topic+SpatialVx'></span>

<h3>Description</h3>

<p><span class="pkg">SpatialVx</span> contains functions to perform many spatial forecast verification methods.
</p>


<h3>Details</h3>

<p>Primary functions include:
</p>
<p>0. <code>make.SpatialVx</code>: An object that contains the verification sets and pertinent information.
</p>
<p>1. Filter Methods:
</p>
<p>1a. Neighborhood Methods:
</p>
<p>Neighborhood methods generally apply a convolution kernel smoother to one or both of the fields in the verificaiton set, and then apply the traditional scores.  Most of the methods reviewed in Ebert (2008, 2009) are included in this package.  The main functions are:
</p>
<p><code>hoods2d</code>, <code>pphindcast2d</code>, <code>kernel2dsmooth</code>, and <code>plot.hoods2d</code>.
</p>
<p>1b. Scale Separation Methods:
</p>
<p>Scale separation refers to the idea of applying a band-pass filter (and/or doing a multi-resolution analysis, MRA) to the verification set.  Typically, skill is assessed on a scale-by-scale basis.  However, other techniques are also applied.  For example, denoising the field before applying traditional statistics, using the variogram, or applying a statistical test based on the variogram (these last are less similar to the spirit of the scale separation idea, but are at least somewhat related).
</p>
<p>There is functionality to do the wavelet methods proposed in Briggs and Levine (1997).  In particular, to simply denoise the fields before applying traditional verification statistics, use 
</p>
<p><code>wavePurifyVx</code>.
</p>
<p>To apply verification statistics to detail fields (in either the wavelet or field space), use:
</p>
<p><code>waverify2d</code> (dyadic fields) or <code>mowaverify2d</code> (non-dyadic or dyadic) fields.
</p>
<p>The intensity-scale technique introduced in Casati et al. (2004) and the new developments of the technique proposed in Casati (2010) can be performed with
</p>
<p><code>waveIS</code>.
</p>
<p>Although not strictly a &ldquo;scale separation&rdquo; method, the structure function (for which the variogram is a special case) is in the same spirit in the sense that it analyzes the field for different separation distances, and these &ldquo;scales&rdquo; are separate from each other (i.e., the score does not necessarily improve or decline as the scale increases).  This package contains essentially wrapper functions to the <code>vgram.matrix</code> and <code>plot.vgram.matrix</code> functions from the fields package, but there is also a function called
</p>
<p><code>variogram.matrix</code>
</p>
<p>that is a modification of <code>vgram.matrix</code> that allows for missing values.  The primary function for doing this is called
</p>
<p><code>griddedVgram</code>, which has a <code>plot</code> method function associated with it.
</p>
<p>There are also slight modifications of these functions (small modifications of the fields functions) to calculate the structure function of Harris et al. (2001).  These functions are called
</p>
<p><code>structurogram</code> (for non-gridded fields) and <code>structurogram.matrix</code> (for gridded fields).
</p>
<p>The latter allows for ignoring zero-valued grid points (as detailed in Harris et al., 2001) where the former does not (they must be removed prior to calling the function).
</p>
<p>2. Displacement Methods:  
</p>
<p>In Gilleland et al (2009), this category was broken into two main types as field deformation and features-based.  The former lumped together binary image measures/metrics with field deformation techniques because the binary image measures inform about the &ldquo;similarity&rdquo; (or dissimilarity) between the spatial extent or pattern of two fields (across the entire field).  Here, they are broken down further into those that yield only a single (or small vector of) metric(s) or measure(s) (location measures), and those that have mechanisms for moving grid-point locations to match the fields better spatially (field deformation).
</p>
<p>2a. Distance-based and Spatial-Alignment Summary Measures: 
</p>
<p>Gilleland (2020) introduced a new spatial alignment summary measure that falls between zero and one, with one representing a perfect match and zero a bad match.  There is one user-selectable parameter/argument that determines the rate of decrease of the measure towards zero.  Another two summary measures also incorporate intensity information.  These summaries are available via <code>Gbeta</code>, <code>GbetaIL</code> and <code>G2IL</code>.
</p>
<p>In addition to the above new measures, older well-known measures are inlcuded, including: the Hausdorff metric, partial-Hausdorff measure, FQI (Venugopal et al., 2005), Baddeley's delta metric (Baddeley, 1992; Gilleland, 2011; Schwedler et al., 2011), metrV (Zhu et al., 2011), as well as the localization performance measures described in Baddeley, 1992: mean error distance, mean square error distance, and Pratt's Figure of Merit (FOM).
</p>
<p><code>locmeasures2d</code>, <code>metrV</code>, <code>distob</code>, <code>locperf</code>
</p>
<p>Image moments can give useful information about location errors, and are used within feature-based methods, particularly MODE, as they give the centroid of an image (or feature), as well as the orientation angle, among other useful properties.  See the <code>imomenter</code> function for more details.
</p>
<p>2b. Field deformation:
</p>
<p>Thanks to Caren Marzban for supplying his optical flow code for this package (it has been modified some).  These functions perform the analyses described in Marzban and Sandgathe (2010) and are based on the work of Lucas and Kanade (1981).  See the help file for
</p>
<p><code>OF</code>.
</p>
<p>Rigid transformations can be estimated using the <code>rigider</code> function.  To simply rigidly transform a field (or feature) using specified parameters (x- and y- translations and/or rotations), the <code>rigidTransform</code> function can be used.  For these functions, which may result in transformations that do not perfectly fall onto grid points, the function <code>Fint2d</code> can be used to interpolate from nearest grid points.  Interpolation options include &ldquo;round&rdquo; (simply take the nearest location value), &ldquo;bilinear&rdquo; and &ldquo;bicubic&rdquo;.
</p>
<p>2c. Features-based methods: These methods are also sometimes called object-based methods (the term &ldquo;features&rdquo; is used in this package in order to differentiate from an R object), and have many similarities to techniques used in Object-Based Image Analysis (OBIA), a relatively new research area that has emerged primarily as a result of advances in earth observations sensors and GIScience (Blaschke et al., 2008).  It is attempted to identify individual features within a field, and subsequently analyze the fields on a feature-by-feature basis.  This may involve intensity error information in addition to location-specific error information.  Additionally, contingency table verifcation statistics can be found using new definitions for hits, misses and false alarms (correct negatives are more difficult to asses, but can also be done).
</p>
<p>Currently, there is functionality for performing the analyses introduced in Davis et al. (2006,2009), including the merge/match algorithm of Gilleland et al (2008), as well as the SAL technique of Wernli et al (2008, 2009).  Some functionality for composite analysis (Nachamkin, 2004) is provided by way of placing individual features onto a relative grid so that each shares the same centroid.  Shape analysis is partially supported by way of functions to identify boundary points (Micheas et al. 2007; Lack et al. 2010).  In particular, see:
</p>
<p>Functions to identify features: <code>FeatureFinder</code>
</p>
<p>Functions to match/merge features: <code>centmatch</code>, <code>deltamm</code>, <code>minboundmatch</code>
</p>
<p>Functions to diagnose features and/or compare matched features:
</p>
<p><code>FeatureAxis</code>, <code>FeatureComps</code>, <code>FeatureMatchAnalyzer</code>, <code>FeatureProps</code>, <code>FeatureTable</code>, <code>interester</code>
</p>
<p>See <code>compositer</code> for setting up composited objects, and see <code>hiw</code> (along with <code>distill</code> and <code>summary</code> method functions) for some shape analysis functionality.
</p>
<p>The cluster analysis methods of Marzban and Sandgathe (2006; 2008) have been added.  The former method was written from scratch by Eric Gilleland 
</p>
<p><code>clusterer</code>
</p>
<p>and the latter variation was modified from code originally written by Hilary Lyons 
</p>
<p><code>CSIsamples</code>.
</p>
<p>The Structure, Amplitude and Location (SAL) method can be performed with <code>saller</code>.
</p>
<p>2d. Geometrical characterization measures:
</p>
<p>Perhaps the measures in this sub-heading are best described as part-and-parcel of 2c.  They are certainly useful in that domain, but have been proposed also for entire fields by AghaKouchak et al. (2011); though similar measures have been applied in, e.g., MODE.  The measures introduced in AghaKouchak et al. (2011) available here are: connectivity index (Cindex), shape index (Sindex), and area index (Aindex):
</p>
<p><code>Cindex</code>, <code>Sindex</code>, <code>Aindex</code>
</p>
<p>3. Statistical inferences for spatial (and/or spatiotemporal) fields:
</p>
<p>In addition to the methods categorized in Gilleland et al. (2009), there are also functions for making comparisons between two spatial fields.  The field significance approach detailed in Elmore et al. (2006), which requires a temporal dimension as well, involves using a circular block bootstrap (CBB) algorithm (usually for the mean error) at each grid point (or location) individually to determine grid-point significance (null hypothesis that the mean error is zero), and then a semi-parametric Monte Carlo method viz. Livezey and Chen (1983) to determine field significance.
</p>
<p><code>spatbiasFS</code>, <code>LocSig</code>, <code>MCdof</code>
</p>
<p>In addition, the spatial prediction comparison test (SPCT) introduced by Hering and Genton (2011) is included via the functions: <code>lossdiff</code>, <code>empiricalVG</code> and <code>flossdiff</code>.  Supporting functions for calculating the loss functions include: absolute error (<code>abserrloss</code>), square error (<code>sqerrloss</code>) and correlation skill (<code>corrskill</code>), as well as the distance map loss function (<code>distmaploss</code>) introduced in Gilleland (2013).
</p>
<p>4. Other:
</p>
<p>The bias corrected TS and ETS (or TS dHdA and ETS dHdA) introduced in Mesinger (2008) are now included within the <code>vxstats</code> function.
</p>
<p>The 2-d Gaussian Mixture Model (GMM) approach introduced in Lakshmanan and Kain (2010) can be carried out using the
</p>
<p><code>gmm2d</code>
</p>
<p>function (to estimate the GMM) and the associated <code>summary</code> function calculates the parameter comparisons.  Also available are <code>plot</code> and <code>predict</code> method functions, but it can be very slow to run.  The <code>gmm2d</code> employs an initialization function that takes the K largest object areas (connected components) and uses their centroids as initial estimates for the means, and uses the axes as initial guesses for the standard deviations.  However, the user may supply their own initial estimate function.
</p>
<p>The S1 score and anomaly correlation (ACC) are available through the functions
</p>
<p><code>S1</code> and <code>ACC</code>.
</p>
<p>See Brown et al. (2012) and Thompson and Carter (1972) for more on these statistics.
</p>
<p>Also included is a function to do the geographic box-plot of Willmott et al. (2007).  Namely,
</p>
<p><code>GeoBoxPlot</code>.
</p>
<p>Datasets:
</p>
<p>All of the initial Spatial Forecast Verification Inter-Comparison Project (ICP, <a href="https://projects.ral.ucar.edu/icp/">https://projects.ral.ucar.edu/icp/</a>) data sets used in the special collection of the Weather and Forecasting journal are included.  See the help file for 
</p>
<p><code>obs0426</code>,
</p>
<p>which gives information on all of these datasets that are included, as well as two examples for plotting them: one that does not preserve projections, but plots the data without modification, and another that preserves the projections, but possibly with some interpolative smoothing.
</p>
<p>Ebert (2008) provides a nice review of these methods.  Roberts and Lean (2008) describes one of the methods, as well as the primary boxcar kernel smoothing method used throughout this package.  Gilleland et al. (2009, 2010) provides an overview of most of the various recently proposed methods, and Ahijevych et al. (2009) describes the data sets included in this package.  Some of these have been applied to the ICP test cases in Ebert (2009).
</p>
<p>Additionally, one of the NIMROD cases (as provided by the UK Met Office) analyzed in Casati et al (2004) (case 6) is included along with approximate lon/lat locations.  See the help file for UKobs6 more information.
</p>
<p>A spatio-temporal verification dataset is also included for testing the method of Elmore et al. (2006).  See the help file for
</p>
<p><code>GFSNAMfcstEx</code>.
</p>
<p>A simulated dataset similar to the one used in Marzban and Sandgathe (2010) is also available and is called 
</p>
<p><code>hump</code>.
</p>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>References</h3>

<p>AghaKouchak, A., Nasrollahi, N. Li, J. Imam, B. and Sorooshian, S. (2011) Geometrical characterization of precipitation patterns.  <em>J. Hydrometeorology</em>, <b>12</b>, 274&ndash;285, doi:10.1175/2010JHM1298.1.
</p>
<p>Ahijevych, D., Gilleland, E., Brown, B. G. and Ebert, E. E. (2009) Application of spatial verification methods to idealized and NWP gridded precipitation forecasts. <em>Wea. Forecasting</em>, <b>24</b> (6), 1485&ndash;1497.
</p>
<p>Baddeley, A. J. (1992) An error metric for binary images.  In <em>Robust Computer Vision Algorithms</em>, Forstner, W. and Ruwiedel, S. Eds., Wichmann, 59&ndash;78.
</p>
<p>Blaschke, T., Lang, S. and  Hay, G. (Eds.) (2008)  Object-Based Image Analysis.  Berlin, Germany: Springer-Verlag, 818 pp.
</p>
<p>Briggs, W. M. and Levine, R. A. (1997) Wavelets and field forecast verification. <em>Mon. Wea. Rev.</em>, <b>125</b>, 1329&ndash;1341.
</p>
<p>Brown, B. G., Gilleland, E. and Ebert, E. E. (2012) Chapter 6: Forecasts of spatial fields. pp. 95&ndash;117, In <em>Forecast Verification: A Practitioner's Guide in Atmospheric Science</em>, 2nd edition. Edts. Jolliffe, I. T. and Stephenson, D. B., Chichester, West Sussex, U.K.: Wiley, 274 pp.
</p>
<p>Casati, B. (2010) New Developments of the Intensity-Scale Technique within the Spatial Verification Methods Inter-Comparison Project. <em>Wea. Forecasting</em> <b>25</b>, (1), 113&ndash;143, doi:10.1175/2009WAF2222257.1.
</p>
<p>Casati, B., Ross, G. and Stephenson, D. B. (2004) A new intensity-scale approach for the verification of spatial precipitation forecasts. <em>Meteorol. Appl.</em> <b>11</b>, 141&ndash;154.
</p>
<p>Davis, C. A., Brown, B. G. and Bullock, R. G. (2006) Object-based verification of precipitation forecasts, Part I: Methodology and application to mesoscale rain areas. <em>Mon. Wea. Rev.</em>, <b>134</b>, 1772&ndash;1784.
</p>
<p>Ebert, E. E. (2008) Fuzzy verification of high resolution gridded forecasts: A review and proposed framework.  <em>Meteorol. Appl.</em>, <b>15</b>, 51&ndash;64. DOI: 10.1002/met.25 
</p>
<p>Ebert, E. E. (2009) Neighborhood verification: A strategy for rewarding close forecasts.  <em>Wea. Forecasting</em>, <b>24</b>, 1498&ndash;1510, doi:10.1175/2009WAF2222251.1.
</p>
<p>Elmore, K. L., Baldwin, M. E. and Schultz, D. M. (2006) Field significance revisited: Spatial bias errors in forecasts as applied to the Eta model.  <em>Mon. Wea. Rev.</em>, <b>134</b>, 519&ndash;531.
</p>
<p>Gilleland, E., 2020. Novel measures for summarizing high-resolution forecast performance. Submitted to <em>Advances in Statistical Climatology, Meteorology and Oceanography</em> on 19 July 2020.
</p>
<p>Gilleland, E. (2013) Testing competing precipitation forecasts accurately and efficiently: The spatial prediction comparison test.  <em>Mon. Wea. Rev.</em>, <b>141</b>, (1), 340&ndash;355.
</p>
<p>Gilleland, E. (2011)  Spatial forecast verification: Baddeley's delta metric applied to the ICP test cases.  <em>Wea. Forecasting</em>, <b>26</b>, 409&ndash;415, doi:10.1175/WAF-D-10-05061.1.
</p>
<p>Gilleland, E., Lee, T. C. M., Halley Gotway, J., Bullock, R. G. and Brown, B. G. (2008) Computationally efficient spatial forecast verification using Baddeley's delta image metric.  <em>Mon. Wea. Rev.</em>, <b>136</b>, 1747&ndash;1757.
</p>
<p>Gilleland, E., Ahijevych, D., Brown, B. G., Casati, B. and Ebert, E. E. (2009) Intercomparison of Spatial Forecast Verification Methods. <em>Wea. Forecasting</em>, <b>24</b>, 1416&ndash;1430, doi:10.1175/2009WAF2222269.1.
</p>
<p>Gilleland, E., Ahijevych, D. A., Brown, B. G. and Ebert, E. E. (2010) Verifying Forecasts Spatially. <em>Bull. Amer. Meteor. Soc.</em>, October, 1365&ndash;1373.
</p>
<p>Harris, D., Foufoula-Georgiou, E., Droegemeier, K. K. and Levit, J. J. (2001)  Multiscale statistical properties of a high-resolution precipitation forecast.  <em>J. Hydrometeorol.</em>, <b>2</b>, 406&ndash;418.
</p>
<p>Hering, A. S. and Genton, M. G. (2011) Comparing spatial predictions.  <em>Technometrics</em> <b>53</b>, (4), 414&ndash;425.
</p>
<p>Lack, S., Limpert, G. L. and Fox, N. I. (2010) An object-oriented multiscale verification scheme. <em>Wea. Forecasting</em>, <b>25</b>, 79&ndash;92, DOI: 10.1175/2009WAF2222245.1
</p>
<p>Lakshmanan, V. and Kain, J. S. (2010) A Gaussian Mixture Model Approach to Forecast Verification. <em>Wea. Forecasting</em>, <b>25</b> (3), 908&ndash;920.
</p>
<p>Livezey, R. E. and Chen, W. Y. (1983) Statistical field significance and its determination by Monte Carlo techniques.  <em>Mon. Wea. Rev.</em>, <b>111</b>, 46&ndash;59.
</p>
<p>Lucas, B D. and Kanade, T. (1981)  An iterative image registration technique with an application to stereo vision.  <em>Proc. Imaging Understanding Workshop</em>, DARPA, 121&ndash;130.
</p>
<p>Marzban, C. and Sandgathe, S. (2006) Cluster analysis for verification of precipitation fields.  <em>Wea. Forecasting</em>, <b>21</b>, 824&ndash;838.
</p>
<p>Marzban, C. and Sandgathe, S. (2008) Cluster Analysis for Object-Oriented Verification of Fields: A Variation. <em>Mon. Wea. Rev.</em>, <b>136</b>, (3), 1013&ndash;1025.
</p>
<p>Marzban, C. and Sandgathe, S. (2009) Verification with variograms.  <em>Wea. Forecasting</em>, <b>24</b> (4), 1102&ndash;1120, doi: 10.1175/2009WAF2222122.1
</p>
<p>Marzban, C. and Sandgathe, S. (2010) Optical flow for verification.  <em>Wea. Forecasting</em>, <b>25</b>, 1479&ndash;1494, doi:10.1175/2010WAF2222351.1.
</p>
<p>Mesinger, F. (2008) Bias adjusted precipitation threat scores.  <em>Adv. Geosci.</em>, <b>16</b>, 137&ndash;142.
</p>
<p>Micheas, A. C., Fox, N. I., Lack, S. A. and Wikle, C. K. (2007) Cell identification and verification of QPF ensembles using shape analysis techniques. <em>J. of Hydrology</em>, <b>343</b>, 105&ndash;116.
</p>
<p>Nachamkin, J. E. (2004) Mesoscale verification using meteorological composites. <em>Mon. Wea. Rev.</em>, <b>132</b>, 941&ndash;955.
</p>
<p>Roberts, N. M. and Lean, H. W. (2008) Scale-selective verification of rainfall accumulations from high-resolution forecasts of convective events.  <em>Mon. Wea. Rev.</em>, <b>136</b>, 78&ndash;97. doi:10.1175/2007MWR2123.1.
</p>
<p>Schwedler, B. R. J. and Baldwin, M. E. (2011)  Diagnosing the sensitivity of binary image measures to bias, location, and event frequency within a forecast verification framework.  <em>Wea. Forecasting</em>, <b>26</b>, 1032&ndash;1044, doi:10.1175/WAF-D-11-00032.1.
</p>
<p>Thompson, J. C. and Carter, G. M. (1972) On some characteristics of the S1 score.  <em>J. Appl. Meteorol.</em>, <b>11</b>, 1384&ndash;1385.
</p>
<p>Venugopal, V., Basu, S. and Foufoula-Georgiou, E. (2005) A new metric for comparing precipitation patterns with an application to ensemble forecasts.  <em>J. Geophys. Res.</em>, <b>110</b>, D08111, doi:10.1029/2004JD005395, 11pp.
</p>
<p>Wernli, H., Paulat, M., Hagen, M. and Frei, C. (2008)  SAL&ndash;A novel quality measure for the verification of quantitative precipitation forecasts.  <em>Mon. Wea. Rev.</em>, <b>136</b>, 4470&ndash;4487.
</p>
<p>Wernli, H., Hofmann, C. and Zimmer, M. (2009)  Spatial forecast verification methods intercomparison project: Application of the SAL technique.  <em>Wea. Forecasting</em>, <b>24</b>, 1472&ndash;1484, doi:10.1175/2009WAF2222271.1
</p>
<p>Willmott, C. J., Robeson, S. M. and Matsuura, K. (2007)  Geographic box plots.  <em>Physical Geography</em>, <b>28</b>, 331&ndash;344, DOI: 10.2747/0272-3646.28.4.331.
</p>
<p>Zhu, M., Lakshmanan, V. Zhang, P. Hong, Y. Cheng, K. and Chen, S. (2011) Spatial verification using a true metric.  <em>Atmos. Res.</em>, <b>102</b>, 408&ndash;419, doi:10.1016/j.atmosres.2011.09.004.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## See help files for above named functions and datasets
## for specific examples.
</code></pre>

<hr>
<h2 id='abserrloss'>
Loss functions for the spatial prediction comparison test (SPCT)
</h2><span id='topic+abserrloss'></span><span id='topic+corrskill'></span><span id='topic+sqerrloss'></span><span id='topic+distmaploss'></span>

<h3>Description</h3>

<p>Loss functions for applying the spatial prediction comparison test (SPCT) for competing forecasts.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>abserrloss(x, y, ...)
corrskill(x, y, ...)
sqerrloss(x, y, ...)
distmaploss(x, y, threshold = 0, const = Inf, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="abserrloss_+3A_x">x</code>, <code id="abserrloss_+3A_y">y</code></td>
<td>

<p>m by n numeric matrices against which to calculate the loss (or skill) functions.
</p>
</td></tr>
<tr><td><code id="abserrloss_+3A_threshold">threshold</code></td>
<td>
<p>numeric giving the threshold over which (and including) binary fields are created from <code>x</code> and <code>y</code> in order to make a distance map.</p>
</td></tr>
<tr><td><code id="abserrloss_+3A_const">const</code></td>
<td>
<p>numeric giving the constant beyond which the differences in distance maps between <code>x</code> and <code>y</code> are set to zero.  If <code>Inf</code> (default), then no cut-off is taken.  The SPCT is probably not powerful for large values of <code>const</code>.</p>
</td></tr>
<tr><td><code id="abserrloss_+3A_...">...</code></td>
<td>

<p>Not used by <code>abserrloss</code> or <code>sqerrloss</code> (there for consistency only, and in order to work with <code>lossdiff</code>).  For <code>corrskill</code>, these are optional arguments to <code>sd</code>.  For <code>distmaploss</code>, these are optional arguments to the <code>distmap</code> function from pacakge <span class="pkg">spatstat</span>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These are simple loss functions that can be used in conjunction with <code>lossdiff</code> to carry out the spatial prediction comparison test (SPCT) as introduced in Hering and Genton (2011); see also Gilleland (2013) in particular for details about the distance map loss function.
</p>
<p>The distance map loss function does not zero-out well as the other loss functions do.  Therefore, <code>zero.out</code> should be <code>FALSE</code> in the call to <code>lossdiff</code>.  Further, as pointed out in Gilleland (2013), the distance map loss function can easily be hedged by having a lot of correct negatives.  The image warp loss function is probably better for this purpose if, e.g., there are numerous zero-valued grid points in all fields.
</p>


<h3>Value</h3>

<p>numeric m by n matrices containing the value of the loss (or skill) function at each location i of the original set of locations (or grid of points).
</p>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>References</h3>

<p>Gilleland, E. (2013) Testing competing precipitation forecasts accurately and efficiently: The spatial prediction comparison test.  <em>Mon. Wea. Rev.</em>, <b>141</b>, (1), 340&ndash;355.
</p>
<p>Hering, A. S. and Genton, M. G. (2011) Comparing spatial predictions.  <em>Technometrics</em> <b>53</b>, (4), 414&ndash;425.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+lossdiff">lossdiff</a></code>, <code><a href="fields.html#topic+vgram.matrix">vgram.matrix</a></code>, <code><a href="fields.html#topic+vgram">vgram</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># See help file for lossdiff for examples.
</code></pre>

<hr>
<h2 id='Aindex'>
Area Index
</h2><span id='topic+Aindex'></span><span id='topic+Aindex.default'></span><span id='topic+Aindex.SpatialVx'></span>

<h3>Description</h3>

<p>Calculate Area index described in AghaKouchak et al. (2011).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Aindex(x, thresh = NULL, dx = 1, dy = 1, ...)

## Default S3 method:
Aindex(x, thresh = NULL, dx = 1, dy = 1, ...)

## S3 method for class 'SpatialVx'
Aindex(x, thresh = NULL, dx = 1, dy = 1, ...,
    time.point=1, obs = 1, model=1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Aindex_+3A_x">x</code></td>
<td>

<p>Default: m by n numeric matrix giving the field for which the area index is to be calculated.
</p>
<p><code>Aindex.SpatialVx</code>: list object of class &ldquo;SpatialVx&rdquo;.
</p>
</td></tr>
<tr><td><code id="Aindex_+3A_thresh">thresh</code></td>
<td>

<p>Values under this threshold are set to zero.  If NULL, it will be set to 1e-8 (a very small value).
</p>
</td></tr>
<tr><td><code id="Aindex_+3A_dx">dx</code>, <code id="Aindex_+3A_dy">dy</code></td>
<td>

<p>numeric giving the grid point size in each direction if it is desired to apply such a correction.  However, the values are simply canceled out in the index, so these arguments are probably not necessary.  If it is desired to only get the area of the non-zero values in the field, or the convex hull, then these make sense.
</p>
</td></tr>
<tr><td><code id="Aindex_+3A_time.point">time.point</code></td>
<td>
<p>numeric or character indicating which time point from the &ldquo;SpatialVx&rdquo; verification set to select for analysis.</p>
</td></tr>
<tr><td><code id="Aindex_+3A_obs">obs</code>, <code id="Aindex_+3A_model">model</code></td>
<td>
<p>numeric indicating which observation/forecast model to select for the analysis.</p>
</td></tr>
<tr><td><code id="Aindex_+3A_...">...</code></td>
<td>
<p>Not used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The area index introduced in AghaKouchak et al. (2011) is given by
</p>
<p>Aindex = A/Aconvex,
</p>
<p>where A is the area of the pattern, and Aconvex the area of its convex hull (area.owin from package spatstat is used to calculate this latter area, and the functions as.im and solutionset from spatstat are also used by this function).  Values are between 0 and 1.  Values closer to unity indicate a more structured pattern, and values closer to zero indicate higher dispersiveness of the pattern, but note that two highly structured patterns far away from each other may also give a low value (see examples below).  Because of this property, this measure is perhaps best applied to individual features in a field.
</p>


<h3>Value</h3>

<p>numeric vector (or two-row matrix in the case of <code>Aindex.SpatialVx</code>) with named components (columns):
</p>
<table>
<tr><td><code>Aindex</code></td>
<td>
<p>numeric giving the area index.</p>
</td></tr>
<tr><td><code>A</code>, <code>Aconvex</code></td>
<td>
<p>numeric giving the area of th epattern and the convex hull, resp.</p>
</td></tr>
<tr><td><code>dx</code>, <code>dy</code></td>
<td>
<p>the values of dx and dy as input to the function.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>References</h3>

<p>AghaKouchak, A., Nasrollahi, N., Li, J., Imam, B. and Sorooshian, S. (2011) Geometrical characterization of precipitation patterns.  <em>J. Hydrometeorology</em>, <b>12</b>, 274&ndash;285, doi:10.1175/2010JHM1298.1.
</p>


<h3>See Also</h3>

<p><code><a href="spatstat.geom.html#topic+as.im">as.im</a></code>, <code><a href="spatstat.geom.html#topic+solutionset">solutionset</a></code>, <code><a href="spatstat.geom.html#topic+convexhull">convexhull</a></code>, <code><a href="#topic+Cindex">Cindex</a></code>, <code><a href="#topic+Sindex">Sindex</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Gemetric shape that is highly structured.
# Re-create Fig. 7a from AghaKouchak et al. (2011).
tmp &lt;- matrix(0, 8, 8)
tmp[3,2:4] &lt;- 1
tmp[5,4:6] &lt;- 1
tmp[7,6:7] &lt;- 1
Aindex(tmp)
</code></pre>

<hr>
<h2 id='bearing'>
Bearing from One Spatial Location to Another
</h2><span id='topic+bearing'></span>

<h3>Description</h3>

<p>Find the bearing from one spatial location to another.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bearing(point1, point2, deg = TRUE, aty = "compass")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bearing_+3A_point1">point1</code>, <code id="bearing_+3A_point2">point2</code></td>
<td>
<p> two-column numeric matrices giving lon/lat coordinates for the origin point(s) (<code>point1</code>) and the destination point(s) (<code>point2</code>).</p>
</td></tr>
<tr><td><code id="bearing_+3A_deg">deg</code></td>
<td>

<p>logical, should the output be converted from radians to degrees?
</p>
</td></tr>
<tr><td><code id="bearing_+3A_aty">aty</code></td>
<td>

<p>character stating either &ldquo;compass&rdquo; (default) or &ldquo;radial&rdquo;.  The former gives the standard compass bearing angle (0 is north, increase clockwise), and the latter is for polar coordinates (0 is East, increase counter-clockwise).
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The bearing, beta, of a point B as seen from a point A is given by
</p>
<p>beta = atan2(S,T)
</p>
<p>where
</p>
<p>S = cos(phi_B) * sin(L_A - L_B), and
</p>
<p>T = cos(phi_A)*sin(phi_B) - sin(phi_A)*cos(phi_B)*cos(L_A - L_B)
</p>
<p>where phi_A (phi_B) is the latitude of point A (B), and L_A (L_B) is the longitude of point A (B).
</p>
<p>Note that there is no simple relationship between the bearing of A to B vs. the bearing of B to A.  The bearing given here is in the usual R convention for lon/lat information, which gives points east of Greenwich as negative longitude, and south of the equator as negative latitude.
</p>


<h3>Value</h3>

<p>numeric giving the bearing angle.
</p>


<h3>Author(s)</h3>

<p>Eric Gilleland and Randy Bullock, bullock &ldquo;at&rdquo; ucar.edu
</p>


<h3>References</h3>

<p>Keay, W. (1995) Land Navigation: Routefinding with Map &amp; Compass, Coventry, UK: Clifford Press Ltd., ISBN 0319008452, 978-0319008454
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+atan2">atan2</a></code>, <code><a href="#topic+FeatureAxis">FeatureAxis</a></code>, <code><a href="fields.html#topic+rdist.earth">rdist.earth</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Boulder, Colorado and Wallaroo, Australia.
A &lt;- rbind(c(-105.2833, 40.0167), c(137.65, -33.9333))

# Wallaroo, Australia and Boulder, Colorado.
B &lt;- rbind(c(137.65, -33.9333), c(-105.2833, 40.0167))

bearing(A,B)
bearing(A,B,aty="radial")

plot(A, type="n", xlab="", ylab="")
points(A[,1], A[,2], pch="*", col="darkblue")

# Boulder, Colorado to Wallaroo, Australia.
arrows(A[1,1], A[1,2], A[2,1], A[2,2], col="red", lwd=1.5)
</code></pre>

<hr>
<h2 id='binarizer'>
Create Binary Fields
</h2><span id='topic+binarizer'></span>

<h3>Description</h3>

<p>Convert a spatial field to a binary spatial field via thresholding.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>binarizer(X, Xhat, threshold = NULL,
	  rule = c("&gt;", "&lt;", "&gt;=", "&lt;=", "&lt;&gt;", "&gt;&lt;", "=&lt;&gt;",
		   "&lt;&gt;=", "=&lt;&gt;=", "=&gt;&lt;", "&gt;&lt;=", "=&gt;&lt;="),
	  value = c("matrix", "owin"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="binarizer_+3A_x">X</code>, <code id="binarizer_+3A_xhat">Xhat</code></td>
<td>

<p>matrix or &ldquo;owin&rdquo; objects.
</p>
</td></tr>
<tr><td><code id="binarizer_+3A_threshold">threshold</code></td>
<td>

<p>single number, numeric vector of two numbers, or two-by-two matrix; depending on the value of rule.  May be missing or null if both <code>X</code> and <code>Xhat</code> are &ldquo;owin&rdquo; class objects, in which case binary fields are made wherever these fields are greater than zero, and other arguments are ignored.
</p>
</td></tr>
<tr><td><code id="binarizer_+3A_rule">rule</code></td>
<td>

<p>character giving the rule for identifying 1-valued grid squares.  For example, if rule is the default (&ldquo;&gt;&rdquo;), then threshold should either be a single numeric or a vector of length two.  If the latter, it specifies a different threshold for <code>X</code> and <code>Xhat</code>, the result is that everywhere <code>X</code> &gt; <code>threshold</code> will have a value of 1 and zero otherwise, etc.  The rule &ldquo;&lt;&gt;&rdquo; means 1-values whenever <code>X</code> (<code>Xhat</code>) are less than the lower threshold and higher than the higher threshold, etc.  For rules requiring two threshold values, if a two-by-two matrix is given, the first column is associated with <code>X</code> and the second with <code>Xhat</code>.</p>
</td></tr>
<tr><td><code id="binarizer_+3A_value">value</code></td>
<td>

<p>character telling whether the returned object be a list with two matrices or a list with two &ldquo;owin&rdquo; class objects.
</p>
</td></tr>
<tr><td><code id="binarizer_+3A_...">...</code></td>
<td>
<p> Not used.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The binary fields are created by assigning ones according to the rule:
1. &quot;&gt;&quot;: if X &gt; threshold, assign 1, zero otherwise.
2. &quot;&lt;&quot;: if X &lt; threshold, assign 1, zero otherwise.
3. &quot;&gt;=&quot;: if X &gt;= threshold, assign 1, zero otherwise.
4. &quot;&lt;=&quot;: if X &lt;= threshold, assign 1, zero otherwise.
5. &quot;&lt;&gt;&quot;: if X &lt; threshold[ 1 ] or X &gt; threshold[ 2 ], assign 1, zero otherwise.
6. &quot;&gt;&lt;&quot;: if threshold[ 1 ] &lt; X &lt; threshold[ 2 ], assign 1, zero otherwise.
7. &quot;=&lt;&gt;&quot;: if threshold[ 1 ] &lt;= X or X &gt; threshold[ 2 ], assign 1, zero otherwise.
8. &quot;&lt;&gt;=&quot;: if threshold[ 1 ] &lt; X or X &gt;= threshold[ 2 ], assign 1, zero otherwise.
9. &quot;=&lt;&gt;=&quot;: if X &lt;= threshold[ 1 ] or X &gt;= threshold[ 2 ], assign 1, zero otherwise.
10. &quot;=&gt;&lt;&quot;: if threshold[ 1 ] &lt;= X &lt; threshold[ 2 ], assign 1, zero otherwise.
11. &quot;&gt;&lt;=&quot;: if threshold[ 1 ] &lt; X &lt;= threshold[ 2 ], assign 1, zero otherwise.
12. &quot;=&gt;&lt;=&quot;: if threshold[ 1 ] &lt;= X &lt;= threshold[ 2 ], assign 1, zero otherwise.
</p>


<h3>Value</h3>

<p>A list object with two components is returned with the first component being the binary version of X and the second that for Xhat.  These fields will either be matrices of the same dimension as X and Xhat or they will be owin objects depending on the value argument.
</p>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>See Also</h3>

<p><code><a href="#topic+thresholder">thresholder</a></code>, <code><a href="spatstat.geom.html#topic+im">im</a></code>, <code><a href="spatstat.geom.html#topic+solutionset">solutionset</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data( "obs0601" )
data( "wrf4ncar0531" )
bin &lt;- binarizer( X = obs0601, Xhat = wrf4ncar0531, threshold = 2.1 )

image.plot( bin[[ 1 ]] )
image.plot( bin[[ 2 ]] )

bin2 &lt;- binarizer( X = obs0601, Xhat = wrf4ncar0531,
		  threshold = 2.1, value = "owin" )
plot( bin2[[ 1 ]] )
plot( bin2[[ 2 ]] )

</code></pre>

<hr>
<h2 id='calculate_dFSS'>
Calculate dFSS Score Value
</h2><span id='topic+calculate_dFSS'></span>

<h3>Description</h3>

<p>Calculates the value for the dFSS binary distance metric. The dFSS uses the Fraction Skill Score to provide a measure of spatial displacement of precipitation in two precipitation fields.   
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calculate_dFSS(fbin1, fbin2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="calculate_dFSS_+3A_fbin1">fbin1</code></td>
<td>

<p>A numeric matrix representing the first binary field. Only values 0 and 1 are allowed in the matrix.   
</p>
</td></tr>
<tr><td><code id="calculate_dFSS_+3A_fbin2">fbin2</code></td>
<td>

<p>A numeric matrix representing the second binary field.  Only values 0 and 1 are allowed in the matrix. The matrix needs to have the same dimensions as <code>fbin1</code>.   
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The dFSS uses the Fraction Skill Score to provide a measure of spatial displacement of precipitation in two precipitation fields. 
</p>
<p>The function requires two binary fields as input. A binary field can only have values of 0 or 1 and can be obtained through a thresholding process of the original continuous precipitation field (e.g., by setting all values below a selected precipitation threshold to zero, and all values above the threshold to one). 
</p>
<p>The dFSS has a requirement that the frequency bias of precipitation needs to be small in order for the metric to work properly (i.e. the number of non-zero grid points has to be similar in both binary fields). The unbiased fields can be obtained from the original continuous precipitation fields via the use of a frequency (percentile) threshold. For example, instead of using a predefined physical threshold (e.g. 1 mm/h), which might produce binary fields with a different number of non-zero points, a frequency threshold (e.g. 5 %) can be used which guarantees that both fields will have the same number of non-zero grid-points and will thus be unbiased (provided that enough grid points in the domain contain non-zero precipitation). Function <code>quantile</code> can be used to determine the value of a physical threshold that corresponds to a prescribed frequency threshold. 
</p>
<p>If the frequency bias is larger than 1.5 the function will work but produce a warning. If the frequency bias is larger than 2 the function will produce an error. The dFSS value can only be calculated if both fields contain at least one non-zero grid point. For correct interpretation of the results and some other considerations please look at the &quot;Recipe&quot; in the Conclusions section of Skok and Roberts (2018). 
</p>
<p>The code utilizes the fast method for computing fractions (Faggian et al., 2015) and the Bisection method to arrive more quickly at the correct displacement. Optionally, a significantly faster R code that requires significantly less memory and uses some embedded C++ code is available upon request from the author.
</p>


<h3>Value</h3>

<p>The function returns a single numeric value representing the size of the estimated spatial displacement (expressed as a number of grid points - see the example below).
</p>


<h3>Author(s)</h3>

<p>Gregor Skok (Gregor.Skok@fmf.uni-lj.si)
</p>


<h3>References</h3>

<p>Skok, G. and Roberts, N. (2018), Estimating the displacement in precipitation forecasts using the Fractions Skill Score. Q.J.R. Meteorol. Soc. doi:10.1002/qj.3212. 
</p>
<p>Faggian N., Roux B., Steinle P., Ebert B., 2015: Fast calculation of the Fractions Skill Score, MAUSAM, 66 (3), 457-466.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+hoods2d">hoods2d</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# ---------------------------------------------
# A simple example with two 500 x 500 fields
# ---------------------------------------------

# generate two empty 500 x 500 fields where all values are 0 
fbin1=matrix(0, 500, 500, byrow = FALSE)
fbin2=fbin1

# in the fields define a single 20x20 non-zero region of precipitation
# that is horizontally displaced in the second field by 100 grid points   
fbin1[200:220,200:220]=1
fbin2[200:220,300:320]=1

# calulate dFSS value
dFSS=calculate_dFSS(fbin1, fbin2)

# print dFSS value 
print(dFSS)

# The example should output 97 which means that the spatial displacement
# estimated by dFSS is 97 grid points.
</code></pre>

<hr>
<h2 id='calculate_FSSvector_from_binary_fields'>
Calculate FSS Values
</h2><span id='topic+calculate_FSSvector_from_binary_fields'></span>

<h3>Description</h3>

<p>Calculates the value of Fraction Skill Score (FSS) for multiple neighborhood sizes.</p>


<h3>Usage</h3>

<pre><code class='language-R'>calculate_FSSvector_from_binary_fields(fbin1, fbin2, nvector)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="calculate_FSSvector_from_binary_fields_+3A_fbin1">fbin1</code></td>
<td>

<p>A numeric matrix representing the first binary field. Only values 0 and 1 are allowed in the matrix.   
</p>
</td></tr>
<tr><td><code id="calculate_FSSvector_from_binary_fields_+3A_fbin2">fbin2</code></td>
<td>

<p>A numeric matrix representing the second binary field.  Only values 0 and 1 are allowed in the matrix. The matrix needs to have the same dimensions as <code>fbin1</code>.   
</p>
</td></tr>
<tr><td><code id="calculate_FSSvector_from_binary_fields_+3A_nvector">nvector</code></td>
<td>

<p>A numeric vector containing neighborhood sizes for which the FSS values are to be calculated. Only positive odd values are allowed in the vector. A square neighborhood shape is assumed and the specified value represents the length of square side.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Fractions Skill Score is a neighborhood-based spatial verification metric frequently used for verifying precipitation (see Roberts and Lean, 2008, for details).
</p>
<p>The function requires two binary fields as input. A binary field can only have values of 0 or 1 and can be obtained through a thresholding process of the original continuous precipitation field (e.g., by setting all values below a selected precipitation threshold to zero, and all values above the threshold to one). Either a predefined physical threshold (e.g. 1 mm/h) or a frequency threshold (e.g. 5 %) can be used to produce the binary fields from the original continuous precipitation fields. If a frequency threshold is used the binary fields will be unbiased and the FSS value will asymptote to 1 at large neighborhoods. Function <code>quantile</code> can be used to determine the value of a physical threshold that corresponds to a prescribed frequency threshold. 
</p>
<p>The code utilizes the fast method for computing fractions (Faggian et al., 2015) that enables fast computation of FSS values at multiple neighborhood sizes. Optionally, a significantly faster R code that requires significantly less memory and uses some embedded C++ code is available upon request from the author.
</p>


<h3>Value</h3>

<p>A numeric vector of the same dimension as <code>nvector</code> that contains the FSS values at corresponding neighborhood sizes.
</p>


<h3>Author(s)</h3>

<p>Gregor Skok (Gregor.Skok@fmf.uni-lj.si)
</p>


<h3>References</h3>

<p>Roberts, N.M., Lean, H.W., 2008. Scale-Selective Verification of Rainfall Accumulations from High-Resolution Forecasts of Convective Events. Mon. Wea. Rev. 136, 78-97.
</p>
<p>Faggian N., Roux B., Steinle P., Ebert B., 2015: Fast calculation of the Fractions Skill Score, MAUSAM, 66 (3), 457-466.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+hoods2d">hoods2d</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# ---------------------------------------------
# A simple example with two 500 x 500 fields
# ---------------------------------------------

# generate two empty 500 x 500 binary fields where all values are 0 
fbin1=matrix(0, 500, 500, byrow = FALSE)
fbin2=fbin1

# in the fields define a single 20x20 non-zero region of precipitation that
# is horizontally displaced in the second field by 100 grid points 
fbin1[200:220,200:220]=1
fbin2[200:220,300:320]=1

# specify a vector of neighborhood sizes for which the FSS values are to be calculated
nvector = c(1,51,101,201,301,601,901,1501)

# calulate FSS values
FSSvector=calculate_FSSvector_from_binary_fields(fbin1, fbin2, nvector)

# print FSS values 
print(FSSvector)

# The example should output:
#  0.00000000 0.00000000 0.04271484 0.52057596 0.68363656 0.99432823 1.00000000 1.00000000
</code></pre>

<hr>
<h2 id='calculate_FSSwind'>
Calculate FSSwind Score Value
</h2><span id='topic+calculate_FSSwind'></span>

<h3>Description</h3>

<p>Calculates the value for the FSSwind metric that can be used for spatial verification of 2D wind fields. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calculate_FSSwind(findex1, findex2, nvector)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="calculate_FSSwind_+3A_findex1">findex1</code></td>
<td>

<p>A numeric matrix representing the first wind class index field. Only integer values larger than 0 are allowed in the matrix. Each integer value corresponds to a certain wind class.    
</p>
</td></tr>
<tr><td><code id="calculate_FSSwind_+3A_findex2">findex2</code></td>
<td>

<p>A numeric matrix representing the second wind class index field. Only integer values larger than 0 are allowed in the matrix. Each integer value corresponds to a certain wind class. The matrix needs to have the same dimensions as <code>findex1</code>.   
</p>
</td></tr>
<tr><td><code id="calculate_FSSwind_+3A_nvector">nvector</code></td>
<td>

<p>A numeric vector containing neighborhood sizes for which the FSSwind score value is to be calculated. Only positive odd values are allowed in the vector.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The FSSwind is based on the idea of the Fractions Skill Score, a neighborhood-based spatial verification metric frequently used for verifying precipitation. The FSSwind avoids some of the problems of traditional non-spatial verification metrics (the &quot;double penalty&quot; problem and the failure to distinguish between a &quot;near miss&quot; and much poorer forecasts) and can distinguish forecasts even when the spatial displacement of wind patterns is large. Moreover, the time-averaged score value in combination with a statistical significance test enables different wind forecasts to be ranked by their performance (see Skok and Hladnik, 2018, for details).
</p>
<p>The score can be used to spatially compare two 2D wind vector fields. In order to calculate the FSSwind value, wind classes have first to be defined. The choice of classes will define how the score behaves and influence the results. The definition of classes should reflect what a user wants to verify. The score will evaluate the spatial matching of the areas of the wind classes. The class definitions should cover the whole phase space of possible wind values (i.e. to make sure every wind vector can be assigned a wind class). It does not make sense choosing a class definition with an overly large number of classes or a definition where a single class would totally dominate over all the other classes. Once the class definition is chosen, the wind vector fields need to be converted to wind class index fields with each class assigned an unique integer value. These wind class index fields serve as input to the calculate_FSSwind function. The FSSwind can have values between 0 and 1 with 0 indicating the worst possible forecast and 1 indicating a perfect forecast.  For guidance on correct interpretation of the results and other important considerations please refer to Skok and Hladnik (2018). 
</p>
<p>The code utilizes the fast method for computing fractions (Faggian et al., 2015). Optionally, a significantly faster R code that requires significantly less memory and uses some embedded C++ code is available upon request from the author.
</p>


<h3>Value</h3>

<p>A numeric vector of the same dimension as <code>nvector</code> that contains the FSSwind values at corresponding neighborhood sizes.
</p>


<h3>Author(s)</h3>

<p>Gregor Skok (Gregor.Skok@fmf.uni-lj.si)
</p>


<h3>References</h3>

<p>Skok, G. and V. Hladnik, 2018: Verification of Gridded Wind Forecasts in Complex Alpine Terrain: A New Wind Verification Methodology Based on the Neighborhood Approach. Mon. Wea. Rev., 146, 63-75, https://doi.org/10.1175/MWR-D-16-0471.1
</p>
<p>Faggian N., Roux B., Steinle P., Ebert B., 2015: Fast calculation of the Fractions Skill Score, MAUSAM, 66 (3), 457-466.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+hoods2d">hoods2d</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># ---------------------------------------------
# A simple example with two 500 x 500 fields
# ---------------------------------------------

# generate two 500 x 500 wind class index fields where all values are 1 (wind class 1)
findex1=matrix(1, 500, 500, byrow = FALSE)
findex2=findex1

# in the fields generate some rectangular areas with other wind classes (classes 2,3 and 4)
findex1[001:220,200:220]=2
findex1[100:220,300:220]=3
findex1[300:500,100:200]=4

findex2[050:220,100:220]=2
findex2[200:320,300:220]=3
findex2[300:500,300:500]=4

# specify a vector of neighborhood sizes for which the FSSwind values are to be calculated
nvector = c(1,51,101,201,301,601,901,1501)

# calulate FSSwind values
FSSwindvector=calculate_FSSwind(findex1, findex2, nvector)

# print FSSwind values 
print(FSSwindvector)

# The example should output:
# 0.6199600 0.6580598 0.7056385 0.8029494 0.8838075 0.9700274 0.9754587 0.9756134
</code></pre>

<hr>
<h2 id='censqdelta'>
Centered on Square Domain Baddeley's Delta Metric
</h2><span id='topic+censqdelta'></span>

<h3>Description</h3>

<p>Baddeley's delta metric is sensitive to the position of non-zero grid points within the domain, as well as to the size of the domain.  In order to obtain consistent values of the metric across cases, it is recommended to first position the sets to be compared so that they are centered with respect to one another on a square domain; where the square domain is the same for all comparison sets.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>censqdelta(x, y, N, const = Inf, p = 2, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="censqdelta_+3A_x">x</code>, <code id="censqdelta_+3A_y">y</code></td>
<td>
<p> Matrices representing binary images to be compared.  If they are not binary, then they will be forced to binary by setting anything above zero to one.
</p>
</td></tr>
<tr><td><code id="censqdelta_+3A_n">N</code></td>
<td>

<p>The size of the square domain.  If missing, it will be the size of the largest side, and if it is even, one will be added to it.
</p>
</td></tr>
<tr><td><code id="censqdelta_+3A_const">const</code></td>
<td>
<p> single numeric giving the <code>c</code> argument to <code>deltametric</code>, which is the constant value over which the distance map is reduced to this value.</p>
</td></tr>
<tr><td><code id="censqdelta_+3A_p">p</code></td>
<td>
<p> single numeric giving the <code>p</code> argument to <code>deltametric</code>, which specifies the type of Lp norm used to calculate the delta maetric.</p>
</td></tr>
<tr><td><code id="censqdelta_+3A_...">...</code></td>
<td>
<p> Not used.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Baddeley's delta metric (Baddeley, 1992a,b) is the L_p norm over the absolute difference of distance maps for two binary images, A and B.  A concave function (e.g., f(t) = min(t, constant)) may first be applied ot each distance map before taking their absolute differences, which makes the result less sensitive to small changes in one or both images than other similar metrics.  The metric is sensitive to size, shape and location differences, which make it very practical for comparing forecasts to observations in terms of the position, area extent, and area shape  errors.  However, its sensitivity to domain size and position within the domain are undesirable, but are easily fixed by calculating the metric over a consistent, square domain with the combined verification set centerd on that domain.  See the example section below to see the issue.
</p>
<p>This function essentially takes a window of size N by N and moves so that the centroid of each pair of sets, A and B, is the center of the window before calculating the metric.
</p>
<p>Centering and squaring is recommended for carrying out a procedure such as that proposed in Gilleland et al. (2008).  Centering on a square domain alleviates the problems discovered by Schwedler and Baldwin (2011) who suggested using a small value of the constant in f(t) = min(t, constant) applied to the distance maps.  This solution is not very appealing because of the sensitivity in choice of the constant that generally diminishes as it approaches the domain size (Gilleland, 2011).
</p>
<p>After centering the sets on a square domain, the function <code>deltametric</code> from package <span class="pkg">spatstat</span> is used to calculate the metric.
</p>


<h3>Value</h3>

<p>A single numeric value is returned.
</p>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>References</h3>

<p>Baddeley, A. (1992a)  An error metric for binary images.  In <em>Robust Computer Vision Algorithms</em>, W. Forstner and S. Ruwiedel, Eds., Wichmann, 59&ndash;78.
</p>
<p>Baddeley, A. (1992b)  Errors in binary images and an Lp version of the Hausdorff metric.  <em>Nieuw Arch. Wiskunde</em>, <b>10</b>, 157&ndash;183.
</p>
<p>Gilleland, E. (2011) Spatial Forecast Verification: Baddeley's Delta Metric Applied to the ICP Test Cases. <em>Weather Forecast.</em>, <b>26</b> (3), 409&ndash;415.
</p>
<p>Gilleland, E. (2017) A new characterization in the spatial verification framework for false alarms, misses, and overall patterns. <em>Weather Forecast.</em>, <b>32</b> (1), 187&ndash;198, DOI: 10.1175/WAF-D-16-0134.1.
</p>
<p>Gilleland, E., Lee, T. C. M.,  Halley Gotway, J., Bullock, R. G. and Brown, B. G. (2008) Computationally efficient spatial forecast verification using Baddeley's delta image metric.  <em>Mon. Wea. Rev.</em>, <b>136</b>, 1747&ndash;1757.
</p>
<p>Schwedler, B. R. J. and Baldwin, M. E. (2011) Diagnosing the sensitivity of binary image measures to bias, location, and event frequency within a forecast verification framework. <em>Weather Forecast.</em>, <b>26</b>, 1032&ndash;1044.
</p>


<h3>See Also</h3>

<p><code><a href="spatstat.geom.html#topic+deltametric">deltametric</a></code>, <code><a href="#topic+locmeasures2d">locmeasures2d</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- y &lt;- matrix( 0, 100, 200 )
x[ 45, 10 ] &lt;- 1
x &lt;- kernel2dsmooth( x, kernel.type = "disk", r = 4 )

y[ 50, 60 ] &lt;- 1
y &lt;- kernel2dsmooth( y, kernel.type = "disk", r = 10 )

censqdelta( x, y )

## Not run: 
# Example form Gilleland (2017).
#
# I1 = circle with radius = 20 centered at 100, 100
# I2 = circle with radius = 20 centered at 140, 100
# I3 = circle with radius = 20 centered at 180, 100
# I4 = circle with radius = 20 centered at 140, 140

I1 &lt;- I2 &lt;- I3 &lt;- I4 &lt;- matrix( 0, 200, 200 )

I1[ 100, 100 ] &lt;- 1
I1 &lt;- kernel2dsmooth( I1, kernel.type = "disk", r = 20 )
I1[ I1 &gt; 0 ] &lt;- 1
if( any( I1 &lt; 0 ) ) I1[ I1 &lt; 0 ] &lt;- 0

I2[ 140, 100 ] &lt;- 1
I2 &lt;- kernel2dsmooth( I2, kernel.type = "disk", r = 20 )
I2[ I2 &gt; 0 ] &lt;- 1
if( any( I2 &lt; 0 ) ) I2[ I2 &lt; 0 ] &lt;- 0

I3[ 180, 100 ] &lt;- 1
I3 &lt;- kernel2dsmooth( I3, kernel.type = "disk", r = 20 )
I3[ I3 &gt; 0 ] &lt;- 1
if( any( I3 &lt; 0 ) ) I3[ I3 &lt; 0 ] &lt;- 0

I4[ 140, 140 ] &lt;- 1
I4 &lt;- kernel2dsmooth( I4, kernel.type = "disk", r = 20 )
I4[ I4 &gt; 0 ] &lt;- 1
if( any( I4 &lt; 0 ) ) I4[ I4 &lt; 0 ] &lt;- 0

image( I1, col = c("white", "darkblue") )
contour( I2, add = TRUE )
contour( I3, add = TRUE )
contour( I4, add = TRUE )

# Each circle is the same size and shape, and the domain is square.
# I1 and I2, I2 and I3, and I2 and I4 are all the same distance
# away from each other.  I1 and I4 and I3 and I4 are also the same distance
# from each other.  I3 touches the edge of the domain.
# 

# First, calculate the Baddeley delta metric on each 
# comparison.

I1im &lt;- as.im( I1 )
I2im &lt;- as.im( I2 )
I3im &lt;- as.im( I3 )
I4im &lt;- as.im( I4 )

I1im &lt;- solutionset( I1im &gt; 0 )
I2im &lt;- solutionset( I2im &gt; 0 )
I3im &lt;- solutionset( I3im &gt; 0 )
I4im &lt;- solutionset( I4im &gt; 0 )

deltametric( I1im, I2im )
deltametric( I2im, I3im )
deltametric( I2im, I4im )

# Above are all different values.
# Below, they are all 28.84478.
censqdelta( I1, I2 )
censqdelta( I2, I3 )
censqdelta( I2, I4 )

# Similarly for I1 and I4 vs I3 and I4.
deltametric( I1im, I4im )
deltametric( I3im, I4im )

censqdelta( I1, I4 )
censqdelta( I3, I4 )

# To see why this problem exists.
dm1 &lt;- distmap( I1im )
dm1 &lt;- as.matrix( dm1 )
dm2 &lt;- distmap( I2im )
dm2 &lt;- as.matrix( dm2 )

par( mfrow = c( 2, 2 ) )
image.plot( dm1 )
contour( I1, add = TRUE, col = "white" )
image.plot( dm2 )
contour( I2, add = TRUE, col = "white" )

image.plot( abs( dm1 ) - abs( dm2 ) )
contour( I1, add = TRUE, col = "white" )
contour( I2, add = TRUE, col = "white" )


## End(Not run)

</code></pre>

<hr>
<h2 id='centdist'>
Centroid Distance Between Two Identified Objects
</h2><span id='topic+centdist'></span>

<h3>Description</h3>

<p>Find the centroid distance between two identified objects/features.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
centdist(x, y, distfun = "rdist", loc = NULL, ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="centdist_+3A_x">x</code>, <code id="centdist_+3A_y">y</code></td>
<td>
<p>objects of class &ldquo;owin&rdquo; (package <span class="pkg">spatstat</span>) containing binary images of features of interest.</p>
</td></tr>
<tr><td><code id="centdist_+3A_distfun">distfun</code></td>
<td>
<p>character string naming a distance function that should take arguments <code>x1</code> and <code>x2</code> as 1 by 2 matrices, and return a single numeric value.  Default uses the <span class="pkg">fields</span> function, <code>rdist</code>, where the fields function <code>rdist.earth</code> is an obvious alternative.</p>
</td></tr>
<tr><td><code id="centdist_+3A_loc">loc</code></td>
<td>
<p>two-column matrix giving the location values for which to calculate the centroids.  If NULL, indices according to the dimension of the fields are used.</p>
</td></tr>
<tr><td><code id="centdist_+3A_...">...</code></td>
<td>
<p>optional arguments to <code>distfun</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a simple function that calculates the centroid for each of x and y (to get their centroids), and then finds the distance between them according to <code>distfun</code>.  The centroids are calculated using <code>FeatureProps</code>.
</p>


<h3>Value</h3>

<p>numeric giving the centroid distance.
</p>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>See Also</h3>

<p><code><a href="#topic+FeatureProps">FeatureProps</a></code>, <code><a href="spatstat.geom.html#topic+as.im">as.im</a></code>, <code><a href="spatstat.geom.html#topic+solutionset">solutionset</a></code>, <code><a href="#topic+FeatureMatchAnalyzer">FeatureMatchAnalyzer</a></code>, <code><a href="#topic+FeatureComps">FeatureComps</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- y &lt;- matrix(0, 10, 12)
x[2:3,c(3:6, 8:10)] &lt;- 1
y[c(4:7, 9:10),c(7:9, 11:12)] &lt;- 1

x &lt;- as.im(x)
x &lt;- solutionset(x&gt;0)
y &lt;- as.im(y)
y &lt;- solutionset(y&gt;0)
centdist(x,y)

</code></pre>

<hr>
<h2 id='Cindex'>
Connectivity Index
</h2><span id='topic+Cindex'></span><span id='topic+Cindex.default'></span><span id='topic+Cindex.SpatialVx'></span>

<h3>Description</h3>

<p>Calculate the connectivity index of an image.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Cindex(x, thresh = NULL, connect.method = "C", ...)

## Default S3 method:
Cindex(x, thresh = NULL, connect.method = "C", ...)

## S3 method for class 'SpatialVx'
Cindex(x, thresh = NULL, connect.method = "C", ...,
    time.point = 1, obs = 1, model = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Cindex_+3A_x">x</code></td>
<td>

<p>Default: m by n numeric matrix giving the field for which the connectivity index is to be calculated.
</p>
<p><code>Sindex.SpatialVx</code>: list object of class &ldquo;SpatialVx&rdquo;.
</p>
</td></tr>
<tr><td><code id="Cindex_+3A_thresh">thresh</code></td>
<td>

<p>Set values under (strictly less than) this threshold to zero, and calculate the connectivity index for the resulting image.  If NULL, no threshold is applied.
</p>
</td></tr>
<tr><td><code id="Cindex_+3A_connect.method">connect.method</code></td>
<td>

<p>character string giving the <code>method</code> argument for the <code>connected</code> function of package <span class="pkg">spatstat</span>.  This must be one of &ldquo;C&rdquo; or &ldquo;interpreted&rdquo;.  See the help file for <code>connected</code> for more details.
</p>
</td></tr>
<tr><td><code id="Cindex_+3A_time.point">time.point</code></td>
<td>
<p>numeric or character indicating which time point from the &ldquo;SpatialVx&rdquo; verification set to select for analysis.</p>
</td></tr>
<tr><td><code id="Cindex_+3A_obs">obs</code>, <code id="Cindex_+3A_model">model</code></td>
<td>
<p>numeric indicating which observation/forecast model to select for the analysis.</p>
</td></tr>
<tr><td><code id="Cindex_+3A_...">...</code></td>
<td>
<p>Not used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The connectivity index is introduced in AghaKouchak et al. (2011), and is designed to automatically determine how connected an image is.  It is defined by
</p>
<p>Cindex = 1 - (NC - 1)/(sqrt(NP) + NC),
</p>
<p>where 0 &lt;= Cindex &lt;= 1 is the connectivity index (values close to zero are less connected, and values close to 1 are more connected), NP is the number of nonzero pixels, and NC is the number of isolated clusters.  
</p>
<p>The function <code>connected</code> from package <span class="pkg">spatstat</span> is used to identify the number of isolated clusters.
</p>


<h3>Value</h3>

<p>numeric giving the connectivity index.
</p>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>References</h3>

<p>AghaKouchak, A., Nasrollahi, N., Li, J., Imam, B. and Sorooshian, S. (2011) Geometrical characterization of precipitation patterns.  <em>J. Hydrometerology</em>, <b>12</b>, 274&ndash;285, doi:10.1175/2010JHM1298.1.
</p>


<h3>See Also</h3>

<p><code><a href="spatstat.geom.html#topic+connected">connected</a></code>, <code><a href="spatstat.geom.html#topic+as.im">as.im</a></code>, <code><a href="#topic+Sindex">Sindex</a></code>, <code><a href="#topic+Aindex">Aindex</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Re-create Fig. 7a from AghaKouchak et al. (2011).
tmp &lt;- matrix(0, 8, 8)
tmp[3,2:4] &lt;- 1
tmp[5,4:6] &lt;- 1
tmp[7,6:7] &lt;- 1
Cindex(tmp)
</code></pre>

<hr>
<h2 id='CircleHistogram'>
Circular Histogram
</h2><span id='topic+CircleHistogram'></span>

<h3>Description</h3>

<p>Make a circle histogram, also known as a wind-rose diagram.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CircleHistogram(wspd, wdir, numPetals = 12, radians = FALSE,
		COLS = NULL, scale.factor = 3, varwidth = TRUE,
		minW = NULL, maxW = NULL, circFr = 10,
		main = "Wind Rose", cir.ind = 0.05,
		max.perc = NULL, leg = FALSE, units = "units",
		verbose = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CircleHistogram_+3A_wspd">wspd</code></td>
<td>
<p>numeric vector of length <code>n</code> giving the magnitudes for the histograms (i.e., the petal colors).  For example, with MODE output, one might use the centroid distances between the paired objects, or the Baddeley Delta metric, or whatever other attribute is of interest.  The argument is called <code>wspd</code> because in the original context of windrose graphs, the windspeed was used for the petal colors.
</p>
</td></tr>
<tr><td><code id="CircleHistogram_+3A_wdir">wdir</code></td>
<td>
<p> numeric vector of length <code>n</code> giving the angles (from the north) between two matched objects.  In the original context of a windrose diagram, the direction from the north of the wind.  See <code>bearing</code> for a way to obtain such vectors.
</p>
</td></tr>
<tr><td><code id="CircleHistogram_+3A_numpetals">numPetals</code></td>
<td>
<p> numeric giving the number of petals to use.  </p>
</td></tr>
<tr><td><code id="CircleHistogram_+3A_radians">radians</code></td>
<td>
<p> logical if TRUE the angles displayed are radians, if FALSE degrees.  </p>
</td></tr>
<tr><td><code id="CircleHistogram_+3A_cols">COLS</code></td>
<td>
<p> vector defining the colors to be used for the petals.  See, for example, <code>topo.colors</code>.  </p>
</td></tr>
<tr><td><code id="CircleHistogram_+3A_scale.factor">scale.factor</code></td>
<td>
<p>numeric determining the line widths (scaled against the bin sizes), only used if <code>varwidth</code> is TRUE. </p>
</td></tr>
<tr><td><code id="CircleHistogram_+3A_varwidth">varwidth</code></td>
<td>
<p> logical determining whether to vary the widths of the petals or not.  </p>
</td></tr>
<tr><td><code id="CircleHistogram_+3A_minw">minW</code>, <code id="CircleHistogram_+3A_maxw">maxW</code></td>
<td>
<p>single numerics giving the minimum and maximum break ranges for the histogram of each petal.  If NULL, it will be computed as min( wspd) and max( wspd).</p>
</td></tr>
<tr><td><code id="CircleHistogram_+3A_circfr">circFr</code></td>
<td>
<p>numeric giving the bin width. </p>
</td></tr>
<tr><td><code id="CircleHistogram_+3A_main">main</code></td>
<td>
<p>character string giving the title to add to the plot.</p>
</td></tr>
<tr><td><code id="CircleHistogram_+3A_cir.ind">cir.ind</code></td>
<td>
<p>numeric only used if <code>max.perc</code> is NULL.</p>
</td></tr>
<tr><td><code id="CircleHistogram_+3A_max.perc">max.perc</code></td>
<td>
<p>numeric giving the maximum percentage to show</p>
</td></tr>
<tr><td><code id="CircleHistogram_+3A_leg">leg</code></td>
<td>
<p>logical determining whether or not to add a legend to the plot.</p>
</td></tr>
<tr><td><code id="CircleHistogram_+3A_units">units</code></td>
<td>
<p>character string giving the units for use with the legend.  Not used if <code>leg</code> is FALSE.</p>
</td></tr>
<tr><td><code id="CircleHistogram_+3A_verbose">verbose</code></td>
<td>
<p>logical telling whether or not to print information to the screen.</p>
</td></tr>
<tr><td><code id="CircleHistogram_+3A_...">...</code></td>
<td>
<p>optional arguments to the <code>plot</code> or <code>text</code> functions.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The windrose diagram, or circle histogram, is similar to a regular histogram, but adds more information when direction is important.  Binned directions are placed in a full circle with frequencies of for each angle shown via the lengths of each pedal.  Colors along the pedal show the conditional histogram of another variable along the pedal.
</p>


<h3>Value</h3>

<p>A plot is produced.  If assigned to an object, then a list object is returned with the components:
</p>
<table>
<tr><td><code>summary</code></td>
<td>
<p>a list object giving the histogram information for each petal.</p>
</td></tr>
<tr><td><code>number.obs</code></td>
<td>
<p>numeric giving the number of observations.</p>
</td></tr>
<tr><td><code>number.calm</code></td>
<td>
<p>numeric giving the number of zero wspd and missing values.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Matt Pocernich
</p>


<h3>See Also</h3>

<p><code><a href="#topic+bearing">bearing</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed( 1001 )
wdir &lt;- runif( 1000, 0, 360 )
set.seed( 2002 )
wspd &lt;- rgamma( 1000, 15 )
CircleHistogram( wspd = wspd, wdir = wdir, leg = TRUE )

</code></pre>

<hr>
<h2 id='clusterer'>
Cluster Analysis Verification
</h2><span id='topic+clusterer'></span><span id='topic+clusterer.default'></span><span id='topic+clusterer.SpatialVx'></span><span id='topic+summary.clusterer'></span><span id='topic+plot.clusterer'></span><span id='topic+plot.summary.clusterer'></span><span id='topic+print.clusterer'></span>

<h3>Description</h3>

<p>Perform Cluster Analysis (CA) verifcation per Marzban and Sandgathe (2006).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>clusterer(X, Y = NULL, ...)

## Default S3 method:
clusterer(X, Y = NULL, ..., xloc = NULL, xyp = TRUE, threshold = 1e-08, 
    linkage.method = "complete", stand = TRUE, trans = "identity", 
    a = NULL, verbose = FALSE)

## S3 method for class 'SpatialVx'
clusterer(X, Y = NULL, ..., time.point = 1, obs = 1, model = 1, xyp = TRUE, 
    threshold = 1e-08, linkage.method = "complete", stand = TRUE, 
    trans = "identity", verbose = FALSE)

## S3 method for class 'clusterer'
plot(x, ..., mfrow = c(1, 2), col = c("gray", tim.colors(64)), 
    horizontal = FALSE)

## S3 method for class 'summary.clusterer'
plot(x, ...)

## S3 method for class 'clusterer'
print(x, ...)

## S3 method for class 'clusterer'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="clusterer_+3A_x">X</code>, <code id="clusterer_+3A_y">Y</code></td>
<td>
<p><code>clusterer</code> default method, these are m by n matrices giving the verification and forecast fields, resp.
</p>
<p>&ldquo;SpatialVx&rdquo; method function, <code>X</code> is an object of class &ldquo;SpatialVx&rdquo; and <code>Y</code> is not used (a warning is given if it is not missing and not NULL).</p>
</td></tr>
<tr><td><code id="clusterer_+3A_object">object</code>, <code id="clusterer_+3A_x">x</code></td>
<td>
<p>list object of class &ldquo;clusterer&rdquo; as returned by <code>clusterer</code> (or <code>summary.clusterer</code> in the case of <code>plot.summary.clusterer</code>).</p>
</td></tr>
<tr><td><code id="clusterer_+3A_xloc">xloc</code></td>
<td>
<p>(optional) numeric mn by 2  matrix giving the gridpoint locations.  If NULL, this will be created using 1:m and 1:n.</p>
</td></tr>
<tr><td><code id="clusterer_+3A_xyp">xyp</code></td>
<td>
<p>logical, should the cluster analysis be performed on the locations and intensities (TRUE) or only the locations (FALSE)?</p>
</td></tr>
<tr><td><code id="clusterer_+3A_threshold">threshold</code></td>
<td>
<p>numeric of length one or two giving the threshold to apply to each field (&gt;=).  If length is two, the first value corresponds to the threshold for the verification field, and the second to the foreast field.</p>
</td></tr>
<tr><td><code id="clusterer_+3A_linkage.method">linkage.method</code></td>
<td>
<p>character naming a valid linkage method accepted by <code>hclust</code>.</p>
</td></tr>
<tr><td><code id="clusterer_+3A_stand">stand</code></td>
<td>
<p>logical, should the data matrices consisting of <code>xloc</code> and each field first be standardized before performing cluster analysis?</p>
</td></tr>
<tr><td><code id="clusterer_+3A_trans">trans</code></td>
<td>
<p>character naming a function to be applied to the field intensities before performing the CA.  Only used if <code>xyp</code> is TRUE.  Default applies no transformation.</p>
</td></tr>
<tr><td><code id="clusterer_+3A_time.point">time.point</code></td>
<td>
<p>numeric or character indicating which time point from the &ldquo;SpatialVx&rdquo; verification set to select for analysis.</p>
</td></tr>
<tr><td><code id="clusterer_+3A_obs">obs</code>, <code id="clusterer_+3A_model">model</code></td>
<td>
<p>numeric indicating which observation/forecast model to select for the analysis.</p>
</td></tr>
<tr><td><code id="clusterer_+3A_a">a</code></td>
<td>
<p>(optional) list giving object attributes associated with a &ldquo;SpatialVx&rdquo; class object.  The <code>clusterer</code> method for &ldquo;SpatialVx&rdquo; objects calls the default method function, and uses this argument to pass the attributes through to the final returned object, as well as to grab location information.</p>
</td></tr>
<tr><td><code id="clusterer_+3A_mfrow">mfrow</code></td>
<td>
<p>mfrow parameter (see help file for <code>par</code>).  If NULL, then the parameter is not re-set.  </p>
</td></tr>
<tr><td><code id="clusterer_+3A_col">col</code></td>
<td>
<p>color vector for image plots of fields after applying the threshold(s).</p>
</td></tr>
<tr><td><code id="clusterer_+3A_horizontal">horizontal</code></td>
<td>
<p>logical, should the image plot color legend be placed horizontally or vertically?  Only for image plot sof the fields.</p>
</td></tr>
<tr><td><code id="clusterer_+3A_verbose">verbose</code></td>
<td>
<p>logical, should progress information be printed to the screen?</p>
</td></tr>
<tr><td><code id="clusterer_+3A_...">...</code></td>
<td>
<p>optional arguments to the <code>hclust</code> function.  In the case of the <code>summary</code> method function, <code>z</code> and/or <code>sigma</code> giving a numeric value used to find the cut-off given by median + z*sigma for detemining matched objects (see Marzban and Sandgathe 2006) where defaults of 1 and the standard deviation of minimum inter-cluster distances are used, and <code>silent</code> (logical should information be printed to the screen (FALSE) or not (TRUE); default is to print to the screen.  In the case of the <code>plot</code> method functions, these are optional arguments to the <code>summary</code> method function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function performs cluster analysis (CA) on positive values from each of two fields in a verification set using the hclust function from package fastcluster.  Inter-cluster distances are computed between each cluster of each field at every level of the CA.  The function clusterer performs CA on both fields, and finds the inter-cluster distances across fields for every possible combination of objects at each iteration of each CA.  The summary method function finishes the analysis by determining hits, misses and false alarms as well as the numbers of clusters.  It also computes CSI for each number of cluster combinations.  This is the verification approach described in Marzban and Sandgathe (2006).
</p>
<p>The <code>plot</code> method function creates a 4 by 2 panel of plots.  The top two plots give image plots of the verification and forecast fields with grid points below the threshold(s) showing zero.  The next two plots are dendrograms as performed by the plot method function for <code>hclust</code> (<code>dendrogram</code>) objects.  The next row gives a histogram of the minimum inter-cluster distances, then box plots showing the hits, misses and false alarms for every possible combination of levels of each CA.  Finally, the bottom two plots show, for each combination of CA level (i.e., numbers of clusters), the CSI and average error (inter-cluster distance) for all matched objects.  These last three plots are the ones made by the plot method for values returned from the <code>summary</code> method function.
</p>
<p><code>print</code> is currently not very useful here, but it prevents printing a big mess to the screen.
</p>


<h3>Value</h3>

<p>A list object of class &ldquo;clusterer&rdquo; is returned with components:
</p>
<table>
<tr><td><code>linkage.method</code></td>
<td>
<p>character vector of length one or two giving the linkage method as passed into the function.  The length is two only if the McQuitty method is chosen in which case this method is used for the CA, but not for the inter-cluster differencs across fields (average is used for that instead).</p>
</td></tr>
<tr><td><code>trans</code></td>
<td>
<p>character naming the transformation function applied to the intensities.</p>
</td></tr>
<tr><td><code>N</code></td>
<td>
<p>numeric giving the size of the fields.</p>
</td></tr>
<tr><td><code>threshold</code></td>
<td>
<p>numeric of length two giving the threshold applied to each field.</p>
</td></tr>
<tr><td><code>NCo</code>, <code>NCf</code></td>
<td>
<p>numeric vectors giving the number of clusters at each iteration of the CA for the verification and forecast fields, resp.</p>
</td></tr>
<tr><td><code>cluster.identifiers</code></td>
<td>
<p>a list with components X and Y giving lists of lists identifying specific CA components at each level of the CA for both fields.</p>
</td></tr>
<tr><td><code>idX</code>, <code>idY</code></td>
<td>
<p>logical vectors describing which grid points were included in the CA for each field (i.e., which grid points were &gt;= threshold and had non-missing values).</p>
</td></tr>
<tr><td><code>cluster.objects</code></td>
<td>
<p>a list with components X and Y giving the objects returned by hclust for each field.</p>
</td></tr>
<tr><td><code>inter.cluster.dist</code></td>
<td>
<p>a list of list objects with NCf by NCo matrix components giving the inter-cluster distances (between verification and forecast fields) for each iteration of CA for each field.</p>
</td></tr>
<tr><td><code>min.intercluster.dists</code></td>
<td>
<p>numeric vector givng the minimum values inter.cluster.dist at each iteration.  Used to determine the cut-off for matched objects.</p>
</td></tr>
</table>
<p>The summary method function returns a list with the same components as above, but also the components:
</p>
<table>
<tr><td><code>cutoff</code></td>
<td>
<p>The cut-off value used for determining matches.</p>
</td></tr>
<tr><td><code>csi</code>, <code>AvgErr</code></td>
<td>
<p>NCo by NCf numeric matrix giving the critical success index (CSI) and average intercluster error (distance) based on matched/un-matched objects.</p>
</td></tr>
<tr><td><code>HMF</code></td>
<td>
<p>NCo by NCf by 3 array giving the hits, misses and false alarms based on matched/un-matched objects.</p>
</td></tr>
</table>
<p>If the argument a is not NULL, then these are returned as attributes of the returned object.  In the case of &ldquo;SpatialVx&rdquo; objects, the attributes are preserved.
</p>
<p>plot and print methods do not return anything.
</p>


<h3>Warning</h3>

<p>Although some effort has been put into making the functions in this package as computationally efficient as possible, there is a lot of bookeeping involved with this approach, and the current functions are probably not as efficient as they could be.  In any case, they will likely be slow for large data sets.  The function can work quickly on large fields if an adequately high threshold is used (e.g., if threshold=10 is replaced for 16 in the not run example below, the function is VERY slow).  Performing the actual cluster analysis on each field is fast because the hclust function from the fastcluster package is used, which works very well.  However, bookeeping after the CA is done employs a lot of loops within loops, which possibly can be made more efficient (and maybe someday will be), but for now...
</p>
<p>If it is desired to simply look at the CA for the two fields, the function hclust from fastcluster can be used, which essentially replaces the hclust function from the stats package with a faster version, but otherwise operates the same as far as what is returned, etc., and the same method functions can be employed.
</p>


<h3>Note</h3>

<p>Contact Caren Marzban, marzban &ldquo;at&rdquo;  u.washington.edu, for questions about the method, and Eric Gilleland, ericg &ldquo;at&rdquo; ucar.edu, for problems with the code.</p>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>References</h3>

<p>Marzban, C. and Sandgathe, S. (2006) Cluster analysis for verification of precipitation fields.  <em>Wea. Forecasting</em>, <b>21</b>, 824&ndash;838.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+hclust">hclust</a></code>, <code><a href="fastcluster.html#topic+hclust">hclust</a></code>, <code><a href="stats.html#topic+as.dendrogram">as.dendrogram</a></code>, <code><a href="stats.html#topic+cutree">cutree</a></code>, <code><a href="#topic+make.SpatialVx">make.SpatialVx</a></code>, <code><a href="#topic+CSIsamples">CSIsamples</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data( "UKobs6" )
data( "UKfcst6" )
look &lt;- clusterer(X=UKobs6, Y=UKfcst6, threshold=16, trans="log", verbose=TRUE)
plot( look )

## Not run: 
data( "UKloc" )

# Now, do the same thing, but using a "SpatialVx" object.
hold &lt;- make.SpatialVx( UKobs6, UKfcst6, loc = UKloc, map = TRUE,
    field.type = "Rainfall", units = "mm/h",
    data.name = "Nimrod", obs.name = "obs 6", model.name = "fcst 6" )

look2 &lt;- clusterer(hold, threshold=16, trans="log", verbose=TRUE)
plot( look2 )
# Note that values differ because now we're using the
# actual locations instead of integer indicators of
# positions.

## End(Not run)
</code></pre>

<hr>
<h2 id='combiner'>
Combine Features/Matched Objects
</h2><span id='topic+combiner'></span>

<h3>Description</h3>

<p>Combine two or more features or matched class objects into one object for aggregation purposes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>combiner(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="combiner_+3A_...">...</code></td>
<td>

<p>Two or more objects of class &ldquo;features&rdquo; or &ldquo;matched&rdquo; (can also be a list of these objects).
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Useful for functions such as <code>compositer</code> and/or (coming soon) aggregating results for feature-based methods.
</p>


<h3>Value</h3>

<p>A list object of class &ldquo;combined&rdquo; with the same components as the input arguments, but where some components (namely, X, Xhat, X.labeled, Y.labeled) are now arrays containing these values from each combined object.  The lists of lists contained in the X.feats and Y.feats include one long list of lists containing all of the individual features from each object.
</p>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>See Also</h3>

<p>Functions that create objects of class &ldquo;features&rdquo; and &ldquo;matched&rdquo;: <code><a href="#topic+FeatureFinder">FeatureFinder</a></code>, <code><a href="#topic+centmatch">centmatch</a></code>, and <code><a href="#topic+deltamm">deltamm</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># TO DO
</code></pre>

<hr>
<h2 id='compositer'>
Create Composite Features
</h2><span id='topic+compositer'></span><span id='topic+compositer.features'></span><span id='topic+compositer.combined'></span><span id='topic+compositer.matched'></span><span id='topic+plot.composited'></span>

<h3>Description</h3>

<p>After identifying features in a verification set, re-grid them so that their centroids are all the same, and the new grid is as small as possible to completely contain all of the features in the verification set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compositer(x, level = 0, verbose = FALSE, ...)

## S3 method for class 'features'
compositer(x, level = 0, verbose = FALSE, ...)

## S3 method for class 'matched'
compositer(x, level = 0, verbose = FALSE, ...)

## S3 method for class 'combined'
compositer(x, level = 0, verbose = FALSE, ...) 

## S3 method for class 'composited'
plot(x, ..., type = c("all", "X", "Xhat", "X|Xhat", "Xhat|X"),
    dist.crit = 100, FUN = "mean", col = c("gray", tim.colors(64)))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="compositer_+3A_x">x</code></td>
<td>

<p><code>compositer</code>: an object of class &ldquo;features&rdquo;, &ldquo;matched&rdquo;, or &ldquo;combined&rdquo;.
</p>
<p><code>plot</code>: an object of class &ldquo;composited&rdquo;.
</p>
</td></tr>
<tr><td><code id="compositer_+3A_type">type</code></td>
<td>
<p>character, stating which composite features should be plotted.  Default makes a two by two panel of plots with all of the choices.</p>
</td></tr>
<tr><td><code id="compositer_+3A_dist.crit">dist.crit</code></td>
<td>
<p>maximum value beyond which any minimum centroid distances are considered too far for features to be &ldquo;present&rdquo; in the area.</p>
</td></tr>
<tr><td><code id="compositer_+3A_fun">FUN</code></td>
<td>
<p>name of a function to be applied to the composites in order to give the distributional summary.</p>
</td></tr>
<tr><td><code id="compositer_+3A_col">col</code></td>
<td>
<p>color pallette to be used.</p>
</td></tr>
<tr><td><code id="compositer_+3A_level">level</code></td>
<td>
<p>numeric used in shrinking the grid to a smaller size.</p>
</td></tr>
<tr><td><code id="compositer_+3A_verbose">verbose</code></td>
<td>

<p>logical, should information be printed to the screen?
</p>
</td></tr>
<tr><td><code id="compositer_+3A_...">...</code></td>
<td>

<p>Not used by <code>compositer</code>.  
</p>
<p><code>plot</code>: Optional arguments to <code>image.plot</code> (only for the scale legend), such as <code>legend.only</code>, <code>legend.lab</code>, <code>legend.mar</code>, <code>horizontal</code>, etc.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is functionality for performing an analysis similar to the composite verification method of Nachamkin (2004).  See also Nachamkin et al. (2005) and Nachamkin (2009).  The main difference is that this function centers all features to the same point, then re-sizes the grid to the smallest possible size to contain all features.  The &quot;existence&quot; of a feature at the same time point is determined by the centroid distance (because, here, the compositing is done for a large field rather than a small area), but it does not allow for having half of the feature in the domain in order to be considered.
</p>
<p><code>compositer</code> takes an object of class &ldquo;features&rdquo; or &ldquo;matched&rdquo; and centers all of the identified features onto the same point so that all features have the same centroid.  It also then re-grids the composited features so that they are contained on the smallest possible domain that includes all of the features in the verification set.
</p>
<p>Generally, because the composite approach is distributional in nature, it makes sense to look at features across multiple time points.  The function <code>combiner</code> allows for combining features from more than one object of class &ldquo;features&rdquo; or &ldquo;matched&rdquo; in order to subsequently run with <code>compositer</code>.
</p>
<p><code>plot</code> takes the composite features and adds them together creating a density of the composite features, then, Depending on the <code>type</code> argument, the verification (<code>type</code> = &ldquo;X&rdquo;), model (<code>type</code> = &ldquo;Xhat&rdquo;), verification conditioned on the model (<code>type</code> = &ldquo;X|Xhat&rdquo;), or the model conditioned on the verification composite features are plotted.  In the case of <code>type</code> = &ldquo;all&rdquo;, then a panel of four plots are made with all of these choices.  In the case of the conditional plots, the sum of composites for one field are masked out so that only the density of the other field is plotted where composited features from the first field exist.
</p>


<h3>Value</h3>

<p>A list object of class &ldquo;composited&rdquo; is returned with all of the same components and attributes as the x argument, but with additional components:
</p>
<table>
<tr><td><code>distances</code></td>
<td>
<p>List with components X and Xhat giving the minimum centroid distances from each feature in X (Xhat) to a feature in the other field (used for determining the conditional distributions; i.e., a feature is present if its centroid distance is less than some pre-specified amount).</p>
</td></tr>
<tr><td><code>Xcentered</code>, <code>Ycentered</code></td>
<td>
<p>list of &ldquo;owin&rdquo; objects containing each feature similar to X.feats and Y.feats, but centered on the same spot and re-gridded</p>
</td></tr>
</table>


<h3>Note</h3>

<p>centroids are rounded to the nearest whole number so that interpolation is not necessary.  This may introduce a slight bias in results, but it should not be a major issue.
</p>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>References</h3>

<p>Nachamkin, J. E. (2004) Mesoscale verification using meteorological composites. <em>Mon. Wea. Rev.</em>, <b>132</b>, 941&ndash;955.
</p>
<p>Nachamkin, J. E. (2009) Application of the Composite Method to the Spatial Forecast Verification Methods Intercomparison Dataset. <em>Wea. Forecasting</em>, <b>24</b> (5), 1390&ndash;1400, DOI: 10.1175/2009WAF2222225.1.
</p>
<p>Nachamkin, J. E., Chen, S. and Schmidt, J. S. (2005) Evaluation of heavy precipitation forecasts using composite-based methods: A distributions-oriented approach. <em>Mon. Wea. Rev.</em>, <b>133</b>, 2163&ndash;2177.
</p>


<h3>See Also</h3>

<p>Identifying features: <code><a href="#topic+FeatureFinder">FeatureFinder</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- y &lt;- matrix(0, 100, 100)
x[2:3,c(3:6, 8:10)] &lt;- 1
y[c(4:7, 9:10),c(7:9, 11:12)] &lt;- 1

x[30:50,45:65] &lt;- 1
y[c(22:24, 99:100),c(50:52, 99:100)] &lt;- 1

hold &lt;- make.SpatialVx( x, y, field.type = "contrived", units = "none",
    data.name = "Example", obs.name = "x", model.name = "y" )

look &lt;- FeatureFinder(hold, smoothpar=0.5)

look2 &lt;- compositer(look)
plot(look2, horizontal = TRUE)

</code></pre>

<hr>
<h2 id='CSIsamples'>
Forecast Verification with Cluster Analysis: The Variation
</h2><span id='topic+CSIsamples'></span><span id='topic+CSIsamples.default'></span><span id='topic+CSIsamples.SpatialVx'></span><span id='topic+summary.CSIsamples'></span><span id='topic+plot.CSIsamples'></span><span id='topic+plot.summary.CSIsamples'></span><span id='topic+print.CSIsamples'></span>

<h3>Description</h3>

<p>A variation on cluster analysis for forecast verification as proposed by Marzban and Sandgathe (2008).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CSIsamples(x, ...)

## Default S3 method:
CSIsamples(x, ..., xhat, nbr.csi.samples = 100, threshold = 20, 
    k = 100, width = 25, stand = TRUE, z.mult = 0, hit.threshold = 0.1, 
    max.csi.clust = 100, diss.metric = "euclidean", linkage.method = "average", 
    verbose = FALSE)

## S3 method for class 'SpatialVx'
CSIsamples(x, ..., time.point = 1, obs = 1, model = 1, nbr.csi.samples = 100, 
    threshold = 20, k = 100, width = 25, stand = TRUE, z.mult = 0, 
    hit.threshold = 0.1, max.csi.clust = 100, diss.metric = "euclidean", 
    linkage.method = "average", verbose = FALSE)

## S3 method for class 'CSIsamples'
summary(object, ...)

## S3 method for class 'CSIsamples'
plot(x, ...)

## S3 method for class 'summary.CSIsamples'
plot(x, ...)

## S3 method for class 'CSIsamples'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CSIsamples_+3A_x">x</code>, <code id="CSIsamples_+3A_xhat">xhat</code></td>
<td>
<p>default method: matrices giving the verification and forecast fields, resp.
</p>
<p>&ldquo;SpatialVx&rdquo; method: <code>x</code> is an object of class &ldquo;SpatialVx&rdquo;.
</p>
<p><code>plot</code>, <code>print</code> methods:  list object of class &ldquo;CSIsamples&rdquo; or &ldquo;summary.CSIsamples&rdquo; (in the case of <code>plot</code>).
</p>
</td></tr>
<tr><td><code id="CSIsamples_+3A_object">object</code></td>
<td>
<p>list object of class &ldquo;CSIsamples&rdquo;.</p>
</td></tr>
<tr><td><code id="CSIsamples_+3A_nbr.csi.samples">nbr.csi.samples</code></td>
<td>
<p>integer giving the number of samples to take at each level of the CA.</p>
</td></tr>
<tr><td><code id="CSIsamples_+3A_threshold">threshold</code></td>
<td>
<p>numeric giving a value over which is to be considered an event.</p>
</td></tr>
<tr><td><code id="CSIsamples_+3A_k">k</code></td>
<td>
<p>numeric giving the value for <code>centers</code> in the call to <code>kmeans</code>.</p>
</td></tr>
<tr><td><code id="CSIsamples_+3A_width">width</code></td>
<td>
<p>numeric giving the size of the samples for each cluster sample.</p>
</td></tr>
<tr><td><code id="CSIsamples_+3A_stand">stand</code></td>
<td>
<p>logical, should the data first be standardized before applying CA?</p>
</td></tr>
<tr><td><code id="CSIsamples_+3A_z.mult">z.mult</code></td>
<td>
<p>numeric giving a value by which to multiply the z- component.  If zero, then the CA is performed on locations only.  Can be used to give more or less weight to the actual values at these locations.</p>
</td></tr>
<tr><td><code id="CSIsamples_+3A_hit.threshold">hit.threshold</code></td>
<td>
<p>numeric between zero and one giving the threshold for the proportion of a cluster that is from the verification field vs the forecast field used for determining whether the cluster consitutes a hit (vs false alarm or miss depending).</p>
</td></tr>
<tr><td><code id="CSIsamples_+3A_max.csi.clust">max.csi.clust</code></td>
<td>
<p>integer giving the maximum number of clusters allowed.</p>
</td></tr>
<tr><td><code id="CSIsamples_+3A_diss.metric">diss.metric</code></td>
<td>
<p>character giving which <code>method</code> to use in the call to <code>dist</code> (which dissimilarity metric should be used?).</p>
</td></tr>
<tr><td><code id="CSIsamples_+3A_linkage.method">linkage.method</code></td>
<td>
<p>character giving the name of a linkage method acceptable to the <code>method</code> argument from the <code>hclust</code> function of package <span class="pkg">fastcluster</span>.</p>
</td></tr>
<tr><td><code id="CSIsamples_+3A_time.point">time.point</code></td>
<td>
<p>numeric or character indicating which time point from the &ldquo;SpatialVx&rdquo; verification set to select for analysis.</p>
</td></tr>
<tr><td><code id="CSIsamples_+3A_obs">obs</code>, <code id="CSIsamples_+3A_model">model</code></td>
<td>
<p>numeric indicating which observation/forecast model to select for the analysis.</p>
</td></tr>
<tr><td><code id="CSIsamples_+3A_verbose">verbose</code></td>
<td>
<p>logical, should progress information be printed to the screen?</p>
</td></tr>
<tr><td><code id="CSIsamples_+3A_...">...</code></td>
<td>

<p>Not used by <code>CSIsamples</code> method functions.
</p>
<p><code>summary</code> method function: the argument <code>silent</code> may be specified, which is a logical stating whether to print the information to the screen (FALSE) or not (TRUE).  If not given, the summary information will be printed to the screen.
</p>
<p>Not used by the <code>plot</code> method function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function carries out the procedure described in Marzban and Sandgathe (2008) for verifying forecasts.  Effectively, it combines the verification and forecast fields (keeping track of which values belong to which field) and applies CA to the combined field.  Clusters identified with a proportion of values belonging to the verification field within a certain range (defined by the hit.threshold argument) are determined to be hits, misses or false alarms.  From this information, the CSI (at each number of clusters; scale) is calculated.  A sampling scheme is used to speed up the process.
</p>
<p>The <code>plot</code> and <code>summary</code> functions all give the same information, but in different formats: i.e., CSI by number of clusters (scale).
</p>


<h3>Value</h3>

<p>A list is returned by CSIsamples with components:
</p>
<table>
<tr><td><code>data.name</code></td>
<td>
<p>character vector giving the names of the verification and forecast fields analyzed, resp.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>an object of class &ldquo;call&rdquo; giving the function call.</p>
</td></tr>
<tr><td><code>results</code></td>
<td>
<p>max.csi.clust by nbr.csi.samples matrix giving the caluclated CSI for each sample and iteration of CA.</p>
</td></tr>
</table>
<p>The summary method function invisibly returns the same list, but with the additional component:
</p>
<table>
<tr><td><code>csi</code></td>
<td>
<p>vector of length max.csi.clust giving the sample average CSI for each iteration of CA.</p>
</td></tr>
</table>
<p>The plot method functions do not return anything.  Plots are created.
</p>


<h3>Note</h3>

<p>Special thanks to Caren Marzban, marzban &ldquo;at&rdquo; u.washington.edu, for making the CSIsamples (originally called csi.samples) function available for use with this package.
</p>


<h3>Author(s)</h3>

<p>Hillary Lyons, h.lyons &ldquo;at&rdquo; comcast.net, and modified by Eric Gilleland
</p>


<h3>References</h3>

<p>Marzban, C., Sandgathe, S. (2008) Cluster Analysis for Object-Oriented Verification of Fields: A Variation. <em>Mon. Wea. Rev.</em>, <b>136</b>, (3), 1013&ndash;1025.
</p>


<h3>See Also</h3>

<p><code><a href="fastcluster.html#topic+hclust">hclust</a></code>, <code><a href="stats.html#topic+hclust">hclust</a></code>, <code><a href="stats.html#topic+kmeans">kmeans</a></code>, <code><a href="#topic+clusterer">clusterer</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
grid&lt;- list( x= seq( 0,5,,100), y= seq(0,5,,100))
obj&lt;-Exp.image.cov( grid=grid, theta=.5, setup=TRUE)
look&lt;- sim.rf( obj)
look2 &lt;- sim.rf( obj)

res &lt;- CSIsamples(x=look, xhat=look2, 10, threshold=0, k=100,
                  width=2, z.mult=0, hit.threshold=0.25, max.csi.clust=75)
plot(res)
y &lt;- summary(res)
plot(y)

## End(Not run)
## Not run: 
data( "UKfcst6" )
data( "UKobs6" )
data( "UKloc" )

hold &lt;- make.SpatialVx(UKobs6, UKfcst6, thresholds=0,
    loc=UKloc, map=TRUE, field.type="Rainfall", units="mm/h",
    data.name = "Nimrod", obs.name = "obs 6", model.name = "fcst 6" )

res &lt;- CSIsamples( hold, threshold = 0, k = 200, z.mult = 0.3, hit.threshold = 0.2,
                  max.csi.clust = 150, verbose = TRUE)
plot( res )

summary( res )

y &lt;- summary( res )

plot( y )

## End(Not run)
</code></pre>

<hr>
<h2 id='deltamm'>
Merge and/or Match Identified Features Within Two Fields
</h2><span id='topic+deltamm'></span><span id='topic+plot.matched'></span><span id='topic+print.matched'></span><span id='topic+summary.matched'></span><span id='topic+centmatch'></span>

<h3>Description</h3>

<p>Merge and/or match identified features within two fields using the delta metric method described in Gilleland et al. (2008), or the matching only method of Davis et al. (2006a).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>deltamm(x, p = 2, max.delta = Inf, const = Inf, type = c( "sqcen", "original" ),
    N = NULL, verbose = FALSE, ...)

centmatch(x, criteria = 1, const = 14, distfun = "rdist", areafac = 1,
    verbose = FALSE, ...)

## S3 method for class 'matched'
plot(x, mfrow = c(1, 2), ...)

## S3 method for class 'matched'
print(x, ...)

## S3 method for class 'matched'
summary(object, ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="deltamm_+3A_x">x</code></td>
<td>

<p>For <code>deltamm</code> and <code>centmatch</code>, an object of class &ldquo;features&rdquo; with components <code>X.labeled</code>, <code>Y.labeled</code> (matrices with numbered features), as well as, <code>X.feats</code> and <code>Y.feats</code>, each of which are list objects containing numbered components within which are objects of class &ldquo;owin&rdquo; containing logical matrices that define features for the forecast (Y) and verification (X) fields, resp.  For example, as returned by the <code>FeatureFinder</code> function.  For <code>plot</code> an object of class &ldquo;matched&rdquo;.  Argument <code>y</code> is used if it is not NULL, otherwise argument <code>x</code> is used (but only one of <code>x</code> or <code>y</code> is used).  If <code>x</code> and <code>y</code> are missing, but not <code>object</code>, then <code>object</code> will be used, in which case it must be of class &ldquo;features&rdquo;.
</p>
</td></tr>
<tr><td><code id="deltamm_+3A_object">object</code></td>
<td>
<p>list object of class &ldquo;matched&rdquo;.</p>
</td></tr>
<tr><td><code id="deltamm_+3A_p">p</code></td>
<td>
<p>Baddeley delta metric parameter.  A value of 1 gives arithmetic averages, Inf gives the Hausdorff metric and -Inf gives a minimum.  The default of 2 is most common.</p>
</td></tr>
<tr><td><code id="deltamm_+3A_max.delta">max.delta</code></td>
<td>
<p>single numeric giving a cut-off value for delta that disallows two features to be merged or matched if the delta between them is larger than this value.</p>
</td></tr>
<tr><td><code id="deltamm_+3A_const">const</code></td>
<td>
<p><code>deltamm</code>: a constant value over which the shortest distances in a distance map are set.  See Gilleland (2011) and Schwedler and Baldwin (2011) for more information about this parameter.</p>
</td></tr>
<tr><td><code id="deltamm_+3A_type">type</code></td>
<td>
<p> character specifying whether Baddeley's delta metric should be calculated after centering object pairs on a new square grid (default) or performed in their original positions on the original grid.  </p>
</td></tr>
<tr><td><code id="deltamm_+3A_n">N</code></td>
<td>
<p> If <code>type</code> is &ldquo;sqcen&rdquo;, then <code>N</code> is the argument to <code>censqdelta</code> that specifies the common grid size.  If not specified the maximum side of the original domain is used (possibly adding one first to make it an odd number).  It is possible that values could be pushed off the new grid, and making <code>N</code> larger might alleviate the issue. </p>
</td></tr>
<tr><td><code id="deltamm_+3A_centmatch">centmatch</code></td>
<td>
<p> numeric giving the number of grid squares whereby if the centroid distance (D) is less than this value, a match is declared (only used if <code>criteria</code> is 3.
</p>
</td></tr>
<tr><td><code id="deltamm_+3A_criteria">criteria</code></td>
<td>
<p>1, 2 or 3 telling which criteria for determining a match based on centroid distance, D, to use.  The first (1) is a match if D is less than the sum of the sizes of the two features in question (size is the square root of the area of the feature).  The second is a match if D is less than the average size of the two features in question.  The third is a match if D is less than a constant given by the argument <code>const</code>.</p>
</td></tr>
<tr><td><code id="deltamm_+3A_distfun">distfun</code></td>
<td>
<p>character string naming a distance function.  Default uses <code>rdist</code> from the <span class="pkg">fields</span> package.</p>
</td></tr>
<tr><td><code id="deltamm_+3A_areafac">areafac</code></td>
<td>
<p>single numeric used to multiply by grid-space based area in order to at least approximate the correct distance (e.g., using the ICP test cases, 4 would make the areas approximately square km instead of grid points).  This should not be used unless <code>distfun</code> is &ldquo;rdist.earth&rdquo; in which case it will use the locations given in the call to <code>make.SpatialVx</code>, which are assumed to be lat/lon coordinates.</p>
</td></tr>
<tr><td><code id="deltamm_+3A_mfrow">mfrow</code></td>
<td>
<p>mfrow parameter (see help file for <code>par</code>).  If NULL, then the parameter is not re-set.  </p>
</td></tr>
<tr><td><code id="deltamm_+3A_verbose">verbose</code></td>
<td>

<p>logical, should progress information be printed to the screen?
</p>
</td></tr>
<tr><td><code id="deltamm_+3A_...">...</code></td>
<td>

<p>For <code>deltamm</code>: additional optional arguments to the <code>distmap</code> function from package <span class="pkg">spatstat</span>.  For <code>centmatch</code>: optional arguments to <code>distfun</code>.  For <code>plot.matched</code>, additional arguments to <code>image.plot</code> concerning the color legend bar only.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>deltamm</code>:
</p>
<p>Gilleland et al. (2008) describe a method for automatically merging, and simultaneously, matching identified features within two fields (a verification set).  The method was proposed with the general method for spatial forecast verification introduced by Davis et al. (2006 a,b) in mind.  It relies heavily on use of a binary image metric introduced by Baddeley (1992a,b) for comparing binary images; henceforth referred to as the delta metric, or just delta.
</p>
<p>The procedure is as follows.  Suppose there are m identified forecast features and n identified verification features.
</p>
<p>1. Compute delta for each feature identified in the forecast field against each feature identified in the verification field.  Store these values in an m by n matrix, Upsilon.
</p>
<p>2. For each of the m rows of Upsilon, rank the values of delta to identify the features, j_1, ..., j_n that provide the lowest (best) to highest (worst) value, and do the same for each of the n columns to find the forecast features i1, ...,i_m that yield the lowest to highest values for each verification feature.
</p>
<p>3. Create a new m by n matrix, Psi, whose columns contain delta computed between each of the individual features in the forecast and (first column) the corresponding j_1 feature from the verification field, and each successive column, k, has delta between the i-th forecast feature and the union of j_1, j_2, ..., j_k.
</p>
<p>4. Create a similar m by n matrix, Ksi, that has delta computed between each individual feature in the verification field and the successively bigger unions i_1, ..., i_l for the l-th column.
</p>
<p>5. Let Q=[Upsilon, Psi, Ksi], and merge and match features based on the rankings of delta in Q.  That is, find the smallest delta in Q, and determine which mergings (if any) and matchings correspond to this value.  Remove the appropriate row(s) and column(s) of Q corresponding to the already determined matchings and/or mergings.  Repeat this until all features in at least one field have been exhausted.
</p>
<p>The above algorithm suffers from two deficiencies.  First, features that are merged in one field cannot be matched to merged features in another field.  One possible remedy for this is to run this algorithm twice, though this is not a universally good solution.  Second, features can be merged and/or matched to features that are very different from each other.  A possible remedy for this is to use the cut-off argument, max.delta, to disallow mergings or matchings between features whose delta value is not &lt;= this cut-off.  In practice, these two deficiencies are not likely very problematic.
</p>
<p><code>centmatch</code>:
</p>
<p>This function works similarly as <code>deltamm</code>, though it does not merge features.  It is based on the method proposed by Davis et al. (2006a).  It is possible for more than one object to be matched to the same object in another field.  As a result, when plotting, it might appear that features have been merged, but they have not been.  For informational purposes, the criteria, appelled <code>criteria.values</code> (as determined by the <code>criteria</code> argument), along with the centroid distance matrix, appelled <code>centroid.distances</code>, are returned.
</p>
<p><code>plot</code>: The plot method function for matched features plots matched features across fields in the same color using <code>rainbow</code>.  Unmatched features in either field are all colored gray.  Zero values are colored white.  The function <code>MergeForce</code> must first be called, however, in order to organize the object into a format that allows the <code>plot</code> method function to determine the correct color coding.
</p>
<p>The <code>print</code> method function will tell you which features matched between fields, so one can plot the originally derived features (e.g., from <code>FeatureFinder</code>) to identify matched features.
</p>
<p><code>summary</code>:
</p>
<p>The summary method function so far simply reverts the class back to &ldquo;features&rdquo; and calls that summary function.
</p>


<h3>Value</h3>

<p>A list object of class &ldquo;matched&rdquo; is returned by both centmatch and deltamm containing several components added to the value of x or y passed in, and possibly with attributes inhereted from object.
</p>
<table>
<tr><td><code>match.message</code></td>
<td>
<p>A character string stating how features were matched.</p>
</td></tr>
<tr><td><code>match.type</code></td>
<td>
<p>character string naming the matching function used.</p>
</td></tr>
<tr><td><code>matches</code></td>
<td>
<p>two-column matrix with forecast object numbers in the first column and corresponding matched observed features in the second column.  If no matches, this will have value integer(0) for each column giving a matrix with dimension 0 by 2.</p>
</td></tr>
<tr><td><code>unmatched</code></td>
<td>
<p>list with components X and Xhat giving the unmatched object numbers, if any, from the observed and forecast fields, resp.  If none, the value will be integer(0).</p>
</td></tr>
<tr><td><code>Q</code></td>
<td>
<p>(deltamm only) an array of dimension n by m by 3 giving all of the delta values that were computed in determining the mergings and matchings.</p>
</td></tr>
<tr><td><code>criteria</code></td>
<td>
<p>(centmatch only) 1, 2, or 3 as given by the criteria argument.</p>
</td></tr>
<tr><td><code>criteria.values</code>, <code>centroid.distances</code></td>
<td>
<p>(centmatch only) matrices giving the forecast by observed object criteria and centroid distances.</p>
</td></tr>
<tr><td><code>implicit.merges</code></td>
<td>
<p>(centmatch only) list displaying multiple matches for each field (this could define potential merges).  Each component of the list is a unified set of matched features in the form of two-column matrices analogous to the matches component.  If there are no implicit mergings or no matched features, this component will be named, but also NULL.  Note: such implicit mergings may or may not make physical sense, and are not considered to be merged generally, but will show up as having been merged/clustered when plotted.</p>
</td></tr>
</table>
<p>If the argument &lsquo;object&rsquo; is passed in, then the list object will also contain nearly the same attributes, with the data.name attribute possibly changed to reflect the specific model used.  It will also contain a time.point and model attribute.
</p>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>References</h3>

<p>Baddeley, A. (1992a)  An error metric for binary images.  In <em>Robust Computer Vision Algorithms</em>, W. Forstner and S. Ruwiedel, Eds., Wichmann, 59&ndash;78.
</p>
<p>Baddeley, A. (1992b)  Errors in binary images and an Lp version of the Hausdorff metric.  <em>Nieuw Arch. Wiskunde</em>, <b>10</b>, 157&ndash;183.
</p>
<p>Davis, C. A., Brown, B. G. and Bullock, R. G. (2006a) Object-based verification of precipitation forecasts, Part I: Methodology and application to mesoscale rain areas. <em>Mon. Wea. Rev.</em>, <b>134</b>, 1772&ndash;1784.
</p>
<p>Davis, C. A., Brown, B. G. and Bullock, R. G. (2006b) Object-based verification of precipitation forecasts, Part II: Application to convective rain systems. <em>Mon. Wea. Rev.</em>, <b>134</b>, 1785&ndash;1795.
</p>
<p>Gilleland, E. (2011) Spatial Forecast Verification: Baddeley's Delta Metric Applied to the ICP Test Cases. <em>Wea. Forecasting</em>, <b>26</b> (3), 409&ndash;415.
</p>
<p>Gilleland, E., Lee, T. C. M.,  Halley Gotway, J., Bullock, R. G. and Brown, B. G. (2008) Computationally efficient spatial forecast verification using Baddeley's delta image metric.  <em>Mon. Wea. Rev.</em>, <b>136</b>, 1747&ndash;1757.
</p>
<p>Schwedler, B. R. J. and Baldwin, M. E. (2011) Diagnosing the sensitivity of binary image measures to bias, location, and event frequency within a forecast verification framework. <em>Wea. Forecasting</em>, <b>26</b>, 1032&ndash;1044.
</p>


<h3>See Also</h3>

<p>To identify features: <code><a href="#topic+FeatureFinder">FeatureFinder</a></code>
</p>
<p><code><a href="#topic+minboundmatch">minboundmatch</a></code> is another feature matching function.
</p>
<p>To force merges (implicit or otherwise): <code><a href="#topic+MergeForce">MergeForce</a></code>, 
</p>
<p>Other functions used to identify features within the above mentioned functions (all from <span class="pkg">spatstat</span>):
</p>
<p><code><a href="#topic+disjointer">disjointer</a></code>, <code><a href="spatstat.geom.html#topic+deltametric">deltametric</a></code>, <code><a href="spatstat.geom.html#topic+owin">owin</a></code>, <code><a href="spatstat.geom.html#topic+tess">tess</a></code>, <code><a href="spatstat.geom.html#topic+tiles">tiles</a></code>, <code><a href="spatstat.geom.html#topic+connected">connected</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
x &lt;- y &lt;- matrix(0, 100, 100)
x[2:3,c(3:6, 8:10)] &lt;- 1
y[c(4:7, 9:10),c(7:9, 11:12)] &lt;- 1

x[30:50,45:65] &lt;- 1
y[c(22:24, 99:100),c(50:52, 99:100)] &lt;- 1

hold &lt;- make.SpatialVx( x, y, field.type = "contrived", units = "none",
    data.name = "Example", obs.name = "x", model.name = "y" )

look &lt;- FeatureFinder( hold, smoothpar = 0.5 ) 

# The next line fails because the centering pushes one object out of the new domain.
# look2 &lt;- deltamm( look )
# Setting N larger fixes the problem.
look2 &lt;- deltamm( look, N = 300 )
look2 &lt;- MergeForce( look2 )

look2

plot( look2 )

FeatureTable(look2)

look3 &lt;- centmatch(look)

FeatureTable(look3)

look3 &lt;- MergeForce( look3 )

plot( look3 )

## End(Not run)

</code></pre>

<hr>
<h2 id='disjointer'>
Identify Disjoint Sets of Connected Components
</h2><span id='topic+disjointer'></span>

<h3>Description</h3>

<p>Identify disjoint sets of contiguous events in a binary field.  In many areas of research, this function finds connected components.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>disjointer(x, method = "C")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="disjointer_+3A_x">x</code></td>
<td>

<p>A numeric matrix or other object that <code>as.im</code> from package <span class="pkg">spatstat</span> works on.
</p>
</td></tr>
<tr><td><code id="disjointer_+3A_method">method</code></td>
<td>

<p>Same argument as that in <code>connected</code> from package <code>spatstat</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>disjointer</code> essentially follows the help file for <code>connected</code> to produce a list object where each component is an image describing one set of connected components (or blobs).  It is essentially a wrapper function to <code>connected</code>.  This function is mainly used internally by <code>FeatureFinder</code> and similar, but could be of use outside such functions.
</p>


<h3>Value</h3>

<p>An unnamed list object where each component is an image describing one set of connected components (or blobs).
</p>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>References</h3>

<p>Park, J.-M., Looney, C.G. and Chen, H.-C. (2000) Fast connected
component labeling algorithm using a divide and conquer technique.
Pages 373-376 in S.Y. Shin (ed) <em>Computers and Their
Applications:</em> Proceedings of the ISCA 15th International
Conference on Computers and Their Applications, March 29&ndash;31, 2000,
New Orleans, Louisiana USA. ISCA 2000, ISBN 1-880843-32-3.
</p>
<p>Rosenfeld, A. and Pfalz, J.L. (1966) Sequential operations in
digital processing.  <em>Journal of the Association for Computing
Machinery</em> <b>13</b> 471&ndash;494.
</p>


<h3>See Also</h3>

<p><code><a href="spatstat.geom.html#topic+connected">connected</a></code>, <code><a href="#topic+FeatureFinder">FeatureFinder</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>##
## For examples, see FeatureFinder
##
</code></pre>

<hr>
<h2 id='EBS'>
Elmore, Baldwin and Schultz Method for Field Significance for Spatial Bias Errors
</h2><span id='topic+EBS'></span><span id='topic+plot.EBS'></span>

<h3>Description</h3>

<p>Apply the method of Elmore, Baldwin and Schultz (2006) for calculating field significance of spatial bias errors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>EBS(object, model = 1, block.length = NULL, alpha.boot = 0.05,
    field.sig = 0.05, bootR = 1000, ntrials = 1000,
    verbose = FALSE)

## S3 method for class 'EBS'
plot(x, ..., mfrow = c(1, 2), col, horizontal)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="EBS_+3A_object">object</code></td>
<td>
<p> list object of class &ldquo;SpatialVx&rdquo;.
</p>
</td></tr>
<tr><td><code id="EBS_+3A_x">x</code></td>
<td>
<p>object of class &ldquo;EBS&rdquo; as returned by <code>EBS</code>.</p>
</td></tr>
<tr><td><code id="EBS_+3A_model">model</code></td>
<td>

<p>number or character describing which model (if more than one in the &ldquo;SpatialVx&rdquo; object) to compare.
</p>
</td></tr>
<tr><td><code id="EBS_+3A_block.length">block.length</code></td>
<td>

<p>numeric giving the block length to be used n the block bootstrap algorithm.  If NULL, floor(sqrt(n)) is used.
</p>
</td></tr>
<tr><td><code id="EBS_+3A_alpha.boot">alpha.boot</code></td>
<td>

<p>numeric between 0 and 1 giving the confidence level desired for the bootstrap algorithm.
</p>
</td></tr>
<tr><td><code id="EBS_+3A_field.sig">field.sig</code></td>
<td>

<p>numeric between 0 and 1 giving the desired field significance level.
</p>
</td></tr>
<tr><td><code id="EBS_+3A_bootr">bootR</code></td>
<td>

<p>numeric integer giving the number of bootstrap replications to use.
</p>
</td></tr>
<tr><td><code id="EBS_+3A_ntrials">ntrials</code></td>
<td>

<p>numeric integer giving the number of Monte Carol iterations to use.
</p>
</td></tr>
<tr><td><code id="EBS_+3A_mfrow">mfrow</code></td>
<td>
<p>mfrow parameter (see help file for <code>par</code>).  If NULL, then the parameter is not re-set.  </p>
</td></tr>
<tr><td><code id="EBS_+3A_col">col</code>, <code id="EBS_+3A_horizontal">horizontal</code></td>
<td>
<p>optional arguments to <code>image.plot</code> from <span class="pkg">fields</span>.</p>
</td></tr>
<tr><td><code id="EBS_+3A_verbose">verbose</code></td>
<td>

<p>logical, should progress information be printed to the screen?
</p>
</td></tr>
<tr><td><code id="EBS_+3A_...">...</code></td>
<td>
<p>optional arguments to <code>image.plot</code> from <span class="pkg">fields</span>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>this is a wrapper function for the <code>spatbiasFS</code> function utilizing the &ldquo;SpatialVx&rdquo; object class to simplify the arguments.
</p>


<h3>Value</h3>

<p>A list object of class &ldquo;EBS&rdquo; with the same attributes as the input object and additional attribute (called &ldquo;arguments&rdquo;)that is a named vector giving information provided by the user.  Components of the list include:
</p>
<table>
<tr><td><code>block.boot.results</code></td>
<td>
<p>object of class &ldquo;LocSig&rdquo;.</p>
</td></tr>
<tr><td><code>sig.results</code></td>
<td>
<p>list object containing information about the significance of the results.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>References</h3>

<p>Elmore, K. L., Baldwin, M. E. and Schultz, D. M. (2006) Field significance revisited: Spatial bias errors in forecasts as applied to the Eta model.  <em>Mon. Wea. Rev.</em>, <b>134</b>, 519&ndash;531.
</p>


<h3>See Also</h3>

<p><code><a href="boot.html#topic+boot">boot</a></code>, <code><a href="boot.html#topic+tsboot">tsboot</a></code>, <code><a href="#topic+spatbiasFS">spatbiasFS</a></code>, <code><a href="#topic+LocSig">LocSig</a></code>, <code><a href="fields.html#topic+poly.image">poly.image</a></code>, <code><a href="fields.html#topic+image.plot">image.plot</a></code>, <code><a href="#topic+make.SpatialVx">make.SpatialVx</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data( "GFSNAMfcstEx" )
data( "GFSNAMobsEx" )
data( "GFSNAMlocEx" )

id &lt;- GFSNAMlocEx[,"Lon"] &gt;=-95
id &lt;- id &amp; GFSNAMlocEx[,"Lon"] &lt;= -75
id &lt;- id &amp; GFSNAMlocEx[,"Lat"] &lt;= 32

##
## This next step is a bit awkward, but these data
## are not in the format of the SpatialVx class.
## These are being set up with arbitrarily chosen
## dimensions (49 X 48) for the spatial part.  It
## won't matter to the analyses or plots.
##
Vx &lt;- GFSNAMobsEx
Fcst &lt;- GFSNAMfcstEx
Ref &lt;- array(t(Vx), dim=c(49, 48, 361))
Mod &lt;- array(t(Fcst), dim=c(49, 48, 361)) 

hold &lt;- make.SpatialVx(Ref, Mod, loc=GFSNAMlocEx,
    projection=TRUE, map=TRUE, loc.byrow = TRUE, subset=id,
    field.type="Precipitation", units="mm",
    data.name = "GFS/NAM", obs.name = "Reference", model.name = "Model" )

look &lt;- EBS(hold, bootR=500, ntrials=500, verbose=TRUE)
plot( look )

## Not run: 
# Same as above, but now we'll do it for all points.
# A little slower, but not terribly bad.

hold &lt;- make.SpatialVx(Ref, Mod, loc = GFSNAMlocEx,
    projection = TRUE, map = TRUE, loc.byrow = TRUE,
    field.type = "Precipitation", reg.grid = FALSE, units = "mm",
    data.name = "GFS/NAM", obs.name = "Reference", model.name = "Model" )

look &lt;- EBS(hold, bootR=500, ntrials=500, verbose=TRUE)
plot( look )

## End(Not run)
</code></pre>

<hr>
<h2 id='ExampleSpatialVxSet'>
Simulated Spatial Verification Set
</h2><span id='topic+ExampleSpatialVxSet'></span>

<h3>Description</h3>

<p>A simulated spatial verification set for use by various examples for this package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(ExampleSpatialVxSet)</code></pre>


<h3>Format</h3>

<p>The format is:
List of 2
$ vx  : num [1:50, 1:50] 0 0 0 0 0 0 0 0 0 0 ...
$ fcst: num [1:50, 1:50] 0.0141 0 0 0 0 ...
</p>


<h3>Details</h3>

<p>The data here were generated using the <code>sim.rf</code> function from <span class="pkg">fields</span> (Furrer et al., 2012):
</p>
<p>x &lt;- y &lt;- matrix(0, 10, 12)
x[2:3,c(3:6, 8:10)] &lt;- 1
y[c(1:2, 9:10),c(3:6)] &lt;- 1
</p>
<p>grid &lt;- list(x=seq(0,5,,50), y=seq(0,5,,50))
obj &lt;- Exp.image.cov(grid=grid, theta=0.5, setup=TRUE)
x &lt;- sim.rf(obj)
x[x &lt; 0] &lt;- 0
x &lt;- zapsmall(x)
</p>
<p>y &lt;- sim.rf(obj)
y[y &lt; 0] &lt;- 0
y &lt;- zapsmall(y)
</p>


<h3>References</h3>

<p>Reinhard Furrer, Douglas Nychka and Stephen Sain (2012). fields: Tools for spatial data. R package version 6.6.3.  http://CRAN.R-project.org/package=fields
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data( "ExampleSpatialVxSet" )
x &lt;- ExampleSpatialVxSet$vx
xhat &lt;- ExampleSpatialVxSet$fcst
par(mfrow=c(1,2))
image.plot(x, col=c("gray",tim.colors(64)))
image.plot(xhat, col=c("gray",tim.colors(64)))
</code></pre>

<hr>
<h2 id='expvg'>
Exponential Variogram
</h2><span id='topic+expvg'></span><span id='topic+predict.flossdiff.expvg'></span><span id='topic+print.flossdiff.expvg'></span>

<h3>Description</h3>

<p>Compute the exponential variogram.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>expvg(p, vg, ...)

## S3 method for class 'flossdiff.expvg'
predict(object, newdata, ...)

## S3 method for class 'flossdiff.expvg'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="expvg_+3A_p">p</code></td>
<td>

<p>numeric vector of length two.  Each component should be positively valued.  The first component is the nugget and the second is the range parameter.
</p>
</td></tr>
<tr><td><code id="expvg_+3A_vg">vg</code></td>
<td>

<p>A list object with component <code>d</code> giving a numeric vector of distances over which the variogram is to be calculated.
</p>
</td></tr>
<tr><td><code id="expvg_+3A_object">object</code>, <code id="expvg_+3A_x">x</code></td>
<td>
<p>A list object returned by <code>flossdiff</code> using <code>expvg</code> as the variogram model.</p>
</td></tr>
<tr><td><code id="expvg_+3A_newdata">newdata</code></td>
<td>
<p>Numeric giving the distances over which to use the fitted exponential variogram model to make predictions.  The default is to go from zero to the maximum lag distance for a given data set, which is not the usual convention for the generic <code>predict</code>, which usually defaults to operate on the lags used in performing the fit.</p>
</td></tr>
<tr><td><code id="expvg_+3A_...">...</code></td>
<td>

<p>Not used.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A very simple function used mainly internally by <code>flossdiff</code> when fitting the exponential variogram to the empirical one, and by the <code>predict</code>, <code>print</code> and <code>summary</code> method functions for <code>lossdiff</code> objects.  For those wishing to use a different variogram model than the exponential, use this function and its method functions as a template.  Be sure to create <code>predict</code> and <code>print</code> method functions to operate on objects of class &ldquo;flossdiff.XXX&rdquo; where &ldquo;XXX&rdquo; is the name of the variogram function you write (so, &ldquo;expvg&rdquo; in the current example).
</p>


<h3>Value</h3>

<p>Numeric vector of length equal to that of the <code>d</code> component of <code>vg</code> giving the corresponding exponential variogram values with nugget and range defined by <code>p</code>.
</p>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>References</h3>

<p>Cressie, N. A. (2015) Statistics for Spatial Data.  Wiley-Interscience; Revised Edition edition (July 27, 2015), ISBN-10: 1119114616, ISBN-13: 978-1119114611, 928 pp.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+lossdiff">lossdiff</a></code>, <code><a href="#topic+flossdiff">flossdiff</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>##
## For examples, see lossdiff and flossdiff
##
</code></pre>

<hr>
<h2 id='expvgram'>
Exponential Variogram
</h2><span id='topic+expvgram'></span>

<h3>Description</h3>

<p>Calculates the empirical variogram for use with function spct.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>expvgram(p, h, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="expvgram_+3A_p">p</code></td>
<td>
<p> numeric vector of length two giving the nugget and range parameter values, resp.
</p>
</td></tr>
<tr><td><code id="expvgram_+3A_h">h</code></td>
<td>

<p>numeric vector of separation distances.
</p>
</td></tr>
<tr><td><code id="expvgram_+3A_...">...</code></td>
<td>

<p>Not used.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Simple function to work with <code>spct</code> to calculate the exponential variogram for given parameters and separation distances.  The exponential variogram employed here is parameterized by
</p>
<p>gamma(h) = sigma * ( 1 - exp( - h * theta ) )
</p>
<p>where <code>p</code> is <code>c( sigma, theta )</code>.
</p>


<h3>Value</h3>

<p>A numeric vector of variogram values for each separation distance in <code>h</code>.
</p>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>See Also</h3>

<p><code><a href="#topic+spct">spct</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># See help file for spct for examples.
</code></pre>

<hr>
<h2 id='FeatureAxis'>
Major and Minor Axes of a Feature
</h2><span id='topic+FeatureAxis'></span><span id='topic+plot.FeatureAxis'></span><span id='topic+summary.FeatureAxis'></span>

<h3>Description</h3>

<p>Calculate the major and minor axes of a feature and various other properties such as the aspect ratio.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>FeatureAxis(x, fac = 1, flipit = FALSE, twixt = FALSE)

## S3 method for class 'FeatureAxis'
plot(x, ..., zoom = FALSE)

## S3 method for class 'FeatureAxis'
summary(object, ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="FeatureAxis_+3A_x">x</code></td>
<td>

<p>For <code>FeatureAxis</code> this is an object of class &ldquo;owin&rdquo; containing a binary image matrix defining the feature.  In the case of <code>plot.FeatureAxis</code>, this is the value returned from <code>FeatureAxis</code>.
</p>
</td></tr>
<tr><td><code id="FeatureAxis_+3A_object">object</code></td>
<td>
<p>list object of class &ldquo;FeatureAxis&rdquo; as returned by <code>FeatureAxis</code>.</p>
</td></tr>
<tr><td><code id="FeatureAxis_+3A_fac">fac</code></td>
<td>

<p>numeric, in determining the lengths of the axes, they are multiplied by a factor of <code>fac</code> (e.g., if the grid points are k by k km each, then one could set this to k so that the resulting lengths are in terms of km rather than grid points.
</p>
</td></tr>
<tr><td><code id="FeatureAxis_+3A_flipit">flipit</code></td>
<td>
<p>logical, should the objects be flipped over x and y?  The disjointer function results in images that are flipped, this would flip them back.
</p>
</td></tr>
<tr><td><code id="FeatureAxis_+3A_twixt">twixt</code></td>
<td>

<p>logical, should the major axis angle be forced to be between +/- 90 degrees?
</p>
</td></tr>
<tr><td><code id="FeatureAxis_+3A_zoom">zoom</code></td>
<td>
<p>logical, should the object be plotted on its bounding box (TRUE) or on the original grid (FALSE, default)?  Useful if the feature is too small to be seen well on the original gid.</p>
</td></tr>
<tr><td><code id="FeatureAxis_+3A_...">...</code></td>
<td>
<p>For <code>plot.FeatureAxis</code> these are additional arguments to the <code>plot</code> function.  Not used by <code>summary.FeatureAxis</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function attempts to identify the major and minor axes for a pre-defined feature (sometimes referred to as an object).  This function relies heavily on the <span class="pkg">spatstat</span> and <span class="pkg">smatr</span> packages.  First, the convex hull of the feature is determined using the <code>convexhull</code> function from the <span class="pkg">spatstat</span> package.  The major axis is then found using the <code>sma</code> function from package <span class="pkg">smatr</span>, which is then converted into a <code>psp</code> object (see <code>as.psp</code> from <span class="pkg">spatstat</span>) from which the axis angle and length are found (using <code>angles.psp</code> and <code>lengths_psp</code>, resp., from <span class="pkg">spatstat</span>).
</p>
<p>The minor axis anlge is easily found after rotating the major axis 90 degrees using <code>rotate.psp</code> from <span class="pkg">spatstat</span>.  The length of the minor axis is more difficult.  Here, it is found by rotating the convex hull of the feature by the major axis angle (so that it is upright) using <code>rotate.owin</code> from <span class="pkg">spatstat</span>, and then computing the bounding box (using <code>boundingbox</code> from <span class="pkg">spatstat</span>).  The differnce is then taken between the range of x- coordinates of the bounding box.  This seems to give a reasonable value for the length of the minor axis.  A <code>psp</code> object is then created using the mid point of the major axis (which should be close to the centroid of the feature) using <code>as.psp</code> and <code>midpoints.psp</code> from <span class="pkg">spatstat</span> along with the length and angle already found for the minor axis.
</p>
<p>See the help files for the above mentioned functions for references, etc.
</p>


<h3>Value</h3>

<p>FeatureAxis: A list object of class &ldquo;FeatureAxis&rdquo; is returned with components:
</p>
<table>
<tr><td><code>z</code></td>
<td>
<p>same as the argument x passed in.</p>
</td></tr>
<tr><td><code>MajorAxis</code>, <code>MinorAxis</code></td>
<td>
<p>a psp object with one segment that is the major (minor) axis.</p>
</td></tr>
<tr><td><code>OrientationAngle</code></td>
<td>
<p>list with two components: MajorAxis (the angle in degrees of the major axis wrt the abscissa), MinorAxis (the angle in degrees wrt the abscissa).</p>
</td></tr>
<tr><td><code>aspect.ratio</code></td>
<td>
<p>numeric giving the ratio of the length of the minor axis to that of the major axis (always between 0 and 1).</p>
</td></tr>
<tr><td><code>MidPoint</code></td>
<td>
<p>an object of class &ldquo;ppp&rdquo; giving the mid point of the major (minor) axis.</p>
</td></tr>
<tr><td><code>lengths</code></td>
<td>
<p>list object with components: MajorAxis giving the length (possibly multiplied by a factor) of the major axis, and MinorAxis same as MajorAxis but for the minor axis.</p>
</td></tr>
<tr><td><code>sma.fit</code></td>
<td>
<p>The fitted object returned by the sma function.  This is useful, e.g., if confidence intervals for the axis are desired.  See the sma help file for more details.</p>
</td></tr>
</table>
<p>No value is returned from the <code>plot</code> or <code>summary</code> method functions.
</p>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>See Also</h3>

<p><code><a href="spatstat.geom.html#topic+owin">owin</a></code>, <code><a href="spatstat.geom.html#topic+convexhull">convexhull</a></code>, <code><a href="smatr.html#topic+sma">sma</a></code>, <code><a href="spatstat.geom.html#topic+as.psp">as.psp</a></code>, <code><a href="spatstat.geom.html#topic+angles.psp">angles.psp</a></code>, <code><a href="spatstat.geom.html#topic+rotate.owin">rotate.owin</a></code>, <code><a href="spatstat.geom.html#topic+rotate.psp">rotate.psp</a></code>, <code><a href="spatstat.geom.html#topic+boundingbox">boundingbox</a></code>, <code><a href="spatstat.geom.html#topic+midpoints.psp">midpoints.psp</a></code>, <code><a href="spatstat.geom.html#topic+lengths_psp">lengths_psp</a></code>, <code><a href="spatstat.geom.html#topic+infline">infline</a></code>, <code><a href="spatstat.geom.html#topic+clip.infline">clip.infline</a></code>, <code><a href="#topic+deltamm">deltamm</a></code>, <code><a href="#topic+FeatureFinder">FeatureFinder</a></code>, <code><a href="#topic+disjointer">disjointer</a></code>, <code><a href="spatstat.geom.html#topic+connected">connected</a></code>, <code><a href="spatstat.geom.html#topic+tiles">tiles</a></code>, <code><a href="spatstat.geom.html#topic+tess">tess</a></code>, <code><a href="spatstat.geom.html#topic+solutionset">solutionset</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data( "ExampleSpatialVxSet" )

x &lt;- ExampleSpatialVxSet$vx

look &lt;- disk2dsmooth(x,5)
u &lt;- quantile(look,0.99)
sIx &lt;- matrix(0, 100, 100)
sIx[ look &gt; u] &lt;- 1
look2 &lt;- disjointer(sIx)[[1]]
look2 &lt;- flipxy(look2)
tmp &lt;- FeatureAxis(look2)
plot(tmp)
summary(tmp)

## Not run: 
data( "pert000" )
data( "pert004" )
data( "ICPg240Locs" )

hold &lt;- make.SpatialVx( pert000, pert004,
    loc = ICPg240Locs, projection = TRUE, map = TRUE,
    loc.byrow = TRUE,
    field.type = "Precipitation", units = "mm/h",
    data.name = "Perturbed ICP Cases", obs.name = "pert000",
    model.name = "pert004" )

look &lt;- FeatureFinder(hold, smoothpar=10.5)
par(mfrow=c(1,2))
plot(look)

par(mfrow=c(2,2))
image.plot(look$X.labeled)
image.plot(look$Y.labeled)

# The next line will likely be very slow.
look2 &lt;- deltamm(x=look, verbose=TRUE)
image.plot(look2$X.labeled)
image.plot(look2$Y.labeled)

look2$mm.new.labels # the first seven features are matched.

ang1 &lt;- FeatureAxis(look2$X.feats[[1]])
ang2 &lt;- FeatureAxis(look2$Y.feats[[1]])
plot(ang1)
plot(ang2)
summary(ang1)
summary(ang2)

ang3 &lt;- FeatureAxis(look2$X.feats[[4]])
ang4 &lt;- FeatureAxis(look2$Y.feats[[4]])
plot(ang3)
plot(ang4)
summary(ang3)
summary(ang4)
   
## End(Not run)

</code></pre>

<hr>
<h2 id='FeatureFinder'>
Threshold-based Feature Finder
</h2><span id='topic+FeatureFinder'></span><span id='topic+plot.features'></span><span id='topic+print.features'></span><span id='topic+summary.features'></span><span id='topic+plot.summary.features'></span>

<h3>Description</h3>

<p>Identify spatial features within a verification set using a threshold-based method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>FeatureFinder(object, smoothfun = "disk2dsmooth", do.smooth = TRUE,
    smoothpar = 1, smoothfunargs = NULL, thresh = 1e-08, idfun = "disjointer",
    min.size = 1, max.size = Inf, fac = 1, zero.down = FALSE, time.point = 1,
    obs = 1, model = 1, ...)

## S3 method for class 'features'
plot(x, ..., type = c("both", "obs", "model"))

## S3 method for class 'features'
print(x, ...)

## S3 method for class 'features'
summary(object, ...)

## S3 method for class 'summary.features'
plot(x, ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="FeatureFinder_+3A_object">object</code></td>
<td>

<p>An object of class &ldquo;SpatialVx&rdquo;.
</p>
</td></tr>
<tr><td><code id="FeatureFinder_+3A_x">x</code></td>
<td>
<p>list object of class &ldquo;features&rdquo; as returned by <code>FeatureFinder</code>.</p>
</td></tr>
<tr><td><code id="FeatureFinder_+3A_smoothfun">smoothfun</code></td>
<td>

<p>character naming a 2-d smoothing function from package <span class="pkg">smoothie</span>.  Not used if <code>do.smooth</code> is FALSE.
</p>
</td></tr>
<tr><td><code id="FeatureFinder_+3A_do.smooth">do.smooth</code></td>
<td>

<p>logical, should the field first be smoothed before trying to identify features (resulting field will not be smoothed, this is just for identifying features).  Default is to do convolution smoothing using a disc kernel as is recommended by Davis et al (2006a).
</p>
</td></tr>
<tr><td><code id="FeatureFinder_+3A_smoothpar">smoothpar</code></td>
<td>

<p>numeric of length one or two giving the smoothing parameter for <code>smoothfun</code>.  If length is two, the first value is applied to the forecast field and the second to the verification field.  The default smooth function (<code>smoothfun</code> argument) is the disk kernel smoother, so this argument gives the radius of the disk.  See the help file for <code>hoods2dsmooth</code> from package <span class="pkg">smoothie</span> for other smoother choices; this argument corresponds to the <code>lambda</code> argument of these functions.</p>
</td></tr>
<tr><td><code id="FeatureFinder_+3A_smoothfunargs">smoothfunargs</code></td>
<td>

<p>list object with named additional arguments to <code>smoothfun</code>.
</p>
</td></tr>
<tr><td><code id="FeatureFinder_+3A_thresh">thresh</code></td>
<td>

<p>numeric vector of length one or two giving the threshold over which (inclusive) features should be identified.  If different thresholds are used for the forecast and verification fields, then the first element is the threshold for the forecast, and the second for the verification field.
</p>
</td></tr>
<tr><td><code id="FeatureFinder_+3A_idfun">idfun</code></td>
<td>

<p>character naming the function used to identify (and label) individual features in the thresholded, and possibly smoothed, fields.  Must take an argument 'x', the thresholded, and possibly smoothed, field.
</p>
</td></tr>
<tr><td><code id="FeatureFinder_+3A_min.size">min.size</code></td>
<td>

<p>numeric of length one or two giving the minimum number of contiguous grid points exceeding the threshold in order to be included as a feature (can be used to exclude any small features).  Default does not exclude any features.  If length is two, first value applies to the forecast and second to the verification field.
</p>
</td></tr>
<tr><td><code id="FeatureFinder_+3A_max.size">max.size</code></td>
<td>

<p>numeric of length one or two giving the maximum number of contiguous grid points exceeding the threshold in order to be included as a feature (can be used to exclude large features, if the need be).  Default does not exclude any features.  If length is two, then the first value applies ot the forecast field, and the second to the verification.
</p>
</td></tr>
<tr><td><code id="FeatureFinder_+3A_fac">fac</code></td>
<td>

<p>numeric of length one or two giving a factor by which to multiply the R quantile in determining the threshold from the fields.  For example, ~ 1/15 is suggested in Wernli et al (2008, 2009).  If length is two, then the first value applies to the threshold of the forecast and the second to that of the verification field.
</p>
</td></tr>
<tr><td><code id="FeatureFinder_+3A_zero.down">zero.down</code></td>
<td>

<p>logical, should negative values and relatively very small values be set to zero after smoothing the fields?  For thresholds larger than such values, this argument is moot.  'zapsmall' is used to set the very small positive values to zero.
</p>
</td></tr>
<tr><td><code id="FeatureFinder_+3A_time.point">time.point</code></td>
<td>

<p>numeric or character indicating which time point from the &ldquo;SpatialVx&rdquo; verification set to select for analysis.
</p>
</td></tr>
<tr><td><code id="FeatureFinder_+3A_obs">obs</code>, <code id="FeatureFinder_+3A_model">model</code></td>
<td>

<p>numeric indicating which observation/forecast model to select for the analysis.
</p>
</td></tr>
<tr><td><code id="FeatureFinder_+3A_type">type</code></td>
<td>

<p>character string stating which features to plot (observed, forecast or both).  If both, a panel of two plots will be made side-by-side.
</p>
</td></tr>
<tr><td><code id="FeatureFinder_+3A_...">...</code></td>
<td>

<p><code>FeatureFinder</code>: additional arguments to <code>idfun</code>.
</p>
<p><code>plot</code>: optional arguments to <code>image.plot</code> for the color legend bar only.
</p>
<p>Not used by the <code>print</code> or <code>summary</code> functions.
</p>
<p>The 'summary' method function can take the argument: 'silent'-logical, should information be printed to the screen (FALSE) or not (TRUE).
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>FeatureFinder</code> applies for finding features based on three proposed methods from different papers; and also allows for combinations of the methods.  The methods include: the convolution-threshold approach of Davis et al. (2006a,b), which uses a disc kernel convolution smoother to first smooth the fields, then applies a threshold to remove low-intensity areas.  Feautres are identified by groups of contiguous &ldquo;events&rdquo; (or connected components in the computer vision/image analysis literature) using <code>idfun</code>.  Nachamkin (2009) and Lack et al (2010) further require that features have at least <code>min.size</code> connected components in order to be considered a feature (in order to remove very small areas of threshold excesses).  Wernli et al. (2009) modify the threshold by a factor (see the <code>fac</code> argument).
</p>
<p>In addition to the above options, it is also possible to remove features that are too large, as for some purposes, it is the small-scale features that are of interest, and sometimes the larger features can cause problems when merging and matching features across fields.
</p>


<h3>Value</h3>

<p><code>FeatureFinder</code> returns a list object of class &ldquo;features&rdquo; with comopnents:
</p>
<table>
<tr><td><code>data.name</code></td>
<td>
<p>character vector naming the verification and forecast (R object) fields, resp.</p>
</td></tr>
<tr><td><code>X.feats</code>, <code>Y.feats</code></td>
<td>
<p>The identified features for the verification and forecast fields as returned by the idfun function.</p>
</td></tr>
<tr><td><code>X.labeled</code>, <code>Y.labeled</code></td>
<td>
<p>matrices of same dimension as the forecast and verification fields giving the images of the convolved and thresholded verification and forecast fields, but with each individually identified object labeled 1 to the number of objects in each field.</p>
</td></tr>
<tr><td><code>identifier.function</code>, <code>identifier.label</code></td>
<td>
<p>character strings naming the function and giving the long name (for use with plot method function).</p>
</td></tr>
</table>
<p>An additional attribute, named &ldquo;call&rdquo;, is given.  This attribute shows the original function call, and is used mainly by the print function..
</p>
<p>The plot method functions do not return anything.
</p>
<p>The summary method function for objects of class &ldquo;features&rdquo; returns a list with components:
</p>
<table>
<tr><td><code>X</code>, <code>Y</code></td>
<td>
<p>matrices whose rows are objects and columns are properties: centroidX and centroidY (the x- and y- coordinates for the feature centroids), area (the area of each feature in squared grid points), the orientation angle for the fitted major axis, the aspect ratio, Intensity0.25 and Intensity0.9 (the lower quartile and 0.9 quantile of intensity values for each feature).</p>
</td></tr>
</table>


<h3>Note</h3>

<p>This function replaces the now deprecated functions: <code>convthresh</code>, <code>threshsizer</code> and <code>threshfac</code>.
</p>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>References</h3>

<p>Davis, C. A., Brown, B. G. and Bullock, R. G. (2006a) Object-based verification of precipitation forecasts, Part I: Methodology and application to mesoscale rain areas. _Mon. Wea. Rev._, *134*, 1772-1784.
</p>
<p>Davis, C. A., Brown, B. G. and Bullock, R. G. (2006b) Object-based verification of precipitation forecasts, Part II: Application to convective rain systems. _Mon. Wea. Rev._, *134*, 1785-1795.
</p>
<p>Lack, S. A., Limpert, G. L. and Fox, N. I. (2010) An object-oriented multiscale verification scheme.  _Wea.  Forecasting_, *25*, 79-92, doi:10.1175/2009WAF2222245.1.
</p>
<p>Nachamkin, J. E. (2009) Application of the composite method to the spatial forecast verification methods intercomparison dataset.  _Wea. Forecasting_, *24*, 1390-1400, doi:10.1175/2009WAF2222225.1.
</p>
<p>Wernli, H., Paulat, M. Hagen, M. and Frei, C. (2008) SAL-A novel quality measure for the verification of quantitative precipitation forecasts.  _Mon. Wea. Rev._, *136*, 4470-4487.
</p>
<p>Wernli, H., Hofmann, C. and Zimmer, M. (2009) Spatial forecast verification methods intercomparison project: Application of the SAL technique.  _Wea. Forecasting_, *24*, 1472-1484, doi:10.1175/2009WAF2222271.1.
</p>


<h3>See Also</h3>

<p>Functions used in identifying the features (mostly from package <span class="pkg">spatstat</span>:
</p>
<p><code><a href="spatstat.geom.html#topic+connected">connected</a></code>, <code><a href="spatstat.geom.html#topic+as.im">as.im</a></code>, <code><a href="spatstat.geom.html#topic+tess">tess</a></code>, <code><a href="spatstat.geom.html#topic+tiles">tiles</a></code>, <code><a href="spatstat.geom.html#topic+owin">owin</a></code>, <code><a href="#topic+make.SpatialVx">make.SpatialVx</a></code>, <code><a href="#topic+disjointer">disjointer</a></code>
</p>
<p>Functions that work on the resulting &ldquo;features&rdquo; objects for merging and/or matching features within/across fields:
</p>
<p><code><a href="#topic+centmatch">centmatch</a></code>, <code><a href="#topic+deltamm">deltamm</a></code>, <code><a href="#topic+minboundmatch">minboundmatch</a></code>
</p>
<p>To force merges (implicit or otherwise; recommended): <code><a href="#topic+MergeForce">MergeForce</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data( "ExampleSpatialVxSet" )

x &lt;- ExampleSpatialVxSet$vx
xhat &lt;- ExampleSpatialVxSet$fcst

hold &lt;- make.SpatialVx( x, xhat, field.type = "simulated",
		       units = "none", data.name = "Example",
		       obs.name = "x", model.name = "xhat" )

look &lt;- FeatureFinder( hold, smoothpar = 0.5, thresh = 1 )

par( mfrow=c(1,2))
image.plot(look$X.labeled)
image.plot(look$Y.labeled)

## Not run: 
x &lt;- y &lt;- matrix(0, 100, 100)
x[2:3,c(3:6, 8:10)] &lt;- 1
y[c(4:7, 9:10),c(7:9, 11:12)] &lt;- 1
     
x[30:50,45:65] &lt;- 1
y[c(22:24, 99:100),c(50:52, 99:100)] &lt;- 1
     
hold &lt;- make.SpatialVx( x, y, field.type = "contrived", units = "none",
         data.name = "Example", obs.name = "x", model.name = "y" )
     
look &lt;- FeatureFinder(hold, smoothpar=0.5) 

par( mfrow=c(1,2))
image.plot(look$X.labeled)
image.plot(look$Y.labeled)
     
look2 &lt;- centmatch(look)
     
FeatureTable(look2)
     
look3 &lt;- deltamm( look, N = 201, verbose = TRUE ) 
FeatureTable( look3 )


# data( "pert000" )
# data( "pert004" )
# data( "ICPg240Locs" )
     
# hold &lt;- make.SpatialVx( pert000, pert004,
#    loc = ICPg240Locs, projection = TRUE, map = TRUE, loc.byrow = TRUE,
#    field.type = "Precipitation", units = "mm/h",
#    data.name = "ICP Perturbed Cases", obs.name = "pert000",
#    model.name = "pert004" )
     
# look &lt;- FeatureFinder(hold, smoothpar=10.5, thresh = 5)
# plot(look)

# look2 &lt;- deltamm( look, N = 701, verbose = TRUE )

# look2 &lt;- MergeForce( look2 )

# plot(look2)

# summary( look2 )

# Now remove smallest features ( those with fewer than 700 grid squares).

# look &lt;- FeatureFinder( hold, smoothpar = 10.5, thresh = 5, min.size = 700 )

# look # Now only two features.

# plot( look )

# Now remove the largest features (those with more than 1000 grid squares). 

# look &lt;- FeatureFinder( hold, smoothpar = 10.5, thresh = 5, max.size = 1000 )

# look

# plot( look )

# Remove any features smaller than 700 and larger than 2000 grid squares).

# look &lt;- FeatureFinder( hold, smoothpar = 10.5, thresh = 5,
 #    min.size = 700, max.size = 2000 )

# look

# plot( look )

# Find features according to Wernli et al. (2008).
# look &lt;- FeatureFinder( hold, thresh = 5, do.smooth = FALSE, fac = 1 / 15 )

# look

# plot( look )

# Now do a mix of the two types of methods.
# look &lt;- FeatureFinder( hold, smoothpar = 10.5, thresh = 5, fac = 1 / 15 )

# look

# plot( look )


## End(Not run)
</code></pre>

<hr>
<h2 id='FeatureMatchAnalyzer'>
Analyze Features of a Verification Set
</h2><span id='topic+FeatureMatchAnalyzer'></span><span id='topic+FeatureMatchAnalyzer.matched.centmatch'></span><span id='topic+FeatureMatchAnalyzer.matched.deltamm'></span><span id='topic+plot.FeatureMatchAnalyzer'></span><span id='topic+print.FeatureMatchAnalyzer'></span><span id='topic+summary.FeatureMatchAnalyzer'></span><span id='topic+FeatureComps'></span><span id='topic+distill.FeatureComps'></span>

<h3>Description</h3>

<p>Analyze matched features of a verification set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>FeatureMatchAnalyzer(x, which.comps=c("cent.dist", "angle.diff", "area.ratio", "int.area",
                    "bdelta", "haus", "ph", "med", "msd", "fom", "minsep",
		    "bearing"), sizefac=1, alpha=0.1, k=4, p=2, c=Inf,
		    distfun="distmapfun", ...)

## S3 method for class 'matched.centmatch'
FeatureMatchAnalyzer(x, which.comps=c("cent.dist", "angle.diff",
		    "area.ratio", "int.area", "bdelta", "haus", "ph", "med",
		    "msd", "fom", "minsep", "bearing"), sizefac=1, alpha=0.1, k=4, p=2,
		    c=Inf, distfun="distmapfun", ...)

## S3 method for class 'matched.deltamm'
FeatureMatchAnalyzer(x, which.comps = c("cent.dist", "angle.diff",
		    "area.ratio", "int.area", "bdelta", "haus", "ph", "med", "msd",
		    "fom", "minsep", "bearing"), sizefac = 1, alpha = 0.1, k = 4, p = 2,
		    c = Inf, distfun = "distmapfun", ..., y = NULL, matches = NULL,
		    object = NULL)

## S3 method for class 'FeatureMatchAnalyzer'
summary(object, ...)

## S3 method for class 'FeatureMatchAnalyzer'
plot(x, ..., type = c("all", "ph", "med", "msd", 
    "fom", "minsep", "cent.dist", "angle.diff", "area.ratio",
    "int.area", "bearing", "bdelta", "haus"))

## S3 method for class 'FeatureMatchAnalyzer'
print(x, ...)

FeatureComps(Y, X, which.comps=c("cent.dist", "angle.diff", "area.ratio", "int.area",
    "bdelta", "haus", "ph", "med", "msd", "fom", "minsep", "bearing"),
    sizefac=1, alpha=0.1, k=4, p=2, c=Inf, distfun="distmapfun", deg = TRUE,
    aty = "compass", loc = NULL, ...)

## S3 method for class 'FeatureComps'
distill(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="FeatureMatchAnalyzer_+3A_x">x</code>, <code id="FeatureMatchAnalyzer_+3A_y">y</code>, <code id="FeatureMatchAnalyzer_+3A_matches">matches</code></td>
<td>

<p><code>x</code>, <code>y</code> and <code>matches</code> are list objects with components as output by <code>deltamm</code> or similar function.  Only one is used, and it first checks for <code>matches</code>, then <code>y</code>, and finally <code>x</code>.  It expects a component named <code>mm.new.labels</code> that gives the number of matched objects.  In the case of the <code>plot</code> and <code>print</code> method functions, <code>x</code> is a list object as returned by <code>FeatureMatchAnalyzer</code>.
</p>
<p><code>distill</code>: output from <code>FeatureComps</code>.
</p>
</td></tr>
<tr><td><code id="FeatureMatchAnalyzer_+3A_x">X</code>, <code id="FeatureMatchAnalyzer_+3A_y">Y</code></td>
<td>
<p>list object giving a pixel image as output from <code>solutionset</code> from package <span class="pkg">spatstat</span> for the verification and forecast fields, resp.  These arguments are passed directly to the <code>locperf</code> function.</p>
</td></tr>
<tr><td><code id="FeatureMatchAnalyzer_+3A_object">object</code></td>
<td>

<p>list object returned of class &ldquo;FeatureMatchAnalyzer&rdquo;, this is the returned value from the self-same function.
</p>
</td></tr>
<tr><td><code id="FeatureMatchAnalyzer_+3A_which.comps">which.comps</code>, <code id="FeatureMatchAnalyzer_+3A_type">type</code></td>
<td>

<p>character vector indicating which properties of the features are to be analyzed (<code>which.comps</code>) or plotted (<code>type</code>).
</p>
</td></tr>
<tr><td><code id="FeatureMatchAnalyzer_+3A_sizefac">sizefac</code></td>
<td>

<p>single numeric by which area calculations should be multiplied in order to get the desired units.  If unity (default) results are in terms of grid squares.
</p>
</td></tr>
<tr><td><code id="FeatureMatchAnalyzer_+3A_alpha">alpha</code></td>
<td>

<p>numeric value for the FOM measure (see the help file for <code>locperf</code>.
</p>
</td></tr>
<tr><td><code id="FeatureMatchAnalyzer_+3A_k">k</code></td>
<td>

<p>numeric indicating which quantile to use if the partial Hausdorff measure is to be used.
</p>
</td></tr>
<tr><td><code id="FeatureMatchAnalyzer_+3A_p">p</code></td>
<td>

<p>numeric giving the value of the parameter p for the Baddeley metric.
</p>
</td></tr>
<tr><td><code id="FeatureMatchAnalyzer_+3A_c">c</code></td>
<td>

<p>numeric giving the cut-off value for the Baddeley metric.
</p>
</td></tr>
<tr><td><code id="FeatureMatchAnalyzer_+3A_distfun">distfun</code></td>
<td>

<p>character naming a distance functions to use in calculating the various binary image measures.  Default is Euclidean distance.
</p>
</td></tr>
<tr><td><code id="FeatureMatchAnalyzer_+3A_deg">deg</code>, <code id="FeatureMatchAnalyzer_+3A_aty">aty</code></td>
<td>
<p>optional arguments to the <code>bearing</code> function.</p>
</td></tr>
<tr><td><code id="FeatureMatchAnalyzer_+3A_loc">loc</code></td>
<td>
<p>two-column matrix giving location coordinates for centroid distance.  If NULL, uses an indices based on the dimension of the field.</p>
</td></tr>
<tr><td><code id="FeatureMatchAnalyzer_+3A_...">...</code></td>
<td>

<p>Additional arguments to <code>deltametric</code> from package <span class="pkg">spatstat</span>.  In case of the <code>summary</code> method function, additional optional arguments may be passed, which include <code>silent</code> (logical, should the information be printed to the screen or not?), <code>interest</code> (numeric vector defining an interest value for calculating total interest for each matched object, if NULL, this is not performed), <code>con</code> (name of function that takes three arguments, the first two are matrices whose rows are objects and columns are matched feature properties, where the former is a matrix of matched feature property values (e.g., angle difference) and the latter is a matrix of interest values determined by the <code>interest</code> argument (whereby each row is identical), the third argument to <code>con</code> must be called <code>which.comps</code>, and it gives the short-form feature property names (i.e., same as which.comps argument); see details section).  In the case of the <code>plot</code> method function, these are optional arguments to the function <code>barplot</code>.
</p>
<p>Not used by <code>distill</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>FeatureMatchAnalyzer</code> operates on objects of class &ldquo;matched&rdquo;.  It is set up to calculate the values discussed in sec. 4 of Davis et al. (2006) for a single verification set (i.e., mean and standard deviation are not computed because it is only a single case).  If criteria is 1, then features separated by a distance D &lt; the sum of the sizes of the two features (size of a feature is defined as the square root of its area) are considered a match.  If criteria is 2, then a match is made if D &lt; the average of the sizes of the two features.  Finally, criteria 3 decides a match as being anything less than a pre-determined constant.
</p>
<p><code>FeatureComps</code> is the primary function called by <code>FeatureMatchAnalyzer</code>, and is designed as a more stand-alone type of function.  Several of the measures that can be calculated are simply the binary image measures/metrics available via, e.g., <code>locperf</code>.  It calculates comparisons between two matched features (i.e., between the verification and forecast fields).
</p>
<p><code>distill</code> reduces a &ldquo;FeatureComps&rdquo; list object to a named numeric vector containing (in this order) the components that exist from &quot;cent.dist&quot;, &quot;angle.diff&quot;, &quot;area.ratio&quot;, &quot;int.area&quot;, &quot;bdelta&quot;, &quot;haus&quot;, &quot;ph&quot;, &quot;med&quot;, &quot;msd&quot;, &quot;fom&quot;, and &quot;minsep&quot;.  This is used, for example, by <code>interester</code>, which is why the order is important.
</p>
<p>The <code>summary</code> method function for <code>FeatureMatchAnalyzer</code> allows for passing a function, con, to determine confidence for each interest value.  The idea being to set the interest to zero when the particular interest value does not make sense.  For example, angle difference makes no sense if both objects are circles.  Currently, no functions are included in this package for actually doing this, and so the functionality itself has not been tested. 
</p>
<p>The <code>print</code> method function for <code>FeatureMatchAnalyzer</code> first converts the object to a simple named matrix, then prints the matrix out.  The resulting matrix is returned invisibly.
</p>


<h3>Value</h3>

<p>FeatureMatchAnalyzer returns a list of list objects.  The specific components depend on the 'which.comps' argument, and are the same as those returned by FeatureComps.  These can be any of the following.
</p>
<table>
<tr><td><code>cent.dist</code></td>
<td>
<p>numeric giving the centroid (Euclidean) distance.</p>
</td></tr>
<tr><td><code>angle.diff</code></td>
<td>
<p>numeric giving the orientation (major axis) angle difference.</p>
</td></tr>
<tr><td><code>area.ratio</code></td>
<td>
<p>numeric giving the area ratio, which is always between 0 and 1 because this is defined by Davis et al. (2006) to be the area of the smaller feature divided by that of the larger feature regardless of which field the feature belongs to.</p>
</td></tr>
<tr><td><code>int.area</code></td>
<td>
<p>numeric giving the intersection area of the features.</p>
</td></tr>
<tr><td><code>bdelta</code></td>
<td>
<p>numeric giving Baddeley's delta metric between the two features.</p>
</td></tr>
<tr><td><code>haus</code>, <code>ph</code>, <code>med</code>, <code>msd</code>, <code>fom</code>, <code>minsep</code></td>
<td>
<p>numeric, see locperf for specific information.</p>
</td></tr>
<tr><td><code>bearing</code></td>
<td>
<p>numeric giving the bearing from the forecast object centroid to the observed object centroid.</p>
</td></tr>
</table>
<p>The summary method for FeatureMatchAnalyzer invisibly returns a matrix with the same information, but where each matched object is a row and each column is the specific statistic.  Or, if optional interest argument is passed, a list with components:
</p>
<p><code>print</code> returns a named vector invisibly.
</p>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>References</h3>

<p>Davis, C. A., Brown, B. G. and Bullock, R. G. (2006) Object-based verification of precipitation forecasts, Part I: Methodology and application to mesoscale rain areas. <em>Mon. Wea. Rev.</em>, <b>134</b>, 1772&ndash;1784.
</p>


<h3>See Also</h3>

<p>Functions to identify features: <code><a href="#topic+FeatureFinder">FeatureFinder</a></code>
</p>
<p>Functions to merge and/or match objects: <code><a href="#topic+deltamm">deltamm</a></code>, <code><a href="#topic+centmatch">centmatch</a></code>, <code><a href="#topic+MergeForce">MergeForce</a></code>
</p>
<p>Functions to compute feature properties: <code><a href="#topic+locperf">locperf</a></code>, <code><a href="spatstat.geom.html#topic+deltametric">deltametric</a></code>, <code><a href="#topic+bearing">bearing</a></code>
</p>
<p>Function to calculate fuzzy logic interest values: <code><a href="#topic+interester">interester</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data( "ExampleSpatialVxSet" )

x &lt;- ExampleSpatialVxSet$vx
xhat &lt;- ExampleSpatialVxSet$fcst

hold &lt;- make.SpatialVx( x, xhat, field.type="Example",
    units = "units", data.name = "Example", 
    obs.name = "x", model.name = "xhat" )

look &lt;- FeatureFinder(hold, smoothpar=1.5)
look2 &lt;- centmatch(look)

tmp &lt;- FeatureMatchAnalyzer(look2)
tmp
summary(tmp)
plot(tmp)


</code></pre>

<hr>
<h2 id='FeatureProps'>
Single Feature Properties
</h2><span id='topic+FeatureProps'></span>

<h3>Description</h3>

<p>Calculate properties for an identified feature.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>FeatureProps(x, Im = NULL, which.props = c("centroid", "area", "axis", "intensity"),
    areafac = 1, q = c(0.25, 0.9), loc = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="FeatureProps_+3A_x">x</code></td>
<td>

<p>object of class &ldquo;owin&rdquo; containing a binary image matrix defining the feature.
</p>
</td></tr>
<tr><td><code id="FeatureProps_+3A_im">Im</code></td>
<td>

<p>Matrix giving the original values of the field from which the feature was extracted.  Only needed if the feature intensity is desired.
</p>
</td></tr>
<tr><td><code id="FeatureProps_+3A_which.props">which.props</code></td>
<td>

<p>character vector giving one or more of &ldquo;centroid&rdquo;, &ldquo;area&rdquo;, &ldquo;axis&rdquo; and &ldquo;intensity&rdquo;.  If &ldquo;axis&rdquo; is given, then a call to <code>FeatureAxis</code> is made.
</p>
</td></tr>
<tr><td><code id="FeatureProps_+3A_areafac">areafac</code></td>
<td>

<p>numeric, in determining the lengths of the axes, they are multiplied by a factor of <code>fac</code> (e.g., if the grid points are k by k km each, then one could set this to k so that the resulting lengths are in terms of km rather than grid points.
</p>
</td></tr>
<tr><td><code id="FeatureProps_+3A_q">q</code></td>
<td>

<p>numeric vector of values between 0 and 1 inclusive giving the quantiles for determining the intensity of the feature.
</p>
</td></tr>
<tr><td><code id="FeatureProps_+3A_loc">loc</code></td>
<td>
<p>optional argument giving a two-column matrix of grid locations for finding the centroid.  If NULL, indices based on the dimension of x are used.</p>
</td></tr>
<tr><td><code id="FeatureProps_+3A_...">...</code></td>
<td>

<p>additional arguments to <code>FeatureAxis</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function takes an <code>owin</code> image and returns several property values for that image, including: centroid, spatial area, major and minor axis angle/length, as well as the overall intensity of the field (cf., Davis et al., 2006a, b).
</p>


<h3>Value</h3>

<p>list object with components depending on the which.props argument.  One or more of:
</p>
<table>
<tr><td><code>centroid</code></td>
<td>
<p>list with components x and y giving the centroid of the object.</p>
</td></tr>
<tr><td><code>area</code></td>
<td>
<p>numeric giving the area of the feature.</p>
</td></tr>
<tr><td><code>axis</code></td>
<td>
<p>list object of class FeatureAxis as returned by the same-named function.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>References</h3>

<p>Davis, C. A., Brown, B. G. and Bullock, R. G. (2006a) Object-based verification of precipitation forecasts, Part I: Methodology and application to mesoscale rain areas. <em>Mon. Wea. Rev.</em>, <b>134</b>, 1772&ndash;1784.
</p>
<p>Davis, C. A., Brown, B. G. and Bullock, R. G. (2006b) Object-based verification of precipitation forecasts, Part II: Application to convective rain systems. <em>Mon. Wea. Rev.</em>, <b>134</b>, 1785&ndash;1795.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+FeatureAxis">FeatureAxis</a></code>, <code><a href="spatstat.geom.html#topic+owin">owin</a></code>, <code><a href="spatstat.geom.html#topic+convexhull">convexhull</a></code>, <code><a href="smatr.html#topic+sma">sma</a></code>, <code><a href="spatstat.geom.html#topic+as.psp">as.psp</a></code>, <code><a href="spatstat.geom.html#topic+angles.psp">angles.psp</a></code>, <code><a href="spatstat.geom.html#topic+rotate.owin">rotate.owin</a></code>, <code><a href="spatstat.geom.html#topic+rotate.psp">rotate.psp</a></code>, <code><a href="spatstat.geom.html#topic+boundingbox">boundingbox</a></code>, <code><a href="spatstat.geom.html#topic+midpoints.psp">midpoints.psp</a></code>, <code><a href="spatstat.geom.html#topic+lengths_psp">lengths_psp</a></code>, <code><a href="spatstat.geom.html#topic+infline">infline</a></code>, <code><a href="spatstat.geom.html#topic+clip.infline">clip.infline</a></code>, <code><a href="#topic+deltamm">deltamm</a></code>, <code><a href="#topic+FeatureFinder">FeatureFinder</a></code>, <code><a href="#topic+disjointer">disjointer</a></code>, <code><a href="spatstat.geom.html#topic+connected">connected</a></code>, <code><a href="spatstat.geom.html#topic+tiles">tiles</a></code>, <code><a href="spatstat.geom.html#topic+tess">tess</a></code>, <code><a href="spatstat.geom.html#topic+solutionset">solutionset</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data( "ExampleSpatialVxSet" )

x &lt;- ExampleSpatialVxSet$vx

look &lt;- disk2dsmooth(x,5)
u &lt;- quantile(look,0.99)
sIx &lt;- matrix(0, 100, 100)
sIx[ look &gt; u] &lt;- 1
look2 &lt;- disjointer(sIx)[[1]]
look2 &lt;- flipxy(look2)

FeatureProps(look2,
    which.props=c("centroid", "area", "axis"))

</code></pre>

<hr>
<h2 id='FeatureTable'>
Feature-Based Contingency Table
</h2><span id='topic+FeatureTable'></span><span id='topic+ci.FeatureTable'></span><span id='topic+print.FeatureTable'></span><span id='topic+summary.FeatureTable'></span>

<h3>Description</h3>

<p>Create a feature-based contingency table from a matched object and calculate some summary scores with their standard errors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>FeatureTable(x, fudge = 1e-08, hits.random = NULL, correct.negatives = NULL, fA = 0.05)

## S3 method for class 'FeatureTable'
ci(x, alpha = 0.05, ...)

## S3 method for class 'FeatureTable'
print(x, ...)

## S3 method for class 'FeatureTable'
summary(object, ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="FeatureTable_+3A_x">x</code></td>
<td>

<p><code>FeatureTable</code>: An object of class &ldquo;matched&rdquo; (e.g., from <code>deltamm</code> or <code>centmatch</code>.
</p>
<p><code>print</code>: An object of class &ldquo;FeatureTable&rdquo;.
</p>
</td></tr>
<tr><td><code id="FeatureTable_+3A_object">object</code></td>
<td>
<p>An object of class &ldquo;FeatureTable&rdquo;.</p>
</td></tr>
<tr><td><code id="FeatureTable_+3A_fudge">fudge</code></td>
<td>

<p>value added to denominators of scores to ensure no division by zero.  Set to zero if this practice is not desired.
</p>
</td></tr>
<tr><td><code id="FeatureTable_+3A_hits.random">hits.random</code></td>
<td>

<p>If a different value for random hits in the caluclation of GSS than provided is desired, it can be given here.  Default uses Eq (3) from Davis et al (2009).
</p>
</td></tr>
<tr><td><code id="FeatureTable_+3A_correct.negatives">correct.negatives</code></td>
<td>

<p>If a different value for correct negatives than provided is desired, it can be given here.  Default uses Eq (4) from Davis et al (2009).
</p>
</td></tr>
<tr><td><code id="FeatureTable_+3A_fa">fA</code></td>
<td>

<p>numeric between zero and 1 giving the fraction of area occupied for the purpose of matching as in Davis et al (2009).
</p>
</td></tr>
<tr><td><code id="FeatureTable_+3A_alpha">alpha</code></td>
<td>
<p>numeric between zero and one giving the (1 - alpha) * 100 percent confidence level.</p>
</td></tr>
<tr><td><code id="FeatureTable_+3A_...">...</code></td>
<td>
<p>Additional arguments to <code>ci</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function takes an object of class &ldquo;matched&rdquo; and calculates a contingency table based on matched and unmatched objects.  If no value for correct negatives is given, then it will also determine them based on Eq (3) from Davis et al (2009).  The following contingency table scores and their standard errors (based on their usual traditional version) are returned.  It should be noted that the standard errors may not be entirely meaningful because they do not capture the uncertainty associated with identiying, merging and matching features within the fields.  Neverhteless, they are calculated here for investigative purposes.  Note that hits are determined by number of matched objects, which for some matching algorithms can mean that features are matched more than once (e.g., if using <code>centmatch</code>).  In essence, this fact may artificially increase thenumber of hits.  On the other hand, situations exist where such handling may be more appropriate than not having duplicate matches.
</p>
<p>hits are determined by the total number of matched features.
</p>
<p>false alarms are the total number of unmatched forecast features.
</p>
<p>misses are the total number of unmatched observed features.
</p>
<p>correct negatives are less obviously defined.  If the user does not supply a value, then these are calculated according to Eq (4) in Davis et al (2009).
</p>
<p>GSS: Gilbert skill score (aka Equitable Threat Score) based on Eq (2) of Davis et al (2009).
</p>
<p>POD: probability of detecting an event (aka the hit rate).
</p>
<p>false alarm rate: (aka probability of false detection) is the ratio of false alarms to the number of false alarms and correct negtives.
</p>
<p>FAR: the false alarm ratio is the ratio of false alarms to the total forecast events (in this case, the total number of forecast features in the field).
</p>
<p>HSS: Heidke skill score
</p>
<p>The <code>print</code> method function simply calls <code>summary</code>, which prints the feature-based contingency table in addition to calling <code>ci</code>.  The confidence intervals are based on the normal approximation method using the estimated standard errors, which themselves are suspicious.  In any case, the intervals can give a feel for some of the uncertainty associated with the scores, but should not be considered as solid.
</p>


<h3>Value</h3>

<p>A list with inherited attributes from x and components:
</p>
<table>
<tr><td><code>estimates</code></td>
<td>
<p>named numeric vector giving the estimated scores.</p>
</td></tr>
<tr><td><code>se</code></td>
<td>
<p>named numeric vector giving the estimated standard errors of the scores.</p>
</td></tr>
<tr><td><code>feature.contingency.table</code></td>
<td>
<p>named numeric vector giving the feature-based contingency table.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Standard error estimates are based on the univariate equivalent formulations, which do not account for uncertainties introduced in the feature identification, merging/clustering and matching.  They should not be considered as legitimate, and resulting confidence intervals should be mistrusted. 
</p>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>References</h3>

<p>Davis, C. A., Brown, B. G., Bullock, R. G. and Halley Gotway, J. (2009) The Method for Object-based Diagnostic Evaluation (MODE) applied to numerical forecasts from the 2005 NSSL/SPC Spring Program.  <em>Wea. Forecsting</em>, <b>24</b>, 1252&ndash;1267, DOI: 10.1175/2009WAF2222241.1.
</p>


<h3>See Also</h3>

<p>To identify features in the fields: <code><a href="#topic+FeatureFinder">FeatureFinder</a></code>
</p>
<p>To match (and merge) features: <code><a href="#topic+centmatch">centmatch</a></code>, <code><a href="#topic+deltamm">deltamm</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>##
## See help file for 'deltamm' for examples.
##
</code></pre>

<hr>
<h2 id='Fint2d'>
2-d Interpolation
</h2><span id='topic+Fint2d'></span>

<h3>Description</h3>

<p>Interpolate a function of two variables by rounding (i.e. taking the nearest value), bilinear or bicubic interpolation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Fint2d(X, Ws, s, method = c("round", "bilinear", "bicubic"), derivs = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Fint2d_+3A_x">X</code></td>
<td>

<p>A numeric n by m matrix giving the value of the function at the old coordinates.
</p>
</td></tr>
<tr><td><code id="Fint2d_+3A_ws">Ws</code></td>
<td>

<p>A numeric k by 2 matrix of new grid coordinates where k &lt;= m * n.
</p>
</td></tr>
<tr><td><code id="Fint2d_+3A_s">s</code></td>
<td>

<p>A numeric k by 2 matrix of old grid coordinates where k &lt;= m * n.
</p>
</td></tr>
<tr><td><code id="Fint2d_+3A_method">method</code></td>
<td>

<p>character naming one of &ldquo;round&rdquo; (default), &ldquo;bilinear&rdquo;, or &ldquo;bicubic&rdquo; giving the specific interpolation method to use.
</p>
</td></tr>
<tr><td><code id="Fint2d_+3A_derivs">derivs</code></td>
<td>

<p>logical, should the gradient interpolatants be returned?
</p>
</td></tr>
<tr><td><code id="Fint2d_+3A_...">...</code></td>
<td>

<p>Not used.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Method round simply returns the values at each grid point that correspond to the nearest points in the old grid.
</p>
<p>Interpolation of a function, say H, is achieved by the following formula (cf. Gilleland et al 2010, sec. 3), where r and s represent the fractional part of their respective coordinate.  that is, r = x - g( x ) and s = y - g( y ), where g( x ) is the greatest integer less than x. 
</p>
<p>sum_k sum_l b_k( r ) * b_l( s ) * H(g( x ) + l,  g( y ) + k).
</p>
<p>The specific choices for the values of b_l and b_k and their ranges depends on the type of interpolation.  For bilinear interpolation, they both range from 0 to 1, and are given by: b_0( x ) = 1 - x and b_1( x ) = x.  for bicubic interpolation, they both range from -1 to 2 and are given by:
</p>
<p>b_(-1)( t ) = (2 * t^2 - t^3 - t) / 2
</p>
<p>b_(0)( t ) = (3 * t^3 - 5 * t^2 + 2) / 2
</p>
<p>b_(1)( t ) = (4 * t^2 - 3 * t^3 + t) / 2
</p>
<p>b_(2)( t ) = ((t - 1) * t^2) / 2.
</p>


<h3>Value</h3>

<p>If deriv is FALSE, then a matrix is returned whose values correspond to the new coordinates.  Otherwise a list is returned with components:
</p>
<table>
<tr><td><code>xy</code></td>
<td>
<p>matrix whose values correspond to the new coordinates.</p>
</td></tr>
<tr><td><code>dx</code>, <code>dy</code></td>
<td>
<p> matrices giving the x and y direction gradients of the interpolation.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>References</h3>

<p>Gilleland and co-authors (2010) Spatial forecast verification: Image warping.  <em>NCAR Technical Note</em>, NCAR/TN-482+STR, DOI: 10.5065/D62805JJ.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rigider">rigider</a></code>, <code><a href="#topic+rigidTransform">rigidTransform</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# see rigider for an example.

</code></pre>

<hr>
<h2 id='FQI'>
Forecast Quality Index
</h2><span id='topic+FQI'></span><span id='topic+UIQI'></span><span id='topic+ampstats'></span><span id='topic+print.fqi'></span><span id='topic+summary.fqi'></span>

<h3>Description</h3>

<p>Functions for calculating the Forecast Quality Index (FQI) and its components.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>FQI(object, surr = NULL, k = 4, time.point = 1, obs = 1, model = 1, ...)

UIQI(X, Xhat, ...)

ampstats(X, Xhat, only.nonzero = FALSE)

## S3 method for class 'fqi'
print(x, ...)

## S3 method for class 'fqi'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="FQI_+3A_object">object</code></td>
<td>

<p>list object of class &ldquo;SpatialVx&rdquo;.  In the case of the <code>summary</code> method, <code>object</code> is the list object returned by <code>FQI</code>.
</p>
</td></tr>
<tr><td><code id="FQI_+3A_x">X</code>, <code id="FQI_+3A_xhat">Xhat</code></td>
<td>
<p>numeric matrices giving the fields for the verification set.</p>
</td></tr>
<tr><td><code id="FQI_+3A_x">x</code></td>
<td>
<p>list object of class &ldquo;fqi&rdquo; as returned by <code>FQI</code>.</p>
</td></tr>
<tr><td><code id="FQI_+3A_surr">surr</code></td>
<td>

<p>three-dimesnional array containing surrogate fields for <code>X</code>, e.g. as returned by <code>surrogater2d</code>.  If NULL, these will be calculated using <code>surrogater2d</code>.
</p>
</td></tr>
<tr><td><code id="FQI_+3A_only.nonzero">only.nonzero</code></td>
<td>
<p>logical, should the means and variances of only the non-zero values of the fields be calculated (if so, the covariance is returned as NA)?</p>
</td></tr>
<tr><td><code id="FQI_+3A_k">k</code></td>
<td>

<p>numeric vector for use with the partial Hausdorff distance.  For k that are whole numerics or integers &gt;= 1, then the k-th highest value is returned by <code>locmeasures2d</code>.  If 0 &lt;= k &lt; 1, then the corresponding quantile is returned.
</p>
</td></tr>
<tr><td><code id="FQI_+3A_time.point">time.point</code></td>
<td>
<p>numeric or character indicating which time point from the &ldquo;SpatialVx&rdquo; verification set to select for analysis.</p>
</td></tr>
<tr><td><code id="FQI_+3A_obs">obs</code>, <code id="FQI_+3A_model">model</code></td>
<td>
<p>numeric indicating which forecast model to select for the analysis.</p>
</td></tr>
<tr><td><code id="FQI_+3A_...">...</code></td>
<td>

<p>In the case of <code>FQI</code>, additional arguments to <code>surrogater2d</code>.  Only used if <code>surr</code> is NULL.  In the case of <code>UIQI</code>, additional arguments to <code>ampstats</code>.  In the case of <code>summary.fqi</code>, these are not used.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The FQI was proposed as a spatial verification metric (a true metric in the mathematical sense) by Venugopal et al. (2005) to combine amplitude and displacement error information in a single summary statistic.  It is given by
</p>
<p>FQI = (PHD_k(X, Xhat)/mean( PHD_k(X, surr_i); i in 1 to number of surrogates)) / (brightness * distortion)
</p>
<p>where the numerator is a normalized partial Hausdorff distance (see help file for locperf), brightness (also called bias) is given by 2*(mu1*mu2)/(mu1^2+mu2^2), where mu1 (mu2) is the mean value of X (Xhat), and the distortion term is given by 2*(sig1*sig2)/(sig1^2+sig2^2), where sig1^2 (sig2^2) is the variance of X (Xhat) values.  The denominator is a modified UIQI (Universal Image Quality Index; Wang and Bovik, 2002), which itself is given by
</p>
<p>UIQI = cor(X,Xhat)*brightness*distortion.
</p>
<p>Note that if <code>only.nonzero</code> is <code>TRUE</code> in the call to <code>UIQI</code>, then the modified UIQI used in the FQI formulation is returned (i.e., without multiplying by the correlation term).
</p>
<p>The <code>print</code> method so far just calls the <code>summary</code> method.
</p>


<h3>Value</h3>

<p>FQI returns a list with with the following components:
</p>
<table>
<tr><td><code>phd.norm</code></td>
<td>
<p>matrix of normalized partial Hausdorff distances for each value of k (rows) and each threshold (columns).</p>
</td></tr>
<tr><td><code>uiqi.norm</code></td>
<td>
<p>numeric vector of modified UIQI values for each threshold.</p>
</td></tr>
<tr><td><code>fqi</code></td>
<td>
<p>matrix of FQI values for each value of k (rows) and each threshold (columns).</p>
</td></tr>
</table>
<p>It will also have the same attributes as the &ldquo;SpatialVx&rdquo; object with additional attributes defining the arguments specific to parameters used by the function.
</p>
<p>UIQI returns a list with components:
</p>
<table>
<tr><td><code>data.name</code></td>
<td>
<p>character vector giving the names of the two fields.</p>
</td></tr>
<tr><td><code>cor</code></td>
<td>
<p>single numeric giving the correlation between the two fields.</p>
</td></tr>
<tr><td><code>brightness.bias</code></td>
<td>
<p>single numeric giving the brightness (bias) value.</p>
</td></tr>
<tr><td><code>distortion.variability</code></td>
<td>
<p>single numeric giving the distortion (variability) value.</p>
</td></tr>
<tr><td><code>UIQI</code></td>
<td>
<p>single numeric giving the UIQI (or modified UIQI if only.nonzero is set to TRUE) value.</p>
</td></tr>
</table>
<p>ampstats returns a list object with components:
</p>
<table>
<tr><td><code>mean.fcst</code>, <code>mean.vx</code></td>
<td>
<p>single numerics giving the mean of Xhat and X, resp.</p>
</td></tr>
<tr><td><code>var.fcst</code>, <code>var.vx</code></td>
<td>
<p>single numerics giving the variance of Xhat and X, resp.</p>
</td></tr>
<tr><td><code>cov</code></td>
<td>
<p>single numeric giving the covariance between Xhat and X (if only.nonzero is TRUE, this will be NA).</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>References</h3>

<p>Venugopal, V., Basu, S. and Foufoula-Georgiou, E. (2005) A new metric for comparing precipitation patterns with an application to ensemble forecasts.  <em>J. Geophys. Res.</em>, <b>110</b>, D08111, 11 pp., doi:10.1029/2004JD005395.
</p>
<p>Wang, Z. and Bovik, A. C. (2002) A universal image quality index.  <em>IEEE Signal Process. Lett.</em>, <b>9</b>, 81&ndash;84.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+locperf">locperf</a></code>, <code><a href="#topic+surrogater2d">surrogater2d</a></code>, <code><a href="#topic+locmeasures2d">locmeasures2d</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data( "ExampleSpatialVxSet" )

 x &lt;- ExampleSpatialVxSet$vx
 xhat &lt;- ExampleSpatialVxSet$fcst

 # Now, find surrogates of the simulated field.
 z &lt;- surrogater2d(x, zero.down=TRUE, n=10)

 u &lt;- list( X = cbind( quantile( c(x), c(0.75, 0.9)) ),
	Xhat = cbind( quantile( c(xhat), c(0.75, 0.9) ) ) )

hold &lt;- make.SpatialVx(x, xhat, thresholds = u,
			field.type = "Example", units = "none",
			data.name = "ExampleSpatialVxSet",
			obs.name = "X", model.name = "Xhat" )

FQI(hold, surr = z, k = c(4, 0.75) )
</code></pre>

<hr>
<h2 id='fss2dfun'>Various Verification Statistics on Possibly Neighborhood-Smoothed Fields.</h2><span id='topic+fss2dfun'></span><span id='topic+fuzzyjoint2dfun'></span><span id='topic+MinCvg2dfun'></span><span id='topic+multicon2dfun'></span><span id='topic+pragmatic2dfun'></span><span id='topic+upscale2dfun'></span>

<h3>Description</h3>

<p>Functions to calculate various verification statistics on possibly neighborhood smoothed fields.  Used by hoods2d, but can be called on their own.</p>


<h3>Usage</h3>

<pre><code class='language-R'>fss2dfun(sPy, sPx, subset = NULL, verbose = FALSE)

fuzzyjoint2dfun(sPy, sPx, subset = NULL)

MinCvg2dfun(sIy, sIx, subset = NULL)

multicon2dfun(sIy, Ix, subset = NULL)

pragmatic2dfun(sPy, Ix, mIx = NULL, subset = NULL)

upscale2dfun(sYy, sYx, threshold = NULL, which.stats = c("rmse",
                 "bias", "ts", "ets"), rule = "&gt;=", subset = NULL)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fss2dfun_+3A_spy">sPy</code></td>
<td>
<p>n by m matrix giving a smoothed binary forecast field.</p>
</td></tr>
<tr><td><code id="fss2dfun_+3A_spx">sPx</code></td>
<td>
<p>n by m matrix giving a smoothed binary observed field.</p>
</td></tr>
<tr><td><code id="fss2dfun_+3A_siy">sIy</code></td>
<td>
<p>n by m matrix giving a binary forecast field.</p>
</td></tr>
<tr><td><code id="fss2dfun_+3A_six">sIx</code></td>
<td>
<p>n by m matrix giving a binary observed field (the s indicates that the binary field is obtained from a smoothed field).</p>
</td></tr>
<tr><td><code id="fss2dfun_+3A_ix">Ix</code></td>
<td>
<p>n by m matrix giving a binary observed field.</p>
</td></tr>
<tr><td><code id="fss2dfun_+3A_mix">mIx</code></td>
<td>
<p>(optional) single numeric giving the base rate.  If NULL, this will be calculated by the function.  Simply a computation saving step if this has already been calculated.</p>
</td></tr>
<tr><td><code id="fss2dfun_+3A_syy">sYy</code></td>
<td>
<p>n by m matrix giving a smoothed forecast field.</p>
</td></tr>
<tr><td><code id="fss2dfun_+3A_syx">sYx</code></td>
<td>
<p>n by m matrix giving a smoothed observed field.</p>
</td></tr>
<tr><td><code id="fss2dfun_+3A_threshold">threshold</code></td>
<td>
<p>(optional) numeric vector of length 2 giving the threshold over which to calculate the verification statistics: bias, ts and ets.  If NULL, only the rmse will be calculated.</p>
</td></tr>
<tr><td><code id="fss2dfun_+3A_which.stats">which.stats</code></td>
<td>
<p>character vector naming which statistic(s) should be caluclated for <code>upscale2dfun</code>.</p>
</td></tr>
<tr><td><code id="fss2dfun_+3A_subset">subset</code></td>
<td>
<p>(optional) numeric indicating over which points the summary scores should be calculated.  If NULL, all of the points are used.</p>
</td></tr>
<tr><td><code id="fss2dfun_+3A_rule">rule</code></td>
<td>
<p>character string giving the sort of thresholding process desired.  See the help file for <code>thresholder</code> for more information.</p>
</td></tr>
<tr><td><code id="fss2dfun_+3A_verbose">verbose</code></td>
<td>
<p>logical, should progress information be printed to the screen?</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These are modular functions that calculate the neighborhood smoothing method statistics in spatial forecast verification (see, e.g., Ebert, 2008, 2009; Gilleland et al., 2009, 2010; Roberts and Lean,2008).  These functions take fields that have already had the neighborhood smoothing applied (e.g., using <code>kernele2d</code>) when appropriate.  They are called by <code>hoods2d</code>, so need not be called by the user, but they can be.
</p>


<h3>Value</h3>

<p>In the case of <code>fss2dfun</code>, a single numeric giving the FSS value is returned.  In the other cases, list objects are returned with one or more of the following components, depending on the particular function.
</p>
<table>
<tr><td><code>fuzzy</code></td>
<td>
<p><code>fuzzyjoint2dfun</code> returns a list with this list as one component.  The list component fuzzy has the components: pod, far and ets.</p>
</td></tr>
<tr><td><code>joint</code></td>
<td>
<p><code>fuzzyjoint2dfun</code> returns a list with this list as one component.  The list component joint has the components: pod, far and ets.</p>
</td></tr>
<tr><td><code>pod</code></td>
<td>
<p>numeric giving the probability of detection, or hit rate.</p>
</td></tr>
<tr><td><code>far</code></td>
<td>
<p>numeric giving the false alarm ratio.</p>
</td></tr>
<tr><td><code>ets</code></td>
<td>
<p>numeric giving the equitable threat score, or Gilbert Skill Score.</p>
</td></tr>
<tr><td><code>f</code></td>
<td>
<p>numeric giving the false alarm rate.</p>
</td></tr>
<tr><td><code>hk</code></td>
<td>
<p>numeric giving the Hanssen-Kuipers statistic.</p>
</td></tr>
<tr><td><code>bs</code></td>
<td>
<p>Brier Score</p>
</td></tr>
<tr><td><code>bss</code></td>
<td>
<p>Brier Skill Score.  The <code>pragmatic2dfun</code> returns the bs and bss values.  The Brier Skill Score here uses the mean square error between the base rate and the Ix field as the reference forecast.</p>
</td></tr>
<tr><td><code>ts</code></td>
<td>
<p>numeric giving the threat score.</p>
</td></tr>
<tr><td><code>bias</code></td>
<td>
<p>numeric giving the frequency bias.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Eric Gilleland</p>


<h3>References</h3>

<p>Ebert, E. E. (2008) Fuzzy verification of high resolution gridded forecasts: A review and proposed framework.  <em>Meteorol. Appl.</em>, <b>15</b>, 51&ndash;64. doi:10.1002/met.25 
</p>
<p>Ebert, E. E. (2009) Neighborhood verification: A strategy for rewarding close forecasts.  <em>Wea. Forecasting</em>, <b>24</b>, 1498&ndash;1510, doi:10.1175/2009WAF2222251.1.
</p>
<p>Gilleland, E., Ahijevych, D., Brown, B. G., Casati, B. and Ebert, E. E. (2009) Intercomparison of Spatial Forecast Verification Methods. <em>Wea. Forecasting</em>, <b>24</b>, 1416&ndash;1430, doi:10.1175/2009WAF2222269.1.
</p>
<p>Gilleland, E., Ahijevych, D. A., Brown, B. G. and Ebert, E. E. (2010) Verifying Forecasts Spatially. <em>Bull. Amer. Meteor. Soc.</em>, October, 1365&ndash;1373.
</p>
<p>Roberts, N. M. and Lean, H. W. (2008) Scale-selective verification of rainfall accumulations from high-resolution forecasts of convective events.  <em>Mon. Wea. Rev.</em>, <b>136</b>, 78&ndash;97. doi:10.1175/2007MWR2123.1.</p>


<h3>See Also</h3>

<p><code><a href="#topic+hoods2d">hoods2d</a></code>,<code><a href="smoothie.html#topic+kernel2dsmooth">kernel2dsmooth</a></code>,<code><a href="#topic+vxstats">vxstats</a></code>, <code><a href="#topic+thresholder">thresholder</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- y &lt;- matrix( 0, 100, 100)
x[ sample(1:100, 10), sample(1:100, 10)] &lt;- 1
y[ sample(1:100, 20), sample(1:100, 20)] &lt;- 1
Px &lt;- kernel2dsmooth( x, kernel.type="boxcar", n=9, xdim=c(100, 100))
Py &lt;- kernel2dsmooth( y, kernel.type="boxcar", n=9, xdim=c(100, 100))
par( mfrow=c(2,2))
image( x, col=c("grey", "darkblue"), main="Simulated Observed Events")
image( y, col=c("grey", "darkblue"), main="Simulated Forecast Events")
image( Px, col=c("grey", tim.colors(256)), main="Forecast Event Frequencies (9 nearest neighbors)")
image( Py, col=c("grey", tim.colors(256)), main="Smoothed Observed Events (9 nearest neighbors)")
fss2dfun( Py, Px)

</code></pre>

<hr>
<h2 id='fss2dPlot'>
Create Several Graphics for List Objects Returned from hoods2d
</h2><span id='topic+fss2dPlot'></span><span id='topic+upscale2dPlot'></span>

<h3>Description</h3>

<p>Creates several graphics for list objects returned from hoods2d.  Mostly quilt and matrix plots for displaying results of smoothing fields over different neighborhood lengths and thresholds.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fss2dPlot(x, ..., matplotcol = 1:6, mfrow = c(1, 2), add.text = FALSE)

upscale2dPlot(object, args, ..., type = c("all",
                 "gss", "ts", "bias", "rmse"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fss2dPlot_+3A_x">x</code></td>
<td>
<p>list object with components fss, fss.random and fss.uniform.  Effectively, it does the same thing as <code>hoods2dPlot</code>, but adds the fss.random and fss.uniform horizontal lines to the matrix plot.</p>
</td></tr>
<tr><td><code id="fss2dPlot_+3A_object">object</code></td>
<td>
<p>list object with named components: rmse (numeric vector), ets, ts and bias all matrices whose rows represent neighborhood lengths, and whose columns represent thresholds.</p>
</td></tr>
<tr><td><code id="fss2dPlot_+3A_args">args</code></td>
<td>
<p>list object passed to <code>hoods2dPlot</code>, see its help file for more details.</p>
</td></tr>
<tr><td><code id="fss2dPlot_+3A_mfrow">mfrow</code></td>
<td>
<p>mfrow parameter (see help file for <code>par</code>).  If NULL, then the parameter is not re-set.  </p>
</td></tr>
<tr><td><code id="fss2dPlot_+3A_add.text">add.text</code></td>
<td>
<p>logical, if TRUE, FSS values will be added to the quilt plot as text (in addition to the color).</p>
</td></tr>
<tr><td><code id="fss2dPlot_+3A_type">type</code></td>
<td>
<p>character string stating which plots to make (default is &ldquo;all&rdquo;).</p>
</td></tr>
<tr><td><code id="fss2dPlot_+3A_...">...</code></td>
<td>
<p>Optional arguments to <code>image</code> and <code>image.plot</code> for <code>fss2dPlot</code>, and optional arguments to <code>hoods2dPlot</code> for <code>upscale2dPlot</code></p>
</td></tr>
<tr><td><code id="fss2dPlot_+3A_matplotcol">matplotcol</code></td>
<td>
<p>col argument to function <code>matplot</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>makes quilt and matrix plots for output from <code>hoods2d</code>.</p>


<h3>Value</h3>

<p>No value is returned.  A series of plots are created.  It may be useful to use this function in conjunction with <code>pdf</code> in order to view all of the plots.  See the help file for <code>hoods2dPlot</code> to plot individual results.
</p>


<h3>Author(s)</h3>

<p>Eric Gilleland</p>


<h3>See Also</h3>

<p><code><a href="#topic+hoods2dPlot">hoods2dPlot</a></code>, <code><a href="graphics.html#topic+matplot">matplot</a></code>, <code><a href="Matrix.html#topic+image">image</a></code>, <code><a href="fields.html#topic+image.plot">image.plot</a></code>, <code><a href="#topic+hoods2d">hoods2d</a></code>, <code><a href="grDevices.html#topic+pdf">pdf</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>##
## This is effectively an internal function, so the example is commented out
## in order for R's check to run faster.
##
## Not run: 
data( "geom001" )
data( "geom000" )
data( "ICPg240Locs" )

hold &lt;- make.SpatialVx( geom000, geom001, thresholds = c(0.01,50.01),
    loc = ICPg240Locs, map = TRUE, projection = TRUE, loc.byrow = TRUE,
    units = "mm/h", data.name = "Geometric", obs.name = "observation",
    model.name = "case 1" )

look &lt;- hoods2d(hold, levels=c(1, 3, 5, 33, 65),
    verbose=TRUE)
plot( look)

## End(Not run)
</code></pre>

<hr>
<h2 id='Gbeta'>
Spatial-Alignment Summary Measures
</h2><span id='topic+Gbeta'></span><span id='topic+GbetaIL'></span><span id='topic+G2IL'></span>

<h3>Description</h3>

<p>Calculates the spatial alignment summary measures from Gilleland (2020)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Gbeta(X, Xhat, threshold, beta, alpha = 0, rule = "&gt;", ...)

GbetaIL(X, Xhat, threshold, beta, alpha = 0, rule = "&gt;", w = 0.5, ...)

G2IL(X, Xhat, threshold, beta, alpha = 0, rule = "&gt;", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Gbeta_+3A_x">X</code>, <code id="Gbeta_+3A_xhat">Xhat</code></td>
<td>
<p> Observed and Forecast fields in the form of a matrix.  <code>X</code> and <code>Xhat</code> must have the same dimensions. May be class &ldquo;owin&rdquo; objects for <code>Gbeta</code>.</p>
</td></tr>
<tr><td><code id="Gbeta_+3A_threshold">threshold</code></td>
<td>

<p>If <code>X</code> and <code>Xhat</code> are not binary &ldquo;owin&rdquo; objects, then the threshold is used to define the binary field.  See <code>binarizer</code> for more information about this argument.
</p>
</td></tr>
<tr><td><code id="Gbeta_+3A_beta">beta</code></td>
<td>

<p>single numeric defining the upper limit for y = y1y2 or y = y1y2(1+y3) determining the rate of decrease in the measure.  Default is half the domain size squared.
</p>
</td></tr>
<tr><td><code id="Gbeta_+3A_alpha">alpha</code></td>
<td>

<p>single numeric defining a lower limit for y = y1y2 or y = y1y2(1+y3) determining what constitutes a perfect match.  The default of zero requires the two fields to be identical, and is probably what is wanted in most cases.
</p>
</td></tr>
<tr><td><code id="Gbeta_+3A_rule">rule</code></td>
<td>

<p>See <code>binarizer</code> for more information about this argument.
</p>
</td></tr>
<tr><td><code id="Gbeta_+3A_w">w</code></td>
<td>

<p>single numeric between zero and one describing how much weight to give to the first term in the definition of GbetaIL.  See details section below.
</p>
</td></tr>
<tr><td><code id="Gbeta_+3A_...">...</code></td>
<td>

<p>Not used.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These summary measures were proposed in Gilleland (2020) and provide an index between zero (bad score) and one (perfect score) describing the closeness in spatial alignment between the two fields.  GbetaIL and G2IL also incorporate a distributional summary of the intensity errors.  Gbeta applied between two fields A and B is defined as
</p>
<p>Gbeta(A,B) = max( 1 - y/beta, 0),
</p>
<p>where y = y1 * y2, with y1 a measure of the overlap between A and B (if they overlap completely, y1 = 0); it is the number of points in AB^c and A^cB, where ^c denotes set complement. The term y2 = (MED(A,B) * nB + MED(B,A) * nA), where MED is the mean-error distance (see <code>locperf</code> for more about MED), with nA and nB representing the number of 1-valued grid points in the sets A and B, resp.  If alpha != 0, then the term y/beta is replaced with (y - alpha) / (beta - alpha).
</p>
<p>GbetaIL is defined to be
</p>
<p>GbetaIL(A,B) = w * Gbeta + (1-w) * theta(A,B),
</p>
<p>where theta is the maximum of zero and the linear correlation coefficient between the intensity values after having sorted them; this part is carried out via a call to <code>qqplot</code>.  If the number of points in the two sets differs (i.e., if nA != nB), then the larger set is linear interpolated to be the same size as the smaller set.  If both fields are empty, theta = 1.  If field A is empty, then theta = 1 - (nB / N), where N is the size of the domain.  Similarly, if field B is empty.  The rationale is that if one field is empty and the other has very few nonzero points, then the two fields are more similar than if the other field has many points.
</p>
<p>G2betaIL(A,B) = max(1 - (y1 * y2 *(1 + y3 ))/beta, 0),
</p>
<p>where y1 and y2 are as above and y3 is the mean-absolute difference between the sorted values from the sets A and B analogous as for GbetaIL.  If nA = nB = 0, then y3 = 0.  If only one of nA or nB is zero, then the absolute value of the maximum intensity of the other set is used.  The rationale being that this value represents the most egregious error so that if it is large, then the difference is penalized more.
</p>
<p>See Gilleland (2020) for more details about these measures.
</p>


<h3>Value</h3>

<p>An object of class &ldquo;Gbeta&rdquo; giving a single numeric giving the value of the summary (index) measures is returned, but with additional attributes that can be obtained using the attributes function, and are also displayed via the print function.  To remove the attributes, the function c can be used.  For Gbeta these include:
</p>
<table>
<tr><td><code>components</code></td>
<td>
<p>numeric vector giving nA, nB, nAB (the number of points in both sets), nA + nB - 2nAB (i.e., the overlap measure, y1), medAB, medBA, medAB * nnB, medBA * nA, and the two asymmetric versions of this measure (see Gilleland, 2020). </p>
</td></tr>
<tr><td><code>beta</code></td>
<td>
<p>The value of beta used.</p>
</td></tr>
<tr><td><code>alpha</code></td>
<td>
<p>The value of alpha used.</p>
</td></tr>
<tr><td><code>threshold</code></td>
<td>
<p> numeric vector giving the value of the threshold and the rule used.</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>character vector giving the object names used for X and Xhat.</p>
</td></tr>
</table>
<p>The attributes for GbetaIL and G2IL are the same, but the components vector also includes the value of theta.  GbetaIL has an additional attribute called weights that gives w and 1 - w.
</p>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>References</h3>

<p>Gilleland, E. (2020) Novel measures for summarizing high-resolution forecast performance. <em>Advances in Statistical Climatology, Meteorology and Oceanography</em>, <b>7</b> (1), 13&ndash;34, doi: 10.5194/ascmo-7-13-2021.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+TheBigG">TheBigG</a></code>,<a href="stats.html#topic+qqplot">qqplot</a>, <code><a href="#topic+binarizer">binarizer</a></code>, <code><a href="spatstat.geom.html#topic+solutionset">solutionset</a></code>, <code><a href="#topic+locperf">locperf</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data( "obs0601" )
data( "wrf4ncar0531" )
res &lt;- Gbeta( X = obs0601, Xhat = wrf4ncar0531, threshold = 2.1 )
c( res )
res
attributes( res )

GbetaIL( X = obs0601, Xhat = wrf4ncar0531, threshold = 2.1,
	beta = 601 * 501 )

G2IL( X = obs0601, Xhat = wrf4ncar0531, threshold = 2.1,
     beta = 601 * 501 )
</code></pre>

<hr>
<h2 id='GeoBoxPlot'>
Geographic Box Plot
</h2><span id='topic+GeoBoxPlot'></span>

<h3>Description</h3>

<p>Make a geographic box plot as detailed in Willmott et al. (2007).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>GeoBoxPlot(x, areas, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="GeoBoxPlot_+3A_x">x</code></td>
<td>

<p>numeric giving the values to be box-plotted.
</p>
</td></tr>
<tr><td><code id="GeoBoxPlot_+3A_areas">areas</code></td>
<td>

<p>numeric of same length as x giving the associated areas for each value.
</p>
</td></tr>
<tr><td><code id="GeoBoxPlot_+3A_...">...</code></td>
<td>

<p>optional arguments to the <code>boxplot</code> function of R.  The argument plot is not allowed.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function makes the geographic box plots described in Willmott et al. (2007) that calculates the five statistics in such a way as to account for the associated areas (e.g., over a grid where each grid box may have differing areas).
</p>
<p>Missing values are not handled, and ideally should be handled before calling ths routine.
</p>
<p>In future, this function may allow other options for <code>x</code> than currently, but for now, only numeric vectors are allowed.
</p>


<h3>Value</h3>

<p>List with the same components as returned by <code>boxplot</code>.
</p>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>References</h3>

<p>Willmott, C. J., Robeson, S. M. and Matsuura, K. (2007)  Geographic box plots.  <em>Physical Geography</em>, <b>28</b>, 331&ndash;344, doi:10.2747/0272-3646.28.4.331.
</p>


<h3>See Also</h3>

<p><code><a href="graphics.html#topic+boxplot">boxplot</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>##
## Reproduce the boxplots of Fig. 1 in Willmott et al. (2007).
##
x &lt;- c(4,9,1,3,10,6,7)
a &lt;- c(rep(1,4),2,1,3)
boxplot( x, at=1, xlim=c(0,3))
GeoBoxPlot(x, a, at=2, add=TRUE)
axis( 1, at=c(1,2), labels=c("Traditional", "Geographic"))
</code></pre>

<hr>
<h2 id='GFSNAMfcstEx'>
Example Verification Set
</h2><span id='topic+GFSNAMfcstEx'></span><span id='topic+GFSNAMobsEx'></span><span id='topic+GFSNAMlocEx'></span>

<h3>Description</h3>

 
<p>Example verification set of accumulated precipitation (mm) with 361 time points in addition to 2352 spatial locations on a grid.  Taken from a real, but unknown, weather model and observation analysis (one of GFS or NAM).  Accumulation is either 3-h or 24-h.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(GFSNAMfcstEx)
data(GFSNAMobsEx)
data(GFSNAMlocEx)
</code></pre>


<h3>Format</h3>

<p>The format is:
num [1:2352, 1:361] 0 0 0 0 0 ...
</p>
<p>The format is:
num [1:2352, 1:361] 0 0 0 0 0 ...
</p>
<p>The format is: A data frame with 2352 observations on the following 2 variables.
</p>

<dl>
<dt><code>Lat</code></dt><dd><p>a numeric vector of latitude coordinates for GFS/NAM example verification set.</p>
</dd>
<dt><code>Lon</code></dt><dd><p>a numeric vector of longitude coordinates for GFS/NAM example verification set.</p>
</dd>
</dl>



<h3>Details</h3>

<p>Example verification set with 2352 spatial locations over the United States, and 361 time points.  For both the forecast (<code>GFSNAMfcstEx</code>) and verification (<code>GFSNAMobsEx</code>), these are numeric matrices whose rows represent time, and columns represent space.  The associated lon/lat coordinates are provided by <code>GFSNAMlocEx</code> (2352 by 2 data frame with named components giving the lon and lat values).
</p>
<p>Note that the available spatial locations are a subset of the original 70 X 100 60-km grid where each time point had no missing observations.  This example set is included with the package simply toi demonstrate some functionality that involves both space and time; though this is mostly a spatial-only package.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data( "GFSNAMfcstEx" )
data( "GFSNAMobsEx" )
data( "GFSNAMlocEx" )

x &lt;- colMeans(GFSNAMfcstEx, na.rm=TRUE)
y &lt;- colMeans(GFSNAMobsEx, na.rm=TRUE)
look &lt;- as.image(x - y, x=GFSNAMlocEx)
image.plot(look)
</code></pre>

<hr>
<h2 id='gmm2d'>
2-d Gaussian Mixture Models Verification
</h2><span id='topic+gmm2d'></span><span id='topic+gmm2d.default'></span><span id='topic+gmm2d.SpatialVx'></span><span id='topic+plot.gmm2d'></span><span id='topic+predict.gmm2d'></span><span id='topic+print.gmm2d'></span><span id='topic+summary.gmm2d'></span>

<h3>Description</h3>

<p>Use 2-d Gaussian Mixture Models (GMM) to assess forecast performance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gmm2d(x, ...)

## Default S3 method:
gmm2d(x, ..., xhat, K = 3, gamma = 1, threshold = NULL,
    initFUN = "initGMM", verbose = FALSE)

## S3 method for class 'SpatialVx'
gmm2d(x, ..., time.point = 1, obs = 1, model = 1, K = 3, gamma = 1,
    threshold = NULL, initFUN = "initGMM", verbose = FALSE)

## S3 method for class 'gmm2d'
plot(x, ..., col = c("gray", tim.colors(64)),
    zlim = c(0, 1), horizontal = TRUE)

## S3 method for class 'gmm2d'
predict(object, ..., x)

## S3 method for class 'gmm2d'
print(x, ...)

## S3 method for class 'gmm2d'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gmm2d_+3A_x">x</code>, <code id="gmm2d_+3A_xhat">xhat</code></td>
<td>

<p>Default: m by n numeric matrices giving the verification and forecast fields, resp.
</p>
<p><code>gmm2d.SpatialVx</code>: object of class &ldquo;SpatialVx&rdquo;.
</p>
<p><code>plot</code> and <code>print</code>: object returned by <code>gmm2d</code>.
</p>
<p><code>predict</code>: k by 2 matrix of lon/lat coordinates on which to predict the model.
</p>
</td></tr>
<tr><td><code id="gmm2d_+3A_object">object</code></td>
<td>
<p>output from <code>gmm2d</code>.</p>
</td></tr>
<tr><td><code id="gmm2d_+3A_k">K</code></td>
<td>

<p>single numeric giving the number of mixture components to use.
</p>
</td></tr>
<tr><td><code id="gmm2d_+3A_gamma">gamma</code></td>
<td>

<p>Value of the gamma parameter from Eq (11) of Lakshmanan and Kain (2010).  This affects the number of times a location is repeated.
</p>
</td></tr>
<tr><td><code id="gmm2d_+3A_threshold">threshold</code></td>
<td>

<p>numeric giving a threshold over which (and including) the GMM is to be fit (zero-valued grid points are not included in the estimation here for speed).  If NULL, no thresholding is applied.
</p>
</td></tr>
<tr><td><code id="gmm2d_+3A_initfun">initFUN</code></td>
<td>

<p>character naming a function to provide initial estimates for the GMM.  Must take an m by n matrix as input, and return a dataframe a component called <code>ind</code> that is a vector indicating the order of the rows for which the first <code>K</code> will be used, a third column giving the x-coordinates of the initial estimate of the mean for the x direction, fourth column giving the initial estimate for the mean of the  y-direction, and fifth and sixth columns giving initial estimates for the standard deviations of the x- and y-directions.  The default identifies all connected components using the <code>disjointer</code> function, then uses their centroids as the initial estimates of the means, and their axes as initial estimates for the standard deviations.  The <code>ind</code> component gives the order of the object areas from largest to smallest so that the <code>K</code> largest objects are used to provide initial estimates.  Note that this differs from the initial estimates in Lakshmanan and Kain (2010) where they break the field into different areas first.
</p>
</td></tr>
<tr><td><code id="gmm2d_+3A_verbose">verbose</code></td>
<td>

<p>logical, should progress information be printed to the screen?
</p>
</td></tr>
<tr><td><code id="gmm2d_+3A_time.point">time.point</code></td>
<td>
<p>numeric or character indicating which time point from the &ldquo;SpatialVx&rdquo; verification set to select for analysis.</p>
</td></tr>
<tr><td><code id="gmm2d_+3A_obs">obs</code>, <code id="gmm2d_+3A_model">model</code></td>
<td>
<p>numeric indicating which observation/forecast model to select for the analysis.</p>
</td></tr>
<tr><td><code id="gmm2d_+3A_col">col</code>, <code id="gmm2d_+3A_zlim">zlim</code>, <code id="gmm2d_+3A_horizontal">horizontal</code></td>
<td>
<p>optional arguments to <span class="pkg">fields</span> function(s) <code>poly.image</code>, <code>image.plot</code>.</p>
</td></tr>
<tr><td><code id="gmm2d_+3A_...">...</code></td>
<td>

<p>In the case of <code>gmm2d</code>: optional arguments to <code>initFUN</code>.  In the case of <code>plot</code>: not used.  In the case of <code>predict</code>: N by 2 matrix of grid point locations on which to predict the probability from the 2-d GMM model.  In the case of <code>summary</code>: this can include the arguments: <code>silent</code>, logical stating whether to print summaries to the screen (FALSE) or not (TURE), <code>e1</code>, <code>e2</code>, ..., <code>e5</code>, giving alternative weights in calculating the overall error (Eq 15 in Lakshmanan and Kain, 2010, but see details section below).
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These functions carry out the spatial verification approach described in Lakshmanan and Kain (2010), which fits a 2-d Gaussian Mixture Model (GMM) to the locations for each field in the verification set, and makes comparisons using the estimated parameters.  In fitting the GMMs, first an initial estimate is provided by using the initFUN argument, which is a function.  The default function is relatively fast (it might seem slow, but for what it does, it is very fast!), but is typically the slowest part of the process.  Although the EM algorithm is a fairly computationally intensive procedure, acceleration algorithms are employed (via the turboem function of the turboEM package) so that once initial estimates are found, the procedure is very fast.
</p>
<p>Because the fit is to the locations only, Lakshmanan and Kain (2010) suggest two ways to incorporate intensity information.  The first is to repeat points with higher intensities, and the second is to multiply the results by the total intensities over the fields.  The points are repeated M times according to the formula (Eq 11 in Lakshmanan and Kain, 2010):
</p>
<p>M = 1 + gamma * round( CFD(I_(xy))/frequency(I_MODE)),
</p>
<p>where CFD is the cumulative *frequency* distribution (here estimated from the histogram using the &lsquo;hist&rsquo; function), I_(xy) is intensity at grid point (x,y), I_MODE is the mode of intensity values, and gamma is a user-supplied parameter controlling how much to repeat points where higher numbers will result in larger repetitions of high intensity values.
</p>
<p>The function <code>gmm2d</code> fits the 2-d GMM to both fields, <code>plot.gmm2d</code> first uses <code>predict.gmm2d</code> to obtain probabilities for each grid point, and then makes a plot similar to those in Lakshmanan and Kain (2010) Figs. 3, 4 and 5, but giving the probabilities instead of the probabilities times A.  Note that <code>predict.gmm2d</code> can be very slow to compute so that <code>plot.gmm2d</code> can also be very slow.  Less effort was put into speeding these functions up because they are not necessary for obtaining results via the parameters.  However, they can give the user an idea of how good the fit is.
</p>
<p>The 2-d GMM is given by
</p>
<p>G(x,y) = A*sum(lambda*f(x,y))
</p>
<p>where lambda and f(x,y) are numeric vectors of length K, lambda components describe the mixing, and f(x,y) is the bivariate normal distribution with mean (mu.x, mu.y) and covariance function.  &lsquo;A&rsquo; is the total sum of intensities over the field.
</p>
<p>Comparisons between forecast and observed fields are carried out finally by the summary method function.  In particular, the translation error
</p>
<p>e.tr = sqrt((mu.xf - mu.xo)^2 + (mu.yf - mu.yo)^2),
</p>
<p>where f means forecast and o verification fields, resp., and mu .x is the mean in the x- direction, and mu.y in the y- direction.  The rotation error is given by
</p>
<p>e.rot = (180/pi)*acos(theta),
</p>
<p>where theta is the dot product between the first eigenvectors of the covariance matrices for the verification and forecast fields.  The scaling error is given by
</p>
<p>e.sc = Af*lambda.f/Ao*lambda.o,
</p>
<p>where lambda is the mixture component and Af/Ao is the forecast/observed total intensity.
</p>
<p>The overall error (Eq 15 of Lakshmanana and Kain, 2010) is given by
</p>
<p>e.overall = e1 * min(e.tr/e2, 1) + e3*min(e.rot,180 - e.rot)/e4 + e5*(max(e.sc,1/e.sc)-1),
</p>
<p>where e1 to e5 can be supplied by the user, but the defaults are those given by Lakshmanan and Kain (2010).  Namely, e1 = 0.3, e2 = 100, e3=0.2, e4 = 90, and e5=0.5.
</p>


<h3>Value</h3>

<p>For gmm2d, a list object of class &ldquo;gmm2d&rdquo; is returned with components:
</p>
<table>
<tr><td><code>fitX</code>, <code>fitY</code></td>
<td>
<p>list objects returned by the <code>turboem</code> function from the <span class="pkg">turboEM</span> package that describe the EM estimates of the 2-d GMM parameters for the verification and forecast fields, resp.</p>
</td></tr>
<tr><td><code>initX</code>, <code>initY</code></td>
<td>
<p>numeric vectors giving the initial estimates used in the EM algorithm for the verification and forecast fields, resp.  The first 2*K values are the initial mean estiamtes for the x- and y- directions, resp.  The next 4*K values are the initial estiamtes of the covariances (note that the cross-covariance terms are zero regardless of initialization function employed (maybe this will be improved in the future).  The final K values are the initial estimates for lambda.</p>
</td></tr>
<tr><td><code>sX</code>, <code>sY</code></td>
<td>
<p> N by 2 matrix giving the repeated coordinates calculated per M as described in the details section for the verification and forecast fields, resp.</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>single numeric giving the value of K</p>
</td></tr>
<tr><td><code>Ax</code>, <code>Ay</code></td>
<td>
<p>single numerics giving the value of A (the total sum of intensities over the field) for the verifiaction and forecast fields, resp.</p>
</td></tr>
</table>
<p>For plot.gmm2d no value is returned.  A plot is created.
</p>
<p>For predict.gmm2d, a list is returned with components:
</p>
<table>
<tr><td><code>predX</code>, <code>predY</code></td>
<td>
<p>numeric vectors giving the GMM predicted values for the verification and forecast fields, resp.</p>
</td></tr>
</table>
<p>For summary.gmm2d, a list is returned invisibly (if silent is FALSE, information is printed to the screen) with components:
</p>
<table>
<tr><td><code>meanX</code>, <code>meanY</code></td>
<td>
<p>Estimated mean vectors for each GMM component for the verification and forecast fields, resp.</p>
</td></tr>
<tr><td><code>covX</code>, <code>covY</code></td>
<td>
<p>Estimated covariances for each GMM component for the verification and forecast fields, resp.</p>
</td></tr>
<tr><td><code>lambdasX</code>, <code>lambdasY</code></td>
<td>
<p>Estimated mixture components for each GMM component for the verification and forecast fields, resp.</p>
</td></tr>
<tr><td><code>e.tr</code>, <code>e.rot</code>, <code>e.sc</code>, <code>e.overall</code></td>
<td>
<p>K by K matrices giving the errors between each GMM component in the verification field (rows) to each GMM component in the forecast field (columns).  The errors are: translation (e.tr), rotation (e.rot), scaling (e.sc), and overall (e.overall).</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>References</h3>

<p>Lakshmanan, V. and Kain, J. S. (2010) A Gaussian Mixture Model Approach to Forecast Verification. <em>Wea. Forecasting</em>, <b>25</b> (3), 908&ndash;920.
</p>


<h3>See Also</h3>

<p><code><a href="turboEM.html#topic+turboem">turboem</a></code>, <code><a href="#topic+disjointer">disjointer</a></code>, <code><a href="spatstat.geom.html#topic+connected">connected</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
data( "ExampleSpatialVxSet" )

x &lt;- ExampleSpatialVxSet$vx
xhat &lt;- ExampleSpatialVxSet$fcst

u &lt;- min(quantile(c(x[x &gt; 0]), probs = 0.75),
    quantile(c(xhat[xhat &gt; 0]), probs = 0.75))

look &lt;- gmm2d(x, xhat=xhat, threshold=u, verbose=TRUE)
summary(look)
plot(look)

## End(Not run)
## Not run: 
# Alternative method to skin the cat.
hold &lt;- make.SpatialVx( x, xhat, field.type = "MV Gaussian w/ Exp. Cov.",
    units = "units", data.name = "Example", obs.name = "x",
    model.name = "xhat" )

look2 &lt;- gmm2d( hold, threshold = u, verbose = TRUE)
summary(look2)
plot(look2)


## End(Not run)
</code></pre>

<hr>
<h2 id='griddedVgram'>
Variograms for a Gridded Verification Set
</h2><span id='topic+griddedVgram'></span><span id='topic+plot.griddedVgram'></span>

<h3>Description</h3>

<p>Find (and plot) variograms for each field in a gridded verification set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>griddedVgram(object, zero.in = TRUE, zero.out = TRUE, time.point = 1, 
    obs = 1, model = 1, ...)

## S3 method for class 'griddedVgram'
plot(x, ... )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="griddedVgram_+3A_object">object</code></td>
<td>

<p>list object of class &ldquo;SpatialVx&rdquo; containing information on the verification set.
</p>
</td></tr>
<tr><td><code id="griddedVgram_+3A_zero.in">zero.in</code>, <code id="griddedVgram_+3A_zero.out">zero.out</code></td>
<td>
<p>logical, should the variogram be calculated over the entire field (zero.in), and/or over only the non-zero values (zero.out)?</p>
</td></tr>
<tr><td><code id="griddedVgram_+3A_x">x</code></td>
<td>
<p>list object as returned by <code>griddedVgram</code>.</p>
</td></tr>
<tr><td><code id="griddedVgram_+3A_time.point">time.point</code></td>
<td>
<p>numeric or character indicating which time point from the &ldquo;SpatialVx&rdquo; verification set to select for analysis.</p>
</td></tr>
<tr><td><code id="griddedVgram_+3A_obs">obs</code>, <code id="griddedVgram_+3A_model">model</code></td>
<td>
<p>numeric indicating which observation/forecast model to select for the analysis.</p>
</td></tr>
<tr><td><code id="griddedVgram_+3A_...">...</code></td>
<td>

<p>In the case of <code>griddedVgram</code>, these are optional arguments to the <code>vgram.matrix</code> function from package <code>fields</code>.  In the case of <code>plot.griddedVgram</code>, these are optional arguments to <code>plot.vgram.matrix</code>, which in turn are optional arguments to <code>image.plot</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Here, the terms semi-variogram and variogram are used interchangeably.
</p>
<p>This is a simple wrapper function to <code>vgram.matrix</code> (entire field) from <span class="pkg">fields</span> and/or <code>variogram.matrix</code> (non-zero grid points only) for finding the variogram between two gridded fields.  It calls this function for each of two fields in a verification set.  This function allows one to do the diagnostic analysis proposed in Marzban and Sangathe (2009).
</p>


<h3>Value</h3>

<p>A list object containing the entire list passed in by the object argument, and components:
</p>
<table>
<tr><td><code>Vx.cgram.matrix</code>, <code>Fcst.vgram.matrix</code></td>
<td>
<p>list objects as returned by vgram.matrix containing the variogram information for each field.</p>
</td></tr>
</table>
<p>No value is returned by plot.griddedVgram, plots are created showing the empirical variogram (circles), along with directional empirical variograms (dots), and the variogram by direction (image plot).
</p>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>References</h3>

<p>Marzban, C. and Sandgathe, S. (2009) Verification with variograms.  <em>Wea. Forecasting</em>, <b>24</b> (4), 1102&ndash;1120, doi:10.1175/2009WAF2222122.1.
</p>


<h3>See Also</h3>

<p><code><a href="fields.html#topic+vgram.matrix">vgram.matrix</a></code>, <code><a href="#topic+make.SpatialVx">make.SpatialVx</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>data( "ExampleSpatialVxSet" )

x &lt;- ExampleSpatialVxSet$vx
xhat &lt;- ExampleSpatialVxSet$fcst

hold &lt;- make.SpatialVx( x, xhat, field.type = "contrived",
    units="none", data.name = "Example", obs.name = "x",
    model.name = "xhat" )

res &lt;- griddedVgram( hold, R = 8 )
plot( res )

</code></pre>

<hr>
<h2 id='hiw'>
Spatial Forecast Verification Shape Analysis
</h2><span id='topic+hiw'></span><span id='topic+distill.hiw'></span><span id='topic+plot.hiw'></span><span id='topic+print.hiw'></span><span id='topic+summary.hiw'></span>

<h3>Description</h3>

<p>Shape analysis for spatial forecast verification (hiw is OE for shape; yields MnE hue).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hiw(x, simplify = 0, A = pi * c(0, 1/16, 1/8, 1/6, 1/4, 1/2, 9/16, 5/8, 2/3, 3/4),
    verbose = FALSE, ...)

## S3 method for class 'hiw'
distill(x, ...)

## S3 method for class 'hiw'
plot(x, ..., which = c("X", "Xhat"), ftr.num = 1, zoom = TRUE, 
    seg.col = "darkblue")

## S3 method for class 'hiw'
print(x, ...)

## S3 method for class 'hiw'
summary(object, ..., silent = FALSE)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hiw_+3A_x">x</code>, <code id="hiw_+3A_object">object</code></td>
<td>

<p><code>hiw</code>: object of class &ldquo;features&rdquo;.
</p>
<p><code>distill</code>, <code>plot</code>, <code>summary</code>: object of class &ldquo;hiw&rdquo;.
</p>
</td></tr>
<tr><td><code id="hiw_+3A_simplify">simplify</code></td>
<td>

<p><code>dmin</code> argument in call to <code>simplify.owin</code> from <span class="pkg">spatstat</span>.  If 0 (default), then no call is made to <code>simplify.owin</code>.
</p>
</td></tr>
<tr><td><code id="hiw_+3A_a">A</code></td>
<td>

<p>numeric vector of angles for which to apply shape analysis.  Note that this vector will be rounded to 6 digits.  If values are less than that, might be prudent to add 1e-6 to them.
</p>
</td></tr>
<tr><td><code id="hiw_+3A_verbose">verbose</code></td>
<td>
<p>logical, should progress information be printed to the screen?</p>
</td></tr>
<tr><td><code id="hiw_+3A_which">which</code></td>
<td>
<p>character string naming whether to plot a feature from the obsevation field (default) or the forecast field.</p>
</td></tr>
<tr><td><code id="hiw_+3A_ftr.num">ftr.num</code></td>
<td>
<p>integer stating which feature number to plot.</p>
</td></tr>
<tr><td><code id="hiw_+3A_zoom">zoom</code></td>
<td>
<p>logical, should the feature be plotted within its original domain, or a blow-up of the feature (default)?</p>
</td></tr>
<tr><td><code id="hiw_+3A_seg.col">seg.col</code></td>
<td>
<p>color for the line segments.</p>
</td></tr>
<tr><td><code id="hiw_+3A_silent">silent</code></td>
<td>
<p>logical, should the summary information be printed to the screen?</p>
</td></tr>
<tr><td><code id="hiw_+3A_...">...</code></td>
<td>

<p>Not used by <code>hiw</code>, <code>distill</code>, <code>summary</code>.  Optional arguments to <code>plot</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is an attempt to approximate the technique described first in Micheas et al. (2007) and as modified in Lack et al. (2010).  It will only find the centroids, rays extending from them to the boundaries, and the boundary points.  Use <code>distill</code> to convert this output into an object readable by, for example, <code>procGPA</code> from package <span class="pkg">shapes</span>.
</p>
<p>First, identified features (which may be identified by any feature identification function that yields an object of class &ldquo;features&rdquo;) are taken, the centroid is found (the centroid is found via <code>centroid.owin</code> so that the x- and y- coordinates are fliped from what you might expect) and very long line segments are found radiating out in both directions from the center.  They are then clipped by where they cross the boundaries of the features.
</p>
<p>The <span class="pkg">spatstat</span> package is used heavily by this function.  In particular, the function <code>as.polygonal</code> is applied to the <code>owin</code> objects (possible after first calling <code>simplify.owin</code>).  Line segments are created using the feature centroids, as found by <code>centroid.owin</code>, and the user-supplied angles, along with a very long length (equal to the domain size).  Boundary crossings are found using <code>crossing.psp</code>, and new line segment patterns are created using the centroids and boundary crossing information (extra points along line segments are subsequently removed through a painstaking process, and <code>as.psp</code> is called again, and any missing line segments are subsequently accounted for, for later calculations).  Additionally, lengths of line segments are found via <code>lengths_psp</code>.  Angles must also be re-determined and corresponded to the originally passed angles.  Therefore, it is necessary to round the angles to 6 digits, or &ldquo;equal&rdquo; angles may not be considered equal, which will cause problems.
</p>
<p>The <code>hiw</code> function merely does the above step, as well as finds the lengths of the resulting line segments.  For non-convex objects, the longest line segment is returned, and if the boundary crossings do not lie on opposite sides of the centroid, then the negative of the shortest segment is returned for that particular value.  Also returned are the mean, min and maximum intensities for each feature, as well as the final angles returned.  It is possible to have missing values for some of these components.
</p>
<p>The <code>summary</code> function computes SSloc, SSavg, SSmin, and SSmax between each pair of features between fields.  <code>distill</code> may be used to create an object that can be further analyzed (for shape) using the <span class="pkg">shapes</span> package.
</p>
<p>While any feature identification function may be used, it is recommended to throw out small sized features as the results may be misleading (e.g., comparisons between features consisting of single points, etc.).
</p>


<h3>Value</h3>

<p>A list object of class &ldquo;hiw&rdquo; is returned with components the same as in the original &ldquo;features&rdquo; class object, as well as:
</p>
<table>
<tr><td><code>radial.segments</code></td>
<td>
<p>a list with components X and Xhat each giving lists of the &ldquo;psp&rdquo; class (i.e., line segment) object for each feature containing the radial segments from the feature centroids to the boundaries.</p>
</td></tr>
<tr><td><code>centers</code></td>
<td>
<p>list with components X and Xhat giving two-column matrices containing the x- and y- coordinate centroids for each feature (as determined by centroid.owin).</p>
</td></tr>
<tr><td><code>intensities</code></td>
<td>
<p>list with components X and Xhat giving three-column matrices that contain the mean, min and max intensities for each feature.</p>
</td></tr>
<tr><td><code>angles</code>, <code>lengths</code></td>
<td>
<p>list with components X and Xhat each giving lists containing the lengths of the line segments and their respective angles.  Missing values are possible here.</p>
</td></tr>
</table>
<p>distill returns an array whose dimensions are the number of landmarks (i.e., boundary points) by two by the number of observed and forecast features.  An attribute called &ldquo;field.identifier&rdquo; is also given that is a character vector containing repeated &ldquo;X&rdquo; and &ldquo;Xhat&rdquo; values identiying which of the third dimension are associated with the observed field (X) and those identified with the forecast field (Xhat).  Note that missing values may be present, which may need to be dealt with (by the user) before using functions from the shapes package.
</p>
<p>summary invisibly returns a list object with components:
</p>
<table>
<tr><td><code>X</code>, <code>Xhat</code></td>
<td>
<p>matrices whose rows represent features and whose columns give their centroids (note that x refers to the columns and y to the rows), as well as the average, min and max intensities.</p>
</td></tr>
<tr><td><code>SS</code></td>
<td>
<p>matrix with four rows and columns equal to the number of possible combinations of feature matchings between fields.  Gives the sum of square translation/location error (i.e., squared centroid distance), as well as the average, min and max squared differences between each combination of features.</p>
</td></tr>
<tr><td><code>ind</code></td>
<td>
<p>two-column matrix whose rows indicate the feature numbers from each field being compared; corresponding to the columns of SS above.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>References</h3>

<p>Lack, S. A., Limpert, G. L. and Fox, N. I. (2010) An object-oriented multiscale verification scheme.  <em>Wea. Forecasting</em>, <b>25</b>, 79&ndash;92.
</p>
<p>Micheas, A. C., Fox, N. I., Lack, S. A., and Wikle, C. K. (2007) Cell identification and verification of QPF ensembles using shape analysis techniques.  <em>J. Hydrology</em>, <b>343</b>, 105&ndash;116.
</p>


<h3>See Also</h3>

<p>To indentify features and create objects of class &ldquo;features&rdquo;, see, for example: <code><a href="#topic+FeatureFinder">FeatureFinder</a></code>
</p>
<p><code><a href="spatstat.geom.html#topic+centroid.owin">centroid.owin</a></code>, <code><a href="spatstat.geom.html#topic+as.psp">as.psp</a></code>, <code><a href="spatstat.geom.html#topic+psp">psp</a></code>, <code><a href="spatstat.geom.html#topic+crossing.psp">crossing.psp</a></code>, <code><a href="spatstat.geom.html#topic+lengths_psp">lengths_psp</a></code>, <code><a href="spatstat.geom.html#topic+angles.psp">angles.psp</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data( "geom000" )
data( "geom001" )
data( "geom004" )
data( "ICPg240Locs" )

hold &lt;- make.SpatialVx( geom000, geom001, thresholds = c(0.01, 50.01),
    projection = TRUE, map = TRUE, loc = ICPg240Locs, loc.byrow = TRUE,
    field.type = "Geometric Objects Pretending to be Precipitation",
    units = "mm/h", data.name = "ICP Geometric Cases", obs.name = "geom000",
    model.name = "geom001" )

look &lt;- FeatureFinder(hold, do.smooth = FALSE, thresh = 2, min.size = 200)

look &lt;- hiw(look)

distill.hiw(look)

# Actually, you just need to type:
# distill(look)

summary(look)

# Note: procGPA will not allow missing values.

par(mfrow=c(1,2))
plot(look)
plot(look, which = "Xhat")

</code></pre>

<hr>
<h2 id='hoods2d'>
Neighborhood Verification Statistics for a Gridded Verification Set.
</h2><span id='topic+hoods2d'></span><span id='topic+plot.hoods2d'></span><span id='topic+print.hoods2d'></span>

<h3>Description</h3>

<p>Calculates most of the various neighborhood verification statistics for a gridded verification set as reviewed in Ebert (2008).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hoods2d(object, which.methods = c("mincvr", "multi.event",
                 "fuzzy", "joint", "fss", "pragmatic"), time.point = 1,
                 obs = 1, model = 1, Pe = NULL, levels = NULL, max.n =
                 NULL, smooth.fun = "hoods2dsmooth", smooth.params =
                 NULL, rule = "&gt;=", verbose = FALSE)

## S3 method for class 'hoods2d'
plot(x, ..., add.text = FALSE)

## S3 method for class 'hoods2d'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hoods2d_+3A_object">object</code></td>
<td>
<p>list object of class &ldquo;SpatialVx&rdquo;.</p>
</td></tr>
<tr><td><code id="hoods2d_+3A_which.methods">which.methods</code></td>
<td>
<p>character vector giving the names of the methods.  Default is for the entire list to be executed.  See Details section for specific option information.</p>
</td></tr>
<tr><td><code id="hoods2d_+3A_time.point">time.point</code></td>
<td>
<p>numeric or character indicating which time point from the &ldquo;SpatialVx&rdquo; verification set to select for analysis.</p>
</td></tr>
<tr><td><code id="hoods2d_+3A_obs">obs</code>, <code id="hoods2d_+3A_model">model</code></td>
<td>
<p>numeric indicating which observation/forecast model to select for the analysis.</p>
</td></tr>
<tr><td><code id="hoods2d_+3A_pe">Pe</code></td>
<td>
<p>(optional) numeric vector of length q &gt;= 1 to be applied to the fields sPy and possibly sPx (see details section).  If NULL, then it is taken to be the most relaxed requirement (i.e., that an event occurs at least once in a neighborhood) of Pe=1/(nlen^2), where nlen is the length of the neighborhood.</p>
</td></tr>
<tr><td><code id="hoods2d_+3A_levels">levels</code></td>
<td>
<p>numeric vector giving the successive values of the smoothing parameter.  For example, for the default method, these are the neighborhood lengths over which the levels^2 nearest neighbors are averaged for each point.  Values should make sense for the specific smoothing function.  For example, for the default method, these should be odd integers.</p>
</td></tr>
<tr><td><code id="hoods2d_+3A_max.n">max.n</code></td>
<td>
<p>(optional) single numeric giving the maximum neighborhood length to use.  Only used if levels are NULL.</p>
</td></tr>
<tr><td><code id="hoods2d_+3A_smooth.fun">smooth.fun</code></td>
<td>
<p>character giving the name of a smoothing function to be applied.  Default is an average over the n^2 nearest neighbors, where n is taken to be each value of the <code>levels</code> argument.</p>
</td></tr>
<tr><td><code id="hoods2d_+3A_smooth.params">smooth.params</code></td>
<td>
<p>list object containing any optional arguments to <code>smooth.fun</code>.  Use NULL if none.</p>
</td></tr>
<tr><td><code id="hoods2d_+3A_rule">rule</code></td>
<td>
<p>character string giving the threshold rule to be applied.  See help file for <code>thresholder</code> function for more information.</p>
</td></tr>
<tr><td><code id="hoods2d_+3A_verbose">verbose</code></td>
<td>
<p>logical, should progress information be printed to the screen?  Will also give the amount of time (in hours, minutes, or seconds) that the function took to run.</p>
</td></tr>
<tr><td><code id="hoods2d_+3A_x">x</code></td>
<td>
<p>list object output from <code>hoods2d</code>.</p>
</td></tr>
<tr><td><code id="hoods2d_+3A_add.text">add.text</code></td>
<td>
<p>logical, should the text values of FSS be added to its quilt plot?</p>
</td></tr>
<tr><td><code id="hoods2d_+3A_...">...</code></td>
<td>
<p>not used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>hoods2d</code> uses an object of class &ldquo;SpatialVx&rdquo; that includes some information utilized by this function, including the thresholds to be used.  The neighborhood methods (cf. Ebert 2008, 2009; Gilleland et al., 2009, 2010) apply a (kernel) smoothing filter (cf. Hastie and Tibshirani, 1990) to either the raw forecast (and possibly also the observed) field(s) or to the binary counterpart(s) determined by thresholding.
</p>
<p>The specific smoothing filter applied for these methods could be of any type, but those described in Ebert (2008) are generally taken to be &ldquo;neighborhood&rdquo; filters.  In some circles, this is referred to as a convolution filter with a boxcar kernel.  Because the smoothing filter can be represented this way, it is possible to use the convolution theorem with the Fast Fourier Transform (FFT) to perform the neighborhood smoothing operation very quickly. The particular approach used here &ldquo;zero pads&rdquo; the field, and replaces all missing values with zero as well, which is also the approach proposed in Roberts and Lean (2008).  If any missing values are introduced after the convolution, they are removed.
</p>
<p>To simplify the notation for the descriptions of the specific methods employed here, the notation of Ebert (2008) is adopted.  That is, if a method uses neighborhood smoothed observations (NO), then the neighborhood smoothed observed field is denoted &lt;X&gt;s, and the associated binary field, by &lt;Ix&gt;s.  Otherwise, if the observation field is not smoothed (denoted by SO in Ebert, 2008), then simply X or Ix are used.  Similarly, for the forecast field, &lt;Y&gt;s or &lt;Iy&gt;s are used for neighborhood smoothed forecast fields (NF).  If it is the binary fields that are smoothed (e.g., the original fields are thresholded before smoothing), then the resulting fields are denoted &lt;Px&gt;s and &lt;Py&gt;s, resp.  Below, NO-NF indicates that a neighborhood smoothed observed field (&lt;Yx&gt;s, &lt;Ix&gt;s, or &lt;Px&gt;s) is compared with a neighborhood smoothed forecast field, and SO-NF indicates that the observed field is not smoothed.
</p>
<p>Options for which.methods include:
</p>
<p>&ldquo;mincvr&rdquo;: (NO-NF) The minimum coverage method compares &lt;Ix&gt;s and &lt;Iy&gt;s by thresholding the neighborhood smoothed fields &lt;Px&gt;s and &lt;Py&gt;s (i.e., smoothed versions of Ix and Iy) to obtain &lt;Ix&gt;s and &lt;Iy&gt;s.  Indicator fields &lt;Ix&gt;s and &lt;Iy&gt;s are created by thresholding &lt;Px&gt;s and &lt;Py&gt;s by frequency threshold <code>Pe</code> given by the <code>obj</code> argument.  Scores calculated between &lt;Ix&gt;s and &lt;Iy&gt;s include: probability of detecting an event (pod, also known as the hit rate), false alarm ratio (far) and ets (cf. Ebert, 2008, 2009).
</p>
<p>&ldquo;multi.event&rdquo;: (SO-NF) The Multi-event Contingency Table method compares the binary observed field Ix against the smoothed forecast indicator field, &lt;Iy&gt;s, which is determined similarly as for &ldquo;mincvr&rdquo; (i.e., using Pe as a threshold on &lt;Py&gt;s).  The hit rate and false alarm rate (F) are calculated (cf. Atger, 2001).
</p>
<p>&ldquo;fuzzy&rdquo;: (NO-NF) The fuzzy logic approach compares &lt;Px&gt;s to &lt;Py&gt;s by creating a new contingency table where hits = sum_i min(&lt;Px&gt;s_i,&lt;Py&gt;s_i), misses = sum_i min(&lt;Px&gt;s_i,1-&lt;Py&gt;s_i), false alarms = sum_i min(1-&lt;Px&gt;s_i,&lt;Py&gt;s_i), and correct negatives = sum_i min(1-&lt;Px&gt;s_i,1-&lt;Py&gt;s_i) (cf. Ebert 2008).
</p>
<p>&ldquo;joint&rdquo;: (NO-NF) Similar to &ldquo;fuzzy&rdquo; above, but hits  = sum_i prod(&lt;Px&gt;s_i,&lt;Py&gt;s_i), misses = sum_i prod(&lt;Px&gt;s_i,1-&lt;Py&gt;s_i), false alarms = sum_i prod(1-&lt;Px&gt;s_i,&lt;Py&gt;s_i), and correct negatives = sum_i prod(1-&lt;Px&gt;s_i,1-&lt;Py&gt;s_i) (cf. Ebert, 2008).
</p>
<p>&ldquo;fss&rdquo;: (NO-NF) Compares &lt;Px&gt;s and &lt;Py&gt;s directly using a Fractions Brier and Fractions Skill Score (FBS and FSS, resp.), where FBS is the mean square difference between &lt;Px&gt;s and &lt;Py&gt;s, and the FSS is one minus the FBS divided by a reference MSE given by the sum of the sum of squares of &lt;Px&gt;s and &lt;Py&gt;s individually, divided by the total (cf. Roberts and Lean, 2008).
</p>
<p>&ldquo;pragmatic&rdquo;: (SO-NF) Compares Ix with &lt;Py&gt;s, calculating the Brier and Brier Skill Score (BS and BSS, resp.), where the reference forecast used for the BSS is taken to be the mean square error between the base rate and Ix (cf. Theis et al., 2005).
</p>


<h3>Value</h3>

<p>A list object of class &ldquo;hoods2d&rdquo; with components determined by the <code>which.methods</code> argument.  Each component is itself a list object containing relevant components to the given method.  For example, hit rate is abbreviated pod here, and if this is an output for a method, then there will be a component named pod (all lower case).  The Gilbert Skill Score is abbreviated 'ets' (equitable threat score; again all lower case here).  The list components will be some or all of the following.
</p>
<table>
<tr><td><code>mincvr</code></td>
<td>
<p>list with components: pod, far and ets</p>
</td></tr>
<tr><td><code>multi.event</code></td>
<td>
<p>list with components: pod, f and hk</p>
</td></tr>
<tr><td><code>fuzzy</code></td>
<td>
<p>list with components: pod, far and ets</p>
</td></tr>
<tr><td><code>joint</code></td>
<td>
<p>list with components: pod, far and ets</p>
</td></tr>
<tr><td><code>fss</code></td>
<td>
<p>list with components: fss, fss.uniform, fss.random</p>
</td></tr>
<tr><td><code>pragmatic</code></td>
<td>
<p>list with components: bs and bss</p>
</td></tr>
</table>
<p>New attributes are added giving the values for some of the optional arguments: levels, max.n, smooth.fun, smooth.params and Pe.
</p>


<h3>Note</h3>

<p>Thresholded fields are taken to be &gt;= the threshold.</p>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>References</h3>

<p>Atger, F. (2001) Verification of intense precipitation forecasts from single models and ensemble prediction systems.  <em>Nonlin. Proc. Geophys.</em>, <b>8</b>, 401&ndash;417.
</p>
<p>Ebert, E. E. (2008) Fuzzy verification of high resolution gridded forecasts: A review and proposed framework.  <em>Meteorol. Appl.</em>, <b>15</b>, 51&ndash;64. doi:10.1002/met.25 
</p>
<p>Ebert, E. E. (2009) Neighborhood verification: A strategy for rewarding close forecasts.  <em>Wea. Forecasting</em>, <b>24</b>, 1498&ndash;1510, doi:10.1175/2009WAF2222251.1.
</p>
<p>Gilleland, E., Ahijevych, D., Brown, B. G., Casati, B. and Ebert, E. E. (2009) Intercomparison of Spatial Forecast Verification Methods. <em>Wea. Forecasting</em>, <b>24</b>, 1416&ndash;1430, doi:10.1175/2009WAF2222269.1.
</p>
<p>Gilleland, E., Ahijevych, D. A., Brown, B. G. and Ebert, E. E. (2010) Verifying Forecasts Spatially. <em>Bull. Amer. Meteor. Soc.</em>, October, 1365&ndash;1373.
</p>
<p>Hastie, T. J. and Tibshirani, R. J. (1990) <em>Generalized Additive Models</em>. Chapman and Hall/CRC Monographs on Statistics and Applied Probability 43, 335pp.
</p>
<p>Roberts, N. M. and Lean, H. W. (2008) Scale-selective verification of rainfall accumulations from high-resolution forecasts of convective events.  <em>Mon. Wea. Rev.</em>, <b>136</b>, 78&ndash;97. doi:10.1175/2007MWR2123.1.
</p>
<p>Theis, S. E., Hense, A. Damrath, U. (2005) Probabilistic precipitation forecasts from a deterministic model: A pragmatic approach.  <em>Meteorol. Appl.</em>, <b>12</b>, 257&ndash;268.
</p>
<p>Yates, E., Anquetin, S. Ducrocq, V., Creutin, J.-D., Ricard, D. and Chancibault, K. (2006) Point and areal validation of forecast precipitation fields.  <em>Meteorol. Appl.</em>, <b>13</b>, 1&ndash;20.
</p>
<p>Zepeda-Arce, J., Foufoula-Georgiou, E., Droegemeier, K. K. (2000) Space-time rainfall organization and its role in validating quantitative precipitation forecasts.  <em>J. Geophys. Res.</em>, <b>105</b>(D8), 10,129&ndash;10,146.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+fft">fft</a></code>, <code><a href="smoothie.html#topic+kernel2dsmooth">kernel2dsmooth</a></code>, <code><a href="#topic+plot.hoods2d">plot.hoods2d</a></code>, <code><a href="#topic+vxstats">vxstats</a></code>, <code><a href="#topic+thresholder">thresholder</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- y &lt;- matrix( 0, 50, 50)
x[ sample(1:50,10), sample(1:50,10)] &lt;- rexp( 100, 0.25)
y[ sample(1:50,20), sample(1:50,20)] &lt;- rexp( 400)
hold &lt;- make.SpatialVx( x, y, thresholds = c(0.1, 0.5), field.type = "Random Exp. Var." )
look &lt;- hoods2d( hold, which.methods=c("multi.event", "fss"), levels=c(1, 3, 19))
look
plot(look)

## Not run: 
data( "UKobs6" )
data( "UKfcst6" )
data( "UKloc" )

hold &lt;- make.SpatialVx( UKobs6, UKfcst6, thresholds = c(0.01, 20.01),
    projection = TRUE, map = TRUE, loc = UKloc, loc.byrow = TRUE,
    field.type = "Precipitation", units = "mm/h",
    data.name = "Nimrod", obs.name = "Observations 6",
    model.name = "Forecast 6" )

hold
plot(hold)
hist(hold, col="darkblue")

look &lt;- hoods2d(hold, which.methods=c("multi.event", "fss"), 
    levels=c(1, 3, 5, 9, 17), verbose=TRUE)
plot(look)

data( "geom001" )
data( "geom000" )
data( "ICPg240Locs" )

hold &lt;- make.SpatialVx( geom000, geom001, thresholds = c(0.01, 50.01),
    projection = TRUE, map = TRUE, loc = ICPg240Locs, loc.byrow = TRUE,
    field.type = "Geometric Objects Pretending to be Precipitation",
    units = "mm/h", data.name = "ICP Geometric Cases", obs.name = "geom000",
    model.name = "geom001" )

look &lt;- hoods2d(hold, levels=c(1, 3, 9, 17, 33, 65, 129, 257), verbose=TRUE)

plot( look) # Might want to use 'pdf' first.

    
## End(Not run)
</code></pre>

<hr>
<h2 id='hoods2dPlot'>Quilt Plot and a Matrix Plot.</h2><span id='topic+hoods2dPlot'></span>

<h3>Description</h3>

<p>Function to make a quilt plot and a matrix plot for a matrix whose rows represent neighborhood lengths, and whose columns represent different threshold choices.</p>


<h3>Usage</h3>

<pre><code class='language-R'>hoods2dPlot(x, args, matplotcol = 1:6, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hoods2dPlot_+3A_x">x</code></td>
<td>
<p>l by q numeric matrix.</p>
</td></tr>
<tr><td><code id="hoods2dPlot_+3A_args">args</code></td>
<td>
<p>list object with components: threshold (numeric vector giving the threshold values), qs (optional numeric vector giving the quantiles used if the thresholds represent quantiles rather than hard values), levels (numeric giving the neighborhood lengths (in grid squares) used, units (optional character giving the units for the thresholds)</p>
</td></tr>
<tr><td><code id="hoods2dPlot_+3A_matplotcol">matplotcol</code></td>
<td>
<p>col argument to <code>matplot</code>.</p>
</td></tr>
<tr><td><code id="hoods2dPlot_+3A_...">...</code></td>
<td>
<p>optional arguments to <code>image</code> and <code>image.plot</code> functions.  May not include xaxt, yaxt, lab, lab, col, or legend.only</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Used by <code>plot.hoods2d</code>, but can be useful for other functions.  Generally, however, this is an internal function that should not be called by the user.  However, it might be called instead of <code>plot.hoods2d</code> in order to make a subset of the available plots.</p>


<h3>Value</h3>

<p>No value is returned.  A plot is created.</p>


<h3>Author(s)</h3>

<p>Eric Gilleland</p>


<h3>References</h3>

<p>Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988) <em>The New S Language</em>.  Wadsworth &amp; Brooks/Cole.
</p>


<h3>See Also</h3>

<p><code><a href="graphics.html#topic+matplot">matplot</a></code>, <code><a href="Matrix.html#topic+image">image</a></code>, <code><a href="fields.html#topic+image.plot">image.plot</a></code>, <code><a href="#topic+plot.hoods2d">plot.hoods2d</a></code>, <code><a href="#topic+hoods2d">hoods2d</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- y &lt;- matrix( 0, 50, 50)
x[ sample(1:50,10), sample(1:50,10)] &lt;- rexp( 100, 0.25)
y[ sample(1:50,20), sample(1:50,20)] &lt;- rexp( 400)

hold &lt;- make.SpatialVx(x, y, thresholds=c(0.1, 0.5), field.type="random")

look &lt;- hoods2d(hold, which.methods=c("multi.event", "fss"),
    levels=c(1, 3, 20))

hoods2dPlot( look$multi.event$hk, args=hold,
    main="Hanssen Kuipers Score (Multi-Event Cont. Table)")
## Not run: 
data( "geom001" )
data( "geom000" )
data( "ICPg240Locs" )

hold &lt;- make.SpatialVx( geom000, geom001, thresholds = c(0.01, 50.01),
    loc = ICPg240Locs, projection = TRUE, map = TRUE, loc.byrow = TRUE,
    field.type = "Precipitation", units = "mm/h",
    data.name = "Geometric ICP Test Cases", obs.name = "geom000",
    model.name = "geom001" )

look &lt;- hoods2d(hold, levels=c(1, 3, 5, 17, 33, 65), verbose=TRUE)
par(mfrow=c(1,2))
hoods2dPlot(look$pragmatic$bss, args=attributes(hold))

## End(Not run)
</code></pre>

<hr>
<h2 id='hump'>
Simulated Forecast and Verification Fields
</h2><span id='topic+hump'></span>

<h3>Description</h3>

<p>Simulated forecast and verification fields for optical flow example
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(hump)</code></pre>


<h3>Format</h3>

<p>The format is:
List of 2
$ initial: num [1:50, 1:50] 202 210 214 215 212 ...
$ final  : num [1:50, 1:50] 244 252 257 258 257 ...
</p>


<h3>Details</h3>

<p>Although not identically the same as the data used in Fig. 1 of Marzban and Sandgathe (2010), these are forecast data simulated from the self-same distribution and perturbed in the same manner to get the observation.  The component initial is the forecast and final is the observation.
</p>
<p>The forecast is on a 50 X 50 grid simulated from a bivariate Gaussian with standard deviation of 11 and centered on the coordinate (10, 10).  The observed field is the same as the forecast field, but shifted one grid length in each direction and has 60 added to it everywhere.
</p>


<h3>References</h3>

<p>Marzban, C. and Sandgathe, S. (2010)  Optical flow for verification.  <em>Wea. Forecasting</em>, <b>25</b>, 1479&ndash;1494, doi:10.1175/2010WAF2222351.1.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(hump)
str(hump)
## Not run: 
initial &lt;- hump$initial
final &lt;- hump$final
look &lt;- OF(final, initial, W=9, verbose=TRUE)
plot(look) # Compare with Fig. 1 in Marzban and Sandgathe (2010).
hist(look) # 2-d histogram.
plot(look, full=TRUE) # More plots.

## End(Not run)
</code></pre>

<hr>
<h2 id='imomenter'>
Image Moments
</h2><span id='topic+imomenter'></span><span id='topic+imomenter.im'></span><span id='topic+imomenter.matrix'></span><span id='topic+print.imomented'></span>

<h3>Description</h3>

<p>Calculate some of the raw image moments, as well as some useful image characteristics.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>imomenter(x, loc = NULL, ...)

## S3 method for class 'im'
imomenter(x, loc = NULL, ...)

## S3 method for class 'matrix'
imomenter(x, loc = NULL, ...)

## S3 method for class 'imomented'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="imomenter_+3A_x">x</code></td>
<td>

<p><code>imomenter</code>: matrix or object of class &ldquo;im&rdquo; (from package <span class="pkg">spatstat</span>).
</p>
<p><code>print</code>: object of class &ldquo;imomenter&rdquo;.
</p>
</td></tr>
<tr><td><code id="imomenter_+3A_loc">loc</code></td>
<td>

<p>A two-column matrix giving the location coordinates.  May be missing in which case they are assumed to be integers giving the row and column numbers.
</p>
</td></tr>
<tr><td><code id="imomenter_+3A_...">...</code></td>
<td>

<p>Not used.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Calculates Hu's image moments (Hu 1962).  Calculates the raw moments: M00 (aka area), M10, M01, M11, M20, and M02, as well as the (normalized) central moments: mu11', mu20', and mu02', which are returned as the image covariance matrix: rbind(c(mu20', mu11'), c(mu11', mu02')).  In addition, the image centroid and orientation angle are returned, as calculated using the image moments.  It should be noted that while the centroid is technically defined for the null case (all zero-valued grid points), the way it is calculated using image moments means that it will be undefined because of division by zero in the formulation.
</p>
<p>The orientation angle calculated here is that which is used by MODE, although not currently used in the MODE analyses in this package (smatr is used instead to find the major axis, etc).  The eigenvalues of the image covariance correspond to the major and minor axes of the image.
</p>
<p>For more information on image moments, see Hu (1962).
</p>


<h3>Value</h3>

<p>A list object of class &ldquo;imomented&rdquo; is returned with components:
</p>
<table>
<tr><td><code>area</code></td>
<td>
<p>Same as M00.</p>
</td></tr>
<tr><td><code>centroid</code></td>
<td>
<p>numeric with named components &ldquo;x&rdquo; and &ldquo;y&rdquo; giving the x- and y- coordinates of the centroid as calculated by the image moment method.</p>
</td></tr>
<tr><td><code>orientation.angle</code></td>
<td>
<p>The orientation angle of the image as calculated by image moments.</p>
</td></tr>
<tr><td><code>raw.moments</code></td>
<td>
<p>named numeric vector with the raw image moments: M00, M10, M01, M11, M20 and M02 used in calculating the other returned values.</p>
</td></tr>
<tr><td><code>cov</code></td>
<td>
<p>2 by 2 image covariance as calculated by the image moment method.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>References</h3>

<p>Hu, M. K. (1962) Visual Pattern Recognition by Moment Invariants. <em>IRE Trans. Info. Theory</em>, <b>IT-8</b>, 179&ndash;187.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Mij">Mij</a></code>, <code><a href="#topic+FeatureAxis">FeatureAxis</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
look &lt;- matrix(0, 10, 10)

look[3:5, 7:8] &lt;- rnorm(6)

imomenter(look)

## Not run: 
data( "geom000" )
data( "ICPg240Locs" )

imomenter( geom000 )
imomenter( geom000, loc = ICPg240Locs )

data( "geom004" )

imomenter( geom004 )

imomenter( geom004, loc = ICPg240Locs )

## End(Not run)

</code></pre>

<hr>
<h2 id='interester'>
Feature Interest
</h2><span id='topic+interester'></span><span id='topic+print.interester'></span><span id='topic+summary.interester'></span>

<h3>Description</h3>

<p>Calculate interest maps for specific feature comparisons and compute the total interest, as well as median of maximum interest.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>interester(x, properties = c("cent.dist", "angle.diff", "area.ratio", "int.area",
    "bdelta", "haus", "ph", "med", "msd", "fom", "minsep"),
    weights = c(0.24, 0.12, 0.17, 0.12, 0, 0, 0, 0, 0, 0, 0.35),
    b1 = c(35, 30, 0, 0, 0.5, 35, 20, 40, 120, 1, 40),
    b2 = c(100, 90, 0.8, 0.25, 85, 400, 200, 200, 400, 0.25, 200),
    verbose = FALSE, ...)

## S3 method for class 'interester'
print(x, ...)

## S3 method for class 'interester'
summary(object, ...,
    min.interest = 0.8, long = TRUE, silent = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="interester_+3A_x">x</code></td>
<td>

<p><code>interester</code>: object of class &ldquo;features&rdquo; or &ldquo;matched&rdquo;.
</p>
<p><code>print</code>: object of class &ldquo;interester&rdquo;.
</p>
</td></tr>
<tr><td><code id="interester_+3A_object">object</code></td>
<td>
<p>object of class &ldquo;interester&rdquo;.</p>
</td></tr>
<tr><td><code id="interester_+3A_properties">properties</code></td>
<td>

<p>character vector naming which properties from <code>FeatureComps</code> should be considered in the total interest.
</p>
</td></tr>
<tr><td><code id="interester_+3A_weights">weights</code></td>
<td>

<p>numeric of length equal to the length of <code>properties</code>.  Weights equal to zero will result in the removal of those properties from <code>properties</code> so that the default computes only those values utilized in Davis et al (2009).
</p>
</td></tr>
<tr><td><code id="interester_+3A_b1">b1</code>, <code id="interester_+3A_b2">b2</code></td>
<td>

<p>All interest maps (except that for &ldquo;fom&rdquo;) are piecewise linear, and of the form: f(x) = 1,0 (depending on the property) if x &lt; <code>b1</code>, x &lt;= <code>b1</code>, x &gt; <code>b2</code> or x &gt;= <code>b2</code> (see details for more information).
</p>
</td></tr>
<tr><td><code id="interester_+3A_verbose">verbose</code></td>
<td>

<p>logical, should progress information be printed to the screen?
</p>
</td></tr>
<tr><td><code id="interester_+3A_min.interest">min.interest</code></td>
<td>
<p>numeric between zero and one giving the desired minimum value of interest.  Only used for display purposes.  If <code>long</code> is TRUE, a dashed line is printed where the total interest falls below this value.  If <code>long</code> is FALSE, only the total interest values above this value are displayed.  All values are returned invisibly regardless of the values of <code>min.interest</code> and <code>long</code>.</p>
</td></tr>
<tr><td><code id="interester_+3A_long">long</code></td>
<td>
<p>logical, should all interest values be displayed (TRUE) or only those above <code>min.interest</code> (FALSE)?</p>
</td></tr>
<tr><td><code id="interester_+3A_silent">silent</code></td>
<td>
<p>logical, should summary information be displayed to the screen (FALSE)?</p>
</td></tr>
<tr><td><code id="interester_+3A_...">...</code></td>
<td>

<p><code>interester</code>: optional arguments to <code>FeatueComps</code>.
</p>
<p>Not used by <code>print</code> or <code>summary</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function calculates the feature interest according to the MODE algorithm described in Davis et al (2009).  Properties that can be computed are those available in <code>FeatureComps</code>, except for &ldquo;bearing&rdquo;.  Interest maps are computed according to piece-wise linear functions (except for &ldquo;fom&rdquo;) depending on the property.  For all properties besides &ldquo;area.ratio&rdquo;, &ldquo;int.area&rdquo; and &ldquo;fom&rdquo;, the interest maps are of the form:
</p>
<p>f(x) = 1, if x &lt;= b1
</p>
<p>f(x) = a0 + a1 * x, if x &gt; b1 and x &lt;= b2, where a1 = -1/(b2 - b1) and a0 = 1 - a1 * b1
</p>
<p>f(x) = 0, if x &gt; b2
</p>
<p>For properties &ldquo;area.ratio&rdquo; and &ldquo;int.area&rdquo;, the interest maps are of the form:
</p>
<p>f(x) = 0, if x &lt; b1
</p>
<p>f(x) = a0 + a1 * x, if x &gt;= b1 and x &lt; b2, where a1 = 1/(b2 - b1) and a0 = 1 - a1 * b2
</p>
<p>f(x) = 1, if x &gt;= b2
</p>
<p>Finally, for &ldquo;fom&rdquo;, a function that tries to give as much weight to values near one is applied.  It is given by:
</p>
<p>f(x) = b1 * exp(-0.5 * ((x - 1) / b2)^4)
</p>
<p>The default values for b1 and b2 will not necessarily give the same results as in Davis et al (2009), but also, the distance map for their intersection area ratio differs from that here.  The interest function for FOM is further restricted to fall within the interval [0, 1], so care should be taken if b1 and/or b2 are changed for this function.
</p>
<p>The <code>interester</code> function calculates the individual interest values for each property and each pair of features, and returns both these individual interest values as well as a matrix of total interest.  The <code>print</code> function will print the entire matrix of individual interest values if there are fewer than twenty pairs of features, and will print their summary otherwise.  The <code>summary</code> function will order the total interest from highest to lowest and print this information (along with which feature pairs correspond to the total interest value).  It will also calculate the median of maximum interest (MMI) as suggested by Davis et al (2009).  If there is only one feature in either field, then this value will just be the maximum total interest.
</p>
<p>The centroid distance property is less meaningful if the sizes of the two features differ greatly, and therefore, the interest value for this property is further multiplied by the area ratio of the two features.  Similarly, angle difference is less meaningful if one or both of the features are circular in shape.  Therefore, this property is multiplied by the following factor, following Davis et al (2009) Eq (A1), where r1 and r2 are the aspect ratios (defined as the length of the minor axis divided my that of the major axis) of the two features, respectively.
</p>
<p>sqrt( [ (r1 - 1)^2 / (r1^2 + 1) ]^0.3 * [ (r2 - 1)^2 / (r2^2 + 1) ]^0.3 )
</p>
<p>The <code>print</code> function displays either the individualinterest values for each property and feature pairings, or more often, a summary of these values (if the display would otherwise be too large).  It also shows a matrix whose rows are observed features and columns forecast features, with the total interest values therein associated.
</p>
<p><code>summary</code> shows the sorted total interest from highest to lowest for each pair.  A dashed line separates the values above <code>min.interest</code> from those below, and if <code>long</code> is TRUE, then values below that line are not displayed.  It also reports the median of maximum interest (MMI) defined by Davis et al (2009) as an overall feature-based summary of forecast performance.  It is derived by collecting the row maxima and column maxima from the total interest matrix, shown by the <code>print</code> command, into a vector, and then finding the median of this vector.
</p>


<h3>Value</h3>

<p>A list of class &ldquo;interester&rdquo; is returned with components:
</p>
<table>
<tr><td><code>interest</code></td>
<td>
<p>matrix whose named rows correspond to the each property that was calculated and whose columns are feature pairings.  The values are the interest calculated for the specific property and pair of features.</p>
</td></tr>
<tr><td><code>total.interest</code></td>
<td>
<p>matrix of total interest for each pair of features where rows are observed features and columns are forecast features.</p>
</td></tr>
</table>
<p>If no features are available in either field, NULL is returned.
</p>
<p>Nothing is returned by print.
</p>
<p>summary invisibly returns a list object of class &ldquo;summary.interester&rdquo; with components:
</p>
<table>
<tr><td><code>sorted.interest</code></td>
<td>
<p>similar to the interest component from the value returned by interester, but sorted from highest to lowest interest, along with the feature number information for each field.</p>
</td></tr>
<tr><td><code>mmi</code></td>
<td>
<p>the median of maximum interest value.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The terminology used for features within the entire <span class="pkg">SpatialVx</span> package attempts to avoid conflict with terminology used by R.  So, for example, the term property is used in lieu of &ldquo;attributes&rdquo; so as not to be confused with R object attributes.  The term &ldquo;feature&rdquo; is used in place of &ldquo;object&rdquo; to avoid confusion with an R object, etc.
</p>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>References</h3>

<p>Davis, C. A., Brown, B. G., Bullock, R. G. and Halley Gotway, J. (2009) The Method for Object-based Diagnostic Evaluation (MODE) applied to numerical forecasts from the 2005 NSSL/SPC Spring Program.  <em>Wea. Forecsting</em>, <b>24</b>, 1252&ndash;1267, DOI: 10.1175/2009WAF2222241.1.
</p>


<h3>See Also</h3>

<p>Identifying features: <code><a href="#topic+FeatureFinder">FeatureFinder</a></code>
</p>
<p>Functions for calculating the properties: <code><a href="#topic+FeatureComps">FeatureComps</a></code>, <code><a href="#topic+FeatureProps">FeatureProps</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
x &lt;- y &lt;- matrix(0, 100, 100)
x[ 2:3, c(3:6, 8:10) ] &lt;- 1
y[ c(4:7, 9:10), c(7:9, 11:12) ] &lt;- 1
     
x[ 30:50, 45:65 ] &lt;- 1
y[ c(22:24, 99:100), c(50:52, 99:100) ] &lt;- 1
     
hold &lt;- make.SpatialVx(x, y, field.type = "contrived", units = "none",
    data.name = "Example", obs.name = "x", model.name = "y" )
     
look &lt;- FeatureFinder(hold, smoothpar = 0.5) 
     
look2 &lt;- interester(look)
look2
summary(look2)

</code></pre>

<hr>
<h2 id='iwarper'>
Image Warping By Hand
</h2><span id='topic+iwarper'></span>

<h3>Description</h3>

<p>Instigate an image warp by selecting control points in the zero- and one-energy images by hand.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>iwarper(x0, x1, nc = 4, labcol = "magenta", 
    col = c("gray", tim.colors(64)), zlim, cex = 2, alwd = 1.25, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="iwarper_+3A_x0">x0</code>, <code id="iwarper_+3A_x1">x1</code></td>
<td>

<p>Numeric matrices giving the zero- and one-energy images.  The <code>x1</code> image is ultimately warped into the <code>x0</code> image.
</p>
</td></tr>
<tr><td><code id="iwarper_+3A_nc">nc</code></td>
<td>

<p>integer giving the number of control points to select.
</p>
</td></tr>
<tr><td><code id="iwarper_+3A_labcol">labcol</code></td>
<td>

<p>character describing the color to use when labeling the control points as they are selected.
</p>
</td></tr>
<tr><td><code id="iwarper_+3A_col">col</code></td>
<td>

<p>The color scheme to use in plotting the images.
</p>
</td></tr>
<tr><td><code id="iwarper_+3A_zlim">zlim</code></td>
<td>

<p>Range of values for the color scheme in plotting the images.
</p>
</td></tr>
<tr><td><code id="iwarper_+3A_cex">cex</code></td>
<td>

<p>The usual <code>cex</code> parameter.  See the help file for <code>par</code> for more information.
</p>
</td></tr>
<tr><td><code id="iwarper_+3A_alwd">alwd</code></td>
<td>

<p>line width for the arrows added to the deformation plot.
</p>
</td></tr>
<tr><td><code id="iwarper_+3A_...">...</code></td>
<td>

<p>Optional arguments to <code>poly.image</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A pair-of-thin-plate-splines image warp mapping is estimated by hand.  See Dryden and Mardia (1998) Chapter 10 for more information.
</p>


<h3>Value</h3>

<p>A list object of class &ldquo;iwarped&rdquo;
</p>
<table>
<tr><td><code>Im0</code>, <code>Im1</code>, <code>Im1.def</code></td>
<td>
<p>The zero- and one-energy images and the deformed one-energy image, resp.</p>
</td></tr>
<tr><td><code>p0</code>, <code>p1</code></td>
<td>
<p>The nc by 2 column matrices of hand-selected zero- and one-energy control points, resp.</p>
</td></tr> 
<tr><td><code>warped.locations</code>, <code>s</code></td>
<td>
<p>Two-column matrices giving the entire set of warped locations and original locations, resp.</p>
</td></tr>
<tr><td><code>theta</code></td>
<td>
<p>The matrices defining the image warp, L, iL and B, where the last is the bending energy, and the first two are nc + 3 by nc + 3 matrices describing the control points and inverse control-point matrices.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>References</h3>

<p>Dryden, I. L. and K. V. Mardia (1998) <em>Statistical Shape Analysis</em>.  Wiley, New York, NY, 347pp.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+warper">warper</a></code>
</p>

<hr>
<h2 id='locmeasures2d'>
Binary Image Measures
</h2><span id='topic+locmeasures2d'></span><span id='topic+locmeasures2d.default'></span><span id='topic+locmeasures2d.SpatialVx'></span><span id='topic+summary.locmeasures2d'></span><span id='topic+print.locmeasures2d'></span>

<h3>Description</h3>

<p>Calculate some binary image measures between two fields.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>locmeasures2d(object, which.stats = c("bdelta", "haus", "qdmapdiff", 
    "med", "msd", "ph", "fom"), distfun = "distmapfun", distfun.params = NULL, 
    k = NULL, alpha = 0.1, bdconst = NULL, p = 2, ...)

## Default S3 method:
locmeasures2d(object, which.stats = c("bdelta", "haus", "qdmapdiff", 
    "med", "msd", "ph", "fom"), distfun = "distmapfun", distfun.params = NULL, 
    k = NULL, alpha = 0.1, bdconst = NULL, p = 2, ..., Y, thresholds=NULL)

## S3 method for class 'SpatialVx'
locmeasures2d(object, which.stats = c("bdelta", "haus", "qdmapdiff", 
    "med", "msd", "ph", "fom"), distfun = "distmapfun", distfun.params = NULL, 
    k = NULL, alpha = 0.1, bdconst = NULL, p = 2, ..., time.point = 1, 
    obs = 1, model = 1)

## S3 method for class 'locmeasures2d'
print(x, ...)

## S3 method for class 'locmeasures2d'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="locmeasures2d_+3A_object">object</code></td>
<td>

<p>For <code>locmeasures2d</code>, an object of class &ldquo;SpatialVx&rdquo; or a valid matrix in which case <code>Y</code> must be explicitly provided.  For <code>summary</code> method function, a list object output from <code>locmeasures2d</code>.
</p>
</td></tr>
<tr><td><code id="locmeasures2d_+3A_x">x</code></td>
<td>
<p>returned object from <code>locmeasures2d</code>.</p>
</td></tr>
<tr><td><code id="locmeasures2d_+3A_which.stats">which.stats</code></td>
<td>

<p>character vector telling which measures should be calculated.
</p>
</td></tr>
<tr><td><code id="locmeasures2d_+3A_distfun">distfun</code></td>
<td>
<p>character naming a function to calculate the shortest distances between each point x in the grid and the set of events.  Default is the Euclidean distance metric. Must take <code>x</code> as an argument, which is the event field for which the distances are to be calculated.  Must return a matrix of the same dimension as x.</p>
</td></tr>
<tr><td><code id="locmeasures2d_+3A_distfun.params">distfun.params</code></td>
<td>
<p>list with named components giving any additional arguments to the <code>distfun</code> function.</p>
</td></tr>
<tr><td><code id="locmeasures2d_+3A_k">k</code></td>
<td>

<p>numeric vector for use with the partial Hausdorff distance.  For k that are whole numerics or integers &gt;= 1, then the k-th highest value is returned by <code>locmeasures2d</code>.  If 0 &lt;= k &lt; 1, then the corresponding quantile is returned.
</p>
</td></tr>
<tr><td><code id="locmeasures2d_+3A_alpha">alpha</code></td>
<td>
<p>numeric giving the alpha parameter for Pratt's Figure of Merit (FOM).  See the help file for <code>locperf</code> for more details.</p>
</td></tr>
<tr><td><code id="locmeasures2d_+3A_bdconst">bdconst</code></td>
<td>

<p>numeric giving the cut-off value for Baddeley's delta metric.
</p>
</td></tr>
<tr><td><code id="locmeasures2d_+3A_p">p</code></td>
<td>

<p>numeric vector giving one or more values for the parameter p in Baddeley's delta metric.  Usually this is just 2.
</p>
</td></tr>
<tr><td><code id="locmeasures2d_+3A_y">Y</code></td>
<td>
<p>m X n matrix giving the forecast field.</p>
</td></tr>
<tr><td><code id="locmeasures2d_+3A_thresholds">thresholds</code></td>
<td>
<p>numeric or two-column matrix giving the threshold to be applied to the verification (column one) and forecast (column two) fields.  If a vector, same thresholds are applied to both fields.</p>
</td></tr>
<tr><td><code id="locmeasures2d_+3A_time.point">time.point</code></td>
<td>
<p>numeric or character indicating which time point from the &ldquo;SpatialVx&rdquo; verification set to select for analysis.</p>
</td></tr>
<tr><td><code id="locmeasures2d_+3A_obs">obs</code>, <code id="locmeasures2d_+3A_model">model</code></td>
<td>
<p>numeric indicating which observation/forecast model to select for the analysis.</p>
</td></tr>
<tr><td><code id="locmeasures2d_+3A_...">...</code></td>
<td>

<p>optional arguments to <code>deltametric</code> and <code>distmap</code> from package spatstat.  Not used by the <code>summary</code> or <code>print</code> methods here.  </p>
</td></tr>
</table>


<h3>Details</h3>

<p>It is useful to introduce some notation.  Let d(x,A) be the shortest distance from a point x, anywhere in the grid, to a set A contained in the grid.  Here, Euclidean distance is used (default) for d(x,A), but note that some papers (e.g., Venugopal et al., 2005) use other distances, such as the taxi-cab distance (use <code>distfun</code> argument to change the distance method).
</p>
<p>The Hausdorff distance between two sets A and B contained in the finite grid is given by max( max( d(x,A), x in B), max( d(x,B), x in A)), and can be re-written as H(A,B) = max( abs( d(x,A) - d(x,B))), where x is taken over all points in the grid.  Several of the distances here are modifications of the Hausdorff distance.  The Baddeley metric, for example, is the Lp norm of abs( w(d(x,A)) - w(d(x,B))), where again x is taken from over the entire grid, and w is any concave continuous function that is strictly increasing at zero.  Here, w(t) = min( t, c), where c is some constant given by the <code>bdconst</code> argument.
</p>
<p>Calculates one or more of the following binary image measures:
</p>
<p>&ldquo;bdelta&rdquo; Baddeley delta metric (Baddeley, 1992a,b; Gilleland, 2011; Schwedler and Baldwin, 2011)
</p>
<p>&ldquo;haus&rdquo; Hausdorff distance (Baddeley, 1992b; Schwedler and Baldwin, 2011)
</p>
<p>&ldquo;qdmapdiff&rdquo; Quantile (or rank) of the differences in distance maps.  See the help file for <code>locperf</code>.
</p>
<p>&ldquo;med&rdquo; Mean Error Distance (Peli and Malah, 1982; Baddeley, 1992a).  See the help file for <code>locperf</code>.
</p>
<p>&ldquo;msd&rdquo; Mean Square Error Distance (Peli and Malah, 1982; Baddeley, 1992a).  See the help file for <code>locperf</code>.
</p>
<p>&ldquo;ph&rdquo; Partial Hausdorff distance.  See the help file for <code>locperf</code>.
</p>
<p>&ldquo;fom&rdquo; Pratt's Figure of Merit (Peli and Malah, 1982; Baddeley, 1992a, Eq (1)).  See the help file for <code>locperf</code>.
</p>
<p>These distances are summaries in and of themselves, so the summary method function simply displays the results in an easy to read manner.
</p>


<h3>Value</h3>

<p>A list with at least one of the following components depending on the argument which.stats
</p>
<table>
<tr><td><code>bdelta</code></td>
<td>
<p>p by q matrix giving the Baddeley delta metric for each desired value of p (rows) and each threshold (columns)</p>
</td></tr>
<tr><td><code>haus</code></td>
<td>
<p>numeric vector giving the Hausdorff distance for each threshold</p>
</td></tr>
<tr><td><code>qdmapdiff</code></td>
<td>
<p>k by q matrix giving the difference in distance maps for each of the k-th largest value(s) or quantile(s) (rows) for each threshold (columns).</p>
</td></tr>
<tr><td><code>medMiss</code>, <code>medFalseAlarm</code>, <code>msdMiss</code>, <code>msdFalseAlarm</code></td>
<td>
<p>two-row matrix giving the mean error (or square error) distance as (Forecast, Observation) or misses and (Observation, Forecast) or false alarms.</p>
</td></tr>
<tr><td><code>ph</code></td>
<td>
<p>k by q matrix giving the k-th largest value(s) or quantile(s) (rows) for each threshold (columns) of the maximum between the distances from one field to the other.</p>
</td></tr>
<tr><td><code>fom</code></td>
<td>
<p>numeric vector giving Pratt's figure of merit.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Binary fields are determined by having values &gt;= the thresholds.
</p>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>References</h3>

<p>Baddeley, A. (1992a)  An error metric for binary images.  In <em>Robust Computer Vision Algorithms</em>, W. Forstner and S. Ruwiedel, Eds., Wichmann, 59&ndash;78.
</p>
<p>Baddeley, A. (1992b)  Errors in binary images and an Lp version of the Hausdorff metric.  <em>Nieuw Arch. Wiskunde</em>, <b>10</b>, 157&ndash;183.
</p>
<p>Gilleland, E. (2011)  Spatial forecast verification: Baddeley's delta metric applied to the ICP test cases.  <em>Wea. Forecasting</em>, <b>26</b>, 409&ndash;415, doi:10.1175/WAF-D-10-05061.1.  
</p>
<p>Peli, T. and Malah, D. (1982) A study on edge detection algorithms.  <em>Computer Graphics and Image Processing</em>, <b>20</b>, 1&ndash;21.
</p>
<p>Schwedler, B. R. J. and Baldwin, M. E. (2011)  Diagnosing the sensitivity of binary image measures to bias, location, and event frequency within a forecast verification framework.  <em>Wea. Forecasting</em>, <b>26</b>, 1032&ndash;1044, doi:10.1175/WAF-D-11-00032.1.
</p>
<p>Venugopal, V., Basu, S. and Foufoula-Georgiou, E. (2005) A new metric for comparing precipitation patterns with an application to ensemble forecasts.  <em>J. Geophys. Res.</em>, <b>110</b>, D08111, doi:10.1029/2004JD005395, 11pp.
</p>


<h3>See Also</h3>

<p><code><a href="spatstat.geom.html#topic+deltametric">deltametric</a></code>, <code><a href="spatstat.geom.html#topic+distmap">distmap</a></code>, <code><a href="#topic+make.SpatialVx">make.SpatialVx</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- y &lt;- matrix(0, 10, 12)
x[2,3] &lt;- 1
y[4,7] &lt;- 1

hold &lt;- make.SpatialVx(x, y, thresholds = 0.1,
    field.type = "random", units = "grid squares")
locmeasures2d(hold, k = 1)

# Alternatively ...
locmeasures2d(x, thresholds = 0.1, k = 1, Y = y)

## Not run: 
data( "geom000" )
data( "geom001" )
data( "ICPg240Locs" )

hold &lt;- make.SpatialVx( geom000, geom001, thresholds = c(0.1, 50.1), 
    projection = TRUE, map=TRUE, loc = ICPg240Locs, loc.byrow = TRUE,
    field.type = "Precipitation", units = "in/100",
    data.name= "ICP Geometric Cases", obs.name = "geom000",
    model.name = "geom001" )

hold2 &lt;- locmeasures2d(hold, k=c(4, 0.975), alpha=c(0.1,0.9))
summary(hold2)

## End(Not run)
</code></pre>

<hr>
<h2 id='locperf'>
Localization Performance Measures
</h2><span id='topic+locperf'></span><span id='topic+distob'></span><span id='topic+distmapfun'></span>

<h3>Description</h3>

<p>Some localization performance (distance) measures for binary images.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>locperf(X, Y, which.stats = c("qdmapdiff", "med", "msd", "ph", "fom", "minsep"),
    alpha = 0.1, k = 4, distfun = "distmapfun", a=NULL, ...)

distob(X, Y, distfun = "distmapfun", ...)

distmapfun(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="locperf_+3A_x">X</code></td>
<td>

<p>list object giving a pixel image as output from <code>solutionset</code> from package <span class="pkg">spatstat</span>.  This corresponds to the set B in the Details section below.
</p>
</td></tr>
<tr><td><code id="locperf_+3A_y">Y</code></td>
<td>

<p>list object giving a pixel image as output from <code>solutionset</code> from package <span class="pkg">spatstat</span>.  This corresponds to the set A in the Details section below.
</p>
</td></tr>
<tr><td><code id="locperf_+3A_x">x</code></td>
<td>
<p>list object of class &ldquo;owin&rdquo; as returned by <code>solutionset</code> from package <span class="pkg">spatstat</span>.</p>
</td></tr>
<tr><td><code id="locperf_+3A_which.stats">which.stats</code></td>
<td>

<p>character vector stating which localization performance measure to calculate.
</p>
</td></tr>
<tr><td><code id="locperf_+3A_alpha">alpha</code></td>
<td>

<p>numeric giving the scaling constant for Pratt's figure of merit (FOM).  Only used for <code>which.stat</code> method &ldquo;fom&rdquo;.
</p>
</td></tr>
<tr><td><code id="locperf_+3A_k">k</code></td>
<td>

<p>single numeric giving the order for the rank/quantile of the difference in distance maps.  If 0 &lt;= k &lt; 1, this is assumed to be a quantile for use with the <code>quantile</code> function.  Otherwise, k should be a whole number such that 1 &lt;= k &lt;= Nxy, where nxy is the total number of grid points in the set.
</p>
</td></tr>
<tr><td><code id="locperf_+3A_distfun">distfun</code></td>
<td>
<p>character specifying a distance metric that returns a matrix of same dimension as <code>X</code> yielding, at each point x, the shortest distances from x to the set of events in the field.  Default is <code>distmapfun</code>, which returns the Euclidean distances.</p>
</td></tr>
<tr><td><code id="locperf_+3A_a">a</code></td>
<td>
<p>Not used.  For compatibility with <code>locmeasures2d</code>.</p>
</td></tr>
<tr><td><code id="locperf_+3A_...">...</code></td>
<td>

<p>Optional arguments to the <code>distfun</code> function.  In the case of <code>distmapfun</code>, these are the optional arguments to <code>distmap</code> from package <span class="pkg">spatstat</span>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function computes localization performance (or distance) measures detailed in Peli and Malah (1982) and Baddeley (1992), as well as a modification of one of these distances detailed in Zhu et al. (2011); <code>distob</code>.
</p>
<p>First, it is helpful to establish some notation.  Suppose a distance rho(x,y) is defined between any two pixels x and y in the entire raster of pixels/grid (If <code>distfun</code> is <code>distmapfun</code> (default), then rho is the Euclidean distance) that satisfies the formal mathematical axioms of a metric.  Let d(x,A) denote the shortest distance (smallest value of rho) from the point x in the entire raster to the the set A contained in the raster.  That is, d(x,A) = min(rho(x,a): a in A contained in the raster) [formally, the minimum should be the infimum], with d(x, empty set) defined to be infinity.  Note that the <code>distfun</code> argument is a function that returns d(x,A) for all x in the raster.
</p>
<p>The mean error distance (&ldquo;med&rdquo;) is the mean of d(x,A) over the points in B.  That is e.bar = mean( d(x,A)), over all x in B.  Because it is not symmetric (i.e., MED(A, B) != MED(B, A)), it is given as medMiss = MED(Forecast, Observation) and medFalseAlarm = MED(Observation, Forecast).
</p>
<p>The mean square error distance (&ldquo;msd&rdquo;) is the mean of the squared d(x,A) over the points in B.  That is, e2.bar = mean( d(x,A)^2), over all x in B.  Similarly to MED, it is given as msdMiss or msdFalseAlarm.
</p>
<p>Pratt's figure of merit (&ldquo;fom&rdquo;) is given by: FOM(A,B) = sum( 1/(1+alpha*d(x,A)^2))/max(N(A),N(B)), where x in B, and N(A) (N(B)) is the number of points in the set A (B) and alpha is a scaling constant (see, e.g., Pratt, 1977; Abdou and Pratt, 1979).  The scaling constant is typically set to 1/9 when rho is normalized so that the smallest nonzero distance between pixel neighbors is 1.  The default (0.1) here is approximately 1/9.  If both A and B are empty, the value returned for max(N(A), N(B)) is 1e16 and for d(x,A) for x in B is given a value of zero so that the returned value should be close to zero.
</p>
<p>Minimum separation distance between boundaries (&ldquo;minsep&rdquo;) is just the smallest value of the distance map of one field over the subset where events occur in the other.  This is mainly for when single features within the fields are being compared.
</p>
<p>distob is a modification of the mean error distance where if there are no events in either field, the value is 0, and if there are no events in one field only, the value is something large (in this case the length of the longest side of the grid).
</p>
<p>The Hausdorff distance for a finite grid is given by max( max( d(x,B); x in A), max( d(x,A); x in B)), and can be written as max( abs(d(x,A) - d(x,B)), over all x in the raster).  The quantile of the difference in distance mapse (&ldquo;qdmapdiff&rdquo;) is also potentially useful, and replaces the maximum in the latter equation with a k-th order statistic (or quantile).  The modified Hausdorff distance is no longer given from this function, but can easily be computed using output from this function as it is given by mhd(A,B) = max( e.bar(A,B), e.bar(B,A)), and in some literature the maximum is replaced by the minimum.  See, e.g., Baddeley, (1992) and Schwedler and Baldwin (2011).
</p>
<p>For computational efficiency, the distance transform method is used via <code>distmap</code> from package <span class="pkg">spatstat</span> for calculating d(x,A) x in the raster.
</p>


<h3>Value</h3>

<p><code>locperf</code> returns a list object with components depending on <code>which.stats</code>: one or more of the following, each of which is a single numeric, except as indicated.
</p>
<table>
<tr><td><code>bdelta</code></td>
<td>
<p>matrix or numeric depending on p and number of thresholds.</p>
</td></tr>
<tr><td><code>haus</code></td>
<td>
<p>numeric giving the Hausdorff distances for each threshold.</p>
</td></tr>
<tr><td><code>qdmapdiff</code></td>
<td>
<p>matrix or numeric, depending on k and number of thresholds, giving the value of the quantile (or k-th highest value) of the difference in distance maps for each threshold.</p>
</td></tr>
<tr><td><code>medMiss</code>, <code>medFalseAlarm</code>, <code>msdMiss</code>, <code>msdFalseAlarm</code></td>
<td>
<p>numeric giving the value of the mean error/square error distance for each threshold.</p>
</td></tr>
<tr><td><code>fom</code></td>
<td>
<p>matrix or numeric, depending on alpha and number of thresholds, giving the value of Pratt his Figure of Merit for each threshold.</p>
</td></tr>
<tr><td><code>minsep</code></td>
<td>
<p>numeric giving the value of the minimum boundary separation distance for each threshold.</p>
</td></tr>
</table>
<p><code>distob</code> returns a single numeric.
</p>
<p><code>distmapfun</code> returns a matrix of same dimension as the input argument's field.
</p>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>References</h3>

<p>Abdou, I. E. and Pratt, W. K. (1979) Quantitative design and evaluation of enhancement/thresholding edge detectors.  <em>Proc. IEEE</em>, <b>67</b>, 753&ndash;763.
</p>
<p>Baddeley, A. (1992)  An error metric for binary images.  In <em>Robust Computer Vision Algorithms</em>, W. Forstner and S. Ruwiedel, Eds., Wichmann, 59&ndash;78.
</p>
<p>Peli, T. and Malah, D. (1982) A study on edge detection algorithms.  <em>Computer Graphics and Image Processing</em>, <b>20</b>, 1&ndash;21.
</p>
<p>Pratt, W. K. (1977) <em>Digital Image Processing</em>.  John Wiley and Sons, New York.
</p>
<p>Schwedler, B. R. J. and Baldwin, M. E. (2011)  Diagnosing the sensitivity of binary image measures to bias, location, and event frequency within a forecast verification framework.  <em>Wea. Forecasting</em>, <b>26</b>, 1032&ndash;1044, doi:10.1175/WAF-D-11-00032.1.
</p>
<p>Zhu, M., Lakshmanan, V. Zhang, P. Hong, Y. Cheng, K. and Chen, S. (2011) Spatial verification using a true metric.  <em>Atmos. Res.</em>, <b>102</b>, 408&ndash;419, doi:10.1016/j.atmosres.2011.09.004.
</p>


<h3>See Also</h3>

<p><code><a href="spatstat.geom.html#topic+distmap">distmap</a></code>, <code><a href="spatstat.geom.html#topic+solutionset">solutionset</a></code>, <code><a href="spatstat.geom.html#topic+im">im</a></code>, <code><a href="spatstat.geom.html#topic+boundingbox">boundingbox</a></code>, <code><a href="spatstat.geom.html#topic+as.rectangle">as.rectangle</a></code>, <code><a href="#topic+metrV">metrV</a></code>, <code><a href="#topic+locmeasures2d">locmeasures2d</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- y &lt;- matrix( 0, 10, 12)
x[2,3] &lt;- 1
y[4,7] &lt;- 1
x &lt;- im( x)
y &lt;- im( y)
x &lt;- solutionset( x &gt; 0)
y &lt;- solutionset( y &gt; 0)
locperf( x, y)

# Note that ph is NA because there is only 1 event.
# need to have at least k events if k &gt; 1.

par( mfrow=c(1,2))
image.plot( distmapfun(x))
image.plot( distmapfun(y))
</code></pre>

<hr>
<h2 id='LocSig'>
Temporal Block Bootstrap Keeping Locations in Space Constant
</h2><span id='topic+LocSig'></span><span id='topic+plot.LocSig'></span>

<h3>Description</h3>

<p>Temporal block bootstrap for data at spatial locations (holding locations constant at each iteration).  This is a wrapper function to the tsboot or boot functions for use with the field significance approach of Elmore et al. (2006).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>LocSig(Z, numrep = 1000, block.length = NULL, bootfun = "mean",
    alpha = 0.05, bca = FALSE, ...)

## S3 method for class 'LocSig'
plot(x, loc = NULL, nx = NULL, ny = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="LocSig_+3A_z">Z</code></td>
<td>

<p>n by m numeric matrix whose rows represent contiguous time points, and whose columns represent spatial locations.
</p>
</td></tr>
<tr><td><code id="LocSig_+3A_numrep">numrep</code></td>
<td>

<p>numeric/integer giving the number of bootstrap replications to use.
</p>
</td></tr>
<tr><td><code id="LocSig_+3A_block.length">block.length</code></td>
<td>

<p>positive numeric/integer giving the desired block lengths.  If NULL, <code>floor(sqrt(n))</code> is used.  If 1, then the IID bootstrap is performed, and the BCa method may be used to find CI's, if <code>bca</code> is TRUE.
</p>
</td></tr>
<tr><td><code id="LocSig_+3A_bootfun">bootfun</code></td>
<td>

<p>character naming an R function to be applied to each replicate sample.  Must return a single number, but is otherwise the <code>statistic</code> argument for function <code>tsboot</code> (or <code>boot</code> if <code>block.length</code> = 1).
</p>
</td></tr>
<tr><td><code id="LocSig_+3A_alpha">alpha</code></td>
<td>

<p>numeric giving the value of <code>alpha</code> to obtain (1-<code>alpha</code>)*100 percent CI's for <code>bootfun</code>.
</p>
</td></tr>
<tr><td><code id="LocSig_+3A_bca">bca</code></td>
<td>

<p>logical, should bias-corrected and adjusted (BCa) CI's be calculated?  Only used if <code>block.length</code> = 1.  Will give a warning if this argument is TRUE, and <code>block.length</code> &gt; 1, and will use the percentile method.
</p>
</td></tr>
<tr><td><code id="LocSig_+3A_x">x</code></td>
<td>
<p> data frame of class &ldquo;LocSig&rdquo; as returned by <code>LocSig</code>.</p>
</td></tr>
<tr><td><code id="LocSig_+3A_loc">loc</code></td>
<td>
<p> m by 2 matrix of location coordinates.</p>
</td></tr>
<tr><td><code id="LocSig_+3A_nx">nx</code>, <code id="LocSig_+3A_ny">ny</code></td>
<td>
<p> If <code>loc</code> is NULL, then <code>nx</code> and <code>ny</code> must be supplied.  These give the number of rows and columns of a grid to make an image (using <code>as.image</code>) for plotting.  If these are used, the data <code>Z</code> must be from a regular grid of points.</p>
</td></tr>
<tr><td><code id="LocSig_+3A_...">...</code></td>
<td>

<p><code>LocSig</code>: optional additional arguments to the <code>tsboot</code> (or <code>boot</code> if <code>block.length</code>=1) function.
<code>plot.LocSig</code>: optional additional arguments to <code>image.plot</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function performs the circular block bootstrap algorithm over time at each of m locations (columns of <code>x</code>).  So, at each bootstrap iteration, entire blocks of rows of x are resampled with replacement.  If <code>Z</code> represents forecast errors at grid points, and <code>bootfun</code>=&ldquo;mean&rdquo;, then this finds the grid-point CI's in steps 1 (a) to 1 (c) of Elmore et al. (2006).
</p>


<h3>Value</h3>

<p>LocSig: A data frame with class attribute &ldquo;LocSig&rdquo; with components:
</p>
<table>
<tr><td><code>Estimate</code></td>
<td>
<p>numeric giving the estimated values of bootfun (the statistic for which CI's are computed).</p>
</td></tr>
<tr><td><code>Lower</code>, <code>Upper</code></td>
<td>
<p>numeric giving the estimated lower (upper) (1-alpha)*100 percent CI's.</p>
</td></tr>
</table>
<p>plot.LocSig: invisibly returns a list containing the estimate as returned by LocSig, and the confidence range.
</p>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>References</h3>

<p>Elmore, K. L., Baldwin, M. E. and Schultz, D. M. (2006) Field significance revisited: Spatial bias errors in forecasts as applied to the Eta model.  <em>Mon. Wea. Rev.</em>, <b>134</b>, 519&ndash;531.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+spatbiasFS">spatbiasFS</a></code>, <code><a href="boot.html#topic+tsboot">tsboot</a></code>, <code><a href="boot.html#topic+boot">boot</a></code>, <code><a href="boot.html#topic+boot.ci">boot.ci</a></code>, <code><a href="#topic+MCdof">MCdof</a></code>, <code><a href="#topic+sig.cor.t">sig.cor.t</a></code>, <code><a href="#topic+sig.cor.Z">sig.cor.Z</a></code>, <code><a href="stats.html#topic+cor.test">cor.test</a></code>, <code><a href="fields.html#topic+image.plot">image.plot</a></code>, <code><a href="fields.html#topic+as.image">as.image</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
data( "GFSNAMfcstEx" )
data( "GFSNAMobsEx" )
data( "GFSNAMlocEx" )

id &lt;- GFSNAMlocEx[,"Lon"] &gt;=-90 &amp; GFSNAMlocEx[,"Lon"] &lt;= -75 &amp; GFSNAMlocEx[,"Lat"] &lt;= 40

look &lt;- LocSig(GFSNAMfcstEx[,id] - GFSNAMobsEx[,id], numrep=500)

stats(look)

plot(look, loc = GFSNAMlocEx[ id, ] )

## End(Not run)
</code></pre>

<hr>
<h2 id='lossdiff'>
Test for Equal Predictive Ability on Average Over a Regularly Gridded Space
</h2><span id='topic+lossdiff'></span><span id='topic+lossdiff.default'></span><span id='topic+lossdiff.SpatialVx'></span><span id='topic+empiricalVG.lossdiff'></span><span id='topic+flossdiff'></span><span id='topic+summary.lossdiff'></span><span id='topic+plot.lossdiff'></span><span id='topic+print.lossdiff'></span>

<h3>Description</h3>

<p>Test for equal predictive ability (for two forecast models) on average over a regularly gridded space using the method of Hering and Genton (2011).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lossdiff(x, ...)

## Default S3 method:
lossdiff(x, ..., xhat1, xhat2, threshold = NULL,
    lossfun = "corrskill", loc = NULL, zero.out = FALSE)

## S3 method for class 'SpatialVx'
lossdiff(x, ..., time.point = 1, obs = 1, model = c(1, 2),
    threshold = NULL, lossfun = "corrskill", zero.out = FALSE)

empiricalVG.lossdiff( x, trend = 0, maxrad, dx = 1, dy = 1 )

flossdiff(object, vgmodel = "expvg", ...)

## S3 method for class 'lossdiff'
summary(object, ...)

## S3 method for class 'lossdiff'
plot(x, ..., icol = c("gray", tim.colors(64)))

## S3 method for class 'lossdiff'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lossdiff_+3A_x">x</code>, <code id="lossdiff_+3A_xhat1">xhat1</code>, <code id="lossdiff_+3A_xhat2">xhat2</code></td>
<td>

<p><code>lossdiff</code>: m by n matrices defining the (gridded) verification set where <code>xhat1</code> and <code>xhat2</code> are the two forecast models being compared.  <code>plot.lossdiff</code>: <code>x</code> is a list returned by <code>lossdiff</code>.
</p>
</td></tr>
<tr><td><code id="lossdiff_+3A_object">object</code></td>
<td>

<p><code>flossdiff</code> this is the output returned by <code>lossdiff</code>.  <code>summary.lossdiff</code>: list object returned by <code>lossdiff</code> or <code>flossdiff</code>.
</p>
</td></tr>
<tr><td><code id="lossdiff_+3A_threshold">threshold</code></td>
<td>
<p>numeric vector of length one, two or three giving a threshold under which (non-inclusive) all values will be set to zero.  If length is one, the same threshold is used for all fields (observed, and both models).  If length is two, the same threshold will be used for both models (the second value of <code>threshold</code>).  Otherwise, the first entry is used for the observed field, the second for the first model and the third for the second model.</p>
</td></tr>
<tr><td><code id="lossdiff_+3A_lossfun">lossfun</code></td>
<td>

<p>character anming a loss function to use in finding the loss differential for the fields.  Default is to use correlation as the loss function.  Must have arguments <code>x</code> and <code>y</code>, and may have any additional arguments.
</p>
</td></tr>
<tr><td><code id="lossdiff_+3A_trend">trend</code></td>
<td>
<p>a matrix (of appropriate dimension) or single numeric (if constant trend) giving the value of the spatial trend.  the value is simply subtracted from the loss differential field before finding the empirical variogram.  If <code>zero.out</code> is TRUE, then wherever the original three fields all had zero-valued grid points are returned back to zero before continuing (hence ignored in the computation of the variogram).</p>
</td></tr>
<tr><td><code id="lossdiff_+3A_loc">loc</code></td>
<td>
<p>(optional) mn by 2 matrix giving location coordinates for each grid point.  If NULL, they are taken to be the grid expansion of the dimension of <code>x</code> (i.e., cbind(rep(1:dim(x)[1],dim(x)[2]), rep(1:dim(x)[2],each=dim(x)[1]))).  This argument is not used by <code>lossdiff</code>, but may be used subsequently by the <code>plot</code> method function.</p>
</td></tr>
<tr><td><code id="lossdiff_+3A_maxrad">maxrad</code></td>
<td>

<p>numeric giving the maximum radius for finding variogram differences per the <code>R</code> argument of <code>vgram.matrix</code>.
</p>
</td></tr>
<tr><td><code id="lossdiff_+3A_dx">dx</code>, <code id="lossdiff_+3A_dy">dy</code></td>
<td>

<p><code>dx</code> and <code>dy</code> of <code>vgram.matrix</code>.
</p>
</td></tr>
<tr><td><code id="lossdiff_+3A_zero.out">zero.out</code></td>
<td>
<p>logical, should the variogram be computed only over non-zero values of the process?  If TRUE, a modified version of <code>vgram.matrix</code> is used (<code>variogram.matrix</code>).</p>
</td></tr>
<tr><td><code id="lossdiff_+3A_vgmodel">vgmodel</code></td>
<td>
<p>character string naming a variogram model function to use.  Default is the exponential variogram, <code>expvg</code>. </p>
</td></tr>
<tr><td><code id="lossdiff_+3A_time.point">time.point</code></td>
<td>
<p>numeric or character indicating which time point from the &ldquo;SpatialVx&rdquo; verification set to select for analysis.</p>
</td></tr>
<tr><td><code id="lossdiff_+3A_obs">obs</code>, <code id="lossdiff_+3A_model">model</code></td>
<td>
<p>numeric indicating which observation/forecast model to select for the analysis.</p>
</td></tr>
<tr><td><code id="lossdiff_+3A_icol">icol</code></td>
<td>
<p>(optional) color scheme.</p>
</td></tr>
<tr><td><code id="lossdiff_+3A_...">...</code></td>
<td>

<p><code>lossdiff</code>: optional additional arguments to <code>lossfun</code>.  Not used by the summary or plot functions.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Hering and Genton (2011) introduce a test procedure for comparing spatial fields, which is based on a time series test introduced by Diebold and Mariano (1995).  First, a loss function, g(x,y), is calculated, which can be any appropriate loss function.  This is calculated for each of two forecast fields.  The loss differential field is then given by:
</p>
<p>D(s) = g(x(s),y1(s)) - g(x(s),y2(s)), where s are the spatial locations, x is the verification field, and y1 and y2 are the two forecast fields.
</p>
<p>It is assumed that D(s) = phi(s) + psi(s), where phi(s) is the mean trend and psi(s) is a mean zero stationary process with unknown covariance function C(h) = cov(psi(s),psi(s+h)).  In particular, the argument trend represents phi(s), and the default is that the mean is equal (and zero) over the entire domain.  If it is believed that this is not the case, then it should be removed before finding the covariance.
</p>
<p>To estimate the trend, see e.g. Hering and Genton (2011) and references therein.
</p>
<p>A test is constructed to test the null hypothesis of equal predictive ability on average.  That is,
</p>
<p>H_0: 1/|D| int_D E[D(s)]ds = 0, where |D| is the area of the domain, 
</p>
<p>The test statistic is given by
</p>
<p>S_V = mean(D(s))/sqrt(mean(C(h))),
</p>
<p>where C(h) = gamma(infinity|p) - gamma(h|p) is a fitted covariance function for the loss differential field.  The test statistic is assumed to be N(0,1) so that if the p-value is smaller than the desired level of significance, the null hypothesis is not accepted.
</p>
<p>For 'flossdiff', an exponential variogram is used. Specifically,
</p>
<p>gamma(h | theta=(s,r)) = s^2*(1 - exp(-h/r)),
</p>
<p>where s is sqrt(sill) and r is the range (nugget effects are not accounted for here).  If <code>flossdiff</code> should fail, and the empirical variogram appears to be reasonable (e.g., use the <code>plot</code> method function on <code>lossdiff</code> output to check that the empirical variogram is concave), then try giving alternative starting values for the <code>nls</code> function by using the <code>start.list</code> argument.  The default is to use the variogram value for the shortest separation distance as an initial estimate for s, and <code>maxrad</code> as the initial estimate for r.
</p>
<p>Currently, it is not possible to fit other variogram models with this function.  Such flexibility may possibly be added in a future release.  In the meantime, use <code>flossdiff</code> as a template to make your own similar function; just be sure to return an object of class &ldquo;nls&rdquo;, and it should work seamlessly with the <code>plot</code> and <code>summary</code> method functions for a &ldquo;lossdiff&rdquo; object.  For example, if it is desired to include the nugget or an extra factor (e.g., 3 as used in Hering and Genton, 2011), then a new similar function would need to be created.
</p>
<p>Also, although the testing procedure can be applied to irregularly spaced locations (non-gridded), this function is set up only for gridded fields in order to take advantage of computational efficiencies (i.e., use of vgram.matrix), as these are the types of verification sets in mind for this package.  For irregularly spaced grids, the function <code>spct</code> can be used.
</p>
<p>The above test assumes constant spatial trend.  It is possible to remove any spatial trend in D(s) before applying the test.
</p>
<p>The procedure requires four steps (hence four functions).  The first is to calculate the loss differential field using <code>lossdiff</code>.  Next, calculate the empirical variogram of the loss differential field using <code>empiricalVG.lossdiff</code>.  This second step was originally included within the first step in <code>lossdiff</code>, but that setup presented a problem for determining if a spatial trend exists or not.  It is important to determine if a trend exists, and if so, to (with care) estimate the trend, and remove it.  If a trend is detected (and estimated), it can be removed before calling <code>empiricalVG.lossdiff</code> (then use the default <code>trend</code> = 0), or it can be passed in via the <code>trend</code> argument; the advantage (or disadvantage) of which is that the trend term will be included in the output object.  The third step is to fit a parametric variogram model to the empirical one using <code>flossdiff</code>.  The final, fourth step, is to conduct the test, which is performed by the <code>summary</code> function.
</p>
<p>In each step, different aspects of the model assumptions can be checked.  For example, isotropy can be checked by the plot in the lower right panel of the result of the <code>plot</code> method function after having called <code>empiricalVG.lossdiff</code>.  The function <code>nlminb</code> is used to fit the variogram model.
</p>
<p>For application to precipitation fields, and introduction to the image warp (coming soon) and distance map loss functions, see Gilleland (2013).
</p>


<h3>Value</h3>

<p>A list object is returned with possible components:
</p>
<table>
<tr><td><code>data.name</code></td>
<td>
<p>character vector naming the fields under comparison</p>
</td></tr>
<tr><td><code>lossfun</code>, <code>lossfun.args</code>, <code>vgram.args</code></td>
<td>
<p>same as the arguments input to the lossdiff function.</p>
</td></tr>
<tr><td><code>d</code></td>
<td>
<p>m by n matrix giving the loss differential field, D(s).</p>
</td></tr>
<tr><td><code>trend.fit</code></td>
<td>
<p>An OLS trend fitting the locations to the field via lm.</p>
</td></tr>
<tr><td><code>loc</code></td>
<td>
<p>the self-same value as the argument passed in, or if NULL, it is the expanded grid coordinates.</p>
</td></tr>
</table>
<p>empiricalVG.lossdiff returns all of the above (carried over) along with 
</p>
<table>
<tr><td><code>lossdiff.vgram</code></td>
<td>
<p>list object as returned by vgram.matrix</p>
</td></tr>
<tr><td><code>trend</code></td>
<td>
<p>it is the self-same as the value passed in.</p>
</td></tr>
</table>
<p>flossdiff returns all of the above plus:
</p>
<table>
<tr><td><code>vgmodel</code></td>
<td>
<p>list object as returned by nls containing the fitted exponential variogram model where s is the estimate of sqrt(sill), and r of the range parameter (assuming 'flossdiff' was used to fit the variogram model).</p>
</td></tr>
</table>
<p>summary.lossdiff invisibly returns the same list object as above with additional components:
</p>
<table>
<tr><td><code>Dbar</code></td>
<td>
<p>the estimated mean loss differential (over the entire field).</p>
</td></tr>
<tr><td><code>test.statistic</code></td>
<td>
<p>the test statistic.</p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p>list object with components: two.sided&ndash;the two-sided alternative hypothesis&ndash;, less&ndash;the one-sided alternative hypothesis that the true value mu(D) &lt; 0&ndash;and greater&ndash;the one-sided alternative hypothesis that mu(D) &gt; 0&ndash;, p-values under the assumption of standard normality of the test statistic.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>References</h3>

<p>Diebold, F. X. and Mariano, R. S. (1995) Comparing predictive accuracy. <em>Journal of Business and Economic Statistics</em>, <b>13</b>, 253&ndash;263.
</p>
<p>Gilleland, E. (2013) Testing competing precipitation forecasts accurately and efficiently: The spatial prediction comparison test.  <em>Mon. Wea. Rev.</em>, <b>141</b>, (1), 340&ndash;355.
</p>
<p>Hering, A. S. and Genton, M. G. (2011) Comparing spatial predictions.  <em>Technometrics</em> <b>53</b>, (4), 414&ndash;425.
</p>


<h3>See Also</h3>

<p><code><a href="fields.html#topic+vgram.matrix">vgram.matrix</a></code>, <code><a href="stats.html#topic+nls">nls</a></code>, <code><a href="#topic+corrskill">corrskill</a></code>, <code><a href="#topic+abserrloss">abserrloss</a></code>, <code><a href="#topic+sqerrloss">sqerrloss</a></code>, <code><a href="#topic+distmaploss">distmaploss</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>grid&lt;- list( x = seq( 0, 5,, 25), y = seq(0,5,,25) )
obj&lt;-Exp.image.cov( grid = grid, theta = .5, setup = TRUE)

look&lt;- sim.rf( obj )
look[ look &lt; 0 ] &lt;- 0
look &lt;- zapsmall( look )
     
look2 &lt;- sim.rf( obj ) * .25
look2[ look2 &lt; 0 ] &lt;- 0
look2 &lt;- zapsmall( look2 )

look3 &lt;- sim.rf( obj) * 2 + 5
look3[ look3 &lt; 0 ] &lt;- 0 
look3 &lt;- zapsmall( look3 )

res &lt;- lossdiff( x = look, xhat1 = look2, xhat2 = look3, lossfun = "abserrloss" )
res &lt;- empiricalVG.lossdiff( res, maxrad = 8 )
res &lt;- flossdiff( res )
res &lt;- summary( res )

plot( res )

</code></pre>

<hr>
<h2 id='make.SpatialVx'>
Spatial Verification Sets &ndash; SpatialVx Object
</h2><span id='topic+make.SpatialVx'></span><span id='topic+hist.SpatialVx'></span><span id='topic+plot.SpatialVx'></span><span id='topic+print.SpatialVx'></span><span id='topic+summary.SpatialVx'></span>

<h3>Description</h3>

<p>A list object containing the verification sets of spatial verification and forecast fields with pertinent information.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>make.SpatialVx(X, Xhat, thresholds = NULL, loc = NULL, projection =
                 FALSE, subset = NULL, time.vals = NULL, reg.grid =
                 TRUE, map = FALSE, loc.byrow = FALSE, field.type = "",
                 units = "", data.name = "", obs.name = "X", model.name
                 = "Xhat", q = c(0, 0.1, 0.25, 0.33, 0.5, 0.66, 0.75,
                 0.9, 0.95), qs = NULL)

## S3 method for class 'SpatialVx'
hist(x, ..., time.point = 1, obs = 1, model = 1,
                 threshold.num = NULL)

## S3 method for class 'SpatialVx'
plot( x, ..., time.point = 1, obs = 1, model = 1,
    col = c( "gray", tim.colors( 64 ) ), zlim, mfrow = c(1, 2) )

## S3 method for class 'SpatialVx'
print(x, ...)

## S3 method for class 'SpatialVx'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="make.SpatialVx_+3A_x">X</code></td>
<td>
<p>An n X m  matrix or n X m X T array giving the verification field of interest.  If an array, T is the number of time points.
</p>
</td></tr>
<tr><td><code id="make.SpatialVx_+3A_xhat">Xhat</code></td>
<td>

<p>An n X m matrix or n X m X T array giving the forecast field of interest, or a list of such matrices/arrays with each component of the list an n X m matrix or n X m X T array defining a separate forecast model.
</p>
</td></tr>
<tr><td><code id="make.SpatialVx_+3A_thresholds">thresholds</code></td>
<td>

<p>single numeric, numeric vector, or Nu X Nf matrix, where Nu are the number of thresholds and Nf the number of forecast models plus one (for the verification) giving the threshold values of interest for the verification set or components of the set.  If NULL (default), then thresholds will be calculated as the quantiles (defined through argument <code>q</code>) of each field.  If a single numeric or a numeric vector, then an n X 2 matrix will be created (with column names &ldquo;X&rdquo; and &ldquo;Xhat&rdquo; where each column is identical.  Otherwise, different thresholds may be applied to each of the verification and forecast fields.  For example, if quantiles are used for thresholds, then each field will have their own unique thresholds.
</p>
</td></tr>
<tr><td><code id="make.SpatialVx_+3A_loc">loc</code></td>
<td>

<p>If lon/lat coordinates are available, then this is an n * m X 2 matrix giving the lon/lat coordinates of each grid point or location.  Should follow the convention used by the <span class="pkg">maps</span> package.
</p>
</td></tr>
<tr><td><code id="make.SpatialVx_+3A_projection">projection</code></td>
<td>

<p>logical, are the grids projections onto the globe?  If so, when plotting, it will be attempted to account for this by using the <code>poly.image</code> function from package <code>fields</code>.  In this case, each column of <code>loc</code> will be converted to a matrix using <code>byrow</code> equal to the value of <code>loc.byrow</code>.
</p>
</td></tr>
<tr><td><code id="make.SpatialVx_+3A_subset">subset</code></td>
<td>

<p>vector identifying which specific grid points should be included (if not all of them).  This argument may be ignored by most functions and is included for possible future functionality.
</p>
</td></tr>
<tr><td><code id="make.SpatialVx_+3A_time.vals">time.vals</code></td>
<td>

<p>If more than one time point is available in the set (i.e., the set is of n X m X T arrays, with T &gt; 1), then this argument can be used to define the time points.  If missing, the default will yield the vector <code>1:T</code>.  But, it is possible to include actual time information.  This is also a forward looking feature that may or may not have any subsequent functionality.
</p>
</td></tr>
<tr><td><code id="make.SpatialVx_+3A_reg.grid">reg.grid</code></td>
<td>

<p>logical, is the verification set on a regular grid?  This is another feature intended for possible future functionality.  Most functions in this package assume the set is on a regular grid.
</p>
</td></tr>
<tr><td><code id="make.SpatialVx_+3A_map">map</code></td>
<td>

<p>logical, should the plot function attempt to place a map onto the plot?  Only possible if the <code>loc</code> argument is given.
</p>
</td></tr>
<tr><td><code id="make.SpatialVx_+3A_field.type">field.type</code>, <code id="make.SpatialVx_+3A_units">units</code></td>
<td>

<p>character used for plot labelling and printing information to the screen.  Describes what variable and in what units the field represents.
</p>
</td></tr>
<tr><td><code id="make.SpatialVx_+3A_data.name">data.name</code>, <code id="make.SpatialVx_+3A_obs.name">obs.name</code>, <code id="make.SpatialVx_+3A_model.name">model.name</code></td>
<td>

<p>character vector describing the verification set overall, the observation(s) and the model(s), resp.
</p>
</td></tr>
<tr><td><code id="make.SpatialVx_+3A_q">q</code></td>
<td>

<p>numeric vector giving the values of quantiles to be used for thresholds.  Only used if <code>thresholds</code> is NULL.
</p>
</td></tr>
<tr><td><code id="make.SpatialVx_+3A_qs">qs</code></td>
<td>

<p>character vector describing the quantiles used.  Again, only used if <code>thresholds</code> is NULL.  This is for subsequent plot/print labelling purposes.
</p>
</td></tr>
<tr><td><code id="make.SpatialVx_+3A_time.point">time.point</code></td>
<td>
<p>numeric or character indicating which time point from the &ldquo;SpatialVx&rdquo; verification set to select for analysis.  May also be a function name, in which case the function is applied at each grid point individually across time.  </p>
</td></tr>
<tr><td><code id="make.SpatialVx_+3A_obs">obs</code>, <code id="make.SpatialVx_+3A_model">model</code></td>
<td>
<p>numeric indicating which observation/forecast model to select for the analysis.</p>
</td></tr>
<tr><td><code id="make.SpatialVx_+3A_col">col</code>, <code id="make.SpatialVx_+3A_zlim">zlim</code></td>
<td>
<p>optional arguments to <code>image</code>, and/or <span class="pkg">fields</span> functions <code>poly.image</code> and <code>image.plot</code></p>
</td></tr>
<tr><td><code id="make.SpatialVx_+3A_mfrow">mfrow</code></td>
<td>
<p> optional argument to change the mfrow argument for the graphic device.  Default is one row with two plots (obs and model).  If null, then the mfrow argument will not be changed. </p>
</td></tr>
<tr><td><code id="make.SpatialVx_+3A_x">x</code>, <code id="make.SpatialVx_+3A_object">object</code></td>
<td>
<p>list object of class &ldquo;SpatialVx&rdquo;.</p>
</td></tr>
<tr><td><code id="make.SpatialVx_+3A_loc.byrow">loc.byrow</code></td>
<td>
<p>logical determining whether to set up the location matrices using <code>byrow</code> = TRUE or FALSE (for use with <code>poly.image</code>.</p>
</td></tr>
<tr><td><code id="make.SpatialVx_+3A_threshold.num">threshold.num</code></td>
<td>
<p>If not null, then the threshold index to apply a threshold to the fields before creating the histogram.</p>
</td></tr>
<tr><td><code id="make.SpatialVx_+3A_...">...</code></td>
<td>
<p><code>hist</code> method: optional arguments to <code>hist</code>.
</p>
<p><code>plot</code> method: if <code>time.point</code> is a function, then these allow for optional arguments to this function to be passed.
</p>
<p><code>print</code> and <code>summary</code> methods: Not used.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function merely describes a spatial verification set that includes the actual data as well as numerous attributes that are used by several of the subsequent functions that might be employed.  In many cases, the attribute information may be passed on to output from other functions for plot labelling and printing purposes (e.g., in order to identify the verification set, time point(s), etc.).
</p>
<p>All (or perhaps most) subsequent functions in this package utilize objects of this class and the information contained in the attributes.  This function simply gathers information and data sets into a particular form.
</p>
<p>The plot method function attempts to create an image plot of each field in the set (at each time point).  If projection is TRUE, then it will attempt to preserve the projection (via <code>poly.image</code> of package <span class="pkg">fields</span>).  It will also add white contour lines showing the thresholds.  If map is TRUE and <code>loc</code> was supplied, then a map will also be added, if possible.
</p>


<h3>Value</h3>

<p>A list object with two (unnamed) components:
</p>
<table>
<tr><td><code>1</code></td>
<td>
<p>matrix or array (same as input argument) giving the observation</p>
</td></tr>
<tr><td><code>2</code></td>
<td>
<p>Either a matrix or array (same as input argument) or a list of such objects if more than one forecast model.</p>
</td></tr>
</table>
<p>Several attributes are also included among the following:
</p>
<table>
<tr><td><code>xdim</code></td>
<td>
<p>numeric of length 2 or 3 giving the dimensions of the verification set (i.e., m, n and T, if relevant).</p>
</td></tr>
<tr><td><code>time</code></td>
<td>
<p>vector giving the time values</p>
</td></tr>
<tr><td><code>thresholds</code></td>
<td>
<p>matrix giving the thresholds for each field.  If there is more than one forecast, and they use the same threshold, this matrix may have only two columns.</p>
</td></tr>
<tr><td><code>udim</code></td>
<td>
<p>the dimensions of the thresholds matrix.</p>
</td></tr>
<tr><td><code>loc</code></td>
<td>
<p>nm X 2 matrix giving the locations.  If loc was not given, this will be c(rep(1:n, m), rep(1:m, each=n)).</p>
</td></tr>
<tr><td><code>subset</code></td>
<td>
<p>If given, this is a numeric vector describing a subset of loc to be used.</p>
</td></tr>
<tr><td><code>data.name</code>, <code>obs.name</code>, <code>model.name</code></td>
<td>
<p>character vector giving the names of the data sets (same as input arguments).</p>
</td></tr>
<tr><td><code>nforecast</code></td>
<td>
<p>single numeric giving the number of different forecast models contained in the object.</p>
</td></tr>
<tr><td><code>field.type</code>, <code>units</code></td>
<td>
<p>character strings, same as input arguments.</p>
</td></tr>
<tr><td><code>projection</code></td>
<td>
<p>logical, is the grid a projection?</p>
</td></tr>
<tr><td><code>reg.grid</code></td>
<td>
<p>logical, is the grid a regular grid?</p>
</td></tr>
<tr><td><code>map</code></td>
<td>
<p>logical, should a map be added to image plots of the data?</p>
</td></tr>
<tr><td><code>qs</code></td>
<td>
<p>character vector giving the names of the threshold quantiles.</p>
</td></tr>
<tr><td><code>msg</code></td>
<td>
<p>A message involving the data name, field type and units for adding info to plots, etc.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>See Also</h3>

<p><code><a href="#topic+hoods2d">hoods2d</a></code>, <code><a href="fields.html#topic+poly.image">poly.image</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data( "UKobs6" )
data( "UKfcst6" )
data( "UKloc" )

hold &lt;- make.SpatialVx( UKobs6, UKfcst6, thresholds = c(0.01, 20.01),
    loc = UKloc, field.type = "Precipitation", units = "mm/h",
    data.name = "Nimrod", obs.name = "Observations 6", model.name = "Forecast 6",
    map = TRUE)

hold

plot( hold )

hist( hold )

hist( hold, threshold.num = 2 )

</code></pre>

<hr>
<h2 id='MCdof'>
Monte Carlo Degrees of Freedom
</h2><span id='topic+MCdof'></span><span id='topic+sig.cor.t'></span><span id='topic+sig.cor.Z'></span><span id='topic+fisherz'></span>

<h3>Description</h3>

<p>Estimate the distribution of the proportion of spatial locations that contain significant correlations with randomly generated data along the lines of Livezey and Chen (1983).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MCdof(x, ntrials = 5000, field.sig = 0.05, zfun = "rnorm", zfun.args = NULL,
      which.test = c("t", "Z", "cor.test"), verbose = FALSE, ...)

sig.cor.t(r, len = 40, ...)

sig.cor.Z(r, len = 40, H0 = 0)

fisherz(r)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MCdof_+3A_x">x</code></td>
<td>

<p>n by m numeric matrix whose rows represent temporal points, and whose columns are spatial locations.
</p>
</td></tr>
<tr><td><code id="MCdof_+3A_ntrials">ntrials</code></td>
<td>

<p>numeric/integer giving the number of times to generate random samples of size n, and correlate them with the columns of <code>x</code>.
</p>
</td></tr>
<tr><td><code id="MCdof_+3A_field.sig">field.sig</code></td>
<td>

<p>numeric between 0 and 1 giving the desired fields significance level.
</p>
</td></tr>
<tr><td><code id="MCdof_+3A_zfun">zfun</code></td>
<td>

<p>character naming a random number generator that takes <code>n</code> (the size of the sample to be drawn) as an argument, and any other arguments necessary.
</p>
</td></tr>
<tr><td><code id="MCdof_+3A_zfun.args">zfun.args</code></td>
<td>

<p>list object giving the values for additional arguments to the function named by <code>zfun</code>.
</p>
</td></tr>
<tr><td><code id="MCdof_+3A_which.test">which.test</code></td>
<td>

<p>character naming which type of test to do (default, &ldquo;t&rdquo;, is a t-test, calls <code>sig.cor.t</code>). &ldquo;Z&rdquo; does Fisher's Z transform (calls <code>sig.cor.Z</code>).   &ldquo;cor.test&rdquo; calls <code>cor.test</code> giving more options, but is also considerably slower than &ldquo;t&rdquo; or &ldquo;Z&rdquo;.
</p>
</td></tr>
<tr><td><code id="MCdof_+3A_r">r</code></td>
<td>
<p>numeric giving the correlation value(s).</p>
</td></tr>
<tr><td><code id="MCdof_+3A_len">len</code></td>
<td>
<p>numeric giving the size of the data for the test.</p>
</td></tr>
<tr><td><code id="MCdof_+3A_h0">H0</code></td>
<td>
<p>numeric giving the null hypothesis value (not used by <code>MCdof</code>).</p>
</td></tr>
<tr><td><code id="MCdof_+3A_verbose">verbose</code></td>
<td>

<p>logical, should progress information (including total run time) be printed to the screen?
</p>
</td></tr>
<tr><td><code id="MCdof_+3A_...">...</code></td>
<td>

<p>optional arguments to <code>sig.cor.t</code> (not used), <code>sig.cor.Z</code>, or <code>cor.test</code> depending on argument <code>which.test</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function does the Livezey and Chen (1983) Monte Carlo step 2 (a) from Elmore et al. (2006).  It generates a random sample of size n, and finds the p-values of a correlation test with this random sample and each column of <code>x</code>.  From this, it estimates the proportion of spatial locations that could contain significant bias purely by chance.
</p>


<h3>Value</h3>

<p>MCdof returns a list object with components:
</p>
<table>
<tr><td><code>MCprops</code></td>
<td>
<p>numeric vector of length ntrials giving the proportion of locations with significant bias found by chance for each repition of the experiment.</p>
</td></tr>
<tr><td><code>minsigcov</code></td>
<td>
<p> single numeric giving the 1 - field.sig quantile of the resulting proportions given by MCprops.</p>
</td></tr>
</table>
<p>sig.cor.t and sig.cor.Z return umeric vectors of p-values, and fisherz returns a numeric vector of test statistics.
</p>


<h3>Author(s)</h3>

<p>Kimberly L. Elmore, Kim.Elmore &ldquo;at&rdquo; noaa.gov, and Eric Gilleland
</p>


<h3>References</h3>

<p>Elmore, K. L., Baldwin, M. E. and Schultz, D. M. (2006) Field significance revisited: Spatial bias errors in forecasts as applied to the Eta model.  <em>Mon. Wea. Rev.</em>, <b>134</b>, 519&ndash;531.
</p>
<p>Livezey, R. E. and Chen, W. Y. (1983) Statistical field significance and its determination by Monte Carlo techniques.  <em>Mon. Wea. Rev.</em>, <b>111</b>, 46&ndash;59.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+spatbiasFS">spatbiasFS</a></code>, <code><a href="#topic+LocSig">LocSig</a></code>, <code><a href="stats.html#topic+cor.test">cor.test</a></code>, <code><a href="stats.html#topic+rnorm">rnorm</a></code>, <code><a href="stats.html#topic+runif">runif</a></code>, <code><a href="stats.html#topic+rexp">rexp</a></code>, <code><a href="stats.html#topic+rgamma">rgamma</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data( "GFSNAMfcstEx" )
data( "GFSNAMobsEx" )
data( "GFSNAMlocEx" )

id &lt;- GFSNAMlocEx[,"Lon"] &gt;=-90 &amp; GFSNAMlocEx[,"Lon"] &lt;= -75 &amp; GFSNAMlocEx[,"Lat"] &lt;= 40

look &lt;- MCdof(GFSNAMfcstEx[,id] - GFSNAMobsEx[,id], ntrials=500)

stats(look$MCprops)
look$minsigcov

fisherz( abs(cor(rnorm(10),rexp(10), use="pairwise.complete.obs")))

</code></pre>

<hr>
<h2 id='MergeForce'>
Force Merges in Matched Feature Objects
</h2><span id='topic+MergeForce'></span>

<h3>Description</h3>

<p>Force merges in matched feature objects so that, among other things, subsequent analyses are quicker and cleaner.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MergeForce(x, verbose = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MergeForce_+3A_x">x</code></td>
<td>

<p>list object of class &ldquo;matched&rdquo;.
</p>
</td></tr>
<tr><td><code id="MergeForce_+3A_verbose">verbose</code></td>
<td>

<p>logical, should progress information be printed to the screen.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Objects returned by functions such as <code>deltamm</code> and <code>centmatch</code> provide information necessary to merge and match features from &ldquo;features&rdquo; objects.  In the case of <code>centmatch</code>, only implicit merges are given, and this function creates objects where the implicit merges are forced to be merged.  In the case of <code>deltamm</code>, a second pass through might yield better merges/matches in that without a second pass, only features in one field or the other can be merged and matched (not both simultaneously).  Using this function, and apssing the result back through <code>deltamm</code> can result in subsequent matches of merged features from both fields simultaneously.  Moreover, in some cases, it may be more computationally efficient to run this function once for subsequent analyses/plotting.
</p>


<h3>Value</h3>

<p>A list object of class &ldquo;matched&rdquo; is returned containing several components and the same attributes as x.
</p>
<table>
<tr><td><code>match.message</code></td>
<td>
<p>A character string stating how features were matched with (merged) apended.</p>
</td></tr>
<tr><td><code>match.type</code></td>
<td>
<p>character of length 2 naming the original matching function used and this function to note that the features have been forced to be merged/clustered together.  </p>
</td></tr>
<tr><td><code>matches</code></td>
<td>
<p>two-column matrix with forecast object numbers in the first column and corresponding matched observed features in the second column.  If no matches, this will have value integer(0) for each column giving a matrix with dimension 0 by 2.</p>
</td></tr>
<tr><td><code>unmatched</code></td>
<td>
<p>list with components X and Xhat giving the unmatched object numbers, if any, from the observed and forecast fields, resp.  If none, the value will be integer(0).</p>
</td></tr>
</table>
<p>Note that all of the same list components of x are passed back, except for special information (which is usually no longer relevant) such as Q (deltamm), criteria, criteria.values, centroid.distances (centmatch)
</p>
<p>Additionally, merges and/or implicit.merges (centmatch) are not included as they have been merged.
</p>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>See Also</h3>

<p>For identifying features in a field: <code><a href="#topic+FeatureFinder">FeatureFinder</a></code>
</p>
<p>For merging and/or matching features: <code><a href="#topic+deltamm">deltamm</a></code>, <code><a href="#topic+centmatch">centmatch</a></code>, <code><a href="#topic+plot.matched">plot.matched</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- y &lt;- matrix(0, 100, 100)
x[2:3,c(3:6, 8:10)] &lt;- 1
y[c(4:7, 9:10),c(7:9, 11:12)] &lt;- 1

x[30:50,45:65] &lt;- 1
y[c(22:24, 99:100),c(50:52, 99:100)] &lt;- 1

hold &lt;- make.SpatialVx( x, y, field.type="contrived", units="none",
    data.name = "Example", obs.name = "x", model.name = "y" )

look &lt;- FeatureFinder(hold, smoothpar=0.5)

look2 &lt;- centmatch( look )

look2

look2 &lt;- MergeForce( look2 )

look2

# plot( look2 )


</code></pre>

<hr>
<h2 id='metrV'>
Binary Location Metric Proposed in Zhu et al. (2011)
</h2><span id='topic+metrV'></span><span id='topic+metrV.default'></span><span id='topic+metrV.SpatialVx'></span><span id='topic+print.metrV'></span>

<h3>Description</h3>

<p>Calculate the metric metrV proposed in Zhu et al (2011), which is a linear combination
of the square root of the sum of squared error between two binary fields, and the
mean error distance (Peli and Malah, 1982); or the difference in mean error distances
between two forecast fields and the verification field, if the comparison is performed
between two forecast models against the same verification field.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>metrV(x, ...)

## Default S3 method:
metrV(x, xhat, xhat2 = NULL, thresholds, lam1 = 0.5, lam2 = 0.5, 
    distfun = "distmapfun", a = NULL, verbose = FALSE, ...)

## S3 method for class 'SpatialVx'
metrV(x, time.point = 1, obs = 1, model = 1, lam1 = 0.5, lam2 = 0.5, 
    distfun = "distmapfun", verbose = FALSE, ...)

## S3 method for class 'metrV'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="metrV_+3A_x">x</code></td>
<td>

<p>Either a list object as returned by <code>make.SpatialVx</code> or a matrix representing a verificaiton grid.  For the <code>print</code> method, this is an object returned by <code>metrV</code>.
</p>
</td></tr>
<tr><td><code id="metrV_+3A_xhat">xhat</code>, <code id="metrV_+3A_xhat2">xhat2</code></td>
<td>

<p>(xhat2 is optional) matrix representing a forecast grid.
</p>
</td></tr>
<tr><td><code id="metrV_+3A_thresholds">thresholds</code></td>
<td>
<p>q X 2 or q X 3 (if <code>xhat</code> is not NULL) matrix giving the thresholds to apply to the verification field (first column) and each forecast field.</p>
</td></tr>
<tr><td><code id="metrV_+3A_lam1">lam1</code></td>
<td>

<p>numeric giving the weight to be applied to the square root of the sum of squared errors of binary fields term in metrV.
</p>
</td></tr>
<tr><td><code id="metrV_+3A_lam2">lam2</code></td>
<td>

<p>numeric giving the weight to be applied to the mean error distance term in metrV.
</p>
</td></tr>
<tr><td><code id="metrV_+3A_distfun">distfun</code></td>
<td>
<p>character naming a function with which to calculate the shortest distances between each point x in the grid and the set of events.  Default is the Euclidean distance metric (see the help file for <code>locperf</code> for more information). </p>
</td></tr>
<tr><td><code id="metrV_+3A_a">a</code></td>
<td>
<p>list object giving certain information about the verification set.  These are the attributes of the &ldquo;SpatailVx&rdquo; object.  May be used here to include information (as attributes of the returned object) that would otherwise not be available to the <code>print</code> method function.  In particular, the components, <code>msg</code>, <code>data.name</code> and <code>qs</code> are printed if available.</p>
</td></tr>
<tr><td><code id="metrV_+3A_time.point">time.point</code></td>
<td>
<p>numeric or character indicating which time point from the &ldquo;SpatialVx&rdquo; verification set to select for analysis.</p>
</td></tr>
<tr><td><code id="metrV_+3A_obs">obs</code>, <code id="metrV_+3A_model">model</code></td>
<td>
<p>numeric indicating which observation/forecast model to select for the analysis.  May have length one or two.  If it has length two, the second value is taken to be the second forecast model (i.e., <code>xhat2</code> in the call to <code>metrV.default</code>).</p>
</td></tr>
<tr><td><code id="metrV_+3A_verbose">verbose</code></td>
<td>

<p>logical, should progress information be printed ot the screen.
</p>
</td></tr>
<tr><td><code id="metrV_+3A_...">...</code></td>
<td>

<p>Optional arguments to the <code>distfun</code> function.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The binary location metric proposed in Zhu et al. (2011) is a linear combination of two measures: the amount of overlap between events in two fields, given by <code>distOV</code> (simply the square root of sum of squared errors between two binary fields), and (if there are events in both fields) the mean error distance described in Peli and Malah (1982); see also Baddeley (1992).  The metric can be computed between a forecast field, M1, and the verificaiton field, V, or it can be compared between two foreast models M1 and M2 with reference to V.  That is,
</p>
<p>metrV(M1,M2) = lam1*distOV(I.M1,I.M2) + lam2*distDV(I.M1,I.M2),
</p>
<p>where I.M1 (I.M2) is the binary field determined by M1 &gt;= threshold (M2 &gt;= threshold), distOV(I.M1,I.M2) = sqrt( sum( (I.M1 - I.M2)^2)), distDV(I.M1,I.M2) = abs(distob(I.V,I.M1) - distob(I.V,I.M2)), where distob(A,B) is the mean error distance between A and B, given by:
</p>
<p>e(A,B) = 1/(N(A))*sqrt( sum( d(x,B)), where the summation is over all the points x corresponding to events in A, and d(x,B) is the minimum of the shortest distance from the point x to each point in B.  e(A,B) is calculated by using the distance transform as calculated by the <code>distmap</code> function from package <code>spatstat</code> for computational efficiency.
</p>
<p>Note that if there are no events in both fields, then by definition, the term distob(A,B) = 0, and if there are no events in one and only one of the two fields, then a large constant (here, the maximum dimension of the field), is returned.  In this way, distob differs from the mean error distance described in Peli and Malah (1982).
</p>
<p>If comparing between the verification field and one forecast model, then the distDV term simplifies to just distob(I.V,I.M1).
</p>
<p>One final note is that Eq (6) that defines <code>distOV</code> in Zhu et al. (2011) is correct (or rather, what is used in the paper).  It is not, as is stated below Eq (6) in Zhu et al. (2011) the root *mean* square error, but rather the root square error.  This function computes Eq (6) as written.
</p>


<h3>Value</h3>

<p>list object of class &ldquo;metrV&rdquo; with components:
</p>
<table>
<tr><td><code>OvsM1</code></td>
<td>
<p>k by 3 matrix whose rows represent thresholds and columns give the component distOV, distob and metrV between the verification field and the forecast model 1.</p>
</td></tr>
<tr><td><code>OvsM2</code></td>
<td>
<p>If object2 supplied, k by 3 matrix whose rows represent thresholds and columns give the component distOV, distob and metrV between the verification field and the forecast model 2.</p>
</td></tr>
<tr><td><code>M1vsM2</code></td>
<td>
<p>If object2 supplied, k by 3 matrix whose rows represent thresholds and columns give the component distOV, distob and metrV between model 1 and model 2.</p>
</td></tr>
</table>
<p>May also contain attributes as passed by either the a argument or the &ldquo;SpatialVx&rdquo; object.
</p>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>References</h3>

<p>Baddeley, A. J. (1992) An error metric for binary images.  In <em>Robust Computer Vision Algorithms</em>, W. Forstner and S. Ruwiedel, Eds., Wichmann, 59&ndash;78.
</p>
<p>Peli, T. and Malah, D. (1982) A study on edge detection algorithms.  <em>Computer Graphics and Image Processing</em>, <b>20</b>, 1&ndash;21.
</p>
<p>Zhu, M., Lakshmanan, V. Zhang, P. Hong, Y. Cheng, K. and Chen, S. (2011) Spatial verification using a true metric.  <em>Atmos. Res.</em>, <b>102</b>, 408&ndash;419, doi:10.1016/j.atmosres.2011.09.004.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+distob">distob</a></code>, <code><a href="spatstat.geom.html#topic+distmap">distmap</a></code>, <code><a href="spatstat.geom.html#topic+im">im</a></code>, <code><a href="spatstat.geom.html#topic+solutionset">solutionset</a></code>, <code><a href="spatstat.geom.html#topic+deltametric">deltametric</a></code>, <code><a href="#topic+locmeasures2d">locmeasures2d</a></code>, <code><a href="#topic+make.SpatialVx">make.SpatialVx</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
A &lt;- B &lt;- B2 &lt;- matrix( 0, 10, 12)
A[2,3] &lt;- 3
B[4,7] &lt;- 400
B2[10,12] &lt;- 17
hold &lt;- make.SpatialVx( A, list(B, B2), thresholds = c(0.1, 3.1, 500),
    field.type = "contrived", units = "none",
    data.name = "Example", obs.name = "A",
    model.name = c("B", "B2") )

metrV(hold)

metrV(hold, model = c(1,2) )

## Not run: 

data( "geom000" )
data( "geom001" )

testobj &lt;- make.SpatialVx( geom000, geom001, thresholds = 0,
    projection = TRUE, map = TRUE, loc = ICPg240Locs, loc.byrow = TRUE,
    field.type = "Precipitation", units = "mm/h",
    data.name = "ICP Geometric Cases", obs.name = "geom000",
    model.name = "geom001" )

metrV(testobj)

# compare above to results in Fig. 2 (top right panel)
# of Zhu et al. (2011).  Note that they differ wildly.
# Perhaps because an actual elliptical area is taken in
# the paper instead of finding the values from the fields
# themselves?

## End(Not run)

</code></pre>

<hr>
<h2 id='Mij'>
Raw Image Moments.
</h2><span id='topic+Mij'></span>

<h3>Description</h3>

<p>Calculate the raw Hu image moment Mij.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Mij(x, s, i = 0, j = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Mij_+3A_x">x</code></td>
<td>

<p>A matrix.
</p>
</td></tr>
<tr><td><code id="Mij_+3A_s">s</code></td>
<td>

<p>A two-column matrix giving the location coordinates.  May be missing in which case they are assumed to be integers giving the row and column numbers.
</p>
</td></tr>
<tr><td><code id="Mij_+3A_i">i</code>, <code id="Mij_+3A_j">j</code></td>
<td>

<p>Integer giving the moment order for each coordinate x and y, resp.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The raw moment M(ij) (Hu 1962) is calculated by
</p>
<p>M(ij) = sum(x^i * y^j * Im[i, j])
</p>
<p>where x and y are the pixel coordinates and Im is the (image) matrix.  Various useful properties of an image may be gleaned from certain moments.  For example, the image area is given by M(00), and the image centroid is (M(10) / M(00), M(01) / M(00)).  The image orientation angle can also be derived.
</p>


<h3>Value</h3>

<p>A single numeric giving the desired moment is returned.
</p>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>References</h3>

<p>Hu, M. K. (1962) Visual Pattern Recognition by Moment Invariants. <em>IRE Trans. Info. Theory</em>, <b>IT-8</b>, 179&ndash;187.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+imomenter">imomenter</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data( "geom000" )

Mij( geom000 ) # area
</code></pre>

<hr>
<h2 id='minboundmatch'>
Minimum Boundary Separation Feature Matching
</h2><span id='topic+minboundmatch'></span>

<h3>Description</h3>

<p>Match identified features within a spatial verification set via their minimum boundary separation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>minboundmatch(x, type = c("single", "multiple"), mindist = Inf, verbose = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="minboundmatch_+3A_x">x</code></td>
<td>

<p>An object of class &ldquo;features&rdquo;.
</p>
</td></tr>
<tr><td><code id="minboundmatch_+3A_type">type</code></td>
<td>

<p>character string stating either &ldquo;single&rdquo; or &ldquo;multiple&rdquo;.  In the former case, each feature in one field will be matched to only one feature in the other, which will be taken to be the features who have the smallest minimum boundary separation.  In the case of &ldquo;multiple&rdquo;, the <code>mindist</code> argument should be set to something small enough so that not every feature will be matched to every other feature.  Also, the <code>MergeForce</code> function may be useful in this case.
</p>
</td></tr>
<tr><td><code id="minboundmatch_+3A_mindist">mindist</code></td>
<td>

<p>single numeric giving the minimum boundary separation distance (measured by grid squares) beyond which features should not be matched.
</p>
</td></tr>
<tr><td><code id="minboundmatch_+3A_verbose">verbose</code></td>
<td>

<p>logical, should progress information be printed to the screen?
</p>
</td></tr>
<tr><td><code id="minboundmatch_+3A_...">...</code></td>
<td>

<p>Optional arguments to the <code>distmap</code> function from package <span class="pkg">spatstat</span>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>the minimum boundary separation is calculated by first finding the distance map for every feature in the observed field, masking it by each feature in the forecast field, and then finding the minimum of the resulting masked distance map.  If <code>type</code> is &ldquo;single&rdquo;, then the features are matched by the smallest minimum boundary separation per feature in each field.  If <code>type</code> is &ldquo;multiple&rdquo;, then every feature is matched so long as their minimum boundary separation (measured in grid squares) is less than or equal to <code>mindist</code>.
</p>


<h3>Value</h3>

<p>A list object of class &ldquo;matched&rdquo; is returned.  If the type argument is &ldquo;multiple&rdquo;, then an implicite.merges component is included, which will work with the MergeForce function.
</p>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>See Also</h3>

<p><code><a href="#topic+deltamm">deltamm</a></code>, <code><a href="#topic+centmatch">centmatch</a></code>, <code><a href="#topic+MergeForce">MergeForce</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
x &lt;- y &lt;- matrix(0, 100, 100)
x[2:3,c(3:6, 8:10)] &lt;- 1
y[c(4:7, 9:10),c(7:9, 11:12)] &lt;- 1

x[30:50,45:65] &lt;- 1
y[c(22:24, 99:100),c(50:52, 99:100)] &lt;- 1

hold &lt;- make.SpatialVx( x, y, field.type = "contrived", units = "none",
    data.name = "Example", obs.name = "x", model.name = "y" )

look &lt;- FeatureFinder(hold, smoothpar=0.5)

look2 &lt;- minboundmatch( look )

look2 &lt;- MergeForce( look2 )

par( mfrow = c(1,2) )
plot( look2 )

look3 &lt;- minboundmatch( look, type = "multiple", mindist = 50 )
look3 &lt;- MergeForce( look2 )
plot( look3 )

look4 &lt;- minboundmatch( look, type = "multiple", mindist = 20 )
look4 &lt;- MergeForce( look4 )
plot( look4 )


</code></pre>

<hr>
<h2 id='obs0601'>
Spatial Forecast Verification Methods Inter-Comparison Project (ICP) Test Cases and other example verification sets
</h2><span id='topic+obs0601'></span><span id='topic+wrf4ncar0531'></span><span id='topic+geom000'></span><span id='topic+geom001'></span><span id='topic+geom002'></span><span id='topic+geom003'></span><span id='topic+geom004'></span><span id='topic+geom005'></span><span id='topic+ICPg240Locs'></span>

<h3>Description</h3>

<p>Test cases used for the ICP.  In particular, those actually analyzed in the special collection of the journal, Weather and Forecasting.  Includes the nine &ldquo;real&rdquo; cases, five simple geometric cases, and the seven perturbed &ldquo;real&rdquo; cases.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data( "obs0601" )
data( "wrf4ncar0531" )
data( "geom000" )
data( "geom001" )
data( "geom002" )
data( "geom003" )
data( "geom004" )
data( "geom005" )
data( "ICPg240Locs" )
</code></pre>


<h3>Format</h3>

<p>The format is:
num [1:601, 1:501] 0 0 0 0 0 0 0 0 0 0 ...
</p>
<p>The format is:
num [1:301101, 1:2] -110 -110 -110 -110 -110 ...
- attr(*, &quot;dimnames&quot;)=List of 2
..$ : NULL
..$ : chr [1:2] &quot;lon&quot; &quot;lat&quot;
</p>


<h3>Details</h3>

<p>One of the nine ICP &ldquo;real&rdquo; cases is from one version of the Weather Research Forecast (WRF) model denoted <code>wrf4ncar</code> (see Kain et al. 2008; Ahijevych et al., 2009 for complete details), and the corresponding &ldquo;observed&rdquo; field is stage II reanalyses denoted here by &ldquo;obs&rdquo;.  The model is a 24-h forecast so that the valid time is for the next day (e.g., <code>obs0531</code> corresponds with <code>obs0601</code>).
</p>
<p>These data are a subset from the 2005 Spring Program of the Storm Prediction Center/National Severe Storms Laboratory (SPC/NSSL, cf. Weiss et al., 2005; Kain et al., 2008).  Units for the real cases are in mm/h, and are on the NCEP g240 grid (~4-km resolution) with 601 X 501 grid points.  Both SPC and NSSL should be cited as sources for these cases, as well as Weiss et al. (2005) and possibly also Kain et al. (2008).  The data were made available to the ICP by M. E. Baldwin.
</p>
<p>The five geometric cases are simple ellipses (each with two intensities) that are compared against the verification case (<code>geom000</code>) on the same NCEP g240 grid as the nine real cases.  See Ahijevych et al. (2009) for complete details.  Case <code>geom001</code> is exactly the same as <code>geom000</code>, but is displaced 50 grid points to the right (i.e., ~200 km too far east).  Case <code>geom002</code> is also identical to <code>geom000</code>, but displaced 200 grid points to the right.  case <code>geom003</code> is displaced 125 grid points to the right, and is also too big 9i.e., has a spatial extent, or coverage, bias).  Case <code>geom004</code> is also displaced 125 grid points to the right, but also has a different orientation (note, however, that it is not a true rotation of <code>geom000</code>).  Case <code>geom005</code> is displaced 125 grid points to the right, and has a huge spatial extent bias.  This last case is also the only one that actually overlaps with <code>geom000</code>, and therefore may be regarded by some as the best case.  It is certainly the case that comes out on top by the traditional verification statistics that are calculated on a grid point by grid point basis.  Ahijevych et al. (2009) should be cited if these geometric cases are used for publications, etc.
</p>
<p>The longitude and latitude information for each grid (the NCEP g240 grid) is contained in the <code>ICPg240Locs</code> dataset.
</p>
<p>Other data sets for the ICP can be obtained from the ICP web site (<a href="https://projects.ral.ucar.edu/icp/">https://projects.ral.ucar.edu/icp/</a>).  MesoVICT data sets are also available there.  All of the ICP test cases used to be available in this package, but had to be removed because of space concerns on CRAN.
</p>


<h3>Source</h3>

<p><a href="https://projects.ral.ucar.edu/icp/">https://projects.ral.ucar.edu/icp/</a>
</p>


<h3>References</h3>

<p>Ahijevych, D., Gilleland, E., Brown, B. G. and Ebert, E. E. (2009) Application of spatial verification methods to idealized and NWP gridded precipitation forecasts. <em>Wea. Forecasting</em>, <b>24</b> (6), 1485&ndash;1497.
</p>
<p>Kain, J. S., Weiss, S. J., Bright, D. R., Baldwin, M. E. Levit, J. J. Carbin, G. W. Schwartz, C. S. Weisman, M. L. Droegemeier, K. K. Weber, and D. B. Thomas, K. W. (2008)  Some Practical Considerations Regarding Horizontal Resolution in the First Generation of Operational Convection-Allowing NWP. <em>Wea. Forecasting</em>, <b>23</b>, 931&ndash;952.
</p>
<p>Weiss, S., Kain, J. Levit, J. Baldwin, M. E., Bright, D. Carbin, G. and Hart, J. (2005) NOAA Hazardous Weather Testbed. SPC/NSSL Spring Program 2005 Program Overview and Operations Plan. 61pp.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
data( "obs0601" )
data( "wrf4ncar0531" )
data( "ICPg240Locs" )
## Plot verification sets with a map.
## Two different methods.

# First way does not preserve projections.
locr &lt;- c( range( ICPg240Locs[,1]), range( ICPg240Locs[,2]))
zl &lt;- range( c( c(obs0601), c( wrf4ncar0531) ) )
par( mfrow=c(2,1), mar=rep(0.1,4))
image( obs0601, axes=FALSE, col=c("grey", tim.colors(256)), zlim=zl)
par( usr=locr)
# if( map.available) map( add=TRUE, database="state") # from library( "maps" )
image( wrf4ncar0531, axes=FALSE, col=c("grey", tim.colors(256)), zlim=zl)
par( usr=locr)
# if( map.available) map( add=TRUE, database="state")
image.plot( obs0601, legend.only=TRUE, horizontal=TRUE, 
		col=c("grey", tim.colors(256)), zlim=zl)

# Second way preserves projections, but values are slighlty interpolated.
zl &lt;- range( c( c(obs0601), c( wrf4ncar0531) ) )
par( mfrow=c(2,2), mar=rep(2.1,4))
image(as.image(c(t(obs0601)), x=ICPg240Locs, nx=601, ny=501, na.rm=TRUE), zlim=zl,
        col=c("grey", tim.colors(64)), axes=FALSE, main="Stage II Reanalysis 4/26/05 0000 UTC")
# map(add=TRUE, lwd=1.5)
# map(add=TRUE, database="state", lty=2)
image(as.image(c(t(wrf4ncar0531)), x=ICPg240Locs, nx=601, ny=501, na.rm=TRUE), zlim=zl,
        col=c("grey", tim.colors(64)), axes=FALSE, main="WRF NCAR valid 4/26/05 0000 UTC")
image.plot(obs0601, col=c("grey", tim.colors(64)), zlim=zl, legend.only=TRUE, horizontal=TRUE)

## End(Not run)
</code></pre>

<hr>
<h2 id='OF'>
Optical Flow Verification
</h2><span id='topic+OF'></span><span id='topic+OF.default'></span><span id='topic+OF.SpatialVx'></span><span id='topic+plot.OF'></span><span id='topic+print.OF'></span><span id='topic+hist.OF'></span><span id='topic+summary.OF'></span>

<h3>Description</h3>

<p>Perform verification using optical flow as described in Marzban and Sandgathe (2010).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>OF(x, ...)

## Default S3 method:
OF(x, ..., xhat, W = 5, grads.diff = 1, center = TRUE, 
    cutoffpar = 4, verbose = FALSE)

## S3 method for class 'SpatialVx'
OF(x, ..., time.point = 1, obs = 1, model = 1, W = 5, grads.diff = 1,
    center = TRUE, cutoffpar = 4, verbose = FALSE)

## S3 method for class 'OF'
plot(x, ...)

## S3 method for class 'OF'
print(x, ...)

## S3 method for class 'OF'
hist(x, ...)

## S3 method for class 'OF'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="OF_+3A_x">x</code>, <code id="OF_+3A_xhat">xhat</code></td>
<td>

<p>Default: m by n matrices describing the verification and forecast fields, resp.  The forecast field is considered the initial field that is morphed into the final (verification) field.
</p>
<p><code>OF.SpatialVx</code>: list object of class &ldquo;SpatialVx&rdquo;.
</p>
<p><code>plot</code>, <code>hist</code> and <code>print</code> methods: list object as returned by <code>OF</code>.
</p>
</td></tr>
<tr><td><code id="OF_+3A_object">object</code></td>
<td>

<p>list object as returned by <code>OF</code>.
</p>
</td></tr>
<tr><td><code id="OF_+3A_w">W</code></td>
<td>

<p>numeric/integer giving the window size (should be no smaller than 5).
</p>
</td></tr>
<tr><td><code id="OF_+3A_grads.diff">grads.diff</code></td>
<td>

<p>1 or 2 describing whether to use first or second differences in finding the first derivative.
</p>
</td></tr>
<tr><td><code id="OF_+3A_center">center</code></td>
<td>

<p>logical, should the fields be centered before performing the optical flow?
</p>
</td></tr>
<tr><td><code id="OF_+3A_cutoffpar">cutoffpar</code></td>
<td>

<p>numeric, set to NaN everything exceeding median +/- <code>cutoffpar</code>*sd.
</p>
</td></tr>
<tr><td><code id="OF_+3A_verbose">verbose</code></td>
<td>

<p>logical, should progress information be printed to the screen?
</p>
</td></tr>
<tr><td><code id="OF_+3A_time.point">time.point</code></td>
<td>
<p>numeric or character indicating which time point from the &ldquo;SpatialVx&rdquo; verification set to select for analysis.</p>
</td></tr>
<tr><td><code id="OF_+3A_obs">obs</code>, <code id="OF_+3A_model">model</code></td>
<td>
<p>numeric indicating which observation/forecast model to select for the analysis.</p>
</td></tr>
<tr><td><code id="OF_+3A_...">...</code></td>
<td>

<p>For <code>OF</code>: optional arguments to the <code>optim</code> function (cannot be <code>par</code>, <code>fn</code>, <code>gr</code> or <code>method</code>).  See details section for <code>plot</code> and <code>hist</code> method functions.  Not used by the <code>summary</code> method function.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Estimates the optical flow of the forecast field into the verification field.  Letting I_o(x,y) and I_f(x,y) represent the intensities of each field at coordinate (x,y), the collection of pairs (dx, dy) is the optical flow field, where:
</p>
<p>I_o(x,y) ~ I_f(x,y) + [partial(I_f) wrt x]*dx + [partial(I_f) wrt y]*dy.
</p>
<p>The procedure follows that proposed by Lucas and Kanade (1981) whereby for some window, W, it is assumed that all dx (dy) are assumed constant, and least squares estimation is used to estimate dx and dy (see Marzban and Sandgathe, 2010 for more on this implementation).  This function iteratively calls optflow for each window in the field.
</p>
<p>The above formulation is linear in the parameters.  Marzban and Sandgathe (2010) also introduce an additive error component, which leads to a nonlinear version of the above.  Namely,
</p>
<p>I_o(x,y) ~ I_f(x,y) + [partial(I_f) wrt x]*dx + [partial(I_f) wrt y]*dy + A(x,y).
</p>
<p>See Marzban and Sandgathe for more details.
</p>
<p>The plot method function can produce a figure like that of Fig. 1, 5, and 6 in Marzban and Sandgathe (2010) or with option <code>full=TRUE</code>, even more plots.  Optional arguments that may be passed in via the ellipses include: <code>full</code> (logical, produce a figure analogous to Fig. 1, 5 and 6 from Marzban and Sandgathe (2010) (FALSE/default) or make more plots (TRUE)), <code>scale</code> (default is 1 or no scaling, any numeric value by which the fields are divided/scaled before plotting), <code>of.scale</code> (default is 1, factor by which display vectors can be magnified), <code>of.step</code> (plot OF vectors every of.step, default is 4), <code>prop</code> (default is 2, value for <code>prop</code> argument in the call to <code>rose.diag</code> from package <span class="pkg">CircStats</span>), <code>nbins</code> (default is 40, number of bins to use in the call to <code>rose.diag</code>).
</p>
<p>The <code>hist</code> method function produces a two-dimensional histogram like that of Fig. 3 and 7 in Marzban and Sandgathe (2010).  It can also take various arguments passed via the ellipses.  They include: <code>xmin</code>, <code>xmax</code>, <code>ymin</code>, <code>ymax</code> (lower and upper bounds for the histogram breaks in the x- (angle) and y- (magnitude/displacement error) directions, resp.  Defaults to (0,360) and (0,4)), nbreaks (default is 100, the number of breaks to use).
</p>
<p>The <code>summary</code> method mostly uses the <code>stats</code> function from package <span class="pkg">fields</span> to summarize results of the errors, but also uses <code>circ.summary</code> from package <span class="pkg">CircStats</span> for the angular errors.
</p>


<h3>Value</h3>

<p>OF returns a list object of class &ldquo;OF&rdquo; with components:
</p>
<table>
<tr><td><code>data</code></td>
<td>
<p>list with components x and xhat containing the data.</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>character vector giving the names of the verification and forecast fields.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>object of class &ldquo;call&rdquo; giving the original function call.</p>
</td></tr>
<tr><td><code>rows</code>, <code>cols</code></td>
<td>
<p>numeric vector giving the rows and columns used for finding the centers of windows.  Needed by the plot and hist method functions.</p>
</td></tr>
<tr><td><code>err.add.lin</code></td>
<td>
<p>m by n matrix giving the linear additive errors (intensities).</p>
</td></tr>
<tr><td><code>err.mag.lin</code></td>
<td>
<p>m by n matrix giving the linear magnitude (displacement) errors.</p>
</td></tr>
<tr><td><code>err.ang.lin</code></td>
<td>
<p>m by n matrix giving the linear angular errors.</p>
</td></tr>
<tr><td><code>err.add.nlin</code>, <code>err.mag.nlin</code>, <code>err.ang.nlin</code></td>
<td>
<p>same as above but for nonlinear errors.</p>
</td></tr>
<tr><td><code>err.vc.lin</code>, <code>err.vr.lin</code>, <code>err.vc.nlin</code>, <code>err.vr.nlin</code></td>
<td>
<p>m by n matrices giving the x- and y- direction movements for the linear and nonlinear cases, resp.</p>
</td></tr>
</table>
<p>The hist method function invisibly returns a list object of class &ldquo;OF&rdquo; that contains the same object that was passed in along with new components:
</p>
<table>
<tr><td><code>breaks</code></td>
<td>
<p>a list with components x and y giving the breaks in each direction</p>
</td></tr>
<tr><td><code>hist.vals</code></td>
<td>
<p>itself a list with components xb, yb (the number of breaks -1 used for each direction), and nb (the histogram values for each break)</p>
</td></tr>
</table>
<p>The plot and summary mehtod functions do not return anything.
</p>


<h3>Author(s)</h3>

<p>Caren Marzban, marzban &ldquo;at&rdquo; u.washington.edu, with modifications by Eric Gilleland
</p>


<h3>References</h3>

<p>Lucas, B D. and Kanade, T. (1981)  An iterative image registration technique with an application to stereo vision.  <em>Proc. Imaging Understanding Workshop</em>, DARPA, 121&ndash;130.
</p>
<p>Marzban, C. and Sandgathe, S. (2010)  Optical flow for verification.  <em>Wea. Forecasting</em>, <b>25</b>, 1479&ndash;1494, doi:10.1175/2010WAF2222351.1.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+optflow">optflow</a></code>, <code><a href="stats.html#topic+optim">optim</a></code>, <code><a href="CircStats.html#topic+circ.summary">circ.summary</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 
data(hump)
initial &lt;- hump$initial
final &lt;- hump$final
look &lt;- OF(final, xhat=initial, W=9, verbose=TRUE)
plot(look) # Compare with Fig. 1 in Marzban and Sandgathe (2010).
par(mfrow=c(1,1))
hist(look) # 2-d histogram.
plot(look, full=TRUE) # More plots.
summary(look)

# Another way to skin the cat.
hold &lt;- make.SpatialVx( final, initial, field.type = "Bi-variate Gaussian",
    obs.name = "final", model.name = "initial" )

look2 &lt;- OF(hold, W=9, verbose=TRUE)
plot(look2)
par(mfrow=c(1,1))
hist(look2)
plot(look2, full=TRUE)
summary(look2)

## End(Not run)
</code></pre>

<hr>
<h2 id='optflow'>
Optical Flow
</h2><span id='topic+optflow'></span>

<h3>Description</h3>

<p>Estimate the optical flow from one gridded field (image) to another.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optflow(initial, final, grads.diff = 1, mean.field = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="optflow_+3A_initial">initial</code>, <code id="optflow_+3A_final">final</code></td>
<td>
<p>m by n matrices where the optical flow is determined from initial (forecast) to final (observation).</p>
</td></tr>
<tr><td><code id="optflow_+3A_grads.diff">grads.diff</code></td>
<td>
<p>either 1 or 2, where 1 calculates first derivatives with first differences and 2 first derivatives with second differences.</p>
</td></tr>
<tr><td><code id="optflow_+3A_mean.field">mean.field</code></td>
<td>
<p>Should they first be centered?  If so, give the value for the centering here (usually the mean of initial).</p>
</td></tr>
<tr><td><code id="optflow_+3A_...">...</code></td>
<td>
<p>optional arguments to the <code>optim</code> function (cannot be <code>par</code>, <code>fn</code>, <code>gr</code> or <code>method</code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function estimates the optical flow from the initial field (image) to the final one as described in Marzban and Sandgathe (2010).  Letting I_o(x,y) and I_f(x,y) represent the intensities of each field at coordinate (x,y), the collection of pairs (dx, dy) is the optical flow field, where:
</p>
<p>I_o(x,y) ~ I_f(x,y) + [partial(I_f) wrt x]*dx + [partial(I_f) wrt y]*dy.
</p>
<p>The procedure follows that proposed by Lucas and Kanade (1981) whereby for some window, W, it is assumed that all dx (dy) are assumed constant, and least squares estimation is used to estimate dx and dy (see Marzban and Sandgathe, 2010 for more on this implementation).  It is assumed that the fields (initial and final) include only the window around the point of interest (i.e., this function finds the optical flow estimate for a single window).  See the function <code>OF</code>, which iteratively calls this function, for performing optical flow over the entire field.
</p>
<p>The above formulation is linear in the parameters.  Marzban and Sandgathe (2010) also introduce an additive error component, which leads to a nonlinear version of the above.  Namely,
</p>
<p>I_o(x,y) ~ I_f(x,y) + [partial(I_f) wrt x]*dx + [partial(I_f) wrt y]*dy + A(x,y).
</p>
<p>See Marzban and Sandgathe for more details.
</p>


<h3>Value</h3>

<p>numeric vector whose first three components are the optimized estimates (returned by  the par component of optim) for the regression I_o(x,y) - I_f(x,y) = a0 + a1*[partial(I_f) wrt x] + a2*[partial(I_f) wrt y] (i.e., a1 and a2 are the estimates for dx and dy, resp.) and the latter three values are the initial estimates to optim as determined by linear regression (i.e., returned from the lm function).
</p>


<h3>Author(s)</h3>

<p>Caren Marzban, marzban &ldquo;at&rdquo; u.washington.edu, and modified by Eric Gilleland
</p>


<h3>References</h3>

<p>Lucas, B D. and Kanade, T. (1981)  An iterative image registration technique with an application to stereo vision.  <em>Proc. Imaging Understanding Workshop</em>, DARPA, 121&ndash;130.
</p>
<p>Marzban, C. and Sandgathe, S. (2010)  Optical flow for verification.  <em>Wea. Forecasting</em>, <b>25</b>, 1479&ndash;1494, doi:10.1175/2010WAF2222351.1.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+OF">OF</a></code>, <code><a href="stats.html#topic+optim">optim</a></code>, <code><a href="stats.html#topic+lm">lm</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- y &lt;- matrix(0, 10, 10)
x[1:2,3:4] &lt;- 1
y[3:4,5:6] &lt;- 2

optflow(x,y)

## Not run: 
initial &lt;- hump$initial
final &lt;- hump$final
look &lt;- OF(final, initial, W=9, verbose=TRUE)
plot(look) # Compare with Fig. 1 in Marzban and Sandgathe (2010).
hist(look) # 2-d histogram.
plot(look, full=TRUE) # More plots.

## End(Not run)
</code></pre>

<hr>
<h2 id='pphindcast2d'>
Practically Perfect Hindcast Neighborhood Verification Method
</h2><span id='topic+pphindcast2d'></span><span id='topic+plot.pphindcast2d'></span><span id='topic+print.pphindcast2d'></span>

<h3>Description</h3>

<p>Function to perform the practically perfect hindcast neighborhood verification method.  Finds the
optimal threhsold, Pthresh, and calculates the desired statistic for that threshold.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pphindcast2d(object, which.score = "ets", time.point = 1, obs = 1,
                 model = 1, levels = NULL, max.n = NULL, smooth.fun =
                 "hoods2dsmooth", smooth.params = NULL, rule = "&gt;=",
                 verbose = FALSE, ...)

## S3 method for class 'pphindcast2d'
plot(x, ..., mfrow = NULL,
    type = c("quilt", "line"), 
    col = heat.colors(12), horizontal = FALSE)

## S3 method for class 'pphindcast2d'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pphindcast2d_+3A_object">object</code></td>
<td>

<p>A list object returned by the <code>make.SpatialVx</code> function.
</p>
</td></tr>
<tr><td><code id="pphindcast2d_+3A_which.score">which.score</code></td>
<td>

<p>character stating which verification score is to be used.  Must be one that is accepted by <code>vxstats</code>.
</p>
</td></tr>
<tr><td><code id="pphindcast2d_+3A_time.point">time.point</code></td>
<td>
<p>numeric or character indicating which time point from the &ldquo;SpatialVx&rdquo; verification set to select for analysis.</p>
</td></tr>
<tr><td><code id="pphindcast2d_+3A_obs">obs</code>, <code id="pphindcast2d_+3A_model">model</code></td>
<td>
<p>numeric indicating which observation/forecast model to select for the analysis.</p>
</td></tr>
<tr><td><code id="pphindcast2d_+3A_levels">levels</code></td>
<td>
<p>numeric vector giving the successive values of the smoothing parameter.  For example, for the default method, these are the neighborhood lengths over which the levels^2 nearest neighbors are averaged for each point.  Values should make sense for the specific smoothing function.  For example, for the default method, these should be odd integers.</p>
</td></tr>
<tr><td><code id="pphindcast2d_+3A_max.n">max.n</code></td>
<td>
<p>(optional) single numeric giving the maximum neighborhood length to use.  Only used if levels are NULL.</p>
</td></tr>
<tr><td><code id="pphindcast2d_+3A_smooth.fun">smooth.fun</code></td>
<td>
<p>character giving the name of a smoothing function to be applied.  Default is an average over the n^2 nearest neighbors, where n is taken to be each value of the <code>levels</code> argument.</p>
</td></tr> 
<tr><td><code id="pphindcast2d_+3A_smooth.params">smooth.params</code></td>
<td>
<p>list object containing any optional arguments to <code>smooth.fun</code>.  Use NULL if none.</p>
</td></tr>
<tr><td><code id="pphindcast2d_+3A_rule">rule</code></td>
<td>
<p>character string giving the threshold rule to be applied.  See help file for <code>thresholder</code> function for more information.</p>
</td></tr>
<tr><td><code id="pphindcast2d_+3A_verbose">verbose</code></td>
<td>

<p>logical, should progress information be printed to the screen?
</p>
</td></tr>
<tr><td><code id="pphindcast2d_+3A_x">x</code></td>
<td>
<p>An object of class &ldquo;pphindcast2d&rdquo; as returned by the self-same function.</p>
</td></tr>
<tr><td><code id="pphindcast2d_+3A_mfrow">mfrow</code></td>
<td>
<p> mfrow parameter (see help file for <code>par</code>).  If NULL, then the parameter is not re-set. </p>
</td></tr>
<tr><td><code id="pphindcast2d_+3A_type">type</code></td>
<td>
<p>character specifying whether two quilt plots (one for the score and one for Pthresh) should be made, or one line plot incorporating both the score and the Pthresh values; the latter's values being displayed on the right axis.</p>
</td></tr>
<tr><td><code id="pphindcast2d_+3A_col">col</code>, <code id="pphindcast2d_+3A_horizontal">horizontal</code></td>
<td>
<p>arguments used in the calls by <code>image</code> and <code>image.plot</code>.</p>
</td></tr>
<tr><td><code id="pphindcast2d_+3A_...">...</code></td>
<td>

<p><code>pphindcast2d</code>: optional arguments to the <code>optim</code> function.  May not include lower, upper or method as these are hard coded into the function.
</p>
<p><code>plot</code> method function: optional arguments to the <code>image</code> function.
</p>
<p><code>print</code> method function: not used.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The practically perfect hindcast method is described in Ebert (2008).  Using a similar notation as that described therein (and in the help
page for <code>hoods2d</code>), the method is a SO-NF approach that first compares the observed binary field (obtained from the trheshold(s) provided by <code>object</code>), Ix, with the smoothed binary field, &lt;Px&gt;s.  This smoothed binary field is thresholded by
Pthresh to obtain a new binary field.  The value of Pthresh that maximizes the verification score (provided by the which.score argument)
is then used to compare Ix with &lt;Iy&gt;s, the binary forecast field obtained by thresholding the smoothed binary forecast field Iy using
the value of Pthresh found above.  The verification statistic determined by which.score is calculated between Ix and &lt;Iy&gt;s.
</p>


<h3>Value</h3>

<p>A list object is returned with components:
</p>
<table>
<tr><td><code>which.score</code></td>
<td>
<p>value of which.score, same as the argument passed in.</p>
</td></tr>
<tr><td><code>Pthresh</code></td>
<td>
<p>l by q matrix giving the value of Pthresh applied at each level (rows) and threshold (columns).</p>
</td></tr>
<tr><td><code>values</code></td>
<td>
<p>l by q matrix giving the value of which.score found for each level (rows) and threshold (columns).</p>
</td></tr>
</table>


<h3>Warning </h3>

<p>The value Pthresh is optimized under the assumption that larger values of which.score are better.</p>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>References</h3>

<p>Ebert, E. E. (2008) Fuzzy verification of high resolution gridded forecasts: A review and proposed framework.  <em>Meteorol. Appl.</em>, <b>15</b>, 51&ndash;64. doi:10.1002/met.25 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+hoods2d">hoods2d</a></code>, <code><a href="smoothie.html#topic+kernel2dsmooth">kernel2dsmooth</a></code>, <code><a href="#topic+vxstats">vxstats</a></code>, <code><a href="#topic+hoods2dPlot">hoods2dPlot</a></code>, <code><a href="stats.html#topic+optim">optim</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- y &lt;- matrix( 0, 50, 50)
x[ sample(1:50,10), sample(1:50,10)] &lt;- rexp( 100, 0.25)
y[ sample(1:50,20), sample(1:50,20)] &lt;- rexp( 400)

hold &lt;- make.SpatialVx( x, y, thresholds=c(0.1, 0.5), field.type = "random")
look &lt;- pphindcast2d(hold, levels=c(1, 3))
look
## Not run: 
data( "geom001" )
data( "geom000" )
data( "ICPg240Locs" )

hold &lt;- make.SpatialVx( geom000, geom001, thresholds = c(0.01, 50.01),
    loc = ICPg240Locs, projection = TRUE, map = TRUE, loc.byrow = TRUE,
    data.name = "Geometric", obs.name = "geom000", model.name = "geom001",
    field.type = "Precipitation", units = "mm/h")

look &lt;- pphindcast2d( hold, levels=c(1, 3, 65), verbose=TRUE)

plot(look, mfrow = c(1, 2) )
plot(look, mfrow = c(1, 2), type = "line")

# Alternatively:
par( mfrow = c(1, 2) )
hoods2dPlot( look$values, args = attributes( look ),
    main="Gilbert Skill Score")

## End(Not run)
</code></pre>

<hr>
<h2 id='rigider'>
Rigid Transformation
</h2><span id='topic+rigider'></span><span id='topic+plot.rigided'></span><span id='topic+print.rigided'></span><span id='topic+summary.rigided'></span><span id='topic+rigidTransform'></span>

<h3>Description</h3>

<p>Find the optimal rigid transformation for a spatial field (e.g. an image).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
rigider(x1, x0, p0, init = c(0, 0, 0), type = c("regular", "fast"),
    translate = TRUE, rotate = FALSE, loss, loss.args = NULL,
    interp = "bicubic", stages = TRUE,
    verbose = FALSE, ...)

## S3 method for class 'rigided'
plot(x, ...)

## S3 method for class 'rigided'
print(x, ...)

## S3 method for class 'rigided'
summary(object, ...)

rigidTransform(theta, p0, N, cen)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rigider_+3A_x1">x1</code>, <code id="rigider_+3A_x0">x0</code></td>
<td>

<p>matrices of same dimensions giving the forecast (or 1-energy) and observation (or 0-energy) fields, resp.
</p>
</td></tr>
<tr><td><code id="rigider_+3A_x">x</code>, <code id="rigider_+3A_object">object</code></td>
<td>
<p>list object of class &ldquo;rigided&rdquo; as output by <code>rigider</code>.</p>
</td></tr>
<tr><td><code id="rigider_+3A_n">N</code></td>
<td>
<p>(optional) the dimension of the fields (i.e., if <code>x1</code> and <code>x0</code> are n by m, then <code>N</code> is the product m * n).</p>
</td></tr>
<tr><td><code id="rigider_+3A_cen">cen</code></td>
<td>
<p>N by 2 matrix whoes rows are all the same giving the center of the field (used to subtract before determining rotations, etc.).</p>
</td></tr>
<tr><td><code id="rigider_+3A_p0">p0</code></td>
<td>

<p>N by 2 matrix giving the coordinates for the 0-energy (observed) field.
</p>
</td></tr>
<tr><td><code id="rigider_+3A_init">init</code></td>
<td>

<p>(optional) numeric vector of length equal to the number of parameters (e.g., 2 for translation only, 3 for both, and 1 for rotation only).  If missing, then these will be estimated by taking the difference in centroids (translation) and the difference in orientation angles (rotation) as determined using image moments by way of <code>imomenter</code>.
</p>
</td></tr>
<tr><td><code id="rigider_+3A_theta">theta</code></td>
<td>
<p>numeric vector of length 1, 2 or 3 (depending on whether you want to translate only (2), rotate only (1) or both (3)) giving the rigid transformation parameters.</p>
</td></tr>
<tr><td><code id="rigider_+3A_type">type</code></td>
<td>
<p>character stating whether to optimize a loss function or just find the centroid (and possibly orientation angle) difference(s).</p>
</td></tr>
<tr><td><code id="rigider_+3A_translate">translate</code>, <code id="rigider_+3A_rotate">rotate</code></td>
<td>

<p>logical, should the optimal translation/rotation be found?
</p>
</td></tr>
<tr><td><code id="rigider_+3A_loss">loss</code></td>
<td>

<p>character naming a loss function (see details) to use in optimizing the rigid transformation (defaults to square error loss.
</p>
</td></tr>
<tr><td><code id="rigider_+3A_loss.args">loss.args</code></td>
<td>

<p>named list giving any optional arguments to <code>loss</code>.
</p>
</td></tr>
<tr><td><code id="rigider_+3A_interp">interp</code></td>
<td>

<p>character naming the 2-d interpolation method to use in calls to <code>Fint2d</code>.  Must be one of &ldquo;round&rdquo; (default), &ldquo;bilinear&rdquo; or &ldquo;bicubic&rdquo;.
</p>
</td></tr>
<tr><td><code id="rigider_+3A_stages">stages</code></td>
<td>

<p>logical.  Should the optimal translation be found before finding both the optimal tranlsation and rotation?
</p>
</td></tr>
<tr><td><code id="rigider_+3A_verbose">verbose</code></td>
<td>

<p>logical.  Should progress information be printed to the screen?
</p>
</td></tr>
<tr><td><code id="rigider_+3A_...">...</code></td>
<td>

<p>optional arguments to <code>nlminb</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A rigid transformation translates coordinates of values in a matrix and/or rotates them.  That is, if (r, s) are coordinates in a field with center (c1, c2), then the rigid transformation with parameters (x, y) and theta is given by:
</p>
<p>(r, s) + (x, y) + Phi ((r, s) - (c1, c2)),
</p>
<p>where Phi is the matrix with first column given by (cos( theta ), - sin( theta)) and second column given by (sin( theta), cos( theta )).
</p>
<p>The optimal transformation is found by way of numerical optimization using the <code>nlminb</code> function on the loss function given by <code>loss</code>.  If no value is given for <code>loss</code>, then square error loss is assumed.  In this case, the loss function is based on an assumption of Gaussian errors, but this assumption is only important if you try to make inferences based on this model, in which case you should probably think much harder about what you are doing.  In particular, the default objective function, Q, is given by:
</p>
<p>Q = - sum( ( F(W(s)) - O(s) )^2 / (2 * sigma^2) - (N / 2) * log( sigma^2 ),
</p>
<p>where s are the coordinates, W(s) are the rigidly transformed coordinates, F(W(s)) is the value of the 1-enegy field (forecast) evaluated at W(s) (which is interpolated as the translations typically do not give integer translations), O(s) is the 0-energy (observed) field evaluated at coordinate s, and sigma^2 is the estimated variance of the error field.  A good alternative is to use &ldquo;QcorrRigid&rdquo;, which calculates the correlation between F and O instead, and has been found by some to give better performance.
</p>
<p>The function <code>rigidTransform</code> performs a rigid transform for given parameter values.  It is intended as an internal function, but may be of use to some users.
</p>


<h3>Value</h3>

<p>A list object of class &ldquo;rigided&rdquo; is returned with components:
</p>
<table>
<tr><td><code>call</code></td>
<td>
<p>the function call.</p>
</td></tr>
<tr><td><code>translation.only</code></td>
<td>
<p>If stages argument is true, this part is the optimal translation before rotation.</p>
</td></tr>
<tr><td><code>rotate</code></td>
<td>
<p> optimal translation and rotation together, if stages argument is true.</p>
</td></tr>
<tr><td><code>initial</code></td>
<td>
<p>initial values used.</p>
</td></tr>
<tr><td><code>interp.method</code></td>
<td>
<p> same as input argument interp.</p>
</td></tr>
<tr><td><code>optim.args</code></td>
<td>
<p>optional arguments passed to nlminb.</p>
</td></tr>
<tr><td><code>loss</code>, <code>loss.args</code></td>
<td>
<p>same as input arguments.</p>
</td></tr>
<tr><td><code>par</code></td>
<td>
<p>optimal parameter values found.</p>
</td></tr>
<tr><td><code>value</code></td>
<td>
<p>value of loss function at optimal parameters.</p>
</td></tr>
<tr><td><code>x0</code>, <code>x1</code>, <code>p0</code></td>
<td>
<p>same as input arguments.</p>
</td></tr>
<tr><td><code>p1</code></td>
<td>
<p>transformed p0 coordinates.</p>
</td></tr>
<tr><td><code>x1.transformed</code></td>
<td>
<p>The field F(W(s)).</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Finding the optimal rigid transformation can be very tricky when applying both rotatons and translations.  This function helps, but for some fields may require more user input than is ideal, and should be considered experimental for the time being; as the examples will demonstrate.  It does seem to work well for translations only, which has been the recommended course of action for the CRA method.
</p>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+nlminb">nlminb</a></code>, <code><a href="#topic+Fint2d">Fint2d</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Simple uninteresting example for the R robots.
x &lt;- y &lt;- matrix(0, 20, 40)

x[ 12:18, 2:3 ] &lt;- 1

y[ 13:19, 5:6 ] &lt;- 1

xycoords &lt;- cbind(rep(1:20, 40), rep(1:40, each = 20))

tmp &lt;- rigider(x1 = x, x0 = y, p0 = xycoords)
tmp
plot(tmp)

# Rotate a coordinate system.
data( "geom000" )

loc &lt;- cbind(rep(1:601, 501), rep(1:501, each = 601))

# Rotate the coordinates by pi / 4.
th &lt;- c(0, 0, pi / 4)
names(th) &lt;- c("x", "y", "rotation")
cen &lt;- colMeans(loc[ geom000 &gt; 0, ])
loc2 &lt;- rigidTransform(theta = th, p0 = loc, cen = cen)

geom101 &lt;- Fint2d(X = geom000, Ws = loc2, s = loc, method = "round")

## Not run: 

image.plot(geom101)

# Try to find the optimal rigid transformation.
# First, allow a translation as well as rotation.

tmp &lt;- rigider(x1 = geom101, x0 = geom000, p0 = loc,
    rotate = TRUE, verbose = TRUE)
tmp
plot(tmp)

# Now, only allow rotation, which does not work as
# well as one would hope.
tmp &lt;- rigider(x1 = geom101, x0 = geom000, p0 = loc,
    translate = FALSE, rotate = TRUE, verbose = TRUE)
tmp
plot(tmp)

# Using correlation.
tmp &lt;- rigider(x1 = geom101, x0 = geom000, p0 = loc,
    rotate = TRUE, loss = "QcorrRigid", verbose = TRUE)
tmp
summary(tmp)
plot(tmp)

##
## Examples from ICP phase 1.
##
## Geometric cases.
##

data( "geom001" )
data( "geom002" )
data( "geom003" )
data( "geom004" )
data( "geom005" )


tmp &lt;- rigider(x1 = geom001, x0 = geom000, p0 = loc, verbose = TRUE)
tmp
plot(tmp)

tmp &lt;- rigider(x1 = geom002, x0 = geom000, p0 = loc, verbose = TRUE)
tmp
plot(tmp)

tmp &lt;- rigider(x1 = geom003, x0 = geom000, p0 = loc, verbose = TRUE)
tmp
plot(tmp)

tmp &lt;- rigider(x1 = geom004, x0 = geom000, p0 = loc, verbose = TRUE)
tmp
plot(tmp)

# Note: Above is a scale error rather than a rotation, but can we
# approximate it with a rotation?
tmp &lt;- rigider(x1 = geom004, x0 = geom000, p0 = loc, rotate = TRUE,
    verbose = TRUE)
tmp
plot(tmp)
# Ok, maybe need to give it better starting values?  Or, run it again
# with just the translation.

tmp &lt;- rigider(x1 = geom005, x0 = geom000, p0 = loc, verbose = TRUE)
tmp
plot(tmp)



## End(Not run)
</code></pre>

<hr>
<h2 id='S1'>
S1 Score, Anomaly Correlation
</h2><span id='topic+S1'></span><span id='topic+S1.default'></span><span id='topic+S1.SpatialVx'></span><span id='topic+ACC'></span><span id='topic+ACC.default'></span><span id='topic+ACC.SpatialVx'></span>

<h3>Description</h3>

<p>Calculate the S1 score and anomaly correlation for a verification set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>S1(x, ...)

## Default S3 method:
S1(x, ..., xhat, gradFUN = "KernelGradFUN")

## S3 method for class 'SpatialVx'
S1(x, ..., xhat, gradFUN = "KernelGradFUN",
    time.point = 1, obs = 1, model = 1)

ACC(x, ...)

## Default S3 method:
ACC(x, ..., xhat, xclim = NULL, xhatclim = NULL)

## S3 method for class 'SpatialVx'
ACC(x, ..., xclim = NULL, xhatclim = NULL,
    time.point = 1, obs = 1, model = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="S1_+3A_x">x</code>, <code id="S1_+3A_xhat">xhat</code></td>
<td>

<p>m by n matrices giving the verification and forecast fields, resp.
</p>
<p>For <code>S1.SpatialVx</code> and <code>ACC.SpatialVx</code>, <code>x</code> is an object of class &ldquo;SpatialVx&rdquo;.
</p>
</td></tr>
<tr><td><code id="S1_+3A_xclim">xclim</code>, <code id="S1_+3A_xhatclim">xhatclim</code></td>
<td>
<p>m by n matrices giving the climatologies for <code>X</code> and <code>Y</code>, resp.  If NULL, the result is simply a usual correlation.</p>
</td></tr>
<tr><td><code id="S1_+3A_gradfun">gradFUN</code></td>
<td>

<p>character identifying a function used to calculate the gradient fields for <code>X</code> and <code>Y</code>.  The default <code>KernelGradFUN</code> is to use a Laplacian of Gaussian kernel.
</p>
</td></tr>
<tr><td><code id="S1_+3A_time.point">time.point</code></td>
<td>
<p>numeric or character indicating which time point from the &ldquo;SpatialVx&rdquo; verification set to select for analysis.</p>
</td></tr>
<tr><td><code id="S1_+3A_obs">obs</code>, <code id="S1_+3A_model">model</code></td>
<td>
<p>numeric indicating which observation/forecast model to select for the analysis.</p>
</td></tr>
<tr><td><code id="S1_+3A_...">...</code></td>
<td>

<p>optional arguments to the <code>gradFUN</code> function.  In the case of the default, the kernel can be changed (e.g., if only &ldquo;laplacian&rdquo; is desired), and optional arguments to the <code>kernel2dmeitsjer</code> function (in this case, <code>nx</code>, <code>ny</code> and <code>sigma</code>).  Not used by <code>ACC</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The S1 score is given by
</p>
<p>S1 = 100*sum(abs(DY_i - DX_i))/sum(max(abs(DY_i),abs(DX_i))),
</p>
<p>where DY_i (DX_i)is the gradient at grid point i for the forecast (verification).  See Brown et al. (2012) and Thompson and Carter (1972) for more on this score.
</p>
<p>The ACC is just the correlation between X - Xclim and Y - Yclim.
</p>


<h3>Value</h3>

<p>single numeric
</p>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>References</h3>

<p>Brown, B.G., Gilleland, E. and Ebert, E.E. (2012) Chapter 6: Forecasts of spatial fields. pp. 95&ndash;117, In <em>Forecast Verification: A Practitioner's Guide in Atmospheric Science</em>, 2nd edition. Edts. Jolliffee, I. T. and Stephenson, D. B., Chichester, West Sussex, U.K.: Wiley, 274 pp.
</p>
<p>Thompson, J. C. and Carter, G. M. (1972) On some characteristics of the S1 score.  <em>J. Appl. Meteorol.</em>, <b>11</b>, 1384&ndash;1385.
</p>


<h3>See Also</h3>

<p><code><a href="smoothie.html#topic+kernel2dmeitsjer">kernel2dmeitsjer</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data( "UKobs6" )
data( "UKfcst6" )

S1( UKobs6, xhat = UKfcst6 )
ACC( UKobs6, xhat = UKfcst6 )

## Not run: 
data( "obs0601" )
data( "wrf4ncar0531" )
data( "ICPg240Locs" )

hold &lt;- make.SpatialVx( obs0601, wrf4ncar0531, loc = ICPg240Locs,
    projection = TRUE, map = TRUE, loc.byrow = TRUE,
    field.type = "Precipitation", units = "mm/h",
    data.name = "ICP NSSL/SPC Spring 2005 Cases",
    obs.name = "obs0601", model.name = "wrf4ncar0531" )

plot( hold )

S1( hold )
ACC( hold )

## End(Not run)
</code></pre>

<hr>
<h2 id='saller'>
Feature-based Analysis of a Field (Image)
</h2><span id='topic+saller'></span><span id='topic+print.saller'></span><span id='topic+summary.saller'></span>

<h3>Description</h3>

<p>Feature-based analysis of a field (image) 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>saller(x, d = NULL, distfun = "rdist", ...)

## S3 method for class 'saller'
print(x, ...)

## S3 method for class 'saller'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="saller_+3A_x">x</code></td>
<td>

<p><code>saller</code>: <code>x</code> is a list object returned by <code>FeatureFinder</code> or other feature identification function that returns a list with components <code>X.feats</code>, <code>Y.feats</code> (themselves lists with owin class objects defining separate features in the verification and forecast fields, resp.), and <code>X.labeled</code>, <code>Y.labeled</code> (fields with the numbers from 0 to the number of features also defining the separate feature locations (e.g., as returned by the <code>connected</code> function of package <span class="pkg">spatstat</span>.
</p>
<p><code>print</code>: list object returned by <code>saller</code>.
</p>
</td></tr>
<tr><td><code id="saller_+3A_object">object</code></td>
<td>

<p><code>summary</code>: object the returned by <code>saller</code>.
</p>
</td></tr>
<tr><td><code id="saller_+3A_d">d</code></td>
<td>

<p>(optional) the SAL (<code>saller</code>) method requires division by the longest distance between two border points.  If NULL, this is taken to be simply the length of the longest side.
</p>
</td></tr>
<tr><td><code id="saller_+3A_distfun">distfun</code></td>
<td>
<p>Function with which to calculate centroid distances.  Default uses straight Euclidean.  To do great-circle distance, use <code>rdist.earth</code> and be sure that <code>object</code> has a loc attribute with lon/lat coordinates.</p>
</td></tr>
<tr><td><code id="saller_+3A_...">...</code></td>
<td>
<p>Optional arguments to <code>distfun</code>.  Not used by <code>print</code> or <code>summary</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>saller: Computes S, A, and L of the SAL method introduced by Wernli et al. (2008).
</p>


<h3>Value</h3>

<p>saller returns a list with components:
</p>
<table>
<tr><td><code>A</code></td>
<td>
<p>numeric giving the amplitude component.</p>
</td></tr>
<tr><td><code>L</code></td>
<td>
<p>numeric giving the lcoation component.</p>
</td></tr>
<tr><td><code>S</code></td>
<td>
<p>numeric giving the structure component.</p>
</td></tr>
<tr><td><code>L1</code>, <code>L2</code></td>
<td>
<p>numeric giving the values that sum together to give L.</p>
</td></tr>
<tr><td><code>L1.alt</code>, <code>L.alt</code></td>
<td>
<p>numeric giving an alternative L1 component, and subsequently alternative L where it is calculated using the centroid of the field containing only defined features rather than the original raw field.</p>
</td></tr>
</table>
<p>print invisibly returns a named vector with S, A and L.
</p>
<p>summary does not return anything.
</p>


<h3>Note</h3>

<p>There are several ways to identify features, and some are provided by this package, but only a few.  For example, the method for identifying features in the SAL method as introduced by Wernli et al. (2008) utilizes information from a contour field of a particular variable, and is therefore not currently included in this package.  Users are encouraged to write their own such functions, and should feel free to contribute them to this package by contacting the maintainer.
</p>
<p>The SAL method typically looks at a small domain, and it is up to the user to set this up before calling these functions, as they are not designed to handle such a situation.
</p>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>References</h3>

<p>Wernli, H., Paulat, M., Hagen, M. and Frei, C. (2008) SAL&ndash;A novel quality measure for the verification of quantitative precipitation forecasts.  <em>Mon. Wea. Rev.</em>, <b>136</b>, 4470&ndash;4487, doi:10.1175/2008MWR2415.1.
</p>


<h3>See Also</h3>

<p><code><a href="spatstat.geom.html#topic+centroid.owin">centroid.owin</a></code>, <code><a href="spatstat.geom.html#topic+connected">connected</a></code>, <code><a href="spatstat.geom.html#topic+tiles">tiles</a></code>, <code><a href="spatstat.geom.html#topic+tess">tess</a></code>, <code><a href="#topic+deltamm">deltamm</a></code>, <code><a href="#topic+make.SpatialVx">make.SpatialVx</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data( "ExampleSpatialVxSet" )

x &lt;- ExampleSpatialVxSet$vx
xhat &lt;- ExampleSpatialVxSet$fcst

q &lt;- mean( c(c(x[x&gt;0]),c(xhat[xhat&gt;0])), na.rm=TRUE)

hold &lt;- make.SpatialVx( x, xhat, field.type="contrived", units="none",
    data.name = "Example", obs.name = "x", model.name = "xhat" )

hold2 &lt;- FeatureFinder(hold, smoothpar=5, thresh=q)
## Not run: plot(hold2)

look &lt;- saller(hold2)   
summary(look)


</code></pre>

<hr>
<h2 id='Sindex'>
Shape Index
</h2><span id='topic+Sindex'></span><span id='topic+Sindex.default'></span><span id='topic+Sindex.SpatialVx'></span>

<h3>Description</h3>

<p>Calculate the shape index (Sindex) as described in AghaKouchak et al. (2011)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Sindex(x, thresh = NULL, ...)

## Default S3 method:
Sindex(x, thresh = NULL, ...,
    loc = NULL)

## S3 method for class 'SpatialVx'
Sindex(x, thresh = NULL, ...,
    time.point = 1, obs = 1, model = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Sindex_+3A_x">x</code></td>
<td>

<p>Default: m by n numeric matrix giving the field for which the shape index is to be calculated.
</p>
<p><code>Sindex.SpatialVx</code>: list object of class &ldquo;SpatialVx&rdquo;.
</p>
</td></tr>
<tr><td><code id="Sindex_+3A_thresh">thresh</code></td>
<td>

<p>numeric giving a threshold under which (and including, i.e., &lt;=) all values are set to zero, and the shape index is calculated for the non-zero (positive-valued) grid-points.
</p>
</td></tr>
<tr><td><code id="Sindex_+3A_loc">loc</code></td>
<td>

<p>(optional) mn by 2 numeric matrix giving the grid point locations.  If NULL, the expanded grid with x=1:m and y=1:n is used.
</p>
</td></tr>
<tr><td><code id="Sindex_+3A_time.point">time.point</code></td>
<td>
<p>numeric or character indicating which time point from the &ldquo;SpatialVx&rdquo; verification set to select for analysis.</p>
</td></tr>
<tr><td><code id="Sindex_+3A_obs">obs</code>, <code id="Sindex_+3A_model">model</code></td>
<td>
<p>numeric indicating which observation/forecast model to select for the analysis.</p>
</td></tr>
<tr><td><code id="Sindex_+3A_...">...</code></td>
<td>
<p>Not used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The shape index introduced in AghaKouchak et al. (2011) is defined as
</p>
<p>Sindex = Pmin/P,
</p>
<p>where for n = the number of positive-valued grid points, Pmin = 4*sqrt(n) if floor(sqrt(n)) = sqrt(n), and Pmin = 2 * floor(2*sqrt(n)+1) otherwise.  P is the permieter of the non-zero grid points.  Range is 0 to 1.  Values closer to 1 indicate shapes that are closer to circular.
</p>


<h3>Value</h3>

<p>numeric with named components:
</p>
<table>
<tr><td><code>Sindex</code></td>
<td>
<p>the shape index</p>
</td></tr>
<tr><td><code>Pmin</code>, <code>P</code></td>
<td>
<p>the numerator and denominator (perimeter) that make the Sindex.</p>
</td></tr>
</table>
<p>For &ldquo;SpatialVx&rdquo; objects, the routine is applied to both the verification and forecast objects so that a two-row matrix is returned containing the above vectors for each field.
</p>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>References</h3>

<p>AghaKouchak, A., Nasrohllahi, N., Li, J., Imam, B. and Sorooshian, S. (2011) Geometrical characterization of precipitation patterns.  <em>J. Hyrdometeorology</em>, <b>12</b>, 274&ndash;285, doi:10.1175/2010JHM1298.1.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Cindex">Cindex</a></code>, <code><a href="#topic+Aindex">Aindex</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Re-create Fig. 7a from AghaKouchak et al. (2011).
tmp &lt;- matrix(0, 8, 8)
tmp[3,2:4] &lt;- 1
tmp[5,4:6] &lt;- 1
tmp[7,6:7] &lt;- 1
Sindex(tmp)


</code></pre>

<hr>
<h2 id='spatbiasFS'>
Field Significance Method of Elmore et al. (2006)
</h2><span id='topic+spatbiasFS'></span><span id='topic+plot.spatbiasFS'></span><span id='topic+summary.spatbiasFS'></span>

<h3>Description</h3>

<p>Apply field significance method of Elmore et al. (2006).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spatbiasFS(X, Y, loc = NULL, block.length = NULL, alpha.boot = 0.05, field.sig = 0.05,
    bootR = 1000, ntrials = 1000, verbose = FALSE)

## S3 method for class 'spatbiasFS'
summary(object, ...)

## S3 method for class 'spatbiasFS'
plot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="spatbiasFS_+3A_x">X</code>, <code id="spatbiasFS_+3A_y">Y</code></td>
<td>

<p>m by n matrices giving the verification and forecast fields, resp., for each of m time points (rows) and n locations (columns).
</p>
</td></tr>
<tr><td><code id="spatbiasFS_+3A_x">x</code>, <code id="spatbiasFS_+3A_object">object</code></td>
<td>
<p>list object as returned by <code>spatbiasFS</code>.</p>
</td></tr>
<tr><td><code id="spatbiasFS_+3A_loc">loc</code></td>
<td>

<p>optional (for subsequent plotting) n by 2 matrix giving the lon/lat coordinates for the locations.
</p>
</td></tr>
<tr><td><code id="spatbiasFS_+3A_block.length">block.length</code></td>
<td>

<p>numeric giving the block length to be used n the block bootstrap algorithm.  If NULL, floor(sqrt(n)) is used.
</p>
</td></tr>
<tr><td><code id="spatbiasFS_+3A_alpha.boot">alpha.boot</code></td>
<td>

<p>numeric between 0 and 1 giving the confidence level desired for the bootstrap algorithm.
</p>
</td></tr>
<tr><td><code id="spatbiasFS_+3A_field.sig">field.sig</code></td>
<td>

<p>numeric between 0 and 1 giving the desired field significance level.
</p>
</td></tr>
<tr><td><code id="spatbiasFS_+3A_bootr">bootR</code></td>
<td>

<p>numeric integer giving the number of bootstrap replications to use.
</p>
</td></tr>
<tr><td><code id="spatbiasFS_+3A_ntrials">ntrials</code></td>
<td>

<p>numeric integer giving the number of Monte Carol iterations to use.
</p>
</td></tr>
<tr><td><code id="spatbiasFS_+3A_verbose">verbose</code></td>
<td>

<p>logical, should progress information be printed to the screen?
</p>
</td></tr>
<tr><td><code id="spatbiasFS_+3A_...">...</code></td>
<td>
<p>not used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See Elmore et al. (2006) for details.
</p>


<h3>Value</h3>

<p>A list object with components:
</p>
<table>
<tr><td><code>data.name</code></td>
<td>
<p>character vector giving the name of the verification and forecast spatio-temporal fields used, and the associated location object (if not NULL).</p>
</td></tr>
<tr><td><code>block.boot.results</code></td>
<td>
<p>object of class LocSig</p>
</td></tr>
<tr><td><code>sig.results</code></td>
<td>
<p>list object containing information about the significance of the results.</p>
</td></tr>
<tr><td><code>field.significance</code>, <code>alpha.boot</code></td>
<td>
<p>field significance level and bootstrap CI level as input by field.sig alpha.boot arguments.</p>
</td></tr>
<tr><td><code>bootR</code>, <code>ntrials</code></td>
<td>
<p>same as arguments above.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Eric Gilleland and Kimberly L. Elmore
</p>


<h3>References</h3>

<p>Elmore, K. L., Baldwin, M. E. and Schultz, D. M. (2006) Field significance revisited: Spatial bias errors in forecasts as applied to the Eta model.  <em>Mon. Wea. Rev.</em>, <b>134</b>, 519&ndash;531.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+MCdof">MCdof</a></code>, <code><a href="#topic+LocSig">LocSig</a></code>, <code><a href="boot.html#topic+tsboot">tsboot</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(GFSNAMfcstEx)
data(GFSNAMobsEx)
data(GFSNAMlocEx)
id &lt;- GFSNAMlocEx[,"Lon"] &gt;=-95 &amp; GFSNAMlocEx[,"Lon"] &lt;= -75 &amp; GFSNAMlocEx[,"Lat"] &lt;= 32
loc &lt;- GFSNAMlocEx[id,]
GFSobsSub &lt;- GFSNAMobsEx[,id]
GFSfcstSub &lt;- GFSNAMfcstEx[,id]
look &lt;- spatbiasFS(GFSobsSub, GFSfcstSub, loc=loc, bootR=500, ntrials=500)
plot(look)
summary(look)
</code></pre>

<hr>
<h2 id='SpatialVx-internal'>
Internal functions for the SpatialVx package
</h2><span id='topic+print.G2IL'></span><span id='topic+print.Gbeta'></span><span id='topic+print.GbetaIL'></span><span id='topic+print.TheBigG'></span><span id='topic+ubalancer'></span><span id='topic+calculate_FSS_from_enlarged_summed_fields'></span><span id='topic+calculate_FSSvector_from_enlarged_summed_fields'></span><span id='topic+calculate_fractions_from_enlarged_summed_field'></span><span id='topic+calculate_n05_using_bisection_from_the_summed_fields'></span><span id='topic+calculate_summed_field'></span><span id='topic+enlarge_matrix'></span><span id='topic+datagrabber'></span><span id='topic+datagrabber.SpatialVx'></span><span id='topic+datagrabber.features'></span><span id='topic+datagrabber.matched'></span><span id='topic+deltammSqCen'></span><span id='topic+deltammOrig'></span><span id='topic+FeatureMatchAnalyzer.matched'></span><span id='topic+MergeIdentifier'></span><span id='topic+hoods2dPrep'></span><span id='topic+hoods2dSetUpLists'></span><span id='topic+KernelGradFUN'></span><span id='topic+locmeasures2dPrep'></span><span id='topic+LocListSetup'></span><span id='topic+MakeClusterList'></span><span id='topic+makeEmptyWave'></span><span id='topic+makeWaveNames'></span><span id='topic+ORSS'></span><span id='topic+QlossRigid'></span><span id='topic+QcorrRigid'></span><span id='topic+inside'></span><span id='topic+CI.fun'></span><span id='topic+sig.coverage'></span><span id='topic+is.sig'></span><span id='topic+detailer'></span><span id='topic+energizer'></span><span id='topic+variogram.matrix'></span><span id='topic+initGMM'></span><span id='topic+gmmEMstep'></span><span id='topic+gmmNegLogLik'></span><span id='topic+grads1'></span><span id='topic+grads2'></span><span id='topic+plot.matchedMap'></span><span id='topic+plot.matchedNoMap'></span><span id='topic+plot.saller'></span><span id='topic+plot.SpatialVxMap'></span><span id='topic+plot.SpatialVxNoMap'></span><span id='topic+plot.variographied'></span><span id='topic+plot.wavePurifyVxFieldsMap'></span><span id='topic+plot.wavePurifyVxFieldsNoMap'></span><span id='topic+plot.wavePurifyVxStats'></span><span id='topic+print.summary.interester'></span><span id='topic+print.variographied'></span><span id='topic+shrinkgrid'></span><span id='topic+Q'></span><span id='topic+Qgauss'></span><span id='topic+Qnonzero'></span><span id='topic+dF'></span><span id='topic+plot.iwarped'></span><span id='topic+plot.warped'></span><span id='topic+plot.warped.lonlat'></span><span id='topic+print.warped'></span><span id='topic+prinwarp'></span><span id='topic+summary.iwarped'></span><span id='topic+summary.warped'></span><span id='topic+warpTps'></span><span id='topic+warpTpsConverter'></span><span id='topic+warpTpsMatrices'></span><span id='topic+warpingRadialBasis'></span><span id='topic+equal.axis'></span>

<h3>Description</h3>

<p>Not to be called by the user.
</p>

<hr>
<h2 id='spct'>
Spatial Prediction Comparison Test
</h2><span id='topic+spct'></span>

<h3>Description</h3>

<p>Spatial Prediction Comparison Test (SPCT) for spatial locations that are on a regular or irregular coordinate system.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spct(d, loc, trend = 0, lon.lat = TRUE,
    dmax = NULL, vgmodel = "expvgram", vgmodel.args = NULL,
    init, alpha = 0.05, alternative = c("two.sided", "less", "greater"), mu = 0,
    verbose = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="spct_+3A_d">d</code></td>
<td>

<p>numeric vector of length n giving the (spatial) loss differential field (at a single point in time).
</p>
</td></tr>
<tr><td><code id="spct_+3A_loc">loc</code></td>
<td>

<p>n by 2 numeric matrix giving the spatial coordinates for each data point in <code>d</code>.
</p>
</td></tr>
<tr><td><code id="spct_+3A_trend">trend</code></td>
<td>

<p>a numeric vector of length one or n to be subtracted from d before finding the variogram and perfomring the test.
</p>
</td></tr>
<tr><td><code id="spct_+3A_lon.lat">lon.lat</code></td>
<td>

<p>logical stating whether or not the values in <code>loc</code> are longitude/latitude coordinates or not.  If TRUE, then the <span class="pkg">fields</span> function <code>rdist.earth</code> is used to calculate distances.  If FALSE, then the <span class="pkg">fields</span> function <code>rdist</code> is used.
</p>
</td></tr>
<tr><td><code id="spct_+3A_dmax">dmax</code></td>
<td>

<p>single numeric giving the maximum lag distance over which to fit the parametric variogram model.  The default uses half of the maximum lag.
</p>
</td></tr>
<tr><td><code id="spct_+3A_vgmodel">vgmodel</code></td>
<td>

<p>character string naming a function defining the parametric variogram model to be used.  The default uses <code>expvgram</code>, the exponential variogram model.  Must have arguments <code>p</code> (vector of parameters), <code>h</code> (vector of distances) and <code>...</code>.
</p>
</td></tr>
<tr><td><code id="spct_+3A_vgmodel.args">vgmodel.args</code></td>
<td>

<p>Optional list of other arguments to be passed to <code>vgmodel</code>.  Not used by the default method.
</p>
</td></tr>
<tr><td><code id="spct_+3A_init">init</code></td>
<td>

<p>Initial parameter values to be used in the call to <code>nlminb</code> for estimating the parameters of the variogram model.  The default for the default exponential variogram is to use the square root of the first-lag of the empirical variogram for the nugget and the difference between the second and first lag variogram values (if the second lag term is positive), and the first lag term otherwise for the range parameter.
</p>
</td></tr>
<tr><td><code id="spct_+3A_alpha">alpha</code></td>
<td>

<p>single numeric giving the desired level of significance.
</p>
</td></tr>
<tr><td><code id="spct_+3A_alternative">alternative</code></td>
<td>

<p>character string naming which type of hypothesis test to conduct.  Default is to do a two-sided test.  Note that the SPCT is paired test.
</p>
</td></tr>
<tr><td><code id="spct_+3A_mu">mu</code></td>
<td>

<p>The mean loss differential value under the null hypothesis.  Usually, this will be zero (the default value).
</p>
</td></tr>
<tr><td><code id="spct_+3A_verbose">verbose</code></td>
<td>

<p>logical, should progress information be printed to the screen?  It may also provide other useful information in the event that a problem occurs somewhere.
</p>
</td></tr>
<tr><td><code id="spct_+3A_...">...</code></td>
<td>

<p>Optional arguments to <code>vgram</code> from the <span class="pkg">fields</span> package.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If using a large spatial data set that occurs on a regular grid, you should probably use <code>lossdiff</code>, <code>empiricalVG.lossdiff</code>, <code>flossdiff</code> and <code>summary</code> to perform this self-same test (the SPCT), as those functions make use of special tricks for regular grids to speed things up.  Otherwise, this function should work on either type of grid.
</p>
<p>The SPCT is a paired test introduced by Hering and Genton (2011)&ndash;and based on the time series test introduced by Diebold and Mariano (1995) of whether one of two competing forecasts is better than the other (alternative) or not (null).  Apart from being a test for spatial fields, the SPCT test fits a parametric model to the empirical variogram (instead of using the empirical one), which turns out to be more accurate.
</p>
<p>The loss differential field is a field giving the straight difference between the two loss functions calculated for each of two forecasts.  For example, suppose Z(x,y) is an observed spatial field with (possibly irregularly spaced) locations (x, y), and Y1(x, y) and Y2(x, y) are two competing forecasts.  One might be interested in whether or not, on average, the difference in the absolute error for Y1 and Y2 is significantly different from zero.  First, g1 = abs( Y1(x, y) - Z(x, y) ) and g2 = abs( Y2(x, y) - Z(x, y) ).  Second, the loss differential field is D(x, y) = g1 - g2.  It is the average of D(x, y) that is of interest.  Because D(x, y) is likely to have a strong spatial correlation, the standard error for Dbar = mean( D(x, y) ) is calculated from the variogram.  Hering and Genton (2011) found the test to have proper size and good power, and found it to be relatively robust to contemporaneous correlation&ndash;i.e., if Y1 and Y2 are correlated (even if they are not, which is unlikely, g1 and g2 will necessarily be correlated because both involve the same field Z).
</p>
<p>If the sample size is less than 30, a t-test is used, and a normal approximation otherwise.
</p>
<p>See also, Gilleland (2013) for a modification of this test that accounts for location errors (coming soon).
</p>


<h3>Value</h3>

<p>A list object of class &ldquo;htest&rdquo; with components:
</p>
<table>
<tr><td><code>data.name</code></td>
<td>
<p>a character string giving the name of the loss differential field.</p>
</td></tr>
<tr><td><code>loss.differential</code></td>
<td>
<p>The original loss differential field as passed by argument d.</p>
</td></tr>
<tr><td><code>nloc</code></td>
<td>
<p>the number of spatial locations.</p>
</td></tr>
<tr><td><code>trend</code></td>
<td>
<p>Same as the argument passed in.</p>
</td></tr>
<tr><td><code>optional.arguments</code></td>
<td>
<p>list with any arguments passed into vgram.</p>
</td></tr>
<tr><td><code>empirical.variogram</code></td>
<td>
<p>the object returned by vgram giving the empirical variogram.</p>
</td></tr>
<tr><td><code>parametric.vgram.fit</code></td>
<td>
<p>the value returned by nlminb or an object of class &ldquo;try-error&rdquo;.</p>
</td></tr>
<tr><td><code>estimate</code></td>
<td>
<p>the estimated mean loss differential value.</p>
</td></tr>
<tr><td><code>se</code></td>
<td>
<p>the estimated standard error estimated from the fitted variogram model.</p>
</td></tr>
<tr><td><code>statistic</code></td>
<td>
<p>the value of the statistic ( mean( d ) - mu ) / se. </p>
</td></tr>
<tr><td><code>null.value</code></td>
<td>
<p>the argument mu.</p>
</td></tr>
<tr><td><code>parameter</code></td>
<td>
<p>numeric vector giving the parameter values estimated for the variogram model.</p>
</td></tr>
<tr><td><code>fitted.values</code></td>
<td>
<p>the predicted variogram values from the fitted parametric model.</p>
</td></tr>
<tr><td><code>loss.differential.detrended</code></td>
<td>
<p>this is the loss differential field after having been de-trended. </p>
</td></tr>
<tr><td><code>alternative</code></td>
<td>
<p>a character string describing the alternative hypothesis.</p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p>the p-value for the test.</p>
</td></tr>
<tr><td><code>conf.int</code></td>
<td>
<p>The (1 - alpha) * 100 percent confidence interval found using the standard error based on the variogram model per hering and Genton (2011).</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>a character string indicating the type of test performed.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>References</h3>

<p>Diebold, F.X. and Mariano, R.S. (1995) Comparing predictive accuracy. <em>Journal of Business and Economic Statistics</em>, <b>13</b>, 253&ndash;263.
</p>
<p>Gilleland, E. (2013) Testing competing precipitation forecasts accurately and efficiently: The spatial prediction comparison test.  <em>Mon. Wea. Rev.</em>, <b>141</b>, (1), 340&ndash;355.
</p>
<p>Hering, A. S. and Genton, M. G. (2011) Comparing spatial predictions.  <em>Technometrics</em> <b>53</b>, (4), 414&ndash;425.
</p>


<h3>See Also</h3>

<p><code><a href="fields.html#topic+vgram">vgram</a></code>, <code><a href="#topic+lossdiff">lossdiff</a></code>, <code><a href="#topic+flossdiff">flossdiff</a></code>, <code><a href="#topic+summary.lossdiff">summary.lossdiff</a></code>, <code><a href="#topic+expvgram">expvgram</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 
y1 &lt;- predict( Tps( fields::ozone$x, fields::ozone$y ) )
y2 &lt;- predict( Krig( fields::ozone$x, fields::ozone$y, theta = 20 ) )

y &lt;- fields::ozone$y

spct( abs( y1 - y ) - abs( y2 - y ), loc = fields::ozone$x )

spct( abs( y1 - y ) - abs( runif( 20, 1, 5 ) - y ), loc = fields::ozone$x )

## End(Not run)
</code></pre>

<hr>
<h2 id='structurogram'>
Structure Function for Non-Gridded Spatial Fields.
</h2><span id='topic+structurogram'></span><span id='topic+plot.structurogram'></span>

<h3>Description</h3>

<p>Computes pairwise differences (raised to the q-th power) as a function of distance.  Returns either raw values or statistics from binning.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>structurogram(loc, y, q = 2, id = NULL, d = NULL, lon.lat = FALSE, dmax = NULL,
    N = NULL, breaks = NULL)

## S3 method for class 'structurogram'
plot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="structurogram_+3A_loc">loc</code></td>
<td>

<p>numeric matrix where each row is the coordinate of a point in the field.
</p>
</td></tr>
<tr><td><code id="structurogram_+3A_x">x</code></td>
<td>
<p>list object returned by <code>structurogram</code> function.</p>
</td></tr>
<tr><td><code id="structurogram_+3A_y">y</code></td>
<td>

<p>numeric vector giving the value of the field at each location.
</p>
</td></tr>
<tr><td><code id="structurogram_+3A_q">q</code></td>
<td>

<p>numeric giving the value to which the paired differences should be raised.  Default (q=2) gives the usual semivariogram.
</p>
</td></tr>
<tr><td><code id="structurogram_+3A_id">id</code></td>
<td>

<p>A 2 column matrix that specifies which variogram differnces to find.  If omitted all possible pairings are found.  This can used if the data has an additional covariate that determines proximity, for example a time window.
</p>
</td></tr>
<tr><td><code id="structurogram_+3A_d">d</code></td>
<td>

<p>numeric matrix giving the distances among pairs (indexed by <code>id</code>).  If not included, these are determined directly from <code>loc</code>.
</p>
</td></tr>
<tr><td><code id="structurogram_+3A_lon.lat">lon.lat</code></td>
<td>

<p>logical, are the coordinates longitude/latitude coordinates?  If so, distances are found using great-circle distance.
</p>
</td></tr>
<tr><td><code id="structurogram_+3A_dmax">dmax</code></td>
<td>

<p>numeric giving the maximum distance for which to compute the structure function.
</p>
</td></tr>
<tr><td><code id="structurogram_+3A_n">N</code></td>
<td>

<p>numeric giving the number of bins to use.
</p>
</td></tr>
<tr><td><code id="structurogram_+3A_breaks">breaks</code></td>
<td>

<p>numeric vector giving bin boundaries for binning structure function values.  Need not be equally spaced, but must be ordered.
</p>
</td></tr>
<tr><td><code id="structurogram_+3A_...">...</code></td>
<td>
<p>optional arguments to plot function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is basically an exact copy of <code>vgram</code> from package <span class="pkg">fields</span> whereby the differences are raised to a power of q instead of 2.  That is, it calculates the structure function given by Eq (4) in harris et al. (2001).  Namely,
</p>
<p>S_q(l_x,l_y) = &lt;|R(x+l_x,y+l_y) - R(x,y)|^q&gt;
</p>
<p>where R is the field of interest, &lt;&gt; denotes the average over pixels in the image (note, in Harris et al. (2001), this is only over non-zero pixels, so is only equivalent to this equation if zero-valued points are first removed from y and loc), l_x and l_y are lags in the x and y directions, resp.  If q=2, then this is the semivariogram.
</p>
<p>The <code>plot</code> method function plots the structure by separation distance (circles) along with a dark blue line giving the bin centers.
</p>


<h3>Value</h3>

<p>A list object of class &ldquo;structurogram&rdquo; is returned with components:
</p>
<table>
<tr><td><code>d</code></td>
<td>
<p>numeric vector giving the pair-wise distances.</p>
</td></tr>
<tr><td><code>val</code></td>
<td>
<p>numeric vector giving the structure function values for each distance.</p>
</td></tr>
<tr><td><code>q</code></td>
<td>
<p>numeric giving the value of q passed into the function.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>Calling string</p>
</td></tr>
<tr><td><code>stats</code></td>
<td>
<p>Matrix of statistics for values in each bin.  Rows are the summaries returned by the stats function or describe (see package fields).  If either breaks or N arguments are not supplied then this component is not computed.</p>
</td></tr>
<tr><td><code>centers</code></td>
<td>
<p>numeric vector giving the bin centers.</p>
</td></tr>
</table>
<p>The plot method function does not return anything.
</p>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>References</h3>

<p>Harris, D., Foufoula-Georgiou, E., Droegemeier, K. K. and Levit, J. J. (2001)  Multiscale statistical properties of a high-resolution precipitation forecast.  <em>J. Hydrometeorol.</em>, <b>2</b>, 406&ndash;418.
</p>


<h3>See Also</h3>

<p><code><a href="fields.html#topic+vgram">vgram</a></code>, <code><a href="fields.html#topic+vgram.matrix">vgram.matrix</a></code>, <code><a href="#topic+structurogram.matrix">structurogram.matrix</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data( ozone2)
good&lt;- !is.na(ozone2$y[16,])
x&lt;- ozone2$lon.lat[good,] 
y&lt;- ozone2$y[16,good]
     
look &lt;- structurogram( x,y, N=15, lon.lat=TRUE)
plot(look)
# Compare above with results from example for function vgram from package fields.

look &lt;- structurogram( x,y, N=15, lon.lat=TRUE, q=1)
plot(look)

</code></pre>

<hr>
<h2 id='structurogram.matrix'>
Structure Function for Gridded Fields
</h2><span id='topic+structurogram.matrix'></span><span id='topic+plot.structurogram.matrix'></span>

<h3>Description</h3>

<p>Calculates the structure function to the q-th order for gridded fields.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>structurogram.matrix(dat, q = 2, R = 5, dx = 1, dy = 1, zero.out = FALSE)

## S3 method for class 'structurogram.matrix'
plot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="structurogram.matrix_+3A_dat">dat</code></td>
<td>

<p>n by m matrix of numeric values defining a gridded spatial field (or image) such that distances can be determined from their positions in the matrix.
</p>
</td></tr>
<tr><td><code id="structurogram.matrix_+3A_x">x</code></td>
<td>
<p>list object output from <code>structurogram.matrix</code></p>
</td></tr>
<tr><td><code id="structurogram.matrix_+3A_q">q</code></td>
<td>

<p>numeric giving the order for the structure function (q = 2 yields the more common semi-variogram).
</p>
</td></tr>
<tr><td><code id="structurogram.matrix_+3A_r">R</code></td>
<td>

<p>numeric giving the maximum radius for finding the structure differences assuming that the grid points are spaced one unit apart.  Default is to go to a radius of 5.
</p>
</td></tr>
<tr><td><code id="structurogram.matrix_+3A_dx">dx</code>, <code id="structurogram.matrix_+3A_dy">dy</code></td>
<td>

<p>numeric giving the spacing of the grid points on the x- (y-) axis.  This is used to calculate the correct distance between grid points.
</p>
</td></tr>
<tr><td><code id="structurogram.matrix_+3A_zero.out">zero.out</code></td>
<td>

<p>logical, should zero-valued pixels be ignored?
</p>
</td></tr>
<tr><td><code id="structurogram.matrix_+3A_...">...</code></td>
<td>
<p>optional arguments to the <code>plot</code> function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is basically an exact copy of <code>variogram.matrix</code>, which itself is a copy of <code>vgram.matrix</code> from package <span class="pkg">fields</span> (but allows and ignores missing values, in order to ignore zero-valued pixels and does not include Cressie's robust version of the variogram), whereby the differences are raised to a power of q instead of 2.  That is, it calculates the structure function given by Eq (4) in harris et al. (2001).  Namely,
</p>
<p>S_q(l_x,l_y) = &lt;|R(x+l_x,y+l_y) - R(x,y)|^q&gt;
</p>
<p>where R is the field of interest, &lt;&gt; denotes the average over pixels in the image (note, in Harris et al. (2001), this is only over non-zero pixels, so is only equivalent to this equation if zero.out=TRUE), l_x and l_y are lags in the x and y directions, resp.  If q=2, then this is the semivariogram.
</p>
<p>The <code>plot</code> method function makes two plots.  The first shows the structure by separation distance ignoring direction (circles) and all values (i.e., for each direction, dots).  The second shows the structure function values for separation distance and direction (see, e.g., <code>plot.vgram.matrix</code>).
</p>


<h3>Value</h3>

<p>A list with the following components:
</p>
<table>
<tr><td><code>d</code></td>
<td>
<p>numeric vector of distances for the differences (ignoring direction).</p>
</td></tr>
<tr><td><code>vgram</code></td>
<td>
<p>numeric vector giving the structure function values.  Note that the term 'vgram' is used here for compatibility with the plot.vgram.matrix function, which is employed by the plot method function used here.  This set of values ignores direction.</p>
</td></tr>
<tr><td><code>d.full</code></td>
<td>
<p>numeric vector of distances for all possible shifts up distance R.</p>
</td></tr>
<tr><td><code>ind</code></td>
<td>
<p>two column matrix giving the x- and y- increment used to compute shifts.</p>
</td></tr>
<tr><td><code>vgram.full</code></td>
<td>
<p>numeric vector giving the structure function for each direction in addition to separation distance.  Again, the word 'vgram' is used for compatibility with plot.vgram.matrix.</p>
</td></tr>
</table>
<p>Note that the plot method function does not return anything.
</p>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>References</h3>

<p>Harris, D., Foufoula-Georgiou, E., Droegemeier, K. K. and Levit, J. J. (2001)  Multiscale statistical properties of a high-resolution precipitation forecast.  <em>J. Hydrometeorol.</em>, <b>2</b>, 406&ndash;418.
</p>


<h3>See Also</h3>

<p><code><a href="fields.html#topic+vgram.matrix">vgram.matrix</a></code>, <code><a href="fields.html#topic+vgram">vgram</a></code>, <code><a href="#topic+structurogram">structurogram</a></code>, <code><a href="fields.html#topic+plot.vgram.matrix">plot.vgram.matrix</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data( "lennon" )
look &lt;- structurogram.matrix(lennon, q=2)
plot(look)
# Compare the above with
## Not run: 
look2 &lt;- vgram.matrix(lennon)
dev.new()
par(mfrow=c(1,2),bg="beige")
plot(look2$d, look2$vgram, xlab="separation distance", ylab="variogram")
points(look2$d.full, look2$vgram.full, pch=".")
plot.vgram.matrix(look2)

look &lt;- structurogram.matrix(lennon, q=1)
plot(look)

look &lt;- structurogram.matrix(lennon, q=1, zero.out=TRUE)
plot(look)

## End(Not run)
</code></pre>

<hr>
<h2 id='surrogater2d'>
Create Surrogate Fields
</h2><span id='topic+surrogater2d'></span><span id='topic+aaft2d'></span><span id='topic+fft2d'></span><span id='topic+mae'></span>

<h3>Description</h3>

<p>Create surrogate fields that have the same power spectrum and pdf as the original field.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>surrogater2d(Im, frac = 0.95, n = 10, lossfun = "mae", maxiter = 100, zero.down = TRUE,
    verbose = FALSE, ...)

aaft2d(Im, bigdim = NULL)

fft2d(x, bigdim = NULL, ...)

mae(x1, x2, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="surrogater2d_+3A_im">Im</code></td>
<td>

<p>matrix from which surrogates are to be made.
</p>
</td></tr>
<tr><td><code id="surrogater2d_+3A_x">x</code></td>
<td>
<p>matrix to be Fourier transformed.</p>
</td></tr>
<tr><td><code id="surrogater2d_+3A_x1">x1</code>, <code id="surrogater2d_+3A_x2">x2</code></td>
<td>
<p>numeric or array of same dimensions giving the two fields over which to calculate the mean aboslute error.</p>
</td></tr>
<tr><td><code id="surrogater2d_+3A_frac">frac</code></td>
<td>

<p>single numeric giving the fraction of original amplitudes to maintain.
</p>
</td></tr>
<tr><td><code id="surrogater2d_+3A_n">n</code></td>
<td>

<p>single numeric giving the number of surrogate fields to create (should be a whole number).
</p>
</td></tr>
<tr><td><code id="surrogater2d_+3A_lossfun">lossfun</code></td>
<td>

<p>character naming the loss function to use in computing the error between simulated surrogate fields in the iterative process.  Default is the mean absolute error given by the <code>mae</code> function detailed here.
</p>
</td></tr>
<tr><td><code id="surrogater2d_+3A_maxiter">maxiter</code></td>
<td>

<p>Maximum number of iterations allowed per surrogate.
</p>
</td></tr>
<tr><td><code id="surrogater2d_+3A_zero.down">zero.down</code></td>
<td>

<p>logical, does <code>Im</code> contain many zeros, and is otherwise positive?  If so, this sets negative numbers and unusually small numbers to zero.
</p>
</td></tr>
<tr><td><code id="surrogater2d_+3A_bigdim">bigdim</code></td>
<td>
<p>numeric vector of length two giving the dimensions (larger than dimensions of <code>Im</code>) to compute the FFT's more efficiently (at least potentially).</p>
</td></tr>
<tr><td><code id="surrogater2d_+3A_verbose">verbose</code></td>
<td>

<p>logical, should progress information be printed to the screen?
</p>
</td></tr>
<tr><td><code id="surrogater2d_+3A_...">...</code></td>
<td>

<p>additional arguments: in the case of <code>fft2d</code>, they are additional arguments to <code>fft</code> (i.e., to use inverse=TRUE), in the case of <code>surrogater2d</code>, they are additional arguments to the loss function given by <code>lossfun</code>, and in the case of <code>mae</code> (default), these are not used.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>fft2d</code> function was written to simplify some of the code in <code>surrogater2d</code> and <code>aaft2d</code>.  It is simply a call to the <span class="rlang"><b>R</b></span> function <code>fft</code>, but it first resets the dimensions to ones that should maximize the efficiency.  It will also return the dimensions if they are not passed in.
</p>
<p>Surrogates are used in non-linear time series analysis to simulate similar time series for hypothesis testing purposes (e.g., Kantz and Schreiber, 1997).  Venugopal et al. (2005) use surrogates of two-dimensional fields as part of their Forecast Quality Index (FQI); which is the intention here.  Theiler et al. (1992) proposed a method known as the amplitude adjusted Fourier transform (AAFT) algorithm, and Schreiber and Schmitz (1996) proposed a modification to this approach in order to obtain surrogates with both the same power spectrum and pdf as the original series.
</p>
<p>The AAFT method first renders the original data, denoted here as s_n, Gaussian via a rank ordering based on randomly generated Gaussian simulated data.  The resulting series, s_n'=g(s_n), is Gaussian and follows the same measured time evolution as s_n.  Next, phase randomized surrogates are made for s_n', call them s_n&quot;.  The rescaling g is then inverted by rank ordering s_n&quot; according to the distribution of the original data, s_n.  This algorithm yields surrogates with the same pdf of amplitudes as s_n by construction, but typically not the same power spectra.  The algorithm proposed by Schreiber and Schmitz (1996) begins with the AAFT, and then iterates through a further algorithm as follows.
</p>
<p>1. Hold a sorted list of s_n and the squared amplitudes of the Fourier transform of s_n, denote them by S2_k.
</p>
<p>2. Take a random shuffle without replacement of the data, denote as s_n(0).
</p>
<p>3. Take the Fourier transform of s_n(i).
</p>
<p>4. Replace the S2_k(i) with S2_k.
</p>
<p>5. Inverse the Fourier transform with the replaced amplitudes.
</p>
<p>6. Rank order the series from 5 in order to assume exactly the values taken by s_n.
</p>
<p>7. Check the accuracy of 6 using a loss function of some sort, and repeat steps 3 through 6 until a desired level of accuracy is achieved.
</p>


<h3>Value</h3>

<p>In the case of surrogater2d: A three dimesnional array of matrices with same dimension as Im, and third dimension giving the n surrogate fields.
</p>
<p>In the case of aaft2d: A matrix of the same dimension as Im.
</p>
<p>In the case of fft2d: If bigdim is NULL, a list object is returned with components fft and bigdim giving the FFT of x and the larger dimesnions used.  Otherwise, a matrix of dimension x is returned giving the FFT (or inverse FFT) of x.
</p>
<p>In the case of mae: a single numeric giving the mean absolute error between x1 and x2.
</p>


<h3>Author(s)</h3>

<p>Eric Gilleland, this code was adapted from matlab code written by Sukanta Basu (2007) available at: http://projects.ral.ucar.edu/icp/Software/FeaturesBased/FQI/Perturbed.m
</p>


<h3>References</h3>

<p>Kantz, H. and Schreiber, T. (1997) <em>Nonlinear time series analysis</em>.  Cambridge University Press, Cambridge, U.K., 304pp.
</p>
<p>Schreiber, T. and Schmitz, A. (1996) Improved surrogate data for nonlinearity tests.  <em>Physical Review Letters</em>, <b>77</b>(4), 635&ndash;638.
</p>
<p>Theiler, J., Eubank, S. Longtin, A. Galdrikian, B. and Farmer, J. D. (1992) <em>Physica</em> (Amsterdam) <b>58D</b>, 77.
</p>
<p>Venugopal, V., Basu, S. and Foufoula-Georgiou, E. (2005) A new metric for comparing precipitation patterns with an application to ensemble forecasts.  <em>J. Geophys. Res.</em>, <b>110</b>, D08111, doi:10.1029/2004JD005395, 11pp.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+fft">fft</a></code>, <code><a href="#topic+locmeasures2d">locmeasures2d</a></code>, <code><a href="#topic+UIQI">UIQI</a></code>, <code><a href="#topic+ampstats">ampstats</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data( "ExampleSpatialVxSet" )

x &lt;- ExampleSpatialVxSet$vx

z &lt;- surrogater2d( x, zero.down=FALSE, n=3)

## Not run: 
par( mfrow=c(2,2))
image.plot( look)
image.plot( look2[,,1])
image.plot( look2[,,2])
image.plot( look2[,,3])

  
## End(Not run)
</code></pre>

<hr>
<h2 id='TheBigG'>
The Spatial Alignment Summary Measure Called G
</h2><span id='topic+TheBigG'></span>

<h3>Description</h3>

<p>The spatial alignment summary measure, G, is a summary comparison for two gridded binary fields.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>TheBigG(X, Xhat, threshold, rule = "&gt;", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="TheBigG_+3A_x">X</code>, <code id="TheBigG_+3A_xhat">Xhat</code></td>
<td>

<p>m by n matrices giving the &ldquo;observed&rdquo; and forecast fields, respectively.
</p>
</td></tr>
<tr><td><code id="TheBigG_+3A_threshold">threshold</code>, <code id="TheBigG_+3A_rule">rule</code></td>
<td>

<p>The threshold and rule arguments to the <code>binarizer</code> function.
</p>
</td></tr>
<tr><td><code id="TheBigG_+3A_...">...</code></td>
<td>

<p>Not used.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is an alternative version of Gbeta that does not require the user to select a parameter.  It is not informative about rare events relative to the domain size.  It is the cubed root of the product of two terms.  If A is the set of one-valued grid points in the binary version of <code>X</code> and B those for <code>Xhat</code>, then the first term is the size of the symmetric difference between A and B (i.e., an area with grid points squared as the units) and the second term is MED(A,B) * nB with MED(B,A) * nA, where MED is the mean-error distance and nA, nB are the numbers of grid points in each of A and B, respectively.  The second term has units of grid squares so that the product is units of grid squares cubed; hence, the reason for taking the cubed root for G.  The units for G are grid squares with zero being a perfect score and increasing scores imply worsening matches between the sets A and B.  See Gilleland (2021) for more details.
</p>


<h3>Value</h3>

<p>An object of class &ldquo;TheBigG&rdquo; is returned.  It is a single number giving the value of G but also has a list of attributes that can be accessed using the <code>attributes</code> function.  This list includes:
</p>
<table>
<tr><td><code>components</code></td>
<td>
<p>A vector giving: nA, nB, nAB (number of points in the intersection), number of points in the symmetric difference, MED(A,B), MED(B,A), MED(A,B) * nB, MED(B,A) * nA, followed by the asymmetric versions of G for G(A,B) and G(B,A).</p>
</td></tr>
<tr><td><code>threshold</code></td>
<td>
<p>If a threshold is provided, then this component gives the threshold and rule arguments used.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>References</h3>

<p>Gilleland, E. (2020) Novel measures for summarizing high-resolution forecast performance. <em>Advances in Statistical Climatology, Meteorology and Oceanography</em>, <b>7</b> (1), 13&ndash;34, doi: 10.5194/ascmo-7-13-2021.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Gbeta">Gbeta</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data( "obs0601" )
data( "wrf4ncar0531" )
res &lt;- TheBigG( X = obs0601, Xhat = wrf4ncar0531, threshold = 2.1 )
res
</code></pre>

<hr>
<h2 id='thresholder'>
Apply a Threshold to a Field
</h2><span id='topic+thresholder'></span><span id='topic+thresholder.default'></span><span id='topic+thresholder.SpatialVx'></span>

<h3>Description</h3>

<p>App;y a threshold to a field and return either a binary field or a field with replace.width everywhere the rule is not true.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>thresholder(x, type = c("binary", "replace.below"), th, rule = "&gt;=",
    replace.with = 0, ...)

## Default S3 method:
thresholder(x, type = c("binary", "replace.below"), th, rule = "&gt;=", 
    replace.with = 0, ... )

## S3 method for class 'SpatialVx'
thresholder(x, type = c("binary", "replace.below"), th, rule = "&gt;=",
    replace.with = 0, ..., time.point = 1, obs = 1, model = 1 )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="thresholder_+3A_x">x</code></td>
<td>

<p>A field or &ldquo;SpatialVx&rdquo; object to which to apply the thresholds.
</p>
</td></tr>
<tr><td><code id="thresholder_+3A_type">type</code></td>
<td>

<p>character describing which type of field(s) to return: binary or replace.
</p>
</td></tr>
<tr><td><code id="thresholder_+3A_rule">rule</code></td>
<td>

<p>If <code>type</code> is &ldquo;binary&rdquo;, return 0 when the rule applied to a grid point's value is not true in relation to the threshold value, and 1 elsewhere.  If <code>type</code> is &ldquo;replace.below&rdquo;, then return <code>replace.with</code> wherever the rule is not true and return the original value otherwise.  By default, it replaces values below the threshold with zero (hence its name), but if <code>rule</code> is, e.g., &ldquo;&lt;=&rdquo;, then it will replace values above with zero; or whatever value is chosen for <code>replace.with</code>.
</p>
</td></tr>
<tr><td><code id="thresholder_+3A_replace.with">replace.with</code></td>
<td>

<p>Only used if <code>type</code> is &ldquo;replace.below&rdquo;.  The value with which to replace values that are below (default) the threshold.
</p>
</td></tr>
<tr><td><code id="thresholder_+3A_th">th</code></td>
<td>
<p>Value of the threshold (default) or index to which row of threshold matrices in <code>thresholds</code> attribute of &ldquo;SpatialVx&rdquo; object.  Must be a single number.  </p>
</td></tr>
<tr><td><code id="thresholder_+3A_time.point">time.point</code></td>
<td>
<p>numeric or character indicating which time point from the &ldquo;SpatialVx&rdquo; verification set to select for analysis.</p>
</td></tr>
<tr><td><code id="thresholder_+3A_obs">obs</code>, <code id="thresholder_+3A_model">model</code></td>
<td>
<p>numeric indicating which observation/forecast model to select for the analysis.</p>
</td></tr>
<tr><td><code id="thresholder_+3A_...">...</code></td>
<td>
<p> Not used.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>At each point, p, in the field, the expression: p <code>rule</code> <code>threshold</code> is applied.  If <code>type</code> is &ldquo;binary&rdquo;, then if the expression is false, zero is returned for that grid point, and if it is true, then one is returned.  If <code>type</code> is &ldquo;replace.below&rdquo;, then if the expression is false, <code>replace.with</code> is returned for that grid point, and if true, then the original value is returned.  By default, the original field is returned, but with values below the threshold set to zero.  If <code>rule</code> is &ldquo;&lt;=&rdquo;, then <code>replace.below</code> will actually replace values above the threshold with &ldquo;replace.with&rdquo; instead.
</p>
<p>If applied to a &ldquo;SpatialVx&rdquo; class object, then observation <code>obs</code> and model <code>model</code> at time point <code>time.point</code> will each be thresholded using the respective <code>th</code> threshold value for the observed and modeled fields as taken from the thresholds attribute of the object (see the help file for <code>make.SpatialVx</code>).
</p>


<h3>Value</h3>

<p>A field of the same dimension as <code>x</code> if a matrix.  If <code>x</code> is a &ldquo;SpatialVx&rdquo; class object, then a list is returned with components:
</p>
<table>
<tr><td><code>X</code>, <code>Xhat</code></td>
<td>
<p>The matrices giving the respective thresholded fields for the observation and forecast.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>See Also</h3>

<p><code><a href="#topic+make.SpatialVx">make.SpatialVx</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
x &lt;- matrix( 12 + rnorm( 100, 10, 10 ), 10, 10 )

par( mfrow = c(2, 2) )
image.plot( thresholder( x, th = 12 ), main = "binary" )

image.plot( thresholder( x, type = "replace.below", th = 12 ),
    main = "replace.below" ) 

image.plot( thresholder( x, th = 12, rule = "&lt;=" ),
    main = "binary with rule &lt;=" )

image.plot( thresholder( x, type = "replace.below", th = 12, rule = "&lt;=" ),
    main = "replace.below with rule &lt;=" )

par( mfrow = c(1,1) )
## Not run: 
data("geom000")
data("geom004")
data("ICPg240Locs")

hold &lt;- make.SpatialVx( geom000, geom004, thresholds = c(0.01, 50.01),
    projection = TRUE, map = TRUE, loc = ICPg240Locs, loc.byrow = TRUE,
    field.type = "Geometric Objects Pretending to be Precipitation",
    units = "mm/h", data.name = "ICP Geometric Cases", obs.name = "geom000",
    model.name = "geom004" )

# Note: th = 1 means threshold = 0.01.
look &lt;- thresholder( hold, th = 1 )

image.plot( look$X )
contour( look$Xhat, add = TRUE, col = "white" )

# Note: th = 2, means threshold = 50.01
look &lt;- thresholder( hold, th = 2 )

image.plot( look$X )
contour( look$Xhat, add = TRUE, col = "white" )

look &lt;- thresholder( hold, th = 1, rule = "&lt;" )

image.plot( look$X )
contour( look$Xhat, add = TRUE, col = "white" )

## End(Not run)
</code></pre>

<hr>
<h2 id='UKobs6'>
Example Precipitation Rate Verification Set (NIMROD)
</h2><span id='topic+UKobs6'></span><span id='topic+UKfcst6'></span><span id='topic+UKloc'></span>

<h3>Description</h3>

<p>Example precipitation rate verification set from the very short-range mesoscale Numerical Weather Prediction (NWP) system used operationally at the UK Met Office.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(UKobs6)
	data(UKloc)</code></pre>


<h3>Format</h3>

<p>The format is:
chr &quot;UKobs6&quot;
</p>
<p>The format is:
num [1:65536, 1:2] -11 -10.9 -10.9 -10.8 -10.7 ...
</p>


<h3>Details</h3>

<p>Precipitation rate (mm/h) verification set from the very short-range NWP system called NIMROD used operationally at the UK Met Office, and described in detail in Casati et al. (2004).  In particular, this is case 6 from Casati et al. (2004), showing a front timing error.  These data are made available for scientific purposes only.  Please cite the source
in any papers or presentations.  The proper reference is the U.K. Met Office.
</p>
<p>Refer to Casati et al. (2004) for more information on these data.
</p>
<p>The original lon/lat information is not available.  'UKloc' was created to match reasonably well with the figures in Casati et al. (2004), but should not be considered definite.
</p>


<h3>Source</h3>

<p>UK Met Office
</p>


<h3>References</h3>

<p>Casati, B., Ross, G. and Stephenson, D. B. (2004) A new intensity-scale approach for the verification of spatial precipitation forecasts. <em>Meteorol. Appl.</em>, <b>11</b>, 141&ndash;154, doi:10.1017/S1350482704001239.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data( "UKobs6" )
data( "UKfcst6" )
data( "UKloc" )

zl &lt;- range(c(c(UKobs6), c(UKfcst6)))
par(mfrow=c(1,2))
image(UKobs6, col=c("grey", tim.colors(64)), zlim=zl, main="analysis", axes=FALSE)
par(usr=apply(UKloc, 2, range))
# map(add=TRUE) # from library( "maps" )
image.plot(UKfcst6, col=c("grey", tim.colors(64)), zlim=zl, main="forecast", axes=FALSE)
par(usr=apply(UKloc, 2, range))
# map(add=TRUE)
</code></pre>

<hr>
<h2 id='upscale2d'>
Upscaling Neighborhood Verification on a 2-d Verification Set
</h2><span id='topic+upscale2d'></span><span id='topic+plot.upscale2d'></span><span id='topic+print.upscale2d'></span>

<h3>Description</h3>

<p>Perform upscaling neighborhood verification on a 2-d verification set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>upscale2d(object, time.point = 1, obs = 1,
                 model = 1, levels = NULL, max.n = NULL, smooth.fun =
                 "hoods2dsmooth", smooth.params = NULL, rule = "&gt;=",
                 verbose = FALSE)

## S3 method for class 'upscale2d'
plot(x, ... )

## S3 method for class 'upscale2d'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="upscale2d_+3A_object">object</code></td>
<td>

<p>list object of class &ldquo;SpatialVx&rdquo;.
</p>
</td></tr>
<tr><td><code id="upscale2d_+3A_time.point">time.point</code></td>
<td>
<p>numeric or character indicating which time point from the &ldquo;SpatialVx&rdquo; verification set to select for analysis.</p>
</td></tr>
<tr><td><code id="upscale2d_+3A_obs">obs</code>, <code id="upscale2d_+3A_model">model</code></td>
<td>
<p>numeric indicating which observation/forecast model to select for the analysis.</p>
</td></tr>
<tr><td><code id="upscale2d_+3A_levels">levels</code></td>
<td>
<p>numeric vector giving the successive values of the smoothing parameter.  For example, for the default method, these are the neighborhood lengths over which the levels^2 nearest neighbors are averaged for each point.  Values should make sense for the specific smoothing function.  For example, for the default method, these should be odd integers.</p>
</td></tr>
<tr><td><code id="upscale2d_+3A_max.n">max.n</code></td>
<td>
<p>(optional) single numeric giving the maximum neighborhood length to use.  Only used if levels are NULL.</p>
</td></tr>
<tr><td><code id="upscale2d_+3A_smooth.fun">smooth.fun</code></td>
<td>
<p>character giving the name of a smoothing function to be applied.  Default is an average over the n^2 nearest neighbors, where n is taken to be each value of the <code>levels</code> argument.</p>
</td></tr>
<tr><td><code id="upscale2d_+3A_smooth.params">smooth.params</code></td>
<td>
<p>list object containing any optional arguments to <code>smooth.fun</code>.  Use NULL if none.</p>
</td></tr>
<tr><td><code id="upscale2d_+3A_rule">rule</code></td>
<td>
<p>character string giving the threshold rule to be applied.  See help file for <code>thresholder</code> function for more information.</p>
</td></tr>
<tr><td><code id="upscale2d_+3A_verbose">verbose</code></td>
<td>
<p>logical, should progress information be printed to the screen?</p>
</td></tr>
<tr><td><code id="upscale2d_+3A_x">x</code></td>
<td>
<p>list object of class &ldquo;upscale2d&rdquo; as returned by <code>upscale2d</code>.</p>
</td></tr>
<tr><td><code id="upscale2d_+3A_...">...</code></td>
<td>
<p>optional arguments to the <code>image.plot</code> function from package <span class="pkg">fields</span>.  Can also include the argument <code>type</code>, which must be one of &ldquo;all&rdquo;, &ldquo;gss&rdquo;, &ldquo;ts&rdquo;, &ldquo;bias&rdquo; or &ldquo;rmse&rdquo;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Upscaling is performed via neighborhood smoothing.  Here, a boxcar kernel is convolved (using the convolution theorem with FFT's) to obtain an average over the nearest n^2 grid squares at each grid point.  This is performed on the raw forecast and verification fields.  The root mean square error (RMSE) is taken for each threshold (Yates et al., 2006; Ebert, 2008).  Further, binary fields are obtained for each smoothed field via thresholding, and frequency bias, threat score ts) and equitable threat score (ets) are calculated (Zepeda-Arce et al., 2000; Ebert, 2008).
</p>


<h3>Value</h3>

<p><code>upscale2d</code> returns a list of class &ldquo;upscale2d&rdquo; with components: 
</p>
<table>
<tr><td><code>rmse</code></td>
<td>
<p>numeric vector giving the root mean square error for each neighborhood size provided by object.</p>
</td></tr>
<tr><td><code>bias</code>, <code>ts</code>, <code>ets</code></td>
<td>
<p>numeric matrices giving the frequency bias, ts and ets for each neighborhood size (rows) and threshold (columns).</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>References</h3>

<p>Ebert, E. E. (2008) Fuzzy verification of high resolution gridded forecasts: A review and proposed framework.  <em>Meteorol. Appl.</em>, <b>15</b>, 51&ndash;64. doi:10.1002/met.25
</p>
<p>Yates, E., Anquetin, S., Ducrocq, V., Creutin, J.-D., Ricard, D. and Chancibault, K. (2006) Point and areal validation of forecast precipitation fields.  <em>Meteorol. Appl.</em>, <b>13</b>, 1&ndash;20.
</p>
<p>Zepeda-Arce, J., Foufoula-Georgiou, E., Droegemeier, K. K. (2000) Space-time rainfall organization and its role in validating quantitative precipitation forecasts.  <em>J. Geophys. Res.</em>, <b>105</b>(D8), 10,129&ndash;10,146.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+hoods2d">hoods2d</a></code>, <code><a href="smoothie.html#topic+kernel2dsmooth">kernel2dsmooth</a></code>, <code><a href="smoothie.html#topic+kernel2dmeitsjer">kernel2dmeitsjer</a></code>, <code><a href="stats.html#topic+fft">fft</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- matrix( 0, 50, 50)
x[ sample(1:50,10), sample(1:50,10)] &lt;- rexp( 100, 0.25)
y &lt;- kernel2dsmooth( x, kernel.type="disk", r=6.5)
x &lt;- kernel2dsmooth( x, kernel.type="gauss", nx=50, ny=50, sigma=3.5)

hold &lt;- make.SpatialVx( x, y, thresholds = seq(0.01,1,,5), field.type = "random")

look &lt;- upscale2d( hold, levels=c(1, 3, 20) )
look

par( mfrow = c(4, 2 ) )
plot( look )

## Not run: 
data( "geom001" )
data( "geom000" )
data( "ICPg240Locs" )

hold &lt;- make.SpatialVx( geom000, geom001, thresholds = c(0.01, 50.01),
    loc = ICPg240Locs, projection = TRUE, map = TRUE, loc.byrow = TRUE,
    field.type = "Precipitation", units = "mm/h",
    data.name = "Geometric", obs.name = "geom000", model.name = "geom001" )

look &lt;- upscale2d(hold, levels=c(1, 3, 9, 17, 33, 65, 129, 257),
    verbose=TRUE)

par( mfrow = c(4, 2 ) )

plot(look )
look &lt;- upscale2d(hold, q.gt.zero=TRUE, verbose=TRUE)
plot(look)
look &lt;- upscale2d(hold, verbose=TRUE)
plot(look)


## End(Not run)
</code></pre>

<hr>
<h2 id='variographier'>
Variography Score
</h2><span id='topic+variographier'></span><span id='topic+variographier.default'></span><span id='topic+variographier.SpatialVx'></span>

<h3>Description</h3>

<p>Calculate the variography score between two spatial fields based on the fitted exponential variogram.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>variographier(x, init, zero.out = FALSE, ...)

## Default S3 method:
variographier( x, init, zero.out = FALSE, ..., y )

## S3 method for class 'SpatialVx'
variographier( x, init, zero.out = FALSE, ...,
    obs = 1, model = 1, time.point = 1 )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="variographier_+3A_x">x</code>, <code id="variographier_+3A_y">y</code></td>
<td>

<p>matrices giving the fields on which to calculate the variography or a &ldquo;SpatialVx&rdquo; class object (<code>x</code> only).
</p>
</td></tr>
<tr><td><code id="variographier_+3A_init">init</code></td>
<td>

<p>list with components <code>px</code> and <code>py</code> that give initial values for parameter estimates (sill + nugget and range).  If missing, default will attempt to find reasonable starting values.
</p>
</td></tr>
<tr><td><code id="variographier_+3A_zero.out">zero.out</code></td>
<td>

<p>logical should the variogram be calculated over all grid points or just ones where one or both fields are non-zero?  See <code>variogram.matrix</code>.
</p>
</td></tr>
<tr><td><code id="variographier_+3A_time.point">time.point</code></td>
<td>

<p>numeric or character indicating which time point from the &ldquo;SpatialVx&rdquo; verification set to select for analysis.
</p>
</td></tr>
<tr><td><code id="variographier_+3A_obs">obs</code>, <code id="variographier_+3A_model">model</code></td>
<td>

<p>numeric indicating which observation/forecast model to select for the analysis.
</p>
</td></tr>
<tr><td><code id="variographier_+3A_...">...</code></td>
<td>

<p>optional arguments to <code>vgram.matrix</code> or <code>variogram.matrix</code> (if zero.out is TRUE).  Can also have optional arguments to <code>nlminb</code> (but not <code>lower</code> or <code>upper</code>).
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The variography score calculated here is that from Ekstrom (2016).  So far, only the exponential variogram is allowed.
</p>
<p>Note that in the fitting, the model g(h) = c * ( 1 - exp( -a * h ) ) is used, but the variography is calculated for theta = 3 / a.  Therefore, the values in the par component of the returned fitted variograms correspond to a, while the variography score corresponds to theta.  The score is given by:
</p>
<p>v = 1 / sqrt( c_0^2 + c_m^2 + ( theta_0 - theta_m )^2 )
</p>
<p>where c_0 and c_m are the sill + nugget terms for the observation and model, resp., and similarly for theta_0 and theta_m.
</p>
<p>The parameters are *not* currently normalized, here, to give equal weight between sill + nugget and range.  If several fields are analyzed (e.g., an ensemble), then the fitted parameters could be gathered, and one could use that information to calculate the score based on a normalized version.
</p>


<h3>Value</h3>

<p>A list object of class &ldquo;variographied&rdquo; is returned with components:
</p>
<table>
<tr><td><code>obs.vg</code>, <code>mod.vg</code></td>
<td>
<p>Empirical variogram objects as returned by either vgram.matrix or variogram.matrix</p>
</td></tr>
<tr><td><code>obs.parvg</code>, <code>mod.parvg</code></td>
<td>
<p>objects returned by nlminb containing the fitted exponential variogram model parameters and some information about the optimization.  </p>
</td></tr>
<tr><td><code>variography</code></td>
<td>
<p> single numeric giving the variography measure.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>References</h3>

<p>Ekstrom, M. (2016) Metrics to identify meaningful downscaling skill in WRF simulations of intense rainfall events.  <em>Environmental Modelling and Software</em>, <b>79</b>, 267&ndash;284, DOI: 10.1016/j.envsoft.2016.01.012.
</p>


<h3>See Also</h3>

<p><code><a href="fields.html#topic+vgram.matrix">vgram.matrix</a></code>, <code><a href="#topic+variogram.matrix">variogram.matrix</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data( "UKobs6" )
data( "UKfcst6" )
data( "UKloc" )

hold &lt;- make.SpatialVx( UKobs6, UKfcst6, thresholds = c(0.01, 20.01),
    loc = UKloc, field.type = "Precipitation", units = "mm/h",
    data.name = "Nimrod", obs.name = "Observations 6", model.name = "Forecast 6",
    map = TRUE)

look &lt;- variographier( hold )
look
plot( look )
</code></pre>

<hr>
<h2 id='vxstats'>
Some Common Traditional Forecast Verification Statistics.
</h2><span id='topic+vxstats'></span>

<h3>Description</h3>

<p>Calculates some common traditional forecast verification statistics.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vxstats(X, Xhat, which.stats = c("bias", "ts", "ets", "pod",
                 "far", "f", "hk", "bcts", "bcets", "mse"), subset =
                 NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vxstats_+3A_x">X</code>, <code id="vxstats_+3A_xhat">Xhat</code></td>
<td>

<p>k by m matrix of verification and forecast values, resp.
</p>
</td></tr>
<tr><td><code id="vxstats_+3A_which.stats">which.stats</code></td>
<td>

<p>character vector giving the names of the desired statistics.  See Details below.
</p>
</td></tr>
<tr><td><code id="vxstats_+3A_subset">subset</code></td>
<td>
<p>numeric vector indicating a subset of the verification set over which to calculate the verification statistics.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Computes several traditional verification statistics (see Wilks, 2006, Ch. 7; Jolliffe and Stephenson, 2012 for more on these forecast verification statistics)
The possible statistics that can be computed, as determined by <code>which.stats</code> are:
</p>
<p>&ldquo;bias&rdquo; the number of forecast events divided by the number of observed events (sometimes called frequency bias).
</p>
<p>&ldquo;ts&rdquo; threat score, given by hits/(hits + misses + false alarms)
</p>
<p>&ldquo;ets&rdquo; equitable threat score, given by (hits - hits.random)/(hits + misses + false alarms - hits.random), where hits.random is the number of observed events times the number of forecast events divided by the total number of forecasts.
</p>
<p>&ldquo;pod&rdquo; probability of detecting an observed event (aka, hit rate).  It is given by hits/(hits + misses).
</p>
<p>&ldquo;far&rdquo; false alarm ratio, given by (false alarms)/(hits + false alarms).
</p>
<p>&ldquo;f&rdquo; false alarm rate (aka probability of false detection) is given by (false alarms)/(correct rejections + false alarms).
</p>
<p>&ldquo;hk&rdquo; Hanssen-Kuipers Score is given by the difference between the hit rate (&ldquo;pod&rdquo;) and the false alarm rate (&ldquo;f&rdquo;).
</p>
<p>&ldquo;bcts&rdquo;, &ldquo;bcets&rdquo;, Bias Corrected Threat Score (Equitable Threat Score) as introduced in Mesinger (2008); see also Brill and Mesinger (2009).  Also referred to as the dHdA versions of these scores.
</p>
<p>&ldquo;mse&rdquo; mean square error (not a contingency table statistic, but can be used with binary fields).  This is the only statistic that can be calculated here that does not require binary fields for <code>Fcst</code> and <code>Obs</code>.
</p>


<h3>Value</h3>

<p> A list with components determined by which.stats, which may include any or all of the following.
</p>
<table>
<tr><td><code>bias</code></td>
<td>
<p>numeric giving the frequency bias.</p>
</td></tr>
<tr><td><code>ts</code></td>
<td>
<p>numeric giving the threat score.</p>
</td></tr>
<tr><td><code>ets</code></td>
<td>
<p>numeric giving the equitable threat score, also known as the Gilbert Skill Score.</p>
</td></tr>
<tr><td><code>pod</code></td>
<td>
<p>numeric giving the probability of decking an event, also known as the hit rate.</p>
</td></tr>
<tr><td><code>far</code></td>
<td>
<p>numeric giving the false alarm ratio.</p>
</td></tr>
<tr><td><code>f</code></td>
<td>
<p>numeric giving the false alarm rate.</p>
</td></tr>
<tr><td><code>hk</code></td>
<td>
<p>numeric giving the Hanssen and Kuipers statistic.</p>
</td></tr>
<tr><td><code>bcts</code>, <code>bcets</code></td>
<td>
<p>numeric giving the bias corrected version of the threat- and/or equitable threat score.</p>
</td></tr>
<tr><td><code>mse</code></td>
<td>
<p>numeric giving the mean square error.</p>
</td></tr>
</table>


<h3>Warning </h3>

<p> It is up to the user to provide the appropriate type of fields for the given statistics to be computed.  For example, they must be binary for all types of which.stats except mse.</p>


<h3>Note</h3>

<p>See the web page: https://www.cawcr.gov.au/projects/verification/ for more details about these statistics, and references.</p>


<h3>Author(s)</h3>

<p>Eric Gilleland</p>


<h3>References</h3>

<p>Brill, K. F. and Mesinger, F. (2009) Applying a general analytic method for assessing bias sensitivity to bias-adjusted threat and equitable threat scores.  <em>Wea. Forecasting</em>, <b>24</b>, 1748&ndash;1754.
</p>
<p>Jolliffe, I. T. and Stephenson, D. B., Edts. (2012) <em>Forecast Verification: A Practitioner's Guide in Atmospheric Science</em>, 2nd edition. Chichester, West Sussex, U.K.: Wiley, 274 pp.
</p>
<p>Mesinger, F. (2008) Bias adjusted precipitation threat scores.  <em>Adv. Geosci.</em>, <b>16</b>, 137&ndash;142.
</p>
<p>Wilks, D. S. (2006) <em>Statistical Methods in the Atmospheric Sciences</em>. 2nd Edition, Academic Press, Burlington, Massachusetts, 627pp.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+hoods2d">hoods2d</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Calculate the traditional verification scores for the first geometric case
# of the ICP.
data( "geom001" )
data( "geom000" )

rmse &lt;- sqrt(vxstats( geom001, geom000, which.stats="mse")$mse)
rmse
vxstats( geom001 &gt; 0, geom000 &gt; 0, which.stats=c("bias", "ts", "ets", "pod", "far", "f", "hk"))

data( "geom005" )
vxstats( geom005 &gt; 0, geom000 &gt;0, which.stats=c("ts","ets","bcts","bcets"))
</code></pre>

<hr>
<h2 id='warper'>
Image Warp
</h2><span id='topic+warper'></span>

<h3>Description</h3>

<p>Estimate an image warp
</p>


<h3>Usage</h3>

<pre><code class='language-R'>warper(Im0, Im1, p0, init, s, imethod = "bicubic", lossfun = "Q", 
    lossfun.args = list(beta = 0, Cmat = NULL), grlossfun = "defaultQ", 
    lower, upper, verbose = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="warper_+3A_im0">Im0</code>, <code id="warper_+3A_im1">Im1</code></td>
<td>

<p>Numeric matrices giving the zero- and one-energy images.  The <code>Im1</code> image is ultimately warped into the <code>Im0</code> image.
</p>
</td></tr>
<tr><td><code id="warper_+3A_p0">p0</code></td>
<td>

<p>nc by 2 matrix giving the zero-energy control points.
</p>
</td></tr>
<tr><td><code id="warper_+3A_init">init</code></td>
<td>

<p>nc by 2 matrix giving an initial estimate of the one-energy control points.
</p>
</td></tr>
<tr><td><code id="warper_+3A_s">s</code></td>
<td>

<p>Two-column matrix giving the full set of locations.  Works best if these are integer-valued coordinate indices.
</p>
</td></tr>
<tr><td><code id="warper_+3A_imethod">imethod</code></td>
<td>

<p>character giving he interpolation method to use.  May be one of &quot;round&quot;, &quot;bilinear&quot; or &quot;bicubic&quot;.
</p>
</td></tr>
<tr><td><code id="warper_+3A_lossfun">lossfun</code></td>
<td>

<p>Function giving the loss function over which to optimize the warp.  Default is <code>Q</code>, see <code>args{Q}</code> to see the required arguments for this function.
</p>
</td></tr>
<tr><td><code id="warper_+3A_lossfun.args">lossfun.args</code></td>
<td>

<p>A list giving optional arguments to <code>lossfun</code>.
</p>
</td></tr>
<tr><td><code id="warper_+3A_grlossfun">grlossfun</code></td>
<td>

<p>(optional) function giving the gradient of the loss function given by <code>lossfun</code>.
</p>
</td></tr>
<tr><td><code id="warper_+3A_lower">lower</code>, <code id="warper_+3A_upper">upper</code></td>
<td>

<p>(optional) arguments to the <code>nlminb</code> function which is used to optimize the loss function.
</p>
</td></tr>
<tr><td><code id="warper_+3A_verbose">verbose</code></td>
<td>

<p>logical, should progress information be printed to the screen?
</p>
</td></tr>
<tr><td><code id="warper_+3A_...">...</code></td>
<td>

<p>Optional arguments to <code>nlminb</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A pair-of-thin-plate-splines image warp is estimated by optimizing a loss function using nlminb.  It can be very difficult to get a good estimate.  It is suggested, therefore, to obtain good initial estimates for the one-energy control points.  The function <code>iwarper</code> can be useful in this context.
</p>


<h3>Value</h3>

<p> A list object of class &ldquo;warped&rdquo; is returned with components:
</p>
<table>
<tr><td><code>Im0</code>, <code>Im1</code>, <code>Im1.def</code></td>
<td>
<p>Matrices giving the zero- and one-energy images and the deformed one-energy image, resp.</p>
</td></tr>
<tr><td><code>p0</code>, <code>p1</code></td>
<td>
<p>zero- and one-energy control points, resp.</p>
</td></tr>
<tr><td><code>sigma</code></td>
<td>
<p>Estimated standard error of the mean difference between the zero-energy and deformed one-energy images.</p>
</td></tr></table>
<p>            &quot;warped.locations&quot; &quot;init&quot;            
</p>
<table>
<tr><td><code>s</code>, <code>imethod</code>, <code>lossfun</code>, <code>lossfun.args</code></td>
<td>
<p>Same as input arguments.</p>
</td></tr>
<tr><td><code>theta</code></td>
<td>
<p>The matrices defining the image warp, L, iL and B, where the last is the bending energy, and the first two are nc + 3 by nc + 3 matrices describing the control points and inverse control-point matrices.</p>
</td></tr>
<tr><td><code>arguments</code></td>
<td>
<p>Any arguments passed via ...</p>
</td></tr>
<tr><td><code>fit</code></td>
<td>
<p>The output from nlminb.</p>
</td></tr>
<tr><td><code>proc.time</code></td>
<td>
<p>The process time.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>References</h3>

<p>Dryden, I. L. and K. V. Mardia (1998) <em>Statistical Shape Analysis</em>.  Wiley, New York, NY, 347pp.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+iwarper">iwarper</a></code>
</p>

<hr>
<h2 id='waveIS'>
Intensity Scale (IS) Verification
</h2><span id='topic+waveIS'></span><span id='topic+waveIS.default'></span><span id='topic+waveIS.SpatialVx'></span><span id='topic+plot.waveIS'></span><span id='topic+summary.waveIS'></span>

<h3>Description</h3>

<p>Intensity Scale (IS) verification based on Casat et al (2004) and Casati (2010).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>waveIS(x, th = NULL, J = NULL, wavelet.type = "haar", levels
                 = NULL, max.n = NULL, smooth.fun = "hoods2dsmooth",
                 smooth.params = NULL, rule = "&gt;=", verbose = FALSE,
                 ...)

## S3 method for class 'SpatialVx'
waveIS(x, th = NULL, J = NULL, wavelet.type = "haar", levels
                 = NULL, max.n = NULL, smooth.fun = "hoods2dsmooth",
                 smooth.params = NULL, rule = "&gt;=", verbose = FALSE,
                 ..., time.point = 1, obs = 1, model = 1 )

## Default S3 method:
waveIS(x, th = NULL, J = NULL, wavelet.type = "haar", levels
                 = NULL, max.n = NULL, smooth.fun = "hoods2dsmooth",
                 smooth.params = NULL, rule = "&gt;=", verbose = FALSE,
                 ...)

## S3 method for class 'waveIS'
plot(x, main1 = "X", main2 = "Y",
    which.plots = c("all", "mse", "ss", "energy"),
    level.label = NULL, ...)

## S3 method for class 'waveIS'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="waveIS_+3A_x">x</code></td>
<td>
<p>For <code>waveIS</code> either a list object of class &ldquo;SpatialVx&rdquo;, a two-component list giving the two fields to be compared (the verification field is assumed to be the first one) or a named list with components &ldquo;X&rdquo; and &ldquo;Xhat&rdquo; giving the two fields to be compared.  list object returned by <code>waveIS</code>.</p>
</td></tr>
<tr><td><code id="waveIS_+3A_object">object</code></td>
<td>
<p> list object returned by <code>waveIS</code>.  </p>
</td></tr>
<tr><td><code id="waveIS_+3A_main1">main1</code>, <code id="waveIS_+3A_main2">main2</code></td>
<td>
<p>character giving labels for the plots where <code>main1</code> refers to the verification field and <code>main2</code> to the forecast field.</p>
</td></tr>
<tr><td><code id="waveIS_+3A_which.plots">which.plots</code></td>
<td>
<p>character vector naming one or more specific plots to do.</p>
</td></tr>
<tr><td><code id="waveIS_+3A_level.label">level.label</code></td>
<td>
<p>optional character vector to use for level names on the plot(s).</p>
</td></tr>
<tr><td><code id="waveIS_+3A_j">J</code></td>
<td>

<p>numeric integer giving the number of levels to use.  If NULL and the field is dyadic, this will be log2(min(dim(X))), where X is a field from the verification set.  If NULL and the field is not dyadic, then <code>J</code> is set equal to 4.  Note that if the fields are not dyadic, the function will be much slower.
</p>
</td></tr>
<tr><td><code id="waveIS_+3A_wavelet.type">wavelet.type</code></td>
<td>

<p>character giving the name of the wavelet type to use as accepted by <code>dwt.2d</code> and <code>modwt.2d</code>.
</p>
</td></tr>
<tr><td><code id="waveIS_+3A_th">th</code></td>
<td>
<p> list object with named components &ldquo;X&rdquo; and &ldquo;Xhat&rdquo; giving the thresholds to use for each field.  If null, taken from teh thresholds attribute for &ldquo;SpatialVx&rdquo; objects. </p>
</td></tr>
<tr><td><code id="waveIS_+3A_time.point">time.point</code></td>
<td>
<p>numeric or character indicating which time point from the &ldquo;SpatialVx&rdquo; verification set to select for analysis.</p>
</td></tr>
<tr><td><code id="waveIS_+3A_obs">obs</code>, <code id="waveIS_+3A_model">model</code></td>
<td>
<p>numeric indicating which observation/forecast model to select for the analysis.</p>
</td></tr>
<tr><td><code id="waveIS_+3A_levels">levels</code></td>
<td>
<p>numeric vector giving the successive values of the smoothing parameter.  For example, for the default method, these are the neighborhood lengths over which the levels^2 nearest neighbors are averaged for each point.  Values should make sense for the specific smoothing function.  For example, for the default method, these should be odd integers.</p>
</td></tr>
<tr><td><code id="waveIS_+3A_max.n">max.n</code></td>
<td>
<p>(optional) single numeric giving the maximum neighborhood length to use.  Only used if levels are NULL.</p>
</td></tr>
<tr><td><code id="waveIS_+3A_smooth.fun">smooth.fun</code></td>
<td>
<p>character giving the name of a smoothing function to be applied.  Default is an average over the n^2 nearest neighbors, where n is taken to be each value of the <code>levels</code> argument.</p>
</td></tr>
<tr><td><code id="waveIS_+3A_smooth.params">smooth.params</code></td>
<td>
<p>list object containing any optional arguments to <code>smooth.fun</code>.  Use NULL if none.</p>
</td></tr>
<tr><td><code id="waveIS_+3A_rule">rule</code></td>
<td>
<p>If <code>type</code> is &ldquo;binary&rdquo;, return 0 when the rule applied to a grid point's value is not true in relation to the threshold value, and 1 elsewhere.  If <code>type</code> is &ldquo;replace.below&rdquo;, then return <code>replace.with</code> wherever the rule is not true and return the original value otherwise.  By default, it replaces values below the threshold with zero (hence its name), but if <code>rule</code> is, e.g., &ldquo;&lt;=&rdquo;, then it will replace values above with zero; or whatever value is chosen for <code>replace.with</code>.</p>
</td></tr>
<tr><td><code id="waveIS_+3A_verbose">verbose</code></td>
<td>

<p>logical, should progress information be printed to the screen?
</p>
</td></tr>
<tr><td><code id="waveIS_+3A_...">...</code></td>
<td>
<p>Not used by <code>waveIS</code> (or its method functions) or <code>plot.waveIS</code>.  Only sort of used by <code>summary.waveIS</code>.  One can put the argument silent=TRUE so that nothing is printed to the screen (useful if you just want the values calculated and stored without writing to the screen).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function applies various statistics to the detail fields (in wavelet space) of a discrete wavelet decomposition (DWT) of the binary error fields for a verification set.  In particular, the statistics described in Casati et al (2004) and Casati (2010) are calculated.  This function depends on the <code>waverify2d</code> or <code>mowaverify2d</code> function (depending on whether the fields are dyadic or not, resp.), which themselves depend on the <code>dwt.2d</code> and <code>idwt.2d</code> or <code>modwt.2d</code> and <code>imodwt.2d</code> functions.
</p>
<p>See the references herein and the help files and references therein for <code>dwt.2d</code> and <code>modwt.2d</code> for more information on this approach, as well as Percival and Guttorp (1994) and Lindsay et al. (1996).
</p>


<h3>Value</h3>

<p>A list object of class &ldquo;waveIS&rdquo; that contains the entire prep object passed in by obj, as well as additional components:
</p>
<table>
<tr><td><code>EnVx</code>, <code>EnFcst</code></td>
<td>
<p>J by q matrices giving the energy for the verification and forecast fields, resp., for each threshold (columns) and scale (rows).</p>
</td></tr>
<tr><td><code>MSE</code>, <code>SS</code></td>
<td>
<p>J by q matrices giving the mean square error and IS skill score for each threshold (column) and scale (rows).</p>
</td></tr>
<tr><td><code>Bias</code></td>
<td>
<p>numeric vector of length q giving the frequency bias of the original fields for each threshold.</p>
</td></tr>
</table>
<p>plot.waveIS does not return any value.  A plot is created on the current graphic device.
summary.waveIS returns a list invisibly with the same components as returned by waveIS along with extra components:
</p>
<table>
<tr><td><code>MSEu</code>, <code>SSu</code>, <code>EnVx.u</code>, <code>EnFcst.u</code></td>
<td>
<p>length q numeric vectors giving the MSE, SS, and Vx and Fcst energy for each threshold (i.e., ignoring the wavelet decomposition).</p>
</td></tr>
<tr><td><code>MSEperc</code>, <code>EnVx.perc</code>, <code>EnFcst.perc</code></td>
<td>
<p>J by q numeric matrices giving percentage form of MSE, Vx Energy and Fcst Energy values, resp.</p>
</td></tr>
<tr><td><code>EnRelDiff</code></td>
<td>
<p>J by q numeric matrix giving the energy relative difference.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>References</h3>

<p>Casati, B., Ross, G. and Stephenson, D. B. (2004) A new intensity-scale approach for the verification of spatial precipitation forecasts. <em>Meteorol. Appl.</em> <b>11</b>, 141&ndash;154.
</p>
<p>Casati, B. (2010) New Developments of the Intensity-Scale Technique within the Spatial Verification Methods Inter-Comparison Project. <em>Wea. Forecasting</em> <b>25</b>, (1), 113&ndash;143, doi:10.1175/2009WAF2222257.1.
</p>
<p>Lindsay, R. W., Percival, D. B. and Rothrock, D. A. (1996)  The discrete wavelet transform and the scale analysis of the surface properties of sea ice.  <em>IEEE Transactions on Geoscience and Remote Sensing</em>, <b>34</b> (3), 771&ndash;787.
</p>
<p>Percival, D. B. and Guttorp, P. (1994)  Long-memory processes, the Allan variance and wavelets.  In <em>Wavelets in Geophysics</em>, Foufoula-Georgiou, E. and Kumar, P., Eds., New York: Academic, 325&ndash;343.
</p>


<h3>See Also</h3>

<p><code>IS</code>, <code>int.scale.verify</code> from package <span class="pkg">verification</span>,
</p>
<p><code><a href="waveslim.html#topic+dwt.2d">dwt.2d</a></code>, <code><a href="waveslim.html#topic+modwt.2d">modwt.2d</a></code>, <code><a href="waveslim.html#topic+idwt.2d">idwt.2d</a></code>, <code><a href="waveslim.html#topic+imodwt.2d">imodwt.2d</a></code>, <code><a href="#topic+hoods2d">hoods2d</a></code>
</p>
<p><code><a href="#topic+thresholder">thresholder</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data( "UKobs6" )
data( "UKfcst6" )
data( "UKloc" )

hold &lt;- make.SpatialVx( UKobs6, UKfcst6,
    thresholds = c(0.1, 0.2, 0.5, 1, 2, 5, 10, 20, 50),
    loc = UKloc, map = TRUE, field.type = "Rainfall", units = "mm/h",
    data.name = "Nimrod", obs.name = "UKobs6", model.name = "UKfcst6" )

look &lt;- waveIS(hold, J=8, levels=2^(8-1:8), verbose=TRUE)
plot(look, which.plots="mse") 
plot(look, which.plots="ss")
plot(look, which.plots="energy")
summary(look)

</code></pre>

<hr>
<h2 id='wavePurifyVx'>
Apply Traditional Forecast Verification After Wavelet Denoising
</h2><span id='topic+wavePurifyVx'></span><span id='topic+wavePurifyVx.default'></span><span id='topic+wavePurifyVx.SpatialVx'></span><span id='topic+plot.wavePurifyVx'></span><span id='topic+summary.wavePurifyVx'></span>

<h3>Description</h3>

<p>Apply traditional forecast verification after wavelet denoising ala Briggs and Levine (1997).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>wavePurifyVx( x, climate = NULL, which.stats = c("bias",
    "ts", "ets", "pod", "far", "f", "hk", "mse"), thresholds = NULL,
    rule = "&gt;=", return.fields = FALSE, verbose = FALSE, ...)

## S3 method for class 'SpatialVx'
wavePurifyVx( x, climate = NULL, which.stats = c("bias",
    "ts", "ets", "pod", "far", "f", "hk", "mse"), thresholds = NULL,
    rule = "&gt;=", return.fields = FALSE, verbose = FALSE, ...,
    time.point = 1, obs = 1, model = 1 )

## Default S3 method:
wavePurifyVx( x, climate = NULL, which.stats = c("bias",
    "ts", "ets", "pod", "far", "f", "hk", "mse"), thresholds = NULL,
    rule = "&gt;=", return.fields = FALSE, verbose = FALSE, ...)

## S3 method for class 'wavePurifyVx'
plot(x, ..., col = c("gray", tim.colors(64)), zlim, mfrow,
                 horizontal = TRUE, type = c("stats", "fields") )

## S3 method for class 'wavePurifyVx'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="wavePurifyVx_+3A_x">x</code></td>
<td>

<p>For <code>wavePurifyVx</code>, either a list object of class &ldquo;SpatialVx&rdquo;, or a list with only two components consisting of m by n matrices giving the verification and forecast fields, resp., or a list with named components &ldquo;X&rdquo; and &ldquo;Xhat&rdquo;.
</p>
<p>For <code>plot.wavePurifyVx</code>, list object as output from <code>wavePurifyVx</code>.
</p>
</td></tr>
<tr><td><code id="wavePurifyVx_+3A_object">object</code></td>
<td>
<p> list object as returned by <code>wavePurifyVx</code>.  </p>
</td></tr>
<tr><td><code id="wavePurifyVx_+3A_climate">climate</code></td>
<td>
<p>m by n matrix defining a climatology field.  If not NULL, then the anamoly correlation coefficient will be applied to the wavelet denoised fields.</p>
</td></tr>
<tr><td><code id="wavePurifyVx_+3A_which.stats">which.stats</code></td>
<td>

<p>character describing which traditional verification statistics to calculate on the wavelet denoised fields.  This is the argument passed to the argument of the same name in <code>vxstats</code>.
</p>
</td></tr>
<tr><td><code id="wavePurifyVx_+3A_thresholds">thresholds</code></td>
<td>

<p>Either a numeric vector or a list with components named &ldquo;X&rdquo; and &ldquo;Xhat&rdquo; giving thresholds  used to define events for all of the verification statistics except MSE.  However, if supplied or other statistics are to be computed, then MSE will be calculated for the fields at values &gt;= <code>thresholds</code>.  If only MSE is to be computed, and <code>thresholds</code> is NULL, then no thresholding is applied.  If NULL, and any of the statistics besides MSE are to be calculated, then default values of the 0, 0.1, 0.25, 0.33, 0.5, 0.66, 0.75, 0.9 and 0.95 quantiles (for each field, so that the thresholds may differ between fields) are used.  The same holds for anamoly correlation coefficient.  The exception is that if the argument is null and <code>x</code> is a &ldquo;SpatialVx&rdquo; class object, then the thresholds are taken to be those associated with the &ldquo;thresholds&rdquo; attribute of this object.
</p>
</td></tr>
<tr><td><code id="wavePurifyVx_+3A_rule">rule</code></td>
<td>
<p>character string giving the threshold rule to be applied.  See help file for <code>thresholder</code> function for more information.</p>
</td></tr>
<tr><td><code id="wavePurifyVx_+3A_return.fields">return.fields</code></td>
<td>
<p>logical, should the denoised fields be returned (e.g., for subsequent plotting)?</p>
</td></tr>
<tr><td><code id="wavePurifyVx_+3A_time.point">time.point</code></td>
<td>
<p>numeric or character indicating which time point from the &ldquo;SpatialVx&rdquo; verification set to select for analysis.</p>
</td></tr>
<tr><td><code id="wavePurifyVx_+3A_obs">obs</code>, <code id="wavePurifyVx_+3A_model">model</code></td>
<td>
<p>numeric indicating which observation/forecast model to select for the analysis.</p>
</td></tr>
<tr><td><code id="wavePurifyVx_+3A_verbose">verbose</code></td>
<td>
<p>logical, should progress information (including total run time) be printed to the screen?</p>
</td></tr>
<tr><td><code id="wavePurifyVx_+3A_col">col</code>, <code id="wavePurifyVx_+3A_zlim">zlim</code>, <code id="wavePurifyVx_+3A_horizontal">horizontal</code></td>
<td>
<p>optional arguments to <code>image</code>, and/or <span class="pkg">fields</span> functions <code>poly.image</code> and <code>image.plot</code></p>
</td></tr>
<tr><td><code id="wavePurifyVx_+3A_mfrow">mfrow</code></td>
<td>
<p> optionally set the plotting panel via <code>mfrow</code> (see help file for <code>par</code>).  Default sets a rgion that will show all plots in one set of panels. </p>
</td></tr>
<tr><td><code id="wavePurifyVx_+3A_type">type</code></td>
<td>
<p>character string stating whether to plot the resulting statistics or the original fields along with their de-noised counter parts.</p>
</td></tr>
<tr><td><code id="wavePurifyVx_+3A_...">...</code></td>
<td>

<p>For <code>wavePurifyVx</code>, optional additional arguments to <code>denoise.dwt.2d</code> (or <code>denoise.modwt.2d</code>) from package <span class="pkg">waveslim</span>.  Note that if the argument <code>J</code> is not passed, then it will be determined as J=log2(min(m,n)).  If the fields are dyadic, then the usual DWT is used, otherwise the maximal overlap DWT is used instead.  For the plot and summary method functions, these are not used.  Also passed to <code>poly.image</code> and <code>image.plot</code> for plotting routine for &ldquo;fields&rdquo; type plots when the &ldquo;maps&rdquo; attribute from the &ldquo;SpatialVx&rdquo; object is TRUE.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If the fields are dyadic, then the <code>denoise.dwt.2d</code> function from package <span class="pkg">waveslim</span> is applied to each field before calculating the chosen verification statistics.  Otherwise <code>denoise.modwt.2d</code> from the same package is used.  The result is that high-frequency fluctuations in the two fields are removed before calculating verification statistics so that the resulting statistics are less susceptible to small-scale errors (see Briggs and Levine, 1997).  See Percival and Guttorp (1994) and Lindsay et al. (1996) for more on this type of wavelet analysis including maximal overlap DWT.
</p>


<h3>Value</h3>

<p>A list object of class &ldquo;wavePurifyVx&rdquo; is returned with possible components (depending on what is supplied in the arguments, etc.):
</p>
<table>
<tr><td><code>X2</code>, <code>Y2</code></td>
<td>
<p>m by n matrices of the denoised verification and forecast fields, resp. (only if return.fields is TRUE).</p>
</td></tr>
<tr><td><code>thresholds</code></td>
<td>
<p>q by 2 matrix of thresholds applied to the forecast (first column) and verification (second column) fields, resp.  If climate is not NULL, then the same thresholds for the forecast field are applied to the climatology.</p>
</td></tr>
<tr><td><code>qs</code></td>
<td>
<p>If object and thresholds are NULL, and statistics other than MSE or ACC are desired, then this will be created along with the thresholds, and is just a character version of the trhesholds. </p>
</td></tr>
<tr><td><code>args</code></td>
<td>
<p>list object containing all the optional arguments passed into ..., and the value of J used (e.g., even if not passed into ...).</p>
</td></tr>
<tr><td><code>bias</code>, <code>ts</code>, <code>ets</code>, <code>pod</code>, <code>far</code>, <code>f</code>, <code>hk</code>, <code>mse</code>, <code>acc</code></td>
<td>
<p>numeric vectors of length q (i.e., the number of thresholds) giving the associated verification statistics.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>References</h3>

<p>Briggs, W. M. and Levine, R. A. (1997) Wavelets and field forecast verification. <em>Mon. Wea. Rev.</em>, <b>125</b>, 1329&ndash;1341.
</p>
<p>Lindsay, R. W., Percival, D. B. and Rothrock, D. A. (1996)  The discrete wavelet transform and the scale analysis of the surface properties of sea ice.  <em>IEEE Transactions on Geoscience and Remote Sensing</em>, <b>34</b> (3), 771&ndash;787.
</p>
<p>Percival, D. B. and Guttorp, P. (1994)  Long-memory processes, the Allan variance and wavelets.  In <em>Wavelets in Geophysics</em>, Foufoula-Georgiou, E. and Kumar, P., Eds., New York: Academic, pp. 325&ndash;343.
</p>


<h3>See Also</h3>

<p><code><a href="waveslim.html#topic+denoise.dwt.2d">denoise.dwt.2d</a></code>, <code><a href="waveslim.html#topic+denoise.modwt.2d">denoise.modwt.2d</a></code>, <code><a href="#topic+waverify2d">waverify2d</a></code>, <code><a href="#topic+mowaverify2d">mowaverify2d</a></code>, <code><a href="#topic+waveIS">waveIS</a></code>, <code><a href="#topic+vxstats">vxstats</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>grid &lt;- list( x= seq( 0,5,,100), y= seq(0,5,,100))
obj &lt;- Exp.image.cov( grid=grid, theta=.5, setup=TRUE)
look &lt;- sim.rf( obj)
look[ look &lt; 0] &lt;- 0
look &lt;- zapsmall( look)
     
look2 &lt;- sim.rf( obj)
look2[ look2 &lt; 0] &lt;- 0
look2 &lt;- zapsmall( look2)

look3 &lt;- sim.rf( obj) 
look3[ look3 &lt; 0] &lt;- 0 
look3 &lt;- zapsmall( look3)

hold &lt;- make.SpatialVx( look, look2, thresholds = c(0.1, 1),
            field.type = "random", units = "units")

plot( hold )

res &lt;- wavePurifyVx( hold, climate = look3, return.fields = TRUE, verbose = TRUE )

plot(res, type="fields")
plot(res, type="stats")

summary(res)

## Not run: 
data( "UKobs6" )
data( "UKfcst6" )
data( "UKloc" )

hold &lt;- surrogater2d( UKobs6, n=1, maxiter=50, verbose=TRUE)
hold &lt;- matrix(hold, 256, 256)

UKobj &lt;- make.SpatialVx( UKobs6, UKfcst6, thresholds = c(0.1, 2, 5, 10),
    loc = UKloc, map = TRUE, field.type = "Rainfall", units = "mm/h",
    data.name = "Nimrod", obs.name = "obs 6", model.name = "fcst 6" )

plot(UKobj ) 

look &lt;- wavePurifyVx( object = UKobj, climate = hold,
    return.fields = TRUE, verbose = TRUE)

plot(look, type = "fields" )
plot(look, type = "stats" )

summary( look )


## End(Not run)

</code></pre>

<hr>
<h2 id='waverify2d'>
High-Resolution Gridded Forecast Verification Using Discrete Wavelet Decomposition
</h2><span id='topic+waverify2d'></span><span id='topic+waverify2d.default'></span><span id='topic+waverify2d.SpatialVx'></span><span id='topic+mowaverify2d'></span><span id='topic+mowaverify2d.default'></span><span id='topic+mowaverify2d.SpatialVx'></span><span id='topic+plot.waverify2d'></span><span id='topic+print.waverify2d'></span>

<h3>Description</h3>

<p>High-resolution gridded forecast verification using discrete wavelet decomposition.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>waverify2d(X, ..., Clim = NULL, wavelet.type = "haar", J = NULL)

## Default S3 method:
waverify2d(X, ..., Y, Clim = NULL, wavelet.type = "haar", J =
                 NULL, useLL = FALSE, compute.shannon = FALSE,
                 which.space = "field", verbose = FALSE)

## S3 method for class 'SpatialVx'
waverify2d(X, ..., Clim = NULL, wavelet.type = "haar", J = NULL, 
    useLL = FALSE, compute.shannon = FALSE, which.space = "field", 
    time.point = 1, obs = 1, model = 1, verbose = FALSE)

mowaverify2d(X, ..., Clim = NULL, wavelet.type = "haar", J = 4)

## Default S3 method:
mowaverify2d(X, ..., Clim = NULL, Y, wavelet.type = "haar", J = 4,
    useLL = FALSE, compute.shannon = FALSE, which.space = "field", verbose = FALSE)

## S3 method for class 'SpatialVx'
mowaverify2d(X, ..., Clim = NULL, wavelet.type = "haar", J = 4, 
    useLL = FALSE, compute.shannon = FALSE, which.space = "field", 
    time.point = 1, obs = 1, model = 1, verbose = FALSE)

## S3 method for class 'waverify2d'
plot(x, ..., main1 = "X", main2 = "Y", main3 = "Climate", 
    which.plots = c("all", "dwt2d", "details", "energy", "mse", 
        "rmse", "acc"), separate = FALSE, col, horizontal = TRUE)

## S3 method for class 'waverify2d'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="waverify2d_+3A_x">X</code>, <code id="waverify2d_+3A_y">Y</code>, <code id="waverify2d_+3A_clim">Clim</code></td>
<td>

<p>m by n dyadic matrices (i.e., m = 2^M and n = 2^N, for M, N some integers) giving the verification and forecast fields (and optionally a climatology field), resp.  Alternatively, <code>X</code> may be a &ldquo;SpatialVx&rdquo; object, in which case, <code>Y</code> is not given and in either case <code>Clim</code> must be provided if it is to be used.
</p>
</td></tr>
<tr><td><code id="waverify2d_+3A_x">x</code></td>
<td>

<p>list object of class &ldquo;waverify2d&rdquo; as returned by <code>waverify2d</code>.
</p>
</td></tr>
<tr><td><code id="waverify2d_+3A_wavelet.type">wavelet.type</code></td>
<td>

<p>character naming the type of wavelet to be used.  This is given as the <code>wf</code> argument to the <code>dwt.2d</code> function of package <span class="pkg">waveslim</span>.
</p>
</td></tr>
<tr><td><code id="waverify2d_+3A_j">J</code></td>
<td>

<p>(optional) numeric integer giving the pre-determined number of levels to use.  If NULL, J is set to be log2(m) = M in <code>waverify2d</code> only.
</p>
</td></tr>
<tr><td><code id="waverify2d_+3A_usell">useLL</code></td>
<td>

<p>logical, should the LL submatrix (i.e., the father wavelet or grand mean) be used to find the inverse DWT's for calculating the detail fields?
</p>
</td></tr>
<tr><td><code id="waverify2d_+3A_compute.shannon">compute.shannon</code></td>
<td>

<p>logical, should the Shannon entropy be calculated for the wavelet decomposition?
</p>
</td></tr>
<tr><td><code id="waverify2d_+3A_which.space">which.space</code></td>
<td>
<p>character (one of &ldquo;field&rdquo; or &ldquo;wavelet&rdquo;) naming from which space the detail fields should be used.  If &ldquo;field&rdquo;, then it is in the original field (or image) space (i.e., the detail reconstruction), and if &ldquo;wavelet&rdquo;, it will be done in the wavelet space (i.e., the detail wavelet coefficients).</p>
</td></tr>
<tr><td><code id="waverify2d_+3A_time.point">time.point</code></td>
<td>
<p>numeric or character indicating which time point from the &ldquo;SpatialVx&rdquo; verification set to select for analysis.</p>
</td></tr>
<tr><td><code id="waverify2d_+3A_obs">obs</code>, <code id="waverify2d_+3A_model">model</code></td>
<td>
<p>numeric indicating which observation/forecast model to select for the analysis.</p>
</td></tr>
<tr><td><code id="waverify2d_+3A_main1">main1</code>, <code id="waverify2d_+3A_main2">main2</code>, <code id="waverify2d_+3A_main3">main3</code></td>
<td>
<p>optional characters naming each field to be used for the detail field plots and legend labelling on the energy plot.</p>
</td></tr>
<tr><td><code id="waverify2d_+3A_which.plots">which.plots</code></td>
<td>
<p>character vector describing which plots to make.  The default is to make all of them.  &ldquo;dwt2d&rdquo; option uses <code>plot.dwt2d</code> from <span class="pkg">waveslim</span>.  &ldquo;details&rdquo; option makes image plots of the detail fields on which the various statistics are calculated.  The rest of the options give line plots showing the statistics.</p>
</td></tr>
<tr><td><code id="waverify2d_+3A_separate">separate</code></td>
<td>
<p>logical, should the plots be on their own devices (TRUE) or should some of them be put onto a single multi-panel device (FALSE, default)?</p>
</td></tr>
<tr><td><code id="waverify2d_+3A_col">col</code></td>
<td>
<p>optional argument specifying the <code>col</code> argument in calls to functions like <code>image</code>.  Default is a concatenation of gray with <code>time.colors(64)</code>.</p>
</td></tr>
<tr><td><code id="waverify2d_+3A_horizontal">horizontal</code></td>
<td>
<p>logical, should the legend on image plots be horizontal (TRUE, placed at the bottom of the plot) or vertical (FALSE, placed at the right side of the plot)?</p>
</td></tr>
<tr><td><code id="waverify2d_+3A_verbose">verbose</code></td>
<td>
<p>logical, should progress information be printed to the screen, including total run time?
</p>
</td></tr>
<tr><td><code id="waverify2d_+3A_...">...</code></td>
<td>

<p>optional additonal plot or image.plot parameters.  If detail and energy, mse, rmse or acc plots are desired, must be applicable to both types of plots.  Not used by <code>print</code> method function.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a function to use discrete wavelet decomposition to analyze verification sets along the lines of Briggs and Levine (1997), as well as Casati et al. (2004) and Casati (2009).  In the originally proposed formulation of Briggs and Levine (1997), continuous verification statistics (namely, the anomaly correlation coefficient (ACC) and root mean square error (RMSE)) are calculated for detail fields obtained from wavelet decompositions of each of a forecast and verification field (and for ACC a climatology field as well).  Casati et al. (2004) introduced an intensity scale approach that applies 2-d DWT to binary (obtained from thresholding) difference fields (Forecast - Verification), and applying a skill score at each level based on the mean square error (MSE).  Casati (2009) extended this idea to look at the energy at each level as well.
</p>
<p>This function makes use of the <code>dwt.2d</code> and <code>idwt.2d</code> functions from package <span class="pkg">waveslim</span>, and <code>plot.waverify2d</code> uses the <code>plot.dwt.2d</code> function if <code>dwt2d</code> is selected through the <code>which.plots</code> argument.  See the help file for these functions, the references therein and the references herein for more on these approaches.
</p>
<p>Generally, it is not necessary to use the father wavelet for the detail fields, but for some purposes, it may be desired.
</p>
<p><code>mowaverify2d</code> is very similar to <code>waverify2d</code>, but it allows fields to be non-dyadic (and may subsequently be slower).  It uses the <code>modwt.2d</code> and <code>imodwt.2d</code> functions from the package <span class="pkg">waveslim</span>.  In particular, it performs a maximal overlap discrete wavelet transform on a matrix of arbitrary dimension.  See the help file and references therein for <code>modwt.2d</code> for more information, as well as Percival and Guttorp (1994) and Lindsay et al. (1996).
</p>
<p>In Briggs and Levine (1997), they state that the calculations can be done in either the data (called field here) space or the wavelet space, and they do their examples in the field space.  If the wavelets are orthogonal, then the detail coefficeints (wavelet space), can be analyzed with the assumption that they are independent; whereas in the data space, they typically cannot be assumed to be independent.  Therefore, most statistical tests should be performed in the wavelet space to avoid issues arising from spatial dependence.
</p>


<h3>Value</h3>

<p>A list object of class &ldquo;waverify2d&rdquo; with components:
</p>
<table>
<tr><td><code>J</code></td>
<td>
<p>single numeric giving the number of levels.</p>
</td></tr>
<tr><td><code>X.wave</code>, <code>Y.wave</code>, <code>Clim.wave</code></td>
<td>
<p>objects of class &ldquo;dwt.2d&rdquo; describing the wavelet decompositions for the verification and forecast fields (and climatology, if applicable), resp. (see the help file for dwt.2d from package waveslim for more about these objects).</p>
</td></tr>
<tr><td><code>Shannon.entropy</code></td>
<td>
<p>numeric matrix giving the Shannon entropy for each field.</p>
</td></tr>
<tr><td><code>energy</code></td>
<td>
<p>numeric matrix giving the energy at each level and field.</p>
</td></tr>
<tr><td><code>mse</code>, <code>rmse</code></td>
<td>
<p>numeric vectors of length J giving the MSE/RMSE for each level between the verification and forecast fields.</p>
</td></tr>
<tr><td><code>acc</code></td>
<td>
<p>If a climatology field is supplied, this is a numeric vector giving the ACC for each level.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>References</h3>

<p>Briggs, W. M. and Levine, R. A. (1997) Wavelets and field forecast verification. <em>Mon. Wea. Rev.</em>, <b>125</b>, 1329&ndash;1341.
</p>
<p>Casati, B., Ross, G. and Stephenson, D. B. (2004) A new intensity-scale approach for the verification of spatial precipitation forecasts. <em>Meteorol. Appl.</em> <b>11</b>, 141&ndash;154.
</p>
<p>Casati, B. (2010) New Developments of the Intensity-Scale Technique within the Spatial Verification Methods Inter-Comparison Project. <em>Wea. Forecasting</em> <b>25</b>, (1), 113&ndash;143, doi:10.1175/2009WAF2222257.1.
</p>
<p>Lindsay, R. W., Percival, D. B. and Rothrock, D. A. (1996)  The discrete wavelet transform and the scale analysis of the surface properties of sea ice.  <em>IEEE Transactions on Geoscience and Remote Sensing</em>, <b>34</b> (3), 771&ndash;787.
</p>
<p>Percival, D. B. and Guttorp, P. (1994)  Long-memory processes, the Allan variance and wavelets.  In <em>Wavelets in Geophysics</em>, Foufoula-Georgiou, E. and Kumar, P., Eds., New York: Academic, pp. 325&ndash;343.
</p>


<h3>See Also</h3>

<p><code><a href="waveslim.html#topic+dwt.2d">dwt.2d</a></code>, <code><a href="waveslim.html#topic+idwt.2d">idwt.2d</a></code>, <code><a href="#topic+hoods2d">hoods2d</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>grid&lt;- list( x= seq( 0,5,,64), y= seq(0,5,,64))
obj&lt;-Exp.image.cov( grid=grid, theta=.5, setup=TRUE)
look&lt;- sim.rf( obj) 
look[ look &lt; 0] &lt;- 0
look &lt;- zapsmall( look)

look2 &lt;- sim.rf( obj) 
look2[ look2 &lt; 0] &lt;- 0 
look2 &lt;- zapsmall( look2)

res &lt;- waverify2d(look, Y=look2)
plot(res)
summary(res)

## Not run: 
data( "UKobs6" )
data( "UKfcst6" )

look &lt;- waverify2d(UKobs6, Y=UKfcst6)

plot(look, which.plots="energy")
look2 &lt;- mowaverify2d(UKobs6, UKfcst6, J=8)
plot(look2, which.plots="energy")

plot(look, main1="NIMROD Analysis", main2="NIMROD Forecast")

plot(look2, main1="NIMROD Analysis", main2="NIMROD Forecast")

# Alternative using "SpatialVx" object.
data( "UKloc" )

hold &lt;- make.SpatialVx( UKobs6, UKfcst6, loc = UKloc,
    map = TRUE, field.type = "Rainfall", units = "mm/h",
    data.name = "Nimrod", obs.name = "Obs 6",
    model.name = "Fcst 6" )

look &lt;- waverify2d(hold)

plot(look, which.plots="details")



## End(Not run)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
