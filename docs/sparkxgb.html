<!DOCTYPE html><html lang="en"><head><title>Help for package sparkxgb</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {sparkxgb}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#+25+26gt+3B+25'><p>Pipe operator</p></a></li>
<li><a href='#xgboost_classifier'><p>XGBoost Classifier</p></a></li>
<li><a href='#xgboost_regressor'><p>XGBoost Regressor</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Interface for 'XGBoost' on 'Apache Spark'</td>
</tr>
<tr>
<td>Version:</td>
<td>0.2.0</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Edgar Ruiz &lt;edgar@posit.co&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>A 'sparklyr' <a href="https://spark.posit.co/">https://spark.posit.co/</a> extension that provides an R
  interface for 'XGBoost' <a href="https://github.com/dmlc/xgboost">https://github.com/dmlc/xgboost</a> on 'Apache Spark'. 
  'XGBoost' is an optimized distributed gradient boosting library.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.apache.org/licenses/LICENSE-2.0">Apache License (&ge; 2.0)</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.1.2)</td>
</tr>
<tr>
<td>Imports:</td>
<td>sparklyr, rlang, magrittr, vctrs, fs</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.1</td>
</tr>
<tr>
<td>Suggests:</td>
<td>dplyr, purrr, testthat (&ge; 3.0.0), withr</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-04-30 15:13:34 UTC; edgar</td>
</tr>
<tr>
<td>Author:</td>
<td>Kevin Kuo <a href="https://orcid.org/0000-0001-7803-7901"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut],
  Yitao Li <a href="https://orcid.org/0000-0002-1261-905X"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut],
  Edgar Ruiz [aut, cre]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-04-30 20:40:07 UTC</td>
</tr>
</table>
<hr>
<h2 id='+25+26gt+3B+25'>Pipe operator</h2><span id='topic++25+3E+25'></span>

<h3>Description</h3>

<p>See <code>magrittr::<a href="magrittr.html#topic+pipe">%&gt;%</a></code> for details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lhs %&gt;% rhs
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="+2B25+2B26gt+2B3B+2B25_+3A_lhs">lhs</code></td>
<td>
<p>A value or the magrittr placeholder.</p>
</td></tr>
<tr><td><code id="+2B25+2B26gt+2B3B+2B25_+3A_rhs">rhs</code></td>
<td>
<p>A function call using the magrittr semantics.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The result of calling 'rhs(lhs)'.
</p>

<hr>
<h2 id='xgboost_classifier'>XGBoost Classifier</h2><span id='topic+xgboost_classifier'></span>

<h3>Description</h3>

<p>XGBoost classifier for Spark.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>xgboost_classifier(
  x,
  formula = NULL,
  eta = 0.3,
  gamma = 0,
  max_depth = 6,
  min_child_weight = 1,
  max_delta_step = 0,
  grow_policy = "depthwise",
  max_bins = 16,
  subsample = 1,
  colsample_bytree = 1,
  colsample_bylevel = 1,
  lambda = 1,
  alpha = 0,
  tree_method = "auto",
  sketch_eps = NULL,
  scale_pos_weight = 1,
  sample_type = "uniform",
  normalize_type = "tree",
  rate_drop = 0,
  skip_drop = 0,
  lambda_bias = 0,
  tree_limit = 0,
  num_round = 1,
  num_workers = 1,
  nthread = 1,
  use_external_memory = FALSE,
  silent = 0,
  custom_obj = NULL,
  custom_eval = NULL,
  missing = NaN,
  seed = 0,
  timeout_request_workers = NULL,
  checkpoint_path = "",
  checkpoint_interval = -1,
  objective = "multi:softprob",
  base_score = 0.5,
  train_test_ratio = 1,
  num_early_stopping_rounds = 0,
  objective_type = "classification",
  eval_metric = NULL,
  maximize_evaluation_metrics = FALSE,
  num_class = NULL,
  base_margin_col = NULL,
  thresholds = NULL,
  weight_col = NULL,
  features_col = "features",
  label_col = "label",
  prediction_col = "prediction",
  probability_col = "probability",
  raw_prediction_col = "rawPrediction",
  uid = random_string("xgboost_classifier_"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="xgboost_classifier_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="xgboost_classifier_+3A_formula">formula</code></td>
<td>
<p>Used when <code>x</code> is a <code>tbl_spark</code>. R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see <a href="sparklyr.html#topic+ft_r_formula">ft_r_formula</a> for details.</p>
</td></tr>
<tr><td><code id="xgboost_classifier_+3A_eta">eta</code></td>
<td>
<p>Step size shrinkage used in update to prevents overfitting. After
each boosting step, we can directly get the weights of new features and eta
actually shrinks the feature weights to make the boosting process more
conservative. [default=0.3] range: [0,1]</p>
</td></tr>
<tr><td><code id="xgboost_classifier_+3A_gamma">gamma</code></td>
<td>
<p>Minimum loss reduction required to make a further partition on
a leaf node of the tree. the larger, the more conservative the algorithm
will be. [default=0]</p>
</td></tr>
<tr><td><code id="xgboost_classifier_+3A_max_depth">max_depth</code></td>
<td>
<p>Maximum depth of a tree, increase this value will make model
more complex / likely to be overfitting. [default=6]</p>
</td></tr>
<tr><td><code id="xgboost_classifier_+3A_min_child_weight">min_child_weight</code></td>
<td>
<p>Minimum sum of instance weight(hessian) needed in
a child. If the tree partition step results in a leaf node with the sum of
instance weight less than min_child_weight, then the building process will
give up further partitioning. In linear regression mode, this simply
corresponds to minimum number of instances needed to be in each node. The
larger, the more conservative the algorithm will be. [default=1]</p>
</td></tr>
<tr><td><code id="xgboost_classifier_+3A_max_delta_step">max_delta_step</code></td>
<td>
<p>Maximum delta step we allow each tree's weight
estimation to be. If the value is set to 0, it means there is no constraint.
If it is set to a positive value, it can help making the update step more
conservative. Usually this parameter is not needed, but it might help in
logistic regression when class is extremely imbalanced. Set it to value
of 1-10 might help control the update. [default=0]</p>
</td></tr>
<tr><td><code id="xgboost_classifier_+3A_grow_policy">grow_policy</code></td>
<td>
<p>Growth policy for fast histogram algorithm.</p>
</td></tr>
<tr><td><code id="xgboost_classifier_+3A_max_bins">max_bins</code></td>
<td>
<p>Maximum number of bins in histogram.</p>
</td></tr>
<tr><td><code id="xgboost_classifier_+3A_subsample">subsample</code></td>
<td>
<p>Subsample ratio of the training instance. Setting it to 0.5
means that XGBoost randomly collected half of the data instances to grow
trees and this will prevent overfitting. [default=1] range:(0,1]</p>
</td></tr>
<tr><td><code id="xgboost_classifier_+3A_colsample_bytree">colsample_bytree</code></td>
<td>
<p>Subsample ratio of columns when constructing each tree.
[default=1] range: (0,1]</p>
</td></tr>
<tr><td><code id="xgboost_classifier_+3A_colsample_bylevel">colsample_bylevel</code></td>
<td>
<p>Subsample ratio of columns for each split, in each
level. [default=1] range: (0,1]</p>
</td></tr>
<tr><td><code id="xgboost_classifier_+3A_lambda">lambda</code></td>
<td>
<p>L2 regularization term on weights, increase this value will
make model more conservative. [default=1]</p>
</td></tr>
<tr><td><code id="xgboost_classifier_+3A_alpha">alpha</code></td>
<td>
<p>L1 regularization term on weights, increase this value will make
model more conservative, defaults to 0.</p>
</td></tr>
<tr><td><code id="xgboost_classifier_+3A_tree_method">tree_method</code></td>
<td>
<p>The tree construction algorithm used in XGBoost. options:
'auto', 'exact' or'approx' [default='auto']</p>
</td></tr>
<tr><td><code id="xgboost_classifier_+3A_sketch_eps">sketch_eps</code></td>
<td>
<p>No longer supported as of XGBoost 1.6.0. Consider using
'max_bins' instead.</p>
</td></tr>
<tr><td><code id="xgboost_classifier_+3A_scale_pos_weight">scale_pos_weight</code></td>
<td>
<p>Control the balance of positive and negative weights,
useful for unbalanced classes. A typical value to consider:
sum(negative cases) / sum(positive cases). [default=1]</p>
</td></tr>
<tr><td><code id="xgboost_classifier_+3A_sample_type">sample_type</code></td>
<td>
<p>Parameter for Dart booster. Type of sampling algorithm.
&quot;uniform&quot;: dropped trees are selected uniformly. &quot;weighted&quot;: dropped trees
are selected in proportion to weight. [default=&quot;uniform&quot;]</p>
</td></tr>
<tr><td><code id="xgboost_classifier_+3A_normalize_type">normalize_type</code></td>
<td>
<p>Parameter of Dart booster. type of normalization
algorithm, options: 'tree', or 'forest'. [default=&quot;tree&quot;]</p>
</td></tr>
<tr><td><code id="xgboost_classifier_+3A_rate_drop">rate_drop</code></td>
<td>
<p>Parameter of Dart booster. dropout rate. [default=0.0]
range: [0.0, 1.0]</p>
</td></tr>
<tr><td><code id="xgboost_classifier_+3A_skip_drop">skip_drop</code></td>
<td>
<p>Parameter of Dart booster. probability of skip dropout.
If a dropout is skipped, new trees are added in the same manner as gbtree.
[default=0.0] range: [0.0, 1.0]</p>
</td></tr>
<tr><td><code id="xgboost_classifier_+3A_lambda_bias">lambda_bias</code></td>
<td>
<p>Parameter of linear booster L2 regularization term on
bias, default 0 (no L1 reg on bias because it is not important.)</p>
</td></tr>
<tr><td><code id="xgboost_classifier_+3A_tree_limit">tree_limit</code></td>
<td>
<p>Limit number of trees in the prediction; defaults to 0
(use all trees.)</p>
</td></tr>
<tr><td><code id="xgboost_classifier_+3A_num_round">num_round</code></td>
<td>
<p>The number of rounds for boosting.</p>
</td></tr>
<tr><td><code id="xgboost_classifier_+3A_num_workers">num_workers</code></td>
<td>
<p>number of workers used to train xgboost model.
Defaults to 1.</p>
</td></tr>
<tr><td><code id="xgboost_classifier_+3A_nthread">nthread</code></td>
<td>
<p>Number of threads used by per worker. Defaults to 1.</p>
</td></tr>
<tr><td><code id="xgboost_classifier_+3A_use_external_memory">use_external_memory</code></td>
<td>
<p>The tree construction algorithm used in XGBoost.
options: 'auto', 'exact' or 'approx' [default='auto']</p>
</td></tr>
<tr><td><code id="xgboost_classifier_+3A_silent">silent</code></td>
<td>
<p>0 means printing running messages, 1 means silent mode.
default: 0</p>
</td></tr>
<tr><td><code id="xgboost_classifier_+3A_custom_obj">custom_obj</code></td>
<td>
<p>Customized objective function provided by user. Currently
unsupported.</p>
</td></tr>
<tr><td><code id="xgboost_classifier_+3A_custom_eval">custom_eval</code></td>
<td>
<p>Customized evaluation function provided by user. Currently
unsupported.</p>
</td></tr>
<tr><td><code id="xgboost_classifier_+3A_missing">missing</code></td>
<td>
<p>The value treated as missing. default: Float.NaN</p>
</td></tr>
<tr><td><code id="xgboost_classifier_+3A_seed">seed</code></td>
<td>
<p>Random seed for the C++ part of XGBoost and train/test splitting.</p>
</td></tr>
<tr><td><code id="xgboost_classifier_+3A_timeout_request_workers">timeout_request_workers</code></td>
<td>
<p>No longer supported as of XGBoost 1.7.0.</p>
</td></tr>
<tr><td><code id="xgboost_classifier_+3A_checkpoint_path">checkpoint_path</code></td>
<td>
<p>The hdfs folder to load and save checkpoint boosters.</p>
</td></tr>
<tr><td><code id="xgboost_classifier_+3A_checkpoint_interval">checkpoint_interval</code></td>
<td>
<p>Param for set checkpoint interval (&gt;= 1) or disable
checkpoint (-1). E.g. 10 means that the trained model will get checkpointed
every 10 iterations. Note: checkpoint_path must also be set if the checkpoint
interval is greater than 0.</p>
</td></tr>
<tr><td><code id="xgboost_classifier_+3A_objective">objective</code></td>
<td>
<p>Specify the learning task and the corresponding learning
objective. options: reg:linear, reg:logistic, binary:logistic,
binary:logitraw, count:poisson, multi:softmax, multi:softprob, rank:pairwise,
reg:gamma. default: reg:linear.</p>
</td></tr>
<tr><td><code id="xgboost_classifier_+3A_base_score">base_score</code></td>
<td>
<p>Param for initial prediction (aka base margin) column name.
Defaults to 0.5.</p>
</td></tr>
<tr><td><code id="xgboost_classifier_+3A_train_test_ratio">train_test_ratio</code></td>
<td>
<p>Fraction of training points to use for testing.</p>
</td></tr>
<tr><td><code id="xgboost_classifier_+3A_num_early_stopping_rounds">num_early_stopping_rounds</code></td>
<td>
<p>If non-zero, the training will be stopped
after a specified number of consecutive increases in any evaluation metric.</p>
</td></tr>
<tr><td><code id="xgboost_classifier_+3A_objective_type">objective_type</code></td>
<td>
<p>The learning objective type of the specified custom
objective and eval. Corresponding type will be assigned if custom objective
is defined options: regression, classification.</p>
</td></tr>
<tr><td><code id="xgboost_classifier_+3A_eval_metric">eval_metric</code></td>
<td>
<p>Evaluation metrics for validation data, a default metric
will be assigned according to objective(rmse for regression, and error for
classification, mean average precision for ranking). options: rmse, mae,
logloss, error, merror, mlogloss, auc, aucpr, ndcg, map, gamma-deviance</p>
</td></tr>
<tr><td><code id="xgboost_classifier_+3A_maximize_evaluation_metrics">maximize_evaluation_metrics</code></td>
<td>
<p>Whether to maximize evaluation metrics.
Defaults to FALSE (for minization.)</p>
</td></tr>
<tr><td><code id="xgboost_classifier_+3A_num_class">num_class</code></td>
<td>
<p>Number of classes.</p>
</td></tr>
<tr><td><code id="xgboost_classifier_+3A_base_margin_col">base_margin_col</code></td>
<td>
<p>Param for initial prediction (aka base margin) column
name.</p>
</td></tr>
<tr><td><code id="xgboost_classifier_+3A_thresholds">thresholds</code></td>
<td>
<p>Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values &gt; 0 excepting that at most one value may be 0. The class with largest value <code>p/t</code> is predicted, where <code>p</code> is the original probability of that class and <code>t</code> is the class's threshold.</p>
</td></tr>
<tr><td><code id="xgboost_classifier_+3A_weight_col">weight_col</code></td>
<td>
<p>Weight column.</p>
</td></tr>
<tr><td><code id="xgboost_classifier_+3A_features_col">features_col</code></td>
<td>
<p>Features column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by <code><a href="sparklyr.html#topic+ft_r_formula">ft_r_formula</a></code>.</p>
</td></tr>
<tr><td><code id="xgboost_classifier_+3A_label_col">label_col</code></td>
<td>
<p>Label column name. The column should be a numeric column. Usually this column is output by <code><a href="sparklyr.html#topic+ft_r_formula">ft_r_formula</a></code>.</p>
</td></tr>
<tr><td><code id="xgboost_classifier_+3A_prediction_col">prediction_col</code></td>
<td>
<p>Prediction column name.</p>
</td></tr>
<tr><td><code id="xgboost_classifier_+3A_probability_col">probability_col</code></td>
<td>
<p>Column name for predicted class conditional probabilities.</p>
</td></tr>
<tr><td><code id="xgboost_classifier_+3A_raw_prediction_col">raw_prediction_col</code></td>
<td>
<p>Raw prediction (a.k.a. confidence) column name.</p>
</td></tr>
<tr><td><code id="xgboost_classifier_+3A_uid">uid</code></td>
<td>
<p>A character string used to uniquely identify the ML estimator.</p>
</td></tr>
<tr><td><code id="xgboost_classifier_+3A_...">...</code></td>
<td>
<p>Optional arguments; see Details.</p>
</td></tr>
</table>

<hr>
<h2 id='xgboost_regressor'>XGBoost Regressor</h2><span id='topic+xgboost_regressor'></span>

<h3>Description</h3>

<p>XGBoost regressor for Spark.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>xgboost_regressor(
  x,
  formula = NULL,
  eta = 0.3,
  gamma = 0,
  max_depth = 6,
  min_child_weight = 1,
  max_delta_step = 0,
  grow_policy = "depthwise",
  max_bins = 16,
  subsample = 1,
  colsample_bytree = 1,
  colsample_bylevel = 1,
  lambda = 1,
  alpha = 0,
  tree_method = "auto",
  sketch_eps = NULL,
  scale_pos_weight = 1,
  sample_type = "uniform",
  normalize_type = "tree",
  rate_drop = 0,
  skip_drop = 0,
  lambda_bias = 0,
  tree_limit = 0,
  num_round = 1,
  num_workers = 1,
  nthread = 1,
  use_external_memory = FALSE,
  silent = 0,
  custom_obj = NULL,
  custom_eval = NULL,
  missing = NaN,
  seed = 0,
  timeout_request_workers = NULL,
  checkpoint_path = "",
  checkpoint_interval = -1,
  objective = "reg:linear",
  base_score = 0.5,
  train_test_ratio = 1,
  num_early_stopping_rounds = 0,
  objective_type = "regression",
  eval_metric = NULL,
  maximize_evaluation_metrics = FALSE,
  base_margin_col = NULL,
  weight_col = NULL,
  features_col = "features",
  label_col = "label",
  prediction_col = "prediction",
  uid = random_string("xgboost_regressor_"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="xgboost_regressor_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="xgboost_regressor_+3A_formula">formula</code></td>
<td>
<p>Used when <code>x</code> is a <code>tbl_spark</code>. R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see <a href="sparklyr.html#topic+ft_r_formula">ft_r_formula</a> for details.</p>
</td></tr>
<tr><td><code id="xgboost_regressor_+3A_eta">eta</code></td>
<td>
<p>Step size shrinkage used in update to prevents overfitting. After
each boosting step, we can directly get the weights of new features and eta
actually shrinks the feature weights to make the boosting process more
conservative. [default=0.3] range: [0,1]</p>
</td></tr>
<tr><td><code id="xgboost_regressor_+3A_gamma">gamma</code></td>
<td>
<p>Minimum loss reduction required to make a further partition on
a leaf node of the tree. the larger, the more conservative the algorithm
will be. [default=0]</p>
</td></tr>
<tr><td><code id="xgboost_regressor_+3A_max_depth">max_depth</code></td>
<td>
<p>Maximum depth of a tree, increase this value will make model
more complex / likely to be overfitting. [default=6]</p>
</td></tr>
<tr><td><code id="xgboost_regressor_+3A_min_child_weight">min_child_weight</code></td>
<td>
<p>Minimum sum of instance weight(hessian) needed in
a child. If the tree partition step results in a leaf node with the sum of
instance weight less than min_child_weight, then the building process will
give up further partitioning. In linear regression mode, this simply
corresponds to minimum number of instances needed to be in each node. The
larger, the more conservative the algorithm will be. [default=1]</p>
</td></tr>
<tr><td><code id="xgboost_regressor_+3A_max_delta_step">max_delta_step</code></td>
<td>
<p>Maximum delta step we allow each tree's weight
estimation to be. If the value is set to 0, it means there is no constraint.
If it is set to a positive value, it can help making the update step more
conservative. Usually this parameter is not needed, but it might help in
logistic regression when class is extremely imbalanced. Set it to value
of 1-10 might help control the update. [default=0]</p>
</td></tr>
<tr><td><code id="xgboost_regressor_+3A_grow_policy">grow_policy</code></td>
<td>
<p>Growth policy for fast histogram algorithm.</p>
</td></tr>
<tr><td><code id="xgboost_regressor_+3A_max_bins">max_bins</code></td>
<td>
<p>Maximum number of bins in histogram.</p>
</td></tr>
<tr><td><code id="xgboost_regressor_+3A_subsample">subsample</code></td>
<td>
<p>Subsample ratio of the training instance. Setting it to 0.5
means that XGBoost randomly collected half of the data instances to grow
trees and this will prevent overfitting. [default=1] range:(0,1]</p>
</td></tr>
<tr><td><code id="xgboost_regressor_+3A_colsample_bytree">colsample_bytree</code></td>
<td>
<p>Subsample ratio of columns when constructing each tree.
[default=1] range: (0,1]</p>
</td></tr>
<tr><td><code id="xgboost_regressor_+3A_colsample_bylevel">colsample_bylevel</code></td>
<td>
<p>Subsample ratio of columns for each split, in each
level. [default=1] range: (0,1]</p>
</td></tr>
<tr><td><code id="xgboost_regressor_+3A_lambda">lambda</code></td>
<td>
<p>L2 regularization term on weights, increase this value will
make model more conservative. [default=1]</p>
</td></tr>
<tr><td><code id="xgboost_regressor_+3A_alpha">alpha</code></td>
<td>
<p>L1 regularization term on weights, increase this value will make
model more conservative, defaults to 0.</p>
</td></tr>
<tr><td><code id="xgboost_regressor_+3A_tree_method">tree_method</code></td>
<td>
<p>The tree construction algorithm used in XGBoost. options:
'auto', 'exact' or'approx' [default='auto']</p>
</td></tr>
<tr><td><code id="xgboost_regressor_+3A_sketch_eps">sketch_eps</code></td>
<td>
<p>No longer supported as of XGBoost 1.6.0. Consider using
'max_bins' instead.</p>
</td></tr>
<tr><td><code id="xgboost_regressor_+3A_scale_pos_weight">scale_pos_weight</code></td>
<td>
<p>Control the balance of positive and negative weights,
useful for unbalanced classes. A typical value to consider:
sum(negative cases) / sum(positive cases). [default=1]</p>
</td></tr>
<tr><td><code id="xgboost_regressor_+3A_sample_type">sample_type</code></td>
<td>
<p>Parameter for Dart booster. Type of sampling algorithm.
&quot;uniform&quot;: dropped trees are selected uniformly. &quot;weighted&quot;: dropped trees
are selected in proportion to weight. [default=&quot;uniform&quot;]</p>
</td></tr>
<tr><td><code id="xgboost_regressor_+3A_normalize_type">normalize_type</code></td>
<td>
<p>Parameter of Dart booster. type of normalization
algorithm, options: 'tree', or 'forest'. [default=&quot;tree&quot;]</p>
</td></tr>
<tr><td><code id="xgboost_regressor_+3A_rate_drop">rate_drop</code></td>
<td>
<p>Parameter of Dart booster. dropout rate. [default=0.0]
range: [0.0, 1.0]</p>
</td></tr>
<tr><td><code id="xgboost_regressor_+3A_skip_drop">skip_drop</code></td>
<td>
<p>Parameter of Dart booster. probability of skip dropout.
If a dropout is skipped, new trees are added in the same manner as gbtree.
[default=0.0] range: [0.0, 1.0]</p>
</td></tr>
<tr><td><code id="xgboost_regressor_+3A_lambda_bias">lambda_bias</code></td>
<td>
<p>Parameter of linear booster L2 regularization term on
bias, default 0 (no L1 reg on bias because it is not important.)</p>
</td></tr>
<tr><td><code id="xgboost_regressor_+3A_tree_limit">tree_limit</code></td>
<td>
<p>Limit number of trees in the prediction; defaults to 0
(use all trees.)</p>
</td></tr>
<tr><td><code id="xgboost_regressor_+3A_num_round">num_round</code></td>
<td>
<p>The number of rounds for boosting.</p>
</td></tr>
<tr><td><code id="xgboost_regressor_+3A_num_workers">num_workers</code></td>
<td>
<p>number of workers used to train xgboost model.
Defaults to 1.</p>
</td></tr>
<tr><td><code id="xgboost_regressor_+3A_nthread">nthread</code></td>
<td>
<p>Number of threads used by per worker. Defaults to 1.</p>
</td></tr>
<tr><td><code id="xgboost_regressor_+3A_use_external_memory">use_external_memory</code></td>
<td>
<p>The tree construction algorithm used in XGBoost.
options: 'auto', 'exact' or 'approx' [default='auto']</p>
</td></tr>
<tr><td><code id="xgboost_regressor_+3A_silent">silent</code></td>
<td>
<p>0 means printing running messages, 1 means silent mode.
default: 0</p>
</td></tr>
<tr><td><code id="xgboost_regressor_+3A_custom_obj">custom_obj</code></td>
<td>
<p>Customized objective function provided by user. Currently
unsupported.</p>
</td></tr>
<tr><td><code id="xgboost_regressor_+3A_custom_eval">custom_eval</code></td>
<td>
<p>Customized evaluation function provided by user. Currently
unsupported.</p>
</td></tr>
<tr><td><code id="xgboost_regressor_+3A_missing">missing</code></td>
<td>
<p>The value treated as missing. default: Float.NaN</p>
</td></tr>
<tr><td><code id="xgboost_regressor_+3A_seed">seed</code></td>
<td>
<p>Random seed for the C++ part of XGBoost and train/test splitting.</p>
</td></tr>
<tr><td><code id="xgboost_regressor_+3A_timeout_request_workers">timeout_request_workers</code></td>
<td>
<p>No longer supported as of XGBoost 1.7.0.</p>
</td></tr>
<tr><td><code id="xgboost_regressor_+3A_checkpoint_path">checkpoint_path</code></td>
<td>
<p>The hdfs folder to load and save checkpoint boosters.</p>
</td></tr>
<tr><td><code id="xgboost_regressor_+3A_checkpoint_interval">checkpoint_interval</code></td>
<td>
<p>Param for set checkpoint interval (&gt;= 1) or disable
checkpoint (-1). E.g. 10 means that the trained model will get checkpointed
every 10 iterations. Note: checkpoint_path must also be set if the checkpoint
interval is greater than 0.</p>
</td></tr>
<tr><td><code id="xgboost_regressor_+3A_objective">objective</code></td>
<td>
<p>Specify the learning task and the corresponding learning
objective. options: reg:linear, reg:logistic, binary:logistic,
binary:logitraw, count:poisson, multi:softmax, multi:softprob, rank:pairwise,
reg:gamma. default: reg:linear.</p>
</td></tr>
<tr><td><code id="xgboost_regressor_+3A_base_score">base_score</code></td>
<td>
<p>Param for initial prediction (aka base margin) column name.
Defaults to 0.5.</p>
</td></tr>
<tr><td><code id="xgboost_regressor_+3A_train_test_ratio">train_test_ratio</code></td>
<td>
<p>Fraction of training points to use for testing.</p>
</td></tr>
<tr><td><code id="xgboost_regressor_+3A_num_early_stopping_rounds">num_early_stopping_rounds</code></td>
<td>
<p>If non-zero, the training will be stopped
after a specified number of consecutive increases in any evaluation metric.</p>
</td></tr>
<tr><td><code id="xgboost_regressor_+3A_objective_type">objective_type</code></td>
<td>
<p>The learning objective type of the specified custom
objective and eval. Corresponding type will be assigned if custom objective
is defined options: regression, classification.</p>
</td></tr>
<tr><td><code id="xgboost_regressor_+3A_eval_metric">eval_metric</code></td>
<td>
<p>Evaluation metrics for validation data, a default metric
will be assigned according to objective(rmse for regression, and error for
classification, mean average precision for ranking). options: rmse, mae,
logloss, error, merror, mlogloss, auc, aucpr, ndcg, map, gamma-deviance</p>
</td></tr>
<tr><td><code id="xgboost_regressor_+3A_maximize_evaluation_metrics">maximize_evaluation_metrics</code></td>
<td>
<p>Whether to maximize evaluation metrics.
Defaults to FALSE (for minization.)</p>
</td></tr>
<tr><td><code id="xgboost_regressor_+3A_base_margin_col">base_margin_col</code></td>
<td>
<p>Param for initial prediction (aka base margin) column
name.</p>
</td></tr>
<tr><td><code id="xgboost_regressor_+3A_weight_col">weight_col</code></td>
<td>
<p>Weight column.</p>
</td></tr>
<tr><td><code id="xgboost_regressor_+3A_features_col">features_col</code></td>
<td>
<p>Features column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by <code><a href="sparklyr.html#topic+ft_r_formula">ft_r_formula</a></code>.</p>
</td></tr>
<tr><td><code id="xgboost_regressor_+3A_label_col">label_col</code></td>
<td>
<p>Label column name. The column should be a numeric column. Usually this column is output by <code><a href="sparklyr.html#topic+ft_r_formula">ft_r_formula</a></code>.</p>
</td></tr>
<tr><td><code id="xgboost_regressor_+3A_prediction_col">prediction_col</code></td>
<td>
<p>Prediction column name.</p>
</td></tr>
<tr><td><code id="xgboost_regressor_+3A_uid">uid</code></td>
<td>
<p>A character string used to uniquely identify the ML estimator.</p>
</td></tr>
<tr><td><code id="xgboost_regressor_+3A_...">...</code></td>
<td>
<p>Optional arguments; see Details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When <code>x</code> is a <code>tbl_spark</code> and <code>formula</code> (alternatively, <code>response</code> and <code>features</code>) is specified, the function returns a <code>ml_model</code> object wrapping a <code>ml_pipeline_model</code> which contains data pre-processing transformers, the ML predictor, and, for classification models, a post-processing transformer that converts predictions into class labels. For classification, an optional argument <code>predicted_label_col</code> (defaults to <code>"predicted_label"</code>) can be used to specify the name of the predicted label column. In addition to the fitted <code>ml_pipeline_model</code>, <code>ml_model</code> objects also contain a <code>ml_pipeline</code> object where the ML predictor stage is an estimator ready to be fit against data. This is utilized by <code><a href="sparklyr.html#topic+ml_save">ml_save</a></code> with <code>type = "pipeline"</code> to facilitate model refresh workflows.
</p>


<h3>Value</h3>

<p>The object returned depends on the class of <code>x</code>.
</p>

<ul>
<li> <p><code>spark_connection</code>: When <code>x</code> is a <code>spark_connection</code>, the function returns an instance of a <code>ml_estimator</code> object. The object contains a pointer to
a Spark <code>Predictor</code> object and can be used to compose
<code>Pipeline</code> objects.
</p>
</li>
<li> <p><code>ml_pipeline</code>: When <code>x</code> is a <code>ml_pipeline</code>, the function returns a <code>ml_pipeline</code> with
the predictor appended to the pipeline.
</p>
</li>
<li> <p><code>tbl_spark</code>: When <code>x</code> is a <code>tbl_spark</code>, a predictor is constructed then
immediately fit with the input <code>tbl_spark</code>, returning a prediction model.
</p>
</li>
<li> <p><code>tbl_spark</code>, with <code>formula</code>: specified When <code>formula</code>
is specified, the input <code>tbl_spark</code> is first transformed using a
<code>RFormula</code> transformer before being fit by
the predictor. The object returned in this case is a <code>ml_model</code> which is a
wrapper of a <code>ml_pipeline_model</code>.
</p>
</li></ul>


</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
