<!DOCTYPE html><html><head><title>Help for package horseshoe</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {horseshoe}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#Basic.integrand'><p>Helper function for computing the posterior mean, posterior variance</p></a></li>
<li><a href='#Basic.y'><p>Helper function for computing the posterior mean, posterior variance</p></a></li>
<li><a href='#Basic.y.vec'><p>Helper function for computing the posterior mean, posterior variance</p></a></li>
<li><a href='#horseshoe'><p>Function to implement the horseshoe shrinkage prior in Bayesian linear regression</p></a></li>
<li><a href='#HS.MMLE'><p>MMLE for the horseshoe prior for the sparse normal means problem.</p></a></li>
<li><a href='#HS.normal.means'><p>The horseshoe prior for the sparse normal means problem</p></a></li>
<li><a href='#HS.post.mean'><p>Posterior mean for the horseshoe for the normal means problem.</p></a></li>
<li><a href='#HS.post.var'><p>Posterior variance for the horseshoe for the normal means problem.</p></a></li>
<li><a href='#HS.var.select'><p>Variable selection using the horseshoe prior</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Implementation of the Horseshoe Prior</td>
</tr>
<tr>
<td>Version:</td>
<td>0.2.0</td>
</tr>
<tr>
<td>Description:</td>
<td>Contains functions for applying the horseshoe prior to high-
    dimensional linear regression, yielding the posterior mean and credible
    intervals, amongst other things. The key parameter tau can be equipped with
    a prior or estimated via maximum marginal likelihood estimation (MMLE).
    The main function, horseshoe, is for linear regression. In addition, there
    are functions specifically for the sparse normal means problem, allowing
    for faster computation of for example the posterior mean and posterior
    variance. Finally, there is a function available to perform variable
    selection, using either a form of thresholding, or credible intervals.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.1.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>stats</td>
</tr>
<tr>
<td>Suggests:</td>
<td>Hmisc, ggplot2, knitr, rmarkdown</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>LazyData:</td>
<td>false</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>6.1.1</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2019-07-18 09:48:43 UTC; stephanie</td>
</tr>
<tr>
<td>Author:</td>
<td>Stephanie van der Pas [cre, aut],
  James Scott [aut],
  Antik Chakraborty [aut],
  Anirban Bhattacharya [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Stephanie van der Pas &lt;svdpas@math.leidenuniv.nl&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2019-07-18 10:14:05 UTC</td>
</tr>
</table>
<hr>
<h2 id='Basic.integrand'>Helper function for computing the posterior mean, posterior variance</h2><span id='topic+Basic.integrand'></span>

<h3>Description</h3>

<p>Helper function for computing the posterior mean, posterior variance
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Basic.integrand(u, y, k, tau, data.var)
</code></pre>

<hr>
<h2 id='Basic.y'>Helper function for computing the posterior mean, posterior variance</h2><span id='topic+Basic.y'></span>

<h3>Description</h3>

<p>Helper function for computing the posterior mean, posterior variance
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Basic.y(y, tau, k, data.var)
</code></pre>

<hr>
<h2 id='Basic.y.vec'>Helper function for computing the posterior mean, posterior variance</h2><span id='topic+Basic.y.vec'></span>

<h3>Description</h3>

<p>Helper function for computing the posterior mean, posterior variance
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Basic.y.vec(y, tau, k, data.var)
</code></pre>

<hr>
<h2 id='horseshoe'>Function to implement the horseshoe shrinkage prior in Bayesian linear regression</h2><span id='topic+horseshoe'></span>

<h3>Description</h3>

<p>This function employs the algorithm proposed in Bhattacharya et al. (2016).
The global-local scale parameters are updated via a slice sampling scheme given
in the online supplement of Polson et al. (2014). Two different algorithms are
used to compute posterior samples of the <code class="reqn">p*1</code> vector of regression coefficients <code class="reqn">\beta</code>.
The method proposed in Bhattacharya et al. (2016) is used when <code class="reqn">p&gt;n</code>, and the
algorithm provided in Rue (2001) is used for the  case <code class="reqn">p&lt;=n</code>. The function
includes options for full hierarchical Bayes versions with hyperpriors on all
parameters, or empirical Bayes versions where some parameters are taken equal
to a user-selected value.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>horseshoe(y, X, method.tau = c("fixed", "truncatedCauchy", "halfCauchy"),
  tau = 1, method.sigma = c("fixed", "Jeffreys"), Sigma2 = 1,
  burn = 1000, nmc = 5000, thin = 1, alpha = 0.05)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="horseshoe_+3A_y">y</code></td>
<td>
<p>Response, a <code class="reqn">n*1</code> vector.</p>
</td></tr>
<tr><td><code id="horseshoe_+3A_x">X</code></td>
<td>
<p>Matrix of covariates, dimension <code class="reqn">n*p</code>.</p>
</td></tr>
<tr><td><code id="horseshoe_+3A_method.tau">method.tau</code></td>
<td>
<p>Method for handling <code class="reqn">\tau</code>. Select &quot;truncatedCauchy&quot; for full
Bayes with the Cauchy prior truncated to [1/p, 1], &quot;halfCauchy&quot; for full Bayes with
the half-Cauchy prior, or &quot;fixed&quot; to use a fixed value (an empirical Bayes estimate,
for example).</p>
</td></tr>
<tr><td><code id="horseshoe_+3A_tau">tau</code></td>
<td>
<p>Use this argument to pass the (estimated) value of <code class="reqn">\tau</code> in case &quot;fixed&quot;
is selected for method.tau. Not necessary when method.tau is equal to&quot;halfCauchy&quot; or
&quot;truncatedCauchy&quot;. The default (tau = 1) is not suitable for most purposes and should be replaced.</p>
</td></tr>
<tr><td><code id="horseshoe_+3A_method.sigma">method.sigma</code></td>
<td>
<p>Select &quot;Jeffreys&quot; for full Bayes with Jeffrey's prior on the error
variance <code class="reqn">\sigma^2</code>, or &quot;fixed&quot; to use a fixed value (an empirical Bayes
estimate, for example).</p>
</td></tr>
<tr><td><code id="horseshoe_+3A_sigma2">Sigma2</code></td>
<td>
<p>A fixed value for the error variance <code class="reqn">\sigma^2</code>. Not necessary
when method.sigma is equal to &quot;Jeffreys&quot;. Use this argument to pass the (estimated)
value of Sigma2 in case &quot;fixed&quot; is selected for method.sigma. The default (Sigma2 = 1)
is not suitable for most purposes and should be replaced.</p>
</td></tr>
<tr><td><code id="horseshoe_+3A_burn">burn</code></td>
<td>
<p>Number of burn-in MCMC samples. Default is 1000.</p>
</td></tr>
<tr><td><code id="horseshoe_+3A_nmc">nmc</code></td>
<td>
<p>Number of posterior draws to be saved. Default is 5000.</p>
</td></tr>
<tr><td><code id="horseshoe_+3A_thin">thin</code></td>
<td>
<p>Thinning parameter of the chain. Default is 1 (no thinning).</p>
</td></tr>
<tr><td><code id="horseshoe_+3A_alpha">alpha</code></td>
<td>
<p>Level for the credible intervals. For example, alpha = 0.05 results in
95% credible intervals.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The model is:
</p>
<p style="text-align: center;"><code class="reqn">y=X\beta+\epsilon, \epsilon \sim N(0,\sigma^2)</code>
</p>

<p>The full Bayes version of the horseshoe, with hyperpriors on both <code class="reqn">\tau</code> and <code class="reqn">\sigma^2</code> is:
</p>
<p style="text-align: center;"><code class="reqn">\beta_j \sim N(0,\sigma^2 \lambda_j^2 \tau^2)</code>
</p>

<p style="text-align: center;"><code class="reqn">\lambda_j \sim Half-Cauchy(0,1), \tau \sim Half-Cauchy (0,1)</code>
</p>

<p style="text-align: center;"><code class="reqn">\sigma^2 \sim 1/\sigma^2</code>
</p>

<p>There is an option for a truncated Half-Cauchy prior (truncated to [1/p, 1]) on <code class="reqn">\tau</code>.
Empirical Bayes versions are available as well, where <code class="reqn">\tau</code> and/or
<code class="reqn">\sigma^2</code> are taken equal to fixed values, possibly estimated using the data.
</p>


<h3>Value</h3>

<table>
<tr><td><code>BetaHat</code></td>
<td>
<p>Posterior mean of Beta, a <code class="reqn">p</code> by 1 vector.</p>
</td></tr>
<tr><td><code>LeftCI</code></td>
<td>
<p>The left bounds of the credible intervals.</p>
</td></tr>
<tr><td><code>RightCI</code></td>
<td>
<p>The right bounds of the credible intervals.</p>
</td></tr>
<tr><td><code>BetaMedian</code></td>
<td>
<p>Posterior median of Beta, a <code class="reqn">p</code> by 1 vector.</p>
</td></tr>
<tr><td><code>Sigma2Hat</code></td>
<td>
<p>Posterior mean of error variance <code class="reqn">\sigma^2</code>. If method.sigma =
&quot;fixed&quot; is used, this value will be equal to the user-selected value of Sigma2
passed to the function.</p>
</td></tr>
<tr><td><code>TauHat</code></td>
<td>
<p>Posterior mean of global scale parameter tau, a positive scalar.
If method.tau = &quot;fixed&quot; is used, this value will be equal to the user-selected value
of tau passed to the function.</p>
</td></tr>
<tr><td><code>BetaSamples</code></td>
<td>
<p>Posterior samples of Beta.</p>
</td></tr>
<tr><td><code>TauSamples</code></td>
<td>
<p>Posterior samples of tau.</p>
</td></tr>
<tr><td><code>Sigma2Samples</code></td>
<td>
<p>Posterior samples of Sigma2.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Bhattacharya A., Chakraborty A., and Mallick B.K (2016), Fast sampling
with Gaussian scale-mixture priors in high-dimensional regression.
Biometrika 103(4), 985–991.
</p>
<p>Polson, N.G., Scott, J.G. and Windle, J. (2014) The Bayesian Bridge.
Journal of Royal Statistical Society, B, 76(4), 713-733.
</p>
<p>Rue, H. (2001). Fast sampling of Gaussian Markov random fields. Journal of the Royal
Statistical Society: Series B (Statistical Methodology) 63, 325–338.
</p>
<p>Carvalho, C. M., Polson, N. G., and Scott, J. G. (2010), The Horseshoe
Estimator for Sparse Signals. Biometrika 97(2), 465–480.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+HS.normal.means">HS.normal.means</a></code> for a faster version specifically for the sparse
normal means problem (design matrix X equal to identity matrix) and
<code><a href="#topic+HS.post.mean">HS.post.mean</a></code> for a fast way to estimate the posterior mean in the sparse
normal means problem when a value for tau is available.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: #In this example, there are no relevant predictors
#20 observations, 30 predictors (betas)
y &lt;- rnorm(20)
X &lt;- matrix(rnorm(20*30) , 20)
res &lt;- horseshoe(y, X, method.tau = "truncatedCauchy", method.sigma = "Jeffreys")

plot(y, X%*%res$BetaHat) #plot predicted values against the observed data
res$TauHat #posterior mean of tau
HS.var.select(res, y, method = "intervals") #selected betas
#Ideally, none of the betas is selected (all zeros)
#Plot the credible intervals
library(Hmisc)
xYplot(Cbind(res$BetaHat, res$LeftCI, res$RightCI) ~ 1:30)

## End(Not run)

## Not run:  #The horseshoe applied to the sparse normal means problem
# (note that HS.normal.means is much faster in this case)
X &lt;- diag(100)
beta &lt;- c(rep(0, 80), rep(8, 20))
y &lt;- beta + rnorm(100)
res2 &lt;- horseshoe(y, X, method.tau = "truncatedCauchy", method.sigma = "Jeffreys")
#Plot predicted values against the observed data (signals in blue)
plot(y, X%*%res2$BetaHat, col = c(rep("black", 80), rep("blue", 20)))
res2$TauHat #posterior mean of tau
HS.var.select(res2, y, method = "intervals") #selected betas
#Ideally, the final 20 predictors are selected
#Plot the credible intervals
library(Hmisc)
xYplot(Cbind(res2$BetaHat, res2$LeftCI, res2$RightCI) ~ 1:100)

## End(Not run)

</code></pre>

<hr>
<h2 id='HS.MMLE'>MMLE for the horseshoe prior for the sparse normal means problem.</h2><span id='topic+HS.MMLE'></span>

<h3>Description</h3>

<p>Compute the marginal maximum likelihood estimator (MMLE) of tau for the
horseshoe for the normal means problem (i.e. linear regression with the
design matrix equal to the identity matrix). The MMLE is explained and
studied in Van der Pas et al. (2016).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>HS.MMLE(y, Sigma2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="HS.MMLE_+3A_y">y</code></td>
<td>
<p>The data, a <code class="reqn">n*1</code> vector.</p>
</td></tr>
<tr><td><code id="HS.MMLE_+3A_sigma2">Sigma2</code></td>
<td>
<p>The variance of the data.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The normal means model is:
</p>
<p style="text-align: center;"><code class="reqn">y_i=\beta_i+\epsilon_i, \epsilon_i \sim N(0,\sigma^2)</code>
</p>

<p>And the horseshoe prior:
</p>
<p style="text-align: center;"><code class="reqn">\beta_j \sim N(0,\sigma^2 \lambda_j^2 \tau^2)</code>
</p>

<p style="text-align: center;"><code class="reqn">\lambda_j \sim Half-Cauchy(0,1).</code>
</p>

<p>This function estimates <code class="reqn">\tau</code>. A plug-in value of <code class="reqn">\sigma^2</code> is used.
</p>


<h3>Value</h3>

<p>The MMLE for the parameter tau of the horseshoe.
</p>


<h3>Note</h3>

<p>Requires a minimum of 2 observations. May return an error for
vectors of length larger than 400 if the truth  is very sparse. In that
case, try <code><a href="#topic+HS.normal.means">HS.normal.means</a></code>.
</p>


<h3>References</h3>

<p>van der Pas, S.L., Szabo, B., and van der Vaart, A. (2017), Uncertainty
quantification for the horseshoe (with discussion). Bayesian Analysis
12(4), 1221-1274.
</p>
<p>van der Pas, S.L., Szabo, B., and van der Vaart A. (2017), Adaptive
posterior contraction rates for the horseshoe. Electronic Journal of
Statistics 10(1), 3196-3225.
</p>


<h3>See Also</h3>

<p>The estimated value of <code class="reqn">\tau</code> can be plugged into <code><a href="#topic+HS.post.mean">HS.post.mean</a></code>
to obtain the posterior mean, and into <code><a href="#topic+HS.post.var">HS.post.var</a></code> to obtain the posterior
variance. These functions are all for empirical Bayes; if a full Bayes version with a hyperprior
on <code class="reqn">\tau</code> is preferred, see <code><a href="#topic+HS.normal.means">HS.normal.means</a></code> for the normal means problem, or
<code><a href="#topic+horseshoe">horseshoe</a></code> for linear regression.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: #Example with 5 signals, rest is noise
truth &lt;- c(rep(0, 95), rep(8, 5))
y &lt;-  truth + rnorm(100)
(tau.hat &lt;- HS.MMLE(y, 1)) #returns estimate of tau
plot(y, HS.post.mean(y, tau.hat, 1)) #plot estimates against the data

## End(Not run)
## Not run: #Example where the data variance is estimated first
truth &lt;- c(rep(0, 950), rep(8, 50))
y &lt;-  truth + rnorm(100, mean = 0, sd = sqrt(2))
sigma2.hat &lt;- var(y)
(tau.hat &lt;- HS.MMLE(y, sigma2.hat)) #returns estimate of tau
plot(y, HS.post.mean(y, tau.hat, sigma2.hat)) #plot estimates against the data

## End(Not run)

</code></pre>

<hr>
<h2 id='HS.normal.means'>The horseshoe prior for the sparse normal means problem</h2><span id='topic+HS.normal.means'></span>

<h3>Description</h3>

<p>Apply the horseshoe prior to the normal means problem
(i.e. linear regression with the design matrix equal to the identity matrix).
Computes the posterior mean, median and credible intervals. There are options for
empirical Bayes (estimate of tau and or Sigma2 plugged in) and full Bayes (truncated
or non-truncated half-Cauchy on tau, Jeffrey's prior on Sigma2). For the full Bayes
version, the truncated half-Cauchy prior is recommended by Van der Pas et al. (2016).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>HS.normal.means(y, method.tau = c("fixed", "truncatedCauchy",
  "halfCauchy"), tau = 1, method.sigma = c("fixed", "Jeffreys"),
  Sigma2 = 1, burn = 1000, nmc = 5000, alpha = 0.05)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="HS.normal.means_+3A_y">y</code></td>
<td>
<p>The data. A <code class="reqn">n*1</code> vector.</p>
</td></tr>
<tr><td><code id="HS.normal.means_+3A_method.tau">method.tau</code></td>
<td>
<p>Method for handling <code class="reqn">\tau</code>. Select &quot;fixed&quot; to plug in an
estimate of tau (empirical Bayes), &quot;truncatedCauchy&quot; for the half-
Cauchy prior truncated to [1/n, 1], or &quot;halfCauchy&quot; for a
non-truncated half-Cauchy prior. The truncated Cauchy prior is recommended over
the non-truncated version.</p>
</td></tr>
<tr><td><code id="HS.normal.means_+3A_tau">tau</code></td>
<td>
<p>Use this argument to pass the (estimated) value of <code class="reqn">\tau</code> in case &quot;fixed&quot;
is selected for method.tau. Not necessary when method.tau is equal to&quot;halfCauchy&quot; or
&quot;truncatedCauchy&quot;. The function <code><a href="#topic+HS.MMLE">HS.MMLE</a></code> can be used to compute an
estimate of tau. The default (tau = 1) is not suitable for most purposes and should
be replaced.</p>
</td></tr>
<tr><td><code id="HS.normal.means_+3A_method.sigma">method.sigma</code></td>
<td>
<p>Select &quot;fixed&quot; for a fixed error variance, or &quot;Jeffreys&quot;
to use Jeffrey's prior.</p>
</td></tr>
<tr><td><code id="HS.normal.means_+3A_sigma2">Sigma2</code></td>
<td>
<p>The variance of the data - only necessary when &quot;fixed&quot; is
selected for method.sigma. The default (Sigma2 = 1) is not suitable for
most purposes and should be replaced.</p>
</td></tr>
<tr><td><code id="HS.normal.means_+3A_burn">burn</code></td>
<td>
<p>Number of samples used for burn-in. Default is 1000.</p>
</td></tr>
<tr><td><code id="HS.normal.means_+3A_nmc">nmc</code></td>
<td>
<p>Number of MCMC samples taken after burn-in. Default is 5000.</p>
</td></tr>
<tr><td><code id="HS.normal.means_+3A_alpha">alpha</code></td>
<td>
<p>The level for the credible intervals. E.g. alpha = 0.05 yields
95% credible intervals</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The normal means model is:
</p>
<p style="text-align: center;"><code class="reqn">y_i=\beta_i+\epsilon_i, \epsilon_i \sim N(0,\sigma^2)</code>
</p>

<p>And the horseshoe prior:
</p>
<p style="text-align: center;"><code class="reqn">\beta_j \sim N(0,\sigma^2 \lambda_j^2 \tau^2)</code>
</p>

<p style="text-align: center;"><code class="reqn">\lambda_j \sim Half-Cauchy(0,1).</code>
</p>

<p>Estimates of <code class="reqn">\tau</code> and <code class="reqn">\sigma^2</code> may be plugged in (empirical Bayes), or those
parameters are equipped with hyperpriors (full Bayes).
</p>


<h3>Value</h3>

<table>
<tr><td><code>BetaHat</code></td>
<td>
<p>The posterior mean (horseshoe estimator) for each of the datapoints.</p>
</td></tr>
<tr><td><code>LeftCI</code></td>
<td>
<p>The left bounds of the credible intervals.</p>
</td></tr>
<tr><td><code>RightCI</code></td>
<td>
<p>The right bounds of the credible intervals.</p>
</td></tr>
<tr><td><code>BetaMedian</code></td>
<td>
<p>Posterior median of Beta, a <code class="reqn">n</code> by 1 vector.</p>
</td></tr>
<tr><td><code>Sigma2Hat</code></td>
<td>
<p>Posterior mean of error variance <code class="reqn">\sigma^2</code>. If method.sigma =
&quot;fixed&quot; is used, this value will be equal to the user-selected value of Sigma2
passed to the function.</p>
</td></tr>
<tr><td><code>TauHat</code></td>
<td>
<p>Posterior mean of global scale parameter tau, a positive scalar.
If method.tau = &quot;fixed&quot; is used, this value will be equal to the user-selected value
of tau passed to the function.</p>
</td></tr>
<tr><td><code>BetaSamples</code></td>
<td>
<p>Posterior samples of Beta.</p>
</td></tr>
<tr><td><code>TauSamples</code></td>
<td>
<p>Posterior samples of tau.</p>
</td></tr>
<tr><td><code>Sigma2Samples</code></td>
<td>
<p>Posterior samples of Sigma2.</p>
</td></tr>
</table>


<h3>References</h3>

<p>van der Pas, S.L., Szabo, B., and van der Vaart, A. (2017), Uncertainty
quantification for the horseshoe (with discussion). Bayesian Analysis
12(4), 1221-1274.
</p>
<p>van der Pas, S.L., Szabo, B., and van der Vaart A. (2017), Adaptive
posterior contraction rates for the horseshoe. Electronic Journal of
Statistics 10(1), 3196-3225.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+HS.post.mean">HS.post.mean</a></code> for a fast way to compute the posterior mean
if an estimate of tau is available. <code><a href="#topic+horseshoe">horseshoe</a></code> for linear regression.
<code><a href="#topic+HS.var.select">HS.var.select</a></code> to perform variable selection.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#Empirical Bayes example with 20 signals, rest is noise
#Posterior mean for the signals is plotted
#And variable selection is performed using the credible intervals
#And the credible intervals are plotted
truth &lt;- c(rep(0, 80), rep(8, 20))
data &lt;-  truth + rnorm(100, 1)
tau.hat &lt;- HS.MMLE(data, Sigma2 = 1)
res.HS1 &lt;- HS.normal.means(data, method.tau = "fixed", tau = tau.hat,
method.sigma = "fixed", Sigma2 = 1)
#Plot the posterior mean against the data (signals in blue)
plot(data, res.HS1$BetaHat, col = c(rep("black", 80), rep("blue", 20)))
#Find the selected betas (ideally, the last 20 are equal to 1)
HS.var.select(res.HS1, data, method = "intervals")
#Plot the credible intervals
library(Hmisc)
xYplot(Cbind(res.HS1$BetaHat, res.HS1$LeftCI, res.HS1$RightCI) ~ 1:100)


#Full Bayes example with 20 signals, rest is noise
#Posterior mean for the signals is plotted
#And variable selection is performed using the credible intervals
#And the credible intervals are plotted
truth &lt;- c(rep(0, 80), rep(8, 20))
data &lt;-  truth + rnorm(100, 3)
res.HS2 &lt;- HS.normal.means(data, method.tau = "truncatedCauchy", method.sigma = "Jeffreys")
#Plot the posterior mean against the data (signals in blue)
plot(data, res.HS2$BetaHat, col = c(rep("black", 80), rep("blue", 20)))
#Find the selected betas (ideally, the last 20 are equal to 1)
HS.var.select(res.HS2, data, method = "intervals")
#Plot the credible intervals
library(Hmisc)
xYplot(Cbind(res.HS2$BetaHat, res.HS2$LeftCI, res.HS2$RightCI) ~ 1:100)
</code></pre>

<hr>
<h2 id='HS.post.mean'>Posterior mean for the horseshoe for the normal means problem.</h2><span id='topic+HS.post.mean'></span>

<h3>Description</h3>

<p>Compute the posterior mean for the horseshoe for the normal means problem
(i.e. linear regression with the design matrix equal to the identity matrix),
for a fixed value of tau, without using MCMC, leading to a quick estimate of
the underlying parameters (betas). Details on computation are given
in Carvalho et al. (2010) and Van der Pas et al. (2014).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>HS.post.mean(y, tau, Sigma2 = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="HS.post.mean_+3A_y">y</code></td>
<td>
<p>The data. An <code class="reqn">n*1</code> vector.</p>
</td></tr>
<tr><td><code id="HS.post.mean_+3A_tau">tau</code></td>
<td>
<p>Value for tau. Warning: tau should be greater than 1/450.</p>
</td></tr>
<tr><td><code id="HS.post.mean_+3A_sigma2">Sigma2</code></td>
<td>
<p>The variance of the data.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The normal means model is:
</p>
<p style="text-align: center;"><code class="reqn">y_i=\beta_i+\epsilon_i, \epsilon_i \sim N(0,\sigma^2)</code>
</p>

<p>And the horseshoe prior:
</p>
<p style="text-align: center;"><code class="reqn">\beta_j \sim N(0,\sigma^2 \lambda_j^2 \tau^2)</code>
</p>

<p style="text-align: center;"><code class="reqn">\lambda_j \sim Half-Cauchy(0,1).</code>
</p>

<p>If <code class="reqn">\tau</code> and <code class="reqn">\sigma^2</code> are known, the posterior mean can be computed without
using MCMC.
</p>


<h3>Value</h3>

<p>The posterior mean (horseshoe estimator) for each of the datapoints.
</p>


<h3>References</h3>

<p>Carvalho, C. M., Polson, N. G., and Scott, J. G. (2010), The horseshoe
estimator for sparse signals. Biometrika 97(2), 465–480.
</p>
<p>van der Pas, S. L., Kleijn, B. J. K., and van der Vaart, A. W. (2014), The horseshoe
estimator: Posterior concentration around nearly black vectors. Electronic
Journal of Statistics 8(2), 2585–2618.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+HS.post.var">HS.post.var</a></code> to compute the posterior variance. See
<code><a href="#topic+HS.normal.means">HS.normal.means</a></code> for an implementation that does use MCMC, and
returns credible intervals as well as the posterior mean (and other quantities).
See <code><a href="#topic+horseshoe">horseshoe</a></code> for linear regression.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
#Plot the posterior mean for a range of deterministic values
y &lt;- seq(-5, 5, 0.05)
plot(y, HS.post.mean(y, tau = 0.5, Sigma2 = 1))

#Example with 20 signals, rest is noise
#Posterior mean for the signals is plotted in blue
truth &lt;- c(rep(0, 80), rep(8, 20))
data &lt;-  truth + rnorm(100)
tau.example &lt;- HS.MMLE(data, 1)
plot(data, HS.post.mean(data, tau.example, 1),
 col = c(rep("black", 80), rep("blue", 20)))


</code></pre>

<hr>
<h2 id='HS.post.var'>Posterior variance for the horseshoe for the normal means problem.</h2><span id='topic+HS.post.var'></span>

<h3>Description</h3>

<p>Compute the posterior variance for the horseshoe for the normal means problem
(i.e. linear regression with the design matrix equal to the identity matrix),
for a fixed value of tau, without using MCMC. Details on computation are given
in Carvalho et al. (2010) and Van der Pas et al. (2014).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>HS.post.var(y, tau, Sigma2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="HS.post.var_+3A_y">y</code></td>
<td>
<p>The data. An <code class="reqn">n*1</code> vector.</p>
</td></tr>
<tr><td><code id="HS.post.var_+3A_tau">tau</code></td>
<td>
<p>Value for tau. Tau should be greater than 1/450.</p>
</td></tr>
<tr><td><code id="HS.post.var_+3A_sigma2">Sigma2</code></td>
<td>
<p>The variance of the data.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The normal means model is:
</p>
<p style="text-align: center;"><code class="reqn">y_i=\beta_i+\epsilon_i, \epsilon_i \sim N(0,\sigma^2)</code>
</p>

<p>And the horseshoe prior:
</p>
<p style="text-align: center;"><code class="reqn">\beta_j \sim N(0,\sigma^2 \lambda_j^2 \tau^2)</code>
</p>

<p style="text-align: center;"><code class="reqn">\lambda_j \sim Half-Cauchy(0,1).</code>
</p>

<p>If <code class="reqn">\tau</code> and <code class="reqn">\sigma^2</code> are known, the posterior variance can be computed without
using MCMC.
</p>


<h3>Value</h3>

<p>The posterior variance for each of the datapoints.
</p>


<h3>References</h3>

<p>Carvalho, C. M., Polson, N. G., and Scott, J. G. (2010), The horseshoe
estimator for sparse signals. Biometrika 97(2), 465–480.
</p>
<p>van der Pas, S. L., Kleijn, B. J. K., and van der Vaart, A. W. (2014), The horseshoe
estimator: Posterior concentration around nearly black vectors. Electronic
Journal of Statistics 8(2), 2585–2618.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+HS.post.mean">HS.post.mean</a></code> to compute the posterior mean. See
<code><a href="#topic+HS.normal.means">HS.normal.means</a></code> for an implementation that does use MCMC, and
returns credible intervals as well as the posterior mean (and other quantities).
See <code><a href="#topic+horseshoe">horseshoe</a></code> for linear regression.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
#Plot the posterior variance for a range of deterministic values
y &lt;- seq(-8, 8, 0.05)
plot(y, HS.post.var(y, tau = 0.05, Sigma2 = 1))

#Example with 20 signals, rest is noise
#Posterior variance for the signals is plotted in blue
#Posterior variance for the noise is plotted in black
truth &lt;- c(rep(0, 80), rep(8, 20))
data &lt;-  truth + rnorm(100)
tau.example &lt;- HS.MMLE(data, 1)
plot(data, HS.post.var(data, tau.example, 1),
 col = c(rep("black", 80), rep("blue", 20)) )

</code></pre>

<hr>
<h2 id='HS.var.select'>Variable selection using the horseshoe prior</h2><span id='topic+HS.var.select'></span>

<h3>Description</h3>

<p>The function implements two methods to perform variable selection. The first checks
whether 0 is contained in the credible set (see Van der Pas et al. (2016)). The second
is only intended for the sparse normal means problem (regression with identity matrix).
It is described in Carvalho et al. (2010). The horseshoe posterior mean can be written
as <code class="reqn">c_i y_i</code>, with <code class="reqn">y_i</code> the observation. A variable is selected if
<code class="reqn">c_i \geq c</code>, where <code class="reqn">c</code> is a user-specified threshold.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>HS.var.select(hsobject, y, method = c("intervals", "threshold"),
  threshold = 0.5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="HS.var.select_+3A_hsobject">hsobject</code></td>
<td>
<p>The outcome from one of the horseshoe functions <code><a href="#topic+horseshoe">horseshoe</a></code>
or <code><a href="#topic+HS.normal.means">HS.normal.means</a></code>.</p>
</td></tr>
<tr><td><code id="HS.var.select_+3A_y">y</code></td>
<td>
<p>The data.</p>
</td></tr>
<tr><td><code id="HS.var.select_+3A_method">method</code></td>
<td>
<p>Use &quot;intervals&quot; to perform variable selection using the credible sets
(at the level specified when creating the hsobject), &quot;threshold&quot; to perform variable
selection using the thresholding procedure (only for the sparse normal means problem).</p>
</td></tr>
<tr><td><code id="HS.var.select_+3A_threshold">threshold</code></td>
<td>
<p>Threshold for the thresholding procedure. Default is 0.5.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of zeroes and ones. The ones correspond to the selected variables.
</p>


<h3>References</h3>

<p>van der Pas, S.L., Szabo, B., and van der Vaart, A. (2017), Uncertainty
quantification for the horseshoe (with discussion). Bayesian Analysis
12(4), 1221-1274.
</p>
<p>van der Pas, S.L., Szabo, B., and van der Vaart A. (2017), Adaptive
posterior contraction rates for the horseshoe. Electronic Journal of
Statistics 10(1), 3196-3225.
</p>
<p>Carvalho, C. M., Polson, N. G., and Scott, J. G. (2010), The Horseshoe
Estimator for Sparse Signals. Biometrika 97(2), 465–480.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+horseshoe">horseshoe</a></code> and <code><a href="#topic+HS.normal.means">HS.normal.means</a></code> to obtain the required
hsobject.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#Example with 20 signals (last 20 entries), rest is noise
truth &lt;- c(rep(0, 80), rep(8, 20))
data &lt;-  truth + rnorm(100)
horseshoe.results &lt;- HS.normal.means(data, method.tau = "truncatedCauchy",
 method.sigma = "fixed")
#Using credible sets. Ideally, the first 80 entries are equal to 0,
#and the last 20 entries equal to 1.
HS.var.select(horseshoe.results, data, method = "intervals")
#Using thresholding. Ideally, the first 80 entries are equal to 0,
#and the last 20 entries equal to 1.
HS.var.select(horseshoe.results, data, method = "threshold")

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
