<!DOCTYPE html><html><head><title>Help for package CAISEr</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {CAISEr}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#boot_sdm'><p>Bootstrap the sampling distribution of the mean</p></a></li>
<li><a href='#calc_instances'><p>Calculates number of instances for the comparison of multiple algorithms</p></a></li>
<li><a href='#calc_nreps'><p>Determine sample sizes for a set of algorithms on a single problem instance</p></a></li>
<li><a href='#calc_se'><p>Calculates the standard error for simple and percent differences</p></a></li>
<li><a href='#consolidate_partial_results'><p>Consolidate results from partial files</p></a></li>
<li><a href='#dummyalgo'><p>Dummy algorithm routine to test the sampling procedures</p></a></li>
<li><a href='#dummyinstance'><p>Dummy instance routine to test the sampling procedures</p></a></li>
<li><a href='#example_SANN'><p>Simulated annealing (for testing/examples)</p></a></li>
<li><a href='#get_observations'><p>Run an algorithm on a problem.</p></a></li>
<li><a href='#plot.CAISEr'><p>plot.CAISEr</p></a></li>
<li><a href='#plot.nreps'><p>plot.nreps</p></a></li>
<li><a href='#print.CAISEr'><p>print.CAISEr</p></a></li>
<li><a href='#run_experiment'><p>Run a full experiment for comparing multiple algorithms using multiple</p>
instances</a></li>
<li><a href='#se_boot'><p>Bootstrap standard errors</p></a></li>
<li><a href='#se_param'><p>Parametric standard errors</p></a></li>
<li><a href='#summary.CAISEr'><p>summary.CAISEr</p></a></li>
<li><a href='#summary.nreps'><p>summary.nreps</p></a></li>
<li><a href='#TSP.dist'><p>TSP instance generator (for testing/examples)</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Comparison of Algorithms with Iterative Sample Size Estimation</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.17</td>
</tr>
<tr>
<td>Date:</td>
<td>2022-11-16</td>
</tr>
<tr>
<td>Description:</td>
<td>Functions for performing experimental comparisons of algorithms 
             using adequate sample sizes for power and accuracy. Implements the 
             methodology originally presented in Campelo and Takahashi (2019) 
             &lt;<a href="https://doi.org/10.1007%2Fs10732-018-9396-7">doi:10.1007/s10732-018-9396-7</a>&gt; 
             for the comparison of two algorithms, and later generalised in 
             Campelo and Wanner (Submitted, 2019) &lt;<a href="https://doi.org/10.48550/arXiv.1908.01720">doi:10.48550/arXiv.1908.01720</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>assertthat (&ge; 0.2.1), parallel (&ge; 3.5.1), pbmcapply (&ge;
1.4.1), ggplot2 (&ge; 3.1.1), gridExtra (&ge; 2.3)</td>
</tr>
<tr>
<td>Suggests:</td>
<td>smoof, knitr, rmarkdown, car, dplyr, pkgdown</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.0.2</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://fcampelo.github.io/CAISEr/">https://fcampelo.github.io/CAISEr/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/fcampelo/CAISEr/issues">https://github.com/fcampelo/CAISEr/issues</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-11-16 17:15:32 UTC; campelof</td>
</tr>
<tr>
<td>Author:</td>
<td>Felipe Campelo [aut, cre],
  Fernanda Takahashi [ctb],
  Elizabeth Wanner [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Felipe Campelo &lt;fcampelo@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-11-16 21:10:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='boot_sdm'>Bootstrap the sampling distribution of the mean</h2><span id='topic+boot_sdm'></span>

<h3>Description</h3>

<p>Bootstraps the sampling distribution of the means for a given vector of observations
</p>


<h3>Usage</h3>

<pre><code class='language-R'>boot_sdm(x, boot.R = 999, ncpus = 1, seed = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="boot_sdm_+3A_x">x</code></td>
<td>
<p>vector of observations</p>
</td></tr>
<tr><td><code id="boot_sdm_+3A_boot.r">boot.R</code></td>
<td>
<p>number of bootstrap resamples</p>
</td></tr>
<tr><td><code id="boot_sdm_+3A_ncpus">ncpus</code></td>
<td>
<p>number of cores to use</p>
</td></tr>
<tr><td><code id="boot_sdm_+3A_seed">seed</code></td>
<td>
<p>seed for the PRNG</p>
</td></tr>
</table>


<h3>Value</h3>

<p>vector of bootstrap estimates of the sample mean
</p>


<h3>References</h3>


<ul>
<li><p> A.C. Davison, D.V. Hinkley:
Bootstrap methods and their application. Cambridge University Press (1997)
</p>
</li>
<li><p> F. Campelo, F. Takahashi:
Sample size estimation for power and accuracy in the experimental
comparison of algorithms. Journal of Heuristics 25(2):305-338, 2019.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Felipe Campelo (<a href="mailto:fcampelo@ufmg.br">fcampelo@ufmg.br</a>,
<a href="mailto:f.campelo@aston.ac.uk">f.campelo@aston.ac.uk</a>)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- rnorm(15, mean = 4, sd = 1)
my.sdm &lt;- boot_sdm(x)
hist(my.sdm, breaks = 30)
qqnorm(my.sdm, pch = 20)

x &lt;- runif(12)
my.sdm &lt;- boot_sdm(x)
qqnorm(my.sdm, pch = 20)

# Convergence of the SDM to a Normal distribution as sample size is increased
X &lt;- rchisq(1000, df = 3)
x1 &lt;- rchisq(10, df = 3)
x2 &lt;- rchisq(20, df = 3)
x3 &lt;- rchisq(40, df = 3)
par(mfrow = c(2, 2))
plot(density(X), main = "Estimated pop distribution");
hist(boot_sdm(x1), breaks = 25, main = "SDM, n = 10")
hist(boot_sdm(x2), breaks = 25, main = "SDM, n = 20")
hist(boot_sdm(x3), breaks = 25, main = "SDM, n = 40")
par(mfrow = c(1, 1))
</code></pre>

<hr>
<h2 id='calc_instances'>Calculates number of instances for the comparison of multiple algorithms</h2><span id='topic+calc_instances'></span>

<h3>Description</h3>

<p>Calculates either the number of instances, or the power(s) of the
comparisons of multiple algorithms.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calc_instances(
  ncomparisons,
  d,
  ninstances = NULL,
  power = NULL,
  sig.level = 0.05,
  alternative.side = "two.sided",
  test = "t.test",
  power.target = "mean"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="calc_instances_+3A_ncomparisons">ncomparisons</code></td>
<td>
<p>number of comparisons planned</p>
</td></tr>
<tr><td><code id="calc_instances_+3A_d">d</code></td>
<td>
<p>minimally relevant effect size (MRES, expressed as a standardized
effect size, i.e., &quot;deviation from H0&quot; / &quot;standard deviation&quot;)</p>
</td></tr>
<tr><td><code id="calc_instances_+3A_ninstances">ninstances</code></td>
<td>
<p>the number of instances to be used in the experiment.</p>
</td></tr>
<tr><td><code id="calc_instances_+3A_power">power</code></td>
<td>
<p>target power for the comparisons (see <code>Details</code>)</p>
</td></tr>
<tr><td><code id="calc_instances_+3A_sig.level">sig.level</code></td>
<td>
<p>desired family-wise significance level (alpha) for the
experiment</p>
</td></tr>
<tr><td><code id="calc_instances_+3A_alternative.side">alternative.side</code></td>
<td>
<p>type of alternative hypothesis to be performed
(&quot;two.sided&quot; or &quot;one.sided&quot;)</p>
</td></tr>
<tr><td><code id="calc_instances_+3A_test">test</code></td>
<td>
<p>type of test to be used
(&quot;t.test&quot;, &quot;wilcoxon&quot; or &quot;binomial&quot;)</p>
</td></tr>
<tr><td><code id="calc_instances_+3A_power.target">power.target</code></td>
<td>
<p>which comparison should have the desired <code>power</code>?
Accepts &quot;mean&quot;, &quot;median&quot;, or &quot;worst.case&quot; (this last one
is equivalent to the Bonferroni correction).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The main use of this routine uses the closed formula of the t-test to
calculate the number of instances required for the comparison of pairs of
algorithms, given a desired power and standardized effect size of
interest. Significance levels of each comparison are adjusted using
Holm's step-down correction (the default). The routine also takes into
account whether the desired statistical power refers to the mean power
(the default), median, or worst-case (which is equivalent to
designing the experiment for the more widely-known Bonferroni correction).
See the reference by <code style="white-space: pre;">&#8288;Campelo and Wanner&#8288;</code> for details.
</p>


<h3>Value</h3>

<p>a list object containing the following items:
</p>

<ul>
<li> <p><code>ninstances</code> - number of instances
</p>
</li>
<li> <p><code>power</code> - the power of the comparison
</p>
</li>
<li> <p><code>d</code> - the effect size
</p>
</li>
<li> <p><code>sig.level</code> - significance level
</p>
</li>
<li> <p><code>alternative.side</code> - type of alternative hypothesis
</p>
</li>
<li> <p><code>test</code> - type of test
</p>
</li></ul>



<h3>Sample Sizes for Nonparametric Methods</h3>

<p>If the parameter <code>test</code> is set to either <code>Wilcoxon</code> or <code>Binomial</code>, this
routine approximates the number of instances using the ARE of these tests
in relation to the paired t.test, using the formulas (see reference by
<code style="white-space: pre;">&#8288;Campelo and Takahashi&#8288;</code> for details):
</p>
<p style="text-align: center;"><code class="reqn">n.wilcox = n.ttest / 0.86 = 1.163 * n.ttest</code>
</p>

<p style="text-align: center;"><code class="reqn">n.binom = n.ttest / 0.637 = 1.570 * n.ttest</code>
</p>



<h3>Author(s)</h3>

<p>Felipe Campelo (<a href="mailto:fcampelo@ufmg.br">fcampelo@ufmg.br</a>,
<a href="mailto:f.campelo@aston.ac.uk">f.campelo@aston.ac.uk</a>)
</p>


<h3>References</h3>


<ul>
<li><p> P. Mathews.
Sample size calculations: Practical methods for engineers and scientists.
Mathews Malnar and Bailey, 2010.
</p>
</li>
<li><p> F. Campelo, F. Takahashi:
Sample size estimation for power and accuracy in the experimental
comparison of algorithms. Journal of Heuristics 25(2):305-338, 2019.
</p>
</li>
<li><p> F. Campelo, E. Wanner:
Sample size calculations for the experimental comparison of multiple
algorithms on multiple problem instances.
Submitted, Journal of Heuristics, 2019.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'># Calculate sample size for mean-case power
K      &lt;- 10   # number of comparisons
alpha  &lt;- 0.05 # significance level
power  &lt;- 0.9  # desired power
d      &lt;- 0.5  # MRES

out &lt;- calc_instances(K, d,
                      power     = power,
                      sig.level = alpha)

# Plot power of each comparison to detect differences of magnitude d
plot(1:K, out$power,
     type = "b", pch = 20, las = 1, ylim = c(0, 1), xlab = "comparison",
     ylab = "power", xaxs = "i", xlim = c(0, 11))
grid(11, NA)
points(c(0, K+1), c(power, power), type = "l", col = 2, lty = 2, lwd = .5)
text(1, 0.93, sprintf("Mean power = %2.2f for N = %d",
                     out$mean.power, out$ninstances), adj = 0)

# Check sample size if planning for Wilcoxon tests:
calc_instances(K, d,
               power     = power,
               sig.level = alpha,
               test = "wilcoxon")$ninstances


# Calculate power profile for predefined sample size
N &lt;- 45
out2 &lt;- calc_instances(K, d, ninstances = N, sig.level = alpha)

points(1:K, out2$power, type = "b", pch = 19, col = 3)
text(6, .7, sprintf("Mean power = %2.2f for N = %d",
                     out2$mean.power, out2$ninstances), adj = 0)

# Sample size for worst-case (Bonferroni) power of 0.8, using Wilcoxon
out3 &lt;- calc_instances(K, d, power = 0.9, sig.level = alpha,
                       test = "wilcoxon", power.target = "worst.case")
out3$ninstances

# For median power:
out4 &lt;- calc_instances(K, d, power = 0.9, sig.level = alpha,
                       test = "wilcoxon", power.target = "median")
out4$ninstances
out4$power

</code></pre>

<hr>
<h2 id='calc_nreps'>Determine sample sizes for a set of algorithms on a single problem instance</h2><span id='topic+calc_nreps'></span>

<h3>Description</h3>

<p>Iteratively calculates the required sample sizes for K algorithms
on a given problem instance, so that the standard errors of the estimates of
the pairwise differences in performance is controlled at a predefined level.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calc_nreps(
  instance,
  algorithms,
  se.max,
  dif = "simple",
  comparisons = "all.vs.all",
  method = "param",
  nstart = 20,
  nmax = 1000,
  seed = NULL,
  boot.R = 499,
  ncpus = 1,
  force.balanced = FALSE,
  load.folder = NA,
  save.folder = NA
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="calc_nreps_+3A_instance">instance</code></td>
<td>
<p>a list object containing the definitions of the problem
instance.
See Section <code>Instance</code> for details.</p>
</td></tr>
<tr><td><code id="calc_nreps_+3A_algorithms">algorithms</code></td>
<td>
<p>a list object containing the definitions of all algorithms.
See Section <code>Algorithms</code> for details.</p>
</td></tr>
<tr><td><code id="calc_nreps_+3A_se.max">se.max</code></td>
<td>
<p>desired upper limit for the standard error of the estimated
difference between pairs of algorithms. See Section
<code style="white-space: pre;">&#8288;Pairwise Differences&#8288;</code> for details.</p>
</td></tr>
<tr><td><code id="calc_nreps_+3A_dif">dif</code></td>
<td>
<p>type of difference to be used. Accepts &quot;perc&quot; (for percent
differences) or &quot;simple&quot; (for simple differences)</p>
</td></tr>
<tr><td><code id="calc_nreps_+3A_comparisons">comparisons</code></td>
<td>
<p>type of comparisons being performed. Accepts &quot;all.vs.first&quot;
(in which cases the first object in <code>algorithms</code> is considered to be
the reference algorithm) or &quot;all.vs.all&quot; (if there is no reference
and all pairwise comparisons are desired).</p>
</td></tr>
<tr><td><code id="calc_nreps_+3A_method">method</code></td>
<td>
<p>method to use for estimating the standard errors. Accepts
&quot;param&quot; (for parametric) or &quot;boot&quot; (for bootstrap)</p>
</td></tr>
<tr><td><code id="calc_nreps_+3A_nstart">nstart</code></td>
<td>
<p>initial number of algorithm runs for each algorithm.
See Section <code style="white-space: pre;">&#8288;Initial Number of Observations&#8288;</code> for details.</p>
</td></tr>
<tr><td><code id="calc_nreps_+3A_nmax">nmax</code></td>
<td>
<p>maximum <strong>total</strong> allowed number of runs to execute. Loaded
results (see <code>load.folder</code> below) do not count towards this
total.</p>
</td></tr>
<tr><td><code id="calc_nreps_+3A_seed">seed</code></td>
<td>
<p>seed for the random number generator</p>
</td></tr>
<tr><td><code id="calc_nreps_+3A_boot.r">boot.R</code></td>
<td>
<p>number of bootstrap resamples to use (if <code>method == "boot"</code>)</p>
</td></tr>
<tr><td><code id="calc_nreps_+3A_ncpus">ncpus</code></td>
<td>
<p>number of cores to use</p>
</td></tr>
<tr><td><code id="calc_nreps_+3A_force.balanced">force.balanced</code></td>
<td>
<p>logical flag to force the use of balanced sampling for
the algorithms on each instance</p>
</td></tr>
<tr><td><code id="calc_nreps_+3A_load.folder">load.folder</code></td>
<td>
<p>name of folder to load results from. Use either &quot;&quot; or
&quot;./&quot; for the current working directory. Accepts relative paths.
Use <code>NA</code> for not saving. <code>calc_nreps()</code> will look for a .RDS file
with the same name</p>
</td></tr>
<tr><td><code id="calc_nreps_+3A_save.folder">save.folder</code></td>
<td>
<p>name of folder to save the results. Use either &quot;&quot; or
&quot;./&quot; for the current working directory. Accepts relative paths.
Use <code>NA</code> for not saving.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list object containing the following items:
</p>

<ul>
<li> <p><code>instance</code> - alias for the problem instance considered
</p>
</li>
<li> <p><code>Xk</code> - list of observed performance values for all <code>algorithms</code>
</p>
</li>
<li> <p><code>Nk</code> - vector of sample sizes generated for each algorithm
</p>
</li>
<li> <p><code>Diffk</code> - data frame with point estimates, standard errors and
other information for all algorithm pairs of interest
</p>
</li>
<li> <p><code>seed</code> - seed used for the PRNG
</p>
</li>
<li> <p><code>dif</code> - type of difference used
</p>
</li>
<li> <p><code>method</code> - method used (&quot;param&quot; / &quot;boot&quot;)
</p>
</li>
<li> <p><code>comparisons</code> - type of pairings (&quot;all.vs.all&quot; / &quot;all.vs.first&quot;)
</p>
</li></ul>



<h3>Instance</h3>

<p>Parameter <code>instance</code> must be a named list containing all relevant parameters
that define the problem instance. This list must contain at least the field
<code>instance$FUN</code>, with the name of the function implementing the problem
instance, that is, a routine that calculates y = f(x). If the instance
requires additional parameters, these must also be provided as named fields.
</p>


<h3>Algorithms</h3>

<p>Object <code>algorithms</code> is a list in which each component is a named
list containing all relevant parameters that define an algorithm to be
applied for solving the problem instance. In what follows <code>algorithm[[k]]</code>
refers to any algorithm specified in the <code>algorithms</code> list.
</p>
<p><code>algorithm[[k]]</code> must contain an <code>algorithm[[k]]$FUN</code> field, which is a
character object with the name of the function that calls the algorithm; as
well as any other elements/parameters that <code>algorithm[[k]]$FUN</code> requires
(e.g., stop criteria, operator names and parameters, etc.).
</p>
<p>The function defined by the routine <code>algorithm[[k]]$FUN</code> must have the
following structure: supposing that the list in <code>algorithm[[k]]</code> has
fields <code>algorithm[[k]]$FUN = "myalgo"</code>, <code>algorithm[[k]]$par1 = "a"</code> and
<code>algorithm$par2 = 5</code>, then:
</p>
<pre>
         myalgo &lt;- function(par1, par2, instance, ...){
               # do stuff
               # ...
               return(results)
         }
   </pre>
<p>That is, it must be able to run if called as:
</p>
<pre>
         # remove '$FUN' and '$alias' fields from list of arguments
         # and include the problem definition as field 'instance'
         myargs          &lt;- algorithm[names(algorithm) != "FUN"]
         myargs          &lt;- myargs[names(myargs) != "alias"]
         myargs$instance &lt;- instance

         # call function
         do.call(algorithm$FUN,
                 args = myargs)
   </pre>
<p>The <code>algorithm$FUN</code> routine must return a list containing (at
least) the performance value of the final solution obtained, in a field named
<code>value</code> (e.g., <code>result$value</code>) after a given run.
</p>


<h3>Initial Number of Observations</h3>

<p>In the <strong>general case</strong> the initial number of observations per algorithm
(<code>nstart</code>) should be relatively high. For the parametric case
we recommend between 10 and 20 if outliers are not expected, or between 30
and 50 if that assumption cannot be made. For the bootstrap approach we
recommend using at least 20. However, if some distributional assumptions can
be made - particularly low skewness of the population of algorithm results on
the test instances), then <code>nstart</code> can in principle be as small as 5 (if the
output of the algorithms were known to be normal, it could be 1).
</p>
<p>In general, higher sample sizes are the price to pay for abandoning
distributional assumptions. Use lower values of <code>nstart</code> with caution.
</p>


<h3>Pairwise Differences</h3>

<p>Parameter <code>dif</code> informs the type of difference in performance to be used
for the estimation (<code class="reqn">\mu_a</code> and <code class="reqn">\mu_b</code> represent the mean
performance of any two algorithms on the test instance, and <code class="reqn">mu</code>
represents the grand mean of all algorithms given in <code>algorithms</code>):
</p>

<ul>
<li><p> If <code>dif == "perc"</code> and <code>comparisons == "all.vs.first"</code>, the estimated quantity is
<code class="reqn">\phi_{1b} = (\mu_1 - \mu_b) / \mu_1 = 1 - (\mu_b / \mu_1)</code>.
</p>
</li>
<li><p> If <code>dif == "perc"</code> and <code>comparisons == "all.vs.all"</code>, the estimated quantity is
<code class="reqn">\phi_{ab} = (\mu_a - \mu_b) / \mu</code>.
</p>
</li>
<li><p> If <code>dif == "simple"</code> it estimates <code class="reqn">\mu_a - \mu_b</code>.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Felipe Campelo (<a href="mailto:fcampelo@gmail.com">fcampelo@gmail.com</a>)
</p>


<h3>References</h3>


<ul>
<li><p> F. Campelo, F. Takahashi:
Sample size estimation for power and accuracy in the experimental
comparison of algorithms. Journal of Heuristics 25(2):305-338, 2019.
</p>
</li>
<li><p> P. Mathews.
Sample size calculations: Practical methods for engineers and scientists.
Mathews Malnar and Bailey, 2010.
</p>
</li>
<li><p> A.C. Davison, D.V. Hinkley:
Bootstrap methods and their application. Cambridge University Press (1997)
</p>
</li>
<li><p> E.C. Fieller:
Some problems in interval estimation. Journal of the Royal Statistical
Society. Series B (Methodological) 16(2), 175â€“185 (1954)
</p>
</li>
<li><p> V. Franz:
Ratios: A short guide to confidence limits and proper use (2007).
https://arxiv.org/pdf/0710.2024v1.pdf
</p>
</li>
<li><p> D.C. Montgomery, C.G. Runger:
Applied Statistics and Probability for Engineers, 6th ed. Wiley (2013)
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'># Example using dummy algorithms and instances. See ?dummyalgo for details.
# We generate dummy algorithms with true means 15, 10, 30, 15, 20; and true
# standard deviations 2, 4, 6, 8, 10.
algorithms &lt;- mapply(FUN = function(i, m, s){
                          list(FUN   = "dummyalgo",
                               alias = paste0("algo", i),
                               distribution.fun  = "rnorm",
                               distribution.pars = list(mean = m, sd = s))},
                     i = c(alg1 = 1, alg2 = 2, alg3 = 3, alg4 = 4, alg5 = 5),
                     m = c(15, 10, 30, 15, 20),
                     s = c(2, 4, 6, 8, 10),
                     SIMPLIFY = FALSE)

# Make a dummy instance with a centered (zero-mean) exponential distribution:
instance = list(FUN = "dummyinstance", distr = "rexp", rate = 5, bias = -1/5)

# Explicitate all other parameters (just this one time:
# most have reasonable default values)
myreps &lt;- calc_nreps(instance   = instance,
                      algorithms = algorithms,
                      se.max     = 0.05,          # desired (max) standard error
                      dif        = "perc",        # type of difference
                      comparisons = "all.vs.all", # differences to consider
                      method     = "param",       # method ("param", "boot")
                      nstart     = 15,            # initial number of samples
                      nmax       = 1000,          # maximum allowed sample size
                      seed       = 1234,          # seed for PRNG
                      boot.R     = 499,           # number of bootstrap resamples (unused)
                      ncpus      = 1,             # number of cores to use
                      force.balanced = FALSE,     # force balanced sampling?
                      load.folder   = NA,         # file to load results from
                      save.folder = NA)         # folder to save results
summary(myreps)
plot(myreps)
</code></pre>

<hr>
<h2 id='calc_se'>Calculates the standard error for simple and percent differences</h2><span id='topic+calc_se'></span>

<h3>Description</h3>

<p>Calculates the sample standard error for the estimator differences between
multiple algorithms on a given instance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calc_se(
  Xk,
  dif = "simple",
  comparisons = "all.vs.all",
  method = "param",
  boot.R = 999
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="calc_se_+3A_xk">Xk</code></td>
<td>
<p>list object where each position contains a vector of observations
of algorithm k on a given problem instance.</p>
</td></tr>
<tr><td><code id="calc_se_+3A_dif">dif</code></td>
<td>
<p>name of the difference for which the SEs are desired.
Accepts &quot;perc&quot; (for percent differences) or &quot;simple&quot; (for simple
differences)</p>
</td></tr>
<tr><td><code id="calc_se_+3A_comparisons">comparisons</code></td>
<td>
<p>standard errors to be calculated. Accepts &quot;all.vs.first&quot;
(in which cases the first object in <code>algorithms</code> is considered to be
the reference algorithm) or &quot;all.vs.all&quot; (if there is no reference
and all pairwise SEs are desired).</p>
</td></tr>
<tr><td><code id="calc_se_+3A_method">method</code></td>
<td>
<p>method used to calculate the interval. Accepts &quot;param&quot;
(using parametric formulas based on normality of the sampling
distribution of the means) or &quot;boot&quot; (for bootstrap).</p>
</td></tr>
<tr><td><code id="calc_se_+3A_boot.r">boot.R</code></td>
<td>
<p>(optional) number of bootstrap resamples
(if <code>method == "boot"</code>)</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> If <code>dif == "perc"</code> it returns the standard errors for the sample
estimates of pairs
<code class="reqn">(mu2 - mu1) / mu</code>, where <code class="reqn">mu1, mu2</code> are the means of the
populations that generated sample vectors <code class="reqn">x1, x2</code>, and
</p>
</li>
<li><p> If <code>dif == "simple"</code> it returns the SE for sample estimator of
<code class="reqn">(mu2 - mu1)</code>
</p>
</li></ul>



<h3>Value</h3>

<p>a list object containing the following items:
</p>

<ul>
<li> <p><code>Phi.est</code> - estimated values of the statistic of interest for
each pair of algorithms of interest (all pairs if <code>comparisons == "all.vs.all"</code>,
or all pairs containing the first algorithm if <code>comparisons == "all.vs.first"</code>).
</p>
</li>
<li> <p><code>se</code> - standard error estimates
</p>
</li></ul>



<h3>References</h3>


<ul>
<li><p> F. Campelo, F. Takahashi:
Sample size estimation for power and accuracy in the experimental
comparison of algorithms. Journal of Heuristics 25(2):305-338, 2019.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Felipe Campelo (<a href="mailto:fcampelo@ufmg.br">fcampelo@ufmg.br</a>,
<a href="mailto:f.campelo@aston.ac.uk">f.campelo@aston.ac.uk</a>)
</p>


<h3>Examples</h3>

<pre><code class='language-R'># three vectors of normally distributed observations
set.seed(1234)
Xk &lt;- list(rnorm(10, 5, 1),  # mean = 5, sd = 1,
           rnorm(20, 10, 2), # mean = 10, sd = 2,
           rnorm(50, 15, 5)) # mean = 15, sd = 3

calc_se(Xk, dif = "simple", comparisons = "all.vs.all", method = "param")
calc_se(Xk, dif = "simple", comparisons = "all.vs.all", method = "boot")

calc_se(Xk, dif = "perc", comparisons = "all.vs.first", method = "param")
calc_se(Xk, dif = "perc", comparisons = "all.vs.first", method = "boot")

calc_se(Xk, dif = "perc", comparisons = "all.vs.all", method = "param")
calc_se(Xk, dif = "perc", comparisons = "all.vs.all", method = "boot")
</code></pre>

<hr>
<h2 id='consolidate_partial_results'>Consolidate results from partial files</h2><span id='topic+consolidate_partial_results'></span>

<h3>Description</h3>

<p>Consolidates results from a set of partial files (each generated by an
individual call to <code><a href="#topic+calc_nreps">calc_nreps()</a></code>) into a single output structure, similar
(but not identical) to the output of <code><a href="#topic+run_experiment">run_experiment()</a></code>. This is useful
e.g., to consolidate the results from interrupted experiments.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>consolidate_partial_results(Configuration, folder = "./nreps_files")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="consolidate_partial_results_+3A_configuration">Configuration</code></td>
<td>
<p>a named list containing all parameters
required in a call to <code><a href="#topic+run_experiment">run_experiment()</a></code> except
<code>instances</code> and <code>algorithms</code>. See the parameter list
and default values in <code><a href="#topic+run_experiment">run_experiment()</a></code>. Notice that
this is always returned as part of the output structure
of <code><a href="#topic+run_experiment">run_experiment()</a></code>, so it generally easier to just
retrieve it from previously saved results.</p>
</td></tr>
<tr><td><code id="consolidate_partial_results_+3A_folder">folder</code></td>
<td>
<p>folder where the partial files are located.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list object containing the following fields:
</p>

<ul>
<li> <p><code>data.raw</code> - data frame containing all observations generated
</p>
</li>
<li> <p><code>data.summary</code> - data frame summarizing the experiment.
</p>
</li>
<li> <p><code>N</code> - number of instances sampled
</p>
</li>
<li> <p><code>total.runs</code> - total number of algorithm runs performed
</p>
</li>
<li> <p><code>instances.sampled</code> - names of the instances sampled
</p>
</li></ul>


<hr>
<h2 id='dummyalgo'>Dummy algorithm routine to test the sampling procedures</h2><span id='topic+dummyalgo'></span>

<h3>Description</h3>

<p>This is a dummy algorithm routine to test the sampling procedures, in
combination with <code><a href="#topic+dummyinstance">dummyinstance()</a></code>.
<code>dummyalgo()</code> receives two parameters that determine the distribution of
performances it will exhibit on a hypothetical problem class:
<code>distribution.fun</code> (with the name of a random number generation function,
e.g. <code>rnorm</code>, <code>runif</code>, <code>rexp</code> etc.); and <code>distribution.pars</code>, a named list of
parameters to be passed on to <code>distribution.fun</code>.
The third parameter is an instance object (see <code><a href="#topic+calc_nreps">calc_nreps()</a></code> for details),
which is a named list with the following fields:
</p>

<ul>
<li><p><code>FUN = "dummyinstance"</code> - must always be &quot;dummyinstance&quot; (will
be ignored otherwise).
</p>
</li>
<li><p><code>distr</code> - the name of a random number generation function.
</p>
</li>
<li><p><code>...</code> - other named fields with parameters to be passed down
to the function in <code>distr</code>.
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>dummyalgo(
  distribution.fun = "rnorm",
  distribution.pars = list(mean = 0, sd = 1),
  instance = list(FUN = "dummyinstance", distr = "rnorm", mean = 0, sd = 1)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dummyalgo_+3A_distribution.fun">distribution.fun</code></td>
<td>
<p>name of a function that generates random values
according to a given distribution, e.g., &quot;rnorm&quot;, &quot;runif&quot;, &quot;rexp&quot; etc.</p>
</td></tr>
<tr><td><code id="dummyalgo_+3A_distribution.pars">distribution.pars</code></td>
<td>
<p>list of named parameters required by the function
in <code>distribution.fun</code>. Parameter <code>n</code> (number of points to
generate) is unnecessary (this routine always forces <code>n = 1</code>).</p>
</td></tr>
<tr><td><code id="dummyalgo_+3A_instance">instance</code></td>
<td>
<p>instance parameters (see <code>Details</code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>distribution.fun</code> and <code>distribution.pars</code> regulate the mean performance of
the dummy algorithm on a given (hypothetical) problem class, and the
between-instances variance of performance. The instance specification in
<code>instance</code> regulates the within-instance variability of results. Ideally the
distribution parameters passed to the <code>instance</code> should result in a
within-instance distribution of values with zero mean, so that the mean of
the values returned by <code>dummyalgo</code> is regulated only by <code>distribution.fun</code>
and <code>distribution.pars</code>.
</p>
<p>The value returned by dummyalgo is sampled as follows:
</p>
<pre>
   offset &lt;- do.call(distribution.fun, args = distribution.pars)
   y &lt;- offset + do.call("dummyinstance", args = instance)
</pre>


<h3>Value</h3>

<p>a list object with a single field <code>$value</code>, containing a scalar
numerical value distributed as described at the end of <code>Details</code>.
</p>


<h3>Author(s)</h3>

<p>Felipe Campelo (<a href="mailto:fcampelo@ufmg.br">fcampelo@ufmg.br</a>,
<a href="mailto:f.campelo@aston.ac.uk">f.campelo@aston.ac.uk</a>)
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dummyinstance">dummyinstance()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Make a dummy instance with a centered (zero-mean) exponential distribution:
instance = list(FUN = "dummyinstance", distr = "rexp", rate = 5, bias = -1/5)

# Simulate a dummy algorithm that has a uniform distribution of expected
# performance values, between -25 and 50.
dummyalgo(distribution.fun = "runif",
          distribution.pars = list(min = -25, max = 50),
          instance = instance)

</code></pre>

<hr>
<h2 id='dummyinstance'>Dummy instance routine to test the sampling procedures</h2><span id='topic+dummyinstance'></span>

<h3>Description</h3>

<p>This is a dummy instance routine to test the sampling procedures, in
combination with <code><a href="#topic+dummyalgo">dummyalgo()</a></code>.
<code>dummyinstance()</code> receives a parameter <code>distr</code> containing the name of a
random number generation function (e.g. <code>rnorm</code>, <code>runif</code>, <code>rexp</code> etc.), plus
a variable number of arguments to be passed down to the function in <code>distr</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dummyinstance(distr, ..., bias = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dummyinstance_+3A_distr">distr</code></td>
<td>
<p>name of a function that generates random values
according to a given distribution, e.g., &quot;rnorm&quot;, &quot;runif&quot;, &quot;rexp&quot; etc.</p>
</td></tr>
<tr><td><code id="dummyinstance_+3A_...">...</code></td>
<td>
<p>additional parameters to be passed down to the function in
<code>distr</code>. Parameter <code>n</code> (number of points to generate) is unnecessary
(this routine always forces <code>n = 1</code>).</p>
</td></tr>
<tr><td><code id="dummyinstance_+3A_bias">bias</code></td>
<td>
<p>a bias term to add to the results of the distribution function
(e.g., to set the mean to zero).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a single numeric value sampled from the desired distribution.
</p>


<h3>Author(s)</h3>

<p>Felipe Campelo (<a href="mailto:fcampelo@ufmg.br">fcampelo@ufmg.br</a>,
<a href="mailto:f.campelo@aston.ac.uk">f.campelo@aston.ac.uk</a>)
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dummyalgo">dummyalgo()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dummyinstance(distr = "rnorm", mean = 10, sd = 1)

# Make a centered (zero-mean) exponential distribution:
lambda = 4

# 10000 observations
set.seed(1234)
y &lt;- numeric(10000)
for (i in 1:10000) y[i] &lt;- dummyinstance(distr = "rexp", rate = lambda,
                                         bias = -1/lambda)
mean(y)
hist(y, breaks = 50, xlim = c(-0.5, 2.5))
</code></pre>

<hr>
<h2 id='example_SANN'>Simulated annealing (for testing/examples)</h2><span id='topic+example_SANN'></span>

<h3>Description</h3>

<p>Adapted from stats::optim(). Check their documentation / examples for
details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>example_SANN(Temp, budget, instance)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="example_SANN_+3A_temp">Temp</code></td>
<td>
<p>controls the &quot;SANN&quot; method. It is the starting temperature for
the cooling schedule.</p>
</td></tr>
<tr><td><code id="example_SANN_+3A_budget">budget</code></td>
<td>
<p>stop criterion: number of function evaluations to execute</p>
</td></tr>
<tr><td><code id="example_SANN_+3A_instance">instance</code></td>
<td>
<p>an instance object (see <code><a href="#topic+calc_nreps">calc_nreps()</a></code> for details)</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
instance &lt;- list(FUN = "TSP.dist", mydist = datasets::eurodist)

example_SANN(Temp = 2000, budget = 10000, instance = instance)

## End(Not run)


</code></pre>

<hr>
<h2 id='get_observations'>Run an algorithm on a problem.</h2><span id='topic+get_observations'></span>

<h3>Description</h3>

<p>Call algorithm routine for the solution of a problem instance
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_observations(algo, instance, n = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_observations_+3A_algo">algo</code></td>
<td>
<p>a list object containing the definitions of the algorithm.
See <code><a href="#topic+calc_nreps">calc_nreps()</a></code> for details.</p>
</td></tr>
<tr><td><code id="get_observations_+3A_instance">instance</code></td>
<td>
<p>a list object containing the definitions of the problem
instance. See <code><a href="#topic+calc_nreps">calc_nreps()</a></code> for details.</p>
</td></tr>
<tr><td><code id="get_observations_+3A_n">n</code></td>
<td>
<p>number of observations to generate.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>vector of observed performance values
</p>


<h3>Author(s)</h3>

<p>Felipe Campelo (<a href="mailto:fcampelo@ufmg.br">fcampelo@ufmg.br</a>,
<a href="mailto:f.campelo@aston.ac.uk">f.campelo@aston.ac.uk</a>)
</p>


<h3>See Also</h3>

<p><code><a href="#topic+calc_nreps">calc_nreps()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Make a dummy instance with a centered (zero-mean) exponential distribution:
instance &lt;- list(FUN = "dummyinstance", distr = "rexp", rate = 5, bias = -1/5)

# Simulate a dummy algorithm that has a uniform distribution of expected
# performance values, between -25 and 50.
algorithm &lt;- list(FUN = "dummyalgo",
                 distribution.fun = "runif",
                 distribution.pars = list(min = -25, max = 50))
x &lt;- get_observations(algorithm, instance, n = 1000)
hist(x)
</code></pre>

<hr>
<h2 id='plot.CAISEr'>plot.CAISEr</h2><span id='topic+plot.CAISEr'></span>

<h3>Description</h3>

<p>S3 method for plotting <em>CAISEr</em> objects output by <code><a href="#topic+run_experiment">run_experiment()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'CAISEr'
plot(
  x,
  y = NULL,
  ...,
  latex = FALSE,
  reorder = FALSE,
  show.text = TRUE,
  digits = 3,
  layout = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.CAISEr_+3A_x">x</code></td>
<td>
<p>list object of class <em>CAISEr</em>.</p>
</td></tr>
<tr><td><code id="plot.CAISEr_+3A_y">y</code></td>
<td>
<p>unused. Included for consistency with generic <code>plot</code> method.</p>
</td></tr>
<tr><td><code id="plot.CAISEr_+3A_...">...</code></td>
<td>
<p>other parameters to be passed down to specific
plotting functions (currently unused)</p>
</td></tr>
<tr><td><code id="plot.CAISEr_+3A_latex">latex</code></td>
<td>
<p>logical: should labels be formatted for LaTeX? (useful for
later saving using library <code>TikzDevice</code>)</p>
</td></tr>
<tr><td><code id="plot.CAISEr_+3A_reorder">reorder</code></td>
<td>
<p>logical: should the comparisons be reordered alphabetically?</p>
</td></tr>
<tr><td><code id="plot.CAISEr_+3A_show.text">show.text</code></td>
<td>
<p>logical: should text be plotted?</p>
</td></tr>
<tr><td><code id="plot.CAISEr_+3A_digits">digits</code></td>
<td>
<p>how many significant digits should be used in text?</p>
</td></tr>
<tr><td><code id="plot.CAISEr_+3A_layout">layout</code></td>
<td>
<p>optional parameter to override the layout of the plots (see
<code>gridExtra::arrangeGrobs()</code> for details. The default layout is
<code>lay = rbind(c(1,1,1,1,1,1), c(1,1,1,1,1,1), c(2,2,2,3,3,3))</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>list containing (1) a list of of <code>ggplot2</code> objects generated, and
(2) a list of data frames used for the creation of the plots.
</p>

<hr>
<h2 id='plot.nreps'>plot.nreps</h2><span id='topic+plot.nreps'></span>

<h3>Description</h3>

<p>S3 method for plotting <em>nreps</em> objects output by <code><a href="#topic+calc_nreps">calc_nreps()</a></code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'nreps'
plot(
  x,
  y = NULL,
  ...,
  instance.name = NULL,
  latex = FALSE,
  show.SE = TRUE,
  show.CI = TRUE,
  sig.level = 0.05,
  show.text = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.nreps_+3A_x">x</code></td>
<td>
<p>list object of class <em>nreps</em> (generated by <code><a href="#topic+calc_nreps">calc_nreps()</a></code>)
or of class <em>CAISEr</em> (in which case an <code>instance.name</code>
must be provided).</p>
</td></tr>
<tr><td><code id="plot.nreps_+3A_y">y</code></td>
<td>
<p>unused. Included for consistency with generic <code>plot</code> method.</p>
</td></tr>
<tr><td><code id="plot.nreps_+3A_...">...</code></td>
<td>
<p>other parameters to be passed down to specific
plotting functions (currently unused)</p>
</td></tr>
<tr><td><code id="plot.nreps_+3A_instance.name">instance.name</code></td>
<td>
<p>name for instance to be plotted if <code>object</code> is
of class <em>CAISEr</em>. Ignored otherwise.</p>
</td></tr>
<tr><td><code id="plot.nreps_+3A_latex">latex</code></td>
<td>
<p>logical: should labels be formatted for LaTeX? (useful for
later saving using library <code>TikzDevice</code>)</p>
</td></tr>
<tr><td><code id="plot.nreps_+3A_show.se">show.SE</code></td>
<td>
<p>logical: should standard errors be plotted?</p>
</td></tr>
<tr><td><code id="plot.nreps_+3A_show.ci">show.CI</code></td>
<td>
<p>logical: should confidence intervals be plotted?</p>
</td></tr>
<tr><td><code id="plot.nreps_+3A_sig.level">sig.level</code></td>
<td>
<p>significance level for the confidence interval.
0 &lt; sig.level &lt; 1</p>
</td></tr>
<tr><td><code id="plot.nreps_+3A_show.text">show.text</code></td>
<td>
<p>logical: should text be plotted?</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>ggplot</code> object (invisibly)
</p>

<hr>
<h2 id='print.CAISEr'>print.CAISEr</h2><span id='topic+print.CAISEr'></span>

<h3>Description</h3>

<p>S3 method for printing <em>CAISEr</em> objects (the output of
<code><a href="#topic+run_experiment">run_experiment()</a></code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'CAISEr'
print(x, ..., echo = TRUE, digits = 4, right = TRUE, breakrows = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.CAISEr_+3A_x">x</code></td>
<td>
<p>list object of class <em>CAISEr</em>
(generated by <code><a href="#topic+run_experiment">run_experiment()</a></code>)</p>
</td></tr>
<tr><td><code id="print.CAISEr_+3A_...">...</code></td>
<td>
<p>other parameters to be passed down to specific
summary functions (currently unused)</p>
</td></tr>
<tr><td><code id="print.CAISEr_+3A_echo">echo</code></td>
<td>
<p>logical flag: should the print method actually print to screen?</p>
</td></tr>
<tr><td><code id="print.CAISEr_+3A_digits">digits</code></td>
<td>
<p>the minimum number of significant digits to be used.
See <code><a href="base.html#topic+print.default">print.default()</a></code>.</p>
</td></tr>
<tr><td><code id="print.CAISEr_+3A_right">right</code></td>
<td>
<p>logical, indicating whether or not strings should be
right-aligned.</p>
</td></tr>
<tr><td><code id="print.CAISEr_+3A_breakrows">breakrows</code></td>
<td>
<p>logical, indicating whether to &quot;widen&quot; the output table by
placing the bottom half to the right of the top half.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data frame object containing the summary table (invisibly)
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Example using four dummy algorithms and 100 dummy instances.
# See [dummyalgo()] and [dummyinstance()] for details.
# Generating 4 dummy algorithms here, with means 15, 10, 30, 15 and standard
# deviations 2, 4, 6, 8.
algorithms &lt;- mapply(FUN = function(i, m, s){
  list(FUN   = "dummyalgo",
       alias = paste0("algo", i),
       distribution.fun  = "rnorm",
       distribution.pars = list(mean = m, sd = s))},
  i = c(alg1 = 1, alg2 = 2, alg3 = 3, alg4 = 4),
  m = c(15, 10, 30, 15),
  s = c(2, 4, 6, 8),
  SIMPLIFY = FALSE)

# Generate 100 dummy instances with centered exponential distributions
instances &lt;- lapply(1:100,
                    function(i) {rate &lt;- runif(1, 1, 10)
                                 list(FUN   = "dummyinstance",
                                      alias = paste0("Inst.", i),
                                      distr = "rexp", rate = rate,
                                      bias  = -1 / rate)})

my.results &lt;- run_experiment(instances, algorithms,
                             d = 1, se.max = .1,
                             power = .9, sig.level = .05,
                             power.target = "mean",
                             dif = "perc", comparisons = "all.vs.all",
                             seed = 1234, ncpus = 1)
my.results


</code></pre>

<hr>
<h2 id='run_experiment'>Run a full experiment for comparing multiple algorithms using multiple
instances</h2><span id='topic+run_experiment'></span>

<h3>Description</h3>

<p>Design and run a full experiment - calculate the required number of
instances, run the algorithms on each problem instance using the iterative
approach based on optimal sample size ratios, and return the results of the
experiment. This routine builds upon <code><a href="#topic+calc_instances">calc_instances()</a></code> and <code><a href="#topic+calc_nreps">calc_nreps()</a></code>,
so refer to the documentation of these two functions for details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>run_experiment(
  instances,
  algorithms,
  d,
  se.max,
  power = 0.8,
  sig.level = 0.05,
  power.target = "mean",
  dif = "simple",
  comparisons = "all.vs.all",
  alternative = "two.sided",
  test = "t.test",
  method = "param",
  nstart = 20,
  nmax = 100 * length(algorithms),
  force.balanced = FALSE,
  ncpus = 2,
  boot.R = 499,
  seed = NULL,
  save.partial.results = NA,
  load.partial.results = NA,
  save.final.result = NA
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="run_experiment_+3A_instances">instances</code></td>
<td>
<p>list object containing the definitions of the
<em>available</em> instances. This list may (or may not) be exhausted in the
experiment. To estimate the number of required instances,
see <code><a href="#topic+calc_instances">calc_instances()</a></code>. For more details, see Section <code style="white-space: pre;">&#8288;Instance List&#8288;</code>.</p>
</td></tr>
<tr><td><code id="run_experiment_+3A_algorithms">algorithms</code></td>
<td>
<p>a list object containing the definitions of all algorithms.
See Section <code>Algorithms</code> for details.</p>
</td></tr>
<tr><td><code id="run_experiment_+3A_d">d</code></td>
<td>
<p>minimally relevant effect size (MRES), expressed as a standardized
effect size, i.e., &quot;deviation from H0&quot; / &quot;standard deviation&quot;.
See <code><a href="#topic+calc_instances">calc_instances()</a></code> for details.</p>
</td></tr>
<tr><td><code id="run_experiment_+3A_se.max">se.max</code></td>
<td>
<p>desired upper limit for the standard error of the estimated
difference between pairs of algorithms. See Section
<code style="white-space: pre;">&#8288;Pairwise Differences&#8288;</code> for details.</p>
</td></tr>
<tr><td><code id="run_experiment_+3A_power">power</code></td>
<td>
<p>(desired) test power. See <code><a href="#topic+calc_instances">calc_instances()</a></code> for details.
Any value equal to or greater than one will force the method to use all
available instances in <code>Instance.list</code>.</p>
</td></tr>
<tr><td><code id="run_experiment_+3A_sig.level">sig.level</code></td>
<td>
<p>family-wise significance level (alpha) for the experiment.
See <code><a href="#topic+calc_instances">calc_instances()</a></code> for details.</p>
</td></tr>
<tr><td><code id="run_experiment_+3A_power.target">power.target</code></td>
<td>
<p>which comparison should have the desired <code>power</code>?
Accepts &quot;mean&quot;, &quot;median&quot;, or &quot;worst.case&quot; (this last one
is equivalent to the Bonferroni correction).</p>
</td></tr>
<tr><td><code id="run_experiment_+3A_dif">dif</code></td>
<td>
<p>type of difference to be used. Accepts &quot;perc&quot; (for percent
differences) or &quot;simple&quot; (for simple differences)</p>
</td></tr>
<tr><td><code id="run_experiment_+3A_comparisons">comparisons</code></td>
<td>
<p>type of comparisons being performed. Accepts &quot;all.vs.first&quot;
(in which cases the first object in <code>algorithms</code> is considered to be
the reference algorithm) or &quot;all.vs.all&quot; (if there is no reference
and all pairwise comparisons are desired).</p>
</td></tr>
<tr><td><code id="run_experiment_+3A_alternative">alternative</code></td>
<td>
<p>type of alternative hypothesis (&quot;two.sided&quot; or
&quot;less&quot; or &quot;greater&quot;). See <code><a href="#topic+calc_instances">calc_instances()</a></code> for details.</p>
</td></tr>
<tr><td><code id="run_experiment_+3A_test">test</code></td>
<td>
<p>type of test to be used
(&quot;t.test&quot;, &quot;wilcoxon&quot; or &quot;binomial&quot;)</p>
</td></tr>
<tr><td><code id="run_experiment_+3A_method">method</code></td>
<td>
<p>method to use for estimating the standard errors. Accepts
&quot;param&quot; (for parametric) or &quot;boot&quot; (for bootstrap)</p>
</td></tr>
<tr><td><code id="run_experiment_+3A_nstart">nstart</code></td>
<td>
<p>initial number of algorithm runs for each algorithm.
See Section <code style="white-space: pre;">&#8288;Initial Number of Observations&#8288;</code> for details.</p>
</td></tr>
<tr><td><code id="run_experiment_+3A_nmax">nmax</code></td>
<td>
<p>maximum number of runs to execute on each instance (see
<code><a href="#topic+calc_nreps">calc_nreps()</a></code>). Loaded results (see <code>load.partial.results</code>
below) do not count towards this maximum.</p>
</td></tr>
<tr><td><code id="run_experiment_+3A_force.balanced">force.balanced</code></td>
<td>
<p>logical flag to force the use of balanced sampling for
the algorithms on each instance</p>
</td></tr>
<tr><td><code id="run_experiment_+3A_ncpus">ncpus</code></td>
<td>
<p>number of cores to use</p>
</td></tr>
<tr><td><code id="run_experiment_+3A_boot.r">boot.R</code></td>
<td>
<p>number of bootstrap resamples to use (if <code>method == "boot"</code>)</p>
</td></tr>
<tr><td><code id="run_experiment_+3A_seed">seed</code></td>
<td>
<p>seed for the random number generator</p>
</td></tr>
<tr><td><code id="run_experiment_+3A_save.partial.results">save.partial.results</code></td>
<td>
<p>should partial results be saved to files? Can be
either <code>NA</code> (do not save) or a character string
pointing to a folder. File names are generated
based on the instance aliases. <strong>Existing files with
matching names will be overwritten.</strong>
<code>run_experiment()</code> uses <strong>.RDS</strong> files for saving
and loading.</p>
</td></tr>
<tr><td><code id="run_experiment_+3A_load.partial.results">load.partial.results</code></td>
<td>
<p>should partial results be loaded from files? Can
be either <code>NA</code> (do not save) or a character
string pointing to a folder containing the
file(s) to be loaded. <code>run_experiment()</code> will
use .RDS file(s) with a name(s) matching instance
<code>alias</code>es. <code>run_experiment()</code> uses <strong>.RDS</strong> files
for saving and loading.</p>
</td></tr>
<tr><td><code id="run_experiment_+3A_save.final.result">save.final.result</code></td>
<td>
<p>should the final results be saved to file? Can be
either <code>NA</code> (do not save) or a character string
pointing to a folder where the results will be
saved on a <strong>.RDS</strong> file starting with
<code>CAISEr_results_</code> and ending with 12-digit
datetime tag in the format <code>YYYYMMDDhhmmss</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list object containing the following fields:
</p>

<ul>
<li> <p><code>Configuration</code> - the full input configuration (for reproducibility)
</p>
</li>
<li> <p><code>data.raw</code> - data frame containing all observations generated
</p>
</li>
<li> <p><code>data.summary</code> - data frame summarizing the experiment.
</p>
</li>
<li> <p><code>N</code> - number of instances sampled
</p>
</li>
<li> <p><code>N.star</code> - number of instances required
</p>
</li>
<li> <p><code>total.runs</code> - total number of algorithm runs performed
</p>
</li>
<li> <p><code>instances.sampled</code> - names of the instances sampled
</p>
</li>
<li> <p><code>Underpowered</code> - flag: TRUE if N &lt; N.star
</p>
</li></ul>



<h3>Instance List</h3>

<p>Parameter <code>instances</code> must contain a list of instance objects, where
each field is itself a list, as defined in the documentation of function
<code><a href="#topic+calc_nreps">calc_nreps()</a></code>. In short, each element of <code>instances</code> is an <code>instance</code>, i.e.,
a named list containing all relevant parameters that define the problem
instance. This list must contain at least the field <code>instance$FUN</code>, with the
name of the problem instance function, that is, a routine that calculates
y = f(x). If the instance requires additional parameters, these must also be
provided as named fields.
An additional field, &quot;instance$alias&quot;, can be used to provide the instance
with a unique identifier (e.g., when using an instance generator).
</p>


<h3>Algorithm List</h3>

<p>Object <code>algorithms</code> is a list in which each component is a named
list containing all relevant parameters that define an algorithm to be
applied for solving the problem instance. In what follows <code>algorithms[[k]]</code>
refers to any algorithm specified in the <code>algorithms</code> list.
</p>
<p><code>algorithms[[k]]</code> must contain an <code>algorithms[[k]]$FUN</code> field, which is a
character object with the name of the function that calls the algorithm; as
well as any other elements/parameters that <code>algorithms[[k]]$FUN</code> requires
(e.g., stop criteria, operator names and parameters, etc.).
</p>
<p>The function defined by the routine <code>algorithms[[k]]$FUN</code> must have the
following structure: supposing that the list in <code>algorithms[[k]]</code> has
fields <code>algorithm[[k]]$FUN = "myalgo"</code>, <code>algorithms[[k]]$par1 = "a"</code> and
<code>algorithms[[k]]$par2 = 5</code>, then:
</p>
<pre>
         myalgo &lt;- function(par1, par2, instance, ...){
               #
               # &lt;do stuff&gt;
               #
               return(results)
         }
   </pre>
<p>That is, it must be able to run if called as:
</p>
<pre>
         # remove '$FUN' and '$alias' field from list of arguments
         # and include the problem definition as field 'instance'
         myargs          &lt;- algorithm[names(algorithm) != "FUN"]
         myargs          &lt;- myargs[names(myargs) != "alias"]
         myargs$instance &lt;- instance

         # call function
         do.call(algorithm$FUN,
                 args = myargs)
   </pre>
<p>The <code>algorithm$FUN</code> routine must return a list containing (at
least) the performance value of the final solution obtained, in a field named
<code>value</code> (e.g., <code>result$value</code>) after a given run. In general it is easier to
write a small wrapper function around existing implementations.
</p>


<h3>Initial Number of Observations</h3>

<p>In the <em>general case</em> the initial number of observations / algorithm /
instance (<code>nstart</code>) should be relatively high. For the parametric case
we recommend 10~15 if outliers are not expected, and 30~40 (at least) if that
assumption cannot be made. For the bootstrap approach we recommend using at
least 15 or 20. However, if some distributional assumptions can be
made - particularly low skewness of the population of algorithm results on
the test instances), then <code>nstart</code> can in principle be as small as 5 (if the
output of the algorithm were known to be normal, it could be 1).
</p>
<p>In general, higher sample sizes are the price to pay for abandoning
distributional assumptions. Use lower values of <code>nstart</code> with caution.
</p>


<h3>Pairwise Differences</h3>

<p>Parameter <code>dif</code> informs the type of difference in performance to be used
for the estimation (<code class="reqn">\mu_a</code> and <code class="reqn">\mu_b</code> represent the mean
performance of any two algorithms on the test instance, and <code class="reqn">mu</code>
represents the grand mean of all algorithms given in <code>algorithms</code>):
</p>

<ul>
<li><p> If <code>dif == "perc"</code> and <code>comparisons == "all.vs.first"</code>, the estimated
quantity is:
<code class="reqn">\phi_{1b} = (\mu_1 - \mu_b) / \mu_1 = 1 - (\mu_b / \mu_1)</code>.
</p>
</li>
<li><p> If <code>dif == "perc"</code> and <code>comparisons == "all.vs.all"</code>, the estimated
quantity is:
<code class="reqn">\phi_{ab} = (\mu_a - \mu_b) / \mu</code>.
</p>
</li>
<li><p> If <code>dif == "simple"</code> it estimates <code class="reqn">\mu_a - \mu_b</code>.
</p>
</li></ul>



<h3>Sample Sizes for Nonparametric Methods</h3>

<p>If the parameter &ldquo; is set to either <code>Wilcoxon</code> or 'Binomial', this
routine approximates the number of instances using the ARE of these tests
in relation to the paired t.test:
</p>

<ul>
<li> <p><code>n.wilcox = n.ttest / 0.86 = 1.163 * n.ttest</code>
</p>
</li>
<li> <p><code>n.binom = n.ttest / 0.637 = 1.570 * n.ttest</code>
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Felipe Campelo (<a href="mailto:fcampelo@ufmg.br">fcampelo@ufmg.br</a>,
<a href="mailto:f.campelo@aston.ac.uk">f.campelo@aston.ac.uk</a>)
</p>


<h3>References</h3>


<ul>
<li><p> F. Campelo, F. Takahashi:
Sample size estimation for power and accuracy in the experimental
comparison of algorithms. Journal of Heuristics 25(2):305-338, 2019.
</p>
</li>
<li><p> P. Mathews.
Sample size calculations: Practical methods for engineers and scientists.
Mathews Malnar and Bailey, 2010.
</p>
</li>
<li><p> A.C. Davison, D.V. Hinkley:
Bootstrap methods and their application. Cambridge University Press (1997)
</p>
</li>
<li><p> E.C. Fieller:
Some problems in interval estimation. Journal of the Royal Statistical
Society. Series B (Methodological) 16(2), 175â€“185 (1954)
</p>
</li>
<li><p> V. Franz:
Ratios: A short guide to confidence limits and proper use (2007).
https://arxiv.org/pdf/0710.2024v1.pdf
</p>
</li>
<li><p> D.C. Montgomery, C.G. Runger:
Applied Statistics and Probability for Engineers, 6th ed. Wiley (2013)
</p>
</li>
<li><p> D.J. Sheskin:
Handbook of Parametric and Nonparametric Statistical Procedures,
4th ed., Chapman &amp; Hall/CRC, 1996.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'># Example using four dummy algorithms and 100 dummy instances.
# See [dummyalgo()] and [dummyinstance()] for details.
# Generating 4 dummy algorithms here, with means 15, 10, 30, 15 and standard
# deviations 2, 4, 6, 8.
algorithms &lt;- mapply(FUN = function(i, m, s){
  list(FUN   = "dummyalgo",
       alias = paste0("algo", i),
       distribution.fun  = "rnorm",
       distribution.pars = list(mean = m, sd = s))},
  i = c(alg1 = 1, alg2 = 2, alg3 = 3, alg4 = 4),
  m = c(15, 10, 30, 15),
  s = c(2, 4, 6, 8),
  SIMPLIFY = FALSE)

# Generate 100 dummy instances with centered exponential distributions
instances &lt;- lapply(1:100,
                    function(i) {rate &lt;- runif(1, 1, 10)
                                 list(FUN   = "dummyinstance",
                                      alias = paste0("Inst.", i),
                                      distr = "rexp", rate = rate,
                                      bias  = -1 / rate)})

my.results &lt;- run_experiment(instances, algorithms,
                             d = .5, se.max = .1,
                             power = .9, sig.level = .05,
                             power.target = "mean",
                             dif = "perc", comparisons = "all.vs.all",
                             ncpus = 1, seed = 1234)

# Take a look at the results
summary(my.results)
plot(my.results)

</code></pre>

<hr>
<h2 id='se_boot'>Bootstrap standard errors</h2><span id='topic+se_boot'></span>

<h3>Description</h3>

<p>Calculates the standard errors of a given statistic using bootstrap
</p>


<h3>Usage</h3>

<pre><code class='language-R'>se_boot(Xk, dif = "simple", comparisons = "all.vs.all", boot.R = 999, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="se_boot_+3A_xk">Xk</code></td>
<td>
<p>list object where each position contains a vector of observations
of algorithm k on a given problem instance.</p>
</td></tr>
<tr><td><code id="se_boot_+3A_dif">dif</code></td>
<td>
<p>name of the difference for which the SEs are desired.
Accepts &quot;perc&quot; (for percent differences) or &quot;simple&quot; (for simple
differences)</p>
</td></tr>
<tr><td><code id="se_boot_+3A_comparisons">comparisons</code></td>
<td>
<p>standard errors to be calculated. Accepts &quot;all.vs.first&quot;
(in which cases the first object in <code>algorithms</code> is considered to be
the reference algorithm) or &quot;all.vs.all&quot; (if there is no reference
and all pairwise SEs are desired).</p>
</td></tr>
<tr><td><code id="se_boot_+3A_boot.r">boot.R</code></td>
<td>
<p>(optional) number of bootstrap resamples
(if <code>method == "boot"</code>)</p>
</td></tr>
<tr><td><code id="se_boot_+3A_...">...</code></td>
<td>
<p>other parameters (used only for compatibility with calls to
<code><a href="#topic+se_param">se_param()</a></code>, unused in this function)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Data frame containing, for each pair of interest, the estimated
difference (column &quot;Phi&quot;) and the sample standard error (column &quot;SE&quot;)
</p>


<h3>References</h3>


<ul>
<li><p> A.C. Davison, D.V. Hinkley:
Bootstrap methods and their application. Cambridge University Press (1997)
</p>
</li>
<li><p> F. Campelo, F. Takahashi:
Sample size estimation for power and accuracy in the experimental
comparison of algorithms. Journal of Heuristics 25(2):305-338, 2019.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Felipe Campelo (<a href="mailto:fcampelo@ufmg.br">fcampelo@ufmg.br</a>,
<a href="mailto:f.campelo@aston.ac.uk">f.campelo@aston.ac.uk</a>)
</p>


<h3>Examples</h3>

<pre><code class='language-R'># three vectors of normally distributed observations
set.seed(1234)
Xk &lt;- list(rnorm(10, 5, 1),  # mean = 5, sd = 1,
           rnorm(20, 10, 2), # mean = 10, sd = 2,
           rnorm(20, 15, 5)) # mean = 15, sd = 3

se_boot(Xk, dif = "simple", comparisons = "all.vs.all")
se_boot(Xk, dif = "perc", comparisons = "all.vs.first")
se_boot(Xk, dif = "perc", comparisons = "all.vs.all")
</code></pre>

<hr>
<h2 id='se_param'>Parametric standard errors</h2><span id='topic+se_param'></span>

<h3>Description</h3>

<p>Calculates the standard errors of a given statistic using parametric formulas
</p>


<h3>Usage</h3>

<pre><code class='language-R'>se_param(Xk, dif = "simple", comparisons = "all.vs.all", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="se_param_+3A_xk">Xk</code></td>
<td>
<p>list object where each position contains a vector of observations
of algorithm k on a given problem instance.</p>
</td></tr>
<tr><td><code id="se_param_+3A_dif">dif</code></td>
<td>
<p>name of the difference for which the SEs are desired.
Accepts &quot;perc&quot; (for percent differences) or &quot;simple&quot; (for simple
differences)</p>
</td></tr>
<tr><td><code id="se_param_+3A_comparisons">comparisons</code></td>
<td>
<p>standard errors to be calculated. Accepts &quot;all.vs.first&quot;
(in which cases the first object in <code>algorithms</code> is considered to be
the reference algorithm) or &quot;all.vs.all&quot; (if there is no reference
and all pairwise SEs are desired).</p>
</td></tr>
<tr><td><code id="se_param_+3A_...">...</code></td>
<td>
<p>other parameters (used only for compatibility with calls to
<code><a href="#topic+se_boot">se_boot()</a></code>, unused in this function)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Data frame containing, for each pair of interest, the estimated
difference (column &quot;Phi&quot;) and the sample standard error (column &quot;SE&quot;)
</p>


<h3>References</h3>


<ul>
<li><p> E.C. Fieller:
Some problems in interval estimation. Journal of the Royal Statistical
Society. Series B (Methodological) 16(2), 175â€“185 (1954)
</p>
</li>
<li><p> V. Franz:
Ratios: A short guide to confidence limits and proper use (2007).
https://arxiv.org/pdf/0710.2024v1.pdf
</p>
</li>
<li><p> D.C. Montgomery, C.G. Runger:
Applied Statistics and Probability for Engineers, 6th ed. Wiley (2013)
</p>
</li>
<li><p> F. Campelo, F. Takahashi:
Sample size estimation for power and accuracy in the experimental
comparison of algorithms. Journal of Heuristics 25(2):305-338, 2019.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Felipe Campelo (<a href="mailto:fcampelo@ufmg.br">fcampelo@ufmg.br</a>,
<a href="mailto:f.campelo@aston.ac.uk">f.campelo@aston.ac.uk</a>)
</p>


<h3>Examples</h3>

<pre><code class='language-R'># three vectors of normally distributed observations
set.seed(1234)
Xk &lt;- list(rnorm(10, 5, 1),  # mean = 5, sd = 1,
           rnorm(20, 10, 2), # mean = 10, sd = 2,
           rnorm(20, 15, 5)) # mean = 15, sd = 3

se_param(Xk, dif = "simple", comparisons = "all.vs.all")
se_param(Xk, dif = "perc", comparisons = "all.vs.first")
se_param(Xk, dif = "perc", comparisons = "all.vs.all")
</code></pre>

<hr>
<h2 id='summary.CAISEr'>summary.CAISEr</h2><span id='topic+summary.CAISEr'></span>

<h3>Description</h3>

<p>S3 method for summarizing <em>CAISEr</em> objects output by <code><a href="#topic+run_experiment">run_experiment()</a></code>).
Input parameters <code>test</code>, <code>alternative</code> and <code>sig.level</code> can be used to
override the ones used in the call to <code><a href="#topic+run_experiment">run_experiment()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'CAISEr'
summary(object, test = NULL, alternative = NULL, sig.level = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.CAISEr_+3A_object">object</code></td>
<td>
<p>list object of class <em>CAISEr</em>
(generated by <code><a href="#topic+run_experiment">run_experiment()</a></code>)</p>
</td></tr>
<tr><td><code id="summary.CAISEr_+3A_test">test</code></td>
<td>
<p>type of test to be used
(&quot;t.test&quot;, &quot;wilcoxon&quot; or &quot;binomial&quot;)</p>
</td></tr>
<tr><td><code id="summary.CAISEr_+3A_alternative">alternative</code></td>
<td>
<p>type of alternative hypothesis (&quot;two.sided&quot; or
&quot;less&quot; or &quot;greater&quot;). See <code><a href="#topic+calc_instances">calc_instances()</a></code> for details.</p>
</td></tr>
<tr><td><code id="summary.CAISEr_+3A_sig.level">sig.level</code></td>
<td>
<p>desired family-wise significance level (alpha) for the
experiment</p>
</td></tr>
<tr><td><code id="summary.CAISEr_+3A_...">...</code></td>
<td>
<p>other parameters to be passed down to specific
summary functions (currently unused)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list object is returned invisibly, containing the details of all
tests performed as well as information on the total number of runs
dedicated to each algorithm.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Example using four dummy algorithms and 100 dummy instances.
# See [dummyalgo()] and [dummyinstance()] for details.
# Generating 4 dummy algorithms here, with means 15, 10, 30, 15 and standard
# deviations 2, 4, 6, 8.
algorithms &lt;- mapply(FUN = function(i, m, s){
  list(FUN   = "dummyalgo",
       alias = paste0("algo", i),
       distribution.fun  = "rnorm",
       distribution.pars = list(mean = m, sd = s))},
  i = c(alg1 = 1, alg2 = 2, alg3 = 3, alg4 = 4),
  m = c(15, 10, 30, 15),
  s = c(2, 4, 6, 8),
  SIMPLIFY = FALSE)

# Generate 100 dummy instances with centered exponential distributions
instances &lt;- lapply(1:100,
                    function(i) {rate &lt;- runif(1, 1, 10)
                                 list(FUN   = "dummyinstance",
                                      alias = paste0("Inst.", i),
                                      distr = "rexp", rate = rate,
                                      bias  = -1 / rate)})

my.results &lt;- run_experiment(instances, algorithms,
                             d = 1, se.max = .1,
                             power = .9, sig.level = .05,
                             power.target = "mean",
                             dif = "perc", comparisons = "all.vs.all",
                             seed = 1234, ncpus = 1)
summary(my.results)

# You can override some defaults if you want:
summary(my.results, test = "wilcoxon")

</code></pre>

<hr>
<h2 id='summary.nreps'>summary.nreps</h2><span id='topic+summary.nreps'></span>

<h3>Description</h3>

<p>S3 method for summarizing <em>nreps</em> objects output by <code><a href="#topic+calc_nreps">calc_nreps()</a></code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'nreps'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.nreps_+3A_object">object</code></td>
<td>
<p>list object of class <em>nreps</em>
(generated by <code><a href="#topic+calc_nreps">calc_nreps()</a></code>)</p>
</td></tr>
<tr><td><code id="summary.nreps_+3A_...">...</code></td>
<td>
<p>other parameters to be passed down to specific
summary functions (currently unused)</p>
</td></tr>
</table>

<hr>
<h2 id='TSP.dist'>TSP instance generator (for testing/examples)</h2><span id='topic+TSP.dist'></span>

<h3>Description</h3>

<p>Adapted from stats::optim(). Check their documentation / examples for
details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>TSP.dist(x, mydist)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="TSP.dist_+3A_x">x</code></td>
<td>
<p>a valid closed route for the TSP instance</p>
</td></tr>
<tr><td><code id="TSP.dist_+3A_mydist">mydist</code></td>
<td>
<p>object of class <em>dist</em> defining the TSP instance</p>
</td></tr>
</table>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
