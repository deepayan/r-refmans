<!DOCTYPE html><html><head><title>Help for package randomForestSRC</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {randomForestSRC}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#breast'><p>Wisconsin Prognostic Breast Cancer Data</p></a></li>
<li><a href='#find.interaction.rfsrc'><p>Find Interactions Between Pairs of Variables</p></a></li>
<li><a href='#follic'><p>Follicular Cell Lymphoma</p></a></li>
<li><a href='#get.tree.rfsrc'><p>Extract a Single Tree from a Forest and plot it on your browser</p></a></li>
<li><a href='#hd'><p>Hodgkin's Disease</p></a></li>
<li><a href='#holdout.vimp.rfsrc'><p>Hold out variable importance (VIMP)</p></a></li>
<li><a href='#housing'><p>Ames Iowa Housing Data</p></a></li>
<li><a href='#imbalanced.rfsrc'><p>Imbalanced Two Class Problems</p></a></li>
<li><a href='#impute.rfsrc'><p>Impute Only Mode</p></a></li>
<li><a href='#max.subtree.rfsrc'><p>Acquire Maximal Subtree Information</p></a></li>
<li><a href='#nutrigenomic'><p>Nutrigenomic Study</p></a></li>
<li><a href='#partial.rfsrc'><p>Acquire Partial Effect of a Variable</p></a></li>
<li><a href='#pbc'><p>Primary Biliary Cirrhosis (PBC) Data</p></a></li>
<li><a href='#peakVO2'><p>Systolic Heart Failure Data</p></a></li>
<li><a href='#plot.competing.risk.rfsrc'><p>Plots for Competing Risks</p></a></li>
<li><a href='#plot.quantreg.rfsrc'><p>Plot Quantiles from Quantile Regression Forests</p></a></li>
<li><a href='#plot.rfsrc'><p>Plot Error Rate and Variable Importance from a RF-SRC analysis</p></a></li>
<li><a href='#plot.subsample.rfsrc'><p>Plot Subsampled VIMP Confidence Intervals</p></a></li>
<li><a href='#plot.survival.rfsrc'><p>Plot of Survival Estimates</p></a></li>
<li><a href='#plot.variable.rfsrc'><p>Plot Marginal Effect of Variables</p></a></li>
<li><a href='#predict.rfsrc'><p>Prediction for Random Forests for Survival, Regression, and Classification</p></a></li>
<li><a href='#print.rfsrc'><p>Print Summary Output of a RF-SRC Analysis</p></a></li>
<li><a href='#quantreg.rfsrc'><p>Quantile Regression Forests</p></a></li>
<li><a href='#randomForestSRC-package'>
<p>Fast Unified Random Forests for Survival, Regression, and Classification (RF-SRC)</p></a></li>
<li><a href='#rfsrc'><p>Fast Unified Random Forests for Survival, Regression, and Classification (RF-SRC)</p></a></li>
<li><a href='#rfsrc.anonymous'><p>Anonymous Random Forests</p></a></li>
<li><a href='#rfsrc.fast'><p>Fast Random Forests</p></a></li>
<li><a href='#rfsrc.news'><p>Show the NEWS file</p></a></li>
<li><a href='#sidClustering.rfsrc'><p>sidClustering using SID (Staggered Interaction Data) for Unsupervised Clustering</p></a></li>
<li><a href='#stat.split.rfsrc'><p>Acquire Split Statistic Information</p></a></li>
<li><a href='#subsample.rfsrc'><p>Subsample Forests for VIMP Confidence Intervals</p></a></li>
<li><a href='#synthetic'><p>Synthetic Random Forests</p></a></li>
<li><a href='#tune.rfsrc'><p>Tune Random Forest for the optimal mtry and nodesize parameters</p></a></li>
<li><a href='#var.select.rfsrc'><p>Variable Selection</p></a></li>
<li><a href='#vdv'><p>van de Vijver Microarray Breast Cancer</p></a></li>
<li><a href='#veteran'><p>Veteran's Administration Lung Cancer Trial</p></a></li>
<li><a href='#vimp.rfsrc'><p>VIMP for Single or Grouped Variables</p></a></li>
<li><a href='#wihs'><p>Women's Interagency HIV Study (WIHS)</p></a></li>
<li><a href='#wine'><p>White Wine Quality Data</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Version:</td>
<td>3.2.3</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-12-05</td>
</tr>
<tr>
<td>Title:</td>
<td>Fast Unified Random Forests for Survival, Regression, and
Classification (RF-SRC)</td>
</tr>
<tr>
<td>Author:</td>
<td>Hemant Ishwaran &lt;hemant.ishwaran@gmail.com&gt;, Udaya B. Kogalur &lt;ubk@kogalur.com&gt;</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Udaya B. Kogalur &lt;ubk@kogalur.com&gt;</td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/kogalur/randomForestSRC/issues/">https://github.com/kogalur/randomForestSRC/issues/</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.6.0),</td>
</tr>
<tr>
<td>Imports:</td>
<td>parallel, data.tree, DiagrammeR</td>
</tr>
<tr>
<td>Suggests:</td>
<td>survival, pec, prodlim, mlbench, interp, caret, imbalance,
cluster</td>
</tr>
<tr>
<td>Description:</td>
<td>Fast OpenMP parallel computing of Breiman's random forests for univariate, multivariate, unsupervised, survival, competing risks, class imbalanced classification and quantile regression. New Mahalanobis splitting for correlated outcomes.  Extreme random forests and randomized splitting.  Suite of imputation methods for missing data.  Fast random forests using subsampling. Confidence regions and standard errors for variable importance. New improved holdout importance. Case-specific importance.  Minimal depth variable importance. Visualize trees on your Safari or Google Chrome browser. Anonymous random forests for data privacy.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://www.randomforestsrc.org/">https://www.randomforestsrc.org/</a> <a href="https://ishwaran.org/">https://ishwaran.org/</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-12-05 23:34:07 UTC; kogalur</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-12-06 14:30:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='breast'>Wisconsin Prognostic Breast Cancer Data</h2><span id='topic+breast'></span>

<h3>Description</h3>

<p>Recurrence of breast cancer from 198 breast cancer patients, all of
which exhibited no evidence of distant metastases at the time of
diagnosis.  The first 30 features of the data describe characteristics
of the cell nuclei present in the digitized image of a fine needle
aspirate (FNA) of the breast mass.
</p>


<h3>Source</h3>

<p>The data were obtained from the UCI machine learning repository, see
<a href="http://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Prognostic)">http://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Prognostic)</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## ------------------------------------------------------------
## Standard analysis
## ------------------------------------------------------------

data(breast, package = "randomForestSRC")
breast &lt;- na.omit(breast)
o &lt;- rfsrc(status ~ ., data = breast, nsplit = 10)
print(o)

</code></pre>

<hr>
<h2 id='find.interaction.rfsrc'>Find Interactions Between Pairs of Variables</h2><span id='topic+find.interaction'></span><span id='topic+find.interaction.rfsrc'></span>

<h3>Description</h3>

<p>Find pairwise interactions between variables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'rfsrc'
find.interaction(object, xvar.names, cause, m.target,
  importance = c("permute", "random", "anti",
                 "permute.ensemble", "random.ensemble", "anti.ensemble"),
  method = c("maxsubtree", "vimp"), sorted = TRUE, nvar, nrep = 1, 
  na.action = c("na.omit", "na.impute", "na.random"),
  seed = NULL, do.trace = FALSE, verbose = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="find.interaction.rfsrc_+3A_object">object</code></td>
<td>
<p>An object of class <code>(rfsrc, grow)</code> or
<code>(rfsrc, forest)</code>.</p>
</td></tr>  
<tr><td><code id="find.interaction.rfsrc_+3A_xvar.names">xvar.names</code></td>
<td>
<p>Character vector of names of target x-variables.
Default is to use all variables.</p>
</td></tr>
<tr><td><code id="find.interaction.rfsrc_+3A_cause">cause</code></td>
<td>
<p>For competing risk families, integer value between 1
and <code>J</code> indicating the event of interest, where <code>J</code> is
the number of event types. The default is to use the first event
type.</p>
</td></tr>
<tr><td><code id="find.interaction.rfsrc_+3A_m.target">m.target</code></td>
<td>
<p>Character value for multivariate families
specifying the target outcome to be used.  If left unspecified, the
algorithm will choose a default target.</p>
</td></tr>
<tr><td><code id="find.interaction.rfsrc_+3A_importance">importance</code></td>
<td>
<p>Type of variable importance (VIMP). See
<code>rfsrc</code> for details.</p>
</td></tr>
<tr><td><code id="find.interaction.rfsrc_+3A_method">method</code></td>
<td>
<p>Method of analysis: maximal subtree or VIMP.  See details
below.</p>
</td></tr>
<tr><td><code id="find.interaction.rfsrc_+3A_sorted">sorted</code></td>
<td>
<p>Should variables be sorted by VIMP?  Does not apply for
competing risks.</p>
</td></tr>
<tr><td><code id="find.interaction.rfsrc_+3A_nvar">nvar</code></td>
<td>
<p>Number of variables to be used.</p>
</td></tr>
<tr><td><code id="find.interaction.rfsrc_+3A_nrep">nrep</code></td>
<td>
<p>Number of Monte Carlo replicates when <span class="option">method="vimp"</span>.</p>
</td></tr>
<tr><td><code id="find.interaction.rfsrc_+3A_na.action">na.action</code></td>
<td>
<p>Action to be taken if the data contains <code>NA</code>
values.  Applies only when <span class="option">method="vimp"</span>.</p>
</td></tr>
<tr><td><code id="find.interaction.rfsrc_+3A_seed">seed</code></td>
<td>
<p>Seed for random number generator.  Must be a negative
integer.</p>
</td></tr>
<tr><td><code id="find.interaction.rfsrc_+3A_do.trace">do.trace</code></td>
<td>
<p>Number of seconds between updates to the user on
approximate time to completion.</p>
</td></tr>
<tr><td><code id="find.interaction.rfsrc_+3A_verbose">verbose</code></td>
<td>
<p>Set to <code>TRUE</code> for verbose output.</p>
</td></tr>
<tr><td><code id="find.interaction.rfsrc_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Using a previously grown forest, identify pairwise interactions for
all pairs of variables from a specified list.  There are two
distinct approaches specified by the option <span class="option">method</span>.
</p>

<ol>
<li> <p><span class="option">method="maxsubtree"</span>
</p>
<p>This invokes a maximal subtree analysis.  In this case, a matrix
is returned where entries [i][i] are the normalized minimal depth
of variable [i] relative to the root node (normalized wrt the
size of the tree) and entries [i][j] indicate the normalized
minimal depth of a variable [j] wrt the maximal subtree for
variable [i] (normalized wrt the size of [i]'s maximal subtree).
Smaller [i][i] entries indicate predictive variables.  Small
[i][j] entries having small [i][i] entries are a sign of an
interaction between variable i and j (note: the user should scan
rows, not columns, for small entries).  See Ishwaran et
al. (2010, 2011) for more details.
</p>
</li>
<li> <p><span class="option">method="vimp"</span>
</p>
<p>This invokes a joint-VIMP approach.  Two variables are paired and
their paired VIMP calculated (refered to as 'Paired' importance).
The VIMP for each separate variable is also calculated.  The sum
of these two values is refered to as 'Additive' importance.  A
large positive or negative difference between 'Paired' and
'Additive' indicates an association worth pursuing if the
univariate VIMP for each of the paired-variables is reasonably
large.  See Ishwaran (2007) for more details.
</p>
</li></ol>

<p>Computations might be slow depending upon the size of the data
and the forest.  In such cases, consider setting <span class="option">nvar</span> to a
smaller number.  If <span class="option">method="maxsubtree"</span>, consider using a
smaller number of trees in the original grow call.
</p>
<p>If <span class="option">nrep</span> is greater than 1, the analysis is repeated
<code>nrep</code> times and results averaged over the replications
(applies only when <span class="option">method="vimp"</span>).
</p>


<h3>Value</h3>

<p>Invisibly, the interaction table (a list for competing risk data)
or the maximal subtree matrix.
</p>


<h3>Author(s)</h3>

<p>Hemant Ishwaran and Udaya B. Kogalur
</p>


<h3>References</h3>

<p>Ishwaran H. (2007).  Variable importance in binary regression
trees and forests,  <em>Electronic J. Statist.</em>, 1:519-537.
</p>
<p>Ishwaran H., Kogalur U.B., Gorodeski E.Z, Minn A.J. and 
Lauer M.S. (2010).  High-dimensional variable selection for survival
data.  <em>J. Amer. Statist. Assoc.</em>, 105:205-217.
</p>
<p>Ishwaran H., Kogalur U.B., Chen X. and Minn A.J. (2011).  Random
survival forests for high-dimensional data. <em>Statist. Anal. Data
Mining</em>, 4:115-132.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+holdout.vimp.rfsrc">holdout.vimp.rfsrc</a></code>,
<code><a href="#topic+max.subtree.rfsrc">max.subtree.rfsrc</a></code>,
<code><a href="#topic+var.select.rfsrc">var.select.rfsrc</a></code>,
<code><a href="#topic+vimp.rfsrc">vimp.rfsrc</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## ------------------------------------------------------------
## find interactions, survival setting
## ------------------------------------------------------------

data(pbc, package = "randomForestSRC") 
pbc.obj &lt;- rfsrc(Surv(days,status) ~ ., pbc, importance = TRUE)
find.interaction(pbc.obj, method = "vimp", nvar = 8)

## ------------------------------------------------------------
## find interactions, competing risks
## ------------------------------------------------------------

data(wihs, package = "randomForestSRC")
wihs.obj &lt;- rfsrc(Surv(time, status) ~ ., wihs, nsplit = 3, ntree = 100,
                       importance = TRUE)
find.interaction(wihs.obj)
find.interaction(wihs.obj, method = "vimp")

## ------------------------------------------------------------
## find interactions, regression setting
## ------------------------------------------------------------

airq.obj &lt;- rfsrc(Ozone ~ ., data = airquality, importance = TRUE)
find.interaction(airq.obj, method = "vimp", nrep = 3)
find.interaction(airq.obj)

## ------------------------------------------------------------
## find interactions, classification setting
## ------------------------------------------------------------

iris.obj &lt;- rfsrc(Species ~., data = iris, importance = TRUE)
find.interaction(iris.obj, method = "vimp", nrep = 3)
find.interaction(iris.obj)

## ------------------------------------------------------------
## interactions for multivariate mixed forests
## ------------------------------------------------------------

mtcars2 &lt;- mtcars
mtcars2$cyl &lt;- factor(mtcars2$cyl)
mtcars2$carb &lt;- factor(mtcars2$carb, ordered = TRUE)
mv.obj &lt;- rfsrc(cbind(carb, mpg, cyl) ~., data = mtcars2, importance = TRUE)
find.interaction(mv.obj, method = "vimp", outcome.target = "carb")
find.interaction(mv.obj, method = "vimp", outcome.target = "mpg")
find.interaction(mv.obj, method = "vimp", outcome.target = "cyl")

</code></pre>

<hr>
<h2 id='follic'>Follicular Cell Lymphoma</h2><span id='topic+follic'></span>

<h3>Description</h3>

<p>Competing risk data set involving follicular cell lymphoma. 
</p>


<h3>Format</h3>

<p>A data frame containing:
</p>

<table>
<tr>
 <td style="text-align: left;">
    age      </td><td style="text-align: left;"> age</td>
</tr>
<tr>
 <td style="text-align: left;">
	  hgb      </td><td style="text-align: left;"> hemoglobin (g/l)</td>
</tr>
<tr>
 <td style="text-align: left;">
    clinstg	 </td><td style="text-align: left;"> clinical stage: 1=stage I, 2=stage II</td>
</tr>
<tr>
 <td style="text-align: left;">
	  ch       </td><td style="text-align: left;"> chemotherapy</td>
</tr>
<tr>
 <td style="text-align: left;">
	  rt       </td><td style="text-align: left;"> radiotherapy</td>
</tr>
<tr>
 <td style="text-align: left;">
    time     </td><td style="text-align: left;"> first failure time</td>
</tr>
<tr>
 <td style="text-align: left;">
    status   </td><td style="text-align: left;"> censoring status: 0=censored, 1=relapse, 2=death
  </td>
</tr>

</table>



<h3>Source</h3>

<p>Table 1.4b, <em>Competing Risks: A Practical Perspective</em>.</p>


<h3>References</h3>

<p>Pintilie M., (2006) <em>Competing Risks: A
Practical Perspective.</em> West Sussex: John Wiley and Sons.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(follic, package = "randomForestSRC")
follic.obj &lt;- rfsrc(Surv(time, status) ~ ., follic, nsplit = 3, ntree = 100)

</code></pre>

<hr>
<h2 id='get.tree.rfsrc'>Extract a Single Tree from a Forest and plot it on your browser</h2><span id='topic+get.tree.rfsrc'></span><span id='topic+get.tree'></span>

<h3>Description</h3>

<p>Extracts a single tree from a forest which can then be plotted on the users
browser.  Works for all families.  Missing data not permitted.</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'rfsrc'
get.tree(object, tree.id, target, m.target = NULL,
   time, surv.type = c("mort", "rel.freq", "surv", "years.lost", "cif", "chf"),
   class.type = c("bayes", "rfq", "prob"),
   ensemble = FALSE, oob = TRUE, show.plots = TRUE, do.trace = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get.tree.rfsrc_+3A_object">object</code></td>
<td>
<p>An object of class <code>(rfsrc, grow)</code>.</p>
</td></tr>
<tr><td><code id="get.tree.rfsrc_+3A_tree.id">tree.id</code></td>
<td>
<p>Integer value specifying the tree to be extracted.</p>
</td></tr>
<tr><td><code id="get.tree.rfsrc_+3A_target">target</code></td>
<td>
<p>For classification, an integer or
character value specifying the class to focus on (defaults to the
first class).  For competing risks, an integer value between
1 and <code>J</code> indicating the event of interest, where <code>J</code> is
the number of event types.  The default is to use the first event
type.</p>
</td></tr>
<tr><td><code id="get.tree.rfsrc_+3A_m.target">m.target</code></td>
<td>
<p>Character value for multivariate families
specifying the target outcome to be used.  If left unspecified, the
algorithm will choose a default target.</p>
</td></tr>
<tr><td><code id="get.tree.rfsrc_+3A_time">time</code></td>
<td>
<p>For survival, the time at which the predicted
survival value is evaluated at (depends on <code>surv.type</code>).</p>
</td></tr>
<tr><td><code id="get.tree.rfsrc_+3A_surv.type">surv.type</code></td>
<td>
<p>For survival, specifies the predicted value.
See details below.</p>
</td></tr>
<tr><td><code id="get.tree.rfsrc_+3A_class.type">class.type</code></td>
<td>
<p>For classification, specifies the predicted value.
See details below.</p>
</td></tr>
<tr><td><code id="get.tree.rfsrc_+3A_ensemble">ensemble</code></td>
<td>
<p>Use the ensemble (of all trees) for prediction, or use
the requested tree for prediction (this is the default).</p>
</td></tr>
<tr><td><code id="get.tree.rfsrc_+3A_oob">oob</code></td>
<td>
<p>OOB (TRUE) or in-bag (FALSE) predicted values.  Only
applies when <code>ensemble=TRUE</code>.</p>
</td></tr>
<tr><td><code id="get.tree.rfsrc_+3A_show.plots">show.plots</code></td>
<td>
<p>Should plots be displayed?</p>
</td></tr>
<tr><td><code id="get.tree.rfsrc_+3A_do.trace">do.trace</code></td>
<td>
<p>Number of seconds between updates to the user on
approximate time to completion.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Extracts a specified tree from a forest and converts the tree to a
hierarchical structure suitable for use with the &quot;data.tree&quot; package.
Plotting the object will conveniently render the tree on the users
browser. Left tree splits are displayed.  For continuous values, left
split is displayed as an inequality with right split equal to the
reversed inequality.  For factors, split values are described in terms
of the levels of the factor. In this case, the left daughter split is
a set consisting of all levels that are assigned to the left daughter
node.  The right daughter split is the complement of this set.
</p>
<p>Terminal nodes are highlighted by color and display the sample size
and predicted value.  By default, predicted value equals the tree
predicted value and sample size are terminal node inbag sample sizes.
If <code>ensemble=TRUE</code>, then the predicted value equals the forest
ensemble value which could be useful as it allows one to visualize the
ensemble predictor over a given tree and therefore for a given
partition of the feature space.  In this case, sample sizes are for
all cases and not the tree specific inbag cases.
</p>
<p>The predicted value displayed is as follows:
</p>

<ol>
<li><p> For regression, the mean of the response.
</p>
</li>
<li><p> For classification, for the target class specified by
<span class="option">target</span>, either the class with most votes if
<code>class.type="bayes"</code>; or in a two-class problem the classifier
using the RFQ quantile threshold if <code>class.type="bayes"</code>
(see <a href="#topic+imbalanced">imbalanced</a> for more details); or the relative class
frequency when <code>class.type="prob"</code>.
</p>
</li>
<li><p> For multivariate families, the predicted value of the outcome
specified by <span class="option">m.target</span>.  This being the value for regression
or classification described above, depending on whether the outcome
is real valued or a factor.
</p>
</li>
<li><p> For survival, the choices are:
</p>

<ul>
<li><p>  Mortality (<code>mort</code>).
</p>
</li>
<li><p>  Relative frequency of mortality (<code>rel.freq</code>).
</p>
</li>
<li><p> Predicted survival (<code>surv</code>), where the predicted
survival is for the time point specified using
<code>time</code> (the default is the median follow up time).
</p>
</li></ul>

</li>
<li><p> For competing risks, the choices are:
</p>

<ul>
<li><p>  The expected number of life years lost (<code>years.lost</code>).
</p>
</li>
<li><p>  The cumulative incidence function (<code>cif</code>).
</p>
</li>
<li><p>  The cumulative hazard function (<code>chf</code>).
</p>
</li></ul>

<p>In all three cases, the predicted value is for the event type
specified by <span class="option">target</span>.  For <code>cif</code> and
<code>chf</code> the quantity is evaluated at the time point specified
by <code>time</code>.
</p>
</li></ol>



<h3>Value</h3>

<p>Invisibly, returns an object with hierarchical structure formatted for use with
the data.tree package.
</p>


<h3>Author(s)</h3>

<p>Hemant Ishwaran and Udaya B. Kogalur
</p>
<p>Many thanks to @dbarg1 on GitHub for the initial prototype of this
function
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## ------------------------------------------------------------
## survival/competing risk
## ------------------------------------------------------------

## survival - veteran data set but with factors
## note that diagtime has many levels
data(veteran, package = "randomForestSRC")
vd &lt;- veteran
vd$celltype=factor(vd$celltype)
vd$diagtime=factor(vd$diagtime)
vd.obj &lt;- rfsrc(Surv(time,status)~., vd, ntree = 100, nodesize = 5)
plot(get.tree(vd.obj, 3))

## competing risks
data(follic, package = "randomForestSRC")
follic.obj &lt;- rfsrc(Surv(time, status) ~ ., follic, nsplit = 3, ntree = 100)
plot(get.tree(follic.obj, 2))

## ------------------------------------------------------------
## regression
## ------------------------------------------------------------

airq.obj &lt;- rfsrc(Ozone ~ ., data = airquality)
plot(get.tree(airq.obj, 10))

## ------------------------------------------------------------
## two-class imbalanced data (see imbalanced function)
## ------------------------------------------------------------

data(breast, package = "randomForestSRC")
breast &lt;- na.omit(breast)
f &lt;- as.formula(status ~ .)
breast.obj &lt;- imbalanced(f, breast)

## compare RFQ to Bayes Rule
plot(get.tree(breast.obj, 1, class.type = "rfq", ensemble = TRUE))
plot(get.tree(breast.obj, 1, class.type = "bayes", ensemble = TRUE))

## ------------------------------------------------------------
## classification
## ------------------------------------------------------------

iris.obj &lt;- rfsrc(Species ~., data = iris, nodesize = 10)

## equivalent
plot(get.tree(iris.obj, 25))
plot(get.tree(iris.obj, 25, class.type = "bayes"))

## predicted probability displayed for terminal nodes
plot(get.tree(iris.obj, 25, class.type = "prob", target = "setosa"))
plot(get.tree(iris.obj, 25, class.type = "prob", target = "versicolor"))
plot(get.tree(iris.obj, 25, class.type = "prob", target = "virginica"))


## ------------------------------------------------------------
## multivariate regression
## ------------------------------------------------------------

mtcars.mreg &lt;- rfsrc(Multivar(mpg, cyl) ~., data = mtcars)
plot(get.tree(mtcars.mreg, 10, m.target = "mpg"))
plot(get.tree(mtcars.mreg, 10, m.target = "cyl"))


## ------------------------------------------------------------
## multivariate mixed outcomes
## ------------------------------------------------------------

mtcars2 &lt;- mtcars
mtcars2$carb &lt;- factor(mtcars2$carb)
mtcars2$cyl &lt;- factor(mtcars2$cyl)
mtcars.mix &lt;- rfsrc(Multivar(carb, mpg, cyl) ~ ., data = mtcars2)
plot(get.tree(mtcars.mix, 5, m.target = "cyl"))
plot(get.tree(mtcars.mix, 5, m.target = "carb"))

## ------------------------------------------------------------
## unsupervised analysis
## ------------------------------------------------------------

mtcars.unspv &lt;- rfsrc(data = mtcars)
plot(get.tree(mtcars.unspv, 5))




</code></pre>

<hr>
<h2 id='hd'>Hodgkin's Disease</h2><span id='topic+hd'></span>

<h3>Description</h3>

<p>Competing risk data set involving Hodgkin's disease.
</p>


<h3>Format</h3>

<p>A data frame containing:
</p>

<table>
<tr>
 <td style="text-align: left;">
    age      </td><td style="text-align: left;"> age</td>
</tr>
<tr>
 <td style="text-align: left;">
    sex      </td><td style="text-align: left;"> gender</td>
</tr>
<tr>
 <td style="text-align: left;">
	  trtgiven </td><td style="text-align: left;"> treatment: RT=radition, CMT=Chemotherapy and radiation</td>
</tr>
<tr>
 <td style="text-align: left;">
	  medwidsi </td><td style="text-align: left;"> mediastinum involvement: N=no, S=small, L=Large</td>
</tr>
<tr>
 <td style="text-align: left;">
	  extranod </td><td style="text-align: left;"> extranodal disease: Y=extranodal disease, N=nodal disease</td>
</tr>
<tr>
 <td style="text-align: left;">
    clinstg	 </td><td style="text-align: left;"> clinical stage: 1=stage I, 2=stage II</td>
</tr>
<tr>
 <td style="text-align: left;">
    time     </td><td style="text-align: left;"> first failure time</td>
</tr>
<tr>
 <td style="text-align: left;">
    status   </td><td style="text-align: left;"> censoring status: 0=censored, 1=relapse, 2=death
  </td>
</tr>

</table>



<h3>Source</h3>

<p>Table 1.6b, <em>Competing Risks: A Practical Perspective</em>.
</p>


<h3>References</h3>

<p>Pintilie M., (2006) <em>Competing Risks: A
Practical Perspective.</em> West Sussex: John Wiley and Sons.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(hd, package = "randomForestSRC")</code></pre>

<hr>
<h2 id='holdout.vimp.rfsrc'>Hold out variable importance (VIMP)</h2><span id='topic+holdout.vimp.rfsrc'></span><span id='topic+holdout.vimp'></span>

<h3>Description</h3>

<p>Hold out VIMP is calculated from the error rate of mini ensembles of
trees (blocks of trees) grown with and without a variable.  Applies to
all families.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'rfsrc'
holdout.vimp(formula, data,
  ntree = function(p, vtry){1000 * p / vtry},
  nsplit = 10,
  ntime = 50,
  sampsize = function(x){x * .632},
  samptype = "swor",
  block.size = 10,
  vtry = 1,
  ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="holdout.vimp.rfsrc_+3A_formula">formula</code></td>
<td>
<p>A symbolic description of the model to be fit.</p>
</td></tr>
<tr><td><code id="holdout.vimp.rfsrc_+3A_data">data</code></td>
<td>
<p>Data frame containing the y-outcome and x-variables.</p>
</td></tr>
<tr><td><code id="holdout.vimp.rfsrc_+3A_ntree">ntree</code></td>
<td>
<p>Function specifying requested number of trees used for
growing the forest.  Inputs are dimension and number of holdout
variables. The requested number of trees can also be a number.</p>
</td></tr>
<tr><td><code id="holdout.vimp.rfsrc_+3A_nsplit">nsplit</code></td>
<td>
<p>Non-negative integer value specifying number of 
random split points used to split a node (deterministic splitting
corresponds to the value zero and is much slower).</p>
</td></tr>
<tr><td><code id="holdout.vimp.rfsrc_+3A_ntime">ntime</code></td>
<td>
<p>Integer value used for survival to
constrain ensemble calculations to a grid of <code>ntime</code> time points.</p>
</td></tr>
<tr><td><code id="holdout.vimp.rfsrc_+3A_sampsize">sampsize</code></td>
<td>
<p>Function specifying size of subsampled data. Can also
be a number.</p>
</td></tr>
<tr><td><code id="holdout.vimp.rfsrc_+3A_samptype">samptype</code></td>
<td>
<p>Type of bootstrap used.</p>
</td></tr>
<tr><td><code id="holdout.vimp.rfsrc_+3A_vtry">vtry</code></td>
<td>
<p>Number of variables randomly selected to be held out when
growing a tree.  This can also be set to a list for a targeted hold
out VIMP analysis.  See details below for more information.</p>
</td></tr>
<tr><td><code id="holdout.vimp.rfsrc_+3A_block.size">block.size</code></td>
<td>
<p>Specifies number of trees in a block when calculating
holdout variable importance.</p>
</td></tr>
<tr><td><code id="holdout.vimp.rfsrc_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed to <code><a href="#topic+rfsrc">rfsrc</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Holdout variable importance (holdout VIMP) is based on comparing error
performance of two mini forests of trees (blocks of trees): the first in
which a random set of <code>vtry</code> features are held out (the holdout
forest), and the second in which no features are held out (the
baseline forest).  
</p>
<p>To summarize, holdout VIMP measures the importance of a variable when
that variable is truly removed from the tree growing process.
</p>
<p>Specifically, if a feature is held out in a block of trees, we refer
to this as the (feature, block) pair.  The bootstrap for the trees in
a (feature, block) pair are identical in both forests. That is, the
holdout block is grown by holding out the feature, and the baseline
block is grown over the same trees, with the same bootstrap, but
without holding out any features.  <code>vtry</code> controls how many
features are held out in every tree. If set to one (default), only one
variable is held out in every tree.  Once a (feature, block) of trees
has been grown, holdout VIMP for a given variable v is calculated as
follows.  Gather the block of trees where the feature was held out
(from the holdout forest) and calculate OOB prediction error.  Next
gather the corresponding block of trees where v was not held out (from
the baseline forest) and calculate OOB prediction error.  Holdout VIMP
for the (feature, block) pair is the difference between these two
values.  The final holdout VIMP estimate for a feature v is obtained
by averaging holdout VIMP for (feature=v, block) over all blocks.
</p>
<p>Accuracy of hold out VIMP depends critically on total number of trees.
If total number of trees is too small, then number of times a variable
is held out will be small and OOB error can suffer from high variance.
Therefore, <code>ntree</code> should be set fairly high&mdash;we recommend using
1000 times the number of features.  Increasing <code>vtry</code> is another
way to increase number of times a variable is held out and therefore
reduces the burden of growing a large number of trees.  In particular,
total number of trees needed decreases linearly with <code>vtry</code>.  The
default <code>ntree</code> equals 1000 trees for each feature divided by
<code>vtry</code>.  Keep in mind intrepretation of holdout VIMP is altered
when <code>vtry</code> is different than one.  Thus this option should be
used with caution.
</p>
<p>Accuracy also depends on the value of <code>block.size</code>.  Smaller
values generally produce better results but are more computationally
demanding.  The most computationally demanding, but most accurate, is
<code>block.size=1</code>.  This is similar to how <code>block.size</code> is used
for usual variable importance: see the help file for <code>rfsrc</code>
for details.  Note the value of <code>block.size</code> should not exceed
<code>ntree</code> divided by number of features, otherwise there may not be
enough trees to satisify the target block size for a feature and
missing values will result.
</p>
<p>A targeted hold out VIMP analysis can be requested by setting
<code>vtry</code> to a list with two entries.  The first entry is a vector
of integer values specifying the variables of interest.  The second
entry is a boolean logical flag indicating whether individual or joint
VIMP should be calculated.  For example, suppose variables 1, 4 and 5
are our variables of interest.  To calculate holdout VIMP for these
variables, and these variables only, <code>vtry</code> would be specified by
</p>
<p>vtry = list(xvar = c(1, 4, 5), joint = FALSE)
</p>
<p>On the other hand, if we are interested in the joint effect when we
remove the three variables simultaneously, then
</p>
<p>vtry = list(xvar = c(1, 4, 5), joint = TRUE)
</p>
<p>The benefits of a targeted analysis is that the user may have a
pre-conceived idea of which variables are interesting.  Only VIMP for
these variables will be calculated which greatly reduces
computational time.  Another benefit is that when joint VIMP is
requested, this provides the user with a way to assess importance of
specific groups of variables.  See the iris example below for
illustration.
</p>


<h3>Value</h3>

<p>Invisibly a list with the following components (which themselves can be lists):
</p>
<table>
<tr><td><code>importance</code></td>
<td>
<p>Holdout VIMP.</p>
</td></tr>
<tr><td><code>baseline</code></td>
<td>
<p>Prediction error for the baseline forest.</p>
</td></tr>
<tr><td><code>holdout</code></td>
<td>
<p>Prediction error for the holdout forest.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Hemant Ishwaran and Udaya B. Kogalur
</p>


<h3>References</h3>

<p>Lu M. and Ishwaran H. (2018). Expert Opinion: A prediction-based
alternative to p-values in regression models. <em>J. Thoracic and
Cardiovascular Surgery</em>, 155(3), 1130&ndash;1136.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+vimp.rfsrc">vimp.rfsrc</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>


## ------------------------------------------------------------
## regression analysis
## ------------------------------------------------------------

## new York air quality measurements
airq.obj &lt;- holdout.vimp(Ozone ~ ., data = airquality, na.action = "na.impute")
print(airq.obj$importance)

## ------------------------------------------------------------
## classification analysis
## ------------------------------------------------------------

## iris data
iris.obj &lt;- holdout.vimp(Species ~., data = iris)
print(iris.obj$importance)

## iris data using brier prediction error
iris.obj &lt;- holdout.vimp(Species ~., data = iris, perf.type = "brier")
print(iris.obj$importance)

## ------------------------------------------------------------
## illustration of targeted holdout vimp analysis
## ------------------------------------------------------------

## iris data - only interested in variables 3 and 4
vtry &lt;- list(xvar = c(3, 4), joint = FALSE)
print(holdout.vimp(Species ~., data = iris, vtry = vtry)$impor)

## iris data - joint importance of variables 3 and 4
vtry &lt;- list(xvar = c(3, 4), joint = TRUE)
print(holdout.vimp(Species ~., data = iris, vtry = vtry)$impor)

## iris data - joint importance of variables 1 and 2
vtry &lt;- list(xvar = c(1, 2), joint = TRUE)
print(holdout.vimp(Species ~., data = iris, vtry = vtry)$impor)


## ------------------------------------------------------------
## imbalanced classification (using RFQ)
## ------------------------------------------------------------

if (library("caret", logical.return = TRUE)) {

  ## experimental settings
  n &lt;- 400
  q &lt;- 20
  ir &lt;- 6
  f &lt;- as.formula(Class ~ .)
 
  ## simulate the data, create minority class data
  d &lt;- twoClassSim(n, linearVars = 15, noiseVars = q)
  d$Class &lt;- factor(as.numeric(d$Class) - 1)
  idx.0 &lt;- which(d$Class == 0)
  idx.1 &lt;- sample(which(d$Class == 1), sum(d$Class == 1) / ir , replace = FALSE)
  d &lt;- d[c(idx.0,idx.1),, drop = FALSE]

  ## VIMP for RFQ with and without blocking
  vmp1 &lt;- imbalanced(f, d, importance = TRUE, block.size = 1)$importance[, 1]
  vmp10 &lt;- imbalanced(f, d, importance = TRUE, block.size = 10)$importance[, 1]

  ## holdout VIMP for RFQ with and without blocking
  hvmp1 &lt;- holdout.vimp(f, d, rfq =  TRUE,
               perf.type = "g.mean", block.size = 1)$importance[, 1]
  hvmp10 &lt;- holdout.vimp(f, d, rfq =  TRUE,
               perf.type = "g.mean", block.size = 10)$importance[, 1]
  
  ## compare VIMP values
  imp &lt;- 100 * cbind(vmp1, vmp10, hvmp1, hvmp10)
  legn &lt;- c("vimp-1", "vimp-10","hvimp-1", "hvimp-10")
  colr &lt;- rep(4,20+q)
  colr[1:20] &lt;- 2
  ylim &lt;- range(c(imp))
  nms &lt;- 1:(20+q)
  par(mfrow=c(2,2))
  barplot(imp[,1],col=colr,las=2,main=legn[1],ylim=ylim,names.arg=nms)
  barplot(imp[,2],col=colr,las=2,main=legn[2],ylim=ylim,names.arg=nms)
  barplot(imp[,3],col=colr,las=2,main=legn[3],ylim=ylim,names.arg=nms)
  barplot(imp[,4],col=colr,las=2,main=legn[4],ylim=ylim,names.arg=nms)

}

## ------------------------------------------------------------
## multivariate regression analysis
## ------------------------------------------------------------
mtcars.mreg &lt;- holdout.vimp(Multivar(mpg, cyl) ~., data = mtcars,
                                    vtry = 3,
                                    block.size = 1,
                                    samptype = "swr",
                                    sampsize = dim(mtcars)[1])
print(mtcars.mreg$importance)

## ------------------------------------------------------------
## mixed outcomes analysis
## ------------------------------------------------------------

mtcars.new &lt;- mtcars
mtcars.new$cyl &lt;- factor(mtcars.new$cyl)
mtcars.new$carb &lt;- factor(mtcars.new$carb, ordered = TRUE)
mtcars.mix &lt;- holdout.vimp(cbind(carb, mpg, cyl) ~., data = mtcars.new,
                                   ntree = 100,
                                   block.size = 2,
                                   vtry = 1)
print(mtcars.mix$importance)

##------------------------------------------------------------
## survival analysis
##------------------------------------------------------------

## Primary biliary cirrhosis (PBC) of the liver
data(pbc, package = "randomForestSRC")
pbc.obj &lt;- holdout.vimp(Surv(days, status) ~ ., pbc,
                                nsplit = 10,
                                ntree = 1000,
                                na.action = "na.impute")
print(pbc.obj$importance)

##------------------------------------------------------------
## competing risks
##------------------------------------------------------------

## WIHS analysis
## cumulative incidence function (CIF) for HAART and AIDS stratified by IDU

data(wihs, package = "randomForestSRC")
wihs.obj &lt;- holdout.vimp(Surv(time, status) ~ ., wihs,
                                 nsplit = 3,
                                 ntree = 100)
print(wihs.obj$importance)

</code></pre>

<hr>
<h2 id='housing'>Ames Iowa Housing Data</h2><span id='topic+housing'></span>

<h3>Description</h3>

<p>Data from the Ames Assessor's Office used in assessing values of
individual residential properties sold in Ames, Iowa from 2006 to
2010.  This is a regression problem and the goal is to predict 
&quot;SalePrice&quot; which records the price of a home in thousands of dollars.
</p>


<h3>References</h3>

<p>De Cock, D., (2011). Ames, Iowa: Alternative to the Boston housing
data as an end of semester regression project. <em>Journal of Statistics
Education</em>, 19(3), 1&ndash;14. 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## load the data
data(housing, package = "randomForestSRC")

## the original data contains lots of missing data, so impute it
## use missForest, can be slow so grow trees with small training sizes
housing2 &lt;- impute(data = housing, mf.q = 1, sampsize = function(x){x * .1})

## same idea ... but directly use rfsrc.fast and multivariate missForest 
housing3 &lt;- impute(data = housing, mf.q = .5, fast = TRUE)

## even faster, but potentially less acurate
housing4 &lt;- impute(SalePrice~., housing, splitrule = "random", nimpute = 1)


</code></pre>

<hr>
<h2 id='imbalanced.rfsrc'>Imbalanced Two Class Problems</h2><span id='topic+imbalanced.rfsrc'></span><span id='topic+imbalanced'></span><span id='topic+get.imbalanced.performance'></span><span id='topic+get.imbalanced.optimize'></span><span id='topic+get.pr.auc'></span><span id='topic+get.pr.curve'></span><span id='topic+get.rfq.threshold'></span>

<h3>Description</h3>

<p>Implements various solutions to the two-class imbalanced problem,
including the newly proposed quantile-classifier approach of
O'Brien and Ishwaran (2017).  Also includes Breiman's balanced random
forests undersampling of the majority class.  Performance is assesssed
using the G-mean, but misclassification error can be requested.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'rfsrc'
imbalanced(formula, data, ntree = 3000, 
  method = c("rfq", "brf", "standard"), splitrule = "auc",
  perf.type = NULL, block.size = NULL, fast = FALSE,
  ratio = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="imbalanced.rfsrc_+3A_formula">formula</code></td>
<td>
<p>A symbolic description of the model to be fit.</p>
</td></tr>
<tr><td><code id="imbalanced.rfsrc_+3A_data">data</code></td>
<td>
<p>Data frame containing the two-class y-outcome and
x-variables.</p>
</td></tr>
<tr><td><code id="imbalanced.rfsrc_+3A_ntree">ntree</code></td>
<td>
<p>Number of trees.</p>
</td></tr>
<tr><td><code id="imbalanced.rfsrc_+3A_method">method</code></td>
<td>
<p>Method used for fitting the classifier.  The default is
<code>rfq</code> which is the random forests quantile-classifer (RFQ)
approach of O'Brien and Ishwaran (2017).  The method <code>brf</code>
implements the balanced random forest (BRF) method of Chen et
al. (2004) which undersamples the majority class so that its
cardinality matches that of the minority class.  The method
<code>standard</code> implements a standard random forest analysis.</p>
</td></tr>
<tr><td><code id="imbalanced.rfsrc_+3A_splitrule">splitrule</code></td>
<td>
<p>Default is AUC splitting which maximizes gmean
performance.  Other choices are &quot;gini&quot; and &quot;entropy&quot;.</p>
</td></tr>
<tr><td><code id="imbalanced.rfsrc_+3A_perf.type">perf.type</code></td>
<td>
<p>Measure used for assessing performance (and all
downstream calculations based on it such as variable importance).
The default for <code>rfq</code> and <code>brf</code> is to use the G-mean
(Kubat et al., 1997).  For standard random forests, the default is
misclassification error.  Users can over-ride the default
performance measure by manually selecting either <code>gmean</code> for
the G-mean, <code>misclass</code> for misclassification error, or
<code>brier</code> for the normalized Brier score. See the examples
below.</p>
</td></tr>
<tr><td><code id="imbalanced.rfsrc_+3A_block.size">block.size</code></td>
<td>
<p>Should the cumulative error rate be calculated on
every tree?  When <code>NULL</code>, it will only be calculated on the
last tree. If importance is requested, VIMP is calculated in
&quot;blocks&quot; of size equal to <code>block.size</code>.  If not specified, uses
the default value specified in <code>rfsrc</code>.</p>
</td></tr>
<tr><td><code id="imbalanced.rfsrc_+3A_fast">fast</code></td>
<td>
<p>Use fast random forests, <code>rfsrc.fast</code>, in place of
<code>rfsrc</code>?  Improves speed but is less accurate.  Only applies to
RFQ.</p>
</td></tr>
<tr><td><code id="imbalanced.rfsrc_+3A_ratio">ratio</code></td>
<td>
<p>This is an optional parameter for expert users 
and included only for experimental purposes.  Used to specify the
ratio (between 0 and 1) for undersampling the majority class.
Sampling is without replacement.  Option is ignored for BRF.</p>
</td></tr> 
<tr><td><code id="imbalanced.rfsrc_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed to the <code>rfsrc</code>
function to specify random forest parameters.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Imbalanced data, or the so-called imbalanced minority class problem,
refers to classification settings involving two-classes where the
ratio of the majority class to the minority class is much larger than
one. Two solutions to the two-class imbalanced problem are provided
here, including the newly proposed random forests quantile-classifier
(RFQ) of O'Brien and Ishwaran (2017), and the balanced random forests
(BRF) undersampling approach of Chen et al. (2004).  The default
performance metric is the G-mean (Kubat et al., 1997).
</p>
<p>Currently, missing values cannot be handled for BRF or when the
<code>ratio</code> option is used; in these cases, missing data is removed
prior to the analysis.
</p>
<p>Permutation VIMP is used by default and not anti-VIMP which is the
default for all other families and settings.  Our experiments indicate
the former performs better in imbalanced settings, especially when
imbalanced ratio is high.
</p>
<p>We recommend setting <code>ntree</code> to a relatively large value when
dealing with imbalanced data to ensure convergence of the performance
value &ndash; this is especially true for the G-mean.  Consider using 5 times
the usual number of trees.
</p>
<p>A new helper function <code>get.imbalanced.performance</code> has been added
for extracting performance metrics.  Metrics are self-titled and their
meaning should generally be clear.  Metrics that may be less familiar
include: F1, the F-score or the F-measure which measures balance
between the precision and the recall.  F1mod, the harmonic mean of
sensitivity, specificity, precision and the negative predictive value.
F1gmean, the average of F1 and the G-mean.  F1modgmean, the average of
F1mod and the G-mean.
</p>


<h3>Value</h3>

<p>A two-class random forest fit under the requested method and
performance value.
</p>


<h3>Author(s)</h3>

<p>Hemant Ishwaran and Udaya B. Kogalur
</p>


<h3>References</h3>

<p>Chen, C., Liaw, A. and Breiman, L. (2004). Using random forest to learn
imbalanced data. University of California, Berkeley, Technical Report
110.
</p>
<p>Kubat, M., Holte, R. and Matwin, S. (1997). Learning when negative
examples abound. <em>Machine Learning</em>, ECML-97: 146-153.
</p>
<p>O'Brien R. and Ishwaran H. (2019).  A random forests quantile
classifier for class imbalanced data. <em>Pattern Recognition</em>,
90, 232-249
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rfsrc">rfsrc</a></code>,
<code><a href="#topic+rfsrc.fast">rfsrc.fast</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## ------------------------------------------------------------
## use the breast data for illustration
## ------------------------------------------------------------

data(breast, package = "randomForestSRC")
breast &lt;- na.omit(breast)
f &lt;- as.formula(status ~ .)

##----------------------------------------------------------------
## default RFQ call
##----------------------------------------------------------------

o.rfq &lt;- imbalanced(f, breast)
print(o.rfq)

## equivalent to:
## rfsrc(f, breast, rfq =  TRUE, ntree = 3000,
##       perf.type = "gmean", splitrule = "auc") 

##----------------------------------------------------------------
## detailed output using customized performance function
##----------------------------------------------------------------

print(get.imbalanced.performance(o.rfq))

##-----------------------------------------------------------------
## RF using misclassification error with gini splitting
## ------------------------------------------------------------

o.std &lt;- imbalanced(f, breast, method = "stand", splitrule = "gini")

##-----------------------------------------------------------------
## RF using G-mean performance with AUC splitting
## ------------------------------------------------------------

o.std &lt;- imbalanced(f, breast, method = "stand", perf.type = "gmean")

## equivalent to:
## rfsrc(f, breast, ntree = 3000, perf.type = "gmean", splitrule = "auc")

##----------------------------------------------------------------
## default BRF call 
##----------------------------------------------------------------

o.brf &lt;- imbalanced(f, breast, method = "brf")

## equivalent to:
## imbalanced(f, breast, method = "brf", perf.type = "gmean")

##----------------------------------------------------------------
## BRF call with misclassification performance 
##----------------------------------------------------------------

o.brf &lt;- imbalanced(f, breast, method = "brf", perf.type = "misclass")

##----------------------------------------------------------------
## train/test example
##----------------------------------------------------------------

trn &lt;- sample(1:nrow(breast), size = nrow(breast) / 2)
o.trn &lt;- imbalanced(f, breast[trn,], importance = TRUE)
o.tst &lt;- predict(o.trn, breast[-trn,], importance = TRUE)
print(o.trn)
print(o.tst)
print(100 * cbind(o.trn$impo[, 1], o.tst$impo[, 1]))


##----------------------------------------------------------------
##
##  illustrates how to optimize threshold on training data
##  improves Gmean for RFQ in many situations
##
##----------------------------------------------------------------

if (library("caret", logical.return = TRUE)) {

  ## experimental settings
  n &lt;- 2 * 5000 
  q &lt;- 20
  ir &lt;- 6
  f &lt;- as.formula(Class ~ .)
 
  ## simulate the data, create minority class data
  d &lt;- twoClassSim(n, linearVars = 15, noiseVars = q)
  d$Class &lt;- factor(as.numeric(d$Class) - 1)
  idx.0 &lt;- which(d$Class == 0)
  idx.1 &lt;- sample(which(d$Class == 1), sum(d$Class == 1) / ir , replace = FALSE)
  d &lt;- d[c(idx.0,idx.1),, drop = FALSE]

  ## split data into train and test
  trn.pt &lt;- sample(1:nrow(d), size = nrow(d) / 2)
  trn &lt;- d[trn.pt, ]
  tst &lt;- d[setdiff(1:nrow(d), trn.pt), ]

  ## run rfq on training data
  o &lt;- imbalanced(f, trn)

  ## (1) default threshold (2) directly optimized gmean threshold
  th.1 &lt;- get.imbalanced.performance(o)["threshold"]
  th.2 &lt;- get.imbalanced.optimize(o)["threshold"]

  ## training performance
  cat("-------- train performance ---------\n")
  print(get.imbalanced.performance(o, thresh=th.1))
  print(get.imbalanced.performance(o, thresh=th.2))

  ## test performance
  cat("-------- test performance ---------\n")
  pred.o &lt;- predict(o, tst)
  print(get.imbalanced.performance(pred.o, thresh=th.1))
  print(get.imbalanced.performance(pred.o, thresh=th.2))
 
} 

##----------------------------------------------------------------
##  illustrates RFQ with and without SMOTE
## 
## - simulation example using the caret R-package
## - creates imbalanced data by randomly sampling the class 1 data
## - use SMOTE from "imbalance" package to oversample the minority
## 
##----------------------------------------------------------------

if (library("caret", logical.return = TRUE) &amp;
    library("imbalance", logical.return = TRUE)) {

  ## experimental settings
  n &lt;- 5000
  q &lt;- 20
  ir &lt;- 6
  f &lt;- as.formula(Class ~ .)
 
  ## simulate the data, create minority class data
  d &lt;- twoClassSim(n, linearVars = 15, noiseVars = q)
  d$Class &lt;- factor(as.numeric(d$Class) - 1)
  idx.0 &lt;- which(d$Class == 0)
  idx.1 &lt;- sample(which(d$Class == 1), sum(d$Class == 1) / ir , replace = FALSE)
  d &lt;- d[c(idx.0,idx.1),, drop = FALSE]
  d &lt;- d[sample(1:nrow(d)), ]

  ## define train/test split
  trn &lt;- sample(1:nrow(d), size = nrow(d) / 2, replace = FALSE)

  ## now make SMOTE training data
  newd.50 &lt;- mwmote(d[trn, ], numInstances = 50, classAttr = "Class")
  newd.500 &lt;- mwmote(d[trn, ], numInstances = 500, classAttr = "Class")

  ## fit RFQ with and without SMOTE
  o.with.50 &lt;- imbalanced(f, rbind(d[trn, ], newd.50)) 
  o.with.500 &lt;- imbalanced(f, rbind(d[trn, ], newd.500))
  o.without &lt;- imbalanced(f, d[trn, ])
  
  ## compare performance on test data
  print(predict(o.with.50, d[-trn, ]))
  print(predict(o.with.500, d[-trn, ]))
  print(predict(o.without, d[-trn, ]))
  
}

##----------------------------------------------------------------
##
## illustrates effectiveness of blocked VIMP 
##
##----------------------------------------------------------------

if (library("caret", logical.return = TRUE)) {

  ## experimental settings
  n &lt;- 1000
  q &lt;- 20
  ir &lt;- 6
  f &lt;- as.formula(Class ~ .)
 
  ## simulate the data, create minority class data
  d &lt;- twoClassSim(n, linearVars = 15, noiseVars = q)
  d$Class &lt;- factor(as.numeric(d$Class) - 1)
  idx.0 &lt;- which(d$Class == 0)
  idx.1 &lt;- sample(which(d$Class == 1), sum(d$Class == 1) / ir , replace = FALSE)
  d &lt;- d[c(idx.0,idx.1),, drop = FALSE]

  ## permutation VIMP for BRF with and without blocking
  ## blocked VIMP is a hybrid of Breiman-Cutler/Ishwaran-Kogalur VIMP
  brf &lt;- imbalanced(f, d, method = "brf", importance = "permute", block.size = 1)
  brfB &lt;- imbalanced(f, d, method = "brf", importance = "permute", block.size = 10)

  ## permutation VIMP for RFQ with and without blocking
  rfq &lt;- imbalanced(f, d, importance = "permute", block.size = 1)
  rfqB &lt;- imbalanced(f, d, importance = "permute", block.size = 10)

  ## compare VIMP values
  imp &lt;- 100 * cbind(brf$importance[, 1], brfB$importance[, 1],
                     rfq$importance[, 1], rfqB$importance[, 1])
  legn &lt;- c("BRF", "BRF-block", "RFQ", "RFQ-block")
  colr &lt;- rep(4,20+q)
  colr[1:20] &lt;- 2
  ylim &lt;- range(c(imp))
  nms &lt;- 1:(20+q)
  par(mfrow=c(2,2))
  barplot(imp[,1],col=colr,las=2,main=legn[1],ylim=ylim,names.arg=nms)
  barplot(imp[,2],col=colr,las=2,main=legn[2],ylim=ylim,names.arg=nms)
  barplot(imp[,3],col=colr,las=2,main=legn[3],ylim=ylim,names.arg=nms)
  barplot(imp[,4],col=colr,las=2,main=legn[4],ylim=ylim,names.arg=nms)

}

##----------------------------------------------------------------
##
## confidence intervals for G-mean permutation VIMP using subsampling
##
##----------------------------------------------------------------

if (library("caret", logical.return = TRUE)) {

  ## experimental settings
  n &lt;- 1000
  q &lt;- 20
  ir &lt;- 6
  f &lt;- as.formula(Class ~ .)
 
  ## simulate the data, create minority class data
  d &lt;- twoClassSim(n, linearVars = 15, noiseVars = q)
  d$Class &lt;- factor(as.numeric(d$Class) - 1)
  idx.0 &lt;- which(d$Class == 0)
  idx.1 &lt;- sample(which(d$Class == 1), sum(d$Class == 1) / ir , replace = FALSE)
  d &lt;- d[c(idx.0,idx.1),, drop = FALSE]

  ## RFQ
  o &lt;- imbalanced(Class ~ ., d, importance = "permute", block.size = 10)

  ## subsample RFQ
  smp.o &lt;- subsample(o, B = 100)
  plot(smp.o, cex.axis = .7)

}


</code></pre>

<hr>
<h2 id='impute.rfsrc'>Impute Only Mode</h2><span id='topic+impute.rfsrc'></span><span id='topic+impute'></span>

<h3>Description</h3>

<p>Fast imputation mode.  A random forest is grown and used to impute
missing data.  No ensemble estimates or error rates are calculated.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'rfsrc'
impute(formula, data,
  ntree = 100, nodesize = 1, nsplit = 10,
  nimpute = 2, fast = FALSE, blocks, 
  mf.q, max.iter = 10, eps = 0.01, 
  ytry = NULL, always.use = NULL, verbose = TRUE,
  ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="impute.rfsrc_+3A_formula">formula</code></td>
<td>
<p>A symbolic description of the model to be fit.  Can be
left unspecified if there are no outcomes or we don't care to
distinguish between y-outcomes and x-variables in the imputation.
Ignored when using multivariate missForest imputation.</p>
</td></tr>
<tr><td><code id="impute.rfsrc_+3A_data">data</code></td>
<td>
<p>Data frame containing the data to be imputed.</p>
</td></tr>
<tr><td><code id="impute.rfsrc_+3A_ntree">ntree</code></td>
<td>
<p>Number of trees to grow.</p>
</td></tr>
<tr><td><code id="impute.rfsrc_+3A_nodesize">nodesize</code></td>
<td>
<p>Forest average terminal node size.</p>
</td></tr>
<tr><td><code id="impute.rfsrc_+3A_nsplit">nsplit</code></td>
<td>
<p>Non-negative integer value used to specify random splitting.</p>
</td></tr>
<tr><td><code id="impute.rfsrc_+3A_nimpute">nimpute</code></td>
<td>
<p>Number of iterations of the missing data algorithm.
Ignored for multivariate missForest; in which case the algorithm
iterates until a convergence criteria is achieved (users can
however enforce a maximum number of iterations with the option
<code>max.iter</code>).</p>
</td></tr>
<tr><td><code id="impute.rfsrc_+3A_fast">fast</code></td>
<td>
<p>Use fast random forests, <code>rfsrcFast</code>, in place of
<code>rfsrc</code>?  Improves speed but is less accurate.</p>
</td></tr>
<tr><td><code id="impute.rfsrc_+3A_blocks">blocks</code></td>
<td>
<p>Integer value specifying the number of blocks the data
should be broken up into (by rows).  This can improve computational
efficiency when the sample size is large but imputation efficiency
decreases.  By default, no action is taken if left unspecified.</p>
</td></tr>
<tr><td><code id="impute.rfsrc_+3A_mf.q">mf.q</code></td>
<td>
<p>Use this to turn on missForest (which is off by default).
Specifies fraction of variables (between 0 and 1) used as responses
in multivariate missForest imputation.  When set to 1 this
corresponds to missForest, otherwise multivariate missForest is
used. Can also be an integer, in
which case this equals the number of multivariate responses.</p>
</td></tr>
<tr><td><code id="impute.rfsrc_+3A_max.iter">max.iter</code></td>
<td>
<p>Maximum number of iterations used when implementing
multivariate missForest imputation.</p>
</td></tr>
<tr><td><code id="impute.rfsrc_+3A_eps">eps</code></td>
<td>
<p>Tolerance value used to determine convergence of
multivariate missForest imputation.</p>
</td></tr>
<tr><td><code id="impute.rfsrc_+3A_ytry">ytry</code></td>
<td>
<p>Number of variables used as pseudo-responses in
unsupervised forests.  See details below.</p>
</td></tr>
<tr><td><code id="impute.rfsrc_+3A_always.use">always.use</code></td>
<td>
<p>Character vector of variable names to always
be included as a response in multivariate missForest imputation.
Does not apply for other imputation methods.</p>
</td></tr>
<tr><td><code id="impute.rfsrc_+3A_verbose">verbose</code></td>
<td>
<p>Send verbose output to terminal (only applies to
multivariate missForest imputation).</p>
</td></tr>
<tr><td><code id="impute.rfsrc_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>


<ol>
<li><p> Grow a forest and use this to impute data.  All external
calculations such as ensemble calculations, error rates, etc. are
turned off.  Use this function if your only interest is imputing the
data.
</p>
</li>
<li><p> Split statistics are calculated using non-misssing data only.
If a node splits on a variable with missing data, the variable's
missing data is imputed by randomly drawing values from non-missing
in-bag data.  The purpose of this is to make it possible to assign
cases to daughter nodes based on the split.
</p>
</li>
<li><p> If no formula is specified, unsupervised splitting is
implemented using a <code>ytry</code> value of sqrt(<code>p</code>) where
<code>p</code> equals the number of variables.  More precisely,
<code>mtry</code> variables are selected at random, and for each of these
a random subset of <code>ytry</code> variables are selected and defined as
the multivariate pseudo-responses.  A multivariate composite
splitting rule of dimension <code>ytry</code> is then applied to each of
the <code>mtry</code> multivariate regression problems and the node split
on the variable leading to the best split (Tang and Ishwaran, 2017).
</p>
</li>
<li><p> If <code>mf.q</code> is specified, a multivariate version of
missForest imputation (Stekhoven and Buhlmann, 2012) is applied.
Specifically, a fraction <code>mf.q</code> of variables are used as
multivariate responses and split by the remaining variables using
multivariate composite splitting (Tang and Ishwaran, 2017).  Missing
data for responses are imputed by prediction.  The process is
repeated using a new set of variables for responses (mutually
exclusive to the previous fit), until all variables have been
imputed.  This is one iteration.  The entire process is repeated,
and the algorithm iterated until a convergence criteria is met
(specified using options <code>max.iter</code> and <code>eps</code>).  Integer
values for <code>mf.q</code> are allowed and interpreted as a request that
<code>mf.q</code> variables be selected for the multivariate response.  If
<code>mf.q=1</code>, the algorithm reverts to the original missForest
procedure. This is generally the most accurate of all the
imputation procedures, but also the most computationally demanding.
See examples below for strategies to increase speed.
</p>
</li>
<li><p> Prior to imputation, the data is processed and records with
all values missing are removed, as are variables having all missing
values.
</p>
</li>
<li><p> If there is no missing data, either before or after processing
of the data, the algorithm returns the processed data and no
imputation is performed.
</p>
</li>
<li><p> All options are the same as <code>rfsrc</code> and the user should
consult the <code>rfsrc</code> help file for details.
</p>
</li></ol>



<h3>Value</h3>

<p>Invisibly, the data frame containing the orginal data with imputed
data overlaid.
</p>


<h3>Author(s)</h3>

<p>Hemant Ishwaran and Udaya B. Kogalur
</p>


<h3>References</h3>

<p>Ishwaran H., Kogalur U.B., Blackstone E.H. and Lauer M.S.
(2008).  Random survival forests, <em>Ann. App.
Statist.</em>, 2:841-860.
</p>
<p>Stekhoven D.J. and Buhlmann P. (2012). MissForest&ndash;non-parametric
missing value imputation for mixed-type data.
<em>Bioinformatics</em>, 28(1):112-118.
</p>
<p>Tang F. and Ishwaran H. (2017).  Random forest missing data
algorithms.  <em>Statistical Analysis and Data Mining</em>, 10:363-377.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rfsrc">rfsrc</a></code>,
<code><a href="#topic+rfsrc.fast">rfsrc.fast</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## ------------------------------------------------------------
## example of survival imputation
## ------------------------------------------------------------

## default everything - unsupervised splitting
data(pbc, package = "randomForestSRC")
pbc1.d &lt;- impute(data = pbc)

## imputation using outcome splitting
f &lt;- as.formula(Surv(days, status) ~ .)
pbc2.d &lt;- impute(f, data = pbc, nsplit = 3)

## random splitting can be reasonably good
pbc3.d &lt;- impute(f, data = pbc, splitrule = "random", nimpute = 5)

## ------------------------------------------------------------
## example of regression imputation
## ------------------------------------------------------------

air1.d &lt;- impute(data = airquality, nimpute = 5)
air2.d &lt;- impute(Ozone ~ ., data = airquality, nimpute = 5)
air3.d &lt;- impute(Ozone ~ ., data = airquality, fast = TRUE)

## ------------------------------------------------------------
## multivariate missForest imputation
## ------------------------------------------------------------

data(pbc, package = "randomForestSRC")

## missForest algorithm - uses 1 variable at a time for the response
pbc.d &lt;- impute(data = pbc, mf.q = 1)

## multivariate missForest - use 10 percent of variables as responses
## i.e. multivariate missForest
pbc.d &lt;- impute(data = pbc, mf.q = .01)

## missForest but faster by using random splitting
pbc.d &lt;- impute(data = pbc, mf.q = 1, splitrule = "random")

## missForest but faster by increasing nodesize
pbc.d &lt;- impute(data = pbc, mf.q = 1, nodesize = 20, splitrule = "random")

## missForest but faster by using rfsrcFast
pbc.d &lt;- impute(data = pbc, mf.q = 1, fast = TRUE)

## ------------------------------------------------------------
## another example of multivariate missForest imputation
## (suggested by John Sheffield)
## ------------------------------------------------------------

test_rows &lt;- 1000

set.seed(1234)

a &lt;- rpois(test_rows, 500)
b &lt;- a + rnorm(test_rows, 50, 50)
c &lt;- b + rnorm(test_rows, 50, 50)
d &lt;- c + rnorm(test_rows, 50, 50)
e &lt;- d + rnorm(test_rows, 50, 50)
f &lt;- e + rnorm(test_rows, 50, 50)
g &lt;- f + rnorm(test_rows, 50, 50)
h &lt;- g + rnorm(test_rows, 50, 50)
i &lt;- h + rnorm(test_rows, 50, 50)

fake_data &lt;- data.frame(a, b, c, d, e, f, g, h, i)

fake_data_missing &lt;- data.frame(lapply(fake_data, function(x) {
  x[runif(test_rows) &lt;= 0.4] &lt;- NA
  x
}))
  
imputed_data &lt;- impute(
  data = fake_data_missing,
  mf.q = 0.2,
  ntree = 100,
  fast = TRUE,
  verbose = TRUE
)

par(mfrow=c(3,3))
o=lapply(1:ncol(imputed_data), function(j) {
  pt &lt;- is.na(fake_data_missing[, j])
  x &lt;- fake_data[pt, j]
  y &lt;- imputed_data[pt, j]
  plot(x, y, pch = 16, cex = 0.8, xlab = "raw data",
    ylab = "imputed data", col = 2)
  points(x, y, pch = 1, cex = 0.8, col = gray(.9))
  lines(supsmu(x, y, span = .25), lty = 1, col = 4, lwd = 4)
  mtext(colnames(imputed_data)[j])
  NULL
})


</code></pre>

<hr>
<h2 id='max.subtree.rfsrc'>Acquire Maximal Subtree Information</h2><span id='topic+max.subtree.rfsrc'></span><span id='topic+max.subtree'></span>

<h3>Description</h3>

<p>Extract maximal subtree information from a RF-SRC object.  Used for
variable selection and identifying interactions between variables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'rfsrc'
max.subtree(object,
  max.order = 2, sub.order = FALSE, conservative = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="max.subtree.rfsrc_+3A_object">object</code></td>
<td>
<p>An object of class <code>(rfsrc, grow)</code> or <code>(rfsrc,
	    forest).</code></p>
</td></tr> 
<tr><td><code id="max.subtree.rfsrc_+3A_max.order">max.order</code></td>
<td>
<p>Non-negative integer specifying the target number
of order depths.  Default is to return the first and second order
depths.  Used to identify predictive variables.  Setting
<span class="option">max.order=0</span> returns the first order depth for each
variable by tree.  A side effect is that <span class="option">conservative</span> is
automatically set to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="max.subtree.rfsrc_+3A_sub.order">sub.order</code></td>
<td>
<p>Set this value to <code>TRUE</code> to return the
minimal depth of each variable relative to another variable.
Used to identify interrelationship between variables.  See
details below.</p>
</td></tr>
<tr><td><code id="max.subtree.rfsrc_+3A_conservative">conservative</code></td>
<td>
<p>If <code>TRUE</code>, the threshold value for selecting
variables is calculated using a conservative marginal
approximation to the minimal depth distribution (the method used
in Ishwaran et al. 2010).  Otherwise, the minimal depth
distribution is the tree-averaged distribution.  The latter method
tends to give larger threshold values and discovers more
variables, especially in high-dimensions.</p>
</td></tr>
<tr><td><code id="max.subtree.rfsrc_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The maximal subtree for a variable <em>x</em> is the largest subtree
whose root node splits on <em>x</em>.  Thus, all parent nodes of
<em>x</em>'s maximal subtree have nodes that split on variables other
than <em>x</em>.  The largest maximal subtree possible is the root
node.  In general, however, there can be more than one maximal
subtree for a variable. A maximal subtree may also not exist if
there are no splits on the variable.  See Ishwaran et al. (2010,
2011) for details.
</p>
<p>The minimal depth of a maximal subtree (the first order depth)
measures predictiveness of a variable <em>x</em>.  It equals the
shortest distance (the depth) from the root node to the parent node
of the maximal subtree (zero is the smallest value possible).  The
smaller the minimal depth, the more impact <em>x</em> has on
prediction. The mean of the minimal depth distribution is used as
the threshold value for deciding whether a variable's minimal depth
value is small enough for the variable to be classified as strong.
</p>
<p>The second order depth is the distance from the root node to the
second closest maximal subtree of <em>x</em>.  To specify the target
order depth, use the <code>max.order</code> option (e.g., setting
<span class="option">max.order=2</span> returns the first and second order depths).
Setting <span class="option">max.order=0</span> returns the first order depth for each
variable for each tree.
</p>
<p>Set <span class="option">sub.order=TRUE</span> to obtain the minimal depth of a
variable relative to another variable.  This returns a
<code>p</code>x<code>p</code> matrix, where <code>p</code> is the number of variables,
and entries (i,j) are the normalized relative minimal depth of a
variable j within the maximal subtree for variable i, where
normalization adjusts for the size of i's maximal subtree.  Entry
(i,i) is the normalized minimal depth of i relative to the root
node.  The matrix should be read by looking across rows (not down
columns) and identifies interrelationship between variables.  Small
(i,j) entries indicate interactions.  See
<code>find.interaction</code> for related details.
</p>
<p>For competing risk data, maximal subtree analyses are unconditional
(i.e., they are non-event specific).
</p>


<h3>Value</h3>

<p>Invisibly, a list with the following components:
</p>
<table>
<tr><td><code>order</code></td>
<td>
<p>Order depths for a given variable up to <code>max.order</code>
averaged over a tree and the forest.  Matrix of dimension
<code>p</code>x<code>max.order</code>.  If <span class="option">max.order=0</span>, a matrix of
<code>p</code>x<code>ntree</code> is returned containing the first order depth
for each variable by tree.</p>
</td></tr>
<tr><td><code>count</code></td>
<td>
<p>Averaged number of  maximal subtrees, normalized by
the size of a tree, for each variable.</p>
</td></tr>
<tr><td><code>nodes.at.depth</code></td>
<td>
<p>Number of non-terminal nodes by depth for each tree.</p>
</td></tr>  
<tr><td><code>sub.order</code></td>
<td>
<p>Average minimal depth of a variable relative to another
variable.  Can be <code>NULL</code>.</p>
</td></tr>
<tr><td><code>threshold</code></td>
<td>
<p>Threshold value (the mean minimal depth) used to
select variables.</p>
</td></tr>
<tr><td><code>threshold.1se</code></td>
<td>
<p>Mean minimal depth plus one standard error.</p>
</td></tr>
<tr><td><code>topvars</code></td>
<td>
<p>Character vector of names of the final selected
variables.</p>
</td></tr>  
<tr><td><code>topvars.1se</code></td>
<td>
<p>Character vector of names of the final selected
variables using the 1se threshold rule.</p>
</td></tr>  
<tr><td><code>percentile</code></td>
<td>
<p>Minimal depth percentile for each variable.</p>
</td></tr>
<tr><td><code>density</code></td>
<td>
<p>Estimated minimal depth density.</p>
</td></tr>
<tr><td><code>second.order.threshold</code></td>
<td>
<p>Threshold for second order depth.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Hemant Ishwaran and Udaya B. Kogalur
</p>


<h3>References</h3>

<p>Ishwaran H., Kogalur U.B., Gorodeski E.Z, Minn A.J. and 
Lauer M.S. (2010).  High-dimensional variable selection for survival
data.  <em>J. Amer. Statist. Assoc.</em>, 105:205-217.
</p>
<p>Ishwaran H., Kogalur U.B., Chen X. and Minn A.J. (2011).  Random
survival forests for high-dimensional data. <em>Statist. Anal. Data
Mining</em>, 4:115-132.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+holdout.vimp.rfsrc">holdout.vimp.rfsrc</a></code>,
<code><a href="#topic+var.select.rfsrc">var.select.rfsrc</a></code>,
<code><a href="#topic+vimp.rfsrc">vimp.rfsrc</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## ------------------------------------------------------------
## survival analysis
## first and second order depths for all variables
## ------------------------------------------------------------

data(veteran, package = "randomForestSRC")
v.obj &lt;- rfsrc(Surv(time, status) ~ . , data = veteran)
v.max &lt;- max.subtree(v.obj)

# first and second order depths
print(round(v.max$order, 3))

# the minimal depth is the first order depth
print(round(v.max$order[, 1], 3))

# strong variables have minimal depth less than or equal
# to the following threshold
print(v.max$threshold)

# this corresponds to the set of variables
print(v.max$topvars)

## ------------------------------------------------------------
## regression analysis
## try different levels of conservativeness
## ------------------------------------------------------------

mtcars.obj &lt;- rfsrc(mpg ~ ., data = mtcars)
max.subtree(mtcars.obj)$topvars
max.subtree(mtcars.obj, conservative = TRUE)$topvars

</code></pre>

<hr>
<h2 id='nutrigenomic'>Nutrigenomic Study</h2><span id='topic+nutrigenomic'></span>

<h3>Description</h3>

<p>Study the effects of five diet treatments on 21 liver lipids and 120
hepatic gene expression in wild-type and PPAR-alpha deficient mice.  We
use a multivariate mixed random forest analysis by regressing gene
expression, diet and genotype (the x-variables) on lipid expressions
(the multivariate y-responses).
</p>


<h3>References</h3>

<p>Martin P.G. et al. (2007). Novel aspects of PPAR-alpha-mediated
regulation of lipid and xenobiotic metabolism revealed through a
nutrigenomic study. <em>Hepatology</em>, 45(3), 767&ndash;777.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## ------------------------------------------------------------
## multivariate regression forests using Mahalanobis splitting
## lipids (all real values) used as the multivariate y
## ------------------------------------------------------------

## load the data
data(nutrigenomic, package = "randomForestSRC")

## parse into y and x data
ydta &lt;- nutrigenomic$lipids
xdta &lt;- data.frame(nutrigenomic$genes,
                   diet = nutrigenomic$diet,
                   genotype = nutrigenomic$genotype)

## multivariate mixed forest call
obj &lt;- rfsrc(get.mv.formula(colnames(ydta)),
             data.frame(ydta, xdta),
             importance=TRUE, nsplit = 10,
             splitrule = "mahalanobis")
print(obj)

## ------------------------------------------------------------
## plot the standarized performance and VIMP values
## ------------------------------------------------------------

## acquire the error rate for each of the 21-coordinates 
## standardize to allow for comparison across coordinates
serr &lt;- get.mv.error(obj, standardize = TRUE)

## acquire standardized VIMP 
svimp &lt;- get.mv.vimp(obj, standardize = TRUE)

par(mfrow = c(1,2))
plot(serr, xlab = "Lipids", ylab = "Standardized Performance")
matplot(svimp, xlab = "Genes/Diet/Genotype", ylab = "Standardized VIMP")


## ------------------------------------------------------------
## plot some trees
## ------------------------------------------------------------

plot(get.tree(obj, 1))
plot(get.tree(obj, 2))
plot(get.tree(obj, 3))


## ------------------------------------------------------------
##
## Compare above to (1) user specified covariance matrix
##                  (2) default composite (independent) splitting
##
## ------------------------------------------------------------

## user specified sigma matrix
obj2 &lt;- rfsrc(get.mv.formula(colnames(ydta)),
              data.frame(ydta, xdta),
              importance = TRUE, nsplit = 10,
              splitrule = "mahalanobis",
              sigma = cov(ydta))
print(obj2)

## default independence split rule
obj3 &lt;- rfsrc(get.mv.formula(colnames(ydta)),
              data.frame(ydta, xdta),
              importance=TRUE, nsplit = 10)
print(obj3)

## compare vimp
imp &lt;- data.frame(mahalanobis  = rowMeans(get.mv.vimp(obj,  standardize = TRUE)),
                  mahalanobis2 = rowMeans(get.mv.vimp(obj2, standardize = TRUE)),
                  default      = rowMeans(get.mv.vimp(obj3, standardize = TRUE)))

print(head(100 * imp[order(imp$mahalanobis, decreasing = TRUE), ], 15))

</code></pre>

<hr>
<h2 id='partial.rfsrc'>Acquire Partial Effect of a Variable</h2><span id='topic+partial.rfsrc'></span><span id='topic+partial'></span><span id='topic+get.partial.plot.data'></span>

<h3>Description</h3>

<p>Direct, fast inferface for partial effect of a variable.  Works for all families.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>partial.rfsrc(object, oob = TRUE, 
  partial.type = NULL, partial.xvar = NULL, partial.values = NULL,
  partial.xvar2 = NULL, partial.values2 = NULL,
  partial.time = NULL, get.tree = NULL, seed = NULL, do.trace = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="partial.rfsrc_+3A_object">object</code></td>
<td>
<p>An object of class <code>(rfsrc, grow)</code>.</p>
</td></tr>
<tr><td><code id="partial.rfsrc_+3A_oob">oob</code></td>
<td>
<p>By default out-of-bag values are returned, but inbag
values can be requested by setting this option to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="partial.rfsrc_+3A_partial.type">partial.type</code></td>
<td>
<p>Character vector specifying type of predicted
value requested.  See details below.</p>
</td></tr>
<tr><td><code id="partial.rfsrc_+3A_partial.xvar">partial.xvar</code></td>
<td>
<p>Character value specifying the single primary
partial x-variable to be used.</p>
</td></tr>
<tr><td><code id="partial.rfsrc_+3A_partial.values">partial.values</code></td>
<td>
<p>Vector of values that the primary partialy
x-variable will assume.</p>
</td></tr>
<tr><td><code id="partial.rfsrc_+3A_partial.xvar2">partial.xvar2</code></td>
<td>
<p>Vector of character values specifying the second order x-variables to be
used.</p>
</td></tr>
<tr><td><code id="partial.rfsrc_+3A_partial.values2">partial.values2</code></td>
<td>
<p>Vector of values that the second order
x-variables will assume.  Each second order x-variable can only
assume a single value.  This the length of <code>partial.xvar2</code> and
<code>partial.values2</code> will be the same.  In addition, the user must
do the appropriate conversion for factors, and represent a value as
a numeric element.</p>
</td></tr>
<tr><td><code id="partial.rfsrc_+3A_partial.time">partial.time</code></td>
<td>
<p>For survival families, the time at which the predicted
survival value is evaluated at (depends on <code>partial.type</code>).</p>
</td></tr>
<tr><td><code id="partial.rfsrc_+3A_get.tree">get.tree</code></td>
<td>
<p>Vector of integer(s) identifying trees over which the
partial values are calculated over.  By default, uses all trees in the
forest.</p>
</td></tr>
<tr><td><code id="partial.rfsrc_+3A_seed">seed</code></td>
<td>
<p>Negative integer specifying seed for the random number
generator.</p>
</td></tr>
<tr><td><code id="partial.rfsrc_+3A_do.trace">do.trace</code></td>
<td>
<p>Number of seconds between updates to the user on
approximate time to completion.</p>
</td></tr>
<tr><td><code id="partial.rfsrc_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Used for direct, efficient call to obtain partial plot effects.
This function is intended primarily for experts. 
</p>
<p>Out-of-bag (OOB) values are returned by default.
</p>
<p>For factors, the partial value should be encoded as a positive integer
reflecting the level number of the factor.  The actual label of the
factor should not be used.
</p>
<p>The utility function <code>get.partial.plot.data</code> is supplied for
processing returned raw partial effects in a format more convenient
for plotting.  Options are specified as in <code>plot.variable</code>.  See
examples for illustration.
</p>
<p>Raw partial plot effects data is returned either as an array or a list
of length equal to the number of outcomes (length is one for
univariate families) with entries depending on the underlying family:
</p>

<ol>
<li><p> For regression, partial plot data is returned as a list
in <code>regrOutput</code> with dim <code>[n] x  [length(partial.values)]</code>.
</p>
</li>
<li><p> For classification, partial plot data is returned as a list
in <code>classOutput</code> of dim <code>[n] x [1 + yvar.nlevels[.]] x
      [length(partial.values)]</code>.
</p>
</li>
<li><p> For mixed multivariate regression, values are returned in 
list format both in <code>regrOutput</code> and <code>classOutput</code>
</p>
</li>
<li><p> For survival, values are returned as either a matrix or array
in <code>survOutput</code>. Depending on partial type specified this can be:
</p>

<ul>
<li><p> For partial type <code>surv</code> returns the survival function
of dim <code>[n] x [length(partial.time)] x [length(partial.values)]</code>.
</p>
</li>
<li><p> For partial type <code>mort</code> returns mortality
of dim <code>[n] x [length(partial.values)]</code>.
</p>
</li>
<li><p> For partial type <code>chf</code> returns the cumulative hazard function
of dim <code>[n] x [length(partial.time)] x [length(partial.values)]</code>.
</p>
</li></ul>

</li>
<li><p> For competing risks, values are returned as either a matrix or
array in <code>survOutput</code>.  Depending on the options specified this
can be:
</p>

<ul>
<li><p> For partial type <code>years.lost</code> returns the expected number of life years lost 
of dim <code>[n] x [length(event.info$event.type)] x
        [length(partial.values)]</code>.
</p>
</li>
<li><p> For partial type <code>cif</code> returns the cumulative incidence function of dim
<code>[n] x [length(partial.time)] x
	      [length(event.info$event.type)] x
	      [length(partial.values)]</code>.
</p>
</li>
<li><p> For partial type <code>chf</code> returns the cumulative hazard function of dim
<code>[n] x [length(partial.time)] x [length(event.info$event.type)]
            x [length(partial.values)]</code>.
</p>
</li></ul>

</li></ol>



<h3>Author(s)</h3>

<p>Hemant Ishwaran and Udaya B. Kogalur
</p>


<h3>References</h3>

<p>Ishwaran H., Kogalur U.B. (2007).  Random survival forests for R,
<em>Rnews</em>, 7(2):25-31.
</p>
<p>Ishwaran H., Kogalur U.B., Blackstone E.H. and Lauer M.S.
(2008).  Random survival forests, <em>Ann. App.
Statist.</em>, 2:841-860.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plot.variable.rfsrc">plot.variable.rfsrc</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

## ------------------------------------------------------------
##
## regression
##
## ------------------------------------------------------------

airq.obj &lt;- rfsrc(Ozone ~ ., data = airquality)

## partial effect for wind
partial.obj &lt;- partial(airq.obj,
                  partial.xvar = "Wind",
                  partial.values = airq.obj$xvar$Wind)
pdta &lt;- get.partial.plot.data(partial.obj)

## plot partial values
plot(pdta$x, pdta$yhat, type = "b", pch = 16,
      xlab = "wind", ylab = "partial effect of wind")

## ------------------------------------------------------------
##
## regression: partial effects for two variables simultaneously
##
## ------------------------------------------------------------

airq.obj &lt;- rfsrc(Ozone ~ ., data = airquality)

## specify wind and temperature values of interest
wind &lt;- sort(unique(airq.obj$xvar$Wind))
temp &lt;- sort(unique(airq.obj$xvar$Temp))

## partial effect for wind, for a given temp
pdta &lt;- do.call(rbind, lapply(temp, function(x2) {
  o &lt;- partial(airq.obj,
         partial.xvar = "Wind", partial.xvar2 = "Temp",
         partial.values = wind, partial.values2 = x2)
  cbind(wind, x2, get.partial.plot.data(o)$yhat)
}))
pdta &lt;- data.frame(pdta)
colnames(pdta) &lt;- c("wind", "temp", "effectSize")

## coplot of partial effect of wind and temp 
coplot(effectSize ~ wind|temp, pdta, pch = 16, overlap = 0)


## ------------------------------------------------------------
##
## regression: partial effects for three variables simultaneously
## (can be slow, so modify accordingly)
##
## ------------------------------------------------------------

n &lt;- 1000
x &lt;- matrix(rnorm(n * 3), ncol = 3)
y &lt;- x[, 1] + x[, 1] * x[, 2] + x[, 1] * x[, 2] * x[, 3]
o &lt;- rfsrc(y ~ ., data = data.frame(y = y, x))

## define target x values
x1 &lt;- seq(-3, 3, length = 40)
x2 &lt;- x3 &lt;- seq(-3, 3, length = 10)

## extract second order partial effects
pdta &lt;- do.call(rbind,
          lapply(x3, function(x3v) {
            cat("outer loop x3 = ", x3v, "\n")
            do.call(rbind,lapply(x2, function(x2v) {
              o &lt;- partial(o,
                      partial.xvar = "X1",
                      partial.values = x1,
                      partial.xvar2 = c("X2", "X3"),
                      partial.values2 = c(x2v, x3v))
              cbind(x1, x2v, x3v, get.partial.plot.data(o)$yhat)
            }))
          }))
pdta &lt;- data.frame(pdta)
colnames(pdta) &lt;- c("x1", "x2", "x3", "effectSize")

## coplot of partial effects
coplot(effectSize ~ x1|x2*x3, pdta, pch = 16, overlap = 0)


## ------------------------------------------------------------
##
## classification
##
## ------------------------------------------------------------

iris.obj &lt;- rfsrc(Species ~., data = iris)

## partial effect for sepal length
partial.obj &lt;- partial(iris.obj,
                  partial.xvar = "Sepal.Length",
                  partial.values = iris.obj$xvar$Sepal.Length)

## extract partial effects for each species outcome
pdta1 &lt;- get.partial.plot.data(partial.obj, target = "setosa")
pdta2 &lt;- get.partial.plot.data(partial.obj, target = "versicolor")
pdta3 &lt;- get.partial.plot.data(partial.obj, target = "virginica")

## plot the results
par(mfrow=c(1,1))
plot(pdta1$x, pdta1$yhat, type="b", pch = 16,
     xlab = "sepal length", ylab = "adjusted probability", 
     ylim = range(pdta1$yhat,pdta2$yhat,pdta3$yhat))
points(pdta2$x, pdta2$yhat, col = 2, type = "b", pch = 16)
points(pdta3$x, pdta3$yhat, col = 4, type = "b", pch = 16)
legend("topleft", legend=levels(iris.obj$yvar), fill = c(1, 2, 4))

## ------------------------------------------------------------
##
## survival
##
## ------------------------------------------------------------

data(veteran, package = "randomForestSRC")
v.obj &lt;- rfsrc(Surv(time,status)~., veteran, nsplit = 10, ntree = 100)

## partial effect of age on mortality
partial.obj &lt;- partial(v.obj,
  partial.type = "mort",
  partial.xvar = "age",
  partial.values = v.obj$xvar$age,
  partial.time = v.obj$time.interest)
pdta &lt;- get.partial.plot.data(partial.obj)

plot(lowess(pdta$x, pdta$yhat, f = 1/3),
   type = "l", xlab = "age", ylab = "adjusted mortality")

## partial effects of karnofsky score on survival
karno &lt;- quantile(v.obj$xvar$karno)
partial.obj &lt;- partial(v.obj,
  partial.type = "surv",
  partial.xvar = "karno",
  partial.values = karno,
  partial.time = v.obj$time.interest)
pdta &lt;- get.partial.plot.data(partial.obj)

matplot(pdta$partial.time, t(pdta$yhat), type = "l", lty = 1,
     xlab = "time", ylab = "karnofsky adjusted survival")
legend("topright", legend = paste0("karnofsky = ", karno), fill = 1:5)


## ------------------------------------------------------------
##
## competing risk
##
## ------------------------------------------------------------

data(follic, package = "randomForestSRC")
follic.obj &lt;- rfsrc(Surv(time, status) ~ ., follic, nsplit = 3, ntree = 100)

## partial effect of age on years lost
partial.obj &lt;- partial(follic.obj,
  partial.type = "years.lost",
  partial.xvar = "age",
  partial.values = follic.obj$xvar$age,
  partial.time = follic.obj$time.interest)
pdta1 &lt;- get.partial.plot.data(partial.obj, target = 1)
pdta2 &lt;- get.partial.plot.data(partial.obj, target = 2)

par(mfrow=c(2,2))
plot(lowess(pdta1$x, pdta1$yhat),
   type = "l", xlab = "age", ylab = "adjusted years lost relapse")
plot(lowess(pdta2$x, pdta2$yhat),
   type = "l", xlab = "age", ylab = "adjusted years lost death")


## partial effect of age on cif
partial.obj &lt;- partial(follic.obj,
  partial.type = "cif",
  partial.xvar = "age",
  partial.values = quantile(follic.obj$xvar$age),
  partial.time = follic.obj$time.interest)
pdta1 &lt;- get.partial.plot.data(partial.obj, target = 1)
pdta2 &lt;- get.partial.plot.data(partial.obj, target = 2)

matplot(pdta1$partial.time, t(pdta1$yhat), type = "l", lty = 1,
     xlab = "time", ylab = "age adjusted cif for relapse")
matplot(pdta2$partial.time, t(pdta2$yhat), type = "l", lty = 1,
     xlab = "time", ylab = "age adjusted cif for death")


## ------------------------------------------------------------
##
## multivariate mixed outcomes
##
## ------------------------------------------------------------

mtcars2 &lt;- mtcars
mtcars2$carb &lt;- factor(mtcars2$carb)
mtcars2$cyl &lt;- factor(mtcars2$cyl)
mtcars.mix &lt;- rfsrc(Multivar(carb, mpg, cyl) ~ ., data = mtcars2)

## partial effect of displacement for each the three-outcomes
partial.obj &lt;- partial(mtcars.mix,
                  partial.xvar = "disp",
                  partial.values = mtcars.mix$xvar$disp)
pdta1 &lt;- get.partial.plot.data(partial.obj, m.target = "carb")
pdta2 &lt;- get.partial.plot.data(partial.obj, m.target = "mpg")
pdta3 &lt;- get.partial.plot.data(partial.obj, m.target = "cyl")

par(mfrow=c(2,2))
plot(lowess(pdta1$x, pdta1$yhat), type = "l", xlab="displacement", ylab="carb")
plot(lowess(pdta2$x, pdta2$yhat), type = "l", xlab="displacement", ylab="mpg")
plot(lowess(pdta3$x, pdta3$yhat), type = "l", xlab="displacement", ylab="cyl")



</code></pre>

<hr>
<h2 id='pbc'>Primary Biliary Cirrhosis (PBC) Data</h2><span id='topic+pbc'></span>

<h3>Description</h3>

<p>Data from the Mayo Clinic trial in primary biliary cirrhosis (PBC) of
the liver conducted between 1974 and 1984.  A total of 424 PBC
patients, referred to Mayo Clinic during that ten-year interval, met
eligibility criteria for the randomized placebo controlled trial of
the drug D-penicillamine.  The first 312 cases in the data set
participated in the randomized trial and contain largely complete
data.  
</p>


<h3>Source</h3>

<p>Flemming and Harrington, 1991, Appendix D.1.</p>


<h3>References</h3>

<p>Flemming T.R and Harrington D.P., (1991) <em>Counting Processes
and Survival Analysis.</em> New York: Wiley.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(pbc, package = "randomForestSRC")
pbc.obj &lt;- rfsrc(Surv(days, status) ~ ., pbc, nsplit = 3)

</code></pre>

<hr>
<h2 id='peakVO2'>Systolic Heart Failure Data</h2><span id='topic+peakVO2'></span>

<h3>Description</h3>

<p>The data involve 2231 patients with systolic heart failure who
underwent cardiopulmonary stress testing at the Cleveland Clinic.  The
primary end point was all-cause death.  In total, 39 variables were
measured for each patient, including baseline clinical values and
exercise stress test results.  A key variable of interest is
peak VO2 (mL/kg per min), the peak respiratory exchange ratio.
More details regarding the data can be found in Hsich et al. (2011).
</p>


<h3>References</h3>

<p>Hsich E., Gorodeski E.Z.,Blackstone E.H., Ishwaran H. and Lauer
M.S. (2011). Identifying important risk factors for survival in
systolic heart failure patients using random survival
forests. Circulation: Cardio. Qual. Outcomes, 4(1), 39-45.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## load the data
data(peakVO2, package = "randomForestSRC")

## random survival forest analysis
o &lt;- rfsrc(Surv(ttodead, died)~., peakVO2)
print(o)

## partial effect of peak V02 on mortality
partial.o &lt;- partial(o,
       partial.type = "mort",
       partial.xvar = "peak.vo2",
       partial.values = o$xvar$peak.vo2,
       partial.time = o$time.interest)
pdta.m &lt;- get.partial.plot.data(partial.o)


## partial effect of peak V02 on survival
pvo2 &lt;- quantile(o$xvar$peak.vo2)
partial.o &lt;- partial(o,
       partial.type = "surv",
       partial.xvar = "peak.vo2",
       partial.values = pvo2,
       partial.time = o$time.interest)
pdta.s &lt;- get.partial.plot.data(partial.o)
     

## compare the two plots
par(mfrow=c(1,2))    

plot(lowess(pdta.m$x, pdta.m$yhat, f = 2/3),
     type = "l", xlab = "peak VO2", ylab = "adjusted mortality")
rug(o$xvar$peak.vo2)

matplot(pdta.s$partial.time, t(pdta.s$yhat), type = "l", lty = 1,
          xlab = "years", ylab = "peak VO2 adjusted survival")
legend("bottomleft", legend = paste0("peak VO2 = ", pvo2),
       bty = "n", cex = .75, fill = 1:5)


</code></pre>

<hr>
<h2 id='plot.competing.risk.rfsrc'>Plots for Competing Risks</h2><span id='topic+plot.competing.risk.rfsrc'></span><span id='topic+plot.competing.risk'></span>

<h3>Description</h3>

<p>Plot useful summary curves from a random survival forest competing risk
analysis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'rfsrc'
plot.competing.risk(x, plots.one.page = FALSE, ...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.competing.risk.rfsrc_+3A_x">x</code></td>
<td>
<p>An object of class <code>(rfsrc, grow)</code> or
<code>(rfsrc, predict)</code>.</p>
</td></tr>
<tr><td><code id="plot.competing.risk.rfsrc_+3A_plots.one.page">plots.one.page</code></td>
<td>
<p>Should plots be placed on one page?</p>
</td></tr>
<tr><td><code id="plot.competing.risk.rfsrc_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Given a random survival forest object from a competing risk analysis
(Ishwaran et al. 2014), plots from top to bottom, left to right: (1)
cause-specific cumulative hazard function (CSCHF) for each event, (2)
cumulative incidence function (CIF) for each event, and (3) continuous
probability curves (CPC) for each event (Pepe and Mori, 1993).
</p>
<p>Does not apply to right-censored data.  Whenever possible, out-of-bag
(OOB) values are displayed.
</p>


<h3>Author(s)</h3>

<p>Hemant Ishwaran and Udaya B. Kogalur
</p>


<h3>References</h3>

<p>Ishwaran H., Gerds T.A., Kogalur U.B., Moore R.D., Gange S.J. and Lau
B.M. (2014). Random survival forests for competing risks.
<em>Biostatistics</em>, 15(4):757-773.
</p>
<p>Pepe, M.S. and Mori, M., (1993). Kaplan-Meier, marginal or conditional
probability curves in summarizing competing risks failure time
data? <em>Statistics in Medicine</em>, 12(8):737-751.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+follic">follic</a></code>,
<code><a href="#topic+hd">hd</a></code>,
<code><a href="#topic+rfsrc">rfsrc</a></code>,
<code><a href="#topic+wihs">wihs</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## ------------------------------------------------------------
## follicular cell lymphoma
## ------------------------------------------------------------

  data(follic, package = "randomForestSRC")
  follic.obj &lt;- rfsrc(Surv(time, status) ~ ., follic, nsplit = 3, ntree = 100)
  print(follic.obj)
  plot.competing.risk(follic.obj)

## ------------------------------------------------------------
## Hodgkin's Disease
## ------------------------------------------------------------

  data(hd, package = "randomForestSRC")
  hd.obj &lt;- rfsrc(Surv(time, status) ~ ., hd, nsplit = 3, ntree = 100)
  print(hd.obj)
  plot.competing.risk(hd.obj)

## ------------------------------------------------------------
## competing risk analysis of pbc data from the survival package
## events are transplant (1) and death (2)
## ------------------------------------------------------------

if (library("survival", logical.return = TRUE)) {
   data(pbc, package = "survival")
   pbc$id &lt;- NULL
   plot.competing.risk(rfsrc(Surv(time, status) ~ ., pbc))
}

</code></pre>

<hr>
<h2 id='plot.quantreg.rfsrc'>Plot Quantiles from Quantile Regression Forests</h2><span id='topic+plot.quantreg.rfsrc'></span><span id='topic+plot.quantreg'></span>

<h3>Description</h3>

<p>Plots quantiles obtained from a quantile regression forest.
Additionally insets the continuous rank probability score (crps), a
useful diagnostic of accuracy.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'rfsrc'
plot.quantreg(x, prbL = .25, prbU = .75,
   m.target = NULL, crps = TRUE, subset = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.quantreg.rfsrc_+3A_x">x</code></td>
<td>
<p>A quantile regression object obtained from calling <code>quantreg</code>.</p>
</td></tr>
<tr><td><code id="plot.quantreg.rfsrc_+3A_prbl">prbL</code></td>
<td>
<p>Lower quantile (preferably &lt; .5).</p>
</td></tr>
<tr><td><code id="plot.quantreg.rfsrc_+3A_prbu">prbU</code></td>
<td>
<p>Upper quantile (preferably &gt; .5).</p>
</td></tr>
<tr><td><code id="plot.quantreg.rfsrc_+3A_m.target">m.target</code></td>
<td>
<p>Character value for multivariate families
specifying the target outcome to be used.  If left unspecified, the
algorithm will choose a default target.</p>
</td></tr>
<tr><td><code id="plot.quantreg.rfsrc_+3A_crps">crps</code></td>
<td>
<p>Calculate crps and inset it?</p>
</td></tr>
<tr><td><code id="plot.quantreg.rfsrc_+3A_subset">subset</code></td>
<td>
<p>Restricts plotted values to a subset of the data.
Default is to use the entire data.</p>
</td></tr>
<tr><td><code id="plot.quantreg.rfsrc_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Hemant Ishwaran and Udaya B. Kogalur
</p>


<h3>See Also</h3>

<p><code><a href="#topic+quantreg.rfsrc">quantreg.rfsrc</a></code>
</p>

<hr>
<h2 id='plot.rfsrc'>Plot Error Rate and Variable Importance from a RF-SRC analysis</h2><span id='topic+plot.rfsrc'></span>

<h3>Description</h3>

<p>Plot out-of-bag (OOB) error rates and variable importance (VIMP)
from a RF-SRC analysis. This is the default plot method for the package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'rfsrc'
plot(x, m.target = NULL,
  plots.one.page = TRUE, sorted = TRUE, verbose = TRUE,  ...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.rfsrc_+3A_x">x</code></td>
<td>
<p>An object of class <code>(rfsrc, grow)</code>, <code>(rfsrc, synthetic)</code>,
or <code>(rfsrc, predict)</code>.</p>
</td></tr>
<tr><td><code id="plot.rfsrc_+3A_m.target">m.target</code></td>
<td>
<p>Character value for multivariate families
specifying the target outcome to be used.  If left unspecified, the
algorithm will choose a default target.</p>
</td></tr>
<tr><td><code id="plot.rfsrc_+3A_plots.one.page">plots.one.page</code></td>
<td>
<p>Should plots be placed on one page?</p>
</td></tr>
<tr><td><code id="plot.rfsrc_+3A_sorted">sorted</code></td>
<td>
<p>Should variables be sorted by importance values?</p>
</td></tr>
<tr><td><code id="plot.rfsrc_+3A_verbose">verbose</code></td>
<td>
<p>Should VIMP be printed?</p>
</td></tr>
<tr><td><code id="plot.rfsrc_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

  
<p>Plot cumulative OOB error rates as a function of number of trees and
variable importance (VIMP) if available.  Note that the default
settings are now such that the error rate is no longer calculated on
every tree and VIMP is only calculated if requested.  To get OOB error
rates for ever tree, use the option <code>block.size = 1</code> when
growing or restoring the forest.  Likewise, to view VIMP, use the option
<code>importance</code> when growing or restoring the forest.
</p>


<h3>Author(s)</h3>

<p>Hemant Ishwaran and Udaya B. Kogalur
</p>


<h3>References</h3>

<p>Breiman L. (2001). Random forests, <em>Machine Learning</em>, 45:5-32.
</p>
<p>Ishwaran H. and Kogalur U.B. (2007).  Random survival forests for R,
<em>Rnews</em>, 7(2):25-31.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## ------------------------------------------------------------
## classification example
## ------------------------------------------------------------

iris.obj &lt;- rfsrc(Species ~ ., data = iris,
     block.size = 1, importance = TRUE)
plot(iris.obj)

## ------------------------------------------------------------
## competing risk example
## ------------------------------------------------------------

## use the pbc data from the survival package
## events are transplant (1) and death (2)
if (library("survival", logical.return = TRUE)) {
  data(pbc, package = "survival")
  pbc$id &lt;- NULL
  plot(rfsrc(Surv(time, status) ~ ., pbc, block.size = 1))
}

## ------------------------------------------------------------
## multivariate mixed forests
## ------------------------------------------------------------

mtcars.new &lt;- mtcars
mtcars.new$cyl &lt;- factor(mtcars.new$cyl)
mtcars.new$carb &lt;- factor(mtcars.new$carb, ordered = TRUE)
mv.obj &lt;- rfsrc(cbind(carb, mpg, cyl) ~., data = mtcars.new, block.size = 1)
plot(mv.obj, m.target = "carb")
plot(mv.obj, m.target = "mpg")
plot(mv.obj, m.target = "cyl")


</code></pre>

<hr>
<h2 id='plot.subsample.rfsrc'>Plot Subsampled VIMP Confidence Intervals</h2><span id='topic+plot.subsample.rfsrc'></span><span id='topic+plot.subsample'></span>

<h3>Description</h3>

<p>Plots VIMP (variable importance) confidence regions obtained from
subsampling a forest.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'rfsrc'
plot.subsample(x, alpha = .01, xvar.names,
 standardize = TRUE, normal = TRUE, jknife = FALSE,
 target, m.target = NULL, pmax = 75, main = "", sorted = TRUE, ...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.subsample.rfsrc_+3A_x">x</code></td>
<td>
<p>An object obtained from calling <code>subample</code>.</p>
</td></tr>
<tr><td><code id="plot.subsample.rfsrc_+3A_alpha">alpha</code></td>
<td>
<p>Desired level of significance.</p>
</td></tr>
<tr><td><code id="plot.subsample.rfsrc_+3A_xvar.names">xvar.names</code></td>
<td>
<p>Names of the x-variables to be used.  If not
specified all variables used.</p>
</td></tr>
<tr><td><code id="plot.subsample.rfsrc_+3A_standardize">standardize</code></td>
<td>
<p>Standardize VIMP?  For regression families, VIMP is
standardized by dividing by the variance and then multipled by 100.
For all other families, VIMP is scaled by 100.</p>
</td></tr>
<tr><td><code id="plot.subsample.rfsrc_+3A_normal">normal</code></td>
<td>
<p>Use parametric normal confidence regions or
nonparametric regions?  Generally, parametric regions perform better.</p>
</td></tr>
<tr><td><code id="plot.subsample.rfsrc_+3A_jknife">jknife</code></td>
<td>
<p>Use the delete-d jackknife variance estimator?</p>
</td></tr>
<tr><td><code id="plot.subsample.rfsrc_+3A_target">target</code></td>
<td>
<p>For classification families, an integer or character
value specifying the class VIMP will be conditioned on (default is
to use unconditional VIMP).  For competing risk families, an integer
value between 1 and <code>J</code> indicating the event VIMP is requested,
where <code>J</code> is the number of event types.  The default is to use
the first event.</p>
</td></tr>
<tr><td><code id="plot.subsample.rfsrc_+3A_m.target">m.target</code></td>
<td>
<p>Character value for multivariate families
specifying the target outcome to be used.  If left unspecified, the
algorithm will choose a default target.</p>
</td></tr>
<tr><td><code id="plot.subsample.rfsrc_+3A_pmax">pmax</code></td>
<td>
<p>Trims the data to this number of variables (sorted by VIMP).</p>
</td></tr>
<tr><td><code id="plot.subsample.rfsrc_+3A_main">main</code></td>
<td>
<p>Title used for plot.</p>
</td></tr>
<tr><td><code id="plot.subsample.rfsrc_+3A_sorted">sorted</code></td>
<td>
<p>Should variables be sorted by importance values?</p>
</td></tr>
<tr><td><code id="plot.subsample.rfsrc_+3A_...">...</code></td>
<td>
<p>Further arguments that can be passed to <code>bxp</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Most of the options used by the R function bxp will work here and can
be used for customization of plots.  Currently the following
parameters will work:
</p>
<p>&quot;xaxt&quot;, &quot;yaxt&quot;, &quot;las&quot;, &quot;cex.axis&quot;, 
&quot;col.axis&quot;, &quot;cex.main&quot;,
&quot;col.main&quot;, &quot;sub&quot;, &quot;cex.sub&quot;, &quot;col.sub&quot;, 
&quot;ylab&quot;, &quot;cex.lab&quot;, &quot;col.lab&quot;
</p>


<h3>Value</h3>

<p>Invisibly, returns the boxplot data that is plotted.
</p>


<h3>Author(s)</h3>

<p>Hemant Ishwaran and Udaya B. Kogalur
</p>


<h3>References</h3>

<p>Ishwaran H. and Lu M.  (2017).  Standard errors and confidence
intervals for variable importance in random forest regression,
classification, and survival.
</p>
<p>Politis, D.N. and Romano, J.P. (1994). Large sample confidence
regions based on subsamples under minimal assumptions. <em>The
Annals of Statistics</em>, 22(4):2031-2050.
</p>
<p>Shao, J. and Wu, C.J. (1989). A general theory for jackknife variance
estimation. <em>The Annals of Statistics</em>, 17(3):1176-1197.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+subsample.rfsrc">subsample.rfsrc</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
o &lt;- rfsrc(Ozone ~ ., airquality)
oo &lt;- subsample(o)
plot.subsample(oo)
plot.subsample(oo, xvar.names = o$xvar.names[1:3])
plot.subsample(oo, jknife = FALSE)
plot.subsample(oo, alpha = .01)
plot(oo,cex.axis=.5)

</code></pre>

<hr>
<h2 id='plot.survival.rfsrc'>Plot of Survival Estimates</h2><span id='topic+plot.survival.rfsrc'></span><span id='topic+plot.survival'></span><span id='topic+get.brier.survival'></span>

<h3>Description</h3>

<p>Plot various survival estimates.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'rfsrc'
plot.survival(x, show.plots = TRUE, subset,
  collapse = FALSE, cens.model = c("km", "rfsrc"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.survival.rfsrc_+3A_x">x</code></td>
<td>
<p>An object of class <code>(rfsrc, grow)</code> or
<code>(rfsrc, predict)</code>.</p>
</td></tr>
<tr><td><code id="plot.survival.rfsrc_+3A_show.plots">show.plots</code></td>
<td>
<p>Should plots be displayed?</p>
</td></tr>
<tr><td><code id="plot.survival.rfsrc_+3A_subset">subset</code></td>
<td>
<p>Vector indicating which cases from <code>x</code> we want
estimates for.  All cases used if not specified.</p>
</td></tr>
<tr><td><code id="plot.survival.rfsrc_+3A_collapse">collapse</code></td>
<td>
<p>Collapse the survival function?</p>
</td></tr>
<tr><td><code id="plot.survival.rfsrc_+3A_cens.model">cens.model</code></td>
<td>
<p>Using the training data, specifies method for
estimating the censoring distribution used in the inverse
probability of censoring weights (IPCW) for calculating the Brier
score:
</p>

<dl>
<dt><code>km</code>:</dt><dd><p>Uses the Kaplan-Meier estimator.</p>
</dd>
<dt><code>rfscr</code>:</dt><dd><p>Uses a censoring random survival forest estimator.</p>
</dd>
</dl>

</td></tr>
<tr><td><code id="plot.survival.rfsrc_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

  
<p>Produces the following plots (going from top to bottom, left to right):
</p>

<ol>
<li><p> Forest estimated survival function for each individual (thick
red line is overall ensemble survival, thick green line is
Nelson-Aalen estimator).
</p>
</li>
<li><p> Brier score (0=perfect, 1=poor, and 0.25=guessing) stratified
by ensemble mortality.  Based on the IPCW method described in Gerds
et al. (2006).  Stratification is into 4 groups corresponding to the
0-25, 25-50, 50-75 and 75-100 percentile values of mortality.  Red
line is overall (non-stratified) Brier score.
</p>
</li>
<li><p> Continuous rank probability score (CRPS) equal to the
integrated Brier score divided by time.
</p>
</li>
<li><p> Plot of mortality of each individual versus observed time.
Points in blue correspond to events, black points are censored
observations.  Not given for prediction settings lacking
survival response information.
</p>
</li></ol>

<p>Whenever possible, out-of-bag (OOB) values are used.
</p>
<p>Only applies to survival families.  In particular, fails for competing
risk analyses.  Use <code>plot.competing.risk</code> in such cases.
</p>
<p>Mortality (Ishwaran et al., 2008) represents estimated risk for an
individual calibrated to the scale of number of events (as a specific
example, if <em>i</em> has a mortality value of 100, then if all
individuals had the same x-values as <em>i</em>, we would expect an
average of 100 events).
</p>
<p>The utility function <code>get.brier.survival</code> can be used to extract
the Brier score among other useful quantities.
</p>


<h3>Value</h3>

<p>Invisibly, the conditional and unconditional Brier scores, and the
integrated Brier score.
</p>


<h3>Author(s)</h3>

<p>Hemant Ishwaran and Udaya B. Kogalur
</p>


<h3>References</h3>

<p>Gerds T.A and Schumacher M. (2006).  Consistent estimation of the
expected Brier score in general survival models with right-censored
event times, <em>Biometrical J.</em>, 6:1029-1040.
</p>
<p>Graf E., Schmoor C., Sauerbrei W. and Schumacher M. (1999).
Assessment and comparison of prognostic classification
schemes for survival data, <em>Statist. in Medicine</em>,
18:2529-2545.
</p>
<p>Ishwaran H. and Kogalur U.B. (2007).  Random survival forests for R,
<em>Rnews</em>, 7(2):25-31.
</p>
<p>Ishwaran H., Kogalur U.B., Blackstone E.H. and Lauer M.S.
(2008).  Random survival forests, <em>Ann. App. Statist.</em>, 2:841-860.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plot.competing.risk.rfsrc">plot.competing.risk.rfsrc</a></code>,
<code><a href="#topic+predict.rfsrc">predict.rfsrc</a></code>,
<code><a href="#topic+rfsrc">rfsrc</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## veteran data
data(veteran, package = "randomForestSRC") 
plot.survival(rfsrc(Surv(time, status)~ ., veteran), cens.model = "rfsrc")

## pbc data
data(pbc, package = "randomForestSRC") 
pbc.obj &lt;- rfsrc(Surv(days, status) ~ ., pbc)

## use subset to focus on specific individuals
plot.survival(pbc.obj, subset = 3)
plot.survival(pbc.obj, subset = c(3, 10))
plot.survival(pbc.obj, subset = c(3, 10), collapse = TRUE)

## get.brier.survival function does many nice things!
plot(get.brier.survival(pbc.obj, cens.model="km")$brier.score,type="s", col=2)
lines(get.brier.survival(pbc.obj, cens.model="rfsrc")$brier.score, type="s", col=4)
legend("bottomright", legend=c("cens.model = km", "cens.model = rfsrc"), fill=c(2,4))


</code></pre>

<hr>
<h2 id='plot.variable.rfsrc'>Plot Marginal Effect of Variables</h2><span id='topic+plot.variable.rfsrc'></span><span id='topic+plot.variable'></span>

<h3>Description</h3>

<p>Plot the marginal effect of an x-variable on the class probability
(classification), response (regression), mortality (survival), or the
expected years lost (competing risk).  Users can select between
marginal (unadjusted, but fast) and partial plots (adjusted, but
slower).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'rfsrc'
plot.variable(x, xvar.names, target,
  m.target = NULL, time, surv.type = c("mort", "rel.freq",
  "surv", "years.lost", "cif", "chf"), class.type =
  c("prob", "bayes"), partial = FALSE, oob = TRUE,
  show.plots = TRUE, plots.per.page = 4, granule = 5, sorted = TRUE,
  nvar, npts = 25, smooth.lines = FALSE, subset, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.variable.rfsrc_+3A_x">x</code></td>
<td>
<p>An object of class <code>(rfsrc, grow)</code>, <code>(rfsrc,
    synthetic)</code>, or <code>(rfsrc, plot.variable)</code>.</p>
</td></tr>
<tr><td><code id="plot.variable.rfsrc_+3A_xvar.names">xvar.names</code></td>
<td>
<p>Names of the x-variables to be used.</p>
</td></tr>
<tr><td><code id="plot.variable.rfsrc_+3A_target">target</code></td>
<td>
<p>For classification, an integer or
character value specifying the class to focus on (defaults to the
first class).  For competing risks, an integer value between
1 and <code>J</code> indicating the event of interest, where <code>J</code> is
the number of event types.  The default is to use the first event
type.</p>
</td></tr>
<tr><td><code id="plot.variable.rfsrc_+3A_m.target">m.target</code></td>
<td>
<p>Character value for multivariate families
specifying the target outcome to be used.  If left unspecified, the
algorithm will choose a default target.</p>
</td></tr>
<tr><td><code id="plot.variable.rfsrc_+3A_time">time</code></td>
<td>
<p>For survival, the time at which the predicted
survival value is evaluated at (depends on <code>surv.type</code>).</p>
</td></tr>
<tr><td><code id="plot.variable.rfsrc_+3A_surv.type">surv.type</code></td>
<td>
<p>For survival, specifies the predicted value.
See details below.</p>
</td></tr>
<tr><td><code id="plot.variable.rfsrc_+3A_class.type">class.type</code></td>
<td>
<p>For classification, specifies the predicted value.
See details below.</p>
</td></tr>
<tr><td><code id="plot.variable.rfsrc_+3A_partial">partial</code></td>
<td>
<p>Should partial plots be used?</p>
</td></tr>
<tr><td><code id="plot.variable.rfsrc_+3A_oob">oob</code></td>
<td>
<p>OOB (TRUE)  or in-bag (FALSE) predicted values.</p>
</td></tr>
<tr><td><code id="plot.variable.rfsrc_+3A_show.plots">show.plots</code></td>
<td>
<p>Should plots be displayed?</p>
</td></tr>
<tr><td><code id="plot.variable.rfsrc_+3A_plots.per.page">plots.per.page</code></td>
<td>
<p>Integer value controlling page layout.</p>
</td></tr>
<tr><td><code id="plot.variable.rfsrc_+3A_granule">granule</code></td>
<td>
<p>Integer value controlling whether a plot for a
specific variable should be treated as a factor and therefore given
as a boxplot. Larger values coerce boxplots.</p>
</td></tr>
<tr><td><code id="plot.variable.rfsrc_+3A_sorted">sorted</code></td>
<td>
<p>Should variables be sorted by importance values.</p>
</td></tr>
<tr><td><code id="plot.variable.rfsrc_+3A_nvar">nvar</code></td>
<td>
<p>Number of variables to be plotted. Default is all.</p>
</td></tr>
<tr><td><code id="plot.variable.rfsrc_+3A_npts">npts</code></td>
<td>
<p>Maximum number of points used when generating partial
plots for continuous variables.</p>
</td></tr>
<tr><td><code id="plot.variable.rfsrc_+3A_smooth.lines">smooth.lines</code></td>
<td>
<p>Use lowess to smooth partial plots.</p>
</td></tr>
<tr><td><code id="plot.variable.rfsrc_+3A_subset">subset</code></td>
<td>
<p>Vector indicating which rows of the x-variable matrix
<code>x$xvar</code> to use. All rows are used if not specified.  Do not
define subset based on the original data (which could have been processed
due to missing values or for other reasons in the previous forest call) but
define subset based on the rows of <code>x$xvar</code>.</p>
</td></tr>
<tr><td><code id="plot.variable.rfsrc_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The vertical axis displays the ensemble predicted value, while
x-variables are plotted on the horizontal axis.
</p>

<ol>
<li><p> For regression, the predicted response is used.
</p>
</li>
<li><p> For classification, it is the predicted class probability
specified by <span class="option">target</span>, or the class of maximum
probability depending on <span class="option">class.type</span> is set to &quot;prob&quot; or
&quot;bayes&quot;.
</p>
</li>
<li><p> For multivariate families, it is the predicted value of the
outcome specified by <span class="option">m.target</span> and if that is a
classification outcome, by <span class="option">target</span>.
</p>
</li>
<li><p> For survival, the choices are:
</p>

<ul>
<li><p> Mortality (<code>mort</code>).  Mortality (Ishwaran et al.,
2008) represents estimated risk for an individual
calibrated to the scale of number of events (as a
specific example, if <em>i</em> has a mortality value of
100, then if all individuals had the same x-values as
<em>i</em>, we would expect an average of 100 events).
</p>
</li>
<li><p>  Relative frequency of mortality (<code>rel.freq</code>).
</p>
</li>
<li><p> Predicted survival (<code>surv</code>), where the predicted
survival is for the time point specified using
<code>time</code> (the default is the median follow up
time).
</p>
</li></ul>

</li>
<li><p> For competing risks, the choices are:
</p>

<ul>
<li><p>  The expected number of life years lost (<code>years.lost</code>).
</p>
</li>
<li><p>  The cumulative incidence function (<code>cif</code>).
</p>
</li>
<li><p>  The cumulative hazard function (<code>chf</code>).
</p>
</li></ul>

<p>In all three cases, the predicted value is for the event type
specified by <span class="option">target</span>.  For <code>cif</code> and
<code>chf</code> the quantity is evaluated at the time point specified
by <code>time</code>.
</p>
</li></ol>

<p>For partial plots use <span class="option">partial=TRUE</span>.  Their interpretation are
different than marginal plots.  The y-value for a variable <code class="reqn">X</code>,
evaluated at <code class="reqn">X=x</code>, is
</p>
<p style="text-align: center;"><code class="reqn">
    \tilde{f}(x) = \frac{1}{n} \sum_{i=1}^n \hat{f}(x, x_{i,o}),
  </code>
</p>

<p>where <code class="reqn">x_{i,o}</code> represents the value for all other variables
other than <code class="reqn">X</code> for individual <code class="reqn">i</code> and <code class="reqn">\hat{f}</code> is the
predicted value. Generating partial plots can be very slow.
Choosing a small value for <code>npts</code> can speed up computational
times as this restricts the number of distinct <code class="reqn">x</code> values used
in computing <code class="reqn">\tilde{f}</code>.
</p>
<p>For continuous variables, red points are used to indicate partial
values and dashed red lines indicate a smoothed error bar of +/- two
standard errors.  Black dashed line are the partial values.  Set
<span class="option">smooth.lines=TRUE</span> for lowess smoothed lines.  For discrete
variables, partial values are indicated using boxplots with whiskers
extending out approximately two standard errors from the mean.
Standard errors are meant only to be a guide and should be
interpreted with caution.
</p>
<p>Partial plots can be slow.  Setting <span class="option">npts</span> to a smaller number
can help.
</p>
<p>For greater customization and computational speed for partial plot calls,
consider using the function <code>partial.rfsrc</code> which provides a
direct interface for calculating partial plot data.   
</p>


<h3>Author(s)</h3>

<p>Hemant Ishwaran and Udaya B. Kogalur
</p>


<h3>References</h3>

<p>Friedman J.H. (2001). Greedy function approximation: a gradient
boosting machine, <em>Ann. of Statist.</em>, 5:1189-1232.
</p>
<p>Ishwaran H., Kogalur U.B. (2007).  Random survival forests for R,
<em>Rnews</em>, 7(2):25-31.
</p>
<p>Ishwaran H., Kogalur U.B., Blackstone E.H. and Lauer M.S.
(2008).  Random survival forests, <em>Ann. App.
Statist.</em>, 2:841-860.
</p>
<p>Ishwaran H., Gerds T.A., Kogalur U.B., Moore R.D., Gange S.J. and Lau
B.M. (2014). Random survival forests for competing risks.
<em>Biostatistics</em>, 15(4):757-773.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rfsrc">rfsrc</a></code>,
<code><a href="#topic+synthetic.rfsrc">synthetic.rfsrc</a></code>,
<code><a href="#topic+partial.rfsrc">partial.rfsrc</a></code>,
<code><a href="#topic+predict.rfsrc">predict.rfsrc</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## ------------------------------------------------------------
## survival/competing risk
## ------------------------------------------------------------

## survival
data(veteran, package = "randomForestSRC")
v.obj &lt;- rfsrc(Surv(time,status)~., veteran, ntree = 100)
plot.variable(v.obj, plots.per.page = 3)
plot.variable(v.obj, plots.per.page = 2, xvar.names = c("trt", "karno", "age"))
plot.variable(v.obj, surv.type = "surv", nvar = 1, time = 200)
plot.variable(v.obj, surv.type = "surv", partial = TRUE, smooth.lines = TRUE)
plot.variable(v.obj, surv.type = "rel.freq", partial = TRUE, nvar = 2)

## example of plot.variable calling a pre-processed plot.variable object
p.v &lt;- plot.variable(v.obj, surv.type = "surv", partial = TRUE, smooth.lines = TRUE)
plot.variable(p.v)
p.v$plots.per.page &lt;- 1
p.v$smooth.lines &lt;- FALSE
plot.variable(p.v)

## competing risks
data(follic, package = "randomForestSRC")
follic.obj &lt;- rfsrc(Surv(time, status) ~ ., follic, nsplit = 3, ntree = 100)
plot.variable(follic.obj, target = 2)

## ------------------------------------------------------------
## regression
## ------------------------------------------------------------

## airquality 
airq.obj &lt;- rfsrc(Ozone ~ ., data = airquality)
plot.variable(airq.obj, partial = TRUE, smooth.lines = TRUE)
plot.variable(airq.obj, partial = TRUE, subset = airq.obj$xvar$Solar.R &lt; 200)

## motor trend cars
mtcars.obj &lt;- rfsrc(mpg ~ ., data = mtcars)
plot.variable(mtcars.obj, partial = TRUE, smooth.lines = TRUE)

## ------------------------------------------------------------
## classification
## ------------------------------------------------------------

## iris
iris.obj &lt;- rfsrc(Species ~., data = iris)
plot.variable(iris.obj, partial = TRUE)

## motor trend cars: predict number of carburetors
mtcars2 &lt;- mtcars
mtcars2$carb &lt;- factor(mtcars2$carb,
   labels = paste("carb", sort(unique(mtcars$carb))))
mtcars2.obj &lt;- rfsrc(carb ~ ., data = mtcars2)
plot.variable(mtcars2.obj, partial = TRUE)

## ------------------------------------------------------------
## multivariate regression
## ------------------------------------------------------------
mtcars.mreg &lt;- rfsrc(Multivar(mpg, cyl) ~., data = mtcars)
plot.variable(mtcars.mreg, m.target = "mpg", partial = TRUE, nvar = 1)
plot.variable(mtcars.mreg, m.target = "cyl", partial = TRUE, nvar = 1)


## ------------------------------------------------------------
## multivariate mixed outcomes
## ------------------------------------------------------------
mtcars2 &lt;- mtcars
mtcars2$carb &lt;- factor(mtcars2$carb)
mtcars2$cyl &lt;- factor(mtcars2$cyl)
mtcars.mix &lt;- rfsrc(Multivar(carb, mpg, cyl) ~ ., data = mtcars2)
plot.variable(mtcars.mix, m.target = "cyl", target = "4", partial = TRUE, nvar = 1)
plot.variable(mtcars.mix, m.target = "cyl", target = 2, partial = TRUE, nvar = 1)



</code></pre>

<hr>
<h2 id='predict.rfsrc'>Prediction for Random Forests for Survival, Regression, and Classification</h2><span id='topic+predict.rfsrc'></span>

<h3>Description</h3>

<p>Obtain predicted values using a forest.  Also returns performance
values if the test data contains y-outcomes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'rfsrc'
predict(object,
  newdata,
  m.target = NULL,
  importance = c(FALSE, TRUE, "none", "anti", "permute", "random"),
  get.tree = NULL,
  block.size = if (any(is.element(as.character(importance),
                     c("none", "FALSE")))) NULL else 10,
  na.action = c("na.omit", "na.impute", "na.random"),
  outcome = c("train", "test"),
  perf.type = NULL,
  proximity = FALSE,
  forest.wt = FALSE,
  ptn.count = 0,
  distance = FALSE,
  var.used = c(FALSE, "all.trees", "by.tree"),
  split.depth = c(FALSE, "all.trees", "by.tree"), seed = NULL,
  do.trace = FALSE, membership = FALSE, statistics = FALSE,
   
  ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.rfsrc_+3A_object">object</code></td>
<td>
<p>An object of class <code>(rfsrc, grow)</code> or <code>(rfsrc,
	    forest)</code>.</p>
</td></tr>
<tr><td><code id="predict.rfsrc_+3A_newdata">newdata</code></td>
<td>
<p>Test data. If missing, the original grow (training)
data is used.</p>
</td></tr>
<tr><td><code id="predict.rfsrc_+3A_m.target">m.target</code></td>
<td>
<p>Character vector for multivariate families
specifying the target outcomes to be used. The default uses all
coordinates.</p>
</td></tr>
<tr><td><code id="predict.rfsrc_+3A_importance">importance</code></td>
<td>
<p>Method used for variable importance (VIMP).  Also
see <code>vimp</code> for more flexibility, including joint vimp
calculations.  See <code>holdoutvimp</code> for an alternate importance
measure.</p>
</td></tr>
<tr><td><code id="predict.rfsrc_+3A_get.tree">get.tree</code></td>
<td>
<p>Vector of integer(s) identifying trees over which the
ensembles are calculated over.  By default, uses all trees in the
forest.  As an example, the user can extract the ensemble, the VIMP
, or proximity from a single tree (or several trees).  Note that
<code>block.size</code> will be over-ridden so that it is no larger than
the requested number of trees.  See example below illustrating how
to extract VIMP for each tree.</p>
</td></tr>
<tr><td><code id="predict.rfsrc_+3A_block.size">block.size</code></td>
<td>
<p>Should the error rate be calculated on every tree?
When <code>NULL</code>, it will only be calculated on the last tree.  To
view the error rate on every nth tree, set the value to an integer
between <code>1</code> and <code>ntree</code>.  If importance is requested, VIMP
is calculated in &quot;blocks&quot; of size equal to <code>block.size</code>, thus
resulting in a compromise between ensemble and permutation VIMP.</p>
</td></tr>
<tr><td><code id="predict.rfsrc_+3A_na.action">na.action</code></td>
<td>
<p>Missing value action. The default <code>na.omit</code>
removes the entire record if any entry is <code>NA</code>.  
Selecting <span class="option">na.random</span> uses fast random imputation, while
<span class="option">na.impute</span> uses the imputation method described in
<code>rfsrc</code>.</p>
</td></tr>
<tr><td><code id="predict.rfsrc_+3A_outcome">outcome</code></td>
<td>
<p>Determines whether the y-outcomes from the training
data or the test data are used to calculate the predicted value.
The default and natural choice is <code>train</code> which uses the
original training data.  Option is ignored when <code>newdata</code> is
missing as the training data is used for the test data in such
settings.  The option is also ignored whenever the test data is
devoid of y-outcomes.  See the details and examples below for more
information.</p>
</td></tr>
<tr><td><code id="predict.rfsrc_+3A_perf.type">perf.type</code></td>
<td>
<p>Optional character value for requesting metric used
for predicted value, variable importance (VIMP) and error rate.  If not
specified, values returned are calculated by the default action used
for the family. Currently applicable only to classification and
multivariate classification; allowed values are
<code>perf.type="misclass"</code> (default), <code>perf.type="brier"</code> and
<code>perf.type="gmean"</code>.</p>
</td></tr>
<tr><td><code id="predict.rfsrc_+3A_proximity">proximity</code></td>
<td>
<p>Should proximity between test observations
be calculated?  Possible choices are <code>"inbag"</code>, <code>"oob"</code>,
<code>"all"</code>, <code>TRUE</code>, or <code>FALSE</code> &mdash; but some options may
not be valid and will depend on the context of the predict call.
The safest choice is <code>TRUE</code> if proximity is desired.</p>
</td></tr>
<tr><td><code id="predict.rfsrc_+3A_distance">distance</code></td>
<td>
<p>Should distance between test observations
be calculated?  Possible choices are <code>"inbag"</code>, <code>"oob"</code>,
<code>"all"</code>, <code>TRUE</code>, or <code>FALSE</code> &mdash; but some options may
not be valid and will depend on the context of the predict call.
The safest choice is <code>TRUE</code> if distance is desired.</p>
</td></tr>
<tr><td><code id="predict.rfsrc_+3A_forest.wt">forest.wt</code></td>
<td>
<p>Should the forest weight matrix for test observations
be calculated?  Choices are the same as proximity.</p>
</td></tr>
<tr><td><code id="predict.rfsrc_+3A_ptn.count">ptn.count</code></td>
<td>
<p>The number of terminal nodes that each tree in the
grow forest should be pruned back to.  The terminal node membership
for the pruned forest is returned but no other action is taken.  The
default is <code>ptn.count=0</code> which does no pruning.</p>
</td></tr>
<tr><td><code id="predict.rfsrc_+3A_var.used">var.used</code></td>
<td>
<p>Record the number of times a variable is split?</p>
</td></tr>
<tr><td><code id="predict.rfsrc_+3A_split.depth">split.depth</code></td>
<td>
<p>Return minimal depth for each variable for each case?</p>
</td></tr>
<tr><td><code id="predict.rfsrc_+3A_seed">seed</code></td>
<td>
<p>Negative integer specifying seed for the random number
generator.</p>
</td></tr> 
<tr><td><code id="predict.rfsrc_+3A_do.trace">do.trace</code></td>
<td>
<p>Number of seconds between updates to the user on
approximate time to completion.</p>
</td></tr>
<tr><td><code id="predict.rfsrc_+3A_membership">membership</code></td>
<td>
<p>Should terminal node membership and inbag
information be returned?</p>
</td></tr>
<tr><td><code id="predict.rfsrc_+3A_statistics">statistics</code></td>
<td>
<p>Should split statistics be returned?  Values can be
parsed using <code>stat.split</code>.</p>
</td></tr>
<tr><td><code id="predict.rfsrc_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Predicted values are obtained by &quot;dropping&quot; test data down the trained
forest (forest calculated using training data).  Performance values are
returned if test data contains y-outcome values.  Single as well as
joint VIMP are also returned if requested.
</p>
<p>If no test data is provided, the original training data is used, and
the code reverts to restore mode allowing the user to restore the
original trained forest.  This feature allows extracting outputs from
the forest not asked for in the original grow call.
</p>
<p>If <span class="option">outcome="test"</span>, the predictor is calculated by using
y-outcomes from the test data (outcome information must be present).
Terminal nodes from the trained forest are recalculated using y-outcomes
from the test set.  This yields a modified predictor in which the
topology of the forest is based solely on the training data, but where
predicted values are obtained from test data.  Error rates and VIMP
are calculated by bootstrapping the test data and using out-of-bagging
to ensure unbiased estimates.  
</p>
<p><code>csv=TRUE</code> returns case specific VIMP;  <code>cse=TRUE</code> returns
case specific error rates.  Applies to all families except survival.
These options can also be applied while training.
</p>


<h3>Value</h3>

<p>An object of class <code>(rfsrc, predict)</code>, which is a list with the
following components:
</p>
<table>
<tr><td><code>call</code></td>
<td>
<p>The original grow call to <code>rfsrc</code>.</p>
</td></tr>
<tr><td><code>family</code></td>
<td>
<p>The family used in the analysis.</p>
</td></tr>
<tr><td><code>n</code></td>
<td>
<p>Sample size of test data (depends upon <code>NA</code> values).</p>
</td></tr>
<tr><td><code>ntree</code></td>
<td>
<p>Number of trees in the grow forest.</p>
</td></tr>
<tr><td><code>yvar</code></td>
<td>
<p>Test set y-outcomes or original grow y-outcomes if none.</p>
</td></tr>
<tr><td><code>yvar.names</code></td>
<td>
<p>A character vector of the y-outcome names.</p>
</td></tr>
<tr><td><code>xvar</code></td>
<td>
<p>Data frame of test set x-variables.</p>
</td></tr>
<tr><td><code>xvar.names</code></td>
<td>
<p>A character vector of the x-variable names.</p>
</td></tr>
<tr><td><code>leaf.count</code></td>
<td>
<p>Number of terminal nodes for each tree in the
grow forest.  Vector of length <code>ntree</code>.</p>
</td></tr>
<tr><td><code>proximity</code></td>
<td>
<p>Symmetric proximity matrix of the test data.</p>
</td></tr>
<tr><td><code>forest</code></td>
<td>
<p>The grow forest.</p>
</td></tr>
<tr><td><code>membership</code></td>
<td>
<p>Matrix recording terminal node membership for the
test data where each column contains the node number that a
case falls in for that tree.</p>
</td></tr>
<tr><td><code>inbag</code></td>
<td>
<p>Matrix recording inbag membership for the test data
where each column contains the number of times that a case
appears in the bootstrap sample for that tree.</p>
</td></tr>
<tr><td><code>var.used</code></td>
<td>
<p>Count of the number of times a variable was used in
growing the forest.</p>
</td></tr>
<tr><td><code>imputed.indv</code></td>
<td>
<p>Vector of indices of records in test data with
missing values.</p>
</td></tr>
<tr><td><code>imputed.data</code></td>
<td>
<p>Data frame comprising imputed test data.  The first
columns are the y-outcomes followed by the x-variables.</p>
</td></tr>
<tr><td><code>split.depth</code></td>
<td>
<p>Matrix (i,j) or array (i,j,k) recording the
minimal depth for variable j for case i, either averaged over
the forest, or by tree k.</p>
</td></tr>
<tr><td><code>node.stats</code></td>
<td>
<p>Split statistics returned when
<code>statistics=TRUE</code> which can be parsed using <code>stat.split</code>.</p>
</td></tr>
<tr><td><code>err.rate</code></td>
<td>
<p>Cumulative OOB error rate for the test data if
y-outcomes are present.</p>
</td></tr>
<tr><td><code>importance</code></td>
<td>
<p>Test set variable importance (VIMP).  Can be
<code>NULL</code>.</p>
</td></tr>
<tr><td><code>predicted</code></td>
<td>
<p>Test set predicted value.</p>
</td></tr>
<tr><td><code>predicted.oob</code></td>
<td>
<p>OOB predicted value (<code>NULL</code> unless
<span class="option">outcome="test"</span>).</p>
</td></tr></table>
<p><br />
</p>
<table>
<tr><td><code>quantile</code></td>
<td>
<p>Quantile value at probabilities requested.</p>
</td></tr>
<tr><td><code>quantile.oob</code></td>
<td>
<p>OOB quantile value at probabilities requested (<code>NULL</code> unless
<span class="option">outcome="test"</span>).</p>
</td></tr></table>
<p><br />
</p>
<table>
<tr><td><code>++++++++</code></td>
<td>
<p>for classification settings, additionally ++++++++</p>
</td></tr> </table>
<p><br />     
</p>
<table>
<tr><td><code>class</code></td>
<td>
<p>In-bag predicted class labels.</p>
</td></tr>
<tr><td><code>class.oob</code></td>
<td>
<p>OOB predicted class labels (<code>NULL</code> unless <span class="option">outcome="test"</span>).</p>
</td></tr></table>
<p><br />
</p>
<table>
<tr><td><code>++++++++</code></td>
<td>
<p>for multivariate settings, additionally ++++++++</p>
</td></tr> </table>
<p><br />
</p>
<table>
<tr><td><code>regrOutput</code></td>
<td>
<p>List containing performance values for
test multivariate regression responses (applies only in
multivariate settings).</p>
</td></tr>
<tr><td><code>clasOutput</code></td>
<td>
<p>List containing performance values for
test multivariate categorical (factor) responses (applies only in
multivariate settings).</p>
</td></tr>
<tr><td><code>++++++++</code></td>
<td>
<p>for survival settings, additionally ++++++++</p>
</td></tr> </table>
<p><br />
</p>
<table>
<tr><td><code>chf</code></td>
<td>
<p>Cumulative hazard function (CHF).</p>
</td></tr>
<tr><td><code>chf.oob</code></td>
<td>
<p>OOB CHF (<code>NULL</code> unless <span class="option">outcome="test"</span>).</p>
</td></tr>
<tr><td><code>survival</code></td>
<td>
<p>Survival function.</p>
</td></tr>
<tr><td><code>survival.oob</code></td>
<td>
<p>OOB survival function (<code>NULL</code> unless <span class="option">outcome="test"</span>).</p>
</td></tr>
<tr><td><code>time.interest</code></td>
<td>
<p>Ordered unique death times.</p>
</td></tr>
<tr><td><code>ndead</code></td>
<td>
<p>Number of deaths.</p>
</td></tr></table>
<p><br />
</p>
<table>
<tr><td><code>++++++++</code></td>
<td>
<p>for competing risks, additionally ++++++++</p>
</td></tr> </table>
<p><br />
</p>
<table>
<tr><td><code>chf</code></td>
<td>
<p>Cause-specific cumulative hazard function (CSCHF)
for each event.</p>
</td></tr>
<tr><td><code>chf.oob</code></td>
<td>
<p>OOB CSCHF for each event (<code>NULL</code> unless <span class="option">outcome="test"</span>).</p>
</td></tr>
<tr><td><code>cif</code></td>
<td>
<p>Cumulative incidence function (CIF) for each event.</p>
</td></tr>
<tr><td><code>cif.oob</code></td>
<td>
<p>OOB CIF (<code>NULL</code> unless <span class="option">outcome="test"</span>).</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The dimensions and values of returned objects depend heavily on the
underlying family and whether y-outcomes are present in the test data.
In particular, items related to performance will be <code>NULL</code> when
y-outcomes are not present.  For multivariate families, predicted
values, VIMP, error rate, and performance values are stored in the
lists <code>regrOutput</code> and <code>clasOutput</code> which can be extracted
using functions <code>get.mv.error</code>, <code>get.mv.predicted</code> and
<code>get.mv.vimp</code>.
</p>


<h3>Author(s)</h3>

<p>Hemant Ishwaran and Udaya B. Kogalur
</p>


<h3>References</h3>

<p>Breiman L. (2001). Random forests, <em>Machine Learning</em>, 45:5-32.
</p>
<p>Ishwaran H., Kogalur U.B., Blackstone E.H. and Lauer M.S.
(2008).  Random survival forests, <em>Ann. App.
Statist.</em>, 2:841-860.
</p>
<p>Ishwaran H. and Kogalur U.B. (2007).  Random survival forests for R,
<em>Rnews</em>, 7(2):25-31.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+holdout.vimp.rfsrc">holdout.vimp.rfsrc</a></code>,
<code><a href="#topic+plot.competing.risk.rfsrc">plot.competing.risk.rfsrc</a></code>,
<code><a href="#topic+plot.rfsrc">plot.rfsrc</a></code>,
<code><a href="#topic+plot.survival.rfsrc">plot.survival.rfsrc</a></code>,
<code><a href="#topic+plot.variable.rfsrc">plot.variable.rfsrc</a></code>,
<code><a href="#topic+rfsrc">rfsrc</a></code>,
<code><a href="#topic+rfsrc.fast">rfsrc.fast</a></code>,
<code><a href="#topic+stat.split.rfsrc">stat.split.rfsrc</a></code>,
<code><a href="#topic+synthetic.rfsrc">synthetic.rfsrc</a></code>,
<code><a href="#topic+vimp.rfsrc">vimp.rfsrc</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## ------------------------------------------------------------
## typical train/testing scenario
## ------------------------------------------------------------
options(rf.cores=2, mc.cores=2)

data(veteran, package = "randomForestSRC")
train &lt;- sample(1:nrow(veteran), round(nrow(veteran) * 0.80))
veteran.grow &lt;- rfsrc(Surv(time, status) ~ ., veteran[train, ], ntree = 10) 
veteran.pred &lt;- predict(veteran.grow, veteran[-train, ])
print(veteran.grow)
print(veteran.pred)


## ------------------------------------------------------------
## restore mode
## - if predict is called without specifying the test data
##   the original training data is used and the forest is restored
## ------------------------------------------------------------

## first train the forest
airq.obj &lt;- rfsrc(Ozone ~ ., data = airquality)

## now we restore it and compare it to the original call
## they are identical
predict(airq.obj)
print(airq.obj)

## we can retrieve various outputs that were not asked for in
## in the original call

## here we extract the proximity matrix
prox &lt;- predict(airq.obj, proximity = TRUE)$proximity
print(prox[1:10,1:10])

## here we extract the number of times a variable was used to grow
## the grow forest
var.used &lt;- predict(airq.obj, var.used = "by.tree")$var.used
print(head(var.used))

## ------------------------------------------------------------
## prediction when test data has missing values
## ------------------------------------------------------------

data(pbc, package = "randomForestSRC")
trn &lt;- pbc[1:312,]
tst &lt;- pbc[-(1:312),]
o &lt;- rfsrc(Surv(days, status) ~ ., trn)

## default imputation method used by rfsrc
print(predict(o, tst, na.action = "na.impute"))

## random imputation
print(predict(o, tst, na.action = "na.random"))

## ------------------------------------------------------------
## requesting different performance for classification
## ------------------------------------------------------------

## default performance is misclassification
o &lt;- rfsrc(Species~., iris)
print(o)

## get (normalized) brier performance
print(predict(o, perf.type = "brier"))

## ------------------------------------------------------------
## vimp for each tree: illustrates get.tree 
## ------------------------------------------------------------

## regression analysis but no VIMP
o &lt;- rfsrc(mpg~., mtcars)

## now extract VIMP for each tree using get.tree
vimp.tree &lt;- do.call(rbind, lapply(1:o$ntree, function(b) {
     predict(o, get.tree = b, importance = TRUE)$importance
}))

## boxplot of tree VIMP
boxplot(vimp.tree, outline = FALSE, col = "cyan")
abline(h = 0, lty = 2, col = "red")

## summary information of tree VIMP
print(summary(vimp.tree))

## extract tree-averaged VIMP using importance=TRUE
## remember to set block.size to 1
print(predict(o, importance = TRUE, block.size = 1)$importance)

## use direct call to vimp() for tree-averaged VIMP
print(vimp(o, block.size = 1)$importance)

## ------------------------------------------------------------
## vimp for just a few trees
## illustrates how to get vimp if you have a large data set
## ------------------------------------------------------------

## survival analysis but no VIMP
data(pbc, package = "randomForestSRC")
o &lt;- rfsrc(Surv(days, status) ~ ., pbc, ntree = 2000)

## get vimp for a small number of trees
print(predict(o, get.tree=1:250, importance = TRUE)$importance)


## ------------------------------------------------------------
## case-specific vimp
## returns VIMP for each case
## ------------------------------------------------------------

o &lt;- rfsrc(mpg~., mtcars)
op &lt;- predict(o, importance = TRUE, csv = TRUE)
csvimp &lt;- get.mv.csvimp(op, standardize=TRUE)
print(csvimp)

## ------------------------------------------------------------
## case-specific error rate
## returns tree-averaged error rate for each case
## ------------------------------------------------------------

o &lt;- rfsrc(mpg~., mtcars)
op &lt;- predict(o, importance = TRUE, cse = TRUE)
cserror &lt;- get.mv.cserror(op, standardize=TRUE)
print(cserror)


## ------------------------------------------------------------
## predicted probability and predicted class labels are returned
## in the predict object for classification analyses
## ------------------------------------------------------------

data(breast, package = "randomForestSRC")
breast.obj &lt;- rfsrc(status ~ ., data = breast[(1:100), ])
breast.pred &lt;- predict(breast.obj, breast[-(1:100), ])
print(head(breast.pred$predicted))
print(breast.pred$class)


## ------------------------------------------------------------
## unique feature of randomForestSRC
## cross-validation can be used when factor labels differ over
## training and test data
## ------------------------------------------------------------

## first we convert all x-variables to factors
data(veteran, package = "randomForestSRC")
veteran2 &lt;- data.frame(lapply(veteran, factor))
veteran2$time &lt;- veteran$time
veteran2$status &lt;- veteran$status

## split the data into unbalanced train/test data (25/75)
## the train/test data have the same levels, but different labels
train &lt;- sample(1:nrow(veteran2), round(nrow(veteran2) * .25))
summary(veteran2[train,])
summary(veteran2[-train,])

## train the forest and use this to predict on test data
o.grow &lt;- rfsrc(Surv(time, status) ~ ., veteran2[train, ]) 
o.pred &lt;- predict(o.grow, veteran2[-train , ])
print(o.grow)
print(o.pred)

## even harder ... factor level not previously encountered in training
veteran3 &lt;- veteran2[1:3, ]
veteran3$celltype &lt;- factor(c("newlevel", "1", "3"))
o2.pred &lt;- predict(o.grow, veteran3)
print(o2.pred)
## the unusual level is treated like a missing value but is not removed
print(o2.pred$xvar)

## ------------------------------------------------------------
## example illustrating the flexibility of outcome = "test"
## illustrates restoration of forest via outcome = "test"
## ------------------------------------------------------------

## first we train the forest
data(pbc, package = "randomForestSRC")
pbc.grow &lt;- rfsrc(Surv(days, status) ~ ., pbc)

## use predict with outcome = TEST
pbc.pred &lt;- predict(pbc.grow, pbc, outcome = "test")

## notice that error rates are the same!!
print(pbc.grow)
print(pbc.pred)

## note this is equivalent to restoring the forest
pbc.pred2 &lt;- predict(pbc.grow)
print(pbc.grow)
print(pbc.pred)
print(pbc.pred2)

## similar example, but with na.action = "na.impute"
airq.obj &lt;- rfsrc(Ozone ~ ., data = airquality, na.action = "na.impute")
print(airq.obj)
print(predict(airq.obj))
## ... also equivalent to outcome="test" but na.action = "na.impute" required
print(predict(airq.obj, airquality, outcome = "test", na.action = "na.impute"))

## classification example
iris.obj &lt;- rfsrc(Species ~., data = iris)
print(iris.obj)
print(predict.rfsrc(iris.obj, iris, outcome = "test"))

## ------------------------------------------------------------
## another example illustrating outcome = "test"
## unique way to check reproducibility of the forest
## ------------------------------------------------------------

## training step
set.seed(542899)
data(pbc, package = "randomForestSRC")
train &lt;- sample(1:nrow(pbc), round(nrow(pbc) * 0.50))
pbc.out &lt;- rfsrc(Surv(days, status) ~ .,  data=pbc[train, ])

## standard prediction call
pbc.train &lt;- predict(pbc.out, pbc[-train, ], outcome = "train")
##non-standard predict call: overlays the test data on the grow forest
pbc.test &lt;- predict(pbc.out, pbc[-train, ], outcome = "test")

## check forest reproducibilility by comparing "test" predicted survival
## curves to "train" predicted survival curves for the first 3 individuals
Time &lt;- pbc.out$time.interest
matplot(Time, t(pbc.train$survival[1:3,]), ylab = "Survival", col = 1, type = "l")
matlines(Time, t(pbc.test$survival[1:3,]), col = 2)

## ------------------------------------------------------------
## ... just for _fun_ ...
## survival analysis using mixed multivariate outcome analysis 
## compare the predicted value to RSF
## ------------------------------------------------------------

## train survival forest using pbc data
data(pbc, package = "randomForestSRC")
rsf.obj &lt;- rfsrc(Surv(days, status) ~ ., pbc)
yvar &lt;- rsf.obj$yvar

## fit a mixed outcome forest using days and status as y-variables
pbc.mod &lt;- pbc
pbc.mod$status &lt;- factor(pbc.mod$status)
mix.obj &lt;- rfsrc(Multivar(days, status) ~., pbc.mod)

## compare oob predicted values
rsf.pred &lt;- rsf.obj$predicted.oob
mix.pred &lt;- mix.obj$regrOutput$days$predicted.oob
plot(rsf.pred, mix.pred)

## compare C-index error rate
rsf.err &lt;- get.cindex(yvar$days, yvar$status, rsf.pred)
mix.err &lt;- 1 - get.cindex(yvar$days, yvar$status, mix.pred)
cat("RSF                :", rsf.err, "\n")
cat("multivariate forest:", mix.err, "\n")


</code></pre>

<hr>
<h2 id='print.rfsrc'>Print Summary Output of a RF-SRC Analysis</h2><span id='topic+print.rfsrc'></span>

<h3>Description</h3>

<p>Print summary output from a RF-SRC analysis. This is the default
print method for the package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'rfsrc'
print(x, outcome.target = NULL, ...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.rfsrc_+3A_x">x</code></td>
<td>
<p>An object of class <code>(rfsrc, grow)</code>, <code>(rfsrc, synthetic)</code>,
or <code>(rfsrc, predict)</code>.</p>
</td></tr>
<tr><td><code id="print.rfsrc_+3A_outcome.target">outcome.target</code></td>
<td>
<p>Character value for multivariate families
specifying the target outcome to be used. The default is to use the
first coordinate from the continuous outcomes (otherwise if none,
the first coordinate from the categorical outcomes).</p>
</td></tr>
<tr><td><code id="print.rfsrc_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Hemant Ishwaran and Udaya B. Kogalur
</p>


<h3>References</h3>

<p>Ishwaran H. and Kogalur U.B. (2007).  Random survival forests for R,
<em>Rnews</em>, 7/2:25-31.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  options(rf.cores=2, mc.cores=2)
  iris.obj &lt;- rfsrc(Species ~., data = iris, ntree=10)
  print(iris.obj)
</code></pre>

<hr>
<h2 id='quantreg.rfsrc'>Quantile Regression Forests</h2><span id='topic+quantreg.rfsrc'></span><span id='topic+quantreg'></span><span id='topic+extract.quantile'></span><span id='topic+get.quantile'></span><span id='topic+get.quantile.crps'></span><span id='topic+get.quantile.stat'></span>

<h3>Description</h3>

<p>Grows a univariate or multivariate quantile regression forest and returns
its conditional quantile and density values.  Can be used for
both training and testing purposes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'rfsrc'
quantreg(formula, data, object, newdata,
  method = "local", splitrule = NULL, prob = NULL, prob.epsilon = NULL,
  oob = TRUE, fast = FALSE, maxn = 1e3, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="quantreg.rfsrc_+3A_formula">formula</code></td>
<td>
<p>A symbolic description of the model to be fit.
Must be specified unless <code>object</code> is given.</p>
</td></tr>
<tr><td><code id="quantreg.rfsrc_+3A_data">data</code></td>
<td>
<p>Data frame containing the y-outcome and x-variables in
the model. Must be specified unless <code>object</code> is given.</p>
</td></tr>
<tr><td><code id="quantreg.rfsrc_+3A_object">object</code></td>
<td>
<p>(Optional) A previously grown quantile regression
forest.</p>
</td></tr>
<tr><td><code id="quantreg.rfsrc_+3A_newdata">newdata</code></td>
<td>
<p>(Optional) Test data frame used for prediction.  Note
that prediction on test data must always be done with the
<code>quantreg</code> function and not the <code>predict</code> function.  See
example below.</p>
</td></tr>
<tr><td><code id="quantreg.rfsrc_+3A_method">method</code></td>
<td>
<p>Method used to calculate quantiles.  Three methods are
provided: (1) A variation of the method used in Meinshausen (2006)
based on forest weight (<code>method = "forest"</code>); (2) The
Greenwald-Khanna algorithm, suited for big data, and specified by
any one of the following: &quot;gk&quot;, &quot;GK&quot;, &quot;G-K&quot;, &quot;g-k&quot;; (3) The default
method, <code>method = "local"</code>, which uses the local adjusted cdf
approach of Zhang et al. (2019).  This does not rely on forest
weights and is reasonably fast. See below for further discussion.</p>
</td></tr>
<tr><td><code id="quantreg.rfsrc_+3A_splitrule">splitrule</code></td>
<td>
<p>The default action is local adaptive quantile regression splitting,
but this can be over-ridden by the user.  Not applicable to
multivariate forests.  See details below.</p>
</td></tr>
<tr><td><code id="quantreg.rfsrc_+3A_prob">prob</code></td>
<td>
<p>Target quantile probabilities when training.  If left unspecified,
uses percentiles (1 through 99) for <code>method = "forest"</code>, and 
for Greenwald-Khanna selects equally spaced percentiles optimized
for accuracy (see below).</p>
</td></tr>
<tr><td><code id="quantreg.rfsrc_+3A_prob.epsilon">prob.epsilon</code></td>
<td>
<p>Greenwald-Khanna allowable error for quantile
probabilities when training.</p>
</td></tr>
<tr><td><code id="quantreg.rfsrc_+3A_oob">oob</code></td>
<td>
<p>Return OOB (out-of-bag) quantiles?  If false, in-bag values
are returned.</p>
</td></tr>
<tr><td><code id="quantreg.rfsrc_+3A_fast">fast</code></td>
<td>
<p>Use fast random forests, <code>rfsrc.fast</code>, in place of
<code>rfsrc</code>?  Improves speed but may be less accurate.</p>
</td></tr>
<tr><td><code id="quantreg.rfsrc_+3A_maxn">maxn</code></td>
<td>
<p>Maximum number of unique y training values used when
calculating the conditional density.</p>
</td></tr>
<tr><td><code id="quantreg.rfsrc_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed to the <code>rfsrc</code>
function used for fitting the quantile regression forest.</p>
</td></tr>  
</table>


<h3>Details</h3>

<p>The most common method for calculating RF quantiles uses the method
described in Meinshausen (2006) using forest weights.  The forest
weights method employed here (specified using <code>method</code>=&quot;forest&quot;),
however differs in that quantiles are estimated using a
weighted local cumulative distribution function estimator.  For this
reason, results may differ from Meinshausen (2006).  Moreover, results
may also differ as the default splitting rule uses local adaptive
quantile regression splitting instead of CART regression mean squared
splitting which was used by Meinshausen (2006).  Note that local
adaptive quantile regression splitting is not available for
multivariate forests which reverts to the default multivariate
composite splitting rule. In multivariate regression, users however do
have the option to over-ride this using
Mahalanobis splitting by setting <code>splitrule="mahalanobis"</code>
</p>
<p>A second method for estimating quantiles uses the Greenwald-Khanna
(2001) algorithm (invoked by <code>method</code>=&quot;gk&quot;, &quot;GK&quot;, &quot;G-K&quot; or
&quot;g-k&quot;).  While this will not be as accurate as forest weights, the
high memory efficiency of Greenwald-Khanna makes it feasible to
implement in big data settings unlike forest weights.
</p>
<p>The Greenwald-Khanna algorithm is implemented roughly as follows.  To
form a distribution of values for each case, from which we sample to
determine quantiles, we create a chain of values for the case as we
grow the forest.  Every time a case lands in a terminal node, we
insert all of its co-inhabitants to its chain of values.
</p>
<p>The best case scenario is when tree node size is 1 because each case
gets only one insert into its chain for that tree.  The worst case
scenario is when node size is so large that trees stump. This is
because each case receives insertions for the entire in-bag
population.  
</p>
<p>What the user needs to know is that Greenwald-Khanna can become slow
in counter-intutive settings such as when node size is large.  The
easy fix is to change the epsilon quantile approximation that is
requested.  You will see a significant speed-up just by doubling
<code>prob.epsilon</code>.  This is because the chains stay a lot smaller as
epsilon increases, which is exactly what you want when node sizes are
large. Both time and space requirements for the algorithm are affected
by epsilon.
</p>
<p>The best results for Greenwald-Khanna come from setting the number of
quantiles equal to 2 times the sample size and epsilon to 1 over 2
times the sample size which is the default values used if left
unspecified.  This will be slow, especially for big data, and less
stringent choices should be used if computational speed is of concern.
</p>
<p>Finally, the default method, <code>method</code>=&quot;local&quot;, implements the
locally adjusted cdf estimator of Zhang et al. (2019).  This does not
use forest weights and is reasonably fast and can be used for large
data.  However, this relies on the assumption of homogeneity of the
error distribution, i.e. that errors are iid and therefore have equal
variance.  While this is reasonably robust to departures of homogeneity,
there are instances where this may perform poorly; see Zhang et
al. (2019) for details.  If hetereogeneity is suspected we recommend
<code>method</code>=&quot;forest&quot;.
</p>


<h3>Value</h3>

<p>Returns the object <code>quantreg</code> containing quantiles for each of
the requested probabilities (which can be conveniently extracted using
<code>get.quantile</code>).  Also contains the conditional density (and
conditional cdf) for each case in the training data (or test data if
provided) evaluated at each of the unique grow y-values.  The
conditional density can be used to calculate conditional moments, such
as the mean and standard deviation.  Use <code>get.quantile.stat</code> as a
way to conveniently obtain these quantities.
</p>
<p>For multivariate forests, returned values will be a list of length
equal to the number of target outcomes.
</p>


<h3>Author(s)</h3>

<p>Hemant Ishwaran and Udaya B. Kogalur
</p>


<h3>References</h3>

<p>Greenwald M. and Khanna S. (2001).  Space-efficient online computation of
quantile summaries. <em>Proceedings of ACM SIGMOD</em>, 30(2):58-66.
</p>
<p>Meinshausen N. (2006) Quantile regression forests, <em>Journal of
Machine Learning Research</em>, 7:983-999.
</p>
<p>Zhang H., Zimmerman J., Nettleton D. and Nordman D.J. (2019).  Random
forest prediction intervals. <em>The American Statistician</em>. 4:1-5.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rfsrc">rfsrc</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## ------------------------------------------------------------
## regression example
## ------------------------------------------------------------

## standard call
o &lt;- quantreg(mpg ~ ., mtcars)

## extract conditional quantiles
print(get.quantile(o))
print(get.quantile(o, c(.25, .50, .75)))

## extract conditional mean and standard deviation
print(get.quantile.stat(o))

## continuous rank probabiliy score (crps) performance
plot(get.quantile.crps(o), type = "l")


## ------------------------------------------------------------
## train/test regression example
## ------------------------------------------------------------

## train (grow) call followed by test call
o &lt;- quantreg(mpg ~ ., mtcars[1:20,])
o.tst &lt;- quantreg(object = o, newdata = mtcars[-(1:20),])

## extract test set quantiles and conditional statistics
print(get.quantile(o.tst))
print(get.quantile.stat(o.tst))


## ------------------------------------------------------------
## quantile regression for Boston Housing using forest method
## ------------------------------------------------------------

if (library("mlbench", logical.return = TRUE)) {

  ## quantile regression with mse splitting
  data(BostonHousing)
  o &lt;- quantreg(medv ~ ., BostonHousing, method = "forest", nodesize = 1)

  ## continuous rank probabiliy score (crps) 
  plot(get.quantile.crps(o), type = "l")

  ## quantile regression plot
  plot.quantreg(o, .05, .95)
  plot.quantreg(o, .25, .75)

  ## (A) extract 25,50,75 quantiles
  quant.dat &lt;- get.quantile(o, c(.25, .50, .75))

  ## (B) values expected under normality
  quant.stat &lt;- get.quantile.stat(o)
  c.mean &lt;- quant.stat$mean
  c.std &lt;- quant.stat$std
  q.25.est &lt;- c.mean + qnorm(.25) * c.std
  q.75.est &lt;- c.mean + qnorm(.75) * c.std

  ## compare (A) and (B)
  print(head(data.frame(quant.dat[, -2],  q.25.est, q.75.est)))


}

## ------------------------------------------------------------
## multivariate mixed outcomes example
## quantiles are only returned for the continous outcomes
## ------------------------------------------------------------

dta &lt;- mtcars
dta$cyl &lt;- factor(dta$cyl)
dta$carb &lt;- factor(dta$carb, ordered = TRUE)
o &lt;- quantreg(cbind(carb, mpg, cyl, disp) ~., data = dta)

plot.quantreg(o, m.target = "mpg")
plot.quantreg(o, m.target = "disp")

## ------------------------------------------------------------
## multivariate regression example using Mahalanobis splitting
## ------------------------------------------------------------

dta &lt;- mtcars
o &lt;- quantreg(cbind(mpg, disp) ~., data = dta, splitrule = "mahal")

plot.quantreg(o, m.target = "mpg")
plot.quantreg(o, m.target = "disp")

## ------------------------------------------------------------
## example of quantile regression for ordinal data
## ------------------------------------------------------------

## use the wine data for illustration
data(wine, package = "randomForestSRC")

## run quantile regression
o &lt;- quantreg(quality ~ ., wine, ntree = 100)

## extract "probabilities" = density values
qo.dens &lt;- o$quantreg$density
yunq &lt;- o$quantreg$yunq
colnames(qo.dens) &lt;- yunq

## convert y to a factor 
yvar &lt;- factor(cut(o$yvar, c(-1, yunq), labels = yunq)) 
 
## confusion matrix
qo.confusion &lt;- get.confusion(yvar, qo.dens)
print(qo.confusion)

## normalized Brier score
cat("Brier:", 100 * get.brier.error(yvar, qo.dens), "\n")


## ------------------------------------------------------------
## example of large data using Greenwald-Khanna algorithm 
## ------------------------------------------------------------

## load the data and do quick and dirty imputation
data(housing, package = "randomForestSRC")
housing &lt;- impute(SalePrice ~ ., housing,
         ntree = 50, nimpute = 1, splitrule = "random")

## Greenwald-Khanna algorithm 
## request a small number of quantiles 
o &lt;- quantreg(SalePrice ~ ., housing, method = "gk",
        prob = (1:20) / 20, prob.epsilon = 1 / 20, ntree = 250)
plot.quantreg(o)
  
## ------------------------------------------------------------
## using mse splitting with local cdf method for large data
## ------------------------------------------------------------

## load the data and do quick and dirty imputation
data(housing, package = "randomForestSRC")
housing &lt;- impute(SalePrice ~ ., housing,
         ntree = 50, nimpute = 1, splitrule = "random")

## use mse splitting and reduce number of trees
o &lt;- quantreg(SalePrice ~ ., housing, splitrule = "mse", ntree = 250)
plot.quantreg(o)
  


</code></pre>

<hr>
<h2 id='randomForestSRC-package'>
Fast Unified Random Forests for Survival, Regression, and Classification (RF-SRC)
</h2><span id='topic+randomForestSRC-package'></span>

<h3>Description</h3>

<p>Fast OpenMP parallel computing of Breiman random forests (Breiman
2001) for regression, classification, survival analysis (Ishwaran
2008), competing risks (Ishwaran 2012), multivariate (Segal and Xiao
2011), unsupervised (Mantero and Ishwaran 2020), quantile regression
(Meinhausen 2006, Zhang et al. 2019, Greenwald-Khanna 2001), and class
imbalanced q-classification (O'Brien and Ishwaran 2019).  Different
splitting rules invoked under deterministic or random splitting
(Geurts et al. 2006, Ishwaran 2015) are available for all families.
Variable importance (VIMP), and holdout VIMP, as well as confidence
regions (Ishwaran and Lu 2019) can be calculated for single and
grouped variables.  Minimal depth variable selection (Ishwaran et
al. 2010, 2011).  Fast interface for missing data imputation using a
variety of different random forest methods (Tang and Ishwaran 2017).
Visualize trees on your Safari or Google Chrome browser (works for all
families, see <code><a href="#topic+get.tree">get.tree</a></code>).
</p>


<h3>Package Overview</h3>

<p>This package contains many useful functions and users should read the
help file in its entirety for details.  However, we briefly mention
several key functions that may make it easier to navigate and
understand the layout of the package.
</p>

<ol>
<li> <p><code><a href="#topic+rfsrc">rfsrc</a></code>
</p>
<p>This is the main entry point to the package.  It grows a random forest
using user supplied training data.  We refer to the resulting object
as a RF-SRC grow object.  Formally, the resulting object has class
<code>(rfsrc, grow)</code>.
</p>
</li>
<li> <p><code><a href="#topic+rfsrc.fast">rfsrc.fast</a></code>
</p>
<p>A fast implementation of <code>rfsrc</code> using subsampling.
</p>
</li>
<li> <p><code><a href="#topic+quantreg.rfsrc">quantreg.rfsrc</a></code>, <code><a href="#topic+quantreg">quantreg</a></code>
</p>
<p>Univariate and multivariate quantile regression forest for training
and testing.  Different methods available including the
Greenwald-Khanna (2001) algorithm, which is especially suitable for
big data due to its high memory efficiency.
</p>
</li>
<li> <p><code><a href="#topic+predict.rfsrc">predict.rfsrc</a></code>, <code>predict</code>
</p>
<p>Used for prediction.  Predicted values are obtained by dropping the
user supplied test data down the grow forest.  The resulting object
has class <code>(rfsrc, predict)</code>.
</p>
</li>
<li> <p><code><a href="#topic+sidClustering.rfsrc">sidClustering.rfsrc</a></code>, <code>sidClustering</code>
</p>
<p>Clustering of unsupervised data using SID (Staggered Interaction
Data).  Also implements the artificial two-class approach of Breiman
(2003).
</p>
</li>
<li> <p><code><a href="#topic+vimp">vimp</a></code>, <code><a href="#topic+subsample">subsample</a></code>, <code><a href="#topic+holdout.vimp">holdout.vimp</a></code>
</p>
<p>Used for variable selection:
</p>

<ol>
<li> <p><code>vimp</code> calculates variable imporance (VIMP) from a
RF-SRC grow/predict object by noising up the variable (for example
by permutation).  Note that grow/predict calls can always directly
request VIMP.
</p>
</li>
<li> <p><code>subsample</code> calculates VIMP confidence itervals via
subsampling.
</p>
</li>
<li> <p><code>holdout.vimp</code> measures the importance of a variable
when it is removed from the model.
</p>
</li></ol>

</li>
<li> <p><code><a href="#topic+imbalanced.rfsrc">imbalanced.rfsrc</a></code>, <code><a href="#topic+imbalanced">imbalanced</a></code>
</p>
<p>q-classification and G-mean VIMP for class imbalanced data.
</p>
</li>
<li> <p><code><a href="#topic+impute.rfsrc">impute.rfsrc</a></code>, <code><a href="#topic+impute">impute</a></code>
</p>
<p>Fast imputation mode for RF-SRC.  Both <code>rfsrc</code> and
<code>predict.rfsrc</code> are capable of imputing missing data.
However, for users whose only interest is imputing data, this function
provides an efficient and fast interface for doing so.
</p>
</li>
<li> <p><code><a href="#topic+partial.rfsrc">partial.rfsrc</a></code>, <code><a href="#topic+partial">partial</a></code>
</p>
<p>Used to extract the partial effects of a variable or variables on the ensembles.
</p>
</li></ol>



<h3>Home page, Vignettes, Discussions, Bug Reporting, Source Code, Beta Builds</h3>


<ol>
<li><p> The home page for the package, containing vignettes, manuals,
links to GitHub and other useful information is found at
<a href="https://www.randomforestsrc.org/index.html">https://www.randomforestsrc.org/index.html</a>
</p>
</li>
<li><p> Questions, comments, and non-bug related issues may be sent
via <a href="https://github.com/kogalur/randomForestSRC/discussions/">https://github.com/kogalur/randomForestSRC/discussions/</a>.
</p>
</li>
<li><p> Bugs may be reported via <a href="https://github.com/kogalur/randomForestSRC/issues/">https://github.com/kogalur/randomForestSRC/issues/</a>.
This is for bugs only.  Please provide the accompanying information with any reports:
</p>

<ol>
<li> <p><code>sessionInfo()</code>
</p>
</li>
<li><p> A minimal reproducible example consisting of the following items:
</p>

<ul>
<li><p> a minimal dataset, necessary to reproduce the error
</p>
</li>
<li><p> the minimal runnable code necessary to reproduce the error, 
which can be run on the given dataset
</p>
</li>
<li><p> the necessary information on the used packages,
R version and system it is run on
</p>
</li>
<li><p> in the case of random processes, a seed (set by
<code>set.seed()</code>) for reproducibility
</p>
</li></ul>

</li></ol>

</li>
<li><p> Regular stable releases of this package are available on CRAN
at <a href="https://cran.r-project.org/package=randomForestSRC/">https://cran.r-project.org/package=randomForestSRC/</a>
</p>
</li>
<li><p> Interim unstable development builds with bug fixes and
sometimes additional functionality are available at
<a href="https://github.com/kogalur/randomForestSRC/">https://github.com/kogalur/randomForestSRC/</a>
</p>
</li></ol>



<h3>OpenMP Parallel Processing &ndash; Installation</h3>

<p>This package
implements OpenMP shared-memory parallel programming if the
target architecture and operating system support it.  This is the
default mode of execution.
</p>
<p>Additional instructions for configuring OpenMP parallel processing
are available at
<a href="https://www.randomforestsrc.org/articles/installation.html">https://www.randomforestsrc.org/articles/installation.html</a>.
</p>
<p>An understanding of resource utilization (CPU and RAM) is necessary
when running the package using OpenMP and Open MPI parallel execution.
Memory usage is greater when running with OpenMP
enabled.  Diligence should be used not to overtax the hardware available.
</p>


<h3>Reproducibility</h3>

<p>With respect to reproducibility, a model is
defined by a seed, the topology of the trees in the forest, and terminal
node membership of the training data. This allows the user to restore a
model and, in particular, its terminal node statistics.  On the other
hand, VIMP and many other statistics are dependent on additional
randomization, which we do not consider part of the model. These
statistics are susceptible to Monte Carlo effects.</p>


<h3>Author(s)</h3>

<p>Hemant Ishwaran and Udaya B. Kogalur
</p>


<h3>References</h3>

<p>Breiman L. (2001). Random forests, <em>Machine Learning</em>, 45:5-32.
</p>
<p>Geurts, P., Ernst, D. and Wehenkel, L., (2006). Extremely randomized
trees. <em>Machine learning</em>, 63(1):3-42.
</p>
<p>Greenwald M. and Khanna S. (2001).  Space-efficient online computation of
quantile summaries. <em>Proceedings of ACM SIGMOD</em>, 30(2):58-66.
</p>
<p>Ishwaran H. and Kogalur U.B. (2007).  Random survival forests for R,
<em>Rnews</em>, 7(2):25-31.
</p>
<p>Ishwaran H. (2007).  Variable importance in binary regression
trees and forests,  <em>Electronic J. Statist.</em>, 1:519-537.
</p>
<p>Ishwaran H., Kogalur U.B., Blackstone E.H. and Lauer M.S.
(2008).  Random survival forests, <em>Ann. App.
Statist.</em>, 2:841-860.
</p>
<p>Ishwaran H., Kogalur U.B., Gorodeski E.Z, Minn A.J. and
Lauer M.S. (2010).  High-dimensional variable selection for survival
data.  <em>J. Amer. Statist. Assoc.</em>, 105:205-217.
</p>
<p>Ishwaran H., Kogalur U.B., Chen X. and Minn A.J. (2011). Random survival
forests for high-dimensional data. <em>Stat. Anal. Data Mining</em>, 4:115-132
</p>
<p>Ishwaran H., Gerds T.A., Kogalur U.B., Moore R.D., Gange S.J. and Lau
B.M. (2014). Random survival forests for competing risks.
<em>Biostatistics</em>, 15(4):757-773.
</p>
<p>Ishwaran H. and Malley J.D. (2014). Synthetic learning
machines. <em>BioData Mining</em>, 7:28.
</p>
<p>Ishwaran H. (2015).  The effect of splitting on random forests.
<em>Machine Learning</em>, 99:75-118.
</p>
<p>Ishwaran H. and Lu M.  (2019).  Standard errors and confidence
intervals for variable importance in random forest regression,
classification, and survival. <em>Statistics in Medicine</em>, 38,
558-582.
</p>
<p>Lu M., Sadiq S., Feaster D.J. and Ishwaran H. (2018). Estimating
individual treatment effect in observational data using random forest
methods. <em>J. Comp. Graph. Statist</em>, 27(1), 209-219
</p>
<p>Mantero A. and Ishwaran H. (2021).  Unsupervised random forests.
<em>Statistical Analysis and Data Mining</em>, 14(2):144-167.
</p>
<p>Meinshausen N. (2006) Quantile regression forests, <em>Journal of
Machine Learning Research</em>, 7:983-999.
</p>
<p>O'Brien R. and Ishwaran H. (2019).  A random forests quantile
classifier for class imbalanced data. <em>Pattern Recognition</em>,
90, 232-249
</p>
<p>Segal M.R. and Xiao Y. Multivariate random
forests. (2011). <em>Wiley Interdisciplinary Reviews: Data Mining
and Knowledge Discovery</em>. 1(1):80-87.
</p>
<p>Tang F. and Ishwaran H. (2017).  Random forest missing data
algorithms.  <em>Statistical Analysis and Data Mining</em>, 10:363-377.
</p>
<p>Zhang H., Zimmerman J., Nettleton D. and Nordman D.J. (2019).  Random
forest prediction intervals. <em>The American Statistician</em>. 4:1-5.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+find.interaction.rfsrc">find.interaction.rfsrc</a></code>,
</p>
<p><code><a href="#topic+get.tree.rfsrc">get.tree.rfsrc</a></code>,
</p>
<p><code><a href="#topic+holdout.vimp.rfsrc">holdout.vimp.rfsrc</a></code>,
</p>
<p><code><a href="#topic+imbalanced.rfsrc">imbalanced.rfsrc</a></code>,
<code><a href="#topic+impute.rfsrc">impute.rfsrc</a></code>,
</p>
<p><code><a href="#topic+max.subtree.rfsrc">max.subtree.rfsrc</a></code>,
</p>
<p><code><a href="#topic+partial.rfsrc">partial.rfsrc</a></code>,
<code><a href="#topic+plot.competing.risk.rfsrc">plot.competing.risk.rfsrc</a></code>,
<code><a href="#topic+plot.rfsrc">plot.rfsrc</a></code>,
<code><a href="#topic+plot.survival.rfsrc">plot.survival.rfsrc</a></code>,
<code><a href="#topic+plot.variable.rfsrc">plot.variable.rfsrc</a></code>,
<code><a href="#topic+predict.rfsrc">predict.rfsrc</a></code>,
<code><a href="#topic+print.rfsrc">print.rfsrc</a></code>,
</p>
<p><code><a href="#topic+quantreg.rfsrc">quantreg.rfsrc</a></code>,
</p>
<p><code><a href="#topic+rfsrc">rfsrc</a></code>,
<code><a href="#topic+rfsrc.cart">rfsrc.cart</a></code>,
<code><a href="#topic+rfsrc.fast">rfsrc.fast</a></code>,
</p>
<p><code><a href="#topic+sidClustering.rfsrc">sidClustering.rfsrc</a></code>,
</p>
<p><code><a href="#topic+stat.split.rfsrc">stat.split.rfsrc</a></code>,
<code><a href="#topic+subsample.rfsrc">subsample.rfsrc</a></code>,
<code><a href="#topic+synthetic.rfsrc">synthetic.rfsrc</a></code>,
</p>
<p><code><a href="#topic+tune.rfsrc">tune.rfsrc</a></code>,
</p>
<p><code><a href="#topic+var.select.rfsrc">var.select.rfsrc</a></code>,
<code><a href="#topic+vimp.rfsrc">vimp.rfsrc</a></code>
</p>

<hr>
<h2 id='rfsrc'>Fast Unified Random Forests for Survival, Regression, and Classification (RF-SRC)</h2><span id='topic+rfsrc'></span><span id='topic+rfsrc.cart'></span><span id='topic+get.auc'></span><span id='topic+get.bayes.rule'></span><span id='topic+get.brier.error'></span><span id='topic+get.cindex'></span><span id='topic+get.confusion'></span><span id='topic+get.misclass.error'></span><span id='topic+get.mv.cserror'></span><span id='topic+get.mv.csvimp'></span><span id='topic+get.mv.error'></span><span id='topic+get.mv.error.block'></span><span id='topic+get.mv.formula'></span><span id='topic+get.mv.predicted'></span><span id='topic+get.mv.vimp'></span><span id='topic+randomForestSRC'></span>

<h3>Description</h3>

<p>Fast OpenMP parallel computing of random forests (Breiman 2001) for
regression, classification, survival analysis (Ishwaran et al. 2008),
competing risks (Ishwaran et al. 2012), multivariate (Segal and Xiao
2011), unsupervised (Mantero and Ishwaran 2020), quantile regression
(Meinhausen 2006, Zhang et al. 2019, Greenwald-Khanna 2001), and class
imbalanced q-classification (O'Brien and Ishwaran 2019).  Different
splitting rules invoked under deterministic or random splitting
(Geurts et al. 2006, Ishwaran 2015) are available for all families.
Different types of variable importance (VIMP), holdout VIMP, as well
as confidence regions (Ishwaran and Lu 2019) can be calculated for
single and grouped variables.  Minimal depth variable selection
(Ishwaran et al. 2010, 2011).  Fast interface for missing data
imputation using a variety of different random forest methods (Tang
and Ishwaran 2017).
</p>
<p>New items to be aware of:
</p>

<ol>
<li><p> For computational speed, the default VIMP is no longer
&quot;permute&quot; (Breiman-Cutler permutation importance) and has been
switched to &quot;anti&quot; (<code>importance="anti"</code>,
<code>importance=TRUE</code>; see below for details).  Be aware in some
situations, such as highly imbalanced classification, that
permutation VIMP may perform better.  Permutation VIMP is 
obtained using <code>importance="permute"</code>.
</p>
</li>
<li> <p><code>save.memory</code> can be used for big data to save memory;
especially useful for survival and competing risks.
</p>
</li>
<li><p> Mahalanobis splitting for multivariate regression with
correlated y-outcomes (<code>splitrule="mahalanobis"</code>).  Now allows
for a user specified covariance matrix.
</p>
</li>
<li><p> Visualize trees on your Safari or Google Chrome
browser (works for all families).  See <code><a href="#topic+get.tree">get.tree</a></code>.
</p>














</li></ol>

<p>This is the main entry point to the <span class="pkg">randomForestSRC</span>
package.  For more information about this package and OpenMP parallel
processing, use the command <code>package?randomForestSRC</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rfsrc(formula, data, ntree = 500,
  mtry = NULL, ytry = NULL,
  nodesize = NULL, nodedepth = NULL,
  splitrule = NULL, nsplit = NULL,
  importance = c(FALSE, TRUE, "none", "anti", "permute", "random"),
  block.size = if (any(is.element(as.character(importance),
                     c("none", "FALSE")))) NULL else 10,
  bootstrap = c("by.root", "none", "by.user"),
  samptype = c("swor", "swr"), samp = NULL, membership = FALSE,
  sampsize = if (samptype == "swor") function(x){x * .632} else function(x){x},
  na.action = c("na.omit", "na.impute"), nimpute = 1,
  ntime = 150, cause,
  perf.type = NULL,
  proximity = FALSE, distance = FALSE, forest.wt = FALSE,
  xvar.wt = NULL, yvar.wt = NULL, split.wt = NULL, case.wt  = NULL,
  forest = TRUE,
  save.memory = FALSE,
  var.used = c(FALSE, "all.trees", "by.tree"),
  split.depth = c(FALSE, "all.trees", "by.tree"),
  seed = NULL,
  do.trace = FALSE,
  statistics = FALSE,
  ...)

## convenient interface for growing a CART tree
rfsrc.cart(formula, data, ntree = 1, mtry = ncol(data), bootstrap = "none", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rfsrc_+3A_formula">formula</code></td>
<td>
<p>Object of class 'formula' describing the model to fit.
Interaction terms are not supported. If missing, unsupervised
splitting is implemented.</p>
</td></tr>
<tr><td><code id="rfsrc_+3A_data">data</code></td>
<td>
<p>Data frame containing the y-outcome and x-variables.</p>
</td></tr>
<tr><td><code id="rfsrc_+3A_ntree">ntree</code></td>
<td>
<p>Number of trees.</p>
</td></tr>
<tr><td><code id="rfsrc_+3A_mtry">mtry</code></td>
<td>
<p>Number of variables to possibly split at each node.
Default is number of variables divided by 3 for regression.  For all
other families (including unsupervised settings), the square root
of number of variables.  Values are rounded up.</p>
</td></tr>
<tr><td><code id="rfsrc_+3A_ytry">ytry</code></td>
<td>
<p>The number of randomly selected pseudo-outcomes for
unsupervised families (see details below).  Default is
<code>ytry</code>=1.</p>
</td></tr>
<tr><td><code id="rfsrc_+3A_nodesize">nodesize</code></td>
<td>
<p>Minumum size of terminal node.  The defaults are:
survival (15), competing risk (15), regression (5), classification
(1), mixed outcomes (3), unsupervised (3).  It is recommended to
experiment with different <code>nodesize</code> values.</p>
</td></tr>
<tr><td><code id="rfsrc_+3A_nodedepth">nodedepth</code></td>
<td>
<p>Maximum depth to which a tree should be grown.
Parameter is ignored by default.</p>
</td></tr>
<tr><td><code id="rfsrc_+3A_splitrule">splitrule</code></td>
<td>
<p>Splitting rule (see below).</p>
</td></tr>
<tr><td><code id="rfsrc_+3A_nsplit">nsplit</code></td>
<td>
<p>Non-negative integer specifying number of random splits
for splitting a variable.  When zero, all split values are
used (deterministic splitting), which can be slower.  By default 
10 is used.</p>
</td></tr>
<tr><td><code id="rfsrc_+3A_importance">importance</code></td>
<td>
<p>Method for computing variable importance (VIMP); see
below.  Default action is <code>importance="none"</code> but VIMP can
be recovered later using <code>vimp</code> or <code>predict</code>.</p>
</td></tr>
<tr><td><code id="rfsrc_+3A_block.size">block.size</code></td>
<td>
<p>Determines how cumulative error rate is calculated.
When <code>NULL</code>, only done once for entire forest; thus plot of the
cumulative error rate will result in a flat line.  To view the
cumulative error rate on every nth tree, set the value to an integer
between <code>1</code> and <code>ntree</code>.  As an intended side effect, if
importance is requested, VIMP is calculated in &quot;blocks&quot; of size
equal to <code>block.size</code>, thus resulting in a compromise between
ensemble and tree VIMP. The default action in that case is to use 10
trees.</p>
</td></tr>
<tr><td><code id="rfsrc_+3A_bootstrap">bootstrap</code></td>
<td>
<p>Bootstrap protocol.  Default is <code>by.root</code> which
bootstraps the data by sampling with or without replacement (without
replacement is the default; see the option <code>samptype</code> below).
If <code>none</code>, the data is not bootstrapped (it is not possible to
return OOB ensembles or prediction error in this case).  If
<code>by.user</code>, the bootstrap specified by <code>samp</code> is used.</p>
</td></tr>
<tr><td><code id="rfsrc_+3A_samptype">samptype</code></td>
<td>
<p>Type of bootstrap used when <code>by.root</code> is in
effect.  Choices are <code>swor</code> (sampling without replacement; 
the default) and <code>swr</code> (sampling with replacement).</p>
</td></tr>
<tr><td><code id="rfsrc_+3A_samp">samp</code></td>
<td>
<p>Bootstrap specification when <code>by.user</code> is in
effect.  Array of dim <code>n x ntree</code> specifying how many
times each record appears inbag in the bootstrap for each tree.</p>
</td></tr>
<tr><td><code id="rfsrc_+3A_membership">membership</code></td>
<td>
<p>Should terminal node membership and inbag
information be returned?</p>
</td></tr>
<tr><td><code id="rfsrc_+3A_sampsize">sampsize</code></td>
<td>
<p>Function specifying bootstrap size when <code>by.root</code>
is in effect.  For sampling without replacement, it is the requested
size of the sample, which by default is .632 times the sample size.
For sampling with replacement, it is the sample size.  Can also be
specified using a number.</p>
</td></tr>
<tr><td><code id="rfsrc_+3A_na.action">na.action</code></td>
<td>
<p>Action taken if the data contains <code>NA</code>'s.
Possible values are <code>na.omit</code> or <code>na.impute</code>.  The default
<code>na.omit</code> removes the entire record if any entry is
<code>NA</code>. Selecting <code>na.impute</code> imputes the data (see below
for details).  Also see the function <code>impute</code> for fast
imputation.</p>
</td></tr>
<tr><td><code id="rfsrc_+3A_nimpute">nimpute</code></td>
<td>
<p>Number of iterations of the missing data algorithm.
Performance measures such as out-of-bag (OOB) error rates are
optimistic if <code>nimpute</code> is greater than 1.</p>
</td></tr>
<tr><td><code id="rfsrc_+3A_ntime">ntime</code></td>
<td>
<p>Integer value used for survival to constrain ensemble
calculations to an <code>ntime</code> grid of time points over the
observed event times.  Alternatively if a vector of values of length
greater than one is supplied, it is assumed these are the time
points to be used (these will be adjusted to match closest observed
event times).  Setting <code>ntime</code> to zero (or NULL) uses all
observed event times.</p>
</td></tr>
<tr><td><code id="rfsrc_+3A_cause">cause</code></td>
<td>
<p>Integer value between 1 and <code>J</code> indicating the event
of interest for splitting a node for competing risks, where <code>J</code>
is the number of event types. If not specified, the default is to
use a composite splitting rule that averages over all event types.
Can also be a vector of non-negative weights of length <code>J</code>
specifying weights for each event (for example, a vector of ones
reverts to the default composite split statistic).  Regardless of
how <code>cause</code> is specified, estimates for all event types are
returned.</p>
</td></tr>
<tr><td><code id="rfsrc_+3A_perf.type">perf.type</code></td>
<td>
<p>Optional character value specifying metric used
for predicted value, variable importance (VIMP), and error rate.
Reverts to the family default metric if not specified.
<code>perf.type="none"</code> turns off performance entirely which is a
useful way to turn off C-index calculations for big survival data
(which can be expensive).  Values allowed for
univariate/multivariate classification are:
<code>perf.type="misclass"</code> (default), <code>perf.type="brier"</code> and
<code>perf.type="gmean"</code>.</p>
</td></tr>
<tr><td><code id="rfsrc_+3A_proximity">proximity</code></td>
<td>
<p>Proximity of cases as measured by the frequency of
sharing the same terminal node.  This is an <code>n</code>x<code>n</code>
matrix, which can be large.  Choices are <code>inbag</code>, <code>oob</code>,
<code>all</code>, <code>TRUE</code>, or <code>FALSE</code>.  Setting <code>proximity =
      TRUE</code> is equivalent to <code>proximity = "inbag"</code>.</p>
</td></tr>
<tr><td><code id="rfsrc_+3A_distance">distance</code></td>
<td>
<p>Distance between cases as measured by the ratio of the
sum of the count of edges from each case to their immediate common
ancestor node to the sum of the count of edges from each case to the
root node.  If the cases are co-terminal for a tree, this measure is
zero and reduces to 1 - the proximity measure.  This is an
<code>n</code>x<code>n</code> matrix, which can be large.  Choices are
<code>inbag</code>, <code>oob</code>, <code>all</code>, <code>TRUE</code>, or <code>FALSE</code>.
Setting <code>distance = TRUE</code> is equivalent to <code>distance =
    "inbag"</code>.</p>
</td></tr>
<tr><td><code id="rfsrc_+3A_forest.wt">forest.wt</code></td>
<td>
<p>Calculate the forest weight matrix?  Creates an
<code>n</code>x<code>n</code> matrix which can be used for prediction and
constructing customized estimators.  Choices are similar to
proximity: <code>inbag</code>, <code>oob</code>, <code>all</code>, <code>TRUE</code>, or
<code>FALSE</code>.  The default is <code>TRUE</code> which is equivalent to
<code>inbag</code>.</p>
</td></tr>
<tr><td><code id="rfsrc_+3A_xvar.wt">xvar.wt</code></td>
<td>
<p>Vector of non-negative weights (does not have to sum
to 1) representing the probability of selecting a variable for
splitting.  Default is uniform weights.</p>
</td></tr>
<tr><td><code id="rfsrc_+3A_yvar.wt">yvar.wt</code></td>
<td>
<p>Used for sending in features with custom splitting.
For expert use only.</p>
</td></tr>
<tr><td><code id="rfsrc_+3A_split.wt">split.wt</code></td>
<td>
<p>Vector of non-negative weights used for multiplying
the split statistic for a variable. A large value encourages the
node to split on a specific variable. Default is uniform
weights.</p>
</td></tr>
<tr><td><code id="rfsrc_+3A_case.wt">case.wt</code></td>
<td>
<p>Vector of non-negative weights (does not have to sum to
1) for sampling cases.  Observations with larger weights will be
selected with higher probability in the bootstrap (or subsampled)
samples.  It is generally better to use real weights rather than
integers. See the breast data example below illustrating its use
for class imbalanced data.</p>
</td></tr>
<tr><td><code id="rfsrc_+3A_forest">forest</code></td>
<td>
<p>Save key forest values?  Used for prediction on new data
and required by many of the package functions. Turn this off if you
are only interested in training a forest.</p>
</td></tr>
<tr><td><code id="rfsrc_+3A_save.memory">save.memory</code></td>
<td>
<p>Save memory?  Default is to store terminal node
quantities used for prediction on test data.  This yields rapid
prediction but can be memory intensive for big data, especially
competing risks and survival models.  Turn this flag off in those
cases.</p>
</td></tr>
<tr><td><code id="rfsrc_+3A_var.used">var.used</code></td>
<td>
<p>Return statistics on number of times a variable split?
Default is <code>FALSE</code>.  Possible values are <code>all.trees</code> which
returns total number of splits of each variable, and <code>by.tree</code>
which returns a matrix of number a splits for each variable for each
tree.</p>
</td></tr>
<tr><td><code id="rfsrc_+3A_split.depth">split.depth</code></td>
<td>
<p>Records the minimal depth for each variable.
Default is <code>FALSE</code>.  Possible values are <code>all.trees</code> which
returns a matrix of the average minimal depth for a variable
(columns) for a specific case (rows), and <code>by.tree</code> which
returns a three-dimensional array recording minimal depth for a
specific case (first dimension) for a variable (second dimension)
for a specific tree (third dimension).</p>
</td></tr>
<tr><td><code id="rfsrc_+3A_seed">seed</code></td>
<td>
<p>Negative integer specifying seed for the random number
generator.</p>
</td></tr>
<tr><td><code id="rfsrc_+3A_do.trace">do.trace</code></td>
<td>
<p>Number of seconds between updates to the user on
approximate time to completion.</p>
</td></tr>
<tr><td><code id="rfsrc_+3A_statistics">statistics</code></td>
<td>
<p>Should split statistics be returned?  Values can be
parsed using <code>stat.split</code>.</p>
</td></tr>
<tr><td><code id="rfsrc_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>


<ol>
<li> <p><em>Types of forests</em>
</p>
<p>There is no need to set the type of forest as the package
automagically determines the underlying random forest requested from
the type of outcome and the formula supplied.  There are
several possible scenarios:
</p>

<ol>
<li><p> Regression forests for continuous outcomes.
</p>
</li>
<li><p> Classification forests for factor outcomes.
</p>
</li>
<li><p> Multivariate forests for continuous and/or factor outcomes
and for mixed (both type) of outcomes.	    
</p>
</li>
<li><p> Unsupervised forests when there is no outcome.
</p>
</li>
<li><p> Survival forests for right-censored survival.
</p>
</li>
<li><p> Competing risk survival forests for competing risk.
</p>
</li></ol>

</li>
<li> <p><em>Splitting</em>
</p>

<ol>
<li><p> Splitting rules are specified by the option <code>splitrule</code>.
</p>
</li>
<li><p> For all families, pure random splitting can be invoked by setting
<code>splitrule="random"</code>.
</p>
</li>
<li><p> For all families, computational speed can be increased using
randomized splitting invoked by the option <code>nsplit</code>.
See Improving Computational Speed.
</p>
</li></ol>

</li>
<li> <p><em>Available splitting rules</em>
</p>

<ul>
<li><p> Regression analysis:
</p>

<ol>
<li> <p><code>splitrule="mse"</code> (default split rule):  weighted
mean-squared error splitting (Breiman et al. 1984, Chapter 8.4).
</p>
</li>
<li> <p><code>splitrule="quantile.regr"</code>: quantile regression splitting via
the &quot;check-loss&quot; function.  Requires specifying the target
quantiles.  See <code>quantreg.rfsrc</code> for further details.
</p>
</li>
<li> <p><code>la.quantile.regr</code>: local adaptive quantile
regression splitting. See <code>quantreg.rfsrc</code>.
</p>
</li></ol>

</li>
<li><p> Classification analysis:
</p>

<ol>
<li> <p><code>splitrule="gini"</code> (default splitrule): Gini
index splitting (Breiman et al. 1984, Chapter 4.3).
</p>
</li>
<li> <p><code>splitrule="auc"</code>: AUC (area under the ROC curve) splitting
for both two-class and multiclass setttings.  AUC splitting is
appropriate for imbalanced data.  See <code>imbalanced</code> for
more information.
</p>
</li>
<li> <p><code>splitrule="entropy"</code>: entropy splitting (Breiman et
al. 1984, Chapter 2.5, 4.3).
</p>
</li></ol>

</li>
<li><p> Survival analysis:
</p>

<ol>
<li> <p><code>splitrule="logrank"</code> (default splitrule):
log-rank splitting (Segal, 1988; Leblanc and Crowley, 1993).
</p>
</li>
<li> <p><code>splitrule="bs.gradient"</code>: gradient-based (global non-quantile)
brier score splitting.  The time horizon used for the Brier
score is set to the 90th percentile of the observed event times.
This can be over-ridden by the option <code>prob</code>, which must be
a value between 0 and 1 (set to .90 by default).
</p>
</li>
<li> <p><code>splitrule="logrankscore"</code>:  log-rank score splitting (Hothorn and
Lausen, 2003).
</p>
</li></ol>

</li>
<li><p> Competing risk analysis (for details see Ishwaran et al., 2014):
</p>

<ol>
<li> <p><code>splitrule="logrankCR"</code> (default splitrule): modified
weighted log-rank splitting rule modeled after Gray's test
(Gray, 1988).  Use this to find *all* variables 
that are informative and when the goal is long term
prediction. 
</p>
</li>
<li> <p><code>splitrule="logrank"</code>: weighted log-rank splitting where each
event type is treated as the event of interest and all
other events are treated as censored.  The split rule is
the weighted value of each of log-rank statistics,
standardized by the variance.  Use this to find variables
that affect a *specific* cause of interest and when
the goal is a targeted analysis of a specific cause.
However in order for this to be effective, remember to set
the <code>cause</code> option to the targeted cause of interest.
See examples below.
</p>
</li></ol>

</li>
<li><p> Multivariate analysis:
</p>

<ol>
<li><p> Default is the multivariate normalized composite split
rule using mean-squared error and Gini index (Tang and
Ishwaran, 2017).
</p>
</li>
<li> <p><code>splitrule="mahalanobis"</code>: Mahalanobis splitting
that adjusts for correlation (also allows for a user specified
covariance matrix, see example below).  Only works for
multivariate regression (all outcomes must be real).
</p>
</li></ol>

</li>
<li><p> Unsupervised analysis: In settings where there is no
outcome, unsupervised splitting that uses pseudo-outcomes is
applied using the default multivariate splitting rule (see
below for details) Also see <code>sidClustering</code> for a
more sophisticated method for unsupervised analysis (Mantero
and Ishwaran, 2020).
</p>
</li>
<li><p> Custom splitting: All families except unsupervised are
available for user defined custom splitting.  Some basic
C-programming skills are required.  The harness for defining
these rules is in <code>splitCustom.c</code>.  In this file we
give examples of how to code rules for regression,
classification, survival, and competing risk.  Each family
can support up to sixteen custom split rules.  Specifying
<code>splitrule="custom"</code> or <code>splitrule="custom1"</code> will
trigger the first split rule for the family defined by the
training data set. Multivariate families will need a custom
split rule for both regression and classification.  In the
examples, we demonstrate how the user is presented with the
node specific membership.  The task is then to define a
split statistic based on that membership.  Take note of the
instructions in <code>splitCustom.c</code> on how to
<em>register</em> the custom split rules.  It is suggested
that the existing custom split rules be kept in place for
reference and that the user proceed to develop
<code>splitrule="custom2"</code> and so on. The package must be
recompiled and installed for the custom split rules to
become available.
</p>
</li></ul>

</li>
<li> <p><em>Improving computational speed</em>
</p>
<p>See the function <code><a href="#topic+rfsrc.fast">rfsrc.fast</a></code> for a fast
implementation of <code>rfsrc</code>.  Key methods for
increasing speed are as follows:
</p>

<ul>
<li> <p><em>Nodesize</em>
</p>
<p>Increasing <code>nodesize</code> has the greatest effect in speeding
calculations.  In some big data settings this can also lead to
better prediction performance.
</p>
</li>
<li> <p><em>Save memory</em>
</p>
<p>Use option <code>save.memory="TRUE"</code> for big data competing risk
and survival models.  By default the package stores terminal
node quantities to be used in prediction for test data but this
can be memory intensive for big data.
</p>
</li>
<li> <p><em>Block size</em>
</p>
<p>Make sure <code>block.size="NULL"</code> (or set to number of trees)
so that the cumulative error is calculated only once.
</p>
</li>
<li> <p><em>Turn off performace</em>
</p>
<p>The C-index error rate calculation can be very expensive for big
survival data.  Set <code>perf.type="none"</code> to turn this off and
all other performance calculations (then consider using the
function <code>get.brier.survival</code> as a fast way to get survival
performance).
</p>
<p><code>perf.type="none"</code> applies to all other families as well.
</p>
</li>
<li> <p><em>Randomized splitting rules</em>
</p>
<p>Set <code>nsplit</code> to a small non-zero integer value.  Then a
maximum of <code>nsplit</code> split points are chosen randomly for
each of the candidate splitting variables when splitting a tree
node, thus significantly reducing computational costs.
</p>
<p>For more details about randomized splitting see Loh and Shih
(1997), Dietterich (2000), and Lin and Jeon (2006).  Geurts et
al. (2006) introduced extremely randomized trees using the
extra-trees algorithm.  This algorithm corresponds to
<code>nsplit</code>=1.  In our experience however this may be too low
for general use (Ishwaran, 2015).
</p>
<p>For completely randomized (pure random) splitting use
<code>splitrule="random"</code>.  In pure splitting, nodes are split
by randomly selecting a variable and randomly selecting its
split point (Cutler and Zhao, 2001).
</p>
</li>
<li> <p><em>Subsampling</em>
</p>
<p>Reduce the size of the bootstrap using <code>sampsize</code> and
<code>samptype</code>.  See <code>rfsrc.fast</code> for a fast forest
implementation using subsampling.
</p>
</li>
<li> <p><em>Unique time points</em>
</p>
<p>Setting <code>ntime</code> to a reasonably small value such as 50
constrains survival ensemble calculations to a restricted grid
of time points and significantly improves computational times.
</p>
</li>
<li> <p><em>Large number of variables</em>
</p>
<p>Try filtering variables ahead of time.  Make sure not to request
VIMP (variable importance can always be recovered later using
<code>vimp</code> or <code>predict</code>).  Also if variable
selection is desired, but is too slow, consider using
<code>max.subtree</code> which calculates minimal depth, a measure
of the depth that a variable splits, and yields fast variable
selection (Ishwaran, 2010).
</p>
</li></ul>

</li>
<li> <p><em>Prediction Error</em>
</p>
<p>Prediction error is calculated using OOB data.  The metric used is
mean-squared-error for regression, and misclassification error for
classification.  A normalized Brier score (relative to a coin-toss)
and the AUC (area under the ROC curve) is also provided upon
printing a classification forest.  Performance for Brier score can
be specified using <code>perf.type="brier"</code>.  G-mean performance is
also available, see the function <code>imbalanced</code> for more
details.
</p>
<p>For survival, prediction error is measured by 1-C, where C is
Harrell's (Harrell et al., 1982) concordance index.  Prediction error
is between 0 and 1, and measures how well the predictor correctly
ranks (classifies) two random individuals in terms of survival.  A
value of 0.5 is no better than random guessing. A value of 0 is
perfect.
</p>
<p>When bootstrapping is by <code>none</code>, a coherent OOB subset is not
available to assess prediction error.  Thus, all outputs dependent
on this are suppressed.  In such cases, prediction error is only
available via classical cross-validation (the user will need to use
the <code>predict.rfsrc</code> function).
</p>
</li>
<li> <p><em>Variable Importance (VIMP)</em>
</p>
<p>VIMP is calculated using OOB data in several ways.
<code>importance="permute"</code> yields permutation VIMP (Breiman-Cutler
importance) by permuting OOB cases. <code>importance="random"</code> uses
random left/right assignments whenever a split is encountered for
the target variable.  The default <code>importance="anti"</code>
(equivalent to <code>importance=TRUE</code>) assigns cases to the anti
(opposite) split.
</p>
<p>VIMP depends upon <code>block.size</code>, an integer value between 1 and
<code>ntree</code>, specifying number of trees in a block used for VIMP.
When <code>block.size</code>=1, VIMP is calculated for each tree.  When
<code>block.size="ntree"</code>, VIMP is calculated for the entire forest
by comparing the perturbed OOB forest ensemble (using all trees) to
the unperturbed OOB forest ensemble (using all trees).  This yields
ensemble VIMP, which does not measure the tree average effect of a
variable, but rather its overall forest effect.  
</p>
<p>A useful compromise between tree VIMP and ensemble VIMP can be
obtained by setting <code>block.size</code> to a value between 1 and
<code>ntree</code>.  Smaller values generally gives better accuracy,
however computational times will be higher because VIMP is
calculated over more blocks.  However, see <code>imbalanced</code> for
imbalanced classification data where larger <code>block.size</code>
often works better (O'Brien and Ishwaran, 2019).
</p>
<p>See <code>vimp</code> for a user interface for extracting VIMP and
<code>subsampling</code> for calculating confidence intervals for VIMP.
</p>
<p>Also see <code>holdout.vimp</code> for holdout VIMP, which calculates
importance by holding out variables.  This is more conservative, but
with good false discovery properties.
</p>
<p>For classification, VIMP is returned as a matrix with J+1
columns where J is the number of classes.  The first column &quot;all&quot; is
the unconditional VIMP, while the remaining columns are conditional VIMP
calculated using only OOB cases with the class label.
</p>
</li>
<li> <p><em>Multivariate Forests</em>
</p>
<p>Multivariate forests can be specified in two ways:
</p>
<p>rfsrc(Multivar(y1, y2, ..., yd) ~ . , my.data, ...)
</p>
<p>rfsrc(cbind(y1, y2, ..., yd) ~ . , my.data, ...)
</p>
<p>By default, a multivariate normalized composite splitting rule is
used to split nodes (for multivariate regression, users have
the option to use Mahalanobis splitting).
</p>
<p>The nature of the outcomes informs the code as to what type of
multivariate forest is grown; i.e. whether it is real-valued,
categorical, or a combination of both (mixed). Performance measures
(when requested) are returned for all outcomes.
</p>
<p>Helper functions <code>get.mv.formula</code>,
<code>get.mv.predicted</code>, <code>get.mv.error</code> can be used for
defining the multivariate forest formula and extracting predicted
values (all outcomes) and VIMP (all variables, all outcomes;
assuming importance was requested in the call).  The latter two
functions also work for univariate (regular) forests.  Both
functions return standardized values (dividing by the variance for
regression, or multiplying by 100, otherwise) using option
<code>standardize="TRUE"</code>.
</p>
</li>
<li> <p><em>Unsupervised Forests and sidClustering</em>
</p>
<p>See sidClustering <code>sidClustering</code> for a more sophisticated
method for unsupervised analysis.
</p>
<p>Otherwise a more direct (but naive) way to proceed is to use the
unsupervised splitting rule.  The following are equivalent ways to
grow an unsupervised forest via unsupervised splitting:
</p>
<p>rfsrc(data = my.data)
</p>
<p>rfsrc(Unsupervised() ~ ., data = my.data)
</p>
<p>In unsupervised mode, features take turns acting as target
y-outcomes and x-variables for splitting.  Specifically, <code>mtry</code>
x-variables are randomly selected for splitting the node.  Then for
each <code>mtry</code> feature, <code>ytry</code> variables are selected
from the remaining features to act as the target pseduo-outcomes.
Splitting uses the multivariate normalized composite splitting rule.
</p>
<p>The default value of <code>ytry</code> is 1 but can be increased.  As
illustration, the following equivalent unsupervised calls set
<code>mtry=10</code> and <code>ytry=5</code>:
</p>
<p>rfsrc(data = my.data, ytry = 5, mtry = 10)
</p>
<p>rfsrc(Unsupervised(5) ~ ., my.data, mtry = 10)
</p>
<p>Note that all performance values (error rates, VIMP, prediction) are
turned off in unsupervised mode.
</p>
</li>
<li> <p><em>Survival, Competing Risks</em>
</p>

<ol>
<li><p> Survival settings require a time and censoring variable which
should be identifed in the formula as the outcome using the standard
<code>Surv</code> formula specification.  A typical formula call looks like:
</p>
<p>Surv(my.time, my.status) ~ .
</p>
<p>where <code>my.time</code> and <code>my.status</code> are the variables names for
the event time and status variable in the users data set.
</p>
</li>
<li><p> For survival forests (Ishwaran et al. 2008), the censoring
variable must be coded as a non-negative integer with 0 reserved for
censoring and (usually) 1=death (event). 
</p>
</li>
<li><p> For competing risk forests (Ishwaran et al., 2013), the
implementation is similar to survival, but with the following
caveats:
</p>

<ul>
<li><p> Censoring must be coded as a non-negative integer, where 0
indicates right-censoring, and non-zero values indicate different
event types.  While <code>0,1,2,..,J</code> is standard, and recommended,
events can be coded non-sequentially, although 0 must always be used
for censoring.
</p>
</li>
<li><p> Setting the splitting rule to <code>logrankscore</code> will result
in a survival analysis in which all events are treated as if they
are the same type (indeed, they will coerced as such).
</p>
</li>
<li><p> Generally, competing risks requires a larger <code>nodesize</code> than
survival settings.
</p>
</li></ul>

</li></ol>

</li>
<li> <p><em>Missing data imputation</em>
</p>
<p><code>na.action="na.impute"</code> imputes missing data (both x and
y-variables) using the missing data algorithm of Ishwaran et
al. (2008).  But also see the <code>impute</code> for an
alternate way to do fast and accurate imputation.
</p>
<p>The missing data algorithm can be iterated by setting <code>nimpute</code>
to a positive integer greater than 1.  When iterated, at the
completion of each iteration, missing data is imputed using OOB
non-missing terminal node data which is then used as input to grow a
new forest.  A side effect of iteration is that missing values in
the returned objects <code>xvar</code>, <code>yvar</code> are replaced by
imputed values.  In other words the incoming data is overlaid with
the missing data.  Also, performance measures such as error rates
and VIMP become optimistically biased.
</p>
<p>Records in which all outcome and x-variable information are missing
are removed from the forest analysis.  Variables having all missing
values are also removed.
</p>
</li>
<li> <p><em>Allowable data types and factors</em>
</p>
<p>Data types must be real valued, integer, factor or logical &ndash;
however all except factors are coerced and treated as if real
valued.  For ordered x-variable factors, splits are similar to real
valued variables.  For unordered factors, a split will move a subset
of the levels in the parent node to the left daughter, and the
complementary subset to the right daughter.  All possible
complementary pairs are considered and apply to factors with an
unlimited number of levels.  However, there is an optimization check
to ensure number of splits attempted is not greater than number of
cases in a node or the value of <code>nsplit</code>.
</p>
<p>For coherence, an immutable map is applied to each factor that
ensures factor levels in the training data are consistent with the
factor levels in any subsequent test data.  This map is applied to
each factor before and after the native C library is executed.
Because of this, if all x-variables all factors, then computational
time will be long in high dimensional problems.  Consider converting
factors to real if this is the case.
</p>
</li></ol>


<h3>Value</h3>

<p>An object of class <code>(rfsrc, grow)</code> with the following
components:
</p>
<table>
<tr><td><code>call</code></td>
<td>
<p>The original call to <code>rfsrc</code>.</p>
</td></tr>
<tr><td><code>family</code></td>
<td>
<p>The family used in the analysis.</p>
</td></tr>
<tr><td><code>n</code></td>
<td>
<p>Sample size of the data (depends upon <code>NA</code>'s, see <code>na.action</code>).</p>
</td></tr>
<tr><td><code>ntree</code></td>
<td>
<p>Number of trees grown.</p>
</td></tr>
<tr><td><code>mtry</code></td>
<td>
<p>Number of variables randomly selected for splitting at each node.</p>
</td></tr>
<tr><td><code>nodesize</code></td>
<td>
<p>Minimum size of terminal nodes.</p>
</td></tr>
<tr><td><code>nodedepth</code></td>
<td>
<p>Maximum depth allowed for a tree.</p>
</td></tr>
<tr><td><code>splitrule</code></td>
<td>
<p>Splitting rule used.</p>
</td></tr>
<tr><td><code>nsplit</code></td>
<td>
<p>Number of randomly selected split points.</p>
</td></tr>
<tr><td><code>yvar</code></td>
<td>
<p>y-outcome values.</p>
</td></tr>
<tr><td><code>yvar.names</code></td>
<td>
<p>A character vector of the y-outcome names.</p>
</td></tr>
<tr><td><code>xvar</code></td>
<td>
<p>Data frame of x-variables.</p>
</td></tr>
<tr><td><code>xvar.names</code></td>
<td>
<p>A character vector of the x-variable names.</p>
</td></tr>
<tr><td><code>xvar.wt</code></td>
<td>
<p>Vector of non-negative weights specifying the
probability used to select a variable for splitting a node.</p>
</td></tr>
<tr><td><code>split.wt</code></td>
<td>
<p>Vector of non-negative weights specifying
multiplier by which the split statistic for a covariate is adjusted.</p>
</td></tr>
<tr><td><code>cause.wt</code></td>
<td>
<p>Vector of weights used for the composite competing
risk splitting rule.</p>
</td></tr>
<tr><td><code>leaf.count</code></td>
<td>
<p>Number of terminal nodes for each tree in the
forest. Vector of length <code>ntree</code>.  A value of zero indicates
a rejected tree (can occur when imputing missing data).
Values of one indicate tree stumps.</p>
</td></tr>
<tr><td><code>proximity</code></td>
<td>
<p>Proximity matrix recording the frequency of pairs of data points
occur within the same terminal node.</p>
</td></tr>
<tr><td><code>forest</code></td>
<td>
<p>If <code>forest=TRUE</code>, the forest object is returned.
This object is used for prediction with new test data
sets and is required for other R-wrappers.</p>
</td></tr>
<tr><td><code>forest.wt</code></td>
<td>
<p>Forest weight matrix.</p>
</td></tr>
<tr><td><code>membership</code></td>
<td>
<p>Matrix recording terminal node membership where
each column records node mebership for a case for a tree (rows).</p>
</td></tr>
<tr><td><code>splitrule</code></td>
<td>
<p>Splitting rule used.</p>
</td></tr>
<tr><td><code>inbag</code></td>
<td>
<p>Matrix recording inbag membership where each column
contains the number of times that a case appears in the bootstrap
sample for a tree (rows).</p>
</td></tr>
<tr><td><code>var.used</code></td>
<td>
<p>Count of the number of times a variable is used in
growing the forest.</p>
</td></tr>
<tr><td><code>imputed.indv</code></td>
<td>
<p>Vector of indices for cases with missing
values.</p>
</td></tr>
<tr><td><code>imputed.data</code></td>
<td>
<p>Data frame of the imputed data. The first
column(s) are reserved for the y-outcomes, after which the
x-variables are listed.</p>
</td></tr>
<tr><td><code>split.depth</code></td>
<td>
<p>Matrix (i,j) or array (i,j,k) recording the
minimal depth for variable j for case i, either averaged over
the forest, or by tree k.</p>
</td></tr>
<tr><td><code>node.stats</code></td>
<td>
<p>Split statistics returned when
<code>statistics=TRUE</code> which can be parsed using <code>stat.split</code>.</p>
</td></tr>
<tr><td><code>err.rate</code></td>
<td>
<p>Tree cumulative OOB error rate.</p>
</td></tr>
<tr><td><code>err.block.rate</code></td>
<td>
<p>When <code>importance=TRUE</code>, vector of the
cumulative error rate for each ensemble block comprised of
<code>block.size</code> trees.  So with <code>block.size = 10</code>, entries are
the cumulative error rate for the first 10 trees, the first 20 trees,
30 trees, and so on.  As another exmple, if <code>block.size = 1</code>,
entries are the error rate for each tree.</p>
</td></tr>
<tr><td><code>importance</code></td>
<td>
<p>Variable importance (VIMP) for each x-variable.</p>
</td></tr>
<tr><td><code>predicted</code></td>
<td>
<p>In-bag predicted value.</p>
</td></tr>
<tr><td><code>predicted.oob</code></td>
<td>
<p>OOB predicted value.</p>
</td></tr></table>
<p><br />
</p>
<table>
<tr><td><code>++++++++</code></td>
<td>
<p>for classification settings, additionally ++++++++</p>
</td></tr> </table>
<p><br />
</p>
<table>
<tr><td><code>class</code></td>
<td>
<p>In-bag predicted class labels.</p>
</td></tr>
<tr><td><code>class.oob</code></td>
<td>
<p>OOB predicted class labels.</p>
</td></tr></table>
<p><br />
</p>
<table>
<tr><td><code>++++++++</code></td>
<td>
<p>for multivariate settings, additionally ++++++++</p>
</td></tr> </table>
<p><br />
</p>
<table>
<tr><td><code>regrOutput</code></td>
<td>
<p>List containing performance values for
multivariate regression outcomes (applies only in
multivariate settings).</p>
</td></tr>
<tr><td><code>clasOutput</code></td>
<td>
<p>List containing performance values for
multivariate categorical (factor) outcomes (applies only in
multivariate settings).</p>
</td></tr></table>
<p><br />
</p>
<table>
<tr><td><code>++++++++</code></td>
<td>
<p>for survival settings, additionally ++++++++</p>
</td></tr> </table>
<p><br />
</p>
<table>
<tr><td><code>survival</code></td>
<td>
<p>In-bag survival function.</p>
</td></tr>
<tr><td><code>survival.oob</code></td>
<td>
<p>OOB survival function.</p>
</td></tr>
<tr><td><code>chf</code></td>
<td>
<p>In-bag cumulative hazard function (CHF).</p>
</td></tr>
<tr><td><code>chf.oob</code></td>
<td>
<p>OOB CHF.</p>
</td></tr>
<tr><td><code>time.interest</code></td>
<td>
<p>Ordered unique death times.</p>
</td></tr>
<tr><td><code>ndead</code></td>
<td>
<p>Number of deaths.</p>
</td></tr></table>
<p><br />
</p>
<table>
<tr><td><code>++++++++</code></td>
<td>
<p>for competing risks, additionally ++++++++</p>
</td></tr> </table>
<p><br />
</p>
<table>
<tr><td><code>chf</code></td>
<td>
<p>In-bag cause-specific cumulative hazard function (CSCHF)
for each event.</p>
</td></tr>
<tr><td><code>chf.oob</code></td>
<td>
<p>OOB CSCHF.</p>
</td></tr>
<tr><td><code>cif</code></td>
<td>
<p>In-bag cumulative incidence function (CIF) for each event.</p>
</td></tr>
<tr><td><code>cif.oob</code></td>
<td>
<p>OOB CIF.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Values returned depend heavily on the family.  In particular,
predicted values from the forest (<code>predicted</code> and
<code>predicted.oob</code>) are as follows:
</p>

<ol>
<li><p> For regression, a vector of predicted y-outcomes.
</p>
</li>
<li><p> For classification, a matrix with columns containing the
estimated class probability for each class.  Performance values
and VIMP for classification are reported as a matrix with J+1
columns where J is the number of classes.  The first column &quot;all&quot;
is the unconditional value for performance (VIMP), while the
remaining columns are performance (VIMP) conditioned on cases
corresponding to that class label.
</p>
</li>
<li><p> For survival, a vector of mortality values (Ishwaran et al.,
2008) representing estimated risk for each individual calibrated
to the scale of the number of events (as a specific example, if
<em>i</em> has a mortality value of 100, then if all individuals had
the same x-values as <em>i</em>, we would expect an average of 100
events).  Also returned are matrices containing
the CHF and survival function.  Each row corresponds to an
individual's ensemble CHF or survival function evaluated at each
time point in <code>time.interest</code>.  
</p>
</li>
<li><p> For competing risks, a matrix with one column for each event
recording the expected number of life years lost due to the event
specific cause up to the maximum follow up (Ishwaran et al.,
2013).  Also returned are the cause-specific cumulative hazard
function (CSCHF) and the cumulative incidence function (CIF) for
each event type.  These are encoded as a three-dimensional array,
with the third dimension used for the event type, each time point
in <code>time.interest</code> making up the second dimension (columns),
and the case (individual) being the first dimension (rows).
</p>
</li>
<li><p> For multivariate families, predicted values (and other
performance values such as VIMP and error rates) are stored in the
lists <code>regrOutput</code> and <code>clasOutput</code> which can be
extracted using functions <code>get.mv.error</code>,
<code>get.mv.predicted</code> and <code>get.mv.vimp</code>.
</p>
</li></ol>



<h3>Author(s)</h3>

<p>Hemant Ishwaran and Udaya B. Kogalur
</p>


<h3>References</h3>

<p>Breiman L., Friedman J.H., Olshen R.A. and Stone C.J. (1984).
<em>Classification and Regression Trees</em>, Belmont, California.
</p>
<p>Breiman L. (2001). Random forests, <em>Machine Learning</em>, 45:5-32.
</p>
<p>Cutler A. and Zhao G. (2001). PERT-Perfect random tree ensembles.
<em>Comp. Sci. Statist.</em>, 33: 490-497.
</p>
<p>Dietterich, T. G. (2000). An experimental comparison of three methods for
constructing ensembles of decision trees: bagging, boosting, and randomization.
<em>Machine Learning</em>, 40, 139-157.
</p>
<p>Gray R.J. (1988).  A class of k-sample tests for comparing the
cumulative incidence of a competing risk, <em>Ann. Statist.</em>,
16: 1141-1154.
</p>
<p>Geurts, P., Ernst, D. and Wehenkel, L., (2006). Extremely randomized
trees. <em>Machine learning</em>, 63(1):3-42.
</p>
<p>Greenwald M. and Khanna S. (2001).  Space-efficient online computation of
quantile summaries. <em>Proceedings of ACM SIGMOD</em>, 30(2):58-66.
</p>
<p>Harrell et al. F.E. (1982).  Evaluating the yield of medical tests,
<em>J. Amer. Med. Assoc.</em>, 247:2543-2546.
</p>
<p>Hothorn T. and Lausen B. (2003). On the exact distribution of maximally selected
rank statistics, <em>Comp. Statist. Data Anal.</em>, 43:121-137.
</p>
<p>Ishwaran H. (2007).  Variable importance in binary regression
trees and forests,  <em>Electronic J. Statist.</em>, 1:519-537.
</p>
<p>Ishwaran H. and Kogalur U.B. (2007).  Random survival forests for R,
<em>Rnews</em>, 7(2):25-31.
</p>
<p>Ishwaran H., Kogalur U.B., Blackstone E.H. and Lauer M.S.
(2008).  Random survival forests, <em>Ann. App.
Statist.</em>, 2:841-860.
</p>
<p>Ishwaran H., Kogalur U.B., Gorodeski E.Z, Minn A.J. and
Lauer M.S. (2010).  High-dimensional variable selection for survival
data.  <em>J. Amer. Statist. Assoc.</em>, 105:205-217.
</p>
<p>Ishwaran H., Kogalur U.B., Chen X. and Minn A.J. (2011). Random survival
forests for high-dimensional data. <em>Stat. Anal. Data Mining</em>, 4:115-132
</p>
<p>Ishwaran H., Gerds T.A., Kogalur U.B., Moore R.D., Gange S.J. and Lau
B.M. (2014). Random survival forests for competing risks.
<em>Biostatistics</em>, 15(4):757-773.
</p>
<p>Ishwaran H. and Malley J.D. (2014). Synthetic learning
machines. <em>BioData Mining</em>, 7:28.
</p>
<p>Ishwaran H. (2015).  The effect of splitting on random forests.
<em>Machine Learning</em>, 99:75-118.
</p>
<p>Lin, Y. and Jeon, Y. (2006). Random forests and adaptive nearest
neighbors. <em>J. Amer. Statist. Assoc.</em>, 101(474), 578-590.
</p>
<p>Lu M., Sadiq S., Feaster D.J. and Ishwaran H. (2018). Estimating
individual treatment effect in observational data using random forest
methods. <em>J. Comp. Graph. Statist</em>, 27(1), 209-219
</p>
<p>Ishwaran H. and Lu M.  (2019).  Standard errors and confidence
intervals for variable importance in random forest regression,
classification, and survival. <em>Statistics in Medicine</em>, 38,
558-582.
</p>
<p>LeBlanc M. and Crowley J. (1993).  Survival trees by goodness of split,
<em>J. Amer. Statist. Assoc.</em>, 88:457-467.
</p>
<p>Loh W.-Y and Shih Y.-S (1997).  Split selection methods for
classification trees, <em>Statist. Sinica</em>, 7:815-840.
</p>
<p>Mantero A. and Ishwaran H. (2021).  Unsupervised random forests.
<em>Statistical Analysis and Data Mining</em>, 14(2):144-167.
</p>
<p>Meinshausen N. (2006) Quantile regression forests, <em>Journal of
Machine Learning Research</em>, 7:983-999.
</p>
<p>Mogensen, U.B, Ishwaran H. and Gerds T.A. (2012). Evaluating random
forests for survival analysis using prediction error curves,
<em>J.  Statist. Software</em>, 50(11): 1-23.
</p>
<p>O'Brien R. and Ishwaran H. (2019).  A random forests quantile
classifier for class imbalanced data. <em>Pattern Recognition</em>,
90, 232-249
</p>
<p>Segal M.R. (1988).  Regression trees for censored data,
<em>Biometrics</em>,  44:35-47.
</p>
<p>Segal M.R. and Xiao Y. Multivariate random
forests. (2011). <em>Wiley Interdisciplinary Reviews: Data Mining
and Knowledge Discovery</em>. 1(1):80-87.
</p>
<p>Tang F. and Ishwaran H. (2017).  Random forest missing data
algorithms.  <em>Statistical Analysis and Data Mining</em>, 10:363-377.
</p>
<p>Zhang H., Zimmerman J., Nettleton D. and Nordman D.J. (2019).  Random
forest prediction intervals. <em>The American Statistician</em>. 4:1-5.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+find.interaction.rfsrc">find.interaction.rfsrc</a></code>,
</p>
<p><code><a href="#topic+get.tree.rfsrc">get.tree.rfsrc</a></code>,
</p>
<p><code><a href="#topic+holdout.vimp.rfsrc">holdout.vimp.rfsrc</a></code>,
</p>
<p><code><a href="#topic+imbalanced.rfsrc">imbalanced.rfsrc</a></code>,
<code><a href="#topic+impute.rfsrc">impute.rfsrc</a></code>,
</p>
<p><code><a href="#topic+max.subtree.rfsrc">max.subtree.rfsrc</a></code>,
</p>
<p><code><a href="#topic+partial.rfsrc">partial.rfsrc</a></code>,
<code><a href="#topic+plot.competing.risk.rfsrc">plot.competing.risk.rfsrc</a></code>,
<code><a href="#topic+plot.rfsrc">plot.rfsrc</a></code>,
<code><a href="#topic+plot.survival.rfsrc">plot.survival.rfsrc</a></code>,
<code><a href="#topic+plot.variable.rfsrc">plot.variable.rfsrc</a></code>,
<code><a href="#topic+predict.rfsrc">predict.rfsrc</a></code>,
<code><a href="#topic+print.rfsrc">print.rfsrc</a></code>,
</p>
<p><code><a href="#topic+quantreg.rfsrc">quantreg.rfsrc</a></code>,
</p>
<p><code><a href="#topic+rfsrc">rfsrc</a></code>,
<code><a href="#topic+rfsrc.anonymous">rfsrc.anonymous</a></code>,
<code><a href="#topic+rfsrc.cart">rfsrc.cart</a></code>,
<code><a href="#topic+rfsrc.fast">rfsrc.fast</a></code>,
</p>
<p><code><a href="#topic+sidClustering.rfsrc">sidClustering.rfsrc</a></code>,
</p>
<p><code><a href="#topic+stat.split.rfsrc">stat.split.rfsrc</a></code>,
<code><a href="#topic+subsample.rfsrc">subsample.rfsrc</a></code>,
<code><a href="#topic+synthetic.rfsrc">synthetic.rfsrc</a></code>,
</p>
<p><code><a href="#topic+tune.rfsrc">tune.rfsrc</a></code>,
</p>
<p><code><a href="#topic+var.select.rfsrc">var.select.rfsrc</a></code>,
<code><a href="#topic+vimp.rfsrc">vimp.rfsrc</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>##------------------------------------------------------------
## survival analysis
##------------------------------------------------------------
options(rf.cores=2, mc.cores=2)

## veteran data
## randomized trial of two treatment regimens for lung cancer
data(veteran, package = "randomForestSRC")
v.obj &lt;- rfsrc(Surv(time, status) ~ ., data = veteran, 
                   ntree = 10, block.size = 1)

## plot tree number 3
plot(get.tree(v.obj, 3))

## print results of trained forest
print(v.obj)

## plot results of trained forest
plot(v.obj)

## plot survival curves for first 10 individuals -- direct way
matplot(v.obj$time.interest, 100 * t(v.obj$survival.oob[1:10, ]),
    xlab = "Time", ylab = "Survival", type = "l", lty = 1)

## plot survival curves for first 10 individuals
## using function "plot.survival" 
plot.survival(v.obj, subset = 1:10)


## obtain Brier score using KM and RSF censoring distribution estimators
bs.km &lt;- get.brier.survival(v.obj, cens.model = "km")$brier.score
bs.rsf &lt;- get.brier.survival(v.obj, cens.model = "rfsrc")$brier.score

## plot the brier score
plot(bs.km, type = "s", col = 2)
lines(bs.rsf, type ="s", col = 4)
legend("topright", legend = c("cens.model = km", "cens.model = rfsrc"), fill = c(2,4))

## plot CRPS (continuous rank probability score) as function of time
## here's how to calculate the CRPS for every time point
trapz &lt;- randomForestSRC:::trapz
time &lt;- v.obj$time.interest
crps.km &lt;- sapply(1:length(time), function(j) {
  trapz(time[1:j], bs.km[1:j, 2] / diff(range(time[1:j])))
})
crps.rsf &lt;- sapply(1:length(time), function(j) {
  trapz(time[1:j], bs.rsf[1:j, 2] / diff(range(time[1:j])))
})
plot(time, crps.km, ylab = "CRPS", type = "s", col = 2)
lines(time, crps.rsf, type ="s", col = 4)
legend("bottomright", legend=c("cens.model = km", "cens.model = rfsrc"), fill=c(2,4))


## fast nodesize optimization for veteran data
## optimal nodesize in survival is larger than other families
## see the function "tune" for more examples
tune.nodesize(Surv(time,status) ~ ., veteran)


## Primary biliary cirrhosis (PBC) of the liver
data(pbc, package = "randomForestSRC")
pbc.obj &lt;- rfsrc(Surv(days, status) ~ ., pbc)
print(pbc.obj)


## save.memory example for survival
## growing many deep trees creates memory issue without this option!
data(pbc, package = "randomForestSRC")
print(rfsrc(Surv(days, status) ~ ., pbc, splitrule = "random",
            ntree = 25000, nodesize = 1, save.memory = TRUE))



##------------------------------------------------------------
## trees can be plotted for any family 
## see get.tree for details and more examples
##------------------------------------------------------------

## survival where factors have many levels
data(veteran, package = "randomForestSRC")
vd &lt;- veteran
vd$celltype=factor(vd$celltype)
vd$diagtime=factor(vd$diagtime)
vd.obj &lt;- rfsrc(Surv(time,status)~., vd, ntree = 100, nodesize = 5)
plot(get.tree(vd.obj, 3))

## classification
iris.obj &lt;- rfsrc(Species ~., data = iris)
plot(get.tree(iris.obj, 25, class.type = "bayes"))
plot(get.tree(iris.obj, 25, target = "setosa"))
plot(get.tree(iris.obj, 25, target = "versicolor"))
plot(get.tree(iris.obj, 25, target = "virginica"))

## ------------------------------------------------------------
## simple example of VIMP using iris classification
## ------------------------------------------------------------

## directly from trained forest
print(rfsrc(Species~.,iris,importance=TRUE)$importance)

## VIMP (and performance) use misclassification error by default
## but brier prediction error can be requested
print(rfsrc(Species~.,iris,importance=TRUE,perf.type="brier")$importance)

## example using vimp function (see vimp help file for details)
iris.obj &lt;- rfsrc(Species ~., data = iris)
print(vimp(iris.obj)$importance)
print(vimp(iris.obj,perf.type="brier")$importance)

## example using hold out vimp (see holdout.vimp help file for details)
print(holdout.vimp(Species~.,iris)$importance)
print(holdout.vimp(Species~.,iris,perf.type="brier")$importance)

## ------------------------------------------------------------
## confidence interval for vimp using subsampling
## compare with holdout vimp
## ------------------------------------------------------------

## new York air quality measurements
o &lt;- rfsrc(Ozone ~ ., data = airquality)
so &lt;- subsample(o)
plot(so)

## compare with holdout vimp
print(holdout.vimp(Ozone ~ ., data = airquality)$importance)


##------------------------------------------------------------
## example of imputation in survival analysis
##------------------------------------------------------------

data(pbc, package = "randomForestSRC")
pbc.obj2 &lt;- rfsrc(Surv(days, status) ~ ., pbc, na.action = "na.impute")

## same as above but iterate the missing data algorithm
pbc.obj3 &lt;- rfsrc(Surv(days, status) ~ ., pbc,
         na.action = "na.impute", nimpute = 3)

## fast way to impute data (no inference is done)
## see impute for more details
pbc.imp &lt;- impute(Surv(days, status) ~ ., pbc, splitrule = "random")

##------------------------------------------------------------
## compare RF-SRC to Cox regression
## Illustrates C-index and Brier score measures of performance
## assumes "pec" and "survival" libraries are loaded
##------------------------------------------------------------

if (library("survival", logical.return = TRUE)
    &amp; library("pec", logical.return = TRUE)
    &amp; library("prodlim", logical.return = TRUE))

{
  ##prediction function required for pec
  predictSurvProb.rfsrc &lt;- function(object, newdata, times, ...){
    ptemp &lt;- predict(object,newdata=newdata,...)$survival
    pos &lt;- sindex(jump.times = object$time.interest, eval.times = times)
    p &lt;- cbind(1,ptemp)[, pos + 1]
    if (NROW(p) != NROW(newdata) || NCOL(p) != length(times))
      stop("Prediction failed")
    p
  }

  ## data, formula specifications
  data(pbc, package = "randomForestSRC")
  pbc.na &lt;- na.omit(pbc)  ##remove NA's
  surv.f &lt;- as.formula(Surv(days, status) ~ .)
  pec.f &lt;- as.formula(Hist(days,status) ~ 1)

  ## run cox/rfsrc models
  ## for illustration we use a small number of trees
  cox.obj &lt;- coxph(surv.f, data = pbc.na, x = TRUE)
  rfsrc.obj &lt;- rfsrc(surv.f, pbc.na, ntree = 150)

  ## compute bootstrap cross-validation estimate of expected Brier score
  ## see Mogensen, Ishwaran and Gerds (2012) Journal of Statistical Software
  set.seed(17743)
  prederror.pbc &lt;- pec(list(cox.obj,rfsrc.obj), data = pbc.na, formula = pec.f,
                        splitMethod = "bootcv", B = 50)
  print(prederror.pbc)
  plot(prederror.pbc)

  ## compute out-of-bag C-index for cox regression and compare to rfsrc
  rfsrc.obj &lt;- rfsrc(surv.f, pbc.na)
  cat("out-of-bag Cox Analysis ...", "\n")
  cox.err &lt;- sapply(1:100, function(b) {
    if (b%%10 == 0) cat("cox bootstrap:", b, "\n")
    train &lt;- sample(1:nrow(pbc.na), nrow(pbc.na), replace = TRUE)
    cox.obj &lt;- tryCatch({coxph(surv.f, pbc.na[train, ])}, error=function(ex){NULL})
    if (!is.null(cox.obj)) {
      get.cindex(pbc.na$days[-train], pbc.na$status[-train], predict(cox.obj, pbc.na[-train, ]))
    } else NA
  })
  cat("\n\tOOB error rates\n\n")
  cat("\tRSF            : ", rfsrc.obj$err.rate[rfsrc.obj$ntree], "\n")
  cat("\tCox regression : ", mean(cox.err, na.rm = TRUE), "\n")
}

##------------------------------------------------------------
## competing risks
##------------------------------------------------------------

## WIHS analysis
## cumulative incidence function (CIF) for HAART and AIDS stratified by IDU

data(wihs, package = "randomForestSRC")
wihs.obj &lt;- rfsrc(Surv(time, status) ~ ., wihs, nsplit = 3, ntree = 100)
plot.competing.risk(wihs.obj)
cif &lt;- wihs.obj$cif.oob
Time &lt;- wihs.obj$time.interest
idu &lt;- wihs$idu
cif.haart &lt;- cbind(apply(cif[,,1][idu == 0,], 2, mean),
                   apply(cif[,,1][idu == 1,], 2, mean))
cif.aids  &lt;- cbind(apply(cif[,,2][idu == 0,], 2, mean),
                   apply(cif[,,2][idu == 1,], 2, mean))
matplot(Time, cbind(cif.haart, cif.aids), type = "l",
        lty = c(1,2,1,2), col = c(4, 4, 2, 2), lwd = 3,
        ylab = "Cumulative Incidence")
legend("topleft",
       legend = c("HAART (Non-IDU)", "HAART (IDU)", "AIDS (Non-IDU)", "AIDS (IDU)"),
       lty = c(1,2,1,2), col = c(4, 4, 2, 2), lwd = 3, cex = 1.5)


## illustrates the various splitting rules
## illustrates event specific and non-event specific variable selection
if (library("survival", logical.return = TRUE)) {

  ## use the pbc data from the survival package
  ## events are transplant (1) and death (2)
  data(pbc, package = "survival")
  pbc$id &lt;- NULL

  ## modified Gray's weighted log-rank splitting
  ## (equivalent to cause=c(1,1) and splitrule="logrankCR")
  pbc.cr &lt;- rfsrc(Surv(time, status) ~ ., pbc)
 
  ## log-rank cause-1 specific splitting and targeted VIMP for cause 1
  pbc.log1 &lt;- rfsrc(Surv(time, status) ~ ., pbc, 
              splitrule = "logrankCR", cause = c(1,0), importance = TRUE)

  ## log-rank cause-2 specific splitting and targeted VIMP for cause 2
  pbc.log2 &lt;- rfsrc(Surv(time, status) ~ ., pbc, 
              splitrule = "logrankCR", cause = c(0,1), importance = TRUE)

  ## extract VIMP from the log-rank forests: event-specific
  ## extract minimal depth from the Gray log-rank forest: non-event specific
  var.perf &lt;- data.frame(md = max.subtree(pbc.cr)$order[, 1],
                         vimp1 = 100 * pbc.log1$importance[ ,1],
                         vimp2 = 100 * pbc.log2$importance[ ,2])
  print(var.perf[order(var.perf$md), ], digits = 2)

}

## ------------------------------------------------------------
## regression analysis
## ------------------------------------------------------------

## new York air quality measurements
airq.obj &lt;- rfsrc(Ozone ~ ., data = airquality, na.action = "na.impute")

# partial plot of variables (see plot.variable for more details)
plot.variable(airq.obj, partial = TRUE, smooth.lines = TRUE)

## motor trend cars
mtcars.obj &lt;- rfsrc(mpg ~ ., data = mtcars)

## ------------------------------------------------------------
## regression with custom bootstrap
## ------------------------------------------------------------

ntree &lt;- 25
n &lt;- nrow(mtcars)
s.size &lt;- n / 2
swr &lt;- TRUE
samp &lt;- randomForestSRC:::make.sample(ntree, n, s.size, swr)
o &lt;- rfsrc(mpg ~ ., mtcars, bootstrap = "by.user", samp = samp)

## ------------------------------------------------------------
## classification analysis
## ------------------------------------------------------------

## iris data
iris.obj &lt;- rfsrc(Species ~., data = iris)

## wisconsin prognostic breast cancer data
data(breast, package = "randomForestSRC")
breast.obj &lt;- rfsrc(status ~ ., data = breast, block.size=1)
plot(breast.obj)

## ------------------------------------------------------------
## big data set, reduce number of variables using simple method
## ------------------------------------------------------------

## use Iowa housing data set
data(housing, package = "randomForestSRC")

## original data contains lots of missing data, use fast imputation
## however see impute for other methods
housing2 &lt;- impute(data = housing, fast = TRUE)

## run shallow trees to find variables that split any tree
xvar.used &lt;- rfsrc(SalePrice ~., housing2, ntree = 250, nodedepth = 4,
                   var.used="all.trees", mtry = Inf, nsplit = 100)$var.used

## now fit forest using filtered variables
xvar.keep  &lt;- names(xvar.used)[xvar.used &gt;= 1]
o &lt;- rfsrc(SalePrice~., housing2[, c("SalePrice", xvar.keep)])
print(o)

## ------------------------------------------------------------
## imbalanced classification data
## see the "imbalanced" function for further details
##
## (a) use balanced random forests with undersampling of the majority class
## Specifically let n0, n1 be sample sizes for majority, minority
## cases.  We sample 2 x n1 cases with majority, minority cases chosen
## with probabilities n1/n, n0/n where n=n0+n1
##
## (b) balanced random forests using "imbalanced"
##
## (c) q-classifier (RFQ) using "imbalanced" 
##
## ------------------------------------------------------------

## Wisconsin breast cancer example
data(breast, package = "randomForestSRC")
breast &lt;- na.omit(breast)

## balanced random forests - brute force
y &lt;- breast$status
obdirect &lt;- rfsrc(status ~ ., data = breast, nsplit = 10,
            case.wt = randomForestSRC:::make.wt(y),
            sampsize = randomForestSRC:::make.size(y))
print(obdirect)
print(get.imbalanced.performance(obdirect))

## balanced random forests - using "imbalanced" 
ob &lt;- imbalanced(status ~ ., data = breast, method = "brf")
print(ob)
print(get.imbalanced.performance(ob))

## q-classifier (RFQ) - using "imbalanced" 
oq &lt;- imbalanced(status ~ ., data = breast)
print(oq)
print(get.imbalanced.performance(oq))

## q-classifier (RFQ) - with auc splitting
oqauc &lt;- imbalanced(status ~ ., data = breast, splitrule = "auc")
print(oqauc)
print(get.imbalanced.performance(oqauc))

## ------------------------------------------------------------
## unsupervised analysis
## ------------------------------------------------------------

## two equivalent ways to implement unsupervised forests
mtcars.unspv &lt;- rfsrc(Unsupervised() ~., data = mtcars)
mtcars2.unspv &lt;- rfsrc(data = mtcars)


## illustration of sidClustering for the mtcars data
## see sidClustering for more details
mtcars.sid &lt;- sidClustering(mtcars, k = 1:10)
print(split(mtcars, mtcars.sid$cl[, 3]))
print(split(mtcars, mtcars.sid$cl[, 10]))


## ------------------------------------------------------------
## bivariate regression using Mahalanobis splitting
## also illustrates user specified covariance matrix
## ------------------------------------------------------------

if (library("mlbench", logical.return = TRUE)) {

  ## load boston housing data, specify the bivariate regression
  data(BostonHousing)
  f &lt;- formula("Multivar(lstat, nox) ~.")
  
  ## Mahalanobis splitting  
  bh.mreg &lt;- rfsrc(f, BostonHousing, importance = TRUE, splitrule = "mahal")
  
  ## performance error and vimp
  vmp &lt;- get.mv.vimp(bh.mreg)
  pred &lt;- get.mv.predicted(bh.mreg)

  ## standardized error and vimp
  err.std &lt;- get.mv.error(bh.mreg, standardize = TRUE)
  vmp.std &lt;- get.mv.vimp(bh.mreg, standardize = TRUE)

  ## same analysis, but with user specified covariance matrix
  sigma &lt;- cov(BostonHousing[, c("lstat","nox")])
  bh.mreg2 &lt;- rfsrc(f, BostonHousing, splitrule = "mahal", sigma = sigma)
  
}

## ------------------------------------------------------------
## multivariate mixed forests (nutrigenomic study)
## study effects of diet, lipids and gene expression for mice
## diet, genotype and lipids used as the multivariate y
## genes used for the x features
## ------------------------------------------------------------

## load the data (data is a list)
data(nutrigenomic, package = "randomForestSRC")

## assemble the multivariate y data
ydta &lt;- data.frame(diet = nutrigenomic$diet,
                   genotype = nutrigenomic$genotype,
                   nutrigenomic$lipids)

## multivariate mixed forest call
## uses "get.mv.formula" for conveniently setting formula
mv.obj &lt;- rfsrc(get.mv.formula(colnames(ydta)),
                data.frame(ydta, nutrigenomic$genes),
		importance=TRUE, nsplit = 10)

## print results for diet and genotype y values	    
print(mv.obj, outcome.target = "diet")
print(mv.obj, outcome.target = "genotype")

## extract standardized VIMP
svimp &lt;- get.mv.vimp(mv.obj, standardize = TRUE)

## plot standardized VIMP for diet, genotype and lipid for each gene
boxplot(t(svimp), col = "bisque", cex.axis = .7, las = 2,
        outline = FALSE,
        ylab = "standardized VIMP",
        main = "diet/genotype/lipid VIMP for each gene")
	
## ------------------------------------------------------------
## custom splitting using the pre-coded examples
## ------------------------------------------------------------

## motor trend cars
mtcars.obj &lt;- rfsrc(mpg ~ ., data = mtcars, splitrule = "custom")

## iris analysis
iris.obj &lt;- rfsrc(Species ~., data = iris, splitrule = "custom1")

## WIHS analysis
wihs.obj &lt;- rfsrc(Surv(time, status) ~ ., wihs, nsplit = 3,
                  ntree = 100, splitrule = "custom1")


</code></pre>

<hr>
<h2 id='rfsrc.anonymous'>Anonymous Random Forests</h2><span id='topic+rfsrc.anonymous'></span>

<h3>Description</h3>

<p>Anonymous random forests applies random forests but is carefully
modified so as not to save the original training data.  This allows
users to share their forest with other researchers but without having
to share their original data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rfsrc.anonymous(formula, data, forest = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rfsrc.anonymous_+3A_formula">formula</code></td>
<td>
<p>A symbolic description of the model to be fit. If
missing, unsupervised splitting is implemented.</p>
</td></tr>
<tr><td><code id="rfsrc.anonymous_+3A_data">data</code></td>
<td>
<p>Data frame containing the y-outcome and x-variables.</p>
</td></tr> 
<tr><td><code id="rfsrc.anonymous_+3A_forest">forest</code></td>
<td>
<p>Should the forest object be returned?  Used for
prediction on new data and required by many of the package
functions.</p>
</td></tr>
<tr><td><code id="rfsrc.anonymous_+3A_...">...</code></td>
<td>
<p>Further arguments as in <code><a href="#topic+rfsrc">rfsrc</a></code>.  See the
<code>rfsrc</code> help file for details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Calls <code><a href="#topic+rfsrc">rfsrc</a></code> and returns an object with the training data
removed so that users can share their forest while maintaining privacy
of their data.
</p>
<p>In order to predict on test data, it is however necessary for certain
minimal information to be saved from the training data.  This includes
the names of the original variables, and if factor variables are
present, the levels of the factors.  The mean value and maximal class
value for real and factor variables in the training data are also
stored for the purposes of imputation on test data (see below).  The
topology of grow trees is also saved, which includes among other
things, the split values used for splitting tree nodes.
</p>
<p>For the most privacy, we recommend that variable names be made
non-identifiable and that data be coerced to real values.  If factors
are required, the user should consider using non-identifiable factor
levels.  However, in all cases, it is the users responsibility to
de-identify their data and to check that data privacy holds.  We
provide NO GUARANTEES of this.
</p>
<p>Missing data is especially delicate with anonymous forests.  Training
data cannot be imputed and the option <code>na.action="na.impute"</code>
simply reverts to <code>na.action="na.omit"</code>.  Therefore if you have
training data with missing values consider using pre-imputing the data
using <code>impute</code>.  It is however possible to impute on test
data.  The option <code>na.action="na.impute"</code> in the prediction call
triggers a rough and fast imputation method where the value of missing
test data are replaced by the mean (or maximal class) value from the
training data.  A second option <code>na.action="na.random"</code> uses a
fast random imputation method.
</p>
<p>In general, it is important to keep in mind that while anonymous
forests tries to play nice with other functions in the package, it
only works with calls that do not specifically require training data.
</p>


<h3>Value</h3>

<p>An object of class <code>(rfsrc, grow, anonymous)</code>.
</p>


<h3>Author(s)</h3>

<p>Hemant Ishwaran and Udaya B. Kogalur
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rfsrc">rfsrc</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

## ------------------------------------------------------------
## regression
## ------------------------------------------------------------
print(rfsrc.anonymous(mpg ~ ., mtcars))

## ------------------------------------------------------------
## plot anonymous regression tree (using get.tree)
## TBD CURRENTLY NOT IMPLEMENTED 
## ------------------------------------------------------------
## plot(get.tree(rfsrc.anonymous(mpg ~ ., mtcars), 10))

## ------------------------------------------------------------
## classification
## ------------------------------------------------------------
print(rfsrc.anonymous(Species ~ ., iris))

## ------------------------------------------------------------
## survival
## ------------------------------------------------------------
data(veteran, package = "randomForestSRC")
print(rfsrc.anonymous(Surv(time, status) ~ ., data = veteran))

## ------------------------------------------------------------
## competing risks
## ------------------------------------------------------------
data(wihs, package = "randomForestSRC")
print(rfsrc.anonymous(Surv(time, status) ~ ., wihs, ntree = 100))

## ------------------------------------------------------------
## unsupervised forests
## ------------------------------------------------------------
print(rfsrc.anonymous(data = iris))

## ------------------------------------------------------------
## multivariate regression
## ------------------------------------------------------------
print(rfsrc.anonymous(Multivar(mpg, cyl) ~., data = mtcars))

## ------------------------------------------------------------
## prediction on test data with missing values using pbc data
## cases 1 to 312 have no missing values
## cases 313 to 418 having missing values
## ------------------------------------------------------------
data(pbc, package = "randomForestSRC")
pbc.obj &lt;- rfsrc.anonymous(Surv(days, status) ~ ., pbc)
print(pbc.obj)

## mean value imputation
print(predict(pbc.obj, pbc[-(1:312),], na.action = "na.impute"))

## random imputation
print(predict(pbc.obj, pbc[-(1:312),], na.action = "na.random"))

## ------------------------------------------------------------
## train/test setting but tricky because factor labels differ over
## training and test data
## ------------------------------------------------------------

# first we convert all x-variables to factors
data(veteran, package = "randomForestSRC")
veteran.factor &lt;- data.frame(lapply(veteran, factor))
veteran.factor$time &lt;- veteran$time
veteran.factor$status &lt;- veteran$status

# split the data into train/test data (25/75)
# the train/test data have the same levels, but different labels
train &lt;- sample(1:nrow(veteran), round(nrow(veteran) * .5))
summary(veteran.factor[train, ])
summary(veteran.factor[-train, ])

# grow the forest on the training data and predict on the test data
v.grow &lt;- rfsrc.anonymous(Surv(time, status) ~ ., veteran.factor[train, ]) 
v.pred &lt;- predict(v.grow, veteran.factor[-train, ])
print(v.grow)
print(v.pred)



</code></pre>

<hr>
<h2 id='rfsrc.fast'>Fast Random Forests</h2><span id='topic+rfsrc.fast'></span>

<h3>Description</h3>

<p>Fast approximate random forests using subsampling with forest options
set to encourage computational speed.  Applies to all families.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rfsrc.fast(formula, data,
  ntree = 500,
  nsplit = 10,
  bootstrap = "by.root",
  sampsize = function(x){min(x * .632, max(150, x ^ (3/4)))},
  samptype = "swor",
  samp = NULL,
  ntime = 50,
  forest = FALSE,
  save.memory = TRUE,
  ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rfsrc.fast_+3A_formula">formula</code></td>
<td>
<p>Model to be fit. If missing, unsupervised splitting is
implemented.</p>
</td></tr>
<tr><td><code id="rfsrc.fast_+3A_data">data</code></td>
<td>
<p>Data frame containing the y-outcome and x-variables.</p>
</td></tr> 
<tr><td><code id="rfsrc.fast_+3A_ntree">ntree</code></td>
<td>
<p>Number of trees.</p>
</td></tr>
<tr><td><code id="rfsrc.fast_+3A_nsplit">nsplit</code></td>
<td>
<p>Non-negative integer value specifying number of 
random split points used to split a node (deterministic splitting
corresponds to the value zero and can be slower).</p>
</td></tr>
<tr><td><code id="rfsrc.fast_+3A_bootstrap">bootstrap</code></td>
<td>
<p>Bootstrap protocol used in growing a tree.</p>
</td></tr>
<tr><td><code id="rfsrc.fast_+3A_sampsize">sampsize</code></td>
<td>
<p>Function specifying size of subsampled data. Can also be a number.</p>
</td></tr>
<tr><td><code id="rfsrc.fast_+3A_samptype">samptype</code></td>
<td>
<p>Type of bootstrap used.</p>
</td></tr>
<tr><td><code id="rfsrc.fast_+3A_samp">samp</code></td>
<td>
<p>Bootstrap specification when <code>"by.user"</code> is used.</p>
</td></tr>
<tr><td><code id="rfsrc.fast_+3A_ntime">ntime</code></td>
<td>
<p>Integer value used for survival to
constrain ensemble calculations to a grid of <code>ntime</code> time points.</p>
</td></tr>
<tr><td><code id="rfsrc.fast_+3A_forest">forest</code></td>
<td>
<p>Save key forest values? Turn this on if you want prediction on test data.</p>
</td></tr>
<tr><td><code id="rfsrc.fast_+3A_save.memory">save.memory</code></td>
<td>
<p>Save memory?  Setting this to <code>FALSE</code> stores
terminal node quantities used for prediction on test data.  This
yields rapid prediction but can be memory intensive for big data,
especially competing risks and survival models.</p>
</td></tr>
<tr><td><code id="rfsrc.fast_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed to <code><a href="#topic+rfsrc">rfsrc</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Calls <code><a href="#topic+rfsrc">rfsrc</a></code> by choosing options (like subsampling) to
encourage computational speeds.  This will provide a good
approximation but will not be as good as default settings of
<code><a href="#topic+rfsrc">rfsrc</a></code>.
</p>


<h3>Value</h3>

<p>An object of class <code>(rfsrc, grow)</code>.
</p>


<h3>Author(s)</h3>

<p>Hemant Ishwaran and Udaya B. Kogalur
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rfsrc">rfsrc</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## ------------------------------------------------------------
## regression 
## ------------------------------------------------------------

## load the Iowa housing data
data(housing, package = "randomForestSRC")

## do quick and *dirty* imputation
housing &lt;- impute(SalePrice ~ ., housing,
         ntree = 50, nimpute = 1, splitrule = "random")

## grow a fast forest
o1 &lt;- rfsrc.fast(SalePrice ~ ., housing)
o2 &lt;- rfsrc.fast(SalePrice ~ ., housing, nodesize = 1)
print(o1)
print(o2)

## grow a fast bivariate forest
o3 &lt;- rfsrc.fast(cbind(SalePrice,Overall.Qual) ~ ., housing)
print(o3)

## ------------------------------------------------------------
## classification 
## ------------------------------------------------------------

data(wine, package = "randomForestSRC")
wine$quality &lt;- factor(wine$quality)
o &lt;- rfsrc.fast(quality ~ ., wine)
print(o)

## ------------------------------------------------------------
## grow fast random survival forests without C-calculation
## use brier score to assess model performance
## compare pure random splitting to logrank splitting
## ------------------------------------------------------------

data(peakVO2, package = "randomForestSRC")
f &lt;- as.formula(Surv(ttodead, died)~.)
o1 &lt;- rfsrc.fast(f, peakVO2, perf.type = "none")
o2 &lt;- rfsrc.fast(f, peakVO2, perf.type = "none", splitrule = "random")
bs1 &lt;- get.brier.survival(o1, cens.model = "km")
bs2 &lt;- get.brier.survival(o2, cens.model = "km")
plot(bs2$brier.score, type = "s", col = 2)
lines(bs1$brier.score, type = "s", col = 4)
legend("bottomright", legend = c("random", "logrank"), fill = c(2,4))

## ------------------------------------------------------------
## competing risks
## ------------------------------------------------------------

data(wihs, package = "randomForestSRC")
o &lt;- rfsrc.fast(Surv(time, status) ~ ., wihs)
print(o)

## ------------------------------------------------------------
## class imbalanced data using gmean performance
## ------------------------------------------------------------

data(breast, package = "randomForestSRC")
breast &lt;- na.omit(breast)
f &lt;- as.formula(status ~ .)
o &lt;- rfsrc.fast(f, breast, perf.type = "gmean")
print(o)

## ------------------------------------------------------------
## class imbalanced data using random forests quantile-classifer (RFQ)
## fast=TRUE =&gt; rfsrc.fast
## see imbalanced function for further details
## ------------------------------------------------------------

data(breast, package = "randomForestSRC")
breast &lt;- na.omit(breast)
f &lt;- as.formula(status ~ .)
o &lt;- imbalanced(f, breast, fast = TRUE)
print(o)

</code></pre>

<hr>
<h2 id='rfsrc.news'>Show the NEWS file</h2><span id='topic+rfsrc.news'></span>

<h3>Description</h3>

<p>Show the NEWS file of the <span class="pkg">randomForestSRC</span> package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rfsrc.news(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rfsrc.news_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None.
</p>


<h3>Author(s)</h3>

<p>Hemant Ishwaran and Udaya B. Kogalur
</p>

<hr>
<h2 id='sidClustering.rfsrc'>sidClustering using SID (Staggered Interaction Data) for Unsupervised Clustering</h2><span id='topic+sidClustering.rfsrc'></span><span id='topic+sidClustering'></span><span id='topic+sid.perf.metric'></span>

<h3>Description</h3>

<p>Clustering of unsupervised data using SID (Mantero and Ishwaran,
2020).  Also implements the artificial two-class approach of
Breiman (2003).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'rfsrc'
sidClustering(data,
  method = "sid",
  k = NULL,
  reduce = TRUE,
  ntree = 500,
  ntree.reduce = function(p, vtry){100 * p / vtry},
  fast = FALSE,
  x.no.sid = NULL,
  use.sid.for.x = TRUE,
  x.only = NULL, y.only = NULL,
  dist.sharpen = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sidClustering.rfsrc_+3A_data">data</code></td>
<td>
<p>Data frame containing the unsupervised data.</p>
</td></tr>
<tr><td><code id="sidClustering.rfsrc_+3A_method">method</code></td>
<td>
<p>The method used for unsupervised clustering.  Default is
&quot;sid&quot; which implements sidClustering using SID (Staggered
Interaction Data; see Mantero and Ishwaran, 2020).  A second
approach transforms the unsupervised learning problem into a
two-class supervised problem (Breiman, 2003) using artificial data
created using mode 1 or mode 2 of Shi-Horvath (2006).  This approach
is specified by any one of the following: &quot;sh&quot;, &quot;SH&quot;, &quot;sh1&quot;, &quot;SH1&quot;
for mode 1, or &quot;sh2&quot;, &quot;SH2&quot; for mode 2.  Finally, a third approach
is a plain vanilla method where the data are used both as features
and response with splitting implemented using the multivariate
splitting rule.  This is faster than sidClustering but potentially
less accurate.  This method is specified using &quot;unsupv&quot;.</p>
</td></tr>
<tr><td><code id="sidClustering.rfsrc_+3A_k">k</code></td>
<td>
<p>Requested number of clusters.  Can be a number or a vector.
If a fixed number, returns a vector recording clustering of data.
If a vector, returns a matrix of clusters with each column recording
the clustering of the data for the specified number of clusters.</p>
</td></tr>
<tr><td><code id="sidClustering.rfsrc_+3A_reduce">reduce</code></td>
<td>
<p>Apply dimension reduction? Uses holdout vimp which is
computationally intensive and conservative but has good false
discovery properties.  Only applies to <code>method="sid"</code>.</p>
</td></tr>
<tr><td><code id="sidClustering.rfsrc_+3A_ntree">ntree</code></td>
<td>
<p>Number of trees used by sidClustering in the main analysis.</p>
</td></tr>
<tr><td><code id="sidClustering.rfsrc_+3A_ntree.reduce">ntree.reduce</code></td>
<td>
<p>Number of trees used by holdout vimp in the
reduction step.  See <code>holdout.vimp</code> for details.</p>
</td></tr>
<tr><td><code id="sidClustering.rfsrc_+3A_fast">fast</code></td>
<td>
<p>Use fast random forests, <code>rfsrcFast</code>, in place of
<code>rfsrc</code>?  Improves speed but is less accurate.</p>
</td></tr>
<tr><td><code id="sidClustering.rfsrc_+3A_x.no.sid">x.no.sid</code></td>
<td>
<p>Features not to be &quot;sid-ified&quot;: meaning that these
features are to be included in the final design matrix without SID
processing.  Can be either a data frame (should not overlap with
<code>data</code>), or a character vector containing the names of
features from the original data that the user wishes to protect
from sidification.  Applies only to <code>method="sid"</code>.</p>
</td></tr>
<tr><td><code id="sidClustering.rfsrc_+3A_use.sid.for.x">use.sid.for.x</code></td>
<td>
<p>If FALSE, reverses features and outcomes in the
SID analysis.  Thus, staggered interactions are used for the
outcomes rather than staggered features.  This is much slower and is
generally much less effective.  This option is only retained for
legacy reasons. Applies only to <code>method="sid"</code>.</p>
</td></tr>
<tr><td><code id="sidClustering.rfsrc_+3A_x.only">x.only</code></td>
<td>
<p>Use only these variables for the features.  Applies only
to <code>method="unsupv"</code>.</p>
</td></tr>
<tr><td><code id="sidClustering.rfsrc_+3A_y.only">y.only</code></td>
<td>
<p>Use only these variables for the multivariate outcomes.  Applies only
to <code>method="unsupv"</code>.</p>
</td></tr>
<tr><td><code id="sidClustering.rfsrc_+3A_dist.sharpen">dist.sharpen</code></td>
<td>
<p>By default, distance sharpening is requested,
which applies Euclidean distance to the random forest distance
matrix to sharpen it.  Because of this, the returned distance matrix
will not have values between 0 and 1 (as for random forests distance) when
this option is in effect.  Distance sharpening is a useful, but slow
step. Set this option to <code>FALSE</code> to improve computational
times, however clustering performance will not be as good.  Applies
only when <code>method="sid"</code> or <code>method="unsupv"</code>.</p>
</td></tr>
<tr><td><code id="sidClustering.rfsrc_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed to the <code>rfsrc</code>
function to specify random forest parameters.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Given an unsupervised data set, random forests is used to calculate
the distance between all pairs of data points.  The distance matrix is
used for clustering the unsupervised data where the default is to use
hierarchcial clustering.  Users can apply other clustering procedures
to the distance matrix.  See the examples below.
</p>
<p>The default method, <code>method="sid"</code>, implements sidClustering.
The sidClustering algorithm begins by first creating an enhanced SID
(Staggered Interaction Data) feature space by sidification of the
original variables.  Sidification results in: (a) SID main features
which are the original features that have been shifted in order to
make them strictly positive and staggered so all of their ranges are
mutually exclusive; and (b) SID interaction features which are the
multiplicative interactions formed between every pair of SID main
features.  Multivariate random forests are then trained to predict the
main SID features using the interaction SID features as predictors.
The basic premise is if features are informative for clusters, then
they will vary over the space in a systematic manner, and because each
SID interaction feature is uniquely determined by the original feature
values used to form the interaction, cuts along the SID interaction
feature will be able to find the regions where the informative
features vary by cluster, thereby not only reducing impurity, but also
separating the clusters which are dependent on those features.  See
Mantero and Ishwaran (2020) for details.
</p>
<p>Because SID uses all pairwise interactions, the dimension of the
feature space is proportional to the square of the number of original
features (or even larger if factors are present).  Thus it is helpful
to reduce the feature space. The reduction step (applied by default)
utilizes holdout VIMP to accomplish this.  It is recommended this step
be skipped only when the dimension is reasonably small.  For very
large data sets this step may be slow.
</p>
<p>A second approach (Breiman, 2003; Shi-Horvath, 2006) transforms the
unsupervised learning problem into a two class supervised problem.
The first class consists of the original observations, while the
second class is artificially created.  The idea is that in detecting
the first class out of the second, the model will generate the random
forest proximity between observations of which those for the original
class can be extracted and used for clustering.  Note in this approach
the distance matrix is defined to equal one minus the proximity.  This
is unlike the distance matrix from SID which is not proximity based.
Artificial data is created using &quot;mode 1&quot; or &quot;mode 2&quot; of Shi-Horvath
(2006).  Mode 1 randomly draws from each set of observed features.
Mode 2 draws a uniform value from the minimum and maximum values of a
feature.
</p>
<p>Mantero and Ishwaran (2020) studied both methods and found SID worked
well in all settings, whereas Breiman/Shi-Horvath was sensitive to
cluster structure.  Performance was poor when clusters were hidden in
lower dimensional subspaces; for example when interactions were
present or in mixed variable settings (factors/continuous variables).
See the V-shaped cluster example below.  Generally Shi-Horvath mode 1
outperforms mode 2.
</p>
<p>Finally, a third method where the data is used for both the features
and outcome is implemented using <code>method="unsupv"</code>.  Tree nodes
are split using the multivariate splitting rule.  This is much faster
than sidClustering but potentially less accurate.
</p>
<p>There is an internal function <code>sid.perf.metric</code> for evaluating
performance of the procedures using a normalized measure score.
Smaller values indicate better performance.  See Mantero and Ishwaran
(2020) for details.
</p>


<h3>Value</h3>

<p> A list with the following components:
</p>
<table>
<tr><td><code>clustering</code></td>
<td>
<p>Vector or matrix containing indices mapping data
points to their clusters.</p>
</td></tr>
<tr><td><code>rf</code></td>
<td>
<p>Random forest object (either a multivariate forest or RF-C object).</p>
</td></tr>
<tr><td><code>dist</code></td>
<td>
<p>Distance matrix.</p>
</td></tr>
<tr><td><code>sid</code></td>
<td>
<p>The &quot;sid-ified&quot; data. Conveniently broken up into
separate values for outcomes and features used by the
multivariate forest.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Hemant Ishwaran and Udaya B. Kogalur</p>


<h3>References</h3>

<p>Breiman, L. (2003). <em>Manual on setting up, using and
understanding random forest, V4.0</em>.  University of California
Berkeley, Statistics Department, Berkeley.
</p>
<p>Mantero A. and Ishwaran H. (2021).  Unsupervised random forests.
<em>Statistical Analysis and Data Mining</em>, 14(2):144-167.
</p>
<p>Shi, T. and Horvath, S. (2006). Unsupervised learning with random forest
predictors. <em>Journal of Computational and Graphical Statistics</em>,
15(1):118-138.  
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rfsrc">rfsrc</a></code>,
<code><a href="#topic+rfsrc.fast">rfsrc.fast</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## ------------------------------------------------------------
## mtcars example
## ------------------------------------------------------------

## default SID method 
o1 &lt;- sidClustering(mtcars)
print(split(mtcars, o1$cl[, 10]))

## using artifical class approach
o1.sh &lt;- sidClustering(mtcars, method = "sh")
print(split(mtcars, o1.sh$cl[, 10]))


## ------------------------------------------------------------
## glass data set
## ------------------------------------------------------------

if (library("mlbench", logical.return = TRUE)) {

  ## this is a supervised problem, so we first strip the class label
  data(Glass)
  glass &lt;- Glass
  y &lt;- Glass$Type
  glass$Type &lt;- NULL

  ## default SID call 
  o2 &lt;- sidClustering(glass, k = 6)
  print(table(y, o2$cl))
  print(sid.perf.metric(y, o2$cl))

  ## compare with Shi-Horvath mode 1 
  o2.sh &lt;- sidClustering(glass, method = "sh1", k = 6)
  print(table(y, o2.sh$cl))
  print(sid.perf.metric(y, o2.sh$cl))

  ## plain-vanilla unsupervised analysis
  o2.un &lt;- sidClustering(glass, method = "unsupv", k = 6)
  print(table(y, o2.un$cl))
  print(sid.perf.metric(y, o2.un$cl))

}

## ------------------------------------------------------------
## vowel data set
## ------------------------------------------------------------

if (library("mlbench", logical.return = TRUE) &amp;&amp;
    library("cluster", logical.return = TRUE)) {

  ## strip the class label
  data(Vowel)
  vowel &lt;- Vowel
  y &lt;- Vowel$Class
  vowel$Class &lt;- NULL

  ## SID 
  o3 &lt;- sidClustering(vowel, k = 11)
  print(table(y, o3$cl))
  print(sid.perf.metric(y, o3$cl))

  ## compare to Shi-Horvath which performs poorly in
  ## mixed variable settings
  o3.sh &lt;- sidClustering(vowel, method = "sh1", k = 11)
  print(table(y, o3.sh$cl))
  print(sid.perf.metric(y, o3.sh$cl))

  ## Shi-Horvath improves with PAM clustering
  ## but still not as good as SID
  o3.sh.pam &lt;- pam(o3.sh$dist, k = 11)$clustering
  print(table(y, o3.sh.pam))
  print(sid.perf.metric(y, o3.sh.pam))

  ## plain-vanilla unsupervised analysis
  o3.un &lt;- sidClustering(vowel, method = "unsupv", k = 11)
  print(table(y, o3.un$cl))
  print(sid.perf.metric(y, o3.un$cl))

}

## ------------------------------------------------------------
##  two-d V-shaped cluster (y=x, y=-x) sitting in 12-dimensions 
##  illustrates superiority of SID to Breiman/Shi-Horvath
## ------------------------------------------------------------

p &lt;- 10
m &lt;- 250
n &lt;- 2 * m
std &lt;- .2

x &lt;- runif(n, 0, 1)
noise &lt;- matrix(runif(n * p, 0, 1), n)
y &lt;- rep(NA, n)
y[1:m] &lt;- x[1:m] + rnorm(m, sd = std)
y[(m+1):n] &lt;- -x[(m+1):n] + rnorm(m, sd = std)
vclus &lt;- data.frame(clus = c(rep(1, m), rep(2,m)), x = x, y = y, noise)

## SID
o4 &lt;- sidClustering(vclus[, -1], k = 2)
print(table(vclus[, 1], o4$cl))
print(sid.perf.metric(vclus[, 1], o4$cl))

## Shi-Horvath
o4.sh &lt;- sidClustering(vclus[, -1], method = "sh1", k = 2)
print(table(vclus[, 1], o4.sh$cl))
print(sid.perf.metric(vclus[, 1], o4.sh$cl))

## plain-vanilla unsupervised analysis
o4.un &lt;- sidClustering(vclus[, -1], method = "unsupv", k = 2)
print(table(vclus[, 1], o4.un$cl))
print(sid.perf.metric(vclus[, 1], o4.un$cl))


## ------------------------------------------------------------
##  two-d V-shaped cluster using fast random forests
## ------------------------------------------------------------

o5 &lt;- sidClustering(vclus[, -1], k = 2, fast = TRUE)
print(table(vclus[, 1], o5$cl))
print(sid.perf.metric(vclus[, 1], o5$cl))


</code></pre>

<hr>
<h2 id='stat.split.rfsrc'>Acquire Split Statistic Information</h2><span id='topic+stat.split.rfsrc'></span><span id='topic+stat.split'></span>

<h3>Description</h3>

<p>Extract split statistic information from the forest.  The function
returns a list of length <code>ntree</code>, in which each element
corresponds to a tree.  The element [[b]] is itself a vector of length
xvar.names identified by its x-variable name.  Each element [[b]]$xvar
contains the complete list of splits on xvar with associated
identifying information.  The information is as follows:
</p>

<ol>
<li> <p><em>treeID</em> Tree identifier.
</p>
</li>
<li> <p><em>nodeID</em> Node identifier.
</p>
</li>
<li> <p><em>parmID</em> Variable indentifier.
</p>
</li>
<li> <p><em>contPT</em> Value node was split in the case of a
continuous variable.
</p>
</li>
<li> <p><em>mwcpSZ</em> Size of the multi-word complementary pair
in the case of a factor split.
</p>
</li>
<li> <p><em>dpthID</em> Zero (0) based depth of split.
</p>
</li>
<li> <p><em>spltTY</em> Split type for parent node:
</p>

<table>
<tr>
 <td style="text-align: left;">
      bit 1 </td><td style="text-align: left;">  bit 0  </td><td style="text-align: left;"> meaning</td>
</tr>
<tr>
 <td style="text-align: left;">
      ----- </td><td style="text-align: left;">  -----  </td><td style="text-align: left;"> ------- </td>
</tr>
<tr>
 <td style="text-align: left;">
      0   </td><td style="text-align: left;">    0    </td><td style="text-align: left;"> 0 = both daughters have valid splits</td>
</tr>
<tr>
 <td style="text-align: left;"> 
      0   </td><td style="text-align: left;">    1    </td><td style="text-align: left;"> 1 = only the right daughter is terminal</td>
</tr>
<tr>
 <td style="text-align: left;"> 
      1   </td><td style="text-align: left;">    0    </td><td style="text-align: left;"> 2 = only the left daughter is terminal</td>
</tr>
<tr>
 <td style="text-align: left;"> 
      1   </td><td style="text-align: left;">    1    </td><td style="text-align: left;"> 3 = both daughters are terminal</td>
</tr>
<tr>
 <td style="text-align: left;">
    </td>
</tr>

</table>

</li>
<li> <p><em>spltEC</em> End cut statistic for real valued variables
between [0,0.5] that is small when the split is towards the edge and
large when the split is towards the middle.  Subtracting this value
from 0.5 yields the end cut statistic studied in Ishwaran (2014) and
is a way to identify ECP behavior (end cut preference behavior).
</p>
</li>
<li> <p><em>spltST</em> Split statistic:
</p>

<ol>
<li><p> For objects of class (rfsrc, grow), this is the split
statistic that resulted in the variable being choosen for
the split.
</p>
</li>
<li><p> For an object of class (rfsrc, pred) this is the
variance of the response within the node for the test data.
This value is relevant only for real valued responses.  In
classification and survival, it is not relevant.
</p>
</li></ol>

</li></ol>
           


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'rfsrc'
stat.split(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stat.split.rfsrc_+3A_object">object</code></td>
<td>
<p>An object of class <code>(rfsrc, grow)</code>,
<code>(rfsrc, synthetic)</code>  or <code>(rfsrc,
	    predict)</code></p>
</td></tr>
<tr><td><code id="stat.split.rfsrc_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Invisibly, a list with the following components:
</p>
<table>
<tr><td><code>...</code></td>
<td>
<p>...</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Hemant Ishwaran and Udaya B. Kogalur
</p>


<h3>References</h3>

<p>Ishwaran H. (2015).  The effect of splitting on random forests.
<em>Machine Learning</em>, 99:75-118.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## run a forest, then make a call to stat.split
grow.obj &lt;- rfsrc(mpg ~., data = mtcars, membership=TRUE, statistics=TRUE)
stat.obj &lt;- stat.split(grow.obj)

## nice wrapper to extract split-statistic for desired variable
## for continuous variables plots ECP data
get.split &lt;- function(splitObj, xvar, inches = 0.1, ...) {
  which.var &lt;- which(names(splitObj[[1]]) == xvar)
  ntree &lt;- length(splitObj)
  stat &lt;- data.frame(do.call(rbind, sapply(1:ntree, function(b) {
    splitObj[[b]][which.var]})))
  dpth &lt;- stat$dpthID
  ecp &lt;- 1/2 - stat$spltEC
  sp &lt;- stat$contPT
  if (!all(is.na(sp))) {
    fgC &lt;- function(x) {
      as.numeric(as.character(cut(x, breaks = c(-1, 0.2, 0.35, 0.5),
      labels = c(1, 4, 2))))
    }
    symbols(jitter(sp), jitter(dpth), ecp, inches = inches, bg = fgC(ecp),
      xlab = xvar, ylab = "node depth", ...)
    legend("topleft", legend = c("low ecp", "med ecp", "high ecp"),
      fill = c(1, 4, 2))
   }
  invisible(stat)
}

## use get.split to investigate ECP behavior of variables
get.split(stat.obj, "disp")

</code></pre>

<hr>
<h2 id='subsample.rfsrc'>Subsample Forests for VIMP Confidence Intervals</h2><span id='topic+subsample'></span><span id='topic+subsample.rfsrc'></span><span id='topic+extract.subsample'></span><span id='topic+extract.bootsample'></span><span id='topic+print.subsample'></span><span id='topic+print.subsample.rfsrc'></span><span id='topic+print.bootsample'></span><span id='topic+print.bootsample.rfsrc'></span>

<h3>Description</h3>

<p>Use subsampling to calculate confidence intervals and standard
errors for VIMP (variable importance). Applies to all families.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'rfsrc'
subsample(obj,
  B = 100,
  block.size = 1,
  importance,
  subratio = NULL,
  stratify = TRUE,
  performance = FALSE,
  performance.only = FALSE,
  joint = FALSE,
  xvar.names = NULL,
  bootstrap = FALSE,
  verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="subsample.rfsrc_+3A_obj">obj</code></td>
<td>
<p>A forest grow object.</p>
</td></tr>
<tr><td><code id="subsample.rfsrc_+3A_b">B</code></td>
<td>
<p>Number of subsamples (or number of bootstraps).</p>
</td></tr>
<tr><td><code id="subsample.rfsrc_+3A_block.size">block.size</code></td>
<td>
<p>Specifies number of trees in a block when calculating
VIMP.  This is over-ridden if VIMP is present in the original grow
call in which case the grow value is used.</p>
</td></tr>
<tr><td><code id="subsample.rfsrc_+3A_importance">importance</code></td>
<td>
<p>Optional: specifies the type of importance to be used,
selected from one of &quot;anti&quot;, &quot;permute&quot;, &quot;random&quot;.  If not specified
reverts to default importance used by the package.  Also,
this is over-ridden if the original grow object contains importance,
in which case importance used in the original grow
call is used.</p>
</td></tr>
<tr><td><code id="subsample.rfsrc_+3A_subratio">subratio</code></td>
<td>
<p>Ratio of subsample size to original sample size.  The
default is approximately equal to the inverse square root of the
sample size.</p>
</td></tr>
<tr><td><code id="subsample.rfsrc_+3A_stratify">stratify</code></td>
<td>
<p>Use stratified subsampling?  See details below.</p>
</td></tr>
<tr><td><code id="subsample.rfsrc_+3A_performance">performance</code></td>
<td>
<p>Generalization error?  User can also request
standard error and confidence regions for generalization error.</p>
</td></tr>
<tr><td><code id="subsample.rfsrc_+3A_performance.only">performance.only</code></td>
<td>
<p>Only calculate standard error and confidence
region for the generalization error (no VIMP).</p>
</td></tr>
<tr><td><code id="subsample.rfsrc_+3A_joint">joint</code></td>
<td>
<p>Joint VIMP for all variables? Users can also request joint
VIMP for specific variables using <code>xvar.names</code>.</p>
</td></tr>
<tr><td><code id="subsample.rfsrc_+3A_xvar.names">xvar.names</code></td>
<td>
<p>Specifies variables for calculating joint VIMP.  By
default all variables are used.</p>
</td></tr>
<tr><td><code id="subsample.rfsrc_+3A_bootstrap">bootstrap</code></td>
<td>
<p>Use double bootstrap approach in place of
subsampling?  Much slower, but potentially more accurate.</p>
</td></tr>
<tr><td><code id="subsample.rfsrc_+3A_verbose">verbose</code></td>
<td>
<p>Provide verbose output?</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Using a previously trained forest, subsamples the data and constructs
subsampled forests to estimate standard errors and confidence
intervals for VIMP (Ishwaran and Lu, 2019).  If bootstrapping is
requested, a double bootstrap is applied in place of subsampling.  The
option <code>performance="TRUE"</code> constructs standard errors and
confidence regions for the error rate (OOB performance) of the trained
forest.  Options <code>joint</code> and <code>xvar.names</code> can be used to
obtain joint VIMP for all or some variables.
</p>
<p>If the trained forest does not have VIMP values, the algorithm first
needs to calculate VIMP.  Therefore, if the user plans to make
repeated calls to <code>subsample</code>, it is advisable to include VIMP in
the original grow call.  Also, by calling VIMP in the original call,
the type of importance used and other related parameters are set by
values used in the original call which can eliminate confusion about
what parameters are being used in the subsampled forests.  Thus, it is
generally advised to call VIMP in the original call.
</p>
<p>Subsampled forests are calculated using the same tuning parameters as
the original forest.  While a sophisticated algorithm is utilized to
acquire as many of these parameters as possible, keep in mind there
are some conditions where this will fail: for example there are
certain settings where the user has specified non-standard sampling in
the grow forest.
</p>
<p>Delete-d jackknife estimators of the variance (Shao and Wu, 1989) are
returned alongside subsampled variance estimators (Politis and Romano,
1994).  While these methods are closely related, the jackknife
estimator generally gives *larger* standard errors, which is a form of
bias correction, and which occurs primarily for the signal variables.
</p>
<p>By default, stratified subsampling is used for classification,
survival, and competing risk families.  For classification,
stratification is on the class label, while for survival and competing
risk, stratification is on the event type and censoring.  Users are
discouraged from over-riding this option, especially in small sample
settings, as this could lead to error due to subsampled data not
having full representation of class labels in classification settings,
and in survival settings, subsampled data may be devoid of deaths
and/or have reduced number of competing risks.  Note also that
stratified sampling is not available for multivariate families &ndash;
users should especially exercise caution when selecting subsampling
rates here.
</p>
<p>The function <code>extract.subsample</code> can be used to extract
information from the subsampled object.  Returned values for VIMP
are &quot;standardized&quot; (this means for regression families, VIMP is
standardized by dividing by the variance of Y and multiplying by 100;
for all other families, VIMP is scaled by 100).  Use
<code>standardize</code>=&quot;FALSE&quot; if you want unstandardized VIMP.  Setting
the option <code>raw</code>=&quot;TRUE&quot; returns a more complete set of
information that is used by the function <a href="#topic+plot.subsample.rfsrc">plot.subsample.rfsrc</a>
for plotting confidence intervals.  Keep in mind some of this
information will be subsampled VIMP that is &quot;raw&quot; in the sense it
equals VIMP from a forest constructed with a much smaller sample
size.  This option is for experts only.
</p>
<p>When printing or plotting results, the default is to standardize
VIMP which can be turned off using the option <code>standardize</code>.
Also these wrappers preset the &quot;alpha&quot; value used for confidence
intervals; users can change this using option <code>alpha</code>.
</p>


<h3>Value</h3>

<p>A list with the following key components:
</p>
<table>
<tr><td><code>rf</code></td>
<td>
<p>Original forest grow object.</p>
</td></tr>
<tr><td><code>vmp</code></td>
<td>
<p>Variable importance values for grow forest.</p>
</td></tr>
<tr><td><code>vmpS</code></td>
<td>
<p>Variable importance subsampled values.</p>
</td></tr>
<tr><td><code>subratio</code></td>
<td>
<p>Subratio used.</p>
</td></tr>  
</table>


<h3>Author(s)</h3>

<p>Hemant Ishwaran and Udaya B. Kogalur
</p>


<h3>References</h3>

<p>Ishwaran H. and Lu M.  (2019).  Standard errors and confidence
intervals for variable importance in random forest regression,
classification, and survival. <em>Statistics in Medicine</em>, 38,
558-582.
</p>
<p>Politis, D.N. and Romano, J.P. (1994). Large sample confidence
regions based on subsamples under minimal assumptions. <em>The
Annals of Statistics</em>, 22(4):2031-2050.
</p>
<p>Shao, J. and Wu, C.J. (1989). A general theory for jackknife variance
estimation. <em>The Annals of Statistics</em>, 17(3):1176-1197.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+holdout.vimp.rfsrc">holdout.vimp.rfsrc</a></code>
<code><a href="#topic+plot.subsample.rfsrc">plot.subsample.rfsrc</a></code>,
<code><a href="#topic+rfsrc">rfsrc</a></code>,
<code><a href="#topic+vimp.rfsrc">vimp.rfsrc</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## ------------------------------------------------------------
## regression
## ------------------------------------------------------------

## training the forest
reg.o &lt;- rfsrc(Ozone ~ ., airquality)

## default subsample call
reg.smp.o &lt;- subsample(reg.o)

## plot confidence regions
plot.subsample(reg.smp.o)

## summary of results
print(reg.smp.o)

## joint vimp and confidence region for generalization error
reg.smp.o2 &lt;- subsample(reg.o, performance = TRUE,
           joint = TRUE, xvar.names = c("Day", "Month"))
plot.subsample(reg.smp.o2)

## now try the double bootstrap (slower)
reg.dbs.o &lt;- subsample(reg.o, B = 25, bootstrap = TRUE)
print(reg.dbs.o)
plot.subsample(reg.dbs.o)

## standard error and confidence region for generalization error only
gerror &lt;- subsample(reg.o, performance.only = TRUE)
plot.subsample(gerror)

## ------------------------------------------------------------
## classification
## ------------------------------------------------------------

## 3 non-linear, 15 linear, and 5 noise variables 
if (library("caret", logical.return = TRUE)) {
  d &lt;- twoClassSim(1000, linearVars = 15, noiseVars = 5)

  ## VIMP based on (default) misclassification error
  cls.o &lt;- rfsrc(Class ~ ., d)
  cls.smp.o &lt;- subsample(cls.o, B = 100)
  plot.subsample(cls.smp.o, cex.axis = .7)

  ## same as above, but with VIMP defined using normalized Brier score
  cls.o2 &lt;- rfsrc(Class ~ ., d, perf.type = "brier")
  cls.smp.o2 &lt;- subsample(cls.o2, B = 100)
  plot.subsample(cls.smp.o2, cex.axis = .7)
}

## ------------------------------------------------------------
## class-imbalanced data using RFQ classifier with G-mean VIMP
## ------------------------------------------------------------

if (library("caret", logical.return = TRUE)) {

  ## experimental settings
  n &lt;- 1000
  q &lt;- 20
  ir &lt;- 6
  f &lt;- as.formula(Class ~ .)
 
  ## simulate the data, create minority class data
  d &lt;- twoClassSim(n, linearVars = 15, noiseVars = q)
  d$Class &lt;- factor(as.numeric(d$Class) - 1)
  idx.0 &lt;- which(d$Class == 0)
  idx.1 &lt;- sample(which(d$Class == 1), sum(d$Class == 1) / ir , replace = FALSE)
  d &lt;- d[c(idx.0,idx.1),, drop = FALSE]

  ## RFQ classifier
  oq &lt;- imbalanced(Class ~ ., d, importance = TRUE, block.size = 10)

  ## subsample the RFQ-classifier
  smp.oq &lt;- subsample(oq, B = 100)
  plot.subsample(smp.oq, cex.axis = .7)

}

## ------------------------------------------------------------
## survival 
## ------------------------------------------------------------

data(pbc, package = "randomForestSRC")
srv.o &lt;- rfsrc(Surv(days, status) ~ ., pbc)
srv.smp.o &lt;- subsample(srv.o, B = 100)
plot(srv.smp.o)

## ------------------------------------------------------------
## competing risks
## target event is death (event = 2)
## ------------------------------------------------------------

if (library("survival", logical.return = TRUE)) {
  data(pbc, package = "survival")
  pbc$id &lt;- NULL
  cr.o &lt;- rfsrc(Surv(time, status) ~ ., pbc, splitrule = "logrankCR", cause = 2)
  cr.smp.o &lt;- subsample(cr.o, B = 100)
  plot.subsample(cr.smp.o, target = 2)
}

## ------------------------------------------------------------
## multivariate 
## ------------------------------------------------------------

if (library("mlbench", logical.return = TRUE)) {
  ## simulate the data 
  data(BostonHousing)
  bh &lt;- BostonHousing
  bh$rm &lt;- factor(round(bh$rm))
  o &lt;- rfsrc(cbind(medv, rm) ~ ., bh)
  so &lt;- subsample(o)
  plot.subsample(so)
  plot.subsample(so, m.target = "rm")
  ##generalization error
  gerror &lt;- subsample(o, performance.only = TRUE)
  plot.subsample(gerror, m.target = "medv")
  plot.subsample(gerror, m.target = "rm")
}

## ------------------------------------------------------------
## largish data example - use rfsrc.fast for fast forests
## ------------------------------------------------------------

if (library("caret", logical.return = TRUE)) {
  ## largish data set
  d &lt;- twoClassSim(1000, linearVars = 15, noiseVars = 5)

  ## use a subsampled forest with Brier score performance
  ## remember to set forest=TRUE for rfsrc.fast
  o &lt;- rfsrc.fast(Class ~ ., d, ntree = 100,
           forest = TRUE, perf.type = "brier")
  so &lt;- subsample(o, B = 100)
  plot.subsample(so, cex.axis = .7)
}



</code></pre>

<hr>
<h2 id='synthetic'>Synthetic Random Forests</h2><span id='topic+synthetic.rfsrc'></span><span id='topic+synthetic'></span>

<h3>Description</h3>

<p>Grows a synthetic random forest (RF) using RF machines as synthetic
features.  Applies only to regression and classification settings.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'rfsrc'
synthetic(formula, data, object, newdata,
  ntree = 1000, mtry = NULL, nodesize = 5, nsplit = 10,
  mtrySeq = NULL, nodesizeSeq = c(1:10,20,30,50,100),
  min.node = 3,
  fast = TRUE,
  use.org.features = TRUE,
  na.action = c("na.omit", "na.impute"),
  oob = TRUE,
  verbose = TRUE,
  ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="synthetic_+3A_formula">formula</code></td>
<td>
<p>Model to be fit. Must be specified unless <code>object</code> is given.</p>
</td></tr>
<tr><td><code id="synthetic_+3A_data">data</code></td>
<td>
<p>Data frame containing the y-outcome and x-variables in
the model. Must be specified unless <code>object</code> is given.</p>
</td></tr>
<tr><td><code id="synthetic_+3A_object">object</code></td>
<td>
<p>An object of class <code>(rfsrc, synthetic)</code>.
Not required when <code>formula</code> and <code>data</code> are supplied.</p>
</td></tr>
<tr><td><code id="synthetic_+3A_newdata">newdata</code></td>
<td>
<p>Test data used for prediction (optional).</p>
</td></tr>
<tr><td><code id="synthetic_+3A_ntree">ntree</code></td>
<td>
<p>Number of trees.</p>
</td></tr>
<tr><td><code id="synthetic_+3A_mtry">mtry</code></td>
<td>
<p>mtry value for over-arching synthetic forest.</p>
</td></tr>
<tr><td><code id="synthetic_+3A_nodesize">nodesize</code></td>
<td>
<p>Nodesize value for over-arching synthetic forest.</p>
</td></tr>
<tr><td><code id="synthetic_+3A_nsplit">nsplit</code></td>
<td>
<p>nsplit-randomized splitting for significantly increased speed.</p>
</td></tr>
<tr><td><code id="synthetic_+3A_mtryseq">mtrySeq</code></td>
<td>
<p>Sequence of mtry values used for fitting the
collection of RF machines.  If <code>NULL</code>, default is number of
variables divided by 3, rounded up.</p>
</td></tr>
<tr><td><code id="synthetic_+3A_nodesizeseq">nodesizeSeq</code></td>
<td>
<p>Sequence of nodesize values used for the fitting the
collection of RF machines.</p>
</td></tr>
<tr><td><code id="synthetic_+3A_min.node">min.node</code></td>
<td>
<p>Minimum forest averaged number of nodes a RF machine
must exceed in order to be used as a synthetic feature.</p>
</td></tr>
<tr><td><code id="synthetic_+3A_fast">fast</code></td>
<td>
<p>Use fast random forests, <code>rfsrc.fast</code>, in place of
<code>rfsrc</code>?  Improves speed but may be less accurate.</p>
</td></tr>
<tr><td><code id="synthetic_+3A_use.org.features">use.org.features</code></td>
<td>
<p>In addition to synthetic features, should
the original features be used when fitting synthetic forests?</p>
</td></tr>
<tr><td><code id="synthetic_+3A_na.action">na.action</code></td>
<td>
<p>Missing value action. The default <code>na.omit</code>
removes the entire record if even one of its entries is <code>NA</code>.
The action <code>na.impute</code> pre-imputes the data using fast
imputation via <code>impute.rfsrc</code>.</p>
</td></tr>
<tr><td><code id="synthetic_+3A_oob">oob</code></td>
<td>
<p>Preserve &quot;out-of-bagness&quot; so that error rates and
VIMP are honest?  Default is yes (<span class="option">oob=TRUE</span>).</p>
</td></tr>
<tr><td><code id="synthetic_+3A_verbose">verbose</code></td>
<td>
<p>Set to <code>TRUE</code> for verbose output.</p>
</td></tr>
<tr><td><code id="synthetic_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed to the <code>rfsrc</code>
function used for fitting the synthetic forest.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A collection of random forests are fit using different nodesize
values.  The predicted values from these machines are then used as
synthetic features (called RF machines) to fit a synthetic random
forest (the original features are also used in constructing the
synthetic forest).  Currently only implemented for regression and
classification settings (univariate and multivariate).
</p>
<p>Synthetic features are calculated using out-of-bag (OOB) data to avoid
over-using training data.  However, to guarantee that performance
values such as error rates and VIMP are honest, bootstrap draws are
fixed across all trees used in the construction of the synthetic
forest and its synthetic features.  The option <span class="option">oob=TRUE</span>
ensures that this happens.  Change this option at your own peril.
</p>
<p>If values for <code>mtrySeq</code> are given, RF machines are constructed
for each combination of nodesize and mtry values specified by
<code>nodesizeSeq</code> <code>mtrySeq</code>.
</p>


<h3>Value</h3>

<p>A list with the following components:
</p>
<table>
<tr><td><code>rfMachines</code></td>
<td>
<p>RF machines used to construct the synthetic
features.</p>
</td></tr>
<tr><td><code>rfSyn</code></td>
<td>
<p>The (grow) synthetic RF built over training data.</p>
</td></tr>
<tr><td><code>rfSynPred</code></td>
<td>
<p>The predict synthetic RF built over test data (if available).</p>
</td></tr>
<tr><td><code>synthetic</code></td>
<td>
<p>List containing the synthetic features.</p>
</td></tr>
<tr><td><code>opt.machine</code></td>
<td>
<p>Optimal machine: RF machine with smallest
OOB error rate.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Hemant Ishwaran and Udaya B. Kogalur
</p>


<h3>References</h3>

<p>Ishwaran H. and Malley J.D. (2014).  Synthetic learning machines.
<em>BioData Mining</em>, 7:28.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rfsrc">rfsrc</a></code>,
<code><a href="#topic+rfsrc.fast">rfsrc.fast</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## ------------------------------------------------------------
## compare synthetic forests to regular forest (classification)
## ------------------------------------------------------------

## rfsrc and synthetic calls
if (library("mlbench", logical.return = TRUE)) {

  ## simulate the data 
  ring &lt;- data.frame(mlbench.ringnorm(250, 20))

  ## classification forests
  ringRF &lt;- rfsrc(classes ~., ring)

  ## synthetic forests
  ## 1 = nodesize varied
  ## 2 = nodesize/mtry varied
  ringSyn1 &lt;- synthetic(classes ~., ring)
  ringSyn2 &lt;- synthetic(classes ~., ring, mtrySeq = c(1, 10, 20))

  ## test-set performance
  ring.test &lt;- data.frame(mlbench.ringnorm(500, 20))
  pred.ringRF &lt;- predict(ringRF, newdata = ring.test)
  pred.ringSyn1 &lt;- synthetic(object = ringSyn1, newdata = ring.test)$rfSynPred
  pred.ringSyn2 &lt;- synthetic(object = ringSyn2, newdata = ring.test)$rfSynPred


  print(pred.ringRF)
  print(pred.ringSyn1)
  print(pred.ringSyn2)

}

## ------------------------------------------------------------
## compare synthetic forest to regular forest (regression)
## ------------------------------------------------------------

## simulate the data
n &lt;- 250
ntest &lt;- 1000
N &lt;- n + ntest
d &lt;- 50
std &lt;- 0.1
x &lt;- matrix(runif(N * d, -1, 1), ncol = d)
y &lt;- 1 * (x[,1] + x[,4]^3 + x[,9] + sin(x[,12]*x[,18]) + rnorm(n, sd = std)&gt;.38)
dat &lt;- data.frame(x = x, y = y)
test &lt;- (n+1):N

## regression forests
regF &lt;- rfsrc(y ~ ., dat[-test, ], )
pred.regF &lt;- predict(regF, dat[test, ])

## synthetic forests using fast rfsrc
synF1 &lt;- synthetic(y ~ ., dat[-test, ], newdata = dat[test, ])
synF2 &lt;- synthetic(y ~ ., dat[-test, ],
  newdata = dat[test, ], mtrySeq = c(1, 10, 20, 30, 40, 50))

## standardized MSE performance
mse &lt;- c(tail(pred.regF$err.rate, 1),
         tail(synF1$rfSynPred$err.rate, 1),
         tail(synF2$rfSynPred$err.rate, 1)) / var(y[-test])
names(mse) &lt;- c("forest", "synthetic1", "synthetic2")
print(mse)

## ------------------------------------------------------------
## multivariate synthetic forests
## ------------------------------------------------------------

mtcars.new &lt;- mtcars
mtcars.new$cyl &lt;- factor(mtcars.new$cyl)
mtcars.new$carb &lt;- factor(mtcars.new$carb, ordered = TRUE)
trn &lt;- sample(1:nrow(mtcars.new), nrow(mtcars.new)/2)
mvSyn &lt;- synthetic(cbind(carb, mpg, cyl) ~., mtcars.new[trn,])
mvSyn.pred &lt;- synthetic(object = mvSyn, newdata = mtcars.new[-trn,])

</code></pre>

<hr>
<h2 id='tune.rfsrc'>Tune Random Forest for the optimal mtry and nodesize parameters</h2><span id='topic+tune.rfsrc'></span><span id='topic+tune'></span><span id='topic+tune.nodesize.rfsrc'></span><span id='topic+tune.nodesize'></span>

<h3>Description</h3>

<p>Finds the optimal mtry and nodesize tuning parameter for a random
forest using out-of-sample error.  Applies to all families. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
## S3 method for class 'rfsrc'
tune(formula, data,
  mtryStart = ncol(data) / 2,
  nodesizeTry = c(1:9, seq(10, 100, by = 5)), ntreeTry = 100,
  sampsize = function(x){min(x * .632, max(150, x ^ (3/4)))},
  nsplit = 1, stepFactor = 1.25, improve = 1e-3, strikeout = 3, maxIter = 25,
  trace = FALSE, doBest = FALSE, ...)

## S3 method for class 'rfsrc'
tune.nodesize(formula, data,
  nodesizeTry = c(1:9, seq(10, 150, by = 5)), ntreeTry = 100,
  sampsize = function(x){min(x * .632, max(150, x ^ (4/5)))},
  nsplit = 1, trace = TRUE, ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tune.rfsrc_+3A_formula">formula</code></td>
<td>
<p>A symbolic description of the model to be fit.</p>
</td></tr>
<tr><td><code id="tune.rfsrc_+3A_data">data</code></td>
<td>
<p>Data frame containing the y-outcome and x-variables.</p>
</td></tr> 
<tr><td><code id="tune.rfsrc_+3A_mtrystart">mtryStart</code></td>
<td>
<p>Starting value of mtry.</p>
</td></tr>
<tr><td><code id="tune.rfsrc_+3A_nodesizetry">nodesizeTry</code></td>
<td>
<p>Values of nodesize optimized over.</p>
</td></tr>
<tr><td><code id="tune.rfsrc_+3A_ntreetry">ntreeTry</code></td>
<td>
<p>Number of trees used for the tuning step.</p>
</td></tr>
<tr><td><code id="tune.rfsrc_+3A_sampsize">sampsize</code></td>
<td>
<p>Function specifying requested size of subsampled data.
Can also be passed in as a number.</p>
</td></tr>
<tr><td><code id="tune.rfsrc_+3A_nsplit">nsplit</code></td>
<td>
<p>Number of random splits used for splitting.</p>
</td></tr>
<tr><td><code id="tune.rfsrc_+3A_stepfactor">stepFactor</code></td>
<td>
<p>At each iteration, mtry is inflated (or deflated) by
this value.</p>
</td></tr>
<tr><td><code id="tune.rfsrc_+3A_improve">improve</code></td>
<td>
<p>The (relative) improvement in out-of-sample error must be by this
much for the search to continue.</p>
</td></tr>
<tr><td><code id="tune.rfsrc_+3A_strikeout">strikeout</code></td>
<td>
<p>The search is discontinued when the relative
improvement in OOB error is negative.  However <code>strikeout</code>
allows for some tolerance in this.  If a negative improvement is
noted a total of <code>strikeout</code> times, the search is stopped.
Increase this value only if you want an exhaustive search.</p>
</td></tr>
<tr><td><code id="tune.rfsrc_+3A_maxiter">maxIter</code></td>
<td>
<p>The maximum number of iterations allowed for each mtry
bisection search.</p>
</td></tr>
<tr><td><code id="tune.rfsrc_+3A_trace">trace</code></td>
<td>
<p>Print the progress of the search?</p>
</td></tr>
<tr><td><code id="tune.rfsrc_+3A_dobest">doBest</code></td>
<td>
<p>Return a forest fit with the optimal mtry and nodesize parameters?</p>
</td></tr>
<tr><td><code id="tune.rfsrc_+3A_...">...</code></td>
<td>
<p>Further options to be passed to <code><a href="#topic+rfsrc.fast">rfsrc.fast</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>tune</code> returns a matrix whose first and second
columns contain the nodesize and mtry values searched and whose third
column is the corresponding out-of-sample error.  Uses standardized error
and in the case of multivariate forests it is the averaged
standardized rror over the outcomes and for competing risks it is
the averaged standardized error over the event types.
</p>
<p>If <code>doBest=TRUE</code>, also returns a forest object fit using the
optimal <code>mtry</code> and <code>nodesize</code> values.
</p>
<p>All calculations (including the final optimized forest) are based on
the fast forest interface <code><a href="#topic+rfsrc.fast">rfsrc.fast</a></code> which utilizes
subsampling.  However, while this yields a fast optimization strategy,
such a solution can only be considered approximate.  Users may wish to
tweak various options to improve accuracy.  Increasing the default
<code>sampsize</code> will definitely help. Increasing <code>ntreeTry</code>
(which is set to 100 for speed) may also help.  It is also useful to
look at contour plots of the out-of-sample error as a function of
<code>mtry</code> and <code>nodesize</code> (see example below) to identify
regions of the parameter space where error rate is small.
</p>
<p><code>tune.nodesize</code> returns the optimal nodesize where optimization is
over <code>nodesize</code> only.  
</p>


<h3>Author(s)</h3>

<p>Hemant Ishwaran and Udaya B. Kogalur
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rfsrc.fast">rfsrc.fast</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## ------------------------------------------------------------
## White wine classification example
## ------------------------------------------------------------

## load the data
data(wine, package = "randomForestSRC")
wine$quality &lt;- factor(wine$quality)

## set the sample size manually
o &lt;- tune(quality ~ ., wine, sampsize = 100)

## here is the optimized forest 
print(o$rf)

## visualize the nodesize/mtry OOB surface
if (library("interp", logical.return = TRUE)) {

  ## nice little wrapper for plotting results
  plot.tune &lt;- function(o, linear = TRUE) {
    x &lt;- o$results[,1]
    y &lt;- o$results[,2]
    z &lt;- o$results[,3]
    so &lt;- interp(x=x, y=y, z=z, linear = linear)
    idx &lt;- which.min(z)
    x0 &lt;- x[idx]
    y0 &lt;- y[idx]
    filled.contour(x = so$x,
                   y = so$y,
                   z = so$z,
                   xlim = range(so$x, finite = TRUE) + c(-2, 2),
                   ylim = range(so$y, finite = TRUE) + c(-2, 2),
                   color.palette =
                     colorRampPalette(c("yellow", "red")),
                   xlab = "nodesize",
                   ylab = "mtry",
                   main = "error rate for nodesize and mtry",
                   key.title = title(main = "OOB error", cex.main = 1),
                   plot.axes = {axis(1);axis(2);points(x0,y0,pch="x",cex=1,font=2);
                                points(x,y,pch=16,cex=.25)})
  }

  ## plot the surface
  plot.tune(o)

}

## ------------------------------------------------------------
## tuning for class imbalanced data problem
## - see imbalanced function for details
## - use rfq and perf.type = "gmean" 
## ------------------------------------------------------------

data(breast, package = "randomForestSRC")
breast &lt;- na.omit(breast)
o &lt;- tune(status ~ ., data = breast, rfq = TRUE, perf.type = "gmean")
print(o)


## ------------------------------------------------------------
## tune nodesize for competing risk - wihs data 
## ------------------------------------------------------------

data(wihs, package = "randomForestSRC")
plot(tune.nodesize(Surv(time, status) ~ ., wihs, trace = TRUE)$err)

</code></pre>

<hr>
<h2 id='var.select.rfsrc'>Variable Selection</h2><span id='topic+var.select.rfsrc'></span><span id='topic+var.select'></span>

<h3>Description</h3>

<p>Variable selection using minimal depth.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'rfsrc'
var.select(formula,
  data,
  object,
  cause,
  m.target,
  method = c("md", "vh", "vh.vimp"),
  conservative = c("medium", "low", "high"),
  ntree = (if (method == "md") 1000 else 500),
  mvars = (if (method != "md") ceiling(ncol(data)/5) else NULL),
  mtry = (if (method == "md") ceiling(ncol(data)/3) else NULL),
  nodesize = 2, splitrule = NULL, nsplit = 10, xvar.wt = NULL,
  refit = (method != "md"), fast = FALSE,
  na.action = c("na.omit", "na.impute"),
  always.use = NULL, nrep = 50, K = 5, nstep = 1,
  prefit =  list(action = (method != "md"), ntree = 100,
  mtry = 500, nodesize = 3, nsplit = 1),
  verbose = TRUE, block.size = 10, seed = NULL,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="var.select.rfsrc_+3A_formula">formula</code></td>
<td>
<p>A symbolic description of the model to be fit. 
Must be specified unless <code>object</code> is given.</p>
</td></tr>
<tr><td><code id="var.select.rfsrc_+3A_data">data</code></td>
<td>
<p>Data frame containing the y-outcome and x-variables in
the model. Must be specified unless <code>object</code> is given.</p>
</td></tr>
<tr><td><code id="var.select.rfsrc_+3A_object">object</code></td>
<td>
<p>An object of class <code>(rfsrc, grow)</code>. 
Not required when <code>formula</code> and <code>data</code> are supplied.</p>
</td></tr>
<tr><td><code id="var.select.rfsrc_+3A_cause">cause</code></td>
<td>
<p>Integer value between 1 and <code>J</code> indicating
the event of interest for competing risks, where <code>J</code> is
the number of event types (this option applies only to
competing risk families).  The default is to use the first
event type.</p>
</td></tr>
<tr><td><code id="var.select.rfsrc_+3A_m.target">m.target</code></td>
<td>
<p>Character value for multivariate families
specifying the target outcome to be used.  If left unspecified, the
algorithm will choose a default target.</p>
</td></tr>
<tr><td><code id="var.select.rfsrc_+3A_method">method</code></td>
<td>
<p>Variable selection method:
</p>
  
<dl>
<dt><code>md</code>:</dt><dd><p>minimal depth (default).</p>
</dd>
<dt><code>vh</code>:</dt><dd><p>variable hunting.</p>
</dd>
<dt><code>vh.vimp</code>:</dt><dd><p>variable hunting with VIMP (variable
importance).</p>
</dd>
</dl>

</td></tr>
<tr><td><code id="var.select.rfsrc_+3A_conservative">conservative</code></td>
<td>
<p>Level of conservativeness of the thresholding
rule used in minimal depth selection:
</p>

<dl>
<dt><code>high</code>:</dt><dd><p>Use the most conservative threshold.</p>
</dd>
<dt><code>medium</code>:</dt><dd><p>Use the default less conservative tree-averaged
threshold.</p>
</dd>
<dt><code>low</code>:</dt><dd><p>Use the more liberal one standard error rule.</p>
</dd>
</dl>

</td></tr>
<tr><td><code id="var.select.rfsrc_+3A_ntree">ntree</code></td>
<td>
<p>Number of trees to grow.</p>
</td></tr>
<tr><td><code id="var.select.rfsrc_+3A_mvars">mvars</code></td>
<td>
<p>Number of randomly selected variables used in the
variable hunting algorithm (ignored when <span class="option">method="md"</span>).</p>
</td></tr>
<tr><td><code id="var.select.rfsrc_+3A_mtry">mtry</code></td>
<td>
<p>The mtry value used.</p>
</td></tr>
<tr><td><code id="var.select.rfsrc_+3A_nodesize">nodesize</code></td>
<td>
<p>Forest average terminal node size.</p>
</td></tr>
<tr><td><code id="var.select.rfsrc_+3A_splitrule">splitrule</code></td>
<td>
<p>Splitting rule used.</p>
</td></tr>
<tr><td><code id="var.select.rfsrc_+3A_nsplit">nsplit</code></td>
<td>
<p>If non-zero, the specified tree splitting rule is
randomized which significantly increases speed.</p>
</td></tr>
<tr><td><code id="var.select.rfsrc_+3A_xvar.wt">xvar.wt</code></td>
<td>
<p>Vector of non-negative weights specifying the
probability of selecting a variable for splitting a node.  Must be of
dimension equal to the number of variables.  Default (<code>NULL</code>)
invokes uniform weighting or a data-adaptive method depending on
<code>prefit$action</code>.</p>
</td></tr>
<tr><td><code id="var.select.rfsrc_+3A_refit">refit</code></td>
<td>
<p>Should a forest be refit using the selected variables?</p>
</td></tr>
<tr><td><code id="var.select.rfsrc_+3A_fast">fast</code></td>
<td>
<p>Speeds up the cross-validation used for variable hunting
for a faster analysis.  See miscellanea below.</p>
</td></tr>
<tr><td><code id="var.select.rfsrc_+3A_na.action">na.action</code></td>
<td>
<p>Action to be taken if the data contains <code>NA</code> values.</p>
</td></tr>
<tr><td><code id="var.select.rfsrc_+3A_always.use">always.use</code></td>
<td>
<p>Character vector of variable names to always
be included in the model selection procedure and in the final
selected model.</p>
</td></tr>
<tr><td><code id="var.select.rfsrc_+3A_nrep">nrep</code></td>
<td>
<p>Number of Monte Carlo iterations of the variable hunting algorithm.</p>
</td></tr>
<tr><td><code id="var.select.rfsrc_+3A_k">K</code></td>
<td>
<p>Integer value specifying the <code>K</code>-fold size used in the variable hunting
algorithm.</p>
</td></tr>
<tr><td><code id="var.select.rfsrc_+3A_nstep">nstep</code></td>
<td>
<p>Integer value controlling the step size used in the
forward selection process of the variable hunting algorithm.
Increasing this will encourage more variables to be selected.</p>
</td></tr>
<tr><td><code id="var.select.rfsrc_+3A_prefit">prefit</code></td>
<td>
<p>List containing parameters used in preliminary forest
analysis for determining weight selection of variables.  Users can
set all or some of the following parameters:
</p>

<dl>
<dt><code>action</code>:</dt><dd><p>Determines how (or if) the preliminary forest is
fit.  See details below.</p>
</dd>
<dt><code>ntree</code>:</dt><dd><p>Number of trees used in the preliminary analysis.</p>
</dd>
<dt><code>mtry</code>:</dt><dd><p>mtry used in the preliminary analysis.</p>
</dd>
<dt><code>nodesize</code>:</dt><dd><p>nodesize used in the preliminary analysis.</p>
</dd>
<dt><code>nsplit</code>:</dt><dd><p>nsplit value used in the preliminary analysis.</p>
</dd>
</dl>

</td></tr>  
<tr><td><code id="var.select.rfsrc_+3A_verbose">verbose</code></td>
<td>
<p>Set to <code>TRUE</code> for verbose output.</p>
</td></tr>
<tr><td><code id="var.select.rfsrc_+3A_block.size">block.size</code></td>
<td>
<p>VIMP is calculated in &quot;blocks&quot; of trees of this size.</p>
</td></tr>
<tr><td><code id="var.select.rfsrc_+3A_seed">seed</code></td>
<td>
<p>Negative integer specifying seed for the random number generator.</p>
</td></tr>
<tr><td><code id="var.select.rfsrc_+3A_...">...</code></td>
<td>
<p>Further arguments passed to forest grow call.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function implements random forest variable selection using
tree minimal depth methodology (Ishwaran et al., 2010).  The option
<span class="option">method</span> allows for two different approaches:
</p>

<ol>
<li> <p><span class="option">method="md"</span>
</p>
<p>Invokes minimal depth variable selection.  Variables are selected
using minimal depth variable selection.  Uses all data and all
variables simultaneously.  This is basically a front-end to the
<code>max.subtree</code> wrapper.  Users should consult the
<code>max.subtree</code> help file for details.
</p>
<p>Set <span class="option">mtry</span> to larger values in high-dimensional problems.
</p>
</li>
<li> <p><span class="option">method="vh"</span> or <span class="option">method="vh.vimp"</span>
</p>
<p>Invokes variable hunting.  Variable hunting is used for problems
where the number of variables is substantially larger than the
sample size (e.g., p/n is greater than 10).  It is always prefered
to use <span class="option">method="md"</span>, but to find more variables, or when
computations are high, variable hunting may be preferred.
</p>
<p>When <span class="option">method="vh"</span>: Using training data from a stratified
K-fold subsampling (stratification based on the y-outcomes), a
forest is fit using <code>mvars</code> randomly selected variables
(variables are chosen with probability proportional to weights
determined using an initial forest fit; see below for more
details).  The <code>mvars</code> variables are ordered by increasing
minimal depth and added sequentially (starting from an initial
model determined using minimal depth selection) until joint VIMP
no longer increases (signifying the final model).  A forest is
refit to the final model and applied to test data to estimate
prediction error.  The process is repeated <code>nrep</code> times.
Final selected variables are the top P ranked variables, where P
is the average model size (rounded up to the nearest integer) and
variables are ranked by frequency of occurrence.
</p>
<p>The same algorithm is used when <span class="option">method="vh.vimp"</span>, but
variables are ordered using VIMP.  This is faster, but not as
accurate.
</p>
</li></ol>

<p><em>Miscellanea</em>
</p>

<ol>
<li><p> When variable hunting is used, a preliminary forest is run
and its VIMP is used to define the probability of selecting a
variable for splitting a node.  Thus, instead of randomly
selecting <code>mvars</code> at random, variables are selected with
probability proportional to their VIMP (the probability is zero
if VIMP is negative).  A preliminary forest is run once prior
to the analysis if <code>prefit$action=TRUE</code>, otherwise it is
run prior to each iteration (this latter scenario can be slow).
When <span class="option">method="md"</span>, a preliminary forest is fit only if
<code>prefit$action=TRUE</code>.  Then instead of randomly selecting
<code>mtry</code> variables at random, <code>mtry</code> variables are
selected with probability proportional to their VIMP.  In all
cases, the entire option is overridden if <code>xvar.wt</code> is
non-null.
</p>
</li>
<li><p> If <code>object</code> is supplied and <span class="option">method="md"</span>,
the grow forest from <code>object</code> is parsed for minimal depth
information.  While this avoids fitting another forest, thus
saving computational time, certain options no longer apply.  In
particular, the value of <code>cause</code> plays no role in the
final selected variables as minimal depth is extracted from the
grow forest, which has already been grown under a preselected
<code>cause</code> specification.  Users wishing to specify
<code>cause</code> should instead use the formula and data interface.
Also, if the user requests a prefitted forest via
<code>prefit$action=TRUE</code>, then <code>object</code> is not used and a
refitted forest is used in its place for variable selection.
Thus, the effort spent to construct the original grow forest is
not used in this case.
</p>
</li>
<li><p> If <span class="option">fast=TRUE</span>, and variable hunting is used, the
training data is chosen to be of size n/K, where n=sample size
(i.e., the size of the training data is swapped with the test
data).  This speeds up the algorithm.  Increasing K also helps.
</p>
</li>
<li><p> Can be used for competing risk data.  When
<span class="option">method="vh.vimp"</span>, variable selection based on VIMP is
confined to an event specific cause specified by <code>cause</code>.
However, this can be unreliable as not all y-outcomes can be
guaranteed when subsampling (this is true even when stratifed
subsampling is used as done here).
</p>
</li></ol>



<h3>Value</h3>

<p>Invisibly, a list with the following components:
</p>
<table>
<tr><td><code>err.rate</code></td>
<td>
<p>Prediction error for the forest (a vector of
length <code>nrep</code> if variable hunting is used).</p>
</td></tr>
<tr><td><code>modelsize</code></td>
<td>
<p>Number of variables selected.</p>
</td></tr>
<tr><td><code>topvars</code></td>
<td>
<p>Character vector of names of the final selected variables.</p>
</td></tr>  
<tr><td><code>varselect</code></td>
<td>
<p>Useful output summarizing the final selected variables.</p>
</td></tr>
<tr><td><code>rfsrc.refit.obj</code></td>
<td>
<p>Refitted forest using the final set of selected variables  
(requires <span class="option">refit=TRUE</span>).</p>
</td></tr>
<tr><td><code>md.obj</code></td>
<td>
<p>Minimal depth object.  <code>NULL</code> unless <span class="option">method="md"</span>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Hemant Ishwaran and Udaya B. Kogalur
</p>


<h3>References</h3>

<p>Ishwaran H., Kogalur U.B., Gorodeski E.Z, Minn A.J. and 
Lauer M.S. (2010).  High-dimensional variable selection for survival
data.  <em>J. Amer. Statist. Assoc.</em>, 105:205-217.
</p>
<p>Ishwaran H., Kogalur U.B., Chen X. and Minn A.J. (2011).  Random
survival forests for high-dimensional data. <em>Statist. Anal. Data
Mining</em>, 4:115-132.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+find.interaction.rfsrc">find.interaction.rfsrc</a></code>,
<code><a href="#topic+holdout.vimp.rfsrc">holdout.vimp.rfsrc</a></code>,
<code><a href="#topic+max.subtree.rfsrc">max.subtree.rfsrc</a></code>,
<code><a href="#topic+vimp.rfsrc">vimp.rfsrc</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## ------------------------------------------------------------
## Minimal depth variable selection
## survival analysis
## use larger node size which is better for minimal depth
## ------------------------------------------------------------

data(pbc, package = "randomForestSRC")
pbc.obj &lt;- rfsrc(Surv(days, status) ~ ., pbc, nodesize = 20, importance = TRUE)

# default call corresponds to minimal depth selection
vs.pbc &lt;- var.select(object = pbc.obj)
topvars &lt;- vs.pbc$topvars

# the above is equivalent to
max.subtree(pbc.obj)$topvars

# different levels of conservativeness
var.select(object = pbc.obj, conservative = "low")
var.select(object = pbc.obj, conservative = "medium")
var.select(object = pbc.obj, conservative = "high")

## ------------------------------------------------------------
## Minimal depth variable selection
## competing risk analysis
## use larger node size which is better for minimal depth
## ------------------------------------------------------------

## competing risk data set involving AIDS in women
data(wihs, package = "randomForestSRC")
vs.wihs &lt;- var.select(Surv(time, status) ~ ., wihs, nsplit = 3, 
                      nodesize = 20, ntree = 100, importance = TRUE)

## competing risk analysis of pbc data from survival package
## implement cause-specific variable selection 
if (library("survival", logical.return = TRUE)) {
  data(pbc, package = "survival")
  pbc$id &lt;- NULL
  var.select(Surv(time, status) ~ ., pbc, cause = 1)
  var.select(Surv(time, status) ~ ., pbc, cause = 2)
}

## ------------------------------------------------------------
## Minimal depth variable selection
## classification analysis
## ------------------------------------------------------------

vs.iris &lt;- var.select(Species ~ ., iris)

## ------------------------------------------------------------
## Variable hunting high-dimensional example
## van de Vijver microarray breast cancer survival data
## nrep is small for illustration; typical values are nrep = 100
## ------------------------------------------------------------

data(vdv, package = "randomForestSRC")
vh.breast &lt;- var.select(Surv(Time, Censoring) ~ ., vdv,
      method = "vh", nrep = 10, nstep = 5)

# plot top 10 variables
plot.variable(vh.breast$rfsrc.refit.obj,
  xvar.names = vh.breast$topvars[1:10])
plot.variable(vh.breast$rfsrc.refit.obj,
  xvar.names = vh.breast$topvars[1:10], partial = TRUE)

## similar analysis, but using weights from univarate cox p-values
if (library("survival", logical.return = TRUE))
{
  cox.weights &lt;- function(rfsrc.f, rfsrc.data) {
    event.names &lt;- all.vars(rfsrc.f)[1:2]
    p &lt;- ncol(rfsrc.data) - 2
    event.pt &lt;- match(event.names, names(rfsrc.data))
    xvar.pt &lt;- setdiff(1:ncol(rfsrc.data), event.pt)
    sapply(1:p, function(j) {
      cox.out &lt;- coxph(rfsrc.f, rfsrc.data[, c(event.pt, xvar.pt[j])])
      pvalue &lt;- summary(cox.out)$coef[5]
      if (is.na(pvalue)) 1.0 else 1/(pvalue + 1e-100)
    })
  }       
  data(vdv, package = "randomForestSRC")
  rfsrc.f &lt;- as.formula(Surv(Time, Censoring) ~ .)
  cox.wts &lt;- cox.weights(rfsrc.f, vdv)
  vh.breast.cox &lt;- var.select(rfsrc.f, vdv, method = "vh", nstep = 5,
    nrep = 10, xvar.wt = cox.wts)
}


</code></pre>

<hr>
<h2 id='vdv'>van de Vijver Microarray Breast Cancer</h2><span id='topic+vdv'></span>

<h3>Description</h3>

<p>Gene expression profiling for predicting clinical outcome of breast
cancer (van't Veer et al., 2002).  Microarray breast cancer data set
of 4707 expression values on 78 patients with survival information.
</p>


<h3>References</h3>

<p>van't Veer L.J. et al. (2002).  Gene expression profiling predicts
clinical outcome of breast cancer.  <em>Nature</em>, <b>12</b>,
530&ndash;536.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(vdv, package = "randomForestSRC")</code></pre>

<hr>
<h2 id='veteran'>Veteran's Administration Lung Cancer Trial</h2><span id='topic+veteran'></span>

<h3>Description</h3>

<p>Randomized trial of two treatment regimens for lung cancer.
This is a standard survival analysis data set.
</p>


<h3>Source</h3>

<p>Kalbfleisch and Prentice, <em>The Statistical
Analysis of Failure Time Data.</em></p>


<h3>References</h3>

<p>Kalbfleisch J. and Prentice R, (1980) <em>The Statistical
Analysis of Failure Time Data.</em>  New York: Wiley.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(veteran, package = "randomForestSRC")</code></pre>

<hr>
<h2 id='vimp.rfsrc'>VIMP for Single or Grouped Variables</h2><span id='topic+vimp.rfsrc'></span><span id='topic+vimp'></span>

<h3>Description</h3>

<p>Calculate variable importance (VIMP) for a single variable or group of
variables for training or test data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'rfsrc'
vimp(object, xvar.names, m.target = NULL, 
  importance = c("anti", "permute", "random"), block.size = 10,
  joint = FALSE, seed = NULL, do.trace = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vimp.rfsrc_+3A_object">object</code></td>
<td>
<p>An object of class <code>(rfsrc, grow)</code> or
<code>(rfsrc, forest)</code>. Requires <span class="option">forest=TRUE</span> in the
original <code>rfsrc</code> call.</p>
</td></tr>
<tr><td><code id="vimp.rfsrc_+3A_xvar.names">xvar.names</code></td>
<td>
<p>Names of the x-variables to be used.  If not
specified all variables are used.</p>
</td></tr>
<tr><td><code id="vimp.rfsrc_+3A_m.target">m.target</code></td>
<td>
<p>Character value for multivariate families
specifying the target outcome to be used.  If left unspecified, the
algorithm will choose a default target.</p>
</td></tr>
<tr><td><code id="vimp.rfsrc_+3A_importance">importance</code></td>
<td>
<p>Type of VIMP.</p>
</td></tr>
<tr><td><code id="vimp.rfsrc_+3A_block.size">block.size</code></td>
<td>
<p>Specifies number of trees in a block when calculating
VIMP.</p>
</td></tr>
<tr><td><code id="vimp.rfsrc_+3A_joint">joint</code></td>
<td>
<p>Individual or joint VIMP?</p>
</td></tr>
<tr><td><code id="vimp.rfsrc_+3A_seed">seed</code></td>
<td>
<p>Negative integer specifying seed for the random number
generator.</p>
</td></tr> 
<tr><td><code id="vimp.rfsrc_+3A_do.trace">do.trace</code></td>
<td>
<p>Number of seconds between updates to the user on
approximate time to completion.</p>
</td></tr>
<tr><td><code id="vimp.rfsrc_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Using a previously trained forest, calculate the VIMP for variables
<code>xvar.names</code>.  By default, VIMP is calculated for the original
data, but the user can specify a new test data for the VIMP
calculation using <code>newdata</code>.  See <code>rfsrc</code> for more
details about how VIMP is calculated.
</p>
<p><span class="option">joint=TRUE</span> returns joint VIMP, defined as importance for
a group of variables when the group is perturbed simultaneously.
</p>
<p><code>csv=TRUE</code> return case specific VIMP.  Applies to
all families except survival families.  See example below.  
</p>


<h3>Value</h3>

<p>An object of class <code>(rfsrc, predict)</code> containing importance
values.
</p>


<h3>Author(s)</h3>

<p>Hemant Ishwaran and Udaya B. Kogalur
</p>


<h3>References</h3>

<p>Ishwaran H. (2007).  Variable importance in binary regression
trees and forests,  <em>Electronic J. Statist.</em>, 1:519-537.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+holdout.vimp.rfsrc">holdout.vimp.rfsrc</a></code>,
<code><a href="#topic+rfsrc">rfsrc</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## ------------------------------------------------------------
## classification example
## showcase different vimp
## ------------------------------------------------------------

iris.obj &lt;- rfsrc(Species ~ ., data = iris)

## anti vimp (default)
print(vimp(iris.obj)$importance)

## anti vimp using brier prediction error
print(vimp(iris.obj, perf.type = "brier")$importance)

## permutation vimp
print(vimp(iris.obj, importance = "permute")$importance)

## random daughter vimp
print(vimp(iris.obj, importance = "random")$importance)

## joint anti vimp 
print(vimp(iris.obj, joint = TRUE)$importance)

## paired anti vimp
print(vimp(iris.obj, c("Petal.Length", "Petal.Width"), joint = TRUE)$importance)
print(vimp(iris.obj, c("Sepal.Length", "Petal.Width"), joint = TRUE)$importance)

## ------------------------------------------------------------
## survival example
## anti versus permute VIMP with different block sizes
## ------------------------------------------------------------

data(pbc, package = "randomForestSRC")
pbc.obj &lt;- rfsrc(Surv(days, status) ~ ., pbc)

print(vimp(pbc.obj)$importance)
print(vimp(pbc.obj, block.size=1)$importance)
print(vimp(pbc.obj, importance="permute")$importance)
print(vimp(pbc.obj, importance="permute", block.size=1)$importance)

## ------------------------------------------------------------
## imbalanced classification example
## see the imbalanced function for more details
## ------------------------------------------------------------

data(breast, package = "randomForestSRC")
breast &lt;- na.omit(breast)
f &lt;- as.formula(status ~ .)
o &lt;- rfsrc(f, breast, ntree = 2000)

## permutation vimp
print(100 * vimp(o, importance = "permute")$importance)

## anti vimp using gmean performance
print(100 * vimp(o, perf.type = "gmean")$importance[, 1])

## ------------------------------------------------------------
## regression example
## ------------------------------------------------------------

airq.obj &lt;- rfsrc(Ozone ~ ., airquality)
print(vimp(airq.obj))

## ------------------------------------------------------------
## regression example where vimp is calculated on test data
## ------------------------------------------------------------

set.seed(100080)
train &lt;- sample(1:nrow(airquality), size = 80)
airq.obj &lt;- rfsrc(Ozone~., airquality[train, ])

## training data vimp
print(airq.obj$importance)
print(vimp(airq.obj)$importance)

## test data vimp
print(vimp(airq.obj, newdata = airquality[-train, ])$importance)

## ------------------------------------------------------------
## case-specific vimp
## returns VIMP for each case
## ------------------------------------------------------------

o &lt;- rfsrc(mpg~., mtcars)
v &lt;- vimp(o, csv = TRUE)
csvimp &lt;- get.mv.csvimp(v, standardize=TRUE)
print(csvimp)

## ------------------------------------------------------------
## case-specific joint vimp
## returns joint VIMP for each case
## ------------------------------------------------------------

o &lt;- rfsrc(mpg~., mtcars)
v &lt;- vimp(o, joint = TRUE, csv = TRUE)
csvimp &lt;- get.mv.csvimp(v, standardize=TRUE)
print(csvimp)

## ------------------------------------------------------------
## case-specific joint vimp for multivariate regression
## returns joint VIMP for each case, for each outcome
## ------------------------------------------------------------

o &lt;- rfsrc(Multivar(mpg, cyl) ~., data = mtcars)
v &lt;- vimp(o, joint = TRUE, csv = TRUE)
csvimp &lt;- get.mv.csvimp(v, standardize=TRUE)
print(csvimp)


</code></pre>

<hr>
<h2 id='wihs'>Women's Interagency HIV Study (WIHS)</h2><span id='topic+wihs'></span>

<h3>Description</h3>

<p>Competing risk data set involving AIDS in women.
</p>


<h3>Format</h3>

<p>A data frame containing:
</p>

<table>
<tr>
 <td style="text-align: left;">
    time </td><td style="text-align: left;"> time to event</td>
</tr>
<tr>
 <td style="text-align: left;">
    status </td><td style="text-align: left;"> censoring status: 0=censoring, 1=HAART initiation, 2=AIDS/Death before HAART</td>
</tr>
<tr>
 <td style="text-align: left;">
    ageatfda </td><td style="text-align: left;"> age in years at time of FDA approval of first protease inhibitor</td>
</tr>
<tr>
 <td style="text-align: left;">
    idu </td><td style="text-align: left;"> history of IDU: 0=no history, 1=history</td>
</tr>
<tr>
 <td style="text-align: left;">
    black </td><td style="text-align: left;"> race:  0=not African-American; 1=African-American</td>
</tr>
<tr>
 <td style="text-align: left;">
    cd4nadir </td><td style="text-align: left;"> CD4 count (per 100 cells/ul)
  </td>
</tr>

</table>



<h3>Source</h3>

<p>Study included 1164 women enrolled in WIHS, who were alive, infected
with HIV, and free of clinical AIDS on December, 1995, when the first
protease inhibitor (saquinavir mesylate) was approved by the Federal
Drug Administration. Women were followed until the first of the
following occurred: treatment initiation, AIDS diagnosis, death, or
administrative censoring (September, 2006). Variables included history
of injection drug use at WIHS enrollment, whether an individual was
African American, age, and CD4 nadir prior to baseline.</p>


<h3>References</h3>

<p>Bacon M.C, von Wyl V., Alden C., et al. (2005). The Women's Interagency
HIV Study: an observational cohort brings clinical sciences to the
bench, <em>Clin Diagn Lab Immunol</em>, 12(9):1013-1019.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(wihs, package = "randomForestSRC")
wihs.obj &lt;- rfsrc(Surv(time, status) ~ ., wihs, nsplit = 3, ntree = 100)

</code></pre>

<hr>
<h2 id='wine'>White Wine Quality Data</h2><span id='topic+wine'></span>

<h3>Description</h3>

<p>The inputs include objective tests (e.g. PH values) and the output is
based on sensory data (median of at least 3 evaluations made by wine
experts) of white wine. Each expert graded the wine quality between 0
(very bad) and 10 (very excellent).
</p>


<h3>References</h3>

<p>Cortez, P., Cerdeira, A., Almeida, F., Matos T. and Reis, J. (2009).
Modeling wine preferences by data mining from physicochemical properties.
In <em>Decision Support Systems</em>, Elsevier, 47(4):547-553.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## load wine and convert to a multiclass problem
data(wine, package = "randomForestSRC")
wine$quality &lt;- factor(wine$quality)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
