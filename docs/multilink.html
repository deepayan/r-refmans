<!DOCTYPE html><html lang="en"><head><title>Help for package multilink</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {multilink}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#create_comparison_data'><p>Create Comparison Data</p></a></li>
<li><a href='#dup_data'><p>Duplicate Dataset</p></a></li>
<li><a href='#dup_data_small'><p>Small Duplicate Dataset</p></a></li>
<li><a href='#find_bayes_estimate'><p>Find the Bayes Estimate of a Partition</p></a></li>
<li><a href='#gibbs_sampler'><p>Gibbs Sampler for Posterior Inference</p></a></li>
<li><a href='#initialize_partition'><p>Initialize the Partition</p></a></li>
<li><a href='#multilink'><p>Multifile Record Linkage and Duplicate Detection</p></a></li>
<li><a href='#no_dup_data'><p>No Duplicate Dataset</p></a></li>
<li><a href='#no_dup_data_small'><p>Small No Duplicate Dataset</p></a></li>
<li><a href='#reduce_comparison_data'><p>Reduce Comparison Data Size</p></a></li>
<li><a href='#relabel_bayes_estimate'><p>Relabel the Bayes Estimate of a Partition</p></a></li>
<li><a href='#specify_prior'><p>Specify the Prior Distributions</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>Multifile Record Linkage and Duplicate Detection</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.1</td>
</tr>
<tr>
<td>Description:</td>
<td>Implementation of the methodology of Aleshin-Guendel &amp; Sadinle (2022) &lt;<a href="https://doi.org/10.1080%2F01621459.2021.2013242">doi:10.1080/01621459.2021.2013242</a>&gt;. It handles the general problem of multifile record linkage and duplicate detection, where any number of files are to be linked, and any of the files may have duplicates.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.2</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/aleshing/multilink">https://github.com/aleshing/multilink</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/aleshing/multilink/issues">https://github.com/aleshing/multilink/issues</a></td>
</tr>
<tr>
<td>Imports:</td>
<td>igraph, RecordLinkage, Rcpp, utils, mcclust, geosphere,
stringr</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-06-08 20:25:20 UTC; sergealeshin-guendel</td>
</tr>
<tr>
<td>Author:</td>
<td>Serge Aleshin-Guendel [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Serge Aleshin-Guendel &lt;saleshinguendel@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-06-09 14:20:07 UTC</td>
</tr>
</table>
<hr>
<h2 id='create_comparison_data'>Create Comparison Data</h2><span id='topic+create_comparison_data'></span>

<h3>Description</h3>

<p>Create comparison data for all pairs of records, except for those records in
files which are assumed to have no duplicates.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_comparison_data(
  records,
  types,
  breaks,
  file_sizes,
  duplicates,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="create_comparison_data_+3A_records">records</code></td>
<td>
<p>A <code>data.frame</code> containing the records to be linked, where
each column of <code>records</code> is a field to be compared. If there are
multiple files, <code>records</code> should be obtained by stacking the files on
top of each other so that <code>records[1:file_sizes[1], ]</code> contains the
records for file <code>1</code>,
<code>records[(file_sizes[1] + 1):(file_sizes[1] + file_sizes[2]), ]</code>
contains the records for file <code>2</code>, and so on. Missing values should be
coded as <code>NA</code>.</p>
</td></tr>
<tr><td><code id="create_comparison_data_+3A_types">types</code></td>
<td>
<p>A <code>character</code> vector, indicating the comparison to be used
for each field (i.e. each column of <code>records</code>). The options are:
<code>"bi"</code> for binary comparisons, <code>"nu"</code> for numeric comparisons
(absolute difference), <code>"lv"</code> for string comparisons (normalized
Levenshtein distance), <code>"lv_sep"</code> for string comparisons (normalized
Levenshtein distance) where each string may contain multiple spellings
separated by the &quot;|&quot; character. We assume that fields using options
<code>"bi"</code>, <code>"lv"</code>, and <code>"lv_sep"</code> are  of class
<code>character</code>, and fields using the <code>"nu"</code> option are of class
<code>numeric</code>. For fields using the <code>"lv_sep"</code> option, for each record
pair the normalized Levenshtein distance is computed between each possible
spelling, and the minimum normalized Levenshtein distance between spellings
is then used as the comparison for that record pair.</p>
</td></tr>
<tr><td><code id="create_comparison_data_+3A_breaks">breaks</code></td>
<td>
<p>A <code>list</code>, the same length as <code>types</code>, indicating the
break points used to compute disagreement levels for each fields'
comparisons. If <code>types[f]="bi"</code>, <code>breaks[[f]]</code> is ignored (and thus
can be set to <code>NA</code>). See Details for more information on specifying this
argument.</p>
</td></tr>
<tr><td><code id="create_comparison_data_+3A_file_sizes">file_sizes</code></td>
<td>
<p>A <code>numeric</code> vector indicating the size of each file.</p>
</td></tr>
<tr><td><code id="create_comparison_data_+3A_duplicates">duplicates</code></td>
<td>
<p>A <code>numeric</code> vector indicating which files are assumed
to have duplicates. <code>duplicates[k]</code> should be <code>1</code> if file <code>k</code>
has duplicates, and <code>duplicates[k]</code> should be <code>0</code> if file <code>k</code>
has no duplicates. If any files do not have duplicates, we strongly recommend
that the largest such file is organized to be the first file.</p>
</td></tr>
<tr><td><code id="create_comparison_data_+3A_verbose">verbose</code></td>
<td>
<p>A <code>logical</code> indicator of whether progress messages should
be print (default <code>TRUE</code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The purpose of this function is to construct comparison vectors for each pair
of records. In order to construct these vectors, one needs to specify the
<code>types</code> and <code>breaks</code> arguments. The <code>types</code> argument specifies
how each field should be compared, and the <code>breaks</code> argument specifies
how to discretize these comparisons.
</p>
<p>Currently, the <code>types</code> argument supports three types of field
comparisons: binary, absolute difference, and the normalized Levenshtein
distance. Please contact the package maintainer if you need a new type of
comparison to be supported.
</p>
<p>The <code>breaks</code> argument should be a <code>list</code>, with with one element for
each field. If a field is being compared with a binary comparison, i.e.
<code>types[f]="bi"</code>, then the corresponding element of <code>breaks</code> should
be <code>NA</code>, i.e. <code>breaks[[f]]=NA</code>. If a field is being compared with a
numeric or string comparison, then the corresponding element of <code>breaks</code>
should be a vector of cut points used to discretize the comparisons. To give
more detail, suppose you pass in cut points
<code>breaks[[f]]=c(cut_1, ...,cut_L)</code>. These cut points
discretize the range of the comparisons into <code>L+1</code> intervals:
<code class="reqn">I_0=(-\infty, cut_1], I_1=(cut_1, cut_2], ..., I_L=(cut_L, \infty]</code>. The
raw comparisons, which lie in <code class="reqn">[0,\infty)</code> for numeric comparisons and
<code class="reqn">[0,1]</code> for  string comparisons, are then replaced with indicators of
which interval the comparisons lie in. The interval <code class="reqn">I_0</code> corresponds to
the lowest level of disagreement for a comparison, while the interval
<code class="reqn">I_L</code> corresponds to the highest level of disagreement for a comparison.
</p>


<h3>Value</h3>

<p>a list containing:
</p>

<dl>
<dt><code>record_pairs</code></dt><dd><p>A <code>data.frame</code>, where each row
contains the pair of records being compared in the corresponding row of
<code>comparisons</code>. The rows are sorted in ascending order according to the
first column, with ties broken according to the second column in ascending
order. For any given row, the first column is less than the second column,
i.e. <code>record_pairs[i, 1] &lt; record_pairs[i, 2]</code> for each row <code>i</code>.</p>
</dd>
<dt><code>comparisons</code></dt><dd><p>A <code>logical</code> matrix, where each row contains
the comparisons for the record pair in the corresponding row of
<code>record_pairs</code>. Comparisons are in the same order as the columns of
<code>records</code>, and are represented by <code>L + 1</code> columns of
<code>TRUE/FALSE</code> indicators, where <code>L + 1</code> is the number of
disagreement levels for the field based on <code>breaks</code>.</p>
</dd>
<dt><code>K</code></dt><dd><p>The number of files, assumed to be of class
<code>numeric</code>.</p>
</dd>
<dt><code>file_sizes</code></dt><dd><p>A <code>numeric</code> vector of length <code>K</code>,
indicating the size of each file.</p>
</dd>
<dt><code>duplicates</code></dt><dd><p>A <code>numeric</code> vector of length <code>K</code>,
indicating which files are assumed to have duplicates. <code>duplicates[k]</code>
should be <code>1</code> if file <code>k</code> has duplicates, and
<code>duplicates[k]</code> should be <code>0</code> if file <code>k</code> has no duplicates.
If any files do not have duplicates, we strongly recommend that the largest
such file is organized to be the first file.</p>
</dd>
<dt><code>field_levels</code></dt><dd><p>A <code>numeric</code> vector indicating the number of
disagreement levels for each field.</p>
</dd>
<dt><code>file_labels</code></dt><dd><p>An <code>integer</code> vector of length
<code>sum(file_sizes)</code>, where <code>file_labels[i]</code> indicates which file
record <code>i</code> is in.</p>
</dd>
<dt><code>fp_matrix</code></dt><dd><p>An <code>integer</code> matrix, where
<code>fp_matrix[k1, k2]</code> is a label for the file pair <code>(k1, k2)</code>. Note
that <code>fp_matrix[k1, k2] = fp_matrix[k2, k1]</code>.</p>
</dd>
<dt><code>rp_to_fp</code></dt><dd><p>A <code>logical</code> matrix that indicates which record
pairs belong to which file pairs. <code>rp_to_fp[fp, rp]</code> is <code>TRUE</code> if
the records  <code>record_pairs[rp, ]</code> belong to the file pair <code>fp</code>,
and is FALSE otherwise. Note that <code>fp</code> is given by the labeling in
<code>fp_matrix</code>.</p>
</dd>
<dt><code>ab</code></dt><dd><p>An <code>integer</code> vector, of length
<code>ncol(comparisons) * K * (K + 1) / 2</code> that indicates how many record
pairs there are with a given disagreement level for a given field, for each
file pair.</p>
</dd>
<dt><code>file_sizes_not_included</code></dt><dd><p>A <code>numeric</code> vector of <code>0</code>s.
This element is non-zero when <code><a href="#topic+reduce_comparison_data">reduce_comparison_data</a></code> is
used.</p>
</dd>
<dt><code>ab_not_included</code></dt><dd><p>A <code>numeric</code> vector of <code>0</code>s. This
element is non-zero when <code><a href="#topic+reduce_comparison_data">reduce_comparison_data</a></code> is used.</p>
</dd>
<dt><code>labels</code></dt><dd><p><code>NA</code>. This element is not <code>NA</code> when
<code><a href="#topic+reduce_comparison_data">reduce_comparison_data</a></code> is used.</p>
</dd>
<dt><code>pairs_to_keep</code></dt><dd><p><code>NA</code>. This element is not <code>NA</code> when
<code><a href="#topic+reduce_comparison_data">reduce_comparison_data</a></code> is used.</p>
</dd>
<dt><code>cc</code></dt><dd><p><code>0</code>. This element is non-zero when
<code><a href="#topic+reduce_comparison_data">reduce_comparison_data</a></code> is used.</p>
</dd>
</dl>



<h3>References</h3>

<p>Serge Aleshin-Guendel &amp; Mauricio Sadinle (2022). Multifile Partitioning for Record Linkage and Duplicate Detection. <em>Journal of the
American Statistical Association</em>. [doi: <a href="https://doi.org/10.1080/01621459.2021.2013242">10.1080/01621459.2021.2013242</a>][<a href="https://arxiv.org/abs/2110.03839">arXiv</a>]
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Example with small no duplicate dataset
data(no_dup_data_small)

# Create the comparison data
comparison_list &lt;- create_comparison_data(no_dup_data_small$records,
 types = c("bi", "lv", "lv", "lv", "lv", "bi", "bi"),
 breaks = list(NA,  c(0, 0.25, 0.5),  c(0, 0.25, 0.5),
               c(0, 0.25, 0.5), c(0, 0.25, 0.5),  NA, NA),
 file_sizes = no_dup_data_small$file_sizes,
 duplicates = c(0, 0, 0))

## Example with small duplicate dataset
data(dup_data_small)

# Create the comparison data
comparison_list &lt;- create_comparison_data(dup_data_small$records,
 types = c("bi", "lv", "lv", "lv", "lv", "bi", "bi"),
 breaks = list(NA,  c(0, 0.25, 0.5),  c(0, 0.25, 0.5),
               c(0, 0.25, 0.5), c(0, 0.25, 0.5),  NA, NA),
 file_sizes = dup_data_small$file_sizes,
 duplicates = c(1, 1, 1))
</code></pre>

<hr>
<h2 id='dup_data'>Duplicate Dataset</h2><span id='topic+dup_data'></span>

<h3>Description</h3>

<p>A dataset containing <code>867</code> simulated records from <code>3</code> files with
no duplicate records in each file.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dup_data
</code></pre>


<h3>Format</h3>

<p>A list with three elements:
</p>

<dl>
<dt>records</dt><dd><p>A <code>data.frame</code> with the records, containing <code>7</code>
fields, from all three files, in the format used for input to
<code><a href="#topic+create_comparison_data">create_comparison_data</a></code>.</p>
</dd>
<dt>file_sizes</dt><dd><p>The size of each file.</p>
</dd>
<dt>IDs</dt><dd><p>The true partition of the records, represented as an
<code>integer</code>  vector of arbitrary labels of length
<code>sum(file_sizes)</code>.</p>
</dd>
</dl>



<h3>Source</h3>

<p>Extracted from the datasets used in the simulation study of the
paper. The datasets were generated using code from Peter Christen's group
<a href="https://dmm.anu.edu.au/geco/index.php">https://dmm.anu.edu.au/geco/index.php</a>.
</p>


<h3>References</h3>

<p>Serge Aleshin-Guendel &amp; Mauricio Sadinle (2022). Multifile Partitioning for Record Linkage and Duplicate Detection. <em>Journal of the
American Statistical Association</em>. [doi: <a href="https://doi.org/10.1080/01621459.2021.2013242">10.1080/01621459.2021.2013242</a>][<a href="https://arxiv.org/abs/2110.03839">arXiv</a>]
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(dup_data)

# There are 500 entities represented in the records
length(unique(dup_data$IDs))
</code></pre>

<hr>
<h2 id='dup_data_small'>Small Duplicate Dataset</h2><span id='topic+dup_data_small'></span>

<h3>Description</h3>

<p>A dataset containing <code>96</code> simulated records from <code>3</code> files with
no duplicate records in each file, subset from <code><a href="#topic+dup_data">dup_data</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dup_data_small
</code></pre>


<h3>Format</h3>

<p>A list with three elements:
</p>

<dl>
<dt>records</dt><dd><p>A <code>data.frame</code> with the records, containing <code>7</code>
fields, from all three files, in the format used for input to
<code><a href="#topic+create_comparison_data">create_comparison_data</a></code>.</p>
</dd>
<dt>file_sizes</dt><dd><p>The size of each file.</p>
</dd>
<dt>IDs</dt><dd><p>The true partition of the records, represented as an
<code>integer</code>  vector of arbitrary labels of length
<code>sum(file_sizes)</code>.</p>
</dd>
</dl>



<h3>Source</h3>

<p>Extracted from the datasets used in the simulation study of the
paper. The datasets were generated using code from Peter Christen's group
<a href="https://dmm.anu.edu.au/geco/index.php">https://dmm.anu.edu.au/geco/index.php</a>.
</p>


<h3>References</h3>

<p>Serge Aleshin-Guendel &amp; Mauricio Sadinle (2022). Multifile Partitioning for Record Linkage and Duplicate Detection. <em>Journal of the
American Statistical Association</em>. [doi: <a href="https://doi.org/10.1080/01621459.2021.2013242">10.1080/01621459.2021.2013242</a>][<a href="https://arxiv.org/abs/2110.03839">arXiv</a>]
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(dup_data_small)

# There are 96 entities represented in the records
length(unique(dup_data_small$IDs))
</code></pre>

<hr>
<h2 id='find_bayes_estimate'>Find the Bayes Estimate of a Partition</h2><span id='topic+find_bayes_estimate'></span>

<h3>Description</h3>

<p>Find the (approximate) Bayes estimate of a partition based on MCMC samples
of the partition and a specified loss function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>find_bayes_estimate(
  partitions,
  burn_in,
  L_FNM = 1,
  L_FM1 = 1,
  L_FM2 = 2,
  L_A = Inf,
  max_cc_size = nrow(partitions),
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="find_bayes_estimate_+3A_partitions">partitions</code></td>
<td>
<p>Posterior samples of the partition, where each column
is one sample and the partition is represented as an <code>integer</code> vector of
arbitrary labels, as produced by the output of a call to
<code><a href="#topic+gibbs_sampler">gibbs_sampler</a></code>.</p>
</td></tr>
<tr><td><code id="find_bayes_estimate_+3A_burn_in">burn_in</code></td>
<td>
<p>The number of samples to discard for burn in.</p>
</td></tr>
<tr><td><code id="find_bayes_estimate_+3A_l_fnm">L_FNM</code></td>
<td>
<p>Positive loss for a false non-match. Default is <code>1</code>.</p>
</td></tr>
<tr><td><code id="find_bayes_estimate_+3A_l_fm1">L_FM1</code></td>
<td>
<p>Positive loss for a type 1 false match. Default is <code>1</code>.</p>
</td></tr>
<tr><td><code id="find_bayes_estimate_+3A_l_fm2">L_FM2</code></td>
<td>
<p>Positive loss for a type 2 false match. Default is <code>2</code>.</p>
</td></tr>
<tr><td><code id="find_bayes_estimate_+3A_l_a">L_A</code></td>
<td>
<p>Positive loss for abstaining from making a decision for a record.
Default is <code>Inf</code>, i.e. decisions are made for all records.</p>
</td></tr>
<tr><td><code id="find_bayes_estimate_+3A_max_cc_size">max_cc_size</code></td>
<td>
<p>The maximum allowable connected component size over which
the posterior expected loss is minimized. Default is <code>nrow(partitions)</code>,
i.e. no approximation is used. When <code>is.infinite(L_A)</code>, we recommend
setting this argument to <code>50</code>, then increasing based on a computational
budget. When <code>!is.infinite(L_A)</code>, we recommend setting this argument to
<code>10-12</code>, then increasing based on a computational budget (although an
increase of <code>1</code> in this argument can in the worst case lead to a
doubling in computation time).</p>
</td></tr>
<tr><td><code id="find_bayes_estimate_+3A_verbose">verbose</code></td>
<td>
<p>A <code>logical</code> indicator of whether progress messages should
be print (default <code>TRUE</code>).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector, the same length of a column of <code>partitions</code> containing the
(approximate) Bayes estimate of the partition. If <code>!is.infinite(L_A)</code>
the output may be a partial estimate. A positive number <code>l</code> in index
<code>i</code> indicates that record <code>i</code> is in the same cluster as every other
record <code>j</code> with <code>l</code> in index <code>j</code>. A value of <code>-1</code> in
index <code>i</code> indicates that the Bayes estimate abstained from  making a
decision for record <code>i</code>.
</p>


<h3>References</h3>

<p>Serge Aleshin-Guendel &amp; Mauricio Sadinle (2022). Multifile Partitioning for Record Linkage and Duplicate Detection. <em>Journal of the
American Statistical Association</em>. [doi: <a href="https://doi.org/10.1080/01621459.2021.2013242">10.1080/01621459.2021.2013242</a>][<a href="https://arxiv.org/abs/2110.03839">arXiv</a>]
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Example with small no duplicate dataset
data(no_dup_data_small)

# Create the comparison data
comparison_list &lt;- create_comparison_data(no_dup_data_small$records,
 types = c("bi", "lv", "lv", "lv", "lv", "bi", "bi"),
 breaks = list(NA,  c(0, 0.25, 0.5),  c(0, 0.25, 0.5),
               c(0, 0.25, 0.5), c(0, 0.25, 0.5),  NA, NA),
 file_sizes = no_dup_data_small$file_sizes,
 duplicates = c(0, 0, 0))

# Specify the prior
prior_list &lt;- specify_prior(comparison_list, mus = NA, nus = NA, flat = 0,
 alphas = rep(1, 7), dup_upper_bound = c(1, 1, 1),
 dup_count_prior_family = NA, dup_count_prior_pars = NA,
 n_prior_family = "uniform", n_prior_pars = NA)

# Find initialization for the matching (this step is optional)
# The following line corresponds to only keeping pairs of records as
# potential matches in the initialization for which neither gname nor fname
# disagree at the highest level
pairs_to_keep &lt;- (comparison_list$comparisons[, "gname_DL_3"] != TRUE) &amp;
 (comparison_list$comparisons[, "fname_DL_3"] != TRUE)
Z_init &lt;- initialize_partition(comparison_list, pairs_to_keep, seed = 42)

# Run the Gibbs sampler
results &lt;- gibbs_sampler(comparison_list, prior_list, n_iter = 1000,
 Z_init = Z_init, seed = 42)

# Find the full Bayes estimate

full_estimate &lt;- find_bayes_estimate(results$partitions, burn_in = 100,
 L_FNM = 1, L_FM1 = 1, L_FM2 = 2, L_A = Inf, max_cc_size = 50)

# Find the partial Bayes estimate
partial_estimate &lt;- find_bayes_estimate(results$partitions, burn_in = 100,
 L_FNM = 1, L_FM1 = 1, L_FM2 = 2, L_A = 0.1, max_cc_size = 12)


# Example with small duplicate dataset
data(dup_data_small)

# Create the comparison data
comparison_list &lt;- create_comparison_data(dup_data_small$records,
 types = c("bi", "lv", "lv", "lv", "lv", "bi", "bi"),
 breaks = list(NA,  c(0, 0.25, 0.5),  c(0, 0.25, 0.5),
               c(0, 0.25, 0.5), c(0, 0.25, 0.5),  NA, NA),
 file_sizes = dup_data_small$file_sizes,
 duplicates = c(1, 1, 1))

# Reduce the comparison data
# The following line corresponds to only keeping pairs of records for which
# neither gname nor fname disagree at the highest level
pairs_to_keep &lt;- (comparison_list$comparisons[, "gname_DL_3"] != TRUE) &amp;
 (comparison_list$comparisons[, "fname_DL_3"] != TRUE)
reduced_comparison_list &lt;- reduce_comparison_data(comparison_list,
 pairs_to_keep, cc = 1)

# Specify the prior
prior_list &lt;- specify_prior(reduced_comparison_list, mus = NA, nus = NA,
 flat = 0, alphas = rep(1, 7), dup_upper_bound = c(10, 10, 10),
 dup_count_prior_family = c("Poisson", "Poisson", "Poisson"),
 dup_count_prior_pars = list(c(1), c(1), c(1)), n_prior_family = "uniform",
 n_prior_pars = NA)

# Run the Gibbs sampler
results &lt;- gibbs_sampler(reduced_comparison_list, prior_list, n_iter = 1000,
 seed = 42)

# Find the full Bayes estimate

full_estimate &lt;- find_bayes_estimate(results$partitions, burn_in = 100,
 L_FNM = 1, L_FM1 = 1, L_FM2 = 2, L_A = Inf, max_cc_size = 50)

# Find the partial Bayes estimate
partial_estimate &lt;- find_bayes_estimate(results$partitions, burn_in = 100,
 L_FNM = 1, L_FM1 = 1, L_FM2 = 2, L_A = 0.1, max_cc_size = 12)

</code></pre>

<hr>
<h2 id='gibbs_sampler'>Gibbs Sampler for Posterior Inference</h2><span id='topic+gibbs_sampler'></span>

<h3>Description</h3>

<p>Run a Gibbs sampler to explore the posterior distribution of partitions of
records.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gibbs_sampler(
  comparison_list,
  prior_list,
  n_iter = 2000,
  Z_init = 1:sum(comparison_list$file_sizes),
  seed = 70,
  single_likelihood = FALSE,
  chaperones_info = NA,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="gibbs_sampler_+3A_comparison_list">comparison_list</code></td>
<td>
<p>The output from a call to
<code><a href="#topic+create_comparison_data">create_comparison_data</a></code> or <code><a href="#topic+reduce_comparison_data">reduce_comparison_data</a></code>.</p>
</td></tr>
<tr><td><code id="gibbs_sampler_+3A_prior_list">prior_list</code></td>
<td>
<p>The output from a call to <code><a href="#topic+specify_prior">specify_prior</a></code>.</p>
</td></tr>
<tr><td><code id="gibbs_sampler_+3A_n_iter">n_iter</code></td>
<td>
<p>The number of iterations of the Gibbs sampler to run.</p>
</td></tr>
<tr><td><code id="gibbs_sampler_+3A_z_init">Z_init</code></td>
<td>
<p>Initialization of the partition of records, represented as an
<code>integer</code> vector of arbitrary labels of length
<code>sum(comparison_list$file_sizes)</code>. The default initialization places
each record in its own cluster. See <code><a href="#topic+initialize_partition">initialize_partition</a></code> for an
alternative initialization when there are no duplicates in each file.</p>
</td></tr>
<tr><td><code id="gibbs_sampler_+3A_seed">seed</code></td>
<td>
<p>The seed to use while running the Gibbs sampler.</p>
</td></tr>
<tr><td><code id="gibbs_sampler_+3A_single_likelihood">single_likelihood</code></td>
<td>
<p>A <code>logical</code> indicator of whether to use a
single likelihood for comparisons for all file pairs, or whether to use a
separate likelihood for comparisons for each file pair. When
<code>single_likelihood=TRUE</code>, a single likelihood is used, and the prior
hyperparameters for <code>m</code> and <code>u</code> from the first file pair are used.
We do not recommend using a single likelihood in general.</p>
</td></tr>
<tr><td><code id="gibbs_sampler_+3A_chaperones_info">chaperones_info</code></td>
<td>
<p>If <code>chaperones_info</code> is set to <code>NA</code>, then
Gibbs updates to the partition are used during the Gibbs sampler, as
described in Aleshin-Guendel &amp; Sadinle (2022). Else, Chaperones updates,
as described in Miller et al. (2015) and Betancourt et al. (2016), are used
and <code>chaperones_info</code> should be a <code>list</code> with five elements
controlling Chaperones updates to the partition during the Gibbs sampler:
<code>chap_type</code>, <code>num_chap_iter</code>, <code>nonuniform_chap_type</code>,
<code>extra_gibbs</code>, <code>num_restrict</code>. <code>chap_type</code> is <code>0</code> if
using a uniform Chaperones distribution, and <code>1</code> if
using a nonuniform Chaperones distribution. <code>num_chap_iter</code> is the
number of Chaperones updates to the partition that are made during each
iteration of the Gibbs sampler. When using a nonuniform Chaperones
distribution, <code>nonuniform_chap_type</code> is <code>0</code> if using the exact
version, or <code>1</code> if using the partial version. <code>extra_gibbs</code> is a
<code>logical</code> indicator of whether a Gibbs update to the partition should be
done after the Chaperones updates, at each iteration of the Gibbs sampler.
<code>num_restrict</code> is the number of restricted Gibbs steps to take during
each Chaperones update to the partition.</p>
</td></tr>
<tr><td><code id="gibbs_sampler_+3A_verbose">verbose</code></td>
<td>
<p>A <code>logical</code> indicator of whether progress messages should
be print (default <code>TRUE</code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Given the prior specified using <code><a href="#topic+specify_prior">specify_prior</a></code>, this function
runs a Gibbs sampler to explore the posterior distribution of partitions of
records, conditional on the comparison data created using
<code><a href="#topic+create_comparison_data">create_comparison_data</a></code> or <code><a href="#topic+reduce_comparison_data">reduce_comparison_data</a></code>.
</p>


<h3>Value</h3>

<p>a list containing:
</p>

<dl>
<dt><code>m</code></dt><dd><p>Posterior samples of the <code>m</code> parameters. Each column
is one sample.</p>
</dd>
<dt><code>u</code></dt><dd><p>Posterior samples of the <code>u</code> parameters. Each column
is one sample.</p>
</dd>
<dt><code>partitions</code></dt><dd><p>Posterior samples of the partition. Each column
is one sample. Note that the partition is represented as an <code>integer</code>
vector of arbitrary labels of length
<code>sum(comparison_list$file_sizes)</code>.</p>
</dd>
<dt><code>contingency_tables</code></dt><dd><p>Posterior samples of the overlap table.
Each column is one sample. This incorporates counts of records determined
not to be candidate matches to any other records using
<code><a href="#topic+reduce_comparison_data">reduce_comparison_data</a></code>.</p>
</dd>
<dt><code>cluster_sizes</code></dt><dd><p>Posterior samples of the size of each cluster
(associated with an arbitrary label from <code>1</code> to
<code>sum(comparison_list$file_sizes)</code>). Each column is one sample.</p>
</dd>
<dt><code>sampling_time</code></dt><dd><p>The time in seconds it took to run the
sampler.</p>
</dd>
</dl>



<h3>References</h3>

<p>Serge Aleshin-Guendel &amp; Mauricio Sadinle (2022). Multifile Partitioning for Record Linkage and Duplicate Detection. <em>Journal of the
American Statistical Association</em>. [doi: <a href="https://doi.org/10.1080/01621459.2021.2013242">10.1080/01621459.2021.2013242</a>][<a href="https://arxiv.org/abs/2110.03839">arXiv</a>]
</p>
<p>Jeffrey Miller, Brenda Betancourt, Abbas Zaidi, Hanna Wallach, &amp; Rebecca C. Steorts (2015).
Microclustering: When the cluster sizes grow sublinearly with the size of the data set.
<em>NeurIPS Bayesian Nonparametrics: The Next Generation Workshop Series</em>. [<a href="https://arxiv.org/abs/1512.00792">arXiv</a>]
</p>
<p>Brenda Betancourt, Giacomo Zanella, Jeffrey Miller, Hanna Wallach, Abbas Zaidi, &amp; Rebecca C. Steorts (2016).
Flexible Models for Microclustering with Application to Entity Resolution.
<em>Advances in neural information processing systems</em>. [<a href="https://proceedings.neurips.cc/paper/2016/hash/670e8a43b246801ca1eaca97b3e19189-Abstract.html">Published</a>] [<a href="https://arxiv.org/abs/1610.09780">arXiv</a>]
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Example with small no duplicate dataset
data(no_dup_data_small)

# Create the comparison data
comparison_list &lt;- create_comparison_data(no_dup_data_small$records,
 types = c("bi", "lv", "lv", "lv", "lv", "bi", "bi"),
 breaks = list(NA,  c(0, 0.25, 0.5),  c(0, 0.25, 0.5),
               c(0, 0.25, 0.5), c(0, 0.25, 0.5),  NA, NA),
 file_sizes = no_dup_data_small$file_sizes,
 duplicates = c(0, 0, 0))

# Specify the prior
prior_list &lt;- specify_prior(comparison_list, mus = NA, nus = NA, flat = 0,
 alphas = rep(1, 7), dup_upper_bound = c(1, 1, 1),
 dup_count_prior_family = NA, dup_count_prior_pars = NA,
 n_prior_family = "uniform", n_prior_pars = NA)

# Find initialization for the matching (this step is optional)
# The following line corresponds to only keeping pairs of records as
# potential matches in the initialization for which neither gname nor fname
# disagree at the highest level
pairs_to_keep &lt;- (comparison_list$comparisons[, "gname_DL_3"] != TRUE) &amp;
 (comparison_list$comparisons[, "fname_DL_3"] != TRUE)
Z_init &lt;- initialize_partition(comparison_list, pairs_to_keep, seed = 42)

# Run the Gibbs sampler
{
results &lt;- gibbs_sampler(comparison_list, prior_list, n_iter = 1000,
 Z_init = Z_init, seed = 42)
}

# Example with small duplicate dataset
data(dup_data_small)

# Create the comparison data
comparison_list &lt;- create_comparison_data(dup_data_small$records,
 types = c("bi", "lv", "lv", "lv", "lv", "bi", "bi"),
 breaks = list(NA,  c(0, 0.25, 0.5),  c(0, 0.25, 0.5),
               c(0, 0.25, 0.5), c(0, 0.25, 0.5),  NA, NA),
 file_sizes = dup_data_small$file_sizes,
 duplicates = c(1, 1, 1))

# Reduce the comparison data
# The following line corresponds to only keeping pairs of records for which
# neither gname nor fname disagree at the highest level
pairs_to_keep &lt;- (comparison_list$comparisons[, "gname_DL_3"] != TRUE) &amp;
 (comparison_list$comparisons[, "fname_DL_3"] != TRUE)
reduced_comparison_list &lt;- reduce_comparison_data(comparison_list,
 pairs_to_keep, cc = 1)

# Specify the prior
prior_list &lt;- specify_prior(reduced_comparison_list, mus = NA, nus = NA,
 flat = 0, alphas = rep(1, 7), dup_upper_bound = c(10, 10, 10),
 dup_count_prior_family = c("Poisson", "Poisson", "Poisson"),
 dup_count_prior_pars = list(c(1), c(1), c(1)), n_prior_family = "uniform",
 n_prior_pars = NA)

# Run the Gibbs sampler
{
results &lt;- gibbs_sampler(reduced_comparison_list, prior_list, n_iter = 1000,
 seed = 42)
}
</code></pre>

<hr>
<h2 id='initialize_partition'>Initialize the Partition</h2><span id='topic+initialize_partition'></span>

<h3>Description</h3>

<p>Generate an initialization for the partition in the case when it is assumed
there are no duplicates in all files (so that the partition is a matching).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>initialize_partition(comparison_list, pairs_to_keep, seed = NA)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="initialize_partition_+3A_comparison_list">comparison_list</code></td>
<td>
<p>the output from a call to
<code><a href="#topic+create_comparison_data">create_comparison_data</a></code> or <code><a href="#topic+reduce_comparison_data">reduce_comparison_data</a></code>.
Note that in order to correctly specify the initialization, if
<code><a href="#topic+reduce_comparison_data">reduce_comparison_data</a></code> is used to the reduce the number of
record pairs that are candidate matches, then the output of
<code><a href="#topic+reduce_comparison_data">reduce_comparison_data</a></code> (not
<code><a href="#topic+create_comparison_data">create_comparison_data</a></code>) should be used for this argument.</p>
</td></tr>
<tr><td><code id="initialize_partition_+3A_pairs_to_keep">pairs_to_keep</code></td>
<td>
<p>A <code>logical</code> vector, the same length as
<code>comparison_list$record_pairs</code>, indicating which record pairs are
potential matches in the initialization.</p>
</td></tr>
<tr><td><code id="initialize_partition_+3A_seed">seed</code></td>
<td>
<p>The seed to use to generate the initialization.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When it is assumed that there are no duplicates in all files, and
<code><a href="#topic+reduce_comparison_data">reduce_comparison_data</a></code> is not used to reduce the number of
potential matches, the Gibbs sampler used for posterior inference may
experience slow mixing when using an initialization for the partition where
each record is in its own cluster (the default option for the Gibbs sampler).
The purpose of this function is to provide an alternative initialization
scheme.
</p>
<p>To use this initialization scheme, the user passes in a <code>logical</code> vector
that indicates which record pairs are potential matches according to an
indexing method (as in <code><a href="#topic+reduce_comparison_data">reduce_comparison_data</a></code>). Note that this
indexing is only used to generate the initialization, it is not used for
inference. The initialization scheme first finds the transitive closure of
the potential matches, which partitions the records into blocks. Within each
block of records, the scheme randomly selects a record from each file, and
these selected records are then placed in the same cluster for the partition
initialization. All other records are placed in their own clusters.
</p>


<h3>Value</h3>

<p>an <code>integer</code> vector of arbitrary labels of length
<code>sum(comparison_list$file_sizes)</code>, giving an initialization for the
partition.
</p>


<h3>References</h3>

<p>Serge Aleshin-Guendel &amp; Mauricio Sadinle (2022). Multifile Partitioning for Record Linkage and Duplicate Detection. <em>Journal of the
American Statistical Association</em>. [doi: <a href="https://doi.org/10.1080/01621459.2021.2013242">10.1080/01621459.2021.2013242</a>][<a href="https://arxiv.org/abs/2110.03839">arXiv</a>]
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Example with small no duplicate dataset
data(no_dup_data_small)

# Create the comparison data
comparison_list &lt;- create_comparison_data(no_dup_data_small$records,
 types = c("bi", "lv", "lv", "lv", "lv", "bi", "bi"),
 breaks = list(NA,  c(0, 0.25, 0.5),  c(0, 0.25, 0.5),
               c(0, 0.25, 0.5), c(0, 0.25, 0.5),  NA, NA),
 file_sizes = no_dup_data_small$file_sizes,
 duplicates = c(0, 0, 0))

# Find initialization for the matching
# The following line corresponds to only keeping pairs of records as
# potential matches in the initialization for which neither gname nor fname
# disagree at the highest level
pairs_to_keep &lt;- (comparison_list$comparisons[, "gname_DL_3"] != TRUE) &amp;
 (comparison_list$comparisons[, "fname_DL_3"] != TRUE)
Z_init &lt;- initialize_partition(comparison_list, pairs_to_keep, seed = 42)
</code></pre>

<hr>
<h2 id='multilink'>Multifile Record Linkage and Duplicate Detection</h2><span id='topic+multilink'></span>

<h3>Description</h3>

<p>The multilink package implements the methodology of Aleshin-Guendel &amp; Sadinle
(2022). It handles the general problem of multifile record linkage and
duplicate detection, where any number of files are to be linked, and any of
the files may have duplicates.
</p>


<h3>References</h3>

<p>Serge Aleshin-Guendel &amp; Mauricio Sadinle (2022). Multifile Partitioning for Record Linkage and Duplicate Detection. <em>Journal of the
American Statistical Association</em>. [doi: <a href="https://doi.org/10.1080/01621459.2021.2013242">10.1080/01621459.2021.2013242</a>] [<a href="https://arxiv.org/abs/2110.03839">arXiv</a>]
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Here we demonstrate an example workflow with the small no duplicate dataset
data(no_dup_data_small)

# Create the comparison data
comparison_list &lt;- create_comparison_data(no_dup_data_small$records,
 types = c("bi", "lv", "lv", "lv", "lv", "bi", "bi"),
 breaks = list(NA,  c(0, 0.25, 0.5),  c(0, 0.25, 0.5),
               c(0, 0.25, 0.5), c(0, 0.25, 0.5),  NA, NA),
 file_sizes = no_dup_data_small$file_sizes,
 duplicates = c(0, 0, 0))

# Specify the prior
prior_list &lt;- specify_prior(comparison_list, mus = NA, nus = NA, flat = 0,
 alphas = rep(1, 7), dup_upper_bound = c(1, 1, 1),
 dup_count_prior_family = NA, dup_count_prior_pars = NA,
 n_prior_family = "uniform", n_prior_pars = NA)

# Find initialization for the matching (this step is optional)
# The following line corresponds to only keeping pairs of records as
# potential matches in the initialization for which neither gname nor fname
# disagree at the highest level
pairs_to_keep &lt;- (comparison_list$comparisons[, "gname_DL_3"] != TRUE) &amp;
 (comparison_list$comparisons[, "fname_DL_3"] != TRUE)
Z_init &lt;- initialize_partition(comparison_list, pairs_to_keep, seed = 42)

# Run the Gibbs sampler
results &lt;- gibbs_sampler(comparison_list, prior_list, n_iter = 1000,
 Z_init = Z_init, seed = 42)

# Find the full Bayes estimate

full_estimate &lt;- find_bayes_estimate(results$partitions, burn_in = 100,
 L_FNM = 1, L_FM1 = 1, L_FM2 = 2, L_A = Inf, max_cc_size = 50)

# The number of clusters in the full estimate
length(unique(full_estimate))
# The number of entities represented in the records
length(unique(no_dup_data_small$IDs))

# Find which record pairs are truly coreferent based on IDs
true_links &lt;- no_dup_data_small$IDs[comparison_list$record_pairs[, 1]] ==
no_dup_data_small$IDs[comparison_list$record_pairs[, 2]]

# Find which record pairs are in the same clusters in the full estimate
full_estimate_links &lt;- full_estimate[comparison_list$record_pairs[, 1]] ==
full_estimate[comparison_list$record_pairs[, 2]]

# Find the number of true matches in the full estimate
true_matches &lt;- sum(full_estimate_links &amp; true_links)

# Precision of the full estimate
true_matches / sum(full_estimate_links)

# Recall of the full estimate
true_matches / sum(true_links)

# Find the partial Bayes estimate
partial_estimate &lt;- find_bayes_estimate(results$partitions, burn_in = 100,
 L_FNM = 1, L_FM1 = 1, L_FM2 = 2, L_A = 0.1, max_cc_size = 12)

# The partial estimate abstains from making decisions for how many records?
sum(partial_estimate == -1)

# For the records which decisions were made for in the partial estimate,
# there are how many clusters?
length(unique(partial_estimate))

# Abstain rate of partial_estimate
sum(partial_estimate == -1) / length(partial_estimate)

# Relabel records where we abstained
partial_estimate[which(partial_estimate == -1)] &lt;- length(partial_estimate) +
which(partial_estimate == -1)

# Find which record pairs are in the same clusters in the full estimate
partial_estimate_links &lt;-
 partial_estimate[comparison_list$record_pairs[, 1]] ==
 partial_estimate[comparison_list$record_pairs[, 2]]

# Find the number of true matches in the partial estimate
true_matches_A &lt;- sum(partial_estimate_links &amp; true_links)

# Precision of the partial estimate
true_matches_A / sum(partial_estimate_links)


# Here we demonstrate an example workflow with the small duplicate dataset
data(dup_data_small)

# Create the comparison data
comparison_list &lt;- create_comparison_data(dup_data_small$records,
 types = c("bi", "lv", "lv", "lv", "lv", "bi", "bi"),
 breaks = list(NA,  c(0, 0.25, 0.5),  c(0, 0.25, 0.5),
               c(0, 0.25, 0.5), c(0, 0.25, 0.5),  NA, NA),
 file_sizes = dup_data_small$file_sizes,
 duplicates = c(1, 1, 1))

# Reduce the comparison data
# The following line corresponds to only keeping pairs of records for which
# neither gname nor fname disagree at the highest level
pairs_to_keep &lt;- (comparison_list$comparisons[, "gname_DL_3"] != TRUE) &amp;
 (comparison_list$comparisons[, "fname_DL_3"] != TRUE)
reduced_comparison_list &lt;- reduce_comparison_data(comparison_list,
 pairs_to_keep, cc = 1)

# Specify the prior
prior_list &lt;- specify_prior(reduced_comparison_list, mus = NA, nus = NA,
 flat = 0, alphas = rep(1, 7), dup_upper_bound = c(10, 10, 10),
 dup_count_prior_family = c("Poisson", "Poisson", "Poisson"),
 dup_count_prior_pars = list(c(1), c(1), c(1)), n_prior_family = "uniform",
 n_prior_pars = NA)

# Run the Gibbs sampler
results &lt;- gibbs_sampler(reduced_comparison_list, prior_list, n_iter = 1000,
 seed = 42)

# Find the full Bayes estimate

full_estimate &lt;- find_bayes_estimate(results$partitions, burn_in = 100,
 L_FNM = 1, L_FM1 = 1, L_FM2 = 2, L_A = Inf, max_cc_size = 50)

# The number of  clusters in the full estimate (including records records
# determined not to be candidate matches to any other records using
# reduce_comparison_data)
length(unique(full_estimate)) +
sum(reduced_comparison_list$file_sizes_not_included)
# The number of entities represented in the records
length(unique(dup_data_small$IDs))

# Find which record pairs are truly coreferent based on IDs
true_links &lt;- dup_data_small$IDs[comparison_list$record_pairs[, 1]] ==
dup_data_small$IDs[comparison_list$record_pairs[, 2]]

# Focus on the record pairs that were candidate matches
true_links_reduced &lt;- true_links[reduced_comparison_list$pairs_to_keep]

# Calculate the number of prior false non-matches based on the indexing
# scheme used
prior_fnm &lt;-
 nrow(comparison_list$record_pairs[true_links &amp;
 (!reduced_comparison_list$pairs_to_keep), ])

# Find which record pairs are in the same clusters in the full estimate
full_estimate_links &lt;-
 full_estimate[reduced_comparison_list$record_pairs[, 1]] ==
 full_estimate[reduced_comparison_list$record_pairs[, 2]]

# Find the number of true matches in the full estimate
true_matches &lt;- sum(full_estimate_links &amp; true_links_reduced)

# Precision of the full estimate
true_matches / sum(full_estimate_links)

# Recall of the full estimate
true_matches / (sum(true_links_reduced) + prior_fnm)

# Find the partial Bayes estimate
partial_estimate &lt;- find_bayes_estimate(results$partitions, burn_in = 100,
 L_FNM = 1, L_FM1 = 1, L_FM2 = 2, L_A = 0.1, max_cc_size = 12)

# The partial estimate abstains from making decisions for how many records?
sum(partial_estimate == -1)

# For the records which decisions were made for in the partial estimate,
# there are how many clusters? (including records determined not to be
# candidate matches to any other records using reduce_comparison_data)
length(unique(partial_estimate)) +
 sum(reduced_comparison_list$file_sizes_not_included)

# Abstain rate of partial_estimat (excluding records determined not
# to be candidate matches to any other records using reduce_comparison_data)
sum(partial_estimate == -1) / length(partial_estimate)

# Relabel records where we abstained
partial_estimate[which(partial_estimate == -1)] &lt;- length(partial_estimate) +
which(partial_estimate == -1)

# Find which record pairs are in the same clusters in the full estimate
partial_estimate_links &lt;-
 partial_estimate[reduced_comparison_list$record_pairs[, 1]] ==
 partial_estimate[reduced_comparison_list$record_pairs[, 2]]

# Find the number of true matches in the partial estimate
true_matches_A &lt;- sum(partial_estimate_links &amp; true_links_reduced)

# Precision of the partial estimate
true_matches_A / sum(partial_estimate_links)

# Relabel the full and partial Bayes estimates
full_estimate_relabel &lt;- relabel_bayes_estimate(reduced_comparison_list,
 full_estimate)

partial_estimate_relabel &lt;- relabel_bayes_estimate(reduced_comparison_list,
 partial_estimate)

# Add columns to the records corresponding to their full and partial
# Bayes estimates
dup_data_small$records &lt;- cbind(dup_data_small$records,
 full_estimate_id = full_estimate_relabel$link_id,
 partial_estimate_id = partial_estimate_relabel$link_id)



</code></pre>

<hr>
<h2 id='no_dup_data'>No Duplicate Dataset</h2><span id='topic+no_dup_data'></span>

<h3>Description</h3>

<p>A dataset containing <code>730</code> simulated records from <code>3</code> files with
no duplicate records in each file.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>no_dup_data
</code></pre>


<h3>Format</h3>

<p>A list with three elements:
</p>

<dl>
<dt>records</dt><dd><p>A <code>data.frame</code> with the records, containing <code>7</code>
fields, from all three files, in the format used for input to
<code><a href="#topic+create_comparison_data">create_comparison_data</a></code>.</p>
</dd>
<dt>file_sizes</dt><dd><p>The size of each file.</p>
</dd>
<dt>IDs</dt><dd><p>The true partition of the records, represented as an
<code>integer</code>  vector of arbitrary labels of length
<code>sum(file_sizes)</code>.</p>
</dd>
</dl>



<h3>Source</h3>

<p>Extracted from the datasets used in the simulation study of the
paper. The datasets were generated using code from Peter Christen's group
<a href="https://dmm.anu.edu.au/geco/index.php">https://dmm.anu.edu.au/geco/index.php</a>.
</p>


<h3>References</h3>

<p>Serge Aleshin-Guendel &amp; Mauricio Sadinle (2022). Multifile Partitioning for Record Linkage and Duplicate Detection. <em>Journal of the
American Statistical Association</em>. [doi: <a href="https://doi.org/10.1080/01621459.2021.2013242">10.1080/01621459.2021.2013242</a>] [<a href="https://arxiv.org/abs/2110.03839">arXiv</a>]
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(no_dup_data)

# There are 500 entities represented in the records
length(unique(no_dup_data$IDs))
</code></pre>

<hr>
<h2 id='no_dup_data_small'>Small No Duplicate Dataset</h2><span id='topic+no_dup_data_small'></span>

<h3>Description</h3>

<p>A dataset containing <code>71</code> simulated records from <code>3</code> files with
no duplicate records in each file, subset from <code><a href="#topic+no_dup_data">no_dup_data</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>no_dup_data_small
</code></pre>


<h3>Format</h3>

<p>A list with three elements:
</p>

<dl>
<dt>records</dt><dd><p>A <code>data.frame</code> with the records, containing <code>7</code>
fields, from all three files, in the format used for input to
<code><a href="#topic+create_comparison_data">create_comparison_data</a></code>.</p>
</dd>
<dt>file_sizes</dt><dd><p>The size of each file.</p>
</dd>
<dt>IDs</dt><dd><p>The true partition of the records, represented as an
<code>integer</code>  vector of arbitrary labels of length
<code>sum(file_sizes)</code>.</p>
</dd>
</dl>



<h3>Source</h3>

<p>Extracted from the datasets used in the simulation study of the
paper. The datasets were generated using code from Peter Christen's group
<a href="https://dmm.anu.edu.au/geco/index.php">https://dmm.anu.edu.au/geco/index.php</a>.
</p>


<h3>References</h3>

<p>Serge Aleshin-Guendel &amp; Mauricio Sadinle (2022). Multifile Partitioning for Record Linkage and Duplicate Detection. <em>Journal of the
American Statistical Association</em>. [doi: <a href="https://doi.org/10.1080/01621459.2021.2013242">10.1080/01621459.2021.2013242</a>] [<a href="https://arxiv.org/abs/2110.03839">arXiv</a>]
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(no_dup_data_small)

# There are 71 entities represented in the records
length(unique(no_dup_data_small$IDs))
</code></pre>

<hr>
<h2 id='reduce_comparison_data'>Reduce Comparison Data Size</h2><span id='topic+reduce_comparison_data'></span>

<h3>Description</h3>

<p>Use indexing to reduce the number of record pairs that are potential matches.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>reduce_comparison_data(comparison_list, pairs_to_keep, cc = 1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="reduce_comparison_data_+3A_comparison_list">comparison_list</code></td>
<td>
<p>The output of a call to
<code><a href="#topic+create_comparison_data">create_comparison_data</a></code>.</p>
</td></tr>
<tr><td><code id="reduce_comparison_data_+3A_pairs_to_keep">pairs_to_keep</code></td>
<td>
<p>A <code>logical</code> vector, the same length as
<code>comparison_list$record_pairs</code>, indicating which record pairs should be
kept as potential matches. These potential matches do not have to be
transitive (see the argument <code>cc</code>).</p>
</td></tr>
<tr><td><code id="reduce_comparison_data_+3A_cc">cc</code></td>
<td>
<p>A <code>numeric</code> indicator of whether to find the transitive
closure of <code>pairs_to_keep</code>, and use these potential matches instead
of just those from <code>pairs_to_keep</code>. <code>cc</code> should be <code>1</code> if the
transitive closure is being used, and <code>cc</code> should be <code>0</code> if the
transitive closure is not being used. We recommend setting <code>cc</code> to
<code>1</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When using comparison-based record linkage methods, scalability is a concern,
as the number of record pairs is quadratic in the number of records. In
order to address these concerns, it's common to declare certain record pairs
to not be potential matches a priori, using indexing methods. The user is
free to index using any method they like, as long as they can produce a
<code>logical</code> vector that indicates which record pairs are potential matches
according to their indexing method. We recommend, if the user chosen indexing
method does not output potential matches that are transitive, to set the
<code>cc</code> argument to <code>1</code>. By transitive we mean, for any three records
<code class="reqn">i</code>, <code class="reqn">j</code>, and <code class="reqn">k</code>, if <code class="reqn">i</code> and <code class="reqn">j</code> are potential matches,
and <code class="reqn">j</code> and <code class="reqn">k</code> are potential matches, then <code class="reqn">i</code> and <code class="reqn">k</code> are
potential matches. Non-transitive indexing schemes can lead to poor mixing of
the Gibbs sampler used for posterior inference, and suggests that the
indexing method used may have been too stringent.
</p>
<p>If indexing is used, it may be the case that some records are declared to not
be potential matches to any other records. In this case, the indexing method
has made the decision that these records have no matches, and thus we can
remove them from the data set and relabel the remaining records; see the
documentation for <code>labels</code> for information on how to go between the
original labeling and the new labeling.
</p>
<p>If indexing is used, comparisons for record pairs that aren't potential
matches are still used during inference, where they're used to inform the
distribution of comparisons for non-matches.
</p>


<h3>Value</h3>

<p>a list containing:
</p>

<dl>
<dt><code>record_pairs</code></dt><dd><p>A <code>data.frame</code>, where each row
contains the pair of records being compared in the corresponding row of
<code>comparisons</code>. The rows are sorted in ascending order according to the
first column, with ties broken according to the second column in ascending
order. For any given row, the first column is less than the second column,
i.e. <code>record_pairs[i, 1] &lt; record_pairs[i, 2]</code> for each row <code>i</code>.
If according to <code>pairs_to_keep</code> there are records which are not
potential matches to any other records, the remaining records are
relabeled (see <code>labels</code>).</p>
</dd>
<dt><code>comparisons</code></dt><dd><p>A <code>logical</code> matrix, where each row contains
the comparisons between the record pair in the corresponding row of
<code>record_pairs</code>. Comparisons are in the same order as the columns of
<code>records</code>, and are represented by <code>L + 1</code> columns of
<code>TRUE/FALSE</code> indicators, where <code>L + 1</code> is the number of
disagreement levels for the field based on <code>breaks</code>.</p>
</dd>
<dt><code>K</code></dt><dd><p>The number of files, assumed to be of class
<code>numeric</code>.</p>
</dd>
<dt><code>file_sizes</code></dt><dd><p>A <code>numeric</code> vector of length <code>K</code>,
indicating the size of each file. If according to <code>pairs_to_keep</code>
there are records which are not potential matches to any other records, the
remaining records are relabeled (see <code>labels</code>), and <code>file_sizes</code>
now represents the sizes of each file after removing such records.</p>
</dd>
<dt><code>duplicates</code></dt><dd><p>A <code>numeric</code> vector of length <code>K</code>,
indicating which files are assumed to have duplicates. <code>duplicates[k]</code>
should be <code>1</code> if file <code>k</code> has duplicates, and
<code>duplicates[k]</code> should be <code>0</code> if file <code>k</code> has no
duplicates.</p>
</dd>
<dt><code>field_levels</code></dt><dd><p>A <code>numeric</code> vector indicating the number of
disagreement levels for each field.</p>
</dd>
<dt><code>file_labels</code></dt><dd><p>An <code>integer</code> vector of length
<code>sum(file_sizes)</code>, where <code>file_labels[i]</code> indicated which file
record <code>i</code> is in.</p>
</dd>
<dt><code>fp_matrix</code></dt><dd><p>An <code>integer</code> matrix, where
<code>fp_matrix[k1, k2]</code> is a label for the file pair <code>(k1, k2)</code>. Note
that <code>fp_matrix[k1, k2] = fp_matrix[k2, k1]</code>.</p>
</dd>
<dt><code>rp_to_fp</code></dt><dd><p>A <code>logical</code> matrix that indicates which record
pairs belong to which file pairs. <code>rp_to_fp[fp, rp]</code> is <code>TRUE</code> if
the records  <code>record_pairs[rp, ]</code> belong to the file pair <code>fp</code>,
and is FALSE otherwise. Note that <code>fp</code> is given by the labeling in
<code>fp_matrix</code>.</p>
</dd>
<dt><code>ab</code></dt><dd><p>An <code>integer</code> vector, of length
<code>ncol(comparisons) * K * (K + 1) / 2</code> that indicates how many record
pairs there are with a given disagreement level for a given field, for each
file pair.</p>
</dd>
<dt><code>file_sizes_not_included</code></dt><dd><p>If according to <code>pairs_to_keep</code>
there are records which are not potential matches to any other records, the
remaining records are relabeled (see <code>labels</code>), and
<code>file_sizes_not_included</code> indicates, for each file, the number of such
records that were removed.</p>
</dd>
<dt><code>ab_not_included</code></dt><dd><p>For record pairs not included according to
<code>pairs_to_keep</code>, this is an <code>integer</code> vector, of length
<code>ncol(comparisons) * K * (K + 1) / 2</code> that indicates how many record
pairs there are with a given disagreement level for a given field, for each
file pair.</p>
</dd>
<dt><code>labels</code></dt><dd><p>If according to <code>pairs_to_keep</code>
there are records which are not potential matches to any other records, the
remaining records are relabeled. <code>labels</code> provides a dictionary that
indicates, for each of the new labels, which record in the original
labeling the new label corresponds to. In particular, the first column
indicates the record in the original labeling, and the second column
indicates the new labeling.</p>
</dd>
<dt><code>pairs_to_keep</code></dt><dd><p>A <code>logical</code> vector, the same length as
<code>comparison_list$record_pairs</code>, indicating which record pairs were
kept as potential matches. This may not be the same as the input
<code>pairs_to_keep</code> if <code>cc</code> was set to 1.</p>
</dd>
<dt><code>cc</code></dt><dd><p>A <code>numeric</code> indicator of whether the connected
components of the potential matches are closed under transitivity.</p>
</dd>
</dl>



<h3>References</h3>

<p>Serge Aleshin-Guendel &amp; Mauricio Sadinle (2022). Multifile Partitioning for Record Linkage and Duplicate Detection. <em>Journal of the
American Statistical Association</em>. [doi: <a href="https://doi.org/10.1080/01621459.2021.2013242">10.1080/01621459.2021.2013242</a>][<a href="https://arxiv.org/abs/2110.03839">arXiv</a>]
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Example with small duplicate dataset
data(dup_data_small)

# Create the comparison data
comparison_list &lt;- create_comparison_data(dup_data_small$records,
 types = c("bi", "lv", "lv", "lv", "lv", "bi", "bi"),
 breaks = list(NA,  c(0, 0.25, 0.5),  c(0, 0.25, 0.5),
               c(0, 0.25, 0.5), c(0, 0.25, 0.5),  NA, NA),
 file_sizes = dup_data_small$file_sizes,
 duplicates = c(1, 1, 1))

# Reduce the comparison data
# The following line corresponds to only keeping pairs of records for which
# neither gname nor fname disagree at the highest level
pairs_to_keep &lt;- (comparison_list$comparisons[, "gname_DL_3"] != TRUE) &amp;
 (comparison_list$comparisons[, "fname_DL_3"] != TRUE)
reduced_comparison_list &lt;- reduce_comparison_data(comparison_list,
 pairs_to_keep, cc = 1)
</code></pre>

<hr>
<h2 id='relabel_bayes_estimate'>Relabel the Bayes Estimate of a Partition</h2><span id='topic+relabel_bayes_estimate'></span>

<h3>Description</h3>

<p>Relabel the Bayes estimate of a partition, for use after using indexing to
reduce the number of record pairs that are potential matches.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>relabel_bayes_estimate(reduced_comparison_list, bayes_estimate)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="relabel_bayes_estimate_+3A_reduced_comparison_list">reduced_comparison_list</code></td>
<td>
<p>The output from a call to
<code><a href="#topic+reduce_comparison_data">reduce_comparison_data</a></code>.</p>
</td></tr>
<tr><td><code id="relabel_bayes_estimate_+3A_bayes_estimate">bayes_estimate</code></td>
<td>
<p>The output from a call to
<code><a href="#topic+find_bayes_estimate">find_bayes_estimate</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When the function <code><a href="#topic+reduce_comparison_data">reduce_comparison_data</a></code> is used to reduce the
number of record pairs that are potential matches, it may be the case that
some records are declared to not be potential matches to any other records.
In this case, the indexing method has made the decision that these records
have no matches, and thus we can remove them from the data set and relabel
the remaining records; see the documentation for <code>labels</code> in
<code><a href="#topic+reduce_comparison_data">reduce_comparison_data</a></code> for information on how to go between the
original labeling and the new labeling. The purpose of this function is to
relabel the output of <code><a href="#topic+find_bayes_estimate">find_bayes_estimate</a></code> when the function
<code><a href="#topic+reduce_comparison_data">reduce_comparison_data</a></code> is used, so that the user doesn't have
to do this relabeling themselves.
</p>


<h3>Value</h3>

<p>A <code>data.frame</code>, with as many rows as
<code>sum(reduced_comparison_list$file_sizes +
reduced_comparison_list$file_sizes_not_included)</code>, i.e. the number of
records originally input to <code><a href="#topic+create_comparison_data">create_comparison_data</a></code>, before
indexing occurred. This <code>data.frame</code> has two columns,
<code>"original_labels"</code> and <code>"link_id"</code>. Given row <code>i</code> of
<code>records</code> originally input to <code><a href="#topic+create_comparison_data">create_comparison_data</a></code>,
the linkage id according to <code>bayes_estimate</code> is given by the <code>i</code>th
row of the <code>link_id</code> column. See the documentation for
<code><a href="#topic+find_bayes_estimate">find_bayes_estimate</a></code> for information on how to interpret this
linkage id.
</p>


<h3>References</h3>

<p>Serge Aleshin-Guendel &amp; Mauricio Sadinle (2022). Multifile Partitioning for Record Linkage and Duplicate Detection. <em>Journal of the
American Statistical Association</em>. [doi: <a href="https://doi.org/10.1080/01621459.2021.2013242">10.1080/01621459.2021.2013242</a>][<a href="https://arxiv.org/abs/2110.03839">arXiv</a>]
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Example with small duplicate dataset
data(dup_data_small)

# Create the comparison data
comparison_list &lt;- create_comparison_data(dup_data_small$records,
 types = c("bi", "lv", "lv", "lv", "lv", "bi", "bi"),
 breaks = list(NA,  c(0, 0.25, 0.5),  c(0, 0.25, 0.5),
               c(0, 0.25, 0.5), c(0, 0.25, 0.5),  NA, NA),
 file_sizes = dup_data_small$file_sizes,
 duplicates = c(1, 1, 1))

# Reduce the comparison data
# The following line corresponds to only keeping pairs of records for which
# neither gname nor fname disagree at the highest level
pairs_to_keep &lt;- (comparison_list$comparisons[, "gname_DL_3"] != TRUE) &amp;
 (comparison_list$comparisons[, "fname_DL_3"] != TRUE)
reduced_comparison_list &lt;- reduce_comparison_data(comparison_list,
 pairs_to_keep, cc = 1)

# Specify the prior
prior_list &lt;- specify_prior(reduced_comparison_list, mus = NA, nus = NA,
 flat = 0, alphas = rep(1, 7), dup_upper_bound = c(10, 10, 10),
 dup_count_prior_family = c("Poisson", "Poisson", "Poisson"),
 dup_count_prior_pars = list(c(1), c(1), c(1)), n_prior_family = "uniform",
 n_prior_pars = NA)

# Run the Gibbs sampler
{
results &lt;- gibbs_sampler(reduced_comparison_list, prior_list, n_iter = 1000,
 seed = 42)

# Find the full Bayes estimate
full_estimate &lt;- find_bayes_estimate(results$partitions, burn_in = 100,
 L_FNM = 1, L_FM1 = 1, L_FM2 = 2, L_A = Inf, max_cc_size = 50)

# Find the partial Bayes estimate
partial_estimate &lt;- find_bayes_estimate(results$partitions, burn_in = 100,
 L_FNM = 1, L_FM1 = 1, L_FM2 = 2, L_A = 0.1, max_cc_size = 12)

# Relabel the full and partial Bayes estimates
full_estimate_relabel &lt;- relabel_bayes_estimate(reduced_comparison_list,
 full_estimate)

partial_estimate_relabel &lt;- relabel_bayes_estimate(reduced_comparison_list,
 partial_estimate)

# Add columns to the records corresponding to their full and partial
# Bayes estimates
dup_data_small$records &lt;- cbind(dup_data_small$records,
 full_estimate_id = full_estimate_relabel$link_id,
 partial_estimate_id = partial_estimate_relabel$link_id)
}
</code></pre>

<hr>
<h2 id='specify_prior'>Specify the Prior Distributions</h2><span id='topic+specify_prior'></span>

<h3>Description</h3>

<p>Specify the prior distributions for the <code class="reqn">m</code> and <code class="reqn">u</code> parameters of the
models for comparison data among matches and non-matches, and the partition.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>specify_prior(
  comparison_list,
  mus = NA,
  nus = NA,
  flat = 0,
  alphas = NA,
  dup_upper_bound = NA,
  dup_count_prior_family = NA,
  dup_count_prior_pars = NA,
  n_prior_family = NA,
  n_prior_pars = NA
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="specify_prior_+3A_comparison_list">comparison_list</code></td>
<td>
<p>the output from a call to
<code><a href="#topic+create_comparison_data">create_comparison_data</a></code> or <code><a href="#topic+reduce_comparison_data">reduce_comparison_data</a></code>.
Note that in order to correctly specify the prior, if
<code><a href="#topic+reduce_comparison_data">reduce_comparison_data</a></code> is used to the reduce the number of
record pairs that are potential matches, then the output of
<code><a href="#topic+reduce_comparison_data">reduce_comparison_data</a></code> (not
<code><a href="#topic+create_comparison_data">create_comparison_data</a></code>) should be used for this argument.</p>
</td></tr>
<tr><td><code id="specify_prior_+3A_mus">mus</code>, <code id="specify_prior_+3A_nus">nus</code></td>
<td>
<p>The hyperparameters of the Dirichlet priors for the <code class="reqn">m</code>
and <code class="reqn">u</code> parameters for the comparisons among matches and non-matches,
respectively. These are positive <code>numeric</code> vectors which have length
equal to the number of  columns of <code>comparison_list$comparisons</code> times
the number of file pairs
<code>(comparison_list$K * (comparison_list$K + 1) / 2)</code>. If set to
<code>NA</code>, flat priors are used. We recommend using flat priors for <code class="reqn">m</code>
and <code class="reqn">u</code>.</p>
</td></tr>
<tr><td><code id="specify_prior_+3A_flat">flat</code></td>
<td>
<p>A <code>numeric</code> indicator of whether a flat prior for partitions
should be used.  <code>flat</code> should be <code>1</code> if a flat prior is used, and
<code>flat</code> should be <code>0</code> if a structured prior is used. If a flat prior
is used, the remaining arguments should be set to <code>NA</code>. Otherwise, the
remaining arguments should be specified. We do not recommend using a flat
prior for partitions in general.</p>
</td></tr>
<tr><td><code id="specify_prior_+3A_alphas">alphas</code></td>
<td>
<p>The hyperparameters for the Dirichlet-multinomial overlap table
prior, a positive <code>numeric</code> vector of length
<code>2 ^ comparison_list$K - 1</code>. The indexing of these hyperparameters is
based on the the <code>comparison_list$K</code>-bit binary representation of the
inclusion patterns of the overlap table. To give a few examples, suppose
<code>comparison_list$K</code> is <code>3</code>. <code>1</code> in <code>3</code>-bit binary is
<code>001</code>, so <code>alphas[1]</code> is the hyperparameter for the
<code>001</code> cell of the overlap table, representing clusters containing only
records from the third file. <code>2</code> in <code>3</code>-bit binary is
<code>010</code>, so <code>alphas[2]</code> is the hyperparameter for the
<code>010</code> cell of the overlap table, representing clusters containing only
records from the second file. <code>3</code> in <code>3</code>-bit binary is
<code>011</code>, so <code>alphas[3]</code> is the hyperparameter for the
<code>011</code> cell of the overlap table, representing clusters containing only
records from the second and third files. If set to <code>NA</code>, the
hyperparameters will all be set to <code>1</code>.</p>
</td></tr>
<tr><td><code id="specify_prior_+3A_dup_upper_bound">dup_upper_bound</code></td>
<td>
<p>A <code>numeric</code> vector indicating the maximum number
of duplicates, from each file, allowed in each cluster. For a given file
<code>k</code>, <code>dup_upper_bound[k]</code> should be between <code>1</code> and
<code>comparison_list$file_sizes[k]</code>, i.e. even if you don't want to impose
an upper bound, you have to implicitly place an upper bound: the number of
records in a file. If set to <code>NA</code>, the upper bound for file <code>k</code>
will be set to <code>1</code> if no duplicates are allowed for that file, or
<code>comparison_list$file_sizes[k]</code> if duplicates are allowed for that file.</p>
</td></tr>
<tr><td><code id="specify_prior_+3A_dup_count_prior_family">dup_count_prior_family</code></td>
<td>
<p>A <code>character</code> vector indicating the
prior distribution family used for the number of duplicates in each cluster,
for each file. Currently the only option is <code>"Poisson"</code> for a Poisson
prior, truncated to lie between <code>1</code> and <code>dup_upper_bound[k]</code>. The
mean parameter of the Poisson distribution is specified using the
<code>dup_count_prior_pars</code> argument. If set to <code>NA</code>, a Poisson prior
with mean <code>1</code> will be used.</p>
</td></tr>
<tr><td><code id="specify_prior_+3A_dup_count_prior_pars">dup_count_prior_pars</code></td>
<td>
<p>A <code>list</code> containing the parameters for
the prior distribution for the number of duplicates in each cluster, for each
file. For file <code>k</code>, when <code>dup_count_prior_family[k]="Poisson"</code>,
<code>dup_count_prior_pars[[k]]</code> is a positive constant representing the mean
of the Poisson prior.</p>
</td></tr>
<tr><td><code id="specify_prior_+3A_n_prior_family">n_prior_family</code></td>
<td>
<p>A <code>character</code> indicating the prior distribution
family used for <code>n</code>, the number of clusters represented in the
records. Note that this includes records determined not to be potential
matches to any other records using <code><a href="#topic+reduce_comparison_data">reduce_comparison_data</a></code>.
Currently the there are two options: <code>"uniform"</code> for a uniform prior
for <code>n</code>, i.e. <code class="reqn">p(n) \propto 1</code>, and <code>"scale"</code> for a scale prior
for <code>n</code>, i.e. <code class="reqn">p(n) \propto 1/n</code>. If set to <code>NA</code>, a uniform
prior will be used.</p>
</td></tr>
<tr><td><code id="specify_prior_+3A_n_prior_pars">n_prior_pars</code></td>
<td>
<p>Currently set to <code>NA</code>. When more prior distribution
families for <code>n</code> are implemented, this will be a vector of parameters
for those priors.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The purpose of this function is to specify prior distributions for all
parameters of the model. Please note that if
<code><a href="#topic+reduce_comparison_data">reduce_comparison_data</a></code> is used to the reduce the number of
record pairs that are potential matches, then the output of
<code><a href="#topic+reduce_comparison_data">reduce_comparison_data</a></code> (not
<code><a href="#topic+create_comparison_data">create_comparison_data</a></code>) should be used as input.
</p>
<p>For the hyperparameters of the Dirichlet priors for the <code class="reqn">m</code>
and <code class="reqn">u</code> parameters for the comparisons among matches and non-matches,
respectively, we recommend using a flat prior. This is accomplished by
setting <code>mus=NA</code> and <code>nus=NA</code>. Informative prior specifications
are possible, but in practice they will be overwhelmed by the large number of
comparisons.
</p>
<p>For the prior for partitions, we do not recommend using a flat prior. Instead
we recommend using our structure prior for partitions. By setting
<code>flat=0</code> and the remaining arguments to <code>NA</code>, one obtains the
default specification for the structured prior that we have found to perform
well in simulation studies. The structured prior for partitions is specified
as follows:
</p>

<ul>
<li><p> Specify a prior for <code>n</code>, the number of clusters represented in
the records. Note that this includes records determined not to be potential
matches to any other records using <code><a href="#topic+reduce_comparison_data">reduce_comparison_data</a></code>.
Currently, a uniform prior and a scale prior for <code>n</code> are supported.
Our default specification uses a uniform prior.
</p>
</li>
<li><p> Specify a prior for the overlap table (see the documentation for
<code>alphas</code> for more information).  Currently a Dirichlet-multinomial
prior is supported. Our default specification sets all hyperparameters of
the Dirichlet-multinomial prior to <code>1</code>.
</p>
</li>
<li><p> For each file, specify a prior for the number of duplicates in each
cluster. As a part of this prior, we specify the maximum number of records
in a cluster for each file, through <code>dup_upper_bound</code>. When there
are assumed to be no duplicates in a file, the maximum number of records in
a cluster for that file is set to <code>1</code>. When there are assumed to be
duplicates in a file, we recommend setting the maximum number of records in
a cluster for that file to be less than the file size, if prior knowledge
allows. Currently, a Poisson prior for the the number of duplicates in
each cluster is supported. Our default specification uses a Poisson prior
with mean <code>1</code>.
</p>
</li></ul>

<p>Please contact the package maintainer if you need new prior families
for <code>n</code> or the number of duplicates in each cluster to be supported.
</p>


<h3>Value</h3>

<p>a list containing:
</p>

<dl>
<dt><code>mus</code></dt><dd><p>The hyperparameters of the Dirichlet priors for the
<code>m</code> parameters for the comparisons among matches.</p>
</dd>
<dt><code>nus</code></dt><dd><p>The hyperparameters of the Dirichlet priors for the
<code>u</code> parameters for the comparisons among non-matches. Includes data
from comparisons of record pairs that were declared to not be potential
matches using <code><a href="#topic+reduce_comparison_data">reduce_comparison_data</a></code>.</p>
</dd>
<dt><code>flat</code></dt><dd><p>A <code>numeric</code> indicator of whether a flat prior for
partitions should be used. <code>flat</code> is <code>1</code> if a flat prior is used,
and <code>flat</code> is <code>0</code> if a structured prior is used.</p>
</dd>
<dt><code>no_dups</code></dt><dd><p>A <code>numeric</code> indicator of whether no duplicates
are allowed in all of the files.</p>
</dd>
<dt><code>alphas</code></dt><dd><p>The hyperparameters for the Dirichlet-multinomial
overlap table prior, a positive <code>numeric</code> vector of length
<code>2 ^ comparison_list$K</code>, where the first element is <code>0</code>.</p>
</dd>
<dt><code>alpha_0</code></dt><dd><p>The sum of <code>alphas</code>.</p>
</dd>
<dt><code>dup_upper_bound</code></dt><dd><p>A <code>numeric</code> vector indicating the
maximum number of duplicates, from each file, allowed in each cluster. For
a given file <code>k</code>, <code>dup_upper_bound[k]</code> should be between <code>1</code>
and <code>comparison_list$file_sizes[k]</code>, i.e. even if you don't want to
impose an upper bound, you have to implicitly place an upper bound: the
number of records in a file.</p>
</dd>
<dt><code>log_dup_count_prior</code></dt><dd><p>A <code>list</code> containing the log density
of the prior distribution for the number of duplicates in each cluster, for
each file.</p>
</dd>
<dt><code>log_n_prior</code></dt><dd><p>A <code>numeric</code> vector containing the log
density of the prior distribution for the number of clusters represented in
the records.</p>
</dd>
<dt><code>nus_specified</code></dt><dd><p>The <code>nus</code> before data from comparisons of
record pairs that were declared to not be potential matches using
<code><a href="#topic+reduce_comparison_data">reduce_comparison_data</a></code> are added. Used for input checking.</p>
</dd>
</dl>



<h3>References</h3>

<p>Serge Aleshin-Guendel &amp; Mauricio Sadinle (2022). Multifile Partitioning for Record Linkage and Duplicate Detection. <em>Journal of the
American Statistical Association</em>. [doi: <a href="https://doi.org/10.1080/01621459.2021.2013242">10.1080/01621459.2021.2013242</a>] [<a href="https://arxiv.org/abs/2110.03839">arXiv</a>]
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Example with small no duplicate dataset
data(no_dup_data_small)

# Create the comparison data
comparison_list &lt;- create_comparison_data(no_dup_data_small$records,
 types = c("bi", "lv", "lv", "lv", "lv", "bi", "bi"),
 breaks = list(NA,  c(0, 0.25, 0.5),  c(0, 0.25, 0.5),
               c(0, 0.25, 0.5), c(0, 0.25, 0.5),  NA, NA),
 file_sizes = no_dup_data_small$file_sizes,
 duplicates = c(0, 0, 0))

# Specify the prior
prior_list &lt;- specify_prior(comparison_list, mus = NA, nus = NA, flat = 0,
 alphas = rep(1, 7), dup_upper_bound = c(1, 1, 1),
 dup_count_prior_family = NA, dup_count_prior_pars = NA,
 n_prior_family = "uniform", n_prior_pars = NA)

# Example with small duplicate dataset
data(dup_data_small)

# Create the comparison data
comparison_list &lt;- create_comparison_data(dup_data_small$records,
 types = c("bi", "lv", "lv", "lv", "lv", "bi", "bi"),
 breaks = list(NA,  c(0, 0.25, 0.5),  c(0, 0.25, 0.5),
               c(0, 0.25, 0.5), c(0, 0.25, 0.5),  NA, NA),
 file_sizes = dup_data_small$file_sizes,
 duplicates = c(1, 1, 1))

# Reduce the comparison data
# The following line corresponds to only keeping pairs of records for which
# neither gname nor fname disagree at the highest level
pairs_to_keep &lt;- (comparison_list$comparisons[, "gname_DL_3"] != TRUE) &amp;
 (comparison_list$comparisons[, "fname_DL_3"] != TRUE)
reduced_comparison_list &lt;- reduce_comparison_data(comparison_list,
 pairs_to_keep, cc = 1)

# Specify the prior
prior_list &lt;- specify_prior(reduced_comparison_list, mus = NA, nus = NA,
 flat = 0, alphas = rep(1, 7), dup_upper_bound = c(10, 10, 10),
 dup_count_prior_family = c("Poisson", "Poisson", "Poisson"),
 dup_count_prior_pars = list(c(1), c(1), c(1)), n_prior_family = "uniform",
 n_prior_pars = NA)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
