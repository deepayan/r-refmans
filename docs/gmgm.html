<!DOCTYPE html><html lang="en"><head><title>Help for package gmgm</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {gmgm}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#gmgm-package'><p>Gaussian mixture graphical model learning and inference</p></a></li>
<li><a href='#add_arcs'><p>Add arcs to a Gaussian mixture graphical model</p></a></li>
<li><a href='#add_nodes'><p>Add nodes to a Gaussian mixture graphical model</p></a></li>
<li><a href='#add_var'><p>Add variables to a Gaussian mixture model</p></a></li>
<li><a href='#aggregation'><p>Aggregate particles to obtain inferred values</p></a></li>
<li><a href='#AIC'><p>Compute the Akaike Information Criterion (AIC) of a Gaussian mixture model or</p>
graphical model</a></li>
<li><a href='#BIC'><p>Compute the Bayesian Information Criterion (BIC) of a Gaussian mixture model</p>
or graphical model</a></li>
<li><a href='#conditional'><p>Conditionalize a Gaussian mixture model</p></a></li>
<li><a href='#data_air'><p>Beijing air quality dataset</p></a></li>
<li><a href='#data_body'><p>NHANES body composition dataset</p></a></li>
<li><a href='#density'><p>Compute densities of a Gaussian mixture model</p></a></li>
<li><a href='#ellipses'><p>Display the mixture components of a Gaussian mixture model</p></a></li>
<li><a href='#em'><p>Estimate the parameters of a Gaussian mixture model</p></a></li>
<li><a href='#expectation'><p>Compute expectations of a Gaussian mixture model</p></a></li>
<li><a href='#filtering'><p>Perform filtering inference in a Gaussian mixture dynamic Bayesian network</p></a></li>
<li><a href='#gmbn'><p>Create a Gaussian mixture Bayesian network</p></a></li>
<li><a href='#gmbn_body'><p>Gaussian mixture Bayesian network learned from the NHANES body composition</p>
dataset</a></li>
<li><a href='#gmdbn'><p>Create a Gaussian mixture dynamic Bayesian network</p></a></li>
<li><a href='#gmdbn_air'><p>Gaussian mixture dynamic Bayesian network learned from the Beijing air</p>
quality dataset</a></li>
<li><a href='#gmm'><p>Create a Gaussian mixture model</p></a></li>
<li><a href='#gmm_body'><p>Gaussian mixture model learned from the NHANES body composition dataset</p></a></li>
<li><a href='#inference'><p>Perform inference in a Gaussian mixture Bayesian network</p></a></li>
<li><a href='#logLik'><p>Compute the log-likelihood of a Gaussian mixture model or graphical model</p></a></li>
<li><a href='#merge_comp'><p>Merge mixture components of a Gaussian mixture model</p></a></li>
<li><a href='#network'><p>Display the graphical structure of a Gaussian mixture Bayesian network</p></a></li>
<li><a href='#param_em'><p>Learn the parameters of a Gaussian mixture graphical model with incomplete</p>
data</a></li>
<li><a href='#param_learn'><p>Learn the parameters of a Gaussian mixture graphical model</p></a></li>
<li><a href='#particles'><p>Initialize particles to perform inference in a Gaussian mixture graphical</p>
model</a></li>
<li><a href='#prediction'><p>Perform predictive inference in a Gaussian mixture dynamic Bayesian network</p></a></li>
<li><a href='#propagation'><p>Propagate particles forward in time</p></a></li>
<li><a href='#relevant'><p>Extract the minimal sub-Gaussian mixture graphical model required to infer a</p>
subset of nodes</a></li>
<li><a href='#remove_arcs'><p>Remove arcs from a Gaussian mixture graphical model</p></a></li>
<li><a href='#remove_nodes'><p>Remove nodes from a Gaussian mixture graphical model</p></a></li>
<li><a href='#remove_var'><p>Remove variables from a Gaussian mixture model</p></a></li>
<li><a href='#rename_nodes'><p>Rename nodes of a Gaussian mixture graphical model</p></a></li>
<li><a href='#rename_var'><p>Rename variables of a Gaussian mixture model</p></a></li>
<li><a href='#reorder'><p>Reorder the variables and the mixture components of a Gaussian mixture model</p></a></li>
<li><a href='#sampling'><p>Sample a Gaussian mixture model</p></a></li>
<li><a href='#smem'><p>Select the number of mixture components and estimate the parameters of a</p>
Gaussian mixture model</a></li>
<li><a href='#smoothing'><p>Perform smoothing inference in a Gaussian mixture dynamic Bayesian network</p></a></li>
<li><a href='#split_comp'><p>Split a mixture component of a Gaussian mixture model</p></a></li>
<li><a href='#stepwise'><p>Select the explanatory variables, the number of mixture components and</p>
estimate the parameters of a conditional Gaussian mixture model</a></li>
<li><a href='#struct_em'><p>Learn the structure and the parameters of a Gaussian mixture graphical model</p>
with incomplete data</a></li>
<li><a href='#struct_learn'><p>Learn the structure and the parameters of a Gaussian mixture graphical model</p></a></li>
<li><a href='#structure'><p>Provide the graphical structure of a Gaussian mixture graphical model</p></a></li>
<li><a href='#summary'><p>Summarize a Gaussian mixture model or graphical model</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Gaussian Mixture Graphical Model Learning and Inference</td>
</tr>
<tr>
<td>Version:</td>
<td>1.1.2</td>
</tr>
<tr>
<td>Description:</td>
<td>Gaussian mixture graphical models include Bayesian networks and
    dynamic Bayesian networks (their temporal extension) whose local probability
    distributions are described by Gaussian mixture models. They are powerful
    tools for graphically and quantitatively representing nonlinear dependencies
    between continuous variables. This package provides a complete framework to
    create, manipulate, learn the structure and the parameters, and perform
    inference in these models. Most of the algorithms are described in the PhD
    thesis of Roos (2018) <a href="https://tel.archives-ouvertes.fr/tel-01943718">https://tel.archives-ouvertes.fr/tel-01943718</a>.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>dplyr (&ge; 1.0.5), ggplot2 (&ge; 3.2.1), purrr (&ge; 0.3.3), rlang
(&ge; 0.4.10), stats (&ge; 3.5.0), stringr (&ge; 1.4.0), tidyr (&ge;
1.0.0), visNetwork (&ge; 2.0.8)</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat (&ge; 2.3.2)</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-09-08 20:12:45 UTC; jerem</td>
</tr>
<tr>
<td>Author:</td>
<td>Jérémy Roos [aut, cre, cph],
  RATP Group [fnd, cph]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Jérémy Roos &lt;jeremy.roos@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-09-08 20:32:55 UTC</td>
</tr>
</table>
<hr>
<h2 id='gmgm-package'>Gaussian mixture graphical model learning and inference</h2><span id='topic+gmgm-package'></span>

<h3>Description</h3>

<p>This package provides a complete framework to deal with Gaussian mixture
graphical models, which covers Bayesian networks and dynamic Bayesian
networks (their temporal extension) whose local probability distributions are
described by Gaussian mixture models. It includes a wide range of functions
for:
</p>

<ul>
<li><p> creating or modifying the structure of Gaussian mixture models
(<code><a href="#topic+add_var">add_var</a></code>, <code><a href="#topic+gmm">gmm</a></code>, <code><a href="#topic+merge_comp">merge_comp</a></code>,
<code><a href="#topic+remove_var">remove_var</a></code>, <code><a href="#topic+rename_var">rename_var</a></code>, <code><a href="#topic+reorder">reorder</a></code>,
<code><a href="#topic+split_comp">split_comp</a></code>) or graphical models (<code><a href="#topic+add_arcs">add_arcs</a></code>,
<code><a href="#topic+add_nodes">add_nodes</a></code>, <code><a href="#topic+gmbn">gmbn</a></code>, <code><a href="#topic+gmdbn">gmdbn</a></code>,
<code><a href="#topic+relevant">relevant</a></code>, <code><a href="#topic+remove_arcs">remove_arcs</a></code>,
<code><a href="#topic+remove_nodes">remove_nodes</a></code>, <code><a href="#topic+rename_nodes">rename_nodes</a></code>);
</p>
</li>
<li><p> describing or visualizing Gaussian mixture models
(<code><a href="#topic+conditional">conditional</a></code>, <code><a href="#topic+ellipses">ellipses</a></code>,
<code><a href="#topic+summary.gmm">summary.gmm</a></code>) or graphical models (<code><a href="#topic+network">network</a></code>,
<code><a href="#topic+structure">structure</a></code>, <code><a href="#topic+summary.gmbn">summary.gmbn</a></code>,
<code><a href="#topic+summary.gmdbn">summary.gmdbn</a></code>);
</p>
</li>
<li><p> computing densities, expectations, or sampling Gaussian mixture models
(<code><a href="#topic+density">density</a></code>, <code><a href="#topic+expectation">expectation</a></code>, <code><a href="#topic+sampling">sampling</a></code>);
</p>
</li>
<li><p> computing scores of Gaussian mixture models (<code><a href="#topic+AIC.gmm">AIC.gmm</a></code>,
<code><a href="#topic+BIC.gmm">BIC.gmm</a></code>, <code><a href="#topic+logLik.gmm">logLik.gmm</a></code>) or graphical models
(<code><a href="#topic+AIC.gmbn">AIC.gmbn</a></code>, <code><a href="#topic+AIC.gmdbn">AIC.gmdbn</a></code>, <code><a href="#topic+BIC.gmbn">BIC.gmbn</a></code>,
<code><a href="#topic+BIC.gmdbn">BIC.gmdbn</a></code>, <code><a href="#topic+logLik.gmbn">logLik.gmbn</a></code>,
<code><a href="#topic+logLik.gmdbn">logLik.gmdbn</a></code>);
</p>
</li>
<li><p> learning the structure and/or the parameters of Gaussian mixture models
(<code><a href="#topic+em">em</a></code>, <code><a href="#topic+smem">smem</a></code>, <code><a href="#topic+stepwise">stepwise</a></code>) or graphical
models (<code><a href="#topic+param_em">param_em</a></code>, <code><a href="#topic+param_learn">param_learn</a></code>,
<code><a href="#topic+struct_em">struct_em</a></code>, <code><a href="#topic+struct_learn">struct_learn</a></code>);
</p>
</li>
<li><p> performing inference in Gaussian mixture graphical models
(<code><a href="#topic+aggregation">aggregation</a></code>, <code><a href="#topic+filtering">filtering</a></code>, <code><a href="#topic+inference">inference</a></code>,
<code><a href="#topic+particles">particles</a></code>, <code><a href="#topic+prediction">prediction</a></code>, <code><a href="#topic+propagation">propagation</a></code>,
<code><a href="#topic+smoothing">smoothing</a></code>).
</p>
</li></ul>

<p>Descriptions of these functions are provided in this manual with related
references. Most of the algorithms are described in the PhD thesis of Roos
(2018, in french). To better handle this package, two real-world datasets are
provided (<code><a href="#topic+data_air">data_air</a></code>, <code><a href="#topic+data_body">data_body</a></code>) with examples of
Gaussian mixture models and graphical models (<code><a href="#topic+gmbn_body">gmbn_body</a></code>,
<code><a href="#topic+gmdbn_air">gmdbn_air</a></code>, <code><a href="#topic+gmm_body">gmm_body</a></code>).
</p>


<h3>References</h3>

<p>Roos, J. (2018). <em>Pr&eacute;vision a court terme des
flux de voyageurs : une approche par les r&eacute;seaux
bay&eacute;siens</em>. PhD thesis, University of Lyon.
</p>

<hr>
<h2 id='add_arcs'>Add arcs to a Gaussian mixture graphical model</h2><span id='topic+add_arcs'></span>

<h3>Description</h3>

<p>This function adds arcs to a Gaussian mixture graphical model. For each added
arc, a variable related to the start node is added to the Gaussian mixture
model describing the local distribution over the end node and its parents,
with mean 0 and variance 1 for each mixture component.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>add_arcs(gmgm, arcs)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="add_arcs_+3A_gmgm">gmgm</code></td>
<td>
<p>An object of class <code>gmbn</code> or <code>gmdbn</code>.</p>
</td></tr>
<tr><td><code id="add_arcs_+3A_arcs">arcs</code></td>
<td>
<p>A data frame containing the added arcs. The column <code>from</code>
describes the start node, the column <code>to</code> the end node and the column
<code>lag</code> the time lag between them. Missing values in <code>from</code> or
<code>to</code> are interpreted as &quot;all possible nodes&quot;, which allows to quickly
define large set of arcs that share common attributes. Missing values in
<code>lag</code> are replaced by 0. If <code>gmgm</code> is a <code>gmdbn</code> object, the
same arcs are added to each of its <code>gmbn</code> elements. This constraint can
be overcome by passing a list of data frames named after some of these
elements (<code>b_1</code>, ...) and containing arcs specifically added to them.
The arcs whose time lags exceed the maximum temporal depth of their
<code>gmbn</code> element are not taken into account.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>gmbn</code> or <code>gmdbn</code> object after adding the arcs.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+add_nodes">add_nodes</a></code>, <code><a href="#topic+relevant">relevant</a></code>,
<code><a href="#topic+remove_arcs">remove_arcs</a></code>, <code><a href="#topic+remove_nodes">remove_nodes</a></code>,
<code><a href="#topic+rename_nodes">rename_nodes</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(gmbn_body)
gmbn_1 &lt;- add_arcs(gmbn_body,
                   data.frame(from = c("GENDER", "AGE"),
                              to = c("GLYCO", "WEIGHT")))

data(gmdbn_air)
gmdbn_1 &lt;- add_arcs(gmdbn_air,
                    list(b_2 = data.frame(from = "WIND", to = "NO2", lag = 1),
                         b_13 = data.frame(from = c("NO2", "NO2"),
                                           to = c("O3", "O3"), lag = c(0, 1))))

</code></pre>

<hr>
<h2 id='add_nodes'>Add nodes to a Gaussian mixture graphical model</h2><span id='topic+add_nodes'></span>

<h3>Description</h3>

<p>This function adds nodes to a Gaussian mixture graphical model. If this model
is a dynamic Bayesian network, the nodes are added to each of its transition
models. For each added node, a one-component univariate Gaussian mixture
model is created with mean 0 and variance 1.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>add_nodes(gmgm, nodes)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="add_nodes_+3A_gmgm">gmgm</code></td>
<td>
<p>An object of class <code>gmbn</code> or <code>gmdbn</code>. If <code>NULL</code>, a
<code>gmbn</code> object is created with the added nodes.</p>
</td></tr>
<tr><td><code id="add_nodes_+3A_nodes">nodes</code></td>
<td>
<p>A character vector containing the added nodes.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>gmbn</code> or <code>gmdbn</code> object after adding the nodes.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+add_arcs">add_arcs</a></code>, <code><a href="#topic+relevant">relevant</a></code>,
<code><a href="#topic+remove_arcs">remove_arcs</a></code>, <code><a href="#topic+remove_nodes">remove_nodes</a></code>,
<code><a href="#topic+rename_nodes">rename_nodes</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(gmbn_body)
gmbn_1 &lt;- add_nodes(gmbn_body, c("CHOL", "TRIGLY"))

data(gmdbn_air)
gmdbn_1 &lt;- add_nodes(gmdbn_air, "PM10")

</code></pre>

<hr>
<h2 id='add_var'>Add variables to a Gaussian mixture model</h2><span id='topic+add_var'></span>

<h3>Description</h3>

<p>This function adds variables to a Gaussian mixture model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>add_var(gmm, var)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="add_var_+3A_gmm">gmm</code></td>
<td>
<p>An object of class <code>gmm</code>. If <code>NULL</code>, a <code>gmm</code> object
is created with the added variables and one mixture component.</p>
</td></tr>
<tr><td><code id="add_var_+3A_var">var</code></td>
<td>
<p>A character vector containing the added variables, or a data frame
or numeric matrix whose columns are named after the added variables. In the
first case, for each mixture component, the marginal mean vector of the added
variables is 0 and the marginal covariance matrix the identity matrix. In the
second case, these mean vector and covariance matrix are computed from the
data (after removing the rows that contain missing values).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>gmm</code> object after adding the variables.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+remove_var">remove_var</a></code>, <code><a href="#topic+rename_var">rename_var</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(gmm_body, data_body)
gmm_1 &lt;- add_var(gmm_body, "GENDER")
gmm_2 &lt;- add_var(gmm_body, data_body[, "GENDER"])

</code></pre>

<hr>
<h2 id='aggregation'>Aggregate particles to obtain inferred values</h2><span id='topic+aggregation'></span>

<h3>Description</h3>

<p>This function aggregates particles to obtain inferred values. Assuming that
the particles have been propagated to a given time slice <code class="reqn">t</code>, the
weighted average of the samples is computed to estimate the state of the
system at <code class="reqn">t</code> or at previous time slices (Koller and Friedman, 2009).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>aggregation(part, nodes, col_seq = NULL, col_weight = "weight", lag = 0)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="aggregation_+3A_part">part</code></td>
<td>
<p>A data frame containing the particles propagated to time slice
<code class="reqn">t</code>, as obtained from function <code><a href="#topic+particles">particles</a></code> or
<code><a href="#topic+propagation">propagation</a></code>.</p>
</td></tr>
<tr><td><code id="aggregation_+3A_nodes">nodes</code></td>
<td>
<p>A character vector containing the inferred nodes.</p>
</td></tr>
<tr><td><code id="aggregation_+3A_col_seq">col_seq</code></td>
<td>
<p>A character vector containing the column names of <code>part</code>
that describe the observation sequence. If <code>NULL</code> (the default), all the
particles belong to a single sequence.</p>
</td></tr>
<tr><td><code id="aggregation_+3A_col_weight">col_weight</code></td>
<td>
<p>A character string corresponding to the column name of
<code>part</code> that describes the particle weight.</p>
</td></tr>
<tr><td><code id="aggregation_+3A_lag">lag</code></td>
<td>
<p>A non-negative integer vector containing the time lags <code class="reqn">l_1,
l_2, \dots</code> such that the samples of time slices <code class="reqn">t - l_1,
t - l_2, \dots</code> are aggregated.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>If <code>lag</code> has one element, a data frame (tibble) containing the
aggregated values of the inferred nodes and their observation sequences (if
<code>col_seq</code> is not <code>NULL</code>). If <code>lag</code> has two or more elements, a
list of data frames (tibbles) containing these values for each time lag.
</p>


<h3>References</h3>

<p>Koller, D. and Friedman, N. (2009). <em>Probabilistic Graphical Models:
Principles and Techniques</em>. The MIT Press.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+aggregation">aggregation</a></code>, <code><a href="#topic+particles">particles</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(dplyr)
set.seed(0)
data(gmdbn_air, data_air)
evid &lt;- data_air %&gt;%
  group_by(DATE) %&gt;%
  slice(1:3) %&gt;%
  ungroup()
evid$NO2[sample.int(150, 30)] &lt;- NA
evid$O3[sample.int(150, 30)] &lt;- NA
evid$TEMP[sample.int(150, 30)] &lt;- NA
evid$WIND[sample.int(150, 30)] &lt;- NA
aggreg &lt;- particles(data.frame(DATE = unique(evid$DATE))) %&gt;%
  propagation(gmdbn_air, evid, col_seq = "DATE", n_times = 3) %&gt;%
  aggregation(c("NO2", "O3", "TEMP", "WIND"), col_seq = "DATE", lag = c(0, 1))

</code></pre>

<hr>
<h2 id='AIC'>Compute the Akaike Information Criterion (AIC) of a Gaussian mixture model or
graphical model</h2><span id='topic+AIC'></span><span id='topic+AIC.gmm'></span><span id='topic+AIC.gmbn'></span><span id='topic+AIC.gmdbn'></span>

<h3>Description</h3>

<p>This function computes the Akaike Information Criterion (AIC) of a Gaussian
mixture model or graphical model:
</p>
<p style="text-align: center;"><code class="reqn">AIC = logLik - n_{par}</code>
</p>

<p>where <code class="reqn">logLik</code> is the log-likelihood and <code class="reqn">n_{par}</code> the number of free
parameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'gmm'
AIC(object, data, y = NULL, regul = 0.01, ...)

## S3 method for class 'gmbn'
AIC(object, data, col_seq = NULL, ...)

## S3 method for class 'gmdbn'
AIC(object, data, col_seq = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="AIC_+3A_object">object</code></td>
<td>
<p>An object of class <code>gmm</code>, <code>gmbn</code> or <code>gmdbn</code>.</p>
</td></tr>
<tr><td><code id="AIC_+3A_data">data</code></td>
<td>
<p>A data frame containing the data used to compute the AIC. Its
columns must explicitly be named after the variables (or nodes) of
<code>object</code>. If <code>object</code> is a <code>gmm</code> object, a numeric matrix can
be passed.</p>
</td></tr>
<tr><td><code id="AIC_+3A_y">y</code></td>
<td>
<p>A character vector containing the dependent variables if a
conditional AIC is computed. If <code>NULL</code> (the default), the joint AIC is
computed.</p>
</td></tr>
<tr><td><code id="AIC_+3A_regul">regul</code></td>
<td>
<p>A positive numeric value corresponding to the regularization
constant if a penalty term is added for Bayesian regularization. If
<code>NULL</code>, no penalty term is added. If a conditional AIC is computed, this
argument is ignored.</p>
</td></tr>
<tr><td><code id="AIC_+3A_...">...</code></td>
<td>
<p>Unused arguments from the generic function.</p>
</td></tr>
<tr><td><code id="AIC_+3A_col_seq">col_seq</code></td>
<td>
<p>A character vector containing the column names of <code>data</code>
that describe the observation sequence. If <code>NULL</code> (the default), all the
observations belong to a single sequence. If <code>object</code> is a temporal
<code>gmbn</code> or <code>gmdbn</code> object, the observations of a same sequence must
be ordered such that the <code class="reqn">t</code>th one is related to time slice <code class="reqn">t</code>
(note that the sequences can have different lengths). If <code>object</code> is a
non-temporal <code>gmbn</code> object, this argument is ignored.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>If <code>object</code> is a <code>gmm</code> object, a numeric value
corresponding to the AIC.
</p>
<p>If <code>object</code> is a <code>gmbn</code> or <code>gmdbn</code> object, a list with
elements:
</p>
<table role = "presentation">
<tr><td><code>global</code></td>
<td>
<p>A numeric value corresponding to the global AIC.</p>
</td></tr>
<tr><td><code>local</code></td>
<td>
<p>For a <code>gmbn</code> object, a numeric vector containing the local
conditional AICs. For a <code>gmdbn</code> object, a list of numeric vectors
containing these values for each <code>gmbn</code> element.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+BIC">BIC</a></code>, <code><a href="#topic+logLik">logLik</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(gmm_body, data_body)
aic_1 &lt;- AIC(gmm_body, data_body)
aic_2 &lt;- AIC(gmm_body, data_body, y = "WAIST")

data(gmbn_body, data_body)
aic_3 &lt;- AIC(gmbn_body, data_body)

data(gmdbn_air, data_air)
aic_4 &lt;- AIC(gmdbn_air, data_air, col_seq = "DATE")

</code></pre>

<hr>
<h2 id='BIC'>Compute the Bayesian Information Criterion (BIC) of a Gaussian mixture model
or graphical model</h2><span id='topic+BIC'></span><span id='topic+BIC.gmm'></span><span id='topic+BIC.gmbn'></span><span id='topic+BIC.gmdbn'></span>

<h3>Description</h3>

<p>This function computes the Bayesian Information Criterion (BIC) of a Gaussian
mixture model or graphical model:
</p>
<p style="text-align: center;"><code class="reqn">BIC = logLik - \frac{\log(n_{obs})}{2} n_{par}</code>
</p>

<p>where <code class="reqn">logLik</code> is the log-likelihood, <code class="reqn">n_{obs}</code> the number of
observations in the data and <code class="reqn">n_{par}</code> the number of free parameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'gmm'
BIC(object, data, y = NULL, regul = 0.01, ...)

## S3 method for class 'gmbn'
BIC(object, data, col_seq = NULL, ...)

## S3 method for class 'gmdbn'
BIC(object, data, col_seq = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="BIC_+3A_object">object</code></td>
<td>
<p>An object of class <code>gmm</code>, <code>gmbn</code> or <code>gmdbn</code>.</p>
</td></tr>
<tr><td><code id="BIC_+3A_data">data</code></td>
<td>
<p>A data frame containing the data used to compute the BIC. Its
columns must explicitly be named after the variables (or nodes) of
<code>object</code>. If <code>object</code> is a <code>gmm</code> object, a numeric matrix can
be passed.</p>
</td></tr>
<tr><td><code id="BIC_+3A_y">y</code></td>
<td>
<p>A character vector containing the dependent variables if a
conditional BIC is computed. If <code>NULL</code> (the default), the joint BIC is
computed.</p>
</td></tr>
<tr><td><code id="BIC_+3A_regul">regul</code></td>
<td>
<p>A positive numeric value corresponding to the regularization
constant if a penalty term is added for Bayesian regularization. If
<code>NULL</code>, no penalty term is added. If a conditional BIC is computed, this
argument is ignored.</p>
</td></tr>
<tr><td><code id="BIC_+3A_...">...</code></td>
<td>
<p>Unused arguments from the generic function.</p>
</td></tr>
<tr><td><code id="BIC_+3A_col_seq">col_seq</code></td>
<td>
<p>A character vector containing the column names of <code>data</code>
that describe the observation sequence. If <code>NULL</code> (the default), all the
observations belong to a single sequence. If <code>object</code> is a temporal
<code>gmbn</code> or <code>gmdbn</code> object, the observations of a same sequence must
be ordered such that the <code class="reqn">t</code>th one is related to time slice <code class="reqn">t</code>
(note that the sequences can have different lengths). If <code>object</code> is a
non-temporal <code>gmbn</code> object, this argument is ignored.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>If <code>object</code> is a <code>gmm</code> object, a numeric value
corresponding to the BIC.
</p>
<p>If <code>object</code> is a <code>gmbn</code> or <code>gmdbn</code> object, a list with
elements:
</p>
<table role = "presentation">
<tr><td><code>global</code></td>
<td>
<p>A numeric value corresponding to the global BIC.</p>
</td></tr>
<tr><td><code>local</code></td>
<td>
<p>For a <code>gmbn</code> object, a numeric vector containing the local
conditional BICs. For a <code>gmdbn</code> object, a list of numeric vectors
containing these values for each <code>gmbn</code> element.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+AIC">AIC</a></code>, <code><a href="#topic+logLik">logLik</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(gmm_body, data_body)
bic_1 &lt;- BIC(gmm_body, data_body)
bic_2 &lt;- BIC(gmm_body, data_body, y = "WAIST")

data(gmbn_body, data_body)
bic_3 &lt;- BIC(gmbn_body, data_body)

data(gmdbn_air, data_air)
bic_4 &lt;- BIC(gmdbn_air, data_air, col_seq = "DATE")

</code></pre>

<hr>
<h2 id='conditional'>Conditionalize a Gaussian mixture model</h2><span id='topic+conditional'></span>

<h3>Description</h3>

<p>This function conditionalizes a Gaussian mixture model (Sun <em>et al.</em>,
2006).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>conditional(gmm, y = rownames(gmm$mu)[1])
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="conditional_+3A_gmm">gmm</code></td>
<td>
<p>An object of class <code>gmm</code>.</p>
</td></tr>
<tr><td><code id="conditional_+3A_y">y</code></td>
<td>
<p>A character vector containing the dependent variables (by default
the first variable of <code>gmm</code>).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with elements:
</p>
<table role = "presentation">
<tr><td><code>alpha</code></td>
<td>
<p>A numeric vector containing the mixture proportions.</p>
</td></tr>
<tr><td><code>mu_x</code></td>
<td>
<p>A numeric matrix containing the marginal mean vectors of the
explanatory variables bound by column.</p>
</td></tr>
<tr><td><code>sigma_x</code></td>
<td>
<p>A list containing the marginal covariance matrices of the
explanatory variables.</p>
</td></tr>
<tr><td><code>coeff</code></td>
<td>
<p>A list containing the regression coefficient matrices of
the dependent variables on the explanatory variables.</p>
</td></tr>
<tr><td><code>sigma_c</code></td>
<td>
<p>A list containing the conditional covariance matrices.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Sun, S., Zhang, C. and Yu, G. (2006). A Bayesian Network Approach
to Traffic Flow Forecasting. <em>IEEE Transactions on Intelligent
Transportation Systems</em>, 7(1):124&ndash;132.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(gmm_body)
cond &lt;- conditional(gmm_body)

</code></pre>

<hr>
<h2 id='data_air'>Beijing air quality dataset</h2><span id='topic+data_air'></span>

<h3>Description</h3>

<p>This dataset includes hourly air pollutants and weather data measured at the
Dongsi air quality monitoring site in Beijing (China) for 320 complete days
of the year 2015. These data are taken from the Beijing Multi-Site Air
Quality Dataset published in the UCI Machine Learning Repository:
<a href="https://archive.ics.uci.edu/ml/datasets/Beijing+Multi-Site+Air-Quality+Data">https://archive.ics.uci.edu/ml/datasets/Beijing+Multi-Site+Air-Quality+Data</a>
(Zhang <em>et al.</em>, 2017).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data_air
</code></pre>


<h3>Format</h3>

<p>A data frame (tibble) with 7680 rows and 6 columns:
</p>

<ul>
<li> <p><code>DATE</code>: day's date;
</p>
</li>
<li> <p><code>HOUR</code>: hour of the day;
</p>
</li>
<li> <p><code>NO2</code>: nitrogen dioxide concentration
(&mu;g/m&sup3;);
</p>
</li>
<li> <p><code>O3</code>: ozone concentration
(&mu;g/m&sup3;);
</p>
</li>
<li> <p><code>TEMP</code>: temperature (&deg;C);
</p>
</li>
<li> <p><code>WIND</code>: wind speed (m/s).
</p>
</li></ul>



<h3>References</h3>

<p>Zhang, S., Guo, B., Dong, A., He, J., Xu, Z. and Chen, S. X. (2017).
Cautionary Tales on Air-Quality Improvement in Beijing. <em>Proceedings of
the Royal Society A</em>, 473.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+data_body">data_body</a></code>, <code><a href="#topic+gmbn_body">gmbn_body</a></code>,
<code><a href="#topic+gmdbn_air">gmdbn_air</a></code>, <code><a href="#topic+gmm_body">gmm_body</a></code>
</p>

<hr>
<h2 id='data_body'>NHANES body composition dataset</h2><span id='topic+data_body'></span>

<h3>Description</h3>

<p>This dataset includes body composition data measured in 2148 adults aged 20
to 59 years in the United States. These data are taken from the National
Health and Nutrition Examination Survey (NHANES) 2017-2018:
<a href="https://wwwn.cdc.gov/nchs/nhanes/continuousnhanes/default.aspx?BeginYear=2017">https://wwwn.cdc.gov/nchs/nhanes/continuousnhanes/default.aspx?BeginYear=2017</a> (Centers for Disease Control and
Prevention, 2020).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data_body
</code></pre>


<h3>Format</h3>

<p>A data frame (tibble) with 2148 rows and 8 columns:
</p>

<ul>
<li> <p><code>ID</code>: respondent identifier;
</p>
</li>
<li> <p><code>GENDER</code>: gender (0: male, 1: female);
</p>
</li>
<li> <p><code>AGE</code>: age (years);
</p>
</li>
<li> <p><code>HEIGHT</code>: height (cm);
</p>
</li>
<li> <p><code>WEIGHT</code>: weight (kg);
</p>
</li>
<li> <p><code>FAT</code>: body fat (%);
</p>
</li>
<li> <p><code>WAIST</code>: waist circumference (cm);
</p>
</li>
<li> <p><code>GLYCO</code>: glycohemoglobin (%).
</p>
</li></ul>



<h3>References</h3>

<p>Centers for Disease Control and Prevention (2020). National Health and
Nutrition Examination Survey Data.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+data_air">data_air</a></code>, <code><a href="#topic+gmbn_body">gmbn_body</a></code>,
<code><a href="#topic+gmdbn_air">gmdbn_air</a></code>, <code><a href="#topic+gmm_body">gmm_body</a></code>
</p>

<hr>
<h2 id='density'>Compute densities of a Gaussian mixture model</h2><span id='topic+density'></span>

<h3>Description</h3>

<p>This function computes densities of a Gaussian mixture model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>density(gmm, data, y = NULL, log = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="density_+3A_gmm">gmm</code></td>
<td>
<p>An object of class <code>gmm</code>.</p>
</td></tr>
<tr><td><code id="density_+3A_data">data</code></td>
<td>
<p>A data frame or numeric matrix containing the observations whose
densities are computed. Its columns must explicitly be named after the
variables of <code>gmm</code>.</p>
</td></tr>
<tr><td><code id="density_+3A_y">y</code></td>
<td>
<p>A character vector containing the dependent variables if conditional
densities are computed. If <code>NULL</code> (the default), joint densities are
computed.</p>
</td></tr>
<tr><td><code id="density_+3A_log">log</code></td>
<td>
<p>A logical value indicating whether the densities are returned as
log-densities.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numeric vector containing the (log-)densities.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+expectation">expectation</a></code>, <code><a href="#topic+sampling">sampling</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(gmm_body, data_body)
dens_1 &lt;- density(gmm_body, data_body, log = TRUE)
dens_2 &lt;- density(gmm_body, data_body, y = "WAIST", log = TRUE)

</code></pre>

<hr>
<h2 id='ellipses'>Display the mixture components of a Gaussian mixture model</h2><span id='topic+ellipses'></span>

<h3>Description</h3>

<p>This function displays the mixture components of a Gaussian mixture model.
For each pair of variables, the covariance matrices are represented by
confidence ellipses.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ellipses(
  gmm,
  data = NULL,
  y = rownames(gmm$mu),
  x = rownames(gmm$mu),
  level = 0.95
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ellipses_+3A_gmm">gmm</code></td>
<td>
<p>An object of class <code>gmm</code>.</p>
</td></tr>
<tr><td><code id="ellipses_+3A_data">data</code></td>
<td>
<p>A data frame or numeric matrix containing the data displayed with
the mixture components. Its columns must explicitly be named after the
variables of <code>gmm</code>. If <code>NULL</code> (the default), no data is displayed.</p>
</td></tr>
<tr><td><code id="ellipses_+3A_y">y</code></td>
<td>
<p>A character vector containing the variables displayed on the y-axis
(by default all the variables of <code>gmm</code>).</p>
</td></tr>
<tr><td><code id="ellipses_+3A_x">x</code></td>
<td>
<p>A character vector containing the variables displayed on the x-axis
(by default all the variables of <code>gmm</code>).</p>
</td></tr>
<tr><td><code id="ellipses_+3A_level">level</code></td>
<td>
<p>A numeric value in [0, 1[ corresponding to the confidence level
of the ellipses.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>ggplot</code> object displaying the mixture components (and the
data if required).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(0)
data(gmm_body)
ellipses(gmm_body, sampling(gmm_body, n = 500))

</code></pre>

<hr>
<h2 id='em'>Estimate the parameters of a Gaussian mixture model</h2><span id='topic+em'></span>

<h3>Description</h3>

<p>This function estimates the parameters of a Gaussian mixture model using the
expectation-maximization (EM) algorithm. Given an initial model, this
algorithm iteratively updates the parameters, monotonically increasing the
log-likelihood until convergence to a local maximum (Bilmes, 1998). A
Bayesian regularization is applied by default to prevent that a mixture
component comes down to a single point and leads to a zero covariance matrix
(Ormoneit and Tresp, 1996). Although the EM algorithm only applies to the
joint model, good parameters can be found for a derived conditional model.
However, care should be taken as the monotonic increase of the conditional
log-likelihood is not guaranteed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>em(
  gmm,
  data,
  regul = 0.01,
  epsilon = 1e-06,
  max_iter_em = 100,
  verbose = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="em_+3A_gmm">gmm</code></td>
<td>
<p>An initial object of class <code>gmm</code>.</p>
</td></tr>
<tr><td><code id="em_+3A_data">data</code></td>
<td>
<p>A data frame or numeric matrix containing the data used in the
EM algorithm. Its columns must explicitly be named after the variables of
<code>gmm</code> and must not contain missing values.</p>
</td></tr>
<tr><td><code id="em_+3A_regul">regul</code></td>
<td>
<p>A positive numeric value corresponding to the regularization
constant if a Bayesian regularization is applied. If <code>NULL</code>, no
regularization is applied.</p>
</td></tr>
<tr><td><code id="em_+3A_epsilon">epsilon</code></td>
<td>
<p>A positive numeric value corresponding to the convergence
threshold for the increase in log-likelihood.</p>
</td></tr>
<tr><td><code id="em_+3A_max_iter_em">max_iter_em</code></td>
<td>
<p>A non-negative integer corresponding to the maximum number
of iterations.</p>
</td></tr>
<tr><td><code id="em_+3A_verbose">verbose</code></td>
<td>
<p>A logical value indicating whether iterations in progress
are displayed.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with elements:
</p>
<table role = "presentation">
<tr><td><code>gmm</code></td>
<td>
<p>The final <code>gmm</code> object.</p>
</td></tr>
<tr><td><code>posterior</code></td>
<td>
<p>A numeric matrix containing the posterior probabilities for
each observation.</p>
</td></tr>
<tr><td><code>seq_loglik</code></td>
<td>
<p>A numeric vector containing the sequence of log-likelihoods
measured initially and after each iteration.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Bilmes, J. A. (1998). A Gentle Tutorial of the EM Algorithm and its
Application to Parameter Estimation for Gaussian Mixture and Hidden Markov
Models. Technical report, International Computer Science Institute.
</p>
<p>Ormoneit, D. and Tresp, V. (1996). Improved Gaussian Mixture Density
Estimates Using Bayesian Penalty Terms and Network Averaging. In
<em>Advances in Neural Information Processing Systems 8</em>, pages 542&ndash;548.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+smem">smem</a></code>, <code><a href="#topic+stepwise">stepwise</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(data_body)
gmm_1 &lt;- split_comp(add_var(NULL,
                            data_body[, c("WAIST", "AGE", "FAT", "HEIGHT",
                                          "WEIGHT")]),
                    n_sub = 3)
res_em &lt;- em(gmm_1, data_body, verbose = TRUE)

</code></pre>

<hr>
<h2 id='expectation'>Compute expectations of a Gaussian mixture model</h2><span id='topic+expectation'></span>

<h3>Description</h3>

<p>This function computes expectations of a Gaussian mixture model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>expectation(gmm, data_x = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="expectation_+3A_gmm">gmm</code></td>
<td>
<p>An object of class <code>gmm</code>.</p>
</td></tr>
<tr><td><code id="expectation_+3A_data_x">data_x</code></td>
<td>
<p>A data frame or numeric matrix containing observations of the
explanatory variables if conditional expectations are computed. Its columns
must explicitly be named after the explanatory variables. If <code>NULL</code> (the
default), the joint expectation is computed in a one-row matrix.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numeric matrix containing the expectations.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+density">density</a></code>, <code><a href="#topic+sampling">sampling</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(gmm_body, data_body)
expect_1 &lt;- expectation(gmm_body)
expect_2 &lt;- expectation(gmm_body,
                        data_body[, c("WEIGHT", "FAT", "HEIGHT", "AGE")])

</code></pre>

<hr>
<h2 id='filtering'>Perform filtering inference in a Gaussian mixture dynamic Bayesian network</h2><span id='topic+filtering'></span>

<h3>Description</h3>

<p>This function performs filtering inference in a Gaussian mixture dynamic
Bayesian network. For a sequence of <code class="reqn">T</code> time slices, this task consists
in estimating the state of the system at each time slice <code class="reqn">t</code> (for
<code class="reqn">1 \le t \le T</code>) given all the data (the evidence) collected up to
<code class="reqn">t</code>. This function is also designed to perform fixed-lag smoothing
inference, which consists in defining a time lag <code class="reqn">l</code> such that at each
time slice <code class="reqn">t</code> (for <code class="reqn">l + 1 \le t \le T</code>), the state at <code class="reqn">t - l</code> is
estimated given the evidence collected up to <code class="reqn">t</code> (Murphy, 2002).
Filtering and fixed-lag smoothing inference are performed by sequential
importance resampling, which is a particle-based approximate method (Koller
and Friedman, 2009).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>filtering(
  gmdbn,
  evid,
  nodes = names(gmdbn$b_1),
  col_seq = NULL,
  lag = 0,
  n_part = 1000,
  max_part_sim = 1e+06,
  min_ess = 1,
  verbose = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="filtering_+3A_gmdbn">gmdbn</code></td>
<td>
<p>An object of class <code>gmdbn</code>.</p>
</td></tr>
<tr><td><code id="filtering_+3A_evid">evid</code></td>
<td>
<p>A data frame containing the evidence. Its columns must explicitly
be named after nodes of <code>gmdbn</code> and can contain missing values (columns
with no value can be removed).</p>
</td></tr>
<tr><td><code id="filtering_+3A_nodes">nodes</code></td>
<td>
<p>A character vector containing the inferred nodes (by default all
the nodes of <code>gmdbn</code>).</p>
</td></tr>
<tr><td><code id="filtering_+3A_col_seq">col_seq</code></td>
<td>
<p>A character vector containing the column names of <code>evid</code>
that describe the observation sequence. If <code>NULL</code> (the default), all the
observations belong to a single sequence. The observations of a same sequence
must be ordered such that the <code class="reqn">t</code>th one is related to time slice <code class="reqn">t</code>
(note that the sequences can have different lengths).</p>
</td></tr>
<tr><td><code id="filtering_+3A_lag">lag</code></td>
<td>
<p>A non-negative integer vector containing the time lags for which
fixed-lag smoothing inference is performed. If <code>0</code> (the default),
filtering inference is performed.</p>
</td></tr>
<tr><td><code id="filtering_+3A_n_part">n_part</code></td>
<td>
<p>A positive integer corresponding to the number of particles
generated for each observation sequence.</p>
</td></tr>
<tr><td><code id="filtering_+3A_max_part_sim">max_part_sim</code></td>
<td>
<p>An integer greater than or equal to <code>n_part</code>
corresponding to the maximum number of particles that can be processed
simultaneously. This argument is used to prevent memory overflow, dividing
<code>evid</code> into smaller subsets that are handled sequentially.</p>
</td></tr>
<tr><td><code id="filtering_+3A_min_ess">min_ess</code></td>
<td>
<p>A numeric value in [0, 1] corresponding to the minimum ESS
(expressed as a proportion of <code>n_part</code>) under which the renewal step of
sequential importance resampling is performed. If <code>1</code> (the default),
this step is performed at each time slice.</p>
</td></tr>
<tr><td><code id="filtering_+3A_verbose">verbose</code></td>
<td>
<p>A logical value indicating whether subsets of <code>evid</code> and
time slices in progress are displayed.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>If <code>lag</code> has one element, a data frame (tibble) with a structure
similar to <code>evid</code> containing the estimated values of the inferred
nodes and their observation sequences (if <code>col_seq</code> is not <code>NULL</code>).
If <code>lag</code> has two or more elements, a list of data frames (tibbles)
containing these values for each time lag.
</p>


<h3>References</h3>

<p>Koller, D. and Friedman, N. (2009). <em>Probabilistic Graphical Models:
Principles and Techniques</em>. The MIT Press.
</p>
<p>Murphy, K. (2002). <em>Dynamic Bayesian Networks: Representation, Inference
and Learning</em>. PhD thesis, University of California.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+inference">inference</a></code>, <code><a href="#topic+prediction">prediction</a></code>,
<code><a href="#topic+smoothing">smoothing</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
set.seed(0)
data(gmdbn_air, data_air)
evid &lt;- data_air
evid$NO2[sample.int(7680, 1536)] &lt;- NA
evid$O3[sample.int(7680, 1536)] &lt;- NA
evid$TEMP[sample.int(7680, 1536)] &lt;- NA
evid$WIND[sample.int(7680, 1536)] &lt;- NA
filt &lt;- filtering(gmdbn_air, evid, col_seq = "DATE", lag = c(0, 1),
                  verbose = TRUE)

</code></pre>

<hr>
<h2 id='gmbn'>Create a Gaussian mixture Bayesian network</h2><span id='topic+gmbn'></span>

<h3>Description</h3>

<p>This function creates a Gaussian mixture Bayesian network as an object of S3
class <code>gmbn</code>. A Bayesian network is a probabilistic graphical model that
represents the conditional dependencies and independencies between random
variables by a directed acyclic graph. It encodes a global joint distribution
over the nodes, which decomposes into a product of local conditional
distributions:
</p>
<p style="text-align: center;"><code class="reqn">p(X_1, \dots , X_n) = \prod_{i = 1}^n p(X_i | Pa(X_i))</code>
</p>

<p>where <code class="reqn">Pa(X_i)</code> is the set of parents of <code class="reqn">X_i</code> in the graph. In a
Gaussian mixture Bayesian network, each local joint distribution over a node
and its parents is described by a Gaussian mixture model, which means that
the global distribution is a product of local conditional Gaussian mixture
models (Davies and Moore, 2000). The <code>gmbn</code> class can be extended to the
time factor by regarding the nodes as the state of the system at a given time
slice <code class="reqn">t</code> (denoted by <code class="reqn">X^{(t)}</code>) and allowing them to have parents at
previous time slices. This makes it possible to create a (<code class="reqn">k + 1</code>)-slice
temporal Bayesian network that encodes the transition distribution
<code class="reqn">p(X^{(t)} | X^{(t - 1)}, \dots , X^{(t - k)})</code> (Hulst, 2006). Finally,
note that a Gaussian mixture Bayesian network can be created with functions
<code><a href="#topic+add_nodes">add_nodes</a></code> (by passing <code>NULL</code> as argument <code>gmgm</code>) and
<code><a href="#topic+add_arcs">add_arcs</a></code>, which allows to quickly initialize a <code>gmbn</code>
object that can be passed to a learning function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gmbn(...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="gmbn_+3A_...">...</code></td>
<td>
<p>Objects of class <code>gmm</code> describing the local joint
distributions over the nodes and their parents. Each <code>gmm</code> object must
be named after the node whose distribution it describes and contain variables
named after this node and its parents. Two types of parents are accepted:
other nodes (whose <code>gmm</code> objects must be defined) and instantiations of
nodes at previous time slices (if the created <code>gmbn</code> object is a
temporal Bayesian network). In the second case, the time lag must be added at
the end of the variable name after a period <code>.</code> (e.g. the instantiation
of a node <code>X</code> at time slice <code class="reqn">t - 1</code> is represented by the variable
<code>X.1</code>).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of class <code>gmbn</code> containing the <code>gmm</code> objects passed
as arguments.
</p>


<h3>References</h3>

<p>Davies, S. and Moore, A. (2000). Mix-nets: Factored Mixtures of Gaussians in
Bayesian Networks with Mixed Continuous And Discrete Variables. <em>In
Proceedings of the 16th Conference on Uncertainty in Artificial
Intelligence</em>, pages 168&ndash;175, Stanford, CA, USA.
</p>
<p>Hulst, J. (2006). <em>Modeling physiological processes with dynamic
Bayesian networks</em>. Master's thesis, Delft University of Technology.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gmdbn">gmdbn</a></code>, <code><a href="#topic+gmm">gmm</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(data_body)
gmbn_1 &lt;- gmbn(
  AGE = split_comp(add_var(NULL, data_body[, "AGE"]), n_sub = 3),
  FAT = split_comp(add_var(NULL,
                           data_body[, c("FAT", "GENDER", "HEIGHT", "WEIGHT")]),
                   n_sub = 2),
  GENDER = split_comp(add_var(NULL, data_body[, "GENDER"]), n_sub = 2),
  GLYCO = split_comp(add_var(NULL, data_body[, c("GLYCO", "AGE", "WAIST")]),
                     n_sub = 2),
  HEIGHT = split_comp(add_var(NULL, data_body[, c("HEIGHT", "GENDER")])),
  WAIST = split_comp(add_var(NULL,
                             data_body[, c("WAIST", "AGE", "FAT", "HEIGHT",
                                           "WEIGHT")]),
                     n_sub = 3),
  WEIGHT = split_comp(add_var(NULL, data_body[, c("WEIGHT", "HEIGHT")]),
                      n_sub = 2)
)

library(dplyr)
data(data_air)
data &lt;- data_air %&gt;%
  group_by(DATE) %&gt;%
  mutate(NO2.1 = lag(NO2), O3.1 = lag(O3), TEMP.1 = lag(TEMP),
         WIND.1 = lag(WIND)) %&gt;%
  ungroup()
gmbn_2 &lt;- gmbn(
  NO2 = split_comp(add_var(NULL, data[, c("NO2", "NO2.1", "WIND")]), n_sub = 3),
  O3 = split_comp(add_var(NULL,
                          data[, c("O3", "NO2", "NO2.1", "O3.1", "TEMP",
                                   "TEMP.1")]),
                  n_sub = 3),
  TEMP = split_comp(add_var(NULL, data[, c("TEMP", "TEMP.1")]), n_sub = 3),
  WIND = split_comp(add_var(NULL, data[, c("WIND", "WIND.1")]), n_sub = 3)
)

</code></pre>

<hr>
<h2 id='gmbn_body'>Gaussian mixture Bayesian network learned from the NHANES body composition
dataset</h2><span id='topic+gmbn_body'></span>

<h3>Description</h3>

<p>This Gaussian mixture dynamic Bayesian network is learned from the NHANES
body composition dataset, following the example provided in the documentation
page of function <code><a href="#topic+struct_learn">struct_learn</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gmbn_body
</code></pre>


<h3>Format</h3>

<p>A <code>gmbn</code> object.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+data_air">data_air</a></code>, <code><a href="#topic+data_body">data_body</a></code>,
<code><a href="#topic+gmdbn_air">gmdbn_air</a></code>, <code><a href="#topic+gmm_body">gmm_body</a></code>
</p>

<hr>
<h2 id='gmdbn'>Create a Gaussian mixture dynamic Bayesian network</h2><span id='topic+gmdbn'></span>

<h3>Description</h3>

<p>This function creates a Gaussian mixture dynamic Bayesian network as an
object of S3 class <code>gmdbn</code>. Assuming that the system evolves over time
(possibly non-stationary) and denoting by <code class="reqn">X^{(t)}</code> its state at time
slice <code class="reqn">t</code>, a dynamic Bayesian network is a probabilistic graphical model
that encodes the joint distribution over any finite time sequence:
</p>
<p style="text-align: center;"><code class="reqn">p(X^{(1)}, \dots , X^{(T)}) = p(X^{(1)})
\prod_{t = 2}^T p(X^{(t)} | X^{(t - 1)}, \dots , X^{(1)})</code>
</p>

<p>It is defined by a sequence of transition models
<code class="reqn">\mathcal{B}_1, \mathcal{B}_2, \dots , \mathcal{B}_N</code> associated with
transition time slices <code class="reqn">t_1 = 1 &lt; t_2 &lt; \dots &lt; t_N</code>, where:
</p>

<ul>
<li> <p><code style="white-space: pre;">&#8288;&#8288;</code><code class="reqn">\mathcal{B}_1</code> is a Bayesian network that encodes the
distribution <code class="reqn">p(X^{(t)})</code> for <code class="reqn">1 \le t \le t_2 - 1</code>, assuming that
the states at these time slices do not depend on previous states;
</p>
</li>
<li><p> for each <code class="reqn">i \ge 2</code>, <code class="reqn">\mathcal{B}_i</code> is a (<code class="reqn">k_i + 1</code>)-slice
temporal Bayesian network (where <code class="reqn">k_i &lt; t_i</code>) that encodes the transition
distribution <code class="reqn">p(X^{(t)} | X^{(t - 1)}, \dots , X^{(t - k_i)})</code> for
<code class="reqn">t_i \le t \le t_{i + 1} - 1</code> (or <code class="reqn">t \ge t_i</code> if <code class="reqn">i = N</code>),
assuming that the states at these time slices only depend on the <code class="reqn">k_i</code>
previous states (Hourbracq <em>et al.</em>, 2017).
</p>
</li></ul>

<p>In a Gaussian mixture dynamic Bayesian network, these transition models are
Gaussian mixture Bayesian networks (Roos <em>et al.</em>, 2017).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gmdbn(...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="gmdbn_+3A_...">...</code></td>
<td>
<p>Objects of class <code>gmbn</code> corresponding to the transition
models. Each <code>gmbn</code> object must be named with the prefix <code>b_</code>
followed by its associated transition time slice (e.g. a transition model
whose transition time slice is 8 is represented by the <code>gmbn</code> object
<code>b_8</code>). If the first <code>gmbn</code> object (chronologically) is associated
with a transition time slice <code class="reqn">t \ge 2</code> (i.e. <code>b_1</code> is not
specified), it is duplicated to create transition models associated with
<code class="reqn">1, \dots , t - 1</code> (removing the arcs whose time lags exceed the maximum
temporal depths of these models).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of class <code>gmdbn</code> containing the <code>gmbn</code> objects
passed as arguments.
</p>


<h3>References</h3>

<p>Hourbracq, M., Wuillemin, P.-H., Gonzales, C. and Baumard, P. (2017).
Learning and Selection of Dynamic Bayesian Networks for Non-Stationary
Processes in Real Time. <em>In Proceedings of the 30th International
Flairs Conference</em>, pages 742&ndash;747, Marco Island, FL, USA.
</p>
<p>Roos, J., Bonnevay, S. and Gavin, G. (2017). Dynamic Bayesian Networks with
Gaussian Mixture Models for Short-Term Passenger Flow Forecasting. <em>In
Proceedings of the 12th International Conference on Intelligent Systems and
Knowledge Engineering</em>, Nanjing, China.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gmbn">gmbn</a></code>, <code><a href="#topic+gmm">gmm</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(dplyr)
data(data_air)
data &lt;- data_air %&gt;%
  group_by(DATE) %&gt;%
  mutate(NO2.1 = lag(NO2), O3.1 = lag(O3), TEMP.1 = lag(TEMP),
         WIND.1 = lag(WIND)) %&gt;%
  ungroup()
gmdbn_1 &lt;- gmdbn(
  b_2 = gmbn(
    NO2 = split_comp(add_var(NULL, data[, c("NO2", "NO2.1", "WIND")]),
                     n_sub = 3),
    O3 = split_comp(add_var(NULL,
                            data[, c("O3", "NO2", "NO2.1", "O3.1", "TEMP",
                                     "TEMP.1")]),
                    n_sub = 3),
    TEMP = split_comp(add_var(NULL, data[, c("TEMP", "TEMP.1")]), n_sub = 3),
    WIND = split_comp(add_var(NULL, data[, c("WIND", "WIND.1")]), n_sub = 3)
  ),
  b_13 = gmbn(
    NO2 = split_comp(add_var(NULL, data[, c("NO2", "NO2.1", "WIND")]),
                     n_sub = 3),
    O3 = split_comp(add_var(NULL,
                            data[, c("O3", "O3.1", "TEMP", "TEMP.1", "WIND")]),
                    n_sub = 3),
    TEMP = split_comp(add_var(NULL, data[, c("TEMP", "TEMP.1")]), n_sub = 3),
    WIND = split_comp(add_var(NULL, data[, c("WIND", "WIND.1")]), n_sub = 3)
  )
)

</code></pre>

<hr>
<h2 id='gmdbn_air'>Gaussian mixture dynamic Bayesian network learned from the Beijing air
quality dataset</h2><span id='topic+gmdbn_air'></span>

<h3>Description</h3>

<p>This Gaussian mixture dynamic Bayesian network is learned from the Beijing
air quality dataset, following the example provided in the documentation page
of function <code><a href="#topic+struct_learn">struct_learn</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gmdbn_air
</code></pre>


<h3>Format</h3>

<p>A <code>gmdbn</code> object.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+data_air">data_air</a></code>, <code><a href="#topic+data_body">data_body</a></code>,
<code><a href="#topic+gmbn_body">gmbn_body</a></code>, <code><a href="#topic+gmm_body">gmm_body</a></code>
</p>

<hr>
<h2 id='gmm'>Create a Gaussian mixture model</h2><span id='topic+gmm'></span>

<h3>Description</h3>

<p>This function creates a Gaussian mixture model as an object of S3 class
<code>gmm</code>. A Gaussian mixture model is a weighted sum of multivariate
Gaussian distributions:
</p>
<p style="text-align: center;"><code class="reqn">p(x) = \sum_{i = 1}^M \alpha_i \mathcal{N}(x | \mu_i, \Sigma_i)</code>
</p>

<p>where <code class="reqn">\alpha_i</code> is the <code class="reqn">i</code>th mixture proportion such that
<code class="reqn">\alpha_i &gt; 0</code> and <code class="reqn">\sum_{i = 1}^M \alpha_i = 1</code>, <code class="reqn">\mu_i</code> the
mean vector and <code class="reqn">\Sigma_i</code> the covariance matrix of the <code class="reqn">i</code>th mixture
component (Bilmes, 1998). Since conditional distributions can be derived from
joint distributions, the <code>gmm</code> class is also used to work with
conditional Gaussian mixture models (see function <code><a href="#topic+conditional">conditional</a></code>
to explicit their parameters). Finally, note that a one-component Gaussian
mixture model can be created with function <code><a href="#topic+add_var">add_var</a></code> (by passing
<code>NULL</code> as argument <code>gmm</code>), which allows to quickly initialize a
<code>gmm</code> object that can be passed to a learning function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gmm(alpha, mu, sigma, var = rownames(mu))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="gmm_+3A_alpha">alpha</code></td>
<td>
<p>A positive numeric vector containing the mixture proportions. If
the sum of these proportions is not 1, a normalization is performed by
dividing them by this sum.</p>
</td></tr>
<tr><td><code id="gmm_+3A_mu">mu</code></td>
<td>
<p>A numeric matrix containing the mean vectors bound by column.</p>
</td></tr>
<tr><td><code id="gmm_+3A_sigma">sigma</code></td>
<td>
<p>A list containing the covariance matrices.</p>
</td></tr>
<tr><td><code id="gmm_+3A_var">var</code></td>
<td>
<p>A character vector containing the variable names (by default the
row names of <code>mu</code>).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of class <code>gmm</code> containing the elements <code>alpha</code>,
<code>mu</code> and <code>sigma</code> passed as arguments (completed with the variable
names passed as argument <code>var</code>).
</p>


<h3>References</h3>

<p>Bilmes, J. A. (1998). A Gentle Tutorial of the EM Algorithm and its
Application to Parameter Estimation for Gaussian Mixture and Hidden Markov
Models. Technical report, International Computer Science Institute.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gmbn">gmbn</a></code>, <code><a href="#topic+gmdbn">gmdbn</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>gmm_1 &lt;- gmm(alpha = c(0.2, 0.5, 0.3),
             mu = matrix(c(109, 91, 44, 160, 41, 99, 87, 27, 173, 40, 86, 65,
                           35, 161, 40),
                         nrow = 5),
             sigma = list(matrix(c(208, 240, 32, 17, -6, 240, 378, 40, 55, -38,
                                   32, 40, 15, -2, 1, 17, 55, -2, 47, -13, -6,
                                   -38, 1, -13, 127),
                                 nrow = 5),
                          matrix(c(242, 270, 82, 10, 49, 270, 363, 83, 44, 19,
                                   82, 83, 38, -2, 15, 10, 44, -2, 45, -7, 49,
                                   19, 15, -7, 137),
                                 nrow = 5),
                          matrix(c(109, 102, 41, 11, 29, 102, 128, 34, 38, 10,
                                   41, 34, 36, -9, 16, 11, 38, -9, 56, -5, 29,
                                   10, 16, -5, 138),
                                 nrow = 5)),
             var = c("WAIST", "WEIGHT", "FAT", "HEIGHT", "AGE"))

</code></pre>

<hr>
<h2 id='gmm_body'>Gaussian mixture model learned from the NHANES body composition dataset</h2><span id='topic+gmm_body'></span>

<h3>Description</h3>

<p>This Gaussian mixture model is learned from the NHANES body composition
dataset, following the example provided in the documentation page of function
<code><a href="#topic+stepwise">stepwise</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gmm_body
</code></pre>


<h3>Format</h3>

<p>A <code>gmm</code> object.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+data_air">data_air</a></code>, <code><a href="#topic+data_body">data_body</a></code>,
<code><a href="#topic+gmbn_body">gmbn_body</a></code>, <code><a href="#topic+gmdbn_air">gmdbn_air</a></code>
</p>

<hr>
<h2 id='inference'>Perform inference in a Gaussian mixture Bayesian network</h2><span id='topic+inference'></span>

<h3>Description</h3>

<p>This function performs inference in a (non-temporal) Gaussian mixture
Bayesian network. This task consists in estimating the state of the system
given partial observations of it (the evidence). Inference is performed by
likelihood weighting, which is a particle-based approximate method (Koller
and Friedman, 2009).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>inference(
  gmbn,
  evid,
  nodes = names(gmbn),
  n_part = 1000,
  max_part_sim = 1e+06,
  verbose = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="inference_+3A_gmbn">gmbn</code></td>
<td>
<p>A (non-temporal) object of class <code>gmbn</code>.</p>
</td></tr>
<tr><td><code id="inference_+3A_evid">evid</code></td>
<td>
<p>A data frame containing the evidence. Its columns must explicitly
be named after nodes of <code>gmbn</code> and can contain missing values (columns
with no value can be removed).</p>
</td></tr>
<tr><td><code id="inference_+3A_nodes">nodes</code></td>
<td>
<p>A character vector containing the inferred nodes (by default all
the nodes of <code>gmbn</code>).</p>
</td></tr>
<tr><td><code id="inference_+3A_n_part">n_part</code></td>
<td>
<p>A positive integer corresponding to the number of particles
generated for each observation.</p>
</td></tr>
<tr><td><code id="inference_+3A_max_part_sim">max_part_sim</code></td>
<td>
<p>An integer greater than or equal to <code>n_part</code>
corresponding to the maximum number of particles that can be processed
simultaneously. This argument is used to prevent memory overflow, dividing
<code>evid</code> into smaller subsets that are handled sequentially.</p>
</td></tr>
<tr><td><code id="inference_+3A_verbose">verbose</code></td>
<td>
<p>A logical value indicating whether subsets of <code>evid</code> in
progress are displayed.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame (tibble) with a structure similar to <code>evid</code>
containing the estimated values of the inferred nodes.
</p>


<h3>References</h3>

<p>Koller, D. and Friedman, N. (2009). <em>Probabilistic Graphical Models:
Principles and Techniques</em>. The MIT Press.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+filtering">filtering</a></code>, <code><a href="#topic+prediction">prediction</a></code>,
<code><a href="#topic+smoothing">smoothing</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
set.seed(0)
data(gmbn_body, data_body)
evid &lt;- data_body
evid$GENDER[sample.int(2148, 430)] &lt;- NA
evid$AGE[sample.int(2148, 430)] &lt;- NA
evid$HEIGHT[sample.int(2148, 430)] &lt;- NA
evid$WEIGHT[sample.int(2148, 430)] &lt;- NA
evid$FAT[sample.int(2148, 430)] &lt;- NA
evid$WAIST[sample.int(2148, 430)] &lt;- NA
evid$GLYCO[sample.int(2148, 430)] &lt;- NA
infer &lt;- inference(gmbn_body, evid, verbose = TRUE)

</code></pre>

<hr>
<h2 id='logLik'>Compute the log-likelihood of a Gaussian mixture model or graphical model</h2><span id='topic+logLik'></span><span id='topic+logLik.gmm'></span><span id='topic+logLik.gmbn'></span><span id='topic+logLik.gmdbn'></span>

<h3>Description</h3>

<p>This function computes the log-likelihood of a Gaussian mixture model or
graphical model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'gmm'
logLik(object, data, y = NULL, regul = 0.01, ...)

## S3 method for class 'gmbn'
logLik(object, data, col_seq = NULL, ...)

## S3 method for class 'gmdbn'
logLik(object, data, col_seq = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="logLik_+3A_object">object</code></td>
<td>
<p>An object of class <code>gmm</code>, <code>gmbn</code> or <code>gmdbn</code>.</p>
</td></tr>
<tr><td><code id="logLik_+3A_data">data</code></td>
<td>
<p>A data frame containing the data used to compute the
log-likelihood. Its columns must explicitly be named after the variables (or
nodes) of <code>object</code>. If <code>object</code> is a <code>gmm</code> object, a numeric
matrix can be passed.</p>
</td></tr>
<tr><td><code id="logLik_+3A_y">y</code></td>
<td>
<p>A character vector containing the dependent variables if a
conditional log-likelihood is computed. If <code>NULL</code> (the default), the
joint log-likelihood is computed.</p>
</td></tr>
<tr><td><code id="logLik_+3A_regul">regul</code></td>
<td>
<p>A positive numeric value corresponding to the regularization
constant if a penalty term is added for Bayesian regularization. If
<code>NULL</code>, no penalty term is added. If a conditional
log-likelihood is computed, this argument is ignored.</p>
</td></tr>
<tr><td><code id="logLik_+3A_...">...</code></td>
<td>
<p>Unused arguments from the generic function.</p>
</td></tr>
<tr><td><code id="logLik_+3A_col_seq">col_seq</code></td>
<td>
<p>A character vector containing the column names of <code>data</code>
that describe the observation sequence. If <code>NULL</code> (the default), all the
observations belong to a single sequence. If <code>object</code> is a temporal
<code>gmbn</code> or <code>gmdbn</code> object, the observations of a same sequence must
be ordered such that the <code class="reqn">t</code>th one is related to time slice <code class="reqn">t</code>
(note that the sequences can have different lengths). If <code>object</code> is a
non-temporal <code>gmbn</code> object, this argument is ignored.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>If <code>object</code> is a <code>gmm</code> object, a numeric value
corresponding to the log-likelihood.
</p>
<p>If <code>object</code> is a <code>gmbn</code> or <code>gmdbn</code> object, a list with
elements:
</p>
<table role = "presentation">
<tr><td><code>global</code></td>
<td>
<p>A numeric value corresponding to the global log-likelihood.</p>
</td></tr>
<tr><td><code>local</code></td>
<td>
<p>For a <code>gmbn</code> object, a numeric vector containing the local
conditional log-likelihoods. For a <code>gmdbn</code> object, a list of numeric
vectors containing these values for each <code>gmbn</code> element.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+AIC">AIC</a></code>, <code><a href="#topic+BIC">BIC</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(gmm_body, data_body)
loglik_1 &lt;- logLik(gmm_body, data_body)
loglik_2 &lt;- logLik(gmm_body, data_body, y = "WAIST")

data(gmbn_body, data_body)
loglik_3 &lt;- logLik(gmbn_body, data_body)

data(gmdbn_air, data_air)
loglik_4 &lt;- logLik(gmdbn_air, data_air, col_seq = "DATE")

</code></pre>

<hr>
<h2 id='merge_comp'>Merge mixture components of a Gaussian mixture model</h2><span id='topic+merge_comp'></span>

<h3>Description</h3>

<p>This function merges mixture components of a Gaussian mixture model (Zhang
<em>et al.</em>, 2003).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>merge_comp(gmm, comp = seq_along(gmm$alpha))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="merge_comp_+3A_gmm">gmm</code></td>
<td>
<p>An object of class <code>gmm</code>.</p>
</td></tr>
<tr><td><code id="merge_comp_+3A_comp">comp</code></td>
<td>
<p>An integer vector containing the indexes of the merged mixture
components (by default all the components of <code>gmm</code>).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>gmm</code> object after merging the mixture components.
</p>


<h3>References</h3>

<p>Zhang, Z., Chen, C., Sun, J. and Chan, K. L. (2003). EM algorithms for
Gaussian mixtures with split-and-merge operation. <em>Pattern Recognition</em>,
36(9):1973&ndash;1983.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+split_comp">split_comp</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(gmm_body)
gmm_1 &lt;- merge_comp(gmm_body, c(1, 2))

</code></pre>

<hr>
<h2 id='network'>Display the graphical structure of a Gaussian mixture Bayesian network</h2><span id='topic+network'></span>

<h3>Description</h3>

<p>This function displays the graphical structure of a Gaussian mixture
Bayesian network.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>network(gmbn)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="network_+3A_gmbn">gmbn</code></td>
<td>
<p>An object of class <code>gmbn</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>visNetwork</code> object displaying the graphical structure.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(gmbn_body)
network(gmbn_body)

data(gmdbn_air)
network(gmdbn_air$b_2)

</code></pre>

<hr>
<h2 id='param_em'>Learn the parameters of a Gaussian mixture graphical model with incomplete
data</h2><span id='topic+param_em'></span>

<h3>Description</h3>

<p>This function learns the parameters of a Gaussian mixture graphical model
with incomplete data using the parametric EM algorithm. At each iteration,
inference (smoothing inference for a dynamic Bayesian network) is performed
to complete the data given the current estimate of the parameters (E step).
The completed data are then used to update the parameters (M step), and so
on. Each iteration is guaranteed to increase the log-likelihood until
convergence to a local maximum (Koller and Friedman, 2009). In practice, due
to the sampling process inherent in particle-based inference, it may happen
that the monotonic increase no longer occurs when approaching the local
maximum, resulting in an earlier termination of the algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>param_em(
  gmgm,
  data,
  nodes = structure(gmgm)$nodes,
  col_seq = NULL,
  n_part = 1000,
  max_part_sim = 1e+06,
  min_ess = 1,
  max_iter_pem = 5,
  verbose = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="param_em_+3A_gmgm">gmgm</code></td>
<td>
<p>An object of class <code>gmbn</code> (non-temporal) or <code>gmdbn</code>.</p>
</td></tr>
<tr><td><code id="param_em_+3A_data">data</code></td>
<td>
<p>A data frame containing the data used for learning. Its columns
must explicitly be named after nodes of <code>gmgm</code> and can contain missing
values (columns with no value can be removed).</p>
</td></tr>
<tr><td><code id="param_em_+3A_nodes">nodes</code></td>
<td>
<p>A character vector containing the nodes whose local conditional
models are learned (by default all the nodes of <code>gmgm</code>). If <code>gmgm</code>
is a <code>gmdbn</code> object, the same nodes are learned for each of its
<code>gmbn</code> elements. This constraint can be overcome by passing a list of
character vectors named after some of these elements (<code>b_1</code>, ...) and
containing learned nodes specific to them.</p>
</td></tr>
<tr><td><code id="param_em_+3A_col_seq">col_seq</code></td>
<td>
<p>A character vector containing the column names of <code>data</code>
that describe the observation sequence. If <code>NULL</code> (the default), all the
observations belong to a single sequence. If <code>gmgm</code> is a <code>gmdbn</code>
object, the observations of a same sequence must be ordered such that the
<code class="reqn">t</code>th one is related to time slice <code class="reqn">t</code> (note that the sequences can
have different lengths). If <code>gmgm</code> is a <code>gmbn</code> object, this
argument is ignored.</p>
</td></tr>
<tr><td><code id="param_em_+3A_n_part">n_part</code></td>
<td>
<p>A positive integer corresponding to the number of particles
generated for each observation (if <code>gmgm</code> is a <code>gmbn</code> object) or
observation sequence (if <code>gmgm</code> is a <code>gmdbn</code> object) during
inference.</p>
</td></tr>
<tr><td><code id="param_em_+3A_max_part_sim">max_part_sim</code></td>
<td>
<p>An integer greater than or equal to <code>n_part</code>
corresponding to the maximum number of particles that can be processed
simultaneously during inference. This argument is used to prevent memory
overflow, dividing <code>data</code> into smaller subsets that are handle
sequentially.</p>
</td></tr>
<tr><td><code id="param_em_+3A_min_ess">min_ess</code></td>
<td>
<p>A numeric value in [0, 1] corresponding to the minimum ESS
(expressed as a proportion of <code>n_part</code>) under which the renewal step of
sequential importance resampling is performed. If <code>1</code> (the default),
this step is performed at each time slice. If <code>gmgm</code> is a <code>gmbn</code>
object, this argument is ignored.</p>
</td></tr>
<tr><td><code id="param_em_+3A_max_iter_pem">max_iter_pem</code></td>
<td>
<p>A non-negative integer corresponding to the maximum
number of iterations.</p>
</td></tr>
<tr><td><code id="param_em_+3A_verbose">verbose</code></td>
<td>
<p>A logical value indicating whether iterations in progress
are displayed.</p>
</td></tr>
<tr><td><code id="param_em_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to function <code><a href="#topic+em">em</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with elements:
</p>
<table role = "presentation">
<tr><td><code>gmgm</code></td>
<td>
<p>The final <code>gmbn</code> or <code>gmdbn</code> object (with the highest
log-likelihood).</p>
</td></tr>
<tr><td><code>data</code></td>
<td>
<p>A data frame (tibble) containing the complete data used to learn
the final <code>gmbn</code> or <code>gmdbn</code> object.</p>
</td></tr>
<tr><td><code>seq_loglik</code></td>
<td>
<p>A numeric matrix containing the sequence of log-likelihoods
measured after the E and M steps of each iteration.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Koller, D. and Friedman, N. (2009). <em>Probabilistic Graphical Models:
Principles and Techniques</em>. The MIT Press.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+param_learn">param_learn</a></code>, <code><a href="#topic+struct_em">struct_em</a></code>,
<code><a href="#topic+struct_learn">struct_learn</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
set.seed(0)
data(data_body)
data_1 &lt;- data_body
data_1$GENDER[sample.int(2148, 430)] &lt;- NA
data_1$AGE[sample.int(2148, 430)] &lt;- NA
data_1$HEIGHT[sample.int(2148, 430)] &lt;- NA
data_1$WEIGHT[sample.int(2148, 430)] &lt;- NA
data_1$FAT[sample.int(2148, 430)] &lt;- NA
data_1$WAIST[sample.int(2148, 430)] &lt;- NA
data_1$GLYCO[sample.int(2148, 430)] &lt;- NA
gmbn_1 &lt;- gmbn(
  AGE = split_comp(add_var(NULL, data_1[, "AGE"]), n_sub = 3),
  FAT = split_comp(add_var(NULL,
                           data_1[, c("FAT", "GENDER", "HEIGHT", "WEIGHT")]),
                   n_sub = 2),
  GENDER = split_comp(add_var(NULL, data_1[, "GENDER"]), n_sub = 2),
  GLYCO = split_comp(add_var(NULL, data_1[, c("GLYCO", "AGE", "WAIST")]),
                     n_sub = 2),
  HEIGHT = split_comp(add_var(NULL, data_1[, c("HEIGHT", "GENDER")])),
  WAIST = split_comp(add_var(NULL,
                             data_1[, c("WAIST", "AGE", "FAT", "HEIGHT",
                                        "WEIGHT")]),
                     n_sub = 3),
  WEIGHT = split_comp(add_var(NULL, data_1[, c("WEIGHT", "HEIGHT")]), n_sub = 2)
)
res_learn_1 &lt;- param_em(gmbn_1, data_1, verbose = TRUE)

library(dplyr)
set.seed(0)
data(data_air)
data_2 &lt;- data_air
data_2$NO2[sample.int(7680, 1536)] &lt;- NA
data_2$O3[sample.int(7680, 1536)] &lt;- NA
data_2$TEMP[sample.int(7680, 1536)] &lt;- NA
data_2$WIND[sample.int(7680, 1536)] &lt;- NA
data_3 &lt;- data_2 %&gt;%
  group_by(DATE) %&gt;%
  mutate(NO2.1 = lag(NO2), O3.1 = lag(O3), TEMP.1 = lag(TEMP),
         WIND.1 = lag(WIND)) %&gt;%
  ungroup()
gmdbn_1 &lt;- gmdbn(
  b_2 = gmbn(
    NO2 = split_comp(add_var(NULL, data_3[, c("NO2", "NO2.1", "WIND")]),
                     n_sub = 3),
    O3 = split_comp(add_var(NULL,
                            data_3[, c("O3", "NO2", "NO2.1", "O3.1", "TEMP",
                                       "TEMP.1")]),
                    n_sub = 3),
    TEMP = split_comp(add_var(NULL, data_3[, c("TEMP", "TEMP.1")]), n_sub = 3),
    WIND = split_comp(add_var(NULL, data_3[, c("WIND", "WIND.1")]), n_sub = 3)
  ),
  b_13 = gmbn(
    NO2 = split_comp(add_var(NULL, data_3[, c("NO2", "NO2.1", "WIND")]),
                     n_sub = 3),
    O3 = split_comp(add_var(NULL,
                            data_3[, c("O3", "O3.1", "TEMP", "TEMP.1",
                                       "WIND")]),
                    n_sub = 3),
    TEMP = split_comp(add_var(NULL, data_3[, c("TEMP", "TEMP.1")]), n_sub = 3),
    WIND = split_comp(add_var(NULL, data_3[, c("WIND", "WIND.1")]), n_sub = 3)
  )
)
res_learn_2 &lt;- param_em(gmdbn_1, data_2, col_seq = "DATE", verbose = TRUE)

</code></pre>

<hr>
<h2 id='param_learn'>Learn the parameters of a Gaussian mixture graphical model</h2><span id='topic+param_learn'></span>

<h3>Description</h3>

<p>This function learns the parameters of a Gaussian mixture graphical model.
Using the local decomposability of the log-likelihood, this task consists in
learning each local conditional model independently with the EM algorithm
(Koller and Friedman, 2009).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>param_learn(
  gmgm,
  data,
  nodes = structure(gmgm)$nodes,
  col_seq = NULL,
  verbose = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="param_learn_+3A_gmgm">gmgm</code></td>
<td>
<p>An initial object of class <code>gmbn</code> or <code>gmdbn</code>.</p>
</td></tr>
<tr><td><code id="param_learn_+3A_data">data</code></td>
<td>
<p>A data frame containing the data used for learning. Its columns
must explicitly be named after the nodes of <code>gmgm</code> and must not contain
missing values.</p>
</td></tr>
<tr><td><code id="param_learn_+3A_nodes">nodes</code></td>
<td>
<p>A character vector containing the nodes whose local conditional
models are learned (by default all the nodes of <code>gmgm</code>). If <code>gmgm</code>
is a <code>gmdbn</code> object, the same nodes are learned for each of its
<code>gmbn</code> elements. This constraint can be overcome by passing a list of
character vectors named after some of these elements (<code>b_1</code>, ...) and
containing learned nodes specific to them.</p>
</td></tr>
<tr><td><code id="param_learn_+3A_col_seq">col_seq</code></td>
<td>
<p>A character vector containing the column names of <code>data</code>
that describe the observation sequence. If <code>NULL</code> (the default), all the
observations belong to a single sequence. If <code>gmgm</code> is a temporal
<code>gmbn</code> or <code>gmdbn</code> object, the observations of a same sequence must
be ordered such that the <code class="reqn">t</code>th one is related to time slice <code class="reqn">t</code>
(note that the sequences can have different lengths). If <code>gmgm</code> is a
non-temporal <code>gmbn</code> object, this argument is ignored.</p>
</td></tr>
<tr><td><code id="param_learn_+3A_verbose">verbose</code></td>
<td>
<p>A logical value indicating whether learned nodes in progress
are displayed.</p>
</td></tr>
<tr><td><code id="param_learn_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to function <code><a href="#topic+em">em</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with elements:
</p>
<table role = "presentation">
<tr><td><code>gmgm</code></td>
<td>
<p>The final <code>gmbn</code> or <code>gmdbn</code> object.</p>
</td></tr>
<tr><td><code>evol_loglik</code></td>
<td>
<p>A list with elements:
</p>

<dl>
<dt><code>global</code></dt><dd><p>A numeric vector containing the global log-likelihood
before and after learning.</p>
</dd>
<dt><code>local</code></dt><dd><p>For a <code>gmbn</code> object, a numeric matrix containing the
local conditional log-likelihoods before and after learning. For a
<code>gmdbn</code> object, a list of numeric matrices containing these values for
each <code>gmbn</code> element.</p>
</dd>
</dl>
</td></tr>
</table>


<h3>References</h3>

<p>Koller, D. and Friedman, N. (2009). <em>Probabilistic Graphical Models:
Principles and Techniques</em>. The MIT Press.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+param_em">param_em</a></code>, <code><a href="#topic+struct_em">struct_em</a></code>,
<code><a href="#topic+struct_learn">struct_learn</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(data_body)
gmbn_1 &lt;- gmbn(
  AGE = split_comp(add_var(NULL, data_body[, "AGE"]), n_sub = 3),
  FAT = split_comp(add_var(NULL,
                           data_body[, c("FAT", "GENDER", "HEIGHT", "WEIGHT")]),
                   n_sub = 2),
  GENDER = split_comp(add_var(NULL, data_body[, "GENDER"]), n_sub = 2),
  GLYCO = split_comp(add_var(NULL, data_body[, c("GLYCO", "AGE", "WAIST")]),
                     n_sub = 2),
  HEIGHT = split_comp(add_var(NULL, data_body[, c("HEIGHT", "GENDER")])),
  WAIST = split_comp(add_var(NULL,
                             data_body[, c("WAIST", "AGE", "FAT", "HEIGHT",
                                           "WEIGHT")]),
                     n_sub = 3),
  WEIGHT = split_comp(add_var(NULL, data_body[, c("WEIGHT", "HEIGHT")]),
                      n_sub = 2)
)
res_learn_1 &lt;- param_learn(gmbn_1, data_body, verbose = TRUE)

library(dplyr)
data(data_air)
data &lt;- data_air %&gt;%
  group_by(DATE) %&gt;%
  mutate(NO2.1 = lag(NO2), O3.1 = lag(O3), TEMP.1 = lag(TEMP),
         WIND.1 = lag(WIND)) %&gt;%
  ungroup()
gmdbn_1 &lt;- gmdbn(
  b_2 = gmbn(
    NO2 = split_comp(add_var(NULL, data[, c("NO2", "NO2.1", "WIND")]),
                     n_sub = 3),
    O3 = split_comp(add_var(NULL,
                            data[, c("O3", "NO2", "NO2.1", "O3.1", "TEMP",
                                     "TEMP.1")]),
                    n_sub = 3),
    TEMP = split_comp(add_var(NULL, data[, c("TEMP", "TEMP.1")]), n_sub = 3),
    WIND = split_comp(add_var(NULL, data[, c("WIND", "WIND.1")]), n_sub = 3)
  ),
  b_13 = gmbn(
    NO2 = split_comp(add_var(NULL, data[, c("NO2", "NO2.1", "WIND")]),
                     n_sub = 3),
    O3 = split_comp(add_var(NULL,
                            data[, c("O3", "O3.1", "TEMP", "TEMP.1", "WIND")]),
                    n_sub = 3),
    TEMP = split_comp(add_var(NULL, data[, c("TEMP", "TEMP.1")]), n_sub = 3),
    WIND = split_comp(add_var(NULL, data[, c("WIND", "WIND.1")]), n_sub = 3)
  )
)
res_learn_2 &lt;- param_learn(gmdbn_1, data_air, col_seq = "DATE", verbose = TRUE)

</code></pre>

<hr>
<h2 id='particles'>Initialize particles to perform inference in a Gaussian mixture graphical
model</h2><span id='topic+particles'></span>

<h3>Description</h3>

<p>This function initializes particles to perform (approximate) inference in a
Gaussian mixture graphical model. Particles consist in weighted sample
sequences propagated forward in time by sampling the model and aggregated to
obtain the inferred values (Koller and Friedman, 2009).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>particles(seq = NULL, col_weight = "weight", n_part = 1000)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="particles_+3A_seq">seq</code></td>
<td>
<p>A data frame containing the observation sequences for which
particles are initialized. If <code>NULL</code> (the default), the initialization
is performed for a single sequence.</p>
</td></tr>
<tr><td><code id="particles_+3A_col_weight">col_weight</code></td>
<td>
<p>A character string corresponding to the column name of the
resulting data frame that describes the particle weight.</p>
</td></tr>
<tr><td><code id="particles_+3A_n_part">n_part</code></td>
<td>
<p>A positive integer corresponding to the number of particles
initialized for each observation sequence.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame (tibble) containing the initial particles.
</p>


<h3>References</h3>

<p>Koller, D. and Friedman, N. (2009). <em>Probabilistic Graphical Models:
Principles and Techniques</em>. The MIT Press.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+aggregation">aggregation</a></code>, <code><a href="#topic+propagation">propagation</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(data_air)
part &lt;- particles(data.frame(DATE = unique(data_air$DATE)))

</code></pre>

<hr>
<h2 id='prediction'>Perform predictive inference in a Gaussian mixture dynamic Bayesian network</h2><span id='topic+prediction'></span>

<h3>Description</h3>

<p>This function performs predictive inference in a Gaussian mixture dynamic
Bayesian network. For a sequence of <code class="reqn">T</code> time slices, this task consists
in defining a time horizon <code class="reqn">h</code> such that at each time slice <code class="reqn">t</code>
(for <code class="reqn">0 \le t \le T - h</code>), the state of the system at <code class="reqn">t + h</code> is
estimated given all the data (the evidence) collected up to <code class="reqn">t</code>. Although
the states at <code class="reqn">t + 1, \dots , t + h</code> are observed in the future, some
information about them can be known a priori (such as contextual information
or features controlled by the user). This &quot;predicted&quot; evidence can be taken
into account when propagating the particles from <code class="reqn">t</code> to <code class="reqn">t + h</code> in
order to improve the predictions. Predictive inference is performed by
sequential importance resampling, which is a particle-based approximate
method (Koller and Friedman, 2009).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prediction(
  gmdbn,
  evid,
  evid_pred = NULL,
  nodes = names(gmdbn$b_1),
  col_seq = NULL,
  horizon = 1,
  n_part = 1000,
  max_part_sim = 1e+06,
  min_ess = 1,
  verbose = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="prediction_+3A_gmdbn">gmdbn</code></td>
<td>
<p>An object of class <code>gmdbn</code>.</p>
</td></tr>
<tr><td><code id="prediction_+3A_evid">evid</code></td>
<td>
<p>A data frame containing the evidence. Its columns must explicitly
be named after nodes of <code>gmdbn</code> and can contain missing values (columns
with no value can be removed).</p>
</td></tr>
<tr><td><code id="prediction_+3A_evid_pred">evid_pred</code></td>
<td>
<p>A data frame containing the &quot;predicted&quot; evidence. Its
columns must explicitly be named after nodes of <code>gmdbn</code> and can contain
missing values (columns with no value can be removed).</p>
</td></tr>
<tr><td><code id="prediction_+3A_nodes">nodes</code></td>
<td>
<p>A character vector containing the inferred nodes (by default all
the nodes of <code>gmdbn</code>).</p>
</td></tr>
<tr><td><code id="prediction_+3A_col_seq">col_seq</code></td>
<td>
<p>A character vector containing the column names of <code>evid</code>
and <code>evid_pred</code> that describe the observation sequence. If <code>NULL</code>
(the default), all the observations belong to a single sequence. The
observations of a same sequence must be ordered such that the <code class="reqn">t</code>th one
is related to time slice <code class="reqn">t</code> (note that the sequences can have different
lengths).</p>
</td></tr>
<tr><td><code id="prediction_+3A_horizon">horizon</code></td>
<td>
<p>A positive integer vector containing the time horizons for
which predictive inference is performed.</p>
</td></tr>
<tr><td><code id="prediction_+3A_n_part">n_part</code></td>
<td>
<p>A positive integer corresponding to the number of particles
generated for each observation sequence.</p>
</td></tr>
<tr><td><code id="prediction_+3A_max_part_sim">max_part_sim</code></td>
<td>
<p>An integer greater than or equal to <code>n_part</code>
corresponding to the maximum number of particles that can be processed
simultaneously. This argument is used to prevent memory overflow, dividing
<code>evid</code> into smaller subsets that are handled sequentially.</p>
</td></tr>
<tr><td><code id="prediction_+3A_min_ess">min_ess</code></td>
<td>
<p>A numeric value in [0, 1] corresponding to the minimum ESS
(expressed as a proportion of <code>n_part</code>) under which the renewal step of
sequential importance resampling is performed. If <code>1</code> (the default),
this step is performed at each time slice.</p>
</td></tr>
<tr><td><code id="prediction_+3A_verbose">verbose</code></td>
<td>
<p>A logical value indicating whether subsets of <code>evid</code> and
time slices in progress are displayed.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>If <code>horizon</code> has one element, a data frame with a structure
similar to <code>evid</code> containing the predicted values of the inferred
nodes and their observation sequences (if <code>col_seq</code> is not <code>NULL</code>).
If <code>horizon</code> has two or more elements, a list of data frames (tibbles)
containing these values for each time horizon.
</p>


<h3>References</h3>

<p>Koller, D. and Friedman, N. (2009). <em>Probabilistic Graphical Models:
Principles and Techniques</em>. The MIT Press.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+filtering">filtering</a></code>, <code><a href="#topic+inference">inference</a></code>,
<code><a href="#topic+smoothing">smoothing</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
set.seed(0)
data(gmdbn_air, data_air)
evid &lt;- data_air
evid$NO2[sample.int(7680, 1536)] &lt;- NA
evid$O3[sample.int(7680, 1536)] &lt;- NA
pred &lt;- prediction(gmdbn_air, evid, evid[, c("DATE", "TEMP", "WIND")],
                   nodes = c("NO2", "O3"), col_seq = "DATE",
                   horizon = c(1, 2), verbose = TRUE)

</code></pre>

<hr>
<h2 id='propagation'>Propagate particles forward in time</h2><span id='topic+propagation'></span>

<h3>Description</h3>

<p>This function propagates particles forward in time. Assuming that the
particles have been propagated to a given time slice <code class="reqn">t</code>, the aim is to
propagate them to a later time slice <code class="reqn">t + k</code> according to the Gaussian
mixture graphical model and to the evidence collected over time. At first, a
renewal step is performed if the effective sample size (ESS) is below a given
threshold (Doucet and Johansen, 2009). This step consists in randomly
selecting new particles among the old ones proportionately to their current
weights. Upon receiving the data (the evidence) of <code class="reqn">t + 1</code>, each particle
is used to generate samples for the unknown values. Its weight is then
updated to the likelihood for the observed values. The higher this
likelihood, the more likely the particle is selected at the next renewal step
for propagation to <code class="reqn">t + 2</code>, and so on (Koller and Friedman, 2009).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>propagation(
  part,
  gmgm,
  evid = NULL,
  col_seq = NULL,
  col_weight = "weight",
  n_times = 1,
  min_ess = 1
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="propagation_+3A_part">part</code></td>
<td>
<p>A data frame containing the particles propagated to time slice
<code class="reqn">t</code>, as obtained from function <code><a href="#topic+particles">particles</a></code> or
<code><a href="#topic+propagation">propagation</a></code>.</p>
</td></tr>
<tr><td><code id="propagation_+3A_gmgm">gmgm</code></td>
<td>
<p>An object of class <code>gmbn</code> or <code>gmdbn</code>. For a
<code>gmdbn</code> object, the <code>gmbn</code> elements used for propagation are
selected according to the temporal depth of the particles, assuming that the
particles contain all the samples since the first time slice (this depth is
thus considered as the current time slice).</p>
</td></tr>
<tr><td><code id="propagation_+3A_evid">evid</code></td>
<td>
<p>A data frame containing the evidence of time slices
<code class="reqn">t + 1, \dots , t + k</code>. Its columns must explicitly be named after nodes
of <code>gmgm</code> and can contain missing values (columns with no value can be
removed). If <code>NULL</code> (the default), no evidence is taken into account.</p>
</td></tr>
<tr><td><code id="propagation_+3A_col_seq">col_seq</code></td>
<td>
<p>A character vector containing the column names of <code>part</code>
and <code>evid</code> that describe the observation sequence. If <code>NULL</code> (the
default), all the particles and observations belong to a single sequence. In
<code>evid</code>, the observations of a same sequence must be ordered such that
the <code class="reqn">k</code>th one is related to time slice <code class="reqn">t + k</code> (note that the
sequences can have different lengths).</p>
</td></tr>
<tr><td><code id="propagation_+3A_col_weight">col_weight</code></td>
<td>
<p>A character string corresponding to the column name of
<code>part</code> that describes the particle weight.</p>
</td></tr>
<tr><td><code id="propagation_+3A_n_times">n_times</code></td>
<td>
<p>A non-negative integer corresponding to the number of time
slices <code class="reqn">k</code> over which the particles are propagated.</p>
</td></tr>
<tr><td><code id="propagation_+3A_min_ess">min_ess</code></td>
<td>
<p>A numeric value in [0, 1] corresponding to the minimum ESS
(expressed as a proportion of the number of particles) under which the
renewal step is performed. If <code>1</code> (the default), this step is performed
at each time slice.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame (tibble) containing the particles supplemented with the
samples of time slices <code class="reqn">t + 1, \dots , t + k</code>.
</p>


<h3>References</h3>

<p>Doucet, A. and Johansen, A. M. (2009). A Tutorial on Particle Filtering and
Smoothing: Fifteen years later. <em>Handbook of nonlinear filtering</em>,
12:656&ndash;704.
</p>
<p>Koller, D. and Friedman, N. (2009). <em>Probabilistic Graphical Models:
Principles and Techniques</em>. The MIT Press.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+aggregation">aggregation</a></code>, <code><a href="#topic+particles">particles</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(dplyr)
set.seed(0)
data(gmdbn_air, data_air)
evid &lt;- data_air %&gt;%
  group_by(DATE) %&gt;%
  slice(1:3) %&gt;%
  ungroup()
evid$NO2[sample.int(150, 30)] &lt;- NA
evid$O3[sample.int(150, 30)] &lt;- NA
evid$TEMP[sample.int(150, 30)] &lt;- NA
evid$WIND[sample.int(150, 30)] &lt;- NA
part &lt;- particles(data.frame(DATE = unique(evid$DATE))) %&gt;%
  propagation(gmdbn_air, evid, col_seq = "DATE", n_times = 3)

</code></pre>

<hr>
<h2 id='relevant'>Extract the minimal sub-Gaussian mixture graphical model required to infer a
subset of nodes</h2><span id='topic+relevant'></span>

<h3>Description</h3>

<p>This function extracts the minimal sub-Gaussian mixture graphical model
required to infer a subset of nodes (i.e. the sub-model relevant to these
nodes). The nodes that do not contribute to inference are removed, which
includes those that are d-separated from the inferred ones by the nodes whose
values are observed, as well as the barren nodes (Druzdzel and Suermondt,
1994).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>relevant(gmgm, nodes, nodes_obs = NULL, nodes_miss = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="relevant_+3A_gmgm">gmgm</code></td>
<td>
<p>An object of class <code>gmbn</code> or <code>gmdbn</code>.</p>
</td></tr>
<tr><td><code id="relevant_+3A_nodes">nodes</code></td>
<td>
<p>A character vector containing the inferred nodes.</p>
</td></tr>
<tr><td><code id="relevant_+3A_nodes_obs">nodes_obs</code></td>
<td>
<p>A character vector containing the nodes whose values are
observed.</p>
</td></tr>
<tr><td><code id="relevant_+3A_nodes_miss">nodes_miss</code></td>
<td>
<p>A character vector containing the nodes whose values are
missing. Note that if a node is neither in <code>nodes_obs</code> nor in
<code>nodes_miss</code>, its observability is considered uncertain or varying.
Thus, it is not treated as an observed node, nor can it be removed as a
barren node.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>gmbn</code> or <code>gmdbn</code> object relevant to the subset of
nodes.
</p>


<h3>References</h3>

<p>Druzdzel, M. J. and Suermondt, H. J. (1994). Relevance in Probabilistic
Models: &quot;Backyards&quot; in a &quot;Small World&quot;. <em>In Working Notes of the AAAI
1994 Fall Symposium Series: Relevance</em>, pages 60&ndash;63, New Orleans, LA, USA.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+add_arcs">add_arcs</a></code>, <code><a href="#topic+add_nodes">add_nodes</a></code>,
<code><a href="#topic+remove_arcs">remove_arcs</a></code>, <code><a href="#topic+remove_nodes">remove_nodes</a></code>,
<code><a href="#topic+rename_nodes">rename_nodes</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(gmbn_body)
gmbn_1 &lt;- relevant(gmbn_body, "AGE",
                   nodes_obs = c("FAT", "HEIGHT", "WEIGHT"),
                   nodes_miss = "GLYCO")

data(gmdbn_air)
gmdbn_1 &lt;- do.call("gmdbn", gmdbn_air[c("b_1", "b_2")])
gmdbn_2 &lt;- relevant(gmdbn_1, "O3", nodes_obs = "NO2")

</code></pre>

<hr>
<h2 id='remove_arcs'>Remove arcs from a Gaussian mixture graphical model</h2><span id='topic+remove_arcs'></span>

<h3>Description</h3>

<p>This function removes arcs from a Gaussian mixture graphical model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>remove_arcs(gmgm, arcs)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="remove_arcs_+3A_gmgm">gmgm</code></td>
<td>
<p>An object of class <code>gmbn</code> or <code>gmdbn</code>.</p>
</td></tr>
<tr><td><code id="remove_arcs_+3A_arcs">arcs</code></td>
<td>
<p>A data frame containing the removed arcs. The column <code>from</code>
describes the start node, the column <code>to</code> the end node and the column
<code>lag</code> the time lag between them. Missing values in <code>from</code> or
<code>to</code> are interpreted as &quot;all possible nodes&quot;, which allows to quickly
define large set of arcs that share common attributes. Missing values in
<code>lag</code> are replaced by 0. If <code>gmgm</code> is a <code>gmdbn</code> object, the
same arcs are removed from each of its <code>gmbn</code> elements. This constraint
can be overcome by passing a list of data frames named after some of these
elements (<code>b_1</code>, ...) and containing arcs specifically removed from
them.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>gmbn</code> or <code>gmdbn</code> object after removing the arcs.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+add_arcs">add_arcs</a></code>, <code><a href="#topic+add_nodes">add_nodes</a></code>,
<code><a href="#topic+relevant">relevant</a></code>, <code><a href="#topic+remove_nodes">remove_nodes</a></code>,
<code><a href="#topic+rename_nodes">rename_nodes</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(gmbn_body)
gmbn_1 &lt;- remove_arcs(gmbn_body,
                      data.frame(from = c("HEIGHT", "AGE"),
                                 to = c("FAT", "WAIST")))

data(gmdbn_air)
gmdbn_1 &lt;- remove_arcs(gmdbn_air,
                       list(b_2 = data.frame(from = c("NO2", "TEMP"),
                                             to = c("O3", "O3"), lag = c(1, 1)),
                            b_13 = data.frame(from = "TEMP", to = "O3",
                                              lag = 1)))

</code></pre>

<hr>
<h2 id='remove_nodes'>Remove nodes from a Gaussian mixture graphical model</h2><span id='topic+remove_nodes'></span>

<h3>Description</h3>

<p>This function removes nodes from a Gaussian mixture graphical model. If this
model is a dynamic Bayesian network, the nodes are removed from each of its
transition models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>remove_nodes(gmgm, nodes)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="remove_nodes_+3A_gmgm">gmgm</code></td>
<td>
<p>An object of class <code>gmbn</code> or <code>gmdbn</code>.</p>
</td></tr>
<tr><td><code id="remove_nodes_+3A_nodes">nodes</code></td>
<td>
<p>A character vector containing the removed nodes.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>gmbn</code> or <code>gmdbn</code> object after removing the nodes.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+add_arcs">add_arcs</a></code>, <code><a href="#topic+add_nodes">add_nodes</a></code>,
<code><a href="#topic+relevant">relevant</a></code>, <code><a href="#topic+remove_arcs">remove_arcs</a></code>, <code><a href="#topic+rename_nodes">rename_nodes</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(gmbn_body)
gmbn_1 &lt;- remove_nodes(gmbn_body, c("FAT", "GLYCO"))

data(gmdbn_air)
gmdbn_1 &lt;- remove_nodes(gmdbn_air, "TEMP")

</code></pre>

<hr>
<h2 id='remove_var'>Remove variables from a Gaussian mixture model</h2><span id='topic+remove_var'></span>

<h3>Description</h3>

<p>This function removes variables from a Gaussian mixture model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>remove_var(gmm, var)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="remove_var_+3A_gmm">gmm</code></td>
<td>
<p>An object of class <code>gmm</code>.</p>
</td></tr>
<tr><td><code id="remove_var_+3A_var">var</code></td>
<td>
<p>A character vector containing the removed variables.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>gmm</code> object after removing the variables.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+add_var">add_var</a></code>, <code><a href="#topic+rename_var">rename_var</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(gmm_body)
gmm_1 &lt;- remove_var(gmm_body, "FAT")

</code></pre>

<hr>
<h2 id='rename_nodes'>Rename nodes of a Gaussian mixture graphical model</h2><span id='topic+rename_nodes'></span>

<h3>Description</h3>

<p>This function renames nodes of a Gaussian mixture graphical model. If this
model is a dynamic Bayesian network, the nodes are renamed for each of its
transition models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rename_nodes(gmgm, nodes, names)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="rename_nodes_+3A_gmgm">gmgm</code></td>
<td>
<p>An object of class <code>gmbn</code> or <code>gmdbn</code>.</p>
</td></tr>
<tr><td><code id="rename_nodes_+3A_nodes">nodes</code></td>
<td>
<p>A character vector containing the renamed nodes.</p>
</td></tr>
<tr><td><code id="rename_nodes_+3A_names">names</code></td>
<td>
<p>A character vector containing the respective new names of the
nodes.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>gmbn</code> or <code>gmdbn</code> object after renaming the nodes.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+add_arcs">add_arcs</a></code>, <code><a href="#topic+add_nodes">add_nodes</a></code>,
<code><a href="#topic+relevant">relevant</a></code>, <code><a href="#topic+remove_arcs">remove_arcs</a></code>, <code><a href="#topic+remove_nodes">remove_nodes</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(gmbn_body)
gmbn_1 &lt;- rename_nodes(gmbn_body, c("FAT", "GLYCO"),
                       c("BODY_FAT", "GLYCOHEMOGLOBIN"))

data(gmdbn_air)
gmdbn_1 &lt;- rename_nodes(gmdbn_air, "TEMP", "TEMPERATURE")

</code></pre>

<hr>
<h2 id='rename_var'>Rename variables of a Gaussian mixture model</h2><span id='topic+rename_var'></span>

<h3>Description</h3>

<p>This function renames variables of a Gaussian mixture model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rename_var(gmm, var, names)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="rename_var_+3A_gmm">gmm</code></td>
<td>
<p>An object of class <code>gmm</code>.</p>
</td></tr>
<tr><td><code id="rename_var_+3A_var">var</code></td>
<td>
<p>A character vector containing the renamed variables.</p>
</td></tr>
<tr><td><code id="rename_var_+3A_names">names</code></td>
<td>
<p>A character vector containing the respective new names of the
variables.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>gmm</code> object after renaming the variables.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+add_var">add_var</a></code>, <code><a href="#topic+remove_var">remove_var</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(gmm_body)
gmm_1 &lt;- rename_var(gmm_body, "FAT", "BODY_FAT")

</code></pre>

<hr>
<h2 id='reorder'>Reorder the variables and the mixture components of a Gaussian mixture model</h2><span id='topic+reorder'></span>

<h3>Description</h3>

<p>This function reorders the variables and the mixture components of a Gaussian
mixture model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>reorder(gmm, var = NULL, comp = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="reorder_+3A_gmm">gmm</code></td>
<td>
<p>An object of class <code>gmm</code>.</p>
</td></tr>
<tr><td><code id="reorder_+3A_var">var</code></td>
<td>
<p>A character vector containing the variables in the desired order.
If variables are not specified, they are added after the ordered ones. If
<code>NULL</code> (the default), the variables are not reordered.</p>
</td></tr>
<tr><td><code id="reorder_+3A_comp">comp</code></td>
<td>
<p>An integer vector containing the indexes of the mixture component
in the desired order. If components are not specified, they are added after
the ordered ones. If <code>NULL</code> (the default), the components are not
reordered.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The reordered <code>gmm</code> object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(gmm_body)
gmm_1 &lt;- reorder(gmm_body, var = c("WAIST", "AGE", "FAT", "HEIGHT", "WEIGHT"),
                 comp = c(2, 1, 3))

</code></pre>

<hr>
<h2 id='sampling'>Sample a Gaussian mixture model</h2><span id='topic+sampling'></span>

<h3>Description</h3>

<p>This function samples a Gaussian mixture model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sampling(gmm, data_x = NULL, n = 1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sampling_+3A_gmm">gmm</code></td>
<td>
<p>An object of class <code>gmm</code>.</p>
</td></tr>
<tr><td><code id="sampling_+3A_data_x">data_x</code></td>
<td>
<p>A data frame or numeric matrix containing observations of the
explanatory variables if conditional sampling is performed. Its columns must
explicitly be named after the explanatory variables. If <code>NULL</code> (the
default), joint sampling is performed.</p>
</td></tr>
<tr><td><code id="sampling_+3A_n">n</code></td>
<td>
<p>A non-negative integer corresponding to the number of samples. If
conditional sampling is performed, this argument is ignored.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numeric matrix containing the samples.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+density">density</a></code>, <code><a href="#topic+expectation">expectation</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(0)
data(gmm_body, data_body)
sampl_1 &lt;- sampling(gmm_body, n = 500)
sampl_2 &lt;- sampling(gmm_body,
                    data_body[, c("WEIGHT", "FAT", "HEIGHT", "AGE")])

</code></pre>

<hr>
<h2 id='smem'>Select the number of mixture components and estimate the parameters of a
Gaussian mixture model</h2><span id='topic+smem'></span>

<h3>Description</h3>

<p>This function selects the number of mixture components and estimates the
parameters of a Gaussian mixture model using a split-and-merge EM (SMEM)
algorithm. At the first iteration, the classic EM algorithm is performed to
update the parameters of the initial model. Then each following iteration
consists in splitting a component into two or merging two components, before
re-estimating the parameters with the EM algorithm. The selected split or
merge operation is the one that maximizes a scoring function (after the
re-estimation process). To avoid testing all possible operations, the split
and merge candidates are initially ranked according to relevant criteria
(Zhang <em>et al.</em>, 2003). At first, the top-ranked split and top-ranked
merge operations are tested. If neither of them increases the score, the
second-ranked ones are considered, and so on. The SMEM algorithm stops if a
given maximum rank is reached without improving the score.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>smem(
  gmm,
  data,
  y = NULL,
  score = "bic",
  split = TRUE,
  merge = TRUE,
  min_comp = 1,
  max_comp = Inf,
  space = 0.5,
  max_rank = 1,
  max_iter_smem = 10,
  verbose = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="smem_+3A_gmm">gmm</code></td>
<td>
<p>An initial object of class <code>gmm</code>.</p>
</td></tr>
<tr><td><code id="smem_+3A_data">data</code></td>
<td>
<p>A data frame or numeric matrix containing the data used in the
SMEM algorithm. Its columns must explicitly be named after the variables of
<code>gmm</code> and must not contain missing values.</p>
</td></tr>
<tr><td><code id="smem_+3A_y">y</code></td>
<td>
<p>A character vector containing the dependent variables if a
conditional model is estimated (which involves maximizing a conditional
score). If <code>NULL</code> (the default), the joint model is estimated.</p>
</td></tr>
<tr><td><code id="smem_+3A_score">score</code></td>
<td>
<p>A character string (<code>"aic"</code>, <code>"bic"</code> or
<code>"loglik"</code>) corresponding to the scoring function.</p>
</td></tr>
<tr><td><code id="smem_+3A_split">split</code></td>
<td>
<p>A logical value indicating whether split operations are allowed
(if <code>FALSE</code>, no mixture component can be split).</p>
</td></tr>
<tr><td><code id="smem_+3A_merge">merge</code></td>
<td>
<p>A logical value indicating whether merge operations are allowed
(if <code>FALSE</code>, no mixture component can be merged).</p>
</td></tr>
<tr><td><code id="smem_+3A_min_comp">min_comp</code></td>
<td>
<p>A positive integer corresponding to the minimum number of
mixture components.</p>
</td></tr>
<tr><td><code id="smem_+3A_max_comp">max_comp</code></td>
<td>
<p>A positive integer corresponding to the maximum number of
mixture components.</p>
</td></tr>
<tr><td><code id="smem_+3A_space">space</code></td>
<td>
<p>A numeric value in [0, 1[ corresponding to the space between two
subcomponents resulting from a split.</p>
</td></tr>
<tr><td><code id="smem_+3A_max_rank">max_rank</code></td>
<td>
<p>A positive integer corresponding to the maximum rank for
testing the split and merge candidates.</p>
</td></tr>
<tr><td><code id="smem_+3A_max_iter_smem">max_iter_smem</code></td>
<td>
<p>A non-negative integer corresponding to the maximum
number of iterations.</p>
</td></tr>
<tr><td><code id="smem_+3A_verbose">verbose</code></td>
<td>
<p>A logical value indicating whether iterations in progress
are displayed.</p>
</td></tr>
<tr><td><code id="smem_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to function <code><a href="#topic+em">em</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with elements:
</p>
<table role = "presentation">
<tr><td><code>gmm</code></td>
<td>
<p>The final <code>gmm</code> object.</p>
</td></tr>
<tr><td><code>posterior</code></td>
<td>
<p>A numeric matrix containing the posterior probabilities for
each observation.</p>
</td></tr>
<tr><td><code>seq_score</code></td>
<td>
<p>A numeric vector containing the sequence of scores measured
initially and after each iteration.</p>
</td></tr>
<tr><td><code>seq_oper</code></td>
<td>
<p>A character vector containing the sequence of split and merge
operations performed at each iteration.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Zhang, Z., Chen, C., Sun, J. and Chan, K. L. (2003). EM algorithms for
Gaussian mixtures with split-and-merge operation. <em>Pattern Recognition</em>,
36(9):1973&ndash;1983.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+em">em</a></code>, <code><a href="#topic+stepwise">stepwise</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(data_body)
gmm_1 &lt;- add_var(NULL, c("WAIST", "AGE", "FAT", "HEIGHT", "WEIGHT"))
res_smem &lt;- smem(gmm_1, data_body, max_comp = 3, verbose = TRUE)

</code></pre>

<hr>
<h2 id='smoothing'>Perform smoothing inference in a Gaussian mixture dynamic Bayesian network</h2><span id='topic+smoothing'></span>

<h3>Description</h3>

<p>This function performs smoothing inference in a Gaussian mixture dynamic
Bayesian network. For a sequence of <code class="reqn">T</code> time slices, this task consists
in estimating the state of the system at each time slice <code class="reqn">t</code> (for
<code class="reqn">1 \le t \le T</code>) given all the data (the evidence) collected up to
<code class="reqn">T</code>. Smoothing inference is performed by sequential importance
resampling, which is a particle-based approximate method (Koller and
Friedman, 2009).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>smoothing(
  gmdbn,
  evid,
  nodes = names(gmdbn$b_1),
  col_seq = NULL,
  n_part = 1000,
  max_part_sim = 1e+06,
  min_ess = 1,
  verbose = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="smoothing_+3A_gmdbn">gmdbn</code></td>
<td>
<p>An object of class <code>gmdbn</code>.</p>
</td></tr>
<tr><td><code id="smoothing_+3A_evid">evid</code></td>
<td>
<p>A data frame containing the evidence. Its columns must explicitly
be named after nodes of <code>gmdbn</code> and can contain missing values (columns
with no value can be removed).</p>
</td></tr>
<tr><td><code id="smoothing_+3A_nodes">nodes</code></td>
<td>
<p>A character vector containing the inferred nodes (by default all
the nodes of <code>gmdbn</code>).</p>
</td></tr>
<tr><td><code id="smoothing_+3A_col_seq">col_seq</code></td>
<td>
<p>A character vector containing the column names of <code>evid</code>
that describe the observation sequence. If <code>NULL</code> (the default), all the
observations belong to a single sequence. The observations of a same sequence
must be ordered such that the <code class="reqn">t</code>th one is related to time slice <code class="reqn">t</code>
(note that the sequences can have different lengths).</p>
</td></tr>
<tr><td><code id="smoothing_+3A_n_part">n_part</code></td>
<td>
<p>A positive integer corresponding to the number of particles
generated for each observation sequence.</p>
</td></tr>
<tr><td><code id="smoothing_+3A_max_part_sim">max_part_sim</code></td>
<td>
<p>An integer greater than or equal to <code>n_part</code>
corresponding to the maximum number of particles that can be processed
simultaneously. This argument is used to prevent memory overflow, dividing
<code>evid</code> into smaller subsets that are handled sequentially.</p>
</td></tr>
<tr><td><code id="smoothing_+3A_min_ess">min_ess</code></td>
<td>
<p>A numeric value in [0, 1] corresponding to the minimum ESS
(expressed as a proportion of <code>n_part</code>) under which the renewal step of
sequential importance resampling is performed. If <code>1</code> (the default),
this step is performed at each time slice.</p>
</td></tr>
<tr><td><code id="smoothing_+3A_verbose">verbose</code></td>
<td>
<p>A logical value indicating whether subsets of <code>evid</code> and
time slices in progress are displayed.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame (tibble) with a structure similar to <code>evid</code>
containing the estimated values of the inferred nodes and their observation
sequences (if <code>col_seq</code> is not <code>NULL</code>).
</p>


<h3>References</h3>

<p>Koller, D. and Friedman, N. (2009). <em>Probabilistic Graphical Models:
Principles and Techniques</em>. The MIT Press.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+filtering">filtering</a></code>, <code><a href="#topic+inference">inference</a></code>,
<code><a href="#topic+prediction">prediction</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
set.seed(0)
data(gmdbn_air, data_air)
evid &lt;- data_air
evid$NO2[sample.int(7680, 1536)] &lt;- NA
evid$O3[sample.int(7680, 1536)] &lt;- NA
evid$TEMP[sample.int(7680, 1536)] &lt;- NA
evid$WIND[sample.int(7680, 1536)] &lt;- NA
smooth &lt;- smoothing(gmdbn_air, evid, col_seq = "DATE", verbose = TRUE)

</code></pre>

<hr>
<h2 id='split_comp'>Split a mixture component of a Gaussian mixture model</h2><span id='topic+split_comp'></span>

<h3>Description</h3>

<p>This function splits a mixture component of a Gaussian mixture model using
the singular value decomposition of the covariance matrix (Zhang
<em>et al.</em>, 2003).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>split_comp(gmm, comp = 1, n_sub = 2, space = 0.5)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="split_comp_+3A_gmm">gmm</code></td>
<td>
<p>An object of class <code>gmm</code>.</p>
</td></tr>
<tr><td><code id="split_comp_+3A_comp">comp</code></td>
<td>
<p>An integer corresponding to the index of the split mixture
component.</p>
</td></tr>
<tr><td><code id="split_comp_+3A_n_sub">n_sub</code></td>
<td>
<p>A positive integer corresponding to the number of subcomponents.</p>
</td></tr>
<tr><td><code id="split_comp_+3A_space">space</code></td>
<td>
<p>A numeric value in [0, 1[ corresponding to the space between the
subcomponents.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>gmm</code> object after splitting the mixture component.
</p>


<h3>References</h3>

<p>Zhang, Z., Chen, C., Sun, J. and Chan, K. L. (2003). EM algorithms for
Gaussian mixtures with split-and-merge operation. <em>Pattern Recognition</em>,
36(9):1973&ndash;1983.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+merge_comp">merge_comp</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(gmm_body)
gmm_1 &lt;- split_comp(gmm_body, n_sub = 3)

</code></pre>

<hr>
<h2 id='stepwise'>Select the explanatory variables, the number of mixture components and
estimate the parameters of a conditional Gaussian mixture model</h2><span id='topic+stepwise'></span>

<h3>Description</h3>

<p>This function selects the explanatory variables, the number of mixture
components and estimates the parameters of a conditional Gaussian mixture
model using a stepwise algorithm. At the first iteration, the SMEM algorithm
is performed to update the number of components and the parameters of the
initial model. Then each following iteration consists in adding or removing a
candidate explanatory variable, before re-estimating the model with the SMEM
algorithm. The selected add or remove operation is the one that maximizes a
conditional scoring function (after the re-estimation process). The stepwise
algorithm stops if none of the candidate operations improves the score.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stepwise(
  gmm,
  data,
  y = rownames(gmm$mu)[1],
  x_cand = setdiff(colnames(data), y),
  score = "bic",
  add = TRUE,
  remove = TRUE,
  min_x = 0,
  max_x = Inf,
  max_iter_step = 10,
  verbose = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="stepwise_+3A_gmm">gmm</code></td>
<td>
<p>An initial object of class <code>gmm</code>.</p>
</td></tr>
<tr><td><code id="stepwise_+3A_data">data</code></td>
<td>
<p>A data frame or numeric matrix containing the data used in the
stepwise algorithm. Its columns must explicitly be named after the variables
of <code>gmm</code> and the candidate explanatory variables, and must not contain
missing values.</p>
</td></tr>
<tr><td><code id="stepwise_+3A_y">y</code></td>
<td>
<p>A character vector containing the dependent variables (by default
the first variable of <code>gmm</code>).</p>
</td></tr>
<tr><td><code id="stepwise_+3A_x_cand">x_cand</code></td>
<td>
<p>A character vector containing the candidate explanatory
variables for addition or removal (by default all the column names of
<code>data</code> except <code>y</code>). If variables already in <code>gmm</code> are not
candidates, they cannot be removed.</p>
</td></tr>
<tr><td><code id="stepwise_+3A_score">score</code></td>
<td>
<p>A character string (<code>"aic"</code>, <code>"bic"</code> or
<code>"loglik"</code>) corresponding to the scoring function.</p>
</td></tr>
<tr><td><code id="stepwise_+3A_add">add</code></td>
<td>
<p>A logical value indicating whether add operations are allowed (if
<code>FALSE</code>, no variable can be added).</p>
</td></tr>
<tr><td><code id="stepwise_+3A_remove">remove</code></td>
<td>
<p>A logical value indicating whether remove operations are
allowed (if <code>FALSE</code>, no variable can be removed).</p>
</td></tr>
<tr><td><code id="stepwise_+3A_min_x">min_x</code></td>
<td>
<p>A non-negative integer corresponding to the minimum number of
explanatory variables.</p>
</td></tr>
<tr><td><code id="stepwise_+3A_max_x">max_x</code></td>
<td>
<p>A non-negative integer corresponding to the maximum number of
explanatory variables.</p>
</td></tr>
<tr><td><code id="stepwise_+3A_max_iter_step">max_iter_step</code></td>
<td>
<p>A non-negative integer corresponding to the maximum
number of iterations.</p>
</td></tr>
<tr><td><code id="stepwise_+3A_verbose">verbose</code></td>
<td>
<p>A logical value indicating whether iterations in progress
are displayed.</p>
</td></tr>
<tr><td><code id="stepwise_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to function <code><a href="#topic+smem">smem</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with elements:
</p>
<table role = "presentation">
<tr><td><code>gmm</code></td>
<td>
<p>The final <code>gmm</code> object.</p>
</td></tr>
<tr><td><code>posterior</code></td>
<td>
<p>A numeric matrix containing the posterior probabilities for
each observation.</p>
</td></tr>
<tr><td><code>seq_score</code></td>
<td>
<p>A numeric vector containing the sequence of scores measured
initially and after each iteration.</p>
</td></tr>
<tr><td><code>seq_oper</code></td>
<td>
<p>A character vector containing the sequence of add and remove
operations performed at each iteration.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+em">em</a></code>, <code><a href="#topic+smem">smem</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(data_body)
gmm_1 &lt;- add_var(NULL, "WAIST")
res_step &lt;- stepwise(gmm_1, data_body, verbose = TRUE, max_comp = 3)

</code></pre>

<hr>
<h2 id='struct_em'>Learn the structure and the parameters of a Gaussian mixture graphical model
with incomplete data</h2><span id='topic+struct_em'></span>

<h3>Description</h3>

<p>This function learns the structure and the parameters of a Gaussian mixture
graphical model with incomplete data using the structural EM algorithm. At
each iteration, the parametric EM algorithm is performed to complete the data
and update the parameters (E step). The completed data are then used to
update the structure (M step), and so on. Each iteration is guaranteed to
increase the scoring function until convergence to a local maximum (Koller
and Friedman, 2009). In practice, due to the sampling process inherent in
particle-based inference, it may happen that the monotonic increase no longer
occurs when approaching the local maximum, resulting in an earlier
termination of the algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>struct_em(
  gmgm,
  data,
  nodes = structure(gmgm)$nodes,
  arcs_cand = tibble(lag = 0),
  col_seq = NULL,
  score = "bic",
  n_part = 1000,
  max_part_sim = 1e+06,
  min_ess = 1,
  max_iter_sem = 5,
  max_iter_pem = 5,
  verbose = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="struct_em_+3A_gmgm">gmgm</code></td>
<td>
<p>An object of class <code>gmbn</code> (non-temporal) or <code>gmdbn</code>.</p>
</td></tr>
<tr><td><code id="struct_em_+3A_data">data</code></td>
<td>
<p>A data frame containing the data used for learning. Its columns
must explicitly be named after nodes of <code>gmgm</code> and can contain missing
values (columns with no value can be removed).</p>
</td></tr>
<tr><td><code id="struct_em_+3A_nodes">nodes</code></td>
<td>
<p>A character vector containing the nodes whose local conditional
models are learned (by default all the nodes of <code>gmgm</code>). If <code>gmgm</code>
is a <code>gmdbn</code> object, the same nodes are learned for each of its
<code>gmbn</code> elements. This constraint can be overcome by passing a list of
character vectors named after some of these elements (<code>b_1</code>, ...) and
containing learned nodes specific to them.</p>
</td></tr>
<tr><td><code id="struct_em_+3A_arcs_cand">arcs_cand</code></td>
<td>
<p>A data frame containing the candidate arcs for addition or
removal (by default all possible non-temporal arcs). The column <code>from</code>
describes the start node, the column <code>to</code> the end node and the column
<code>lag</code> the time lag between them. Missing values in <code>from</code> or
<code>to</code> are interpreted as &quot;all possible nodes&quot;, which allows to quickly
define large set of arcs that share common attributes. Missing values in
<code>lag</code> are replaced by 0. If <code>gmgm</code> is a <code>gmdbn</code> object, the
same candidate arcs are used for each of its <code>gmbn</code> elements. This
constraint can be overcome by passing a list of data frames named after some
of these elements (<code>b_1</code>, ...) and containing candidate arcs specific
to them. If arcs already in <code>gmgm</code> are not candidates, they cannot be
removed. Therefore, setting <code>arcs_cand</code> to <code>NULL</code> is equivalent to
learning only the mixture structure (and the parameters) of the model.</p>
</td></tr>
<tr><td><code id="struct_em_+3A_col_seq">col_seq</code></td>
<td>
<p>A character vector containing the column names of <code>data</code>
that describe the observation sequence. If <code>NULL</code> (the default), all the
observations belong to a single sequence. If <code>gmgm</code> is a <code>gmdbn</code>
object, the observations of a same sequence must be ordered such that the
<code class="reqn">t</code>th one is related to time slice <code class="reqn">t</code> (note that the sequences can
have different lengths). If <code>gmgm</code> is a <code>gmbn</code> object, this
argument is ignored.</p>
</td></tr>
<tr><td><code id="struct_em_+3A_score">score</code></td>
<td>
<p>A character string (<code>"aic"</code>, <code>"bic"</code> or
<code>"loglik"</code>) corresponding to the scoring function.</p>
</td></tr>
<tr><td><code id="struct_em_+3A_n_part">n_part</code></td>
<td>
<p>A positive integer corresponding to the number of particles
generated for each observation (if <code>gmgm</code> is a <code>gmbn</code> object) or
observation sequence (if <code>gmgm</code> is a <code>gmdbn</code> object) during
inference.</p>
</td></tr>
<tr><td><code id="struct_em_+3A_max_part_sim">max_part_sim</code></td>
<td>
<p>An integer greater than or equal to <code>n_part</code>
corresponding to the maximum number of particles that can be processed
simultaneously during inference. This argument is used to prevent memory
overflow, dividing <code>data</code> into smaller subsets that are handle
sequentially.</p>
</td></tr>
<tr><td><code id="struct_em_+3A_min_ess">min_ess</code></td>
<td>
<p>A numeric value in [0, 1] corresponding to the minimum ESS
(expressed as a proportion of <code>n_part</code>) under which the renewal step of
sequential importance resampling is performed. If <code>1</code> (the default),
this step is performed at each time slice. If <code>gmgm</code> is a <code>gmbn</code>
object, this argument is ignored.</p>
</td></tr>
<tr><td><code id="struct_em_+3A_max_iter_sem">max_iter_sem</code></td>
<td>
<p>A non-negative integer corresponding to the maximum
number of iterations.</p>
</td></tr>
<tr><td><code id="struct_em_+3A_max_iter_pem">max_iter_pem</code></td>
<td>
<p>A non-negative integer corresponding to the maximum
number of iterations of the parametric EM algorithm.</p>
</td></tr>
<tr><td><code id="struct_em_+3A_verbose">verbose</code></td>
<td>
<p>A logical value indicating whether iterations in progress
are displayed.</p>
</td></tr>
<tr><td><code id="struct_em_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to function <code><a href="#topic+stepwise">stepwise</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with elements:
</p>
<table role = "presentation">
<tr><td><code>gmgm</code></td>
<td>
<p>The final <code>gmbn</code> or <code>gmdbn</code> object (with the highest
score).</p>
</td></tr>
<tr><td><code>data</code></td>
<td>
<p>A data frame (tibble) containing the complete data used to learn
the final <code>gmbn</code> or <code>gmdbn</code> object.</p>
</td></tr>
<tr><td><code>seq_score</code></td>
<td>
<p>A numeric matrix containing the sequence of scores measured
after the E and M steps of each iteration.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Koller, D. and Friedman, N. (2009). <em>Probabilistic Graphical Models:
Principles and Techniques</em>. The MIT Press.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+param_em">param_em</a></code>, <code><a href="#topic+param_learn">param_learn</a></code>,
<code><a href="#topic+struct_learn">struct_learn</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
set.seed(0)
data(data_body)
data_1 &lt;- data_body
data_1$GENDER[sample.int(2148, 430)] &lt;- NA
data_1$AGE[sample.int(2148, 430)] &lt;- NA
data_1$HEIGHT[sample.int(2148, 430)] &lt;- NA
data_1$WEIGHT[sample.int(2148, 430)] &lt;- NA
data_1$FAT[sample.int(2148, 430)] &lt;- NA
data_1$WAIST[sample.int(2148, 430)] &lt;- NA
data_1$GLYCO[sample.int(2148, 430)] &lt;- NA
gmbn_1 &lt;- add_nodes(NULL,
                    c("AGE", "FAT", "GENDER", "GLYCO", "HEIGHT", "WAIST",
                      "WEIGHT"))
arcs_cand_1 &lt;- data.frame(from = c("AGE", "GENDER", "HEIGHT", "WEIGHT", NA,
                                   "AGE", "GENDER", "AGE", "FAT", "GENDER",
                                   "HEIGHT", "WEIGHT", "AGE", "GENDER",
                                   "HEIGHT"),
                          to = c("FAT", "FAT", "FAT", "FAT", "GLYCO", "HEIGHT",
                                 "HEIGHT", "WAIST", "WAIST", "WAIST", "WAIST",
                                 "WAIST", "WEIGHT", "WEIGHT", "WEIGHT"))
res_learn_1 &lt;- struct_em(gmbn_1, data_1, arcs_cand = arcs_cand_1,
                         verbose = TRUE, max_comp = 3)

set.seed(0)
data(data_air)
data_2 &lt;- data_air
data_2$NO2[sample.int(7680, 1536)] &lt;- NA
data_2$O3[sample.int(7680, 1536)] &lt;- NA
data_2$TEMP[sample.int(7680, 1536)] &lt;- NA
data_2$WIND[sample.int(7680, 1536)] &lt;- NA
gmdbn_1 &lt;- gmdbn(b_2 = add_nodes(NULL, c("NO2", "O3", "TEMP", "WIND")),
                 b_13 = add_nodes(NULL, c("NO2", "O3", "TEMP", "WIND")))
arcs_cand_2 &lt;- data.frame(from = c("NO2", "NO2", "NO2", "O3", "TEMP", "TEMP",
                                   "WIND", "WIND"),
                          to = c("NO2", "O3", "O3", "O3", NA, NA, NA, NA),
                          lag = c(1, 0, 1, 1, 0, 1, 0, 1))
res_learn_2 &lt;- struct_em(gmdbn_1, data_2, arcs_cand = arcs_cand_2,
                         col_seq = "DATE", verbose = TRUE, max_comp = 3)

</code></pre>

<hr>
<h2 id='struct_learn'>Learn the structure and the parameters of a Gaussian mixture graphical model</h2><span id='topic+struct_learn'></span>

<h3>Description</h3>

<p>This function learns the (graphical and mixture) structure and the parameters
of a Gaussian mixture graphical model. Using the local decomposability of the
scoring function, this task consists in learning each local conditional model
independently with the stepwise algorithm (Koller and Friedman, 2009). Note
that some candidate arcs may be discarded to avoid that the global graphical
structure contains cycles. To limit recourse to this action, the learning
process is performed sequentially. The more arcs of a local model are likely
to be part of cycles (considering the worst case where all the candidate arcs
are selected), the later this local model is processed. By gradually taking
into account the local structures learned over time, the number of possible
cycles decreases and, with it, the number of candidate arcs to discard.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>struct_learn(
  gmgm,
  data,
  nodes = structure(gmgm)$nodes,
  arcs_cand = tibble(lag = 0),
  col_seq = NULL,
  score = "bic",
  verbose = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="struct_learn_+3A_gmgm">gmgm</code></td>
<td>
<p>An initial object of class <code>gmbn</code> or <code>gmdbn</code>.</p>
</td></tr>
<tr><td><code id="struct_learn_+3A_data">data</code></td>
<td>
<p>A data frame containing the data used for learning. Its columns
must explicitly be named after the nodes of <code>gmgm</code> and must not contain
missing values.</p>
</td></tr>
<tr><td><code id="struct_learn_+3A_nodes">nodes</code></td>
<td>
<p>A character vector containing the nodes whose local conditional
models are learned (by default all the nodes of <code>gmgm</code>). If <code>gmgm</code>
is a <code>gmdbn</code> object, the same nodes are learned for each of its
<code>gmbn</code> elements. This constraint can be overcome by passing a list of
character vectors named after some of these elements (<code>b_1</code>, ...) and
containing learned nodes specific to them.</p>
</td></tr>
<tr><td><code id="struct_learn_+3A_arcs_cand">arcs_cand</code></td>
<td>
<p>A data frame containing the candidate arcs for addition or
removal (by default all possible non-temporal arcs). The column <code>from</code>
describes the start node, the column <code>to</code> the end node and the column
<code>lag</code> the time lag between them. Missing values in <code>from</code> or
<code>to</code> are interpreted as &quot;all possible nodes&quot;, which allows to quickly
define large set of arcs that share common attributes. Missing values in
<code>lag</code> are replaced by 0. If <code>gmgm</code> is a <code>gmdbn</code> object, the
same candidate arcs are used for each of its <code>gmbn</code> elements. This
constraint can be overcome by passing a list of data frames named after some
of these elements (<code>b_1</code>, ...) and containing candidate arcs specific
to them. If arcs already in <code>gmgm</code> are not candidates, they cannot be
removed. Therefore, setting <code>arcs_cand</code> to <code>NULL</code> is equivalent to
learning only the mixture structure (and the parameters) of the model.</p>
</td></tr>
<tr><td><code id="struct_learn_+3A_col_seq">col_seq</code></td>
<td>
<p>A character vector containing the column names of <code>data</code>
that describe the observation sequence. If <code>NULL</code> (the default), all the
observations belong to a single sequence. If <code>gmgm</code> is a temporal
<code>gmbn</code> or <code>gmdbn</code> object, the observations of a same sequence must
be ordered such that the <code class="reqn">t</code>th one is related to time slice <code class="reqn">t</code> (note
that the sequences can have different lengths). If <code>gmgm</code> is a
non-temporal <code>gmbn</code> object, this argument is ignored.</p>
</td></tr>
<tr><td><code id="struct_learn_+3A_score">score</code></td>
<td>
<p>A character string (<code>"aic"</code>, <code>"bic"</code> or
<code>"loglik"</code>) corresponding to the scoring function.</p>
</td></tr>
<tr><td><code id="struct_learn_+3A_verbose">verbose</code></td>
<td>
<p>A logical value indicating whether learned nodes in progress
are displayed.</p>
</td></tr>
<tr><td><code id="struct_learn_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to function <code><a href="#topic+stepwise">stepwise</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with elements:
</p>
<table role = "presentation">
<tr><td><code>gmgm</code></td>
<td>
<p>The final <code>gmbn</code> or <code>gmdbn</code> object.</p>
</td></tr>
<tr><td><code>evol_score</code></td>
<td>
<p>A list with elements:
</p>

<dl>
<dt><code>global</code></dt><dd><p>A numeric vector containing the global score before and
after learning.</p>
</dd>
<dt><code>local</code></dt><dd><p>For a <code>gmbn</code> object, a numeric matrix containing the
local conditional scores before and after learning. For a <code>gmdbn</code>
object, a list of numeric matrices containing these values for each
<code>gmbn</code> element.</p>
</dd>
</dl>
</td></tr>
</table>


<h3>References</h3>

<p>Koller, D. and Friedman, N. (2009). <em>Probabilistic Graphical Models:
Principles and Techniques</em>. The MIT Press.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+param_em">param_em</a></code>, <code><a href="#topic+param_learn">param_learn</a></code>,
<code><a href="#topic+struct_em">struct_em</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(data_body)
gmbn_1 &lt;- add_nodes(NULL,
                    c("AGE", "FAT", "GENDER", "GLYCO", "HEIGHT", "WAIST",
                      "WEIGHT"))
arcs_cand_1 &lt;- data.frame(from = c("AGE", "GENDER", "HEIGHT", "WEIGHT", NA,
                                   "AGE", "GENDER", "AGE", "FAT", "GENDER",
                                   "HEIGHT", "WEIGHT", "AGE", "GENDER",
                                   "HEIGHT"),
                          to = c("FAT", "FAT", "FAT", "FAT", "GLYCO", "HEIGHT",
                                 "HEIGHT", "WAIST", "WAIST", "WAIST", "WAIST",
                                 "WAIST", "WEIGHT", "WEIGHT", "WEIGHT"))
res_learn_1 &lt;- struct_learn(gmbn_1, data_body, arcs_cand = arcs_cand_1,
                            verbose = TRUE, max_comp = 3)

data(data_air)
gmdbn_1 &lt;- gmdbn(b_2 = add_nodes(NULL, c("NO2", "O3", "TEMP", "WIND")),
                 b_13 = add_nodes(NULL, c("NO2", "O3", "TEMP", "WIND")))
arcs_cand_2 &lt;- data.frame(from = c("NO2", "NO2", "NO2", "O3", "TEMP", "TEMP",
                                   "WIND", "WIND"),
                          to = c("NO2", "O3", "O3", "O3", NA, NA, NA, NA),
                          lag = c(1, 0, 1, 1, 0, 1, 0, 1))
res_learn_2 &lt;- struct_learn(gmdbn_1, data_air, arcs_cand = arcs_cand_2,
                            col_seq = "DATE", verbose = TRUE, max_comp = 3)

</code></pre>

<hr>
<h2 id='structure'>Provide the graphical structure of a Gaussian mixture graphical model</h2><span id='topic+structure'></span>

<h3>Description</h3>

<p>This function provides the graphical structure of a Gaussian mixture
graphical model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>structure(gmgm)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="structure_+3A_gmgm">gmgm</code></td>
<td>
<p>An object of class <code>gmbn</code> or <code>gmdbn</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with elements:
</p>
<table role = "presentation">
<tr><td><code>nodes</code></td>
<td>
<p>A character vector containing the nodes.</p>
</td></tr>
<tr><td><code>arcs</code></td>
<td>
<p>For a <code>gmbn</code> object, a data frame (tibble) containing the
arcs. For a <code>gmdbn</code> object, a list of data frames (tibbles) containing
the arcs of each <code>gmbn</code> element.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>data(gmbn_body)
struct_1 &lt;- structure(gmbn_body)

data(gmdbn_air)
struct_2 &lt;- structure(gmdbn_air)

</code></pre>

<hr>
<h2 id='summary'>Summarize a Gaussian mixture model or graphical model</h2><span id='topic+summary'></span><span id='topic+summary.gmm'></span><span id='topic+summary.gmbn'></span><span id='topic+summary.gmdbn'></span>

<h3>Description</h3>

<p>This function summarizes a Gaussian mixture model or graphical model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'gmm'
summary(object, ...)

## S3 method for class 'gmbn'
summary(object, ...)

## S3 method for class 'gmdbn'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="summary_+3A_object">object</code></td>
<td>
<p>An object of class <code>gmm</code>, <code>gmbn</code> or <code>gmdbn</code>.</p>
</td></tr>
<tr><td><code id="summary_+3A_...">...</code></td>
<td>
<p>Unused arguments from the generic function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>If <code>object</code> is a <code>gmm</code> object, an integer vector containing
the number of variables, mixture components and free parameters.
</p>
<p>If <code>object</code> is a <code>gmbn</code> or <code>gmdbn</code> object, a list with
elements:
</p>
<table role = "presentation">
<tr><td><code>global</code></td>
<td>
<p>An integer vector containing the global number of nodes, arcs,
mixture components and free parameters (for a <code>gmdbn</code> object, also the
number of <code>gmbn</code> elements).</p>
</td></tr>
<tr><td><code>local</code></td>
<td>
<p>For a <code>gmbn</code> object, an integer matrix containing the local
numbers of arcs, mixture components and free parameters. For a <code>gmdbn</code>
object, a list of integer matrices containing these statistics for each
<code>gmbn</code> elements.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>data(gmm_body)
summ_1 &lt;- summary(gmm_body)

data(gmbn_body)
summ_2 &lt;- summary(gmbn_body)

data(gmdbn_air)
summ_3 &lt;- summary(gmdbn_air)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
