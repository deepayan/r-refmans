<!DOCTYPE html><html lang="en"><head><title>Help for package sensitivity</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {sensitivity}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#sensitivity-package'><p>Sensitivity Analysis</p></a></li>
<li><a href='#addelman_const'><p>Addelman and Kempthorne construction</p></a></li>
<li><a href='#correlRatio'><p>Correlation Ratio</p></a></li>
<li><a href='#decoupling'><p>Decoupling Simulations and Estimations</p></a></li>
<li><a href='#delsa'><p>Distributed Evaluation of Local Sensitivity Analysis</p></a></li>
<li><a href='#discrepancyCriteria_cplus'><p>Discrepancy measure</p></a></li>
<li><a href='#EPtest'><p>Non-parametric variable significance test based on the empirical process</p></a></li>
<li><a href='#fast99'><p>Extended Fourier Amplitude Sensitivity Test</p></a></li>
<li><a href='#johnson'><p>Johnson indices</p></a></li>
<li><a href='#johnsonshap'><p>Johnson-Shapley indices</p></a></li>
<li><a href='#lmg'><p>LMG <code class="reqn">R^2</code> decomposition for linear and logistic regression models</p></a></li>
<li><a href='#maximin_cplus'><p>Maximin criterion</p></a></li>
<li><a href='#morris'><p>Morris's Elementary Effects Screening Method</p></a></li>
<li><a href='#morrisMultOut'><p>Morris's Elementary Effects Screening Method for Multidimensional Outputs</p></a></li>
<li><a href='#parameterSets'>
<p>Generate parameter sets</p></a></li>
<li><a href='#pcc'><p>Partial Correlation Coefficients</p></a></li>
<li><a href='#PLI'><p>Perturbed-Law based sensitivity Indices (PLI) for failure probability</p></a></li>
<li><a href='#PLIquantile'><p>Perturbed-Law based sensitivity Indices (PLI) for quantile</p></a></li>
<li><a href='#PLIquantile_multivar'>
<p>Perturbed-Law based sensitivity Indices (PLI) for quantile</p>
and simultaneous perturbations of 2 inputs</a></li>
<li><a href='#PLIsuperquantile'><p>Perturbed-Law based sensitivity Indices (PLI) for superquantile</p></a></li>
<li><a href='#PLIsuperquantile_multivar'>
<p>Perturbed-Law based sensitivity Indices (PLI) for superquantile</p>
and simultaneous perturbations of 2 inputs</a></li>
<li><a href='#plot.support'>
<p>Support index functions: Measuring the effect of input variables over their support</p></a></li>
<li><a href='#pme_knn'><p>Data-given proportional marginal effects estimation via nearest-neighbors procedure</p></a></li>
<li><a href='#pmvd'><p>Proportional Marginal Variance Decomposition indices</p>
for linear and logistic models</a></li>
<li><a href='#PoincareChaosSqCoef'><p>Squared coefficients computation in generalized chaos</p></a></li>
<li><a href='#PoincareConstant'><p>Poincare constants for Derivative-based Global Sensitivity Measures (DGSM)</p></a></li>
<li><a href='#PoincareOptimal'><p>Optimal Poincare constants for Derivative-based Global Sensitivity Measures (DGSM)</p></a></li>
<li><a href='#qosa'><p>Quantile-oriented sensitivity analysis</p></a></li>
<li><a href='#sb'><p>Sequential Bifurcations</p></a></li>
<li><a href='#sensiFdiv'><p>Sensitivity Indices based on Csiszar f-divergence</p></a></li>
<li><a href='#sensiHSIC'><p>Sensitivity Indices based on the Hilbert-Schmidt Independence Criterion (HSIC)</p></a></li>
<li><a href='#shapleyBlockEstimation'><p>Computation of the Shapley effects in the Gaussian linear framework</p>
with an unknown block-diagonal covariance matrix</a></li>
<li><a href='#shapleyLinearGaussian'><p>Computation of the Shapley effects in the linear Gaussian framework</p></a></li>
<li><a href='#shapleyPermEx'><p>Estimation of Shapley effects by examining all permutations of inputs</p>
(Agorithm of Song et al, 2016), in cases of independent or dependent inputs</a></li>
<li><a href='#shapleyPermRand'><p>Estimation of Shapley effects by random permutations of inputs</p>
(Agorithm of Song et al, 2016), in cases of independent or dependent inputs</a></li>
<li><a href='#shapleysobol_knn'><p>Data given Shapley effects estimation via nearest-neighbors procedure</p></a></li>
<li><a href='#shapleySubsetMc'><p>Estimation of Shapley effects from data using nearest neighbors method</p></a></li>
<li><a href='#sobol'><p>Monte Carlo Estimation of Sobol' Indices</p></a></li>
<li><a href='#sobol2002'><p>Monte Carlo Estimation of Sobol' Indices (scheme by Saltelli 2002)</p></a></li>
<li><a href='#sobol2007'><p>Monte Carlo Estimation of Sobol' Indices (improved formulas of Mauntz: Sobol et al. (2007) and Saltelli et al. (2010))</p></a></li>
<li><a href='#sobolEff'><p>Monte Carlo Estimation of Sobol' Indices (formulas of Janon-Monod)</p></a></li>
<li><a href='#sobolGP'><p>Kriging-based sensitivity analysis</p></a></li>
<li><a href='#soboljansen'><p>Monte Carlo Estimation of Sobol' Indices (improved formulas of Jansen (1999) and Saltelli et al. (2010))</p></a></li>
<li><a href='#sobolmara'><p>Monte Carlo Estimation of Sobol' Indices via matrix permutations</p></a></li>
<li><a href='#sobolmartinez'><p>Monte Carlo Estimation of Sobol' Indices (formulas of Martinez (2011))</p></a></li>
<li><a href='#sobolMultOut'><p>Monte Carlo Estimation of Aggregated Sobol' Indices for multiple and functional outputs</p></a></li>
<li><a href='#sobolowen'><p>Monte Carlo Estimation of Sobol' Indices (improved formulas of Owen (2013)</p></a></li>
<li><a href='#sobolrank'><p>First-order sensitivity indices estimation via ranking</p></a></li>
<li><a href='#sobolrec'><p>Recursive estimation of Sobol' indices</p></a></li>
<li><a href='#sobolrep'><p>Sobol' indices estimation based on replicated orthogonal arrays</p></a></li>
<li><a href='#sobolroalhs'><p>Sobol' Indices Estimation Using Replicated OA-based LHS</p></a></li>
<li><a href='#sobolroauc'><p>Sobol' Indices estimation under inequality constraints</p></a></li>
<li><a href='#sobolSalt'><p>Monte Carlo Estimation of Sobol' Indices based on Saltelli schemes</p></a></li>
<li><a href='#sobolshap_knn'><p>Flexible sensitivity analysis via ranking / nearest neighbours</p></a></li>
<li><a href='#sobolSmthSpl'><p>Estimation of Sobol' First Order Indices with B-spline Smoothing</p></a></li>
<li><a href='#sobolTIIlo'><p>Liu and Owen Estimation of Total Interaction Indices</p></a></li>
<li><a href='#sobolTIIpf'><p>Pick-freeze Estimation of Total Interaction Indices</p></a></li>
<li><a href='#soboltouati'><p>Monte Carlo Estimation of Sobol' Indices (formulas of Martinez (2011) and Touati (2016))</p></a></li>
<li><a href='#squaredIntEstim'>
<p>Squared integral estimate</p></a></li>
<li><a href='#src'><p>Standardized Regression Coefficients</p></a></li>
<li><a href='#support'>
<p>Support index functions: Measuring the effect of input variables over their support</p></a></li>
<li><a href='#template.replace'><p>Replace Values in a Template Text</p></a></li>
<li><a href='#testHSIC'><p>Tests of Independence based on the Hilbert-Schmidt Independence Criterion (HSIC)</p></a></li>
<li><a href='#testmodels'><p>Test Models for Sensitivity Analysis</p></a></li>
<li><a href='#truncateddistrib'><p>Truncated distributions</p></a></li>
<li><a href='#weightTSA'>
<p>Weight-function to transform an output variable in order to perform Target Sensitivity Analysis (TSA)</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Version:</td>
<td>1.30.1</td>
</tr>
<tr>
<td>Title:</td>
<td>Global Sensitivity Analysis of Model Outputs and Importance
Measures</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Bertrand Iooss &lt;biooss@yahoo.fr&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.0.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>boot, numbers, methods, ggplot2, Rcpp, foreach, dtwclust</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo</td>
</tr>
<tr>
<td>Suggests:</td>
<td>condMVNorm, DiceDesign, DiceKriging, doParallel, evd,
ggExtra, grid, gplots, gtools, igraph, IncDTW, ks, lattice,
MASS, mc2d, mvtnorm, parallel, plotrix, pracma, proxy,
randtoolbox, RANN, reshape2, rgl, stringr, triangle, TSP,
viridisLite, whitening</td>
</tr>
<tr>
<td>Description:</td>
<td>A collection of functions for sensitivity analysis of model outputs (factor screening, global sensitivity analysis and robustness analysis), for variable importance measures of data, as well as for interpretability of machine learning models. Most of the functions have to be applied on scalar output, but several functions support multi-dimensional outputs.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-08-28 10:45:33 UTC; Bertrand</td>
</tr>
<tr>
<td>Author:</td>
<td>Bertrand Iooss [aut, cre],
  Sebastien Da Veiga [aut],
  Alexandre Janon [aut],
  Gilles Pujol [aut]</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-08-28 13:40:13 UTC</td>
</tr>
</table>
<hr>
<h2 id='sensitivity-package'>Sensitivity Analysis</h2><span id='topic+sensitivity-package'></span><span id='topic+sensitivity'></span>

<h3>Description</h3>

<p>Methods and functions for global sensitivity analysis of model outputs, importance measures and machine learning model interpretability</p>


<h3>Details</h3>

<p>The <span class="pkg">sensitivity</span> package implements some global sensitivity analysis
methods and importance measures:
</p>

<ul>
<li><p> Linear regression importance measures in regression or classification (logistic regression) contexts (Iooss et al., 2022; Clouvel et al., 2024):
</p>

<ul>
<li><p> SRC and SRRC (<code><a href="#topic+src">src</a></code>), and correlation ratio (<code><a href="#topic+correlRatio">correlRatio</a></code>)
</p>
</li>
<li><p> PCC, SPCC, PRCC and SPRCC (<code><a href="#topic+pcc">pcc</a></code>),
</p>
</li>
<li><p> LMG and LMG on ranks (<code><a href="#topic+lmg">lmg</a></code>),
</p>
</li>
<li><p> PMVD and PMVD on ranks (<code><a href="#topic+pmvd">pmvd</a></code>),
</p>
</li>
<li><p> Johnson indices (<code><a href="#topic+johnson">johnson</a></code>);
</p>
</li></ul>

</li>
<li><p> Bettonvil's sequential bifurcations (Bettonvil and Kleijnen, 1996) (<code><a href="#topic+sb">sb</a></code>);
</p>
</li>
<li><p> Morris's &quot;OAT&quot; elementary effects screening method (<code><a href="#topic+morris">morris</a></code>);
</p>
</li>
<li><p> Derivative-based Global Sensitivity Measures:
</p>

<ul>
<li><p> Poincare constants for Derivative-based Global Sensitivity Measures (DGSM) (Lamboni et al., 2013; Roustant et al., 2017) (<code><a href="#topic+PoincareConstant">PoincareConstant</a></code>) and (<code><a href="#topic+PoincareOptimal">PoincareOptimal</a></code>),
</p>
</li>
<li><p> Squared coefficients computation in generalized chaos via Poincare differential operators (Roustant et al., 2019) (<code><a href="#topic+PoincareChaosSqCoef">PoincareChaosSqCoef</a></code>),
</p>
</li>
<li><p> Distributed Evaluation of Local Sensitivity Analysis (DELSA) (Rakovec et al., 2014) (<code><a href="#topic+delsa">delsa</a></code>);
</p>
</li></ul>

</li>
<li><p> Variance-based sensitivity indices (Sobol' indices) for independent inputs:
</p>

<ul>
<li><p> Estimation of the Sobol' first order indices with with B-spline Smoothing (Ratto and Pagano, 2010) (<code><a href="#topic+sobolSmthSpl">sobolSmthSpl</a></code>),
</p>
</li>
<li><p> Monte Carlo estimation of Sobol' indices with independent inputs (also called pick-freeze method): 
</p>

<ul>
<li><p> Sobol' scheme (Sobol, 1993) to compute the indices given by the variance decomposition up to a specified order (<code><a href="#topic+sobol">sobol</a></code>),
</p>
</li>
<li><p> Saltelli's scheme (Saltelli, 2002) to compute first order, second order and total indices  (<code><a href="#topic+sobolSalt">sobolSalt</a></code>), 
</p>
</li>
<li><p> Saltelli's scheme (Saltelli, 2002) to compute first order and total indices (<code><a href="#topic+sobol2002">sobol2002</a></code>), 
</p>
</li>
<li><p> Mauntz-Kucherenko's scheme (Sobol et al., 2007) to compute first order and total indices using improved formulas for small indices (<code><a href="#topic+sobol2007">sobol2007</a></code>),
</p>
</li>
<li><p> Jansen-Sobol's scheme (Jansen, 1999) to compute first order and total indices using improved formulas (<code><a href="#topic+soboljansen">soboljansen</a></code>),
</p>
</li>
<li><p> Martinez's scheme using correlation coefficient-based formulas (Martinez, 2011; Touati, 2016) to compute first order and total indices, associated with theoretical confidence intervals (<code><a href="#topic+sobolmartinez">sobolmartinez</a></code> and <code><a href="#topic+soboltouati">soboltouati</a></code>), 
</p>
</li>
<li><p> Janon-Monod's scheme (Monod et al., 2006; Janon et al., 2013) to compute first order indices with optimal asymptotic variance (<code><a href="#topic+sobolEff">sobolEff</a></code>),
</p>
</li>
<li><p> Mara's scheme (Mara and Joseph, 2008) to compute first order indices with a cost independent of the dimension, via permutations on a single matrix (<code><a href="#topic+sobolmara">sobolmara</a></code>),
</p>
</li>
<li><p> Mighty estimator of first-order sensitivity indices based on rank statistics (correlation coefficient of Chatterjee, 2019; Gamboa et al., 2020) (<code><a href="#topic+sobolrank">sobolrank</a></code>),
</p>
</li>
<li><p> Owen's scheme (Owen, 2013) to compute first order and total indices using improved formulas (via 3 input independent matrices) for small indices (<code><a href="#topic+sobolowen">sobolowen</a></code>),
</p>
</li>
<li><p> Total Interaction Indices using Liu-Owen's scheme (Liu and Owen, 2006) (<code><a href="#topic+sobolTIIlo">sobolTIIlo</a></code>) and pick-freeze scheme (Fruth et al., 2014) (<code><a href="#topic+sobolTIIpf">sobolTIIpf</a></code>),
</p>
</li></ul>

</li>
<li><p> Replication-based procedures:
</p>

<ul>
<li><p> Estimation of the Sobol' first order and closed second order indices using replicated orthogonal array-based Latin hypecube sample (Tissot and Prieur, 2015) (<code><a href="#topic+sobolroalhs">sobolroalhs</a></code>),
</p>
</li>
<li><p> Recursive estimation of the Sobol' first order and closed second order indices using replicated orthogonal array-based Latin hypecube sample (Gilquin et al., 2016) (<code><a href="#topic+sobolrec">sobolrec</a></code>),
</p>
</li>
<li><p> Estimation of the Sobol' first order, second order and total indices using the generalized method with replicated orthogonal array-based Latin hypecube sample (Tissot and Prieur, 2015) (<code><a href="#topic+sobolrep">sobolrep</a></code>),
</p>
</li>
<li><p> Sobol' indices estimation under inequality constraints (Gilquin et al., 2015) by extension of the replication procedure (Tissot and Prieur, 2015) (<code><a href="#topic+sobolroauc">sobolroauc</a></code>),
</p>
</li></ul>

</li>
<li><p> Estimation of the Sobol' first order and total indices with Saltelli's so-called &quot;extended-FAST&quot; method (Saltelli et al., 1999) (<code><a href="#topic+fast99">fast99</a></code>),
</p>
</li>
<li><p> Estimation of the Sobol' first order and total indices with kriging-based global sensitivity analysis (Le Gratiet et al., 2014) (<code><a href="#topic+sobolGP">sobolGP</a></code>);
</p>
</li></ul>

</li>
<li><p> Variance-based sensitivity indices valid for dependent inputs:
</p>

<ul>
<li><p> Exact computation of Shapley effects in the linear Gaussian framework (Broto et al., 2019) (<code><a href="#topic+shapleyLinearGaussian">shapleyLinearGaussian</a></code>),
</p>
</li>
<li><p> Computation of Shapley effects in the Gaussian linear framework with an unknown block-diagonal covariance matrix (Broto et al., 2020) (<code><a href="#topic+shapleyBlockEstimation">shapleyBlockEstimation</a></code>),
</p>
</li>
<li><p> Johnson-Shapley indices (Iooss and Clouvel, 2024) (<code><a href="#topic+johnsonshap">johnsonshap</a></code>),
</p>
</li>
<li><p> Estimation of Shapley effects by examining all permutations of inputs (Song et al., 2016) (<code><a href="#topic+shapleyPermEx">shapleyPermEx</a></code>),
</p>
</li>
<li><p> Estimation of Shapley effects by randomly sampling permutations of inputs (Song et al., 2016) (<code><a href="#topic+shapleyPermRand">shapleyPermRand</a></code>),
</p>
</li>
<li><p> Estimation of Shapley effects from data using nearest neighbors method (Broto et al., 2018) (<code><a href="#topic+shapleySubsetMc">shapleySubsetMc</a></code>),
</p>
</li>
<li><p> Estimation of Shapley effects and all Sobol indices from data using nearest neighbors (Broto et al., 2018) (using a fast approximate algorithm) or ranking (Gamboa et al., 2020) (<code><a href="#topic+shapleysobol_knn">shapleysobol_knn</a></code>) and (<code><a href="#topic+sobolshap_knn">sobolshap_knn</a></code>),
</p>
</li>
<li><p> Estimation of Shapley effects from data using nearest neighbors method (Broto et al., 2018) with an optimized/parallelized computations and bootstrap confidence intervals estimations  (<code><a href="#topic+shapleysobol_knn">shapleysobol_knn</a></code>),
</p>
</li>
<li><p> Estimation of Proportional Marginal Effects (PME) (Herin et al., 2024) (<code><a href="#topic+pme_knn">pme_knn</a></code>);
</p>
</li></ul>

</li>
<li><p> Support index functions (<code><a href="#topic+support">support</a></code>) of Fruth et al. (2016);
</p>
</li>
<li><p> Sensitivity Indices based on Csiszar f-divergence (<code><a href="#topic+sensiFdiv">sensiFdiv</a></code>) (particular cases: Borgonovo's indices and mutual-information based indices) and Hilbert-Schmidt Independence Criterion (<code><a href="#topic+sensiHSIC">sensiHSIC</a></code> and <code><a href="#topic+testHSIC">testHSIC</a></code>) (Da Veiga, 2015; De Lozzo and Marrel, 2016; Meynaoui et al., 2019);
</p>
</li>
<li><p> Non-parametric variable significance test based on the empirical process (<code><a href="#topic+EPtest">EPtest</a></code>) of Klein and Rochet (2022);
</p>
</li>
<li><p> First-order quantile-oriented sensitivity indices as defined in Fort et al. (2016) via a kernel-based estimator related (Maume-Deschamps and Niang, 2018) (<code><a href="#topic+qosa">qosa</a></code>);
</p>
</li>
<li><p> Target Sensitivity Analysis via Hilbert-Schmidt Independence Criterion (<code><a href="#topic+sensiHSIC">sensiHSIC</a></code>) (Spagnol et al., 2019);
</p>
</li>
<li><p> Robustness analysis by the Perturbed-Law based Indices (<code><a href="#topic+PLI">PLI</a></code>) of Lemaitre et al. (2015), (<code><a href="#topic+PLIquantile">PLIquantile</a></code>) of Sueur et al. (2017), (<code><a href="#topic+PLIsuperquantile">PLIsuperquantile</a></code>) of Iooss et al. (2021), and extension as (<code><a href="#topic+PLIquantile_multivar">PLIquantile_multivar</a></code>) and (<code><a href="#topic+PLIsuperquantile_multivar">PLIsuperquantile_multivar</a></code>) ;
</p>
</li>
<li><p> Extensions to multidimensional outputs for:
</p>

<ul>
<li><p> Sobol' indices (<code><a href="#topic+sobolMultOut">sobolMultOut</a></code>): Aggregated Sobol' indices (Lamboni et al., 2011; Gamboa et al., 2014) and functional (1D) Sobol' indices,
</p>
</li>
<li><p> Shapley effects and Sobol' indices (<code><a href="#topic+shapleysobol_knn">shapleysobol_knn</a></code>) and (<code><a href="#topic+sobolshap_knn">sobolshap_knn</a></code>): Functional (1D) indices,
</p>
</li>
<li><p> HSIC indices (<code><a href="#topic+sensiHSIC">sensiHSIC</a></code>) (Da Veiga, 2015): Aggregated HSIC, potentially via a PCA step (Da Veiga, 2015),
</p>
</li>
<li><p> Morris method (<code><a href="#topic+morrisMultOut">morrisMultOut</a></code>).
</p>
</li></ul>

</li></ul>

<p>Moreover, some utilities are provided: standard test-cases (<code><a href="#topic+testmodels">testmodels</a></code>), weight transformation function of the output sample (<code><a href="#topic+weightTSA">weightTSA</a></code>) to perform Target Sensitivity Analysis, normal and Gumbel truncated distributions (<code><a href="#topic+truncateddistrib">truncateddistrib</a></code>), squared integral estimate (<code><a href="#topic+squaredIntEstim">squaredIntEstim</a></code>), Addelman and Kempthorne construction of orthogonal arrays of strength two (<code><a href="#topic+addelman_const">addelman_const</a></code>), discrepancy criteria (<code><a href="#topic+discrepancyCriteria_cplus">discrepancyCriteria_cplus</a></code>), maximin criteria (<code><a href="#topic+maximin_cplus">maximin_cplus</a></code>) and template file generation (<code><a href="#topic+template.replace">template.replace</a></code>).
</p>


<h3>Model managing</h3>

<p>The <span class="pkg">sensitivity</span> package has been designed to work either models written in <span class="rlang"><b>R</b></span>
than external models such as heavy computational codes. This is achieved with
the input argument <code>model</code> present in all functions of this package.
</p>
<p>The argument <code>model</code> is expected to be either a
funtion or a predictor (i.e. an object with a <code>predict</code> function such as
<code>lm</code>).
</p>

<ul>
<li><p> If <code>model = m</code> where <code>m</code> is a function, it will be invoked
once by <code>y &lt;- m(X)</code>.
</p>
</li>
<li><p> If <code>model = m</code> where <code>m</code> is a predictor, it will be invoked
once by <code>y &lt;- predict(m, X)</code>.
</p>
</li></ul>

<p><code>X</code> is the design of experiments, i.e. a <code>data.frame</code> with
<code>p</code> columns (the input factors) and <code>n</code> lines (each, an
experiment), and <code>y</code> is the vector of length <code>n</code> of the
model responses.
</p>
<p>The model in invoked once for the whole design of experiment.
</p>
<p>The argument <code>model</code> can be left to <code>NULL</code>. This is refered to as 
the decoupled approach and used with external computational codes that rarely
run on the statistician's computer. See <code><a href="#topic+decoupling">decoupling</a></code>.
</p>


<h3>Author(s)</h3>

<p>Bertrand Iooss, Sebastien Da Veiga, Alexandre Janon and Gilles Pujol with contributions from Paul Lemaitre for <code><a href="#topic+PLI">PLI</a></code>, Thibault Delage and Roman Sueur for <code><a href="#topic+PLIquantile">PLIquantile</a></code>, Vanessa Verges for <code><a href="#topic+PLIquantile">PLIquantile</a></code>, <code><a href="#topic+PLIsuperquantile">PLIsuperquantile</a></code>, <code><a href="#topic+PLIquantile_multivar">PLIquantile_multivar</a></code> and <code><a href="#topic+PLIsuperquantile_multivar">PLIsuperquantile_multivar</a></code>, Laurent Gilquin for <code><a href="#topic+sobolroalhs">sobolroalhs</a></code>, <code><a href="#topic+sobolroauc">sobolroauc</a></code>, <code><a href="#topic+sobolSalt">sobolSalt</a></code>, <code><a href="#topic+sobolrep">sobolrep</a></code>, <code><a href="#topic+sobolrec">sobolrec</a></code>, as well as <code><a href="#topic+addelman_const">addelman_const</a></code>, <code><a href="#topic+discrepancyCriteria_cplus">discrepancyCriteria_cplus</a></code> and <code><a href="#topic+maximin_cplus">maximin_cplus</a></code>, Loic le Gratiet for <code><a href="#topic+sobolGP">sobolGP</a></code>, Khalid Boumhaout, Taieb Touati and Bernardo Ramos for <code><a href="#topic+sobolowen">sobolowen</a></code> and <code><a href="#topic+soboltouati">soboltouati</a></code>, Jana Fruth for <code><a href="#topic+PoincareConstant">PoincareConstant</a></code>, <code><a href="#topic+sobolTIIlo">sobolTIIlo</a></code> and <code><a href="#topic+sobolTIIpf">sobolTIIpf</a></code>, Gabriel Sarazin, Amandine Marrel, Anouar Meynaoui and Reda El Amri for their contributions to <code><a href="#topic+sensiHSIC">sensiHSIC</a></code> and <code><a href="#topic+testHSIC">testHSIC</a></code>, Joseph Guillaume and Oldrich Rakovec for <code><a href="#topic+delsa">delsa</a></code> and <code><a href="#topic+parameterSets">parameterSets</a></code>, Olivier Roustant for <code><a href="#topic+PoincareOptimal">PoincareOptimal</a></code>, <code><a href="#topic+PoincareChaosSqCoef">PoincareChaosSqCoef</a></code>, <code><a href="#topic+squaredIntEstim">squaredIntEstim</a></code> and <code><a href="#topic+support">support</a></code>, Eunhye Song, Barry L. Nelson and Jeremy Staum for <code><a href="#topic+shapleyPermEx">shapleyPermEx</a></code> and <code><a href="#topic+shapleyPermRand">shapleyPermRand</a></code>, Baptiste Broto for <code><a href="#topic+shapleySubsetMc">shapleySubsetMc</a></code>, <code><a href="#topic+shapleyLinearGaussian">shapleyLinearGaussian</a></code> and <code><a href="#topic+shapleyBlockEstimation">shapleyBlockEstimation</a></code>, Filippo Monari for (<code><a href="#topic+sobolSmthSpl">sobolSmthSpl</a></code>) and (<code><a href="#topic+morrisMultOut">morrisMultOut</a></code>), Marouane Il Idrissi for <code><a href="#topic+lmg">lmg</a></code>, <code><a href="#topic+pmvd">pmvd</a></code> and <code><a href="#topic+shapleysobol_knn">shapleysobol_knn</a></code>, associated to Margot Herin for <code><a href="#topic+pme_knn">pme_knn</a></code>, Laura Clouvel for <code><a href="#topic+johnson">johnson</a></code>, Paul Rochet for <code><a href="#topic+EPtest">EPtest</a></code>, Frank Weber and Roelof Oomen for other contributions.
</p>
<p>(maintainer: Bertrand Iooss <a href="mailto:biooss@yahoo.fr">biooss@yahoo.fr</a>)</p>


<h3>References</h3>

<p>S. Da Veiga, F. Gamboa, B. Iooss and C. Prieur, <em>Basics and trends in sensitivity analysis, Theory and practice in R</em>, SIAM, 2021.
</p>
<p>R. Faivre, B. Iooss, S. Mahevas, D. Makowski, H. Monod, editors, 2013, <em>Analyse de sensibilite et exploration de modeles. Applications aux modeles environnementaux</em>, Editions Quae.
</p>
<p>L. Clouvel, B. Iooss, V. Chabridon, M. Il Idrissi and F. Robin, 2023, <em>An overview of variance-based importance measures in the linear regression context: comparative analyses and numerical tests</em>, Preprint. <a href="https://hal.science/hal-04102053">https://hal.science/hal-04102053</a>
</p>
<p>B. Iooss, V. Chabridon and V. Thouvenot, <em>Variance-based importance measures for machine learning model interpretability</em>, Congres lambda-mu23, Saclay, France, 10-13 octobre 2022. <a href="https://hal.science/hal-03741384">https://hal.science/hal-03741384</a>
</p>
<p>B. Iooss, R. Kennet and P. Secchi, 2022, <em>Different views of interpretability</em>, In: <em>Interpretability for Industry 4.0: Statistical and Machine Learning Approaches</em>, A. Lepore, B. Palumbo and J-M. Poggi (Eds), Springer.
</p>
<p>B. Iooss and A. Saltelli, 2017, <em>Introduction: Sensitivity analysis.</em> In: <em>Springer Handbook on Uncertainty Quantification</em>, R. Ghanem, D. Higdon and H. Owhadi (Eds), Springer.
</p>
<p>A. Saltelli, K. Chan and E. M. Scott eds, 2000, <em>Sensitivity Analysis</em>, Wiley.
</p>

<hr>
<h2 id='addelman_const'>Addelman and Kempthorne construction</h2><span id='topic+addelman_const'></span>

<h3>Description</h3>

<p><code>addelman_const</code> implements the Addelman and Kempthorne construction of orthogonal arrays of strength two. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>addelman_const(dimension, levels, choice="U")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="addelman_const_+3A_dimension">dimension</code></td>
<td>
<p>The number of columns of the orthogonal array.</p>
</td></tr>
<tr><td><code id="addelman_const_+3A_levels">levels</code></td>
<td>
<p>The number of levels of the orthogonal array. Either a prime number or a prime power number.</p>
</td></tr>
<tr><td><code id="addelman_const_+3A_choice">choice</code></td>
<td>
<p>A character from the list (&quot;U&quot;,&quot;V&quot;,&quot;W&quot;,&quot;X&quot;) specifying which orthogonal array to construct (see &quot;Details&quot;).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The method of Addelman and Kempthorne allows to construct up to four orthogonal arrays. <code>choice</code> specify which orthogonal array is to be constructed. Note that the four orthognal arrays depends on each others through linear equations.
</p>


<h3>Value</h3>

<p>A matrix corresponding to the orthogonal array constructed.
</p>


<h3>Author(s)</h3>

<p>Laurent Gilquin
</p>


<h3>References</h3>

<p>A.S. Hedayat, N.J.A. Sloane and J. Stufken, 1999, <em>Orthogonal Arrays: Theory and Applications</em>, Springer Series in Statistics.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dimension &lt;- 6
levels &lt;- 7
OA &lt;- addelman_const(dimension,levels,choice="U")
</code></pre>

<hr>
<h2 id='correlRatio'>Correlation Ratio</h2><span id='topic+correlRatio'></span>

<h3>Description</h3>

<p><code>correlRatio</code> computes the correlation ratio between
a quantitative variable and a qualitative variable
</p>


<h3>Usage</h3>

<pre><code class='language-R'>correlRatio(X, y)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="correlRatio_+3A_x">X</code></td>
<td>
<p>a vector containing the quantitative variable.</p>
</td></tr>
<tr><td><code id="correlRatio_+3A_y">y</code></td>
<td>
<p>a vector containing the qualitative variable (e.g. a factor).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The value of the correlation ratio
</p>


<h3>Author(s)</h3>

<p>Bertrand Iooss
</p>


<h3>References</h3>

<p>L. Clouvel, B. Iooss, V. Chabridon, M. Il Idrissi and F. Robin, 2024,
<em>An overview of variance-based importance measures in the linear regression context: 
comparative analyses and numerical tests</em>, Preprint.
<a href="https://hal.science/hal-04102053">https://hal.science/hal-04102053</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- runif(100)
y &lt;- round(x)
correlRatio(x,y)
</code></pre>

<hr>
<h2 id='decoupling'>Decoupling Simulations and Estimations</h2><span id='topic+decoupling'></span><span id='topic+tell'></span><span id='topic+ask'></span><span id='topic+extract'></span>

<h3>Description</h3>

<p><code>tell</code> and <code>ask</code> are S3 generic methods for decoupling
simulations and sensitivity measures estimations. In general, they are
not used by the end-user for a simple <span class="rlang"><b>R</b></span> model, but rather for an
external computational code. Most of the sensitivity analyses objects
of this package overload <code>tell</code>, whereas <code>ask</code> is overloaded
for iterative methods only.
<code>extract</code> is used as a post-treatment of a <code>sobolshap_knn</code> object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tell(x, y = NULL, ...)
ask(x, ...)
extract(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="decoupling_+3A_x">x</code></td>
<td>
<p>a typed list storing the state of the sensitivity study
(parameters, data, estimates), as returned by sensitivity analyses
objects constructors, such as <code><a href="#topic+src">src</a></code>, <code><a href="#topic+morris">morris</a></code>,
etc.</p>
</td></tr>
<tr><td><code id="decoupling_+3A_y">y</code></td>
<td>
<p>a vector of model responses.</p>
</td></tr>
<tr><td><code id="decoupling_+3A_...">...</code></td>
<td>
<p>additional arguments, depending on the method used.</p>
</td></tr>
</table>


<h3>Details</h3>

 
<p>When a sensitivity analysis method is called with no model
(i.e. argument <code>model = NULL</code>), it generates an incomplete object
<code>x</code> that stores the design of experiments (field <code>X</code>),
allowing the user to launch &quot;by hand&quot; the corresponding
simulations. The method <code>tell</code> allows to pass these simulation
results to the  incomplete object <code>x</code>, thereafter estimating the
sensitivity measures.
</p>
<p>The <code>extract</code> method is useful if in a first step the Shapley effects 
have been computed and thus sensitivity indices for all possible subsets 
are available. The resulting <code>sobolshap_knn</code> object can be 
post-treated by <code>extract</code> to get first-order and total Sobol indices 
very easily.
</p>
<p>When the method is iterative, the data to simulate are not stored in
the sensitivity analysis object <code>x</code>, but generated at each
iteration with the <code>ask</code> method; see for example
<code><a href="#topic+sb">sb</a></code>.
</p>


<h3>Value</h3>

<p><code>tell</code> doesn't return anything. It computes the sensitivity
measures, and stores them in the list <code>x</code>.
<strong>Side effect: <code>tell</code> modifies its argument <code>x</code>.</strong>
</p>
<p><code>ask</code> returns the set of data to simulate.
</p>
<p><code>extract</code> returns an object, from a <code>sobolshap_knn</code> object,
containing first-order and total Sobol indices.
</p>


<h3>Author(s)</h3>

<p>Gilles Pujol and Bertrand Iooss
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Example of use of fast99 with "model = NULL"
x &lt;- fast99(model = NULL, factors = 3, n = 1000,
            q = "qunif", q.arg = list(min = -pi, max = pi))
y &lt;- ishigami.fun(x$X)
tell(x, y)
print(x)
plot(x)
</code></pre>

<hr>
<h2 id='delsa'>Distributed Evaluation of Local Sensitivity Analysis</h2><span id='topic+delsa'></span><span id='topic+tell.delsa'></span><span id='topic+print.delsa'></span><span id='topic+plot.delsa'></span>

<h3>Description</h3>

 <p><code>delsa</code> implements Distributed Evaluation of 
Local Sensitivity Analysis to calculate first order parameter 
sensitivity at multiple locations in parameter space. The locations 
in parameter space can either be obtained by a call to <code><a href="#topic+parameterSets">parameterSets</a></code> 
or by specifying <code>X0</code> directly, in which case the prior variance 
of each parameter <code>varprior</code> also needs to be specified. Via <code>plot</code> 
(which uses functions of the package <code>ggplot2</code> and <code>reshape2</code>), 
the indices can be visualized.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>delsa(model = NULL, perturb=1.01,
  par.ranges, samples, method,
  X0, varprior, varoutput,
  ...)
  
## S3 method for class 'delsa'
tell(x, y = NULL,...)

## S3 method for class 'delsa'
print(x, ...)

## S3 method for class 'delsa'
plot(x, which=1:3, ask = dev.interactive(), ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="delsa_+3A_model">model</code></td>
<td>
<p>a function, or a model with a <code>predict</code> method,
defining the model to analyze.</p>
</td></tr>
<tr><td><code id="delsa_+3A_perturb">perturb</code></td>
<td>
<p>Perturbation used to calculate sensitivity at each evaluation location</p>
</td></tr>
<tr><td><code id="delsa_+3A_par.ranges">par.ranges</code></td>
<td>
<p>A named list of minimum and maximum parameter values</p>
</td></tr>
<tr><td><code id="delsa_+3A_samples">samples</code></td>
<td>
<p>Number of samples to generate. For the <code>"grid"</code> and
<code>"innergrid"</code> method, corresponds to the number of samples for
each parameter, and may be a vector.</p>
</td></tr>
<tr><td><code id="delsa_+3A_method">method</code></td>
<td>
<p>Sampling scheme. See <code><a href="#topic+parameterSets">parameterSets</a></code></p>
</td></tr>
<tr><td><code id="delsa_+3A_x0">X0</code></td>
<td>
<p>Parameter values at which to evaluate sensitivity indices. 
Can be used instead of specifying sampling <code>method</code></p>
</td></tr>
<tr><td><code id="delsa_+3A_varprior">varprior</code></td>
<td>
<p>Prior variance. If <code>X0</code> is specified, <code>varprior</code> 
must also be specified.</p>
</td></tr>
<tr><td><code id="delsa_+3A_varoutput">varoutput</code></td>
<td>
<p>Output variance. If <code>"summation"</code> is specified (default value), 
the ouput variance is computed by summing the first order effects. If <code>"empirical"</code> 
is specified, the ouput variance is estimated frome the output sample.</p>
</td></tr>
<tr><td><code id="delsa_+3A_...">...</code></td>
<td>
<p>any other arguments for <code>model</code> which are passed
unchanged each time it is called.</p>
</td></tr>
<tr><td><code id="delsa_+3A_x">x</code></td>
<td>
<p>a list of class <code>"delsa"</code> storing the state of the
sensitivity study (parameters, data, estimates).</p>
</td></tr>
<tr><td><code id="delsa_+3A_y">y</code></td>
<td>
<p>a vector of model responses.</p>
</td></tr>
<tr><td><code id="delsa_+3A_which">which</code></td>
<td>
<p>if a subset of the plots is required, specify a subset of the numbers 1:3</p>
</td></tr>
<tr><td><code id="delsa_+3A_ask">ask</code></td>
<td>
<p>logical; if TRUE, the user is asked before each plot, see <code><a href="graphics.html#topic+par">par</a>(ask=.)</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>print</code> shows summary of the first order indices across parameter space.
</p>
<p><code>plot</code> shows: (1) the cumulative distribution function of first order 
sensitivity across parameter space, (2) variation of first order sensitivity 
in relation to model response, and (3) sensitivity in relation to parameter value.
</p>


<h3>Value</h3>

<p><code>delsa</code> returns a list of class <code>"delsa"</code>, containing all
the input arguments detailed before, plus the following components:
</p>
<table role = "presentation">
<tr><td><code>call</code></td>
<td>
<p>the matched call.</p>
</td></tr>
<tr><td><code>X</code></td>
<td>
<p>a <code>data.frame</code> containing the design of experiments.</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>a vector of model responses.</p>
</td></tr>
<tr><td><code>delsafirst</code></td>
<td>
<p>the first order indices for each location in <code>X0</code>.</p>
</td></tr>
<tr><td><code>deriv</code></td>
<td>
<p>the values of derivatives for each location in <code>X0</code></p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Conversion for <code>sensitivity</code> package by Joseph Guillaume, 
based on original R code by Oldrich Rakovec.
Addition of the <code>varoutput</code> argument by Bertrand Iooss (2020).
</p>


<h3>References</h3>

<p>Rakovec, O., M. C. Hill, M. P. Clark, A. H. Weerts, A. J. Teuling, R. Uijlenhoet (2014), 
Distributed 
Evaluation of Local Sensitivity Analysis (DELSA), with application to hydrologic models, 
Water Resour. Res., 50, 1-18
</p>


<h3>See Also</h3>

<p><code><a href="#topic+parameterSets">parameterSets</a></code> which is used to generate points, <code><a href="#topic+sensitivity">sensitivity</a></code> 
for other methods in the package
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Test case : the non-monotonic Sobol g-function
# (there are 8 factors, all following the uniform distribution on [0,1])


library(randtoolbox)
x &lt;- delsa(model=sobol.fun,
           par.ranges=replicate(8,c(0,1),simplify=FALSE),
           samples=100,method="sobol")

# Summary of sensitivity indices of each parameter across parameter space
print(x)

library(ggplot2)
library(reshape2)
plot(x)

</code></pre>

<hr>
<h2 id='discrepancyCriteria_cplus'>Discrepancy measure</h2><span id='topic+discrepancyCriteria_cplus'></span>

<h3>Description</h3>

<p>Compute discrepancy criteria. This function uses a C++ implementation of the function <code>discrepancyCriteria</code> from package <span class="pkg">DiceDesign</span>.</p>


<h3>Usage</h3>

<pre><code class='language-R'>discrepancyCriteria_cplus(design,type='all')</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="discrepancyCriteria_cplus_+3A_design">design</code></td>
<td>
<p>a matrix corresponding to the design of experiments.
The discrepancy criteria are computed for a design in the unit cube [0,1]<code class="reqn">^d</code>.
If this condition is not satisfied the design is automatically rescaled.</p>
</td></tr>
<tr><td><code id="discrepancyCriteria_cplus_+3A_type">type</code></td>
<td>
<p>type of discrepancies (single value or vector) to be computed:
</p>

<table>
<tr>
 <td style="text-align: left;">
		<code>'all'</code> </td><td style="text-align: left;"> all type of discrepancies (default) </td>
</tr>
<tr>
 <td style="text-align: left;">
  	<code>'C2'</code> </td><td style="text-align: left;"> centered L2-discrepancy </td>
</tr>
<tr>
 <td style="text-align: left;">
  	<code>'L2'</code> </td><td style="text-align: left;"> L2-discrepancy </td>
</tr>
<tr>
 <td style="text-align: left;">
  	<code>'L2star'</code> </td><td style="text-align: left;"> L2star-discrepancy </td>
</tr>
<tr>
 <td style="text-align: left;">
		<code>'M2'</code> </td><td style="text-align: left;"> modified L2-discrepancy </td>
</tr>
<tr>
 <td style="text-align: left;">
		<code>'S2'</code> </td><td style="text-align: left;"> symmetric L2-discrepancy </td>
</tr>
<tr>
 <td style="text-align: left;">
		<code>'W2'</code> </td><td style="text-align: left;"> wrap-around L2-discrepancy </td>
</tr>
<tr>
 <td style="text-align: left;">
	</td>
</tr>

</table>

</td></tr>
</table>


<h3>Details</h3>

<p>The discrepancy measures how far a given distribution of points deviates
from a perfectly uniform one. Different discrepancies are available.
For example, if we denote by <code class="reqn">Vol(J)</code> the volume of a subset <code class="reqn">J</code> of <code class="reqn">[0; 1]^d</code> and <code class="reqn">A(X; J)</code> the number of points of <code class="reqn">X</code> falling in <code class="reqn">J</code>, the <code class="reqn">L2</code> discrepancy is:
</p>
<p style="text-align: center;"><code class="reqn">D_{L2} (X) = \left[ \int_{[0,1]^{2d}}{} \left( \frac{A(X,J_{a,b})}{n} - Vol (J_{a,b}) \right)^{2} da db \right]^{1/2}</code>
</p>

<p>where <code class="reqn">a = (a_{1}; ... ; a_{d})'</code>, <code class="reqn">b = (b_{1};...; b_{d})'</code> and <code class="reqn">J_{a,b} =
[a_{1}; b_{1}) \times ... \times [a_{d};b_{d})</code>. The other L2-discrepancies are defined according to the same principle with different form from the subset <code class="reqn">J</code>.
Among all the possibilities, discrepancyCriteria_cplus implements only the L2 discrepancies because it can be expressed analytically even for high dimension.
</p>
<p>Centered L2-discrepancy is computed using the analytical expression done by Hickernell (1998). The user will refer to Pleming and Manteufel (2005) to have more details about the wrap around discrepancy.
</p>


<h3>Value</h3>

<p>A list containing the L2-discrepancies of the <code>design</code>.</p>


<h3>Author(s)</h3>

<p>Laurent Gilquin</p>


<h3>References</h3>

<p>Fang K.T, Li R. and Sudjianto A. (2006) Design and Modeling for
Computer Experiments, <em>Chapman &amp; Hall</em>.
</p>
<p>Hickernell F.J. (1998) A generalized discrepancy and quadrature error bound.
<em>Mathematics of Computation</em>, <b>67</b>, 299-322.
</p>
<p>Pleming J.B. and Manteufel R.D. (2005) <em>Replicated Latin Hypercube Sampling</em>,
46th Structures, Structural Dynamics &amp; Materials Conference, 16-21 April 2005, Austin
(Texas) &ndash; AIAA 2005-1819.
</p>


<h3>See Also</h3>

<p>The distance criterion provided by <code><a href="#topic+maximin_cplus">maximin_cplus</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>dimension &lt;- 2
n &lt;- 40
X &lt;- matrix(runif(n*dimension),n,dimension)
discrepancyCriteria_cplus(X)
</code></pre>

<hr>
<h2 id='EPtest'>Non-parametric variable significance test based on the empirical process</h2><span id='topic+EPtest'></span>

<h3>Description</h3>

<p><code>EPtest</code> builds the non-parametric variable significance test from Klein and Rochet (2022) for the null hypothesis <code class="reqn">H_0: S^u = S</code> where <code class="reqn">S^u</code> is the Sobol index for the inputs <code class="reqn">X_i, i \in u</code> ans <code class="reqn">S</code> is the Sobol index for all the inputs in <code class="reqn">X</code>. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>EPtest(X, y, u = NULL, doe = NULL, Kdoe = 10, tau = 0.1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="EPtest_+3A_x">X</code></td>
<td>
<p>a matrix or data.frame that contains the numerical inputs as columns.</p>
</td></tr>
<tr><td><code id="EPtest_+3A_y">y</code></td>
<td>
<p>a vector of output.</p>
</td></tr>
<tr><td><code id="EPtest_+3A_u">u</code></td>
<td>
<p>the vector of indices of the columns of X for which we want to test the significance.</p>
</td></tr>
<tr><td><code id="EPtest_+3A_doe">doe</code></td>
<td>
<p>the design of experiment on which the empirical process is to be evaluated. It should be independent from X.</p>
</td></tr>
<tr><td><code id="EPtest_+3A_kdoe">Kdoe</code></td>
<td>
<p>if doe is null and Kdoe is specified, the design of experiment is taken as Kdoe points drawn uniformly independently on intervals delimited by the range of each input.</p>
</td></tr>
<tr><td><code id="EPtest_+3A_tau">tau</code></td>
<td>
<p>a regularization parameter to approximate the limit chi2 distribution of the test statistics under H0.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>EPtest</code> returns a list containing:
</p>
<table role = "presentation">
<tr><td><code>statistics</code></td>
<td>
<p>The test statistics that follows a chi-squared distribution under the null hypothesis.</p>
</td></tr>
<tr><td><code>ddl</code></td>
<td>
<p>The number of degrees of freedom used in the limit chi-square distribution for the test.</p>
</td></tr>
<tr><td><code>p-value</code></td>
<td>
<p>The test p-value.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Paul Rochet</p>


<h3>References</h3>

<p>T. Klein and P. Rochet, <em>Test comparison for Sobol Indices over nested sets of variables</em>, SIAM/ASA Journal on Uncertainty Quantification 10.4 (2022): 1586-1600.</p>


<h3>See Also</h3>

<p><a href="#topic+sobol">sobol</a></p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Model: Ishigami
  
n = 100
X = matrix(runif(3*n, -pi, pi), ncol = 3)
  
y = ishigami.fun(X)
	
# Test the significance of X1, H0: S1 = 0
EPtest(X[, 1], y, u = NULL)

# Test if X1 is sufficient to explain Y, H0: S1 = S123
EPtest(X, y, u = 1)
  
# Test if X3 is significant in presence of X2, H0: S2 = S23
EPtest(X[, 2:3], y, u = 1)
  
</code></pre>

<hr>
<h2 id='fast99'>Extended Fourier Amplitude Sensitivity Test</h2><span id='topic+fast99'></span><span id='topic+tell.fast99'></span><span id='topic+print.fast99'></span><span id='topic+plot.fast99'></span>

<h3>Description</h3>

<p><code>fast99</code> implements the so-called &quot;extended-FAST&quot; method
(Saltelli et al. 1999). This method allows the estimation of first
order and total Sobol' indices for all the factors (alltogether
<code class="reqn">2p</code> indices, where <code class="reqn">p</code> is the number of factors) at a
total cost of <code class="reqn">n \times p</code> simulations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fast99(model = NULL, factors, n, M = 4, omega = NULL,
       q = NULL, q.arg = NULL, ...)
## S3 method for class 'fast99'
tell(x, y = NULL, ...)
## S3 method for class 'fast99'
print(x, ...)
## S3 method for class 'fast99'
plot(x, ylim = c(0, 1), ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="fast99_+3A_model">model</code></td>
<td>
<p>a function, or a model with a <code>predict</code> method,
defining the model to analyze.</p>
</td></tr>
<tr><td><code id="fast99_+3A_factors">factors</code></td>
<td>
<p>an integer giving the number of factors, or a vector of
character strings giving their names.</p>
</td></tr>
<tr><td><code id="fast99_+3A_n">n</code></td>
<td>
<p>an integer giving the sample size, i.e. the length of the
discretization of the s-space (see Cukier et al.).</p>
</td></tr>
<tr><td><code id="fast99_+3A_m">M</code></td>
<td>
<p>an integer specifying the interference parameter, i.e. the
number of harmonics to sum in the Fourier series decomposition (see
Cukier et al.).</p>
</td></tr>
<tr><td><code id="fast99_+3A_omega">omega</code></td>
<td>
<p>a vector giving the set of frequencies, one frequency for
each factor (see details below).</p>
</td></tr>
<tr><td><code id="fast99_+3A_q">q</code></td>
<td>
<p>a vector of quantile functions names corresponding to
wanted factors distributions (see details below).</p>
</td></tr>
<tr><td><code id="fast99_+3A_q.arg">q.arg</code></td>
<td>
<p>a list of quantile functions parameters (see details below).</p>
</td></tr>
<tr><td><code id="fast99_+3A_x">x</code></td>
<td>
<p>a list of class <code>"fast99"</code> storing the state of the
sensitivity study (parameters, data, estimates).</p>
</td></tr>
<tr><td><code id="fast99_+3A_y">y</code></td>
<td>
<p>a vector of model responses.</p>
</td></tr>
<tr><td><code id="fast99_+3A_ylim">ylim</code></td>
<td>
<p>y-coordinate plotting limits.</p>
</td></tr>
<tr><td><code id="fast99_+3A_...">...</code></td>
<td>
<p>any other arguments for <code>model</code> which are passed
unchanged each time it is called.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If not given, the set of frequencies <code>omega</code> is taken from
Saltelli et al.  The first frequency of the vector <code>omega</code> is
assigned to each factor <code class="reqn">X_i</code> in turn (corresponding to the
estimation of Sobol' indices <code class="reqn">S_i</code> and <code class="reqn">S_{T_i}</code>),
other frequencies being assigned to the remaining factors.
</p>
<p>If the arguments <code>q</code> and <code>q.args</code> are not given, the factors
are taken uniformly distributed on <code class="reqn">[0,1]</code>. The
argument <code>q</code> must be list of character strings, giving the names
of the quantile functions (one for each factor), such as <code>qunif</code>,
<code>qnorm</code>... It can also be a single character string, meaning
same distribution for all. The argument <code>q.arg</code> must be a list of
lists, each one being additional parameters for the corresponding
quantile function. For example, the parameters of the quantile
function <code>qunif</code> could be <code>list(min=1, max=2)</code>, giving an
uniform distribution on <code class="reqn">[1,2]</code>.  If <code>q</code> is a single
character string, then <code>q.arg</code> must be a single list (rather than
a list of one list).
</p>


<h3>Value</h3>

<p><code>fast99</code> returns a list of class <code>"fast99"</code>, containing all
the input arguments detailed before, plus the following components:
</p>
<table role = "presentation">
<tr><td><code>call</code></td>
<td>
<p>the matched call.</p>
</td></tr>
<tr><td><code>X</code></td>
<td>
<p>a <code>data.frame</code> containing the factors sample values.</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>a vector of model responses.</p>
</td></tr>
<tr><td><code>V</code></td>
<td>
<p>the estimation of variance.</p>
</td></tr>
<tr><td><code>D1</code></td>
<td>
<p>the estimations of Variances of the Conditional Expectations
(VCE) with respect to each factor.</p>
</td></tr>
<tr><td><code>Dt</code></td>
<td>
<p>the estimations of VCE with respect to each factor
complementary set of factors (&quot;all but <code class="reqn">X_i</code>&quot;).</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Gilles Pujol
</p>


<h3>References</h3>

<p>A. Saltelli, S. Tarantola and K. Chan, 1999, <em>A quantitative, model
independent method for global sensitivity analysis of model
output</em>, Technometrics, 41, 39&ndash;56.
</p>
<p>R. I. Cukier, H. B. Levine and K. E. Schuler, 1978, <em>Nonlinear
sensitivity analysis of multiparameter model
systems</em>. J. Comput. Phys., 26, 1&ndash;42.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Test case : the non-monotonic Ishigami function
x &lt;- fast99(model = ishigami.fun, factors = 3, n = 1000,
            q = "qunif", q.arg = list(min = -pi, max = pi))
print(x)
plot(x)
</code></pre>

<hr>
<h2 id='johnson'>Johnson indices</h2><span id='topic+johnson'></span><span id='topic+print.johnson'></span><span id='topic+plot.johnson'></span><span id='topic+ggplot.johnson'></span>

<h3>Description</h3>

<p><code>johnson</code> computes the Johnson indices for correlated input relative importance by 
<code class="reqn">R^2</code> decomposition for linear and logistic regression models. These 
indices allocates  a share of <code class="reqn">R^2</code> to each input based on the relative 
weight allocation (RWA) system, in the case of dependent or correlated inputs.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>johnson(X, y, rank = FALSE, logistic = FALSE, nboot = 0, conf = 0.95)
## S3 method for class 'johnson'
print(x, ...)
## S3 method for class 'johnson'
plot(x, ylim = c(0,1), ...)
## S3 method for class 'johnson'
ggplot(data,  mapping = aes(), ylim = c(0, 1), ..., environment
                 = parent.frame())
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="johnson_+3A_x">X</code></td>
<td>
<p>a data frame (or object coercible by <code>as.data.frame</code>)
containing the design of experiments (model input variables).</p>
</td></tr>
<tr><td><code id="johnson_+3A_y">y</code></td>
<td>
<p>a vector containing the responses corresponding to the design
of experiments (model output variables).</p>
</td></tr>
<tr><td><code id="johnson_+3A_rank">rank</code></td>
<td>
<p>logical. If <code>TRUE</code>, the analysis is done on the
ranks.</p>
</td></tr>
<tr><td><code id="johnson_+3A_logistic">logistic</code></td>
<td>
<p>logical. If <code>TRUE</code>, the analysis is done via a
logistic regression (binomial GLM).</p>
</td></tr>
<tr><td><code id="johnson_+3A_nboot">nboot</code></td>
<td>
<p>the number of bootstrap replicates.</p>
</td></tr>
<tr><td><code id="johnson_+3A_conf">conf</code></td>
<td>
<p>the confidence level of the bootstrap confidence intervals.</p>
</td></tr>
<tr><td><code id="johnson_+3A_x">x</code></td>
<td>
<p>the object returned by <code>johnson</code>.</p>
</td></tr>
<tr><td><code id="johnson_+3A_data">data</code></td>
<td>
<p>the object returned by <code>johnson</code>.</p>
</td></tr>
<tr><td><code id="johnson_+3A_ylim">ylim</code></td>
<td>
<p>the y-coordinate limits of the plot.</p>
</td></tr>
<tr><td><code id="johnson_+3A_mapping">mapping</code></td>
<td>
<p>Default list of aesthetic mappings to use for plot. If not specified, 
must be supplied in each layer added to the plot.</p>
</td></tr>
<tr><td><code id="johnson_+3A_environment">environment</code></td>
<td>
<p>[Deprecated] Used prior to tidy evaluation.</p>
</td></tr>
<tr><td><code id="johnson_+3A_...">...</code></td>
<td>
<p>arguments to be passed to methods, such as graphical
parameters (see <code>par</code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Logistic regression model (<code>logistic = TRUE</code>) and rank-based indices
(<code>rank = TRUE</code>) are incompatible.
</p>


<h3>Value</h3>

<p><code>johnson</code> returns a list of class <code>"johnson"</code>, containing the following
components:
</p>
<table role = "presentation">
<tr><td><code>call</code></td>
<td>
<p>the matched call.</p>
</td></tr>
<tr><td><code>johnson</code></td>
<td>
<p>a data frame containing the estimations of the johnson
indices, bias and confidence intervals.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Bertrand Iooss and Laura Clouvel
</p>


<h3>References</h3>

<p>L. Clouvel, B. Iooss, V. Chabridon, M. Il Idrissi and F. Robin, 2024,
<em>An overview of variance-based importance measures in the linear regression context: 
comparative analyses and numerical tests</em>, Preprint.
<a href="https://hal.science/hal-04102053">https://hal.science/hal-04102053</a>
</p>
<p>B. Iooss, V. Chabridon and V. Thouvenot, <em>Variance-based importance 
measures for machine learning model interpretability</em>, Congres lambda-mu23,
Saclay, France, 10-13 octobre 2022
<a href="https://hal.science/hal-03741384">https://hal.science/hal-03741384</a>
</p>
<p>J.W. Johnson, 2000, <em>A heuristic method for estimating the relative 
weight of predictor variables in multiple regression</em>, Multivariate 
Behavioral Research, 35:1-19.
</p>
<p>J.W. Johnson and J.M. LeBreton, 2004, <em>History and use of relative 
importance indices in organizational research</em>, Organizational 
Research Methods, 7:238-257.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+src">src</a></code>, <code><a href="#topic+lmg">lmg</a></code>, <code><a href="#topic+pmvd">pmvd</a></code>, <code><a href="#topic+johnsonshap">johnsonshap</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
##################################
# Same example than the one in src()

# a 100-sample with X1 ~ U(0.5, 1.5)
#                   X2 ~ U(1.5, 4.5)
#                   X3 ~ U(4.5, 13.5)

library(boot)
n &lt;- 100
X &lt;- data.frame(X1 = runif(n, 0.5, 1.5),
                X2 = runif(n, 1.5, 4.5),
                X3 = runif(n, 4.5, 13.5))

# linear model : Y = X1 + X2 + X3

y &lt;- with(X, X1 + X2 + X3)

# sensitivity analysis

x &lt;- johnson(X, y, nboot = 100)
print(x)
plot(x)

library(ggplot2)
ggplot(x)


#################################
# Same examples than the ones in lmg()

library(boot)
library(mvtnorm)

set.seed(1234)
n &lt;- 1000
beta&lt;-c(1,-1,0.5)
sigma&lt;-matrix(c(1,0,0,
                0,1,-0.8,
                0,-0.8,1),
              nrow=3,
              ncol=3)

##########
# Gaussian correlated inputs

X &lt;-rmvnorm(n, rep(0,3), sigma)
colnames(X)&lt;-c("X1","X2", "X3")

#########
# Linear Model

y &lt;- X%*%beta + rnorm(n,0,2)

# Without Bootstrap confidence intervals
x&lt;-johnson(X, y)
print(x)
plot(x)

# With Boostrap confidence intervals
x&lt;-johnson(X, y, nboot=100, conf=0.95)
print(x)
plot(x)

# Rank-based analysis
x&lt;-johnson(X, y, rank=TRUE, nboot=100, conf=0.95)
print(x)
plot(x)

#######
# Logistic Regression
y&lt;-as.numeric(X%*%beta + rnorm(n)&gt;0)
x&lt;-johnson(X,y, logistic = TRUE)
plot(x)
print(x)

#################################
# Test on a modified Linkletter fct with: 
# - multivariate normal inputs (all multicollinear)
# - in dimension 50 (there are 42 dummy inputs)
# - large-size sample (1e4)

library(mvtnorm)

n &lt;- 1e4
d &lt;- 50
sigma &lt;- matrix(0.5,ncol=d,nrow=d)
diag(sigma) &lt;- 1
X &lt;- rmvnorm(n, rep(0,d), sigma)

y &lt;- linkletter.fun(X)
joh &lt;- johnson(X,y)
sum(joh$johnson) # gives the R2
plot(joh)
</code></pre>

<hr>
<h2 id='johnsonshap'>Johnson-Shapley indices</h2><span id='topic+johnsonshap'></span><span id='topic+print.johnsonshap'></span><span id='topic+plot.johnsonshap'></span><span id='topic+ggplot.johnsonshap'></span>

<h3>Description</h3>

<p><code>johnsonshap</code> computes the Johnson-Shapley indices for correlated input 
relative importance. These indices allocate a share of the output variance to 
each input based on the relative weight allocation system, 
in the case of dependent or correlated inputs.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>johnsonshap(model = NULL, X1, N, nboot = 0, conf = 0.95)
## S3 method for class 'johnsonshap'
print(x, ...)
## S3 method for class 'johnsonshap'
plot(x, ylim = c(0,1), ...)
## S3 method for class 'johnsonshap'
ggplot(data,  mapping = aes(), ylim = c(0, 1), ..., 
                environment = parent.frame())
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="johnsonshap_+3A_model">model</code></td>
<td>
<p>a function, or a model with a <code>predict</code> method,
defining the model to analyze.</p>
</td></tr>
<tr><td><code id="johnsonshap_+3A_x1">X1</code></td>
<td>
<p>a data frame (or object coercible by <code>as.data.frame</code>)
containing a design of experiments (model input variables).</p>
</td></tr>
<tr><td><code id="johnsonshap_+3A_n">N</code></td>
<td>
<p>an integer giving the size of each replicated design for
the Sobol' indices computations via the sobolrep() fct.</p>
</td></tr>
<tr><td><code id="johnsonshap_+3A_nboot">nboot</code></td>
<td>
<p>the number of bootstrap replicates.</p>
</td></tr>
<tr><td><code id="johnsonshap_+3A_conf">conf</code></td>
<td>
<p>the confidence level of the bootstrap confidence intervals.</p>
</td></tr>
<tr><td><code id="johnsonshap_+3A_x">x</code></td>
<td>
<p>the object returned by <code>johnsonshap</code>.</p>
</td></tr>
<tr><td><code id="johnsonshap_+3A_data">data</code></td>
<td>
<p>the object returned by <code>johnsonshap</code>.</p>
</td></tr>
<tr><td><code id="johnsonshap_+3A_ylim">ylim</code></td>
<td>
<p>the y-coordinate limits of the plot.</p>
</td></tr>
<tr><td><code id="johnsonshap_+3A_mapping">mapping</code></td>
<td>
<p>Default list of aesthetic mappings to use for plot. If not 
specified, must be supplied in each layer added to the plot.</p>
</td></tr>
<tr><td><code id="johnsonshap_+3A_environment">environment</code></td>
<td>
<p>[Deprecated] Used prior to tidy evaluation.</p>
</td></tr>
<tr><td><code id="johnsonshap_+3A_...">...</code></td>
<td>
<p>arguments to be passed to methods, such as graphical
parameters (see <code>par</code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>X1 is not used to run the model but just to perform the SVD; the model is run 
on a specific   design which is internally generated.
</p>
<p>By using bootstrap, values in the columns 'bias' and 'std. error' are 
arbitrarily put at 0 because of impossible computations; values in columns 
'original', 'min c.i.' and 'max c.i.' are correctly computed.
</p>


<h3>Value</h3>

<p><code>johnsonshap</code> returns a list of class <code>"johnsonshap"</code>, containing 
all the input arguments detailed before, plus the following components:
</p>
<table role = "presentation">
<tr><td><code>call</code></td>
<td>
<p>the matched call.</p>
</td></tr>
<tr><td><code>X</code></td>
<td>
<p>a matrix containing the design of experiments.</p>
</td></tr>
<tr><td><code>sobrepZ</code></td>
<td>
<p>the Sobol' indices of the transformed inputs (independent)</p>
</td></tr>
<tr><td><code>Wstar</code></td>
<td>
<p>the standardized weight matrix.</p>
</td></tr>
<tr><td><code>johnsonshap</code></td>
<td>
<p>a data frame containing the estimations of the 
Johnson-Shapley indices, bias and confidence intervals.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Bertrand Iooss
</p>


<h3>References</h3>

<p>B. Iooss and L. Clouvel, <em>Une methode d'approximation des effets 
de Shapley en grande dimension</em>, 54emes  Journees de Statistique, 
Bruxelles, Belgique, July 3-7, 2023
</p>


<h3>See Also</h3>

<p><code><a href="#topic+johnson">johnson</a>, <a href="#topic+shapleysobol_knn">shapleysobol_knn</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(ggplot2)
library(boot)

#####################################################
# Test case: the non-monotonic Sobol g-function (with independent inputs)
n &lt;- 1000
X &lt;- data.frame(matrix(runif(8 * n), nrow = n))
x &lt;- johnsonshap(model = sobol.fun, X1 = X, N = n)
print(x)
plot(x)
ggplot(x)


#############################################
# 3D analytical toy functions described in Iooss &amp; Clouvel (2023)

library(mvtnorm)

Xall &lt;- function(n) mvtnorm::rmvnorm(n,mu,Covmat)
# 2 correlated inputs
Cov3d2 &lt;- function(rho){ # correl (X1,X2)
  Cormat &lt;- matrix(c(1,rho,0,rho,1,0,0,0,1),3,3)
  return( ( sig %*% t(sig) ) * Cormat)
}
mu3d &lt;- c(1,0,0) ; sig3d &lt;- c(0.25,1,1)
d &lt;- 3 ; mu &lt;- mu3d ; sig &lt;- sig3d ; Covm &lt;- Cov3d2
Xvec &lt;- c("X1","X2","X3")

n &lt;- 1e4    # initial sample size
N &lt;- 1e4    # cost to estimate indices 
rho &lt;- 0.9  # correlation coef for dependent inputs' case

################
# Linear model + a strong 2nd order interaction

toy3d &lt;- function(x) return(x[,1]*(1+x[,1]*(cos(x[,2]+x[,3])^2))) 
# interaction X2X3
toy &lt;- toy3d 

# Independent case

Covmat &lt;- Covm(0)
X &lt;- as.data.frame(Xall(n))
Y &lt;- toy(X)
joh &lt;- johnson(X, Y, nboot=100)
print(joh)
johshap &lt;- johnsonshap(model = toy, X1 = X, N = N, nboot=100)
print(johshap)
ggplot(johshap)

# Dependent case

Covmat &lt;- Covm(rho)
Xdep &lt;- as.data.frame(Xall(n))
Ydep &lt;- toy(Xdep)
joh &lt;- johnson(Xdep, Ydep, nboot=0)
print(joh)
johshap &lt;- johnsonshap(model = toy, X1 = Xdep, N = N, nboot=100)
print(johshap)
ggplot(johshap)

################
# Strongly non-inear model + a strong 2nd order interaction

toy3dNL &lt;- function(x) return(sin(x[,1]*pi/2)*(1+x[,1]*(cos(x[,2]+x[,3])^2))) 
# non linearity in X1
toy &lt;- toy3dNL

# Independent case

Covmat &lt;- Covm(0)
X &lt;- as.data.frame(Xall(n))
Y &lt;- toy(X)
joh &lt;- johnson(X, Y, nboot=100)
print(joh)
johshap &lt;- johnsonshap(model = toy, X1 = X, N = N, nboot=100)
print(johshap)
ggplot(johshap)

# Dependent case

Covmat &lt;- Covm(rho)
Xdep &lt;- as.data.frame(Xall(n))
Ydep &lt;- toy(Xdep)
joh &lt;- johnson(Xdep, Ydep, nboot=0)
print(joh)
johshap &lt;- johnsonshap(model = NULL, X1 = Xdep, N = N, nboot=100)
y &lt;- toy(johshap$X)
tell(johshap, y)
print(johshap)
ggplot(johshap)


</code></pre>

<hr>
<h2 id='lmg'>LMG <code class="reqn">R^2</code> decomposition for linear and logistic regression models</h2><span id='topic+lmg'></span><span id='topic+print.lmg'></span><span id='topic+plot.lmg'></span>

<h3>Description</h3>

<p><code>lmg</code> computes the Lindeman, Merenda and Gold (LMG) indices for correlated
input relative importance by <code class="reqn">R^2</code> decomposition for linear and logistic
regression models. These indices allocates  a share of <code class="reqn">R^2</code> to each input
based on the Shapley attribution system, in the case of dependent or correlated inputs.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lmg(X, y, logistic = FALSE,  rank = FALSE, nboot = 0, 
    conf = 0.95, max.iter = 1000, parl = NULL)
## S3 method for class 'lmg'
print(x, ...)
## S3 method for class 'lmg'
plot(x, ylim = c(0,1), ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="lmg_+3A_x">X</code></td>
<td>
<p>a matrix or data frame containing the observed covariates
(i.e., features, input variables...).</p>
</td></tr>
<tr><td><code id="lmg_+3A_y">y</code></td>
<td>
<p>a numeric vector containing the observed outcomes (i.e.,
dependent variable). If <code>logistic=TRUE</code>, can be a numeric vector
of zeros and ones, or a logical vector, or a factor.</p>
</td></tr>
<tr><td><code id="lmg_+3A_logistic">logistic</code></td>
<td>
<p>logical. If <code>TRUE</code>, the analysis is done via a
logistic regression(binomial GLM).</p>
</td></tr>
<tr><td><code id="lmg_+3A_rank">rank</code></td>
<td>
<p>logical. If <code>TRUE</code>, the analysis is done on the
ranks.</p>
</td></tr>
<tr><td><code id="lmg_+3A_nboot">nboot</code></td>
<td>
<p>the number of bootstrap replicates for the computation
of confidence intervals.</p>
</td></tr>
<tr><td><code id="lmg_+3A_conf">conf</code></td>
<td>
<p>the confidence level of the bootstrap confidence intervals.</p>
</td></tr>
<tr><td><code id="lmg_+3A_max.iter">max.iter</code></td>
<td>
<p>if <code>logistic=TRUE</code>, the maximum number of iterative 
optimization steps allowed for the logistic regression. Default is <code>1000</code>.</p>
</td></tr> 
<tr><td><code id="lmg_+3A_parl">parl</code></td>
<td>
<p>number of cores on which to parallelize the computation. If
<code>NULL</code>, then no parallelization is done.</p>
</td></tr>
<tr><td><code id="lmg_+3A_x">x</code></td>
<td>
<p>the object returned by <code>lmg</code>.</p>
</td></tr>
<tr><td><code id="lmg_+3A_ylim">ylim</code></td>
<td>
<p>the y-coordinate limits of the plot.</p>
</td></tr>
<tr><td><code id="lmg_+3A_...">...</code></td>
<td>
<p>arguments to be passed to methods, such as graphical
parameters (see <code>par</code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The computation is done using the subset procedure, defined in Broto, Bachoc
and Depecker (2020), that is computing all the <code class="reqn">R^2</code> for all possible
sub-models first, and then affecting the Shapley weights according to the Lindeman,
Merenda and Gold (1980) definition.
</p>
<p>For logistic regression (<code>logistic=TRUE</code>), the <code class="reqn">R^2</code>
value is equal to:
</p>
<p style="text-align: center;"><code class="reqn">R^2 = 1-\frac{\textrm{model deviance}}{\textrm{null deviance}}</code>
</p>

<p>If either a logistic regression model (<code>logistic = TRUE</code>), or any column
of <code>X</code> is categorical (i.e., of class <code>factor</code>), then the rank-based
indices cannot be computed. In both those cases, <code>rank = FALSE</code> is forced
by default (with a <code>warning</code>).
</p>
<p>If too many cores for the machine are passed on to the <code>parl</code> argument,
the chosen number of cores is defaulted to the available cores minus one.
</p>


<h3>Value</h3>

<p><code>lmg</code> returns a list of class <code>"lmg"</code>, containing the following
components:
</p>
<table role = "presentation">
<tr><td><code>call</code></td>
<td>
<p>the matched call.</p>
</td></tr>
<tr><td><code>lmg</code></td>
<td>
<p>a data frame containing the estimations of the LMG indices.</p>
</td></tr>
<tr><td><code>R2s</code></td>
<td>
<p>the estimations of the <code class="reqn">R^2</code> for all possible sub-models.</p>
</td></tr>
<tr><td><code>indices</code></td>
<td>
<p>list of all subsets corresponding to the structure of R2s.</p>
</td></tr>
<tr><td><code>w</code></td>
<td>
<p>the Shapley weights.</p>
</td></tr>
<tr><td><code>conf_int</code></td>
<td>
<p>a matrix containing the estimations, biais and confidence
intervals by bootstrap (if <code>nboot&gt;0</code>).</p>
</td></tr>
<tr><td><code>X</code></td>
<td>
<p>the observed covariates.</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>the observed outcomes.</p>
</td></tr>
<tr><td><code>logistic</code></td>
<td>
<p>logical. <code>TRUE</code> if the analysis has been made by
logistic regression.</p>
</td></tr>
<tr><td><code>boot</code></td>
<td>
<p>logical. <code>TRUE</code> if bootstrap estimates have been produced.</p>
</td></tr>
<tr><td><code>nboot</code></td>
<td>
<p>number of bootstrap replicates.</p>
</td></tr>
<tr><td><code>rank</code></td>
<td>
<p>logical. <code>TRUE</code> if a rank analysis has been made.</p>
</td></tr>
<tr><td><code>parl</code></td>
<td>
<p>number of chosen cores for the computation.</p>
</td></tr>
<tr><td><code>conf</code></td>
<td>
<p>level for the confidence intervals by bootstrap.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Marouane Il Idrissi
</p>


<h3>References</h3>

<p>Broto B., Bachoc F. and Depecker M. (2020) <em>Variance Reduction for Estimation
of Shapley Effects and Adaptation to Unknown Input Distribution.</em> SIAM/ASA Journal
on Uncertainty Quantification, 8(2).
</p>
<p>D.V. Budescu (1993). <em>Dominance analysis: A new approach to the problem of relative
importance of predictors in multiple regression.</em> Psychological Bulletin, 114:542-551.
</p>
<p>L. Clouvel, B. Iooss, V. Chabridon, M. Il Idrissi and F. Robin, 2024,
<em>An overview of variance-based importance measures in the linear regression context: 
comparative analyses and numerical tests</em>, Preprint.
<a href="https://hal.science/hal-04102053">https://hal.science/hal-04102053</a>
</p>
<p>U. Gromping (2006). <em>Relative importance for linear regression in R: the Package
relaimpo.</em>  Journal of Statistical Software, 17:1-27.
</p>
<p>M. Il Idrissi, V. Chabridon and B. Iooss (2021). <em>Developments and applications
of Shapley effects   to reliability-oriented sensitivity analysis with correlated 
inputs</em>, Environmental Modelling &amp; Software, 143, 105115, 2021
</p>
<p>M. Il Idrissi, V. Chabridon and B. Iooss (2021). <em>Mesures d'importance relative  
par decompositions de la performance de modeles de regression,</em> Actes des 52emes
Journees   de Statistiques de la Societe Francaise de Statistique (SFdS), pp 497-502,
Nice, France, Juin 2021
</p>
<p>B. Iooss, V. Chabridon and V. Thouvenot, <em>Variance-based importance 
measures for machine learning model interpretability</em>, Congres lambda-mu23,
Saclay, France, 10-13 octobre 2022
<a href="https://hal.science/hal-03741384">https://hal.science/hal-03741384</a>
</p>
<p>Lindeman RH, Merenda PF, Gold RZ (1980). <em>Introduction to Bivariate and Multivariate
Analysis.</em> Scott, Foresman, Glenview, IL.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pcc">pcc</a></code>, <code><a href="#topic+src">src</a></code>, <code><a href="#topic+johnson">johnson</a></code>, <code><a href="#topic+shapleyPermEx">shapleyPermEx</a></code>, <code><a href="#topic+shapleysobol_knn">shapleysobol_knn</a></code>, <code><a href="#topic+pmvd">pmvd</a></code>, <code><a href="#topic+pme_knn">pme_knn</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(parallel)
library(doParallel)
library(foreach)
library(gtools)
library(boot)

library(mvtnorm)

set.seed(1234)
n &lt;- 1000
beta&lt;-c(1,-1,0.5)
sigma&lt;-matrix(c(1,0,0,
                0,1,-0.8,
                0,-0.8,1),
              nrow=3,
              ncol=3)

############################
# Gaussian correlated inputs

X &lt;-rmvnorm(n, rep(0,3), sigma)
colnames(X)&lt;-c("X1","X2", "X3")

#############################
# Linear Model

y &lt;- X%*%beta + rnorm(n,0,2)

# Without Bootstrap confidence intervals
x&lt;-lmg(X, y)
print(x)
plot(x)

# With Boostrap confidence intervals
x&lt;-lmg(X, y, nboot=100, conf=0.95)
print(x)
plot(x)

# Rank-based analysis
x&lt;-lmg(X, y, rank=TRUE, nboot=100, conf=0.95)
print(x)
plot(x)

############################
# Logistic Regression
y&lt;-as.numeric(X%*%beta + rnorm(n)&gt;0)
x&lt;-lmg(X,y, logistic = TRUE)
plot(x)
print(x)

# Parallel computing
#x&lt;-lmg(X,y, logistic = TRUE, parl=2)
#plot(x)
#print(x)

</code></pre>

<hr>
<h2 id='maximin_cplus'>Maximin criterion</h2><span id='topic+maximin_cplus'></span>

<h3>Description</h3>

<p>Compute the <code>maximin</code> criterion (also called mindist). This function uses a C++ implementation of the function mindist from package <span class="pkg">DiceDesign</span>.</p>


<h3>Usage</h3>

<pre><code class='language-R'>maximin_cplus(design)</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="maximin_cplus_+3A_design">design</code></td>
<td>
<p>a matrix representing the design of experiments in the unit cube [0,1]<code class="reqn">^d</code>. If this last condition is not fulfilled, a transformation into [0,1]<code class="reqn">^{d}</code> is applied before the computation of the criteria.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The maximin criterion is defined by:
</p>
<p style="text-align: center;"><code class="reqn">maximin= \min_{x_{i}\in X} \left( \gamma_{i} \right)</code>
</p>

<p>where <code class="reqn">\gamma_{i}</code> is the minimal distance between the point <code class="reqn">x_{i}</code>
and the other points <code class="reqn">x_{k}</code> of the <code>design</code>.
</p>
<p>A higher value corresponds to a more regular scaterring of design points.
</p>


<h3>Value</h3>

<p>A real number equal to the value of the maximin criterion for the <code>design</code>.</p>


<h3>Author(s)</h3>

<p>Laurent Gilquin</p>


<h3>References</h3>

<p>Gunzburer M., Burkdart J. (2004) <em>Uniformity measures for point
samples in hypercubes</em>
<a href="https://people.sc.fsu.edu/~jburkardt/">https://people.sc.fsu.edu/~jburkardt/</a>.
</p>
<p>Jonshon M.E., Moore L.M. and Ylvisaker D. (1990) <em>Minmax and maximin distance
designs</em>, J. of Statis. Planning and Inference, 26, 131-148.
</p>
<p>Chen V.C.P., Tsui K.L., Barton R.R. and Allen J.K. (2003) <em>A review of design
and modeling in computer experiments</em>, Handbook of Statistics, 22, 231-261.
</p>


<h3>See Also</h3>

<p>discrepancy measures provided by <code><a href="#topic+discrepancyCriteria_cplus">discrepancyCriteria_cplus</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'>dimension &lt;- 2
n &lt;- 40
X &lt;- matrix(runif(n*dimension),n,dimension)
maximin_cplus(X)
</code></pre>

<hr>
<h2 id='morris'>Morris's Elementary Effects Screening Method</h2><span id='topic+morris'></span><span id='topic+tell.morris'></span><span id='topic+print.morris'></span><span id='topic+plot.morris'></span><span id='topic+plot3d.morris'></span>

<h3>Description</h3>

<p><code>morris</code> implements the Morris's elementary effects screening
method (Morris, 1991). This method, based on design of experiments,
allows to identify the few important factors at a cost of <code class="reqn">r
  \times (p+1)</code> simulations (where <code class="reqn">p</code> is the number
of factors). This implementation includes some improvements of the
original method: space-filling optimization of the design (Campolongo
et al. 2007) and simplex-based design (Pujol 2009).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>morris(model = NULL, factors, r, design, binf = 0, bsup = 1,
       scale = TRUE, ...)
## S3 method for class 'morris'
tell(x, y = NULL, ...)
## S3 method for class 'morris'
print(x, ...)
## S3 method for class 'morris'
plot(x, identify = FALSE, atpen = FALSE, y_col = NULL, 
  y_dim3 = NULL, ...)
## S3 method for class 'morris'
plot3d(x, alpha = c(0.2, 0), sphere.size = 1, y_col = NULL, 
  y_dim3 = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="morris_+3A_model">model</code></td>
<td>
<p>a function, or a model with a <code>predict</code> method,
defining the model to analyze.</p>
</td></tr>
<tr><td><code id="morris_+3A_factors">factors</code></td>
<td>
<p>an integer giving the number of factors, or a vector of
character strings giving their names.</p>
</td></tr>
<tr><td><code id="morris_+3A_r">r</code></td>
<td>
<p>either an integer giving the number of repetitions of the design,
i.e. the number of elementary effect computed per factor, or a
vector of two integers <code>c(r1, r2)</code> for the space-filling
improvement (Campolongo et al. 2007). In this case, <code>r1</code> is the
wanted design size, and <code>r2</code> (<code class="reqn">&gt; \code{r1}</code>)
is the size of the (bigger) population in which is extracted the
design (this can throw a warning, see below).
</p>
</td></tr>
<tr><td><code id="morris_+3A_design">design</code></td>
<td>
<p>a list specifying the design type and its
parameters:
</p>

<ul>
<li> <p><code>type = "oat"</code> for Morris's OAT design (Morris 1991),
with the parameters:
</p>

<ul>
<li> <p><code>levels</code> : either an integer specifying the number of
levels of the design, or a vector of integers for different
values for each factor.
</p>
</li>
<li> <p><code>grid.jump</code> : either an integer specifying the number of
levels that are increased/decreased for computing the
elementary effects, or a vector of integers for different values
for each factor. If not given, it is set to <code>grid.jump =
	  1</code>. Notice that this default value of one does not follow
Morris's recommendation of 
<code class="reqn">\texttt{levels} / 2</code>.
</p>
</li></ul>

</li>
<li> <p><code>type = "simplex"</code> for simplex-based design (Pujol
2009), with the parameter:
</p>

<ul>
<li> <p><code>scale.factor</code> : a numeric value, the homothety factor of
the (isometric) simplexes. Edges equal one with a scale factor
of one.
</p>
</li></ul>

</li></ul>

</td></tr>
<tr><td><code id="morris_+3A_binf">binf</code></td>
<td>
<p>either an integer, specifying the minimum value for the
factors, or a vector for different values for each factor.</p>
</td></tr>
<tr><td><code id="morris_+3A_bsup">bsup</code></td>
<td>
<p>either an integer, specifying the maximum value for the
factors, or a vector for different values for each factor.</p>
</td></tr>
<tr><td><code id="morris_+3A_scale">scale</code></td>
<td>
<p>logical. If <code>TRUE</code>, the input design of experiments is
scaled after building the design and before computing the elementary 
effects so that all factors vary within the range [0,1]. For each factor, 
the scaling is done relatively to its corresponding bsup and binf.</p>
</td></tr>
<tr><td><code id="morris_+3A_x">x</code></td>
<td>
<p>a list of class <code>"morris"</code> storing the state of the
screening study (parameters, data, estimates).</p>
</td></tr>
<tr><td><code id="morris_+3A_y">y</code></td>
<td>
<p>a vector of model responses.</p>
</td></tr>
<tr><td><code id="morris_+3A_identify">identify</code></td>
<td>
<p>logical. If <code>TRUE</code>, the user selects with the
mouse the factors to label on the <code class="reqn">(\mu^*,\sigma)</code>
graph (see <code>identify</code>).</p>
</td></tr>
<tr><td><code id="morris_+3A_atpen">atpen</code></td>
<td>
<p>logical. If <code>TRUE</code> (and <code>identify = TRUE</code>), the 
user-identified labels (more precisely: their lower-left corners) of the 
factors are plotted at the place where the user had clicked (if near enough 
to one of the factor points). If <code>FALSE</code> (and <code>identify = TRUE</code>), 
the labels are automatically adjusted to the lower, left, upper or right 
side of the factor point. For further information, see 
<code><a href="graphics.html#topic+identify">identify</a></code>. Defaults to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="morris_+3A_y_col">y_col</code></td>
<td>
<p>an integer defining the index of the column of <code>x$y</code> to be
used for plotting the corresponding Morris statistics <code class="reqn">\mu^*</code>
and <code class="reqn">\sigma</code> (only applies if <code>x$y</code> is a matrix or an 
array).
If set to <code>NULL</code> (as per default) and <code>x$y</code> is a matrix or an 
array, the first column (respectively the first element in
the second dimension) of <code>x$y</code> is used (i.e. <code>y_col = 1</code>).</p>
</td></tr>
<tr><td><code id="morris_+3A_y_dim3">y_dim3</code></td>
<td>
<p>an integer defining the index in the third dimension of 
<code>x$y</code> to be used for plotting the corresponding Morris statistics 
<code class="reqn">\mu^*</code> and <code class="reqn">\sigma</code> (only applies if <code>x$y</code> is an 
array).
If set to <code>NULL</code> (as per default) and <code>x$y</code> is a 
three-dimensional array, the first element in the third dimension of
<code>x$y</code> is used (i.e. <code>y_dim3 = 1</code>).</p>
</td></tr>
<tr><td><code id="morris_+3A_alpha">alpha</code></td>
<td>
<p>a vector of three values between 0.0 (fully transparent) and 1.0
(opaque) (see <code>rgl.material</code>). The first value is for the
cone, the second for the planes.</p>
</td></tr>
<tr><td><code id="morris_+3A_sphere.size">sphere.size</code></td>
<td>
<p>a numeric value, the scale factor for displaying the
spheres.</p>
</td></tr>
<tr><td><code id="morris_+3A_...">...</code></td>
<td>
<p>for <code>morris</code>: any other arguments for <code>model</code> which 
are passed unchanged each time it is called. For <code>plot.morris</code>: 
arguments to be passed to <code>plot.default</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>plot.morris</code> draws the <code class="reqn">(\mu^*,\sigma)</code> graph.
</p>
<p><code>plot3d.morris</code> draws the <code class="reqn">(\mu, \mu^*,\sigma)</code> graph (requires the <span class="pkg">rgl</span> package). On this graph, the
points are in a domain bounded by a cone and two planes (application
of the Cauchy-Schwarz inequality).
</p>
<p>When using the space-filling improvement (Campolongo et al. 2007) of the 
Morris design, we recommend to install before the &quot;pracma&quot; R package: 
its &quot;distmat&quot;&quot; function makes running the function with a large number of 
initial estimates (r2) significantly faster (by accelerating the 
inter-point distances calculations).
</p>
<p>This version of <code>morris</code> also supports matrices and three-dimensional
arrays as output of <code>model</code>.
</p>


<h3>Value</h3>

<p><code>morris</code> returns a list of class <code>"morris"</code>, containing all
the input argument detailed before, plus the following components:
</p>
<table role = "presentation">
<tr><td><code>call</code></td>
<td>
<p>the matched call.</p>
</td></tr>
<tr><td><code>X</code></td>
<td>
<p>a <code>data.frame</code> containing the design of experiments.</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>either a vector, a matrix or a three-dimensional array of model 
responses (depends on the output of <code>model</code>).</p>
</td></tr>
<tr><td><code>ee</code></td>
<td>


<ul>
<li><p>if <code>y</code> is a vector: a <code class="reqn">(r \times p)</code> - matrix of 
elementary effects for all the factors.
</p>
</li>
<li><p>if <code>y</code> is a matrix: a 
<code class="reqn">(r \times p \times ncol(y))</code> - array of 
elementary effects for all the factors and all columns of <code>y</code>.
</p>
</li>
<li><p>if <code>y</code> is a three-dimensional array: a 
<code class="reqn">(r \times p \times dim(y)[2] \times dim(y)[3])</code> - array of elementary effects for all 
the factors and all elements of the second and third dimension of 
<code>y</code>.
</p>
</li></ul>

</td></tr>
</table>
<p>Notice that the statistics of interest (<code class="reqn">\mu</code>, <code class="reqn">\mu^*</code>
and <code class="reqn">\sigma</code>) are not stored. They can be printed by the
<code>print</code> method, but to extract numerical values, one has to
compute them with the following instructions:
</p>
<p>If <code>x$y</code> is a vector:
</p>
<pre>
mu &lt;- apply(x$ee, 2, mean)
mu.star &lt;- apply(x$ee, 2, function(x) mean(abs(x)))
sigma &lt;- apply(x$ee, 2, sd)
</pre>
<p>If <code>x$y</code> is a matrix:
</p>
<pre>
mu &lt;- apply(x$ee, 3, function(M){
  apply(M, 2, mean)
})
mu.star &lt;- apply(abs(x$ee), 3, function(M){
  apply(M, 2, mean)
})
sigma &lt;- apply(x$ee, 3, function(M){
  apply(M, 2, sd)
})
</pre>
<p>If <code>x$y</code> is a three-dimensional array:
</p>
<pre>
mu &lt;- sapply(1:dim(x$ee)[4], function(i){
  apply(x$ee[, , , i, drop = FALSE], 3, function(M){
    apply(M, 2, mean)
  })
}, simplify = "array")
mu.star &lt;- sapply(1:dim(x$ee)[4], function(i){
  apply(abs(x$ee)[, , , i, drop = FALSE], 3, function(M){
    apply(M, 2, mean)
  })
}, simplify = "array")
sigma &lt;- sapply(1:dim(x$ee)[4], function(i){
  apply(x$ee[, , , i, drop = FALSE], 3, function(M){
    apply(M, 2, sd)
  })
}, simplify = "array")
  </pre>
<p>It is highly recommended to use the function with the argument 
<code>scale = TRUE</code> to avoid an uncorrect interpretation of factors that
would have different orders of magnitude.
</p>


<h3>Warning messages</h3>


<dl>
<dt>&quot;keeping r' repetitions out of r&quot;</dt><dd><p>when generating the design of
experiments, identical repetitions are removed, leading to a lower
number than requested.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Gilles Pujol, with contributions from Frank Weber (2016)
</p>


<h3>References</h3>

<p>M. D. Morris, 1991, <em>Factorial sampling plans for preliminary
computational experiments</em>, Technometrics, 33, 161&ndash;174.
</p>
<p>F. Campolongo, J. Cariboni and A. Saltelli, 2007, <em>An effective
screening design for sensitivity</em>, Environmental Modelling and
Software, 22, 1509&ndash;1518.
</p>
<p>G. Pujol, 2009, <em>Simplex-based screening designs for estimating
metamodels</em>, Reliability Engineering and System Safety 94, 1156&ndash;1160.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+morrisMultOut">morrisMultOut</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Test case : the non-monotonic function of Morris
x &lt;- morris(model = morris.fun, factors = 20, r = 4,
            design = list(type = "oat", levels = 5, grid.jump = 3))
print(x)
plot(x)

library(rgl)
plot3d.morris(x)  # (requires the package 'rgl')


# Only for demonstration purposes: a model function returning a matrix
morris.fun_matrix &lt;- function(X){
  res_vector &lt;- morris.fun(X)
  cbind(res_vector, 2 * res_vector)
}
x &lt;- morris(model = morris.fun_matrix, factors = 20, r = 4,
            design = list(type = "oat", levels = 5, grid.jump = 3))
plot(x, y_col = 2)
title(main = "y_col = 2")

# Also only for demonstration purposes: a model function returning a
# three-dimensional array
morris.fun_array &lt;- function(X){
  res_vector &lt;- morris.fun(X)
  res_matrix &lt;- cbind(res_vector, 2 * res_vector)
  array(data = c(res_matrix, 5 * res_matrix), 
        dim = c(length(res_vector), 2, 2))
}
x &lt;- morris(model = morris.fun_array, factors = 20, r = 4,
            design = list(type = "simplex", scale.factor = 1))
plot(x, y_col = 2, y_dim3 = 2)
title(main = "y_col = 2, y_dim3 = 2")

</code></pre>

<hr>
<h2 id='morrisMultOut'>Morris's Elementary Effects Screening Method for Multidimensional Outputs</h2><span id='topic+morrisMultOut'></span><span id='topic+tell.morrisMultOut'></span>

<h3>Description</h3>

<p><code>morrisMultOut</code> extend the Morris's elementary effects screening
method (Morris 1991) to model with multidimensional outputs. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>morrisMultOut(model = NULL, factors, r, design, binf = 0, bsup = 1,
       scale = TRUE, ...)
## S3 method for class 'morrisMultOut'
tell(x, y = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="morrisMultOut_+3A_model">model</code></td>
<td>
<p>NULL or a function returning a outputs a matrix having as columns the model outputs.</p>
</td></tr>
<tr><td><code id="morrisMultOut_+3A_factors">factors</code></td>
<td>
<p>an integer giving the number of factors, or a vector of
character strings giving their names.</p>
</td></tr>
<tr><td><code id="morrisMultOut_+3A_r">r</code></td>
<td>
<p>either an integer giving the number of repetitions of the design,
i.e. the number of elementary effect computed per factor, or a
vector of two integers <code>c(r1, r2)</code> for the space-filling
improvement (Campolongo et al. 2007). In this case, <code>r1</code> is the
wanted design size, and <code>r2</code> (<code class="reqn">&gt; \code{r1}</code>)
is the size of the (bigger) population in which is extracted the
design (this can throw a warning, see below).
</p>
</td></tr>
<tr><td><code id="morrisMultOut_+3A_design">design</code></td>
<td>
<p>a list specifying the design type and its
parameters:
</p>

<ul>
<li> <p><code>type = "oat"</code> for Morris's OAT design (Morris 1991),
with the parameters:
</p>

<ul>
<li> <p><code>levels</code> : either an integer specifying the number of
levels of the design, or a vector of integers for different
values for each factor.
</p>
</li>
<li> <p><code>grid.jump</code> : either an integer specifying the number of
levels that are increased/decreased for computing the
elementary effects, or a vector of integers for different values
for each factor. If not given, it is set to <code>grid.jump =
	  1</code>. Notice that this default value of one does not follow
Morris's recommendation of 
<code class="reqn">\texttt{levels} / 2</code>.
</p>
</li></ul>

</li>
<li> <p><code>type = "simplex"</code> for simplex-based design (Pujol
2009), with the parameter:
</p>

<ul>
<li> <p><code>scale.factor</code> : a numeric value, the homothety factor of
the (isometric) simplexes. Edges equal one with a scale factor
of one.
</p>
</li></ul>

</li></ul>

</td></tr>
<tr><td><code id="morrisMultOut_+3A_binf">binf</code></td>
<td>
<p>either an integer, specifying the minimum value for the
factors, or a vector for different values for each factor.</p>
</td></tr>
<tr><td><code id="morrisMultOut_+3A_bsup">bsup</code></td>
<td>
<p>either an integer, specifying the maximum value for the
factors, or a vector for different values for each factor.</p>
</td></tr>
<tr><td><code id="morrisMultOut_+3A_scale">scale</code></td>
<td>
<p>logical. If <code>TRUE</code>, the input design of experiments is
scaled after building the design and before computing the elementary 
effects so that all factors vary within the range [0,1]. For each factor, 
the scaling is done relatively to its corresponding bsup and binf.</p>
</td></tr>
<tr><td><code id="morrisMultOut_+3A_x">x</code></td>
<td>
<p>a list of class <code>"morris"</code> storing the state of the
screening study (parameters, data, estimates).</p>
</td></tr>
<tr><td><code id="morrisMultOut_+3A_y">y</code></td>
<td>
<p>a vector of model responses.</p>
</td></tr>
<tr><td><code id="morrisMultOut_+3A_...">...</code></td>
<td>
<p>for <code>morrisMultOut</code>: any other arguments for <code>model</code> which 
are passed unchanged each time it is called. For <code>plot.morris</code>: 
arguments to be passed to <code>plot.default</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>All the methods available for object of class <code>"morris"</code> are available also for objects of class <code>"morrisMultOut"</code>.
See the documentation relative to the function <code>"morris"</code> for more details.
</p>


<h3>Value</h3>

<p><code>morrisMultOut</code> returns a list of class <code>"c(morrisMultOut, morris)"</code>, containing all
the input argument detailed before, plus the following components:
</p>
<table role = "presentation">
<tr><td><code>call</code></td>
<td>
<p>the matched call.</p>
</td></tr>
<tr><td><code>X</code></td>
<td>
<p>a <code>data.frame</code> containing the design of experiments.</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>a matrix having as columns the model responses.</p>
</td></tr>
<tr><td><code>ee</code></td>
<td>
<p>a vector of aggregated elementary effects.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Filippo Monari
</p>


<h3>References</h3>

<p>Monari F. and P. Strachan, 2017. <em>Characterization of an airflow network model by sensitivity analysis: parameter screening, fixing, prioritizing and mapping</em>.
Journal of Building Performance Simulation, 2017, 10, 17-36.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+morris">morris</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
  mdl &lt;- function (X) t(atantemp.fun(X))

  x = morrisMultOut(model = mdl, factors = 4, r = 50, 
  design = list(type = "oat", levels = 5, grid.jump = 3), binf = -1, bsup = 5, 
    scale = FALSE)
  print(x)
  plot(x)

  x = morrisMultOut(model = NULL, factors = 4, r = 50, 
  design = list(type = "oat", levels = 5, grid.jump = 3), binf = -1, bsup = 5, 
    scale = FALSE)
  Y = mdl(x[['X']])
  tell(x, Y)	
  print(x)
  plot(x)	
</code></pre>

<hr>
<h2 id='parameterSets'>
Generate parameter sets
</h2><span id='topic+parameterSets'></span>

<h3>Description</h3>

<p>Generate parameter sets from given ranges, with chosen sampling scheme
</p>


<h3>Usage</h3>

<pre><code class='language-R'>parameterSets(par.ranges, samples, method = c("sobol", "innergrid", "grid"))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="parameterSets_+3A_par.ranges">par.ranges</code></td>
<td>

<p>A named list of minimum and maximum parameter values
</p>
</td></tr>
<tr><td><code id="parameterSets_+3A_samples">samples</code></td>
<td>

<p>Number of samples to generate. For the <code>"grid"</code> and <code>"innergrid"</code>
method, may be a vector of number of samples for each parameter.
</p>
</td></tr>
<tr><td><code id="parameterSets_+3A_method">method</code></td>
<td>

<p>the sampling scheme; see Details
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Method <code>"sobol"</code> generates uniformly distributed Sobol low discrepancy numbers, 
using the sobol function in the randtoolbox package.
</p>
<p>Method <code>"grid"</code> generates a grid within the parameter ranges, including its extremes, 
with number of points determined by <code>samples</code>
</p>
<p>Method <code>"innergrid"</code> generates a grid within the parameter ranges, with edges 
of the grid offset from the extremes. The offset is calculated as half 
of the resolution of the grid <code>diff(par.ranges)/samples/2</code>.
</p>


<h3>Value</h3>

<p>the result is a <code>matrix</code>, with named columns for each parameter in <code>par.ranges</code>. 
Each row represents one parameter set.
</p>


<h3>Author(s)</h3>

<p>Joseph Guillaume, based on similar function by Felix Andrews
</p>


<h3>See Also</h3>

<p><code><a href="#topic+delsa">delsa</a></code>, which uses this function
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
X.grid &lt;- parameterSets(par.ranges=list(V1=c(1,1000),V2=c(1,4)),
                          samples=c(10,10),method="grid")
plot(X.grid)

X.innergrid&lt;-parameterSets(par.ranges=list(V1=c(1,1000),V2=c(1,4)),
                          samples=c(10,10),method="innergrid")
points(X.innergrid,col="red")


library(randtoolbox)
X.sobol&lt;-parameterSets(par.ranges=list(V1=c(1,1000),V2=c(1,4)),
                           samples=100,method="sobol")
plot(X.sobol)

</code></pre>

<hr>
<h2 id='pcc'>Partial Correlation Coefficients</h2><span id='topic+pcc'></span><span id='topic+print.pcc'></span><span id='topic+plot.pcc'></span><span id='topic+ggplot.pcc'></span>

<h3>Description</h3>

<p><code>pcc</code> computes the Partial Correlation Coefficients (PCC),
Semi-Partial Correlation Coefficients (SPCC), Partial Rank Correlation 
Coefficients (PRCC) or Semi-Partial Rank Correlation Coefficients (SPRCC), 
which are variance-based measures based on linear (resp. monotonic) 
assumptions, in the case of (linearly) correlated factors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pcc(X, y, rank = FALSE, semi = FALSE, logistic = FALSE, nboot = 0, conf = 0.95)
## S3 method for class 'pcc'
print(x, ...)
## S3 method for class 'pcc'
plot(x, ylim = c(-1,1), ...)
## S3 method for class 'pcc'
ggplot(data, mapping = aes(), ..., environment
                 = parent.frame(), ylim = c(-1,1))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="pcc_+3A_x">X</code></td>
<td>
<p>a data frame (or object coercible by <code>as.data.frame</code>)
containing the design of experiments (model input variables).</p>
</td></tr>
<tr><td><code id="pcc_+3A_y">y</code></td>
<td>
<p>a vector containing the responses corresponding to the design
of experiments (model output variables).</p>
</td></tr>
<tr><td><code id="pcc_+3A_rank">rank</code></td>
<td>
<p>logical. If <code>TRUE</code>, the analysis is done on the
ranks.</p>
</td></tr>
<tr><td><code id="pcc_+3A_semi">semi</code></td>
<td>
<p>logical. If <code>TRUE</code>, semi-PCC are computed.</p>
</td></tr>
<tr><td><code id="pcc_+3A_logistic">logistic</code></td>
<td>
<p>logical. If <code>TRUE</code>, the analysis is done via a
logistic regression (binomial GLM).</p>
</td></tr>
<tr><td><code id="pcc_+3A_nboot">nboot</code></td>
<td>
<p>the number of bootstrap replicates.</p>
</td></tr>
<tr><td><code id="pcc_+3A_conf">conf</code></td>
<td>
<p>the confidence level of the bootstrap confidence intervals.</p>
</td></tr>
<tr><td><code id="pcc_+3A_x">x</code></td>
<td>
<p>the object returned by <code>pcc</code>.</p>
</td></tr>
<tr><td><code id="pcc_+3A_data">data</code></td>
<td>
<p>the object returned by <code>pcc</code>.</p>
</td></tr>
<tr><td><code id="pcc_+3A_ylim">ylim</code></td>
<td>
<p>the y-coordinate limits of the plot.</p>
</td></tr>
<tr><td><code id="pcc_+3A_mapping">mapping</code></td>
<td>
<p>Default list of aesthetic mappings to use for plot. If not specified, 
must be supplied in each layer added to the plot.</p>
</td></tr>
<tr><td><code id="pcc_+3A_environment">environment</code></td>
<td>
<p>[Deprecated] Used prior to tidy evaluation.</p>
</td></tr>
<tr><td><code id="pcc_+3A_...">...</code></td>
<td>
<p>arguments to be passed to methods, such as graphical
parameters (see <code>par</code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Logistic regression model (<code>logistic = TRUE</code>) and rank-based indices
(<code>rank = TRUE</code>) are incompatible.
</p>


<h3>Value</h3>

<p><code>pcc</code> returns a list of class <code>"pcc"</code>, containing the following
components:
</p>
<table role = "presentation">
<tr><td><code>call</code></td>
<td>
<p>the matched call.</p>
</td></tr>
<tr><td><code>PCC</code></td>
<td>
<p>a data frame containing the estimations of the PCC
indices, bias and confidence intervals (if <code>rank = TRUE</code> 
and <code>semi = FALSE</code>).</p>
</td></tr>
<tr><td><code>PRCC</code></td>
<td>
<p>a data frame containing the estimations of the PRCC
indices, bias and confidence intervals (if <code>rank = TRUE</code>
and <code>semi = FALSE</code>).</p>
</td></tr>
<tr><td><code>SPCC</code></td>
<td>
<p>a data frame containing the estimations of the PCC
indices, bias and confidence intervals (if <code>rank = TRUE</code> 
and <code>semi = TRUE</code>).</p>
</td></tr>
<tr><td><code>SPRCC</code></td>
<td>
<p>a data frame containing the estimations of the PRCC
indices, bias and confidence intervals (if <code>rank = TRUE</code>
and <code>semi = TRUE</code>).</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Gilles Pujol and Bertrand Iooss
</p>


<h3>References</h3>

<p>L. Clouvel, B. Iooss, V. Chabridon, M. Il Idrissi and F. Robin, 2023,
<em>An overview of variance-based importance measures in the linear regression context: 
comparative analyses and numerical tests</em>, Preprint.
<a href="https://hal.science/hal-04102053">https://hal.science/hal-04102053</a>
</p>
<p>B. Iooss, V. Chabridon and V. Thouvenot, <em>Variance-based importance 
measures for machine learning model interpretability</em>, Congres lambda-mu23,
Saclay, France, 10-13 octobre 2022
<a href="https://hal.science/hal-03741384">https://hal.science/hal-03741384</a>
</p>
<p>J.W. Johnson and J.M. LeBreton, 2004, <em>History and use of relative 
importance indices in organizational research</em>, Organizational 
Research Methods, 7:238-257.
</p>
<p>A. Saltelli, K. Chan and E. M. Scott eds, 2000, <em>Sensitivity
Analysis</em>, Wiley.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+src">src</a></code>, <code><a href="#topic+lmg">lmg</a></code>, <code><a href="#topic+pmvd">pmvd</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# a 100-sample with X1 ~ U(0.5, 1.5)
#                   X2 ~ U(1.5, 4.5)
#                   X3 ~ U(4.5, 13.5)
library(boot)
n &lt;- 100
X &lt;- data.frame(X1 = runif(n, 0.5, 1.5),
                X2 = runif(n, 1.5, 4.5),
                X3 = runif(n, 4.5, 13.5))

# linear model : Y = X1^2 + X2 + X3
y &lt;- with(X, X1^2 + X2 + X3)

# sensitivity analysis
x &lt;- pcc(X, y, nboot = 100)
print(x)
plot(x)

library(ggplot2)
ggplot(x)
ggplot(x, ylim = c(-1.5,1.5))

x &lt;- pcc(X, y, semi = TRUE, nboot = 100)
print(x)
plot(x)

</code></pre>

<hr>
<h2 id='PLI'>Perturbed-Law based sensitivity Indices (PLI) for failure probability</h2><span id='topic+PLI'></span>

<h3>Description</h3>

<p>PLI computes the Perturbed-Law based Indices (PLI), also known as the
Density Modification Based Reliability Sensitivity Indices (DMBRSI), 
which are robustness indices related to a probability of exceedence of a 
model output (i.e. a failure probability), estimated by a Monte Carlo method. 
See Lemaitre et al. (2015).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PLI(failurepoints,failureprobabilityhat,samplesize,deltasvector,
       InputDistributions,type="MOY",samedelta=TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="PLI_+3A_failurepoints">failurepoints</code></td>
<td>
<p>a matrix of failure points coordinates, 
one column per variable.</p>
</td></tr>
<tr><td><code id="PLI_+3A_failureprobabilityhat">failureprobabilityhat</code></td>
<td>
<p>the estimation of failure probability P 
through rough Monte Carlo method.</p>
</td></tr>
<tr><td><code id="PLI_+3A_samplesize">samplesize</code></td>
<td>
<p>the size of the sample used to estimate P. 
One must have Pchap=dim(failurepoints)[1]/samplesize</p>
</td></tr>
<tr><td><code id="PLI_+3A_deltasvector">deltasvector</code></td>
<td>
<p>a vector containing the values of delta for which
the indices will be computed.</p>
</td></tr>
<tr><td><code id="PLI_+3A_inputdistributions">InputDistributions</code></td>
<td>
<p>a list of list. Each list contains, as a list,
the name of the distribution to be used and the parameters.
Implemented cases so far:
</p>

<ul>
<li><p> For a mean perturbation: Gaussian, Uniform, Triangle,
Left Trucated Gaussian, Left Truncated Gumbel. Using Gumbel
requires the package <code>evd</code>.
</p>
</li>
<li><p> For a variance perturbation: Gaussian, Uniform.
</p>
</li></ul>
</td></tr>
<tr><td><code id="PLI_+3A_type">type</code></td>
<td>
<p>a character string in which the user will specify the type of 
perturbation wanted. 
The sense of &quot;deltasvector&quot; varies according to the type of perturbation:
</p>

<ul>
<li><p> type can take the value &quot;MOY&quot;,in which case deltasvector is a 
vector of perturbated means.
</p>
</li>
<li><p> type can take the value &quot;VAR&quot;,in which case deltasvector is a 
vector of perturbated variances, therefore needs to be positive integers.
</p>
</li></ul>
</td></tr>
<tr><td><code id="PLI_+3A_samedelta">samedelta</code></td>
<td>
<p>a boolean used with the value &quot;MOY&quot; for type. 
</p>

<ul>
<li><p> If it is set at TRUE, the mean perturbation will be the same for all  
the variables. 
</p>
</li>
<li><p> If not, the mean perturbation will be new_mean = mean+sigma*delta 
where mean, sigma are parameters defined in InputDistributions and 
delta is a value of deltasvector.
</p>
</li></ul>
</td></tr>
</table>


<h3>Value</h3>

<p><code>PLI</code> returns a list of matrices, containing:
</p>

<ul>
<li><p> A matrix where the PLI are stored. Each column corresponds to 
an input, each line corresponds to a twist of amplitude delta.
</p>
</li>
<li><p> A matrix where their standard deviation are stored.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Paul Lemaitre and Bertrand Iooss
</p>


<h3>References</h3>

<p>C. Gauchy and J. Stenger and R. Sueur and B. Iooss, <em>An information geometry
approach for robustness analysis in uncertainty quantification of computer codes</em>,
Technometrics, 64:80-91, 2022.
</p>
<p>P. Lemaitre, E. Sergienko, A. Arnaud, N. Bousquet, F. Gamboa and B. Iooss,
<em>Density modification based reliability sensitivity analysis</em>, Journal of Statistical 
Computation and Simulation, 85:1200-1223. 
</p>
<p>E. Borgonovo and B. Iooss, 2017, <em>Moment independent importance measures and a common rationale</em>,
In: <em>Springer Handbook on UQ</em>, R. Ghanem, D. Higdon and H. Owhadi (Eds).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+PLIquantile">PLIquantile</a>, <a href="#topic+PLIquantile_multivar">PLIquantile_multivar</a>, <a href="#topic+PLIsuperquantile">PLIsuperquantile</a>, 
  <a href="#topic+PLIsuperquantile_multivar">PLIsuperquantile_multivar</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

# Model: Ishigami function with a treshold at -7
# Failure points are those &lt; -7

  distributionIshigami = list()
	for (i in 1:3){
		distributionIshigami[[i]]=list("unif",c(-pi,pi))
		distributionIshigami[[i]]$r=("runif")
	}
  
# Monte Carlo sampling to obtain failure points

  N = 100000
	X = matrix(0,ncol=3,nrow=N)
	for( i in 1:3) X[,i] = runif(N,-pi,pi)
	T = ishigami.fun(X)
	s = sum(as.numeric(T &lt; -7)) # Number of failure
	pdefchap = s/N      # Failure probability
	ptsdef = X[T &lt; -7,]	# Failure points
	
# sensitivity indices with perturbation of the mean 
  
	v_delta = seq(-3,3,1/20) 
	Toto = PLI(failurepoints=ptsdef,failureprobabilityhat=pdefchap,samplesize=N,
		deltasvector=v_delta,InputDistributions=distributionIshigami,type="MOY",
		samedelta=TRUE)
	BIshm = Toto[[1]]
	SIshm = Toto[[2]]

	par(mfrow=c(1,1),mar=c(4,5,1,1))
	plot(v_delta,BIshm[,2],ylim=c(-4,4),xlab=expression(delta),
		ylab=expression(hat(PLI[i*delta])),pch=19,cex=1.5)
	points(v_delta,BIshm[,1],col="darkgreen",pch=15,cex=1.5)
	points(v_delta,BIshm[,3],col="red",pch=17,cex=1.5)
	lines(v_delta,BIshm[,2]+1.96*SIshm[,2],col="black")
	lines(v_delta,BIshm[,2]-1.96*SIshm[,2],col="black")
	lines(v_delta,BIshm[,1]+1.96*SIshm[,1],col="darkgreen")
	lines(v_delta,BIshm[,1]-1.96*SIshm[,1],col="darkgreen")
	lines(v_delta,BIshm[,3]+1.96*SIshm[,3],col="red")
	lines(v_delta,BIshm[,3]-1.96*SIshm[,3],col="red")
	abline(h=0,lty=2)
	legend(0,3,legend=c("X1","X2","X3"),
		col=c("darkgreen","black","red"),pch=c(15,19,17),cex=1.5)
  
# sensitivity indices with perturbation of the variance 

	v_delta = seq(1,5,1/4) # user parameter. (the true variance is 3.29)	
	Toto = PLI(failurepoints=ptsdef,failureprobabilityhat=pdefchap,samplesize=N,
		deltasvector=v_delta,InputDistributions=distributionIshigami,type="VAR",
		samedelta=TRUE)
	BIshv=Toto[[1]]
	SIshv=Toto[[2]]

	par(mfrow=c(2,1),mar=c(1,5,1,1)+0.1)
	plot(v_delta,BIshv[,2],ylim=c(-.5,.5),xlab=expression(V_f),
		ylab=expression(hat(PLI[i*delta])),pch=19,cex=1.5)
	points(v_delta,BIshv[,1],col="darkgreen",pch=15,cex=1.5)
	points(v_delta,BIshv[,3],col="red",pch=17,cex=1.5)
	lines(v_delta,BIshv[,2]+1.96*SIshv[,2],col="black")
	lines(v_delta,BIshv[,2]-1.96*SIshv[,2],col="black")
	lines(v_delta,BIshv[,1]+1.96*SIshv[,1],col="darkgreen")
	lines(v_delta,BIshv[,1]-1.96*SIshv[,1],col="darkgreen")
	lines(v_delta,BIshv[,3]+1.96*SIshv[,3],col="red")
	lines(v_delta,BIshv[,3]-1.96*SIshv[,3],col="red")

	par(mar=c(4,5.1,1.1,1.1))
	plot(v_delta,BIshv[,2],ylim=c(-30,.7),xlab=expression(V[f]),
		ylab=expression(hat(PLI[i*delta])),pch=19,cex=1.5)
	points(v_delta,BIshv[,1],col="darkgreen",pch=15,cex=1.5)
	points(v_delta,BIshv[,3],col="red",pch=17,cex=1.5)
	lines(v_delta,BIshv[,2]+1.96*SIshv[,2],col="black")
	lines(v_delta,BIshv[,2]-1.96*SIshv[,2],col="black")
	lines(v_delta,BIshv[,1]+1.96*SIshv[,1],col="darkgreen")
	lines(v_delta,BIshv[,1]-1.96*SIshv[,1],col="darkgreen")
	lines(v_delta,BIshv[,3]+1.96*SIshv[,3],col="red")
	lines(v_delta,BIshv[,3]-1.96*SIshv[,3],col="red")
	legend(2.5,-10,legend=c("X1","X2","X3"),col=c("darkgreen","black","red"),
		pch=c(15,19,17),cex=1.5)
  
##############################################################
# Example with an inverse probability transform 
# (to obtain Gaussian inputs from Uniform ones)

# Monte Carlo sampling (the inputs are Uniform)

  N = 100000
	X = matrix(0,ncol=3,nrow=N)
	for( i in 1:3) X[,i] = runif(N,-pi,pi)
	T = ishigami.fun(X)
	s = sum(as.numeric(T &lt; -7)) # Number of failure
	pdefchap = s/N      # Failure probability
	
# Empirical transform (applied on the sample)

  Xn &lt;- matrix(0,nrow=N,ncol=3)
  for (i in 1:3){
    ecdfx &lt;- ecdf(X[,i])
    q &lt;- ecdfx(X[,i])
    Xn[,i] &lt;- qnorm(q) # Gaussian anamorphosis
    # infinite max values =&gt; putting the symetrical values of min values
    Xn[which(Xn[,i]==Inf),i] &lt;- - Xn[which.min(Xn[,i]),i] 
    }
# Visualization of a perturbed density (the one of X1 perturbed on the mean)
  delta_mean_gauss &lt;- 1 # perturbed value on the mean of the Gaussian transform
  Xtr &lt;- quantile(ecdfx,pnorm(Xn[,1] + delta_mean_gauss)) # backtransform
	par(mfrow=c(1,1))
  plot(density(Xtr), col="red") ; lines(density(X[,1]))
  
# sensitivity indices with perturbation of the mean 
  
  distributionIshigami = list()
	for (i in 1:3){
		distributionIshigami[[i]]=list("norm",c(0,1))
		distributionIshigami[[i]]$r=("rnorm")
	}
	
	ptsdef = Xn[T &lt; -7,]	# Failure points # failure points with Gaussian distrib.
	
	v_delta = seq(-1.5,1.5,1/20) 
	Toto = PLI(failurepoints=ptsdef,failureprobabilityhat=pdefchap,samplesize=N,
		deltasvector=v_delta,InputDistributions=distributionIshigami,type="MOY",
		samedelta=TRUE)
	BIshm = Toto[[1]]
	SIshm = Toto[[2]]

	par(mfrow=c(1,1),mar=c(4,5,1,1))
	plot(v_delta,BIshm[,2],ylim=c(-4,4),xlab=expression(delta),
		ylab=expression(hat(PLI[i*delta])),pch=19,cex=1.5)
	points(v_delta,BIshm[,1],col="darkgreen",pch=15,cex=1.5)
	points(v_delta,BIshm[,3],col="red",pch=17,cex=1.5)
	lines(v_delta,BIshm[,2]+1.96*SIshm[,2],col="black")
	lines(v_delta,BIshm[,2]-1.96*SIshm[,2],col="black")
	lines(v_delta,BIshm[,1]+1.96*SIshm[,1],col="darkgreen")
	lines(v_delta,BIshm[,1]-1.96*SIshm[,1],col="darkgreen")
	lines(v_delta,BIshm[,3]+1.96*SIshm[,3],col="red")
	lines(v_delta,BIshm[,3]-1.96*SIshm[,3],col="red")
	abline(h=0,lty=2)
	legend(0,3,legend=c("X1","X2","X3"),
		col=c("darkgreen","black","red"),pch=c(15,19,17),cex=1.5)
  

</code></pre>

<hr>
<h2 id='PLIquantile'>Perturbed-Law based sensitivity Indices (PLI) for quantile</h2><span id='topic+PLIquantile'></span>

<h3>Description</h3>

<p>PLIquantile computes the Perturbed-Law based Indices (PLI) for quantile, 
which are robustness indices related to a quantile of a 
model output, estimated by a Monte Carlo method, See Sueur et al. (2017)
and Iooss et al. (2020).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PLIquantile(order,x,y,deltasvector,InputDistributions,type="MOY",samedelta=TRUE,
            percentage=TRUE,nboot=0,conf=0.95,bootsample=TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="PLIquantile_+3A_order">order</code></td>
<td>
<p>the order of the quantile to estimate.</p>
</td></tr>
<tr><td><code id="PLIquantile_+3A_x">x</code></td>
<td>
<p>the matrix of simulation points coordinates, 
one column per variable.</p>
</td></tr>
<tr><td><code id="PLIquantile_+3A_y">y</code></td>
<td>
<p>the vector of model outputs.</p>
</td></tr>
<tr><td><code id="PLIquantile_+3A_deltasvector">deltasvector</code></td>
<td>
<p>a vector containing the values of delta for which
the indices will be computed.</p>
</td></tr>
<tr><td><code id="PLIquantile_+3A_inputdistributions">InputDistributions</code></td>
<td>
<p>a list of list. Each list contains, as a list,
the name of the distribution to be used and the parameters.
Implemented cases so far:
</p>

<ul>
<li><p> For a mean perturbation: Gaussian, Uniform, Triangle,
Left Trucated Gaussian, Left Truncated Gumbel. Using Gumbel
requires the package <code>evd</code>.
</p>
</li>
<li><p> For a variance perturbation: Gaussian, Uniform.
</p>
</li></ul>
</td></tr>
<tr><td><code id="PLIquantile_+3A_type">type</code></td>
<td>
<p>a character string in which the user will specify the type of 
perturbation wanted. 
The sense of &quot;deltasvector&quot; varies according to the type of perturbation:
</p>

<ul>
<li><p> type can take the value &quot;MOY&quot;,in which case deltasvector is a 
vector of perturbated means.
</p>
</li>
<li><p> type can take the value &quot;VAR&quot;,in which case deltasvector is a 
vector of perturbated variances, therefore needs to be positive integers.
</p>
</li></ul>
</td></tr>
<tr><td><code id="PLIquantile_+3A_samedelta">samedelta</code></td>
<td>
<p>a boolean used with the value &quot;MOY&quot; for type. 
</p>

<ul>
<li><p> If it is set at TRUE, the mean perturbation will be the same for all  
the variables. 
</p>
</li>
<li><p> If not, the mean perturbation will be new_mean = mean+sigma*delta 
where mean, sigma are parameters defined in InputDistributions and 
delta is a value of deltasvector.
</p>
</li></ul>
</td></tr>
<tr><td><code id="PLIquantile_+3A_percentage">percentage</code></td>
<td>
<p>a boolean that defines the formula used for the PLI.
</p>

<ul>
<li><p> If it is set at FALSE, the initially proposed formula is used
(see Sueur et al., 2017).
</p>
</li>
<li><p> If not (set as TRUE), the PLI is given in percentage of variation 
of the quantile (see Iooss et al., 2020).
</p>
</li></ul>
</td></tr>
<tr><td><code id="PLIquantile_+3A_nboot">nboot</code></td>
<td>
<p>the number of bootstrap replicates.</p>
</td></tr>
<tr><td><code id="PLIquantile_+3A_conf">conf</code></td>
<td>
<p>the confidence level for bootstrap confidence intervals.</p>
</td></tr>
<tr><td><code id="PLIquantile_+3A_bootsample">bootsample</code></td>
<td>
<p>If TRUE, the uncertainty about the original quantile estimation
is taken into account in the PLI confidence intervals (see Iooss et al., 2021). 
If FALSE, standard confidence intervals are computed for the PLI.
It mainly changes the CI at small delta values.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>PLIquantile</code> returns a list of matrices (each column corresponds to an input, 
each line corresponds to a twist of amplitude delta) 
containing the following components:
</p>
<table role = "presentation">
<tr><td><code>PLI</code></td>
<td>
<p>the PLI.</p>
</td></tr>
<tr><td><code>PLICIinf</code></td>
<td>
<p>the bootstrap lower confidence interval values of the PLI.</p>
</td></tr>
<tr><td><code>PLICIsup</code></td>
<td>
<p>the bootstrap upper confidence interval values of the PLI.</p>
</td></tr>
<tr><td><code>quantile</code></td>
<td>
<p>the perturbed quantile.</p>
</td></tr>
<tr><td><code>quantileCIinf</code></td>
<td>
<p>the bootstrap lower confidence interval values of the perturbed quantile.</p>
</td></tr>
<tr><td><code>quantileCIsup</code></td>
<td>
<p>the bootstrap upper confidence interval values of the perturbed quantile.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Paul Lemaitre, Bertrand Iooss, Thibault Delage and Roman Sueur
</p>


<h3>References</h3>

<p>T. Delage, R. Sueur and B. Iooss, 2018, <em>Robustness analysis of epistemic uncertainties 
propagation studies in LOCA assessment thermal-hydraulic model</em>, 
ANS Best Estimate Plus Uncertainty International Conference (BEPU 2018), Lucca, Italy, May 13-19, 2018.
</p>
<p>C. Gauchy, J. Stenger, R. Sueur and B. Iooss, 2022, <em>An information geometry approach 
for robustness analysis in uncertainty quantification of computer codes</em>, 
Technometrics, 64:80-91.
</p>
<p>B. Iooss, V. Verges and V. Larget, 2022, <em>BEPU robustness analysis via perturbed law-based 
sensitivity indices</em>, Proceedings of the Institution of Mechanical Engineers, 
Part O: Journal of Risk and Reliability, 236:855-865.
</p>
<p>P. Lemaitre, E. Sergienko, A. Arnaud, N. Bousquet, F. Gamboa and B. Iooss, 2015, 
<em>Density modification based reliability sensitivity analysis</em>, Journal of Statistical 
Computation and Simulation, 85:1200-1223. 
</p>
<p>R. Sueur, N. Bousquet, B. Iooss and J. Bect, 2016,
<em>Perturbed-Law based sensitivity Indices for sensitivity analysis in structural reliability</em>,
Proceedings of the SAMO 2016 Conference, Reunion Island, France, December 2016.
</p>
<p>R. Sueur, B. Iooss and T. Delage, 2017,
<em>Sensitivity analysis using perturbed-law based indices for quantiles and application to an industrial case</em>, 
10th International Conference on Mathematical Methods in Reliability (MMR 2017), Grenoble, France, July 2017.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+PLI">PLI</a>, <a href="#topic+PLIsuperquantile">PLIsuperquantile</a> <a href="#topic+PLIquantile_multivar">PLIquantile_multivar</a>, 
  <a href="#topic+PLIsuperquantile_multivar">PLIsuperquantile_multivar</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

# Model: 3D function 

  distribution = list()
	for (i in 1:3) distribution[[i]]=list("norm",c(0,1))
  
# Monte Carlo sampling 

  N = 5000
	X = matrix(0,ncol=3,nrow=N)
	for(i in 1:3) X[,i] = rnorm(N,0,1)
     
	Y = 2 * X[,1] + X[,2] + X[,3]/2
	alpha &lt;- 0.95 # quantile order
	
	q95 = quantile(Y,alpha)
	
	nboot=20 # put nboot=200 for consistency
	
# sensitivity indices with perturbation of the mean 
  
	v_delta = seq(-1,1,1/10) 
	toto = PLIquantile(alpha,X,Y,deltasvector=v_delta,
	  InputDistributions=distribution,type="MOY",samedelta=TRUE,
	  percentage=FALSE,nboot=nboot)

# Plotting the PLI

  par(mar=c(4,5,1,1))
	plot(v_delta,toto$PLI[,2],ylim=c(-1.5,1.5),xlab=expression(delta),
		ylab=expression(hat(PLI[i*delta])),pch=19,cex=1.5)
	points(v_delta,toto$PLI[,1],col="darkgreen",pch=15,cex=1.5)
	points(v_delta,toto$PLI[,3],col="red",pch=17,cex=1.5)
	lines(v_delta,toto$PLICIinf[,2],col="black")
	lines(v_delta,toto$PLICIsup[,2],col="black")
	lines(v_delta,toto$PLICIinf[,1],col="darkgreen")
	lines(v_delta,toto$PLICIsup[,1],col="darkgreen")
	lines(v_delta,toto$PLICIinf[,3],col="red")
	lines(v_delta,toto$PLICIsup[,3],col="red")
	abline(h=0,lty=2)
	legend(0.8,1.5,legend=c("X1","X2","X3"),
		col=c("darkgreen","black","red"),pch=c(15,19,17),cex=1.5)
  
# Plotting the perturbed quantiles

  par(mar=c(4,5,1,1))
	plot(v_delta,toto$quantile[,2],ylim=c(1.5,6.5),xlab=expression(delta),
		ylab=expression(hat(q[i*delta])),pch=19,cex=1.5)
	points(v_delta,toto$quantile[,1],col="darkgreen",pch=15,cex=1.5)
	points(v_delta,toto$quantile[,3],col="red",pch=17,cex=1.5)
	lines(v_delta,toto$quantileCIinf[,2],col="black")
	lines(v_delta,toto$quantileCIsup[,2],col="black")
	lines(v_delta,toto$quantileCIinf[,1],col="darkgreen")
	lines(v_delta,toto$quantileCIsup[,1],col="darkgreen")
	lines(v_delta,toto$quantileCIinf[,3],col="red")
	lines(v_delta,toto$quantileCIsup[,3],col="red")
	abline(h=q95,lty=2)
	legend(0.5,2.4,legend=c("X1","X2","X3"),
		col=c("darkgreen","black","red"),pch=c(15,19,17),cex=1.5)
		
###########################################################		
# Plotting the PLI in percentage with refined confidence intervals

	toto = PLIquantile(alpha,X,Y,deltasvector=v_delta,
	  InputDistributions=distribution,type="MOY",samedelta=TRUE,
	  percentage=TRUE,nboot=nboot,bootsample=FALSE)
	  
  par(mar=c(4,5,1,1))
	plot(v_delta,toto$PLI[,2],ylim=c(-0.6,0.6),xlab=expression(delta),
		ylab=expression(hat(PLI[i*delta])),pch=19,cex=1.5)
	points(v_delta,toto$PLI[,1],col="darkgreen",pch=15,cex=1.5)
	points(v_delta,toto$PLI[,3],col="red",pch=17,cex=1.5)
	lines(v_delta,toto$PLICIinf[,2],col="black")
	lines(v_delta,toto$PLICIsup[,2],col="black")
	lines(v_delta,toto$PLICIinf[,1],col="darkgreen")
	lines(v_delta,toto$PLICIsup[,1],col="darkgreen")
	lines(v_delta,toto$PLICIinf[,3],col="red")
	lines(v_delta,toto$PLICIsup[,3],col="red")
	abline(h=0,lty=2)
	legend(0,0.6,legend=c("X1","X2","X3"),
		col=c("darkgreen","black","red"),pch=c(15,19,17),cex=1.5)

###################################################		
# another visualization by using the plotCI() fct 
# (from plotrix package) for the CI plotting(from Vanessa Verges)

  library(plotrix)
  parameters = list(colors=c("darkgreen","black","red"),
                  symbols=c(15,19,17),overlay=c(FALSE,TRUE,TRUE))
  par(mar=c(4,5,1,1),xpd=TRUE)
  for (i in 1:3){
    plotCI(v_delta,toto$PLI[,i],ui=toto$PLICIsup[,i],li=toto$PLICIinf[,i],
         cex=1.5,col=parameters$colors[i],pch=parameters$symbols[i],
         add=parameters$overlay[i], xlab="", ylab="")
  }
  title(xlab=expression(delta),ylab=expression(hat(PLI[i*delta])),
       main=bquote("PLI-quantile (N ="~.(N) ~ ","~alpha~"="~.(alpha)~
       ") of Y="~2*X[1] + X[2] + X[3]/2))
  abline(h=0,lty=2)
  legend("topleft",legend=c("X1","X2","X3"),col=parameters$colors,
          pch=parameters$symbols,cex=1.5)

	

</code></pre>

<hr>
<h2 id='PLIquantile_multivar'>
Perturbed-Law based sensitivity Indices (PLI) for quantile 
and simultaneous perturbations of 2 inputs</h2><span id='topic+PLIquantile_multivar'></span>

<h3>Description</h3>

<p>PLIquantile_multivar computes the Perturbed-Law based Indices (PLI) for quantile
and simultaneous perturbations of the means of 2 inputs, 
estimated by a Monte Carlo method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PLIquantile_multivar(order,x,y,inputs,deltasvector,InputDistributions,samedelta=TRUE,
            percentage=TRUE,nboot=0,conf=0.95,bootsample=TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="PLIquantile_multivar_+3A_order">order</code></td>
<td>
<p>the order of the quantile to estimate.</p>
</td></tr>
<tr><td><code id="PLIquantile_multivar_+3A_x">x</code></td>
<td>
<p>the matrix of simulation points coordinates, 
one column per variable.</p>
</td></tr>
<tr><td><code id="PLIquantile_multivar_+3A_y">y</code></td>
<td>
<p>the vector of model outputs.</p>
</td></tr>
<tr><td><code id="PLIquantile_multivar_+3A_inputs">inputs</code></td>
<td>
<p>the vector of the two inputs' indices for which
the indices will be computed.</p>
</td></tr>  
<tr><td><code id="PLIquantile_multivar_+3A_deltasvector">deltasvector</code></td>
<td>
<p>a vector containing the values of the perturbed means
for which the indices will be computed. Warning: if samedelta=FALSE, 
deltasvector has to be the vector of deltas (mean perturbations)</p>
</td></tr>
<tr><td><code id="PLIquantile_multivar_+3A_inputdistributions">InputDistributions</code></td>
<td>
<p>a list of list. Each list contains, as a list,
the name of the distribution to be used and the parameters.
Implemented cases so far (for a mean perturbation): 
Gaussian, Uniform, Triangle, Left Trucated Gaussian, 
Left Truncated Gumbel. Using Gumbel requires the package <code>evd</code>.</p>
</td></tr>
<tr><td><code id="PLIquantile_multivar_+3A_samedelta">samedelta</code></td>
<td>
<p>a boolean used with the value &quot;MOY&quot; for type. 
</p>

<ul>
<li><p> If it is set at TRUE, the mean perturbation will be the same for all  
the variables. 
</p>
</li>
<li><p> If not, the mean perturbation will be new_mean = mean+sigma*delta 
where mean, sigma are parameters defined in InputDistributions and 
delta is a value of deltasvector.
</p>
</li></ul>
</td></tr>
<tr><td><code id="PLIquantile_multivar_+3A_percentage">percentage</code></td>
<td>
<p>a boolean that defines the formula used for the PLI.
</p>

<ul>
<li><p> If it is set at FALSE, the initially proposed formula is used
(see Sueur et al., 2017).
</p>
</li>
<li><p> If not (set as TRUE), the PLI is given in percentage of variation 
of the quantile (see Iooss et al., 2021).
</p>
</li></ul>
</td></tr>
<tr><td><code id="PLIquantile_multivar_+3A_nboot">nboot</code></td>
<td>
<p>the number of bootstrap replicates.</p>
</td></tr>
<tr><td><code id="PLIquantile_multivar_+3A_conf">conf</code></td>
<td>
<p>the confidence level for bootstrap confidence intervals.</p>
</td></tr>
<tr><td><code id="PLIquantile_multivar_+3A_bootsample">bootsample</code></td>
<td>
<p>If TRUE, the uncertainty about the original quantile estimation
is taken into account in the PLI confidence intervals (see Iooss et al., 2021). 
If FALSE, standard confidence intervals are computed for the PLI.
It mainly changes the CI at small delta values.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function does not allow perturbations on the variance of the inputs' distributions.
</p>


<h3>Value</h3>

<p><code>PLIquantile_multivar</code> returns a list of matrices 
(delta twist of input 1 (in rows) vs. delta twist of input 2 (in columns)) 
containing the following components:
</p>
<table role = "presentation">
<tr><td><code>PLI</code></td>
<td>
<p>the PLI.</p>
</td></tr>
<tr><td><code>PLICIinf</code></td>
<td>
<p>the bootstrap lower confidence interval values of the PLI.</p>
</td></tr>
<tr><td><code>PLICIsup</code></td>
<td>
<p>the bootstrap upper confidence interval values of the PLI.</p>
</td></tr>
<tr><td><code>quantile</code></td>
<td>
<p>the perturbed quantile.</p>
</td></tr>
<tr><td><code>quantileCIinf</code></td>
<td>
<p>the bootstrap lower confidence interval values of the perturbed quantile.</p>
</td></tr>
<tr><td><code>quantileCIsup</code></td>
<td>
<p>the bootstrap upper confidence interval values of the perturbed quantile.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Bertrand Iooss
</p>


<h3>References</h3>

<p>T. Delage, R. Sueur and B. Iooss, 2018, <em>Robustness analysis of epistemic uncertainties 
propagation studies in LOCA assessment thermal-hydraulic model</em>, 
ANS Best Estimate Plus Uncertainty International Conference (BEPU 2018), Lucca, Italy, May 13-19, 2018.
</p>
<p>B. Iooss, V. Verges and V. Larget, 2022, <em>BEPU robustness analysis via perturbed law-based 
sensitivity indices</em>, Proceedings of the Institution of Mechanical Engineers, 
Part O: Journal of Risk and Reliability, 236:855-865.
</p>
<p>P. Lemaitre, E. Sergienko, A. Arnaud, N. Bousquet, F. Gamboa and B. Iooss, 2015, 
<em>Density modification based reliability sensitivity analysis</em>, Journal of Statistical 
Computation and Simulation, 85:1200-1223. 
</p>
<p>R. Sueur, N. Bousquet, B. Iooss and J. Bect, 2016,
<em>Perturbed-Law based sensitivity Indices for sensitivity analysis in structural reliability</em>,
Proceedings of the SAMO 2016 Conference, Reunion Island, France, December 2016.
</p>
<p>R. Sueur, B. Iooss and T. Delage, 2017,
<em>Sensitivity analysis using perturbed-law based indices for quantiles and application to an industrial case</em>, 
10th International Conference on Mathematical Methods in Reliability (MMR 2017), Grenoble, France, July 2017.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+PLI">PLI</a>, <a href="#topic+PLIquantile">PLIquantile</a>, <a href="#topic+PLIsuperquantile">PLIsuperquantile</a>, <a href="#topic+PLIsuperquantile_multivar">PLIsuperquantile_multivar</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

# Model: 3D function 

distribution = list()
for (i in 1:3) distribution[[i]]=list("norm",c(0,1))
N = 5000
X = matrix(0,ncol=3,nrow=N)
for(i in 1:3) X[,i] = rnorm(N,0,1)
Y = 2 * X[,1] + X[,2] + X[,3]/2
alpha &lt;- 0.95
nboot &lt;- 20 # put nboot=200 for consistency

q95 = quantile(Y,alpha)
v_delta = seq(-1,1,1/10) 
toto12 = PLIquantile_multivar(alpha,X,Y,c(1,2),deltasvector=v_delta,
    InputDistributions=distribution,samedelta=TRUE)
toto = PLIquantile(alpha,X,Y,deltasvector=v_delta,InputDistributions=distribution,
    type="MOY",samedelta=TRUE,nboot=0)

par(mar=c(4,5,1,1))
plot(v_delta,diag(toto12$PLI),,ylim=c(-1,1),xlab=expression(delta),
    ylab=expression(hat(PLI[i*delta])),pch=16,cex=1.5,col="blue")
points(v_delta,toto$PLI[,1],col="darkgreen",pch=15,cex=1.5)
points(v_delta,toto$PLI[,2],col="black",pch=19,cex=1.5)
points(v_delta,toto$PLI[,3],col="red",pch=17,cex=1.5)
abline(h=0,lty=2)
legend(-1,1.,legend=c("X1","X2","X3","X1X2"),col=c("darkgreen","black","red","blue"),
    pch=c(15,19,17,16),cex=1.5)

# with bootstrap

v_delta = seq(-1,1,2/10) 

toto12 = PLIquantile_multivar(alpha,X,Y,c(1,2),deltasvector=v_delta,
    InputDistributions=distribution,samedelta=TRUE,nboot=nboot,bootsample=FALSE)
toto = PLIquantile(alpha,X,Y,deltasvector=v_delta,InputDistributions=distribution,
    type="MOY",samedelta=TRUE,nboot=nboot,bootsample=FALSE)

par(mar=c(4,5,1,1))
plot(v_delta,diag(toto12$PLI),ylim=c(-1,1),xlab=expression(delta),
    ylab=expression(hat(PLI[i*delta])),pch=16,cex=1.5,col="blue")
points(v_delta,toto$PLI[,1],col="darkgreen",pch=15,cex=1.5)
points(v_delta,toto$PLI[,2],col="black",pch=19,cex=1.5)
points(v_delta,toto$PLI[,3],col="red",pch=17,cex=1.5)
lines(v_delta,diag(toto12$PLICIinf),col="blue")
lines(v_delta,diag(toto12$PLICIsup),col="blue")
lines(v_delta,toto$PLICIinf[,2],col="black")
lines(v_delta,toto$PLICIsup[,2],col="black")
lines(v_delta,toto$PLICIinf[,1],col="darkgreen")
lines(v_delta,toto$PLICIsup[,1],col="darkgreen")
lines(v_delta,toto$PLICIinf[,3],col="red")
lines(v_delta,toto$PLICIsup[,3],col="red")
abline(h=0,lty=2)
legend(-1,1,legend=c("X1","X2","X3","X1X2"),col=c("darkgreen","black","red","blue"),
    pch=c(15,19,17,16),cex=1.5)

###################################################		
# another visualizations by using the plotrix, 
# viridisLite, lattice and grid packages (from Vanessa Verges)

library(plotrix)

parameters = list(colors=c("darkgreen","black","red"),symbols=c(15,19,17))
par(mar=c(4,5,1,1),xpd=TRUE)
plotCI(v_delta,diag(toto12$PLI),ui=diag(toto12$PLICIsup),li=diag(toto12$PLICIinf),
       xlab=expression(delta),ylab=expression(hat(PLI[i*delta])),
       main=bquote("PLI-quantile (N ="~.(N) ~ ","~alpha~"="~.(alpha)~
       ") on "~X[1]~"and"~X[2]~"of Y="~2*X[1] + X[2] + X[3]/2),
       cex=1.5,col="blue",pch=16)
for (i in 1:3){
  plotCI(v_delta,toto$PLI[,i],ui=toto$PLICIsup[,i],li=toto$PLICIinf[,i],
         cex=1.5,col=parameters$colors[i],pch=parameters$symbols[i],
         add=TRUE)
}
abline(h=0,lty=2)
legend("topleft",legend=c("X1","X2","X3","X1X2"),
        col=c(parameters$colors,"blue"),pch=c(parameters$symbols,16),cex=1.5)

# Visu of all the PLIs (at any paired combinations of deltas)

library(viridisLite)
library(lattice)
library(grid)

colnames(toto12$PLI) = round(v_delta,2)
rownames(toto12$PLI) = round(v_delta,2)
coul = viridis(100)
levelplot(toto12$PLI, col.regions = coul, xlab=bquote(delta[X~.(1)]), 
  ylab=bquote(delta[X~.(2)]), main=bquote(hat(PLI)[quantile[~X[1]~X[2]]]))


</code></pre>

<hr>
<h2 id='PLIsuperquantile'>Perturbed-Law based sensitivity Indices (PLI) for superquantile</h2><span id='topic+PLIsuperquantile'></span>

<h3>Description</h3>

<p>PLIsuperquantile computes the Perturbed-Law based Indices (PLI) for superquantile, 
which are robustness indices related to a superquantile of a 
model output, estimated by a Monte Carlo method. 
See Iooss et al. (2020).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PLIsuperquantile(order,x,y,deltasvector,InputDistributions,type="MOY",
  samedelta=TRUE, percentage=TRUE,nboot=0,conf=0.95,bootsample=TRUE,bias=TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="PLIsuperquantile_+3A_order">order</code></td>
<td>
<p>the order of the superquantile to estimate.</p>
</td></tr>
<tr><td><code id="PLIsuperquantile_+3A_x">x</code></td>
<td>
<p>the matrix of simulation points coordinates, 
one column per variable.</p>
</td></tr>
<tr><td><code id="PLIsuperquantile_+3A_y">y</code></td>
<td>
<p>the vector of model outputs.</p>
</td></tr>
<tr><td><code id="PLIsuperquantile_+3A_deltasvector">deltasvector</code></td>
<td>
<p>a vector containing the values of delta for which
the indices will be computed.</p>
</td></tr>
<tr><td><code id="PLIsuperquantile_+3A_inputdistributions">InputDistributions</code></td>
<td>
<p>a list of list. Each list contains, as a list,
the name of the distribution to be used and the parameters.
Implemented cases so far:
</p>

<ul>
<li><p> For a mean perturbation: Gaussian, Uniform, Triangle,
Left Trucated Gaussian, Left Truncated Gumbel. Using Gumbel
requires the package <code>evd</code>.
</p>
</li>
<li><p> For a variance perturbation: Gaussian, Uniform.
</p>
</li></ul>
</td></tr>
<tr><td><code id="PLIsuperquantile_+3A_type">type</code></td>
<td>
<p>a character string in which the user will specify the type of 
perturbation wanted. 
The sense of &quot;deltasvector&quot; varies according to the type of perturbation:
</p>

<ul>
<li><p> type can take the value &quot;MOY&quot;,in which case deltasvector is a 
vector of perturbated means.
</p>
</li>
<li><p> type can take the value &quot;VAR&quot;,in which case deltasvector is a 
vector of perturbated variances, therefore needs to be positive integers.
</p>
</li></ul>
</td></tr>
<tr><td><code id="PLIsuperquantile_+3A_samedelta">samedelta</code></td>
<td>
<p>a boolean used with the value &quot;MOY&quot; for type. 
</p>

<ul>
<li><p> If it is set at TRUE, the mean perturbation will be the same for all  
the variables. 
</p>
</li>
<li><p> If not, the mean perturbation will be new_mean = mean+sigma*delta 
where mean, sigma are parameters defined in InputDistributions and 
delta is a value of deltasvector.
</p>
</li></ul>
</td></tr>
<tr><td><code id="PLIsuperquantile_+3A_percentage">percentage</code></td>
<td>
<p>a boolean that defines the formula used for the PLI.
</p>

<ul>
<li><p> If it is set at FALSE, the classical formula used in the bibliographical 
references is used.
</p>
</li>
<li><p> If not (set as TRUE), the PLI is given in percentage of variation 
of the superquantile (even if it is negative).
</p>
</li></ul>
</td></tr>
<tr><td><code id="PLIsuperquantile_+3A_nboot">nboot</code></td>
<td>
<p>the number of bootstrap replicates.</p>
</td></tr>
<tr><td><code id="PLIsuperquantile_+3A_conf">conf</code></td>
<td>
<p>the confidence level for bootstrap confidence intervals.</p>
</td></tr>
<tr><td><code id="PLIsuperquantile_+3A_bootsample">bootsample</code></td>
<td>
<p>If TRUE, the uncertainty about the original quantile estimation
is taken into account in the PLI confidence intervals (see Iooss et al., 2020). 
If FALSE, standard confidence intervals are computed for the PLI.
It mainly changes the CI at small delta values.</p>
</td></tr>
<tr><td><code id="PLIsuperquantile_+3A_bias">bias</code></td>
<td>
<p>defines the version of PLI-superquantile:
</p>

<ul>
<li><p> If it is set at &quot;TRUE&quot;, it gives the mean of outputs above the perturbed 
quantile (alternative formula)
</p>
</li>
<li><p> If it is set at &quot;FALSE&quot;, it gives the mean of perturbed outputs above the 
perturbed quantile (original formula)
</p>
</li></ul>
</td></tr>
</table>


<h3>Value</h3>

<p><code>PLIsuperquantile</code> returns a list of matrices (each column corresponds to an input, 
each line corresponds to a twist of amplitude delta) 
containing the following components:
</p>
<table role = "presentation">
<tr><td><code>PLI</code></td>
<td>
<p>the PLI.</p>
</td></tr>
<tr><td><code>PLICIinf</code></td>
<td>
<p>the bootstrap lower confidence interval values of the PLI.</p>
</td></tr>
<tr><td><code>PLICIsup</code></td>
<td>
<p>the bootstrap upper confidence interval values of the PLI.</p>
</td></tr>
<tr><td><code>superquantile</code></td>
<td>
<p>the perturbed superquantile.</p>
</td></tr>
<tr><td><code>superquantileCIinf</code></td>
<td>
<p>the bootstrap lower confidence interval values of the perturbed superquantile.</p>
</td></tr>
<tr><td><code>superquantileCIsup</code></td>
<td>
<p>the bootstrap upper confidence interval values of the perturbed superquantile.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Bertrand Iooss
</p>


<h3>References</h3>

<p>B. Iooss, V. Verges and V. Larget, 2022, <em>BEPU robustness analysis via perturbed law-based 
sensitivity indices</em>, Proceedings of the Institution of Mechanical Engineers, 
Part O: Journal of Risk and Reliability, 236:855-865.
</p>
<p>P. Lemaitre, E. Sergienko, A. Arnaud, N. Bousquet, F. Gamboa and B. Iooss, 2015, 
<em>Density modification based reliability sensitivity analysis</em>, Journal of Statistical 
Computation and Simulation, 85:1200-1223. 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+PLI">PLI</a>, <a href="#topic+PLIquantile">PLIquantile</a>, <a href="#topic+PLIsuperquantile_multivar">PLIsuperquantile_multivar</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

# Model: 3D function 

  distribution = list()
	for (i in 1:3) distribution[[i]]=list("norm",c(0,1))
  
# Monte Carlo sampling 

  N = 10000
	X = matrix(0,ncol=3,nrow=N)
	for(i in 1:3) X[,i] = rnorm(N,0,1)
     
	Y = 2 * X[,1] + X[,2] + X[,3]/2
	alpha &lt;- 0.95
	
	q95 = quantile(Y,alpha)
  sq95a &lt;- mean(Y*(Y&gt;q95)/(1-alpha)) ; sq95b &lt;- mean(Y[Y&gt;q95])
	
	nboot=20 # change to nboot=200 for consistency
	
# sensitivity indices with perturbation of the mean 
  
	v_delta = seq(-1,1,1/10) 
	toto = PLIsuperquantile(alpha,X,Y,deltasvector=v_delta,
	  InputDistributions=distribution,type="MOY",samedelta=TRUE,
	  percentage=FALSE,nboot=nboot,bias=TRUE)

# Plotting the PLI
  par(mar=c(4,5,1,1))
	plot(v_delta,toto$PLI[,2],ylim=c(-0.5,0.5),xlab=expression(delta),
		ylab=expression(hat(PLI[i*delta])),pch=19,cex=1.5)
	points(v_delta,toto$PLI[,1],col="darkgreen",pch=15,cex=1.5)
	points(v_delta,toto$PLI[,3],col="red",pch=17,cex=1.5)
	lines(v_delta,toto$PLICIinf[,2],col="black")
	lines(v_delta,toto$PLICIsup[,2],col="black")
	lines(v_delta,toto$PLICIinf[,1],col="darkgreen")
	lines(v_delta,toto$PLICIsup[,1],col="darkgreen")
	lines(v_delta,toto$PLICIinf[,3],col="red")
	lines(v_delta,toto$PLICIsup[,3],col="red")
	abline(h=0,lty=2)
	legend(-1,0.5,legend=c("X1","X2","X3"),
		col=c("darkgreen","black","red"),pch=c(15,19,17),cex=1.5)
  
# Plotting the perturbed superquantiles
  par(mar=c(4,5,1,1))
	plot(v_delta,toto$superquantile[,2],ylim=c(3,7),xlab=expression(delta),
		ylab=expression(hat(q[i*delta])),pch=19,cex=1.5)
	points(v_delta,toto$superquantile[,1],col="darkgreen",pch=15,cex=1.5)
	points(v_delta,toto$superquantile[,3],col="red",pch=17,cex=1.5)
	lines(v_delta,toto$superquantileCIinf[,2],col="black")
	lines(v_delta,toto$superquantileCIsup[,2],col="black")
	lines(v_delta,toto$superquantileCIinf[,1],col="darkgreen")
	lines(v_delta,toto$superquantileCIsup[,1],col="darkgreen")
	lines(v_delta,toto$superquantileCIinf[,3],col="red")
	lines(v_delta,toto$superquantileCIsup[,3],col="red")
	abline(h=q95,lty=2)
	legend(-1,7,legend=c("X1","X2","X3"),
		col=c("darkgreen","black","red"),pch=c(15,19,17),cex=1.5)
		
# Plotting the unbiased PLI in percentage with refined confidence intervals
	toto = PLIsuperquantile(alpha,X,Y,deltasvector=v_delta,
	  InputDistributions=distribution,type="MOY",samedelta=TRUE,percentage=TRUE,
	  nboot=nboot,bootsample=FALSE,bias=FALSE)
	  
  par(mar=c(4,5,1,1))
	plot(v_delta,toto$PLI[,2],ylim=c(-0.4,0.5),xlab=expression(delta),
		ylab=expression(hat(PLI[i*delta])),pch=19,cex=1.5)
	points(v_delta,toto$PLI[,1],col="darkgreen",pch=15,cex=1.5)
	points(v_delta,toto$PLI[,3],col="red",pch=17,cex=1.5)
	lines(v_delta,toto$PLICIinf[,2],col="black")
	lines(v_delta,toto$PLICIsup[,2],col="black")
	lines(v_delta,toto$PLICIinf[,1],col="darkgreen")
	lines(v_delta,toto$PLICIsup[,1],col="darkgreen") 
	lines(v_delta,toto$PLICIinf[,3],col="red")
	lines(v_delta,toto$PLICIsup[,3],col="red")
	abline(h=0,lty=2)
	legend(-1,0.5,legend=c("X1","X2","X3"),
		col=c("darkgreen","black","red"),pch=c(15,19,17),cex=1.5)

##################################################
# another visualization by using the plotCI() fct 
# (from plotrix package) for the CI plotting (from Vanessa Verges)

	library(plotrix)
	parameters = list(colors=c("darkgreen","black","red"),symbols=c(15,19,17),
	  overlay=c(FALSE,TRUE,TRUE))
  par(mar=c(4,5,1,1),xpd=TRUE)
  for (i in 1:3){
  plotCI(v_delta,toto$PLI[,i],ui=toto$PLICIsup[,i],li=toto$PLICIinf[,i],
         cex=1.5,col=parameters$colors[i],pch=parameters$symbols[i],
         add=parameters$overlay[i], xlab="", ylab="")
  }
  title(xlab=expression(delta),ylab=expression(hat(PLI[i*delta])),
      main=bquote("PLI-superquantile (N ="~.(N) ~ ","~alpha~"="~.(alpha)~
      ") of Y="~2*X[1] + X[2] + X[3]/2))
  abline(h=0,lty=2)
  legend("topleft",legend=c("X1","X2","X3"),
          col=parameters$colors,pch=parameters$symbols,cex=1.5)


</code></pre>

<hr>
<h2 id='PLIsuperquantile_multivar'>
Perturbed-Law based sensitivity Indices (PLI) for superquantile 
and simultaneous perturbations of 2 inputs</h2><span id='topic+PLIsuperquantile_multivar'></span>

<h3>Description</h3>

<p>PLIquantile_multivar computes the Perturbed-Law based Indices (PLI) for superquantile
and simultaneous perturbations of the means of 2 inputs, 
estimated by a Monte Carlo method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PLIsuperquantile_multivar(order,x,y,inputs,deltasvector,InputDistributions,
  samedelta=TRUE, percentage=TRUE,nboot=0,conf=0.95,bootsample=TRUE,bias=TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="PLIsuperquantile_multivar_+3A_order">order</code></td>
<td>
<p>the order of the quantile to estimate.</p>
</td></tr>
<tr><td><code id="PLIsuperquantile_multivar_+3A_x">x</code></td>
<td>
<p>the matrix of simulation points coordinates, 
one column per variable.</p>
</td></tr>
<tr><td><code id="PLIsuperquantile_multivar_+3A_y">y</code></td>
<td>
<p>the vector of model outputs.</p>
</td></tr>
<tr><td><code id="PLIsuperquantile_multivar_+3A_inputs">inputs</code></td>
<td>
<p>the vector of the two inputs' indices for which
the indices will be computed.</p>
</td></tr>  
<tr><td><code id="PLIsuperquantile_multivar_+3A_deltasvector">deltasvector</code></td>
<td>
<p>a vector containing the values of the perturbed means
for which the indices will be computed. Warning: if samedelta=FALSE, 
deltasvector has to be the vector of deltas (mean perturbations)</p>
</td></tr>
<tr><td><code id="PLIsuperquantile_multivar_+3A_inputdistributions">InputDistributions</code></td>
<td>
<p>a list of list. Each list contains, as a list,
the name of the distribution to be used and the parameters.
Implemented cases so far (for a mean perturbation): 
Gaussian, Uniform, Triangle, Left Trucated Gaussian, 
Left Truncated Gumbel. Using Gumbel requires the package <code>evd</code>.</p>
</td></tr>
<tr><td><code id="PLIsuperquantile_multivar_+3A_samedelta">samedelta</code></td>
<td>
<p>a boolean used with the value &quot;MOY&quot; for type. 
</p>

<ul>
<li><p> If it is set at TRUE, the mean perturbation will be the same for all  
the variables. 
</p>
</li>
<li><p> If not, the mean perturbation will be new_mean = mean+sigma*delta 
where mean, sigma are parameters defined in InputDistributions and 
delta is a value of deltasvector.
</p>
</li></ul>
</td></tr>
<tr><td><code id="PLIsuperquantile_multivar_+3A_percentage">percentage</code></td>
<td>
<p>a boolean that defines the formula used for the PLI.
</p>

<ul>
<li><p> If it is set at FALSE, the initially proposed formula is used
(see Sueur et al., 2017).
</p>
</li>
<li><p> If not (set as TRUE), the PLI is given in percentage of variation 
of the superquantile (see Iooss et al., 2021).
</p>
</li></ul>
</td></tr>
<tr><td><code id="PLIsuperquantile_multivar_+3A_nboot">nboot</code></td>
<td>
<p>the number of bootstrap replicates.</p>
</td></tr>
<tr><td><code id="PLIsuperquantile_multivar_+3A_conf">conf</code></td>
<td>
<p>the confidence level for bootstrap confidence intervals.</p>
</td></tr>
<tr><td><code id="PLIsuperquantile_multivar_+3A_bootsample">bootsample</code></td>
<td>
<p>If TRUE, the uncertainty about the original quantile estimation
is taken into account in the PLI confidence intervals (see Iooss et al., 2021). 
If FALSE, standard confidence intervals are computed for the PLI.
It mainly changes the CI at small delta values.</p>
</td></tr>
<tr><td><code id="PLIsuperquantile_multivar_+3A_bias">bias</code></td>
<td>
<p>defines the version of PLI-superquantile:
</p>

<ul>
<li><p> If it is set at &quot;TRUE&quot;, it gives the mean of outputs above the perturbed 
quantile (alternative formula)
</p>
</li>
<li><p> If it is set at &quot;FALSE&quot;, it gives the mean of perturbed outputs above the 
perturbed quantile (original formula)
</p>
</li></ul>
</td></tr>
</table>


<h3>Details</h3>

<p>This function does not allow perturbations on the variance of the inputs' distributions.
</p>


<h3>Value</h3>

<p><code>PLIsuperquantile_multivar</code> returns a list of matrices 
(delta twist of input 1 (in rows) vs. delta twist of input 2 (in columns)) 
containing the following components:
</p>
<table role = "presentation">
<tr><td><code>PLI</code></td>
<td>
<p>the PLI.</p>
</td></tr>
<tr><td><code>PLICIinf</code></td>
<td>
<p>the bootstrap lower confidence interval values of the PLI.</p>
</td></tr>
<tr><td><code>PLICIsup</code></td>
<td>
<p>the bootstrap upper confidence interval values of the PLI.</p>
</td></tr>
<tr><td><code>quantile</code></td>
<td>
<p>the perturbed quantile.</p>
</td></tr>
<tr><td><code>quantileCIinf</code></td>
<td>
<p>the bootstrap lower confidence interval values of the perturbed superquantile.</p>
</td></tr>
<tr><td><code>quantileCIsup</code></td>
<td>
<p>the bootstrap upper confidence interval values of the perturbed superquantile.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Bertrand Iooss
</p>


<h3>References</h3>

<p>T. Delage, R. Sueur and B. Iooss, 2018, <em>Robustness analysis of epistemic uncertainties 
propagation studies in LOCA assessment thermal-hydraulic model</em>, 
ANS Best Estimate Plus Uncertainty International Conference (BEPU 2018), Lucca, Italy, May 13-19, 2018.
</p>
<p>B. Iooss, V. Verges and V. Larget, 2022, <em>BEPU robustness analysis via perturbed law-based 
sensitivity indices</em>, Proceedings of the Institution of Mechanical Engineers, 
Part O: Journal of Risk and Reliability, 236:855-865.
</p>
<p>P. Lemaitre, E. Sergienko, A. Arnaud, N. Bousquet, F. Gamboa and B. Iooss, 2015, 
<em>Density modification based reliability sensitivity analysis</em>, Journal of Statistical 
Computation and Simulation, 85:1200-1223. 
</p>
<p>R. Sueur, N. Bousquet, B. Iooss and J. Bect, 2016,
<em>Perturbed-Law based sensitivity Indices for sensitivity analysis in structural reliability</em>,
Proceedings of the SAMO 2016 Conference, Reunion Island, France, December 2016.
</p>
<p>R. Sueur, B. Iooss and T. Delage, 2017,
<em>Sensitivity analysis using perturbed-law based indices for quantiles and application to an industrial case</em>, 
10th International Conference on Mathematical Methods in Reliability (MMR 2017), Grenoble, France, July 2017.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+PLI">PLI</a>, <a href="#topic+PLIquantile">PLIquantile</a>, <a href="#topic+PLIsuperquantile">PLIsuperquantile</a>, <a href="#topic+PLIquantile_multivar">PLIquantile_multivar</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

# Model: 3D function 

distribution = list()
for (i in 1:3) distribution[[i]]=list("norm",c(0,1))
N = 10000
X = matrix(0,ncol=3,nrow=N)
for(i in 1:3) X[,i] = rnorm(N,0,1)
Y = 2 * X[,1] + X[,2] + X[,3]/2
alpha &lt;- 0.95
nboot &lt;- 20 # put nboot=200 for consistency

q95 = quantile(Y,alpha)
sq95a &lt;- mean(Y*(Y&gt;q95)/(1-alpha)) ; sq95b &lt;- mean(Y[Y&gt;q95])
  
v_delta = seq(-1,1,1/10) 
toto12 = PLIsuperquantile_multivar(alpha,X,Y,c(1,2),deltasvector=v_delta,
    InputDistributions=distribution,samedelta=TRUE,bias=FALSE)
toto = PLIsuperquantile(alpha,X,Y,deltasvector=v_delta,InputDistributions=distribution,
    type="MOY",samedelta=TRUE,nboot=0,bias=FALSE)

par(mar=c(4,5,1,1))
plot(v_delta,diag(toto12$PLI),,ylim=c(-1,1),xlab=expression(delta),
    ylab=expression(hat(PLI[i*delta])),pch=16,cex=1.5,col="blue")
points(v_delta,toto$PLI[,1],col="darkgreen",pch=15,cex=1.5)
points(v_delta,toto$PLI[,2],col="black",pch=19,cex=1.5)
points(v_delta,toto$PLI[,3],col="red",pch=17,cex=1.5)
abline(h=0,lty=2)
legend(-1,1.,legend=c("X1","X2","X3","X1X2"),col=c("darkgreen","black","red","blue"),
    pch=c(15,19,17,16),cex=1.5)

# with bootstrap (put in comment because too long for the CRAN tests)

v_delta = seq(-1,1,2/10) 

toto12 = PLIsuperquantile_multivar(alpha,X,Y,c(1,2),deltasvector=v_delta,
    InputDistributions=distribution,samedelta=TRUE,nboot=nboot,bootsample=FALSE,bias=FALSE)
toto = PLIsuperquantile(alpha,X,Y,deltasvector=v_delta,InputDistributions=distribution,
    type="MOY",samedelta=TRUE,nboot=nboot,bootsample=FALSE,bias=FALSE)

par(mar=c(4,5,1,1))
plot(v_delta,diag(toto12$PLI),ylim=c(-1,1),xlab=expression(delta),
    ylab=expression(hat(PLI[i*delta])),pch=16,cex=1.5,col="blue")
points(v_delta,toto$PLI[,1],col="darkgreen",pch=15,cex=1.5)
points(v_delta,toto$PLI[,2],col="black",pch=19,cex=1.5)
points(v_delta,toto$PLI[,3],col="red",pch=17,cex=1.5)
lines(v_delta,diag(toto12$PLICIinf),col="blue")
lines(v_delta,diag(toto12$PLICIsup),col="blue")
lines(v_delta,toto$PLICIinf[,2],col="black")
lines(v_delta,toto$PLICIsup[,2],col="black")
lines(v_delta,toto$PLICIinf[,1],col="darkgreen")
lines(v_delta,toto$PLICIsup[,1],col="darkgreen")
lines(v_delta,toto$PLICIinf[,3],col="red")
lines(v_delta,toto$PLICIsup[,3],col="red")
abline(h=0,lty=2)
legend(-1,1,legend=c("X1","X2","X3","X1X2"),col=c("darkgreen","black","red","blue"),
    pch=c(15,19,17,16),cex=1.5)

###################################################		
# another visualizations by using the plotrix, 
# viridisLite, lattice and grid packages (from Vanessa Verges)

library(plotrix)
parameters = list(colors=c("darkgreen","black","red"),symbols=c(15,19,17))
par(mar=c(4,5,1,1),xpd=TRUE)
plotCI(v_delta,diag(toto12$PLI),ui=diag(toto12$PLICIsup),li=diag(toto12$PLICIinf),
       xlab=expression(delta),ylab=expression(hat(PLI[i*delta])),
       main=bquote("PLI-superquantile (N ="~.(N) ~ ","~alpha~"="~.(alpha)~
       ") on "~X[1]~"and"~X[2]~"of Y="~2*X[1] + X[2] + X[3]/2),
       cex=1.5,col="blue",pch=16)
for (i in 1:3){
  plotCI(v_delta,toto$PLI[,i],ui=toto$PLICIsup[,i],li=toto$PLICIinf[,i],
         cex=1.5,col=parameters$colors[i],pch=parameters$symbols[i],
         add=TRUE)
}
abline(h=0,lty=2)
legend("topleft",legend=c("X1","X2","X3","X1X2"),
        col=c(parameters$colors,"blue"),pch=c(parameters$symbols,16),cex=1.5)

# Visu of all the PLIs (at any paired combinations of deltas)

library(viridisLite)
library(lattice)
library(grid)

colnames(toto12$PLI) = round(v_delta,2)
rownames(toto12$PLI) = round(v_delta,2)
coul = viridis(100)
levelplot(toto12$PLI,col.regions=coul,main=bquote(hat(PLI)[superquantile[~X[1]~X[2]]]),
          xlab=bquote(delta[X~.(1)]),ylab=bquote(delta[X~.(2)]))


</code></pre>

<hr>
<h2 id='plot.support'>
Support index functions: Measuring the effect of input variables over their support
</h2><span id='topic+plot.support'></span><span id='topic+plot'></span><span id='topic+scatterplot.support'></span><span id='topic+scatterplot'></span>

<h3>Description</h3>

<p>Methods to plot the normalized support index functions (Fruth et al., 2016). 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'support'
plot(x, i = 1:ncol(x$X),
        xprob = FALSE, p = NULL, p.arg = NULL,
        ylim = NULL, col = 1:3, lty = 1:3, lwd = c(2,2,1), cex = 1, ...)
## S3 method for class 'support'
scatterplot(x, i = 1:ncol(x$X), 
               xprob = FALSE, p = NULL, p.arg = NULL, 
               cex = 1, cex.lab = 1, ...)

</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.support_+3A_x">x</code></td>
<td>
<p>an object of class support.</p>
</td></tr>
<tr><td><code id="plot.support_+3A_i">i</code></td>
<td>
<p>an optional vector of integers indicating the subset of input variables <code>X_i</code> for plotting. Default is the entire set of input variables.</p>
</td></tr>
<tr><td><code id="plot.support_+3A_xprob">xprob</code></td>
<td>
<p>an optional boolean indicating whether the inputs should be plotted in probability scale.</p>
</td></tr>
<tr><td><code id="plot.support_+3A_p">p</code></td>
<td>
<p>,</p>
</td></tr>
<tr><td><code id="plot.support_+3A_p.arg">p.arg</code></td>
<td>
<p>list of probability names and parameters for the input distribution.</p>
</td></tr>
<tr><td><code id="plot.support_+3A_ylim">ylim</code></td>
<td>
<p>,</p>
</td></tr>
<tr><td><code id="plot.support_+3A_col">col</code></td>
<td>
<p>,</p>
</td></tr>
<tr><td><code id="plot.support_+3A_lty">lty</code></td>
<td>
<p>,</p>
</td></tr>
<tr><td><code id="plot.support_+3A_lwd">lwd</code></td>
<td>
<p>,</p>
</td></tr>
<tr><td><code id="plot.support_+3A_cex">cex</code></td>
<td>
<p>,</p>
</td></tr>
<tr><td><code id="plot.support_+3A_cex.lab">cex.lab</code></td>
<td>
<p>usual graphical parameters.</p>
</td></tr>
<tr><td><code id="plot.support_+3A_...">...</code></td>
<td>
<p>additional graphical parameters to be passed to <code>scatterplot</code> method (<code>ggMarginal</code> function).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>xprob = TRUE</code>, the input variable <code>X_i</code> is plotted in probability scale according to the informations provided in the arguments <code>p, p.arg</code>: The x-axis is thus <code>F(x)</code>, where <code>F</code> is the cdf of <code>X_i</code>. If these ones are not provided, the empirical distribution is used for rescaling: The x-axis is thus <code>Fn(x)</code>, where <code>Fn</code> is the empirical cdf of <code>X_i</code>.
</p>
<p>Legend details:
</p>
<p>zeta*T : normalized total support index function
</p>
<p>zeta* : normalized 1st-order support index function
</p>
<p>nu* : normalized DGSM
</p>
<p>Notice that the sum of (normalized) DGSM (nu*) over all input variables is equal to 1. 
Furthermore, the expectation of the total support index function (zeta*T) is equal to the (normalized) DGSM (nu*).
</p>


<h3>Author(s)</h3>

<p>O. Roustant</p>


<h3>See Also</h3>

 
<p>Estimation of support index functions: <code><a href="#topic+support">support</a></code> 
</p>

<hr>
<h2 id='pme_knn'>Data-given proportional marginal effects estimation via nearest-neighbors procedure</h2><span id='topic+pme_knn'></span><span id='topic+tell.pme_knn'></span><span id='topic+print.pme_knn'></span><span id='topic+plot.pme_knn'></span><span id='topic+ggplot.pme_knn'></span>

<h3>Description</h3>

<p><code>pme_knn</code> computes the proportional marginal effects (PME), from Herin et al. (2024) 
via a nearest neighbor estimation.
Parallelized computations are possible to accelerate the estimation process.
It can be used with categorical inputs (which are transformed with one-hot encoding before 
computing the nearest-neighbors), dependent inputs and multiple outputs.
For large sample sizes, the nearest neighbour algorithm can be significantly accelerated 
by using approximate nearest neighbour search.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pme_knn(model=NULL, X, method = "knn", tol = NULL, marg = T, n.knn = 2, 
          n.limit = 2000, noise = F, rescale = F, nboot = NULL, 
          boot.level = 0.8, conf=0.95, parl=NULL, ...)
## S3 method for class 'pme_knn'
tell(x, y, ...)
## S3 method for class 'pme_knn'
print(x, ...)
## S3 method for class 'pme_knn'
plot(x, ylim = c(0,1), ...)
## S3 method for class 'pme_knn'
ggplot(data, mapping = aes(), ylim = c(0, 1), ..., environment
                 = parent.frame())
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="pme_knn_+3A_model">model</code></td>
<td>
<p>a function defining the model to analyze, taking X as an argument.</p>
</td></tr>
<tr><td><code id="pme_knn_+3A_x">X</code></td>
<td>
<p>a matrix or data frame containing the observed inputs.</p>
</td></tr>
<tr><td><code id="pme_knn_+3A_method">method</code></td>
<td>
<p>the algorithm to be used for estimation, either &quot;rank&quot; or &quot;knn&quot;,
see details. Default is <code>method="knn"</code>.</p>
</td></tr>
<tr><td><code id="pme_knn_+3A_tol">tol</code></td>
<td>
<p>tolerance under which an input is considered as being a zero input. 
See details.</p>
</td></tr>
<tr><td><code id="pme_knn_+3A_marg">marg</code></td>
<td>
<p>whether to chose the closed Sobol' (<code>FALSE</code>) or total Sobol' 
(<code>TRUE</code>) indices as value functions.</p>
</td></tr>
<tr><td><code id="pme_knn_+3A_n.knn">n.knn</code></td>
<td>
<p>the number of nearest neighbours used for estimation.</p>
</td></tr>
<tr><td><code id="pme_knn_+3A_n.limit">n.limit</code></td>
<td>
<p>sample size limit above which approximate nearest neighbour
search is activated.</p>
</td></tr>
<tr><td><code id="pme_knn_+3A_noise">noise</code></td>
<td>
<p>a logical which is TRUE if the model or the output sample is 
noisy. See details.</p>
</td></tr>
<tr><td><code id="pme_knn_+3A_rescale">rescale</code></td>
<td>
<p>a logical indicating if continuous inputs must be rescaled before distance computations.
If TRUE, continuous inputs are first whitened with the ZCA-cor whitening procedure 
(cf. whiten() function in package <code>whitening</code>). If the inputs are independent, 
this first step will have a very limited impact. Then, the resulting whitened inputs 
are individually modified via a copula transform such that each input has the same scale.</p>
</td></tr>
<tr><td><code id="pme_knn_+3A_nboot">nboot</code></td>
<td>
<p>the number of bootstrap resamples for the bootstrap estimate of 
confidence intervals. See details.</p>
</td></tr>
<tr><td><code id="pme_knn_+3A_boot.level">boot.level</code></td>
<td>
<p>a numeric between 0 and 1 for the proportion of the 
bootstrap sample size.</p>
</td></tr>
<tr><td><code id="pme_knn_+3A_conf">conf</code></td>
<td>
<p>the confidence level of the bootstrap confidence intervals.</p>
</td></tr>
<tr><td><code id="pme_knn_+3A_parl">parl</code></td>
<td>
<p>number of cores on which to parallelize the computation. If
<code>NULL</code>, then no parallelization is done.</p>
</td></tr>
<tr><td><code id="pme_knn_+3A_x">x</code></td>
<td>
<p>the object returned by <code>pme_knn</code>.</p>
</td></tr>
<tr><td><code id="pme_knn_+3A_data">data</code></td>
<td>
<p>the object returned by <code>pme_knn</code>.</p>
</td></tr>
<tr><td><code id="pme_knn_+3A_y">y</code></td>
<td>
<p>a numeric univariate vector containing the observed outputs.</p>
</td></tr>
<tr><td><code id="pme_knn_+3A_ylim">ylim</code></td>
<td>
<p>the y-coordinate limits for plotting.</p>
</td></tr>
<tr><td><code id="pme_knn_+3A_mapping">mapping</code></td>
<td>
<p>Default list of aesthetic mappings to use for plot. If not specified, 
must be supplied in each layer added to the plot.</p>
</td></tr>
<tr><td><code id="pme_knn_+3A_environment">environment</code></td>
<td>
<p>[Deprecated] Used prior to tidy evaluation.</p>
</td></tr>
<tr><td><code id="pme_knn_+3A_...">...</code></td>
<td>
<p>additional arguments to be passed to <code>model</code>, or to the 
methods, such as graphical parameters (see <code>par</code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For <code>method="rank"</code>, the estimator is defined in Gamboa et al. (2020) 
following Chatterjee (2019).For first-order indices it is based on an input 
ranking   (same algorithm as in <code>sobolrank</code>) while for higher orders, 
it uses an approximate heuristic solution of the traveling salesman problem 
applied to the input sample distances (cf. TSP() function in package 
<code>TSP</code>).  For <code>method="knn"</code>, ranking and TSP are replaced by a 
nearest neighbour search   as proposed in Broto et al. (2020) and in Azadkia 
&amp; Chatterjee (2020) for a similar coefficient. 
</p>
<p>The computation is done using the subset procedure, defined in Broto, Bachoc
and Depecker (2020), that is computing all the Sobol' closed indices for all 
possible sub-models first, and then computing the proportional values 
recursively, as detailed in Feldman (2005), but using an extension to 
non strictly positive games (Herin et al., 2024).
</p>
<p>Since boostrap creates ties which are not accounted for in the algorithm, 
confidence intervals are obtained by sampling without replacement with a 
proportion of the total sample size <code>boot.level</code>, drawn uniformly.
</p>
<p>If the outputs are noisy, the argument <code>noise</code> can be used: it only has 
an impact on the estimation of one specific sensitivity index, namely 
<code class="reqn">Var(E(Y|X1,\ldots,Xp))/Var(Y)</code>. If there is no noise this index is equal 
to 1, while in the presence of noise it must be estimated.
</p>
<p>The distance used for subsets with mixed inputs (continuous and categorical) 
is the Euclidean distance, thanks to a one-hot encoding of categorical inputs.
</p>
<p>If too many cores for the machine are passed on to the <code>parl</code> argument,
the chosen number of cores is defaulted to the available cores minus one.
</p>
<p>If <code>marg = TRUE</code> (default), the chosen value function to compute the 
proportional values are the total Sobol' indices (dual of the underlying 
cooperative game). If <code>marg = FALSE</code>, then the closed Sobol' indices 
are used instead. Differences may appear between the two.
</p>
<p>Zero inputs are defined by the <code>tol</code> argument. If <code>null</code>,
then inputs with:
</p>
<p style="text-align: center;"><code class="reqn">S^T_{\{i\}}) = 0</code>
</p>

<p>are considered as zero input in the detection of spurious variables. If 
provided, zero inputs are detected when:
</p>
<p style="text-align: center;"><code class="reqn">S^T_{\{i\}} \leq \textrm{tol}</code>
</p>



<h3>Value</h3>

<p><code>pme_knn</code> returns a list of class <code>"pme_knn"</code>:
</p>
<table role = "presentation">
<tr><td><code>call</code></td>
<td>
<p>the matched call.</p>
</td></tr>
<tr><td><code>PME</code></td>
<td>
<p>the estimations of the PME indices.</p>
</td></tr>
<tr><td><code>VE</code></td>
<td>
<p>the estimations of the closed Sobol' indices for all possible sub-models.</p>
</td></tr>
<tr><td><code>indices</code></td>
<td>
<p>list of all subsets corresponding to the structure of VE.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>which estimation method has been used.</p>
</td></tr>
<tr><td><code>conf_int</code></td>
<td>
<p>a matrix containing the estimations, biais and confidence
intervals by bootstrap (if <code>nboot&gt;0</code>).</p>
</td></tr>
<tr><td><code>X</code></td>
<td>
<p>the observed covariates.</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>the observed outcomes.</p>
</td></tr>
<tr><td><code>n.knn</code></td>
<td>
<p>value of the <code>n.knn</code> argument.</p>
</td></tr>
<tr><td><code>rescale</code></td>
<td>
<p>wheter the design matrix has been rescaled.</p>
</td></tr>
<tr><td><code>n.limit</code></td>
<td>
<p>value of the <code>n.limit</code> argument.</p>
</td></tr>
<tr><td><code>boot.level</code></td>
<td>
<p>value of the <code>boot.level</code> argument.</p>
</td></tr>
<tr><td><code>noise</code></td>
<td>
<p>wheter the PME must sum up to one or not.</p>
</td></tr>
<tr><td><code>boot</code></td>
<td>
<p>logical, wheter bootstrap confidence interval estimates have 
been performed.</p>
</td></tr>
<tr><td><code>nboot</code></td>
<td>
<p>value of the <code>nboot</code> argument.</p>
</td></tr>
<tr><td><code>parl</code></td>
<td>
<p>value of the <code>parl</code> argument.</p>
</td></tr>
<tr><td><code>conf</code></td>
<td>
<p>value of the <code>conf</code> argument.</p>
</td></tr>
<tr><td><code>marg</code></td>
<td>
<p>value of the <code>marg</code> argument.</p>
</td></tr>
<tr><td><code>tol</code></td>
<td>
<p>value of the <code>tol</code> argument.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Marouane Il Idrissi, Margot Herin
</p>


<h3>References</h3>

<p>Azadkia M., Chatterjee S., 2021), <em>A simple measure of conditional dependence</em>, 
Ann. Statist. 49(6):3070-3102.
</p>
<p>Chatterjee, S., 2021, <em>A new coefficient of correlation</em>, Journal of the American 
Statistical Association, 116:2009-2022. 
</p>
<p>Gamboa, F., Gremaud, P., Klein, T., &amp; Lagnoux, A., 2022, <em>Global Sensitivity Analysis: 
a novel generation of mighty estimators based on rank statistics</em>, 
Bernoulli 28: 2345-2374.
</p>
<p>Broto B., Bachoc F. and Depecker M. (2020) <em>Variance Reduction for Estimation
of Shapley Effects and Adaptation to Unknown Input Distribution.</em> SIAM/ASA Journal
on Uncertainty Quantification, 8(2).
</p>
<p>M. Herin, M. Il Idrissi, V. Chabridon and B. Iooss, Proportional marginal effects for 
sensitivity analysis with correlated inputs, Proceedings of the 10th International
Conferenceon Sensitivity Analysis of Model Output (SAMO 2022), p 42-43, 
Tallahassee, Florida, March 2022.
</p>
<p>M. Herin, M. Il Idrissi, V. Chabridon and B. Iooss, <em>Proportional marginal effects 
for global sensitivity analysis</em>, SIAM/ASA Journal of Uncertainty Quantification, 
12:667-692 2024
</p>
<p>M. Il Idrissi, V. Chabridon and B. Iooss (2021). <em>Developments and applications
of Shapley effects   to reliability-oriented sensitivity analysis with correlated inputs.</em>
Environmental Modelling &amp; Software, 143, 105115.
</p>
<p>B. Iooss, V. Chabridon and V. Thouvenot, <em>Variance-based importance 
measures for machine learning model interpretability</em>, Congres lambda-mu23,
Saclay, France, 10-13 octobre 2022
<a href="https://hal.science/hal-03741384">https://hal.science/hal-03741384</a>
</p>
<p>Feldman, B. (2005) <em>Relative Importance and Value</em> SSRN Electronic Journal.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+sobolrank">sobolrank</a></code>, <code><a href="#topic+shapleysobol_knn">shapleysobol_knn</a></code>, <code><a href="#topic+shapleyPermEx">shapleyPermEx</a></code>, <code><a href="#topic+shapleySubsetMc">shapleySubsetMc</a></code>, <code><a href="#topic+lmg">lmg</a></code>, <code><a href="#topic+pmvd">pmvd</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  
  
library(parallel)
library(doParallel)
library(foreach)
library(gtools)
library(boot)
library(RANN)

###########################################################
# Linear Model with Gaussian correlated inputs

library(mvtnorm)

set.seed(1234)
n &lt;- 1000
beta&lt;-c(1,-1,0.5)
sigma&lt;-matrix(c(1,0,0,
                0,1,-0.8,
                0,-0.8,1),
              nrow=3,
              ncol=3)

X &lt;-rmvnorm(n, rep(0,3), sigma)
colnames(X)&lt;-c("X1","X2", "X3")


y &lt;- X%*%beta + rnorm(n,0,2)

# Without Bootstrap confidence intervals
x&lt;-pme_knn(model=NULL, X=X,
            n.knn=3,
            noise=TRUE)
tell(x,y)
print(x)
plot(x)

# With Boostrap confidence intervals
x&lt;-pme_knn(model=NULL, X=X,
            nboot=10, 
            n.knn=3,
            noise=TRUE,
            boot.level=0.7, 
            conf=0.95)
tell(x,y)
print(x)
plot(x)

#####################################################
# Test case: the Ishigami function
# Example with given data and the use of approximate nearest neighbour search
n &lt;- 5000
X &lt;- data.frame(matrix(-pi+2*pi*runif(3 * n), nrow = n))
Y &lt;- ishigami.fun(X)
x &lt;- pme_knn(model = NULL, X = X,  method = "knn", n.knn = 5, 
                       n.limit = 2000)
tell(x,Y)
plot(x)

library(ggplot2) ; ggplot(x)

######################################################
# Test case : Linear model (3 Gaussian inputs including 2 dependent) with scaling
# See Iooss and Prieur (2019)
library(mvtnorm) # Multivariate Gaussian variables
library(whitening) # For scaling
modlin &lt;- function(X) apply(X,1,sum)
d &lt;- 3
n &lt;- 10000
mu &lt;- rep(0,d)
sig &lt;- c(1,1,2)
ro &lt;- 0.9
Cormat &lt;- matrix(c(1,0,0,0,1,ro,0,ro,1),d,d)
Covmat &lt;- ( sig %*% t(sig) ) * Cormat
Xall &lt;- function(n) mvtnorm::rmvnorm(n,mu,Covmat)
X &lt;- Xall(n)
x &lt;- pme_knn(model = modlin, X = X, method = "knn", n.knn = 5, 
                       rescale = TRUE, n.limit = 2000)
print(x)
plot(x)

</code></pre>

<hr>
<h2 id='pmvd'>Proportional Marginal Variance Decomposition indices
for linear and logistic models</h2><span id='topic+pmvd'></span><span id='topic+print.pmvd'></span><span id='topic+plot.pmvd'></span>

<h3>Description</h3>

<p><code>pmvd</code> computes the PMVD indices derived from Feldman (2005) applied to
the explained variance (<code class="reqn">R^2</code>) as a performance metric. 
They allow for relative importance indices by <code class="reqn">R^2</code> decomposition 
for linear and logistic regression models. These indices allocate a share of
<code class="reqn">R^2</code> to each input based on a Proportional attribution system,
allowing for covariates with null regression coefficients to have indices
equal to 0, despite their potential dependence with other covariates (Exclusion
principle).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pmvd(X, y, logistic = FALSE, tol = NULL, rank = FALSE, nboot = 0, 
    conf = 0.95, max.iter = 1000, parl = NULL)
## S3 method for class 'pmvd'
print(x, ...)
## S3 method for class 'pmvd'
plot(x, ylim = c(0,1), ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="pmvd_+3A_x">X</code></td>
<td>
<p>a matrix or data frame containing the observed covariates
(i.e., features, input variables...).</p>
</td></tr>
<tr><td><code id="pmvd_+3A_y">y</code></td>
<td>
<p>a numeric vector containing the observed outcomes (i.e.,
dependent variable). If <code>logistic=TRUE</code>, can be a numeric vector
of zeros and ones, or a logical vector, or a factor.</p>
</td></tr>
<tr><td><code id="pmvd_+3A_logistic">logistic</code></td>
<td>
<p>logical. If <code>TRUE</code>, the analysis is done via a
logistic regression(binomial GLM).</p>
</td></tr>
<tr><td><code id="pmvd_+3A_tol">tol</code></td>
<td>
<p>covariates with absolute marginal contributions less or equal to 
<code>tol</code> are omitted. By default, if <code>tol=NULL</code>, only covariates with no 
marginal contribution are omitted.</p>
</td></tr>
<tr><td><code id="pmvd_+3A_rank">rank</code></td>
<td>
<p>logical. If <code>TRUE</code>, the analysis is done on the
ranks.</p>
</td></tr>
<tr><td><code id="pmvd_+3A_nboot">nboot</code></td>
<td>
<p>the number of bootstrap replicates for the computation
of confidence intervals.</p>
</td></tr>
<tr><td><code id="pmvd_+3A_conf">conf</code></td>
<td>
<p>the confidence level of the bootstrap confidence intervals.</p>
</td></tr>
<tr><td><code id="pmvd_+3A_max.iter">max.iter</code></td>
<td>
<p>if <code>logistic=TRUE</code>, the maximum number of iterative 
optimization steps allowed for the logistic regression. Default is <code>1000</code>.</p>
</td></tr> 
<tr><td><code id="pmvd_+3A_parl">parl</code></td>
<td>
<p>number of cores on which to parallelize the computation. If
<code>NULL</code>, then no parallelization is done.</p>
</td></tr>
<tr><td><code id="pmvd_+3A_x">x</code></td>
<td>
<p>the object returned by <code>lmg</code>.</p>
</td></tr>
<tr><td><code id="pmvd_+3A_ylim">ylim</code></td>
<td>
<p>the y-coordinate limits of the plot.</p>
</td></tr>
<tr><td><code id="pmvd_+3A_...">...</code></td>
<td>
<p>arguments to be passed to methods, such as graphical
parameters (see <code>par</code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The computation of the PMVD is done using the recursive method defined in
Feldman (2005), but using the subset procedure defined in Broto, Bachoc
and Depecker (2020), that is computing all the <code class="reqn">R^2</code> for all
possible sub-models first, and then computing <code class="reqn">P(.)</code> recursively for all
subsets of covariates. See Il Idrissi et al. (2021).
</p>
<p>For logistic regression (<code>logistic=TRUE</code>), the <code class="reqn">R^2</code>
value is equal to:
</p>
<p style="text-align: center;"><code class="reqn">R^2 = 1-\frac{\textrm{model deviance}}{\textrm{null deviance}}</code>
</p>

<p>If either a logistic regression model (<code>logistic = TRUE</code>), or any column
of <code>X</code> is categorical (i.e., of class <code>factor</code>), then the rank-based
indices cannot be computed. In both those cases, <code>rank = FALSE</code> is forced
by default (with a <code>warning</code>).
</p>
<p>If too many cores for the machine are passed on to the <code>parl</code> argument,
the chosen number of cores is defaulted to the available cores minus one.
</p>
<p>Spurious covariates are defined by the <code>tol</code> argument. If <code>null</code>,
then covariates with:
</p>
<p style="text-align: center;"><code class="reqn">w(\{i\}) = 0</code>
</p>

<p>are omitted, and their <code>pmvd</code> index is set to zero. In other cases, the 
spurious covariates are detected by:
</p>
<p style="text-align: center;"><code class="reqn">|w(\{i\})| \leq \textrm{tol}</code>
</p>



<h3>Value</h3>

<p><code>pmvd</code> returns a list of class <code>"pmvd"</code>, containing the following
components:
</p>
<table role = "presentation">
<tr><td><code>call</code></td>
<td>
<p>the matched call.</p>
</td></tr>
<tr><td><code>pmvd</code></td>
<td>
<p>a data frame containing the estimations of the PMVD indices.</p>
</td></tr>
<tr><td><code>R2s</code></td>
<td>
<p>the estimations of the <code class="reqn">R^2</code> for all possible sub-models.</p>
</td></tr>
<tr><td><code>indices</code></td>
<td>
<p>list of all subsets corresponding to the structure of R2s.</p>
</td></tr>
<tr><td><code>P</code></td>
<td>
<p>the values of <code class="reqn">P(.)</code> of all subsets for recursive computing. Equal to
<code>NULL</code> if bootstrap estimates are made.</p>
</td></tr>
<tr><td><code>conf_int</code></td>
<td>
<p>a matrix containing the estimations, biais and confidence
intervals by bootstrap (if <code>nboot&gt;0</code>).</p>
</td></tr>
<tr><td><code>X</code></td>
<td>
<p>the observed covariates.</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>the observed outcomes.</p>
</td></tr>
<tr><td><code>logistic</code></td>
<td>
<p>logical. <code>TRUE</code> if the analysis has been made by
logistic regression.</p>
</td></tr>
<tr><td><code>boot</code></td>
<td>
<p>logical. <code>TRUE</code> if bootstrap estimates have been produced.</p>
</td></tr>
<tr><td><code>nboot</code></td>
<td>
<p>number of bootstrap replicates.</p>
</td></tr>
<tr><td><code>rank</code></td>
<td>
<p>logical. <code>TRUE</code> if a rank analysis has been made.</p>
</td></tr>
<tr><td><code>parl</code></td>
<td>
<p>number of chosen cores for the computation.</p>
</td></tr>
<tr><td><code>conf</code></td>
<td>
<p>level for the confidence intervals by bootstrap.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Marouane Il Idrissi
</p>


<h3>References</h3>

<p>Broto B., Bachoc F. and Depecker M. (2020) <em>Variance Reduction for Estimation
of Shapley Effects and Adaptation to Unknown Input Distribution.</em> SIAM/ASA Journal
on Uncertainty Quantification, 8(2).
</p>
<p>D.V. Budescu (1993). <em>Dominance analysis: A new approach to the problem of relative
importance of predictors in multiple regression.</em> Psychological Bulletin, 114:542-551.
</p>
<p>L. Clouvel, B. Iooss, V. Chabridon, M. Il Idrissi and F. Robin, 2024,
<em>An overview of variance-based importance measures in the linear regression context: 
comparative analyses and numerical tests</em>, Preprint.
<a href="https://hal.science/hal-04102053">https://hal.science/hal-04102053</a>
</p>
<p>Feldman, B. (2005) <em>Relative Importance and Value</em> SSRN Electronic Journal.
</p>
<p>U. Gromping (2006). <em>Relative importance for linear regression in R: the Package
relaimpo.</em>  Journal of Statistical Software, 17:1-27.
</p>
<p>M. Il Idrissi, V. Chabridon and B. Iooss (2021). <em>Mesures d'importance relative  
par decompositions de la performance de modeles de regression,</em> Actes des 52emes Journees 
de Statistiques de la Societe Francaise de Statistique (SFdS), pp 497-502,
Nice, France, Juin 2021
</p>
<p>B. Iooss, V. Chabridon and V. Thouvenot, <em>Variance-based importance 
measures for machine learning model interpretability</em>, Congres lambda-mu23,
Saclay, France, 10-13 octobre 2022
<a href="https://hal.science/hal-03741384">https://hal.science/hal-03741384</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pcc">pcc</a></code>, <code><a href="#topic+src">src</a></code>, <code><a href="#topic+lmg">lmg</a></code>, <code><a href="#topic+pme_knn">pme_knn</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(parallel)
library(gtools)
library(boot)

library(mvtnorm)

set.seed(1234)
n &lt;- 100
beta&lt;-c(1,-2,3)
sigma&lt;-matrix(c(1,0,0,
                0,1,-0.8,
                0,-0.8,1),
              nrow=3,
              ncol=3)

############################
# Gaussian correlated inputs

X &lt;-rmvnorm(n, rep(0,3), sigma)

#############################
# Linear Model

y &lt;- X%*%beta + rnorm(n)

# Without Bootstrap confidence intervals
x&lt;-pmvd(X, y)
print(x)
plot(x)

# With Boostrap confidence intervals
x&lt;-pmvd(X, y, nboot=100, conf=0.95)
print(x)
plot(x)

# Rank-based analysis
x&lt;-pmvd(X, y, rank=TRUE, nboot=100, conf=0.95)
print(x)
plot(x)

############################
# Logistic Regression
y&lt;-as.numeric(X%*%beta + rnorm(n)&gt;0)
x&lt;-pmvd(X,y, logistic = TRUE)
plot(x)
print(x)

# Parallel computing
#x&lt;-pmvd(X,y, logistic = TRUE, parl=2)
#plot(x)
#print(x)

</code></pre>

<hr>
<h2 id='PoincareChaosSqCoef'>Squared coefficients computation in generalized chaos</h2><span id='topic+PoincareChaosSqCoef'></span>

<h3>Description</h3>

<p>This program computes the squared coefficient of the function decomposition in the tensor basis formed by eigenfunctions of Poincare differential operators.
After division by the variance of the model output, it provides lower bounds of first-order and total Sobol' indices.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PoincareChaosSqCoef(PoincareEigen, multiIndex, design, output, outputGrad = NULL, 
                    inputIndex = 1, der = FALSE, method = "unbiased")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="PoincareChaosSqCoef_+3A_poincareeigen">PoincareEigen</code></td>
<td>
<p>output list from PoincareOptimal() function</p>
</td></tr>
<tr><td><code id="PoincareChaosSqCoef_+3A_multiindex">multiIndex</code></td>
<td>
<p>vector of indices (l1, ..., ld). A coordinate equal to 0 corresponds to the constant basis function 1</p>
</td></tr>
<tr><td><code id="PoincareChaosSqCoef_+3A_design">design</code></td>
<td>
<p>design of experiments (matrix of size n x d) with d the number of inputs and n the number of observations</p>
</td></tr>
<tr><td><code id="PoincareChaosSqCoef_+3A_output">output</code></td>
<td>
<p>vector of length n (y1, ..., yn) of output values at <code>design</code> points</p>
</td></tr>
<tr><td><code id="PoincareChaosSqCoef_+3A_outputgrad">outputGrad</code></td>
<td>
<p>matrix n x d whose columns contain the output partial derivatives at <code>design</code> points</p>
</td></tr>
<tr><td><code id="PoincareChaosSqCoef_+3A_inputindex">inputIndex</code></td>
<td>
<p>index of the input variable (between 1 and d)</p>
</td></tr>
<tr><td><code id="PoincareChaosSqCoef_+3A_der">der</code></td>
<td>
<p>logical (default=FALSE): should we use the formula with derivatives to compute the squared coefficient?</p>
</td></tr>
<tr><td><code id="PoincareChaosSqCoef_+3A_method">method</code></td>
<td>
<p>&quot;biased&quot; or &quot;unbiased&quot; formula when estimating the squared integral. See <code><a href="#topic+squaredIntEstim">squaredIntEstim</a></code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Similarly to polynomial chaos, where tensors of polynomials are used, we consider here tensor 
basis formed by eigenfunctions of Poincare differential operators. This basis is also orthonormal, 
and Parseval formula lead to lower bound for (unnormalized) Sobol, total Sobol indices, and any variance-based index.
Denoting by <code class="reqn">(e_{1, l1}... e_{d, ld})</code> one tensor basis, the corresponding coefficient is equal to
</p>
<p><code class="reqn">c_{l1, ..., ld} = &lt;f, e_{1, l1}... e_{d, ld}&gt;</code>.
</p>
<p>For a given input variable (say <code class="reqn">x1</code> to simplify notations), it can be rewritten with derivatives as:
</p>
<p><code class="reqn">c_{l1, ..., ld} = &lt;df/dx1, de_{1, l1}/dx1 e_{2, l2}...e_{d, ld}&gt; / eigenvalue_{1, l1}</code>
</p>
<p>The function returns an estimate of <code class="reqn">c_{l1, ..., ld}^2</code>, corresponding to one of these two forms (derivative-free, or derivative-based).
</p>


<h3>Value</h3>

<p>An estimate of the squared coefficient.
</p>


<h3>Author(s)</h3>

<p>Olivier Roustant and Bertrand Iooss
</p>


<h3>References</h3>

<p>O. Roustant, F. Gamboa and B. Iooss, <em>Parseval inequalities and lower bounds for 
variance-based sensitivity indices</em>, Electronic Journal of Statistics, 14:386-412, 2020
</p>


<h3>See Also</h3>

<p><code><a href="#topic+PoincareOptimal">PoincareOptimal</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# A simple example

g &lt;- function(x, a){
  res &lt;- x[, 1] + a*x[, 1]*x[, 2]
  attr(res, "grad") &lt;- cbind(1 + a * x[, 2], a * x[, 1])
  return(res)
}

n &lt;- 1e3
set.seed(0)
X &lt;- matrix(runif(2*n, min = -1/2, max = 1/2), nrow = n, ncol = 2)
a &lt;- 3
fX &lt;- g(X, a = a)

out_1 &lt;- out_2 &lt;- PoincareOptimal(distr = list("unif", -1/2, 1/2), 
                                  only.values = FALSE, der = TRUE, 
                                  method = "quad")
out &lt;- list(out_1, out_2)

# Lower bounds for X1
c2_10 &lt;- PoincareChaosSqCoef(PoincareEigen = out, multiIndex = c(1, 0), 
                             design = X, output = fX, outputGrad = attr(fX, "grad"), 
                             inputIndex = 1, der = FALSE)
c2_11 &lt;- PoincareChaosSqCoef(PoincareEigen = out, multiIndex = c(1, 1), 
                             design = X, output = fX, outputGrad = attr(fX, "grad"), 
                             inputIndex = 1, der = FALSE)
c2_10_der &lt;- PoincareChaosSqCoef(PoincareEigen = out, multiIndex = c(1, 0), 
                                 design = X, output = fX, outputGrad = attr(fX, "grad"), 
                                 inputIndex = 1, der = TRUE)
c2_11_der &lt;- PoincareChaosSqCoef(PoincareEigen = out, multiIndex = c(1, 1), 
                                 design = X, output = fX, outputGrad = attr(fX, "grad"), 
                                 inputIndex = 1, der = TRUE)

LB1 &lt;- c(8/pi^4, c2_10, c2_10_der)
LB1tot &lt;- LB1 + c(64/pi^8 * a^2, c2_11, c2_11_der)
LB &lt;- cbind(LB1, LB1tot)
rownames(LB) &lt;- c("True lower bound value", 
                  "Estimated, no derivatives", "Estimated, with derivatives")
colnames(LB) &lt;- c("D1", "D1tot")
cat("True values of D1 and D1tot:", c(1/12, 1/12 + a^2 / 144),"\n")
cat("Sample size: ", n, "\n")
cat("Lower bounds computed with the first Poincare eigenvalue:\n")
print(LB)
cat("\nN.B. Increase the sample size to see the convergence to true lower bound values.\n")

############################################################
# Flood model example (see Roustant et al., 2017, 2019)



library(evd) # Gumbel law
library(triangle) # Triangular law

# Flood model
Fcrues_full2=function(X,ans=0){ 
  # ans=1 gives Overflow output; ans=2 gives Cost output; ans=0 gives both
  mat=matrix(X,ncol=8);
  if (ans==0){ reponse=matrix(NA,nrow(mat),2);}
  else{ reponse=rep(NA,nrow(mat));}
  for (i in 1:nrow(mat)) {
    H = (mat[i,1] / (mat[i,2]*mat[i,8]*sqrt((mat[i,4] - mat[i,3])/mat[i,7])))^(0.6) ;
    S = mat[i,3] + H - mat[i,5] - mat[i,6] ;
    if (S &gt; 0){ Cp = 1 ;}
    else{ Cp = 0.2 + 0.8 * (1 - exp(-1000 / S^4));}
    if (mat[i,5]&gt;8){ Cp = Cp + mat[i,5]/20 ;}
    else{ Cp = Cp + 8/20 ;}
    if (ans==0){
      reponse[i,1] = S ;
      reponse[i,2] = Cp ;
    }
    if (ans==1){ reponse[i] = S ;}
    if (ans==2){ reponse[i] = Cp ;}
    
  }
  return(RES=reponse)
}

# Flood model derivatives (by finite-differences)
dFcrues_full2 &lt;- function(X, i, ans, eps){
  der = X
  X1 = X
  X1[,i] = X[,i]+eps
  der = (Fcrues_full2(X1,ans) - Fcrues_full2(X,ans))/(eps)
  return(der)
}

# Function for flood model inputs sampling
EchantFcrues_full2&lt;-function(taille){
  X = matrix(NA,taille,8)
  X[,1] = rgumbel.trunc(taille,loc=1013.0,scale=558.0,min=500,max=3000)
  X[,2] = rnorm.trunc(taille,mean=30.0,sd=8,min=15.)
  X[,3] = rtriangle(taille,a=49,b=51,c=50)
  X[,4] = rtriangle(taille,a=54,b=56,c=55)
  X[,5] = runif(taille,min=7,max=9)
  X[,6] = rtriangle(taille,a=55,b=56,c=55.5)
  X[,7] = rtriangle(taille,a=4990,b=5010,c=5000)
  X[,8] = rtriangle(taille,a=295,b=305,c=300)
  return(X)
}

d &lt;- 8
n &lt;- 1e3
eps &lt;- 1e-7 # finite-differences for derivatives
x &lt;- EchantFcrues_full2(n)
yy &lt;- Fcrues_full2(x, ans=2)
y &lt;- scale(yy, center = TRUE, scale = FALSE)[,1]
dy &lt;- NULL
for (i in 1:d) dy &lt;- cbind(dy, dFcrues_full2(x, i, ans=2, eps))

method &lt;- "quad"
out_1 &lt;- PoincareOptimal(distr = list("gumbel", 1013, 558), min=500,max=3000, 
                         only.values = FALSE, der = TRUE, method = method)
out_2 &lt;- PoincareOptimal(distr = list("norm", 30, 8), min=15, max=200, 
                         only.values = FALSE, der = TRUE, method = method)
out_3 &lt;- PoincareOptimal(distr = list("triangle", 49, 51, 50), 
                         only.values = FALSE, der = TRUE, method = method)
out_4 &lt;- PoincareOptimal(distr = list("triangle", 54, 56, 55), 
                         only.values = FALSE, der = TRUE, method = method)
out_5 &lt;- PoincareOptimal(distr = list("unif", 7, 9), 
                         only.values = FALSE, der = TRUE, method = method)
out_6 &lt;- PoincareOptimal(distr = list("triangle", 55, 56, 55.5), 
                         only.values = FALSE, der = TRUE, method = method)
out_7 &lt;- PoincareOptimal(distr = list("triangle", 4990, 5010, 5000), 
                         only.values = FALSE, der = TRUE, method = method)
out_8 &lt;- PoincareOptimal(distr = list("triangle", 295, 305, 300), 
                         only.values = FALSE, der = TRUE, method = method)
out_ &lt;- list(out_1,out_2,out_3,out_4,out_5,out_6,out_7,out_8)

c2 &lt;- c2der &lt;- c2tot &lt;- c2totder &lt;- rep(0,d)

for (i in 1:d){
  m &lt;- diag(1,d,d) ; m[,i] &lt;- 1
  
  for (j in 1:d){
    cc &lt;- PoincareChaosSqCoef(PoincareEigen = out_, multiIndex = m[j,], 
            design = x, output = y, outputGrad = NULL, 
            inputIndex = i, der = FALSE)
    c2tot[i] &lt;- c2tot[i] + cc
    if (j == i) c2[i] &lt;- cc
    
    cc &lt;- PoincareChaosSqCoef(PoincareEigen = out_, multiIndex = m[j,], 
            design = x, output = y, outputGrad = dy, 
            inputIndex = i, der = TRUE)
    c2totder[i] &lt;- c2totder[i] + cc
    if (j == i) c2der[i] &lt;- cc
  }
}

print("Lower bounds of first-order Sobol' indices without derivatives:")
print(c2/var(y))
print("Lower bounds of first-order Sobol' indices with derivatives:")
print(c2der/var(y))

print("Lower bounds of total Sobol' indices without derivatives:")
print(c2tot/var(y))
print("Lower bounds of total Sobol' indices with derivatives:")
print(c2totder/var(y))



</code></pre>

<hr>
<h2 id='PoincareConstant'>Poincare constants for Derivative-based Global Sensitivity Measures (DGSM)</h2><span id='topic+PoincareConstant'></span>

<h3>Description</h3>

<p>A DGSM is a sensitivity index relying on the integral (over the space domain of the input variables) 
of the squared derivatives of a model output with respect to one model input variable. 
The product between a DGSM and a Poincare Constant (Roustant et al., 2014: Roustant et al., 2017) 
gives an upper bound of the total Sobol' index corresponding to the same input 
(Lamboni et al., 2013; Kucherenko and Iooss, 2016).
</p>
<p>This Poincare constant depends on the type of probability distribution of the input variable.
In the particular case of log-concave distribution, analytical formulas are available for
double-exponential transport by the way of the median value (Lamboni et al., 2013). For truncated 
log-concave distributions, different formulas are available (Roustant et al., 2014). For general 
distributions (truncated or not), some Poincare constants can be computed via a relatively simple
optimization process using different formula coming from transport inequalities (Roustant et al., 2017).
</p>
<p>Notice that the analytical formula based on the log-concave law cases is a subcase of the 
double-exponential transport. In all cases, with this function, the smallest constant is obtained using
the logistic transport formula. <code><a href="#topic+PoincareOptimal">PoincareOptimal</a></code> allows to obtained the best (optimal)
constant using another (spectral) method.
</p>
<p>IMPORTANT: This program is useless for the two following input variable distributions:
</p>

<ul>
<li><p> uniform on <code class="reqn">[min,max]</code> interval: The optimal Poincare constant is 
<code class="reqn">\frac{(max-min)^2}{pi^2}</code>.
</p>
</li>
<li><p> normal with a standard deviation <code class="reqn">sd</code>: The optimal Poincare constant is <code class="reqn">sd^2</code>.
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>PoincareConstant(dfct=dnorm, qfct=qnorm, pfct=pnorm,
                 logconcave=FALSE, transport="logistic", 
                 optimize.interval=c(-100, 100),
                 truncated=FALSE, min=0, max=1,  ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="PoincareConstant_+3A_dfct">dfct</code></td>
<td>
<p>the probability density function of the input variable</p>
</td></tr>
<tr><td><code id="PoincareConstant_+3A_qfct">qfct</code></td>
<td>
<p>the quantile function of the input variable</p>
</td></tr>
<tr><td><code id="PoincareConstant_+3A_pfct">pfct</code></td>
<td>
<p>the distribution function of the input variable</p>
</td></tr>
<tr><td><code id="PoincareConstant_+3A_logconcave">logconcave</code></td>
<td>
<p>logical value: TRUE for a log-concave distribution (analyical formula will be used). 
Requires argument 'dfct' and 'qfct'. FALSE (default value) means that the calculations will be performed 
using transport-based formulas (applicable for log-concave and non-log concave cases)</p>
</td></tr>
<tr><td><code id="PoincareConstant_+3A_transport">transport</code></td>
<td>
<p>If logconcave=FALSE, choice of the transport inequalities to be used: &quot;double_exp&quot; (default value) 
for double exponential transport and &quot;logistic&quot; for logistic transport&quot;. Requires argument 'dfct' and 'pfct'</p>
</td></tr>
<tr><td><code id="PoincareConstant_+3A_optimize.interval">optimize.interval</code></td>
<td>
<p>In the transport-based case (logconcave=FALSE), a vector containing the end-points 
of the interval to be searched for the maximum of the function to be optimized</p>
</td></tr>
<tr><td><code id="PoincareConstant_+3A_truncated">truncated</code></td>
<td>
<p>logical value: TRUE for a truncated distribution. Default value is FALSE</p>
</td></tr>
<tr><td><code id="PoincareConstant_+3A_min">min</code></td>
<td>
<p>the minimal bound in the case of a truncated distribution</p>
</td></tr>
<tr><td><code id="PoincareConstant_+3A_max">max</code></td>
<td>
<p>the maximal bound in the case of a truncated distribution</p>
</td></tr>
<tr><td><code id="PoincareConstant_+3A_...">...</code></td>
<td>
<p>additional arguments</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In the case of truncated distributions (truncated=TRUE), in addition to the min and max arguments: 
- the truncated distribution name has to be passed in the 'dfct' and 'pfct' arguments if logconcave=FALSE, 
- the non-truncated distribution name has to be passed in the 'dfct' and 'qfct' arguments if logconcave=TRUE.
Moreover, if min and max are finite, optimize.interval is required to be defined as c(min,max).
</p>


<h3>Value</h3>

<p><code>PoincareConstant</code> returns the value of the Poincare constant.
</p>


<h3>Author(s)</h3>

<p>Jana Fruth, Bertrand Iooss and Olivier Roustant
</p>


<h3>References</h3>

<p>S. Kucherenko and B. Iooss,
Derivative-based global sensitivity measures,
In: R. Ghanem, D. Higdon and H. Owhadi (eds.), Handbook of Uncertainty Quantification, 
2016.
</p>
<p>M. Lamboni, B. Iooss, A-L. Popelin and F. Gamboa,
Derivative-based global sensitivity measures: General links with Sobol' indices and 
numerical tests, Mathematics and Computers in Simulation, 87:45-54, 2013.
</p>
<p>O. Roustant, F. Barthe and B. Iooss, 
Poincare inequalities on intervals - application to sensitivity analysis,
Electronic Journal of Statistics, Vol. 11, No. 2, 3081-3119, 2017. 
</p>
<p>O. Roustant, J. Fruth, B. Iooss and S. Kuhnt,
Crossed-derivative-based sensitivity measures for interaction screening, 
Mathematics and Computers in Simulation, 105:105-118, 2014.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+PoincareOptimal">PoincareOptimal</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Exponential law (log-concave)
PoincareConstant(dfct=dexp,qfct=qexp,pfct=NULL,rate=1,
  logconcave=TRUE) # log-concave assumption
PoincareConstant(dfct=dexp,qfct=NULL,pfct=pexp,rate=1,
  optimize.interval=c(0, 15)) # logistic transport approach

# Weibull law (log-concave)
PoincareConstant(dfct=dweibull,qfct=NULL,pfct=pweibull,
  optimize.interval=c(0, 15),shape=1,scale=1) # logistic transport approach


# Triangular law (log-concave)
library(triangle)
PoincareConstant(dfct=dtriangle, qfct=qtriangle, pfct=NULL, a=-1, b=1, c=0, 
  logconcave=TRUE) # log-concave assumption
PoincareConstant(dfct=dtriangle, qfct=NULL, pfct=ptriangle, a=-1, b=1, c=0, 
  transport="double_exp", optimize.interval=c(-1,1)) # Double-exp transport 
PoincareConstant(dfct=dtriangle, qfct=NULL, pfct=ptriangle, a=-1, b=1, c=0, 
  optimize.interval=c(-1,1)) # Logistic transport calculation

# Normal N(0,1) law truncated on [-1.87,+infty]
PoincareConstant(dfct=dnorm,qfct=qnorm,pfct=pnorm,mean=0,sd=1,logconcave=TRUE, 
  transport="double_exp", truncated=TRUE, min=-1.87, max=999) # log-concave hyp 
# Double-exponential transport approach
PoincareConstant(dfct=dnorm.trunc, qfct=qnorm.trunc, pfct=pnorm.trunc, 
  mean=0, sd=1, truncated=TRUE, min=-1.87, max=999,   transport="double_exp", 
    optimize.interval=c(-1.87,20)) 
# Logistic transport approach
PoincareConstant(dfct=dnorm.trunc, qfct=qnorm.trunc, pfct=pnorm.trunc, 
  mean=0, sd=1, truncated=TRUE, min=-1.87, max=999, optimize.interval=c(-1.87,20)) 


# Gumbel law (log-concave)
library(evd)
PoincareConstant(dfct=dgumbel, qfct=qgumbel, pfct=NULL, loc=0, scale=1, 
  logconcave=TRUE, transport="double_exp") # log-concave assumption
PoincareConstant(dfct=dgumbel, qfct=NULL, pfct=pgumbel, loc=0, scale=1, 
  transport="double_exp", optimize.interval=c(-3,20)) # Double-exp transport 
PoincareConstant(dfct=dgumbel, qfct=qgumbel, pfct=pgumbel, loc=0, scale=1, 
  optimize.interval=c(-3,20)) # Logistic transport approach

# Truncated Gumbel law (log-concave)
# Double-exponential transport approach
PoincareConstant(dfct=dgumbel, qfct=qgumbel, pfct=pgumbel, loc=0, scale=1, 
  logconcave=TRUE, transport="double_exp", truncated=TRUE, 
  min=-0.92, max=3.56) # log-concave assumption
PoincareConstant(dfct=dgumbel.trunc, qfct=NULL, pfct=pgumbel.trunc, loc=0, scale=1, 
  truncated=TRUE, min=-0.92, max=3.56, transport="double_exp", 
  optimize.interval=c(-0.92,3.56))
# Logistic transport approach
PoincareConstant(dfct=dgumbel.trunc, qfct=qgumbel.trunc, pfct=pgumbel.trunc, 
  loc=0, scale=1, truncated=TRUE, min=-0.92, max=3.56, 
  optimize.interval=c(-0.92,3.56)) 
  


</code></pre>

<hr>
<h2 id='PoincareOptimal'>Optimal Poincare constants for Derivative-based Global Sensitivity Measures (DGSM)</h2><span id='topic+PoincareOptimal'></span>

<h3>Description</h3>

<p>A DGSM is a sensitivity index relying on the integral (over the space domain of the input variables) 
of the squared derivatives of a model output with respect to one model input variable. 
The product between a DGSM and a Poincare Constant (Roustant et al., 2014: Roustant et al., 2017),
on the type of probability distribution of the input variable, gives an   upper bound of the total 
Sobol' index corresponding to the same input (Lamboni et al., 2013; Kucherenko and Iooss, 2016).
</p>
<p>This function provides the optimal Poincare constant as explained in Roustant et al. (2017).
It solves numerically the spectral problem corresponding to the Poincare inequality, with 
Neumann conditions. The differential equation is  f&rdquo; - V'f'= - lambda f with f'(a) = f'(b) = 0.
In addition, all the spectral decomposition can be returned by the function. 
The eigenvalues are sorted in ascending order, starting from zero. 
The information corresponding to the optimal constant is thus given in the second column.
</p>
<p>IMPORTANT: This program is useless for the two following input variable distributions:
</p>

<ul>
<li><p> uniform on <code class="reqn">[min,max]</code> interval: The optimal Poincare constant is <code class="reqn">\frac{(max-min)^2}{pi^2}</code>.
</p>
</li>
<li><p> normal with a standard deviation <code class="reqn">sd</code>: The optimal Poincare constant is <code class="reqn">sd^2</code>.
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>PoincareOptimal(distr=list("unif",c(0,1)), min=NULL, max=NULL, 
                n = 500, method = c("quadrature", "integral"), only.values = TRUE, 
                der = FALSE, plot = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="PoincareOptimal_+3A_distr">distr</code></td>
<td>
<p>a list or a function corresponding to the probability distribution. 
</p>

<ul>
<li><p> If it is a list, it contains the name of the R distribution of the variable and its parameters. Possible choices are: &quot;unif&quot; (uniform), &quot;norm&quot; (normal), &quot;exp&quot; (exponential), &quot;triangle&quot; (triangular from package triangle), &quot;gumbel&quot; (from package evd), &quot;beta&quot;, &quot;gamma&quot;, &quot;weibull&quot; and &quot;lognorm&quot; (lognormal). The values of the distribution parameters have to be passed in arguments in the same order than the corresponding R function.
</p>
</li>
<li><p> If it is a function, it corresponds to the pdf. Notice that the normalizing constant has no impact on the computation of the optimal Poincare constant and can be ommitted.
</p>
</li></ul>
</td></tr>
<tr><td><code id="PoincareOptimal_+3A_min">min</code></td>
<td>
<p>see below</p>
</td></tr>
<tr><td><code id="PoincareOptimal_+3A_max">max</code></td>
<td>
<p>[min,max]: interval on which the distribution is truncated. Choose low and high quantiles in case of unbounded distribution. Choose NULL for uniform and triangular distributions</p>
</td></tr>
<tr><td><code id="PoincareOptimal_+3A_n">n</code></td>
<td>
<p>number of discretization steps</p>
</td></tr>
<tr><td><code id="PoincareOptimal_+3A_method">method</code></td>
<td>
<p>method of integration: &quot;quadrature&quot; (default value) uses the trapez quadrature (close and quicker), &quot;integral&quot; is longer but does not make any approximation</p>
</td></tr>
<tr><td><code id="PoincareOptimal_+3A_only.values">only.values</code></td>
<td>
<p>if TRUE, only eigen values are computed and returned, otherwise both eigenvalues and   eigenvectors are returned (default value is TRUE)</p>
</td></tr>
<tr><td><code id="PoincareOptimal_+3A_der">der</code></td>
<td>
<p>if TRUE, compute the eigenfunction derivatives (default value is FALSE)</p>
</td></tr>
<tr><td><code id="PoincareOptimal_+3A_plot">plot</code></td>
<td>
<p>logical:if TRUE and only.values=FALSE, plots a minimizer of the Rayleigh ratio (default value is FALSE)</p>
</td></tr>
<tr><td><code id="PoincareOptimal_+3A_...">...</code></td>
<td>
<p>additional arguments</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For the uniform, normal, triangular and Gumbel distributions, the optimal constants are computed on the standardized correponding distributions (for a better numerical efficiency). In these cases, the return optimal constant and eigenvalues correspond to original distributions.
</p>


<h3>Value</h3>

<p><code>PoincareOptimal</code> returns a list containing:
</p>
<table role = "presentation">
<tr><td><code>opt</code></td>
<td>
<p>the optimal Poincare constant</p>
</td></tr>
<tr><td><code>values</code></td>
<td>
<p>the eigenvalues in increasing order, starting from 0. Thus, the second one is the spectral gap, equal to the inverse of the Poincare constant</p>
</td></tr>
<tr><td><code>vectors</code></td>
<td>
<p>the values of eigenfunctions at <code>knots</code></p>
</td></tr>
<tr><td><code>der</code></td>
<td>
<p>the values of eigenfunction derivatives at <code>knots</code></p>
</td></tr>
<tr><td><code>knots</code></td>
<td>
<p>a sequence of length <code>n</code> formed by equally spaced real numbers in the support of the probability distribution, used for discretization</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Olivier Roustant and Bertrand Iooss
</p>


<h3>References</h3>

<p>O. Roustant, F. Barthe and B. Iooss, 
Poincare inequalities on intervals - application to sensitivity analysis,
Electronic Journal of Statistics, Vol. 11, No. 2, 3081-3119, 2017.
</p>
<p>O Roustant, F. Gamboa, B Iooss. Parseval inequalities and lower bounds 
# for variance-based sensitivity indices. 2019. hal-02140127
</p>


<h3>See Also</h3>

<p><code><a href="#topic+PoincareConstant">PoincareConstant</a>, <a href="#topic+PoincareChaosSqCoef">PoincareChaosSqCoef</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

# uniform on [a, b]
a &lt;- -1 ; b &lt;- 1
out &lt;- PoincareOptimal(distr = list("unif", a, b))
cat("Poincare constant (theory -- estimated):", (b-a)^2/pi^2, "--", out$opt, "\n")

# truncated standard normal on [-1, 1]
# the optimal Poincare constant is then equal to 1/3,
# as -1 and 1 are consecutive roots of the 2nd Hermite polynomial X*X - 1.
out &lt;- PoincareOptimal(distr = dnorm, min = -1, max = 1, 
                       plot = TRUE, only.values = FALSE)
cat("Poincare constant (theory -- estimated):", 1/3, "--", out$opt, "\n")


# truncated standard normal on [-1.87, +infty]
out &lt;- PoincareOptimal(distr = list("norm", 0, 1), min = -1.87, max = 5, 
                       method = "integral", n = 500)
print(out$opt)

# truncated Gumbel(0,1) on [-0.92, 3.56]
library(evd)
out &lt;- PoincareOptimal(distr = list("gumbel", 0, 1), min = -0.92, max = 3.56, 
                       method = "integral", n = 500)
print(out$opt)

# symetric triangular [-1,1]
library(triangle)
out &lt;- PoincareOptimal(distr = list("triangle", -1, 1, 0), min = NULL, max = NULL)
cat("Poincare constant (theory -- estimated):", 0.1729, "--", out$opt, "\n")


# Lognormal distribution
out &lt;- PoincareOptimal(distr = list("lognorm", 1, 2), min = 3, max = 10, 
                       only.values = FALSE, plot = TRUE, method = "integral")
print(out$opt)


## -------------------------------

## Illustration for eigenfunctions on the uniform distribution
## (corresponds to Fourier series)
b &lt;- 1
a &lt;- -b
out &lt;- PoincareOptimal(distr = list("unif", a, b), 
                       only.values = FALSE, der = TRUE, method = "quad")

# Illustration for 3 eigenvalues

par(mfrow = c(3,2))
eigenNumber &lt;- 1:3 # eigenvalue number
for (k in eigenNumber[1:3]){ # keep the 3 first ones (for graphics)
  plot(out$knots, out$vectors[, k + 1], type = "l", 
       ylab = "", main = paste("Eigenfunction", k), 
       xlab = paste("Eigenvalue:", round(out$values[k+1], digits = 3)))
  sgn &lt;- sign(out$vectors[1, k + 1])
  lines(out$knots, sgn * sqrt(2) * cos(pi * k * (out$knots/(b-a) + 0.5)), 
        col = "red", lty = "dotted")
  
  plot(out$knots, out$der[, k + 1], type = "l", 
       ylab = "", main = paste("Eigenfunction derivative", k), 
       xlab = "")
  sgn &lt;- sign(out$vectors[1, k + 1])
  lines(out$knots, - sgn * sqrt(2) / (b-a) * pi * k * sin(pi * k * (out$knots/(b-a) + 0.5)), 
        col = "red", lty = "dotted")
}


# how to create a function for one eigenfunction and eigenvalue,
# given N values 
eigenFun &lt;- approxfun(x = out$knots, y = out$vectors[, 2])
eigenDerFun &lt;- approxfun(x = out$knots, y = out$der[, 2])
x &lt;- runif(n = 3, min = -1/2, max = 1/2)
eigenFun(x)
eigenDerFun(x)


</code></pre>

<hr>
<h2 id='qosa'>Quantile-oriented sensitivity analysis</h2><span id='topic+qosa'></span><span id='topic+tell.qosa'></span><span id='topic+print.qosa'></span><span id='topic+plot.qosa'></span><span id='topic+ggplot.qosa'></span>

<h3>Description</h3>

 <p><code>qosa</code> implements the estimation of first-order quantile-oriented sensitivity indices 
as defined in Fort et al. (2016) with a kernel-based estimator of conditonal probability density functions 
closely related to the one proposed by Maume-Deschamps and Niang (2018). 
<code>qosa</code> also supports a kernel-based estimation of Sobol first-order indices (i.e. Nadaraya-Watson).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>qosa(model = NULL, X1, X2 = NULL, type = "quantile", alpha = 0.1, split.sample = 2/3, 
nsample = 1e4, nboot = 0, conf = 0.95, ...)
## S3 method for class 'qosa'
tell(x, y = NULL, ...)
## S3 method for class 'qosa'
print(x, ...)
## S3 method for class 'qosa'
plot(x, ylim = c(0, 1), ...)
## S3 method for class 'qosa'
ggplot(data, mapping = aes(), ylim = c(0, 1), ..., environment
                 = parent.frame())
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="qosa_+3A_model">model</code></td>
<td>
<p>a function, or a model with a <code>predict</code> method, defining the model to analyze.</p>
</td></tr>
<tr><td><code id="qosa_+3A_x1">X1</code></td>
<td>
<p>a random sample of the inputs used for the estimation of conditional probability density functions. 
If <code>X2</code> is NULL, <code>X1</code> is split in two samples, with the first <code>split.sample</code> proportion of observations assigned to <code>X1</code> and the rest to <code>X2</code>.</p>
</td></tr>
<tr><td><code id="qosa_+3A_x2">X2</code></td>
<td>
<p>a random sample of the inputs used to evaluate the conditional probability density functions. 
If NULL, it is constructed with the last <code>(1-split.sample)</code> proportion of observations from <code>X1</code>, see above.</p>
</td></tr>
<tr><td><code id="qosa_+3A_type">type</code></td>
<td>
<p>a string specifying which first-order sensitivity indices must be estimated: quantile-oriented indices (<code>type="quantile"</code>) 
or Sobol' indices (<code>type="mean"</code>).</p>
</td></tr>
<tr><td><code id="qosa_+3A_alpha">alpha</code></td>
<td>
<p>if <code>type="quantile"</code> the quantile level.</p>
</td></tr>
<tr><td><code id="qosa_+3A_split.sample">split.sample</code></td>
<td>
<p>if <code>X2=NULL</code> the proportion of observations from <code>X1</code> assigned 
to the estimation of conditional probability density functions.</p>
</td></tr>
<tr><td><code id="qosa_+3A_nsample">nsample</code></td>
<td>
<p>the number of samples from the conditional probability density functions used 
to estimate the conditional quantiles (if <code>type="quantile"</code>) or the conditional means (if <code>type="mean"</code>).</p>
</td></tr>
<tr><td><code id="qosa_+3A_nboot">nboot</code></td>
<td>
<p>the number of bootstrap replicates.</p>
</td></tr>
<tr><td><code id="qosa_+3A_conf">conf</code></td>
<td>
<p>the confidence level for confidence intervals.</p>
</td></tr>
<tr><td><code id="qosa_+3A_x">x</code></td>
<td>
<p>a list of class <code>"sobolrank"</code> storing the state of the
sensitivity study (parameters, data, estimates).</p>
</td></tr>
<tr><td><code id="qosa_+3A_data">data</code></td>
<td>
<p>a list of class <code>"sobolrank"</code> storing the state of the
sensitivity study (parameters, data, estimates).</p>
</td></tr>
<tr><td><code id="qosa_+3A_y">y</code></td>
<td>
<p>a vector of model responses.</p>
</td></tr>
<tr><td><code id="qosa_+3A_ylim">ylim</code></td>
<td>
<p>y-coordinate plotting limits.</p>
</td></tr>
<tr><td><code id="qosa_+3A_mapping">mapping</code></td>
<td>
<p>Default list of aesthetic mappings to use for plot. If not specified, 
must be supplied in each layer added to the plot.</p>
</td></tr>
<tr><td><code id="qosa_+3A_environment">environment</code></td>
<td>
<p>[Deprecated] Used prior to tidy evaluation.</p>
</td></tr>
<tr><td><code id="qosa_+3A_...">...</code></td>
<td>
<p>any other arguments for <code>model</code> which are passed
unchanged each time it is called.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Quantile-oriented sensitivty indices were defined as a special case of sensitivity indices based on contrast functions in Fort et al. (2016).
The estimator used by <code>qosa</code> follows closely the one proposed by Maume-Deschamps &amp; Niang (2018). 
The only difference is that Maume-Deschamps and Niang (2018) use the following kernel-based estimate of the conditional cumulative distribution function:
</p>
<p style="text-align: center;"><code class="reqn">\hat{F}(y\Vert X=x) = \frac{ \sum_{i=1}^n K_{h_x}(x - X_i) \bold{1}\{Y_i&lt; y\}}{\sum_{i=1}^n K_{h_x}(x - X_i)}</code>
</p>

<p>whereas we use
</p>
<p style="text-align: center;"><code class="reqn">\hat{F}(y\vert X=x) = \frac{ \sum_{i=1}^n K_{h_x}(x - X_i) \int_{-\infty}^y K_{h_y}(t - Y_i)dt} {\sum_{i=1}^n K_{h_x}(x - X_i)},</code>
</p>

<p>meaning that <code class="reqn">\bold{1}\{Y_i&lt; y\}</code> is replaced by <code class="reqn">\int_{-\infty}^y K_{h_y}(t - Y_i)dt = \Phi (\frac{y-Y_i}{h_y})</code> where <code class="reqn">\Phi</code> 
is the cumulative distribution function of the standard normal distribution (since kernel <code class="reqn">K</code> is Gaussian). 
The two definitions thus coincide when <code class="reqn">h_y \rightarrow 0</code>. Our formula arises from a kernel density estimator of the joint pdf with a diagonal bandwidth. 
In a future version, it will be genralized to a general bandwidth matrix for improved performance.
</p>


<h3>Value</h3>

<p><code>qosa</code> returns a list of class <code>"qosa"</code>, containing all
the input arguments detailed before, plus the following components:
</p>
<table role = "presentation">
<tr><td><code>call</code></td>
<td>
<p>the matched call.</p>
</td></tr>
<tr><td><code>X</code></td>
<td>
<p>a <code>data.frame</code> containing the design of experiments.</p>
</td></tr>
<tr><td><code>X1</code></td>
<td>
<p>a <code>data.frame</code> containing the design of experiments used for the estimation of conditional probability density functions.</p>
</td></tr>
<tr><td><code>X</code></td>
<td>
<p>a <code>data.frame</code> containing the design of experiments used for the evaluation of conditional probability density functions.</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>a vector of model responses.</p>
</td></tr>
<tr><td><code>S</code></td>
<td>
<p>the estimations of the Sobol' sensitivity indices.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Sebastien Da Veiga
</p>


<h3>References</h3>

<p>Fort, J. C., Klein, T., and Rachdi, N. (2016). New sensitivity analysis subordinated to a contrast. Communications in Statistics-Theory and Methods, 45(15), 4349-4364.
</p>
<p>Maume-Deschamps, V., and Niang, I. (2018). Estimation of quantile oriented sensitivity indices. Statistics &amp; Probability Letters, 134, 122-127.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> 
library(ks)
library(ggplot2)
library(boot)

# Test case : difference of two exponential distributions (Fort et al. (2016))
# We use two samples with different sizes
n1 &lt;- 5000
X1 &lt;- data.frame(matrix(rexp(2 * n1,1), nrow = n1))
n2 &lt;- 1000
X2 &lt;- data.frame(matrix(rexp(2 * n2,1), nrow = n2))
Y1 &lt;- X1[,1] - X1[,2]
Y2 &lt;- X2[,1] - X2[,2]
x &lt;- qosa(model = NULL, X1, X2, type = "quantile", alpha = 0.1)
tell(x,c(Y1,Y2))
print(x)
ggplot(x)

# Test case : difference of two exponential distributions (Fort et al. (2016))
# We use only one sample
n &lt;- 1000 # put n=10000 for more consistency
X &lt;- data.frame(matrix(rexp(2 * n,1), nrow = n))
Y &lt;- X[,1] - X[,2]
x &lt;- qosa(model = NULL, X1 = X, type = "quantile", alpha = 0.7)
tell(x,Y)
print(x)
ggplot(x)

# Test case : the Ishigami function
# We estimate first-order Sobol' indices (by specifying 'mean')
# Next lines are put in comment because too long fro CRAN tests
#n &lt;- 5000 
#nboot &lt;- 50 
#X &lt;- data.frame(matrix(-pi+2*pi*runif(3 * n), nrow = n))
#x &lt;- qosa(model = ishigami.fun, X1 = X, type = "mean", nboot = nboot)
#print(x)
#ggplot(x)


</code></pre>

<hr>
<h2 id='sb'>Sequential Bifurcations</h2><span id='topic+sb'></span><span id='topic+ask.sb'></span><span id='topic+tell.sb'></span><span id='topic+print.sb'></span><span id='topic+plot.sb'></span>

<h3>Description</h3>

 <p><code>sb</code> implements the Sequential Bifurcations screening
method (Bettonvil and Kleijnen 1996).</p>


<h3>Usage</h3>

<pre><code class='language-R'>sb(p, sign = rep("+", p), interaction = FALSE)
## S3 method for class 'sb'
ask(x, i = NULL, ...)
## S3 method for class 'sb'
tell(x, y, ...)
## S3 method for class 'sb'
print(x, ...)
## S3 method for class 'sb'
plot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sb_+3A_p">p</code></td>
<td>
<p>number of factors.</p>
</td></tr>
<tr><td><code id="sb_+3A_sign">sign</code></td>
<td>
<p>a vector fo length <code>p</code> filled with <code>"+"</code> and
<code>"-"</code>, giving the (assumed) signs of the factors effects.</p>
</td></tr>
<tr><td><code id="sb_+3A_interaction">interaction</code></td>
<td>
<p>a boolean, <code>TRUE</code> if the model is supposed to
be with interactions, <code>FALSE</code> otherwise.</p>
</td></tr>
<tr><td><code id="sb_+3A_x">x</code></td>
<td>
<p>a list of class <code>"sb"</code> storing the state of the
screening study at the current iteration.</p>
</td></tr>
<tr><td><code id="sb_+3A_y">y</code></td>
<td>
<p>a vector of model responses.</p>
</td></tr>
<tr><td><code id="sb_+3A_i">i</code></td>
<td>
<p>an integer, used to force a wanted bifurcation instead of that
proposed by the algorithm.</p>
</td></tr>
<tr><td><code id="sb_+3A_...">...</code></td>
<td>
<p>not used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The model without interaction is
</p>
<p style="text-align: center;"><code class="reqn">Y=\beta_0 + \sum_{i=1}^p \beta_i X_i</code>
</p>

<p>while the model with interactions is
</p>
<p style="text-align: center;"><code class="reqn">Y=\beta_0 + \sum_{i=1}^p \beta_i X_i + \sum_{1 \leq i &lt; j \leq
    p} \gamma_{ij} X_i X_j</code>
</p>

<p>In both cases, the factors are assumed to be uniformly distributed on
<code class="reqn">[-1,1]</code>. This is a difference with Bettonvil
et al. where the factors vary across <code class="reqn">[0,1]</code> in the former
case, while <code class="reqn">[-1,1]</code> in the latter.
</p>
<p>Another difference with Bettonvil et al. is that in the current
implementation, the groups are splitted right in the middle.
</p>


<h3>Value</h3>

<p><code>sb</code> returns a list of class <code>"sb"</code>, containing all
the input arguments detailed before, plus the following components:
</p>
<table role = "presentation">
<tr><td><code>i</code></td>
<td>
<p>the vector of bifurcations.</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>the vector of observations.</p>
</td></tr>
<tr><td><code>ym</code></td>
<td>
<p>the vector of mirror observations (model with interactions
only).</p>
</td></tr>
</table>
<p>The groups effects can be displayed with the <code>print</code> method.
</p>


<h3>Author(s)</h3>

<p>Gilles Pujol
</p>


<h3>References</h3>

<p>B. Bettonvil and J. P. C. Kleijnen, 1996, <em>Searching for important
factors in simulation models with many factors: sequential
bifurcations</em>, European Journal of Operational Research, 96, 180&ndash;194.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># a model with interactions
p &lt;- 50
beta &lt;- numeric(length = p)
beta[1:5] &lt;- runif(n = 5, min = 10, max = 50)
beta[6:p] &lt;- runif(n = p - 5, min = 0, max = 0.3)
beta &lt;- sample(beta)
gamma &lt;- matrix(data = runif(n = p^2, min = 0, max = 0.1), nrow = p, ncol = p)
gamma[lower.tri(gamma, diag = TRUE)] &lt;- 0
gamma[1,2] &lt;- 5
gamma[5,9] &lt;- 12
f &lt;- function(x) { return(sum(x * beta) + (x %*% gamma %*% x))}

# 10 iterations of SB
sa &lt;- sb(p, interaction = TRUE)
for (i in 1 : 10) {
  x &lt;- ask(sa)
  y &lt;- list()
  for (i in names(x)) {
    y[[i]] &lt;- f(x[[i]])
  }
  tell(sa, y)
}
print(sa)
plot(sa)
</code></pre>

<hr>
<h2 id='sensiFdiv'>Sensitivity Indices based on Csiszar f-divergence</h2><span id='topic+sensiFdiv'></span><span id='topic+tell.sensiFdiv'></span><span id='topic+print.sensiFdiv'></span><span id='topic+plot.sensiFdiv'></span><span id='topic+ggplot.sensiFdiv'></span>

<h3>Description</h3>

 <p><code>sensiFdiv</code> conducts a density-based sensitivity 
analysis where the impact of an input variable is defined 
in terms of dissimilarity between the original output density function 
and the output density function when the input variable is fixed. 
The dissimilarity between density functions is measured with Csiszar f-divergences. 
Estimation is performed through kernel density estimation and 
the function <code>kde</code> of the package <code>ks</code>.</p>


<h3>Usage</h3>

<pre><code class='language-R'>sensiFdiv(model = NULL, X, fdiv = "TV", nboot = 0, conf = 0.95, ...)
## S3 method for class 'sensiFdiv'
tell(x, y = NULL, ...)
## S3 method for class 'sensiFdiv'
print(x, ...)
## S3 method for class 'sensiFdiv'
plot(x, ylim = c(0, 1), ...)
## S3 method for class 'sensiFdiv'
ggplot(data, mapping = aes(), ylim = c(0, 1), ..., environment
                 = parent.frame())
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sensiFdiv_+3A_model">model</code></td>
<td>
<p>a function, or a model with a <code>predict</code> method,
defining the model to analyze.</p>
</td></tr>
<tr><td><code id="sensiFdiv_+3A_x">X</code></td>
<td>
<p>a matrix or <code>data.frame</code> representing the input random sample.</p>
</td></tr>
<tr><td><code id="sensiFdiv_+3A_fdiv">fdiv</code></td>
<td>
<p>a string or a list of strings specifying the Csiszar f-divergence 
to be used. Available choices are &quot;TV&quot; (Total-Variation), 
&quot;KL&quot; (Kullback-Leibler), &quot;Hellinger&quot; and &quot;Chi2&quot; (Neyman chi-squared).</p>
</td></tr>
<tr><td><code id="sensiFdiv_+3A_nboot">nboot</code></td>
<td>
<p>the number of bootstrap replicates</p>
</td></tr>
<tr><td><code id="sensiFdiv_+3A_conf">conf</code></td>
<td>
<p>the confidence level for confidence intervals.</p>
</td></tr>
<tr><td><code id="sensiFdiv_+3A_x">x</code></td>
<td>
<p>a list of class <code>"sensiFdiv"</code> storing the state of the
sensitivity study (parameters, data, estimates).</p>
</td></tr>
<tr><td><code id="sensiFdiv_+3A_data">data</code></td>
<td>
<p>a list of class <code>"sensiFdiv"</code> storing the state of the
sensitivity study (parameters, data, estimates).</p>
</td></tr>
<tr><td><code id="sensiFdiv_+3A_y">y</code></td>
<td>
<p>a vector of model responses.</p>
</td></tr>
<tr><td><code id="sensiFdiv_+3A_ylim">ylim</code></td>
<td>
<p>y-coordinate plotting limits.</p>
</td></tr>
<tr><td><code id="sensiFdiv_+3A_mapping">mapping</code></td>
<td>
<p>Default list of aesthetic mappings to use for plot. If not specified, 
must be supplied in each layer added to the plot.</p>
</td></tr>
<tr><td><code id="sensiFdiv_+3A_environment">environment</code></td>
<td>
<p>[Deprecated] Used prior to tidy evaluation.</p>
</td></tr>
<tr><td><code id="sensiFdiv_+3A_...">...</code></td>
<td>
<p>any other arguments for <code>model</code> which are passed
unchanged each time it is called.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Some of the Csiszar f-divergences produce sensitivity indices that have 
already been studied in the context of sensitivity analysis.
In particular, &quot;TV&quot; leads to the importance measure proposed by Borgonovo (2007) 
(up to a constant), &quot;KL&quot; corresponds to the mutual information (Krzykacz-Hausmann 2001) and 
&quot;Chi2&quot; produces the squared-loss mutual information. See Da Veiga (2015) for details.
</p>


<h3>Value</h3>

<p><code>sensiFdiv</code> returns a list of class <code>"sensiFdiv"</code>, containing all
the input arguments detailed before, plus the following components:
</p>
<table role = "presentation">
<tr><td><code>call</code></td>
<td>
<p>the matched call.</p>
</td></tr>
<tr><td><code>X</code></td>
<td>
<p>a <code>data.frame</code> containing the design of experiments.</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>a vector of model responses.</p>
</td></tr>
<tr><td><code>S</code></td>
<td>
<p>the estimations of the Csiszar f-divergence sensitivity indices. 
If several divergences have been selected, Sis a list where each element 
encompasses the estimations of the sensitivity indices for one of the divergence.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Sebastien Da Veiga, Snecma
</p>


<h3>References</h3>

<p>Borgonovo E. (2007), <em>A new uncertainty importance measure</em>, 
Reliability Engineering and System Safety 92(6), 771&ndash;784.
</p>
<p>Da Veiga S. (2015), <em>Global sensitivity analysis with dependence measures</em>,
Journal of Statistical Computation and Simulation, 85(7), 1283&ndash;1305. 
</p>
<p>Krzykacz-Hausmann B. (2001), <em>Epistemic sensitivity analysis based on the 
concept of entropy</em>, Proceedings of SAMO2001, 53&ndash;57.
</p>


<h3>See Also</h3>

<p><code><a href="ks.html#topic+kde">kde</a>, <a href="#topic+sensiHSIC">sensiHSIC</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(ks)

# Test case : the non-monotonic Sobol g-function
n &lt;- 100
X &lt;- data.frame(matrix(runif(8 * n), nrow = n))

# Density-based sensitivity analysis
# the next lines are put in comment because too long for CRAN tests
#x &lt;- sensiFdiv(model = sobol.fun, X = X, fdiv = c("TV","KL"), nboot=30)
#print(x)
#library(ggplot2)
#ggplot(x)

</code></pre>

<hr>
<h2 id='sensiHSIC'>Sensitivity Indices based on the Hilbert-Schmidt Independence Criterion (HSIC)</h2><span id='topic+sensiHSIC'></span><span id='topic+tell.sensiHSIC'></span><span id='topic+print.sensiHSIC'></span><span id='topic+plot.sensiHSIC'></span>

<h3>Description</h3>

 
<p><code>sensiHSIC</code> allows to conduct <b>global sensitivity analysis (<abbr><span class="acronym">GSA</span></abbr>)</b> in many different contexts thanks to several sensitivity measures based on the <b>Hilbert-Schmidt independence criterion (<abbr><span class="acronym">HSIC</span></abbr>)</b>. The so-called <abbr><span class="acronym">HSIC</span></abbr> sensitivity indices depend on the kernels which are affected to the input variables <code class="reqn">Xi</code> as well as on the kernel which is affected to the output object <code class="reqn">Y</code>. For each random entity, a reproducing kernel Hilbert space (<abbr><span class="acronym">RKHS</span></abbr>) is associated to the chosen kernel and allows to represent probability distributions in an appropriate function space. The influence of <code class="reqn">Xi</code> on <code class="reqn">Y</code> is then measured through the distance between the joint probability distribution (true impact of <code class="reqn">Xi</code> on <code class="reqn">Y</code> in the numerical model) and the product of marginal distributions (no impact of <code class="reqn">Xi</code> on <code class="reqn">Y</code>) after embedding those distributions into a bivariate <abbr><span class="acronym">RKHS</span></abbr>. Such a GSA approach has three main advantages:
</p>

<ul>
<li><p> The input variables <code class="reqn">Xi</code> may be correlated.
</p>
</li>
<li><p> Any kind of mathematical object is supported (provided that a kernel function is available).
</p>
</li>
<li><p> Accurate estimation is possible even in presence of very few data (no more than a hundred of input-output samples).
</p>
</li></ul>

<p>In <code>sensiHSIC</code>, each input variable <code class="reqn">Xi</code> is expected to be scalar (either discrete or continous). On the contrary, a much wider collection of mathematical objects are supported for the output variable <code class="reqn">Y</code>. In particular, <code class="reqn">Y</code> may be:
</p>

<ul>
<li><p> A <b>scalar output</b> (either discrete or continous). If so, one single kernel family is selected among the kernel collection.
</p>
</li>
<li><p> A <b>low-dimensional vector output</b>. If so, a kernel is selected for each output variable and the final output kernel is built by <b>tensorization</b>.
</p>
</li>
<li><p> A <b>high-dimensional vector output</b> or a <b>functional output</b>. In this case, the output data must be seen as time series observations. Three different methods are proposed.
</p>

<ol>
<li><p> A preliminary <b>dimension reduction</b> may be performed. In order to achieve this, a <b>principal component analysis (<abbr><span class="acronym">PCA</span></abbr>)</b> based on the empirical covariance matrix helps identify the first terms of the Kharunen-Loeve expansion. The final output kernel is then built in the reduced subspace where the functional data are projected.
</p>
</li>
<li><p> The <b>dynamic time warping (<abbr><span class="acronym">DTW</span></abbr>)</b> algorithm may be combined with a translation-invariant kernel. The resulting <abbr><span class="acronym">DTW</span></abbr>-based output kernel is well-adapted to measure similarity between two given time series.
</p>
</li>
<li><p> The <b>global alignment kernel (<abbr><span class="acronym">GAK</span></abbr>)</b> may be directly applied on the functional data. Unlike the <abbr><span class="acronym">DTW</span></abbr> kernel, it was specifically designed to deal with time series and to measure the impact of one input variable on the shape of the output curve.
</p>
</li></ol>

</li></ul>

<p>Many variants of the original <abbr><span class="acronym">HSIC</span></abbr> indices are now available in <code>sensiHSIC</code>.
</p>

<ul>
<li> <p><b>Normalized <abbr><span class="acronym">HSIC</span></abbr> indices (<abbr><span class="acronym">R2-HSIC</span></abbr>)</b> <br />
The original <abbr><span class="acronym">HSIC</span></abbr> indices defined in Gretton et al. (2005) may be hard to interpret because they do not admit a universal upper bound. A first step to overcome this difficulty was enabled by Da Veiga (2015) with the definition of the <abbr><span class="acronym">R2-HSIC</span></abbr> indices. The resulting sensitivity indices can no longer be greater than <code class="reqn">1</code>.
</p>
</li>
<li> <p><b>Target <abbr><span class="acronym">HSIC</span></abbr> indices (<abbr><span class="acronym">T-HSIC</span></abbr>)</b> <br />
They were thought by Marrel and Chabridon (2021) to meet the needs of <b>target sensitivity analysis (<abbr><span class="acronym">TSA</span></abbr>)</b>. The idea is to measure the impact of each input variable <code class="reqn">Xi</code> on a specific part of the output distribution (for example the upper tail). To achieve this, a weight function <code class="reqn">w</code> is applied on <code class="reqn">Y</code> before computing <abbr><span class="acronym">HSIC</span></abbr> indices.
</p>
</li>
<li> <p><b>Conditional <abbr><span class="acronym">HSIC</span></abbr> indices (<abbr><span class="acronym">C-HSIC</span></abbr>)</b> <br />
They were thought by Marrel and Chabridon (2021) to meet the needs of <b>conditional sensitivity analysis (<abbr><span class="acronym">CSA</span></abbr>)</b>. The idea is to measure the impact of each input variable <code class="reqn">Xi</code> on <code class="reqn">Y</code> when a specific event occurs. This conditioning event is detected on the output variable <code class="reqn">Y</code> and its amplitude is taken into account thanks to a weight function <code class="reqn">w</code>.
</p>
</li>
<li> <p><b><abbr><span class="acronym">HSIC-ANOVA</span></abbr> indices</b> <br />
To improve the interpretability of <abbr><span class="acronym">HSIC</span></abbr> indices, Da Veiga (2021) came up with an <b>ANOVA-like decomposition</b> that allows to establish a strict separation of main effects and interaction effects in the <abbr><span class="acronym">HSIC</span></abbr> paradigm. The first-order and total-order <abbr><span class="acronym">HSIC-ANOVA</span></abbr> indices are then defined just as the first-order and total-order Sobol' indices. However, this framework only holds if two major assumptions are verified: the input variables <code class="reqn">Xi</code> must be mutually independent and all input kernels must belong to the very restrained class of <abbr><span class="acronym">ANOVA</span></abbr> kernels. 
</p>
</li></ul>

<p>As most sensitivity measures, <abbr><span class="acronym">HSIC</span></abbr> indices allow to rank the input variables <code class="reqn">Xi</code> according to the influence they have on the output variable <code class="reqn">Y</code>. They can also be used for a screening purpose, that is to distinguish between truly influential input variables and non-influential input variables. The user who is interested in this topic is invited to consult the documentation of the function <code>testHSIC</code>. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sensiHSIC(model = NULL, X, target = NULL, cond = NULL, 
          kernelX = "rbf", paramX = NA,
          kernelY = "rbf", paramY = NA,
          estimator.type = "V-stat",
          nboot = 0, conf = 0.95,
          anova = list(obj = "no", is.uniform = TRUE),
          sensi = NULL, 
          save.GM = list(KX = TRUE, KY = TRUE), ...)
          
## S3 method for class 'sensiHSIC'
tell(x, y = NULL, ...)

## S3 method for class 'sensiHSIC'
print(x, ...)

## S3 method for class 'sensiHSIC'
plot(x, ylim = c(0, 1), ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sensiHSIC_+3A_model">model</code></td>
<td>
<p>A function, or a statistical model with a <code>predict</code> method. It defines the input-output model that needs to be studied.</p>
</td></tr>
<tr><td><code id="sensiHSIC_+3A_x">X</code></td>
<td>
<p>A <code class="reqn">n</code>-by-<code class="reqn">p</code> matrix containing all input samples. It comprises <code class="reqn">n</code> joint observations of the <code class="reqn">p</code> input variables.
</p>

<ul>
<li><p> If the user is only wanting to estimate <abbr><span class="acronym">HSIC</span></abbr> indices or <abbr><span class="acronym">R2-HSIC</span></abbr> indices, the input variables can be correlated.
</p>
</li>
<li><p> If the user is also wanting to estimate <abbr><span class="acronym">HSIC-ANOVA</span></abbr> indices, the input variables have to be mutually independent.
</p>
</li></ul>

</td></tr>
<tr><td><code id="sensiHSIC_+3A_target">target</code></td>
<td>
<p>A list of options to perform TSA. At least, <code>target</code> must contain an option named <code>"c"</code>. For other options, there exist default assignments.
</p>

<ul>
<li> <p><code>type</code> is a string specifying the weight function. Available choices include <code>"indicTh"</code>, <code>"zeroTh"</code>, <code>"logistic"</code> and <code>"exp1side"</code>. Default value is <code>"exp1side"</code>.
</p>

<ul>
<li> <p><code>"indicTh"</code> and <code>"zeroTh"</code> only depend on a threshold parameter. 
</p>
</li>
<li> <p><code>"logistic"</code> and <code>"exp1side"</code> depend on both a threshold parameter and a smoothness parameter.
</p>
</li></ul>

</li>
<li> <p><code>c</code> is a scalar value specifying the threshold parameter.
</p>
</li>
<li> <p><code>upper</code> is a boolean indicating whether the target region is located above (<code>TRUE</code>) or below (<code>FALSE</code>) the threshold parameter <code>c</code>. Only relevant when <code>type</code> is <code>"indicTh"</code>, <code>"zeroTh"</code> or <code>"exp1side"</code>. Default value is <code>TRUE</code>. 
</p>
</li>
<li> <p><code>param</code> is a scalar value specifying the smoothness parameter. Only relevant when <code>type</code> is <code>"logistic"</code> or <code>"exp1side"</code>. Default value is <code>1</code>.
</p>
</li></ul>

</td></tr>
<tr><td><code id="sensiHSIC_+3A_cond">cond</code></td>
<td>
<p>A list of options to perform CSA. At least, <code>cond</code> must contain an option named <code>"c"</code>. For other options, there exist default assignments.
</p>

<ul>
<li> <p><code>type</code> is a string specifying the weight function. Available choices include <code>"indicTh"</code>, <code>"zeroTh"</code>, <code>"logistic"</code> and <code>"exp1side"</code>. Default value is <code>"exp1side"</code>.
</p>

<ul>
<li> <p><code>"indicTh"</code> and <code>"zeroTh"</code> only depend on a threshold parameter. 
</p>
</li>
<li> <p><code>"logistic"</code> and <code>"exp1side"</code> depend on both a threshold parameter and a smoothness parameter.
</p>
</li></ul>

</li>
<li> <p><code>c</code> is a scalar value specifying the threshold parameter.
</p>
</li>
<li> <p><code>upper</code> is a boolean indicating whether the conditioning region is located above (<code>TRUE</code>) or below (<code>FALSE</code>) the threshold parameter <code>c</code>. Only relevant when <code>type</code> is <code>"indicTh"</code>, <code>"zeroTh"</code> or <code>"exp1side"</code>. Default value is <code>TRUE</code>. 
</p>
</li>
<li> <p><code>param</code> is a scalar value specifying the smoothness parameter. Only relevant if <code>type</code> is <code>"logistic"</code> or <code>"exp1side"</code>. Default value is <code>1</code>.
</p>
</li></ul>

</td></tr>
<tr><td><code id="sensiHSIC_+3A_kernelx">kernelX</code></td>
<td>
<p>A string or a vector of <code class="reqn">p</code> strings that specifies how to choose input kernels.
</p>

<ul>
<li><p> If only one string is provided, the associated kernel is affected to all inputs.
</p>
</li>
<li><p> For dimension-wise kernel selection, a vector of <code class="reqn">p</code> strings must be provided.
</p>
</li></ul>

<p>For each input variable, available choices include <code>"categ"</code> (categorical kernel), <code>"dcov"</code> (covariance kernel of the fractional Brownian motion), <code>"invmultiquad"</code> (inverse multiquadratic kernel), <code>"laplace"</code> (exponential kernel), <code>"linear"</code> (dot-product kernel), <code>"matern3"</code> (Matern <code class="reqn">3/2</code> kernel), <code>"matern5"</code> (Matern <code class="reqn">5/2</code> kernel), <code>"raquad"</code> (rationale quadratic kernel), <code>"rbf"</code> (Gaussian kernel), <code>"sobolev1"</code> (Sobolev kernel with smoothness parameter <code class="reqn">r=1</code>) and <code>"sobolev2"</code> (Sobolev kernel with smoothness parameter <code class="reqn">r=2</code>).
</p>
<p>In addition, let us assume that all input variables are uniformly distributed on <code class="reqn">[0,1]</code>. Under this assumption, the kernels <code>"laplace"</code>, <code>"matern3"</code>, <code>"matern5"</code> and <code>"rbf"</code> can be easily transformed into <abbr><span class="acronym">ANOVA</span></abbr> kernels. The resulting kernels are respectively called <code>"laplace_anova"</code>, <code>"matern3_anova"</code>, <code>"matern5_anova"</code> and <code>"rbf_anova"</code>.
</p>

<ul>
<li><p> One-parameter kernels: <code>"categ"</code>, <code>"dcov"</code>, <code>"invmultiquad"</code>, <code>"laplace"</code>, <code>"laplace_anova"</code>, <code>"matern3"</code>, <code>"matern3_anova"</code>, <code>"matern5"</code>, <code>"matern5_anova"</code>, <code>"raquad"</code>, <code>"rbf"</code> and <code>"rbf_anova"</code>.
</p>
</li>
<li><p> Parameter-free kernels: <code>"linear"</code>, <code>"sobolev1"</code> and <code>"sobolev2"</code>. 
</p>
</li></ul>

</td></tr>
<tr><td><code id="sensiHSIC_+3A_paramx">paramX</code></td>
<td>
<p>A scalar value or a vector of <code class="reqn">p</code> values with input kernel parameters.
</p>

<ul>
<li><p> If <code>paramX=NA</code>, input kernel parameters are computed automatically with rules of thumb.
</p>
</li>
<li><p> If <code>paramX</code> is a scalar value, it is affected to all input kernels.
</p>
</li>
<li><p> For dimension-wise kernel parametrization, a vector of <code class="reqn">p</code> values must be provided. If <code>kernelX</code> combines one-parameter kernels and parameter-free kernels, <code>NA</code> must be specified for parameter-free kernels.
</p>
</li></ul>

</td></tr> 
<tr><td><code id="sensiHSIC_+3A_kernely">kernelY</code></td>
<td>
<p>A string, a vector of <code class="reqn">q</code> strings or a list of options that specifies how to construct the output kernel. Regardless of its mathematical nature, the model output must be envisioned as a <code class="reqn">q</code>-dimensional random vector.
</p>
<p>To deal with a <b>scalar output</b> or a <b>low-dimensional vector output</b>, it is advised to select one kernel per output dimension and to tensorize all selected kernels. In this case, <code>kernelY</code> must be a string or a vector of <code class="reqn">q</code> strings.
</p>

<ul>
<li><p> If only one string is provided, the associated kernel is repeated <code class="reqn">q</code> times.
</p>
</li>
<li><p> For dimension-wise kernel selection, a vector of <code class="reqn">q</code> strings must be provided.
</p>
</li></ul>

<p>Have a look at <code>kernelX</code> for an exhaustive list of available kernels.
</p>
<p>To deal with a <b>high-dimensional vector output</b> or a <b>functional output</b>, it is advised to reduce dimension or to use a dedicated kernel. In this case, <code>kernelY</code> must be specified as a list of options. At least, <code>kernelY</code> must contain an option named <code>"method"</code>. For other options, there exist default assignments.
</p>

<ul>
<li> <p><code>method</code> is a string indicating the strategy used to construct the output kernel. Available choices include <code>"PCA"</code> (dimension reduction through principal component analysis), <code>"DTW"</code> (dynamic type warping) and <code>"GAK"</code> (global alignment kernel).
</p>
</li></ul>


<ol>
<li><p> If <code>method="PCA"</code>, the following options may also be specified:
</p>

<ul>
<li> <p><code>data.centering</code> is a boolean indicating whether the input samples must be centered before performing the preliminary PCA. Default value is <code>TRUE</code>.
</p>
</li>
<li> <p><code>data.scaling</code> is a boolean indicating whether the input samples must be scaled before performing the preliminary PCA. Default value is <code>TRUE</code>.
</p>
</li>
<li> <p><code>fam</code> is a string specifying the input kernel which is applied on principal components. Available choices only include <code>"dcov"</code>, <code>"invmultiquad"</code>, <code>"laplace"</code>, <code>"linear"</code>, <code>"matern3"</code>, <code>"matern5"</code>, <code>"raquad"</code> and <code>"rbf"</code>. Default value is <code>"rbf"</code>.
</p>
</li>
<li> <p><code>expl.var</code> is a scalar value (between <code class="reqn">0</code> and <code class="reqn">1</code>) specifying the expected percentage of output variance that must be explained by <abbr><span class="acronym">PCA</span></abbr>. Default value is <code>0.95</code>.
</p>
</li>
<li> <p><code>PC</code> is the expected number of principal components in <abbr><span class="acronym">PCA</span></abbr>. Default value is <code>NA</code>.
</p>
</li>
<li> <p><code>combi</code> is a string indicating how the final output kernel is built in the reduced subspace. Available options include <code>"sum"</code> or <code>"prod"</code>. The chosen kernel in <code>fam</code> is applied on all principal components before summation (if <code>"sum"</code>) or tensorization (if <code>"prod"</code>).
</p>
</li>
<li> <p><code>position</code> is a string indicating whether weights have to be involved in the construction of the final output kernel in the reduced subspace. Available choices include <code>"nowhere"</code> (no weights), <code>"intern"</code> (weights applied on principal components) or <code>"extern"</code> (weights applied on kernels). Default value is <code>"intern"</code>.
</p>
</li></ul>

<p><b>Remark:</b> <code>expl.var</code> and <code>PC</code> are conflicting options. Only one of both needs to be specified and the other one must be set to <code>NA</code>. If both are specified, <code>expl.var</code> is prioritized. If both are set to <code>NA</code>, <code>expl.var</code> is then set to its default value.
</p>
</li>
<li><p> If <code>method="DTW"</code>, the following option may also be specified:
</p>

<ul>
<li> <p><code>fam</code> is a string specifying the translation-invariant kernel which is combined with <abbr><span class="acronym">DTW</span></abbr>. Available choices only include <code>"invmultiquad"</code>, <code>"laplace"</code>, <code>"matern3"</code>, <code>"matern5"</code>, <code>"raquad"</code> and <code>"rbf"</code>. Default value is <code>"rbf"</code>.
</p>
</li></ul>

</li>
<li><p> If <code>method="GAK"</code>, there is no other option to specify.
</p>
</li></ol>

</td></tr>
<tr><td><code id="sensiHSIC_+3A_paramy">paramY</code></td>
<td>
<p>A scalar value or a vector of values with output kernel parameters.
</p>

<ul>
<li><p> If <code>paramY=NA</code>, output kernel parameters are computed automatically with rules of thumb.
</p>
</li></ul>

<p>In other cases, <code>paramY</code> must be specified in agreement with <code>kernelY</code>. 
</p>
<p><b>Case 1:</b> <code>kernelY</code> is a string or a vector of <code class="reqn">q</code> strings.
</p>
<p><code>paramY</code> must be a scalar value or a vector of <code class="reqn">q</code> values with output kernel parameters.
</p>

<ul>
<li><p> If <code>paramY</code> is a scalar value, it is affected to all output kernels.
</p>
</li>
<li><p> For dimension-wise kernel parametrization, a vector of <code class="reqn">q</code> values must be provided. If <code>kernelY</code> combines one parameter kernels and parameter-free kernels, <code>NA</code> must be specified for parameter-free kernels.
</p>
</li></ul>

<p><b>Case 2:</b> <code>kernelY</code> is a list of options with <code>method="PCA"</code>.
</p>
<p><code>paramY</code> must be set to <code>NA</code> because the parameters involved in the final output kernel are computed automatically. Their optimal tuning depends on the reduced subspace given by <abbr><span class="acronym">PCA</span></abbr>.
</p>
<p><b>Case 3:</b> <code>kernelY</code> is a list of options with <code>method="DTW"</code>.
</p>
<p><code>paramY</code> must be set to <code>NA</code>.
</p>
<p><b>Case 4:</b> <code>kernelY</code> is a list of options with <code>method="GAK"</code>.
</p>
<p><code>paramY</code> must be a vector of <code class="reqn">2</code> values. If the user only wants to specify one parameter, the other one must be set to <code>NA</code>. The two parameters correspond to the arguments <code>sigma</code> and <code>window.size</code> in the function <code>gak</code> from the package <code>dtwclust</code>. However, automatical computation (specified by <code>paramY=NA</code>) is strongly advised for this kind of output kernel.
</p>
</td></tr>
<tr><td><code id="sensiHSIC_+3A_estimator.type">estimator.type</code></td>
<td>
<p>A string specifying the kind of estimator used for HSIC indices. Available choices include <code>"U-stat"</code> (U-stastics) and <code>"V-stat"</code> (V-statistics). U-statistics are unbiased estimators. V-statistics are biased estimators but they become unbiased asymptotically. In the specific case of <abbr><span class="acronym">HSIC</span></abbr> indices, V-statistics are non-negative estimators and they offer more flexibility for further test procedures (see <code>testHSIC</code>). Both kinds of estimators can be computed with complexity <code class="reqn">O(n^2)</code> where <code class="reqn">n</code> denotes the sample size.</p>
</td></tr>
<tr><td><code id="sensiHSIC_+3A_nboot">nboot</code></td>
<td>
<p>Number of bootstrap replicates.</p>
</td></tr>
<tr><td><code id="sensiHSIC_+3A_conf">conf</code></td>
<td>
<p>A scalar value (between <code class="reqn">0</code> and <code class="reqn">1</code>) specifying the level of confidence intervals.</p>
</td></tr>
<tr><td><code id="sensiHSIC_+3A_anova">anova</code></td>
<td>
<p>A list of parameters to achieve an <abbr><span class="acronym">ANOVA</span></abbr>-like decomposition based on HSIC indices. At least, <code>anova</code> must contain an option named <code>"obj"</code>. For other options, there exist default assignments.
</p>

<ul>
<li> <p><code>obj</code> is a string specifying which kinds of <abbr><span class="acronym">HSIC-ANOVA</span></abbr> indices are expected. Available choices include <code>"no"</code> (<code>anova</code> is disabled), <code>"FO"</code> (first-order only), <code>"TO"</code> (total-order only) and <code>"both"</code> (first-order and total-order). 
</p>
</li>
<li> <p><code>is.uniform</code> is a boolean indicating whether the samples stored in <code>X</code> come from random variables that are uniformly distributed on <code class="reqn">[0,1]</code>. Let us recall that <abbr><span class="acronym">HSIC-ANOVA</span></abbr> indices can only be computed by means of <abbr><span class="acronym">ANOVA</span></abbr> kernels. Among available kernels, only <code>"laplace_anova"</code>, <code>"matern3_anova"</code>, <code>"matern5_anova"</code>, <code>"rbf_anova"</code>, <code>"sobolev1"</code> and <code>"sobole2"</code> verify this constraint (provided that all input variables are uniformly distributed on <code class="reqn">[0,1]</code>).
</p>

<ul>
<li><p> If <code>is.uniform=TRUE</code>, it is checked that each input data stored in <code class="reqn">X</code> actually lies in <code class="reqn">[0,1]</code>. If this condition is not verified, an error is returned.
</p>
</li>
<li><p> If <code>is.uniform=FALSE</code>, non-parametric rescaling (based on empirical distribution functions) is operated. 
</p>
</li></ul>

</li></ul>

</td></tr>
<tr><td><code id="sensiHSIC_+3A_sensi">sensi</code></td>
<td>
<p>An object of class <code>"sensiHSIC"</code> resulting from a prior call to the hereby function. If an object of class <code>"sensiHSIC"</code> is indeed provided, the following happens:
</p>

<ul>
<li><p> If <code>sensi</code> contains an object named <code>"KX"</code>, it is extracted from <code>sensi</code> and the input Gram matrices (required to estimate <abbr><span class="acronym">HSIC</span></abbr> indices) are not built from <code>X</code>, <code>kernelX</code> and <code>paramX</code>.
</p>
</li>
<li><p> If <code>sensi</code> contains an object named <code>"KY"</code>, it is extracted from <code>sensi</code> and the output Gram matrix (required to estimate <abbr><span class="acronym">HSIC</span></abbr> indices) is not built from <code>model</code>, <code>kernelY</code> and <code>paramY</code>. 
</p>
</li></ul>

</td></tr>
<tr><td><code id="sensiHSIC_+3A_save.gm">save.GM</code></td>
<td>
<p>A list of parameters indicating whether Gram matrices have to be saved. The list <code>save.GM</code> must contain options named <code>"KX"</code> and <code>"KY"</code>.
</p>

<ul>
<li> <p><code>KX</code> is a boolean indicating whether the input Gram matrices have to be saved.
</p>
</li>
<li> <p><code>KY</code> is a boolean indicating whether the output Gram matrix has to be saved.
</p>
</li></ul>

</td></tr>
<tr><td><code id="sensiHSIC_+3A_x">x</code></td>
<td>
<p>An object of class <code>"sensiHSIC"</code> storing the state of the sensitivity study (parameters, data, estimates).</p>
</td></tr>
<tr><td><code id="sensiHSIC_+3A_y">y</code></td>
<td>
<p>A <code class="reqn">n</code>-by-<code class="reqn">q</code> matrix containing all output samples. It comprises <code class="reqn">n</code> observations of the <code class="reqn">q</code> output variables.</p>
</td></tr>
<tr><td><code id="sensiHSIC_+3A_ylim">ylim</code></td>
<td>
<p>A vector of two values specifying the <code class="reqn">y</code>-coordinate plotting limits.</p>
</td></tr>
<tr><td><code id="sensiHSIC_+3A_...">...</code></td>
<td>
<p>Any other arguments for <code>model</code> which are passed unchanged each time <code>model</code> is called.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Let <code class="reqn">(Xi,Y)</code> be an input-output pair. The kernels assigned to <code class="reqn">Xi</code> and <code class="reqn">Y</code> are respectively denoted by <code class="reqn">Ki</code> and <code class="reqn">KY</code>. 
</p>
<p>For many global sensitivity measures, the influence of <code class="reqn">Xi</code> on <code class="reqn">Y</code> is measured in the light of the probabilistic dependence that exists within the input-output pair <code class="reqn">(Xi,Y)</code>. For this, a dissimilarity measure is applied between the joint probability distribution (true impact of <code class="reqn">Xi</code> and <code class="reqn">Y</code> in the numerical model) and the product of marginal distributions (no impact of <code class="reqn">Xi</code> on <code class="reqn">Y</code>). For instance, Borgonovo's sensitivity measure is built upon the total variation distance between those two probability distributions. See Borgonovo and Plischke (2016) for further details.
</p>
<p>The <abbr><span class="acronym">HSIC</span></abbr>-based sensitivity measure can be understood in this way since the index <code class="reqn">HSIC(Xi,Y)</code> results from the application of the <b>Hilbert-Schmidt independence criterion (<abbr><span class="acronym">HSIC</span></abbr>)</b> on the pair <code class="reqn">(Xi,Y)</code>. This criterion is nothing but a special kind of dissimilarity measure between the joint probability distribution and the product of marginal distributions. This dissimilarity measure is called the <b>maximum mean discrepancy (MMD)</b> and its definition relies on the selected kernels <code class="reqn">Ki</code> and <code class="reqn">KY</code>. According to the theory of reproducing kernels, every kernel <code class="reqn">K</code> is related to a <b>reproducing kernel Hilbert space (<abbr><span class="acronym">RKHS</span></abbr>)</b>.Then, if <code class="reqn">K</code> is affected to a random variable <code class="reqn">Z</code>, any probability distribution describing the random behavior of <code class="reqn">Z</code> may be represented within the induced <abbr><span class="acronym">RKHS</span></abbr>. In this setup, the dissimilarity between the joint probability distribution and the product of marginal distributions is then measured through the squared norm of their images into the bivariate <abbr><span class="acronym">RKHS</span></abbr>. The user is referred to Gretton et al. (2006) for additional details on the mathematical construction of <abbr><span class="acronym">HSIC</span></abbr> indices.
</p>
<p>In practice, it may be difficult to understand how <code class="reqn">HSIC(Xi,Y)</code> measures dependence within <code class="reqn">(Xi,Y)</code>. An alternative definition relies on the concept of <b>feature map</b>. Let us recall that the value taken by a kernel function can always be seen as the scalar product of two <b>feature functions</b> lying in a <b>feature space</b>. Definition 1 in Gretton et al. (2005) introduces <code class="reqn">HSIC(Xi,Y)</code> as the Hilbert-Schmidt norm of a covariance-like operator between random features. For this reason, having access to the input and output feature maps may help identify the dependence patterns captured by <code class="reqn">HSIC(Xi,Y)</code>.
</p>
<p>Kernels must be chosen very carefully. There exists a wide variety of kernels but only a few f them meet the needs of <abbr><span class="acronym">GSA</span></abbr>. As <code class="reqn">HSIC(Xi,Y)</code> is supposed to be a dependence measure, it must be equal to <code class="reqn">0</code> if and only if <code class="reqn">Xi</code> and <code class="reqn">Y</code> are independent. A sufficient condition to enable this equivalence is to take two characteristic kernels. The reader is referred to Fukumizu et al. (2004) for the mathematical definition of a characteristic kernel and to Sriperumbur et al. (2010) for an overview of the major related results. In particular:
</p>

<ul>
<li><p> The Gaussian kernel, the Laplace kernel, the Matern <code class="reqn">3/2</code> kernel and the Matern <code class="reqn">5/2</code> kernel (all defined on <code class="reqn">R^2</code>) are <b>characteristic</b>.
</p>
</li>
<li><p> The transformed versions of the four abovementioned kernels (all defined on <code class="reqn">[0,1]^2</code>) are <b>characteristic</b>.
</p>
</li>
<li><p> All Sobolev kernels (defined on <code class="reqn">[0,1]^2</code>) are <b>characteristic</b>.
</p>
</li>
<li><p> The categorical kernel (defined on any discrete probability space) is <b>characteristic</b>.
</p>
</li></ul>

<p>Lemma 1 in Gretton et al. (2005) provides a third way of defining <code class="reqn">HSIC(Xi,Y)</code>. Since the associated formula is only based on three expectation terms, the corresponding estimation procedures are very simple and they do not ask for a large amount of input-output samples to be accurate. Two kinds of estimators may be used for <code class="reqn">HSIC(Xi,Y)</code>: the <b>V-statistic estimator</b> (which is non negative, biased and asymptotically unbiased) or the <b>U-statistic estimator</b> (unbiased). For both estimators,  the computational complexity is <code class="reqn">O(n^2)</code> where <code class="reqn">n</code> is the sample size.
</p>
<p>The user must always keep in mind the key steps leading to the estimation of <code class="reqn">HSIC(Xi,Y)</code>:
</p>

<ul>
<li><p> Input samples are simulated and the corresponding output samples are computed with the numerical model.
</p>
</li>
<li><p> An input kernel <code class="reqn">Ki</code> and an output kernel <code class="reqn">KY</code> are selected.
</p>
</li>
<li> <p><b>In case of target sensitivity analysis:</b> output samples are transformed by means of a weight function <code class="reqn">w</code>.
</p>
</li>
<li><p> The input and output Gram matrices are constructed.
</p>
</li>
<li> <p><b>In case of conditional sensitivity analysis:</b> conditioning weights are computed by means of a weight function <code class="reqn">w</code>.
</p>
</li>
<li><p> The final estimate is computed. It depends on the selected estimator type (either a U-statistic or a V-statistic).
</p>
</li></ul>



<h4>Kernel functions for random variables</h4>

<p>All what follows is written for a scalar output <code class="reqn">Y</code> but the same is true for any scalar input <code class="reqn">Xi</code>. 
</p>
<p>Let <code class="reqn">D</code> denote the support of the output probability distribution. A kernel is a symmetric and positive definite function defined on the domain <code class="reqn">D</code>. Different kernel families are available in <code>sensiHSIC</code>. 
</p>

<ul>
<li><p> To deal with continuous probability distributions on <code class="reqn">R</code>, one can use:
</p>

<ul>
<li><p> The covariance kernel of the fractional Browian motion (<code>"dcov"</code>), the inverse multiquadratic kernel (<code>"invmultiquad"</code>), the exponential kernel (<code>"laplace"</code>), the dot-product kernel (<code>"linear"</code>), the Matern <code class="reqn">3/2</code> kernel (<code>"matern3"</code>), the Matern <code class="reqn">5/2</code> kernel (<code>"matern5"</code>), the rationale quadratic kernel (<code>"raquad"</code>) and the Gaussian kernel (<code>"rbf"</code>).
</p>
</li></ul>

</li>
<li><p> To deal with continuous probability distributions on <code class="reqn">[0,1]</code>, one can use:
</p>

<ul>
<li><p> Any of the abovementioned kernel (restricted to <code class="reqn">[0,1]</code>).
</p>
</li>
<li><p> The transformed exponential kernel (<code>"laplace_anova"</code>), the transformed Matern <code class="reqn">3/2</code> kernel (<code>"matern3_anova"</code>), the transformed Matern <code class="reqn">5/2</code> kernel (<code>"matern5_anova"</code>), the transformed Gaussian kernel (<code>"rbf_anova"</code>), the Sobolev kernel with smoothness parameter <code class="reqn">r=1</code> (<code>"sobolev1"</code>) and the Sobolev kernel with smoothness parameter <code class="reqn">r=2</code> (<code>"sobolev2"</code>).
</p>
</li></ul>

</li>
<li><p> To deal with any discrete probability distribution, the categorical kernel (<code>"categ"</code>) must be used.
</p>
</li></ul>

<p>Two kinds of kernels must be distinguished:
</p>

<ul>
<li> <p><b>Parameter-free kernels:</b> the dot-product kernel (<code>"linear"</code>), the Sobolev kernel with smoothness parameter <code class="reqn">r=1</code> (<code>"sobolev1"</code>) and the Sobolev kernel with smoothness parameter <code class="reqn">r=2</code> (<code>"sobolev2"</code>).
</p>
</li>
<li> <p><b>One-parameter kernels:</b> the categorical kernel (<code>"categ"</code>), the covariance kernel of the fractional Brownian motion kernel (<code>"dcov"</code>), the inverse multiquadratic kernel (<code>"invmultiquad"</code>), the exponential kernel (<code>"laplace"</code>), the transformed exponential kernel (<code>"laplace_anova"</code>), the Matern <code class="reqn">3/2</code> kernel (<code>"matern3"</code>), the transformed Matern <code class="reqn">3/2</code> kernel (<code>"matern3_anova"</code>), the Matern <code class="reqn">5/2</code> kernel (<code>"matern5"</code>), the transformed Matern <code class="reqn">5/2</code> kernel (<code>"matern5_anova"</code>), the rationale quadratic kernel (<code>"raquad"</code>), the Gaussian kernel (<code>"rbf"</code>) and the transformed Gaussian kernel (<code>"rbf_anova"</code>).
</p>
</li></ul>

<p>A major issue related to one-parameter kernels is how to set the parameter. It mainly depends on the role played by the parameter in the kernel expression.
</p>

<ul>
<li><p> For translation-invariant kernels and their <abbr><span class="acronym">ANOVA</span></abbr> variants (that is all one-parameter kernels except <code>"categ"</code> and <code>"dcov"</code>), the parameter may be interpreted as a correlation length (or a scale parameter). The rule of thumb is to compute the empirical standard deviation of the provided samples.
</p>
</li>
<li><p> For the covariance kernel of the fractional Brownian motion (<code>"dcov"</code>), the parameter is an exponent. Default value is <code class="reqn">1</code>.
</p>
</li>
<li><p> For the categorical kernel (<code>"categ"</code>), the parameter has no physical sense. It is just a kind of binary encoding.
</p>

<ul>
<li> <p><code class="reqn">0</code> means the user wants to use the basic categorical kernel.
</p>
</li>
<li> <p><code class="reqn">1</code> means the user wants to use the weighted variant of the categorical kernel.
</p>
</li></ul>

</li></ul>




<h4>How to deal with a low-dimensional vector output?</h4>

<p>Let us assume that the output vector <code class="reqn">Y</code> is composed of <code class="reqn">q</code> random variables <code class="reqn">Y1,...,Yq</code>.
</p>
<p>A kernel <code class="reqn">Kj</code> is affected to each output variable <code class="reqn">Yj</code> and this leads to embed the <code class="reqn">j</code>-th output probability distribution in a <abbr><span class="acronym">RKHS</span></abbr> denoted by <code class="reqn">Hj</code>. Then, the <b>tensorization</b> of <code class="reqn">H1,...,Hq</code> allows to build the final <abbr><span class="acronym">RKHS</span></abbr>, that is the <abbr><span class="acronym">RKHS</span></abbr> where the <code class="reqn">q</code>-variate output probability distribution describing the overall random behavior of <code class="reqn">Y</code> will be embedded. In this situation:
</p>

<ul>
<li><p> The final output kernel is the tensor product of all output kernels.
</p>
</li>
<li><p> The final output Gram matrix is the Hadamard product of all output Gram matrices.
</p>
</li></ul>

<p>Once the final output Gram matrix is built, <abbr><span class="acronym">HSIC</span></abbr> indices can be estimated, just as in the case of a scalar output.
</p>



<h4>How to deal with a high-dimensional vector output or a functional output?</h4>

<p>In <code>sensiHSIC</code>, three different methods are proposed in order to compute <abbr><span class="acronym">HSIC</span></abbr>-based sensitivity indices in presence of functional outputs.
</p>
<p><b>Dimension reduction</b>
</p>
<p>This approach was initially proposed by Da Veiga (2015). The key idea is to approximate the random functional output by the first terms of its <b>Kharunen-Loeve expansion</b>. This can be achived with a <b>principal component analysis (PCA)</b> that is carried out on the empirical covariance matrix.
</p>

<ul>
<li><p> The eigenvectors (or <b>principal directions</b>) allow to approximate the (deterministic) functional terms involved in the Kharunen-Loeve decomposition.
</p>
</li>
<li><p> The eigenvalues allow to determine how many principal directions are sufficient in order to accurately represent the random function by means of its truncated Kharunen-Loeve expansion. The key idea behind dimension reduction is to keep as few principal directions as possible while preserving a prescribed level of explained variance.
</p>
</li></ul>

<p>The <b>principal components</b> are the coordinates of the functional output in the low-dimensional subspace resulting from <abbr><span class="acronym">PCA</span></abbr>. There are computed for all output samples (time series observations). See Le Maitre and Knio (2010) for more detailed explanations.
</p>
<p>The last step consists in constructing a kernel in the reduced subspace. One single kernel family is selected and affected to all principal directions. Moreover, all kernel parameters are computed automatically (with appropriate rules of thumb). Then, several strategies may be considered.
</p>

<ul>
<li><p> The initial method described in Da Veiga (2015) is based on a direct tensorization. One can also decide to sum kernels.
</p>
</li>
<li><p> This approach was improved by El Amri and Marrel (2021). For each principal direction, a weight coefficient (equal the ratio between the eigenvalue and the sum of all selected eigenvalues) is computed.
</p>

<ul>
<li><p> The principal components are multiplied by their respective weight coefficients before summing kernels or tensorizing kernels.
</p>
</li>
<li><p> The kernels can also be directly applied on the principal components before being linearly combined according to the weight coefficients.
</p>
</li></ul>

</li></ul>

<p>In sensiHSIC, all these strategies correspond to the following specifications in <code>kernelY</code>:
</p>

<ul>
<li> <p><b>Direct tensorization:</b>
<code>kernelY=list(method="PCA", combi="prod", position="nowhere")</code>
</p>
</li>
<li> <p><b>Direct sum:</b>
<code>kernelY=list(method="PCA", combi="sum", position="nowhere")</code>
</p>
</li>
<li> <p><b>Rescaled tensorization:</b>
<code>kernelY=list(method="PCA", combi="prod", position="intern")</code>
</p>
</li>
<li> <p><b>Rescaled sum:</b> 
<code>kernelY=list(method="PCA", combi="sum", position="intern")</code>
</p>
</li>
<li> <p><b>Weighted linear combination:</b>
<code>kernelY=list(method="PCA", combi="sum", position="extern")</code>
</p>
</li></ul>

<p><b>Dynamic Time Warping (<abbr><span class="acronym">DTW</span></abbr>)</b>
</p>
<p>The <abbr><span class="acronym">DTW</span></abbr> algorithm developed by Sakoe and Chiba (1978) can be combined with a translation-invariant kernel in order to create a kernel function for times series. The resulting <abbr><span class="acronym">DTW</span></abbr>-based output kernel is well-adapted to measure similarity between two given time series.
</p>
<p>Suitable translation-invariant kernels include the inverse multiquadratic kernel (<code>"invmultiquad"</code>), the exponential kernel (<code>"laplace"</code>), the Matern <code class="reqn">3/2</code> kernel (<code>"matern3"</code>), the Matern <code class="reqn">5/2</code> kernel (<code>"matern5"</code>), the rationale quadratic kernel (<code>"raquad"</code>) and the Gaussian kernel (<code>"rbf"</code>).
</p>
<p>The user is warned against the fact that <abbr><span class="acronym">DTW</span></abbr>-based kernels are not positive definite functions. As a consequence, many theoretical properties do not hold anymore for <abbr><span class="acronym">HSIC</span></abbr> indices.
</p>
<p>For faster computations, <code>sensiHSIC</code> is using the function <code>dtw_dismat</code> from the package <code>incDTW</code>.
</p>
<p><b>Global Alignment Kernel (<abbr><span class="acronym">GAK</span></abbr>)</b>
</p>
<p>Unlike <abbr><span class="acronym">DTW</span></abbr>-based kernels, the <abbr><span class="acronym">GAK</span></abbr> is a positive definite function. This time-series kernel was originally introduced in Cuturi et al. (2007) and further investigated in Cuturi (2011). It was used to compute <abbr><span class="acronym">HSIC</span></abbr> indices on a simplified compartmental epidemiological model in Da Veiga (2021).
</p>
<p>For faster computations, <code>sensiHSIC</code> is using the function <code>gak</code> from the package <code>dtwclust</code>. 
</p>
<p>In <code>sensiHSIC</code>, two <abbr><span class="acronym">GAK</span></abbr>-related parameters may be tuned by the user with <code>paramY</code>. They exactly correspond to the arguments <code>sigma</code> and <code>window.size</code> in the function <code>gak</code>.
</p>



<h4>About normalized <abbr><span class="acronym">HSIC</span></abbr> indices (<abbr><span class="acronym">R2-HSIC</span></abbr>)</h4>

<p>No doubt interpretability is the major drawback of <abbr><span class="acronym">HSIC</span></abbr> indices. This shortcoming led Da Veiga (2021) to introduce a normalized version of <code class="reqn">HSIC(Xi,Y)</code>. The so-called R2-HSIC index is thus defined as the ratio between <code class="reqn">HSIC(Xi,Y)</code> and the square root of a normalizing constant equal to <code class="reqn">HSIC(Xi,Xi)*HSIC(Y,Y)</code>. 
</p>
<p>This normalized sensitivity measure is inspired from the <b>distance correlation measure</b> proposed by Szekely et al. (2007) and the resulting sensitivity indices are easier to interpret since they all fall in the interval <code class="reqn">[0,1]</code>.
</p>



<h4>About target <abbr><span class="acronym">HSIC</span></abbr> indices (<abbr><span class="acronym">T-HSIC</span></abbr>)</h4>

<p>T-HSIC indices were designed by Marrel and Chabridon (2021) for <abbr><span class="acronym">TSA</span></abbr>. They are only defined for a scalar output. Vector and functional outputs are not supported. The main idea of <abbr><span class="acronym">TSA</span></abbr> is to measure the influence of each input variable <code class="reqn">Xi</code> on a modified version of <code class="reqn">Y</code>. To do so, a preliminary mathematical transform <code class="reqn">w</code> (called the <b>weight function</b>) is applied on <code class="reqn">Y</code>. The collection of <abbr><span class="acronym">HSIC</span></abbr> indices is then estimated with respect to <code class="reqn">w(Y)</code>. Here are two examples of situations where <abbr><span class="acronym">TSA</span></abbr> is particularly relevant:
</p>

<ul>
<li><p> How to measure the impact of <code class="reqn">Xi</code> on the upper values taken by <code class="reqn">Y</code> (for example the values above a given threshold <code class="reqn">T</code>)?
</p>

<ul>
<li><p> To answer this question, one may take <code class="reqn">w(Y)=Y*1_{Y&gt;T}</code> (<b>zero-thresholding</b>). <br />
This can be specified in <code>sensiHSIC</code> with <code>target=list(c=T, type="zeroTh", upper=TRUE)</code>.
</p>
</li></ul>

</li>
<li><p> How to measure the influence of <code class="reqn">Xi</code> on the occurrence of the event <code class="reqn">{Y&gt;T}</code>?
</p>

<ul>
<li><p> To answer this question, one may take <code class="reqn">w(Y)=1_{Y&lt;T}</code> (<b>indicator-thresholding</b>). <br />
This can be specified in <code>sensiHSIC</code> with <code>target=list(c=T, type="indicTh", upper=FALSE)</code>.
</p>
</li></ul>

</li></ul>

<p>In Marrel and Chabridon (2021), the two situations described above are referred to as <b>&quot;hard thresholding&quot;</b>. To avoid using discontinuous weight functions, <b>&quot;smooth thresholding&quot;</b> may be used instead.
</p>

<ul>
<li><p> Spagnol et al. (2019): logistic transformation on both sides of the threshold <code class="reqn">T</code>.
</p>
</li>
<li><p> Marrel and Chabridon (2021): exponential transformation above or below the threshold <code class="reqn">T</code>.
</p>
</li></ul>

<p>These two smooth relaxation functions depend on a tuning parameter that helps control smoothness. For further details, the user is invited to consult the documentation of the function <code>weightTSA</code>.
</p>
<p><b>Remarks:</b>
</p>

<ul>
<li><p> When <code>type="indicTh"</code> (<b>indicator-thesholding</b>), <code class="reqn">w(Y)</code> becomes a binary random variable. Accordingly, the output kernel selected in <code>kernelY</code> must be the categorical kernel.
</p>
</li>
<li><p> In the spirit of <abbr><span class="acronym">R2-HSIC</span></abbr> indices, <abbr><span class="acronym">T-HSIC</span></abbr> indices can be normalized. The associated normalizing constant is equal to the square root of <code class="reqn">HSIC(Xi,Xi)*HSIC(w(Y),w(Y))</code>.
</p>
</li>
<li> <p><abbr><span class="acronym">T-HSIC</span></abbr> indices can be very naturally combined with the <abbr><span class="acronym">HSIC-ANOVA</span></abbr> decomposition proposed by Da Veiga (2021). As a consequence, the arguments <code>target</code> and <code>anova</code> in <code>sensiHSIC</code> can be enabled simultaneously. Compared with basic <abbr><span class="acronym">HSIC</span></abbr> indices, there are three main differences: the input variables must be mutually independent, <abbr><span class="acronym">ANOVA</span></abbr> kernels must be used for all input variables and the output of interest is <code class="reqn">w(Y)</code>.
</p>
</li>
<li> <p><abbr><span class="acronym">T-HSIC</span></abbr> indices can be very naturally combined with the tests of independence proposed in <code>testHSIC</code>. In this context, the null hypothesis is <code class="reqn">H0</code>: &quot;<code class="reqn">Xi</code> and <code class="reqn">w(Y)</code> are independent&quot;.
</p>
</li></ul>




<h4>About conditional <abbr><span class="acronym">HSIC</span></abbr> indices (<abbr><span class="acronym">C-HSIC</span></abbr>)</h4>

<p><abbr><span class="acronym">C-HSIC</span></abbr> indices were designed by Marrel and Chabridon (2021) for <abbr><span class="acronym">CSA</span></abbr>. They are only defined for a scalar output. Vector and functional outputs are not supported. The idea is to measure the impact of each input variable <code class="reqn">Xi</code> on <code class="reqn">Y</code> when a specific event occurs. This conditioning event is defined on <code class="reqn">Y</code> thanks to a <b>weight function</b> <code class="reqn">w</code>. In order to compute the conditioning weights, <code class="reqn">w</code> is applied on the output samples and an empirical normalization is carried out (so that the overall sum of conditioning weights is equal to <code class="reqn">1</code>). The conditioning weights are then combined with the simulated Gram matrices in order to estimate <abbr><span class="acronym">C-HSIC</span></abbr> indices. All formulas can be found in Marrel and Chabridon (2021). Here is an exemple of a situation where <abbr><span class="acronym">CSA</span></abbr> is particularly relevant:
</p>

<ul>
<li><p>  Let us imagine that the event <code class="reqn">{Y&gt;T}</code> coincides with a system failure. <br />
How to measure the influence of <code class="reqn">Xi</code> on <code class="reqn">Y</code> when failure occurs?
</p>

<ul>
<li><p> To answer this question, one may take <code class="reqn">w(Y) = 1_{Y&gt;T}</code> (<b>indicator-thresholding</b>). <br />
This can be specified in <code>sensiHSIC</code> with <code>cond=list(c=T, type="indicTh", upper=TRUE)</code>.
</p>
</li></ul>

</li></ul>

<p>The three other weight functions proposed for TSA (namely <code>"zeroTh"</code>, <code>"logistic"</code> and <code>"exp1side"</code>) can also be used but the role they play is less intuitive to understand. See Marrel and Chabridon (2021) for better explanations.
</p>
<p><b>Remarks:</b>
</p>

<ul>
<li><p> Unlike what is pointed out for <abbr><span class="acronym">TSA</span></abbr>, when <code>type="thresholding"</code>, the output of interest <code class="reqn">Y</code> remains a continuous random variable. The categorical kernel is thus inappropriate. A continuous kernel must be used instead.
</p>
</li>
<li><p> In the spirit of <abbr><span class="acronym">R2-HSIC</span></abbr> indices, <abbr><span class="acronym">C-HSIC</span></abbr> indices can be normalized. The associated normalizing constant is equal to the square root of <code class="reqn">C-HSIC(Xi,Xi)*C-HSIC(Y,Y)</code>.
</p>
</li>
<li><p> Only V-statistics are supported to estimate C-HSIC indices. The reason is because the normalized version of C-HSIC indices cannot always be estimated with U-statistics. In particular, the estimates of <code class="reqn">C-HSIC(Xi,Xi)*C-HSIC(Y,Y)</code> may be negative.
</p>
</li>
<li> <p><abbr><span class="acronym">C-HSIC</span></abbr> indices cannot be combined with the <abbr><span class="acronym">HSIC-ANOVA</span></abbr> decomposition proposed in Da Veiga (2021). In fact, the conditioning operation is feared to introduce statistical dependence among input variables, which forbids using <abbr><span class="acronym">HSIC-ANOVA</span></abbr> indices. As a consequence, the arguments <code>cond</code> and <code>anova</code> in <code>sensiHSIC</code> cannot be enabled simultaneously.
</p>
</li>
<li> <p><abbr><span class="acronym">C-HSIC</span></abbr> indices can harly be combined with the tests of inpendence proposed in <code>testHSIC</code>. This is only possible if <code>type="indicTh"</code>. In this context, the null hypothesis is <code class="reqn">H0</code>: &quot;<code class="reqn">Xi</code> and <code class="reqn">Y</code> are independent if the event described in <code>cond</code> occurs&quot;.
</p>
</li></ul>




<h4>About <abbr><span class="acronym">HSIC-ANOVA</span></abbr> indices</h4>

<p>In comparison with <abbr><span class="acronym">HSIC</span></abbr> indices, <abbr><span class="acronym">R2-HSIC</span></abbr> indices are easier to interpret. However, in terms of interpretability, Sobol' indices remain much more convenient since they can be understood as shares of the total output variance. Such an interpretation is made possible by the Hoeffding decomposition, also known as <abbr><span class="acronym">ANOVA</span></abbr> decomposition.
</p>
<p>It was proved in Da Veiga (2021) that an <abbr><span class="acronym">ANOVA</span></abbr>-like decomposition can be achived for <abbr><span class="acronym">HSIC</span></abbr> indices under certain conditions:
</p>

<ul>
<li><p> The input variables must be mutually independent (which was not required to compute all other kinds of <abbr><span class="acronym">HSIC</span></abbr> indices).
</p>
</li>
<li> <p><b><abbr><span class="acronym">ANOVA</span></abbr> kernels</b> must be assigned to all input variables.
</p>
</li></ul>

<p>This <abbr><span class="acronym">ANOVA</span></abbr> setup allows to establish a strict separation between main effects and interaction effects in the <abbr><span class="acronym">HSIC</span></abbr> sense. The first-order and total-order <abbr><span class="acronym">HSIC-ANOVA</span></abbr> indices are then defined in the same fashion than first-order and total-order Sobol' indices. It is worth noting that the <abbr><span class="acronym">HSIC-ANOVA</span></abbr> normalizing constant is equal to <code class="reqn">HSIC(X,Y)</code> and is thus different from the one used for <abbr><span class="acronym">R2-HSIC</span></abbr> indices.
</p>
<p>For a given probability measure <code class="reqn">P</code>, an <abbr><span class="acronym">ANOVA</span></abbr> kernel <code class="reqn">K</code> is a kernel that can rewritten <code class="reqn">1+k</code> where <code class="reqn">k</code> is an orthogonal kernel with respect to <code class="reqn">P</code>. Among the well-known parametric families of probability distributions and kernel functions, there are very few examples of orthogonal kernels. One example is given by <b>Sobolev kernels</b> when there are matched with the uniform probability measure on [0,1]. See Wahba et al. (1995) for further details on Sobolev kernels.
</p>
<p>Moreover, several strategies to construct orthogonal kernels from non-orthogonal kernels are recalled in Da Veiga (2021). One of them consists in translating the feature map so that the resulting kernel becomes centered at the prescribed probability measure <code class="reqn">P</code>. This can be done analytically for some basic kernels (Gaussian, exponential, Matern <code class="reqn">3/2</code> and Matern <code class="reqn">5/2</code>) when <code class="reqn">P</code> is the uniform measure on <code class="reqn">[0,1]</code>. See Section 9 in Ginsbourger et al. (2016) for the corresponding formulas.
</p>
<p>In <code>sensiHSIC</code>, <abbr><span class="acronym">ANOVA</span></abbr> kernels are only available for the uniform probability measure on <code class="reqn">[0,1]</code>. This includes the Sobolev kernel with parameter <code class="reqn">r=1</code> (<code>"sobolev1"</code>), the Sobolev kernel with parameter <code class="reqn">r=2</code> (<code>"sobolev2"</code>), the transformed Gaussian kernel (<code>"rbf_anova"</code>), the transformed exponential kernel (<code>"laplace_anova"</code>), the transformed Matern <code class="reqn">3/2</code> kernel (<code>"matern3_anova"</code>) and the transformed Matern <code class="reqn">5/2</code> kernel (<code>"matern5_anova"</code>).
</p>
<p>As explained above, the <abbr><span class="acronym">HSIC-ANOVA</span></abbr> indices can only be computed if all input variables are uniformly distributed on <code class="reqn">[0,1]</code>. Because of this limitation, a preliminary reformulation is needed if the <abbr><span class="acronym">GSA</span></abbr> problem includes other kinds of input probability distributions. The <b>probability integral transform (PIT)</b> must be applied on each input variable <code class="reqn">Xi</code>. In addition, all quantile functions must be encapsulated in the numerical model, which may lead to reconsider the way <code>model</code> is specified. In <code>sensiHSIC</code>, if <code>check=TRUE</code> is selected in <code>anova</code>, it is checked that all input samples lie in <code class="reqn">[0,1]</code>. If this is not the case, a non-parametric rescaling (based on empirical distribution functions) is operated.
</p>
<p><abbr><span class="acronym">HSIC-ANOVA</span></abbr> indices can be used for <abbr><span class="acronym">TSA</span></abbr>. The only difference with <abbr><span class="acronym">GSA</span></abbr> is the use of a weight function <code class="reqn">w</code>. On the contrary, CSA cannot be conducted with <abbr><span class="acronym">HSIC-ANOVA</span></abbr> indices. Indeed, the conditioning operation is feared to introduce statistical independence among the input variables, which prevents using the <abbr><span class="acronym">HSIC-ANOVA</span></abbr> approach.
</p>



<h3>Value</h3>

<p><code>sensiHSIC</code> returns a list of class <code>"sensiHSIC"</code>. It contains all the input arguments detailed before, except <code>sensi</code> which is not kept. It must be noted that some of them might have been altered, corrected or completed.
</p>
<table role = "presentation">
<tr><td><code>kernelX</code></td>
<td>
<p>A vector of <code class="reqn">p</code> strings with input kernels.</p>
</td></tr>
<tr><td><code>paramX</code></td>
<td>
<p>A vector of <code class="reqn">p</code> values with input kernel parameters. For each one-parameter kernel, a real number is returned. It is either the original value (if correct), a corrected value (if not) or the default value (computed from a rule of thumb when <code>NA</code> is specified). For each parameter-free kernel, <code>NA</code> is returned.</p>
</td></tr>
<tr><td><code>kernelY</code></td>
<td>
<p>A vector of <code class="reqn">q</code> strings or a list of options that specifies how the output kernel was constructed. In the case where <code>kernelY</code> is a list of options with <code>method="PCA"</code>, <code>kernelY</code> contains additional information resulting from <abbr><span class="acronym">PCA</span></abbr>.
</p>

<ul>
<li><p> If <code>kernelY</code> initally contained an option named <code>"expl.var"</code>, <code>kernelY</code> now also contains an option named <code>"PC"</code> that provides the associated number of principal components.
</p>
</li>
<li><p> If <code>kernelY</code> initially contained an option named <code>"PC"</code>, <code>kernelY</code> now also contains an option named <code>"expl.var"</code> that provides the associated percentage of output variance that is explained by <abbr><span class="acronym">PCA</span></abbr>.
</p>
</li>
<li><p> If <code>kernelY</code> initally contained an option named <code>"position"</code> that was set to <code>"intern"</code> or <code>"extern"</code>, <code>kernelY</code> now contains an option named <code>"ratios"</code> that provides the weights used to combine kernels in the reduced subspace given by <abbr><span class="acronym">PCA</span></abbr>.
</p>
</li></ul>

</td></tr>
<tr><td><code>paramY</code></td>
<td>
<p>A vector of values with output kernel parameters.
</p>
<p><b>Case 1:</b> <code>kernelY</code> is a list of <code class="reqn">q</code> strings.
</p>
<p><code>paramY</code> is a vector of <code>q</code> values. For each one-parameter kernel, a real number is returned. It is either the original value (if correct), a corrected value or the default value (computed with a rule of thumb if <code>NA</code> was initially specified). For each parameter-free kernel, <code>NA</code> is returned.
</p>
<p><b>Case 2:</b> <code>kernelY</code> is a list of options with <code>method="PCA"</code>.
</p>
<p><code>paramY</code> is a vector of <code>PC</code> values. For this method, let us recall that all kernels belong to the same family which is specified by an option named <code>"fam"</code> within <code>kernelY</code>. For each dimension in the reduced subspace, the kernel parameter is computed (with a rule of thumb) from the corresponding principal component. If the kernel in <code>fam</code> is parameter-free, <code>paramY</code> is a vector where <code>NA</code> is repeated <code>PC</code> times.
</p>
<p><b>Case 3:</b> <code>kernelY</code> is a list of options with <code>method="DTW"</code>.
</p>
<p><code>paramY</code> remains equal to <code>NA</code>.
</p>
<p><b>Case 4:</b> <code>kernelY</code> is a list of options with <code>method="GAK"</code>.
</p>
<p><code>paramY</code> is a vector of <code class="reqn">2</code> values. For each parameter, the returned value is either the original value (if correct), a corrected value or the default value (computed with a rule of thumb if <code>NA</code> was initially specified). 
</p>
</td></tr>
</table>
<p>More importantly, the list of class <code>"sensiHSIC"</code> contains all expected results (output samples, sensitivity measures and conditioning weights).
</p>
<table role = "presentation">
<tr><td><code>call</code></td>
<td>
<p>The matched call.</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>A <code class="reqn">n</code>-row matrix containing all output samples. The <code class="reqn">i</code>-th row in <code>y</code> is obtained from the <code class="reqn">i</code>-th row in <code>X</code> after computing the model response. If <code>target</code> is passed to <code>sensiHSIC</code>, output samples in <code>y</code> are obtained after applying consecutively <code>model</code> and the specified weight function.</p>
</td></tr>
<tr><td><code>HSICXY</code></td>
<td>
<p>The estimated <abbr><span class="acronym">HSIC</span></abbr> indices.</p>
</td></tr>
<tr><td><code>S</code></td>
<td>
<p>The estimated <abbr><span class="acronym">R2-HSIC</span></abbr> indices (also called normalized <abbr><span class="acronym">HSIC</span></abbr> indices).</p>
</td></tr>
<tr><td><code>weights</code></td>
<td>
<p>Only if <code>cond</code> is passed to <code>sensiHSIC</code>. <br />
A vector of <code class="reqn">n</code> values containing all conditioning weights. In the <abbr><span class="acronym">CSA</span></abbr> context, the conditioning factor is defined by <code class="reqn">w(Y)/E[w(Y)]</code>. See Marrel and Chabridon (2021) for further explanations.</p>
</td></tr>
</table>
<p>Depending on what is specified in <code>anova</code>, the list of class <code>"sensiHSIC"</code> may also contain the following objects:
</p>
<table role = "presentation">
<tr><td><code>FO</code></td>
<td>
<p>The estimated first-order <abbr><span class="acronym">HSIC-ANOVA</span></abbr> indices.</p>
</td></tr>
<tr><td><code>TO</code></td>
<td>
<p>The estimated total-order <abbr><span class="acronym">HSIC-ANOVA</span></abbr> indices.</p>
</td></tr>
<tr><td><code>TO.num</code></td>
<td>
<p>The estimated numerators of total-order <abbr><span class="acronym">HSIC-ANOVA</span></abbr> indices.</p>
</td></tr>
<tr><td><code>denom</code></td>
<td>
<p>The estimated common denominator of <abbr><span class="acronym">HSIC-ANOVA</span></abbr> indices.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Sebastien Da Veiga, Amandine Marrel, Anouar Meynaoui, Reda El Amri and Gabriel Sarazin.
</p>


<h3>References</h3>

<p>Borgonovo, E. and Plischke, E. (2016), <em>Sensitivity analysis: a review of recent advances</em>, European Journal of Operational Research, 248(3), 869-887.
</p>
<p>Cuturi, M., Vert, J. P., Birkenes, O. and Matsui, T. (2007), <em>A kernel for time series based on global alignments</em>, 2007 IEEE International Conference on Acoustics, Speech and Signal Processing-ICASSP'07 (Vol. 2, pp. II-413), IEEE.
</p>
<p>Cuturi, M. (2011), <em>Fast global alignment kernels</em>, Proceedings of the 28th International Conference on Machine Learning (ICML-11) (pp. 929-936).
</p>
<p>Da Veiga, S. (2015), <em>Global sensitivity analysis with dependence measures</em>, Journal of Statistical Computation and Simulation, 85(7), 1283-1305.
</p>
<p>Da Veiga, S. (2021). <em>Kernel-based <abbr><span class="acronym">ANOVA</span></abbr> decomposition and Shapley effects: application to global sensitivity analysis</em>, arXiv preprint arXiv:2101.05487.
</p>
<p>El Amri, M. R. and Marrel, A. (2021), <em>More powerful <abbr><span class="acronym">HSIC</span></abbr>-based independence tests, extension to space-filling designs and functional data</em>.
<a href="https:/cea.hal.science/cea-03406956/">https:/cea.hal.science/cea-03406956/</a>
</p>
<p>Fukumizu, K., Bach, F. R. and Jordan, M. I. (2004), <em>Dimensionality reduction for supervised learning with reproducing kernel Hilbert spaces</em>, Journal of Machine Learning Research, 5(Jan), 73-99.
</p>
<p>Ginsbourger, D., Roustant, O., Schuhmacher, D., Durrande, N. and Lenz, N. (2016), <em>On <abbr><span class="acronym">ANOVA</span></abbr> decompositions of kernels and Gaussian random field paths</em>, Monte Carlo and Quasi-Monte Carlo Methods (pp. 315-330), Springer, Cham.
</p>
<p>Gretton, A., Bousquet, O., Smola, A., and Scholkopf, B. (2005), <em>Measuring statistical dependence with Hilbert-Schmidt norms</em>, International Conference on Algorithmic Learning Theory (pp. 63-77), Springer, Berlin, Heidelberg.
</p>
<p>Gretton, A., Borgwardt, K., Rasch, M., Scholkopf, B. and Smola, A. (2006), <em>A kernel method for the two-sample-problem</em>, Advances in Neural Information Processing Systems, 19.
</p>
<p>Le Maitre, O. and Knio, O. M. (2010), <em>Spectral methods for uncertainty quantification with applications to computational fluid dynamics</em>, Springer Science &amp; Business Media.
</p>
<p>Marrel, A. and Chabridon, V. (2021), <em>Statistical developments for target and conditional sensitivity analysis: application on safety studies for nuclear reactor</em>, Reliability Engineering &amp; System Safety, 214, 107711.
</p>
<p>Sakoe, H. and Chiba, S. (1978), <em>Dynamic programming algorithm optimization for spoken word recognition</em>, IEEE International Conference on Acoustics, Speech and Signal, 26(1), 43-49.
</p>
<p>Spagnol, A., Riche, R. L. and Veiga, S. D. (2019), <em>Global sensitivity analysis for optimization with variable selection</em>, SIAM/ASA Journal on Uncertainty Quantification, 7(2), 417-443.
</p>
<p>Sriperumbudur, B., Fukumizu, K. and Lanckriet, G. (2010), <em>On the relation between universality, characteristic kernels and <abbr><span class="acronym">RKHS</span></abbr> embedding of measures</em>, Proceedings of the 13th International Conference on Artificial Intelligence and Statistics (pp. 773-780). JMLR Workshop and Conference Proceedings.
</p>
<p>Szekely, G. J., Rizzo, M. L. and Bakirov, N. K. (2007), <em>Measuring and testing dependence by correlation of distances</em>, The Anals of Statistics, 35(6), 2769-2794.
</p>
<p>Wahba, G., Wang, Y., Gu, C., Klein, R. and Klein, B. (1995), <em>Smoothing spline <abbr><span class="acronym">ANOVA</span></abbr> for exponential families, with application to the Wisconsin Epidemiological Study of Diabetic Retinopathy: the 1994 Neyman Memorial Lecture</em>, The Annals of Statistics, 23(6), 1865-1895.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+testHSIC">testHSIC</a>, <a href="#topic+weightTSA">weightTSA</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'> 

############################
### HSIC indices for GSA ###
############################

# Test case 1: the Friedman function
# --&gt; 5 input variables

### GSA with a given model ###

n &lt;- 800
p &lt;- 5
X &lt;- matrix(runif(n*p), n, p)

kernelX &lt;- c("rbf", "rbf", "laplace", "laplace", "sobolev1")
paramX &lt;- c(0.2, 0.3, 0.4, NA, NA)

# kernel for X1: Gaussian kernel with given parameter 0.2
# kernel for X2: Gaussian kernel with given parameter 0.3
# kernel for X3: exponential kernel with given parameter 0.4
# kernel for X4: exponential kernel with automatic computation of the parameter
# kernel for X5: Sobolev kernel (r=1) with no parameter

kernelY &lt;- "raquad"
paramY &lt;- NA 

sensi &lt;- sensiHSIC(model=friedman.fun, X,
                   kernelX=kernelX, paramX=paramX, 
                   kernelY=kernelY, paramY=paramY)

print(sensi)
plot(sensi)
title("GSA for the Friedman function")

### GSA with given data ###

Y &lt;- friedman.fun(X)
sensi &lt;- sensiHSIC(model=NULL, X,
                   kernelX=kernelX, paramX=paramX, 
                   kernelY=kernelY, paramY=paramY)
tell(sensi, y=Y)

print(sensi)

### GSA from a prior object of class "sensiHSIC" ###

new.sensi &lt;- sensiHSIC(model=friedman.fun, X,
                       kernelX=kernelX, paramX=paramX, 
                       kernelY=kernelY, paramY=paramY,
                       estimator.type="U-stat", 
                       sensi=sensi,
                       save.GM=list(KX=FALSE, KY=FALSE))

print(new.sensi)

# U-statistics are computed without rebuilding all Gram matrices.
# Those Gram matrices are not saved a second time.

##################################
### HSIC-ANOVA indices for GSA ###
##################################

# Test case 2: the Matyas function with Gaussian input variables
# --&gt; 3 input variables (including 1 dummy variable)

n &lt;- 10^3
p &lt;- 2

X &lt;- matrix(rnorm(n*p), n, p)

# The Sobolev kernel (with r=1) is used to achieve the HSIC-ANOVA decomposition.
# Both first-order and total-order HSIC-ANOVA indices are expected.

### AUTOMATIC RESCALING ###

kernelX &lt;- "sobolev1"
anova &lt;- list(obj="both", is.uniform=FALSE)

sensi.A &lt;- sensiHSIC(model=matyas.fun, X, kernelX=kernelX, anova=anova)

print(sensi.A)
plot(sensi.A)
title("GSA for the Matyas function")

### PROBLEM REFORMULATION ###

U &lt;- matrix(runif(n*p), n, p)
new.matyas.fun &lt;- function(U){ matyas.fun(qnorm(U)) }

kernelX &lt;- "sobolev1"
anova &lt;- list(obj="both", is.uniform=TRUE)

sensi.B &lt;- sensiHSIC(model=new.matyas.fun, U, kernelX=kernelX, anova=anova)

print(sensi.B)

####################################
### T-HSIC indices for target SA ###
####################################

# Test case 3: the Sobol function
# --&gt; 8 input variables

n &lt;- 10^3
p &lt;- 8

X &lt;- matrix(runif(n*p), n, p)

kernelY &lt;- "categ"
target &lt;- list(c=0.4, type="indicTh")

sensi &lt;- sensiHSIC(model=sobol.fun, X, kernelY=kernelY, target=target)

print(sensi)
plot(sensi)
title("TSA for the Sobol function")

#########################################
### C-HSIC indices for conditional SA ###
#########################################

# Test case 3: the Sobol function
# --&gt; 8 input variables

n &lt;- 10^3
p &lt;- 8

X &lt;- matrix(runif(n*p), n, p)

cond &lt;- list(c=0.2, type="exp1side", upper=FALSE)

sensi &lt;- sensiHSIC(model=sobol.fun, X, cond=cond)

print(sensi)
plot(sensi)
title("CSA for the Sobol function")

##########################################
### How to deal with discrete outputs? ###
##########################################

# Test case 4: classification of the Ishigami output
# --&gt; 3 input variables
# --&gt; 3 categories

classif &lt;- function(X){
  
  Ytemp &lt;- ishigami.fun(X) 
  Y &lt;- rep(NA, n)
  Y[Ytemp&lt;0] &lt;- 0
  Y[Ytemp&gt;=0 &amp; Ytemp&lt;10] &lt;- 1                
  Y[Ytemp&gt;=10] &lt;- 2  
  
  return(Y)
  
}

###

n &lt;- 10^3
p &lt;- 3

X &lt;- matrix(runif(n*p, -pi, pi), n, p)

kernelY &lt;- "categ"
paramY &lt;- 0

sensi &lt;- sensiHSIC(model=classif, X, kernelY=kernelY, paramY=paramY)
print(sensi)
plot(sensi)
title("GSA for the classified Ishigami function")

############################################
### How to deal with functional outputs? ###
############################################

# Test case 5: the arctangent temporal function
# --&gt; 3 input variables (including 1 dummy variable)

n &lt;- 500
p &lt;- 3

X &lt;- matrix(runif(n*p,-7,7), n, p)

### with a preliminary dimension reduction by PCA ###

kernelY &lt;- list(method="PCA", 
                data.centering=TRUE, data.scaling=TRUE,
                fam="rbf", expl.var=0.95, combi="sum", position="extern")

sensi &lt;- sensiHSIC(model=atantemp.fun, X, kernelY=kernelY)

print(sensi)
plot(sensi)
title("PCA-based GSA for the arctangent temporal function")

### with a kernel based on dynamic time warping ###

kernelY &lt;- list(method="DTW", fam="rbf")

sensi &lt;- sensiHSIC(model=atantemp.fun, X, kernelY=kernelY)

print(sensi)
plot(sensi)
title("DTW-based GSA for the arctangent temporal function")



### with the global alignment kernel ###

kernelY &lt;- list(method="GAK")

sensi &lt;- sensiHSIC(model=atantemp.fun, X, kernelY=kernelY)

print(sensi)
plot(sensi)
title("GAK-based GSA for the arctangent temporal function")

  
</code></pre>

<hr>
<h2 id='shapleyBlockEstimation'>Computation of the Shapley effects in the Gaussian linear framework
with an unknown block-diagonal covariance matrix</h2><span id='topic+shapleyBlockEstimation'></span><span id='topic+shapleyBlockEstimationS'></span><span id='topic+shapleyBlockEstimationX'></span>

<h3>Description</h3>

<p><code>shapleyBlockEstimation</code> estimates the Shapley effects of a Gaussian linear model
when the parameters are unknown and when the number of inputs is large,
choosing the most likely block-diagonal structure of the covariance matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>shapleyBlockEstimationS(Beta, S, kappa=0,  M=20, tol=10^(-6))
shapleyBlockEstimationX(X, Y, delta=NULL, M=20, tol=10^(-6))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="shapleyBlockEstimation_+3A_beta">Beta</code></td>
<td>
<p>A vector containing the (estimated) coefficients of the linear model.</p>
</td></tr>
<tr><td><code id="shapleyBlockEstimation_+3A_s">S</code></td>
<td>
<p>Empirical covariance matrix of the inputs. Has to be positive semi-definite matrix with same size that Beta.</p>
</td></tr>
<tr><td><code id="shapleyBlockEstimation_+3A_x">X</code></td>
<td>
<p>Matrix containing an i.i.d. sample of the inputs.</p>
</td></tr>
<tr><td><code id="shapleyBlockEstimation_+3A_y">Y</code></td>
<td>
<p>Vector containing the corresponding i.i.d. sample of the (noisy) output.</p>
</td></tr>
<tr><td><code id="shapleyBlockEstimation_+3A_kappa">kappa</code></td>
<td>
<p>The positive penalization coefficient that promotes block-diagonal matrices. It is advised to choose <code>kappa=0</code> to get the largest block structure such that the maximal block size is <code>M</code>.</p>
</td></tr>
<tr><td><code id="shapleyBlockEstimation_+3A_delta">delta</code></td>
<td>
<p>Positive number that fixes the positive penalization coefficient
<code>kappa</code> to <code class="reqn">1/(p n^{delta})</code>. It is advised to choose <code>delta</code> to 2/3 for a positive penalisation or <code>delta=NULL</code> to get the largest block structure such that the maximal block size is <code>M</code>.</p>
</td></tr>
<tr><td><code id="shapleyBlockEstimation_+3A_m">M</code></td>
<td>
<p>Maximal size of the estimate of the block-diagonal structure. The computation time grows exponentially with <code>M</code>.</p>
</td></tr>
<tr><td><code id="shapleyBlockEstimation_+3A_tol">tol</code></td>
<td>
<p>A relative tolerance to detect zero singular values of Sigma.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>kappa = 0</code> or if <code>delta = NULL</code>, there is no penalization.
</p>
<p>It is advised to choose <code>M</code> smaller or equal than 20. For <code>M</code> larger or equal than 25, the computation is very long.
</p>


<h3>Value</h3>

<p><code>shapleyBlockEstimationS</code> and <code>shapleyblockEstimationX</code> return a list containing the following compopents:
</p>
<table role = "presentation">
<tr><td><code>label</code></td>
<td>
<p>a vector containing the label of the group of each input variable.</p>
</td></tr>
<tr><td><code>S_B</code></td>
<td>
<p>the block-diagonal estimated covariance matrix of the inputs.</p>
</td></tr>
<tr><td><code>Shapley</code></td>
<td>
<p>a vector containing all the estimated Shapley effects.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Baptiste Broto, CEA LIST
</p>


<h3>References</h3>

<p>B. Broto, F. Bachoc, L. Clouvel and J-M Martinez, 2022,<em>Block-diagonal
covariance estimation and application to the Shapley effects in sensitivity analysis</em>,
SIAM/ASA Journal on Uncertainty Quantification, 10, 379&ndash;403.
</p>
<p>B. Broto, F. Bachoc, M. Depecker, and J-M. Martinez, 2019, <em>Sensitivity indices
for independent groups of variables</em>, Mathematics and Computers in Simulation, 163, 19&ndash;31.
</p>
<p>B. Iooss and C. Prieur, 2019, <em>Shapley effects for sensitivity analysis with 
correlated inputs: comparisons with Sobol' indices, numerical estimation and 
applications</em>, International Journal of Uncertainty Quantification, 9, 493&ndash;514.
</p>
<p>A.B. Owen and C. Prieur, 2016, <em>On Shapley value for measuring importance
of dependent inputs</em>, SIAM/ASA Journal of Uncertainty Quantification, 5, 986&ndash;1002.
</p>


<h3>See Also</h3>

<p><a href="#topic+shapleyLinearGaussian">shapleyLinearGaussian</a>, <a href="#topic+shapleyPermEx">shapleyPermEx</a>, <a href="#topic+shapleyPermRand">shapleyPermRand</a>, <a href="#topic+shapleySubsetMc">shapleySubsetMc</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# packages for the plots of the matrices
library(gplots)
library(graphics)


# the following function improves the plots of the matrices
sig=function(x,alpha=0.4)
{
  return(1/(1+exp(-x/alpha)))
}


# 1) we generate the parameters by groups in order

K=4 # number or groups

pk=rep(0,K)
for(k in 1:K)
{
  pk[k]=round(6+4*runif(1))
}
p=sum(pk)
Sigma_ord=matrix(0,nrow=p, ncol=p)
ind_min=0
L=5
for(k in 1:K)
{
  p_k=pk[k]
  ind=ind_min+(1:p_k)
  ind_min=ind_min+p_k
  
  A=2*matrix(runif(p_k*L),nrow=L,ncol=p_k)-1
  Sigma_ord[ind,ind]=t(A)%*%A + 0.2*diag(rep(1,p_k))
}


image((0:p)+0.5,(0:p)+0.5,z=sig(Sigma_ord),col=cm.colors(100), zlim=c(0,1),
      ylim=c(p+0.5,0.5), main=expression(Sigma["order"]), 
      cex.main=3,ylab = "", xlab = "",axes=FALSE)
box()


Beta_ord=3*runif(p)+1
eta_ord=shapleyLinearGaussian(Beta=Beta_ord, Sigma=Sigma_ord)
barplot(eta_ord,main=expression(eta["order"]),cex.axis = 2,cex.main=3)


# 2) We sample the input variables to get an input vector more general

samp=sample(1:p)
Sigma=Sigma_ord[samp,samp]

image((0:p)+0.5,(0:p)+0.5,z=sig(Sigma),col=cm.colors(100), zlim=c(0,1),
      ylim=c(p+0.5,0.5), main=expression(Sigma), 
      cex.main=3,ylab = "",xlab = "",axes=FALSE)
box()


Beta=Beta_ord[samp]
eta=shapleyLinearGaussian(Beta=Beta, Sigma=Sigma)
barplot(eta,main=expression(eta),cex.axis = 2,cex.main=3)




# 3) We generate the observations with these parameters

n=5*p #sample size


C=chol(Sigma)
X0=matrix(rnorm(p*n),ncol=p)
X=X0%*%C

S=var(X) #empirical covariance matrix
image((0:p)+0.5,(0:p)+0.5,z=sig(S),col=cm.colors(100), zlim=c(0,1),
      ylim=c(p+0.5,0.5), main=expression(S), 
      cex.main=3,ylab = "", xlab = "",axes=FALSE)
box()

beta0=rnorm(1)
Y=X%*%as.matrix(Beta)+beta0+0.2*rnorm(p)



# 4) We estimate the block-diagonal covariance matrix 
# and the Shapley effects using the observations
# We assume that we know that the groups are smaller than 15

Estim=shapleyBlockEstimationX(X,Y,delta=3/4, M=15, tol=10^(-6))

eta_hat=Estim$Shapley
S_B=Estim$S_B

image((0:p)+0.5,(0:p)+0.5,z=sig(S_B),col=cm.colors(100), zlim=c(0,1),
      ylim=c(p+0.5,0.5), main=expression(S[hat(B)]), 
      cex.main=3,ylab = "",xlab = "",axes=FALSE)
box()

barplot(eta_hat,main=expression(hat(eta)),cex.axis = 2,cex.main=3)


sum(abs(eta_hat-eta))
</code></pre>

<hr>
<h2 id='shapleyLinearGaussian'>Computation of the Shapley effects in the linear Gaussian framework</h2><span id='topic+shapleyLinearGaussian'></span>

<h3>Description</h3>

<p><code>shapleyLinearGaussian</code> implements the computation of
the Shapley effects in the linear Gaussian framework, using the linear model
(without the value at zero) and the covariance matrix of the inputs.
It uses the block-diagonal covariance trick of Broto et al. (2019) which allows 
to go through high-dimensional cases (nb of inputs &gt; 25). 
It gives a warning in case of dim(block) &gt; 25.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>shapleyLinearGaussian(Beta, Sigma, tol=10^(-6))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="shapleyLinearGaussian_+3A_beta">Beta</code></td>
<td>
<p>a vector containing the coefficients of the linear model (without the value at zero).</p>
</td></tr>
<tr><td><code id="shapleyLinearGaussian_+3A_sigma">Sigma</code></td>
<td>
<p>covariance matrix of the inputs. Has to be positive semi-definite matrix with same size that Beta.</p>
</td></tr>
<tr><td><code id="shapleyLinearGaussian_+3A_tol">tol</code></td>
<td>
<p>a relative tolerance to detect zero singular values of Sigma.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>shapleyLinearGaussian</code> returns a numeric vector containing all the Shapley effects.
</p>


<h3>Author(s)</h3>

<p>Baptiste Broto
</p>


<h3>References</h3>

<p>B. Broto, F. Bachoc, M. Depecker, and J-M. Martinez, 2019, <em>Sensitivity indices
for independent groups of variables</em>, Mathematics and Computers in Simulation, 163, 19&ndash;31.
</p>
<p>B. Broto, F. Bachoc, L. Clouvel and J-M Martinez, 2022,<em>Block-diagonal
covariance estimation and application to the Shapley effects in sensitivity analysis</em>,
SIAM/ASA Journal on Uncertainty Quantification, 10, 379&ndash;403.
</p>
<p>B. Iooss and C. Prieur, 2019, <em>Shapley effects for sensitivity analysis with 
correlated inputs: comparisons with Sobol' indices, numerical estimation and 
applications</em>, International Journal for Uncertainty Quantification, 9, 493&ndash;514.
</p>
<p>A.B. Owen and C. Prieur, 2016, <em>On Shapley value for measuring importance
of dependent inputs</em>, SIAM/ASA Journal of Uncertainty Quantification, 5, 986&ndash;1002.
</p>


<h3>See Also</h3>

<p><a href="#topic+shapleyBlockEstimation">shapleyBlockEstimation</a>, <a href="#topic+shapleyPermEx">shapleyPermEx</a>, <a href="#topic+shapleyPermRand">shapleyPermRand</a>, 
<a href="#topic+shapleySubsetMc">shapleySubsetMc</a>, <a href="#topic+shapleysobol_knn">shapleysobol_knn</a>, <a href="#topic+johnsonshap">johnsonshap</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(MASS)
library(igraph)

# First example:

p=5 #dimension
A=matrix(rnorm(p^2),nrow=p,ncol=p)
Sigma=t(A)%*%A
Beta=runif(p)
Shapley=shapleyLinearGaussian(Beta,Sigma)
plot(Shapley)


# Second Example, block-diagonal:

K=5 #number of groups
m=5 # number of variables in each group
p=K*m
Sigma=matrix(0,ncol=p,nrow=p)

for(k in 1:K)
{
  A=matrix(rnorm(m^2),nrow=m,ncol=m)
  Sigma[(m*(k-1)+1):(m*k),(m*(k-1)+1):(m*k)]=t(A)%*%A
}
# we mix the variables:
samp=sample(1:p,p)
Sigma=Sigma[samp,samp]

Beta=runif(p)
Shapley=shapleyLinearGaussian(Beta,Sigma)
plot(Shapley)

</code></pre>

<hr>
<h2 id='shapleyPermEx'>Estimation of Shapley effects by examining all permutations of inputs 
(Agorithm of Song et al, 2016), in cases of independent or dependent inputs</h2><span id='topic+shapleyPermEx'></span><span id='topic+tell.shapleyPermEx'></span><span id='topic+print.shapleyPermEx'></span><span id='topic+plot.shapleyPermEx'></span><span id='topic+ggplot.shapleyPermEx'></span>

<h3>Description</h3>

<p><code>shapleyPermEx</code> implements the Monte Carlo estimation of
the Shapley effects (Owen, 2014) and their standard errors by examining all
permutations of inputs (Song et al., 2016; Iooss and Prieur, 2019). It also 
estimates full first order and independent total Sobol' indices 
(Mara et al., 2015). The function also allows the estimations of all these 
sensitivity indices in case of dependent inputs. The total cost of this 
algorithm is <code class="reqn">Nv + d! \times (d-1) \times No \times Ni</code> 
model evaluations. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>shapleyPermEx(model = NULL, Xall, Xset, d, Nv, No, Ni = 3, colnames = NULL, ...)
## S3 method for class 'shapleyPermEx'
tell(x, y = NULL, return.var = NULL, ...)
## S3 method for class 'shapleyPermEx'
print(x, ...)
## S3 method for class 'shapleyPermEx'
plot(x, ylim = c(0, 1), ...)
## S3 method for class 'shapleyPermEx'
ggplot(data, mapping = aes(), ylim = c(0, 1), title = NULL,
                 ..., environment = parent.frame())
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="shapleyPermEx_+3A_model">model</code></td>
<td>
<p>a function, or a model with a <code>predict</code> method,
defining the model to analyze.</p>
</td></tr>
<tr><td><code id="shapleyPermEx_+3A_xall">Xall</code></td>
<td>
<p>Xall(n) is a function to generate a n-sample of a d-dimensional 
input vector (following the required joint distribution).</p>
</td></tr>
<tr><td><code id="shapleyPermEx_+3A_xset">Xset</code></td>
<td>
<p>Xset(n, Sj, Sjc, xjc) is a function to generate a n-sample of 
a d-dimensional input vector corresponding to the indices in Sj conditional 
on the input values xjc with the index set Sjc (following the required joint 
distribution).</p>
</td></tr>
<tr><td><code id="shapleyPermEx_+3A_d">d</code></td>
<td>
<p>number of inputs.</p>
</td></tr>
<tr><td><code id="shapleyPermEx_+3A_nv">Nv</code></td>
<td>
<p>Monte Carlo sample size to estimate the output variance.</p>
</td></tr>
<tr><td><code id="shapleyPermEx_+3A_no">No</code></td>
<td>
<p>Outer Monte Carlo sample size to estimate the expectation 
of the conditional variance of the model output.</p>
</td></tr>
<tr><td><code id="shapleyPermEx_+3A_ni">Ni</code></td>
<td>
<p>Inner Monte Carlo sample size to estimate the 
conditional variance of the model output.</p>
</td></tr>
<tr><td><code id="shapleyPermEx_+3A_colnames">colnames</code></td>
<td>
<p>Optional: A vector containing the names of the inputs.</p>
</td></tr>
<tr><td><code id="shapleyPermEx_+3A_x">x</code></td>
<td>
<p>a list of class <code>"shapleyPermEx"</code> storing the state of the
sensitivity study (parameters, data, estimates).</p>
</td></tr>
<tr><td><code id="shapleyPermEx_+3A_data">data</code></td>
<td>
<p>a list of class <code>"shapleyPermEx"</code> storing the state of the
sensitivity study (parameters, data, estimates).</p>
</td></tr>
<tr><td><code id="shapleyPermEx_+3A_y">y</code></td>
<td>
<p>a vector of model responses.</p>
</td></tr>
<tr><td><code id="shapleyPermEx_+3A_return.var">return.var</code></td>
<td>
<p>a vector of character strings giving further
internal variables names to store in the output object <code>x</code>.</p>
</td></tr>
<tr><td><code id="shapleyPermEx_+3A_ylim">ylim</code></td>
<td>
<p>y-coordinate plotting limits.</p>
</td></tr>
<tr><td><code id="shapleyPermEx_+3A_title">title</code></td>
<td>
<p>a title of the plot with ggplot() function.</p>
</td></tr>
<tr><td><code id="shapleyPermEx_+3A_mapping">mapping</code></td>
<td>
<p>Default list of aesthetic mappings to use for plot. If not specified, 
must be supplied in each layer added to the plot.</p>
</td></tr>
<tr><td><code id="shapleyPermEx_+3A_environment">environment</code></td>
<td>
<p>[Deprecated] Used prior to tidy evaluation.</p>
</td></tr>
<tr><td><code id="shapleyPermEx_+3A_...">...</code></td>
<td>
<p>any other arguments for <code>model</code> which are passed
unchanged each time it is called.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function requires R package &quot;gtools&quot;.
</p>
<p>The default values Ni = 3 is the optimal one obtained by the theoretical
analysis of Song et al., 2016.
</p>
<p>The computations of the standard errors (and then the confidence intervals)
come from Iooss and prieur (2019). Based on the outer Monte carlo loop 
(calculation of expectation of conditional variance), the variance of the 
Monte carlo estimate is divided by No. The standard error is then averaged 
over the exact permutation loop. The confidence intervals at 95% correspond 
to +- 1.96 standard deviations.
</p>


<h3>Value</h3>

<p><code>shapleyPermEx</code> returns a list of class <code>"shapleyPermEx"</code>, containing
all the input arguments detailed before, plus the following components:
</p>
<table role = "presentation">
<tr><td><code>call</code></td>
<td>
<p>the matched call.</p>
</td></tr>
<tr><td><code>X</code></td>
<td>
<p>a <code>data.frame</code> containing the design of experiments.</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>the response used.</p>
</td></tr>
<tr><td><code>E</code></td>
<td>
<p>the estimation of the ouput mean.</p>
</td></tr>
<tr><td><code>V</code></td>
<td>
<p>the estimation of the ouput variance.</p>
</td></tr>
<tr><td><code>Shapley</code></td>
<td>
<p>the estimations of the Shapley effects.</p>
</td></tr>
<tr><td><code>SobolS</code></td>
<td>
<p>the estimations of the full first-order Sobol' indices.</p>
</td></tr>
<tr><td><code>SobolT</code></td>
<td>
<p>the estimations of the independent total sensitivity Sobol' indices.</p>
</td></tr>
</table>
<p>Users can ask more ouput variables with the argument <code>return.var</code>
(for example, the list of permutations <code>perms</code>).
</p>


<h3>Author(s)</h3>

<p>Bertrand Iooss, Eunhye Song, Barry L. Nelson, Jeremy Staum 
</p>


<h3>References</h3>

<p>B. Iooss and C. Prieur, 2019, <em>Shapley effects for sensitivity analysis with 
correlated inputs: comparisons with Sobol' indices, numerical estimation and 
applications</em>, International Journal for Uncertainty Quantification, 9, 493&ndash;514.
</p>
<p>T. Mara, S. Tarantola, P. Annoni, 2015, <em>Non-parametric methods for global 
sensitivity analysis of model output with dependent inputs</em>, Environmental
Modeling &amp; Software 72, 173&ndash;183.
</p>
<p>A.B. Owen, 2014, <em>Sobol' indices and Shapley value</em>, SIAM/ASA 
Journal of Uncertainty Quantification, 2, 245&ndash;251.
</p>
<p>A.B. Owen and C. Prieur, 2016, <em>On Shapley value for measuring importance
of dependent inputs</em>, SIAM/ASA Journal of Uncertainty Quantification, 5, 986&ndash;1002.
</p>
<p>E. Song, B.L. Nelson, and J. Staum, 2016, <em>Shapley effects for 
global sensitivity analysis: Theory and computation</em>, SIAM/ASA Journal 
of Uncertainty Quantification, 4, 1060&ndash;1083.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+shapleyPermRand">shapleyPermRand</a>, <a href="#topic+shapleyLinearGaussian">shapleyLinearGaussian</a>,  <a href="#topic+shapleySubsetMc">shapleySubsetMc</a>, <a href="#topic+shapleysobol_knn">shapleysobol_knn</a></code>, <code><a href="#topic+lmg">lmg</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>


##################################
# Test case : the Ishigami function (3 uniform independent inputs)
# See Iooss and Prieur (2019)

library(gtools)

d &lt;- 3
Xall &lt;- function(n) matrix(runif(d*n,-pi,pi),nc=d)
Xset &lt;- function(n, Sj, Sjc, xjc) matrix(runif(n*length(Sj),-pi,pi),nc=length(Sj))

x &lt;- shapleyPermEx(model = ishigami.fun, Xall=Xall, Xset=Xset, d=d, Nv=1e4, No = 1e3, Ni = 3)
print(x)
plot(x)

library(ggplot2)
ggplot(x)

##################################
# Test case : Linear model (3 Gaussian inputs including 2 dependent)
# See Iooss and Prieur (2019)

library(ggplot2)
library(gtools)
library(mvtnorm) # Multivariate Gaussian variables
library(condMVNorm) # Conditional multivariate Gaussian variables

modlin &lt;- function(X) apply(X,1,sum)

d &lt;- 3
mu &lt;- rep(0,d)
sig &lt;- c(1,1,2)
ro &lt;- 0.9
Cormat &lt;- matrix(c(1,0,0,0,1,ro,0,ro,1),d,d)
Covmat &lt;- ( sig %*% t(sig) ) * Cormat

Xall &lt;- function(n) mvtnorm::rmvnorm(n,mu,Covmat)

Xset &lt;- function(n, Sj, Sjc, xjc){
  if (is.null(Sjc)){
    if (length(Sj) == 1){ rnorm(n,mu[Sj],sqrt(Covmat[Sj,Sj]))
    } else{ mvtnorm::rmvnorm(n,mu[Sj],Covmat[Sj,Sj])}
  } else{ condMVNorm::rcmvnorm(n, mu, Covmat, dependent.ind=Sj, given.ind=Sjc, 
                                X.given=xjc)}}

x &lt;- shapleyPermEx(model = modlin, Xall=Xall, Xset=Xset, d=d, Nv=1e4, 
                    No = 1e3, Ni = 3)
print(x)
ggplot(x)


</code></pre>

<hr>
<h2 id='shapleyPermRand'>Estimation of Shapley effects by random permutations of inputs 
(Agorithm of Song et al, 2016), in cases of independent or dependent inputs</h2><span id='topic+shapleyPermRand'></span><span id='topic+tell.shapleyPermRand'></span><span id='topic+print.shapleyPermRand'></span><span id='topic+plot.shapleyPermRand'></span><span id='topic+ggplot.shapleyPermRand'></span>

<h3>Description</h3>

<p><code>shapleyPermRand</code> implements the Monte Carlo estimation of
the Shapley effects (Owen, 2014) and their standard errors by randomly sampling 
permutations of inputs (Song et al., 2016). It also estimates full first order 
and independent total Sobol' indices (Mara et al., 2015), and their standard errors. 
The function also allows the estimations of all these sensitivity indices in case 
of dependent inputs.
The total cost of   this algorithm is <code class="reqn">Nv + m \times (d-1) \times No 
  \times Ni</code> model evaluations. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>shapleyPermRand(model = NULL, Xall, Xset, d, Nv, m, No = 1, Ni = 3, 
                colnames = NULL, ...)
## S3 method for class 'shapleyPermRand'
tell(x, y = NULL, return.var = NULL, ...)
## S3 method for class 'shapleyPermRand'
print(x, ...)
## S3 method for class 'shapleyPermRand'
plot(x, ylim = c(0, 1), ...)
## S3 method for class 'shapleyPermRand'
ggplot(data, mapping = aes(), ylim = c(0, 1), 
                          title = NULL, ..., environment = parent.frame())
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="shapleyPermRand_+3A_model">model</code></td>
<td>
<p>a function, or a model with a <code>predict</code> method,
defining the model to analyze.</p>
</td></tr>
<tr><td><code id="shapleyPermRand_+3A_xall">Xall</code></td>
<td>
<p>Xall(n) is a function to generate a n-sample of a d-dimensional 
input vector (following the required joint distribution).</p>
</td></tr>
<tr><td><code id="shapleyPermRand_+3A_xset">Xset</code></td>
<td>
<p>Xset(n, Sj, Sjc, xjc) is a function to generate a n-sample of 
a d-dimensional input vector corresponding to the indices in Sj conditional 
on the input values xjc with the index set Sjc (following the required joint 
distribution).</p>
</td></tr>
<tr><td><code id="shapleyPermRand_+3A_d">d</code></td>
<td>
<p>number of inputs.</p>
</td></tr>
<tr><td><code id="shapleyPermRand_+3A_nv">Nv</code></td>
<td>
<p>Monte Carlo sample size to estimate the output variance.</p>
</td></tr>
<tr><td><code id="shapleyPermRand_+3A_m">m</code></td>
<td>
<p>Number of randomly sampled permutations.</p>
</td></tr>
<tr><td><code id="shapleyPermRand_+3A_no">No</code></td>
<td>
<p>Outer Monte Carlo sample size to estimate the expectation 
of the conditional variance of the model output.</p>
</td></tr>
<tr><td><code id="shapleyPermRand_+3A_ni">Ni</code></td>
<td>
<p>Inner Monte Carlo sample size to estimate the 
conditional variance of the model output.</p>
</td></tr>
<tr><td><code id="shapleyPermRand_+3A_colnames">colnames</code></td>
<td>
<p>Optional: A vector containing the names of the inputs.</p>
</td></tr>
<tr><td><code id="shapleyPermRand_+3A_x">x</code></td>
<td>
<p>a list of class <code>"shapleyPermRand"</code> storing the state of the
sensitivity study (parameters, data, estimates).</p>
</td></tr>
<tr><td><code id="shapleyPermRand_+3A_data">data</code></td>
<td>
<p>a list of class <code>"shapleyPermRand"</code> storing the state of the
sensitivity study (parameters, data, estimates).</p>
</td></tr>
<tr><td><code id="shapleyPermRand_+3A_y">y</code></td>
<td>
<p>a vector of model responses.</p>
</td></tr>
<tr><td><code id="shapleyPermRand_+3A_return.var">return.var</code></td>
<td>
<p>a vector of character strings giving further
internal variables names to store in the output object <code>x</code>.</p>
</td></tr>
<tr><td><code id="shapleyPermRand_+3A_ylim">ylim</code></td>
<td>
<p>y-coordinate plotting limits.</p>
</td></tr>
<tr><td><code id="shapleyPermRand_+3A_title">title</code></td>
<td>
<p>a title of the plot with ggplot() function.</p>
</td></tr>
<tr><td><code id="shapleyPermRand_+3A_mapping">mapping</code></td>
<td>
<p>Default list of aesthetic mappings to use for plot. If not specified, 
must be supplied in each layer added to the plot.</p>
</td></tr>
<tr><td><code id="shapleyPermRand_+3A_environment">environment</code></td>
<td>
<p>[Deprecated] Used prior to tidy evaluation.</p>
</td></tr>
<tr><td><code id="shapleyPermRand_+3A_...">...</code></td>
<td>
<p>any other arguments for <code>model</code> which are passed
unchanged each time it is called.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function requires R package &quot;gtools&quot;.
</p>
<p>The default values No = 1 and Ni = 3 are the optimal ones obtained by the
theoretical analysis of Song et al., 2016.
</p>
<p>The computations of the standard errors do not consider the samples to estimate 
expectation of conditional variances. They are only made regarding the random 
permutations and are based on the variance of the Monte carlo estimates divided 
by m. The confidence intervals at 95% correspond to +- 1.96 standard deviations.
</p>


<h3>Value</h3>

<p><code>shapleyPermRand</code> returns a list of class <code>"shapleyPermRand"</code>, containing
all the input arguments detailed before, plus the following components:
</p>
<table role = "presentation">
<tr><td><code>call</code></td>
<td>
<p>the matched call.</p>
</td></tr>
<tr><td><code>X</code></td>
<td>
<p>a <code>data.frame</code> containing the design of experiments.</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>the response used.</p>
</td></tr>
<tr><td><code>E</code></td>
<td>
<p>the estimation of the ouput mean.</p>
</td></tr>
<tr><td><code>V</code></td>
<td>
<p>the estimation of the ouput variance.</p>
</td></tr>
<tr><td><code>Shapley</code></td>
<td>
<p>the estimations of the Shapley effects.</p>
</td></tr>
<tr><td><code>SobolS</code></td>
<td>
<p>the estimations of the full first-order Sobol' indices.</p>
</td></tr>
<tr><td><code>SobolT</code></td>
<td>
<p>the estimations of the independent total sensitivity Sobol' indices.</p>
</td></tr>
</table>
<p>Users can ask more ouput variables with the argument <code>return.var</code>
(for example, the list of permutations <code>perms</code>).
</p>


<h3>Author(s)</h3>

<p>Bertrand Iooss, Eunhye Song, Barry L. Nelson, Jeremy Staum 
</p>


<h3>References</h3>

<p>B. Iooss and C. Prieur, 2019, <em>Shapley effects for sensitivity analysis with 
correlated inputs: comparisons with Sobol' indices, numerical estimation and 
applications</em>, International Journal of Uncertainty Quantification, 9, 493&ndash;514.
</p>
<p>T. Mara, S. Tarantola, P. Annoni, 2015, <em>Non-parametric methods for global 
sensitivity analysis of model output with dependent inputs</em>, Environmental
Modeling &amp; Software 72, 173&ndash;183.
</p>
<p>A.B. Owen, 2014, <em>Sobol' indices and Shapley value</em>, SIAM/ASA 
Journal of Uncertainty Quantification, 2, 245&ndash;251.
</p>
<p>A.B. Owen and C. Prieur, 2016, <em>On Shapley value for measuring importance
of dependent inputs</em>, SIAM/ASA Journal of Uncertainty Quantification, 5, 986&ndash;1002.
</p>
<p>E. Song, B.L. Nelson, and J. Staum, 2016, <em>Shapley effects for 
global sensitivity analysis: Theory and computation</em>, SIAM/ASA Journal 
of Uncertainty Quantification, 4, 1060&ndash;1083.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+shapleyPermEx">shapleyPermEx</a>, <a href="#topic+shapleyLinearGaussian">shapleyLinearGaussian</a>, <a href="#topic+shapleySubsetMc">shapleySubsetMc</a>, <a href="#topic+shapleysobol_knn">shapleysobol_knn</a>
</code></p>


<h3>Examples</h3>

<pre><code class='language-R'>


##################################
# Test case : the Ishigami function
# See Iooss and Prieur (2019)

library(gtools)

d &lt;- 3
Xall &lt;- function(n) matrix(runif(d*n,-pi,pi),nc=d)
Xset &lt;- function(n, Sj, Sjc, xjc) matrix(runif(n*length(Sj),-pi,pi),nc=length(Sj))

x &lt;- shapleyPermRand(model = ishigami.fun, Xall=Xall, Xset=Xset, d=d, Nv=1e4, 
                      m=1e4, No = 1, Ni = 3)
print(x)
plot(x)

library(ggplot2)
ggplot(x)

##################################
# Test case : Linear model (3 Gaussian inputs including 2 dependent)
# See Iooss and Prieur (2019)

library(ggplot2)
library(gtools)
library(mvtnorm) # Multivariate Gaussian variables
library(condMVNorm) # Conditional multivariate Gaussian variables

modlin &lt;- function(X) apply(X,1,sum)

d &lt;- 3
mu &lt;- rep(0,d)
sig &lt;- c(1,1,2)
ro &lt;- 0.9
Cormat &lt;- matrix(c(1,0,0,0,1,ro,0,ro,1),d,d)
Covmat &lt;- ( sig %*% t(sig) ) * Cormat

Xall &lt;- function(n) mvtnorm::rmvnorm(n,mu,Covmat)

Xset &lt;- function(n, Sj, Sjc, xjc){
  if (is.null(Sjc)){
    if (length(Sj) == 1){ rnorm(n,mu[Sj],sqrt(Covmat[Sj,Sj]))
    } else{ mvtnorm::rmvnorm(n,mu[Sj],Covmat[Sj,Sj])}
  } else{ condMVNorm::rcmvnorm(n, mu, Covmat, dependent.ind=Sj, given.ind=Sjc, 
                                X.given=xjc)}}

m &lt;- 1e3 # put m)1e4 for more precised results
x &lt;- shapleyPermRand(model = modlin, Xall=Xall, Xset=Xset, d=d, Nv=1e3, m = m, 
                      No = 1, Ni = 3)
print(x)
ggplot(x)



</code></pre>

<hr>
<h2 id='shapleysobol_knn'>Data given Shapley effects estimation via nearest-neighbors procedure</h2><span id='topic+shapleysobol_knn'></span><span id='topic+tell.shapleysobol_knn'></span><span id='topic+extract.shapleysobol_knn'></span><span id='topic+print.shapleysobol_knn'></span><span id='topic+plot.shapleysobol_knn'></span><span id='topic+ggplot.shapleysobol_knn'></span><span id='topic+print.sobol_knn'></span><span id='topic+plot.sobol_knn'></span>

<h3>Description</h3>

<p><code>shapleysobol_knn</code> implements the estimation of several sensitivity indices using 
only N model evaluations via ranking (following Gamboa et al. (2020) and Chatterjee (2019)) 
or nearest neighbour search (Broto et al. (2020) and Azadkia &amp; Chatterjee (2020)). 
Parallelized computations are possible to accelerate the estimation process.
It can be used with categorical inputs (which are transformed with one-hot encoding), 
dependent inputs and multiple outputs. Sensitivity indices of any group of inputs can be computed,
which means that in particular (full) first-order, (independent) total Sobol indices 
and Shapley effects are accessible. For large sample sizes, the nearest neightbour algorithm 
can be significantly accelerated   by using approximate nearest neighbour search. 
It is also possible to estimate Shapley effects   with the random permutation approach of 
Castro et al.(2009), where all the terms are obtained with ranking or nearest neighbours.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>shapleysobol_knn(model=NULL, X, method = "knn", n.knn = 2, n.limit = 2000, 
          U = NULL, n.perm = NULL, noise = F, rescale = F, nboot = NULL, 
          boot.level = 0.8, conf=0.95, parl=NULL, ...)
## S3 method for class 'shapleysobol_knn'
tell(x, y, ...)
## S3 method for class 'shapleysobol_knn'
extract(x, ...)
## S3 method for class 'shapleysobol_knn'
print(x, ...)
## S3 method for class 'shapleysobol_knn'
plot(x, ylim = c(0,1), ...)
## S3 method for class 'shapleysobol_knn'
ggplot(data, mapping = aes(), ylim = c(0, 1), ..., 
                environment = parent.frame())
## S3 method for class 'sobol_knn'
print(x, ...)
## S3 method for class 'sobol_knn'
plot(x, ylim = c(0,1), ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="shapleysobol_knn_+3A_model">model</code></td>
<td>
<p>a function defining the model to analyze, taking X as an argument.</p>
</td></tr>
<tr><td><code id="shapleysobol_knn_+3A_x">X</code></td>
<td>
<p>a matrix or data frame containing the observed inputs.</p>
</td></tr>
<tr><td><code id="shapleysobol_knn_+3A_method">method</code></td>
<td>
<p>the algorithm to be used for estimation, either &quot;rank&quot; or &quot;knn&quot;,
see details. Default is <code>method="knn"</code>.</p>
</td></tr>
<tr><td><code id="shapleysobol_knn_+3A_n.knn">n.knn</code></td>
<td>
<p>the number of nearest neighbours used for estimation.</p>
</td></tr>
<tr><td><code id="shapleysobol_knn_+3A_n.limit">n.limit</code></td>
<td>
<p>sample size limit above which approximate nearest neighbour
search is activated.</p>
</td></tr>
<tr><td><code id="shapleysobol_knn_+3A_u">U</code></td>
<td>
<p>an integer equal to 0 (total Sobol indices) or 1 (first-order Sobol indices) 
or a list of vector indices defining the subsets of inputs whose sensitivity indices 
must be computed or a matrix of 0s and 1s where each row encodes a subset of inputs 
whose sensitivity indices must be computed (see examples). Default value is <code>NULL</code>,
meaning that Shapley values are returned (see details).</p>
</td></tr>
<tr><td><code id="shapleysobol_knn_+3A_n.perm">n.perm</code></td>
<td>
<p>an integer, indicating the number of random permutations used 
for the Shapley effects' estimation. Default is <code>n.perm=NULL</code>, indicating
that all possible permutations are used.</p>
</td></tr>
<tr><td><code id="shapleysobol_knn_+3A_noise">noise</code></td>
<td>
<p>a logical which is TRUE if the model or the output sample is 
noisy. See details.</p>
</td></tr>
<tr><td><code id="shapleysobol_knn_+3A_rescale">rescale</code></td>
<td>
<p>a logical indicating if continuous inputs must be rescaled before distance computations.
If TRUE, continuous inputs are first whitened with the ZCA-cor whitening procedure 
(cf. whiten() function in package <code>whitening</code>). If the inputs are independent, 
this first step will have a very limited impact. Then, the resulting whitened inputs 
are individually modified via a copula transform such that each input has the same scale.</p>
</td></tr>
<tr><td><code id="shapleysobol_knn_+3A_nboot">nboot</code></td>
<td>
<p>the number of bootstrap resamples for the bootstrap estimate of 
confidence intervals. See details.</p>
</td></tr>
<tr><td><code id="shapleysobol_knn_+3A_boot.level">boot.level</code></td>
<td>
<p>a numeric between 0 and 1 for the proportion of the 
bootstrap sample size.</p>
</td></tr>
<tr><td><code id="shapleysobol_knn_+3A_conf">conf</code></td>
<td>
<p>the confidence level of the bootstrap confidence intervals.</p>
</td></tr>
<tr><td><code id="shapleysobol_knn_+3A_parl">parl</code></td>
<td>
<p>number of cores on which to parallelize the computation. If
<code>NULL</code>, then no parallelization is done.</p>
</td></tr>
<tr><td><code id="shapleysobol_knn_+3A_x">x</code></td>
<td>
<p>the object returned by <code>shapleysobol_knn</code>.</p>
</td></tr>
<tr><td><code id="shapleysobol_knn_+3A_data">data</code></td>
<td>
<p>the object returned by <code>shapleysobol_knn</code>.</p>
</td></tr>
<tr><td><code id="shapleysobol_knn_+3A_y">y</code></td>
<td>
<p>a numeric univariate vector containing the observed outputs.</p>
</td></tr>
<tr><td><code id="shapleysobol_knn_+3A_ylim">ylim</code></td>
<td>
<p>the y-coordinate limits for plotting.</p>
</td></tr>
<tr><td><code id="shapleysobol_knn_+3A_mapping">mapping</code></td>
<td>
<p>Default list of aesthetic mappings to use for plot. If not specified, 
must be supplied in each layer added to the plot.</p>
</td></tr>
<tr><td><code id="shapleysobol_knn_+3A_environment">environment</code></td>
<td>
<p>[Deprecated] Used prior to tidy evaluation.</p>
</td></tr>
<tr><td><code id="shapleysobol_knn_+3A_...">...</code></td>
<td>
<p>additional arguments to be passed to <code>model</code>, or to the 
methods, such as graphical parameters (see <code>par</code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For <code>method="rank"</code>, the estimator is defined in Gamboa et al. (2020) 
following Chatterjee (2019). For first-order indices it is based on an input 
ranking (same algorithm as in <code>sobolrank</code>) while for higher orders, 
it uses an approximate heuristic solution of the traveling salesman problem 
applied to the input sample distances (cf. TSP() function in package 
<code>TSP</code>).  For <code>method="knn"</code>, ranking and TSP are replaced by a 
nearest neighbour search   as proposed in Broto et al. (2020) and in Azadkia 
&amp; Chatterjee (2020) for a similar coefficient. 
</p>
<p>The computation is done using the subset procedure, defined in Broto, Bachoc
and Depecker (2020), that is computing all the Sobol' closed indices for all 
possible sub-models first, and then affecting the Shapley weights.
</p>
<p>It is the same algorithm as <code>sobolshap_knn</code> with <code>method = "knn"</code>
with a slight computational improvement (the search for weight affectations is
done on much smaller matrices, stored in a list indexed by their order), and 
ability to perform parallel computation   and boostrap confidence interval 
estimates.
</p>
<p>Since boostrap creates ties which are not accounted for in the algorithm, 
confidence intervals are obtained by sampling without replacement with a 
proportion of the total sample size <code>boot.level</code>, drawn uniformly.
</p>
<p>If the outputs are noisy, the argument <code>noise</code> can be used: it only has 
an impact on the estimation of one specific sensitivity index, namely 
<code class="reqn">Var(E(Y|X1,\ldots,Xp))/Var(Y)</code>. If there is no noise this index is equal 
to 1, while in the presence of noise it must be estimated.
</p>
<p>The distance used for subsets with mixed inputs (continuous and categorical) 
is the Euclidean distance, thanks to a one-hot encoding of categorical inputs.
</p>
<p>If too many cores for the machine are passed on to the <code>parl</code> argument,
the chosen number of cores is defaulted to the available cores minus one.
</p>
<p>If argument <code>U</code> is specified, only the estimated first-order or total 
Sobol' indices are returned, or the estimated closed Sobol' indices for the 
selected subsets. The Shapley effects are not computed, and thus, not returned.
</p>
<p>The <code>extract</code> method can be used for extracting first-order and total
Sobol' indices, after the Shapley effects have been computed. It returns a list
containing both sensitivity indices.
</p>


<h3>Value</h3>

<p><code>shapleysobol_knn</code> returns a list of class <code>"shapleysobol_knn"</code> if <code>U=NULL</code>, 
containing the following components:
</p>
<table role = "presentation">
<tr><td><code>call</code></td>
<td>
<p>the matched call.</p>
</td></tr>
<tr><td><code>Shap</code></td>
<td>
<p>the estimations of the Shapley effect indices.</p>
</td></tr>
<tr><td><code>VE</code></td>
<td>
<p>the estimations of the closed Sobol' indices for all possible sub-models.</p>
</td></tr>
<tr><td><code>indices</code></td>
<td>
<p>list of all subsets corresponding to the structure of VE.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>which estimation method has been used.</p>
</td></tr>
<tr><td><code>n.perm</code></td>
<td>
<p>number of random permutations.</p>
</td></tr>
<tr><td><code>w</code></td>
<td>
<p>the Shapley weights.</p>
</td></tr>
<tr><td><code>conf_int</code></td>
<td>
<p>a matrix containing the estimations, biais and confidence
intervals by bootstrap (if <code>nboot&gt;0</code>).</p>
</td></tr>
<tr><td><code>X</code></td>
<td>
<p>the observed covariates.</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>the observed outcomes.</p>
</td></tr>
<tr><td><code>n.knn</code></td>
<td>
<p>value of the <code>n.knn</code> argument.</p>
</td></tr>
<tr><td><code>n.limit</code></td>
<td>
<p>value of the <code>n.limit</code> argument.</p>
</td></tr>
<tr><td><code>U</code></td>
<td>
<p>value of the <code>U</code> argument.</p>
</td></tr>
<tr><td><code>rescale</code></td>
<td>
<p>wheter the design matrix has been rescaled.</p>
</td></tr>
<tr><td><code>n.limit</code></td>
<td>
<p>maximum number of sample before nearest-neighbor approximation.</p>
</td></tr>
<tr><td><code>boot.level</code></td>
<td>
<p>value of the <code>boot.level</code> argument.</p>
</td></tr>
<tr><td><code>noise</code></td>
<td>
<p>wheter the Shapley values must sum up to one or not.</p>
</td></tr>
<tr><td><code>boot</code></td>
<td>
<p>logical, wheter bootstrap confidence interval estimates have 
been performed.</p>
</td></tr>
<tr><td><code>nboot</code></td>
<td>
<p>value of the <code>nboot</code> argument.</p>
</td></tr>
<tr><td><code>parl</code></td>
<td>
<p>value of the <code>parl</code> argument.</p>
</td></tr>
<tr><td><code>conf</code></td>
<td>
<p>value of the <code>conf</code> argument.</p>
</td></tr>
</table>
<p><code>shapleysobol_knn</code> returns a list of class <code>"sobol_knn"</code> if <code>U</code>, 
is specified, containing the following components:
</p>
<table role = "presentation">
<tr><td><code>call</code></td>
<td>
<p>the matched call.</p>
</td></tr>
<tr><td><code>Sobol</code></td>
<td>
<p>the estimations of the Sobol' indices.</p>
</td></tr>
<tr><td><code>indices</code></td>
<td>
<p>list of all subsets corresponding to the structure of VE.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>which estimation method has been used.</p>
</td></tr>
<tr><td><code>conf_int</code></td>
<td>
<p>a matrix containing the estimations, biais and confidence
intervals by bootstrap (if <code>nboot&gt;0</code>).</p>
</td></tr>
<tr><td><code>X</code></td>
<td>
<p>the observed covariates.</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>the observed outcomes.</p>
</td></tr>
<tr><td><code>U</code></td>
<td>
<p>value of the <code>U</code> argument.</p>
</td></tr>
<tr><td><code>n.knn</code></td>
<td>
<p>value of the <code>n.knn</code> argument.</p>
</td></tr>
<tr><td><code>rescale</code></td>
<td>
<p>wheter the design matrix has been rescaled.</p>
</td></tr>
<tr><td><code>n.limit</code></td>
<td>
<p>value of the <code>n.limit</code> argument.</p>
</td></tr>
<tr><td><code>boot.level</code></td>
<td>
<p>value of the <code>boot.level</code> argument.</p>
</td></tr>
<tr><td><code>boot</code></td>
<td>
<p>logical, wheter bootstrap confidence interval estimates have 
been performed.</p>
</td></tr>
<tr><td><code>nboot</code></td>
<td>
<p>value of the <code>nboot</code> argument.</p>
</td></tr>
<tr><td><code>parl</code></td>
<td>
<p>value of the <code>parl</code> argument.</p>
</td></tr>
<tr><td><code>conf</code></td>
<td>
<p>value of the <code>conf</code> argument.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Marouane Il Idrissi, Sebastien Da Veiga
</p>


<h3>References</h3>

<p>Azadkia M., Chatterjee S., 2021), <em>A simple measure of conditional dependence</em>, 
Ann. Statist. 49(6):3070-3102.
</p>
<p>Chatterjee, S., 2021, <em>A new coefficient of correlation</em>, Journal of the American 
Statistical Association, 116:2009-2022. 
</p>
<p>Gamboa, F., Gremaud, P., Klein, T., &amp; Lagnoux, A., 2022, <em>Global Sensitivity Analysis: 
a novel generation of mighty estimators based on rank statistics</em>, 
Bernoulli 28: 2345-2374.
</p>
<p>Broto B., Bachoc F. and Depecker M. (2020) <em>Variance Reduction for Estimation
of Shapley Effects and Adaptation to Unknown Input Distribution.</em> SIAM/ASA Journal
on Uncertainty Quantification, 8(2).
</p>
<p>Castro J., Gomez D, Tejada J. (2009). <em>Polynomial calculation of the Shapley value based 
on sampling.</em> Computers &amp; Operations Research, 36(5):1726-1730.
</p>
<p>M. Il Idrissi, V. Chabridon and B. Iooss (2021). <em>Developments and applications
of Shapley effects   to reliability-oriented sensitivity analysis with correlated inputs.</em>
Environmental Modelling &amp; Software, 143, 105115.
</p>
<p>M. Il Idrissi, V. Chabridon and B. Iooss (2021). <em>Mesures d'importance relative  
par decompositions de la performance de modeles de regression,</em> Preprint, 52emes Journees 
de Statistiques de la Societe Francaise de Statistique (SFdS), pp. 497-502, 
Nice, France, Juin 2021
</p>


<h3>See Also</h3>

<p><code><a href="#topic+sobolrank">sobolrank</a></code>, <code><a href="#topic+sobolshap_knn">sobolshap_knn</a></code>, <code><a href="#topic+shapleyPermEx">shapleyPermEx</a></code>, 
<code><a href="#topic+shapleySubsetMc">shapleySubsetMc</a></code>, <code><a href="#topic+johnsonshap">johnsonshap</a></code>, <code><a href="#topic+lmg">lmg</a></code>, <code><a href="#topic+pme_knn">pme_knn</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  
  
library(parallel)
library(doParallel)
library(foreach)
library(gtools)
library(boot)
library(RANN)

###########################################################
# Linear Model with Gaussian correlated inputs

library(mvtnorm)

set.seed(1234)
n &lt;- 1000
beta&lt;-c(1,-1,0.5)
sigma&lt;-matrix(c(1,0,0,
                0,1,-0.8,
                0,-0.8,1),
              nrow=3,
              ncol=3)

X &lt;-rmvnorm(n, rep(0,3), sigma)
colnames(X)&lt;-c("X1","X2", "X3")


y &lt;- X%*%beta + rnorm(n,0,2)

# Without Bootstrap confidence intervals
x&lt;-shapleysobol_knn(model=NULL, X=X,
            n.knn=3,
            noise=TRUE)
tell(x,y)
print(x)
plot(x)

#Using the extract method to get first-order and total Sobol' indices
extract(x)

# With Boostrap confidence intervals
x&lt;-shapleysobol_knn(model=NULL, X=X,
            nboot=10, 
            n.knn=3,
            noise=TRUE,
            boot.level=0.7, 
            conf=0.95)
tell(x,y)
print(x)
plot(x)

#####################
# Extracting Sobol' indices with Bootstrap confidence intervals

nboot &lt;- 10 # put nboot=50 for consistency

#Total Sobol' indices
x&lt;-shapleysobol_knn(model=NULL, X=X,
            nboot=nboot, 
            n.knn=3,
            U=0,
            noise=TRUE,
            boot.level=0.7, 
            conf=0.95)
tell(x,y)
print(x)
plot(x)

#First-order Sobol' indices
x&lt;-shapleysobol_knn(model=NULL, X=X,
            nboot=nboot, 
            n.knn=3,
            U=1,
            noise=TRUE,
            boot.level=0.7, 
            conf=0.95)
tell(x,y)
print(x)
plot(x)

#Closed Sobol' indices for specific subsets (list)
x&lt;-shapleysobol_knn(model=NULL, X=X,
            nboot=nboot, 
            n.knn=3,
            U=list(c(1,2), c(1,2,3), 2),
            noise=TRUE,
            boot.level=0.7, 
            conf=0.95)
tell(x,y)
print(x)
plot(x)


#####################################################
# Test case: the non-monotonic Sobol g-function
# Example with a call to a numerical model
# First compute first-order indices with ranking
    
n &lt;- 1000
X &lt;- data.frame(matrix(runif(8 * n), nrow = n))
x &lt;- shapleysobol_knn(model = sobol.fun, X = X, U = 1, method = "rank")
print(x)
plot(x)

library(ggplot2) ; ggplot(x)

# We can use the output sample generated for this estimation to compute total indices 
# without additional calls to the model
x2 &lt;- shapleysobol_knn(model = NULL, X = X, U = 0, method = "knn", n.knn = 5)
tell(x2,x$y)
plot(x2)

ggplot(x2)


#####################################################
# Test case: the Ishigami function
# Example with given data and the use of approximate nearest neighbour search
n &lt;- 5000
X &lt;- data.frame(matrix(-pi+2*pi*runif(3 * n), nrow = n))
Y &lt;- ishigami.fun(X)
x &lt;- shapleysobol_knn(model = NULL, X = X, U = NULL, method = "knn", n.knn = 5, 
                       n.limit = 2000)
tell(x,Y)
plot(x)

library(ggplot2) ; ggplot(x)

# Extract first-order and total Sobol indices
x1 &lt;- extract(x) ; print(x1)
    
######################################################
# Test case : Linear model (3 Gaussian inputs including 2 dependent) with scaling
# See Iooss and Prieur (2019)
library(mvtnorm) # Multivariate Gaussian variables
library(whitening) # For scaling
modlin &lt;- function(X) apply(X,1,sum)
d &lt;- 3
n &lt;- 10000
mu &lt;- rep(0,d)
sig &lt;- c(1,1,2)
ro &lt;- 0.9
Cormat &lt;- matrix(c(1,0,0,0,1,ro,0,ro,1),d,d)
Covmat &lt;- ( sig %*% t(sig) ) * Cormat
Xall &lt;- function(n) mvtnorm::rmvnorm(n,mu,Covmat)
X &lt;- Xall(n)
x &lt;- shapleysobol_knn(model = modlin, X = X, U = NULL, method = "knn", n.knn = 5, 
                       rescale = TRUE, n.limit = 2000)
print(x)
plot(x)

</code></pre>

<hr>
<h2 id='shapleySubsetMc'>Estimation of Shapley effects from data using nearest neighbors method</h2><span id='topic+shapleySubsetMc'></span><span id='topic+plot.shapleySubsetMc'></span>

<h3>Description</h3>

<p><code>shapleySubsetMc</code> implements the estimation of
the Shapley effects from data using some nearest neighbors method
to generate according to the conditional distributions of the inputs.
It can be used with categorical inputs.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>shapleySubsetMc(X,Y, Ntot=NULL, Ni=3, cat=NULL, weight=NULL, discrete=NULL, 
                noise=FALSE)
## S3 method for class 'shapleySubsetMc'
plot(x, ylim = c(0, 1), ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="shapleySubsetMc_+3A_x">X</code></td>
<td>
<p>a matrix or a dataframe of the input sample</p>
</td></tr>
<tr><td><code id="shapleySubsetMc_+3A_y">Y</code></td>
<td>
<p>a vector of the output sample</p>
</td></tr>
<tr><td><code id="shapleySubsetMc_+3A_ntot">Ntot</code></td>
<td>
<p>an integer of the approximate cost wanted</p>
</td></tr>
<tr><td><code id="shapleySubsetMc_+3A_ni">Ni</code></td>
<td>
<p>the number of nearest neighbours taken for each point</p>
</td></tr>
<tr><td><code id="shapleySubsetMc_+3A_cat">cat</code></td>
<td>
<p>a vector giving the indices of the input categorical variables</p>
</td></tr>
<tr><td><code id="shapleySubsetMc_+3A_weight">weight</code></td>
<td>
<p>a vector with the same length of <code>cat</code> giving the weight of each
categorical variable in the product distance</p>
</td></tr>
<tr><td><code id="shapleySubsetMc_+3A_discrete">discrete</code></td>
<td>
<p>a vector giving the indices of the input variable that are
real, and not categorical, but that can take several times the same values</p>
</td></tr>
<tr><td><code id="shapleySubsetMc_+3A_noise">noise</code></td>
<td>
<p>logical. If FALSE (the default), the variable Y is a function of X</p>
</td></tr>
<tr><td><code id="shapleySubsetMc_+3A_x">x</code></td>
<td>
<p>a list of class <code>"shapleySubsetMc"</code> storing the state of the
sensitivity study (Shapley effects, cost, names of inputs)</p>
</td></tr>
<tr><td><code id="shapleySubsetMc_+3A_ylim">ylim</code></td>
<td>
<p>y-coordinate plotting limits</p>
</td></tr>
<tr><td><code id="shapleySubsetMc_+3A_...">...</code></td>
<td>
<p>any other arguments for plotting</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>weight = NULL</code>, all the categorical variables will have the same weight 1.
</p>
<p>If <code>Ntot = NULL</code>, the nearest neighbours will be compute for all the <code class="reqn">n (2^p-2)</code> points,
where n is the length of the sample. The estimation can be very long with this parameter. 
</p>


<h3>Value</h3>

<p><code>shapleySubsetMc</code> returns a list of class <code>"shapleySubsetMc"</code>, 
containing:
</p>
<table role = "presentation">
<tr><td><code>shapley</code></td>
<td>
<p>the Shapley effects estimates.</p>
</td></tr>
<tr><td><code>cost</code></td>
<td>
<p>the real total cost of these estimates: the total number of points for which
the nearest neighbours were computed.</p>
</td></tr>
<tr><td><code>names</code></td>
<td>
<p>the labels of the input variables.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Baptiste Broto
</p>


<h3>References</h3>

<p>B. Broto, F. Bachoc, M. Depecker, 2020, <em>Variance reduction for estimation of 
Shapley effects and adaptation to unknown input distribution</em>, 
SIAM/ASA Journal of Uncertainty Quantification, 8:693-716.
</p>


<h3>See Also</h3>

<p><a href="#topic+shapleyPermEx">shapleyPermEx</a>, <a href="#topic+shapleyPermRand">shapleyPermRand</a>, <a href="#topic+shapleyLinearGaussian">shapleyLinearGaussian</a>, <a href="#topic+sobolrank">sobolrank</a>, <a href="#topic+shapleysobol_knn">shapleysobol_knn</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

# First example: the linear Gaussian framework

# we generate a covariance matrice Sigma
p &lt;- 4 #dimension
A &lt;- matrix(rnorm(p^2),nrow=p,ncol=p)
Sigma &lt;- t(A)%*%A # it means t(A)%*%A
C &lt;- chol(Sigma)
n &lt;- 500 #sample size (put n=2000 for more consistency)

Z=matrix(rnorm(p*n),nrow=n,ncol=p)
X=Z%*%C # X is a gaussian vector with zero mean and covariance Sigma
Y=rowSums(X) 
Shap=shapleySubsetMc(X=X,Y=Y,Ntot=5000)
plot(Shap)


#Second example: The Sobol model with heterogeneous inputs

p=8 #dimension
A=matrix(rnorm(p^2),nrow=p,ncol=p)
Sigma=t(A)%*%A
C=chol(Sigma)
n=500 #sample size (put n=5000 for more consistency)

Z=matrix(rnorm(p*n),nrow=n,ncol=p)
X=Z

#we create discrete and categorical variables
X[,1]=round(X[,1]/2) 
X[,2]=X[,2]&gt;2
X[,4]=-2*round(X[,4])+4
X[(X[,6]&gt;0 &amp;X[,6]&lt;1),6]=1

cat=c(1,2)  # we choose to take X1 and X2 as categorical variables 
            #   (with the discrete distance)
discrete=c(4,6) # we indicate that X4 and X6 can take several times the same value

Y=sobol.fun(X)
Ntot &lt;- 2000 # put Ntot=20000 for more consistency
Shap=shapleySubsetMc(X=X,Y=Y, cat=cat, discrete=discrete, Ntot=Ntot, Ni=10)

plot(Shap)


</code></pre>

<hr>
<h2 id='sobol'>Monte Carlo Estimation of Sobol' Indices</h2><span id='topic+sobol'></span><span id='topic+tell.sobol'></span><span id='topic+print.sobol'></span><span id='topic+plot.sobol'></span><span id='topic+plotMultOut.sobol'></span><span id='topic+ggplot.sobol'></span>

<h3>Description</h3>

 <p><code>sobol</code> implements the Monte Carlo estimation of
the Sobol' sensitivity indices (standard estimator). This method allows the estimation of
the indices of the variance decomposition, sometimes referred to as
functional ANOVA decomposition, up to a given order, at a total cost
of <code class="reqn">(N+1) \times n</code> where <code class="reqn">N</code> is the number
of indices to estimate. This function allows also the estimation of
the so-called subset (or group) indices, i.e. the first-order indices with respect to 
single multidimensional inputs.</p>


<h3>Usage</h3>

<pre><code class='language-R'>sobol(model = NULL, X1, X2, order = 1, nboot = 0, conf = 0.95, ...)
## S3 method for class 'sobol'
tell(x, y = NULL, return.var = NULL, ...)
## S3 method for class 'sobol'
print(x, ...)
## S3 method for class 'sobol'
plot(x, ylim = c(0, 1), ...)
  ## S3 method for class 'sobol'
plotMultOut(x, ylim = c(0, 1), ...)
## S3 method for class 'sobol'
ggplot(data, mapping = aes(), ylim = c(0, 1), ..., environment
                 = parent.frame())
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sobol_+3A_model">model</code></td>
<td>
<p>a function, or a model with a <code>predict</code> method,
defining the model to analyze.</p>
</td></tr>
<tr><td><code id="sobol_+3A_x1">X1</code></td>
<td>
<p>the first random sample.</p>
</td></tr>
<tr><td><code id="sobol_+3A_x2">X2</code></td>
<td>
<p>the second random sample.</p>
</td></tr>
<tr><td><code id="sobol_+3A_order">order</code></td>
<td>
<p>either an integer, the maximum order in the ANOVA
decomposition (all indices up to this order will be computed), or a
list of numeric vectors, the multidimensional compounds
of the wanted subset indices.</p>
</td></tr>
<tr><td><code id="sobol_+3A_nboot">nboot</code></td>
<td>
<p>the number of bootstrap replicates.</p>
</td></tr>
<tr><td><code id="sobol_+3A_conf">conf</code></td>
<td>
<p>the confidence level for bootstrap confidence intervals.</p>
</td></tr>
<tr><td><code id="sobol_+3A_x">x</code></td>
<td>
<p>a list of class <code>"sobol"</code> storing the state of the
sensitivity study (parameters, data, estimates).</p>
</td></tr>
<tr><td><code id="sobol_+3A_data">data</code></td>
<td>
<p>a list of class <code>"sobol"</code> storing the state of the
sensitivity study (parameters, data, estimates).</p>
</td></tr>
<tr><td><code id="sobol_+3A_y">y</code></td>
<td>
<p>a vector of model responses.</p>
</td></tr>
<tr><td><code id="sobol_+3A_return.var">return.var</code></td>
<td>
<p>a vector of character strings giving further
internal variables names to store in  the output object <code>x</code>.</p>
</td></tr>
<tr><td><code id="sobol_+3A_ylim">ylim</code></td>
<td>
<p>y-coordinate plotting limits.</p>
</td></tr>
<tr><td><code id="sobol_+3A_mapping">mapping</code></td>
<td>
<p>Default list of aesthetic mappings to use for plot. If not specified, 
must be supplied in each layer added to the plot.</p>
</td></tr>
<tr><td><code id="sobol_+3A_environment">environment</code></td>
<td>
<p>[Deprecated] Used prior to tidy evaluation.</p>
</td></tr>
<tr><td><code id="sobol_+3A_...">...</code></td>
<td>
<p>any other arguments for <code>model</code> which are passed
unchanged each time it is called.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>sobol</code> returns a list of class <code>"sobol"</code>, containing all
the input arguments detailed before, plus the following components:
</p>
<table role = "presentation">
<tr><td><code>call</code></td>
<td>
<p>the matched call.</p>
</td></tr>
<tr><td><code>X</code></td>
<td>
<p>a <code>data.frame</code> containing the design of experiments.</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>a vector of model responses.</p>
</td></tr>
<tr><td><code>V</code></td>
<td>
<p>the estimations of Variances of the Conditional Expectations
(VCE) with respect to one factor or one group of factors.</p>
</td></tr>
<tr><td><code>D</code></td>
<td>
<p>the estimations of the terms of the ANOVA decomposition (not
for subset indices).</p>
</td></tr>
<tr><td><code>S</code></td>
<td>
<p>the estimations of the Sobol' sensitivity indices (not for
subset indices).</p>
</td></tr>
</table>
<p>Users can ask more ouput variables with the argument
<code>return.var</code> (for example, bootstrap outputs <code>V.boot</code>,
<code>D.boot</code> and <code>S.boot</code>).
</p>


<h3>Author(s)</h3>

<p>Gilles Pujol
</p>


<h3>References</h3>

<p>I. M. Sobol, 1993, <em>Sensitivity analysis for non-linear mathematical
model</em>, Math. Modelling Comput. Exp., 1, 407&ndash;414.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+sobol2002">sobol2002</a>, <a href="#topic+sobolSalt">sobolSalt</a>, <a href="#topic+sobol2007">sobol2007</a>, <a href="#topic+soboljansen">soboljansen</a>, 
  <a href="#topic+sobolmartinez">sobolmartinez</a></code>,<code><a href="#topic+sobolEff">sobolEff</a>, <a href="#topic+sobolSmthSpl">sobolSmthSpl</a>, <a href="#topic+sobolmara">sobolmara</a>, 
  <a href="#topic+sobolroalhs">sobolroalhs</a>, <a href="#topic+fast99">fast99</a>, <a href="#topic+sobolGP">sobolGP</a></code>,<code><a href="#topic+sobolMultOut">sobolMultOut</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Test case : the non-monotonic Sobol g-function

# The method of sobol requires 2 samples
# (there are 8 factors, all following the uniform distribution on [0,1])
library(boot)
n &lt;- 1000
X1 &lt;- data.frame(matrix(runif(8 * n), nrow = n))
X2 &lt;- data.frame(matrix(runif(8 * n), nrow = n))

# sensitivity analysis
x &lt;- sobol(model = sobol.fun, X1 = X1, X2 = X2, order = 2, nboot = 100)
print(x)
#plot(x)

library(ggplot2)
ggplot(x)
</code></pre>

<hr>
<h2 id='sobol2002'>Monte Carlo Estimation of Sobol' Indices (scheme by Saltelli 2002)</h2><span id='topic+sobol2002'></span><span id='topic+tell.sobol2002'></span><span id='topic+print.sobol2002'></span><span id='topic+plot.sobol2002'></span><span id='topic+plotMultOut.sobol2002'></span><span id='topic+ggplot.sobol2002'></span>

<h3>Description</h3>

<p><code>sobol2002</code> implements the Monte Carlo estimation of
the Sobol' indices for both first-order and total indices at the same
time (alltogether <code class="reqn">2p</code> indices), at a total cost of <code class="reqn">(p+2)
    \times n</code> model evaluations. These are called the Saltelli estimators.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sobol2002(model = NULL, X1, X2, nboot = 0, conf = 0.95, ...)
## S3 method for class 'sobol2002'
tell(x, y = NULL, return.var = NULL, ...)
## S3 method for class 'sobol2002'
print(x, ...)
## S3 method for class 'sobol2002'
plot(x, ylim = c(0, 1), ...)
  ## S3 method for class 'sobol2002'
plotMultOut(x, ylim = c(0, 1), ...)
## S3 method for class 'sobol2002'
ggplot(data, mapping = aes(), ylim = c(0, 1), ..., environment
                 = parent.frame())
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sobol2002_+3A_model">model</code></td>
<td>
<p>a function, or a model with a <code>predict</code> method,
defining the model to analyze.</p>
</td></tr>
<tr><td><code id="sobol2002_+3A_x1">X1</code></td>
<td>
<p>the first random sample.</p>
</td></tr>
<tr><td><code id="sobol2002_+3A_x2">X2</code></td>
<td>
<p>the second random sample.</p>
</td></tr>
<tr><td><code id="sobol2002_+3A_nboot">nboot</code></td>
<td>
<p>the number of bootstrap replicates.</p>
</td></tr>
<tr><td><code id="sobol2002_+3A_conf">conf</code></td>
<td>
<p>the confidence level for bootstrap confidence intervals.</p>
</td></tr>
<tr><td><code id="sobol2002_+3A_x">x</code></td>
<td>
<p>a list of class <code>"sobol2002"</code> storing the state of the
sensitivity study (parameters, data, estimates).</p>
</td></tr>
<tr><td><code id="sobol2002_+3A_data">data</code></td>
<td>
<p>a list of class <code>"sobol2002"</code> storing the state of the
sensitivity study (parameters, data, estimates).</p>
</td></tr>
<tr><td><code id="sobol2002_+3A_y">y</code></td>
<td>
<p>a vector of model responses.</p>
</td></tr>
<tr><td><code id="sobol2002_+3A_return.var">return.var</code></td>
<td>
<p>a vector of character strings giving further
internal variables names to store in  the output object <code>x</code>.</p>
</td></tr>
<tr><td><code id="sobol2002_+3A_ylim">ylim</code></td>
<td>
<p>y-coordinate plotting limits.</p>
</td></tr>
<tr><td><code id="sobol2002_+3A_mapping">mapping</code></td>
<td>
<p>Default list of aesthetic mappings to use for plot. If not specified, 
must be supplied in each layer added to the plot.</p>
</td></tr>
<tr><td><code id="sobol2002_+3A_environment">environment</code></td>
<td>
<p>[Deprecated] Used prior to tidy evaluation.</p>
</td></tr>
<tr><td><code id="sobol2002_+3A_...">...</code></td>
<td>
<p>any other arguments for <code>model</code> which are passed
unchanged each time it is called</p>
</td></tr>
</table>


<h3>Details</h3>

<p>BE CAREFUL! This estimator suffers from a conditioning problem when estimating 
the variances behind the indices computations. This can seriously affect the 
Sobol' indices estimates in case of largely non-centered output. To avoid this 
effect, you have to center the model output before applying <code>"sobol2002"</code>. 
Functions <code>"sobolEff"</code>, <code>"soboljansen"</code> and <code>"sobolmartinez"</code> 
do not suffer from this problem.
</p>


<h3>Value</h3>

<p><code>sobol2002</code> returns a list of class <code>"sobol2002"</code>, containing all
the input arguments detailed before, plus the following components:
</p>
<table role = "presentation">
<tr><td><code>call</code></td>
<td>
<p>the matched call.</p>
</td></tr>
<tr><td><code>X</code></td>
<td>
<p>a <code>data.frame</code> containing the design of experiments.</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>the response used</p>
</td></tr>
<tr><td><code>V</code></td>
<td>
<p>the estimations of Variances of the Conditional Expectations
(VCE) with respect to each factor and also with respect to the
complementary set of each factor (&quot;all but <code class="reqn">X_i</code>&quot;).</p>
</td></tr>
<tr><td><code>S</code></td>
<td>
<p>the estimations of the Sobol' first-order indices.</p>
</td></tr>
<tr><td><code>T</code></td>
<td>
<p>the estimations of the Sobol' total sensitivity indices.</p>
</td></tr>
</table>
<p>Users can ask more ouput variables with the argument
<code>return.var</code> (for example, bootstrap outputs <code>V.boot</code>,
<code>S.boot</code> and <code>T.boot</code>).
</p>


<h3>Author(s)</h3>

<p>Gilles Pujol
</p>


<h3>References</h3>

<p>A. Saltelli, 2002, <em>Making best use of model evaluations to compute
sensitivity indices</em>, Computer Physics Communication, 145, 580&ndash;297.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+sobol">sobol</a>, <a href="#topic+sobolSalt">sobolSalt</a>, <a href="#topic+sobol2007">sobol2007</a>, <a href="#topic+soboljansen">soboljansen</a>, <a href="#topic+sobolmartinez">sobolmartinez</a>, <a href="#topic+sobolEff">sobolEff</a>, <a href="#topic+sobolmara">sobolmara</a>, <a href="#topic+sobolGP">sobolGP</a>, <a href="#topic+sobolMultOut">sobolMultOut</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Test case : the non-monotonic Sobol g-function

# The method of sobol requires 2 samples
# There are 8 factors, all following the uniform distribution
# on [0,1]

library(boot)
n &lt;- 1000
X1 &lt;- data.frame(matrix(runif(8 * n), nrow = n))
X2 &lt;- data.frame(matrix(runif(8 * n), nrow = n))

# sensitivity analysis

x &lt;- sobol2002(model = sobol.fun, X1, X2, nboot = 100)
print(x)
plot(x)

library(ggplot2)
ggplot(x)
</code></pre>

<hr>
<h2 id='sobol2007'>Monte Carlo Estimation of Sobol' Indices (improved formulas of Mauntz: Sobol et al. (2007) and Saltelli et al. (2010))</h2><span id='topic+sobol2007'></span><span id='topic+tell.sobol2007'></span><span id='topic+print.sobol2007'></span><span id='topic+plot.sobol2007'></span><span id='topic+plotMultOut.sobol2007'></span><span id='topic+ggplot.sobol2007'></span>

<h3>Description</h3>

<p><code>sobol2007</code> implements the Monte Carlo estimation of
the Sobol' indices for both first-order and total indices at the same
time (alltogether <code class="reqn">2p</code> indices), at a total cost of <code class="reqn">(p+2)
    \times n</code> model evaluations. These are called the Mauntz estimators.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sobol2007(model = NULL, X1, X2, nboot = 0, conf = 0.95, ...)
## S3 method for class 'sobol2007'
tell(x, y = NULL, return.var = NULL, ...)
## S3 method for class 'sobol2007'
print(x, ...)
## S3 method for class 'sobol2007'
plot(x, ylim = c(0, 1), ...)
  ## S3 method for class 'sobol2007'
plotMultOut(x, ylim = c(0, 1), ...)
## S3 method for class 'sobol2007'
ggplot(data, mapping = aes(), ylim = c(0, 1), ..., environment
                 = parent.frame())
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sobol2007_+3A_model">model</code></td>
<td>
<p>a function, or a model with a <code>predict</code> method,
defining the model to analyze.</p>
</td></tr>
<tr><td><code id="sobol2007_+3A_x1">X1</code></td>
<td>
<p>the first random sample.</p>
</td></tr>
<tr><td><code id="sobol2007_+3A_x2">X2</code></td>
<td>
<p>the second random sample.</p>
</td></tr>
<tr><td><code id="sobol2007_+3A_nboot">nboot</code></td>
<td>
<p>the number of bootstrap replicates.</p>
</td></tr>
<tr><td><code id="sobol2007_+3A_conf">conf</code></td>
<td>
<p>the confidence level for bootstrap confidence intervals.</p>
</td></tr>
<tr><td><code id="sobol2007_+3A_x">x</code></td>
<td>
<p>a list of class <code>"sobol2007"</code> storing the state of the
sensitivity study (parameters, data, estimates).</p>
</td></tr>
<tr><td><code id="sobol2007_+3A_data">data</code></td>
<td>
<p>a list of class <code>"sobol2007"</code> storing the state of the
sensitivity study (parameters, data, estimates).</p>
</td></tr>
<tr><td><code id="sobol2007_+3A_y">y</code></td>
<td>
<p>a vector of model responses.</p>
</td></tr>
<tr><td><code id="sobol2007_+3A_return.var">return.var</code></td>
<td>
<p>a vector of character strings giving further
internal variables names to store in  the output object <code>x</code>.</p>
</td></tr>
<tr><td><code id="sobol2007_+3A_ylim">ylim</code></td>
<td>
<p>y-coordinate plotting limits.</p>
</td></tr>
<tr><td><code id="sobol2007_+3A_mapping">mapping</code></td>
<td>
<p>Default list of aesthetic mappings to use for plot. If not specified, 
must be supplied in each layer added to the plot.</p>
</td></tr>
<tr><td><code id="sobol2007_+3A_environment">environment</code></td>
<td>
<p>[Deprecated] Used prior to tidy evaluation.</p>
</td></tr>
<tr><td><code id="sobol2007_+3A_...">...</code></td>
<td>
<p>any other arguments for <code>model</code> which are passed
unchanged each time it is called</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This estimator is good for small first-order and total indices.
</p>
<p>BE CAREFUL! This estimator suffers from a conditioning problem when estimating 
the variances behind the indices computations. This can seriously affect the 
Sobol' indices estimates in case of largely non-centered output. To avoid this 
effect, you have to center the model output before applying <code>"sobol2007"</code>. 
Functions <code>"sobolEff"</code>, <code>"soboljansen"</code> and <code>"sobolmartinez"</code> 
do not suffer from this problem.
</p>


<h3>Value</h3>

<p><code>sobol2007</code> returns a list of class <code>"sobol2007"</code>, containing all
the input arguments detailed before, plus the following components:
</p>
<table role = "presentation">
<tr><td><code>call</code></td>
<td>
<p>the matched call.</p>
</td></tr>
<tr><td><code>X</code></td>
<td>
<p>a <code>data.frame</code> containing the design of experiments.</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>the response used</p>
</td></tr>
<tr><td><code>V</code></td>
<td>
<p>the estimations of Variances of the Conditional Expectations
(VCE) with respect to each factor and also with respect to the
complementary set of each factor (&quot;all but <code class="reqn">X_i</code>&quot;).</p>
</td></tr>
<tr><td><code>S</code></td>
<td>
<p>the estimations of the Sobol' first-order indices.</p>
</td></tr>
<tr><td><code>T</code></td>
<td>
<p>the estimations of the Sobol' total sensitivity indices.</p>
</td></tr>
</table>
<p>Users can ask more ouput variables with the argument
<code>return.var</code> (for example, bootstrap outputs <code>V.boot</code>,
<code>S.boot</code> and <code>T.boot</code>).
</p>


<h3>Author(s)</h3>

<p>Bertrand Iooss
</p>


<h3>References</h3>

<p>I.M. Sobol, S. Tarantola, D. Gatelli, S.S. Kucherenko and W. Mauntz, 2007, <em>Estimating
the approximation errors when fixing unessential factors in global sensitivity analysis</em>,
Reliability Engineering and System Safety, 92, 957&ndash;960.
</p>
<p>A. Saltelli, P. Annoni, I. Azzini, F. Campolongo, M. Ratto and S. Tarantola, 2010,
<em>Variance based sensitivity analysis of model output. Design and estimator for the
total sensitivity index</em>, Computer Physics Communications 181, 259&ndash;270.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+sobol">sobol</a>, <a href="#topic+sobol2002">sobol2002</a>, <a href="#topic+sobolSalt">sobolSalt</a>, <a href="#topic+soboljansen">soboljansen</a>, <a href="#topic+sobolmartinez">sobolmartinez</a>, <a href="#topic+sobolEff">sobolEff</a>, <a href="#topic+sobolmara">sobolmara</a>, <a href="#topic+sobolMultOut">sobolMultOut</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Test case : the non-monotonic Sobol g-function

# The method of sobol requires 2 samples
# There are 8 factors, all following the uniform distribution
# on [0,1]

library(boot)
n &lt;- 1000
X1 &lt;- data.frame(matrix(runif(8 * n), nrow = n))
X2 &lt;- data.frame(matrix(runif(8 * n), nrow = n))

# sensitivity analysis

x &lt;- sobol2007(model = sobol.fun, X1, X2, nboot = 100)
print(x)
plot(x)

library(ggplot2)
ggplot(x)
</code></pre>

<hr>
<h2 id='sobolEff'>Monte Carlo Estimation of Sobol' Indices (formulas of Janon-Monod)</h2><span id='topic+sobolEff'></span><span id='topic+tell.sobolEff'></span><span id='topic+print.sobolEff'></span><span id='topic+plot.sobolEff'></span><span id='topic+ggplot.sobolEff'></span>

<h3>Description</h3>

 <p><code>sobolEff</code> implements the Monte Carlo estimation of the Sobol' sensitivity indices using the asymptotically efficient formulas in section 4.2.4.2 of Monod et al. (2006). Either all first-order indices or all total-effect indices are estimated at a cost of <code class="reqn">N \times (p+1)</code> model calls or all closed second-order indices are estimated at a cost of <code class="reqn">N \times p \choose 2)</code> model calls.</p>


<h3>Usage</h3>

<pre><code class='language-R'>sobolEff(model = NULL, X1, X2, order=1, nboot = 0, conf = 0.95, ...)
## S3 method for class 'sobolEff'
tell(x, y = NULL, ...)
## S3 method for class 'sobolEff'
print(x, ...)
## S3 method for class 'sobolEff'
plot(x, ylim = c(0, 1), ...)
## S3 method for class 'sobolEff'
ggplot(data, mapping = aes(), ylim = c(0, 1), ..., environment
                 = parent.frame())
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sobolEff_+3A_model">model</code></td>
<td>
<p>a function, or a model with a <code>predict</code> method,
defining the model to analyze.</p>
</td></tr>
<tr><td><code id="sobolEff_+3A_x1">X1</code></td>
<td>
<p>the first random sample.</p>
</td></tr>
<tr><td><code id="sobolEff_+3A_x2">X2</code></td>
<td>
<p>the second random sample.</p>
</td></tr>
<tr><td><code id="sobolEff_+3A_order">order</code></td>
<td>
<p>an integer specifying the indices to estimate: 0 for total effect indices,1 for first-order indices and 2 for closed second-order indices.</p>
</td></tr>
<tr><td><code id="sobolEff_+3A_nboot">nboot</code></td>
<td>
<p>the number of bootstrap replicates, or zero to use asymptotic 
standard deviation estimates given in Janon et al. (2012).</p>
</td></tr>
<tr><td><code id="sobolEff_+3A_conf">conf</code></td>
<td>
<p>the confidence level for confidence intervals.</p>
</td></tr>
<tr><td><code id="sobolEff_+3A_x">x</code></td>
<td>
<p>a list of class <code>"sobolEff"</code> storing the state of the
sensitivity study (parameters, data, estimates).</p>
</td></tr>
<tr><td><code id="sobolEff_+3A_data">data</code></td>
<td>
<p>a list of class <code>"sobolEff"</code> storing the state of the
sensitivity study (parameters, data, estimates).</p>
</td></tr>
<tr><td><code id="sobolEff_+3A_y">y</code></td>
<td>
<p>a vector of model responses.</p>
</td></tr>
<tr><td><code id="sobolEff_+3A_ylim">ylim</code></td>
<td>
<p>y-coordinate plotting limits.</p>
</td></tr>
<tr><td><code id="sobolEff_+3A_mapping">mapping</code></td>
<td>
<p>Default list of aesthetic mappings to use for plot. If not specified, 
must be supplied in each layer added to the plot.</p>
</td></tr>
<tr><td><code id="sobolEff_+3A_environment">environment</code></td>
<td>
<p>[Deprecated] Used prior to tidy evaluation.</p>
</td></tr>
<tr><td><code id="sobolEff_+3A_...">...</code></td>
<td>
<p>any other arguments for <code>model</code> which are passed
unchanged each time it is called.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The estimator used by sobolEff is defined in Monod et al. (2006), 
Section 4.2.4.2 and studied under the name T_N in Janon et al. (2012). 
This estimator is good for large first-order indices.
</p>


<h3>Value</h3>

<p><code>sobolEff</code> returns a list of class <code>"sobolEff"</code>, containing all
the input arguments detailed before, plus the following components:
</p>
<table role = "presentation">
<tr><td><code>call</code></td>
<td>
<p>the matched call.</p>
</td></tr>
<tr><td><code>X</code></td>
<td>
<p>a <code>data.frame</code> containing the design of experiments.</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>a vector of model responses.</p>
</td></tr>
<tr><td><code>S</code></td>
<td>
<p>the estimations of the Sobol' sensitivity indices.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Alexandre Janon, Laurent Gilquin
</p>


<h3>References</h3>

<p>Monod, H., Naud, C., Makowski, D. (2006), Uncertainty and sensitivity 
analysis for crop models in Working with Dynamic Crop Models: Evaluation, 
Analysis, Parameterization, and Applications, Elsevier.
</p>
<p>A. Janon, T. Klein, A. Lagnoux, M. Nodet, C. Prieur (2014), <em>Asymptotic normality and efficiency of two Sobol index estimators</em>, ESAIM: Probability and Statistics, 18:342-364.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+sobol">sobol</a>, <a href="#topic+sobol2002">sobol2002</a>, <a href="#topic+sobolSalt">sobolSalt</a>, <a href="#topic+sobol2007">sobol2007</a>, <a href="#topic+soboljansen">soboljansen</a>, <a href="#topic+sobolmartinez">sobolmartinez</a>,
  <a href="#topic+sobolSmthSpl">sobolSmthSpl</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Test case : the non-monotonic Sobol g-function

# The method of sobol requires 2 samples
# (there are 8 factors, all following the uniform distribution on [0,1])
n &lt;- 1000
X1 &lt;- data.frame(matrix(runif(8 * n), nrow = n))
X2 &lt;- data.frame(matrix(runif(8 * n), nrow = n))

# sensitivity analysis
x &lt;- sobolEff(model = sobol.fun, X1 = X1, X2 = X2, nboot = 0)
print(x)

library(ggplot2)
ggplot(x)
</code></pre>

<hr>
<h2 id='sobolGP'>Kriging-based sensitivity analysis</h2><span id='topic+sobolGP'></span><span id='topic+plot.sobolGP'></span><span id='topic+ask.sobolGP'></span><span id='topic+tell.sobolGP'></span><span id='topic+print.sobolGP'></span>

<h3>Description</h3>

<p>Perform a kriging-based global sensitivity analysis taking into account both the meta-model and the Monte-Carlo errors. The Sobol indices are estimated with a Monte-Carlo integration and the true function is substituted by a kriging model. It is built thanks to the function <code>km</code> of the package <code>DiceKriging</code>.
The complete conditional predictive distribution of the kriging model is considered (not only the predictive mean).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sobolGP(
	model,
	type="SK",
	MCmethod="sobol",
	X1,
	X2, 
	nsim=100,
	nboot=1,
	conf = 0.95,
	sequential = FALSE, 
	candidate, 
	sequential.tot=FALSE,
	max_iter = 1000) 

## S3 method for class 'sobolGP'
ask(x, tot = FALSE, ...)

## S3 method for class 'sobolGP'
tell(x, y=NULL, xpoint=NULL, newcandidate=NULL, ...)

## S3 method for class 'sobolGP'
print(x, ...)

## S3 method for class 'sobolGP'
plot(x,...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sobolGP_+3A_model">model</code></td>
<td>

<p>an object of class <code>"km"</code> specifying the kriging model built from package <code>"DiceKriging"</code> (see <code><a href="DiceKriging.html#topic+km">km</a></code>).
</p>
</td></tr>
<tr><td><code id="sobolGP_+3A_type">type</code></td>
<td>

<p>a character string giving the type of the considered kriging model. <code>"SK"</code> refers to simple kriging and <code>"UK"</code> refers to universal kriging (see <code><a href="DiceKriging.html#topic+km">km</a></code>).
</p>
</td></tr>
<tr><td><code id="sobolGP_+3A_mcmethod">MCmethod</code></td>
<td>

<p>a character string specifying the Monte-Carlo procedure used to estimate the Sobol indices. The avaible methods are : <code>"sobol"</code>, <code>"sobol2002"</code>, <code>"sobol2007"</code>, <code>"sobolEff"</code> and <code>"soboljansen"</code>.
</p>
</td></tr>
<tr><td><code id="sobolGP_+3A_x1">X1</code></td>
<td>

<p>a matrix representing the first random sample.  
</p>
</td></tr>
<tr><td><code id="sobolGP_+3A_x2">X2</code></td>
<td>

<p>a matrix representing the second random sample.
</p>
</td></tr>
<tr><td><code id="sobolGP_+3A_nsim">nsim</code></td>
<td>

<p>an integer giving the number of samples for the conditional Gaussian process. It is used to quantify the uncertainty due to the kriging approximation.
</p>
</td></tr>
<tr><td><code id="sobolGP_+3A_nboot">nboot</code></td>
<td>

<p>an integer representing the number of bootstrap replicates. It is used to quantify the uncertainty due to the Monte-Carlo integrations. We recommend to set <code>nboot = 100</code>.
</p>
</td></tr>
<tr><td><code id="sobolGP_+3A_conf">conf</code></td>
<td>

<p>a numeric representing the confidence intervals taking into account the uncertainty due to the bootstrap procedure and the Gaussian process samples.
</p>
</td></tr>
<tr><td><code id="sobolGP_+3A_sequential">sequential</code></td>
<td>

<p>a boolean. If <code>sequential=TRUE</code>, the procedure provides a new point where to perform a simulation. It is the one minimizing the sum of the MAIN effect estimate variances. The variance is taken with respect to the conditional Gaussian process. The new point is selected in the points <code>candidate</code>.
</p>
</td></tr>
<tr><td><code id="sobolGP_+3A_candidate">candidate</code></td>
<td>

<p>a matrix representing the candidate points where the best new point to be simulated is selected. The lines represent the points and the columns represent the dimension.
</p>
</td></tr>
<tr><td><code id="sobolGP_+3A_sequential.tot">sequential.tot</code></td>
<td>

<p>a boolean. If <code>sequential.tot=TRUE</code>, the procedure provides a new point where to perform the simulation. It is the one minimizing the sum of the TOTAL effect estimate. The variance is taken with respect to the conditional Gaussian process. The new point is selected in the points <code>candidate</code>.
</p>
</td></tr>
<tr><td><code id="sobolGP_+3A_max_iter">max_iter</code></td>
<td>

<p>a numeric giving the maximal number of iterations for the propagative Gibbs sampler. It is used to simulate  the realizations of the Gaussian process.
</p>
</td></tr>
<tr><td><code id="sobolGP_+3A_x">x</code></td>
<td>

<p>an object of class S3 <code>"sobolGP"</code> obtaining from the procedure <code>sobolGP</code>. It stores the results of the Kriging-based global sensitivity analysis.
</p>
</td></tr>
<tr><td><code id="sobolGP_+3A_tot">tot</code></td>
<td>

<p>a boolean. If <code>tot=TRUE</code>, the procedure ask provides a point relative to the uncertainty of the total Sobol' indices (instead of first order' ones). 
</p>
</td></tr>
<tr><td><code id="sobolGP_+3A_xpoint">xpoint</code></td>
<td>

<p>a matrix representing a new point added to the kriging model.
</p>
</td></tr>
<tr><td><code id="sobolGP_+3A_y">y</code></td>
<td>

<p>a numeric giving the response of the function at <code>xpoint</code>.
</p>
</td></tr>
<tr><td><code id="sobolGP_+3A_newcandidate">newcandidate</code></td>
<td>

<p>a matrix representing the new candidate points where the best point to be simulated is selected. If <code>newcandidate=NULL</code>, these points correspond to <code>candidate</code> without the new point <code>xpoint</code>.
</p>
</td></tr>
<tr><td><code id="sobolGP_+3A_...">...</code></td>
<td>
<p>any other arguments to be passed</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function <code>ask</code> provides the new point where the function should be simulated. Furthermore, the function <code>tell</code> performs a new kriging-based sensitivity analysis when the point <code>x</code> with the corresponding observation <code>y</code> is added.
</p>


<h3>Value</h3>

<p>An object of class S3 <code>sobolGP</code>.
</p>

<ul>
<li><p> call : a list containing the arguments of the function <code>sobolGP</code> :
</p>

<ul>
<li><p> X1 : X1
</p>
</li>
<li><p> X2 : X2
</p>
</li>
<li><p> conf : conf
</p>
</li>
<li><p> nboot : nboot
</p>
</li>
<li><p> candidate : candidate
</p>
</li>
<li><p> sequential : sequential
</p>
</li>
<li><p> max_iter : max_iter
</p>
</li>
<li><p> sequential.tot : sequential.tot
</p>
</li>
<li><p> model : model
</p>
</li>
<li><p> tot : tot
</p>
</li>
<li><p> method : MCmethod
</p>
</li>
<li><p> type : type
</p>
</li>
<li><p> nsim : nsim
</p>
</li></ul>

</li>
<li><p> S : a list containing the results of the kriging-based sensitivity analysis for the MAIN effects:
</p>

<ul>
<li><p> mean : a matrix giving the mean of the Sobol index estimates.
</p>
</li>
<li><p> var : a matrix giving the variance of the Sobol index estimates.
</p>
</li>
<li><p> ci : a matrix giving the confidence intervals of the Sobol index estimates according to <code>conf</code>.
</p>
</li>
<li><p> varPG : a matrix giving the variance of the Sobol index estimates due to the Gaussian process approximation.
</p>
</li>
<li><p> varMC : a matrix giving the variance of the Sobol index estimates due to the Monte-Carlo integrations.
</p>
</li>
<li><p> xnew : if <code>sequential=TRUE</code>, a matrix giving the point in <code>candidate</code> which is the best to simulate.
</p>
</li>
<li><p> xnewi : if <code>sequential=TRUE</code>, an integer giving the index of the point in <code>candidate</code> which is the best to simulate.
</p>
</li></ul>

</li>
<li><p> T : a list containing the results of the kriging-based sensitivity analysis for the TOTAL effects:
</p>

<ul>
<li><p> mean : a matrix giving the mean of the Sobol index estimates.
</p>
</li>
<li><p> var : a matrix giving the variance of the Sobol index estimates.
</p>
</li>
<li><p> ci : a matrix giving the confidence intervals of the Sobol index estimates according to <code>conf</code>.
</p>
</li>
<li><p> varPG : a matrix giving the variance of the Sobol index estimates due to the Gaussian process approximation.
</p>
</li>
<li><p> varMC : a matrix giving the variance of the Sobol index estimates due to the Monte-Carlo integrations.
</p>
</li>
<li><p> xnew : if <code>sequential.tot=TRUE</code>, a matrix giving the point in <code>candidate</code> which is the best to simulate.
</p>
</li>
<li><p> xnewi : if <code>sequential.tot=TRUE</code>, an integer giving the index of the point in <code>candidate</code> which is the best to simulate.
</p>
</li></ul>

</li></ul>



<h3>Author(s)</h3>

<p>Loic Le Gratiet, EDF R&amp;D
</p>


<h3>References</h3>

<p>L. Le Gratiet, C. Cannamela and B. Iooss (2014), A Bayesian approach for global sensitivity analysis of (multifidelity) computer codes, SIAM/ASA J. Uncertainty Quantification 2-1, pp. 336-363.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+sobol">sobol</a></code>, <code><a href="#topic+sobol2002">sobol2002</a></code>, <code><a href="#topic+sobol2007">sobol2007</a></code>, <code><a href="#topic+sobolEff">sobolEff</a></code>, <code><a href="#topic+soboljansen">soboljansen</a></code>, <a href="#topic+sobolMultOut">sobolMultOut</a>, <a href="DiceKriging.html#topic+km">km</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

library(DiceKriging)

#--------------------------------------#
# kriging model building
#--------------------------------------#

d &lt;- 2; n &lt;- 16
design.fact &lt;- expand.grid(x1=seq(0,1,length=4), x2=seq(0,1,length=4))
y &lt;- apply(design.fact, 1, branin) 

m &lt;- km(design=design.fact, response=y)

#--------------------------------------#
# sobol samples &amp; candidate points
#--------------------------------------#

n &lt;- 1000
X1 &lt;- data.frame(matrix(runif(d * n), nrow = n))
X2 &lt;- data.frame(matrix(runif(d * n), nrow = n))

candidate &lt;- data.frame(matrix(runif(d * 100), nrow = 100))

#--------------------------------------#
# Kriging-based Sobol
#--------------------------------------#

nsim &lt;- 10 # put nsim &lt;- 100
nboot &lt;- 10 # put nboot &lt;- 100

res &lt;- sobolGP(
model = m,
type="UK",
MCmethod="sobol",
X1,
X2,
nsim = nsim,
conf = 0.95,
nboot = nboot,
sequential = TRUE,
candidate,
sequential.tot=FALSE,
max_iter = 1000
) 

res
plot(res)

x &lt;- ask(res)
y &lt;- branin(x)

# The following line doesn't work (uncorrected bug: 
#     unused argument in km(), passed by update(), eval(), tell.sobolGP() ??)
#res.new &lt;- tell(res,y,x)
#res.new


</code></pre>

<hr>
<h2 id='soboljansen'>Monte Carlo Estimation of Sobol' Indices (improved formulas of Jansen (1999) and Saltelli et al. (2010))</h2><span id='topic+soboljansen'></span><span id='topic+tell.soboljansen'></span><span id='topic+print.soboljansen'></span><span id='topic+plot.soboljansen'></span><span id='topic+plotMultOut.soboljansen'></span><span id='topic+ggplot.soboljansen'></span>

<h3>Description</h3>

<p><code>soboljansen</code> implements the Monte Carlo estimation of
the Sobol' indices for both first-order and total indices at the same
time (alltogether <code class="reqn">2p</code> indices), at a total cost of <code class="reqn">(p+2)
    \times n</code> model evaluations. These are called the Jansen estimators.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>soboljansen(model = NULL, X1, X2, nboot = 0, conf = 0.95, ...)
## S3 method for class 'soboljansen'
tell(x, y = NULL, return.var = NULL, ...)
## S3 method for class 'soboljansen'
print(x, ...)
## S3 method for class 'soboljansen'
plot(x, ylim = c(0, 1), y_col = NULL, y_dim3 = NULL, ...)
  ## S3 method for class 'soboljansen'
plotMultOut(x, ylim = c(0, 1), ...)
## S3 method for class 'soboljansen'
ggplot(data, mapping = aes(), ylim = c(0, 1), y_col = NULL,
                 y_dim3 = NULL, ..., environment = parent.frame())
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="soboljansen_+3A_model">model</code></td>
<td>
<p>a function, or a model with a <code>predict</code> method,
defining the model to analyze.</p>
</td></tr>
<tr><td><code id="soboljansen_+3A_x1">X1</code></td>
<td>
<p>the first random sample.</p>
</td></tr>
<tr><td><code id="soboljansen_+3A_x2">X2</code></td>
<td>
<p>the second random sample.</p>
</td></tr>
<tr><td><code id="soboljansen_+3A_nboot">nboot</code></td>
<td>
<p>the number of bootstrap replicates.</p>
</td></tr>
<tr><td><code id="soboljansen_+3A_conf">conf</code></td>
<td>
<p>the confidence level for bootstrap confidence intervals.</p>
</td></tr>
<tr><td><code id="soboljansen_+3A_x">x</code></td>
<td>
<p>a list of class <code>"soboljansen"</code> storing the state of the
sensitivity study (parameters, data, estimates).</p>
</td></tr>
<tr><td><code id="soboljansen_+3A_data">data</code></td>
<td>
<p>a list of class <code>"soboljansen"</code> storing the state of the
sensitivity study (parameters, data, estimates).</p>
</td></tr>
<tr><td><code id="soboljansen_+3A_y">y</code></td>
<td>
<p>a vector of model responses.</p>
</td></tr>
<tr><td><code id="soboljansen_+3A_return.var">return.var</code></td>
<td>
<p>a vector of character strings giving further
internal variables names to store in  the output object <code>x</code>.</p>
</td></tr>
<tr><td><code id="soboljansen_+3A_ylim">ylim</code></td>
<td>
<p>y-coordinate plotting limits.</p>
</td></tr>
<tr><td><code id="soboljansen_+3A_y_col">y_col</code></td>
<td>
<p>an integer defining the index of the column of <code>x$y</code> to be
used for plotting the corresponding sensitivity indices (only applies if 
<code>x$y</code> is a matrix or an array). If set to <code>NULL</code> (as per default) 
and <code>x$y</code> is a matrix or an array, the first column (respectively the 
first element in the second dimension) of <code>x$y</code> is used (i.e. 
<code>y_col = 1</code>).</p>
</td></tr>
<tr><td><code id="soboljansen_+3A_y_dim3">y_dim3</code></td>
<td>
<p>an integer defining the index in the third dimension of 
<code>x$y</code> to be used for plotting the corresponding sensitivity indices 
(only applies if <code>x$y</code> is an array). If set to <code>NULL</code> (as per 
default) and <code>x$y</code> is a three-dimensional array, the first element in 
the third dimension of <code>x$y</code> is used (i.e. <code>y_dim3 = 1</code>).</p>
</td></tr>
<tr><td><code id="soboljansen_+3A_mapping">mapping</code></td>
<td>
<p>Default list of aesthetic mappings to use for plot. If not specified, 
must be supplied in each layer added to the plot.</p>
</td></tr>
<tr><td><code id="soboljansen_+3A_environment">environment</code></td>
<td>
<p>[Deprecated] Used prior to tidy evaluation.</p>
</td></tr>
<tr><td><code id="soboljansen_+3A_...">...</code></td>
<td>
<p>for <code>soboljansen</code>: any other arguments for <code>model</code> 
which are passed unchanged each time it is called.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This estimator is good for large first-order indices, and (large and small) total indices.
</p>
<p>This version of <code>soboljansen</code> also supports matrices and three-dimensional
arrays as output of <code>model</code>. If the model output is a matrix or an array, 
<code>V</code>, <code>S</code> and <code>T</code> are matrices or arrays as well (depending on the
type of <code>y</code> and the value of <code>nboot</code>).
</p>
<p>The bootstrap outputs <code>V.boot</code>, <code>S.boot</code> and <code>T.boot</code> can only be
returned if the model output is a vector (using argument <code>return.var</code>). For
matrix or array output, these objects can't be returned.
</p>


<h3>Value</h3>

<p><code>soboljansen</code> returns a list of class <code>"soboljansen"</code>, containing all
the input arguments detailed before, plus the following components:
</p>
<table role = "presentation">
<tr><td><code>call</code></td>
<td>
<p>the matched call.</p>
</td></tr>
<tr><td><code>X</code></td>
<td>
<p>a <code>data.frame</code> containing the design of experiments.</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>either a vector, a matrix or a three-dimensional array of model 
responses (depends on the output of <code>model</code>).</p>
</td></tr>
<tr><td><code>V</code></td>
<td>
<p>the estimations of Variances of the Conditional Expectations
(VCE) with respect to each factor and also with respect to the
complementary set of each factor (&quot;all but <code class="reqn">X_i</code>&quot;).</p>
</td></tr>
<tr><td><code>S</code></td>
<td>
<p>the estimations of the Sobol' first-order indices.</p>
</td></tr>
<tr><td><code>T</code></td>
<td>
<p>the estimations of the Sobol' total sensitivity indices.</p>
</td></tr>
</table>
<p>Users can ask more ouput variables with the argument
<code>return.var</code> (for example, bootstrap outputs <code>V.boot</code>,
<code>S.boot</code> and <code>T.boot</code>).
</p>


<h3>Author(s)</h3>

<p>Bertrand Iooss, with contributions from Frank Weber (2016)
</p>


<h3>References</h3>

<p>M.J.W. Jansen, 1999, <em>Analysis of variance designs for model output</em>,
Computer Physics Communication, 117, 35&ndash;43.
</p>
<p>A. Saltelli, P. Annoni, I. Azzini, F. Campolongo, M. Ratto and S. Tarantola, 2010,
<em>Variance based sensitivity analysis of model output. Design and estimator for the
total sensitivity index</em>, Computer Physics Communications 181, 259&ndash;270.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+sobol">sobol</a>, <a href="#topic+sobol2002">sobol2002</a>, <a href="#topic+sobolSalt">sobolSalt</a>, <a href="#topic+sobol2007">sobol2007</a>, <a href="#topic+sobolmartinez">sobolmartinez</a>, <a href="#topic+sobolEff">sobolEff</a>, <a href="#topic+sobolmara">sobolmara</a>, <a href="#topic+sobolMultOut">sobolMultOut</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Test case : the non-monotonic Sobol g-function

# The method of sobol requires 2 samples
# There are 8 factors, all following the uniform distribution
# on [0,1]

library(boot)
n &lt;- 1000
X1 &lt;- data.frame(matrix(runif(8 * n), nrow = n))
X2 &lt;- data.frame(matrix(runif(8 * n), nrow = n))

# sensitivity analysis

x &lt;- soboljansen(model = sobol.fun, X1, X2, nboot = 100)
print(x)
plot(x)

library(ggplot2)
ggplot(x)


# Only for demonstration purposes: a model function returning a matrix
sobol.fun_matrix &lt;- function(X){
  res_vector &lt;- sobol.fun(X)
  cbind(res_vector, 2 * res_vector)
}
x_matrix &lt;- soboljansen(model = sobol.fun_matrix, X1, X2)
plot(x_matrix, y_col = 2)
title(main = "y_col = 2")

# Also only for demonstration purposes: a model function returning a
# three-dimensional array
sobol.fun_array &lt;- function(X){
  res_vector &lt;- sobol.fun(X)
  res_matrix &lt;- cbind(res_vector, 2 * res_vector)
  array(data = c(res_matrix, 5 * res_matrix), 
        dim = c(length(res_vector), 2, 2))
}
x_array &lt;- soboljansen(model = sobol.fun_array, X1, X2)
plot(x_array, y_col = 2, y_dim3 = 2)
title(main = "y_col = 2, y_dim3 = 2")


</code></pre>

<hr>
<h2 id='sobolmara'>Monte Carlo Estimation of Sobol' Indices via matrix permutations</h2><span id='topic+sobolmara'></span><span id='topic+tell.sobolmara'></span><span id='topic+print.sobolmara'></span><span id='topic+plot.sobolmara'></span><span id='topic+plotMultOut.sobolmara'></span><span id='topic+ggplot.sobolmara'></span>

<h3>Description</h3>

 <p><code>sobolmara</code> implements the Monte Carlo estimation of
the first-order Sobol' sensitivity indices using the formula of Mara and Joseph (2008), called the
Mara estimator.
This method allows the estimation of all first-order p indices at a cost of 
2N model calls (the random sample size), then independently of p (the number of inputs). </p>


<h3>Usage</h3>

<pre><code class='language-R'>sobolmara(model = NULL, X1, ...)
## S3 method for class 'sobolmara'
tell(x, y = NULL, return.var = NULL, ...)
## S3 method for class 'sobolmara'
print(x, ...)
## S3 method for class 'sobolmara'
plot(x, ylim = c(0, 1), ...)
  ## S3 method for class 'sobolmara'
plotMultOut(x, ylim = c(0, 1), ...)
## S3 method for class 'sobolmara'
ggplot(data, mapping = aes(), ylim = c(0, 1), ..., environment
                 = parent.frame())
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sobolmara_+3A_model">model</code></td>
<td>
<p>a function, or a model with a <code>predict</code> method,
defining the model to analyze.</p>
</td></tr>
<tr><td><code id="sobolmara_+3A_x1">X1</code></td>
<td>
<p>the random sample.</p>
</td></tr>
<tr><td><code id="sobolmara_+3A_x">x</code></td>
<td>
<p>a list of class <code>"sobolmara"</code> storing the state of the
sensitivity study (parameters, data, estimates).</p>
</td></tr>
<tr><td><code id="sobolmara_+3A_data">data</code></td>
<td>
<p>a list of class <code>"sobolmara"</code> storing the state of the
sensitivity study (parameters, data, estimates).</p>
</td></tr>
<tr><td><code id="sobolmara_+3A_y">y</code></td>
<td>
<p>a vector of model responses.</p>
</td></tr>
<tr><td><code id="sobolmara_+3A_return.var">return.var</code></td>
<td>
<p>a vector of character strings giving further
internal variables names to store in  the output object <code>x</code>.</p>
</td></tr>
<tr><td><code id="sobolmara_+3A_ylim">ylim</code></td>
<td>
<p>y-coordinate plotting limits.</p>
</td></tr>
<tr><td><code id="sobolmara_+3A_mapping">mapping</code></td>
<td>
<p>Default list of aesthetic mappings to use for plot. If not specified, 
must be supplied in each layer added to the plot.</p>
</td></tr>
<tr><td><code id="sobolmara_+3A_environment">environment</code></td>
<td>
<p>[Deprecated] Used prior to tidy evaluation.</p>
</td></tr>
<tr><td><code id="sobolmara_+3A_...">...</code></td>
<td>
<p>any other arguments for <code>model</code> which are passed
unchanged each time it is called.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The estimator used by sobolmara is based on rearragement of a unique matrix via random permutations (see Mara and Joseph, 2008). Bootstrap confidence intervals are not available.
</p>


<h3>Value</h3>

<p><code>sobolmara</code> returns a list of class <code>"sobolmara"</code>, containing all
the input arguments detailed before, plus the following components:
</p>
<table role = "presentation">
<tr><td><code>call</code></td>
<td>
<p>the matched call.</p>
</td></tr>
<tr><td><code>X</code></td>
<td>
<p>a <code>data.frame</code> containing the design of experiments.</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>a vector of model responses.</p>
</td></tr>
<tr><td><code>S</code></td>
<td>
<p>the estimations of the Sobol' sensitivity indices.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Bertrand Iooss
</p>


<h3>References</h3>

<p>Mara, T. and Joseph, O.R. (2008), <em>Comparison of some efficient methods to evaluate the main effect of computer model factors</em>, Journal of Statistical Computation and Simulation, 78:167&ndash;178
</p>


<h3>See Also</h3>

<p><code><a href="#topic+sobolroalhs">sobolroalhs</a>, <a href="#topic+sobol">sobol</a>, <a href="#topic+sobolMultOut">sobolMultOut</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Test case : the non-monotonic Sobol g-function

# The method of sobolmara requires 1 sample
# (there are 8 factors, all following the uniform distribution on [0,1])
n &lt;- 1000
X1 &lt;- data.frame(matrix(runif(8 * n), nrow = n))

# sensitivity analysis
x &lt;- sobolmara(model = sobol.fun, X1 = X1)
print(x)
plot(x)

library(ggplot2)
ggplot(x)
</code></pre>

<hr>
<h2 id='sobolmartinez'>Monte Carlo Estimation of Sobol' Indices (formulas of Martinez (2011))</h2><span id='topic+sobolmartinez'></span><span id='topic+tell.sobolmartinez'></span><span id='topic+print.sobolmartinez'></span><span id='topic+plot.sobolmartinez'></span><span id='topic+ggplot.sobolmartinez'></span>

<h3>Description</h3>

<p><code>sobolmartinez</code> implements the Monte Carlo estimation of
the Sobol' indices for both first-order and total indices using 
correlation coefficients-based formulas, at a total cost of 
<code class="reqn">(p+2) \times n</code> model evaluations. 
These are called the Martinez estimators.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sobolmartinez(model = NULL, X1, X2, nboot = 0, conf = 0.95, ...)
## S3 method for class 'sobolmartinez'
tell(x, y = NULL, return.var = NULL, ...)
## S3 method for class 'sobolmartinez'
print(x, ...)
## S3 method for class 'sobolmartinez'
plot(x, ylim = c(0, 1), y_col = NULL, y_dim3 = NULL, ...)
## S3 method for class 'sobolmartinez'
ggplot(data, mapping = aes(), ylim = c(0, 1), y_col = NULL,
                 y_dim3 = NULL, ..., environment = parent.frame())
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sobolmartinez_+3A_model">model</code></td>
<td>
<p>a function, or a model with a <code>predict</code> method,
defining the model to analyze.</p>
</td></tr>
<tr><td><code id="sobolmartinez_+3A_x1">X1</code></td>
<td>
<p>the first random sample.</p>
</td></tr>
<tr><td><code id="sobolmartinez_+3A_x2">X2</code></td>
<td>
<p>the second random sample.</p>
</td></tr>
<tr><td><code id="sobolmartinez_+3A_nboot">nboot</code></td>
<td>
<p>the number of bootstrap replicates, or zero to use theoretical 
formulas based on confidence interfaces of correlation coefficient 
(Martinez, 2011).</p>
</td></tr>
<tr><td><code id="sobolmartinez_+3A_conf">conf</code></td>
<td>
<p>the confidence level for bootstrap confidence intervals.</p>
</td></tr>
<tr><td><code id="sobolmartinez_+3A_x">x</code></td>
<td>
<p>a list of class <code>"sobolmartinez"</code> storing the state of the
sensitivity study (parameters, data, estimates).</p>
</td></tr>
<tr><td><code id="sobolmartinez_+3A_data">data</code></td>
<td>
<p>a list of class <code>"sobolmartinez"</code> storing the state of the
sensitivity study (parameters, data, estimates).</p>
</td></tr>
<tr><td><code id="sobolmartinez_+3A_y">y</code></td>
<td>
<p>a vector of model responses.</p>
</td></tr>
<tr><td><code id="sobolmartinez_+3A_return.var">return.var</code></td>
<td>
<p>a vector of character strings giving further
internal variables names to store in  the output object <code>x</code>.</p>
</td></tr>
<tr><td><code id="sobolmartinez_+3A_ylim">ylim</code></td>
<td>
<p>y-coordinate plotting limits.</p>
</td></tr>
<tr><td><code id="sobolmartinez_+3A_y_col">y_col</code></td>
<td>
<p>an integer defining the index of the column of <code>x$y</code> to be
used for plotting the corresponding sensitivity indices (only applies if 
<code>x$y</code> is a matrix or an array). If set to <code>NULL</code> (as per default) 
and <code>x$y</code> is a matrix or an array, the first column (respectively the 
first element in the second dimension) of <code>x$y</code> is used (i.e. 
<code>y_col = 1</code>).</p>
</td></tr>
<tr><td><code id="sobolmartinez_+3A_y_dim3">y_dim3</code></td>
<td>
<p>an integer defining the index in the third dimension of 
<code>x$y</code> to be used for plotting the corresponding sensitivity indices 
(only applies if <code>x$y</code> is an array). If set to <code>NULL</code> (as per 
default) and <code>x$y</code> is a three-dimensional array, the first element in 
the third dimension of <code>x$y</code> is used (i.e. <code>y_dim3 = 1</code>).</p>
</td></tr>
<tr><td><code id="sobolmartinez_+3A_mapping">mapping</code></td>
<td>
<p>Default list of aesthetic mappings to use for plot. If not specified, 
must be supplied in each layer added to the plot.</p>
</td></tr>
<tr><td><code id="sobolmartinez_+3A_environment">environment</code></td>
<td>
<p>[Deprecated] Used prior to tidy evaluation.</p>
</td></tr>
<tr><td><code id="sobolmartinez_+3A_...">...</code></td>
<td>
<p>for <code>sobolmartinez</code>: any other arguments for <code>model</code> 
which are passed unchanged each time it is called</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This estimator supports missing values (NA or NaN) which can occur during the
simulation of the model on the design of experiments (due to code failure) 
even if Sobol' indices are no more rigorous variance-based sensitivity 
indices if missing values are present. In this case, a warning is displayed.
</p>
<p>This version of <code>sobolmartinez</code> also supports matrices and 
three-dimensional arrays as output of <code>model</code>. Bootstrapping (including 
bootstrap confidence intervals) is also supported for matrix or array output. 
However, theoretical confidence intervals (for <code>nboot = 0</code>) are only 
supported for vector output. If the model output is a matrix or an array, 
<code>V</code>, <code>S</code> and <code>T</code> are matrices or arrays as well (depending on the
type of <code>y</code> and the value of <code>nboot</code>).
</p>
<p>The bootstrap outputs <code>V.boot</code>, <code>S.boot</code> and <code>T.boot</code> can only be
returned if the model output is a vector (using argument <code>return.var</code>). For
matrix or array output, these objects can't be returned.
</p>


<h3>Value</h3>

<p><code>sobolmartinez</code> returns a list of class <code>"sobolmartinez"</code>, 
containing all the input arguments detailed before, plus the following 
components:
</p>
<table role = "presentation">
<tr><td><code>call</code></td>
<td>
<p>the matched call.</p>
</td></tr>
<tr><td><code>X</code></td>
<td>
<p>a <code>data.frame</code> containing the design of experiments.</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>either a vector, a matrix or a three-dimensional array of model 
responses (depends on the output of <code>model</code>).</p>
</td></tr>
<tr><td><code>V</code></td>
<td>
<p>the estimations of normalized variances of the Conditional 
Expectations (VCE) with respect to each factor and also with respect 
to the complementary set of each factor (&quot;all but <code class="reqn">X_i</code>&quot;).</p>
</td></tr>
<tr><td><code>S</code></td>
<td>
<p>the estimations of the Sobol' first-order indices.</p>
</td></tr>
<tr><td><code>T</code></td>
<td>
<p>the estimations of the Sobol' total sensitivity indices.</p>
</td></tr>
</table>
<p>Users can ask more ouput variables with the argument
<code>return.var</code> (for example, bootstrap outputs <code>V.boot</code>,
<code>S.boot</code> and <code>T.boot</code>).
</p>


<h3>Author(s)</h3>

<p>Bertrand Iooss, with contributions from Frank Weber (2016)
</p>


<h3>References</h3>

<p>J-M. Martinez, 2011, <em>Analyse de sensibilite globale par decomposition 
de la variance</em>, Presentation in the meeting of GdR Ondes and GdR MASCOT-NUM, 
January, 13th, 2011, Institut Henri Poincare, Paris, France.
</p>
<p>M. Baudin, K. Boumhaout, T. Delage, B. Iooss and J-M. Martinez, 2016, Numerical stability of Sobol' indices estimation formula, Proceedings of the SAMO 2016 Conference, Reunion Island, France, December 2016
</p>


<h3>See Also</h3>

<p><code><a href="#topic+sobol">sobol</a>, <a href="#topic+sobol2002">sobol2002</a>, <a href="#topic+sobolSalt">sobolSalt</a>, <a href="#topic+sobol2007">sobol2007</a>, <a href="#topic+soboljansen">soboljansen</a>, <a href="#topic+soboltouati">soboltouati</a>, <a href="#topic+sobolMultOut">sobolMultOut</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Test case : the non-monotonic Sobol g-function

# The method of sobol requires 2 samples
# There are 8 factors, all following the uniform distribution
# on [0,1]

library(boot)
n &lt;- 1000
X1 &lt;- data.frame(matrix(runif(8 * n), nrow = n))
X2 &lt;- data.frame(matrix(runif(8 * n), nrow = n))

# sensitivity analysis

x &lt;- sobolmartinez(model = sobol.fun, X1, X2, nboot = 0)
print(x)
plot(x)

library(ggplot2)
ggplot(x)


# Only for demonstration purposes: a model function returning a matrix
sobol.fun_matrix &lt;- function(X){
  res_vector &lt;- sobol.fun(X)
  cbind(res_vector, 2 * res_vector)
}
x_matrix &lt;- sobolmartinez(model = sobol.fun_matrix, X1, X2)
plot(x_matrix, y_col = 2)
title(main = "y_col = 2")

# Also only for demonstration purposes: a model function returning a
# three-dimensional array
sobol.fun_array &lt;- function(X){
  res_vector &lt;- sobol.fun(X)
  res_matrix &lt;- cbind(res_vector, 2 * res_vector)
  array(data = c(res_matrix, 5 * res_matrix), 
        dim = c(length(res_vector), 2, 2))
}
x_array &lt;- sobolmartinez(model = sobol.fun_array, X1, X2)
plot(x_array, y_col = 2, y_dim3 = 2)
title(main = "y_col = 2, y_dim3 = 2")

</code></pre>

<hr>
<h2 id='sobolMultOut'>Monte Carlo Estimation of Aggregated Sobol' Indices for multiple and functional outputs</h2><span id='topic+sobolMultOut'></span><span id='topic+print.sobolMultOut'></span><span id='topic+plot.sobolMultOut'></span><span id='topic+plotMultOut.sobolMultOut'></span><span id='topic+plotMultOut'></span><span id='topic+ggplot.sobolMultOut'></span>

<h3>Description</h3>

<p><code>sobolMultOut</code> implements the aggregated Sobol' indices for 
multiple outputs. It consists in averaging all the Sobol indices weighted
by the variance of their corresponding output. Moreover, this function computes and plots
the functional (unidimensional) Sobol' indices for functional (unidimensional)
model output via <code>plotMultOut</code>. Sobol' indices for both first-order and total indices are estimated 
by Monte Carlo formulas. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  sobolMultOut(model = NULL, q = 1, X1, X2, MCmethod = "sobol", 
               ubiquitous = FALSE, ...)
  ## S3 method for class 'sobolMultOut'
print(x, ...)
  ## S3 method for class 'sobolMultOut'
plot(x, ylim = c(0, 1), ...)
  ## S3 method for class 'sobolMultOut'
plotMultOut(x, ylim = c(0, 1), ...)
  ## S3 method for class 'sobolMultOut'
ggplot(data, mapping = aes(), ylim = c(0, 1), ..., environment
                 = parent.frame())
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sobolMultOut_+3A_model">model</code></td>
<td>
<p>a function, or a model with a <code>predict</code> method,
defining the model to analyze.</p>
</td></tr>
<tr><td><code id="sobolMultOut_+3A_q">q</code></td>
<td>
<p>dimension of the model output vector.</p>
</td></tr>
<tr><td><code id="sobolMultOut_+3A_x1">X1</code></td>
<td>
<p>the first random sample.</p>
</td></tr>
<tr><td><code id="sobolMultOut_+3A_x2">X2</code></td>
<td>
<p>the second random sample.</p>
</td></tr>
<tr><td><code id="sobolMultOut_+3A_mcmethod">MCmethod</code></td>
<td>
<p>a character string specifying the Monte-Carlo procedure used 
to estimate the Sobol indices. The avaible methods are : <code>"sobol"</code>, 
<code>"sobol2002"</code>, <code>"sobol2007"</code>, <code>"soboljansen"</code>, <code>"sobolmara"</code>
and <code>"sobolGP"</code>.</p>
</td></tr>
<tr><td><code id="sobolMultOut_+3A_ubiquitous">ubiquitous</code></td>
<td>
<p>if TRUE, 1D functional Sobol indices are computed (default=FALSE).</p>
</td></tr>
<tr><td><code id="sobolMultOut_+3A_x">x</code></td>
<td>
<p>a list of class <code>MCmethod</code> storing the state of the
sensitivity study (parameters, data, estimates).</p>
</td></tr>
<tr><td><code id="sobolMultOut_+3A_data">data</code></td>
<td>
<p>a list of class <code>MCmethod</code> storing the state of the
sensitivity study (parameters, data, estimates).</p>
</td></tr>
<tr><td><code id="sobolMultOut_+3A_ylim">ylim</code></td>
<td>
<p>y-coordinate plotting limits.</p>
</td></tr>
<tr><td><code id="sobolMultOut_+3A_mapping">mapping</code></td>
<td>
<p>Default list of aesthetic mappings to use for plot. If not specified, 
must be supplied in each layer added to the plot.</p>
</td></tr>
<tr><td><code id="sobolMultOut_+3A_environment">environment</code></td>
<td>
<p>[Deprecated] Used prior to tidy evaluation.</p>
</td></tr>
<tr><td><code id="sobolMultOut_+3A_...">...</code></td>
<td>
<p>any other arguments for <code>model</code> which are passed
unchanged each time it is called</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For this function, there are several gaps: the bootstrap estimation of confidence 
intervals is not avalaible and the tell function does not work.
Aggregated Sobol' indices can be plotted with the S3 method <code>plot</code> and ubiquitous Sobol' indices can be visualized with the S3 method <code>plotMultOut</code> (does not work for the <code>"sobolGP"</code> method).
</p>


<h3>Value</h3>

<p><code>sobolMultOut</code> returns a list of class <code>MCmethod</code>, containing all
its input arguments, plus the following components:
</p>
<table role = "presentation">
<tr><td><code>call</code></td>
<td>
<p>the matched call.</p>
</td></tr>
<tr><td><code>X</code></td>
<td>
<p>a <code>data.frame</code> containing the design of experiments.</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>the response used</p>
</td></tr>
<tr><td><code>V</code></td>
<td>
<p>the estimations of the aggregated Variances of the Conditional Expectations
(VCE) with respect to each factor and also with respect to the
complementary set of each factor (&quot;all but <code class="reqn">X_i</code>&quot;).</p>
</td></tr>
<tr><td><code>S</code></td>
<td>
<p>the estimations of the aggregated Sobol' first-order indices.</p>
</td></tr>
<tr><td><code>T</code></td>
<td>
<p>the estimations of the aggregated Sobol' total sensitivity indices.</p>
</td></tr>
<tr><td><code>Sfct</code></td>
<td>
<p>the estimations of the functional Sobol' first-order indices 
(if ubiquitous=TRUE and plot.fct=TRUE).</p>
</td></tr>
<tr><td><code>Tfct</code></td>
<td>
<p>the estimations of the functional Sobol' total sensitivity indices
(if ubiquitous=TRUE and plot.fct=TRUE).</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Bertrand Iooss
</p>


<h3>References</h3>

<p>M. Lamboni, H. Monod and D. Makowski, 2011, <em>Multivariate sensitivity analysis to measure global contribution of input factors in dynamic models</em>, Reliability Engineering and System Safety, 96:450-459.
</p>
<p>F. Gamboa, A. Janon, T. Klein and A. Lagnoux, 2014, <em>Sensitivity indices for multivariate outputs</em>, Electronic Journal of Statistics, 8:575-603.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+sobol">sobol</a>, <a href="#topic+sobol2002">sobol2002</a>, <a href="#topic+sobol2007">sobol2007</a>, <a href="#topic+soboljansen">soboljansen</a>,
    <a href="#topic+sobolmara">sobolmara</a>, <a href="#topic+sobolGP">sobolGP</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  
    
    # Tests on the functional toy fct 'Arctangent temporal function'
    
    y0 &lt;- atantemp.fun(matrix(c(-7,0,7,-7,0,7),ncol=2))
    #plot(y0[1,],type="l")
    #apply(y0,1,lines)
    
    n &lt;- 100
    X &lt;- matrix(c(runif(2*n,-7,7)),ncol=2)
    y &lt;- atantemp.fun(X)
    plot(y0[2,],ylim=c(-2,2),type="l")
    apply(y,1,lines)
    
    # Sobol indices computations
    
    n &lt;- 1000
    X1 &lt;- data.frame(matrix(runif(2*n,-7,7), nrow = n))
    X2 &lt;- data.frame(matrix(runif(2*n,-7,7), nrow = n))
    
    sa &lt;- sobolMultOut(model=atantemp.fun, q=100, X1, X2, 
                       MCmethod="soboljansen", ubiquitous=TRUE)
    print(sa)
    plot(sa)
    plotMultOut(sa)
    
    library(ggplot2)
    ggplot(sa)
  
</code></pre>

<hr>
<h2 id='sobolowen'>Monte Carlo Estimation of Sobol' Indices (improved formulas of Owen (2013)</h2><span id='topic+sobolowen'></span><span id='topic+tell.sobolowen'></span><span id='topic+print.sobolowen'></span><span id='topic+plot.sobolowen'></span><span id='topic+ggplot.sobolowen'></span>

<h3>Description</h3>

<p><code>sobolowen</code> implements the Monte Carlo estimation of
the Sobol' indices for both first-order and total indices at the same
time (alltogether <code class="reqn">2p</code> indices). Take as input 3 independent matrices.
These are called the Owen estimators.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sobolowen(model = NULL, X1, X2, X3, nboot = 0, conf = 0.95, varest = 2, ...)
## S3 method for class 'sobolowen'
tell(x, y = NULL, return.var = NULL, varest = 2, ...)
## S3 method for class 'sobolowen'
print(x, ...)
## S3 method for class 'sobolowen'
plot(x, ylim = c(0, 1), ...)
## S3 method for class 'sobolowen'
ggplot(data, mapping = aes(), ylim = c(0, 1), ..., environment
                 = parent.frame())
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sobolowen_+3A_model">model</code></td>
<td>
<p>a function, or a model with a <code>predict</code> method,
defining the model to analyze.</p>
</td></tr>
<tr><td><code id="sobolowen_+3A_x1">X1</code></td>
<td>
<p>the first random sample.</p>
</td></tr>
<tr><td><code id="sobolowen_+3A_x2">X2</code></td>
<td>
<p>the second random sample.</p>
</td></tr>
<tr><td><code id="sobolowen_+3A_x3">X3</code></td>
<td>
<p>the third random sample.</p>
</td></tr>
<tr><td><code id="sobolowen_+3A_nboot">nboot</code></td>
<td>
<p>the number of bootstrap replicates.</p>
</td></tr>
<tr><td><code id="sobolowen_+3A_conf">conf</code></td>
<td>
<p>the confidence level for bootstrap confidence intervals.</p>
</td></tr>
<tr><td><code id="sobolowen_+3A_varest">varest</code></td>
<td>
<p>choice for the variance estimator for the denominator of
the Sobol' indices. varest=1 is for a classical estimator.
varest=2 (default) is for the estimator proposed in Janon et al. (2012).</p>
</td></tr>
<tr><td><code id="sobolowen_+3A_x">x</code></td>
<td>
<p>a list of class <code>"sobolowen"</code> storing the state of the
sensitivity study (parameters, data, estimates).</p>
</td></tr>
<tr><td><code id="sobolowen_+3A_data">data</code></td>
<td>
<p>a list of class <code>"sobolowen"</code> storing the state of the
sensitivity study (parameters, data, estimates).</p>
</td></tr>
<tr><td><code id="sobolowen_+3A_y">y</code></td>
<td>
<p>a vector of model responses.</p>
</td></tr>
<tr><td><code id="sobolowen_+3A_return.var">return.var</code></td>
<td>
<p>a vector of character strings giving further
internal variables names to store in  the output object <code>x</code>.</p>
</td></tr>
<tr><td><code id="sobolowen_+3A_ylim">ylim</code></td>
<td>
<p>y-coordinate plotting limits.</p>
</td></tr>
<tr><td><code id="sobolowen_+3A_mapping">mapping</code></td>
<td>
<p>Default list of aesthetic mappings to use for plot. If not specified, 
must be supplied in each layer added to the plot.</p>
</td></tr>
<tr><td><code id="sobolowen_+3A_environment">environment</code></td>
<td>
<p>[Deprecated] Used prior to tidy evaluation.</p>
</td></tr>
<tr><td><code id="sobolowen_+3A_...">...</code></td>
<td>
<p>any other arguments for <code>model</code> which are passed
unchanged each time it is called</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>sobolowen</code> returns a list of class <code>"sobolowen"</code>, containing all
the input arguments detailed before, plus the following components:
</p>
<table role = "presentation">
<tr><td><code>call</code></td>
<td>
<p>the matched call.</p>
</td></tr>
<tr><td><code>X</code></td>
<td>
<p>a <code>data.frame</code> containing the design of experiments.</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>the response used</p>
</td></tr>
<tr><td><code>V</code></td>
<td>
<p>the estimations of Variances of the Conditional Expectations
(VCE) with respect to each factor and also with respect to the
complementary set of each factor (&quot;all but <code class="reqn">X_i</code>&quot;).</p>
</td></tr>
<tr><td><code>S</code></td>
<td>
<p>the estimations of the Sobol' first-order indices.</p>
</td></tr>
<tr><td><code>T</code></td>
<td>
<p>the estimations of the Sobol' total sensitivity indices.</p>
</td></tr>
</table>
<p>Users can ask more ouput variables with the argument
<code>return.var</code> (for example, bootstrap outputs <code>V.boot</code>,
<code>S.boot</code> and <code>T.boot</code>).
</p>


<h3>Author(s)</h3>

<p>Taieb Touati and Bernardo Ramos
</p>


<h3>References</h3>

<p>A. Owen, 2013, <em>Better estimations of small Sobol' sensitivity indices</em>, 
ACM Transactions on Modeling and Computer Simulations (TOMACS), 23(2), 11.
</p>
<p>Janon, A., Klein T., Lagnoux A., Nodet M., Prieur C. (2012), Asymptotic 
normality and efficiency of two Sobol index estimators. Accepted in 
ESAIM: Probability and Statistics.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+sobol">sobol</a>, <a href="#topic+sobol2002">sobol2002</a>, <a href="#topic+sobolSalt">sobolSalt</a>, <a href="#topic+sobol2007">sobol2007</a>, <a href="#topic+soboljansen">soboljansen</a>, <a href="#topic+sobolmartinez">sobolmartinez</a>, <a href="#topic+sobolEff">sobolEff</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Test case : the non-monotonic Sobol g-function

# The method of sobolowen requires 3 samples
# There are 8 factors, all following the uniform distribution
# on [0,1]

library(boot)
n &lt;- 1000
X1 &lt;- data.frame(matrix(runif(8 * n), nrow = n))
X2 &lt;- data.frame(matrix(runif(8 * n), nrow = n))
X3 &lt;- data.frame(matrix(runif(8 * n), nrow = n))

# sensitivity analysis


x &lt;- sobolowen(model = sobol.fun, X1, X2, X3, nboot = 10) # put nboot=100
print(x)
plot(x)

library(ggplot2)
ggplot(x)

</code></pre>

<hr>
<h2 id='sobolrank'>First-order sensitivity indices estimation via ranking</h2><span id='topic+sobolrank'></span><span id='topic+tell.sobolrank'></span><span id='topic+print.sobolrank'></span><span id='topic+plot.sobolrank'></span><span id='topic+ggplot.sobolrank'></span>

<h3>Description</h3>

 <p><code>sobolrank</code> implements the estimation of all first-order indices using only N model evaluations 
via ranking following Gamboa et al. (2020) and inspired by Chatterjee (2019).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sobolrank(model = NULL, X, nboot = 0, conf = 0.95, nsample = round(0.8*nrow(X)), 
          ...)
## S3 method for class 'sobolrank'
tell(x, y = NULL, ...)
## S3 method for class 'sobolrank'
print(x, ...)
## S3 method for class 'sobolrank'
plot(x, ylim = c(0, 1), ...)
## S3 method for class 'sobolrank'
ggplot(data, mapping = aes(), ..., environment
                 = parent.frame(), ylim = c(0, 1))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sobolrank_+3A_model">model</code></td>
<td>
<p>a function, or a model with a <code>predict</code> method,
defining the model to analyze.</p>
</td></tr>
<tr><td><code id="sobolrank_+3A_x">X</code></td>
<td>
<p>a random sample of the inputs.</p>
</td></tr>
<tr><td><code id="sobolrank_+3A_nboot">nboot</code></td>
<td>
<p>the number of bootstrap replicates, see details.</p>
</td></tr>
<tr><td><code id="sobolrank_+3A_conf">conf</code></td>
<td>
<p>the confidence level for confidence intervals, see details.</p>
</td></tr>
<tr><td><code id="sobolrank_+3A_nsample">nsample</code></td>
<td>
<p>the size of the bootstrap sample, see details.</p>
</td></tr>
<tr><td><code id="sobolrank_+3A_x">x</code></td>
<td>
<p>a list of class <code>"sobolrank"</code> storing the state of the
sensitivity study (parameters, data, estimates).</p>
</td></tr>
<tr><td><code id="sobolrank_+3A_data">data</code></td>
<td>
<p>a list of class <code>"sobolrank"</code> storing the state of the
sensitivity study (parameters, data, estimates).</p>
</td></tr>
<tr><td><code id="sobolrank_+3A_y">y</code></td>
<td>
<p>a vector of model responses.</p>
</td></tr>
<tr><td><code id="sobolrank_+3A_ylim">ylim</code></td>
<td>
<p>y-coordinate plotting limits.</p>
</td></tr>
<tr><td><code id="sobolrank_+3A_mapping">mapping</code></td>
<td>
<p>Default list of aesthetic mappings to use for plot. If not specified, 
must be supplied in each layer added to the plot.</p>
</td></tr>
<tr><td><code id="sobolrank_+3A_environment">environment</code></td>
<td>
<p>[Deprecated] Used prior to tidy evaluation.</p>
</td></tr>
<tr><td><code id="sobolrank_+3A_...">...</code></td>
<td>
<p>any other arguments for <code>model</code> which are passed
unchanged each time it is called.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The estimator used by sobolrank is defined in Gamboa et al. (2020).
It is based on ranking the inputs as was first proposed by Chatterjee (2019) for a Cramer-Von Mises based estimator.
All first-order indices can be estimated with a single sample of size N.
Since boostrap creates ties which are not accounted for in the algorithm, confidence intervals are obtained by 
sampling without replacement with a sample size <code>nsample</code>.
</p>


<h3>Value</h3>

<p><code>sobolrank</code> returns a list of class <code>"sobolrank"</code>, containing all
the input arguments detailed before, plus the following components:
</p>
<table role = "presentation">
<tr><td><code>call</code></td>
<td>
<p>the matched call.</p>
</td></tr>
<tr><td><code>X</code></td>
<td>
<p>a <code>data.frame</code> containing the design of experiments.</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>a vector of model responses.</p>
</td></tr>
<tr><td><code>S</code></td>
<td>
<p>the estimations of the Sobol' sensitivity indices.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Sebastien Da Veiga
</p>


<h3>References</h3>

<p>Gamboa, F., Gremaud, P., Klein, T., &amp; Lagnoux, A., 2022, <em>Global Sensitivity Analysis: 
a novel generation of mighty estimators based on rank statistics</em>, 
Bernoulli 28: 2345-2374.
</p>
<p>Chatterjee, S., 2021, <em>A new coefficient of correlation</em>, Journal of the American 
Statistical Association, 116:2009-2022. 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+sobol">sobol</a>, <a href="#topic+sobol2002">sobol2002</a>, <a href="#topic+sobolSalt">sobolSalt</a>, <a href="#topic+sobol2007">sobol2007</a>, <a href="#topic+soboljansen">soboljansen</a>, <a href="#topic+sobolmartinez">sobolmartinez</a>,
  <a href="#topic+sobolSmthSpl">sobolSmthSpl</a>, <a href="#topic+sobolEff">sobolEff</a>, <a href="#topic+sobolshap_knn">sobolshap_knn</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Test case : the non-monotonic Sobol g-function
# Example with a call to a numerical model
library(boot)
n &lt;- 1000
X &lt;- data.frame(matrix(runif(8 * n), nrow = n))
x &lt;- sobolrank(model = sobol.fun, X = X, nboot = 100)
print(x)
library(ggplot2)
ggplot(x)
# Test case : the Ishigami function
# Example with given data
n &lt;- 500
X &lt;- data.frame(matrix(-pi+2*pi*runif(3 * n), nrow = n))
Y &lt;- ishigami.fun(X)
x &lt;- sobolrank(model = NULL, X)
tell(x,Y)
print(x)
ggplot(x)
</code></pre>

<hr>
<h2 id='sobolrec'>Recursive estimation of Sobol' indices</h2><span id='topic+sobolrec'></span><span id='topic+ask.sobolrec'></span><span id='topic+tell.sobolrec'></span><span id='topic+print.sobolrec'></span><span id='topic+plot.sobolrec'></span>

<h3>Description</h3>

<p><code>sobolrec</code> implements a recursive version of the procedure introduced by Tissot &amp; Prieur (2015) using two replicated nested designs. This function estimates either all first-order indices or all closed second-order indices at a total cost of <code class="reqn">2 \times N</code> model evaluations where <code class="reqn">N</code> is the size of each replicated nested design.</p>


<h3>Usage</h3>

<pre><code class='language-R'>sobolrec(model=NULL, factors, layers, order, precision, method=NULL, tail=TRUE, 
          ...)
## S3 method for class 'sobolrec'
ask(x, index, ...)
## S3 method for class 'sobolrec'
tell(x, y = NULL, index, ...)
## S3 method for class 'sobolrec'
print(x, ...)
## S3 method for class 'sobolrec'
plot(x, ylim = c(0,1), ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sobolrec_+3A_model">model</code></td>
<td>
<p>a function, or a model with a <code>predict</code> method,
defining the model to analyze.</p>
</td></tr>
<tr><td><code id="sobolrec_+3A_factors">factors</code></td>
<td>
<p>an integer giving the number of factors, or a vector of character strings giving their names.</p>
</td></tr>
<tr><td><code id="sobolrec_+3A_layers">layers</code></td>
<td>
<p>If <code>order=1</code>, a vector specifying the respective sizes of each layer (see &quot;Details&quot;). If <code>order=2</code>, an integer specifying the size of all layers.</p>
</td></tr>
<tr><td><code id="sobolrec_+3A_order">order</code></td>
<td>
<p>an integer specifying which indices to estimate: <code>1</code> for first-order indices, <code>2</code> for closed second-order indices.</p>
</td></tr>
<tr><td><code id="sobolrec_+3A_precision">precision</code></td>
<td>
<p>a vector containing:
</p>

<ul>
<li><p> the target precision for the stopping criterion.
</p>
</li>
<li><p> the number of steps for the stopping criterion (must be greater than 1).</p>
</li></ul>
</td></tr>
<tr><td><code id="sobolrec_+3A_tail">tail</code></td>
<td>
<p>a boolean specifying the method used to choose the number of levels of the orthogonal array (see &quot;Warning messages&quot;).</p>
</td></tr>
<tr><td><code id="sobolrec_+3A_method">method</code></td>
<td>
<p>If <code>order=2</code>, a character specifying the method to construct the orthogonal arrays (see &quot;Details&quot;):
</p>

<ul>
<li> <p><code>"al"</code> for the algebraic method 
</p>
</li>
<li> <p><code>"ar"</code> for the accept-reject method
</p>
</li></ul>

<p>Set to <code>NULL</code> if <code>order=1</code>.</p>
</td></tr>
<tr><td><code id="sobolrec_+3A_x">x</code></td>
<td>
<p>a list of class <code>"sobolrec"</code> storing the state of the
sensitivity study (parameters, data, estimates).</p>
</td></tr>
<tr><td><code id="sobolrec_+3A_index">index</code></td>
<td>
<p>an integer specifying the step of the recursion</p>
</td></tr>
<tr><td><code id="sobolrec_+3A_y">y</code></td>
<td>
<p>the model response.</p>
</td></tr>
<tr><td><code id="sobolrec_+3A_ylim">ylim</code></td>
<td>
<p>y-coordinate plotting limits.</p>
</td></tr>
<tr><td><code id="sobolrec_+3A_...">...</code></td>
<td>
<p>any other arguments for <code>model</code> which are passed
unchanged each time it is called.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For first-order indices, <code>layers</code> is a vector:
</p>
<p style="text-align: center;"><code class="reqn">\left(s_1, ...,s_m \right)</code>
</p>

<p>specifying the number <code class="reqn">m</code> of layers of the nested design whose respective size are given by: 
</p>
<p style="text-align: center;"><code class="reqn">\prod_{i=1}^{k-1} s_i, \ k=2, ...,m+1</code>
</p>

<p>For closed second-order indices, <code>layers</code> directly specifies the size of all layers.
</p>
<p>For each Sobol' index <code class="reqn">S</code> the stopping criterion writes:
</p>
<p style="text-align: center;"><code class="reqn">\mid S_{l-1}-S_{l} \mid &lt; \epsilon</code>
</p>

<p>This criterion is tested for the last <code class="reqn">l_0</code> steps (including the current one). <code class="reqn">\epsilon</code> and <code class="reqn">l_0</code> are respectively the target precision and the number of steps of the stopping criterion specified in <code>precision</code>.
</p>
<p><code>sobolrec</code> uses either an algebraic or an accept-rejet <code>method</code> to construct the orthogonal arrays for the estimation of closed second-order indices. The algebraic method is less precise than the accept-reject method but offers more steps when the number of <code>factors</code> is small.
</p>
<p><code>sobolrec</code> automatically assigns a uniform distribution on [0,1] to each input. Transformations of distributions (between U[0,1] and the wanted distribution) have to be performed before the call to tell().
</p>


<h3>Value</h3>

<p><code>sobolrec</code> returns a list of class <code>"sobolrec"</code>, containing all
the input arguments detailed before, plus the following components:
</p>
<table role = "presentation">
<tr><td><code>call</code></td>
<td>
<p>the matched call.</p>
</td></tr>
<tr><td><code>X</code></td>
<td>
<p>a <code>data.frame</code> containing the design of experiments (row concatenation of the two replicated designs).</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>a list of the response used at each step.</p>
</td></tr>
<tr><td><code>V</code></td>
<td>
<p>a list of the model variance estimated at each step.</p>
</td></tr>
<tr><td><code>S</code></td>
<td>
<p>a list of the Sobol' indices estimated at each step.</p>
</td></tr>
<tr><td><code>steps</code></td>
<td>
<p>the number of steps performed.</p>
</td></tr>
<tr><td><code>N</code></td>
<td>
<p>the size of each replicated nested design.</p>
</td></tr>
</table>


<h3>Warning messages</h3>


<dl>
<dt>&quot;The value entered for <code>layers</code> is not the square of a prime number. It has been replaced by: &quot;</dt><dd><p>When <code>order=2</code>, the value of <code>layers</code> must be the square of a prime power number.  This warning message indicates that it was not the case and the value has been replaced depending on <code>tail</code>. If <code>tail=TRUE</code> (resp. <code>tail=FALSE</code>) the new value of <code>layers</code> is equal to the square of the prime number preceding (resp. following) the square root of <code>layers</code>.</p>
</dd>
<dt>&quot;The value entered for <code>layers</code> is not satisfying the constraint. It has been replaced by: &quot;</dt><dd><p>the value <code class="reqn">N</code> for <code>layers</code> must satisfied the constraint <code class="reqn">N \geq (d-1)^{2}</code> where <code class="reqn">d</code> is the number of factors. This warning message indicates that <code>N</code> was replaced by the square of the prime number following (or equals to) <code class="reqn">d-1</code>.</p>
</dd></dl>



<h3>References</h3>

<p>A.S. Hedayat, N.J.A. Sloane and J. Stufken, 1999, <em>Orthogonal Arrays: Theory and Applications</em>, Springer Series in Statistics.
</p>
<p>L. Gilquin, E. Arnaud, H. Monod and C. Prieur, 2021, <em>Recursive estimation procedure of Sobol' indices based on replicated designs</em>, 
Computational and Applied Mathematics, 40:1&ndash;23. 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>	
# Test case: the non-monotonic Sobol g-function

# The method of sobol requires 2 samples
# (there are 8 factors, all following the uniform distribution on [0,1])

# first-order indices estimation
x &lt;- sobolrec(model = sobol.fun, factors = 8, layers=rep(2,each=15), order=1,
              precision = c(5*10^(-2),2), method=NULL, tail=TRUE)
print(x)

# closed second-order indices estimation
x &lt;- sobolrec(model = sobol.fun, factors = 8, layers=11^2, order=2,
              precision = c(10^(-2),3), method="al", tail=TRUE)
print(x)


# Test case: dealing with external model 
# put in comment because of bug with ask use !

#x &lt;- sobolrec(model = NULL, factors = 8, layers=rep(2,each=15), order=1,
#              precision = c(5*10^(-2),2), method=NULL, tail=TRUE)
#toy &lt;- sobol.fun
#k &lt;- 1
#stop_crit &lt;- FALSE
#while(!(stop_crit) &amp; (k&lt;length(x$layers))){
#  ask(x, index=k)
#  y &lt;- toy(x$block)
#  tell(x, y, index=k)
#  stop_crit &lt;- x$stop_crit
#  k &lt;- k+1
#}
#print(x)


</code></pre>

<hr>
<h2 id='sobolrep'>Sobol' indices estimation based on replicated orthogonal arrays</h2><span id='topic+sobolrep'></span><span id='topic+tell.sobolrep'></span><span id='topic+print.sobolrep'></span><span id='topic+plot.sobolrep'></span>

<h3>Description</h3>

<p><code>sobolrep</code> generalizes the estimation of the Sobol' sensitivity indices introduced by Tissot &amp; Prieur (2015) using two replicated orthogonal arrays. This function estimates either 
</p>

<ul>
<li><p> all first-order and second-order indices at a total cost of <code class="reqn">2 \times N</code> model evaluations,
</p>
</li>
<li><p> or all first-order, second-order and total-effect indices at a total cost of <code class="reqn">N \times (d+2)</code> model evaluations,</p>
</li></ul>

<p>where <code class="reqn">N=q^{2}</code> and <code class="reqn">q \geq d-1</code> is a prime number corresponding to the number of levels of each orthogonal array.</p>


<h3>Usage</h3>

<pre><code class='language-R'>sobolrep(model = NULL, factors, N, tail=TRUE, 
			conf=0.95, nboot=0, nbrep=1, total=FALSE, ...)
## S3 method for class 'sobolrep'
tell(x, y = NULL, ...)
## S3 method for class 'sobolrep'
print(x, ...)
## S3 method for class 'sobolrep'
plot(x, ylim = c(0,1), choice, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sobolrep_+3A_model">model</code></td>
<td>
<p>a function, or a model with a <code>predict</code> method,
defining the model to analyze.</p>
</td></tr>
<tr><td><code id="sobolrep_+3A_factors">factors</code></td>
<td>
<p>an integer giving the number of factors, or a vector of character strings giving their names.</p>
</td></tr>
<tr><td><code id="sobolrep_+3A_n">N</code></td>
<td>
<p>an integer giving the size of each replicated design (for a total of <code class="reqn">2 \times N</code> model evaluations).</p>
</td></tr>
<tr><td><code id="sobolrep_+3A_tail">tail</code></td>
<td>
<p>a boolean specifying the method used to choose the number of levels of the orthogonal array (see &quot;Warning messages&quot;).</p>
</td></tr>
<tr><td><code id="sobolrep_+3A_conf">conf</code></td>
<td>
<p>the confidence level for confidence intervals.</p>
</td></tr>
<tr><td><code id="sobolrep_+3A_nboot">nboot</code></td>
<td>
<p>the number of bootstrap replicates.</p>
</td></tr>
<tr><td><code id="sobolrep_+3A_nbrep">nbrep</code></td>
<td>
<p>the number of times the estimation procedure is repeated (see &quot;Details&quot;).</p>
</td></tr>
<tr><td><code id="sobolrep_+3A_total">total</code></td>
<td>
<p>a boolean specifying whether or not total effect indices are estimated.</p>
</td></tr>
<tr><td><code id="sobolrep_+3A_x">x</code></td>
<td>
<p>a list of class <code>"sobolrep"</code> storing the state of the
sensitivity study (parameters, data, estimates).</p>
</td></tr>
<tr><td><code id="sobolrep_+3A_y">y</code></td>
<td>
<p>the model response.</p>
</td></tr>
<tr><td><code id="sobolrep_+3A_ylim">ylim</code></td>
<td>
<p>y-coordinate plotting limits.</p>
</td></tr>
<tr><td><code id="sobolrep_+3A_choice">choice</code></td>
<td>
<p>an integer specifying which indices to plot: <code>1</code> for first-order indices, <code>2</code> for second-order indices, <code>3</code> for total-effect indices.</p>
</td></tr>
<tr><td><code id="sobolrep_+3A_...">...</code></td>
<td>
<p>any other arguments for <code>model</code> which are passed
unchanged each time it is called.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>sobolrep</code> automatically assigns a uniform distribution on [0,1] to each input. Transformations of distributions (between U[0,1] and the wanted distribution) have to be performed before the call to tell() (see &quot;Examples&quot;).
</p>
<p><code>nbrep</code> specifies the number of times the estimation procedure is repeated. Each repetition makes use of the orthogonal array structure to obtain a new set of Sobol' indices. It is important to note that no additional model evaluations are performed (the cost of the procedure remains the same).
</p>


<h3>Value</h3>

<p><code>sobolrep</code> returns a list of class <code>"sobolrep"</code>, containing all
the input arguments detailed before, plus the following components:
</p>
<table role = "presentation">
<tr><td><code>call</code></td>
<td>
<p>the matched call.</p>
</td></tr>
<tr><td><code>X</code></td>
<td>
<p>a <code>data.frame</code> containing the design of experiments (row concatenation of the two replicated designs).</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>the response used.</p>
</td></tr>
<tr><td><code>RP</code></td>
<td>
<p>the matrix of permutations.</p>
</td></tr>
<tr><td><code>V</code></td>
<td>
<p>the model variance.</p>
</td></tr>
<tr><td><code>S</code></td>
<td>
<p>a data.frame containing estimations of the first-order Sobol' indices plus confidence intervals if specified.</p>
</td></tr>
<tr><td><code>S2</code></td>
<td>
<p>a data.frame containing estimations of the second-order Sobol' indices plus confidence intervals if specified.</p>
</td></tr>
<tr><td><code>T</code></td>
<td>
<p>a data.frame containing estimations of the total-effect indices plus confidence intervals if specified.</p>
</td></tr>
</table>


<h3>Warning messages</h3>


<dl>
<dt>&quot;The value entered for <code>N</code> is not the square of a prime number. It has been replaced by: &quot;</dt><dd><p>the number of levels <code>q</code> of each orthogonal array must be a prime number. If <code>N</code> is not a square of a prime number, then this warning message indicates that it was replaced depending on the value of <code>tail</code>. If <code>tail=TRUE</code> (resp. <code>tail=FALSE</code>) the new value of <code>N</code> is equal to the square of the prime number preceding (resp. following) the square root of <code>N</code>.</p>
</dd>
<dt>&quot;The value entered for <code>N</code> is not satisfying the constraint <code class="reqn">N \geq (d-1)^2</code>. It has been replaced by: &quot;</dt><dd><p>the following constraint must be satisfied <code class="reqn">N \geq (d-1)^{2}</code> where <code class="reqn">d</code> is the number of factors. This warning message indicates that <code>N</code> was replaced by the square of the prime number following (or equals to) <code class="reqn">d-1</code>.</p>
</dd></dl>



<h3>References</h3>

<p>A.S. Hedayat, N.J.A. Sloane and J. Stufken, 1999, <em>Orthogonal Arrays: Theory and Applications</em>, Springer Series in Statistics.
</p>
<p>J-Y. Tissot and C. Prieur, 2015, <em>A randomized orthogonal orray-based procedure for the estimation of first- and second-order Sobol' indices</em>, J. Statist. Comput. Simulation, 85:1358-1381.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Test case: the non-monotonic Sobol g-function

# The method of sobol requires 2 samples
# (there are 8 factors, all following the uniform distribution on [0,1])

x &lt;- sobolrep(model = sobol.fun, factors = 8, N = 1000, nboot=100, nbrep=1, total=FALSE)
print(x)
plot(x,choice=1)
plot(x,choice=2)

# Test case: dealing with non-uniform distributions

x &lt;- sobolrep(model = NULL, factors = 3, N = 1000, nboot=0, nbrep=1, total=FALSE)

# X1 follows a log-normal distribution:
x$X[,1] &lt;- qlnorm(x$X[,1])

# X2 follows a standard normal distribution:
x$X[,2] &lt;- qnorm(x$X[,2])

# X3 follows a gamma distribution:
x$X[,3] &lt;- qgamma(x$X[,3],shape=0.5)

# toy example
toy &lt;- function(x){rowSums(x)}
y &lt;- toy(x$X)
tell(x, y)
print(x)
plot(x,choice=1)
plot(x,choice=2)
</code></pre>

<hr>
<h2 id='sobolroalhs'>Sobol' Indices Estimation Using Replicated OA-based LHS</h2><span id='topic+sobolroalhs'></span><span id='topic+tell.sobolroalhs'></span><span id='topic+print.sobolroalhs'></span><span id='topic+plot.sobolroalhs'></span><span id='topic+ggplot.sobolroalhs'></span>

<h3>Description</h3>

<p><code>sobolroalhs</code> implements the estimation of the Sobol' sensitivity indices introduced by Tissot &amp; Prieur (2015) using two replicated designs (Latin hypercubes or orthogonal arrays). This function estimates either all first-order indices or all closed second-order indices at a total cost of <code class="reqn">2 \times N</code> model evaluations. For closed second-order indices <code class="reqn">N=q^{2}</code> where <code class="reqn">q \geq d-1</code> is a prime number corresponding to the number of levels of the orthogonal array, and where <code class="reqn">d</code> indicates the number of factors.</p>


<h3>Usage</h3>

<pre><code class='language-R'>sobolroalhs(model = NULL, factors, N, p=1, order, tail=TRUE, conf=0.95, nboot=0, ...)
## S3 method for class 'sobolroalhs'
tell(x, y = NULL, ...)
## S3 method for class 'sobolroalhs'
print(x, ...)
## S3 method for class 'sobolroalhs'
plot(x, ylim = c(0,1), ...)
## S3 method for class 'sobolroalhs'
ggplot(data, mapping = aes(), ylim = c(0, 1), ..., environment
                 = parent.frame())
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sobolroalhs_+3A_model">model</code></td>
<td>
<p>a function, or a model with a <code>predict</code> method,
defining the model to analyze.</p>
</td></tr>
<tr><td><code id="sobolroalhs_+3A_factors">factors</code></td>
<td>
<p>an integer giving the number of factors, or a vector of character strings giving their names.</p>
</td></tr>
<tr><td><code id="sobolroalhs_+3A_n">N</code></td>
<td>
<p>an integer giving the size of each replicated design (for a total of <code class="reqn">2 \times N</code> model evaluations).</p>
</td></tr>
<tr><td><code id="sobolroalhs_+3A_p">p</code></td>
<td>
<p>an integer giving the number of model outputs.</p>
</td></tr>
<tr><td><code id="sobolroalhs_+3A_order">order</code></td>
<td>
<p>an integer giving the order of the indices (1 or 2).</p>
</td></tr>
<tr><td><code id="sobolroalhs_+3A_tail">tail</code></td>
<td>
<p>a boolean specifying the method used to choose the number of levels of the orthogonal array (see &quot;Warning messages&quot;).</p>
</td></tr>
<tr><td><code id="sobolroalhs_+3A_conf">conf</code></td>
<td>
<p>the confidence level for confidence intervals.</p>
</td></tr>
<tr><td><code id="sobolroalhs_+3A_nboot">nboot</code></td>
<td>
<p>the number of bootstrap replicates.</p>
</td></tr>
<tr><td><code id="sobolroalhs_+3A_x">x</code></td>
<td>
<p>a list of class <code>"sobolroalhs"</code> storing the state of the
sensitivity study (parameters, data, estimates).</p>
</td></tr>
<tr><td><code id="sobolroalhs_+3A_data">data</code></td>
<td>
<p>a list of class <code>"sobolroalhs"</code> storing the state of the
sensitivity study (parameters, data, estimates).</p>
</td></tr>
<tr><td><code id="sobolroalhs_+3A_y">y</code></td>
<td>
<p>a vector of model responses.</p>
</td></tr>
<tr><td><code id="sobolroalhs_+3A_ylim">ylim</code></td>
<td>
<p>y-coordinate plotting limits.</p>
</td></tr>
<tr><td><code id="sobolroalhs_+3A_mapping">mapping</code></td>
<td>
<p>Default list of aesthetic mappings to use for plot. If not specified, 
must be supplied in each layer added to the plot.</p>
</td></tr>
<tr><td><code id="sobolroalhs_+3A_environment">environment</code></td>
<td>
<p>[Deprecated] Used prior to tidy evaluation.</p>
</td></tr>
<tr><td><code id="sobolroalhs_+3A_...">...</code></td>
<td>
<p>any other arguments for <code>model</code> which are passed
unchanged each time it is called.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>sobolroalhs</code> automatically assigns a uniform distribution on [0,1] to each input. Transformations of distributions (between U[0,1] and the wanted distribution) have to be realized before the call to tell() (see &quot;Examples&quot;).
</p>
<p>Missing values (i.e <code>NA</code> values) in outputs are automatically handled by the function.
</p>
<p>This function also supports multidimensional outputs (matrices in <code>y</code> or as output of <code>model</code>). In this case, aggregated Sobol' indices are returned (<code>see sobolMultOut</code>).
</p>


<h3>Value</h3>

<p><code>sobolroalhs</code> returns a list of class <code>"sobolroalhs"</code>, containing all
the input arguments detailed before, plus the following components:
</p>
<table role = "presentation">
<tr><td><code>call</code></td>
<td>
<p>the matched call.</p>
</td></tr>
<tr><td><code>X</code></td>
<td>
<p>a <code>data.frame</code> containing the design of experiments (row concatenation of the two replicated designs).</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>the responses used.</p>
</td></tr>
<tr><td><code>OA</code></td>
<td>
<p>the orthogonal array constructed (<code>NULL</code> if <code>order</code>=1).</p>
</td></tr>
<tr><td><code>V</code></td>
<td>
<p>the estimations of Variances of the Conditional Expectations (VCE) with respect to each factor.</p>
</td></tr>
<tr><td><code>S</code></td>
<td>
<p>the estimations of the Sobol' indices.</p>
</td></tr>
</table>


<h3>Warning messages</h3>


<dl>
<dt>&quot;The value entered for <code>N</code> is not the square of a prime number. It has been replaced by: &quot;</dt><dd><p>when <code>order</code><code class="reqn">=2</code>, the number of levels of the orthogonal array must be a prime number. If <code>N</code> is not a square of a prime number, then this warning message indicates that it was replaced depending on the value of <code>tail</code>. If <code>tail=TRUE</code> (resp. <code>tail=FALSE</code>) the new value of <code>N</code> is equal to the square of the prime number preceding (resp. following) the square root of <code>N</code>.</p>
</dd>
<dt>&quot;The value entered for <code>N</code> is not satisfying the constraint <code class="reqn">N \geq (d-1)^2</code>. It has been replaced by: &quot;</dt><dd><p>when <code>order</code><code class="reqn">=2</code>, the following constraint must be satisfied <code class="reqn">N \geq (d-1)^{2}</code> where <code class="reqn">d</code> is the number of factors. This warning message indicates that <code>N</code> was replaced by the square of the prime number following (or equals to) <code class="reqn">d-1</code>.</p>
</dd></dl>



<h3>Author(s)</h3>

<p>Laurent Gilquin
</p>


<h3>References</h3>

<p>A.S. Hedayat, N.J.A. Sloane and J. Stufken, 1999, <em>Orthogonal Arrays: Theory and Applications</em>, Springer Series in Statistics.
</p>
<p>F. Gamboa, A. Janon, T. Klein and A. Lagnoux, 2014, <em>Sensitivity indices for multivariate outputs</em>, Electronic Journal of Statistics, 8:575-603.
</p>
<p>J.Y. Tissot and C. Prieur, 2015, <em>A randomized orthogonal orray-based procedure for the estimation of first- and second-order Sobol' indices</em>, J. Statist. Comput. Simulation, 85:1358-1381.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+sobolmara">sobolmara</a></code>,
<code><a href="#topic+sobolroauc">sobolroauc</a></code>,
<code><a href="#topic+sobolMultOut">sobolMultOut</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(boot)
library(numbers)

####################
# Test case: the non-monotonic Sobol g-function

# The method of sobol requires 2 samples
# (there are 8 factors, all following the uniform distribution on [0,1])

# first-order sensitivity indices
x &lt;- sobolroalhs(model = sobol.fun, factors = 8, N = 1000, order = 1, nboot=100)
print(x)
plot(x)

library(ggplot2)
ggplot(x)

# closed second-order sensitivity indices
x &lt;- sobolroalhs(model = sobol.fun, factors = 8, N = 1000, order = 2, nboot=100)
print(x)
ggplot(x)

####################
# Test case: dealing with non-uniform distributions

x &lt;- sobolroalhs(model = NULL, factors = 3, N = 1000, order =1, nboot=0)

# X1 follows a log-normal distribution:
x$X[,1] &lt;- qlnorm(x$X[,1])

# X2 follows a standard normal distribution:
x$X[,2] &lt;- qnorm(x$X[,2])

# X3 follows a gamma distribution:
x$X[,3] &lt;- qgamma(x$X[,3],shape=0.5)

# toy example
toy &lt;- function(x){rowSums(x)}
y &lt;- toy(x$X)
tell(x, y)
print(x)
ggplot(x)

####################
# Test case : multidimensional outputs


toy &lt;- function(x){cbind(x[,1]+x[,2]+x[,1]*x[,2],2*x[,1]+3*x[,1]*x[,2]+x[,2])}
x &lt;- sobolroalhs(model = toy, factors = 3, N = 1000, p=2, order =1, nboot=100)
print(x)
ggplot(x)


</code></pre>

<hr>
<h2 id='sobolroauc'>Sobol' Indices estimation under inequality constraints</h2><span id='topic+sobolroauc'></span><span id='topic+tell.sobolroauc'></span><span id='topic+print.sobolroauc'></span><span id='topic+plot.sobolroauc'></span><span id='topic+ggplot.sobolroauc'></span>

<h3>Description</h3>

<p><code>sobolroauc</code> deals with the estimation of Sobol' sensitivity indices when there exists one or multiple sets of constrained factors. Constraints within a set are expressed as inequality constraints (simplex constraint). This function generalizes the procedure of Tissot and Prieur (2015) to estimate either all first-order indices or all closed second-order indices at a total cost of <code class="reqn">2 \times N</code> model evaluations. For closed second-order indices <code class="reqn">N=q^{2}</code> where <code class="reqn">q \geq d-1</code> is a prime number denoting the number of levels of the orthogonal array, and where <code class="reqn">d</code> indicates the number of independent factors or sets of factors.</p>


<h3>Usage</h3>

<pre><code class='language-R'>sobolroauc(model = NULL, factors, constraints = NULL, N, p = 1, order, 
            tail = TRUE, conf = 0.95, nboot = 0, ...)
## S3 method for class 'sobolroauc'
tell(x, y = NULL, ...)
## S3 method for class 'sobolroauc'
print(x, ...)
## S3 method for class 'sobolroauc'
plot(x, ylim = c(0,1), ...)
## S3 method for class 'sobolroauc'
ggplot(data, mapping = aes(), ylim = c(0, 1), ..., environment
                 = parent.frame())
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sobolroauc_+3A_model">model</code></td>
<td>
<p>a function, or a model with a <code>predict</code> method,
defining the model to analyze.</p>
</td></tr>
<tr><td><code id="sobolroauc_+3A_factors">factors</code></td>
<td>
<p>an integer giving the number of factors, or a vector of character strings giving their names.</p>
</td></tr>
<tr><td><code id="sobolroauc_+3A_constraints">constraints</code></td>
<td>
<p>a list giving the sets of constrained factors (see &quot;Details&quot;).</p>
</td></tr>
<tr><td><code id="sobolroauc_+3A_n">N</code></td>
<td>
<p>an integer giving the size of each replicated design (for a total of <code class="reqn">2 \times N</code> model evaluations).</p>
</td></tr>
<tr><td><code id="sobolroauc_+3A_p">p</code></td>
<td>
<p>an integer giving the number of model outputs.</p>
</td></tr>
<tr><td><code id="sobolroauc_+3A_order">order</code></td>
<td>
<p>an integer giving the order of the indices (1 or 2).</p>
</td></tr>
<tr><td><code id="sobolroauc_+3A_tail">tail</code></td>
<td>
<p>a boolean specifying the method used to choose the number of levels of the orthogonal array (see &quot;Warning messages&quot;).</p>
</td></tr>
<tr><td><code id="sobolroauc_+3A_conf">conf</code></td>
<td>
<p>the confidence level for confidence intervals.</p>
</td></tr>
<tr><td><code id="sobolroauc_+3A_nboot">nboot</code></td>
<td>
<p>the number of bootstrap replicates.</p>
</td></tr>
<tr><td><code id="sobolroauc_+3A_x">x</code></td>
<td>
<p>a list of class <code>"sobolroauc"</code> storing the state of the
sensitivity study (parameters, data, estimates).</p>
</td></tr>
<tr><td><code id="sobolroauc_+3A_data">data</code></td>
<td>
<p>a list of class <code>"sobolroauc"</code> storing the state of the
sensitivity study (parameters, data, estimates).</p>
</td></tr>
<tr><td><code id="sobolroauc_+3A_y">y</code></td>
<td>
<p>a vector of model responses.</p>
</td></tr>
<tr><td><code id="sobolroauc_+3A_ylim">ylim</code></td>
<td>
<p>y-coordinate plotting limits.</p>
</td></tr>
<tr><td><code id="sobolroauc_+3A_mapping">mapping</code></td>
<td>
<p>Default list of aesthetic mappings to use for plot. If not specified, 
must be supplied in each layer added to the plot.</p>
</td></tr>
<tr><td><code id="sobolroauc_+3A_environment">environment</code></td>
<td>
<p>[Deprecated] Used prior to tidy evaluation.</p>
</td></tr>
<tr><td><code id="sobolroauc_+3A_...">...</code></td>
<td>
<p>any other arguments for <code>model</code> which are passed
unchanged each time it is called.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>constraints</code> list the sets of factors depending on each other through inequality constraints (see &quot;Examples&quot;). A same factor is not allowed to appear in multiple sets. Factors not appearing in <code>constraints</code> are assumed to be independent and follow each a uniform distribution on [0,1]. One Sobol' index is estimated for each independent factor or set of factors.
</p>
<p>Missing values (i.e <code>NA</code> values) in the model responses are automatically handled by the function.
</p>
<p>This function also supports multidimensional outputs (matrices in <code>y</code> or as output of <code>model</code>).
In this case, aggregated Sobol' indices are returned (<code>see sobolMultOut</code>).
</p>


<h3>Value</h3>

<p><code>sobolroauc</code> returns a list of class <code>"sobolroauc"</code>, containing all
the input arguments detailed before, plus the following components:
</p>
<table role = "presentation">
<tr><td><code>call</code></td>
<td>
<p>the matched call.</p>
</td></tr>
<tr><td><code>X</code></td>
<td>
<p>a <code>data.frame</code> containing the design of experiments (concatenation of two replicated designs).</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>the responses used.</p>
</td></tr>
<tr><td><code>OA</code></td>
<td>
<p>the orthogonal array constructed (<code>NULL</code> if <code>order</code>=1).</p>
</td></tr>
<tr><td><code>V</code></td>
<td>
<p>the estimations of Variances of the Conditional Expectations (VCE) with respect to each factor.</p>
</td></tr>
<tr><td><code>S</code></td>
<td>
<p>the estimations of the Sobol' indices.</p>
</td></tr>
</table>


<h3>Warning messages</h3>


<dl>
<dt>&quot;The value entered for <code>N</code> is not the square of a prime number. It has been replaced by: &quot;</dt><dd><p>when <code>order</code><code class="reqn">=2</code>, the number of levels of the orthogonal array must be a prime number. If <code>N</code> is not a square of a prime number, then this warning message indicates that it was replaced depending on the value of <code>tail</code>. If <code>tail=TRUE</code> (resp. <code>tail=FALSE</code>) the new value of <code>N</code> is equal to the square of the prime number preceding (resp. following) the square root of <code>N</code>.</p>
</dd>
<dt>&quot;The value entered for <code>N</code> is not satisfying the constraint <code class="reqn">N \geq (d-1)^2</code>. It has been replaced by: &quot;</dt><dd><p>when <code>order</code><code class="reqn">=2</code>, the following constraint must be satisfied <code class="reqn">N \geq (d-1)^{2}</code> where <code class="reqn">d</code> is the number of independent factors or sets of factors. This warning message indicates that <code>N</code> was replaced by the square of the prime number following (or equals to) <code class="reqn">d-1</code>.</p>
</dd></dl>



<h3>Author(s)</h3>

<p>Laurent Gilquin
</p>


<h3>References</h3>

<p>L. Devroye, 1986, Non-Uniform Random Variate Generation. Springer-Verlag.
</p>
<p>J. Jacques, C. Lavergne and N. Devictor, 2006, Sensitivity Analysis in presence of model
uncertainty and correlated inputs. Reliability Engineering &amp; System Safety, 91:1126-1134.
</p>
<p>L. Gilquin, C. Prieur and E. Arnaud, 2015, <em>Replication procedure for grouped Sobol' indices estimation in dependent uncertainty spaces</em>, Information and Inference, 4:354-379.
</p>
<p>J.Y. Tissot and C. Prieur, 2015, <em>A randomized orthogonal orray-based procedure for the estimation of first- and second-order Sobol' indices</em>, J. Statist. Comput. Simulation, 85:1358-1381.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+sobolroalhs">sobolroalhs</a></code>, 
<code><a href="#topic+sobolmara">sobolmara</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(boot)
library(numbers)

# Test case: the non-monotonic Sobol g-function
# (there are 8 factors, all following the uniform distribution on [0,1])

# Suppose we have the inequality constraints: X1 &lt;= X3 and X4 &lt;= X6.

# first-order sensitivity indices
x &lt;- sobolroauc(model = sobol.fun, factors = 8, constraints = list(c(1,3),c(4,6)), 
                N = 1000, order = 1, nboot=100)
print(x)
plot(x)

library(ggplot2)
ggplot(x)

# closed second-order sensitivity indices
x &lt;- sobolroauc(model = sobol.fun, factors = 8, constraints = list(c(1,3),c(4,6)), 
                N = 1000, order = 2, nboot=100)
print(x)
ggplot(x)

</code></pre>

<hr>
<h2 id='sobolSalt'>Monte Carlo Estimation of Sobol' Indices based on Saltelli schemes</h2><span id='topic+sobolSalt'></span><span id='topic+tell.sobolSalt'></span><span id='topic+print.sobolSalt'></span><span id='topic+plot.sobolSalt'></span><span id='topic+ggplot.sobolSalt'></span>

<h3>Description</h3>

<p><code>sobolSalt</code> implements the Monte Carlo estimation of
the Sobol' indices for either both first-order and total effect indices at the same
time (alltogether <code class="reqn">2p</code> indices) at a total cost of <code class="reqn">n\times(p+2)</code> model evaluations; or first-order, second-order and total indices at the same time (alltogether <code class="reqn">2p+ p\times(p-1)/2</code> indices) at a total cost of <code class="reqn">n\times(2\times p+2)</code> model evaluations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sobolSalt(model = NULL, X1, X2, scheme="A", nboot = 0, conf = 0.95, ...)
## S3 method for class 'sobolSalt'
tell(x, y = NULL, ...)
## S3 method for class 'sobolSalt'
print(x, ...)
## S3 method for class 'sobolSalt'
plot(x, ylim = c(0, 1), choice, ...)
## S3 method for class 'sobolSalt'
ggplot(data, mapping = aes(), ylim = c(0, 1), choice, ..., environment
                 = parent.frame())
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sobolSalt_+3A_model">model</code></td>
<td>
<p>a function, or a model with a <code>predict</code> method,
defining the model to analyze.</p>
</td></tr>
<tr><td><code id="sobolSalt_+3A_x1">X1</code></td>
<td>
<p>the first random sample (containing <code>n</code> points).</p>
</td></tr>
<tr><td><code id="sobolSalt_+3A_x2">X2</code></td>
<td>
<p>the second random sample (containing <code>n</code> points).</p>
</td></tr>
<tr><td><code id="sobolSalt_+3A_scheme">scheme</code></td>
<td>
<p>a letter <code>"A"</code> or <code>"B"</code> indicating which scheme to use (see &quot;Details&quot;)</p>
</td></tr>
<tr><td><code id="sobolSalt_+3A_nboot">nboot</code></td>
<td>
<p>the number of bootstrap replicates.</p>
</td></tr>
<tr><td><code id="sobolSalt_+3A_conf">conf</code></td>
<td>
<p>the confidence level for bootstrap confidence intervals.</p>
</td></tr>
<tr><td><code id="sobolSalt_+3A_x">x</code></td>
<td>
<p>a list of class <code>"sobolSalt"</code> storing the state of the
sensitivity study (parameters, data, estimates).</p>
</td></tr>
<tr><td><code id="sobolSalt_+3A_data">data</code></td>
<td>
<p>a list of class <code>"sobolSalt"</code> storing the state of the
sensitivity study (parameters, data, estimates).</p>
</td></tr>
<tr><td><code id="sobolSalt_+3A_y">y</code></td>
<td>
<p>a vector of model responses.</p>
</td></tr>
<tr><td><code id="sobolSalt_+3A_ylim">ylim</code></td>
<td>
<p>y-coordinate plotting limits.</p>
</td></tr>
<tr><td><code id="sobolSalt_+3A_choice">choice</code></td>
<td>
<p>an integer specifying which indices to plot: <code>1</code> for first-order and total effect indices, <code>2</code> for second-order indices.</p>
</td></tr>
<tr><td><code id="sobolSalt_+3A_mapping">mapping</code></td>
<td>
<p>Default list of aesthetic mappings to use for plot. If not specified, 
must be supplied in each layer added to the plot.</p>
</td></tr>
<tr><td><code id="sobolSalt_+3A_environment">environment</code></td>
<td>
<p>[Deprecated] Used prior to tidy evaluation.</p>
</td></tr>
<tr><td><code id="sobolSalt_+3A_...">...</code></td>
<td>
<p>any other arguments for <code>model</code> which are passed
unchanged each time it is called</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The estimators used are the one implemented in <code>"sobolEff"</code>.
</p>
<p><code>scheme</code> specifies which Saltelli's scheme is to be used: <code>"A"</code> to estimate both first-order and total effect indices, <code>"B"</code> to estimate first-order, second-order and total effect indices.
</p>


<h3>Value</h3>

<p><code>sobolSalt</code> returns a list of class <code>"sobolSalt"</code>, containing all
the input arguments detailed before, plus the following components:
</p>
<table role = "presentation">
<tr><td><code>call</code></td>
<td>
<p>the matched call.</p>
</td></tr>
<tr><td><code>X</code></td>
<td>
<p>a <code>data.frame</code> containing the design of experiments.</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>the response used.</p>
</td></tr>
<tr><td><code>V</code></td>
<td>
<p>the model variance.</p>
</td></tr>
<tr><td><code>S</code></td>
<td>
<p>the estimations of the Sobol' first-order indices.</p>
</td></tr>
<tr><td><code>S2</code></td>
<td>
<p>the estimations of the Sobol' second-order indices (only for scheme <code>"B"</code>).</p>
</td></tr>
<tr><td><code>T</code></td>
<td>
<p>the estimations of the Sobol' total sensitivity indices.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Laurent Gilquin
</p>


<h3>References</h3>

<p>A. Janon, T. Klein, A. Lagnoux, M. Nodet, C. Prieur (2014), <em>Asymptotic normality and efficiency of two Sobol index estimators</em>, ESAIM: Probability and Statistics, 18:342-364.
</p>
<p>A. Saltelli, 2002, <em>Making best use of model evaluations to compute
sensitivity indices</em>, Computer Physics Communication, 145:580-297.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+sobol">sobol</a>, <a href="#topic+sobol2007">sobol2007</a>, <a href="#topic+soboljansen">soboljansen</a>, <a href="#topic+sobolmartinez">sobolmartinez</a>, <a href="#topic+sobolEff">sobolEff</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Test case : the non-monotonic Sobol g-function

# The method of sobol requires 2 samples
# There are 8 factors, all following the uniform distribution
# on [0,1]

library(boot)
n &lt;- 1000
X1 &lt;- data.frame(matrix(runif(8 * n), nrow = n))
X2 &lt;- data.frame(matrix(runif(8 * n), nrow = n))

# sensitivity analysis

x &lt;- sobolSalt(model = sobol.fun, X1, X2, scheme="A", nboot = 100)
print(x)
plot(x, choice=1)

library(ggplot2)
ggplot(x, choice=1)
</code></pre>

<hr>
<h2 id='sobolshap_knn'>Flexible sensitivity analysis via ranking / nearest neighbours</h2><span id='topic+sobolshap_knn'></span><span id='topic+tell.sobolshap_knn'></span><span id='topic+extract.sobolshap_knn'></span><span id='topic+print.sobolshap_knn'></span><span id='topic+plot.sobolshap_knn'></span><span id='topic+ggplot.sobolshap_knn'></span>

<h3>Description</h3>

<p>WARNING: DEPRECATED function: use <code>shapleysobol_knn</code> instead.
<code>sobolshap_knn</code> implements the estimation of several sensitivity indices using 
only N model evaluations via ranking (following Gamboa et al. (2020) and Chatterjee (2019)) 
or nearest neighbour search (Broto et al. (2020) and Azadkia &amp; Chatterjee (2020)). 
It can be used with categorical inputs (which are transformed with one-hot encoding), 
dependent inputs and multiple outputs. Sensitivity indices of any group of inputs can be computed,
which means that in particular first-order/total Sobol indices and Shapley effects are accessible. 
For large sample sizes, the nearest neightbour algorithm can be significantly accelerated 
by using approximate nearest neighbour search. It is also possible to estimate Shapley effects 
with the random permutation approach of Castro et al.(2009), where all the terms are obtained 
with ranking or nearest neighbours.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  sobolshap_knn(model = NULL, X, id.cat = NULL, U = NULL, method = "knn", 
                n.knn = 2, return.shap = FALSE, randperm = FALSE, n.perm = 1e4, 
                rescale = FALSE, n.limit = 2000, noise = FALSE, ...)
  ## S3 method for class 'sobolshap_knn'
tell(x, y = NULL, ...)
  ## S3 method for class 'sobolshap_knn'
extract(x, ...)
  ## S3 method for class 'sobolshap_knn'
print(x, ...)
  ## S3 method for class 'sobolshap_knn'
plot(x, ylim = c(0, 1), type.multout = "lines", ...)
  ## S3 method for class 'sobolshap_knn'
ggplot(data,  mapping = aes(), ylim = c(0, 1), 
              type.multout = "lines", ..., environment = parent.frame())
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sobolshap_knn_+3A_model">model</code></td>
<td>
<p>a function, or a model with a <code>predict</code> method, defining the model to analyze.</p>
</td></tr>
<tr><td><code id="sobolshap_knn_+3A_x">X</code></td>
<td>
<p>a random sample of the inputs.</p>
</td></tr>
<tr><td><code id="sobolshap_knn_+3A_id.cat">id.cat</code></td>
<td>
<p>a vector with the indices of the categorical inputs.</p>
</td></tr>
<tr><td><code id="sobolshap_knn_+3A_u">U</code></td>
<td>
<p>an integer equal to 0 (total Sobol indices) or 1 (first-order Sobol indices) 
or a list of vector indices defining the subsets of inputs whose sensitivity indices 
must be computed or a matrix of 0s and 1s where each row encodes a subset of inputs 
whose sensitivity indices must be computed (see examples) or NULL (all possible subsets).</p>
</td></tr>
<tr><td><code id="sobolshap_knn_+3A_method">method</code></td>
<td>
<p>the algorithm to be used for estimation, either &quot;rank&quot; or &quot;knn&quot;, see details.</p>
</td></tr>
<tr><td><code id="sobolshap_knn_+3A_n.knn">n.knn</code></td>
<td>
<p>the number of nearest neighbours used for estimation if <code>method="knn"</code>.</p>
</td></tr>
<tr><td><code id="sobolshap_knn_+3A_return.shap">return.shap</code></td>
<td>
<p>a logical indicating if Shapley effects must be estimated, 
can only be TRUE if <code>U=NULL</code>.</p>
</td></tr>
<tr><td><code id="sobolshap_knn_+3A_randperm">randperm</code></td>
<td>
<p>a logical indicating if random permutations are used to estimate Shapley effects, 
only if <code>U=NULL</code> and <code>return.shap=TRUE</code>.</p>
</td></tr>
<tr><td><code id="sobolshap_knn_+3A_n.perm">n.perm</code></td>
<td>
<p>the number of random permutations used for estimation if <code>randperm=TRUE</code>.</p>
</td></tr>
<tr><td><code id="sobolshap_knn_+3A_rescale">rescale</code></td>
<td>
<p>a logical indicating if continuous inputs must be rescaled before distance computations.
If TRUE, continuous inputs are first whitened with the ZCA-cor whitening procedure 
(cf. whiten() function in package <code>whitening</code>). If the inputs are independent, 
this first step will have a very limited impact. Then, the resulting whitened inputs 
are individually modified via a copula transform such that each input has the same scale.</p>
</td></tr>
<tr><td><code id="sobolshap_knn_+3A_n.limit">n.limit</code></td>
<td>
<p>the sample size limit above which approximate nearest neighbour search is activated, 
only used if <code>method="knn"</code>.</p>
</td></tr>
<tr><td><code id="sobolshap_knn_+3A_noise">noise</code></td>
<td>
<p>a logical which is TRUE if the model or the output sample is noisy, see details.</p>
</td></tr>
<tr><td><code id="sobolshap_knn_+3A_x">x</code></td>
<td>
<p>a list of class <code>"sobolshap_knn"</code> storing the state of the sensitivity study 
(parameters, data, estimates).</p>
</td></tr>
<tr><td><code id="sobolshap_knn_+3A_data">data</code></td>
<td>
<p>a list of class <code>"sobolshap_knn"</code> storing the state of the sensitivity study 
(parameters, data, estimates).</p>
</td></tr>
<tr><td><code id="sobolshap_knn_+3A_y">y</code></td>
<td>
<p>a vector of model responses.</p>
</td></tr>
<tr><td><code id="sobolshap_knn_+3A_ylim">ylim</code></td>
<td>
<p>y-coordinate plotting limits.</p>
</td></tr>
<tr><td><code id="sobolshap_knn_+3A_type.multout">type.multout</code></td>
<td>
<p>the plotting method in the case of multiple outputs, either &quot;points&quot; or &quot;lines&quot;, 
see examples.</p>
</td></tr>
<tr><td><code id="sobolshap_knn_+3A_mapping">mapping</code></td>
<td>
<p>Default list of aesthetic mappings to use for plot. If not specified, 
must be supplied in each layer added to the plot.</p>
</td></tr>
<tr><td><code id="sobolshap_knn_+3A_environment">environment</code></td>
<td>
<p>[Deprecated] Used prior to tidy evaluation.</p>
</td></tr>
<tr><td><code id="sobolshap_knn_+3A_...">...</code></td>
<td>
<p>any other arguments for <code>model</code> which are passed unchanged each time it is called.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For <code>method="rank"</code>, the estimator is defined in Gamboa et al. (2020) 
following Chatterjee (2019). For first-order indices it is based on an input ranking 
(same algorithm as in <code>sobolrank</code>) while for higher orders, 
it uses an approximate heuristic solution of the traveling salesman problem 
applied to the input sample distances (cf. TSP() function in package <code>TSP</code>).
For <code>method="knn"</code>, ranking and TSP are replaced by a nearest neighbour search 
as proposed in Broto et al. (2020) and in Azadkia &amp; Chatterjee (2020) for a similar coefficient. 
The algorithm is the same as in <code>shapleySubsetMc</code> but with an optimized implementation. 
In particular, the distance used for subsets with mixed inputs (continuous and categorical) 
are the same but here the additional one-hot encoding of categorical variables makes it possible to
work only with Euclidean distances. Furthermore, a fast approximate nearest neighbour search is also
available, which is strongly recommended for large sample sizes. The main difference 
with <code>shapleySubsetMc</code> is that here we use the entire N sample to compute all indices, 
while in <code>shapleySubsetMc</code> the user can specify a total cost Ntot which performs 
a specific allocation of sample sizes to the estimation of each index. 
In addition, the <code>weights</code> option is not available here yet.
If the outputs are noisy, the argument <code>noise</code> can be used: it only has an impact on the
estimation of one specific sensitivity index, namely <code class="reqn">Var(E(Y|X1,\ldots,Xp))/Var(Y)</code>. 
If there is no noise this index is equal to 1, while in the presence of noise it must be estimated.
</p>
<p>When <code>randperm=TRUE</code>, Shapley effects are no longer estimated by computing all the possible 
subsets of variables but only on subsets obtained with random permutations as proposed in Castro et al.(2009).
This is useful for problems with a large number of inputs, since the number of subsets increases exponentially 
with dimension.
</p>
<p>The <code>extract</code> method is useful if in a first step the Shapley effects have been computed 
and thus sensitivity indices for all possible subsets are available. 
The resulting <code>sobolshap_knn</code> object can be post-treated by <code>extract</code> 
to get first-order and total Sobol indices very easily.
</p>


<h3>Value</h3>

<p><code>sobolshap_knn</code> returns a list of class <code>"sobolshap_knn"</code>, containing all
the input arguments detailed before, plus the following components:
</p>
<table role = "presentation">
<tr><td><code>call</code></td>
<td>
<p>the matched call.</p>
</td></tr>
<tr><td><code>X</code></td>
<td>
<p>a <code>data.frame</code> containing the design of experiments.</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>a vector of model responses.</p>
</td></tr>
<tr><td><code>U</code></td>
<td>
<p>the subsets of inputs for which sensitivity indices have been computed.</p>
</td></tr>
<tr><td><code>S</code></td>
<td>
<p>the estimations of the Sobol sensitivity indices (see details).</p>
</td></tr>
<tr><td><code>Shap</code></td>
<td>
<p>the estimations of Shapley effects, if return.shap was set to TRUE.</p>
</td></tr>
<tr><td><code>order</code></td>
<td>
<p>0 (total indices), 1 (first-order indices) or NULL. Used for plotting defaults.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Sebastien Da Veiga
</p>


<h3>References</h3>

<p>Azadkia M., Chatterjee S., 2021), <em>A simple measure of conditional dependence</em>, 
Ann. Statist. 49(6):3070-3102.
</p>
<p>Broto B., Bachoc F., Depecker M. (2020), Variance reduction for estimation of Shapley effects 
and adaptation to unknown input distribution, SIAM/ASA Journal of Uncertainty Quantification, 
8:693-716.
</p>
<p>Castro J., Gomez D, Tejada J. (2009). Polynomial calculation of the Shapley value based on sampling. 
Computers &amp; Operations Research, 36(5):1726-1730.
</p>
<p>Chatterjee, S., 2021, <em>A new coefficient of correlation</em>, Journal of the American 
Statistical Association, 116:2009-2022. 
</p>
<p>Gamboa, F., Gremaud, P., Klein, T., &amp; Lagnoux, A., 2022, <em>Global Sensitivity Analysis: 
a novel generation of mighty estimators based on rank statistics</em>, 
Bernoulli 28: 2345-2374.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+sobolrank">sobolrank</a>, <a href="#topic+shapleysobol_knn">shapleysobol_knn</a>, <a href="#topic+shapleySubsetMc">shapleySubsetMc</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  
    # Test case: the non-monotonic Sobol g-function
    # Example with a call to a numerical model
    # First compute first-order indices with ranking
    n &lt;- 1000
    X &lt;- data.frame(matrix(runif(8 * n), nrow = n))
    x &lt;- sobolshap_knn(model = sobol.fun, X = X, U = 1, method = "rank")
    print(x)
    library(ggplot2)
    ggplot(x)
    # We can use the output sample generated for this estimation to compute 
    # total indices without additional calls to the model
    x2 &lt;- sobolshap_knn(model = NULL, X = X, U = 0, method = "knn", n.knn = 5)
    tell(x2,x$y)
    ggplot(x2)
    
    # Test case: the Ishigami function
    # Example with given data and the use of approximate nearest neighbour search
    library(RANN)
    n &lt;- 5000
    X &lt;- data.frame(matrix(-pi+2*pi*runif(3 * n), nrow = n))
    Y &lt;- ishigami.fun(X)
    x &lt;- sobolshap_knn(model = NULL, X = X, U = NULL, method = "knn", n.knn = 5, 
                       return.shap = TRUE, n.limit = 2000)
    tell(x,Y)
    library(ggplot2)
    ggplot(x)
    # We can also extract first-order and total Sobol indices
    x1 &lt;- extract(x)
    print(x1)
    
    # Test case : Linear model (3 Gaussian inputs including 2 dependent) with scaling
    # See Iooss and Prieur (2019)
    library(mvtnorm) # Multivariate Gaussian variables
    library(whitening) # For scaling
    modlin &lt;- function(X) apply(X,1,sum)
    d &lt;- 3
    n &lt;- 10000
    mu &lt;- rep(0,d)
    sig &lt;- c(1,1,2)
    ro &lt;- 0.9
    Cormat &lt;- matrix(c(1,0,0,0,1,ro,0,ro,1),d,d)
    Covmat &lt;- ( sig %*% t(sig) ) * Cormat
    Xall &lt;- function(n) mvtnorm::rmvnorm(n,mu,Covmat)
    X &lt;- Xall(n)
    x &lt;- sobolshap_knn(model = modlin, X = X, U = NULL, method = "knn", n.knn = 5, 
                       return.shap = TRUE, rescale = TRUE, n.limit = 2000)
    print(x)
    
    # Test case: functional toy fct 'Arctangent temporal function'
    n &lt;- 3000
    X &lt;- data.frame(matrix(runif(2*n,-7,7), nrow = n))
    Y &lt;- atantemp.fun(X)
    x &lt;- sobolshap_knn(model = NULL, X = X, U = NULL, method = "knn", n.knn = 5, 
                       return.shap = TRUE, n.limit = 2000)
    tell(x,Y)
    library(ggplot2)
    library(reshape2)
    ggplot(x, type.multout="lines")
  
</code></pre>

<hr>
<h2 id='sobolSmthSpl'>Estimation of Sobol' First Order Indices with B-spline Smoothing</h2><span id='topic+sobolSmthSpl'></span>

<h3>Description</h3>

<p>Determines the Si coefficient for singular parameters through B-spline smoothing with roughness penalty.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>	sobolSmthSpl(Y, X)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sobolSmthSpl_+3A_y">Y</code></td>
<td>
<p>vector of model responses.</p>
</td></tr>
<tr><td><code id="sobolSmthSpl_+3A_x">X</code></td>
<td>
<p>matrix having as rows the input vectors corresponding to the responses in Y.</p>
</td></tr>	
</table>


<h3>Details</h3>

<p>WARNING: This function can give bad results for reasons that have not been yet investigated.
</p>


<h3>Value</h3>

<p>sobolSmthSpl returns a list of class &quot;sobolSmthSpl&quot; containing the following components:
</p>
<table role = "presentation">
<tr><td><code>call</code></td>
<td>
<p>the matched call.</p>
</td></tr>
<tr><td><code>X</code></td>
<td>
<p>the provided input matrix.</p>
</td></tr>
<tr><td><code>Y</code></td>
<td>
<p>the provided matrix of model responses.</p>
</td></tr>
<tr><td><code>S</code></td>
<td>
<p>a matrix having the following columns:
Si (the estimated first order Sobol' indices), 
Si.e (the standard errors for the estimated first order Sobol' indices) and
q0.05 (the 0.05 quantiles assuming for the Si indices Normal distributions centred on the 
Si estimates and with standard deviations the calculated standard errors)
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Filippo Monari
</p>


<h3>References</h3>

<p>Saltelli, A; Ratto, M; Andres, T; Campolongo, F; Cariboni, J; Gatelli, D; Saisana, M &amp; Tarantola, S.
<em>Global Sensitivity Analysis: The Primer Wiley-Interscience</em>, 2008
</p>
<p>M Ratto and A. Pagano, 2010, <em>Using recursive algorithms for the efficient identification
of smoothing spline ANOVA models</em>, Advances in Statistical Analysis, 94, 367&ndash;388.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+sobol">sobol</a>, <a href="#topic+sobolEff">sobolEff</a>, <a href="#topic+sobolGP">sobolGP</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>	X = matrix(runif(5000), ncol = 10)
	Y = sobol.fun(X)
	sa = sobolSmthSpl(Y, X)
	plot(sa)
</code></pre>

<hr>
<h2 id='sobolTIIlo'>Liu and Owen Estimation of Total Interaction Indices</h2><span id='topic+sobolTIIlo'></span><span id='topic+tell.sobolTIIlo'></span><span id='topic+print.sobolTIIlo'></span><span id='topic+plot.sobolTIIlo'></span><span id='topic+ggplot.sobolTIIlo'></span><span id='topic+plotFG.sobolTIIlo'></span>

<h3>Description</h3>

<p><code>sobolTIIlo</code> implements the  asymptotically efficient formula of Liu and Owen (2006) for the estimation of total interaction indices as described e.g. in Section 3.4 of Fruth et al. (2014). Total interaction indices (TII) are superset indices of pairs of variables, thus give the total influence of each second-order interaction. The total cost of the method is <code class="reqn">(1+N+\choose(N,2)) \times n</code> where <code class="reqn">N</code> is the number
of indices to estimate. Asymptotic confidence intervals are provided. Via <code>plotFG</code> (which uses functions of the package <code>igraph</code>), the TIIs can be visualized in a so-called FANOVA graph as described in section 2.2 of Muehlenstaedt et al. (2012).</p>


<h3>Usage</h3>

<pre><code class='language-R'>sobolTIIlo(model = NULL, X1, X2, conf = 0.95, ...)
## S3 method for class 'sobolTIIlo'
tell(x, y = NULL, ...)
## S3 method for class 'sobolTIIlo'
print(x, ...)
## S3 method for class 'sobolTIIlo'
plot(x, ylim = NULL, ...)
## S3 method for class 'sobolTIIlo'
ggplot(data, mapping = aes(), ylim = NULL, ..., environment
                 = parent.frame())
## S3 method for class 'sobolTIIlo'
plotFG(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sobolTIIlo_+3A_model">model</code></td>
<td>
<p>a function, or a model with a <code>predict</code> method,
defining the model to analyze.</p>
</td></tr>
<tr><td><code id="sobolTIIlo_+3A_x1">X1</code></td>
<td>
<p>the first random sample.</p>
</td></tr>
<tr><td><code id="sobolTIIlo_+3A_x2">X2</code></td>
<td>
<p>the second random sample.</p>
</td></tr>
<tr><td><code id="sobolTIIlo_+3A_conf">conf</code></td>
<td>
<p>the confidence level for asymptotic confidence intervals, defaults to 0.95.</p>
</td></tr>
<tr><td><code id="sobolTIIlo_+3A_x">x</code></td>
<td>
<p>a list of class <code>"sobolTIIlo"</code> storing the state of the
sensitivity study (parameters, data, estimates).</p>
</td></tr>
<tr><td><code id="sobolTIIlo_+3A_data">data</code></td>
<td>
<p>a list of class <code>"sobolTIIlo"</code> storing the state of the
sensitivity study (parameters, data, estimates).</p>
</td></tr>
<tr><td><code id="sobolTIIlo_+3A_y">y</code></td>
<td>
<p>a vector of model responses.</p>
</td></tr>
<tr><td><code id="sobolTIIlo_+3A_mapping">mapping</code></td>
<td>
<p>Default list of aesthetic mappings to use for plot. If not specified, 
must be supplied in each layer added to the plot.</p>
</td></tr>
<tr><td><code id="sobolTIIlo_+3A_environment">environment</code></td>
<td>
<p>[Deprecated] Used prior to tidy evaluation.</p>
</td></tr>
<tr><td><code id="sobolTIIlo_+3A_...">...</code></td>
<td>
<p>any other arguments for <code>model</code> which are passed
unchanged each time it is called.</p>
</td></tr>
<tr><td><code id="sobolTIIlo_+3A_ylim">ylim</code></td>
<td>
<p>optional, the y limits of the plot.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>sobolTIIlo</code> returns a list of class <code>"sobolTIIlo"</code>, containing all
the input arguments detailed before, plus the following components:
</p>
<table role = "presentation">
<tr><td><code>call</code></td>
<td>
<p>the matched call.</p>
</td></tr>
<tr><td><code>X</code></td>
<td>
<p>a <code>data.frame</code> containing the design of experiments.</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>a vector of model responses.</p>
</td></tr>
<tr><td><code>V</code></td>
<td>
<p>the estimation of the overall variance.</p>
</td></tr>
<tr><td><code>tii.unscaled</code></td>
<td>
<p>the unscaled estimations of the TIIs.</p>
</td></tr>
<tr><td><code>tii.scaled</code></td>
<td>
<p>the scaled estimations of the TIIs together with asymptotic confidence intervals.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Jana Fruth
</p>


<h3>References</h3>

<p>R. Liu, A. B. Owen, 2006,  <em>Estimating mean dimensionality of analysis of variance decompositions</em>, JASA, 101 (474), 712&ndash;721.
</p>
<p>J. Fruth, O. Roustant, S. Kuhnt, 2014, <em>Total interaction index: A variance-based sensitivity index for second-order interaction screening</em>, J. Stat. Plan. Inference, 147, 212&ndash;223.
</p>
<p>T. Muehlenstaedt, O. Roustant, L. Carraro, S. Kuhnt, 2012, <em>Data-driven Kriging models based on FANOVA-decomposition</em>, Stat. Comput., 22 (3), 723&ndash;738.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+sobolTIIpf">sobolTIIpf</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Test case : the Ishigami function

# The method requires 2 samples
n &lt;- 1000
X1 &lt;- data.frame(matrix(runif(3 * n, -pi, pi), nrow = n))
X2 &lt;- data.frame(matrix(runif(3 * n, -pi, pi), nrow = n))

# sensitivity analysis (the true values of the scaled TIIs are 0, 0.244, 0)
x &lt;- sobolTIIlo(model = ishigami.fun, X1 = X1, X2 = X2)
print(x)

# plot of tiis and FANOVA graph
plot(x)

library(ggplot2)
ggplot(x)


library(igraph)
plotFG(x)



</code></pre>

<hr>
<h2 id='sobolTIIpf'>Pick-freeze Estimation of Total Interaction Indices</h2><span id='topic+sobolTIIpf'></span><span id='topic+tell.sobolTIIpf'></span><span id='topic+print.sobolTIIpf'></span><span id='topic+plot.sobolTIIpf'></span><span id='topic+ggplot.sobolTIIpf'></span><span id='topic+plotFG.sobolTIIpf'></span><span id='topic+plotFG'></span>

<h3>Description</h3>

<p><code>sobolTIIpf</code> implements the pick-freeze estimation of total interaction indices as described in Section 3.3 of Fruth et al. (2014). Total interaction indices (TII) are superset indices of pairs of variables, thus give the total influence of each second-order interaction. The pick-freeze estimation enables the strategy to reuse evaluations of Saltelli (2002). The total costs are <code class="reqn">(1+N) \times n</code> where <code class="reqn">N</code> is the number of indices to estimate. Via <code>plotFG</code>, the TIIs can be visualized in a so-called FANOVA graph as described in section 2.2 of Muehlenstaedt et al. (2012).</p>


<h3>Usage</h3>

<pre><code class='language-R'>sobolTIIpf(model = NULL, X1, X2, ...)
## S3 method for class 'sobolTIIpf'
tell(x, y = NULL, ...)
## S3 method for class 'sobolTIIpf'
print(x, ...)
## S3 method for class 'sobolTIIpf'
plot(x, ylim = NULL, ...)
## S3 method for class 'sobolTIIpf'
ggplot(data, mapping = aes(), ylim = NULL, ..., environment
                 = parent.frame())
## S3 method for class 'sobolTIIpf'
plotFG(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sobolTIIpf_+3A_model">model</code></td>
<td>
<p>a function, or a model with a <code>predict</code> method,
defining the model to analyze.</p>
</td></tr>
<tr><td><code id="sobolTIIpf_+3A_x1">X1</code></td>
<td>
<p>the first random sample.</p>
</td></tr>
<tr><td><code id="sobolTIIpf_+3A_x2">X2</code></td>
<td>
<p>the second random sample.</p>
</td></tr>
<tr><td><code id="sobolTIIpf_+3A_x">x</code></td>
<td>
<p>a list of class <code>"sobolTIIpf"</code> storing the state of the
sensitivity study (parameters, data, estimates).</p>
</td></tr>
<tr><td><code id="sobolTIIpf_+3A_data">data</code></td>
<td>
<p>a list of class <code>"sobolTIIpf"</code> storing the state of the
sensitivity study (parameters, data, estimates).</p>
</td></tr>
<tr><td><code id="sobolTIIpf_+3A_y">y</code></td>
<td>
<p>a vector of model responses.</p>
</td></tr>
<tr><td><code id="sobolTIIpf_+3A_mapping">mapping</code></td>
<td>
<p>Default list of aesthetic mappings to use for plot. If not specified, 
must be supplied in each layer added to the plot.</p>
</td></tr>
<tr><td><code id="sobolTIIpf_+3A_environment">environment</code></td>
<td>
<p>[Deprecated] Used prior to tidy evaluation.</p>
</td></tr>
<tr><td><code id="sobolTIIpf_+3A_...">...</code></td>
<td>
<p>any other arguments for <code>model</code> which are passed
unchanged each time it is called.</p>
</td></tr>
<tr><td><code id="sobolTIIpf_+3A_ylim">ylim</code></td>
<td>
<p>optional, the y limits of the plot.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>sobolTIIpf</code> returns a list of class <code>"sobolTIIpf"</code>, containing all
the input arguments detailed before, plus the following components:
</p>
<table role = "presentation">
<tr><td><code>call</code></td>
<td>
<p>the matched call.</p>
</td></tr>
<tr><td><code>X</code></td>
<td>
<p>a <code>data.frame</code> containing the design of experiments.</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>a vector of model responses.</p>
</td></tr>
<tr><td><code>V</code></td>
<td>
<p>the estimation of the overall variance.</p>
</td></tr>
<tr><td><code>tii.unscaled</code></td>
<td>
<p>the unscaled estimations of the TIIs together.</p>
</td></tr>
<tr><td><code>tii.scaled</code></td>
<td>
<p>the scaled estimations of the TIIs.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Jana Fruth
</p>


<h3>References</h3>

<p>J. Fruth, O. Roustant, S. Kuhnt, 2014, <em>Total interaction index: A variance-based sensitivity index for second-order interaction screening</em>, J. Stat. Plan. Inference, 147, 212&ndash;223.
</p>
<p>A. Saltelli, 2002, <em>Making best use of model evaluations to compute sensitivity indices</em>, Comput. Phys. Commun., 145, 580-297.
</p>
<p>T. Muehlenstaedt, O. Roustant, L. Carraro, S. Kuhnt, 2012, <em>Data-driven Kriging models based on FANOVA-decomposition</em>, Stat. Comput., 22 (3), 723&ndash;738.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+sobolTIIlo">sobolTIIlo</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Test case : the Ishigami function

# The method requires 2 samples
n &lt;- 1000
X1 &lt;- data.frame(matrix(runif(3 * n, -pi, pi), nrow = n))
X2 &lt;- data.frame(matrix(runif(3 * n, -pi, pi), nrow = n))

# sensitivity analysis (the true values are 0, 0.244, 0)
x &lt;- sobolTIIpf(model = ishigami.fun, X1 = X1, X2 = X2)
print(x)

# plot of tiis and FANOVA graph
plot(x)

library(ggplot2)
ggplot(x)


library(igraph)
plotFG(x)

</code></pre>

<hr>
<h2 id='soboltouati'>Monte Carlo Estimation of Sobol' Indices (formulas of Martinez (2011) and Touati (2016))</h2><span id='topic+soboltouati'></span><span id='topic+tell.soboltouati'></span><span id='topic+print.soboltouati'></span><span id='topic+plot.soboltouati'></span><span id='topic+ggplot.soboltouati'></span>

<h3>Description</h3>

<p><code>soboltouati</code> implements the Monte Carlo estimation of
the Sobol' indices for both first-order and total indices using 
correlation coefficients-based formulas, at a total cost of 
<code class="reqn">(p+2) \times n</code> model evaluations. 
These are called the Martinez estimators. It also computes their
confidence intervals based on asymptotic properties of empirical 
correlation coefficients.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>soboltouati(model = NULL, X1, X2, conf = 0.95, ...)
## S3 method for class 'soboltouati'
tell(x, y = NULL, return.var = NULL, ...)
## S3 method for class 'soboltouati'
print(x, ...)
## S3 method for class 'soboltouati'
plot(x, ylim = c(0, 1), ...)
## S3 method for class 'soboltouati'
ggplot(data,  mapping = aes(), ylim = c(0, 1), ..., environment
                 = parent.frame())
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="soboltouati_+3A_model">model</code></td>
<td>
<p>a function, or a model with a <code>predict</code> method,
defining the model to analyze.</p>
</td></tr>
<tr><td><code id="soboltouati_+3A_x1">X1</code></td>
<td>
<p>the first random sample.</p>
</td></tr>
<tr><td><code id="soboltouati_+3A_x2">X2</code></td>
<td>
<p>the second random sample.</p>
</td></tr>
<tr><td><code id="soboltouati_+3A_conf">conf</code></td>
<td>
<p>the confidence level for confidence intervals, or zero to
avoid their computation if they are not needed.</p>
</td></tr>
<tr><td><code id="soboltouati_+3A_x">x</code></td>
<td>
<p>a list of class <code>"soboltouati"</code> storing the state of the
sensitivity study (parameters, data, estimates).</p>
</td></tr>
<tr><td><code id="soboltouati_+3A_data">data</code></td>
<td>
<p>a list of class <code>"soboltouati"</code> storing the state of the
sensitivity study (parameters, data, estimates).</p>
</td></tr>
<tr><td><code id="soboltouati_+3A_y">y</code></td>
<td>
<p>a vector of model responses.</p>
</td></tr>
<tr><td><code id="soboltouati_+3A_return.var">return.var</code></td>
<td>
<p>a vector of character strings giving further
internal variables names to store in  the output object <code>x</code>.</p>
</td></tr>
<tr><td><code id="soboltouati_+3A_ylim">ylim</code></td>
<td>
<p>y-coordinate plotting limits.</p>
</td></tr>
<tr><td><code id="soboltouati_+3A_mapping">mapping</code></td>
<td>
<p>Default list of aesthetic mappings to use for plot. If not specified, 
must be supplied in each layer added to the plot.</p>
</td></tr>
<tr><td><code id="soboltouati_+3A_environment">environment</code></td>
<td>
<p>[Deprecated] Used prior to tidy evaluation.</p>
</td></tr>
<tr><td><code id="soboltouati_+3A_...">...</code></td>
<td>
<p>any other arguments for <code>model</code> which are passed
unchanged each time it is called</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This estimator supports missing values (NA or NaN) which can occur during the
simulation of the model on the design of experiments (due to code failure) 
even if Sobol' indices are no more rigorous variance-based sensitivity 
indices if missing values are present. In this case, a warning is displayed. 
</p>


<h3>Value</h3>

<p><code>soboltouati</code> returns a list of class <code>"soboltouati"</code>, 
containing all the input arguments detailed before, plus the following 
components:
</p>
<table role = "presentation">
<tr><td><code>call</code></td>
<td>
<p>the matched call.</p>
</td></tr>
<tr><td><code>X</code></td>
<td>
<p>a <code>data.frame</code> containing the design of experiments.</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>the response used</p>
</td></tr>
<tr><td><code>V</code></td>
<td>
<p>the estimations of normalized variances of the Conditional 
Expectations (VCE) with respect to each factor and also with respect 
to the complementary set of each factor (&quot;all but <code class="reqn">X_i</code>&quot;).</p>
</td></tr>
<tr><td><code>S</code></td>
<td>
<p>the estimations of the Sobol' first-order indices.</p>
</td></tr>
<tr><td><code>T</code></td>
<td>
<p>the estimations of the Sobol' total sensitivity indices.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Taieb Touati, Khalid Boumhaout
</p>


<h3>References</h3>

<p>J-M. Martinez, 2011, <em>Analyse de sensibilite globale par decomposition 
de la variance</em>, Presentation in the meeting of GdR Ondes and GdR MASCOT-NUM, 
January, 13th, 2011, Institut Henri Poincare, Paris, France.
</p>
<p>T. Touati, 2016, Confidence intervals for Sobol' indices. 
Proceedings of the SAMO 2016 Conference, Reunion Island, France, December 2016.
</p>
<p>T. Touati, 2017, <em>Intervalles de confiance pour les indices de Sobol</em>,
49emes Journees de la SFdS, Avignon, France, Juin 2017.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+sobol">sobol</a>, <a href="#topic+sobol2002">sobol2002</a>, <a href="#topic+sobolSalt">sobolSalt</a>, <a href="#topic+sobol2007">sobol2007</a>, <a href="#topic+soboljansen">soboljansen</a>, <a href="#topic+sobolmartinez">sobolmartinez</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Test case : the non-monotonic Sobol g-function

# The method of sobol requires 2 samples
# There are 8 factors, all following the uniform distribution
# on [0,1]

library(boot)
n &lt;- 1000
X1 &lt;- data.frame(matrix(runif(8 * n), nrow = n))
X2 &lt;- data.frame(matrix(runif(8 * n), nrow = n))

# sensitivity analysis

x &lt;- soboltouati(model = sobol.fun, X1, X2)
print(x)
plot(x)

library(ggplot2)
ggplot(x)
</code></pre>

<hr>
<h2 id='squaredIntEstim'>
Squared integral estimate
</h2><span id='topic+squaredIntEstim'></span>

<h3>Description</h3>

<p>This function provides two estimators of a squared expectation. 
The first one, naive, is the square of the sample mean. It is positively biased.
The second one is a U-statistics, and unbiased. The two are equivalent for large sample sizes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>squaredIntEstim(x, method = "unbiased")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="squaredIntEstim_+3A_x">x</code></td>
<td>
<p>A vector of observations supposed to be drawn independently from a square integrable random variable</p>
</td></tr>
<tr><td><code id="squaredIntEstim_+3A_method">method</code></td>
<td>
<p>If &quot;unbiased&quot;, computes the U-statistics, otherwise the square of the sample mean is computed</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Let X1, ..., Xn be i.i.d. random variables. The aim is to estimate t = E(Xi)^2.
The naive estimator is the square of the sample mean: T1 = [(X1 + ... + Xn)/n]^2.
It is positively biased, and the bias is equal to s^2/n, where s^2 = var(X1).
The U-statistics estimator is the average of Xi * Xj over all unordered pairs (i,j).
Equivalently, it is equal to T1 minus the (unbiased) sample variance divided by n.
</p>


<h3>Value</h3>

<p>A real number, corresponding to the estimated value of the squared integral.
</p>


<h3>Author(s)</h3>

<p>O. Roustant
</p>


<h3>References</h3>

<p>O. Roustant, F. Gamboa and B. Iooss, <em>Parseval inequalities and lower bounds for 
variance-based sensitivity indices</em>, Electronic Journal of Statistics, 14:386-412, 2020
</p>
<p>Van der Vaart, A. W. Asymptotic statistics. Vol. 3. Cambridge university press, 2000.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 100  # sample size
nsim &lt;- 100 # number of simulations
mu &lt;- 0

T &lt;- Tunb &lt;- rep(NA, nsim)
theta &lt;- mu^2  # E(X)^2, with X following N(mu, 1)

for (i in 1:nsim){
  x &lt;- rnorm(n, mean = mu, sd = 1)
  T[i] &lt;- squaredIntEstim(x, method = "biased")
  Tunb[i] &lt;- squaredIntEstim(x, method = "unbiased")
}

par(mfrow = c(1, 1))
boxplot(cbind(T, Tunb))
abline(h = theta, col = "red")
abline(h = c(mean(T), mean(Tunb)), col = c("blue", "cyan"), lty = "dotted")
# look at the difference between median and mean
</code></pre>

<hr>
<h2 id='src'>Standardized Regression Coefficients</h2><span id='topic+src'></span><span id='topic+print.src'></span><span id='topic+plot.src'></span><span id='topic+ggplot.src'></span>

<h3>Description</h3>

<p><code>src</code> computes the Standardized Regression Coefficients
(SRC), or the Standardized Rank Regression Coefficients (SRRC), which
are sensitivity indices based on linear or monotonic assumptions in
the case of independent factors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>src(X, y, rank = FALSE, logistic = FALSE, nboot = 0, conf = 0.95)
## S3 method for class 'src'
print(x, ...)
## S3 method for class 'src'
plot(x, ylim = c(-1,1), ...)
## S3 method for class 'src'
ggplot(data,  mapping = aes(), ylim = c(-1, 1), ..., environment
                 = parent.frame())
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="src_+3A_x">X</code></td>
<td>
<p>a data frame (or object coercible by <code>as.data.frame</code>)
containing the design of experiments (model input variables).</p>
</td></tr>
<tr><td><code id="src_+3A_y">y</code></td>
<td>
<p>a vector containing the responses corresponding to the design
of experiments (model output variables).</p>
</td></tr>
<tr><td><code id="src_+3A_rank">rank</code></td>
<td>
<p>logical. If <code>TRUE</code>, the analysis is done on the
ranks.</p>
</td></tr>
<tr><td><code id="src_+3A_logistic">logistic</code></td>
<td>
<p>logical. If <code>TRUE</code>, the analysis is done via a
logistic regression (binomial GLM).</p>
</td></tr>
<tr><td><code id="src_+3A_nboot">nboot</code></td>
<td>
<p>the number of bootstrap replicates.</p>
</td></tr>
<tr><td><code id="src_+3A_conf">conf</code></td>
<td>
<p>the confidence level of the bootstrap confidence intervals.</p>
</td></tr>
<tr><td><code id="src_+3A_x">x</code></td>
<td>
<p>the object returned by <code>src</code>.</p>
</td></tr>
<tr><td><code id="src_+3A_data">data</code></td>
<td>
<p>the object returned by <code>src</code>.</p>
</td></tr>
<tr><td><code id="src_+3A_ylim">ylim</code></td>
<td>
<p>the y-coordinate limits of the plot.</p>
</td></tr>
<tr><td><code id="src_+3A_mapping">mapping</code></td>
<td>
<p>Default list of aesthetic mappings to use for plot. If not specified, 
must be supplied in each layer added to the plot.</p>
</td></tr>
<tr><td><code id="src_+3A_environment">environment</code></td>
<td>
<p>[Deprecated] Used prior to tidy evaluation.</p>
</td></tr>
<tr><td><code id="src_+3A_...">...</code></td>
<td>
<p>arguments to be passed to methods, such as graphical
parameters (see <code>par</code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Logistic regression model (<code>logistic = TRUE</code>) and rank-based indices
(<code>rank = TRUE</code>) are incompatible.
</p>


<h3>Value</h3>

<p><code>src</code> returns a list of class <code>"src"</code>, containing the following
components:
</p>
<table role = "presentation">
<tr><td><code>call</code></td>
<td>
<p>the matched call.</p>
</td></tr>
<tr><td><code>SRC</code></td>
<td>
<p>a data frame containing the estimations of the SRC
indices, bias and confidence intervals (if <code>rank = FALSE</code>).</p>
</td></tr>
<tr><td><code>SRRC</code></td>
<td>
<p>a data frame containing the estimations of the SRRC
indices, bias and confidence intervals (if <code>rank = TRUE</code>).</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Gilles Pujol and Bertrand Iooss
</p>


<h3>References</h3>

<p>L. Clouvel, B. Iooss, V. Chabridon, M. Il Idrissi and F. Robin, 2023,
<em>An overview of variance-based importance measures in the linear regression context: 
comparative analyses and numerical tests</em>, Preprint.
<a href="https://hal.science/hal-04102053">https://hal.science/hal-04102053</a>
</p>
<p>B. Iooss, V. Chabridon and V. Thouvenot, <em>Variance-based importance 
measures for machine learning model interpretability</em>, Congres lambda-mu23,
Saclay, France, 10-13 octobre 2022
<a href="https://hal.science/hal-03741384">https://hal.science/hal-03741384</a>
</p>
<p>A. Saltelli, K. Chan and E. M. Scott eds, 2000, <em>Sensitivity
Analysis</em>, Wiley.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pcc">pcc</a></code>, <code><a href="#topic+lmg">lmg</a></code>, <code><a href="#topic+pmvd">pmvd</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# a 100-sample with X1 ~ U(0.5, 1.5)
#                   X2 ~ U(1.5, 4.5)
#                   X3 ~ U(4.5, 13.5)

library(boot)
n &lt;- 100
X &lt;- data.frame(X1 = runif(n, 0.5, 1.5),
                X2 = runif(n, 1.5, 4.5),
                X3 = runif(n, 4.5, 13.5))

# linear model : Y = X1 + X2 + X3

y &lt;- with(X, X1 + X2 + X3)

# sensitivity analysis

x &lt;- src(X, y, nboot = 100)
print(x)
plot(x)

library(ggplot2)
ggplot(x)
</code></pre>

<hr>
<h2 id='support'>
Support index functions: Measuring the effect of input variables over their support
</h2><span id='topic+support'></span>

<h3>Description</h3>

<p>Function to estimate the first-order and total support index functions (Fruth et al., 2016).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>support(model, X, Xnew = NULL, fX = NULL, gradfX = NULL, h = 1e-06, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="support_+3A_model">model</code></td>
<td>
<p>a function, or a model with a predict method, defining the model to analyze.</p>
</td></tr>
<tr><td><code id="support_+3A_x">X</code></td>
<td>
<p>a random sample.</p>
</td></tr>
<tr><td><code id="support_+3A_xnew">Xnew</code></td>
<td>
<p>an optional set of points where to visualize the support indices. If missing, <code>X</code> is used.</p>
</td></tr>
<tr><td><code id="support_+3A_fx">fX</code></td>
<td>
<p>an optional vector containing the evaluations of <code>model</code> at <code>X</code>. If missing, <code>fX</code> is computed by evaluating <code>model</code> at <code>X</code>.</p>
</td></tr>
<tr><td><code id="support_+3A_gradfx">gradfX</code></td>
<td>
<p>an optional vector containing the evaluations of the gradient of <code>model</code> at <code>X</code>. If missing, <code>gradfX</code> is approximated by finite differences of <code>model</code> at <code>X</code>.</p>
</td></tr>
<tr><td><code id="support_+3A_h">h</code></td>
<td>
<p>a small number for computing finite differences <code>(f(X_i + h) - f(X_i))/h</code>. Default is <code>1e-6</code>.</p>
</td></tr>
<tr><td><code id="support_+3A_...">...</code></td>
<td>
<p>optional arguments to be passed to <code>model</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The first-order support index of <code>f(X)</code> relative to <code>X_i</code> is the squared conditional expectation of its partial derivative with respect to <code>X_i</code>.
</p>
<p>The total support index of <code>f(X)</code> relative to <code>X_i</code> is the conditional expectation of its squared partial derivative with respect to <code>X_i</code>.
</p>
<p>These two functions measure the local influence of <code>X_i</code>, in the global space of the other input variables. 
Up to square transformations, support indices can be viewed as regression curves of partial derivatives <code>df(X)/dX_i</code> with respect to <code>X_i</code>.
Estimation is performed by smoothing from the diagonal scatterplots <code>(X_i, df/dX_i)</code> with the function <code>smooth.spline{stats}</code> with the default options.
</p>
<p>For the sake of comparison, support index functions may be normalized. The proposed normalization is the sum of the DGSM, equal to the sum of the overall means of total support functions. 
Normalized support index functions can be plotted with the S3 method <code>plot</code>, as well as the underlying diagonal scatterplots of derivatives (S3 method <code>scatterplot</code>).
</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>main</code></td>
<td>
<p>a matrix whose columns contain the first-order support index functions, estimated at <code>Xnew</code>.</p>
</td></tr>
<tr><td><code>total</code></td>
<td>
<p>a matrix whose columns contain the total support index functions, estimated at <code>Xnew</code>.</p>
</td></tr>
<tr><td><code>DGSM</code></td>
<td>
<p>a vector containing an estimation of DGSM.</p>
</td></tr>
<tr><td><code>X</code></td>
<td>
<p>...</p>
</td></tr>
<tr><td><code>Xnew</code></td>
<td>
<p>...</p>
</td></tr>
<tr><td><code>fX</code></td>
<td>
<p>...</p>
</td></tr>
<tr><td><code>gradfX</code></td>
<td>
<p>... see 'arguments' section.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>O. Roustant
</p>


<h3>References</h3>

<p>J. Fruth, O. Roustant, S. Kuhnt, 2019, <em>Support indices: Measuring the effects 
of input variables over their support</em>, Reliability Engineering and System Safety,
187:17-27.
</p>


<h3>See Also</h3>

 
<p>S3 methods <code>plot</code> and <code>scatterplot</code>: <code><a href="#topic+plot.support">plot.support</a></code> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

# -----------------
# ishigami function
# -----------------
n &lt;- 5000
n.points &lt;- 1000
d &lt;- 3

set.seed(0)
X &lt;- matrix(runif(d*n, min = -pi, max = pi), n, d)
Xnew &lt;- matrix(seq(from = -pi, to = pi, length=n.points), n.points, d)

b &lt;- support(model = ishigami.fun, X, Xnew)

# plot method (x-axis in probability scale), of the normalized support index functions
plot(b, col = c("lightskyblue4", "lightskyblue1", "black"), 
     xprob = TRUE, p = 'punif', p.arg = list(min = -pi, max = pi), ylim = c(0, 2))

# below : diagonal scatterplots of the gradient, 
# on which are based the estimation by smoothing
scatterplot(b, xprob = TRUE) 

# now with normal margins
# -----------------------
X &lt;- matrix(rnorm(d*n), n, d)
Xnew &lt;- matrix(rnorm(d*n.points), n.points, d)
b &lt;- support(model = ishigami.fun, X, Xnew)

plot(b, col = c("lightskyblue4", "lightskyblue1", "black"), xprob = FALSE)
scatterplot(b, xprob = FALSE, type = "histogram", bins = 10, cex = 1, cex.lab = 1.5)

</code></pre>

<hr>
<h2 id='template.replace'>Replace Values in a Template Text</h2><span id='topic+template.replace'></span>

<h3>Description</h3>

<p><code>template.replace</code> replaces keys within special markups with
values in a so-called template file. Pieces of <span class="rlang"><b>R</b></span> code can be put into
the markups of the template file, and are evaluated during the
replacement.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>template.replace(text, replacement, eval = FALSE,
                 key.pattern = NULL, code.pattern = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="template.replace_+3A_text">text</code></td>
<td>
<p>vector of character strings, the template text.</p>
</td></tr>
<tr><td><code id="template.replace_+3A_replacement">replacement</code></td>
<td>
<p>the list values to replace in <code>text</code>.</p>
</td></tr>
<tr><td><code id="template.replace_+3A_eval">eval</code></td>
<td>
<p>boolean, <code>TRUE</code> if the code within
<code>code.pattern</code> has to be evaluated, <code>FALSE</code> otherwise.</p>
</td></tr>
<tr><td><code id="template.replace_+3A_key.pattern">key.pattern</code></td>
<td>
<p>custom pattern for key replacement (see below)</p>
</td></tr>
<tr><td><code id="template.replace_+3A_code.pattern">code.pattern</code></td>
<td>
<p>custom pattern for code replacement (see below)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In most cases, a computational code reads its inputs from a text
file. A template file is like an input file, but where some missing
values, identified with generic keys, will be replaced by specific
values.
</p>
<p>By default, the keys are enclosed into markups of the form <code>$(KEY)</code>.
</p>
<p>Code to be interpreted with <span class="rlang"><b>R</b></span> can be put in the template text. Pieces
of code must be enclosed into markups of the form
<code>@{CODE}</code>. This is useful for example for formating the key
values (see example). For interpreting the code, set <code>eval = TRUE</code>.
</p>
<p>Users can define custom patterns. These patterns must be
perl-compatible regular expressions (see <code><a href="base.html#topic+regexpr">regexpr</a></code>.
The default ones are:
</p>
<pre>key.pattern = "\\$\\(KEY\\)"
code.pattern = "@\\{CODE\\}"</pre>
<p>Note that special characters have to
be escaped both (one for perl, one for <span class="rlang"><b>R</b></span>).
</p>


<h3>Author(s)</h3>

<p>Gilles Pujol
</p>


<h3>Examples</h3>

<pre><code class='language-R'>txt &lt;- c("Hello $(name)!", "$(a) + $(b) = @{$(a)+$(b)}",
         "pi = @{format(pi,digits=5)}")
replacement &lt;- list(name = "world", a = 1, b = 2)
# 1. without code evaluation:
txt.rpl1 &lt;- template.replace(txt, replacement)
print(txt.rpl1)
# 2. with code evalutation:
txt.rpl2 &lt;- template.replace(txt, replacement, eval = TRUE)
print(txt.rpl2)
</code></pre>

<hr>
<h2 id='testHSIC'>Tests of Independence based on the Hilbert-Schmidt Independence Criterion (HSIC)</h2><span id='topic+testHSIC'></span><span id='topic+print.testHSIC'></span><span id='topic+plot.testHSIC'></span><span id='topic+seq.permutation.test.testHSIC'></span><span id='topic+gamma.test.testHSIC'></span><span id='topic+mean.nondiag.testHSIC'></span>

<h3>Description</h3>

 
<p><code>testHSIC</code> allows to test independence among all input-output pairs <code class="reqn">(Xi, Y)</code> after a preliminary sensitivity analysis based on <abbr><span class="acronym">HSIC</span></abbr> indices. <code>testHSIC</code> takes an object of class <code>sensiHSIC</code> (produced by a prior call to the function <code>sensiHSIC</code> that estimates <abbr><span class="acronym">HSIC</span></abbr> indices) and it returns the estimated p-values after testing independence among all input-output pairs. For each input-output pair, having access to the p-value helps the user decide whether the null hypothesis <code class="reqn">H0</code>: &quot;<code class="reqn">Xi</code> and <code class="reqn">Y</code> are independent&quot; must be accepted or rejected. If the kernels selected in <code>sensiHSIC</code> are all <b>characteristic</b>, <code class="reqn">H0</code> can be rewritten &quot;<code class="reqn">HSIC(Xi, Y)=0</code>&quot; and this paves the way to several test procedures.
</p>
<p>Depending on the sample size and the chosen test statistic (either a <b>U-statistic</b> or a <b>V-statistic</b>), there are up to four different methods to test <code class="reqn">H0</code>. The <b>asymptotic test</b> is recommended when the sample size <code class="reqn">n</code> is around a few hundreds (or more). When <code class="reqn">n</code> is smaller, a <b>permutation-based test</b> must be considered instead. As a general rule, permutation-based tests can always be applied but a much heavier computational load is to be expected. However, if <abbr><span class="acronym">HSIC</span></abbr> indices were initially estimated with V-statistics, the <b>Gamma test</b> is a parametric method that offers an enticing tradeoff.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>testHSIC(sensi, test.method = "Asymptotic", B = 3000,
         seq.options = list(criterion = "screening", alpha = 0.05,
                            Bstart = 200, Bfinal = 5000, Bbatch = 100, 
                            Bconv = 200, graph = TRUE) )

## S3 method for class 'testHSIC'
print(x, ...)

## S3 method for class 'testHSIC'
plot(x, ylim = c(0, 1), err, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="testHSIC_+3A_sensi">sensi</code></td>
<td>
<p>An object of class <code>"sensiHSIC"</code> which is produced by a prior call to the function <code>sensiHSIC</code>. In particular, <code>sensi</code> must contain objects named <code>"KX"</code> (3D-array filled with all input Gram matrices), <code>"KY"</code> (output Gram matrix), <code>"HSICXY"</code> (estimated <abbr><span class="acronym">HSIC</span></abbr> indices) and <code>"estimator.type"</code> (either <code>"U-stat"</code> or <code>"V-stat"</code>). In addition, if <code>sensi</code> results from a conditional sensitivity analysis, <code>sensi</code> must also contain objects named <code>"cond"</code> (list of options describing the conditioning event) and <code>"weights"</code> (normalized conditioning weights).</p>
</td></tr>
<tr><td><code id="testHSIC_+3A_test.method">test.method</code></td>
<td>
<p>A string specifying the numerical procedure used to estimate the p-values of the HSIC-based independence tests. Available procedure include <code>"Asymptotic"</code> (asymptotic test), <code>"Permutation"</code> (permutation-based test), <code>"Seq_Permutation"</code> (sequential permutation-based test) and <code>"Gamma"</code> (Gamma test). 
</p>

<ul>
<li><p> If <code>sensi</code> contains V-statistics, the asymptotic test (resp. the Gamma test) is recommended for large (resp. small) sample sizes. Otherwise, permutation-based tests can be used as well.
</p>
</li>
<li><p> If <code>sensi</code> contains U-statistics, the Gamma test must not be employed. The asymptotic test is recommended for large sample sizes. Otherwise, permutation-based tests can be used as well. 
</p>
</li></ul>

</td></tr>
<tr><td><code id="testHSIC_+3A_b">B</code></td>
<td>
<p>Number of random permutations carried out on the output samples before the non-parametric estimation of p-values. Only relevant if <code>test.method="Permutation"</code>.</p>
</td></tr>
<tr><td><code id="testHSIC_+3A_seq.options">seq.options</code></td>
<td>
<p>A list of options guiding the sequential procedure. 
Only relevant if <code>test.method="Seq_Permutation"</code>.
</p>

<ul>
<li> <p><code>criterion</code> is a string specifying the stopping criterion. Available criteria include <code>"screening"</code> (permutations stop as soons as the estimated p-values have sufficiently converged so that they can be compared to the reference threshold <code>alpha</code>), <code>"ranking"</code> (permutations stop as soon as the estimated p-values have sufficiently converged so that they can be ranked) and <code>"both"</code> (permutations stop as soon as the two previous criteria are fulfilled).
</p>
</li>
<li> <p><code>alpha</code> is a scalar value (between <code class="reqn">0</code> and <code class="reqn">1</code>) specifying the type I error (probability of wrongly accepting <code class="reqn">H0</code>). Only relevant if <code>criterion</code> is <code>"screening"</code> or <code>"both"</code>.
</p>
</li>
<li> <p><code>Bstart</code> is the initial number of random permutations before the first criterion check.
</p>
</li>
<li> <p><code>Bfinal</code> is the maximum number of random permutations.
</p>
</li>
<li> <p><code>Bbatch</code> is the number of permutations at each new iteration of the sequential procedure.
</p>
</li>
<li> <p><code>Bconv</code> is the number of permutations that is used to determine whether convergence has already occured or not. For <code>criterion="screening"</code>, convergence is assumed to be reached if the positions of the estimated p-values with respect to <code>alpha</code> no longer evolve after the <code>Bconv</code> latest permutations. For <code>criterion="ranking"</code>, convergence is assumed to be reached if the rankings of the estimated p-values no longer evolve after the <code>Bconv</code> latest permutations.
</p>
</li>
<li> <p><code>graph</code> is a boolean indicating whether the estimated p-values have to be plotted against the number of permutations.
</p>
</li></ul>

</td></tr>
<tr><td><code id="testHSIC_+3A_x">x</code></td>
<td>
<p>An object of class <code>"testHSIC"</code> storing the parameters and results of independence testing.</p>
</td></tr>
<tr><td><code id="testHSIC_+3A_ylim">ylim</code></td>
<td>
<p>A vector of two values specifying the y-coordinate plotting limits.</p>
</td></tr>
<tr><td><code id="testHSIC_+3A_err">err</code></td>
<td>
<p>A scalar value (between <code class="reqn">0</code> and <code class="reqn">1</code>) specifying the reference type I error. This value is used to plot a vertical line.</p>
</td></tr>
<tr><td><code id="testHSIC_+3A_...">...</code></td>
<td>
<p>Additional options.</p>
</td></tr>
</table>


<h3>Details</h3>



<h4>Why and how to properly choose kernels?</h4>

<p>For a given input-output pair of variables, the Hilbert-Schmidt independence criterion <abbr><span class="acronym">(HSIC)</span></abbr> is a dissimilarity measure between the joint bivariate distribution and the product of marginal distributions. Dissimilarity between those two distributions is measured through the squared norm of the distance between their respective embeddings in a reproducing kernel Hilbert space (<abbr><span class="acronym">RKHS</span></abbr>) that directly depends on the selected input kernel <code class="reqn">Ki</code> and the selected output kernel <code class="reqn">KY</code>.
</p>
<p>It must always be kept in mind that this criterion allows to detect independence within the pair <code class="reqn">(Xi, Y)</code> provided that the two kernels are <b>characteristic</b>.  
</p>

<ul>
<li><p> If both kernels are characteristic, <code class="reqn">H0</code>: &quot;<code class="reqn">Xi</code> and <code class="reqn">Y</code> are independent&quot; is equivalent to <code class="reqn">H0</code>: &quot;<code class="reqn">HSIC(Xi, Y)=0</code>&quot; and any estimator of <code class="reqn">HSIC(Xi, Y)</code> emerges as a relevant test statistic.
</p>
</li>
<li><p> If they are not, testing <code class="reqn">H0</code>: &quot;<code class="reqn">HSIC(Xi, Y)=0</code>&quot; is no longer sufficient for testing  <code class="reqn">H0</code>: &quot;<code class="reqn">Xi</code> and <code class="reqn">Y</code> are independent&quot;.
</p>
</li></ul>

<p>The reader is referred to Fukumizu et al. (2004) for the mathematical definition of a characteristic kernel and to Sriperumbur et al. (2010) for an overview of the major related results. 
</p>
<p>Responsability for kernel selection is left to the user while calling the function <code>sensiHSIC</code>. Let us simply recall that:
</p>

<ul>
<li><p> The Gaussian kernel, the exponential kernel, the Matern <code class="reqn">3/2</code> kernel and the Matern <code class="reqn">5/2</code> kernel (all defined on <code class="reqn">R^2</code>) are <b>characteristic</b>. They remain <b>characteristic</b> when they are restricted to a compact domain <code class="reqn">D</code> within <code class="reqn">R^2</code>.
</p>
</li>
<li><p> The transformed versions of the four abovementioned kernels (all defined on <code class="reqn">[0,1]^2</code>) are <b>characteristic</b>.
</p>
</li>
<li><p> All Sobolev kernels (defined on <code class="reqn">[0,1]^2</code>) are <b>characteristic</b>.
</p>
</li>
<li><p> The categorical kernel (defined on any discrete probability space) is <b>characteristic</b>.
</p>
</li></ul>




<h4>Which test method is most appropriate?</h4>

<p>The test statistic for the pair <code class="reqn">(Xi, Y)</code> is either the <b>U-statistic</b> or the <b>V-statistic</b> associated to <code class="reqn">HSIC(Xi, Y)</code>.
</p>
<p>If a <b>V-statistic</b> was used in <code>sensiHSIC</code>, four different test methods can be considered.
</p>

<ul>
<li><p> The <b>asymptotic test</b> can be used if the sample size <code class="reqn">n</code> is large enough (at least a hundred of samples). The asymptotic distribution of the test statistic is approximated by a Gamma distribution whose parameters are estimated with the method of moments. See Gretton et al. (2007) for more details about how to estimate the first two moments of the asymptotic Gamma distribution.
</p>
</li>
<li><p> The <b>permutation-based test</b> is more expensive in terms of computational cost but it can be used whatever the sample size <code class="reqn">n</code> is. The initial output samples (stored in the object of class <code>sensiHSIC</code>) are randomly permuted <code class="reqn">B</code> times and the test statistic is recomputed as many times. This allows to simulate <code class="reqn">B</code> observations of the test statistic under <code class="reqn">H0</code> and to estimate the p-value in a non-parametric way. See Meynaoui (2019) for more details on how to correctly estimate the p-value in order to preserve the expected level of the test.
</p>
</li>
<li><p> The <b>sequential permutation-based test</b> is a goal-oriented variant of the previous test. The main idea is to reduce the computational cost by stopping permutations as soon as the estimation of the p-value has sufficiently converged so that it can be compared to a reference threshold or be given a final ranking. See El Amri and Marrel (2022) for more details on how to implement this sequential approach for the three stopping criteria (namely <code>"ranking"</code>, <code>"screening"</code> or <code>"both"</code>).
</p>
</li>
<li><p> The <b>Gamma test</b> is a parametric alternative to permutation-based tests when <code class="reqn">n</code> is not large enough to resort to the asymptotic test. The permutation-based test reveals the test statistic under <code class="reqn">H0</code> follows a unimodal distribution having significant positive skewness. Thus, it seems quite natural to estimate the p-value with a Gamma distribution, especially in view of the fact that the asymptotic distribution is properly approximated by this parametric family. See El Amri and Marrel (2021) for more details on how to estimate the parameters of the Gamma distribution with the method of moments. In particular, the first two moments of the test statistic under <code class="reqn">H0</code> are computed thanks to the formulas that were initially provided in Kazi-Aoual et al. (1995).
</p>
</li></ul>

<p>If a <b>U-statistic</b> was used in <code>sensiHSIC</code>, the estimated value of <code class="reqn">HSIC(Xi,Y)</code> may be negative. 
</p>

<ul>
<li><p> The <b>asymptotic test</b> can no longer be conducted with a Gamma distribution (whose support is limited to <code class="reqn">[0,+\infty[</code>). It is replaced by a Pearson III  distribution (which is a left-shifted Gamma distribution).
</p>
</li>
<li><p> The <b>permutation-based test</b> and the <b>sequential permutation-based test</b> can be applied directly.
</p>
</li>
<li><p> The <b>Gamma test</b> has no longer any theoretical justification.
</p>
</li></ul>




<h4>What about target and conditional <abbr><span class="acronym">HSIC</span></abbr> indices?</h4>

<p>In Marrel and Chabridon (2021), <abbr><span class="acronym">HSIC</span></abbr> indices were adapted to <b>target sensitivity analysis</b> (thus becoming <abbr><span class="acronym">T-HSIC</span></abbr> indices) and to <b>conditional sensitivity analysis</b> (thus becoming C-HSIC indices). Tests of independence can still be useful after estimating <abbr><span class="acronym">T-HSIC</span></abbr> indices or <abbr><span class="acronym">C-HSIC</span></abbr> indices.
</p>

<ul>
<li><p> For <abbr><span class="acronym">T-HSIC</span></abbr> indices, the null hypothesis is <code class="reqn">H0</code>: &quot;<code class="reqn">Xi</code> and <code class="reqn">w(Y)</code> are independent&quot; where <code class="reqn">w</code> is the weight function selected in <code>target</code> and passed to the function <code class="reqn">sensiHSIC</code>. Everything works just as for basic <abbr><span class="acronym">HSIC</span></abbr> indices (apart from the fact that <code class="reqn">w</code> is applied on the original output variable <code class="reqn">Y</code>). Available test methods include <code>"Asymptotic"</code>, <code>"Permutation"</code>, <code>"Seq_Permutation"</code> and <code>"Gamma"</code> (for V-statistics only).
</p>
</li>
<li><p> For <abbr><span class="acronym">C-HSIC</span></abbr> indices, the null hypothesis is <code class="reqn">H0</code>: &quot;<code class="reqn">Xi</code> and <code class="reqn">Y</code> are independent if the event described in <code>cond</code> occurs&quot;. In this specific context, testing conditional independence is only relevant if the weight function is an indicator function. For this reason, if conditional independence has to be tested, the user must select <code>type="indicTh"</code> in <code>cond</code> while calling the function <code>sensiHSIC</code>. Let us recall that only V-statistic estimators can be used for <abbr><span class="acronym">C-HSIC</span></abbr> indices. As a result, available test methods include <code>"Asymptotic"</code>, <code>"Permutation"</code>, <code>"Seq_Permutation"</code> and <code>"Gamma"</code>.
</p>
</li></ul>




<h3>Value</h3>

<p><code>testHSIC</code> returns a list of class <code>"testHSIC"</code>. It contains <code>test.method</code>, <code>B</code> (for the permutation-based test), <code>seq.options</code> (for the sequential permutation-based test) and the following objects:
</p>
<table role = "presentation">
<tr><td><code>call</code></td>
<td>
<p>The matched call.</p>
</td></tr>
<tr><td><code>pval</code></td>
<td>
<p>The estimated p-values after testing independence for all input-output pairs.</p>
</td></tr>
<tr><td><code>prop</code></td>
<td>
<p>A vector of two strings.
</p>

<ul>
<li><p> The first string indicates if the chosen test method is asymptotic or non-asymptotic.
</p>
</li>
<li><p> The second string indicates if the chosen test method is parametric or non-parametric.
</p>
</li></ul>

</td></tr>
<tr><td><code>family</code></td>
<td>
<p>Only if <code>test.method</code> is <code>"Asymptotic"</code> or <code>"Gamma"</code>. <br />
A string indicating the parametric family used to estimate p-values.</p>
</td></tr>
<tr><td><code>param</code></td>
<td>
<p>Only if <code>test.method</code> is <code>"Asymptotic"</code> or <code>"Gamma"</code>. <br /> 
A <code class="reqn">2</code>-column (resp. <code class="reqn">3</code>-column) matrix containing the parameters of the Gamma (resp. Pearson III) distributions used to estimate p-values.</p>
</td></tr>
<tr><td><code>Hperm</code></td>
<td>
<p>Only if <code>test.method="Permutation"</code>. <br />
A <code class="reqn">B</code>-column matrix containing simulated values of the test statistics after randomly permuting the output samples. Each column in <code>Hperm</code> corresponds to one random permutation.</p>
</td></tr>
<tr><td><code>paths</code></td>
<td>
<p>Only if <code>test.method="Seq_Permutation"</code>. <br />
A matrix containing all estimated p-values over the sequential test procedure. The <code class="reqn">i</code>-th row provides all estimates of the <code class="reqn">i</code>-th p-value as the number of permutations increases. If one row ends with a sequence of missing values <code>NA</code>, it means permutations were stopped earlier for this input variable. This can only happen if <code>test.method=screening</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Sebastien Da Veiga, Amandine Marrel, Anouar Meynaoui, Reda El Amri and Gabriel Sarazin.
</p>


<h3>References</h3>

<p>El Amri, M. R. and Marrel, A. (2022), <em>Optimized <abbr><span class="acronym">HSIC</span></abbr>-based tests for sensitivity analysis: application to thermalhydraulic simulation of accidental scenario on nuclear reactor</em>, Quality and Reliability Engineering International, 38(3), 1386-1403.
</p>
<p>El Amri, M. R. and Marrel, A. (2021), <em>More powerful <abbr><span class="acronym">HSIC</span></abbr>-based independence tests, extension to space-filling designs and functional data</em>.
<a href="https://cea.hal.science/cea-03406956/">https://cea.hal.science/cea-03406956/</a>
</p>
<p>Fukumizu, K., Bach, F. R. and Jordan, M. I. (2004), <em>Dimensionality reduction for supervised learning with reproducing kernel Hilbert spaces</em>, Journal of Machine Learning Research, 5(Jan), 73-99.
</p>
<p>Gretton, A., Fukumizu, K., Teo, C., Song, L., Scholkopf, B. and Smola, A. (2007), <em>A kernel statistical test of independence</em>, Advances in Neural Information Processing Systems, 20.
</p>
<p>Kazi-Aoual, F., Hitier, S., Sabatier, R. and Lebreton, J. D. (1995), <em>Refined approximations to permutation tests for multivariate inference</em>, Computational Statistics &amp; Data Analysis, 20(6), 643-656.
</p>
<p>Marrel, A. and Chabridon, V. (2021), <em>Statistical developments for target and conditional sensitivity analysis: application on safety studies for nuclear reactor</em>, Reliability Engineering &amp; System Safety, 214, 107711.
</p>
<p>Meynaoui, A. (2019), <em>New developments around dependence measures for sensitivity analysis: application to severe accident studies for generation IV reactors</em> (Doctoral dissertation, INSA de Toulouse).
</p>
<p>Sriperumbudur, B., Fukumizu, K. and Lanckriet, G. (2010), <em>On the relation between universality, characteristic kernels and <abbr><span class="acronym">RKHS</span></abbr> embedding of measures</em>, Proceedings of the 13th International Conference on Artificial Intelligence and Statistics (pp. 773-780). JMLR Workshop and Conference Proceedings.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+sensiHSIC">sensiHSIC</a>, <a href="#topic+weightTSA">weightTSA</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  

# Test case: the Ishigami function.

n &lt;- 20   # very few input-output samples
p &lt;- 3    # nb of input variables

########################################
### PRELIMINARY SENSITIVITY ANALYSIS ###
########################################

X &lt;- matrix(runif(n*p), n, p)
sensi &lt;- sensiHSIC(model=ishigami.fun, X)
print(sensi)
plot(sensi)
title("GSA for the Ishigami function")

#############################
### TESTS OF INDEPENDENCE ###
#############################

test.asymp &lt;- testHSIC(sensi)

test.perm &lt;- testHSIC(sensi, test.method="Permutation")

test.seq.screening &lt;- testHSIC(sensi, test.method="Seq_Permutation")

test.seq.ranking &lt;- testHSIC(sensi, test.method="Seq_Permutation", 
                             seq.options=list(criterion="ranking"))

test.seq.both &lt;- testHSIC(sensi, test.method="Seq_Permutation", 
                          seq.options=list(criterion="both"))

test.gamma &lt;- testHSIC(sensi, test.method="Gamma")

# comparison of p-values

res &lt;- rbind( t(as.matrix(test.asymp$pval)), t(as.matrix(test.perm$pval)), 
              t(as.matrix(test.seq.screening$pval)), t(as.matrix(test.seq.ranking$pval)),
              t(as.matrix(test.seq.both$pval)), t(as.matrix(test.gamma$pval)) )

rownames(res) &lt;- c("asymp", "perm", "seq_perm_screening", 
                   "seq_perm_ranking", "seq_perm_both", "gamma")
res

# Conclusion: n is too small for the asymptotic test.
# Take n=200 and all four test methods will provide very close p-values.

#####################
### VISUALIZATION ###
#####################

# simulated values of HSIC indices under H0 (random permutations)
Hperm &lt;- t(unname(test.perm$Hperm))

for(i in 1:p){
  
  # histogram of the test statistic under H0 (random permutations)
  
  title &lt;- paste0("Histogram of S", i, " = HSIC(X", i, ",Y)")
  
  hist(Hperm[,i], probability=TRUE,
       nclass=70, main=title, xlab="", ylab="", col="cyan")
  
  # asymptotic Gamma distribution
  
  shape.asymp &lt;- test.asymp$param[i, "shape"]
  scale.asymp &lt;- test.asymp$param[i, "scale"]
  
  xx &lt;- seq(0, max(Hperm[,i]), length.out=200)
  dens.asymp &lt;- dgamma(xx, shape=shape.asymp, scale=scale.asymp)
  
  lines(xx, dens.asymp, lwd=2, col="darkorchid")
  
  # finite-sample Gamma distribution
  
  shape.perm &lt;- test.gamma$param[i, "shape"]
  scale.perm &lt;- test.gamma$param[i, "scale"]
  
  dens.perm &lt;- dgamma(xx, shape=shape.perm, scale=scale.perm)
  
  lines(xx, dens.perm, lwd=2, col="blue")
  
  all.cap &lt;- c("Asymptotic Gamma distribution", "Finite-sample Gamma distribution")
  all.col &lt;- c("darkorchid", "blue")
  
  legend("topright", legend=all.cap, col=all.col, lwd=2, y.intersp=1.3)

}
 
</code></pre>

<hr>
<h2 id='testmodels'>Test Models for Sensitivity Analysis</h2><span id='topic+testmodels'></span><span id='topic+sobol.fun'></span><span id='topic+ishigami.fun'></span><span id='topic+morris.fun'></span><span id='topic+atantemp.fun'></span><span id='topic+campbell1D.fun'></span><span id='topic+linkletter.fun'></span><span id='topic+heterdisc.fun'></span><span id='topic+friedman.fun'></span><span id='topic+matyas.fun'></span>

<h3>Description</h3>

<p>These functions are standard testcases for sensitivity analysis
benchmarks. For a scalar output 
(see Saltelli et al. 2000 and https://www.sfu.ca/~ssurjano/):
</p>

<ul>
<li><p> the g-function of Sobol' with 8 inputs, X ~ U[0,1];
</p>
</li>
<li><p> the function  of Ishigami with 3 inputs, X ~ U[-pi,pi];
</p>
</li>
<li><p> the function of Morris with 20 inputs, X ~ U[0,1];
</p>
</li>
<li><p> the Linkletter  decreasing coefficients function, X ~ U[0,1] 
(Linkletter et al. (2006));
</p>
</li>
<li><p> the heterdisc function with 4 inputs, X ~ U[0,20];
</p>
</li>
<li><p> the Friedman function with 5 inputs, X ~ U[0,1] 
(Friedman, 1991);
</p>
</li>
<li><p> the Matyas function with 2 inputs, X ~ U[0,1].
</p>
</li></ul>

<p>For functional output cases:
</p>

<ul>
<li><p> the Arctangent temporal function with 2 inputs, X ~ U[-7,7] 
(Auder, 2011). The functional support is on [0,2pi];
</p>
</li>
<li><p> the Cambell1D function with 4 inputs, X ~U[-1,5] 
(Campbell et al. 2006). The functional support is on [-90,90].
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>sobol.fun(X)
ishigami.fun(X)
morris.fun(X)
atantemp.fun(X, q = 100)
campbell1D.fun(X, theta = -90:90)
linkletter.fun(X)
heterdisc.fun(X)
friedman.fun(X)
matyas.fun(X)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="testmodels_+3A_x">X</code></td>
<td>
<p>a matrix (or <code>data.frame</code>) containing the input
sample.</p>
</td></tr>
<tr><td><code id="testmodels_+3A_q">q</code></td>
<td>
<p>for the atantemp() function: 
the number of discretization steps of the functional output</p>
</td></tr>
<tr><td><code id="testmodels_+3A_theta">theta</code></td>
<td>
<p>for the campbell1D() function: 
the discretization steps (angles in degrees)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of function responses.
</p>


<h3>Author(s)</h3>

<p>Gilles Pujol and Bertrand Iooss
</p>


<h3>References</h3>

<p>A. Saltelli, K. Chan and E. M. Scott eds, 2000, <em>Sensitivity   Analysis</em>, Wiley.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

# Examples for the functional toy fonctions

# atantemp function

y0 &lt;- atantemp.fun(matrix(c(-7,0,7,-7,0,7),ncol=2))
plot(y0[1,],type="l")
apply(y0,1,lines)

n &lt;- 100
X &lt;- matrix(c(runif(2*n,-7,7)),ncol=2)
y &lt;- atantemp.fun(X)
plot(y0[2,],ylim=c(-2,2),type="l")
apply(y,1,lines)

# campbell1D function

N1=100         # nombre de simulations pour courbes 1D
min=-1 ; max=5
nominal=(max+min)/2

X1 = NULL ; y1 = NULL
Xnom=matrix(nominal,nr=1,nc=4)
ynom=campbell1D.fun(Xnom,theta=-90:90)
plot(ynom,ylim=c(8,30),type="l",col="red")
for (i in 1:N1){
  X=matrix(runif(4,min=min,max=max),nr=1,nc=4)
  rbind(X1,X)
  y=campbell1D.fun(X,theta=-90:90)
  rbind(y1,y)
  lines(y)
}


</code></pre>

<hr>
<h2 id='truncateddistrib'>Truncated distributions</h2><span id='topic+truncateddistrib'></span><span id='topic+dnorm.trunc'></span><span id='topic+pnorm.trunc'></span><span id='topic+qnorm.trunc'></span><span id='topic+rnorm.trunc'></span><span id='topic+dgumbel.trunc'></span><span id='topic+pgumbel.trunc'></span><span id='topic+qgumbel.trunc'></span><span id='topic+rgumbel.trunc'></span>

<h3>Description</h3>

<p><code>dnorm.trunc</code>, <code>pnorm.trunc</code>, <code>qnorm.trunc</code> and 
<code>rnorm.trunc</code> are functions for the Truncated Normal Distribution.
<code>dgumbel.trunc</code>, <code>pgumbel.trunc</code>, <code>qgumbel.trunc</code> and 
<code>rgumbel.trunc</code> are functions for the Truncated Gumbel Distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dnorm.trunc(x, mean = 0, sd = 1, min = -1e6, max = 1e6)
pnorm.trunc(q, mean = 0, sd = 1, min = -1e6, max = 1e6)
qnorm.trunc(p, mean = 0, sd = 1, min = -1e6, max = 1e6)
rnorm.trunc(n, mean = 0, sd = 1, min = -1e6, max = 1e6)
dgumbel.trunc(x, loc = 0, scale = 1, min = -1e6, max = 1e6)
pgumbel.trunc(q, loc = 0, scale = 1, min = -1e6, max = 1e6)
qgumbel.trunc(p, loc = 0, scale = 1, min = -1e6, max = 1e6)
rgumbel.trunc(n, loc = 0, scale = 1, min = -1e6, max = 1e6)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="truncateddistrib_+3A_x">x</code>, <code id="truncateddistrib_+3A_q">q</code></td>
<td>
<p>vector of quantiles</p>
</td></tr>
<tr><td><code id="truncateddistrib_+3A_p">p</code></td>
<td>
<p>vector of probabilities</p>
</td></tr>
<tr><td><code id="truncateddistrib_+3A_n">n</code></td>
<td>
<p>number of observations</p>
</td></tr>
<tr><td><code id="truncateddistrib_+3A_mean">mean</code>, <code id="truncateddistrib_+3A_sd">sd</code></td>
<td>
<p>means and standard deviation parameters</p>
</td></tr>
<tr><td><code id="truncateddistrib_+3A_loc">loc</code>, <code id="truncateddistrib_+3A_scale">scale</code></td>
<td>
<p>location and scale parameters</p>
</td></tr>
<tr><td><code id="truncateddistrib_+3A_min">min</code></td>
<td>
<p>vector of minimal bound values</p>
</td></tr>
<tr><td><code id="truncateddistrib_+3A_max">max</code></td>
<td>
<p>vector of maximal bound values</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See <code>dnorm</code> for details on the Normal distribution.
The Gumbel distribution comes from the evd package.
See <code>dgumbel</code> for details on the Gumbel distribution.
</p>


<h3>Value</h3>

<p><code>dnorm.trunc</code> and <code>dgumbel.trunc</code> give the density, <code>pnorm</code> and <code>pgumbel.trunc</code> give the distribution function, <code>qnorm</code> and <code>qgumbel.trunc</code> give the quantile function, <code>rnorm</code> and <code>rgumbel.trunc</code> generate random deviates.
</p>


<h3>Author(s)</h3>

<p>Gilles Pujol and Bertrand Iooss
</p>

<hr>
<h2 id='weightTSA'>
Weight-function to transform an output variable in order to perform Target Sensitivity Analysis (TSA)
</h2><span id='topic+weightTSA'></span>

<h3>Description</h3>

<p>Transformation function of one variable (vector sample)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>weightTSA(Y, c, upper = TRUE, type="indicTh", param=1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="weightTSA_+3A_y">Y</code></td>
<td>
<p>The output vector</p>
</td></tr>
<tr><td><code id="weightTSA_+3A_c">c</code></td>
<td>
<p>The threshold</p>
</td></tr>
<tr><td><code id="weightTSA_+3A_upper">upper</code></td>
<td>
<p>TRUE for upper threshold and FALSE for lower threshold</p>
</td></tr>
<tr><td><code id="weightTSA_+3A_type">type</code></td>
<td>
<p>The weight function type (&quot;indicTh&quot;, &quot;zeroTh&quot;, logistic&quot;, &quot;exp1side&quot;):
</p>

<ul>
<li><p> indicTh : indicator-thresholding
</p>
</li>
<li><p> zeroTh : zero-thresholding (keeps the variable value above (upper=TRUE case) or below the threshold)
</p>
</li>
<li><p> logistic : logistic transformation at the threshold
</p>
</li>
<li><p> exp1side : exponential transformation above (upper=TRUE case) or below the threshold (see Raguet and Marrel)
</p>
</li></ul>
</td></tr>
<tr><td><code id="weightTSA_+3A_param">param</code></td>
<td>
<p>The parameter value for &quot;logistic&quot; and &quot;exp1side&quot; types</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The weight functions depend on a threshold <code class="reqn">c</code> and/or a smooth relaxation. These functions are defined as follows
</p>

<ul>
<li><p> if type = &quot;indicTh&quot;: <code class="reqn">w = 1_{Y&gt;c}</code> (upper threshold) and <code class="reqn">w = 1_{Y&lt;c}</code> (lower threshold),
</p>
</li>
<li><p> if type = &quot;zeroTh&quot;: <code class="reqn">w = Y 1_{Y&gt;c}</code> (upper threshold) and <code class="reqn">w = Y 1_{Y&lt;c}</code> (lower threshold),
</p>
</li>
<li><p> if type = &quot;logistic&quot;: </p>
<p style="text-align: center;"><code class="reqn">w = \left[ 1 + \exp{\left( -param\frac{Y-c}{|c|}\right)}\right]^{-1}</code>
</p>
<p> (upper threshold) and </p>
<p style="text-align: center;"><code class="reqn">w = \left[ 1 + \exp{\left( -param\frac{c-Y}{|c|}\right)}\right]^{-1}</code>
</p>
<p> (lower threshold),
</p>
</li>
<li><p> if type = &quot;exp1side&quot;: </p>
<p style="text-align: center;"><code class="reqn">w = \left[ 1 + \exp{\left( -\frac{\max(c - Y, 0)}{\frac{param}{5} \sigma(Y)}\right)}\right]</code>
</p>
<p> (upper threshold) and </p>
<p style="text-align: center;"><code class="reqn">w = \left[ 1 + \exp{\left( -\frac{\max(Y - c, 0)}{\frac{param}{5} \sigma(Y)}\right) }\right]</code>
</p>
<p> (lower threshold), where <code class="reqn">\sigma(Y)</code> is an estimation of the standard deviation of Y and <code class="reqn">param = 1</code> is a parameter tuning the smoothness.
</p>
</li></ul>



<h3>Value</h3>

<p>The vector sample of the transformed variable
</p>


<h3>Author(s)</h3>

<p>B. Iooss
</p>


<h3>References</h3>

<p>H. Raguet and A. Marrel, <em>Target and conditional sensitivity analysis with emphasis
on dependence measures</em>, Preprint, https://hal.archives-ouvertes.fr/hal-01694129
</p>
<p>A. Marrel and V. Chabridon, 2021, <em>Statistical developments for target and conditional 
sensitivity analysis: Application on safety studies for nuclear reactor</em>, 
Reliability Engineering &amp; System Safety, 214:107711.
</p>
<p>A. Spagnol, <em>Kernel-based sensitivity indices for high-dimensional optimization problems</em>,
PhD Thesis, Universite de Lyon, 2020
</p>
<p>Spagnol A., Le Riche R., Da Veiga S. (2019), <em>Global sensitivity analysis for optimization 
with variable selection</em>, SIAM/ASA J. Uncertainty Quantification, 7(2), 417&ndash;443.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 100  # sample size
c &lt;- 1.5
Y &lt;- rnorm(n)
Yt &lt;- weightTSA(Y, c)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
