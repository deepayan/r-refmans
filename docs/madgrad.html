<!DOCTYPE html><html><head><title>Help for package madgrad</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {madgrad}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#optim_madgrad'><p>A Momentumized, Adaptive, Dual Averaged Gradient Method for Stochastic</p>
Optimization.</a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>'MADGRAD' Method for Stochastic Optimization</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.0</td>
</tr>
<tr>
<td>Description:</td>
<td>A Momentumized, Adaptive, Dual Averaged Gradient Method for Stochastic 
  Optimization algorithm. MADGRAD is a 'best-of-both-worlds' optimizer with the 
  generalization performance of stochastic gradient descent and at least as fast 
  convergence as that of Adam, often faster. A drop-in optim_madgrad() implementation
  is provided based on Defazio et al (2020) &lt;<a href="https://arxiv.org/abs/2101.11075">arXiv:2101.11075</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.1</td>
</tr>
<tr>
<td>Imports:</td>
<td>torch (&ge; 0.3.0), rlang</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat (&ge; 3.0.0)</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2021-05-07 14:17:39 UTC; dfalbel</td>
</tr>
<tr>
<td>Author:</td>
<td>Daniel Falbel [aut, cre, cph],
  RStudio [cph],
  MADGRAD original implementation authors. [cph]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Daniel Falbel &lt;daniel@rstudio.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2021-05-10 07:30:12 UTC</td>
</tr>
</table>
<hr>
<h2 id='optim_madgrad'>A Momentumized, Adaptive, Dual Averaged Gradient Method for Stochastic
Optimization.</h2><span id='topic+optim_madgrad'></span>

<h3>Description</h3>

<p><a href="https://arxiv.org/abs/2101.11075">MADGRAD</a> is a general purpose optimizer that
can be used in place of SGD or Adam may converge faster and generalize better.
Currently GPU-only. Typically, the same learning rate schedule that is used
for SGD or Adam may be used. The overall learning rate is not comparable to
either method and should be determined by a hyper-parameter sweep.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optim_madgrad(params, lr = 0.01, momentum = 0.9, weight_decay = 0, eps = 1e-06)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="optim_madgrad_+3A_params">params</code></td>
<td>
<p>(list): List of parameters to optimize.</p>
</td></tr>
<tr><td><code id="optim_madgrad_+3A_lr">lr</code></td>
<td>
<p>(float): Learning rate (default: 1e-2).</p>
</td></tr>
<tr><td><code id="optim_madgrad_+3A_momentum">momentum</code></td>
<td>
<p>(float): Momentum value in  the range [0,1) (default: 0.9).</p>
</td></tr>
<tr><td><code id="optim_madgrad_+3A_weight_decay">weight_decay</code></td>
<td>
<p>(float): Weight decay, i.e. a L2 penalty (default: 0).</p>
</td></tr>
<tr><td><code id="optim_madgrad_+3A_eps">eps</code></td>
<td>
<p>(float): Term added to the denominator outside of the root operation to improve numerical stability. (default: 1e-6).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>MADGRAD requires less weight decay than other methods, often as little as
zero. Momentum values used for SGD or Adam's beta1 should work here also.
</p>
<p>On sparse problems both weight_decay and momentum should be set to 0.
(not yet supported in the R implementation).
</p>


<h3>Value</h3>

<p>An optimizer object implementing the <code>step</code> and <code>zero_grad</code> methods.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch::torch_is_installed()) {
library(torch)
x &lt;- torch_randn(1, requires_grad = TRUE)
opt &lt;- optim_madgrad(x)
for (i in 1:100) {
  opt$zero_grad()
  y &lt;- x^2
  y$backward()
  opt$step()
}
all.equal(x$item(), 0, tolerance = 1e-9)
}

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
