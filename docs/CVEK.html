<!DOCTYPE html><html><head><title>Help for package CVEK</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {CVEK}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#compute_info'><p>Computing Information Matrices</p></a></li>
<li><a href='#compute_stat'><p>Computing Score Test Statistics.</p></a></li>
<li><a href='#cvek'><p>Conducting Cross-validated Kernel Ensemble</p></a></li>
<li><a href='#cvek_test'><p>Conduct Hypothesis Testing</p></a></li>
<li><a href='#define_library'><p>Defining Kernel Library</p></a></li>
<li><a href='#ensemble'><p>Estimating Ensemble Kernel Matrices</p></a></li>
<li><a href='#ensemble_avg'><p>Estimating Ensemble Kernel Matrices Using AVG</p></a></li>
<li><a href='#ensemble_exp'><p>Estimating Ensemble Kernel Matrices Using EXP</p></a></li>
<li><a href='#ensemble_kernel_matrix'><p>Calculating Ensemble Kernel Matrix</p></a></li>
<li><a href='#ensemble_stack'><p>Estimating Ensemble Kernel Matrices Using Stack</p></a></li>
<li><a href='#estimate_base'><p>Estimating Projection Matrices</p></a></li>
<li><a href='#estimate_ridge'><p>Estimating a Single Model</p></a></li>
<li><a href='#estimate_sigma2'><p>Estimating Noise</p></a></li>
<li><a href='#estimation'><p>Conducting Gaussian Process Regression</p></a></li>
<li><a href='#euc_dist'><p>Computing Euclidean Distance between Two Vectors (Matrices)</p></a></li>
<li><a href='#generate_kernel'><p>Generating A Single Kernel</p></a></li>
<li><a href='#kernel_intercept'><p>Generating A Single Matrix-wise Function Using Intercept</p></a></li>
<li><a href='#kernel_linear'><p>Generating A Single Matrix-wise Function Using Linear</p></a></li>
<li><a href='#kernel_matern'><p>Generating A Single Matrix-wise Function Using Matern</p></a></li>
<li><a href='#kernel_nn'><p>Generating A Single Matrix-wise Function Using Neural Network</p></a></li>
<li><a href='#kernel_polynomial'><p>Generating A Single Matrix-wise Function Using Polynomial</p></a></li>
<li><a href='#kernel_rational'><p>Generating A Single Matrix-wise Function Using Rational Quadratic</p></a></li>
<li><a href='#kernel_rbf'><p>Generating A Single Matrix-wise Function Using RBF</p></a></li>
<li><a href='#parse_cvek_formula'><p>Parsing User-supplied Formula</p></a></li>
<li><a href='#parse_kernel_terms'><p>Compute Kernel Matrix</p></a></li>
<li><a href='#parse_kernel_variable'><p>Create Kernel Matrix</p></a></li>
<li><a href='#predict.cvek'><p>Predicting New Response</p></a></li>
<li><a href='#square_dist'><p>Computing Square Distance between Two Sets of Variables</p></a></li>
<li><a href='#standardize'><p>Standardizing Matrix</p></a></li>
<li><a href='#test_asymp'><p>Conducting Score Tests for Interaction Using Asymptotic Test</p></a></li>
<li><a href='#test_boot'><p>Conducting Score Tests for Interaction Using Bootstrap Test</p></a></li>
<li><a href='#tuning'><p>Calculating Tuning Parameters</p></a></li>
<li><a href='#tuning_AIC'><p>Calculating Tuning Parameters Using AIC</p></a></li>
<li><a href='#tuning_AICc'><p>Calculating Tuning Parameters Using AICc</p></a></li>
<li><a href='#tuning_BIC'><p>Calculating Tuning Parameters Using BIC</p></a></li>
<li><a href='#tuning_GCV'><p>Calculating Tuning Parameters Using GCV</p></a></li>
<li><a href='#tuning_GCVc'><p>Calculating Tuning Parameters Using GCVc</p></a></li>
<li><a href='#tuning_gmpml'><p>Calculating Tuning Parameters Using GMPML</p></a></li>
<li><a href='#tuning_loocv'><p>Calculating Tuning Parameters Using looCV</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Cross-Validated Kernel Ensemble</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1-2</td>
</tr>
<tr>
<td>Date:</td>
<td>2020-11-27</td>
</tr>
<tr>
<td>Description:</td>
<td>
  Implementation of Cross-Validated Kernel Ensemble (CVEK), 
  a flexible modeling framework for robust nonlinear regression and 
  hypothesis testing based on ensemble learning with kernel-ridge estimators 
  (Jeremiah et al. (2017) &lt;<a href="https://arxiv.org/abs/1710.01406">arXiv:1710.01406</a>&gt; and 
  Wenying et al. (2018) &lt;<a href="https://arxiv.org/abs/1811.11025">arXiv:1811.11025</a>&gt;). It allows user to conduct 
  nonlinear regression with minimal assumption on the function form by 
  aggregating nonlinear models generated from a diverse collection of kernel 
  families. It also provides utilities to test for the estimated nonlinear 
  effect under this ensemble estimator, using either the asymptotic or 
  the bootstrap version of a generalized score test.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.6.0), MASS, limSolve</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.1</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat, knitr, rmarkdown, ggplot2, ggrepel</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2020-11-28 04:23:15 UTC; irisdeng</td>
</tr>
<tr>
<td>Author:</td>
<td>Wenying Deng [aut, cre],
  Jeremiah Zhe Liu [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Wenying Deng &lt;wdeng@g.harvard.edu&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2020-12-18 16:10:18 UTC</td>
</tr>
</table>
<hr>
<h2 id='compute_info'>Computing Information Matrices</h2><span id='topic+compute_info'></span>

<h3>Description</h3>

<p>Compute information matrices based on block matrices.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compute_info(P0_mat, mat_del = NULL, mat_sigma2 = NULL, mat_tau = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="compute_info_+3A_p0_mat">P0_mat</code></td>
<td>
<p>(matrix, n*n) Scale projection matrix under REML.</p>
</td></tr>
<tr><td><code id="compute_info_+3A_mat_del">mat_del</code></td>
<td>
<p>(matrix, n*n) Derivative of the scale covariance matrix of Y
with respect to delta.</p>
</td></tr>
<tr><td><code id="compute_info_+3A_mat_sigma2">mat_sigma2</code></td>
<td>
<p>(matrix, n*n) Derivative of the scale covariance matrix of
Y with respect to sigma2.</p>
</td></tr>
<tr><td><code id="compute_info_+3A_mat_tau">mat_tau</code></td>
<td>
<p>(matrix, n*n) Derivative of the scale covariance matrix of Y
with respect to tau.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function gives the information value of the interaction strength.
</p>


<h3>Value</h3>

<table>
<tr><td><code>I0</code></td>
<td>
<p>(matrix, n*n) The computed information value.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Wenying Deng
</p>


<h3>References</h3>

<p>Arnab Maity and Xihong Lin. Powerful tests for detecting a gene
effect in the presence of possible gene-gene interactions using garrote
kernel machines. December 2011.
</p>

<hr>
<h2 id='compute_stat'>Computing Score Test Statistics.</h2><span id='topic+compute_stat'></span>

<h3>Description</h3>

<p>Compute score test statistics.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compute_stat(Y, K_int, y_fixed, K0, sigma2_hat, tau_hat)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="compute_stat_+3A_y">Y</code></td>
<td>
<p>(matrix, n*1) The vector of response variable.</p>
</td></tr>
<tr><td><code id="compute_stat_+3A_k_int">K_int</code></td>
<td>
<p>(matrix, n*n) The kernel matrix to be tested.</p>
</td></tr>
<tr><td><code id="compute_stat_+3A_y_fixed">y_fixed</code></td>
<td>
<p>(vector of length n) Estimated fixed effect of the
response.</p>
</td></tr>
<tr><td><code id="compute_stat_+3A_sigma2_hat">sigma2_hat</code></td>
<td>
<p>(numeric) The estimated noise of the fixed effect.</p>
</td></tr>
<tr><td><code id="compute_stat_+3A_tau_hat">tau_hat</code></td>
<td>
<p>(numeric) The estimated noise of the kernel effect.</p>
</td></tr>
<tr><td><code id="compute_stat_+3A_k_0">K_0</code></td>
<td>
<p>(matrix, n*n) Estimated ensemble kernel matrix.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The test statistic is distributed as a scaled Chi-squared distribution.
</p>


<h3>Value</h3>

<table>
<tr><td><code>test_stat</code></td>
<td>
<p>(numeric) The computed test statistic.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Wenying Deng
</p>


<h3>References</h3>

<p>Arnab Maity and Xihong Lin. Powerful tests for detecting a gene
effect in the presence of possible gene-gene interactions using garrote
kernel machines. December 2011.
</p>

<hr>
<h2 id='cvek'>Conducting Cross-validated Kernel Ensemble</h2><span id='topic+cvek'></span>

<h3>Description</h3>

<p>Conducting Cross-validated Kernel Ensemble based on user-specified formula.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cvek(
  formula,
  kern_func_list,
  data,
  formula_test = NULL,
  mode = "loocv",
  strategy = "stack",
  beta_exp = 1,
  lambda = exp(seq(-10, 5)),
  test = "boot",
  alt_kernel_type = "linear",
  B = 100,
  verbose = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cvek_+3A_formula">formula</code></td>
<td>
<p>(formula) A user-supplied formula for the null model. Should 
contain at least one kernel term.</p>
</td></tr>
<tr><td><code id="cvek_+3A_kern_func_list">kern_func_list</code></td>
<td>
<p>(list) A list of kernel functions in the kernel library.</p>
</td></tr>
<tr><td><code id="cvek_+3A_data">data</code></td>
<td>
<p>(data.frame, n*d) A data.frame, list or environment (or object
coercible by as.data.frame to a data.frame), containing the variables in
formula. Neither a matrix nor an array will be accepted.</p>
</td></tr>
<tr><td><code id="cvek_+3A_formula_test">formula_test</code></td>
<td>
<p>(formula) A user-supplied formula indicating the alternative
effect to test. All terms in the alternative mode must be specified as kernel terms.</p>
</td></tr>
<tr><td><code id="cvek_+3A_mode">mode</code></td>
<td>
<p>(character) A character string indicating which tuning parameter
criteria is to be used.</p>
</td></tr>
<tr><td><code id="cvek_+3A_strategy">strategy</code></td>
<td>
<p>(character) A character string indicating which ensemble
strategy is to be used.</p>
</td></tr>
<tr><td><code id="cvek_+3A_beta_exp">beta_exp</code></td>
<td>
<p>(numeric/character) A numeric value specifying the parameter
when strategy = &quot;exp&quot; <code><a href="#topic+ensemble_exp">ensemble_exp</a></code>.</p>
</td></tr>
<tr><td><code id="cvek_+3A_lambda">lambda</code></td>
<td>
<p>(numeric) A numeric string specifying the range of 
tuning parameter to be chosen. The lower limit of lambda must be above 0.</p>
</td></tr>
<tr><td><code id="cvek_+3A_test">test</code></td>
<td>
<p>(character) Type of hypothesis test to conduct. Must be either 
'asymp' or 'boot'.</p>
</td></tr>
<tr><td><code id="cvek_+3A_alt_kernel_type">alt_kernel_type</code></td>
<td>
<p>(character) Type of alternative kernel effect to consider.
Must be either &quot;linear&quot; or &quot;ensemble&quot;.</p>
</td></tr>
<tr><td><code id="cvek_+3A_b">B</code></td>
<td>
<p>(numeric) Number of bootstrap samples.</p>
</td></tr>
<tr><td><code id="cvek_+3A_verbose">verbose</code></td>
<td>
<p>(logical) Whether to print additional messages.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Perform Cross-validated Kernel Ensemble and optionally test for kernel effect
based on user-specified formula.
</p>


<h3>Value</h3>

<table>
<tr><td><code>lambda</code></td>
<td>
<p>(numeric) The selected tuning parameter based on the
estimated ensemble kernel matrix.</p>
</td></tr>
<tr><td><code>beta</code></td>
<td>
<p>(matrix, d_fixed*1) Fixed effect estimates.</p>
</td></tr>
<tr><td><code>alpha</code></td>
<td>
<p>(matrix, n*1) Kernel effect estimates.</p>
</td></tr>
<tr><td><code>K</code></td>
<td>
<p>(matrix, n*n) Estimated ensemble kernel matrix.</p>
</td></tr>
<tr><td><code>u_hat</code></td>
<td>
<p>(vector of length K) A vector of weights of the kernels in the
library.</p>
</td></tr>
<tr><td><code>base_est</code></td>
<td>
<p>(list) The detailed estimation results of K kernels.</p>
</td></tr>
<tr><td><code>pvalue</code></td>
<td>
<p>(numeric) If formula_test is given, p-value of the test is returned.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Jeremiah Zhe Liu
</p>


<h3>References</h3>

<p>Xihong Lin. Variance component testing in generalised linear
models with random effects. June 1997.
</p>
<p>Arnab Maity and Xihong Lin. Powerful tests for detecting a gene effect in
the presence of possible gene-gene interactions using garrote kernel
machines. December 2011.
</p>
<p>Petra Bu z kova, Thomas Lumley, and Kenneth Rice. Permutation and
parametric bootstrap tests for gene-gene and gene-environment interactions.
January 2011.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+estimation">estimation</a></code>
</p>
<p>method: <code><a href="#topic+generate_kernel">generate_kernel</a></code>
</p>
<p>mode: <code><a href="#topic+tuning">tuning</a></code>
</p>
<p>strategy: <code><a href="#topic+ensemble">ensemble</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
kern_par &lt;- data.frame(method = rep("rbf", 3),
l = rep(3, 3), p = rep(2, 3), 
stringsAsFactors = FALSE)
# define kernel library
kern_func_list &lt;- define_library(kern_par)

n &lt;- 10
d &lt;- 4
formula &lt;- y ~ x1 + x2 + k(x3, x4)
formula_test &lt;- y ~ k(x1, x2) * k(x3, x4)
set.seed(1118)
data &lt;- as.data.frame(matrix(
  rnorm(n * d),
  ncol = d,
  dimnames = list(NULL, paste0("x", 1:d))
))
beta_true &lt;- c(1, .41, 2.37)
lnr_kern_func &lt;- generate_kernel(method = "rbf", l = 3)
kern_effect_lnr &lt;-
  parse_kernel_variable("k(x3, x4)", lnr_kern_func, data)
alpha_lnr_true &lt;- rnorm(n)

data$y &lt;- as.matrix(cbind(1, data[, c("x1", "x2")])) %*% beta_true +
  kern_effect_lnr %*% alpha_lnr_true

data_train &lt;- data

pvalue &lt;- cvek(formula,
               kern_func_list,
               data_train,
               formula_test,
               mode = "loocv",
               strategy = "stack",
               beta_exp = 1,
               lambda = exp(seq(-2, 2)),
               test = "asymp",
               alt_kernel_type = "linear",
               verbose = FALSE)$pvalue

</code></pre>

<hr>
<h2 id='cvek_test'>Conduct Hypothesis Testing</h2><span id='topic+cvek_test'></span>

<h3>Description</h3>

<p>Conduct hypothesis testing based on CVEK estimation result.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cvek_test(
  est_res,
  formula_test,
  kern_func_list,
  data,
  test = "boot",
  alt_kernel_type = "linear",
  B = 100,
  verbose = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cvek_test_+3A_est_res">est_res</code></td>
<td>
<p>(list) Estimation results returned by estimation() procedure.</p>
</td></tr>
<tr><td><code id="cvek_test_+3A_formula_test">formula_test</code></td>
<td>
<p>(formula) A user-supplied formula indicating the alternative
effect to test. All terms in the alternative mode must be specified as kernel terms.</p>
</td></tr>
<tr><td><code id="cvek_test_+3A_kern_func_list">kern_func_list</code></td>
<td>
<p>(list) A list of kernel functions in the kernel library</p>
</td></tr>
<tr><td><code id="cvek_test_+3A_data">data</code></td>
<td>
<p>(data.frame, n*d) A data.frame, list or environment (or object
coercible by as.data.frame to a data.frame), containing the variables in
formula. Neither a matrix nor an array will be accepted.</p>
</td></tr>
<tr><td><code id="cvek_test_+3A_test">test</code></td>
<td>
<p>(character) Type of hypothesis test to conduct.
Must be either 'asymp' or 'boot'.</p>
</td></tr>
<tr><td><code id="cvek_test_+3A_alt_kernel_type">alt_kernel_type</code></td>
<td>
<p>(character) Type of alternative kernel effect to consider.
Must be either 'linear' or 'ensemble'</p>
</td></tr>
<tr><td><code id="cvek_test_+3A_b">B</code></td>
<td>
<p>(integer) A numeric value indicating times of resampling when test
= &quot;boot&quot;.</p>
</td></tr>
<tr><td><code id="cvek_test_+3A_verbose">verbose</code></td>
<td>
<p>(logical) Whether to print additional messages.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Conduct score tests comparing a fitted model and a more general alternative
model.
</p>
<p>There are two tests available here:
</p>
<p><b>Asymptotic Test</b>
</p>
<p>This is based on the classical variance component test to construct a
testing procedure for the hypothesis about Gaussian process function.
</p>
<p><b>Bootstrap Test</b>
</p>
<p>When it comes to small sample size, we can use bootstrap test instead, which
can give valid tests with moderate sample sizes and requires similar
computational effort to a permutation test. In the case of bootstrap test, 
we can specify the form of the null derivative kernel (alt_kernel_type) to be 
linear or ensemble. The bootstrap test allows the null derivative 
kernel to be estimated adaptively from data using the same corresponding 
ensemble weights to better represent the alternative hypothesis space.
</p>


<h3>Value</h3>

<table>
<tr><td><code>pvalue</code></td>
<td>
<p>(numeric) p-value of the test.</p>
</td></tr>
</table>

<hr>
<h2 id='define_library'>Defining Kernel Library</h2><span id='topic+define_library'></span>

<h3>Description</h3>

<p>Generate the expected kernel library based on user-specified dataframe.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>define_library(kern_par = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="define_library_+3A_kern_par">kern_par</code></td>
<td>
<p>(dataframe, K*3) A dataframe indicating the parameters of
base kernels to fit kernel effect. See Details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>It creates a kernel library according to the parameters given in kern_par.
</p>
<p>* kern_par: for a library of K kernels, the dimension of this dataframe is
K*3. Each row represents a kernel. The first column is method, with entries
of character class. The second and the third are l and p respectively, both
with entries of numeric class.
</p>


<h3>Value</h3>

<table>
<tr><td><code>kern_func_list</code></td>
<td>
<p>(list of length K) A list of kernel functions 
given by user. Will be overwritten to linear kernel if kern_par is NULL.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Wenying Deng
</p>


<h3>See Also</h3>

<p>method: <code><a href="#topic+generate_kernel">generate_kernel</a></code>
</p>

<hr>
<h2 id='ensemble'>Estimating Ensemble Kernel Matrices</h2><span id='topic+ensemble'></span>

<h3>Description</h3>

<p>Give the ensemble projection matrix and weights of the kernels in the
library.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ensemble(strategy, beta_exp, error_mat, A_hat)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ensemble_+3A_strategy">strategy</code></td>
<td>
<p>(character) A character string indicating which ensemble
strategy is to be used.</p>
</td></tr>
<tr><td><code id="ensemble_+3A_beta_exp">beta_exp</code></td>
<td>
<p>(numeric/character) A numeric value specifying the parameter
when strategy = &quot;exp&quot; <code><a href="#topic+ensemble_exp">ensemble_exp</a></code>.</p>
</td></tr>
<tr><td><code id="ensemble_+3A_error_mat">error_mat</code></td>
<td>
<p>(matrix, n*K) A n\*K matrix indicating errors.</p>
</td></tr>
<tr><td><code id="ensemble_+3A_a_hat">A_hat</code></td>
<td>
<p>(list of length K) A list of projection matrices to kernel space 
for each kernel in the kernel library.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>There are three ensemble strategies available here:
</p>
<p><b>Empirical Risk Minimization (Stacking)</b>
</p>
<p>After obtaining the estimated errors <code class="reqn">\{\hat{\epsilon}_d\}_{d=1}^D</code>, we
estimate the ensemble weights <code class="reqn">u=\{u_d\}_{d=1}^D</code> such that it minimizes
the overall error </p>
<p style="text-align: center;"><code class="reqn">\hat{u}={argmin}_{u \in \Delta}\parallel
\sum_{d=1}^Du_d\hat{\epsilon}_d\parallel^2 \quad where\; \Delta=\{u | u \geq
0, \parallel u \parallel_1=1\}</code>
</p>
<p> Then produce the final ensemble prediction:
</p>
<p style="text-align: center;"><code class="reqn">\hat{h}=\sum_{d=1}^D \hat{u}_d h_d=\sum_{d=1}^D \hat{u}_d
A_{d,\hat{\lambda}_d}y=\hat{A}y</code>
</p>
<p> where <code class="reqn">\hat{A}=\sum_{d=1}^D \hat{u}_d
A_{d,\hat{\lambda}_d}</code> is the ensemble matrix.
</p>
<p><b>Simple Averaging</b>
</p>
<p>Motivated by existing literature in omnibus kernel, we propose another way
to obtain the ensemble matrix by simply choosing unsupervised weights
<code class="reqn">u_d=1/D</code> for <code class="reqn">d=1,2,...D</code>.
</p>
<p><b>Exponential Weighting</b>
</p>
<p>Additionally, another scholar gives a new strategy to calculate weights
based on the estimated errors <code class="reqn">\{\hat{\epsilon}_d\}_{d=1}^D</code>.
</p>
<p style="text-align: center;"><code class="reqn">u_d(\beta)=\frac{exp(-\parallel \hat{\epsilon}_d
\parallel_2^2/\beta)}{\sum_{d=1}^Dexp(-\parallel \hat{\epsilon}_d
\parallel_2^2/\beta)}</code>
</p>



<h3>Value</h3>

<table>
<tr><td><code>A_est</code></td>
<td>
<p>(matrix, n*n) The ensemble projection matrix.</p>
</td></tr>
<tr><td><code>u_hat</code></td>
<td>
<p>(vector of length K) A vector of weights of the kernels in the
library.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Wenying Deng
</p>


<h3>References</h3>

<p>Jeremiah Zhe Liu and Brent Coull. Robust Hypothesis Test for
Nonlinear Effect with Gaussian Processes. October 2017.
</p>
<p>Xiang Zhan, Anna Plantinga, Ni Zhao, and Michael C. Wu. A fast small-sample
kernel independence test for microbiome community-level association
analysis. December 2017.
</p>
<p>Arnak S. Dalalyan and Alexandre B. Tsybakov. Aggregation by Exponential
Weighting and Sharp Oracle Inequalities. In Learning Theory, Lecture Notes
in Computer Science, pages 97– 111. Springer, Berlin, Heidelberg, June 2007.
</p>


<h3>See Also</h3>

<p>mode: <code><a href="#topic+tuning">tuning</a></code>
</p>

<hr>
<h2 id='ensemble_avg'>Estimating Ensemble Kernel Matrices Using AVG</h2><span id='topic+ensemble_avg'></span>

<h3>Description</h3>

<p>Give the ensemble projection matrix and weights of the kernels in the
library using simple averaging.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ensemble_avg(beta_exp, error_mat, A_hat)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ensemble_avg_+3A_beta_exp">beta_exp</code></td>
<td>
<p>(numeric/character) A numeric value specifying the parameter
when strategy = &quot;exp&quot; <code><a href="#topic+ensemble_exp">ensemble_exp</a></code>.</p>
</td></tr>
<tr><td><code id="ensemble_avg_+3A_error_mat">error_mat</code></td>
<td>
<p>(matrix, n*K) A n\*K matrix indicating errors.</p>
</td></tr>
<tr><td><code id="ensemble_avg_+3A_a_hat">A_hat</code></td>
<td>
<p>(list of length K) A list of projection matrices to kernel space 
for each kernel in the kernel library.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><b>Simple Averaging</b>
</p>
<p>Motivated by existing literature in omnibus kernel, we propose another way
to obtain the ensemble matrix by simply choosing unsupervised weights
<code class="reqn">u_d=1/D</code> for <code class="reqn">d=1,2,...D</code>.
</p>


<h3>Value</h3>

<table>
<tr><td><code>A_est</code></td>
<td>
<p>(matrix, n*n) The ensemble projection matrix.</p>
</td></tr>
<tr><td><code>u_hat</code></td>
<td>
<p>(vector of length K) A vector of weights of the kernels in the
library.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Wenying Deng
</p>


<h3>References</h3>

<p>Jeremiah Zhe Liu and Brent Coull. Robust Hypothesis Test for
Nonlinear Effect with Gaussian Processes. October 2017.
</p>
<p>Xiang Zhan, Anna Plantinga, Ni Zhao, and Michael C. Wu. A fast small-sample
kernel inde- pendence test for microbiome community-level association
analysis. December 2017.
</p>
<p>Arnak S. Dalalyan and Alexandre B. Tsybakov. Aggregation by Exponential
Weighting and Sharp Oracle Inequalities. In Learning Theory, Lecture Notes
in Computer Science, pages 97– 111. Springer, Berlin, Heidelberg, June 2007.
</p>


<h3>See Also</h3>

<p>mode: <code><a href="#topic+tuning">tuning</a></code>
</p>

<hr>
<h2 id='ensemble_exp'>Estimating Ensemble Kernel Matrices Using EXP</h2><span id='topic+ensemble_exp'></span>

<h3>Description</h3>

<p>Give the ensemble projection matrix and weights of the kernels in the
library using exponential weighting.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ensemble_exp(beta_exp, error_mat, A_hat)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ensemble_exp_+3A_beta_exp">beta_exp</code></td>
<td>
<p>(numeric/character) A numeric value specifying the parameter
when strategy = &quot;exp&quot;. See Details.</p>
</td></tr>
<tr><td><code id="ensemble_exp_+3A_error_mat">error_mat</code></td>
<td>
<p>(matrix, n*K) A n\*K matrix indicating errors.</p>
</td></tr>
<tr><td><code id="ensemble_exp_+3A_a_hat">A_hat</code></td>
<td>
<p>(list of length K) A list of projection matrices to kernel space 
for each kernel in the kernel library.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><b>Exponential Weighting</b>
</p>
<p>Additionally, another scholar gives a new strategy to calculate weights
based on the estimated errors <code class="reqn">\{\hat{\epsilon}_d\}_{d=1}^D</code>.
</p>
<p style="text-align: center;"><code class="reqn">u_d(\beta)=\frac{exp(-\parallel \hat{\epsilon}_d
\parallel_2^2/\beta)}{\sum_{d=1}^Dexp(-\parallel \hat{\epsilon}_d
\parallel_2^2/\beta)}</code>
</p>

<p><b>beta_exp</b>
</p>
<p>The value of beta_exp can be &quot;min&quot;=<code class="reqn">min\{RSS\}_{d=1}^D/10</code>,
&quot;med&quot;=<code class="reqn">median\{RSS\}_{d=1}^D</code>, &quot;max&quot;=<code class="reqn">max\{RSS\}_{d=1}^D*2</code> and any
other positive numeric number, where <code class="reqn">\{RSS\} _{d=1}^D</code> are the set of
residual sum of squares of <code class="reqn">D</code> base kernels.
</p>


<h3>Value</h3>

<table>
<tr><td><code>A_est</code></td>
<td>
<p>(matrix, n*n) The ensemble projection matrix.</p>
</td></tr>
<tr><td><code>u_hat</code></td>
<td>
<p>(vector of length K) A vector of weights of the kernels in the
library.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Wenying Deng
</p>


<h3>References</h3>

<p>Jeremiah Zhe Liu and Brent Coull. Robust Hypothesis Test for
Nonlinear Effect with Gaussian Processes. October 2017.
</p>
<p>Xiang Zhan, Anna Plantinga, Ni Zhao, and Michael C. Wu. A fast small-sample
kernel inde- pendence test for microbiome community-level association
analysis. December 2017.
</p>
<p>Arnak S. Dalalyan and Alexandre B. Tsybakov. Aggregation by Exponential
Weighting and Sharp Oracle Inequalities. In Learning Theory, Lecture Notes
in Computer Science, pages 97– 111. Springer, Berlin, Heidelberg, June 2007.
</p>


<h3>See Also</h3>

<p>mode: <code><a href="#topic+tuning">tuning</a></code>
</p>

<hr>
<h2 id='ensemble_kernel_matrix'>Calculating Ensemble Kernel Matrix</h2><span id='topic+ensemble_kernel_matrix'></span>

<h3>Description</h3>

<p>Calculating ensemble kernel matrix and truncating those columns whose
eigenvalues are smaller than the given threshold.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ensemble_kernel_matrix(A_est, eig_thres = 1e-11)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ensemble_kernel_matrix_+3A_a_est">A_est</code></td>
<td>
<p>(matrix) Ensemble projection matrix.</p>
</td></tr>
<tr><td><code id="ensemble_kernel_matrix_+3A_eig_thres">eig_thres</code></td>
<td>
<p>(numeric) Threshold to truncate the kernel matrix.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>After we obtain the ensemble projection matrix, we can calculate the
ensemble kernel matrix.
</p>


<h3>Value</h3>

<table>
<tr><td><code>K_hat</code></td>
<td>
<p>(matrix, n*n) Estimated ensemble kernel matrix.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Wenying Deng
</p>

<hr>
<h2 id='ensemble_stack'>Estimating Ensemble Kernel Matrices Using Stack</h2><span id='topic+ensemble_stack'></span>

<h3>Description</h3>

<p>Give the ensemble projection matrix and weights of the kernels in the
library using stacking.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ensemble_stack(beta_exp, error_mat, A_hat)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ensemble_stack_+3A_beta_exp">beta_exp</code></td>
<td>
<p>(numeric/character) A numeric value specifying the parameter
when strategy = &quot;exp&quot; <code><a href="#topic+ensemble_exp">ensemble_exp</a></code>.</p>
</td></tr>
<tr><td><code id="ensemble_stack_+3A_error_mat">error_mat</code></td>
<td>
<p>(matrix, n*K) A n\*K matrix indicating errors.</p>
</td></tr>
<tr><td><code id="ensemble_stack_+3A_a_hat">A_hat</code></td>
<td>
<p>(list of length K) A list of projection matrices to kernel space 
for each kernel in the kernel library.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><b>Empirical Risk Minimization (Stacking)</b>
</p>
<p>After obtaining the estimated errors <code class="reqn">\{\hat{\epsilon}_d\}_{d=1}^D</code>, we
estimate the ensemble weights <code class="reqn">u=\{u_d\}_{d=1}^D</code> such that it minimizes
the overall error </p>
<p style="text-align: center;"><code class="reqn">\hat{u}={argmin}_{u \in \Delta}\parallel
\sum_{d=1}^Du_d\hat{\epsilon}_d\parallel^2 \quad where\; \Delta=\{u | u \geq
0, \parallel u \parallel_1=1\}</code>
</p>
<p> Then produce the final ensemble prediction:
</p>
<p style="text-align: center;"><code class="reqn">\hat{h}=\sum_{d=1}^D \hat{u}_d h_d=\sum_{d=1}^D \hat{u}_d
A_{d,\hat{\lambda}_d}y=\hat{A}y</code>
</p>
<p> where <code class="reqn">\hat{A}=\sum_{d=1}^D \hat{u}_d
A_{d,\hat{\lambda}_d}</code> is the ensemble matrix.
</p>


<h3>Value</h3>

<table>
<tr><td><code>A_est</code></td>
<td>
<p>(matrix, n*n) The ensemble projection matrix.</p>
</td></tr>
<tr><td><code>u_hat</code></td>
<td>
<p>(vector of length K) A vector of weights of the kernels in the
library.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Wenying Deng
</p>


<h3>References</h3>

<p>Jeremiah Zhe Liu and Brent Coull. Robust Hypothesis Test for
Nonlinear Effect with Gaussian Processes. October 2017.
</p>
<p>Xiang Zhan, Anna Plantinga, Ni Zhao, and Michael C. Wu. A fast small-sample
kernel inde- pendence test for microbiome community-level association
analysis. December 2017.
</p>
<p>Arnak S. Dalalyan and Alexandre B. Tsybakov. Aggregation by Exponential
Weighting and Sharp Oracle Inequalities. In Learning Theory, Lecture Notes
in Computer Science, pages 97– 111. Springer, Berlin, Heidelberg, June 2007.
</p>


<h3>See Also</h3>

<p>mode: <code><a href="#topic+tuning">tuning</a></code>
</p>

<hr>
<h2 id='estimate_base'>Estimating Projection Matrices</h2><span id='topic+estimate_base'></span>

<h3>Description</h3>

<p>Calculate the estimated projection matrices for every kernels in the kernel
library.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>estimate_base(Y, X, K_list, mode, lambda, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="estimate_base_+3A_y">Y</code></td>
<td>
<p>(matrix, n*1) The vector of response variable.</p>
</td></tr>
<tr><td><code id="estimate_base_+3A_x">X</code></td>
<td>
<p>(matrix, n*d_fix) The fixed effect matrix.</p>
</td></tr>
<tr><td><code id="estimate_base_+3A_k_list">K_list</code></td>
<td>
<p>(list of matrices) A nested list of kernel term matrices. 
The first level corresponds to each base kernel function in kern_func_list, 
the second level corresponds to each kernel term specified in the formula.</p>
</td></tr>
<tr><td><code id="estimate_base_+3A_mode">mode</code></td>
<td>
<p>(character) A character string indicating which tuning parameter
criteria is to be used.</p>
</td></tr>
<tr><td><code id="estimate_base_+3A_lambda">lambda</code></td>
<td>
<p>(numeric) A numeric string specifying the range of tuning 
parameter to be chosen. The lower limit of lambda must be above 0.</p>
</td></tr>
<tr><td><code id="estimate_base_+3A_...">...</code></td>
<td>
<p>Additional parameters to pass to estimate_ridge.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For a given mode, this function returns a list of projection matrices for
every kernel in the kernel library and a n*K matrix indicating errors.
</p>


<h3>Value</h3>

<table>
<tr><td><code>A_hat</code></td>
<td>
<p>(list of length K) A list of projection matrices for
each kernel in the kernel library.</p>
</td></tr>
<tr><td><code>P_K_hat</code></td>
<td>
<p>(list of length K) A list of projection matrices to kernel
space for each kernel in the kernel library.</p>
</td></tr>
<tr><td><code>beta_list</code></td>
<td>
<p>(list of length K) A list of fixed effect estimators for
each kernel in the kernel library.</p>
</td></tr>
<tr><td><code>alpha_list</code></td>
<td>
<p>(list of length K) A list of kernel effect estimates for
each kernel in the kernel library.</p>
</td></tr>
<tr><td><code>kern_term_list</code></td>
<td>
<p>(list of length K) A list of kernel effects for
each kernel in the kernel library.</p>
</td></tr>
<tr><td><code>A_proc_list</code></td>
<td>
<p>(list of length K) A list of projection matrices for
each kernel in the kernel library.</p>
</td></tr>
<tr><td><code>lambda_list</code></td>
<td>
<p>(list of length K) A list of selected tuning parameters for
each kernel in the kernel library.</p>
</td></tr>
<tr><td><code>error_mat</code></td>
<td>
<p>(matrix, n*K) A n\*K matrix indicating errors.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Wenying Deng
</p>


<h3>References</h3>

<p>Jeremiah Zhe Liu and Brent Coull. Robust Hypothesis Test for
Nonlinear Effect with Gaussian Processes. October 2017.
</p>

<hr>
<h2 id='estimate_ridge'>Estimating a Single Model</h2><span id='topic+estimate_ridge'></span>

<h3>Description</h3>

<p>Estimating projection matrices and parameter estimates for a single model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>estimate_ridge(
  Y,
  X,
  K,
  lambda,
  compute_kernel_terms = TRUE,
  converge_thres = 1e-04
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="estimate_ridge_+3A_y">Y</code></td>
<td>
<p>(matrix, n*1) The vector of response variable.</p>
</td></tr>
<tr><td><code id="estimate_ridge_+3A_x">X</code></td>
<td>
<p>(matrix, n*d_fix) The fixed effect matrix.</p>
</td></tr>
<tr><td><code id="estimate_ridge_+3A_k">K</code></td>
<td>
<p>(list of matrices) A nested list of kernel term matrices, 
corresponding to each kernel term specified in the formula for 
a base kernel function in kern_func_list.</p>
</td></tr>
<tr><td><code id="estimate_ridge_+3A_lambda">lambda</code></td>
<td>
<p>(numeric) A numeric string specifying the range of tuning parameter 
to be chosen. The lower limit of lambda must be above 0.</p>
</td></tr>
<tr><td><code id="estimate_ridge_+3A_compute_kernel_terms">compute_kernel_terms</code></td>
<td>
<p>(logic) Whether to computing effect for each individual terms.
If FALSE then only compute the overall effect.</p>
</td></tr>
<tr><td><code id="estimate_ridge_+3A_converge_thres">converge_thres</code></td>
<td>
<p>(numeric) The convergence threshold for computing kernel terms.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For a single model, we can calculate the output of gaussian process
regression, the solution is given by </p>
<p style="text-align: center;"><code class="reqn">\hat{\beta}=[X^T(K+\lambda
I)^{-1}X]^{-1}X^T(K+\lambda I)^{-1}y</code>
</p>
 <p style="text-align: center;"><code class="reqn">\hat{\alpha}=(K+\lambda
I)^{-1}(y-\hat{\beta}X)</code>
</p>
<p>.
</p>


<h3>Value</h3>

<table>
<tr><td><code>beta</code></td>
<td>
<p>(matrix, d_fixed*1) Fixed effect estimates.</p>
</td></tr>
<tr><td><code>alpha</code></td>
<td>
<p>(matrix, n*k_terms) Kernel effect estimates for each kernel term.</p>
</td></tr>
<tr><td><code>kern_term_mat</code></td>
<td>
<p>(matrix, n*k_terms) Kernel effect for each kernel term.</p>
</td></tr>
<tr><td><code>A_list</code></td>
<td>
<p>(list of length k_terms) Projection matrices for each kernel term.</p>
</td></tr>
<tr><td><code>proj_matrix</code></td>
<td>
<p>(list of length 4) Estimated projection matrices, combined 
across kernel terms.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Wenying Deng
</p>


<h3>References</h3>

<p>Andreas Buja, Trevor Hastie, and Robert Tibshirani. (1989) 
Linear Smoothers and Additive Models. Ann. Statist. Volume 17, Number 2, 453-510.
</p>

<hr>
<h2 id='estimate_sigma2'>Estimating Noise</h2><span id='topic+estimate_sigma2'></span>

<h3>Description</h3>

<p>An implementation of Gaussian processes for estimating noise.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>estimate_sigma2(Y, X, lambda_hat, y_fixed_hat, alpha_hat, K_hat)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="estimate_sigma2_+3A_y">Y</code></td>
<td>
<p>(matrix, n*1) The vector of response variable.</p>
</td></tr>
<tr><td><code id="estimate_sigma2_+3A_x">X</code></td>
<td>
<p>(matrix, n*d_fix) The fixed effect matrix.</p>
</td></tr>
<tr><td><code id="estimate_sigma2_+3A_lambda_hat">lambda_hat</code></td>
<td>
<p>(numeric) The selected tuning parameter based on the
estimated ensemble kernel matrix.</p>
</td></tr>
<tr><td><code id="estimate_sigma2_+3A_y_fixed_hat">y_fixed_hat</code></td>
<td>
<p>(vector of length n) Estimated fixed effect of the
response.</p>
</td></tr>
<tr><td><code id="estimate_sigma2_+3A_alpha_hat">alpha_hat</code></td>
<td>
<p>(vector of length n) Kernel effect estimators of the
estimated ensemble kernel matrix.</p>
</td></tr>
<tr><td><code id="estimate_sigma2_+3A_k_hat">K_hat</code></td>
<td>
<p>(matrix, n*n) Estimated ensemble kernel matrix.</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>sigma2_hat</code></td>
<td>
<p>(numeric) The estimated noise of the fixed
effect.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Wenying Deng
</p>


<h3>References</h3>

<p>Jeremiah Zhe Liu and Brent Coull. Robust Hypothesis Test for
Nonlinear Effect with Gaussian Processes. October 2017.s
</p>

<hr>
<h2 id='estimation'>Conducting Gaussian Process Regression</h2><span id='topic+estimation'></span>

<h3>Description</h3>

<p>Conduct Gaussian process regression based on the estimated ensemble kernel
matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>estimation(
  Y,
  X,
  K_list = NULL,
  mode = "loocv",
  strategy = "stack",
  beta_exp = 1,
  lambda = exp(seq(-10, 5)),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="estimation_+3A_y">Y</code></td>
<td>
<p>(matrix, n*1) The vector of response variable.</p>
</td></tr>
<tr><td><code id="estimation_+3A_x">X</code></td>
<td>
<p>(matrix, n*d_fix) The fixed effect matrix.</p>
</td></tr>
<tr><td><code id="estimation_+3A_k_list">K_list</code></td>
<td>
<p>(list of matrices) A nested list of kernel term matrices. 
The first level corresponds to each base kernel function in kern_func_list, 
the second level corresponds to each kernel term specified in the formula.</p>
</td></tr>
<tr><td><code id="estimation_+3A_mode">mode</code></td>
<td>
<p>(character) A character string indicating which tuning parameter
criteria is to be used.</p>
</td></tr>
<tr><td><code id="estimation_+3A_strategy">strategy</code></td>
<td>
<p>(character) A character string indicating which ensemble
strategy is to be used.</p>
</td></tr>
<tr><td><code id="estimation_+3A_beta_exp">beta_exp</code></td>
<td>
<p>(numeric/character) A numeric value specifying the parameter
when strategy = &quot;exp&quot; <code><a href="#topic+ensemble_exp">ensemble_exp</a></code>.</p>
</td></tr>
<tr><td><code id="estimation_+3A_lambda">lambda</code></td>
<td>
<p>(numeric) A numeric string specifying the range of tuning 
parameter to be chosen. The lower limit of lambda must be above 0.</p>
</td></tr>
<tr><td><code id="estimation_+3A_...">...</code></td>
<td>
<p>Additional parameters to pass to estimate_ridge.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>After obtaining the ensemble kernel matrix, we can calculate the output of
Gaussian process regression.
</p>


<h3>Value</h3>

<table>
<tr><td><code>lambda</code></td>
<td>
<p>(numeric) The selected tuning parameter based on the
estimated ensemble kernel matrix.</p>
</td></tr>
<tr><td><code>beta</code></td>
<td>
<p>(matrix, d_fixed*1) Fixed effect estimates.</p>
</td></tr>
<tr><td><code>alpha</code></td>
<td>
<p>(matrix, n*1) Kernel effect estimates.</p>
</td></tr>
<tr><td><code>K</code></td>
<td>
<p>(matrix, n*n) Estimated ensemble kernel matrix.</p>
</td></tr>
<tr><td><code>u_hat</code></td>
<td>
<p>(vector of length K) A vector of weights of the kernels in the
library.</p>
</td></tr>
<tr><td><code>kern_term_effect</code></td>
<td>
<p>(matrix, n*n) Estimated ensemble kernel effect matrix.</p>
</td></tr>
<tr><td><code>base_est</code></td>
<td>
<p>(list) The detailed estimation results of K kernels.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Wenying Deng
</p>


<h3>See Also</h3>

<p>strategy: <code><a href="#topic+ensemble">ensemble</a></code>
</p>

<hr>
<h2 id='euc_dist'>Computing Euclidean Distance between Two Vectors (Matrices)</h2><span id='topic+euc_dist'></span>

<h3>Description</h3>

<p>Compute the L2 distance between two vectors or matrices.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>euc_dist(x1, x2 = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="euc_dist_+3A_x1">x1</code></td>
<td>
<p>(vector/matrix) The first vector/matrix.</p>
</td></tr>
<tr><td><code id="euc_dist_+3A_x2">x2</code></td>
<td>
<p>(vector/matrix, the same dimension as x1) 
The second vector/matrix.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function gives the Euclidean distance between two 
vectors or matrices.
</p>


<h3>Value</h3>

<table>
<tr><td><code>dist</code></td>
<td>
<p>(numeric) Euclidean distance.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Wenying Deng
</p>

<hr>
<h2 id='generate_kernel'>Generating A Single Kernel</h2><span id='topic+generate_kernel'></span>

<h3>Description</h3>

<p>Generate kernels for the kernel library.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>generate_kernel(method = "rbf", l = 1, p = 2, sigma = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="generate_kernel_+3A_method">method</code></td>
<td>
<p>(character) A character string indicating which kernel 
is to be computed.</p>
</td></tr>
<tr><td><code id="generate_kernel_+3A_l">l</code></td>
<td>
<p>(numeric) A numeric number indicating the hyperparameter 
(flexibility) of a specific kernel.</p>
</td></tr>
<tr><td><code id="generate_kernel_+3A_p">p</code></td>
<td>
<p>(integer) For polynomial, p is the power; for matern, v = p + 1 / 2; for
rational, alpha = p.</p>
</td></tr>
<tr><td><code id="generate_kernel_+3A_sigma">sigma</code></td>
<td>
<p>(numeric) The covariance coefficient for neural network kernel.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>There are seven kinds of kernel available here. For convenience, we define
<code class="reqn">r=\mid x-x'\mid</code>.
</p>
<p><b>Gaussian RBF Kernels</b> </p>
<p style="text-align: center;"><code class="reqn">k_{SE}(r)=exp\Big(-\frac{r^2}{2l^2}\Big)</code>
</p>

<p><b>Matern Kernels</b>
</p>
<p style="text-align: center;"><code class="reqn">k_{Matern}(r)=\frac{2^{1-\nu}}{\Gamma(\nu)}\Big(\frac{\sqrt{2\nu
r}}{l}\Big)^\nu K_\nu \Big(\frac{\sqrt{2\nu r}}{l}\Big)</code>
</p>

<p><b>Rational Quadratic Kernels</b> </p>
<p style="text-align: center;"><code class="reqn">k_{RQ}(r)=\Big(1+\frac{r^2}{2\alpha
l^2}\Big)^{-\alpha}</code>
</p>

<p><b>Polynomial Kernels</b> </p>
<p style="text-align: center;"><code class="reqn">k(x, x')=(x \cdot x')^p</code>
</p>
<p> We have intercept
kernel when <code class="reqn">p=0</code>, and linear kernel when <code class="reqn">p=1</code>.
</p>
<p><b>Neural Network Kernels</b> </p>
<p style="text-align: center;"><code class="reqn">k_{NN}(x,
x')=\frac{2}{\pi}sin^{-1}\Big(\frac{2\sigma \tilde{x}^T
\tilde{x}'}{\sqrt{(1+2\sigma \tilde{x}^T \tilde{x})(1+2\sigma \tilde{x}'^T
\tilde{x}')}}\Big)</code>
</p>
<p> where <code class="reqn">\tilde{x}</code> is the vector <code class="reqn">x</code> prepending
with <code class="reqn">1</code>.
</p>


<h3>Value</h3>

<table>
<tr><td><code>kern</code></td>
<td>
<p>(function) A function indicating the generated kernel.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Wenying Deng
</p>


<h3>References</h3>

<p>The MIT Press. Gaussian Processes for Machine Learning, 2006.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
kern_par &lt;- data.frame(method = c("rbf", "polynomial", "matern"),
l = c(.5, 1, 1.5), p = 1:3, sigma = rep(1, 3), stringsAsFactors = FALSE)

kern_func_list &lt;- list()
for (j in 1:nrow(kern_par)) {
  kern_func_list[[j]] &lt;- generate_kernel(kern_par[j, ]$method,
                                         kern_par[j, ]$l,
                                         kern_par[j, ]$p, 
                                         kern_par[j, ]$sigma)
}


</code></pre>

<hr>
<h2 id='kernel_intercept'>Generating A Single Matrix-wise Function Using Intercept</h2><span id='topic+kernel_intercept'></span>

<h3>Description</h3>

<p>Generate matrix-wise functions for two matrices using intercept kernel.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kernel_intercept(l, p, sigma)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kernel_intercept_+3A_l">l</code></td>
<td>
<p>(numeric) A numeric number indicating the hyperparameter
(flexibility) of a specific kernel.</p>
</td></tr>
<tr><td><code id="kernel_intercept_+3A_p">p</code></td>
<td>
<p>(integer) For polynomial, p is the power; for matern, v = p + 1 /
2; for rational, alpha = p.</p>
</td></tr>
<tr><td><code id="kernel_intercept_+3A_sigma">sigma</code></td>
<td>
<p>(numeric) The covariance coefficient for neural network kernel.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><b>Polynomial Kernels</b> </p>
<p style="text-align: center;"><code class="reqn">k(x, x')=(x \cdot x')^p</code>
</p>
<p> We have intercept
kernel when <code class="reqn">p=0</code>, and linear kernel when <code class="reqn">p=1</code>.
</p>


<h3>Value</h3>

<table>
<tr><td><code>matrix_wise</code></td>
<td>
<p>(function) A function calculating the relevance
of two matrices.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Wenying Deng
</p>


<h3>References</h3>

<p>The MIT Press. Gaussian Processes for Machine Learning, 2006.
</p>

<hr>
<h2 id='kernel_linear'>Generating A Single Matrix-wise Function Using Linear</h2><span id='topic+kernel_linear'></span>

<h3>Description</h3>

<p>Generate matrix-wise functions for two matrices using linear kernel.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kernel_linear(l, p, sigma)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kernel_linear_+3A_l">l</code></td>
<td>
<p>(numeric) A numeric number indicating the hyperparameter
(flexibility) of a specific kernel.</p>
</td></tr>
<tr><td><code id="kernel_linear_+3A_p">p</code></td>
<td>
<p>(integer) For polynomial, p is the power; for matern, v = p + 1 /
2; for rational, alpha = p.</p>
</td></tr>
<tr><td><code id="kernel_linear_+3A_sigma">sigma</code></td>
<td>
<p>(numeric) The covariance coefficient for neural network kernel.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><b>Polynomial Kernels</b> </p>
<p style="text-align: center;"><code class="reqn">k(x, x')=(x \cdot x')^p</code>
</p>
<p> We have intercept
kernel when <code class="reqn">p=0</code>, and linear kernel when <code class="reqn">p=1</code>.
</p>


<h3>Value</h3>

<table>
<tr><td><code>matrix_wise</code></td>
<td>
<p>(function) A function calculating the relevance
of two matrices.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Wenying Deng
</p>


<h3>References</h3>

<p>The MIT Press. Gaussian Processes for Machine Learning, 2006.
</p>

<hr>
<h2 id='kernel_matern'>Generating A Single Matrix-wise Function Using Matern</h2><span id='topic+kernel_matern'></span>

<h3>Description</h3>

<p>Generate matrix-wise functions for two matrices using matern kernel.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kernel_matern(l, p, sigma)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kernel_matern_+3A_l">l</code></td>
<td>
<p>(numeric) A numeric number indicating the hyperparameter
(flexibility) of a specific kernel.</p>
</td></tr>
<tr><td><code id="kernel_matern_+3A_p">p</code></td>
<td>
<p>(integer) For polynomial, p is the power; for matern, v = p + 1 /
2; for rational, alpha = p.</p>
</td></tr>
<tr><td><code id="kernel_matern_+3A_sigma">sigma</code></td>
<td>
<p>(numeric) The covariance coefficient for neural network kernel.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><b>Matern Kernels</b>
</p>
<p style="text-align: center;"><code class="reqn">k_{Matern}(r)=\frac{2^{1-\nu}}{\Gamma(\nu)}\Big(\frac{\sqrt{2\nu
r}}{l}\Big)^\nu K_\nu \Big(\frac{\sqrt{2\nu r}}{l}\Big)</code>
</p>



<h3>Value</h3>

<table>
<tr><td><code>matrix_wise</code></td>
<td>
<p>(function) A function calculating the relevance
of two matrices.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Wenying Deng
</p>


<h3>References</h3>

<p>The MIT Press. Gaussian Processes for Machine Learning, 2006.
</p>

<hr>
<h2 id='kernel_nn'>Generating A Single Matrix-wise Function Using Neural Network</h2><span id='topic+kernel_nn'></span>

<h3>Description</h3>

<p>Generate matrix-wise functions for two matrices using neural network kernel.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kernel_nn(l, p, sigma)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kernel_nn_+3A_l">l</code></td>
<td>
<p>(numeric) A numeric number indicating the hyperparameter
(flexibility) of a specific kernel.</p>
</td></tr>
<tr><td><code id="kernel_nn_+3A_p">p</code></td>
<td>
<p>(integer) For polynomial, p is the power; for matern, v = p + 1 /
2; for rational, alpha = p.</p>
</td></tr>
<tr><td><code id="kernel_nn_+3A_sigma">sigma</code></td>
<td>
<p>(numeric) The covariance coefficient for neural network kernel.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><b>Neural Network Kernels</b> </p>
<p style="text-align: center;"><code class="reqn">k_{NN}(x,
x')=\frac{2}{\pi}sin^{-1}\Big(\frac{2\tilde{x}^T
\tilde{x}'}{\sqrt{(1+2\tilde{x}^T \tilde{x})(1+2\tilde{x}'^T
\tilde{x}')}}\Big)</code>
</p>



<h3>Value</h3>

<table>
<tr><td><code>matrix_wise</code></td>
<td>
<p>(function) A function calculating the relevance
of two matrices.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Wenying Deng
</p>


<h3>References</h3>

<p>The MIT Press. Gaussian Processes for Machine Learning, 2006.
</p>

<hr>
<h2 id='kernel_polynomial'>Generating A Single Matrix-wise Function Using Polynomial</h2><span id='topic+kernel_polynomial'></span>

<h3>Description</h3>

<p>Generate matrix-wise functions for two matrices using polynomial kernel.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kernel_polynomial(l, p, sigma)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kernel_polynomial_+3A_l">l</code></td>
<td>
<p>(numeric) A numeric number indicating the hyperparameter
(flexibility) of a specific kernel.</p>
</td></tr>
<tr><td><code id="kernel_polynomial_+3A_p">p</code></td>
<td>
<p>(integer) For polynomial, p is the power; for matern, v = p + 1 /
2; for rational, alpha = p.</p>
</td></tr>
<tr><td><code id="kernel_polynomial_+3A_sigma">sigma</code></td>
<td>
<p>(numeric) The covariance coefficient for neural network kernel.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><b>Polynomial Kernels</b> </p>
<p style="text-align: center;"><code class="reqn">k(x, x')=(x \cdot x')^p</code>
</p>
<p> We have intercept
kernel when <code class="reqn">p=0</code>, and linear kernel when <code class="reqn">p=1</code>.
</p>


<h3>Value</h3>

<table>
<tr><td><code>matrix_wise</code></td>
<td>
<p>(function) A function calculating the relevance
of two matrices.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Wenying Deng
</p>


<h3>References</h3>

<p>The MIT Press. Gaussian Processes for Machine Learning, 2006.
</p>

<hr>
<h2 id='kernel_rational'>Generating A Single Matrix-wise Function Using Rational Quadratic</h2><span id='topic+kernel_rational'></span>

<h3>Description</h3>

<p>Generate matrix-wise functions for two matrices using rational kernel.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kernel_rational(l, p, sigma)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kernel_rational_+3A_l">l</code></td>
<td>
<p>(numeric) A numeric number indicating the hyperparameter
(flexibility) of a specific kernel.</p>
</td></tr>
<tr><td><code id="kernel_rational_+3A_p">p</code></td>
<td>
<p>(integer) For polynomial, p is the power; for matern, v = p + 1 /
2; for rational, alpha = p.</p>
</td></tr>
<tr><td><code id="kernel_rational_+3A_sigma">sigma</code></td>
<td>
<p>(numeric) The covariance coefficient for neural network kernel.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><b>Rational Quadratic Kernels</b> </p>
<p style="text-align: center;"><code class="reqn">k_{RQ}(r)=\Big(1+\frac{r^2}{2\alpha
l^2}\Big)^{-\alpha}</code>
</p>



<h3>Value</h3>

<table>
<tr><td><code>matrix_wise</code></td>
<td>
<p>(function) A function calculating the relevance
of two matrices.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Wenying Deng
</p>


<h3>References</h3>

<p>The MIT Press. Gaussian Processes for Machine Learning, 2006.
</p>

<hr>
<h2 id='kernel_rbf'>Generating A Single Matrix-wise Function Using RBF</h2><span id='topic+kernel_rbf'></span>

<h3>Description</h3>

<p>Generate matrix-wise functions for two matrices using rbf kernel.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kernel_rbf(l, p, sigma)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kernel_rbf_+3A_l">l</code></td>
<td>
<p>(numeric) A numeric number indicating the hyperparameter
(flexibility) of a specific kernel.</p>
</td></tr>
<tr><td><code id="kernel_rbf_+3A_p">p</code></td>
<td>
<p>(integer) For polynomial, p is the power; for matern, v = p + 1 /
2; for rational, alpha = p.</p>
</td></tr>
<tr><td><code id="kernel_rbf_+3A_sigma">sigma</code></td>
<td>
<p>(numeric) The covariance coefficient for neural network kernel.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><b>Gaussian RBF Kernels</b> </p>
<p style="text-align: center;"><code class="reqn">k_{SE}(r)=exp\Big(-\frac{r^2}{2l^2}\Big)</code>
</p>



<h3>Value</h3>

<table>
<tr><td><code>matrix_wise</code></td>
<td>
<p>(function) A function calculating the relevance
of two matrices.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Wenying Deng
</p>


<h3>References</h3>

<p>The MIT Press. Gaussian Processes for Machine Learning, 2006.
</p>

<hr>
<h2 id='parse_cvek_formula'>Parsing User-supplied Formula</h2><span id='topic+parse_cvek_formula'></span>

<h3>Description</h3>

<p>Parsing user-supplied formula to fixed-effect and kernel matrices.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>parse_cvek_formula(
  formula,
  kern_func_list,
  data,
  data_new = NULL,
  verbose = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="parse_cvek_formula_+3A_formula">formula</code></td>
<td>
<p>(formula) A user-supplied formula.</p>
</td></tr>
<tr><td><code id="parse_cvek_formula_+3A_kern_func_list">kern_func_list</code></td>
<td>
<p>(list) A list of kernel functions in the kernel library</p>
</td></tr>
<tr><td><code id="parse_cvek_formula_+3A_data">data</code></td>
<td>
<p>(data.frame, n*d) A data.frame, list or environment (or object
coercible by as.data.frame to a data.frame), containing the variables in
formula. Neither a matrix nor an array will be accepted.</p>
</td></tr>
<tr><td><code id="parse_cvek_formula_+3A_data_new">data_new</code></td>
<td>
<p>(data.frame, n_new*d) New data for computing predictions.</p>
</td></tr>
<tr><td><code id="parse_cvek_formula_+3A_verbose">verbose</code></td>
<td>
<p>(logical) Whether to print additional messages.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The formula object is exactly like the formula for a GLM except that user can
use k() to specify kernel terms.
Additionally, user can specify interaction between kernel terms (using either '*' and ':'),
and exclude interaction term by including -1 on the RHS of formula.
</p>


<h3>Value</h3>

<p>A list of three slots:
</p>
<table>
<tr><td><code>Y</code></td>
<td>
<p>(matrix, n*1) The vector of response variable.</p>
</td></tr>
<tr><td><code>X</code></td>
<td>
<p>(matrix, n*d_fix) The fixed effect matrix.</p>
</td></tr>
<tr><td><code>K</code></td>
<td>
<p>(list of matrices) A nested list of kernel term matrices. 
The first level corresponds to each base kernel function in 
kern_func_list, the second level corresponds to each kernel term
specified in the formula.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Jeremiah Zhe Liu
</p>

<hr>
<h2 id='parse_kernel_terms'>Compute Kernel Matrix</h2><span id='topic+parse_kernel_terms'></span>

<h3>Description</h3>

<p>Compute kernel matrix for each kernel term in the formula.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>parse_kernel_terms(kern_effect_formula, kern_func, data, data_new = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="parse_kernel_terms_+3A_kern_effect_formula">kern_effect_formula</code></td>
<td>
<p>(character) A term in the formula.</p>
</td></tr>
<tr><td><code id="parse_kernel_terms_+3A_kern_func">kern_func</code></td>
<td>
<p>(function) A kernel function. Will be overwritten to linear
kernel if the variable doesn't contain 'k()'.</p>
</td></tr>
<tr><td><code id="parse_kernel_terms_+3A_data">data</code></td>
<td>
<p>(data.frame, n*d) A data.frame, list or environment (or object
coercible by as.data.frame to a data.frame), containing the variables in
formula. Neither a matrix nor an array will be accepted.</p>
</td></tr>
<tr><td><code id="parse_kernel_terms_+3A_data_new">data_new</code></td>
<td>
<p>(data.frame, n_new*d) New data for computing predictions.</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>kern_term_list</code></td>
<td>
<p>(list) A list of kernel matrices for each term 
in the formula.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Jeremiah Zhe Liu
</p>

<hr>
<h2 id='parse_kernel_variable'>Create Kernel Matrix</h2><span id='topic+parse_kernel_variable'></span>

<h3>Description</h3>

<p>Create kernel matrix for each variable in the formula.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>parse_kernel_variable(kern_var_name, kern_func, data, data_new = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="parse_kernel_variable_+3A_kern_var_name">kern_var_name</code></td>
<td>
<p>(vector of characters) Names of variables in data to
create the kernel matrix from. Must be a single term that is either of the form
<code class="reqn">x</code> (a single linear term) or <code class="reqn">k(x1, x2, \dots)</code> (a kernel term that may
contain multiple variables).</p>
</td></tr>
<tr><td><code id="parse_kernel_variable_+3A_kern_func">kern_func</code></td>
<td>
<p>(function) A kernel function. Will be overwritten to linear
kernel if the variable doesn't contain 'k()'.</p>
</td></tr>
<tr><td><code id="parse_kernel_variable_+3A_data">data</code></td>
<td>
<p>(data.frame, n*d) A data.frame, list or environment (or object
coercible by as.data.frame to a data.frame), containing the variables in
formula. Neither a matrix nor an array will be accepted.</p>
</td></tr>
<tr><td><code id="parse_kernel_variable_+3A_data_new">data_new</code></td>
<td>
<p>(data.frame, n_new*d) New data for computing predictions.</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>kernel_mat</code></td>
<td>
<p>(matrix, n*n) The kernel matrix corresponding to 
the variable being computed.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Jeremiah Zhe Liu
</p>

<hr>
<h2 id='predict.cvek'>Predicting New Response</h2><span id='topic+predict.cvek'></span>

<h3>Description</h3>

<p>Predicting new response based on given design matrix and 
the estimation result.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'cvek'
predict(object, newdata, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.cvek_+3A_object">object</code></td>
<td>
<p>(list) Estimation results returned by cvek() procedure.</p>
</td></tr>
<tr><td><code id="predict.cvek_+3A_newdata">newdata</code></td>
<td>
<p>(dataframe) The new set of predictors, whose name is 
the same as those of formula in cvek().</p>
</td></tr>
<tr><td><code id="predict.cvek_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>After we obtain the estimation result, we can predict new response.
</p>


<h3>Value</h3>

<table>
<tr><td><code>y_pred</code></td>
<td>
<p>(matrix, n*1) Predicted new response.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Wenying Deng
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
kern_par &lt;- data.frame(method = rep("rbf", 3),
l = rep(3, 3), p = rep(2, 3), 
stringsAsFactors = FALSE)
# define kernel library
kern_func_list &lt;- define_library(kern_par)

n &lt;- 10
d &lt;- 4
formula &lt;- y ~ x1 + x2 + k(x3, x4)
set.seed(1118)
data &lt;- as.data.frame(matrix(
  rnorm(n * d),
  ncol = d,
  dimnames = list(NULL, paste0("x", 1:d))
))
beta_true &lt;- c(1, .41, 2.37)
lnr_kern_func &lt;- generate_kernel(method = "rbf", l = 3)
kern_effect_lnr &lt;-
  parse_kernel_variable("k(x3, x4)", lnr_kern_func, data)
alpha_lnr_true &lt;- rnorm(n)

data$y &lt;- as.matrix(cbind(1, data[, c("x1", "x2")])) %*% beta_true +
  kern_effect_lnr %*% alpha_lnr_true

data_train &lt;- data[1:6, ]
data_test &lt;- data[7:10, ]

result &lt;- cvek(formula,
               kern_func_list,
               data_train,
               mode = "loocv",
               strategy = "stack",
               beta_exp = 1,
               lambda = exp(seq(-2, 2)),
               test = "asymp",
               alt_kernel_type = "linear",
               verbose = FALSE)

predict(result, data_test)

</code></pre>

<hr>
<h2 id='square_dist'>Computing Square Distance between Two Sets of Variables</h2><span id='topic+square_dist'></span>

<h3>Description</h3>

<p>Compute Squared Euclidean distance between two sets of variables with the
same dimension.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>square_dist(X1, X2 = NULL, l = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="square_dist_+3A_x1">X1</code></td>
<td>
<p>(matrix, n1*p0) The first set of variables.</p>
</td></tr>
<tr><td><code id="square_dist_+3A_x2">X2</code></td>
<td>
<p>(matrix, n2*p0) The second set of variables.</p>
</td></tr>
<tr><td><code id="square_dist_+3A_l">l</code></td>
<td>
<p>(numeric) A numeric number indicating the hyperparameter
(flexibility) of a specific kernel.</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>dist_sq</code></td>
<td>
<p>(matrix, n1*n2) The computed squared Euclidean distance.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Wenying Deng
</p>


<h3>References</h3>

<p>The MIT Press. Gaussian Processes for Machine Learning, 2006.
</p>

<hr>
<h2 id='standardize'>Standardizing Matrix</h2><span id='topic+standardize'></span>

<h3>Description</h3>

<p>Center and scale the data matrix into mean zero and standard deviation one.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>standardize(X)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="standardize_+3A_x">X</code></td>
<td>
<p>(matrix) Original data matrix.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function gives the standardized data matrix.
</p>


<h3>Value</h3>

<table>
<tr><td><code>X</code></td>
<td>
<p>(matrix) Standardized data matrix.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Wenying Deng
</p>

<hr>
<h2 id='test_asymp'>Conducting Score Tests for Interaction Using Asymptotic Test</h2><span id='topic+test_asymp'></span>

<h3>Description</h3>

<p>Conduct score tests comparing a fitted model and a more general alternative
model using asymptotic test.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>test_asymp(Y, X, y_fixed, alpha0, K_ens, K_int, sigma2_hat, tau_hat, B)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="test_asymp_+3A_y">Y</code></td>
<td>
<p>(matrix, n*1) The vector of response variable.</p>
</td></tr>
<tr><td><code id="test_asymp_+3A_x">X</code></td>
<td>
<p>(matrix, n*d_fix) The fixed effect matrix.</p>
</td></tr>
<tr><td><code id="test_asymp_+3A_y_fixed">y_fixed</code></td>
<td>
<p>(vector of length n) Estimated fixed effect of the
response.</p>
</td></tr>
<tr><td><code id="test_asymp_+3A_alpha0">alpha0</code></td>
<td>
<p>(vector of length n) Kernel effect estimator of the estimated
ensemble kernel matrix.</p>
</td></tr>
<tr><td><code id="test_asymp_+3A_k_ens">K_ens</code></td>
<td>
<p>(matrix, n*n) Estimated ensemble kernel matrix.</p>
</td></tr>
<tr><td><code id="test_asymp_+3A_k_int">K_int</code></td>
<td>
<p>(matrix, n*n) The kernel matrix to be tested.</p>
</td></tr>
<tr><td><code id="test_asymp_+3A_sigma2_hat">sigma2_hat</code></td>
<td>
<p>(numeric) The estimated noise of the fixed effect.</p>
</td></tr>
<tr><td><code id="test_asymp_+3A_tau_hat">tau_hat</code></td>
<td>
<p>(numeric) The estimated noise of the kernel effect.</p>
</td></tr>
<tr><td><code id="test_asymp_+3A_b">B</code></td>
<td>
<p>(integer) A numeric value indicating times of resampling when test
= &quot;boot&quot;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><b>Asymptotic Test</b>
</p>
<p>This is based on the classical variance component test to construct a
testing procedure for the hypothesis about Gaussian process function.
</p>


<h3>Value</h3>

<table>
<tr><td><code>pvalue</code></td>
<td>
<p>(numeric) p-value of the test.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Wenying Deng
</p>


<h3>References</h3>

<p>Xihong Lin. Variance component testing in generalised linear
models with random effects. June 1997.
</p>
<p>Arnab Maity and Xihong Lin. Powerful tests for detecting a gene effect in
the presence of possible gene-gene interactions using garrote kernel
machines. December 2011.
</p>
<p>Petra Bu z kova, Thomas Lumley, and Kenneth Rice. Permutation and
parametric bootstrap tests for gene-gene and gene-environment interactions.
January 2011.
</p>


<h3>See Also</h3>

<p>method: <code><a href="#topic+generate_kernel">generate_kernel</a></code>
</p>
<p>mode: <code><a href="#topic+tuning">tuning</a></code>
</p>
<p>strategy: <code><a href="#topic+ensemble">ensemble</a></code>
</p>

<hr>
<h2 id='test_boot'>Conducting Score Tests for Interaction Using Bootstrap Test</h2><span id='topic+test_boot'></span>

<h3>Description</h3>

<p>Conduct score tests comparing a fitted model and a more general alternative
model using bootstrap test.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>test_boot(Y, X, y_fixed, alpha0, K_ens, K_int, sigma2_hat, tau_hat, B)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="test_boot_+3A_y">Y</code></td>
<td>
<p>(matrix, n*1) The vector of response variable.</p>
</td></tr>
<tr><td><code id="test_boot_+3A_x">X</code></td>
<td>
<p>(matrix, n*d_fix) The fixed effect matrix.</p>
</td></tr>
<tr><td><code id="test_boot_+3A_y_fixed">y_fixed</code></td>
<td>
<p>(vector of length n) Estimated fixed effect of the
response.</p>
</td></tr>
<tr><td><code id="test_boot_+3A_alpha0">alpha0</code></td>
<td>
<p>(vector of length n) Kernel effect estimator of the estimated
ensemble kernel matrix.</p>
</td></tr>
<tr><td><code id="test_boot_+3A_k_ens">K_ens</code></td>
<td>
<p>(matrix, n*n) Estimated ensemble kernel matrix.</p>
</td></tr>
<tr><td><code id="test_boot_+3A_k_int">K_int</code></td>
<td>
<p>(matrix, n*n) The kernel matrix to be tested.</p>
</td></tr>
<tr><td><code id="test_boot_+3A_sigma2_hat">sigma2_hat</code></td>
<td>
<p>(numeric) The estimated noise of the fixed effect.</p>
</td></tr>
<tr><td><code id="test_boot_+3A_tau_hat">tau_hat</code></td>
<td>
<p>(numeric) The estimated noise of the kernel effect.</p>
</td></tr>
<tr><td><code id="test_boot_+3A_b">B</code></td>
<td>
<p>(integer) A numeric value indicating times of resampling when test
= &quot;boot&quot;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><b>Bootstrap Test</b>
</p>
<p>When it comes to small sample size, we can use bootstrap test instead, which
can give valid tests with moderate sample sizes and requires similar
computational effort to a permutation test.
</p>


<h3>Value</h3>

<table>
<tr><td><code>pvalue</code></td>
<td>
<p>(numeric) p-value of the test.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Wenying Deng
</p>


<h3>References</h3>

<p>Xihong Lin. Variance component testing in generalised linear
models with random effects. June 1997.
</p>
<p>Arnab Maity and Xihong Lin. Powerful tests for detecting a gene effect in
the presence of possible gene-gene interactions using garrote kernel
machines. December 2011.
</p>
<p>Petra Bu z kova, Thomas Lumley, and Kenneth Rice. Permutation and
parametric bootstrap tests for gene-gene and gene-environment interactions.
January 2011.
</p>


<h3>See Also</h3>

<p>method: <code><a href="#topic+generate_kernel">generate_kernel</a></code>
</p>
<p>mode: <code><a href="#topic+tuning">tuning</a></code>
</p>
<p>strategy: <code><a href="#topic+ensemble">ensemble</a></code>
</p>

<hr>
<h2 id='tuning'>Calculating Tuning Parameters</h2><span id='topic+tuning'></span>

<h3>Description</h3>

<p>Calculate tuning parameters based on given criteria.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tuning(Y, X, K_mat, mode, lambda)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tuning_+3A_y">Y</code></td>
<td>
<p>(matrix, n*1) The vector of response variable.</p>
</td></tr>
<tr><td><code id="tuning_+3A_x">X</code></td>
<td>
<p>(matrix, n*d_fix) The fixed effect matrix.</p>
</td></tr>
<tr><td><code id="tuning_+3A_k_mat">K_mat</code></td>
<td>
<p>(list of matrices) A nested list of kernel term matrices, 
corresponding to each kernel term specified in the formula for 
a base kernel function in kern_func_list.</p>
</td></tr>
<tr><td><code id="tuning_+3A_mode">mode</code></td>
<td>
<p>(character) A character string indicating which tuning parameter
criteria is to be used.</p>
</td></tr>
<tr><td><code id="tuning_+3A_lambda">lambda</code></td>
<td>
<p>(numeric) A numeric string specifying the range of tuning 
parameter to be chosen. The lower limit of lambda must be above 0.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>There are seven tuning parameter selections here:
</p>
<p><b>leave-one-out Cross Validation</b>
</p>
<p style="text-align: center;"><code class="reqn">\lambda_{n-CV}={argmin}_{\lambda \in
\Lambda}\;\Big\{log\;y^{\star
T}[I-diag(A_\lambda)-\frac{1}{n}I]^{-1}(I-A_\lambda)^2[I-diag(A_\lambda)-
\frac{1}{n}I]^{-1}y^\star \Big\}</code>
</p>

<p><b>Akaike Information Criteria</b>
</p>
<p style="text-align: center;"><code class="reqn">\lambda_{AIC}={argmin}_{\lambda \in \Lambda}\Big\{log\;
y^{\star T}(I-A_\lambda)^2y^\star+\frac{2[tr(A_\lambda)+2]}{n}\Big\}</code>
</p>

<p><b>Akaike Information Criteria (small-sample variant)</b>
</p>
<p style="text-align: center;"><code class="reqn">\lambda_{AICc}={argmin}_{\lambda \in \Lambda}\Big\{log\;
y^{\star
T}(I-A_\lambda)^2y^\star+\frac{2[tr(A_\lambda)+2]}{n-tr(A_\lambda)-3}\Big\}</code>
</p>

<p><b>Bayesian Information Criteria</b>
</p>
<p style="text-align: center;"><code class="reqn">\lambda_{BIC}={argmin}_{\lambda \in \Lambda}\Big\{log\;
y^{\star T}(I-A_\lambda)^2y^\star+\frac{log(n)[tr(A_\lambda)+2]}{n}\Big\}</code>
</p>

<p><b>Generalized Cross Validation</b>
</p>
<p style="text-align: center;"><code class="reqn">\lambda_{GCV}={argmin}_{\lambda \in \Lambda}\Big\{log\;
y^{\star
T}(I-A_\lambda)^2y^\star-2log[1-\frac{tr(A_\lambda)}{n}-\frac{1}{n}]_+\Big\}</code>
</p>

<p><b>Generalized Cross Validation (small-sample variant)</b>
</p>
<p style="text-align: center;"><code class="reqn">\lambda_{GCVc}={argmin}_{\lambda \in \Lambda}\Big\{log\;
y^{\star
T}(I-A_\lambda)^2y^\star-2log[1-\frac{tr(A_\lambda)}{n}-\frac{2}{n}]_+\Big\}</code>
</p>

<p><b>Generalized Maximum Profile Marginal Likelihood</b>
</p>
<p style="text-align: center;"><code class="reqn">\lambda_{GMPML}={argmin}_{\lambda \in \Lambda}\Big\{log\;
y^{\star T}(I-A_\lambda)y^\star-\frac{1}{n-1}log \mid I-A_\lambda \mid
\Big\}</code>
</p>



<h3>Value</h3>

<table>
<tr><td><code>lambda0</code></td>
<td>
<p>(numeric) The selected tuning parameter.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Wenying Deng
</p>


<h3>References</h3>

<p>Philip S. Boonstra, Bhramar Mukherjee, and Jeremy M. G. Taylor.
A Small-Sample Choice of the Tuning Parameter in Ridge Regression. July
2015.
</p>
<p>Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of
Statistical Learning: Data Mining, Inference, and Prediction, Second
Edition. Springer Series in Statistics. Springer- Verlag, New York, 2
edition, 2009.
</p>
<p>Hirotogu Akaike. Information Theory and an Extension of the Maximum
Likelihood Principle. In Selected Papers of Hirotugu Akaike, Springer
Series in Statistics, pages 199–213. Springer, New York, NY, 1998.
</p>
<p>Clifford M. Hurvich and Chih-Ling Tsai. Regression and time series model
selection in small samples. June 1989.
</p>
<p>Hurvich Clifford M., Simonoff Jeffrey S., and Tsai Chih-Ling. Smoothing
parameter selection in nonparametric regression using an improved Akaike
information criterion. January 2002.
</p>

<hr>
<h2 id='tuning_AIC'>Calculating Tuning Parameters Using AIC</h2><span id='topic+tuning_AIC'></span>

<h3>Description</h3>

<p>Calculate tuning parameters based on AIC.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tuning_AIC(Y, X, K_mat, lambda)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tuning_AIC_+3A_y">Y</code></td>
<td>
<p>(matrix, n*1) The vector of response variable.</p>
</td></tr>
<tr><td><code id="tuning_AIC_+3A_x">X</code></td>
<td>
<p>(matrix, n*d_fix) The fixed effect matrix.</p>
</td></tr>
<tr><td><code id="tuning_AIC_+3A_k_mat">K_mat</code></td>
<td>
<p>(list of matrices) A nested list of kernel term matrices, 
corresponding to each kernel term specified in the formula for 
a base kernel function in kern_func_list.</p>
</td></tr>
<tr><td><code id="tuning_AIC_+3A_lambda">lambda</code></td>
<td>
<p>(numeric) A numeric string specifying the range of tuning parameter 
to be chosen. The lower limit of lambda must be above 0.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><b>Akaike Information Criteria</b>
</p>
<p style="text-align: center;"><code class="reqn">\lambda_{AIC}={argmin}_{\lambda \in \Lambda}\Big\{log\;
y^{\star T}(I-A_\lambda)^2y^\star+\frac{2[tr(A_\lambda)+2]}{n}\Big\}</code>
</p>



<h3>Value</h3>

<table>
<tr><td><code>lambda0</code></td>
<td>
<p>(numeric) The estimated tuning parameter.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Wenying Deng
</p>


<h3>References</h3>

<p>Philip S. Boonstra, Bhramar Mukherjee, and Jeremy M. G. Taylor.
A Small-Sample Choice of the Tuning Parameter in Ridge Regression. July
2015.
</p>
<p>Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of
Statistical Learning: Data Mining, Inference, and Prediction, Second
Edition. Springer Series in Statistics. Springer- Verlag, New York, 2
edition, 2009.
</p>
<p>Hirotogu Akaike. Information Theory and an Extension of the Maximum
Likelihood Principle. In Selected Papers of Hirotugu Akaike, Springer
Series in Statistics, pages 199–213. Springer, New York, NY, 1998.
</p>
<p>Clifford M. Hurvich and Chih-Ling Tsai. Regression and time series model
selection in small samples. June 1989.
</p>
<p>Hurvich Clifford M., Simonoff Jeffrey S., and Tsai Chih-Ling. Smoothing
parameter selection in nonparametric regression using an improved Akaike
information criterion. January 2002.
</p>

<hr>
<h2 id='tuning_AICc'>Calculating Tuning Parameters Using AICc</h2><span id='topic+tuning_AICc'></span>

<h3>Description</h3>

<p>Calculate tuning parameters based on AICc.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tuning_AICc(Y, X, K_mat, lambda)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tuning_AICc_+3A_y">Y</code></td>
<td>
<p>(matrix, n*1) The vector of response variable.</p>
</td></tr>
<tr><td><code id="tuning_AICc_+3A_x">X</code></td>
<td>
<p>(matrix, n*d_fix) The fixed effect matrix.</p>
</td></tr>
<tr><td><code id="tuning_AICc_+3A_k_mat">K_mat</code></td>
<td>
<p>(list of matrices) A nested list of kernel term matrices, 
corresponding to each kernel term specified in the formula for 
a base kernel function in kern_func_list.</p>
</td></tr>
<tr><td><code id="tuning_AICc_+3A_lambda">lambda</code></td>
<td>
<p>(numeric) A numeric string specifying the range of tuning parameter 
to be chosen. The lower limit of lambda must be above 0.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><b>Akaike Information Criteria (small sample size)</b>
</p>
<p style="text-align: center;"><code class="reqn">\lambda_{AICc}={argmin}_{\lambda \in \Lambda}\Big\{log\;
y^{\star
T}(I-A_\lambda)^2y^\star+\frac{2[tr(A_\lambda)+2]}{n-tr(A_\lambda)-3}\Big\}</code>
</p>



<h3>Value</h3>

<table>
<tr><td><code>lambda0</code></td>
<td>
<p>(numeric) The estimated tuning parameter.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Wenying Deng
</p>


<h3>References</h3>

<p>Philip S. Boonstra, Bhramar Mukherjee, and Jeremy M. G. Taylor.
A Small-Sample Choice of the Tuning Parameter in Ridge Regression. July
2015.
</p>
<p>Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of
Statistical Learning: Data Mining, Inference, and Prediction, Second
Edition. Springer Series in Statistics. Springer- Verlag, New York, 2
edition, 2009.
</p>
<p>Hirotogu Akaike. Information Theory and an Extension of the Maximum
Likelihood Principle. In Selected Papers of Hirotugu Akaike, Springer
Series in Statistics, pages 199–213. Springer, New York, NY, 1998.
</p>
<p>Clifford M. Hurvich and Chih-Ling Tsai. Regression and time series model
selection in small samples. June 1989.
</p>
<p>Hurvich Clifford M., Simonoff Jeffrey S., and Tsai Chih-Ling. Smoothing
parameter selection in nonparametric regression using an improved Akaike
information criterion. January 2002.
</p>

<hr>
<h2 id='tuning_BIC'>Calculating Tuning Parameters Using BIC</h2><span id='topic+tuning_BIC'></span>

<h3>Description</h3>

<p>Calculate tuning parameters based on BIC.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tuning_BIC(Y, X, K_mat, lambda)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tuning_BIC_+3A_y">Y</code></td>
<td>
<p>(matrix, n*1) The vector of response variable.</p>
</td></tr>
<tr><td><code id="tuning_BIC_+3A_x">X</code></td>
<td>
<p>(matrix, n*d_fix) The fixed effect matrix.</p>
</td></tr>
<tr><td><code id="tuning_BIC_+3A_k_mat">K_mat</code></td>
<td>
<p>(list of matrices) A nested list of kernel term matrices, 
corresponding to each kernel term specified in the formula for 
a base kernel function in kern_func_list.</p>
</td></tr>
<tr><td><code id="tuning_BIC_+3A_lambda">lambda</code></td>
<td>
<p>(numeric) A numeric string specifying the range of tuning parameter 
to be chosen. The lower limit of lambda must be above 0.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><b>Bayesian Information Criteria</b>
</p>
<p style="text-align: center;"><code class="reqn">\lambda_{BIC}={argmin}_{\lambda \in \Lambda}\Big\{log\;
y^{\star T}(I-A_\lambda)^2y^\star+\frac{log(n)[tr(A_\lambda)+2]}{n}\Big\}</code>
</p>



<h3>Value</h3>

<table>
<tr><td><code>lambda0</code></td>
<td>
<p>(numeric) The estimated tuning parameter.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Wenying Deng
</p>


<h3>References</h3>

<p>Philip S. Boonstra, Bhramar Mukherjee, and Jeremy M. G. Taylor.
A Small-Sample Choice of the Tuning Parameter in Ridge Regression. July
2015.
</p>
<p>Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of
Statistical Learning: Data Mining, Inference, and Prediction, Second
Edition. Springer Series in Statistics. Springer- Verlag, New York, 2
edition, 2009.
</p>
<p>Hirotogu Akaike. Information Theory and an Extension of the Maximum
Likelihood Principle. In Selected Papers of Hirotugu Akaike, Springer
Series in Statistics, pages 199–213. Springer, New York, NY, 1998.
</p>
<p>Clifford M. Hurvich and Chih-Ling Tsai. Regression and time series model
selection in small samples. June 1989.
</p>
<p>Hurvich Clifford M., Simonoff Jeffrey S., and Tsai Chih-Ling. Smoothing
parameter selection in nonparametric regression using an improved Akaike
information criterion. January 2002.
</p>

<hr>
<h2 id='tuning_GCV'>Calculating Tuning Parameters Using GCV</h2><span id='topic+tuning_GCV'></span>

<h3>Description</h3>

<p>Calculate tuning parameters based on GCV.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tuning_GCV(Y, X, K_mat, lambda)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tuning_GCV_+3A_y">Y</code></td>
<td>
<p>(matrix, n*1) The vector of response variable.</p>
</td></tr>
<tr><td><code id="tuning_GCV_+3A_x">X</code></td>
<td>
<p>(matrix, n*d_fix) The fixed effect matrix.</p>
</td></tr>
<tr><td><code id="tuning_GCV_+3A_k_mat">K_mat</code></td>
<td>
<p>(list of matrices) A nested list of kernel term matrices, 
corresponding to each kernel term specified in the formula for 
a base kernel function in kern_func_list.</p>
</td></tr>
<tr><td><code id="tuning_GCV_+3A_lambda">lambda</code></td>
<td>
<p>(numeric) A numeric string specifying the range of tuning parameter 
to be chosen. The lower limit of lambda must be above 0.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><b>Generalized Cross Validation</b>
</p>
<p style="text-align: center;"><code class="reqn">\lambda_{GCV}={argmin}_{\lambda \in \Lambda}\Big\{log\;
y^{\star
T}(I-A_\lambda)^2y^\star-2log[1-\frac{tr(A_\lambda)}{n}-\frac{1}{n}]_+\Big\}</code>
</p>



<h3>Value</h3>

<table>
<tr><td><code>lambda0</code></td>
<td>
<p>(numeric) The estimated tuning parameter.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Wenying Deng
</p>


<h3>References</h3>

<p>Philip S. Boonstra, Bhramar Mukherjee, and Jeremy M. G. Taylor.
A Small-Sample Choice of the Tuning Parameter in Ridge Regression. July
2015.
</p>
<p>Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of
Statistical Learning: Data Mining, Inference, and Prediction, Second
Edition. Springer Series in Statistics. Springer- Verlag, New York, 2
edition, 2009.
</p>
<p>Hirotogu Akaike. Information Theory and an Extension of the Maximum
Likelihood Principle. In Selected Papers of Hirotugu Akaike, Springer
Series in Statistics, pages 199–213. Springer, New York, NY, 1998.
</p>
<p>Clifford M. Hurvich and Chih-Ling Tsai. Regression and time series model
selection in small samples. June 1989.
</p>
<p>Hurvich Clifford M., Simonoff Jeffrey S., and Tsai Chih-Ling. Smoothing
parameter selection in nonparametric regression using an improved Akaike
information criterion. January 2002.
</p>

<hr>
<h2 id='tuning_GCVc'>Calculating Tuning Parameters Using GCVc</h2><span id='topic+tuning_GCVc'></span>

<h3>Description</h3>

<p>Calculate tuning parameters based on GCVc.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tuning_GCVc(Y, X, K_mat, lambda)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tuning_GCVc_+3A_y">Y</code></td>
<td>
<p>(matrix, n*1) The vector of response variable.</p>
</td></tr>
<tr><td><code id="tuning_GCVc_+3A_x">X</code></td>
<td>
<p>(matrix, n*d_fix) The fixed effect matrix.</p>
</td></tr>
<tr><td><code id="tuning_GCVc_+3A_k_mat">K_mat</code></td>
<td>
<p>(list of matrices) A nested list of kernel term matrices, 
corresponding to each kernel term specified in the formula for 
a base kernel function in kern_func_list.</p>
</td></tr>
<tr><td><code id="tuning_GCVc_+3A_lambda">lambda</code></td>
<td>
<p>(numeric) A numeric string specifying the range of tuning parameter 
to be chosen. The lower limit of lambda must be above 0.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><b>Generalized Cross Validation (small sample size)</b>
</p>
<p style="text-align: center;"><code class="reqn">\lambda_{GCVc}={argmin}_{\lambda \in \Lambda}\Big\{log\;
y^{\star
T}(I-A_\lambda)^2y^\star-2log[1-\frac{tr(A_\lambda)}{n}-\frac{2}{n}]_+\Big\}</code>
</p>



<h3>Value</h3>

<table>
<tr><td><code>lambda0</code></td>
<td>
<p>(numeric) The estimated tuning parameter.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Wenying Deng
</p>


<h3>References</h3>

<p>Philip S. Boonstra, Bhramar Mukherjee, and Jeremy M. G. Taylor.
A Small-Sample Choice of the Tuning Parameter in Ridge Regression. July
2015.
</p>
<p>Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of
Statistical Learning: Data Mining, Inference, and Prediction, Second
Edition. Springer Series in Statistics. Springer- Verlag, New York, 2
edition, 2009.
</p>
<p>Hirotogu Akaike. Information Theory and an Extension of the Maximum
Likelihood Principle. In Selected Papers of Hirotugu Akaike, Springer
Series in Statistics, pages 199–213. Springer, New York, NY, 1998.
</p>
<p>Clifford M. Hurvich and Chih-Ling Tsai. Regression and time series model
selection in small samples. June 1989.
</p>
<p>Hurvich Clifford M., Simonoff Jeffrey S., and Tsai Chih-Ling. Smoothing
parameter selection in nonparametric regression using an improved Akaike
information criterion. January 2002.
</p>

<hr>
<h2 id='tuning_gmpml'>Calculating Tuning Parameters Using GMPML</h2><span id='topic+tuning_gmpml'></span>

<h3>Description</h3>

<p>Calculate tuning parameters based on Generalized Maximum Profile Marginal
Likelihood.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tuning_gmpml(Y, X, K_mat, lambda)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tuning_gmpml_+3A_y">Y</code></td>
<td>
<p>(matrix, n*1) The vector of response variable.</p>
</td></tr>
<tr><td><code id="tuning_gmpml_+3A_x">X</code></td>
<td>
<p>(matrix, n*d_fix) The fixed effect matrix.</p>
</td></tr>
<tr><td><code id="tuning_gmpml_+3A_k_mat">K_mat</code></td>
<td>
<p>(list of matrices) A nested list of kernel term matrices, 
corresponding to each kernel term specified in the formula for 
a base kernel function in kern_func_list.</p>
</td></tr>
<tr><td><code id="tuning_gmpml_+3A_lambda">lambda</code></td>
<td>
<p>(numeric) A numeric string specifying the range of tuning parameter 
to be chosen. The lower limit of lambda must be above 0.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><b>Generalized Maximum Profile Marginal Likelihood</b>
</p>
<p style="text-align: center;"><code class="reqn">\lambda_{GMPML}={argmin}_{\lambda \in \Lambda}\Big\{log\;
y^{\star T}(I-A_\lambda)y^\star-\frac{1}{n-1}log \mid I-A_\lambda \mid
\Big\}</code>
</p>



<h3>Value</h3>

<table>
<tr><td><code>lambda0</code></td>
<td>
<p>(numeric) The estimated tuning parameter.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Wenying Deng
</p>


<h3>References</h3>

<p>Philip S. Boonstra, Bhramar Mukherjee, and Jeremy M. G. Taylor.
A Small-Sample Choice of the Tuning Parameter in Ridge Regression. July
2015.
</p>
<p>Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of
Statistical Learning: Data Mining, Inference, and Prediction, Second
Edition. Springer Series in Statistics. Springer- Verlag, New York, 2
edition, 2009.
</p>
<p>Hirotogu Akaike. Information Theory and an Extension of the Maximum
Likelihood Principle. In Selected Papers of Hirotugu Akaike, Springer
Series in Statistics, pages 199–213. Springer, New York, NY, 1998.
</p>
<p>Clifford M. Hurvich and Chih-Ling Tsai. Regression and time series model
selection in small samples. June 1989.
</p>
<p>Hurvich Clifford M., Simonoff Jeffrey S., and Tsai Chih-Ling. Smoothing
parameter selection in nonparametric regression using an improved Akaike
information criterion. January 2002.
</p>

<hr>
<h2 id='tuning_loocv'>Calculating Tuning Parameters Using looCV</h2><span id='topic+tuning_loocv'></span>

<h3>Description</h3>

<p>Calculate tuning parameters based on given leave-one-out Cross Validation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tuning_loocv(Y, X, K_mat, lambda)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tuning_loocv_+3A_y">Y</code></td>
<td>
<p>(matrix, n*1) The vector of response variable.</p>
</td></tr>
<tr><td><code id="tuning_loocv_+3A_x">X</code></td>
<td>
<p>(matrix, n*d_fix) The fixed effect matrix.</p>
</td></tr>
<tr><td><code id="tuning_loocv_+3A_k_mat">K_mat</code></td>
<td>
<p>(list of matrices) A nested list of kernel term matrices, 
corresponding to each kernel term specified in the formula for 
a base kernel function in kern_func_list.</p>
</td></tr>
<tr><td><code id="tuning_loocv_+3A_lambda">lambda</code></td>
<td>
<p>(numeric) A numeric string specifying the range of tuning parameter 
to be chosen. The lower limit of lambda must be above 0.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><b>leave-one-out Cross Validation</b>
</p>
<p style="text-align: center;"><code class="reqn">\lambda_{n-CV}={argmin}_{\lambda \in
\Lambda}\;\Big\{log\;y^{\star
T}[I-diag(A_\lambda)-\frac{1}{n}I]^{-1}(I-A_\lambda)^2[I-diag(A_\lambda)-
\frac{1}{n}I]^{-1}y^\star \Big\}</code>
</p>



<h3>Value</h3>

<table>
<tr><td><code>lambda0</code></td>
<td>
<p>(numeric) The estimated tuning parameter.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Wenying Deng
</p>


<h3>References</h3>

<p>Philip S. Boonstra, Bhramar Mukherjee, and Jeremy M. G. Taylor.
A Small-Sample Choice of the Tuning Parameter in Ridge Regression. July
2015.
</p>
<p>Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of
Statistical Learning: Data Mining, Inference, and Prediction, Second
Edition. Springer Series in Statistics. Springer- Verlag, New York, 2
edition, 2009.
</p>
<p>Hirotogu Akaike. Information Theory and an Extension of the Maximum
Likelihood Principle. In Selected Papers of Hirotugu Akaike, Springer
Series in Statistics, pages 199–213. Springer, New York, NY, 1998.
</p>
<p>Clifford M. Hurvich and Chih-Ling Tsai. Regression and time series model
selection in small samples. June 1989.
</p>
<p>Hurvich Clifford M., Simonoff Jeffrey S., and Tsai Chih-Ling. Smoothing
parameter selection in nonparametric regression using an improved Akaike
information criterion. January 2002.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
