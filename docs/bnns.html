<!DOCTYPE html><html lang="en"><head><title>Help for package bnns</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {bnns}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#bnns'><p>Generic Function for Fitting Bayesian Neural Network Models</p></a></li>
<li><a href='#bnns_train'><p>Internal function for training the BNN</p></a></li>
<li><a href='#bnns.default'><p>Bayesian Neural Network Model Using Formula(default) Interface</p></a></li>
<li><a href='#generate_stan_code'><p>Internal function to generate Stan Code Based on Output Activation Function</p></a></li>
<li><a href='#generate_stan_code_bin'><p>Internal function to generate Stan Code for Binary Response Models</p></a></li>
<li><a href='#generate_stan_code_cat'><p>Internal function to generate Stan Code for Neural Networks with Categorical Response</p></a></li>
<li><a href='#generate_stan_code_cont'><p>Internal function to generate Stan Code for Continuous Response Models</p></a></li>
<li><a href='#measure_bin'><p>Measure Performance for Binary Classification Models</p></a></li>
<li><a href='#measure_cat'><p>Measure Performance for Multi-Class Classification Models</p></a></li>
<li><a href='#measure_cont'><p>Measure Performance for Continuous Response Models</p></a></li>
<li><a href='#predict.bnns'><p>Predict Method for <code>"bnns"</code> Objects</p></a></li>
<li><a href='#print.bnns'><p>Print Method for <code>"bnns"</code> Objects</p></a></li>
<li><a href='#relu'><p>relu transformation</p></a></li>
<li><a href='#sigmoid'><p>sigmoid transformation</p></a></li>
<li><a href='#softmax_3d'><p>Apply Softmax Function to a 3D Array</p></a></li>
<li><a href='#softplus'><p>softplus transformation</p></a></li>
<li><a href='#summary.bnns'><p>Summary of a Bayesian Neural Network (BNN) Model</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>Bayesian Neural Network with 'Stan'</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.2</td>
</tr>
<tr>
<td>Description:</td>
<td>Offers a flexible formula-based interface for building and training Bayesian Neural Networks powered by 'Stan'. The package supports modeling complex relationships while providing rigorous uncertainty quantification via posterior distributions. With features like user chosen priors, clear predictions, and support for regression, binary, and multi-class classification, it is well-suited for applications in clinical trials, finance, and other fields requiring robust Bayesian inference and decision-making. References: Neal(1996) &lt;<a href="https://doi.org/10.1007%2F978-1-4612-0745-0">doi:10.1007/978-1-4612-0745-0</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.2</td>
</tr>
<tr>
<td>Suggests:</td>
<td>ggplot2, knitr, mlbench, ranger, rmarkdown, rsample, testthat
(&ge; 3.0.0)</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>Imports:</td>
<td>BH, pROC, RcppEigen, rstan, stats</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/swarnendu-stat/bnns">https://github.com/swarnendu-stat/bnns</a>,
<a href="https://swarnendu-stat.github.io/bnns/">https://swarnendu-stat.github.io/bnns/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/swarnendu-stat/bnns/issues">https://github.com/swarnendu-stat/bnns/issues</a></td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Config/Needs/website:</td>
<td>rmarkdown</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2025-01-10 13:05:02 UTC; antpc</td>
</tr>
<tr>
<td>Author:</td>
<td>Swarnendu Chatterjee [aut, cre, cph]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Swarnendu Chatterjee &lt;swarnendu.stat@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2025-01-13 17:30:17 UTC</td>
</tr>
</table>
<hr>
<h2 id='bnns'>Generic Function for Fitting Bayesian Neural Network Models</h2><span id='topic+bnns'></span>

<h3>Description</h3>

<p>This is a generic function for fitting Bayesian Neural Network (BNN) models. It dispatches to methods based on the class of the input data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bnns(
  formula,
  data,
  L = 1,
  nodes = rep(2, L),
  act_fn = rep(2, L),
  out_act_fn = 1,
  iter = 1000,
  warmup = 200,
  thin = 1,
  chains = 2,
  cores = 2,
  seed = 123,
  prior_weights = NULL,
  prior_bias = NULL,
  prior_sigma = NULL,
  verbose = FALSE,
  refresh = max(iter/10, 1),
  normalize = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="bnns_+3A_formula">formula</code></td>
<td>
<p>A symbolic description of the model to be fitted. The formula should specify the response variable and predictors (e.g., <code>y ~ x1 + x2</code>). <code>y</code> must be continuous for regression (<code>out_act_fn = 1</code>), numeric 0/1 for binary classification (<code>out_act_fn = 2</code>), and factor with at least 3 levels for multi-classification (<code>out_act_fn = 3</code>).</p>
</td></tr>
<tr><td><code id="bnns_+3A_data">data</code></td>
<td>
<p>A data frame containing the variables in the model.</p>
</td></tr>
<tr><td><code id="bnns_+3A_l">L</code></td>
<td>
<p>An integer specifying the number of hidden layers in the neural network. Default is 1.</p>
</td></tr>
<tr><td><code id="bnns_+3A_nodes">nodes</code></td>
<td>
<p>An integer or vector specifying the number of nodes in each hidden layer. If a single value is provided, it is applied to all layers. Default is 16.</p>
</td></tr>
<tr><td><code id="bnns_+3A_act_fn">act_fn</code></td>
<td>
<p>An integer or vector specifying the activation function(s) for the hidden layers. Options are:
</p>

<ul>
<li> <p><code>1</code> for tanh
</p>
</li>
<li> <p><code>2</code> for sigmoid (default)
</p>
</li>
<li> <p><code>3</code> for softplus
</p>
</li>
<li> <p><code>4</code> for ReLU
</p>
</li>
<li> <p><code>5</code> for linear
</p>
</li></ul>
</td></tr>
<tr><td><code id="bnns_+3A_out_act_fn">out_act_fn</code></td>
<td>
<p>An integer specifying the activation function for the output layer. Options are:
</p>

<ul>
<li> <p><code>1</code> for linear (default)
</p>
</li>
<li> <p><code>2</code> for sigmoid
</p>
</li>
<li> <p><code>3</code> for softmax
</p>
</li></ul>
</td></tr>
<tr><td><code id="bnns_+3A_iter">iter</code></td>
<td>
<p>An integer specifying the total number of iterations for the Stan sampler. Default is <code>1e3</code>.</p>
</td></tr>
<tr><td><code id="bnns_+3A_warmup">warmup</code></td>
<td>
<p>An integer specifying the number of warmup iterations for the Stan sampler. Default is <code>2e2</code>.</p>
</td></tr>
<tr><td><code id="bnns_+3A_thin">thin</code></td>
<td>
<p>An integer specifying the thinning interval for Stan samples. Default is 1.</p>
</td></tr>
<tr><td><code id="bnns_+3A_chains">chains</code></td>
<td>
<p>An integer specifying the number of Markov chains. Default is 2.</p>
</td></tr>
<tr><td><code id="bnns_+3A_cores">cores</code></td>
<td>
<p>An integer specifying the number of CPU cores to use for parallel sampling. Default is 2.</p>
</td></tr>
<tr><td><code id="bnns_+3A_seed">seed</code></td>
<td>
<p>An integer specifying the random seed for reproducibility. Default is 123.</p>
</td></tr>
<tr><td><code id="bnns_+3A_prior_weights">prior_weights</code></td>
<td>
<p>A list specifying the prior distribution for the weights in the neural network.
The list must include two components:
</p>

<ul>
<li> <p><code>dist</code>: A character string specifying the distribution type. Supported values are
<code>"normal"</code>, <code>"uniform"</code>, and <code>"cauchy"</code>.
</p>
</li>
<li> <p><code>params</code>: A named list specifying the parameters for the chosen distribution:
</p>

<ul>
<li><p> For <code>"normal"</code>: Provide <code>mean</code> (mean of the distribution) and <code>sd</code> (standard deviation).
</p>
</li>
<li><p> For <code>"uniform"</code>: Provide <code>alpha</code> (lower bound) and <code>beta</code> (upper bound).
</p>
</li>
<li><p> For <code>"cauchy"</code>: Provide <code>mu</code> (location parameter) and <code>sigma</code> (scale parameter).
</p>
</li></ul>

</li></ul>

<p>If <code>prior_weights</code> is <code>NULL</code>, the default prior is a <code>normal(0, 1)</code> distribution.
For example:
</p>

<ul>
<li> <p><code>list(dist = "normal", params = list(mean = 0, sd = 1))</code>
</p>
</li>
<li> <p><code>list(dist = "uniform", params = list(alpha = -1, beta = 1))</code>
</p>
</li>
<li> <p><code>list(dist = "cauchy", params = list(mu = 0, sigma = 2.5))</code>
</p>
</li></ul>
</td></tr>
<tr><td><code id="bnns_+3A_prior_bias">prior_bias</code></td>
<td>
<p>A list specifying the prior distribution for the biases in the neural network.
The list must include two components:
</p>

<ul>
<li> <p><code>dist</code>: A character string specifying the distribution type. Supported values are
<code>"normal"</code>, <code>"uniform"</code>, and <code>"cauchy"</code>.
</p>
</li>
<li> <p><code>params</code>: A named list specifying the parameters for the chosen distribution:
</p>

<ul>
<li><p> For <code>"normal"</code>: Provide <code>mean</code> (mean of the distribution) and <code>sd</code> (standard deviation).
</p>
</li>
<li><p> For <code>"uniform"</code>: Provide <code>alpha</code> (lower bound) and <code>beta</code> (upper bound).
</p>
</li>
<li><p> For <code>"cauchy"</code>: Provide <code>mu</code> (location parameter) and <code>sigma</code> (scale parameter).
</p>
</li></ul>

</li></ul>

<p>If <code>prior_bias</code> is <code>NULL</code>, the default prior is a <code>normal(0, 1)</code> distribution.
For example:
</p>

<ul>
<li> <p><code>list(dist = "normal", params = list(mean = 0, sd = 1))</code>
</p>
</li>
<li> <p><code>list(dist = "uniform", params = list(alpha = -1, beta = 1))</code>
</p>
</li>
<li> <p><code>list(dist = "cauchy", params = list(mu = 0, sigma = 2.5))</code>
</p>
</li></ul>
</td></tr>
<tr><td><code id="bnns_+3A_prior_sigma">prior_sigma</code></td>
<td>
<p>A list specifying the prior distribution for the <code>sigma</code> parameter in regression
models (<code>out_act_fn = 1</code>). This allows for setting priors on the standard deviation of the residuals.
The list must include two components:
</p>

<ul>
<li> <p><code>dist</code>: A character string specifying the distribution type. Supported values are
<code>"half-normal"</code> and <code>"inverse-gamma"</code>.
</p>
</li>
<li> <p><code>params</code>: A named list specifying the parameters for the chosen distribution:
</p>

<ul>
<li><p> For <code>"half-normal"</code>: Provide <code>sd</code> (standard deviation of the half-normal distribution).
</p>
</li>
<li><p> For <code>"inverse-gamma"</code>: Provide <code>shape</code> (shape parameter) and <code>scale</code> (scale parameter).
</p>
</li></ul>

</li></ul>

<p>If <code>prior_sigma</code> is <code>NULL</code>, the default prior is a <code>half-normal(0, 1)</code> distribution.
For example:
</p>

<ul>
<li> <p><code>list(dist = "half_normal", params = list(mean = 0, sd = 1))</code>
</p>
</li>
<li> <p><code>list(dist = "inv_gamma", params = list(alpha = 1, beta = 1))</code>
</p>
</li></ul>
</td></tr>
<tr><td><code id="bnns_+3A_verbose">verbose</code></td>
<td>
<p>TRUE or FALSE: flag indicating whether to print intermediate output from Stan on the console, which might be helpful for model debugging.</p>
</td></tr>
<tr><td><code id="bnns_+3A_refresh">refresh</code></td>
<td>
<p>refresh (integer) can be used to control how often the progress of the sampling is reported (i.e. show the progress every refresh iterations). By default, refresh = max(iter/10, 1). The progress indicator is turned off if refresh &lt;= 0.</p>
</td></tr>
<tr><td><code id="bnns_+3A_normalize">normalize</code></td>
<td>
<p>Logical. If <code>TRUE</code> (default), the input predictors
are normalized to have zero mean and unit variance before training.
Normalization ensures stable and efficient Bayesian sampling by standardizing
the input scale, which is particularly beneficial for neural network training.
If <code>FALSE</code>, no normalization is applied, and it is assumed that the input data
is already pre-processed appropriately.</p>
</td></tr>
<tr><td><code id="bnns_+3A_...">...</code></td>
<td>
<p>Currently not in use.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function serves as a generic interface to different methods of fitting Bayesian Neural Networks. The specific method dispatched depends on the class of the input arguments, allowing for flexibility in the types of inputs supported.
</p>


<h3>Value</h3>

<p>The result of the method dispatched by the class of the input data. Typically, this would be an object of class <code>"bnns"</code> containing the fitted model and associated information.
</p>


<h3>References</h3>


<ol>
<li><p> Bishop, C.M., 1995. Neural networks for pattern recognition. Oxford university press.
</p>
</li>
<li><p> Carpenter, B., Gelman, A., Hoffman, M.D., Lee, D., Goodrich, B., Betancourt, M., Brubaker, M.A., Guo, J., Li, P. and Riddell, A., 2017. Stan: A probabilistic programming language. Journal of statistical software, 76.
</p>
</li>
<li><p> Neal, R.M., 2012. Bayesian learning for neural networks (Vol. 118). Springer Science &amp; Business Media.
</p>
</li></ol>



<h3>See Also</h3>

<p><code><a href="#topic+bnns.default">bnns.default</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Example usage with formula interface:
data &lt;- data.frame(x1 = runif(10), x2 = runif(10), y = rnorm(10))
model &lt;- bnns(y ~ -1 + x1 + x2,
  data = data, L = 1, nodes = 2, act_fn = 1,
  iter = 1e1, warmup = 5, chains = 1
)

# See the documentation for bnns.default for more details on the default implementation.

</code></pre>

<hr>
<h2 id='bnns_train'>Internal function for training the BNN</h2><span id='topic+bnns_train'></span>

<h3>Description</h3>

<p>This function performs the actual fitting of the Bayesian Neural Network.
It is called by the exported bnns methods and is not intended for direct use.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bnns_train(
  train_x,
  train_y,
  L = 1,
  nodes = rep(2, L),
  act_fn = rep(2, L),
  out_act_fn = 1,
  iter = 1000,
  warmup = 200,
  thin = 1,
  chains = 2,
  cores = 2,
  seed = 123,
  prior_weights = NULL,
  prior_bias = NULL,
  prior_sigma = NULL,
  verbose = FALSE,
  refresh = max(iter/10, 1),
  normalize = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="bnns_train_+3A_train_x">train_x</code></td>
<td>
<p>A numeric matrix representing the input features (predictors) for training. Rows correspond to observations, and columns correspond to features.</p>
</td></tr>
<tr><td><code id="bnns_train_+3A_train_y">train_y</code></td>
<td>
<p>A numeric vector representing the target values for training. Its length must match the number of rows in <code>train_x</code>.</p>
</td></tr>
<tr><td><code id="bnns_train_+3A_l">L</code></td>
<td>
<p>An integer specifying the number of hidden layers in the neural network. Default is 1.</p>
</td></tr>
<tr><td><code id="bnns_train_+3A_nodes">nodes</code></td>
<td>
<p>An integer or vector specifying the number of nodes in each hidden layer. If a single value is provided, it is applied to all layers. Default is 16.</p>
</td></tr>
<tr><td><code id="bnns_train_+3A_act_fn">act_fn</code></td>
<td>
<p>An integer or vector specifying the activation function(s) for the hidden layers. Options are:
</p>

<ul>
<li> <p><code>1</code> for tanh
</p>
</li>
<li> <p><code>2</code> for sigmoid (default)
</p>
</li>
<li> <p><code>3</code> for softplus
</p>
</li>
<li> <p><code>4</code> for ReLU
</p>
</li>
<li> <p><code>5</code> for linear
</p>
</li></ul>
</td></tr>
<tr><td><code id="bnns_train_+3A_out_act_fn">out_act_fn</code></td>
<td>
<p>An integer specifying the activation function for the output layer. Options are:
</p>

<ul>
<li> <p><code>1</code> for linear (default)
</p>
</li>
<li> <p><code>2</code> for sigmoid
</p>
</li>
<li> <p><code>3</code> for softmax
</p>
</li></ul>
</td></tr>
<tr><td><code id="bnns_train_+3A_iter">iter</code></td>
<td>
<p>An integer specifying the total number of iterations for the Stan sampler. Default is <code>1e3</code>.</p>
</td></tr>
<tr><td><code id="bnns_train_+3A_warmup">warmup</code></td>
<td>
<p>An integer specifying the number of warmup iterations for the Stan sampler. Default is <code>2e2</code>.</p>
</td></tr>
<tr><td><code id="bnns_train_+3A_thin">thin</code></td>
<td>
<p>An integer specifying the thinning interval for Stan samples. Default is 1.</p>
</td></tr>
<tr><td><code id="bnns_train_+3A_chains">chains</code></td>
<td>
<p>An integer specifying the number of Markov chains. Default is 2.</p>
</td></tr>
<tr><td><code id="bnns_train_+3A_cores">cores</code></td>
<td>
<p>An integer specifying the number of CPU cores to use for parallel sampling. Default is 2.</p>
</td></tr>
<tr><td><code id="bnns_train_+3A_seed">seed</code></td>
<td>
<p>An integer specifying the random seed for reproducibility. Default is 123.</p>
</td></tr>
<tr><td><code id="bnns_train_+3A_prior_weights">prior_weights</code></td>
<td>
<p>A list specifying the prior distribution for the weights in the neural network.
The list must include two components:
</p>

<ul>
<li> <p><code>dist</code>: A character string specifying the distribution type. Supported values are
<code>"normal"</code>, <code>"uniform"</code>, and <code>"cauchy"</code>.
</p>
</li>
<li> <p><code>params</code>: A named list specifying the parameters for the chosen distribution:
</p>

<ul>
<li><p> For <code>"normal"</code>: Provide <code>mean</code> (mean of the distribution) and <code>sd</code> (standard deviation).
</p>
</li>
<li><p> For <code>"uniform"</code>: Provide <code>alpha</code> (lower bound) and <code>beta</code> (upper bound).
</p>
</li>
<li><p> For <code>"cauchy"</code>: Provide <code>mu</code> (location parameter) and <code>sigma</code> (scale parameter).
</p>
</li></ul>

</li></ul>

<p>If <code>prior_weights</code> is <code>NULL</code>, the default prior is a <code>normal(0, 1)</code> distribution.
For example:
</p>

<ul>
<li> <p><code>list(dist = "normal", params = list(mean = 0, sd = 1))</code>
</p>
</li>
<li> <p><code>list(dist = "uniform", params = list(alpha = -1, beta = 1))</code>
</p>
</li>
<li> <p><code>list(dist = "cauchy", params = list(mu = 0, sigma = 2.5))</code>
</p>
</li></ul>
</td></tr>
<tr><td><code id="bnns_train_+3A_prior_bias">prior_bias</code></td>
<td>
<p>A list specifying the prior distribution for the biases in the neural network.
The list must include two components:
</p>

<ul>
<li> <p><code>dist</code>: A character string specifying the distribution type. Supported values are
<code>"normal"</code>, <code>"uniform"</code>, and <code>"cauchy"</code>.
</p>
</li>
<li> <p><code>params</code>: A named list specifying the parameters for the chosen distribution:
</p>

<ul>
<li><p> For <code>"normal"</code>: Provide <code>mean</code> (mean of the distribution) and <code>sd</code> (standard deviation).
</p>
</li>
<li><p> For <code>"uniform"</code>: Provide <code>alpha</code> (lower bound) and <code>beta</code> (upper bound).
</p>
</li>
<li><p> For <code>"cauchy"</code>: Provide <code>mu</code> (location parameter) and <code>sigma</code> (scale parameter).
</p>
</li></ul>

</li></ul>

<p>If <code>prior_bias</code> is <code>NULL</code>, the default prior is a <code>normal(0, 1)</code> distribution.
For example:
</p>

<ul>
<li> <p><code>list(dist = "normal", params = list(mean = 0, sd = 1))</code>
</p>
</li>
<li> <p><code>list(dist = "uniform", params = list(alpha = -1, beta = 1))</code>
</p>
</li>
<li> <p><code>list(dist = "cauchy", params = list(mu = 0, sigma = 2.5))</code>
</p>
</li></ul>
</td></tr>
<tr><td><code id="bnns_train_+3A_prior_sigma">prior_sigma</code></td>
<td>
<p>A list specifying the prior distribution for the <code>sigma</code> parameter in regression
models (<code>out_act_fn = 1</code>). This allows for setting priors on the standard deviation of the residuals.
The list must include two components:
</p>

<ul>
<li> <p><code>dist</code>: A character string specifying the distribution type. Supported values are
<code>"half-normal"</code> and <code>"inverse-gamma"</code>.
</p>
</li>
<li> <p><code>params</code>: A named list specifying the parameters for the chosen distribution:
</p>

<ul>
<li><p> For <code>"half-normal"</code>: Provide <code>sd</code> (standard deviation of the half-normal distribution).
</p>
</li>
<li><p> For <code>"inverse-gamma"</code>: Provide <code>shape</code> (shape parameter) and <code>scale</code> (scale parameter).
</p>
</li></ul>

</li></ul>

<p>If <code>prior_sigma</code> is <code>NULL</code>, the default prior is a <code>half-normal(0, 1)</code> distribution.
For example:
</p>

<ul>
<li> <p><code>list(dist = "half_normal", params = list(mean = 0, sd = 1))</code>
</p>
</li>
<li> <p><code>list(dist = "inv_gamma", params = list(alpha = 1, beta = 1))</code>
</p>
</li></ul>
</td></tr>
<tr><td><code id="bnns_train_+3A_verbose">verbose</code></td>
<td>
<p>TRUE or FALSE: flag indicating whether to print intermediate output from Stan on the console, which might be helpful for model debugging.</p>
</td></tr>
<tr><td><code id="bnns_train_+3A_refresh">refresh</code></td>
<td>
<p>refresh (integer) can be used to control how often the progress of the sampling is reported (i.e. show the progress every refresh iterations). By default, refresh = max(iter/10, 1). The progress indicator is turned off if refresh &lt;= 0.</p>
</td></tr>
<tr><td><code id="bnns_train_+3A_normalize">normalize</code></td>
<td>
<p>Logical. If <code>TRUE</code> (default), the input predictors
are normalized to have zero mean and unit variance before training.
Normalization ensures stable and efficient Bayesian sampling by standardizing
the input scale, which is particularly beneficial for neural network training.
If <code>FALSE</code>, no normalization is applied, and it is assumed that the input data
is already pre-processed appropriately.</p>
</td></tr>
<tr><td><code id="bnns_train_+3A_...">...</code></td>
<td>
<p>Currently not in use.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function uses the <code>generate_stan_code</code> function to dynamically generate Stan code based on the specified number of layers and nodes. Stan is then used to fit the Bayesian Neural Network.
</p>


<h3>Value</h3>

<p>An object of class <code>"bnns"</code> containing the following components:
</p>

<dl>
<dt><code>fit</code></dt><dd><p>The fitted Stan model object.</p>
</dd>
<dt><code>call</code></dt><dd><p>The matched call.</p>
</dd>
<dt><code>data</code></dt><dd><p>A list containing the Stan data used in the model.</p>
</dd>
</dl>



<h3>See Also</h3>

<p><code><a href="rstan.html#topic+stan">stan</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Example usage:
train_x &lt;- matrix(runif(20), nrow = 10, ncol = 2)
train_y &lt;- rnorm(10)
model &lt;- bnns::bnns_train(train_x, train_y,
  L = 1, nodes = 2, act_fn = 2,
  iter = 1e1, warmup = 5, chains = 1
)

# Access Stan model fit
model$fit

</code></pre>

<hr>
<h2 id='bnns.default'>Bayesian Neural Network Model Using Formula(default) Interface</h2><span id='topic+bnns.default'></span>

<h3>Description</h3>

<p>Fits a Bayesian Neural Network (BNN) model using a formula interface. The function parses the formula and data to create the input feature matrix and target vector, then fits the model using <code><a href="#topic+bnns.default">bnns.default</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## Default S3 method:
bnns(
  formula,
  data,
  L = 1,
  nodes = rep(2, L),
  act_fn = rep(2, L),
  out_act_fn = 1,
  iter = 1000,
  warmup = 200,
  thin = 1,
  chains = 2,
  cores = 2,
  seed = 123,
  prior_weights = NULL,
  prior_bias = NULL,
  prior_sigma = NULL,
  verbose = FALSE,
  refresh = max(iter/10, 1),
  normalize = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="bnns.default_+3A_formula">formula</code></td>
<td>
<p>A symbolic description of the model to be fitted. The formula should specify the response variable and predictors (e.g., <code>y ~ x1 + x2</code>).</p>
</td></tr>
<tr><td><code id="bnns.default_+3A_data">data</code></td>
<td>
<p>A data frame containing the variables in the model.</p>
</td></tr>
<tr><td><code id="bnns.default_+3A_l">L</code></td>
<td>
<p>An integer specifying the number of hidden layers in the neural network. Default is 1.</p>
</td></tr>
<tr><td><code id="bnns.default_+3A_nodes">nodes</code></td>
<td>
<p>An integer or vector specifying the number of nodes in each hidden layer. If a single value is provided, it is applied to all layers. Default is 16.</p>
</td></tr>
<tr><td><code id="bnns.default_+3A_act_fn">act_fn</code></td>
<td>
<p>An integer or vector specifying the activation function(s) for the hidden layers. Options are:
</p>

<ul>
<li> <p><code>1</code> for tanh
</p>
</li>
<li> <p><code>2</code> for sigmoid (default)
</p>
</li>
<li> <p><code>3</code> for softplus
</p>
</li>
<li> <p><code>4</code> for ReLU
</p>
</li>
<li> <p><code>5</code> for linear
</p>
</li></ul>
</td></tr>
<tr><td><code id="bnns.default_+3A_out_act_fn">out_act_fn</code></td>
<td>
<p>An integer specifying the activation function for the output layer. Options are:
</p>

<ul>
<li> <p><code>1</code> for linear (default)
</p>
</li>
<li> <p><code>2</code> for sigmoid
</p>
</li>
<li> <p><code>3</code> for softmax
</p>
</li></ul>
</td></tr>
<tr><td><code id="bnns.default_+3A_iter">iter</code></td>
<td>
<p>An integer specifying the total number of iterations for the Stan sampler. Default is <code>1e3</code>.</p>
</td></tr>
<tr><td><code id="bnns.default_+3A_warmup">warmup</code></td>
<td>
<p>An integer specifying the number of warmup iterations for the Stan sampler. Default is <code>2e2</code>.</p>
</td></tr>
<tr><td><code id="bnns.default_+3A_thin">thin</code></td>
<td>
<p>An integer specifying the thinning interval for Stan samples. Default is 1.</p>
</td></tr>
<tr><td><code id="bnns.default_+3A_chains">chains</code></td>
<td>
<p>An integer specifying the number of Markov chains. Default is 2.</p>
</td></tr>
<tr><td><code id="bnns.default_+3A_cores">cores</code></td>
<td>
<p>An integer specifying the number of CPU cores to use for parallel sampling. Default is 2.</p>
</td></tr>
<tr><td><code id="bnns.default_+3A_seed">seed</code></td>
<td>
<p>An integer specifying the random seed for reproducibility. Default is 123.</p>
</td></tr>
<tr><td><code id="bnns.default_+3A_prior_weights">prior_weights</code></td>
<td>
<p>A list specifying the prior distribution for the weights in the neural network.
The list must include two components:
</p>

<ul>
<li> <p><code>dist</code>: A character string specifying the distribution type. Supported values are
<code>"normal"</code>, <code>"uniform"</code>, and <code>"cauchy"</code>.
</p>
</li>
<li> <p><code>params</code>: A named list specifying the parameters for the chosen distribution:
</p>

<ul>
<li><p> For <code>"normal"</code>: Provide <code>mean</code> (mean of the distribution) and <code>sd</code> (standard deviation).
</p>
</li>
<li><p> For <code>"uniform"</code>: Provide <code>alpha</code> (lower bound) and <code>beta</code> (upper bound).
</p>
</li>
<li><p> For <code>"cauchy"</code>: Provide <code>mu</code> (location parameter) and <code>sigma</code> (scale parameter).
</p>
</li></ul>

</li></ul>

<p>If <code>prior_weights</code> is <code>NULL</code>, the default prior is a <code>normal(0, 1)</code> distribution.
For example:
</p>

<ul>
<li> <p><code>list(dist = "normal", params = list(mean = 0, sd = 1))</code>
</p>
</li>
<li> <p><code>list(dist = "uniform", params = list(alpha = -1, beta = 1))</code>
</p>
</li>
<li> <p><code>list(dist = "cauchy", params = list(mu = 0, sigma = 2.5))</code>
</p>
</li></ul>
</td></tr>
<tr><td><code id="bnns.default_+3A_prior_bias">prior_bias</code></td>
<td>
<p>A list specifying the prior distribution for the biases in the neural network.
The list must include two components:
</p>

<ul>
<li> <p><code>dist</code>: A character string specifying the distribution type. Supported values are
<code>"normal"</code>, <code>"uniform"</code>, and <code>"cauchy"</code>.
</p>
</li>
<li> <p><code>params</code>: A named list specifying the parameters for the chosen distribution:
</p>

<ul>
<li><p> For <code>"normal"</code>: Provide <code>mean</code> (mean of the distribution) and <code>sd</code> (standard deviation).
</p>
</li>
<li><p> For <code>"uniform"</code>: Provide <code>alpha</code> (lower bound) and <code>beta</code> (upper bound).
</p>
</li>
<li><p> For <code>"cauchy"</code>: Provide <code>mu</code> (location parameter) and <code>sigma</code> (scale parameter).
</p>
</li></ul>

</li></ul>

<p>If <code>prior_bias</code> is <code>NULL</code>, the default prior is a <code>normal(0, 1)</code> distribution.
For example:
</p>

<ul>
<li> <p><code>list(dist = "normal", params = list(mean = 0, sd = 1))</code>
</p>
</li>
<li> <p><code>list(dist = "uniform", params = list(alpha = -1, beta = 1))</code>
</p>
</li>
<li> <p><code>list(dist = "cauchy", params = list(mu = 0, sigma = 2.5))</code>
</p>
</li></ul>
</td></tr>
<tr><td><code id="bnns.default_+3A_prior_sigma">prior_sigma</code></td>
<td>
<p>A list specifying the prior distribution for the <code>sigma</code> parameter in regression
models (<code>out_act_fn = 1</code>). This allows for setting priors on the standard deviation of the residuals.
The list must include two components:
</p>

<ul>
<li> <p><code>dist</code>: A character string specifying the distribution type. Supported values are
<code>"half-normal"</code> and <code>"inverse-gamma"</code>.
</p>
</li>
<li> <p><code>params</code>: A named list specifying the parameters for the chosen distribution:
</p>

<ul>
<li><p> For <code>"half-normal"</code>: Provide <code>sd</code> (standard deviation of the half-normal distribution).
</p>
</li>
<li><p> For <code>"inverse-gamma"</code>: Provide <code>shape</code> (shape parameter) and <code>scale</code> (scale parameter).
</p>
</li></ul>

</li></ul>

<p>If <code>prior_sigma</code> is <code>NULL</code>, the default prior is a <code>half-normal(0, 1)</code> distribution.
For example:
</p>

<ul>
<li> <p><code>list(dist = "half_normal", params = list(mean = 0, sd = 1))</code>
</p>
</li>
<li> <p><code>list(dist = "inv_gamma", params = list(alpha = 1, beta = 1))</code>
</p>
</li></ul>
</td></tr>
<tr><td><code id="bnns.default_+3A_verbose">verbose</code></td>
<td>
<p>TRUE or FALSE: flag indicating whether to print intermediate output from Stan on the console, which might be helpful for model debugging.</p>
</td></tr>
<tr><td><code id="bnns.default_+3A_refresh">refresh</code></td>
<td>
<p>refresh (integer) can be used to control how often the progress of the sampling is reported (i.e. show the progress every refresh iterations). By default, refresh = max(iter/10, 1). The progress indicator is turned off if refresh &lt;= 0.</p>
</td></tr>
<tr><td><code id="bnns.default_+3A_normalize">normalize</code></td>
<td>
<p>Logical. If <code>TRUE</code> (default), the input predictors
are normalized to have zero mean and unit variance before training.
Normalization ensures stable and efficient Bayesian sampling by standardizing
the input scale, which is particularly beneficial for neural network training.
If <code>FALSE</code>, no normalization is applied, and it is assumed that the input data
is already pre-processed appropriately.</p>
</td></tr>
<tr><td><code id="bnns.default_+3A_...">...</code></td>
<td>
<p>Currently not in use.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function uses the provided formula and data to generate the design matrix for the predictors and the response vector. It then calls helper function bnns_train to fit the Bayesian Neural Network model.
</p>


<h3>Value</h3>

<p>An object of class <code>"bnns"</code> containing the fitted model and associated information, including:
</p>

<ul>
<li> <p><code>fit</code>: The fitted Stan model object.
</p>
</li>
<li> <p><code>data</code>: A list containing the processed training data.
</p>
</li>
<li> <p><code>call</code>: The matched function call.
</p>
</li>
<li> <p><code>formula</code>: The formula used for the model.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
# Example usage:
data &lt;- data.frame(x1 = runif(10), x2 = runif(10), y = rnorm(10))
model &lt;- bnns(y ~ -1 + x1 + x2,
  data = data, L = 1, nodes = 2, act_fn = 3,
  iter = 1e1, warmup = 5, chains = 1
)

</code></pre>

<hr>
<h2 id='generate_stan_code'>Internal function to generate Stan Code Based on Output Activation Function</h2><span id='topic+generate_stan_code'></span>

<h3>Description</h3>

<p>This function serves as a wrapper to generate Stan code for Bayesian neural networks
tailored to different types of response variables. Based on the specified output
activation function (<code>out_act_fn</code>), it delegates the code generation to the
appropriate function for continuous, binary, or categorical response models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>generate_stan_code(num_layers, nodes, out_act_fn = 1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="generate_stan_code_+3A_num_layers">num_layers</code></td>
<td>
<p>An integer specifying the number of hidden layers in the neural network.</p>
</td></tr>
<tr><td><code id="generate_stan_code_+3A_nodes">nodes</code></td>
<td>
<p>A vector of integers, where each element specifies the number of nodes
in the corresponding hidden layer. The length of the vector must match <code>num_layers</code>.</p>
</td></tr>
<tr><td><code id="generate_stan_code_+3A_out_act_fn">out_act_fn</code></td>
<td>
<p>An integer specifying the output activation function, determining
the type of response variable. Supported values are:
</p>

<ul>
<li> <p><code>1</code>: Continuous response (identity function as output layer).
</p>
</li>
<li> <p><code>2</code>: Binary response (sigmoid function as output layer).
</p>
</li>
<li> <p><code>3</code>: Categorical response (softmax function as output layer).
</p>
</li></ul>
</td></tr>
</table>


<h3>Details</h3>

<p>This function dynamically calls one of the following functions based on the value of <code>out_act_fn</code>:
</p>

<ul>
<li> <p><strong>Continuous response:</strong> Calls <code>generate_stan_code_cont</code>.
</p>
</li>
<li> <p><strong>Binary response:</strong> Calls <code>generate_stan_code_bin</code>.
</p>
</li>
<li> <p><strong>Categorical response:</strong> Calls <code>generate_stan_code_cat</code>.
</p>
</li></ul>

<p>If an unsupported value is provided for <code>out_act_fn</code>, the function throws an error.
The generated Stan code is adapted for the response type, including appropriate
likelihood functions and transformations.
</p>


<h3>Value</h3>

<p>A character string containing the Stan code for the specified Bayesian neural network model.
The Stan model includes data, parameters, transformed parameters, and model blocks,
adjusted based on the specified response type.
</p>


<h3>See Also</h3>


<ul>
<li> <p><a href="#topic+generate_stan_code_cont">generate_stan_code_cont</a>: For continuous response models.
</p>
</li>
<li> <p><a href="#topic+generate_stan_code_bin">generate_stan_code_bin</a>: For binary response models.
</p>
</li>
<li> <p><a href="#topic+generate_stan_code_cat">generate_stan_code_cat</a>: For categorical response models.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'># Generate Stan code for a continuous response model
stan_code &lt;- generate_stan_code(num_layers = 2, nodes = c(10, 5), out_act_fn = 1)
cat(stan_code)

# Generate Stan code for a binary response model
stan_code &lt;- generate_stan_code(num_layers = 2, nodes = c(10, 5), out_act_fn = 2)
cat(stan_code)

# Generate Stan code for a categorical response model
stan_code &lt;- generate_stan_code(num_layers = 2, nodes = c(10, 5), out_act_fn = 3)
cat(stan_code)

</code></pre>

<hr>
<h2 id='generate_stan_code_bin'>Internal function to generate Stan Code for Binary Response Models</h2><span id='topic+generate_stan_code_bin'></span>

<h3>Description</h3>

<p>This function generates Stan code for a Bayesian neural network model
designed to predict binary response variables. The Stan code is dynamically
constructed based on the specified number of hidden layers and nodes per layer.
It supports various activation functions for the hidden layers, including
tanh, sigmoid, softplus and relu. The model uses a Bernoulli likelihood for binary outcomes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>generate_stan_code_bin(num_layers, nodes)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="generate_stan_code_bin_+3A_num_layers">num_layers</code></td>
<td>
<p>An integer specifying the number of hidden layers in the neural network.</p>
</td></tr>
<tr><td><code id="generate_stan_code_bin_+3A_nodes">nodes</code></td>
<td>
<p>A vector of integers, where each element specifies the number of nodes
in the corresponding hidden layer. The length of the vector must match <code>num_layers</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The generated Stan code models a binary response variable using a neural network.
The hidden layers apply the specified activation functions, while the output layer
applies the logistic function to predict the probability of the binary outcome.
</p>

<ul>
<li> <p><strong>For one hidden layer:</strong> The function simplifies the Stan code structure.
</p>
</li>
<li> <p><strong>For multiple hidden layers:</strong> The code dynamically includes additional layers
based on the input arguments.
</p>
</li></ul>

<p>Supported activation functions for the hidden layers:
</p>

<ul>
<li><p> 1: Tanh
</p>
</li>
<li><p> 2: Sigmoid
</p>
</li>
<li><p> 3: Softplus
</p>
</li>
<li><p> 4: ReLU
</p>
</li>
<li><p> 5: linear
</p>
</li></ul>

<p>The output layer uses a logistic transformation (<code>inv_logit</code>) to constrain
predictions between 0 and 1, which aligns with the Bernoulli likelihood.
</p>


<h3>Value</h3>

<p>A character string containing the Stan code for the specified Bayesian neural network model.
The Stan model includes data, parameters, transformed parameters, and model blocks.
The code is adjusted based on whether the network has one or multiple hidden layers.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Generate Stan code for a single hidden layer with 10 nodes
stan_code &lt;- generate_stan_code_bin(1, c(10))
cat(stan_code)

# Generate Stan code for two hidden layers with 8 and 4 nodes
stan_code &lt;- generate_stan_code_bin(2, c(8, 4))
cat(stan_code)

</code></pre>

<hr>
<h2 id='generate_stan_code_cat'>Internal function to generate Stan Code for Neural Networks with Categorical Response</h2><span id='topic+generate_stan_code_cat'></span>

<h3>Description</h3>

<p>This function generates Stan code for modeling a categorical response using
neural networks with multiple layers. The generated code supports customizable
activation functions for each layer and softmax-based prediction for the categorical output.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>generate_stan_code_cat(num_layers, nodes)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="generate_stan_code_cat_+3A_num_layers">num_layers</code></td>
<td>
<p>Integer. Number of layers in the neural network.</p>
</td></tr>
<tr><td><code id="generate_stan_code_cat_+3A_nodes">nodes</code></td>
<td>
<p>Integer vector. Number of nodes in each layer. The length of
this vector must match <code>num_layers</code>, and all values must be positive.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Stan code includes the following components:
</p>

<ul>
<li> <p><strong>Data Block</strong>: Defines inputs, response variable, layer configurations, and activation functions.
</p>
</li>
<li> <p><strong>Parameters Block</strong>: Declares weights and biases for all layers and the output layer.
</p>
</li>
<li> <p><strong>Transformed Parameters Block</strong>: Computes intermediate outputs (<code>z</code> and <code>a</code>) for each layer
and calculates the final predictions (<code>y_hat</code>) using the softmax function.
</p>
</li>
<li> <p><strong>Model Block</strong>: Specifies priors for parameters and models the categorical response
using <code>categorical_logit</code>.
</p>
</li></ul>

<p>Supported activation functions for the hidden layers:
</p>

<ul>
<li><p> 1: Tanh
</p>
</li>
<li><p> 2: Sigmoid
</p>
</li>
<li><p> 3: Softplus
</p>
</li>
<li><p> 4: ReLU
</p>
</li>
<li><p> 5: linear
</p>
</li></ul>

<p>The categorical response (<code>y</code>) is assumed to take integer values from 1 to <code>K</code>,
where <code>K</code> is the total number of categories.
</p>


<h3>Value</h3>

<p>A string containing the Stan code for the specified neural network
architecture and categorical response model.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+generate_stan_code_bin">generate_stan_code_bin()</a></code>, <code><a href="#topic+generate_stan_code_cont">generate_stan_code_cont()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Generate Stan code for a neural network with 3 layers
num_layers &lt;- 3
nodes &lt;- c(10, 8, 6) # 10 nodes in the first layer, 8 in the second, 6 in the third
stan_code &lt;- generate_stan_code_cat(num_layers, nodes)
cat(stan_code)

</code></pre>

<hr>
<h2 id='generate_stan_code_cont'>Internal function to generate Stan Code for Continuous Response Models</h2><span id='topic+generate_stan_code_cont'></span>

<h3>Description</h3>

<p>This function generates Stan code for a Bayesian neural network model
designed to predict continuous response variables. The Stan code is dynamically
constructed based on the specified number of hidden layers and nodes per layer.
It supports various activation functions for the hidden layers, including
tanh, sigmoid, softplus and relu.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>generate_stan_code_cont(num_layers, nodes)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="generate_stan_code_cont_+3A_num_layers">num_layers</code></td>
<td>
<p>An integer specifying the number of hidden layers in the neural network.</p>
</td></tr>
<tr><td><code id="generate_stan_code_cont_+3A_nodes">nodes</code></td>
<td>
<p>A vector of integers, where each element specifies the number of nodes
in the corresponding hidden layer. The length of the vector must match <code>num_layers</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The generated Stan code models a continuous response variable using a neural network.
The hidden layers apply the specified activation functions, while the output layer
performs a linear transformation to predict the response. The likelihood assumes
normally distributed residuals.
</p>

<ul>
<li> <p><strong>For one hidden layer:</strong> The function simplifies the Stan code structure.
</p>
</li>
<li> <p><strong>For multiple hidden layers:</strong> The code dynamically includes additional layers
based on the input arguments.
</p>
</li></ul>

<p>Supported activation functions for the hidden layers:
</p>

<ul>
<li><p> 1: Tanh
</p>
</li>
<li><p> 2: Sigmoid
</p>
</li>
<li><p> 3: Softplus
</p>
</li>
<li><p> 4: ReLU
</p>
</li>
<li><p> 5: linear
</p>
</li></ul>



<h3>Value</h3>

<p>A character string containing the Stan code for the specified Bayesian neural network model.
The Stan model includes data, parameters, transformed parameters, and model blocks.
The code is adjusted based on whether the network has one or multiple hidden layers.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Generate Stan code for a single hidden layer with 10 nodes
stan_code &lt;- generate_stan_code_cont(1, c(10))
cat(stan_code)

# Generate Stan code for two hidden layers with 8 and 4 nodes
stan_code &lt;- generate_stan_code_cont(2, c(8, 4))
cat(stan_code)

</code></pre>

<hr>
<h2 id='measure_bin'>Measure Performance for Binary Classification Models</h2><span id='topic+measure_bin'></span>

<h3>Description</h3>

<p>Evaluates the performance of a binary classification model using a confusion matrix and accuracy.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>measure_bin(obs, pred, cut = 0.5)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="measure_bin_+3A_obs">obs</code></td>
<td>
<p>A numeric or integer vector of observed binary class labels (0 or 1).</p>
</td></tr>
<tr><td><code id="measure_bin_+3A_pred">pred</code></td>
<td>
<p>A numeric vector of predicted probabilities for the positive class.</p>
</td></tr>
<tr><td><code id="measure_bin_+3A_cut">cut</code></td>
<td>
<p>A numeric threshold (between 0 and 1) to classify predictions into binary labels.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing:
</p>

<dl>
<dt><code>conf_mat</code></dt><dd><p>A confusion matrix comparing observed and predicted class labels.</p>
</dd>
<dt><code>accuracy</code></dt><dd><p>The proportion of correct predictions.</p>
</dd>
<dt><code>ROC</code></dt><dd><p>ROC generated using <code>pROC::roc</code></p>
</dd>
<dt><code>AUC</code></dt><dd><p>Area under the ROC curve.</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>obs &lt;- c(1, 0, 1, 1, 0)
pred &lt;- c(0.9, 0.4, 0.8, 0.7, 0.3)
cut &lt;- 0.5
measure_bin(obs, pred, cut)
# Returns: list(conf_mat = &lt;confusion matrix&gt;, accuracy = 1, ROC = &lt;ROC&gt;, AUC = 1)

</code></pre>

<hr>
<h2 id='measure_cat'>Measure Performance for Multi-Class Classification Models</h2><span id='topic+measure_cat'></span>

<h3>Description</h3>

<p>Evaluates the performance of a multi-class classification model using log loss and multiclass AUC.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>measure_cat(obs, pred)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="measure_cat_+3A_obs">obs</code></td>
<td>
<p>A factor vector of observed class labels. Each level represents a unique class.</p>
</td></tr>
<tr><td><code id="measure_cat_+3A_pred">pred</code></td>
<td>
<p>A numeric matrix of predicted probabilities, where each row corresponds to an observation,
and each column corresponds to a class. The number of columns must match the number of levels in <code>obs</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The log loss is calculated as:
</p>
<p style="text-align: center;"><code class="reqn">-\frac{1}{N} \sum_{i=1}^N \sum_{c=1}^C y_{ic} \log(p_{ic})</code>
</p>

<p>where <code class="reqn">y_{ic}</code> is 1 if observation <code class="reqn">i</code> belongs to class <code class="reqn">c</code>, and <code class="reqn">p_{ic}</code> is the
predicted probability for that class.
</p>
<p>The AUC is computed using the <code>pROC::multiclass.roc</code> function, which provides an overall measure
of model performance for multiclass classification.
</p>


<h3>Value</h3>

<p>A list containing:
</p>

<dl>
<dt><code>log_loss</code></dt><dd><p>The negative log-likelihood averaged across observations.</p>
</dd>
<dt><code>ROC</code></dt><dd><p>ROC generated using <code>pROC::roc</code></p>
</dd>
<dt><code>AUC</code></dt><dd><p>The multiclass Area Under the Curve (AUC) as computed by <code>pROC::multiclass.roc</code>.</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>library(pROC)
obs &lt;- factor(c("A", "B", "C"), levels = LETTERS[1:3])
pred &lt;- matrix(
  c(
    0.8, 0.1, 0.1,
    0.2, 0.6, 0.2,
    0.7, 0.2, 0.1
  ),
  nrow = 3, byrow = TRUE
)
measure_cat(obs, pred)
# Returns: list(log_loss = 1.012185, ROC = &lt;ROC&gt;, AUC = 0.75)

</code></pre>

<hr>
<h2 id='measure_cont'>Measure Performance for Continuous Response Models</h2><span id='topic+measure_cont'></span>

<h3>Description</h3>

<p>Evaluates the performance of a continuous response model using RMSE and MAE.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>measure_cont(obs, pred)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="measure_cont_+3A_obs">obs</code></td>
<td>
<p>A numeric vector of observed (true) values.</p>
</td></tr>
<tr><td><code id="measure_cont_+3A_pred">pred</code></td>
<td>
<p>A numeric vector of predicted values.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing:
</p>

<dl>
<dt><code>rmse</code></dt><dd><p>Root Mean Squared Error.</p>
</dd>
<dt><code>mae</code></dt><dd><p>Mean Absolute Error.</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>obs &lt;- c(3.2, 4.1, 5.6)
pred &lt;- c(3.0, 4.3, 5.5)
measure_cont(obs, pred)
# Returns: list(rmse = 0.1732051, mae = 0.1666667)

</code></pre>

<hr>
<h2 id='predict.bnns'>Predict Method for <code>"bnns"</code> Objects</h2><span id='topic+predict.bnns'></span>

<h3>Description</h3>

<p>Generates predictions from a fitted Bayesian Neural Network (BNN) model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'bnns'
predict(object, newdata = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.bnns_+3A_object">object</code></td>
<td>
<p>An object of class <code>"bnns"</code>, typically the result of a call to <code><a href="#topic+bnns.default">bnns.default</a></code>.</p>
</td></tr>
<tr><td><code id="predict.bnns_+3A_newdata">newdata</code></td>
<td>
<p>A matrix or data frame of new input data for which predictions are required. If <code>NULL</code>, predictions are made on the training data used to fit the model.</p>
</td></tr>
<tr><td><code id="predict.bnns_+3A_...">...</code></td>
<td>
<p>Additional arguments (currently not used).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function uses the posterior distribution from the Stan model in the <code>bnns</code> object to compute predictions for the provided input data.
</p>


<h3>Value</h3>

<p>A matrix/array of predicted values(regression)/probabilities(classification) where first dimension corresponds to the rows of <code>newdata</code> or the training data if <code>newdata</code> is <code>NULL</code>. Second dimension corresponds to the number of posterior samples. In case of <code>out_act_fn = 3</code>, the third dimension corresponds to the class.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+bnns">bnns</a></code>, <code><a href="#topic+print.bnns">print.bnns</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Example usage:
data &lt;- data.frame(x1 = runif(10), x2 = runif(10), y = rnorm(10))
model &lt;- bnns(y ~ -1 + x1 + x2,
  data = data, L = 1, nodes = 2, act_fn = 2,
  iter = 1e1, warmup = 5, chains = 1
)
new_data &lt;- data.frame(x1 = runif(5), x2 = runif(5))
predictions &lt;- predict(model, newdata = new_data)
print(predictions)

</code></pre>

<hr>
<h2 id='print.bnns'>Print Method for <code>"bnns"</code> Objects</h2><span id='topic+print.bnns'></span>

<h3>Description</h3>

<p>Displays a summary of a fitted Bayesian Neural Network (BNN) model, including the function call and the Stan fit details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'bnns'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.bnns_+3A_x">x</code></td>
<td>
<p>An object of class <code>"bnns"</code>, typically the result of a call to <code><a href="#topic+bnns.default">bnns.default</a></code>.</p>
</td></tr>
<tr><td><code id="print.bnns_+3A_...">...</code></td>
<td>
<p>Additional arguments (currently not used).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function is called for its side effects and does not return a value. It prints the following:
</p>

<ul>
<li><p> The function call used to generate the <code>"bnns"</code> object.
</p>
</li>
<li><p> A summary of the Stan fit object stored in <code>x$fit</code>.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+bnns">bnns</a></code>, <code><a href="#topic+summary.bnns">summary.bnns</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Example usage:
data &lt;- data.frame(x1 = runif(10), x2 = runif(10), y = rnorm(10))
model &lt;- bnns(y ~ -1 + x1 + x2,
  data = data, L = 1, nodes = 2, act_fn = 2,
  iter = 1e1, warmup = 5, chains = 1
)
print(model)

</code></pre>

<hr>
<h2 id='relu'>relu transformation</h2><span id='topic+relu'></span>

<h3>Description</h3>

<p>relu transformation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>relu(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="relu_+3A_x">x</code></td>
<td>
<p>A numeric vector or matrix on which relu transformation is going to be applied.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numeric vector or matrix after relu transformation.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>relu(matrix(1:4, , nrow = 2))
</code></pre>

<hr>
<h2 id='sigmoid'>sigmoid transformation</h2><span id='topic+sigmoid'></span>

<h3>Description</h3>

<p>sigmoid transformation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sigmoid(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sigmoid_+3A_x">x</code></td>
<td>
<p>A numeric vector or matrix on which sigmoid transformation is going to be applied.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numeric vector or matrix after sigmoid transformation.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>sigmoid(matrix(1:4, nrow = 2))
</code></pre>

<hr>
<h2 id='softmax_3d'>Apply Softmax Function to a 3D Array</h2><span id='topic+softmax_3d'></span>

<h3>Description</h3>

<p>This function applies the softmax transformation along the third dimension
of a 3D array. The softmax function converts raw scores into probabilities
such that they sum to 1 for each slice along the third dimension.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>softmax_3d(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="softmax_3d_+3A_x">x</code></td>
<td>
<p>A 3D array. The input array on which the softmax function will be applied.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The softmax transformation is computed as:
</p>
<p style="text-align: center;"><code class="reqn">\text{softmax}(x_{ijk}) = \frac{\exp(x_{ijk})}{\sum_{l} \exp(x_{ijl})}</code>
</p>

<p>This is applied for each pair of indices <code style="white-space: pre;">&#8288;(i, j)&#8288;</code> across the third dimension <code>(k)</code>.
</p>
<p>The function processes the input array slice-by-slice for the first two dimensions
<code style="white-space: pre;">&#8288;(i, j)&#8288;</code>, normalizing the values along the third dimension <code>(k)</code> for each slice.
</p>


<h3>Value</h3>

<p>A 3D array of the same dimensions as <code>x</code>, where the values along the
third dimension are transformed using the softmax function.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Example: Apply softmax to a 3D array
x &lt;- array(runif(24), dim = c(2, 3, 4)) # Random 3D array (2x3x4)
softmax_result &lt;- softmax_3d(x)

</code></pre>

<hr>
<h2 id='softplus'>softplus transformation</h2><span id='topic+softplus'></span>

<h3>Description</h3>

<p>softplus transformation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>softplus(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="softplus_+3A_x">x</code></td>
<td>
<p>A numeric vector or matrix on which softplus transformation is going to be applied.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numeric vector or matrix after softplus transformation.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>softplus(matrix(1:4, nrow = 2))
</code></pre>

<hr>
<h2 id='summary.bnns'>Summary of a Bayesian Neural Network (BNN) Model</h2><span id='topic+summary.bnns'></span>

<h3>Description</h3>

<p>Provides a comprehensive summary of a fitted Bayesian Neural Network (BNN) model, including details about the model call, data, network architecture, posterior distributions, and model fitting information.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'bnns'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="summary.bnns_+3A_object">object</code></td>
<td>
<p>An object of class <code>bnns</code>, representing a fitted Bayesian Neural Network model.</p>
</td></tr>
<tr><td><code id="summary.bnns_+3A_...">...</code></td>
<td>
<p>Additional arguments (currently unused).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function prints the following information:
</p>

<ul>
<li> <p><strong>Call:</strong> The original function call used to fit the model.
</p>
</li>
<li> <p><strong>Data Summary:</strong> Number of observations and features in the training data.
</p>
</li>
<li> <p><strong>Network Architecture:</strong> Structure of the BNN including the number of hidden layers, nodes per layer, and activation functions.
</p>
</li>
<li> <p><strong>Posterior Summary:</strong> Summarized posterior distributions of key parameters (e.g., weights, biases, and noise parameter).
</p>
</li>
<li> <p><strong>Model Fit Information:</strong> Bayesian sampling details, including the number of iterations, warmup period, thinning, and chains.
</p>
</li>
<li> <p><strong>Notes:</strong> Remarks and warnings, such as checks for convergence diagnostics.
</p>
</li></ul>



<h3>Value</h3>

<p>A list (returned invisibly) containing the following elements:
</p>

<ul>
<li> <p><code>"Number of observations"</code>: The number of observations in the training data.
</p>
</li>
<li> <p><code>"Number of features"</code>: The number of features in the training data.
</p>
</li>
<li> <p><code>"Number of hidden layers"</code>: The number of hidden layers in the neural network.
</p>
</li>
<li> <p><code>"Nodes per layer"</code>: A comma-separated string representing the number of nodes in each hidden layer.
</p>
</li>
<li> <p><code>"Activation functions"</code>: A comma-separated string representing the activation functions used in each hidden layer.
</p>
</li>
<li> <p><code>"Output activation function"</code>: The activation function used in the output layer.
</p>
</li>
<li> <p><code>"Stanfit Summary"</code>: A summary of the Stan model, including key parameter posterior distributions.
</p>
</li>
<li> <p><code>"Iterations"</code>: The total number of iterations used for sampling in the Bayesian model.
</p>
</li>
<li> <p><code>"Warmup"</code>: The number of iterations used as warmup in the Bayesian model.
</p>
</li>
<li> <p><code>"Thinning"</code>: The thinning interval used in the Bayesian model.
</p>
</li>
<li> <p><code>"Chains"</code>: The number of Markov chains used in the Bayesian model.
</p>
</li>
<li> <p><code>"Performance"</code>: Predictive performance metrics, which vary based on the output activation function.
</p>
</li></ul>

<p>The function also prints the summary to the console.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+bnns">bnns</a></code>, <code><a href="#topic+print.bnns">print.bnns</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Fit a Bayesian Neural Network
data &lt;- data.frame(x1 = runif(10), x2 = runif(10), y = rnorm(10))
model &lt;- bnns(y ~ -1 + x1 + x2,
  data = data, L = 1, nodes = 2, act_fn = 2,
  iter = 1e1, warmup = 5, chains = 1
)

# Get a summary of the model
summary(model)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
