<!DOCTYPE html><html lang="en"><head><title>Help for package TULIP</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {TULIP}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#adjten'><p>Adjust tensor for covariates.</p></a></li>
<li><a href='#adjvec'><p>Adjust vector for covariates.</p></a></li>
<li><a href='#catch'>
<p>Fit a CATCH model and predict categorical response.</p></a></li>
<li><a href='#catch_matrix'>
<p>Fit a CATCH model for matrix and predict categorical response.</p></a></li>
<li><a href='#csa'>
<p>Colorimetric sensor array (CSA) data</p></a></li>
<li><a href='#cv.catch'>
<p>Cross-validation for CATCH</p></a></li>
<li><a href='#cv.dsda'><p>Cross validation for direct sparse discriminant analysis</p></a></li>
<li><a href='#cv.msda'>
<p>Cross-validation for DSDA/MSDA through function <code>msda</code></p></a></li>
<li><a href='#cv.SeSDA'><p>Cross validation for semiparametric sparse discriminant analysis</p></a></li>
<li><a href='#dsda'><p>Solution path for direct sparse discriminant analysis</p></a></li>
<li><a href='#dsda.all'><p>Direct sparse discriminant analysis</p></a></li>
<li><a href='#GDS1615'><p>GDS1615 data introduced in Burczynski et al. (2012).</p></a></li>
<li><a href='#getnorm'><p>Direct sparse discriminant analysis</p></a></li>
<li><a href='#msda'>
<p>Fits a regularization path of Sparse Discriminant Analysis and predicts</p></a></li>
<li><a href='#predict.catch'>
<p>Predict categorical responses for matrix/tensor data.</p></a></li>
<li><a href='#predict.dsda'><p>Prediction for direct sparse discriminant analysis</p></a></li>
<li><a href='#predict.msda'>
<p>Predict categorical responses for vector data.</p></a></li>
<li><a href='#predict.SeSDA'><p>Prediction for semiparametric sparse discriminant analysis</p></a></li>
<li><a href='#ROAD'><p>Solution path for regularized optimal affine discriminant</p></a></li>
<li><a href='#SeSDA'><p>Solution path for semiparametric sparse discriminant analysis</p></a></li>
<li><a href='#sim.bi.vector'><p>Simulate data</p></a></li>
<li><a href='#sim.tensor.cov'><p>Simulate data</p></a></li>
<li><a href='#SOS'><p>Solution path for sparse discriminant analysis</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>A Toolbox for Linear Discriminant Analysis with Penalties</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.2</td>
</tr>
<tr>
<td>Description:</td>
<td>Integrates several popular high-dimensional methods based on Linear Discriminant Analysis (LDA) and provides a comprehensive and user-friendly toolbox for linear, semi-parametric and tensor-variate classification as mentioned in Yuqing Pan, Qing Mai and Xin Zhang (2019) &lt;<a href="https://doi.org/10.48550/arXiv.1904.03469">doi:10.48550/arXiv.1904.03469</a>&gt;. Functions are included for covariate adjustment, model fitting, cross validation and prediction.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.1.1)</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>Imports:</td>
<td>tensr, Matrix, MASS, glmnet, methods</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2021-01-04 11:54:22 UTC; ypan</td>
</tr>
<tr>
<td>Author:</td>
<td>Yuqing Pan &lt;yuqing.pan@stat.fsu.edu&gt;,
	Qing Mai &lt;mai@stat.fsu.edu&gt;,
	Xin Zhang &lt;henry@stat.fsu.edu&gt;</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Yuqing Pan &lt;yuqing.pan@stat.fsu.edu&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2021-01-04 17:10:06 UTC</td>
</tr>
</table>
<hr>
<h2 id='adjten'>Adjust tensor for covariates.
</h2><span id='topic+adjten'></span>

<h3>Description</h3>

<p>Adjusts tensor with respect to covariates to achieve a more accurate performance. Tensor depends on the covariates through a linear regression model. The function returns the coefficients of covariates in regression and adjusted tensor list for further classifier modeling. It estimates coefficients based on training data, and then adjusts training tensor. When testing data is provided, the function will automatically adjust testing data by learned coefficients as well.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>adjten(x, z, y, testx = NULL, testz = NULL, is.centered = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="adjten_+3A_x">x</code></td>
<td>
<p>Input tensor or matrix list of length <code class="reqn">N</code>, where <code class="reqn">N</code> is the number of observations. Each element of the list is a tensor or matrix. The order of tensor can be any integer not less than 2.</p>
</td></tr>
<tr><td><code id="adjten_+3A_z">z</code></td>
<td>
<p>Input covariate matrix of dimension <code class="reqn">N \times q</code>, where <code class="reqn">q&lt;N</code>. Each row of <code>z</code> is an observation.</p>
</td></tr>
<tr><td><code id="adjten_+3A_y">y</code></td>
<td>
<p>Class label vector of dimention <code class="reqn">N\times 1</code>. For <code>K</code> class problems, <code>y</code> takes values in <code class="reqn">\{1,\cdots,\code{K}\}</code>.</p>
</td></tr>
<tr><td><code id="adjten_+3A_testx">testx</code></td>
<td>
<p>Input testing tensor or matrix list. Each element of the list is a test case. When <code>testx</code> is not provided, the function will only adjust training data.</p>
</td></tr>
<tr><td><code id="adjten_+3A_testz">testz</code></td>
<td>
<p>Input testing covariate matrix with each row being an observation.</p>
</td></tr>
<tr><td><code id="adjten_+3A_is.centered">is.centered</code></td>
<td>
<p>Indicates whether the input tensor and covariates have already been centered by their within class mean or not. If <code>is.centered</code> is FALSE, the function <code>adjten</code> will center data by class. If <code>is.centered</code> is TRUE, the function will skip the centering step.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The model CATCH assumes the linear relationship bewteen covariates and tensor as
</p>
<p style="text-align: center;"><code class="reqn">\mathbf{X}=\boldsymbol{\mu}_k+\boldsymbol{\alpha}\overline{\times}_{M+1}\mathbf{Z}+\mathbf{E},</code>
</p>
<p> where <code class="reqn">\boldsymbol{\alpha}</code> is the matrix of estimated coefficient of covariates.
The function removes the effects of covariates on response variable through tensor and obtain <code class="reqn">\mathbf{X}-\boldsymbol{\alpha}\overline{\times}_{M+1}\mathbf{Z}</code> as adjusted tensor to fit tensor discriminant analysis model.
</p>
<p>In estimating <code class="reqn">\boldsymbol{\alpha}</code>, which is the <code>alpha</code> in the package, <code><a href="#topic+adjten">adjten</a></code> first centers both tensor and covariates within their individual classes, then performs tensor response regression which regresses <code class="reqn">{\mathbf{X}}</code> on <code class="reqn">{\mathbf{Z}}</code>. 
</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>gamma</code></td>
<td>
<p>The estimated coefficients of covariates to plug in classifier. <code>gamma</code> is the <code class="reqn">\boldsymbol{\gamma}_k</code> defined function <code><a href="#topic+catch">catch</a></code> of dimension <code class="reqn">q\times (K-1)</code>, where <code>q</code> is the size of covariates and <code>K</code> is the number of classes.</p>
</td></tr>
<tr><td><code>xres</code></td>
<td>
<p>Adjusted training tensor list <code class="reqn">\mathbf{X}-\boldsymbol{\alpha}\overline{\times}_{M+1}\mathbf{Z}</code> after adjusting for covariates. The effect of the covariate is removed.</p>
</td></tr>
<tr><td><code>testxres</code></td>
<td>
<p>Adjusted testing tensor list <code class="reqn">\mathbf{X}-\boldsymbol{\alpha}\overline{\times}_{M+1}\mathbf{Z}</code> after adjusting for covariates. The effect of the covariate is removed.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Yuqing Pan, Qing Mai, Xin Zhang
</p>


<h3>References</h3>

<p>Pan, Y., Mai, Q., and Zhang, X. (2018), &quot;Covariate-Adjusted Tensor Classification in High-Dimensions.&quot; Journal of the American Statistical Association, <em>accepted</em>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+catch">catch</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 20
p &lt;- 4
k &lt;- 2
nvars &lt;- p*p*p
x &lt;- array(list(),n)
vec_x &lt;- matrix(rnorm(n*nvars),nrow=n,ncol=nvars)
vec_x[1:10,] &lt;- vec_x[1:10,]+2
z &lt;- matrix(rnorm(n*2),nrow=n,ncol=2)
z[1:10,] &lt;- z[1:10,]+0.5
y &lt;- c(rep(1,10),rep(2,10))
for (i in 1:n){
  x[[i]] &lt;- array(vec_x[i,],dim=c(p,p,p))
}
obj &lt;- adjten(x, z, y)
</code></pre>

<hr>
<h2 id='adjvec'>Adjust vector for covariates.
</h2><span id='topic+adjvec'></span>

<h3>Description</h3>

<p>Adjusts vector with respect to covariates. Vector depends on the covariates through a linear regression model. The function returns the coefficients of covariates in regression and adjusted predictor matrix for further classifier modeling. It estimates coefficients based on training data, and then adjusts training tensor. When testing data is provided, the function will automatically adjust testing data by learned coefficients as well.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>adjvec(x, z, y, testx = NULL, testz = NULL, is.centered = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="adjvec_+3A_x">x</code></td>
<td>
<p>Input matrix of dimension <code class="reqn">N\times p</code>, where <code class="reqn">N</code> is the number of observations and <code class="reqn">p</code> is the number of variables. Each row is an observation</p>
</td></tr>
<tr><td><code id="adjvec_+3A_z">z</code></td>
<td>
<p>Input covariate matrix of dimension <code class="reqn">N \times q</code>, where <code class="reqn">q&lt;N</code>. Each row of <code>z</code> is an observation.</p>
</td></tr>
<tr><td><code id="adjvec_+3A_y">y</code></td>
<td>
<p>Class label vector of dimention <code class="reqn">N\times 1</code>. For <code>K</code> class problems, <code>y</code> takes values in <code class="reqn">\{1,\cdots,\code{K}\}</code>.</p>
</td></tr>
<tr><td><code id="adjvec_+3A_testx">testx</code></td>
<td>
<p>Input testing matrix. Each row is a test case. When <code>testx</code> is not provided, the function will only adjust training data.</p>
</td></tr>
<tr><td><code id="adjvec_+3A_testz">testz</code></td>
<td>
<p>Input testing covariate matrix with each row being an observation.</p>
</td></tr>
<tr><td><code id="adjvec_+3A_is.centered">is.centered</code></td>
<td>
<p>Indicates whether the input vector and covariates have already been centered by their within class mean or not. If <code>is.centered</code> is FALSE, the function <code>adjvec</code> will center data by class. If <code>is.centered</code> is TRUE, the function will skip the centering step.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Similar as CATCH model, assume the linear relationship between vector predictors and covariates as
</p>
<p style="text-align: center;"><code class="reqn">\mathbf{X}=\boldsymbol{\mu}_k+\boldsymbol{\alpha}\times\mathbf{Z}+\mathbf{E},</code>
</p>
<p> where <code class="reqn">\mathbf{X}</code> is a <code class="reqn">N\times p</code> matrix and <code class="reqn">\boldsymbol{\alpha}</code> is the matrix of estimated coefficient of covariates.
The function removes the effects of covariates on response variable through vector and obtain <code class="reqn">\mathbf{X}-\boldsymbol{\alpha}\times\mathbf{Z}</code> as adjusted predictors to fit MSDA and DSDA model.
</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>gamma</code></td>
<td>
<p>The estimated coefficients of covariates to plug in classifier. <code>gamma</code> is similar as the <code class="reqn">\boldsymbol{\gamma}_k</code> defined function <code><a href="#topic+catch">catch</a></code> of dimension <code class="reqn">q\times (K-1)</code>, where <code>q</code> is the size of covariates and <code>K</code> is the number of classes.</p>
</td></tr>
<tr><td><code>xres</code></td>
<td>
<p>Adjusted training predictor matrix <code class="reqn">\mathbf{X}-\boldsymbol{\alpha}\times\mathbf{Z}</code> after adjusting for covariates. The effect of the covariate is removed.</p>
</td></tr>
<tr><td><code>testxres</code></td>
<td>
<p>Adjusted testing predictor matrix <code class="reqn">\mathbf{X}-\boldsymbol{\alpha}\times\mathbf{Z}</code> after adjusting for covariates. The effect of the covariate is removed.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Yuqing Pan, Qing Mai, Xin Zhang
</p>


<h3>References</h3>

<p>Pan, Y., Mai, Q., and Zhang, X. (2018), &quot;Covariate-Adjusted Tensor Classification in High-Dimensions.&quot; Journal of the American Statistical Association, <em>accepted</em>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+adjten">adjten</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 50
p &lt;- 200
k &lt;- 2
q &lt;- 2
x &lt;- matrix(rnorm(n*p), n, p)
z &lt;- matrix(rnorm(n*q), n, q)
x[1:20, ] &lt;- x[1:20, ] + 2
z[1:20, ] &lt;- z[1:20, ] + 0.5
y &lt;- c(rep(1, 20), rep(2, 30))
obj &lt;- adjvec(x, z, y)
</code></pre>

<hr>
<h2 id='catch'>
Fit a CATCH model and predict categorical response.
</h2><span id='topic+catch'></span>

<h3>Description</h3>

<p>The <code>catch</code> function solves classification problems and selects variables by fitting a covariate-adjusted tensor classification in high-dimensions (CATCH) model. The input training predictors include two parts: tensor data and low dimensional covariates. The tensor data could be matrix as a special case of tensor. In <code>catch</code>, tensor data should be stored in a list form. If the dataset contains no covariate, <code>catch</code> can also fit a classifier only based on the tensor predictors. If covariates are provided, the method will adjust tensor for covariates and then fit a classifier based on the adjusted tensor along with the covariates. If users specify testing data at the same time, predicted response will be obtained as well. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>catch(x, z = NULL, y, testx = NULL, testz = NULL, nlambda = 100, 
lambda.factor = ifelse((nobs - nclass) &lt;= nvars, 0.2, 1E-03), 
lambda = NULL,dfmax = nobs, pmax = min(dfmax * 2 + 20, nvars), 
pf = rep(1, nvars), eps = 1e-04, maxit = 1e+05, sml = 1e-06, 
verbose = FALSE, perturb = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="catch_+3A_x">x</code></td>
<td>
<p>Input tensor (or matrix) list of length <code class="reqn">N</code>, where <code class="reqn">N</code> is the number of observations. Each element of the list is a tensor or matrix. The order of tensor can be any positive integer not less than 2. </p>
</td></tr>
<tr><td><code id="catch_+3A_z">z</code></td>
<td>
<p>Input covariate matrix of dimension <code class="reqn">N \times q</code>, where <code class="reqn">q&lt;N</code>. <code>z</code> can be omitted if covariate is absent. </p>
</td></tr>
<tr><td><code id="catch_+3A_y">y</code></td>
<td>
<p>Class label. For <code>K</code> class problems, <code>y</code> takes values in <code class="reqn">\{1,\cdots,\code{K}\}</code>.</p>
</td></tr>
<tr><td><code id="catch_+3A_testx">testx</code></td>
<td>
<p>Input testing tensor or matrix list. Each element of the list is a test case. When <code>testx</code> is not provided, the function will only fit the model and return the classifier. When <code>testx</code> is provided, the function will predict response on <code>testx</code> as well.</p>
</td></tr>
<tr><td><code id="catch_+3A_testz">testz</code></td>
<td>
<p>Input testing covariate matrix. Can be omitted if covariate is absent. However, training covariates <code>z</code> and testing covariates <code>testz</code> must be provided or not at the same time.</p>
</td></tr>
<tr><td><code id="catch_+3A_nlambda">nlambda</code></td>
<td>
<p>The number of tuning values in sequence <code>lambda</code>. If users do not specify <code>lambda</code> values, the package will generate a solution path containing <code>nlambda</code> many tuning values of <code>lambda</code>. Default is <code>100</code>. </p>
</td></tr>
<tr><td><code id="catch_+3A_lambda.factor">lambda.factor</code></td>
<td>
<p>When <code>lambda</code> is not supplied, <code>catch</code> first finds the largest value in <code>lambda</code> which yields <code class="reqn">\boldsymbol{\beta}=0</code>. Then the minimum value in <code>lambda</code> is obtained by <code>(largest value*lambda.factor)</code>. The sequence of <code>lambda</code> is generated by evenly sampling <code>nlambda</code> numbers within the range. Default value of <code>lambda.factor</code> is 0.2 if <code class="reqn">N&lt;p</code> and 0.0001 if <code class="reqn">N&gt;p</code>.</p>
</td></tr>
<tr><td><code id="catch_+3A_lambda">lambda</code></td>
<td>
<p>A sequence of user-specified <code>lambda</code> values. <code>lambda</code> is the weight of L1 penalty and a smaller <code>lambda</code> allows more variables to be nonzero. If <code>NULL</code>, then the algorithm will generate a sequence of <code>nlambda</code> many potential 
<code>lambda</code>s according to <code>lambda.factor</code>.</p>
</td></tr>
<tr><td><code id="catch_+3A_dfmax">dfmax</code></td>
<td>
<p>The maximum number of selected variables in the model. Default is the number of observations <code>N</code>.</p>
</td></tr>
<tr><td><code id="catch_+3A_pmax">pmax</code></td>
<td>
<p>The maximum number of potential selected variables during iteration. In middle step, the algorithm can select at most <code>pmax</code> variables and then shrink part of them such that the nubmer of final selected variables is less than <code>dfmax</code>. Default is <code class="reqn">\min(dfmax\times 2+20, N)</code>.
</p>
</td></tr>
<tr><td><code id="catch_+3A_pf">pf</code></td>
<td>
<p>Weight of lasso penalty. Default is a vector of value <code>1</code> and length <code>p</code>, representing L1 penalty of length <code class="reqn">p</code>. Can be mofidied to use adaptive lasso penalty.</p>
</td></tr>
<tr><td><code id="catch_+3A_eps">eps</code></td>
<td>
<p>Convergence threshold for coordinate descent difference between iterations. Default value is <code>1e-04</code>.</p>
</td></tr>
<tr><td><code id="catch_+3A_maxit">maxit</code></td>
<td>
<p>Maximum iteration times for all lambda. Default value is <code>1e+05</code>.</p>
</td></tr>
<tr><td><code id="catch_+3A_sml">sml</code></td>
<td>
<p>Threshold for ratio of loss function change after each iteration to old loss function value. Default value is <code>1e-06</code>.</p>
</td></tr>
<tr><td><code id="catch_+3A_verbose">verbose</code></td>
<td>
<p>Indicates whether print out lambda during iteration or not. Default value is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="catch_+3A_perturb">perturb</code></td>
<td>
<p>Perturbation scaler. If it is specified, the value will be added to diagonal of estimated covariance matrix. A small value can be used to accelerate iteration. Default value is <code>NULL</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code><a href="#topic+catch">catch</a></code> function fits a linear discriminant analysis model as follows:
</p>
<p style="text-align: center;"><code class="reqn">\mathbf{Z}|(Y=k)\sim N(\boldsymbol{\phi_k},\boldsymbol{\psi}),</code>
</p>

<p style="text-align: center;"><code class="reqn">\mathbf{X}|(\mathbf{Z}=\mathbf{z}, Y=k)\sim TN(\boldsymbol{\mu}_k+\boldsymbol{\alpha}\bar{\times}_{M+1}\mathbf{z},\boldsymbol{\Sigma}_1,\cdots,\boldsymbol{\Sigma}_M).</code>
</p>

<p>The categorical response is predicted from the estimated Bayes rule:
</p>
<p style="text-align: center;"><code class="reqn">\widehat{Y}=\arg\max_{k=1,\cdots,K}{a_k+\boldsymbol{\gamma}_k^T\mathbf{Z}+&lt;\boldsymbol{\beta}_k,\mathbf{X}-\boldsymbol{\alpha}\overline{\times}_{M+1}\mathbf{Z}&gt;},</code>
</p>
 
<p>where <code class="reqn">\mathbf{X}</code> is the tensor, <code class="reqn">\mathbf{Z}</code> is the covariates, <code class="reqn">a_k</code>, <code class="reqn">\boldsymbol{\gamma}_k</code> and <code class="reqn">\boldsymbol{\alpha}</code> are parameters estimated by CATCH. A detailed explanation can be found in reference. When <code>Z</code> is not <code>NULL</code>, the function will first adjust tensor on covariates by modeling
</p>
<p style="text-align: center;"><code class="reqn">\mathbf{X}=\boldsymbol{\mu}_k+\boldsymbol{\alpha}\overline{\times}_{M+1}\mathbf{Z}+\mathbf{E},</code>
</p>
<p> where <code class="reqn">\mathbf{E}</code> is an unobservable tensor normal error independent of <code class="reqn">\mathbf{Z}</code>. 
Then <code><a href="#topic+catch">catch</a></code> fits model on the adjusted training tensor <code class="reqn">\mathbf{X}-\boldsymbol{\alpha}\overline{\times}_{M+1}\mathbf{Z}</code> and makes predictions on testing data by using the adjusted tensor list. If <code>Z</code> is <code>NULL</code>, it reduces to a simple tensor discriminant analysis model. 
</p>
<p>The coefficient of tensor <code class="reqn">\boldsymbol{\beta}</code>, represented by <code>beta</code> in package, is estimated by 
</p>
<p style="text-align: center;"><code class="reqn">\min_{\boldsymbol{\beta}_2,\ldots,\boldsymbol{\beta}_K}\left[\sum_{k=2}^K\left(\langle\boldsymbol{\beta}_k,[\![\boldsymbol{\beta}_k;\widehat{\boldsymbol{\Sigma}}_{1},\dots,\widehat{\boldsymbol{\Sigma}}_{M}]\!]\rangle-2\langle\boldsymbol{\beta}_k,\widehat{\boldsymbol{\mu}}_{k}-\widehat{\boldsymbol{\mu}}_{1}\rangle\right)+\lambda\sum_{j_{1}\dots j_{M}}\sqrt{\sum_{k=2}^{K}\beta_{k,j_{1}\cdots j_{M}}^2}\right].</code>
</p>

<p>When response is multi-class, the group lasso penalty over categories is added to objective function through parameter <code>lambda</code>, and it reduces to a lasso penalty in binary problems.
</p>
<p>The function <code><a href="#topic+catch">catch</a></code> will predict categorical response when testing data is provided. 
If testing data is not provided or if one wishes to perform prediction separately, <code><a href="#topic+catch">catch</a></code> can be used to only fit model with a catch object outcome. The object outcome can be combined with the adjusted tensor list from <code><a href="#topic+adjten">adjten</a></code> to perform prediction by <code><a href="#topic+predict.catch">predict.catch</a></code>.
</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>beta</code></td>
<td>
<p>Output variable coefficients for each <code>lambda</code>, which is the estimation of <code class="reqn">\boldsymbol{\beta}</code> in the Bayes rule. <code>beta</code> is a list of length being the number of <code>lambda</code>s. Each element of <code>beta</code> is a matrix of dimension <code class="reqn">nvars\times (nclass-1)</code>.</p>
</td></tr>
<tr><td><code>df</code></td>
<td>
<p>The number of nonzero variables for each value in sequence <code>lambda</code>.</p>
</td></tr>
<tr><td><code>dim</code></td>
<td>
<p>Dimension of coefficient array.</p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>
<p>The actual <code>lambda</code> sequence used. The user specified sequence or automatically generated sequence could be truncated by constraints on <code>dfmax</code> and <code>pmax</code>.</p>
</td></tr>
<tr><td><code>obj</code></td>
<td>
<p>Objective function value for each value in sequence <code>lambda</code>.</p>
</td></tr>
<tr><td><code>x</code></td>
<td>
<p>The tensor list after adjustment in training data. If covariate is absent, this is the original input tensor list.</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>Class label in training data.</p>
</td></tr>
<tr><td><code>npasses</code></td>
<td>
<p>Total number of iterations.</p>
</td></tr>
<tr><td><code>jerr</code></td>
<td>
<p>Error flag.</p>
</td></tr>
<tr><td><code>sigma</code></td>
<td>
<p>Estimated covariance matrix on each mode. <code>sigma</code> is a list with the <code>i</code>th element being covariance matrix on <code>i</code>th mode.</p>
</td></tr>
<tr><td><code>delta</code></td>
<td>
<p>Estimated delta matrix <code class="reqn">(vec(\widehat{\boldsymbol{\mu}}_2-\widehat{\boldsymbol{\mu}}_1),\cdots,vec(\widehat{\boldsymbol{\mu}}_K-\widehat{\boldsymbol{\mu}}_1))</code>.</p>
</td></tr>
<tr><td><code>mu</code></td>
<td>
<p>Estimated mean array of <code class="reqn">\mathbf{X}</code>.</p>
</td></tr>
<tr><td><code>prior</code></td>
<td>
<p>Proportion of samples in each class.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>The call that produces this object.</p>
</td></tr>
<tr><td><code>pred</code></td>
<td>
<p>Predicted categorical response for each value in sequence <code>lambda</code> when <code>testx</code> is provided.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Yuqing Pan, Qing Mai, Xin Zhang
</p>


<h3>References</h3>

<p>Pan, Y., Mai, Q., and Zhang, X. (2018), &quot;Covariate-Adjusted Tensor Classification in High-Dimensions.&quot; Journal of the American Statistical Association, <em>accepted</em>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cv.catch">cv.catch</a></code>, <code><a href="#topic+predict.catch">predict.catch</a></code>, <code><a href="#topic+adjten">adjten</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#without prediction
n &lt;- 20
p &lt;- 4
k &lt;- 2
nvars &lt;- p*p*p
x &lt;- array(list(),n)
vec_x &lt;- matrix(rnorm(n*nvars), nrow=n, ncol=nvars)
vec_x[1:10,] &lt;- vec_x[1:10,]+2
z &lt;- matrix(rnorm(n*2), nrow=n, ncol=2)
z[1:10,] &lt;- z[1:10,]+0.5
y &lt;- c(rep(1,10),rep(2,10))
for (i in 1:n){
  x[[i]] &lt;- array(vec_x[i,],dim=c(p,p,p))
}
obj &lt;- catch(x,z,y=y)
</code></pre>

<hr>
<h2 id='catch_matrix'>
Fit a CATCH model for matrix and predict categorical response.
</h2><span id='topic+catch_matrix'></span>

<h3>Description</h3>

<p>Fits a classifier for matrix data. <code>catch_matrix</code> is a special case of <code><a href="#topic+catch">catch</a></code> when each observation <code class="reqn">\mathbf{X}_i</code> is a matrix. Different from <code><a href="#topic+catch">catch</a></code> takes list as input, data need to be formed in an array to call the function (see arguments). The function will perform prediction as well.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>catch_matrix(x, z = NULL, y, testx = NULL, testz = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="catch_matrix_+3A_x">x</code></td>
<td>
<p>Input matrix array. The array should be organized with dimension <code class="reqn">p_1 \times p_2 \times N</code>.
</p>
</td></tr>
<tr><td><code id="catch_matrix_+3A_z">z</code></td>
<td>
<p>Input covariate matrix of dimension <code class="reqn">N \times q</code>, where <code class="reqn">q&lt;N</code>. <code>z</code> can be omitted if covariate is absent. </p>
</td></tr>
<tr><td><code id="catch_matrix_+3A_y">y</code></td>
<td>
<p>Class label. For <code>K</code> class problems, <code>y</code> takes values in <code class="reqn">\{1,\cdots,\code{K}\}</code>.</p>
</td></tr>
<tr><td><code id="catch_matrix_+3A_testx">testx</code></td>
<td>
<p>Input testing matrix array. When <code>testx</code> is not provided, the function will only fit model. When <code>testx</code> is provided, the function will predict response on <code>testx</code> as well.</p>
</td></tr>
<tr><td><code id="catch_matrix_+3A_testz">testz</code></td>
<td>
<p>Input testing covariate matrix. Can be omitted if there is no covariate.</p>
</td></tr>
<tr><td><code id="catch_matrix_+3A_...">...</code></td>
<td>
<p>Other arguments that can be passed to <code><a href="#topic+catch">catch</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function fits a matrix classifier as a special case of <code><a href="#topic+catch">catch</a></code>. The fitted model and predictions should be identical to <code><a href="#topic+catch">catch</a></code> when matrix data is provided. Input matrix should be organized as three-way array where sample size is the last dimension. If the matrix is organized in a list, users can either reorganize it or use <code><a href="#topic+catch">catch</a></code> directly to fit model, which takes a matrix or tensor list as input and has the same output as <code><a href="#topic+catch_matrix">catch_matrix</a></code>.
</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>beta</code></td>
<td>
<p>Output variable coefficients for each <code>lambda</code>. <code>beta</code> is a list of length being the number of <code>lambda</code>s. Each element of <code>beta</code> is a matrix of dimension <code class="reqn">(p_1\times p_2)\times (nclass-1)</code>.</p>
</td></tr>
<tr><td><code>df</code></td>
<td>
<p>The number of nonzero variables for each value in sequence <code>lambda</code>.</p>
</td></tr>
<tr><td><code>dim</code></td>
<td>
<p>Dimension of coefficient array.</p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>
<p>The actual <code>lambda</code> sequence used. The user specified sequence or automatically generated sequence could be truncated by constraints on <code>dfmax</code> and <code>pmax</code>.</p>
</td></tr>
<tr><td><code>obj</code></td>
<td>
<p>Objective function value for each value in sequence <code>lambda</code>.</p>
</td></tr>
<tr><td><code>x</code></td>
<td>
<p>The matrix list after adjustment in training data. If covariate is absent, this is the original input matrix.</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>Class label in training data.</p>
</td></tr>
<tr><td><code>npasses</code></td>
<td>
<p>Total number of iterations.</p>
</td></tr>
<tr><td><code>jerr</code></td>
<td>
<p>Error flag.</p>
</td></tr>
<tr><td><code>sigma</code></td>
<td>
<p>Estimated covariance matrix on each mode. <code>sigma</code> is a list with the <code>i</code>th element being covariance matrix on <code>i</code>th mode.</p>
</td></tr>
<tr><td><code>delta</code></td>
<td>
<p>Estimated delta matrix <code class="reqn">(vec(\widehat{\boldsymbol{\mu}}_2-\widehat{\boldsymbol{\mu}}_1),\cdots,vec(\widehat{\boldsymbol{\mu}}_K-\widehat{\boldsymbol{\mu}}_1))</code>.</p>
</td></tr>
<tr><td><code>mu</code></td>
<td>
<p>Estimated mean array.</p>
</td></tr>
<tr><td><code>prior</code></td>
<td>
<p>Prior proportion of observations in each class.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>The call that produces this object.</p>
</td></tr>
<tr><td><code>pred</code></td>
<td>
<p>Predicted categorical response for each value in sequence <code>lambda</code> when <code>testx</code> is provided.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Yuqing Pan, Qing Mai, Xin Zhang
</p>


<h3>References</h3>

<p>Pan, Y., Mai, Q., and Zhang, X. (2018), &quot;Covariate-Adjusted Tensor Classification in High-Dimensions.&quot; Journal of the American Statistical Association, <em>accepted</em>. 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+catch">catch</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#without prediction
n &lt;- 20
p &lt;- 4
k &lt;- 2
nvars &lt;- p*p
x=array(rnorm(n*nvars),dim=c(p,p,n))
x[,,11:20]=x[,,11:20]+0.3
z &lt;- matrix(rnorm(n*2), nrow=n, ncol=2)
z[1:10,] &lt;- z[1:10,]+0.5
y &lt;- c(rep(1,10),rep(2,10))
obj &lt;- catch_matrix(x,z,y=y)
</code></pre>

<hr>
<h2 id='csa'>
Colorimetric sensor array (CSA) data</h2><span id='topic+csa'></span>

<h3>Description</h3>

<p>A dataset collected from a series of CSA experiments to identify volatile chemical toxicants (VCT). Chemical dyes were exposed to VCT under different concentration conditions and colors of dyes were recorded to identify the class of VCT. There are two concentration conditions PEL (permissible exposure level) and IDLH (immediately dangerous to life of health). 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(csa)
</code></pre>


<h3>Format</h3>

<p>Two lists, <em>PEL</em> and <em>IDLH</em>, and a numeric vector <em>y</em>. Each list contains 147 matrices of dimension <code class="reqn">36 \times 3</code>. 
</p>

<dl>
<dt><code>PEL</code></dt><dd><p>A list of matrices containing the observations after exposure at PEL.</p>
</dd>
<dt><code>IDLH</code></dt><dd><p>A list of matrices containing the observations after exposure at IDLH level.</p>
</dd>
<dt><code>y</code></dt><dd><p>Class label ranging from 1 to 21.</p>
</dd>
</dl>



<h3>Details</h3>

<p>This dataset is provided in the Supplementary matrial of Zhong (2015). In each concentration case, there are 147 observations and 21 classes. We reorganize the data into a list to be directly called by <code>catch</code>. For matrices in the list, each row represents a dye and the three columns correspond to red, green and blue.
</p>


<h3>Source</h3>

<p>Wenxuan Zhong and Kenneth S. Suslick (2015). &quot;Matrix discriminant analysis with application to colorimetric sensor array data&quot; <em>Technometrics</em> <b>57</b>(4), 524&ndash;534.
</p>

<hr>
<h2 id='cv.catch'>
Cross-validation for CATCH
</h2><span id='topic+cv.catch'></span>

<h3>Description</h3>

<p>Performs k-fold cross validation for CATCH and returns the best tuning parameter <code class="reqn">\lambda</code> in the user-specified or automatically generated choices.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cv.catch(x, z = NULL, y, nfolds = 5, lambda = NULL, 
lambda.opt = "min",...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cv.catch_+3A_x">x</code></td>
<td>
<p>Input tensor or matrix list of length <code class="reqn">N</code>, where <code class="reqn">N</code> is the number of observations. Each element of the list is a tensor or matrix. The order of tensor can be any number and not limited to three.</p>
</td></tr>
<tr><td><code id="cv.catch_+3A_z">z</code></td>
<td>
<p>Input covariate matrix of dimension <code class="reqn">N \times q</code>, where <code class="reqn">q&lt;N</code>. <code>z</code> can be omitted if covariate is absent. </p>
</td></tr>
<tr><td><code id="cv.catch_+3A_y">y</code></td>
<td>
<p>Class label. For <code>K</code> class problems, <code>y</code> takes values in <code class="reqn">\{1,\cdots,\code{K}\}</code>.</p>
</td></tr>
<tr><td><code id="cv.catch_+3A_nfolds">nfolds</code></td>
<td>
<p>Number of folds. Default value is <code>5</code>.</p>
</td></tr>
<tr><td><code id="cv.catch_+3A_lambda">lambda</code></td>
<td>
<p>User-specified <code>lambda</code> sequence for cross validation. If not specified, the algorithm will generate a sequence of <code>lambda</code>s based on all data and cross validate on the sequence.</p>
</td></tr>
<tr><td><code id="cv.catch_+3A_lambda.opt">lambda.opt</code></td>
<td>
<p>The optimal criteria when multiple elements in <code>lambda</code> return the same minimum classification error. &quot;<code>min</code>&quot; will return the smallest <code>lambda</code> with minimum cross validation error. &quot;<code>max</code>&quot; will return the largest <code>lambda</code> with the minimum cross validation error.</p>
</td></tr>
<tr><td><code id="cv.catch_+3A_...">...</code></td>
<td>
<p>Other arguments that can be passed to <code><a href="#topic+catch">catch</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function <code><a href="#topic+cv.catch">cv.catch</a></code> runs function <code><a href="#topic+catch">catch</a></code> <code>nfolds+1</code> times. The first one fits model on all data. If <code>lambda</code> is specified, it will check if all <code>lambda</code> satisfies the constraints of <code>dfmax</code> and <code>pmax</code> in <code><a href="#topic+catch">catch</a></code>. If not, a <code>lambda</code> sequence will be generated according to <code>lambda.factor</code> in <code><a href="#topic+catch">catch</a></code>. Then the rest <code>nfolds</code> many replicates will fit model on <code>nfolds-1</code> many folds data and predict on the omitted fold, repectively. Return the <code>lambda</code> with minimum average cross validation error and the largest <code>lambda</code> within one standard error of the minimum.
</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>lambda</code></td>
<td>
<p>The actual <code>lambda</code> sequence used. The user specified sequence or automatically generated sequence could be truncated by constraints on <code>dfmax</code> and <code>pmax</code>.</p>
</td></tr>
<tr><td><code>cvm</code></td>
<td>
<p>The mean of cross validation errors for each <code>lambda</code>.</p>
</td></tr>
<tr><td><code>cvsd</code></td>
<td>
<p>The standard error of cross validaiton errors for each <code>lambda</code>.</p>
</td></tr>
<tr><td><code>lambda.min</code></td>
<td>
<p>The <code>lambda</code> with minimum cross validation error. If <code>lambda.opt</code> is <code>min</code>, then returns the smallest <code>lambda</code> with minimum cross validation error. If <code>lambda.opt</code> is <code>max</code>, then returns the largest <code>lambda</code> with minimum cross validation error.</p>
</td></tr>
<tr><td><code>lambda.1se</code></td>
<td>
<p>The largest <code>lambda</code> with cross validation error within one standard error of the minimum.</p>
</td></tr>
<tr><td><code>catch.fit</code></td>
<td>
<p>The fitted <code>catchobj</code> object.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Yuqing Pan, Qing Mai, Xin Zhang
</p>


<h3>References</h3>

<p>Pan, Y., Mai, Q., and Zhang, X. (2018), &quot;Covariate-Adjusted Tensor Classification in High-Dimensions.&quot; Journal of the American Statistical Association, <em>accepted</em>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+catch">catch</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 20
p &lt;- 4
k &lt;- 2
nvars &lt;- p*p*p
x &lt;- array(list(),n)
vec_x &lt;- matrix(rnorm(n*nvars), nrow=n, ncol=nvars)
vec_x[1:10,] &lt;- vec_x[1:10,]+2
z &lt;- matrix(rnorm(n*2),nrow=n,ncol=2)
z[1:10,] &lt;- z[1:10,]+0.5
y &lt;- c(rep(1,10),rep(2,10))
for (i in 1:n){
  x[[i]] &lt;- array(vec_x[i,], dim=c(p,p,p))
}
objcv &lt;- cv.catch(x, z, y=y)
</code></pre>

<hr>
<h2 id='cv.dsda'>Cross validation for direct sparse discriminant analysis</h2><span id='topic+cv.dsda'></span>

<h3>Description</h3>

<p>Choose the optimal lambda for direct sparse discriminant analysis by cross validation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cv.dsda(x, y, nfolds = 5, lambda=lambda, lambda.opt="min", 
 standardize=FALSE, alpha=1, eps=1e-7)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cv.dsda_+3A_x">x</code></td>
<td>
<p>An n by p matrix containing the predictors.</p>
</td></tr>
<tr><td><code id="cv.dsda_+3A_y">y</code></td>
<td>
<p>An n-dimensional vector containing the class labels.</p>
</td></tr>
<tr><td><code id="cv.dsda_+3A_nfolds">nfolds</code></td>
<td>
<p>The number of folds to be used in cross validation. Default is 5.</p>
</td></tr>
<tr><td><code id="cv.dsda_+3A_lambda">lambda</code></td>
<td>
<p>A sequence of lambda's.</p>
</td></tr>
<tr><td><code id="cv.dsda_+3A_lambda.opt">lambda.opt</code></td>
<td>
<p>Should be either &quot;min&quot; or &quot;max&quot;, specifying whether the smallest or the largest lambda with the smallest cross validation error should be used for the final classification rule.</p>
</td></tr>
<tr><td><code id="cv.dsda_+3A_standardize">standardize</code></td>
<td>
<p>A logic object indicating whether x.matrix should be standardized before performing DSDA. Default is FALSE.</p>
</td></tr>
<tr><td><code id="cv.dsda_+3A_alpha">alpha</code></td>
<td>
<p>The elasticnet mixing parameter, the same as in glmnet. Default is alpha=1 so that the lasso penalty is used.</p>
</td></tr>
<tr><td><code id="cv.dsda_+3A_eps">eps</code></td>
<td>
<p>Convergence threshold for coordinate descent, the same as in glmnet. Default is 1e-7.</p>
</td></tr>
</table>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>lambda</code></td>
<td>
<p>The sequence of lambda's used in cross validation.</p>
</td></tr>
<tr><td><code>cvm</code></td>
<td>
<p>Cross validation errors.</p>
</td></tr>
<tr><td><code>cvsd</code></td>
<td>
<p>The standard error of the cross validation errors.</p>
</td></tr>
<tr><td><code>lambda.min</code></td>
<td>
<p>The optimal lambda chosen by cross validation.</p>
</td></tr>
<tr><td><code>model.fit</code></td>
<td>
<p>The fitted model.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Mai, Q., Zou, H. and Yuan, M. (2013). A direct approach to sparse discriminant analysis in ultra-high dimensions. Biometrika, 99, 29-42.
</p>


<h3>See Also</h3>

<p><code>cv.dsda</code>
<code>predict.dsda</code>
<code>dsda</code>
</p>

<hr>
<h2 id='cv.msda'>
Cross-validation for DSDA/MSDA through function <code>msda</code>
</h2><span id='topic+cv.msda'></span>

<h3>Description</h3>

<p>Performs K-fold cross validation for <code>msda</code> and returns the best tuning parameter <code class="reqn">\lambda</code> in the user-specified or automatically generated choices.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cv.msda(x, y, model = NULL, nfolds = 5, lambda = NULL,
 lambda.opt = "min", ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cv.msda_+3A_x">x</code></td>
<td>
<p>Input matrix of predictors. <code>x</code> is of dimension <code class="reqn">N \times p</code>; each row is an observation vector.</p>
</td></tr>
<tr><td><code id="cv.msda_+3A_y">y</code></td>
<td>
<p>Class label. For <code>K</code> class problems, <code>y</code> takes values in <code class="reqn">\{1,\cdots,\code{K}\}</code>.</p>
</td></tr>
<tr><td><code id="cv.msda_+3A_model">model</code></td>
<td>
<p>Method type. The <code>model</code> argument can be one of <code>'binary'</code>, <code>'multi.original'</code>, <code>'multi.modified'</code> and the default is NULL. The function supports fitting DSDA and MSDA models by specifying method type. Without specification, the function will automatically choose one of the methods. If the response variable is binary, the function will fit a DSDA model. If the response variable is multi-class, the function will fit an original MSDA model for dimension <code class="reqn">p&lt;=2000</code> and a modified MSDA model for dimension <code class="reqn">p&gt;2000</code>.</p>
</td></tr>
<tr><td><code id="cv.msda_+3A_nfolds">nfolds</code></td>
<td>

<p>Number of folds. Default value is 5. Although <code>nfolds</code>
can be as large as the sample size (leave-one-out CV), it is not
recommended for large datasets. Smallest value allowable is <code>nfolds=3</code> for <code>multi.original</code> and <code>multi.modified</code>.</p>
</td></tr>
<tr><td><code id="cv.msda_+3A_lambda">lambda</code></td>
<td>
<p>User-specified <code>lambda</code> sequence for cross validation. If not specified, the algorithm will generate a sequence of <code>lambda</code>s based on all data and cross validate on the sequence.</p>
</td></tr>
<tr><td><code id="cv.msda_+3A_lambda.opt">lambda.opt</code></td>
<td>
<p>The optimal criteria when multiple elements in <code>lambda</code> return the same minimum classification error. &quot;<code>min</code>&quot; will return the smallest <code>lambda</code> with minimum cross validation error. &quot;<code>max</code>&quot; will return the largest <code>lambda</code> with the minimum cross validation error.
</p>
</td></tr>
<tr><td><code id="cv.msda_+3A_...">...</code></td>
<td>

<p>other arguments that can be passed to <code>msda</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function <code><a href="#topic+cv.msda">cv.msda</a></code> runs function <code><a href="#topic+msda">msda</a></code> <code>nfolds+1</code> times. The first one fits model on all data. If <code>lambda</code> is specified, it will check if all <code>lambda</code> satisfies the constraints of <code>dfmax</code> and <code>pmax</code> in <code><a href="#topic+msda">msda</a></code>. If not, a <code>lambda</code> sequence will be generated according to <code>lambda.factor</code> in <code><a href="#topic+msda">msda</a></code>. Then the rest <code>nfolds</code> many replicates will fit model on <code>nfolds-1</code> many folds data and predict on the omitted fold, repectively. Return the <code>lambda</code> with minimum average cross validation error and the largest <code>lambda</code> within one standard error of the minimum.
</p>
<p>Similar as <code><a href="#topic+msda">msda</a></code>, user can specify which method to use by inputing argument <code>model</code>. Without specification, the function can automatically decide the method by number of classes and variables.
</p>


<h3>Value</h3>

<p>An object of class <code>cv.dsda</code> or <code>cv.msda.original</code> or <code>cv.msda.modified</code> is returned, which is a
list with the ingredients of the cross-validation fit.
</p>
<table role = "presentation">
<tr><td><code>lambda</code></td>
<td>
<p>The actual <code>lambda</code> sequence used. The user specified sequence or automatically generated sequence could be truncated by constraints on <code>dfmax</code> and <code>pmax</code>.</p>
</td></tr>
<tr><td><code>cvm</code></td>
<td>
<p>The mean of cross validation errors for each <code>lambda</code>.</p>
</td></tr>
<tr><td><code>cvsd</code></td>
<td>
<p>The standard error of cross validaiton errors for each <code>lambda</code>.</p>
</td></tr>
<tr><td><code>lambda.min</code></td>
<td>
<p>The <code>lambda</code> with minimum cross validation error. If <code>lambda.opt</code> is <code>min</code>, then returns the smallest <code>lambda</code> with minimum cross validation error. If <code>lambda.opt</code> is <code>max</code>, then returns the largest <code>lambda</code> with minimum cross validation error.</p>
</td></tr>
<tr><td><code>lambda.1se</code></td>
<td>
<p>The largest value of <code>lambda</code> such that error is
within one standard error of the minimum. This arguement is only available for object <code>cv.msda.original</code> and <code>cv.msda.modified</code>.</p>
</td></tr>
<tr><td><code>model.fit</code></td>
<td>
<p>A fitted <code>cv.dsda</code> or <code>cv.msda.original</code> or <code>cv.msda.modified</code> object for the full data.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Yuqing Pan, Qing Mai, Xin Zhang</p>


<h3>References</h3>

<p>Mai, Q., Zou, H. and Yuan, M. (2012), &quot;A direct approach to sparse discriminant analysis in ultra-high dimensions.&quot; Biometrica, 99, 29-42.
</p>
<p>Mai, Q., Yang, Y., and Zou, H. (2017), &quot;Multiclass sparse discriminant analysis.&quot; Statistica Sinica, in press.
</p>
<p>URL: <a href="https://github.com/emeryyi/msda">https://github.com/emeryyi/msda</a><br />
</p>


<h3>See Also</h3>

<p><code><a href="#topic+msda">msda</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>data(GDS1615)
x &lt;- GDS1615$x
y &lt;- GDS1615$y
obj.cv &lt;- cv.msda(x=x, y=y, nfolds=5, lambda.opt="max")
lambda.min &lt;- obj.cv$lambda.min
obj &lt;- msda(x=x, y=y, lambda=lambda.min)
pred &lt;- predict(obj,x)
</code></pre>

<hr>
<h2 id='cv.SeSDA'>Cross validation for semiparametric sparse discriminant analysis</h2><span id='topic+cv.SeSDA'></span>

<h3>Description</h3>

<p>Choose the optimal lambda for semiparametric sparse discriminant analysis by cross validation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cv.SeSDA(x, y, nfolds = 5, lambda=NULL, lambda.opt="min",
  standardize=FALSE, alpha=1, eps=1e-7)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cv.SeSDA_+3A_x">x</code></td>
<td>
<p>An n by p matrix containing the predictors.</p>
</td></tr>
<tr><td><code id="cv.SeSDA_+3A_y">y</code></td>
<td>
<p>An n-dimensional vector containing the class labels.</p>
</td></tr>
<tr><td><code id="cv.SeSDA_+3A_nfolds">nfolds</code></td>
<td>
<p>The number of folds to be used in cross validation. Default is 5.</p>
</td></tr>
<tr><td><code id="cv.SeSDA_+3A_lambda">lambda</code></td>
<td>
<p>A sequence of lambda's.</p>
</td></tr>
<tr><td><code id="cv.SeSDA_+3A_lambda.opt">lambda.opt</code></td>
<td>
<p>Should be either &quot;min&quot; or &quot;max&quot;, specifying whether the smallest or the largest lambda with the smallest cross validation error should be used for the final classification rule.</p>
</td></tr>
<tr><td><code id="cv.SeSDA_+3A_standardize">standardize</code></td>
<td>
<p>A logic object indicating whether x.matrix should be standardized before performing DSDA. Default is FALSE.</p>
</td></tr>
<tr><td><code id="cv.SeSDA_+3A_alpha">alpha</code></td>
<td>
<p>The elasticnet mixing parameter, the same as in glmnet. Default is alpha=1 so that the lasso penalty is used.</p>
</td></tr>
<tr><td><code id="cv.SeSDA_+3A_eps">eps</code></td>
<td>
<p>Convergence threshold for coordinate descent, the same as in glmnet. Default is 1e-7.</p>
</td></tr>
</table>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>transform</code></td>
<td>
<p>The transformation functions.</p>
</td></tr>
<tr><td><code>objdsda</code></td>
<td>
<p>The output of cross validation from <code>cv.dsda</code> on transformed data.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Mai, Q., Zou, H. and Yuan, M. (2013). A direct approach to sparse discriminant analysis in ultra-high dimensions. Biometrika, 99, 29-42.
</p>


<h3>See Also</h3>

<p><code>cv.dsda</code>
<code>SeSDA</code>
</p>

<hr>
<h2 id='dsda'>Solution path for direct sparse discriminant analysis</h2><span id='topic+dsda'></span>

<h3>Description</h3>

<p>Compute the solution path for direct sparse discriminant analysis (DSDA).</p>


<h3>Usage</h3>

<pre><code class='language-R'>dsda(x, z=NULL, y, testx=NULL, testz=NULL, standardize=FALSE, 
 lambda=lambda, alpha=1, eps=1e-7)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="dsda_+3A_x">x</code></td>
<td>
<p>Input matrix of predictors. <code>x</code> is of dimension <code class="reqn">N \times p</code>; each row is an observation vector.</p>
</td></tr>
<tr><td><code id="dsda_+3A_z">z</code></td>
<td>
<p>Input covariate matrix of dimension <code class="reqn">N \times q</code>, where <code class="reqn">q&lt;N</code>. <code>z</code> can be omitted if covariate is absent. </p>
</td></tr>
<tr><td><code id="dsda_+3A_y">y</code></td>
<td>
<p>An n-dimensional vector containing the class labels. The classes have to be labeled as 1 and 2.</p>
</td></tr>
<tr><td><code id="dsda_+3A_testx">testx</code></td>
<td>
<p>Input testing matrix. Each row is a test case. When <code>testx</code> is not provided, the function will only fit the model and return the classifier. When <code>testx</code> is provided, the function will predict response on <code>testx</code> as well.</p>
</td></tr>
<tr><td><code id="dsda_+3A_testz">testz</code></td>
<td>
<p>Input testing covariate matrix. Can be omitted if covariate is absent. However, training covariates <code>z</code> and testing covariates <code>testz</code> must be provided or not at the same time.</p>
</td></tr>
<tr><td><code id="dsda_+3A_standardize">standardize</code></td>
<td>
<p>A logic object indicating whether x should be standardized before performing DSDA. Default is FALSE.</p>
</td></tr>
<tr><td><code id="dsda_+3A_lambda">lambda</code></td>
<td>
<p>A sequence of lambda's. If lambda is missed, the function will automatically generates a sequence of lambda's to fit model.</p>
</td></tr>
<tr><td><code id="dsda_+3A_alpha">alpha</code></td>
<td>
<p>The elasticnet mixing parameter, the same as in glmnet. Default is alpha=1 so that the lasso penalty is used.</p>
</td></tr>
<tr><td><code id="dsda_+3A_eps">eps</code></td>
<td>
<p>Convergence threshold for coordinate descent, the same as in glmnet. Default is 1e-7.</p>
</td></tr>
</table>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>beta</code></td>
<td>
<p>Output variable coefficients for each lambda. The first element of each solution is the intercept.</p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>
<p>The sequence of lambda's used in computing the solution path.</p>
</td></tr>
<tr><td><code>x</code></td>
<td>
<p>The predictor matrix in training data.</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>The class label in training data.</p>
</td></tr>
<tr><td><code>pred</code></td>
<td>
<p>Predicted categorical response for each value in sequence <code>lambda</code> when <code>testx</code> is provided.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Yuqing Pan, Qing Mai, Xin Zhang
</p>


<h3>References</h3>

<p>Mai, Q., Zou, H. and Yuan, M. (2013). A direct approach to sparse discriminant analysis in ultra-high dimensions. Biometrika, 99, 29-42.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  data(GDS1615)   ##load the prostate data
  x&lt;-GDS1615$x
  y&lt;-GDS1615$y
  x=x[which(y&lt;3),]
  y=y[which(y&lt;3)]
  obj.path &lt;- dsda(x, y=y)

</code></pre>

<hr>
<h2 id='dsda.all'>Direct sparse discriminant analysis</h2><span id='topic+dsda.all'></span>

<h3>Description</h3>

<p>Performs direct sparse discriminant analysis, with the optimal lambda chosen by cross validation. The function can perform prediction on test data as well.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dsda.all(x, y, x.test.matrix=NULL, y.test=NULL, standardize=FALSE, 
lambda.opt="min", nfolds=10, lambda=lambda, alpha=1, eps=1e-7)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="dsda.all_+3A_x">x</code></td>
<td>
<p>An n by p matrix containing the predictors.</p>
</td></tr>
<tr><td><code id="dsda.all_+3A_y">y</code></td>
<td>
<p>An n-dimensional vector containing the class labels 1 and 2.</p>
</td></tr>
<tr><td><code id="dsda.all_+3A_x.test.matrix">x.test.matrix</code></td>
<td>
<p>The predictors of a testing set. (Optional.)</p>
</td></tr>
<tr><td><code id="dsda.all_+3A_y.test">y.test</code></td>
<td>
<p>The class labels of the testing set. (Required if x.test.matrix is supplied, but otherwise optional.)</p>
</td></tr>
<tr><td><code id="dsda.all_+3A_standardize">standardize</code></td>
<td>
<p>A logic object indicating whether x.matrix should be standardized before performing DSDA. Default is FALSE.</p>
</td></tr>
<tr><td><code id="dsda.all_+3A_lambda.opt">lambda.opt</code></td>
<td>
<p>Should be either &quot;min&quot; or &quot;max&quot;, specifying whether the smallest or the largest lambda with the smallest cross validation error should be used for the final classification rule.</p>
</td></tr>
<tr><td><code id="dsda.all_+3A_nfolds">nfolds</code></td>
<td>
<p>The number of folds to be used in cross validation. Default is 10.</p>
</td></tr>
<tr><td><code id="dsda.all_+3A_lambda">lambda</code></td>
<td>
<p>A sequence of lambda's.</p>
</td></tr>
<tr><td><code id="dsda.all_+3A_alpha">alpha</code></td>
<td>
<p>The elasticnet mixing parameter, the same as in glmnet. Default is alpha=1 so that the lasso penalty is used.</p>
</td></tr>
<tr><td><code id="dsda.all_+3A_eps">eps</code></td>
<td>
<p>Convergence threshold for coordinate descent, the same as in glmnet. Default is 1e-7.</p>
</td></tr>
</table>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>error</code></td>
<td>
<p>Testing error if x.test.matrix is supplied.</p>
</td></tr>
<tr><td><code>beta</code></td>
<td>
<p>The coefficients of the classification rule corresponding to the optimal lambda chosen by cross validation.</p>
</td></tr>
<tr><td><code>s</code></td>
<td>
<p>The optimal lambda chosen by cross validation.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Yuqing Pan, Qing Mai, Xin Zhang
</p>


<h3>References</h3>

<p>Mai, Q., Zou, H. and Yuan, M., (2012), &quot;A direct approach to sparse discriminant analysis in ultra-high dimensions.&quot; Biometrika, 99, 29-42.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dsda">dsda</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(GDS1615)   ##load the prostate data
x&lt;-GDS1615$x
y&lt;-GDS1615$y

x=x[which(y&lt;3),]
y=y[which(y&lt;3)]

n&lt;-length(y)    ##split the original dataset to a training set and a testing set
n.test&lt;-round(n/3)
set.seed(20120822)
id&lt;-sample(n,n.test,replace=FALSE)
x.train&lt;-x[-id,]
x.test&lt;-x[id,]
y.train&lt;-y[-id]
y.test&lt;-y[id]

set.seed(123)
##perform direct sparse discriminant analysis
obj&lt;-dsda.all(x.train,y.train,x.test,y.test)  
obj$error 
</code></pre>

<hr>
<h2 id='GDS1615'>GDS1615 data introduced in Burczynski et al. (2012).</h2><span id='topic+GDS1615'></span><span id='topic+x'></span><span id='topic+y'></span>

<h3>Description</h3>

<p>The  dataset is a subset of the dataset available on Gene Expression Omnibus with the accession number GDS1615. The original dataset contains 22283 gene expression levels and the disease states of the observed subjects. In Mai, Yang and Zou, the dimension of the original dataset was first reduced to 127 by F-test screening.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(GDS1615)</code></pre>


<h3>Value</h3>

<p>This data frame contains the following:
</p>
<table role = "presentation">
<tr><td><code>x</code></td>
<td>
<p>Gene expression levels.</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>Disease state that is coded as 1,2,3. 1: normal; 2: ulcerative colitis; 3: Crohn's disease.</p>
</td></tr>
</table>


<h3>References</h3>

<p>M. E. Burczynski, R. L Peterson, N. C. Twine, K. A. Zuberek, B. J. Brodeur, L. Casciotti, V. Maganti, P. S. Reddy, A. Strahs, F. Immermann, W. Spinelli, U. Schwertschlag, A. M. Slager, M. M. Cotreau, and A. J. Dorner.  (2012), &quot;Molecular classification of crohn's disease and ulcerative colitis patients using transcriptional profiles in peripheral blood mononuclear cells&quot;. <em>Journal of Molecular Diagnostics</em>, 8:51&ndash;61.
</p>
<p>Mai, Q., Zou, H. and Yuan, M. (2012), &quot;A direct approach to sparse discriminant analysis in ultra-high dimensions.&quot; Biometrica, 99, 29-42.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(GDS1615)
</code></pre>

<hr>
<h2 id='getnorm'>Direct sparse discriminant analysis</h2><span id='topic+getnorm'></span>

<h3>Description</h3>

<p>Transform the predictors to achieve normality.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getnorm(x, y, type="pooled")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="getnorm_+3A_x">x</code></td>
<td>
<p>an n dimensional vector containing n observations for one predictor.</p>
</td></tr>
<tr><td><code id="getnorm_+3A_y">y</code></td>
<td>
<p>an n-dimensional vector containing the class labels.</p>
</td></tr>
<tr><td><code id="getnorm_+3A_type">type</code></td>
<td>
<p>The type of estimator. Two estimators were proposed in Mai &amp; Zou (2015), the naive estimator and the pooled estimator. The function getnorm() uses the naive estimator if type=&quot;naive&quot;, and it uses the pooled estimator if type=&quot;pooled&quot;. The default is &quot;pooled&quot;. When the naive estimator is used, it is recommended to label the class with more samples as Class 0.</p>
</td></tr>
</table>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>x.norm</code></td>
<td>
<p>Transformed x.</p>
</td></tr>
<tr><td><code>f0</code></td>
<td>
<p>The transformation computed based on observations from Class 0. Not applicable if type=&quot;naive&quot;.</p>
</td></tr>
<tr><td><code>f1</code></td>
<td>
<p>The transformation computed based on observations from Class 1. Not applicable if type=&quot;naive&quot;.</p>
</td></tr>
<tr><td><code>mu.hat</code></td>
<td>
<p>The sample mean for transformed x from Class 1.</p>
</td></tr>
<tr><td><code>transform</code></td>
<td>
<p>The transformation that was actually used to transform x.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Mai, Q., Zou, H. and Yuan, M. (2013). A direct approach to sparse discriminant analysis in ultra-high dimensions. Biometrika, 99, 29-42.
</p>
<p>Mai, Q. and Zou, H. (2015). Sparse semiparametric discriminant analysis. Journal of Multivariate Analysis, 135, 175-188. 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(GDS1615)   ##load the prostate data
x&lt;-GDS1615$x
y&lt;-GDS1615$y
x&lt;-exp(x[which(y&lt;3),])
y&lt;-y[which(y&lt;3)]

n&lt;-length(y)
n1&lt;-sum(y==1)
n2&lt;-n-n1
n1.test&lt;-round(n1/2)
n2.test&lt;-round(n2/2)
n.test&lt;-n1.test+n2.test
n.train&lt;-n-n.test
id.test&lt;-c(sample(which(y==1),n1.test),sample(which(y==2),n2.test))

p&lt;-ncol(x)
x.train&lt;-x[-id.test,]
y.train&lt;-y[-id.test]
x.test&lt;-x[id.test,]
y.test&lt;-y[id.test]

 x.norm&lt;-matrix(0,n.train,p)
  x.test.norm&lt;-matrix(0,n.test,p)
  for(i in 1:p){
    obj.norm&lt;-getnorm(x.train[,i],y.train)
    x.norm[,i]&lt;-obj.norm$x.norm
    x.test.norm[,i]&lt;-obj.norm$transform(x.test[,i])
  }
    
  obj&lt;-dsda.all(x.norm,y.train,x.test.norm,y.test)



</code></pre>

<hr>
<h2 id='msda'>
Fits a regularization path of Sparse Discriminant Analysis and predicts</h2><span id='topic+msda'></span>

<h3>Description</h3>

<p>Fits a regularization path of Sparse Discriminant Analysis at a sequence of regularization parameters lambda. Performs prediction when testing data is provided. The <code>msda</code> function solves classification problem by fitting a sparse discriminant analysis model. When covariates are provided, the function will first make adjustment on the training data. It provides three models: <code>binary</code> for fitting DSDA model to solve binary classification problems, <code>multi.original</code> and <code>multi.modified</code> for fitting MSDA model to solve multi-class classification problems. <code>multi.original</code> runs faster for small dimension case but the computation ability is limited to a relatively large dimension. <code>multi.modified</code> has no such limitation and works in ultra-high dimensions. User can specify method by argument or use the default settings.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>msda(x, z=NULL, y, testx=NULL,testz=NULL, model = NULL, lambda = NULL, 
 standardize=FALSE, alpha=1, nlambda = 100, 
 lambda.factor = ifelse((nobs - nclass)&lt;= nvars, 0.2, 1e-03), dfmax = nobs, 
 pmax = min(dfmax * 2 + 20, nvars), pf = rep(1, nvars), eps = 1e-04, 
 maxit = 1e+06, sml = 1e-06, verbose = FALSE, perturb = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="msda_+3A_x">x</code></td>
<td>
<p>Input matrix of predictors. <code>x</code> is of dimension <code class="reqn">N \times p</code>; each row is an observation vector.</p>
</td></tr>
<tr><td><code id="msda_+3A_z">z</code></td>
<td>
<p>Input covariate matrix of dimension <code class="reqn">N \times q</code>, where <code class="reqn">q&lt;N</code>. <code>z</code> can be omitted if covariate is absent. </p>
</td></tr>
<tr><td><code id="msda_+3A_y">y</code></td>
<td>
<p>Class labl. This argument should be a factor for classification. For <code>model</code>=<code>'binary'</code>, <code>y</code> should be a binary variable with values 1 and 2. For <code>model</code>=<code>'multi.original'</code> or <code>'multi.modified'</code>, <code>y</code> should be a multi-class variable starting from 1.
</p>
</td></tr>
<tr><td><code id="msda_+3A_testx">testx</code></td>
<td>
<p>Input testing matrix. Each row is a test case. When <code>testx</code> is not provided, the function will only fit the model and return the classifier. When <code>testx</code> is provided, the function will predict response on <code>testx</code> as well.</p>
</td></tr>
<tr><td><code id="msda_+3A_testz">testz</code></td>
<td>
<p>Input testing covariate matrix. Can be omitted if covariate is absent. However, training covariates <code>z</code> and testing covariates <code>testz</code> must be provided or not at the same time.</p>
</td></tr>
<tr><td><code id="msda_+3A_model">model</code></td>
<td>
<p>Method type. The <code>model</code> argument can be one of <code>'binary'</code>, <code>'multi.original'</code>, <code>'multi.modified'</code> and the default is NULL. The function supports fitting DSDA and MSDA models by specifying method type. Without specification, the function will automatically choose one of the methods. If the response variable is binary, the function will fit a DSDA model. If the response variable is multi-class, the function will fit an original MSDA model for dimension <code class="reqn">p&lt;=2000</code> and a modified MSDA model for dimension <code class="reqn">p&gt;2000</code>.</p>
</td></tr>
<tr><td><code id="msda_+3A_lambda">lambda</code></td>
<td>

<p>A user supplied <code>lambda</code> sequence. Typically, by leaving this option unspecified users can have the program compute its own <code>lambda</code> sequence based on
<code>nlambda</code> and <code>lambda.factor</code>. Supplying a value of
<code>lambda</code> overrides this. It is better to supply
a decreasing sequence of <code>lambda</code> values than a single (small) value, if not, the program will sort user-defined <code>lambda</code> sequence in decreasing order automatically.</p>
</td></tr>
<tr><td><code id="msda_+3A_standardize">standardize</code></td>
<td>
<p>A logic object indicating whether x should be standardized before performing DSDA. Default is FALSE. This argument is only valid for <code>model = 'binary'</code>.</p>
</td></tr>
<tr><td><code id="msda_+3A_alpha">alpha</code></td>
<td>
<p>The elasticnet mixing parameter, the same as in glmnet. Default is alpha=1 so that the lasso penalty is used in DSDA. This argument is only valid for <code>model = 'binary'</code>.</p>
</td></tr>
<tr><td><code id="msda_+3A_nlambda">nlambda</code></td>
<td>
<p>The number of tuning values in sequence <code>lambda</code>. If users do not specify <code>lambda</code> values, the package will generate a solution path containing <code>nlambda</code> many tuning values of <code>lambda</code>. Default is 100 for <code>model = 'multi.original'</code> and 50 for <code>model = 'multi.modified'</code>.</p>
</td></tr>
<tr><td><code id="msda_+3A_lambda.factor">lambda.factor</code></td>
<td>

<p>The factor for getting the minimal lambda in <code>lambda</code> sequence, where <code>min(lambda)</code> = <code>lambda.factor</code> * <code>max(lambda)</code>.  <code>max(lambda)</code> is the smallest value of <code>lambda</code> for which all coefficients are zero. The default depends on <code class="reqn">p</code> (the number of predictors) and its relationship with <code class="reqn">N</code> (the number of rows in the matrix of predictors). 
For Original MSDA, if <code class="reqn">N &gt; p</code>, the default is <code>0.0001</code>, close to zero.  If <code class="reqn">N&lt;p</code>, the default is <code>0.2</code>. For Modified MSDA, if <code class="reqn">p\le 5000</code>, the default is <code>0.2</code>. If <code class="reqn">5000&lt;p\le 30000</code>, the default is <code>0.4</code>. If <code class="reqn">p&gt;30000</code>, the default is <code>0.5</code>.
A very small value of <code>lambda.factor</code> will lead to a saturated fit. It takes no effect if there is user-defined <code>lambda</code> sequence. This argument is only valid for <code>multi.original</code> and <code>multi.modified</code>.</p>
</td></tr>
<tr><td><code id="msda_+3A_dfmax">dfmax</code></td>
<td>

<p>The maximum number of selected variables in the model. Default is the number of observations <code>N</code>. This argument is only valid for <code>multi.original</code> and <code>multi.modified</code>.</p>
</td></tr>
<tr><td><code id="msda_+3A_pmax">pmax</code></td>
<td>

<p>The maximum number of potential selected variables during iteration. In middle step, the algorithm can select at most <code>pmax</code> variables and then shrink part of them such that the nubmer of final selected variables is less than <code>dfmax</code>. Default is <code class="reqn">\min(dfmax\times 2+20, N)</code>.</p>
</td></tr>
<tr><td><code id="msda_+3A_pf">pf</code></td>
<td>

<p>L1 penalty factor of length <code class="reqn">p</code>. Separate L1 penalty weights can be applied to each coefficient of <code class="reqn">\theta</code> to allow
differential L1 shrinkage. Can be 0 for some variables, which implies
no L1 shrinkage, and results in that variable always being included in the
model. Default is 1 for all variables (and implicitly infinity for
variables listed in <code>exclude</code>). This argument is only valid for <code>multi.original</code> and <code>multi.modified</code>.</p>
</td></tr>
<tr><td><code id="msda_+3A_eps">eps</code></td>
<td>

<p>Convergence threshold for coordinate descent. Each inner
coordinate descent loop continues until the relative change in any
coefficient. Defaults value is <code>1e-4</code>.</p>
</td></tr>
<tr><td><code id="msda_+3A_maxit">maxit</code></td>
<td>

<p>Maximum number of outer-loop iterations allowed at fixed lambda value. Default is 1e6. If models do not converge, consider increasing <code>maxit</code>. This argument is only valid for <code>multi.original</code> and <code>multi.modified</code>.</p>
</td></tr>
<tr><td><code id="msda_+3A_sml">sml</code></td>
<td>

<p>Threshold for ratio of loss function change after each iteration to old loss function value. Default is <code>1e-06</code>. This argument is only valid for <code>multi.original</code> and <code>multi.modified</code>.
</p>
</td></tr>
<tr><td><code id="msda_+3A_verbose">verbose</code></td>
<td>

<p>Whether to print out computation progress. The default is <code>FALSE</code>. This argument is only valid for <code>multi.original</code> and <code>multi.modified</code>.</p>
</td></tr>
<tr><td><code id="msda_+3A_perturb">perturb</code></td>
<td>
<p>A scalar number. If it is specified, the number will be added to each diagonal element of the covariance matrix as perturbation. The default is <code>NULL</code>. This argument is only valid for <code>multi.original</code> and <code>multi.modified</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>msda</code> function fits a linear discriminant analysis model for vector <code class="reqn">X</code> as follows:
</p>
<p style="text-align: center;"><code class="reqn">\mathbf{X}|Y=k\sim N(\boldsymbol{\mu}_k,\boldsymbol{\Sigma}).</code>
</p>

<p>The categorical response is predicted from the Bayes rule:
</p>
<p style="text-align: center;"><code class="reqn">\widehat{Y}=\arg\max_{k=1,\cdots,K}{(\mathbf{X}-\frac{\boldsymbol{\mu}_k}{2})^T\boldsymbol{\beta}_k+\log\pi_k}.</code>
</p>

<p>The parameter <code>model</code> specifies which method to use in estimating <code class="reqn">\boldsymbol{\beta}</code>. Users can use <code>binary</code> for binary problems and <code>binary</code> and <code>multi.modified</code> for multi-class problems. In <code>multi.original</code>, the algorithm first computes and stores <code class="reqn">\boldsymbol{\Sigma}</code>, while it doesn't compute or store the entire covariance matrix in <code>multi.modified</code>. Since the algorithm is element-wise based, <code>multi.modified</code> computes each element of covariance matrix when needed. Therefore, <code>multi.original</code> is faster for low dimension but <code>multi.modified</code> can fit model for a much higher dimension case.
</p>
<p>Note that for computing speed reason, if models are not converging or running slow, consider increasing <code>eps</code> and <code>sml</code>, or decreasing
<code>nlambda</code>, or increasing <code>lambda.factor</code> before increasing
<code>maxit</code>. Users can also reduce <code>dfmax</code> to limit the maximum number of variables in the model.
</p>
<p>The arguments list out all parameters in the three models, but not all of them are necessary in applying one of the methods. See the specific explaination of each argument for more detail. Meanwhile, the output of DSDA model only includes <code>beta</code> and <code>lambda</code>.
</p>


<h3>Value</h3>

<p>An object with S3 class <code>dsda</code> or <code>msda.original</code> and <code>msda.modified</code>.
</p>
<table role = "presentation">
<tr><td><code>beta</code></td>
<td>
<p>Output variable coefficients for each <code>lambda</code>, which is the estimation of <code class="reqn">\boldsymbol{\beta}</code> in the Bayes rule. <code>beta</code> is a list of length being the number of <code>lambda</code>s. Each element of <code>beta</code> is a matrix of dimension <code class="reqn">nvars\times (nclass-1)</code>. For <code>model = 'dsda'</code>, <code>beta</code> is a vector of length <code class="reqn">nvars+1</code>, where the first element is intercept.</p>
</td></tr>
<tr><td><code>df</code></td>
<td>
<p>The number of nonzero coefficients for each value of <code>lambda</code>.</p>
</td></tr>
<tr><td><code>obj</code></td>
<td>
<p>The fitted value of the objective function for each value of <code>lambda</code>.</p>
</td></tr>
<tr><td><code>dim</code></td>
<td>
<p>Dimension of each coefficient matrix.</p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>
<p>The actual <code>lambda</code> sequence used. The user specified sequence or automatically generated sequence could be truncated by constraints on <code>dfmax</code> and <code>pmax</code>.</p>
</td></tr>
<tr><td><code>x</code></td>
<td>
<p>The input matrix of predictors for training.</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>Class label in training data.</p>
</td></tr>
<tr><td><code>npasses</code></td>
<td>
<p>Total number of iterations (the most inner loop) summed over all lambda values</p>
</td></tr>
<tr><td><code>jerr</code></td>
<td>
<p>Error flag, for warnings and errors, 0 if no error.</p>
</td></tr>
<tr><td><code>sigma</code></td>
<td>
<p>Estimated sigma matrix. This argument is only available in object <code>msda.original</code>.</p>
</td></tr>
<tr><td><code>delta</code></td>
<td>
<p>Estimated delta matrix. delta[k] = mu[k]-mu[1].</p>
</td></tr>
<tr><td><code>mu</code></td>
<td>
<p>Estimated mu vector.</p>
</td></tr>
<tr><td><code>prior</code></td>
<td>
<p>Prior probability that y belong to class k, estimated by mean(y that belong to k).</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>The call that produced this object</p>
</td></tr>
<tr><td><code>pred</code></td>
<td>
<p>Predicted categorical response for each value in sequence <code>lambda</code> when <code>testx</code> is provided.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Yuqing Pan, Qing Mai, Xin Zhang
</p>


<h3>References</h3>

<p>Mai, Q., Zou, H. and Yuan, M. (2012), &quot;A direct approach to sparse discriminant analysis in ultra-high dimensions.&quot; Biometrica, 99, 29-42.
</p>
<p>Mai, Q., Yang, Y., and Zou, H. (2017), &quot;Multiclass sparse discriminant analysis.&quot; Statistica Sinica, in press.
</p>
<p>URL: <a href="https://github.com/emeryyi/msda">https://github.com/emeryyi/msda</a><br />
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cv.msda">cv.msda</a></code>, <code><a href="#topic+predict.msda">predict.msda</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(GDS1615)
x&lt;-GDS1615$x
y&lt;-GDS1615$y
obj &lt;- msda(x = x, y = y)
</code></pre>

<hr>
<h2 id='predict.catch'>
Predict categorical responses for matrix/tensor data.
</h2><span id='topic+predict.catch'></span>

<h3>Description</h3>

<p>Predict categorical responses on new matrix/tensor data given the fitted CATCH model input.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'catch'
predict(object, newx, z = NULL, ztest = NULL, gamma = NULL,...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.catch_+3A_object">object</code></td>
<td>
<p>Input <code>catchobj</code> class object as fitted model.</p>
</td></tr>
<tr><td><code id="predict.catch_+3A_newx">newx</code></td>
<td>
<p>Input adjusted testing tensor or matrix list. Each element of the list is a tensor. The tensor should of the same dimension as training data.</p>
</td></tr>
<tr><td><code id="predict.catch_+3A_z">z</code></td>
<td>
<p>Input training covariates matrix. <code>z</code> can be omitted if there is no covariate. </p>
</td></tr>
<tr><td><code id="predict.catch_+3A_ztest">ztest</code></td>
<td>
<p>Input testing covariates matrix. <code>ztest</code> can be omitted if there is no covariate.</p>
</td></tr>
<tr><td><code id="predict.catch_+3A_gamma">gamma</code></td>
<td>
<p>Coefficients of covariates obtained from <code><a href="#topic+adjten">adjten</a></code>. <code>gamma</code> is <code>NULL</code> if there is no covariate.</p>
</td></tr>
<tr><td><code id="predict.catch_+3A_...">...</code></td>
<td>
<p>Other arguments that can be passed to <code>predict</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function fits LDA model on selected discriminant vectors. Call <code>predict</code> or <code>predict.catch</code> to perform predictions.
</p>
<p>There are two ways to make predictions. One way is to directly predict at the same time as fitting model by <code><a href="#topic+catch">catch</a></code> since <code><a href="#topic+predict.catch">predict.catch</a></code> has already been embedded in <code><a href="#topic+catch">catch</a></code> and it will predicts response when testing data is provided. The other way is to first use <code><a href="#topic+adjten">adjten</a></code> to adjuste tensor and <code><a href="#topic+catch">catch</a></code> to fit model. <code><a href="#topic+predict.catch">predict.catch</a></code> will take the input adjusted tensor list <code>newx</code>, covariate coefficient <code>gamma</code> from <code><a href="#topic+adjten">adjten</a></code> and the fitted model from <code><a href="#topic+catch">catch</a></code> to perform prediction. The prediction is identical to providing <code><a href="#topic+catch">catch</a></code> testing data.
</p>


<h3>Value</h3>

<p>Predicted response of <code>newx</code> for each <code>lambda</code> in model <code>object</code>.</p>


<h3>Author(s)</h3>

<p>Yuqing Pan, Qing Mai, Xin Zhang
</p>


<h3>References</h3>

<p>Pan, Y., Mai, Q., and Zhang, X. (2018) <em>Covariate-Adjusted Tensor Classification in High-Dimensions</em>, arXiv:1805.04421.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+catch">catch</a></code>, <code><a href="#topic+adjten">adjten</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#generate training data
n &lt;- 20
p &lt;- 4
k &lt;- 2
nvars &lt;- p*p*p
x &lt;- array(list(),n)
vec_x &lt;- matrix(rnorm(n*nvars),nrow=n,ncol=nvars)
vec_x[1:10,] &lt;- vec_x[1:10,]+2
z &lt;- matrix(rnorm(n*2),nrow=n,ncol=2)
z[1:10,] &lt;- z[1:10,]+0.5
y &lt;- c(rep(1,10),rep(2,10))
for (i in 1:n){
  x[[i]] &lt;- array(vec_x[i,],dim=c(p,p,p))
}

#generate testing data
newx &lt;- array(list(),n)
vec_newx &lt;- matrix(rnorm(n*nvars),nrow=n,ncol=nvars)
vec_newx[1:10,] &lt;- vec_newx[1:10,]+2
newz &lt;- matrix(rnorm(n*2),nrow=n,ncol=2)
newz[1:10,] &lt;- newz[1:10,]+0.5
for (i in 1:n){
  newx[[i]] &lt;- array(vec_newx[i,],dim=c(p,p,p))
}

#Make adjustment and fit model
obj &lt;- adjten(x, z, y, newx, newz)
fit &lt;- catch(x, z, y)
#Predict
pred &lt;- predict(fit, obj$testxres, z, newz, obj$gamma)

#The adjusting, fitting model and predicting step can also be completed
#by one command.
pred &lt;- catch(x, z, y, newx, newz)$pred
</code></pre>

<hr>
<h2 id='predict.dsda'>Prediction for direct sparse discriminant analysis</h2><span id='topic+predict.dsda'></span>

<h3>Description</h3>

<p>Predict the class labels by direct sparse discriminant analysis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'dsda'
predict(object, newx, z=NULL, ztest=NULL, gamma=NULL,...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.dsda_+3A_object">object</code></td>
<td>
<p>An object returned by <code>dsda</code> or <code>msda</code> with binary setting.</p>
</td></tr>
<tr><td><code id="predict.dsda_+3A_newx">newx</code></td>
<td>
<p>An n by p matrix containing the predictors.</p>
</td></tr>
<tr><td><code id="predict.dsda_+3A_z">z</code></td>
<td>
<p>Input training covariates matrix. <code>z</code> can be omitted if there is no covariate. </p>
</td></tr>
<tr><td><code id="predict.dsda_+3A_ztest">ztest</code></td>
<td>
<p>Input testing covariates matrix. <code>ztest</code> can be omitted if there is no covariate.</p>
</td></tr>
<tr><td><code id="predict.dsda_+3A_gamma">gamma</code></td>
<td>
<p>Coefficients of covariates obtained from <code><a href="#topic+adjvec">adjvec</a></code>. <code>gamma</code> is <code>NULL</code> if there is no covariate.</p>
</td></tr>
<tr><td><code id="predict.dsda_+3A_...">...</code></td>
<td>
<p>Other arguments that can be passed to <code>predict</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>pred</code></td>
<td>
<p>The the predicted class labels.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Mai, Q., Zou, H. and Yuan, M. (2013), &quot;A direct approach to sparse discriminant analysis in ultra-high dimensions.&quot; Biometrika, 99, 29-42.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dsda">dsda</a></code>, <code><a href="#topic+dsda.all">dsda.all</a></code>, <code><a href="#topic+predict.msda">predict.msda</a></code>
</p>

<hr>
<h2 id='predict.msda'>
Predict categorical responses for vector data.
</h2><span id='topic+predict.msda'></span>

<h3>Description</h3>

<p>Predict categorical responses on new vector data given the fitted DSDA/MSDA model input.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'msda'
predict(object, newx, z = NULL, ztest = NULL, gamma = NULL,...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.msda_+3A_object">object</code></td>
<td>
<p>Fitted model object from <code>msda</code>. The model object can be anyone of <code>binary</code>, <code>multi.original</code> and <code>multi.modified</code>.</p>
</td></tr>
<tr><td><code id="predict.msda_+3A_newx">newx</code></td>
<td>
<p>The matrix of new values for <code>x</code> at which predictions are
to be made. If covariates exist, then <code>newx</code> should be adjusted matrix.</p>
</td></tr>
<tr><td><code id="predict.msda_+3A_z">z</code></td>
<td>
<p>Input training covariates matrix. <code>z</code> can be omitted if there is no covariate. </p>
</td></tr>
<tr><td><code id="predict.msda_+3A_ztest">ztest</code></td>
<td>
<p>Input testing covariates matrix. <code>ztest</code> can be omitted if there is no covariate.</p>
</td></tr>
<tr><td><code id="predict.msda_+3A_gamma">gamma</code></td>
<td>
<p>Coefficients of covariates obtained from <code><a href="#topic+adjvec">adjvec</a></code>. <code>gamma</code> is <code>NULL</code> if there is no covariate.</p>
</td></tr>
<tr><td><code id="predict.msda_+3A_...">...</code></td>
<td>
<p>Other arguments that can be passed to <code>predict</code>. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function fits LDA model on selected discriminant vectors. Call <code><a href="stats.html#topic+predict">predict</a></code> or <code><a href="#topic+predict.msda">predict.msda</a></code> to perform prediction. When covariates exist, users could first call <code><a href="#topic+adjvec">adjvec</a></code> to make adjustment and obtain obtain <code>gamma</code>. The fitted model from <code><a href="#topic+msda">msda</a></code> should also takes adjusted vector as input. The <code>newx</code> in <code><a href="#topic+predict.msda">predict.msda</a></code> shoudl be adjusted vector as well.
</p>


<h3>Value</h3>

<p>Predicted class label(s) at the entire sequence of the penalty parameter <code>lambda</code> used to create the model.
</p>


<h3>Author(s)</h3>

<p>Yuqing Pan, Qing Mai, Xin Zhang
</p>


<h3>References</h3>

<p>Mai, Q., Zou, H. and Yuan, M. (2012), &quot;A direct approach to sparse discriminant analysis in ultra-high dimensions.&quot; Biometrica, 99, 29-42.
</p>
<p>Mai, Q., Yang, Y., and Zou, H. (2017), &quot;Multiclass sparse discriminant analysis.&quot; Statistica Sinica, in press.
</p>
<p>Pan, Y., Mai, Q., and Zhang, X. (2018), &quot;Covariate-Adjusted Tensor Classification in High-Dimensions.&quot; Journal of the American Statistical Association, <em>accepted</em>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+msda">msda</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>data(GDS1615)
x&lt;-GDS1615$x
y&lt;-GDS1615$y
obj &lt;- msda(x = x, y = y)
pred&lt;-predict(obj,x)
</code></pre>

<hr>
<h2 id='predict.SeSDA'>Prediction for semiparametric sparse discriminant analysis</h2><span id='topic+predict.SeSDA'></span>

<h3>Description</h3>

<p>Predict the class labels by semiparametric sparse discriminant analysis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'SeSDA'
predict(object, x.test,...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.SeSDA_+3A_object">object</code></td>
<td>
<p>An object returned by <code>SeSDA</code>.</p>
</td></tr>
<tr><td><code id="predict.SeSDA_+3A_x.test">x.test</code></td>
<td>
<p>An n by p matrix containing the predictors.</p>
</td></tr>
<tr><td><code id="predict.SeSDA_+3A_...">...</code></td>
<td>
<p>Other arguments that can be passed to <code>predict</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>pred</code></td>
<td>
<p>The the predicted class labels.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Mai, Q., Zou, H. and Yuan, M. (2013), &quot;A direct approach to sparse discriminant analysis in ultra-high dimensions.&quot; Biometrika, 99, 29-42.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dsda">dsda</a></code>, <code><a href="#topic+SeSDA">SeSDA</a></code>
</p>

<hr>
<h2 id='ROAD'>Solution path for regularized optimal affine discriminant</h2><span id='topic+ROAD'></span>

<h3>Description</h3>

<p>Compute the solution path for regularized optimal affine discriminant (ROAD).</p>


<h3>Usage</h3>

<pre><code class='language-R'>  ROAD(x,y,standardize=FALSE,lambda=NULL,eps=1e-7)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ROAD_+3A_x">x</code></td>
<td>
<p>Input matrix of predictors. <code>x</code> is of dimension <code class="reqn">N \times p</code>; each row is an observation vector.</p>
</td></tr>
<tr><td><code id="ROAD_+3A_y">y</code></td>
<td>
<p>An n-dimensional vector containing the class labels. The classes have to be labeled as 1 and 2.</p>
</td></tr>
<tr><td><code id="ROAD_+3A_standardize">standardize</code></td>
<td>
<p>A logic object indicating whether x should be standardized before performing ROAD. Default is FALSE.</p>
</td></tr>
<tr><td><code id="ROAD_+3A_lambda">lambda</code></td>
<td>
<p>A sequence of lambda's. If lambda is missed, the function will automatically generates a sequence of lambda's to fit model.</p>
</td></tr>
<tr><td><code id="ROAD_+3A_eps">eps</code></td>
<td>
<p>Convergence threshold for coordinate descent, the same as in glmnet. Default is 1e-7.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function obtains the solution path of ROAD through <code><a href="#topic+dsda">dsda</a></code>.
</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>beta</code></td>
<td>
<p>Output variable coefficients for each lambda. </p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>
<p>The sequence of lambda's used in computing the solution path.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Yuqing Pan, Qing Mai, Xin Zhang
</p>


<h3>References</h3>

<p>Mai, Q. and Zou, H. (2013), &quot;A note on the connection and equivalence of three sparse linear discriminant analysis methods.&quot; Technometrics, 55, 243-246.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>    data(GDS1615)   ##load the prostate data
    x&lt;-GDS1615$x
    y&lt;-GDS1615$y
    x=x[which(y&lt;3),]
    y=y[which(y&lt;3)]
    obj.path &lt;- ROAD(x, y)
    
    </code></pre>

<hr>
<h2 id='SeSDA'>Solution path for semiparametric sparse discriminant analysis</h2><span id='topic+SeSDA'></span>

<h3>Description</h3>

<p>Compute the solution path for semiparametric sparse discriminant analysis.</p>


<h3>Usage</h3>

<pre><code class='language-R'>SeSDA(x,y,standardize=FALSE,lambda=NULL,alpha=1,eps=1e-7)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="SeSDA_+3A_x">x</code></td>
<td>
<p>Input matrix of predictors. <code>x</code> is of dimension <code class="reqn">N \times p</code>; each row is an observation vector.</p>
</td></tr>
<tr><td><code id="SeSDA_+3A_y">y</code></td>
<td>
<p>An n-dimensional vector containing the class labels. The classes have to be labeled as 1 and 2.</p>
</td></tr>
<tr><td><code id="SeSDA_+3A_standardize">standardize</code></td>
<td>
<p>A logic object indicating whether x should be standardized after transformation but before fitting classifier. Default is FALSE.</p>
</td></tr>
<tr><td><code id="SeSDA_+3A_lambda">lambda</code></td>
<td>
<p>A sequence of lambda's. If lambda is missed or NULL, the function will automatically generates a sequence of lambda's to fit model.</p>
</td></tr>
<tr><td><code id="SeSDA_+3A_alpha">alpha</code></td>
<td>
<p>The elasticnet mixing parameter, the same as in glmnet. Default is alpha=1 so that the lasso penalty is used.</p>
</td></tr>
<tr><td><code id="SeSDA_+3A_eps">eps</code></td>
<td>
<p>Convergence threshold for coordinate descent, the same as in glmnet. Default is 1e-7.</p>
</td></tr>
</table>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>transform</code></td>
<td>
<p>The tranformation functions.</p>
</td></tr>
<tr><td><code>objdsda</code></td>
<td>
<p>A DSDA object fitted on transformed data.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Yuqing Pan, Qing Mai, Xin Zhang
</p>


<h3>References</h3>

<p>Mai, Q., Zou, H. and Yuan, M. (2013). A direct approach to sparse discriminant analysis in ultra-high dimensions. Biometrika, 99, 29-42.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  data(GDS1615)   ##load the prostate data
  x&lt;-GDS1615$x
  y&lt;-GDS1615$y
  x=x[which(y&lt;3),]
  y=y[which(y&lt;3)]
  obj.path &lt;- SeSDA(x,y)

</code></pre>

<hr>
<h2 id='sim.bi.vector'>Simulate data</h2><span id='topic+sim.bi.vector'></span>

<h3>Description</h3>

<p>Simulate a binary data set with vector predictor.</p>


<h3>Usage</h3>

<pre><code class='language-R'>  sim.bi.vector(tesize = 100)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sim.bi.vector_+3A_tesize">tesize</code></td>
<td>
<p>Number of observations in testing data.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function simulates a data set with <code class="reqn">p=500</code>. Response are binary.
</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>x</code></td>
<td>
<p>Simulated vector predictor. </p>
</td></tr>
<tr><td><code>testx</code></td>
<td>
<p>Simulated testing vector predictor.</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>Response corresponding to <code>x</code>.</p>
</td></tr>
<tr><td><code>testy</code></td>
<td>
<p>Response corresponding to <code>testx</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Yuqing Pan, Qing Mai, Xin Zhang
</p>

<hr>
<h2 id='sim.tensor.cov'>Simulate data</h2><span id='topic+sim.tensor.cov'></span>

<h3>Description</h3>

<p>Simulate a data set with tensor predictor and covariates.</p>


<h3>Usage</h3>

<pre><code class='language-R'>  sim.tensor.cov(tesize = 100)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sim.tensor.cov_+3A_tesize">tesize</code></td>
<td>
<p>Number of observations in testing data.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function simulates a data set with <code class="reqn">10\times 10\times 10</code> tensor and covariate being a two-dimensional vector. Response are binary.
</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>x</code></td>
<td>
<p>Simulated tensor predictor. </p>
</td></tr>
<tr><td><code>z</code></td>
<td>
<p>Simulated covariate.</p>
</td></tr>
<tr><td><code>testx</code></td>
<td>
<p>Simulated testing tensor predictor.</p>
</td></tr>
<tr><td><code>testz</code></td>
<td>
<p>Simualted testing covariate.</p>
</td></tr>
<tr><td><code>vec_x</code></td>
<td>
<p>Vectorization of <code>x</code>.</p>
</td></tr>
<tr><td><code>vec_testx</code></td>
<td>
<p>Vectorization of <code>testx</code>.</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>Response corresponding to <code>x</code> and <code>z</code>.</p>
</td></tr>
<tr><td><code>testy</code></td>
<td>
<p>Response corresponding to <code>testx</code> and <code>testz</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Yuqing Pan, Qing Mai, Xin Zhang
</p>

<hr>
<h2 id='SOS'>Solution path for sparse discriminant analysis</h2><span id='topic+SOS'></span>

<h3>Description</h3>

<p>Compute the solution path for sparse optimal scoring (SOS).</p>


<h3>Usage</h3>

<pre><code class='language-R'>  SOS(x,y,standardize=FALSE,lambda=NULL,eps=1e-7)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="SOS_+3A_x">x</code></td>
<td>
<p>Input matrix of predictors. <code>x</code> is of dimension <code class="reqn">N \times p</code>; each row is an observation vector.</p>
</td></tr>
<tr><td><code id="SOS_+3A_y">y</code></td>
<td>
<p>An n-dimensional vector containing the class labels. The classes have to be labeled as 1 and 2.</p>
</td></tr>
<tr><td><code id="SOS_+3A_standardize">standardize</code></td>
<td>
<p>A logic object indicating whether x should be standardized before performing SOS. Default is FALSE.</p>
</td></tr>
<tr><td><code id="SOS_+3A_lambda">lambda</code></td>
<td>
<p>A sequence of lambda's. If lambda is missed, the function will automatically generates a sequence of lambda's to fit model.</p>
</td></tr>
<tr><td><code id="SOS_+3A_eps">eps</code></td>
<td>
<p>Convergence threshold for coordinate descent, the same as in glmnet. Default is 1e-7.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function obtains the solution path of sparse optimal scoring model through <code><a href="#topic+dsda">dsda</a></code>.
</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>beta</code></td>
<td>
<p>Output variable coefficients for each lambda. </p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>
<p>The sequence of lambda's used in computing the solution path.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Yuqing Pan, Qing Mai, Xin Zhang
</p>


<h3>References</h3>

<p>Mai, Q. and Zou, H. (2013), &quot;A note on the connection and equivalence of three sparse linear discriminant analysis methods.&quot; Technometrics, 55, 243-246.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>    data(GDS1615)   ##load the prostate data
    x&lt;-GDS1615$x
    y&lt;-GDS1615$y
    x=x[which(y&lt;3),]
    y=y[which(y&lt;3)]
    obj.path &lt;- SOS(x, y)
    
    </code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
