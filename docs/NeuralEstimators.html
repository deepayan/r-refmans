<!DOCTYPE html><html lang="en"><head><title>Help for package NeuralEstimators</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {NeuralEstimators}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#NeuralEstimators-package'><p>NeuralEstimators: Likelihood-Free Parameter Estimation using Neural Networks</p></a></li>
<li><a href='#assess'><p>assess a neural estimator</p></a></li>
<li><a href='#bias'><p>computes a Monte Carlo approximation of an estimator's bias</p></a></li>
<li><a href='#bootstrap'><p>bootstrap</p></a></li>
<li><a href='#encodedata'><p>encodedata</p></a></li>
<li><a href='#estimate'><p>estimate</p></a></li>
<li><a href='#loadstate'><p>Load a saved state of a neural estimator</p></a></li>
<li><a href='#plotdistribution'><p>Plot the empirical sampling distribution of an estimator.</p></a></li>
<li><a href='#plotestimates'><p>Plot estimates vs. true values.</p></a></li>
<li><a href='#posteriormode'><p>posteriormode</p></a></li>
<li><a href='#risk'><p>computes a Monte Carlo approximation of an estimator's Bayes risk</p></a></li>
<li><a href='#rmse'><p>computes a Monte Carlo approximation of an estimator's root-mean-square error (RMSE)</p></a></li>
<li><a href='#sampleposterior'><p>sampleposterior</p></a></li>
<li><a href='#savestate'><p>save the state of a neural estimator</p></a></li>
<li><a href='#spatialgraph'><p>spatialgraph</p></a></li>
<li><a href='#tanhloss'><p>tanhloss</p></a></li>
<li><a href='#train'><p>Train a neural estimator</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>Likelihood-Free Parameter Estimation using Neural Networks</td>
</tr>
<tr>
<td>Version:</td>
<td>0.2.0</td>
</tr>
<tr>
<td>Description:</td>
<td>An 'R' interface to the 'Julia' package 'NeuralEstimators.jl'. The package facilitates the user-friendly development of neural Bayes estimators, which are neural networks that map data to a point summary of the posterior distribution (Sainsbury-Dale et al., 2024, &lt;<a href="https://doi.org/10.1080%2F00031305.2023.2249522">doi:10.1080/00031305.2023.2249522</a>&gt;). These estimators are likelihood-free and amortised, in the sense that, once the neural networks are trained on simulated data, inference from observed data can be made in a fraction of the time required by conventional approaches. The package also supports amortised Bayesian or frequentist inference using neural networks that approximate the posterior or likelihood-to-evidence ratio (Zammit-Mangion et al., 2025, Sec. 3.2, 5.2, &lt;<a href="https://doi.org/10.48550%2FarXiv.2404.12484">doi:10.48550/arXiv.2404.12484</a>&gt;). The package accommodates any model for which simulation is feasible by allowing users to define models implicitly through simulated data.</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Matthew Sainsbury-Dale &lt;msainsburydale@gmail.com&gt;</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.2</td>
</tr>
<tr>
<td>Imports:</td>
<td>JuliaConnectoR, magrittr</td>
</tr>
<tr>
<td>Suggests:</td>
<td>dplyr, ggplot2, ggplotify, ggpubr, gridExtra, knitr,
rmarkdown, markdown, R.rsp, testthat (&ge; 3.0.0)</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>Julia (&gt;= 1.11)</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>R.rsp</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/msainsburydale/NeuralEstimators">https://github.com/msainsburydale/NeuralEstimators</a>,
<a href="https://msainsburydale.github.io/NeuralEstimators.jl/dev/">https://msainsburydale.github.io/NeuralEstimators.jl/dev/</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2025-03-02 07:27:42 UTC; sainsbmd</td>
</tr>
<tr>
<td>Author:</td>
<td>Matthew Sainsbury-Dale [aut, cre]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2025-03-02 11:10:06 UTC</td>
</tr>
</table>
<hr>
<h2 id='NeuralEstimators-package'>NeuralEstimators: Likelihood-Free Parameter Estimation using Neural Networks</h2><span id='topic+NeuralEstimators'></span><span id='topic+NeuralEstimators-package'></span>

<h3>Description</h3>

<p>An 'R' interface to the 'Julia' package 'NeuralEstimators.jl'. The package facilitates the user-friendly development of neural Bayes estimators, which are neural networks that map data to a point summary of the posterior distribution (Sainsbury-Dale et al., 2024, <a href="https://doi.org/10.1080/00031305.2023.2249522">doi:10.1080/00031305.2023.2249522</a>). These estimators are likelihood-free and amortised, in the sense that, once the neural networks are trained on simulated data, inference from observed data can be made in a fraction of the time required by conventional approaches. The package also supports amortised Bayesian or frequentist inference using neural networks that approximate the posterior or likelihood-to-evidence ratio (Zammit-Mangion et al., 2025, Sec. 3.2, 5.2, <a href="https://doi.org/10.48550/arXiv.2404.12484">doi:10.48550/arXiv.2404.12484</a>). The package accommodates any model for which simulation is feasible by allowing users to define models implicitly through simulated data.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Matthew Sainsbury-Dale <a href="mailto:msainsburydale@gmail.com">msainsburydale@gmail.com</a>
</p>


<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://github.com/msainsburydale/NeuralEstimators">https://github.com/msainsburydale/NeuralEstimators</a>
</p>
</li>
<li> <p><a href="https://msainsburydale.github.io/NeuralEstimators.jl/dev/">https://msainsburydale.github.io/NeuralEstimators.jl/dev/</a>
</p>
</li></ul>


<hr>
<h2 id='assess'>assess a neural estimator</h2><span id='topic+assess'></span>

<h3>Description</h3>

<p>assess a neural estimator
</p>


<h3>Usage</h3>

<pre><code class='language-R'>assess(
  estimators,
  parameters,
  Z,
  estimator_names = NULL,
  parameter_names = NULL,
  use_gpu = TRUE,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="assess_+3A_estimators">estimators</code></td>
<td>
<p>a neural estimator (or a list of neural estimators)</p>
</td></tr>
<tr><td><code id="assess_+3A_parameters">parameters</code></td>
<td>
<p>true parameters, stored as a <code class="reqn">d\times K</code> matrix, where <code class="reqn">d</code> is the dimension of the parameter vector and <code class="reqn">K</code> is the number of sampled parameter vectors</p>
</td></tr>
<tr><td><code id="assess_+3A_z">Z</code></td>
<td>
<p>data simulated conditionally on the <code>parameters</code>. If <code>length(Z)</code> &gt; K, the parameter matrix will be recycled by horizontal concatenation as <code>parameters = parameters[, rep(1:K, J)]</code>, where <code>J = length(Z) / K</code></p>
</td></tr>
<tr><td><code id="assess_+3A_estimator_names">estimator_names</code></td>
<td>
<p>list of names of the estimators (sensible defaults provided)</p>
</td></tr>
<tr><td><code id="assess_+3A_parameter_names">parameter_names</code></td>
<td>
<p>list of names of the parameters (sensible defaults provided)</p>
</td></tr>
<tr><td><code id="assess_+3A_use_gpu">use_gpu</code></td>
<td>
<p>a boolean indicating whether to use the GPU if it is available (default true)</p>
</td></tr>
<tr><td><code id="assess_+3A_verbose">verbose</code></td>
<td>
<p>a boolean indicating whether information should be printed to the console</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list of two data frames: <code>runtimes</code> contains the
total time taken for each estimator, while <code>df</code> is a long-form
data frame with columns:
</p>

<ul>
<li><p>&quot;estimator&quot;; the name of the estimator
</p>
</li>
<li><p>&quot;parameter&quot;; the name of the parameter
</p>
</li>
<li><p>&quot;truth&quot;; the true value of the parameter
</p>
</li>
<li><p>&quot;estimate&quot;; the estimated value of the parameter
</p>
</li>
<li><p>&quot;m&quot;; the sample size (number of iid replicates)
</p>
</li>
<li><p>&quot;k&quot;; the index of the parameter vector in the test set
</p>
</li>
<li><p>&quot;j&quot;; the index of the data set
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+risk">risk()</a></code>, <code><a href="#topic+rmse">rmse()</a></code>, <code><a href="#topic+bias">bias()</a></code>, <code><a href="#topic+plotestimates">plotestimates()</a></code>, and <code><a href="#topic+plotdistribution">plotdistribution()</a></code> for computing various empirical diagnostics and visualisations from an object returned by <code>assess()</code>
</p>

<hr>
<h2 id='bias'>computes a Monte Carlo approximation of an estimator's bias</h2><span id='topic+bias'></span>

<h3>Description</h3>

<p>computes a Monte Carlo approximation of an estimator's bias
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bias(assessment, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="bias_+3A_assessment">assessment</code></td>
<td>
<p>an object returned by <code><a href="#topic+assess">assess()</a></code></p>
</td></tr>
<tr><td><code id="bias_+3A_...">...</code></td>
<td>
<p>optional arguments inherited from <code>risk()</code> (excluding the argument <code>loss</code>)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a dataframe giving the estimated bias
</p>


<h3>See Also</h3>

<p><code><a href="#topic+assess">assess()</a></code>, <code><a href="#topic+risk">risk()</a></code>, <code><a href="#topic+rmse">rmse()</a></code>
</p>

<hr>
<h2 id='bootstrap'>bootstrap</h2><span id='topic+bootstrap'></span>

<h3>Description</h3>

<p>Compute bootstrap estimates from a neural point estimator
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bootstrap(estimator, Z, B = 400, blocks = NULL, use_gpu = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="bootstrap_+3A_estimator">estimator</code></td>
<td>
<p>a neural point estimator</p>
</td></tr>
<tr><td><code id="bootstrap_+3A_z">Z</code></td>
<td>
<p>either a list of data sets simulated conditionally on the fitted parameters (parametric bootstrap); or a single observed data set containing independent replicates, which will be sampled with replacement <code>B</code> times (non-parametric bootstrap)</p>
</td></tr>
<tr><td><code id="bootstrap_+3A_b">B</code></td>
<td>
<p>number of non-parametric bootstrap samples</p>
</td></tr>
<tr><td><code id="bootstrap_+3A_blocks">blocks</code></td>
<td>
<p>integer vector specifying the blocks in non-parameteric bootstrap. For example, with 5 replicates, the first two corresponding to block 1 and the remaining three corresponding to block 2, <code>blocks</code> should be <code>c(1,1,2,2,2)</code></p>
</td></tr>
<tr><td><code id="bootstrap_+3A_use_gpu">use_gpu</code></td>
<td>
<p>boolean indicating whether to use the GPU if it is available</p>
</td></tr>
</table>


<h3>Value</h3>

<p>d × <code>B</code> matrix, where d is the dimension of the parameter vector
</p>

<hr>
<h2 id='encodedata'>encodedata</h2><span id='topic+encodedata'></span>

<h3>Description</h3>

<p>For data <code>Z</code> with missing (<code>NA</code>) entries, computes an augmented data set (U, W) where W encodes the missingness pattern as an indicator vector and U is the original data Z with missing entries replaced by a fixed constant <code>c</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>encodedata(Z, c = 0)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="encodedata_+3A_z">Z</code></td>
<td>
<p>data containing <code>NA</code> entries</p>
</td></tr>
<tr><td><code id="encodedata_+3A_c">c</code></td>
<td>
<p>fixed constant with which to replace <code>NA</code> entries</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Augmented data set (U, W). If <code>Z</code> is provided as a list, the return type will be a <code>JuliaProxy</code> object; these objects can be indexed in the usual manner using <code>[[</code>, or converted to an R object using <code>juliaGet()</code> (note however that <code>juliaGet()</code> can be slow for large data sets).
</p>


<h3>See Also</h3>

<p>the Julia version of <a href="https://msainsburydale.github.io/NeuralEstimators.jl/dev/API/utility/#NeuralEstimators.encodedata"><code>encodedata()</code></a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library("NeuralEstimators")
Z &lt;- matrix(c(1, 2, NA, NA, 5, 6, 7, NA, 9), nrow = 3)
encodedata(Z)
encodedata(list(Z, Z))
## End(Not run)
</code></pre>

<hr>
<h2 id='estimate'>estimate</h2><span id='topic+estimate'></span>

<h3>Description</h3>

<p>Apply a neural estimator to data
</p>


<h3>Usage</h3>

<pre><code class='language-R'>estimate(estimator, Z, X = NULL, batchsize = 32, use_gpu = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="estimate_+3A_estimator">estimator</code></td>
<td>
<p>a neural estimator that can be applied to data in a call of the form <code>estimator(Z)</code></p>
</td></tr>
<tr><td><code id="estimate_+3A_z">Z</code></td>
<td>
<p>data in a format amenable to the neural-network architecture of <code>estimator</code></p>
</td></tr>
<tr><td><code id="estimate_+3A_x">X</code></td>
<td>
<p>additional inputs to the neural network; if provided, the call will be of the form <code style="white-space: pre;">&#8288;estimator((Z, X))&#8288;</code></p>
</td></tr>
<tr><td><code id="estimate_+3A_batchsize">batchsize</code></td>
<td>
<p>the batch size for applying <code>estimator</code> to <code>Z</code>. Batching occurs only if <code>Z</code> is a list, indicating multiple data sets</p>
</td></tr>
<tr><td><code id="estimate_+3A_use_gpu">use_gpu</code></td>
<td>
<p>boolean indicating whether to use the GPU if it is available</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a matrix of outputs resulting from applying <code>estimator</code> to <code>Z</code> (and possibly <code>X</code>)
</p>


<h3>See Also</h3>

<p><code><a href="#topic+sampleposterior">sampleposterior()</a></code> for making inference with neural posterior or likelihood-to-evidence-ratio estimators
</p>

<hr>
<h2 id='loadstate'>Load a saved state of a neural estimator</h2><span id='topic+loadstate'></span>

<h3>Description</h3>

<p>Load a saved state of a neural estimator (e.g., optimised neural-network parameters). Useful for amortised inference, whereby a neural network is trained once and then used repeatedly to make inference with new data sets.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>loadstate(estimator, filename)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="loadstate_+3A_estimator">estimator</code></td>
<td>
<p>the neural estimator that we wish to load the state into</p>
</td></tr>
<tr><td><code id="loadstate_+3A_filename">filename</code></td>
<td>
<p>file name (including path) of the neural-network state stored in a <code>bson</code> file</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>estimator</code> updated with the saved state
</p>

<hr>
<h2 id='plotdistribution'>Plot the empirical sampling distribution of an estimator.</h2><span id='topic+plotdistribution'></span>

<h3>Description</h3>

<p>Plot the empirical sampling distribution of an estimator.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotdistribution(
  df,
  type = c("box", "density", "scatter"),
  parameter_labels = NULL,
  estimator_labels = ggplot2::waiver(),
  truth_colour = "black",
  truth_size = 8,
  truth_line_size = NULL,
  pairs = FALSE,
  upper_triangle_plots = NULL,
  legend = TRUE,
  return_list = FALSE,
  flip = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plotdistribution_+3A_df">df</code></td>
<td>
<p>a long form data frame containing fields <code>estimator</code>, <code>parameter</code>, <code>estimate</code>, <code>truth</code>, and a column (e.g., <code>replicate</code>) to uniquely identify each observation.</p>
</td></tr>
<tr><td><code id="plotdistribution_+3A_type">type</code></td>
<td>
<p>string indicating whether to plot kernel density estimates for each individual parameter (<code>type = "density"</code>) or scatter plots for all parameter pairs (<code>type = "scatter"</code>).</p>
</td></tr>
<tr><td><code id="plotdistribution_+3A_parameter_labels">parameter_labels</code></td>
<td>
<p>a named vector containing parameter labels.</p>
</td></tr>
<tr><td><code id="plotdistribution_+3A_estimator_labels">estimator_labels</code></td>
<td>
<p>a named vector containing estimator labels.</p>
</td></tr>
<tr><td><code id="plotdistribution_+3A_truth_colour">truth_colour</code></td>
<td>
<p>the colour used to denote the true parameter value.</p>
</td></tr>
<tr><td><code id="plotdistribution_+3A_truth_size">truth_size</code></td>
<td>
<p>the size of the point used to denote the true parameter value (applicable only for <code>type = "scatter"</code>).</p>
</td></tr>
<tr><td><code id="plotdistribution_+3A_truth_line_size">truth_line_size</code></td>
<td>
<p>the size of the cross-hairs used to denote the true parameter value. If <code>NULL</code> (default), the cross-hairs are not plotted. (applicable only for <code>type = "scatter"</code>).</p>
</td></tr>
<tr><td><code id="plotdistribution_+3A_pairs">pairs</code></td>
<td>
<p>logical; should we combine the scatter plots into a single pairs plot (applicable only for <code>type = "scatter"</code>)?</p>
</td></tr>
<tr><td><code id="plotdistribution_+3A_upper_triangle_plots">upper_triangle_plots</code></td>
<td>
<p>an optional list of plots to include in the uppertriangle of the pairs plot.</p>
</td></tr>
<tr><td><code id="plotdistribution_+3A_legend">legend</code></td>
<td>
<p>Flag; should we include the legend (only applies when constructing a pairs plot)</p>
</td></tr>
<tr><td><code id="plotdistribution_+3A_return_list">return_list</code></td>
<td>
<p>Flag; should the parameters be split into a list?</p>
</td></tr>
<tr><td><code id="plotdistribution_+3A_flip">flip</code></td>
<td>
<p>Flag; should the boxplots be &quot;flipped&quot; using <code>coord_flip()</code> (default <code>FALSE</code>)?</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list of <code>'ggplot'</code> objects or, if <code>pairs = TRUE</code>, a single <code>'ggplot'</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# In the following, we have two estimators and, for each parameter, 50 estimates
# from each estimator.

estimators &lt;- c("Estimator 1", "Estimator 2")
estimator_labels &lt;- c("Estimator 1" = expression(hat(theta)[1]("·")),
                      "Estimator 2" = expression(hat(theta)[2]("·")))

# Single parameter:
df &lt;- data.frame(
  estimator = estimators, truth = 0, parameter = "mu",
  estimate  = rnorm(2*50),
  replicate = rep(1:50, each = 2)
)
parameter_labels &lt;- c("mu" = expression(mu))
plotdistribution(df)
plotdistribution(df, type = "density")
plotdistribution(df, parameter_labels = parameter_labels, estimator_labels = estimator_labels)

# Two parameters:
df &lt;- rbind(df, data.frame(
  estimator = estimators, truth = 1, parameter = "sigma",
  estimate  = rgamma(2*50, shape = 1, rate = 1),
  replicate = rep(1:50, each = 2)
))
parameter_labels &lt;- c(parameter_labels, "sigma" = expression(sigma))
plotdistribution(df, parameter_labels = parameter_labels)
plotdistribution(df, parameter_labels = parameter_labels, type = "density")
plotdistribution(df, parameter_labels = parameter_labels, type = "scatter")

# Three parameters:
df &lt;- rbind(df, data.frame(
  estimator = estimators, truth = 0.25, parameter = "alpha",
  estimate  = 0.5 * runif(2*50),
  replicate = rep(1:50, each = 2)
))
parameter_labels &lt;- c(parameter_labels, "alpha" = expression(alpha))
plotdistribution(df, parameter_labels = parameter_labels)
plotdistribution(df, parameter_labels = parameter_labels, type = "density")
plotdistribution(df, parameter_labels = parameter_labels, type = "scatter")
plotdistribution(df, parameter_labels = parameter_labels, type = "scatter", pairs = TRUE)

# Pairs plot with user-specified plots in the upper triangle:
upper_triangle_plots &lt;- lapply(1:3, function(i) {
  x = rnorm(10)
  y = rnorm(10)
  shape = sample(c("Class 1", "Class 2"), 10, replace = TRUE)
  ggplot() +
    geom_point(aes(x = x, y = y, shape = shape)) + 
    labs(shape = "") +
    theme_bw()
})
plotdistribution(
    df, 
    parameter_labels = parameter_labels, estimator_labels = estimator_labels,
    type = "scatter", pairs = TRUE, upper_triangle_plots = upper_triangle_plots
    )
## End(Not run)
</code></pre>

<hr>
<h2 id='plotestimates'>Plot estimates vs. true values.</h2><span id='topic+plotestimates'></span>

<h3>Description</h3>

<p>Plot estimates vs. true values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotestimates(
  df,
  estimator_labels = ggplot2::waiver(),
  parameter_labels = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plotestimates_+3A_df">df</code></td>
<td>
<p>a long form data frame containing fields <code>estimator</code>, <code>parameter</code>, <code>estimate</code>, and <code>truth</code>.</p>
</td></tr>
<tr><td><code id="plotestimates_+3A_estimator_labels">estimator_labels</code></td>
<td>
<p>a named vector containing estimator labels.</p>
</td></tr>
<tr><td><code id="plotestimates_+3A_parameter_labels">parameter_labels</code></td>
<td>
<p>a named vector containing parameter labels.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>'ggplot'</code> of the estimates for each parameter against the true value.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
K &lt;- 50
df &lt;- data.frame(
  estimator = c("Estimator 1", "Estimator 2"), 
  parameter = rep(c("mu", "sigma"), each = K),
  truth = 1:(2*K), 
  estimate = 1:(2*K) + rnorm(4*K)
)
estimator_labels &lt;- c("Estimator 1" = expression(hat(theta)[1]("·")),
                      "Estimator 2" = expression(hat(theta)[2]("·")))
parameter_labels &lt;- c("mu" = expression(mu), "sigma" = expression(sigma))

plotestimates(df,  parameter_labels = parameter_labels, estimator_labels)
## End(Not run)
</code></pre>

<hr>
<h2 id='posteriormode'>posteriormode</h2><span id='topic+posteriormode'></span>

<h3>Description</h3>

<p>Computes the (approximate) posterior mode (maximum a posteriori estimate) given data <code>Z</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>posteriormode(estimator, Z, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="posteriormode_+3A_estimator">estimator</code></td>
<td>
<p>a neural posterior or likelihood-to-evidence-ratio estimator</p>
</td></tr>
<tr><td><code id="posteriormode_+3A_z">Z</code></td>
<td>
<p>data in a format amenable to the neural-network architecture of <code>estimator</code></p>
</td></tr>
<tr><td><code id="posteriormode_+3A_...">...</code></td>
<td>
<p>additional keyword arguments passed to the Julia version of <a href="https://msainsburydale.github.io/NeuralEstimators.jl/dev/API/core/#NeuralEstimators.posteriormode"><code>posteriormode()</code></a></p>
</td></tr>
</table>


<h3>Value</h3>

<p>a d × K matrix of posterior samples, where d is the dimension of the parameter vector and K is the number of data sets provided in <code>Z</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+sampleposterior">sampleposterior()</a></code> for sampling from the approximate posterior distribution
</p>

<hr>
<h2 id='risk'>computes a Monte Carlo approximation of an estimator's Bayes risk</h2><span id='topic+risk'></span>

<h3>Description</h3>

<p>computes a Monte Carlo approximation of an estimator's Bayes risk
</p>


<h3>Usage</h3>

<pre><code class='language-R'>risk(
  assessment,
  loss = function(x, y) abs(x - y),
  average_over_parameters = FALSE,
  average_over_sample_sizes = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="risk_+3A_assessment">assessment</code></td>
<td>
<p>an object returned by <code><a href="#topic+assess">assess()</a></code></p>
</td></tr>
<tr><td><code id="risk_+3A_loss">loss</code></td>
<td>
<p>a binary operator defining the loss function (default absolute-error loss)</p>
</td></tr>
<tr><td><code id="risk_+3A_average_over_parameters">average_over_parameters</code></td>
<td>
<p>if <code>TRUE</code>, the loss is averaged over all parameters; otherwise (default), the loss is averaged over each parameter separately</p>
</td></tr>
<tr><td><code id="risk_+3A_average_over_sample_sizes">average_over_sample_sizes</code></td>
<td>
<p>if <code>TRUE</code> (default), the loss is averaged over all sample sizes (the column <code>m</code> in <code>df</code>); otherwise, the loss is averaged over each sample size separately</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a dataframe giving an estimate of the Bayes risk
</p>


<h3>See Also</h3>

<p><code><a href="#topic+assess">assess()</a></code>, <code><a href="#topic+bias">bias()</a></code>, <code><a href="#topic+rmse">rmse()</a></code>
</p>

<hr>
<h2 id='rmse'>computes a Monte Carlo approximation of an estimator's root-mean-square error (RMSE)</h2><span id='topic+rmse'></span>

<h3>Description</h3>

<p>computes a Monte Carlo approximation of an estimator's root-mean-square error (RMSE)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rmse(assessment, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="rmse_+3A_assessment">assessment</code></td>
<td>
<p>an object returned by <code><a href="#topic+assess">assess()</a></code></p>
</td></tr>
<tr><td><code id="rmse_+3A_...">...</code></td>
<td>
<p>optional arguments inherited from <code>risk()</code> (excluding the argument <code>loss</code>)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a dataframe giving the estimated RMSE
</p>


<h3>See Also</h3>

<p><code><a href="#topic+assess">assess()</a></code>, <code><a href="#topic+bias">bias()</a></code>, <code><a href="#topic+risk">risk()</a></code>
</p>

<hr>
<h2 id='sampleposterior'>sampleposterior</h2><span id='topic+sampleposterior'></span>

<h3>Description</h3>

<p>Samples from the approximate posterior distribution given data <code>Z</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sampleposterior(estimator, Z, N = 1000, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sampleposterior_+3A_estimator">estimator</code></td>
<td>
<p>a neural posterior or likelihood-to-evidence-ratio estimator</p>
</td></tr>
<tr><td><code id="sampleposterior_+3A_z">Z</code></td>
<td>
<p>data in a format amenable to the neural-network architecture of <code>estimator</code></p>
</td></tr>
<tr><td><code id="sampleposterior_+3A_n">N</code></td>
<td>
<p>number of approximate posterior samples to draw</p>
</td></tr>
<tr><td><code id="sampleposterior_+3A_...">...</code></td>
<td>
<p>additional keyword arguments passed to the Julia version of <a href="https://msainsburydale.github.io/NeuralEstimators.jl/dev/API/core/#NeuralEstimators.sampleposterior"><code>sampleposterior()</code></a>, applicable when <code>estimator</code> is a likelihood-to-evidence-ratio estimator</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a d × <code>N</code> matrix of posterior samples, where d is the dimension of the parameter vector. If <code>Z</code> is a list containing multiple data sets, a list of matrices will be returned
</p>


<h3>See Also</h3>

<p><code><a href="#topic+estimate">estimate()</a></code> for making inference with neural Bayes estimators
</p>

<hr>
<h2 id='savestate'>save the state of a neural estimator</h2><span id='topic+savestate'></span>

<h3>Description</h3>

<p>save the state of a neural estimator
</p>


<h3>Usage</h3>

<pre><code class='language-R'>savestate(estimator, filename)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="savestate_+3A_estimator">estimator</code></td>
<td>
<p>the neural estimator that we wish to save</p>
</td></tr>
<tr><td><code id="savestate_+3A_filename">filename</code></td>
<td>
<p>file in which to save the neural-network state as a <code>bson</code> file</p>
</td></tr>
</table>


<h3>Value</h3>

<p>No return value, called for side effects
</p>

<hr>
<h2 id='spatialgraph'>spatialgraph</h2><span id='topic+spatialgraph'></span>

<h3>Description</h3>

<p>Constructs a graph object for use in a graph neural network (GNN).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spatialgraph(S, Z, isotropic = TRUE, stationary = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spatialgraph_+3A_s">S</code></td>
<td>
<p>Spatial locations, provided as:
</p>

<ul>
<li><p> An <code class="reqn">n\times 2</code> matrix when locations are fixed across replicates, where <code class="reqn">n</code> is the number of spatial locations.
</p>
</li>
<li><p> A list of <code class="reqn">n_i \times 2</code> matrices when locations vary across replicates.
</p>
</li>
<li><p> A list of the above elements (i.e., a list of matrices or a list of lists of matrices) when constructing graphs from multiple data sets.
</p>
</li></ul>
</td></tr>
<tr><td><code id="spatialgraph_+3A_z">Z</code></td>
<td>
<p>Spatial data, provided as:
</p>

<ul>
<li><p> An <code class="reqn">n\times m</code> matrix when locations are fixed, where <code class="reqn">m</code> is the number of replicates.
</p>
</li>
<li><p> A list of <code class="reqn">n_i</code>-vectors when locations vary across replicates.
</p>
</li>
<li><p> A list of the above elements (i.e., a list of matrices or a list of lists of vectors) when constructing graphs from multiple data sets.
</p>
</li></ul>
</td></tr>
<tr><td><code id="spatialgraph_+3A_isotropic">isotropic</code></td>
<td>
<p>Logical. If <code>TRUE</code>, edge features store the spatial distance (magnitude)
between nodes. If <code>FALSE</code>, the spatial displacement or spatial location is stored, depending
on the value of <code>stationary</code>.</p>
</td></tr>
<tr><td><code id="spatialgraph_+3A_stationary">stationary</code></td>
<td>
<p>Logical. If <code>TRUE</code>, edge features store the spatial displacement
(vector difference) between nodes, capturing both magnitude and direction. If <code>FALSE</code>,
edge features include the full spatial locations of both nodes.</p>
</td></tr>
<tr><td><code id="spatialgraph_+3A_...">...</code></td>
<td>
<p>Additional keyword arguments from the Julia function <a href="https://msainsburydale.github.io/NeuralEstimators.jl/dev/API/utility/#NeuralEstimators.adjacencymatrix"><code>adjacencymatrix()</code></a>
that define the neighborhood of each node, with the default being a randomly
selected set of <code>k=30</code> neighbors within a radius of <code>r=0.15</code> spatial distance units.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>GNNGraph</code> (<code>JuliaProxy</code> object) or, if multiple data sets are provided, a vector of <code>GNNGraph</code> objects which can be indexed in the usual manner using <code>[[</code> or converted to an R list using a combination of indexing and <code>lapply</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library("NeuralEstimators")

# Number of replicates
m &lt;- 5

# Spatial locations fixed for all replicates
n &lt;- 100 
S &lt;- matrix(runif(n * 2), n, 2)
Z &lt;- matrix(runif(n * m), n, m)
g &lt;- spatialgraph(S, Z)

# Spatial locations varying between replicates
n &lt;- sample(50:100, m, replace = TRUE)
S &lt;- lapply(n, function(ni) matrix(runif(ni * 2), ni, 2))
Z &lt;- lapply(n, function(ni) runif(ni))
g &lt;- spatialgraph(S, Z)

# Multiple data sets: Spatial locations fixed for all replicates within a given data set
K &lt;- 15 # number of data sets
n &lt;- sample(50:100, K, replace = TRUE) # number of spatial locations can vary between data sets
S &lt;- lapply(1:K, function(k) matrix(runif(n[k] * 2), n[k], 2))
Z &lt;- lapply(1:K, function(k) matrix(runif(n[k] * m), n[k], m))
g &lt;- spatialgraph(S, Z)

# Multiple data sets: Spatial locations varying between replicates within a given data set
S &lt;- lapply(1:K, function(k) {
  lapply(1:m, function(i) {
  ni &lt;- sample(50:100, 1)       # randomly generate the number of locations for each replicate
  matrix(runif(ni * 2), ni, 2)  # generate the spatial locations
  })
})
Z &lt;- lapply(1:K, function(k) {
  lapply(1:m, function(i) {
    n &lt;- nrow(S[[k]][[i]])
    runif(n)  
  })
})
g &lt;- spatialgraph(S, Z)

## End(Not run)
</code></pre>

<hr>
<h2 id='tanhloss'>tanhloss</h2><span id='topic+tanhloss'></span>

<h3>Description</h3>

<p>For <code>k</code> &gt; 0, defines Julia code that defines the loss function,
</p>
<p style="text-align: center;"><code class="reqn">L(\hat{\theta}, \theta) = \tanh\left(\frac{|\hat{\theta} - \theta|}{k}\right),</code>
</p>

<p>which approximates the 0-1 loss as <code>k</code> tends to zero.
</p>
<p>The resulting string is intended to be used in the function <code><a href="#topic+train">train</a></code>, but can also be converted to a callable function using <code>juliaEval</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tanhloss(k)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tanhloss_+3A_k">k</code></td>
<td>
<p>Positive numeric value that controls the smoothness of the approximation.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>String defining the tanh loss function in Julia code.
</p>

<hr>
<h2 id='train'>Train a neural estimator</h2><span id='topic+train'></span>

<h3>Description</h3>

<p>The function caters for different variants of &quot;on-the-fly&quot; simulation.
Specifically, a <code>sampler</code> can be provided to continuously sample new
parameter vectors from the prior, and a <code>simulator</code> can be provided to
continuously simulate new data conditional on the parameters. If provided
with specific sets of parameters (<code>theta_train</code> and <code>theta_val</code>)
and/or data (<code>Z_train</code> and <code>Z_val</code>), they will be held fixed during
training.
</p>
<p>Note that using <code>R</code> functions to perform &quot;on-the-fly&quot; simulation requires the user to have installed the Julia package <code>RCall</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>train(
  estimator,
  sampler = NULL,
  simulator = NULL,
  theta_train = NULL,
  theta_val = NULL,
  Z_train = NULL,
  Z_val = NULL,
  m = NULL,
  M = NULL,
  K = 10000,
  xi = NULL,
  loss = "absolute-error",
  learning_rate = 1e-04,
  epochs = 100,
  batchsize = 32,
  savepath = NULL,
  stopping_epochs = 5,
  epochs_per_Z_refresh = 1,
  epochs_per_theta_refresh = 1,
  simulate_just_in_time = FALSE,
  use_gpu = TRUE,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="train_+3A_estimator">estimator</code></td>
<td>
<p>a neural estimator</p>
</td></tr>
<tr><td><code id="train_+3A_sampler">sampler</code></td>
<td>
<p>a function that takes an integer <code>K</code>, samples <code>K</code> parameter vectors from the prior, and returns them as a px<code>K</code> matrix</p>
</td></tr>
<tr><td><code id="train_+3A_simulator">simulator</code></td>
<td>
<p>a function that takes a px<code>K</code> matrix of parameters and an integer <code>m</code>, and returns <code>K</code> simulated data sets each containing <code>m</code> independent replicates</p>
</td></tr>
<tr><td><code id="train_+3A_theta_train">theta_train</code></td>
<td>
<p>a set of parameters used for updating the estimator using stochastic gradient descent</p>
</td></tr>
<tr><td><code id="train_+3A_theta_val">theta_val</code></td>
<td>
<p>a set of parameters used for monitoring the performance of the estimator during training</p>
</td></tr>
<tr><td><code id="train_+3A_z_train">Z_train</code></td>
<td>
<p>a simulated data set used for updating the estimator using stochastic gradient descent</p>
</td></tr>
<tr><td><code id="train_+3A_z_val">Z_val</code></td>
<td>
<p>a simulated data set used for monitoring the performance of the estimator during training</p>
</td></tr>
<tr><td><code id="train_+3A_m">m</code></td>
<td>
<p>vector of sample sizes. If <code>NULL</code> (default), a single neural estimator is trained, with the sample size inferred from <code>Z_val</code>. If <code>m</code> is a vector of integers, a sequence of neural estimators is constructed for each sample size; see the Julia documentation for <code>trainx()</code> for further details</p>
</td></tr>
<tr><td><code id="train_+3A_m">M</code></td>
<td>
<p>deprecated; use <code>m</code></p>
</td></tr>
<tr><td><code id="train_+3A_k">K</code></td>
<td>
<p>the number of parameter vectors sampled in the training set at each epoch; the size of the validation set is set to <code>K</code>/5.</p>
</td></tr>
<tr><td><code id="train_+3A_xi">xi</code></td>
<td>
<p>a list of objects used for data simulation (e.g., distance matrices); if it is provided, the parameter sampler is called as <code>sampler(K, xi)</code>.</p>
</td></tr>
<tr><td><code id="train_+3A_loss">loss</code></td>
<td>
<p>the loss function: a string ('absolute-error' for mean-absolute-error loss or 'squared-error' for mean-squared-error loss), or a string of Julia code defining the loss function. For some classes of estimators (e.g., <code>PosteriorEstimator</code>, <code>QuantileEstimator</code>, <code>RatioEstimator</code>), the loss function does not need to be specified.</p>
</td></tr>
<tr><td><code id="train_+3A_learning_rate">learning_rate</code></td>
<td>
<p>the learning rate for the optimiser ADAM (default 1e-3)</p>
</td></tr>
<tr><td><code id="train_+3A_epochs">epochs</code></td>
<td>
<p>the number of epochs to train the neural network. An epoch is one complete pass through the entire training data set when doing stochastic gradient descent.</p>
</td></tr>
<tr><td><code id="train_+3A_batchsize">batchsize</code></td>
<td>
<p>the batchsize to use when performing stochastic gradient descent, that is, the number of training samples processed between each update of the neural-network parameters.</p>
</td></tr>
<tr><td><code id="train_+3A_savepath">savepath</code></td>
<td>
<p>path to save the trained estimator and other information; if null (default), nothing is saved. Otherwise, the neural-network parameters (i.e., the weights and biases) will be saved during training as <code>bson</code> files; the risk function evaluated over the training and validation sets will also be saved, in the first and second columns of <code>loss_per_epoch.csv</code>, respectively; the best parameters (as measured by validation risk) will be saved as <code>best_network.bson</code>.</p>
</td></tr>
<tr><td><code id="train_+3A_stopping_epochs">stopping_epochs</code></td>
<td>
<p>cease training if the risk doesn't improve in this number of epochs (default 5).</p>
</td></tr>
<tr><td><code id="train_+3A_epochs_per_z_refresh">epochs_per_Z_refresh</code></td>
<td>
<p>integer indicating how often to refresh the training data</p>
</td></tr>
<tr><td><code id="train_+3A_epochs_per_theta_refresh">epochs_per_theta_refresh</code></td>
<td>
<p>integer indicating how often to refresh the training parameters; must be a multiple of <code>epochs_per_Z_refresh</code></p>
</td></tr>
<tr><td><code id="train_+3A_simulate_just_in_time">simulate_just_in_time</code></td>
<td>
<p>flag indicating whether we should simulate &quot;just-in-time&quot;, in the sense that only a <code>batchsize</code> number of parameter vectors and corresponding data are in memory at a given time</p>
</td></tr>
<tr><td><code id="train_+3A_use_gpu">use_gpu</code></td>
<td>
<p>a boolean indicating whether to use the GPU if one is available</p>
</td></tr>
<tr><td><code id="train_+3A_verbose">verbose</code></td>
<td>
<p>a boolean indicating whether information, including empirical risk values and timings, should be printed to the console during training.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a trained neural estimator or, if <code>m</code> is a vector, a list of trained neural estimators
</p>


<h3>See Also</h3>

<p><code><a href="#topic+assess">assess()</a></code> for assessing an estimator post training, and <code><a href="#topic+estimate">estimate()</a></code>/<code><a href="#topic+sampleposterior">sampleposterior()</a></code> for making inference with observed data
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Construct a neural Bayes estimator for replicated univariate Gaussian 
# data with unknown mean and standard deviation. 

# Load R and Julia packages
library("NeuralEstimators")
library("JuliaConnectoR")
juliaEval("using NeuralEstimators, Flux")

# Define the neural-network architecture
estimator &lt;- juliaEval('
 n = 1    # dimension of each replicate
 d = 2    # number of parameters in the model
 w = 32   # width of each hidden layer
 psi = Chain(Dense(n, w, relu), Dense(w, w, relu))
 phi = Chain(Dense(w, w, relu), Dense(w, d))
 deepset = DeepSet(psi, phi)
 estimator = PointEstimator(deepset)
')

# Sampler from the prior
sampler &lt;- function(K) {
  mu    &lt;- rnorm(K)      # Gaussian prior for the mean
  sigma &lt;- rgamma(K, 1)  # Gamma prior for the standard deviation
  theta &lt;- matrix(c(mu, sigma), byrow = TRUE, ncol = K)
  return(theta)
}

# Data simulator
simulator &lt;- function(theta_set, m) {
  apply(theta_set, 2, function(theta) {
    t(rnorm(m, theta[1], theta[2]))
  }, simplify = FALSE)
}

# Train using fixed parameter and data sets 
theta_train &lt;- sampler(10000)
theta_val   &lt;- sampler(2000)
m &lt;- 30 # number of iid replicates
Z_train &lt;- simulator(theta_train, m)
Z_val   &lt;- simulator(theta_val, m)
estimator &lt;- train(estimator, 
                   theta_train = theta_train, 
                   theta_val = theta_val, 
                   Z_train = Z_train, 
                   Z_val = Z_val)
                   
##### Simulation on-the-fly using R functions ####

juliaEval("using RCall") # requires the Julia package RCall
estimator &lt;- train(estimator, sampler = sampler, simulator = simulator, m = m)

##### Simulation on-the-fly using Julia functions ####

# Defining the sampler and simulator in Julia can improve computational 
# efficiency by avoiding the overhead of communicating between R and Julia. 

juliaEval("using Distributions")

# Parameter sampler
sampler &lt;- juliaEval("
      function sampler(K)
      	mu = rand(Normal(0, 1), K)
      	sigma = rand(Gamma(1), K)
      	theta = hcat(mu, sigma)'
      	return theta
      end")

# Data simulator
simulator &lt;- juliaEval("
      function simulator(theta_matrix, m)
      	Z = [rand(Normal(theta[1], theta[2]), 1, m) for theta in eachcol(theta_matrix)]
      	return Z
      end")

# Train
estimator &lt;- train(estimator, sampler = sampler, simulator = simulator, m = m)
## End(Not run)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
