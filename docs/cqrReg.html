<!DOCTYPE html><html><head><title>Help for package cqrReg</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {cqrReg}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#cqr.admm'><p>Composite Quantile regression (cqr) use Alternating Direction Method of Multipliers (ADMM) algorithm.</p></a></li>
<li><a href='#cqr.cd'><p>Composite Quantile Regression (cqr) use Coordinate Descent (cd) Algorithms</p></a></li>
<li><a href='#cqr.fit'><p>Composite Quantile Regression (cqr) model fitting</p></a></li>
<li><a href='#cqr.fit.lasso'><p>Composite Quantile Regression (cqr) with Adaptive Lasso Penalty (lasso)</p></a></li>
<li><a href='#cqr.ip'><p>Composite Quantile Regression (cqr) use Interior Point (ip) Method</p></a></li>
<li><a href='#cqr.lasso.admm'><p>Composite Quantile Regression (cqr) with Adaptive Lasso Penalty (lasso) use Alternating Direction Method of Multipliers (ADMM) algorithm</p></a></li>
<li><a href='#cqr.lasso.cd'><p>Composite Quantile Regression (cqr) with Adaptive Lasso Penalty (lasso) use Coordinate Descent (cd) Algorithms</p></a></li>
<li><a href='#cqr.lasso.mm'><p>Composite Quantile Regression (cqr) with Adaptive Lasso Penalty (lasso) use Majorize and Minimize (mm) Algorithm</p></a></li>
<li><a href='#cqr.mm'><p>Composite Quantile Regression (cqr) use Majorize and Minimize (mm) Algorithm</p></a></li>
<li><a href='#CQRADMMCPP'><p>Composite Quantile regression (cqr) use Alternating Direction Method of Multipliers (ADMM) algorithm core computational part</p></a></li>
<li><a href='#CQRCDCPP'><p>Composite Quantile Regression (cqr) use Coordinate Descent (cd) Algorithms core computational part</p></a></li>
<li><a href='#CQRMMCPP'><p>Composite Quantile Regression (cqr) use Majorize and Minimize (mm) Algorithm core computational part</p></a></li>
<li><a href='#CQRPADMMCPP'><p>Composite Quantile Regression (cqr) with Adaptive Lasso Penalty (lasso) use Alternating Direction Method of Multipliers (ADMM) algorithm core computational part</p></a></li>
<li><a href='#CQRPCDCPP'><p>Composite Quantile Regression (cqr) with Adaptive Lasso Penalty (lasso) use Coordinate Descent (cd) Algorithms core computational part</p></a></li>
<li><a href='#CQRPMMCPP'><p>Composite Quantile Regression (cqr) with Adaptive Lasso Penalty (lasso) use Majorize and Minimize (mm) Algorithm core computational part</p></a></li>
<li><a href='#QR.admm'><p>Quantile Regression (QR) use Alternating Direction Method of Multipliers (ADMM) algorithm</p></a></li>
<li><a href='#QR.cd'><p>Quantile Regression (QR) use Coordinate Descent (cd) Algorithms</p></a></li>
<li><a href='#QR.ip'><p>Quantile Regression (QR) use Interior Point (ip) Method</p></a></li>
<li><a href='#QR.lasso.admm'><p>Quantile Regression (QR) with Adaptive Lasso Penalty (lasso) use Alternating Direction</p>
Method of Multipliers (ADMM) algorithm</a></li>
<li><a href='#QR.lasso.cd'><p>Quantile Regression (QR) with Adaptive Lasso Penalty (lasso) use Coordinate Descent (cd)  Algorithms</p></a></li>
<li><a href='#QR.lasso.ip'><p>Quantile Regression (QR) with Adaptive Lasso Penalty (lasso) use Interior Point (ip) Method</p></a></li>
<li><a href='#QR.lasso.mm'><p>Quantile Regression (QR) with Adaptive Lasso Penalty (lasso) use Majorize and Minimize (mm) algorithm</p></a></li>
<li><a href='#QR.mm'><p>Quantile Regression (QR) use Majorize and Minimize (mm) algorithm</p></a></li>
<li><a href='#QRADMMCPP'><p>Quantile Regression (QR) use Alternating Direction Method of Multipliers (ADMM) algorithm core computational part</p></a></li>
<li><a href='#QRCDCPP'><p>Quantile Regression (QR) use Coordinate Descent (cd) Algorithms core computational part</p></a></li>
<li><a href='#qrfit'><p> Quantile Regression (qr) model fitting</p></a></li>
<li><a href='#qrfit.lasso'><p> Quantile Regression (qr) with Adaptive Lasso Penalty (lasso)</p></a></li>
<li><a href='#QRMMCPP'><p>Quantile Regression (QR) use Majorize and Minimize (mm) algorithm core computational part</p></a></li>
<li><a href='#QRPADMMCPP'><p>Quantile Regression (QR) with Adaptive Lasso Penalty (lasso) use Alternating Direction</p>
Method of Multipliers (ADMM) algorithm core computational part</a></li>
<li><a href='#QRPCDCPP'><p>Quantile Regression (QR) with Adaptive Lasso Penalty (lasso) use Coordinate Descent (cd)  Algorithms core computational part</p></a></li>
<li><a href='#QRPMMCPP'><p>Quantile Regression (QR) with Adaptive Lasso Penalty (lasso) use Majorize and Minimize (mm) algorithm core computational part</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Quantile, Composite Quantile Regression and Regularized Versions</td>
</tr>
<tr>
<td>Version:</td>
<td>1.2.1</td>
</tr>
<tr>
<td>Date:</td>
<td>2022-06-07</td>
</tr>
<tr>
<td>Author:</td>
<td>Jueyu Gao &amp; Linglong Kong </td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Jueyu Gao &lt;jueyu@ualberta.ca&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Estimate quantile regression(QR) and composite quantile regression (cqr) and with adaptive lasso penalty using interior point (IP), majorize and minimize(MM), coordinate descent (CD), and alternating direction method of multipliers algorithms(ADMM).</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Depends:</td>
<td>Rcpp (&ge; 0.10.0),quantreg,R (&ge; 2.6)</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp,RcppArmadillo</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-06-07 00:19:21 UTC; neoga</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-06-07 05:00:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='cqr.admm'>Composite Quantile regression (cqr) use Alternating Direction Method of Multipliers (ADMM) algorithm.</h2><span id='topic+cqr.admm'></span>

<h3>Description</h3>

<p>Composite quantile regression (cqr) find the estimated coefficient which minimize the absolute error for various quantile level.
The problem is well suited to distributed convex optimization and is based on Alternating Direction Method of Multipliers (ADMM) algorithm .
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cqr.admm(X,y,tau,rho,beta, maxit, toler)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cqr.admm_+3A_x">X</code></td>
<td>
<p>the design matrix</p>
</td></tr>
<tr><td><code id="cqr.admm_+3A_y">y</code></td>
<td>
<p>response variable</p>
</td></tr>
<tr><td><code id="cqr.admm_+3A_tau">tau</code></td>
<td>
<p>vector of quantile level</p>
</td></tr>
<tr><td><code id="cqr.admm_+3A_rho">rho</code></td>
<td>
<p>augmented Lagrangian parameter</p>
</td></tr>
<tr><td><code id="cqr.admm_+3A_beta">beta</code></td>
<td>
<p>initial value of estimate coefficient (default naive guess by least square estimation) </p>
</td></tr>
<tr><td><code id="cqr.admm_+3A_maxit">maxit</code></td>
<td>
<p>maxim iteration (default 200)</p>
</td></tr>
<tr><td><code id="cqr.admm_+3A_toler">toler</code></td>
<td>
<p>the tolerance critical for stop the algorithm (default 1e-3)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code><a href="base.html#topic+list">list</a></code> structure is with components
</p>
<table>
<tr><td><code>beta</code></td>
<td>
<p>the vector of estimated coefficient</p>
</td></tr>
<tr><td><code>b</code></td>
<td>
<p>intercept</p>
</td></tr>
</table>


<h3>Note</h3>

<p>cqr.admm(x,y,tau) work properly only if the least square estimation is good. 
</p>


<h3>References</h3>

<p>S. Boyd, N. Parikh, E. Chu, B. Peleato and J. Eckstein.(2010) Distributed Optimization and Statistical Learning via the Alternating Direction. Method of Multipliers <em>Foundations and Trends in Machine Learning</em>, <b>3</b>, No. 1, 1&ndash;122 
</p>
<p>Hui Zou and Ming Yuan(2008). Composite Quantile Regression and the Oracle Model Selection Theory, <em>The Annals of Statistics</em>, <b>36</b>, Number 3, Page 1108&ndash;1126.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
n=100
p=2
a=rnorm(n*p, mean = 1, sd =1)
x=matrix(a,n,p)
beta=rnorm(p,1,1)
beta=matrix(beta,p,1)
y=x%*%beta-matrix(rnorm(n,0.1,1),n,1)
tau=1:5/6
# x is 1000*10 matrix, y is 1000*1 vector, beta is 10*1 vector
cqr.admm(x,y,tau)
</code></pre>

<hr>
<h2 id='cqr.cd'>Composite Quantile Regression (cqr) use Coordinate Descent (cd) Algorithms</h2><span id='topic+cqr.cd'></span>

<h3>Description</h3>

<p>Composite quantile regression (cqr) find the estimated coefficient which minimize the absolute error for various quantile level.
The algorithm base on greedy coordinate descent and Edgeworth's for ordinary <code class="reqn">l_1</code> regression. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cqr.cd(X,y,tau,beta,maxit,toler)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cqr.cd_+3A_x">X</code></td>
<td>
<p>the design matrix</p>
</td></tr>
<tr><td><code id="cqr.cd_+3A_y">y</code></td>
<td>
<p>response variable</p>
</td></tr>
<tr><td><code id="cqr.cd_+3A_tau">tau</code></td>
<td>
<p>vector of quantile level</p>
</td></tr>
<tr><td><code id="cqr.cd_+3A_beta">beta</code></td>
<td>
<p>initial value of estimate coefficient (default naive guess by least square estimation) </p>
</td></tr>
<tr><td><code id="cqr.cd_+3A_maxit">maxit</code></td>
<td>
<p>maxim iteration (default 200)</p>
</td></tr>
<tr><td><code id="cqr.cd_+3A_toler">toler</code></td>
<td>
<p>the tolerance critical for stop the algorithm (default 1e-3)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code><a href="base.html#topic+list">list</a></code> structure is with components
</p>
<table>
<tr><td><code>beta</code></td>
<td>
<p>the vector of estimated coefficient</p>
</td></tr>
<tr><td><code>b</code></td>
<td>
<p>intercept</p>
</td></tr>
</table>


<h3>Note</h3>

<p>cqr.cd(x,y,tau) work properly only if the least square estimation is good. 
</p>


<h3>References</h3>

<p>Wu, T.T. and Lange, K. (2008). Coordinate Descent Algorithms for Lasso Penalized Regression. <em>Annals of Applied Statistics</em>, <b>2</b>, No 1, 224&ndash;244.
</p>
<p>Hui Zou and Ming Yuan(2008). Composite Quantile Regression and the Oracle Model Selection Theory, <em>The Annals of Statistics</em>, <b>36</b>, Number 3, Page 1108&ndash;1126.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
n=100
p=2
a=rnorm(n*p, mean = 1, sd =1)
x=matrix(a,n,p)
beta=rnorm(p,1,1)
beta=matrix(beta,p,1)
y=x%*%beta-matrix(rnorm(n,0.1,1),n,1)
tau=1:5/6
# x is 1000*10 matrix, y is 1000*1 vector, beta is 10*1 vector
cqr.cd(x,y,tau)
</code></pre>

<hr>
<h2 id='cqr.fit'>Composite Quantile Regression (cqr) model fitting</h2><span id='topic+cqr.fit'></span>

<h3>Description</h3>

<p>Composite quantile regression (cqr) find the estimated coefficient which minimize the absolute error for various quantile level.
High level function for estimating parameter by composite quantile regression.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cqr.fit(X,y,tau,beta,method,maxit,toler,rho)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cqr.fit_+3A_x">X</code></td>
<td>
<p>the design matrix</p>
</td></tr>
<tr><td><code id="cqr.fit_+3A_y">y</code></td>
<td>
<p>response variable</p>
</td></tr>
<tr><td><code id="cqr.fit_+3A_tau">tau</code></td>
<td>
<p>vector of quantile level</p>
</td></tr>
<tr><td><code id="cqr.fit_+3A_method">method</code></td>
<td>
<p>&quot;mm&quot; for majorize and minimize method,&quot;cd&quot; for coordinate descent method, &quot;admm&quot; for Alternating method of mulipliers method,&quot;ip&quot; for interior point mehod</p>
</td></tr>
<tr><td><code id="cqr.fit_+3A_rho">rho</code></td>
<td>
<p>augmented Lagrangian parameter</p>
</td></tr>
<tr><td><code id="cqr.fit_+3A_beta">beta</code></td>
<td>
<p>initial value of estimate coefficient (default naive guess by least square estimation) </p>
</td></tr>
<tr><td><code id="cqr.fit_+3A_maxit">maxit</code></td>
<td>
<p>maxim iteration (default 200)</p>
</td></tr>
<tr><td><code id="cqr.fit_+3A_toler">toler</code></td>
<td>
<p>the tolerance critical for stop the algorithm (default 1e-3)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code><a href="base.html#topic+list">list</a></code> structure is with components
</p>
<table>
<tr><td><code>beta</code></td>
<td>
<p>the vector of estimated coefficient</p>
</td></tr>
<tr><td><code>b</code></td>
<td>
<p>intercept</p>
</td></tr>
</table>


<h3>Note</h3>

<p>cqr.fit(x,y,tau) work properly only if the least square estimation is good. 
Interior point method is done by quantreg.
</p>

<hr>
<h2 id='cqr.fit.lasso'>Composite Quantile Regression (cqr) with Adaptive Lasso Penalty (lasso)</h2><span id='topic+cqr.fit.lasso'></span>

<h3>Description</h3>

<p>Composite quantile regression (cqr) find the estimated coefficient which minimize the absolute error for various quantile level.
High level function for estimating and selecting parameter by composite quantile regression with adaptive lasso penalty.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cqr.fit.lasso(X,y,tau,lambda,beta,method,maxit,toler,rho)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cqr.fit.lasso_+3A_x">X</code></td>
<td>
<p>the design matrix</p>
</td></tr>
<tr><td><code id="cqr.fit.lasso_+3A_y">y</code></td>
<td>
<p>response variable</p>
</td></tr>
<tr><td><code id="cqr.fit.lasso_+3A_tau">tau</code></td>
<td>
<p>vector of quantile level</p>
</td></tr>
<tr><td><code id="cqr.fit.lasso_+3A_method">method</code></td>
<td>
<p>&quot;mm&quot; for majorize and minimize method,&quot;cd&quot; for coordinate descent method, &quot;admm&quot; for Alternating method of mulipliers method</p>
</td></tr>
<tr><td><code id="cqr.fit.lasso_+3A_lambda">lambda</code></td>
<td>
<p>The constant coefficient of penalty function. (default lambda=1)</p>
</td></tr>
<tr><td><code id="cqr.fit.lasso_+3A_rho">rho</code></td>
<td>
<p>augmented Lagrangian parameter</p>
</td></tr>
<tr><td><code id="cqr.fit.lasso_+3A_beta">beta</code></td>
<td>
<p>initial value of estimate coefficient (default naive guess by least square estimation) </p>
</td></tr>
<tr><td><code id="cqr.fit.lasso_+3A_maxit">maxit</code></td>
<td>
<p>maxim iteration (default 200)</p>
</td></tr>
<tr><td><code id="cqr.fit.lasso_+3A_toler">toler</code></td>
<td>
<p>the tolerance critical for stop the algorithm (default 1e-3)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code><a href="base.html#topic+list">list</a></code> structure is with components
</p>
<table>
<tr><td><code>beta</code></td>
<td>
<p>the vector of estimated coefficient</p>
</td></tr>
<tr><td><code>b</code></td>
<td>
<p>intercept</p>
</td></tr>
</table>


<h3>Note</h3>

<p>cqr.fit.lasso(x,y,tau) work properly only if the least square estimation is good. 
</p>

<hr>
<h2 id='cqr.ip'>Composite Quantile Regression (cqr) use Interior Point (ip) Method</h2><span id='topic+cqr.ip'></span>

<h3>Description</h3>

<p>The function use the interior point method from quantreg to solve the quantile regression problem.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cqr.ip(X,y,tau)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cqr.ip_+3A_x">X</code></td>
<td>
<p>the design matrix</p>
</td></tr>
<tr><td><code id="cqr.ip_+3A_y">y</code></td>
<td>
<p>response variable</p>
</td></tr>
<tr><td><code id="cqr.ip_+3A_tau">tau</code></td>
<td>
<p>vector of quantile level</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code><a href="base.html#topic+list">list</a></code> structure is with components
</p>
<table>
<tr><td><code>beta</code></td>
<td>
<p>the vector of estimated coefficient</p>
</td></tr>
<tr><td><code>b</code></td>
<td>
<p>intercept</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Need to install quantreg package from CRAN. 
</p>


<h3>References</h3>

<p>Koenker, R. and S. Portnoy (1997).
The Gaussian Hare and the Laplacian Tortoise:
Computability of squared-error vs. absolute-error estimators, with discussion,
<em>Statistical Science</em>, <b>12</b>, 279-300.
</p>
<p>Hui Zou and Ming Yuan(2008). Composite Quantile Regression and the Oracle Model Selection Theory, <em>The Annals of Statistics</em>, <b>36</b>, Number 3, Page 1108&ndash;1126.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
n=100
p=2
a=rnorm(n*p, mean = 1, sd =1)
x=matrix(a,n,p)
beta=rnorm(p,1,1)
beta=matrix(beta,p,1)
y=x%*%beta-matrix(rnorm(n,0.1,1),n,1)
tau=1:5/6
# x is 1000*10 matrix, y is 1000*1 vector, beta is 10*1 vector
#you should install quantreg first to run following command
#cqr.ip(x,y,tau)
</code></pre>

<hr>
<h2 id='cqr.lasso.admm'>Composite Quantile Regression (cqr) with Adaptive Lasso Penalty (lasso) use Alternating Direction Method of Multipliers (ADMM) algorithm </h2><span id='topic+cqr.lasso.admm'></span>

<h3>Description</h3>

<p>The adaptive lasso parameter base on the estimated coefficient without penalty function.
Composite quantile regression find the estimated coefficient which minimize the absolute error for various quantile level.
The problem is well suited to distributed convex optimization and is based on Alternating Direction Method of Multipliers (ADMM) algorithm .
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cqr.lasso.admm(X,y,tau,lambda,rho,beta,maxit)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cqr.lasso.admm_+3A_x">X</code></td>
<td>
<p>the design matrix</p>
</td></tr>
<tr><td><code id="cqr.lasso.admm_+3A_y">y</code></td>
<td>
<p>response variable</p>
</td></tr>
<tr><td><code id="cqr.lasso.admm_+3A_tau">tau</code></td>
<td>
<p>vector of quantile level</p>
</td></tr>
<tr><td><code id="cqr.lasso.admm_+3A_lambda">lambda</code></td>
<td>
<p>The constant coefficient of penalty function. (default lambda=1)</p>
</td></tr>
<tr><td><code id="cqr.lasso.admm_+3A_rho">rho</code></td>
<td>
<p>augmented Lagrangian parameter</p>
</td></tr>
<tr><td><code id="cqr.lasso.admm_+3A_beta">beta</code></td>
<td>
<p>initial value of estimate coefficient (default naive guess by least square estimation) </p>
</td></tr>
<tr><td><code id="cqr.lasso.admm_+3A_maxit">maxit</code></td>
<td>
<p>maxim iteration (default 200)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code><a href="base.html#topic+list">list</a></code> structure is with components
</p>
<table>
<tr><td><code>beta</code></td>
<td>
<p>the vector of estimated coefficient</p>
</td></tr>
<tr><td><code>b</code></td>
<td>
<p>intercept</p>
</td></tr>
</table>


<h3>Note</h3>

<p>cqr.lasso.admm(x,y,tau) work properly only if the least square estimation is good. 
</p>


<h3>References</h3>

<p>S. Boyd, N. Parikh, E. Chu, B. Peleato and J. Eckstein.(2010) Distributed Optimization and Statistical Learning via the Alternating Direction. Method of Multipliers <em>Foundations and Trends in Machine Learning</em>, <b>3</b>, No. 1, 1&ndash;122 
</p>
<p>Hui Zou and Ming Yuan(2008). Composite Quantile Regression and the Oracle Model Selection Theory, <em>The Annals of Statistics</em>, <b>36</b>, Number 3, Page 1108&ndash;1126.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
n=100
p=2
a=2*rnorm(n*2*p, mean = 1, sd =1)
x=matrix(a,n,2*p)
beta=2*rnorm(p,1,1)
beta=rbind(matrix(beta,p,1),matrix(0,p,1))
y=x%*%beta-matrix(rnorm(n,0.1,1),n,1)
tau=1:5/6
# x is 1000*20 matrix, y is 1000*1 vector, beta is 20*1 vector with last ten zero value elements. 
cqr.lasso.admm(x,y,tau)
</code></pre>

<hr>
<h2 id='cqr.lasso.cd'>Composite Quantile Regression (cqr) with Adaptive Lasso Penalty (lasso) use Coordinate Descent (cd) Algorithms </h2><span id='topic+cqr.lasso.cd'></span>

<h3>Description</h3>

<p>The adaptive lasso parameter base on the estimated coefficient without penalty function.
Composite quantile regression find the estimated coefficient which minimize the absolute error for various quantile level.
The algorithm base on greedy coordinate descent and Edgeworth's for ordinary <code class="reqn">l_1</code> regression.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cqr.lasso.cd(X,y,tau,lambda,beta,maxit,toler)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cqr.lasso.cd_+3A_x">X</code></td>
<td>
<p>the design matrix</p>
</td></tr>
<tr><td><code id="cqr.lasso.cd_+3A_y">y</code></td>
<td>
<p>response variable</p>
</td></tr>
<tr><td><code id="cqr.lasso.cd_+3A_tau">tau</code></td>
<td>
<p>vector of quantile level</p>
</td></tr>
<tr><td><code id="cqr.lasso.cd_+3A_lambda">lambda</code></td>
<td>
<p>The constant coefficient of penalty function. (default lambda=1)</p>
</td></tr>
<tr><td><code id="cqr.lasso.cd_+3A_beta">beta</code></td>
<td>
<p>initial value of estimate coefficient (default naive guess by least square estimation) </p>
</td></tr>
<tr><td><code id="cqr.lasso.cd_+3A_maxit">maxit</code></td>
<td>
<p>maxim iteration (default 200)</p>
</td></tr>
<tr><td><code id="cqr.lasso.cd_+3A_toler">toler</code></td>
<td>
<p>the tolerance critical for stop the algorithm (default 1e-3)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code><a href="base.html#topic+list">list</a></code> structure is with components
</p>
<table>
<tr><td><code>beta</code></td>
<td>
<p>the vector of estimated coefficient</p>
</td></tr>
<tr><td><code>b</code></td>
<td>
<p>intercept</p>
</td></tr>
</table>


<h3>Note</h3>

<p>cqr.lasso.cd(x,y,tau) work properly only if the least square estimation is good. 
</p>


<h3>References</h3>

<p>Wu, T.T. and Lange, K. (2008). Coordinate Descent Algorithms for Lasso Penalized Regression. <em>Annals of Applied Statistics</em>, <b>2</b>, No 1, 224&ndash;244.
</p>
<p>Hui Zou and Ming Yuan(2008). Composite Quantile Regression and the Oracle Model Selection Theory, <em>The Annals of Statistics</em>, <b>36</b>, Number 3, Page 1108&ndash;1126.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
n=100
p=2
a=2*rnorm(n*2*p, mean = 1, sd =1)
x=matrix(a,n,2*p)
beta=2*rnorm(p,1,1)
beta=rbind(matrix(beta,p,1),matrix(0,p,1))
y=x%*%beta-matrix(rnorm(n,0.1,1),n,1)
tau=1:5/6
# x is 1000*20 matrix, y is 1000*1 vector, beta is 20*1 vector with last ten zero value elements. 
cqr.lasso.cd(x,y,tau)
</code></pre>

<hr>
<h2 id='cqr.lasso.mm'>Composite Quantile Regression (cqr) with Adaptive Lasso Penalty (lasso) use Majorize and Minimize (mm) Algorithm</h2><span id='topic+cqr.lasso.mm'></span>

<h3>Description</h3>

<p>The adaptive lasso penalty parameter base on the estimated coefficient without penalty function.
Composite quantile regression find the estimated coefficient which minimize the absolute error for various quantile level.
The algorithm majorizing the objective function by a quadratic function followed by minimizing that quadratic. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cqr.lasso.mm(X,y,tau,lambda,beta,maxit,toler)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cqr.lasso.mm_+3A_x">X</code></td>
<td>
<p>the design matrix</p>
</td></tr>
<tr><td><code id="cqr.lasso.mm_+3A_y">y</code></td>
<td>
<p>response variable</p>
</td></tr>
<tr><td><code id="cqr.lasso.mm_+3A_tau">tau</code></td>
<td>
<p>vector of quantile level</p>
</td></tr>
<tr><td><code id="cqr.lasso.mm_+3A_lambda">lambda</code></td>
<td>
<p>The constant coefficient of penalty function. (default lambda=1)</p>
</td></tr>
<tr><td><code id="cqr.lasso.mm_+3A_beta">beta</code></td>
<td>
<p>initial value of estimate coefficient (default naive guess by least square estimation) </p>
</td></tr>
<tr><td><code id="cqr.lasso.mm_+3A_maxit">maxit</code></td>
<td>
<p>maxim iteration (default 200)</p>
</td></tr>
<tr><td><code id="cqr.lasso.mm_+3A_toler">toler</code></td>
<td>
<p>the tolerance critical for stop the algorithm (default 1e-3)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code><a href="base.html#topic+list">list</a></code> structure is with components
</p>
<table>
<tr><td><code>beta</code></td>
<td>
<p>the vector of estimated coefficient</p>
</td></tr>
<tr><td><code>b</code></td>
<td>
<p>intercept for various quantile level</p>
</td></tr>
</table>


<h3>Note</h3>

<p>cqr.lasso.mm(x,y,tau) work properly only if the least square estimation is good. 
</p>


<h3>References</h3>

<p>David R.Hunter and Runze Li.(2005) Variable Selection Using MM Algorithms,<em>The Annals of Statistics</em> <b>33</b>, Number 4, Page 1617&ndash;1642.
</p>
<p>Hui Zou and Ming Yuan(2008). Composite Quantile Regression and the Oracle Model Selection Theory, <em>The Annals of Statistics</em>, <b>36</b>, Number 3, Page 1108&ndash;1126.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
n=100
p=2
a=2*rnorm(n*2*p, mean = 1, sd =1)
x=matrix(a,n,2*p)
beta=2*rnorm(p,1,1)
beta=rbind(matrix(beta,p,1),matrix(0,p,1))
y=x%*%beta-matrix(rnorm(n,0.1,1),n,1)
tau=1:5/6
# x is 1000*20 matrix, y is 1000*1 vector, beta is 20*1 vector with last ten zero value elements. 
cqr.lasso.mm(x,y,tau)
</code></pre>

<hr>
<h2 id='cqr.mm'>Composite Quantile Regression (cqr) use Majorize and Minimize (mm) Algorithm</h2><span id='topic+cqr.mm'></span>

<h3>Description</h3>

<p>Composite quantile regression find the estimated coefficient which minimize the absolute error for various quantile level.
The algorithm majorizing the objective function by a quadratic function followed by minimizing that quadratic.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cqr.mm(X,y,tau,beta,maxit,toler)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cqr.mm_+3A_x">X</code></td>
<td>
<p>the design matrix</p>
</td></tr>
<tr><td><code id="cqr.mm_+3A_y">y</code></td>
<td>
<p>response variable</p>
</td></tr>
<tr><td><code id="cqr.mm_+3A_tau">tau</code></td>
<td>
<p>vector of quantile level</p>
</td></tr>
<tr><td><code id="cqr.mm_+3A_beta">beta</code></td>
<td>
<p>initial value of estimate coefficient (default naive guess by least square estimation) </p>
</td></tr>
<tr><td><code id="cqr.mm_+3A_maxit">maxit</code></td>
<td>
<p>maxim iteration (default 200)</p>
</td></tr>
<tr><td><code id="cqr.mm_+3A_toler">toler</code></td>
<td>
<p>the tolerance critical for stop the algorithm (default 1e-3)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code><a href="base.html#topic+list">list</a></code> structure is with components
</p>
<table>
<tr><td><code>beta</code></td>
<td>
<p>the vector of estimated coefficient</p>
</td></tr>
<tr><td><code>b</code></td>
<td>
<p>intercept for various quantile level</p>
</td></tr>
</table>


<h3>Note</h3>

<p>cqr.mm(x,y,tau) work properly only if the least square estimation is good. 
</p>


<h3>References</h3>

<p>David R.Hunter and Kenneth Lange. Quantile Regression via an MM Algorithm,<em>Journal of Computational and Graphical Statistics</em>, <b>9</b>, Number 1, Page 60&ndash;77.
</p>
<p>Hui Zou and Ming Yuan(2008). Composite Quantile Regression and the Oracle Model Selection Theory, <em>The Annals of Statistics</em>, <b>36</b>, Number 3, Page 1108&ndash;1126.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
n=100
p=2
a=rnorm(n*p, mean = 1, sd =1)
x=matrix(a,n,p)
beta=rnorm(p,1,1)
beta=matrix(beta,p,1)
y=x%*%beta-matrix(rnorm(n,0.1,1),n,1)
tau=1:5/6
# x is 1000*10 matrix, y is 1000*1 vector, beta is 10*1 vector
cqr.mm(x,y,tau)
</code></pre>

<hr>
<h2 id='CQRADMMCPP'>Composite Quantile regression (cqr) use Alternating Direction Method of Multipliers (ADMM) algorithm core computational part</h2><span id='topic+CQRADMMCPP'></span>

<h3>Description</h3>

<p>Composite quantile regression (cqr) find the estimated coefficient which minimize the absolute error for various quantile level.
The problem is well suited to distributed convex optimization and is based on Alternating Direction Method of Multipliers (ADMM) algorithm .
</p>

<hr>
<h2 id='CQRCDCPP'>Composite Quantile Regression (cqr) use Coordinate Descent (cd) Algorithms core computational part</h2><span id='topic+CQRCDCPP'></span>

<h3>Description</h3>

<p>Composite quantile regression (cqr) find the estimated coefficient which minimize the absolute error for various quantile level.
The algorithm base on greedy coordinate descent and Edgeworth's for ordinary <code class="reqn">l_1</code> regression. 
</p>

<hr>
<h2 id='CQRMMCPP'>Composite Quantile Regression (cqr) use Majorize and Minimize (mm) Algorithm core computational part</h2><span id='topic+CQRMMCPP'></span>

<h3>Description</h3>

<p>Composite quantile regression find the estimated coefficient which minimize the absolute error for various quantile level.
The algorithm majorizing the objective function by a quadratic function followed by minimizing that quadratic.
</p>

<hr>
<h2 id='CQRPADMMCPP'>Composite Quantile Regression (cqr) with Adaptive Lasso Penalty (lasso) use Alternating Direction Method of Multipliers (ADMM) algorithm core computational part</h2><span id='topic+CQRPADMMCPP'></span>

<h3>Description</h3>

<p>The adaptive lasso parameter base on the estimated coefficient without penalty function.
Composite quantile regression find the estimated coefficient which minimize the absolute error for various quantile level.
The problem is well suited to distributed convex optimization and is based on Alternating Direction Method of Multipliers (ADMM) algorithm .
</p>

<hr>
<h2 id='CQRPCDCPP'>Composite Quantile Regression (cqr) with Adaptive Lasso Penalty (lasso) use Coordinate Descent (cd) Algorithms core computational part</h2><span id='topic+CQRPCDCPP'></span>

<h3>Description</h3>

<p>The adaptive lasso parameter base on the estimated coefficient without penalty function.
Composite quantile regression find the estimated coefficient which minimize the absolute error for various quantile level.
The algorithm base on greedy coordinate descent and Edgeworth's for ordinary <code class="reqn">l_1</code> regression.
</p>

<hr>
<h2 id='CQRPMMCPP'>Composite Quantile Regression (cqr) with Adaptive Lasso Penalty (lasso) use Majorize and Minimize (mm) Algorithm core computational part</h2><span id='topic+CQRPMMCPP'></span>

<h3>Description</h3>

<p>The adaptive lasso penalty parameter base on the estimated coefficient without penalty function.
Composite quantile regression find the estimated coefficient which minimize the absolute error for various quantile level.
The algorithm majorizing the objective function by a quadratic function followed by minimizing that quadratic. 
</p>

<hr>
<h2 id='QR.admm'>Quantile Regression (QR) use Alternating Direction Method of Multipliers (ADMM) algorithm</h2><span id='topic+QR.admm'></span>

<h3>Description</h3>

<p>The problem is well suited to distributed convex optimization and is based on Alternating Direction Method of Multipliers (ADMM) algorithm .
</p>


<h3>Usage</h3>

<pre><code class='language-R'>QR.admm(X,y,tau,rho,beta, maxit, toler)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="QR.admm_+3A_x">X</code></td>
<td>
<p>the design matrix</p>
</td></tr>
<tr><td><code id="QR.admm_+3A_y">y</code></td>
<td>
<p>response variable</p>
</td></tr>
<tr><td><code id="QR.admm_+3A_tau">tau</code></td>
<td>
<p>quantile level</p>
</td></tr>
<tr><td><code id="QR.admm_+3A_rho">rho</code></td>
<td>
<p>augmented Lagrangian parameter</p>
</td></tr>
<tr><td><code id="QR.admm_+3A_beta">beta</code></td>
<td>
<p>initial value of estimate coefficient (default naive guess by least square estimation) </p>
</td></tr>
<tr><td><code id="QR.admm_+3A_maxit">maxit</code></td>
<td>
<p>maxim iteration (default 200)</p>
</td></tr>
<tr><td><code id="QR.admm_+3A_toler">toler</code></td>
<td>
<p>the tolerance critical for stop the algorithm (default 1e-3)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code><a href="base.html#topic+list">list</a></code> structure is with components
</p>
<table>
<tr><td><code>beta</code></td>
<td>
<p>the vector of estimated coefficient</p>
</td></tr>
<tr><td><code>b</code></td>
<td>
<p>intercept</p>
</td></tr>
</table>


<h3>Note</h3>

<p>QR.admm(x,y,tau) work properly only if the least square estimation is good. 
</p>


<h3>References</h3>

<p>S. Boyd, N. Parikh, E. Chu, B. Peleato and J. Eckstein.(2010) Distributed Optimization and Statistical Learning via the Alternating Direction.Method of Multipliers <em>Foundations and Trends in Machine Learning</em>, <b>3</b>, No.1, 1&ndash;122 
</p>
<p>Koenker, Roger. <em>Quantile Regression</em>, New York, 2005. Print.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
n=100
p=2
a=rnorm(n*p, mean = 1, sd =1)
x=matrix(a,n,p)
beta=rnorm(p,1,1)
beta=matrix(beta,p,1)
y=x%*%beta-matrix(rnorm(n,0.1,1),n,1)
# x is 1000*10 matrix, y is 1000*1 vector, beta is 10*1 vector
QR.admm(x,y,0.1)
</code></pre>

<hr>
<h2 id='QR.cd'>Quantile Regression (QR) use Coordinate Descent (cd) Algorithms</h2><span id='topic+QR.cd'></span>

<h3>Description</h3>

<p>The algorithm base on greedy coordinate descent and Edgeworth's for ordinary <code class="reqn">l_1</code> regression. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>QR.cd(X,y,tau,beta,maxit,toler)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="QR.cd_+3A_x">X</code></td>
<td>
<p>the design matrix</p>
</td></tr>
<tr><td><code id="QR.cd_+3A_y">y</code></td>
<td>
<p>response variable</p>
</td></tr>
<tr><td><code id="QR.cd_+3A_tau">tau</code></td>
<td>
<p>quantile level</p>
</td></tr>
<tr><td><code id="QR.cd_+3A_beta">beta</code></td>
<td>
<p>initial value of estimate coefficient (default naive guess by least square estimation) </p>
</td></tr>
<tr><td><code id="QR.cd_+3A_maxit">maxit</code></td>
<td>
<p>maxim iteration (default 200)</p>
</td></tr>
<tr><td><code id="QR.cd_+3A_toler">toler</code></td>
<td>
<p>the tolerance critical for stop the algorithm (default 1e-3)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code><a href="base.html#topic+list">list</a></code> structure is with components
</p>
<table>
<tr><td><code>beta</code></td>
<td>
<p>the vector of estimated coefficient</p>
</td></tr>
<tr><td><code>b</code></td>
<td>
<p>intercept</p>
</td></tr>
</table>


<h3>Note</h3>

<p>QR.cd(x,y,tau) work properly only if the least square estimation is good. 
</p>


<h3>References</h3>

<p>Wu, T.T. and Lange, K. (2008). Coordinate Descent Algorithms for Lasso Penalized Regression. <em>Annals of Applied Statistics</em>, <b>2</b>, No 1, 224&ndash;244.
</p>
<p>Koenker, Roger. <em>Quantile Regression</em>, New York, 2005. Print.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
n=100
p=2
a=rnorm(n*p, mean = 1, sd =1)
x=matrix(a,n,p)
beta=rnorm(p,1,1)
beta=matrix(beta,p,1)
y=x%*%beta-matrix(rnorm(n,0.1,1),n,1)
# x is 1000*10 matrix, y is 1000*1 vector, beta is 10*1 vector
QR.cd(x,y,0.1)
</code></pre>

<hr>
<h2 id='QR.ip'>Quantile Regression (QR) use Interior Point (ip) Method</h2><span id='topic+QR.ip'></span>

<h3>Description</h3>

<p>The function use the interior point method from quantreg to solve the quantile regression problem.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>QR.ip(X,y,tau)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="QR.ip_+3A_x">X</code></td>
<td>
<p>the design matrix</p>
</td></tr>
<tr><td><code id="QR.ip_+3A_y">y</code></td>
<td>
<p>response variable</p>
</td></tr>
<tr><td><code id="QR.ip_+3A_tau">tau</code></td>
<td>
<p>quantile level</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code><a href="base.html#topic+list">list</a></code> structure is with components
</p>
<table>
<tr><td><code>beta</code></td>
<td>
<p>the vector of estimated coefficient</p>
</td></tr>
<tr><td><code>b</code></td>
<td>
<p>intercept</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Need to install quantreg package from CRAN. 
</p>


<h3>References</h3>

<p>Koenker, Roger. <em>Quantile Regression</em>, New York, 2005. Print.
</p>
<p>Koenker, R. and S. Portnoy (1997).
The Gaussian Hare and the Laplacian Tortoise:
Computability of squared-error vs. absolute-error estimators, with discussion,
<em>Statistical Science</em>, <b>12</b>, 279-300.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
n=100
p=2
a=rnorm(n*p, mean = 1, sd =1)
x=matrix(a,n,p)
beta=rnorm(p,1,1)
beta=matrix(beta,p,1)
y=x%*%beta-matrix(rnorm(n,0.1,1),n,1)
# x is 1000*10 matrix, y is 1000*1 vector, beta is 10*1 vector
#you should install Rmosek first to run following command
#QR.ip(x,y,0.1)
</code></pre>

<hr>
<h2 id='QR.lasso.admm'>Quantile Regression (QR) with Adaptive Lasso Penalty (lasso) use Alternating Direction
Method of Multipliers (ADMM) algorithm  </h2><span id='topic+QR.lasso.admm'></span>

<h3>Description</h3>

<p>The adaptive lasso parameter base on the estimated coefficient without penalty function.
The problem is well suited to distributed convex optimization and is based on Alternating Direction Method of Multipliers (ADMM) algorithm .
</p>


<h3>Usage</h3>

<pre><code class='language-R'>QR.lasso.admm(X,y,tau,lambda,rho,beta,maxit)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="QR.lasso.admm_+3A_x">X</code></td>
<td>
<p>the design matrix</p>
</td></tr>
<tr><td><code id="QR.lasso.admm_+3A_y">y</code></td>
<td>
<p>response variable</p>
</td></tr>
<tr><td><code id="QR.lasso.admm_+3A_tau">tau</code></td>
<td>
<p>quantile level</p>
</td></tr>
<tr><td><code id="QR.lasso.admm_+3A_lambda">lambda</code></td>
<td>
<p>The constant coefficient of penalty function. (default lambda=1)</p>
</td></tr>
<tr><td><code id="QR.lasso.admm_+3A_rho">rho</code></td>
<td>
<p>augmented Lagrangian parameter</p>
</td></tr>
<tr><td><code id="QR.lasso.admm_+3A_beta">beta</code></td>
<td>
<p>initial value of estimate coefficient (default naive guess by least square estimation) </p>
</td></tr>
<tr><td><code id="QR.lasso.admm_+3A_maxit">maxit</code></td>
<td>
<p>maxim iteration (default 200)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code><a href="base.html#topic+list">list</a></code> structure is with components
</p>
<table>
<tr><td><code>beta</code></td>
<td>
<p>the vector of estimated coefficient</p>
</td></tr>
<tr><td><code>b</code></td>
<td>
<p>intercept</p>
</td></tr>
</table>


<h3>Note</h3>

<p>QR.lasso.admm(x,y,tau) work properly only if the least square estimation is good. 
</p>


<h3>References</h3>

<p>S. Boyd, N. Parikh, E. Chu, B. Peleato and J. Eckstein.(2010) Distributed Optimization and Statistical Learning via the Alternating Direction. Method of Multipliers <em>Foundations and Trends in Machine Learning</em>, <b>3</b>, No.1, 1&ndash;122
</p>
<p>Wu, Yichao and Liu, Yufeng (2009). Variable selection in quantile regression. <em>Statistica Sinica</em>, <b>19</b>, 801&ndash;817.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
n=100
p=2
a=2*rnorm(n*2*p, mean = 1, sd =1)
x=matrix(a,n,2*p)
beta=2*rnorm(p,1,1)
beta=rbind(matrix(beta,p,1),matrix(0,p,1))
y=x%*%beta-matrix(rnorm(n,0.1,1),n,1)
# x is 1000*20 matrix, y is 1000*1 vector, beta is 20*1 vector with last ten zero value elements. 
QR.lasso.admm(x,y,0.1)
</code></pre>

<hr>
<h2 id='QR.lasso.cd'>Quantile Regression (QR) with Adaptive Lasso Penalty (lasso) use Coordinate Descent (cd)  Algorithms</h2><span id='topic+QR.lasso.cd'></span>

<h3>Description</h3>

<p>The adaptive lasso parameter base on the estimated coefficient without penalty function.
The algorithm base on greedy coordinate descent and Edgeworth's for ordinary <code class="reqn">l_1</code>    regression. As explored by Tong Tong Wu and Kenneth Lange.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>QR.lasso.cd(X,y,tau,lambda,beta,maxit,toler)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="QR.lasso.cd_+3A_x">X</code></td>
<td>
<p>the design matrix</p>
</td></tr>
<tr><td><code id="QR.lasso.cd_+3A_y">y</code></td>
<td>
<p>response variable</p>
</td></tr>
<tr><td><code id="QR.lasso.cd_+3A_tau">tau</code></td>
<td>
<p>quantile level</p>
</td></tr>
<tr><td><code id="QR.lasso.cd_+3A_lambda">lambda</code></td>
<td>
<p>The constant coefficient of penalty function. (default lambda=1)</p>
</td></tr>
<tr><td><code id="QR.lasso.cd_+3A_beta">beta</code></td>
<td>
<p>initial value of estimate coefficient (default naive guess by least square estimation) </p>
</td></tr>
<tr><td><code id="QR.lasso.cd_+3A_maxit">maxit</code></td>
<td>
<p>maxim iteration (default 200)</p>
</td></tr>
<tr><td><code id="QR.lasso.cd_+3A_toler">toler</code></td>
<td>
<p>the tolerance critical for stop the algorithm (default 1e-3)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code><a href="base.html#topic+list">list</a></code> structure is with components
</p>
<table>
<tr><td><code>beta</code></td>
<td>
<p>the vector of estimated coefficient</p>
</td></tr>
<tr><td><code>b</code></td>
<td>
<p>intercept</p>
</td></tr>
</table>


<h3>Note</h3>

<p>QR.lasso.cd(x,y,tau) work properly only if the least square estimation is good. 
</p>


<h3>References</h3>

<p>Wu, T.T. and Lange, K. (2008). Coordinate Descent Algorithms for Lasso Penalized Regression. <em>Annals of Applied Statistics</em>, <b>2</b>, No 1, 224&ndash;244.
</p>
<p>Wu, Yichao and Liu, Yufeng (2009). Variable selection in quantile regression. <em>Statistica Sinica</em>, <b>19</b>, 801&ndash;817.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
n=100
p=2
a=2*rnorm(n*2*p, mean = 1, sd =1)
x=matrix(a,n,2*p)
beta=2*rnorm(p,1,1)
beta=rbind(matrix(beta,p,1),matrix(0,p,1))
y=x%*%beta-matrix(rnorm(n,0.1,1),n,1)
# x is 1000*20 matrix, y is 1000*1 vector, beta is 20*1 vector with last ten zero value elements. 
QR.lasso.cd(x,y,0.1)
</code></pre>

<hr>
<h2 id='QR.lasso.ip'>Quantile Regression (QR) with Adaptive Lasso Penalty (lasso) use Interior Point (ip) Method</h2><span id='topic+QR.lasso.ip'></span>

<h3>Description</h3>

<p>The function use the interior point method from quantreg to solve the quantile regression problem.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>QR.lasso.ip(X,y,tau,lambda)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="QR.lasso.ip_+3A_x">X</code></td>
<td>
<p>the design matrix</p>
</td></tr>
<tr><td><code id="QR.lasso.ip_+3A_y">y</code></td>
<td>
<p>response variable</p>
</td></tr>
<tr><td><code id="QR.lasso.ip_+3A_tau">tau</code></td>
<td>
<p>quantile level</p>
</td></tr>
<tr><td><code id="QR.lasso.ip_+3A_lambda">lambda</code></td>
<td>
<p>The constant coefficient of penalty function. (default lambda=1)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code><a href="base.html#topic+list">list</a></code> structure is with components
</p>
<table>
<tr><td><code>beta</code></td>
<td>
<p>the vector of estimated coefficient</p>
</td></tr>
<tr><td><code>b</code></td>
<td>
<p>intercept</p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>
<p>The constant coefficient of penalty function. (default lambda=1)</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Need to install quantreg package from CRAN. 
</p>


<h3>References</h3>

<p>Koenker, R. and S. Portnoy (1997).
The Gaussian Hare and the Laplacian Tortoise:
Computability of squared-error vs. absolute-error estimators, with discussion,
<em>Statistical Science</em>, <b>12</b>, 279-300.
</p>
<p>Wu, Yichao and Liu, Yufeng (2009). Variable selection in quantile regression. <em>Statistica Sinica</em>, <b>19</b>, 801&ndash;817.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
n=100
p=2
a=2*rnorm(n*2*p, mean = 1, sd =1)
x=matrix(a,n,2*p)
beta=2*rnorm(p,1,1)
beta=rbind(matrix(beta,p,1),matrix(0,p,1))
y=x%*%beta-matrix(rnorm(n,0.1,1),n,1)
# x is 1000*20 matrix, y is 1000*1 vector, beta is 20*1 vector with last ten zero value elements. 
#you should install Rmosek first to run following command
#QR.lasso.ip(x,y,0.1)
</code></pre>

<hr>
<h2 id='QR.lasso.mm'>Quantile Regression (QR) with Adaptive Lasso Penalty (lasso) use Majorize and Minimize (mm) algorithm</h2><span id='topic+QR.lasso.mm'></span>

<h3>Description</h3>

<p>The adaptive lasso parameter base on the estimated coefficient without penalty function.
The algorithm majorizing the objective function by a quadratic function followed by minimizing that quadratic.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>QR.lasso.mm(X,y,tau,lambda,beta,maxit,toler)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="QR.lasso.mm_+3A_x">X</code></td>
<td>
<p>the design matrix.</p>
</td></tr>
<tr><td><code id="QR.lasso.mm_+3A_y">y</code></td>
<td>
<p>response variable.</p>
</td></tr>
<tr><td><code id="QR.lasso.mm_+3A_tau">tau</code></td>
<td>
<p>quantile level.</p>
</td></tr>
<tr><td><code id="QR.lasso.mm_+3A_lambda">lambda</code></td>
<td>
<p>The constant coefficient of penalty function. (default lambda=1)</p>
</td></tr>
<tr><td><code id="QR.lasso.mm_+3A_beta">beta</code></td>
<td>
<p>initial value of estimate coefficient.(default naive guess by least square estimation) </p>
</td></tr>
<tr><td><code id="QR.lasso.mm_+3A_maxit">maxit</code></td>
<td>
<p>maxim iteration. (default 200)</p>
</td></tr>
<tr><td><code id="QR.lasso.mm_+3A_toler">toler</code></td>
<td>
<p>the tolerance critical for stop the algorithm. (default 1e-3)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code><a href="base.html#topic+list">list</a></code> structure is with components
</p>
<table>
<tr><td><code>beta</code></td>
<td>
<p>the vector of estimated coefficient</p>
</td></tr>
<tr><td><code>b</code></td>
<td>
<p>intercept</p>
</td></tr>
</table>


<h3>Note</h3>

<p>QR.lasso.mm(x,y,tau) work properly only if the least square estimation is good. 
</p>


<h3>References</h3>

<p>David R.Hunter and Runze Li.(2005) Variable Selection Using MM Algorithms,<em>The Annals of Statistics</em> <b>33</b>, Number 4, Page 1617&ndash;1642.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
n=100
p=2
a=2*rnorm(n*2*p, mean = 1, sd =1)
x=matrix(a,n,2*p)
beta=2*rnorm(p,1,1)
beta=rbind(matrix(beta,p,1),matrix(0,p,1))
y=x%*%beta-matrix(rnorm(n,0.1,1),n,1)
# x is 1000*20 matrix, y is 1000*1 vector, beta is 20*1 vector with last ten zero value elements. 
QR.lasso.mm(x,y,0.1)
</code></pre>

<hr>
<h2 id='QR.mm'>Quantile Regression (QR) use Majorize and Minimize (mm) algorithm</h2><span id='topic+QR.mm'></span>

<h3>Description</h3>

<p>The algorithm majorizing the objective function by a quadratic function followed by minimizing that quadratic.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>QR.mm(X,y,tau,beta,maxit,toler)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="QR.mm_+3A_x">X</code></td>
<td>
<p>the design matrix</p>
</td></tr>
<tr><td><code id="QR.mm_+3A_y">y</code></td>
<td>
<p>response variable</p>
</td></tr>
<tr><td><code id="QR.mm_+3A_tau">tau</code></td>
<td>
<p>quantile level</p>
</td></tr>
<tr><td><code id="QR.mm_+3A_beta">beta</code></td>
<td>
<p>initial value of estimate coefficient (default naive guess by least square estimation) </p>
</td></tr>
<tr><td><code id="QR.mm_+3A_maxit">maxit</code></td>
<td>
<p>maxim iteration (default 200)</p>
</td></tr>
<tr><td><code id="QR.mm_+3A_toler">toler</code></td>
<td>
<p>the tolerance critical for stop the algorithm (default 1e-3)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code><a href="base.html#topic+list">list</a></code> structure is with components
</p>
<table>
<tr><td><code>beta</code></td>
<td>
<p>the vector of estimated coefficient</p>
</td></tr>
<tr><td><code>b</code></td>
<td>
<p>intercept</p>
</td></tr>
</table>


<h3>Note</h3>

<p>QR.mm(x,y,tau) work properly only if the least square estimation is good. 
</p>


<h3>References</h3>

<p>David R.Hunter and Kenneth Lange. Quantile Regression via an MM Algorithm, <em>Journal of Computational and Graphical Statistics</em>, <b>9</b>, Number 1, Page 60&ndash;77
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
n=100
p=2
a=rnorm(n*p, mean = 1, sd =1)
x=matrix(a,n,p)
beta=rnorm(p,1,1)
beta=matrix(beta,p,1)
y=x%*%beta-matrix(rnorm(n,0.1,1),n,1)
# x is 1000*10 matrix, y is 1000*1 vector, beta is 10*1 vector
QR.mm(x,y,0.1)
</code></pre>

<hr>
<h2 id='QRADMMCPP'>Quantile Regression (QR) use Alternating Direction Method of Multipliers (ADMM) algorithm core computational part</h2><span id='topic+QRADMMCPP'></span>

<h3>Description</h3>

<p>The problem is well suited to distributed convex optimization and is based on Alternating Direction Method of Multipliers (ADMM) algorithm .
</p>

<hr>
<h2 id='QRCDCPP'>Quantile Regression (QR) use Coordinate Descent (cd) Algorithms core computational part</h2><span id='topic+QRCDCPP'></span>

<h3>Description</h3>

<p>The algorithm base on greedy coordinate descent and Edgeworth's for ordinary <code class="reqn">l_1</code> regression. 
</p>

<hr>
<h2 id='qrfit'> Quantile Regression (qr) model fitting</h2><span id='topic+qrfit'></span>

<h3>Description</h3>

<p>High level function for estimating parameters by quantile regression 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>qrfit(X,y,tau,beta,method,maxit,toler,rho)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="qrfit_+3A_x">X</code></td>
<td>
<p>the design matrix</p>
</td></tr>
<tr><td><code id="qrfit_+3A_y">y</code></td>
<td>
<p>response variable</p>
</td></tr>
<tr><td><code id="qrfit_+3A_tau">tau</code></td>
<td>
<p> quantile level</p>
</td></tr>
<tr><td><code id="qrfit_+3A_method">method</code></td>
<td>
<p>&quot;mm&quot; for majorize and minimize method,&quot;cd&quot; for coordinate descent method, &quot;admm&quot; for Alternating method of mulipliers method,&quot;ip&quot; for interior point mehod</p>
</td></tr>
<tr><td><code id="qrfit_+3A_rho">rho</code></td>
<td>
<p>augmented Lagrangian parameter</p>
</td></tr>
<tr><td><code id="qrfit_+3A_beta">beta</code></td>
<td>
<p>initial value of estimate coefficient (default naive guess by least square estimation) </p>
</td></tr>
<tr><td><code id="qrfit_+3A_maxit">maxit</code></td>
<td>
<p>maxim iteration (default 200)</p>
</td></tr>
<tr><td><code id="qrfit_+3A_toler">toler</code></td>
<td>
<p>the tolerance critical for stop the algorithm (default 1e-3)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code><a href="base.html#topic+list">list</a></code> structure is with components
</p>
<table>
<tr><td><code>beta</code></td>
<td>
<p>the vector of estimated coefficient</p>
</td></tr>
<tr><td><code>b</code></td>
<td>
<p>intercept</p>
</td></tr>
</table>


<h3>Note</h3>

<p>qrfit(x,y,tau) work properly only if the least square estimation is good. 
Interior point method is done by quantreg.
</p>

<hr>
<h2 id='qrfit.lasso'> Quantile Regression (qr) with Adaptive Lasso Penalty (lasso)</h2><span id='topic+qrfit.lasso'></span>

<h3>Description</h3>

<p>High level function for estimating and selecting parameter by quantile regression with adaptive lasso penalty.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>qrfit.lasso(X,y,tau,lambda,beta,method,maxit,toler,rho)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="qrfit.lasso_+3A_x">X</code></td>
<td>
<p>the design matrix</p>
</td></tr>
<tr><td><code id="qrfit.lasso_+3A_y">y</code></td>
<td>
<p>response variable</p>
</td></tr>
<tr><td><code id="qrfit.lasso_+3A_tau">tau</code></td>
<td>
<p> quantile level</p>
</td></tr>
<tr><td><code id="qrfit.lasso_+3A_method">method</code></td>
<td>
<p>&quot;mm&quot; for majorize and minimize method,&quot;cd&quot; for coordinate descent method, &quot;admm&quot; for Alternating method of mulipliers method,&quot;ip&quot; for interior point mehod</p>
</td></tr>
<tr><td><code id="qrfit.lasso_+3A_lambda">lambda</code></td>
<td>
<p>The constant coefficient of penalty function. (default lambda=1)</p>
</td></tr>
<tr><td><code id="qrfit.lasso_+3A_rho">rho</code></td>
<td>
<p>augmented Lagrangian parameter</p>
</td></tr>
<tr><td><code id="qrfit.lasso_+3A_beta">beta</code></td>
<td>
<p>initial value of estimate coefficient (default naive guess by least square estimation) </p>
</td></tr>
<tr><td><code id="qrfit.lasso_+3A_maxit">maxit</code></td>
<td>
<p>maxim iteration (default 200)</p>
</td></tr>
<tr><td><code id="qrfit.lasso_+3A_toler">toler</code></td>
<td>
<p>the tolerance critical for stop the algorithm (default 1e-3)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code><a href="base.html#topic+list">list</a></code> structure is with components
</p>
<table>
<tr><td><code>beta</code></td>
<td>
<p>the vector of estimated coefficient</p>
</td></tr>
<tr><td><code>b</code></td>
<td>
<p>intercept</p>
</td></tr>
</table>


<h3>Note</h3>

<p>qrfit.lasso(x,y,tau) work properly only if the least square estimation is good. 
Interior point method is done by quantreg.
</p>

<hr>
<h2 id='QRMMCPP'>Quantile Regression (QR) use Majorize and Minimize (mm) algorithm core computational part</h2><span id='topic+QRMMCPP'></span>

<h3>Description</h3>

<p>The algorithm majorizing the objective function by a quadratic function followed by minimizing that quadratic.
</p>

<hr>
<h2 id='QRPADMMCPP'>Quantile Regression (QR) with Adaptive Lasso Penalty (lasso) use Alternating Direction
Method of Multipliers (ADMM) algorithm core computational part</h2><span id='topic+QRPADMMCPP'></span>

<h3>Description</h3>

<p>The adaptive lasso parameter base on the estimated coefficient without penalty function.
The problem is well suited to distributed convex optimization and is based on Alternating Direction Method of Multipliers (ADMM) algorithm .
</p>

<hr>
<h2 id='QRPCDCPP'>Quantile Regression (QR) with Adaptive Lasso Penalty (lasso) use Coordinate Descent (cd)  Algorithms core computational part</h2><span id='topic+QRPCDCPP'></span>

<h3>Description</h3>

<p>The adaptive lasso parameter base on the estimated coefficient without penalty function.
The algorithm base on greedy coordinate descent and Edgeworth's for ordinary <code class="reqn">l_1</code>    regression. As explored by Tong Tong Wu and Kenneth Lange.
</p>

<hr>
<h2 id='QRPMMCPP'>Quantile Regression (QR) with Adaptive Lasso Penalty (lasso) use Majorize and Minimize (mm) algorithm core computational part</h2><span id='topic+QRPMMCPP'></span>

<h3>Description</h3>

<p>The adaptive lasso parameter base on the estimated coefficient without penalty function.
The algorithm majorizing the objective function by a quadratic function followed by minimizing that quadratic.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
