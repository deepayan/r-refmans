<!DOCTYPE html><html><head><title>Help for package emulator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {emulator}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#emulator-package'>
<p>Bayesian Emulation of Computer Programs</p></a></li>
<li><a href='#betahat.fun'><p>Calculates MLE coefficients of linear fit</p></a></li>
<li><a href='#corr'><p>correlation function for calculating A</p></a></li>
<li><a href='#estimator'><p>Estimates each known datapoint using the others as datapoints</p></a></li>
<li><a href='#expert.estimates'><p>Expert estimates for Goldstein input parameters</p></a></li>
<li><a href='#interpolant'><p>Interpolates between known points using Bayesian estimation</p></a></li>
<li><a href='#latin.hypercube'><p>Latin hypercube design matrix</p></a></li>
<li><a href='#makeinputfiles'><p>Makes input files for condor runs of goldstein</p></a></li>
<li><a href='#model'><p>Simple model for concept checking</p></a></li>
<li><a href='#OO2002'><p>Implementation of the ideas of Oakley and O'Hagan 2002</p></a></li>
<li><a href='#optimal.scales'><p>Use optimization techniques to find the optimal scales</p></a></li>
<li><a href='#pad'><p>Simple pad function</p></a></li>
<li><a href='#prior.b'><p>Prior linear fits</p></a></li>
<li><a href='#quad.form'><p>Evaluate a quadratic form efficiently</p></a></li>
<li><a href='#regressor.basis'><p>Regressor basis function</p></a></li>
<li><a href='#results.table'><p>Results from 100 Goldstein runs</p></a></li>
<li><a href='#s.chi'><p>Variance estimator</p></a></li>
<li><a href='#sample.n.fit'><p>Sample from a Gaussian process and fit an emulator to the points</p></a></li>
<li><a href='#scales.likelihood'><p>Likelihood of roughness parameters</p></a></li>
<li><a href='#sigmahatsquared'><p>Estimator for sigma squared</p></a></li>
<li><a href='#toy'><p>A toy dataset</p></a></li>
<li><a href='#tr'><p>Trace of a matrix</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Bayesian Emulation of Computer Programs</td>
</tr>
<tr>
<td>Version:</td>
<td>1.2-24</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.0.1), mvtnorm</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr,rmarkdown</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Robin K. S. Hankin &lt;hankin.robin@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>
 Allows one to estimate the output of a computer program,
 as a function of the input parameters, without actually running it.
 The computer program is assumed to be a Gaussian process, whose
 parameters are estimated using Bayesian techniques that give a PDF of
 expected program output.  This PDF is conditional on a training set
 of runs, each consisting of a point in parameter space and the model
 output at that point.  The emphasis is on complex codes that take
 weeks or months to run, and that have a large number of undetermined
 input parameters; many climate prediction models fall into this
 class.  The emulator essentially determines Bayesian posterior
 estimates of the PDF of the output of a model, conditioned on results
 from previous runs and a user-specified prior linear model.  The
 package includes functionality to evaluate quadratic forms 
 efficiently. </td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL]</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/RobinHankin/emulator">https://github.com/RobinHankin/emulator</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/RobinHankin/emulator/issues">https://github.com/RobinHankin/emulator/issues</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-03-22 08:38:26 UTC; rhankin</td>
</tr>
<tr>
<td>Author:</td>
<td>Robin K. S. Hankin
    <a href="https://orcid.org/0000-0001-5982-0415"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut, cre]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-03-22 11:30:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='emulator-package'>
Bayesian Emulation of Computer Programs
</h2><span id='topic+emulator-package'></span><span id='topic+emulator'></span>

<h3>Description</h3>


<p> Allows one to estimate the output of a computer program,
 as a function of the input parameters, without actually running it.
 The computer program is assumed to be a Gaussian process, whose
 parameters are estimated using Bayesian techniques that give a PDF of
 expected program output.  This PDF is conditional on a training set
 of runs, each consisting of a point in parameter space and the model
 output at that point.  The emphasis is on complex codes that take
 weeks or months to run, and that have a large number of undetermined
 input parameters; many climate prediction models fall into this
 class.  The emulator essentially determines Bayesian posterior
 estimates of the PDF of the output of a model, conditioned on results
 from previous runs and a user-specified prior linear model.  The
 package includes functionality to evaluate quadratic forms 
 efficiently. 
</p>


<h3>Details</h3>

<p>The DESCRIPTION file:
</p>

<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> emulator</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Title: </td><td style="text-align: left;"> Bayesian Emulation of Computer Programs</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 1.2-24</td>
</tr>
<tr>
 <td style="text-align: left;">
Authors@R: </td><td style="text-align: left;"> person(given=c("Robin", "K. S."), family="Hankin", role = c("aut","cre"), email="hankin.robin@gmail.com", comment = c(ORCID = "0000-0001-5982-0415"))</td>
</tr>
<tr>
 <td style="text-align: left;">
VignetteBuilder: </td><td style="text-align: left;"> knitr</td>
</tr>
<tr>
 <td style="text-align: left;">
Depends: </td><td style="text-align: left;"> R (&gt;= 3.0.1), mvtnorm</td>
</tr>
<tr>
 <td style="text-align: left;">
Suggests: </td><td style="text-align: left;"> knitr,rmarkdown</td>
</tr>
<tr>
 <td style="text-align: left;">
Maintainer: </td><td style="text-align: left;"> Robin K. S. Hankin &lt;hankin.robin@gmail.com&gt;</td>
</tr>
<tr>
 <td style="text-align: left;">
Description: </td><td style="text-align: left;"> 
 Allows one to estimate the output of a computer program,
 as a function of the input parameters, without actually running it.
 The computer program is assumed to be a Gaussian process, whose
 parameters are estimated using Bayesian techniques that give a PDF of
 expected program output.  This PDF is conditional on a training set
 of runs, each consisting of a point in parameter space and the model
 output at that point.  The emphasis is on complex codes that take
 weeks or months to run, and that have a large number of undetermined
 input parameters; many climate prediction models fall into this
 class.  The emulator essentially determines Bayesian posterior
 estimates of the PDF of the output of a model, conditioned on results
 from previous runs and a user-specified prior linear model.  The
 package includes functionality to evaluate quadratic forms 
 efficiently. </td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL</td>
</tr>
<tr>
 <td style="text-align: left;">
URL: </td><td style="text-align: left;"> https://github.com/RobinHankin/emulator</td>
</tr>
<tr>
 <td style="text-align: left;">
BugReports: </td><td style="text-align: left;"> https://github.com/RobinHankin/emulator/issues</td>
</tr>
<tr>
 <td style="text-align: left;">
Author: </td><td style="text-align: left;"> Robin K. S. Hankin [aut, cre] (&lt;https://orcid.org/0000-0001-5982-0415&gt;)</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>


<p>Index of help topics:
</p>
<pre>
OO2002                  Implementation of the ideas of Oakley and
                        O'Hagan 2002
betahat.fun             Calculates MLE coefficients of linear fit
corr                    correlation function for calculating A
emulator-package        Bayesian Emulation of Computer Programs
estimator               Estimates each known datapoint using the others
                        as datapoints
expert.estimates        Expert estimates for Goldstein input parameters
interpolant             Interpolates between known points using
                        Bayesian estimation
latin.hypercube         Latin hypercube design matrix
makeinputfiles          Makes input files for condor runs of goldstein
model                   Simple model for concept checking
optimal.scales          Use optimization techniques to find the optimal
                        scales
pad                     Simple pad function
prior.b                 Prior linear fits
quad.form               Evaluate a quadratic form efficiently
regressor.basis         Regressor basis function
results.table           Results from 100 Goldstein runs
s.chi                   Variance estimator
sample.n.fit            Sample from a Gaussian process and fit an
                        emulator to the points
scales.likelihood       Likelihood of roughness parameters
sigmahatsquared         Estimator for sigma squared
toy                     A toy dataset
tr                      Trace of a matrix
</pre>


<h3>Author(s)</h3>

<p>Robin K. S. Hankin [aut, cre] (&lt;https://orcid.org/0000-0001-5982-0415&gt;)
</p>
<p>Maintainer: Robin K. S. Hankin &lt;hankin.robin@gmail.com&gt;
</p>


<h3>References</h3>


<ul>
<li>
<p>J. Oakley 1999. &ldquo;Bayesian uncertainty analysis for complex
computer codes&rdquo;, PhD thesis, University of Sheffield.
</p>
</li>
<li>
<p>R. K. S. Hankin 2005. &ldquo;Introducing BACCO, an R bundle for
Bayesian analysis of computer code output&rdquo;, Journal of Statistical
Software, 14(16)
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>## More detail given in optimal.scales.Rd
scales_true  &lt;- c(1,1,1,1,1,4)

## and a real (linear) relation:
real.relation &lt;- function(x){sum( (1:6)*x )}

## Now a design matrix:
val  &lt;- latin.hypercube(100,6)

## apply the real relation:
d &lt;- apply(val,1,real.relation)

## and add some suitably correlated Gaussian noise:
A &lt;- corr.matrix(val,scales=scales_true)
d.noisy &lt;-  as.vector(rmvnorm(n=1,mean=apply(val,1,real.relation), 0.3*A))

## Now try to predict the values at points x:

x &lt;- latin.hypercube(20,6)
predicted &lt;- int.qq(x,d.noisy,xold=val, Ainv=solve(A),pos.def.matrix=diag(scales_true))
observed &lt;- apply(x,1,real.relation)

par(pty='s')
plot(predicted,observed,xlim=c(4,18),ylim=c(4,18))
abline(0,1)

</code></pre>

<hr>
<h2 id='betahat.fun'>Calculates MLE coefficients of linear fit</h2><span id='topic+betahat.fun'></span><span id='topic+betahat.fun.A'></span>

<h3>Description</h3>

<p>Determines the maximum likelihood regression coeffients for the
specified regression basis and correlation matrix <code>A</code>.
</p>
<p>The &ldquo;<code>.A</code>&rdquo; form needs only <code>A</code> (and not <code>Ainv</code>),
thus removing the need to calculate a matrix inverse.  Note that this
form is <em>slower</em> than the other if <code>Ainv</code> is known in
advance, as <code>solve(.,.)</code> is slow.
</p>
<p>If <code>Ainv</code> is not known in advance, the two forms seem to
perform similarly in the cases considered here and in the
<code>goldstein</code> package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>betahat.fun(xold, Ainv, d, give.variance=FALSE, func)
betahat.fun.A(xold, A, d, give.variance=FALSE, func)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="betahat.fun_+3A_xold">xold</code></td>
<td>
<p>Data frame, each line being the parameters of one run</p>
</td></tr>
<tr><td><code id="betahat.fun_+3A_a">A</code></td>
<td>
<p>Correlation matrix, typically provided by
<code>corr.matrix()</code></p>
</td></tr>
<tr><td><code id="betahat.fun_+3A_ainv">Ainv</code></td>
<td>
<p>Inverse of the correlation matrix <code>A</code></p>
</td></tr>
<tr><td><code id="betahat.fun_+3A_d">d</code></td>
<td>
<p>Vector of results at the points specified in <code>xold</code></p>
</td></tr>
<tr><td><code id="betahat.fun_+3A_give.variance">give.variance</code></td>
<td>
<p>Boolean, with <code>TRUE</code> meaning to return
information on the variance of <code class="reqn">\hat{\beta}</code> and
default <code>FALSE</code> meaning to return just the estimator</p>
</td></tr>
<tr><td><code id="betahat.fun_+3A_func">func</code></td>
<td>
<p>Function to generate regression basis; defaults to <code>regressor.basis</code></p>
</td></tr>
</table>


<h3>Note</h3>

<p>Here, the strategy of using two separate functions, eg <code>foo()</code>
and <code>foo.A()</code>, one of which inverts <code>A</code> and one of which
uses notionally more efficient means.  Compare the other
strategy in which a Boolean flag, <code>use.Ainv</code>, has the same
effect.  An example would be <code>scales.likelihood()</code>.
</p>


<h3>Author(s)</h3>

<p>Robin K. S. Hankin</p>


<h3>References</h3>


<ul>
<li>
<p>J. Oakley and A. O'Hagan, 2002. <em>Bayesian Inference for the
Uncertainty Distribution of Computer Model Outputs</em>, Biometrika
89(4), pp769-784
</p>
</li>
<li>
<p>R. K. S. Hankin 2005. <em>Introducing BACCO, an R bundle for
Bayesian analysis of computer code output</em>, Journal of Statistical
Software, 14(16)
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>data(toy)
val &lt;- toy
H &lt;- regressor.multi(val)
d &lt;- apply(H,1,function(x){sum((0:6)*x)})


fish &lt;- rep(2,6)
A &lt;- corr.matrix(val,scales=fish)
Ainv &lt;- solve(A)

# now add suitably correlated Gaussian noise:
d &lt;-  as.vector(rmvnorm(n=1,mean=d, 0.1*A))

betahat.fun(val , Ainv , d)           # should be close to c(0,1:6)


# Now look at the variances:
betahat.fun(val,Ainv,give.variance=TRUE, d)


     # now find the value of the prior expectation (ie the regression
     # plane) at an unknown point:
x.unknown &lt;- rep(0.5 , 6)
regressor.basis(x.unknown) %*% betahat.fun(val, Ainv, d)

     # compare the prior with the posterior
interpolant(x.unknown, d, val, Ainv,scales=fish)
     # Heh, it's the same!  (of course it is, there is no error here!)


     # OK, put some error on the old observations:
d.noisy &lt;- as.vector(rmvnorm(n=1,mean=d,0.1*A))

     # now compute the regression point:
regressor.basis(x.unknown) %*% betahat.fun(val, Ainv, d.noisy)

     # and compare with the output of interpolant():
interpolant(x.unknown, d.noisy, val, Ainv, scales=fish)
     # there is a difference!



     # now try a basis function that has superfluous degrees of freedom.
     # we need a bigger dataset.  Try 100:
val &lt;- latin.hypercube(100,6)
colnames(val) &lt;- letters[1:6]
d &lt;- apply(val,1,function(x){sum((1:6)*x)})
A &lt;- corr.matrix(val,scales=rep(1,6))
Ainv &lt;- solve(A)

    
betahat.fun(val, Ainv, d, func=function(x){c(1,x,x^2)})
     # should be c(0:6 ,rep(0,6).  The zeroes should be zero exactly
     # because the original function didn't include any squares.


## And finally a sanity check:
f &lt;- function(x){c(1,x,x^2)}
jj1 &lt;- betahat.fun(val, Ainv, d, func=f)
jj2 &lt;- betahat.fun.A(val, A, d, func=f)

abs(jj1-jj2)  # should be small

</code></pre>

<hr>
<h2 id='corr'>correlation function for calculating A</h2><span id='topic+corr'></span><span id='topic+corr.matrix'></span>

<h3>Description</h3>

<p>Calculates the correlation function between two points in parameter
space, thus determining the correlation matrix A.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>corr(x1, x2, scales=NULL , pos.def.matrix=NULL,
coords="cartesian", spherical.distance.function=NULL)
corr.matrix(xold, yold=NULL, method=1, distance.function=corr, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="corr_+3A_x1">x1</code></td>
<td>
<p>First point</p>
</td></tr>
<tr><td><code id="corr_+3A_x2">x2</code></td>
<td>
<p>Second point</p>
</td></tr>
<tr><td><code id="corr_+3A_scales">scales</code></td>
<td>
<p>Vector specifying the diagonal elements of <code class="reqn">B</code> (see below)</p>
</td></tr>
<tr><td><code id="corr_+3A_pos.def.matrix">pos.def.matrix</code></td>
<td>
<p>Positive definite matrix to be used by
<code>corr.matrix()</code> for <code class="reqn">B</code>.  Exactly one of <code>scales</code> and
<code>pos.definite.matrix</code> should be specified.  Supplying
<code>scales</code> specifies the diagonal elements of <code class="reqn">B</code> (off diagonal
elements are set to zero); supply <code>pos.definite.matrix</code> in the
general case.  A single value is recycled.   Note that neither
<code>corr()</code> nor <code>corr.matrix()</code> test for positive
definiteness</p>
</td></tr>
<tr><td><code id="corr_+3A_coords">coords</code></td>
<td>
<p>In function <code>corr()</code>, a character string, with
default &ldquo;cartesian&rdquo; meaning to interpret the elements of
<code>x1</code> (and <code>x2</code>) as coordinates in Cartesian space.  The
only other acceptable value is currently &ldquo;spherical&rdquo;, which
means to interpret the first element of <code>x1</code> as row number, and
the second element as column number, on a spherical computational
grid (such as used  by climate model Goldstein; see package
<code>goldstein</code> for an example of this option in use).
Spherical geometry is then used to calculate the geotetic (great
circle) distance between point <code>x1</code> and <code>x2</code>, with
function <code>gcd()</code></p>
</td></tr>
<tr><td><code id="corr_+3A_method">method</code></td>
<td>
<p>An integer with values 1, 2, or 3.  If 1, then use a
fast matrix calculation that returns
<code class="reqn">e^{-(x-x')^TB(x-x')}</code>.  If 2 or 3, return
the appropriate output from <code>corr()</code>, noting that ellipsis
arguments are passed to <code>corr()</code> (for example, 
<code>scales</code>).  The difference between 2 and 3 is a marginal
difference in numerical efficiency; the main difference is
computational elegance.
</p>
<p><strong>Warning 1:</strong> The code for <code>method=2</code> (formerly the
default), has a bug.  If <code>yold</code> has only one row, then
<code>corr.matrix(xold,yold,scales,method=2)</code> returns the transpose
of what one would expect.  Methods 1 and 3 return the correct
matrix.
</p>
<p><strong>Warning 2:</strong> If argument <code>distance.function</code> is not the
default, and <code>method</code> is the default (ie 1), then <code>method</code>
will be silently changed to 2 on the grounds that <code>method=1</code> is
meaningless unless the distance function is <code>corr()</code></p>
</td></tr> 
<tr><td><code id="corr_+3A_distance.function">distance.function</code></td>
<td>
<p>Function to be used to calculate distances in
<code>corr.matrix()</code>.  Defaults to <code>corr()</code></p>
</td></tr>
<tr><td><code id="corr_+3A_xold">xold</code></td>
<td>
<p>Matrix, each row of which is an evaluated point</p>
</td></tr>
<tr><td><code id="corr_+3A_yold">yold</code></td>
<td>
<p>(optional) matrix, each row of which is an evaluated
point.  If missing, use <code>xold</code></p>
</td></tr>
<tr><td><code id="corr_+3A_spherical.distance.function">spherical.distance.function</code></td>
<td>
<p>In <code>corr</code>, a function to
determine the distance between two points; used if
<code>coords</code>=&ldquo;spherical&rdquo;.  A good one to choose is
<code>gcd()</code> (that is, Great Circle Distance) of the goldstein library</p>
</td></tr> 
<tr><td><code id="corr_+3A_...">...</code></td>
<td>
<p>In function <code>corr.matrix()</code>, extra arguments that are
passed on to the distance function.  In the default case in which
the distance.function is <code>corr()</code>, one <em>must</em> pass
<code>scales</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Function <code>corr()</code> calculates the correlation between two points
<code>x1</code> and <code>x2</code> in the parameter space.  Function
<code>corr.matrix()</code> calculates the correlation matrix between each
row of <code>xold</code> and <code>yold</code>.  If <code>yold=NULL</code> then the
correlation matrix between <code>xold</code> and itself is returned, which
should be positive definite.
</p>
<p>Evaluates Oakley's equation 2.12 for the
correlation between <code class="reqn">\eta(x)</code> and <code class="reqn">\eta(x')</code>:
<code class="reqn">e^{-(x-x')^TB(x-x')}</code>.
</p>


<h3>Value</h3>

<p>Returns the correlation function
</p>


<h3>Note</h3>

<p>It is worth reemphasising that supplying <code>scales</code> makes
matrix <code class="reqn">B</code> diagonal.
</p>
<p>Thus, if <code>scales</code> is supplied, <code>B=diag(scales)</code> and
</p>
<p style="text-align: center;"><code class="reqn">c(x,x')=\exp\left[-(x-x')^TB(x-x')\right]=\exp\left[\Sigma_i
    s_i(x_i-{x'}_i)^2\right]</code>
</p>

<p>Thus if <code class="reqn">x</code> has units <code class="reqn">[X]</code>, the units of <code>scales</code> are
<code class="reqn">[X^{-2}]</code>.
</p>
<p>So if <code>scales[i]</code> is big, even small displacements in <code>x[i]</code>
(that is, moving a small distance in parameter space, in the
<code class="reqn">i</code>-th dimension) will result in small correlations.  If
<code>scales[i]</code> is small, even large displacements in <code>x[1]</code>
will have large correlations</p>


<h3>Author(s)</h3>

<p>Robin K. S. Hankin</p>


<h3>References</h3>


<ul>
<li> 
<p>J. Oakley 1999. <em>Bayesian uncertainty analysis for complex
computer codes</em>, PhD thesis, University of Sheffield.
</p>
</li>
<li>
<p>J. Oakley and A. O'Hagan, 2002. <em>Bayesian Inference for the
Uncertainty Distribution of Computer Model Outputs</em>, Biometrika
89(4), pp769-784
</p>
</li>
<li>
<p>R. K. S. Hankin 2005. <em>Introducing BACCO, an R bundle for
Bayesian analysis of computer code output</em>, Journal of Statistical
Software, 14(16)
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
jj &lt;- latin.hypercube(2,10)
x1 &lt;- jj[1,]
x2 &lt;- jj[2,]

corr(x1,x2,scales=rep(1,10))             # correlation between 2 points
corr(x1,x2,pos.def.matrix=0.1+diag(10))  # see effect of offdiagonal elements

x &lt;- latin.hypercube(4,7)                # 4 points in 7-dimensional space
rownames(x) &lt;- letters[1:4]              # name the points

corr.matrix(x,scales=rep(1,7))

x[1,1] &lt;- 100                            # make the first point far away
corr.matrix(x,scales=rep(1,7))

# note that all the first row and first column apart from element [1,1]
# is zero (or very nearly so) because the first point is now very far
# from the other points and has zero correlation with them.

# To use just a single dimension, remember to use the drop=FALSE argument:
corr.matrix(x[,1,drop=FALSE],scales=rep(1,1))


# For problems in 1D, coerce the independent variable to a matrix:
m &lt;- c(0.2, 0.4, 0.403, 0.9)
corr.matrix(cbind(m),scales=1)


# now use a non-default value for distance.function.
# Function f() below taken from Jeremy Oakley's thesis page 12,
# equation 2.10:

f &lt;- function(x,y,theta){
  d &lt;- sum(abs(x-y))
  if(d &gt;= theta){
    return(0)
  }else{
    return(1-d/theta)
  }
}


corr.matrix(xold=x, distance.function=f, method=2, theta=4)

 # Note the first row and first column is a single 1 and 3 zeros
 # (because the first point, viz x[1,], is "far" from the other points).
 # Also note the method=2 argument here; method=1 is the fast slick
 # matrix method suggested by Doug and Jeremy, but this only works
 # for distance.function=corr.



</code></pre>

<hr>
<h2 id='estimator'>Estimates each known datapoint using the others as datapoints</h2><span id='topic+estimator'></span>

<h3>Description</h3>

<p>Uses Bayesian techniques to estimate a model's prediction at each of
<code>n</code> datapoints.  To estimate the <code class="reqn">i^{\rm th}</code> point,
conditioning variables of <code class="reqn">1,\ldots, i-1</code>
and <code class="reqn">i+1,\ldots, n</code> inclusive are used (ie, all points
except point <code class="reqn">i</code>).
</p>
<p>This routine is useful when finding optimal coefficients for the
correlation using boot methods.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>estimator(val, A, d, scales=NULL, pos.def.matrix=NULL,
func=regressor.basis)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="estimator_+3A_val">val</code></td>
<td>
<p>Design matrix with rows corresponding to points at which the
function is known</p>
</td></tr>
<tr><td><code id="estimator_+3A_a">A</code></td>
<td>
<p>Correlation matrix (note that this is <strong>not</strong> the
inverse of the correlation matrix)</p>
</td></tr>
<tr><td><code id="estimator_+3A_d">d</code></td>
<td>
<p>Vector of observations</p>
</td></tr>
<tr><td><code id="estimator_+3A_scales">scales</code></td>
<td>
<p>Scales to be used to calculate <code>t(x)</code>.  Note that
<code>scales</code> has no default value because <code>estimator()</code> is
most often used in the context of assessing the appropriateness of a
given value of <code>scales</code>.  If the desired distance matrix
(called <code class="reqn">B</code> in Oakley) is not diagonal, pass this matrix to
<code>estimator()</code> via the <code>pos.def.matrix</code> argument.</p>
</td></tr>
<tr><td><code id="estimator_+3A_pos.def.matrix">pos.def.matrix</code></td>
<td>
<p>Positive definite matrix <code class="reqn">B</code></p>
</td></tr>
<tr><td><code id="estimator_+3A_func">func</code></td>
<td>
<p>Function used to determine basis vectors, defaulting
to <code>regressor.basis</code> if not given.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Given a matrix of observation points and a vector of observations,
<code>estimator()</code> returns a vector of predictions.  Each prediction is
made in a three step process.  For each index <code class="reqn">i</code>:
</p>

<ul>
<li><p> Observation <code>d[i]</code> is discarded, and row <code>i</code> and
column <code>i</code> deleted from <code>A</code> (giving <code>A[-i,-i]</code>).
Thus <code>d</code> and <code>A</code> are
the observation vector and correlation matrix that would have been
obtained had observation <code>i</code> not  been available.
</p>
</li>
<li><p> The value of <code>d[i]</code> is estimated on the basis of the
shortened observation vector and the comatrix of <code>A</code>.
</p>
</li></ul>

<p>It is then possible to make a scatterplot of <code>d</code> vs <code>dhat</code>
where <code>dhat=estimator(val,A,d)</code>.  If the scales used are
&ldquo;good&rdquo;, then the points of this scatterplot will be close to
<code>abline(0,1)</code>.  The third step is to optimize the goodness of fit
of this scatterplot.
</p>


<h3>Value</h3>

<p>A vector of observations of the same length as <code>d</code>.
</p>


<h3>Author(s)</h3>

<p>Robin K. S. Hankin</p>


<h3>References</h3>


<ul>
<li>
<p>J. Oakley and A. O'Hagan, 2002. <em>Bayesian Inference for the
Uncertainty Distribution of Computer Model Outputs</em>, Biometrika
89(4), pp769-784
</p>
</li>
<li>
<p>R. K. S. Hankin 2005. <em>Introducing BACCO, an R bundle for
Bayesian analysis of computer code output</em>, Journal of Statistical
Software, 14(16)
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+optimal.scales">optimal.scales</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'># example has 11 observations on 6 dimensions.
# function is just sum( (1:6)*x) where x=c(x_1, ... , x_2)

val &lt;- latin.hypercube(11,6)
colnames(val) &lt;- letters[1:6]
d &lt;- apply(val,1,function(x){sum((1:6)*x)})

#pick some scales:
fish &lt;- rep(1,ncol(val))
A &lt;- corr.matrix(val,scales=fish)

#add some suitably correlated noise:
d &lt;- as.vector(rmvnorm(n=1, mean=d, 0.1*A))

# estimate d using the leave-out-one technique in estimator():
d.est &lt;- estimator(val, A, d, scales=fish)

#and plot the result:
lims &lt;- range(c(d,d.est))
par(pty="s")
plot(d, d.est, xaxs="r", yaxs="r", xlim=lims, ylim=lims)
abline(0,1)
  </code></pre>

<hr>
<h2 id='expert.estimates'>Expert estimates for Goldstein input parameters</h2><span id='topic+expert.estimates'></span>

<h3>Description</h3>

<p>A dataframe consisting of expert judgements  of low, best, and high
values for each of 19 variables that are used in the creation of
the <code>QWERTYgoin.*</code> files by <code>makeinputfiles()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(expert.estimates)</code></pre>


<h3>Format</h3>

<p>A data frame with 19 observations on the following 3 variables.
</p>

<dl>
<dt>low</dt><dd><p>a numeric vector: low estimate</p>
</dd>
<dt>best</dt><dd><p>a numeric vector: best estimate</p>
</dd>
<dt>high</dt><dd><p>a numeric vector: high estimate</p>
</dd>
</dl>



<h3>Details</h3>

<p>The rows correspond to the column names of <code>results.table</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(expert.estimates)
</code></pre>

<hr>
<h2 id='interpolant'>Interpolates between known points using Bayesian estimation</h2><span id='topic+interpolant'></span><span id='topic+interpolant.quick'></span><span id='topic+int.qq'></span>

<h3>Description</h3>

<p>Calculates the posterior distribution of results at a point using the
techniques outlined by Oakley.  Function <code>interpolant()</code> is the
primary function of the package.  Function <code>interpolant.quick()</code>
gives the expectation of the emulator at a set of points, and function
<code>interpolant()</code> gives the expectation and other information (such
as the variance) at a single point.  Function <code>int.qq()</code> gives a
quick-quick vectorized interpolant using certain timesaving
assumptions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>interpolant(x, d, xold, Ainv=NULL, A=NULL, use.Ainv=TRUE,
      scales=NULL, pos.def.matrix=NULL, func=regressor.basis,
      give.full.list = FALSE, distance.function=corr, ...)
interpolant.quick(x, d, xold, Ainv=NULL, scales=NULL,
pos.def.matrix=NULL, func=regressor.basis, give.Z = FALSE,
distance.function=corr, ...)
int.qq(x, d, xold, Ainv, pos.def.matrix, func=regressor.basis)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="interpolant_+3A_x">x</code></td>
<td>
<p>Point(s) at which estimation is desired.  For
<code>interpolant.quick()</code>, argument <code>x</code> is a matrix
and an expectation is given for each row</p>
</td></tr>
<tr><td><code id="interpolant_+3A_d">d</code></td>
<td>
<p>vector of observations, one for each row of <code>xold</code></p>
</td></tr>
<tr><td><code id="interpolant_+3A_xold">xold</code></td>
<td>
<p>Matrix with rows corresponding to points at which the
function is known</p>
</td></tr>
<tr><td><code id="interpolant_+3A_a">A</code></td>
<td>
<p>Correlation matrix <code>A</code>.  If not given, it is calculated</p>
</td></tr>
<tr><td><code id="interpolant_+3A_ainv">Ainv</code></td>
<td>
<p>Inverse of correlation matrix <code>A</code>.  Required by
<code>int.qq()</code>.  In <code>interpolant()</code> and <code>interpolant.quick()</code>
using the default value of <code>NULL</code> results in <code>Ainv</code> being
calculated explicitly (which may be slow: see next argument for more
details)</p>
</td></tr>
<tr><td><code id="interpolant_+3A_use.ainv">use.Ainv</code></td>
<td>
<p>Boolean, with default <code>TRUE</code> meaning to use the
inverse matrix <code>Ainv</code> (and, if necessary, calculate it using
<code>solve(.)</code>).  This requires the not inconsiderable overhead of
inverting a matrix.   If, however, <code>Ainv</code> is available, using
the default option is <em>much</em> faster than setting
<code>use.Ainv=FALSE</code>; see below.
</p>
<p>If <code>FALSE</code>, function <code>interpolant()</code> does not use
<code>Ainv</code>, but makes extensive use of <code>solve(A,x)</code>, mostly in
the form of <code>quad.form.inv()</code> calls.  This option avoids the
overhead of inverting a matrix, but has non-negligible marginal
costs.
</p>
<p>If <code>Ainv</code> is not available, there is little to choose, in terms
of execution time, between calculating it explicitly (that is,
setting <code>use.Ainv=TRUE</code>), and using <code>solve(A,x)</code> (ie
<code>use.Ainv=TRUE</code>).
</p>
<p><strong>Note:</strong> if <code>Ainv</code> is given to the function, but
<code>use.Ainv</code> is <code>FALSE</code>, the code will do as requested and use
the slow <code>solve(A,x)</code>, which is probably not what you want</p>
</td></tr>
<tr><td><code id="interpolant_+3A_func">func</code></td>
<td>
<p>Function used to determine basis vectors, defaulting
to <code>regressor.basis</code> if not given</p>
</td></tr>
<tr><td><code id="interpolant_+3A_give.full.list">give.full.list</code></td>
<td>
<p>In <code>interpolant()</code>, Boolean variable with
<code>TRUE</code> meaning to return the whole list of posterior
parameters as detailed on pp12-15 of Oakley, and default <code>FALSE</code>
meaning to return just the best estimate</p>
</td></tr>
<tr><td><code id="interpolant_+3A_scales">scales</code></td>
<td>
<p>Vector of &ldquo;roughness&rdquo; lengths used to calculate
<code>t(x)</code>, the correlations between <code>x</code> and the points in the
design matrix <code>xold</code>.
</p>
<p>Note that <code>scales</code> is needed twice overall: once to calculate
<code>Ainv</code>, and once to calculate <code>t(x)</code> inside
<code>interpolant()</code> (<code>t(x)</code> is determined by calling
<code>corr()</code> inside an <code>apply()</code> loop).  A good place to start
might be <code>scales=rep(1,ncol(xold))</code>.
</p>
<p>It's probably worth restating here that the elements of
<code>scales</code> correspond to the diagonal elements of the <code class="reqn">B</code>
matrix (see <code>?corr</code>) and so have the dimensions of
<code class="reqn">[D]^{-2}</code> where <code class="reqn">D</code> is the dimensions of
<code>xold</code></p>
</td></tr>
<tr><td><code id="interpolant_+3A_pos.def.matrix">pos.def.matrix</code></td>
<td>
<p>A positive definite matrix that is used if
<code>scales</code> is not supplied.  Note that precisely one of
<code>scales</code> and <code>pos.def.matrix</code> must be supplied</p>
</td></tr>
<tr><td><code id="interpolant_+3A_give.z">give.Z</code></td>
<td>
<p>In function <code>interpolant.quick()</code>, Boolean variable
with <code>TRUE</code> meaning to return the best estimate and the error,
and default <code>FALSE</code> meaning to return just the best estimate</p>
</td></tr>
<tr><td><code id="interpolant_+3A_distance.function">distance.function</code></td>
<td>
<p>Function to compute distances between
points, defaulting to <code>corr()</code>.  See <code>corr.Rd</code> for
details. Note that <code>method=2</code> or <code>method=3</code> is required
if a non-standard distance function is used</p>
</td></tr>
<tr><td><code id="interpolant_+3A_...">...</code></td>
<td>
<p>Further arguments passed to the distance function,
usually <code>corr()</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>In function <code>interpolant()</code>, if <code>give.full.list</code> is
<code>TRUE</code>, a list is returned with components
</p>
<table>
<tr><td><code>betahat</code></td>
<td>
<p>Standard MLE of the (linear) fit, given the
observations</p>
</td></tr>
<tr><td><code>prior</code></td>
<td>
<p>Estimate for the prior</p>
</td></tr>
<tr><td><code>sigmahat.square</code></td>
<td>
<p>Posterior estimate for variance</p>
</td></tr>
<tr><td><code>mstar.star</code></td>
<td>
<p>Posterior expectation</p>
</td></tr>
<tr><td><code>cstar</code></td>
<td>
<p>Prior correlation of a point with itself</p>
</td></tr>
<tr><td><code>cstar.star</code></td>
<td>
<p>Posterior correlation of a point with itself</p>
</td></tr>
<tr><td><code>Z</code></td>
<td>
<p>Standard deviation (although the distribution is actually a
t-distribution with <code class="reqn">n-q</code> degrees of freedom)</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Robin K. S. Hankin</p>


<h3>References</h3>


<ul>
<li>
<p>J. Oakley 2004. &ldquo;Estimating percentiles of uncertain
computer code outputs&rdquo;.  Applied Statistics, 53(1), pp89-93.
</p>
</li>
<li>
<p>J. Oakley 1999. &ldquo;Bayesian uncertainty analysis for complex
computer codes&rdquo;, PhD thesis, University of Sheffield.
</p>
</li>
<li>
<p>J. Oakley and A. O'Hagan, 2002. &ldquo;Bayesian Inference for the
Uncertainty Distribution of Computer Model Outputs&rdquo;, Biometrika
89(4), pp769-784
</p>
</li>
<li>
<p>R. K. S. Hankin 2005. &ldquo;Introducing BACCO, an R bundle for
Bayesian analysis of computer code output&rdquo;, Journal of Statistical
Software, 14(16)
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+makeinputfiles">makeinputfiles</a></code>,<code><a href="#topic+corr">corr</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'># example has 10 observations on 6 dimensions.
# function is just sum( (1:6)*x) where x=c(x_1, ... , x_2)

data(toy)
val &lt;- toy
real.relation &lt;- function(x){sum( (0:6)*x )}
H &lt;- regressor.multi(val)
d &lt;- apply(H,1,real.relation)
d &lt;- jitter(d,amount=1e-5)    # to prevent numerical problems

fish &lt;- rep(1,6)
fish[6] &lt;- 4

A &lt;- corr.matrix(val,scales=fish)
Ainv &lt;- solve(A)

# now add some suitably correlated noise to d:
d.noisy &lt;-  as.vector(rmvnorm(n=1, mean=d, 0.1*A))
names(d.noisy) &lt;- names(d)

# First try a value at which we know the answer (the first row of val):
x.known &lt;- as.vector(val[1,])
bayes.known &lt;- interpolant(x.known, d, val, Ainv=Ainv, scales=fish, g=FALSE)
print("error:")
print(d[1]-bayes.known)


# Now try the same value, but with noisy data:
print("error:")
print(d.noisy[1]-interpolant(x.known, d.noisy, val, Ainv=Ainv, scales=fish, g=FALSE))

#And now one we don't know:
x.unknown &lt;- rep(0.5 , 6)
bayes.unknown &lt;- interpolant(x.unknown, d.noisy, val, scales=fish, Ainv=Ainv,g=TRUE)

## [   compare with the "true" value of sum(0.5*0:6) = 10.5   ]



# Just a quickie for int.qq():
int.qq(x=rbind(x.unknown,x.unknown+0.1),d.noisy,val,Ainv,pos.def.matrix=diag(fish))


## (To find the best correlation lengths, use optimal.scales())

 # Now we use the SAME dataset but a different set of basis functions.
 # Here, we use the functional dependence of
 # "A+B*(x[1]&gt;0.5)+C*(x[2]&gt;0.5)+...+F*(x[6]&gt;0.5)".
 # Thus the basis functions will be c(1,x&gt;0.5).
 # The coefficients will again be 1:6.

       # Basis functions:
f &lt;- function(x){c(1,x&gt;0.5)}
       # (other examples might be
       # something like  "f &lt;- function(x){c(1,x&gt;0.5,x[1]^2)}"

       # now create the data
real.relation2 &lt;- function(x){sum( (0:6)*f(x) )}
d2 &lt;- apply(val,1,real.relation2)

       # Define a point at which the function's behaviour is not known:
x.unknown2 &lt;- rep(1,6)
       # Thus real.relation2(x.unknown2) is sum(1:6)=21

       # Now try the emulator:
interpolant(x.unknown2, d2, val, Ainv=Ainv, scales=fish, g=TRUE)$mstar.star
       # Heh, it got it wrong!  (we know that it should be 21)


       # Now try it with the correct basis functions:
interpolant(x.unknown2, d2, val, Ainv=Ainv,scales=fish, func=f,g=TRUE)$mstar.star
       # That's more like it.

       # We can tell that the coefficients are right by:
betahat.fun(val,Ainv,d2,func=f)
       # Giving c(0:6), as expected.

       # It's interesting to note that using the *wrong* basis functions
       # gives the *correct* answer when evaluated at a known point:
interpolant(val[1,], d2, val, Ainv=Ainv,scales=fish, g=TRUE)$mstar.star
real.relation2(val[1,])
       # Which should agree.


       # Now look at Z.  Define a function Z() which determines the
       # standard deviation at a point near a known point.
Z &lt;- function(o) {
    x &lt;- x.known 
    x[1] &lt;- x[1]+ o
    interpolant(x, d.noisy, val, Ainv=Ainv, scales=fish, g=TRUE)$Z
  } 

Z(0)       #should be zero because we know the answer (this is just Z at x.known)
Z(0.1)     #nonzero error.


  ## interpolant.quick() should  give the same results faster, but one
  ##   needs a matrix:
u &lt;- rbind(x.known,x.unknown)
interpolant.quick(u, d.noisy, val, scales=fish, Ainv=Ainv,g=TRUE)




# Now an example from climate science.  "results.table" is a dataframe
# of goldstein (a climate model) results.  Each of its 100 rows shows a
# point in parameter space together with certain key outputs from the
# goldstein program.  The following R code shows how we can set up an
# emulator based on the first 27 goldstein runs, and use the emulator to
# predict the output for the remaining 73 goldstein runs.  The results
# of the emulator are then plotted on a scattergraph showing that the
# emulator is producing estimates that are close to the "real" goldstein
# runs.


data(results.table)
data(expert.estimates)

       # Decide which column we are interested in:
output.col &lt;- 26

       # extract the "important" columns:
wanted.cols &lt;- c(2:9,12:19)

       # Decide how many to keep;
       # 30-40 is about the most we can handle:
wanted.row &lt;- 1:27

       # Values to use are the ones that appear in goin.test2.comments:
val &lt;- results.table[wanted.row , wanted.cols]

       # Now normalize val so that 0&lt;results.table[,i]&lt;1 is 
       # approximately true for all i:

normalize &lt;- function(x){(x-mins)/(maxes-mins)}
unnormalize &lt;- function(x){mins + (maxes-mins)*x}

mins  &lt;- expert.estimates$low 
maxes &lt;- expert.estimates$high
jj &lt;- t(apply(val,1,normalize))

jj &lt;- as.data.frame(jj) 
names(jj) &lt;- names(val)
val &lt;- jj


       ## The value we are interested in  is the 19th (or 20th or ... or 26th) column.
d  &lt;- results.table[wanted.row ,  output.col]

       ##  Now some scales, estimated earlier from the data using
       ##   optimal.scales():

scales.optim &lt;- exp(c( -2.917, -4.954, -3.354, 2.377, -2.457, -1.934, -3.395,
-0.444, -1.448, -3.075, -0.052, -2.890, -2.832, -2.322, -3.092, -1.786))

A &lt;- corr.matrix(val,scales=scales.optim, method=2)
Ainv &lt;-  solve(A)

print("and plot points used in optimization:")
d.observed &lt;- results.table[ , output.col]

A &lt;- corr.matrix(val,scales=scales.optim, method=2)
Ainv &lt;- solve(A)

print("now plot all points:")
design.normalized &lt;- as.matrix(t(apply(results.table[,wanted.cols],1,normalize)))
d.predicted &lt;- interpolant.quick(design.normalized , d , val , Ainv=Ainv,
scales=scales.optim)
jj &lt;- range(c(d.observed,d.predicted))
par(pty="s")
plot(d.observed, d.predicted, pch=16, asp=1,
xlim=jj,ylim=jj,
xlab=expression(paste(temperature," (",{}^o,C,"), model"   )),
ylab=expression(paste(temperature," (",{}^o,C,"), emulator"))
)
abline(0,1)
</code></pre>

<hr>
<h2 id='latin.hypercube'>Latin hypercube design matrix</h2><span id='topic+latin.hypercube'></span>

<h3>Description</h3>

<p>Gives a Latin hypercube design matrix with an arbitrary number of
points in an arbitrary number of dimensions.  The <code>toy</code> dataset
was generated using <code>latin.hypercube()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>latin.hypercube(n, d, names=NULL, normalize=FALSE, complex=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="latin.hypercube_+3A_n">n</code></td>
<td>
<p>Number of points</p>
</td></tr>
<tr><td><code id="latin.hypercube_+3A_d">d</code></td>
<td>
<p>Number of dimensions</p>
</td></tr>
<tr><td><code id="latin.hypercube_+3A_names">names</code></td>
<td>
<p>Character vector for column names (optional)</p>
</td></tr>
<tr><td><code id="latin.hypercube_+3A_normalize">normalize</code></td>
<td>
<p>Boolean variable with <code>TRUE</code> meaning to
normalize each column so the minimum is zero and the maximum is
one.  If it takes its default <code>FALSE</code>, the points represent
midpoints of <code class="reqn">n</code> equispaced intervals; the points thus have a minimum of
<code class="reqn">0.5/n</code> and a maximum of <code class="reqn">1-0.5/n</code></p>
</td></tr>
<tr><td><code id="latin.hypercube_+3A_complex">complex</code></td>
<td>
<p>Boolean with default <code>FALSE</code> meaning to return a
complex latin hypercube in which real and imaginary components separately
form a latin hypercube</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Robin K. S. Hankin</p>


<h3>Examples</h3>

<pre><code class='language-R'>#10 points, 6 dimensions:
(latin.hypercube(10,6) -&gt; x)
plot(as.data.frame(x))

latin.hypercube(10,2,complex=TRUE)

</code></pre>

<hr>
<h2 id='makeinputfiles'>Makes input files for condor runs of goldstein</h2><span id='topic+makeinputfiles'></span><span id='topic+sample.from.exp.est'></span>

<h3>Description</h3>

<p>Wrapper to create arbitrary numbers of condor-compatible goldstein
runnable input files.  Function <code>sample.from.exp.est()</code> samples from the
appropriate distribution.
</p>
<p>This function is not designed for the general user: it is tailored for
use in the environment of the National Oceanographic Centre, with a
particular version of the specialist model &ldquo;goldstein&rdquo;.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>makeinputfiles(number.of.runs = 100, gaussian = TRUE,
   directoryname="~/goldstein/genie-cgoldstein/", filename="QWERTYgoin",
   expert.estimates, area.outside=0.05)
sample.from.exp.est(number.of.runs, expert.estimates,
   gaussian=TRUE, area.outside=0.05)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="makeinputfiles_+3A_number.of.runs">number.of.runs</code></td>
<td>
<p>Number of condor runs to generate</p>
</td></tr>
<tr><td><code id="makeinputfiles_+3A_gaussian">gaussian</code></td>
<td>
<p>Boolean variable with default TRUE meaning use a lognormal
distribution, and FALSE meaning a uniform distribution.  In the case
of a Gaussian distribution, only the <code>upper</code> and <code>lower</code>
columns are used: here these values are interpreted as the
<code class="reqn">2.5\%\rm{ile}</code> and <code class="reqn">97.5\%\rm{ile}</code>
respectively and a lognormal distribution with the appropriate
parameters is used.
</p>
<p>Note that this approach discards the &ldquo;best&rdquo; value, but OTOH it
seemed to me that my expert chose his &ldquo;best&rdquo; value as an arithmetic
(sic) mean of his high and low values, and thus has limited
information content.
</p>
</td></tr>
<tr><td><code id="makeinputfiles_+3A_directoryname">directoryname</code></td>
<td>
<p>Name of directory to which input files are
written</p>
</td></tr>
<tr><td><code id="makeinputfiles_+3A_filename">filename</code></td>
<td>
<p>Basename of input files</p>
</td></tr>
<tr><td><code id="makeinputfiles_+3A_expert.estimates">expert.estimates</code></td>
<td>
<p>Dataframe holding expert estimates (supplied
by a climate scientist).   Use <code>data(expert.estimates)</code> to load
a sample dataset that was supplied by Bob Marsh</p>
</td></tr>
<tr><td><code id="makeinputfiles_+3A_area.outside">area.outside</code></td>
<td>
<p>Area of tails of the lognormal distribution (on a
log scale) that fall  outside the expert ranges.  Default value of 0.05
means interpret <code>a</code> and <code>b</code> as the <code class="reqn">2.5\%\rm{ile}</code>
and <code class="reqn">97.5\%\rm{ile}</code> respectively.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function creates condor-compatible goldstein
runnable input files that are placed in directory
<code>/working/jrd/sat/rksh/goldstein</code>.  The database
<code>results.table</code> is made using the shell scripts currently in
<code>/users/sat/rksh/goldstein/emulator</code>.
</p>
<p>Note that <code>makeinputfiles(number.of.runs=n)</code> creates files
numbered from <code class="reqn">0</code> to <code class="reqn">n-1</code>: so be careful of
off-by-one errors.  It's probably best to avoid reference to the
&ldquo;first&rdquo;, &ldquo;second&rdquo; file etc.  Instead, refer to files
using their suffix number.  Note that the suffix number is not padded
with zeros due to the requirements of Condor.
</p>
<p>The suffix number of a file matches the name of its <code>tmp</code> file
(so, for example, file with suffix number 15 writes output to files
<code>tmp/tmp.15</code> and <code>tmp/tmp.avg.15</code>).
</p>


<h3>Value</h3>

<p>Returns zero on successful completion.  The
function is used for its side-effect of creating a bunch of Goldstein
input files.  </p>


<h3>Author(s)</h3>

<p>Robin K. S. Hankin</p>


<h3>See Also</h3>

<p><code><a href="#topic+expert.estimates">expert.estimates</a></code>, <code><a href="#topic+results.table">results.table</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  data(expert.estimates) system("mkdir /users/sat/rksh/tmp",ignore=TRUE)
  makeinputfiles(number.of.runs = 100, gaussian = TRUE,
  directoryname="~/tmp/", expert.estimate=expert.estimates)

## End(Not run)
data(results.table)
data(expert.estimates)


output.col &lt;- 25
wanted.row &lt;- 1:27
wanted.cols &lt;- c(2:9,12:19)

val &lt;- results.table[wanted.row , wanted.cols]

mins  &lt;- expert.estimates$low 
maxes &lt;- expert.estimates$high

normalize &lt;- function(x){(x-mins)/(maxes-mins)}
unnormalize &lt;- function(x){mins + (maxes-mins)*x}

jj &lt;- t(apply(val,1,normalize))

jj &lt;- as.data.frame(jj)
names(jj) &lt;- names(val)
val &lt;- as.matrix(jj)

scales.optim &lt;- exp(c( -2.63, -3.03, -2.24, 2.61,
-1.65, -3.13, -3.52, 3.16, -3.32, -2.53, -0.25,  -2.55, -4.98, -1.59,
-4.40, -0.81))

d  &lt;- results.table[wanted.row ,  output.col]
A &lt;- corr.matrix(val, scales=scales.optim)
Ainv &lt;- solve(A)

x &lt;- sample.from.exp.est(1000,exp=expert.estimates)
x &lt;- t(apply(x,1,normalize))
ensemble &lt;- interpolant.quick(x , d , val , Ainv, scales=scales.optim) 
hist(ensemble)
</code></pre>

<hr>
<h2 id='model'>Simple model for concept checking</h2><span id='topic+model'></span>

<h3>Description</h3>

<p>Oakley's simple model, used as an example for proof-of-concept
</p>


<h3>Usage</h3>

<pre><code class='language-R'>model(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="model_+3A_x">x</code></td>
<td>
<p>Input argument</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Robin K. S. Hankin</p>


<h3>Examples</h3>

<pre><code class='language-R'>model(seq(0,0.1,10))
</code></pre>

<hr>
<h2 id='OO2002'>Implementation of the ideas of Oakley and O'Hagan 2002</h2><span id='topic+OO2002'></span><span id='topic+oo2002'></span><span id='topic+cond.sample'></span><span id='topic+var.conditional'></span>

<h3>Description</h3>

<p>Implementation of the ideas of Oakley and O'Hagan 2002:
<code>var.conditional()</code> calculates the conditional variance-covariance
matrix, and <code>cond.sample()</code> samples from the appropriate
multivariate t distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cond.sample(n = 1, x, xold, d, A, Ainv, scales = NULL, pos.def.matrix =
NULL, func = regressor.basis, ...)
var.conditional(x, xold, d, A, Ainv, scales = NULL, pos.def.matrix = NULL, 
    func = regressor.basis, distance.function = corr, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="OO2002_+3A_n">n</code></td>
<td>
<p>In function <code>cond.sample()</code>, the number of observations
to take, defaulting to 1</p>
</td></tr>
<tr><td><code id="OO2002_+3A_x">x</code></td>
<td>
<p>Simulation design points</p>
</td></tr>
<tr><td><code id="OO2002_+3A_xold">xold</code></td>
<td>
<p>Design points</p>
</td></tr> 
<tr><td><code id="OO2002_+3A_d">d</code></td>
<td>
<p>Data vector</p>
</td></tr>
<tr><td><code id="OO2002_+3A_a">A</code></td>
<td>
<p>Correlation matrix</p>
</td></tr>
<tr><td><code id="OO2002_+3A_ainv">Ainv</code></td>
<td>
<p>Inverse of correlation matrix <code class="reqn">A</code></p>
</td></tr>
<tr><td><code id="OO2002_+3A_scales">scales</code></td>
<td>
<p>Roughness lengths</p>
</td></tr>
<tr><td><code id="OO2002_+3A_pos.def.matrix">pos.def.matrix</code></td>
<td>
<p>Positive definite matrix of correlations</p>
</td></tr>
<tr><td><code id="OO2002_+3A_func">func</code></td>
<td>
<p>Function to calculate <code class="reqn">H</code></p>
</td></tr>
<tr><td><code id="OO2002_+3A_distance.function">distance.function</code></td>
<td>
<p>Distance function (defaulting to <code>corr()</code>)</p>
</td></tr>
<tr><td><code id="OO2002_+3A_...">...</code></td>
<td>
<p>Further arguments passed to the distance function, usually <code>corr()</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>We wish to generate the distribution for the process at uncertain
point <code>x</code>; uncertainty in <code>x</code> is captured by assuming it to
be drawn from a pdf <code>X</code>.
</p>
<p>The basic idea is to estimate <code class="reqn">m^*</code> at <em>simulated</em> design
points using <code>cond.sample()</code>, which samples from the multivariate
t distribution conditional on the data <code>d</code> at the design points.
The random datavector of estimates <code class="reqn">m^*</code> is called <code>ddash</code>.
</p>
<p>We repeat this process many times, each time estimating
<code class="reqn">\eta(\cdot)</code> using the augmented dataset
<code>c(d,ddash)</code> as a training set.
</p>
<p>For each estimated <code class="reqn">\eta(\cdot)</code>, we have a complete
emulator that can be used to build up an ensemble of estimates.  
</p>


<h3>Value</h3>

<p>Function <code>cond.sample()</code> returns a <code class="reqn">n\times p</code> matrix
whose rows are independent samples from the appropriate multivariate
<code class="reqn">t</code> distribution.  Here, <code class="reqn">p</code> is the number of rows of <code>x</code>
(ie the number of simulated design points).  Consider a case where there
are just two simulated design points, close to each other but far from
any point of the original design points.  Then function
<code>cond.sample(n=4, ...)</code> will give four numbers which are close to
one another but have high (between-instantiation) variance.
</p>
<p>Function <code>var.conditional()</code> calculates the denominator of equation
3 of Oakley and OHagan 2002.  This function is intended to be called by
<code>cond.sample()</code> but might be interesting <em>per se</em> when
debugging or comparing different choices of simulated design points.
</p>


<h3>Note</h3>

<p>Function <code>cond.sample()</code> and <code>var.conditional()</code> together
are a superset of function <code>interpolant()</code> because it accounts
for covariance between multiple observations.  It is, however, much
slower.
</p>
<p>Also note that these functions are used to good effect in the examples
section of <code>oo2002.Rd</code>.
</p>


<h3>Author(s)</h3>

<p>Robin K. S. Hankin</p>


<h3>References</h3>


<ul>
<li>
<p>J. Oakley 2002. <em>Bayesian inference for the
uncertainty distribution of computer model outputs</em>.  Biometrika,
89(4):769&ndash;784
</p>
</li>
<li>
<p>R. K. S. Hankin 2005. <em>Introducing BACCO, an R bundle for
Bayesian analysis of computer code output</em>, Journal of Statistical
Software, 14(16)
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+regressor.basis">regressor.basis</a></code>, for a more visually informative
example of <code>cond.sample()</code> et seq; and <code><a href="#topic+interpolant">interpolant</a></code>
for more examples
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

     # Now we use the functions.  First we set up some design points:
     # Suppose we are given the toy dataset and want to know the PDF of
     # fourth power of the response at point x, where uncertainty in x
     # may be represented as it being drawn from a normnl distribution
     # with mean c(0.5,0.5,...,0.5) and a variance of 0.001.

data(toy)
val &lt;- toy
real.relation &lt;- function(x){sum( (0:6)*x )}
H &lt;- regressor.multi(val)
d &lt;- apply(H,1,real.relation)

     # and some scales (which are assumed to be known):
fish &lt;- rep(1,6)
fish[6] &lt;- 4

     # And determine A and Ainv:
A &lt;- corr.matrix(val,scales=fish)
Ainv &lt;- solve(A)

     # and add some suitably correlated Gaussian noise:
d.noisy &lt;-  as.vector(rmvnorm(n=1, mean=d, 0.1*A))
names(d.noisy) &lt;- names(d)

     # Now some simulation design points.  Choose n'=6:
xdash &lt;-  matrix(runif(36),ncol=6)

     # And just for fun, we insert a near-useless seventh simulation
     # design point (it is nearly useless because it is a near copy of
     # the sixth).  We do this in order to test the coding:
xdash &lt;- rbind(xdash,xdash[6,] + 1e-4)
colnames(xdash) &lt;- colnames(val)
rownames(xdash) &lt;- c("alpha","beta","gamma","delta","epsilon","zeta","zeta.copy")

     # Print the variance matrix:
(vm &lt;- var.conditional(x=xdash,xold=val,d=d.noisy,A=A,Ainv=Ainv,scales=fish))
     # Note that the sixth and seventh columns are almost identical
     # (and so, therefore, are the sixth and seventh rows) as
     # expected.

     # Also, the final eigenvalue of vm should be small:
eigen(vm)$values


     # Now sample from the conditional t-distribution.  Taking n=3 samples:
(cs &lt;- cond.sample(n=3, x=xdash, xold=val, d=d.noisy, A=A, Ainv=Ainv,
                   scales = fish, func = regressor.basis))

     # Note the last two columns are nearly identical, as expected.

     # Just as a test, what is the variance matrix at the design points?
(vc &lt;- var.conditional(x=val,xold=val,d=d.noisy,A=A,Ainv=Ainv,scales=fish))
     # (This should be exactly zero);
max(eigen(vc)$values)
     # should be small

     # Next, we apply the methods of OO2002 using Monte Carlo techniques.
     # We will generate  10 different versions of eta:
number.of.eta &lt;- 10

     # And, for each eta,  we will sample from the posterior t distribution 11 times:
number.of.X &lt;- 11


     # create an augmented design matrix, of the design points plus the
     # simulated design points:
design.augmented &lt;- rbind(val,xdash)
A.augmented &lt;- corr.matrix(design.augmented, scales=fish)
Ainv.augmented &lt;- solve(A.augmented)

out &lt;- NULL
for(i in seq_len(number.of.eta)){
        # Create random data by sampling from the conditional
        # multivariate t at the simulated design points xdash, from
        # the t-distribution given the data d:
   ddash &lt;- cond.sample(n=1, x=xdash, xold=val, d=d.noisy, Ainv=Ainv, scales=fish)

        # Now use the emulator to calculate m^* at points chosen from
        # the PDF of X:
   jj &lt;-
     interpolant.quick(x=rmvnorm(n=number.of.X,rep(0.5,6),diag(6)/1000),
                       d=c(d.noisy,ddash),
                       xold=design.augmented,
                       Ainv=Ainv.augmented,
                       scales=fish)
   out &lt;- c(out,jj)
}

   # histogram of the fourth power:
hist(out^4, col="gray")
   # See oo2002 for another example of cond.sample() in use
</code></pre>

<hr>
<h2 id='optimal.scales'>Use optimization techniques to find the optimal scales</h2><span id='topic+optimal.scales'></span><span id='topic+optimal.scale'></span>

<h3>Description</h3>

<p>Uses optimization techniques (either Nelder-Mead or simulated annealing)
to find the optimal scales, or roughness lengths.  Function
<code>optimal.scale()</code> (ie singular) finds the optimal scale on the
assumption that the roughness is isotropic so all scales are identical.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optimal.scales(val, scales.start, d, use.like = TRUE,  give.answers =
FALSE, func=regressor.basis, ...)
optimal.scale(val, d, use.like = TRUE,  give.answers =
FALSE, func=regressor.basis, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="optimal.scales_+3A_val">val</code></td>
<td>
<p>Matrix with rows corresponding to points at which the
function is known</p>
</td></tr>
<tr><td><code id="optimal.scales_+3A_scales.start">scales.start</code></td>
<td>
<p>Initial guess for the scales (plural).  See details
section for explanation</p>
</td></tr>
<tr><td><code id="optimal.scales_+3A_d">d</code></td>
<td>
<p>vector of observations, one for each row of <code>val</code></p>
</td></tr>
<tr><td><code id="optimal.scales_+3A_use.like">use.like</code></td>
<td>
<p>Boolean, with default <code>TRUE</code> meaning to use
likelihood for the objective function, and <code>FALSE</code>
meaning to use a leave-out-one bootstrap estimator</p>
</td></tr>
<tr><td><code id="optimal.scales_+3A_give.answers">give.answers</code></td>
<td>
<p>Boolean, with default <code>FALSE</code> meaning to
return just the roughness lengths and <code>TRUE</code> meaning to return
extra information as returned by <code>optim()</code></p>
</td></tr>
<tr><td><code id="optimal.scales_+3A_func">func</code></td>
<td>
<p>Function used to determine basis vectors, defaulting
to <code>regressor.basis</code> if not given</p>
</td></tr>
<tr><td><code id="optimal.scales_+3A_...">...</code></td>
<td>
<p>Extra parameters passed to <code>optim()</code> or
<code>optimize()</code>. See examples for usage of this argument</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Internally, this function works with the logarithms of the roughness
lengths, because they are inherently positive.  However, note that the
lengths themselves must be supplied to argument <code>scales.start</code>,
not their logarithms.
</p>
<p>The reason that there are two separate functions is that
<code>optim()</code> and <code>optimize()</code> are very different. 
</p>


<h3>Value</h3>

<p>If <code>give.answers</code> takes the default value of <code>FALSE</code>, a
vector of roughness lengths is returned.  If <code>TRUE</code>, output from
<code>optim()</code> is returned directly (note that element <code>par</code> is
the logarithm of the desired roughness length as the optimization
routine operates with the logs of the lengths as detailed above) 
</p>


<h3>Note</h3>

<p>This function is slow to evaluate because it needs to
calculate and invert <code>A</code> each time it is called, because the
scales change from call to call.
</p>
<p>In this package, &ldquo;scales&rdquo; means the diagonal elements of the
<code class="reqn">B</code> matrix.  See the help page for <code>corr</code> for more
discussion of this topic.
</p>
<p>Note the warning about partial matching under the &ldquo;dot-dot-dot&rdquo;
argument in both <code>optim.Rd</code> [used in <code>optimal.scales()</code>] and
<code>optimize.Rd</code> [used in <code>optimal.scale()</code>]: any unmatched
arguments will be passed to the objective function.  Thus, passing
named but unmatched arguments to <code>optimal.scale[s]()</code> will cause
an error, because those arguments will be passed, by <code>optim()</code> or
<code>optimize()</code>, to the (internal) <code>objective.fun()</code>.
</p>
<p>In particular, note that passing <code>control=list(maxit=4)</code> to
<code>optimal.scale()</code> will cause an error for this reason
[<code>optimize()</code> does not take a <code>control</code> argument].
</p>


<h3>Author(s)</h3>

<p>Robin K. S. Hankin</p>


<h3>References</h3>


<ul>
<li>
<p>J. Oakley 2004. <em>Estimating percentiles of uncertain
computer code outputs</em>.  Applied Statistics, 53(1), pp89-93.
</p>
</li>
<li>
<p>J. Oakley 1999. <em>Bayesian uncertainty analysis for complex
computer codes</em>, PhD thesis, University of Sheffield.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+interpolant">interpolant</a></code>,<code><a href="#topic+corr">corr</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>
##First, define some scales:
fish &lt;- c(1,1,4)

## and a sigmasquared value:
REAL.SIGMASQ &lt;- 0.3

## and a real relation:
real.relation &lt;- function(x){sum( (1:3)*x )}

## Now a design matrix:
val  &lt;- latin.hypercube(7,3)

## apply the real relation:
d &lt;- apply(val,1,real.relation)

## and add some suitably correlated Gaussian noise:
A &lt;- corr.matrix(val,scales=fish)
d.noisy &lt;-  as.vector(rmvnorm(n=1,mean=apply(val,1,real.relation),REAL.SIGMASQ*A))

##  Now see if we can estimate the roughness lengths well.  Remember that
##  the true values are those held in vector "fish":

optimal.scales(val=val, scales.start=rep(1,3), d=d.noisy,
       method="SANN",control=list(trace=1000,maxit=3),
       give=FALSE)



# Now a test of optimal.scale(), where there is only a single roughness
#  scale to estimate.  This should be more straightforward:


df &lt;- latin.hypercube(7,3)
fish2 &lt;- rep(2,3)
A2 &lt;- corr.matrix(df,scales=fish2)
d.noisy &lt;- as.vector(rmvnorm(n=1, mean=apply(df,1,real.relation), sigma=A2))

jj.T &lt;- optimal.scale(val=df,d=d.noisy,use.like=TRUE)
jj.F &lt;- optimal.scale(val=df,d=d.noisy,use.like=FALSE)

</code></pre>

<hr>
<h2 id='pad'>Simple pad function</h2><span id='topic+pad'></span>

<h3>Description</h3>

<p>Places zeros to the left of a string.  If the string
consists only of digits 0-9, <code>pad()</code> does not change the value of
the string if interpreted as a numeric.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pad(x,len,padchar="0",strict=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pad_+3A_x">x</code></td>
<td>
<p>Input argument (converted to character)</p>
</td></tr>
<tr><td><code id="pad_+3A_len">len</code></td>
<td>
<p>Desired length of output</p>
</td></tr>
<tr><td><code id="pad_+3A_padchar">padchar</code></td>
<td>
<p>Character to pad x with, defaulting to &ldquo;0&rdquo;</p>
</td></tr>
<tr><td><code id="pad_+3A_strict">strict</code></td>
<td>
<p>Boolean variable governing the behaviour when length of
<code>x</code> is less than <code>len</code>.  Under these circumstances, if
<code>strict</code> takes the default value of <code>TRUE</code>, then
return an error; if <code>FALSE</code>, return a truncated version of <code>x</code>
(least significant characters retained)</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Robin K. S. Hankin</p>


<h3>Examples</h3>

<pre><code class='language-R'>pad("1234",len=10)
pad("1234",len=3,strict=FALSE)
</code></pre>

<hr>
<h2 id='prior.b'>Prior linear fits</h2><span id='topic+prior.b'></span><span id='topic+prior.B'></span>

<h3>Description</h3>

<p>Gives the fitted regression coefficients corresponding to the
specified regression model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prior.b(H, Ainv, d, b0 = NULL, B0 = NULL)
prior.B(H , Ainv , B0=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prior.b_+3A_h">H</code></td>
<td>
<p>Regression basis function (eg that returned by <code>regressor.multi()</code>)</p>
</td></tr>
<tr><td><code id="prior.b_+3A_ainv">Ainv</code></td>
<td>
<p><code class="reqn">A^{-1}</code> where <code class="reqn">A</code> is a correlation matrix  (eg that
returned by <code>corr.matrix()</code>)</p>
</td></tr>
<tr><td><code id="prior.b_+3A_d">d</code></td>
<td>
<p>Vector of data points</p>
</td></tr>
<tr><td><code id="prior.b_+3A_b0">b0</code></td>
<td>
<p>prior constant</p>
</td></tr>
<tr><td><code id="prior.b_+3A_b0">B0</code></td>
<td>
<p>prior coefficients</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Robin K. S. Hankin</p>


<h3>References</h3>


<ul>
<li>
<p>J. Oakley 2004. <em>Estimating percentiles of uncertain
computer code outputs</em>.  Applied Statistics, 53(1), pp89-93.
</p>
</li>
<li>
<p>J. Oakley 1999. <em>Bayesian uncertainty analysis for complex
computer codes</em>, PhD thesis, University of Sheffield.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
# example has 10 observations on 6 dimensions.
# function is just sum( (1:6)*x) where x=c(x_1, ... , x_2)

data(toy)
val &lt;- toy
d &lt;- apply(val,1,function(x){sum((1:6)*x)})

#add some noise:
d &lt;- jitter(d)

A &lt;- corr.matrix(val,scales=rep(1,ncol(val)))
Ainv &lt;- solve(A)
H &lt;- regressor.multi(val)

prior.b(H,Ainv,d)
prior.B(H,Ainv)
</code></pre>

<hr>
<h2 id='quad.form'>Evaluate a quadratic form efficiently</h2><span id='topic+quad.form'></span><span id='topic+quad.form.inv'></span><span id='topic+quad.3form'></span><span id='topic+quad.3form.inv'></span><span id='topic+quad.tform'></span><span id='topic+quad.3tform'></span><span id='topic+quad.tform.inv'></span><span id='topic+quad.diag'></span><span id='topic+quad.tdiag'></span><span id='topic+quad.3diag'></span><span id='topic+quad.3tdiag'></span><span id='topic+cprod'></span><span id='topic+tcprod'></span><span id='topic+ht'></span>

<h3>Description</h3>

<p>Given a square matrix <code class="reqn">M</code> of size <code class="reqn">n\times n</code>, and a
matrix <code class="reqn">x</code> of size <code class="reqn">n\times p</code> (or a vector of length
<code class="reqn">n</code>), evaluate various quadratic forms.
</p>
<p>(in the following, <code class="reqn">x^T</code> denotes the complex conjugate of
the transpose, also known as the Hermitian transpose.  This only
matters when considering complex numbers).
</p>

<ul>
<li><p> Function <code>quad.form(M,x)</code> evaluates <code class="reqn">x^TMx</code> in an efficient manner
</p>
</li>
<li><p> Function <code>quad.form.inv(M,x)</code> returns <code class="reqn">x^TM^{-1}x</code> using an efficient method that avoids
inverting <code class="reqn">M</code>
</p>
</li>
<li><p> Function <code>quad.tform(M,x)</code> returns <code class="reqn">xMx^T</code> using <code>tcrossprod()</code> without taking
a transpose
</p>
</li>
<li><p> Function <code>quad.tform.inv(M,x)</code> returns <code class="reqn">xM^{-1}x^T</code>, although a single transpose is needed
</p>
</li>
<li><p> Function <code>quad.3form(M,l,r)</code> returns <code class="reqn">l^TMr</code> using nested calls to <code>crossprod()</code>.  It's
no faster than calling <code>crossprod()</code> directly, but makes code
neater and less error-prone (IMHO)
</p>
</li>
<li><p> Function <code>quad.3form.inv(M,l,r)</code> returns
<code class="reqn">l^TM^{-1}r</code>
</p>
</li>
<li><p> Function <code>quad.3tform(M,l,r)</code> returns <code class="reqn">lMr^T</code> using nested calls to <code>tcrossprod()</code>.  Again,
this is to make for neater code
</p>
</li>
<li><p> Function <code>quad.diag(M,x)</code> returns the <em>diagonal</em> of
the (potentially very large) square matrix <code>quad.form(M,x)</code>
without calculating the off diagonal elements
</p>
</li>
<li><p> Function <code>quad.tdiag(M,x)</code> similarly returns the diagonal of
<code>quad.tform(M,x)</code>
</p>
</li>
<li><p> Function <code>quad.3diag(M,l,r)</code> returns the diagonal of
<code>quad.3form(M,l,r)</code> 
</p>
</li>
<li><p> Function <code>quad.3tdiag(M,l,r)</code> returns the diagonal of
<code>quad.3tform(M,l,r)</code> 
</p>
</li></ul>

<p>These functions invoke the following lower-level calls:
</p>

<ul>
<li><p> Function <code>ht(x)</code> returns the Hermitian transpose, that is,
the complex conjugate of the transpose, sometimes written <code class="reqn">x^*</code>
</p>
</li>
<li><p> Function <code>cprod(x,y)</code> returns <code class="reqn">x^T y</code>,
equivalent to <code>crossprod(Conj(x),y)</code>
</p>
</li>
<li><p> Function <code>tcprod(x,y)</code> returns <code class="reqn">x y^T</code>,
equivalent to <code>crossprod(x,Conj(y))</code> </p>
</li></ul>

<p>Note again that in the calls above, &ldquo;transpose&rdquo; [that is,
<code class="reqn">x^T</code>] means &ldquo;Conjugate transpose&rdquo;, or the Hermitian
transpose.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>quad.form(M, x, chol=FALSE)
quad.form.inv(M, x)
quad.tform(M, x)
quad.3form(M,left,right)
quad.3tform(M,left,right)
quad.tform.inv(M,x)
quad.diag(M,x)
quad.tdiag(M,x)
quad.3diag(M,left,right)
quad.3tdiag(M,left,right)
cprod(x,y)
tcprod(x,y)
ht(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="quad.form_+3A_m">M</code></td>
<td>
<p>Square matrix of size <code class="reqn">n\times n</code></p>
</td></tr>
<tr><td><code id="quad.form_+3A_x">x</code>, <code id="quad.form_+3A_y">y</code></td>
<td>
<p>Matrix of size <code class="reqn">n\times p</code>, or vector of length <code class="reqn">n</code></p>
</td></tr>
<tr><td><code id="quad.form_+3A_chol">chol</code></td>
<td>
<p>Boolean, with <code>TRUE</code> meaning to interpret
argument <code>M</code> as the lower triangular Cholesky decomposition
of the quadratic form.  Remember that 
<code>M.lower %*% M.upper == M</code>,  and <code>chol()</code> returns the
upper triangular matrix, so one needs to use the transpose
<code>t(chol(M))</code></p>
</td></tr>
<tr><td><code id="quad.form_+3A_left">left</code>, <code id="quad.form_+3A_right">right</code></td>
<td>
<p>In function <code>quad.3form()</code>, matrices with
<code class="reqn">n</code> rows and arbitrary number of columns</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The &ldquo;meat&rdquo; of <code>quad.form()</code> for <code>chol=FALSE</code> is just
<code>crossprod(crossprod(M, x), x)</code>, and that of
<code>quad.form.inv()</code> is <code>crossprod(x, solve(M, x))</code>.
</p>
<p>If the Cholesky decomposition of <code>M</code> is available, then calling
with <code>chol=TRUE</code> and supplying <code>M.upper</code> should generally be
faster (for large matrices) than calling with <code>chol=FALSE</code> and
using <code>M</code> directly.  The time saving is negligible for matrices
smaller than about <code class="reqn">50\times 50</code>, even if the overhead of
computing <code>M.upper</code> is ignored.
</p>


<h3>Note</h3>

<p>These functions are used extensively in the emulator and
calibrator packages' R code, primarily in the interests of elegant
code, but also speed.  For the problems I usually consider, the
speedup (of <code>quad.form(M,x)</code> over <code>t(x) %*% M %*% x</code>,
say) is marginal at best.
</p>


<h3>Author(s)</h3>

<p>Robin K. S. Hankin</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+optimize">optimize</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>jj &lt;- matrix(rnorm(80),20,4)
M &lt;- crossprod(jj,jj)
M.lower &lt;- t(chol(M))
x &lt;- matrix(rnorm(8),4,2)

jj.1 &lt;- t(x) %*% M %*% x
jj.2 &lt;- quad.form(M,x)
jj.3 &lt;- quad.form(M.lower,x,chol=TRUE)
print(jj.1)
print(jj.2)
print(jj.3)




## Make two Hermitian positive-definite matrices:
L &lt;- matrix(c(1,0.1i,-0.1i,1),2,2)
LL &lt;- diag(11)
LL[2,1] &lt;- -(LL[1,2] &lt;- 0.1i)


z &lt;- t(latin.hypercube(11,2,complex=TRUE))


quad.diag(L,z)     # elements real because L is HPD
quad.tdiag(LL,z)   # ditto




## Now consider accuracy:
quad.form(solve(M),x) - quad.form.inv(M,x)  # should be zero
quad.form(M,x) - quad.tform(M,t(x))         # should be zero
quad.diag(M,x) - diag(quad.form(M,x))       # should be zero
diag(quad.form(L,z))   - quad.diag(L,z)     # should be zero
diag(quad.tform(LL,z)) - quad.tdiag(LL,z)   # should be zero
</code></pre>

<hr>
<h2 id='regressor.basis'>Regressor basis function</h2><span id='topic+regressor.basis'></span><span id='topic+regressor.multi'></span>

<h3>Description</h3>

<p>Creates a regressor basis for a vector. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>regressor.basis(x)
regressor.multi(x.df,func=regressor.basis)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="regressor.basis_+3A_x">x</code></td>
<td>
<p>vector of coordinates</p>
</td></tr>
<tr><td><code id="regressor.basis_+3A_x.df">x.df</code></td>
<td>
<p>Matrix whose rows are coordinates of points</p>
</td></tr>
<tr><td><code id="regressor.basis_+3A_func">func</code></td>
<td>
<p>Regressor basis function to use; defaults to <code>regressor.basis</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>The regressor basis specified by <code>regressor.basis()</code> is just the
addition of a constant term, which is conventionally placed in the
first position.  This is a very common choice for a set of bases,
although it is important to investigate both simpler and more
sophisticated alternatives.  Tony would recommend simpler functions
(perhaps as simple as <code>function(x){1}</code>, that is, nothing but a
constant), and Jonty would recommend more complicated bespoke
functions that reflect prior beliefs.
</p>
<p>Function <code>regressor.multi()</code> is just a wrapper for
<code>regressor.basis()</code> that works for matrices.  This is used
internally and the user should not need to change it.
</p>
<p>Note that the user is free to define and use functions other than this
one when using, for example, <code>corr()</code>.
</p>


<h3>Value</h3>

<p>Returns simple regressor basis for vectors or matrices.
</p>


<h3>Note</h3>

<p>When writing replacements for <code>regressor.basis()</code>, 
it is important to return a vector with at least one <strong>named</strong>
element (see the R source code for function <code>regressor.basis()</code>,
in which the first element is named &ldquo;<code>const</code>&rdquo;).
</p>
<p>Returning a vector all of whose elements are unnamed will cause some
of the package functions to fail in various weird places.  It is good
practice to use named vectors in any case.
</p>
<p>Function <code>regressor.multi()</code> includes an ugly hack to ensure that
the perfectly reasonable choice of
<code>regressor.basis=function(x){1}</code> works.  The hack is needed
because <code>apply()</code> treats functions that return a length-1 value
argument differently from functions that return a vector: if <code>x
  &lt;- as.matrix(1:10)</code>
</p>
<p>then
</p>
<p><code>apply(x,1,function(x){c(1,x)})</code>
</p>
<p>returns a matrix (as desired), but
</p>
<p><code>apply(x,1,function(x){c(1)})</code>
</p>
<p>returns a vector (of <code>1</code>s) which is not what is wanted.  The best
way to deal with this (IMHO) is to confine the ugliness to a single
function, here <code>regressor.multi()</code>.
</p>


<h3>Author(s)</h3>

<p>Robin K. S. Hankin</p>


<h3>Examples</h3>

<pre><code class='language-R'>regressor.basis(rep(5,6))
m &lt;- matrix(1:27,9,3)
regressor.multi(m)
regressor.multi(m,func=function(x){c(a=88,x,x^2,x[1]^4)})


# and now a little example where we can choose the basis functions
# explicitly and see the effect it has.  Note particularly the poor
# performance of func2() in extrapolation:


func1 &lt;- function(x){
  out &lt;- c(1,cos(x))
  names(out) &lt;- letters[1:length(x)]
  return(out)
}

func2 &lt;- function(x){
  out &lt;- c(1,cos(x),cos(2*x),cos(3*x))
  names(out) &lt;- letters[1:length(x)]
  return(out)
}

func3 &lt;- function(x){out &lt;- c(1,x)
names(out)[1] &lt;- "const"
return(out)
}

func.chosen &lt;- func1


toy &lt;- sort(c(seq(from=0,to=1,len=9),0.2))
toy &lt;- as.matrix(toy)
colnames(toy) &lt;- "a"
rownames(toy) &lt;- paste("obs",1:nrow(toy),sep=".")

d.noisy &lt;- as.vector(toy&gt;0.5)+rnorm(length(toy))/40

fish &lt;- 100
x &lt;- seq(from=-1,to=2,len=200)
A &lt;- corr.matrix(toy,scales=fish)
Ainv &lt;- solve(A)

 ## Now the interpolation.  Change func.chosen() from func1() to func2()
 ## and see the difference!

jj &lt;- interpolant.quick(as.matrix(x), d.noisy, toy, scales=fish,
                        func=func.chosen, 
                         Ainv=Ainv,g=TRUE)

plot(x,jj$mstar.star,xlim=range(x),type="l",col="black",lwd=3)
lines(x,jj$prior,col="green",type="l")
lines(x,jj$mstar.star+jj$Z,type="l",col="red",lty=2)
lines(x,jj$mstar.star-jj$Z,type="l",col="red",lty=2)
points(toy,d.noisy,pch=16,cex=2)
legend("topright",lty=c(1,2,1,0),
    col=c("black","red","green","black"),pch=c(NA,NA,NA,16),
    legend=c("best estimate","+/-1 sd","prior","training set"))


  ## Now we will use O&amp;O 2002.

## First, some simulated design points:
xdash &lt;- as.matrix(c(-0.5, -0.1, -0.2, 1.1, 1.15))

## create an augmented design set:
design.augmented &lt;- rbind(toy,xdash)

## And calculate the correlation matrix of the augmented dataset:
A.augmented &lt;- corr.matrix(design.augmented, scales=fish)
Ainv.augmented &lt;- solve(A.augmented)

## Now, define a function that samples from the
## appropriate posterior t-distribution, adds these random
## variables to the dataset, then calculates a new
## etahat and evaluates and plots it:

f &lt;- function(...){

ddash &lt;- cond.sample(n=1, x=xdash, xold=toy, d=d.noisy, A=A,
                           Ainv=Ainv, scales=fish, func=func.chosen)

        jj.aug &lt;-
          interpolant.quick(x      = as.matrix(x),
                            d      = c(d.noisy,as.vector(ddash)),
                            xold   = design.augmented,
                            Ainv   = Ainv.augmented,
                            scales = fish,func=func.chosen)
points(xdash,ddash,type="p",pch=16,col="gray")
points(x, jj.aug, type="l", col="gray")
}


## Now execute the function a few times to assess the uncertainty in eta:
f()
f()
f()

</code></pre>

<hr>
<h2 id='results.table'>Results from 100 Goldstein runs</h2><span id='topic+results.table'></span>

<h3>Description</h3>

<p>A dataframe consisting of 100 rows, corresponding to 100 runs of
Goldstein.   Columns 1-19 are input values; columns 20-27
are outputs gleaned from <code>goout</code> files.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(results.table)</code></pre>


<h3>Format</h3>

<p>A data frame with 100 observations on the following 27 variables.
</p>

<dl>
<dt>filenumber</dt><dd><p>Number of the condor run (ie file appenge of
<code>goin.*</code> and <code>goout.*</code>)</p>
</dd>
<dt>windstress</dt><dd><p>a numeric vector (input value)</p>
</dd> 
<dt>oc.horiz.diffus</dt><dd><p>a numeric vector (input value)</p>
</dd> 
<dt>oc.vert.diffus</dt><dd><p>a numeric vector (input value)</p>
</dd> 
<dt>oc.drag</dt><dd><p>a numeric vector (input value)</p>
</dd> 
<dt>at.heat.diffus</dt><dd><p>a numeric vector (input value)</p>
</dd> 
<dt>at.mois.diffus</dt><dd><p>a numeric vector (input value)</p>
</dd> 
<dt>at.width</dt><dd><p>a numeric vector (input value)</p>
</dd> 
<dt>at.slope</dt><dd><p>a numeric vector (input value)</p>
</dd> 
<dt>advfact.zonalheat</dt><dd><p>a numeric vector (input value)</p>
</dd> 
<dt>advfact.meridheat</dt><dd><p>a numeric vector (input value)</p>
</dd> 
<dt>advfact.zonalmois</dt><dd><p>a numeric vector (input value)</p>
</dd> 
<dt>advfact.meridmois</dt><dd><p>a numeric vector (input value)</p>
</dd> 
<dt>co2.scaling</dt><dd><p>a numeric vector (input value)</p>
</dd> 
<dt>clim.sens</dt><dd><p>a numeric vector (input value)</p>
</dd> 
<dt>thres.humid</dt><dd><p>a numeric vector (input value)</p>
</dd> 
<dt>ice.diffus</dt><dd><p>a numeric vector (input value)</p>
</dd> 
<dt>fw.scaling</dt><dd><p>a numeric vector (input value)</p>
</dd> 
<dt>solar.const</dt><dd><p>a numeric vector (input value)</p>
</dd> 
<dt>ominp</dt><dd><p>a numeric vector (output value)</p>
</dd> 
<dt>omaxp</dt><dd><p>a numeric vector (output value)</p>
</dd> 
<dt>omina</dt><dd><p>a numeric vector (output value)</p>
</dd> 
<dt>omaxa</dt><dd><p>a numeric vector (output value)</p>
</dd> 
<dt>avn</dt><dd><p>a numeric vector (output value)</p>
</dd> 
<dt>rms</dt><dd><p>a numeric vector (output value)</p>
</dd> 
<dt>average.SAT</dt><dd><p>a numeric vector (output value)</p>
</dd> 
<dt>model.error</dt><dd><p>a numeric vector (output value)</p>
</dd> 
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>data(results.table)
</code></pre>

<hr>
<h2 id='s.chi'>Variance estimator</h2><span id='topic+s.chi'></span>

<h3>Description</h3>

<p>Returns estimator for a priori <code class="reqn">\sigma^2</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>s.chi(H, Ainv, d, s0 = 0, fast.but.opaque = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="s.chi_+3A_h">H</code></td>
<td>
<p>Regression basis function (eg that returned by <code>regressor.multi()</code>)</p>
</td></tr>
<tr><td><code id="s.chi_+3A_ainv">Ainv</code></td>
<td>
<p><code class="reqn">A^{-1}</code> where <code class="reqn">A</code> is a correlation matrix  (eg that
returned by <code>corr.matrix()</code>)</p>
</td></tr>
<tr><td><code id="s.chi_+3A_d">d</code></td>
<td>
<p>Vector of data points</p>
</td></tr>
<tr><td><code id="s.chi_+3A_s0">s0</code></td>
<td>
<p>Optional offset</p>
</td></tr>
<tr><td><code id="s.chi_+3A_fast.but.opaque">fast.but.opaque</code></td>
<td>
<p>Boolean, with default <code>TRUE</code> meaning to
use <code>quad.form()</code>, and <code>FALSE</code> meaning to use straightforward
<code>%*%</code>.  The first form should be faster, but the code is less
intelligible than the second form.  Comparing the returned value
with this argument on or off should indicate the likely accuracy attained.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See O'Hagan's paper (ref below), equation 12 for details and context.
</p>


<h3>Author(s)</h3>

<p>Robin K. S. Hankin</p>


<h3>References</h3>

<p>A. O'Hagan 1992. &ldquo;Some Bayesian Numerical Analysis&rdquo;, pp345-363 of
<em>Bayesian Statistics 4</em> (ed J. M. Bernardo et al), Oxford University
Press
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# example has 10 observations on 6 dimensions.
# function is just sum( (1:6)*x) where x=c(x_1, ... , x_2)
data(toy)
val &lt;- toy
colnames(val) &lt;- letters[1:6]
H &lt;- regressor.multi(val)
d &lt;- apply(H,1,function(x){sum((0:6)*x)})


# create A matrix and its inverse:
A &lt;- corr.matrix(val,scales=rep(1,ncol(val)))
Ainv &lt;- solve(A)

# add some suitably correlated noise:
d &lt;- as.vector(rmvnorm(n=1, mean=d, 0.1*A))

# now evaluate s.chi():
s.chi(H, Ainv, d)


# assess accuracy:
s.chi(H, Ainv, d, fast=TRUE) - s.chi(H, Ainv, d, fast=FALSE)

</code></pre>

<hr>
<h2 id='sample.n.fit'>Sample from a Gaussian process and fit an emulator to the points</h2><span id='topic+sample.n.fit'></span>

<h3>Description</h3>

<p>Sample 'n' fit:  sample from an appropriate multivariate Gaussian
process in one dimension, then fit an emulator to it.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sample.n.fit(n = 10, scales.generate = 100, scales.fit = 100, func = regressor.basis, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sample.n.fit_+3A_n">n</code></td>
<td>
<p>Number of observations to make</p>
</td></tr>
<tr><td><code id="sample.n.fit_+3A_scales.generate">scales.generate</code></td>
<td>
<p>Scales to generate the data with: small values
give uncorrelated observations, large values give correlated
observations (hence the points fall on a smooth line)</p>
</td></tr>
<tr><td><code id="sample.n.fit_+3A_scales.fit">scales.fit</code></td>
<td>
<p>Scales to use to fit the emulator.  Small values
give an emulator that is the prior with short, sharp excursions to
make the emulator go through the points; large values give smooth
emulators that exhibit overshoots resembling Gibbs's phenomenon</p>
</td></tr>
<tr><td><code id="sample.n.fit_+3A_func">func</code></td>
<td>
<p>Function used to determine basis vectors, defaulting
to <code>regressor.basis</code> if not given.</p>
</td></tr>
<tr><td><code id="sample.n.fit_+3A_...">...</code></td>
<td>
<p>Further arguments passed to <code>plot()</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The point of this function is to investigate what happens when
inappropriate scales are used for the emulator: that is, when
<code>scales.generate</code> and <code>scales.fit</code> are wildly different.
</p>
<p>Note that the sampling distribution has a constant expectation (of
zero); so the prior should be zero, making it easy to see mispredictions
of beta.
</p>


<h3>Author(s)</h3>

<p>Robin K. S. Hankin</p>


<h3>Examples</h3>

<pre><code class='language-R'>
sample.n.fit(main="Default: scales match")
sample.n.fit(scales.generate=5,main="generate scale small")

# Now use a quadratic function instead of the default linear:
 f &lt;- function(x){out &lt;- c(1,x,x^2)
 names(out) &lt;- c("const","linear","quadratic")
 out}

sample.n.fit(main="quadratic prior" , func=f)


</code></pre>

<hr>
<h2 id='scales.likelihood'>Likelihood of roughness parameters</h2><span id='topic+scales.likelihood'></span>

<h3>Description</h3>

<p>Gives the a postiori likelihood for the roughness parameters as a
function of the observations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>scales.likelihood(pos.def.matrix = NULL, scales = NULL, xold,
use.Ainv = TRUE, d, give_log=TRUE, func = regressor.basis)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="scales.likelihood_+3A_pos.def.matrix">pos.def.matrix</code></td>
<td>
<p>Positive definite matrix used for the distance metric</p>
</td></tr>
<tr><td><code id="scales.likelihood_+3A_scales">scales</code></td>
<td>
<p>If the positive definite matrix is diagonal, 
<code>scales</code> specifies the diagonal elements.  Specify exactly one
of <code>pos.def.matrix</code> or <code>scales</code> (ie not both)</p>
</td></tr>
<tr><td><code id="scales.likelihood_+3A_xold">xold</code></td>
<td>
<p>Points at which code has been run</p>
</td></tr>
<tr><td><code id="scales.likelihood_+3A_use.ainv">use.Ainv</code></td>
<td>
<p>Boolean, with default <code>TRUE</code> meaning to calculate
<code class="reqn">A^{-1}</code> explicitly and use it.  Setting to <code>FALSE</code>
means to use methods (such as <code>quad.form.inv()</code>) which do not
require inverting the <code>A</code> matrix.  Although one should avoid
inverting a matrix if possible, in practice there does not appear
to be much difference in execution time for the two methods</p>
</td></tr>
<tr><td><code id="scales.likelihood_+3A_d">d</code></td>
<td>
<p>Observations in the form of a vector with entries
corresponding to the rows of <code>xold</code></p>
</td></tr>
<tr><td><code id="scales.likelihood_+3A_give_log">give_log</code></td>
<td>
<p>Boolean, with default <code>TRUE</code> meaning to return
the logarithm of the likelihood (ie the support) and <code>FALSE</code>
meaning to return the likelihood itself</p>
</td></tr>
<tr><td><code id="scales.likelihood_+3A_func">func</code></td>
<td>
<p>Function used to determine basis vectors, defaulting
to <code>regressor.basis</code> if not given</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function returns the likelihood function defined in Oakley's PhD
thesis, equation 2.37.  Maximizing this likelihood to estimate the
roughness parameters is an alternative to the leave-out-one method on
the <code>interpolant()</code> helppage; both methods perform similarly.
</p>
<p>The value returned is
</p>
<p style="text-align: center;"><code class="reqn">
\left(\hat{\sigma}\right)^{-(n-q)/2}
\left|A\right|^{-1/2}\cdot\left|H^TA^{-1}H\right|^{-1/2}.
</code>
</p>



<h3>Value</h3>

<p>Returns the likelihood or support.
</p>


<h3>Note</h3>

<p>This function uses a Boolean flag, <code>use.Ainv</code>, to determine
whether <code>A</code> has to be inverted or not.  Compare the other
strategy in which separate functions, eg <code>foo()</code> and
<code>foo.A()</code>, are written.  An example would be <code>betahat.fun()</code>.
</p>


<h3>Author(s)</h3>

<p>Robin K. S. Hankin</p>


<h3>References</h3>


<ul>
<li>
<p>J. Oakley 1999. <em>Bayesian uncertainty analysis for complex
computer codes</em>, PhD thesis, University of Sheffield.
</p>
</li>
<li>
<p>J. Oakley and A. O'Hagan, 2002. <em>Bayesian Inference for the
Uncertainty Distribution of Computer Model Outputs</em>, Biometrika
89(4), pp769-784
</p>
</li>
<li>
<p>R. K. S. Hankin 2005. <em>Introducing BACCO, an R bundle for
Bayesian analysis of computer code output</em>, Journal of Statistical
Software, 14(16)
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+optimal.scales">optimal.scales</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'> data(toy)
 val &lt;- toy

 #define a real relation
 real.relation &lt;- function(x){sum( (0:6)*x )}

 #Some scales:
 fish &lt;- rep(1,6)
 fish[6] &lt;- 4
 A &lt;- corr.matrix(val,scales=fish)
 Ainv &lt;- solve(A)

 # Gaussian process noise:
 H &lt;- regressor.multi(val)
 d &lt;- apply(H,1,real.relation)
 d.noisy &lt;- as.vector(rmvnorm(n=1,mean=d, 0.1*A))

 # Compare likelihoods with true values and another value:
 scales.likelihood(scales=rep(1,6),xold=toy,d=d.noisy)
 scales.likelihood(scales=fish    ,xold=toy,d=d.noisy)


 # Verify that use.Ainv does not affect the numerical result:
u.true  &lt;- scales.likelihood(scales=rep(1,6),xold=toy,d=d.noisy,use.Ainv=TRUE)
u.false &lt;- scales.likelihood(scales=rep(1,6),xold=toy,d=d.noisy,use.Ainv=FALSE)
print(c(u.true, u.false))  # should be identical up to numerical accuracy


 # Now use optim():
 f &lt;- function(fish){scales.likelihood(scales=exp(fish), xold=toy, d=d.noisy)}
 e &lt;-
optim(log(fish),f,method="Nelder-Mead",control=list(trace=0,maxit=10,fnscale=
-1))
best.scales &lt;- exp(e$par)

</code></pre>

<hr>
<h2 id='sigmahatsquared'>Estimator for sigma squared</h2><span id='topic+sigmahatsquared'></span><span id='topic+sigmahatsquared.A'></span>

<h3>Description</h3>

<p>Returns maximum likelihood estimate for sigma squared.  The
&ldquo;<code>.A</code>&rdquo; form does not need <code>Ainv</code>, thus removing the need to
invert <code>A</code>.  Note that this form is <em>slower</em> than
the other if <code>Ainv</code> is known in advance, as <code>solve(.,.)</code> is slow.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sigmahatsquared(H, Ainv, d)
sigmahatsquared.A(H, A, d)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sigmahatsquared_+3A_h">H</code></td>
<td>
<p>Regressor matrix (eg as returned by <code>regressor.multi()</code>)</p>
</td></tr>
<tr><td><code id="sigmahatsquared_+3A_a">A</code></td>
<td>
<p>Correlation matrix (eg <code>corr.matrix(val)</code>)</p>
</td></tr>
<tr><td><code id="sigmahatsquared_+3A_ainv">Ainv</code></td>
<td>
<p>Inverse of the correlation matrix (eg <code>solve(corr.matrix(val))</code>)</p>
</td></tr>
<tr><td><code id="sigmahatsquared_+3A_d">d</code></td>
<td>
<p>Vector of observations</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The formula is
</p>
<p style="text-align: center;"><code class="reqn">\frac{y^T\left(A^{-1}-A^{-1}H(H^T A^{-1}H)^{-1} H^T
    A^{-1}\right)y}{n-q-2}</code>
</p>

<p>where <code class="reqn">y</code> is the data vector, <code class="reqn">H</code> the matrix whose rows are the
regressor functions of the design matrix, <code class="reqn">A</code> the correlation
matrix, <code class="reqn">n</code> the number of observations and <code class="reqn">q</code> the number of
elements in the basis function.
</p>


<h3>Author(s)</h3>

<p>Robin K. S. Hankin</p>


<h3>References</h3>


<ul>
<li>
<p>J. Oakley and A. O'Hagan, 2002. <em>Bayesian Inference for the
Uncertainty Distribution of Computer Model Outputs</em>, Biometrika
89(4), pp769-784
</p>
</li>
<li>
<p>R. K. S. Hankin 2005. <em>Introducing BACCO, an R bundle for
Bayesian analysis of computer code output</em>, Journal of Statistical
Software, 14(16)
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
## First, set sigmasquared to a value that we will try to estimate at the end:
REAL.SIGMASQ &lt;- 0.3

## First, some data:
val &lt;- latin.hypercube(100,6)
H &lt;- regressor.multi(val,func=regressor.basis)

## now some scales:
 fish &lt;- c(1,1,1,1,1,4)

## A and Ainv
A &lt;- corr.matrix(as.matrix(val),scales=fish)
Ainv &lt;- solve(A)

## a real relation; as used in helppage for interpolant:
real.relation &lt;- function(x){sum( (1:6)*x )}

## use the real relation:
d &lt;- apply(val,1,real.relation)

## now add some Gaussian process noise:
d.noisy &lt;-  as.vector(rmvnorm(n=1,mean=d, REAL.SIGMASQ*A))

## now estimate REAL.SIGMASQ:
sigmahatsquared(H,Ainv,d.noisy)

## That shouldn't be too far from the real value specified above.

## Finally, a sanity check:
sigmahatsquared(H,Ainv,d.noisy) - sigmahatsquared.A(H,A=A,d.noisy)

</code></pre>

<hr>
<h2 id='toy'>A toy dataset</h2><span id='topic+toy'></span>

<h3>Description</h3>

<p>A matrix consisting of 10 rows and 6 columns corresponding to 10 points in 
a six-dimensional space.</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(toy)</code></pre>


<h3>Examples</h3>

<pre><code class='language-R'>     data(toy)
     real.relation &lt;- function(x){sum( (1:6)*x )}

     d &lt;- apply(toy, 1, real.relation)

     # Supply some scales:
     fish &lt;- rep(2,6)

     # Calculate the A matrix:
     A &lt;- corr.matrix(toy,scales=fish)
     Ainv &lt;- solve(A)

     # Now add some suitably correlated noise:
     d.noisy &lt;- as.vector(rmvnorm(n=1,mean=d, 0.1*A))

     # Choose a point:
     x.unknown &lt;- rep(0.5,6)

     # Now use interpolant:
     interpolant(x.unknown, d.noisy, toy, Ainv, scales=fish, g=FALSE)


     # Now verify by checking the first row of toy:
     interpolant(toy[1,], d.noisy, toy, Ainv, scales=fish, g=FALSE)
     # Should match d.noisy[1].
</code></pre>

<hr>
<h2 id='tr'>Trace of a matrix</h2><span id='topic+tr'></span>

<h3>Description</h3>

<p>Returns the trace of a matrix
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tr(a)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tr_+3A_a">a</code></td>
<td>
<p>Matrix whose trace is desired</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Robin K. S. Hankin</p>


<h3>Examples</h3>

<pre><code class='language-R'>tr(matrix(1:9,3,3))
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
