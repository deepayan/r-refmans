<!DOCTYPE html><html lang="en"><head><title>Help for package vocaldia</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {vocaldia}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#vocaldia-package'><p>vocaldia: Create and Manipulate Vocalisation Diagrams</p></a></li>
<li><a href='#anonymise'><p>anonymise: anonymise a vocalisation diagram</p></a></li>
<li><a href='#appendSpeechRate'><p>appendSpeechRate: append pre-generated speech rate data to given dataframe t</p></a></li>
<li><a href='#atddia'><p>A sample Medical Team Meeting dialogue encoded as a vocaldia</p></a></li>
<li><a href='#getEntropy'><p>getEntropy: safely return the Shannon entropy of a distribution.</p></a></li>
<li><a href='#getIDs'><p>getIDs get speaker role IDs (PAR, INV) and info from CHA content</p></a></li>
<li><a href='#getPauseType'><p>getPauseType: name pause type between two vocalisation events.</p></a></li>
<li><a href='#getPID'><p>getIDs get  study-wide unique patient IDs from CHA content</p></a></li>
<li><a href='#getPofAgivenB'><p>getPofAgivenB: transtion probability.</p></a></li>
<li><a href='#getSampledVocalCountMatrix'><p>getSampledVocalCountMatrix: generate vocalisation diagrams</p></a></li>
<li><a href='#getSampledVocalMatrix'><p>getSampledVocalCountMatrix: generate vocalisation diagrams</p></a></li>
<li><a href='#getSilences'><p>getSilences read silences file</p></a></li>
<li><a href='#getSyllablesAndSilences'><p>getSyllablesAndSilences: process Praat's grid for syllable nuclei</p></a></li>
<li><a href='#getTranscript'><p>getTranscript: get transcription lines from .cha content</p></a></li>
<li><a href='#getTurnTakingMatrix'><p>getSampledVocalCountMatrix: generate vocalisation diagrams</p></a></li>
<li><a href='#getTurnTakingProbMatrix'><p>getTurnTakingProbMatrix: create a vocaldia from a</p>
data.frame.</a></li>
<li><a href='#getTurnType'><p>getTurnType: return type of turn</p></a></li>
<li><a href='#identifyGrpVocalisations'><p>identifyGrpVocalisations: replace appropriate vocalisation</p>
types</a></li>
<li><a href='#identifyPauses'><p>identifyPauses: label pauses according to type.</p></a></li>
<li><a href='#identifyVocalisations'><p>identifyVocalisations: replace appropriate vocalisation</p>
types</a></li>
<li><a href='#igraph.vocaldia'><p>igraph.vocaldia: Create an igraph vocalisation diagram</p></a></li>
<li><a href='#makeSessionDataSet'><p>makeSessionDataSet: create a data frame for a session (e.g. cookie scene description) based on .cha transcription files</p></a></li>
<li><a href='#makeVocalStatsDataset'><p>makeVocalStatsDataset: create a dataset of vocalisation statistics (1 row per patient)</p></a></li>
<li><a href='#matrixExp'><p>matrixExp: raise matrix to exp.</p></a></li>
<li><a href='#namePauses'><p>namePauses: name pause types.</p></a></li>
<li><a href='#plot.matrixseries'><p>plotConvergence: plots Markov diagram convergence.</p></a></li>
<li><a href='#plot.vocaldia'><p>plot.vocaldia</p></a></li>
<li><a href='#printARFFfile'><p>printARFFfile: Create arff files by creating and flattening vocaldias</p></a></li>
<li><a href='#read.cha'><p>read.cha read CHA transcription file (format used by DementiaBank)</p></a></li>
<li><a href='#startmatrix'><p>startmatrix: return the first matrix of a converging series.</p></a></li>
<li><a href='#staticMatrix'><p>staticMatrix Iterate until transition probabilities converge (or give up).</p></a></li>
<li><a href='#toDotNotation'><p>toDotNotation: conver vocaldia to graphviz dot notation</p></a></li>
<li><a href='#vocmatrix'><p>A sample vocalisation matrix</p></a></li>
<li><a href='#write.vocaldia'><p>write.vocaldia</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Create and Manipulate Vocalisation Diagrams</td>
</tr>
<tr>
<td>Version:</td>
<td>0.8.4</td>
</tr>
<tr>
<td>Date:</td>
<td>2022-08-14</td>
</tr>
<tr>
<td>Description:</td>
<td>Create adjacency matrices of vocalisation graphs from
  dataframes containing sequences of speech and silence intervals,
  transforming these matrices into Markov diagrams, and generating
  datasets for classification of these diagrams by 'flattening' them
  and adding global properties (functionals) etc.  Vocalisation
  diagrams date back to early work in psychiatry (Jaffe and Feldstein,
  1970) and social psychology (Dabbs and Ruback, 1987) but have only
  recently been employed as a data representation method for machine
  learning tasks including meeting segmentation (Luz, 2012)
  &lt;<a href="https://doi.org/10.1145%2F2328967.2328970">doi:10.1145/2328967.2328970</a>&gt; and classification (Luz,
  2013) &lt;<a href="https://doi.org/10.1145%2F2522848.2533788">doi:10.1145/2522848.2533788</a>&gt;.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.0.0)</td>
</tr>
<tr>
<td>Suggests:</td>
<td>igraph, foreign</td>
</tr>
<tr>
<td>Imports:</td>
<td>graphics, stats, utils</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.1</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://git.ecdf.ed.ac.uk/sluzfil/vocaldia">https://git.ecdf.ed.ac.uk/sluzfil/vocaldia</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://git.ecdf.ed.ac.uk/sluzfil/vocaldia/-issues">https://git.ecdf.ed.ac.uk/sluzfil/vocaldia/-issues</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-08-14 20:30:26 UTC; luzs</td>
</tr>
<tr>
<td>Author:</td>
<td>Saturnino Luz [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Saturnino Luz &lt;luzs@acm.org&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-08-14 20:40:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='vocaldia-package'>vocaldia: Create and Manipulate Vocalisation Diagrams</h2><span id='topic+vocaldia'></span><span id='topic+vocaldia-package'></span>

<h3>Description</h3>

<p>Create adjacency matrices of vocalisation graphs from dataframes containing sequences of speech and silence intervals, transforming these matrices into Markov diagrams, and generating datasets for classification of these diagrams by 'flattening' them and adding global properties (functionals) etc. Vocalisation diagrams date back to early work in psychiatry (Jaffe and Feldstein, 1970) and social psychology (Dabbs and Ruback, 1987) but have only recently been employed as a data representation method for machine learning tasks including meeting segmentation (Luz, 2012) doi: <a href="https://doi.org/10.1145/2328967.2328970">10.1145/2328967.2328970</a> and classification (Luz, 2013) doi: <a href="https://doi.org/10.1145/2522848.2533788">10.1145/2522848.2533788</a>.
</p>


<h3>Author(s)</h3>

<p>Saturnino Luz <a href="mailto:luzs@acm.org">luzs@acm.org</a>
</p>


<h3>References</h3>

<p>S. Luz. Automatic identification of experts and performance
prediction in the multimodal math data corpus through analysis
of speech interaction. In <em>Proceedings of the 15th ACM on
International Conference on Multimodal Interaction, ICMI'13</em>,
pages 575&ndash;582, New York, NY, USA, 2013. ACM.
</p>
<p>S. Luz. The non-verbal structure of patient case discussions in
multidisciplinary medical team meetings. <em>ACM Transactions on
Information Systems</em>, 30(3):17:1&ndash;17:24, 2012
</p>
<p>Dabbs, J. M. J. and Ruback, B. Dimensions of group process: Amount and
structure of vocal interaction. <em>Advances in Experimental Social
Psychology</em> 20, 123-169, 1987.
</p>
<p>Jaffe , J. and Feldstein, S. Rhythms of
dialogue. ser. <em>Personality and Psychopathology</em>. Academic
Press, New York, 1976.
</p>


<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://git.ecdf.ed.ac.uk/sluzfil/vocaldia">https://git.ecdf.ed.ac.uk/sluzfil/vocaldia</a>
</p>
</li>
<li><p> Report bugs at <a href="https://git.ecdf.ed.ac.uk/sluzfil/vocaldia/-issues">https://git.ecdf.ed.ac.uk/sluzfil/vocaldia/-issues</a>
</p>
</li></ul>


<hr>
<h2 id='anonymise'>anonymise: anonymise a vocalisation diagram</h2><span id='topic+anonymise'></span><span id='topic+anonymise.vocaldia'></span><span id='topic+anonymise.default'></span>

<h3>Description</h3>

<p>Anonymise a vocalisation diagram
</p>


<h3>Usage</h3>

<pre><code class='language-R'>anonymise(vd)

## S3 method for class 'vocaldia'
anonymise(vd)

## Default S3 method:
anonymise(vd)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="anonymise_+3A_vd">vd</code></td>
<td>
<p>a vocalisation diagram (vocaldia object)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>&quot;anonymise&quot; a <code>vocaldia</code> turn taking probability matrix by
replacing speaker names by variables <code class="reqn">s_1,...,s_n s.t. s_1</code> is
the speaker who spoke the least and <code class="reqn">s_n</code> the one who did the most
talking.
</p>


<h3>Value</h3>

<p>a new vocaldia with speaker names replaced by variables
<code class="reqn">s_1,...,s_n</code> s.t. <code class="reqn">s_1</code> is the speaker who spoke the least
and <code class="reqn">s_n</code> the one who did the most talking.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
data(vocdia)
x2 &lt;- getSampledVocalMatrix(subset(atddia, id=='Abbott_Maddock_01'),
                            individual=TRUE, nodecolumn='speaker')
anonymise(x2)

## End(Not run)
</code></pre>

<hr>
<h2 id='appendSpeechRate'>appendSpeechRate: append pre-generated speech rate data to given dataframe t</h2><span id='topic+appendSpeechRate'></span>

<h3>Description</h3>

<p>appendSpeechRate: append pre-generated speech rate data (see audioproc.R)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>appendSpeechRate(t, file = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="appendSpeechRate_+3A_t">t</code></td>
<td>
<p>a table read through read.cha</p>
</td></tr>
<tr><td><code id="appendSpeechRate_+3A_file">file</code></td>
<td>
<p>speech rate file</p>
</td></tr>
</table>


<h3>Value</h3>

<p>dataframe t bound to speech rates per utterance
</p>


<h3>Author(s)</h3>

<p>luzs
</p>

<hr>
<h2 id='atddia'>A sample Medical Team Meeting dialogue encoded as a vocaldia</h2><span id='topic+atddia'></span>

<h3>Description</h3>

<p>A dataset containing 38 dialogues (17 control patients, and 21 AD
patients) and 7869 vocalisation events.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>atddia
</code></pre>


<h3>Format</h3>

<p>A data frame with 7869 rows and 7 variables:
</p>

<dl>
<dt>id</dt><dd><p>The dialogue indentifier</p>
</dd>
<dt>begin</dt><dd><p>The start time of a speech turn or silence interval</p>
</dd>
<dt>end</dt><dd><p>The end time of a speech turn or silence interval</p>
</dd>
<dt>speaker</dt><dd><p>An identifier for the speaker of the turn, or Floor for silence.</p>
</dd>
<dt>role</dt><dd><p>The speaker's role (patient, interviewer, other, or Floor</p>
</dd>
<dt>trans</dt><dd><p>The transcription of the turn (blanked out for anonymity)</p>
</dd>
<dt>dx</dt><dd><p>The diagnosis (ad or nonad</p>
</dd>
</dl>



<h3>Source</h3>

<p>This dataset was generated from the Carolina Conversations
Collection, and used in the work described in De La Fuente,
Albert and Luz:
&quot;Detecting cognitive decline through dialogue processing&quot;,
2017. For the full data set, please contact the Medical
University of South Carolina (MUSC)
http://carolinaconversations.musc.edu/
</p>

<hr>
<h2 id='getEntropy'>getEntropy: safely return the Shannon entropy of a distribution.</h2><span id='topic+getEntropy'></span>

<h3>Description</h3>

<p>Compute the entropy of a distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getEntropy(distribution)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="getEntropy_+3A_distribution">distribution</code></td>
<td>
<p>a probability distribution.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Compute the entropy of a distribution.
</p>


<h3>Value</h3>

<p>a numeric value.
</p>

<hr>
<h2 id='getIDs'>getIDs get speaker role IDs (PAR, INV) and info from CHA content</h2><span id='topic+getIDs'></span>

<h3>Description</h3>

<p>getIDs get speaker IDs from CHA content
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getIDs(text)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="getIDs_+3A_text">text</code></td>
<td>
<p>a string vector containing the lines of a CHA file</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a vector with participants IDs
</p>


<h3>Author(s)</h3>

<p>luzs
</p>

<hr>
<h2 id='getPauseType'>getPauseType: name pause type between two vocalisation events.</h2><span id='topic+getPauseType'></span>

<h3>Description</h3>

<p>Identify the type of pause between vocalisations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getPauseType(prevspeaker, nextspeaker)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="getPauseType_+3A_prevspeaker">prevspeaker</code></td>
<td>
<p>speaker of the vocalisation immediately before Floor</p>
</td></tr>
<tr><td><code id="getPauseType_+3A_nextspeaker">nextspeaker</code></td>
<td>
<p>speaker of the vocalisation immediately after Floor</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The type of pause a 'Floor' (silence) event represents can be:
'Pause', 'SwitchingPause', 'GrpPause', or 'GrpSwitchingPause'. See
(Luz, 2013) for details.
</p>


<h3>Value</h3>

<p>the pause type.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+namePauses">namePauses</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>getPauseType('a', 'b')
 ## [1]  "SwitchingPause"
getPauseType('a', 'Grp')
 ## [1]  "SwitchingPause"
getPauseType('Grp', 'Grp')
 ## [1]  "GrpPause"
getPauseType('Grp', 'a')
 ## [1]  "GrpSwitchingPause"
getPauseType('a', 'a')
 ##[1] "Pause"
</code></pre>

<hr>
<h2 id='getPID'>getIDs get  study-wide unique patient IDs from CHA content</h2><span id='topic+getPID'></span>

<h3>Description</h3>

<p>getPIDs get study-wide unique patient IDs from CHA content
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getPID(text)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="getPID_+3A_text">text</code></td>
<td>
<p>a string vector containing the lines of a CHA file</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a vector with participants IDs
</p>


<h3>Author(s)</h3>

<p>luzs
</p>

<hr>
<h2 id='getPofAgivenB'>getPofAgivenB: transtion probability.</h2><span id='topic+getPofAgivenB'></span>

<h3>Description</h3>

<p>Conditional (transition ) probability
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getPofAgivenB(a, b, ttarray)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="getPofAgivenB_+3A_a">a</code></td>
<td>
<p>target node</p>
</td></tr>
<tr><td><code id="getPofAgivenB_+3A_b">b</code></td>
<td>
<p>source node</p>
</td></tr>
<tr><td><code id="getPofAgivenB_+3A_ttarray">ttarray</code></td>
<td>
<p>adjacency matrix</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Retrieve <code class="reqn">p(a|b)</code>, probability of a transition from b to a in an
adjacency matrix
</p>


<h3>Value</h3>

<p>a transition probability.
</p>

<hr>
<h2 id='getSampledVocalCountMatrix'>getSampledVocalCountMatrix: generate vocalisation diagrams</h2><span id='topic+getSampledVocalCountMatrix'></span>

<h3>Description</h3>

<p>Generate a count vocalisation diagram through 'sampling'.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getSampledVocalCountMatrix(
  cdf,
  rate = 1,
  individual = FALSE,
  noPauseTypes = FALSE,
  begin = "begin",
  end = "end",
  nodecolumn = "role"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="getSampledVocalCountMatrix_+3A_cdf">cdf</code></td>
<td>
<p>a data frame consisting, minimally, of a column for
vocalisation/pause start times, a column for end times, and a
column identifying the speaker, speaker role or 'Floor' (for
silences).</p>
</td></tr>
<tr><td><code id="getSampledVocalCountMatrix_+3A_rate">rate</code></td>
<td>
<p>the rate at which to sample the vocalisation events
(in seconds)</p>
</td></tr>
<tr><td><code id="getSampledVocalCountMatrix_+3A_individual">individual</code></td>
<td>
<p>whether to include individual speakers or group
them into a single Vocalisation node</p>
</td></tr>
<tr><td><code id="getSampledVocalCountMatrix_+3A_nopausetypes">noPauseTypes</code></td>
<td>
<p>if TRUE, ignore distinctions between pauses
(SwitchingPause, GrpSwitchingPause, etc)</p>
</td></tr>
<tr><td><code id="getSampledVocalCountMatrix_+3A_begin">begin</code></td>
<td>
<p>the name of the column containing the start time of
the vocalisation event in a row.</p>
</td></tr>
<tr><td><code id="getSampledVocalCountMatrix_+3A_end">end</code></td>
<td>
<p>the name of the column containing the end time of the
vocalisation event in the same row.</p>
</td></tr>
<tr><td><code id="getSampledVocalCountMatrix_+3A_nodecolumn">nodecolumn</code></td>
<td>
<p>the name of the column containing the node
(speaker) name (e.g. 'speaker', 'role').</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A vocalisation diagram (vocaldia) is a representation of a
dialogue as a Markov process whose cell &lt;m,n&gt; contains the
transition probability from node n to node m). This function for
'cases' (an identifier for a case or a vector of identifiers
identifying a set of cases) in data frame 'df', obtained by
sampling the timeline every 'rate'-th second (see
getSampledVocalCountMatrix).
</p>


<h3>Value</h3>

<p>a vocaldia object, consisting of a vocalisation matrix
(vocmatrix) where cell &lt;m,n&gt; contains the counts of
transitions from node n to node m, and a table of prior
probabilities (stationary distribution) per node.
</p>


<h3>See Also</h3>

<p>(Luz, 2013)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(vocdia) 
getSampledVocalCountMatrix(subset(atddia,
     id=='Abbott_Maddock_01'), nodecolumn='role')
</code></pre>

<hr>
<h2 id='getSampledVocalMatrix'>getSampledVocalCountMatrix: generate vocalisation diagrams</h2><span id='topic+getSampledVocalMatrix'></span>

<h3>Description</h3>

<p>Generate a probabilistic vocalisation diagram through 'sampling'.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getSampledVocalMatrix(df, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="getSampledVocalMatrix_+3A_df">df</code></td>
<td>
<p>a data frame consisting, minimally, of a column for
vocalisation/pause start times, a column for end times, and a
column identifying the speaker, speaker role or 'Floor' (for
silences).</p>
</td></tr>
<tr><td><code id="getSampledVocalMatrix_+3A_...">...</code></td>
<td>
<p>general parameter to be passed to
<code><a href="#topic+getSampledVocalCountMatrix">getSampledVocalCountMatrix</a></code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>A vocalisation diagram (vocaldia) is a representation of a
dialogue as a Markov process whose cell &lt;m,n&gt; contains the
transition probability from node n to node m).
</p>


<h3>Value</h3>

<p>a vocaldia object, consisting of a vocalisation matrix
(vocmatrix) where cell &lt;m,n&gt; contains the transition
probability from node n to node m, and a table of prior
probabilities (stationary distribution) per node.
</p>


<h3>Author(s)</h3>

<p>Saturnino Luz  <a href="mailto:luzs@acm.org">luzs@acm.org</a>
</p>


<h3>References</h3>

<p>S. Luz. Automatic identification of experts and performance
prediction in the multimodal math data corpus through analysis
of speech interaction. In <em>Proceedings of the 15th ACM on
International Conference on Multimodal Interaction, ICMI'13</em>,
pages 575&ndash;582, New York, NY, USA, 2013. ACM.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+getSampledVocalCountMatrix">getSampledVocalCountMatrix</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(vocdia) 
getSampledVocalMatrix(subset(atddia,
         id=='Abbott_Maddock_01'),nodecolumn='speaker', individual=TRUE)
</code></pre>

<hr>
<h2 id='getSilences'>getSilences read silences file</h2><span id='topic+getSilences'></span>

<h3>Description</h3>

<p>getSilences read silences file
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getSilences(file, sildir = NULL, silsuffix = "c.mp3.csv")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="getSilences_+3A_file">file</code></td>
<td>
<p>CSV formatted silences file</p>
</td></tr>
<tr><td><code id="getSilences_+3A_sildir">sildir</code></td>
<td>
<p>dir where silence files are</p>
</td></tr>
<tr><td><code id="getSilences_+3A_silsuffix">silsuffix</code></td>
<td>
<p>## suffix for silence files</p>
</td></tr>
</table>


<h3>Value</h3>

<p>silences dataframe
</p>


<h3>Author(s)</h3>

<p>luzs
</p>

<hr>
<h2 id='getSyllablesAndSilences'>getSyllablesAndSilences: process Praat's grid for syllable nuclei</h2><span id='topic+getSyllablesAndSilences'></span>

<h3>Description</h3>

<p>getSyllablesAndSilences: process Praat's grid for syllable nuclei, based on De Jong's approach
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getSyllablesAndSilences(txtgrid)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="getSyllablesAndSilences_+3A_txtgrid">txtgrid</code></td>
<td>
<p>Path to Praat grid file generated by praat-syllable-syllable-nuclei-v2</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list of syllables and silences
</p>


<h3>Author(s)</h3>

<p>luzs
</p>


<h3>References</h3>

<p>De Jong, N. H. and Wempe, T. (2009). Praat script to detect syllable nuclei
and measure speech rate automatically. Behavior Research Methods,
41(2):385â€“390, May.
</p>

<hr>
<h2 id='getTranscript'>getTranscript: get transcription lines from .cha content</h2><span id='topic+getTranscript'></span>

<h3>Description</h3>

<p>getTranscript
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getTranscript(text)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="getTranscript_+3A_text">text</code></td>
<td>
<p>a string vector containing the lines of a CHA file</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list of transcriptions (participant and interviewer utterances)
</p>


<h3>Author(s)</h3>

<p>luzs
</p>

<hr>
<h2 id='getTurnTakingMatrix'>getSampledVocalCountMatrix: generate vocalisation diagrams</h2><span id='topic+getTurnTakingMatrix'></span>

<h3>Description</h3>

<p>Generate a vocalisation diagram with absolute vocalisation durations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getTurnTakingMatrix(
  df,
  begin = "begin",
  end = "end",
  nodecolumn = "role",
  individual = FALSE,
  noPauseTypes = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="getTurnTakingMatrix_+3A_df">df</code></td>
<td>
<p>a data frame consisting, minimally, of a column for
vocalisation/pause start times, a column for end times, and a
column identifying the speaker, speaker role or 'Floor' (for
silences).</p>
</td></tr>
<tr><td><code id="getTurnTakingMatrix_+3A_begin">begin</code></td>
<td>
<p>the name of the column containing the start time of
the vocalisation event in a row.</p>
</td></tr>
<tr><td><code id="getTurnTakingMatrix_+3A_end">end</code></td>
<td>
<p>the name of the column containing the end time of the
vocalisation event in the same row.</p>
</td></tr>
<tr><td><code id="getTurnTakingMatrix_+3A_nodecolumn">nodecolumn</code></td>
<td>
<p>the name of the column containing the node
(speaker) name (e.g. 'speaker', 'role').</p>
</td></tr>
<tr><td><code id="getTurnTakingMatrix_+3A_individual">individual</code></td>
<td>
<p>whether to include individual speakers or group
them into a single Vocalisation node</p>
</td></tr>
<tr><td><code id="getTurnTakingMatrix_+3A_nopausetypes">noPauseTypes</code></td>
<td>
<p>if TRUE, ignore distinctions between pauses
(SwitchingPause, GrpSwitchingPause, etc)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A vocalisation diagram (vocaldia) is a representation of a
dialogue as a Markov process whose cell &lt;m,n&gt; contains the
transition probability from node n to node m). Unlike
<code><a href="#topic+getSampledVocalCountMatrix">getSampledVocalCountMatrix</a></code> this function
accummulates event durations directly, therefore resulting in no
self-transitions (in general).
</p>


<h3>Value</h3>

<p>a vocaldia object, consisting of a vocalisation matrix
(vocmatrix) where cell &lt;m,n&gt; contains the counts of
transitions from node n to node m, and a table of absolute
durations of vocalisation events.
</p>


<h3>References</h3>

<p>S. Luz. Automatic identification of experts and performance
prediction in the multimodal math data corpus through analysis
of speech interaction. In <em>Proceedings of the 15th ACM on
International Conference on Multimodal Interaction, ICMI'13</em>,
pages 575&ndash;582, New York, NY, USA, 2013. ACM.
</p>


<h3>See Also</h3>

<p>(Luz, 2013) and <code><a href="#topic+getTurnTakingMatrix">getTurnTakingMatrix</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- subset(atddia, id=='Abbott_Maddock_01')
getTurnTakingMatrix(x)
getTurnTakingMatrix(x, individual=TRUE)
</code></pre>

<hr>
<h2 id='getTurnTakingProbMatrix'>getTurnTakingProbMatrix: create a vocaldia from a
data.frame.</h2><span id='topic+getTurnTakingProbMatrix'></span>

<h3>Description</h3>

<p>Convert a data frame into a vocalisation diagram using counts rather than sampling.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getTurnTakingProbMatrix(df, individual = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="getTurnTakingProbMatrix_+3A_df">df</code></td>
<td>
<p>a data frame consisting, minimally, of a column for
vocalisation/pause start times, a column for end times, and a
column identifying the speaker, speaker role or 'Floor' (for
silences).</p>
</td></tr>
<tr><td><code id="getTurnTakingProbMatrix_+3A_individual">individual</code></td>
<td>
<p>whether to include individual speakers or group
them into a single Vocalisation node</p>
</td></tr>
<tr><td><code id="getTurnTakingProbMatrix_+3A_...">...</code></td>
<td>
<p>other parameters to be passed to
<code><a href="#topic+getTurnTakingMatrix">getTurnTakingMatrix</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Unlike <code><a href="#topic+getSampledVocalMatrix">getSampledVocalMatrix</a></code>, this function is based
on transition counts rather than sampled intervals. As a result,
where in this version self transitions will always be set to 0
(since a vocalisation by a speaker is never followed by another
vocalisation by the same speaker) in the sampled version self
transitons will usually dominate the distribution, since the
speaker who is speaking now is very likely to be the one who were
speaking one second ago.
</p>


<h3>Value</h3>

<p>a vocaldia object, consisting of a vocalisation matrix
(vocmatrix) where cell <code class="reqn">(m,n)</code> contains the probabilities <code class="reqn">P(n|m)</code>
transitions to node <code class="reqn">n</code> from node <code class="reqn">m</code>, and a table of prior
probabilities (stationary distribution) per node.
</p>


<h3>See Also</h3>

<p>(Luz, 2013) and  <code><a href="#topic+getTurnTakingMatrix">getTurnTakingMatrix</a></code>.
</p>
<p>S. Luz. Automatic identification of experts and performance
prediction in the multimodal math data corpus through analysis
of speech interaction. In <em>Proceedings of the 15th ACM on
International Conference on Multimodal Interaction, ICMI'13</em>,
pages 575&ndash;582, New York, NY, USA, 2013. ACM.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- subset(atddia, id=='Abbott_Maddock_01')
getTurnTakingProbMatrix(x)
getTurnTakingProbMatrix(x, individual=TRUE)
</code></pre>

<hr>
<h2 id='getTurnType'>getTurnType: return type of turn</h2><span id='topic+getTurnType'></span>

<h3>Description</h3>

<p>Identify turn types
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getTurnType(
  df,
  i,
  individual = FALSE,
  nodecolumn = "speaker",
  noPauseTypes = F
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="getTurnType_+3A_df">df</code></td>
<td>
<p>a data frame consisting, minimally, of a column for
vocalisation/pause start times, a column for end times, and a
column identifying the speaker, speaker role or 'Floor' (for
silences).</p>
</td></tr>
<tr><td><code id="getTurnType_+3A_i">i</code></td>
<td>
<p>the identifier (index number) whose type will be returned</p>
</td></tr>
<tr><td><code id="getTurnType_+3A_individual">individual</code></td>
<td>
<p>if TRUE, return the identifier, a Pause or Grp</p>
</td></tr>
<tr><td><code id="getTurnType_+3A_nodecolumn">nodecolumn</code></td>
<td>
<p>the name of the column containing the node
(speaker) name (e.g. 'speaker', 'role').</p>
</td></tr>
<tr><td><code id="getTurnType_+3A_nopausetypes">noPauseTypes</code></td>
<td>
<p>if TRUE, ignore distinctions between pauses
(SwitchingPause, GrpSwitchingPause, etc)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Return one of Vocalisation, GrpVocalisation, ... or identifier.
</p>


<h3>Value</h3>

<p>a string containing the turn type or identifier.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(vocdia)
atddia[1:10,]
getTurnType(atddia, 3, nodecolumn='role') ## a vocalisation
getTurnType(atddia, 4, nodecolumn='role') ## a pause
</code></pre>

<hr>
<h2 id='identifyGrpVocalisations'>identifyGrpVocalisations: replace appropriate vocalisation
types</h2><span id='topic+identifyGrpVocalisations'></span>

<h3>Description</h3>

<p>Identify group vocalisations
</p>


<h3>Usage</h3>

<pre><code class='language-R'>identifyGrpVocalisations(vocvector)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="identifyGrpVocalisations_+3A_vocvector">vocvector</code></td>
<td>
<p>a character vector containing a sequence of
vocalisation events</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Standardise identifier for group vocalisations
</p>


<h3>Value</h3>

<p>A vector with all events replaced by the appropriate type
identifier.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(vocdia)
identifyGrpVocalisations(atddia$speaker[1:60])
</code></pre>

<hr>
<h2 id='identifyPauses'>identifyPauses: label pauses according to type.</h2><span id='topic+identifyPauses'></span>

<h3>Description</h3>

<p>Assign types to the pauses (Floor events) in a sequence
</p>


<h3>Usage</h3>

<pre><code class='language-R'>identifyPauses(vocvector)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="identifyPauses_+3A_vocvector">vocvector</code></td>
<td>
<p>a character vector containing a sequence of
vocalisation events</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Identify the pauses in a vector as one of the pauses in
<code>pauseTypes</code>
</p>


<h3>Value</h3>

<p>A vector with all Floor events replaced by the appropriate
pause type identifier.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(vocdia)
identifyPauses(atddia$speaker[1:60])
</code></pre>

<hr>
<h2 id='identifyVocalisations'>identifyVocalisations: replace appropriate vocalisation
types</h2><span id='topic+identifyVocalisations'></span>

<h3>Description</h3>

<p>Identify switching vocalisations
</p>


<h3>Usage</h3>

<pre><code class='language-R'>identifyVocalisations(vocvector, idswitchvoc = T)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="identifyVocalisations_+3A_vocvector">vocvector</code></td>
<td>
<p>a character vector containing a sequence of
vocalisation events</p>
</td></tr>
<tr><td><code id="identifyVocalisations_+3A_idswitchvoc">idswitchvoc</code></td>
<td>
<p>if TRUE distinguise between
SwitchingVocalisation and Vocalisation.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>SwitchingVocalisation is a vocalisation that signals a immediate
speaker transition; that is, a transition from speaker to
speaker (as opposed to speaker to Grp or speaker to Pause).
</p>
<p>E.g (speakers A, B, C):
</p>
<pre>
AAAAAAAABBBBBBBCCCCCBBBBBPauseBBBBSwitchingPauseAAAAAGrp
       ^      ^    ^    ^        ^                  ^
       |      |    |    |        |                  |
       |      |    |    ----------- Non-SwitchingVocalisation
       |      |    |
       ---------------------&gt; SwitchingVocalisation
</pre>


<h3>Value</h3>

<p>A vector with all events replaced by the appropriate type
identifier.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(vocdia)
identifyVocalisations(atddia$speaker[1:60])
</code></pre>

<hr>
<h2 id='igraph.vocaldia'>igraph.vocaldia: Create an igraph vocalisation diagram</h2><span id='topic+igraph.vocaldia'></span>

<h3>Description</h3>

<p>Create an igraph vocalisation diagram
</p>


<h3>Usage</h3>

<pre><code class='language-R'>igraph.vocaldia(vd, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="igraph.vocaldia_+3A_vd">vd</code></td>
<td>
<p>a vocalisation diagram</p>
</td></tr>
<tr><td><code id="igraph.vocaldia_+3A_...">...</code></td>
<td>
<p>arguments for the layout algorithm</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Create a vocalisation diagram
</p>


<h3>Value</h3>

<p>an igraph
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(vocdia)
if (require('igraph'))
    igraph.vocaldia(getSampledVocalMatrix(subset(atddia,
                                          id=='Abbott_Maddock_01'),
                  individual=TRUE, nodecolumn='speaker'))
</code></pre>

<hr>
<h2 id='makeSessionDataSet'>makeSessionDataSet: create a data frame for a session (e.g. cookie scene description) based on .cha transcription files</h2><span id='topic+makeSessionDataSet'></span>

<h3>Description</h3>

<p>makeSessionDataSet: create a data frame for a session (e.g. cookie scene description)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>makeSessionDataSet(
  f,
  sildir = NULL,
  silsuffix = "c.mp3.csv",
  srdir = "../data/ADReSS/speech_rate/",
  srsuffix = "sra",
  sprate = T
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="makeSessionDataSet_+3A_f">f</code></td>
<td>
<p>CHA file to read</p>
</td></tr>
<tr><td><code id="makeSessionDataSet_+3A_sildir">sildir</code></td>
<td>
<p>directory where silence profiles are stored</p>
</td></tr>
<tr><td><code id="makeSessionDataSet_+3A_silsuffix">silsuffix</code></td>
<td>
<p>suffix for silence files</p>
</td></tr>
<tr><td><code id="makeSessionDataSet_+3A_srdir">srdir</code></td>
<td>
<p>directory where speech rate csv (1 value per utterance) files are stored</p>
</td></tr>
<tr><td><code id="makeSessionDataSet_+3A_srsuffix">srsuffix</code></td>
<td>
<p>the suffix of the speech rate files (default: sre)</p>
</td></tr>
<tr><td><code id="makeSessionDataSet_+3A_sprate">sprate</code></td>
<td>
<p>estimate speech rate? (default: TRUE)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a speech session data frame
</p>


<h3>Author(s)</h3>

<p>luzs
</p>

<hr>
<h2 id='makeVocalStatsDataset'>makeVocalStatsDataset: create a dataset of vocalisation statistics (1 row per patient)</h2><span id='topic+makeVocalStatsDataset'></span>

<h3>Description</h3>

<p>Build a data frame createwith vocalisation statistics
</p>


<h3>Usage</h3>

<pre><code class='language-R'>makeVocalStatsDataset(
  dir = c("data/Pitt/Dementia/cookie", "data/Pitt/Control/cookie"),
  sildir = NULL,
  silsuffix = "c.mp3.csv",
  srdir = "data/Pitt/speech_rate/",
  srsuffix = "sra",
  sprate = T
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="makeVocalStatsDataset_+3A_dir">dir</code></td>
<td>
<p>a string or vector containing the location (directory path) of the DementiaBank transcript files (.cha files)</p>
</td></tr>
<tr><td><code id="makeVocalStatsDataset_+3A_sildir">sildir</code></td>
<td>
<p>directory where silence csv files are stored</p>
</td></tr>
<tr><td><code id="makeVocalStatsDataset_+3A_silsuffix">silsuffix</code></td>
<td>
<p>the suffix of the silence profile files 'c.mp3.csv'. The format of such files should be the format used by Audacity label files, i.e. 'start time, end time, label' (without header), where 'label' should be 'silence'</p>
</td></tr>
<tr><td><code id="makeVocalStatsDataset_+3A_srdir">srdir</code></td>
<td>
<p>directory where speech rate csv (1 value per utterance) files are stored</p>
</td></tr>
<tr><td><code id="makeVocalStatsDataset_+3A_srsuffix">srsuffix</code></td>
<td>
<p>the suffix of the speech rate files (default: sre)</p>
</td></tr>
<tr><td><code id="makeVocalStatsDataset_+3A_sprate">sprate</code></td>
<td>
<p>compute speech rate? (not in use yet)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a session's vocalisation feature stats
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
makeVocalStatsDataset(dir=c('ADReSS-IS2020-data/train/transcription/cc/',
                            'ADReSS-IS2020-data/train/transcription/cd/'),
                     sildir='ADReSS/silence/',
                     srdir='ADReSS/speech_rate/',
                     silsuffix='.wav-sil.csv')

## End(Not run)
</code></pre>

<hr>
<h2 id='matrixExp'>matrixExp: raise matrix to exp.</h2><span id='topic+matrixExp'></span>

<h3>Description</h3>

<p>Matrix exponentials
</p>


<h3>Usage</h3>

<pre><code class='language-R'>matrixExp(matrix, exp, mmatrix = matrix)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="matrixExp_+3A_matrix">matrix</code></td>
<td>
<p>a matrix</p>
</td></tr>
<tr><td><code id="matrixExp_+3A_exp">exp</code></td>
<td>
<p>the power to which matrix will be raised</p>
</td></tr>
<tr><td><code id="matrixExp_+3A_mmatrix">mmatrix</code></td>
<td>
<p>a placeholder.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A (sort of) exponential function for matrix multiplication (to be
used with <code><a href="#topic+staticMatrix">staticMatrix</a></code>).
</p>


<h3>Value</h3>

<p>matrix^exp
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(vocdia)
matrixExp(vocmatrix$ttarray, 3)
</code></pre>

<hr>
<h2 id='namePauses'>namePauses: name pause types.</h2><span id='topic+namePauses'></span>

<h3>Description</h3>

<p>Replace identified pause pause types in data frame.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>namePauses(df, nodecolumn = "role")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="namePauses_+3A_df">df</code></td>
<td>
<p>a data frame consisting, minimally, of a column for
vocalisation/pause start times, a column for end times, and a
column identifying the speaker, speaker role or 'Floor' (for
silences).</p>
</td></tr>
<tr><td><code id="namePauses_+3A_nodecolumn">nodecolumn</code></td>
<td>
<p>the name of the column containing the node
(speaker) name (e.g. 'speaker', 'role').</p>
</td></tr>
</table>


<h3>Details</h3>

<p>replace all 'Floor' speakers in df by 'Pause', 'SwitchingPause'
etc, and return a new data fame containing pause types in place of
'Floor' (see markov.R, identifyPauses() for a better
implementation)
</p>


<h3>Value</h3>

<p>a data.frame with pauses in nodecolumn replaced by different pause types.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+identifyPauses">identifyPauses</a></code> for a better implementation
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(vocdia)
x &lt;- subset(atddia, id=='Abbott_Maddock_01')
x[1:15,1:6]
namePauses(x)[1:15,1:6]
</code></pre>

<hr>
<h2 id='plot.matrixseries'>plotConvergence: plots Markov diagram convergence.</h2><span id='topic+plot.matrixseries'></span>

<h3>Description</h3>

<p>Visualise  convergence properties of vocalisation graphs
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'matrixseries'
plot(x, ..., par = list(), interact = F)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.matrixseries_+3A_x">x</code></td>
<td>
<p>an object of class matrixseries; a list where the
<code class="reqn">i^{th}</code> element corresponds to <code class="reqn">M^i</code>.</p>
</td></tr>
<tr><td><code id="plot.matrixseries_+3A_...">...</code></td>
<td>
<p>extra graphics parameters for plot.</p>
</td></tr>
<tr><td><code id="plot.matrixseries_+3A_par">par</code></td>
<td>
<p>graphic parameters alist</p>
</td></tr>
<tr><td><code id="plot.matrixseries_+3A_interact">interact</code></td>
<td>
<p>if TRUE, pauses the drawing after each node.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A 'toy' for visualisation of convergence properties of
vocalisation graphs. Plot the convergence paths of each
Vocalisation event (i.e. each row-column transition probability,
grouped by colour according to the inciding node)
</p>


<h3>Value</h3>

<p>the matrixseries
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(vocdia)
plot(staticMatrix(vocmatrix$ttarray, digits=4, history=TRUE))
</code></pre>

<hr>
<h2 id='plot.vocaldia'>plot.vocaldia</h2><span id='topic+plot.vocaldia'></span>

<h3>Description</h3>

<p>Plot a vocalisation diagram
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'vocaldia'
plot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.vocaldia_+3A_x">x</code></td>
<td>
<p>a vocalisation diagram</p>
</td></tr>
<tr><td><code id="plot.vocaldia_+3A_...">...</code></td>
<td>
<p>arguments for the layout algorithm</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Plot a vocalisation diagram
</p>


<h3>Value</h3>

<p><code>NULL</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(vocdia)
if (require('igraph'))
 plot(getSampledVocalMatrix(subset(atddia, id=='Abbott_Maddock_01'),
                          individual=TRUE, nodecolumn='speaker'))
</code></pre>

<hr>
<h2 id='printARFFfile'>printARFFfile: Create arff files by creating and flattening vocaldias</h2><span id='topic+printARFFfile'></span>

<h3>Description</h3>

<p>Generate ARFF files from vocalisation diagrams
</p>


<h3>Usage</h3>

<pre><code class='language-R'>printARFFfile(
  df,
  ids = c(),
  idcolumn = "id",
  noPauseTypes = F,
  sampled = 0,
  individual = TRUE,
  nodecolumn = "role",
  classcolumn = "dx",
  file = ""
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="printARFFfile_+3A_df">df</code></td>
<td>
<p>df a data frame consisting, minimally, of a column for
vocalisation/pause start times, a column for end times, and a
column identifying the speaker, speaker role or 'Floor' (for
silences).</p>
</td></tr>
<tr><td><code id="printARFFfile_+3A_ids">ids</code></td>
<td>
<p>Ids of dialogues to generate (as defined in column named idcolumn)</p>
</td></tr>
<tr><td><code id="printARFFfile_+3A_idcolumn">idcolumn</code></td>
<td>
<p>the name of the column containing the dialogue id</p>
</td></tr>
<tr><td><code id="printARFFfile_+3A_nopausetypes">noPauseTypes</code></td>
<td>
<p>if TRUE, ignore distinctions between pauses
(SwitchingPause, GrpSwitchingPause, etc)</p>
</td></tr>
<tr><td><code id="printARFFfile_+3A_sampled">sampled</code></td>
<td>
<p>if &gt;0 use <code><a href="#topic+getSampledVocalMatrix">getSampledVocalMatrix</a></code> with rate=sampled</p>
</td></tr>
<tr><td><code id="printARFFfile_+3A_individual">individual</code></td>
<td>
<p>whether to include individual speakers or group
them into a single Vocalisation node</p>
</td></tr>
<tr><td><code id="printARFFfile_+3A_nodecolumn">nodecolumn</code></td>
<td>
<p>the name of the column containing the node
(speaker) name (e.g. 'speaker', 'role').</p>
</td></tr>
<tr><td><code id="printARFFfile_+3A_classcolumn">classcolumn</code></td>
<td>
<p>the name of the column containing the target class (or value).</p>
</td></tr>
<tr><td><code id="printARFFfile_+3A_file">file</code></td>
<td>
<p>name of ARFF file to be generated, or &quot;&quot; (print to console).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Use this function to generate turn-taking diragrams in ARFF format for
</p>


<h3>References</h3>

<p>S. Luz. Automatic identification of experts and performance
prediction in the multimodal math data corpus through analysis
of speech interaction. In <em>Proceedings of the 15th ACM on
International Conference on Multimodal Interaction, ICMI'13</em>,
pages 575&ndash;582, New York, NY, USA, 2013. ACM.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+getSampledVocalCountMatrix">getSampledVocalCountMatrix</a></code>,
<code><a href="#topic+getTurnTakingProbMatrix">getTurnTakingProbMatrix</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(vocdia)
atdarff &lt;- tempfile(pattern='vocaldia-', fileext='arff')
printARFFfile(atddia, individual=TRUE, classcolumn='dx',
              file=atdarff, noPauseTypes=FALSE)
library("foreign")
x1 &lt;- read.arff(atdarff)
x1[1:3,]
## remove empty columns
x1[,c(unlist(apply(x1[1:(ncol(x1)-1)],2,sum)!=0), TRUE)]
</code></pre>

<hr>
<h2 id='read.cha'>read.cha read CHA transcription file (format used by DementiaBank)</h2><span id='topic+read.cha'></span>

<h3>Description</h3>

<p>read.cha: read CHA transcription file (format used by DementiaBank)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>read.cha(file, sildir = NULL, silsuffix = "c.mp3.csv")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="read.cha_+3A_file">file</code></td>
<td>
<p>.cha file to reas</p>
</td></tr>
<tr><td><code id="read.cha_+3A_sildir">sildir</code></td>
<td>
<p>silences directory</p>
</td></tr>
<tr><td><code id="read.cha_+3A_silsuffix">silsuffix</code></td>
<td>
<p>silence files suffix</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list containing the PID, a dataframe containing the speaker IDs and demogrephics, and a dataframe containing the speaker IDs, transcribed utterances, start and en times, speech rates etc.
</p>


<h3>Author(s)</h3>

<p>luzs
</p>

<hr>
<h2 id='startmatrix'>startmatrix: return the first matrix of a converging series.</h2><span id='topic+startmatrix'></span><span id='topic+startmatrix.default'></span><span id='topic+startmatrix.matrixseries'></span>

<h3>Description</h3>

<p>Access initital matrix in a <code>matrixseries</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>startmatrix(mseries)

## Default S3 method:
startmatrix(mseries)

## S3 method for class 'matrixseries'
startmatrix(mseries)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="startmatrix_+3A_mseries">mseries</code></td>
<td>
<p>a matrixseries object</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Access initital matrix in a <code>matrixseries</code>
</p>


<h3>Value</h3>

<p>the initial matrix.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
data(vocdia)
x2 &lt;- staticMatrix(vocmatrix$ttarray, digits=4, history=TRUE)
## original matrix
startmatrix(x2)

## End(Not run)
</code></pre>

<hr>
<h2 id='staticMatrix'>staticMatrix Iterate until transition probabilities converge (or give up).</h2><span id='topic+staticMatrix'></span>

<h3>Description</h3>

<p>Compute the stationary distribution for a Markov diagram
</p>


<h3>Usage</h3>

<pre><code class='language-R'>staticMatrix(matrix, limit = 1000, digits = 4, history = F)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="staticMatrix_+3A_matrix">matrix</code></td>
<td>
<p>an adjecency matrix of trnasition probabilities</p>
</td></tr>
<tr><td><code id="staticMatrix_+3A_limit">limit</code></td>
<td>
<p>maximum number of iterations until we give up on
convergence</p>
</td></tr>
<tr><td><code id="staticMatrix_+3A_digits">digits</code></td>
<td>
<p>the number of decimal places to compare</p>
</td></tr>
<tr><td><code id="staticMatrix_+3A_history">history</code></td>
<td>
<p>if TRUE, keep track of all matrix products</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Return static matrix (i.e. the stationary distribution) for the
Markov process represented by the given adjacency matrix. In the
particular case of vocaldia's, each column should roughly
correspond to the amount of time a speaker held the floor for).
Of course, not all Markov chains converge, an example being:
</p>
<pre>
           1
     /-----&gt;-------\
    A               B
     \----&lt;--------/
           1

which gives

.      | 0  1 |             | 0x0+1x1  0x1+1x0|   | 1  0 |
.  M = | 1  0 |  and  M^2 = | 1x0+0x1  1x1+1x0| = | 0  1 |

</pre>


<h3>Value</h3>

<p>a matrixseries object; that is, a list where each element
is either the initial matrix or the product of the two
preceding matrices
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(vocdia)
x2 &lt;- staticMatrix(vocmatrix$ttarray, digits=4, history=TRUE)
## original matrix
round(x2[[1]],3)
## stationary matrix (M^139)
round(x2[[length(x2)]],3)
</code></pre>

<hr>
<h2 id='toDotNotation'>toDotNotation: conver vocaldia to graphviz dot notation</h2><span id='topic+toDotNotation'></span>

<h3>Description</h3>

<p>Create vocalisation diagram to file in dot (graphviz) notation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>toDotNotation(
  vd,
  individual = T,
  varsizenode = T,
  shape = "circle",
  fontsize = 16,
  rankdir = "LR",
  nodeattribs = "fixedsize=true;",
  comment = ""
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="toDotNotation_+3A_vd">vd</code></td>
<td>
<p>a vocalisation diagram</p>
</td></tr>
<tr><td><code id="toDotNotation_+3A_individual">individual</code></td>
<td>
<p>if TRUE write individual node names</p>
</td></tr>
<tr><td><code id="toDotNotation_+3A_varsizenode">varsizenode</code></td>
<td>
<p>if true set varsizenode in dot</p>
</td></tr>
<tr><td><code id="toDotNotation_+3A_shape">shape</code></td>
<td>
<p>node shape</p>
</td></tr>
<tr><td><code id="toDotNotation_+3A_fontsize">fontsize</code></td>
<td>
<p>font size</p>
</td></tr>
<tr><td><code id="toDotNotation_+3A_rankdir">rankdir</code></td>
<td>
<p>direction of ranking (LR, RF etc)</p>
</td></tr>
<tr><td><code id="toDotNotation_+3A_nodeattribs">nodeattribs</code></td>
<td>
<p>attributes for node</p>
</td></tr>
<tr><td><code id="toDotNotation_+3A_comment">comment</code></td>
<td>
<p>comments</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Create a vocalisation diagram in dot notation
</p>


<h3>Value</h3>

<p>character data containing the diagram in dot format.
</p>


<h3>See Also</h3>

<p>graphviz manual
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(vocdia)
toDotNotation(getSampledVocalMatrix(subset(atddia,
                                           id=='Abbott_Maddock_01'),
                             individual=TRUE, nodecolumn='speaker'))
</code></pre>

<hr>
<h2 id='vocmatrix'>A sample vocalisation matrix</h2><span id='topic+vocmatrix'></span>

<h3>Description</h3>

<p>A <code>vocaldia</code> object containing a 3-speaker dialogue
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vocmatrix
</code></pre>


<h3>Format</h3>

<p>A list containing 2 arrays
</p>

<dl>
<dt>ttarray</dt><dd><p>The vocaldia adjacency matrix</p>
</dd>
<dt>tdarray</dt><dd><p>The proportional durations (stationary probabilities) of each event (node)</p>
</dd>
</dl>



<h3>Source</h3>

<p>This dataset was generated from the Multomodal Learning
Analytics dataset, for the eponymous ICMI'13 Grand
Challenge. The use these vocaldias were put to is described in
Luz (2013). The full dataset and code is available
at https://gitlab.scss.tcd.ie/saturnino.luz/icmi-mla-challenge
</p>


<h3>References</h3>

<p>S. Luz. Automatic identification of experts and performance
prediction in the multimodal math data corpus through analysis
of speech interaction. In <em>Proceedings of the 15th ACM on
International Conference on Multimodal Interaction, ICMI'13</em>,
pages 575&ndash;582, New York, NY, USA, 2013. ACM.
</p>

<hr>
<h2 id='write.vocaldia'>write.vocaldia</h2><span id='topic+write.vocaldia'></span>

<h3>Description</h3>

<p>Write vocalisation diagram to file in dot (graphviz) notation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>write.vocaldia(vd, file = "", ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="write.vocaldia_+3A_vd">vd</code></td>
<td>
<p>a vocalisation diagram</p>
</td></tr>
<tr><td><code id="write.vocaldia_+3A_file">file</code></td>
<td>
<p>name of file to which dot diagram will be written.</p>
</td></tr>
<tr><td><code id="write.vocaldia_+3A_...">...</code></td>
<td>
<p>arguments passed on to toDotNotation. If &quot;&quot;, write to STDOUT.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Write a vocalisation diagram
</p>


<h3>Value</h3>

<p><code>NULL</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(vocdia)
write.vocaldia(getSampledVocalMatrix(subset(atddia,
                                            id=='Abbott_Maddock_01'),
                       individual=TRUE, nodecolumn='speaker'),
                       file=tempfile(pattern='vocaldia-', fileext='.dot'))
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
