<!DOCTYPE html><html lang="en-US"><head><title>Help for package ODRF</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {ODRF}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#+25+26gt+3B+25'><p>Pipe operator</p></a></li>
<li><a href='#Accuracy'><p>accuracy of oblique decision random forest</p></a></li>
<li><a href='#as.party.ODT'><p><code>ODT</code> as <code>party</code></p></a></li>
<li><a href='#best.cut.node'><p>find best splitting variable and node</p></a></li>
<li><a href='#body_fat'><p>Body Fat Prediction Dataset</p></a></li>
<li><a href='#breast_cancer'><p>Breast Cancer Dataset</p></a></li>
<li><a href='#defaults'><p>Default values passed to RotMat*</p></a></li>
<li><a href='#ODRF'><p>Classification and Regression using Oblique Decision Random Forest</p></a></li>
<li><a href='#ODT'><p>Classification and Regression with Oblique Decision Tree</p></a></li>
<li><a href='#online'><p>online structure learning for class <code>ODT</code> and <code>ODRF</code>.</p></a></li>
<li><a href='#online.ODRF'><p>using new training data to update an existing <code>ODRF</code>.</p></a></li>
<li><a href='#online.ODT'><p>using new training data to update an existing <code>ODT</code>.</p></a></li>
<li><a href='#plot_ODT_depth'><p>plot oblique decision tree depth</p></a></li>
<li><a href='#plot.Accuracy'><p>plot method for <code>Accuracy</code> objects</p></a></li>
<li><a href='#plot.ODT'><p>to plot an oblique decision tree</p></a></li>
<li><a href='#plot.prune.ODT'><p>to plot pruned oblique decision tree</p></a></li>
<li><a href='#plot.VarImp'><p>Variable Importance Plot</p></a></li>
<li><a href='#PPO'><p>Projection Pursuit Optimization</p></a></li>
<li><a href='#predict.ODRF'><p>predict based on an ODRF object</p></a></li>
<li><a href='#predict.ODT'><p>making predict based on ODT objects</p></a></li>
<li><a href='#print.ODRF'><p>print ODRF</p></a></li>
<li><a href='#print.ODT'><p>print ODT result</p></a></li>
<li><a href='#prune'><p>prune <code>ODT</code> or <code>ODRF</code></p></a></li>
<li><a href='#prune.ODRF'><p>Pruning of class <code>ODRF</code>.</p></a></li>
<li><a href='#prune.ODT'><p>pruning of class <code>ODT</code></p></a></li>
<li><a href='#RandRot'><p>Samples a p x p uniformly random rotation matrix</p></a></li>
<li><a href='#RotMatMake'><p>Create rotation matrix used to determine the linear combination of features.</p></a></li>
<li><a href='#RotMatPPO'><p>Create a Projection Matrix: RotMatPPO</p></a></li>
<li><a href='#RotMatRand'><p>Random Rotation Matrix</p></a></li>
<li><a href='#RotMatRF'><p>Create a Projection Matrix: Random Forest (RF)</p></a></li>
<li><a href='#seeds'><p>seeds Data Set</p></a></li>
<li><a href='#VarImp'><p>Extract variable importance measure</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Oblique Decision Random Forest for Classification and Regression</td>
</tr>
<tr>
<td>Version:</td>
<td>0.0.4</td>
</tr>
<tr>
<td>Author:</td>
<td>Yu Liu [aut, cre, cph],
  Yingcun Xia [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Yu Liu &lt;liuyuchina123@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>The oblique decision tree (ODT) uses linear combinations of
    predictors as partitioning variables in a decision tree. Oblique
    Decision Random Forest (ODRF) is an ensemble of multiple ODTs
    generated by feature bagging. Both can be used for classification and
    regression as supplements to the classical CART of Breiman (1984)
    &lt;<a href="https://doi.org/10.1201%2F9781315139470">doi:10.1201/9781315139470</a>&gt; and Random Forest of Breiman (2001)
    &lt;<a href="https://doi.org/10.1023%2FA%3A1010933404324">doi:10.1023/A:1010933404324</a>&gt; respectively.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://liuyu-star.github.io/ODRF/">https://liuyu-star.github.io/ODRF/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/liuyu-star/ODRF/issues">https://github.com/liuyu-star/ODRF/issues</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>partykit, R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>doParallel, foreach, glue, graphics, grid, lifecycle,
magrittr, nnet, parallel, Pursuit, Rcpp, rlang (&ge; 0.4.11),
stats</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown, spelling, testthat (&ge; 3.0.0)</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Language:</td>
<td>en-US</td>
</tr>
<tr>
<td>LazyData:</td>
<td>yes</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-05-28 03:59:25 UTC; Administrator</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-05-28 04:20:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='+25+26gt+3B+25'>Pipe operator</h2><span id='topic++25+3E+25'></span>

<h3>Description</h3>

<p>See <code>magrittr::<a href="magrittr.html#topic+pipe">%&gt;%</a></code> for details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lhs %&gt;% rhs
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="+2B25+2B26gt+2B3B+2B25_+3A_lhs">lhs</code></td>
<td>
<p>A value or the magrittr placeholder.</p>
</td></tr>
<tr><td><code id="+2B25+2B26gt+2B3B+2B25_+3A_rhs">rhs</code></td>
<td>
<p>A function call using the magrittr semantics.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The result of calling <code>rhs(lhs)</code>.
</p>

<hr>
<h2 id='Accuracy'>accuracy of oblique decision random forest</h2><span id='topic+Accuracy'></span>

<h3>Description</h3>

<p>Prediction accuracy of ODRF at different tree sizes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Accuracy(obj, data, newdata = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Accuracy_+3A_obj">obj</code></td>
<td>
<p>An object of class <code>ODRF</code>, as that created by the function <code><a href="#topic+ODRF">ODRF</a></code>.</p>
</td></tr>
<tr><td><code id="Accuracy_+3A_data">data</code></td>
<td>
<p>Training data of class <code>data.frame</code> in <code><a href="#topic+ODRF">ODRF</a></code> is used to calculate the OOB error.</p>
</td></tr>
<tr><td><code id="Accuracy_+3A_newdata">newdata</code></td>
<td>
<p>A data frame or matrix containing new data is used to calculate the test error. If it is missing, then it is replaced by <code>data</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>OOB error and test error, misclassification rate (MR) for classification or mean square error (MSE) for regression.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ODRF">ODRF</a></code> <code><a href="#topic+VarImp">VarImp</a></code> <code><a href="#topic+plot.Accuracy">plot.Accuracy</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(breast_cancer)
set.seed(221212)
train &lt;- sample(1:569, 80)
train_data &lt;- data.frame(breast_cancer[train, -1])
test_data &lt;- data.frame(breast_cancer[-train, -1])

forest &lt;- ODRF(diagnosis ~ ., train_data, split = "gini",
parallel = FALSE, ntrees = 50)
(error &lt;- Accuracy(forest, train_data, test_data))

</code></pre>

<hr>
<h2 id='as.party.ODT'><code>ODT</code> as <code>party</code></h2><span id='topic+as.party.ODT'></span>

<h3>Description</h3>

<p>To make <code>ODT</code> object to objects of class <code>party</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ODT'
as.party(obj, data, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="as.party.ODT_+3A_obj">obj</code></td>
<td>
<p>An object of class <code><a href="#topic+ODT">ODT</a></code>.</p>
</td></tr>
<tr><td><code id="as.party.ODT_+3A_data">data</code></td>
<td>
<p>Training data of class <code>data.frame</code> is used to convert the object of class <code>ODT</code>,
and it must be the training data <code>data</code> in <code><a href="#topic+ODT">ODT</a></code>.</p>
</td></tr>
<tr><td><code id="as.party.ODT_+3A_...">...</code></td>
<td>
<p>Arguments to be passed to methods</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An objects of class <code>party</code>.
</p>


<h3>References</h3>

<p>Lee, EK(2017) PPtreeViz: An R Package for Visualizing Projection Pursuit Classification Trees, Journal of Statistical Software.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ODT">ODT</a></code> <code><a href="partykit.html#topic+party">party</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
tree &lt;- ODT(Species ~ ., data = iris)
tree
plot(tree)
party.tree &lt;- as.party(tree, data = iris)
party.tree
plot(party.tree)

</code></pre>

<hr>
<h2 id='best.cut.node'>find best splitting variable and node</h2><span id='topic+best.cut.node'></span>

<h3>Description</h3>

<p>A function to select the splitting variables and nodes using one of three criteria.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>best.cut.node(
  X,
  y,
  split,
  lambda = "log",
  weights = 1,
  MinLeaf = 10,
  numLabels = ifelse(split == "mse", 0, length(unique(y)))
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="best.cut.node_+3A_x">X</code></td>
<td>
<p>An n by d numeric matrix (preferable) or data frame.</p>
</td></tr>
<tr><td><code id="best.cut.node_+3A_y">y</code></td>
<td>
<p>A response vector of length n.</p>
</td></tr>
<tr><td><code id="best.cut.node_+3A_split">split</code></td>
<td>
<p>One of three criteria, 'gini': gini impurity index (classification), 'entropy': information gain (classification)
or 'mse': mean square error (regression).</p>
</td></tr>
<tr><td><code id="best.cut.node_+3A_lambda">lambda</code></td>
<td>
<p>The argument of <code>split</code> is used to determine the penalty level of the partition criterion. Three options are provided including, <code>lambda=0</code>: no penalty; <code>lambda=2</code>: AIC penalty; <code>lambda='log'</code> (Default): BIC penalty. In Addition, lambda can be any value from 0 to n (training set size).</p>
</td></tr>
<tr><td><code id="best.cut.node_+3A_weights">weights</code></td>
<td>
<p>A vector of values which weigh the samples when considering a split.</p>
</td></tr>
<tr><td><code id="best.cut.node_+3A_minleaf">MinLeaf</code></td>
<td>
<p>Minimal node size (Default 10).</p>
</td></tr>
<tr><td><code id="best.cut.node_+3A_numlabels">numLabels</code></td>
<td>
<p>The number of categories.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list which contains:
</p>

<ul>
<li><p> BestCutVar: The best split variable.
</p>
</li>
<li><p> BestCutVal: The best split points for the best split variable.
</p>
</li>
<li><p> BestIndex: Each variable corresponds to maximum decrease in gini impurity index, information gain, and mean square error.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>### Find the best split variable ###
data(iris)
X &lt;- as.matrix(iris[, 1:4])
y &lt;- iris[[5]]
bestcut &lt;- best.cut.node(X, y, split = "gini")
print(bestcut)

</code></pre>

<hr>
<h2 id='body_fat'>Body Fat Prediction Dataset</h2><span id='topic+body_fat'></span>

<h3>Description</h3>

<p>Lists estimates of the percentage of body fat determined by underwater
weighing and various body circumference measurements for 252 men.
Accurate measurement of body fat is inconvenient/costly and it is desirable to have easy methods of estimating body fat that are not inconvenient/costly.
</p>


<h3>Format</h3>

<p>A data frame with 252 rows and 15 covariate variables and 1 response variable
</p>


<h3>Details</h3>

<p>The variables listed below, from left to right, are:
</p>

<ul>
<li><p> Density determined from underwater weighing
</p>
</li>
<li><p> Age (years)
</p>
</li>
<li><p> Weight (lbs)
</p>
</li>
<li><p> Height (inches)
</p>
</li>
<li><p> Neck circumference (cm)
</p>
</li>
<li><p> Chest circumference (cm)
</p>
</li>
<li><p> Abdomen 2 circumference (cm)
</p>
</li>
<li><p> Hip circumference (cm)
</p>
</li>
<li><p> Thigh circumference (cm)
</p>
</li>
<li><p> Knee circumference (cm)
</p>
</li>
<li><p> Ankle circumference (cm)
</p>
</li>
<li><p> Biceps (extended) circumference (cm)
</p>
</li>
<li><p> Forearm circumference (cm)
</p>
</li>
<li><p> Wrist circumference (cm)
</p>
</li></ul>



<h3>Source</h3>

<p><a href="https://www.kaggle.com/datasets/fedesoriano/body-fat-prediction-dataset">https://www.kaggle.com/datasets/fedesoriano/body-fat-prediction-dataset</a>
</p>


<h3>References</h3>

<p>Bailey, Covert (1994). Smart Exercise: Burning Fat, Getting Fit, Houghton-Mifflin Co., Boston, pp. 179-186.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+breast_cancer">breast_cancer</a></code> <code><a href="#topic+seeds">seeds</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(body_fat)
set.seed(221212)
train &lt;- sample(1:252, 60)
train_data &lt;- data.frame(body_fat[train, ])
test_data &lt;- data.frame(body_fat[-train, ])

forest &lt;- ODRF(Density ~ ., train_data, split = "mse", parallel = FALSE, ntrees = 50)
pred &lt;- predict(forest, test_data[, -1])
# estimation error
mean((pred - test_data[, 1])^2)

tree &lt;- ODT(Density ~ ., train_data, split = "mse")
pred &lt;- predict(tree, test_data[, -1])
# estimation error
mean((pred - test_data[, 1])^2)

</code></pre>

<hr>
<h2 id='breast_cancer'>Breast Cancer Dataset</h2><span id='topic+breast_cancer'></span>

<h3>Description</h3>

<p>Breast cancer is the most common cancer amongst women in the world. It accounts for <code class="reqn">25\%</code> of all cancer cases, and affected over 2.1 Million people in 2015 alone.
It starts when cells in the breast begin to grow out of control. These cells usually form tumors that can be seen via X-ray or felt as lumps in the breast area.
The key challenges against it's detection is how to classify tumors into malignant (cancerous) or benign(non cancerous).
</p>


<h3>Format</h3>

<p>A data frame with 569 rows and 30 covariate variables and 1 response variable
</p>


<h3>Details</h3>

<p>The actual linear program used to obtain the separating plane in the 3-dimensional space is that described in:
</p>

<ul>
<li><p> ID number
</p>
</li>
<li><p> Diagnosis (M = malignant, B = benign)
</p>
</li>
<li><p> Ten real-valued features are computed for each cell nucleus:
</p>

<ul>
<li><p> radius (mean of distances from center to points on the perimeter)
</p>
</li>
<li><p> texture (standard deviation of gray-scale values)
</p>
</li>
<li><p> perimeter
</p>
</li>
<li><p> area
</p>
</li>
<li><p> smoothness (local variation in radius lengths)
</p>
</li>
<li><p> compactness (perimeter^2 / area - 1.0)
</p>
</li>
<li><p> concavity (severity of concave portions of the contour)
</p>
</li>
<li><p> concave points (number of concave portions of the contour)
</p>
</li>
<li><p> symmetry
</p>
</li>
<li><p> fractal dimension (&quot;coastline approximation&quot; - 1)
</p>
</li></ul>
</li></ul>



<h3>Source</h3>

<p><a href="https://www.kaggle.com/datasets/yasserh/breast-cancer-dataset?select=breast-cancer.csv">https://www.kaggle.com/datasets/yasserh/breast-cancer-dataset?select=breast-cancer.csv</a> and <a href="https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic)">https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic)</a>
</p>


<h3>References</h3>

<p>Wolberg WH, Street WN, Mangasarian OL. Machine learning techniques to diagnose breast cancer from image-processed nuclear features of fine needle aspirates. Cancer Lett. 1994 Mar 15;77(2-3):163-71.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+body_fat">body_fat</a></code> <code><a href="#topic+seeds">seeds</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(breast_cancer)
set.seed(221212)
train &lt;- sample(1:569, 80)
train_data &lt;- data.frame(breast_cancer[train, -1])
test_data &lt;- data.frame(breast_cancer[-train, -1])

forest &lt;- ODRF(diagnosis ~ ., train_data, split = "gini", parallel = FALSE, ntrees = 50)
pred &lt;- predict(forest, test_data[, -1])
# classification error
(mean(pred != test_data[, 1]))

tree &lt;- ODT(diagnosis ~ ., train_data, split = "gini")
pred &lt;- predict(tree, test_data[, -1])
# classification error
(mean(pred != test_data[, 1]))
</code></pre>

<hr>
<h2 id='defaults'>Default values passed to RotMat*</h2><span id='topic+defaults'></span>

<h3>Description</h3>

<p>Given the parameter list and the categorical map this function populates the values of the parameter list accoding to our 'best'
known general use case parameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>defaults(
  paramList,
  split = "entropy",
  dimX = NULL,
  weights = NULL,
  catLabel = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="defaults_+3A_paramlist">paramList</code></td>
<td>
<p>A list (possibly empty), to be populated with a set of default values to be passed to a <code>RotMat*</code> function.</p>
</td></tr>
<tr><td><code id="defaults_+3A_split">split</code></td>
<td>
<p>The criterion used for splitting the variable. 'gini': gini impurity index (classification, default),
'entropy': information gain (classification) or 'mse': mean square error (regression).</p>
</td></tr>
<tr><td><code id="defaults_+3A_dimx">dimX</code></td>
<td>
<p>An integer denoting the number of columns in the design matrix X.</p>
</td></tr>
<tr><td><code id="defaults_+3A_weights">weights</code></td>
<td>
<p>A vector of length same as <code>data</code> that are positive weights.(default NULL)</p>
</td></tr>
<tr><td><code id="defaults_+3A_catlabel">catLabel</code></td>
<td>
<p>A category labels of class <code>list</code> in predictors. (default NULL, for details see Examples of <code><a href="#topic+ODT">ODT</a></code>)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Default parameters of the RotMat* function.
</p>

<ul>
<li> <p><code>dimX</code> An integer denoting the number of columns in the design matrix X.
</p>
</li>
<li> <p><code>dimProj</code> Number of variables to be projected, default <code>dimProj="Rand"</code>: random from 1 to ncol(X).
</p>
</li>
<li> <p><code>numProj</code> the number of projection directions.(default <code>ceiling(sqrt(dimX))</code>)
</p>
</li>
<li> <p><code>catLabel</code> A category labels of class <code>list</code> in prediction variables, for details see Examples of <code><a href="#topic+ODRF">ODRF</a></code>.
</p>
</li>
<li> <p><code>weights</code> A vector of length same as <code>data</code> that are positive weights.(default NULL)
</p>
</li>
<li> <p><code>lambda</code> Parameter of the Poisson distribution (default 1).
</p>
</li>
<li> <p><code>sparsity</code> A real number in <code class="reqn">(0,1)</code> that specifies the distribution of non-zero elements in the random matrix.
When <code>sparsity</code>=&quot;pois&quot; means that non-zero elements are generated by the p(<code>lambda</code>) Poisson distribution.
</p>
</li>
<li> <p><code>prob</code> A probability <code class="reqn">\in (0,1)</code> used for sampling from.
</p>
</li>
<li> <p><code>randDist</code> Parameter of the Poisson distribution (default 1).
</p>
</li>
<li> <p><code>split</code> The criterion used for splitting the variable. 'gini': gini impurity index (classification, default),
'entropy': information gain (classification) or 'mse': mean square error (regression).
</p>
</li>
<li> <p><code>model</code> Model for projection pursuit. (see <code><a href="#topic+PPO">PPO</a></code>)
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+RotMatPPO">RotMatPPO</a></code> <code><a href="#topic+RotMatRand">RotMatRand</a></code> <code><a href="#topic+RotMatRF">RotMatRF</a></code> <code><a href="#topic+RotMatMake">RotMatMake</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
paramList &lt;- list(dimX = 8, numProj = 3, sparsity = 0.25, prob = 0.5)
(paramList &lt;- defaults(paramList, split = "entropy"))
</code></pre>

<hr>
<h2 id='ODRF'>Classification and Regression using Oblique Decision Random Forest</h2><span id='topic+ODRF'></span><span id='topic+ODRF.formula'></span><span id='topic+ODRF.default'></span>

<h3>Description</h3>

<p>Classification and regression implemented by the oblique decision random forest. ODRF usually produces more accurate predictions than RF, but needs longer computation time.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ODRF(X, ...)

## S3 method for class 'formula'
ODRF(
  formula,
  data = NULL,
  split = "auto",
  lambda = "log",
  NodeRotateFun = "RotMatPPO",
  FunDir = getwd(),
  paramList = NULL,
  ntrees = 100,
  storeOOB = TRUE,
  replacement = TRUE,
  stratify = TRUE,
  ratOOB = 1/3,
  parallel = TRUE,
  numCores = Inf,
  MaxDepth = Inf,
  numNode = Inf,
  MinLeaf = 5,
  subset = NULL,
  weights = NULL,
  na.action = na.fail,
  catLabel = NULL,
  Xcat = 0,
  Xscale = "Min-max",
  TreeRandRotate = FALSE,
  ...
)

## Default S3 method:
ODRF(
  X,
  y,
  split = "auto",
  lambda = "log",
  NodeRotateFun = "RotMatPPO",
  FunDir = getwd(),
  paramList = NULL,
  ntrees = 100,
  storeOOB = TRUE,
  replacement = TRUE,
  stratify = TRUE,
  ratOOB = 1/3,
  parallel = TRUE,
  numCores = Inf,
  MaxDepth = Inf,
  numNode = Inf,
  MinLeaf = 5,
  subset = NULL,
  weights = NULL,
  na.action = na.fail,
  catLabel = NULL,
  Xcat = 0,
  Xscale = "Min-max",
  TreeRandRotate = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ODRF_+3A_x">X</code></td>
<td>
<p>An n by d numeric matrix (preferable) or data frame.</p>
</td></tr>
<tr><td><code id="ODRF_+3A_...">...</code></td>
<td>
<p>Optional parameters to be passed to the low level function.</p>
</td></tr>
<tr><td><code id="ODRF_+3A_formula">formula</code></td>
<td>
<p>Object of class <code>formula</code> with a response describing the model to fit. If this is a data frame, it is taken as the model frame. (see <code><a href="stats.html#topic+model.frame">model.frame</a></code>)</p>
</td></tr>
<tr><td><code id="ODRF_+3A_data">data</code></td>
<td>
<p>Training data of class <code>data.frame</code> containing variables named in the formula. If <code>data</code> is missing it is obtained from the current environment by <code>formula</code>.</p>
</td></tr>
<tr><td><code id="ODRF_+3A_split">split</code></td>
<td>
<p>The criterion used for splitting the nodes. &quot;entropy&quot;: information gain and &quot;gini&quot;: gini impurity index for classification; &quot;mse&quot;: mean square error for regression;
'auto' (default): If the response in <code>data</code> or <code>y</code> is a factor, &quot;gini&quot; is used, otherwise regression is assumed.</p>
</td></tr>
<tr><td><code id="ODRF_+3A_lambda">lambda</code></td>
<td>
<p>The argument of <code>split</code> is used to determine the penalty level of the partition criterion. Three options are provided including, <code>lambda=0</code>: no penalty; <code>lambda=2</code>: AIC penalty; <code>lambda='log'</code> (Default): BIC penalty. In Addition, lambda can be any value from 0 to n (training set size).</p>
</td></tr>
<tr><td><code id="ODRF_+3A_noderotatefun">NodeRotateFun</code></td>
<td>
<p>Name of the function of class <code>character</code> that implements a linear combination of predictors in the split node.
including </p>

<ul>
<li><p>&quot;RotMatPPO&quot;: projection pursuit optimization model (<code><a href="#topic+PPO">PPO</a></code>), see <code><a href="#topic+RotMatPPO">RotMatPPO</a></code> (default, model=&quot;PPR&quot;).
</p>
</li>
<li><p>&quot;RotMatRF&quot;: single feature similar to Random Forest, see <code><a href="#topic+RotMatRF">RotMatRF</a></code>.
</p>
</li>
<li><p>&quot;RotMatRand&quot;: random rotation, see <code><a href="#topic+RotMatRand">RotMatRand</a></code>.
</p>
</li>
<li><p>&quot;RotMatMake&quot;: users can define this function, for details see <code><a href="#topic+RotMatMake">RotMatMake</a></code>.
</p>
</li></ul>
</td></tr>
<tr><td><code id="ODRF_+3A_fundir">FunDir</code></td>
<td>
<p>The path to the <code>function</code> of the user-defined <code>NodeRotateFun</code> (default current working directory).</p>
</td></tr>
<tr><td><code id="ODRF_+3A_paramlist">paramList</code></td>
<td>
<p>List of parameters used by the functions <code>NodeRotateFun</code>. If left unchanged, default values will be used, for details see <code><a href="#topic+defaults">defaults</a></code>.</p>
</td></tr>
<tr><td><code id="ODRF_+3A_ntrees">ntrees</code></td>
<td>
<p>The number of trees in the forest (default 100).</p>
</td></tr>
<tr><td><code id="ODRF_+3A_storeoob">storeOOB</code></td>
<td>
<p>If TRUE then the samples omitted during the creation of a tree are stored as part of the tree (default TRUE).</p>
</td></tr>
<tr><td><code id="ODRF_+3A_replacement">replacement</code></td>
<td>
<p>if TRUE then n samples are chosen, with replacement, from training data (default TRUE).</p>
</td></tr>
<tr><td><code id="ODRF_+3A_stratify">stratify</code></td>
<td>
<p>If TRUE then class sample proportions are maintained during the random sampling. Ignored if replacement = FALSE (default TRUE).</p>
</td></tr>
<tr><td><code id="ODRF_+3A_ratoob">ratOOB</code></td>
<td>
<p>Ratio of 'out-of-bag' (default 1/3).</p>
</td></tr>
<tr><td><code id="ODRF_+3A_parallel">parallel</code></td>
<td>
<p>Parallel computing or not (default TRUE).</p>
</td></tr>
<tr><td><code id="ODRF_+3A_numcores">numCores</code></td>
<td>
<p>Number of cores to be used for parallel computing (default <code>Inf</code>).</p>
</td></tr>
<tr><td><code id="ODRF_+3A_maxdepth">MaxDepth</code></td>
<td>
<p>The maximum depth of the tree (default <code>Inf</code>).</p>
</td></tr>
<tr><td><code id="ODRF_+3A_numnode">numNode</code></td>
<td>
<p>Number of nodes that can be used by the tree (default <code>Inf</code>).</p>
</td></tr>
<tr><td><code id="ODRF_+3A_minleaf">MinLeaf</code></td>
<td>
<p>Minimal node size (Default 5).</p>
</td></tr>
<tr><td><code id="ODRF_+3A_subset">subset</code></td>
<td>
<p>An index vector indicating which rows should be used. (NOTE: If given, this argument must be named.)</p>
</td></tr>
<tr><td><code id="ODRF_+3A_weights">weights</code></td>
<td>
<p>Vector of non-negative observational weights; fractional weights are allowed (default NULL).</p>
</td></tr>
<tr><td><code id="ODRF_+3A_na.action">na.action</code></td>
<td>
<p>A function to specify the action to be taken if NAs are found. (NOTE: If given, this argument must be named.)</p>
</td></tr>
<tr><td><code id="ODRF_+3A_catlabel">catLabel</code></td>
<td>
<p>A category labels of class <code>list</code> in predictors. (default NULL, for details see Examples)</p>
</td></tr>
<tr><td><code id="ODRF_+3A_xcat">Xcat</code></td>
<td>
<p>A class <code>vector</code> is used to indicate which predictor is the categorical variable. The default Xcat=0 means that no special treatment is given to category variables.
When Xcat=NULL, the predictor x that satisfies the condition &quot;<code>(length(table(x))&lt;10) &amp; (length(x)&gt;20)</code>&quot; is judged to be a category variable.</p>
</td></tr>
<tr><td><code id="ODRF_+3A_xscale">Xscale</code></td>
<td>
<p>Predictor standardization methods. &quot; Min-max&quot; (default), &quot;Quantile&quot;, &quot;No&quot; denote Min-max transformation, Quantile transformation and No transformation respectively.</p>
</td></tr>
<tr><td><code id="ODRF_+3A_treerandrotate">TreeRandRotate</code></td>
<td>
<p>If or not to randomly rotate the training data before building the tree (default FALSE, see <code><a href="#topic+RandRot">RandRot</a></code>).</p>
</td></tr>
<tr><td><code id="ODRF_+3A_y">y</code></td>
<td>
<p>A response vector of length n.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class ODRF Containing a list components:
</p>

<ul>
<li><p><code>call</code>: The original call to ODRF.
</p>
</li>
<li><p><code>terms</code>: An object of class <code>c("terms", "formula")</code> (see <code><a href="stats.html#topic+terms.object">terms.object</a></code>) summarizing the formula. Used by various methods, but typically not of direct relevance to users.
</p>
</li>
<li><p><code>split</code>, <code>Levels</code> and <code>NodeRotateFun</code> are important parameters for building the tree.
</p>
</li>
<li><p><code>predicted</code>: the predicted values of the training data based on out-of-bag samples.
</p>
</li>
<li><p><code>paramList</code>: Parameters in a named list to be used by <code>NodeRotateFun</code>.
</p>
</li>
<li><p><code>oobErr</code>: 'out-of-bag' error for forest, misclassification rate (MR) for classification or mean square error (MSE) for regression.
</p>
</li>
<li><p><code>oobConfusionMat</code>: 'out-of-bag' confusion matrix for forest.
</p>
</li>
<li><p><code>structure</code>: Each tree structure used to build the forest. </p>

<ul>
<li><p><code>oobErr</code>: 'out-of-bag' error for tree, misclassification rate (MR) for classification or mean square error (MSE) for regression.
</p>
</li>
<li><p><code>oobIndex</code>: Which training data to use as 'out-of-bag'.
</p>
</li>
<li><p><code>oobPred</code>: Predicted value for 'out-of-bag'.
</p>
</li>
<li><p><code>others</code>: Same tree structure return value as <code><a href="#topic+ODT">ODT</a></code>.
</p>
</li></ul>

</li>
<li><p><code>data</code>: The list of data related parameters used to build the forest.
</p>
</li>
<li><p><code>tree</code>: The list of tree related parameters used to build the tree.
</p>
</li>
<li><p><code>forest</code>: The list of forest related parameters used to build the forest.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Yu Liu and Yingcun Xia
</p>


<h3>References</h3>

<p>Zhan, H., Liu, Y., &amp; Xia, Y. (2022). Consistency of The Oblique Decision Tree and Its Random Forest. arXiv preprint arXiv:2211.12653.
</p>
<p>Tomita, T. M., Browne, J., Shen, C., Chung, J., Patsolic, J. L., Falk, B., ... &amp; Vogelstein, J. T. (2020). Sparse projection oblique randomer forests. Journal of machine learning research, 21(104).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+online.ODRF">online.ODRF</a></code> <code><a href="#topic+prune.ODRF">prune.ODRF</a></code> <code><a href="#topic+predict.ODRF">predict.ODRF</a></code> <code><a href="#topic+print.ODRF">print.ODRF</a></code> <code><a href="#topic+Accuracy">Accuracy</a></code> <code><a href="#topic+VarImp">VarImp</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Classification with Oblique Decision Randome Forest.
data(seeds)
set.seed(221212)
train &lt;- sample(1:209, 80)
train_data &lt;- data.frame(seeds[train, ])
test_data &lt;- data.frame(seeds[-train, ])
forest &lt;- ODRF(varieties_of_wheat ~ ., train_data,
  split = "entropy",parallel = FALSE, ntrees = 50
)
pred &lt;- predict(forest, test_data[, -8])
# classification error
(mean(pred != test_data[, 8]))

# Regression with Oblique Decision Randome Forest.
data(body_fat)
set.seed(221212)
train &lt;- sample(1:252, 80)
train_data &lt;- data.frame(body_fat[train, ])
test_data &lt;- data.frame(body_fat[-train, ])
forest &lt;- ODRF(Density ~ ., train_data,
  split = "mse", parallel = FALSE,
  NodeRotateFun = "RotMatPPO", paramList = list(model = "Log", dimProj = "Rand")
)
pred &lt;- predict(forest, test_data[, -1])
# estimation error
mean((pred - test_data[, 1])^2)


### Train ODRF on one-of-K encoded categorical data ###
# Note that the category variable must be placed at the beginning of the predictor X
# as in the following example.
set.seed(22)
Xcol1 &lt;- sample(c("A", "B", "C"), 100, replace = TRUE)
Xcol2 &lt;- sample(c("1", "2", "3", "4", "5"), 100, replace = TRUE)
Xcon &lt;- matrix(rnorm(100 * 3), 100, 3)
X &lt;- data.frame(Xcol1, Xcol2, Xcon)
Xcat &lt;- c(1, 2)
catLabel &lt;- NULL
y &lt;- as.factor(sample(c(0, 1), 100, replace = TRUE))

forest &lt;- ODRF(y ~ X, split = "entropy", Xcat = NULL, parallel = FALSE)

head(X)
#&gt;   Xcol1 Xcol2          X1         X2          X3
#&gt; 1     B     5 -0.04178453  2.3962339 -0.01443979
#&gt; 2     A     4 -1.66084623 -0.4397486  0.57251733
#&gt; 3     B     2 -0.57973333 -0.2878683  1.24475578
#&gt; 4     B     1 -0.82075051  1.3702900  0.01716528
#&gt; 5     C     5 -0.76337897 -0.9620213  0.25846351
#&gt; 6     A     5 -0.37720294 -0.1853976  1.04872159

# one-of-K encode each categorical feature and store in X1
numCat &lt;- apply(X[, Xcat, drop = FALSE], 2, function(x) length(unique(x)))
# initialize training data matrix X1
X1 &lt;- matrix(0, nrow = nrow(X), ncol = sum(numCat))
catLabel &lt;- vector("list", length(Xcat))
names(catLabel) &lt;- colnames(X)[Xcat]
col.idx &lt;- 0L
# convert categorical feature to K dummy variables
for (j in seq_along(Xcat)) {
  catMap &lt;- (col.idx + 1):(col.idx + numCat[j])
  catLabel[[j]] &lt;- levels(as.factor(X[, Xcat[j]]))
  X1[, catMap] &lt;- (matrix(X[, Xcat[j]], nrow(X), numCat[j]) ==
    matrix(catLabel[[j]], nrow(X), numCat[j], byrow = TRUE)) + 0
  col.idx &lt;- col.idx + numCat[j]
}
X &lt;- cbind(X1, X[, -Xcat])
colnames(X) &lt;- c(paste(rep(seq_along(numCat), numCat), unlist(catLabel),
  sep = "."
), "X1", "X2", "X3")

# Print the result after processing of category variables.
head(X)
#&gt;   1.A 1.B 1.C 2.1 2.2 2.3 2.4 2.5          X1         X2          X3
#&gt; 1   0   1   0   0   0   0   0   1 -0.04178453  2.3962339 -0.01443979
#&gt; 2   1   0   0   0   0   0   1   0 -1.66084623 -0.4397486  0.57251733
#&gt; 3   0   1   0   0   1   0   0   0 -0.57973333 -0.2878683  1.24475578
#&gt; 4   0   1   0   1   0   0   0   0 -0.82075051  1.3702900  0.01716528
#&gt; 5   0   0   1   0   0   0   0   1 -0.76337897 -0.9620213  0.25846351
#&gt; 6   1   0   0   0   0   0   0   1 -0.37720294 -0.1853976  1.04872159
catLabel
#&gt; $Xcol1
#&gt; [1] "A" "B" "C"
#&gt;
#&gt; $Xcol2
#&gt; [1] "1" "2" "3" "4" "5"


forest &lt;- ODRF(X, y,
  split = "gini", Xcat = c(1, 2),
  catLabel = catLabel, parallel = FALSE
)


</code></pre>

<hr>
<h2 id='ODT'>Classification and Regression with Oblique Decision Tree</h2><span id='topic+ODT'></span><span id='topic+ODT.formula'></span><span id='topic+ODT.default'></span>

<h3>Description</h3>

<p>Classification and regression using an oblique decision tree (ODT) in which each node is split by a linear combination of predictors. Different methods are provided for selecting the linear combinations, while the splitting values are chosen by one of three criteria.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ODT(X, ...)

## S3 method for class 'formula'
ODT(
  formula,
  data = NULL,
  split = "auto",
  lambda = "log",
  NodeRotateFun = "RotMatPPO",
  FunDir = getwd(),
  paramList = NULL,
  MaxDepth = Inf,
  numNode = Inf,
  MinLeaf = 10,
  Levels = NULL,
  subset = NULL,
  weights = NULL,
  na.action = na.fail,
  catLabel = NULL,
  Xcat = 0,
  Xscale = "Min-max",
  TreeRandRotate = FALSE,
  ...
)

## Default S3 method:
ODT(
  X,
  y,
  split = "auto",
  lambda = "log",
  NodeRotateFun = "RotMatPPO",
  FunDir = getwd(),
  paramList = NULL,
  MaxDepth = Inf,
  numNode = Inf,
  MinLeaf = 10,
  Levels = NULL,
  subset = NULL,
  weights = NULL,
  na.action = na.fail,
  catLabel = NULL,
  Xcat = 0,
  Xscale = "Min-max",
  TreeRandRotate = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ODT_+3A_x">X</code></td>
<td>
<p>An n by d numeric matrix (preferable) or data frame.</p>
</td></tr>
<tr><td><code id="ODT_+3A_...">...</code></td>
<td>
<p>Optional parameters to be passed to the low level function.</p>
</td></tr>
<tr><td><code id="ODT_+3A_formula">formula</code></td>
<td>
<p>Object of class <code>formula</code> with a response describing the model to fit. If this is a data frame, it is taken as the model frame. (see <code><a href="stats.html#topic+model.frame">model.frame</a></code>)</p>
</td></tr>
<tr><td><code id="ODT_+3A_data">data</code></td>
<td>
<p>Training data of class <code>data.frame</code> containing variables named in the formula. If <code>data</code> is missing it is obtained from the current environment by <code>formula</code>.</p>
</td></tr>
<tr><td><code id="ODT_+3A_split">split</code></td>
<td>
<p>The criterion used for splitting the nodes. &quot;entropy&quot;: information gain and &quot;gini&quot;: gini impurity index for classification; &quot;mse&quot;: mean square error for regression;
'auto' (default): If the response in <code>data</code> or <code>y</code> is a factor, &quot;gini&quot; is used, otherwise regression is assumed.</p>
</td></tr>
<tr><td><code id="ODT_+3A_lambda">lambda</code></td>
<td>
<p>The argument of <code>split</code> is used to determine the penalty level of the partition criterion. Three options are provided including, <code>lambda=0</code>: no penalty; <code>lambda=2</code>: AIC penalty; <code>lambda='log'</code> (Default): BIC penalty. In Addition, lambda can be any value from 0 to n (training set size).</p>
</td></tr>
<tr><td><code id="ODT_+3A_noderotatefun">NodeRotateFun</code></td>
<td>
<p>Name of the function of class <code>character</code> that implements a linear combination of predictors in the split node.
including </p>

<ul>
<li><p>&quot;RotMatPPO&quot;: projection pursuit optimization model (<code><a href="#topic+PPO">PPO</a></code>), see <code><a href="#topic+RotMatPPO">RotMatPPO</a></code> (default, model=&quot;PPR&quot;).
</p>
</li>
<li><p>&quot;RotMatRF&quot;: single feature similar to CART, see <code><a href="#topic+RotMatRF">RotMatRF</a></code>.
</p>
</li>
<li><p>&quot;RotMatRand&quot;: random rotation, see <code><a href="#topic+RotMatRand">RotMatRand</a></code>.
</p>
</li>
<li><p>&quot;RotMatMake&quot;: users can define this function, for details see <code><a href="#topic+RotMatMake">RotMatMake</a></code>.
</p>
</li></ul>
</td></tr>
<tr><td><code id="ODT_+3A_fundir">FunDir</code></td>
<td>
<p>The path to the <code>function</code> of the user-defined <code>NodeRotateFun</code> (default current working directory).</p>
</td></tr>
<tr><td><code id="ODT_+3A_paramlist">paramList</code></td>
<td>
<p>List of parameters used by the functions <code>NodeRotateFun</code>. If left unchanged, default values will be used, for details see <code><a href="#topic+defaults">defaults</a></code>.</p>
</td></tr>
<tr><td><code id="ODT_+3A_maxdepth">MaxDepth</code></td>
<td>
<p>The maximum depth of the tree (default <code>Inf</code>).</p>
</td></tr>
<tr><td><code id="ODT_+3A_numnode">numNode</code></td>
<td>
<p>Number of nodes that can be used by the tree (default <code>Inf</code>).</p>
</td></tr>
<tr><td><code id="ODT_+3A_minleaf">MinLeaf</code></td>
<td>
<p>Minimal node size (Default 10).</p>
</td></tr>
<tr><td><code id="ODT_+3A_levels">Levels</code></td>
<td>
<p>The category label of the response variable when <code>split</code> is not equal to 'mse'.</p>
</td></tr>
<tr><td><code id="ODT_+3A_subset">subset</code></td>
<td>
<p>An index vector indicating which rows should be used. (NOTE: If given, this argument must be named.)</p>
</td></tr>
<tr><td><code id="ODT_+3A_weights">weights</code></td>
<td>
<p>Vector of non-negative observational weights; fractional weights are allowed (default NULL).</p>
</td></tr>
<tr><td><code id="ODT_+3A_na.action">na.action</code></td>
<td>
<p>A function to specify the action to be taken if NAs are found. (NOTE: If given, this argument must be named.)</p>
</td></tr>
<tr><td><code id="ODT_+3A_catlabel">catLabel</code></td>
<td>
<p>A category labels of class <code>list</code> in predictors. (default NULL, for details see Examples)</p>
</td></tr>
<tr><td><code id="ODT_+3A_xcat">Xcat</code></td>
<td>
<p>A class <code>vector</code> is used to indicate which predictor is the categorical variable. The default Xcat=0 means that no special treatment is given to category variables.
When Xcat=NULL, the predictor x that satisfies the condition &quot;<code>(length(table(x))&lt;10) &amp; (length(x)&gt;20)</code>&quot; is judged to be a category variable.</p>
</td></tr>
<tr><td><code id="ODT_+3A_xscale">Xscale</code></td>
<td>
<p>Predictor standardization methods. &quot; Min-max&quot; (default), &quot;Quantile&quot;, &quot;No&quot; denote Min-max transformation, Quantile transformation and No transformation respectively.</p>
</td></tr>
<tr><td><code id="ODT_+3A_treerandrotate">TreeRandRotate</code></td>
<td>
<p>If or not to randomly rotate the training data before building the tree (default FALSE, see <code><a href="#topic+RandRot">RandRot</a></code>).</p>
</td></tr>
<tr><td><code id="ODT_+3A_y">y</code></td>
<td>
<p>A response vector of length n.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class ODT containing a list of components::
</p>

<ul>
<li><p><code>call</code>: The original call to ODT.
</p>
</li>
<li><p><code>terms</code>: An object of class <code>c("terms", "formula")</code> (see <code><a href="stats.html#topic+terms.object">terms.object</a></code>) summarizing the formula. Used by various methods, but typically not of direct relevance to users.
</p>
</li>
<li><p><code>split</code>, <code>Levels</code> and <code>NodeRotateFun</code> are important parameters for building the tree.
</p>
</li>
<li><p><code>predicted</code>: the predicted values of the training data.
</p>
</li>
<li><p><code>projections</code>: Projection direction for each split node.
</p>
</li>
<li><p><code>paramList</code>: Parameters in a named list to be used by <code>NodeRotateFun</code>.
</p>
</li>
<li><p><code>data</code>: The list of data related parameters used to build the tree.
</p>
</li>
<li><p><code>tree</code>: The list of tree related parameters used to build the tree.
</p>
</li>
<li><p><code>structure</code>: A set of tree structure data records.
</p>

<ul>
<li><p><code>nodeRotaMat</code>: Record the split variables (first column), split node serial number (second column) and rotation direction (third column) for each node. (The first column and the third column are 0 means leaf nodes)
</p>
</li>
<li><p><code>nodeNumLabel</code>: Record each leaf node's category for classification or predicted value for regression (second column is data size). (Each column is 0 means it is not a leaf node)
</p>
</li>
<li><p><code>nodeCutValue</code>: Record the split point of each node. (0 means leaf nodes)
</p>
</li>
<li><p><code>nodeCutIndex</code>: Record the index values of the partitioning variables selected based on the partition criterion <code>split</code>.
</p>
</li>
<li><p><code>childNode</code>: Record the number of child nodes after each splitting.
</p>
</li>
<li><p><code>nodeDepth</code>: Record the depth of the tree where each node is located.
</p>
</li></ul>

</li></ul>



<h3>Author(s)</h3>

<p>Yu Liu and Yingcun Xia
</p>


<h3>References</h3>

<p>Zhan, H., Liu, Y., &amp; Xia, Y. (2022). Consistency of The Oblique Decision Tree and Its Random Forest. arXiv preprint arXiv:2211.12653.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+online.ODT">online.ODT</a></code> <code><a href="#topic+prune.ODT">prune.ODT</a></code> <code><a href="partykit.html#topic+as.party">as.party</a></code> <code><a href="#topic+predict.ODT">predict.ODT</a></code> <code><a href="#topic+print.ODT">print.ODT</a></code> <code><a href="#topic+plot.ODT">plot.ODT</a></code> <code><a href="#topic+plot_ODT_depth">plot_ODT_depth</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Classification with Oblique Decision Tree.
data(seeds)
set.seed(221212)
train &lt;- sample(1:209, 100)
train_data &lt;- data.frame(seeds[train, ])
test_data &lt;- data.frame(seeds[-train, ])
tree &lt;- ODT(varieties_of_wheat ~ ., train_data, split = "entropy")
pred &lt;- predict(tree, test_data[, -8])
# classification error
(mean(pred != test_data[, 8]))

# Regression with Oblique Decision Tree.
data(body_fat)
set.seed(221212)
train &lt;- sample(1:252, 100)
train_data &lt;- data.frame(body_fat[train, ])
test_data &lt;- data.frame(body_fat[-train, ])
tree &lt;- ODT(Density ~ ., train_data,
  split = "mse",
  NodeRotateFun = "RotMatPPO", paramList = list(model = "Log", dimProj = "Rand")
)
pred &lt;- predict(tree, test_data[, -1])
# estimation error
mean((pred - test_data[, 1])^2)

# Projection analysis of the oblique decision tree.
data(iris)
tree &lt;- ODT(Species ~ ., data = iris, split="gini",
            paramList = list(model = "PPR", numProj = 1))
print(round(tree[["projections"]],3))

### Train ODT on one-of-K encoded categorical data ###
# Note that the category variable must be placed at the beginning of the predictor X
# as in the following example.
set.seed(22)
Xcol1 &lt;- sample(c("A", "B", "C"), 100, replace = TRUE)
Xcol2 &lt;- sample(c("1", "2", "3", "4", "5"), 100, replace = TRUE)
Xcon &lt;- matrix(rnorm(100 * 3), 100, 3)
X &lt;- data.frame(Xcol1, Xcol2, Xcon)
Xcat &lt;- c(1, 2)
catLabel &lt;- NULL
y &lt;- as.factor(sample(c(0, 1), 100, replace = TRUE))
tree &lt;- ODT(X, y, split = "entropy", Xcat = NULL)
head(X)
#&gt;   Xcol1 Xcol2          X1         X2          X3
#&gt; 1     B     5 -0.04178453  2.3962339 -0.01443979
#&gt; 2     A     4 -1.66084623 -0.4397486  0.57251733
#&gt; 3     B     2 -0.57973333 -0.2878683  1.24475578
#&gt; 4     B     1 -0.82075051  1.3702900  0.01716528
#&gt; 5     C     5 -0.76337897 -0.9620213  0.25846351
#&gt; 6     A     5 -0.37720294 -0.1853976  1.04872159

# one-of-K encode each categorical feature and store in X1
numCat &lt;- apply(X[, Xcat, drop = FALSE], 2, function(x) length(unique(x)))
# initialize training data matrix X
X1 &lt;- matrix(0, nrow = nrow(X), ncol = sum(numCat))
catLabel &lt;- vector("list", length(Xcat))
names(catLabel) &lt;- colnames(X)[Xcat]
col.idx &lt;- 0L
# convert categorical feature to K dummy variables
for (j in seq_along(Xcat)) {
  catMap &lt;- (col.idx + 1):(col.idx + numCat[j])
  catLabel[[j]] &lt;- levels(as.factor(X[, Xcat[j]]))
  X1[, catMap] &lt;- (matrix(X[, Xcat[j]], nrow(X), numCat[j]) ==
    matrix(catLabel[[j]], nrow(X), numCat[j], byrow = TRUE)) + 0
  col.idx &lt;- col.idx + numCat[j]
}
X &lt;- cbind(X1, X[, -Xcat])
colnames(X) &lt;- c(paste(rep(seq_along(numCat), numCat), unlist(catLabel),
  sep = "."
), "X1", "X2", "X3")

# Print the result after processing of category variables.
head(X)
#&gt;   1.A 1.B 1.C 2.1 2.2 2.3 2.4 2.5          X1         X2          X3
#&gt; 1   0   1   0   0   0   0   0   1 -0.04178453  2.3962339 -0.01443979
#&gt; 2   1   0   0   0   0   0   1   0 -1.66084623 -0.4397486  0.57251733
#&gt; 3   0   1   0   0   1   0   0   0 -0.57973333 -0.2878683  1.24475578
#&gt; 4   0   1   0   1   0   0   0   0 -0.82075051  1.3702900  0.01716528
#&gt; 5   0   0   1   0   0   0   0   1 -0.76337897 -0.9620213  0.25846351
#&gt; 6   1   0   0   0   0   0   0   1 -0.37720294 -0.1853976  1.04872159
catLabel
#&gt; $Xcol1
#&gt; [1] "A" "B" "C"
#&gt;
#&gt; $Xcol2
#&gt; [1] "1" "2" "3" "4" "5"

tree &lt;- ODT(X, y, split = "gini", Xcat = c(1, 2), catLabel = catLabel,NodeRotateFun = "RotMatRF")

</code></pre>

<hr>
<h2 id='online'>online structure learning for class <code>ODT</code> and <code>ODRF</code>.</h2><span id='topic+online'></span>

<h3>Description</h3>

<p><code><a href="#topic+ODT">ODT</a></code> and <code><a href="#topic+ODRF">ODRF</a></code> are constantly updated by multiple batches of data to optimize the model. <code>online</code> is a S3 method for class <code>ODT</code> and <code>ODRF</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>online(obj, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="online_+3A_obj">obj</code></td>
<td>
<p>an object of class <code>ODT</code> or <code>ODRF</code>.</p>
</td></tr>
<tr><td><code id="online_+3A_...">...</code></td>
<td>
<p>For other parameters related to class <code>obj</code>, see <code>ODT</code> or <code>ODRF</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>object of class <code>ODT</code> or <code>ODRF</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ODT">ODT</a></code> <code><a href="#topic+ODRF">ODRF</a></code> <code><a href="#topic+online.ODT">online.ODT</a></code> <code><a href="#topic+online.ODRF">online.ODRF</a></code>
</p>

<hr>
<h2 id='online.ODRF'>using new training data to update an existing <code>ODRF</code>.</h2><span id='topic+online.ODRF'></span>

<h3>Description</h3>

<p>Update existing <code><a href="#topic+ODRF">ODRF</a></code> using new data to improve the model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ODRF'
online(obj, X, y, weights = NULL, MaxDepth = Inf, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="online.ODRF_+3A_obj">obj</code></td>
<td>
<p>An object of class <code>ODRF</code>.</p>
</td></tr>
<tr><td><code id="online.ODRF_+3A_x">X</code></td>
<td>
<p>An new n by d numeric matrix (preferable) or data frame  used to update the object of class <code>ODRF</code>.</p>
</td></tr>
<tr><td><code id="online.ODRF_+3A_y">y</code></td>
<td>
<p>A new response vector of length n used to update the object of class <code>ODRF</code>.</p>
</td></tr>
<tr><td><code id="online.ODRF_+3A_weights">weights</code></td>
<td>
<p>A vector of non-negative observational weights; fractional weights are allowed (default NULL).</p>
</td></tr>
<tr><td><code id="online.ODRF_+3A_maxdepth">MaxDepth</code></td>
<td>
<p>The maximum depth of the tree (default <code>Inf</code>).</p>
</td></tr>
<tr><td><code id="online.ODRF_+3A_...">...</code></td>
<td>
<p>Optional parameters to be passed to the low level function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The same result as <code>ODRF</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ODRF">ODRF</a></code> <code><a href="#topic+prune.ODRF">prune.ODRF</a></code> <code><a href="#topic+online.ODT">online.ODT</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Classification with Oblique Decision Random Forest
data(seeds)
set.seed(221212)
train &lt;- sample(1:209, 80)
train_data &lt;- data.frame(seeds[train, ])
test_data &lt;- data.frame(seeds[-train, ])
index &lt;- seq(floor(nrow(train_data) / 2))
forest &lt;- ODRF(varieties_of_wheat ~ ., train_data[index, ],
  split = "gini", parallel = FALSE, ntrees = 50
)
online_forest &lt;- online(forest, train_data[-index, -8], train_data[-index, 8])
pred &lt;- predict(online_forest, test_data[, -8])
# classification error
(mean(pred != test_data[, 8]))

# Regression with Oblique Decision Random Forest

data(body_fat)
set.seed(221212)
train &lt;- sample(1:252, 80)
train_data &lt;- data.frame(body_fat[train, ])
test_data &lt;- data.frame(body_fat[-train, ])
index &lt;- seq(floor(nrow(train_data) / 2))
forest &lt;- ODRF(Density ~ ., train_data[index, ],
  split = "mse", parallel = FALSE
)
online_forest &lt;- online(
  forest, train_data[-index, -1],
  train_data[-index, 1]
)
pred &lt;- predict(online_forest, test_data[, -1])
# estimation error
mean((pred - test_data[, 1])^2)


</code></pre>

<hr>
<h2 id='online.ODT'>using new training data to update an existing <code>ODT</code>.</h2><span id='topic+online.ODT'></span>

<h3>Description</h3>

<p>Update existing <code><a href="#topic+ODT">ODT</a></code> using new data to improve the model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ODT'
online(obj, X = NULL, y = NULL, weights = NULL, MaxDepth = Inf, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="online.ODT_+3A_obj">obj</code></td>
<td>
<p>an object of class <code>ODT</code>.</p>
</td></tr>
<tr><td><code id="online.ODT_+3A_x">X</code></td>
<td>
<p>An new n by d numeric matrix (preferable) or data frame  used to update the object of class <code>ODT</code>.</p>
</td></tr>
<tr><td><code id="online.ODT_+3A_y">y</code></td>
<td>
<p>A new response vector of length n used to update the object of class <code>ODT</code>.</p>
</td></tr>
<tr><td><code id="online.ODT_+3A_weights">weights</code></td>
<td>
<p>Vector of non-negative observational weights; fractional weights are allowed (default NULL).</p>
</td></tr>
<tr><td><code id="online.ODT_+3A_maxdepth">MaxDepth</code></td>
<td>
<p>The maximum depth of the tree (default <code>Inf</code>).</p>
</td></tr>
<tr><td><code id="online.ODT_+3A_...">...</code></td>
<td>
<p>optional parameters to be passed to the low level function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The same result as <code>ODT</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ODT">ODT</a></code> <code><a href="#topic+prune.ODT">prune.ODT</a></code> <code><a href="#topic+online.ODRF">online.ODRF</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Classification with Oblique Decision Tree
data(seeds)
set.seed(221212)
train &lt;- sample(1:209, 100)
train_data &lt;- data.frame(seeds[train, ])
test_data &lt;- data.frame(seeds[-train, ])
index &lt;- seq(floor(nrow(train_data) / 2))
tree &lt;- ODT(varieties_of_wheat ~ ., train_data[index, ], split = "gini")
online_tree &lt;- online(tree, train_data[-index, -8], train_data[-index, 8])
pred &lt;- predict(online_tree, test_data[, -8])
# classification error
(mean(pred != test_data[, 8]))

# Regression with Oblique Decision Tree
data(body_fat)
set.seed(221212)
train &lt;- sample(1:252, 100)
train_data &lt;- data.frame(body_fat[train, ])
test_data &lt;- data.frame(body_fat[-train, ])
index &lt;- seq(floor(nrow(train_data) / 2))
tree &lt;- ODT(Density ~ ., train_data[index, ], split = "mse")
online_tree &lt;- online(tree, train_data[-index, -1], train_data[-index, 1])
pred &lt;- predict(online_tree, test_data[, -1])
# estimation error
mean((pred - test_data[, 1])^2)

</code></pre>

<hr>
<h2 id='plot_ODT_depth'>plot oblique decision tree depth</h2><span id='topic+plot_ODT_depth'></span>

<h3>Description</h3>

<p>Draw the error graph of class <code>ODT</code> at different depths.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_ODT_depth(
  formula,
  data = NULL,
  newdata = NULL,
  split = "gini",
  NodeRotateFun = "RotMatPPO",
  paramList = NULL,
  digits = NULL,
  main = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot_ODT_depth_+3A_formula">formula</code></td>
<td>
<p>Object of class <code>formula</code> with a response describing the model to fit. If this is a data frame, it is taken as the model frame. (see <code><a href="stats.html#topic+model.frame">model.frame</a></code>)</p>
</td></tr>
<tr><td><code id="plot_ODT_depth_+3A_data">data</code></td>
<td>
<p>Training data of class <code>data.frame</code> in <code><a href="#topic+ODT">ODT</a></code> used to calculate the OOB error.</p>
</td></tr>
<tr><td><code id="plot_ODT_depth_+3A_newdata">newdata</code></td>
<td>
<p>A data frame or matrix containing new data is used to calculate the test error. If it is missing, then it is replaced by <code>data</code>.</p>
</td></tr>
<tr><td><code id="plot_ODT_depth_+3A_split">split</code></td>
<td>
<p>The criterion used for splitting the variable. 'gini': gini impurity index (classification, default),
'entropy': information gain (classification) or 'mse': mean square error (regression).</p>
</td></tr>
<tr><td><code id="plot_ODT_depth_+3A_noderotatefun">NodeRotateFun</code></td>
<td>
<p>Name of the function of class <code>character</code> that implements a linear combination of predictors in the split node.
including </p>

<ul>
<li><p>&quot;RotMatPPO&quot;: projection pursuit optimization model (<code><a href="#topic+PPO">PPO</a></code>), see <code><a href="#topic+RotMatPPO">RotMatPPO</a></code> (default, model=&quot;PPR&quot;).
</p>
</li>
<li><p>&quot;RotMatRF&quot;: single feature similar to Random Forest, see <code><a href="#topic+RotMatRF">RotMatRF</a></code>.
</p>
</li>
<li><p>&quot;RotMatRand&quot;: random rotation, see <code><a href="#topic+RotMatRand">RotMatRand</a></code>.
</p>
</li>
<li><p>&quot;RotMatMake&quot;: Users can define this function, for details see <code><a href="#topic+RotMatMake">RotMatMake</a></code>.
</p>
</li></ul>
</td></tr>
<tr><td><code id="plot_ODT_depth_+3A_paramlist">paramList</code></td>
<td>
<p>List of parameters used by the functions <code>NodeRotateFun</code>. If left unchanged, default values will be used, for details see <code><a href="#topic+defaults">defaults</a></code>.</p>
</td></tr>
<tr><td><code id="plot_ODT_depth_+3A_digits">digits</code></td>
<td>
<p>Integer indicating the number of decimal places (round) or significant digits (signif) to be used.</p>
</td></tr>
<tr><td><code id="plot_ODT_depth_+3A_main">main</code></td>
<td>
<p>main title</p>
</td></tr>
<tr><td><code id="plot_ODT_depth_+3A_...">...</code></td>
<td>
<p>Arguments to be passed to methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>OOB error and test error of <code>newdata</code>, misclassification rate (MR) for classification or mean square error (MSE) for regression.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ODT">ODT</a></code> <code><a href="#topic+plot.ODT">plot.ODT</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(body_fat)
set.seed(221212)
train &lt;- sample(1:252, 100)
train_data &lt;- data.frame(body_fat[train, ])
test_data &lt;- data.frame(body_fat[-train, ])
plot_ODT_depth(Density ~ ., train_data, test_data, split = "mse")

</code></pre>

<hr>
<h2 id='plot.Accuracy'>plot method for <code>Accuracy</code> objects</h2><span id='topic+plot.Accuracy'></span>

<h3>Description</h3>

<p>Draw the error graph of class <code>ODRF</code> at different tree sizes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'Accuracy'
plot(x, lty = 1, digits = NULL, main = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.Accuracy_+3A_x">x</code></td>
<td>
<p>Object of class <code><a href="#topic+Accuracy">Accuracy</a></code>.</p>
</td></tr>
<tr><td><code id="plot.Accuracy_+3A_lty">lty</code></td>
<td>
<p>A vector of line types, see <code><a href="graphics.html#topic+par">par</a></code>.</p>
</td></tr>
<tr><td><code id="plot.Accuracy_+3A_digits">digits</code></td>
<td>
<p>Integer indicating the number of decimal places (round) or significant digits (signif) to be used.</p>
</td></tr>
<tr><td><code id="plot.Accuracy_+3A_main">main</code></td>
<td>
<p>main title of the plot.</p>
</td></tr>
<tr><td><code id="plot.Accuracy_+3A_...">...</code></td>
<td>
<p>Arguments to be passed to methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>OOB error and test error, misclassification rate (MR) for classification or mean square error (MSE) for regression.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ODRF">ODRF</a></code> <code><a href="#topic+Accuracy">Accuracy</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(breast_cancer)
set.seed(221212)
train &lt;- sample(1:569, 80)
train_data &lt;- data.frame(breast_cancer[train, -1])
test_data &lt;- data.frame(breast_cancer[-train, -1])

forest &lt;- ODRF(diagnosis ~ ., train_data, split = "gini",
parallel = FALSE, ntrees = 30)
(error &lt;- Accuracy(forest, train_data, test_data))
plot(error)

</code></pre>

<hr>
<h2 id='plot.ODT'>to plot an oblique decision tree</h2><span id='topic+plot.ODT'></span>

<h3>Description</h3>

<p>Draw oblique decision tree with tree structure. It is modified from a function in <code>PPtreeViz</code> library.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ODT'
plot(x, font.size = 17, width.size = 1, xadj = 0, main = NULL, sub = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.ODT_+3A_x">x</code></td>
<td>
<p>An object of class <code><a href="#topic+ODT">ODT</a></code>.</p>
</td></tr>
<tr><td><code id="plot.ODT_+3A_font.size">font.size</code></td>
<td>
<p>Font size of plot</p>
</td></tr>
<tr><td><code id="plot.ODT_+3A_width.size">width.size</code></td>
<td>
<p>Size of eclipse in each node.</p>
</td></tr>
<tr><td><code id="plot.ODT_+3A_xadj">xadj</code></td>
<td>
<p>The size of the left and right movement.</p>
</td></tr>
<tr><td><code id="plot.ODT_+3A_main">main</code></td>
<td>
<p>main title</p>
</td></tr>
<tr><td><code id="plot.ODT_+3A_sub">sub</code></td>
<td>
<p>sub title</p>
</td></tr>
<tr><td><code id="plot.ODT_+3A_...">...</code></td>
<td>
<p>Arguments to be passed to methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Tree Structure.
</p>


<h3>References</h3>

<p>Lee, EK(2017) PPtreeViz: An R Package for Visualizing Projection Pursuit Classification Trees, Journal of Statistical Software.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ODT">ODT</a></code> <code><a href="partykit.html#topic+as.party">as.party</a></code> <code><a href="#topic+plot_ODT_depth">plot_ODT_depth</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
tree &lt;- ODT(Species ~ ., data = iris, split = "gini")
plot(tree)

</code></pre>

<hr>
<h2 id='plot.prune.ODT'>to plot pruned oblique decision tree</h2><span id='topic+plot.prune.ODT'></span>

<h3>Description</h3>

<p>Plot the error graph of the pruned oblique decision tree at different split nodes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'prune.ODT'
plot(x, position = "topleft", digits = NULL, main = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.prune.ODT_+3A_x">x</code></td>
<td>
<p>An object of class <code><a href="#topic+prune.ODT">prune.ODT</a></code>.</p>
</td></tr>
<tr><td><code id="plot.prune.ODT_+3A_position">position</code></td>
<td>
<p>Position of the curve label, including &quot;topleft&quot; (default), &quot;bottomright&quot;, &quot;bottom&quot;, &quot;bottomleft&quot;, &quot;left&quot;, &quot;top&quot;, &quot;topright&quot;, &quot;right&quot; and &quot;center&quot;.</p>
</td></tr>
<tr><td><code id="plot.prune.ODT_+3A_digits">digits</code></td>
<td>
<p>Integer indicating the number of decimal places (round) or significant digits (signif) to be used.</p>
</td></tr>
<tr><td><code id="plot.prune.ODT_+3A_main">main</code></td>
<td>
<p>main title</p>
</td></tr>
<tr><td><code id="plot.prune.ODT_+3A_...">...</code></td>
<td>
<p>Arguments to be passed to methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The leftmost value of the horizontal axis indicates the tree without pruning, while the rightmost value indicates the data without splitting and using the average value as the predicted value.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ODT">ODT</a></code> <code><a href="#topic+prune.ODT">prune.ODT</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(body_fat)
set.seed(221212)
train &lt;- sample(1:252, 100)
train_data &lt;- data.frame(body_fat[train, ])
test_data &lt;- data.frame(body_fat[-train, ])

tree &lt;- ODT(Density ~ ., train_data, split = "mse")
prune_tree &lt;- prune(tree, test_data[, -1], test_data[, 1])
# Plot pruned oblique decision tree structure (default)
plot(prune_tree)
# Plot the error graph of the pruned oblique decision tree.
class(prune_tree) &lt;- "prune.ODT"
plot(prune_tree)

</code></pre>

<hr>
<h2 id='plot.VarImp'>Variable Importance Plot</h2><span id='topic+plot.VarImp'></span>

<h3>Description</h3>

<p>Dotchart of variable importance as measured by an Oblique Decision Random Forest.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'VarImp'
plot(x, nvar = min(30, nrow(x$varImp)), digits = NULL, main = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.VarImp_+3A_x">x</code></td>
<td>
<p>An object of class <code><a href="#topic+VarImp">VarImp</a></code>.</p>
</td></tr>
<tr><td><code id="plot.VarImp_+3A_nvar">nvar</code></td>
<td>
<p>number of variables to show.</p>
</td></tr>
<tr><td><code id="plot.VarImp_+3A_digits">digits</code></td>
<td>
<p>Integer indicating the number of decimal places (round) or significant digits (signif) to be used.</p>
</td></tr>
<tr><td><code id="plot.VarImp_+3A_main">main</code></td>
<td>
<p>plot title.</p>
</td></tr>
<tr><td><code id="plot.VarImp_+3A_...">...</code></td>
<td>
<p>Arguments to be passed to methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The horizontal axis is the increased error of ODRF after replacing the variable, the larger the increased error the more important the variable is.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ODRF">ODRF</a></code> <code><a href="#topic+VarImp">VarImp</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(breast_cancer)
set.seed(221212)
train &lt;- sample(1:569, 200)
train_data &lt;- data.frame(breast_cancer[train, -1])
forest &lt;- ODRF(train_data[, -1], train_data[, 1], split = "gini",
  parallel = FALSE)
varimp &lt;- VarImp(forest, train_data[, -1], train_data[, 1])
plot(varimp)
</code></pre>

<hr>
<h2 id='PPO'>Projection Pursuit Optimization</h2><span id='topic+PPO'></span>

<h3>Description</h3>

<p>Find the optimal projection using various projectin pursuit models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PPO(X, y, model = "PPR", split = "gini", weights = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="PPO_+3A_x">X</code></td>
<td>
<p>An n by d numeric matrix (preferable) or data frame.</p>
</td></tr>
<tr><td><code id="PPO_+3A_y">y</code></td>
<td>
<p>A response vector of length n.</p>
</td></tr>
<tr><td><code id="PPO_+3A_model">model</code></td>
<td>
<p>Model for projection pursuit.
</p>

<ul>
<li><p>&quot;PPR&quot;(default): projection projection regression from <code><a href="stats.html#topic+ppr">ppr</a></code>. When y is a category label, it is
expanded to K binary features.
</p>
</li>
<li><p>&quot;Log&quot;: logistic based on <code><a href="nnet.html#topic+nnet">nnet</a></code>.
</p>
</li>
<li><p>&quot;Rand&quot;: The random projection generated from <code class="reqn">\{-1, 1\}</code>.
The following models can only be used for classification, i.e. the <code>split</code> must be &rdquo;entropy&rdquo; or 'gini'.
</p>
</li>
<li><p>&quot;LDA&quot;, &quot;PDA&quot;, &quot;Lr&quot;, &quot;GINI&quot;, and &quot;ENTROPY&quot; from library <code>PPtreeViz</code>.
</p>
</li>
<li><p>The following models based on <code><a href="Pursuit.html#topic+Pursuit">Pursuit</a></code>.
</p>

<ul>
<li><p>&quot;holes&quot;: Holes index
</p>
</li>
<li><p>&quot;cm&quot;: Central Mass index
</p>
</li>
<li><p>&quot;holes&quot;: Holes index
</p>
</li>
<li><p>&quot;friedmantukey&quot;: Friedman Tukey index
</p>
</li>
<li><p>&quot;legendre&quot;: Legendre index
</p>
</li>
<li><p>&quot;laguerrefourier&quot;: Laguerre Fourier index
</p>
</li>
<li><p>&quot;hermite&quot;: Hermite index
</p>
</li>
<li><p>&quot;naturalhermite&quot;: Natural Hermite index
</p>
</li>
<li><p>&quot;kurtosismax&quot;: Maximum kurtosis index
</p>
</li>
<li><p>&quot;kurtosismin&quot;: Minimum kurtosis index
</p>
</li>
<li><p>&quot;moment&quot;: Moment index
</p>
</li>
<li><p>&quot;mf&quot;: MF index
</p>
</li>
<li><p>&quot;chi&quot;: Chi-square index
</p>
</li></ul>

</li></ul>
</td></tr>
<tr><td><code id="PPO_+3A_split">split</code></td>
<td>
<p>The criterion used for splitting the variable. 'gini': gini impurity index (classification, default),
'entropy': information gain (classification) or 'mse': mean square error (regression).</p>
</td></tr>
<tr><td><code id="PPO_+3A_weights">weights</code></td>
<td>
<p>Vector of non-negative observational weights; fractional weights are allowed (default NULL).</p>
</td></tr>
<tr><td><code id="PPO_+3A_...">...</code></td>
<td>
<p>optional parameters to be passed to the low level function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Optimal projection direction.
</p>


<h3>References</h3>

<p>Friedman, J. H., &amp; Stuetzle, W. (1981). Projection pursuit regression. Journal of the American statistical Association, 76(376), 817-823.
</p>
<p>Ripley, B. D. (1996) Pattern Recognition and Neural Networks. Cambridge.
</p>
<p>Lee, YD, Cook, D., Park JW, and Lee, EK(2013) PPtree: Projection Pursuit Classification Tree, Electronic Journal of Statistics, 7:1369-1386.
</p>
<p>Cook, D., Buja, A., Lee, E. K., &amp; Wickham, H. (2008). Grand tours, projection pursuit guided tours, and manual controls. In Handbook of data visualization (pp. 295-314). Springer, Berlin, Heidelberg.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+RotMatPPO">RotMatPPO</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># classification
data(seeds)
(PP &lt;- PPO(seeds[, 1:7], seeds[, 8], model = "Log", split = "entropy"))
(PP &lt;- PPO(seeds[, 1:7], seeds[, 8], model = "PPR", split = "entropy"))
(PP &lt;- PPO(seeds[, 1:7], seeds[, 8], model = "LDA", split = "entropy"))

# regression
data(body_fat)
(PP &lt;- PPO(body_fat[, 2:15], body_fat[, 1], model = "Log", split = "mse"))
(PP &lt;- PPO(body_fat[, 2:15], body_fat[, 1], model = "Rand", split = "mse"))
(PP &lt;- PPO(body_fat[, 2:15], body_fat[, 1], model = "PPR", split = "mse"))

</code></pre>

<hr>
<h2 id='predict.ODRF'>predict based on an ODRF object</h2><span id='topic+predict.ODRF'></span>

<h3>Description</h3>

<p>Prediction of ODRF for an input matrix or data frame.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ODRF'
predict(object, Xnew, type = "response", weight.tree = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.ODRF_+3A_object">object</code></td>
<td>
<p>An object of class ODRF, the same created by the function <code><a href="#topic+ODRF">ODRF</a></code>.</p>
</td></tr>
<tr><td><code id="predict.ODRF_+3A_xnew">Xnew</code></td>
<td>
<p>An n by d numeric matrix (preferable) or data frame. The rows correspond to observations and columns correspond to features.
Note that if there are NA values in the data 'Xnew', which will be replaced with the average value.</p>
</td></tr>
<tr><td><code id="predict.ODRF_+3A_type">type</code></td>
<td>
<p>One of <code>response</code>, <code>prob</code> or <code>tree</code>, indicating the type of output: predicted values, matrix of class probabilities or predicted value for each tree.</p>
</td></tr>
<tr><td><code id="predict.ODRF_+3A_weight.tree">weight.tree</code></td>
<td>
<p>Whether to weight the tree, if <code>TRUE</code> then use the out-of-bag error of the tree as the weight. (default <code>FALSE</code>)</p>
</td></tr>
<tr><td><code id="predict.ODRF_+3A_...">...</code></td>
<td>
<p>Arguments to be passed to methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A set of vectors in the following list:
</p>

<ul>
<li> <p><code>response</code>: the predicted values of the new data.
</p>
</li>
<li> <p><code>prob</code>: matrix of class probabilities (one column for each class and one row for each input). If <code>object$split</code> is <code>mse</code>, a vector of tree weights is returned.
</p>
</li>
<li> <p><code>tree</code>: It is a matrix where each column is a prediction for each tree.
</p>
</li></ul>



<h3>References</h3>

<p>Zhan, H., Liu, Y., &amp; Xia, Y. (2022). Consistency of The Oblique Decision Tree and Its Random Forest. arXiv preprint arXiv:2211.12653.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ODRF">ODRF</a></code> <code><a href="#topic+predict.ODT">predict.ODT</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Classification with Oblique Decision Random Forest
data(seeds)
set.seed(221212)
train &lt;- sample(1:209, 80)
train_data &lt;- data.frame(seeds[train, ])
test_data &lt;- data.frame(seeds[-train, ])
forest &lt;- ODRF(varieties_of_wheat ~ ., train_data,
  split = "entropy", parallel = FALSE,ntrees = 50
)
pred &lt;- predict(forest, test_data[, -8], weight.tree = TRUE)
# classification error
(mean(pred != test_data[, 8]))

# Regression with Oblique Decision Random Forest

data(body_fat)
set.seed(221212)
train &lt;- sample(1:252, 80)
train_data &lt;- data.frame(body_fat[train, ])
test_data &lt;- data.frame(body_fat[-train, ])
forest &lt;- ODRF(Density ~ ., train_data, split = "mse", parallel = FALSE,
ntrees = 50, TreeRandRotate=TRUE)
pred &lt;- predict(forest, test_data[, -1])
# estimation error
mean((pred - test_data[, 1])^2)


</code></pre>

<hr>
<h2 id='predict.ODT'>making predict based on ODT objects</h2><span id='topic+predict.ODT'></span>

<h3>Description</h3>

<p>Prediction of ODT for an input matrix or data frame.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ODT'
predict(object, Xnew, leafnode = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.ODT_+3A_object">object</code></td>
<td>
<p>An object of class ODT, the same as that created by the function <code><a href="#topic+ODT">ODT</a></code>.</p>
</td></tr>
<tr><td><code id="predict.ODT_+3A_xnew">Xnew</code></td>
<td>
<p>An n by d numeric matrix (preferable) or data frame. The rows correspond to observations and columns correspond to features.
Note that if there are NA values in the data 'Xnew', which will be replaced with the average value.</p>
</td></tr>
<tr><td><code id="predict.ODT_+3A_leafnode">leafnode</code></td>
<td>
<p>If or not output the leaf node sequence number that <code>Xnew</code> is partitioned. (default FALSE)</p>
</td></tr>
<tr><td><code id="predict.ODT_+3A_...">...</code></td>
<td>
<p>Arguments to be passed to methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of the following:
</p>

<ul>
<li><p> prediction: the prediced response of the new data.
</p>
</li>
<li><p> leafnode: the leaf node sequence number that the new data is partitioned.
</p>
</li></ul>



<h3>References</h3>

<p>Zhan, H., Liu, Y., &amp; Xia, Y. (2022). Consistency of The Oblique Decision Tree and Its Random Forest. arXiv preprint arXiv:2211.12653.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ODT">ODT</a></code> <code><a href="#topic+predict.ODRF">predict.ODRF</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Classification with Oblique Decision Tree.
data(seeds)
set.seed(221212)
train &lt;- sample(1:209, 100)
train_data &lt;- data.frame(seeds[train, ])
test_data &lt;- data.frame(seeds[-train, ])

tree &lt;- ODT(varieties_of_wheat ~ ., train_data, split = "entropy")
pred &lt;- predict(tree, test_data[, -8])
# classification error
(mean(pred != test_data[, 8]))

# Regression with Oblique Decision Tree.
data(body_fat)
set.seed(221212)
train &lt;- sample(1:252, 100)
train_data &lt;- data.frame(body_fat[train, ])
test_data &lt;- data.frame(body_fat[-train, ])

tree &lt;- ODT(Density ~ ., train_data, split = "mse")
pred &lt;- predict(tree, test_data[, -1])
# estimation error
mean((pred - test_data[, 1])^2)

</code></pre>

<hr>
<h2 id='print.ODRF'>print ODRF</h2><span id='topic+print.ODRF'></span>

<h3>Description</h3>

<p>Print contents of ODRF object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ODRF'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.ODRF_+3A_x">x</code></td>
<td>
<p>An object of class <code><a href="#topic+ODRF">ODRF</a></code>.</p>
</td></tr>
<tr><td><code id="print.ODRF_+3A_...">...</code></td>
<td>
<p>Arguments to be passed to methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>OOB error, misclassification rate (MR) for classification or mean square error (MSE) for regression.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ODRF">ODRF</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
forest &lt;- ODRF(Species ~ ., data = iris, parallel = FALSE, ntrees = 50)
forest

</code></pre>

<hr>
<h2 id='print.ODT'>print ODT result</h2><span id='topic+print.ODT'></span>

<h3>Description</h3>

<p>Print the oblique decision tree structure.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ODT'
print(x, projection = FALSE, cutvalue = FALSE, verbose = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.ODT_+3A_x">x</code></td>
<td>
<p>An object of class <code><a href="#topic+ODT">ODT</a></code>.</p>
</td></tr>
<tr><td><code id="print.ODT_+3A_projection">projection</code></td>
<td>
<p>Print projection coefficients in each node if TRUE.</p>
</td></tr>
<tr><td><code id="print.ODT_+3A_cutvalue">cutvalue</code></td>
<td>
<p>Print cutoff values in each node if TRUE.</p>
</td></tr>
<tr><td><code id="print.ODT_+3A_verbose">verbose</code></td>
<td>
<p>Print if TRUE, no output if FALSE.</p>
</td></tr>
<tr><td><code id="print.ODT_+3A_...">...</code></td>
<td>
<p>Arguments to be passed to methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The oblique decision tree structure.
</p>


<h3>References</h3>

<p>Lee, EK(2017)
PPtreeViz: An R Package for Visualizing Projection Pursuit Classification
Trees, Journal of Statistical Software.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ODT">ODT</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
tree &lt;- ODT(Species ~ ., data = iris)
tree
print(tree, projection = TRUE, cutvalue = TRUE)

</code></pre>

<hr>
<h2 id='prune'>prune <code>ODT</code> or <code>ODRF</code></h2><span id='topic+prune'></span>

<h3>Description</h3>

<p>Prune <code>ODT</code> or <code>ODRF</code> from bottom to top with validation data based on prediction error, and <code>prune</code> is a S3 method for class <code>ODT</code> and <code>ODRF</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prune(obj, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="prune_+3A_obj">obj</code></td>
<td>
<p>An object of class <code>ODT</code> or <code>ODRF</code>.</p>
</td></tr>
<tr><td><code id="prune_+3A_...">...</code></td>
<td>
<p>For other parameters related to class <code>obj</code>, see <code><a href="#topic+ODT">ODT</a></code> or <code><a href="#topic+ODRF">ODRF</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>ODT</code> and <code>prune.ODT</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ODT">ODT</a></code> <code><a href="#topic+ODRF">ODRF</a></code> <code><a href="#topic+prune.ODT">prune.ODT</a></code> <code><a href="#topic+prune.ODRF">prune.ODRF</a></code>
</p>

<hr>
<h2 id='prune.ODRF'>Pruning of class <code>ODRF</code>.</h2><span id='topic+prune.ODRF'></span>

<h3>Description</h3>

<p>Prune <code>ODRF</code> from bottom to top with test data based on prediction error.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ODRF'
prune(obj, X, y, MaxDepth = 1, useOOB = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="prune.ODRF_+3A_obj">obj</code></td>
<td>
<p>An object of class <code><a href="#topic+ODRF">ODRF</a></code>.</p>
</td></tr>
<tr><td><code id="prune.ODRF_+3A_x">X</code></td>
<td>
<p>An n by d numeric matrix (preferable) or data frame is used to prune the object of class <code>ODRF</code>.</p>
</td></tr>
<tr><td><code id="prune.ODRF_+3A_y">y</code></td>
<td>
<p>A response vector of length n.</p>
</td></tr>
<tr><td><code id="prune.ODRF_+3A_maxdepth">MaxDepth</code></td>
<td>
<p>The maximum depth of the tree after pruning (Default 1).</p>
</td></tr>
<tr><td><code id="prune.ODRF_+3A_useoob">useOOB</code></td>
<td>
<p>Whether to use OOB for pruning (Default TRUE). Note that when <code>useOOB=TRUE</code>, <code>X</code> and <code>y</code> must be the training data in <code><a href="#topic+ODRF">ODRF</a></code>.</p>
</td></tr>
<tr><td><code id="prune.ODRF_+3A_...">...</code></td>
<td>
<p>Optional parameters to be passed to the low level function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>ODRF</code> and <code>prune.ODRF</code>.
</p>

<ul>
<li><p><code>ppForest</code> The same result as <code>ODRF</code>.
</p>
</li>
<li><p><code>pruneError</code> Error of test data or OOB after each pruning in each tree, misclassification rate (MR) for classification or mean square error (MSE) for regression.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+ODRF">ODRF</a></code> <code><a href="#topic+online.ODRF">online.ODRF</a></code> <code><a href="#topic+prune.ODT">prune.ODT</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Classification with Oblique Decision Random Forest
data(seeds)
set.seed(221212)
train &lt;- sample(1:209, 80)
train_data &lt;- data.frame(seeds[train, ])
test_data &lt;- data.frame(seeds[-train, ])
forest &lt;- ODRF(varieties_of_wheat ~ ., train_data,
  split = "entropy", parallel = FALSE, ntrees = 50
)
prune_forest &lt;- prune(forest, train_data[, -8], train_data[, 8])
pred &lt;- predict(prune_forest, test_data[, -8])
# classification error
(mean(pred != test_data[, 8]))

# Regression with Oblique Decision Random Forest
data(body_fat)
set.seed(221212)
train &lt;- sample(1:252,80)
train_data &lt;- data.frame(body_fat[train, ])
test_data &lt;- data.frame(body_fat[-train, ])
index &lt;- seq(floor(nrow(train_data) / 2))
forest &lt;- ODRF(Density ~ ., train_data[index, ], split = "mse", parallel = FALSE, ntrees = 50)
prune_forest &lt;- prune(forest, train_data[-index, -1], train_data[-index, 1], useOOB = FALSE)
pred &lt;- predict(prune_forest, test_data[, -1])
# estimation error
mean((pred - test_data[, 1])^2)

</code></pre>

<hr>
<h2 id='prune.ODT'>pruning of class <code>ODT</code></h2><span id='topic+prune.ODT'></span>

<h3>Description</h3>

<p>Prune <code>ODT</code> from bottom to top with validation data based on prediction error.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ODT'
prune(obj, X, y, MaxDepth = 1, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="prune.ODT_+3A_obj">obj</code></td>
<td>
<p>an object of class <code>ODT</code>.</p>
</td></tr>
<tr><td><code id="prune.ODT_+3A_x">X</code></td>
<td>
<p>An n by d numeric matrix (preferable) or data frame is used to prune the object of class <code>ODT</code>.</p>
</td></tr>
<tr><td><code id="prune.ODT_+3A_y">y</code></td>
<td>
<p>A response vector of length n.</p>
</td></tr>
<tr><td><code id="prune.ODT_+3A_maxdepth">MaxDepth</code></td>
<td>
<p>The maximum depth of the tree after pruning. (Default 1)</p>
</td></tr>
<tr><td><code id="prune.ODT_+3A_...">...</code></td>
<td>
<p>Optional parameters to be passed to the low level function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The leftmost value of the horizontal axis indicates the tree without pruning, while the rightmost value indicates the data without splitting and using the average value as the predicted value.
</p>


<h3>Value</h3>

<p>An object of class <code>ODT</code> and <code>prune.ODT</code>.
</p>

<ul>
<li><p><code>ODT</code> The same result as <code>ODT</code>.
</p>
</li>
<li><p><code>pruneError</code> Error of validation data after each pruning, misclassification rate (MR) for classification or mean square error (MSE) for regression.
The maximum value indicates the tree without pruning, and the minimum value (0) indicates indicates the data without splitting and using the average value as the predicted value.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+ODT">ODT</a></code> <code><a href="#topic+plot.prune.ODT">plot.prune.ODT</a></code> <code><a href="#topic+prune.ODRF">prune.ODRF</a></code> <code><a href="#topic+online.ODT">online.ODT</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Classification with Oblique Decision Tree
data(seeds)
set.seed(221212)
train &lt;- sample(1:209, 100)
train_data &lt;- data.frame(seeds[train, ])
test_data &lt;- data.frame(seeds[-train, ])
index &lt;- seq(floor(nrow(train_data) / 2))
tree &lt;- ODT(varieties_of_wheat ~ ., train_data[index, ], split = "entropy")
prune_tree &lt;- prune(tree, train_data[-index, -8], train_data[-index, 8])
pred &lt;- predict(prune_tree, test_data[, -8])
# classification error
(mean(pred != test_data[, 8]))

# Regression with Oblique Decision Tree
data(body_fat)
set.seed(221212)
train &lt;- sample(1:252, 100)
train_data &lt;- data.frame(body_fat[train, ])
test_data &lt;- data.frame(body_fat[-train, ])
index &lt;- seq(floor(nrow(train_data) / 2))
tree &lt;- ODT(Density ~ ., train_data[index, ], split = "mse")
prune_tree &lt;- prune(tree, train_data[-index, -1], train_data[-index, 1])
pred &lt;- predict(prune_tree, test_data[, -1])
# estimation error
mean((pred - test_data[, 1])^2)

</code></pre>

<hr>
<h2 id='RandRot'>Samples a p x p uniformly random rotation matrix</h2><span id='topic+RandRot'></span>

<h3>Description</h3>

<p>Samples a p x p uniformly random rotation matrix via QR decomposition
of a matrix with elements sampled iid from a standard normal distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>RandRot(p)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="RandRot_+3A_p">p</code></td>
<td>
<p>The columns of an n by p numeric matrix or data frame.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A p x p uniformly random rotation matrix.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+RotMatPPO">RotMatPPO</a></code> <code><a href="#topic+RotMatRand">RotMatRand</a></code> <code><a href="#topic+RotMatRF">RotMatRF</a></code> <code><a href="#topic+RotMatMake">RotMatMake</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(220828)
(RandRot(10))

</code></pre>

<hr>
<h2 id='RotMatMake'>Create rotation matrix used to determine the linear combination of features.</h2><span id='topic+RotMatMake'></span>

<h3>Description</h3>

<p>Create any projection matrix with a self-defined projection matrix function and projection optimization model function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>RotMatMake(
  X = NULL,
  y = NULL,
  RotMatFun = "RotMatPPO",
  PPFun = "PPO",
  FunDir = getwd(),
  paramList = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="RotMatMake_+3A_x">X</code></td>
<td>
<p>An n by d numeric matrix (preferable) or data frame.</p>
</td></tr>
<tr><td><code id="RotMatMake_+3A_y">y</code></td>
<td>
<p>A response vector of length n.</p>
</td></tr>
<tr><td><code id="RotMatMake_+3A_rotmatfun">RotMatFun</code></td>
<td>
<p>A self-defined projection matrix function name, which can also be <code><a href="#topic+RotMatRand">RotMatRand</a></code> and <code><a href="#topic+RotMatPPO">RotMatPPO</a></code>. Note that <code>(,...)</code> is necessary.</p>
</td></tr>
<tr><td><code id="RotMatMake_+3A_ppfun">PPFun</code></td>
<td>
<p>A self-defined projection function name, which can also be <code><a href="#topic+PPO">PPO</a></code>. Note that <code>(,...)</code> is necessary.</p>
</td></tr>
<tr><td><code id="RotMatMake_+3A_fundir">FunDir</code></td>
<td>
<p>The path to the <code>function</code> of the user-defined <code>NodeRotateFun</code> (default current Workspace).</p>
</td></tr>
<tr><td><code id="RotMatMake_+3A_paramlist">paramList</code></td>
<td>
<p>List of parameters used by the functions <code>RotMatFun</code> and <code>PPFun</code>. If left unchanged, default values will be used, for details see <code><a href="#topic+defaults">defaults</a></code>.</p>
</td></tr>
<tr><td><code id="RotMatMake_+3A_...">...</code></td>
<td>
<p>Used to handle superfluous arguments passed in using paramList.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>There are two ways for the user to define a projection direction function. The first way is to connect two custom functions with the function <code>RotMatMake()</code>.
Specifically, <code>RotMatFun()</code> is defined to determine the variables to be projected, the projection dimensions and the number of projections (the first two columns of the rotation matrix).
<code>PPFun()</code> is defined to determine the projection coefficients (the third column of the rotation matrix). After that let the argument <code>RotMatFun="RotMatMake"</code>,
and the argument <code>paramList</code> must contain the parameters <code>RotMatFun</code> and <code>PPFun</code>. The second way is to define a function directly,
and just let the argument <code>RotMatFun</code> be the name of the defined function and let the argument <code>paramList</code> be the arguments list used in the defined function.
</p>


<h3>Value</h3>

<p>A random matrix to use in running <code><a href="#topic+ODT">ODT</a></code>.
</p>

<ul>
<li><p>Variable: Variables to be projected.
</p>
</li>
<li><p>Number: Number of projections.
</p>
</li>
<li><p>Coefficient: Coefficients of the projection matrix.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+RotMatPPO">RotMatPPO</a></code> <code><a href="#topic+RotMatRand">RotMatRand</a></code> <code><a href="#topic+RotMatRF">RotMatRF</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(220828)
X &lt;- matrix(rnorm(1000), 100, 10)
y &lt;- (rnorm(100) &gt; 0) + 0
(RotMat &lt;- RotMatMake(X, y, "RotMatRand", "PPO"))
library(nnet)
(RotMat &lt;- RotMatMake(X, y, "RotMatPPO", "PPO", paramList = list(model = "Log")))

## Define projection matrix function makeRotMat and projection pursuit function makePP.##
##  Note that '...' is necessary.
makeRotMat &lt;- function(dimX, dimProj, numProj, ...) {
  RotMat &lt;- matrix(1, dimProj * numProj, 3)
  for (np in seq(numProj)) {
    RotMat[(dimProj * (np - 1) + 1):(dimProj * np), 1] &lt;-
      sample(1:dimX, dimProj, replace = FALSE)
    RotMat[(dimProj * (np - 1) + 1):(dimProj * np), 2] &lt;- np
  }
  return(RotMat)
}

makePP &lt;- function(dimProj, prob, ...) {
  pp &lt;- sample(c(1L, -1L), dimProj, replace = TRUE, prob = c(prob, 1 - prob))
  return(pp)
}

RotMat &lt;- RotMatMake(
  RotMatFun = "makeRotMat", PPFun = "makePP",
  paramList = list(dimX = 8, dimProj = 5, numProj = 4, prob = 0.5)
)
head(RotMat)
#&gt;      Variable Number Coefficient
#&gt; [1,]        6      1           1
#&gt; [2,]        8      1           1
#&gt; [3,]        1      1          -1
#&gt; [4,]        4      1          -1
#&gt; [5,]        5      1          -1
#&gt; [6,]        6      2           1


# train ODT with defined projection matrix function
tree &lt;- ODT(X, y,
  split = "entropy", NodeRotateFun = "makeRotMat",
  paramList = list(dimX = ncol(X), dimProj = 5, numProj = 4)
)
# train ODT with defined projection matrix function and projection optimization model function
tree &lt;- ODT(X, y,
  split = "entropy", NodeRotateFun = "RotMatMake", paramList =
    list(
      RotMatFun = "makeRotMat", PPFun = "makePP",
      dimX = ncol(X), dimProj = 5, numProj = 4, prob = 0.5
    )
)

</code></pre>

<hr>
<h2 id='RotMatPPO'>Create a Projection Matrix: RotMatPPO</h2><span id='topic+RotMatPPO'></span>

<h3>Description</h3>

<p>Create a projection matrix using projection pursuit optimization (<code><a href="#topic+PPO">PPO</a></code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>RotMatPPO(
  X,
  y,
  model = "PPR",
  split = "entropy",
  weights = NULL,
  dimProj = min(ceiling(length(y)^0.4), ceiling(ncol(X) * 2/3)),
  numProj = ifelse(dimProj == "Rand", sample(floor(ncol(X)/3), 1),
    ceiling(ncol(X)/dimProj)),
  catLabel = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="RotMatPPO_+3A_x">X</code></td>
<td>
<p>An n by d numeric matrix (preferable) or data frame.</p>
</td></tr>
<tr><td><code id="RotMatPPO_+3A_y">y</code></td>
<td>
<p>A response vector of length n.</p>
</td></tr>
<tr><td><code id="RotMatPPO_+3A_model">model</code></td>
<td>
<p>Model for projection pursuit (for details see <code><a href="#topic+PPO">PPO</a></code>).</p>
</td></tr>
<tr><td><code id="RotMatPPO_+3A_split">split</code></td>
<td>
<p>One of three criteria, 'gini': gini impurity index (classification), 'entropy': information gain (classification, default)
or 'mse': mean square error (regression).</p>
</td></tr>
<tr><td><code id="RotMatPPO_+3A_weights">weights</code></td>
<td>
<p>A vector of length same as <code>data</code> that are positive weights. (default NULL)</p>
</td></tr>
<tr><td><code id="RotMatPPO_+3A_dimproj">dimProj</code></td>
<td>
<p>Number of variables to be projected, <code>dimProj</code>=min(ceiling(n^0.4),ceiling(ncol(X)*2/3)) (default) or dimProj=&quot;Rand&quot;: random from 1 to ncol(X).</p>
</td></tr>
<tr><td><code id="RotMatPPO_+3A_numproj">numProj</code></td>
<td>
<p>The number of projection directions, when <code>dimProj</code>=&quot;Rand&quot; default
<code>numProj</code> = sample(ceiling(ncol(<code>X</code>)/3),1) otherwise default <code>numProj</code>=ceiling(ncol(<code>X</code>)/<code>dimProj</code>).</p>
</td></tr>
<tr><td><code id="RotMatPPO_+3A_catlabel">catLabel</code></td>
<td>
<p>A category labels of class <code>list</code> in predictors. (default NULL, for details see Examples of <code><a href="#topic+ODT">ODT</a></code>)</p>
</td></tr>
<tr><td><code id="RotMatPPO_+3A_...">...</code></td>
<td>
<p>Used to handle superfluous arguments passed in using paramList.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A random matrix to use in running <code><a href="#topic+ODT">ODT</a></code>.
</p>

<ul>
<li><p>Variable: Variables to be projected.
</p>
</li>
<li><p>Number: Number of projections.
</p>
</li>
<li><p>Coefficient: Coefficients of the projection matrix.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+RotMatMake">RotMatMake</a></code> <code><a href="#topic+RotMatRand">RotMatRand</a></code> <code><a href="#topic+RotMatRF">RotMatRF</a></code> <code><a href="#topic+PPO">PPO</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(220828)
X &lt;- matrix(rnorm(1000), 100, 10)
y &lt;- (rnorm(100) &gt; 0) + 0
(RotMat &lt;- RotMatPPO(X, y))
(RotMat &lt;- RotMatPPO(X, y, dimProj = "Rand"))
(RotMat &lt;- RotMatPPO(X, y, dimProj = 6, numProj = 4))

# classification
data(seeds)
(PP &lt;- RotMatPPO(seeds[, 1:7], seeds[, 8], model = "Log", split = "entropy"))
(PP &lt;- RotMatPPO(seeds[, 1:7], seeds[, 8], model = "PPR", split = "entropy"))
(PP &lt;- RotMatPPO(seeds[, 1:7], seeds[, 8], model = "LDA", split = "entropy"))

# regression
data(body_fat)
(PP &lt;- RotMatPPO(body_fat[, 2:15], body_fat[, 1], model = "Log", split = "mse"))
(PP &lt;- RotMatPPO(body_fat[, 2:15], body_fat[, 1], model = "Rand", split = "mse"))
(PP &lt;- RotMatPPO(body_fat[, 2:15], body_fat[, 1], model = "PPR", split = "mse"))

</code></pre>

<hr>
<h2 id='RotMatRand'>Random Rotation Matrix</h2><span id='topic+RotMatRand'></span>

<h3>Description</h3>

<p>Generate rotation matrices by different distributions, and it comes from the library <code>rerf</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>RotMatRand(
  dimX,
  randDist = "Binary",
  numProj = ceiling(sqrt(dimX)),
  dimProj = "Rand",
  sparsity = ifelse(dimX &gt;= 10, 3/dimX, 1/dimX),
  prob = 0.5,
  lambda = 1,
  catLabel = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="RotMatRand_+3A_dimx">dimX</code></td>
<td>
<p>The number of dimensions.</p>
</td></tr>
<tr><td><code id="RotMatRand_+3A_randdist">randDist</code></td>
<td>
<p>The probability distribution of the random projection direction, including &quot;Binary&quot;: <code class="reqn">B\{-1,1\}</code> binomial distribution (default),
&quot;Norm&quot;:<code class="reqn">N(0,1)</code> normal distribution, &quot;Uniform&quot;: <code class="reqn">U(-1,1)</code> uniform distribution.</p>
</td></tr>
<tr><td><code id="RotMatRand_+3A_numproj">numProj</code></td>
<td>
<p>The number of projection directions (default ceiling(sqrt(<code>dimX</code>))).</p>
</td></tr>
<tr><td><code id="RotMatRand_+3A_dimproj">dimProj</code></td>
<td>
<p>Number of variables to be projected, default dimProj=&quot;Rand&quot;: random from 1 to <code>dimX</code>.</p>
</td></tr>
<tr><td><code id="RotMatRand_+3A_sparsity">sparsity</code></td>
<td>
<p>A real number in <code class="reqn">(0,1)</code> that specifies the distribution of non-zero elements in the random matrix.
When sparsity=&quot;pois&quot; means that non-zero elements are generated by the p(<code>lambda</code>) Poisson distribution.</p>
</td></tr>
<tr><td><code id="RotMatRand_+3A_prob">prob</code></td>
<td>
<p>A probability in <code class="reqn">(0,1)</code> used for sampling from <code class="reqn">{-1,1}</code> where <code>prob = 0</code> will only sample -1 and <code>prob = 1</code> will only sample 1.</p>
</td></tr>
<tr><td><code id="RotMatRand_+3A_lambda">lambda</code></td>
<td>
<p>Parameter of the Poisson distribution (default 1).</p>
</td></tr>
<tr><td><code id="RotMatRand_+3A_catlabel">catLabel</code></td>
<td>
<p>A category labels of class <code>list</code> in predictors. (default NULL, for details see Examples of <code><a href="#topic+ODT">ODT</a></code>)</p>
</td></tr>
<tr><td><code id="RotMatRand_+3A_...">...</code></td>
<td>
<p>Used to handle superfluous arguments passed in using paramList.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A random matrix to use in running <code><a href="#topic+ODT">ODT</a></code>.
</p>

<ul>
<li><p>Variable: Variables to be projected.
</p>
</li>
<li><p>Number: Number of projections.
</p>
</li>
<li><p>Coefficient: Coefficients of the projection matrix.
</p>
</li></ul>



<h3>References</h3>

<p>Tomita, T. M., Browne, J., Shen, C., Chung, J., Patsolic, J. L., Falk, B., ... &amp; Vogelstein, J. T. (2020). Sparse projection oblique randomer forests. Journal of machine learning research, 21(104).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+RotMatPPO">RotMatPPO</a></code> <code><a href="#topic+RotMatRF">RotMatRF</a></code> <code><a href="#topic+RotMatMake">RotMatMake</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
paramList &lt;- list(dimX = 8, numProj = 3, sparsity = 0.25, prob = 0.5)
(RotMat &lt;- do.call(RotMatRand, paramList))
paramList &lt;- list(dimX = 8, numProj = 3, sparsity = "pois")
(RotMat &lt;- do.call(RotMatRand, paramList))
paramList &lt;- list(dimX = 8, randDist = "Norm", dimProj = 5)
(RotMat &lt;- do.call(RotMatRand, paramList))

</code></pre>

<hr>
<h2 id='RotMatRF'>Create a Projection Matrix: Random Forest (RF)</h2><span id='topic+RotMatRF'></span>

<h3>Description</h3>

<p>Create a projection matrix with coefficient 1 and 0 such that the ODRF (ODT) has the same partition variables as the Random Forest (CART).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>RotMatRF(dimX, numProj, catLabel = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="RotMatRF_+3A_dimx">dimX</code></td>
<td>
<p>The number of dimensions.</p>
</td></tr>
<tr><td><code id="RotMatRF_+3A_numproj">numProj</code></td>
<td>
<p>The number of projection directions (default ceiling(sqrt(<code>dimX</code>))).</p>
</td></tr>
<tr><td><code id="RotMatRF_+3A_catlabel">catLabel</code></td>
<td>
<p>A category labels of class <code>list</code> in predictors. (default NULL, for details see Examples of <code><a href="#topic+ODT">ODT</a></code>)</p>
</td></tr>
<tr><td><code id="RotMatRF_+3A_...">...</code></td>
<td>
<p>Used to handle superfluous arguments passed in using paramList.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A random matrix to use in running <code><a href="#topic+ODT">ODT</a></code>.
</p>

<ul>
<li><p>Variable: Variables to be projected.
</p>
</li>
<li><p>Number: Number of projections.
</p>
</li>
<li><p>Coefficient: Coefficients of the projection matrix.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+RotMatPPO">RotMatPPO</a></code> <code><a href="#topic+RotMatRand">RotMatRand</a></code> <code><a href="#topic+RotMatMake">RotMatMake</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>paramList &lt;- list(dimX = 8, numProj = 3, catLabel = NULL)
set.seed(2)
(RotMat &lt;- do.call(RotMatRF, paramList))

</code></pre>

<hr>
<h2 id='seeds'>seeds Data Set</h2><span id='topic+seeds'></span>

<h3>Description</h3>

<p>Measurements of geometrical properties of kernels belonging to three different varieties of wheat.
A soft X-ray technique and GRAINS package were used to construct all seven, real-valued attributes.
</p>


<h3>Format</h3>

<p>A data frame with 209 rows and 7 covariate variables and 1 response variable.
</p>


<h3>Details</h3>

<p>The variables listed below, from left to right, are:
</p>

<ul>
<li><p> area A
</p>
</li>
<li><p> perimeter P
</p>
</li>
<li><p> compactness C = 4<em>pi</em>A/P^2
</p>
</li>
<li><p> length of kernel
</p>
</li>
<li><p> width of kernel
</p>
</li>
<li><p> asymmetry coefficient
</p>
</li>
<li><p> length of kernel groove
</p>
</li>
<li><p> varieties of wheat (1, 2, 3 for Kama, Rosa and Canadian respectively)
</p>
</li></ul>



<h3>Source</h3>

<p><a href="https://archive.ics.uci.edu/ml/datasets/seeds">https://archive.ics.uci.edu/ml/datasets/seeds</a>
</p>


<h3>References</h3>

<p>M. Charytanowicz, J. Niewczas, P. Kulczycki, P.A. Kowalski, S. Lukasik, S. Zak, 'A Complete Gradient Clustering Algorithm for Features Analysis of X-ray Images', in: Information Technologies in Biomedicine, Ewa Pietka, Jacek Kawa (eds.), Springer-Verlag, Berlin-Heidelberg, 2010, pp. 15-24.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+body_fat">body_fat</a></code> <code><a href="#topic+breast_cancer">breast_cancer</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(seeds)
set.seed(221212)
train &lt;- sample(1:209, 80)
train_data &lt;- data.frame(seeds[train, ])
test_data &lt;- data.frame(seeds[-train, ])

forest &lt;- ODRF(varieties_of_wheat ~ ., train_data,
  split = "gini", parallel = FALSE, ntrees = 50
)
pred &lt;- predict(forest, test_data[, -8])
# classification error
(mean(pred != test_data[, 8]))

tree &lt;- ODT(varieties_of_wheat ~ ., train_data, split = "gini")
pred &lt;- predict(tree, test_data[, -8])
# classification error
(mean(pred != test_data[, 8]))
</code></pre>

<hr>
<h2 id='VarImp'>Extract variable importance measure</h2><span id='topic+VarImp'></span>

<h3>Description</h3>

<p>This is the extractor function for variable importance measures as produced by <code><a href="#topic+ODT">ODT</a></code> and <code><a href="#topic+ODRF">ODRF</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>VarImp(obj, X = NULL, y = NULL, type = "permutation")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="VarImp_+3A_obj">obj</code></td>
<td>
<p>An object of class <code><a href="#topic+ODT">ODT</a></code> and <code><a href="#topic+ODRF">ODRF</a></code>.</p>
</td></tr>
<tr><td><code id="VarImp_+3A_x">X</code></td>
<td>
<p>An n by d numerical matrix (preferably) or data frame is used in the <code>ODRF</code>.</p>
</td></tr>
<tr><td><code id="VarImp_+3A_y">y</code></td>
<td>
<p>A response vector of length n is used in the <code>ODRF</code>.</p>
</td></tr>
<tr><td><code id="VarImp_+3A_type">type</code></td>
<td>
<p>specifying the type of importance measure. &quot;impurity&quot;: mean decrease in node impurity, &quot;permutation&quot; (default): mean decrease in accuracy.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A note from <code>randomForest</code> package, here are the definitions of the variable importance measures.
</p>

<ul>
<li><p> The first measure is the total decrease in node impurities from splitting on the variable, averaged over all trees. For classification, the node impurity is measured by the Gini index. For regression, it is measured by residual sum of squares.
</p>
</li>
<li><p> The second measure is computed from permuting OOB data: For each tree, the prediction error on the out-of-bag portion of the data is recorded.
Then the same is done after permuting each predictor variable. The difference between the two are then averaged over all trees.
</p>
</li></ul>



<h3>Value</h3>

<p>A matrix of importance measure, first column is the predictors and second column is Increased error. Misclassification rate (MR) for classification or mean square error (MSE) for regression.
The larger the increased error the more important the variable is.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ODRF">ODRF</a></code> <code><a href="#topic+Accuracy">Accuracy</a></code> <code><a href="#topic+plot.VarImp">plot.VarImp</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(body_fat)
y=body_fat[,1]
X=body_fat[,-1]

tree &lt;- ODT(X, y, split = "mse")
(varimp &lt;- VarImp(tree, type="impurity"))

forest &lt;- ODRF(X, y, split = "mse", parallel = FALSE, ntrees=50)
(varimp &lt;- VarImp(forest, type="impurity"))
(varimp &lt;- VarImp(forest, X, y, type="permutation"))

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
