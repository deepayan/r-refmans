<!DOCTYPE html><html lang="en"><head><title>Help for package FADA</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {FADA}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#FADA-package'>
<p>Variable selection for supervised classification in high dimension</p></a></li>
<li><a href='#data.test'>
<p>Test dataset simulated with the same distribution as the training dataset data.train.</p></a></li>
<li><a href='#data.train'>
<p>Training data</p></a></li>
<li><a href='#decorrelate.test'><p>Factor Adjusted Discriminant Analysis 2: Decorrelation of a testing data set after running the <code>decorrelate.train</code> function on a training data set</p></a></li>
<li><a href='#decorrelate.train'><p>Factor Adjusted Discriminant Analysis 1: Decorrelation of the training data</p></a></li>
<li><a href='#FADA'><p>Factor Adjusted Discriminant Analysis 3-4 : Supervised classification on decorrelated data</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Variable Selection for Supervised Classification in High
Dimension</td>
</tr>
<tr>
<td>Version:</td>
<td>1.3.5</td>
</tr>
<tr>
<td>Date:</td>
<td>2019-12-10</td>
</tr>
<tr>
<td>Author:</td>
<td>Emeline Perthame (Institut Pasteur, Paris, France), Chloe Friguet
    (Universite de Bretagne Sud, Vannes, France) and David Causeur (Agrocampus
    Ouest, Rennes, France)</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>David Causeur &lt;david.causeur@agrocampus-ouest.fr&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>The functions provided in the FADA (Factor Adjusted Discriminant Analysis) package aim at performing supervised classification of high-dimensional and correlated profiles. The procedure combines a decorrelation step based on a  
   factor modeling of the dependence among covariates and a classification method. The available methods are Lasso regularized logistic model
    (see Friedman et al. (2010)), sparse linear discriminant analysis (see
    Clemmensen et al. (2011)), shrinkage linear and diagonal discriminant
    analysis (see M. Ahdesmaki et al. (2010)). More methods of classification can be used on the decorrelated data provided by the package FADA.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Depends:</td>
<td>MASS, elasticnet</td>
</tr>
<tr>
<td>Imports:</td>
<td>sparseLDA,sda,glmnet,mnormt,crossval,corpcor,
matrixStats,methods</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2019-12-10 12:58:40 UTC; epertham</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2019-12-10 15:30:05 UTC</td>
</tr>
</table>
<hr>
<h2 id='FADA-package'>
Variable selection for supervised classification in high dimension
</h2><span id='topic+FADA-package'></span>

<h3>Description</h3>

<p>The functions provided in the FADA (Factor Adjusted Discriminant Analysis) package aim at performing supervised classification of high-dimensional and correlated profiles. The procedure combines a decorrelation step based on a  
factor modeling of the dependence among covariates and a classification method. The available methods are Lasso regularized logistic model
(see Friedman et al. (2010)), sparse linear discriminant analysis (see
Clemmensen et al. (2011)), shrinkage linear and diagonal discriminant
analysis (see M. Ahdesmaki et al. (2010)). More methods of classification can be used on the decorrelated data provided by the package FADA.
</p>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> FADA</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 1.2</td>
</tr>
<tr>
 <td style="text-align: left;">
Date: </td><td style="text-align: left;"> 2014-10-08</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL (&gt;= 2)</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p>The functions available in this package are used in this order: 
</p>

<ul>
<li><p> Step 1: Decorrelation of the training dataset using a factor model of the covariance by the <code>decorrelate.train</code> function. The number of factors of the model can be estimated or forced.
</p>
</li>
<li><p> Step 2: If needed, decorrelation of the testing dataset by using the <code>decorrelate.test</code> function and the estimated factor model parameters provided by <code>decorrelate.train</code>.
</p>
</li>
<li><p> Step 3: Estimation of a supervised classification model using the decorrelated training dataset by the <code>FADA</code> function. One can choose among several classification methods (more details in the manual of <code>FADA</code> function). 
</p>
</li>
<li><p> Step 4: If needed, computation of the error rate by the <code>FADA</code> function, either using a supplementary test dataset or by K-fold cross-validation.</p>
</li></ul>



<h3>Author(s)</h3>

<p>Emeline Perthame (Agrocampus Ouest, Rennes, France), Chloe Friguet
(Universite de Bretagne Sud, Vannes, France) and David Causeur (Agrocampus
Ouest, Rennes, France)
</p>
<p>Maintainer: David Causeur, http://math.agrocampus-ouest.fr/infoglueDeliverLive/membres/david.causeur, mailto: david.causeur@agrocampus-ouest.fr
</p>


<h3>References</h3>

<p>Ahdesmaki, M. and Strimmer, K. (2010), Feature selection in omics prediction problems using cat scores and false non-discovery rate control. Annals of Applied Statistics, 4, 503-519.
</p>
<p>Clemmensen, L., Hastie, T. and Witten, D. and Ersboll, B. (2011), Sparse discriminant analysis. Technometrics, 53(4), 406-413.
</p>
<p>Friedman, J., Hastie, T. and Tibshirani, R. (2010), Regularization paths for generalized linear models via coordinate descent. Journal of Statistical Software, 33, 1-22.
</p>
<p>Friguet, C., Kloareg, M. and Causeur, D. (2009), A factor model approach to multiple testing under dependence. Journal of the American Statistical Association, 104:488, 1406-1415.
</p>
<p>Perthame, E., Friguet, C. and Causeur, D. (2015), Stability of feature selection in classification issues for high-dimensional correlated data, Statistics and Computing.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> ### Not run 
 ### example of an entire analysis with FADA package if a testing data set is available
 ### loading data
 # data(data.train)
 # data(data.test)
 
 # dim(data.train$x) # 30 250
 # dim(data.test$x) # 1000 250

 ### decorrelation of the training data set
 # res = decorrelate.train(data.train) # Optimal number of factors is 3
 ### decorrelation of the testing data set afterward
 # res2 = decorrelate.test(res,data.test)

 ### classification step with sda, using local false discovery rate for variable selection
 ### linear discriminant analysis
 # FADA.LDA = FADA(res2,method="sda",sda.method="lfdr")
 
 ### diagonal discriminant analysis 
 # FADA.DDA =  FADA(res2, method="sda",sda.method="lfdr",diagonal=TRUE)


### example of an entire analysis with FADA package if no testing data set is available
 ### loading data
 
 ### decorrelation step
 # res = decorrelate.train(data.train) # Optimal number of factors is 3
 
 ### classification step with sda, using local false discovery rate for variable selection
 ### linear discriminant analysis, error rate is computed by 10-fold CV (20 replications of the CV)
 # FADA.LDA = FADA(res,method="sda",sda.method="lfdr")

</code></pre>

<hr>
<h2 id='data.test'>
Test dataset simulated with the same distribution as the training dataset data.train.
</h2><span id='topic+data.test'></span>

<h3>Description</h3>

<p>The test dataset has the same list structure as the training dataset dta. Only the numbers of rows of the x component and length of the y component are different since the test sample 
size is 1000.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(data.test)</code></pre>


<h3>Format</h3>

<p>List with 2 components: <code>x</code>, the 1000x250 matrix of simulated explanatory variables and <code>y</code>, the 1000x1 grouping variable (coded 1 and 2).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(data.test)
dim(data.test$x) # 1000 250
data.test$y # 2 levels
</code></pre>

<hr>
<h2 id='data.train'>
Training data
</h2><span id='topic+data.train'></span>

<h3>Description</h3>

<p>Simulated training dataset. The x component is a matrix of explanatory variables, with 30 rows and 250 columns. Each row is simulated according to a multinormal distribution 
which mean depends on a group membership given by the y component. The variance matrix is the same within each group. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(data.train)</code></pre>


<h3>Format</h3>

<p>A list with 2 components. <code>x</code> is a 30x250 matrix of simulated explanatory variables. <code>y</code> is a 30x1 grouping variable (coded 1 and 2).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(data.train)
dim(data.train$x) # 30 250
data.train$y # 2 levels
hist(cor(data.train$x[data.train$y==1,])) # high dependence 
hist(cor(data.train$x[data.train$y==2,]))
</code></pre>

<hr>
<h2 id='decorrelate.test'>Factor Adjusted Discriminant Analysis 2: Decorrelation of a testing data set after running the <code>decorrelate.train</code> function on a training data set</h2><span id='topic+decorrelate.test'></span>

<h3>Description</h3>

<p>This function decorrelates the test dataset by adjusting data for the effects of latent factors of dependence, after running the <code>decorrelate.train</code> function on a training data set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>decorrelate.test(faobject,data.test)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="decorrelate.test_+3A_faobject">faobject</code></td>
<td>
<p>An object returned by function <code>decorrelate.train</code>.</p>
</td></tr>
<tr><td><code id="decorrelate.test_+3A_data.test">data.test</code></td>
<td>
<p>A list containing the testing dataset, with the following component: <code>x</code> is a n x p matrix of explanatory variables, where n stands for the testing sample size and
p for the number of explanatory variables.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a list with the following elements:
</p>
<table role = "presentation">
<tr><td><code>meanclass</code></td>
<td>
<p>Group means estimated after iterative decorrelation</p>
</td></tr>
<tr><td><code>fa.training</code></td>
<td>
<p>Decorrelated training data</p>
</td></tr>
<tr><td><code>fa.testing</code></td>
<td>
<p>Decorrelated testing data</p>
</td></tr>
<tr><td><code>Psi</code></td>
<td>
<p>Estimation of the factor model parameters: specific variance</p>
</td></tr>
<tr><td><code>B</code></td>
<td>
<p>Estimation of the factor model parameters: loadings</p>
</td></tr>
<tr><td><code>factors.training</code></td>
<td>
<p>Scores of the trainings individuals on the factors</p>
</td></tr>
<tr><td><code>factors.testing</code></td>
<td>
<p>Scores of the testing individuals on the factors</p>
</td></tr>
<tr><td><code>groups</code></td>
<td>
<p>Recall of group variable of training data</p>
</td></tr>
<tr><td><code>proba.training</code></td>
<td>
<p>Internal value (estimation of individual probabilities for the training dataset)</p>
</td></tr>
<tr><td><code>proba.testing</code></td>
<td>
<p>Internal value (estimation of individual probabilities for the testing dataset)</p>
</td></tr>
<tr><td><code>mod.decorrelate.test</code></td>
<td>
<p>Internal value (classification model)</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Emeline Perthame, Chloe Friguet and David Causeur
</p>


<h3>References</h3>

<p>Friedman, J., Hastie, T. and Tibshirani, R. (2010), Regularization paths for generalized linear models via coordinate descent. Journal of Statistical Software, 33, 1-22.
</p>
<p>Friguet, C., Kloareg, M. and Causeur, D. (2009), A factor model approach to multiple testing under dependence. Journal of the American Statistical Association, 104:488, 1406-1415.
</p>
<p>Perthame, E., Friguet, C. and Causeur, D. (2015), Stability of feature selection in classification issues for high-dimensional correlated data, Statistics and Computing.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+FADA-package">FADA-package</a></code> <code><a href="#topic+FADA">FADA</a> </code> <code><a href="glmnet.html#topic+glmnet-package">glmnet-package</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(data.train)
data(data.test)
fa = decorrelate.train(data.train)
fa2 = decorrelate.test(fa,data.test)
names(fa2)
</code></pre>

<hr>
<h2 id='decorrelate.train'>Factor Adjusted Discriminant Analysis 1: Decorrelation of the training data</h2><span id='topic+decorrelate.train'></span>

<h3>Description</h3>

<p>This function decorrelates the training dataset by adjusting data for the effects of latent factors of dependence.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>decorrelate.train(data.train, nbf = NULL, maxnbfactors=12, diagnostic.plot = FALSE, 
min.err = 0.001, verbose = TRUE,EM = TRUE, maxiter = 15,...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="decorrelate.train_+3A_data.train">data.train</code></td>
<td>
<p>A list containing the training dataset with the following components: <code>x</code> is the n x p matrix of explanatory variables, where n stands for the training sample size and
p for the number of explanatory variables ; <code>y</code> is a numeric vector giving the group of each individual numbered from 1 to K.</p>
</td></tr>
<tr><td><code id="decorrelate.train_+3A_nbf">nbf</code></td>
<td>
<p>Number of factors. If <code>nbf = NULL</code>, the number of factors is estimated. <code>nbf</code> can
also be set to a positive integer value. If <code>nbf = 0</code>, the data are not factor-adjusted.</p>
</td></tr>
<tr><td><code id="decorrelate.train_+3A_maxnbfactors">maxnbfactors</code></td>
<td>
<p>The maximum number of factors. Default is <code>maxnbfactors=12</code>.</p>
</td></tr>
<tr><td><code id="decorrelate.train_+3A_diagnostic.plot">diagnostic.plot</code></td>
<td>
<p>If <code>diagnostic.plot =TRUE</code>, the values of the variance inflation criterion are
plotted for each number of factors. Default is <code>diagnostic.plot =FALSE</code>. This option might be helpful
to manually determine the optimal number of factors.</p>
</td></tr>
<tr><td><code id="decorrelate.train_+3A_min.err">min.err</code></td>
<td>
<p>Threshold of convergence of the algorithm criterion. Default is min.err=0.001.</p>
</td></tr>
<tr><td><code id="decorrelate.train_+3A_verbose">verbose</code></td>
<td>
<p>Print out number of factors and values of the objective criterion along the iterations. Default is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="decorrelate.train_+3A_em">EM</code></td>
<td>
<p>The method used to estimate the parameters of the factor model. If <code>EM=TRUE</code>, parameters are estimated by an EM algorithm. Setting <code>EM=TRUE</code> is recommended when the number of covariates exceeds the number of observations. If <code>EM=FALSE</code>, the parameters are estimated by maximum-likelihood using <code>factanal</code>. Default is <code>EM=TRUE</code></p>
</td></tr>
<tr><td><code id="decorrelate.train_+3A_maxiter">maxiter</code></td>
<td>
<p>Maximum number of iterations for estimation of the factor model.</p>
</td></tr>
<tr><td><code id="decorrelate.train_+3A_...">...</code></td>
<td>
<p>Other arguments that can be passed in the <code>cv.glmnet</code> and <code>glmnet</code> functions from glmnet package. These functions are used to estimate individual group probabilities. Modifying these parameters should not affect the decorrelation procedure. However, the argument <code>nfolds</code> in <code>cv.glmnet</code> is set to 10 by default and should be reduced (minimum 3) for large datasets, in order to decrease the computation time of <code>decorrelation.train</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a list with the following elements:
</p>
<table role = "presentation">
<tr><td><code>meanclass</code></td>
<td>
<p>Group means estimated after iterative decorrelation</p>
</td></tr>
<tr><td><code>fa.training</code></td>
<td>
<p>Decorrelated training data</p>
</td></tr>
<tr><td><code>Psi</code></td>
<td>
<p>Estimation of the factor model parameters: specific variance</p>
</td></tr>
<tr><td><code>B</code></td>
<td>
<p>Estimation of the factor model parameters: loadings</p>
</td></tr>
<tr><td><code>factors.training</code></td>
<td>
<p>Scores of the trainings individuals on the factors</p>
</td></tr>
<tr><td><code>groups</code></td>
<td>
<p>Recall of group variable of training data</p>
</td></tr>
<tr><td><code>proba.training</code></td>
<td>
<p>Internal value (estimation of individual probabilities for the training dataset)</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Emeline Perthame, Chloe Friguet and David Causeur
</p>


<h3>References</h3>

<p>Friedman, J., Hastie, T. and Tibshirani, R. (2010), Regularization paths for generalized linear models via coordinate descent. Journal of Statistical Software, 33, 1-22.
</p>
<p>Friguet, C., Kloareg, M. and Causeur, D. (2009), A factor model approach to multiple testing under dependence. Journal of the American Statistical Association, 104:488, 1406-1415.
</p>
<p>Perthame, E., Friguet, C. and Causeur, D. (2015), Stability of feature selection in classification issues for high-dimensional correlated data, Statistics and Computing.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+FADA-package">FADA-package</a></code> <code><a href="#topic+FADA">FADA</a> </code> <code><a href="glmnet.html#topic+glmnet-package">glmnet-package</a></code> <code><a href="stats.html#topic+factanal">factanal</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(data.train)

res0 = decorrelate.train(data.train,nbf=3) #  when the number of factors is forced

res1 = decorrelate.train(data.train) #  when the optimal number of factors is unknown
</code></pre>

<hr>
<h2 id='FADA'>Factor Adjusted Discriminant Analysis 3-4 : Supervised classification on decorrelated data</h2><span id='topic+FADA'></span>

<h3>Description</h3>

<p>This function performs supervised classification on factor-adjusted data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>FADA(faobject, K=10,B=20, nbf.cv = NULL,method = c("glmnet", 
    "sda", "sparseLDA"), sda.method = c("lfdr", "HC"), alpha=0.1, ...) 
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="FADA_+3A_faobject">faobject</code></td>
<td>
<p>An object returned by function <code>decorrelate.train</code> or <code>decorrelate.test</code>.</p>
</td></tr>
<tr><td><code id="FADA_+3A_k">K</code></td>
<td>
<p>Number of folds to estimate classification error rate, only when no testing data is provided. Default is <code>K=10</code>.</p>
</td></tr>
<tr><td><code id="FADA_+3A_b">B</code></td>
<td>
<p> Number of replications of the cross-validation. Default is <code>B=20</code>.</p>
</td></tr>
<tr><td><code id="FADA_+3A_nbf.cv">nbf.cv</code></td>
<td>
<p>Number of factors for cross validation to compute error rate, only when no testing data is provided. By default, <code>nbf = NULL</code> and the number of factors is estimated for each fold of the cross validation. <code>nbf</code> can
also be set to a positive integer value. If <code>nbf = 0</code>, the data are not factor-adjusted.</p>
</td></tr>
<tr><td><code id="FADA_+3A_method">method</code></td>
<td>
<p>The method used to perform supervised classification model. 3 options are available. If
<code>method = "glmnet"</code>, a Lasso penalized logistic regression is performed using <span class="pkg">glmnet</span> R package.
If <code>method = "sda"</code>, a LDA or DDA (see <code>diagonal</code> argument) is performed using Shrinkage Discriminant
Analysis using <span class="pkg">sda</span> R package. If <code>method = "sparseLDA"</code>, a Lasso penalized LDA is performed using
<span class="pkg">SparseLDA</span> R package.</p>
</td></tr>
<tr><td><code id="FADA_+3A_sda.method">sda.method</code></td>
<td>
<p>The method used for variable selection, only if <code>method="sda"</code>. If <code>sda.method="lfdr"</code>,
variables are selected through CAT scores and False Non Discovery Rate control. If sda.method=&quot;HC&quot;, the variable selection
method is Higher Cristicism Thresholding.</p>
</td></tr>
<tr><td><code id="FADA_+3A_alpha">alpha</code></td>
<td>
<p>The proportion of the HC objective to be observed, only if method=&quot;sda&quot; and sda.method=&quot;HC&quot;. Default is 0.1.</p>
</td></tr>
<tr><td><code id="FADA_+3A_...">...</code></td>
<td>
<p>Some arguments to tune the classification method. See the documentation of the chosen method (<a href="glmnet.html#topic+glmnet">glmnet</a>, <a href="sda.html#topic+sda">sda</a> or <a href="sparseLDA.html#topic+sda">sda</a>) for more informations about these parameters.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a list with the following elements:
</p>
<table role = "presentation">
<tr><td><code>method</code></td>
<td>
<p>Recall of the classification method</p>
</td></tr>
<tr><td><code>selected</code></td>
<td>
<p>A vector containing index of the selected variables</p>
</td></tr>
<tr><td><code>proba.train</code></td>
<td>
<p>A matrix containing predicted group frequencies of training data.</p>
</td></tr>
<tr><td><code>proba.test</code></td>
<td>
<p>A matrix containing predicted group frequencies of testing data, if a testing data set has been provided</p>
</td></tr>
<tr><td><code>predict.test</code></td>
<td>
<p>A matrix containing predicted classes of testing data, if a testing data set has been provided</p>
</td></tr>
<tr><td><code>cv.error</code></td>
<td>
<p>A numeric value containing the average classification error, computed by cross validation, if no testing data set has been provided</p>
</td></tr>
<tr><td><code>cv.error.se</code></td>
<td>
<p>A numeric value containing the standard error of the classification error, computed by cross validation, if no testing data set has been provided</p>
</td></tr>
<tr><td><code>mod</code></td>
<td>
<p>The classification model performed. The class of this element is the class of a model returned by the chosen method. See the documentation of the chosen method for more details. </p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Emeline Perthame, Chloe Friguet and David Causeur
</p>


<h3>References</h3>

<p>Ahdesmaki, M. and Strimmer, K. (2010), Feature selection in omics prediction problems using cat scores and false non-discovery rate control. Annals of Applied Statistics, 4, 503-519.
</p>
<p>Clemmensen, L., Hastie, T. and Witten, D. and Ersboll, B. (2011), Sparse discriminant analysis. Technometrics, 53(4), 406-413.
</p>
<p>Friedman, J., Hastie, T. and Tibshirani, R. (2010), Regularization paths for generalized linear models via coordinate descent. Journal of Statistical Software, 33, 1-22.
</p>
<p>Friguet, C., Kloareg, M. and Causeur, D. (2009), A factor model approach to multiple testing under dependence. Journal of the American Statistical Association, 104:488, 1406-1415.
</p>
<p>Perthame, E., Friguet, C. and Causeur, D. (2015), Stability of feature selection in classification issues for high-dimensional correlated data, Statistics and Computing.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+FADA">FADA</a></code>, <code><a href="#topic+decorrelate.train">decorrelate.train</a></code>, <code><a href="#topic+decorrelate.test">decorrelate.test</a></code>, <code><a href="sparseLDA.html#topic+sda">sda</a></code>, <code><a href="sda.html#topic+sda-package">sda-package</a></code>,
<code><a href="glmnet.html#topic+glmnet-package">glmnet-package</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(data.train)
data(data.test)

# When testing data set is provided
res = decorrelate.train(data.train)
res2 = decorrelate.test(res, data.test)
classif = FADA(res2,method="sda",sda.method="lfdr")

### Not run 
# When no testing data set is provided
# Classification error rate is computed by a K-fold cross validation.
# res = decorrelate.train(data.train)
# classif = FADA(res, method="sda",sda.method="lfdr")
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
