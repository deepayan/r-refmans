<!DOCTYPE html><html lang="en"><head><title>Help for package xgboost</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {xgboost}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#a-compatibility-note-for-saveRDS-save'><p>Do not use <code>saveRDS</code> or <code>save</code> for long-term archival of</p>
models. Instead, use <code>xgb.save</code> or <code>xgb.save.raw</code>.</a></li>
<li><a href='#agaricus.test'><p>Test part from Mushroom Data Set</p></a></li>
<li><a href='#agaricus.train'><p>Training part from Mushroom Data Set</p></a></li>
<li><a href='#callbacks'><p>Callback closures for booster training.</p></a></li>
<li><a href='#cb.cv.predict'><p>Callback closure for returning cross-validation based predictions.</p></a></li>
<li><a href='#cb.early.stop'><p>Callback closure to activate the early stopping.</p></a></li>
<li><a href='#cb.evaluation.log'><p>Callback closure for logging the evaluation history</p></a></li>
<li><a href='#cb.gblinear.history'><p>Callback closure for collecting the model coefficients history of a gblinear booster</p>
during its training.</a></li>
<li><a href='#cb.print.evaluation'><p>Callback closure for printing the result of evaluation</p></a></li>
<li><a href='#cb.reset.parameters'><p>Callback closure for resetting the booster's parameters at each iteration.</p></a></li>
<li><a href='#cb.save.model'><p>Callback closure for saving a model file.</p></a></li>
<li><a href='#dim.xgb.DMatrix'><p>Dimensions of xgb.DMatrix</p></a></li>
<li><a href='#dimnames.xgb.DMatrix'><p>Handling of column names of <code>xgb.DMatrix</code></p></a></li>
<li><a href='#getinfo'><p>Get information of an xgb.DMatrix object</p></a></li>
<li><a href='#normalize'><p>Scale feature value to have mean 0, standard deviation 1</p></a></li>
<li><a href='#predict.xgb.Booster'><p>Predict method for eXtreme Gradient Boosting model</p></a></li>
<li><a href='#prepare.ggplot.shap.data'><p>Combine and melt feature values and SHAP contributions for sample</p>
observations.</a></li>
<li><a href='#print.xgb.Booster'><p>Print xgb.Booster</p></a></li>
<li><a href='#print.xgb.cv.synchronous'><p>Print xgb.cv result</p></a></li>
<li><a href='#print.xgb.DMatrix'><p>Print xgb.DMatrix</p></a></li>
<li><a href='#setinfo'><p>Set information of an xgb.DMatrix object</p></a></li>
<li><a href='#slice'><p>Get a new DMatrix containing the specified rows of</p>
original xgb.DMatrix object</a></li>
<li><a href='#xgb.attr'><p>Accessors for serializable attributes of a model.</p></a></li>
<li><a href='#xgb.Booster.complete'><p>Restore missing parts of an incomplete xgb.Booster object.</p></a></li>
<li><a href='#xgb.config'><p>Accessors for model parameters as JSON string.</p></a></li>
<li><a href='#xgb.create.features'><p>Create new features from a previously learned model</p></a></li>
<li><a href='#xgb.cv'><p>Cross Validation</p></a></li>
<li><a href='#xgb.DMatrix'><p>Construct xgb.DMatrix object</p></a></li>
<li><a href='#xgb.DMatrix.save'><p>Save xgb.DMatrix object to binary file</p></a></li>
<li><a href='#xgb.dump'><p>Dump an xgboost model in text format.</p></a></li>
<li><a href='#xgb.gblinear.history'><p>Extract gblinear coefficients history.</p></a></li>
<li><a href='#xgb.ggplot.deepness'><p>Plot model trees deepness</p></a></li>
<li><a href='#xgb.ggplot.importance'><p>Plot feature importance as a bar graph</p></a></li>
<li><a href='#xgb.ggplot.shap.summary'><p>SHAP contribution dependency summary plot</p></a></li>
<li><a href='#xgb.importance'><p>Importance of features in a model.</p></a></li>
<li><a href='#xgb.load'><p>Load xgboost model from binary file</p></a></li>
<li><a href='#xgb.load.raw'><p>Load serialised xgboost model from R's raw vector</p></a></li>
<li><a href='#xgb.model.dt.tree'><p>Parse a boosted tree model text dump</p></a></li>
<li><a href='#xgb.parameters+26lt+3B-'><p>Accessors for model parameters.</p></a></li>
<li><a href='#xgb.plot.multi.trees'><p>Project all trees on one tree and plot it</p></a></li>
<li><a href='#xgb.plot.shap'><p>SHAP contribution dependency plots</p></a></li>
<li><a href='#xgb.plot.tree'><p>Plot a boosted tree model</p></a></li>
<li><a href='#xgb.save'><p>Save xgboost model to binary file</p></a></li>
<li><a href='#xgb.save.raw'><p>Save xgboost model to R's raw vector,</p>
user can call xgb.load.raw to load the model back from raw vector</a></li>
<li><a href='#xgb.serialize'><p>Serialize the booster instance into R's raw vector.  The serialization method differs</p>
from <code>xgb.save.raw</code> as the latter one saves only the model but not
parameters.  This serialization format is not stable across different xgboost versions.</a></li>
<li><a href='#xgb.set.config+2C+20xgb.get.config'><p>Set and get global configuration</p></a></li>
<li><a href='#xgb.shap.data'><p>Prepare data for SHAP plots. To be used in xgb.plot.shap, xgb.plot.shap.summary, etc.</p>
Internal utility function.</a></li>
<li><a href='#xgb.train'><p>eXtreme Gradient Boosting Training</p></a></li>
<li><a href='#xgb.unserialize'><p>Load the instance back from <code>xgb.serialize</code></p></a></li>
<li><a href='#xgboost-deprecated'><p>Deprecation notices.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Extreme Gradient Boosting</td>
</tr>
<tr>
<td>Version:</td>
<td>1.7.8.1</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-07-22</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Jiaming Yuan &lt;jm.yuan@outlook.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Extreme Gradient Boosting, which is an efficient implementation
    of the gradient boosting framework from Chen &amp; Guestrin (2016) &lt;<a href="https://doi.org/10.1145%2F2939672.2939785">doi:10.1145/2939672.2939785</a>&gt;.
    This package is its R interface. The package includes efficient linear
    model solver and tree learning algorithms. The package can automatically
    do parallel computation on a single machine which could be more than 10
    times faster than existing gradient boosting packages. It supports
    various objective functions, including regression, classification and ranking.
    The package is made to be extensible, so that users are also allowed to define
    their own objectives easily.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.apache.org/licenses/LICENSE-2.0">Apache License (== 2.0)</a> | file LICENSE</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/dmlc/xgboost">https://github.com/dmlc/xgboost</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/dmlc/xgboost/issues">https://github.com/dmlc/xgboost/issues</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown, ggplot2 (&ge; 1.0.1), DiagrammeR (&ge; 0.9.0),
Ckmeans.1d.dp (&ge; 3.3.1), vcd (&ge; 1.3), testthat, lintr, igraph
(&ge; 1.0.1), float, crayon, titanic</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.3.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>Matrix (&ge; 1.1-0), methods, data.table (&ge; 1.9.6), jsonlite
(&ge; 1.0),</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>GNU make, C++17</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-07-24 14:21:15 UTC; jiamingy</td>
</tr>
<tr>
<td>Author:</td>
<td>Tianqi Chen [aut],
  Tong He [aut],
  Michael Benesty [aut],
  Vadim Khotilovich [aut],
  Yuan Tang <a href="https://orcid.org/0000-0001-5243-233X"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut],
  Hyunsu Cho [aut],
  Kailong Chen [aut],
  Rory Mitchell [aut],
  Ignacio Cano [aut],
  Tianyi Zhou [aut],
  Mu Li [aut],
  Junyuan Xie [aut],
  Min Lin [aut],
  Yifeng Geng [aut],
  Yutian Li [aut],
  Jiaming Yuan [aut, cre],
  XGBoost contributors [cph] (base XGBoost implementation)</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-07-24 18:40:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='a-compatibility-note-for-saveRDS-save'>Do not use <code><a href="base.html#topic+saveRDS">saveRDS</a></code> or <code><a href="base.html#topic+save">save</a></code> for long-term archival of
models. Instead, use <code><a href="#topic+xgb.save">xgb.save</a></code> or <code><a href="#topic+xgb.save.raw">xgb.save.raw</a></code>.</h2><span id='topic+a-compatibility-note-for-saveRDS-save'></span>

<h3>Description</h3>

<p>It is a common practice to use the built-in <code><a href="base.html#topic+saveRDS">saveRDS</a></code> function (or
<code><a href="base.html#topic+save">save</a></code>) to persist R objects to the disk. While it is possible to persist
<code>xgb.Booster</code> objects using <code><a href="base.html#topic+saveRDS">saveRDS</a></code>, it is not advisable to do so if
the model is to be accessed in the future. If you train a model with the current version of
XGBoost and persist it with <code><a href="base.html#topic+saveRDS">saveRDS</a></code>, the model is not guaranteed to be
accessible in later releases of XGBoost. To ensure that your model can be accessed in future
releases of XGBoost, use <code><a href="#topic+xgb.save">xgb.save</a></code> or <code><a href="#topic+xgb.save.raw">xgb.save.raw</a></code> instead.
</p>


<h3>Details</h3>

<p>Use <code><a href="#topic+xgb.save">xgb.save</a></code> to save the XGBoost model as a stand-alone file. You may opt into
the JSON format by specifying the JSON extension. To read the model back, use
<code><a href="#topic+xgb.load">xgb.load</a></code>.
</p>
<p>Use <code><a href="#topic+xgb.save.raw">xgb.save.raw</a></code> to save the XGBoost model as a sequence (vector) of raw bytes
in a future-proof manner. Future releases of XGBoost will be able to read the raw bytes and
re-construct the corresponding model. To read the model back, use <code><a href="#topic+xgb.load.raw">xgb.load.raw</a></code>.
The <code><a href="#topic+xgb.save.raw">xgb.save.raw</a></code> function is useful if you'd like to persist the XGBoost model
as part of another R object.
</p>
<p>Note: Do not use <code><a href="#topic+xgb.serialize">xgb.serialize</a></code> to store models long-term. It persists not only the
model but also internal configurations and parameters, and its format is not stable across
multiple XGBoost versions. Use <code><a href="#topic+xgb.serialize">xgb.serialize</a></code> only for checkpointing.
</p>
<p>For more details and explanation about model persistence and archival, consult the page
<a href="https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html">https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(agaricus.train, package='xgboost')
bst &lt;- xgboost(data = agaricus.train$data, label = agaricus.train$label, max_depth = 2,
               eta = 1, nthread = 2, nrounds = 2, objective = "binary:logistic")

# Save as a stand-alone file; load it with xgb.load()
xgb.save(bst, 'xgb.model')
bst2 &lt;- xgb.load('xgb.model')

# Save as a stand-alone file (JSON); load it with xgb.load()
xgb.save(bst, 'xgb.model.json')
bst2 &lt;- xgb.load('xgb.model.json')
if (file.exists('xgb.model.json')) file.remove('xgb.model.json')

# Save as a raw byte vector; load it with xgb.load.raw()
xgb_bytes &lt;- xgb.save.raw(bst)
bst2 &lt;- xgb.load.raw(xgb_bytes)

# Persist XGBoost model as part of another R object
obj &lt;- list(xgb_model_bytes = xgb.save.raw(bst), description = "My first XGBoost model")
# Persist the R object. Here, saveRDS() is okay, since it doesn't persist
# xgb.Booster directly. What's being persisted is the future-proof byte representation
# as given by xgb.save.raw().
saveRDS(obj, 'my_object.rds')
# Read back the R object
obj2 &lt;- readRDS('my_object.rds')
# Re-construct xgb.Booster object from the bytes
bst2 &lt;- xgb.load.raw(obj2$xgb_model_bytes)
if (file.exists('my_object.rds')) file.remove('my_object.rds')

</code></pre>

<hr>
<h2 id='agaricus.test'>Test part from Mushroom Data Set</h2><span id='topic+agaricus.test'></span>

<h3>Description</h3>

<p>This data set is originally from the Mushroom data set,
UCI Machine Learning Repository.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(agaricus.test)
</code></pre>


<h3>Format</h3>

<p>A list containing a label vector, and a dgCMatrix object with 1611
rows and 126 variables
</p>


<h3>Details</h3>

<p>This data set includes the following fields:
</p>

<ul>
<li> <p><code>label</code> the label for each record
</p>
</li>
<li> <p><code>data</code> a sparse Matrix of <code>dgCMatrix</code> class, with 126 columns.
</p>
</li></ul>



<h3>References</h3>

<p>https://archive.ics.uci.edu/ml/datasets/Mushroom
</p>
<p>Bache, K. &amp; Lichman, M. (2013). UCI Machine Learning Repository
[http://archive.ics.uci.edu/ml]. Irvine, CA: University of California,
School of Information and Computer Science.
</p>

<hr>
<h2 id='agaricus.train'>Training part from Mushroom Data Set</h2><span id='topic+agaricus.train'></span>

<h3>Description</h3>

<p>This data set is originally from the Mushroom data set,
UCI Machine Learning Repository.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(agaricus.train)
</code></pre>


<h3>Format</h3>

<p>A list containing a label vector, and a dgCMatrix object with 6513
rows and 127 variables
</p>


<h3>Details</h3>

<p>This data set includes the following fields:
</p>

<ul>
<li> <p><code>label</code> the label for each record
</p>
</li>
<li> <p><code>data</code> a sparse Matrix of <code>dgCMatrix</code> class, with 126 columns.
</p>
</li></ul>



<h3>References</h3>

<p>https://archive.ics.uci.edu/ml/datasets/Mushroom
</p>
<p>Bache, K. &amp; Lichman, M. (2013). UCI Machine Learning Repository
[http://archive.ics.uci.edu/ml]. Irvine, CA: University of California,
School of Information and Computer Science.
</p>

<hr>
<h2 id='callbacks'>Callback closures for booster training.</h2><span id='topic+callbacks'></span>

<h3>Description</h3>

<p>These are used to perform various service tasks either during boosting iterations or at the end.
This approach helps to modularize many of such tasks without bloating the main training methods,
and it offers .
</p>


<h3>Details</h3>

<p>By default, a callback function is run after each boosting iteration.
An R-attribute <code>is_pre_iteration</code> could be set for a callback to define a pre-iteration function.
</p>
<p>When a callback function has <code>finalize</code> parameter, its finalizer part will also be run after
the boosting is completed.
</p>
<p>WARNING: side-effects!!! Be aware that these callback functions access and modify things in
the environment from which they are called from, which is a fairly uncommon thing to do in R.
</p>
<p>To write a custom callback closure, make sure you first understand the main concepts about R environments.
Check either R documentation on <code><a href="base.html#topic+environment">environment</a></code> or the
<a href="http://adv-r.had.co.nz/Environments.html">Environments chapter</a> from the &quot;Advanced R&quot;
book by Hadley Wickham. Further, the best option is to read the code of some of the existing callbacks -
choose ones that do something similar to what you want to achieve. Also, you would need to get familiar
with the objects available inside of the <code>xgb.train</code> and <code>xgb.cv</code> internal environments.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cb.print.evaluation">cb.print.evaluation</a></code>,
<code><a href="#topic+cb.evaluation.log">cb.evaluation.log</a></code>,
<code><a href="#topic+cb.reset.parameters">cb.reset.parameters</a></code>,
<code><a href="#topic+cb.early.stop">cb.early.stop</a></code>,
<code><a href="#topic+cb.save.model">cb.save.model</a></code>,
<code><a href="#topic+cb.cv.predict">cb.cv.predict</a></code>,
<code><a href="#topic+xgb.train">xgb.train</a></code>,
<code><a href="#topic+xgb.cv">xgb.cv</a></code>
</p>

<hr>
<h2 id='cb.cv.predict'>Callback closure for returning cross-validation based predictions.</h2><span id='topic+cb.cv.predict'></span>

<h3>Description</h3>

<p>Callback closure for returning cross-validation based predictions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cb.cv.predict(save_models = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cb.cv.predict_+3A_save_models">save_models</code></td>
<td>
<p>a flag for whether to save the folds' models.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This callback function saves predictions for all of the test folds,
and also allows to save the folds' models.
</p>
<p>It is a &quot;finalizer&quot; callback and it uses early stopping information whenever it is available,
thus it must be run after the early stopping callback if the early stopping is used.
</p>
<p>Callback function expects the following values to be set in its calling frame:
<code>bst_folds</code>,
<code>basket</code>,
<code>data</code>,
<code>end_iteration</code>,
<code>params</code>,
<code>num_parallel_tree</code>,
<code>num_class</code>.
</p>


<h3>Value</h3>

<p>Predictions are returned inside of the <code>pred</code> element, which is either a vector or a matrix,
depending on the number of prediction outputs per data row. The order of predictions corresponds
to the order of rows in the original dataset. Note that when a custom <code>folds</code> list is
provided in <code>xgb.cv</code>, the predictions would only be returned properly when this list is a
non-overlapping list of k sets of indices, as in a standard k-fold CV. The predictions would not be
meaningful when user-provided folds have overlapping indices as in, e.g., random sampling splits.
When some of the indices in the training dataset are not included into user-provided <code>folds</code>,
their prediction value would be <code>NA</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+callbacks">callbacks</a></code>
</p>

<hr>
<h2 id='cb.early.stop'>Callback closure to activate the early stopping.</h2><span id='topic+cb.early.stop'></span>

<h3>Description</h3>

<p>Callback closure to activate the early stopping.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cb.early.stop(
  stopping_rounds,
  maximize = FALSE,
  metric_name = NULL,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cb.early.stop_+3A_stopping_rounds">stopping_rounds</code></td>
<td>
<p>The number of rounds with no improvement in
the evaluation metric in order to stop the training.</p>
</td></tr>
<tr><td><code id="cb.early.stop_+3A_maximize">maximize</code></td>
<td>
<p>whether to maximize the evaluation metric</p>
</td></tr>
<tr><td><code id="cb.early.stop_+3A_metric_name">metric_name</code></td>
<td>
<p>the name of an evaluation column to use as a criteria for early
stopping. If not set, the last column would be used.
Let's say the test data in <code>watchlist</code> was labelled as <code>dtest</code>,
and one wants to use the AUC in test data for early stopping regardless of where
it is in the <code>watchlist</code>, then one of the following would need to be set:
<code>metric_name='dtest-auc'</code> or <code>metric_name='dtest_auc'</code>.
All dash '-' characters in metric names are considered equivalent to '_'.</p>
</td></tr>
<tr><td><code id="cb.early.stop_+3A_verbose">verbose</code></td>
<td>
<p>whether to print the early stopping information.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This callback function determines the condition for early stopping
by setting the <code>stop_condition = TRUE</code> flag in its calling frame.
</p>
<p>The following additional fields are assigned to the model's R object:
</p>

<ul>
<li> <p><code>best_score</code> the evaluation score at the best iteration
</p>
</li>
<li> <p><code>best_iteration</code> at which boosting iteration the best score has occurred (1-based index)
</p>
</li></ul>

<p>The Same values are also stored as xgb-attributes:
</p>

<ul>
<li> <p><code>best_iteration</code> is stored as a 0-based iteration index (for interoperability of binary models)
</p>
</li>
<li> <p><code>best_msg</code> message string is also stored.
</p>
</li></ul>

<p>At least one data element is required in the evaluation watchlist for early stopping to work.
</p>
<p>Callback function expects the following values to be set in its calling frame:
<code>stop_condition</code>,
<code>bst_evaluation</code>,
<code>rank</code>,
<code>bst</code> (or <code>bst_folds</code> and <code>basket</code>),
<code>iteration</code>,
<code>begin_iteration</code>,
<code>end_iteration</code>,
<code>num_parallel_tree</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+callbacks">callbacks</a></code>,
<code><a href="#topic+xgb.attr">xgb.attr</a></code>
</p>

<hr>
<h2 id='cb.evaluation.log'>Callback closure for logging the evaluation history</h2><span id='topic+cb.evaluation.log'></span>

<h3>Description</h3>

<p>Callback closure for logging the evaluation history
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cb.evaluation.log()
</code></pre>


<h3>Details</h3>

<p>This callback function appends the current iteration evaluation results <code>bst_evaluation</code>
available in the calling parent frame to the <code>evaluation_log</code> list in a calling frame.
</p>
<p>The finalizer callback (called with <code>finalize = TURE</code> in the end) converts
the <code>evaluation_log</code> list into a final data.table.
</p>
<p>The iteration evaluation result <code>bst_evaluation</code> must be a named numeric vector.
</p>
<p>Note: in the column names of the final data.table, the dash '-' character is replaced with
the underscore '_' in order to make the column names more like regular R identifiers.
</p>
<p>Callback function expects the following values to be set in its calling frame:
<code>evaluation_log</code>,
<code>bst_evaluation</code>,
<code>iteration</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+callbacks">callbacks</a></code>
</p>

<hr>
<h2 id='cb.gblinear.history'>Callback closure for collecting the model coefficients history of a gblinear booster
during its training.</h2><span id='topic+cb.gblinear.history'></span>

<h3>Description</h3>

<p>Callback closure for collecting the model coefficients history of a gblinear booster
during its training.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cb.gblinear.history(sparse = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cb.gblinear.history_+3A_sparse">sparse</code></td>
<td>
<p>when set to FALSE/TRUE, a dense/sparse matrix is used to store the result.
Sparse format is useful when one expects only a subset of coefficients to be non-zero,
when using the &quot;thrifty&quot; feature selector with fairly small number of top features
selected per iteration.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>To keep things fast and simple, gblinear booster does not internally store the history of linear
model coefficients at each boosting iteration. This callback provides a workaround for storing
the coefficients' path, by extracting them after each training iteration.
</p>
<p>Callback function expects the following values to be set in its calling frame:
<code>bst</code> (or <code>bst_folds</code>).
</p>


<h3>Value</h3>

<p>Results are stored in the <code>coefs</code> element of the closure.
The <code><a href="#topic+xgb.gblinear.history">xgb.gblinear.history</a></code> convenience function provides an easy
way to access it.
With <code>xgb.train</code>, it is either a dense of a sparse matrix.
While with <code>xgb.cv</code>, it is a list (an element per each fold) of such
matrices.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+callbacks">callbacks</a></code>, <code><a href="#topic+xgb.gblinear.history">xgb.gblinear.history</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#### Binary classification:

## Keep the number of threads to 1 for examples
nthread &lt;- 1
data.table::setDTthreads(nthread)

# In the iris dataset, it is hard to linearly separate Versicolor class from the rest
# without considering the 2nd order interactions:
x &lt;- model.matrix(Species ~ .^2, iris)[,-1]
colnames(x)
dtrain &lt;- xgb.DMatrix(scale(x), label = 1*(iris$Species == "versicolor"), nthread = nthread)
param &lt;- list(booster = "gblinear", objective = "reg:logistic", eval_metric = "auc",
              lambda = 0.0003, alpha = 0.0003, nthread = nthread)
# For 'shotgun', which is a default linear updater, using high eta values may result in
# unstable behaviour in some datasets. With this simple dataset, however, the high learning
# rate does not break the convergence, but allows us to illustrate the typical pattern of
# "stochastic explosion" behaviour of this lock-free algorithm at early boosting iterations.
bst &lt;- xgb.train(param, dtrain, list(tr=dtrain), nrounds = 200, eta = 1.,
                 callbacks = list(cb.gblinear.history()))
# Extract the coefficients' path and plot them vs boosting iteration number:
coef_path &lt;- xgb.gblinear.history(bst)
matplot(coef_path, type = 'l')

# With the deterministic coordinate descent updater, it is safer to use higher learning rates.
# Will try the classical componentwise boosting which selects a single best feature per round:
bst &lt;- xgb.train(param, dtrain, list(tr=dtrain), nrounds = 200, eta = 0.8,
                 updater = 'coord_descent', feature_selector = 'thrifty', top_k = 1,
                 callbacks = list(cb.gblinear.history()))
matplot(xgb.gblinear.history(bst), type = 'l')
#  Componentwise boosting is known to have similar effect to Lasso regularization.
# Try experimenting with various values of top_k, eta, nrounds,
# as well as different feature_selectors.

# For xgb.cv:
bst &lt;- xgb.cv(param, dtrain, nfold = 5, nrounds = 100, eta = 0.8,
              callbacks = list(cb.gblinear.history()))
# coefficients in the CV fold #3
matplot(xgb.gblinear.history(bst)[[3]], type = 'l')


#### Multiclass classification:
#
dtrain &lt;- xgb.DMatrix(scale(x), label = as.numeric(iris$Species) - 1, nthread = nthread)
param &lt;- list(booster = "gblinear", objective = "multi:softprob", num_class = 3,
              lambda = 0.0003, alpha = 0.0003, nthread = nthread)
# For the default linear updater 'shotgun' it sometimes is helpful
# to use smaller eta to reduce instability
bst &lt;- xgb.train(param, dtrain, list(tr=dtrain), nrounds = 70, eta = 0.5,
                 callbacks = list(cb.gblinear.history()))
# Will plot the coefficient paths separately for each class:
matplot(xgb.gblinear.history(bst, class_index = 0), type = 'l')
matplot(xgb.gblinear.history(bst, class_index = 1), type = 'l')
matplot(xgb.gblinear.history(bst, class_index = 2), type = 'l')

# CV:
bst &lt;- xgb.cv(param, dtrain, nfold = 5, nrounds = 70, eta = 0.5,
              callbacks = list(cb.gblinear.history(FALSE)))
# 1st fold of 1st class
matplot(xgb.gblinear.history(bst, class_index = 0)[[1]], type = 'l')

</code></pre>

<hr>
<h2 id='cb.print.evaluation'>Callback closure for printing the result of evaluation</h2><span id='topic+cb.print.evaluation'></span>

<h3>Description</h3>

<p>Callback closure for printing the result of evaluation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cb.print.evaluation(period = 1, showsd = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cb.print.evaluation_+3A_period">period</code></td>
<td>
<p>results would be printed every number of periods</p>
</td></tr>
<tr><td><code id="cb.print.evaluation_+3A_showsd">showsd</code></td>
<td>
<p>whether standard deviations should be printed (when available)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The callback function prints the result of evaluation at every <code>period</code> iterations.
The initial and the last iteration's evaluations are always printed.
</p>
<p>Callback function expects the following values to be set in its calling frame:
<code>bst_evaluation</code> (also <code>bst_evaluation_err</code> when available),
<code>iteration</code>,
<code>begin_iteration</code>,
<code>end_iteration</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+callbacks">callbacks</a></code>
</p>

<hr>
<h2 id='cb.reset.parameters'>Callback closure for resetting the booster's parameters at each iteration.</h2><span id='topic+cb.reset.parameters'></span>

<h3>Description</h3>

<p>Callback closure for resetting the booster's parameters at each iteration.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cb.reset.parameters(new_params)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cb.reset.parameters_+3A_new_params">new_params</code></td>
<td>
<p>a list where each element corresponds to a parameter that needs to be reset.
Each element's value must be either a vector of values of length <code>nrounds</code>
to be set at each iteration,
or a function of two parameters <code>learning_rates(iteration, nrounds)</code>
which returns a new parameter value by using the current iteration number
and the total number of boosting rounds.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a &quot;pre-iteration&quot; callback function used to reset booster's parameters
at the beginning of each iteration.
</p>
<p>Note that when training is resumed from some previous model, and a function is used to
reset a parameter value, the <code>nrounds</code> argument in this function would be the
the number of boosting rounds in the current training.
</p>
<p>Callback function expects the following values to be set in its calling frame:
<code>bst</code> or <code>bst_folds</code>,
<code>iteration</code>,
<code>begin_iteration</code>,
<code>end_iteration</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+callbacks">callbacks</a></code>
</p>

<hr>
<h2 id='cb.save.model'>Callback closure for saving a model file.</h2><span id='topic+cb.save.model'></span>

<h3>Description</h3>

<p>Callback closure for saving a model file.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cb.save.model(save_period = 0, save_name = "xgboost.model")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cb.save.model_+3A_save_period">save_period</code></td>
<td>
<p>save the model to disk after every
<code>save_period</code> iterations; 0 means save the model at the end.</p>
</td></tr>
<tr><td><code id="cb.save.model_+3A_save_name">save_name</code></td>
<td>
<p>the name or path for the saved model file.
It can contain a <code><a href="base.html#topic+sprintf">sprintf</a></code> formatting specifier
to include the integer iteration number in the file name.
E.g., with <code>save_name</code> = 'xgboost_
the file saved at iteration 50 would be named &quot;xgboost_0050.model&quot;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This callback function allows to save an xgb-model file, either periodically after each <code>save_period</code>'s or at the end.
</p>
<p>Callback function expects the following values to be set in its calling frame:
<code>bst</code>,
<code>iteration</code>,
<code>begin_iteration</code>,
<code>end_iteration</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+callbacks">callbacks</a></code>
</p>

<hr>
<h2 id='dim.xgb.DMatrix'>Dimensions of xgb.DMatrix</h2><span id='topic+dim.xgb.DMatrix'></span>

<h3>Description</h3>

<p>Returns a vector of numbers of rows and of columns in an <code>xgb.DMatrix</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'xgb.DMatrix'
dim(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="dim.xgb.DMatrix_+3A_x">x</code></td>
<td>
<p>Object of class <code>xgb.DMatrix</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note: since <code>nrow</code> and <code>ncol</code> internally use <code>dim</code>, they can also
be directly used with an <code>xgb.DMatrix</code> object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(agaricus.train, package='xgboost')
train &lt;- agaricus.train
dtrain &lt;- xgb.DMatrix(train$data, label=train$label, nthread = 2)

stopifnot(nrow(dtrain) == nrow(train$data))
stopifnot(ncol(dtrain) == ncol(train$data))
stopifnot(all(dim(dtrain) == dim(train$data)))

</code></pre>

<hr>
<h2 id='dimnames.xgb.DMatrix'>Handling of column names of <code>xgb.DMatrix</code></h2><span id='topic+dimnames.xgb.DMatrix'></span><span id='topic+dimnames+3C-.xgb.DMatrix'></span>

<h3>Description</h3>

<p>Only column names are supported for <code>xgb.DMatrix</code>, thus setting of
row names would have no effect and returned row names would be NULL.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'xgb.DMatrix'
dimnames(x)

## S3 replacement method for class 'xgb.DMatrix'
dimnames(x) &lt;- value
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="dimnames.xgb.DMatrix_+3A_x">x</code></td>
<td>
<p>object of class <code>xgb.DMatrix</code></p>
</td></tr>
<tr><td><code id="dimnames.xgb.DMatrix_+3A_value">value</code></td>
<td>
<p>a list of two elements: the first one is ignored
and the second one is column names</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Generic <code>dimnames</code> methods are used by <code>colnames</code>.
Since row names are irrelevant, it is recommended to use <code>colnames</code> directly.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(agaricus.train, package='xgboost')
train &lt;- agaricus.train
dtrain &lt;- xgb.DMatrix(train$data, label=train$label, nthread = 2)
dimnames(dtrain)
colnames(dtrain)
colnames(dtrain) &lt;- make.names(1:ncol(train$data))
print(dtrain, verbose=TRUE)

</code></pre>

<hr>
<h2 id='getinfo'>Get information of an xgb.DMatrix object</h2><span id='topic+getinfo'></span><span id='topic+getinfo.xgb.DMatrix'></span>

<h3>Description</h3>

<p>Get information of an xgb.DMatrix object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getinfo(object, ...)

## S3 method for class 'xgb.DMatrix'
getinfo(object, name, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="getinfo_+3A_object">object</code></td>
<td>
<p>Object of class <code>xgb.DMatrix</code></p>
</td></tr>
<tr><td><code id="getinfo_+3A_...">...</code></td>
<td>
<p>other parameters</p>
</td></tr>
<tr><td><code id="getinfo_+3A_name">name</code></td>
<td>
<p>the name of the information field to get (see details)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>name</code> field can be one of the following:
</p>

<ul>
<li> <p><code>label</code>: label XGBoost learn from ;
</p>
</li>
<li> <p><code>weight</code>: to do a weight rescale ;
</p>
</li>
<li> <p><code>base_margin</code>: base margin is the base prediction XGBoost will boost from ;
</p>
</li>
<li> <p><code>nrow</code>: number of rows of the <code>xgb.DMatrix</code>.
</p>
</li></ul>

<p><code>group</code> can be setup by <code>setinfo</code> but can't be retrieved by <code>getinfo</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(agaricus.train, package='xgboost')
dtrain &lt;- with(agaricus.train, xgb.DMatrix(data, label = label, nthread = 2))

labels &lt;- getinfo(dtrain, 'label')
setinfo(dtrain, 'label', 1-labels)

labels2 &lt;- getinfo(dtrain, 'label')
stopifnot(all(labels2 == 1-labels))
</code></pre>

<hr>
<h2 id='normalize'>Scale feature value to have mean 0, standard deviation 1</h2><span id='topic+normalize'></span>

<h3>Description</h3>

<p>This is used to compare multiple features on the same plot.
Internal utility function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>normalize(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="normalize_+3A_x">x</code></td>
<td>
<p>Numeric vector</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Numeric vector with mean 0 and sd 1.
</p>

<hr>
<h2 id='predict.xgb.Booster'>Predict method for eXtreme Gradient Boosting model</h2><span id='topic+predict.xgb.Booster'></span><span id='topic+predict.xgb.Booster.handle'></span>

<h3>Description</h3>

<p>Predicted values based on either xgboost model or model handle object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'xgb.Booster'
predict(
  object,
  newdata,
  missing = NA,
  outputmargin = FALSE,
  ntreelimit = NULL,
  predleaf = FALSE,
  predcontrib = FALSE,
  approxcontrib = FALSE,
  predinteraction = FALSE,
  reshape = FALSE,
  training = FALSE,
  iterationrange = NULL,
  strict_shape = FALSE,
  ...
)

## S3 method for class 'xgb.Booster.handle'
predict(object, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.xgb.Booster_+3A_object">object</code></td>
<td>
<p>Object of class <code>xgb.Booster</code> or <code>xgb.Booster.handle</code></p>
</td></tr>
<tr><td><code id="predict.xgb.Booster_+3A_newdata">newdata</code></td>
<td>
<p>takes <code>matrix</code>, <code>dgCMatrix</code>, <code>dgRMatrix</code>, <code>dsparseVector</code>,
local data file or <code>xgb.DMatrix</code>.
</p>
<p>For single-row predictions on sparse data, it's recommended to use CSR format. If passing
a sparse vector, it will take it as a row vector.</p>
</td></tr>
<tr><td><code id="predict.xgb.Booster_+3A_missing">missing</code></td>
<td>
<p>Missing is only used when input is dense matrix. Pick a float value that represents
missing values in data (e.g., sometimes 0 or some other extreme value is used).</p>
</td></tr>
<tr><td><code id="predict.xgb.Booster_+3A_outputmargin">outputmargin</code></td>
<td>
<p>whether the prediction should be returned in the for of original untransformed
sum of predictions from boosting iterations' results. E.g., setting <code>outputmargin=TRUE</code> for
logistic regression would result in predictions for log-odds instead of probabilities.</p>
</td></tr>
<tr><td><code id="predict.xgb.Booster_+3A_ntreelimit">ntreelimit</code></td>
<td>
<p>Deprecated, use <code>iterationrange</code> instead.</p>
</td></tr>
<tr><td><code id="predict.xgb.Booster_+3A_predleaf">predleaf</code></td>
<td>
<p>whether predict leaf index.</p>
</td></tr>
<tr><td><code id="predict.xgb.Booster_+3A_predcontrib">predcontrib</code></td>
<td>
<p>whether to return feature contributions to individual predictions (see Details).</p>
</td></tr>
<tr><td><code id="predict.xgb.Booster_+3A_approxcontrib">approxcontrib</code></td>
<td>
<p>whether to use a fast approximation for feature contributions (see Details).</p>
</td></tr>
<tr><td><code id="predict.xgb.Booster_+3A_predinteraction">predinteraction</code></td>
<td>
<p>whether to return contributions of feature interactions to individual predictions (see Details).</p>
</td></tr>
<tr><td><code id="predict.xgb.Booster_+3A_reshape">reshape</code></td>
<td>
<p>whether to reshape the vector of predictions to a matrix form when there are several
prediction outputs per case. This option has no effect when either of predleaf, predcontrib,
or predinteraction flags is TRUE.</p>
</td></tr>
<tr><td><code id="predict.xgb.Booster_+3A_training">training</code></td>
<td>
<p>whether is the prediction result used for training.  For dart booster,
training predicting will perform dropout.</p>
</td></tr>
<tr><td><code id="predict.xgb.Booster_+3A_iterationrange">iterationrange</code></td>
<td>
<p>Specifies which layer of trees are used in prediction.  For
example, if a random forest is trained with 100 rounds.  Specifying
'iterationrange=(1, 21)', then only the forests built during [1, 21) (half open set)
rounds are used in this prediction.  It's 1-based index just like R vector.  When set
to <code>c(1, 1)</code> XGBoost will use all trees.</p>
</td></tr>
<tr><td><code id="predict.xgb.Booster_+3A_strict_shape">strict_shape</code></td>
<td>
<p>Default is <code>FALSE</code>. When it's set to <code>TRUE</code>, output
type and shape of prediction are invariant to model type.</p>
</td></tr>
<tr><td><code id="predict.xgb.Booster_+3A_...">...</code></td>
<td>
<p>Parameters passed to <code>predict.xgb.Booster</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that <code>iterationrange</code> would currently do nothing for predictions from gblinear,
since gblinear doesn't keep its boosting history.
</p>
<p>One possible practical applications of the <code>predleaf</code> option is to use the model
as a generator of new features which capture non-linearity and interactions,
e.g., as implemented in <code><a href="#topic+xgb.create.features">xgb.create.features</a></code>.
</p>
<p>Setting <code>predcontrib = TRUE</code> allows to calculate contributions of each feature to
individual predictions. For &quot;gblinear&quot; booster, feature contributions are simply linear terms
(feature_beta * feature_value). For &quot;gbtree&quot; booster, feature contributions are SHAP
values (Lundberg 2017) that sum to the difference between the expected output
of the model and the current prediction (where the hessian weights are used to compute the expectations).
Setting <code>approxcontrib = TRUE</code> approximates these values following the idea explained
in <a href="http://blog.datadive.net/interpreting-random-forests/">http://blog.datadive.net/interpreting-random-forests/</a>.
</p>
<p>With <code>predinteraction = TRUE</code>, SHAP values of contributions of interaction of each pair of features
are computed. Note that this operation might be rather expensive in terms of compute and memory.
Since it quadratically depends on the number of features, it is recommended to perform selection
of the most important features first. See below about the format of the returned results.
</p>


<h3>Value</h3>

<p>The return type is different depending whether <code>strict_shape</code> is set to <code>TRUE</code>.  By default,
for regression or binary classification, it returns a vector of length <code>nrows(newdata)</code>.
For multiclass classification, either a <code>num_class * nrows(newdata)</code> vector or
a <code>(nrows(newdata), num_class)</code> dimension matrix is returned, depending on
the <code>reshape</code> value.
</p>
<p>When <code>predleaf = TRUE</code>, the output is a matrix object with the
number of columns corresponding to the number of trees.
</p>
<p>When <code>predcontrib = TRUE</code> and it is not a multiclass setting, the output is a matrix object with
<code>num_features + 1</code> columns. The last &quot;+ 1&quot; column in a matrix corresponds to bias.
For a multiclass case, a list of <code>num_class</code> elements is returned, where each element is
such a matrix. The contribution values are on the scale of untransformed margin
(e.g., for binary classification would mean that the contributions are log-odds deviations from bias).
</p>
<p>When <code>predinteraction = TRUE</code> and it is not a multiclass setting, the output is a 3d array with
dimensions <code>c(nrow, num_features + 1, num_features + 1)</code>. The off-diagonal (in the last two dimensions)
elements represent different features interaction contributions. The array is symmetric WRT the last
two dimensions. The &quot;+ 1&quot; columns corresponds to bias. Summing this array along the last dimension should
produce practically the same result as predict with <code>predcontrib = TRUE</code>.
For a multiclass case, a list of <code>num_class</code> elements is returned, where each element is
such an array.
</p>
<p>When <code>strict_shape</code> is set to <code>TRUE</code>, the output is always an array.  For
normal prediction, the output is a 2-dimension array <code>(num_class, nrow(newdata))</code>.
</p>
<p>For <code>predcontrib = TRUE</code>, output is <code>(ncol(newdata) + 1, num_class, nrow(newdata))</code>
For <code>predinteraction = TRUE</code>, output is <code>(ncol(newdata) + 1, ncol(newdata) + 1, num_class, nrow(newdata))</code>
For <code>predleaf = TRUE</code>, output is <code>(n_trees_in_forest, num_class, n_iterations, nrow(newdata))</code>
</p>


<h3>References</h3>

<p>Scott M. Lundberg, Su-In Lee, &quot;A Unified Approach to Interpreting Model Predictions&quot;, NIPS Proceedings 2017, <a href="https://arxiv.org/abs/1705.07874">https://arxiv.org/abs/1705.07874</a>
</p>
<p>Scott M. Lundberg, Su-In Lee, &quot;Consistent feature attribution for tree ensembles&quot;, <a href="https://arxiv.org/abs/1706.06060">https://arxiv.org/abs/1706.06060</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+xgb.train">xgb.train</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## binary classification:

data(agaricus.train, package='xgboost')
data(agaricus.test, package='xgboost')

## Keep the number of threads to 2 for examples
nthread &lt;- 2
data.table::setDTthreads(nthread)

train &lt;- agaricus.train
test &lt;- agaricus.test

bst &lt;- xgboost(data = train$data, label = train$label, max_depth = 2,
               eta = 0.5, nthread = nthread, nrounds = 5, objective = "binary:logistic")
# use all trees by default
pred &lt;- predict(bst, test$data)
# use only the 1st tree
pred1 &lt;- predict(bst, test$data, iterationrange = c(1, 2))

# Predicting tree leafs:
# the result is an nsamples X ntrees matrix
pred_leaf &lt;- predict(bst, test$data, predleaf = TRUE)
str(pred_leaf)

# Predicting feature contributions to predictions:
# the result is an nsamples X (nfeatures + 1) matrix
pred_contr &lt;- predict(bst, test$data, predcontrib = TRUE)
str(pred_contr)
# verify that contributions' sums are equal to log-odds of predictions (up to float precision):
summary(rowSums(pred_contr) - qlogis(pred))
# for the 1st record, let's inspect its features that had non-zero contribution to prediction:
contr1 &lt;- pred_contr[1,]
contr1 &lt;- contr1[-length(contr1)]    # drop BIAS
contr1 &lt;- contr1[contr1 != 0]        # drop non-contributing features
contr1 &lt;- contr1[order(abs(contr1))] # order by contribution magnitude
old_mar &lt;- par("mar")
par(mar = old_mar + c(0,7,0,0))
barplot(contr1, horiz = TRUE, las = 2, xlab = "contribution to prediction in log-odds")
par(mar = old_mar)


## multiclass classification in iris dataset:

lb &lt;- as.numeric(iris$Species) - 1
num_class &lt;- 3
set.seed(11)
bst &lt;- xgboost(data = as.matrix(iris[, -5]), label = lb,
               max_depth = 4, eta = 0.5, nthread = 2, nrounds = 10, subsample = 0.5,
               objective = "multi:softprob", num_class = num_class)
# predict for softmax returns num_class probability numbers per case:
pred &lt;- predict(bst, as.matrix(iris[, -5]))
str(pred)
# reshape it to a num_class-columns matrix
pred &lt;- matrix(pred, ncol=num_class, byrow=TRUE)
# convert the probabilities to softmax labels
pred_labels &lt;- max.col(pred) - 1
# the following should result in the same error as seen in the last iteration
sum(pred_labels != lb)/length(lb)

# compare that to the predictions from softmax:
set.seed(11)
bst &lt;- xgboost(data = as.matrix(iris[, -5]), label = lb,
               max_depth = 4, eta = 0.5, nthread = 2, nrounds = 10, subsample = 0.5,
               objective = "multi:softmax", num_class = num_class)
pred &lt;- predict(bst, as.matrix(iris[, -5]))
str(pred)
all.equal(pred, pred_labels)
# prediction from using only 5 iterations should result
# in the same error as seen in iteration 5:
pred5 &lt;- predict(bst, as.matrix(iris[, -5]), iterationrange=c(1, 6))
sum(pred5 != lb)/length(lb)

</code></pre>

<hr>
<h2 id='prepare.ggplot.shap.data'>Combine and melt feature values and SHAP contributions for sample
observations.</h2><span id='topic+prepare.ggplot.shap.data'></span>

<h3>Description</h3>

<p>Conforms to data format required for ggplot functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prepare.ggplot.shap.data(data_list, normalize = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="prepare.ggplot.shap.data_+3A_data_list">data_list</code></td>
<td>
<p>List containing 'data' and 'shap_contrib' returned by
<code>xgb.shap.data()</code>.</p>
</td></tr>
<tr><td><code id="prepare.ggplot.shap.data_+3A_normalize">normalize</code></td>
<td>
<p>Whether to standardize feature values to have mean 0 and
standard deviation 1 (useful for comparing multiple features on the same
plot). Default <code>FALSE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Internal utility function.
</p>


<h3>Value</h3>

<p>A data.table containing the observation ID, the feature name, the
feature value (normalized if specified), and the SHAP contribution value.
</p>

<hr>
<h2 id='print.xgb.Booster'>Print xgb.Booster</h2><span id='topic+print.xgb.Booster'></span>

<h3>Description</h3>

<p>Print information about xgb.Booster.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'xgb.Booster'
print(x, verbose = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.xgb.Booster_+3A_x">x</code></td>
<td>
<p>an xgb.Booster object</p>
</td></tr>
<tr><td><code id="print.xgb.Booster_+3A_verbose">verbose</code></td>
<td>
<p>whether to print detailed data (e.g., attribute values)</p>
</td></tr>
<tr><td><code id="print.xgb.Booster_+3A_...">...</code></td>
<td>
<p>not currently used</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>data(agaricus.train, package='xgboost')
train &lt;- agaricus.train
bst &lt;- xgboost(data = train$data, label = train$label, max_depth = 2,
               eta = 1, nthread = 2, nrounds = 2, objective = "binary:logistic")
attr(bst, 'myattr') &lt;- 'memo'

print(bst)
print(bst, verbose=TRUE)

</code></pre>

<hr>
<h2 id='print.xgb.cv.synchronous'>Print xgb.cv result</h2><span id='topic+print.xgb.cv.synchronous'></span>

<h3>Description</h3>

<p>Prints formatted results of <code>xgb.cv</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'xgb.cv.synchronous'
print(x, verbose = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.xgb.cv.synchronous_+3A_x">x</code></td>
<td>
<p>an <code>xgb.cv.synchronous</code> object</p>
</td></tr>
<tr><td><code id="print.xgb.cv.synchronous_+3A_verbose">verbose</code></td>
<td>
<p>whether to print detailed data</p>
</td></tr>
<tr><td><code id="print.xgb.cv.synchronous_+3A_...">...</code></td>
<td>
<p>passed to <code>data.table.print</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>When not verbose, it would only print the evaluation results,
including the best iteration (when available).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(agaricus.train, package='xgboost')
train &lt;- agaricus.train
cv &lt;- xgb.cv(data = train$data, label = train$label, nfold = 5, max_depth = 2,
               eta = 1, nthread = 2, nrounds = 2, objective = "binary:logistic")
print(cv)
print(cv, verbose=TRUE)

</code></pre>

<hr>
<h2 id='print.xgb.DMatrix'>Print xgb.DMatrix</h2><span id='topic+print.xgb.DMatrix'></span>

<h3>Description</h3>

<p>Print information about xgb.DMatrix.
Currently it displays dimensions and presence of info-fields and colnames.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'xgb.DMatrix'
print(x, verbose = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.xgb.DMatrix_+3A_x">x</code></td>
<td>
<p>an xgb.DMatrix object</p>
</td></tr>
<tr><td><code id="print.xgb.DMatrix_+3A_verbose">verbose</code></td>
<td>
<p>whether to print colnames (when present)</p>
</td></tr>
<tr><td><code id="print.xgb.DMatrix_+3A_...">...</code></td>
<td>
<p>not currently used</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>data(agaricus.train, package='xgboost')
dtrain &lt;- with(agaricus.train, xgb.DMatrix(data, label = label, nthread = 2))

dtrain
print(dtrain, verbose=TRUE)

</code></pre>

<hr>
<h2 id='setinfo'>Set information of an xgb.DMatrix object</h2><span id='topic+setinfo'></span><span id='topic+setinfo.xgb.DMatrix'></span>

<h3>Description</h3>

<p>Set information of an xgb.DMatrix object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>setinfo(object, ...)

## S3 method for class 'xgb.DMatrix'
setinfo(object, name, info, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="setinfo_+3A_object">object</code></td>
<td>
<p>Object of class &quot;xgb.DMatrix&quot;</p>
</td></tr>
<tr><td><code id="setinfo_+3A_...">...</code></td>
<td>
<p>other parameters</p>
</td></tr>
<tr><td><code id="setinfo_+3A_name">name</code></td>
<td>
<p>the name of the field to get</p>
</td></tr>
<tr><td><code id="setinfo_+3A_info">info</code></td>
<td>
<p>the specific field of information to set</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>name</code> field can be one of the following:
</p>

<ul>
<li> <p><code>label</code>: label XGBoost learn from ;
</p>
</li>
<li> <p><code>weight</code>: to do a weight rescale ;
</p>
</li>
<li> <p><code>base_margin</code>: base margin is the base prediction XGBoost will boost from ;
</p>
</li>
<li> <p><code>group</code>: number of rows in each group (to use with <code>rank:pairwise</code> objective).
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>data(agaricus.train, package='xgboost')
dtrain &lt;- with(agaricus.train, xgb.DMatrix(data, label = label, nthread = 2))

labels &lt;- getinfo(dtrain, 'label')
setinfo(dtrain, 'label', 1-labels)
labels2 &lt;- getinfo(dtrain, 'label')
stopifnot(all.equal(labels2, 1-labels))
</code></pre>

<hr>
<h2 id='slice'>Get a new DMatrix containing the specified rows of
original xgb.DMatrix object</h2><span id='topic+slice'></span><span id='topic+slice.xgb.DMatrix'></span><span id='topic++5B.xgb.DMatrix'></span>

<h3>Description</h3>

<p>Get a new DMatrix containing the specified rows of
original xgb.DMatrix object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>slice(object, ...)

## S3 method for class 'xgb.DMatrix'
slice(object, idxset, ...)

## S3 method for class 'xgb.DMatrix'
object[idxset, colset = NULL]
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="slice_+3A_object">object</code></td>
<td>
<p>Object of class &quot;xgb.DMatrix&quot;</p>
</td></tr>
<tr><td><code id="slice_+3A_...">...</code></td>
<td>
<p>other parameters (currently not used)</p>
</td></tr>
<tr><td><code id="slice_+3A_idxset">idxset</code></td>
<td>
<p>a integer vector of indices of rows needed</p>
</td></tr>
<tr><td><code id="slice_+3A_colset">colset</code></td>
<td>
<p>currently not used (columns subsetting is not available)</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>data(agaricus.train, package='xgboost')
dtrain &lt;- with(agaricus.train, xgb.DMatrix(data, label = label, nthread = 2))

dsub &lt;- slice(dtrain, 1:42)
labels1 &lt;- getinfo(dsub, 'label')
dsub &lt;- dtrain[1:42, ]
labels2 &lt;- getinfo(dsub, 'label')
all.equal(labels1, labels2)

</code></pre>

<hr>
<h2 id='xgb.attr'>Accessors for serializable attributes of a model.</h2><span id='topic+xgb.attr'></span><span id='topic+xgb.attr+3C-'></span><span id='topic+xgb.attributes'></span><span id='topic+xgb.attributes+3C-'></span>

<h3>Description</h3>

<p>These methods allow to manipulate the key-value attribute strings of an xgboost model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>xgb.attr(object, name)

xgb.attr(object, name) &lt;- value

xgb.attributes(object)

xgb.attributes(object) &lt;- value
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="xgb.attr_+3A_object">object</code></td>
<td>
<p>Object of class <code>xgb.Booster</code> or <code>xgb.Booster.handle</code>.</p>
</td></tr>
<tr><td><code id="xgb.attr_+3A_name">name</code></td>
<td>
<p>a non-empty character string specifying which attribute is to be accessed.</p>
</td></tr>
<tr><td><code id="xgb.attr_+3A_value">value</code></td>
<td>
<p>a value of an attribute for <code>xgb.attr&lt;-</code>; for <code>xgb.attributes&lt;-</code>
it's a list (or an object coercible to a list) with the names of attributes to set
and the elements corresponding to attribute values.
Non-character values are converted to character.
When attribute value is not a scalar, only the first index is used.
Use <code>NULL</code> to remove an attribute.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The primary purpose of xgboost model attributes is to store some meta-data about the model.
Note that they are a separate concept from the object attributes in R.
Specifically, they refer to key-value strings that can be attached to an xgboost model,
stored together with the model's binary representation, and accessed later
(from R or any other interface).
In contrast, any R-attribute assigned to an R-object of <code>xgb.Booster</code> class
would not be saved by <code>xgb.save</code> because an xgboost model is an external memory object
and its serialization is handled externally.
Also, setting an attribute that has the same name as one of xgboost's parameters wouldn't
change the value of that parameter for a model.
Use <code><a href="#topic+xgb.parameters+3C-">xgb.parameters&lt;-</a></code> to set or change model parameters.
</p>
<p>The attribute setters would usually work more efficiently for <code>xgb.Booster.handle</code>
than for <code>xgb.Booster</code>, since only just a handle (pointer) would need to be copied.
That would only matter if attributes need to be set many times.
Note, however, that when feeding a handle of an <code>xgb.Booster</code> object to the attribute setters,
the raw model cache of an <code>xgb.Booster</code> object would not be automatically updated,
and it would be user's responsibility to call <code>xgb.serialize</code> to update it.
</p>
<p>The <code>xgb.attributes&lt;-</code> setter either updates the existing or adds one or several attributes,
but it doesn't delete the other existing attributes.
</p>


<h3>Value</h3>

<p><code>xgb.attr</code> returns either a string value of an attribute
or <code>NULL</code> if an attribute wasn't stored in a model.
</p>
<p><code>xgb.attributes</code> returns a list of all attribute stored in a model
or <code>NULL</code> if a model has no stored attributes.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(agaricus.train, package='xgboost')
train &lt;- agaricus.train

bst &lt;- xgboost(data = train$data, label = train$label, max_depth = 2,
               eta = 1, nthread = 2, nrounds = 2, objective = "binary:logistic")

xgb.attr(bst, "my_attribute") &lt;- "my attribute value"
print(xgb.attr(bst, "my_attribute"))
xgb.attributes(bst) &lt;- list(a = 123, b = "abc")

xgb.save(bst, 'xgb.model')
bst1 &lt;- xgb.load('xgb.model')
if (file.exists('xgb.model')) file.remove('xgb.model')
print(xgb.attr(bst1, "my_attribute"))
print(xgb.attributes(bst1))

# deletion:
xgb.attr(bst1, "my_attribute") &lt;- NULL
print(xgb.attributes(bst1))
xgb.attributes(bst1) &lt;- list(a = NULL, b = NULL)
print(xgb.attributes(bst1))

</code></pre>

<hr>
<h2 id='xgb.Booster.complete'>Restore missing parts of an incomplete xgb.Booster object.</h2><span id='topic+xgb.Booster.complete'></span>

<h3>Description</h3>

<p>It attempts to complete an <code>xgb.Booster</code> object by restoring either its missing
raw model memory dump (when it has no <code>raw</code> data but its <code>xgb.Booster.handle</code> is valid)
or its missing internal handle (when its <code>xgb.Booster.handle</code> is not valid
but it has a raw Booster memory dump).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>xgb.Booster.complete(object, saveraw = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="xgb.Booster.complete_+3A_object">object</code></td>
<td>
<p>object of class <code>xgb.Booster</code></p>
</td></tr>
<tr><td><code id="xgb.Booster.complete_+3A_saveraw">saveraw</code></td>
<td>
<p>a flag indicating whether to append <code>raw</code> Booster memory dump data
when it doesn't already exist.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>While this method is primarily for internal use, it might be useful in some practical situations.
</p>
<p>E.g., when an <code>xgb.Booster</code> model is saved as an R object and then is loaded as an R object,
its handle (pointer) to an internal xgboost model would be invalid. The majority of xgboost methods
should still work for such a model object since those methods would be using
<code>xgb.Booster.complete</code> internally. However, one might find it to be more efficient to call the
<code>xgb.Booster.complete</code> function explicitly once after loading a model as an R-object.
That would prevent further repeated implicit reconstruction of an internal booster model.
</p>


<h3>Value</h3>

<p>An object of <code>xgb.Booster</code> class.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(agaricus.train, package='xgboost')
bst &lt;- xgboost(data = agaricus.train$data, label = agaricus.train$label, max_depth = 2,
               eta = 1, nthread = 2, nrounds = 2, objective = "binary:logistic")
saveRDS(bst, "xgb.model.rds")

# Warning: The resulting RDS file is only compatible with the current XGBoost version.
# Refer to the section titled "a-compatibility-note-for-saveRDS-save".
bst1 &lt;- readRDS("xgb.model.rds")
if (file.exists("xgb.model.rds")) file.remove("xgb.model.rds")
# the handle is invalid:
print(bst1$handle)

bst1 &lt;- xgb.Booster.complete(bst1)
# now the handle points to a valid internal booster model:
print(bst1$handle)

</code></pre>

<hr>
<h2 id='xgb.config'>Accessors for model parameters as JSON string.</h2><span id='topic+xgb.config'></span><span id='topic+xgb.config+3C-'></span>

<h3>Description</h3>

<p>Accessors for model parameters as JSON string.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>xgb.config(object)

xgb.config(object) &lt;- value
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="xgb.config_+3A_object">object</code></td>
<td>
<p>Object of class <code>xgb.Booster</code></p>
</td></tr>
<tr><td><code id="xgb.config_+3A_value">value</code></td>
<td>
<p>A JSON string.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>data(agaricus.train, package='xgboost')
## Keep the number of threads to 1 for examples
nthread &lt;- 1
data.table::setDTthreads(nthread)
train &lt;- agaricus.train

bst &lt;- xgboost(
  data = train$data, label = train$label, max_depth = 2,
  eta = 1, nthread = nthread, nrounds = 2, objective = "binary:logistic"
)
config &lt;- xgb.config(bst)

</code></pre>

<hr>
<h2 id='xgb.create.features'>Create new features from a previously learned model</h2><span id='topic+xgb.create.features'></span>

<h3>Description</h3>

<p>May improve the learning by adding new features to the training data based on the decision trees from a previously learned model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>xgb.create.features(model, data, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="xgb.create.features_+3A_model">model</code></td>
<td>
<p>decision tree boosting model learned on the original data</p>
</td></tr>
<tr><td><code id="xgb.create.features_+3A_data">data</code></td>
<td>
<p>original data (usually provided as a <code>dgCMatrix</code> matrix)</p>
</td></tr>
<tr><td><code id="xgb.create.features_+3A_...">...</code></td>
<td>
<p>currently not used</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is the function inspired from the paragraph 3.1 of the paper:
</p>
<p><strong>Practical Lessons from Predicting Clicks on Ads at Facebook</strong>
</p>
<p><em>(Xinran He, Junfeng Pan, Ou Jin, Tianbing Xu, Bo Liu, Tao Xu, Yan, xin Shi, Antoine Atallah, Ralf Herbrich, Stuart Bowers,
Joaquin Quinonero Candela)</em>
</p>
<p>International Workshop on Data Mining for Online Advertising (ADKDD) - August 24, 2014
</p>
<p><a href="https://research.facebook.com/publications/practical-lessons-from-predicting-clicks-on-ads-at-facebook/">https://research.facebook.com/publications/practical-lessons-from-predicting-clicks-on-ads-at-facebook/</a>.
</p>
<p>Extract explaining the method:
</p>
<p>&quot;We found that boosted decision trees are a powerful and very
convenient way to implement non-linear and tuple transformations
of the kind we just described. We treat each individual
tree as a categorical feature that takes as value the
index of the leaf an instance ends up falling in. We use
1-of-K coding of this type of features.
</p>
<p>For example, consider the boosted tree model in Figure 1 with 2 subtrees,
where the first subtree has 3 leafs and the second 2 leafs. If an
instance ends up in leaf 2 in the first subtree and leaf 1 in
second subtree, the overall input to the linear classifier will
be the binary vector <code>[0, 1, 0, 1, 0]</code>, where the first 3 entries
correspond to the leaves of the first subtree and last 2 to
those of the second subtree.
</p>
<p>[...]
</p>
<p>We can understand boosted decision tree
based transformation as a supervised feature encoding that
converts a real-valued vector into a compact binary-valued
vector. A traversal from root node to a leaf node represents
a rule on certain features.&quot;
</p>


<h3>Value</h3>

<p><code>dgCMatrix</code> matrix including both the original data and the new features.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(agaricus.train, package='xgboost')
data(agaricus.test, package='xgboost')
dtrain &lt;- with(agaricus.train, xgb.DMatrix(data, label = label, nthread = 2))
dtest &lt;- with(agaricus.test, xgb.DMatrix(data, label = label, nthread = 2))

param &lt;- list(max_depth=2, eta=1, silent=1, objective='binary:logistic')
nrounds = 4

bst = xgb.train(params = param, data = dtrain, nrounds = nrounds, nthread = 2)

# Model accuracy without new features
accuracy.before &lt;- sum((predict(bst, agaricus.test$data) &gt;= 0.5) == agaricus.test$label) /
                   length(agaricus.test$label)

# Convert previous features to one hot encoding
new.features.train &lt;- xgb.create.features(model = bst, agaricus.train$data)
new.features.test &lt;- xgb.create.features(model = bst, agaricus.test$data)

# learning with new features
new.dtrain &lt;- xgb.DMatrix(
  data = new.features.train, label = agaricus.train$label, nthread = 2
)
new.dtest &lt;- xgb.DMatrix(
  data = new.features.test, label = agaricus.test$label, nthread = 2
)
watchlist &lt;- list(train = new.dtrain)
bst &lt;- xgb.train(params = param, data = new.dtrain, nrounds = nrounds, nthread = 2)

# Model accuracy with new features
accuracy.after &lt;- sum((predict(bst, new.dtest) &gt;= 0.5) == agaricus.test$label) /
                  length(agaricus.test$label)

# Here the accuracy was already good and is now perfect.
cat(paste("The accuracy was", accuracy.before, "before adding leaf features and it is now",
          accuracy.after, "!\n"))

</code></pre>

<hr>
<h2 id='xgb.cv'>Cross Validation</h2><span id='topic+xgb.cv'></span>

<h3>Description</h3>

<p>The cross validation function of xgboost
</p>


<h3>Usage</h3>

<pre><code class='language-R'>xgb.cv(
  params = list(),
  data,
  nrounds,
  nfold,
  label = NULL,
  missing = NA,
  prediction = FALSE,
  showsd = TRUE,
  metrics = list(),
  obj = NULL,
  feval = NULL,
  stratified = TRUE,
  folds = NULL,
  train_folds = NULL,
  verbose = TRUE,
  print_every_n = 1L,
  early_stopping_rounds = NULL,
  maximize = NULL,
  callbacks = list(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="xgb.cv_+3A_params">params</code></td>
<td>
<p>the list of parameters. The complete list of parameters is
available in the <a href="http://xgboost.readthedocs.io/en/latest/parameter.html">online documentation</a>. Below
is a shorter summary:
</p>

<ul>
<li> <p><code>objective</code> objective function, common ones are
</p>

<ul>
<li> <p><code>reg:squarederror</code> Regression with squared loss.
</p>
</li>
<li> <p><code>binary:logistic</code> logistic regression for classification.
</p>
</li>
<li><p> See <code><a href="#topic+xgb.train">xgb.train</a>()</code> for complete list of objectives.
</p>
</li></ul>

</li>
<li> <p><code>eta</code> step size of each boosting step
</p>
</li>
<li> <p><code>max_depth</code> maximum depth of the tree
</p>
</li>
<li> <p><code>nthread</code> number of thread used in training, if not set, all threads are used
</p>
</li></ul>

<p>See <code><a href="#topic+xgb.train">xgb.train</a></code> for further details.
See also demo/ for walkthrough example in R.</p>
</td></tr>
<tr><td><code id="xgb.cv_+3A_data">data</code></td>
<td>
<p>takes an <code>xgb.DMatrix</code>, <code>matrix</code>, or <code>dgCMatrix</code> as the input.</p>
</td></tr>
<tr><td><code id="xgb.cv_+3A_nrounds">nrounds</code></td>
<td>
<p>the max number of iterations</p>
</td></tr>
<tr><td><code id="xgb.cv_+3A_nfold">nfold</code></td>
<td>
<p>the original dataset is randomly partitioned into <code>nfold</code> equal size subsamples.</p>
</td></tr>
<tr><td><code id="xgb.cv_+3A_label">label</code></td>
<td>
<p>vector of response values. Should be provided only when data is an R-matrix.</p>
</td></tr>
<tr><td><code id="xgb.cv_+3A_missing">missing</code></td>
<td>
<p>is only used when input is a dense matrix. By default is set to NA, which means
that NA values should be considered as 'missing' by the algorithm.
Sometimes, 0 or other extreme value might be used to represent missing values.</p>
</td></tr>
<tr><td><code id="xgb.cv_+3A_prediction">prediction</code></td>
<td>
<p>A logical value indicating whether to return the test fold predictions
from each CV model. This parameter engages the <code><a href="#topic+cb.cv.predict">cb.cv.predict</a></code> callback.</p>
</td></tr>
<tr><td><code id="xgb.cv_+3A_showsd">showsd</code></td>
<td>
<p><code>boolean</code>, whether to show standard deviation of cross validation</p>
</td></tr>
<tr><td><code id="xgb.cv_+3A_metrics">metrics</code></td>
<td>
<p>list of evaluation metrics to be used in cross validation,
when it is not specified, the evaluation metric is chosen according to objective function.
Possible options are:
</p>

<ul>
<li> <p><code>error</code> binary classification error rate
</p>
</li>
<li> <p><code>rmse</code> Rooted mean square error
</p>
</li>
<li> <p><code>logloss</code> negative log-likelihood function
</p>
</li>
<li> <p><code>mae</code> Mean absolute error
</p>
</li>
<li> <p><code>mape</code> Mean absolute percentage error
</p>
</li>
<li> <p><code>auc</code> Area under curve
</p>
</li>
<li> <p><code>aucpr</code> Area under PR curve
</p>
</li>
<li> <p><code>merror</code> Exact matching error, used to evaluate multi-class classification
</p>
</li></ul>
</td></tr>
<tr><td><code id="xgb.cv_+3A_obj">obj</code></td>
<td>
<p>customized objective function. Returns gradient and second order
gradient with given prediction and dtrain.</p>
</td></tr>
<tr><td><code id="xgb.cv_+3A_feval">feval</code></td>
<td>
<p>customized evaluation function. Returns
<code>list(metric='metric-name', value='metric-value')</code> with given
prediction and dtrain.</p>
</td></tr>
<tr><td><code id="xgb.cv_+3A_stratified">stratified</code></td>
<td>
<p>a <code>boolean</code> indicating whether sampling of folds should be stratified
by the values of outcome labels.</p>
</td></tr>
<tr><td><code id="xgb.cv_+3A_folds">folds</code></td>
<td>
<p><code>list</code> provides a possibility to use a list of pre-defined CV folds
(each element must be a vector of test fold's indices). When folds are supplied,
the <code>nfold</code> and <code>stratified</code> parameters are ignored.</p>
</td></tr>
<tr><td><code id="xgb.cv_+3A_train_folds">train_folds</code></td>
<td>
<p><code>list</code> list specifying which indicies to use for training. If <code>NULL</code>
(the default) all indices not specified in <code>folds</code> will be used for training.</p>
</td></tr>
<tr><td><code id="xgb.cv_+3A_verbose">verbose</code></td>
<td>
<p><code>boolean</code>, print the statistics during the process</p>
</td></tr>
<tr><td><code id="xgb.cv_+3A_print_every_n">print_every_n</code></td>
<td>
<p>Print each n-th iteration evaluation messages when <code>verbose&gt;0</code>.
Default is 1 which means all messages are printed. This parameter is passed to the
<code><a href="#topic+cb.print.evaluation">cb.print.evaluation</a></code> callback.</p>
</td></tr>
<tr><td><code id="xgb.cv_+3A_early_stopping_rounds">early_stopping_rounds</code></td>
<td>
<p>If <code>NULL</code>, the early stopping function is not triggered.
If set to an integer <code>k</code>, training with a validation set will stop if the performance
doesn't improve for <code>k</code> rounds.
Setting this parameter engages the <code><a href="#topic+cb.early.stop">cb.early.stop</a></code> callback.</p>
</td></tr>
<tr><td><code id="xgb.cv_+3A_maximize">maximize</code></td>
<td>
<p>If <code>feval</code> and <code>early_stopping_rounds</code> are set,
then this parameter must be set as well.
When it is <code>TRUE</code>, it means the larger the evaluation score the better.
This parameter is passed to the <code><a href="#topic+cb.early.stop">cb.early.stop</a></code> callback.</p>
</td></tr>
<tr><td><code id="xgb.cv_+3A_callbacks">callbacks</code></td>
<td>
<p>a list of callback functions to perform various task during boosting.
See <code><a href="#topic+callbacks">callbacks</a></code>. Some of the callbacks are automatically created depending on the
parameters' values. User can provide either existing or their own callback methods in order
to customize the training process.</p>
</td></tr>
<tr><td><code id="xgb.cv_+3A_...">...</code></td>
<td>
<p>other parameters to pass to <code>params</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The original sample is randomly partitioned into <code>nfold</code> equal size subsamples.
</p>
<p>Of the <code>nfold</code> subsamples, a single subsample is retained as the validation data for testing the model, and the remaining <code>nfold - 1</code> subsamples are used as training data.
</p>
<p>The cross-validation process is then repeated <code>nrounds</code> times, with each of the <code>nfold</code> subsamples used exactly once as the validation data.
</p>
<p>All observations are used for both training and validation.
</p>
<p>Adapted from <a href="https://en.wikipedia.org/wiki/Cross-validation_%28statistics%29">https://en.wikipedia.org/wiki/Cross-validation_%28statistics%29</a>
</p>


<h3>Value</h3>

<p>An object of class <code>xgb.cv.synchronous</code> with the following elements:
</p>

<ul>
<li> <p><code>call</code> a function call.
</p>
</li>
<li> <p><code>params</code> parameters that were passed to the xgboost library. Note that it does not
capture parameters changed by the <code><a href="#topic+cb.reset.parameters">cb.reset.parameters</a></code> callback.
</p>
</li>
<li> <p><code>callbacks</code> callback functions that were either automatically assigned or
explicitly passed.
</p>
</li>
<li> <p><code>evaluation_log</code> evaluation history stored as a <code>data.table</code> with the
first column corresponding to iteration number and the rest corresponding to the
CV-based evaluation means and standard deviations for the training and test CV-sets.
It is created by the <code><a href="#topic+cb.evaluation.log">cb.evaluation.log</a></code> callback.
</p>
</li>
<li> <p><code>niter</code> number of boosting iterations.
</p>
</li>
<li> <p><code>nfeatures</code> number of features in training data.
</p>
</li>
<li> <p><code>folds</code> the list of CV folds' indices - either those passed through the <code>folds</code>
parameter or randomly generated.
</p>
</li>
<li> <p><code>best_iteration</code> iteration number with the best evaluation metric value
(only available with early stopping).
</p>
</li>
<li> <p><code>best_ntreelimit</code> and the <code>ntreelimit</code> Deprecated attributes, use <code>best_iteration</code> instead.
</p>
</li>
<li> <p><code>pred</code> CV prediction values available when <code>prediction</code> is set.
It is either vector or matrix (see <code><a href="#topic+cb.cv.predict">cb.cv.predict</a></code>).
</p>
</li>
<li> <p><code>models</code> a list of the CV folds' models. It is only available with the explicit
setting of the <code>cb.cv.predict(save_models = TRUE)</code> callback.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>data(agaricus.train, package='xgboost')
dtrain &lt;- with(agaricus.train, xgb.DMatrix(data, label = label, nthread = 2))
cv &lt;- xgb.cv(data = dtrain, nrounds = 3, nthread = 2, nfold = 5, metrics = list("rmse","auc"),
             max_depth = 3, eta = 1, objective = "binary:logistic")
print(cv)
print(cv, verbose=TRUE)

</code></pre>

<hr>
<h2 id='xgb.DMatrix'>Construct xgb.DMatrix object</h2><span id='topic+xgb.DMatrix'></span>

<h3>Description</h3>

<p>Construct xgb.DMatrix object from either a dense matrix, a sparse matrix, or a local file.
Supported input file formats are either a LIBSVM text file or a binary file that was created previously by
<code><a href="#topic+xgb.DMatrix.save">xgb.DMatrix.save</a></code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>xgb.DMatrix(
  data,
  info = list(),
  missing = NA,
  silent = FALSE,
  nthread = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="xgb.DMatrix_+3A_data">data</code></td>
<td>
<p>a <code>matrix</code> object (either numeric or integer), a <code>dgCMatrix</code> object,
a <code>dgRMatrix</code> object (only when making predictions from a fitted model),
a <code>dsparseVector</code> object (only when making predictions from a fitted model, will be
interpreted as a row vector), or a character string representing a filename.</p>
</td></tr>
<tr><td><code id="xgb.DMatrix_+3A_info">info</code></td>
<td>
<p>a named list of additional information to store in the <code>xgb.DMatrix</code> object.
See <code><a href="#topic+setinfo">setinfo</a></code> for the specific allowed kinds of</p>
</td></tr>
<tr><td><code id="xgb.DMatrix_+3A_missing">missing</code></td>
<td>
<p>a float value to represents missing values in data (used only when input is a dense matrix).
It is useful when a 0 or some other extreme value represents missing values in data.</p>
</td></tr>
<tr><td><code id="xgb.DMatrix_+3A_silent">silent</code></td>
<td>
<p>whether to suppress printing an informational message after loading from a file.</p>
</td></tr>
<tr><td><code id="xgb.DMatrix_+3A_nthread">nthread</code></td>
<td>
<p>Number of threads used for creating DMatrix.</p>
</td></tr>
<tr><td><code id="xgb.DMatrix_+3A_...">...</code></td>
<td>
<p>the <code>info</code> data could be passed directly as parameters, without creating an <code>info</code> list.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>data(agaricus.train, package='xgboost')
## Keep the number of threads to 1 for examples
nthread &lt;- 1
data.table::setDTthreads(nthread)
dtrain &lt;- with(
  agaricus.train, xgb.DMatrix(data, label = label, nthread = nthread)
)
xgb.DMatrix.save(dtrain, 'xgb.DMatrix.data')
dtrain &lt;- xgb.DMatrix('xgb.DMatrix.data')
if (file.exists('xgb.DMatrix.data')) file.remove('xgb.DMatrix.data')
</code></pre>

<hr>
<h2 id='xgb.DMatrix.save'>Save xgb.DMatrix object to binary file</h2><span id='topic+xgb.DMatrix.save'></span>

<h3>Description</h3>

<p>Save xgb.DMatrix object to binary file
</p>


<h3>Usage</h3>

<pre><code class='language-R'>xgb.DMatrix.save(dmatrix, fname)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="xgb.DMatrix.save_+3A_dmatrix">dmatrix</code></td>
<td>
<p>the <code>xgb.DMatrix</code> object</p>
</td></tr>
<tr><td><code id="xgb.DMatrix.save_+3A_fname">fname</code></td>
<td>
<p>the name of the file to write.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>data(agaricus.train, package='xgboost')
dtrain &lt;- with(agaricus.train, xgb.DMatrix(data, label = label, nthread = 2))
xgb.DMatrix.save(dtrain, 'xgb.DMatrix.data')
dtrain &lt;- xgb.DMatrix('xgb.DMatrix.data')
if (file.exists('xgb.DMatrix.data')) file.remove('xgb.DMatrix.data')
</code></pre>

<hr>
<h2 id='xgb.dump'>Dump an xgboost model in text format.</h2><span id='topic+xgb.dump'></span>

<h3>Description</h3>

<p>Dump an xgboost model in text format.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>xgb.dump(
  model,
  fname = NULL,
  fmap = "",
  with_stats = FALSE,
  dump_format = c("text", "json"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="xgb.dump_+3A_model">model</code></td>
<td>
<p>the model object.</p>
</td></tr>
<tr><td><code id="xgb.dump_+3A_fname">fname</code></td>
<td>
<p>the name of the text file where to save the model text dump.
If not provided or set to <code>NULL</code>, the model is returned as a <code>character</code> vector.</p>
</td></tr>
<tr><td><code id="xgb.dump_+3A_fmap">fmap</code></td>
<td>
<p>feature map file representing feature types.
See demo/ for walkthrough example in R, and
<a href="https://github.com/dmlc/xgboost/blob/master/demo/data/featmap.txt">https://github.com/dmlc/xgboost/blob/master/demo/data/featmap.txt</a>
for example Format.</p>
</td></tr>
<tr><td><code id="xgb.dump_+3A_with_stats">with_stats</code></td>
<td>
<p>whether to dump some additional statistics about the splits.
When this option is on, the model dump contains two additional values:
gain is the approximate loss function gain we get in each split;
cover is the sum of second order gradient in each node.</p>
</td></tr>
<tr><td><code id="xgb.dump_+3A_dump_format">dump_format</code></td>
<td>
<p>either 'text' or 'json' format could be specified.</p>
</td></tr>
<tr><td><code id="xgb.dump_+3A_...">...</code></td>
<td>
<p>currently not used</p>
</td></tr>
</table>


<h3>Value</h3>

<p>If fname is not provided or set to <code>NULL</code> the function will return the model
as a <code>character</code> vector. Otherwise it will return <code>TRUE</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(agaricus.train, package='xgboost')
data(agaricus.test, package='xgboost')
train &lt;- agaricus.train
test &lt;- agaricus.test
bst &lt;- xgboost(data = train$data, label = train$label, max_depth = 2,
               eta = 1, nthread = 2, nrounds = 2, objective = "binary:logistic")
# save the model in file 'xgb.model.dump'
dump_path = file.path(tempdir(), 'model.dump')
xgb.dump(bst, dump_path, with_stats = TRUE)

# print the model without saving it to a file
print(xgb.dump(bst, with_stats = TRUE))

# print in JSON format:
cat(xgb.dump(bst, with_stats = TRUE, dump_format='json'))

</code></pre>

<hr>
<h2 id='xgb.gblinear.history'>Extract gblinear coefficients history.</h2><span id='topic+xgb.gblinear.history'></span>

<h3>Description</h3>

<p>A helper function to extract the matrix of linear coefficients' history
from a gblinear model created while using the <code>cb.gblinear.history()</code>
callback.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>xgb.gblinear.history(model, class_index = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="xgb.gblinear.history_+3A_model">model</code></td>
<td>
<p>either an <code>xgb.Booster</code> or a result of <code>xgb.cv()</code>, trained
using the <code>cb.gblinear.history()</code> callback.</p>
</td></tr>
<tr><td><code id="xgb.gblinear.history_+3A_class_index">class_index</code></td>
<td>
<p>zero-based class index to extract the coefficients for only that
specific class in a multinomial multiclass model. When it is NULL, all the
coefficients are returned. Has no effect in non-multiclass models.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>For an <code>xgb.train</code> result, a matrix (either dense or sparse) with the columns
corresponding to iteration's coefficients (in the order as <code>xgb.dump()</code> would
return) and the rows corresponding to boosting iterations.
</p>
<p>For an <code>xgb.cv</code> result, a list of such matrices is returned with the elements
corresponding to CV folds.
</p>

<hr>
<h2 id='xgb.ggplot.deepness'>Plot model trees deepness</h2><span id='topic+xgb.ggplot.deepness'></span><span id='topic+xgb.plot.deepness'></span>

<h3>Description</h3>

<p>Visualizes distributions related to depth of tree leafs.
<code>xgb.plot.deepness</code> uses base R graphics, while <code>xgb.ggplot.deepness</code> uses the ggplot backend.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>xgb.ggplot.deepness(
  model = NULL,
  which = c("2x1", "max.depth", "med.depth", "med.weight")
)

xgb.plot.deepness(
  model = NULL,
  which = c("2x1", "max.depth", "med.depth", "med.weight"),
  plot = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="xgb.ggplot.deepness_+3A_model">model</code></td>
<td>
<p>either an <code>xgb.Booster</code> model generated by the <code>xgb.train</code> function
or a data.table result of the <code>xgb.model.dt.tree</code> function.</p>
</td></tr>
<tr><td><code id="xgb.ggplot.deepness_+3A_which">which</code></td>
<td>
<p>which distribution to plot (see details).</p>
</td></tr>
<tr><td><code id="xgb.ggplot.deepness_+3A_plot">plot</code></td>
<td>
<p>(base R barplot) whether a barplot should be produced.
If FALSE, only a data.table is returned.</p>
</td></tr>
<tr><td><code id="xgb.ggplot.deepness_+3A_...">...</code></td>
<td>
<p>other parameters passed to <code>barplot</code> or <code>plot</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When <code>which="2x1"</code>, two distributions with respect to the leaf depth
are plotted on top of each other:
</p>

<ul>
<li><p> the distribution of the number of leafs in a tree model at a certain depth;
</p>
</li>
<li><p> the distribution of average weighted number of observations (&quot;cover&quot;)
ending up in leafs at certain depth.
</p>
</li></ul>

<p>Those could be helpful in determining sensible ranges of the <code>max_depth</code>
and <code>min_child_weight</code> parameters.
</p>
<p>When <code>which="max.depth"</code> or <code>which="med.depth"</code>, plots of either maximum or median depth
per tree with respect to tree number are created. And <code>which="med.weight"</code> allows to see how
a tree's median absolute leaf weight changes through the iterations.
</p>
<p>This function was inspired by the blog post
<a href="https://github.com/aysent/random-forest-leaf-visualization">https://github.com/aysent/random-forest-leaf-visualization</a>.
</p>


<h3>Value</h3>

<p>Other than producing plots (when <code>plot=TRUE</code>), the <code>xgb.plot.deepness</code> function
silently returns a processed data.table where each row corresponds to a terminal leaf in a tree model,
and contains information about leaf's depth, cover, and weight (which is used in calculating predictions).
</p>
<p>The <code>xgb.ggplot.deepness</code> silently returns either a list of two ggplot graphs when <code>which="2x1"</code>
or a single ggplot graph for the other <code>which</code> options.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+xgb.train">xgb.train</a></code>, <code><a href="#topic+xgb.model.dt.tree">xgb.model.dt.tree</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(agaricus.train, package='xgboost')
## Keep the number of threads to 2 for examples
nthread &lt;- 2
data.table::setDTthreads(nthread)

## Change max_depth to a higher number to get a more significant result
bst &lt;- xgboost(data = agaricus.train$data, label = agaricus.train$label, max_depth = 6,
               eta = 0.1, nthread = nthread, nrounds = 50, objective = "binary:logistic",
               subsample = 0.5, min_child_weight = 2)

xgb.plot.deepness(bst)
xgb.ggplot.deepness(bst)

xgb.plot.deepness(bst, which='max.depth', pch=16, col=rgb(0,0,1,0.3), cex=2)

xgb.plot.deepness(bst, which='med.weight', pch=16, col=rgb(0,0,1,0.3), cex=2)

</code></pre>

<hr>
<h2 id='xgb.ggplot.importance'>Plot feature importance as a bar graph</h2><span id='topic+xgb.ggplot.importance'></span><span id='topic+xgb.plot.importance'></span>

<h3>Description</h3>

<p>Represents previously calculated feature importance as a bar graph.
<code>xgb.plot.importance</code> uses base R graphics, while <code>xgb.ggplot.importance</code> uses the ggplot backend.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>xgb.ggplot.importance(
  importance_matrix = NULL,
  top_n = NULL,
  measure = NULL,
  rel_to_first = FALSE,
  n_clusters = c(1:10),
  ...
)

xgb.plot.importance(
  importance_matrix = NULL,
  top_n = NULL,
  measure = NULL,
  rel_to_first = FALSE,
  left_margin = 10,
  cex = NULL,
  plot = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="xgb.ggplot.importance_+3A_importance_matrix">importance_matrix</code></td>
<td>
<p>a <code>data.table</code> returned by <code><a href="#topic+xgb.importance">xgb.importance</a></code>.</p>
</td></tr>
<tr><td><code id="xgb.ggplot.importance_+3A_top_n">top_n</code></td>
<td>
<p>maximal number of top features to include into the plot.</p>
</td></tr>
<tr><td><code id="xgb.ggplot.importance_+3A_measure">measure</code></td>
<td>
<p>the name of importance measure to plot.
When <code>NULL</code>, 'Gain' would be used for trees and 'Weight' would be used for gblinear.</p>
</td></tr>
<tr><td><code id="xgb.ggplot.importance_+3A_rel_to_first">rel_to_first</code></td>
<td>
<p>whether importance values should be represented as relative to the highest ranked feature.
See Details.</p>
</td></tr>
<tr><td><code id="xgb.ggplot.importance_+3A_n_clusters">n_clusters</code></td>
<td>
<p>(ggplot only) a <code>numeric</code> vector containing the min and the max range
of the possible number of clusters of bars.</p>
</td></tr>
<tr><td><code id="xgb.ggplot.importance_+3A_...">...</code></td>
<td>
<p>other parameters passed to <code>barplot</code> (except horiz, border, cex.names, names.arg, and las).</p>
</td></tr>
<tr><td><code id="xgb.ggplot.importance_+3A_left_margin">left_margin</code></td>
<td>
<p>(base R barplot) allows to adjust the left margin size to fit feature names.
When it is NULL, the existing <code>par('mar')</code> is used.</p>
</td></tr>
<tr><td><code id="xgb.ggplot.importance_+3A_cex">cex</code></td>
<td>
<p>(base R barplot) passed as <code>cex.names</code> parameter to <code>barplot</code>.</p>
</td></tr>
<tr><td><code id="xgb.ggplot.importance_+3A_plot">plot</code></td>
<td>
<p>(base R barplot) whether a barplot should be produced.
If FALSE, only a data.table is returned.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The graph represents each feature as a horizontal bar of length proportional to the importance of a feature.
Features are shown ranked in a decreasing importance order.
It works for importances from both <code>gblinear</code> and <code>gbtree</code> models.
</p>
<p>When <code>rel_to_first = FALSE</code>, the values would be plotted as they were in <code>importance_matrix</code>.
For gbtree model, that would mean being normalized to the total of 1
(&quot;what is feature's importance contribution relative to the whole model?&quot;).
For linear models, <code>rel_to_first = FALSE</code> would show actual values of the coefficients.
Setting <code>rel_to_first = TRUE</code> allows to see the picture from the perspective of
&quot;what is feature's importance contribution relative to the most important feature?&quot;
</p>
<p>The ggplot-backend method also performs 1-D clustering of the importance values,
with bar colors corresponding to different clusters that have somewhat similar importance values.
</p>


<h3>Value</h3>

<p>The <code>xgb.plot.importance</code> function creates a <code>barplot</code> (when <code>plot=TRUE</code>)
and silently returns a processed data.table with <code>n_top</code> features sorted by importance.
</p>
<p>The <code>xgb.ggplot.importance</code> function returns a ggplot graph which could be customized afterwards.
E.g., to change the title of the graph, add <code>+ ggtitle("A GRAPH NAME")</code> to the result.
</p>


<h3>See Also</h3>

<p><code><a href="graphics.html#topic+barplot">barplot</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(agaricus.train)
## Keep the number of threads to 2 for examples
nthread &lt;- 2
data.table::setDTthreads(nthread)

bst &lt;- xgboost(
  data = agaricus.train$data, label = agaricus.train$label, max_depth = 3,
  eta = 1, nthread = nthread, nrounds = 2, objective = "binary:logistic"
)

importance_matrix &lt;- xgb.importance(colnames(agaricus.train$data), model = bst)

xgb.plot.importance(importance_matrix, rel_to_first = TRUE, xlab = "Relative importance")

(gg &lt;- xgb.ggplot.importance(importance_matrix, measure = "Frequency", rel_to_first = TRUE))
gg + ggplot2::ylab("Frequency")

</code></pre>

<hr>
<h2 id='xgb.ggplot.shap.summary'>SHAP contribution dependency summary plot</h2><span id='topic+xgb.ggplot.shap.summary'></span><span id='topic+xgb.plot.shap.summary'></span>

<h3>Description</h3>

<p>Compare SHAP contributions of different features.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>xgb.ggplot.shap.summary(
  data,
  shap_contrib = NULL,
  features = NULL,
  top_n = 10,
  model = NULL,
  trees = NULL,
  target_class = NULL,
  approxcontrib = FALSE,
  subsample = NULL
)

xgb.plot.shap.summary(
  data,
  shap_contrib = NULL,
  features = NULL,
  top_n = 10,
  model = NULL,
  trees = NULL,
  target_class = NULL,
  approxcontrib = FALSE,
  subsample = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="xgb.ggplot.shap.summary_+3A_data">data</code></td>
<td>
<p>data as a <code>matrix</code> or <code>dgCMatrix</code>.</p>
</td></tr>
<tr><td><code id="xgb.ggplot.shap.summary_+3A_shap_contrib">shap_contrib</code></td>
<td>
<p>a matrix of SHAP contributions that was computed earlier for the above
<code>data</code>. When it is NULL, it is computed internally using <code>model</code> and <code>data</code>.</p>
</td></tr>
<tr><td><code id="xgb.ggplot.shap.summary_+3A_features">features</code></td>
<td>
<p>a vector of either column indices or of feature names to plot. When it is NULL,
feature importance is calculated, and <code>top_n</code> high ranked features are taken.</p>
</td></tr>
<tr><td><code id="xgb.ggplot.shap.summary_+3A_top_n">top_n</code></td>
<td>
<p>when <code>features</code> is NULL, top_n [1, 100] most important features in a model are taken.</p>
</td></tr>
<tr><td><code id="xgb.ggplot.shap.summary_+3A_model">model</code></td>
<td>
<p>an <code>xgb.Booster</code> model. It has to be provided when either <code>shap_contrib</code>
or <code>features</code> is missing.</p>
</td></tr>
<tr><td><code id="xgb.ggplot.shap.summary_+3A_trees">trees</code></td>
<td>
<p>passed to <code><a href="#topic+xgb.importance">xgb.importance</a></code> when <code>features = NULL</code>.</p>
</td></tr>
<tr><td><code id="xgb.ggplot.shap.summary_+3A_target_class">target_class</code></td>
<td>
<p>is only relevant for multiclass models. When it is set to a 0-based class index,
only SHAP contributions for that specific class are used.
If it is not set, SHAP importances are averaged over all classes.</p>
</td></tr>
<tr><td><code id="xgb.ggplot.shap.summary_+3A_approxcontrib">approxcontrib</code></td>
<td>
<p>passed to <code><a href="#topic+predict.xgb.Booster">predict.xgb.Booster</a></code> when <code>shap_contrib = NULL</code>.</p>
</td></tr>
<tr><td><code id="xgb.ggplot.shap.summary_+3A_subsample">subsample</code></td>
<td>
<p>a random fraction of data points to use for plotting. When it is NULL,
it is set so that up to 100K data points are used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A point plot (each point representing one sample from <code>data</code>) is
produced for each feature, with the points plotted on the SHAP value axis.
Each point (observation) is coloured based on its feature value. The plot
hence allows us to see which features have a negative / positive contribution
on the model prediction, and whether the contribution is different for larger
or smaller values of the feature. We effectively try to replicate the
<code>summary_plot</code> function from https://github.com/shap/shap
</p>


<h3>Value</h3>

<p>A <code>ggplot2</code> object.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+xgb.plot.shap">xgb.plot.shap</a></code>, <code><a href="#topic+xgb.ggplot.shap.summary">xgb.ggplot.shap.summary</a></code>,
<a href="https://github.com/shap/shap">https://github.com/shap/shap</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># See \code{\link{xgb.plot.shap}}.
</code></pre>

<hr>
<h2 id='xgb.importance'>Importance of features in a model.</h2><span id='topic+xgb.importance'></span>

<h3>Description</h3>

<p>Creates a <code>data.table</code> of feature importances in a model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>xgb.importance(
  feature_names = NULL,
  model = NULL,
  trees = NULL,
  data = NULL,
  label = NULL,
  target = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="xgb.importance_+3A_feature_names">feature_names</code></td>
<td>
<p>character vector of feature names. If the model already
contains feature names, those would be used when <code>feature_names=NULL</code> (default value).
Non-null <code>feature_names</code> could be provided to override those in the model.</p>
</td></tr>
<tr><td><code id="xgb.importance_+3A_model">model</code></td>
<td>
<p>object of class <code>xgb.Booster</code>.</p>
</td></tr>
<tr><td><code id="xgb.importance_+3A_trees">trees</code></td>
<td>
<p>(only for the gbtree booster) an integer vector of tree indices that should be included
into the importance calculation. If set to <code>NULL</code>, all trees of the model are parsed.
It could be useful, e.g., in multiclass classification to get feature importances
for each class separately. IMPORTANT: the tree index in xgboost models
is zero-based (e.g., use <code>trees = 0:4</code> for first 5 trees).</p>
</td></tr>
<tr><td><code id="xgb.importance_+3A_data">data</code></td>
<td>
<p>deprecated.</p>
</td></tr>
<tr><td><code id="xgb.importance_+3A_label">label</code></td>
<td>
<p>deprecated.</p>
</td></tr>
<tr><td><code id="xgb.importance_+3A_target">target</code></td>
<td>
<p>deprecated.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function works for both linear and tree models.
</p>
<p>For linear models, the importance is the absolute magnitude of linear coefficients.
For that reason, in order to obtain a meaningful ranking by importance for a linear model,
the features need to be on the same scale (which you also would want to do when using either
L1 or L2 regularization).
</p>


<h3>Value</h3>

<p>For a tree model, a <code>data.table</code> with the following columns:
</p>

<ul>
<li> <p><code>Features</code> names of the features used in the model;
</p>
</li>
<li> <p><code>Gain</code> represents fractional contribution of each feature to the model based on
the total gain of this feature's splits. Higher percentage means a more important
predictive feature.
</p>
</li>
<li> <p><code>Cover</code> metric of the number of observation related to this feature;
</p>
</li>
<li> <p><code>Frequency</code> percentage representing the relative number of times
a feature have been used in trees.
</p>
</li></ul>

<p>A linear model's importance <code>data.table</code> has the following columns:
</p>

<ul>
<li> <p><code>Features</code> names of the features used in the model;
</p>
</li>
<li> <p><code>Weight</code> the linear coefficient of this feature;
</p>
</li>
<li> <p><code>Class</code> (only for multiclass models) class label.
</p>
</li></ul>

<p>If <code>feature_names</code> is not provided and <code>model</code> doesn't have <code>feature_names</code>,
index of the features will be used instead. Because the index is extracted from the model dump
(based on C++ code), it starts at 0 (as in C/C++ or Python) instead of 1 (usual in R).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# binomial classification using gbtree:
data(agaricus.train, package='xgboost')
bst &lt;- xgboost(data = agaricus.train$data, label = agaricus.train$label, max_depth = 2,
               eta = 1, nthread = 2, nrounds = 2, objective = "binary:logistic")
xgb.importance(model = bst)

# binomial classification using gblinear:
bst &lt;- xgboost(data = agaricus.train$data, label = agaricus.train$label, booster = "gblinear",
               eta = 0.3, nthread = 1, nrounds = 20, objective = "binary:logistic")
xgb.importance(model = bst)

# multiclass classification using gbtree:
nclass &lt;- 3
nrounds &lt;- 10
mbst &lt;- xgboost(data = as.matrix(iris[, -5]), label = as.numeric(iris$Species) - 1,
               max_depth = 3, eta = 0.2, nthread = 2, nrounds = nrounds,
               objective = "multi:softprob", num_class = nclass)
# all classes clumped together:
xgb.importance(model = mbst)
# inspect importances separately for each class:
xgb.importance(model = mbst, trees = seq(from=0, by=nclass, length.out=nrounds))
xgb.importance(model = mbst, trees = seq(from=1, by=nclass, length.out=nrounds))
xgb.importance(model = mbst, trees = seq(from=2, by=nclass, length.out=nrounds))

# multiclass classification using gblinear:
mbst &lt;- xgboost(data = scale(as.matrix(iris[, -5])), label = as.numeric(iris$Species) - 1,
               booster = "gblinear", eta = 0.2, nthread = 1, nrounds = 15,
               objective = "multi:softprob", num_class = nclass)
xgb.importance(model = mbst)

</code></pre>

<hr>
<h2 id='xgb.load'>Load xgboost model from binary file</h2><span id='topic+xgb.load'></span>

<h3>Description</h3>

<p>Load xgboost model from the binary model file.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>xgb.load(modelfile)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="xgb.load_+3A_modelfile">modelfile</code></td>
<td>
<p>the name of the binary input file.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The input file is expected to contain a model saved in an xgboost model format
using either <code><a href="#topic+xgb.save">xgb.save</a></code> or <code><a href="#topic+cb.save.model">cb.save.model</a></code> in R, or using some
appropriate methods from other xgboost interfaces. E.g., a model trained in Python and
saved from there in xgboost format, could be loaded from R.
</p>
<p>Note: a model saved as an R-object, has to be loaded using corresponding R-methods,
not <code>xgb.load</code>.
</p>


<h3>Value</h3>

<p>An object of <code>xgb.Booster</code> class.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+xgb.save">xgb.save</a></code>, <code><a href="#topic+xgb.Booster.complete">xgb.Booster.complete</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(agaricus.train, package='xgboost')
data(agaricus.test, package='xgboost')

## Keep the number of threads to 1 for examples
nthread &lt;- 1
data.table::setDTthreads(nthread)

train &lt;- agaricus.train
test &lt;- agaricus.test
bst &lt;- xgboost(
  data = train$data, label = train$label, max_depth = 2, eta = 1,
  nthread = nthread,
  nrounds = 2,
  objective = "binary:logistic"
)

xgb.save(bst, 'xgb.model')
bst &lt;- xgb.load('xgb.model')
if (file.exists('xgb.model')) file.remove('xgb.model')
</code></pre>

<hr>
<h2 id='xgb.load.raw'>Load serialised xgboost model from R's raw vector</h2><span id='topic+xgb.load.raw'></span>

<h3>Description</h3>

<p>User can generate raw memory buffer by calling xgb.save.raw
</p>


<h3>Usage</h3>

<pre><code class='language-R'>xgb.load.raw(buffer, as_booster = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="xgb.load.raw_+3A_buffer">buffer</code></td>
<td>
<p>the buffer returned by xgb.save.raw</p>
</td></tr>
<tr><td><code id="xgb.load.raw_+3A_as_booster">as_booster</code></td>
<td>
<p>Return the loaded model as xgb.Booster instead of xgb.Booster.handle.</p>
</td></tr>
</table>

<hr>
<h2 id='xgb.model.dt.tree'>Parse a boosted tree model text dump</h2><span id='topic+xgb.model.dt.tree'></span>

<h3>Description</h3>

<p>Parse a boosted tree model text dump into a <code>data.table</code> structure.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>xgb.model.dt.tree(
  feature_names = NULL,
  model = NULL,
  text = NULL,
  trees = NULL,
  use_int_id = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="xgb.model.dt.tree_+3A_feature_names">feature_names</code></td>
<td>
<p>character vector of feature names. If the model already
contains feature names, those would be used when <code>feature_names=NULL</code> (default value).
Non-null <code>feature_names</code> could be provided to override those in the model.</p>
</td></tr>
<tr><td><code id="xgb.model.dt.tree_+3A_model">model</code></td>
<td>
<p>object of class <code>xgb.Booster</code></p>
</td></tr>
<tr><td><code id="xgb.model.dt.tree_+3A_text">text</code></td>
<td>
<p><code>character</code> vector previously generated by the <code>xgb.dump</code>
function  (where parameter <code>with_stats = TRUE</code> should have been set).
<code>text</code> takes precedence over <code>model</code>.</p>
</td></tr>
<tr><td><code id="xgb.model.dt.tree_+3A_trees">trees</code></td>
<td>
<p>an integer vector of tree indices that should be parsed.
If set to <code>NULL</code>, all trees of the model are parsed.
It could be useful, e.g., in multiclass classification to get only
the trees of one certain class. IMPORTANT: the tree index in xgboost models
is zero-based (e.g., use <code>trees = 0:4</code> for first 5 trees).</p>
</td></tr>
<tr><td><code id="xgb.model.dt.tree_+3A_use_int_id">use_int_id</code></td>
<td>
<p>a logical flag indicating whether nodes in columns &quot;Yes&quot;, &quot;No&quot;, &quot;Missing&quot; should be
represented as integers (when FALSE) or as &quot;Tree-Node&quot; character strings (when FALSE).</p>
</td></tr>
<tr><td><code id="xgb.model.dt.tree_+3A_...">...</code></td>
<td>
<p>currently not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>data.table</code> with detailed information about model trees' nodes.
</p>
<p>The columns of the <code>data.table</code> are:
</p>

<ul>
<li> <p><code>Tree</code>: integer ID of a tree in a model (zero-based index)
</p>
</li>
<li> <p><code>Node</code>: integer ID of a node in a tree (zero-based index)
</p>
</li>
<li> <p><code>ID</code>: character identifier of a node in a model (only when <code>use_int_id=FALSE</code>)
</p>
</li>
<li> <p><code>Feature</code>: for a branch node, it's a feature id or name (when available);
for a leaf note, it simply labels it as <code>'Leaf'</code>
</p>
</li>
<li> <p><code>Split</code>: location of the split for a branch node (split condition is always &quot;less than&quot;)
</p>
</li>
<li> <p><code>Yes</code>: ID of the next node when the split condition is met
</p>
</li>
<li> <p><code>No</code>: ID of the next node when the split condition is not met
</p>
</li>
<li> <p><code>Missing</code>: ID of the next node when branch value is missing
</p>
</li>
<li> <p><code>Quality</code>: either the split gain (change in loss) or the leaf value
</p>
</li>
<li> <p><code>Cover</code>: metric related to the number of observation either seen by a split
or collected by a leaf during training.
</p>
</li></ul>

<p>When <code>use_int_id=FALSE</code>, columns &quot;Yes&quot;, &quot;No&quot;, and &quot;Missing&quot; point to model-wide node identifiers
in the &quot;ID&quot; column. When <code>use_int_id=TRUE</code>, those columns point to node identifiers from
the corresponding trees in the &quot;Node&quot; column.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Basic use:

data(agaricus.train, package='xgboost')
## Keep the number of threads to 1 for examples
nthread &lt;- 1
data.table::setDTthreads(nthread)

bst &lt;- xgboost(data = agaricus.train$data, label = agaricus.train$label, max_depth = 2,
               eta = 1, nthread = nthread, nrounds = 2,objective = "binary:logistic")

(dt &lt;- xgb.model.dt.tree(colnames(agaricus.train$data), bst))

# This bst model already has feature_names stored with it, so those would be used when
# feature_names is not set:
(dt &lt;- xgb.model.dt.tree(model = bst))

# How to match feature names of splits that are following a current 'Yes' branch:

merge(dt, dt[, .(ID, Y.Feature=Feature)], by.x='Yes', by.y='ID', all.x=TRUE)[order(Tree,Node)]

</code></pre>

<hr>
<h2 id='xgb.parameters+26lt+3B-'>Accessors for model parameters.</h2><span id='topic+xgb.parameters+3C-'></span>

<h3>Description</h3>

<p>Only the setter for xgboost parameters is currently implemented.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>xgb.parameters(object) &lt;- value
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="xgb.parameters+2B26lt+2B3B-_+3A_object">object</code></td>
<td>
<p>Object of class <code>xgb.Booster</code> or <code>xgb.Booster.handle</code>.</p>
</td></tr>
<tr><td><code id="xgb.parameters+2B26lt+2B3B-_+3A_value">value</code></td>
<td>
<p>a list (or an object coercible to a list) with the names of parameters to set
and the elements corresponding to parameter values.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that the setter would usually work more efficiently for <code>xgb.Booster.handle</code>
than for <code>xgb.Booster</code>, since only just a handle would need to be copied.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(agaricus.train, package='xgboost')
train &lt;- agaricus.train

bst &lt;- xgboost(data = train$data, label = train$label, max_depth = 2,
               eta = 1, nthread = 2, nrounds = 2, objective = "binary:logistic")

xgb.parameters(bst) &lt;- list(eta = 0.1)

</code></pre>

<hr>
<h2 id='xgb.plot.multi.trees'>Project all trees on one tree and plot it</h2><span id='topic+xgb.plot.multi.trees'></span>

<h3>Description</h3>

<p>Visualization of the ensemble of trees as a single collective unit.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>xgb.plot.multi.trees(
  model,
  feature_names = NULL,
  features_keep = 5,
  plot_width = NULL,
  plot_height = NULL,
  render = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="xgb.plot.multi.trees_+3A_model">model</code></td>
<td>
<p>produced by the <code>xgb.train</code> function.</p>
</td></tr>
<tr><td><code id="xgb.plot.multi.trees_+3A_feature_names">feature_names</code></td>
<td>
<p>names of each feature as a <code>character</code> vector.</p>
</td></tr>
<tr><td><code id="xgb.plot.multi.trees_+3A_features_keep">features_keep</code></td>
<td>
<p>number of features to keep in each position of the multi trees.</p>
</td></tr>
<tr><td><code id="xgb.plot.multi.trees_+3A_plot_width">plot_width</code></td>
<td>
<p>width in pixels of the graph to produce</p>
</td></tr>
<tr><td><code id="xgb.plot.multi.trees_+3A_plot_height">plot_height</code></td>
<td>
<p>height in pixels of the graph to produce</p>
</td></tr>
<tr><td><code id="xgb.plot.multi.trees_+3A_render">render</code></td>
<td>
<p>a logical flag for whether the graph should be rendered (see Value).</p>
</td></tr>
<tr><td><code id="xgb.plot.multi.trees_+3A_...">...</code></td>
<td>
<p>currently not used</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function tries to capture the complexity of a gradient boosted tree model
in a cohesive way by compressing an ensemble of trees into a single tree-graph representation.
The goal is to improve the interpretability of a model generally seen as black box.
</p>
<p>Note: this function is applicable to tree booster-based models only.
</p>
<p>It takes advantage of the fact that the shape of a binary tree is only defined by
its depth (therefore, in a boosting model, all trees have similar shape).
</p>
<p>Moreover, the trees tend to reuse the same features.
</p>
<p>The function projects each tree onto one, and keeps for each position the
<code>features_keep</code> first features (based on the Gain per feature measure).
</p>
<p>This function is inspired by this blog post:
<a href="https://wellecks.wordpress.com/2015/02/21/peering-into-the-black-box-visualizing-lambdamart/">https://wellecks.wordpress.com/2015/02/21/peering-into-the-black-box-visualizing-lambdamart/</a>
</p>


<h3>Value</h3>

<p>When <code>render = TRUE</code>:
returns a rendered graph object which is an <code>htmlwidget</code> of class <code>grViz</code>.
Similar to ggplot objects, it needs to be printed to see it when not running from command line.
</p>
<p>When <code>render = FALSE</code>:
silently returns a graph object which is of DiagrammeR's class <code>dgr_graph</code>.
This could be useful if one wants to modify some of the graph attributes
before rendering the graph with <code><a href="DiagrammeR.html#topic+render_graph">render_graph</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(agaricus.train, package='xgboost')
## Keep the number of threads to 2 for examples
nthread &lt;- 2
data.table::setDTthreads(nthread)

bst &lt;- xgboost(
  data = agaricus.train$data, label = agaricus.train$label, max_depth = 15,
  eta = 1, nthread = nthread, nrounds = 30, objective = "binary:logistic",
  min_child_weight = 50, verbose = 0
)

p &lt;- xgb.plot.multi.trees(model = bst, features_keep = 3)
print(p)

## Not run: 
# Below is an example of how to save this plot to a file.
# Note that for `export_graph` to work, the DiagrammeRsvg and rsvg packages must also be installed.
library(DiagrammeR)
gr &lt;- xgb.plot.multi.trees(model=bst, features_keep = 3, render=FALSE)
export_graph(gr, 'tree.pdf', width=1500, height=600)

## End(Not run)

</code></pre>

<hr>
<h2 id='xgb.plot.shap'>SHAP contribution dependency plots</h2><span id='topic+xgb.plot.shap'></span>

<h3>Description</h3>

<p>Visualizing the SHAP feature contribution to prediction dependencies on feature value.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>xgb.plot.shap(
  data,
  shap_contrib = NULL,
  features = NULL,
  top_n = 1,
  model = NULL,
  trees = NULL,
  target_class = NULL,
  approxcontrib = FALSE,
  subsample = NULL,
  n_col = 1,
  col = rgb(0, 0, 1, 0.2),
  pch = ".",
  discrete_n_uniq = 5,
  discrete_jitter = 0.01,
  ylab = "SHAP",
  plot_NA = TRUE,
  col_NA = rgb(0.7, 0, 1, 0.6),
  pch_NA = ".",
  pos_NA = 1.07,
  plot_loess = TRUE,
  col_loess = 2,
  span_loess = 0.5,
  which = c("1d", "2d"),
  plot = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="xgb.plot.shap_+3A_data">data</code></td>
<td>
<p>data as a <code>matrix</code> or <code>dgCMatrix</code>.</p>
</td></tr>
<tr><td><code id="xgb.plot.shap_+3A_shap_contrib">shap_contrib</code></td>
<td>
<p>a matrix of SHAP contributions that was computed earlier for the above
<code>data</code>. When it is NULL, it is computed internally using <code>model</code> and <code>data</code>.</p>
</td></tr>
<tr><td><code id="xgb.plot.shap_+3A_features">features</code></td>
<td>
<p>a vector of either column indices or of feature names to plot. When it is NULL,
feature importance is calculated, and <code>top_n</code> high ranked features are taken.</p>
</td></tr>
<tr><td><code id="xgb.plot.shap_+3A_top_n">top_n</code></td>
<td>
<p>when <code>features</code> is NULL, top_n [1, 100] most important features in a model are taken.</p>
</td></tr>
<tr><td><code id="xgb.plot.shap_+3A_model">model</code></td>
<td>
<p>an <code>xgb.Booster</code> model. It has to be provided when either <code>shap_contrib</code>
or <code>features</code> is missing.</p>
</td></tr>
<tr><td><code id="xgb.plot.shap_+3A_trees">trees</code></td>
<td>
<p>passed to <code><a href="#topic+xgb.importance">xgb.importance</a></code> when <code>features = NULL</code>.</p>
</td></tr>
<tr><td><code id="xgb.plot.shap_+3A_target_class">target_class</code></td>
<td>
<p>is only relevant for multiclass models. When it is set to a 0-based class index,
only SHAP contributions for that specific class are used.
If it is not set, SHAP importances are averaged over all classes.</p>
</td></tr>
<tr><td><code id="xgb.plot.shap_+3A_approxcontrib">approxcontrib</code></td>
<td>
<p>passed to <code><a href="#topic+predict.xgb.Booster">predict.xgb.Booster</a></code> when <code>shap_contrib = NULL</code>.</p>
</td></tr>
<tr><td><code id="xgb.plot.shap_+3A_subsample">subsample</code></td>
<td>
<p>a random fraction of data points to use for plotting. When it is NULL,
it is set so that up to 100K data points are used.</p>
</td></tr>
<tr><td><code id="xgb.plot.shap_+3A_n_col">n_col</code></td>
<td>
<p>a number of columns in a grid of plots.</p>
</td></tr>
<tr><td><code id="xgb.plot.shap_+3A_col">col</code></td>
<td>
<p>color of the scatterplot markers.</p>
</td></tr>
<tr><td><code id="xgb.plot.shap_+3A_pch">pch</code></td>
<td>
<p>scatterplot marker.</p>
</td></tr>
<tr><td><code id="xgb.plot.shap_+3A_discrete_n_uniq">discrete_n_uniq</code></td>
<td>
<p>a maximal number of unique values in a feature to consider it as discrete.</p>
</td></tr>
<tr><td><code id="xgb.plot.shap_+3A_discrete_jitter">discrete_jitter</code></td>
<td>
<p>an <code>amount</code> parameter of jitter added to discrete features' positions.</p>
</td></tr>
<tr><td><code id="xgb.plot.shap_+3A_ylab">ylab</code></td>
<td>
<p>a y-axis label in 1D plots.</p>
</td></tr>
<tr><td><code id="xgb.plot.shap_+3A_plot_na">plot_NA</code></td>
<td>
<p>whether the contributions of cases with missing values should also be plotted.</p>
</td></tr>
<tr><td><code id="xgb.plot.shap_+3A_col_na">col_NA</code></td>
<td>
<p>a color of marker for missing value contributions.</p>
</td></tr>
<tr><td><code id="xgb.plot.shap_+3A_pch_na">pch_NA</code></td>
<td>
<p>a marker type for NA values.</p>
</td></tr>
<tr><td><code id="xgb.plot.shap_+3A_pos_na">pos_NA</code></td>
<td>
<p>a relative position of the x-location where NA values are shown:
<code>min(x) + (max(x) - min(x)) * pos_NA</code>.</p>
</td></tr>
<tr><td><code id="xgb.plot.shap_+3A_plot_loess">plot_loess</code></td>
<td>
<p>whether to plot loess-smoothed curves. The smoothing is only done for features with
more than 5 distinct values.</p>
</td></tr>
<tr><td><code id="xgb.plot.shap_+3A_col_loess">col_loess</code></td>
<td>
<p>a color to use for the loess curves.</p>
</td></tr>
<tr><td><code id="xgb.plot.shap_+3A_span_loess">span_loess</code></td>
<td>
<p>the <code>span</code> parameter in <code><a href="stats.html#topic+loess">loess</a></code>'s call.</p>
</td></tr>
<tr><td><code id="xgb.plot.shap_+3A_which">which</code></td>
<td>
<p>whether to do univariate or bivariate plotting. NOTE: only 1D is implemented so far.</p>
</td></tr>
<tr><td><code id="xgb.plot.shap_+3A_plot">plot</code></td>
<td>
<p>whether a plot should be drawn. If FALSE, only a list of matrices is returned.</p>
</td></tr>
<tr><td><code id="xgb.plot.shap_+3A_...">...</code></td>
<td>
<p>other parameters passed to <code>plot</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These scatterplots represent how SHAP feature contributions depend of feature values.
The similarity to partial dependency plots is that they also give an idea for how feature values
affect predictions. However, in partial dependency plots, we usually see marginal dependencies
of model prediction on feature value, while SHAP contribution dependency plots display the estimated
contributions of a feature to model prediction for each individual case.
</p>
<p>When <code>plot_loess = TRUE</code> is set, feature values are rounded to 3 significant digits and
weighted LOESS is computed and plotted, where weights are the numbers of data points
at each rounded value.
</p>
<p>Note: SHAP contributions are shown on the scale of model margin. E.g., for a logistic binomial objective,
the margin is prediction before a sigmoidal transform into probability-like values.
Also, since SHAP stands for &quot;SHapley Additive exPlanation&quot; (model prediction = sum of SHAP
contributions for all features + bias), depending on the objective used, transforming SHAP
contributions for a feature from the marginal to the prediction space is not necessarily
a meaningful thing to do.
</p>


<h3>Value</h3>

<p>In addition to producing plots (when <code>plot=TRUE</code>), it silently returns a list of two matrices:
</p>

<ul>
<li> <p><code>data</code> the values of selected features;
</p>
</li>
<li> <p><code>shap_contrib</code> the contributions of selected features.
</p>
</li></ul>



<h3>References</h3>

<p>Scott M. Lundberg, Su-In Lee, &quot;A Unified Approach to Interpreting Model Predictions&quot;, NIPS Proceedings 2017, <a href="https://arxiv.org/abs/1705.07874">https://arxiv.org/abs/1705.07874</a>
</p>
<p>Scott M. Lundberg, Su-In Lee, &quot;Consistent feature attribution for tree ensembles&quot;, <a href="https://arxiv.org/abs/1706.06060">https://arxiv.org/abs/1706.06060</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(agaricus.train, package='xgboost')
data(agaricus.test, package='xgboost')

## Keep the number of threads to 1 for examples
nthread &lt;- 1
data.table::setDTthreads(nthread)
nrounds &lt;- 20

bst &lt;- xgboost(agaricus.train$data, agaricus.train$label, nrounds = nrounds,
               eta = 0.1, max_depth = 3, subsample = .5,
               method = "hist", objective = "binary:logistic", nthread = nthread, verbose = 0)

xgb.plot.shap(agaricus.test$data, model = bst, features = "odor=none")
contr &lt;- predict(bst, agaricus.test$data, predcontrib = TRUE)
xgb.plot.shap(agaricus.test$data, contr, model = bst, top_n = 12, n_col = 3)
xgb.ggplot.shap.summary(agaricus.test$data, contr, model = bst, top_n = 12)  # Summary plot

# multiclass example - plots for each class separately:
nclass &lt;- 3
x &lt;- as.matrix(iris[, -5])
set.seed(123)
is.na(x[sample(nrow(x) * 4, 30)]) &lt;- TRUE # introduce some missing values
mbst &lt;- xgboost(data = x, label = as.numeric(iris$Species) - 1, nrounds = nrounds,
                max_depth = 2, eta = 0.3, subsample = .5, nthread = nthread,
                objective = "multi:softprob", num_class = nclass, verbose = 0)
trees0 &lt;- seq(from=0, by=nclass, length.out=nrounds)
col &lt;- rgb(0, 0, 1, 0.5)
xgb.plot.shap(x, model = mbst, trees = trees0, target_class = 0, top_n = 4,
              n_col = 2, col = col, pch = 16, pch_NA = 17)
xgb.plot.shap(x, model = mbst, trees = trees0 + 1, target_class = 1, top_n = 4,
              n_col = 2, col = col, pch = 16, pch_NA = 17)
xgb.plot.shap(x, model = mbst, trees = trees0 + 2, target_class = 2, top_n = 4,
              n_col = 2, col = col, pch = 16, pch_NA = 17)
xgb.ggplot.shap.summary(x, model = mbst, target_class = 0, top_n = 4)  # Summary plot

</code></pre>

<hr>
<h2 id='xgb.plot.tree'>Plot a boosted tree model</h2><span id='topic+xgb.plot.tree'></span>

<h3>Description</h3>

<p>Read a tree model text dump and plot the model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>xgb.plot.tree(
  feature_names = NULL,
  model = NULL,
  trees = NULL,
  plot_width = NULL,
  plot_height = NULL,
  render = TRUE,
  show_node_id = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="xgb.plot.tree_+3A_feature_names">feature_names</code></td>
<td>
<p>names of each feature as a <code>character</code> vector.</p>
</td></tr>
<tr><td><code id="xgb.plot.tree_+3A_model">model</code></td>
<td>
<p>produced by the <code>xgb.train</code> function.</p>
</td></tr>
<tr><td><code id="xgb.plot.tree_+3A_trees">trees</code></td>
<td>
<p>an integer vector of tree indices that should be visualized.
If set to <code>NULL</code>, all trees of the model are included.
IMPORTANT: the tree index in xgboost model is zero-based
(e.g., use <code>trees = 0:2</code> for the first 3 trees in a model).</p>
</td></tr>
<tr><td><code id="xgb.plot.tree_+3A_plot_width">plot_width</code></td>
<td>
<p>the width of the diagram in pixels.</p>
</td></tr>
<tr><td><code id="xgb.plot.tree_+3A_plot_height">plot_height</code></td>
<td>
<p>the height of the diagram in pixels.</p>
</td></tr>
<tr><td><code id="xgb.plot.tree_+3A_render">render</code></td>
<td>
<p>a logical flag for whether the graph should be rendered (see Value).</p>
</td></tr>
<tr><td><code id="xgb.plot.tree_+3A_show_node_id">show_node_id</code></td>
<td>
<p>a logical flag for whether to show node id's in the graph.</p>
</td></tr>
<tr><td><code id="xgb.plot.tree_+3A_...">...</code></td>
<td>
<p>currently not used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The content of each node is organised that way:
</p>

<ul>
<li><p> Feature name.
</p>
</li>
<li> <p><code>Cover</code>: The sum of second order gradient of training data classified to the leaf.
If it is square loss, this simply corresponds to the number of instances seen by a split
or collected by a leaf during training.
The deeper in the tree a node is, the lower this metric will be.
</p>
</li>
<li> <p><code>Gain</code> (for split nodes): the information gain metric of a split
(corresponds to the importance of the node in the model).
</p>
</li>
<li> <p><code>Value</code> (for leafs): the margin value that the leaf may contribute to prediction.
</p>
</li></ul>

<p>The tree root nodes also indicate the Tree index (0-based).
</p>
<p>The &quot;Yes&quot; branches are marked by the &quot;&lt; split_value&quot; label.
The branches that also used for missing values are marked as bold
(as in &quot;carrying extra capacity&quot;).
</p>
<p>This function uses <a href="https://www.graphviz.org/">GraphViz</a> as a backend of DiagrammeR.
</p>


<h3>Value</h3>

<p>When <code>render = TRUE</code>:
returns a rendered graph object which is an <code>htmlwidget</code> of class <code>grViz</code>.
Similar to ggplot objects, it needs to be printed to see it when not running from command line.
</p>
<p>When <code>render = FALSE</code>:
silently returns a graph object which is of DiagrammeR's class <code>dgr_graph</code>.
This could be useful if one wants to modify some of the graph attributes
before rendering the graph with <code><a href="DiagrammeR.html#topic+render_graph">render_graph</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(agaricus.train, package='xgboost')

bst &lt;- xgboost(data = agaricus.train$data, label = agaricus.train$label, max_depth = 3,
               eta = 1, nthread = 2, nrounds = 2,objective = "binary:logistic")
# plot all the trees
xgb.plot.tree(model = bst)
# plot only the first tree and display the node ID:
xgb.plot.tree(model = bst, trees = 0, show_node_id = TRUE)

## Not run: 
# Below is an example of how to save this plot to a file.
# Note that for `export_graph` to work, the DiagrammeRsvg and rsvg packages must also be installed.
library(DiagrammeR)
gr &lt;- xgb.plot.tree(model=bst, trees=0:1, render=FALSE)
export_graph(gr, 'tree.pdf', width=1500, height=1900)
export_graph(gr, 'tree.png', width=1500, height=1900)

## End(Not run)

</code></pre>

<hr>
<h2 id='xgb.save'>Save xgboost model to binary file</h2><span id='topic+xgb.save'></span>

<h3>Description</h3>

<p>Save xgboost model to a file in binary format.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>xgb.save(model, fname)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="xgb.save_+3A_model">model</code></td>
<td>
<p>model object of <code>xgb.Booster</code> class.</p>
</td></tr>
<tr><td><code id="xgb.save_+3A_fname">fname</code></td>
<td>
<p>name of the file to write.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This methods allows to save a model in an xgboost-internal binary format which is universal
among the various xgboost interfaces. In R, the saved model file could be read-in later
using either the <code><a href="#topic+xgb.load">xgb.load</a></code> function or the <code>xgb_model</code> parameter
of <code><a href="#topic+xgb.train">xgb.train</a></code>.
</p>
<p>Note: a model can also be saved as an R-object (e.g., by using <code><a href="base.html#topic+readRDS">readRDS</a></code>
or <code><a href="base.html#topic+save">save</a></code>). However, it would then only be compatible with R, and
corresponding R-methods would need to be used to load it. Moreover, persisting the model with
<code><a href="base.html#topic+readRDS">readRDS</a></code> or <code><a href="base.html#topic+save">save</a></code>) will cause compatibility problems in
future versions of XGBoost. Consult <code><a href="#topic+a-compatibility-note-for-saveRDS-save">a-compatibility-note-for-saveRDS-save</a></code> to learn
how to persist models in a future-proof way, i.e. to make the model accessible in future
releases of XGBoost.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+xgb.load">xgb.load</a></code>, <code><a href="#topic+xgb.Booster.complete">xgb.Booster.complete</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(agaricus.train, package='xgboost')
data(agaricus.test, package='xgboost')

## Keep the number of threads to 1 for examples
nthread &lt;- 1
data.table::setDTthreads(nthread)

train &lt;- agaricus.train
test &lt;- agaricus.test
bst &lt;- xgboost(
  data = train$data, label = train$label, max_depth = 2, eta = 1,
  nthread = nthread,
  nrounds = 2,
  objective = "binary:logistic"
)
xgb.save(bst, 'xgb.model')
bst &lt;- xgb.load('xgb.model')
if (file.exists('xgb.model')) file.remove('xgb.model')
</code></pre>

<hr>
<h2 id='xgb.save.raw'>Save xgboost model to R's raw vector,
user can call xgb.load.raw to load the model back from raw vector</h2><span id='topic+xgb.save.raw'></span>

<h3>Description</h3>

<p>Save xgboost model from xgboost or xgb.train
</p>


<h3>Usage</h3>

<pre><code class='language-R'>xgb.save.raw(model, raw_format = "deprecated")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="xgb.save.raw_+3A_model">model</code></td>
<td>
<p>the model object.</p>
</td></tr>
<tr><td><code id="xgb.save.raw_+3A_raw_format">raw_format</code></td>
<td>
<p>The format for encoding the booster.  Available options are
</p>

<ul>
<li> <p><code>json</code>: Encode the booster into JSON text document.
</p>
</li>
<li> <p><code>ubj</code>:  Encode the booster into Universal Binary JSON.
</p>
</li>
<li> <p><code>deprecated</code>: Encode the booster into old customized binary format.
</p>
</li></ul>

<p>Right now the default is <code>deprecated</code> but will be changed to <code>ubj</code> in upcoming release.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>data(agaricus.train, package='xgboost')
data(agaricus.test, package='xgboost')

## Keep the number of threads to 2 for examples
nthread &lt;- 2
data.table::setDTthreads(nthread)

train &lt;- agaricus.train
test &lt;- agaricus.test
bst &lt;- xgboost(data = train$data, label = train$label, max_depth = 2,
               eta = 1, nthread = nthread, nrounds = 2,objective = "binary:logistic")

raw &lt;- xgb.save.raw(bst)
bst &lt;- xgb.load.raw(raw)

</code></pre>

<hr>
<h2 id='xgb.serialize'>Serialize the booster instance into R's raw vector.  The serialization method differs
from <code><a href="#topic+xgb.save.raw">xgb.save.raw</a></code> as the latter one saves only the model but not
parameters.  This serialization format is not stable across different xgboost versions.</h2><span id='topic+xgb.serialize'></span>

<h3>Description</h3>

<p>Serialize the booster instance into R's raw vector.  The serialization method differs
from <code><a href="#topic+xgb.save.raw">xgb.save.raw</a></code> as the latter one saves only the model but not
parameters.  This serialization format is not stable across different xgboost versions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>xgb.serialize(booster)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="xgb.serialize_+3A_booster">booster</code></td>
<td>
<p>the booster instance</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>data(agaricus.train, package='xgboost')
data(agaricus.test, package='xgboost')
train &lt;- agaricus.train
test &lt;- agaricus.test
bst &lt;- xgboost(data = train$data, label = train$label, max_depth = 2,
               eta = 1, nthread = 2, nrounds = 2,objective = "binary:logistic")
raw &lt;- xgb.serialize(bst)
bst &lt;- xgb.unserialize(raw)

</code></pre>

<hr>
<h2 id='xgb.set.config+2C+20xgb.get.config'>Set and get global configuration</h2><span id='topic+xgb.set.config+2C+20xgb.get.config'></span><span id='topic+xgb.set.config'></span><span id='topic+xgb.get.config'></span>

<h3>Description</h3>

<p>Global configuration consists of a collection of parameters that can be applied in the global
scope. See <a href="https://xgboost.readthedocs.io/en/stable/parameter.html">https://xgboost.readthedocs.io/en/stable/parameter.html</a> for the full list of
parameters supported in the global configuration. Use <code>xgb.set.config</code> to update the
values of one or more global-scope parameters. Use <code>xgb.get.config</code> to fetch the current
values of all global-scope parameters (listed in
<a href="https://xgboost.readthedocs.io/en/stable/parameter.html">https://xgboost.readthedocs.io/en/stable/parameter.html</a>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>xgb.set.config(...)

xgb.get.config()
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="xgb.set.config+2B2C+2B20xgb.get.config_+3A_...">...</code></td>
<td>
<p>List of parameters to be set, as keyword arguments</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>xgb.set.config</code> returns <code>TRUE</code> to signal success. <code>xgb.get.config</code> returns
a list containing all global-scope parameters and their values.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Set verbosity level to silent (0)
xgb.set.config(verbosity = 0)
# Now global verbosity level is 0
config &lt;- xgb.get.config()
print(config$verbosity)
# Set verbosity level to warning (1)
xgb.set.config(verbosity = 1)
# Now global verbosity level is 1
config &lt;- xgb.get.config()
print(config$verbosity)
</code></pre>

<hr>
<h2 id='xgb.shap.data'>Prepare data for SHAP plots. To be used in xgb.plot.shap, xgb.plot.shap.summary, etc.
Internal utility function.</h2><span id='topic+xgb.shap.data'></span>

<h3>Description</h3>

<p>Prepare data for SHAP plots. To be used in xgb.plot.shap, xgb.plot.shap.summary, etc.
Internal utility function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>xgb.shap.data(
  data,
  shap_contrib = NULL,
  features = NULL,
  top_n = 1,
  model = NULL,
  trees = NULL,
  target_class = NULL,
  approxcontrib = FALSE,
  subsample = NULL,
  max_observations = 1e+05
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="xgb.shap.data_+3A_data">data</code></td>
<td>
<p>data as a <code>matrix</code> or <code>dgCMatrix</code>.</p>
</td></tr>
<tr><td><code id="xgb.shap.data_+3A_shap_contrib">shap_contrib</code></td>
<td>
<p>a matrix of SHAP contributions that was computed earlier for the above
<code>data</code>. When it is NULL, it is computed internally using <code>model</code> and <code>data</code>.</p>
</td></tr>
<tr><td><code id="xgb.shap.data_+3A_features">features</code></td>
<td>
<p>a vector of either column indices or of feature names to plot. When it is NULL,
feature importance is calculated, and <code>top_n</code> high ranked features are taken.</p>
</td></tr>
<tr><td><code id="xgb.shap.data_+3A_top_n">top_n</code></td>
<td>
<p>when <code>features</code> is NULL, top_n [1, 100] most important features in a model are taken.</p>
</td></tr>
<tr><td><code id="xgb.shap.data_+3A_model">model</code></td>
<td>
<p>an <code>xgb.Booster</code> model. It has to be provided when either <code>shap_contrib</code>
or <code>features</code> is missing.</p>
</td></tr>
<tr><td><code id="xgb.shap.data_+3A_trees">trees</code></td>
<td>
<p>passed to <code><a href="#topic+xgb.importance">xgb.importance</a></code> when <code>features = NULL</code>.</p>
</td></tr>
<tr><td><code id="xgb.shap.data_+3A_target_class">target_class</code></td>
<td>
<p>is only relevant for multiclass models. When it is set to a 0-based class index,
only SHAP contributions for that specific class are used.
If it is not set, SHAP importances are averaged over all classes.</p>
</td></tr>
<tr><td><code id="xgb.shap.data_+3A_approxcontrib">approxcontrib</code></td>
<td>
<p>passed to <code><a href="#topic+predict.xgb.Booster">predict.xgb.Booster</a></code> when <code>shap_contrib = NULL</code>.</p>
</td></tr>
<tr><td><code id="xgb.shap.data_+3A_subsample">subsample</code></td>
<td>
<p>a random fraction of data points to use for plotting. When it is NULL,
it is set so that up to 100K data points are used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing: 'data', a matrix containing sample observations
and their feature values; 'shap_contrib', a matrix containing the SHAP contribution
values for these observations.
</p>

<hr>
<h2 id='xgb.train'>eXtreme Gradient Boosting Training</h2><span id='topic+xgb.train'></span><span id='topic+xgboost'></span>

<h3>Description</h3>

<p><code>xgb.train</code> is an advanced interface for training an xgboost model.
The <code>xgboost</code> function is a simpler wrapper for <code>xgb.train</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>xgb.train(
  params = list(),
  data,
  nrounds,
  watchlist = list(),
  obj = NULL,
  feval = NULL,
  verbose = 1,
  print_every_n = 1L,
  early_stopping_rounds = NULL,
  maximize = NULL,
  save_period = NULL,
  save_name = "xgboost.model",
  xgb_model = NULL,
  callbacks = list(),
  ...
)

xgboost(
  data = NULL,
  label = NULL,
  missing = NA,
  weight = NULL,
  params = list(),
  nrounds,
  verbose = 1,
  print_every_n = 1L,
  early_stopping_rounds = NULL,
  maximize = NULL,
  save_period = NULL,
  save_name = "xgboost.model",
  xgb_model = NULL,
  callbacks = list(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="xgb.train_+3A_params">params</code></td>
<td>
<p>the list of parameters. The complete list of parameters is
available in the <a href="http://xgboost.readthedocs.io/en/latest/parameter.html">online documentation</a>. Below
is a shorter summary:
</p>
<p>1. General Parameters
</p>

<ul>
<li> <p><code>booster</code> which booster to use, can be <code>gbtree</code> or <code>gblinear</code>. Default: <code>gbtree</code>.
</p>
</li></ul>

<p>2. Booster Parameters
</p>
<p>2.1. Parameters for Tree Booster
</p>

<ul>
<li> <p><code>eta</code> control the learning rate: scale the contribution of each tree by a factor of <code>0 &lt; eta &lt; 1</code> when it is added to the current approximation. Used to prevent overfitting by making the boosting process more conservative. Lower value for <code>eta</code> implies larger value for <code>nrounds</code>: low <code>eta</code> value means model more robust to overfitting but slower to compute. Default: 0.3
</p>
</li>
<li> <p><code>gamma</code> minimum loss reduction required to make a further partition on a leaf node of the tree. the larger, the more conservative the algorithm will be.
</p>
</li>
<li> <p><code>max_depth</code> maximum depth of a tree. Default: 6
</p>
</li>
<li> <p><code>min_child_weight</code> minimum sum of instance weight (hessian) needed in a child. If the tree partition step results in a leaf node with the sum of instance weight less than min_child_weight, then the building process will give up further partitioning. In linear regression mode, this simply corresponds to minimum number of instances needed to be in each node. The larger, the more conservative the algorithm will be. Default: 1
</p>
</li>
<li> <p><code>subsample</code> subsample ratio of the training instance. Setting it to 0.5 means that xgboost randomly collected half of the data instances to grow trees and this will prevent overfitting. It makes computation shorter (because less data to analyse). It is advised to use this parameter with <code>eta</code> and increase <code>nrounds</code>. Default: 1
</p>
</li>
<li> <p><code>colsample_bytree</code> subsample ratio of columns when constructing each tree. Default: 1
</p>
</li>
<li> <p><code>lambda</code> L2 regularization term on weights. Default: 1
</p>
</li>
<li> <p><code>alpha</code> L1 regularization term on weights. (there is no L1 reg on bias because it is not important). Default: 0
</p>
</li>
<li> <p><code>num_parallel_tree</code> Experimental parameter. number of trees to grow per round. Useful to test Random Forest through XGBoost (set <code>colsample_bytree &lt; 1</code>, <code>subsample  &lt; 1</code>  and <code>round = 1</code>) accordingly. Default: 1
</p>
</li>
<li> <p><code>monotone_constraints</code> A numerical vector consists of <code>1</code>, <code>0</code> and <code>-1</code> with its length equals to the number of features in the training data. <code>1</code> is increasing, <code>-1</code> is decreasing and <code>0</code> is no constraint.
</p>
</li>
<li> <p><code>interaction_constraints</code> A list of vectors specifying feature indices of permitted interactions. Each item of the list represents one permitted interaction where specified features are allowed to interact with each other. Feature index values should start from <code>0</code> (<code>0</code> references the first column).  Leave argument unspecified for no interaction constraints.
</p>
</li></ul>

<p>2.2. Parameters for Linear Booster
</p>

<ul>
<li> <p><code>lambda</code> L2 regularization term on weights. Default: 0
</p>
</li>
<li> <p><code>lambda_bias</code> L2 regularization term on bias. Default: 0
</p>
</li>
<li> <p><code>alpha</code> L1 regularization term on weights. (there is no L1 reg on bias because it is not important). Default: 0
</p>
</li></ul>

<p>3. Task Parameters
</p>

<ul>
<li> <p><code>objective</code> specify the learning task and the corresponding learning objective, users can pass a self-defined function to it. The default objective options are below:
</p>

<ul>
<li> <p><code>reg:squarederror</code> Regression with squared loss (Default).
</p>
</li>
<li> <p><code>reg:squaredlogerror</code>: regression with squared log loss <code class="reqn">1/2 * (log(pred + 1) - log(label + 1))^2</code>. All inputs are required to be greater than -1. Also, see metric rmsle for possible issue with this objective.
</p>
</li>
<li> <p><code>reg:logistic</code> logistic regression.
</p>
</li>
<li> <p><code>reg:pseudohubererror</code>: regression with Pseudo Huber loss, a twice differentiable alternative to absolute loss.
</p>
</li>
<li> <p><code>binary:logistic</code> logistic regression for binary classification. Output probability.
</p>
</li>
<li> <p><code>binary:logitraw</code> logistic regression for binary classification, output score before logistic transformation.
</p>
</li>
<li> <p><code>binary:hinge</code>: hinge loss for binary classification. This makes predictions of 0 or 1, rather than producing probabilities.
</p>
</li>
<li> <p><code>count:poisson</code>: Poisson regression for count data, output mean of Poisson distribution. <code>max_delta_step</code> is set to 0.7 by default in poisson regression (used to safeguard optimization).
</p>
</li>
<li> <p><code>survival:cox</code>: Cox regression for right censored survival time data (negative values are considered right censored). Note that predictions are returned on the hazard ratio scale (i.e., as HR = exp(marginal_prediction) in the proportional hazard function <code>h(t) = h0(t) * HR)</code>.
</p>
</li>
<li> <p><code>survival:aft</code>: Accelerated failure time model for censored survival time data. See <a href="https://xgboost.readthedocs.io/en/latest/tutorials/aft_survival_analysis.html">Survival Analysis with Accelerated Failure Time</a> for details.
</p>
</li>
<li> <p><code>aft_loss_distribution</code>: Probability Density Function used by <code>survival:aft</code> and <code>aft-nloglik</code> metric.
</p>
</li>
<li> <p><code>multi:softmax</code> set xgboost to do multiclass classification using the softmax objective. Class is represented by a number and should be from 0 to <code>num_class - 1</code>.
</p>
</li>
<li> <p><code>multi:softprob</code> same as softmax, but prediction outputs a vector of ndata * nclass elements, which can be further reshaped to ndata, nclass matrix. The result contains predicted probabilities of each data point belonging to each class.
</p>
</li>
<li> <p><code>rank:pairwise</code> set xgboost to do ranking task by minimizing the pairwise loss.
</p>
</li>
<li> <p><code>rank:ndcg</code>: Use LambdaMART to perform list-wise ranking where <a href="https://en.wikipedia.org/wiki/Discounted_cumulative_gain">Normalized Discounted Cumulative Gain (NDCG)</a> is maximized.
</p>
</li>
<li> <p><code>rank:map</code>: Use LambdaMART to perform list-wise ranking where <a href="https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Mean_average_precision">Mean Average Precision (MAP)</a> is maximized.
</p>
</li>
<li> <p><code>reg:gamma</code>: gamma regression with log-link. Output is a mean of gamma distribution. It might be useful, e.g., for modeling insurance claims severity, or for any outcome that might be <a href="https://en.wikipedia.org/wiki/Gamma_distribution#Applications">gamma-distributed</a>.
</p>
</li>
<li> <p><code>reg:tweedie</code>: Tweedie regression with log-link. It might be useful, e.g., for modeling total loss in insurance, or for any outcome that might be <a href="https://en.wikipedia.org/wiki/Tweedie_distribution#Applications">Tweedie-distributed</a>.
</p>
</li></ul>

</li>
<li> <p><code>base_score</code> the initial prediction score of all instances, global bias. Default: 0.5
</p>
</li>
<li> <p><code>eval_metric</code> evaluation metrics for validation data. Users can pass a self-defined function to it. Default: metric will be assigned according to objective(rmse for regression, and error for classification, mean average precision for ranking). List is provided in detail section.
</p>
</li></ul>
</td></tr>
<tr><td><code id="xgb.train_+3A_data">data</code></td>
<td>
<p>training dataset. <code>xgb.train</code> accepts only an <code>xgb.DMatrix</code> as the input.
<code>xgboost</code>, in addition, also accepts <code>matrix</code>, <code>dgCMatrix</code>, or name of a local data file.</p>
</td></tr>
<tr><td><code id="xgb.train_+3A_nrounds">nrounds</code></td>
<td>
<p>max number of boosting iterations.</p>
</td></tr>
<tr><td><code id="xgb.train_+3A_watchlist">watchlist</code></td>
<td>
<p>named list of xgb.DMatrix datasets to use for evaluating model performance.
Metrics specified in either <code>eval_metric</code> or <code>feval</code> will be computed for each
of these datasets during each boosting iteration, and stored in the end as a field named
<code>evaluation_log</code> in the resulting object. When either <code>verbose&gt;=1</code> or
<code><a href="#topic+cb.print.evaluation">cb.print.evaluation</a></code> callback is engaged, the performance results are continuously
printed out during the training.
E.g., specifying <code>watchlist=list(validation1=mat1, validation2=mat2)</code> allows to track
the performance of each round's model on mat1 and mat2.</p>
</td></tr>
<tr><td><code id="xgb.train_+3A_obj">obj</code></td>
<td>
<p>customized objective function. Returns gradient and second order
gradient with given prediction and dtrain.</p>
</td></tr>
<tr><td><code id="xgb.train_+3A_feval">feval</code></td>
<td>
<p>customized evaluation function. Returns
<code>list(metric='metric-name', value='metric-value')</code> with given
prediction and dtrain.</p>
</td></tr>
<tr><td><code id="xgb.train_+3A_verbose">verbose</code></td>
<td>
<p>If 0, xgboost will stay silent. If 1, it will print information about performance.
If 2, some additional information will be printed out.
Note that setting <code>verbose &gt; 0</code> automatically engages the
<code>cb.print.evaluation(period=1)</code> callback function.</p>
</td></tr>
<tr><td><code id="xgb.train_+3A_print_every_n">print_every_n</code></td>
<td>
<p>Print each n-th iteration evaluation messages when <code>verbose&gt;0</code>.
Default is 1 which means all messages are printed. This parameter is passed to the
<code><a href="#topic+cb.print.evaluation">cb.print.evaluation</a></code> callback.</p>
</td></tr>
<tr><td><code id="xgb.train_+3A_early_stopping_rounds">early_stopping_rounds</code></td>
<td>
<p>If <code>NULL</code>, the early stopping function is not triggered.
If set to an integer <code>k</code>, training with a validation set will stop if the performance
doesn't improve for <code>k</code> rounds.
Setting this parameter engages the <code><a href="#topic+cb.early.stop">cb.early.stop</a></code> callback.</p>
</td></tr>
<tr><td><code id="xgb.train_+3A_maximize">maximize</code></td>
<td>
<p>If <code>feval</code> and <code>early_stopping_rounds</code> are set,
then this parameter must be set as well.
When it is <code>TRUE</code>, it means the larger the evaluation score the better.
This parameter is passed to the <code><a href="#topic+cb.early.stop">cb.early.stop</a></code> callback.</p>
</td></tr>
<tr><td><code id="xgb.train_+3A_save_period">save_period</code></td>
<td>
<p>when it is non-NULL, model is saved to disk after every <code>save_period</code> rounds,
0 means save at the end. The saving is handled by the <code><a href="#topic+cb.save.model">cb.save.model</a></code> callback.</p>
</td></tr>
<tr><td><code id="xgb.train_+3A_save_name">save_name</code></td>
<td>
<p>the name or path for periodically saved model file.</p>
</td></tr>
<tr><td><code id="xgb.train_+3A_xgb_model">xgb_model</code></td>
<td>
<p>a previously built model to continue the training from.
Could be either an object of class <code>xgb.Booster</code>, or its raw data, or the name of a
file with a previously saved model.</p>
</td></tr>
<tr><td><code id="xgb.train_+3A_callbacks">callbacks</code></td>
<td>
<p>a list of callback functions to perform various task during boosting.
See <code><a href="#topic+callbacks">callbacks</a></code>. Some of the callbacks are automatically created depending on the
parameters' values. User can provide either existing or their own callback methods in order
to customize the training process.</p>
</td></tr>
<tr><td><code id="xgb.train_+3A_...">...</code></td>
<td>
<p>other parameters to pass to <code>params</code>.</p>
</td></tr>
<tr><td><code id="xgb.train_+3A_label">label</code></td>
<td>
<p>vector of response values. Should not be provided when data is
a local data file name or an <code>xgb.DMatrix</code>.</p>
</td></tr>
<tr><td><code id="xgb.train_+3A_missing">missing</code></td>
<td>
<p>by default is set to NA, which means that NA values should be considered as 'missing'
by the algorithm. Sometimes, 0 or other extreme value might be used to represent missing values.
This parameter is only used when input is a dense matrix.</p>
</td></tr>
<tr><td><code id="xgb.train_+3A_weight">weight</code></td>
<td>
<p>a vector indicating the weight for each row of the input.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These are the training functions for <code>xgboost</code>.
</p>
<p>The <code>xgb.train</code> interface supports advanced features such as <code>watchlist</code>,
customized objective and evaluation metric functions, therefore it is more flexible
than the <code>xgboost</code> interface.
</p>
<p>Parallelization is automatically enabled if <code>OpenMP</code> is present.
Number of threads can also be manually specified via the <code>nthread</code>
parameter.
</p>
<p>The evaluation metric is chosen automatically by XGBoost (according to the objective)
when the <code>eval_metric</code> parameter is not provided.
User may set one or several <code>eval_metric</code> parameters.
Note that when using a customized metric, only this single metric can be used.
The following is the list of built-in metrics for which XGBoost provides optimized implementation:
</p>

<ul>
<li> <p><code>rmse</code> root mean square error. <a href="https://en.wikipedia.org/wiki/Root_mean_square_error">https://en.wikipedia.org/wiki/Root_mean_square_error</a>
</p>
</li>
<li> <p><code>logloss</code> negative log-likelihood. <a href="https://en.wikipedia.org/wiki/Log-likelihood">https://en.wikipedia.org/wiki/Log-likelihood</a>
</p>
</li>
<li> <p><code>mlogloss</code> multiclass logloss. <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html">https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html</a>
</p>
</li>
<li> <p><code>error</code> Binary classification error rate. It is calculated as <code>(# wrong cases) / (# all cases)</code>.
By default, it uses the 0.5 threshold for predicted values to define negative and positive instances.
Different threshold (e.g., 0.) could be specified as &quot;error@0.&quot;
</p>
</li>
<li> <p><code>merror</code> Multiclass classification error rate. It is calculated as <code>(# wrong cases) / (# all cases)</code>.
</p>
</li>
<li> <p><code>mae</code> Mean absolute error
</p>
</li>
<li> <p><code>mape</code> Mean absolute percentage error
</p>
</li>
<li> <p><code>auc</code> Area under the curve. <a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic#'Area_under_curve">https://en.wikipedia.org/wiki/Receiver_operating_characteristic#'Area_under_curve</a> for ranking evaluation.
</p>
</li>
<li> <p><code>aucpr</code> Area under the PR curve. <a href="https://en.wikipedia.org/wiki/Precision_and_recall">https://en.wikipedia.org/wiki/Precision_and_recall</a> for ranking evaluation.
</p>
</li>
<li> <p><code>ndcg</code> Normalized Discounted Cumulative Gain (for ranking task). <a href="https://en.wikipedia.org/wiki/NDCG">https://en.wikipedia.org/wiki/NDCG</a>
</p>
</li></ul>

<p>The following callbacks are automatically created when certain parameters are set:
</p>

<ul>
<li> <p><code>cb.print.evaluation</code> is turned on when <code>verbose &gt; 0</code>;
and the <code>print_every_n</code> parameter is passed to it.
</p>
</li>
<li> <p><code>cb.evaluation.log</code> is on when <code>watchlist</code> is present.
</p>
</li>
<li> <p><code>cb.early.stop</code>: when <code>early_stopping_rounds</code> is set.
</p>
</li>
<li> <p><code>cb.save.model</code>: when <code>save_period &gt; 0</code> is set.
</p>
</li></ul>



<h3>Value</h3>

<p>An object of class <code>xgb.Booster</code> with the following elements:
</p>

<ul>
<li> <p><code>handle</code> a handle (pointer) to the xgboost model in memory.
</p>
</li>
<li> <p><code>raw</code> a cached memory dump of the xgboost model saved as R's <code>raw</code> type.
</p>
</li>
<li> <p><code>niter</code> number of boosting iterations.
</p>
</li>
<li> <p><code>evaluation_log</code> evaluation history stored as a <code>data.table</code> with the
first column corresponding to iteration number and the rest corresponding to evaluation
metrics' values. It is created by the <code><a href="#topic+cb.evaluation.log">cb.evaluation.log</a></code> callback.
</p>
</li>
<li> <p><code>call</code> a function call.
</p>
</li>
<li> <p><code>params</code> parameters that were passed to the xgboost library. Note that it does not
capture parameters changed by the <code><a href="#topic+cb.reset.parameters">cb.reset.parameters</a></code> callback.
</p>
</li>
<li> <p><code>callbacks</code> callback functions that were either automatically assigned or
explicitly passed.
</p>
</li>
<li> <p><code>best_iteration</code> iteration number with the best evaluation metric value
(only available with early stopping).
</p>
</li>
<li> <p><code>best_score</code> the best evaluation metric value during early stopping.
(only available with early stopping).
</p>
</li>
<li> <p><code>feature_names</code> names of the training dataset features
(only when column names were defined in training data).
</p>
</li>
<li> <p><code>nfeatures</code> number of features in training data.
</p>
</li></ul>



<h3>References</h3>

<p>Tianqi Chen and Carlos Guestrin, &quot;XGBoost: A Scalable Tree Boosting System&quot;,
22nd SIGKDD Conference on Knowledge Discovery and Data Mining, 2016, <a href="https://arxiv.org/abs/1603.02754">https://arxiv.org/abs/1603.02754</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+callbacks">callbacks</a></code>,
<code><a href="#topic+predict.xgb.Booster">predict.xgb.Booster</a></code>,
<code><a href="#topic+xgb.cv">xgb.cv</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(agaricus.train, package='xgboost')
data(agaricus.test, package='xgboost')

## Keep the number of threads to 1 for examples
nthread &lt;- 1
data.table::setDTthreads(nthread)

dtrain &lt;- with(
  agaricus.train, xgb.DMatrix(data, label = label, nthread = nthread)
)
dtest &lt;- with(
  agaricus.test, xgb.DMatrix(data, label = label, nthread = nthread)
)
watchlist &lt;- list(train = dtrain, eval = dtest)

## A simple xgb.train example:
param &lt;- list(max_depth = 2, eta = 1, verbose = 0, nthread = nthread,
              objective = "binary:logistic", eval_metric = "auc")
bst &lt;- xgb.train(param, dtrain, nrounds = 2, watchlist)

## An xgb.train example where custom objective and evaluation metric are
## used:
logregobj &lt;- function(preds, dtrain) {
   labels &lt;- getinfo(dtrain, "label")
   preds &lt;- 1/(1 + exp(-preds))
   grad &lt;- preds - labels
   hess &lt;- preds * (1 - preds)
   return(list(grad = grad, hess = hess))
}
evalerror &lt;- function(preds, dtrain) {
  labels &lt;- getinfo(dtrain, "label")
  err &lt;- as.numeric(sum(labels != (preds &gt; 0)))/length(labels)
  return(list(metric = "error", value = err))
}

# These functions could be used by passing them either:
#  as 'objective' and 'eval_metric' parameters in the params list:
param &lt;- list(max_depth = 2, eta = 1, verbose = 0, nthread = nthread,
              objective = logregobj, eval_metric = evalerror)
bst &lt;- xgb.train(param, dtrain, nrounds = 2, watchlist)

#  or through the ... arguments:
param &lt;- list(max_depth = 2, eta = 1, verbose = 0, nthread = nthread)
bst &lt;- xgb.train(param, dtrain, nrounds = 2, watchlist,
                 objective = logregobj, eval_metric = evalerror)

#  or as dedicated 'obj' and 'feval' parameters of xgb.train:
bst &lt;- xgb.train(param, dtrain, nrounds = 2, watchlist,
                 obj = logregobj, feval = evalerror)


## An xgb.train example of using variable learning rates at each iteration:
param &lt;- list(max_depth = 2, eta = 1, verbose = 0, nthread = nthread,
              objective = "binary:logistic", eval_metric = "auc")
my_etas &lt;- list(eta = c(0.5, 0.1))
bst &lt;- xgb.train(param, dtrain, nrounds = 2, watchlist,
                 callbacks = list(cb.reset.parameters(my_etas)))

## Early stopping:
bst &lt;- xgb.train(param, dtrain, nrounds = 25, watchlist,
                 early_stopping_rounds = 3)

## An 'xgboost' interface example:
bst &lt;- xgboost(data = agaricus.train$data, label = agaricus.train$label,
               max_depth = 2, eta = 1, nthread = nthread, nrounds = 2,
               objective = "binary:logistic")
pred &lt;- predict(bst, agaricus.test$data)

</code></pre>

<hr>
<h2 id='xgb.unserialize'>Load the instance back from <code><a href="#topic+xgb.serialize">xgb.serialize</a></code></h2><span id='topic+xgb.unserialize'></span>

<h3>Description</h3>

<p>Load the instance back from <code><a href="#topic+xgb.serialize">xgb.serialize</a></code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>xgb.unserialize(buffer, handle = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="xgb.unserialize_+3A_buffer">buffer</code></td>
<td>
<p>the buffer containing booster instance saved by <code><a href="#topic+xgb.serialize">xgb.serialize</a></code></p>
</td></tr>
<tr><td><code id="xgb.unserialize_+3A_handle">handle</code></td>
<td>
<p>An <code>xgb.Booster.handle</code> object which will be overwritten with
the new deserialized object. Must be a null handle (e.g. when loading the model through
'readRDS'). If not provided, a new handle will be created.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An <code>xgb.Booster.handle</code> object.
</p>

<hr>
<h2 id='xgboost-deprecated'>Deprecation notices.</h2><span id='topic+xgboost-deprecated'></span>

<h3>Description</h3>

<p>At this time, some of the parameter names were changed in order to make the code style more uniform.
The deprecated parameters would be removed in the next release.
</p>


<h3>Details</h3>

<p>To see all the current deprecated and new parameters, check the <code>xgboost:::depr_par_lut</code> table.
</p>
<p>A deprecation warning is shown when any of the deprecated parameters is used in a call.
An additional warning is shown when there was a partial match to a deprecated parameter
(as R is able to partially match parameter names).
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
