<!DOCTYPE html><html lang="en"><head><title>Help for package catlearn</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {catlearn}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#act2probrat'>
<p>Convert output activation to a rating of outcome probability</p></a></li>
<li><a href='#catlearn-package'>
<p>Formal Modeling for Psychology.</p>
</p></a></li>
<li><a href='#convertSUSTAIN'>
<p>Convert nominal-dimension input representation into a 'padded'</p>
(slpSUSTAIN) format</a></li>
<li><a href='#homa76'><p>Category breadth CIRP</p></a></li>
<li><a href='#krus96'><p>Inverse Base-rate Effect AP</p></a></li>
<li><a href='#krus96exit'>
<p>Simulation of AP krus96 with EXIT model</p>
</p></a></li>
<li><a href='#krus96train'>
<p>Input representation of krus96 for models input-compatible with</p>
slpEXIT
</p></a></li>
<li><a href='#medin87train'>
<p>Input representation of Exp. 1 in Medin et al. (1987) for models</p>
input-compatible with slpALCOVE or slpSUSTAIN.
</p></a></li>
<li><a href='#nosof88'><p>Instantiation frequency CIRP</p></a></li>
<li><a href='#nosof88exalcove'>
<p>Simulation of CIRP nosof88 with ex-ALCOVE model</p>
</p></a></li>
<li><a href='#nosof88exalcove_opt'>
<p>Parameter optimization of ex-ALCOVE model with nosof88 CIRP</p>
</p></a></li>
<li><a href='#nosof88oat'>
<p>Ordinal adequacy test for simulations of nosof88 CIRP</p>
</p></a></li>
<li><a href='#nosof88protoalcove'>
<p>Simulation of CIRP nosof88 with proto-ALCOVE model</p>
</p></a></li>
<li><a href='#nosof88protoalcove_opt'>
<p>Parameter optimization of proto-ALCOVE model with nosof88 CIRP</p>
</p></a></li>
<li><a href='#nosof88train'>
<p>Input representation of nosof88 for models input-compatible with</p>
slpALCOVE.
</p></a></li>
<li><a href='#nosof94'><p>Type I-VI category structure CIRP</p></a></li>
<li><a href='#nosof94bnalcove'>
<p>Simulation of CIRP nosof94 with BN-ALCOVE model</p>
</p></a></li>
<li><a href='#nosof94exalcove'>
<p>Simulation of CIRP nosof94 with ex-ALCOVE model</p>
</p></a></li>
<li><a href='#nosof94exalcove_opt'>
<p>Parameter optimization of ex-ALCOVE model with nosof94 CIRP</p>
</p></a></li>
<li><a href='#nosof94oat'>
<p>Ordinal adequacy test for simulations of nosof94 CIRP</p>
</p></a></li>
<li><a href='#nosof94plot'>
<p>Plot Nosofsky et al. (1994) data / simulations</p>
</p></a></li>
<li><a href='#nosof94sustain'>
<p>Simulation of CIRP nosof94 with the SUSTAIN model</p>
</p></a></li>
<li><a href='#nosof94train'>
<p>Input representation of nosof94 for models input-compatible with</p>
slpALCOVE or slpSUSTAIN
</p></a></li>
<li><a href='#shin92'><p>Category size CIRP</p></a></li>
<li><a href='#shin92exalcove'>
<p>Simulation of CIRP shin92 with ex-ALCOVE model</p>
</p></a></li>
<li><a href='#shin92exalcove_opt'>
<p>Parameter optimization of ex-ALCOVE model with shin92 CIRP</p>
</p></a></li>
<li><a href='#shin92oat'>
<p>Ordinal adequacy test for simulations of shin92 CIRP</p>
</p></a></li>
<li><a href='#shin92protoalcove'>
<p>Simulation of CIRP shin92 with proto-ALCOVE model</p>
</p></a></li>
<li><a href='#shin92protoalcove_opt'>
<p>Parameter optimization of proto-ALCOVE model with shin92 CIRP</p>
</p></a></li>
<li><a href='#shin92train'>
<p>Input representation of shin92 for models input-compatible with</p>
slpALCOVE.
</p></a></li>
<li><a href='#slpALCOVE'>
<p>ALCOVE category learning model</p></a></li>
<li><a href='#slpBM'>
<p>Bush &amp; Mosteller (1951) simple associative learning model</p></a></li>
<li><a href='#slpCOVIS'>
<p>COVIS category learning model</p></a></li>
<li><a href='#slpDGCM'>
<p>Similarity-Dissimilarity Generalized Context Model (DGCM)</p></a></li>
<li><a href='#slpDIVA'><p>DIVA category learning model</p></a></li>
<li><a href='#slpEXIT'>
<p>EXIT Category Learning Model</p></a></li>
<li><a href='#slpLMSnet'>
<p>Gluck &amp; Bower (1988) network model</p></a></li>
<li><a href='#slpMack75'>
<p>Mackintosh (1975) associative learning model</p></a></li>
<li><a href='#slpMBMF'>
<p>MB/MF reinforcement learning model</p></a></li>
<li><a href='#slpNNCAG'>
<p>A Neural Network with Competitive Attentional Gating (NNCAG)</p></a></li>
<li><a href='#slpNNRAS'>
<p>A Neural Network with Rapid Attentional Shifts (NNRAS)</p></a></li>
<li><a href='#slpRW'>
<p>Rescorla-Wagner (1972) associative learning model.</p></a></li>
<li><a href='#slpSUSTAIN'>
<p>SUSTAIN Category Learning Model</p></a></li>
<li><a href='#ssecl'><p>Sum of squared errors</p></a></li>
<li><a href='#stsimGCM'>
<p>Generalized Context Model</p></a></li>
<li><a href='#thegrid'><p>Ordinal adequacy results for all catlearn simulations</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Formal Psychological Models of Categorization and Learning</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-04-04</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Author:</td>
<td>Andy Wills, Lenard Dome, Charlotte Edmunds, Garrett Honke, Angus Inkster, René Schlegelmilch, Stuart Spicer</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Andy Wills &lt;andy@willslab.co.uk&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Formal psychological models of categorization and learning, independently-replicated data sets against which to test them, and simulation archives.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp (&ge; 1.0.0), doParallel, foreach, tidyr, dplyr</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo (&ge; 0.10.7.5.0)</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5)</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-04-04 10:42:49 UTC; andy</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-04-04 14:00:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='act2probrat'>
Convert output activation to a rating of outcome probability
</h2><span id='topic+act2probrat'></span>

<h3>Description</h3>

<p>Logistic function to convert output activations to rating of
outcome probability (see e.g. Gluck &amp; Bower, 1988).
</p>


<h3>Usage</h3>

<pre><code class='language-R'> act2probrat(act, theta, beta) </code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="act2probrat_+3A_act">act</code></td>
<td>
<p>Vector of output activations</p>
</td></tr>
<tr><td><code id="act2probrat_+3A_theta">theta</code></td>
<td>
<p>Scaling constant</p>
</td></tr>
<tr><td><code id="act2probrat_+3A_beta">beta</code></td>
<td>
<p>Bias constant</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The contents of this help file are relatively brief; a more extensive
tutorial on using act2probrat can be found in Spicer et al. (n.d.).
</p>
<p>The function takes the output activation of a learning model
(e.g. slpRW), and converts it into a rating of the subjective
probability that the outcome will occur. It does this separately for
each activation in the vector <code>act</code>. It uses a logistic function
to do this conversion (see e.g. Gluck &amp; Bower, 1988, Equation 7). This
function can produce a variety of monotonic mappings from activation
to probability rating, determined by the value set for the two
constants: 
</p>
<p><code>theta</code> is a scaling constant; as its value rises, the function
relating activation to rating becomes less linear and at high values
approximates a step function.
</p>
<p><code>beta</code> is a bias parameter; it is the value of the output
activation that results in an output rating of P = 0.5. For example,
if you wish an output activation of 0.4 to produce a rated probability
of 0.5, set beta to 0.4.
</p>


<h3>Value</h3>

<p>Returns a vector of probability ratings.</p>


<h3>Note</h3>

<p>As this function returns probabilities, the numbers returned are
always in the range 0-1. If the data you are fitting use a
different range, convert them. For example, if your data are ratings
on a 0-10 scale, divide them by 10.  If your data are something other
than probability estimates (e.g. you asked participants to use
negative ratings to indicate preventative relationships), don't use this
function unless you are sure it is doing what you intend. 
</p>


<h3>Author(s)</h3>

<p>Andy Wills
</p>


<h3>References</h3>

<p>Gluck, M.A. &amp; Bower, G.H. (1988). From conditioning to category
learning: An adaptive network model. <em>Journal of Experimental
Psychology: General, 117</em>, 227-247.
</p>
<p>Spicer, S., Jones, P.M., Inkster, A.B., Edmunds, C.E.R. &amp; Wills,
A.J. (n.d.). Progress in learning theory through distributed
collaboration: Concepts, tools, and examples. <em>Manuscript in
preparation</em>.
</p>

<hr>
<h2 id='catlearn-package'>
Formal Modeling for Psychology.
</h2><span id='topic+catlearn'></span>

<h3>Description</h3>

<p>Formal psychological models, independently-replicated data sets
against which to test them, and simulation archives.
</p>


<h3>Details</h3>

<p>For a complete list of functions, use <code>library(help =
    "catlearn")</code>.
</p>
<p>For a complete table of simulations, use <code>data(thegrid)</code>.
</p>
<p>All functions are concisely documented, use the help function e.g
<code>?shin92</code>.
</p>
<p>For more detailed documentation, see the references listed by the help
documentation.
</p>
<p>For a tutorial introduction, see Wills et al. (2016a).
</p>
<p>For a guide to contributing to this package, Catlearn Research Group
(2016).
</p>


<h3>Author(s)</h3>

<p>Andy Wills
</p>
<p>Maintainer: Andy Wills <a href="mailto:andy@willslab.co.uk">andy@willslab.co.uk</a>
</p>


<h3>References</h3>

<p>Catlearn Research Group (2016). Contributing to catlearn.
<a href="http://catlearn.r-forge.r-project.org/intro-catlearn.pdf">http://catlearn.r-forge.r-project.org/intro-catlearn.pdf</a>
</p>
<p>Wills, A.J., O'Connell, G., Edmunds, C.E.R. &amp; Inkster, A.B. (2016).
Progress in modeling through distributed collaboration: Concepts,
tools, and category-learning examples. <em>The Psychology of
Learning and Motivation</em>.
</p>

<hr>
<h2 id='convertSUSTAIN'>
Convert nominal-dimension input representation into a 'padded' 
(slpSUSTAIN) format
</h2><span id='topic+convertSUSTAIN'></span>

<h3>Description</h3>

<p>Changes a nominal-dimension input representation (e.g. 3 1 2) into a
padded representation (e.g. 001 100 010). This form out input
representation is required by e.g. <code>slpSUSTAIN</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>convertSUSTAIN(input, dims)</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="convertSUSTAIN_+3A_input">input</code></td>
<td>
<p>A matrix containing the nominal-dimension input
representation. Each row is a trial and each column is a stimulus
dimension.</p>
</td></tr>
<tr><td><code id="convertSUSTAIN_+3A_dims">dims</code></td>
<td>
<p>A vector of the number of nominal values for each
dimension.  For example, if there are three dimensions with three,
one and two possible values, then <code>dims = c(3, 1, 2)</code>.</p>
</td></tr>  </table>


<h3>Value</h3>

<p>Returns a matrix containing the padded input representation.</p>


<h3>Author(s)</h3>

<p>Lenard Dome, Andy Wills
</p>


<h3>See Also</h3>

<p><code><a href="#topic+slpSUSTAIN">slpSUSTAIN</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Create a dummy training matrix with two dimensions. The first
## two dimensions have two possible nominal values, while the
## third and fourth have three possible nominal values.

dummy &lt;- cbind(matrix(sample(1:2, 20, replace=TRUE), ncol = 2),
               matrix(sample(1:3, 20, replace=TRUE), ncol = 2))

## Specify the number of nominal spaces for each dimension
dims &lt;- c(2, 2, 3, 3)

## Convert the input representation into a binary padded representation
convertSUSTAIN(dummy, dims)

</code></pre>

<hr>
<h2 id='homa76'>Category breadth CIRP</h2><span id='topic+homa76'></span>

<h3>Description</h3>

<p>In some category-learning experiments, category members are distortions
of an underlying base pattern. Where this is the case, 'category breadth'
refers to the magnitude of such distortions. Broad categories take
longer to learn than narrow categories. Once trained to an errorless
criterion, the effect of category breadth on performance on novel items
depends on category size. For small categories, narrow categories are
better than broad ones. For larger categries, the reverse is true. Homa
&amp; Vosburgh (1976) provide the data for this CIRP.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(homa76)</code></pre>


<h3>Format</h3>

<p>A data frame with the following columns:
</p>

<dl>
<dt>phase</dt><dd><p>Experimental phase (within-subjects). Takes values :
'train','imm'. The training phase is 'train', 'imm' is the immediate
test phase.</p>
</dd>
<dt>cond</dt><dd><p>Category breadth (between-subjects). Takes values :
'mixed', 'uni-low'</p>
</dd>
<dt>stim</dt><dd><p>Stimulus type (within-subjects). Takes values : 'proto',
'low', 'med', 'high', 'old-low', 'old-med', 'old-high', 'rand'. All
refer to novel stimuli in the test phase, except those beginning
'old-', which are stimuli from the training phase presented during
the test phase. 'low', 'med', 'high' refer to distortion
level. 'proto' are prototypes. 'rand' are a set of 10 random
stimuli, generated from prototypes unrelated to those used in
training. These random stimuli are not mentioned in the Method of
the paper, but are mentioned in the Results section - they are
presented at the end of the test session. Empty cell for training
phase.</p>
</dd>
<dt>catsize</dt><dd><p>Category size (within-subjects). Takes values : 3, 6,
9. NA for training phase, where category size is not a meaningful
variable given that the DV is blocks to criterion. Also NA for old
stimuli; Homa &amp; Vosburgh's (1976) Results section collapses across
category size for old stimuli</p>
</dd>
<dt>val</dt><dd><p>For test phases: probability of a correct response, except
for random stimuli, where 'val' is the probability with which the
random stimuli were placed into the specified category. For training
phase: number of blocks to criterion</p>
</dd>
</dl>



<h3>Details</h3>

<p>Wills et al. (n.d.) discuss the derivation of this CIRP. In brief, the
effects have been independently replicated. Homa &amp; Vosburgh (1976) was
selected as the only experiment to contain all three independently
replicated effects. 
</p>
<p>Homa &amp; Vosburgh's experiment involved the supervised classification of
nine-dot random dot patterns. Stimuli had three different levels of
distortion from the prototype - low (3.5 bit), medium (5.6 bit), and
high (7.7 bit). There were three categories in training, one with 3
members, one with 6 members, and one with 9 members. Participants were
either trained on stimuli that were all low distortion (narrow
categories), or on an equal mix of low, medium, and high distortion
stimuli (broad categories). Training was to an errorless criterion. The
test phase involved the presentation of the prototypes, old stimuli, and
novel stimuli of low, medium, and high distortion. 
</p>
<p>The data for the prototype, and other novel test stimuli, were estimated
from Figure 1 of Homa &amp; Vosburgh (1976), using <code>plot
digitizer</code> (Huwaldt, 2015). The data for old stimuli were estimated from
Figure 3, using the same procedure. The data for the training phase,
and for random stimuli, were reported in the text of Homa &amp; Vosburgh
(1976) and are reproduced here. All data are averages across participants.
</p>
<p>Homa &amp; Vosburgh's (1976) experiment also includes results for further
test phases, delayed by either 1 week, or 10 weeks, from the day of
training. These data are not the focus of this category breadth CIRP and
have not been included.
</p>


<h3>Source</h3>

<p>Homa, D. &amp; Vosburgh, R. (1976). Category breadth and the abstraction
of prototypical information. <em>Journal of Experimental Psychology:
Human Learning and Memory, 2</em>, 322-330.
</p>
<p>Huwaldt, J.A. (2015). Plot Digitizer
[software]. <a href="https://plotdigitizer.sourceforge.net/">https://plotdigitizer.sourceforge.net/</a>
</p>
<p>Wills et al. (n.d.). Benchmarks for category learning. <em>Manuscript
in preparation</em>.
</p>

<hr>
<h2 id='krus96'>Inverse Base-rate Effect AP</h2><span id='topic+krus96'></span>

<h3>Description</h3>

<p>In the inverse base-rate effect, participants are trained that a
compound of two cues (I + PC) leads to a frequently-occurring outcome
(C), while another two-cue compound (I + PR) leads to a
rarely-occuring outcome (R). The key results are that, at test,
participants tend to respond 'C' to cue I on its own, but 'R' to the
cue compound (PC + PR). This latter response is striking because
PC and PR had been perfectly predictive of diseases C and R
respectively, and disease C is more common, so the optimal response to
PC + PR is 'C'. Participants respond in opposition to the underlying
disease base rates.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(krus96)</code></pre>


<h3>Format</h3>

<p>A data frame with the following columns:
</p>

<dl>
<dt>symptom</dt><dd><p>Symptom presented. Take values: I, PC, PR, PC+PR,
I+PC+PR, I+PCo, I+PRo, PC+PRo, I+PC+PRo, as defined by Kruschke
(1996).</p>
</dd>
<dt>disease</dt><dd><p>Response made. Takes values: C, R, Co, Ro, as defined
by Kruschke (1996).</p>
</dd>
<dt>prop</dt><dd><p>Mean probability of response, averaged across participants.</p>
</dd>
</dl>



<h3>Details</h3>

<p>Wills et al. (n.d.) discuss the classification of these data as a
Auxilliary Phenomenon, rather than a CIRP (Canonical Independently
Replicated Phenomenon). In brief, these particular results have been
independently replicated, but are arguably not the best exemplar of
the known phenomena in this area (in particular, they lack a
demonstration of the shared-cue effect in IBRE). Auxilliary Phenomena
may be included in catlearn if are the subject of a simulation
archived in catlearn. 
</p>
<p>The data are from Experiment 1 of Kruschke (1996), which involved the
diagnosis of hyopthetical diseases (F, G, H, J) on the basis of
symptoms presented as text (e.g. &quot;ear aches, skin rash&quot;). Participants
were trained with feedback across 15 blocks of 8 trials each. They
were then tested without feedback on 18 test stimuli, each presented
twice.
</p>
<p>The data are as shown in Table 2 of Kruschke (1996). The data are mean
response probabilities for each stimulus in the test phase, averaged
across the two presentations of the stimulus, the two copies of the
abstract design, and across participants.
</p>


<h3>Author(s)</h3>

<p>Andy J. Wills, René Schlegelmilch
</p>


<h3>Source</h3>

<p>Kruschke, J.K. (1996). Base rates in category learning. <em>Journal
of Experimental Psychology: Learning, Memory, and Cognition, 22</em>,
3-26.
</p>


<h3>References</h3>

<p>Wills et al. (n.d.). Benchmarks for category
learning. <em>Manuscript in preparation</em>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+krus96train">krus96train</a></code>   
</p>

<hr>
<h2 id='krus96exit'>
Simulation of AP krus96 with EXIT model
</h2><span id='topic+krus96exit'></span>

<h3>Description</h3>

<p>Runs a simulation of the <code><a href="#topic+krus96">krus96</a></code> AP using the
<code><a href="#topic+slpEXIT">slpEXIT</a></code> model implementation and
<code><a href="#topic+krus96train">krus96train</a></code> as the input representation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
  krus96exit (params = c(2.87, 2.48, 4.42, 4.42, .222, 1.13, .401))

</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="krus96exit_+3A_params">params</code></td>
<td>
<p>A vector containing values for c, P, phi, l_gain,
l_weight, l_ex, and sigma_bias (i.e. the sigma for the bias unit), in
that order. See <code><a href="#topic+slpEXIT">slpEXIT</a></code> for an explanation of these
parameters.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A simulation using <code><a href="#topic+slpEXIT">slpEXIT</a></code> and
<code><a href="#topic+krus96train">krus96train</a></code>. The stored exemplars are the four stimuli
present during the training phase, using the same representation as in
<code><a href="#topic+krus96train">krus96train</a></code>.
</p>
<p>Other parameters of slpEXIT are set as follows: <code>iterations</code> =
10, sigma for the non-bias units = 1. These values are conventions of
modeling with EXIT, and should not be considered as free
parameters. They are set within the <code>krus96exit</code> function,
and hence can't be changed without re-writing the function.
</p>
<p>This simulation is discussed in Spicer et al. (n.d.). It produces the
same response probabilities (within rounding error) as the simulation
reported in Kruschke (2001), with the same parameters.
</p>
<p>56 simulated participants are used in this simulation, the same number
as used by Kruschke (2001). Kruschke reports using the same trial
randomizations as used for his 56 real participants. These
randomizations were not published, so it we couldn't reproduce that
part of his simulation. It turns out that the choice of set of 56
randomizations matters, it affects some of the predicted response
probabilities. We chose a random seed that reproduced Kruschke's
response probabilities to within rounding error. As luck would have
it, Kruschke's reported response probabilities (and hence this
simulation) are the same (within rounding error) as the results of 
large sample (N = 500) simulations we have run.
</p>


<h3>Value</h3>

<p>A matrix of predicted response probabilities, in the same order and
format as the observed data contained in <code><a href="#topic+krus96">krus96</a></code>.
</p>


<h3>Author(s)</h3>

<p>René Schlegelmilch, Andy Wills</p>


<h3>References</h3>

<p>Kruschke, J. K. (2001). The inverse base rate effect is not explained
by eliminative inference. <em>Journal of Experimental Psychology:
Learning, Memory &amp; Cognition, 27</em>, 1385-1400.
</p>
<p>Spicer, S.G., Schlegelmilch, R., Jones, P.M., Inkster, A.B., Edmunds,
C.E.R. &amp; Wills, A.J. (n.d.). Progress in learning theory through
distributed collaboration: Concepts, tools, and
examples. <em>Manuscript in preparation</em>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+krus96">krus96</a></code>, <code><a href="#topic+krus96train">krus96train</a></code>, <code><a href="#topic+slpEXIT">slpEXIT</a></code>
</p>

<hr>
<h2 id='krus96train'>
Input representation of krus96 for models input-compatible with
slpEXIT
</h2><span id='topic+krus96train'></span>

<h3>Description</h3>

<p>Create randomized training blocks for AP <code>krus96</code>, in a format
suitable for the <code>slpEXIT</code> model, and other models that use the
same input representation format.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
krus96train(blocks = 15, subjs = 56, ctxt = TRUE, seed = 1)


</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="krus96train_+3A_blocks">blocks</code></td>
<td>
<p>Number of training blocks to generate. Omit this
argument to get the same number of blocks (15) as used in
<code>krus96</code>.</p>
</td></tr>
<tr><td><code id="krus96train_+3A_subjs">subjs</code></td>
<td>
<p>Number of simulated subjects to be run.</p>
</td></tr>
<tr><td><code id="krus96train_+3A_ctxt">ctxt</code></td>
<td>
<p>If <code>TRUE</code>, include a context cue (<code>x7</code>) that
appears on every trial.</p>
</td></tr>
<tr><td><code id="krus96train_+3A_seed">seed</code></td>
<td>
<p>Sets the random seed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A data frame is produced, with one row for each trial, and with the
following columns:
</p>
<p><code>ctrl</code> - Set to 1 (reset model) for trial 1 of each simulated
subject, set to zero (normal trial) for all other training trials, and
set to 2 for test trials (i.e. those with no feedback). 
</p>
<p><code>block</code> - training block
</p>
<p><code>stim</code> - Stimulus code, as described in Kruschke (1996). 
</p>
<p><code>x1, x2, ...</code> - symptom representation. Each column represents
one symptom, in the order I1, PC1, PR1, I2, PC2, PR2, context. 1 =
symptom present, 0 = symptom absent
</p>
<p><code>t1, t2, ...</code> - Disease representation. Each column represents
one disease, in the order C1, R1, C2, R2. 1 = disease present. 0 =
disease absent.
</p>
<p>Although the trial ordering is random, a random seed is used, so
multiple calls of this function with the same parameters should produce
the same output. This is usually desirable for reproducibility and
stability of non-linear optimization. To get a different order, use the
seed argument to set a different seed.
</p>
<p>This routine was originally developed to support Wills et al. (n.d.).
</p>


<h3>Value</h3>

<p>A data frame, where each row is one trial, and the columns contain
model input.
</p>


<h3>Author(s)</h3>

<p>René Schlegelmilch, Andy Wills
</p>


<h3>References</h3>

<p>Kruschke, J.K. (1996). Base rates in category learning. <em>Journal
of Experimental Psychology: Learning, Memory, and Cognition, 22</em>,
3-26.
</p>
<p>Wills et al. (n.d.). Benchmarks for category learning. <em>Manuscript
in preparation</em>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+krus96">krus96</a></code>
</p>

<hr>
<h2 id='medin87train'>
Input representation of Exp. 1 in Medin et al. (1987) for models
input-compatible with slpALCOVE or slpSUSTAIN.
</h2><span id='topic+medin87train'></span>

<h3>Description</h3>

<p>Creates randomized training blocks for Experiment 1 in Medin et
al. (1987), in a format that is suitable for <code>slpALCOVE</code>,
<code>slpSUSTAIN</code>, and other models that use either of those 
input-representation formats.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>medin87train(blocks = 2, subjs = 2, seed = 7649, missing = 'pad')</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="medin87train_+3A_subjs">subjs</code></td>
<td>
<p>Number of simulated participants to run.</p>
</td></tr>
<tr><td><code id="medin87train_+3A_blocks">blocks</code></td>
<td>
<p>Number of blocks to generate. The ten trial types
are randomized within each block.</p>
</td></tr>
<tr><td><code id="medin87train_+3A_seed">seed</code></td>
<td>
<p>Set random seed.</p>
</td></tr>
<tr><td><code id="medin87train_+3A_missing">missing</code></td>
<td>
<p>If set to 'geo', output missing dimension flags (see
below). If set to 'pad', use the padded stimulus representation
format of slpSUSTAIN.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A matrix is produced, with one row for each trial, and with the following
columns:
</p>
<p><code>ctrl</code> - Set to 4 on the first trial for each participant - 4 resets
the model to the initial state and does unsupervised learning afterwards.
Set to 3 for unsupervised trials - normal unsupervised learning
trial.
</p>
<p><code>blk</code> - Training block.
</p>
<p><code>stim</code> - Stimulus number, ranging from 1 to 10. The numbering scheme
is the same as in Medin et al. (1987, Fig. 1).
</p>
<p><code>x1, x2, ...</code> - input representation. Where
<code>missing='geo'</code>, x1, x2, and x3 are returned, each set at 1 or
0. This is the binary dimensional representation required by models
such as slpALCOVE, where e.g. x2 is the value on the second
dimension. Where <code>missing='pad'</code>, w1, w2, x1, x2, y1, y2, z1,
z2, are returned. This is the padded represenation required by
models such as slpSUSTAIN; e.g. y1 and y2 represent the two possible
values on dimension 3, so if y1 is black, y2 is white, and the
stimulus is white, then [y1, y2] = [0, 1].
</p>
<p>Although the trial ordering is random, a random seed is used, so
multiple calls of this function with the same parameters should produce
the same output. This is usually desirable for reproducibility and
stability of non-linear optimization. To get a different order, use the
seed argument to set a different seed.
</p>


<h3>Value</h3>

<p>R by C matrix, where each row is one trial, and the columns contain
model input.
</p>


<h3>Author(s)</h3>

<p>Lenard Dome, Andy Wills
</p>


<h3>References</h3>

<p>Medin, D. L., Wattenmaker, W. D., &amp; Hampson, S. E. (1987). Family
resemblance, conceptual cohesiveness, and category construction.
<em>Cognitive Psychology, 19(2)</em>, 242–279.
</p>

<hr>
<h2 id='nosof88'>Instantiation frequency CIRP</h2><span id='topic+nosof88'></span>

<h3>Description</h3>

<p>Instantiation frequency is the number of times a stimulus has been
observed as a member of a specific category (Barsalou, 1985). Increasing
instantiation frequency of a stimulus increases categorization accuracy
for that stimulus ('direct' effect), and for other similar stimuli
('indirect' effect). Experiment 1 of Nosofsky (1988) provides the data
for this CIRP.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(nosof88)</code></pre>


<h3>Format</h3>

<p>A data frame with the following columns:
</p>

<dl>
<dt>cond</dt><dd><p>Experimental condition, see 'details'. 1 = 'B', 2 =
'E2', 3 = 'E7'</p>
</dd>
<dt>stim</dt><dd><p>Stimulus number, see Nosofsky (1988), Figure 1. Takes
values: 1-12</p>
</dd>
<dt>c2acc</dt><dd><p>Mean probability, across participants, of responding
that the item belongs to category 2.</p>
</dd>
</dl>



<h3>Details</h3>

<p>Wills et al. (n.d.) discuss the derivation of this CIRP. In brief,
both the direct and indirect effects have been independently
replicated. Experiment 1 of Nosofsky (1988) was selected due to the
availability of a multidimensional scaling solution for the stimuli,
see <code><a href="#topic+nosof88train">nosof88train</a></code>.
</p>
<p>Experiment 1 of Nosofsky(1988) involved the classification of Munsell
chips of fixed hue (5R) varying in brightness (value) and saturation
(chroma). Instantiation frequency was manipulated between subjects. In
condition B, all stimuli were equally frequent. In condition E2 (E7),
stimulus 2 (7) was approximately five times as frequent as each of the
other stimuli. In condition E2 (E7), stimulus 4 (9) indexes the
indirect effect. There were three blocks of training. Block length was
48 trials for condition B and 63 trials for conditions E2 and E7. The
training phase was followed by a transfer phase, which is not included
in this CIRP (see Nosofsky, 1988, for details).
</p>
<p>The data are as shown in Table 1 of Nosofsky (1988). The data are mean
response probabilities for each stimulus in the training phase,
averaged across blocks and participants.  
</p>


<h3>Author(s)</h3>

<p>Andy J. Wills <a href="mailto:andy@willslab.co.uk">andy@willslab.co.uk</a>
</p>


<h3>Source</h3>

<p>Nosofsky, R.M. (1988). Similarity, frequency, and category
representations, <em>Journal of Experimental Psychology: Learning,
Memory and Cognition, 14</em>, 54-65.
</p>


<h3>References</h3>

<p>Barsalou, L.W. (1985). Ideals, central tendency, and frequency of
instantiation as determinants of graded structure in
categories. <em>Journal of Experimental Psychology: Learning, Memory
&amp; Cognition, 11</em>, 629-654.
</p>
<p>Wills et al. (n.d.). Benchmarks for category learning. <em>Manuscript
in preparation</em>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+nosof88train">nosof88train</a></code>, <code><a href="#topic+nosof88oat">nosof88oat</a></code>    
</p>

<hr>
<h2 id='nosof88exalcove'>
Simulation of CIRP nosof88 with ex-ALCOVE model
</h2><span id='topic+nosof88exalcove'></span>

<h3>Description</h3>

<p>Runs a simulation of the <code><a href="#topic+nosof88">nosof88</a></code> CIRP using the
<code><a href="#topic+slpALCOVE">slpALCOVE</a></code> model implementation as an exemplar model and
<code><a href="#topic+nosof88train">nosof88train</a></code> as the input representation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
  nosof88exalcove(params = NULL)

</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="nosof88exalcove_+3A_params">params</code></td>
<td>
<p>A vector containing values for c, phi, la, and lw, in
that order, e.g. params = c(2.1, 0.6, 0.09, 0.9). See
<code><a href="#topic+slpALCOVE">slpALCOVE</a></code> for an explanation of these parameters. Where
params = NULL, best-fitting parameters are derived from
optimzation archive <code><a href="#topic+nosof88exalcove_opt">nosof88exalcove_opt</a></code> </p>
</td></tr>
</table>


<h3>Details</h3>

<p>An exemplar-based simulation using <code><a href="#topic+slpALCOVE">slpALCOVE</a></code> and
<code><a href="#topic+nosof88train">nosof88train</a></code>. The co-ordinates for the radial-basis units
are taken from the multdimensional scaling solution for these stimuli
reported by Nosofsky (1987).
</p>
<p>Other parameters of slpALCOVE are set as follows: <code>r</code> = 2,
<code>q</code> = 1, initial <code>alpha</code> = 1 / (number of input dimensions),
initial <code>w</code> = 0. These values are conventions of modeling with
ALCOVE, and should not be considered as free parameters. They are set
within the <code>nosof88exalcove</code> function, and hence can't be changed
without re-writing the function.
</p>
<p>This simulation is reported in Wills &amp; O'Connell (n.d.). 
</p>


<h3>Value</h3>

<p>A matrix of predicted response probabilities, in the same order and
format as the observed data contained in <code><a href="#topic+nosof88">nosof88</a></code>.
</p>


<h3>Author(s)</h3>

<p>Andy Wills &amp; Garret O'Connell
</p>


<h3>References</h3>

<p>Nosofsky, R.M. (1987). Attention and learning processes in the
identification and categorization of integral stimuli, <em>Journal
of Experimental Psychology: Learning, Memory and Cognition, 13</em>,
87-108.
</p>
<p>Wills, A.J. &amp; O'Connell (n.d.). Averaging
abstractions. <em>Manuscript in preparation</em>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+nosof88">nosof88</a></code>, <code><a href="#topic+nosof88oat">nosof88oat</a></code>,
<code><a href="#topic+nosof88train">nosof88train</a></code>, <code><a href="#topic+slpALCOVE">slpALCOVE</a></code>
</p>

<hr>
<h2 id='nosof88exalcove_opt'>
Parameter optimization of ex-ALCOVE model with nosof88 CIRP
</h2><span id='topic+nosof88exalcove_opt'></span>

<h3>Description</h3>

<p>Uses <code><a href="#topic+nosof88exalcove">nosof88exalcove</a></code> to find best-fitting parameters for
the ex-ALCOVE model for the <code><a href="#topic+nosof88">nosof88</a></code> CIRP.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
  nosof88exalcove_opt(recompute = FALSE)

</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="nosof88exalcove_opt_+3A_recompute">recompute</code></td>
<td>
<p>When set to TRUE, the function re-runs the
optimization. When set to FALSE, the function returns a stored copy
of the results of the optimization.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is an archive of the optimization procedure used to
derive the best-fitting parameters for the <code><a href="#topic+nosof88exalcove">nosof88exalcove</a></code>
simulation; see Spicer et al. (2017) for a tutorial introduction to
the concept of simulation archives.
</p>
<p>Optimization used the L-BFGS-B method from the <code><a href="stats.html#topic+optim">optim</a></code>
function of the standard R <code>stats</code> package. The objective
function was sum of squared errors. Please inspect the source code for
further details (e.g. type <code>nosof88exalcove_opt</code>). The
optimization was repeated for 16 different sets of starting values.
</p>
<p>Where <code>recompute = TRUE</code>, the function can take many hours to
run, depending on your system, and there is no progress bar. You can
use Task Manager (Windows) or equivalent if you want some kind of
visual feedback that the code is working hard. The code uses all the
processor cores on the local machine, so speed of execution is a
simple function of clock speed times processor cores. So, for example,
a 4 GHz i7 processor (8 virutal cores) will take a quarter of the time
to run this compared to a 2 GHz i5 processor (4 virtual cores).
</p>


<h3>Value</h3>

<p>A vector containing the best-fitting values for c, phi, la, and lw, in
that order. See <code><a href="#topic+slpALCOVE">slpALCOVE</a></code> for an explanation of these
parameters.
</p>


<h3>Author(s)</h3>

<p>Andy Wills
</p>


<h3>References</h3>

<p>Spicer, S., Jones, P.M., Inkster, A.B., Edmunds, C.E.R. &amp; Wills,
A.J. (2017). Progress in learning theory through distributed
collaboration: Concepts, tools, and examples. <em>Manuscript in
preparation</em>.
</p>

<hr>
<h2 id='nosof88oat'>
Ordinal adequacy test for simulations of nosof88 CIRP
</h2><span id='topic+nosof88oat'></span>

<h3>Description</h3>

<p>Tests whether a model output passes the ordinal adequacy criteria for
the <code><a href="#topic+nosof88">nosof88</a></code> CIRP.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
  nosof88oat(dta, xtdo=FALSE)

</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="nosof88oat_+3A_dta">dta</code></td>
<td>
<p>Matrix containing model output. The matrix must have the
same format, row order, and column names, as <code>data(nosof88)</code>;
with that proviso, the output of any simulation implementation can be
handled by this function.</p>
</td></tr>
<tr><td><code id="nosof88oat_+3A_xtdo">xtdo</code></td>
<td>
<p>eXTenDed Output: Either <code>TRUE</code> or <code>FALSE</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function implements the Wills &amp; O'Connell (n.d.) ordinal adequacy
tests for the <code><a href="#topic+nosof88">nosof88</a></code> CIRP. Specifically, a model passes
this test if it passes all four component tests: 1. E2(2) &gt; B(2), 2.
E7(7) &gt; B(7), 3. E2(4) &gt; B(4), 4. E7(9) &gt; B(9). These tests refer to
classification accuracy for particular stimuli in particular
experimental conditions. For example, E7(9) indicates stimulus 9 in
experimental condition E7. 
</p>
<p>Alternatively, by setting <code>xtdo</code> to <code>TRUE</code>, this function
returns the summary model predictions reported by Wills &amp; O'Connell
(2016).
</p>


<h3>Value</h3>

<p>Where <code>xtdo=FALSE</code>, this function returns TRUE if the ordinal
adequacy tests are passed, and FALSE otherwise.
</p>
<p>Where <code>xtdo=TRUE</code>, this function returns a summary matrix. The
columns are stimulus numbers. The rows ('B','E') indicate the baseline
(equal frequency) condition ('B') and the experimental conditions ('E2'
or 'E7', depending on the column).
</p>


<h3>Author(s)</h3>

<p>Andy Wills and Garret O'Connell
</p>


<h3>References</h3>

<p>Wills, A.J. &amp; O'Connell (n.d.). Averaging
abstractions. <em>Manuscript in preparation</em>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+nosof88">nosof88</a></code>    
</p>

<hr>
<h2 id='nosof88protoalcove'>
Simulation of CIRP nosof88 with proto-ALCOVE model
</h2><span id='topic+nosof88protoalcove'></span>

<h3>Description</h3>

<p>Runs a simulation of the <code><a href="#topic+nosof88">nosof88</a></code> CIRP using the
<code><a href="#topic+slpALCOVE">slpALCOVE</a></code> model implementation as a prototype model and
<code><a href="#topic+nosof88train">nosof88train</a></code> as the input representation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
  nosof88protoalcove(params = NULL)

</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="nosof88protoalcove_+3A_params">params</code></td>
<td>
<p>A vector containing values for c, phi, la, and lw, in
that order, e.g. params = c(2.1, 0.6, 0.09, 0.9). See
<code><a href="#topic+slpALCOVE">slpALCOVE</a></code> for an explanation of these parameters.  Where
params = NULL, best-fitting parameters are derived from optimzation
archive <code><a href="#topic+nosof88protoalcove_opt">nosof88protoalcove_opt</a></code> </p>
</td></tr>
</table>


<h3>Details</h3>

<p>An prototype-based simulation using <code><a href="#topic+slpALCOVE">slpALCOVE</a></code> and
<code><a href="#topic+nosof88train">nosof88train</a></code>. There is one radial-basis unit for each
category, representing the prototype. These prototypes are calculated
by taking the mean of the co-ordinates of the stimuli in a category,
with the stimulus co-ordinates coming from the multdimensional
scaling solution reported by Nosofsky (1987). The calculations of the
means are weighted by the instantiation frequency of the
stimuli. Hence, the prototypes for each condition of the experiment
are different. 
</p>
<p>Other parameters of slpALCOVE are set as follows: <code>r</code> = 2,
<code>q</code> = 1, initial <code>alpha</code> = 1 / (number of input dimensions),
initial <code>w</code> = 0. These values are conventions of modeling with
ALCOVE, and should not be considered as free parameters. They are set
within the <code>nosof88protoalcove</code> function, and hence can not be
changed without re-writing the function.
</p>
<p>This simulation is reported in Wills &amp; O'Connell (n.d.). 
</p>


<h3>Value</h3>

<p>A matrix of predicted response probabilities, in the same order and
format as the observed data contained in <code><a href="#topic+nosof88">nosof88</a></code>.
</p>


<h3>Author(s)</h3>

<p>Andy Wills &amp; Garret O'Connell
</p>


<h3>References</h3>

<p>Nosofsky, R.M. (1987). Attention and learning processes in the
identification and categorization of integral stimuli, <em>Journal
of Experimental Psychology: Learning, Memory and Cognition, 13</em>,
87-108.
</p>
<p>Wills, A.J. &amp; O'Connell (n.d.). Averaging
abstractions. <em>Manuscript in preparation</em>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+nosof88">nosof88</a></code>, <code><a href="#topic+nosof88oat">nosof88oat</a></code>,
<code><a href="#topic+nosof88train">nosof88train</a></code>, <code><a href="#topic+slpALCOVE">slpALCOVE</a></code>
</p>

<hr>
<h2 id='nosof88protoalcove_opt'>
Parameter optimization of proto-ALCOVE model with nosof88 CIRP
</h2><span id='topic+nosof88protoalcove_opt'></span>

<h3>Description</h3>

<p>Uses <code><a href="#topic+nosof88protoalcove">nosof88protoalcove</a></code> to find best-fitting parameters
for the ex-ALCOVE model for the <code><a href="#topic+nosof88">nosof88</a></code> CIRP.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
  nosof88protoalcove_opt(recompute = FALSE)

</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="nosof88protoalcove_opt_+3A_recompute">recompute</code></td>
<td>
<p>When set to TRUE, the function re-runs the
optimization. When set to FALSE, the function returns a stored copy
of the results of the optimization.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is an archive of the optimization procedure used to
derive the best-fitting parameters for the
<code><a href="#topic+nosof88protoalcove">nosof88protoalcove</a></code> simulation; see Spicer et al. (2017)
for a tutorial introduction to the concept of simulation archives.
</p>
<p>Optimization used the L-BFGS-B method from the <code><a href="stats.html#topic+optim">optim</a></code>
function of the standard R <code>stats</code> package. The objective
function was sum of squared errors. Please inspect the source code for
further details (e.g. type <code>nosof88protoalcove_opt</code>). The
optimization was repeated for 16 different sets of starting values.
</p>
<p>Where <code>recompute = TRUE</code>, the function can take many hours to
run, depending on your system, and there is no progress bar. You can
use Task Manager (Windows) or equivalent if you want some kind of
visual feedback that the code is working hard. The code uses all the
processor cores on the local machine, so speed of execution is a
simple function of clock speed times processor cores. So, for example,
a 4 GHz i7 processor (8 virutal cores) will take a quarter of the time
to run this compared to a 2 GHz i5 processor (4 virtual cores).
</p>


<h3>Value</h3>

<p>A vector containing the best-fitting values for c, phi, la, and lw, in
that order. See <code><a href="#topic+slpALCOVE">slpALCOVE</a></code> for an explanation of these
parameters.
</p>


<h3>Author(s)</h3>

<p>Andy Wills
</p>


<h3>References</h3>

<p>Spicer, S., Jones, P.M., Inkster, A.B., Edmunds, C.E.R. &amp; Wills,
A.J. (2017). Progress in learning theory through distributed
collaboration: Concepts, tools, and examples. <em>Manuscript in
preparation</em>.
</p>

<hr>
<h2 id='nosof88train'>
Input representation of nosof88 for models input-compatible with
slpALCOVE.
</h2><span id='topic+nosof88train'></span>

<h3>Description</h3>

<p>Create randomized training blocks for CIRP <code><a href="#topic+nosof88">nosof88</a></code>, in a
format suitable for the <code><a href="#topic+slpALCOVE">slpALCOVE</a></code> model, and any other
model that uses the same input representation format. The stimulus
co-ordinates come from a MDS solution reported by Nosofsky (1987) for
the same stimuli. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
nosof88train(condition = 'B', blocks = 3, absval = -1, subjs = 1, seed =
4182, missing = 'geo')

</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="nosof88train_+3A_condition">condition</code></td>
<td>
<p>Experimental condition 'B', 'E2', or 'E7', as defined
by Nosofsky (1988).</p>
</td></tr>
<tr><td><code id="nosof88train_+3A_blocks">blocks</code></td>
<td>
<p>Number of blocks to generate. Omit this argument to get
the same number of blocks as the published study (3).</p>
</td></tr>
<tr><td><code id="nosof88train_+3A_absval">absval</code></td>
<td>
<p>Teaching value to be used where category is absent.</p>
</td></tr>
<tr><td><code id="nosof88train_+3A_subjs">subjs</code></td>
<td>
<p>Number of simulated subjects to be run.</p>
</td></tr>
<tr><td><code id="nosof88train_+3A_seed">seed</code></td>
<td>
<p>Sets the random seed</p>
</td></tr>
<tr><td><code id="nosof88train_+3A_missing">missing</code></td>
<td>
<p>If set to 'geo', output missing dimension flags (see
below)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A matrix is produced, with one row for each trial, and with the
following columns:
</p>
<p><code>ctrl</code> - Set to 1 (reset model) for trial 1, set to zero (normal
trial) for all other trials.
</p>
<p><code>cond</code> - 1 = condition B, 2 = condition E2, 3 = condition E7
</p>
<p><code>blk</code> - training block
</p>
<p><code>stim</code> - stimulus number (as defined by Nosofsky, 1988)
</p>
<p><code>x1, x2</code> - input representation. These are the co-ordinates of an
MDS solution for these stimuli (see Nosofsky, 1987).
</p>
<p><code>t1, t2</code> - teaching signal (1 = category present, absval = category
absent)
</p>
<p><code>m1, m2</code> - Missing dimension flags (always set to zero in this
experiment, indicating all input dimensions are present on all
trials). Only produced if <code>missing = 'geo'</code>.
</p>
<p>Although the trial ordering is random, a random seed is used, so
multiple calls of this function with the same parameters should produce
the same output. This is usually desirable for reproducibility and
stability of non-linear optimization. To get a different order, use the
seed argument to set a different seed.
</p>
<p>This implementation assumes a block length of 64 trials for conditions
E2 and E7, rather than the 63 trials reported by Nosofsky (1988).
</p>
<p>This routine was originally developed to support simulations reported in
Wills &amp; O'Connell (n.d.).
</p>


<h3>Value</h3>

<p>R by C matrix, where each row is one trial, and the columns contain
model input.
</p>


<h3>Author(s)</h3>

<p>Andy Wills &amp; Garret O'Connell
</p>


<h3>References</h3>

<p>Nosofsky, R.M. (1987). Attention and learning processes in the
identification and categorization of integral stimuli, <em>Journal
of Experimental Psychology: Learning, Memory and Cognition, 13</em>,
87-108.
</p>
<p>Nosofsky, R.M. (1988). Similarity, frequency, and category
representations, <em>Journal of Experimental Psychology: Learning,
Memory and Cognition, 14</em>, 54-65.
</p>
<p>Wills, A.J. &amp; O'Connell (n.d.). Averaging
abstractions. <em>Manuscript in preparation</em>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+nosof88">nosof88</a></code>, <code><a href="#topic+nosof88oat">nosof88oat</a></code>, <code><a href="#topic+slpALCOVE">slpALCOVE</a></code>
</p>

<hr>
<h2 id='nosof94'>Type I-VI category structure CIRP</h2><span id='topic+nosof94'></span>

<h3>Description</h3>

<p>Shepard et al. (1961) stated that where there are two, equal-sized
categories constructed from the eight stimuli it is possible to
produce from varying three binary stimulus dimensions, there are only
six logically distinct category structures. Shepard et al. (1961)
labeled these structures as Types I through VI (see e.g. Nosofsky et
al., 1994, Figure 1, for details). The CIRP concerns the relative
difficulty of learning these category structures, as indexed by
classification accuracy. The result, expressed in terms of accuracy,
is:
</p>
<p>I &gt; II &gt; [III, IV, V] &gt; VI
</p>
<p>The experiment reported by Nosofsky et al. (1994) provides the data
for this CIRP.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(nosof94)</code></pre>


<h3>Format</h3>

<p>A data frame with the following columns:
</p>

<dl>
<dt>type</dt><dd><p>Type of category structure, as defined by Shepard et al. (1961). 
Takes values : 1-6</p>
</dd>
<dt>block</dt><dd><p>Training block. Takes values: 1-16</p>
</dd>
<dt>error</dt><dd><p>Mean error probability, averaged across participants</p>
</dd>
</dl>



<h3>Details</h3>

<p>Wills et al. (n.d.) discuss the derivation of this CIRP. In
brief, the effect has been independently replicagted. Nosofsky et
al. (1994) was selected as the CIRP because it had acceptable sample
size (N=40 per Type), and included simulations of the results with a
number of different formal models. Inclusion of this dataset in
<code>catlearn</code> thus permits a validation of <code>catlearn</code> model
implementations against published simulations.
</p>
<p>In Nosofsky et al. (1994) the stimuli varied in shape (squares or
triangles), type of interior line (solid or dotted), and size (large
or small). Each participant learned two problems. Each problem was
trained with feedback, to a criterion of four consecutive sub-blocks
of eight trials with no errors, or for a maximum of 400 trials. 
</p>
<p>The data are as shown in the first 16 rows of Table 1 of Nosofsky et
al. (1994). Only the first 16 blocks are reported, for comparability
with the model fitting reported in that paper. Where a participant
reached criterion before 16 blocks, Nosofsky et al. assumed they would
have made no further errors if they had continued.
</p>


<h3>Author(s)</h3>

<p>Andy J. Wills <a href="mailto:andy@willslab.co.uk">andy@willslab.co.uk</a>
</p>


<h3>Source</h3>

<p>Nosofsky, R.M., Gluck, M.A., Plameri, T.J., McKinley, S.C. and
Glauthier, P.  (1994). Comparing models of rule-based classification
learning: A replication and extension of Shepaard, Hovland, and
Jenkins (1961). <em>Memory and Cognition, 22</em>, 352-369.
</p>


<h3>References</h3>

<p>Shepard, R.N., Hovland, C.I., &amp; Jenkins, H.M. (1961). learning and
memorization of classifications. <em>Psychological Monographs, 75</em>,
Whole No. 517.
</p>
<p>Wills et al. (n.d.). Benchmarks for category learning. <em>Manuscript
in preparation</em>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+nosof94train">nosof94train</a></code>, <code><a href="#topic+nosof94oat">nosof94oat</a></code>    
</p>

<hr>
<h2 id='nosof94bnalcove'>
Simulation of CIRP nosof94 with BN-ALCOVE model
</h2><span id='topic+nosof94bnalcove'></span>

<h3>Description</h3>

<p>Runs a simulation of the <code><a href="#topic+nosof94">nosof94</a></code> CIRP using the
<code><a href="#topic+slpALCOVE">slpALCOVE</a></code> model implementation as an exemplar model and
<code><a href="#topic+nosof94train">nosof94train</a></code> as the input representation. This
simulation replicates the one reported by Nosofsky et al. (1994).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
  nosof94bnalcove(params = c(6.33,0.011,0.409,0.179))

</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="nosof94bnalcove_+3A_params">params</code></td>
<td>
<p>A vector containing values for c, phi, la, and lw, in
that order. See <code><a href="#topic+slpALCOVE">slpALCOVE</a></code> for an explanation of these
parameters.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>An exemplar-based simulation using <code><a href="#topic+slpALCOVE">slpALCOVE</a></code> and
<code><a href="#topic+nosof94train">nosof94train</a></code>. The co-ordinates for the radial-basis units
are assumed, and use the same binary representation as the abstract
category structure.
</p>
<p>The defaults for <code>params</code> are the best fit of the model to the
<code><a href="#topic+nosof94">nosof94</a></code> CIRP. The derivation of this fit is described by
Nosofsky et al. (1994). 
</p>
<p>The other parameters of slpALCOVE are set as follows: <code>r</code> = 1,
<code>q</code> = 1, initial <code>alpha</code> = 1 / number of dimensions, initial
<code>w</code> = 0. These values are conventions of modeling with ALCOVE, and
should not be considered as free parameters. They are set within the
<code>nosof88bnalcove</code> function, and hence can't be changed without
re-writing the function.
</p>
<p>This is a replication of the simulation reported by Nosofsky et
al. (1994). Compared to other published simulations with the ALCOVE
model, their simulation is non-standard in a number of respects:
</p>
<p>1. A background noise ('BN') decision rule is used (other simulations
use an exponential ratio rule).
</p>
<p>2. As a consequence of #1, absence of a category label is represented
by a zero (other simulations use -1).
</p>
<p>3. The sum of the attentional weights is constrained to be 1 on every
trial (other simulations do not apply this constraint).
</p>
<p>The current simulation replicates these non-standard aspects of the
Nosofsky et al. (1994) simulation.
</p>


<h3>Value</h3>

<p>A matrix of predicted response probabilities, in the same order and
format as the observed data contained in <code><a href="#topic+nosof94">nosof94</a></code>.
</p>


<h3>Author(s)</h3>

<p>Andy Wills
</p>


<h3>References</h3>

<p>Nosofsky, R.M., Gluck, M.A., Plameri, T.J., McKinley, S.C. and
Glauthier, P.  (1994). Comparing models of rule-based classification
learning: A replication and extension of Shepaard, Hovland, and
Jenkins (1961). <em>Memory and Cognition, 22</em>, 352&ndash;369
</p>


<h3>See Also</h3>

<p><code><a href="#topic+nosof94">nosof94</a></code>, <code><a href="#topic+nosof94oat">nosof94oat</a></code>,
<code><a href="#topic+nosof94train">nosof94train</a></code>, <code><a href="#topic+slpALCOVE">slpALCOVE</a></code>,
<code><a href="#topic+nosof94bnalcove">nosof94bnalcove</a></code>
</p>

<hr>
<h2 id='nosof94exalcove'>
Simulation of CIRP nosof94 with ex-ALCOVE model
</h2><span id='topic+nosof94exalcove'></span>

<h3>Description</h3>

<p>Runs a simulation of the <code><a href="#topic+nosof94">nosof94</a></code> CIRP using the
<code><a href="#topic+slpALCOVE">slpALCOVE</a></code> model implementation as an exemplar model and
<code><a href="#topic+nosof94train">nosof94train</a></code> as the input representation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
  nosof94exalcove(params = NULL)

</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="nosof94exalcove_+3A_params">params</code></td>
<td>
<p>A vector containing values for c, phi, la, and lw, in
that order, e.g. params = c(2.1, 0.6, 0.09, 0.9). See
<code><a href="#topic+slpALCOVE">slpALCOVE</a></code> for an explanation of these parameters. Where
params = NULL, best-fitting parameters are derived from optimzation
archive <code><a href="#topic+nosof94exalcove_opt">nosof94exalcove_opt</a></code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>N.B.: This simulation uses a standard version of ALCOVE. For a
replication of the ALCOVE simulation of these data reported by
Nosofsky et al. (1994), which is non-standard in a number of respects,
see <code><a href="#topic+nosof94bnalcove">nosof94bnalcove</a></code>.
</p>
<p>An exemplar-based simulation using <code><a href="#topic+slpALCOVE">slpALCOVE</a></code> and
<code><a href="#topic+nosof94train">nosof94train</a></code>. The co-ordinates for the radial-basis units
are assumed, and use the same binary representation as the abstract
category structure.
</p>
<p>Other parameters of slpALCOVE are set as follows: <code>r</code> = 1,
<code>q</code> = 1, initial <code>alpha</code> = 1/3, initial <code>w</code> = 0. These
values are conventions of modeling with ALCOVE, and should not be
considered as free parameters. They are set within the
<code>nosof88exalcove</code> function, and hence can't be changed without
re-writing the function.
</p>
<p>This simulation is reported in Wills &amp; O'Connell (n.d.). 
</p>


<h3>Value</h3>

<p>A matrix of predicted response probabilities, in the same order and
format as the observed data contained in <code><a href="#topic+nosof94">nosof94</a></code>.
</p>


<h3>Author(s)</h3>

<p>Andy Wills
</p>


<h3>References</h3>

<p>Nosofsky, R.M., Gluck, M.A., Plameri, T.J., McKinley, S.C. and
Glauthier, P.  (1994). Comparing models of rule-based classification
learning: A replication and extension of Shepaard, Hovland, and
Jenkins (1961). <em>Memory and Cognition, 22</em>, 352&ndash;369
</p>
<p>Wills, A.J. &amp; O'Connell (n.d.). Averaging
abstractions. <em>Manuscript in preparation</em>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+nosof94">nosof94</a></code>, <code><a href="#topic+nosof94oat">nosof94oat</a></code>,
<code><a href="#topic+nosof94train">nosof94train</a></code>, <code><a href="#topic+slpALCOVE">slpALCOVE</a></code>,
<code><a href="#topic+nosof94bnalcove">nosof94bnalcove</a></code>
</p>

<hr>
<h2 id='nosof94exalcove_opt'>
Parameter optimization of ex-ALCOVE model with nosof94 CIRP
</h2><span id='topic+nosof94exalcove_opt'></span>

<h3>Description</h3>

<p>Uses <code><a href="#topic+nosof94exalcove">nosof94exalcove</a></code> to find best-fitting parameters for
the ex-ALCOVE model for the <code><a href="#topic+nosof94">nosof94</a></code> CIRP.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
  nosof94exalcove_opt(recompute = FALSE, xtdo = FALSE)

</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="nosof94exalcove_opt_+3A_recompute">recompute</code></td>
<td>
<p>When set to TRUE, the function re-runs the
optimization. When set to FALSE, the function returns a stored copy
of the results of the optimization.</p>
</td></tr>
<tr><td><code id="nosof94exalcove_opt_+3A_xtdo">xtdo</code></td>
<td>
<p>eXTenDed Output; where set to TRUE, some further details
of the optimization procedure are printed to the console.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is an archive of the optimization procedure used to
derive the best-fitting parameters for the <code><a href="#topic+nosof94exalcove">nosof94exalcove</a></code>
simulation; see Spicer et al. (2017) for a tutorial introduction to
the concept of simulation archives.
</p>
<p>Optimization used the L-BFGS-B method from the <code><a href="stats.html#topic+optim">optim</a></code>
function of the standard R <code>stats</code> package. The objective
function was sum of squared errors. Please inspect the source code for
further details (e.g. type <code>nosof94exalcove_opt</code>). The
optimization was repeated for 15 different sets of starting values.
</p>
<p>Where <code>recompute = TRUE</code>, the function can take many hours to
run, depending on your system, and there is no progress bar. You can
use Task Manager (Windows) or equivalent if you want some kind of
visual feedback that the code is working hard. The code uses all the
processor cores on the local machine, so speed of execution is a
simple function of clock speed times processor cores. So, for example,
a 4 GHz i7 processor (8 virutal cores) will take a quarter of the time
to run this compared to a 2 GHz i5 processor (4 virtual cores).
</p>


<h3>Value</h3>

<p>A vector containing the best-fitting values for c, phi, la, and lw, in
that order. See <code><a href="#topic+slpALCOVE">slpALCOVE</a></code> for an explanation of these
parameters.
</p>


<h3>Author(s)</h3>

<p>Andy Wills
</p>


<h3>References</h3>

<p>Spicer, S., Jones, P.M., Inkster, A.B., Edmunds, C.E.R. &amp; Wills,
A.J. (2017). Progress in learning theory through distributed
collaboration: Concepts, tools, and examples. <em>Manuscript in
preparation</em>.
</p>

<hr>
<h2 id='nosof94oat'>
Ordinal adequacy test for simulations of nosof94 CIRP
</h2><span id='topic+nosof94oat'></span>

<h3>Description</h3>

<p>Tests whether a model output passes the ordinal adequacy criteria for
the <code><a href="#topic+nosof94">nosof94</a></code> CIRP.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
  nosof94oat(dta, xtdo=FALSE)

</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="nosof94oat_+3A_dta">dta</code></td>
<td>
<p>Matrix containing model output. The matrix must have the
same format, row order, and column names, as <code>data(nosof94)</code>;
with that proviso, the output of any simulation implementation can be
handled by this function.</p>
</td></tr>
<tr><td><code id="nosof94oat_+3A_xtdo">xtdo</code></td>
<td>
<p>eXTenDed Output: Either <code>TRUE</code> or <code>FALSE</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function implements a standard ordinal adequacy test for the
<code><a href="#topic+nosof94">nosof94</a></code> CIRP. Specifically, a model passes this test if
the mean errors (averaged across blocks), obey the following:
</p>
<p>I &lt; II &lt; [III, IV, V] &lt; VI
</p>
<p>Note that '[III, IV, V]' indicates that the these three problems can
be in any order of difficulty (or all be of equal difficulty), as long
as all three are harder than Problem 2 and all three are easier than
Problem 6.   
</p>
<p>Alternatively, by setting <code>xtdo</code> to <code>TRUE</code>, this function
returns the mean classification error by Problem type.
</p>


<h3>Value</h3>

<p>Where <code>xtdo=FALSE</code>, this function returns TRUE if the ordinal
adequacy tests are passed, and FALSE otherwise.
</p>
<p>Where <code>xtdo=TRUE</code>, this function returns a summary matrix,
containing mean errors (across blocks) for each of the six problem
types.
</p>


<h3>Author(s)</h3>

<p>Andy Wills 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+nosof94">nosof94</a></code>    
</p>

<hr>
<h2 id='nosof94plot'>
Plot Nosofsky et al. (1994) data / simulations
</h2><span id='topic+nosof94plot'></span>

<h3>Description</h3>

<p>Produce a line graph similar to that shown in Nosofsky et al. (1994,
Figures 1, 6-9).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
nosof94plot(results,title = 'Nosofsky et al. (1994)')

</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="nosof94plot_+3A_results">results</code></td>
<td>
<p>Mean error probability by block and problem, in the same format 
as data set <code>nosof94</code></p>
</td></tr>
<tr><td><code id="nosof94plot_+3A_title">title</code></td>
<td>
<p>Title to appear at top of plot</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Andy Wills
</p>


<h3>References</h3>

<p>Nosofsky, R.M., Gluck, M.A., Plameri, T.J., McKinley, S.C. and
Glauthier, P.  (1994). Comparing models of rule-based classification
learning: A replication and extension of Shepaard, Hovland, and
Jenkins (1961). <em>Memory and Cognition, 22</em>, 352&ndash;369.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  data(nosof94)
  nosof94plot(nosof94)
</code></pre>

<hr>
<h2 id='nosof94sustain'>
Simulation of CIRP nosof94 with the SUSTAIN model
</h2><span id='topic+nosof94sustain'></span>

<h3>Description</h3>

<p>Runs a simulation of the <code><a href="#topic+nosof94">nosof94</a></code> CIRP using the
<code><a href="#topic+slpSUSTAIN">slpSUSTAIN</a></code> model implementation and
<code><a href="#topic+nosof94train">nosof94train</a></code> as the input representation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
  nosof94sustain(params = c(9.01245, 1.252233, 16.924073, 0.092327))

</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="nosof94sustain_+3A_params">params</code></td>
<td>
<p>A vector containing values for r, beta, d, and eta, in
that order, e.g. params = c(8.1, 1.5, 9.71, 0.8). See
<code><a href="#topic+slpSUSTAIN">slpSUSTAIN</a></code> for an explanation of these parameters.</p>
</td></tr> 
</table>


<h3>Details</h3>

<p>NOTE: The underlying slpSUSTAIN function is currently
written in R, and hence this simulation will take several minutes to
run. slpSUSTAIN may be converted to C++ in a future release, which
will reduce the run time of this simulation to a few seconds.
</p>
<p>A simulation using <code><a href="#topic+slpSUSTAIN">slpSUSTAIN</a></code> and
<code><a href="#topic+nosof94train">nosof94train</a></code>, i.e. a simulation of Nosofsky et al. (1994)
with the Love et al. (2004) SUTAIN model.
</p>
<p>Other parameters of slpSUSTAIN are set as follows: <code>tau</code> = 0,
initial <code>lambda</code> = 1, initial <code>w</code> = 0, inital cluster
centered on the first stimulus presented to the siumulated
subject. These values are conventions of modelling with SUSTAIN, and
should not be considered as free parameters. They are set within the
<code>nosof94sustain</code> function, and hence can't be changed without
re-writing the function.
</p>
<p>The simulation uses 100 simulated subjects. Like the simulations
<code>nosof94exalcove</code> and <code>nosof94protoalcove</code>, all simulated
participants complete 16 blocks of training. This differs from the
Nosofsky et al. (1994) experiment, in which participants are trained to
a criterion of four consecutive errorless 8-trial subblocks.
</p>
<p>The simulation by Gureckis (2014) builds this criterion-based training
into their simulation by using a random number generator to turn the
response probability on each trial into a correct or incorrect
response. This feature of the Gureckis (2014) simulation is not
incorporated here, because the instability in ouput this generates
makes parameter optimization (e.g. via <code>optim</code>) less reliable.
</p>
<p>A comparison of 10,000 simulated participants in the Gureckis (2014)
simulation with 1,000 simulated participants in the current simulation
reveals a mean difference in the 96 reported response probabilities of
less than 0.01.
</p>


<h3>Value</h3>

<p>A matrix of predicted response probabilities, in the same order and
format as the observed data contained in <code><a href="#topic+nosof94">nosof94</a></code>.
</p>


<h3>Author(s)</h3>

<p>Lenard Dome, Andy Wills
</p>


<h3>References</h3>

<p>Love, B. C., Medin, D. L., &amp; Gureckis, T. M. (2004). SUSTAIN: a
network model of category learning. <em>Psychological Review, 111</em>,
309-332.
</p>
<p>Gureckis, T. M. (2014). sustain_python. <em>https://github.com/NYUCCL/sustain_python</em>
</p>
<p>Nosofsky, R.M., Gluck, M.A., Plameri, T.J., McKinley, S.C. and
Glauthier, P.  (1994). Comparing models of rule-based classification
learning: A replication and extension of Shepaard, Hovland, and
Jenkins (1961). <em>Memory and Cognition, 22</em>, 352&ndash;369.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+nosof94">nosof94</a></code>, <code><a href="#topic+nosof94oat">nosof94oat</a></code>,
<code><a href="#topic+nosof94train">nosof94train</a></code>, <code><a href="#topic+slpALCOVE">slpALCOVE</a></code>,
<code><a href="#topic+nosof94bnalcove">nosof94bnalcove</a></code>
</p>

<hr>
<h2 id='nosof94train'>
Input representation of nosof94 for models input-compatible with
slpALCOVE or slpSUSTAIN
</h2><span id='topic+nosof94train'></span>

<h3>Description</h3>

<p>Create randomized training blocks for CIRP <code>nosof94</code>, in a format
suitable for the <code>slpALCOVE</code> or <code>slpSUSTAIN</code> models, and
other models that use the same input representation formats. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
nosof94train(cond = 1, blocks = 16, absval = -1, subjs = 1, seed = 7624,
missing = 'geo', blkstyle = 'accurate')

</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="nosof94train_+3A_cond">cond</code></td>
<td>
<p>Category structure type (1-6), as defined by Shepard et
al. (1961).</p>
</td></tr>
<tr><td><code id="nosof94train_+3A_blocks">blocks</code></td>
<td>
<p>Number of blocks to generate. Omit this argument to get
the same number of blocks (16) as used in the simulations reported
by Nosofsky et al. (1994).</p>
</td></tr>
<tr><td><code id="nosof94train_+3A_absval">absval</code></td>
<td>
<p>Teaching value to be used where category is absent.</p>
</td></tr>
<tr><td><code id="nosof94train_+3A_subjs">subjs</code></td>
<td>
<p>Number of simulated subjects to be run.</p>
</td></tr>
<tr><td><code id="nosof94train_+3A_seed">seed</code></td>
<td>
<p>Sets the random seed.</p>
</td></tr>
<tr><td><code id="nosof94train_+3A_missing">missing</code></td>
<td>
<p>If set to 'geo', output missing dimension flags (see
below). If set to 'pad', use the padded stimulus representation format
of slpSUSTAIN. If set to 'pad', set <code>absval</code> to zero.</p>
</td></tr>
<tr><td><code id="nosof94train_+3A_blkstyle">blkstyle</code></td>
<td>
<p>If set to 'accurate', reproduce the randomization of
this experiment, as described in Nosofsky et al. (1994). If set to
'eights', use instead the randomization used in the Gureckis (2016)
simulation of this experiment.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A matrix is produced, with one row for each trial, and with the
following columns:
</p>
<p><code>ctrl</code> - Set to 1 (reset model) for trial 1 of each simulated
subject, set to zero (normal trial) for all other trials.
</p>
<p><code>blk</code> - training block
</p>
<p><code>stim</code> - Stimulus number, ranging from 1 to 8. The numbering scheme
is the same as in Nosofsky et al. (1994, Figure 1), under the mapping
of dim_1_left = 0, dim_1_right = 1, dim_2_front = 0, dim_2_back = 1,
dim_3_bottom = 0, dim_3_top = 1.
</p>
<p><code>x1, x2, ...</code> - input representation. Where <code>missing='geo'</code>,
x1, x2, and x3 are returned, each set at 1 or 0. This is the binary
dimensional representation required by models such as slpALCOVE, where
e.g. x2 is the value on the second dimension. Where
<code>missing='pad'</code>, x1, x2, y1, y2, z1, z2, are returned. This is the
padded represenation required by models such as slpSUSTAIN; e.g. y1 and
y2 represent the two possible values on dimension 2, so if y1 is black,
y2 is white, and the stimulus is white, then [y1, y2] = [0, 1].
</p>
<p><code>t1, t2</code> - Category label (1 = category present, absval = category
absent)
</p>
<p><code>m1, m2, m3</code> - Missing dimension flags (always set to zero in this
experiment, indicating all input dimensions are present on all
trials). Only produced if <code>missing = 'geo'</code>.
</p>
<p>Although the trial ordering is random, a random seed is used, so
multiple calls of this function with the same parameters should produce
the same output. This is usually desirable for reproducibility and
stability of non-linear optimization. To get a different order, use the
seed argument to set a different seed.
</p>
<p>This routine was originally developed to support Wills et al. (n.d.).
</p>


<h3>Value</h3>

<p>R by C matrix, where each row is one trial, and the columns contain
model input.
</p>


<h3>Author(s)</h3>

<p>Andy Wills, Lenard Dome
</p>


<h3>References</h3>

<p>Nosofsky, R.M., Gluck, M.A., Plameri, T.J., McKinley, S.C. and
Glauthier, P.  (1994). Comparing models of rule-based classification
learning: A replication and extension of Shepaard, Hovland, and
Jenkins (1961). <em>Memory and Cognition, 22</em>, 352&ndash;369
</p>
<p>Gureckis, T. (2016). https://github.com/NYUCCL/sustain_python
</p>
<p>Wills et al. (n.d.). Benchmarks for category learning. <em>Manuscript
in preparation</em>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+nosof94train">nosof94train</a></code>, <code><a href="#topic+nosof94oat">nosof94oat</a></code>
</p>

<hr>
<h2 id='shin92'>Category size CIRP</h2><span id='topic+shin92'></span>

<h3>Description</h3>

<p>Category size is the number of examples of a category that have been
presented to the participant. The category-size effect (e.g. Homa et
al., 1973) is the phenomenon that, as category size increases, the
accuracy of generalization to new members of that category also
increases. The equal-frequency conditions of Experiment 3 of Shin &amp;
Nosofsky (1992) provides the data for this CIRP.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(shin92)</code></pre>


<h3>Format</h3>

<p>A data frame with the following columns:
</p>

<dl>
<dt>catsize</dt><dd><p>Experimental condition (category size). Takes values
: 3, 10</p>
</dd>
<dt>cat</dt><dd><p>Category membership of stimulus. Takes values: 1, 2</p>
</dd>
<dt>stim</dt><dd><p>Stimulus code, as defined by Shin &amp; Nosofsky
(1992). Stimuli beginning 'RN' or 'URN' are the 'novel'
stimuli. Stimuli beginning 'P' are prototypes. The remaining stimuli
are the 'old' (training) stimuli. </p>
</dd>
<dt>c2acc</dt><dd><p>Mean probability, across participants, of responding
that the item belongs to category 2.</p>
</dd>
</dl>



<h3>Details</h3>

<p>Wills et al. (2017) discuss the derivation of this CIRP, with Wills et
al. (n.d.) providing further details. In brief, the effect has been
independently replicated. Experiment 3 of Shin &amp; Nosofsky (1992) was
selected due to the availability of a multi-dimensional scaling
solution for the stimuli, see <code><a href="#topic+shin92train">shin92train</a></code>.
</p>
<p>Experiment 3 of Shin &amp; Nosofsky (1992) involved the classification of
nine-vertex polygon stimuli drawn from two categories. Category size
was manipulated between subjects (3 vs. 10 stimuli per
category). Participants received eight blocks of training, and three
test blocks.
</p>
<p>The data are as shown in Table 10 of Shin &amp; Nosofsky (1992). The data
are mean response probabilities for each stimulus in the test phase,
averaged across test blocks and participants.
</p>


<h3>Author(s)</h3>

<p>Andy J. Wills <a href="mailto:andy@willslab.co.uk">andy@willslab.co.uk</a>
</p>


<h3>Source</h3>

<p>Shin, H.J. &amp;  Nosofsky, R.M. (1992). Similarity-scaling studies of
dot-pattern classification and recognition. <em>Journal
of Experimental Psychology: General, 121</em>, 278-304.
</p>


<h3>References</h3>

<p>Wills et al. (n.d.). Benchmarks for category learning. <em>Manuscript
in preparation</em>.
</p>
<p>Wills, A.J., O'Connell, G., Edmunds, C.E.R. &amp; Inkster, A.B. (2017).
Progress in modeling through distributed collaboration: Concepts,
tools, and category-learning examples. <em>The Psychology of
Learning and Motivation, 66</em>, 79-115.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+shin92train">shin92train</a></code>, <code><a href="#topic+shin92oat">shin92oat</a></code>    
</p>

<hr>
<h2 id='shin92exalcove'>
Simulation of CIRP shin92 with ex-ALCOVE model
</h2><span id='topic+shin92exalcove'></span>

<h3>Description</h3>

<p>Runs a simulation of the <code><a href="#topic+shin92">shin92</a></code> CIRP using the
<code><a href="#topic+slpALCOVE">slpALCOVE</a></code> model implementation as an exemplar model and
<code><a href="#topic+shin92train">shin92train</a></code> as the input representation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
  shin92exalcove(params = NULL)

</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="shin92exalcove_+3A_params">params</code></td>
<td>
<p>A vector containing values for c, phi, la, and lw, in
that order, e.g. params = c(2.1, 0.6, 0.09, 0.9). See
<code><a href="#topic+slpALCOVE">slpALCOVE</a></code> for an explanation of these parameters.  Where
params = NULL, best-fitting parameters are derived from
optimzation archive <code><a href="#topic+shin92exalcove_opt">shin92exalcove_opt</a></code> </p>
</td></tr>
</table>


<h3>Details</h3>

<p>An exemplar-based simulation using <code><a href="#topic+slpALCOVE">slpALCOVE</a></code> and
<code><a href="#topic+shin92train">shin92train</a></code>. The co-ordinates for the radial-basis units
are derived from the test stimuli in <code><a href="#topic+shin92train">shin92train</a></code>. The
output is the average of 100 simulated subjects.
</p>
<p>The defaults for <code>params</code> are the best fit of the model to the
<code><a href="#topic+shin92">shin92</a></code> CIRP. They were derived through minimization of
SSE using non-linear optimization from 16 different initial
states (using code not included in this archive). 
</p>
<p>The other parameters of slpALCOVE are set as follows: <code>r</code> = 2,
<code>q</code> = 1, initial <code>alpha</code> = 1 / (number of input dimensions),
inital <code>w</code> = 0. These values are conventions of modeling with
ALCOVE, and should not be considered as free parameters. They are set
within the <code>shin92exaclove</code> function, and hence can't be changed
without re-writing the function.
</p>
<p>This simulation was reported in Wills et al. (2017). 
</p>


<h3>Value</h3>

<p>A matrix of predicted response probabilities, in the same order and
format as the observed data contained in <code><a href="#topic+shin92">shin92</a></code>.
</p>


<h3>Author(s)</h3>

<p>Andy Wills &amp; Garret O'Connell
</p>


<h3>References</h3>

<p>Shin, H.J. &amp;  Nosofsky, R.M. (1992). Similarity-scaling studies of
dot-pattern classification and recognition. <em>Journal
of Experimental Psychology: General, 121</em>, 278&ndash;304.
</p>
<p>Wills, A.J., O'Connell, G., Edmunds, C.E.R. &amp; Inkster, A.B. (2017).
Progress in modeling through distributed collaboration: Concepts,
tools, and category-learning examples. <em>The Psychology of
Learning and Motivation, 66</em>, 79-115.
</p>

<hr>
<h2 id='shin92exalcove_opt'>
Parameter optimization of ex-ALCOVE model with shin92 CIRP
</h2><span id='topic+shin92exalcove_opt'></span>

<h3>Description</h3>

<p>Uses <code><a href="#topic+shin92exalcove">shin92exalcove</a></code> to find best-fitting parameters for
the ex-ALCOVE model for the <code><a href="#topic+shin92">shin92</a></code> CIRP.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
  shin92exalcove_opt(params = c(2, 1, 0.25, 0.75), recompute = FALSE,
  trace = 0)

</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="shin92exalcove_opt_+3A_params">params</code></td>
<td>
<p>A vector containing the initial values for c, phi, la,
and lw, in that order. See <code><a href="#topic+slpALCOVE">slpALCOVE</a></code> for an explanation
of these parameters. Where <code>recompute</code> is FALSE, this argument
has no effect.</p>
</td></tr>
<tr><td><code id="shin92exalcove_opt_+3A_recompute">recompute</code></td>
<td>
<p>When set to TRUE, the function re-runs the
optimization (which takes about 25 minutes on a 2.4 GHz
processor). When set to FALSE, the function returns a stored copy of
the results of the optimization (which is instantaneous).</p>
</td></tr>
<tr><td><code id="shin92exalcove_opt_+3A_trace">trace</code></td>
<td>
<p>Sets the level of tracing information (i.e. information
about the progress of the optimization), as defined by the
<code><a href="stats.html#topic+optim">optim</a></code> function. Set to 6 for maximally verbose
output. Where <code>recompute</code> is FALSE, this argument has no effect.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is an archive of the optimization procedure used to
derive the best-fitting parameters for the <code><a href="#topic+shin92exalcove">shin92exalcove</a></code>
simulation; see Spicer et al. (2017) for a tutorial introduction to
the concept of simulation archives.
</p>
<p>Optimization used the L-BFGS-B method from the <code><a href="stats.html#topic+optim">optim</a></code>
function of the standard R <code>stats</code> package. The objective
function was sum of squared errors. Please inspect the source code for
further details (e.g. type <code>shin92exalcove_opt</code>). 
</p>
<p>This function was run in 16 times from different starting points,
using 8 threads on a Core i7 3.6 GHz processor. The default
parameters of this function are those for the best fit from those 16
starting points. The 16 starting points were
</p>
<p><code>
    pset &lt;- rbind(
      c(2,1,.25,.25),c(2,1,.25,.75),c(2,1,.75,.25),c(2,1,.75,.75),
      c(2,3,.25,.25),c(2,3,.25,.05),c(2,3,.75,.25),c(2,3,.75,.75),
      c(8,1,.25,.25),c(8,1,.25,.75),c(8,1,.75,.25),c(8,1,.75,.75),
      c(8,3,.25,.25),c(8,3,.25,.75),c(8,3,.75,.25),c(8,3,.75,.75)
    )
  </code>
</p>
<p>not all of which converged successfully.
</p>


<h3>Value</h3>

<p>A vector containing the best-fitting values for c, phi, la,
and lw, in that order. See <code><a href="#topic+slpALCOVE">slpALCOVE</a></code> for an explanation
of these parameters.
</p>


<h3>Author(s)</h3>

<p>Andy Wills
</p>


<h3>References</h3>

<p>Spicer, S., Jones, P.M., Inkster, A.B., Edmunds, C.E.R. &amp; Wills,
A.J. (2017). Progress in learning theory through distributed
collaboration: Concepts, tools, and examples. <em>Manuscript in
preparation</em>.
</p>

<hr>
<h2 id='shin92oat'>
Ordinal adequacy test for simulations of shin92 CIRP
</h2><span id='topic+shin92oat'></span>

<h3>Description</h3>

<p>Tests whether a model output passes the ordinal adequacy criterion for
the <code><a href="#topic+shin92">shin92</a></code> CIRP.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
  shin92oat(dta, xtdo=FALSE)

</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="shin92oat_+3A_dta">dta</code></td>
<td>
<p>Matrix containing model output. The matrix must have the
same format, row order, and column names, as that returned by
<code>shin92exalcove</code>; with that proviso, the output of any simulation
implementation can be handled by this function.</p>
</td></tr>
<tr><td><code id="shin92oat_+3A_xtdo">xtdo</code></td>
<td>
<p>eXTenDed Output: Either <code>TRUE</code> or <code>FALSE</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function implements the Wills et al. (2017) ordinal adequacy
test for the <code><a href="#topic+shin92">shin92</a></code> CIRP. Specifically, a model passes
this test if response accuracy is higher for novel items from the
size-10 condition than novel items from the size-3 condition.
</p>
<p>Alternatively, by setting <code>xtdo</code> to <code>TRUE</code>, this function
returns the summary model predictions reported by Wills et al. (2017). 
</p>


<h3>Value</h3>

<p>Where <code>xtdo=FALSE</code>, this function returns TRUE if the ordinal
adequacy test is passed, and FALSE otherwise.
</p>
<p>Where <code>xtdo=TRUE</code>, this function returns a summary matrix. The rows
are the two category sizes, the columns are the three principal stimulus
types (old, prototype, new), and the values are predicted accuracy
scores.
</p>


<h3>Author(s)</h3>

<p>Andy Wills and Garret O'Connell
</p>


<h3>References</h3>

<p>Shin, H.J. &amp;  Nosofsky, R.M. (1992). Similarity-scaling studies of
dot-pattern classification and recognition. <em>Journal
of Experimental Psychology: General, 121</em>, 278&ndash;304.
</p>
<p>Wills, A.J., O'Connell, G., Edmunds, C.E.R. &amp; Inkster, A.B. (2017).
Progress in modeling through distributed collaboration: Concepts,
tools, and category-learning examples. <em>The Psychology of
Learning and Motivation, 66</em>, 79-115.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+shin92">shin92</a></code>
</p>

<hr>
<h2 id='shin92protoalcove'>
Simulation of CIRP shin92 with proto-ALCOVE model
</h2><span id='topic+shin92protoalcove'></span>

<h3>Description</h3>

<p>Runs a simulation of the <code><a href="#topic+shin92">shin92</a></code> CIRP using the
<code><a href="#topic+slpALCOVE">slpALCOVE</a></code> model implementation as a prototype model and
<code><a href="#topic+shin92train">shin92train</a></code> as the input representation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
  shin92protoalcove(params = NULL)

</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="shin92protoalcove_+3A_params">params</code></td>
<td>
<p>A vector containing values for c, phi, la, and lw, in
that orderr, e.g. params = c(2.1, 0.6, 0.09, 0.9). See
<code><a href="#topic+slpALCOVE">slpALCOVE</a></code> for an explanation of these parameters.  Where
params = NULL, best-fitting parameters are derived from optimzation
archive <code><a href="#topic+shin92exalcove_opt">shin92exalcove_opt</a></code> </p>
</td></tr>
</table>


<h3>Details</h3>

<p>An exemplar-based simulation using <code><a href="#topic+slpALCOVE">slpALCOVE</a></code> and
<code><a href="#topic+shin92train">shin92train</a></code>. The co-ordinates for the radial-basis units
for the two prototypes are derived from the arithmetic means of the
test stimuli in <code><a href="#topic+shin92train">shin92train</a></code>. The output is the average of
100 simulated subjects.
</p>
<p>The defaults for <code>params</code> are the best fit of the model to the
<code><a href="#topic+shin92">shin92</a></code> CIRP. They were derived through minimization of
SSE using non-linear optimization from 16 different initial
states (using code not included in this archive). 
</p>
<p>The other parameters of slpALCOVE are set as follows: <code>r</code> = 2,
<code>q</code> = 1, initial <code>alpha</code> = 1 / (number of input dimensions),
inital <code>w</code> = 0. These values are conventions of modeling with
ALCOVE, and should not be considered as free parameters. They are set
within the <code>shin92exaclove</code> function, and hence can't be changed
without re-writing the function.
</p>
<p>This simulation was reported in Wills et al. (2017). 
</p>


<h3>Value</h3>

<p>A matrix of predicted response probabilities, in the same order and
format as the observed data contained in <code><a href="#topic+shin92">shin92</a></code>.
</p>


<h3>Author(s)</h3>

<p>Andy Wills &amp; Garret O'Connell
</p>


<h3>References</h3>

<p>Shin, H.J. &amp;  Nosofsky, R.M. (1992). Similarity-scaling studies of
dot-pattern classification and recognition. <em>Journal
of Experimental Psychology: General, 121</em>, 278&ndash;304.
</p>
<p>Wills, A.J., O'Connell, G., Edmunds, C.E.R. &amp; Inkster, A.B. (2017).
Progress in modeling through distributed collaboration: Concepts,
tools, and category-learning examples. <em>The Psychology of
Learning and Motivation, 66</em>, 79-115.
</p>

<hr>
<h2 id='shin92protoalcove_opt'>
Parameter optimization of proto-ALCOVE model with shin92 CIRP
</h2><span id='topic+shin92protoalcove_opt'></span>

<h3>Description</h3>

<p>Uses <code><a href="#topic+shin92protoalcove">shin92protoalcove</a></code> to find best-fitting parameters for
the proto-ALCOVE model for the <code><a href="#topic+shin92">shin92</a></code> CIRP.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
  shin92protoalcove_opt(params = c(2,1,.25,.75), recompute = FALSE,
  trace = 0)

</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="shin92protoalcove_opt_+3A_params">params</code></td>
<td>
<p>A vector containing the initial values for c, phi, la,
and lw, in that order. See <code><a href="#topic+slpALCOVE">slpALCOVE</a></code> for an explanation
of these parameters. Where <code>recompute</code> is FALSE, this argument
has no effect.</p>
</td></tr>
<tr><td><code id="shin92protoalcove_opt_+3A_recompute">recompute</code></td>
<td>
<p>When set to TRUE, the function re-runs the
optimization (which takes about 10 minutes on a 2.4 GHz
processor). When set to FALSE, the function returns a stored copy of
the results of the optimization (which is instantaneous).</p>
</td></tr>
<tr><td><code id="shin92protoalcove_opt_+3A_trace">trace</code></td>
<td>
<p>Sets the level of tracing information (i.e. information
about the progress of the optimization), as defined by the
<code><a href="stats.html#topic+optim">optim</a></code> function. Set to 6 for maximally verbose
output. Where <code>recompute</code> is FALSE, this argument has no effect.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is an archive of the optimization procedure used to
derive the best-fitting parameters for the <code><a href="#topic+shin92protoalcove">shin92protoalcove</a></code>
simulation; see Spicer et al. (2017) for a tutorial introduction to
the concept of simulation archives.
</p>
<p>Optimization used the L-BFGS-B method from the <code><a href="stats.html#topic+optim">optim</a></code>
function of the standard R <code>stats</code> package. The objective
function was sum of squared errors. Please inspect the source code for
further details (e.g. type <code>shin92protoalcove_opt</code>). 
</p>
<p>This function was run in 16 times from different starting points,
using 8 threads on a Core i7 3.6 GHz processor. The default
parameters of this function are those for the best fit from those 16
starting points. The 16 starting points were
</p>
<p><code>
    pset &lt;- rbind(
      c(2,1,.25,.25),c(2,1,.25,.75),c(2,1,.75,.25),c(2,1,.75,.75),
      c(2,3,.25,.25),c(2,3,.25,.05),c(2,3,.75,.25),c(2,3,.75,.75),
      c(8,1,.25,.25),c(8,1,.25,.75),c(8,1,.75,.25),c(8,1,.75,.75),
      c(8,3,.25,.25),c(8,3,.25,.75),c(8,3,.75,.25),c(8,3,.75,.75)
    )
  </code>
</p>
<p>not all of which converged successfully.
</p>


<h3>Value</h3>

<p>A vector containing the best-fitting values for c, phi, la,
and lw, in that order. See <code><a href="#topic+slpALCOVE">slpALCOVE</a></code> for an explanation
of these parameters.
</p>


<h3>Author(s)</h3>

<p>Andy Wills
</p>


<h3>References</h3>

<p>Spicer, S., Jones, P.M., Inkster, A.B., Edmunds, C.E.R. &amp; Wills,
A.J. (2017). Progress in learning theory through distributed
collaboration: Concepts, tools, and examples. <em>Manuscript in
preparation</em>.
</p>

<hr>
<h2 id='shin92train'>
Input representation of shin92 for models input-compatible with
slpALCOVE.
</h2><span id='topic+shin92train'></span>

<h3>Description</h3>

<p>Creates randomized training and transfer blocks for CIRP <code>shin92</code>
, in a format suitable for the <code>slpALCOVE</code> model, and any other
model that uses the same input representation format. The stimulus
co-ordinates come from a MDS solution reported by Shin &amp; Nosofsky
(1992). 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
  shin92train(condition = 'equal3', learn.blocks = 8, trans.blocks = 3,
          absval = -1, format = 'mds', subjs = 1, seed = 8416, missing =
          'geo')

</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="shin92train_+3A_condition">condition</code></td>
<td>
<p>Experimental condition 'equal3', 'equal10',
'unequal3', or 'unequal10', as defined by Shin &amp; Nosofsky (1992).</p>
</td></tr>
<tr><td><code id="shin92train_+3A_learn.blocks">learn.blocks</code></td>
<td>
<p>Number of training blocks to generate. Omit this
argument to get the same number of training blocks as the published
study (8).</p>
</td></tr>
<tr><td><code id="shin92train_+3A_trans.blocks">trans.blocks</code></td>
<td>
<p>Number of transfer blocks to generate. Omit this
argument to get the same number of transfer blocks as the published
study (3).</p>
</td></tr>
<tr><td><code id="shin92train_+3A_absval">absval</code></td>
<td>
<p>Teaching value to be used where category is absent.</p>
</td></tr>
<tr><td><code id="shin92train_+3A_format">format</code></td>
<td>
<p>Specifies format used for input representation. Only one
format is currently suported, so this option is provided solely to
support future development.</p>
</td></tr>
<tr><td><code id="shin92train_+3A_subjs">subjs</code></td>
<td>
<p>Number of simulated subjects to be run.</p>
</td></tr>
<tr><td><code id="shin92train_+3A_seed">seed</code></td>
<td>
<p>Sets the random seed</p>
</td></tr>
<tr><td><code id="shin92train_+3A_missing">missing</code></td>
<td>
<p>If set to 'geo', output missing dimension flags (see
below)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A matrix is produced, with one row for each trial, and with the
following columns:
</p>
<p><code>ctrl</code> - Set to 1 (reset model) for trial 1, set to zero (normal
trial) for all other training trials, and set to 2 (freeze learning) for
all transfer trials.
</p>
<p><code>cond</code> - 1 = equal3, 2 = equal10, 3 = unequal3, 4 = unequal10
</p>
<p><code>phase</code> - 1 = training, 2 = transfer
</p>
<p><code>blk</code> - block of trials
</p>
<p><code>stim</code> - stimulus number; these correspond to the rows in Tables A3
and A4 of Shin &amp; Nosofsky (1992)
</p>
<p><code>x1 ... x6</code> - input representation. These are the co-ordinates of
an MDS solution for these stimuli (see Shin &amp; Nosofsky, 1992, Tables A3
and A4). Note: Size 3 conditions have a four-dimensional MDS solution,
so the output is x1 ... x4
</p>
<p><code>t1, t2</code> - teaching signal (1 = category present, absval = category
absent)
</p>
<p><code>m1 ...  m6</code> - Missing dimension flags (always set to zero in this
experiment, indicating all input dimensions are present on all
trials). Note: ranges from m1 to m4 for Size 3 conditions. Only produced
if <code>missing = 'geo'</code>.
</p>
<p>Although the trial ordering is random, a random seed is used, so
multiple calls of this function with the same parameters should produce
the same output. This is usually desirable for reproducibility and
stability of non-linear optimization. To get a different order, use the
seed argument to set a different seed.
</p>
<p>This function was originally developed to support simulations reported
in Wills et al. (2017).
</p>


<h3>Value</h3>

<p>R by C matrix, where each row is one trial, and the columns contain
model input.
</p>


<h3>Author(s)</h3>

<p>Andy Wills
</p>


<h3>References</h3>

<p>Shin, H.J. &amp;  Nosofsky, R.M. (1992). Similarity-scaling studies of
dot-pattern classification and recognition. <em>Journal
of Experimental Psychology: General, 121</em>, 278-304.
</p>
<p>Wills, A.J., O'Connell, G., Edmunds, C.E.R. &amp; Inkster, A.B. (2017).
Progress in modeling through distributed collaboration: Concepts,
tools, and category-learning examples. <em>The Psychology of
Learning and Motivation, 66</em>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+shin92">shin92</a></code>, <code><a href="#topic+shin92oat">shin92oat</a></code>, <code><a href="#topic+slpALCOVE">slpALCOVE</a></code>
</p>

<hr>
<h2 id='slpALCOVE'>
ALCOVE category learning model
</h2><span id='topic+slpALCOVE'></span>

<h3>Description</h3>

<p>Kruschke's (1992) category learning model. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
slpALCOVE(st, tr, dec = 'ER', humble = TRUE, attcon = FALSE, absval = -1,
          xtdo = FALSE)

</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="slpALCOVE_+3A_st">st</code></td>
<td>
<p>List of model parameters</p>
</td></tr>
<tr><td><code id="slpALCOVE_+3A_tr">tr</code></td>
<td>
<p>R-by-C matrix of training items</p>
</td></tr>
<tr><td><code id="slpALCOVE_+3A_dec">dec</code></td>
<td>
<p>String defining decision rule to be used</p>
</td></tr>
<tr><td><code id="slpALCOVE_+3A_humble">humble</code></td>
<td>
<p>Boolean specifying whether a humble or strict teacher is
to be used</p>
</td></tr>
<tr><td><code id="slpALCOVE_+3A_attcon">attcon</code></td>
<td>
<p>Boolean specifying whether attention is constrained</p>
</td></tr>
<tr><td><code id="slpALCOVE_+3A_absval">absval</code></td>
<td>
<p>Real number specifying teaching value for category
absence</p>
</td></tr>
<tr><td><code id="slpALCOVE_+3A_xtdo">xtdo</code></td>
<td>
<p>Boolean specifying whether to write extended information
to the console (see below).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The coverage in this help file is relatively brief; Catlearn Research
Group (2016) provides an introduction to the mathematics of the ALCOVE
model, whilst a more extensive tutorial on using slpALCOVE can be
found in Wills et al. (2016).
</p>
<p>The functions works as a stateful list processor. Specifically, it takes
a matrix as an argument, where each row is one trial for the network,
and the columns specify the input representation, teaching signals, and
other control signals. It returns a matrix where each row is a trial,
and the columns are the response probabilities at the output units. It
also returns the final state of the network (attention and connection
weights), hence its description as a 'stateful' list processor.
</p>
<p>Argument <code>st</code> must be a list containing the following items:
</p>
<p><code>colskip</code> - skip the first N columns of the tr array, where N =
colskip. colskip should be set to the number of optional columns you
have added to matrix tr, PLUS ONE. So, if you have added no optional
columns, colskip = 1. This is because the first (non-optional) column
contains the control values, below.
</p>
<p><code>c</code> - specificity constant (Kruschke, 1992, Eq. 1). Positive real
number. Scales psychological space.
</p>
<p><code>r</code> - distance metric (Kruschke, 1992, Eq. 1). Set to 1
(city-block) or 2 (Euclidean).
</p>
<p><code>q</code> - similarity gradient (Kruschke, 1992, Eq. 1). Set to 1
(exponential) or 2 (Gaussian).
</p>
<p><code>phi</code> - decision constant. For decision rule <code>ER</code>, it is
referred to as mapping constant phi, see Kruschke (1992, Eq. 3). For
decision rule <code>BN</code>, it is referred to as the background noise
constant b, see Nosofsky et al. (1994, Eq. 3).
</p>
<p><code>lw</code> - associative learning rate (Kruschke, 1992, Eq. 5) . Real
number between 0 and 1.
</p>
<p><code>la</code> - attentional learning rate (Kruschke, 1992, Eq. 6). Real
number between 0 and 1.
</p>
<p><code>h</code> - R by C matrix of hidden node locations in psychological
space, where R = number of input dimensions and C = number of hidden
nodes.
</p>
<p><code>alpha</code> - vector of length N giving initial attention weights for
each input dimension, where N = number of input dimensions. If you are
not sure what to use here, set all values to 1.
</p>
<p><code>w</code> - R by C matrix of initial associative strengths, where R =
number of output units and C = number of hidden units. If you are not
sure what to use here, set all values to zero.
</p>
<p>Argument <code>tr</code> must be a matrix, where each row is one trial
presented to the network. Trials are always presented in the order
specified. The columns must be as described below, in the order
described below:
</p>
<p><code>ctrl</code> - vector of control codes. Available codes are: 0 = normal
trial, 1 = reset network (i.e. set attention weights and associative
strengths back to their initial values as specified in h and w (see
below)), 2 = Freeze learning. Control codes are actioned before the
trial is processed.
</p>
<p><code>opt1, opt2, ...</code> - optional columns, which may have any names
you wish, and you may have as many as you like, but they must be
placed after the ctrl column, and before the remaining columns (see
below). These optional columns are ignored by this function, but you
may wish to use them for readability. For example, you might include
columns for block number, trial number, and stimulus ID number. The
argument colskip (see above) must be set to the number of optional
columns plus 1.
</p>
<p><code>x1, x2, ...</code> - input to the model, there must be one column for
each input unit. Each row is one trial.
</p>
<p><code>t1, t2, ...</code> - teaching signal to model, there must be one
column for each output unit. Each row is one trial. If the stimulus is a
member of category X, then the teaching signal for output unit X must be
set to +1, and the teaching signal for all other output units must be
set to <code>absval</code>.
</p>
<p><code>m1, m2, ...</code> - missing dimension flags, there must be one column
for each input unit. Each row is one trial. Where m = 1, that input unit
does not contribute to the activation of the hidden units on that
trial. This permits modelling of stimuli where some dimensions are
missing on some trials (e.g. where modelling base-rate negelct,
Kruschke, 1992, p. 29&ndash;32). Where m = 0, that input unit contributes as
normal. If you are not sure what to use here, set to zero.
</p>
<p>Argument <code>dec</code>, if specified, must take one of the following
values:
</p>
<p><code>ER</code> specifies an exponential ratio rule (Kruschke, 1992, Eq. 3).
</p>
<p><code>BN</code> specifies a background noise ratio rule (Nosofsky et al.,
1994, Eq. 3). Any output activation lower than zero is set to zero
before entering into this rule.
</p>
<p>Argument <code>humble</code> specifies whether a humble or strict teacher is
to be used. The function of a humble teacher is specified in Kruschke
(1992, Eq. 4b). In this implementation, the value -1 in Equation 4b is
replaced by <code>absval</code>.
</p>
<p>Argument <code>attcon</code> specifies whether attention should be constrained
or not. <em>If you are not sure what to use here, set to FALSE</em>. Some
implementations of ALCOVE (e.g. Nosofsky et al., 1994) constrain the sum
of the attentional weights to always be 1 (personal communication,
R. Nosofsky, June 2015). The implementation of attentional constraint in
<em>alcovelp</em> is the same as that used by Nosofsky et al. (1994), and
present as an option in the source code available from Kruschke's
website (Kruschke, 1991).
</p>
<p>Argument <code>xtdo</code> (eXTenDed Output), if set to TRUE, will output to
the console the following information on every trial: (1) trial number,
(2) attention weights at the end of that trial, (3) connection weights
at the end of that trial, one row for each output unit. This output can
be quite lengthy, so diverting the output to a file with the <code>sink</code>
command prior to running <code>alcovelp</code> with extended output is
advised.
</p>


<h3>Value</h3>

<p>Returns a list containing three components: (1) matrix of response
probabilities for each output unit on each trial, (2) attentional
weights after final trial, (3) connection weights after final trial. 
</p>


<h3>Author(s)</h3>

<p>Andy Wills
</p>


<h3>References</h3>

<p>Catlearn Research Group (2016). Description of ALCOVE.
<a href="http://catlearn.r-forge.r-project.org/desc-alcove.pdf">http://catlearn.r-forge.r-project.org/desc-alcove.pdf</a>
</p>
<p>Kruschke, J. (1991). <em>ALCOVE.c</em>. Retrieved 2015-07-20, page since
removed, but archival copy here:
<a href="https://web.archive.org/web/20150605210526/http://www.indiana.edu/~kruschke/articles/ALCOVE.c">https://web.archive.org/web/20150605210526/http://www.indiana.edu/~kruschke/articles/ALCOVE.c</a>
</p>
<p>Kruschke, J. (1992). ALCOVE: an exemplar-based connectionist model of
category learning. <em>Psychological Review, 99</em>, 22-44
</p>
<p>Nosofsky, R.M., Gluck, M.A., Plameri, T.J., McKinley, S.C. and
Glauthier, P.  (1994). Comparing models of rule-based classification
learning: A replication and extension of Shepaard, Hovland, and
Jenkins (1961). <em>Memory and Cognition, 22</em>, 352-369.
</p>
<p>Wills, A.J., O'Connell, G., Edmunds, C.E.R., &amp; Inkster,
A.B.(2017). Progress in modeling through distributed collaboration:
Concepts, tools, and category-learning examples. <em>Psychology of
Learning and Motivation, 66</em>, 79-115.
</p>

<hr>
<h2 id='slpBM'>
Bush &amp; Mosteller (1951) simple associative learning model
</h2><span id='topic+slpBM'></span>

<h3>Description</h3>

<p>A model often attributed to Bush &amp; Mosteller (1951), more precisely
this is the separable error term learning equation discussed by
authors such as Mackintosh (1975) and Le Pelley (2004); see Note 1.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
slpBM(st, tr, xtdo = FALSE)

</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="slpBM_+3A_st">st</code></td>
<td>
<p> List of model parameters
</p>
</td></tr>
<tr><td><code id="slpBM_+3A_tr">tr</code></td>
<td>
<p> R matrix of training items
</p>
</td></tr>
<tr><td><code id="slpBM_+3A_xtdo">xtdo</code></td>
<td>
<p> Boolean specifying whether to include extended information in the output (see below)
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function operates as a stateful list processor (slp; see Wills et
al., 2017). Specifically, it takes a matrix (tr) as an argument, where
each row represents a single training trial, while each column
represents the different types of information required by the model,
such as the elemental representation of the training stimuli, and the
presence or absence of an outcome. It returns the output activation on
each trial (a.k.a sum of associative strengths of cues present on that
trial), as a vector. The slpBM function also returns the final state of
the model - a vector of associative strengths between each stimulus and
the outcome representation.
</p>
<p>Argument <code>st</code> must be a list containing the following items:
</p>
<p><code>lr</code> - the learning rate (fixed for a given simulation), as denoted
by, for example, <code>theta</code> in Equation 1 of Mackintosh (1975). If you
want different elements to differ in salience (different alpha values)
use the input activations (x1, x2, ..., see below) to represent
element-specific salience.
</p>
<p><code>w</code> - a vector of initial associative strengths. If you are not
sure what to use here, set all values to zero.
</p>
<p><code>colskip</code> - the number of optional columns to be skipped in the tr
matrix. colskip should be set to the number of optional columns you have
added to the tr matrix, PLUS ONE. So, if you have added no optional
columns, colskip=1. This is because the first (non-optional) column
contains the control values (details below).
</p>
<p>Argument <code>tr</code> must be a matrix, where each row is one trial
presented to the model. Trials are always presented in the order
specified. The columns must be as described below, in the order
described below:
</p>
<p><code>ctrl</code> - a vector of control codes. Available codes are: 0 = normal
trial; 1 = reset model (i.e. set associative strengths (weights) back to
their initial values as specified in w (see above)); 2 = Freeze
learning. Control codes are actioned before the trial is processed.
</p>
<p><code>opt1, opt2, ...</code> - any number of preferred optional columns, the
names of which can be chosen by the user. It is important that these
columns are placed after the control column, and before the remaining
columns (see below). These optional columns are ignored by the
function, but you may wish to use them for readability. For example, you
might choose to include columns such as block number, trial number and
condition. The argument colskip (see above) must be set to the number of
optional columns plus one.
</p>
<p><code>x1, x2, ...</code> - activation of any number of input elements. There
must be one column for each input element. Each row is one trial. In
simple applications, one element is used for each stimulus (e.g. a
simulation of blocking (Kamin, 1969), A+, AX+, would have two inputs,
one for A and one for X). In simple applications, all present elements
have an activation of 1 and all absence elements have an activation of
0. However, slpBM supports any real number for activations, e.g. one
might use values between 0 and 1 to represent differing cue saliences.
</p>
<p><code>t</code> - Teaching signal (a.k.a. lambda). Traditionally, 1 is used to
represent the presence of the outcome, and 0 is used to represent the
absence of the outcome, altough slpBM supports any real values for lambda..
</p>
<p>Argument <code>xtdo</code> (eXTenDed Output) - if set to TRUE, function will
return the associative strengths for the end of each trial (see Value).
</p>


<h3>Value</h3>

<p>Returns a list containing two components (if xtdo = FALSE) or three 
components (if xtdo = TRUE, xout is also returned):
</p>
<table role = "presentation">
<tr><td><code>st</code></td>
<td>
<p>Vector of final associative strengths
</p>
</td></tr>
<tr><td><code>suma</code></td>
<td>
<p>Vector of output activations for each trial
</p>
</td></tr>
<tr><td><code>xout</code></td>
<td>
<p>Matrix of associative strengths at the end of each trial
</p>
</td></tr>
</table>


<h3>Note</h3>

<p>1. Bush &amp; Mosteller's (1951) Equations 2 outputs response probability,
not associative strength. Also, it has two learning rate paramters,
<code>a</code> and <code>b</code>. At least to a first approximation, <code>b</code>
serves a similar function to <code>beta-outcome-absent</code> in Rescorla &amp;
Wagner (1972), and <code>a-b</code> is similar to
<code>beta-outcome-present</code> in that same model.
</p>


<h3>Author(s)</h3>

<p>Lenard Dome, Stuart Spicer, Andy Wills
</p>


<h3>References</h3>

<p>Bush, R. R., &amp; Mosteller, F. (1951). A mathematical model for
simple learning. <em>Psychological Review, 58(5)</em>, 313-323.
</p>
<p>Kamin,  L.J. (1969). Predictability,  surprise,  attention  and  conditioning. 
In Campbell, B.A. &amp; Church, R.M. (eds.), <em>Punishment and  Aversive  
Behaviour</em>. New York: Appleton-Century-Crofts, 1969,  pp.279-296.
</p>
<p>Le Pelley, M.E. (2004). The role of associative history in models of
associative learning: A selective review and a hybrid model,
<em>Quarterly Journal of Experimental Psychology, 57B</em>, 193-243.
</p>
<p>Mackintosh, N.J. (1975). A theory of attention: Variations in the
associability of stimuli with reinforcement, <em>Psychological Review,
82</em>, 276-298.
</p>
<p>Rescorla, R. A., &amp; Wagner, A. R. (1972). A theory of Pavlovian
conditioning: Variations in the effectiveness of reinforcement and
nonreinforcement. In A. H. Black &amp; W. F. Prokasy (Eds.), <em>Classical
conditioning II: Current research and theory</em> (pp. 64-99). New York:
Appleton-Century-Crofts.
</p>
<p>Spicer, S., Jones, P.M., Inkster, A.B., Edmunds, C.E.R. &amp; Wills,
A.J. (n.d.). Progress in learning theory through distributed
collaboration: Concepts, tools, and examples. <em>Manuscript in preparation</em>.
</p>
<p>Wills, A.J., O'Connell, G., Edmunds, C.E.R., &amp; Inkster,
A.B.(2017). Progress in modeling through distributed collaboration:
Concepts, tools, and category-learning examples. <em>Psychology of
Learning and Motivation, 66</em>, 79-115.
</p>

<hr>
<h2 id='slpCOVIS'>
COVIS category learning model
</h2><span id='topic+slpCOVIS'></span>

<h3>Description</h3>

<p>COmpetition between Verbal and Implicit Systems model of category
learning (Ashby et al. 1998), as described in Ashby et al. (2011). The
current implementation supports two-category experiments, and uses
only single-dimension, not-below-chance, rules in the Explicit system.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
slpCOVIS(st, tr, crx = TRUE, respt = FALSE, rgive = TRUE, xtdo = FALSE)

</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="slpCOVIS_+3A_st">st</code></td>
<td>
<p>List of model parameters</p>
</td></tr>
<tr><td><code id="slpCOVIS_+3A_tr">tr</code></td>
<td>
<p>R-by-C matrix of training items</p>
</td></tr>
<tr><td><code id="slpCOVIS_+3A_crx">crx</code></td>
<td>
<p>Boolean. Explicit System. If set to TRUE, the current rule
is included in the random selection of a rule to receive a weight
increase from the Possion distribution. If set to FALSE, the current
rule is not included in this random selection.</p>
</td></tr>
<tr><td><code id="slpCOVIS_+3A_respt">respt</code></td>
<td>
<p>Set to FALSE for the behaviour described in Note 5;
behaviour when TRUE is undocumented</p>
</td></tr>
<tr><td><code id="slpCOVIS_+3A_rgive">rgive</code></td>
<td>
<p>Set to TRUE; FALSE is undocumented</p>
</td></tr>
<tr><td><code id="slpCOVIS_+3A_xtdo">xtdo</code></td>
<td>
<p>Set to FALSE; TRUE is undocumented</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The coverage in this help file is relatively brief; for a more
extensive tutorial, see Inkster et al. (n.d.).
</p>
<p>The function works as a stateful list processor (slp; see Wills et
al., 2017). Specifically, it takes a matrix (tr) as an argument, where
each row is one trial for the network, and the columns specify the
input representation. It returns a List containing the predictions
made by the model and the final state of the model, hence its
description as a 'stateful' list processor.
</p>
<p>Argument <code>st</code> must be a list containing the following
information. Parameter names given in brackets in the descriptions
below follow the naming conventions of Ashby et al. (2011), and
Edmunds &amp; Wills (2016). Equation numbers are from Ashby et al. (2011);
where there is no equation, the page number is given instead.
</p>
<p>Explicit system variables:
</p>
<p><code>envar</code> - (sigma^2_E) - p. 68 - Variance of the noise distribution
used to determine which response the explicit system makes on the
current trial. See Note 4, below.
</p>
<p><code>decbound</code> - (C) - Eq. 1 - location of the decision bound on a
single dimension. In the current implementation of slpCOVIS, this
location is the same for all dimensions.
</p>
<p><code>corcon</code> - (delta_c) - Eq. 2 - constant by which to increase
current rule saliency in the case of a correct response.
</p>
<p><code>errcon</code> - (delta_e) - Eq. 3 - constant by which to decrease
current rule saliency in the case of an incorrect response.
</p>
<p><code>perscon</code> - (gamma) - Eq. 4 - perseveration constant, i.e. value
to add to the salience of the current rule to obtain its rule weight.
</p>
<p><code>lambda</code> - (lambda) - Eq. 5 - Mean of the Poission
distribution. A value randomly sampled from the Poisson distribution
is added to a randomly-selected rule when calculating the weights for
new rule selection.
</p>
<p><code>decsto</code> - (a) - Eq. 7 - decision stochasticity when using rule
weights to select the rule for the next trial. For Ashby et
al. (2011)'s implementation, a = 1. For other uses, see Edmunds &amp;
Wills (2016).
</p>
<p>Procedural system variables:
</p>
<p><code>sconst</code> - (alpha) - Eq. 8 - scaling constant for cortical unit
activation. See Note 3, below.
</p>
<p><code>invar</code> - (sigma^2_p) - Eq. 9 - Variance of the
normally-distributed noise used to calculate striatal unit activation.
</p>
<p><code>dbase</code> - (D_base) - Eq. 10 - baseline dopamine level.
</p>
<p><code>alphaw</code> - (alpha_w) - Eq. 10 - Learning rate parameter in force
when striatal activation is above the NMDA threshold, and dopamine is
above baseline.
</p>
<p><code>betaw</code> - (beta_w) - Eq. 10 - Learning rate parameter in force
when striatal activation is above the NMDA threshold, and dopamine is
below baseline.
</p>
<p><code>gammaw</code> - (gamma_w) - Eq. 10 - Learning rate parameter in force
when striatal activation is between the AMPA and NMDA thresholds.
</p>
<p><code>nmda</code> - (theta_NMDA) - Eq. 10 - Activation threshold for
post-synaptic NMDA.
</p>
<p><code>ampa</code> - (theta_AMPA) - Eq. 10 - Activation threshold for
post-synaptic AMPA. See Note 1, below.
</p>
<p><code>wmax</code> - (w_max) - Eq. 10 - Intended upper weight limit for a
cortico-striatal link. See Note 2, below.
</p>
<p><code>prep</code> - ( P_(n-1) ) - Eq. 12 - predicted reward value
immediately prior to first trial. If unsure, set to zero.
</p>
<p><code>prer</code> - ( R_(n-1) ) - Eq. 12 - obtained reward value immediately
prior to first trial. If unsure, set to zero.
</p>
<p>Competition / decision system variables:
</p>
<p><code>emaxval</code> - p.77 - The maximum possible value of the the Explicit
system's discriminant variable. For example, if the stimulus value
varies from zero to one, and C (see above) is 0.5, then the maximum
value is 1-0.5 = 0.5
</p>
<p><code>etrust</code> - (theta_E) - Eq. 15 - trust in the explicit system
immediately prior to first trial. If unsure, set to .99.
</p>
<p><code>itrust</code> - (theta_P) - p. 77 - trust in the procedural system
immediately prior to first trial. If unsure, set to .01. See also Note
7, below.
</p>
<p><code>ocp</code> - (delta_OC) - Eq. 15 - constant used to increase trust in
the Explicit system after it suggests a response that turns out to be
correct.
</p>
<p><code>oep</code> - (delta_OE) - Eq. 16 - constant used to decrease trust in
the Explicit system after it suggests a response that turns out to be
incorrect.
</p>
<p>Initial state of model:
</p>
<p><code>initrules</code> - vector of length <code>stimdim</code>, representing the
initial salience of each single-dimensional rule in the Explicit
system.
</p>
<p><code>crule</code> - a number indicating which rule is in use immediately
prior to the first trial (1 = dimension 1, 2 = dimension 2, etc). If
this is not meaningful in the context of your simulation, set it to
zero, and ensure ctrl = 1 in the first row of your training matrix
(see below). This will then randomly pick an initial rule.
</p>
<p><code>initsy</code> - matrix of <code>stimdim</code> rows and two columns -
contains the initial values for the cortico-striatal connection
strengths.
</p>
<p><code>scups</code> - matrix of <code>stimdim</code> columns and as many rows as
you wish to have cortical input units. Each row represents the
position of a cortical unit in N-dimensional stimulus space.
</p>
<p>And finally, a couple of things slpCOVIS needs to interpret your tr
matrix (see below):
</p>
<p><code>stimdim</code> - number of stimulus dimensions in the input
representation.
</p>
<p><code>colskip</code> - skip the first N columns of the tr array, where N =
colskip. colskip should be set to the number of optional columns you
have added to matrix tr, PLUS ONE. So, if you have added no optional
columns, colskip = 1. This is because the first (non-optional) column
contains the control values, see below.
</p>
<p>Argument <code>tr</code> must be a matrix, where each row is one trial
presented to the network. Trials are always presented to the model in
the order specified. The columns must be as described below, in the
order described below:
</p>
<p><code>ctrl</code> - vector of control codes. Available codes are: 0 = normal
trial, 1 = reset network (i.e. set back to the state defined in list
<code>st</code> and randomly select an initial rule for the Explicit System
using Eq. 7) , 2 = Freeze learning. Control codes are actioned before the
trial is processed.
</p>
<p><code>opt1, opt2, ...</code> - optional columns, which may have any names
you wish, and you may have as many as you like, but they must be
placed after the ctrl column, and before the remaining columns (see
below). These optional columns are ignored by this function, but you
may wish to use them for readability. For example, you might include
columns for block number, trial number, and stimulus ID number. The
argument colskip (see above) must be set to the number of optional
columns plus 1.
</p>
<p><code>x1, x2, ...</code> - stimulus input to the model; there must be one
column for each stimulus dimension. 
</p>
<p><code>t1</code> - teaching signal to model. If the correct response is
Category 1, t = 1. If the correct response is Category 2, t =
-1. Experiments with something other than two categories are not
supported in the current implementation.
</p>
<p><code>optend1, optend2, ...</code> - optional columns, which may have any
names you wish, and you may have as many as you like, but they must be
placed after the t1 column. These optional columns are ignored by this
function, but may help with cross-compatibility with other model
implementations. For example, the additional 't' and 'm' columns of
input representations generated for slpALCOVE will be safely ignored
by slpCOVIS.
</p>


<h3>Value</h3>

<p>Returns a List containing eight components:
</p>
<table role = "presentation">
<tr><td><code>foutmat</code></td>
<td>
<p>A two-column matrix, representing the model's response
on each trial. For any given trial, [1,0] indicates a Category 1
response; [0,1] indicates a Category 2 response. Responses are
reported in this manner to facilitate cross-compatibility with models
that produce response probabilities on each trial.</p>
</td></tr>
<tr><td><code>frules</code></td>
<td>
<p>Explicit system - rule saliences after final trial</p>
</td></tr>
<tr><td><code>fsystr</code></td>
<td>
<p>Procedural system - cortico-striatal synaptic strengths
after final trial)</p>
</td></tr>
<tr><td><code>fetrust</code></td>
<td>
<p>Decision system - trust in explicit system after final
trial</p>
</td></tr>
<tr><td><code>fitrust</code></td>
<td>
<p>Decision system - trust in procedural system after
final trial</p>
</td></tr>
<tr><td><code>frule</code></td>
<td>
<p>Explicit system - rule used by explicit system on final
trial</p>
</td></tr>
<tr><td><code>fprep</code></td>
<td>
<p>Implicit system - predicted reward value on final trial</p>
</td></tr>
<tr><td><code>fprer</code></td>
<td>
<p>Implicit system - obtained reward value on final trial</p>
</td></tr>
</table>


<h3>Note</h3>

<p>1. Ashby et al. (2011) state (p. 74) that the intended operation of
COVVIS is theta_NMDA &gt; theta_AMPA, but the values they report are
theta_NMDA = .0022, theta_AMPA = .01.
</p>
<p>2.  Ashby et al. (2011) did not specify a value for w_max; Edmunds &amp;
Wills (2016) assumed the intended value was 1.
</p>
<p>3. Ashby et al. (2011) do not use Eq. 8 in their simulation, they
manually set sensory cortex activation to 1 for the presented stimulus
and 0 for all the others (p. 78). They thus do not have a value for
alpha. Edmunds &amp; Wills (2016) set alpha to 0.14, which produces
similar behaviour for 0,1 coded stimulus dimensions, without having to
manually set the activations.
</p>
<p>4. In Ashby et al. (2011) and Edmunds &amp; Wills (2016), sigma^2_E is set
to zero. In this implementation of slpRW, positive values should also
work but have not been extensively tested.
</p>
<p>5. In the descriptions provided by Ashby et al. (2011, p. 69 &amp; p. 75),
there is some ambiguity about the meaning of the term 'response' -
does this mean the response of a system (e.g. the Explicit system), or
the overall response (i.e. the output of the decision system). In the
current implementation, the response of the Explicit System is
compared to the feedback to determine whether the Explicit System was
correct or incorrect, and the response of the Procedural System is
compared to the feedback to determine whether the Procedural System
was correct or incorrect.
</p>
<p>6. It seems that in Ashby et al.'s (2011) simulations, each dimension
generates only one single-dimension rule for a two-category problem,
rather than two as one might expect (e.g. small = A, large = B, but
also large = A, small = B). Rules that would produce below-chance
responding are excluded from the rule set.
</p>
<p>7. Ashby et al. (2011) state that theta_E + theta_P = 1. However,
slpCOVIS does not perform this check on the initial state, so it is
important to check this manually. 
</p>


<h3>Author(s)</h3>

<p>Angus Inkster, Andy Wills, Charlotte Edmunds
</p>


<h3>References</h3>

<p>Ashby, F.G., Alfonso-Reese, L.A., Turken, A.U. &amp; Waldron, E.M. (1998).
A neuropsychological theory of multiple systems in category learning.
<em>Psychological Review, 105</em>, 442-481.
</p>
<p>Ashby, F.G., Paul, E.J., &amp; Maddox, W.T. (2011). COVIS. In Pothos, E.M. &amp;
Wills, A.J. (2011). <em>Formal approaches in
categorization</em>. Cambridge, UK: Cambridge University Press.
</p>
<p>Edmunds, C.E.R., &amp; Wills, A.J. (2016). Modeling category learning using
a dual-system approach: A simulation of Shepard, Hovland and Jenkins
(1961) by COVIS. In A. Papfragou, D. Grodner, D. Mirman, &amp;
J.C. Trueswell (Eds.). <em>Proceedings of the 38th Annual Conference
of the Cognitive Science Society</em> (pp. 69-74). Austin, TX: Cognitive
Science Society.
</p>
<p>Inkster, A.B., Edmunds, C.E.R., &amp; Wills, A.J. (n.d.). A
distributed-collaboration resource for dual-process modeling in category
learning. <em>Manuscript in preparation</em>.
</p>
<p>Pothos, E.M. &amp; Wills, A.J.(2011). <em>Formal approaches in 
Categorisation.</em>Cambridge: University Press.
</p>
<p>Wills, A.J., O'Connell, G., Edmunds, C.E.R., &amp; Inkster,
A.B.(2017). Progress in modeling through distributed collaboration:
Concepts, tools, and category-learning examples. <em>Psychology of
Learning and Motivation, 66</em>, 79-115.
</p>

<hr>
<h2 id='slpDGCM'>
Similarity-Dissimilarity Generalized Context Model (DGCM)
</h2><span id='topic+slpDGCM'></span>

<h3>Description</h3>

<p>Stewart and Morin (2007)'s extension to Nosofsky's (1984, 2011)
Exemplar-based Generalized Context Model. The implementation also
contains O'Bryan et al. (2018)'s version of the
Similarity-Dissimilarity Generalized Context Model, see Note 1.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  slpDGCM(st, test, dec = "BIAS", exemplar_mute = FALSE, exemplar_decay = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="slpDGCM_+3A_st">st</code></td>
<td>
<p>List of model parameters</p>
</td></tr>
<tr><td><code id="slpDGCM_+3A_test">test</code></td>
<td>
<p>Test matrix.</p>
</td></tr>
<tr><td><code id="slpDGCM_+3A_dec">dec</code></td>
<td>
<p>Decision mechanism. If <code>NOISE</code>, use O'Bryan et al. (2018)'s
background-noise decision rule. If <code>BIAS</code> (default), use
Stewart and Morin (2007)'s category-bias decision rule.</p>
</td></tr>
<tr><td><code id="slpDGCM_+3A_exemplar_mute">exemplar_mute</code></td>
<td>
<p>If <code>TRUE</code>, only those exemplars contribute evidence
to the decision rule, which have at least one feature
common with the current stimuli (O'Bryan et al., 2018).
If <code>FALSE</code> (default), all exemplars contribute.</p>
</td></tr>
<tr><td><code id="slpDGCM_+3A_exemplar_decay">exemplar_decay</code></td>
<td>
<p>If <code>TRUE</code> (default), exemplar weightings decay
as specified by Stewart and Morin (2007). If
<code>FALSE</code>, exemplar weightings are static.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This implementation houses the two version of DGCM.  In order to use the
instantiation of DGCM described in O'Bryan et al.  (2018), set
<code>exemplar_decay = FALSE</code> and <code>exemplar_mute = TRUE</code>.  The
default settings of the function will run the model that corresponds to
Stewart and Morin (2007).
</p>
<p>The functions works as a stateful list processor. Specifically, it takes
a data frame as an argument, where each row is one trial for the model,
and the columns specify the input representation, teaching signals, and
other control signals. It returns two matrices containing, for each
trial, response probabilities and the accumulated evidence for each
category.  It also returns the final state of the network (e.g. memory
decay), hence its description as a 'stateful' list processor, see Note
1.
</p>
<p>This implementation took the assumption that when <code>exemplar_decay =
TRUE</code>, memory strengths for exemplar are equal to each other at the
beginning of the test phase. In future releases, we plan to implement a
feature that allows initial memory strengths to be treated as freely
varying parameters.
</p>
<p><code>st</code> must be a list containing the following items:
</p>
<p><code>attentional_weights</code> - vector of attentional weights, where sum of
all elements equal to 1.
</p>
<p><code>c</code> - generalization constant.
</p>
<p><code>r</code> - The Minkowski metric parameter <em>r</em> gives a city block
metric when r = 1 (used for separable-dimension stimuli) and
a Euclidean metric when r = 2 (used for integral-dimension
stimuli).
</p>
<p><code>s</code> - similarity and dissimilarity weighting. If 0, evidence for a
category will be purely based on the dissimilarity between
current input vector and all exemplars from the other
categories. If it is 1, evidence for a given category will be
solely based on similarity to its own exemplars.
</p>
<p><code>t</code> - exemplar weighting. If <code>memory_decay = FALSE</code>, it is a
vector of exemplar-specific memory strength. If
<code>memory_decay = TRUE</code> (default), it is a vector of
exemplar-specific memory strengths that will update according
to the function as specified in Equation 4 in Stewart and
Morin (2007).
</p>
<p><code>beta</code> - category bias vector. Only used when <code>dec</code> set to
BIAS, otherwise ignored.  Currently, there is no restriction
in place on what values are allowed in this implementation,
but Stewart and Morin (2007) specifies that elements of
<code>beta</code> should sum to 1.
</p>
<p><code>base</code> - a vector of baseline level of similarity. This parameter
will control how much noise will spread over all categories
in the background-noise decision rule. It is only used if
<code>dec</code> is set to NOISE.
</p>
<p><code>gamma</code> - decision scaling constant. Only used when <code>dec</code> is
set to BIAS.
</p>
<p><code>theta</code> - decay rate. If <code>exemplar_decay = FALSE</code>, theta is
ignored.
</p>
<p><code>colskip</code> - the number of optional columns to skip in test plus
one. If you have no optional columns, set it to one.
</p>
<p><code>outcomes</code> - the number of categories.
</p>
<p><code>exemplars</code> - a matrix of exemplars and their corresponding
category indicated by a single integer.
</p>
<p><code>test</code> must be a <code>data.matix</code> with the following columns:
</p>
<p><code>opt1, opt2, ...</code> - any number of optional columns, the names of
which can be chosen by the user. These optional columns are ignored
by the slpDGCM function, but you may wish to use them for
readability.
</p>
<p><code>x1, x2, x3, ...</code>- input to the model, there must be one column
for each input unit. Each row is one trial. DGCM uses a nominal
stimulus representation, which means that features are coded as
either 0 (absent) or 1 (present).
</p>


<h3>Value</h3>

<p>If <code>exemplar_decay = FALSE</code>, returns a list of the following
matrices:
</p>
<p><code>v</code> A matrix of evidence accumulated for each category (columns)
on each trial (rows) as output by Equation 3 in Stewart and Morin
(2007).
</p>
<p><code>p</code> A matrix of response probabilities. Category responses
(columns) for each trial (rows).
</p>
<p>If <code>exemplar_decay = TURE</code>, the function also returns memory
decay for each trial, <code>decay</code>.
</p>


<h3>Note</h3>

<p>1. O'Bryan et al. (2018)'s version of the DGCM is not a stateful list processor, but
we decided to include it in the same implementation. In fact, Stewart and Morin
(2007)'s version only classifies as a stateful list processor, because of the memory
decay function.
</p>


<h3>Author(s)</h3>

<p> Lenard Dome, Andy Wills </p>


<h3>References</h3>

<p>Nosofsky, R. M. (1984). Choice, similarity, and the context theory of
classification. <em>Journal of Experimental Psychology: Learning,
memory, and cognition, 10</em>, 104.
</p>
<p>O'Bryan, Sean R., et al. (2018). Model-based fMRI reveals dissimilarity
processes underlying base rate neglect. <em>ELife 7</em>: e36395.
</p>
<p>Stewart, N., &amp; Morin, C. (2007). Dissimilarity is used as evidence of
category membership in multidimensional perceptual categorization: A
test of the similarity–dissimilarity generalized context
model. <em>Quarterly Journal of Experimental Psychology, 60</em>,
1337-1346.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  ## Replicate O'Bryan et al. (2018)
  # Exemplars
  stim = matrix(c(
                1,1,0,0,0,0, 1,
                1,0,1,0,0,0, 2,
                0,0,0,1,1,0, 3,
                0,0,0,1,0,1, 4), ncol = 7, byrow = TRUE)

  # Transfer/test stimuli
  # This is a row for each unique transfer stimulus
  tr = matrix(c(
               1, 1, 0, 0, 0, 0, #0,1,2
               1, 0, 1, 0, 0, 0, #3
               0, 0, 0, 1, 1, 0, #4,5,6
               0, 0, 0, 1, 0, 1, #7
               1, 0, 0, 0, 0, 0, #8
               0, 0, 0, 1, 0, 0, #9
               0, 1, 0, 0, 0, 0, #10
               0, 0, 1, 0, 0, 0, #11
               0, 0, 0, 0, 1, 0, #12
               0, 0, 0, 0, 0, 1, #13
               0, 1, 1, 0, 0, 0, #14, 15
               0, 0, 0, 0, 1, 1, #16, 17
               1, 0, 0, 0, 1, 0, #18
               1, 0, 0, 0, 0, 1, #19
               0, 1, 0, 1, 0, 0, #20
               0, 0, 1, 1, 0, 0, #21
               0, 0, 1, 0, 1, 0, #22, 23
               0, 1, 0, 0, 0, 1 #24, 25
               ),
               ncol = 6,
               byrow = TRUE)

  # parameters from paper
  aweights = c(0.27692188, 0.66524089, 0.88723335, 0.16967400, 0.71206208,
               0.87939732)

  st &lt;- list(attentional_weights = aweights/sum(abs(aweights)),
             c = 9.04906080,
             s = 0.94614863,
             b = 0.02250668,
             t = c(3, 1, 3, 1),
             beta = c(1, 1, 1, 1)/4,
             gamma = 1,
             theta = 0.4,
             r = 1,
             colskip = 1,
             outcomes = 4,
             exemplars = stim)

  slpDGCM(st, tr, exemplar_decay = FALSE, exemplar_mute = TRUE, dec = "NOISE")

</code></pre>

<hr>
<h2 id='slpDIVA'>DIVA category learning model</h2><span id='topic+slpDIVA'></span>

<h3>Description</h3>

  
<p>DIVergent Autoencoder (Kurtz, 2007; 2015) artificial neural network
category learning model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>slpDIVA(st, tr, xtdo = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="slpDIVA_+3A_st">st</code></td>
<td>
<p>List of model parameters</p>
</td></tr>
<tr><td><code id="slpDIVA_+3A_tr">tr</code></td>
<td>
<p>R-by-C matrix of training items</p>
</td></tr>
<tr><td><code id="slpDIVA_+3A_xtdo">xtdo</code></td>
<td>
<p>When set to TRUE, produce extended output</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function works as a stateful list processor (Wills et al.,
2017). Specifically, it takes a matrix as an argument, where each row
is one trial for the network, and the columns specify the input
representation, teaching signals, and other control signals. It
returns a matrix where each row is a trial, and the columns are the
response probabilities for each category. It also returns the final
state of the network (connection weights and other parameters), hence
its description as a 'stateful' list processor.
</p>
<p>Argument <code>st</code> must be a list containing the following items:
</p>
<p><code>st</code> must contain the following principal model parameters:
</p>
<p><code>learning_rate</code> - Learning rate for weight updates through
backpropagation. The suggested learning rate default is 
<code>learning_rate = 0.15</code>
</p>
<p><code>beta_val</code> - Scalar value for the Beta parameter. <code>beta_val</code>
controls the degree of feature focusing (not unlike attention) that
the model uses to make classification decisions (see: Conaway &amp; Kurtz,
2014; Kurtz, 2015). <code>beta_val = 0</code> turns feature focusing off.
</p>
<p><code>phi</code> - Scalar value for the phi parameter. <code>phi</code> is a
real-valued mapping constant, see Kruschke (1992, Eq. 3).
</p>
<p><code>st</code> must also contain the following information about network
architecture: 
</p>
<p><code>num_feats</code> - Number of input features.
</p>
<p><code>num_hids</code> - Number of hidden units. A rough rule of thumb for
this hyperparameter is to start with <code>num_feats = 2</code> and add
additional units if the model fails to converge.
</p>
<p><code>num_cats</code> - Number of categories.
</p>
<p><code>continuous</code> - A Boolean value to indicate if the model should
work in continuous input or binary input mode. Set <code>continuous =
    TRUE</code> when the inputs are continuous.
</p>
<p><code>st</code> must also contain the following information about the
initial state of the network:
</p>
<p><code>in_wts</code> - A matrix of initial input-to-hidden weights with
<code>num_feats + 1</code> rows and <code>num_hids</code> columns. Can be set to
<code>NULL</code> when the first line of the <code>tr</code> matrix includes
control code 1, <code>ctrl = 1</code>.
</p>
<p><code>out_wts</code> - A matrix of initial hidden-to-output weights with
<code>num_feats + 1</code> rows, <code>num_hids</code> columns and with the third
dimension being <code>num_cats</code> in extent. Can be set to <code>NULL</code>
when the first line of the <code>tr</code> matrix includes control code 1,
<code>ctrl = 1</code>.
</p>
<p><code>st</code> must also contain the following information so that it can
reset these weights to random values when ctrl = 1 (see below):
</p>
<p><code>wts_range</code> - A scalar value for the range of the
randomly-generated weights. The suggested weight range deafult is 
<code>wts_range = 1</code>
</p>
<p><code>wts_center</code> - A scalar value for the center of the
randomly-generated weights. This is commonly set to <code>wts_center =
  0</code>
</p>
<p><code>st</code> must also contain the following parameters that describe
your <code>tr</code> array:
</p>
<p><code>colskip</code> - Skip the first N columns of the tr array, where
<code>N = colskip</code>. <code>colskip</code> should be set to the number of
optional columns you have added to matrix <code>tr</code>, PLUS ONE. So, if
you have added no optional columns, <code>colskip = 1</code>. This is
because the first (non-optional) column contains the control values,
below.
</p>
<p>Argument <code>tr</code> must be a matrix, where each row is one trial
presented to the network. Trials are always presented in the order
specified. The columns must be as described below, in the order
described below:
</p>
<p><code>ctrl</code> - column of control codes. Available codes are: 0 = normal
learning trial, 1 = reset network (i.e. initialize a new set of
weights following the <code>st</code> parameters), 2 = Freeze
learning. Control codes are actioned before the trial is processed.
</p>
<p><code>opt1, opt2, ...</code> - optional columns, which may have any names
you wish, and you may have as many as you like, but they must be
placed after the <code>ctrl</code> column, and before the remaining columns
(see below). These optional columns are ignored by this function, but
you may wish to use them for readability. For example, you might
include columns for block number, trial number, and stimulus ID
number. The argument <code>colskip</code> (see above) must be set to the
number of optional columns plus 1.
</p>
<p><code>x1, x2, ...</code> - input to the model, there must be one column for
each input unit. Each row is one trial. Dichotomous inputs should be
in the format <code>-1, 1</code>. Continuous inputs should be scaled to the
range of <code>-1, 1</code>. As the model's learning objective is to
accurately reconstruct the inputs, the input to the model is also the
teaching signal. For testing under conditions of missing information,
input features can be set to 0 to negate the contribution of the
feature(s) for the classification decision of that trial.
</p>
<p><code>t1, t2, ...</code> - Category membership of the current
stimulus. There must be one column for each category. Each row is one
trial. If the stimulus is a member of category X, then the value in
the category X column must be set to <code>+1</code>, and the values for all
other category columns must be set to <code>-1</code>.
</p>


<h3>Value</h3>

<p>Returns a list containing two components: (1) matrix of response
probabilities for each category on each trial, (2) an <code>st</code> list
object that contains the model's final state. A weight initialization
history is also available when the extended output parameter is set
<code>xtdo = TRUE</code> in the <code>slpDIVA</code> call.
</p>


<h3>Note</h3>

<p>A faster (Rcpp) implementation of slpDIVA is planned for a future
release of catlearn.
</p>


<h3>Author(s)</h3>

<p>Garrett Honke, 
Nolan B. Conaway, 
Andy Wills
</p>


<h3>References</h3>

<p>Conaway, N. B., &amp; Kurtz, K. J. (2014). Now you know it, now you don't:
Asking the right question about category knowledge. In P. Bello,
M. Guarini, M. McShane, &amp; B. Scassellati (Eds.), <em>Proceedings of
the Thirty-Sixth Annual Conference of the Cognitive Science Society</em>
(pp. 2062-2067). Austin, TX: Cognitive Science Society.
</p>
<p>Kruschke, J. (1992). ALCOVE: an exemplar-based connectionist model of
category learning. <em>Psychological Review, 99</em>, 22-44
</p>
<p>Kurtz, K.J. (2007). The divergent autoencoder (DIVA) model of category
learning. <em>Psychonomic Bulletin &amp; Review, 14</em>, 560-576.
</p>
<p>Kurtz, K. J. (2015). Human Category Learning: Toward a Broader
Explanatory Account. <em>Psychology of Learning and Motivation, 63</em>.
</p>
<p>Wills, A.J., O'Connell, G., Edmunds, C.E.R., &amp; Inkster,
A.B.(2017). Progress in modeling through distributed collaboration:
Concepts, tools, and category-learning examples. <em>The Psychology
of Learning and Motivation, 66</em>, 79-115.
</p>

<hr>
<h2 id='slpEXIT'>
EXIT Category Learning Model
</h2><span id='topic+slpEXIT'></span>

<h3>Description</h3>

<p>EXemplar-based attention to distinctive InpuT model (Kruschke, 2001)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  slpEXIT(st, tr, xtdo = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="slpEXIT_+3A_st">st</code></td>
<td>
<p>List of model parameters</p>
</td></tr>
<tr><td><code id="slpEXIT_+3A_tr">tr</code></td>
<td>
<p>R-by-C matrix of training items</p>
</td></tr>
<tr><td><code id="slpEXIT_+3A_xtdo">xtdo</code></td>
<td>
<p>if <code>TRUE</code> extended output is returned</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The contents of this help file are relatively brief; a more extensive
tutorial on using slpEXIT can be found in Spicer et al. (n.d.).
</p>
<p>The functions works as a stateful list processor. Specifically, it
takes a data frame as an argument, where each row is one trial for the
network, and the columns specify the input representation, teaching
signals, and other control signals. It returns a matrix where each row
is a trial, and the columns are the response probabilities at the
output units. It also returns the final state of the network (cue -&gt;
exemplar, and cue -&gt; outcome weights), hence its description as a
'stateful' list processor.
</p>
<p>References to Equations refer to the equation numbers used in the
Appendix of Kruschke (2001). 
</p>
<p>Argument <code>tr</code> must be a data frame, where each row is one trial
presented to the network, in the order of their occurence.
<code>tr</code> requires the following columns:
</p>
<p><code>x1, x2, ...</code> - columns for each cue (<code>1</code> = cue present,
<code>0</code> = cue absent). These columns have to start with <code>x1</code>
ascending with features <code>..., x2, x3, ...</code> at adjacent
columns. See Notes 1, 2.
</p>
<p><code>t1, t2, ...</code> - columns for the teaching values indicating the
category feedback on the current trial. Each category needs a single
teaching signal in a dummy coded fashion, e.g., if the first category
is the correct category for that trial, then <code>t1</code> is set to
<code>1</code>, else it is set to <code>0</code>. These columns have to start with
<code>t1</code> ascending with categories <code>..., t2, t3, ...</code> at
adjacent columns.
</p>
<p><code>ctrl</code> - vector of control codes. Available codes are: 0 = normal
trial, 1 = reset network (i.e. reset connection weights to the values
specified in <code>st</code>). 2 = freeze learning. Control codes are
actioned before the trial processed.
</p>
<p><code>opt1, opt2, ...</code> - optional columns, which may have any name you
wish. These optional columns are ignored by this function, but you may
wish to use them for readability. For example, you might include
columns for block number, trial number, and stimulus ID..
</p>
<p>Argument <code>st</code> must be a list containing the following items:
</p>
<p><code>nFeat</code> - integer indicating the total number of possible
stimulus features, i.e. the number of <code>x1, x2, ...</code> columns in
<code>tr</code>.
</p>
<p><code>nCat</code> - integer indicating the total number of possible
categories, i.e. the number of <code>t1, t2, ...</code> columns in
<code>tr</code>.
</p>
<p><code>phi</code> - response scaling constant - Equation (2)
</p>
<p><code>c</code> - specificity parameter. Defines the narrowness of 
receptive field in exemplar node activation - Equation (3).
</p>
<p><code>P</code> - Attentional normalization power (attentional capacity) -
Equation (5). If <code>P</code> equals <code>1</code> then the attention weights
will satisfy the constraint that attention strength for currently
present features will sum to one. The sum of attention strengths for
present features grows as a function of <code>P</code>.
</p>
<p><code>l_gain</code> - attentional shift rate - Equation (7)
</p>
<p><code>l_weight</code> - learning rate for feature to category associations.
- Equation (8)
</p>
<p><code>l_ex</code> - learning rate for exemplar_node to gain_node associations
- Equation (9)
</p>
<p><code>iterations</code> - number of iterations of shifting attention on each
trial (see Kruschke, 2001, p. 1400). If you're not sure what to use
here, set it to 10.
</p>
<p><code>sigma</code> - Vector of cue saliences, one for each cue. If you're
not sure what to put here, use 1 for all cues except the bias cue. For
the bias cue, use some value between 0 and 1. 
</p>
<p><code>w_in_out</code> - matrix with <code>nFeat</code> columns and <code>nCat</code> rows,
defining the input-to-category association weights, i.e. how much each
feature is associated to a category (see Equation 1). The <code>nFeat</code> 
columns follow the same order as <code>x1, x2, ...</code> in <code>tr</code>, 
and likewise, the <code>nCat</code> rows follow the order of  
<code>t1, t2, ...</code>.
</p>
<p><code>exemplars</code> - matrix with <code>nFeat</code> columns and n rows, where
n is the number of exemplars, such that each row represents a single
exemplar in memory, and their corresponding feature values. 
The <code>nFeat</code> columns follow the same order as <code>x1, x2, ...</code> 
in <code>tr</code>. The n-rows follow the same order as in the 
<code>w_exemplars</code> matrix defined below. See Note 3.
</p>
<p><code>w_exemplars</code> - matrix which is structurally equivalent to 
<code>exemplars</code>. However, the matrix represents the associative weight
from the exemplar nodes to the gain nodes, as given in Equation 4.
The <code>nFeat</code> columns follow the same order as 
<code>x1, x2, ...</code> in <code>tr</code>. The n-rows follow the same order 
as in the <code>exemplars</code> matrix.
</p>


<h3>Value</h3>

<p>Returns a list containing three components (if xtdo = FALSE) or four
components (if xtdo = TRUE, <code>g</code> is also returned):
</p>
<table role = "presentation">
<tr><td><code>p</code></td>
<td>
<p>Matrix of response probabilities for each outcome on each
trial</p>
</td></tr>
<tr><td><code>w_in_out</code></td>
<td>
<p>Matrix of final cue -&gt; outcome associative strengths</p>
</td></tr>
<tr><td><code>w_exemplars</code></td>
<td>
<p>Matrix of final cue -&gt; exemplar associative
strengths</p>
</td></tr>
<tr><td><code>g</code></td>
<td>
<p>Vector of gains at the end of the final trial</p>
</td></tr>
</table>


<h3>Note</h3>

<p>1. Code optimization in slpEXIT means it's essential that every cue is
either set to 1 or to 0. If you use other values, it won't work
properly. If you wish to represent cues of unequal salience, use
<code>sigma</code>.
</p>
<p>2. EXIT simulations normally include a 'bias' cue, i.e. a cue that is
present on all trials. You will need to explicitly include this in
your input representation in <code>tr</code>. For an example, see the output
of <code>krus96train</code>.
</p>
<p>3. The bias cue should be included in these exemplar representations,
i.e. they should be the same as the representation of the stimuli in
<code>tr</code>. For an example, see the output of <code>krus96train</code>.
</p>


<h3>Author(s)</h3>

<p>René Schlegelmilch, Andy Wills, Angus Inkster</p>


<h3>References</h3>

<p>Kruschke, J. K. (1996). Base rates in category learning. <em>Journal 
of Experimental Psychology-Learning Memory and Cognition, 22</em>(1), 3-26.
</p>
<p>Kruschke, J. K. (2001). The inverse base rate effect is not explained
by eliminative inference. <em>Journal of Experimental Psychology:
Learning, Memory &amp; Cognition, 27</em>, 1385-1400.
</p>
<p>Spicer, S.G., Schlegelmilch, R., Jones, P.M., Inkster, A.B., Edmunds,
C.E.R. &amp; Wills, A.J. (n.d.). Progress in learning theory through
distributed collaboration: Concepts, tools, and
examples. <em>Manuscript in preparation</em>.
</p>

<hr>
<h2 id='slpLMSnet'>
Gluck &amp; Bower (1988) network model
</h2><span id='topic+slpLMSnet'></span>

<h3>Description</h3>

<p>Gluck and Bower (1988) adaptive least-mean-square (LMS) network
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
slpLMSnet(st, tr, xtdo = FALSE, dec = "logistic")

</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="slpLMSnet_+3A_st">st</code></td>
<td>
<p>List of model parameters</p>
</td></tr>
<tr><td><code id="slpLMSnet_+3A_tr">tr</code></td>
<td>
<p>Numerical matrix of training items, use <code>data.matrix()</code> if
matrix is not numeric.</p>
</td></tr>
<tr><td><code id="slpLMSnet_+3A_xtdo">xtdo</code></td>
<td>
<p>Boolean specifying whether to include extended information
in the output (see below)</p>
</td></tr>
<tr><td><code id="slpLMSnet_+3A_dec">dec</code></td>
<td>
<p>Specify what response rule to use. <code>logistic</code>,
Equation 7 in Gluck and Bower (1988), which will output probability
ratings for each outcome (these will not necessary sum to
one). <code>softmax</code>, Footnote 2 in Gluck and Bower (1988), which will
output response probabilities for each outcome (these will sum to
one).</p>
</td></tr>  </table>


<h3>Details</h3>

<p>The function operates as a stateful list processor (slp; see Wills et
al., 2017).  Specifically, it takes a matrix as an argument. Each row
represents a single trial. Each column represents different types of
information required by the implementation of the model, such as the
elemental representation of stimuli, teaching signals, and other
variables specifying the model's behaviour (e.g. freezing learning).
</p>
<p>Argument <code>st</code> must be a list containing the following items:
</p>
<p><code>beta</code> - the learning rate (fixed for a given simulation) for the
LMS learning rule. The upper bound of this parameter is not
specified, but we suggest <code class="reqn">0 &lt; beta \le 1</code>.
</p>
<p><code>theta</code> - is a positive scaling constant. When theta rises, the
logistic choice function will become less linear. When theta is
high, the logistic function will approximate the behaviour of a step
function.
</p>
<p><code>bias</code> - is a bias parameter. It is the value of the output
activation that results in an output probability rating of P =
0.5. For example, if you wish an output activation of 0.4 to produce a
rated probability of 0.5, set beta to 0.4. If you are not sure what to
use here, set it to 0. The bias parameter is not part of the original
Gluck and Bower (1988) LMS network, see Note 1.
</p>
<p><code>w</code> - is a matrix of initial connection weights, where each row is
an outcome, and each column is a feature or cue. If you are not sure
what to use here, set all values to 0.
</p>
<p><code>outcomes</code> - is the number of possible categories or outcomes.
</p>
<p><code>colskip</code> - the number of optional columns to be skipped in the tr
matrix.  colskip should be set to the number of optional columns
PLUS ONE. So, if you have added no extra columns, colskip = 1.
</p>
<p>Argument <code>tr</code> must be a matrix, where each row is one trial
presented to the model. Trials are always presented in the order
specified. The columns must be as described below, in the order
described below:
</p>
<p><code>ctrl</code> - a vector of control codes. Available codes are: 0 = normal
trial; 1 = reset model (i.e. set associative strengths (weights)
back to their initial values as specified in w (see above)); 2 =
Freeze learning. Control codes are actioned before the trial is
processed.
</p>
<p><code>opt1, opt2, ...</code> - any number of preferred optional columns, the
names of which can be chosen by the user. It is important that these
columns are placed after the control column, and before the
remaining columns (see below). These optional columns are ignored by
the slpLMSnet function, but you may wish to use them for
readability. For example, you might choose to include columns such
as block number, trial number and condition. The argument colskip
(see above) must be set to the number of optional columns plus one.
</p>
<p><code>x1, x2, ...</code> - activation of input nodes of corresponding features.
Feature patterns usually represented as a bit array. Each element in the
bit array encodes the activations of the input nodes given the presence or
absence of the corresponding features. These activations can take on either
1 or 0, present and absent features respectively. For example, Medin and 
Edelson's (1988) inverse base-rate effect with stimuli AB and AC can be 
represented as [1 1 0] and [1 0 1] respectively. In a more unconventional
scenario, you can set activation to vary between present 1 and absent -1,
see Note 2. slpLMSnet can also support any positive or negative real number
for activations, e.g. one might use values between 0 and 1 to represent the 
salience of the features.
</p>
<p><code>d1, d2, ...</code> - teaching input signals indicating the category feedback
on the current trial. It is a bit array, similar to the activations of
input nodes. If there are two categories and the stimuli on the current
trial belongs to the first, then this would be represented in <code>tr</code> as
[1 0], on edge cases see Note 3. The length of this array must be provided
via <code>outcomes</code> in <code>st</code>.
</p>


<h3>Value</h3>

<p>Returns a list with the following items if <code>xtdo = FALSE</code>:
</p>
<table role = "presentation">
<tr><td><code>p</code></td>
<td>
<p>A matrix with either the probability rating for each
outcome on each trial if <code>dec = "logistic"</code>,
or response probabilities for each outcome on each trial if
<code>dec = "softmax"</code>.</p>
</td></tr>
<tr><td><code>nodeActivation</code></td>
<td>
<p>Output node activations on each trial, as
output by Equation 3 in Gluck and Bower (1988).</p>
</td></tr>
<tr><td><code>connectionWeightMatrix</code></td>
<td>
<p>A connection weight matrix, W, where
each row represents the corresponding element in the teaching
signals array in <code>tr</code>, while each column represents the
corresponding element from the input activation array from
<code>tr</code>. So cell <code class="reqn">w_12</code> would be the connection weight
between the second stimulus and the first category.</p>
</td></tr>
</table>
<p>If <code>xtdo = TRUE</code>, the following item also returned:
</p>
<table role = "presentation">
<tr><td><code>squaredDifferences</code></td>
<td>
<p>The least mean squeared differences between
desired and actual activations of output nodes on each trial
(Eq. 4 in Gluck and Bower, 1988). This metric is an indicator of
the network's performance, which is measured by its accuracy.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>1. The <code>bias</code> parameter is not part of the original Gluck and
Bower (1988) model. <code>bias</code> in the current implementation helps
comparisons between simulations using the <code>act2probrat</code>
logistic choice function. Set bias to 0 for operation as specified
in Gluck &amp; Bower (1988). Also note that, where there is more than
one output node, the same bias value is subtracted from the output
of each node. This form of decision mechanism is not present in the
literature as far as we are aware, although using a negative bias
value would, in multi-outcome cases, approximate a 'background
noise' decision rule, as used in, for example, Nosofsky et
al. (1994).
</p>
<p>2. slpLMSnet can support both positive and negative real numbers as
input node activations. For example, one might wish to follow
Markman's (1989) suggestion that the absence of a feature element is
encoded as -1 instead of 0.
</p>
<p>3. slpLMSnet can process a bit array of teaching signals, where the
model is told that the stimulus belongs to more than one
category. slpLMSnet uses matrix operations to update weights, so it
can encode and update multiple teaching signals on the same trial.
</p>


<h3>Author(s)</h3>

<p>Lenard Dome, Andy Wills 
</p>


<h3>References</h3>

<p>Gluck, M. A., &amp; Bower, G. H. (1988). From conditioning to category 
learning: An adaptive network model. <em>Journal of Experimental
Psychology: General, 117</em>, 227-247.
</p>
<p>Markman, A. B. (1989). LMS rules and the inverse base-rate effect:
Comment on Gluck and Bower (1988). <em>Journal of Experimental
Psychology: General, 118</em>, 417-421.
</p>
<p>Medin, D. L., &amp; Edelson, S. M. (1988). Problem structure and the use of 
base-rate information from experience. <em>Journal of Experimental
Psychology: General, 117</em>, 68-85.
</p>
<p>Nosofsky, R.M., Gluck, M.A., Plameri, T.J., McKinley, S.C. and
Glauthier, P.  (1994). Comparing models of rule-based classification
learning: A replication and extension of Shepard, Hovland, and Jenkins
(1961). <em>Memory and Cognition, 22</em>, 352-369.
</p>
<p>Wills, A.J., O'Connell, G., Edmunds, C.E.R., &amp; Inkster,
A.B.(2017). Progress in modeling through distributed collaboration:
Concepts, tools, and category-learning examples. <em>Psychology of
Learning and Motivation, 66</em>, 79-115.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## load catlearn
library(catlearn)

## create st with initial state
st &lt;- list(beta = 0.025, # learning rate
           theta = 1, # decision scaling parameter
           bias = 0, # decision bias parameter
           # initial weight matrix, 
           # row = number of categories,
           # col = number of cues
           w = matrix(rep(0, 6*4), nrow = 4, ncol = 6),
           outcomes = 4, # number of possible outcomes
           colskip = 3)

## create inverse base-rate effect tr for 1 subject and without bias cue
tr &lt;- krus96train(subjs = 1, ctxt = FALSE)

# run simulation and store output
out &lt;- slpLMSnet(st, data.matrix(tr))

out$connectionWeightMatrix
</code></pre>

<hr>
<h2 id='slpMack75'>
Mackintosh (1975) associative learning model
</h2><span id='topic+slpMack75'></span>

<h3>Description</h3>

<p>Mackintosh's (1975) attentional learning model, as implemented by Le
Pelley et al. (2016).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
slpMack75(st, tr, xtdo = FALSE)

</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="slpMack75_+3A_st">st</code></td>
<td>
<p>List of model parameters</p>
</td></tr>
<tr><td><code id="slpMack75_+3A_tr">tr</code></td>
<td>
<p>Matrix of training items</p>
</td></tr>
<tr><td><code id="slpMack75_+3A_xtdo">xtdo</code></td>
<td>
<p>Boolean specifying whether to include extended 
information in the output (see below)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function operates as a stateful list processor (slp; see Wills et
al., 2017). Specifically, it takes a matrix (tr) as an argument, where
each row represents a single training trial, while each column
represents the different types of information required by the model,
such as the elemental representation of the training stimuli, and the
presence or absence of an outcome. It returns the output activation on
each trial (a.k.a. sum of associative strengths of cues present on that
trial), as a vector. The slpMack75 function also returns the final state of
the model - a vector of associative and attentional strengths between
each stimulus and the outcome representation.
</p>
<p>Argument <code>st</code> must be a list containing the following items:
</p>
<p><code>lr</code> - the associative learning rate (fixed for a given
simulation), as denoted by <code>theta</code> in Equation 1 of Mackintosh
(1975).
</p>
<p><code>alr</code> - the attentional learning rate parameter. It can be set without
limit (see alpha below), but we recommend setting this parameter to somewhere
between 0.1 and 1.
</p>
<p><code>w</code> - a vector of initial associative strengths. If you are not
sure what to use here, set all values to zero.
</p>
<p><code>alpha</code> - a vector of initial attentional strengths. If the 
updated value is above 1 or below 0.1, it is capped to 1 and 0.1
respectively.
</p>
<p><code>colskip</code> - the number of optional columns to be skipped in the tr
matrix. colskip should be set to the number of optional columns you have
added to the tr matrix, PLUS ONE. So, if you have added no optional
columns, colskip=1. This is because the first (non-optional) column
contains the control values (details below).
</p>
<p>Argument <code>tr</code> must be a matrix, where each row is one trial
presented to the model. Trials are always presented in the order
specified. The columns must be as described below, in the order
described below:
</p>
<p><code>ctrl</code> - a vector of control codes. Available codes are: 
</p>
<p>0 = normal trial 
1 = reset model (i.e. set associative strengths back to
their initial values as specified in w)
2 = Freeze learning
3 = Reset associative weights to initial state, but keep
attentional strengths in alpha
4 = Reset attentional strengths to initial state, but keep 
association weights.
</p>
<p>Control codes are actioned before the trial is processed.
</p>
<p><code>opt1, opt2, ...</code> - any number of preferred optional columns, the
names of which can be chosen by the user. It is important that these
columns are placed after the control column, and before the remaining
columns (see below). These optional columns are ignored by the
function, but you may wish to use them for readability. For example, you
might choose to include columns such as block number, trial number and
condition. The argument colskip (see above) must be set to the number of
optional columns plus one.
</p>
<p><code>x1, x2, ...</code> - activation of any number of input elements. There
must be one column for each input element. Each row is one trial. In
simple applications, one element is used for each stimulus (e.g. a
simulation of blocking (Kamin, 1969), A+, AX+, would have two inputs,
one for A and one for X). In simple applications, all present elements
have an activation of 1 and all absence elements have an activation of
0. However, slpMack75 supports any real number for activations, e.g. one
might use values between 0 and 1 to represent differing cue saliences.
</p>
<p><code>t</code> - Teaching signal (a.k.a. lambda). Traditionally, 1 is used to
represent the presence of the outcome, and 0 is used to represent the
absence of the outcome, although slpMack75 supports any real values for lambda.
If you are planning to use multiple outcomes, see Note 2.
</p>
<p>Argument <code>xtdo</code> (eXTenDed Output) - if set to TRUE, function will
additionally return trial-level data including attentional strengths and 
the updated associative strengths after each trial (see Value).
</p>


<h3>Value</h3>

<p>Returns a list containing three components (if xtdo = FALSE) or five
components (if xtdo = TRUE, xoutw and xouta is also returned):
</p>
<table role = "presentation">
<tr><td><code>suma</code></td>
<td>
<p>Vector of summed associative strength for each trial.</p>
</td></tr>
<tr><td><code>w</code></td>
<td>
<p>Vector of final associative strengths.</p>
</td></tr>
<tr><td><code>alpha</code></td>
<td>
<p>Vector of final attentional weights.</p>
</td></tr>
<tr><td><code>xoutw</code></td>
<td>
<p>Matrix of trial-level data of the associative strengths at
the end of the trial, after each has been updated.</p>
</td></tr>
<tr><td><code>xouta</code></td>
<td>
<p>Matrix of trial-level data of the attentional strengths at
the end of the trial, after each has been updated.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>1. Mackintosh (1975) did not formalise how to update the cues' associability,
but described when associability increases or decreases in Equation 4 and 5. 
He assumed that the change in alpha would reflect the difference between
the prediction error generated by the current cue and the combined influence
(a sum) of all other cues. Le Pelley et al. (2016) provided a linear
function in Equation 2 that adheres to this description. This expression
is probably the simplest way to express Mackintosh's somewhat
vague description in mathematical terms. A linear function is also 
easier to computationally implement. So we decided to use Equation 2 from 
Le Pelley et al. (2016) for updating attentional strengths.
</p>
<p>2. At present, only single-outcome experiments are officially
supported. If you want to simulate a two-outcome study, consider using
+1 for one outcome, and -1 for the other outcome. Alternatively, run a
separate simulation for each outcome.
</p>


<h3>Author(s)</h3>

<p>Lenard Dome, Andy Wills, Tom Beesley
</p>


<h3>References</h3>

<p>Kamin,  L.J. (1969). Predictability,  surprise,  attention  and  conditioning. 
In Campbell, B.A. &amp; Church, R.M. (eds.), <em>Punishment and  Aversive  
Behaviour</em>. New York: Appleton-Century-Crofts, 1969,  pp.279-296.
</p>
<p>Le Pelley, M. E., Mitchell, C. J., Beesley, T., George, D. N., &amp; Wills, A. J. 
(2016). Attention and associative learning in humans: 
An integrative review. <em>Psychological Bulletin</em>, 142(10), 
1111–1140. https://doi.org/10.1037/bul0000064
</p>
<p>Mackintosh, N.J. (1975). A theory of attention: Variations in the
associability of stimuli with reinforcement, <em>Psychological Review,
82</em>, 276-298.
</p>
<p>Wills, A.J., O'Connell, G., Edmunds, C.E.R., &amp; Inkster,
A.B.(2017). Progress in modeling through distributed collaboration:
Concepts, tools, and category-learning examples. <em>Psychology of
Learning and Motivation, 66</em>, 79-115.
</p>

<hr>
<h2 id='slpMBMF'>
MB/MF reinforcement learning model
</h2><span id='topic+slpMBMF'></span>

<h3>Description</h3>

<p>Gillan et al.'s (2015) model-free / model-based hybrid Reinforcement
Learning model (see Note 1).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>slpMBMF(st, tr, xtdo = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="slpMBMF_+3A_st">st</code></td>
<td>
<p>List of model parameters</p>
</td></tr>  
<tr><td><code id="slpMBMF_+3A_tr">tr</code></td>
<td>
<p>Matrix of training items</p>
</td></tr>
<tr><td><code id="slpMBMF_+3A_xtdo">xtdo</code></td>
<td>
<p>Boolean. When TRUE, extended output is provided, see below</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The contents of this help file are relatively brief; a more extensive
discussion of this model can be found in the supplementary materials
of Gillan et al. (2015).
</p>
<p>The function operates as a stateful list processor (slp; see Wills et
al., 2017). Specifically, it takes a matrix (tr) as an argument, where
each row represents a single training trial, while each column
represents the different types of information required by the
model. It returns a matrix of predicted response probabilities for
each stage 1 action on each trial. The slpMBMF function also returns
the final Q values for the model.
</p>
<p>The current implementation of slpMBMF deals only with relatively
simple Reinforcement Learning experiments, of which Gillan et
al. (2015, Exp. 2) is one example. Specifically, each trial has two
stages. In the first stage of the trial, there is a single state, and
the participant can emit one of <code>x</code> actions. In the second stage,
there are <code>y</code> states. A reward follows (or doesn't) without a
further action from the participant.
</p>
<p>A hybrid MB/MF model thus has <code>2x</code> Q-values at stage 1 (<code>x</code>
for the model-based system, and <code>x</code> for the model-free system),
and <code>y</code> Q-values at stage 2 (one for each state; there are no
actions at stage 2, and the MB and MF systems evaluate stage 2
Q-values the same way in this model). See Note 3.
</p>
<p>Argument <code>st</code> must be a list containing the following items:
</p>
<p><code>alpha</code> - the model-free learning rate (range: 0-1)
</p>
<p><code>lambda</code> - the eligibility trace parameter (range: 0-1)
</p>
<p><code>w</code> - A number between 0 and 1, representing the relative
contribution of the model-based and model-free parts of the model to
the response (0 = pure model-free, 1 = pure model-based).
</p>
<p><code>beta</code> - Decision stochasticity parameter
</p>
<p><code>p</code> - Decision perseveration (p &gt; 0) or switching (p &lt; 0)
parameter
</p>
<p><code>tprob</code> - A 2 x 2 matrix of transition probabilities, used by the
model-based system. The rows are the actions at stage 1. The columns
are the states at stage 2. The cells are transition probabilities
(e.g. tprob[2,1] is the probability of arriving at stage 2 state #1
given action #2 at stage 1). 
</p>
<p><code>q1.mf</code> - A vector of initial model-free Q values for the actions
at stage 1.
</p>
<p><code>q1.mb</code> - A vector of initial model-based Q values for the
actions at stage 1.
</p>
<p><code>q2</code> - A vector of initial Q values for the states at stage 2
(the MB and MF systems share common Q values at stage 2).
</p>
<p>If you are unsure what initial Q values to use, set all to 0.5.
</p>
<p>Argument <code>tr</code> must be a matrix, where each row is one
trial. Trials are always presented to the model in the order
specified. The matrix must contain the following named columns (other
columns will be ignored):
</p>
<p><code>s1.act</code> - The action made by the participant at stage 1, for
each trial; must be an integer in the range 1-<code>x</code>.
</p>
<p><code>s2.state</code> - State of environment at stage 2, for each trial;
must be an integer in the range 1-<code>y</code>.
</p>
<p><code>t</code> - Reward signal for trial; must be a real number. If you're
unsure what to use here, use 1 = rewarded, 0 = not rewarded.
</p>


<h3>Value</h3>

<p>When xtdo = FALSE, returns a list containing these components:
</p>
<p><code>out</code> - Matrix of response probabilities, for each stage 1 action
on each trial.
</p>
<p><code>q1.mf</code> - A vector of final model-free Q values for the actions at
stage 1.
</p>
<p><code>q1.mb</code> - A vector of final model-based Q values for the
actions at stage 1
</p>
<p><code>q2</code> - A vector of final Q values for the states at stage 2
(the MB and MF systems share common Q values at stage 2).
</p>
<p>When xtdo = TRUE, the list also contains the following model-state
information :
</p>
<p><code>xout</code> - A matrix containing the state of the model at the end of
each trial. Each row is one trial. It has the following columns:
</p>
<p><code>q1.mb.1, q1.mb.2, ...</code> - One column for each model-based Q
value at stage 1.
</p>
<p><code>q1.mf.1, q1.mf.2, ...</code> - One column for each model-free Q
value at stage 1.
</p>
<p><code>q2.1, q2.2, ...</code> - One column for each Q value at stage 2.
</p>
<p><code>q1.h.1, q1.h.2, ...</code> - One column for each hybrid Q value at
stage 1.
</p>
<p><code>s1.d.mf</code> - Model-free delta at stage 2, wrt. stage 1 action.
</p>
<p><code>s2.d.mf</code> - Model-free delta at outcome. 
</p>
<p>In addition, when xtdo = TRUE, the list also contains the following
information that is not used by the model (but which might be handy as
potential neural regressors).
</p>
<p><code>s1.d.mb</code> - Model-based delta at stage 2, wrt. stage 1
action.
</p>
<p><code>s1.d.h</code> - Hybrid delta (based on stage 1 hybrid Q values) at
stage 2, wrt. stage 1 action.
</p>
<p><code>s1.d.diff</code> - s1.d.mf - s1.d.mb
</p>


<h3>Note</h3>

 
<p>1. Gillan et al.'s (2015) choice rule, at least as stated in their
supplementary materials, would lead to the response probabilities
being infinite on switch trials, which is presumably an error. The
current implementation uses Daw et al. (2011, suppl. mat., Eq. 2).
</p>
<p>2. Gillan et al. (2015) decay Q values for unselected actions by
(1-alpha). This is not part of the current implementation.
</p>
<p>3. In the current implementation of the model, <code>x</code> must be 2 and
<code>y</code> must be two, otherwise the model will fail or behave
unpredictably. If you'd like to develop a more general version of this
implementation, contact the author. 
</p>


<h3>Author(s)</h3>

<p>Andy Wills ( andy@willslab.co.uk ), Tom Sambrook
</p>


<h3>References</h3>

<p>Daw, N.D., Gershman, S.J., Seymour, B., Dayan, P., &amp; Dolan, R.J.
(2011). Model-based influences on humans' choices and striatal
prediction errors. <em>Neuron, 69</em>, 1204-1215.
</p>
<p>Gillan, C.M., Otto, A.R., Phelps, E.A. &amp; Daw, N.D. (2015). Model-based
learning protects against forming
habits. <em>Cogn. Affect. Behav. Neurosci., 15</em>, 523-536.
</p>
<p>Wills, A.J., O'Connell, G., Edmunds, C.E.R., &amp; Inkster,
A.B.(2017). Progress in modeling thrXSough distributed collaboration:
Concepts, tools, and category-learning examples. <em>Psychology of
Learning and Motivation, 66</em>, 79-115.
</p>

<hr>
<h2 id='slpNNCAG'>
A Neural Network with Competitive Attentional Gating (NNCAG)
</h2><span id='topic+slpNNCAG'></span>

<h3>Description</h3>

<p>This is Model 4 from Paskewitz and Jones (2020).  Model 4 is a Neural
Network with Competitive Attentional Gating - a fragmented version of
EXIT (Kruschke, 2001) lacking exemplar-based rapid attentional shifts.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
slpNNCAG(st, tr, xtdo = FALSE)

</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="slpNNCAG_+3A_st">st</code></td>
<td>
<p> List of model parameters</p>
</td></tr>
<tr><td><code id="slpNNCAG_+3A_tr">tr</code></td>
<td>
<p> R matrix of training items</p>
</td></tr>
<tr><td><code id="slpNNCAG_+3A_xtdo">xtdo</code></td>
<td>
<p> Boolean specifying whether to include extended
information in the output (see below).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function operates as a stateful list processor (slp; see Wills et
al., 2017). Specifically, it takes a matrix (tr) as an argument, where
each row represents a single training trial, while each column
represents the different types of information required by the model,
such as the elemental representation of the training stimuli, and the
presence or absence of an outcome.
</p>
<p>Argument <code>st</code> must be a list containing the following items:
</p>
<p><code>P</code> - attention normalization constant, <code class="reqn">P</code>.
</p>
<p><code>phi</code> - decision-making constant, <code class="reqn">\phi</code>, also referred to as
specificity constant.
</p>
<p><code>lambda</code> - learning rate, <code class="reqn">\lambda</code>.
</p>
<p><code>mu</code> - attentional learning rate, <code class="reqn">\mu</code>.
</p>
<p><code>outcomes</code> - The number of categories.
</p>
<p><code>w</code> - a <code class="reqn">k \times i</code> matrix of initial weights, where
<code class="reqn">k</code> equals to the number of categories and <code class="reqn">i</code> equals to the
number of stimuli.
</p>
<p><code>eta</code> - <code class="reqn">\eta</code>, a vector with <code class="reqn">i</code> elements, where
<code class="reqn">\eta^{th}</code> is the salience of the <code class="reqn">i^{th}</code>
cue. In edge cases, <code class="reqn">\eta</code> is capped at lower bound of 0.1, see Note
1.
</p>
<p><code>colskip</code> - The number of optional columns to be skipped in the tr
matrix. colskip should be set to the number of optional columns you have
added to the tr matrix, PLUS ONE. So, if you have added no optional
columns, colskip=1. This is because the first (non-optional) column
contains the control values (details below).
</p>
<p>Argument <code>tr</code>must be a matrix, where each row is one trial
presented to the model. Trials are always presented in the order
specified. The columns must be as described below, in the order
described below:
</p>
<p><code>ctrl</code> - a vector of control codes. Available codes are: 0 = normal
trial; 1 = reset model (i.e. set matrix of initial weights and vector of
salience back to their initial values as specified in <code>st</code>); 2 =
Freeze learning. Control codes are actioned before the trial is
processed.
</p>
<p><code>opt1, opt2, ...</code> - any number of preferred optional columns, the
names of which can be chosen by the user. It is important that these
columns are placed after the control column, and before the remaining
columns (see below). These optional columns are ignored by the function,
but you may wish to use them for readability. For example, you might
choose to include columns such as block number, trial number and
condition. The argument colskip (see above) must be set to the number of
optional columns plus one.
</p>
<p><code>x1, x2, ...</code> - columns for each cue (<code>1</code> = cue present,
<code>0</code> = cue absent). There must be one column for each input
element. Each row is one trial. In simple applications, one element is
used for each stimulus (e.g. a simulation of blocking (Kamin, 1969), A+,
AX+, would have two inputs, one for A and one for X). In simple
applications, all present elements have an activation of 1 and all
absent elements have an activation of 0. However, slpNNCAG supports any
real number for activations.
</p>
<p><code>t1, t2, ...</code> - columns for the teaching values indicating the
category feedback on the current trial. Each category needs a single
teaching signal in a dummy coded fashion, e.g., if there are four
categories and the current stimulus belongs to the second category, then
we would have <code>[0, 1, 0, 0]</code>.  </p>


<h3>Value</h3>

<p>Returns a list containing three components (if xtdo = FALSE) or four
components (if xtdo = TRUE).
</p>
<p>if <code>xtdo = FALSE</code>:
</p>
<table role = "presentation">
<tr><td><code>p</code></td>
<td>
<p>Response probabilities for each trial (rows) and each
category (columns).</p>
</td></tr>
<tr><td><code>final_eta</code></td>
<td>
<p>Salience at the end of training. <code class="reqn">\eta</code> for each
stimulus <code class="reqn">i</code>.</p>
</td></tr>
<tr><td><code>final_weights</code></td>
<td>
<p>An <code class="reqn">k \times i</code> weight matrix at the end of
training, where rows are categories and columns are stimuli. Order of
stimuli and categories correspond to their order in <code>tr</code>.</p>
</td></tr>
</table>
<p>if <code>xtdo = TRUE</code>, the following values are also returned:
</p>
<table role = "presentation">
<tr><td><code>model_predictions</code></td>
<td>
<p>The matrix for trial-level predictions of the
model as specified by Equation 5 in Paskewitz and Jones (2021).</p>
</td></tr>
<tr><td><code>eta</code></td>
<td>
<p>The updated salience at the end of each trial.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>1. If there is only one stimulus present on a given trial with
<code class="reqn">\eta</code> = <code>0</code> or with <code class="reqn">g</code> = <code>0</code>, Equation 12 of
Paskewitz &amp; Jones (2020) breaks down. In order to avoid this, <code class="reqn">eta</code>
and <code class="reqn">g</code> are capped at the lower limit of <code>0.01</code>.
</p>
<p>2. This model is implemented in C++ for speed.
</p>


<h3>Author(s)</h3>

<p>Lenard Dome, Andy Wills
</p>


<h3>References</h3>

<p>Kamin, L.J. (1969). Predictability, surprise, attention and
conditioning.  In Campbell, B.A. &amp; Church, R.M. (eds.), <em>Punishment
and Aversive Behaviour</em>. New York: Appleton-Century-Crofts, 1969,
pp.279-296.
</p>
<p>Kruschke, J. K. (2001). Toward a unified model of attention in
associative learning. <em>Journal of Mathematical Psychology, 45(6)</em>,
812-863.
</p>
<p>Paskewitz, S., &amp; Jones, M. (2020). Dissecting EXIT. <em>Journal of
Mathematical Psychology, 97</em>, 102371.
</p>
<p>Wills, A.J., O'Connell, G., Edmunds, C.E.R., &amp; Inkster,
A.B.(2017). Progress in modeling through distributed collaboration:
Concepts, tools, and category-learning examples. <em>Psychology of
Learning and Motivation, 66</em>, 79-115.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+slpEXIT">slpEXIT</a></code>
</p>

<hr>
<h2 id='slpNNRAS'>
A Neural Network with Rapid Attentional Shifts (NNRAS)
</h2><span id='topic+slpNNRAS'></span>

<h3>Description</h3>

<p>This is Model 5 from Paskewitz and Jones (2020).  Model 5 is a Neural
Network with Rapid Attentional Shifts the also contains an competitive
attentional gating mechanism. It is a fragmented version of EXIT
(Kruschke, 2001) lacking exemplar-mediated attention.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
slpNNRAS(st, tr, xtdo = FALSE)

</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="slpNNRAS_+3A_st">st</code></td>
<td>
<p> List of model parameters</p>
</td></tr>
<tr><td><code id="slpNNRAS_+3A_tr">tr</code></td>
<td>
<p> R matrix of training items</p>
</td></tr>
<tr><td><code id="slpNNRAS_+3A_xtdo">xtdo</code></td>
<td>
<p> Boolean specifying whether to include extended
information in the output (see below).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function operates as a stateful list processor (slp; see Wills et
al., 2017). Specifically, it takes a matrix (tr) as an argument, where
each row represents a single training trial, while each column
represents the different types of information required by the model,
such as the elemental representation of the training stimuli, and the
presence or absence of an outcome.
</p>
<p>Argument <code>st</code> must be a list containing the following items:
</p>
<p><code>P</code> - attention normalization constant, <code class="reqn">P</code>.
</p>
<p><code>phi</code> - decision-making constant, <code class="reqn">\phi</code>, also referred to as
specificity constant.
</p>
<p><code>lambda</code> - learning rate, <code class="reqn">\lambda</code>.
</p>
<p><code>mu</code> - attentional learning rate, <code class="reqn">\mu</code>.
</p>
<p><code>rho</code> - attentional shift rate, <code class="reqn">\rho</code>. Attention shifts ten
times per trial.
</p>
<p><code>outcomes</code> - The number of categories.
</p>
<p><code>w</code> - a <code class="reqn">k \times i</code> matrix of initial weights, where
<code class="reqn">k</code> equals to the number of categories and <code class="reqn">i</code> equals to the
number of stimuli.
</p>
<p><code>eta</code> - <code class="reqn">\eta</code>, a vector with <code class="reqn">i</code> elements, where
<code class="reqn">\eta^{th}</code> is the salience of the <code class="reqn">i^{th}</code>
cue. In edge cases, <code class="reqn">\eta</code> is capped at lower bound of 0.1, see Note
1.
</p>
<p><code>colskip</code> - The number of optional columns to be skipped in the tr
matrix. colskip should be set to the number of optional columns you have
added to the tr matrix, PLUS ONE. So, if you have added no optional
columns, colskip=1. This is because the first (non-optional) column
contains the control values (details below).
</p>
<p>Argument <code>tr</code>must be a matrix, where each row is one trial
presented to the model. Trials are always presented in the order
specified. The columns must be as described below, in the order
described below:
</p>
<p><code>ctrl</code> - a vector of control codes. Available codes are: 0 = normal
trial; 1 = reset model (i.e. set matrix of initial weights and vector of
salience back to their initial values as specified in <code>st</code>); 2 =
Freeze learning. Control codes are actioned before the trial is
processed.
</p>
<p><code>opt1, opt2, ...</code> - any number of preferred optional columns, the
names of which can be chosen by the user. It is important that these
columns are placed after the control column, and before the remaining
columns (see below). These optional columns are ignored by the function,
but you may wish to use them for readability. For example, you might
choose to include columns such as block number, trial number and
condition. The argument colskip (see above) must be set to the number of
optional columns plus one.
</p>
<p><code>x1, x2, ...</code> - columns for each cue (<code>1</code> = cue present,
<code>0</code> = cue absent). There must be one column for each input
element. Each row is one trial. In simple applications, one element is
used for each stimulus (e.g. a simulation of blocking (Kamin, 1969), A+,
AX+, would have two inputs, one for A and one for X). In simple
applications, all present elements have an activation of 1 and all
absence elements have an activation of 0. However, slpNNRAS supports any
real number for activations.
</p>
<p><code>t1, t2, ...</code> - columns for the teaching values indicating the
category feedback on the current trial. Each category needs a single
teaching signal in a dummy coded fashion, e.g., if there are four
categories and the current stimulus belongs to the second category, then
we would have <code>[0, 1, 0, 0]</code>.  </p>


<h3>Value</h3>

<p>Returns a list containing three components (if xtdo = FALSE) or four
components (if xtdo = TRUE).
</p>
<p>if <code>xtdo = FALSE</code>:
</p>
<table role = "presentation">
<tr><td><code>p</code></td>
<td>
<p>Response probabilities for each trial (rows) and each
category (columns).</p>
</td></tr>
<tr><td><code>final_eta</code></td>
<td>
<p>Salience at the end of training. <code class="reqn">\eta</code> for each
stimulus <code class="reqn">i</code>.</p>
</td></tr>
<tr><td><code>final_weights</code></td>
<td>
<p>An <code class="reqn">k \times i</code> weight matrix at the
end of training, where rows are categories and columns are
stimuli. Order of stimuli and categories correspond to their order in
<code>tr</code>.</p>
</td></tr>
</table>
<p>if <code>xtdo = TRUE</code>, the following values are also returned:
</p>
<table role = "presentation">
<tr><td><code>model_predictions</code></td>
<td>
<p>The matrix for trial-leve predictions of the
model as specified by Equation 5 in Paskewitz and Jones (2020).</p>
</td></tr>
<tr><td><code>eta</code></td>
<td>
<p>The updated salience at the end of each trial.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>1. If there is only one stimulus present on a given trial with
<code class="reqn">\eta</code> = <code>0</code> or with <code class="reqn">g</code> = <code>0</code>, Equation 12 breaks
down. In order to avoid this, <code class="reqn">eta</code> and <code class="reqn">g</code> is capped at the
lower limit of <code>0.01</code>.
</p>
<p>2. This model is implemented in C++ for speed.
</p>


<h3>Author(s)</h3>

<p>Lenard Dome, Andy Wills
</p>


<h3>References</h3>

<p>Kamin, L.J. (1969). Predictability, surprise, attention and
conditioning.  In Campbell, B.A. &amp; Church, R.M. (eds.), <em>Punishment
and Aversive Behaviour</em>. New York: Appleton-Century-Crofts, 1969,
pp.279-296.
</p>
<p>Kruschke, J. K. (2001). Toward a unified model of attention in
associative learning. <em>Journal of Mathematical Psychology, 45(6)</em>,
812-863.
</p>
<p>Paskewitz, S., &amp; Jones, M. (2020). Dissecting EXIT. <em>Journal of
Mathematical Psychology, 97</em>, 102371.
</p>
<p>Wills, A.J., O'Connell, G., Edmunds, C.E.R., &amp; Inkster,
A.B.(2017). Progress in modeling through distributed collaboration:
Concepts, tools, and category-learning examples. <em>Psychology of
Learning and Motivation, 66</em>, 79-115.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+slpEXIT">slpEXIT</a></code>
</p>

<hr>
<h2 id='slpRW'>
Rescorla-Wagner (1972) associative learning model.
</h2><span id='topic+slpRW'></span>

<h3>Description</h3>

<p>Rescorla &amp; Wagner's (1972) theory of Pavlovian conditioning. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
slpRW(st, tr, xtdo = FALSE)

</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="slpRW_+3A_st">st</code></td>
<td>
<p>List of model parameters</p>
</td></tr>
<tr><td><code id="slpRW_+3A_tr">tr</code></td>
<td>
<p>Matrix of training items</p>
</td></tr>
<tr><td><code id="slpRW_+3A_xtdo">xtdo</code></td>
<td>
<p>Boolean specifying whether to include extended information
in the output (see below)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The contents of this help file are relatively brief; a more extensive
tutorial on using slpRW can be found in Spicer et al. (n.d.).
</p>
<p>The function operates as a stateful list processor (slp; see Wills et
al., 2017). Specifically, it takes a matrix (tr) as an argument, where
each row represents a single training trial, while each column
represents the different types of information required by the model,
such as the elemental representation of the training stimuli, and the
presence/absence of an outcome. It returns the output activation on each
trial (a.k.a. sum of associative strengths of cues present on that
trial), as a vector. The slpRW function also returns the final state of
the model - a vector of associative strengths between each stimulus and
the outcome representation.
</p>
<p>Argument <code>st</code> must be a list containing the following items:
</p>
<p><code>lr</code> - the learning rate (fixed for a given simulation). In order to
calculate lr, calculate the product of Rescorla-Wagner parameters alpha
and beta. For example, if you want alpha = 0.1 and beta = 0.2, set lr =
0.02. If you want different elements to differ in salience (different
alpha values) use the input activations (x1, x2, ..., see below) to
represent element-specific salience. For example, if alpha_A = 0.4,
alpha_X = 0.2, and beta = 0.1, then set lr = 0.1, and the activations of
A and B to 0.4 and 0.2, respectively.
</p>
<p><code>w</code> - a vector of initial associative strengths. If you are not
sure what to use here, set all values to zero.
</p>
<p><code>colskip</code> - the number of optional columns to be skipped in the tr
matrix. colskip should be set to the number of optional columns you have
added to the tr matrix, PLUS ONE. So, if you have added no optional
columns, colskip=1. This is because the first (non-optional) column
contains the control values (details below).
</p>
<p>Argument <code>tr</code> must be a matrix, where each row is one trial
presented to the model. Trials are always presented in the order
specified. The columns must be as described below, in the order
described below:
</p>
<p><code>ctrl</code> - a vector of control codes. Available codes are: 0 = normal
trial; 1 = reset model (i.e. set associative strengths (weights) back to
their initial values as specified in w (see above)); 2 = Freeze
learning. Control codes are actioned before the trial is processed.
</p>
<p><code>opt1, opt2, ...</code> - any number of preferred optional columns, the
names of which can be chosen by the user. It is important that these
columns are placed after the control column, and before the remaining
columns (see below). These optional columns are ignored by the slpRW
function, but you may wish to use them for readability. For example, you
might choose to include columns such as block number, trial number and
condition. The argument colskip (see above) must be set to the number of
optional columns plus one.
</p>
<p><code>x1, x2, ...</code> - activation of any number of input elements. There
must be one column for each input element. Each row is one trial. In
simple applications, one element is used for each stimulus (e.g. a
simulation of blocking (Kamin, 1969), A+, AX+, would have two inputs, one for A and
one for X). In simple applications, all present elements have an
activation of 1 and all absence elements have an activation of
0. However, slpRW supports any real number for activations, e.g. one
might use values between 0 and 1 to represent differing cue saliences.
</p>
<p><code>t</code> - Teaching signal (a.k.a. lambda). Traditionally, 1 is used to
represent the presence of the outcome, and 0 is used to represent the
absence of the outcome, although slpRW suports any real values for lambda.
</p>
<p>Argument <code>xtdo</code> (eXTenDed Output) - if set to TRUE, function will
return associative strength for the end of each trial (see Value).
</p>


<h3>Value</h3>

<p>Returns a list containing two components (if xtdo = FALSE) or three
components (if xtdo = TRUE, xout is also returned):
</p>
<table role = "presentation">
<tr><td><code>suma</code></td>
<td>
<p>Vector of output activations for each trial</p>
</td></tr>
<tr><td><code>st</code></td>
<td>
<p>Vector of final associative strengths</p>
</td></tr>
<tr><td><code>xout</code></td>
<td>
<p>Matrix of associative strengths at the end of each trial</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Stuart Spicer, Lenard Dome, Andy Wills
</p>


<h3>References</h3>

<p>Kamin,  L.J., (1969) Predictability,  surprise,  attention  and  conditioning. 
In Campbell, B.A. &amp; Church, R.M. (eds.), <em>Punishment and  Aversive  
Behaviour</em>. New York: Appleton-Century-Crofts, 1969,  pp.279-296.
</p>
<p>Rescorla, R. A., &amp; Wagner, A. R. (1972). A theory of Pavlovian
conditioning: Variations in the effectiveness of reinforcement and
nonreinforcement. In A. H. Black &amp; W. F. Prokasy (Eds.), <em>Classical
conditioning II: Current research and theory</em> (pp. 64-99). New York:
Appleton-Century-Crofts.
</p>
<p>Spicer, S.G., Jones, P.M., Inkster, A.B., Edmunds, C.E.R. &amp; Wills,
A.J. (n.d.). Progress in learning theory through distributed
collaboration: Concepts, tools, and examples. <em>Manuscript in preparation</em>.
</p>
<p>Wills, A.J., O'Connell, G., Edmunds, C.E.R., &amp; Inkster,
A.B.(2017). Progress in modeling through distributed collaboration:
Concepts, tools, and category-learning examples. <em>Psychology of
Learning and Motivation, 66</em>, 79-115.
</p>

<hr>
<h2 id='slpSUSTAIN'>
SUSTAIN Category Learning Model
</h2><span id='topic+slpSUSTAIN'></span>

<h3>Description</h3>

<p>Supervised and Unsupervised STratified Adaptive Incremental Network
(Love, Medin &amp; Gureckis, 2004)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>slpSUSTAIN(st, tr, xtdo = FALSE, ties = "random")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="slpSUSTAIN_+3A_st">st</code></td>
<td>
<p>List of model parameters</p>
</td></tr>
<tr><td><code id="slpSUSTAIN_+3A_tr">tr</code></td>
<td>
<p>Matrix of training items</p>
</td></tr>
<tr><td><code id="slpSUSTAIN_+3A_xtdo">xtdo</code></td>
<td>
<p>Boolean specifying whether to include extended information
in the output (see below)</p>
</td></tr>
<tr><td><code id="slpSUSTAIN_+3A_ties">ties</code></td>
<td>
<p>Model behaviour where multiple clusters have the same
highest activations. Options are: <code>random</code>, <code>first</code>, see
below.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function works as a stateful list processor (slp; see Wills et
al., 2017). It takes a matrix (tr) as an argument, where each row
represents a single training trial, while each column represents some
information required by the model, such as the stimulus
representation, indications of supervised/unsupervised learning, etc.
(details below).
</p>
<p>Argument <code>st</code> must be a list containing the following items:
</p>
<p><code>r</code> - Attentional focus parameter, always non-negative.
</p>
<p><code>beta</code> - Cluster competition parameter, always non-negative.
</p>
<p><code>d</code> - Decision consistency parameter, always non-negative.
</p>
<p><code>eta</code> - Learning rate parameter, see Note 1.
</p>
<p><code>tau</code> - Threshold parameter for cluster recruitment under
unsupervised learning conditions (Love et al., 2004, Eq. 11). If every
trial is a supervised learning trial, set tau to 0. slpSUSTAIN can
accomodate both supervised and unsupervised learning within the same
simulation, using the <code>ctrl</code> column in <code>tr</code> (see below).
</p>
<p><code>lambda</code> - Vector containing the initial receptive field tuning
value for each stimulus dimension; the order corresponds to the order
of dimensions in <code>tr</code>, below. For a stimulus with three
dimensions, where all receptive fields are equally tuned, lambda = [1,
1, 1].
</p>
<p><code>cluster</code> - A matrix of the initial positions of each recruited
cluster. If set to NA, <code>cluster = NA</code>, then each time the network is
reset, a single cluster will be created, centered on the stimulus
presented on the current trial.
</p>
<p><code>w</code> - A matrix of initial connection weights. If set to NA as
<code>w = NA</code> then, each time the network is reset,
zero-strength weights to a single cluster will be created.
</p>
<p><code>dims</code> - Vector containing the length of each dimension
(excluding category dimension, see <code>tr</code>, below), i.e. the number
of nominal spaces in the representation of each dimension. For Figure
1 of Love et al. (2004), dims = [2, 2, 2].
</p>
<p><code>maxcat</code> - optional. If set, maxcat is an integer specifying the
maximum number of clusters to be recruited during unsupervised
learning. A similar restriction has been used by Love et al. (2004) to
simulate an unsupervised free-sorting task from Experiment 1 in Medin,
Wattenmaker, &amp; Hampson (1987). In this experiment, participants needed
to sort items into two predefined categories. This parameter will only
be used during unsupervised learning. If it is not set, or if it is
set to 0, there is no maximum to the number of clusters that can be
created.
</p>
<p><code>colskip</code> - Number of optional columns skipped in <code>tr</code>,
PLUS ONE. So, if there are no optional columns, set colskip to 1.
</p>
<p>Argument <code>tr</code> must be a matrix, where each row is one trial
presented to the model. Columns are always presented in the order
specified below:
</p>
<p><code>ctrl</code> - A vector of control codes. The control codes are
processed prior to the trial and prior to updating cluster's position,
lambdas and weights (Love et al., 2004, Eq.  12, 13 and 14,
respectively). The available values are:
</p>
<p>0 = do supervised learning.
</p>
<p>1 = reset network and then do supervised learning.
</p>
<p>2 = freeze supervised learning.
</p>
<p>3 = do unsupervised learning.
</p>
<p>4 = reset network and then do unsupervised learning.
</p>
<p>5 = freeze unsupervised learning
</p>
<p>'Reset network' means revert <code>w</code>, <code>cluster</code>,and
<code>lambda</code> back to the values passed in <code>st</code>.
</p>
<p>Unsupervised learning in <code>slpSUSTAIN</code> is at an early stage of
testing, as we have not yet established any CIRP for unsupervised
learning.
</p>
<p><code>opt1, opt2, ...</code> - optional columns, which may have any names
you wish, and you may have as many as you like, but they must be
placed after the ctrl column, and before the remaining columns (see
below). These optional columns are ignored by this function, but you
may wish to use them for readability. For example, you might include
columns for block number, trial number, and stimulus ID number.
</p>
<p><code>x1, x2, y1, y2, y3, ...</code> - Stimulus representation. The
columns represent the kth nominal value for ith dimension. It's a
'padded' way to represent stimulus dimensions and category membership
(as category membership in supervised learning is treated as an
additional dimension) with varying nominal length, see McDonnell &amp;
Gureckis (2011), Fig. 10.2A. All dimensions for the trial are
represented in this single row. For example, if for the presented
stimulus, dimension 1 is [0 1] and dimension 2 is [0 1 0] with
category membership [0 1], then the input representation is [0 1 0 1 0
0 1].
</p>
<p>Argument <code>ties</code> can be either <code>random</code> or <code>first</code>. It specifies
how the model behaves in the event, when there are multiple winning clusters
with the same activations (see Note):
</p>
<p><code>random</code> - The model randomly selects one cluster from the ones
that have the same activations. To increase the reproducibility of
your simulation, set a specific random seed seed before calling
<code>slpSUSTAIN</code> (use e.g.<code>set.seed</code>).
</p>
<p><code>first</code> - The model selects the cluster that was first recruited
from the clusters that have the same activations. Up to and including
version 0.7.1 of <code>catlearn</code>, this was the default behaviour of
<code>slpSUSTAIN</code>.
</p>


<h3>Value</h3>

<p>Returns a list with the following items if <code>xtdo = FALSE</code>:
</p>
<table role = "presentation">
<tr><td><code>probs</code></td>
<td>
<p>Matrix of probabilities of making each response within
the queried dimension (e.g. column 1 = category A; column 2 = category
B), see Love et al. (2004, Eq. 8). Each row is a single trial and
columns are in the order presented in <code>tr</code>, see below. In the
case of unsupervised learning, probabilities are calculated for all
dimensions (as there is no queried dimension for unsupervised
learning).</p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>
<p>Vector of the receptive field tunings for each stimulus
dimension, after the final trial. The order of dimensions corresponds
to the order they are presented in <code>tr</code>, see below.</p>
</td></tr>
<tr><td><code>w</code></td>
<td>
<p>Matrix of connection weights, after the final
trial. Each row is a separate cluster, reported in order of
recruitment (first row is the first cluster to be recruited). The
columns correspond to the columns on the input representation
presented (see <code>tr</code> description, below).</p>
</td></tr>
<tr><td><code>cluster</code></td>
<td>
<p>Matrix of recruited clusters, with their positions in
stimulus space. Each row is a separate cluster, reported in order of
recruitment. The columns correspond to the columns on the input
representation presented (see <code>tr</code> description, below).</p>
</td></tr>
</table>
<p>If <code>xtdo = TRUE</code>, <code>xtdo</code> is returned instead of
<code>probs</code>:
</p>
<table role = "presentation">
<tr><td><code>xtdo</code></td>
<td>
<p>A matrix that includes <code>probs</code>, and in addition includes
the following columns: <code>winning</code> - number of the winning cluster; 
<code>activation</code> - its output activation after cluster competition,
Eq. 6 in Love et al. (2004); <code>recognition_scores</code> - for the current stimulus
from Eq. A6 in Love and Gureckis (2007), this measure represents the model's
overall familiarity with the stimulus; <code>endorsement</code> - the probability of
judging an item as old per Eq. 11 in Love and Gureckis (2007).</p>
</td></tr>
</table>


<h3>Note</h3>

<p>1. Love et al. (2004) do not explicitly set a range for the learning
rate; we recommend a range of 0-1.
</p>
<p>2. The specification of SUSTAIN states that under supervised learning, a
new cluster is recruited each time the model predicts category
membership incorrectly. This new cluster is centered on the current
stimulus. The implementation in slpSUSTAIN adds the stipulation that a
new cluster is NOT recruited if it already exists, i.e. if its location
in stimulus space is identical to the location on an existing
cluster. Instead, it selects the existing cluster and updates as
normal. Love et al. (2004) do not specify model behaviour under such
conditions, so this is an assumption of our implementation. We'd argue
that this is a reasonable implementation - without it SUSTAIN would add
clusters indefinitely under conditions where the stimulus -&gt; category
associations are proabilistic rather than deterministic.
</p>
<p>3. In some cases, two or more clusters can have identical activations
because the presented stimulus is equally similar to multiple
clusters. Love et al. (2004) does not specify how the model will behave
in these cases. In our implementation, we make the assumption that the
model picks randomly between the highest activated clusters (given that
they have the same activations). This, we felt, was in line with the
approximation of lateral inhibition in the SUTAIN specification (Love
et al.  2004, Eq. 6).
</p>


<h3>Author(s)</h3>

<p>Lenard Dome, Andy Wills
</p>


<h3>References</h3>

<p>Love, B. C., &amp; Gureckis, T.M. (2007). Models in Search of a Brain.
<em>Cognitive, Affective, &amp; Behavioral Neuroscience, 7</em>, 90-108.
</p>
<p>Love, B. C., Medin, D. L., &amp; Gureckis, T. M. (2004). SUSTAIN: a
network model of category learning. <em>Psychological Review, 111</em>,
309-332.
</p>
<p>McDonnell, J. V., &amp; Gureckis, T. M. (2011). Adaptive clustering models
of categorization. In E. M. Pothos &amp; A. J. Wills (Eds.), <em>Formal
Approaches in Categorization</em>, pp. 220-252.
</p>
<p>Medin, D. L., Wattenmaker, W. D., &amp; Hampson, S. E. (1987). Family 
resemblance, conceptual cohesiveness, and category construction. 
<em>Cognitive Psychology, 19(2)</em>, 242-279.
</p>
<p>Wills, A.J., O'Connell, G., Edmunds, C.E.R., &amp; Inkster,
A.B.(2017). Progress in modeling through distributed collaboration:
Concepts, tools, and category-learning examples. <em>Psychology of
Learning and Motivation, 66</em>, 79-115.
</p>

<hr>
<h2 id='ssecl'>Sum of squared errors</h2><span id='topic+ssecl'></span>

<h3>Description</h3>

<p>Calculate sum of squared errors
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ssecl(obs,exp)</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ssecl_+3A_obs">obs</code></td>
<td>
<p>Vector of observed values</p>
</td></tr>
<tr><td><code id="ssecl_+3A_exp">exp</code></td>
<td>
<p>Vector of expected values</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns sum of the squared differences.
</p>


<h3>Author(s)</h3>

<p>Andy Wills
</p>

<hr>
<h2 id='stsimGCM'>
Generalized Context Model
</h2><span id='topic+stsimGCM'></span>

<h3>Description</h3>

<p>Nosofsky's (1984, 2011) Generalized Context Model; an exemplar-based
model of categorization.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  stsimGCM(st)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="stsimGCM_+3A_st">st</code></td>
<td>
<p>List of model parameters</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Argument <code>st</code> must be a list containing the following required
items: <code>training_items</code>, <code>tr</code>, <code>nCats</code>, <code>nFeat</code>, 
<code>sensitivity</code>, <code>weights</code>, <code>choice_bias</code>, <code>p</code>,
<code>r_metric</code>, <code>mp</code>, and <code>gamma</code>
</p>
<p><code>nCats</code> - integer indicating the number of categories
</p>
<p><code>nFeat</code> - integer indicating the number of stimulus dimensions
</p>
<p><code>tr</code> - the stimuli presented to the model, for which the choice
probabilities will be predicted. <code>tr</code> has to be a matrix or
dataframe with one row for each stimulus. <code>tr</code> requires the
following columns.
</p>
<p><code>x1, x2, ...</code> - columns for each dimension carrying the
corresponding values (have to be coded as numeric values) for each
exemplar (trial) given in the row. Columns have to start with
<code>x1</code> ascending with dimensions <code>..., x2, x3, ...</code> at
adjacent columns.
</p>
<p><code>tr</code> may have any number of additional columns with any desired 
name and position, e.g. for readability. As long as the feature columns 
<code>x1, x2, ...</code> are given as defined (i.e. not scattered, across 
the range of matrix columns), the output is not affected by optional 
columns.
</p>
<p><code>training_items</code> - all unique exemplars assumed to be stored in 
memory; has to be a matrix or dataframe with one row for each exemplar. 
The rownames have to start with 1 in ascending order.
<code>training_items</code> requires the following columns:
</p>
<p><code>x1, x2, ...</code> - columns for each feature dimension carrying 
the corresponding values (have to be coded as numeric values) for each 
exemplar (row). Columns have to start with <code>x1</code> ascending with 
dimensions <code>..., x2, x3, ...</code> at adjacent columns.
</p>
<p><code>cat1, cat2, ...</code> - columns that indicate the category
assignment of each exemplar (row). For example, if the exemplar in row
2 belongs to category 1 the corresponding cell of <code>cat1</code> has to
be set to <code>1</code>, else <code>0</code>. Columns have to start with
<code>cat1</code> ascending with categories <code>..., cat2, cat3, ...</code>
at adjacent columns.
</p>
<p><code>mem</code> - (optional) one column that indicates whether an exemplar 
receives an extra memory weight, yes = <code>1</code>, no = <code>0</code>. 
For each exemplar (row) in the <code>training_items</code> with <code>mem</code>
set to <code>0</code> the corresponding memory strength parameter is set to 1.
When <code>mem</code> for an exemplar is set to <code>1</code> the memory strength
parameter is set as defined in <code>mp</code>, see below.
</p>
<p><code>training_items</code> may have any number of additional columns with any 
desired name and position, e.g. for readability. As long as the feature
columns <code>x1, x2, ...</code> and <code>cat1, cat2, ...</code> are given as 
defined (i.e. not scattered, across the range of matrix columns), the 
output is not affected by optional columns.
</p>
<p>NOTE: The current model can be implemented as a prototype model if
the <code>training_items</code> only carry one row for each category representing
the values of the corresponding prototypes (e.g. see Minda &amp; Smith, (2011).
</p>
<p><code>mp</code> - memory strength parameter (optional). Can take any numeric 
value between -Inf and +Inf. The default is 1, i.e. all exemplars have 
the same memory strength. There are two ways of specifying <code>mp</code>,
i.e. either <em>globally</em> or <em>exemplar specific</em>:
</p>
<p>When <em>globally</em> setting <code>mp</code> to a single integer,
e.g. to 5, then all exemplars in <code>training_items</code> with <code>mem</code> 
= 1 will receive a memory strength 5 times higher than the memory strengths
for the remaining exemplars. 
</p>
<p>For setting <em>exemplar specific</em> memory strengths <code>mp</code> has to be 
a vector of length n, where n is the overall number of of exemplars with 
<code>mem</code> = 1 in the <code>training_items</code>. The order of memory strengths 
defined in this vector exactly follows their row-wise ascending order of 
appearence in the <code>training_items</code>. E.g. if there are two exemplars 
with <code>mem</code> = 1 in the <code>training_items</code>, the first one in row 2 
and the second one in row 10, then setting <code>mp</code> to c(3,2) will result 
in assigning a memory strength of 3 to the first exemplar (in row 2) and 
a memory strength of 2 to the second exemplar (in row 10). The memory 
strengths for all other exemplars will be set to 1. See Note 1.
</p>
<p><code>sensitivity</code> - sensitivity parameter c; can take any value
between 0 (all exemplars are equally similar) and +infinity 
(towards being  insensitive to large differences). There are two ways 
of specifying <code>sensitivity</code>, i.e. either <em>globally</em> or 
<em>exemplar specific</em>: When <em>globally</em> setting <code>sensitivity</code> 
to a single value, e.g. <code>sensitivity</code>=3, then the same parameter is 
applied to all exemplars. On the other hand, <em>exemplar specific</em>
sensitivity parameters can be used by defining <code>sensitivity</code> as
a vector of length n, where n is the number of rows in 
<code>training_items</code>. The <code>sensitivity</code> vector values then represent 
the sensitivity parameters for all exemplars in <code>training_items</code> at 
the corresponding row positions. E.g. if there are 3 exemplars (rows) in 
<code>training_items</code>, then setting <code>sensitivity</code> to <code>c(1,1,3)</code> 
assigns <code>sensitivity</code> = 1 to the first two exemplars, and 
<code>sensitivity</code> = 3 for the third exemplar. See Note 2. 
</p>
<p><code>weights</code> - dimensional attention weights. Order corresponds
to the definitions of <code>x1, x2, ...</code> in <code>tr</code> and 
<code>training_items</code>. Has to be a vector with length n-1 , where n
equals to <code>nFeat</code> dimension weights, e.g. of length 2 when
there are three features, leaving out the <em>last</em> dimension. A
constraint in the GCM is that all attentional weights sum to 1. Thus,
the sum of n-1 weights should be equal to or smaller than 1, too.  The
last n-th weight then is computed within the model with: 1 - (sum of
n-1 feature weights). When setting the weights to 1/<code>nFeat</code> =
equal weights. See Note 3.
</p>
<p><code>choice_bias</code> - Category choice biases. Has to be a vector with 
length n-1, where n equals to <code>nCats</code> category biases, leaving out 
the last category bias, under the constraint that all biases sum to 1. 
Order corresponds to the definitions of <code>cat1, cat2</code> in the
<code>training_items</code>. The sum of n-1 choice biases has to be equal
to or smaller than 1. Setting the weights to 1/<code>nCats</code> = no
choice bias. The bias for the last category then is computed in the
model with: 1 - (sum of <code>nCats</code>-1 choice biases). See Note 3.
</p>
<p><code>gamma</code> - decision constant/ response scaling.  Can take any 
value between 0 (towards more probabilistic) and +infinity (towards 
deterministic choices). Nosofsky (2011) suggests setting gamma higher 
than 1 when individual participants' data are considered. See Note 2.
</p>
<p><code>r_metric</code> - distance metric. Set to 1 (city-block) or 2
(Euclidean). See Nosofsky (2011), and Note 4, for more details.
</p>
<p><code>p</code> - similarity gradient. Set to 1 (exponential) or 2 (Gaussian). 
See Nosofsky (2011), for more details. 
</p>


<h3>Value</h3>

<p>A matrix of probabilities for category responses (columns) for each 
stimulus (rows) presented to the model (e.g. test trials). Stimuli 
and categories are in the same order as presented to the model in 
<code>st</code>, see below. 
</p>


<h3>Note</h3>

<p>1. Please note that setting mp = 1 or e.g. mp = 5 globally, will 
yield identical response probabilities. Crucially, memory strength 
is indifferent from the category choice bias parameter, if (and only
if) mp's vary between categories, without varying within categories. 
Thus, the memory strength parameter can therefore be interpreted in 
terms of an exemplar choice bias (potentially related to categorization 
accuracy). In addition, if exemplar specific mp's are assigned during 
parameter fitting, one might want to calculate the natural log of the 
corresponding estimates, enabling direct comparisons between mp's 
indicating different directions, e.g. -log(.5) = log(2), for loss and 
gain, respectively, which are equal regarding their extent into different 
directions.
</p>
<p>2. Theoretically, increasing global sensitivity indicates that 
categorization mainly relies on the most similar exemplars, usually
making choices less probabilistic. Thus sensitivity c is likely to 
be correlated with gamma. See Navarro (2007) for a detailed discussion.  
However, it is possible to assume exemplar specific sensitivities, or
specificity. Then, exemplars with lower sensitivity parameters
will have a stronger impact on stimulus similarity and thus 
categorization behavior for stimuli. See Rodrigues &amp; Murre 
(2007) for a related study. 
</p>
<p>3. Setting only the n-1 instead of all n feature weights (or bias
parameters) eases model fitting procedures, in which the last weight
always is a linear combination of the n-1 weights.
</p>
<p>4. See Tversky &amp; Gati (1982) for further info on r. In brief summary,
r=2 (usually termed Euclidean), then a large difference on only one 
feature outweighs small differences on all features. In contrast, 
if r=1 (usually termed City-Block or Manhattan distance) both aspects 
contribute to an equal extent to the distance. Thus, r = 2 comes with 
the assumption that small differences in all features may be less 
recognized, than a large noticable differences on one feature, which 
may be depend on confusability of the stimuli or on the nature of the 
given task domain (perceptual or abstract).
</p>


<h3>Author(s)</h3>

<p> Rene Schlegelmilch, Andy Wills </p>


<h3>References</h3>

<p>Minda, J. P., &amp; Smith, J. D. (2011). Prototype models of 
categorization: Basic formulation, predictions, and limitations. 
Formal approaches in categorization, 40-64.
</p>
<p>Navarro, D. J. (2007). On the interaction between exemplar-based 
concepts and a response scaling process. Journal of Mathematical 
Psychology, 51(2), 85-98.
</p>
<p>Nosofsky, R. M. (1984). Choice, similarity, and the context theory 
of classification. <em>Journal of Experimental Psychology: 
Learning, memory, and cognition, 10</em>(1), 104.
</p>
<p>Nosofsky, R. M. (2011). The generalized context model: An exemplar
model of classification. In Pothos, E.M. &amp; Wills, A.J. <em>Formal
approaches in categorization</em>. Cambridge University Press.
</p>
<p>Rodrigues, P. M., &amp; Murre, J. M. (2007). Rules-plus-exception tasks: 
A problem for exemplar models?. <em>Psychonomic Bulletin &amp; Review, 
14</em>(4), 640-646.
</p>
<p>Tversky, A., &amp; Gati, I. (1982). Similarity, separability, and the 
triangle inequality. <em>Psychological review, 89</em>(2), 123.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
    ## Three Categories with 2 Training Items each, and repeatedly presented
    ## transfer/test items (from nosof94train()). Each item has three 
    ## features with two (binary) values: memory strength (st$mp and 
    ## 'mem' column in st$training_items are optional) is 
    ## equal for all exemplars
    
    st&lt;-list(
        sensitivity = 3,
        weights = c(.2,.3),
        choice_bias = c(1/3),
        gamma = 1,
        mp = 1,
        r_metric = 1,
        p = 1,
        nCats = 2,
        nFeat=3
    )
    
    ## training item definitions 
    st$training_items &lt;- as.data.frame(
        t(matrix(cbind(c(1,0,1,1,1,0,0),c(1,1,0,2,1,0,0),
                       c(0,1,0,5,0,1,0),c(0,0,1,1,0,1,0)),
                 ncol=4, nrow=7,
                 dimnames=list(c("stim","x1", "x2", "x3",
                                 "cat1", "cat2", "mem"),
                               c(1:4)))))
                               
    st$tr &lt;- nosof94train()
    
    ## get the resulting predictions for the test items
    
    ## columns of the output correspond to category numbers as defined
    ## above rows correspond to the column indices of the test_items
    
    stsimGCM(st)

    ## columns of the output correspond to category numbers as defined
    ## above rows correspond to the column indices of the test_items

    ## Example 2

    ## Same (settings) as above, except: memory strength is 5 times higher
    ## for for some exemplars
    st$mp&lt;-5
    
    ## which exemplars?
    ## training item definitions 
    st$training_items &lt;- as.data.frame(
        t(matrix(cbind(c(1,0,1,1,1,0,1),c(1,1,0,2,1,0,0),
                       c(0,1,0,5,0,1,0),c(0,0,1,1,0,1,1)),
                 ncol=4, nrow=7,
                 dimnames=list(c("stim","x1", "x2", "x3",
                                 "cat1", "cat2", "mem"),
                               c(1:4)))))
    ## exemplars in row 1 and 4 will receive a memory strength of 5
    ## get predictions
    stsimGCM(st)

    ## Example 3 
    ## Same (settings) as above, except: memory strength is item specific
    ## for the two exemplars i.e. memory strength boost is not the same 
    ## for both exemplars (3 for the first in row 1, and 5 for the 
    ## second exemplar in row 4)
    st$mp&lt;-c(3,5) 


    ## get predictions
    stsimGCM(st)
</code></pre>

<hr>
<h2 id='thegrid'>Ordinal adequacy results for all catlearn simulations</h2><span id='topic+thegrid'></span>

<h3>Description</h3>

<p>Records results of all ordinal adequacy tests registered in the
catlearn package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(thegrid)</code></pre>


<h3>Format</h3>

<p>A data frame with the following columns:
</p>

<dl>
<dt>id</dt><dd><p>Unique identifier number for each entry into the
grid. When making a new entry, use the next available integer.</p>
</dd>
<dt>cirp</dt><dd><p>The CIRP (Canonical Independently Replicated Phenomenon)
against which a model was tested. This must correspond precisely to
the name of a data set in the catlearn package. </p>
</dd>
<dt>model</dt><dd><p>A one-word description of the model being
tested. Simulations in the same row of The Grid must have precisely
the same one-word description. Note, this is not the name of the
function used to run the simulation, nor the name of the model
implementation function. It is a descriptive term, defined by the
modeler.</p>
</dd>
<dt>result</dt><dd><p>Indicates the result of the simulation. 1 = passes
ordinal adequacy test, 0 = fails ordinal adequacy test, OES =
outside explanatory scope (in other words, this is not a result the
model was designed to accommodate), 'pending' = the function listed
in 'sim' is currently being written or tested.</p>
</dd>
<dt>sim</dt><dd><p>The name of the catlearn function used to run the
simulation.</p>
</dd>
<dt>oat</dt><dd><p>The name of the catlearn function used to perform the
Ordinal Adequacy Test.</p>
</dd>
</dl>



<h3>Details</h3>

<p>The Grid is a means of centrally recording the results of model
simulations centrally, within the catlearn package. For further
discussion, see Wills et al. (2016).  
</p>


<h3>Author(s)</h3>

<p>Andy J. Wills <a href="mailto:andy@willslab.co.uk">andy@willslab.co.uk</a>
</p>


<h3>Source</h3>

<p><code>citation('catlearn')</code>
</p>


<h3>References</h3>

<p>Wills, A.J., O'Connell, G., Edmunds, C.E.R. &amp; Inkster, A.B. (2016).
Progress in modeling through distributed collaboration: Concepts,
tools, and category-learning examples. <em>The Psychology of
Learning and Motivation</em>.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
