<!DOCTYPE html><html lang="en"><head><title>Help for package qlcMatrix</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {qlcMatrix}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#qlcMatrix-package'>
<p>Utility sparse matrix functions for Quantitative Language Comparison (QLC)</p></a></li>
<li><a href='#Array'>
<p>Sparse Arrays (&quot;Tensors&quot;)</p></a></li>
<li><a href='#assocSparse'>
<p>Association between columns (sparse matrices)</p></a></li>
<li><a href='#bibles'>
<p>A selection of bible-texts</p></a></li>
<li><a href='#corSparse'>
<p>Pearson correlation between columns (sparse matrices)</p></a></li>
<li><a href='#cosNominal'>
<p>Associations-measures for sparsely encoded nominal variables</p></a></li>
<li><a href='#cosSparse'>
<p>Cosine similarity between columns (sparse matrices)</p></a></li>
<li><a href='#dimRed'>
<p>Dimensionality Reduction for sparse matrices, based on Cholesky decomposition</p></a></li>
<li><a href='#distSparse'>
<p>Sparse distance matrix calculations</p></a></li>
<li><a href='#huber'>
<p>Comparative vocabulary for indigenous languages of Colombia (Huber &amp; Reed 1992)</p></a></li>
<li><a href='#jMatrix'>
<p>Harmonize (&lsquo;join&rsquo;) sparse matrices</p></a></li>
<li><a href='#pwMatrix'>
<p>Construct &lsquo;part-whole&rsquo; (pw) Matrices from tokenized strings</p></a></li>
<li><a href='#rKhatriRao'>
<p>&lsquo;reduced&rsquo; Khatri-Rao product (sparse matrices)</p></a></li>
<li><a href='#rowMax'>
<p>Row and column extremes (sparse matrices)</p></a></li>
<li><a href='#rSparseMatrix'>
<p>Construct a random sparse matrix</p></a></li>
<li><a href='#sim.nominal'>
<p>Similarity-measures for nominal variables</p></a></li>
<li><a href='#sim.strings'>
<p>String similarity by cosine similarity between bigram vectors</p></a></li>
<li><a href='#sim.wordlist'>
<p>Similarity matrices from wordlists</p></a></li>
<li><a href='#sim.words'>
<p>Similarity-measures for words between two languages, based on co-occurrences in parallel text</p></a></li>
<li><a href='#splitStrings'>
<p>Construct unigram and bigram matrices from a vector of strings</p></a></li>
<li><a href='#splitTable'>
<p>Construct sparse matrices from a nominal matrix/dataframe</p></a></li>
<li><a href='#splitText'>
<p>Construct sparse matrices from parallel texts</p></a></li>
<li><a href='#splitWordlist'>
<p>Construct sparse matrices from comparative wordlists (aka &lsquo;Swadesh list&rsquo;)</p></a></li>
<li><a href='#ttMatrix'>
<p>Construct a &lsquo;type-token&rsquo; (tt) Matrix from a vector</p></a></li>
<li><a href='#unfold'>
<p>Unfolding of Arrays</p></a></li>
<li><a href='#unfoldBlockMatrix'>
<p>Unfolding of block matrices (sparse matrices)</p></a></li>
<li><a href='#WALS'>
<p>The World Atlas of Language Structures (WALS)</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Version:</td>
<td>0.9.8</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-05-06</td>
</tr>
<tr>
<td>Title:</td>
<td>Utility Sparse Matrix Functions for Quantitative Language
Comparison</td>
</tr>
<tr>
<td>Description:</td>
<td>Extension of the functionality of the 'Matrix' package for using sparse matrices. Some of the functions are very general, while other are highly specific for special data format as used for quantitative language comparison.</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/cysouw/qlcMatrix">https://github.com/cysouw/qlcMatrix</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/cysouw/qlcMatrix/issues">https://github.com/cysouw/qlcMatrix/issues</a></td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Depends:</td>
<td>Matrix (&ge; 1.2), R (&ge; 3.2), slam (&ge; 0.1-32), sparsesvd</td>
</tr>
<tr>
<td>Imports:</td>
<td>methods, docopt</td>
</tr>
<tr>
<td>Suggests:</td>
<td>MASS, knitr, rmarkdown</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr, rmarkdown</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>6.0.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-05-08 14:26:33 UTC; cysouw</td>
</tr>
<tr>
<td>Author:</td>
<td>Michael Cysouw <a href="https://orcid.org/0000-0003-3168-4946"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Michael Cysouw &lt;cysouw@mac.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-05-08 21:20:03 UTC</td>
</tr>
</table>
<hr>
<h2 id='qlcMatrix-package'>
Utility sparse matrix functions for Quantitative Language Comparison (QLC)
</h2><span id='topic+qlcMatrix-package'></span><span id='topic+qlcMatrix'></span>

<h3>Description</h3>

<p>This package contains various functions that extend the functionality of the <code>Matrix</code> package for using sparse matrices. Some of the functions are very general, while other are highly specific for special data format as used for quantitative language comparison.
</p>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> qlcMatrix</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 0.9.8</td>
</tr>
<tr>
 <td style="text-align: left;">
Date: </td><td style="text-align: left;"> 2024-05-06</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL-3</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p>This package contains various different kinds of function. 
</p>
<p>First, some general utility functions to deal with sparse matrices: (i) <code><a href="#topic+rowMax">rowMax</a></code> to compute and identify row-wise maxima and minima in sparse matrices, (ii) <code><a href="#topic+rKhatriRao">rKhatriRao</a></code> to remove empty rows in a KhatriRao product (but still get the right rownames) and (iii) <code><a href="#topic+rSparseMatrix">rSparseMatrix</a></code> to produce random sparse matrices. There are also some experimental basic methods for handling sparse arrays (&quot;tensors&quot;), most interestingly <code><a href="#topic+unfold">unfold</a></code>.
</p>
<p>Second, some general functions to compute associations between the columns of sparse matrices, with possibilities for extension for ad-hoc measures: <code><a href="#topic+cosSparse">cosSparse</a></code>, <code><a href="#topic+corSparse">corSparse</a></code>, and <code><a href="#topic+assocSparse">assocSparse</a></code>  There are special versions of these for nominal data <code><a href="#topic+cosNominal">cosNominal</a>, <a href="#topic+assocNominal">assocNominal</a></code>. 
</p>
<p>Third, there are three central functions needed to efficiently turn data from quantitative language comparison into sparse matrices. These basic functions are then used by high-level function in this package. Although these functions might seem almost trivial, they form the basis for many highly complex computations. They are <code><a href="#topic+ttMatrix">ttMatrix</a></code>, <code><a href="#topic+pwMatrix">pwMatrix</a></code> and <code><a href="#topic+jMatrix">jMatrix</a></code>.
</p>
<p>Fourth, there are some high-level convenience function that take specific data formats from quantitative language comparison and turn them into set of sparse matrices for efficient computations. They might also be useful for other data types, but various details decisions are specifically tailored to the envisioned data types. These functions are <code><a href="#topic+splitTable">splitTable</a></code> <code><a href="#topic+splitStrings">splitStrings</a></code>, <code><a href="#topic+splitWordlist">splitWordlist</a></code>, and <code><a href="#topic+splitText">splitText</a></code>.
</p>
<p>Finally, there are various shortcuts to directly compute similarity matrices from various kinds of data: <code><a href="#topic+sim.nominal">sim.nominal</a>, <a href="#topic+sim.words">sim.words</a>, <a href="#topic+sim.strings">sim.strings</a>, <a href="#topic+sim.wordlist">sim.wordlist</a></code>. These are specifically tailored towards specific kinds of data, though they might also be useful elsewhere. Also, the code is mostly easy wrappers around the <code>split</code> and <code>cos/assoc</code> functions, so it should not be difficult to adapt these functions to other needs.
</p>


<h3>Author(s)</h3>

<p>Michael Cysouw &lt;cysouw@mac.com&gt;
</p>


<h3>References</h3>

<p>Cysouw, Michael. 2014. <em>Matrix Algebra for Language Comparison</em>. Manuscript.
</p>
<p>Mayer, Thomas and Michael Cysouw. 2012. Language comparison through sparse multilingual word alignment. <em>Proceedings of the EACL 2012 Joint Workshop of LINGVIS &amp; UNCLH</em>, 54&ndash;62. Avignon: Association for Computational Linguistics.
</p>
<p>Prokić, Jelena and Michael Cysouw. 2013. Combining regular sound correspondences and geographic spread. <em>Language Dynamics and Change</em> 3(2). 147&ndash;168.
</p>

<hr>
<h2 id='Array'>
Sparse Arrays (&quot;Tensors&quot;)
</h2><span id='topic+Array'></span><span id='topic+sparseArray'></span><span id='topic+as.Matrix'></span>

<h3>Description</h3>

<p>Convenient function linking sparse Arrays from the package <code>spam</code> to the sparse Matrices from the package <code>Matrix</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Array(A)
sparseArray(i, v = NULL, ...)

as.Matrix(M)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Array_+3A_a">A</code></td>
<td>

<p>An array to be turned into a sparse Array using <code>as.simple_sparse_array</code>. Can also be a <code>dataframe</code>, but see Details below about the treatment of data frames here.
</p>
</td></tr>
<tr><td><code id="Array_+3A_i">i</code></td>
<td>

<p>Integer matrix of array indices passed to <code>simple_sparse_array</code>.
</p>
</td></tr>
<tr><td><code id="Array_+3A_v">v</code></td>
<td>

<p>vector of values passed to <code>simple_sparse_array</code>. If <code>NULL</code> (by default), all specified indices (i.e. all rows in <code>i</code>) are given the value 1.
</p>
</td></tr>
<tr><td><code id="Array_+3A_m">M</code></td>
<td>

<p>Matrix of type <code>simple_triple_matrix</code> from the package <code>spam</code> to be turned into a <code>TsparseMatrix</code> from the packages <code>Matrix</code>.
</p>
</td></tr>
<tr><td><code id="Array_+3A_...">...</code></td>
<td>

<p>Further arguments passed to <code>simple_sparse_array</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>Array</code> turns an <code>array</code> into a sparse Array. There is a special behavior when a <code>dataframe</code> is supplied. Such a dataframe is treated as 'long format', i.e. the columns of the dataframe are treated as dimensions of the Array, and all rows of the dataframe are interpreted as entries. The coordinates are given by the ordering of the levels in the dataframe, and the dimnames are given by the levels.
</p>
<p><code>sparseArray</code> constructs sparse Arrays from a matrix of indices and a vector of values. dim and dimnames can be added as in <code><a href="slam.html#topic+simple_sparse_array">simple_sparse_array</a></code>
</p>
<p><code>as.Matrix</code> turns a <code>simple_triplet_matrix</code> into a <code>dgTMatrix</code>.
</p>


<h3>Value</h3>

<p>Sparse Arrays use the class &quot;simple_sparse_array&quot; from <code>spam</code>
</p>


<h3>Note</h3>

<p>These functions are only an example of how <code>spam</code> can be linked to <code>Matrix</code>.
</p>


<h3>Author(s)</h3>

<p>Michael Cysouw
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- matrix(c(1, 0, 0, 2), nrow = 2)
s &lt;- as.simple_triplet_matrix(x)
str(s)

as.Matrix(s)
str(as.Matrix(s))
</code></pre>

<hr>
<h2 id='assocSparse'>
Association between columns (sparse matrices)
</h2><span id='topic+assocSparse'></span><span id='topic+pmi'></span><span id='topic+res'></span><span id='topic+poi'></span><span id='topic+wpmi'></span>

<h3>Description</h3>

<p>This function offers an interface to various different measures of association between columns in sparse matrices (based on functions of &lsquo;observed&rsquo; and &lsquo;expected&rsquo; values). Currently, the following measures are available: pointwise mutual information (aka log-odds), a poisson-based measure and Pearson residuals. Further measures can easily be specifically defined by the user. The calculations are optimized to be able to deal with large sparse matrices. Note that these association values are really only (sensibly) defined for binary data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>assocSparse(X, Y = NULL, method = res, N = nrow(X), sparse = TRUE )
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="assocSparse_+3A_x">X</code></td>
<td>

<p>a sparse matrix in a format of the <code>Matrix</code> package, typically a <code>dgCMatrix</code> with only zeros or ones. The association will be calculated between the columns of this matrix.
</p>
</td></tr>
<tr><td><code id="assocSparse_+3A_y">Y</code></td>
<td>

<p>a second matrix in a format of the <code>Matrix</code> package with the same number of rows as X. When <code>Y=NULL</code>, then the associations between the columns of X and itself will be taken. If Y is specified, the association between the columns of X and the columns of Y will be calculated.
</p>
</td></tr>
<tr><td><code id="assocSparse_+3A_method">method</code></td>
<td>

<p>The method to be used for the calculation. Currently <code>res</code> (residuals), <code>poi</code> (poisson), <code>pmi</code> (pointwise mutual information) and <code>wpmi</code> (weighted pointwise mutual information) are available, but further methods can be specified by the user. See details for more information.
</p>
</td></tr>
<tr><td><code id="assocSparse_+3A_n">N</code></td>
<td>

<p>Variable that is needed for the calculations of the expected values. Only in exceptional situations this should be different from the default value (i.e. the number of rows of the matrix).
</p>
</td></tr>
<tr><td><code id="assocSparse_+3A_sparse">sparse</code></td>
<td>

<p>By default, nothing is computed when the observed co-occurrence of two columns is zero. This keeps the computations and the resulting matrix nicely sparse. However, for some measures (specifically the Pearson residuals &lsquo;res&rsquo;) this leads to incorrect results. Mostly the error is negligible, but if the correct behavior is necessary, chose <code>sparse = F</code>. Note that the result will then be a full matrix, so this is not feasible for large datasets.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Computations are based on a comparison of the observed interaction <code>crossprod(X,Y)</code> and the expected interaction. Expectation is in principle computed as <code>tcrossprod(rowSums(abs(X)),rowSums(abs(Y)))/nrow(X)</code>, though in practice the code is more efficient than that.
</p>
<p>Note that calculating the observed interaction as <code>crossprod(X,Y)</code> really only makes sense for binary data (i.e. matrices with only ones and zeros). Currently, all input is coerced to such data by <code>as(X, "nMatrix")*1</code>, meaning that all values that are not one or zero are turned into one (including negative values!).
</p>
<p>Any method can be defined as a function with two arguments, <code>o</code> and <code>e</code>, e.g. simply by specifying <code>method = function(o,e){o/e}</code>. See below for more examples.
</p>
<p>The predefined functions are:
</p>

<ul>
<li><p><code>pmi</code>: 
pointwise mutual information, aka as log-odds in bioinformatics, defined as <br /> 
<code>pmi &lt;- function(o,e) { log(o/e) }</code>.

</p>
</li>
<li><p><code>wpmi</code>: 
weighted pointwise mutual information, defined as<br /> 
<code>wpmi &lt;- function(o,e) { o * log(o/e) }</code>.

</p>
</li>
<li><p><code>res</code>:
Pearson residuals, defined as <br /> 
<code>res &lt;- function(o,e) { (o-e) / sqrt(e) }</code>.

</p>
</li>
<li><p><code>poi</code>:
association assuming a poisson-distribution of the values, defined as <br /> 
<code>poi &lt;- function(o,e) { sign(o-e) * (o * log(o/e) - (o-e)) }</code>. <br /> 
Seems to be very useful when the non-zero data is strongly skewed along the rows, i.e. some rows are much fuller than others. A short explanation of this method can be found in Prokić and Cysouw (2013).

</p>
</li></ul>



<h3>Value</h3>

<p>The result is a sparse matrix with the non-zero association values. Values range between -Inf and +Inf, with values close to zero indicating low association. The exact interpretation of the values depends on the method used.
</p>
<p>When <code>Y = NULL</code>, then the result is a symmetric matrix, so a matrix of type <code>dsCMatrix</code> with size <code>ncol(X)</code> by <code>ncol{X}</code> is returned. When <code>X</code> and <code>Y</code> are both specified, a matrix of type <code>dgCMatrix</code> with size <code>ncol(X)</code> by <code>ncol{Y}</code> is returned.
</p>


<h3>Note</h3>

<p>Care is taken in the implementation not to compute any association between columns that will end up with a value of zero anyway. However, very small association values will be computed. For further usage, these small values are often unnecessary, and can be removed for reasons of sparsity. Consider something like <code>X &lt;- drop0(X, tol = value)</code> on the resulting <code>X</code> matrix (which removes all values between -value and +value). See examples below.
</p>
<p>It is important to realize, that by default noting is computed when the observed co-occurrence is zero. However, this leads to wrong results with <code>method = res</code>, as <code>(o-e)/sqrt(e)</code> will be a negative value when <code>o = 0</code>. In most practically situations this error will be small and not important. However, when needed, the option <code>sparse = F</code> will give the correct results (though the resulting matrix will not be sparse anymore). Note that with all other methods implemented here, the default behavior leads to correct results (i.e. for <code>log(O)</code> nothing is calculated).
</p>
<p>The current implementation will not lead to correct results with lots of missing data (that option is simply not yet implemented). See <code><a href="#topic+cosMissing">cosMissing</a></code> for now.
</p>


<h3>Author(s)</h3>

<p>Michael Cysouw
</p>


<h3>References</h3>

<p>Prokić, Jelena &amp; Michael Cysouw. 2013. Combining regular sound correspondences and geographic spread. <em>Language Dynamics and Change</em> 3(2). 147&ndash;168.
</p>


<h3>See Also</h3>

<p>See <code><a href="#topic+assocCol">assocCol</a></code> and <code><a href="#topic+assocRow">assocRow</a></code> for this measure defined for nominal data. Also, see <code><a href="#topic+corSparse">corSparse</a></code> and <code><a href="#topic+cosSparse">cosSparse</a></code> for other sparse association measures. 
</p>


<h3>Examples</h3>

<pre><code class='language-R'># ----- reasonably fast with large very sparse matrices -----

X &lt;- rSparseMatrix(1e6, 1e6, 1e6, NULL)
system.time(M &lt;- assocSparse(X, method = poi))
length(M@x) / prod(dim(M)) # only one in 1e6 cells non-zero



# ----- reaching limits of sparsity -----

# watch out: 
# with slightly less sparse matrices the result will not be very sparse,
# so this will easily fill up your RAM during computation!

X &lt;- rSparseMatrix(1e4, 1e4, 1e6, NULL)
system.time(M &lt;- assocSparse(X, method = poi))
print(object.size(M), units = "auto") # about 350 Mb
length(M@x) / prod(dim(M)) # 30% filled

# most values are low, so it often makes sense 
# to remove low values to keep results sparse

M &lt;- drop0(M, tol = 2)
print(object.size(M), units = "auto") # reduces to 10 Mb
length(M@x) / prod(dim(M)) # down to less than 1% filled


# ----- defining new methods -----

# Using the following simple 'div' method is the same as
# using a cosine similarity with a 1-norm, up to a factor nrow(X)

div &lt;- function(o,e) {o/e}
X &lt;- rSparseMatrix(10, 10, 30, NULL)
all.equal(
	assocSparse(X, method = div),
	cosSparse(X, norm = norm1) * nrow(X)
	)

# ----- comparing methods -----

# Compare various methods on random data
# ignore values on diagonal, because different methods differ strongly here
# Note the different behaviour of pointwise mutual information (and division)

X &lt;- rSparseMatrix(1e2, 1e2, 1e3, NULL)

p &lt;- assocSparse(X, method = poi); diag(p) &lt;- 0
r &lt;- assocSparse(X, method = res); diag(r) &lt;- 0
m &lt;- assocSparse(X, method = pmi); diag(m) &lt;- 0
w &lt;- assocSparse(X, method = wpmi); diag(w) &lt;- 0
d &lt;- assocSparse(X, method = div); diag(d) &lt;- 0

pairs(~w@x+p@x+r@x+d@x+m@x, 
  labels=c("weighted pointwise\nmutual information","poisson","residuals","division",
           "pointwise\nmutual\ninformation"), cex = 0.7)

</code></pre>

<hr>
<h2 id='bibles'>
A selection of bible-texts
</h2><span id='topic+bibles'></span>

<h3>Description</h3>

<p>A selection of six bible texts as prepared by the paralleltext project.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(bibles)</code></pre>


<h3>Format</h3>

<p>A list of five elements
</p>

<dl>
<dt><code>verses</code></dt><dd><p>a character vector with all 43904 verse numbers as occurring throughout all translations as collected in the paralleltext project. This vector is used to align all texts to each other. The verse-numbers are treated as characters so ordering and matching works as expected.</p>
</dd>
<dt><code>eng</code></dt><dd><p>The English &lsquo;Darby&rsquo; Bible translation from 1890 by John Nelson Darby.
</p>
</dd>
<dt><code>deu</code></dt><dd><p>The Bible in German. Schlachter Version von 1951. Genfer Bibelgesellschaft 1951.
</p>
</dd>
<dt><code>tgl</code></dt><dd><p>The New Testament in Tagalog. Philippine Bible Society 1996.
</p>
</dd>
<dt><code>aak</code></dt><dd><p>The New Testament in the Ankave language of Papua New Guinea. Wycliffe Bible Translators, Inc. 1990.
</p>
</dd>
</dl>



<h3>Details</h3>

<p>Basically, all verse-numbering is harmonized, the text is unicode normalized, translations that capture multiple verses are included in the first of those verses, with the others left empty. Empty verses are thus a sign of combined translations. Verses that are not translated simply do not occur in the original files. Most importantly, the text are tokenized as to wordform, i.e. all punctuation and other non-word-based symbols are separated by spaces. In this way, space can be used for a quick wordform-based tokenization. The addition of spaces has been manually corrected to achieve a high precision of language-specific wordform tokenization.
</p>
<p>The Bible texts are provided as named vectors of strings, each containing one verse. The names of the vector are codes for the verses. See Mayer &amp; Cysouw (2014) for more information about the verse IDs and other formatting issues.
</p>


<h3>References</h3>

<p>Mayer, Thomas and Michael Cysouw. 2014. Creating a massively parallel Bible corpus. <em>Proceedings of LREC 2014</em>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># ----- load data -----

data(bibles)

# ----- separate into sparse matrices -----

# use splitText to turn a bible into a sparse matrix of wordforms x verses
E &lt;- splitText(bibles$eng, simplify = TRUE, lowercase = FALSE)

# all wordforms from the first verse
# (internally using pure Unicode collation, i.e. ordering is determined by Unicode numbering)
which(E[,1] &gt; 0)

# ----- co-occurrence across text -----

# how often do 'father' and 'mother' co-occur in one verse?
# (ignore warnings of chisq.test, because we are not interested in p-values here)

( cooc &lt;- table(E["father",] &gt; 0, E["mother",] &gt; 0) )
suppressWarnings( chisq.test(cooc)$residuals )

# the function 'sim.words' does such computations efficiently 
# for all 15000 x 15000 pairs of words at the same time

system.time( sim &lt;- sim.words(bibles$eng, lowercase = FALSE) )
sim["father", "mother"]

</code></pre>

<hr>
<h2 id='corSparse'>
Pearson correlation between columns (sparse matrices)
</h2><span id='topic+corSparse'></span>

<h3>Description</h3>

<p>This function computes the product-moment correlation coefficients between the columns of sparse matrices. Performance-wise, this improves over the approach taken in the <code><a href="stats.html#topic+cor">cor</a></code> function. However, because the resulting matrix is not-sparse, this function still cannot be used with very large matrices.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>corSparse(X, Y = NULL, cov = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="corSparse_+3A_x">X</code></td>
<td>

<p>a sparse matrix in a format of the <code>Matrix</code> package, typically <code>dgCMatrix</code> . The correlations will be calculated between the columns of this matrix.
</p>
</td></tr>
<tr><td><code id="corSparse_+3A_y">Y</code></td>
<td>

<p>a second matrix in a format of the <code>Matrix</code> package. When <code>Y = NULL</code>, then the correlations between the columns of X and itself will be taken. If Y is specified, the association between the columns of X and the columns of Y will be calculated.
</p>
</td></tr>
<tr><td><code id="corSparse_+3A_cov">cov</code></td>
<td>

<p>when <code>TRUE</code> the covariance matrix is returned, instead of the default correlation matrix.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>To compute the covariance matrix, the code uses the principle that
</p>
<p style="text-align: center;"><code class="reqn">E[(X - \mu(X))' (Y - \mu(Y))] = E[X' Y] - \mu(X') \mu(Y)</code>
</p>

<p>With sample correction n/(n-1) this leads to the covariance between X and Y as
</p>
<p style="text-align: center;"><code class="reqn">( X' Y - n * \mu(X') \mu(Y) ) / (n-1)</code>
</p>

<p>The computation of the standard deviation (to turn covariance into correlation) is trivial in the case <code>Y = NULL</code>, as they are found on the diagonal of the covariance matrix. In the case <code>Y != NULL</code> uses the principle that 
</p>
<p style="text-align: center;"><code class="reqn">E[X - \mu(X)]^2 = E[X^2] - \mu(X)^2</code>
</p>

<p>With sample correction n/(n-1) this leads to 
</p>
<p style="text-align: center;"><code class="reqn">sd^2 = ( X^2 - n * \mu(X)^2 ) / (n-1)</code>
</p>



<h3>Value</h3>

<p>The result is a regular square (non-sparse!) Matrix with the Pearson product-moment correlation coefficients between the columns of <code>X</code>. 
</p>
<p>When <code>Y</code> is specified, the result is a rectangular (non-sparse!) Matrix of size <code>nrow(X)</code> by <code>nrow(Y)</code> with the correlation coefficients between the columns of <code>X</code> and <code>Y</code>.
</p>
<p>When <code>cov = T</code>, the result is a covariance matrix (i.e. a non-normalized correlation).
</p>


<h3>Note</h3>

<p>Because of the &lsquo;centering&rsquo; of the Pearson correlation, the resulting Matrix is completely filled. This implies that this approach is normally not feasible with resulting matrices with more than 1e8 cells or so (except in dedicated computational environments with lots of RAM). However, in most sparse data situations, the cosine similarity <code><a href="#topic+cosSparse">cosSparse</a></code> will almost be identical to the Pearson correlation, so consider using that one instead. For a comparison, see examples below.
</p>
<p>For further usage, the many small coefficients are often unnecessary anyway, and can be removed for reasons of sparsity. Consider something like <code>M &lt;- drop0(M, tol = value)</code> on the resulting <code>M</code> matrix (which removes all values between -value and +value). See examples below.
</p>


<h3>Author(s)</h3>

<p>Michael Cysouw
</p>
<p>Slightly extended and optimized, based on the code from a discussion at <a href="https://stackoverflow.com/questions/5888287/running-cor-or-any-variant-over-a-sparse-matrix-in-r">stackoverflow</a>.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+cor">cor</a></code> in the base packages, <code><a href="#topic+cosSparse">cosSparse</a></code>, <code><a href="#topic+assocSparse">assocSparse</a></code> for other sparse association measures.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# reasonably fast (though not instantly!) with
# sparse matrices 1e4x1e4 up to a resulting matrix size of 1e8 cells.
# However, the calculations and the resulting matrix take up lots of memory

X &lt;- rSparseMatrix(1e3, 1e3, 1e4)
system.time(M &lt;- corSparse(X))
print(object.size(M), units = "auto") # more than 750 Mb

# Most values are low, so it often makes sense 
# to remove low values to keep results sparse

M &lt;- drop0(M, tol = 0.4)
print(object.size(M), units = "auto") # normally reduces size to about a quarter
length(M@x) / prod(dim(M)) # down to less than 0.01% non-zero entries


# comparison with other methods
# corSparse is much faster than cor from the stats package
# but cosSparse is even quicker than both!
# do not try the regular cor-method with larger matrices than 1e3x1e3
X &lt;- rSparseMatrix(1e3, 1e3, 1e4)
X2 &lt;- as.matrix(X)

# if there is a warning, try again with different random X
system.time(McorRegular &lt;- cor(X2)) 
system.time(McorSparse &lt;- corSparse(X))
system.time(McosSparse &lt;- cosSparse(X))

# cor and corSparse give identical results
all.equal(McorSparse, McorRegular)

# corSparse and cosSparse are not identical, but close
McosSparse &lt;- as.matrix(McosSparse)
dimnames(McosSparse) &lt;- NULL
all.equal(McorSparse, McosSparse) 

# Actually, cosSparse and corSparse are *almost* identical!
cor(as.dist(McorSparse), as.dist(McosSparse))

# So: consider using cosSparse instead of cor or corSparse.
# With sparse matrices, this gives mostly the same results, 
# but much larger matrices are possible
# and the computations are quicker and more sparse

</code></pre>

<hr>
<h2 id='cosNominal'>
Associations-measures for sparsely encoded nominal variables
</h2><span id='topic+cosNominal'></span><span id='topic+assocNominal'></span><span id='topic+cosCol'></span><span id='topic+cosRow'></span><span id='topic+assocCol'></span><span id='topic+assocRow'></span>

<h3>Description</h3>

<p>Nominal variables can be encoded as a combination of a sparse incidence and index matrix. Various functions to compute variations of <code>assocSparse</code> and <code>cosSparse</code> for such data are described here.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cosCol(X, colGroupX, Y = NULL, colGroupY = NULL, norm = norm2 )
assocCol(X, colGroupX, Y = NULL, colGroupY = NULL, method = res, sparse = TRUE)

cosRow(X, rowGroup, Y = NULL, norm = norm2 , weight = NULL)
assocRow(X, rowGroup, Y = NULL, method = res)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cosNominal_+3A_x">X</code>, <code id="cosNominal_+3A_y">Y</code></td>
<td>

<p>sparse matrices in a format of the <code>Matrix</code> package, typically <code>dgCMatrix</code> . When <code>Y = NULL</code>, then the similarity between the columns of X and itself will be taken. If Y is specified, the similarity between the columns of X and the columns of Y will be calculated.
</p>
</td></tr>
<tr><td><code id="cosNominal_+3A_colgroupx">colGroupX</code>, <code id="cosNominal_+3A_colgroupy">colGroupY</code></td>
<td>

<p>sparse matrices (typically pattern matrices) with the same number of columns as X and Y, respectively, indicating which columns belong to the same group. Each row of these matrices represents a group. 
</p>
</td></tr>
<tr><td><code id="cosNominal_+3A_rowgroup">rowGroup</code></td>
<td>

<p>sparse matrix (typically pattern matrices) with the same number of rows as X (and Y when not NULL), indicating which rows belong to the same group. Each column of these matrices represents a group.
</p>
</td></tr>
<tr><td><code id="cosNominal_+3A_norm">norm</code></td>
<td>

<p>norm to be used. See <code><a href="#topic+cosSparse">cosSparse</a></code> for details.
</p>
</td></tr>
<tr><td><code id="cosNominal_+3A_weight">weight</code></td>
<td>

<p>weighting of rows. See <code><a href="#topic+cosSparse">cosSparse</a></code> for details. Note that row-weighting only makes sense with <code>cosRow</code>.
</p>
</td></tr>
<tr><td><code id="cosNominal_+3A_method">method</code></td>
<td>

<p>method to be used. See <code><a href="#topic+assocSparse">assocSparse</a></code> for details.
</p>
</td></tr>
<tr><td><code id="cosNominal_+3A_sparse">sparse</code></td>
<td>

<p>All methods try to be as sparse as possible. Specifically, when there are no observed co-occurrence, then nothing is computed. This might lead to slight deviations in the results for some methods. Set <code>sparse=F</code> to force computation for all cells. This leads to non-sparse results, so use with caution with large datasets.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The approaches <code>assoc</code> and <code>cos</code> are described in detail in <code><a href="#topic+assocSparse">assocSparse</a></code> and <code><a href="#topic+cosSparse">cosSparse</a></code>, respectively. Those methods are extended here in case either the columns (<code>.col</code>) or the rows (<code>.row</code>) form groups. Specifically, this occurs with sparse encoding of nominal variables (see <code><a href="#topic+splitTable">splitTable</a></code>). In such encoding, the different values of a nominal variable are encoded in separate columns. However, these columns cannot be treated independently, but have to be treated as groups.
</p>
<p>The <code>.col</code> methods should be used when similarities between the different values of nominal variables are to be computed. The <code>.row</code> methods should be used when similarities between the observations of nominal variables are to be computed.
</p>
<p>Note that the calculations of the <code>assoc</code> functions really only makes sense for binary data (i.e. matrices with only ones and zeros). Currently, all input is coerced to such data by <code>as(X, "nMatrix")*1</code>, meaning that all values that are not one or zero are turned into one (including negative values!).
</p>


<h3>Value</h3>

<p>When <code>Y = NULL</code>, then all methods return symmetric similarity matrices in the form <code>dsCMatrix</code>, only specifying the upper triangle. The only exception is when <code>sparse=T</code> is chose, then the result will be in the form <code>dsyMatrix</code>.
</p>
<p>When a second matrix Y is specified, the result will be of the kind <code>dgCMatrix</code> or <code>dgeMatrix</code>, respectively.
</p>


<h3>Note</h3>

<p>Note that these methods automatically take missing data into account. They also work with large amount of missing data, but of course the validity of any similarity with much missing data is problematic.
</p>


<h3>Author(s)</h3>

<p>Michael Cysouw
</p>


<h3>See Also</h3>

<p><code><a href="#topic+sim.att">sim.att</a>, <a href="#topic+sim.obs">sim.obs</a></code> for convenient shortcuts around these methods.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># convenience functions are easiest to use
# first a simple example using the farms-dataset from MASS
library(MASS)

# to investigate the relation between the individual values
# This is similar to Multiple Correspondence Analysis (see mca in MASS)
f &lt;- splitTable(farms)
s &lt;- assocCol(f$OV,f$AV)
rownames(s) &lt;- f$values
plot(hclust(as.dist(-s)))
</code></pre>

<hr>
<h2 id='cosSparse'>
Cosine similarity between columns (sparse matrices)
</h2><span id='topic+cosSparse'></span><span id='topic+cosMissing'></span><span id='topic+idf'></span><span id='topic+isqrt'></span><span id='topic+none'></span><span id='topic+norm2'></span><span id='topic+norm1'></span><span id='topic+normL'></span>

<h3>Description</h3>

<p><code>cosSparse</code> computes the cosine similarity between the columns of sparse matrices. Different normalizations and weightings can be specified. Performance-wise, this strongly improves over the approach taken in the <code><a href="#topic+corSparse">corSparse</a></code> function, though the results are almost identical for large sparse matrices. <code>cosMissing</code> adds the possibility to deal with large amounts of missing data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cosSparse(X, Y = NULL, norm = norm2, weight = NULL)
cosMissing(X, availX, Y = NULL, availY = NULL, norm = norm2 , weight = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cosSparse_+3A_x">X</code></td>
<td>

<p>a sparse matrix in a format of the <code>Matrix</code> package, typically <code>dgCMatrix</code> . The similarity will be calculated between the columns of this matrix.
</p>
</td></tr>
<tr><td><code id="cosSparse_+3A_y">Y</code></td>
<td>

<p>a second matrix in a format of the <code>Matrix</code> package. When <code>Y = NULL</code>, then the similarity between the columns of X and itself will be taken. If Y is specified, the similarity between the columns of X and the columns of Y will be calculated.
</p>
</td></tr>
<tr><td><code id="cosSparse_+3A_availx">availX</code>, <code id="cosSparse_+3A_availy">availY</code></td>
<td>

<p>sparse Matrices (typically pattern matrices) of the same size of X and Y, respectively, indicating the available information for each matrix.
</p>
</td></tr>
<tr><td><code id="cosSparse_+3A_norm">norm</code></td>
<td>

<p>The function to be used for the normalization of the columns. Currently <code>norm2</code> (euclidean norm) and <code>norm1</code> (manhattan norm) are available, but further methods can be easily specified by the user. See details for more information.
</p>
</td></tr>
<tr><td><code id="cosSparse_+3A_weight">weight</code></td>
<td>

<p>The function to be used for the weighting of the rows. Currently <code>idf</code> (inverse document frequency) and <code>isqrt</code> (inverse square root) are available, but further methods can be easily specified by the user. See details for more information.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This measure is called a &lsquo;cosine&rsquo; similarity as it computes the cosine of the angle between high-dimensional vectors. It can also be considered a Pearson correlation without centering. Because centering removes sparsity, and because centering has almost no influence for highly sparse matrices, this cosine similarity performs much better that the Pearson correlation, both related for speed and memory consumption.
</p>
<p>The variant <code>cosMissing</code> can be used when the available information itself is also sparse. In such a situation, a zero in the data matrix X, Y can mean either &lsquo;zero value&rsquo; or &lsquo;missing data&rsquo;. To deal with the missing data, matrices indicating the available data can be specified. Note that this really only makes sense when the available data is sparse itself. When, say, 90% of the data is available, the <code>availX</code> matrix becomes very large, and the results does not differ strongly from the regular <code>cosSparse</code>, i.e. ignoring the missing data.
</p>
<p>Different normalizations of the columns and weightings of the rows can be specified. 
</p>
<p>The predefined normalizations are defined as a function of the matrix x and a &lsquo;summation function&rsquo; s (to be specified as a sparse matrix or a vector). This slight complexity is needed to be able to deal with missing data. With complete data, then <code>s = rep(1,nrow(X))</code>, leads to <code>crossprod(X,s) == colSums(X)</code>.
</p>

<ul>
<li><p><code>norm2</code>: 
euclidean norm. The default setting, and the same normalization as used in the Pearson correlation. It is defined as <br /> 
<code>norm2 &lt;- function(x,s) { drop(crossprod(x^2,s)) ^ 0.5 } </code>.

</p>
</li>
<li><p><code>norm1</code>:
Manhattan, or taxi-cab norm, defined as <br /> 
<code>norm1 &lt;- function(x,s) { abs(drop(crossprod(x,s))) } </code>.

</p>
</li>
<li><p><code>normL</code>:
normalized Laplacian norm, used in spectral clustering of a graph, defined as <br /> 
<code>normL &lt;- function(x,s) { abs(drop(crossprod(x,s))) ^ 0.5 } </code>.

</p>
</li></ul>

<p>The predefined weightings are defined as a function of the frequency of a row (s) and the number of columns (N):
</p>

<ul>
<li><p><code>idf</code>: 
inverse document frequency, used typically in distributional semantics to down-weight high frequent rows. It is defined as <br /> 
<code>idf &lt;- function(s,N) { log(N/(1+s)) } </code>.

</p>
</li>
<li><p><code>isqrt</code>:
inverse square root, an alternative to idf, defined as <br /> 
<code>isqrt &lt;- function(s,N) { s^-0.5 } </code>.

</p>
</li>
<li><p><code>none</code>:
no weighting. This is only included for use inside later high-level functions (e.g. <code><a href="#topic+sim.words">sim.words</a></code>). Normally, <code>weight = NULL</code> gives identical results, but is slightly quicker. <br />
<code>none &lt;- function(s,N) { s } </code>
</p>
</li></ul>

<p>Further norms of weighting functions can be defined at will.
</p>


<h3>Value</h3>

<p>The result is a sparse matrix with the non-zero association values. Values range between -1 and +1, with values close to zero indicating low association.
</p>
<p>When <code>Y = NULL</code>, then the result is a symmetric matrix, so a matrix of type <code>dsCMatrix</code> with size <code>ncol(X)</code> by <code>ncol{X}</code> is returned. When <code>X</code> and <code>Y</code> are both specified, a matrix of type <code>dgCMatrix</code> with size <code>ncol(X)</code> by <code>ncol{Y}</code> is returned.
</p>


<h3>Note</h3>

<p>For large sparse matrices, consider this as an alternative to <code><a href="stats.html#topic+cor">cor</a></code>. See <code><a href="#topic+corSparse">corSparse</a></code> for a  comparison of performance and results.
</p>


<h3>Author(s)</h3>

<p>Michael Cysouw
</p>


<h3>See Also</h3>

<p><code><a href="#topic+corSparse">corSparse</a></code>, <code><a href="#topic+assocSparse">assocSparse</a></code> for other sparse association measures. See also <code><a href="#topic+cosRow">cosRow</a>, <a href="#topic+cosCol">cosCol</a></code> for variants of cosSparse dealing with nominal data.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># reasonable fast on modern hardware


# try different sizes to find limits on local machine
system.time(X &lt;- rSparseMatrix(1e8, 1e8, 1e6))
system.time(M &lt;- cosSparse(X))


# consider removing small values of result to improve sparsity

X &lt;- rSparseMatrix(1e5, 1e5, 1e6)
print(object.size(X), units = "auto") # 12 Mb
system.time(M &lt;- cosSparse(X))
print(object.size(M), units = "auto") # 59 Mb
M &lt;- drop0(M, tol = 0.1) # remove small values
print(object.size(M), units = "auto") # 14 Mb

# Compare various weightings

# with random data from a normal distribution there is almost no difference
#
# data from a normal distribution
X &lt;- rSparseMatrix(1e2, 1e2, 1e3) 

w0 &lt;- cosSparse(X, norm = norm2, weight = NULL)@x
wi &lt;- cosSparse(X, norm = norm2, weight = idf)@x
ws &lt;- cosSparse(X, norm = norm2, weight = isqrt)@x

pairs(~ w0 + wi + ws, 
  labels=c("no weighting","inverse\ndocument\nfrequency","inverse\nsquare root"))

# with heavily skewed data there is a strong difference!
X &lt;- rSparseMatrix(1e2, 1e2, 1e3,
	rand.x = function(n){round(rpois(1e3, 10), 2)})

w0 &lt;- cosSparse(X, norm = norm2, weight = NULL)@x
wi &lt;- cosSparse(X, norm = norm2, weight = idf)@x
ws &lt;- cosSparse(X, norm = norm2, weight = isqrt)@x

pairs(~ w0 + wi + ws, 
  labels=c("no weighting","inverse\ndocument\nfrequency","inverse\nsquare root"))

</code></pre>

<hr>
<h2 id='dimRed'>
Dimensionality Reduction for sparse matrices, based on Cholesky decomposition
</h2><span id='topic+dimRed'></span>

<h3>Description</h3>

<p>To inspect the structure of a large sparse matrix, it is often highly useful to reduce the matrix to a few major dimensions (cf. multidimensional scaling). This functions implements a rough approach to provide a few major dimensions. The function provides a simple wrapper around <code><a href="Matrix.html#topic+Cholesky">Cholesky</a></code> and <code><a href="sparsesvd.html#topic+sparsesvd">sparsesvd</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dimRed(sim, k = 2, method = "svd")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="dimRed_+3A_sim">sim</code></td>
<td>

<p>Sparse, symmetric, positive-definite matrix (typically a similarity matrix produces by <code>sim</code> or <code>assoc</code> functions)
</p>
</td></tr>
<tr><td><code id="dimRed_+3A_k">k</code></td>
<td>

<p>Number of dimensions to be returned, defaults to two.
</p>
</td></tr>
<tr><td><code id="dimRed_+3A_method">method</code></td>
<td>

<p>Method used for the decomposition. Currently implemted are <code>svd</code> and <code>cholesky</code>. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Based on the Cholesky decomposition, the Matrix <code>sim</code> is decomposed into:
</p>
<p style="text-align: center;"><code class="reqn"> L D L'</code>
</p>

<p>The D Matrix is a diagonal matrix, the values of which are returned here as <code>$D</code>. Only the first few columns of the L Matrix are returned (possibly after permutation, see the details at <code><a href="Matrix.html#topic+Cholesky">Cholesky</a></code>).
</p>
<p>Based on the svd decomposition, the Matrix <code>sim</code> is decomposed into:
</p>
<p style="text-align: center;"><code class="reqn"> U D V</code>
</p>

<p>The U Matrix and the values from D are returned.
</p>


<h3>Value</h3>

<p>A list of two elements is returned:
</p>
<table role = "presentation">
<tr><td><code>L</code></td>
<td>
<p>: a sparse matrix of type <code>dgCMatrix</code> with <code>k</code> columns</p>
</td></tr>
<tr><td><code>D</code></td>
<td>
<p>: the diagional values from the Cholesky decomposition, or the eigenvalues from the svd decomposition</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michael Cysouw &lt;cysouw@mac.com&gt;
</p>


<h3>See Also</h3>

<p>See Also as <code><a href="Matrix.html#topic+Cholesky">Cholesky</a></code> and <code><a href="sparsesvd.html#topic+sparsesvd">sparsesvd</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># some random points in two dimensions
coor &lt;- cbind(sample(1:30), sample(1:30))

# using cmdscale() to reconstruct the coordinates from a distance matrix
d &lt;- dist(coor)
mds &lt;- cmdscale(d)

# using dimRed() on a similarity matrix.
# Note that normL works much better than other norms in this 2-dimensional case
s &lt;- cosSparse(t(coor), norm = normL)
red &lt;- as.matrix(dimRed(s)$L)

# show the different point clouds

oldpar&lt;-par("mfrow")
par(mfrow = c(1,3))

  plot(coor, type = "n", axes = FALSE, xlab = "", ylab = "")
  text(coor, labels = 1:30)
  title("Original coordinates")
  
  plot(mds, type = "n", axes = FALSE, xlab = "", ylab = "")
  text(mds, labels = 1:30)
  title("MDS from euclidean distances")
  
  plot(red, type = "n", axes = FALSE, xlab = "", ylab = "")
  text(red, labels = 1:30)
  title("dimRed from cosSparse similarity")

par(mfrow = oldpar)

# ======

# example, using the iris data
data(iris)
X &lt;- t(as.matrix(iris[,1:4]))
cols &lt;- rainbow(3)[iris$Species]

s &lt;- cosSparse(X, norm = norm1)
d &lt;- dist(t(X), method = "manhattan")

svd &lt;- as.matrix(dimRed(s, method = "svd")$L)
mds &lt;- cmdscale(d)

oldpar&lt;-par("mfrow")
par(mfrow = c(1,2))
  plot(mds, col = cols, main = "cmdscale\nfrom euclidean distances")
  plot(svd, col = cols, main = "dimRed with svd\nfrom cosSparse with norm1")
par(mfrow = oldpar)
</code></pre>

<hr>
<h2 id='distSparse'>
Sparse distance matrix calculations
</h2><span id='topic+distSparse'></span>

<h3>Description</h3>

<p>Sparse alternative to base <code><a href="stats.html#topic+dist">dist</a></code> function. WARNING: the result is not a distance metric, see details! Also: distances are calculated between columns (not between rows, as in the base <code>dist</code> function).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>distSparse(M, method = "euclidean", diag = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="distSparse_+3A_m">M</code></td>
<td>

<p>a sparse matrix in a format of the <code>Matrix</code> package, typically <code>dMatrix</code>. Any other matrices will be converted to such a sparse Matrix. The correlations will be calculated between the columns of this matrix (different from the base <code>dist</code> function!)
</p>
</td></tr>
<tr><td><code id="distSparse_+3A_method">method</code></td>
<td>

<p>method to calculate distances. Currently only <code>"euclidean"</code> is supported.
</p>
</td></tr>
<tr><td><code id="distSparse_+3A_diag">diag</code></td>
<td>

<p>should the diagonal be included in the results?
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A sparse distance matrix is a slightly awkward concept, because distances of zero are rare in most data. Further, it is mostly the small distances that are of interest, and not the large distanes (which are mostly also less trustwhorthy). Note that for random data, this assumption is not necessarily true.
</p>
<p>To obtain sparse results, the current implementation takes a special approach. First, only those distances will be calculated for which there is at least some non-zero data for both columns. The assumption is taken that those distances will be uninteresting (and relatively large anyway).
</p>
<p>Second, to differentiate the non-calculated distances from real zero distances, the distances are converted into similarities by substracting them from the maximum. In this way, all non-calculated distances are zero, and the real zeros have value <code>max(M)</code>.
</p>
<p>Euclidean distances are calculated using the following trick:
</p>
<p style="text-align: center;"><code class="reqn">colSums(M^2) + rowSums(M^2) - 2 * M'M</code>
</p>



<h3>Value</h3>

<p>A symmetric matrix of type <code>dsCMatrix</code>, consisting of similarity(!) values instead of distances (viz. <code>max(dist)-dist</code>).
</p>


<h3>Note</h3>

<p>Please note:
</p>

<ul>
<li><p>The values in the result are not distances, but similarities computed as <code>max(dist)-dist</code>.
</p>
</li>
<li><p>Non-calculated values are zero.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Michael Cysouw &lt;cysouw@mac.com
</p>


<h3>See Also</h3>

<p>See Also as <code><a href="stats.html#topic+dist">dist</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># to be done
</code></pre>

<hr>
<h2 id='huber'>
Comparative vocabulary for indigenous languages of Colombia (Huber &amp; Reed 1992)
</h2><span id='topic+huber'></span>

<h3>Description</h3>

<p>Data from Huber &amp; Reed (1992), containing a comparative vocabulary (a &lsquo;wordlist&rsquo;) for 69 indigenous languages from Colombia.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(huber)</code></pre>


<h3>Format</h3>

<p>A data frame with 27521 observations on the following 4 variables.
</p>

<dl>
<dt><code>CONCEPT</code></dt><dd><p>a factor with 366 levels, indicating the comparative concepts</p>
</dd>
<dt><code>COUNTERPART</code></dt><dd><p>a character vector listing the actual wordforms as described in Huber &amp; Reed 1992</p>
</dd>
<dt><code>DOCULECT</code></dt><dd><p>a factor with 71 levels, indicating the languages from which the wordforms are taken (&lsquo;documented lects&rsquo;, abbreviated as &lsquo;doculect&rsquo;). These are 69 indigenous languages from Colombia, and English and Spanish.</p>
</dd>
<dt><code>TOKENS</code></dt><dd><p>a tokenized version of the counterparts: spaces are added between graphemic units (i.e. groups of unicode characters that are functioning as a single unit in the orthography)</p>
</dd>
</dl>



<h3>Details</h3>

<p>The editors have attempted to use a harmonized orthography throughout all languages, approximately based on IPA, though there are still many language-specific idiosyncrasies included. However, the translations into English and Spanish are written in their regular orthography, and not in the IPA-dialect as used for the other languages. In general, the &lsquo;translations&rsquo; into English and Spanish are simply lowercase versions of the concept-names, included here to more flexibly identify the meaning of words in the Colombian languages. In many cases these translations are somewhat clunky (e.g. &lsquo;spring of water&rsquo;), and are missing the proper orthography details (e.g. &lsquo;Adams apple&rsquo;).
</p>
<p>The book was digitized in the QuantHistLing project and provided here as an example of dealing efficiently with reasonably large data. Care has been taken to faithfully represent the original transcription from the printed version.
</p>


<h3>Source</h3>

<p>Huber, Randall Q. &amp; Robert B. Reed. 1992. <em>Vocabulario Comparativo: Palabras Selectas de Lenguas Indigenas de Colombia.</em> Bogota: Instituto Linguistico de Verano. available online at <a href="https://colombia.sil.org/es/resources/archives/18886">https://colombia.sil.org/es/resources/archives/18886</a>. Copyright 2014 SIL International.</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(huber)
head(huber)
</code></pre>

<hr>
<h2 id='jMatrix'>
Harmonize (&lsquo;join&rsquo;) sparse matrices
</h2><span id='topic+jMatrix'></span><span id='topic+jcrossprod'></span><span id='topic+tjcrossprod'></span>

<h3>Description</h3>

<p>A utility function to make sparse matrices conformable semantically. Not only are the dimensions made conformable in size, but also the content of the dimensions. Formulated differently, this function harmonizes two matrices on a dimensions that have the same entities, but in a different order (and possibly with different subsets). Given two matrices with such (partly overlapping) dimensions, two new matrices are generated to reorder the original matrices via a matrix product to make them conformable. In an abstract sense, this is similar to an SQL &lsquo;inner join&rsquo; operation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>jMatrix(rownamesX, rownamesY, collation.locale = "C")

jcrossprod(X, Y, rownamesX = rownames(X), rownamesY = rownames(Y))
tjcrossprod(X, Y, colnamesX = colnames(X), colnamesY = colnames(Y))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="jMatrix_+3A_rownamesx">rownamesX</code>, <code id="jMatrix_+3A_rownamesy">rownamesY</code></td>
<td>

<p>rownames to be joined from two matrices.
</p>
</td></tr>
<tr><td><code id="jMatrix_+3A_x">X</code>, <code id="jMatrix_+3A_y">Y</code></td>
<td>

<p>sparse matrices to be made (semantically) conformable.
</p>
</td></tr>
<tr><td><code id="jMatrix_+3A_colnamesx">colnamesX</code>, <code id="jMatrix_+3A_colnamesy">colnamesY</code></td>
<td>

<p>colnames to be joined from two matrices.
</p>
</td></tr>
<tr><td><code id="jMatrix_+3A_collation.locale">collation.locale</code></td>
<td>

<p>locale to be used for ordering of the joined dimension. Defaults to pure numerical unicode ordering &quot;C&quot;. See <code><a href="#topic+ttMatrix">ttMatrix</a></code> for details.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Given a sparse matrix X with rownames rX and a sparse matrix Y with rownames rY, the function <code>jMatrix</code> produces joined rownames rXY with all unique entries in <code>c(rX, rY)</code>, reordered according to the specified locale, if necessary.
</p>
<p>Further, two sparse matrices M1 and M2 are returned to link X and Y to the new joined dimension rXY. Specifically, X2 = M1 %*% X and Y2 = M2 %*% Y will have conformable rXY rows, so crossprod(X2, Y2) can be computed. Note that the result will be empty when there is no overlap between the rownames of X and Y.
</p>
<p>The function <code>jcrossprod</code> is a shortcut to compute the above crossproduct immediately, using <code>jMatrix</code> internally to harmonize the rows. Similarly, <code>tjcrossprod</code> computes the tcrossprod, harmonizing the <em>columns</em> of two matrices using <code>jMatrix</code>.
</p>


<h3>Value</h3>

<p><code>jMatrix</code> returns a list of three elements (for naming, see Details above):
</p>
<table role = "presentation">
<tr><td><code>M1</code></td>
<td>

<p>sparse pattern matrix of type <code>ngCMatrix</code> with dimensions <code>c(length(rXY),length(rX))</code>
</p>
</td></tr>
<tr><td><code>M2</code></td>
<td>

<p>sparse pattern matrix of type <code>ngCMatrix</code> with dimensions <code>c(length(rXY),length(rY))</code>
</p>
</td></tr>
<tr><td><code>rownames</code></td>
<td>

<p>unique joined row names rXY
</p>
</td></tr>
</table>
<p><code>jcrossprod</code> and <code>tjcrossprod</code> return a sparse Matrix of type <code>ngCMatrix</code> when both X and Y are pattern matrices. Otherwise they return a sparse Matrix of type <code>dgCMatrix</code>.
</p>


<h3>Note</h3>

<p>Actually, it is unimportant whether the inputs to <code>jMatrix</code> are row or column names. However, care has to be taken to use the resulting matrices in the right transposition. To make this function easier to explain, I consistently talk only about row names above.
</p>


<h3>Author(s)</h3>

<p>Michael Cysouw
</p>


<h3>Examples</h3>

<pre><code class='language-R'># example about INNER JOIN from wikipedia
# http://en.wikipedia.org/wiki/Sql_join#Inner_join
# this might look complex, but it is maximally efficient on large sparse matrices

# Employee table as sparse Matrix
Employee.LastName &lt;- c("Rafferty","Jones","Heisenberg","Robinson","Smith","John")
Employee.DepartmentID &lt;- c(31,33,33,34,34,NA)
E.LN &lt;- ttMatrix(Employee.LastName, simplify = TRUE)
E.DID &lt;- ttMatrix(Employee.DepartmentID, simplify = TRUE)

( Employees &lt;- tcrossprod(E.LN, E.DID) )

# Department table as sparse Matrix
Department.DepartmentID &lt;- c(31,33,34,35)
Department.DepartmentName &lt;- c("Sales","Engineering","Clerical","Marketing")
D.DID &lt;- ttMatrix(Department.DepartmentID, simplify = TRUE)
D.DN &lt;- ttMatrix(Department.DepartmentName, simplify = TRUE)

( Departments &lt;- tcrossprod(D.DN, D.DID) )

# INNER JOIN on DepartmentID (i.e. on the columns of these two matrices)
# result is a sparse matrix linking Employee.LastName to Department.DepartmentName, 
# internally having used the DepartmentID for the linking

( JOIN &lt;- tjcrossprod(Employees, Departments) )

# Note that in this example it is much easier to directly use jMatrix on the DepartmentIDs
# instead of first making sparse matrices from the data
# and then using tjcrossprod on the matrices to get the INNER JOIN
# (only the ordering is different in this direct approach)

J &lt;- jMatrix(Employee.DepartmentID, Department.DepartmentID)
JOIN &lt;- crossprod(J$M1, J$M2)
rownames(JOIN) &lt;- Employee.LastName
colnames(JOIN) &lt;- Department.DepartmentName
JOIN
</code></pre>

<hr>
<h2 id='pwMatrix'>
Construct &lsquo;part-whole&rsquo; (pw) Matrices from tokenized strings
</h2><span id='topic+pwMatrix'></span>

<h3>Description</h3>

<p>A part-whole Matrix is a sparse matrix representation of a vector of strings (&lsquo;wholes&rsquo;) split into smaller parts by a specified separator. It basically summarizes which strings consist of which parts. By itself, this is not a very interesting transformation, but it allows for quite fancy computations by simple matrix manipulations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pwMatrix(strings, sep = "", gap.length = 0, gap.symbol = "\u2043", simplify = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="pwMatrix_+3A_strings">strings</code></td>
<td>

<p>a vector (or list) of strings to be separated into parts
</p>
</td></tr>
<tr><td><code id="pwMatrix_+3A_sep">sep</code></td>
<td>

<p>The separator to be used. Defaults to space <code>sep = " "</code>. If separation in individual characters is needed, use <code>sep = ""</code>. There is no fancy parsing of strings implemented (e.g. to catch complex unicode combined characters), that has to be done externally. The preferred route is to prepare the separation of the strings by using spaces, and then call this function.
</p>
</td></tr>
<tr><td><code id="pwMatrix_+3A_gap.length">gap.length</code></td>
<td>

<p>This adds the specified number of gap symbols between each pair of strings. This is only important for generating higher ngram-statistics later on, when no ordering of the strings is implied. For example, when the strings are alphabetically ordered words, any bigram-statistics should not count the bigrams consisting of the last character of the a word with the first character of the next word. 
</p>
</td></tr>
<tr><td><code id="pwMatrix_+3A_gap.symbol">gap.symbol</code></td>
<td>

<p>The gap symbol to insert (see gap.length above). It defaults to U+2043 <code>HYPHEN BULLET</code> on the assumption that this character will not often be included in data.
</p>
</td></tr>
<tr><td><code id="pwMatrix_+3A_simplify">simplify</code></td>
<td>

<p>by default, the row and column names are not included into the matrix to keep the matrix as lean as possible. The row names (&lsquo;parts&rsquo;) are returned separately. Using <code>simplify = T</code> the row and column names will be added into the matrix. Note that the column names are simply the vector that went into the function.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Internally, this is basically using <code>strsplit</code> and some cosmetic changes, returning a sparse matrix.
</p>


<h3>Value</h3>

<p>By default (when <code>simplify = F</code>) the output is a list with two elements, containing:
</p>
<table role = "presentation">
<tr><td><code>M</code></td>
<td>

<p>a sparse pattern Matrix of type <code>ngCMatrix</code> with all input strings as columns, and all separated elements as rows.
</p>
</td></tr>
<tr><td><code>rownames</code></td>
<td>

<p>all different  characters from the strings in order (i.e. all individual tokens of the original strings).
</p>
</td></tr>
</table>
<p>When <code>simplify = T</code>, then only the matrix M with row and column names is returned.
</p>


<h3>Author(s)</h3>

<p>Michael Cysouw
</p>


<h3>See Also</h3>

<p>Used in <code><a href="#topic+splitStrings">splitStrings</a></code> and <code><a href="#topic+splitWordlist">splitWordlist</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># By itself, this functions does nothing really interesting
example &lt;- c("this","is","an","example")
pw &lt;- pwMatrix(example)
pw

# However, making a type-token Matrix (with ttMatrix) of the rownames
# and then taking a matrix product, results in frequencies of each element in the strings
tt &lt;- ttMatrix(pw$rownames)
distr &lt;- (tt$M*1) %*% (pw$M*1)
rownames(distr) &lt;- tt$rownames
colnames(distr) &lt;- example
distr

# Use banded sparse matrix with superdiagonal ('shift matrix') to get co-occurrence counts
# of adjacent characters. Rows list first character, columns adjacent character. 
# Non-zero entries list number of co-occurrences
S &lt;- bandSparse( n = ncol(tt$M), k = 1) * 1
TT &lt;- tt$M * 1
( C &lt;- TT %*% S %*% t(TT) )

# show the non-zero entries as triplets:
s &lt;- summary(C)
first &lt;- tt$rownames[s[,1]]
second &lt;- tt$rownames[s[,2]]
freq &lt;- s[,3]
data.frame(first,second,freq)	
</code></pre>

<hr>
<h2 id='rKhatriRao'>
&lsquo;reduced&rsquo; Khatri-Rao product (sparse matrices)
</h2><span id='topic+rKhatriRao'></span>

<h3>Description</h3>

<p>This function performs a Khatri-Rao product (&lsquo;column-wise Kronecker product&rsquo;, see <code><a href="Matrix.html#topic+KhatriRao">KhatriRao</a></code> for more info) on two sparse matrices. However, the result of such a product on sparse matrices normally results in very many empty rows. This function removes those empty rows, and, most importantly, it produces row names only for the remaining rows. For large sparse matrices this is <em>much</em> more efficient than first producing all rownames, and then removing the one with the empty rows.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rKhatriRao(X, Y, 
	rownamesX = rownames(X), rownamesY = rownames(Y), 
	simplify = FALSE, binder = ":", FUN = "*")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="rKhatriRao_+3A_x">X</code>, <code id="rKhatriRao_+3A_y">Y</code></td>
<td>

<p>matrices of with the same number of columns.
</p>
</td></tr>
<tr><td><code id="rKhatriRao_+3A_rownamesx">rownamesX</code>, <code id="rKhatriRao_+3A_rownamesy">rownamesY</code></td>
<td>

<p>row names of matrices X and Y. These can be specified separately, but they default to the row names of the matrices.
</p>
</td></tr>
<tr><td><code id="rKhatriRao_+3A_simplify">simplify</code></td>
<td>

<p>by default, the names of rows and columns are not included into the matrix to keep the matrix as lean as possible: the row names are returned separately. Using <code>include.dimnames=T</code> adds the row names into the matrix. The column names are directly taken from X.
</p>
</td></tr>
<tr><td><code id="rKhatriRao_+3A_binder">binder</code></td>
<td>

<p>symbol to include between the row names of X and Y for the resulting matrix
</p>
</td></tr>
<tr><td><code id="rKhatriRao_+3A_fun">FUN</code></td>
<td>

<p>function to be used in the KhatriRao product, passed internally to the workhorse <code><a href="Matrix.html#topic+KhatriRao">KhatriRao</a></code>
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Up to 1e6 row names to be produced goes reasonably quick with the basic <code><a href="Matrix.html#topic+KhatriRao">KhatriRao</a></code> function. However, larger amounts of pasting of row names becomes very slow, and the row names take an enormous amount of RAM. This function solves that problem by only producing row names for the non-empty rows.
</p>


<h3>Value</h3>

<p>By default, the result is a list of two items:
</p>
<table role = "presentation">
<tr><td><code>M</code></td>
<td>
<p>resulting sparse product matrix with empty rows removed</p>
</td></tr>
<tr><td><code>rownames</code></td>
<td>
<p>a vector with the resulting row names for the non-empty rows</p>
</td></tr>
</table>
<p>When <code>simplify=T</code>, then the matrix is return with the row names included.
</p>


<h3>Note</h3>

<p>This function allows for the row names of the input matrices to be added separately, and the resulting row names are returned separately by default. This might seem a bit unusual, given the nice way how R integrates row names into matrices. However, it turns out often to be easier to store row- and column names separately to efficiently work with large sparse matrices.
</p>


<h3>Author(s)</h3>

<p>Michael Cysouw
</p>


<h3>See Also</h3>

<p><code><a href="Matrix.html#topic+KhatriRao">KhatriRao</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># two sparse matrices with row names

X &lt;- rSparseMatrix(1e4, 1e3, 1e4)
Y &lt;- rSparseMatrix(1e4, 1e3, 1e4)

rownames(X) &lt;- 1:nrow(X)
rownames(Y) &lt;- 1:nrow(Y)

# the basic KhatriRao product from the Matrix package is very fast
# but almost all rows are empty

system.time(M &lt;- KhatriRao(X, Y))
sum(rowSums(M)==0)/nrow(M) # 99.9% empty rows

# To produce all row names takes a long time with KhatriRao from Matrix
# with the current example with 1e8 row names it took a minute on my laptop
# so: don't try the following, except on a large machine!


system.time(M &lt;- KhatriRao(X, Y, make.dimnames = TRUE))


# Using the current special version works just fine and is reasonably quick
system.time(M &lt;- rKhatriRao(X, Y))

</code></pre>

<hr>
<h2 id='rowMax'>
Row and column extremes (sparse matrices)
</h2><span id='topic+rowMax'></span><span id='topic+colMax'></span><span id='topic+rowMin'></span><span id='topic+colMin'></span>

<h3>Description</h3>

<p>Compute maxima and minima for all rows or columns of sparse matrices. Optionally also return which elements are the maxima/minima per row/column.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rowMax(X, which = FALSE, ignore.zero = TRUE)
colMax(X, which = FALSE, ignore.zero = TRUE)

rowMin(X, which = FALSE, ignore.zero = TRUE)
colMin(X, which = FALSE, ignore.zero = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="rowMax_+3A_x">X</code></td>
<td>

<p>a sparse matrix in a format of the <code>Matrix</code> package, typically <code>dgCMatrix</code> . The maxima or minima will be calculated for each row or column of this matrix.</p>
</td></tr>
<tr><td><code id="rowMax_+3A_which">which</code></td>
<td>

<p>optionally return a sparse matrix of the same dimensions as <code>X</code> marking the positions of the columns- or row-wise maxima or minima.
</p>
</td></tr>
<tr><td><code id="rowMax_+3A_ignore.zero">ignore.zero</code></td>
<td>

<p>By default, only the non-zero elements are included in the computations. However, when <code>ignore.zero = F</code> then zeros are also considered. This basically means that for all maxima below zero, the maximum will be set to zero. Likewise, for all minima above zero, the minimum will be set to zero.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The basic workhorse of these functions is the function <code>rollup</code> from the package <code>slam</code>.
</p>


<h3>Value</h3>

<p>By default, these functions returns a <code><a href="Matrix.html#topic+sparseVector">sparseVector</a></code> with the non-zero maxima or minima. Use additionally <code>as.vector</code> to turn this into a regular vector.
</p>
<p>When <code>which = T</code>, the result is a list of two items:
</p>
<table role = "presentation">
<tr><td><code>max/min</code></td>
<td>

<p>the same sparse vector as described above.
</p>
</td></tr>
<tr><td><code>which</code></td>
<td>

<p>a sparse pattern matrix of the kind <code>ngCMatrix</code> indicating the position of the extrema. Note that an extreme might occur more than once per row/column. In that case multiple entries in the row/column are indicated.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michael Cysouw
</p>


<h3>Examples</h3>

<pre><code class='language-R'># rowMax(X, ignore.zero = FALSE) is the same as apply(X, 1, max)
# however, with large sparse matrices, the 'apply' approach will start eating away at memory
# and things become slower.
X &lt;- rSparseMatrix(1e3, 1e3, 1e2)
system.time(m1 &lt;- rowMax(X, ignore.zero = FALSE))
system.time(m2 &lt;- apply(X, 1, max)) # slower
all.equal(as.vector(m1), m2) # but same result

# to see the effect even stronger, try something larger
# depending on the amount of available memory, the 'apply' approach will give an error
# "problem too large"
## Not run: 
X &lt;- rSparseMatrix(1e6, 1e6, 1e6)
system.time(m1 &lt;- rowMax(X, ignore.zero = FALSE))
system.time(m2 &lt;- apply(X, 1, max))

## End(Not run)

# speed depends most strongly on the number of entries in the matrix
# also some performance loss with size of matrix
# up to 1e5 entries is still reasonably fast

X &lt;- rSparseMatrix(1e7, 1e7, 1e5)
system.time(m &lt;- rowMax(X))


X &lt;- rSparseMatrix(1e7, 1e7, 1e5)
system.time(M &lt;- rowMax(X)) # about ten times as slow


# apply is not feasably on such large matrices
# Error: problem too large...
## Not run: 
m &lt;- apply(X, 1, max) 

## End(Not run)
</code></pre>

<hr>
<h2 id='rSparseMatrix'>
Construct a random sparse matrix
</h2><span id='topic+rSparseMatrix'></span>

<h3>Description</h3>

<p>This convenience function constructs a random sparse matrix of specified size, with specified sparsity. This is mainly useful for testing speed and memory load of sparse matrix manipulations
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rSparseMatrix(nrow, ncol, nnz, 
	rand.x = function(n) round(rnorm(nnz), 2), ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="rSparseMatrix_+3A_nrow">nrow</code></td>
<td>

<p>number of rows of the resulting matrix.
</p>
</td></tr>
<tr><td><code id="rSparseMatrix_+3A_ncol">ncol</code></td>
<td>

<p>number of columns of the resulting matrix.
</p>
</td></tr>
<tr><td><code id="rSparseMatrix_+3A_nnz">nnz</code></td>
<td>

<p>number of entries of the resulting matrix.
</p>
</td></tr>
<tr><td><code id="rSparseMatrix_+3A_rand.x">rand.x</code></td>
<td>

<p>randomization used for the construction of the entries. if <code>NULL</code> then a pattern matrix is constructed (random entries without values).
</p>
</td></tr>
<tr><td><code id="rSparseMatrix_+3A_...">...</code></td>
<td>

<p>Other arguments passed to <code>sparseMatrix</code> internally.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The sparsity of the resulting matrix (i.e. the fraction of non-zero entries to all entries) is <code class="reqn">\frac{nnz}{nrow * ncol}</code>.
</p>


<h3>Value</h3>

<p>Returns a sparse matrix of the type <code>dgCMatrix</code>. Defaults to random numeric entries with two decimal digits, generated randomly from a normal distribution with <code>mean = 0</code> and <code>sd = 1</code>.
</p>
<p>When <code>rand.x = NULL</code> then the result is a patter<strong>n</strong> matrix of type <code>ngCMatrix</code>.
</p>


<h3>Author(s)</h3>

<p>Martin Maechler with slight tweaks by Michael Cysouw
</p>


<h3>See Also</h3>

<p>For random permutation matrices, see <code><a href="Matrix.html#topic+pMatrix-class">pMatrix-class</a></code>. Specifically note the construction option <br />
<code>(p10 &lt;- as(sample(10),"pMatrix"))</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># example with reasonably large (100.000 by 100.000) but sparse matrix
# (only one in 10.000 entries is non-zero). On average 10 entries per column.
X &lt;- rSparseMatrix(1e5, 1e5, 1e6)
print(object.size(X), units = "auto")

# speed of cosine similarity
system.time(M &lt;- cosSparse(X))

# reduce memory footprint by removing low values
print(object.size(M), units = "auto")
M &lt;- drop0(M, tol = 0.1)
print(object.size(M), units = "auto")
</code></pre>

<hr>
<h2 id='sim.nominal'>
Similarity-measures for nominal variables
</h2><span id='topic+sim.nominal'></span><span id='topic+sim.att'></span><span id='topic+sim.obs'></span>

<h3>Description</h3>

<p>Nominal variables can be encoded as a combination of a sparse incidence and index matrix, as discussed at <code><a href="#topic+splitTable">splitTable</a></code>. The present two functions are easy-to-use shortcuts to use those sparse matrices to computes pairwise similarities, either between observations (<code>sim.obs</code>) or attributes (<code>sim.att</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sim.att(D, method = "chuprov", sparse = TRUE, ...)
sim.obs(D, method = "hamming", sparse = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sim.nominal_+3A_d">D</code></td>
<td>

<p>Dataframe with nominal attributes (&lsquo;variables&rsquo;) as columns and observations as rows.
</p>
</td></tr>
<tr><td><code id="sim.nominal_+3A_method">method</code></td>
<td>

<p>method to be used for similarity computation. See Details below.
</p>
</td></tr>
<tr><td><code id="sim.nominal_+3A_sparse">sparse</code></td>
<td>

<p>All methods try to be as sparse as possible. Specifically, when there are no observed co-occurrence, then nothing is computed. This might lead to slight deviations in the results for some methods. Set <code>sparse=F</code> to force computation for all cells. This leads to non-sparse results, so use with caution with large datasets.
</p>
</td></tr>
<tr><td><code id="sim.nominal_+3A_...">...</code></td>
<td>

<p>Arguments passed internally to <code><a href="#topic+splitTable">splitTable</a></code>, especially useful for multi-valued cells, using the option <code>split</code>. Note that <code>method = hamming</code> will give unexpected results for the comparison of cells that both are multi-valued. Consider using <code>method = weighted</code> instead.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function <code>sim.att</code> and <code>sim.obs</code> are convenience wrappers around the basic <code><a href="#topic+cosRow">cosRow</a>, <a href="#topic+cosCol">cosCol</a></code> and <code><a href="#topic+assocRow">assocRow</a>, <a href="#topic+assocCol">assocCol</a></code> functions. The <code>sim</code> functions take a dataframe as input, internally calling <code>splitTable</code> to turn the dataframe into sparse matrices, and then applying sparse matrix algebra to efficiently compute similarities. Currently only a few exemplary methods are encoded.
</p>
<p><code>sim.att</code> computes similarities between the different nominal variables. The method <code>chuprov</code> computes Chuprov's T (very similar to Cramer's V, but easier to compute efficiently). The method <code>g</code> computes the G-test from Sokal and Rohlf (1982), also known as Dunning's G from Dunning (1993). This G is closely related to Mutual Information (G = 2*N*MI, with N being the sample size). The method <code>mutual</code> returns the mutual information, and the method <code>variation</code> returns the so-called &lsquo;variation of information&rsquo; (join information - mutual information). Note that the this last one is a metric, not a similarity. All these methods can be abbreviated, e.g use &quot;c&quot;, &quot;g&quot;, &quot;m&quot;, and &quot;v&quot;.
</p>
<p><code>sim.obs</code> computes similarities between the different observation for the nominal variables. The method <code>hamming</code> computes the relative Hamming similarity, i.e. the number of similarities devided by the number of comparisons made (Goebl 1984 calls this the &lsquo;Relativer Identitaetswert&rsquo;). The method <code>weighted</code> uses an inverse square root weighting on all similarities, i.e. rare similarities count more. This is very similar to Goebl's &lsquo;Gewichteter Identitaetswert&rsquo;, though note that his definition is slightly different from the one used here. Further, all methods as defined for <code><a href="#topic+assocSparse">assocSparse</a></code> can be used here, i.e. <code>res, pmi, wpmi, poi</code>, and new methods can be defined according to the explanations as <code>assocSparse</code>.
</p>


<h3>Value</h3>

<p>All methods return symmetric similarity matrices in the form <code>dsCMatrix</code>, only specifying the upper triangle. The only exception is when <code>sparse=T</code> is chose, then the result will be in the form <code>dsyMatrix</code>.
</p>


<h3>Note</h3>

<p>Note that these methods automatically take missing data into account. They also work with large amount of missing data, but of course the validity of any similarity with much missing data is problematic.
</p>
<p>The <code>sim.att</code> and <code>sim.obs</code> methods by default use sparse computations, which leads (among other effects) to errors on the diagonal. The main diagonal should be one everywhere by definition, but this will only be the case with the option <code>sparse = F</code>. The deviations with <code>sparse = T</code> should be minimal in the non-diagonal entries, but computations should be faster, and the results often take up less space. 
</p>


<h3>Author(s)</h3>

<p>Michael Cysouw
</p>


<h3>References</h3>

<p>Goebl, Hans. 1984. <em>Dialektometrische Studien: anhand italoromanischer, raetoromanischer und galloromanischer Sprachmaterialien aus AIS und AFL.</em> (Beihefte zur Zeitschrift fuer Romanische Philologie). Tuebingen: Niemeyer.
</p>
<p>Dunning, Ted. 1993. Accurate methods for the statistics of surprise and coincidence. <em>Computational linguistics</em> 19(1). 61-74.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># first a simple example using the farms-dataset from MASS
library(MASS)

# similarities between farms
s &lt;- sim.obs(farms)
plot(hclust(as.dist(1-s), method = "ward.D"))

# similarities between attributes (`variables`)
s &lt;- sim.att(farms)
plot(hclust(as.dist(1-s), method = "ward.D"))

# use the split option for multi-valued cells
farms2 &lt;- as.matrix(farms)
farms2[1,1] &lt;- "M1,M5"

s &lt;- sim.obs(farms2, split = ",")
plot(hclust(as.dist(1-s), method = "ward.D"))

# select only the 168 language from wals with more than 80 datapoints
data(wals)
sel &lt;- wals$data[apply(wals$data,1,function(x){sum(!is.na(x))})&gt;80,]

# compare different similarities
w &lt;- sim.obs(sel, "weighted")
h &lt;- sim.obs(sel, "hamming")
r &lt;- sim.obs(sel, "res")
p &lt;- sim.obs(sel, "poi")
m &lt;- sim.obs(sel, "wpmi")
i &lt;- sim.obs(sel, "pmi")

pairs(~ as.dist(w) + as.dist(h) + as.dist(r) + as.dist(p) + as.dist(m) + as.dist(i),
	labels = c("weighted","hamming","residuals","poisson","weighted PMI","PMI"))


# a larger example with lots of missing data: the WALS-data as included here
# computations go reasonably quick
# (on 2566 observations and 131 attributes with 630 different values in total)
data(wals)
system.time(s &lt;- sim.att(wals$data))
rownames(s) &lt;- colnames(wals$data)
plot(hclust(as.dist(1-s), method = "ward.D"), cex = 0.5)

# Note that using sparse=T speeds up computations because it 
# ignores zero co-occurrences
system.time(
	chup.sparse &lt;- sim.att(wals$data,method = "chuprov", sparse = TRUE)
)

# some more similarities on the attributes
g &lt;- sim.att(wals$data, method = "g") # Dunning's G
m &lt;- sim.att(wals$data, method = "mutual") # Mutual Information
v &lt;- sim.att(wals$data, method = "variation") # Variation of Information

# Note the strong differences between these approaches
pairs(~ as.dist(chup.sparse) + as.dist(m) + as.dist(g) + as.dist(v),
	labels=c("Chuprov's T","Mutual Information","G-statistic","Variation of Information"))
	
# Relative Hamming similarity on all observations (languages) in WALS
# time is not a problem, but the data is so sparse
# that for many language-pairs there is no shared data
system.time( s &lt;- sim.obs(wals$data))

</code></pre>

<hr>
<h2 id='sim.strings'>
String similarity by cosine similarity between bigram vectors
</h2><span id='topic+sim.strings'></span>

<h3>Description</h3>

<p>Efficient computation of pairwise string similarities using a cosine similarity on bigram vectors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sim.strings(strings1, strings2 = NULL, sep = "", boundary = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sim.strings_+3A_strings1">strings1</code>, <code id="sim.strings_+3A_strings2">strings2</code></td>
<td>

<p>Vector with strings to be compared, will be treated <code>as.character</code>. When only <code>strings1</code> is provided, all pairwise similarities between its elements are computed. When two different input vectors are provided, the pairwise similarities between all elements from the first and the second vector are computed.
</p>
</td></tr>
<tr><td><code id="sim.strings_+3A_sep">sep</code></td>
<td>

<p>Separator used to split the strings into parts. This will be passed to <code><a href="base.html#topic+strsplit">strsplit</a></code> internally, so there is no fine-grained control possible on the splitting. If it is important to get the splitting exactly right, consider pre-processing the splitting by inserting a special symbol on the split-positions, and then choosing here to split by this special symbol.
</p>
</td></tr>
<tr><td><code id="sim.strings_+3A_boundary">boundary</code></td>
<td>

<p>In the default setting <code>boundary = T</code>, a special symbol is added to the front and to the end of each string, adding special bigrams for the initial and the final character. With words from real languages (which are mostly not very long) this has a strong impact.
</p>
</td></tr>
<tr><td><code id="sim.strings_+3A_...">...</code></td>
<td>

<p>Further arguments passed to <code><a href="#topic+splitStrings">splitStrings</a></code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The strings are converted into sparse matrices by <code><a href="#topic+splitStrings">splitStrings</a></code>, and then <code><a href="#topic+assocSparse">assocSparse</a></code> computes a cosine similarity on the bigram vectors. Only the option of bigrams is currently used, because for long lists of real words from a real language this seems to be an optimal tradeoff between speed and useful similarity.
</p>


<h3>Value</h3>

<p>When either <code>length(strings1) == 1</code> or <code>length(strings2) == 1</code>, the result will be a normal vector with similarities between 0 and 1.
</p>
<p>When both the input vectors are longer than 1, then the result will be a sparse matrix with similarities. When only <code>strings1</code> is provided, then the result is of type <code>dsCMatrix</code>. When two input vectors are provided, the result is of type <code>dgCMatrix</code>.
</p>


<h3>Note</h3>

<p>The overhead of converting the strings into sparse matrices makes this function not optimal for small datasets. For large datasets the time of the conversion is negligible compared to the actual similarity computation, and then this approach becomes very worthwhile, because fast, and based on sparse matrix computation, that can be sped up by multicore processing in the future.
</p>
<p>The result of <code>sim.strings(a)</code> and <code>sim.strings(a,a)</code> is identical, but the first version is more efficient, both as to processing time, as well as to the size of the resulting objects.
</p>


<h3>Note</h3>

<p>There is a bash-executable <code>simstrings</code> distributed with this package (based on the <code>docopt</code> package) that let you use this function directly in a bash-terminal. The easiest way to use this executable is to softlink the executable to some directory in your bash PATH, for example <code>/usr/local/bin</code> or simply <code>~/bin</code>. To softlink the function <code>sim.strings</code> to this directory, use something like the following in your bash terminal:
</p>
<p><code>ln -is `Rscript -e 'cat(system.file("exec/simstrings", package="qlcMatrix"))'` ~/bin</code>
</p>
<p>From within R your can also use the following (again, optionally changing the linked-to directory from <code>~/bin</code> to anything more suitable on your system):
</p>
<p><code>file.symlink(system.file("exec/simstrings", package="qlcMatrix"), "~/bin")</code>
</p>


<h3>Author(s)</h3>

<p>Michael Cysouw
</p>


<h3>See Also</h3>

<p><code><a href="#topic+splitStrings">splitStrings</a>, <a href="#topic+cosSparse">cosSparse</a></code> on which this function is based. Compare with <code><a href="utils.html#topic+adist">adist</a></code> from the utils package. On large datasets, <code>sim.strings</code> seems to be about a factor 30 quicker. The package <code>stringdist</code> offers many more string comparison methods.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># ----- simple example -----

example &lt;- c("still","till","stable","stale","tale","tall","ill","all")
( sim &lt;- round( sim.strings(example), digits = 3) )



# show similarity in non-metric MDS
mds &lt;- MASS::isoMDS( as.dist(1-sim) )$points
plot(mds, type = "n", ann = FALSE, axes = FALSE)
text(mds, labels = example)

# ----- large example -----

# This similarity is meant to be used for large lists of wordforms.
# for example, all 15526 wordforms from the English Dalby Bible
# takes just a few seconds for the more than 1e8 pairwise comparisons
data(bibles)
words &lt;- splitText(bibles$eng)$wordforms
system.time( sim &lt;- sim.strings(words) )

# see most similar words
rownames(sim) &lt;- colnames(sim) &lt;- words
sort(sim["walk",], decreasing = TRUE)[1:10]

# just compare all words to "walk". This is the same as above, but less comparisons
# note that the overhead for the sparse conversion and matching of matrices is large
# this one is faster than doing all comparisons, but only be a factor 10
system.time( sim &lt;- sim.strings(words, "walk"))
names(sim) &lt;- words
sort(sim, decreasing = TRUE)[1:10]

# ----- comparison with Levinshtein -----

# don't try this with 'adist' from the utils package, it will take long!
# for a comparison, only take 2000 randomly selected strings: about a factor 20 slower
w &lt;- sample(words, 2000)
system.time( sim1 &lt;- sim.strings(w) )
system.time( sim2 &lt;- adist(w) )

# compare the current approach with relative levenshtein similarity
# = number of matches / ( number of edits + number of matches)
# for reasons of speed, just take 1000 random words from the english bible
w &lt;- sample(words, 1000)
sim1 &lt;- sim.strings(w)
tmp &lt;- adist(w, counts = TRUE)
sim2 &lt;- 1- ( tmp / nchar(attr(tmp, "trafos")) )

# plotting relation between the two 'heatmap-style'
# not identical, but usefully similar
image( log(table(
		round(as.dist(sim1) / 3, digits = 2) * 3,
		round(as.dist(sim2) / 3, digits = 2) * 3 )),
	xlab = "bigram similarity", ylab = "relative Levenshtein similarity")

</code></pre>

<hr>
<h2 id='sim.wordlist'>
Similarity matrices from wordlists
</h2><span id='topic+sim.wordlist'></span><span id='topic+sim.con'></span><span id='topic+sim.lang'></span><span id='topic+sim.graph'></span>

<h3>Description</h3>

<p>A few different approaches are implemented here to compute similarities from wordlists. <code>sim.lang</code> computes similarities between languages, assuming a harmonized orthography (i.e. symbols can be equated across languages). <code>sim.con</code> computes similarities between concepts, using only language-internal similarities. <code>sim.graph</code> computes similarities between graphemes (i.e. language-specific symbols) between languages, as a crude approximation of regular sound correspondences.
</p>
<p>WARNING: All these methods are really very crude! If they seem to give expected results, then this should be a lesson to rethink more complex methods proposed in the literature. However, in most cases the methods implemented here should be taken as a proof-of-concept, showing that such high-level similarities can be computed efficiently for large datasets. For actual research, I strongly urge anybody to adapt the current methods, and fine-tune them as needed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sim.lang(wordlist, 
	doculects = "DOCULECT", concepts = "CONCEPT", counterparts = "COUNTERPART",
	method = "parallel", assoc.method =  res, weight = NULL, sep = "")

sim.con(wordlist,
	doculects = "DOCULECT", concepts = "CONCEPT", counterparts = "COUNTERPART",
	method = "bigrams", assoc.method = res, weight = NULL, sep = "")

sim.graph(wordlist,
	doculects = "DOCULECT", concepts = "CONCEPT", counterparts = "TOKENS",
	method = "cooccurrence", assoc.method = poi, weight = NULL, sep = " ")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sim.wordlist_+3A_wordlist">wordlist</code></td>
<td>

<p>Dataframe or matrix containing the wordlist data. Should have at least columns corresponding to languages (DOCULECT), meanings (CONCEPT) and translations (COUNTERPART). 
</p>
</td></tr>
<tr><td><code id="sim.wordlist_+3A_doculects">doculects</code>, <code id="sim.wordlist_+3A_concepts">concepts</code>, <code id="sim.wordlist_+3A_counterparts">counterparts</code></td>
<td>

<p>The name (or number) of the column of <code>wordlist</code> in which the respective information is to be found. The defaults are set to coincide with the naming of the example dataset included in this package. See <code><a href="#topic+huber">huber</a></code>.
</p>
</td></tr>
<tr><td><code id="sim.wordlist_+3A_method">method</code></td>
<td>

<p>Specific approach for the computation of the similarities. See Details below.
</p>
</td></tr>
<tr><td><code id="sim.wordlist_+3A_assoc.method">assoc.method</code>, <code id="sim.wordlist_+3A_weight">weight</code></td>
<td>

<p>Measures to be used internally (passed on to <code>assocSparse</code> or <code>cosSparse</code>). See Details below.
</p>
</td></tr>
<tr><td><code id="sim.wordlist_+3A_sep">sep</code></td>
<td>
<p>Separator to be used to split strings. See <code>link{splitStrings}</code> for details.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The following methods are currently implemented (all methods can be abbreviated):
</p>
<p>For <code>sim.lang</code>:
</p>

<dl>
<dt><code>global</code>:</dt><dd><p>Global bigram similarity, i.e. ignoring the separation into concepts, and simply taking the bigram vector of all words per language. Probably best combined with <code>weight = idf</code>.</p>
</dd>
<dt><code>parallel</code>:</dt><dd><p>By default, computes a parallel bigram similarity, i.e. splitting the bigram vectors per language and per concepts, and then simply making one long vector per language from all individual concept-bigram vectors. This approach seems to be very similar (if not slightly better) than the widespread &lsquo;average Levenshtein&rsquo; distance.</p>
</dd>
</dl>

<p>For <code>sim.con</code>:
</p>

<dl>
<dt><code>colexification</code>:</dt><dd><p>Simply count the number of languages in which two concepts have at least one complete identical translations. No normalization is attempted, and <code>assoc.method</code> and <code>weight</code> are ignored (internally this just uses <code>tcrossprod</code> on the <code>CW (concepts x words)</code> sparse matrix). Because no splitting of strings is necessary, this method is very quick.</p>
</dd>
<dt><code>global</code>:</dt><dd><p>Global bigram similarity, i.e. ignoring the separation into languages, and simply taking the bigram vector of all words per concept. Probably best combined with <code>weight = idf</code>.</p>
</dd>
<dt><code>bigrams</code>:</dt><dd><p>By default, compute the similarity between concepts by comparing bigraphs, i.e. language-specific bigrams. In that way, cross-linguistically recurrent partial similarities are uncovered. It is very interesting to compare this measure with <code>colexification</code> above.</p>
</dd>
</dl>
	
<p>For <code>sim.graph</code>:
</p>

<dl>
<dt><code>cooccurrence</code>:</dt><dd><p>Currently the only method implemented. Computes the co-occurrence statistics for all pair of graphemes (e.g. between symbol x from language L1 and symbol y from language L2). See Prokic &amp; Cysouw (2013) for an example using this approach.</p>
</dd>
</dl>

<p>All these methods (except for <code>sim.con(method = "colexification")</code>) use either <code><a href="#topic+assocSparse">assocSparse</a></code> or <code><a href="#topic+cosSparse">cosSparse</a></code> for the computation of the similarities. For the different measures available, see the documentation there. Currently implemented are <code>res, poi, pmi, wpmi</code> for <code>assocSparse</code> and <code>idf, isqrt, none</code> for <code>cosWeight</code>. It is actually very easy to define your own measure.  
</p>
<p>When <code>weight = NULL</code>, then <code>assocSparse</code> is used with the internal method as specified in <code>assoc.method</code>. When <code>weight</code> is specified, then <code>cosSparse</code> is used with an Euclidean norm and the weighting as specified in <code>weight</code>. When <code>weight</code> is specified, and specification of <code>assoc.method</code> is ignored.
</p>


<h3>Value</h3>

<p>A sparse similarity matrix of class <code>dsCMatrix</code>. The magnitude of the actual values in the matrices depend strongly on the methods chosen.
</p>
<p>With <code>sim.graph</code> a list of two matrices is returned.
</p>
<table role = "presentation">
<tr><td><code>GG</code></td>
<td>
<p>The grapheme by grapheme similarity matrix of class <code>dsCMatrix</code></p>
</td></tr>
<tr><td><code>GD</code></td>
<td>
<p>A pattern matrix of class  indicating which grapheme belongs to which language.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michael Cysouw
</p>


<h3>References</h3>

<p>Prokic, Jelena and Michael Cysouw. 2013. Combining regular sound correspondences and geographic spread. <em>Language Dynamics and Change</em> 3(2). 147&ndash;168.
</p>


<h3>See Also</h3>

<p>Based on <code><a href="#topic+splitWordlist">splitWordlist</a></code> for the underlying conversion of the wordlist into sparse matrices. The actual similarities are mostly computed using <code><a href="#topic+assocSparse">assocSparse</a></code> or <code><a href="#topic+cosSparse">cosSparse</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># ----- load data -----

# an example wordlist, see help(huber) for details
data(huber)

# ----- similarity between languages -----

# most time is spend splitting the strings
# the rest does not really influence the time needed
system.time( sim &lt;- sim.lang(huber, method = "p") )

# a simple distance-based UPGMA tree

## Not run: 
# note non-ASCII characters in data might lead to plot errors on some platforms
plot(hclust(as.dist(-sim), method = "average"), cex = .7)

## End(Not run)

# ----- similarity between concepts -----

# similarity based on bigrams
system.time( simB &lt;- sim.con(huber, method = "b") )
# similarity based on colexification. much easier to calculate
system.time( simC &lt;- sim.con(huber, method = "c") )

# As an example, look at all adjectival concepts
adj &lt;- c(1,5,13,14,28,35,40,48,67,89,105,106,120,131,137,146,148,
	171,179,183,188,193,195,206,222,234,259,262,275,279,292,
	294,300,309,341,353,355,359)

# show them as trees

## Not run: 
# note non-ASCII characters in data might lead to plot errors on some platforms
oldpar&lt;-par("mfrow")
par(mfrow = c(1,2)) 
plot(hclust(as.dist(-simB[adj,adj]), method = "ward.D2"), 
	cex = .5, main = "bigrams")
plot(hclust(as.dist(-simC[adj,adj]), method = "ward.D2"), 
	cex = .5, main = "colexification")
par(mfrow = oldpar)

## End(Not run)

# ----- similarity between graphemes -----

# this is a very crude approach towards regular sound correspondences
# when the languages are not too distantly related, it works rather nicely 
# can be used as a quick first guess of correspondences for input in more advanced methods

# all 2080 graphemes in the data by all 2080 graphemes, from all languages
system.time( X &lt;- sim.graph(huber) )

# throw away the low values
# select just one pair of languages for a quick visualisation
X$GG &lt;- drop0(X$GG, tol = 1)
colnames(X$GG) &lt;- rownames(X$GG)
correspondences &lt;- X$GG[X$GD[,"bora"],X$GD[,"muinane"]]

## Not run: 
# note non-ASCII characters in data might lead to plot errors on some platforms
heatmap(as.matrix(correspondences))

## End(Not run)
</code></pre>

<hr>
<h2 id='sim.words'>
Similarity-measures for words between two languages, based on co-occurrences in parallel text
</h2><span id='topic+sim.words'></span>

<h3>Description</h3>

<p>Based on co-occurrences in a parallel text, this convenience function (a wrapper around various other functions from this package) efficiently computes something close to translational equivalence.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sim.words(text1, text2 = NULL, method = res, weight = NULL, 
	lowercase = TRUE, best = FALSE, tol = 0)</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sim.words_+3A_text1">text1</code>, <code id="sim.words_+3A_text2">text2</code></td>
<td>

<p>Vectors of strings representing sentences. The names of the vectors should contain IDs that identify the parallelism between the two texts. If there are no specific names, the function assumes that the two vectors are perfectly parallel. Within the strings, wordforms are simply separated based on spaces (i.e. everything between two spaces is a wordform). For more details about the format-assumptions, see <code><a href="#topic+splitText">splitText</a></code>, which is used internally here.
</p>
</td></tr>
<tr><td><code id="sim.words_+3A_method">method</code></td>
<td>

<p>Method to be used as a co-occurrence statistic. See <code><a href="#topic+assocSparse">assocSparse</a></code> for a detailed presentation of the available methods. It is possible to define your own statistic, when it can be formulated as a function of observed and expected frequencies.
</p>
</td></tr>
<tr><td><code id="sim.words_+3A_weight">weight</code></td>
<td>

<p>When <code>weight</code> is specified, the function <code><a href="#topic+cosSparse">cosSparse</a></code> is used for the co-occurrence statistics (with a Euclidean normalization, i.e. <code>norm2</code>). The specified weight function will be used, currently <code>idf</code>, <code>sqrt</code>, and <code>none</code> are available. For more details, and for instructions how to formulate your own weight function, see the discussion at <code><a href="#topic+cosSparse">cosSparse</a></code>. When <code>weight</code> is specified, any specification of <code>method</code> is ignored.
</p>
</td></tr>
<tr><td><code id="sim.words_+3A_lowercase">lowercase</code></td>
<td>

<p>Should all words be turned into lowercase? See <code><a href="#topic+splitText">splitText</a></code> for discussion how this is implemented.
</p>
</td></tr>
<tr><td><code id="sim.words_+3A_best">best</code></td>
<td>

<p>When <code>best = T</code>, an additional sparse matrix is returned with a (simplistic) attempt to find the best translational equivalents between the texts.
</p>
</td></tr>
<tr><td><code id="sim.words_+3A_tol">tol</code></td>
<td>

<p>Tolerance: remove all values between <code>-tol</code> and <code>+tol</code> in the result. Low values can mostly be ignored for co-occurrence statistics without any loss of information. However, what is considered &lsquo;low&rsquo; depends on the methods used to calculate the statistics. See discussion below.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Care is taken in this function to match multiple verses that are translated into one verse, see <code><a href="#topic+bibles">bibles</a></code> for a survey of the encoding assumptions taken here.
</p>
<p>The parameter <code>method</code> can take anything that is also available for <code><a href="#topic+assocSparse">assocSparse</a></code>. Similarities are computed using that function.
</p>
<p>When <code>weight</code> is specified, the similarities are computed using <code><a href="#topic+cosSparse">cosSparse</a></code> with default setting of <code>norm = norm2</code>. All available weights can also be used here.
</p>
<p>The option <code>best = T</code> uses <code><a href="#topic+rowMax">rowMax</a></code> and <code><a href="#topic+colMax">colMax</a></code>. This approach to get the &lsquo;best&rsquo; translation is really crude, but it works reasonably well with one-to-one and many-to-one situations. This option takes rather a lot more time to finish, as row-wise maxima for matrices is not trivial to optimize. Consider raising <code>tol</code>, as this removes low values that won't be important for the maxima anyway. See examples below.
</p>
<p>Guidelines for the value of <code>tol</code> are difficult to give, as it depends on the method used, but also on the distribution of the data (i.e. the number of sentences, and the frequency distribution of the words in the text). Some suggestions:
</p>

<ul>
<li><p>when <code>weight</code> is specified, results range between -1 and +1. Then <code>tol = 0.1</code> should never lead to problems, but often even <code>tol = 0.3</code> or higher will lead to identical results.

</p>
</li>
<li><p>when <code>weight</code> is not specified (i.e. <code>assocSparse</code> will be used), then results range between <code>-inf</code> and <code>+inf</code>, so the tolerance is more problematic. In general, <code>tol = 2</code> seems to be unproblematic. Higher tolerance, e.g. <code>tol = 10</code> can be used to find the &lsquo;obvious&rsquo; translations, but you will loose some of the more incidental co-occurrences.

</p>
</li></ul>



<h3>Value</h3>

<p>When <code>best = F</code>, a single sparse matrix is returned of type <code>dgCMatrix</code> with the values of the statistic chosen. All unique wordforms of text1 are included as row names, and those from text2 as column names.
</p>
<p>When <code>best = T</code>, a list of two sparse matrices is returned:
</p>
<table role = "presentation">
<tr><td><code>sim</code></td>
<td>
<p>the same matrix as above</p>
</td></tr>
<tr><td><code>best</code></td>
<td>
<p>a sparse pattern matrix of type <code>ngCMatrix</code> with the same dimensions as the previous matrix. Only the &lsquo;best&rsquo; translations between the two languages are marked</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michael Cysouw
</p>


<h3>References</h3>

<p>Mayer, Thomas and Michael Cysouw. 2012. Language comparison through sparse multilingual word alignment. <em>Proceedings of the EACL 2012 Joint Workshop of LINGVIS &amp; UNCLH</em>, 54&ndash;62. Avignon: Association for Computational Linguistics.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+splitText">splitText</a></code>, <code><a href="#topic+assocSparse">assocSparse</a></code> and <code><a href="#topic+cosSparse">cosSparse</a></code> are the central parts of this function. Also check <code><a href="#topic+rowMax">rowMax</a></code>, which is used to extract the &lsquo;best&rsquo; translations.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(bibles)

# ----- small example of co-occurrences -----

# as an example, just take partially overlapping parts of two bibles
# sim.words uses the names to get the paralellism right, so this works
eng &lt;- bibles$eng[1:5000]
deu &lt;- bibles$deu[2000:7000]
sim &lt;- sim.words(eng, deu, method = res)

# but the statistics are not perfect (because too little data)
# sorted co-occurrences for the english word "your" in German:
sort(sim["your",], decreasing = TRUE)[1:10]


# ----- complete example of co-occurrences -----

# running the complete bibles takes a bit more time (but still manageable)
system.time(sim &lt;- sim.words(bibles$eng, bibles$deu, method = res))

# results are much better
# sorted co-occurrences for the english word "your" in German:
sort(sim["your",], decreasing = TRUE)[1:10]


# ----- look for 'best' translations -----

# note that selecting the 'best' takes even more time
system.time(sim2 &lt;- sim.words(bibles$eng, bibles$deu, method = res, best = TRUE))

# best co-occurrences for the English word "your"
which(sim2$best["your",])

# but can be made faster by removing low values
# (though the boundary in \code{tol =  5} depends on the method used
system.time(sim3 &lt;- sim.words(bibles$eng, bibles$deu, best = TRUE, method = res, tol = 5))

# note that the decision on the 'best' remains the same here
all.equal(sim2$best, sim3$best)


# ----- computations also work with other languages -----

# All works completely language-independent
# translations for 'we' in Tagalog:
sim &lt;- sim.words(bibles$eng, bibles$tgl, best = TRUE, weight = idf, tol = 0.1)
which(sim$best["we",])

</code></pre>

<hr>
<h2 id='splitStrings'>
Construct unigram and bigram matrices from a vector of strings
</h2><span id='topic+splitStrings'></span>

<h3>Description</h3>

<p>A (possibly large) vector of strings is separated into sparse pattern matrices, which allows for efficient computation on the strings.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>splitStrings(strings, sep = "", bigrams = TRUE, boundary = TRUE,
	bigram.binder = "", gap.symbol = "\u2043", left.boundary = "#",
	right.boundary = "#", simplify = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="splitStrings_+3A_strings">strings</code></td>
<td>

<p>Vector of strings to be separated into sparse matrices
</p>
</td></tr>
<tr><td><code id="splitStrings_+3A_sep">sep</code></td>
<td>

<p>Separator used to split the strings into parts. This will be passed to <code><a href="base.html#topic+strsplit">strsplit</a></code> internally, so there is no fine-grained control possible over the splitting. If it is important to get the splitting exactly right, consider pre-processing the splitting by inserting a special symbol on the split-positions, and then choosing to split by this specific symbol.
</p>
</td></tr>
<tr><td><code id="splitStrings_+3A_bigrams">bigrams</code></td>
<td>

<p>By default, both unigrams and bigrams are computer. If bigrams are not needed, setting <code>bigrams = F</code> will save on resources. 	
</p>
</td></tr>
<tr><td><code id="splitStrings_+3A_boundary">boundary</code></td>
<td>

<p>Should a start symbol and a stop symbol be added to each string? This will only be used for the determination of bigrams, and will be ignored if <code>bigrams = F</code>.
</p>
</td></tr>
<tr><td><code id="splitStrings_+3A_bigram.binder">bigram.binder</code></td>
<td>

<p>Only when <code>bigrams = T</code>. What symbol(s) should occur between the two parts of the bigram?
</p>
</td></tr>
<tr><td><code id="splitStrings_+3A_gap.symbol">gap.symbol</code></td>
<td>

<p>Only when <code>bigram = T</code>. What symbol should be included to separate the strings? It defaults to U+2043 <code>HYPHEN BULLET</code> on the assumption that this character will not often be included in data. See <code><a href="#topic+pwMatrix">pwMatrix</a></code> for some more explanation about the necessity of this gap symbol.
</p>
</td></tr>
<tr><td><code id="splitStrings_+3A_left.boundary">left.boundary</code>, <code id="splitStrings_+3A_right.boundary">right.boundary</code></td>
<td>

<p>Symbols to be used as boundaries, only used when <code>boundary = T</code>.
</p>
</td></tr>
<tr><td><code id="splitStrings_+3A_simplify">simplify</code></td>
<td>

<p>By default, various vectors and matrices are returned. However, when <code>simplify = T</code>, only a single sparse matrix is returned. See Value.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>By default, the output is a list of six elements:
</p>
<table role = "presentation">
<tr><td><code>segments</code></td>
<td>

<p>A vector with all splitted parts (i.e. all tokens) in order of occurrence, separated between the original strings with gap symbols.
</p>
</td></tr>
<tr><td><code>unigrams</code></td>
<td>

<p>A vector with all unique parts occuring in the segments.
</p>
</td></tr>
<tr><td><code>bigrams</code></td>
<td>

<p>Only present when <code>bigrams = T</code>. A vector with all unique bigrams.
</p>
</td></tr>
<tr><td><code>SW</code></td>
<td>

<p>A sparse pattern matrix of class <code>ngCMatrix</code> specifying the distribution of segments (S) over the original strings (W, think &lsquo;words&rsquo;). This matrix is only interesting in combination with the following matrices.
</p>
</td></tr>
<tr><td><code>US</code></td>
<td>

<p>A sparse pattern matrix of class <code>ngCMatrix</code> specifying the distribution of the unique unigrams (U) over the tokenized segments (S).
</p>
</td></tr>
<tr><td><code>BS</code></td>
<td>

<p>Only present when <code>bigrams = T</code>. A sparse pattern matrix of class <code>ngCMatrix</code> specifying the distribution of the unique bigrams (B) over the tokenized segments (S)
</p>
</td></tr>
</table>
<p>When <code>simplify = T</code> the output is a single sparse matrix of class <code>dgCMatrix</code>. This is basically BS %8% SW (when <code>bigrams = T</code>) or US %*% SW (when <code>bigrams = F</code>) with rows and column names added into the matrix.
</p>


<h3>Note</h3>

<p>Because of some internal idiosyncrasies, the ordering of the bigrams is first by second element, and then by first element. This might change in future versions.
</p>


<h3>Author(s)</h3>

<p>Michael Cysouw
</p>


<h3>See Also</h3>

<p><code><a href="#topic+sim.strings">sim.strings</a></code> is a convenience function to quickly compute pairwise strings similarities, based on <code>splitStrings</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># a simple example to see the function at work
example &lt;- c("this","is","an","example")
splitStrings(example)
splitStrings(example, simplify = TRUE)


# a bit larger, but still quick and efficient
# taking 15526 wordforms from the English Dalby Bible and splitting them into bigrams
data(bibles)
words &lt;- splitText(bibles$eng)$wordforms
system.time( S &lt;- splitStrings(words, simplify = TRUE) )

# and then taking the cosine similarity between the bigram-vectors for all word pairs
system.time( sim &lt;- cosSparse(S) )

# most similar words to "father"
sort(sim["father",], decreasing = TRUE)[1:20]

</code></pre>

<hr>
<h2 id='splitTable'>
Construct sparse matrices from a nominal matrix/dataframe
</h2><span id='topic+splitTable'></span>

<h3>Description</h3>

<p>This function splits a matrix or dataframe into two sparse matrices: an incidence and an index matrix. The incidence matrix links the observations (rows) to all possible values that occur in the original matrix. The index matrix links the values to the attributes (columns). This encoding allows for highly efficient calculations on nominal data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>splitTable(data, 
    attributes = colnames(data), observations = rownames(data),
		name.binder = ":", split = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="splitTable_+3A_data">data</code></td>
<td>

<p>a matrix (or data.frame) with observations as rows and nominal attributes as columns. Numerical values in the data will be interpreted as classes (i.e. as nominal data, aka categorical data). 
</p>
</td></tr>
<tr><td><code id="splitTable_+3A_attributes">attributes</code>, <code id="splitTable_+3A_observations">observations</code></td>
<td>

<p>The row names and column names of the data will by default be extracted from the input matrix. However, in special situations they can be added separately. Note that names of the attributes (&lsquo;column names&rsquo;) are needed for the production of unique value names. In case of absent column names, new column names of the form &lsquo;X1&rsquo; are automatically generated.
</p>
</td></tr>
<tr><td><code id="splitTable_+3A_name.binder">name.binder</code></td>
<td>

<p>Character string to be added between attribute names and value names. Defaults to colon &lsquo;:&rsquo;.
</p>
</td></tr>
<tr><td><code id="splitTable_+3A_split">split</code></td>
<td>

<p>Character string to split values in each cell of the table, e.g. a comma or semicolon.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing the various row and column names, and the two sparse pattern matrices of format <code>ngCMatrix</code>:
</p>
<table role = "presentation">
<tr><td><code>attributes</code></td>
<td>
<p>vector of attribute names</p>
</td></tr>
<tr><td><code>values</code></td>
<td>
<p>vector of unique value names</p>
</td></tr>
<tr><td><code>observations</code></td>
<td>
<p>vector of observation names</p>
</td></tr>
<tr><td><code>OV</code></td>
<td>
<p>sparse pattern matrix with observations as rows (O) and values as columns (V)</p>
</td></tr>
<tr><td><code>AV</code></td>
<td>
<p>sparse pattern matrix with attributes as rows (A) and values as columns (V)</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Input of data as a matrix or as a data.frame might lead to different ordering of the values because collation differs per locale (see the discussion at <code><a href="#topic+ttMatrix">ttMatrix</a></code>, which does the heavy lifting here).
</p>
<p>The term &lsquo;attribute&rsquo; is used in instead of the more common term &lsquo;variable&rsquo; to allow  for the capital A to uniquely identify attributes and V to identify values.
</p>


<h3>Author(s)</h3>

<p>Michael Cysouw
</p>


<h3>See Also</h3>

<p>More methods to use such split tables can be found at <code><a href="#topic+sim.nominal">sim.nominal</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# start with a simple example from the MASS library
# compare the original data with the encoding as sparse matrices
library(MASS)
farms
splitTable(farms)

# As a more involved example, consider the WALS data included in this package
# Transforming the reasonably large WALS data.frame \code{wals$data} is fast
# (2566 observations, 131 attributes, 630 unique values)
# The function `str' gives a useful summary of the result of the splitting
data(wals)
system.time(W &lt;- splitTable(wals$data))
str(W) 

# Some basic use examples on the complete WALS data.
# The OV-matrix can be used to quickly count the number of similarities 
# between all pairs of observations. Note that with the large amount of missing values
# the resulting numbers are not really meaningfull. Some normalisation is necessary.
system.time( O &lt;- tcrossprod(W$OV*1) )
O[1:10,1:10]

# The number of comparisons available for each pair of attributes
system.time( N &lt;- crossprod(tcrossprod(W$OV*1, W$AV*1)) )
N[1:10,1:10]


# compute the number of available datapoints per observation (language) in WALS
# once the sparse matrices W are computed, such calculations are much quicker than 'apply'
system.time( avail1 &lt;- rowSums(W$OV) )
system.time( avail2 &lt;- apply(wals$data,1,function(x){sum(!is.na(x))}))
names(avail2) &lt;- NULL
all.equal(avail1, avail2)

</code></pre>

<hr>
<h2 id='splitText'>
Construct sparse matrices from parallel texts
</h2><span id='topic+splitText'></span><span id='topic+read.text'></span>

<h3>Description</h3>

<p>Convenience functions to read parallel texts and to split parallel texts into sparse matrices.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>splitText(text, globalSentenceID = NULL, localSentenceID = names(text), sep = " ",
	simplify = FALSE, lowercase = TRUE)

read.text(file)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="splitText_+3A_text">text</code></td>
<td>

<p>vector of strings, typically sentences with wordforms separated by space (see <code>sep</code> below). names of the vector elements are typically IDs to link across texts (cf. the format as used in <code><a href="#topic+bibles">bibles</a></code>).
</p>
</td></tr>
<tr><td><code id="splitText_+3A_globalsentenceid">globalSentenceID</code></td>
<td>

<p>Vector of all IDs that might possibly occur in the parallel texts, used to parallelize the texts. Can for example be constructed by using <code>union</code> on the localSentenceIDs.
</p>
</td></tr>
<tr><td><code id="splitText_+3A_localsentenceid">localSentenceID</code></td>
<td>

<p>Vector of the IDs for the actual sentences in the present text. Typically present as names of the text.
</p>
</td></tr>
<tr><td><code id="splitText_+3A_sep">sep</code></td>
<td>

<p>Separator on which the sentences should be parsed into wordforms. The implementation is very simple here, there are no advanced options for guessing punctuation. The variation in punctuation across a wide variety of languages and scripts normally turns out to be too large to be easily automatically parsed. Any advanced parsing has to be done externally, and here simply the parsed symbol is used to actually split the text into parts. Typically, this parsing of sentences into wordforms will be performed using space <code>sep = " "</code>. See also <code><a href="#topic+bibles">bibles</a></code> for some examples of such pre-parsing.
</p>
</td></tr>
<tr><td><code id="splitText_+3A_simplify">simplify</code></td>
<td>

<p>By default (when <code>simplify = F</code>), this function returns a list of objects that represent the encoding of the text into sparse matrices. With <code>simplify = T</code> this list is reduced to a single matrix (wordforms x globalSentenceID), with the actual wordforms as row names.
</p>
</td></tr>
<tr><td><code id="splitText_+3A_lowercase">lowercase</code></td>
<td>

<p>By default, a mapping between the text and a lowercase version of the same text. In the default output (with <code>simplify = F</code>), this is a sparse matrix linking strings with mixed upper/lower case to string with only lower case. Note that case folding is locale-specific, but here a simple universal case-folding is used (as available through <code><a href="base.html#topic+tolower">tolower</a></code>).
</p>
</td></tr>
<tr><td><code id="splitText_+3A_file">file</code></td>
<td>

<p>file name (or full path) for a file to be read.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function <code>splitText</code> is actually just a nice examples of how <code><a href="#topic+pwMatrix">pwMatrix</a></code>, <code><a href="#topic+jMatrix">jMatrix</a></code>, and <code><a href="#topic+ttMatrix">ttMatrix</a></code> can be used to work with parallel texts. 
</p>
<p>The function <code>read.text</code> is a convenience function to read parallel texts.
</p>


<h3>Value</h3>

<p>When <code>simplify = F</code>, a list is returned with the following elements:
</p>
<table role = "presentation">
<tr><td><code>runningWords</code></td>
<td>
<p>single vector with complete text (ignoring original sentence breaks), separated into strings according to <code>sep</code></p>
</td></tr>
<tr><td><code>wordforms</code></td>
<td>
<p>vector with all wordforms as attested in the text (according to the specified separator). Ordering of wordforms is done by <code><a href="#topic+ttMatrix">ttMatrix</a></code>, which by default uses the &quot;C&quot; collation locale.</p>
</td></tr>
<tr><td><code>lowercase</code></td>
<td>
<p>only returned when <code>lowercase = T</code>. Vector with all unique wordforms after conversion to lowercase.</p>
</td></tr>
<tr><td><code>RS</code></td>
<td>
<p>Sparse pattern matrix of class <code>ngCMatrix</code> with runningWords (R) as rows and sentence IDs (S) as columns. When <code>globalSentenceID = NULL</code>, then the sentences are the elements of the original text. Else, they are the specified globalSentenceIDs.</p>
</td></tr>
<tr><td><code>WR</code></td>
<td>
<p>Sparse pattern matrix of class <code>ngCMatrix</code> with wordforms (W) as rows and running words (R) as columns.</p>
</td></tr>
<tr><td><code>wW</code></td>
<td>
<p>only returned when <code>lowercase = T</code>. Sparse pattern matrix of class <code>ngCMatrix</code> linking between lowercased wordforms and original wordforms.</p>
</td></tr>
</table>
<p>When <code>simplify = T</code> the result is a single sparse Matrix (of type <code>dgCMatrix</code>) linking wordforms (either with or without case) to sentences (either global or local). Note that the result with options <code>(simplify = T, lowercase = F)</code> will result in the sparse matrix as available at paralleltext.info (there the matrix is in <code>.mtx</code> format), with the wordforms included into the matrix as row names. However, note that the resulting matrix from the code here will include frequencies for words that occur more than once per sentence. These have been removed for the <code>.mtx</code> version available online.
</p>


<h3>Author(s)</h3>

<p>Michael Cysouw
</p>


<h3>See Also</h3>

<p><code><a href="#topic+bibles">bibles</a></code> for some texts that led to the development of this function.
<code><a href="#topic+sim.words">sim.words</a></code> for a convenience function to easily extract possible translations equivalents through co-occurrence (using <code>splitText</code> for the data-preparation.)
</p>


<h3>Examples</h3>

<pre><code class='language-R'># a trivial examples to see the results of this function:
text &lt;- c("This is a sentence .","A sentence this is !","Is this a sentence ?")
splitText(text)
splitText(text, simplify = TRUE, lowercase = FALSE)

# reasonably quick with complete bibles (about 1-2 second per complete bible)
# texts with only New Testament is even quicker
data(bibles)
system.time(eng &lt;- splitText(bibles$eng, bibles$verses))
system.time(deu &lt;- splitText(bibles$deu, bibles$verses))

# Use example: Number of co-ocurrences between two bibles
# (this is more conveniently performed by the function sim.words)
# How often do words from the one language cooccur with words from the other language?
ENG &lt;- (eng$wW * 1) %*% (eng$WR * 1) %*% (eng$RS * 1)
DEU &lt;- (deu$wW * 1) %*% (deu$WR * 1) %*% (deu$RS * 1)
C &lt;- tcrossprod(ENG,DEU)
rownames(C) &lt;- eng$lowercase
colnames(C) &lt;- deu$lowercase
C[	c("father","father's","son","son's"),
	c("vater","vaters","sohn","sohne","sohnes","sohns")
	]

# Pure counts are not very interesting. This is better:
R &lt;- assocSparse(t(ENG), t(DEU))
rownames(R) &lt;- eng$lowercase
colnames(R) &lt;- deu$lowercase
R[	c("father","father's","son","son's"),
	c("vater","vaters","sohn","sohne","sohnes","sohns")
	]

# For example: best co-occurrences for the english word "mine"
sort(R["mine",], decreasing = TRUE)[1:10]


# To get a quick-and-dirty translation matrix:
# adding maxima from both sides work quite well
# but this takes some time

cm &lt;- colMax(R, which = TRUE, ignore.zero = FALSE)$which
rm &lt;- rowMax(R, which = TRUE, ignore.zero = FALSE)$which
best &lt;- cm + rm
best &lt;- as(best, "nMatrix")

which(best["your",])
which(best["went",])

# A final speed check:
# split all 4 texts, and simplify them into one matrix
# They have all the same columns, so they can be rbind
system.time(all &lt;- sapply(bibles[-1], function(x){splitText(x, bibles$verses, simplify = TRUE)}))
all &lt;- do.call(rbind, all)

# then try a single co-occerrence measure on all pairs from these 72K words
# (so we are doing about 2.6e9 comparisons here!)
system.time( S &lt;- cosSparse(t(all)) )

# this goes extremely fast! As long as everything fits into RAM this works nicely.
# Note that S quickly gets large
print(object.size(S), units = "auto")

# but most of it can be thrown away, because it is too low anyway
# this leads to a factor 10 reduction in size:
S &lt;- drop0(S, tol = 0.2)
print(object.size(S), units = "auto")

</code></pre>

<hr>
<h2 id='splitWordlist'>
Construct sparse matrices from comparative wordlists (aka &lsquo;Swadesh list&rsquo;)
</h2><span id='topic+splitWordlist'></span>

<h3>Description</h3>

<p>A comparative wordlist (aka &lsquo;Swadesh list&rsquo;) is a collection of wordforms from different languages, which are translations of a selected set of meanings. This function dismantles this data structure into a set of sparse matrices.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>splitWordlist(data,
	doculects = "DOCULECT", concepts = "CONCEPT", counterparts = "COUNTERPART",
	splitstrings = TRUE, sep =  "", bigram.binder = "", grapheme.binder = "_", 
	simplify = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="splitWordlist_+3A_data">data</code></td>
<td>

<p>A dataframe or matrix with each row describing a combination of language (DOCULECT), meaning (CONCEPT) and translation (COUNTERPART).
</p>
</td></tr>
<tr><td><code id="splitWordlist_+3A_doculects">doculects</code>, <code id="splitWordlist_+3A_concepts">concepts</code>, <code id="splitWordlist_+3A_counterparts">counterparts</code></td>
<td>

<p>The name (or number) of the column of <code>data</code> in which the respective information is to be found.  The defaults are set to coincide with the naming of the example dataset included in this package: <code><a href="#topic+huber">huber</a></code>.
</p>
</td></tr>
<tr><td><code id="splitWordlist_+3A_splitstrings">splitstrings</code></td>
<td>

<p>Should the counterparts be separated into unigrams and bigrams (using <code><a href="#topic+splitStrings">splitStrings</a></code>)?
</p>
</td></tr>
<tr><td><code id="splitWordlist_+3A_sep">sep</code></td>
<td>

<p>Separator to be passed to <code>splitStrings</code> to specify where to split the strings. Only used when <code>splitstrings = T</code>, ignored otherwise.
</p>
</td></tr>
<tr><td><code id="splitWordlist_+3A_bigram.binder">bigram.binder</code></td>
<td>

<p>Separator to be passed to <code>splitStrings</code> to be inserted between the parts of the bigrams
</p>
</td></tr>
<tr><td><code id="splitWordlist_+3A_grapheme.binder">grapheme.binder</code></td>
<td>

<p>Separator to be used to separate a grapheme from the language name. Graphemes are language-specific symbols (i.e. the 'a' in the one language is not assumed to be the same as the 'a' from another language).
</p>
</td></tr>
<tr><td><code id="splitWordlist_+3A_simplify">simplify</code></td>
<td>

<p>Should the output be reduced to the most important matrices only, with the row and columns names included in the matrices? Defaults to <code>simplify = F</code>, separating everything into different object. See Value below for details on the format of the results.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The meanings that are selected for a wordlist are called CONCEPTS here, and the translations into the various languages COUNTERPARTS (following Poornima &amp; Good 2010). The languages are called DOCULECTS (&lsquo;documented lects&rsquo;) to generalize over their status as dialects, languages, or even small families (following Cysouw &amp; Good 2013).
</p>


<h3>Value</h3>

<p>There are four different possible outputs, depending on the option chosen.
</p>
<p>By default, when <code>splitstrings = T, simplify = F</code>, the following list of 15 objects is returned. It starts with 8 different character vectors, which are actually the row/column names of the following sparse pattern matrices. The naming of the objects is an attempt to make everything easy to remember.
</p>
<table role = "presentation">
<tr><td><code>doculects</code></td>
<td>
<p>Character vector with names of doculects in the data</p>
</td></tr>
<tr><td><code>concepts</code></td>
<td>
<p>Character vector with names of concepts in the data</p>
</td></tr>
<tr><td><code>words</code></td>
<td>
<p>Character vector with all words, i.e. unique counterparts per language. The same string in the same language is only included once, but an identical string occurring in different doculect is separately included for each doculects.</p>
</td></tr>
<tr><td><code>segments</code></td>
<td>
<p>Character vector with all unigram-tokens in order of appearance, including boundary symbols and gap symbols (see <code><a href="#topic+splitStrings">splitStrings</a></code> for more information about the gap symbols)</p>
</td></tr>
<tr><td><code>unigrams</code></td>
<td>
<p>Character vector with all unique unigrams in the data</p>
</td></tr>
<tr><td><code>bigrams</code></td>
<td>
<p>Character vector with all unique bigrams in the data</p>
</td></tr>
<tr><td><code>graphemes</code></td>
<td>
<p>Character vector with all unique graphemes (i.e. combinations of unigrams+doculects) occurring in the data</p>
</td></tr>
<tr><td><code>digraphs</code></td>
<td>
<p>Character vector with all unique digraphs (i.e. combinations of bigrams+doculects) occurring in the data</p>
</td></tr>
<tr><td><code>DW</code></td>
<td>
<p>Sparse pattern matrix of class <code>ngCMatrix</code> linking doculects (D) to words (W)</p>
</td></tr>
<tr><td><code>CW</code></td>
<td>
<p>Sparse pattern matrix of class <code>ngCMatrix</code> linking concepts (C) to words (W)</p>
</td></tr>
<tr><td><code>SW</code></td>
<td>
<p>Sparse pattern matrix of class <code>ngCMatrix</code> linking all token-segments (S) to words (W)</p>
</td></tr>
<tr><td><code>US</code></td>
<td>
<p>Sparse pattern matrix of class <code>ngCMatrix</code> linking unigrams (U) to segments (S)</p>
</td></tr>
<tr><td><code>BS</code></td>
<td>
<p>Sparse pattern matrix of class <code>ngCMatrix</code> linking bigrams (B) to segments (S)</p>
</td></tr>
<tr><td><code>GS</code></td>
<td>
<p>Sparse pattern matrix of class <code>ngCMatrix</code> linking language-specific graphemes (G) to segments (S)</p>
</td></tr>
<tr><td><code>TS</code></td>
<td>
<p>Sparse pattern matrix of class <code>ngCMatrix</code> linking digraphs (T, as no other letter was available) to segments (S)</p>
</td></tr>
</table>
<p>When <code>splitstrings = F, simplify = F</code>, only the following objects from the above list are returned:
</p>
<table role = "presentation">
<tr><td><code>doculects</code></td>
<td>
<p>Character vector with names of doculects in the data</p>
</td></tr>
<tr><td><code>concepts</code></td>
<td>
<p>Character vector with names of concepts in the data</p>
</td></tr>
<tr><td><code>words</code></td>
<td>
<p>Character vector with all words, i.e. unique counterparts per language. The same string in the same language is only included once, but an identical string occurring in different doculect is separately included for each doculects.</p>
</td></tr>
<tr><td><code>DW</code></td>
<td>
<p>Sparse pattern matrix of class <code>ngCMatrix</code> linking doculects (D) to words (W)</p>
</td></tr>
<tr><td><code>CW</code></td>
<td>
<p>Sparse pattern matrix of class <code>ngCMatrix</code> linking concepts (C) to words (W)</p>
</td></tr>
</table>
<p>When <code>splitstrings = T, simplify = T</code> only the bigram-separation is returned, and all row and columns names are included into the matrices. However, for reasons of space, the <code>words</code> vector is only included once:
</p>
<table role = "presentation">
<tr><td><code>DW</code></td>
<td>
<p>Sparse pattern matrix of class <code>ngCMatrix</code> linking doculects (D) to words (W). Doculects are in the rownames, colnames are left empty.</p>
</td></tr>
<tr><td><code>CW</code></td>
<td>
<p>Sparse pattern matrix of class <code>ngCMatrix</code> linking concepts (C) to words (W). Concepts are in the rownames, colnames are left empty.</p>
</td></tr>
<tr><td><code>BW</code></td>
<td>
<p>Sparse pattern matrix of class <code>ngCMatrix</code> linking bigrams (B) to words (W). Bigrams (note: not digraphs!) are in the rownames. This matrix includes all words as colnames.</p>
</td></tr>
</table>
<p>Finally, when <code>splitstrings = F, simplify = T</code>, only the following subset of the above is returned.
</p>
<table role = "presentation">
<tr><td><code>DW</code></td>
<td>
<p>Sparse pattern matrix of class <code>ngCMatrix</code> linking doculects (D) to words (W). Doculects are in the rownames, colnames are left empty.</p>
</td></tr>
<tr><td><code>CW</code></td>
<td>
<p>Sparse pattern matrix of class <code>ngCMatrix</code> linking concepts (C) to words (W). Concepts are in the rownames, colnames are left empty.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Note that the default behavior probably overgenerates information (specifically when <code>splitstrings = T</code>), and might be performing unnecessary computation for specific goals. In practice, it might be useful to tweak the underlying code (mainly by throwing out unnecessary steps) to optimize performance.
</p>


<h3>Author(s)</h3>

<p>Michael Cysouw
</p>


<h3>References</h3>

<p>Cysouw, Michael &amp; Jeff Good. 2013. Languoid, Doculect, Glossonym: Formalizing the notion “language”. <em>Language Documentation and Conservation</em> 7. 331-359.
</p>
<p>Poornima, Shakthi &amp; Jeff Good. 2010. Modeling and Encoding Traditional Wordlists for Machine Applications. <em>Proceedings of the 2010 Workshop on NLP and Linguistics: Finding the Common Ground.</em>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+sim.wordlist">sim.wordlist</a></code> for various quick similarities that can be computed using these matrices.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># ----- load data -----

# an example wordlist, see the help(huber) for details
data(huber)

# ----- show output -----

# a selection, to see the result of splitWordlist
# only show the simplified output here, 
# the full output is rather long even for just these six words
sel &lt;- c(1:3, 1255:1258)
splitWordlist(huber[sel,], simplify = TRUE)

# ----- split complete data -----

# splitting the complete wordlist is a lot of work !
# it won't get much quicker than this
# most time goes into the string-splitting of the almost 26,000 words
# Default version, included splitStrings:
system.time( H &lt;- splitWordlist(huber) )

# Simplified version without splitStrings is much quicker:
system.time( H &lt;- splitWordlist(huber, splitstrings = FALSE, simplify = TRUE) )

# ----- investigate colexification -----

# The simple version can be used to check how often two concepts 
# are expressed identically across all languages ('colexification')
H &lt;- splitWordlist(huber, splitstrings = FALSE, simplify = TRUE)
sim &lt;- tcrossprod(H$CW*1)

# select only the frequent colexifications for a quick visualisation
diag(sim) &lt;- 0
sim &lt;- drop0(sim, tol = 5)
sim &lt;- sim[rowSums(sim) &gt; 0, colSums(sim) &gt; 0]

## Not run: 
# this might lead to errors on some platforms because of non-ASCII symbols
plot( hclust(as.dist(-sim), method = "average"), cex = .5)

## End(Not run)

# ----- investigate regular sound correspondences -----

# One central problem with data from many languages is the variation of orthography
# It is preferred to solve that problem separately
# e.g. check the column "TOKENS" in the huber data
# This is a grapheme-separated version of the data.
# can be used to investigate co-occurrence of graphemes (approx. phonemes)
H &lt;- splitWordlist(huber, counterparts = "TOKENS", sep = " ")

# co-occurrence of all pairs of the 2150 different graphemes through all languages
system.time( G &lt;- assocSparse( (H$CW*1) %*% t(H$SW*1) %*% t(H$GS*1), method = poi))
rownames(G) &lt;- colnames(G) &lt;- H$graphemes
G &lt;- drop0(G, tol = 1)

# select only one language pair for a quick visualisation
# check the nice sound changes between bora and muinane!
GD &lt;- H$GS %*% H$SW %*% t(H$DW)
colnames(GD) &lt;- H$doculects
correspondences &lt;- G[GD[,"bora"],GD[,"muinane"]]

## Not run: 
# this might lead to errors on some platforms because of non-ASCII symbols
heatmap(as.matrix(correspondences))

## End(Not run)
</code></pre>

<hr>
<h2 id='ttMatrix'>
Construct a &lsquo;type-token&rsquo; (tt) Matrix from a vector
</h2><span id='topic+ttMatrix'></span>

<h3>Description</h3>

<p>A type-token matrix is a sparse matrix representation of a vector of entities. The rows of the matrix (&lsquo;types&rsquo;) represent all different entities in the vector, and the columns of the matrix (&lsquo;tokens&rsquo;) represent the entities themselves. The cells in the matrix represent which token belongs to which type. This is basically a convenience wrapper around <code>factor</code> and <code>sparseMatrix</code>, with an option to influence the ordering of the rows (&lsquo;types&rsquo;) based on locale settings.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ttMatrix(vector, collation.locale = "C", simplify = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ttMatrix_+3A_vector">vector</code></td>
<td>

<p>a vector of tokens to be represented as a sparse matrix. It will work without complaining just as well when given a factor, but be aware that the ordering of the levels in the factor depends on the locale, which is transparently handled by this function. So better let this function turn the vector into a factor.
</p>
</td></tr>
<tr><td><code id="ttMatrix_+3A_simplify">simplify</code></td>
<td>

<p>by default, the row and column names are not included into the matrix to keep the matrix as lean as possible. The row names (&lsquo;types&rsquo;) are returned separately. Using <code>simplify = T</code> the row and columns names will be added into the matrix. Note that the column names are simply the vector that went into the function.
</p>
</td></tr>
<tr><td><code id="ttMatrix_+3A_collation.locale">collation.locale</code></td>
<td>

<p>locale determining the ordering (&lsquo;collation&rsquo;) of the entities. By default R mostly uses &lsquo;en_US.UTF-8&rsquo;, though this might depend on the installation. By default, this function sets the ordering to &lsquo;C&rsquo;, which means that characters are ordered according to their Unicode-number. For more information about locale settings, see <code><a href="base.html#topic+locales">locales</a></code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is a rather low-level preparation for later high level functions. A few simple uses are described in the examples.
</p>


<h3>Value</h3>

<p>By default (<code>simplify = F</code>), then the output is a list with two elements:
</p>
<table role = "presentation">
<tr><td><code>M</code></td>
<td>
<p>sparse pattern Matrix of type <code>ngCMatrix</code>. Because of the structure of these matrices, row-based encoding would be slightly more efficient. If RAM is crucial, consider storing the matrix as its transpose</p>
</td></tr>
<tr><td><code>rownames</code></td>
<td>
<p>a separate vector with the names of the types in the order of occurrence in the matrix. This vector is separated from the matrix itself for reasons of efficiency when dealing with many matrices.</p>
</td></tr>
</table>
<p>When <code>simplify = T</code>, then only the matrix M with row and columns names is returned.
</p>


<h3>Author(s)</h3>

<p>Michael Cysouw
</p>


<h3>See Also</h3>

<p>This function is used in various high-level functions like <code><a href="#topic+pwMatrix">pwMatrix</a></code>, <code><a href="#topic+splitText">splitText</a></code>, <code><a href="#topic+splitTable">splitTable</a></code> and <code><a href="#topic+splitWordlist">splitWordlist</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Consider two nominal variables
# one with eight categories, and one with three categories
var1 &lt;- sample(8, 1000, TRUE)
var2 &lt;- sample(3, 1000, TRUE)

# turn them into type-token matrices
M1 &lt;- ttMatrix(var1, simplify = TRUE)
M2 &lt;- ttMatrix(var2, simplify = TRUE)

# Then taking  the `residuals' from assocSparse ...
x &lt;- as.matrix(assocSparse(t(M1), t(M2), method = res))

# ... is the same as the residuals as given by a chi-square
x2 &lt;- chisq.test(var1, var2)$residuals
class(x2) &lt;- "matrix"
all.equal(x, x2, check.attributes = FALSE) # TRUE


# A second quick example: consider a small piece of English text:
text &lt;- "Once upon a time in midwinter, when the snowflakes were 
falling like feathers from heaven, a queen sat sewing at her window, 
which had a frame of black ebony wood. As she sewed she looked up at the snow 
and pricked her finger with her needle. Three drops of blood fell into the snow. 
The red on the white looked so beautiful that she thought to herself: 
If only I had a child as white as snow, as red as blood, and as black 
as the wood in this frame. Soon afterward she had a little daughter who was 
as white as snow, as red as blood, and as black as ebony wood, and therefore 
they called her Little Snow-White. And as soon as the child was born, 
the queen died." 

# split by characters, make lower-case, and turn into a type-token matrix
split.text &lt;- tolower(strsplit(text,"")[[1]])
M &lt;- ttMatrix(split.text, simplify = TRUE)

# rowSums give the character frequency
freq &lt;- rowSums(M)
names(freq) &lt;- rownames(M)
sort(freq, decreasing = TRUE)

# shift the matrix one character to the right using a bandSparse matrix
S &lt;- bandSparse(n = ncol(M), k = 1)
N &lt;- M %*% S

# use rKhatriRao on M and N to get frequencies of bigrams
B &lt;- rKhatriRao(M, N, binder = "")
freqB &lt;- rowSums(B$M)
names(freqB) &lt;- B$rownames
sort(freqB, decreasing = TRUE)

# then the association between N and M is related 
# to the transition probabilities between the characters. 
P &lt;- assocSparse(t(M), t(N))
plot(hclust(as.dist(-P), method = "ward.D"))

</code></pre>

<hr>
<h2 id='unfold'>
Unfolding of Arrays
</h2><span id='topic+unfold'></span><span id='topic+unfold_to_matrix'></span><span id='topic+tenmat'></span>

<h3>Description</h3>

<p>Multidimensional Arrays (&quot;Tensors&quot;) can be unfolded, i.e. multiple dimensions can be combined into a single dimension in a block-wise fashion. Such unfoldings are central to tensor decomposition. In general, computations on tensors are regularly performed by reducing tensors to matrices (&quot;2-dimensional tensors&quot;) and then use regular matrix algebra.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>unfold(x, MARGINS)

unfold_to_matrix(x, ROWS, COLS = NULL)
tenmat(x, ROWS, COLS = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="unfold_+3A_x">x</code></td>
<td>

<p>Sparse array to be unfolded, using <code>simple_sparse_array</code> from the package <code>spam</code>.
</p>
</td></tr>
<tr><td><code id="unfold_+3A_margins">MARGINS</code></td>
<td>

<p>Margins (&quot;dimensions&quot;) to be unfolded. The margins specified will be turned into a single dimension, to be added as the last dimension of the resulting array (see Details).
</p>
</td></tr>
<tr><td><code id="unfold_+3A_rows">ROWS</code></td>
<td>

<p>Margins of the original array to be unfolded into the rows of the resulting matrix.
</p>
</td></tr>
<tr><td><code id="unfold_+3A_cols">COLS</code></td>
<td>

<p>Margins of the original array to be unfolded into the columns of the resulting matrix. If <code>NULL</code>, then all remaining margins, not included in <code>ROWS</code> are unfolded here.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function <code>unfold</code> is a general approach to combining of multiple dimensions into a single dimensions. The function <code>unfold_to_matrix</code> is a special case in which the result is a 2-dimensional matrix. This second function is made to emulate the functionality of the <code>tenmat</code> (&quot;tensor to matrix&quot;) from the Matlab Tensor Toolbox. For convenience, the function-name <code>tenmat</code> is also added as a synonym for <code>unfold_to_matrix</code>.
</p>
<p>Unfolding basically works by interspercing margins subsequently. E.g. margin A of size 3 (A1, A2, A3) and a margin B of size 2 (B1, B2) are unfolded through <code>c(A,B)</code> as (A1B1, A2B1, A3B1, A1B2, A2B2, A3B2), but they are unfolded through <code>c{B,A}</code> as (B1A1, B2A1, B1A2, B2A2, B1A3, B2A3).
</p>


<h3>Value</h3>

<p><code>unfold</code> returns a <code>simple_sparse_array</code> with the new combined dimension added as the last dimension. All original dimensions are shifted forward. The relation between the original dimensions and the new dimensions is stored as an <code>permutation</code> attribute, e.g. try <code>attr(x, "p")</code>. When multiple unfoldings are performed after each other, these permutations can be subsetted on each other to obtain the final permutation. See examples below.
</p>
<p><code>unfold_to_matrix</code> and <code>tenmat</code> return a sparse matrix of class <code>dgTMatrix</code>.
</p>


<h3>Author(s)</h3>

<p>Michael Cysouw &lt;cysouw@mac.com&gt;
</p>


<h3>References</h3>

<p>see some old notes by Charles Van Loon that inspired this implementation at <a href="https://web.archive.org/web/20210505120659/http://www.cs.cornell.edu/cv/SummerSchool/unfold.pdf">https://web.archive.org/web/20210505120659/http://www.cs.cornell.edu/cv/SummerSchool/unfold.pdf</a>. The Matlab Tensor Toolbox can be found at <a href="https://www.tensortoolbox.org">https://www.tensortoolbox.org</a>. A different Matlab implementation is <a href="https://www.tensorlab.net">https://www.tensorlab.net</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># example from Charles Van Loon:
x &lt;- array(c(111, 211, 311, 411, 121, 221, 321, 
    421, 131, 231, 331, 431, 112, 212, 312, 412, 
    122, 222, 322, 422, 132, 232, 332, 432), dim = c(4, 3, 2))
x

s &lt;- as.simple_sparse_array(x)
( s1  &lt;- as.array(unfold_to_matrix(s,1)) )

# note this is identical to:
( s23 &lt;- as.array(unfold(s,c(2,3))) )
all.equal(s23, s1)

# larger example from same source
x &lt;- array(0, dim = c(2,3,2,2,3))
x[1,2,1,1,2] &lt;- 12112
x[2,3,1,2,2] &lt;- 23122
x[2,2,2,1,1] &lt;- 22211
x[2,2,1,2,3] &lt;- 22123
s &lt;- as.simple_sparse_array(x)

as.array(unfold_to_matrix(s, c(1,2,3), c(4,5)))

# use attribute "permutation" to track dimensions
# first step: unfold 1,2,3 to become dimension 3
# original dimensions 4,5 now become 1,2
s1 &lt;- unfold(s, c(1,2,3))
( p1 &lt;- attr(s1, "permutation") )

# now take these dimension 1,2 (originally 4,5) and unfold them
s2 &lt;- unfold(s1, c(1,2))
( p2 &lt;- attr(s2, "permutation") )

# use subsetting to track dimensions through subsequent unfolding
p2[p1]
</code></pre>

<hr>
<h2 id='unfoldBlockMatrix'>
Unfolding of block matrices (sparse matrices)
</h2><span id='topic+unfoldBlockMatrix'></span>

<h3>Description</h3>

<p>Utility function for some matrix manipulations for sparse block matrices.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>unfoldBlockMatrix(X, colGroups, rowGroups = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="unfoldBlockMatrix_+3A_x">X</code></td>
<td>

<p>Sparse block matrix to be unfolded into the individual blocks
</p>
</td></tr>
<tr><td><code id="unfoldBlockMatrix_+3A_colgroups">colGroups</code>, <code id="unfoldBlockMatrix_+3A_rowgroups">rowGroups</code></td>
<td>

<p>either vectors with group indices of the columns and rows, or sparse pattern matrices with the groups as rows and the columns/rows of the X matrix as columns. See example below. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For some sparse manipulation it turns out the profitable to &lsquo;unfold&rsquo; sparse block matrices, i.e. to separate the blocks into their own rows and columns. Each block can then separately be manipulated by using matrix products with diagonal matrices (see example below). For convenience, the function also returns two matrices to &lsquo;refold&rsquo; the unfolded matrix. Specifically, X = L %*% U %*% R
</p>


<h3>Value</h3>

<p>When <code>rowGroups != NULL</code> then the result is a list of three matrices:
</p>
<table role = "presentation">
<tr><td><code>U</code></td>
<td>
<p>The unfolded block matrix</p>
</td></tr>
<tr><td><code>L</code></td>
<td>
<p>The left matrix for refolding the unfolded matrix</p>
</td></tr>
<tr><td><code>R</code></td>
<td>
<p>The right matrix for refolding the unfolded matrix</p>
</td></tr>
</table>
<p>When <code>rowGroups = NULL</code> then the R matrix is not returned, and the refolding works with only the L matrix: X = L %*% U.
</p>


<h3>Note</h3>

<p>The use of <code>kronecker</code> for sparse matrices in this function often leads to warnings about the sparse format of the resulting matrices. These warnings can be ignored.
</p>


<h3>Author(s)</h3>

<p>Michael Cysouw
</p>


<h3>See Also</h3>

<p>This is used in the sparse computation of <code><a href="#topic+assocCol">assocCol</a></code> to divide each block by a different N.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># specially prepared block matrix. For illustration purpuse, this one is not sparse
( X &lt;- Matrix( c( rep(c(1,1,1,1,2,2,3,3,3,4),3),
		rep(c(5,5,5,5,6,6,7,7,7,8),2)),10,5, sparse = TRUE) )

# this matrix has two column groups, and four row groups
# groups can be specified as sparse matrices, or as grouping vectors
colG &lt;- ttMatrix(c(1,1,1,2,2))$M*1
rowG &lt;- ttMatrix(c(1,1,1,1,2,2,3,3,3,4))$M*1

# unfold the matrix, with the result that each block has it's own rows/columns
# the $L and $R matrices can be used to refold the matrix to it's original state
( M &lt;- unfoldBlockMatrix(X, colG, rowG) )

# unfold and refold back: result is identical with M
with(M, all.equal(X, L %*% U %*% R) )

# Unfolded, each block can be easily reached for computations using diagonal matrices
# for example, multiply each block by the sum of its cells, and then fold back again
# this is a trick to apply computations to blocks in sparse block matrices
sums &lt;- drop(crossprod(kronecker(Diagonal(nrow(colG)),rowG)) %*% rowSums(M$U))
S &lt;- Diagonal( x = sums )

with(M, L %*% S %*% U %*% R )
</code></pre>

<hr>
<h2 id='WALS'>
The World Atlas of Language Structures (WALS)
</h2><span id='topic+wals'></span>

<h3>Description</h3>

<p>The World Atlas of Language Structures (WALS) is a large database of structural (phonological, grammatical, lexical) properties of languages gathered from descriptive materials (such as reference grammars) by a team of 55 authors.
</p>
<p>The first version of WALS was published as a book with CD-ROM in 2005 by Oxford University Press. The first online version was published in April 2008. The second online version was published in April 2011. The current dataset is WALS 2013, published on 14 November 2013.
</p>
<p>The included dataset <code>wals</code> takes a somewhat sensible selection from the complete WALS data. It excludes attributes (&quot;features&quot; in WALS-parlance) that are definitially duplicates of others (3, 25, 95, 96, 97), those attributes that only list languages that are incompatible with other attributes (132, 133, 134, 135, 139, 140, 141, 142), and the &lsquo;additional&rsquo; attributes that are marked as &lsquo;B&rsquo; through &lsquo;Z&rsquo;. Further, it removes those languages that do not have any data left after removing those attributes. The result is a dataset with 2566 languages and 131 attributes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(wals)
</code></pre>


<h3>Format</h3>

<p>A list with two dataframes:
</p>

<dl>
<dt><code>data</code></dt><dd><p>the actual WALS data. The object <code>wals$data</code> contains a dataframe with data from 2566 languages on 131 different attributes. The column names identify the WALS features. For details about these features, see <a href="https://wals.info/chapter">https://wals.info/chapter</a></p>
</dd>
<dt><code>meta</code></dt><dd><p>some metadata for the languages. The object <code>wals$meta</code> contains a dataframe with some limited meta-information about these 2566 languages.</p>
</dd>
</dl>

<p>The three-letter WALS-codes are used as rownames in both dataframes. Further, the object <code>wals$meta</code> contains the following variables.
</p>

<dl>
<dt><code>name</code></dt><dd><p>a character vector giving a name for each language</p>
</dd>
<dt><code>genus</code></dt><dd><p>a factor with 522 levels with the genera according to M. Dryer</p>
</dd>
<dt><code>family</code></dt><dd><p>a factor with 215 levels with the families according to M. Dryer</p>
</dd>
<dt><code>longitude</code></dt><dd><p>a numeric vector with geo coordinates for all languages</p>
</dd>
<dt><code>latitude</code></dt><dd><p>a numeric vector with geo coordinates for all languages</p>
</dd>
</dl>



<h3>Details</h3>

<p>All details about the meaning of the variables and much more meta-information is available at <a href="https://wals.info">https://wals.info</a>.
</p>


<h3>Source</h3>

<p>The current data was downloaded from <a href="https://wals.info">https://wals.info</a> in May 2014. The data is licensed as <a href="https://creativecommons.org/licenses/by-nc-nd/2.0/de/deed.en">https://creativecommons.org/licenses/by-nc-nd/2.0/de/deed.en</a>. Some minor corrections on the metadata have been performed (naming of variables, addition of missing coordinates).
</p>


<h3>References</h3>

<p>Dryer, Matthew S. &amp; Haspelmath, Martin (eds.) 2013. <em>The World Atlas of Language Structures Online.</em> Leipzig: Max Planck Institute for Evolutionary Anthropology. 
(Available online at https://wals.info, Accessed on 2013-11-14.) </p>


<h3>Examples</h3>

<pre><code class='language-R'>

data(wals)

# plot all locations of the WALS languages, looks like a world map
plot(wals$meta[,4:5])

# turn the large and mostly empty dataframe into sparse matrices
# recoding is nicely optimized and quick for this reasonably large dataset
# this works perfect as long as things stay within available RAM of the computer
system.time(
  W &lt;- splitTable(wals$data)
)

# as an aside: note that the recoding takes only about 30% of the space
as.numeric( object.size(W) / object.size(wals$data) )

# compute similarities (Chuprov's T, similar to Cramer's V) 
# between all pairs of variables using sparse Matrix methods
system.time(sim &lt;- sim.att(wals$data, method = "chuprov"))

# some structure visible
rownames(sim) &lt;- colnames(wals$data)
plot(hclust(as.dist(1-sim), method = "ward"), cex = 0.5)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
