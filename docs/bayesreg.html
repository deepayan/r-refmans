<!DOCTYPE html><html lang="en"><head><title>Help for package bayesreg</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {bayesreg}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#bayesreg-package'><p>Getting started with the Bayesreg package</p></a></li>
<li><a href='#bayesreg'><p>Fitting Bayesian Regression Models with Continuous Shrinkage Priors</p></a></li>
<li><a href='#predict.bayesreg'><p>Prediction method for Bayesian penalised regression (<code>bayesreg</code>) models</p></a></li>
<li><a href='#spambase'><p>Spambase</p></a></li>
<li><a href='#summary.bayesreg'><p>Summarization method for Bayesian penalised regression (<code>bayesreg</code>) models</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Bayesian Regression Models with Global-Local Shrinkage Priors</td>
</tr>
<tr>
<td>Version:</td>
<td>1.3</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-09-30</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Daniel F. Schmidt &lt;daniel.schmidt@monash.edu&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Fits linear or generalized linear regression models using Bayesian global-local shrinkage prior hierarchies as described in Polson and Scott (2010) &lt;<a href="https://doi.org/10.1093%2Facprof%3Aoso%2F9780199694587.003.0017">doi:10.1093/acprof:oso/9780199694587.003.0017</a>&gt;. Provides an efficient implementation of ridge, lasso, horseshoe and horseshoe+ regression with logistic, Gaussian, Laplace, Student-t, Poisson or geometric distributed targets using the algorithms summarized in Makalic and Schmidt (2016) &lt;<a href="https://doi.org/10.48550%2FarXiv.1611.06649">doi:10.48550/arXiv.1611.06649</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>Imports:</td>
<td>stats (&ge; 3.0)</td>
</tr>
<tr>
<td>Depends:</td>
<td>pgdraw (&ge; 1.0), doParallel (&ge; 1.0.16), foreach (&ge; 1.5.1)</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.2</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-09-30 07:35:45 UTC; dsch0006</td>
</tr>
<tr>
<td>Author:</td>
<td>Daniel F. Schmidt <a href="https://orcid.org/0000-0002-1788-2375"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cph, cre],
  Enes Makalic <a href="https://orcid.org/0000-0003-3017-0871"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cph]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-09-30 08:00:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='bayesreg-package'>Getting started with the Bayesreg package</h2><span id='topic+bayesreg-package'></span>

<h3>Description</h3>

<p>This is a comprehensive, user-friendly package implementing the state-of-the-art in 
Bayesian linear regression, Bayesian count regression and Bayesian logistic regression. Features of the toolbox include:
</p>

<ul>
<li><p> Supports Gaussian, Laplace, Student-t, Poisson, geometric and logistic binary data models.
</p>
</li>
<li><p> Efficient and numerically stable implementations of Bayesian ridge, Bayesian lasso, horseshoe and horseshoe+ regression.
</p>
</li>
<li><p> Provides variable ranking and importance, credible intervals and diagnostics such as the widely applicable information criterion.
</p>
</li>
<li><p> Factor variables are automatically grouped together and additional shrinkage is applied to the set of indicator variables to which they expand.
</p>
</li>
<li><p> Prediction tools for generating credible intervals and Bayesian averaging of predictions.
</p>
</li>
<li><p> Support for multiple cores
</p>
</li></ul>

<p>The lasso, horseshoe and horseshoe+ priors are recommended for data sets where the number of 
predictors is greater than the sample size. The Laplace, Student-t and logistic models are based on scale-mixture representations;
logistic regression utilises the Polya-gamma sampler implemented in the <code>pgdraw</code> package. The Poisson and
geometric distributions are implemented using a fast gradient-assisted Metropolis-Hastings algorithm.
</p>


<h3>Details</h3>

<p>Count (non-negative integer) regression is now supported through implementation of Poisson and geometric regression models.
To support analysis of data with outliers, we provide two heavy-tailed error models in our 
implementation of Bayesian linear regression: Laplace and Student-t distribution errors. 
The widely applicable information criterion (WAIC) is routinely calculated and displayed
to assist users in selecting an appropriate prior distribution for their particular problem, i.e., choice of regularisation or data model.
Most features are straightforward to use. The package will make use of multiple CPU cores by default, if available. 
This feature may be disabled if necessary for problems with very large predictor matrices.
</p>
<p>Further information on the particular algorithms/methods implemented in this package provided
by the literature referenced below.
</p>
<p>Version history:
</p>

<ul>
<li><p> Version 1.1: Initial release.
</p>
</li>
<li><p> Version 1.2: Added Poisson and geometric regression; user specifiable credible interval levels for <code>summary()</code> and <code>predict()</code>; <code>summary()</code> column &quot;ESS&quot; now reports effective sample size rather than percentage-effective sample size.
</p>
</li>
<li><p> Version 1.3: Added support for multiple cores; if n.cores = Inf (the default) Bayesreg will divide the requested number of samples by the number of available cores (total cores minus one) and run these as seperate sampling chains, in parallel, and then recombine after sampling. It is possible to explicitly control the number of cores being used via the n.cores option if desired; n.cores=1 will disable parallelization.
</p>
</li></ul>



<h3>Note</h3>

<p>To cite this package please reference: 
</p>
<p>Makalic, E. &amp; Schmidt, D. F.
High-Dimensional Bayesian Regularised Regression with the BayesReg Package
arXiv:1611.06649 [stat.CO], 2016 <a href="http://arxiv.org/pdf/1611.06649">http://arxiv.org/pdf/1611.06649</a>
</p>
<p>A MATLAB-compatible implementation of this package can be obtained from:
</p>
<p><a href="https://au.mathworks.com/matlabcentral/fileexchange/60823-flexible-bayesian-penalized-regression-modelling">https://au.mathworks.com/matlabcentral/fileexchange/60823-flexible-bayesian-penalized-regression-modelling</a>
</p>


<h3>Author(s)</h3>

<p>Daniel Schmidt <a href="mailto:daniel.schmidt@monash.edu">daniel.schmidt@monash.edu</a> 
</p>
<p>Department of Data Science and AI, Monash University, Australia
</p>
<p>Enes Makalic <a href="mailto:enes.makalic@monash.edu">enes.makalic@monash.edu</a>
</p>
<p>Department of Data Science and AI, Monash University, Australia
</p>


<h3>References</h3>

<p>Bhadra, A.; Datta, J.; Polson, N. G. &amp; Willard, B. 
The Horseshoe+ Estimator of Ultra-Sparse Signals 
Bayesian Analysis, 2016
</p>
<p>Bhattacharya, A.; Chakraborty, A. &amp; Mallick, B. K. 
Fast sampling with Gaussian scale-mixture priors in high-dimensional regression 
arXiv:1506.04778, 2016
</p>
<p>Carvalho, C. M.; Polson, N. G. &amp; Scott, J. G. 
The horseshoe estimator for sparse signals 
Biometrika, Vol. 97, pp. 465-480, 2010
</p>
<p>Makalic, E. &amp; Schmidt, D. F. 
A Simple Sampler for the Horseshoe Estimator 
IEEE Signal Processing Letters, Vol. 23, pp. 179-182, 2016
</p>
<p>Park, T. &amp; Casella, G. 
The Bayesian Lasso 
Journal of the American Statistical Association, Vol. 103, pp. 681-686, 2008
</p>
<p>Polson, N. G.; Scott, J. G. &amp; Windle, J. 
Bayesian inference for logistic models using Polya-Gamma latent variables 
Journal of the American Statistical Association, Vol. 108, pp. 1339-1349, 2013
</p>
<p>Rue, H. 
Fast sampling of Gaussian Markov random fields 
Journal of the Royal Statistical Society (Series B), Vol. 63, pp. 325-338, 2001
</p>
<p>Xu, Z., Schmidt, D.F., Makalic, E., Qian, G. &amp; Hopper, J.L.
Bayesian Grouped Horseshoe Regression with Application to Additive Models
AI 2016: Advances in Artificial Intelligence, pp. 229-240, 2016
</p>
<p>Schmidt, D.F. &amp; Makalic, E.
Bayesian Generalized Horseshoe Estimation of Generalized Linear Models
ECML PKDD 2019: Machine Learning and Knowledge Discovery in Databases. pp 598-613, 2019
</p>
<p>Stan Development Team, Stan Reference Manual (Version 2.26), Section 15.4, &quot;Effective Sample Size&quot;,
<a href="https://mc-stan.org/docs/2_18/reference-manual/effective-sample-size-section.html">https://mc-stan.org/docs/2_18/reference-manual/effective-sample-size-section.html</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+bayesreg">bayesreg</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# -----------------------------------------------------------------
# By default Bayesreg now utilizes multiple cores if you have them
# available. If you do not want to use multiple cores you can set 
# n.cores=1 when calling Bayesreg.
#
# In most realistic/practical settings, i.e., when a 
# even a moderate number of samples is being requested, parallelization 
# will usually result in substantial speedups, and can dramatically reduce
# run-time for large numbers of samples.
#
# However, if your design matrix and desired number of samples 
# are small (i.e, small nrow(X), ncol(X) and n.samples) 
# then sometimes no parallelization, or parallelization with a small
# number of cores, can be quicker due to the overhead of setting up 
# multiple threads. This is likely only relevant if you are calling Bayesreg
# hundreds/thousands of times with *small* numbers of samples being requested
# for each call, e.g., if you are doing some sort of one-at-a-time testing
# across many predictors such as a genome-wide association test, or similar.
# In this case you may be better off parallelizing the calls to 
# Bayesreg across the tests and disabling parallelization when calling 
# Bayesreg (i.e., n.cores=1).
#
# -----------------------------------------------------------------
# Example 1: Gaussian regression
#
X = matrix(rnorm(100*20),100,20)
b = matrix(0,20,1)
b[1:5] = c(5,4,3,2,1)
y = X %*% b + rnorm(100, 0, 1)

df &lt;- data.frame(X,y)
rv.lm &lt;- lm(y~.,df)                        # Regular least-squares
summary(rv.lm)

# Horseshoe regression -- here we show how to explicitly control the maximum
# number of cores being used for sampling to 4
rv.hs &lt;- bayesreg(y~., df, prior="hs", n.cores=4)       
rv.hs$n.cores  # actual number of cores used (will be &lt;= 4, depending on machine)
rv.hs.s &lt;- summary(rv.hs)

# Expected squared prediction error for least-squares
coef_ls = coef(rv.lm)
as.numeric(sum( (as.matrix(coef_ls[-1]) - b)^2 ) + coef_ls[1]^2)

# Expected squared prediction error for horseshoe
as.numeric(sum( (rv.hs$mu.beta - b)^2 ) + rv.hs$mu.beta0^2)


# -----------------------------------------------------------------
# Example 2: Gaussian v Student-t robust regression
X = 1:10;
y = c(-0.6867, 1.7258, 1.9117, 6.1832, 5.3636, 7.1139, 9.5668, 10.0593, 11.4044, 6.1677);
df = data.frame(X,y)

# Gaussian ridge
rv.G &lt;- bayesreg(y~., df, model = "gaussian", prior = "ridge", n.samples = 1e5)

# Student-t ridge
rv.t &lt;- bayesreg(y~., df, model = "t", prior = "ridge", t.dof = 5, n.samples = 1e5)

# Plot the different estimates with credible intervals
plot(df$X, df$y, xlab="x", ylab="y")

yhat_G &lt;- predict(rv.G, df, bayes.avg=TRUE)
lines(df$X, yhat_G[,1], col="blue", lwd=2.5)
lines(df$X, yhat_G[,3], col="blue", lwd=1, lty="dashed")
lines(df$X, yhat_G[,4], col="blue", lwd=1, lty="dashed")

yhat_t &lt;- predict(rv.t, df, bayes.avg=TRUE)
lines(df$X, yhat_t[,1], col="darkred", lwd=2.5)
lines(df$X, yhat_t[,3], col="darkred", lwd=1, lty="dashed")
lines(df$X, yhat_t[,4], col="darkred", lwd=1, lty="dashed")

legend(1,11,c("Gaussian","Student-t (dof=5)"),lty=c(1,1),col=c("blue","darkred"),
       lwd=c(2.5,2.5), cex=0.7)


# -----------------------------------------------------------------
# Example 3: Poisson/geometric regression example

X  = matrix(rnorm(100*5),100,5)
b  = c(0.5,-1,0,0,1)
nu = X%*%b + 1
y  = rpois(lambda=exp(nu),n=length(nu))

df &lt;- data.frame(X,y)

# Fit a Poisson regression
rv.pois=bayesreg(y~., data=df, model="poisson",prior="hs", burnin=1e4, n.samples=5e4)
summary(rv.pois)

# Fit a geometric regression
rv.geo=bayesreg(y~., data=df, model="geometric",prior="hs", burnin=1e4, n.samples=5e4)
summary(rv.geo)

# Compare the two models in terms of their WAIC scores
cat(sprintf("Poisson regression WAIC=%g vs geometric regression WAIC=%g", 
            rv.pois$waic, rv.geo$waic))
# Poisson is clearly preferred to geometric, which is good as data is generated from a Poisson!
 
 
# -----------------------------------------------------------------
# Example 4: Logistic regression on spambase
data(spambase)
  
# bayesreg expects binary targets to be factors
spambase$is.spam &lt;- factor(spambase$is.spam)

# First take a subset of the data (1/10th) for training, reserve the rest for testing
spambase.tr  = spambase[seq(1,nrow(spambase),10),]
spambase.tst = spambase[-seq(1,nrow(spambase),10),]
  
# Fit a model using logistic horseshoe for 2,000 samples
# In practice, &gt; 10,000 samples would be a more realistic amount to draw
rv &lt;- bayesreg(is.spam ~ ., spambase.tr, model = "logistic", prior = "horseshoe", n.samples = 2e3)
  
# Summarise, sorting variables by their ranking importance
rv.s &lt;- summary(rv,sort.rank=TRUE)
  
# Make predictions about testing data -- get class predictions and class probabilities
y_pred &lt;- predict(rv, spambase.tst, type='class')
  
# Check how well did our predictions did by generating confusion matrix
table(y_pred, spambase.tst$is.spam)
  
# Calculate logarithmic loss on test data
y_prob &lt;- predict(rv, spambase.tst, type='prob')
cat('Neg Log-Like for no Bayes average, posterior mean estimates: ', sum(-log(y_prob[,1])), '\n')
y_prob &lt;- predict(rv, spambase.tst, type='prob', sum.stat="median")
cat('Neg Log-Like for no Bayes average, posterior median estimates: ', sum(-log(y_prob[,1])), '\n')
y_prob &lt;- predict(rv, spambase.tst, type='prob', bayes.avg=TRUE)
cat('Neg Log-Like for Bayes average: ', sum(-log(y_prob[,1])), '\n')

## End(Not run)

</code></pre>

<hr>
<h2 id='bayesreg'>Fitting Bayesian Regression Models with Continuous Shrinkage Priors</h2><span id='topic+bayesreg'></span>

<h3>Description</h3>

<p>Fit a linear or logistic regression model using Bayesian continuous shrinkage prior distributions. Handles ridge, lasso, horseshoe and horseshoe+ regression with logistic,
Gaussian, Laplace, Student-t, Poisson or geometric distributed targets. See <code><a href="#topic+bayesreg-package">bayesreg-package</a></code> for more details on the features available in this package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bayesreg(
  formula,
  data,
  model = "normal",
  prior = "ridge",
  n.samples = 1000,
  burnin = 1000,
  thin = 5,
  t.dof = 5,
  n.cores = Inf
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="bayesreg_+3A_formula">formula</code></td>
<td>
<p>An object of class &quot;<code><a href="stats.html#topic+formula">formula</a></code>&quot;: a symbolic description of the model to be fitted using the standard R formula notation.</p>
</td></tr>
<tr><td><code id="bayesreg_+3A_data">data</code></td>
<td>
<p>A data frame containing the variables in the model.</p>
</td></tr>
<tr><td><code id="bayesreg_+3A_model">model</code></td>
<td>
<p>The distribution of the target (y) variable. Continuous or numeric variables can be distributed as per a Gaussian distribution (<code>model="gaussian"</code> 
or <code>model="normal"</code>), Laplace distribution (<code>model = "laplace"</code> or <code>model = "l1"</code>) or Student-t distribution (<code>"model" = "studentt"</code> or <code>"model" = "t"</code>). 
Integer or count data can be distributed as per a Poisson distribution (<code>model="poisson"</code>) or geometric distribution (<code>model="geometric"</code>).
For binary targets (factors with two levels) either <code>model="logistic"</code> or <code>"model"="binomial"</code> should be used.</p>
</td></tr>
<tr><td><code id="bayesreg_+3A_prior">prior</code></td>
<td>
<p>Which continuous shrinkage prior distribution over the regression coefficients to use. Options include ridge regression 
(<code>prior="rr"</code> or <code>prior="ridge"</code>), lasso regression (<code>prior="lasso"</code>), horseshoe regression (<code>prior="hs"</code> or <code>prior="horseshoe"</code>) and 
horseshoe+ regression (<code>prior="hs+"</code> or <code>prior="horseshoe+"</code>)</p>
</td></tr>
<tr><td><code id="bayesreg_+3A_n.samples">n.samples</code></td>
<td>
<p>Number of posterior samples to generate.</p>
</td></tr>
<tr><td><code id="bayesreg_+3A_burnin">burnin</code></td>
<td>
<p>Number of burn-in samples.</p>
</td></tr>
<tr><td><code id="bayesreg_+3A_thin">thin</code></td>
<td>
<p>Desired level of thinning.</p>
</td></tr>
<tr><td><code id="bayesreg_+3A_t.dof">t.dof</code></td>
<td>
<p>Degrees of freedom for the Student-t distribution.</p>
</td></tr>
<tr><td><code id="bayesreg_+3A_n.cores">n.cores</code></td>
<td>
<p>Maximum number of cores to use; if <code>n.cores=Inf</code> then Bayesreg automatically sets this to one less than the maximum number of available cores. If <code>n.cores=1</code> then no parallelization occurs.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object with S3 class <code>"bayesreg"</code> containing the results of the sampling process, plus some additional information.
</p>
<table role = "presentation">
<tr><td><code>beta</code></td>
<td>
<p>Posterior samples the regression model coefficients.</p>
</td></tr>
<tr><td><code>beta0</code></td>
<td>
<p>Posterior samples of the intercept parameter.</p>
</td></tr>
<tr><td><code>sigma2</code></td>
<td>
<p>Posterior samples of the square of the scale parameter; for Gaussian distributed targets this is equal to the variance. For binary targets this is empty.</p>
</td></tr>
<tr><td><code>mu.beta</code></td>
<td>
<p>The mean of the posterior samples for the regression coefficients.</p>
</td></tr>
<tr><td><code>mu.beta0</code></td>
<td>
<p>The mean of the posterior samples for the intercept parameter.</p>
</td></tr>
<tr><td><code>mu.sigma2</code></td>
<td>
<p>The mean of the posterior samples for squared scale parameter.</p>
</td></tr>
<tr><td><code>tau2</code></td>
<td>
<p>Posterior samples of the global shrinkage parameter.</p>
</td></tr>
<tr><td><code>t.stat</code></td>
<td>
<p>Posterior t-statistics for each regression coefficient.</p>
</td></tr>
<tr><td><code>var.ranks</code></td>
<td>
<p>Ranking of the covariates by their importance, with &quot;1&quot; denoting the most important covariate.</p>
</td></tr>
<tr><td><code>log.l</code></td>
<td>
<p>The log-likelihood at the posterior means of the model parameters</p>
</td></tr>
<tr><td><code>waic</code></td>
<td>
<p>The Widely Applicable Information Criterion (WAIC) score for the model</p>
</td></tr>
<tr><td><code>waic.dof</code></td>
<td>
<p>The effective degrees-of-freedom of the model, as estimated by the WAIC.</p>
</td></tr>
</table>
<p>The returned object also stores the parameters/options used to run <code>bayesreg</code>:
</p>
<table role = "presentation">
<tr><td><code>formula</code></td>
<td>
<p>The object of type &quot;<code><a href="stats.html#topic+formula">formula</a></code>&quot; describing the fitted model.</p>
</td></tr>
<tr><td><code>model</code></td>
<td>
<p>The distribution of the target (y) variable.</p>
</td></tr>
<tr><td><code>prior</code></td>
<td>
<p>The shrinkage prior used to fit the model.</p>
</td></tr>
<tr><td><code>n.samples</code></td>
<td>
<p>The number of samples generated from the posterior distribution.</p>
</td></tr>
<tr><td><code>burnin</code></td>
<td>
<p>The number of burnin samples that were generated.</p>
</td></tr>
<tr><td><code>thin</code></td>
<td>
<p>The level of thinning.</p>
</td></tr>
<tr><td><code>n</code></td>
<td>
<p>The sample size of the data used to fit the model.</p>
</td></tr>
<tr><td><code>p</code></td>
<td>
<p>The number of covariates in the fitted model.</p>
</td></tr>
<tr><td><code>n.cores</code></td>
<td>
<p>The number of cores actually used during sampling.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Draws a series of samples from the posterior distribution of a linear (Gaussian, Laplace or Student-t) or generalized linear (logistic binary, Poisson, geometric) regression model with specified continuous 
shrinkage prior distribution (ridge regression, lasso, horseshoe and horseshoe+) using Gibbs sampling. The intercept parameter is always included, and is never penalised.
</p>
<p>While only <code>n.samples</code> are returned, the total number of samples generated is equal to <code>burnin</code>+<code>n.samples</code>*<code>thin</code>. To generate the samples 
of the regression coefficients, the code will use either Rue's algorithm (when the number of samples is twice the number of covariates) or the algorithm of 
Bhattacharya et al. as appropriate. Factor variables are automatically grouped together and 
additional shrinkage is applied to the set of indicator variables to which they expand.
</p>
<p>If <code>n.cores &gt; 1</code> then Bayesreg will divide the total sampling process between a number of independent sampling chains, with each chain being run on
a seperate core. If <code>n.cores=Inf</code> then Bayesreg will use one less than the total number of available cores.
If <code>n.cores</code> is a positive integer, then this is the maximum number of cores Bayesreg will use, though Bayesreg will cap this at the total number of available cores minus one. 
The total number of samples are split evenly between the number of cores being utilised, and after sampling
the different chains are recombined into a single body of samples. Each chain will perform
<code>burnin</code> burn-in sampling iterations before collecting samples, so the improvement in speed from
using from multiple cores will be greater if <code>n.samples*thin</code> is substantially greater than <code>burnin</code>.
</p>


<h3>Note</h3>

<p>To cite this toolbox please reference: 
</p>
<p>Makalic, E. &amp; Schmidt, D. F.
High-Dimensional Bayesian Regularised Regression with the BayesReg Package
arXiv:1611.06649 [stat.CO], 2016 <a href="http://arxiv.org/pdf/1611.06649">http://arxiv.org/pdf/1611.06649</a>
</p>
<p>A MATLAB implementation of the bayesreg function is also available from:
</p>
<p><a href="https://au.mathworks.com/matlabcentral/fileexchange/60823-flexible-bayesian-penalized-regression-modelling">https://au.mathworks.com/matlabcentral/fileexchange/60823-flexible-bayesian-penalized-regression-modelling</a>
</p>
<p>Copyright (C) Daniel F. Schmidt and Enes Makalic, 2016-2021
</p>


<h3>References</h3>

<p>Makalic, E. &amp; Schmidt, D. F.
High-Dimensional Bayesian Regularised Regression with the BayesReg Package
arXiv:1611.06649 [stat.CO], 2016 <a href="http://arxiv.org/pdf/1611.06649">http://arxiv.org/pdf/1611.06649</a>
</p>
<p>Park, T. &amp; Casella, G. 
The Bayesian Lasso 
Journal of the American Statistical Association, Vol. 103, pp. 681-686, 2008
</p>
<p>Carvalho, C. M.; Polson, N. G. &amp; Scott, J. G. 
The horseshoe estimator for sparse signals 
Biometrika, Vol. 97, 465-480, 2010
</p>
<p>Makalic, E. &amp; Schmidt, D. F. 
A Simple Sampler for the Horseshoe Estimator 
IEEE Signal Processing Letters, Vol. 23, pp. 179-182, 2016 <a href="http://arxiv.org/pdf/1508.03884v4">http://arxiv.org/pdf/1508.03884v4</a>
</p>
<p>Bhadra, A.; Datta, J.; Polson, N. G. &amp; Willard, B. 
The Horseshoe+ Estimator of Ultra-Sparse Signals 
Bayesian Analysis, 2016
</p>
<p>Polson, N. G.; Scott, J. G. &amp; Windle, J. 
Bayesian inference for logistic models using Polya-Gamma latent variables 
Journal of the American Statistical Association, Vol. 108, 1339-1349, 2013
</p>
<p>Rue, H. 
Fast sampling of Gaussian Markov random fields 
Journal of the Royal Statistical Society (Series B), Vol. 63, 325-338, 2001
</p>
<p>Bhattacharya, A.; Chakraborty, A. &amp; Mallick, B. K. 
Fast sampling with Gaussian scale-mixture priors in high-dimensional regression 
arXiv:1506.04778, 2016
</p>
<p>Schmidt, D.F. &amp; Makalic, E.
Bayesian Generalized Horseshoe Estimation of Generalized Linear Models
ECML PKDD 2019: Machine Learning and Knowledge Discovery in Databases. pp 598-613, 2019
</p>
<p>Stan Development Team, Stan Reference Manual (Version 2.26), Section 15.4, &quot;Effective Sample Size&quot;,
<a href="https://mc-stan.org/docs/2_18/reference-manual/effective-sample-size-section.html">https://mc-stan.org/docs/2_18/reference-manual/effective-sample-size-section.html</a>
</p>


<h3>See Also</h3>

<p>The prediction function <code><a href="#topic+predict.bayesreg">predict.bayesreg</a></code> and summary function <code><a href="#topic+summary.bayesreg">summary.bayesreg</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># -----------------------------------------------------------------
# By default Bayesreg now utilizes multiple cores if you have them
# available. If you do not want to use multiple cores you can set 
# n.cores=1 when calling Bayesreg.
#
# In most realistic/practical settings, i.e., when a 
# even a moderate number of samples is being requested, parallelization 
# will usually result in substantial speedups, and can dramatically reduce
# run-time for large numbers of samples.
#
# However, if your design matrix and desired number of samples 
# are small (i.e, small nrow(X), ncol(X) and n.samples) 
# then sometimes no parallelization, or parallelization with a small
# number of cores, can be quicker due to the overhead of setting up 
# multiple threads. This is likely only relevant if you are calling Bayesreg
# hundreds/thousands of times with *small* numbers of samples being requested
# for each call, e.g., if you are doing some sort of one-at-a-time testing
# across many predictors such as a genome-wide association test, or similar.
# In this case you may be better off parallelizing the calls to 
# Bayesreg across the tests and disabling parallelization when calling 
# Bayesreg (i.e., n.cores=1).
#
# -----------------------------------------------------------------
# Example 1: Gaussian regression
X = matrix(rnorm(100*20),100,20)
b = matrix(0,20,1)
b[1:5] = c(5,4,3,2,1)
y = X %*% b + rnorm(100, 0, 1)

df &lt;- data.frame(X,y)
rv.lm &lt;- lm(y~.,df)                        # Regular least-squares
summary(rv.lm)

# Horseshoe regression -- here we show how to explicitly control the maximum
# number of cores being used for sampling to 4
rv.hs &lt;- bayesreg(y~., df, prior="hs", n.cores=4)       
rv.hs$n.cores  # actual number of cores used (will be &lt;= 4, depending on machine)
rv.hs.s &lt;- summary(rv.hs)

# Expected squared prediction error for least-squares
coef_ls = coef(rv.lm)
as.numeric(sum( (as.matrix(coef_ls[-1]) - b)^2 ) + coef_ls[1]^2)

# Expected squared prediction error for horseshoe
as.numeric(sum( (rv.hs$mu.beta - b)^2 ) + rv.hs$mu.beta0^2)


# -----------------------------------------------------------------
# Example 2: Gaussian v Student-t robust regression
X = 1:10;
y = c(-0.6867, 1.7258, 1.9117, 6.1832, 5.3636, 7.1139, 9.5668, 10.0593, 11.4044, 6.1677);
df = data.frame(X,y)

# Gaussian ridge
rv.G &lt;- bayesreg(y~., df, model = "gaussian", prior = "ridge", n.samples = 1e3)

# Student-t ridge
rv.t &lt;- bayesreg(y~., df, model = "t", prior = "ridge", t.dof = 5, n.samples = 1e3)

# Plot the different estimates with credible intervals
plot(df$X, df$y, xlab="x", ylab="y")

yhat_G &lt;- predict(rv.G, df, bayes.avg=TRUE)
lines(df$X, yhat_G[,1], col="blue", lwd=2.5)
lines(df$X, yhat_G[,3], col="blue", lwd=1, lty="dashed")
lines(df$X, yhat_G[,4], col="blue", lwd=1, lty="dashed")

yhat_t &lt;- predict(rv.t, df, bayes.avg=TRUE)
lines(df$X, yhat_t[,1], col="darkred", lwd=2.5)
lines(df$X, yhat_t[,3], col="darkred", lwd=1, lty="dashed")
lines(df$X, yhat_t[,4], col="darkred", lwd=1, lty="dashed")

legend(1,11,c("Gaussian","Student-t (dof=5)"),lty=c(1,1),col=c("blue","darkred"),
       lwd=c(2.5,2.5), cex=0.7)

## Not run: 
# -----------------------------------------------------------------
# Example 3: Poisson/geometric regression example

X  = matrix(rnorm(100*5),100,5)
b  = c(0.5,-1,0,0,1)
nu = X%*%b + 1
y  = rpois(lambda=exp(nu),n=length(nu))

df &lt;- data.frame(X,y)

# Fit a Poisson regression
rv.pois=bayesreg(y~.,data=df,model="poisson",prior="hs", burnin=1e4, n.samples=5e4)
summary(rv.pois)

# Fit a geometric regression
rv.geo=bayesreg(y~.,data=df,model="geometric",prior="hs", burnin=1e4, n.samples=5e4)
summary(rv.geo)

# Compare the two models in terms of their WAIC scores
cat(sprintf("Poisson regression WAIC=%g vs geometric regression WAIC=%g", 
            rv.pois$waic, rv.geo$waic))
# Poisson is clearly preferred to geometric, which is good as data is generated from a Poisson!
 
 
# -----------------------------------------------------------------
# Example 4: Logistic regression on spambase
data(spambase)
  
# bayesreg expects binary targets to be factors
spambase$is.spam &lt;- factor(spambase$is.spam)

# First take a subset of the data (1/10th) for training, reserve the rest for testing
spambase.tr  = spambase[seq(1,nrow(spambase),10),]
spambase.tst = spambase[-seq(1,nrow(spambase),10),]
  
# Fit a model using logistic horseshoe for 2,000 samples
# In practice, &gt; 10,000 samples would be a more realistic amount to draw
rv &lt;- bayesreg(is.spam ~ ., spambase.tr, model = "logistic", prior = "horseshoe", n.samples = 2e3)
  
# Summarise, sorting variables by their ranking importance
rv.s &lt;- summary(rv,sort.rank=TRUE)
  
# Make predictions about testing data -- get class predictions and class probabilities
y_pred &lt;- predict(rv, spambase.tst, type='class')
  
# Check how well did our predictions did by generating confusion matrix
table(y_pred, spambase.tst$is.spam)
  
# Calculate logarithmic loss on test data
y_prob &lt;- predict(rv, spambase.tst, type='prob')
cat('Neg Log-Like for no Bayes average, posterior mean estimates: ', sum(-log(y_prob[,1])), '\n')
y_prob &lt;- predict(rv, spambase.tst, type='prob', sum.stat="median")
cat('Neg Log-Like for no Bayes average, posterior median estimates: ', sum(-log(y_prob[,1])), '\n')
y_prob &lt;- predict(rv, spambase.tst, type='prob', bayes.avg=TRUE)
cat('Neg Log-Like for Bayes average: ', sum(-log(y_prob[,1])), '\n')

## End(Not run)

</code></pre>

<hr>
<h2 id='predict.bayesreg'>Prediction method for Bayesian penalised regression (<code>bayesreg</code>) models</h2><span id='topic+predict.bayesreg'></span>

<h3>Description</h3>

<p>Predict values based on Bayesian penalised regression (<code><a href="#topic+bayesreg">bayesreg</a></code>) models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'bayesreg'
predict(
  object,
  newdata,
  type = "linpred",
  bayes.avg = FALSE,
  sum.stat = "mean",
  CI = 95,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.bayesreg_+3A_object">object</code></td>
<td>
<p>an object of class <code>"bayesreg"</code> created as a result of a call to <code><a href="#topic+bayesreg">bayesreg</a></code>.</p>
</td></tr>
<tr><td><code id="predict.bayesreg_+3A_newdata">newdata</code></td>
<td>
<p>A data frame providing the variables from which to produce predictions.</p>
</td></tr>
<tr><td><code id="predict.bayesreg_+3A_type">type</code></td>
<td>
<p>The type of predictions to produce; if <code>type="linpred"</code> it will return the linear predictor for binary, 
count and continuous data. If <code>type="prob"</code> it will return predictive probability estimates for provided 'y' data (see below for more details).
If <code>type="response"</code> it will return the predicted conditional mean of the target (see below for more details).
If <code>type="class"</code> and the data is binary, it will return the best guess at the class of the target variable.</p>
</td></tr>
<tr><td><code id="predict.bayesreg_+3A_bayes.avg">bayes.avg</code></td>
<td>
<p>logical; whether to produce predictions using Bayesian averaging.</p>
</td></tr>
<tr><td><code id="predict.bayesreg_+3A_sum.stat">sum.stat</code></td>
<td>
<p>The type of summary statistic to use; either <code>sum.stat="mean"</code> or <code>sum.stat="median"</code>.</p>
</td></tr>
<tr><td><code id="predict.bayesreg_+3A_ci">CI</code></td>
<td>
<p>The size (level, as a percentage) of the credible interval to report (default: 95, i.e. a 95% credible interval)</p>
</td></tr>
<tr><td><code id="predict.bayesreg_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>predict.bayesreg</code> produces a vector or matrix of predictions of the specified type. If <code>bayes.avg</code> is 
<code>FALSE</code> a matrix with a single column <code>pred</code> is returned, containing the predictions.
</p>
<p>If <code>bayes.avg</code> is <code>TRUE</code>, three additional columns are returned: <code>se(pred)</code>, which contains 
standard errors for the predictions, and two columns containing the credible intervals (at the specified level) for the predictions.
</p>


<h3>Details</h3>

<p><code>predict.bayesreg</code> produces predicted values using variables from the specified data frame. The type of predictions produced 
depend on the value of the parameter <code>type</code>:
</p>

<ul>
<li><p> If <code>type="linpred"</code>, the predictions that are returned will be the value of the linear predictor formed from the model 
coefficients and the provided data. 
</p>
</li>
<li><p> If <code>type="response"</code>, the predictions will be the conditional mean for each data point. For Gaussian, Laplace and Student-t targets
the conditional mean is simply equal to the linear predictor; for binary data, the predictions will 
be the probability of the target being equal to the second level of the factor variable; for count data, the conditional mean
will be exp(linear predictor).
</p>
</li>
<li><p> If <code>type="prob"</code>, the predictions will be probabilities. The specified data frame must include a column with the same name as the 
target variable on which the model was created. The predictions will then be the probability (density) values for these target values. 
</p>
</li>
<li><p> If <code>type="class"</code> and the target variable is binary, the predictions will be the most likely class.
</p>
</li></ul>

<p>If <code>bayes.avg</code> is <code>FALSE</code> the predictions will be produced by using a summary of the posterior samples of the coefficients 
and scale parameters as estimates for the model. If <code>bayes.avg</code> is <code>TRUE</code>, the predictions will be produced by posterior 
averaging over the posterior samples of the coefficients and scale parameters, allowing the uncertainty in the estimation process to 
be explicitly taken into account in the prediction process. 
</p>
<p>If <code>sum.stat="mean"</code> and <code>bayes.avg</code> is <code>FALSE</code>, the mean of the posterior samples will be used as point estimates for
making predictions. Likewise, if <code>sum.stat="median"</code> and <code>bayes.avg</code> is <code>FALSE</code>, the co-ordinate wise posterior medians 
will be used as estimates for making predictions. If <code>bayes.avg</code> is <code>TRUE</code> and <code>type!="prob"</code>, the posterior mean 
(median) of the predictions from each of the posterior samples will be used as predictions. The value of <code>sum.stat</code> has no effect 
if <code>type="prob"</code>.
</p>


<h3>See Also</h3>

<p>The model fitting function <code><a href="#topic+bayesreg">bayesreg</a></code> and summary function <code><a href="#topic+summary.bayesreg">summary.bayesreg</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

# The examples below that are run by CRAN use n.cores=2 to limit the number 
# of cores to two for CRAN check compliance.

# In practice you can simply omit this option to let bayesreg use as many
# as are available (which is usually total number of cores - 1)

# If you do not want to use multiple cores you can set parallel=F

# -----------------------------------------------------------------
# Example 1: Fitting linear models to data and generating credible intervals
X = 1:10;
y = c(-0.6867, 1.7258, 1.9117, 6.1832, 5.3636, 7.1139, 9.5668, 10.0593, 11.4044, 6.1677);
df = data.frame(X,y)

# Gaussian ridge
rv.L &lt;- bayesreg(y~., df, model = "laplace", prior = "ridge", n.samples = 1e3, n.cores = 2)

# Plot the different estimates with credible intervals
plot(df$X, df$y, xlab="x", ylab="y")

yhat &lt;- predict(rv.L, df, bayes.avg=TRUE)
lines(df$X, yhat[,1], col="blue", lwd=2.5)
lines(df$X, yhat[,3], col="blue", lwd=1, lty="dashed")
lines(df$X, yhat[,4], col="blue", lwd=1, lty="dashed")
yhat &lt;- predict(rv.L, df, bayes.avg=TRUE, sum.stat = "median")
lines(df$X, yhat[,1], col="red", lwd=2.5)

legend(1,11,c("Posterior Mean (Bayes Average)","Posterior Median (Bayes Average)"),
       lty=c(1,1),col=c("blue","red"),lwd=c(2.5,2.5), cex=0.7)


# -----------------------------------------------------------------
# Example 2: Predictive density for continuous data
X = 1:10;
y = c(-0.6867, 1.7258, 1.9117, 6.1832, 5.3636, 7.1139, 9.5668, 10.0593, 11.4044, 6.1677);
df = data.frame(X,y)

# Gaussian ridge
rv.G &lt;- bayesreg(y~., df, model = "gaussian", prior = "ridge", n.samples = 1e3, n.cores = 2)

# Produce predictive density for X=2
df.tst = data.frame(y=seq(-7,12,0.01),X=2)
prob_noavg_mean &lt;- predict(rv.G, df.tst, bayes.avg=FALSE, type="prob", sum.stat = "mean")
prob_noavg_med  &lt;- predict(rv.G, df.tst, bayes.avg=FALSE, type="prob", sum.stat = "median")
prob_avg        &lt;- predict(rv.G, df.tst, bayes.avg=TRUE, type="prob")

# Plot the density
plot(NULL, xlim=c(-7,12), ylim=c(0,0.14), xlab="y", ylab="p(y)")
lines(df.tst$y, prob_noavg_mean[,1],lwd=1.5)
lines(df.tst$y, prob_noavg_med[,1], col="red",lwd=1.5)
lines(df.tst$y, prob_avg[,1], col="green",lwd=1.5)

legend(-7,0.14,c("Mean (no averaging)","Median (no averaging)","Bayes Average"),
       lty=c(1,1,1),col=c("black","red","green"),lwd=c(1.5,1.5,1.5), cex=0.7)

title('Predictive densities for X=2')


## Not run: 
# -----------------------------------------------------------------
# Example 3: Poisson (count) regression

X  = matrix(rnorm(100*20),100,5)
b  = c(0.5,-1,0,0,1)
nu = X%*%b + 1
y  = rpois(lambda=exp(nu),n=length(nu))

df &lt;- data.frame(X,y)

# Fit a Poisson regression
rv.pois = bayesreg(y~.,data=df, model="poisson", prior="hs", burnin=1e4, n.samples=1e4)
 
# Make a prediction for the first five rows
# By default this predicts the log-rate (i.e., the linear predictor)
predict(rv.pois,df[1:5,]) 

# This is the response (i.e., conditional mean of y)
exp(predict(rv.pois,df[1:5,])) 

# Same as above ... compare to the actual targets
cbind(exp(predict(rv.pois,df[1:5,])), y[1:5])

# Slightly different as E[exp(x)]!=exp(E[x])
predict(rv.pois,df[1:5,], type="response", bayes.avg=TRUE) 

# 99% credible interval for response
predict(rv.pois,df[1:5,], type="response", bayes.avg=TRUE, CI=99) 


# -----------------------------------------------------------------
# Example 4: Logistic regression on spambase
data(spambase)
 
# bayesreg expects binary targets to be factors
spambase$is.spam &lt;- factor(spambase$is.spam)
  
# First take a subset of the data (1/10th) for training, reserve the rest for testing
spambase.tr  = spambase[seq(1,nrow(spambase),10),]
spambase.tst = spambase[-seq(1,nrow(spambase),10),]
  
# Fit a model using logistic horseshoe for 2,000 samples
rv &lt;- bayesreg(is.spam ~ ., spambase.tr, model = "logistic", prior = "horseshoe", n.samples = 2e3)
  
# Summarise, sorting variables by their ranking importance
rv.s &lt;- summary(rv,sort.rank=TRUE)

# Make predictions about testing data -- get class predictions and class probabilities
y_pred &lt;- predict(rv, spambase.tst, type='class')
y_prob &lt;- predict(rv, spambase.tst, type='prob')

# Check how well our predictions did by generating confusion matrix
table(y_pred, spambase.tst$is.spam)

# Calculate logarithmic loss on test data
y_prob &lt;- predict(rv, spambase.tst, type='prob')
cat('Neg Log-Like for no Bayes average, posterior mean estimates: ', sum(-log(y_prob[,1])), '\n')
y_prob &lt;- predict(rv, spambase.tst, type='prob', sum.stat="median")
cat('Neg Log-Like for no Bayes average, posterior median estimates: ', sum(-log(y_prob[,1])), '\n')
y_prob &lt;- predict(rv, spambase.tst, type='prob', bayes.avg=TRUE)
cat('Neg Log-Like for Bayes average: ', sum(-log(y_prob[,1])), '\n')

## End(Not run)
</code></pre>

<hr>
<h2 id='spambase'>Spambase</h2><span id='topic+spambase'></span>

<h3>Description</h3>

<p>This is a well known dataset with a binary target obtainable from the UCI machine learning dataset archive. Each row is an e-mail, which is considered to be either spam or not spam. The dataset contains 48 attributes that measure the percentage of times a particular word appears in the email, 6 attributes that measure the percentage of times a particular character appeared in the email, plus three attributes measuring run-lengths of capital letters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(spambase)
</code></pre>


<h3>Format</h3>

<p>A data frame with 4,601 rows and 58 variables (1 categorical, 57 continuous). 
</p>

<dl>
<dt><code>is.spam</code></dt><dd><p>Is the email considered to be spam? (0=no,1=yes)</p>
</dd>
<dt><code>word.freq.make</code></dt><dd><p>Percentage of times the word 'make' appeared in the e-mail</p>
</dd>
<dt><code>word.freq.address</code></dt><dd><p>Percentage of times the word 'address' appeared in the e-mail</p>
</dd>
<dt><code>word.freq.all</code></dt><dd><p>Percentage of times the word 'all' appeared in the e-mail</p>
</dd>
<dt><code>word.freq.3d</code></dt><dd><p>Percentage of times the word '3d' appeared in the e-mail</p>
</dd>
<dt><code>word.freq.our</code></dt><dd><p>Percentage of times the word 'our' appeared in the e-mail</p>
</dd>
<dt><code>word.freq.over</code></dt><dd><p>Percentage of times the word 'over' appeared in the e-mail</p>
</dd>
<dt><code>word.freq.remove</code></dt><dd><p>Percentage of times the word 'remove' appeared in the e-mail</p>
</dd>
<dt><code>word.freq.internet</code></dt><dd><p>Percentage of times the word 'internet' appeared in the e-mail</p>
</dd>
<dt><code>word.freq.order</code></dt><dd><p>Percentage of times the word 'order' appeared in the e-mail</p>
</dd>
<dt><code>word.freq.mail</code></dt><dd><p>Percentage of times the word 'mail' appeared in the e-mail</p>
</dd>
<dt><code>word.freq.receive</code></dt><dd><p>Percentage of times the word 'receive' appeared in the e-mail</p>
</dd>
<dt><code>word.freq.will</code></dt><dd><p>Percentage of times the word 'will' appeared in the e-mail</p>
</dd>
<dt><code>word.freq.people</code></dt><dd><p>Percentage of times the word 'people' appeared in the e-mail</p>
</dd>
<dt><code>word.freq.report</code></dt><dd><p>Percentage of times the word 'report' appeared in the e-mail</p>
</dd>
<dt><code>word.freq.addresses</code></dt><dd><p>Percentage of times the word 'addresses' appeared in the e-mail</p>
</dd>
<dt><code>word.freq.free</code></dt><dd><p>Percentage of times the word 'free' appeared in the e-mail</p>
</dd>
<dt><code>word.freq.business</code></dt><dd><p>Percentage of times the word 'business' appeared in the e-mail</p>
</dd>
<dt><code>word.freq.email</code></dt><dd><p>Percentage of times the word 'email' appeared in the e-mail</p>
</dd>
<dt><code>word.freq.you</code></dt><dd><p>Percentage of times the word 'you' appeared in the e-mail</p>
</dd>
<dt><code>word.freq.credit</code></dt><dd><p>Percentage of times the word 'credit' appeared in the e-mail</p>
</dd>
<dt><code>word.freq.your</code></dt><dd><p>Percentage of times the word 'your' appeared in the e-mail</p>
</dd>
<dt><code>word.freq.font</code></dt><dd><p>Percentage of times the word 'font' appeared in the e-mail</p>
</dd>
<dt><code>word.freq.000</code></dt><dd><p>Percentage of times the word '000' appeared in the e-mail</p>
</dd>
<dt><code>word.freq.money</code></dt><dd><p>Percentage of times the word 'money' appeared in the e-mail</p>
</dd>
<dt><code>word.freq.hp</code></dt><dd><p>Percentage of times the word 'hp' appeared in the e-mail</p>
</dd>
<dt><code>word.freq.hpl</code></dt><dd><p>Percentage of times the word 'hpl' appeared in the e-mail</p>
</dd>
<dt><code>word.freq.george</code></dt><dd><p>Percentage of times the word 'george' appeared in the e-mail</p>
</dd>
<dt><code>word.freq.650</code></dt><dd><p>Percentage of times the word '650' appeared in the e-mail</p>
</dd>
<dt><code>word.freq.lab</code></dt><dd><p>Percentage of times the word 'lab' appeared in the e-mail</p>
</dd>
<dt><code>word.freq.labs</code></dt><dd><p>Percentage of times the word 'labs' appeared in the e-mail</p>
</dd>
<dt><code>word.freq.telnet</code></dt><dd><p>Percentage of times the word 'telnet' appeared in the e-mail</p>
</dd>
<dt><code>word.freq.857</code></dt><dd><p>Percentage of times the word '857' appeared in the e-mail</p>
</dd>
<dt><code>word.freq.data</code></dt><dd><p>Percentage of times the word 'data' appeared in the e-mail</p>
</dd>
<dt><code>word.freq.415</code></dt><dd><p>Percentage of times the word '415' appeared in the e-mail</p>
</dd>
<dt><code>word.freq.85</code></dt><dd><p>Percentage of times the word '85' appeared in the e-mail</p>
</dd>
<dt><code>word.freq.technology</code></dt><dd><p>Percentage of times the word 'technology' appeared in the e-mail</p>
</dd>
<dt><code>word.freq.1999</code></dt><dd><p>Percentage of times the word '1999' appeared in the e-mail</p>
</dd>
<dt><code>word.freq.parts</code></dt><dd><p>Percentage of times the word 'parts' appeared in the e-mail</p>
</dd>
<dt><code>word.freq.pm</code></dt><dd><p>Percentage of times the word 'pm' appeared in the e-mail</p>
</dd>
<dt><code>word.freq.direct</code></dt><dd><p>Percentage of times the word 'direct' appeared in the e-mail</p>
</dd>
<dt><code>word.freq.cs</code></dt><dd><p>Percentage of times the word 'cs' appeared in the e-mail</p>
</dd>
<dt><code>word.freq.meeting</code></dt><dd><p>Percentage of times the word 'meeting' appeared in the e-mail</p>
</dd>
<dt><code>word.freq.original</code></dt><dd><p>Percentage of times the word 'original' appeared in the e-mail</p>
</dd>
<dt><code>word.freq.project</code></dt><dd><p>Percentage of times the word 'project' appeared in the e-mail</p>
</dd>
<dt><code>word.freq.re</code></dt><dd><p>Percentage of times the word 're' appeared in the e-mail</p>
</dd>
<dt><code>word.freq.edu</code></dt><dd><p>Percentage of times the word 'edu' appeared in the e-mail</p>
</dd>
<dt><code>word.freq.table</code></dt><dd><p>Percentage of times the word 'table' appeared in the e-mail</p>
</dd>
<dt><code>word.freq.conference</code></dt><dd><p>Percentage of times the word 'conference' appeared in the e-mail</p>
</dd>
<dt><code>char.freq.;</code></dt><dd><p>Percentage of times the character ';' appeared in the e-mail</p>
</dd>
<dt><code>char.freq.(</code></dt><dd><p>Percentage of times the character '(' appeared in the e-mail</p>
</dd>
<dt><code>char.freq.[</code></dt><dd><p>Percentage of times the character '[' appeared in the e-mail</p>
</dd>
<dt><code>char.freq.!</code></dt><dd><p>Percentage of times the character '!' appeared in the e-mail</p>
</dd>
<dt><code>char.freq.$</code></dt><dd><p>Percentage of times the character '$' appeared in the e-mail</p>
</dd>
<dt><code>char.freq.#</code></dt><dd><p>Percentage of times the character '#' appeared in the e-mail</p>
</dd>
<dt><code>capital.run.length.average</code></dt><dd><p>Average length of contiguous runs of capital letters in the e-mail</p>
</dd>
<dt><code>capital.run.length.longest</code></dt><dd><p>Maximum length of contiguous runs of capital letters in the e-mail</p>
</dd>
<dt><code>capital.run.length.total</code></dt><dd><p>Total number of capital letters in the e-mail</p>
</dd>
</dl>



<h3>Source</h3>

<p><a href="https://archive.ics.uci.edu/ml/datasets/spambase/">https://archive.ics.uci.edu/ml/datasets/spambase/</a>
</p>

<hr>
<h2 id='summary.bayesreg'>Summarization method for Bayesian penalised regression (<code>bayesreg</code>) models</h2><span id='topic+summary.bayesreg'></span>

<h3>Description</h3>

<p><code>summary</code> method for Bayesian regression models fitted using <code><a href="#topic+bayesreg">bayesreg</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'bayesreg'
summary(
  object,
  sort.rank = FALSE,
  display.OR = FALSE,
  CI = 95,
  max.rows = NA,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="summary.bayesreg_+3A_object">object</code></td>
<td>
<p>An object of class <code>"bayesreg"</code> created as a result of a call to <code><a href="#topic+bayesreg">bayesreg</a></code>.</p>
</td></tr>
<tr><td><code id="summary.bayesreg_+3A_sort.rank">sort.rank</code></td>
<td>
<p>logical; if <code>TRUE</code>, the variables in the summary will be sorted by their importance as determined by their rank estimated by 
the Bayesian feature ranking algorithm.</p>
</td></tr>
<tr><td><code id="summary.bayesreg_+3A_display.or">display.OR</code></td>
<td>
<p>logical; if <code>TRUE</code>, the variables will be summarised in terms of their cross-sectional odds-ratios rather than their 
regression coefficients (logistic regression only).</p>
</td></tr>
<tr><td><code id="summary.bayesreg_+3A_ci">CI</code></td>
<td>
<p>numerical; the level of the credible interval reported in summary. Default is 95 (i.e., 95% credible interval).</p>
</td></tr>
<tr><td><code id="summary.bayesreg_+3A_max.rows">max.rows</code></td>
<td>
<p>numerical; the maximum number of rows (variables) to display in the summary. Default is to display all variables.</p>
</td></tr>
<tr><td><code id="summary.bayesreg_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns an object with the following fields:
</p>
<table role = "presentation">
<tr><td><code>log.l</code></td>
<td>
<p>The log-likelihood of the model at the posterior mean estimates of the regression coefficients.</p>
</td></tr>
<tr><td><code>waic</code></td>
<td>
<p>The Widely Applicable Information Criterion (WAIC) score of the model.</p>
</td></tr>
<tr><td><code>waic.dof</code></td>
<td>
<p>The effective degrees-of-freedom of the model, as estimated by the WAIC.</p>
</td></tr>
<tr><td><code>r2</code></td>
<td>
<p>For non-binary data, the R^2 statistic.</p>
</td></tr>
<tr><td><code>sd.error</code></td>
<td>
<p>For non-binary data, the estimated standard deviation of the errors.</p>
</td></tr>
<tr><td><code>p.r2</code></td>
<td>
<p>For binary data, the pseudo-R^2 statistic.</p>
</td></tr>
<tr><td><code>mu.coef</code></td>
<td>
<p>The posterior means of the regression coefficients.</p>
</td></tr>
<tr><td><code>se.coef</code></td>
<td>
<p>The posterior standard deviations of the regression coefficients.</p>
</td></tr>
<tr><td><code>CI.coef</code></td>
<td>
<p>The posterior credible interval for the regression coefficients, at the level specified (default: 95%).</p>
</td></tr>
<tr><td><code>med.OR</code></td>
<td>
<p>For binary data, the posterior median of the cross-sectional odds-ratios.</p>
</td></tr>
<tr><td><code>se.OR</code></td>
<td>
<p>For binary data, the posterior standard deviation of the cross-sectional odds-ratios.</p>
</td></tr>
<tr><td><code>CI.OR</code></td>
<td>
<p>For binary data, the posterior credible interval for the cross-sectional odds-ratios.</p>
</td></tr>
<tr><td><code>t.stat</code></td>
<td>
<p>The posterior t-statistic for the coefficients.</p>
</td></tr>
<tr><td><code>n.stars</code></td>
<td>
<p>The significance level for the variable (see above).</p>
</td></tr>
<tr><td><code>rank</code></td>
<td>
<p>The variable importance rank as estimated by the Bayesian feature ranking algorithm (see above).</p>
</td></tr>
<tr><td><code>ESS</code></td>
<td>
<p>The effective sample size for the variable.</p>
</td></tr>
<tr><td><code>log.l0</code></td>
<td>
<p>For binary data, the log-likelihood of the null model (i.e., with only an intercept).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>summary</code> method computes a number of summary statistics and displays these for each variable in a table, along 
with suitable header information.
</p>
<p>For continuous target variables, the header information includes a posterior estimate of the standard deviation of the random disturbances (errors), the <code class="reqn">R^2</code> statistic
and the Widely applicable information criterion (WAIC) statistic. For logistic regression models, the header information includes the negative 
log-likelihood at the posterior mean of the regression coefficients, the pseudo <code class="reqn">R^2</code> score and the WAIC statistic. For count
data (Poisson and geometric), the header information includes an estimate of the degree of overdispersion (observed variance divided by expected variance around the conditional mean, with a value &lt; 1 indicating underdispersion),
the pseudo <code class="reqn">R^2</code> score and the WAIC statistic.
</p>
<p>The main table summarises properties of the coefficients for each of the variables. The first column is the variable name. The 
second and third columns are either the mean and standard error of the coefficients, or the median and standard error of the 
cross-sectional odds-ratios if <code>display.OR=TRUE</code>. 
</p>
<p>The fourth and fifth columns are the end-points of the credible intervals of the coefficients (odds-ratios). The sixth column displays the 
posterior <code class="reqn">t</code>-statistic, calculated as the ratio of the posterior mean on the posterior standard deviation for the coefficient. 
The seventh column is the importance rank assigned to the variable by the Bayesian feature ranking algorithm. 
</p>
<p>In between the seventh and eighth columns are up to two asterisks indicating significance; a variable scores a first asterisk if 
the 75% credible interval does not include zero, and scores a second asterisk if the 95% credible interval does not include zero. The 
final column gives an estimate of the effective sample size for the variable, ranging from 0 to n.samples, which indicates the 
effective number of i.i.d draws from the posterior (if we could do this instead of using MCMC) represented by the samples
we have drawn. This quantity is computed using the algorithm presented in the Stan Bayesian sampling package documentation.
</p>


<h3>See Also</h3>

<p>The model fitting function <code><a href="#topic+bayesreg">bayesreg</a></code> and prediction function <code><a href="#topic+predict.bayesreg">predict.bayesreg</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
X = matrix(rnorm(100*20),100,20)
b = matrix(0,20,1)
b[1:9] = c(0,0,0,0,5,4,3,2,1)
y = X %*% b + rnorm(100, 0, 1)
df &lt;- data.frame(X,y)

# Horseshoe regression (using max 2 cores for CRAN check compliance)
rv.hs &lt;- bayesreg(y~.,df,prior="hs",n.cores=2)       

# Summarise without sorting by variable rank
rv.hs.s &lt;- summary(rv.hs)

# Summarise sorting by variable rank and provide 75% credible intervals
rv.hs.s &lt;- summary(rv.hs, sort.rank = TRUE, CI=75)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
