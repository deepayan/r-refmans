<!DOCTYPE html><html><head><title>Help for package quest</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {quest}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#.cronbach'><p>Bootstrap Function for <code>cronbach()</code> Function</p></a></li>
<li><a href='#.cronbachs'><p>Bootstrap Function for <code>cronbachs()</code> Function</p></a></li>
<li><a href='#.gtheory'><p>Bootstrap Function for <code>gtheory()</code> Function</p></a></li>
<li><a href='#.gtheorys'><p>Bootstrap Function for <code>gtheorys()</code> Function</p></a></li>
<li><a href='#add_sig'><p>Add Significance Symbols to a (Atomic) Vector, Matrix, or Array</p></a></li>
<li><a href='#add_sig_cor'><p>Add Significance Symbols to a Correlation Matrix</p></a></li>
<li><a href='#agg'><p>Aggregate an Atomic Vector by Group</p></a></li>
<li><a href='#agg_dfm'><p>Data Information by Group</p></a></li>
<li><a href='#aggs'><p>Aggregate Data by Group</p></a></li>
<li><a href='#amd_bi'><p>Amount of Missing Data - Bivariate (Pairwise Deletion)</p></a></li>
<li><a href='#amd_multi'><p>Amount of Missing Data - Multivariate (Listwise Deletion)</p></a></li>
<li><a href='#amd_uni'><p>Amount of Missing Data - Univariate</p></a></li>
<li><a href='#auto_by'><p>Autoregressive Coefficient by Group</p></a></li>
<li><a href='#ave_dfm'><p>Repeated Group Statistics for a Data-Frame</p></a></li>
<li><a href='#boot_ci'><p>Bootstrapped Confidence Intervals from a Matrix of Coefficients</p></a></li>
<li><a href='#by2'><p>Apply a Function to Data by Group</p></a></li>
<li><a href='#center'><p>Centering and/or Standardizing a Numeric Vector</p></a></li>
<li><a href='#center_by'><p>Centering and/or Standardizing a Numeric Vector by Group</p></a></li>
<li><a href='#centers'><p>Centering and/or Standardizing Numeric Data</p></a></li>
<li><a href='#centers_by'><p>Centering and/or Standardizing Numeric Data by Group</p></a></li>
<li><a href='#change'><p>Change Score from a Numeric Vector</p></a></li>
<li><a href='#change_by'><p>Change Scores from a Numeric Vector by Group</p></a></li>
<li><a href='#changes'><p>Change Scores from Numeric Data</p></a></li>
<li><a href='#changes_by'><p>Change Scores from Numeric Data by Group</p></a></li>
<li><a href='#colMeans_if'><p>Column Means Conditional on Frequency of Observed Values</p></a></li>
<li><a href='#colNA'><p>Frequency of Missing Values by Column</p></a></li>
<li><a href='#colSums_if'><p>Column Sums Conditional on Frequency of Observed Values</p></a></li>
<li><a href='#composite'><p>Composite Reliability of a Score</p></a></li>
<li><a href='#composites'><p>Composite Reliability of Multiple Scores</p></a></li>
<li><a href='#confint2'><p>Confidence Intervals from Statistical Information</p></a></li>
<li><a href='#confint2.boot'><p>Bootstrapped Confidence Intervals from a <code>boot</code> Object</p></a></li>
<li><a href='#confint2.default'><p>Confidence Intervals from Parameter Estimates and Standard Errors</p></a></li>
<li><a href='#cor_by'><p>Correlation Matrix by Group</p></a></li>
<li><a href='#cor_miss'><p>Point-biserial Correlations of Missingness</p></a></li>
<li><a href='#cor_ml'><p>Multilevel Correlation Matrices</p></a></li>
<li><a href='#corp'><p>Bivariate Correlations with Significant Symbols</p></a></li>
<li><a href='#corp_by'><p>Bivariate Correlations with Significant Symbols by Group</p></a></li>
<li><a href='#corp_miss'><p>Point-biserial Correlations of Missingness With Significant Symbols</p></a></li>
<li><a href='#corp_ml'><p><code>corp_ml</code> decomposes correlations from multilevel data into within-group</p>
and between-group correlations as well as adds significance symbols to the
end of each value. The workhorse of the function is
<code>statsBy</code>. <code>corp_ml</code> is simply a combination of
<code>cor_ml</code> and <code>add_sig_cor</code>.</a></li>
<li><a href='#covs_test'><p>Covariances Test of Significance</p></a></li>
<li><a href='#cronbach'><p>Cronbach's Alpha of a Set of Variables/Items</p></a></li>
<li><a href='#cronbachs'><p>Cronbach's Alpha for Multiple Sets of Variables/Items</p></a></li>
<li><a href='#decompose'><p>Decompose a Numeric Vector by Group</p></a></li>
<li><a href='#decomposes'><p>Decompose Numeric Data by Group</p></a></li>
<li><a href='#deff'><p>Design Effect from Multilevel Numeric Vector</p></a></li>
<li><a href='#deffs'><p>Design Effects from Multilevel Numeric Data</p></a></li>
<li><a href='#describe_ml'><p>Multilevel Descriptive Statistics</p></a></li>
<li><a href='#dum2nom'><p>Dummy Variables to a Nominal Variable</p></a></li>
<li><a href='#freq'><p>Univariate Frequency Table</p></a></li>
<li><a href='#freq_by'><p>Univariate Frequency Table By Group</p></a></li>
<li><a href='#freqs'><p>Multiple Univariate Frequency Tables</p></a></li>
<li><a href='#freqs_by'><p>Multiple Univariate Frequency Tables</p></a></li>
<li><a href='#gtheory'><p>Generalizability Theory Reliability of a Score</p></a></li>
<li><a href='#gtheory_ml'><p>Generalizability Theory Reliability of a Multilevel Score</p></a></li>
<li><a href='#gtheorys'><p>Generalizability Theory Reliability of Multiple Scores</p></a></li>
<li><a href='#gtheorys_ml'><p>Generalizability Theory Reliability of Multiple Multilevel Scores</p></a></li>
<li><a href='#icc_11'><p>Intraclass Correlation for Multilevel Analysis: ICC(1,1)</p></a></li>
<li><a href='#icc_all_by'><p>All Six Intraclass Correlations by Group</p></a></li>
<li><a href='#iccs_11'><p>Intraclass Correlation for Multiple Variables for Multilevel Analysis:</p>
ICC(1,1)</a></li>
<li><a href='#length_by'><p>Length of a (Atomic) Vector by Group</p></a></li>
<li><a href='#lengths_by'><p>Length of Data Columns by Group</p></a></li>
<li><a href='#long2wide'><p>Reshape Multiple Scores From Long to Wide</p></a></li>
<li><a href='#make.dummy'><p>Make Dummy Columns</p></a></li>
<li><a href='#make.dumNA'><p>Make Dummy Columns For Missing Data.</p></a></li>
<li><a href='#make.fun_if'><p>Make a Function Conditional on Frequency of Observed Values</p></a></li>
<li><a href='#make.latent'><p>Make Model Syntax for a Latent Factor in Lavaan</p></a></li>
<li><a href='#make.product'><p>Make Product Terms (e.g., interactions)</p></a></li>
<li><a href='#mean_change'><p>Mean Change Across Two Timepoints (dependent two-samples t-test)</p></a></li>
<li><a href='#mean_compare'><p>Mean differences for a single variable across 3+ independent groups (one-way</p>
ANOVA)</a></li>
<li><a href='#mean_diff'><p>Mean difference across two independent groups (independent two-samples</p>
t-test)</a></li>
<li><a href='#mean_if'><p>Mean Conditional on Minimum Frequency of Observed Values</p></a></li>
<li><a href='#mean_test'><p>Test for Sample Mean Against Mu (one-sample t-test)</p></a></li>
<li><a href='#means_change'><p>Mean Changes Across Two Timepoints For Multiple PrePost Pairs of Variables</p>
(dependent two-samples t-tests)</a></li>
<li><a href='#means_compare'><p>Mean differences for multiple variables across 3+ independent groups (one-way</p>
ANOVAs)</a></li>
<li><a href='#means_diff'><p>Mean differences across two independent groups (independent two-samples</p>
t-tests)</a></li>
<li><a href='#means_test'><p>Test for Multiple Sample Means Against Mu (one-sample t-tests)</p></a></li>
<li><a href='#mode2'><p>Statistical Mode of a Numeric Vector</p></a></li>
<li><a href='#n_compare'><p>Test for Equal Frequency of Values (chi-square test of goodness of fit)</p></a></li>
<li><a href='#ncases'><p>Number of Cases in Data</p></a></li>
<li><a href='#ncases_by'><p>Number of Cases in Data by Group</p></a></li>
<li><a href='#ncases_desc'><p>Describe Number of Cases in Data by Group</p></a></li>
<li><a href='#ncases_ml'><p>Multilevel Number of Cases</p></a></li>
<li><a href='#ngrp'><p>Number of Groups in Data</p></a></li>
<li><a href='#nhst'><p>Null Hypothesis Significance Testing</p></a></li>
<li><a href='#nom2dum'><p>Nominal Variable to Dummy Variables</p></a></li>
<li><a href='#nrow_by'><p>Number of Rows in Data by Group</p></a></li>
<li><a href='#nrow_ml'><p>Multilevel Number of Rows</p></a></li>
<li><a href='#partial.cases'><p>Find Partial Cases</p></a></li>
<li><a href='#pomp'><p>Recode a Numeric Vector to Percentage of Maximum Possible (POMP) Units</p></a></li>
<li><a href='#pomps'><p>Recode Numeric Data to Percentage of Maximum Possible (POMP) Units</p></a></li>
<li><a href='#prop_compare'><p>Proportion Comparisons for a Single Variable across 3+ Independent Groups</p>
(Chi-square Test of Independence)</a></li>
<li><a href='#prop_diff'><p>Proportion Difference for a Single Variable across Two Independent Groups</p>
(Chi-square Test of Independence)</a></li>
<li><a href='#prop_test'><p>Test for Sample Proportion Against Pi (chi-square test of goodness of fit)</p></a></li>
<li><a href='#props_compare'><p>Proportion Comparisons for Multiple Variables across 3+ Independent Groups</p>
(Chi-square Tests of Independence)</a></li>
<li><a href='#props_diff'><p>Proportion Difference of Multiple Variables Across Two Independent Groups</p>
(Chi-square Tests of Independence)</a></li>
<li><a href='#props_test'><p>Test for Multiple Sample Proportion Against Pi (Chi-square Tests of Goodness</p>
of Fit)</a></li>
<li><a href='#quest-package'><p>Pre-processing Questionnaire Data</p></a></li>
<li><a href='#recode2other'><p>Recode Unique Values in a Character Vector to 0ther (or NA)</p></a></li>
<li><a href='#recodes'><p>Recode Data</p></a></li>
<li><a href='#renames'><p>Rename Data Columns from a Codebook</p></a></li>
<li><a href='#reorders'><p>Reorder Levels of Factor Data</p></a></li>
<li><a href='#revalid'><p>Recode Invalid Values from a Vector</p></a></li>
<li><a href='#revalids'><p>Recode Invalid Values from Data</p></a></li>
<li><a href='#reverse'><p>Reverse Code a Numeric Vector</p></a></li>
<li><a href='#reverses'><p>Reverse Code Numeric Data</p></a></li>
<li><a href='#rowMeans_if'><p>Row Means Conditional on Frequency of Observed Values</p></a></li>
<li><a href='#rowNA'><p>Frequency of Missing Values by Row</p></a></li>
<li><a href='#rowsNA'><p>Frequency of Multiple Sets of Missing Values by Row</p></a></li>
<li><a href='#rowSums_if'><p>Row Sums Conditional on Frequency of Observed Values</p></a></li>
<li><a href='#score'><p>Observed Unweighted Scoring of a Set of Variables/Items</p></a></li>
<li><a href='#scores'><p>Observed Unweighted Scoring of Multiple Sets of Variables/Items</p></a></li>
<li><a href='#shift'><p>Shift a Vector (i.e., lag/lead)</p></a></li>
<li><a href='#shift_by'><p>Shift a Vector (i.e., lag/lead) by Group</p></a></li>
<li><a href='#shifts'><p>Shift Data (i.e., lag/lead)</p></a></li>
<li><a href='#shifts_by'><p>Shift Data (i.e., lag/lead) by Group</p></a></li>
<li><a href='#sum_if'><p>Sum Conditional on Minimum Frequency of Observed Values</p></a></li>
<li><a href='#summary_ucfa'><p>Summary of a Unidimensional Confirmatory Factor Analysis</p></a></li>
<li><a href='#tapply2'><p>Apply a Function to a (Atomic) Vector by Group</p></a></li>
<li><a href='#ucfa'><p>Unidimensional Confirmatory Factor Analysis</p></a></li>
<li><a href='#valid_test'><p>Test for Invalid Elements in a Vector</p></a></li>
<li><a href='#valids_test'><p>Test for Invalid Elements in Data</p></a></li>
<li><a href='#vecNA'><p>Frequency of Missing Values in a Vector</p></a></li>
<li><a href='#wide2long'><p>Reshape Multiple Sets of Variables From Wide to Long</p></a></li>
<li><a href='#winsor'><p>Winsorize a Numeric Vector</p></a></li>
<li><a href='#winsors'><p>Winsorize Numeric Data</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Prepare Questionnaire Data for Analysis</td>
</tr>
<tr>
<td>Version:</td>
<td>0.2.0</td>
</tr>
<tr>
<td>Description:</td>
<td>Offers a suite of functions to prepare questionnaire data for analysis (perhaps other types of data as well). By data preparation, I mean data analytic tasks to get your raw data ready for statistical modeling (e.g., regression). There are functions to investigate missing data, reshape data, validate responses, recode variables, score questionnaires, center variables, aggregate by groups, shift scores (i.e., leads or lags), etc. It provides functions for both single level and multilevel (i.e., grouped) data. With a few exceptions (e.g., ncases()), functions without an "s" at the end of their primary word (e.g., center_by()) act on atomic vectors, while functions with an "s" at the end of their primary word (e.g., centers_by()) act on multiple columns of a data.frame.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.0.0), datasets, stats, utils, methods</td>
</tr>
<tr>
<td>Imports:</td>
<td>str2str, abind, checkmate, plyr, car, psych, boot, MBESS,
nlme, lme4, multilevel, lavaan</td>
</tr>
<tr>
<td>Suggests:</td>
<td>reshape, psychTools, lmeInfo, semTools</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Collate:</td>
<td>'quest_functions.R' 'psymet_functions.R'
'describes_functions.R' 'diary_functions.R' 'mia_functions.R'</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-12-04 22:58:39 UTC; David Disabato</td>
</tr>
<tr>
<td>Author:</td>
<td>David Disabato <a href="https://orcid.org/0000-0001-7094-4996"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>David Disabato &lt;ddisab01@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-12-05 00:10:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='.cronbach'>Bootstrap Function for <code>cronbach()</code> Function</h2><span id='topic+.cronbach'></span>

<h3>Description</h3>

<p><code>.cronbach</code> is the function used by the <code><a href="boot.html#topic+boot">boot</a></code> function
within the <code><a href="#topic+cronbach">cronbach</a></code> function. It is primarily created to increase the
computational efficiency of bootstrap confidence intervals within the
<code>cronbach</code> function by doing only the minimal computations needed to
compute cronbach's alpha.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.cronbach(dat, i, use)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".cronbach_+3A_dat">dat</code></td>
<td>
<p>data.frame with only the items you wish to include in the cronbach's
alpha computation and no other variables.</p>
</td></tr>
<tr><td><code id=".cronbach_+3A_i">i</code></td>
<td>
<p>integer vector of length = <code>nrow(dat)</code> specifying which rows
should be included in the computation. When used by the <code>boot::boot</code> function
this argument will change with every new bootstrapped resample.</p>
</td></tr>
<tr><td><code id=".cronbach_+3A_use">use</code></td>
<td>
<p>character vector of length 1 specifying how missing data should be
handled when computing covariances. See <code>cov</code> for details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>double vector of length 1 providing cronbach's alpha
</p>


<h3>Examples</h3>

<pre><code class='language-R'>.cronbach(dat = attitude,
   i = sample(x = 1:nrow(attitude), size = nrow(attitude), replace = TRUE), use = "pairwise")
</code></pre>

<hr>
<h2 id='.cronbachs'>Bootstrap Function for <code>cronbachs()</code> Function</h2><span id='topic+.cronbachs'></span>

<h3>Description</h3>

<p><code>.cronbachs</code> is the function used by the <code><a href="boot.html#topic+boot">boot</a></code>
function within the <code>cronbachs</code> function. It is primarily created to
increase the computational efficiency of bootstrap confidence intervals
within the <code>cronbachs</code> function by doing only the minimal computations
needed to compute cronbach's alpha for each set of variables/items.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.cronbachs(dat, i, nm.list, use)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".cronbachs_+3A_dat">dat</code></td>
<td>
<p>data.frame of data. It can contain variables other than those used
for cronbach's alpha calculation.</p>
</td></tr>
<tr><td><code id=".cronbachs_+3A_i">i</code></td>
<td>
<p>integer vector of length = <code>nrow(dat)</code> specifying which rows
should be included in the computation. When used by the <code>boot::boot</code>
function this argument will change with every new bootstrapped resample.</p>
</td></tr>
<tr><td><code id=".cronbachs_+3A_nm.list">nm.list</code></td>
<td>
<p>list of character vectors specifying the sets of
variables/items associated with each of the cronbach's alpha calculations.</p>
</td></tr>
<tr><td><code id=".cronbachs_+3A_use">use</code></td>
<td>
<p>character vector of length 1 specifying how missing data should be
handled when computing covariances. See <code>cov</code> for details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>double vector of length = <code>length(nm.list)</code> providing cronbach's
alpha for each set of variables/items.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
dat0 &lt;- psych::bfi[1:250, ]
dat1 &lt;- str2str::pick(x = dat0, val = c("A1","C4","C5","E1","E2","O2","O5",
   "gender","education","age"), not = TRUE, nm = TRUE)
vrb_nm_list &lt;- lapply(X = str2str::sn(c("E","N","C","A","O")), FUN = function(nm) {
   str2str::pick(x = names(dat1), val = nm, pat = TRUE)})
.cronbachs(dat = dat1,
   i = sample(x = 1:nrow(dat1), size = nrow(dat1), replace = TRUE),
   nm.list = vrb_nm_list, use = "pairwise")
</code></pre>

<hr>
<h2 id='.gtheory'>Bootstrap Function for <code>gtheory()</code> Function</h2><span id='topic+.gtheory'></span>

<h3>Description</h3>

<p><code>.gtheory</code> is the function used by the <code><a href="boot.html#topic+boot">boot</a></code> function
within the <code><a href="#topic+gtheory">gtheory</a></code> function. It is primarily created to
increase the computational efficiency of bootstrap confidence intervals
within the <code>gtheory</code> function by doing only the minimal computations
needed to compute the generalizability theory coefficient.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.gtheory(dat, i, cross.vrb)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".gtheory_+3A_dat">dat</code></td>
<td>
<p>data.frame with only the variables/items you wish to include in
the generalizability theory coefficient and no other variables/items.</p>
</td></tr>
<tr><td><code id=".gtheory_+3A_i">i</code></td>
<td>
<p>integer vector of length = <code>nrow(dat)</code> specifying which rows
should be included in the computation. When used by the <code>boot::boot</code>
function this argument will change with every new bootstrapped resample.</p>
</td></tr>
<tr><td><code id=".gtheory_+3A_cross.vrb">cross.vrb</code></td>
<td>
<p>logical vector of length 1 specifying whether the
variables/items should be crossed when computing the generalizability
theory coefficient. If TRUE, then only the covariance structure of the
variables/items will be incorperated into the estimate of reliability. If
FALSE, then the mean structure of the variables/items will be incorperated.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>double vector of length 1 providing the generalizability theory
coefficient.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+.gtheorys">.gtheorys</a></code>
<code><a href="#topic+gtheory">gtheory</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>.gtheory(dat = attitude,
   i = sample(x = 1:nrow(attitude), size = nrow(attitude), replace = TRUE),
   cross.vrb = TRUE)
.gtheory(dat = attitude,
   i = sample(x = 1:nrow(attitude), size = nrow(attitude), replace = TRUE),
   cross.vrb = FALSE)
</code></pre>

<hr>
<h2 id='.gtheorys'>Bootstrap Function for <code>gtheorys()</code> Function</h2><span id='topic+.gtheorys'></span>

<h3>Description</h3>

<p><code>.gtheorys</code> is the function used by the <code><a href="boot.html#topic+boot">boot</a></code>
function within the <code><a href="#topic+gtheorys">gtheorys</a></code> function. It is primarily created
to increase the computational efficiency of bootstrap confidence intervals
within the <code>gtheorys</code> function by doing only the minimal computations
needed to compute the generalizability theory coefficient.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.gtheorys(dat, i, nm.list, cross.vrb)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".gtheorys_+3A_dat">dat</code></td>
<td>
<p>data.frame of data. It can contain variables other than those used
for generalizability theory coefficient calculation.</p>
</td></tr>
<tr><td><code id=".gtheorys_+3A_i">i</code></td>
<td>
<p>integer vector of length = <code>nrow(dat)</code> specifying which rows
should be included in the computation. When used by the <code>boot::boot</code>
function this argument will change with every new bootstrapped resample.</p>
</td></tr>
<tr><td><code id=".gtheorys_+3A_nm.list">nm.list</code></td>
<td>
<p>list of character vectors specifying the sets of
variables/items associated with each of the generalizability theory
coefficient calculations.</p>
</td></tr>
<tr><td><code id=".gtheorys_+3A_cross.vrb">cross.vrb</code></td>
<td>
<p>logical vector of length 1 specifying whether the
variables/items should be crossed when computing the generalizability
theory coefficient. If TRUE, then only the covariance structure of the
variables/items will be incorperated into the estimate of reliability. If
FALSE, then the mean structure of the variables/items will be incorperated.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>double vector of length = <code>length(nm.list)</code> providing the
generalizability theory coefficients.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+.gtheory">.gtheory</a></code>
<code><a href="#topic+gtheorys">gtheorys</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dat0 &lt;- psych::bfi[1:250, ]
dat1 &lt;- str2str::pick(x = dat0, val = c("A1","C4","C5","E1","E2","O2","O5",
   "gender","education","age"), not = TRUE, nm = TRUE)
vrb_nm_list &lt;- lapply(X = str2str::sn(c("E","N","C","A","O")), FUN = function(nm) {
   str2str::pick(x = names(dat1), val = nm, pat = TRUE)})
.gtheorys(dat = dat1,
   i = sample(x = 1:nrow(dat1), size = nrow(dat1), replace = TRUE),
   nm.list = vrb_nm_list, cross.vrb = TRUE)
.gtheorys(dat = dat1,
   i = sample(x = 1:nrow(dat1), size = nrow(dat1), replace = TRUE),
   nm.list = vrb_nm_list, cross.vrb = FALSE)
</code></pre>

<hr>
<h2 id='add_sig'>Add Significance Symbols to a (Atomic) Vector, Matrix, or Array</h2><span id='topic+add_sig'></span>

<h3>Description</h3>

<p><code>add_sig</code> adds symbols for various p-values cutoffs of statistical
significance. The function inputs a numeric vector, matrix, or array of
effect sizes (e.g., correlation matrix) and a numeric vector, matrix, or
array of p-values that correspond to the effect size (i.e., each row and
column match) and then returns a character vector, matrix, or array of effect
sizes with appended significance symbols. One of the primary applications of
this function is use within <code><a href="#topic+corp">corp</a></code> <code><a href="#topic+corp_by">corp_by</a></code>, and
<code><a href="#topic+corp_ml">corp_ml</a></code> for correlation matrices.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>add_sig(
  x,
  p,
  digits = 3,
  p.10 = "",
  p.05 = "*",
  p.01 = "**",
  p.001 = "***",
  lead.zero = FALSE,
  trail.zero = TRUE,
  plus = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="add_sig_+3A_x">x</code></td>
<td>
<p>double numeric vector of effect sizes for which statistical
significance is available.</p>
</td></tr>
<tr><td><code id="add_sig_+3A_p">p</code></td>
<td>
<p>double matrix of p-values for the effect sizes in <code>x</code> that are
matched by element index for vectors, by row and column index with
matrices, by row, column, and layer index for 3D arrays, etc. For example,
the p-value in the first row and second column of <code>p</code> is associated
with the effect size in the first row and second column of <code>x</code>. If
<code>x</code> and <code>p</code> do not have the same dimensions, an error is
returned.</p>
</td></tr>
<tr><td><code id="add_sig_+3A_digits">digits</code></td>
<td>
<p>integer vector of length 1 specifying the number of decimals to
round to.</p>
</td></tr>
<tr><td><code id="add_sig_+3A_p.10">p.10</code></td>
<td>
<p>character vector of length 1 specifying which symbol to append to
the end of any effect size significant at the p &lt; .10 level.</p>
</td></tr>
<tr><td><code id="add_sig_+3A_p.05">p.05</code></td>
<td>
<p>character vector of length 1 specifying which symbol to append to
the end of any effect size significant at the p &lt; .05 level.</p>
</td></tr>
<tr><td><code id="add_sig_+3A_p.01">p.01</code></td>
<td>
<p>character vector of length 1 specifying which symbol to append to
the end of any effect size significant at the p &lt; .01 level.</p>
</td></tr>
<tr><td><code id="add_sig_+3A_p.001">p.001</code></td>
<td>
<p>character vector of length 1 specifying which symbol to append
to the end of any effect size significant at the p &lt; .001 level.</p>
</td></tr>
<tr><td><code id="add_sig_+3A_lead.zero">lead.zero</code></td>
<td>
<p>logical vector of length 1 specifying whether to retain a
zero in front of the decimal place if the effect size is within 1 or -1.</p>
</td></tr>
<tr><td><code id="add_sig_+3A_trail.zero">trail.zero</code></td>
<td>
<p>logical vector of length 1 specifying whether to retain
zeros after the decimal place (due to rounding).</p>
</td></tr>
<tr><td><code id="add_sig_+3A_plus">plus</code></td>
<td>
<p>logical vector of length 1 specifying whether to include a plus
sign in front of positive effect sizes (minus signs are always in front of
negative effect sizes).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>There are several functions out there that do similar things. Here is one
posted to R-bloggers that does it for correlation matrices using the
<code>corr</code> function from the <code>Hmisc</code> package:
<a href="https://www.r-bloggers.com/2020/07/create-a-publication-ready-correlation-matrix-with-significance-levels-in-r/">https://www.r-bloggers.com/2020/07/create-a-publication-ready-correlation-matrix-with-significance-levels-in-r/</a>.
</p>


<h3>Value</h3>

<p>character vector, matrix, or array with the same dimensions as
<code>x</code> and <code>p</code> containing the effect sizes with their significance
symbols appended to the end of each value.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
corr_test &lt;- psych::corr.test(mtcars[1:5])
r &lt;- corr_test[["r"]]
p &lt;- corr_test[["p"]]
add_sig(x = r, p = p)
add_sig(x = r, p = p, digits = 2)
add_sig(x = r, p = p, lead.zero = TRUE, trail.zero = FALSE)
add_sig(x = r, p = p, plus = TRUE)
noquote(add_sig(x = r, p = p)) # no quotes for character elements

</code></pre>

<hr>
<h2 id='add_sig_cor'>Add Significance Symbols to a Correlation Matrix</h2><span id='topic+add_sig_cor'></span>

<h3>Description</h3>

<p><code>add_sig_cor</code> adds symbols for various p-values cutoffs of statistical
significance. The function inputs a correlation matrix and a numeric matrix
of p-values that correspond to the correlations (i.e., each row and column
match) and then returns a data.frame of correlations with appended
significance symbols. One of the primary applications of this function is use
within <code><a href="#topic+corp">corp</a></code> <code><a href="#topic+corp_by">corp_by</a></code>, and <code><a href="#topic+corp_ml">corp_ml</a></code>
for correlation matrices.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>add_sig_cor(
  r,
  p,
  digits = 3,
  p.10 = "",
  p.05 = "*",
  p.01 = "**",
  p.001 = "***",
  lead.zero = FALSE,
  trail.zero = TRUE,
  plus = FALSE,
  diags = FALSE,
  lower = TRUE,
  upper = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="add_sig_cor_+3A_r">r</code></td>
<td>
<p>double numeric matrix of correlation coefficients for which
statistical significance is available. Since its a correlation matrix, it
must be symmetrical and is expected to be a full matrix with all elements
included (not just lower or upper diagonals values included).</p>
</td></tr>
<tr><td><code id="add_sig_cor_+3A_p">p</code></td>
<td>
<p>double matrix of p-values for the correlations in <code>r</code> that are
matched by row and column index. For example, the p-value in the first row
and second column of <code>p</code> is associated with the correlation in the
first row and second column of <code>r</code>. If <code>r</code> and <code>p</code> do not
have the same dimensions, an error is returned.</p>
</td></tr>
<tr><td><code id="add_sig_cor_+3A_digits">digits</code></td>
<td>
<p>integer vector of length 1 specifying the number of decimals to
round to.</p>
</td></tr>
<tr><td><code id="add_sig_cor_+3A_p.10">p.10</code></td>
<td>
<p>character vector of length 1 specifying which symbol to append to
the end of any correlation significant at the p &lt; .10 level.</p>
</td></tr>
<tr><td><code id="add_sig_cor_+3A_p.05">p.05</code></td>
<td>
<p>character vector of length 1 specifying which symbol to append to
the end of any correlation significant at the p &lt; .05 level.</p>
</td></tr>
<tr><td><code id="add_sig_cor_+3A_p.01">p.01</code></td>
<td>
<p>character vector of length 1 specifying which symbol to append to
the end of any correlation significant at the p &lt; .01 level.</p>
</td></tr>
<tr><td><code id="add_sig_cor_+3A_p.001">p.001</code></td>
<td>
<p>character vector of length 1 specifying which symbol to append
to the end of any correlation significant at the p &lt; .001 level.</p>
</td></tr>
<tr><td><code id="add_sig_cor_+3A_lead.zero">lead.zero</code></td>
<td>
<p>logical vector of length 1 specifying whether to retain a
zero in front of the decimal place.</p>
</td></tr>
<tr><td><code id="add_sig_cor_+3A_trail.zero">trail.zero</code></td>
<td>
<p>logical vector of length 1 specifying whether to retain
zeros after the decimal place (due to rounding).</p>
</td></tr>
<tr><td><code id="add_sig_cor_+3A_plus">plus</code></td>
<td>
<p>logical vector of length 1 specifying whether to include a plus
sign in front of positive correlations (minus signs are always in front of
negative correlations).</p>
</td></tr>
<tr><td><code id="add_sig_cor_+3A_diags">diags</code></td>
<td>
<p>logical vector of length 1 specifying whether to retain the
values in the diagonal of the correlation matrix. If TRUE, then the
diagonal will be 1s with <code>digits</code> number of zeros after the decimal
place (and no significant symbols). If FALSE, then the diagonal will be NA.</p>
</td></tr>
<tr><td><code id="add_sig_cor_+3A_lower">lower</code></td>
<td>
<p>logical vector of length 1 specifying whether to retain the
lower triangle of the correlation matrix. If TRUE, then the lower triangle
correlations and their significance symbols are retained. If FAlSE, then
the lower triangle will all be NA.</p>
</td></tr>
<tr><td><code id="add_sig_cor_+3A_upper">upper</code></td>
<td>
<p>logical vector of length 1 specifying whether to retain the
upper triangle of the correlation matrix. If TRUE, then the upper triangle
correlations and their significance symbols are retained. If FAlSE, then
the upper triangle will all be NA.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>There are several functions out there that do similar things. Here is one
posted to R-bloggers that uses the <code>corr</code> function from the <code>Hmisc</code>
package:
<a href="https://www.r-bloggers.com/2020/07/create-a-publication-ready-correlation-matrix-with-significance-levels-in-r/">https://www.r-bloggers.com/2020/07/create-a-publication-ready-correlation-matrix-with-significance-levels-in-r/</a>.
</p>


<h3>Value</h3>

<p>data.frame with the same dimensions as <code>r</code> containing the
correlations and their significance symbols. Elements may or may not contain NA
values depending on the arguments <code>diags</code>, <code>lower</code>, and
<code>upper</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
corr_test &lt;- psych::corr.test(mtcars[1:5])
r &lt;- corr_test[["r"]]
p &lt;- corr_test[["p"]]
add_sig_cor(r = r, p = p)
add_sig_cor(r = r, p = p, digits = 2)
add_sig_cor(r = r, p = p, diags = TRUE)
add_sig_cor(r = r, p = p, lower = FALSE, upper = TRUE)
add_sig_cor(r = r, p = p, lead.zero = TRUE, trail.zero = FALSE)
add_sig_cor(r = r, p = p, plus = TRUE)

</code></pre>

<hr>
<h2 id='agg'>Aggregate an Atomic Vector by Group</h2><span id='topic+agg'></span>

<h3>Description</h3>

<p><code>agg</code> evaluates a function separately for each group and combines the
results back together into an atomic vector of data.frame that is returned.
Depending on the argument <code>rep</code>, the results of <code>fun</code> are repeated
for each element of <code>x</code> in the group (TRUE) or only once for each group
(FALSE). Depending on the argument <code>rtn.grp</code>, the return object is a
data.frame and the groups within <code>grp</code> are included in the data.frame as
columns (TRUE) or the return object is an atomic vector and the groups are
the names (FALSE).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>agg(x, grp, rep = TRUE, rtn.grp = !rep, sep = "_", fun, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="agg_+3A_x">x</code></td>
<td>
<p>atomic vector.</p>
</td></tr>
<tr><td><code id="agg_+3A_grp">grp</code></td>
<td>
<p>atomic vector or list of atomic vectors (e.g., data.frame)
specifying the groups. The atomic vector(s) must be the length of <code>x</code>
or else an error is returned.</p>
</td></tr>
<tr><td><code id="agg_+3A_rep">rep</code></td>
<td>
<p>logical vector of length 1 specifying whether the result of
<code>fun</code> should be repeated for every instance of the group in <code>x</code>
(TRUE) or only once for each group (FALSE).</p>
</td></tr>
<tr><td><code id="agg_+3A_rtn.grp">rtn.grp</code></td>
<td>
<p>logical vector of length 1 specifying whether the groups
(i.e., <code>grp</code>) should be included in the return object as columns. The
default is the opposite of <code>rep</code> as traditionally it is most important
to return the group columns when <code>rep</code> = FALSE.</p>
</td></tr>
<tr><td><code id="agg_+3A_sep">sep</code></td>
<td>
<p>character vector of length 1 specifying what string should
separate different group values when naming the return object. This
argument is only used if <code>grp</code> is a list of atomic vectors (e.g.,
data.frame) AND <code>rep</code> = FALSE AND <code>rtn.grp</code> = FALSE.</p>
</td></tr>
<tr><td><code id="agg_+3A_fun">fun</code></td>
<td>
<p>function to use for aggregation. This function is expected to
return an atomic vector of length 1.</p>
</td></tr>
<tr><td><code id="agg_+3A_...">...</code></td>
<td>
<p>additional named arguments to <code>fun</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>rep</code> = TRUE, then <code>agg</code> calls <code>ave</code>; if <code>rep</code> =
FALSE, then <code>agg</code> calls <code>aggregate</code>.
</p>


<h3>Value</h3>

<p>result of <code>fun</code> applied to <code>x</code> for each group
within <code>grp</code>. The structure of the return object depends on the
arguments <code>rep</code> and <code>rtn.grp</code>:
</p>

<dl>
<dt>If rep = TRUE and rtn.grp = TRUE:</dt><dd><p>then the return
object is a data.frame with nrow = <code>nrow(data)</code> where the first
columns are <code>grp</code> and the last column is the result of <code>fun</code>. If
<code>grp</code> is not a list with names, then its colnames will be &quot;Group.1&quot;,
&quot;Group.2&quot;, &quot;Group.3&quot; etc. similar to <code>aggregate</code>'s return object. The
colname for the result of <code>fun</code> will be &quot;x&quot;.</p>
</dd>
<dt>If rep = TRUE and rtn.grp = FALSE:</dt><dd><p>then the return
object is an atomic vector with length = <code>length(x)</code> where the values
are the result of <code>fun</code> and the names = <code>names(x)</code>.</p>
</dd>
<dt>If rep = FALSE and rtn.grp = TRUE:</dt><dd><p>then the return
object is a data.frame with nrow =
<code>length(levels(interaction(grp)))</code>
where the first columns are the unique group combinations in <code>grp</code> and
the last column is the result of <code>fun</code>. If <code>grp</code> is not a list
with names, then its colnames will be &quot;Group.1&quot;, &quot;Group.2&quot;, &quot;Group.3&quot; etc.
similar to <code>aggregate</code>'s return object. The colname for the result of
<code>fun</code> will be &quot;x&quot;.</p>
</dd>
<dt>If rep = FALSE and rtn.grp = FALSE:</dt><dd><p>then the return
object is an atomic vector with length
<code>length(levels(interaction(grp)))</code> where the values are the result of
<code>fun</code> and the names are each group value pasted together by <code>sep</code>
if there are multiple grouping variables within <code>grp</code> (i.e.,
<code>is.list(grp) &amp;&amp; length(grp) &gt; 2</code>).</p>
</dd>
</dl>



<h3>See Also</h3>

<p><code><a href="#topic+aggs">aggs</a></code>,
<code><a href="#topic+agg_dfm">agg_dfm</a></code>,
<code><a href="stats.html#topic+ave">ave</a></code>,
<code><a href="stats.html#topic+aggregate">aggregate</a></code>,
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# one grouping variable
agg(x = airquality$"Solar.R", grp = airquality$"Month", fun = mean)
agg(x = airquality$"Solar.R", grp = airquality$"Month", fun = mean,
   na.rm = TRUE) # ignoring missing values
agg(x = setNames(airquality$"Solar.R", nm = row.names(airquality)), grp = airquality$"Month",
   fun = mean, na.rm = TRUE) # keeps the names in the return object
agg(x = airquality$"Solar.R", grp = airquality$"Month", rep = FALSE,
   fun = mean, na.rm = TRUE) # do NOT repeat aggregated values
agg(x = airquality$"Solar.R", grp = airquality$"Month", rep = FALSE, rtn.grp = FALSE,
   fun = mean, na.rm = TRUE) # groups are the names of the returned atomic vector

# two grouping variables
tmp_nm &lt;- c("vs","am") # Roxygen2 doesn't like a c() within a []
agg(x = mtcars$"mpg", grp = mtcars[tmp_nm], rep = TRUE, fun = sd)
agg(x = mtcars$"mpg", grp = mtcars[tmp_nm], rep = FALSE,
   fun = sd) # do NOT repeat aggregated values
agg(x = mtcars$"mpg", grp = mtcars[tmp_nm], rep = FALSE, rtn.grp = FALSE,
   fun = sd) # groups are the names of the returned atomic vector
agg(x = mtcars$"mpg", grp = mtcars[tmp_nm], rep = FALSE, rtn.grp = FALSE,
   sep = ".", fun = sd) # change the separater for naming

# error messages
## Not run: 
   agg(x = airquality$"Solar.R", grp = mtcars[tmp_nm]) # error returned
   # b/c  atomic vectors within \code{grp} not having the same length as \code{x}

## End(Not run)

</code></pre>

<hr>
<h2 id='agg_dfm'>Data Information by Group</h2><span id='topic+agg_dfm'></span>

<h3>Description</h3>

<p><code>agg_dfm</code> evaluates a function on a set of variables in a data.frame
separately for each group and combines the results back together. The
<code>rep</code> and <code>rtn.grp</code> arguments determine exactly how the results are
combined together. If <code>rep</code> = TRUE, then the result of <code>fun</code> is
repeated for every row of the group in <code>data[grp.nm]</code>; If <code>rep</code> =
FALSE, then the result of <code>fun</code> for each unique combination of
<code>data[grp.nm]</code> is returned once. If <code>rtn.grp</code> = TRUE, then the
results are returned in a data.frame where the first columns are the groups
from <code>data[grp.nm]</code>; If <code>rtn.grp</code> = FALSE, then the results are
returned in an atomic vector. Note, <code>agg_dfm</code> evaluates <code>fun</code> on
all the variables in <code>data[vrb.nm]</code> as a whole, If instead, you want to
evaluate <code>fun</code> separately for variable <code>vrb.nm</code> in <code>data</code>,
then use <code>Agg</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>agg_dfm(
  data,
  vrb.nm,
  grp.nm,
  rep = FALSE,
  rtn.grp = !rep,
  sep = ".",
  rtn.result.nm = "result",
  fun,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="agg_dfm_+3A_data">data</code></td>
<td>
<p>data.frame of data.</p>
</td></tr>
<tr><td><code id="agg_dfm_+3A_vrb.nm">vrb.nm</code></td>
<td>
<p>character vector of colnames from <code>data</code> specifying the
set of variables to evaluate <code>fun</code> on.</p>
</td></tr>
<tr><td><code id="agg_dfm_+3A_grp.nm">grp.nm</code></td>
<td>
<p>character vector of colnames from <code>data</code> specifying the
groups.</p>
</td></tr>
<tr><td><code id="agg_dfm_+3A_rep">rep</code></td>
<td>
<p>logical vector of length 1 specifying whether the result of
<code>fun</code> should be repeated for every instance of the group in
<code>data[vrb.nm]</code> (TRUE) or only once for each group (FALSE).</p>
</td></tr>
<tr><td><code id="agg_dfm_+3A_rtn.grp">rtn.grp</code></td>
<td>
<p>logical vector of length 1 specifying whether the group
columns (i.e., <code>data[grp.nm]</code>) should be included in the return object
as columns. The default is the opposite of <code>rep</code> as traditionally it
is most important to return the group columns when <code>rep</code> = FALSE.</p>
</td></tr>
<tr><td><code id="agg_dfm_+3A_sep">sep</code></td>
<td>
<p>character vector of length 1 specifying the string to paste the
group values together with when there are multiple grouping variables
(i.e., <code>length(grp.nm) &gt; 1</code>). Only used if <code>rep</code> = FALSE and
<code>rtn.grp</code> = FALSE.</p>
</td></tr>
<tr><td><code id="agg_dfm_+3A_rtn.result.nm">rtn.result.nm</code></td>
<td>
<p>character vector of length 1 specifying the name for the
column of results in the return object. Only used if <code>rtn.grp</code> = TRUE.</p>
</td></tr>
<tr><td><code id="agg_dfm_+3A_fun">fun</code></td>
<td>
<p>function to evaluate each grouping of <code>data[vrb.nm]</code> by. This
function must return an atomic vector of length 1. If not, then consider
using <code>by2</code> or <code>plyr::dlply</code>.</p>
</td></tr>
<tr><td><code id="agg_dfm_+3A_...">...</code></td>
<td>
<p>additional named arguments to <code>fun</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>rep</code> = TRUE, then <code>agg_dfm</code> calls <code>ave_dfm</code>; if <code>rep</code>
= FALSE, then <code>agg_dfm</code> calls <code>by</code>. When <code>rep</code> = FALSE and
<code>rtn.grp</code> = TRUE, <code>agg_dfm</code> is very similar to <code>plyr::ddply</code>;
when <code>rep</code> = FALSE and <code>rtn.grp</code> = FALSE, then <code>agg_dfm</code> is
very similar to <code>plyr::daply</code>.
</p>


<h3>Value</h3>

<p>result of <code>fun</code> applied to each grouping of
<code>data[vrb.nm]</code>. The structure of the return object depends on the
arguments <code>rep</code> and <code>rtn.grp</code>.
</p>

<dl>
<dt>If rep = TRUE and rtn.grp = TRUE:</dt><dd><p>then the return
object is a data.frame with nrow = <code>nrow(data)</code> where the first
columns are <code>data[grp.nm]</code> and the last column is the result of
<code>fun</code> with colname = <code>rtn.result.nm</code>.</p>
</dd>
<dt>If rep = TRUE and rtn.grp = FALSE:</dt><dd><p>then the return
object is an atomic vector with length = <code>nrow(data)</code> where the values
are the result of <code>fun</code> and the names = <code>row.names(data)</code>.</p>
</dd>
<dt>If rep = FALSE and rtn.grp = TRUE:</dt><dd><p>then the return
object is a data.frame with nrow =
<code>length(levels(interaction(data[grp.nm])))</code> where the first columns
are the unique group combinations in <code>data[grp.nm]</code> and the last
column is the result of <code>fun</code> with colname = <code>rtn.result.nm</code>.</p>
</dd>
<dt>If rep = FALSE and rtn.grp = FALSE:</dt><dd><p>then the return
object is an atomic vector with length
<code>length(levels(interaction(data[grp.nm])))</code> where the values are the
result of <code>fun</code> and the names are each group value pasted together by
<code>sep</code> if there are multiple grouping variables (i.e.,
<code>length(grp.nm)</code> &gt; 2).</p>
</dd>
</dl>



<h3>See Also</h3>

<p><code><a href="#topic+agg">agg</a></code>
<code><a href="#topic+aggs">aggs</a></code>
<code><a href="#topic+by2">by2</a></code>
<code><a href="plyr.html#topic+ddply">ddply</a></code>
<code><a href="plyr.html#topic+daply">daply</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
### one grouping variable

## by in base R
by(data = airquality[c("Ozone","Solar.R")], INDICES = airquality["Month"],
   simplify = FALSE, FUN = function(dat) cor(dat, use = "complete")[1,2])

## rep = TRUE

# rtn.group = TRUE
agg_dfm(data = airquality, vrb.nm = c("Ozone","Solar.R"), grp.nm = "Month",
   rep = TRUE, rtn.grp = TRUE, fun = function(dat) cor(dat, use = "complete")[1,2])

# rtn.group = FALSE
agg_dfm(data = airquality, vrb.nm = c("Ozone","Solar.R"), grp.nm = "Month",
   rep = TRUE, rtn.grp = FALSE, fun = function(dat) cor(dat, use = "complete")[1,2])

## rep = FALSE

# rtn.group = TRUE
agg_dfm(data = airquality, vrb.nm = c("Ozone","Solar.R"), grp.nm = "Month",
   rep = FALSE, rtn.grp = TRUE, fun = function(dat) cor(dat, use = "complete")[1,2])
suppressWarnings(plyr::ddply(.data = airquality[c("Ozone","Solar.R","Month")],
   .variables = "Month", .fun = function(dat) cor(dat, use = "complete")[1,2]))

# rtn.group = FALSE
agg_dfm(data = airquality, vrb.nm = c("Ozone","Solar.R"), grp.nm = "Month",
   rep = FALSE, rtn.grp = FALSE, fun = function(dat) cor(dat, use = "complete")[1,2])
suppressWarnings(plyr::daply(.data = airquality[c("Ozone","Solar.R","Month")],
   .variables = "Month", .fun = function(dat) cor(dat, use = "complete")[1,2]))

### two grouping variables

## by in base R
by(data = mtcars[c("mpg","cyl","disp")], INDICES = mtcars[c("vs","am")],
   FUN = nrow, simplify = FALSE) # with multiple group columns

## rep = TRUE

# rtn.grp = TRUE
agg_dfm(data = mtcars, vrb.nm = c("mpg","cyl","disp"), grp.nm = c("vs","am"),
   rep = TRUE, rtn.grp = TRUE, fun = nrow)

# rtn.grp = FALSE
agg_dfm(data = mtcars, vrb.nm = c("mpg","cyl","disp"), grp.nm = c("vs","am"),
   rep = TRUE, rtn.grp = FALSE, fun = nrow)

## rep = FALSE

# rtn.grp = TRUE
agg_dfm(data = mtcars, vrb.nm = c("mpg","cyl","disp"), grp.nm = c("vs","am"),
   rep = FALSE, rtn.grp = TRUE, fun = nrow)
agg_dfm(data = mtcars, vrb.nm = c("mpg","cyl","disp"), grp.nm = c("vs","am"),
   rep = FALSE, rtn.grp = TRUE, rtn.result.nm = "value", fun = nrow)

# rtn.grp = FALSE
agg_dfm(data = mtcars, vrb.nm = c("mpg","cyl","disp"), grp.nm = c("vs","am"),
   rep = FALSE, rtn.grp = FALSE, fun = nrow)
agg_dfm(data = mtcars, vrb.nm = c("mpg","cyl","disp"), grp.nm = c("vs","am"),
   rep = FALSE, rtn.grp = FALSE, sep = "_", fun = nrow)

</code></pre>

<hr>
<h2 id='aggs'>Aggregate Data by Group</h2><span id='topic+aggs'></span>

<h3>Description</h3>

<p><code>aggs</code> evaluates a function separately for each group and combines the
results back together into a data.frame that is returned. Depending on
<code>rep</code>, the results of <code>fun</code> are repeated for each element of
<code>data[vrb.nm]</code> in the group (TRUE) or only once for each group (FALSE).
Note, <code>aggs</code> evaluates <code>fun</code> separately for each variable
<code>vrb.nm</code> within <code>data</code>. If instead, you want to evaluate <code>fun</code>
for variables as a set <code>data[vrb.nm]</code>, then use <code>agg_dfm</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>aggs(
  data,
  vrb.nm,
  grp.nm,
  rep = TRUE,
  rtn.grp = !rep,
  sep = "_",
  suffix = "_a",
  fun,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="aggs_+3A_data">data</code></td>
<td>
<p>data.frame of data.</p>
</td></tr>
<tr><td><code id="aggs_+3A_vrb.nm">vrb.nm</code></td>
<td>
<p>character vector of colnames from <code>data</code> specifying the
variables.</p>
</td></tr>
<tr><td><code id="aggs_+3A_grp.nm">grp.nm</code></td>
<td>
<p>character vector of colnames from <code>data</code> specifying the
groups.</p>
</td></tr>
<tr><td><code id="aggs_+3A_rep">rep</code></td>
<td>
<p>logical vector of length 1 specifying whether the result of
<code>fun</code> should be repeated for every instance of the group in
<code>data[vrb.nm]</code> (TRUE) or only once for each group (FALSE).</p>
</td></tr>
<tr><td><code id="aggs_+3A_rtn.grp">rtn.grp</code></td>
<td>
<p>logical vector of length 1 specifying whether the group
columns (i.e., <code>data[grp.nm]</code>) should be included in the return object
as columns. The default is the opposite of <code>rep</code> as traditionally it
is most important to return the group columns when <code>rep</code> = FALSE.</p>
</td></tr>
<tr><td><code id="aggs_+3A_sep">sep</code></td>
<td>
<p>character vector of length 1 specifying what string should
separate different group values when naming the return object. This
argument is only used if <code>grp.nm</code> has length &gt; 1 AND <code>rep</code> =
FALSE AND <code>rtn.grp</code> = FALSE.</p>
</td></tr>
<tr><td><code id="aggs_+3A_suffix">suffix</code></td>
<td>
<p>character vector of length 1 specifying the string to append to
the end of the colnames in the return object.</p>
</td></tr>
<tr><td><code id="aggs_+3A_fun">fun</code></td>
<td>
<p>function to use for aggregation. This function is expected to
return an atomic vector of length 1.</p>
</td></tr>
<tr><td><code id="aggs_+3A_...">...</code></td>
<td>
<p>additional named arguments to <code>fun</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>rep</code> = TRUE, then <code>agg</code> calls <code>ave</code>; if <code>rep</code> =
FALSE, then <code>agg</code> calls <code>aggregate</code>.
</p>


<h3>Value</h3>

<p>data.frame of aggregated values. If <code>rep</code> is TRUE, then nrow =
<code>nrow(data)</code>. If <code>rep</code> = FALSE, then nrow =
<code>length(levels(interaction(data[grp.nm])))</code>. The names are specified
by <code>paste0(vrb.nm, suffix)</code>. If <code>rtn.grp</code> = TRUE, then the group
columns are appended to the begining of the data.frame.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+agg">agg</a></code>,
<code><a href="#topic+agg_dfm">agg_dfm</a></code>,
<code><a href="stats.html#topic+ave">ave</a></code>,
<code><a href="stats.html#topic+aggregate">aggregate</a></code>,
</p>


<h3>Examples</h3>

<pre><code class='language-R'>aggs(data = airquality, vrb.nm = c("Ozone","Solar.R"), grp.nm = "Month",
   fun = mean, na.rm = TRUE)
aggs(data = airquality, vrb.nm = c("Ozone","Solar.R"), grp.nm = "Month",
   rtn.grp = TRUE, fun = mean, na.rm = TRUE) # include the group columns
aggs(data = airquality, vrb.nm = c("Ozone","Solar.R"), grp.nm = "Month",
   rep = FALSE, fun = mean, na.rm = TRUE) # do NOT repeat aggregated values
aggs(data = mtcars, vrb.nm = c("mpg","cyl","disp"), grp.nm = c("vs","am"),
   rep = FALSE, fun = mean, na.rm = TRUE) # with multiple group columns
aggs(data = mtcars, vrb.nm = c("mpg","cyl","disp"), grp.nm = c("vs","am"),
   rep = FALSE, rtn.grp = FALSE, fun = mean, na.rm = TRUE) # without returning groups
</code></pre>

<hr>
<h2 id='amd_bi'>Amount of Missing Data - Bivariate (Pairwise Deletion)</h2><span id='topic+amd_bi'></span>

<h3>Description</h3>

<p><code>amd_bi</code> by default computes the proportion of missing data for pairs of
variables in a data.frame, with arguments to allow for counts instead of
proportions (i.e., <code>prop</code>) or observed data rather than missing data
(i.e., <code>ov</code>). It is bivariate in that each pair of variables is treated
in isolation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>amd_bi(data, vrb.nm, prop = TRUE, ov = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="amd_bi_+3A_data">data</code></td>
<td>
<p>data.frame of data.</p>
</td></tr>
<tr><td><code id="amd_bi_+3A_vrb.nm">vrb.nm</code></td>
<td>
<p>character vector of the colnames from <code>data</code> specifying
the variables.</p>
</td></tr>
<tr><td><code id="amd_bi_+3A_prop">prop</code></td>
<td>
<p>logical vector of length 1 specifying whether the frequency of
missing values should be returned as a proportion (TRUE) or a count
(FALSE).</p>
</td></tr>
<tr><td><code id="amd_bi_+3A_ov">ov</code></td>
<td>
<p>logical vector of length 1 specifying whether the frequency of
observed values (TRUE) should be returned rather than the frequency of
missing values (FALSE).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data.frame of nrow = ncol = <code>length(vrb.nm)</code> and rowames =
colnames = <code>vrb.nm</code> providing the frequency of missing (or observed if
<code>ov</code> = TRUE) values per pair of variables. If <code>prop</code> = TRUE, the
values will range from 0 to 1. If <code>prop</code> = FALSE, the values will
range from 0 to <code>nrow(data)</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+amd_bi">amd_bi</a></code>
<code><a href="#topic+amd_multi">amd_multi</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
amd_bi(data = airquality, vrb.nm = names(airquality)) # proportion of missing data
amd_bi(data = airquality, vrb.nm = names(airquality),
   ov = TRUE) # proportion of observed data
amd_bi(data = airquality, vrb.nm = names(airquality),
   prop = FALSE) # count of missing data
amd_bi(data = airquality, vrb.nm = names(airquality),
   prop = FALSE, ov = TRUE) # count of observed data

</code></pre>

<hr>
<h2 id='amd_multi'>Amount of Missing Data - Multivariate (Listwise Deletion)</h2><span id='topic+amd_multi'></span>

<h3>Description</h3>

<p><code>amd_multi</code> by default computes the proportion of missing data from
listwise deletion for a set of variables in a data.frame, with arguments to
allow for counts instead of proportions (i.e., <code>prop</code>) or observed data
rather than missing data (i.e., <code>ov</code>). It is multivariate in that the
variables are treated together as a set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>amd_multi(data, vrb.nm, prop = TRUE, ov = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="amd_multi_+3A_data">data</code></td>
<td>
<p>data.frame of data.</p>
</td></tr>
<tr><td><code id="amd_multi_+3A_vrb.nm">vrb.nm</code></td>
<td>
<p>character vector of the colnames from <code>data</code> specifying
the variables.</p>
</td></tr>
<tr><td><code id="amd_multi_+3A_prop">prop</code></td>
<td>
<p>logical vector of length 1 specifying whether the frequency of
missing values should be returned as a proportion (TRUE) or a count
(FALSE).</p>
</td></tr>
<tr><td><code id="amd_multi_+3A_ov">ov</code></td>
<td>
<p>logical vector of length 1 specifying whether the frequency of
observed values (TRUE) should be returned rather than the frequency of
missing values (FALSE).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>numeric vector of length 1 providing the frequency of missing (or
observed if <code>ov</code> = TRUE) rows from listwise deletion for the set of
variables <code>vrb.nm</code>. If <code>prop</code> = TRUE, the value will range from 0
to 1. If <code>prop</code> = FALSE, the value will range from 0 to
<code>nrow(data)</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+amd_uni">amd_uni</a></code>
<code><a href="#topic+amd_bi">amd_bi</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
amd_multi(airquality, vrb.nm = names(airquality)) # proportion of missing data
amd_multi(airquality, vrb.nm = names(airquality),
   ov = TRUE) # proportion of observed data
amd_multi(airquality, vrb.nm = names(airquality),
   prop = FALSE) # count of missing data
amd_multi(airquality, vrb.nm = names(airquality),
   prop = FALSE, ov = TRUE) # count of observed data

</code></pre>

<hr>
<h2 id='amd_uni'>Amount of Missing Data - Univariate</h2><span id='topic+amd_uni'></span>

<h3>Description</h3>

<p><code>amd_uni</code> by default computes the proportion of missing data for
variables in a data.frame, with arguments to allow for counts instead of
proportions (i.e., <code>prop</code>) or observed data rather than missing data
(i.e., <code>ov</code>). It is univariate in that each variable is treated in
isolation. <code>amd_uni</code> is a simple wrapper for <code><a href="#topic+colNA">colNA</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>amd_uni(data, vrb.nm, prop = TRUE, ov = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="amd_uni_+3A_data">data</code></td>
<td>
<p>data.frame of data.</p>
</td></tr>
<tr><td><code id="amd_uni_+3A_vrb.nm">vrb.nm</code></td>
<td>
<p>character vector of the colnames from <code>data</code> specifying
the variables.</p>
</td></tr>
<tr><td><code id="amd_uni_+3A_prop">prop</code></td>
<td>
<p>logical vector of length 1 specifying whether the frequency of
missing values should be returned as a proportion (TRUE) or a count
(FALSE).</p>
</td></tr>
<tr><td><code id="amd_uni_+3A_ov">ov</code></td>
<td>
<p>logical vector of length 1 specifying whether the frequency of
observed values (TRUE) should be returned rather than the frequency of
missing values (FALSE).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>numeric vector of length = <code>length(vrb.nm)</code> and names =
<code>vrb.nm</code> providing the frequency of missing (or observed if <code>ov</code>
= TRUE) values per variable. If <code>prop</code> = TRUE, the values will range
from 0 to 1. If <code>prop</code> = FALSE, the values will range from 0 to
<code>nrow(data)</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+amd_bi">amd_bi</a></code>
<code><a href="#topic+amd_multi">amd_multi</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
amd_uni(data = airquality, vrb.nm = names(airquality)) # proportion of missing data
amd_uni(data = airquality, vrb.nm = names(airquality),
   ov = TRUE) # proportion of observed data
amd_uni(data = airquality, vrb.nm = names(airquality),
   prop = FALSE) # count of missing data
amd_uni(data = airquality, vrb.nm = names(airquality),
   prop = FALSE, ov = TRUE) # count of observed data

</code></pre>

<hr>
<h2 id='auto_by'>Autoregressive Coefficient by Group</h2><span id='topic+auto_by'></span>

<h3>Description</h3>

<p><code>auto_by</code> computes the autoregressive coefficient by group for
longitudinal data where each observation within the group represents a
different timepoint. The function assumes the data are already sorted by
time.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>auto_by(
  x,
  grp,
  n = -1L,
  how = "cor",
  cw = TRUE,
  method = "pearson",
  use = "na.or.complete",
  REML = TRUE,
  control = NULL,
  sep = "."
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="auto_by_+3A_x">x</code></td>
<td>
<p>numeric vector.</p>
</td></tr>
<tr><td><code id="auto_by_+3A_grp">grp</code></td>
<td>
<p>list of atomic vector(s) and/or factor(s) (e.g., data.frame),
which each have same length as <code>x</code>. It can also be an atomic vector or
factor, which will then be made the first element of a list.</p>
</td></tr>
<tr><td><code id="auto_by_+3A_n">n</code></td>
<td>
<p>integer vector with length 1. Specifies the direction and magnitude
of the shift. See <code>shift</code> for details. The default is -1L, which is a
one-lag autoregressive coefficient' +2L would be a two-lead autoregressive
coefficient. The sign of <code>n</code> only affects the results for <code>how</code> =
&quot;lm&quot;, &quot;lme&quot;, or &quot;lmer&quot;.</p>
</td></tr>
<tr><td><code id="auto_by_+3A_how">how</code></td>
<td>
<p>character vector of length 1 specifying how to compute the
autoregressive coefficients. The options are 1) &quot;cor&quot; for correlation with
the <code>cor</code> function, 2) &quot;cov&quot; for covariance with the <code>cov</code>
function, 3) &quot;lm&quot; for the linear regression slope with the <code>lm</code>
function, 4) &quot;lme&quot; for empirical Bayes estimates from a linear mixed
effects model with the <code><a href="nlme.html#topic+lme">lme</a></code> function, 5) &quot;lmer&quot; for
empirical Bayes estimates from a linear mixed effects model with the
<code><a href="lme4.html#topic+lmer">lmer</a></code> function.</p>
</td></tr>
<tr><td><code id="auto_by_+3A_cw">cw</code></td>
<td>
<p>logical vector of length 1 specifying whether the shifted vector
should be group-mean centered (TRUE) or not (FALSE). This only affects the
results for <code>how</code> = &quot;lme&quot; or &quot;lmer&quot;.</p>
</td></tr>
<tr><td><code id="auto_by_+3A_method">method</code></td>
<td>
<p>character vector of length 1 specifying the type of correlation
or covariance to compute. Only used when <code>how</code> = &quot;cor&quot; or &quot;cov&quot;. See
<code><a href="stats.html#topic+cor">cor</a></code> for details.</p>
</td></tr>
<tr><td><code id="auto_by_+3A_use">use</code></td>
<td>
<p>character vector of length 1 specifying how to handle missing
data. Only used when <code>how</code> = &quot;cor&quot; or &quot;cov&quot;. See
<code><a href="stats.html#topic+cor">cor</a></code> for details.</p>
</td></tr>
<tr><td><code id="auto_by_+3A_reml">REML</code></td>
<td>
<p>logical vector of length 1 specifying whether to use restricted
estimated maximum liklihood (TRUE) rather than traditional maximum
likelihood (FALSE). Only used when <code>how</code> = &quot;lme&quot; or &quot;lmer&quot;.</p>
</td></tr>
<tr><td><code id="auto_by_+3A_control">control</code></td>
<td>
<p>list of control parameters for <code>lme</code> or <code>lmer</code> when
<code>how</code> = &quot;lme&quot; or &quot;lmer&quot;, respectively. See
<code><a href="nlme.html#topic+lmeControl">lmeControl</a></code> and <code><a href="lme4.html#topic+lmerControl">lmerControl</a></code> for
details.</p>
</td></tr>
<tr><td><code id="auto_by_+3A_sep">sep</code></td>
<td>
<p>character vector of length 1 specifying what string should
separate different group values when naming the return object. This
argument is only used if <code>grp</code> is a list of atomic vectors (e.g.,
data.frame).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>There are several different ways to estimate the autoregressive parameter.
This function offers a variety of ways with the <code>how</code> and <code>cw</code>
arguments. Note, that a recent simulation suggests that group-mean centering
via <code>cw</code> is the best approach when using linear mixed effects modeling
via <code>how</code> = &quot;lme&quot; or &quot;lmer&quot; (Hamaker  &amp; Grasman, 2015).
</p>


<h3>Value</h3>

<p>numeric vector of autoregressive coefficients with length =
<code>length(levels(interaction(grp)))</code> and names = pasteing of the
grouping value(s) together separated by <code>sep</code>.
</p>


<h3>References</h3>

<p>Hamaker, E. L., &amp; Grasman, R. P. (2015). To center or not to center?  Investigating
inertia with a multilevel autoregressive model. Frontiers in Psychology, 5, 1492.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# cor
auto_by(x = airquality$"Ozone", grp = airquality$"Month", how = "cor")
auto_by(x = airquality$"Ozone", grp = airquality$"Month",
   n = -2L, how = "cor") # lag across 2 timepoints
auto_by(x = airquality$"Ozone", grp = airquality$"Month",
   n = +1L, how = "cor") # lag and lead identical for cor
auto_by(x = airquality$"Ozone", grp = airquality$"Month", how = "cor",
   cw = FALSE) # centering within-person identical for cor

# cov
auto_by(x = airquality$"Ozone", grp = airquality$"Month", how = "cov")
auto_by(x = airquality$"Ozone", grp = airquality$"Month",
   n = -2L, how = "cov") # lag across 2 timepoints
auto_by(x = airquality$"Ozone", grp = airquality$"Month",
   n = +1L, how = "cov") # lag and lead identical for cov
auto_by(x = airquality$"Ozone", grp = airquality$"Month", how = "cov",
   cw = FALSE) # centering within-person identical for cov

# lm
auto_by(x = airquality$"Ozone", grp = airquality$"Month", how = "lm")
auto_by(x = airquality$"Ozone", grp = airquality$"Month",
   n = -2L, how = "lm") # lag across 2 timepoints
auto_by(x = airquality$"Ozone", grp = airquality$"Month",
   n = +1L, how = "lm") # lag and lead NOT identical for lm
auto_by(x = airquality$"Ozone", grp = airquality$"Month", how = "lm",
   cw = FALSE) # centering within-person identical for lm

# lme
chick_weight &lt;- as.data.frame(ChickWeight)
auto_by(x = chick_weight$"weight", grp = chick_weight$"Chick", how = "lme")
control_lme &lt;- nlme::lmeControl(maxIter = 250L, msMaxIter = 250L,
   tolerance = 1e-3, msTol = 1e-3) # custom controls
auto_by(x = chick_weight$"weight", grp = chick_weight$"Chick", how = "lme",
   control = control_lme)
auto_by(x = chick_weight$"weight", grp = chick_weight$"Chick",
   n = -2L, how = "lme") # lag across 2 timepoints
auto_by(x = chick_weight$"weight", grp = chick_weight$"Chick",
   n = +1L, how = "lme") # lag and lead NOT identical for lme
auto_by(x = chick_weight$"weight", grp = chick_weight$"Chick", how = "lme",
   cw = FALSE) # centering within-person NOT identical for lme

# lmer
bryant_2016 &lt;- as.data.frame(lmeInfo::Bryant2016)
## Not run: 
auto_by(x = bryant_2016$"outcome", grp = bryant_2016$"case", how = "lmer")
control_lmer &lt;- lme4::lmerControl(check.conv.grad = lme4::.makeCC("stop",
   tol = 2e-3, relTol = NULL), check.conv.singular = lme4::.makeCC("stop",
   tol = formals(lme4::isSingular)$"tol"), check.conv.hess = lme4::.makeCC(action = "stop",
   tol = 1e-6)) # custom controls
auto_by(x = bryant_2016$"outcome", grp = bryant_2016$"case", how = "lmer",
   control = control_lmer) # TODO: for some reason lmer doesn't like this
   # and is not taking into account the custom controls
auto_by(x = bryant_2016$"outcome", grp = bryant_2016$"case",
   n = -2L, how = "lmer") # lag across 2 timepoints
auto_by(x = bryant_2016$"outcome", grp = bryant_2016$"case",
   n = +1L, how = "lmer") # lag and lead NOT identical for lmer
auto_by(x = bryant_2016$"outcome", grp = bryant_2016$"case", how = "lmer",
   cw = FALSE) # centering within-person NOT identical for lmer

## End(Not run)

</code></pre>

<hr>
<h2 id='ave_dfm'>Repeated Group Statistics for a Data-Frame</h2><span id='topic+ave_dfm'></span>

<h3>Description</h3>

<p><code>ave_dfm</code> evaluates a function on a set of variables <code>vrb.nm</code>
separately for each group within <code>grp.nm</code>. The results are combined back
together in line with the rows of <code>data</code> similar to <code><a href="stats.html#topic+ave">ave</a></code>.
<code>ave_dfm</code> is different than <code>ave</code> or <code>agg</code> because it operates
on a data.frame, not an atomic vector.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ave_dfm(data, vrb.nm, grp.nm, fun, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ave_dfm_+3A_data">data</code></td>
<td>
<p>data.frame of data.</p>
</td></tr>
<tr><td><code id="ave_dfm_+3A_vrb.nm">vrb.nm</code></td>
<td>
<p>character vector of colnames in <code>data</code> specifying the
variables to use for the aggregation function <code>fun</code>.</p>
</td></tr>
<tr><td><code id="ave_dfm_+3A_grp.nm">grp.nm</code></td>
<td>
<p>character vector of colnames in <code>data</code> specifying the
grouping variables.</p>
</td></tr>
<tr><td><code id="ave_dfm_+3A_fun">fun</code></td>
<td>
<p>function that returns an atomic vector of length 1. Probably makes
sense to ensure the function always returns the same typeof as well.</p>
</td></tr>
<tr><td><code id="ave_dfm_+3A_...">...</code></td>
<td>
<p>additional named arguments to <code>fun</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>atomic vector of length = <code>nrow(data)</code> providing the result of
the function <code>fun</code> for the subset of data with that group value (i.e.,
<code>data[levels(interaction(data[grp.nm]))[i], vrb.nm]</code>) for that row.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+ave">ave</a></code> for the same functionality with atomic vector inputs
<code><a href="#topic+agg_dfm">agg_dfm</a></code> for similar functionality with data.frames, but can return
the result for each group once rather than repeating the result for each group
value in the data.frame
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# one grouping variables
ave_dfm(data = airquality, vrb.nm = c("Ozone","Solar.R"), grp.nm = "Month",
   fun = function(dat) cor(dat, use = "complete")[1,2])

# two grouping variables
ave_dfm(data = mtcars, vrb.nm = c("mpg","cyl","disp"), grp.nm = c("vs","am"),
   fun = nrow) # with multiple group columns

</code></pre>

<hr>
<h2 id='boot_ci'>Bootstrapped Confidence Intervals from a Matrix of Coefficients</h2><span id='topic+boot_ci'></span>

<h3>Description</h3>

<p><code>boot_ci</code> computes bootstrapped confidence intervals from a matrix of
coefficients (or any statistical information of interest). The function is an
alternative to <code>confint2.boot</code> for when the user does not have an object
of class <code>boot</code>, but rather creates their own matrix of coefficients. It
has limited types of bootstrapped confidence intervals at the moment, but
future versions are expected to have more options.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>boot_ci(coef, est = colMeans(coef), boot.ci.type = "perc2", level = 0.95)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="boot_ci_+3A_coef">coef</code></td>
<td>
<p>numeric matrix (or data.frame of numeric columns) of
coefficients. The rows correspond to each bootstrapped resample and the
columns to different coefficients. This is the equivalent of the &quot;t&quot;
element in a <code>boot</code> object.</p>
</td></tr>
<tr><td><code id="boot_ci_+3A_est">est</code></td>
<td>
<p>numeric vector of observed coefficients from the full sample. This
is the equivalent of the &quot;t0&quot; element in a <code>boot</code> object. The default
takes the mean of each coefficient across bootstrapped resamples; however,
this usually results in small amount of bias in the coefficients.</p>
</td></tr>
<tr><td><code id="boot_ci_+3A_boot.ci.type">boot.ci.type</code></td>
<td>
<p>character vector of length 1 specifying the type of
bootstrapped confidence interval to compute. The options are 1) &quot;perc2&quot; for
the naive percentile method using <code><a href="stats.html#topic+quantile">quantile</a></code>, and 2) &quot;norm2&quot;
for the normal method that uses the bootstrapped standard error to
construct symmetrical confidence intervals with the classic formula around
the estimate, The options have a &quot;2&quot; after them because, although they are
conceptually similar to the &quot;perc&quot; and &quot;norm&quot; methods in the
<code><a href="boot.html#topic+boot.ci">boot.ci</a></code> function, they are slightly different
mathematically.</p>
</td></tr>
<tr><td><code id="boot_ci_+3A_level">level</code></td>
<td>
<p>double vector of length 1 specifying the confidence level. Must
be between 0 and 1.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data.frame will be returned with nrow equal to the number of
coefficients bootstrapped and columns specified below. The rownames are the
colnames in the <code>coef</code> argument or the names in the <code>est</code> argument
(default data.frame rownames if neither have any names). The columns are the
following:
</p>

<dl>
<dt>est</dt><dd><p>original parameter estimates</p>
</dd>
<dt>se</dt><dd><p>bootstrapped standard errors (does not differ by <code>boot.ci.type</code>)</p>
</dd>
<dt>lwr</dt><dd><p>lower bound of the bootstrapped confidence intervals</p>
</dd>
<dt>upr</dt><dd><p>upper bound of the bootstrapped confidence intervals</p>
</dd>
</dl>



<h3>See Also</h3>

<p><code><a href="boot.html#topic+boot.ci">boot.ci</a></code> for the confidence interval function in the <code>boot</code> package,
<code><a href="car.html#topic+confint.boot">confint.boot</a></code> for an alternative function with <code>boot</code> objects
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
tmp &lt;- replicate(n = 100, expr = {
   i &lt;- sample.int(nrow(attitude), replace = TRUE)
   colMeans(attitude[i, ])
}, simplify = FALSE)
mat &lt;- str2str::lv2m(tmp, along = 1)
boot_ci(mat, est = colMeans(attitude))

</code></pre>

<hr>
<h2 id='by2'>Apply a Function to Data by Group</h2><span id='topic+by2'></span>

<h3>Description</h3>

<p><code>by2</code> applies a function to data by group and is an alternative to the
base R function <code><a href="base.html#topic+by">by</a></code>. The function is apart of the
split-apply-combine type of function discussed in the <code>plyr</code> R package
and is very similar to <code><a href="plyr.html#topic+dlply">dlply</a></code>. It splits up one data.frame
<code>.data[.vrb.nm]</code>into a data.frame for each group in
<code>.data[.grp.nm]</code>, applies a function <code>.fun</code> to each data.frame, and
then returns the results as a list with names equal to the group values
<code>unique(interaction(.data[.grp.nm], sep = .sep))</code>. <code>by2</code> is simply
<code>split.data.frame</code> + <code>lapply</code>. Similar to <code>dlply</code>, The
arguments all start with <code>.</code> so that they do not conflict with arguments
from the function <code>.fun</code>. If you want to apply a function a (atomic)
vector rather than data.frame, then use <code><a href="#topic+tapply2">tapply2</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>by2(.data, .vrb.nm, .grp.nm, .sep = ".", .fun, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="by2_+3A_.data">.data</code></td>
<td>
<p>data.frame of data.</p>
</td></tr>
<tr><td><code id="by2_+3A_.vrb.nm">.vrb.nm</code></td>
<td>
<p>character vector specifying the colnames of <code>.data</code> to
select the set of variables to apply <code>.fun</code> to.</p>
</td></tr>
<tr><td><code id="by2_+3A_.grp.nm">.grp.nm</code></td>
<td>
<p>character vector specifying the colnames of <code>.data</code> to
select the grouping variables.</p>
</td></tr>
<tr><td><code id="by2_+3A_.sep">.sep</code></td>
<td>
<p>character vector of length 1 specifying the string to combine the
group values together with. <code>.sep</code> is only used if there are multiple
grouping variables (i.e., <code>length(.grp.nm)</code> &gt; 1).</p>
</td></tr>
<tr><td><code id="by2_+3A_.fun">.fun</code></td>
<td>
<p>function to apply to the set of variables <code>.data[.vrb.nm]</code>
for each group.</p>
</td></tr>
<tr><td><code id="by2_+3A_...">...</code></td>
<td>
<p>additional named arguments to pass to <code>.fun</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list of objects containing the return object of <code>.fun</code> for each
group. The names are the unique combinations of the grouping variables
(i.e., <code>unique(interaction(.data[.grp.nm], sep = .sep))</code>).
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+by">by</a></code>
<code><a href="#topic+tapply2">tapply2</a></code>
<code><a href="plyr.html#topic+dlply">dlply</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# one grouping variable
by2(mtcars, .vrb.nm = c("mpg","cyl","disp"), .grp.nm = "vs",
   .fun = cov, use = "complete.obs")

# two grouping variables
x &lt;- by2(mtcars, .vrb.nm = c("mpg","cyl","disp"), .grp.nm = c("vs","am"),
   .fun = cov, use = "complete.obs")
print(x)
str(x)

# compare to by
vrb_nm &lt;- c("mpg","cyl","disp") # Roxygen runs the whole script if I put a c() in a []
grp_nm &lt;- c("vs","am") # Roxygen runs the whole script if I put a c() in a []
y &lt;- by(mtcars[vrb_nm], INDICES = mtcars[grp_nm],
   FUN = cov, use = "complete.obs", simplify = FALSE)
str(y) # has dimnames rather than names
</code></pre>

<hr>
<h2 id='center'>Centering and/or Standardizing a Numeric Vector</h2><span id='topic+center'></span>

<h3>Description</h3>

<p><code>center</code> centers and/or standardized a numeric vector. It is an
alternative to <code>scale.default</code> that returns a numeric vector rather than
a numeric matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>center(x, center = TRUE, scale = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="center_+3A_x">x</code></td>
<td>
<p>numeric vector.</p>
</td></tr>
<tr><td><code id="center_+3A_center">center</code></td>
<td>
<p>logical vector with length 1 specifying whether grand-mean
centering should be done.</p>
</td></tr>
<tr><td><code id="center_+3A_scale">scale</code></td>
<td>
<p>logical vector with length 1 specifying whether grand-SD scaling
should be done.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>center</code> first coerces <code>x</code> to a matrix in preparation for the call
to <code>scale.default</code>. If the coercion results in a non-numeric matrix
(e.g., <code>x</code> is a character vector or factor), then an error is returned.
</p>


<h3>Value</h3>

<p>numeric vector of <code>x</code> centered and/or standardized with the same
names as <code>x</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+centers">centers</a></code>
<code><a href="#topic+center_by">center_by</a></code>
<code><a href="#topic+centers_by">centers_by</a></code>
<code><a href="base.html#topic+scale.default">scale.default</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>center(x = mtcars$"disp")
center(x = mtcars$"disp", scale = TRUE)
center(x = mtcars$"disp", center = FALSE, scale = TRUE)
center(x = setNames(mtcars$"disp", nm = row.names(mtcars)))
</code></pre>

<hr>
<h2 id='center_by'>Centering and/or Standardizing a Numeric Vector by Group</h2><span id='topic+center_by'></span>

<h3>Description</h3>

<p><code>center_by</code> centers and/or standardized a numeric vector by group. This
is sometimes called group-mean centering and/or group-SD standardizing.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>center_by(x, grp, center = TRUE, scale = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="center_by_+3A_x">x</code></td>
<td>
<p>numeric vector.</p>
</td></tr>
<tr><td><code id="center_by_+3A_grp">grp</code></td>
<td>
<p>list of atomic vector(s) and/or factor(s) (e.g., data.frame)
containing the groups. They should each have same length as <code>x</code>. It
can also be an atomic vector or factor, which will then be made the first
element of a list internally.</p>
</td></tr>
<tr><td><code id="center_by_+3A_center">center</code></td>
<td>
<p>logical vector with length 1 specifying whether group-mean
centering should be done.</p>
</td></tr>
<tr><td><code id="center_by_+3A_scale">scale</code></td>
<td>
<p>logical vector with length 1 specifying whether group-SD scaling
should be done.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>center_by</code> first coerces <code>x</code> to a matrix in preparation for the
core of the function, which is essentially: <code>lapply(X = split(x = x, f =
grp), FUN = scale.default)</code>. If the coercion results in a non-numeric matrix
(e.g., <code>x</code> is a character vector or factor), then an error is returned.
An error is also returned if <code>x</code> and the elements of <code>grp</code> do not
have the same length.
</p>


<h3>Value</h3>

<p>numeric vector of <code>x</code> centered and/or standardized by group with
the same names as <code>x</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+centers_by">centers_by</a></code>
<code><a href="#topic+center">center</a></code>
<code><a href="#topic+centers">centers</a></code>
<code><a href="base.html#topic+scale.default">scale.default</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>chick_data &lt;- as.data.frame(ChickWeight) # because the "groupedData" class calls
   # `[.groupedData`, which is different than `[.data.frame`
center_by(x = ChickWeight[["weight"]], grp = ChickWeight[["Chick"]])
center_by(x = setNames(obj = ChickWeight[["weight"]], nm = row.names(ChickWeight)),
   grp = ChickWeight[["Chick"]]) # with names
tmp_nm &lt;- c("Type","Treatment") # b/c Roxygen2 doesn't like a c() within a []
center_by(x = as.data.frame(CO2)[["uptake"]], grp = as.data.frame(CO2)[tmp_nm],
   scale = TRUE) # multiple grouping vectors
</code></pre>

<hr>
<h2 id='centers'>Centering and/or Standardizing Numeric Data</h2><span id='topic+centers'></span>

<h3>Description</h3>

<p><code>centers</code> centers and/or standardized data. It is an alternative to
<code>scale.default</code> that returns a data.frame rather than a numeric matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>centers(data, vrb.nm, center = TRUE, scale = FALSE, suffix)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="centers_+3A_data">data</code></td>
<td>
<p>data.frame of data.</p>
</td></tr>
<tr><td><code id="centers_+3A_vrb.nm">vrb.nm</code></td>
<td>
<p>character vector of colnames from <code>data</code> specifying the
variables.</p>
</td></tr>
<tr><td><code id="centers_+3A_center">center</code></td>
<td>
<p>logical vector with length 1 specifying whether grand-mean
centering should be done.</p>
</td></tr>
<tr><td><code id="centers_+3A_scale">scale</code></td>
<td>
<p>logical vector with length 1 specifying whether grand-SD scaling
should be done.</p>
</td></tr>
<tr><td><code id="centers_+3A_suffix">suffix</code></td>
<td>
<p>character vector with a single element specifying the string to
append to the end of the colnames of the return object. The default depends
on the <code>center</code> and <code>scale</code> arguments: 1)if <code>center</code> = TRUE
and <code>scale</code> = FALSE, then <code>suffix</code> = &quot;_c&quot;, 2) if <code>center</code> =
FALSE and <code>scale</code> = TRUE, then <code>suffix</code> = &quot;_s&quot;, 3) if
<code>center</code> = TRUE and <code>scale</code> = TRUE, then <code>suffix</code> = &quot;_z&quot;, 4)
if <code>center</code> = FALSE and <code>scale</code> = FALSE, then <code>suffix</code> = &quot;&quot;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>centers</code> first coerces <code>data[vrb.nm]</code> to a matrix in preparation
for the call to <code>scale.default</code>. If the coercion results in a
non-numeric matrix (e.g., any columns in <code>data[vrb.nm]</code> are character
vectors or factors), then an error is returned.
</p>


<h3>Value</h3>

<p>data.frame of centered and/or standardized variables with colnames
specified by <code>paste0(vrb.nm, suffix)</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+center">center</a></code>
<code><a href="#topic+centers_by">centers_by</a></code>
<code><a href="#topic+center_by">center_by</a></code>
<code><a href="base.html#topic+scale.default">scale.default</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>centers(data = mtcars, vrb.nm = c("disp","hp","drat","wt","qsec"))
centers(data = mtcars, vrb.nm = c("disp","hp","drat","wt","qsec"),
   scale = TRUE)
centers(data = mtcars, vrb.nm = c("disp","hp","drat","wt","qsec"),
   center = FALSE, scale = TRUE)
centers(data = mtcars, vrb.nm = c("disp","hp","drat","wt","qsec"),
   scale = TRUE, suffix = "_std")
</code></pre>

<hr>
<h2 id='centers_by'>Centering and/or Standardizing Numeric Data by Group</h2><span id='topic+centers_by'></span>

<h3>Description</h3>

<p><code>centers_by</code> centers and/or standardized data by group. This is sometimes
called group-mean centering and/or group-SD standardizing. The groups can be
specified by multiple columns in <code>data</code> (e.g., <code>grp.nm</code> with length
&gt; 1), and <code>interaction</code> will be implicitly called to create the groups.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>centers_by(data, vrb.nm, grp.nm, center = TRUE, scale = FALSE, suffix)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="centers_by_+3A_data">data</code></td>
<td>
<p>data.frame of data.</p>
</td></tr>
<tr><td><code id="centers_by_+3A_vrb.nm">vrb.nm</code></td>
<td>
<p>character vector of colnames from <code>data</code> specifying the
variables.</p>
</td></tr>
<tr><td><code id="centers_by_+3A_grp.nm">grp.nm</code></td>
<td>
<p>character vector of colnames from <code>data</code> specifying the
groups.</p>
</td></tr>
<tr><td><code id="centers_by_+3A_center">center</code></td>
<td>
<p>logical vector with length 1 specifying whether group-mean
centering should be done.</p>
</td></tr>
<tr><td><code id="centers_by_+3A_scale">scale</code></td>
<td>
<p>logical vector with length 1 specifying whether group-SD scaling
should be done.</p>
</td></tr>
<tr><td><code id="centers_by_+3A_suffix">suffix</code></td>
<td>
<p>character vector with a single element specifying the string to
append to the end of the colnames of the return object. The default depends
on the <code>center</code> and <code>scale</code> arguments: 1)if <code>center</code> = TRUE
and <code>scale</code> = FALSE, then <code>suffix</code> = &quot;_cw&quot;, 2) if <code>center</code> =
FALSE and <code>scale</code> = TRUE, then <code>suffix</code> = &quot;_sw&quot;, 3) if
<code>center</code> = TRUE and <code>scale</code> = TRUE, then <code>suffix</code> = &quot;_zw&quot;,
4) if <code>center</code> = FALSE and <code>scale</code> = FALSE, then <code>suffix</code> =
&quot;&quot;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>centers_by</code> first coerces <code>data[vrb.nm]</code> to a matrix in preparation
for the core of the function, which is essentially <code>lapply(X = split(x =
data[vrb.nm], f = data[grp.nm]), FUN = scale.default)</code> If the coercion
results in a non-numeric matrix (e.g., any columns in <code>data[vrb.nm]</code> are
character vectors or factors), then an error is returned.
</p>


<h3>Value</h3>

<p>data.frame of centered and/or standardized variables by group with
colnames specified by <code>paste0(vrb.nm, suffix)</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+center_by">center_by</a></code>
<code><a href="#topic+centers">centers</a></code>
<code><a href="#topic+center">center</a></code>
<code><a href="base.html#topic+scale.default">scale.default</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>ChickWeight2 &lt;- as.data.frame(ChickWeight) # because the "groupedData" class calls
   # `[.groupedData`, which is different than `[.data.frame`
row.names(ChickWeight2) &lt;- as.numeric(row.names(ChickWeight)) / 1000
centers_by(data = ChickWeight2, vrb.nm = c("weight","Time"), grp.nm = "Chick")
centers_by(data = ChickWeight2, vrb.nm = c("weight","Time"), grp.nm = "Chick",
   scale = TRUE, suffix = "_within")
centers_by(data = as.data.frame(CO2), vrb.nm = c("conc","uptake"),
   grp.nm = c("Type","Treatment"), scale = TRUE) # multiple grouping columns
</code></pre>

<hr>
<h2 id='change'>Change Score from a Numeric Vector</h2><span id='topic+change'></span>

<h3>Description</h3>

<p><code>change</code> creates a change score (aka difference score) from a numeric
vector. It is assumed that the vector is already sorted by time such that the
first element is earliest in time and the last element is the latest in time.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>change(x, n, undefined = NA)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="change_+3A_x">x</code></td>
<td>
<p>numeric vector.</p>
</td></tr>
<tr><td><code id="change_+3A_n">n</code></td>
<td>
<p>integer vector with length 1. Specifies how the change score is
calculated. If <code>n</code> is positive, then the change score is calculated
from lead - original; if <code>n</code> is negative, then the change score is
calculated from original - lag. The magnitude of <code>n</code> determines how
many elements are shifted for the lead/lag within the calculation. If
<code>n</code> is zero, then <code>change</code> simply returns a vector or zeros. See
details of <code><a href="#topic+shift">shift</a></code>.</p>
</td></tr>
<tr><td><code id="change_+3A_undefined">undefined</code></td>
<td>
<p>atomic vector with length 1 (probably makes sense to be the
same typeof as <code>x</code>). Specifies what to insert for undefined values
after the shifting takes place. See details of <code><a href="#topic+shift">shift</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>It is recommended to use <code>L</code> when specifying <code>n</code> to prevent
problems with floating point numbers. <code>shift</code> tries to circumvent this
issue by a call to <code>round</code> within <code>shift</code> if <code>n</code> is not an
integer; however that is not a complete fail safe. The problem is that
<code>as.integer(n)</code> implicit in <code>shift</code> truncates rather than rounds.
See details of <code><a href="#topic+shift">shift</a></code>.
</p>


<h3>Value</h3>

<p>an atomic vector of the same length as <code>x</code> that is the change
score. If <code>x</code> and <code>undefined</code> are different typeofs, then the
return will be coerced to the most complex typeof (i.e., complex to simple:
character, double, integer, logical).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+changes">changes</a></code>
<code><a href="#topic+change_by">change_by</a></code>
<code><a href="#topic+changes_by">changes_by</a></code>
<code><a href="#topic+shift">shift</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>change(x = attitude[[1]], n = -1L) # use L to prevent problems with floating point numbers
change(x = attitude[[1]], n = -2L) # can specify any integer up to the length of `x`
change(x = attitude[[1]], n = +1L) # can specify negative or positive integers
change(x = attitude[[1]], n = +2L, undefined = -999) # user-specified indefined value
change(x = attitude[[1]], n = -2L, undefined = -999) # user-specified indefined value
change(x = attitude[[1]], n = 0L) # returns a vector of zeros
## Not run: 
change(x = setNames(object = letters, nm = LETTERS), n = 3L) # character vector returns an error

## End(Not run)
</code></pre>

<hr>
<h2 id='change_by'>Change Scores from a Numeric Vector by Group</h2><span id='topic+change_by'></span>

<h3>Description</h3>

<p><code>change_by</code> creates a change score (aka difference score) from a numeric
vector separately for each group. It is assumed that the vector is already
sorted within each group by time such that the first element for that group
is earliest in time and the last element for that group is the latest in
time.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>change_by(x, grp, n, undefined = NA)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="change_by_+3A_x">x</code></td>
<td>
<p>numeric vector.</p>
</td></tr>
<tr><td><code id="change_by_+3A_grp">grp</code></td>
<td>
<p>list of atomic vector(s) and/or factor(s) (e.g., data.frame),
which each have same length as <code>x</code>. It can also be an atomic vector or
factor, which will then be made the first element of a list internally.</p>
</td></tr>
<tr><td><code id="change_by_+3A_n">n</code></td>
<td>
<p>integer vector with length 1. Specifies how the change score is
calculated. If <code>n</code> is positive, then the change score is calculated
from lead - original; if <code>n</code> is negative, then the change score is
calculated from original - lag. The magnitude of <code>n</code> determines how
many rows are shifted for the lead/lag within the calculation. See details
of <code><a href="#topic+shift_by">shift_by</a></code>.</p>
</td></tr>
<tr><td><code id="change_by_+3A_undefined">undefined</code></td>
<td>
<p>atomic vector with length 1 (probably makes sense to be the
same typeof as <code>x</code>). Specifies what to insert for undefined values
after the shifting takes place. See details of <code><a href="#topic+shift_by">shift_by</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>It is recommended to use <code>L</code> when specifying <code>n</code> to prevent
problems with floating point numbers. <code>shift_by</code> tries to circumvent
this issue by a call to <code>round</code> within <code>shift_by</code> if <code>n</code> is
not an integer; however that is not a complete fail safe. The problem is that
<code>as.integer(n)</code> implicit in <code>shift_by</code> truncates rather than
rounds. See details of <code><a href="#topic+shift_by">shift_by</a></code>.
</p>


<h3>Value</h3>

<p>an atomic vector of the same length as <code>x</code> that is the change
score by group. If <code>x</code> and <code>undefined</code> are different typeofs,
then the return will be coerced to the more complex typoof (i.e., complex
to simple: character, double, integer, logical).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+changes_by">changes_by</a></code>
<code><a href="#topic+change">change</a></code>
<code><a href="#topic+changes">changes</a></code>
<code><a href="#topic+shift_by">shift_by</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>change_by(x = ChickWeight[["Time"]], grp = ChickWeight[["Chick"]], n = -1L)
tmp_nm &lt;- c("vs","am") # multiple grouping vectors
change_by(x = mtcars[["disp"]], grp = mtcars[tmp_nm], n = +1L)
tmp_nm &lt;- c("Type","Treatment") # multiple grouping vectors
change_by(x = as.data.frame(CO2)[["uptake"]], grp = as.data.frame(CO2)[tmp_nm], n = 2L)
</code></pre>

<hr>
<h2 id='changes'>Change Scores from Numeric Data</h2><span id='topic+changes'></span>

<h3>Description</h3>

<p><code>changes</code> creates change scores (aka difference scores) from numeric
data. It is assumed that the data is already sorted by time such that the
first row is earliest in time and the last row is the latest in time.
<code>changes</code> is a multivariate version of <code><a href="#topic+change">change</a></code> that operates
on multiple variabes rather than just one.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>changes(data, vrb.nm, n, undefined = NA, suffix)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="changes_+3A_data">data</code></td>
<td>
<p>data.frame of data.</p>
</td></tr>
<tr><td><code id="changes_+3A_vrb.nm">vrb.nm</code></td>
<td>
<p>character vector of colnames from <code>data</code> specifying the
variables.</p>
</td></tr>
<tr><td><code id="changes_+3A_n">n</code></td>
<td>
<p>integer vector with length 1. Specifies how the change score is
calculated. If <code>n</code> is positive, then the change score is calculated
from lead - original; if <code>n</code> is negative, then the change score is
calculated from original - lag. The magnitude of <code>n</code> determines how
many rows are shifted for the lead/lag within the calculation. See details
of <code><a href="#topic+shifts">shifts</a></code>.</p>
</td></tr>
<tr><td><code id="changes_+3A_undefined">undefined</code></td>
<td>
<p>atomic vector with length 1 (probably makes sense to be the
same typeof as <code>x</code>). Specifies what to insert for undefined values
after the shifting takes place. See details of <code><a href="#topic+shifts">shifts</a></code>.</p>
</td></tr>
<tr><td><code id="changes_+3A_suffix">suffix</code></td>
<td>
<p>character vector of length 1 specifying the string to append to
the end of the colnames of the return object. The default depends on the
<code>n</code> argument: 1) if <code>n</code> &lt; 0, then <code>suffix</code> =
<code>paste0("_hg", -n)</code>, 2) if <code>n</code> &gt; 0, then <code>suffix</code> =
<code>paste0("_hd", +n)</code>, 3) if <code>n</code> = 0, then <code>suffix</code> = &quot;&quot;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>It is recommended to use <code>L</code> when specifying <code>n</code> to prevent
problems with floating point numbers. <code>shifts</code> tries to circumvent this
issue by a call to <code>round</code> within <code>shifts</code> if <code>n</code> is not an
integer; however that is not a complete fail safe. The problem is that
<code>as.integer(n)</code> implicit in <code>shifts</code> truncates rather than rounds.
See details of <code><a href="#topic+shifts">shifts</a></code>.
</p>


<h3>Value</h3>

<p>data.frame of change scores with colnames specified by
<code>paste0(vrb.nm, suffix)</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+change">change</a></code>
<code><a href="#topic+changes_by">changes_by</a></code>
<code><a href="#topic+change_by">change_by</a></code>
<code><a href="#topic+shifts">shifts</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>changes(attitude, vrb.nm = names(attitude),
   n = -1L) # use L to prevent problems with floating point numbers
changes(attitude, vrb.nm = names(attitude),
   n = -2L) # can specify any integer up to the length of `x`
changes(attitude, vrb.nm = names(attitude),
   n = +1L) # can specify negative or positive integers
changes(attitude, vrb.nm = names(attitude),
   n = +2L, undefined = -999) # user-specified indefined value
changes(attitude, vrb.nm = names(attitude),
   n = -2L, undefined = -999) # user-specified indefined value
## Not run: 
changes(str2str::d2d(InsectSprays), names(InsectSprays),
  n = 3L) # character vector returns an error

## End(Not run)
</code></pre>

<hr>
<h2 id='changes_by'>Change Scores from Numeric Data by Group</h2><span id='topic+changes_by'></span>

<h3>Description</h3>

<p><code>changes_by</code> creates change scores (aka difference scores) from numeric
data separately for each group. It is assumed that the data is already sorted
within each group by time such that the first row for that group is earliest
in time and the last row for that group is the latest in time.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>changes_by(data, vrb.nm, grp.nm, n, undefined = NA, suffix)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="changes_by_+3A_data">data</code></td>
<td>
<p>data.frame of data.</p>
</td></tr>
<tr><td><code id="changes_by_+3A_vrb.nm">vrb.nm</code></td>
<td>
<p>character vector of colnames from <code>data</code> specifying the
variables.</p>
</td></tr>
<tr><td><code id="changes_by_+3A_grp.nm">grp.nm</code></td>
<td>
<p>character vector of colnames from <code>data</code> specifying the
groups.</p>
</td></tr>
<tr><td><code id="changes_by_+3A_n">n</code></td>
<td>
<p>integer vector with length 1. Specifies how the change score is
calculated. If <code>n</code> is positive, then the change score is calculated
from lead - original; if <code>n</code> is negative, then the change score is
calculated from original - lag. The magnitude of <code>n</code> determines how
many rows are shifted for the lead/lag within the calculation. See details
of <code><a href="#topic+shifts_by">shifts_by</a></code>.</p>
</td></tr>
<tr><td><code id="changes_by_+3A_undefined">undefined</code></td>
<td>
<p>atomic vector with length 1 (probably makes sense to be the
same typeof as <code>x</code>). Specifies what to insert for undefined values
after the shifting takes place. See details of <code><a href="#topic+shifts_by">shifts_by</a></code>.</p>
</td></tr>
<tr><td><code id="changes_by_+3A_suffix">suffix</code></td>
<td>
<p>character vector of length 1 specifying the string to append to
the end of the colnames of the return object. The default depends on the
<code>n</code> argument: 1) if <code>n</code> &lt; 0, then <code>suffix</code> =
<code>paste0("_hgw", -n)</code>, 2) if <code>n</code> &gt; 0, then <code>suffix</code> =
<code>paste0("_hdw", +n)</code>, 3) if <code>n</code> = 0, then <code>suffix</code> = &quot;&quot;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>It is recommended to use <code>L</code> when specifying <code>n</code> to prevent
problems with floating point numbers. <code>shifts_by</code> tries to circumvent
this issue by a call to <code>round</code> within <code>shifts_by</code> if <code>n</code> is
not an integer; however that is not a complete fail safe. The problem is that
<code>as.integer(n)</code> implicit in <code>shifts_by</code> truncates rather than
rounds. See details of <code><a href="#topic+shifts_by">shifts_by</a></code>.
</p>


<h3>Value</h3>

<p>data.frame of change scores by group with colnames specified by
<code>paste0(vrb.nm, suffix)</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+change_by">change_by</a></code>
<code><a href="#topic+changes">changes</a></code>
<code><a href="#topic+change">change</a></code>
<code><a href="#topic+shifts_by">shifts_by</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>changes_by(data = ChickWeight, vrb.nm = c("weight","Time"), grp.nm = "Chick", n = -1L)
changes_by(data = mtcars, vrb.nm = c("disp","mpg"), grp.nm = c("vs","am"), n = 1L)
changes_by(data = as.data.frame(CO2), vrb.nm = c("conc","uptake"),
   grp.nm = c("Type","Treatment"), n = 2L) # multiple grouping columns
</code></pre>

<hr>
<h2 id='colMeans_if'>Column Means Conditional on Frequency of Observed Values</h2><span id='topic+colMeans_if'></span>

<h3>Description</h3>

<p><code>colMeans_if</code> calculates the mean of every column in a numeric or
logical matrix conditional on the frequency of observed data. If the
frequency of observed values in that column is less than (or equal to) that
specified by <code>ov.min</code>, then NA is returned for that row.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>colMeans_if(x, ov.min = 1, prop = TRUE, inclusive = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="colMeans_if_+3A_x">x</code></td>
<td>
<p>numeric or logical matrix. If not a matrix, it will be coerced to
one.</p>
</td></tr>
<tr><td><code id="colMeans_if_+3A_ov.min">ov.min</code></td>
<td>
<p>minimum frequency of observed values required per column. If
<code>prop</code> = TRUE, then this is a decimal between 0 and 1. If <code>prop</code>
= FALSE, then this is a integer between 0 and <code>nrow(x)</code>.</p>
</td></tr>
<tr><td><code id="colMeans_if_+3A_prop">prop</code></td>
<td>
<p>logical vector of length 1 specifying whether <code>ov.min</code>
should refer to the proportion of observed values (TRUE) or the count of
observed values (FALSE).</p>
</td></tr>
<tr><td><code id="colMeans_if_+3A_inclusive">inclusive</code></td>
<td>
<p>logical vector of length 1 specifying whether the mean
should be calculated if the frequency of observed values in a column is
exactly equal to <code>ov.min</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Conceptually this function does: <code>apply(X = x, MARGIN = 2, FUN =
mean_if, ov.min = ov.min, prop = prop, inclusive = inclusive)</code>. But for
computational efficiency purposes it does not because then the missing values
conditioning would not be vectorized. Instead, it uses <code>colMeans</code> and
then inserts NAs for columns that have too few observed values.
</p>


<h3>Value</h3>

<p>numeric vector of length = <code>ncol(x)</code> with names =
<code>colnames(x)</code> providing the mean of each column or NA depending on the
frequency of observed values.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+colSums_if">colSums_if</a></code>
<code><a href="#topic+rowMeans_if">rowMeans_if</a></code>
<code><a href="#topic+rowSums_if">rowSums_if</a></code>
<code><a href="Matrix.html#topic+colMeans">colMeans</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>colMeans_if(airquality)
colMeans_if(x = airquality, ov.min = 150, prop = FALSE)
</code></pre>

<hr>
<h2 id='colNA'>Frequency of Missing Values by Column</h2><span id='topic+colNA'></span>

<h3>Description</h3>

<p><code>rowNA</code> compute the frequency of missing values in a matrix by column.
This function essentially does <code>apply(X = x, MARGIN = 2, FUN = vecNA)</code>.
It is also used by other functions in the quest package related to missing
values (e.g., <code><a href="#topic+colMeans_if">colMeans_if</a></code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>colNA(x, prop = FALSE, ov = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="colNA_+3A_x">x</code></td>
<td>
<p>matrix with any typeof. If not a matrix, it will be coerced to a
matrix via <code>as.matrix</code>. The function allows for colnames to carry over
for non-matrix objects (e.g., data.frames).</p>
</td></tr>
<tr><td><code id="colNA_+3A_prop">prop</code></td>
<td>
<p>logical vector of length 1 specifying whether the frequency of
missing values should be returned as a proportion (TRUE) or a count
(FALSE).</p>
</td></tr>
<tr><td><code id="colNA_+3A_ov">ov</code></td>
<td>
<p>logical vector of length 1 specifying whether the frequency of
observed values (TRUE) should be returned rather than the frequency of
missing values (FALSE).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>numeric vector of length = <code>ncol(x)</code>, and names =
<code>colnames(x)</code> providing the frequency of missing values (or observed
values if <code>ov</code> = TRUE) per column. If <code>prop</code> = TRUE, the values
will range from 0 to 1. If <code>prop</code> = FALSE, the values will range from
1 to <code>nrow(x)</code>.
</p>


<h3>See Also</h3>

<p><code><a href="Matrix.html#topic+is.na">is.na</a></code>
<code><a href="#topic+vecNA">vecNA</a></code>
<code><a href="#topic+rowNA">rowNA</a></code>
<code><a href="#topic+rowsNA">rowsNA</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>colNA(as.matrix(airquality)) # count of missing values
colNA(as.matrix(airquality), prop = TRUE) # proportion of missing values
colNA(as.matrix(airquality), ov = TRUE) # count of observed values
colNA(as.data.frame(airquality), prop = TRUE, ov = TRUE) # proportion of observed values
</code></pre>

<hr>
<h2 id='colSums_if'>Column Sums Conditional on Frequency of Observed Values</h2><span id='topic+colSums_if'></span>

<h3>Description</h3>

<p><code>colSums_if</code> calculates the sum of every column in a numeric or logical
matrix conditional on the frequency of observed data. If the frequency of
observed values in that column is less than (or equal to) that specified by
<code>ov.min</code>, then NA is returned for that column. It also has the option to
return a value other than 0 (e.g., NA) when all columns are NA, which differs
from <code>colSums(x, na.rm = TRUE)</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>colSums_if(
  x,
  ov.min = 1,
  prop = TRUE,
  inclusive = TRUE,
  impute = TRUE,
  allNA = NA_real_
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="colSums_if_+3A_x">x</code></td>
<td>
<p>numeric or logical matrix. If not a matrix, it will be coerced to
one.</p>
</td></tr>
<tr><td><code id="colSums_if_+3A_ov.min">ov.min</code></td>
<td>
<p>minimum frequency of observed values required per column. If
<code>prop</code> = TRUE, then this is a decimal between 0 and 1. If <code>prop</code>
= FALSE, then this is a integer between 0 and <code>nrow(x)</code>.</p>
</td></tr>
<tr><td><code id="colSums_if_+3A_prop">prop</code></td>
<td>
<p>logical vector of length 1 specifying whether <code>ov.min</code>
should refer to the proportion of observed values (TRUE) or the count of
observed values (FALSE).</p>
</td></tr>
<tr><td><code id="colSums_if_+3A_inclusive">inclusive</code></td>
<td>
<p>logical vector of length 1 specifying whether the sum should
be calculated if the frequency of observed values in a column is exactly
equal to <code>ov.min</code>.</p>
</td></tr>
<tr><td><code id="colSums_if_+3A_impute">impute</code></td>
<td>
<p>logical vector of length 1 specifying if missing values should
be imputed with the mean of observed values of <code>x[, i]</code>. If TRUE
(default), this will make sums over the same rows with different amounts of
observed data comparable.</p>
</td></tr>
<tr><td><code id="colSums_if_+3A_allna">allNA</code></td>
<td>
<p>numeric vector of length 1 specifying what value should be
returned for columns that are all NA. This is most applicable when
<code>ov.min = 0</code> and <code>inclusive = TRUE</code>. The default is NA, which
differs from <code>colSums</code> with <code>na.rm = TRUE</code> where 0 is returned.
Note, the value is overwritten by NA if the frequency of observed values in
that column is less than (or equal to) that specified by <code>ov.min</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Conceptually this function does: <code>apply(X = x, MARGIN = 2, FUN = sum_if,
ov.min = ov.min, prop = prop, inclusive = inclusive)</code>. But for computational
efficiency purposes it does not because then the observed values conditioning
would not be vectorized. Instead, it uses <code>colSums</code> and then inserts NAs
for columns that have too few observed values.
</p>


<h3>Value</h3>

<p>numeric vector of length = <code>ncol(x)</code> with names =
<code>colnames(x)</code> providing the sum of each column or NA depending on the
frequency of observed values.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+colMeans_if">colMeans_if</a></code>
<code><a href="#topic+rowSums_if">rowSums_if</a></code>
<code><a href="#topic+rowMeans_if">rowMeans_if</a></code>
<code><a href="Matrix.html#topic+colSums">colSums</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>colSums_if(airquality)
colSums_if(x = airquality, ov.min = 150, prop = FALSE)
x &lt;- data.frame("x" = c(1, 2, NA), "y" = c(1, NA, NA), "z" = c(NA, NA, NA))
colSums_if(x)
colSums_if(x, ov.min = 0)
colSums_if(x, ov.min = 0, allNA = 0)
identical(x = colSums(x, na.rm = TRUE),
   y = colSums_if(x, impute = FALSE, ov.min = 0, allNA = 0)) # identical to
   # colSums(x, na.rm = TRUE)
</code></pre>

<hr>
<h2 id='composite'>Composite Reliability of a Score</h2><span id='topic+composite'></span>

<h3>Description</h3>

<p><code>composite</code> computes the composite reliability coefficient (sometimes
referred to as omega) for a set of variables/items. The composite reliability
computed in <code>composite</code> assumes a undimensional factor model with no
error covariances. In addition to the coefficient itself, its standard error
and confidence interval are returned, the average standardized factor loading
from the factor model and number of variables/items, and (optional) model fit
indices of the factor model. Note, any reverse coded items need to be recoded
ahead of time so that all variables/items are keyed in the same direction.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>composite(
  data,
  vrb.nm,
  level = 0.95,
  std = FALSE,
  ci.type = "delta",
  boot.ci.type = "bca.simple",
  R = 200L,
  fit.measures = c("chisq", "df", "tli", "cfi", "rmsea", "srmr"),
  se = "standard",
  test = "standard",
  missing = "fiml",
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="composite_+3A_data">data</code></td>
<td>
<p>data.frame of data.</p>
</td></tr>
<tr><td><code id="composite_+3A_vrb.nm">vrb.nm</code></td>
<td>
<p>character vector of colnames in <code>data</code> specifying the set
of variables/items.</p>
</td></tr>
<tr><td><code id="composite_+3A_level">level</code></td>
<td>
<p>double vector of length 1 with a value between 0 and 1
specifying what confidence level to use.</p>
</td></tr>
<tr><td><code id="composite_+3A_std">std</code></td>
<td>
<p>logical element of length 1 specifying if the composite
reliability should be computed for the standardized version of the
variables <code>data[vrb.nm]</code>.</p>
</td></tr>
<tr><td><code id="composite_+3A_ci.type">ci.type</code></td>
<td>
<p>character vector of length 1 specifying which type of
confidence interval to compute. The &quot;delta&quot; option uses the delta method to
compute a standard error and a symmetrical confidence interval. The &quot;boot&quot;
option uses bootstrapping to compute an asymmetrical confidence interval as
well as a (pseudo) standard error.</p>
</td></tr>
<tr><td><code id="composite_+3A_boot.ci.type">boot.ci.type</code></td>
<td>
<p>character vector of length 1 specifying which type of
bootstrapped confidence interval to compute. The options are: 1) &quot;norm&quot;, 2)
&quot;basic&quot;, 3) &quot;perc&quot;, 4) &quot;bca.simple&quot;. Only used if <code>ci.type</code> = &quot;boot&quot;.
See <code><a href="lavaan.html#topic+parameterEstimates">parameterEstimates</a></code> and its <code>boot.ci.type</code>
argument for details.</p>
</td></tr>
<tr><td><code id="composite_+3A_r">R</code></td>
<td>
<p>integer vector of length 1 specifying how many bootstrapped
resamples to compute. Note, as the number of bootstrapped resamples
increases, the computation time will increase. Only used if <code>ci.type</code>
is &quot;boot&quot;.</p>
</td></tr>
<tr><td><code id="composite_+3A_fit.measures">fit.measures</code></td>
<td>
<p>character vector specifying which model fit indices to
include in the return object. The default option includes the chi-square
test statistic (&quot;chisq&quot;), degrees of freedom (&quot;df&quot;), tucker-lewis index
(&quot;tli&quot;), comparative fit index (&quot;cfi&quot;), root mean square error of
approximation (&quot;rmsea&quot;), and standardized root mean residual (&quot;srmr&quot;). If
NULL, then no model fit indices are included in the return object. See
<code><a href="lavaan.html#topic+fitMeasures">fitMeasures</a></code> for details.</p>
</td></tr>
<tr><td><code id="composite_+3A_se">se</code></td>
<td>
<p>character vector of length 1 specifying which type of standard
errors to compute. If ci.type = &quot;boot&quot;, then the input value is ignored and
set to &quot;bootstrap&quot;. See <code><a href="lavaan.html#topic+lavOptions">lavOptions</a></code> and its <code>se</code>
argument for details.</p>
</td></tr>
<tr><td><code id="composite_+3A_test">test</code></td>
<td>
<p>character vector of length 1 specifying which type of test
statistic to compute. If ci.type = &quot;boot&quot;, then the input value is ignored
and set to &quot;bootstrap&quot;. See <code><a href="lavaan.html#topic+lavOptions">lavOptions</a></code> and its
<code>test</code> argument for details.</p>
</td></tr>
<tr><td><code id="composite_+3A_missing">missing</code></td>
<td>
<p>character vector of length 1 specifying how to handle missing
data. The default is &quot;fiml&quot; for full information maximum likelihood). See
<code><a href="lavaan.html#topic+lavOptions">lavOptions</a></code> and its <code>missing</code> argument for
details.</p>
</td></tr>
<tr><td><code id="composite_+3A_...">...</code></td>
<td>
<p>other arguments passed to <code><a href="lavaan.html#topic+cfa">cfa</a></code>. Use at your
own peril as some argument values could cause the function to break.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The factor model is estimated using the R package <code>lavaan</code>. The
reliability coefficients are calculated based on the square of the sum of the
factor loadings divided by the sum of the square of the sum of the factors
loadings and the sum of the error variances (Raykov, 2001).
</p>
<p><code>composite</code> is only able to use the &quot;ML&quot; estimator at the moment and
cannot model items as categorical/ordinal. However, different versions of
standard errors and test statistics are possible. For example, the &quot;MLM&quot;
estimator can be specified by <code>se</code> = &quot;robust.sem&quot; and <code>test</code> =
&quot;satorra.bentler&quot;; the &quot;MLR&quot; estimator can be specified by <code>se</code> =
&quot;robust.huber.white&quot; and <code>test</code> = &quot;yuan.bentler.mplus&quot;. See
<code><a href="lavaan.html#topic+lavOptions">lavOptions</a></code> and scroll down to Estimation options.
</p>


<h3>Value</h3>

<p>double vector where the first element is the composite reliability
coefficient (&quot;est&quot;) followed by its standard error (&quot;se&quot;), then its
confidence interval (&quot;lwr&quot; and &quot;upr&quot;), the average standardized factor
loading of the factor model (&quot;average_l&quot;) and number of variables (&quot;nvrb&quot;),
and finally any of the <code>fit.measures</code> requested.
</p>


<h3>References</h3>

<p>Raykov, T. (2001). Estimation of congeneric scale reliability using covariance
structure analysis with nonlinear constraints. British Journal of Mathematical
and Statistical Psychology, 54(2), 315323.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+composites">composites</a></code>
<code><a href="#topic+cronbach">cronbach</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# data
dat &lt;- psych::bfi[1:250, 2:5] # the first item is reverse coded

# delta method CI
composite(data = dat, vrb.nm = names(dat), ci.type = "delta")
composite(data = dat, vrb.nm = names(dat), ci.type = "delta", level = 0.99)
composite(data = dat, vrb.nm = names(dat), ci.type = "delta", std = TRUE)
composite(data = dat, vrb.nm = names(dat), ci.type = "delta", fit.measures = NULL)
composite(data = dat, vrb.nm = names(dat), ci.type = "delta",
   se = "robust.sem", test = "satorra.bentler", missing = "listwise") # MLM estimator
composite(data = dat, vrb.nm = names(dat), ci.type = "delta",
   se = "robust.huber.white", test = "yuan.bentler.mplus", missing = "fiml") # MLR estimator

## Not run: 
# bootstrapped CI
composite(data = dat, vrb.nm = names(dat), level = 0.95,
   ci.type = "boot") # slightly different estimate for some reason...
composite(data = dat, vrb.nm = names(dat), level = 0.95, ci.type = "boot",
   boot.ci.type = "perc", R = 250L) # probably want to use more resamples - this is just an example

## End(Not run)

# compare to semTools::reliability
psymet_obj &lt;- composite(data = dat, vrb.nm = names(dat))
psymet_est &lt;- unname(psymet_obj["est"])
lavaan_obj &lt;- lavaan::cfa(model = make.latent(names(dat)), data = dat,
   std.lv = TRUE, missing = "fiml")
semTools_obj &lt;- semTools::reliability(lavaan_obj)
semTools_est &lt;- semTools_obj["omega", "latent"]
all.equal(psymet_est, semTools_est)

</code></pre>

<hr>
<h2 id='composites'>Composite Reliability of Multiple Scores</h2><span id='topic+composites'></span>

<h3>Description</h3>

<p><code>composites</code> computes the composite reliability coefficient (sometimes
referred to as omega) for multiple sets of variables/items. The composite
reliability computed in <code>composites</code> assumes a undimensional factor
model for each set of variables/items with no error covariances. In addition
to the coefficients themselves, their standard errors and confidence
intervals are returned, the average standardized factor loading from the
factor models and number of variables/items in each set, and (optional) model
fit indices of the factor models. Note, any reverse coded items need to be
recoded ahead of time so that all items are keyed in the same direction for
each set of variables/items.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>composites(
  data,
  vrb.nm.list,
  level = 0.95,
  std = FALSE,
  ci.type = "delta",
  boot.ci.type = "bca.simple",
  R = 200L,
  fit.measures = c("chisq", "df", "tli", "cfi", "rmsea", "srmr"),
  se = "standard",
  test = "standard",
  missing = "fiml",
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="composites_+3A_data">data</code></td>
<td>
<p>data.frame of data.</p>
</td></tr>
<tr><td><code id="composites_+3A_vrb.nm.list">vrb.nm.list</code></td>
<td>
<p>list of character vectors containing colnames in
<code>data</code> specifying the multiple sets of variables/items.</p>
</td></tr>
<tr><td><code id="composites_+3A_level">level</code></td>
<td>
<p>double vector of length 1 with a value between 0 and 1
specifying what confidence level to use.</p>
</td></tr>
<tr><td><code id="composites_+3A_std">std</code></td>
<td>
<p>logical element of length 1 specifying if the composite
reliability should be computed for the standardized version of the
variables/items <code>data[unlist(vrb.nm.list)]</code>.</p>
</td></tr>
<tr><td><code id="composites_+3A_ci.type">ci.type</code></td>
<td>
<p>character vector of length 1 specifying which type of
confidence interval to compute. The &quot;delta&quot; option uses the delta method to
compute a standard error and a symmetrical confidence interval. The &quot;boot&quot;
option uses bootstrapping to compute an asymmetrical confidence interval as
well as a (pseudo) standard error.</p>
</td></tr>
<tr><td><code id="composites_+3A_boot.ci.type">boot.ci.type</code></td>
<td>
<p>character vector of length 1 specifying which type of
bootstrapped confidence interval to compute. The options are: 1) &quot;norm&quot;, 2)
&quot;basic&quot;, 3) &quot;perc&quot;, 4) &quot;bca.simple&quot;. Only used if <code>ci.type</code> = &quot;boot&quot;.
See <code><a href="lavaan.html#topic+parameterEstimates">parameterEstimates</a></code> and its <code>boot.ci.type</code>
argument for details.</p>
</td></tr>
<tr><td><code id="composites_+3A_r">R</code></td>
<td>
<p>integer vector of length 1 specifying how many bootstrapped
resamples to compute. Note, as the number of bootstrapped resamples
increases, the computation time will increase. Only used if <code>ci.type</code>
is &quot;boot&quot;.</p>
</td></tr>
<tr><td><code id="composites_+3A_fit.measures">fit.measures</code></td>
<td>
<p>character vector specifying which model fit indices to
include in the return object. The default option includes the chi-square
test statistic (&quot;chisq&quot;), degrees of freedom (&quot;df&quot;), tucker-lewis index
(&quot;tli&quot;), comparative fit index (&quot;cfi&quot;), root mean square error of
approximation (&quot;rmsea&quot;), and standardized root mean residual (&quot;srmr&quot;). If
NULL, then no model fit indices are included in the return object. See
<code><a href="lavaan.html#topic+fitMeasures">fitMeasures</a></code> for details.</p>
</td></tr>
<tr><td><code id="composites_+3A_se">se</code></td>
<td>
<p>character vector of length 1 specifying which type of standard
errors to compute. If ci.type = &quot;boot&quot;, then the input value is ignored and
implicitly set to &quot;bootstrap&quot;. See <code><a href="lavaan.html#topic+lavOptions">lavOptions</a></code> and its
<code>se</code> argument for details.</p>
</td></tr>
<tr><td><code id="composites_+3A_test">test</code></td>
<td>
<p>character vector of length 1 specifying which type of test
statistic to compute. If ci.type = &quot;boot&quot;, then the input value is ignored
and implicitly set to &quot;bootstrap&quot;. See <code><a href="lavaan.html#topic+lavOptions">lavOptions</a></code> and
its <code>test</code> argument for details.</p>
</td></tr>
<tr><td><code id="composites_+3A_missing">missing</code></td>
<td>
<p>character vector of length 1 specifying how to handle missing
data. The default is &quot;fiml&quot; for full information maximum likelihood. See
<code><a href="lavaan.html#topic+lavOptions">lavOptions</a></code> and its <code>missing</code> argument for
details.</p>
</td></tr>
<tr><td><code id="composites_+3A_...">...</code></td>
<td>
<p>other arguments passed to <code><a href="lavaan.html#topic+cfa">cfa</a></code>. Use at your
own peril as some argument values could cause the function to break.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The factor models are estimated using the R package <code>lavaan</code>. The
reliability coefficients are calculated based on the square of the sum of the
factor loadings divided by the sum of the square of the sum of the factors
loadings and the sum of the error variances (Raykov, 2001).
</p>
<p><code>composites</code> is only able to use the &quot;ML&quot; estimator at the moment and
cannot model items as categorical/ordinal. However, different versions of
standard errors and test statistics are possible. For example, the &quot;MLM&quot;
estimator can be specified by <code>se</code> = &quot;robust.sem&quot; and <code>test</code> =
&quot;satorra.bentler&quot;; the &quot;MLR&quot; estimator can be specified by <code>se</code> =
&quot;robust.huber.white&quot; and <code>test</code> = &quot;yuan.bentler.mplus&quot;. See
<code><a href="lavaan.html#topic+lavOptions">lavOptions</a></code> and scroll down to Estimation options for
details.
</p>


<h3>Value</h3>

<p>data.frame containing the composite reliability of each set of variables/items.
</p>

<dl>
<dt>est</dt><dd><p>estimate of the reliability coefficient</p>
</dd>
<dt>se</dt><dd><p>standard error of the reliability coefficient</p>
</dd>
<dt>lwr</dt><dd><p>lower bound of the confidence interval of the reliability coefficient</p>
</dd>
<dt>upr</dt><dd><p>upper bound of the confidence interval of the reliability coefficient</p>
</dd>
<dt>average_l</dt><dd><p>average standardized factor loading from the factor model</p>
</dd>
<dt>nvrb</dt><dd><p>number of variables/items</p>
</dd>
<dt>???</dt><dd><p>any model fit indices requested by the <code>fit.measures</code> argument</p>
</dd>
</dl>



<h3>References</h3>

<p>Raykov, T. (2001). Estimation of congeneric scale reliability using covariance
structure analysis with nonlinear constraints. British Journal of Mathematical
and Statistical Psychology, 54(2), 315323.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+composite">composite</a></code>
<code><a href="#topic+cronbachs">cronbachs</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
dat0 &lt;- psych::bfi[1:250, ]
dat1 &lt;- str2str::pick(x = dat0, val = c("A1","C4","C5","E1","E2","O2","O5",
   "gender","education","age"), not = TRUE, nm = TRUE)
vrb_nm_list &lt;- lapply(X = str2str::sn(c("E","N","C","A","O")), FUN = function(nm) {
   str2str::pick(x = names(dat1), val = nm, pat = TRUE)})
composites(data = dat1, vrb.nm.list = vrb_nm_list)
## Not run: 
start_time &lt;- Sys.time()
composites(data = dat1, vrb.nm.list = vrb_nm_list, ci.type = "boot",
   R = 5000L) # the function is not optimized for speed at the moment
   # since it will bootstrap separately for each set of variables/items
end_time &lt;- Sys.time()
print(end_time - start_time) # takes 10 minutes on my laptop

## End(Not run)
composites(data = attitude,
   vrb.nm.list = list(names(attitude))) # also works with only one set of variables/items

</code></pre>

<hr>
<h2 id='confint2'>Confidence Intervals from Statistical Information</h2><span id='topic+confint2'></span>

<h3>Description</h3>

<p><code>confint2</code> is a generic function for creating confidence intervals from
various statistical information (e.g., <code><a href="#topic+confint2.default">confint2.default</a></code>) or
object classes (e.g., <code><a href="#topic+confint2.boot">confint2.boot</a></code>). It is an alternative to
the original <code><a href="stats.html#topic+confint">confint</a></code> generic function in the <code>stats</code>
package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>confint2(obj, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="confint2_+3A_obj">obj</code></td>
<td>
<p>object of a particular class (e.g., &quot;boot&quot;) or the first argument
in the default method (e.g., the <code>obj</code> argument in <code><a href="#topic+confint2.default">confint2.default</a></code>)</p>
</td></tr>
<tr><td><code id="confint2_+3A_...">...</code></td>
<td>
<p>additional arguments specific to the particular method of <code>confint2</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>depends on the particular method of <code>confint2</code>, but usually a data.frame
with a column for the parameter estimate (&quot;est&quot;), standard error (&quot;se&quot;),
lower bound of the confidence interval (&quot;lwr&quot;), and upper bound of the confidence interval (&quot;upr&quot;).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+confint2.default">confint2.default</a></code> for the default method,
<code><a href="#topic+confint2.boot">confint2.boot</a></code> for the <code>boot</code> method,
</p>

<hr>
<h2 id='confint2.boot'>Bootstrapped Confidence Intervals from a <code>boot</code> Object</h2><span id='topic+confint2.boot'></span>

<h3>Description</h3>

<p><code>confint2.boot</code> is the <code>boot</code> method for the generic function
<code><a href="#topic+confint2">confint2</a></code> and computes bootstrapped confidence intervals from an object
of class <code>boot</code> (aka an object returned by the function
<code><a href="boot.html#topic+boot">boot</a></code>. The function is a simple wrapper for the car boot
methods for the <code>summary</code> and <code>confint</code> generics. See
<code><a href="car.html#topic+hist.boot">hist.boot</a></code> for details on those methods.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'boot'
confint2(obj, boot.ci.type = "perc", level = 0.95, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="confint2.boot_+3A_obj">obj</code></td>
<td>
<p>an object of class <code>boot</code> (aka an object returned by the
function <code><a href="boot.html#topic+boot">boot</a></code>).</p>
</td></tr>
<tr><td><code id="confint2.boot_+3A_boot.ci.type">boot.ci.type</code></td>
<td>
<p>character vector of length 1 specifying the type of
bootstrapped confidence interval to compute. The options are 1) &quot;perc&quot; for
the regular percentile method, 2) &quot;bca&quot; for bias-corrected and accelerated
percentile method, 3) &quot;norm&quot; for the normal method that uses the
bootstrapped standard error to construct symmetrical confidence intervals
with the classic formula around the bias-corrected estimate, and 4) &quot;basic&quot;
for the basic method. Note, &quot;stud&quot; for the studentized method is NOT an
option. See <code><a href="boot.html#topic+boot.ci">boot.ci</a></code> for details. Although a more
informative link is the following blogpost on bootstrapped confidence
intervals with the boot package
<a href="https://www.r-bloggers.com/2019/09/understanding-bootstrap-confidence-interval-output-from-the-r-boot-package/">https://www.r-bloggers.com/2019/09/understanding-bootstrap-confidence-interval-output-from-the-r-boot-package/</a>.</p>
</td></tr>
<tr><td><code id="confint2.boot_+3A_level">level</code></td>
<td>
<p>double vector of length 1 specifying the confidence level. Must
be between 0 and 1.</p>
</td></tr>
<tr><td><code id="confint2.boot_+3A_...">...</code></td>
<td>
<p>This argument has no use. Technically, it is additional arguments
for <code>confint2.boot</code>, but is only included for Roxygen2 to satisfy
&quot;checking S3 generic/method consistency&quot;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The bias-corrected and accelerated percentile method (<code>boot.ci.type</code> =
&quot;bca&quot;) will often fail if the number of bootstrapped resamples is less than
the sample size. Even still, it can fail for other reasons. Following
<code>car:::confint.boot</code>, <code>confint2.boot</code> gives a warning if the
bias-corrected and accelerated percentile method fails for any statistic, and
implicitly switches to the regular percentile method to prevent an error.
When multiple statistics were bootstrapped, it might be that the
bias-corrected and accelerated percentile method succeeded for most of the
statistics and only failed for one statistic; however, <code>confint2.boot</code>
will switch to using the regular percentile method for ALL the statistics.
This may change in the future.
</p>


<h3>Value</h3>

<p>data.frame will be returned with nrow equal to the number of
statistics bootstrapped and columns specified below. The rownames are the
names in the &quot;t0&quot; element of the <code>boot</code> object (default data.frame
rownames if the &quot;t0&quot; element does not have any names). The columns are the
following:
</p>

<dl>
<dt>est</dt><dd><p>original parameter estimates</p>
</dd>
<dt>se</dt><dd><p>bootstrapped standard errors (does not differ by <code>boot.ci.type</code>)</p>
</dd>
<dt>lwr</dt><dd><p>lower bound of the bootstrapped confidence intervals</p>
</dd>
<dt>upr</dt><dd><p>upper bound of the bootstrapped confidence intervals</p>
</dd>
</dl>



<h3>See Also</h3>

<p><code><a href="boot.html#topic+boot.ci">boot.ci</a></code>
<code><a href="car.html#topic+hist.boot">hist.boot</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# a single statistic
mean2 &lt;- function(x, i) mean(x[i], na.rm = TRUE)
boot_obj &lt;- boot::boot(data = attitude[[1]], statistic = mean2, R = 200L)
confint2.boot(boot_obj)
confint2.boot(boot_obj, boot.ci.type = "bca")
confint2.boot(boot_obj, level = 0.99)

# multiple statistics
colMeans2 &lt;- function(dat, i) colMeans(dat[i, ], na.rm = TRUE)
boot_obj &lt;- boot::boot(data = attitude, statistic = colMeans2, R = 200L)
confint2.boot(boot_obj)
confint2.boot(boot_obj, boot.ci.type = "bca")
confint2.boot(boot_obj, level = 0.99)

</code></pre>

<hr>
<h2 id='confint2.default'>Confidence Intervals from Parameter Estimates and Standard Errors</h2><span id='topic+confint2.default'></span>

<h3>Description</h3>

<p><code>confint2.default</code> is the default method for the generic function
<code><a href="#topic+confint2">confint2</a></code> and computes the statistical information for confidence
intervals from parameter estimates, standard errors, and degrees of freedom.
If degrees of freedom are not applicable or available, then <code>df</code> can be
set to <code>Inf</code> (the default) and critical z-values rather than critical
t-values will be used.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## Default S3 method:
confint2(obj, se, df = Inf, level = 0.95, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="confint2.default_+3A_obj">obj</code></td>
<td>
<p>numeric vector of parameter estimates. A better name for this
argument would be <code>est</code>; however, uses of S3 generic functions requires
the first argument to be the same name (i.e., <code>obj</code>) across methods.</p>
</td></tr>
<tr><td><code id="confint2.default_+3A_se">se</code></td>
<td>
<p>numeric vector of standard errors. Must be the same length as
<code>obj</code>.</p>
</td></tr>
<tr><td><code id="confint2.default_+3A_df">df</code></td>
<td>
<p>numeric vector of degrees of freedom. Must have length 1 or the
same length as <code>obj</code> and <code>se</code>. If degrees of freedom are not
applicable or available, then <code>df</code> can be set to <code>Inf</code> (the
default) and critical z-values rather than critical t-values will be used.</p>
</td></tr>
<tr><td><code id="confint2.default_+3A_level">level</code></td>
<td>
<p>double vector of length 1 specifying the confidence level. Must
be between 0 and 1.</p>
</td></tr>
<tr><td><code id="confint2.default_+3A_...">...</code></td>
<td>
<p>This argument has no use. Technically, it is additional arguments
for <code>confint2.default</code>, but is only included for Roxygen2 to satisfy
&quot;checking S3 generic/method consistency&quot;.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data.frame with nrow equal to the lengths of <code>obj</code> and
<code>se</code>. The rownames are taken from <code>obj</code>, unless <code>obj</code> does not
have any names and then the rownames are taken from the names of <code>se</code>.
If neither have names, then the rownames are automatic (i.e.,
<code>1:nrow()</code>). The columns are the following:
</p>

<dl>
<dt>est</dt><dd><p>parameter estimates</p>
</dd>
<dt>se</dt><dd><p>standard errors</p>
</dd>
<dt>lwr</dt><dd><p>lower bound of the confidence intervals</p>
</dd>
<dt>upr</dt><dd><p>upper bound of the confidence intervals</p>
</dd>
</dl>



<h3>See Also</h3>

<p><code><a href="#topic+confint2.boot">confint2.boot</a></code>
<code><a href="#topic+nhst">nhst</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# single estimate
confint2.default(obj = 10, se = 3)

# multiple estimates
est &lt;- colMeans(attitude)
se &lt;- apply(X = str2str::d2m(attitude), MARGIN = 2, FUN = function(vec)
   sqrt(var(vec) / length(vec)))
df &lt;- nrow(attitude) - 1
confint2.default(obj = est, se = se, df = df)
confint2.default(obj = est, se = se) # default is df = Inf and use of ctitical z-values
confint2.default(obj = est, se = se, df = df, level = 0.99)

# error
## Not run: 
confint2.default(obj = c(10, 12), se = c(3, 4, 5))

## End(Not run)

</code></pre>

<hr>
<h2 id='cor_by'>Correlation Matrix by Group</h2><span id='topic+cor_by'></span>

<h3>Description</h3>

<p><code>cor_by</code> computes a correlation matrix for each group within numeric
data. Only the correlation coefficients are determined and not any NHST
information. If that is desired, use <code><a href="#topic+corp_by">corp_by</a></code> which includes
significance symbols. <code>cor_by</code> is simply <code>cor</code> + <code>by2</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cor_by(
  data,
  vrb.nm,
  grp.nm,
  use = "pairwise.complete.obs",
  method = "pearson",
  sep = ".",
  check = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cor_by_+3A_data">data</code></td>
<td>
<p>data.frame of data.</p>
</td></tr>
<tr><td><code id="cor_by_+3A_vrb.nm">vrb.nm</code></td>
<td>
<p>character vector of colnames from <code>data</code> specifying the
variables.</p>
</td></tr>
<tr><td><code id="cor_by_+3A_grp.nm">grp.nm</code></td>
<td>
<p>character vector of colnames from <code>data</code> specifying the
groups.</p>
</td></tr>
<tr><td><code id="cor_by_+3A_use">use</code></td>
<td>
<p>character vector of length 1 specifying how to handle missing data
when computing the correlations. The options are 1)
&quot;pairwise.complete.obs&quot;, 2) &quot;complete.obs&quot;, 3) &quot;na.or.complete&quot;, 4)
&quot;all.obs&quot;, or 5) &quot;everything&quot;. See details of <code><a href="stats.html#topic+cor">cor</a></code>.</p>
</td></tr>
<tr><td><code id="cor_by_+3A_method">method</code></td>
<td>
<p>character vector of length 1 specifying the type of
correlations to be computed. The options are 1) &quot;pearson&quot;, 2) &quot;kendall&quot;, or
3) &quot;spearman&quot;. See details of <code><a href="stats.html#topic+cor">cor</a></code>.</p>
</td></tr>
<tr><td><code id="cor_by_+3A_sep">sep</code></td>
<td>
<p>character vector of length 1 specifying the string to combine the
group values together with. <code>sep</code> is only used if there are multiple
grouping variables (i.e., <code>length(grp.nm)</code> &gt; 1).</p>
</td></tr>
<tr><td><code id="cor_by_+3A_check">check</code></td>
<td>
<p>logical vector of length 1 specifying whether to check the
structure of the input arguments. For example, check whether
<code>data[vrb.nm]</code> are all mode numeric. This argument is available to
allow flexibility in whether the user values informative error messages
(TRUE) vs. computational efficiency (FALSE).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list of numeric matrices containing the correlations from each group.
The listnames are the unique combinations of the grouping variables,
separated by &quot;sep&quot; if multiple grouping variables (i.e.,
<code>length(grp.nm)</code> &gt; 1) are input:
<code>unique(interaction(data[grp.nm], sep = sep))</code>. The rownames and
colnames of each numeric matrix are <code>vrb.nm</code>.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+cor">cor</a></code> for full sample correlation matrixes,
<code><a href="#topic+corp">corp</a></code> for full sample correlation data.frames with significance symbols,
<code><a href="#topic+corp_by">corp_by</a></code> for full sample correlation data.farmes with significance symbols
by group.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# one grouping variable
cor_by(airquality, vrb.nm = c("Ozone","Solar.R","Wind"), grp.nm = "Month")
cor_by(airquality, vrb.nm = c("Ozone","Solar.R","Wind"), grp.nm = "Month",
   use = "complete.obs", method = "spearman")

# two grouping variables
cor_by(mtcars, vrb.nm = c("mpg","disp","drat","wt"), grp.nm = c("vs","am"))
cor_by(mtcars, vrb.nm = c("mpg","disp","drat","wt"), grp.nm = c("vs","am"),
   use = "complete.obs", method = "spearman", sep = "_")

</code></pre>

<hr>
<h2 id='cor_miss'>Point-biserial Correlations of Missingness</h2><span id='topic+cor_miss'></span>

<h3>Description</h3>

<p><code>cor_miss</code> computes (point-biserial) correlations between missingness on
data columns and scores on other data columns.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cor_miss(
  data,
  x.nm,
  m.nm,
  ov = FALSE,
  use = "pairwise.complete.obs",
  method = "pearson"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cor_miss_+3A_data">data</code></td>
<td>
<p>data.frame of data.</p>
</td></tr>
<tr><td><code id="cor_miss_+3A_x.nm">x.nm</code></td>
<td>
<p>character vector of colnames in <code>data</code> to be the predictors
of missingness.</p>
</td></tr>
<tr><td><code id="cor_miss_+3A_m.nm">m.nm</code></td>
<td>
<p>character vector of colnames in <code>data</code> to specify missing
data on.</p>
</td></tr>
<tr><td><code id="cor_miss_+3A_ov">ov</code></td>
<td>
<p>logical vector of length 1 specifying whether the correlations
should be with &quot;observedness&quot; rather than missingness.</p>
</td></tr>
<tr><td><code id="cor_miss_+3A_use">use</code></td>
<td>
<p>character vector of length 1 specifying how to deal with missing
data in the predictor columns. See <code><a href="stats.html#topic+cor">cor</a></code>.</p>
</td></tr>
<tr><td><code id="cor_miss_+3A_method">method</code></td>
<td>
<p>character vector of length 1 specifying what type of
correlations to compute. See <code><a href="stats.html#topic+cor">cor</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>cor_miss</code> calls <code><a href="#topic+make.dumNA">make.dumNA</a></code> to create dummy vectors representing
missingness on the <code>data[m.nm]</code> columns.
</p>


<h3>Value</h3>

<p>numeric matrix of (point-biserial) correlations between rows of
predictors and columns of missingness.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
cor_miss(data = airquality, x.nm = c("Wind","Temp","Month","Day"),
   m.nm = c("Ozone","Solar.R"))
cor_miss(data = airquality, x.nm = c("Wind","Temp","Month","Day"),
   m.nm = c("Ozone","Solar.R"), ov = TRUE) # correlations with "observedness"
cor_miss(data = airquality, x.nm = c("Wind","Temp","Month","Day"),
   m.nm = c("Ozone","Solar.R"), use = "complete.obs", method = "kendall")

</code></pre>

<hr>
<h2 id='cor_ml'>Multilevel Correlation Matrices</h2><span id='topic+cor_ml'></span>

<h3>Description</h3>

<p><code>cor_ml</code> decomposes correlations from multilevel data into within-group
and between-group correlations. The workhorse of the function is
<code><a href="psych.html#topic+statsBy">statsBy</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cor_ml(data, vrb.nm, grp.nm, use = "pairwise.complete.obs", method = "pearson")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cor_ml_+3A_data">data</code></td>
<td>
<p>data.frame of data.</p>
</td></tr>
<tr><td><code id="cor_ml_+3A_vrb.nm">vrb.nm</code></td>
<td>
<p>character vector of colnames from <code>data</code> specifying the
variable columns.</p>
</td></tr>
<tr><td><code id="cor_ml_+3A_grp.nm">grp.nm</code></td>
<td>
<p>character vector of length 1 of a colname from <code>data</code>
specifying the grouping column.</p>
</td></tr>
<tr><td><code id="cor_ml_+3A_use">use</code></td>
<td>
<p>character vector of length 1 specifying how to handle missing
values when computing the correlations. The options are: 1.
&quot;pairwise.complete.obs&quot; which uses pairwise deletion, 2. &quot;complete.obs&quot;
which uses listwise deletion, and 3. &quot;everything&quot; which uses all cases and
returns NA for any correlations from columns in <code>data[vrb.nm]</code> with
missing values.</p>
</td></tr>
<tr><td><code id="cor_ml_+3A_method">method</code></td>
<td>
<p>character vector of length 1 specifying which type of
correlations to compute. The options are: 1. &quot;pearson&quot; for traditional
Pearson product-moment correlations, 2. &quot;kendall&quot; for Kendall rank
correlations, and 3. &quot;spearman&quot; for Spearman rank correlations.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list with two elements named &quot;within&quot; and &quot;between&quot; each containing a
numeric matrix. The first &quot;within&quot; matrix is the within-group correlation
matrix and the second &quot;between&quot; matrix is the between-group correlation
matrix. The rownames and colnames of each numeric matrix are <code>vrb.nm</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+corp_ml">corp_ml</a></code> for multilevel correlations with significance symbols,
<code><a href="#topic+cor_by">cor_by</a></code> for correlation matrices by group,
<code><a href="stats.html#topic+cor">cor</a></code> for traditional, single-level correlation matrices,
<code><a href="psych.html#topic+statsBy">statsBy</a></code> the workhorse for the <code>cor_ml</code> function,
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# traditional use
tmp &lt;- c("outcome","case","session","trt_time") # roxygen2 does not like c() inside []
dat &lt;- as.data.frame(lmeInfo::Bryant2016)[tmp]
stats_by &lt;- psych::statsBy(dat, group = "case") # requires you to include "case" column in dat
cor_ml(data = dat, vrb.nm = c("outcome","session","trt_time"), grp.nm = "case")

# varying the \code{use} and \code{method} arguments
cor_ml(data = airquality, vrb.nm = c("Ozone","Solar.R","Wind","Temp"), grp.nm = "Month",
   use = "pairwise", method = "pearson")
cor_ml(data = airquality, vrb.nm = c("Ozone","Solar.R","Wind","Temp"), grp.nm = "Month",
   use = "complete", method = "kendall")
cor_ml(data = airquality, vrb.nm = c("Ozone","Solar.R","Wind","Temp"), grp.nm = "Month",
   use = "everything", method = "spearman")

</code></pre>

<hr>
<h2 id='corp'>Bivariate Correlations with Significant Symbols</h2><span id='topic+corp'></span>

<h3>Description</h3>

<p><code>corp</code> computes bivariate correlations and their associated p-values.
The function is primarily for preparing a correlation table for publication:
the correlations are appended by significant symbols (e.g., asterixis),
<code>corp</code> is simply <code><a href="psych.html#topic+corr.test">corr.test</a></code> + <code>add_sig_cor</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>corp(
  data,
  vrb.nm,
  use = "pairwise.complete.obs",
  method = "pearson",
  digits = 3L,
  p.10 = "",
  p.05 = "*",
  p.01 = "**",
  p.001 = "***",
  lead.zero = FALSE,
  trail.zero = TRUE,
  plus = FALSE,
  diags = FALSE,
  lower = TRUE,
  upper = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="corp_+3A_data">data</code></td>
<td>
<p>data.frame of data.</p>
</td></tr>
<tr><td><code id="corp_+3A_vrb.nm">vrb.nm</code></td>
<td>
<p>character vector of colnames from <code>data</code> specifying the
variable columns.</p>
</td></tr>
<tr><td><code id="corp_+3A_use">use</code></td>
<td>
<p>character vector of length 1 specifying how to handle missing data
when computing the correlations. The options are 1)
&quot;pairwise.complete.obs&quot;, 2) &quot;complete.obs&quot;, 3) &quot;na.or.complete&quot;, 4)
&quot;all.obs&quot;, or 5) &quot;everything&quot;. See details of <code><a href="stats.html#topic+cor">cor</a></code>.</p>
</td></tr>
<tr><td><code id="corp_+3A_method">method</code></td>
<td>
<p>character vector of length 1 specifying the type of
correlations to be computed. The options are 1) &quot;pearson&quot;, 2) &quot;kendall&quot;, or
3) &quot;spearman&quot;. See details of <code><a href="stats.html#topic+cor">cor</a></code>.</p>
</td></tr>
<tr><td><code id="corp_+3A_digits">digits</code></td>
<td>
<p>integer vector of length 1 specifying the number of decimals to
round to.</p>
</td></tr>
<tr><td><code id="corp_+3A_p.10">p.10</code></td>
<td>
<p>character vector of length 1 specifying which symbol to append to
the end of any correlation significant at the p &lt; .10 level.</p>
</td></tr>
<tr><td><code id="corp_+3A_p.05">p.05</code></td>
<td>
<p>character vector of length 1 specifying which symbol to append to
the end of any correlation significant at the p &lt; .05 level.</p>
</td></tr>
<tr><td><code id="corp_+3A_p.01">p.01</code></td>
<td>
<p>character vector of length 1 specifying which symbol to append to
the end of any correlation significant at the p &lt; .01 level.</p>
</td></tr>
<tr><td><code id="corp_+3A_p.001">p.001</code></td>
<td>
<p>character vector of length 1 specifying which symbol to append
to the end of any correlation significant at the p &lt; .001 level.</p>
</td></tr>
<tr><td><code id="corp_+3A_lead.zero">lead.zero</code></td>
<td>
<p>logical vector of length 1 specifying whether to retain a
zero in front of the decimal place.</p>
</td></tr>
<tr><td><code id="corp_+3A_trail.zero">trail.zero</code></td>
<td>
<p>logical vector of length 1 specifying whether to retain
zeros after the decimal place (due to rounding).</p>
</td></tr>
<tr><td><code id="corp_+3A_plus">plus</code></td>
<td>
<p>logical vector of length 1 specifying whether to include a plus
sign in front of positive correlations (minus signs are always in front of
negative correlations).</p>
</td></tr>
<tr><td><code id="corp_+3A_diags">diags</code></td>
<td>
<p>logical vector of length 1 specifying whether to retain the
values in the diagonal of the correlation matrix. If TRUE, then the
diagonal will be 1s with <code>digits</code> number of zeros after the decimal
place (and no significant symbols). If FALSE, then the diagonal will be NA.</p>
</td></tr>
<tr><td><code id="corp_+3A_lower">lower</code></td>
<td>
<p>logical vector of length 1 specifying whether to retain the
lower triangle of the correlation matrix. If TRUE, then the lower triangle
correlations and their significance symbols are retained. If FAlSE, then
the lower triangle will all be NA.</p>
</td></tr>
<tr><td><code id="corp_+3A_upper">upper</code></td>
<td>
<p>logical vector of length 1 specifying whether to retain the
upper triangle of the correlation matrix. If TRUE, then the upper triangle
correlations and their significance symbols are retained. If FAlSE, then
the upper triangle will all be NA.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data.frame with rownames and colnames equal to <code>vrb.nm</code>
containing the bivariate correlations with significance symbols after the
correlation value, specified by the arguments <code>p.10</code>, <code>p.05</code>,
<code>p.01</code>, and <code>p.001</code> arguments. The specific elements of the
return object are determined by the other arguments.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+add_sig_cor">add_sig_cor</a></code> for adding significant symbols to a correlation matrix,
<code><a href="#topic+add_sig">add_sig</a></code> for adding significant symbols to any (atomic) vector, matrix, or (3D+) array,
<code><a href="stats.html#topic+cor">cor</a></code> for computing only the correlation coefficients themselves
<code><a href="psych.html#topic+corr.test">corr.test</a></code> for a function providing confidence intervals as well
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
corp(data = mtcars, vrb.nm = c("mpg","cyl","disp","hp","drat")) # no quotes b/c a data.frame
corp(data = attitude, vrb.nm = colnames(attitude))
corp(data = attitude, vrb.nm = colnames(attitude), p.10 = "'") # advance &amp; privileges
corp(data = airquality, vrb.nm = colnames(airquality), plus = TRUE)

</code></pre>

<hr>
<h2 id='corp_by'>Bivariate Correlations with Significant Symbols by Group</h2><span id='topic+corp_by'></span>

<h3>Description</h3>

<p><code>corp_by</code> computes a correlation data.frame for each group within
numeric data. The correlation coefficients are appended by their significant
symbols based on their associated p-values. If only the correlation
coefficients are desired, use <code>cor_by</code> which returns a list of numeric
matrices. <code>corp_by</code> is simply <code>corp</code> + <code>by2</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>corp_by(
  data,
  vrb.nm,
  grp.nm,
  use = "pairwise.complete.obs",
  method = "pearson",
  sep = ".",
  digits = 3L,
  p.10 = "",
  p.05 = "*",
  p.01 = "**",
  p.001 = "***",
  lead.zero = FALSE,
  trail.zero = TRUE,
  plus = FALSE,
  diags = FALSE,
  lower = TRUE,
  upper = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="corp_by_+3A_data">data</code></td>
<td>
<p>data.frame of data.</p>
</td></tr>
<tr><td><code id="corp_by_+3A_vrb.nm">vrb.nm</code></td>
<td>
<p>character vector of colnames from <code>data</code> specifying the
variables.</p>
</td></tr>
<tr><td><code id="corp_by_+3A_grp.nm">grp.nm</code></td>
<td>
<p>character vector of colnames from <code>data</code> specifying the
groups.</p>
</td></tr>
<tr><td><code id="corp_by_+3A_use">use</code></td>
<td>
<p>character vector of length 1 specifying how to handle missing data
when computing the correlations. The options are 1)
&quot;pairwise.complete.obs&quot;, 2) &quot;complete.obs&quot;, 3) &quot;na.or.complete&quot;, 4)
&quot;all.obs&quot;, or 5) &quot;everything&quot;. See details of <code><a href="stats.html#topic+cor">cor</a></code>.</p>
</td></tr>
<tr><td><code id="corp_by_+3A_method">method</code></td>
<td>
<p>character vector of length 1 specifying the type of
correlations to be computed. The options are 1) &quot;pearson&quot;, 2) &quot;kendall&quot;, or
3) &quot;spearman&quot;. See details of <code><a href="stats.html#topic+cor">cor</a></code>.</p>
</td></tr>
<tr><td><code id="corp_by_+3A_sep">sep</code></td>
<td>
<p>character vector of length 1 specifying the string to combine the
group values together with. <code>sep</code> is only used if there are multiple
grouping variables (i.e., <code>length(grp.nm)</code> &gt; 1).</p>
</td></tr>
<tr><td><code id="corp_by_+3A_digits">digits</code></td>
<td>
<p>integer vector of length 1 specifying the number of decimals to
round to.</p>
</td></tr>
<tr><td><code id="corp_by_+3A_p.10">p.10</code></td>
<td>
<p>character vector of length 1 specifying which symbol to append to
the end of any correlation significant at the p &lt; .10 level.</p>
</td></tr>
<tr><td><code id="corp_by_+3A_p.05">p.05</code></td>
<td>
<p>character vector of length 1 specifying which symbol to append to
the end of any correlation significant at the p &lt; .05 level.</p>
</td></tr>
<tr><td><code id="corp_by_+3A_p.01">p.01</code></td>
<td>
<p>character vector of length 1 specifying which symbol to append to
the end of any correlation significant at the p &lt; .01 level.</p>
</td></tr>
<tr><td><code id="corp_by_+3A_p.001">p.001</code></td>
<td>
<p>character vector of length 1 specifying which symbol to append
to the end of any correlation significant at the p &lt; .001 level.</p>
</td></tr>
<tr><td><code id="corp_by_+3A_lead.zero">lead.zero</code></td>
<td>
<p>logical vector of length 1 specifying whether to retain a
zero in front of the decimal place.</p>
</td></tr>
<tr><td><code id="corp_by_+3A_trail.zero">trail.zero</code></td>
<td>
<p>logical vector of length 1 specifying whether to retain
zeros after the decimal place (due to rounding).</p>
</td></tr>
<tr><td><code id="corp_by_+3A_plus">plus</code></td>
<td>
<p>logical vector of length 1 specifying whether to include a plus
sign in front of positive correlations (minus signs are always in front of
negative correlations).</p>
</td></tr>
<tr><td><code id="corp_by_+3A_diags">diags</code></td>
<td>
<p>logical vector of length 1 specifying whether to retain the
values in the diagonal of the correlation matrix. If TRUE, then the
diagonal will be 1s with <code>digits</code> number of zeros after the decimal
place (and no significant symbols). If FALSE, then the diagonal will be NA.</p>
</td></tr>
<tr><td><code id="corp_by_+3A_lower">lower</code></td>
<td>
<p>logical vector of length 1 specifying whether to retain the
lower triangle of the correlation matrix. If TRUE, then the lower triangle
correlations and their significance symbols are retained. If FAlSE, then
the lower triangle will all be NA.</p>
</td></tr>
<tr><td><code id="corp_by_+3A_upper">upper</code></td>
<td>
<p>logical vector of length 1 specifying whether to retain the
upper triangle of the correlation matrix. If TRUE, then the upper triangle
correlations and their significance symbols are retained. If FAlSE, then
the upper triangle will all be NA.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list of data.frames containing the correlation coefficients and their
appended significance symbols based upon their associated p-values. The
listnames are the unique combinations of the grouping variables, separated
by &quot;sep&quot; if multiple grouping variables (i.e., <code>length(grp.nm)</code> &gt; 1)
are input: <code>unique(interaction(data[grp.nm], sep = sep))</code>. For each
data.frame, the rownames and colnames = <code>vrb.nm</code>. The significance
symbols are specified by the arguments <code>p.10</code>, <code>p.05</code>,
<code>p.01</code>, and <code>p.001</code>, after the correlation value. The specific
elements of the return object are determined by the other arguments.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+corp">corp</a></code>
<code><a href="#topic+cor_by">cor_by</a></code>
<code><a href="stats.html#topic+cor">cor</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# one grouping variable
corp_by(airquality, vrb.nm = c("Ozone","Solar.R","Wind"), grp.nm = "Month")
corp_by(airquality, vrb.nm = c("Ozone","Solar.R","Wind"), grp.nm = "Month",
   use = "complete.obs", method = "spearman")

# two grouping variables
corp_by(mtcars, vrb.nm = c("mpg","disp","drat","wt"), grp.nm = c("vs","am"))
corp_by(mtcars, vrb.nm = c("mpg","disp","drat","wt"), grp.nm = c("vs","am"),
   use = "complete.obs", method = "spearman", sep = "_")

</code></pre>

<hr>
<h2 id='corp_miss'>Point-biserial Correlations of Missingness With Significant Symbols</h2><span id='topic+corp_miss'></span>

<h3>Description</h3>

<p><code>corp_miss</code> computes (point-biserial) correlations between missingness
on data columns and scores on other data columns. It also appends
significance symbols at the end of the correlations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>corp_miss(
  data,
  x.nm,
  m.nm,
  ov = FALSE,
  use = "pairwise.complete.obs",
  method = "pearson",
  m.suffix = if (ov) "_ov" else "_na",
  digits = 3L,
  p.10 = "",
  p.05 = "*",
  p.01 = "**",
  p.001 = "***",
  lead.zero = FALSE,
  trail.zero = TRUE,
  plus = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="corp_miss_+3A_data">data</code></td>
<td>
<p>data.frame of data.</p>
</td></tr>
<tr><td><code id="corp_miss_+3A_x.nm">x.nm</code></td>
<td>
<p>character vector of colnames in <code>data</code> to be the predictors
of missingness.</p>
</td></tr>
<tr><td><code id="corp_miss_+3A_m.nm">m.nm</code></td>
<td>
<p>character vector of colnames in <code>data</code> to specify missing
data on.</p>
</td></tr>
<tr><td><code id="corp_miss_+3A_ov">ov</code></td>
<td>
<p>logical vector of length 1 specifying whether the correlations
should be with &quot;observedness&quot; rather than missingness.</p>
</td></tr>
<tr><td><code id="corp_miss_+3A_use">use</code></td>
<td>
<p>character vector of length 1 specifying how to deal with missing
data in the predictor columns. See <code><a href="stats.html#topic+cor">cor</a></code>.</p>
</td></tr>
<tr><td><code id="corp_miss_+3A_method">method</code></td>
<td>
<p>character vector of length 1 specifying what type of
correlations to compute. See <code><a href="stats.html#topic+cor">cor</a></code>.</p>
</td></tr>
<tr><td><code id="corp_miss_+3A_m.suffix">m.suffix</code></td>
<td>
<p>character vector of length 1 specifying a string to oppend to
the end of the colnames to clarify whether they refer to missingness or
&quot;observedness&quot;. Default is &quot;_na&quot; if <code>ov</code> = FALSE and &quot;_ov&quot; if
<code>ov</code> = TRUE.</p>
</td></tr>
<tr><td><code id="corp_miss_+3A_digits">digits</code></td>
<td>
<p>integer vector of length 1 specifying the number of decimals to
round to.</p>
</td></tr>
<tr><td><code id="corp_miss_+3A_p.10">p.10</code></td>
<td>
<p>character vector of length 1 specifying which symbol to append to
the end of any correlation significant at the p &lt; .10 level.</p>
</td></tr>
<tr><td><code id="corp_miss_+3A_p.05">p.05</code></td>
<td>
<p>character vector of length 1 specifying which symbol to append to
the end of any correlation significant at the p &lt; .05 level.</p>
</td></tr>
<tr><td><code id="corp_miss_+3A_p.01">p.01</code></td>
<td>
<p>character vector of length 1 specifying which symbol to append to
the end of any correlation significant at the p &lt; .01 level.</p>
</td></tr>
<tr><td><code id="corp_miss_+3A_p.001">p.001</code></td>
<td>
<p>character vector of length 1 specifying which symbol to append
to the end of any correlation significant at the p &lt; .001 level.</p>
</td></tr>
<tr><td><code id="corp_miss_+3A_lead.zero">lead.zero</code></td>
<td>
<p>logical vector of length 1 specifying whether to retain a
zero in front of the decimal place.</p>
</td></tr>
<tr><td><code id="corp_miss_+3A_trail.zero">trail.zero</code></td>
<td>
<p>logical vector of length 1 specifying whether to retain
zeros after the decimal place (due to rounding).</p>
</td></tr>
<tr><td><code id="corp_miss_+3A_plus">plus</code></td>
<td>
<p>logical vector of length 1 specifying whether to include a plus
sign in front of positive correlations (minus signs are always in front of
negative correlations).</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>cor_miss</code> calls <code>make.dumNA</code> to create dummy vectors representing
missingness on the <code>data[m.nm]</code> columns.
</p>


<h3>Value</h3>

<p>numeric matrix of (point-biserial) correlations between rows of
predictors and columns of missingness.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
corp_miss(data = airquality, x.nm = c("Wind","Temp","Month","Day"),
   m.nm = c("Ozone","Solar.R"))
corp_miss(data = airquality, x.nm = c("Wind","Temp","Month","Day"),
   m.nm = c("Ozone","Solar.R"), ov = TRUE) # correlations with "observedness"
corp_miss(data = airquality, x.nm = c("Wind","Temp","Month","Day"),
   m.nm = c("Ozone","Solar.R"), use = "complete.obs", method = "kendall")

</code></pre>

<hr>
<h2 id='corp_ml'><code>corp_ml</code> decomposes correlations from multilevel data into within-group
and between-group correlations as well as adds significance symbols to the
end of each value. The workhorse of the function is
<code><a href="psych.html#topic+statsBy">statsBy</a></code>. <code>corp_ml</code> is simply a combination of
<code><a href="#topic+cor_ml">cor_ml</a></code> and <code><a href="#topic+add_sig_cor">add_sig_cor</a></code>.</h2><span id='topic+corp_ml'></span>

<h3>Description</h3>

<p><code>corp_ml</code> decomposes correlations from multilevel data into within-group
and between-group correlations as well as adds significance symbols to the
end of each value. The workhorse of the function is
<code><a href="psych.html#topic+statsBy">statsBy</a></code>. <code>corp_ml</code> is simply a combination of
<code><a href="#topic+cor_ml">cor_ml</a></code> and <code><a href="#topic+add_sig_cor">add_sig_cor</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>corp_ml(
  data,
  vrb.nm,
  grp.nm,
  use = "pairwise.complete.obs",
  method = "pearson",
  digits = 3L,
  p.10 = "",
  p.05 = "*",
  p.01 = "**",
  p.001 = "***",
  lead.zero = FALSE,
  trail.zero = TRUE,
  plus = FALSE,
  diags = FALSE,
  lower = TRUE,
  upper = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="corp_ml_+3A_data">data</code></td>
<td>
<p>data.frame of data.</p>
</td></tr>
<tr><td><code id="corp_ml_+3A_vrb.nm">vrb.nm</code></td>
<td>
<p>character vector of colnames from <code>data</code> specifying the
variable columns.</p>
</td></tr>
<tr><td><code id="corp_ml_+3A_grp.nm">grp.nm</code></td>
<td>
<p>character vector of length 1 of a colname from <code>data</code>
specifying the grouping column.</p>
</td></tr>
<tr><td><code id="corp_ml_+3A_use">use</code></td>
<td>
<p>character vector of length 1 specifying how to handle missing
values when computing the correlations. The options are: 1)
&quot;pairwise.complete.obs&quot; which uses pairwise deletion, 2) &quot;complete.obs&quot;
which uses listwise deletion, and 3) &quot;everything&quot; which uses all cases and
returns NA for any correlations from columns in <code>data[vrb.nm]</code> with
missing values.</p>
</td></tr>
<tr><td><code id="corp_ml_+3A_method">method</code></td>
<td>
<p>character vector of length 1 specifying which type of
correlations to compute. The options are: 1) &quot;pearson&quot; for traditional
Pearson product-moment correlations, 2) &quot;kendall&quot; for Kendall rank
correlations, and 3) &quot;spearman&quot; for Spearman rank correlations.</p>
</td></tr>
<tr><td><code id="corp_ml_+3A_digits">digits</code></td>
<td>
<p>integer vector of length 1 specifying the number of decimals to
round to.</p>
</td></tr>
<tr><td><code id="corp_ml_+3A_p.10">p.10</code></td>
<td>
<p>character vector of length 1 specifying which symbol to append to
the end of any correlation significant at the p &lt; .10 level.</p>
</td></tr>
<tr><td><code id="corp_ml_+3A_p.05">p.05</code></td>
<td>
<p>character vector of length 1 specifying which symbol to append to
the end of any correlation significant at the p &lt; .05 level.</p>
</td></tr>
<tr><td><code id="corp_ml_+3A_p.01">p.01</code></td>
<td>
<p>character vector of length 1 specifying which symbol to append to
the end of any correlation significant at the p &lt; .01 level.</p>
</td></tr>
<tr><td><code id="corp_ml_+3A_p.001">p.001</code></td>
<td>
<p>character vector of length 1 specifying which symbol to append
to the end of any correlation significant at the p &lt; .001 level.</p>
</td></tr>
<tr><td><code id="corp_ml_+3A_lead.zero">lead.zero</code></td>
<td>
<p>logical vector of length 1 specifying whether to retain a
zero in front of the decimal place.</p>
</td></tr>
<tr><td><code id="corp_ml_+3A_trail.zero">trail.zero</code></td>
<td>
<p>logical vector of length 1 specifying whether to retain
zeros after the decimal place (due to rounding).</p>
</td></tr>
<tr><td><code id="corp_ml_+3A_plus">plus</code></td>
<td>
<p>logical vector of length 1 specifying whether to include a plus
sign in front of positive correlations (minus signs are always in front of
negative correlations).</p>
</td></tr>
<tr><td><code id="corp_ml_+3A_diags">diags</code></td>
<td>
<p>logical vector of length 1 specifying whether to retain the
values in the diagonal of the correlation matrix. If TRUE, then the
diagonal will be 1s with <code>digits</code> number of zeros after the decimal
place (and no significant symbols). If FALSE, then the diagonal will be NA.</p>
</td></tr>
<tr><td><code id="corp_ml_+3A_lower">lower</code></td>
<td>
<p>logical vector of length 1 specifying whether to retain the
lower triangle of the correlation matrix. If TRUE, then the lower triangle
correlations and their significance symbols are retained. If FAlSE, then
the lower triangle will all be NA.</p>
</td></tr>
<tr><td><code id="corp_ml_+3A_upper">upper</code></td>
<td>
<p>logical vector of length 1 specifying whether to retain the
upper triangle of the correlation matrix. If TRUE, then the upper triangle
correlations and their significance symbols are retained. If FAlSE, then
the upper triangle will all be NA.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list of two elements that are data.frames with names &quot;within&quot; and
&quot;between&quot;. The first data.frame has the within-group correlations with
their significance symbols at the end of the statistically significant
correlations based on their associated p-value. The second data.frame has
the between-group correlations with their significance symbols at the end
of the statistically significant correlations based on their associated
p-values. The rownames and colnames of each dataframe are <code>vrb.nm</code>.
The formatting of the two data.frames depends on several of the arguments.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cor_ml">cor_ml</a></code> for multilevel correlations without significance symbols,
<code><a href="#topic+corp_by">corp_by</a></code> for correlations with significance symbols by group,
<code><a href="psych.html#topic+statsBy">statsBy</a></code> the workhorse for the <code>corp_ml</code> function,
<code><a href="#topic+add_sig_cor">add_sig_cor</a></code> for adding significant symbols to correlation matrices,
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# traditional use
tmp &lt;- c("outcome","case","session","trt_time") # roxygen2 does not like c() inside []
dat &lt;- as.data.frame(lmeInfo::Bryant2016)[tmp]
stats_by &lt;- psych::statsBy(dat, group = "case") # requires you to include "case" column in dat
corp_ml(data = dat, vrb.nm = c("outcome","session","trt_time"), grp.nm = "case")

# varying the `use` and `method` arguments
corp_ml(data = airquality, vrb.nm = c("Ozone","Solar.R","Wind","Temp"), grp.nm = "Month",
   use = "pairwise", method = "pearson")
corp_ml(data = airquality, vrb.nm = c("Ozone","Solar.R","Wind","Temp"), grp.nm = "Month",
   use = "complete", method = "kendall")
corp_ml(data = airquality, vrb.nm = c("Ozone","Solar.R","Wind","Temp"), grp.nm = "Month",
   use = "everything", method = "spearman")

</code></pre>

<hr>
<h2 id='covs_test'>Covariances Test of Significance</h2><span id='topic+covs_test'></span>

<h3>Description</h3>

<p><code>covs_test</code> computes sample covariances and tests for their significance
with the Pearson method assuming multivariate normality of the data. Note,
the normal-theory significance test for the covariance is much more sensitive
to departures from normality than the significant test for the mean. This
function is the covariance analogue to the <code>psych::corr.test()</code> function
for correlations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>covs_test(data, vrb.nm, use = "pairwise", ci.level = 0.95, rtn.dfm = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="covs_test_+3A_data">data</code></td>
<td>
<p>data.frame of data.</p>
</td></tr>
<tr><td><code id="covs_test_+3A_vrb.nm">vrb.nm</code></td>
<td>
<p>character vector of colnames specifying the variables in
<code>data</code> to conduct the significant test of the covariances.</p>
</td></tr>
<tr><td><code id="covs_test_+3A_use">use</code></td>
<td>
<p>character vector of length 1 specifying how missing values are
handled. Currently, there are only two options: 1) &quot;pairwise&quot; for pairwise
deletion (i.e., <code>cov(use = "pairwise.complete.obs")</code>), or 2)
&quot;complete&quot; for listwise deletion (i.e., <code>cov(use = "complete.obs")</code>).</p>
</td></tr>
<tr><td><code id="covs_test_+3A_ci.level">ci.level</code></td>
<td>
<p>numeric vector of length 1 specifying the confidence level.
It must be between 0 and 1 - or it can be NULL in which case confidence
intervals are not computed and the return object does not have &quot;lwr&quot; or
&quot;upr&quot; columns.</p>
</td></tr>
<tr><td><code id="covs_test_+3A_rtn.dfm">rtn.dfm</code></td>
<td>
<p>logical vector of length 1 specifying whether the return
object should be an array (FALSE) or data.frame (TRUE). If an array, then
the first two dimensions are the matrix dimensions from the covariance
matrix and the 3rd dimension (aka layers) contains the statistical
information (e.g., est, se, t). If  data.frame, then the first two columns
are the matrix dimensions from the covariance matrix expanded and the rest
of the columns contain the statistical information (e.g., est, se, t).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>If <code>rtn.dfm = FALSE</code>, an array where its first two dimensions
are the matrix dimensions from the covariance matrix and the 3rd dimension
(aka layers) contains the statistical information detailed below. If
<code>rtn.dfm = TRUE</code>, a data.frame where its first two columns are the
expanded matrix dimensions from the covariance matrix and the rest of the
columns contain the statistical information detailed below:
</p>

<dl>
<dt>cov</dt><dd><p>sample covariances</p>
</dd>
<dt>se</dt><dd><p>standard errors of the covariances</p>
</dd>
<dt>t</dt><dd><p>t-values</p>
</dd>
<dt>df</dt><dd><p>degrees of freedom (n - 2)</p>
</dd>
<dt>p</dt><dd><p>two-sided p-values</p>
</dd>
<dt>lwr</dt><dd><p>lower bound of the confidence intervals (excluded if <code>ci.level = NULL</code>)</p>
</dd>
<dt>upr</dt><dd><p>upper bound of the confidence intervals (excluded if <code>ci.level = NULL</code>)</p>
</dd>
</dl>



<h3>See Also</h3>

<p><code><a href="stats.html#topic+cov">cov</a></code> for covariance matrix estimates,
<code><a href="psych.html#topic+corr.test">corr.test</a></code> for correlation matrix significant testing,
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# traditional use
covs_test(data = attitude, vrb.nm = names(attitude))
covs_test(data = attitude, vrb.nm = names(attitude),
   ci.level = NULL) # no confidence intervals
covs_test(data = attitude, vrb.nm = names(attitude),
   rtn.dfm = TRUE) # return object as data.frame

# NOT same as simple linear regression slope
covTest &lt;- covs_test(data = attitude, vrb.nm = names(attitude),
   ci.level = NULL, rtn.dfm = TRUE)
x &lt;- covTest[with(covTest, rownames == "rating" &amp; colnames == "complaints"), ]
lm_obj &lt;- lm(rating ~ complaints, data = attitude)
y &lt;- coef(summary(lm_obj))["complaints", , drop = FALSE]
print(x); print(y)
z &lt;- x[, "cov"] / var(attitude$"complaints")
print(z) # dividing by variance of the predictor gives you the regression slope
# but the t-values and p-values are still different

# NOT same as correlation coefficient
covTest &lt;- covs_test(data = attitude, vrb.nm = names(attitude),
   ci.level = NULL, rtn.dfm = TRUE)
x &lt;- covTest[with(covTest, rownames == "rating" &amp; colnames == "complaints"), ]
cor_test &lt;- cor.test(x = attitude[[1]], y = attitude[[2]])
print(x); print(cor_test)
z &lt;- x[, "cov"] / sqrt(var(attitude$"rating") * var(attitude$"complaints"))
print(z) # dividing by sqrt of the variances gives you the correlation
# but the t-values and p-values are still different

</code></pre>

<hr>
<h2 id='cronbach'>Cronbach's Alpha of a Set of Variables/Items</h2><span id='topic+cronbach'></span>

<h3>Description</h3>

<p><code>cronbach</code> computes Cronbach's alpha for a set of variables/items as an
estimate of reliability for a score. There are three different options for
confidence intervals. Missing data can be handled by either pairwise deletion
(<code>use</code> = &quot;pairwise.complete.obs&quot;) or listwise deletion (<code>use</code> =
&quot;complete.obs&quot;). <code>cronbach</code> is a wrapper for the
<code><a href="psych.html#topic+alpha">alpha</a></code> function in the <code>psych</code> package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cronbach(
  data,
  vrb.nm,
  ci.type = "delta",
  level = 0.95,
  use = "pairwise.complete.obs",
  stats = c("average_r", "nvrb"),
  R = 200L,
  boot.ci.type = "perc"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cronbach_+3A_data">data</code></td>
<td>
<p>data.frame of data.</p>
</td></tr>
<tr><td><code id="cronbach_+3A_vrb.nm">vrb.nm</code></td>
<td>
<p>character vector of colnames of <code>data</code> specifying the
variables/items.</p>
</td></tr>
<tr><td><code id="cronbach_+3A_ci.type">ci.type</code></td>
<td>
<p>character vector of length 1 specifying the type of confidence
interval to compute. The options are 1) &quot;classic&quot; is the Feldt et al.
(1987) procedure using only the mean covariance, 2) &quot;delta&quot; is the
Duhhacheck &amp; Iacobucci (2004) procedure using the delta method of the
covariance matrix, or 3) &quot;boot&quot; is bootstrapped confidence intervals with
the method specified by <code>boot.ci.type</code>.</p>
</td></tr>
<tr><td><code id="cronbach_+3A_level">level</code></td>
<td>
<p>double vector of length 1 with a value between 0 and 1
specifying what confidence level to use.</p>
</td></tr>
<tr><td><code id="cronbach_+3A_use">use</code></td>
<td>
<p>character vector of length 1 specifying how to handle missing data
when computing the covariances. The options are 1) &quot;pairwise.complete.obs&quot;,
2) &quot;complete.obs&quot;, 3) &quot;na.or.complete&quot;, 4) &quot;all.obs&quot;, or 5) &quot;everything&quot;.
See details of <code><a href="stats.html#topic+cov">cov</a></code>.</p>
</td></tr>
<tr><td><code id="cronbach_+3A_stats">stats</code></td>
<td>
<p>character vector specifying the additional statistical
information you could like related to cronbach's alpha. Options are: 1)
&quot;std.alpha&quot; = cronbach's alpha of the standardized variables/items, 2)
&quot;G6(smc)&quot; = Guttman's Lambda 6 reliability, 3) &quot;average_r&quot; = mean
correlation between the variables/items, 4) &quot;median_r&quot; = median correlation
between the variables/items, 5) &quot;mean&quot; = mean of the the score from
averaging the variables/items together, 6) &quot;sd&quot; = standard deviation of the
scores from averaging the variables/items together, 7) &quot;nvrb&quot; = number of
variables/items. The default is &quot;average_r&quot; and &quot;nvrb&quot;.</p>
</td></tr>
<tr><td><code id="cronbach_+3A_r">R</code></td>
<td>
<p>integer vector of length 1 specifying the number of bootstrapped
resamples to do. Only used when <code>ci.type</code> = &quot;boot&quot;.</p>
</td></tr>
<tr><td><code id="cronbach_+3A_boot.ci.type">boot.ci.type</code></td>
<td>
<p>character vector of length 1 specifying the type of
bootstrapped confidence interval to compute. The options are 1) &quot;perc&quot; for
the regular percentile method, 2) &quot;bca&quot; for bias-corrected and accelerated
percentile method, 3) &quot;norm&quot; for the normal method that uses the
bootstrapped standard error to construct symmetrical confidence intervals
with the classic formula around the bias-corrected estimate, and 4) &quot;basic&quot;
for the basic method. Note, &quot;stud&quot; for the studentized method is NOT an
option. See <code><a href="boot.html#topic+boot.ci">boot.ci</a></code> as well as
<code><a href="#topic+confint2.boot">confint2.boot</a></code> for details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When <code>ci.type</code> = &quot;classic&quot; the confidence interval is based on the mean
covariance. It is the same as the confidence interval used by
<code><a href="psych.html#topic+alpha.ci">alpha.ci</a></code> (Feldt, Woodruff, &amp; Salih, 1987). When
<code>ci.type</code> = &quot;delta&quot; the confidence interval is based on the delta method
of the covariance matrix. It is based on the standard error returned by
<code><a href="psych.html#topic+alpha">alpha</a></code> (Duhachek &amp; Iacobucci, 2004).
</p>


<h3>Value</h3>

<p>double vector containing Cronbach's alpha, it's standard error, and
it's confidence interval, followed by any statistics requested via the
<code>stats</code> argument.
</p>


<h3>References</h3>

<p>Feldt, L. S., Woodruff, D. J., &amp; Salih, F. A. (1987). Statistical inference for
coefficient alpha. Applied Psychological Measurement (11) 93-103.
</p>
<p>Duhachek, A. and Iacobucci, D. (2004). Alpha's standard error (ase): An accurate
and precise confidence interval estimate. Journal of Applied Psychology, 89(5):792-808.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cronbachs">cronbachs</a></code>
<code><a href="#topic+composite">composite</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
tmp_nm &lt;- c("A2","A3","A4","A5")
psych::alpha(psych::bfi[tmp_nm])[["total"]]
a &lt;- suppressMessages(psych::alpha(attitude))[["total"]]["raw_alpha"]
a.ci &lt;- psych::alpha.ci(a, n.obs = 30,
   n.var = 7, digits = 7) # n.var is optional and only needed to find r.bar
cronbach(data = psych::bfi, vrb.nm = c("A2","A3","A4","A5"), ci.type = "classic")
cronbach(data = psych::bfi, vrb.nm = c("A2","A3","A4","A5"), ci.type = "delta")
cronbach(data = psych::bfi, vrb.nm = c("A2","A3","A4","A5"), ci.type = "boot")
cronbach(data = psych::bfi, vrb.nm = c("A2","A3","A4","A5"), stats = NULL)

## Not run: 
cronbach(data = psych::bfi, vrb.nm = c("A2","A3","A4","A5"), ci.type = "boot",
   boot.ci.type = "bca") # will automatically convert to "perc" when "bca" fails

## End(Not run)

</code></pre>

<hr>
<h2 id='cronbachs'>Cronbach's Alpha for Multiple Sets of Variables/Items</h2><span id='topic+cronbachs'></span>

<h3>Description</h3>

<p><code>cronbachs</code> computes Cronbach's alpha for multiple sets of
variables/items as an estimate of reliability for multiple scores. There are
three different options for confidence intervals. Missing data can be handled
by either pairwise deletion (<code>use</code> = &quot;pairwise.complete.obs&quot;) or
listwise deletion (<code>use</code> = &quot;complete.obs&quot;). <code>cronbachs</code> is a
wrapper for the <code><a href="psych.html#topic+alpha">alpha</a></code> function in the <code>psych</code>
package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cronbachs(
  data,
  vrb.nm.list,
  ci.type = "delta",
  level = 0.95,
  use = "pairwise.complete.obs",
  stats = c("average_r", "nvrb"),
  R = 200L,
  boot.ci.type = "perc"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cronbachs_+3A_data">data</code></td>
<td>
<p>data.frame of data.</p>
</td></tr>
<tr><td><code id="cronbachs_+3A_vrb.nm.list">vrb.nm.list</code></td>
<td>
<p>list of character vectors specifying the sets of
variables/items. Each element of <code>vrb.nm.list</code> provides the colnames
of <code>data</code> for that set of variables/items.</p>
</td></tr>
<tr><td><code id="cronbachs_+3A_ci.type">ci.type</code></td>
<td>
<p>character vector of length 1 specifying the type of confidence
interval to compute. The options are 1) &quot;classic&quot; = the Feldt et al. (1987)
procedure using only the mean covariance, 2) &quot;delta&quot; = the Duhhacheck &amp;
Iacobucci (2004) procedure using the delta method of the covariance matrix,
or 3) &quot;boot&quot; = bootstrapped confidence intervals with the method specified
by <code>boot.ci.type</code>.</p>
</td></tr>
<tr><td><code id="cronbachs_+3A_level">level</code></td>
<td>
<p>double vector of length 1 with a value between 0 and 1
specifying what confidence level to use.</p>
</td></tr>
<tr><td><code id="cronbachs_+3A_use">use</code></td>
<td>
<p>character vector of length 1 specifying how to handle missing data
when computing the covariances. The options are 1) &quot;pairwise.complete.obs&quot;,
2) &quot;complete.obs&quot;, 3) &quot;na.or.complete&quot;, 4) &quot;all.obs&quot;, or 5) &quot;everything&quot;.
See details of <code><a href="stats.html#topic+cov">cov</a></code>.</p>
</td></tr>
<tr><td><code id="cronbachs_+3A_stats">stats</code></td>
<td>
<p>character vector specifying the additional statistical
information you could like related to cronbach's alpha. Options are: 1)
&quot;std.alpha&quot; = cronbach's alpha of the standardized variables/items, 2)
&quot;G6(smc)&quot; = Guttman's Lambda 6 reliability, 3) &quot;average_r&quot; = mean
correlation between the variables/items, 4) &quot;median_r&quot; = median correlation
between the variables/items, 5) &quot;mean&quot; = mean of the the scores from
averaging the variables/items together, 6) &quot;sd&quot; = standard deviation of the
scores from averaging the variables/items together, 7) &quot;nvrb&quot; = number of
variables/items. The default is &quot;average_r&quot; and &quot;nvrb&quot;.</p>
</td></tr>
<tr><td><code id="cronbachs_+3A_r">R</code></td>
<td>
<p>integer vector of length 1 specifying the number of bootstrapped
resamples to do. Only used when <code>ci.type</code> = &quot;boot&quot;.</p>
</td></tr>
<tr><td><code id="cronbachs_+3A_boot.ci.type">boot.ci.type</code></td>
<td>
<p>character vector of length 1 specifying the type of
bootstrapped confidence interval to compute. The options are 1) &quot;perc&quot; for
the regular percentile method, 2) &quot;bca&quot; for bias-corrected and accelerated
percentile method, 3) &quot;norm&quot; for the normal method that uses the
bootstrapped standard error to construct symmetrical confidence intervals
with the classic formula around the bias-corrected estimate, and 4) &quot;basic&quot;
for the basic method. Note, &quot;stud&quot; for the studentized method is NOT an
option. See <code><a href="boot.html#topic+boot.ci">boot.ci</a></code> as well as
<code><a href="#topic+confint2.boot">confint2.boot</a></code> for details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When <code>ci.type</code> = &quot;classic&quot; the confidence interval is based on the mean
covariance. It is the same as the confidence interval used by
<code><a href="psych.html#topic+alpha.ci">alpha.ci</a></code> (Feldt, Woodruff, &amp; Salih, 1987). When
<code>ci.type</code> = &quot;delta&quot; the confidence interval is based on the delta method
of the covariance matrix. It is based on the standard error returned by
<code><a href="psych.html#topic+alpha">alpha</a></code> (Duhachek &amp; Iacobucci, 2004).
</p>


<h3>Value</h3>

<p>data.frame containing the following columns:
</p>

<dl>
<dt>est</dt><dd><p>Cronbach's alpha itself</p>
</dd>
<dt>se</dt><dd><p>standard error for Cronbach's alpha</p>
</dd>
<dt>lwr</dt><dd><p>lower bound of the confidence interval of Cronbach's alpha</p>
</dd>
<dt>upr</dt><dd><p>upper bound for the confidence interval of Cronbach's alpha</p>
</dd></dl>
<p>,
</p>
<dl>
<dt>???</dt><dd><p>any statistics requested via the <code>stats</code> argument</p>
</dd>
</dl>



<h3>References</h3>

<p>Feldt, L. S., Woodruff, D. J., &amp; Salih, F. A. (1987). Statistical inference for
coefficient alpha. Applied Psychological Measurement (11) 93-103.
</p>
<p>Duhachek, A. and Iacobucci, D. (2004). Alpha's standard error (ase): An accurate
and precise confidence interval estimate. Journal of Applied Psychology, 89(5):792-808.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cronbach">cronbach</a></code>
<code><a href="#topic+composites">composites</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
dat0 &lt;- psych::bfi
dat1 &lt;- str2str::pick(x = dat0, val = c("A1","C4","C5","E1","E2","O2","O5",
   "gender","education","age"), not = TRUE, nm = TRUE)
vrb_nm_list &lt;- lapply(X = str2str::sn(c("E","N","C","A","O")), FUN = function(nm) {
   str2str::pick(x = names(dat1), val = nm, pat = TRUE)})
cronbachs(data = dat1, vrb.nm.list = vrb_nm_list, ci.type = "classic")
cronbachs(data = dat1, vrb.nm.list = vrb_nm_list, ci.type = "delta")
cronbachs(data = dat1, vrb.nm.list = vrb_nm_list, ci.type = "boot")
suppressMessages(cronbachs(data = attitude, vrb.nm.list =
   list(names(attitude)))) # also works with only one set of variables/items

</code></pre>

<hr>
<h2 id='decompose'>Decompose a Numeric Vector by Group</h2><span id='topic+decompose'></span>

<h3>Description</h3>

<p><code>decompose</code> decomposes a numeric vector into within-group and
between-group components via within-group centering and group-mean
aggregation. There is an option to create a grand-mean centered version of
the between-person component as well as lead/lag versions of the original
vector and the within-group component.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>decompose(x, grp, grand = TRUE, n.shift = NULL, undefined = NA)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="decompose_+3A_x">x</code></td>
<td>
<p>numeric vector.</p>
</td></tr>
<tr><td><code id="decompose_+3A_grp">grp</code></td>
<td>
<p>list of atomic vector(s) and/or factor(s) (e.g., data.frame),
which each have same length as <code>x</code>. It can also be an atomic vector or
factor, which will then be made the first element of a list internally.</p>
</td></tr>
<tr><td><code id="decompose_+3A_grand">grand</code></td>
<td>
<p>logical vector of length 1 specifying whether a grand-mean
centered version of the the between-group component should be computed.</p>
</td></tr>
<tr><td><code id="decompose_+3A_n.shift">n.shift</code></td>
<td>
<p>integer vector specifying the direction and magnitude of the
shifts. For example a one-lead is +1 and a two-lag is -2. See <code>shift</code>
details.</p>
</td></tr>
<tr><td><code id="decompose_+3A_undefined">undefined</code></td>
<td>
<p>atomic vector with length 1 (probably makes sense to be the
same typeof as <code>x</code>). Specifies what to insert for undefined values
after the shifting takes place. See <code>shift</code> details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data.frame with nrow = <code>length(x)</code> and <code>row.names =
  names(x)</code>. The first two columns correspond to the within-group component
(i.e., &quot;wth&quot;) and the between-group component (i.e., &quot;btw&quot;). If grand =
TRUE, then the third column corresponds to the grand-mean centered
between-group component (i.e., &quot;btw_c&quot;). If shift != NULL, then the last
columns are the shifts indicated by n.shift, where the shifts of <code>x</code>
are first (i.e., &quot;tot&quot;) and then the shifts of the within-group component
are second (i.e., &quot;wth&quot;). The naming of the shifted columns is based on the
default behavior of <code>Shift_by</code>. See the details of <code>Shift_by</code>. If
you don't like the default naming, then call <code>Decompose</code> instead and
use the different suffix arguments.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+decomposes">decomposes</a></code>
<code><a href="#topic+center_by">center_by</a></code>
<code><a href="#topic+agg">agg</a></code>
<code><a href="#topic+shift_by">shift_by</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# single grouping variable
chick_data &lt;- as.data.frame(ChickWeight) # because the "groupedData" class
   # calls `[.groupedData`, which is different than `[.data.frame`
decompose(x = ChickWeight[["weight"]], grp = ChickWeight[["Chick"]])
decompose(x = ChickWeight[["weight"]], grp = ChickWeight[["Chick"]],
   grand = FALSE) # no grand-mean centering
decompose(x = setNames(obj = ChickWeight[["weight"]],
   nm = paste0(row.names(ChickWeight),"_row")), grp = ChickWeight[["Chick"]]) # with names

# multiple grouping variables
tmp_nm &lt;- c("Type","Treatment") # b/c Roxygen2 doesn't like c() in a []
decompose(x = as.data.frame(CO2)[["uptake"]], grp = as.data.frame(CO2)[tmp_nm])
decompose(x = as.data.frame(CO2)[["uptake"]], grp = as.data.frame(CO2)[tmp_nm],
   n.shift = 1)
decompose(x = as.data.frame(CO2)[["uptake"]], grp = as.data.frame(CO2)[tmp_nm],
   n.shift = c(+2, +1, -1, -2))
</code></pre>

<hr>
<h2 id='decomposes'>Decompose Numeric Data by Group</h2><span id='topic+decomposes'></span>

<h3>Description</h3>

<p><code>decomposes</code> decomposes numeric data by group into within-group and
between- group components via within-group centering and group-mean
aggregation. There is an option to create a grand-mean centered version of
the between-group components.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>decomposes(
  data,
  vrb.nm,
  grp.nm,
  grand = TRUE,
  n.shift = NULL,
  undefined = NA,
  suffix.wth = "_w",
  suffix.btw = "_b",
  suffix.grand = "c",
  suffix.lead = "_dw",
  suffix.lag = "_gw"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="decomposes_+3A_data">data</code></td>
<td>
<p>data.frame of data.</p>
</td></tr>
<tr><td><code id="decomposes_+3A_vrb.nm">vrb.nm</code></td>
<td>
<p>character vector of colnames from <code>data</code> specifying the
variables.</p>
</td></tr>
<tr><td><code id="decomposes_+3A_grp.nm">grp.nm</code></td>
<td>
<p>character vector of colnames from <code>data</code> specifying the
groups.</p>
</td></tr>
<tr><td><code id="decomposes_+3A_grand">grand</code></td>
<td>
<p>logical vector of length 1 specifying whether grand-mean
centered versions of the the between-group components should be computed.</p>
</td></tr>
<tr><td><code id="decomposes_+3A_n.shift">n.shift</code></td>
<td>
<p>integer vector specifying the direction and magnitude of the
shifts. For example a one-lead is +1 and a two-lag is -2. See
<code>Shift_by</code> details.</p>
</td></tr>
<tr><td><code id="decomposes_+3A_undefined">undefined</code></td>
<td>
<p>atomic vector of length 1 (probably makes sense to be the
same typeof as the vectors in <code>data[vrb.nm]</code>). Specifies what to
insert for undefined values after the shifting takes place. See details of
<code>Shift_by</code>.</p>
</td></tr>
<tr><td><code id="decomposes_+3A_suffix.wth">suffix.wth</code></td>
<td>
<p>character vector with a single element specifying the
string to append to the end of the within-group component colnames of the
return object.</p>
</td></tr>
<tr><td><code id="decomposes_+3A_suffix.btw">suffix.btw</code></td>
<td>
<p>character vector with a single element specifying the
string to append to the end of the between-group component colnames of the
return object.</p>
</td></tr>
<tr><td><code id="decomposes_+3A_suffix.grand">suffix.grand</code></td>
<td>
<p>character vector with a single element specifying the
string to append to the end of the grand-mean centered version of the
between-group component colnames of the return object. Note, this is a
string that is appended after <code>suffix.btw</code> has already been appended.</p>
</td></tr>
<tr><td><code id="decomposes_+3A_suffix.lead">suffix.lead</code></td>
<td>
<p>character vector with a single element specifying the
string to append to the end of the positive shift colnames of the return
object. Note, <code>decomposes</code> will add <code>abs(n.shift)</code> to the end of
<code>suffix.lead</code>.</p>
</td></tr>
<tr><td><code id="decomposes_+3A_suffix.lag">suffix.lag</code></td>
<td>
<p>character vector with a single element specifying the
string to append to the end of the negative shift colnames of the return
object. Note, <code>decomposes</code> will add <code>abs(n.shift)</code> to the end of
<code>suffix.lag</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data.frame with nrow = <code>nrow(data</code> and rownames =
<code>row.names(data)</code>. The first set of columns correspond to the
within-group components, followed by the between-group components. If grand
= TRUE, then the next set of columns correspond to the grand-mean centered
between-group components. If shift != NULL, then the last columns are the
shifts by group indicated by n.shift, where the shifts of
<code>data[vrb.nm]</code> are first and then the shifts of the within-group
components are second.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+decompose">decompose</a></code>
<code><a href="#topic+centers_by">centers_by</a></code>
<code><a href="#topic+aggs">aggs</a></code>
<code><a href="#topic+shifts_by">shifts_by</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>ChickWeight2 &lt;- as.data.frame(ChickWeight)
row.names(ChickWeight2) &lt;- as.numeric(row.names(ChickWeight)) / 1000
decomposes(data = ChickWeight2, vrb.nm = c("weight","Time"), grp.nm = "Chick")
decomposes(data = ChickWeight2, vrb.nm = c("weight","Time"), grp.nm = "Chick",
   suffix.wth = ".wth", suffix.btw = ".btw", suffix.grand = ".grand")
decomposes(data = as.data.frame(CO2), vrb.nm = c("conc","uptake"),
   grp.nm = c("Type","Treatment")) # multiple grouping columns
decomposes(data = as.data.frame(CO2), vrb.nm = c("conc","uptake"),
   grp.nm = c("Type","Treatment"), n.shift = 1) # with lead
decomposes(data = as.data.frame(CO2), vrb.nm = c("conc","uptake"), grp.nm = c("Type","Treatment"),
   n.shift = c(+2, +1, -1, -2)) # with multiple lead/lags
</code></pre>

<hr>
<h2 id='deff'>Design Effect from Multilevel Numeric Vector</h2><span id='topic+deff'></span>

<h3>Description</h3>

<p><code>deff</code> computes the design effect for a multilevel numeric vector.
Design effects summarize how much larger sampling variances (i.e., squared
standard errors) are due to the multilevel structure of the data. By taking
the square root, the value summarizes how much larger standard errors are due
to the multilevel structure of the data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>deff(x, grp, how = "lme", REML = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="deff_+3A_x">x</code></td>
<td>
<p>numeric vector.</p>
</td></tr>
<tr><td><code id="deff_+3A_grp">grp</code></td>
<td>
<p>atomic vector the same length as <code>x</code> providing the grouping
variable.</p>
</td></tr>
<tr><td><code id="deff_+3A_how">how</code></td>
<td>
<p>character vector of length 1 specifying how the ICC(1,1) should be
calculated. There are four options: 1) &quot;lme&quot; uses a linear mixed effects
model with the function <code><a href="nlme.html#topic+lme">lme</a></code> from the package
<code>nlme</code>, 2) &quot;lmer&quot; uses a linear mixed effects modeling with the
function <code><a href="lme4.html#topic+lmer">lmer</a></code> from the package <code>lme4</code>, 3) &quot;aov&quot;
uses a one-way analysis of variance with the function
<code><a href="stats.html#topic+aov">aov</a></code>, and 4) &quot;raw&quot; uses the observed variances, which
provides a biased estimate of the ICC(1,1) and is not recommended (It is
only included for teaching purposes).</p>
</td></tr>
<tr><td><code id="deff_+3A_reml">REML</code></td>
<td>
<p>logical vector of length 1 specifying whether restricted maximum
likelihood estimation (TRUE) should be used rather than traditional maximum
likelihood estimation (FALSE). Only used for linear mixed effects models if
how = &quot;lme&quot; or how = &quot;lmer&quot;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Design effects are a function of both the intraclass correlation (ICC) and
the average group size. Design effects can be large due to large ICCs and
small group sizes or small ICCs and large group sizes. For example, with an
ICC = .01 and average group size of 100, the design effect would be 2.0,
whose square root is 1.41. For more information, see myths 1 and 2 in
Huang (2018).
</p>


<h3>Value</h3>

<p>double vector of lenght 1 providing the design effect.
</p>


<h3>References</h3>

<p>Huang, F. L. (2018). Multilevel modeling myths School Psychology Quarterly,
33(3), 492-499.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+icc_11">icc_11</a></code>
<code><a href="#topic+deffs">deffs</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
icc_11(x = airquality$"Ozone", grp = airquality$"Month")
length_by(x = airquality$"Ozone", grp = airquality$"Month", na.rm = TRUE)
deff(x = airquality$"Ozone", grp = airquality$"Month")
sqrt(deff(x = airquality$"Ozone", grp = airquality$"Month")) # how much SE inflated

</code></pre>

<hr>
<h2 id='deffs'>Design Effects from Multilevel Numeric Data</h2><span id='topic+deffs'></span>

<h3>Description</h3>

<p><code>deffs</code> computes the design effects for multilevel numeric data. Design
effects summarize how much larger sampling variances (i.e., squared standard
errors) are due to the multilevel structure of the data. By taking the square
root, the value summarizes how much larger standard errors are due to the
multilevel structure of the data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>deffs(data, vrb.nm, grp.nm, how = "lme", REML = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="deffs_+3A_data">data</code></td>
<td>
<p>data.frame of data.</p>
</td></tr>
<tr><td><code id="deffs_+3A_vrb.nm">vrb.nm</code></td>
<td>
<p>character vector of colnames from <code>data</code> specifying the
variable columns.</p>
</td></tr>
<tr><td><code id="deffs_+3A_grp.nm">grp.nm</code></td>
<td>
<p>character vector of length 1 of a colname from <code>data</code>
specifying the grouping column.</p>
</td></tr>
<tr><td><code id="deffs_+3A_how">how</code></td>
<td>
<p>character vector of length 1 specifying how the ICC(1,1) should be
calculated. There are four options: 1) &quot;lme&quot; uses a linear mixed effects
model with the function <code><a href="nlme.html#topic+lme">lme</a></code> from the package
<code>nlme</code>, 2) &quot;lmer&quot; uses a linear mixed effects modeling with the
function <code><a href="lme4.html#topic+lmer">lmer</a></code> from the package <code>lme4</code>, 3) &quot;aov&quot;
uses a one-way analysis of variance with the function
<code><a href="stats.html#topic+aov">aov</a></code>, and 4) &quot;raw&quot; uses the observed variances, which
provides a biased estimate of the ICC(1,1) and is not recommended (It is
only included for teaching purposes).</p>
</td></tr>
<tr><td><code id="deffs_+3A_reml">REML</code></td>
<td>
<p>logical vector of length 1 specifying whether restricted maximum
likelihood estimation (TRUE) should be used rather than traditional maximum
likelihood estimation (FALSE). Only used for linear mixed effects models if
how = &quot;lme&quot; or how = &quot;lmer&quot;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Design effects are a function of both the intraclass correlation (ICC) and
the average group size. Design effects can be large due to large ICCs and
small group sizes or small ICCs and large group sizes. For example, with an
ICC = .01 and average group size of 100, the design effect would be 2.0,
whose square root is 1.41. For more information, see myths 1 and 2 in
Huang (2018).
</p>


<h3>Value</h3>

<p>double vector providing the design effects with names =
<code>vrb.nm</code>.
</p>


<h3>References</h3>

<p>Huang, F. L. (2018). Multilevel modeling myths School Psychology Quarterly,
33(3), 492-499.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+iccs_11">iccs_11</a></code>
<code><a href="#topic+deff">deff</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
iccs_11(data = airquality, vrb.nm = c("Ozone","Solar.R"), grp.nm = "Month")
lengths_by(data = airquality, vrb.nm = c("Ozone","Solar.R"), grp.nm = "Month", na.rm = TRUE)
deffs(data = airquality, vrb.nm = c("Ozone","Solar.R"), grp.nm = "Month")

</code></pre>

<hr>
<h2 id='describe_ml'>Multilevel Descriptive Statistics</h2><span id='topic+describe_ml'></span>

<h3>Description</h3>

<p><code>describe_ml</code> decomposes descriptive statistics from multilevel data
into within-group and between-group descriptives. The data is first separated
out into within-group components via <code>centers_by</code> and between-group
components via <code>aggs</code>. Then the <code>psych</code> function
<code><a href="psych.html#topic+describe">describe</a></code> is applied to both.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>describe_ml(
  data,
  vrb.nm,
  grp.nm,
  na.rm = TRUE,
  interp = FALSE,
  skew = TRUE,
  ranges = TRUE,
  trim = 0.1,
  type = 3,
  quant = NULL,
  IQR = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="describe_ml_+3A_data">data</code></td>
<td>
<p>data.frame of data.</p>
</td></tr>
<tr><td><code id="describe_ml_+3A_vrb.nm">vrb.nm</code></td>
<td>
<p>character vector of colnames from <code>data</code> specifying the
variable columns.</p>
</td></tr>
<tr><td><code id="describe_ml_+3A_grp.nm">grp.nm</code></td>
<td>
<p>character vector of length 1 of a colname from <code>data</code>
specifying the grouping column.</p>
</td></tr>
<tr><td><code id="describe_ml_+3A_na.rm">na.rm</code></td>
<td>
<p>logical vector of length 1 specifying whether missing values
should be removed before calculating the descriptive statistics. See
<code>psych::describe</code>.</p>
</td></tr>
<tr><td><code id="describe_ml_+3A_interp">interp</code></td>
<td>
<p>logical vector of length 1 specifying whether the median should
be standard (FALSE) or interpolated (TRUE).</p>
</td></tr>
<tr><td><code id="describe_ml_+3A_skew">skew</code></td>
<td>
<p>logical vector of length 1 specifying whether skewness and
kurtosis should be calculated (TRUE) or not (FALSE).</p>
</td></tr>
<tr><td><code id="describe_ml_+3A_ranges">ranges</code></td>
<td>
<p>logical vector of length 1 specifying whether the minimum,
maximum, and range (i.e., maximum - minimum) should be calculated (TRUE) or
not (FALSE). Note, if <code>ranges</code> = FALSE, the trimmed mean and median
absolute deviation is also not computed as per the <code>psych::describe</code>
function behavior.</p>
</td></tr>
<tr><td><code id="describe_ml_+3A_trim">trim</code></td>
<td>
<p>numeric vector of length 1 specifying the top and bottom
quantiles of data that are to be excluded when calculating the trimmed
mean. For example, the default value of 0.1 means that only data within the
10th - 90th quantiles are used for calculating the trimmed mean.</p>
</td></tr>
<tr><td><code id="describe_ml_+3A_type">type</code></td>
<td>
<p>numeric vector of length 1 specifying the type of skewness and
kurtosis coefficients to compute. See the details of
<code>psych::describe</code>. The options are 1, 2, or 3.</p>
</td></tr>
<tr><td><code id="describe_ml_+3A_quant">quant</code></td>
<td>
<p>numeric vector specifying the quantiles to compute. Foe example,
the default value of c(0.25, 0.75) computes the 25th and 75th quantiles of
the group number of cases. If <code>quant</code> = NULL, then no quantiles are
returned.</p>
</td></tr>
<tr><td><code id="describe_ml_+3A_iqr">IQR</code></td>
<td>
<p>logical vector of length 1 specifying whether to compute the
Interquartile Range (TRUE) or not (FALSE), which is simply the 75th quantil
- 25th quantile.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list of two elements each containing a data.frame of descriptive
statistics, the first for the within-person components (&quot;within&quot;) and the
second for the between-person components (&quot;between&quot;).
</p>


<h3>See Also</h3>

<p><code><a href="psych.html#topic+describe">describe</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
tmp_nm &lt;- c("outcome","case","session","trt_time")
dat &lt;- as.data.frame(lmeInfo::Bryant2016)[tmp_nm]
stats_by &lt;- psych::statsBy(dat, group = "case") # requires you to include "case" column in dat
describe_ml(data = dat, vrb.nm = c("outcome","session","trt_time"), grp.nm = "case")

</code></pre>

<hr>
<h2 id='dum2nom'>Dummy Variables to a Nominal Variable</h2><span id='topic+dum2nom'></span>

<h3>Description</h3>

<p><code>dum2nom</code> converts dummy variables to a nominal variable. The
information from the dummy columns in a data.frame are combined into a
character vector (or factor if <code>rtn.fct</code> = TRUE) representing a nominal
variable. The unique values of the nominal variable will be the dummy
colnames (i.e., <code>dum.nm</code>). Note, *all* the dummy variables associated
with a nominal variable are required for this function to work properly. In
regression-like models, data analysts will exclude one dummy variable for the
category that is the reference group. If d = number of categories in the
nominal variable, then that leads to d - 1 dummy variables in the model.
<code>dum2nom</code> requires all d dummy variables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dum2nom(data, dum.nm, yes = 1L, rtn.fct = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dum2nom_+3A_data">data</code></td>
<td>
<p>data.frame of data.</p>
</td></tr>
<tr><td><code id="dum2nom_+3A_dum.nm">dum.nm</code></td>
<td>
<p>character vector of colnames from <code>data</code> specifying the
dummy variables.</p>
</td></tr>
<tr><td><code id="dum2nom_+3A_yes">yes</code></td>
<td>
<p>atomic vector of length 1 specifying the unique value of the
category in each dummy column. This must be the same value for all the
dummy variables.</p>
</td></tr>
<tr><td><code id="dum2nom_+3A_rtn.fct">rtn.fct</code></td>
<td>
<p>logical vector of length 1 specifying whether the return
object should be a factor (TRUE) or a character vector (FALSE).</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>dum2nom</code> tests to ensure that <code>data[dum.nm]</code> are indeed a set of
dummy columns. First, the dummy columns are expected to have the same mode
such that there is one <code>yes</code> unique value across the dummy columns.
Second, each row in <code>data[dum.nm]</code> is expected to have either 0 or 1
instance of <code>yes</code>. If there is more than one instance of <code>yes</code> in a
row, then an error is returned. If there is 0 instances of <code>yes</code> in a
row (e.g., all missing values), NA is returned for that row. Note, any value
other than <code>yes</code> will be treated as a no.
</p>


<h3>Value</h3>

<p>character vector (or factor if <code>rtn.fct</code> = TRUE) containing the
unique values of <code>dum.nm</code> - one for each dummy variable.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+nom2dum">nom2dum</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dum &lt;- data.frame(
   "Quebec_nonchilled" = ifelse(CO2$"Type" == "Quebec" &amp; CO2$"Treatment" == "nonchilled",
      yes = 1L, no = 0L),
   "Quebec_chilled" = ifelse(CO2$"Type" == "Quebec" &amp; CO2$"Treatment" == "chilled",
      yes = 1L, no = 0L),
   "Mississippi_nonchilled" = ifelse(CO2$"Type" == "Mississippi" &amp; CO2$"Treatment" == "nonchilled",
      yes = 1L, no = 0L),
   "Mississippi_chilled" = ifelse(CO2$"Type" == "Mississippi" &amp; CO2$"Treatment" == "chilled",
      yes = 1L, no = 0L)
)
dum2nom(data = dum, dum.nm = names(dum)) # default
dum2nom(data = dum, dum.nm = names(dum), rtn.fct = TRUE) # return as a factor
## Not run: 
dum2nom(data = npk, dum.nm = c("N","P","K")) # error due to overlapping dummy columns
dum2nom(data = mtcars, dum.nm = c("vs","am"))# error due to overlapping dummy columns

## End(Not run)
</code></pre>

<hr>
<h2 id='freq'>Univariate Frequency Table</h2><span id='topic+freq'></span>

<h3>Description</h3>

<p><code>freq</code> creates univariate frequency tables similar to <code>table</code>. It
differs from <code>table</code> by allowing for custom sorting by something other
than the alphanumerics of the unique values as well as returning an atomic
vector rather than a 1D-array.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>freq(
  x,
  exclude = if (useNA == "no") c(NA, NaN),
  useNA = "always",
  prop = FALSE,
  sort = "frequency",
  decreasing = TRUE,
  na.last = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="freq_+3A_x">x</code></td>
<td>
<p>atomic vector or list vector. If not a vector, it will be coerced to
a vector via <code><a href="base.html#topic+as.vector">as.vector</a></code>.</p>
</td></tr>
<tr><td><code id="freq_+3A_exclude">exclude</code></td>
<td>
<p>unique values of <code>x</code> to exclude from the returned table.
If NULL, then missing values are always included in the returned table. See
<code><a href="base.html#topic+table">table</a></code> for documentation on the same argument.</p>
</td></tr>
<tr><td><code id="freq_+3A_usena">useNA</code></td>
<td>
<p>character vector of length 1 specifying how to handle missing
values (i.e., whether to include NA as an element in the returned table).
There are three options: 1) &quot;no&quot; = don't include missing values in the
table, 2) &quot;ifany&quot; = include missing values if there are any, 3) &quot;always&quot; =
include missing values in the table, regardless of whether there are any or
not. See <code><a href="base.html#topic+table">table</a></code> for documentation on the same argument.</p>
</td></tr>
<tr><td><code id="freq_+3A_prop">prop</code></td>
<td>
<p>logical vector of length 1 specifying whether the returned table
should include counts (FALSE) or proportions (TRUE). If NAs are excluded
(e.g., useNA = &quot;no&quot; or exclude = c(NA, NaN)), then the proportions will be
based on the number of observed elements.</p>
</td></tr>
<tr><td><code id="freq_+3A_sort">sort</code></td>
<td>
<p>character vector of length 1 specifying how the returned table
will be sorted. There are three options: 1) &quot;frequency&quot; = the frequency of
the unique values in <code>x</code>, 2) &quot;position&quot; = the position when each
unique value first appears in <code>x</code>, 3) &quot;alphanum&quot; = alphanumeric
ordering of the unique values in <code>x</code> (the sorting used by
<code>table</code>). When &quot;frequency&quot; is specified and there are ties, then the
ties are sorted alphanumerically.</p>
</td></tr>
<tr><td><code id="freq_+3A_decreasing">decreasing</code></td>
<td>
<p>logical vector of length 1 specifying whether the table
should be sorted in decreasing (TRUE) or increasing (FALSE) order.</p>
</td></tr>
<tr><td><code id="freq_+3A_na.last">na.last</code></td>
<td>
<p>logical vector of length 1 specifying whether the table should
have NAs last or in whatever position they end up at. This argument is only
relevant if NAs exist in <code>x</code> and are included in the table (e.g.,
useNA = &quot;always&quot; or exclude = NULL).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The name for the table element giving the frequency of missing values is
&quot;(NA)&quot;. This is different from <code>table</code> where the name is
<code>NA_character_</code>. This change allows for the sorting of tables that
include missing values, as subsetting in R is not possible with
<code>NA_character_</code> names. In future versions of the package, this might
change as it should be possible to avoid this issue by subetting with a
logical vector or integer indices instead of names. However, it is convenient
to be able to subset the return object fully by names.
</p>


<h3>Value</h3>

<p>numeric vector of frequencies as either counts (if <code>prop</code> =
FALSE) or proportions (if <code>prop</code> = TRUE) with the unique values of
<code>x</code> as names (missing values have name = &quot;(NA)&quot;). Note, this is
different from <code>table</code>, which returns a 1D-array and has class
&quot;table&quot;.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+freqs">freqs</a></code>
<code><a href="#topic+freq_by">freq_by</a></code>
<code><a href="#topic+freqs_by">freqs_by</a></code>
<code><a href="base.html#topic+table">table</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>freq(c(mtcars$"carb", NA, NA, mtcars$"gear"), prop = FALSE,
   sort = "frequency", decreasing = TRUE, na.last = TRUE)
freq(c(mtcars$"carb", NA, NA, mtcars$"gear"), prop = FALSE,
   sort = "frequency", decreasing = TRUE, na.last = FALSE)
freq(c(mtcars$"carb", NA, NA, mtcars$"gear"), prop = TRUE,
   sort = "frequency", decreasing = FALSE, na.last = TRUE)
freq(c(mtcars$"carb", NA, NA, mtcars$"gear"), prop = TRUE,
   sort = "frequency", decreasing = FALSE, na.last = FALSE)
freq(c(mtcars$"carb", NA, NA, mtcars$"gear"), prop = FALSE,
   sort = "position", decreasing = TRUE, na.last = TRUE)
freq(c(mtcars$"carb", NA, NA, mtcars$"gear"), prop = FALSE,
   sort = "position", decreasing = TRUE, na.last = FALSE)
freq(c(mtcars$"carb", NA, NA, mtcars$"gear"), prop = TRUE,
   sort = "position", decreasing = FALSE, na.last = TRUE)
freq(c(mtcars$"carb", NA, NA, mtcars$"gear"), prop = TRUE,
   sort = "position", decreasing = FALSE, na.last = FALSE)
freq(c(mtcars$"carb", NA, NA, mtcars$"gear"), prop = FALSE,
   sort = "alphanum", decreasing = TRUE, na.last = TRUE)
freq(c(mtcars$"carb", NA, NA, mtcars$"gear"), prop = FALSE,
   sort = "alphanum", decreasing = TRUE, na.last = FALSE)
freq(c(mtcars$"carb", NA, NA, mtcars$"gear"), prop = TRUE,
   sort = "alphanum", decreasing = FALSE, na.last = TRUE)
freq(c(mtcars$"carb", NA, NA, mtcars$"gear"), prop = TRUE,
   sort = "alphanum", decreasing = FALSE, na.last = FALSE)
</code></pre>

<hr>
<h2 id='freq_by'>Univariate Frequency Table By Group</h2><span id='topic+freq_by'></span>

<h3>Description</h3>

<p><code>tables_by</code> creates a frequency table for a set of variables in a
data.frame by group. Depending on <code>total</code>, frequencies for all the
variables together can be returned by group. The function probably makes the
most sense for sets of variables with similar unique values (e.g., items from
a questionnaire with similar response options).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>freq_by(
  x,
  grp,
  exclude = if (useNA == "no") c(NA, NaN),
  useNA = "always",
  prop = FALSE,
  sort = "frequency",
  decreasing = TRUE,
  na.last = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="freq_by_+3A_x">x</code></td>
<td>
<p>atomic vector.</p>
</td></tr>
<tr><td><code id="freq_by_+3A_grp">grp</code></td>
<td>
<p>atomic vector or list of atomic vectors (e.g., data.frame)
specifying the groups. The atomic vector(s) must be the length of <code>x</code>
or else an error is returned.</p>
</td></tr>
<tr><td><code id="freq_by_+3A_exclude">exclude</code></td>
<td>
<p>unique values of <code>x</code> to exclude from the returned table.
If NULL, then missing values are always included in the returned table. See
<code><a href="base.html#topic+table">table</a></code> for documentation on the same argument.</p>
</td></tr>
<tr><td><code id="freq_by_+3A_usena">useNA</code></td>
<td>
<p>character vector of length 1 specifying how to handle missing
values (i.e., whether to include NA as an element in the returned table).
There are three options: 1) &quot;no&quot; = don't include missing values in the
table, 2) &quot;ifany&quot; = include missing values if there are any, 3) &quot;always&quot; =
include missing values in the table, regardless of whether there are any or
not. See <code><a href="base.html#topic+table">table</a></code> for documentation on the same argument.</p>
</td></tr>
<tr><td><code id="freq_by_+3A_prop">prop</code></td>
<td>
<p>logical vector of length 1 specifying whether the returned table
should include counts (FALSE) or proportions (TRUE). If NAs are excluded
(e.g., useNA = &quot;no&quot; or exclude = c(NA, NaN)), then the proportions will be
based on the number of observed elements.</p>
</td></tr>
<tr><td><code id="freq_by_+3A_sort">sort</code></td>
<td>
<p>character vector of length 1 specifying how the returned table
will be sorted. There are three options: 1) &quot;frequency&quot; = the frequency of
the unique values in <code>x</code>, 2) &quot;position&quot; = the position when each
unique value first appears in <code>x</code>, 3) &quot;alphanum&quot; = alphanumeric
ordering of the unique values in <code>x</code> (the sorting used by
<code>table</code>). When &quot;frequency&quot; is specified and there are ties, then the
ties are sorted alphanumerically.</p>
</td></tr>
<tr><td><code id="freq_by_+3A_decreasing">decreasing</code></td>
<td>
<p>logical vector of length 1 specifying whether the table
should be sorted in decreasing (TRUE) or increasing (FALSE) order.</p>
</td></tr>
<tr><td><code id="freq_by_+3A_na.last">na.last</code></td>
<td>
<p>logical vector of length 1 specifying whether the table should
have NAs last or in whatever position they end up at. This argument is only
relevant if NAs exist in <code>x</code> and are included in the table (e.g.,
useNA = &quot;always&quot; or exclude = NULL).</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>tables_by</code> uses <code>plyr::rbind.fill</code> to combine the results from
<code>table</code> applied to each variable into a single data.frame for each
group. If a variable from <code>data[vrb.nm]</code> for each group does not have
values present in other variables from <code>data[vrb.nm]</code> for that group,
then the frequencies in the return object will be 0.
</p>
<p>The name for the table element giving the frequency of missing values is
&quot;(NA)&quot;. This is different from <code>table</code> where the name is
<code>NA_character_</code>. This change allows for the sorting of tables that
include missing values, as subsetting in R is not possible with
<code>NA_character_</code> names. In future versions of the package, this might
change as it should be possible to avoid this issue by subetting with a
logical vector or integer indices instead of names. However, it is convenient
to be able to subset the return object fully by names.
</p>


<h3>Value</h3>

<p>list of numeric vector of frequencies by group. The number of list
elements are the groups specified by <code>unique(interaction(grp, sep =
  sep))</code>. The frequencies either counts (if <code>prop</code> = FALSE) or
proportions (if <code>prop</code> = TRUE) with the unique values of <code>x</code> as
names (missing values have name = &quot;(NA)&quot;). Note, this is different from
<code>table</code>, which returns a 1D-array and has class &quot;table&quot;.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+freq">freq</a></code>
<code><a href="#topic+freq_by">freq_by</a></code>
<code><a href="#topic+freqs_by">freqs_by</a></code>
<code><a href="base.html#topic+table">table</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- freq_by(mtcars$"gear", grp = mtcars$"vs")
str(x)
y &lt;- freq_by(mtcars$"am", grp = mtcars$"vs", useNA = "no")
str(y)
str2str::lv2m(lapply(X = y, FUN = rev), along = 1) # ready to pass to prop.test()
</code></pre>

<hr>
<h2 id='freqs'>Multiple Univariate Frequency Tables</h2><span id='topic+freqs'></span>

<h3>Description</h3>

<p><code>freqs</code> creates a frequency table for a set of variables in a
data.frame. Depending on <code>total</code>, frequencies for all the variables
together can be returned. The function probably makes the most sense for sets
of variables with similar unique values (e.g., items from a questionnaire
with similar response options).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>freqs(data, vrb.nm, prop = FALSE, useNA = "always", total = "no")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="freqs_+3A_data">data</code></td>
<td>
<p>data.fame of data.</p>
</td></tr>
<tr><td><code id="freqs_+3A_vrb.nm">vrb.nm</code></td>
<td>
<p>character vector of colnames from <code>data</code> specifying the
variables.</p>
</td></tr>
<tr><td><code id="freqs_+3A_prop">prop</code></td>
<td>
<p>logical vector of length 1 specifying whether the frequencies
should be counts (FALSE) or proportions (TRUE). Note, whether the
proportions include missing values depends on the <code>useNA</code> argument.</p>
</td></tr>
<tr><td><code id="freqs_+3A_usena">useNA</code></td>
<td>
<p>character vector of length 1 specifying how missing values
should be handled. The three options are 1) &quot;no&quot; = do not include NA
frequencies in the return object, 2) &quot;ifany&quot; = only NA frequencies if there
are any missing values (in any variable from <code>data[vrb.nm]</code>), or 3)
&quot;always&quot; = do include NA frequencies regardless of whether there are
missing values or not.</p>
</td></tr>
<tr><td><code id="freqs_+3A_total">total</code></td>
<td>
<p>character vector of length 1 specifying whether the frequencies
for the set of variables as a whole should be returned. The name &quot;total&quot;
refers to tabulating the frequencies for the variables from
<code>data[vrb.nm]</code> together as a set. The three options are 1) &quot;no&quot; = do
not include a row for the total frequencies in the return object, 2) &quot;yes&quot;
= do include the total frequencies as the first row in the return object,
or 3) &quot;only&quot; = only include the total frequencies as a single row in the
return object and do not include rows for each of the individual column
frequencies in <code>data[vrb.nm]</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>freqs</code> uses <code>plyr::rbind.fill</code> to combine the results from
<code>table</code> applied to each variable into a single data.frame. If a variable
from <code>data[vrb.nm]</code> does not have values present in other variables from
<code>data[vrb.nm]</code>, then the frequencies in the return object will be 0.
</p>
<p>The name for the table element giving the frequency of missing values is
&quot;(NA)&quot;. This is different from <code>table</code> where the name is
<code>NA_character_</code>. This change allows for the sorting of tables that
include missing values, as subsetting in R is not possible with
<code>NA_character_</code> names. In future versions of the package, this might
change as it should be possible to avoid this issue by subetting with a
logical vector or integer indices instead of names. However, it is convenient
to be able to subset the return object fully by names.
</p>


<h3>Value</h3>

<p>data.frame of frequencies for the variables in <code>data[vrb.nm]</code>.
Depending on <code>prop</code>, the frequencies are either counts (FALSE) or
proportions (TRUE). Depending on <code>total</code>, the nrow is either 1)
<code>length(vrb.nm)</code> (if <code>total</code> = &quot;no&quot;), 1 + <code>length(vrb.nm)</code>
(if <code>total</code> = &quot;yes&quot;), or 3) 1 (if <code>total</code> = &quot;only&quot;). The rownames
are <code>vrb.nm</code> for each variable in <code>data[vrb.nm]</code> and &quot;_total_&quot;
for the total row (if present). The colnames are the unique values present
in <code>data[vrb.nm]</code>, potentially including &quot;(NA)&quot; depending on
<code>useNA</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+freq">freq</a></code>
<code><a href="#topic+freqs_by">freqs_by</a></code>
<code><a href="#topic+freq_by">freq_by</a></code>
<code><a href="base.html#topic+table">table</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>vrb_nm &lt;- str2str::inbtw(names(psych::bfi), "A1","O5")
freqs(data = psych::bfi, vrb.nm = vrb_nm) # default
freqs(data = psych::bfi, vrb.nm = vrb_nm, prop = TRUE) # proportions by row
freqs(data = psych::bfi, vrb.nm = vrb_nm, useNA = "no") # without NA counts
freqs(data = psych::bfi, vrb.nm = vrb_nm, total = "yes") # include total counts
</code></pre>

<hr>
<h2 id='freqs_by'>Multiple Univariate Frequency Tables</h2><span id='topic+freqs_by'></span>

<h3>Description</h3>

<p><code>freqs_by</code> creates a frequency table for a set of variables in a
data.frame by group. Depending on <code>total</code>, frequencies for all the
variables together can be returned by group. The function probably makes the
most sense for sets of variables with similar unique values (e.g., items from
a questionnaire with similar response options).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>freqs_by(
  data,
  vrb.nm,
  grp.nm,
  prop = FALSE,
  useNA = "always",
  total = "no",
  sep = "."
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="freqs_by_+3A_data">data</code></td>
<td>
<p>data.fame of data.</p>
</td></tr>
<tr><td><code id="freqs_by_+3A_vrb.nm">vrb.nm</code></td>
<td>
<p>character vector of colnames from <code>data</code> specifying the
variables.</p>
</td></tr>
<tr><td><code id="freqs_by_+3A_grp.nm">grp.nm</code></td>
<td>
<p>character vector of colnames from <code>data</code> specifying the
groups.</p>
</td></tr>
<tr><td><code id="freqs_by_+3A_prop">prop</code></td>
<td>
<p>logical vector of length 1 specifying whether the frequencies
should be counts (FALSE) or proportions (TRUE). Note, whether the
proportions include missing values depends on the <code>useNA</code> argument.</p>
</td></tr>
<tr><td><code id="freqs_by_+3A_usena">useNA</code></td>
<td>
<p>character vector of length 1 specifying how missing values
should be handled. The three options are 1) &quot;no&quot; = do not include NA
frequencies in the return object, 2) &quot;ifany&quot; = only NA frequencies if there
are any missing values (in any variable from <code>data[vrb.nm]</code>), or 3)
&quot;always&quot; = do include NA frequencies regardless of whether there are
missing values or not.</p>
</td></tr>
<tr><td><code id="freqs_by_+3A_total">total</code></td>
<td>
<p>character vector of length 1 specifying whether the frequencies
for the set of variables as a whole should be returned. The name &quot;total&quot;
refers to tabulating the frequencies for the variables from
<code>data[vrb.nm]</code> together as a set. The three options are 1) &quot;no&quot; = do
not include a row for the total frequencies in the return object, 2) &quot;yes&quot;
= do include the total frequencies as the first row in the return object,
or 3) &quot;only&quot; = only include the total frequencies as a single row in the
return object and do not include rows for each of the individual column
frequencies in <code>data[vrb.nm]</code>.</p>
</td></tr>
<tr><td><code id="freqs_by_+3A_sep">sep</code></td>
<td>
<p>character vector of length 1 specifying the string to combine the
group values together with. <code>sep</code> is only used if there are multiple
grouping variables (i.e., <code>length(grp.nm)</code> &gt; 1).</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>freqs_by</code> uses <code>plyr::rbind.fill</code> to combine the results from
<code>table</code> applied to each variable into a single data.frame for each
group. If a variable from <code>data[vrb.nm]</code> for each group does not have
values present in other variables from <code>data[vrb.nm]</code> for that group,
then the frequencies in the return object will be 0.
</p>
<p>The name for the table element giving the frequency of missing values is
&quot;(NA)&quot;. This is different from <code>table</code> where the name is
<code>NA_character_</code>. This change allows for the sorting of tables that
include missing values, as subsetting in R is not possible with
<code>NA_character_</code> names. In future versions of the package, this might
change as it should be possible to avoid this issue by subetting with a
logical vector or integer indices instead of names. However, it is convenient
to be able to subset the return object fully by names.
</p>


<h3>Value</h3>

<p>list of data.frames containing the frequencies for the variables in
<code>data[vrb.nm]</code> by group. The number of list elements are the groups
specified by <code>unique(interaction(data[grp.nm], sep = sep))</code>. Depending
on <code>prop</code>, the frequencies are either counts (FALSE) or proportions
(TRUE) by group. Depending on <code>total</code>, the nrow for each data.frame is
either 1) <code>length(vrb.nm)</code> (if <code>total</code> = &quot;no&quot;), 1 +
<code>length(vrb.nm)</code> (if <code>total</code> = &quot;yes&quot;), or 3) 1 (if <code>total</code> =
&quot;only&quot;). The rownames are <code>vrb.nm</code> for each variable in
<code>data[vrb.nm]</code> and &quot;_total_&quot; for the total row (if present). The
colnames for each data.frame are the unique values present in
<code>data[vrb.nm]</code>, potentially including &quot;(NA)&quot; depending on
<code>useNA</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+freqs">freqs</a></code>
<code><a href="#topic+freq_by">freq_by</a></code>
<code><a href="#topic+freqs_by">freqs_by</a></code>
<code><a href="base.html#topic+table">table</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>vrb_nm &lt;- str2str::inbtw(names(psych::bfi), "A1","O5")
freqs_by(data = psych::bfi, vrb.nm = vrb_nm, grp.nm = "gender") # default
freqs_by(data = psych::bfi, vrb.nm = vrb_nm, grp.nm = "gender",
   prop = TRUE) # proportions by row
freqs_by(data = psych::bfi, vrb.nm = vrb_nm, grp.nm = "gender",
   useNA = "no") # without NA counts
freqs_by(data = psych::bfi, vrb.nm = vrb_nm, grp.nm = "gender",
   total = "yes") # include total counts
freqs_by(data = psych::bfi, vrb.nm = vrb_nm,
   grp.nm = c("gender","education")) # multiple grouping variables
</code></pre>

<hr>
<h2 id='gtheory'>Generalizability Theory Reliability of a Score</h2><span id='topic+gtheory'></span>

<h3>Description</h3>

<p><code>gtheory</code> uses generalizability theory to compute the reliability
coefficient of a score. It assumes single-level data where the rows are cases
and the columns are variables/items. Generaliability theory coefficients in
this case are the same as intraclass correlations (ICC). The default computes
ICC(3,k), which is identical to cronbach's alpha, from <code>cross.vrb</code> =
TRUE. When <code>cross.vrb</code> is FALSE, ICC(2,k) is computed, which takes mean
differences between variables/items into account. <code>gtheory</code> is a wrapper
function for <code><a href="psych.html#topic+ICC">ICC</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gtheory(
  data,
  vrb.nm,
  ci.type = "classic",
  level = 0.95,
  cross.vrb = TRUE,
  R = 200L,
  boot.ci.type = "perc"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gtheory_+3A_data">data</code></td>
<td>
<p>data.frame of data.</p>
</td></tr>
<tr><td><code id="gtheory_+3A_vrb.nm">vrb.nm</code></td>
<td>
<p>character vector of colnames from <code>data</code> specifying the
variables/items.</p>
</td></tr>
<tr><td><code id="gtheory_+3A_ci.type">ci.type</code></td>
<td>
<p>character vector of length = 1 specifying the type of
confidence interval to compute. There are currently two options: 1)
&quot;classic&quot; = traditional ICC-based confidence intervals (see details), 2)
&quot;boot&quot; = bootstrapped confidence intervals.</p>
</td></tr>
<tr><td><code id="gtheory_+3A_level">level</code></td>
<td>
<p>double vector of length 1 specifying the confidence level from 0
to 1.</p>
</td></tr>
<tr><td><code id="gtheory_+3A_cross.vrb">cross.vrb</code></td>
<td>
<p>logical vector of length 1 specifying whether the
variables/items should be crossed when computing the generalizability
theory coefficient. If TRUE, then only the covariance structure of the
variables/items will be incorperated into the estimate of reliability. If
FALSE, then the mean structure of the variables/items will be incorperated.</p>
</td></tr>
<tr><td><code id="gtheory_+3A_r">R</code></td>
<td>
<p>integer vector of length 1 specifying the number of bootstrapped
resamples to use. Only used if <code>ci.type</code> = &quot;boot&quot;.</p>
</td></tr>
<tr><td><code id="gtheory_+3A_boot.ci.type">boot.ci.type</code></td>
<td>
<p>character vector of length 1 specifying the type of
bootstrapped confidence interval to compute. The options are 1) &quot;perc&quot; for
the regular percentile method, 2) &quot;bca&quot; for bias-corrected and accelerated
percentile method, 3) &quot;norm&quot; for the normal method that uses the
bootstrapped standard error to construct symmetrical confidence intervals
with the classic formula around the bias-corrected estimate, and 4) &quot;basic&quot;
for the basic method. Note, &quot;stud&quot; for the studentized method is NOT an
option. See <code><a href="boot.html#topic+boot.ci">boot.ci</a></code> as well as
<code><a href="#topic+confint2.boot">confint2.boot</a></code> for details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When <code>ci.type</code> = &quot;classic&quot; the confidence intervals are computed
according to the formulas laid out by McGraw, Kenneth, and Wong, (1996).
These are taken from the <code><a href="psych.html#topic+ICC">ICC</a></code> function in the
<code>psych</code> package. They are appropriately non-symmetrical given ICCs are
not unbounded and range from 0 to 1. Therefore, there is no standard error
associated with the coefficient. Note, they differ from the confidence
intervals available in the <code><a href="#topic+cronbach">cronbach</a></code> function. When
<code>ci.type</code> = &quot;boot&quot; the standard deviation of the empirical sampling
distribution is returned as the standard error, which may or may not be
trustworthy depending on the value of the ICC and sample size.
</p>


<h3>Value</h3>

<p>double vector containing the generalizability theory coefficient,
it's standard error (if <code>ci.type</code> = &quot;boot&quot;), and it's confidence
interval.
</p>


<h3>References</h3>

<p>McGraw, Kenneth O. and Wong, S. P. (1996), Forming inferences about some
intraclass correlation coefficients. Psychological Methods, 1, 30-46. + errata on page 390.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gtheorys">gtheorys</a></code>
<code><a href="#topic+gtheory_ml">gtheory_ml</a></code>
<code><a href="#topic+cronbach">cronbach</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
gtheory(attitude, vrb.nm = names(attitude), ci.type = "classic")
## Not run: 
gtheory(attitude, vrb.nm = names(attitude), ci.type = "boot")
gtheory(attitude, vrb.nm = names(attitude), ci.type = "boot",
   R = 250L, boot.ci.type = "bca")

## End(Not run)

# comparison to cronbach's alpha:
gtheory(attitude, names(attitude))
gtheory(attitude, names(attitude), cross.vrb = FALSE)
a &lt;- suppressMessages(psych::alpha(attitude)[["total"]]["raw_alpha"])
psych::alpha.ci(a, n.obs = 30, n.var = 7, digits = 7) # slightly different confidence interval

</code></pre>

<hr>
<h2 id='gtheory_ml'>Generalizability Theory Reliability of a Multilevel Score</h2><span id='topic+gtheory_ml'></span>

<h3>Description</h3>

<p><code>gtheory_ml</code> uses generalizability theory to compute the reliability
coefficients of a multilevel score. It computes a within-group coefficient
that assesses the reliability of the group-deviated score (e.g., after
calling <code><a href="#topic+center_by">center_by</a></code>) and a between-group coefficient that assess
the reliability of the mean aggregate score (e.g., after calling
<code><a href="#topic+agg">agg</a></code>). It assumes two-level data where the rows are in long
format and the columns are the variables/items of the score. Generaliability
theory coefficients with multilevel data are analagous to intraclass
correlations (ICC), but add an additional grouping variable. The default
computes a multilevel version of ICC(3,k) from <code>cross.obs</code> = TRUE. When
<code>cross.obs</code> = FALSE, a multilevel version of ICC(2,k) is computed, which
takes mean differences between variables/items into account.
<code>gtheory_ml</code> is a wrapper function for <code><a href="psych.html#topic+mlr">mlr</a></code>. Note,
this function can take several minutes to run if you have a moderate to large
dataset.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gtheory_ml(data, vrb.nm, grp.nm, obs.nm, cross.obs = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gtheory_ml_+3A_data">data</code></td>
<td>
<p>data.frame of data.</p>
</td></tr>
<tr><td><code id="gtheory_ml_+3A_vrb.nm">vrb.nm</code></td>
<td>
<p>character vector of colnames from <code>data</code> specifying the
variables/items.</p>
</td></tr>
<tr><td><code id="gtheory_ml_+3A_grp.nm">grp.nm</code></td>
<td>
<p>character vector of length 1 with colname from <code>data</code>
specifying the grouping variable. Because <code>gtheory_ml</code> is specific to
two-level data, this can only be one variable.</p>
</td></tr>
<tr><td><code id="gtheory_ml_+3A_obs.nm">obs.nm</code></td>
<td>
<p>character vector of of length 1 with colname from <code>data</code>
specifying the observation variable. In this context, observation refers to
comparable cases across groups. In a longitudinal study, the groups are
people and the observations are timepoints. For example, each person has a
timepoint 1, timepoint 2, timepoint 3, etc. In an school study, the groups
are classrooms and the observations are students. For example, each
classroom has a student 1, student 2, student 3, etc. While longitudinal
studies often have a time variable in their data, school studies don't
always have a student variable. You would then have to create a student
variable to be able to use <code>gtheory_ml</code>.</p>
</td></tr>
<tr><td><code id="gtheory_ml_+3A_cross.obs">cross.obs</code></td>
<td>
<p>logical vector of length 1 specifying whether the
observations should be crossed when computing the generalizability theory
coefficient. If TRUE, the observations are treated as fixed; if FALSE, they
are treated as random. See details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>gtheory_ml</code> uses <code><a href="psych.html#topic+mlr">mlr</a></code>, which is based on the
formulas in Shrout, Patrick, and Lane (2012). When <code>cross.obs</code> = TRUE,
the within-group coefficient is Rc and the between-group coefficient is RkF.
When <code>cross.obs</code> = FALSE, the within-group coefficient is Rcn and the
between-group coefficient is RkRn.
</p>
<p><code>gtheory_ml</code> does not currently have standard errors or confidence
intervals. I am not aware of mathematical formulas for analytical confidence
intervals, and because the generaliability theory coefficients can take
several minutes to estimate, bootstraped confidence intervals seem too
time-intensive to be useful at the moment.
</p>
<p><code>gtheory_ml</code> does not work with a single variable/item. You can still
use generalizability theory to estimate between-group reliability in that
instance though. To do so, reshape the variable/item from long to wide (e.g.,
<code><a href="str2str.html#topic+unstack2">unstack2</a></code>) so that you have a column for each
observation of that single variable/item and the rows are the groups. Then
you can use <code>gtheory</code> and treat each observation as a &quot;different&quot;
variable/item.
</p>


<h3>Value</h3>

<p>list with two elements. The first is named &quot;within&quot; and refers to the
within-group reliability. The second is named &quot;between&quot; and refers to the
between-group reliability. Each contains a double vector where the first
element is named &quot;est&quot; and contains the generalizability theory coefficient
itself. The second element is named &quot;average_r&quot; and contains the average
correlation at that level of the data based on <code><a href="#topic+cor_ml">cor_ml</a></code> (which
is a wrapper for <code><a href="psych.html#topic+statsBy">statsBy</a></code>). The third element is named
&quot;nvrb&quot; and contains the number of variables/items. These later two elements
are included because even though the reliability coefficients are
calculated from variance components, they are indirectly based on the
average correlation and number of variables/items, similar to Cronbach's
alpha.
</p>


<h3>References</h3>

<p>Shrout, Patrick and Lane, Sean P (2012), Psychometrics. In M.R. Mehl and T.S.
Conner (eds) Handbook of research methods for studying daily life, (p 302-320)
New York. Guilford Press
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gtheorys_ml">gtheorys_ml</a></code>
<code><a href="#topic+gtheory">gtheory</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
shrout &lt;- structure(list(Person = c(1L, 2L, 3L, 4L, 5L, 1L, 2L, 3L, 4L,
   5L, 1L, 2L, 3L, 4L, 5L, 1L, 2L, 3L, 4L, 5L), Time = c(1L, 1L,
      1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 4L, 4L, 4L,
      4L, 4L), Item1 = c(2L, 3L, 6L, 3L, 7L, 3L, 5L, 6L, 3L, 8L, 4L,
         4L, 7L, 5L, 6L, 1L, 5L, 8L, 8L, 6L), Item2 = c(3L, 4L, 6L, 4L,
            8L, 3L, 7L, 7L, 5L, 8L, 2L, 6L, 8L, 6L, 7L, 3L, 9L, 9L, 7L, 8L
         ), Item3 = c(6L, 4L, 5L, 3L, 7L, 4L, 7L, 8L, 9L, 9L, 5L, 7L,
            9L, 7L, 8L, 4L, 7L, 9L, 9L, 6L)), .Names = c("Person", "Time",
               "Item1", "Item2", "Item3"), class = "data.frame", row.names = c(NA,
                  -20L))
mlr_obj &lt;- psych::mlr(x = shrout, grp = "Person", Time = "Time",
   items = c("Item1", "Item2", "Item3"),
   alpha = FALSE, icc = FALSE, aov = FALSE, lmer = TRUE, lme = FALSE,
   long = FALSE, plot = FALSE)
gtheory_ml(data = shrout, vrb.nm = c("Item1", "Item2", "Item3"),
   grp.nm = "Person", obs.nm = "Time", cross.obs = TRUE) # crossed time
gtheory_ml(data = shrout, vrb.nm = c("Item1", "Item2", "Item3"),
   grp.nm = "Person", obs.nm = "Time", cross.obs = FALSE) # nested time

</code></pre>

<hr>
<h2 id='gtheorys'>Generalizability Theory Reliability of Multiple Scores</h2><span id='topic+gtheorys'></span>

<h3>Description</h3>

<p><code>gtheorys</code> uses generalizability theory to compute the reliability
coefficient of multiple scores. It assumes single-level data where the rows
are cases and the columns are variables/items. Generaliability theory
coefficients in this case are the same as intraclass correlations (ICC). The
default computes ICC(3,k), which is identical to cronbach's alpha, from
<code>cross.vrb</code> = TRUE. When <code>cross.vrb</code> is FALSE, ICC(2,k) is
computed, which takes mean differences between variables/items into account.
<code>gtheorys</code> is a wrapper function for <code><a href="psych.html#topic+ICC">ICC</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gtheorys(
  data,
  vrb.nm.list,
  ci.type = "classic",
  level = 0.95,
  cross.vrb = TRUE,
  R = 200L,
  boot.ci.type = "perc"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gtheorys_+3A_data">data</code></td>
<td>
<p>data.frame of data.</p>
</td></tr>
<tr><td><code id="gtheorys_+3A_vrb.nm.list">vrb.nm.list</code></td>
<td>
<p>list of character vectors containing colnames from
<code>data</code> specifying each set of variables/items.</p>
</td></tr>
<tr><td><code id="gtheorys_+3A_ci.type">ci.type</code></td>
<td>
<p>character vector of length = 1 specifying the type of
confidence interval to compute. There are currently two options: 1)
&quot;classic&quot; = traditional ICC-based confidence intervals (see details), 2)
&quot;boot&quot; = bootstrapped confidence intervals.</p>
</td></tr>
<tr><td><code id="gtheorys_+3A_level">level</code></td>
<td>
<p>double vector of length 1 specifying the confidence level from 0
to 1.</p>
</td></tr>
<tr><td><code id="gtheorys_+3A_cross.vrb">cross.vrb</code></td>
<td>
<p>logical vector of length 1 specifying whether the
variables/items should be crossed when computing the generalizability
theory coefficients. If TRUE, then only the covariance structure of the
variables/items will be incorperated into the estimates of reliability. If
FALSE, then the mean structure of the variables/items will be incorperated.</p>
</td></tr>
<tr><td><code id="gtheorys_+3A_r">R</code></td>
<td>
<p>integer vector of length 1 specifying the number of bootstrapped
resamples to use. Only used if <code>ci.type</code> = &quot;boot&quot;.</p>
</td></tr>
<tr><td><code id="gtheorys_+3A_boot.ci.type">boot.ci.type</code></td>
<td>
<p>character vector of length 1 specifying the type of
bootstrapped confidence interval to compute. The options are 1) &quot;perc&quot; for
the regular percentile method, 2) &quot;bca&quot; for bias-corrected and accelerated
percentile method, 3) &quot;norm&quot; for the normal method that uses the
bootstrapped standard error to construct symmetrical confidence intervals
with the classic formula around the bias-corrected estimate, and 4) &quot;basic&quot;
for the basic method. Note, &quot;stud&quot; for the studentized method is NOT an
option. See <code><a href="boot.html#topic+boot.ci">boot.ci</a></code> as well as
<code><a href="#topic+confint2.boot">confint2.boot</a></code> for details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When <code>ci.type</code> = &quot;classic&quot; the confidence intervals are computed
according to the formulas laid out by McGraw, Kenneth and Wong (1996). These
are taken from the <code><a href="psych.html#topic+ICC">ICC</a></code> function in the <code>psych</code>
package. They are appropriately non-symmetrical given ICCs are not unbounded
and range from 0 to 1. Therefore, there is no standard error associated with
the coefficient. Note, they differ from the confidence intervals available in
the <code><a href="#topic+cronbachs">cronbachs</a></code> function. When <code>ci.type</code> = &quot;boot&quot; the
standard deviation of the empirical sampling distribution is returned as the
standard error, which may or may not be trustworthy depending on the value of
the ICC and sample size.
</p>


<h3>Value</h3>

<p>data.frame containing the generalizability theory statistical information.
The columns are as follows:
</p>

<dl>
<dt>est</dt><dd><p>the generalizability theory coefficient itself</p>
</dd>
<dt>se</dt><dd><p>standard error of the reliability coefficient</p>
</dd>
<dt>lwr</dt><dd><p>lower bound of the confidence interval for the reliability coefficient</p>
</dd>
<dt>lwr</dt><dd><p>lower bound of the confidence interval for the reliability coefficient</p>
</dd>
</dl>



<h3>References</h3>

<p>McGraw, Kenneth O. and Wong, S. P. (1996), Forming inferences about some
intraclass correlation coefficients. Psychological Methods, 1, 30-46. + errata on page 390.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gtheory">gtheory</a></code>
<code><a href="#topic+gtheorys_ml">gtheorys_ml</a></code>
<code><a href="#topic+cronbachs">cronbachs</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
dat0 &lt;- psych::bfi[1:100, ] # reduce number of rows
   # to reduce computational time of boot examples
dat1 &lt;- str2str::pick(x = dat0, val = c("A1","C4","C5","E1","E2","O2","O5",
   "gender","education","age"), not = TRUE, nm = TRUE)
vrb_nm_list &lt;- lapply(X = str2str::sn(c("E","N","C","A","O")), FUN = function(nm) {
   str2str::pick(x = names(dat1), val = nm, pat = TRUE)})
gtheorys(data = dat1, vrb.nm.list = vrb_nm_list)
## Not run: 
gtheorys(data = dat1, vrb.nm.list = vrb_nm_list, ci.type = "boot") # singular messages
gtheorys(data = dat1, vrb.nm.list = vrb_nm_list, ci.type = "boot",
   R = 250L, boot.ci.type = "bca")

## End(Not run)
gtheorys(data = attitude,
   vrb.nm.list = list(names(attitude))) # also works with only one set of variables/items

</code></pre>

<hr>
<h2 id='gtheorys_ml'>Generalizability Theory Reliability of Multiple Multilevel Scores</h2><span id='topic+gtheorys_ml'></span>

<h3>Description</h3>

<p><code>gtheorys_ml</code> uses generalizability theory to compute the reliability
coefficients of multiple multilevel score. It computes within-group
coefficients that assess the reliability of the group-deviated scores (e.g.,
after calling <code><a href="#topic+centers_by">centers_by</a></code>) and between-group coefficients that
assess the reliability of the mean aggregate scores (e.g., after calling
<code><a href="#topic+aggs">aggs</a></code>). It assumes two-level data where the rows are in long
format and the columns are the variables/items of the score. Generaliability
theory coefficients with multilevel data are analagous to intraclass
correlations (ICC), but add an additional grouping variable. The default
computes a multilevel version of ICC(3,k) from <code>cross.obs</code> = TRUE. When
<code>cross.obs</code> = FALSE, a multilevel version of ICC(2,k) is computed, which
takes mean differences between variables/items into account.
<code>gtheorys_ml</code> is a wrapper function for <code><a href="psych.html#topic+mlr">mlr</a></code>. Note,
this function can take several minutes to run if you have a moderate to large
dataset.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gtheorys_ml(data, vrb.nm.list, grp.nm, obs.nm, cross.obs = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gtheorys_ml_+3A_data">data</code></td>
<td>
<p>data.frame of data.</p>
</td></tr>
<tr><td><code id="gtheorys_ml_+3A_vrb.nm.list">vrb.nm.list</code></td>
<td>
<p>list of character vectors of colnames from <code>data</code>
specifying the sets of variables/items.</p>
</td></tr>
<tr><td><code id="gtheorys_ml_+3A_grp.nm">grp.nm</code></td>
<td>
<p>character vector of length 1 with colname from <code>data</code>
specifying the grouping variable. Because <code>gtheorys_ml</code> is specific to
two-level data, this can only be one variable.</p>
</td></tr>
<tr><td><code id="gtheorys_ml_+3A_obs.nm">obs.nm</code></td>
<td>
<p>character vector of of length 1 with colname from <code>data</code>
specifying the observation variable. In this context, observation refers to
comparable cases across groups. In a longitudinal study, the groups are
people and the observations are timepoints. For example, each person has a
timepoint 1, timepoint 2, timepoint 3, etc. In an school study, the groups
are classrooms and the observations are students. For example, each
classroom has a student 1, student 2, student 3, etc. While longitudinal
studies often have a time variable in their data, school studies don't have
always a student variable. You would then have to create a student variable
to be able to use this function.</p>
</td></tr>
<tr><td><code id="gtheorys_ml_+3A_cross.obs">cross.obs</code></td>
<td>
<p>logical vector of length 1 specifying whether the
observations should be crossed when computing the generalizability theory
coefficients. If TRUE, the observations are treated as fixed; if FALSE, they
are treated as random. See details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>gtheorys_ml</code> uses <code><a href="psych.html#topic+mlr">mlr</a></code>, which is based on the
formulas in Shrout, Patrick, and Lane (2012). When <code>cross.obs</code> = TRUE,
the within-group coefficient is Rc and the between-group coefficient is RkF.
When <code>cross.obs</code> = FALSE, the within-group coefficient is Rcn and the
between-group coefficient is RkRn.
</p>
<p><code>gtheorys_ml</code> does not currently have standard errors or confidence
intervals. I am not aware of mathematical formulas for analytical confidence
intervals, and because the generaliability theory coefficients can take
several minutes to estimate, bootstraped confidence intervals seem too
time-intensive to be useful at the moment.
</p>
<p><code>gtheorys_ml</code> does not work with multiple single variable/item scores.
You can still use generalizability theory to estimate between-group
reliability in that instance though. To do so, reshape the multiple single
variables/items from long to wide (e.g., <code><a href="#topic+long2wide">long2wide</a></code>) so that you
have a column for each observation of that single variable/item and the rows
are the groups. Then you can use <code>gtheorys</code> and treat each observation as
a &quot;different&quot; variable/item.
</p>


<h3>Value</h3>

<p>list with two elements. The first is named &quot;within&quot; and refers to the
within-group reliability. The second is named &quot;between&quot; and refers to the
between-group reliability. Each contains a data.frame with the following columns:
</p>

<dl>
<dt>est</dt><dd><p>generalizability theory reliability coefficient itself</p>
</dd>
<dt>average_r</dt><dd><p>the average correlation at each level of the data based on
<code><a href="#topic+cor_ml">cor_ml</a></code> (which is a wrapper for <code><a href="psych.html#topic+statsBy">statsBy</a></code>)</p>
</dd>
<dt>nvrb</dt><dd><p>number of variables/items that make up that score</p>
</dd>
</dl>

<p>The later two columns are included because even though the reliability coefficients
are calculated from variance components, they are indirectly based on the average
correlation and number of variables/items similar to Cronbach's alpha.
</p>


<h3>References</h3>

<p>Shrout, Patrick and Lane, Sean P (2012), Psychometrics. In M.R. Mehl and T.S.
Conner (eds) Handbook of research methods for studying daily life, (p 302-320)
New York. Guilford Press
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gtheory_ml">gtheory_ml</a></code>
<code><a href="#topic+gtheorys">gtheorys</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
dat &lt;- psychTools::sai[psychTools::sai$"study" == "VALE", ] # 4 timepoints
vrb_nm_list &lt;- list("positive_affect" = c("calm","secure","at.ease","rested",
   "comfortable","confident"), # extra: "relaxed","content","joyful"
   "negative_affect" = c("tense","regretful","upset","worrying","anxious",
      "nervous")) # extra: "jittery","high.strung","worried","rattled"
suppressMessages(gtheorys_ml(data = dat, vrb.nm.list = vrb_nm_list, grp.nm = "id",
   obs.nm = "time", cross.obs = TRUE))
suppressMessages(gtheorys_ml(data = dat, vrb.nm.list = vrb_nm_list, grp.nm = "id",
   obs.nm = "time", cross.obs = FALSE))
gtheorys_ml(data = dat, vrb.nm.list = vrb_nm_list["positive_affect"], grp.nm = "id",
   obs.nm = "time") # also works with only one set of variables/items

</code></pre>

<hr>
<h2 id='icc_11'>Intraclass Correlation for Multilevel Analysis: ICC(1,1)</h2><span id='topic+icc_11'></span>

<h3>Description</h3>

<p><code>icc_11</code> computes the intraclass correlation (ICC) based on a single
rater with a single dimension, aka ICC(1,1). Traditionally, this is the type
of ICC used for multilevel analysis where the value is interpreted as the
proportion of variance accounted for by group membership. In other words,
ICC(1,1) = the proportion of between-group variance; 1 - ICC(1,1) = the
proportion of within-group variance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>icc_11(x, grp, how = "lme", REML = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="icc_11_+3A_x">x</code></td>
<td>
<p>numeric vector.</p>
</td></tr>
<tr><td><code id="icc_11_+3A_grp">grp</code></td>
<td>
<p>atomic vector the same length as <code>x</code> providing the grouping
variable.</p>
</td></tr>
<tr><td><code id="icc_11_+3A_how">how</code></td>
<td>
<p>character vector of length 1 specifying how the ICC(1,1) should be
calculated. There are four options: 1) &quot;lme&quot; uses a linear mixed effects
model with the function <code><a href="nlme.html#topic+lme">lme</a></code> from the package
<code>nlme</code>, 2) &quot;lmer&quot; uses a linear mixed effects modeling with the
function <code><a href="lme4.html#topic+lmer">lmer</a></code> from the package <code>lme4</code>, 3) &quot;aov&quot;
uses a one-way analysis of variance with the function
<code><a href="stats.html#topic+aov">aov</a></code>, and 4) &quot;raw&quot; uses the observed variances, which
provides a biased estimate of the ICC(1,1) and is not recommended (It is
only included for teaching purposes).</p>
</td></tr>
<tr><td><code id="icc_11_+3A_reml">REML</code></td>
<td>
<p>logical vector of length 1 specifying whether restricted maximum
likelihood estimation (TRUE) should be used rather than traditional maximum
likelihood estimation (FALSE). Only used for linear mixed effects models if
how = &quot;lme&quot; or how = &quot;lmer&quot;.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>numeric vector of length 1 providing ICC(1,1) and computed based on
the <code>how</code> argument.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+iccs_11">iccs_11</a></code> # ICC(1,1) for multiple variables,
<code><a href="#topic+icc_all_by">icc_all_by</a></code> # all six types of ICCs by group,
<code><a href="nlme.html#topic+lme">lme</a></code> # how = &quot;lme&quot; function,
<code><a href="lme4.html#topic+lmer">lmer</a></code> # how = &quot;lmer&quot; function,
<code><a href="stats.html#topic+aov">aov</a></code> # how = &quot;aov&quot; function,
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# BALANCED DATA (how = "aov" and "lme"/"lmer" do YES provide the same value)

str(InsectSprays)
icc_11(x = InsectSprays$"count", grp = InsectSprays$"spray", how = "aov")
icc_11(x = InsectSprays$"count", grp = InsectSprays$"spray", how = "lme")
icc_11(x = InsectSprays$"count", grp = InsectSprays$"spray", how = "lmer")
icc_11(x = InsectSprays$"count", grp = InsectSprays$"spray",
   how = "raw") # biased estimator and not recommended. Only available for teaching purposes.

# UN-BALANCED DATA (how = "aov" and "lme"/"lmer" do NOT provide the same value)

dat &lt;- as.data.frame(lmeInfo::Bryant2016)
icc_11(x = dat$"outcome", grp = dat$"case", how = "aov")
icc_11(x = dat$"outcome", grp = dat$"case", how = "lme")
icc_11(x = dat$"outcome", grp = dat$"case", how = "lmer")
icc_11(x = dat$"outcome", grp = dat$"case", how = "lme", REML = FALSE)
icc_11(x = dat$"outcome", grp = dat$"case", how = "lmer", REML = FALSE)

# how = "lme" does not account for any correlation structure
lme_obj &lt;- nlme::lme(outcome ~ 1, random = ~ 1 | case,
   data = dat, na.action = na.exclude,
   correlation = nlme::corAR1(form = ~ 1 | case), method = "REML")
var_corr &lt;- nlme::VarCorr(lme_obj) # VarCorr.lme
vars &lt;- as.double(var_corr[, "Variance"])
btw &lt;- vars[1]
wth &lt;- vars[2]
btw / (btw + wth)

</code></pre>

<hr>
<h2 id='icc_all_by'>All Six Intraclass Correlations by Group</h2><span id='topic+icc_all_by'></span>

<h3>Description</h3>

<p><code>icc_all_by</code> computes each of the six intraclass correlations (ICC) in
Shrout &amp; Fleiss (1979) by group. The ICCs differ by whether they treat
dimensions as fixed or random and whether they are for a single variable in
<code>data[vrb.nm]</code> of the set of variables <code>data[vrb.nm]</code>.
<code>icc_all_by</code> also returns information about the linear mixed effects
modeling (using <code><a href="lme4.html#topic+lmer">lmer</a></code>) used to compute the ICCs as well as
any warning or error messages by group. For an understanding of the six
different ICCs, see the following blogpost:
<a href="http://www.daviddisabato.com/blog/2021/10/1/the-six-different-types-of-intraclass-correlations-iccs">http://www.daviddisabato.com/blog/2021/10/1/the-six-different-types-of-intraclass-correlations-iccs</a>.
<code>icc_all_by</code> is a combination of <code><a href="#topic+by2">by2</a></code> +
<code><a href="str2str.html#topic+try_fun">try_fun</a></code> + <code><a href="psych.html#topic+ICC">ICC</a></code>
(<code><a href="psych.html#topic+ICC">ICC</a></code> calls <code><a href="lme4.html#topic+lmer">lmer</a></code> internally).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>icc_all_by(data, vrb.nm, grp.nm, ci.level = 0.95, check = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="icc_all_by_+3A_data">data</code></td>
<td>
<p>data.frame of data.</p>
</td></tr>
<tr><td><code id="icc_all_by_+3A_vrb.nm">vrb.nm</code></td>
<td>
<p>character vector of colnames from <code>data</code> specifying the
variables.</p>
</td></tr>
<tr><td><code id="icc_all_by_+3A_grp.nm">grp.nm</code></td>
<td>
<p>character vector of colnames from <code>data</code> specifying the
groups.</p>
</td></tr>
<tr><td><code id="icc_all_by_+3A_ci.level">ci.level</code></td>
<td>
<p>double vector of length 1 specifying the confidence level. It
must range from 0 to 1.</p>
</td></tr>
<tr><td><code id="icc_all_by_+3A_check">check</code></td>
<td>
<p>logical vector of length 1 specifying whether to check the
structure of the input arguments. For example, check whether
<code>data[vrb.nm]</code> are all typeof numeric. This argument is available to
allow flexibility in whether the user values informative error messages
(TRUE) vs. computational efficiency (FALSE).</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>icc_all_by</code> internally suppresses any messages, warnings, or errors
returned by <code><a href="lme4.html#topic+lmer">lmer</a></code> (e.g., &quot;boundary (singular) fit: see
?isSingular&quot;) because that information is provided in the returned
data.frame.
</p>


<h3>Value</h3>

<p>data.frame containing the unique combinations of the grouping variables
<code>data[grp.nm]</code> and each group's intraclass correlations (ICCs), their confidence intervals,
information about the <code>merMod</code> object from the linear mixed effects model,
and any warning or error messages from <code><a href="lme4.html#topic+lmer">lmer</a></code>. For an understanding of the
six different ICCs, see the following blogpost:
<a href="http://www.daviddisabato.com/blog/2021/10/1/the-six-different-types-of-intraclass-correlations-iccs">http://www.daviddisabato.com/blog/2021/10/1/the-six-different-types-of-intraclass-correlations-iccs</a>.
The first columns are always <code>unique.data.frame(data[vrb.nm])</code>. All other columns are in the
following order with the following colnames:
</p>

<dl>
<dt>icc11_est</dt><dd><p>ICC(1,1) parameter estimate</p>
</dd>
<dt>icc11_lwr</dt><dd><p>ICC(1,1) lower bound of the confidence interval</p>
</dd>
<dt>icc11_upr</dt><dd><p>ICC(1,1) lower bound of the confidence interval</p>
</dd>
<dt>icc21_est</dt><dd><p>ICC(2,1) parameter estimate</p>
</dd>
<dt>icc21_lwr</dt><dd><p>ICC(2,1) lower bound of the confidence interval</p>
</dd>
<dt>icc21_upr</dt><dd><p>ICC(2,1) lower bound of the confidence interval</p>
</dd>
<dt>icc31_est</dt><dd><p>ICC(3,1) parameter estimate</p>
</dd>
<dt>icc31_lwr</dt><dd><p>ICC(3,1) lower bound of the confidence interval</p>
</dd>
<dt>icc31_upr</dt><dd><p>ICC(3,1) lower bound of the confidence interval</p>
</dd>
<dt>icc1k_est</dt><dd><p>ICC(1,k) parameter estimate</p>
</dd>
<dt>icc1k_lwr</dt><dd><p>ICC(1,k) lower bound of the confidence interval</p>
</dd>
<dt>icc1k_upr</dt><dd><p>ICC(1,k) lower bound of the confidence interval</p>
</dd>
<dt>icc2k_est</dt><dd><p>ICC(2,k) parameter estimate</p>
</dd>
<dt>icc2k_lwr</dt><dd><p>ICC(2,k) lower bound of the confidence interval</p>
</dd>
<dt>icc2k_upr</dt><dd><p>ICC(2,k) lower bound of the confidence interval</p>
</dd>
<dt>icc3k_est</dt><dd><p>ICC(3,k) parameter estimate</p>
</dd>
<dt>icc3k_lwr</dt><dd><p>ICC(3,k) lower bound of the confidence interval</p>
</dd>
<dt>icc3k_upr</dt><dd><p>ICC(3,k) lower bound of the confidence interval</p>
</dd>
<dt>lmer_nobs</dt><dd><p>number of observations used for the linear mixed effects model.
Note, this is the number of (non-missing) rows after <code>data[vrb.nm]</code>
has been stacked together via <code><a href="utils.html#topic+stack">stack</a></code>.</p>
</dd>
<dt>lmer_ngrps</dt><dd><p>number of groups used for the linear mixed effects model.
This is the number of unique combinations of the grouping variables after <code>data[grp.nm]</code>.</p>
</dd>
<dt>lmer_logLik</dt><dd><p>logLik of the linear mixed effects model</p>
</dd>
<dt>lmer_sing</dt><dd><p>binary variable where 1 = the linear mixed effects model had
a singularity in the random effects covariance matrix or 0 = it did not</p>
</dd>
<dt>lmer_warn</dt><dd><p>binary variable where 1 = the linear mixed effects model
returned a warning or 0 = it did not</p>
</dd>
<dt>lmer_err</dt><dd><p>binary variable where 1 = the linear mixed effects model
returned an error or 0 = it did not</p>
</dd>
<dt>warn_mssg</dt><dd><p>character vector providing the warning messages for any warnings.
If a group did not generate a warning, then the value is NA</p>
</dd>
<dt>err_mssg</dt><dd><p>character vector providing the error messages for any warnings.
If a group did not generate an error, then the value is NA</p>
</dd>
</dl>



<h3>References</h3>

<p>Shrout, P.E., &amp; Fleiss, J.L. (1979). Intraclass correlations: Uses in assessing rater reliability.
Psychological Bulletin, 86(2), 420-428.
</p>


<h3>See Also</h3>

<p><code><a href="psych.html#topic+ICC">ICC</a></code>
<code><a href="lme4.html#topic+lmer">lmer</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# one grouping variable
x &lt;- icc_all_by(data = psych::bfi, vrb.nm = c("A2","A3","A4","A5"),
   grp.nm = "gender")

# two grouping variables
y &lt;- icc_all_by(data = psych::bfi, vrb.nm = c("A2","A3","A4","A5"),
   grp.nm = c("gender","education"))

# with errors
z &lt;- icc_all_by(data = psych::bfi, vrb.nm = c("A2","A3","A4","A5"),
   grp.nm = c("age")) # NA for all ICC columns when there is an error

</code></pre>

<hr>
<h2 id='iccs_11'>Intraclass Correlation for Multiple Variables for Multilevel Analysis:
ICC(1,1)</h2><span id='topic+iccs_11'></span>

<h3>Description</h3>

<p><code>iccs_11</code> computes the intraclass correlation (ICC) for multiple
variables based on a single rater with a single dimension, aka ICC(1,1).
Traditionally, this is the type of ICC used for multilevel analysis where the
value is interpreted as the proportion of variance accounted for by group
membership. In other words, ICC(1,1) = the proportion of between-group
variance; 1 - ICC(1,1) = the proportion of within-group variance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>iccs_11(data, vrb.nm, grp.nm, how = "lme", REML = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="iccs_11_+3A_data">data</code></td>
<td>
<p>data.frame of data.</p>
</td></tr>
<tr><td><code id="iccs_11_+3A_vrb.nm">vrb.nm</code></td>
<td>
<p>character vector of colnames from <code>data</code> specifying the
variable columns.</p>
</td></tr>
<tr><td><code id="iccs_11_+3A_grp.nm">grp.nm</code></td>
<td>
<p>character vector of length 1 of a colname from <code>data</code>
specifying the grouping column.</p>
</td></tr>
<tr><td><code id="iccs_11_+3A_how">how</code></td>
<td>
<p>character vector of length 1 specifying how the ICC(1,1) should be
calculated. There are four options: 1) &quot;lme&quot; uses a linear mixed effects
model with the function <code><a href="nlme.html#topic+lme">lme</a></code> from the package
<code>nlme</code>, 2) &quot;lmer&quot; uses a linear mixed effects modeling with the
function <code><a href="lme4.html#topic+lmer">lmer</a></code> from the package <code>lme4</code>, 3) &quot;aov&quot;
uses a one-way analysis of variance with the function
<code><a href="stats.html#topic+aov">aov</a></code>, and 4) &quot;raw&quot; uses the observed variances, which
provides a biased estimate of the ICC(1,1) and is not recommended (It is
only included for teaching purposes).</p>
</td></tr>
<tr><td><code id="iccs_11_+3A_reml">REML</code></td>
<td>
<p>logical vector of length 1 specifying whether restricted maximum
likelihood estimation (TRUE) should be used rather than traditional maximum
likelihood (FALSE). This is only applicable to linear mixed effects models
when <code>how</code> is &quot;lme&quot; or &quot;lmer&quot;.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>double vector containing ICC(1, 1) of the <code>vrb.nm</code> columns in
<code>data</code> with names of the return object equal to <code>vrb.nm</code>.
</p>


<h3>See Also</h3>

<p><code>icc_11</code> # ICC(1,1) for a single variable,
<code><a href="#topic+icc_all_by">icc_all_by</a></code> # all six types of ICCs by group,
<code><a href="nlme.html#topic+lme">lme</a></code> # how = &quot;lme&quot; function,
<code><a href="lme4.html#topic+lmer">lmer</a></code> # how = &quot;lmer&quot; function,
<code><a href="stats.html#topic+aov">aov</a></code> # how = &quot;aov&quot; function,
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
tmp_nm &lt;- c("outcome","case","session","trt_time")
dat &lt;- as.data.frame(lmeInfo::Bryant2016)[tmp_nm]
stats_by &lt;- psych::statsBy(dat,
   group = "case") # requires you to include "case" column in dat
iccs_11(data = dat, vrb.nm = c("outcome","session","trt_time"), grp.nm = "case")

</code></pre>

<hr>
<h2 id='length_by'>Length of a (Atomic) Vector by Group</h2><span id='topic+length_by'></span>

<h3>Description</h3>

<p><code>length_by</code> computes the the length of a (atomic) vector by group. The
argument <code>na.rm</code> can be used to include (FALSE) or exclude (TRUE)
missing values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>length_by(x, grp, na.rm = FALSE, sep = ".")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="length_by_+3A_x">x</code></td>
<td>
<p>atomic vector.</p>
</td></tr>
<tr><td><code id="length_by_+3A_grp">grp</code></td>
<td>
<p>atomic vector or list of atomic vectors (e.g., data.frame)
specifying the groups. The atomic vector(s) must be the length of x or else
an error is returned.</p>
</td></tr>
<tr><td><code id="length_by_+3A_na.rm">na.rm</code></td>
<td>
<p>logical vector of length 1 specifying whether to include (FALSE)
or exclude (TRUE) missing values.</p>
</td></tr>
<tr><td><code id="length_by_+3A_sep">sep</code></td>
<td>
<p>character vector of length 1 specifying what string should
separate different group values when naming the return object. This
argument is only used if grp is a list of atomic vectors (e.g.,
data.frame).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>integer vector of length = <code>length(levels(interaction(grp)))</code>
with names = <code>length(levels(interaction(grp)))</code> providing the number
of elements (excluding missing values if <code>na.rm</code> = TRUE) in each
group.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+lengths_by">lengths_by</a></code>
<code><a href="base.html#topic+length">length</a></code>
<code><a href="#topic+agg">agg</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
length_by(x = mtcars$"mpg", grp = mtcars$"gear")
length_by(x = airquality$"Ozone", grp = airquality$"Month", na.rm = FALSE)
length_by(x = airquality$"Ozone", grp = airquality$"Month", na.rm = TRUE)

</code></pre>

<hr>
<h2 id='lengths_by'>Length of Data Columns by Group</h2><span id='topic+lengths_by'></span>

<h3>Description</h3>

<p><code>lengths_by</code> computes the the length of multiple columns in a data.frame
by group. The argument <code>na.rm</code> can be used to include (FALSE) or exclude
(TRUE) missing values. Through the use of <code>na.rm</code> = TRUE, the number of
observed values for each variable by each group can be computed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lengths_by(data, vrb.nm, grp.nm, na.rm = FALSE, sep = ".")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lengths_by_+3A_data">data</code></td>
<td>
<p>data.frame of data.</p>
</td></tr>
<tr><td><code id="lengths_by_+3A_vrb.nm">vrb.nm</code></td>
<td>
<p>character vector of colnames from <code>data</code> specifying the
variables.</p>
</td></tr>
<tr><td><code id="lengths_by_+3A_grp.nm">grp.nm</code></td>
<td>
<p>character vector of colnames from <code>data</code> specifying the
groups.</p>
</td></tr>
<tr><td><code id="lengths_by_+3A_na.rm">na.rm</code></td>
<td>
<p>logical vector of length 1 specifying whether to include (FALSE)
or exclude (TRUE) missing values.</p>
</td></tr>
<tr><td><code id="lengths_by_+3A_sep">sep</code></td>
<td>
<p>character vector of length 1 specifying what string should
separate different group values when naming the return object. This
argument is only used if grp is a list of atomic vectors (e.g.,
data.frame).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data.frame with colnames = <code>vrb.nm</code> and rownames =
<code>length(levels(interaction(grp)))</code> providing the number of elements
(excluding missing values if <code>na.rm</code> = TRUE) in each column by group.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+length_by">length_by</a></code>
<code><a href="base.html#topic+length">length</a></code>
<code><a href="#topic+colNA">colNA</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
lengths_by(mtcars, vrb.nm = c("mpg","cyl","disp"), grp = "gear")
lengths_by(mtcars, vrb.nm = c("mpg","cyl","disp"),
   grp = c("gear","vs")) # can handle multiple grouping variables
lengths_by(mtcars, vrb.nm = c("mpg","cyl","disp"),
   grp = c("gear","am")) # can handle zero lengths
lengths_by(airquality, c("Ozone","Solar.R","Wind"), grp = "Month",
   na.rm = FALSE) # include missing values
lengths_by(airquality, c("Ozone","Solar.R","Wind"), grp = "Month",
   na.rm = TRUE) # exclude missing values

</code></pre>

<hr>
<h2 id='long2wide'>Reshape Multiple Scores From Long to Wide</h2><span id='topic+long2wide'></span>

<h3>Description</h3>

<p><code>long2wide</code> reshapes data from long to wide. This if often necessary to
do with multilevel data where variables in the long format seek to be
reshaped to multiple sets of variables in the wide format. If only one column
needs to be reshaped, then you can use <code><a href="str2str.html#topic+unstack2">unstack2</a></code> or
<code><a href="reshape.html#topic+cast">cast</a></code> - but that does not work for *multiple* columns.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>long2wide(
  data,
  vrb.nm,
  grp.nm,
  obs.nm,
  sep = ".",
  colnames.by.obs = TRUE,
  keep.attr = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="long2wide_+3A_data">data</code></td>
<td>
<p>data.frame of data.</p>
</td></tr>
<tr><td><code id="long2wide_+3A_vrb.nm">vrb.nm</code></td>
<td>
<p>character vector of colnames from <code>data</code> specifying the
variables to be reshaped. In longitudinal panel data, this would be the
scores.</p>
</td></tr>
<tr><td><code id="long2wide_+3A_grp.nm">grp.nm</code></td>
<td>
<p>character vector of colnames from <code>data</code> specifying the
groups. In longitudnal panel data, this would be the participant ID
variable.</p>
</td></tr>
<tr><td><code id="long2wide_+3A_obs.nm">obs.nm</code></td>
<td>
<p>character vector of length 1 with a colname from <code>data</code>
specifying the observation within each group. In longitudinal panel data,
this would be the time variable.</p>
</td></tr>
<tr><td><code id="long2wide_+3A_sep">sep</code></td>
<td>
<p>character vector of length 1 specifying the string that separates
the name prefix (e.g., score) from it's number suffix (e.g., timepoint). If
<code>sep</code> = &quot;&quot;, then that implies there is no string separating the name
prefix and the number suffix (e.g., &quot;outcome1&quot;).</p>
</td></tr>
<tr><td><code id="long2wide_+3A_colnames.by.obs">colnames.by.obs</code></td>
<td>
<p>logical vector of length 1 specifying whether to sort
the return object colnames by the observation label (TRUE) or by the order
of <code>vrb.nm</code>. See the example at the end of the &quot;MULTIPLE GROUPING
VARIABLES&quot; section of the examples.</p>
</td></tr>
<tr><td><code id="long2wide_+3A_keep.attr">keep.attr</code></td>
<td>
<p>logical vector of length 1 specifying whether to keep the
&quot;reshapeWide&quot; attribute (from <code>reshape</code>) in the return object.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>long2wide</code> uses <code>reshape(direction = "wide")</code> to reshape the data.
It attempts to streamline the task of reshaping long to wide as the
<code>reshape</code> arguments can be confusing because the same arguments are used
for wide vs. long reshaping. See <code><a href="stats.html#topic+reshape">reshape</a></code> if you are
curious.
</p>


<h3>Value</h3>

<p>data.frame with nrow equal to <code>nrow(unique(data[grp.nm]))</code> and
number of reshaped columns equal to <code>length(vrb.nm) *
  unique(data[[obs.nm]])</code>. The colnames will have the structure
<code>paste0(vrb.nm, sep, unique(data[[obs.nm]]))</code>. The reshaped colnames
are sorted by the observation labels if <code>colnames.by.obs</code> = TRUE and
sorted by <code>vrb.nm</code> if <code>colnames.by.obs</code> = FALSE. Overall, the
columns are in the following order: 1) <code>grp.nm</code> of the groups, 2)
reshaped columns, 3) additional columns that were not reshaped.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+wide2long">wide2long</a></code>
<code><a href="stats.html#topic+reshape">reshape</a></code>
<code><a href="str2str.html#topic+unstack2">unstack2</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# SINGLE GROUPING VARIABLE
dat_long &lt;- as.data.frame(ChickWeight) # b/c groupedData class does weird things...
w1 &lt;- long2wide(data = dat_long, vrb.nm = "weight", grp.nm = "Chick",
   obs.nm = "Time") # NAs inserted for missing observations in some groups
w2 &lt;- long2wide(data = dat_long, vrb.nm = "weight", grp.nm = "Chick",
   obs.nm = "Time", sep = "_")
head(w1); head(w2)
w3 &lt;- long2wide(data = dat_long, vrb.nm = "weight", grp.nm = "Chick",
   obs.nm = "Time", sep = "_T", keep.attr = TRUE)
attributes(w3)

# MULTIPLE GROUPING VARIABLE
tmp &lt;- psychTools::sai
grps &lt;- interaction(tmp[1:3], drop = TRUE)
dups &lt;- duplicated(grps)
dat_long &lt;- tmp[!(dups), ] # for some reason there are duplicate groups in the data
vrb_nm &lt;- str2str::pick(names(dat_long), val = c("study","time","id"), not = TRUE)
w4 &lt;- long2wide(data = dat_long, vrb.nm = vrb_nm, grp.nm = c("study","id"),
   obs.nm = "time")
w5 &lt;- long2wide(data = dat_long, vrb.nm = vrb_nm, grp.nm = c("study","id"),
   obs.nm = "time", colnames.by.obs = FALSE) # colnames sorted by `vrb.nm` instead
head(w4); head(w5)

</code></pre>

<hr>
<h2 id='make.dummy'>Make Dummy Columns</h2><span id='topic+make.dummy'></span>

<h3>Description</h3>

<p><code>make.dummy</code> creates dummy columns (i.e., dichotomous numeric vectors
coded 0 and 1) from logical conditions. If you want to make logical
conditions from columns of a data.frame, you will need to call the data.frame
and its columns explicitly as this function does not use non-standard
evaluation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>make.dummy(..., rtn.lgl = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="make.dummy_+3A_...">...</code></td>
<td>
<p>logical conditions that evaluate to logical vectors of the same
length. If the logical vectors are not the same length, an error is
returned. The names of the arguments are the colnames in the return object.
If unnamed, then default R data.frame naming is used, which can get ugly.</p>
</td></tr>
<tr><td><code id="make.dummy_+3A_rtn.lgl">rtn.lgl</code></td>
<td>
<p>logical vector of length 1 specifying whether the dummy
columns should be logical vectors (TRUE) rather than numeric vectors
(FALSE).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data.frame of dummy columns based on the logical conditions n
<code>...</code>. If <code>rtn.lgl</code> = TRUE, then the columns are logical vectors.
If <code>out.lgl</code> = FALSE, then the columns are numeric vectors where 0 =
FALSE and 1 = TRUE. The colnames are the names of the arguments in
<code>...</code>. If not specified, then default data.frame names are created
from the logical conditions themselves (which can get ugly).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+make.dumNA">make.dumNA</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>make.dummy(attitude$"rating" &gt; 50) # ugly colnames
make.dummy("rating_50plus" = attitude$"rating" &gt; 50,
   "advance_50minus" = attitude$"advance" &lt; 50)
make.dummy("rating_50plus" = attitude$"rating" &gt; 50,
   "advance_50minus" = attitude$"advance" &lt; 50, rtn.lgl = TRUE)
## Not run: 
   make.dummy("rating_50plus" = attitude$"rating" &gt; 50,
      "mpg_20plus" = mtcars$"mpg" &gt; 20)

## End(Not run)
</code></pre>

<hr>
<h2 id='make.dumNA'>Make Dummy Columns For Missing Data.</h2><span id='topic+make.dumNA'></span>

<h3>Description</h3>

<p><code>make.dumNA</code> makes dummy columns (i.e., dichomotous numeric vectors
coded 0 and 1) for missing data. Each variable is treated in isolation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>make.dumNA(data, vrb.nm, ov = FALSE, rtn.lgl = FALSE, suffix = "_m")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="make.dumNA_+3A_data">data</code></td>
<td>
<p>data.frame of data.</p>
</td></tr>
<tr><td><code id="make.dumNA_+3A_vrb.nm">vrb.nm</code></td>
<td>
<p>character vector of colnames from <code>data</code> specifying the
variables.</p>
</td></tr>
<tr><td><code id="make.dumNA_+3A_ov">ov</code></td>
<td>
<p>logical vector of length 1 specifying whether the dummy columns
should be reverse coded such that missing values = 0/FALSE and observed
values = 1/TRUE.</p>
</td></tr>
<tr><td><code id="make.dumNA_+3A_rtn.lgl">rtn.lgl</code></td>
<td>
<p>logical vector of length 1 specifying whether the dummy columns
should be logical vectors (TRUE) rather than numeric vectors (FALSE).</p>
</td></tr>
<tr><td><code id="make.dumNA_+3A_suffix">suffix</code></td>
<td>
<p>character vector of length 1 specifying the string that should
be appended to the end of the colnames in the return object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data.frame of numeric (logical if <code>rtn.lgl</code> = TRUE) columns
where missing = 1 and observed = 0 (flipped if <code>ov</code> = TRUE) for each
variable. The colnames are created by <code>paste0(vrb.nm, suffix)</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+make.dummy">make.dummy</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>make.dumNA(data = airquality, vrb.nm = c("Ozone","Solar.R"))
make.dumNA(data = airquality, vrb.nm = c("Ozone","Solar.R"),
   rtn.lgl = TRUE) # logical vectors returned
make.dumNA(data = airquality, vrb.nm = c("Ozone","Solar.R"),
   ov = TRUE, suffix = "_o") # 1 = observed value
</code></pre>

<hr>
<h2 id='make.fun_if'>Make a Function Conditional on Frequency of Observed Values</h2><span id='topic+make.fun_if'></span>

<h3>Description</h3>

<p><code>make.fun_if</code> makes a function that evaluates conditional on a specified
minimum frequency of observed values. Within the function, if the frequency
of observed values is less than (or equal to) <code>ov.min</code>, then
<code>false</code> is returned rather than the return value.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>make.fun_if(
  fun,
  ...,
  ov.min.default = 1,
  prop.default = TRUE,
  inclusive.default = TRUE,
  false = NA
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="make.fun_if_+3A_fun">fun</code></td>
<td>
<p>function that takes an atomic vector as its first argument. The
first argument does not have to be named &quot;x&quot; within <code>fun</code>, but it will
be named &quot;x&quot; in the returned function.</p>
</td></tr>
<tr><td><code id="make.fun_if_+3A_...">...</code></td>
<td>
<p>additional arguments with parameters to <code>fun</code>. This would be
similar to <code>impute</code> in <code>sum_if</code>. However in the current version
of <code>make.fun_if</code>, the parameters you provide will always be used
within the returned function and cannot be specified by the user of the
returned function. Unfortunately, I cannot figure out how to include
user-specified arguments (with defaults) within the returned function other
than <code>ov.min.default</code>, <code>prop.default</code>, and
<code>inclusive.default</code>.</p>
</td></tr>
<tr><td><code id="make.fun_if_+3A_ov.min.default">ov.min.default</code></td>
<td>
<p>numeric vector of length 1 specifying what the default
should be for the argument <code>ov.min</code> within the returned function,
which specifies the minimum frequency of observed values required. If
<code>prop</code> = TRUE, then this is a decimal between 0 and 1. If <code>prop</code>
= FALSE, then this is a integer between 0 and <code>length(x)</code>.</p>
</td></tr>
<tr><td><code id="make.fun_if_+3A_prop.default">prop.default</code></td>
<td>
<p>logical vector of length 1 specifying what the default
should be for the argument <code>prop</code> within the returned function, which
specifies whether <code>ov.min</code> should refer to the proportion of observed
values (TRUE) or the count of observed values (FALSE).</p>
</td></tr>
<tr><td><code id="make.fun_if_+3A_inclusive.default">inclusive.default</code></td>
<td>
<p>logical vector of length 1 speicfying what the
default should be for the argument <code>inclusive</code> within the returned
function, which specifies whether the function should be evaluated if the
frequency of observed values is exactly equal to <code>ov.min</code>.</p>
</td></tr>
<tr><td><code id="make.fun_if_+3A_false">false</code></td>
<td>
<p>vector of length 1 specifying what should be returned if the
observed values condition is not met within the returned function. The
default is NA. Whatever the value is, it will be coerced to the same mode
as <code>x</code> within the returned function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>function that takes an atomic vector <code>x</code> as its first argument,
<code>...</code> as other arguments, ending with <code>ov.min</code>, <code>prop</code>, and
<code>inclusive</code> as final arguments with defaults specified by
<code>ov.min.default</code>, <code>prop.default</code>, and <code>inclusive.default</code>,
respectively.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+sum_if">sum_if</a></code>
<code><a href="#topic+mean_if">mean_if</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# SD
sd_if &lt;- make.fun_if(fun = sd, na.rm = TRUE) # always have na.rm = TRUE
sd_if(x = airquality[[1]], ov.min = .75) # proportion of observed values
sd_if(x = airquality[[1]], ov.min = 116,
   prop = FALSE) # count of observed values
sd_if(x = airquality[[1]], ov.min = 116, prop = FALSE,
   inclusive = FALSE) # not include ov.min values itself

# skewness
skew_if &lt;- make.fun_if(fun = psych::skew, type = 1) # always have type = 1
skew_if(x = airquality[[1]], ov.min = .75) # proportion of observed values
skew_if(x = airquality[[1]], ov.min = 116,
   prop = FALSE) # count of observed values
skew_if(x = airquality[[1]], ov.min = 116, prop = FALSE,
   inclusive = FALSE) # not include ov.min values itself

# mode
popular &lt;- function(x) names(sort(table(x), decreasing = TRUE))[1]
popular_if &lt;- make.fun_if(fun = popular) # works with character vectors too
popular_if(x = c(unlist(dimnames(HairEyeColor)), rep.int(x = NA, times = 10)),
   ov.min = .50)
popular_if(x = c(unlist(dimnames(HairEyeColor)), rep.int(x = NA, times = 10)),
   ov.min = .60)
</code></pre>

<hr>
<h2 id='make.latent'>Make Model Syntax for a Latent Factor in Lavaan</h2><span id='topic+make.latent'></span>

<h3>Description</h3>

<p><code>make.latent</code> makes the model syntax for a latent factor in
<code>lavaan</code>. The return object can be used as apart of the model syntax for
calls to <code><a href="lavaan.html#topic+lavaan">lavaan</a></code>, <code><a href="lavaan.html#topic+sem">sem</a></code>,
<code><a href="lavaan.html#topic+cfa">cfa</a></code>, etc.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>make.latent(
  x,
  nm.latent = "latent",
  error.var = FALSE,
  nm.par = FALSE,
  suffix.load = "_l",
  suffix.error = "_e"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="make.latent_+3A_x">x</code></td>
<td>
<p>character vector specifying the colnames in your data that
correspond to the variables indicating the latent factor (e.g.,
questionnaire items).</p>
</td></tr>
<tr><td><code id="make.latent_+3A_nm.latent">nm.latent</code></td>
<td>
<p>character vector of length 1 specifying what the latent
factor should be labeled as in the return object.</p>
</td></tr>
<tr><td><code id="make.latent_+3A_error.var">error.var</code></td>
<td>
<p>logical vector of length 1 specifying whether the model
syntax for the error variances should be included in the return object.</p>
</td></tr>
<tr><td><code id="make.latent_+3A_nm.par">nm.par</code></td>
<td>
<p>logical vector of length 1 specifying whether the model syntax
should include names for the factor loading (and error variance)
parameters.</p>
</td></tr>
<tr><td><code id="make.latent_+3A_suffix.load">suffix.load</code></td>
<td>
<p>character vector of length 1 specifying what string should
be appended to the end of the elements of <code>x</code> when creating names for
the factor loading parameters. Only used if <code>nm.par</code> is TRUE.</p>
</td></tr>
<tr><td><code id="make.latent_+3A_suffix.error">suffix.error</code></td>
<td>
<p>character vector of length 1 specifying what string
should be appended to the end of the elements of <code>x</code> when creating
names for the error variance parameters. Only used if <code>nm.par</code> is
TRUE.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>character vector of length 1 providing the model syntax. The regular
expression &quot;\n&quot; is used to delineate new lines within the model
syntax.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
make.latent(x = names(psych::bfi)[1:5], error.var = FALSE, nm.par = FALSE)
make.latent(x = names(psych::bfi)[1:5], error.var = FALSE, nm.par = TRUE)
make.latent(x = names(psych::bfi)[1:5], error.var = TRUE, nm.par = FALSE)
make.latent(x = names(psych::bfi)[1:5], error.var = TRUE, nm.par = TRUE)

</code></pre>

<hr>
<h2 id='make.product'>Make Product Terms (e.g., interactions)</h2><span id='topic+make.product'></span>

<h3>Description</h3>

<p><code>make.product</code> creates product terms (i.e., interactions) from various
components. <code>make.product</code> uses <code>Center</code> for the optional of
centering and/or scaling the predictors and/or moderators before making the
product terms.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>make.product(
  data,
  x.nm,
  m.nm,
  center.x = FALSE,
  center.m = FALSE,
  scale.x = FALSE,
  scale.m = FALSE,
  suffix.x = "",
  suffix.m = "",
  sep = ":",
  combo = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="make.product_+3A_data">data</code></td>
<td>
<p>data.frame of data.</p>
</td></tr>
<tr><td><code id="make.product_+3A_x.nm">x.nm</code></td>
<td>
<p>character vector of colnames from <code>data</code> specifying the
predictor columns.</p>
</td></tr>
<tr><td><code id="make.product_+3A_m.nm">m.nm</code></td>
<td>
<p>character vector of colnames from <code>data</code> specifying the
moderator columns.</p>
</td></tr>
<tr><td><code id="make.product_+3A_center.x">center.x</code></td>
<td>
<p>logical vector of length 1 specifying whether the predictor
columns should be grand-mean centered before making the product terms.</p>
</td></tr>
<tr><td><code id="make.product_+3A_center.m">center.m</code></td>
<td>
<p>logical vector of length 1 specifying whether the moderator
columns should be grand-mean centered before making the product terms.</p>
</td></tr>
<tr><td><code id="make.product_+3A_scale.x">scale.x</code></td>
<td>
<p>logical vector of length 1 specifying whether the predictor
columns should be grand-SD scaled before making the product terms.</p>
</td></tr>
<tr><td><code id="make.product_+3A_scale.m">scale.m</code></td>
<td>
<p>logical vector of length 1 specifying whether the moderator
columns should be grand-SD scaled before making the product terms.</p>
</td></tr>
<tr><td><code id="make.product_+3A_suffix.x">suffix.x</code></td>
<td>
<p>character vector of length 1 specifying any suffix to add to
the end of the predictor colnames <code>x.nm</code> when creating the colnames of
the return object.</p>
</td></tr>
<tr><td><code id="make.product_+3A_suffix.m">suffix.m</code></td>
<td>
<p>character vector of length 1 specifying any suffix to add to
the end of the moderator colnames <code>m.nm</code> when creating the colnames of
the return object.</p>
</td></tr>
<tr><td><code id="make.product_+3A_sep">sep</code></td>
<td>
<p>character vector of length 1 specifying the string to connect
<code>x.nm</code> and <code>m.nm</code> when specifying the colnames of the return
object.</p>
</td></tr>
<tr><td><code id="make.product_+3A_combo">combo</code></td>
<td>
<p>logical vector of length 1 specifying whether all combinations
of the predictors and moderators should be calculated or only those in
parallel to each other (i.e., <code>x.nm[i]</code> and <code>m.nm[i]</code>). This
argument is only applicable when multiple predictors AND multiple
moderators are given.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data.frame with product terms (e.g., interactions) as columns. The
colnames are created by <code>paste(paste0(x.nm, suffix.x), paste0(m.nm,
  suffix.m), sep = sep)</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>make.product(data = attitude, x.nm = c("complaints","privileges"),
   m.nm = "learning", center.x = TRUE, center.m = TRUE,
   suffix.x = "_c", suffix.m = "_c") # with grand-mean centering
make.product(data = attitude, x.nm = c("complaints","privileges"),
   m.nm = c("learning","raises"), combo = TRUE) # all possible combinations
make.product(data = attitude, x.nm = c("complaints","privileges"),
   m.nm = c("learning","raises"), combo = FALSE) # only combinations "in parallel"
</code></pre>

<hr>
<h2 id='mean_change'>Mean Change Across Two Timepoints (dependent two-samples t-test)</h2><span id='topic+mean_change'></span>

<h3>Description</h3>

<p><code>mean_change</code> tests for mean change across two timepoints with a
dependent two-samples t-test. The function also calculates the descriptive
statistics for the timepoints and the standardized mean difference (i.e.,
Cohen's d) based on either the standard deviation of the pre-timepoint,
pooled standard deviation of the pre-timepoint and post-timepoint, or the
standard deviation of the change score (post - pre). <code>mean_change</code> is
simply a wrapper for <code><a href="stats.html#topic+t.test">t.test</a></code> plus some extra
calculations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mean_change(
  pre,
  post,
  standardizer = "pre",
  d.ci.type = "unbiased",
  ci.level = 0.95,
  check = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mean_change_+3A_pre">pre</code></td>
<td>
<p>numeric vector of the variable at the pre-timepoint.</p>
</td></tr>
<tr><td><code id="mean_change_+3A_post">post</code></td>
<td>
<p>numeric vector of the variable at the post-timepoint. The
elements must correspond to the same cases in <code>pre</code> as pairs by
position. Thus, the length of <code>post</code> must be the same as <code>pre</code>.
Note, missing values in <code>post</code> are expected and handled with listwise
deletion.</p>
</td></tr>
<tr><td><code id="mean_change_+3A_standardizer">standardizer</code></td>
<td>
<p>chararacter vector of length 1 specifying what to use for
standardization when computing the standardized mean difference (i.e.,
Cohen's d). There are three options: 1. &quot;pre&quot; for the standard deviation of
the pre-timepoint, 2. &quot;pooled&quot; for the pooled standard deviation of the
pre-timepoint and post-timepoint, 3. &quot;change&quot; for the standard deviation of
the change score (post - pre). The default is &quot;pre&quot;, which I believe makes
the most theoretical sense (see Cumming, 2012); however, &quot;change&quot; is the
traditional choice originally proposed by Jacob Cohen (Cohen, 1988).</p>
</td></tr>
<tr><td><code id="mean_change_+3A_d.ci.type">d.ci.type</code></td>
<td>
<p>character vector of lenth 1 specifying how to compute the
confidence interval (and standard error) of the standardized mean
difference. There are currently two options: 1. &quot;unbiased&quot; which calculates
the unbiased standard error of Cohen's d based on the formulas in
Viechtbauer (2007). If <code>standardizer</code> = &quot;pre&quot; or &quot;pooled&quot;, then
equation 36 from Table 2 is used. If <code>standardizer</code> = &quot;change&quot;, then
equation 25 from Table 1 is used. A symmetrical confidence interval is then
calculated based on the standard error. 2. &quot;classic&quot; which calculates the
confidence interval of Cohen's d based on the confidence interval of the
mean change itself. The lower and upper confidence bounds are divided by
the <code>standardizer</code>. Technically, this confidence interval is biased
due to not taking into account the uncertainty of the <code>standardizer</code>.
No standard error is calculated for this option and NA is returned for
&quot;d_se&quot; in the return object.</p>
</td></tr>
<tr><td><code id="mean_change_+3A_ci.level">ci.level</code></td>
<td>
<p>double vector of length 1 specifying the confidence level.
<code>ci.level</code> must range from 0 to 1.</p>
</td></tr>
<tr><td><code id="mean_change_+3A_check">check</code></td>
<td>
<p>logical vector of length 1 specifying whether the input
arguments should be checked for errors. For example, checking whether
<code>post</code> is the same length as <code>pre</code>. This is a tradeoff between
computational efficiency (FALSE) and more useful error messages (TRUE).</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>mean_change</code> calculates the mean change as <code>post</code> - <code>pre</code>
such that increases over time have a positive mean change estimate and
decreases over time have a negative mean change estimate. This would be as if
the post-timepoint was <code>x</code> and the pre-timepoint was <code>y</code> in
<code>t.test(paired = TRUE)</code>.
</p>


<h3>Value</h3>

<p>list of numeric vectors containing statistical information about the
mean change: 1) nhst = dependent two-samples t-test stat info in a numeric vector,
2) desc = descriptive statistics stat info in a numeric vector, 3) std =
standardized mean difference stat info in a numeric vector
</p>
<p>1) nhst = dependent two-samples t-test stat info in a numeric vector
</p>

<dl>
<dt>est</dt><dd><p>mean change estimate (i.e., post - pre)</p>
</dd>
<dt>se</dt><dd><p>standard error</p>
</dd>
<dt>t</dt><dd><p>t-value</p>
</dd>
<dt>df</dt><dd><p>degrees of freedom</p>
</dd>
<dt>p</dt><dd><p>two-sided p-value</p>
</dd>
<dt>lwr</dt><dd><p>lower bound of the confidence interval</p>
</dd>
<dt>upr</dt><dd><p>upper bound of the confidence interval</p>
</dd>
</dl>

<p>2) desc = descriptive statistics stat info in a numeric vector
</p>

<dl>
<dt>mean_post</dt><dd><p>mean of the post variable</p>
</dd>
<dt>mean_pre</dt><dd><p>mean of the pre variable</p>
</dd>
<dt>sd_post</dt><dd><p>standard deviation of of the post variable</p>
</dd>
<dt>sd_pre</dt><dd><p>standard deviation of the pre variable</p>
</dd>
<dt>n</dt><dd><p>sample size of the change score</p>
</dd>
<dt>r</dt><dd><p>Pearson correlation between the pre and post variables</p>
</dd>
</dl>

<p>3) std = standardized mean difference stat info in a numeric vector
</p>

<dl>
<dt>d_est</dt><dd><p>Cohen's d estimate</p>
</dd>
<dt>d_se</dt><dd><p>Cohen's d standard error</p>
</dd>
<dt>d_lwr</dt><dd><p>Cohen's d lower bound of the confidence interval</p>
</dd>
<dt>d_upr</dt><dd><p>Cohen's d upper bound of the confidence interval</p>
</dd>
</dl>



<h3>References</h3>

<p>Cohen, J. (1988). Statistical power analysis for the behavioral sciences,
2nd ed. Hillsdale, NJ: Erlbaum.
</p>
<p>Cumming, G. (2012). Understanding the new statistics: Effect sizes,
confidence intervals, and meta-analysis. New York, NY: Rouledge.
</p>
<p>Viechtbauer, W. (2007). Approximate confidence intervals for standardized
effect sizes in the two-independent and two-dependent samples design.
Journal of Educational and Behavioral Statistics, 32(1), 39-60.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+means_change">means_change</a></code> for multiple sets of prepost pairs of variables,
<code><a href="stats.html#topic+t.test">t.test</a></code> the workhorse for <code>mean_change</code>,
<code><a href="#topic+mean_diff">mean_diff</a></code> for a independent two-samples t-test,
<code><a href="#topic+mean_test">mean_test</a></code> for a one-sample t-test,
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# dependent two-sample t-test
mean_change(pre = mtcars$"disp", post = mtcars$"hp") # standardizer = "pre"
mean_change(pre = mtcars$"disp", post = mtcars$"hp", d.ci.type = "classic")
mean_change(pre = mtcars$"disp", post = mtcars$"hp", standardizer = "pooled")
mean_change(pre = mtcars$"disp", post = mtcars$"hp", ci.level = 0.99)
mean_change(pre = mtcars$"hp", post = mtcars$"disp",
   ci.level = 0.99) # note, when flipping pre and post, the cohen's d estimate
   # changes with standardizer = "pre" because the "pre" variable is different.
   # This does not happen for standardizer = "pooled" or "change". For example...
mean_change(pre = mtcars$"disp", post = mtcars$"hp", standardizer = "pooled")
mean_change(pre = mtcars$"hp", post = mtcars$"disp", standardizer = "pooled")
mean_change(pre = mtcars$"disp", post = mtcars$"hp", standardizer = "change")
mean_change(pre = mtcars$"hp", post = mtcars$"disp", standardizer = "change")

# same as intercept-only regression with the change score
mean_change(pre = mtcars$"disp", post = mtcars$"hp")
lm_obj &lt;- lm(hp - disp ~ 1, data = mtcars)
coef(summary(lm_obj))

</code></pre>

<hr>
<h2 id='mean_compare'>Mean differences for a single variable across 3+ independent groups (one-way
ANOVA)</h2><span id='topic+mean_compare'></span>

<h3>Description</h3>

<p><code>mean_compare</code> compares means across 3+ independent groups with a
one-way ANOVA. The function also calculates the descriptive statistics for
each group and the variance explained (i.e., R^2 aka eta^2) by the nominal
grouping variable. <code>mean_compare</code> is simply a wrapper for
<code><a href="stats.html#topic+oneway.test">oneway.test</a></code> plus some extra calculations.
<code>mean_compare</code> will work with 2 independent groups; however it arguably
makes more sense to use <code><a href="#topic+mean_diff">mean_diff</a></code> in that case.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mean_compare(
  x,
  nom,
  lvl = levels(as.factor(nom)),
  var.equal = TRUE,
  r2.ci.type = "Fdist",
  ci.level = 0.95,
  rtn.table = TRUE,
  check = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mean_compare_+3A_x">x</code></td>
<td>
<p>numeric vector.</p>
</td></tr>
<tr><td><code id="mean_compare_+3A_nom">nom</code></td>
<td>
<p>atomic vector (e.g., factor) the same length as <code>x</code> that is a
nominal variable. It identifies the 3+ groups with 3+ unique values (other
than missing values).</p>
</td></tr>
<tr><td><code id="mean_compare_+3A_lvl">lvl</code></td>
<td>
<p>character vector with length 3+ specifying the unique values for
the 3+ groups. If <code>nom</code> is a factor, then <code>lvl</code> should be the
factor levels rather than the underlying integer codes. This argument
allows you to specify the order of the descriptive statistics in the return
object, which will be opposite the order of <code>lvl</code> for consistency with
<code><a href="#topic+mean_diff">mean_diff</a></code> and <code><a href="#topic+mean_change">mean_change</a></code>.</p>
</td></tr>
<tr><td><code id="mean_compare_+3A_var.equal">var.equal</code></td>
<td>
<p>logical vector of length 1 specifying whether the variances
of the groups are assumed to be equal (TRUE) or not (FALSE). If TRUE, a
traditional one-way ANOVA is computed; if FALSE, Welch's ANOVA is computed.
These two tests differ by their denominator degrees of freedom, F-value,
and p-value.</p>
</td></tr>
<tr><td><code id="mean_compare_+3A_r2.ci.type">r2.ci.type</code></td>
<td>
<p>character vector with length 1 specifying the type of
confidence intervals to compute for the variance explained (i.e., R^2 aka
eta^2). There are currently two options: 1) &quot;Fdist&quot; which calculates a
non-symmetrical confidence interval based on the non-central F distribution
(pg. 38, Smithson, 2003), 2) &quot;classic&quot; which calculates the confidence
interval based on a large-sample theory standard error (eq. 3.6.3 in Cohen,
Cohen, West, &amp; Aiken, 2003), which is taken from Olkin &amp; Finn (1995) - just
above eq. 10. The confidence intervals for R^2-adjusted use the same
formula as R^2, but replace R^2 with R^2 adjusted. Technically, the R^2
adjusted confidence intervals can have poor coverage (pg. 54, Smithson,
2003)</p>
</td></tr>
<tr><td><code id="mean_compare_+3A_ci.level">ci.level</code></td>
<td>
<p>numeric vector of length 1 specifying the confidence level.
<code>ci.level</code> must range from 0 to 1.</p>
</td></tr>
<tr><td><code id="mean_compare_+3A_rtn.table">rtn.table</code></td>
<td>
<p>logical vector of length 1 specifying whether the traditional
ANOVA table should be returned as the last element of the return object.</p>
</td></tr>
<tr><td><code id="mean_compare_+3A_check">check</code></td>
<td>
<p>logical vector of length 1 specifying whether the input
arguments should be checked for errors. For example, if <code>nom</code> has
length different than the length of <code>x</code>. This is a tradeoff between
computational efficiency (FALSE) and more useful error messages (TRUE).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list of numeric vectors containing statistical information about the
mean comparison: 1) nhst = one-way ANOVA stat info in a numeric vector,
2) desc = descriptive statistics stat info in a numeric vector,
3) std = standardized effect sizes stat info in a numeric vector,
4) anova = traditional ANOVA table in a numeric matrix (only returned
if rtn.table = TRUE).
</p>
<p>1) nhst = one-way ANOVA stat info in a numeric vector
</p>

<dl>
<dt>diff_avg</dt><dd><p>average mean difference across group pairs</p>
</dd>
<dt>se</dt><dd><p>NA to remind the user there is no standard error for the average mean difference</p>
</dd>
<dt>F</dt><dd><p>F-value</p>
</dd>
<dt>df_num</dt><dd><p>numerator degrees of freedom</p>
</dd>
<dt>df_den</dt><dd><p>denominator degrees of freedom</p>
</dd>
<dt>p</dt><dd><p>two-sided p-value</p>
</dd>
</dl>

<p>2) desc = descriptive statistics stat info in a numeric vector (note there
could be more than 3 groups - groups i, j, and k are just provided as an example)
</p>

<dl>
<dt>mean_'lvl[k]'</dt><dd><p>mean of group k</p>
</dd>
<dt>mean_'lvl[j]'</dt><dd><p>mean of group j</p>
</dd>
<dt>mean_'lvl[i]'</dt><dd><p>mean of group i</p>
</dd>
<dt>sd_'lvl[k]'</dt><dd><p>standard deviation of group k</p>
</dd>
<dt>sd_'lvl[j]'</dt><dd><p>standard deviation of group j</p>
</dd>
<dt>sd_'lvl[i]'</dt><dd><p>standard deviation of group i</p>
</dd>
<dt>n_'lvl[k]'</dt><dd><p>sample size of group k</p>
</dd>
<dt>n_'lvl[j]'</dt><dd><p>sample size of group j</p>
</dd>
<dt>n_'lvl[i]'</dt><dd><p>sample size of group i</p>
</dd>
</dl>

<p>3) std = standardized effect sizes stat info in a numeric vector
</p>

<dl>
<dt>r2_reg_est</dt><dd><p>R^2 estimate</p>
</dd>
<dt>r2_reg_se</dt><dd><p>R^2 standard error (only available if <code>r2.ci.type</code> = &quot;classic&quot;)</p>
</dd>
<dt>r2_reg_lwr</dt><dd><p>R^2 lower bound of the confidence interval</p>
</dd>
<dt>r2_reg_upr</dt><dd><p>R^2 upper bound of the confidence interval</p>
</dd>
<dt>r2_adj_est</dt><dd><p>R^2-adjusted estimate</p>
</dd>
<dt>r2_adj_se</dt><dd><p>R^2-adjusted standard error (only available if <code>r2.ci.type</code> = &quot;classic&quot;)</p>
</dd>
<dt>r2_adj_lwr</dt><dd><p>R^2-adjusted lower bound of the confidence interval</p>
</dd>
<dt>r2_adj_upr</dt><dd><p>R^2-adjusted upper bound of the confidence interval</p>
</dd>
</dl>

<p>4) anova = traditional ANOVA table in a numeric matrix (only returned
if rtn.table = TRUE).
</p>
<p>The dimlabels of the matrix was &quot;effect&quot; for the rows
and &quot;info&quot; for the columns. There are two rows with rownames 1. &quot;nom&quot; and 2.
&quot;Residuals&quot; where &quot;nom&quot; refers to the between-group effect of the nominal
variable and &quot;Residuals&quot; refers to the within-group residual errors. There
are 5 columns with colnames 1. &quot;SS&quot; = sum of squares, 2. &quot;df&quot; = degrees of
freedom, 3. &quot;MS&quot; = mean squares, 4. &quot;F&quot; = F-value. and 5. &quot;p&quot; = p-value. Note
the F-value and p-value will differ from the &quot;nhst&quot; returned vector if
<code>var.equal</code> = FALSE because the traditional ANOVA table always assumes
variances are equal (i.e. <code>var.equal</code> = TRUE).
</p>


<h3>References</h3>

<p>Cohen, J., Cohen, P., West, A. G., &amp; Aiken, L. S. (2003). Applied Multiple
Regression/Correlation Analysis for the Behavioral Science - third edition.
New York, NY: Routledge.
</p>
<p>Olkin, I., &amp; Finn, J. D. (1995). Correlations redux. Psychological Bulletin, 118(1), 155-164.
</p>
<p>Smithson, M. (2003). Confidence intervals. Thousand Oaks, CA: Sage Publications.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+oneway.test">oneway.test</a></code> the workhorse for <code>mean_compare</code>,
<code><a href="#topic+means_compare">means_compare</a></code> for multiple variables across the same 3+ groups,
<code><a href="MBESS.html#topic+ci.R2">ci.R2</a></code> for confidence intervals of the variance explained,
<code><a href="#topic+mean_diff">mean_diff</a></code> for a single variable across only 2 groups,
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
mean_compare(x = mtcars$"mpg", nom = mtcars$"gear")
mean_compare(x = mtcars$"mpg", nom = mtcars$"gear", var.equal = FALSE)
mean_compare(x = mtcars$"mpg", nom = mtcars$"gear", rtn.table = FALSE)
mean_compare(x = mtcars$"mpg", nom = mtcars$"gear", r2.ci.type = "classic")

</code></pre>

<hr>
<h2 id='mean_diff'>Mean difference across two independent groups (independent two-samples
t-test)</h2><span id='topic+mean_diff'></span>

<h3>Description</h3>

<p><code>mean_diff</code> tests for mean differences across two independent groups
with an independent two-samples t-test. The function also calculates the
descriptive statistics for each group and the standardized mean difference
(i.e., Cohen's d) based on the pooled standard deviation. <code>mean_diff</code> is
simply a wrapper for <code><a href="stats.html#topic+t.test">t.test</a></code> plus some extra
calculations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mean_diff(
  x,
  bin,
  lvl = levels(as.factor(bin)),
  var.equal = TRUE,
  d.ci.type = "unbiased",
  ci.level = 0.95,
  check = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mean_diff_+3A_x">x</code></td>
<td>
<p>numeric vector.</p>
</td></tr>
<tr><td><code id="mean_diff_+3A_bin">bin</code></td>
<td>
<p>atomic vector (e.g., factor) the same length as <code>x</code> that is a
binary variable. It identifies the two groups with two (and only two)
unique values (other than missing values).</p>
</td></tr>
<tr><td><code id="mean_diff_+3A_lvl">lvl</code></td>
<td>
<p>character vector with length 2 specifying the unique values for
the two groups. If <code>bin</code> is a factor, then <code>lvl</code> should be the
factor levels rather than the underlying integer codes. This argument
allows you to specify the direction of the mean difference.
<code>mean_diff</code> calculates the mean difference as <code>x[bin == lvl[2] ]</code> -
<code>x[bin == lvl[1] ]</code> such that it is group 2 - group 1. By changing which
group is group 1 vs. group 2, the direction of the mean difference can be
changed. See details.</p>
</td></tr>
<tr><td><code id="mean_diff_+3A_var.equal">var.equal</code></td>
<td>
<p>logical vector of length 1 specifying whether the variances
of the groups are assumed to be equal (TRUE) or not (FALSE). If TRUE, a
traditional independent two-samples t-test is computed; if FALSE, Welch's
t-test is computed. These two tests differ by their degrees of freedom and
p-values.</p>
</td></tr>
<tr><td><code id="mean_diff_+3A_d.ci.type">d.ci.type</code></td>
<td>
<p>character vector with length 1 of specifying the type of
confidence intervals to compute for the standardized mean difference (i.e.,
Cohen's d). There are currently three options: 1) &quot;unbiased&quot; which
calculates the unbiased standard error of Cohen's d based on formula 25 in
Viechtbauer (2007). A symmetrical confidence interval is then calculated
based on the standard error. 2) &quot;tdist&quot; which calculates the confidence
intervals based on the t-distribution using the function
<code><a href="psych.html#topic+cohen.d.ci">cohen.d.ci</a></code>, 3) &quot;classic&quot; which calculates the
confidence interval of Cohen's d based on the confidence interval of the
mean difference itself. The lower and upper confidence bounds are divided
by the pooled standard deviation. Technically, this confidence interval is
biased due to not taking into account the uncertainty of the standard
deviations. No standard error is calculated for this option and NA is
returned for &quot;d_se&quot; in the return object.</p>
</td></tr>
<tr><td><code id="mean_diff_+3A_ci.level">ci.level</code></td>
<td>
<p>numeric vector of length 1 specifying the confidence level.
<code>ci.level</code> must range from 0 to 1.</p>
</td></tr>
<tr><td><code id="mean_diff_+3A_check">check</code></td>
<td>
<p>logical vector of length 1 specifying whether the input
arguments should be checked for errors. For example, if <code>bin</code> has more
than 2 unique values (other than missing values) or if <code>bin</code> has
length different than the length of <code>x</code>. This is a tradeoff between
computational efficiency (FALSE) and more useful error messages (TRUE).</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>mean_diff</code> calculates the mean difference as <code>x[bin == lvl[2] ]</code> -
<code>x[bin == lvl[1] ]</code> such that it is group 2 - group 1. Group 1 corresponds
to the first factor level of <code>bin</code> (after being coerced to a factor).
Group 2 correspond to the second factor level <code>bin</code> (after being coerced
to a factor). This was set up to handle dummy coded treatment variables in a
desirable way. For example, if <code>bin</code> is a numeric vector with values
<code>0</code> and <code>1</code>, the default factor coersion will have the first factor
level be &quot;0&quot; and the second factor level &quot;1&quot;. This would result will
correspond to 1 - 0. However, if the first factor level of <code>bin</code> is
&quot;treatment&quot; and the second factor level is &quot;control&quot;, the result will
correspond to control - treatment. If the opposite is desired (e.g.,
treatment - control), this can be reversed within the function by specifying
the <code>lvl</code> argument as <code>c("control","treatment")</code>. Note,
<code>mean_diff</code> diverts from <code>t.test</code> by calculating the mean
difference as group 2 - group 1 (as opposed to the group 1 - group 2 that
<code>t.test</code> does). However, group 2 - group 1 is the convention that
<code>psych::cohen.d</code> uses as well.
</p>
<p><code>mean_diff</code> calculates the pooled standard deviation in a different way
than <code><a href="psych.html#topic+cohen.d">cohen.d</a></code>. Therefore, the Cohen's d estimates (and
confidence intervals if d.ci.type == &quot;tdist&quot;) differ from those in
<code><a href="psych.html#topic+cohen.d">cohen.d</a></code>. <code>mean_diff</code> uses the total degrees of
freedom in the denomenator while <code><a href="psych.html#topic+cohen.d">cohen.d</a></code> uses the total
sample size in the denomenator - based on the notation in McGrath &amp; Meyer
(2006). However, almost every introduction to statistics textbook uses the
total degrees of freedom in the denomenator and that is what makes more sense
to me. See examples.
</p>


<h3>Value</h3>

<p>list of numeric vectors containing statistical information about the
mean difference: 1) nhst = independent two-samples t-test stat info in a
numeric vector, 2) desc = descriptive statistics stat info in a numeric
vector, 3) std = standardized mean difference stat info in a numeric vector
</p>
<p>1) nhst = independent two-samples t-test stat info in a numeric vector
</p>

<dl>
<dt>est</dt><dd><p>mean difference estimate (i.e., group 2 - group 1)</p>
</dd>
<dt>se</dt><dd><p>standard error</p>
</dd>
<dt>t</dt><dd><p>t-value</p>
</dd>
<dt>df</dt><dd><p>degrees of freedom</p>
</dd>
<dt>p</dt><dd><p>two-sided p-value</p>
</dd>
<dt>lwr</dt><dd><p>lower bound of the confidence interval</p>
</dd>
<dt>upr</dt><dd><p>upper bound of the confidence interval</p>
</dd>
</dl>

<p>2) desc = descriptive statistics stat info in a numeric vector
</p>

<dl>
<dt>mean_'lvl[2]'</dt><dd><p>mean of group 2</p>
</dd>
<dt>mean_'lvl[1]'</dt><dd><p>mean of group 1</p>
</dd>
<dt>sd_'lvl[2]'</dt><dd><p>standard deviation of group 2</p>
</dd>
<dt>sd_'lvl[1]'</dt><dd><p>standard deviation of group 1</p>
</dd>
<dt>n_'lvl[2]'</dt><dd><p>sample size of group 2</p>
</dd>
<dt>n_'lvl[1]'</dt><dd><p>sample size of group 1</p>
</dd>
</dl>

<p>3) std = standardized mean difference stat info in a numeric vector
</p>

<dl>
<dt>d_est</dt><dd><p>Cohen's d estimate</p>
</dd>
<dt>d_se</dt><dd><p>Cohen's d standard error</p>
</dd>
<dt>d_lwr</dt><dd><p>Cohen's d lower bound of the confidence interval</p>
</dd>
<dt>d_upr</dt><dd><p>Cohen's d upper bound of the confidence interval</p>
</dd>
</dl>



<h3>References</h3>

<p>McGrath, R. E., &amp; Meyer, G. J. (2006). When effect sizes disagree: the case of
r and d. Psychological Methods, 11(4), 386-401.
</p>
<p>Viechtbauer, W. (2007). Approximate confidence intervals for standardized
effect sizes in the two-independent and two-dependent samples design.
Journal of Educational and Behavioral Statistics, 32(1), 39-60.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+t.test">t.test</a></code> the workhorse for <code>mean_diff</code>,
<code><a href="#topic+means_diff">means_diff</a></code> for multiple variables across the same two groups,
<code><a href="psych.html#topic+cohen.d">cohen.d</a></code> for another standardized mean difference function,
<code><a href="#topic+mean_change">mean_change</a></code> for dependent two-sample t-test,
<code><a href="#topic+mean_test">mean_test</a></code> for one-sample t-test,
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# independent two-samples t-test
mean_diff(x = mtcars$"mpg", bin = mtcars$"vs")
mean_diff(x = mtcars$"mpg", bin = mtcars$"vs", lvl = c("1","0"))
mean_diff(x = mtcars$"mpg", bin = mtcars$"vs", lvl = c(1, 0)) # levels don't have to be character
mean_diff(x = mtcars$"mpg", bin = mtcars$"vs", d.ci.type = "classic")

# compare to psych::cohen.d()
mean_diff(x = mtcars$"mpg", bin = mtcars$"vs", d.ci.type = "tdist")
tmp_nm &lt;- c("mpg","vs") # because otherwise Roxygen2 gets upset
cohend_obj &lt;- psych::cohen.d(mtcars[tmp_nm], group = "vs")
as.data.frame(cohend_obj[["cohen.d"]]) # different estimate of cohen's d
   # of course, this also leads to different confidence interval bounds as well

# same as intercept-only regression when var.equal = TRUE
mean_diff(x = mtcars$"mpg", bin = mtcars$"vs", d.ci.type = "tdist")
lm_obj &lt;- lm(mpg ~ vs, data = mtcars)
coef(summary(lm_obj))

# errors
## Not run: 
mean_diff(x = mtcars$"mpg",
   bin = attitude$"ratings") # `bin` has length different than `x`
mean_diff(x = mtcars$"mpg",
   bin = mtcars$"gear") # `bin` has more than two unique values (other than missing values)

## End(Not run)

</code></pre>

<hr>
<h2 id='mean_if'>Mean Conditional on Minimum Frequency of Observed Values</h2><span id='topic+mean_if'></span>

<h3>Description</h3>

<p><code>mean_if</code> calculates the mean of a numeric or logical vector conditional
on a specified minimum frequency of observed values. If the frequency of
observed values is less than (or equal to) <code>ov.min</code>, then <code>NA</code> is
returned rather than the mean.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mean_if(x, trim = 0, ov.min = 1, prop = TRUE, inclusive = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mean_if_+3A_x">x</code></td>
<td>
<p>numeric or logical vector.</p>
</td></tr>
<tr><td><code id="mean_if_+3A_trim">trim</code></td>
<td>
<p>numeric vector of length 1 specifying the proportion of values
from each end of <code>x</code> to trim. Trimmed values are recoded to their
endpoint for calculation of the mean. See <code>mean.default</code>.</p>
</td></tr>
<tr><td><code id="mean_if_+3A_ov.min">ov.min</code></td>
<td>
<p>minimum frequency of observed values required. If <code>prop</code> =
TRUE, then this is a decimal between 0 and 1. If <code>prop</code> = FALSE, then
this is a integer between 0 and <code>length(x)</code>.</p>
</td></tr>
<tr><td><code id="mean_if_+3A_prop">prop</code></td>
<td>
<p>logical vector of length 1 specifying whether <code>ov.min</code>
should refer to the proportion of observed values (TRUE) or the count of
observed values (FALSE).</p>
</td></tr>
<tr><td><code id="mean_if_+3A_inclusive">inclusive</code></td>
<td>
<p>logical vector of length 1 specifying whether the mean
should be calculated if the frequency of observed values is exactly equal
to <code>ov.min</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>numeric vector of length 1 providing the mean of <code>x</code> or
<code>NA</code> conditional on if the frequency of observed data is greater than
(or equal to) <code>ov.min</code>.
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+mean.default">mean.default</a></code>
<code><a href="#topic+sum_if">sum_if</a></code>
<code><a href="#topic+make.fun_if">make.fun_if</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>mean_if(x = airquality[[1]], ov.min = .75) # proportion of observed values
mean_if(x = airquality[[1]], ov.min = 116,
   prop = FALSE) # count of observe values
mean_if(x = airquality[[1]], ov.min = 116, prop = FALSE,
   inclusive = FALSE) # not include ov.min value itself
mean_if(x = c(TRUE, NA, FALSE, NA),
   ov.min = .50) # works with logical vectors as well as numeric
</code></pre>

<hr>
<h2 id='mean_test'>Test for Sample Mean Against Mu (one-sample t-test)</h2><span id='topic+mean_test'></span>

<h3>Description</h3>

<p><code>mean_test</code> computes the sample mean and compares it against a specified
population <code>mu</code> value. This is sometimes referred to as a one-sample
t-test. It provides the same results as <code><a href="stats.html#topic+t.test">t.test</a></code>, but
provides the confidence interval for the mean difference from mu rather than
the mean itself. The function also calculates the descriptive statistics and
the standardized mean difference (i.e., Cohen's d) based on the sample
standard deviation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mean_test(x, mu = 0, d.ci.type = "tdist", ci.level = 0.95, check = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mean_test_+3A_x">x</code></td>
<td>
<p>numeric vector.</p>
</td></tr>
<tr><td><code id="mean_test_+3A_mu">mu</code></td>
<td>
<p>numeric vector of length 1 specifying the population mean value to
compare the sample mean against.</p>
</td></tr>
<tr><td><code id="mean_test_+3A_d.ci.type">d.ci.type</code></td>
<td>
<p>character vector with length 1 specifying the type of
confidence interval to compute for the standardized mean difference (i.e.,
Cohen's d). There are currently two options: 1. &quot;tdist&quot; which calculates
the confidence intervals based on the t-distribution using the function
<code><a href="psych.html#topic+cohen.d.ci">cohen.d.ci</a></code>. No standard error is calculated for this
option and NA is returned for &quot;d_se&quot; in the return object. 2. &quot;classic&quot;
which calculates the confidence interval of Cohen's d based on the
confidence interval of the mean difference itself. The lower and upper
confidence bounds are divided by the sample standard deviation.
Technically, this confidence interval is biased due to not taking into
account the uncertainty of the standard deviations. No standard error is
calculated for this option and NA is returned for &quot;d_se&quot; in the return
object.</p>
</td></tr>
<tr><td><code id="mean_test_+3A_ci.level">ci.level</code></td>
<td>
<p>numeric vector of length 1 specifying the confidence level.
It must be between 0 and 1.</p>
</td></tr>
<tr><td><code id="mean_test_+3A_check">check</code></td>
<td>
<p>logical vector of length 1 specifying whether the input
arguments should be checked for errors. For example, checking whether
<code>x</code> is a numeric vector. This is a tradeoff between computational
efficiency (FALSE) and more useful error messages (TRUE).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list of numeric vectors containing statistical information about the
sample mean: 1) nhst = one-sample t-test stat info in a numeric vector,
2) desc = descriptive statistics stat info in a numeric vector,
3) std = standardized mean difference stat info in a numeric vector
</p>
<p>1) nhst = one-sample t-test stat info in a numeric vector
</p>

<dl>
<dt>est</dt><dd><p>mean - mu estimate</p>
</dd>
<dt>se</dt><dd><p>standard error</p>
</dd>
<dt>t</dt><dd><p>t-value</p>
</dd>
<dt>df</dt><dd><p>degrees of freedom</p>
</dd>
<dt>p</dt><dd><p>two-sided p-value</p>
</dd>
<dt>lwr</dt><dd><p>lower bound of the confidence interval</p>
</dd>
<dt>upr</dt><dd><p>upper bound of the confidence interval</p>
</dd>
</dl>

<p>2) desc = descriptive statistics stat info in a numeric vector
</p>

<dl>
<dt>mean</dt><dd><p>mean of <code>x</code></p>
</dd>
<dt>mu</dt><dd><p>population value of comparison</p>
</dd>
<dt>sd</dt><dd><p>standard deviation of <code>x</code></p>
</dd>
<dt>n</dt><dd><p>sample size of <code>x</code></p>
</dd>
</dl>

<p>3) std = standardized mean difference stat info in a numeric vector
</p>

<dl>
<dt>d_est</dt><dd><p>Cohen's d estimate</p>
</dd>
<dt>d_se</dt><dd><p>Cohen's d standard error</p>
</dd>
<dt>d_lwr</dt><dd><p>Cohen's d lower bound of the confidence interval</p>
</dd>
<dt>d_upr</dt><dd><p>Cohen's d upper bound of the confidence interval</p>
</dd>
</dl>



<h3>See Also</h3>

<p><code><a href="#topic+means_test">means_test</a></code> one-sample t-tests for multiple variables,
<code><a href="stats.html#topic+t.test">t.test</a></code> same results,
<code><a href="#topic+mean_diff">mean_diff</a></code> independent two-sample t-test,
<code><a href="#topic+mean_change">mean_change</a></code> dependent two-sample t-test,
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# one-sample t-test
mean_test(x = mtcars$"mpg")
mean_test(x = attitude$"rating", mu = 50)
mean_test(x = attitude$"rating", mu = 50, d.ci.type = "classic")

# compare to t.test()
mean_test(x = attitude$"rating", mu = 50, ci.level = .99)
t.test(attitude$"rating", mu = 50, conf.level = .99)

# same as intercept-only regression when mu = 0
mean_test(x = mtcars$"mpg")
lm_obj &lt;- lm(mpg ~ 1, data = mtcars)
coef(summary(lm_obj))

</code></pre>

<hr>
<h2 id='means_change'>Mean Changes Across Two Timepoints For Multiple PrePost Pairs of Variables
(dependent two-samples t-tests)</h2><span id='topic+means_change'></span>

<h3>Description</h3>

<p><code>means_change</code> tests for mean changes across two timepoints for multiple
prepost pairs of variables via dependent two-samples t-tests. The function
also calculates the descriptive statistics for the timepoints and the
standardized mean differences (i.e., Cohen's d) based on either the standard
deviation of the pre-timepoint, pooled standard deviation of the
pre-timepoint and post-timepoint, or the standard deviation of the change
score (post - pre). <code>means_change</code> is simply a wrapper for
<code><a href="stats.html#topic+t.test">t.test</a></code> plus some extra calculations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>means_change(
  data,
  prepost.nm.list,
  standardizer = "pre",
  d.ci.type = "unbiased",
  ci.level = 0.95,
  check = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="means_change_+3A_data">data</code></td>
<td>
<p>data.frame of data.</p>
</td></tr>
<tr><td><code id="means_change_+3A_prepost.nm.list">prepost.nm.list</code></td>
<td>
<p>list of length-2 character vectors specifying the
colnames from <code>data</code> corresponding to the prepost pairs of variables.
For each element of the list, the character vector should have length 2
where the first element corresponds to the pre-timepoint variable colname
of that prepost pair and the second element corresponds to the
post-timepoint variable colname of that prepost pair. The names of the list
will be the rownames in the data.frames of the return object. See examples.
<code>prepost.nm.list</code> can also be a single length-2 character vector for
the case of a single pre-post pair of variables, which is functionally
equivalent to <code><a href="#topic+mean_change">mean_change</a></code>.</p>
</td></tr>
<tr><td><code id="means_change_+3A_standardizer">standardizer</code></td>
<td>
<p>chararacter vector of length 1 specifying what to use for
standardization when computing the standardized mean difference (i.e.,
Cohen's d). There are three options: 1. &quot;pre&quot; for the standard deviation of
the pre-timepoint, 2. &quot;pooled&quot; for the pooled standard deviation of the
pre-timepoint and post-timepoint, 3. &quot;change&quot; for the standard deviation of
the change score (post - pre). The default is &quot;pre&quot;, which I believe makes
the most theoretical sense (see Cumming, 2012); however, &quot;change&quot; is the
traditional choice originally proposed by Jacob Cohen (Cohen, 1988).</p>
</td></tr>
<tr><td><code id="means_change_+3A_d.ci.type">d.ci.type</code></td>
<td>
<p>character vector of lenth 1 specifying how to compute the
confidence intervals (and standard errors) of the standardized mean
differences. There are currently two options: 1. &quot;unbiased&quot; which
calculates the unbiased standard error of Cohen's d based on the formulas
in Viechtbauer (2007). If <code>standardizer</code> = &quot;pre&quot; or &quot;pooled&quot;, then
equation 36 from Table 2 is used. If <code>standardizer</code> = &quot;change&quot;, then
equation 25 from Table 1 is used. A symmetrical confidence interval is then
calculated based on the standard error. 2. &quot;classic&quot; which calculates the
confidence interval of Cohen's d based on the confidence interval of the
mean change itself. The lower and upper confidence bounds are divided by
the <code>standardizer</code>. Technically, this confidence interval is biased
due to not taking into account the uncertainty of the <code>standardizer</code>.
No standard error is calculated for this option and NA is returned for
&quot;d_se&quot; in the return object.</p>
</td></tr>
<tr><td><code id="means_change_+3A_ci.level">ci.level</code></td>
<td>
<p>double vector of length 1 specifying the confidence level.
<code>ci.level</code> must range from 0 to 1.</p>
</td></tr>
<tr><td><code id="means_change_+3A_check">check</code></td>
<td>
<p>logical vector of length 1 specifying whether the input
arguments should be checked for errors. For example, checking whether
<code>prepost.nm.list</code> is a list of length-2 character vectors. This is a
tradeoff between computational efficiency (FALSE) and more useful error
messages (TRUE).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For each prepost pair of variables, <code>means_change</code> calculates the mean
change as <code>data[[ prepost.nm.list[[i]][2] ]]</code> - <code>data[[
prepost.nm.list[[i]][1] ]]</code> (which corresponds to post - pre) such that
increases over time have a positive mean change estimate and decreases over
time have a negative mean change estimate. This would be as if the
post-timepoint was <code>x</code> and the pre-timepoint <code>y</code> in
<code>t.test(paired = TRUE)</code>.
</p>


<h3>Value</h3>

<p>list of data.frames containing statistical information about the mean
change for each prepost pair of variables (the rownames of the data.frames
are the names of <code>prepost.nm.list</code>): 1) nhst = dependent two-samples
t-test stat info in a data.frame, 2) desc = descriptive statistics stat info
in a data.frame, 3) std = standardized mean difference stat info in a data.frame,
</p>
<p>1) nhst = dependent two-samples t-test stat info in a data.frame
</p>

<dl>
<dt>est</dt><dd><p>mean change estimate (i.e., post - pre)</p>
</dd>
<dt>se</dt><dd><p>standard error</p>
</dd>
<dt>t</dt><dd><p>t-value</p>
</dd>
<dt>df</dt><dd><p>degrees of freedom</p>
</dd>
<dt>p</dt><dd><p>two-sided p-value</p>
</dd>
<dt>lwr</dt><dd><p>lower bound of the confidence interval</p>
</dd>
<dt>upr</dt><dd><p>upper bound of the confidence interval</p>
</dd>
</dl>

<p>2) desc = descriptive statistics stat info in a data.frame
</p>

<dl>
<dt>mean_post</dt><dd><p>mean of the post variable</p>
</dd>
<dt>mean_pre</dt><dd><p>mean of the pre variable</p>
</dd>
<dt>sd_post</dt><dd><p>standard deviation of of the post variable</p>
</dd>
<dt>sd_pre</dt><dd><p>standard deviation of the pre variable</p>
</dd>
<dt>n</dt><dd><p>sample size of the change score</p>
</dd>
<dt>r</dt><dd><p>Pearson correlation between the pre and post variables</p>
</dd>
</dl>

<p>3) std = standardized mean difference stat info in a data.frame
</p>

<dl>
<dt>d_est</dt><dd><p>Cohen's d estimate</p>
</dd>
<dt>d_se</dt><dd><p>Cohen's d standard error</p>
</dd>
<dt>d_lwr</dt><dd><p>Cohen's d lower bound of the confidence interval</p>
</dd>
<dt>d_upr</dt><dd><p>Cohen's d upper bound of the confidence interval</p>
</dd>
</dl>



<h3>References</h3>

<p>Cohen, J. (1988). Statistical power analysis for the behavioral sciences,
2nd ed. Hillsdale, NJ: Erlbaum.
</p>
<p>Cumming, G. (2012). Understanding the new statistics: Effect sizes,
confidence intervals, and meta-analysis. New York, NY: Rouledge.
</p>
<p>Viechtbauer, W. (2007). Approximate confidence intervals for standardized
effect sizes in the two-independent and two-dependent samples design.
Journal of Educational and Behavioral Statistics, 32(1), 39-60.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mean_change">mean_change</a></code> for a single pair of prepost variables,
<code><a href="stats.html#topic+t.test">t.test</a></code> fixes the table of contents for some unknown reason,
<code><a href="#topic+means_diff">means_diff</a></code> for multiple independent two-sample t-tests,
<code><a href="#topic+means_test">means_test</a></code> for multiple one-sample t-tests,
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# dependent two-sample t-tests
prepost_nm_list &lt;- list("first_pair" = c("disp","hp"), "second_pair" = c("carb","gear"))
means_change(mtcars, prepost.nm.list = prepost_nm_list)
means_change(mtcars, prepost.nm.list = prepost_nm_list, d.ci.type = "classic")
means_change(mtcars, prepost.nm.list = prepost_nm_list, standardizer = "change")
means_change(mtcars, prepost.nm.list = prepost_nm_list, ci.level = 0.99)

# same as intercept-only regression with the change score
means_change(data = mtcars, prepost.nm.list = c("disp","hp"))
lm_obj &lt;- lm(hp - disp ~ 1, data = mtcars)
coef(summary(lm_obj))

</code></pre>

<hr>
<h2 id='means_compare'>Mean differences for multiple variables across 3+ independent groups (one-way
ANOVAs)</h2><span id='topic+means_compare'></span>

<h3>Description</h3>

<p><code>means_compare</code> compares means across 3+ independent groups with a
separate one-way ANOVA for each variable. The function also calculates the
descriptive statistics for each group and the variance explained (i.e., R^2 -
aka eta^2) by the nominal grouping variable. <code>means_compare</code> is simply a
wrapper for <code><a href="stats.html#topic+oneway.test">oneway.test</a></code> plus some extra calculations.
<code>mean_compare</code> will work with 2 independent groups; however it arguably
makes more sense to use <code><a href="#topic+mean_diff">mean_diff</a></code> in that case.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>means_compare(
  data,
  vrb.nm,
  nom.nm,
  lvl = levels(as.factor(data[[nom.nm]])),
  var.equal = TRUE,
  r2.ci.type = "classic",
  ci.level = 0.95,
  rtn.table = TRUE,
  check = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="means_compare_+3A_data">data</code></td>
<td>
<p>data.frame of data.</p>
</td></tr>
<tr><td><code id="means_compare_+3A_vrb.nm">vrb.nm</code></td>
<td>
<p>character vector of length 1 with colnames from <code>data</code>
specifying the variables.</p>
</td></tr>
<tr><td><code id="means_compare_+3A_nom.nm">nom.nm</code></td>
<td>
<p>character vector of length 1 with colnames from <code>data</code>
specifying the nominal variable. It identifies the 3+ groups with 3+ unique values (other
than missing values).</p>
</td></tr>
<tr><td><code id="means_compare_+3A_lvl">lvl</code></td>
<td>
<p>character vector with length 3+ specifying the unique values for
the 3+ groups. If <code>nom</code> is a factor, then <code>lvl</code> should be the
factor levels rather than the underlying integer codes. This argument
allows you to specify the order of the descriptive statistics in the return
object, which will be opposite the order of <code>lvl</code> for consistency with
<code><a href="#topic+mean_diff">mean_diff</a></code> and <code><a href="#topic+mean_change">mean_change</a></code>.</p>
</td></tr>
<tr><td><code id="means_compare_+3A_var.equal">var.equal</code></td>
<td>
<p>logical vector of length 1 specifying whether the variances
of the groups are assumed to be equal (TRUE) or not (FALSE). If TRUE, a
traditional one-way ANOVA is computed; if FALSE, Welch's ANOVA is computed.
These two tests differ by their denominator degrees of freedoms, F-values,
and p-values.</p>
</td></tr>
<tr><td><code id="means_compare_+3A_r2.ci.type">r2.ci.type</code></td>
<td>
<p>character vector with length 1 specifying the type of
confidence intervals to compute for the variance explained (i.e., R^2 or
eta^2). There are currently two options: 1) &quot;Fdist&quot; which calculates a
non-symmetrical confidence interval based on the non-central F distribution
(pg. 38, Smithson, 2003), 2) &quot;classic&quot; which calculates the confidence
interval based on a large-sample theory standard error (eq. 3.6.3 in Cohen,
Cohen, West, &amp; Aiken, 2003), which is taken from Olkin &amp; Finn (1995) - just
above eq. 10. The confidence intervals for R^2-adjusted use the same
formula as R^2, but replace R^2 with R^2 adjusted. Technically, the R^2
adjusted confidence intervals can have poor coverage (pg. 54, Smithson,
2003)</p>
</td></tr>
<tr><td><code id="means_compare_+3A_ci.level">ci.level</code></td>
<td>
<p>numeric vector of length 1 specifying the confidence level.
<code>ci.level</code> must range from 0 to 1.</p>
</td></tr>
<tr><td><code id="means_compare_+3A_rtn.table">rtn.table</code></td>
<td>
<p>logical vector of length 1 specifying whether the traditional
ANOVA tables should be returned as the last element of the return object.</p>
</td></tr>
<tr><td><code id="means_compare_+3A_check">check</code></td>
<td>
<p>logical vector of length 1 specifying whether the input
arguments should be checked for errors. For example, if <code>vrb.nm</code> are
not colnames within <code>data</code>. This is a tradeoff between computational
efficiency (FALSE) and more useful error messages (TRUE).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list of data.frames containing statistical information about the mean
comparisons for each variable (the rows of the data.frames are
<code>vrb.nm</code>): 1) nhst = one-way ANOVA stat info in a data.frame,
2) desc = descriptive statistics stat info in a data.frame,
3) std = standardized effect sizes stat info in a data.frame,
4) anova = traditional ANOVA table in a numeric 3D array (only
returned if rtn.table = TRUE)
</p>
<p>1) nhst = one-way ANOVA stat info in a data.frame
</p>

<dl>
<dt>diff_avg</dt><dd><p>average mean difference across group pairs</p>
</dd>
<dt>se</dt><dd><p>NA to remind the user there is no standard error for the average mean difference</p>
</dd>
<dt>F</dt><dd><p>F-value</p>
</dd>
<dt>df_num</dt><dd><p>numerator degrees of freedom</p>
</dd>
<dt>df_den</dt><dd><p>denominator degrees of freedom</p>
</dd>
<dt>p</dt><dd><p>two-sided p-value</p>
</dd>
</dl>

<p>2) desc = descriptive statistics stat info in a data.frame (note there
could be more than 3 groups - groups i, j, and k are just provided as an example)
</p>

<dl>
<dt>mean_'lvl[k]'</dt><dd><p>mean of group k</p>
</dd>
<dt>mean_'lvl[j]'</dt><dd><p>mean of group j</p>
</dd>
<dt>mean_'lvl[i]'</dt><dd><p>mean of group i</p>
</dd>
<dt>sd_'lvl[k]'</dt><dd><p>standard deviation of group k</p>
</dd>
<dt>sd_'lvl[j]'</dt><dd><p>standard deviation of group j</p>
</dd>
<dt>sd_'lvl[i]'</dt><dd><p>standard deviation of group i</p>
</dd>
<dt>n_'lvl[k]'</dt><dd><p>sample size of group k</p>
</dd>
<dt>n_'lvl[j]'</dt><dd><p>sample size of group j</p>
</dd>
<dt>n_'lvl[i]'</dt><dd><p>sample size of group i</p>
</dd>
</dl>

<p>3) std = standardized effect sizes stat info in a data.frame
</p>

<dl>
<dt>r2_reg_est</dt><dd><p>R^2 estimate</p>
</dd>
<dt>r2_reg_se</dt><dd><p>R^2 standard error (only available if <code>r2.ci.type</code> = &quot;classic&quot;)</p>
</dd>
<dt>r2_reg_lwr</dt><dd><p>R^2 lower bound of the confidence interval</p>
</dd>
<dt>r2_reg_upr</dt><dd><p>R^2 upper bound of the confidence interval</p>
</dd>
<dt>r2_adj_est</dt><dd><p>R^2-adjusted estimate</p>
</dd>
<dt>r2_adj_se</dt><dd><p>R^2-adjusted standard error (only available if <code>r2.ci.type</code> = &quot;classic&quot;)</p>
</dd>
<dt>r2_adj_lwr</dt><dd><p>R^2-adjusted lower bound of the confidence interval</p>
</dd>
<dt>r2_adj_upr</dt><dd><p>R^2-adjusted upper bound of the confidence interval</p>
</dd>
</dl>

<p>4) anova = traditional ANOVA table in a numeric 3D array (only
returned if rtn.table = TRUE).
</p>
<p>The dimlabels of the array are &quot;effect&quot; for
the rows, &quot;info&quot; for the columns, and &quot;vrb&quot; for the layers. There are two
rows with rownames 1. &quot;nom&quot; and 2. &quot;Residuals&quot; where &quot;nom&quot; refers to the
between-group effect of the nominal variable and &quot;Residuals&quot; refers to the
within-group residual errors. There are 5 columns with colnames 1. &quot;SS&quot; = sum
of squares, 2. &quot;df&quot; = degrees of freedom, 3. &quot;MS&quot; = mean squares, 4. &quot;F&quot; =
F-value. and 5. &quot;p&quot; = p-value. Note the F-value and p-value will differ from
the &quot;nhst&quot; returned vector if <code>var.equal</code> = FALSE because the
traditional ANOVA table always assumes variances are equal (i.e.
<code>var.equal</code> = TRUE). There are as many layers as <code>length(vrb.nm)</code>
with the laynames equal to <code>vrb.nm</code>.
</p>


<h3>References</h3>

<p>Cohen, J., Cohen, P., West, A. G., &amp; Aiken, L. S. (2003). Applied Multiple
Regression/Correlation Analysis for the Behavioral Science - third edition.
New York, NY: Routledge.
</p>
<p>Olkin, I., &amp; Finn, J. D. (1995). Correlations redux. Psychological Bulletin, 118(1), 155-164.
</p>
<p>Smithson, M. (2003). Confidence intervals. Thousand Oaks, CA: Sage Publications.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+oneway.test">oneway.test</a></code> the workhorse for <code>means_compare</code>,
<code><a href="#topic+mean_compare">mean_compare</a></code> for a single variable across the same 3+ groups,
<code><a href="MBESS.html#topic+ci.R2">ci.R2</a></code> for confidence intervals of the variance explained,
<code><a href="#topic+means_diff">means_diff</a></code> for multiple variables across only 2 groups,
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
means_compare(mtcars, vrb.nm = c("mpg","wt","qsec"), nom.nm = "gear")
means_compare(mtcars, vrb.nm = c("mpg","wt","qsec"), nom.nm = "gear",
   var.equal = FALSE)
means_compare(mtcars, vrb.nm = c("mpg","wt","qsec"), nom.nm = "gear",
   rtn.table = FALSE)
means_compare(mtcars, vrb.nm = "mpg", nom.nm = "gear")

</code></pre>

<hr>
<h2 id='means_diff'>Mean differences across two independent groups (independent two-samples
t-tests)</h2><span id='topic+means_diff'></span>

<h3>Description</h3>

<p><code>means_diff</code> tests for mean differences across two independent groups
with independent two-samples t-tests. The function also calculates the
descriptive statistics for each group and the standardized mean differences
(i.e., Cohen's d) based on the pooled standard deviations. <code>mean_diff</code>
is simply a wrapper for <code><a href="stats.html#topic+t.test">t.test</a></code> plus some extra
calculations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>means_diff(
  data,
  vrb.nm,
  bin.nm,
  lvl = levels(as.factor(data[[bin.nm]])),
  var.equal = TRUE,
  d.ci.type = "unbiased",
  ci.level = 0.95,
  check = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="means_diff_+3A_data">data</code></td>
<td>
<p>data.frame of data.</p>
</td></tr>
<tr><td><code id="means_diff_+3A_vrb.nm">vrb.nm</code></td>
<td>
<p>character vector of colnames specifying the variables in
<code>data</code> to conduct the independent two-sample t-tests for.</p>
</td></tr>
<tr><td><code id="means_diff_+3A_bin.nm">bin.nm</code></td>
<td>
<p>character vector of length 1 specifying the binary variable in
<code>data</code>. It identifies the two groups with two (and only two) unique
values (other than missing values).</p>
</td></tr>
<tr><td><code id="means_diff_+3A_lvl">lvl</code></td>
<td>
<p>character vector with length 2 specifying the unique values for
the two groups. If <code>data[[bin.nm]]</code> is a factor, then <code>lvl</code>
should be the factor levels rather than the underlying integer codes. This
argument allows you to specify the direction of the mean difference.
<code>means_diff</code> calculates the mean differences as
<code>data[[vrb.nm]][data[[bin.nm]] == lvl[2], ]</code> -
<code>data[[vrb.nm]][data[[bin.nm]] == lvl[1], ]</code> such that it is group 2 -
group 1. By changing which group is group 1 vs. group 2, the direction of
the mean difference can be changed. See details.</p>
</td></tr>
<tr><td><code id="means_diff_+3A_var.equal">var.equal</code></td>
<td>
<p>logical vector of length 1 specifying whether the variances
of the groups are assumed to be equal (TRUE) or not (FALSE). If TRUE, a
traditional independent two-samples t-test is computed; if FALSE, Welch's
t-test is computed. These two tests differ by their degrees of freedom and
p-values.</p>
</td></tr>
<tr><td><code id="means_diff_+3A_d.ci.type">d.ci.type</code></td>
<td>
<p>character vector with length 1 specifying the type of
confidence intervals to compute for the standardized mean difference (i.e.,
Cohen's d). There are currently three options: 1) &quot;unbiased&quot; which
calculates the unbiased standard error of Cohen's d based on formula 25 in
Viechtbauer (2007). A symmetrical confidence interval is then calculated
based on the standard error. 2) &quot;tdist&quot; which calculates the confidence
intervals based on the t-distribution using the function
<code><a href="psych.html#topic+cohen.d.ci">cohen.d.ci</a></code>, 3) &quot;classic&quot; which calculates the
confidence interval of Cohen's d based on the confidence interval of the
mean difference itself. The lower and upper confidence bounds are divided
by the pooled standard deviation. Technically, this confidence interval is
biased due to not taking into account the uncertainty of the standard
deviations. No standard error is calculated for this option and NA is
returned for &quot;d_se&quot; in the return object.</p>
</td></tr>
<tr><td><code id="means_diff_+3A_ci.level">ci.level</code></td>
<td>
<p>numeric vector of length 1 specifying the confidence level.
<code>ci.level</code> must range from 0 to 1.</p>
</td></tr>
<tr><td><code id="means_diff_+3A_check">check</code></td>
<td>
<p>logical vector of length 1 specifying whether the input
arguments should be checked for errors. For example, if
<code>data[[bin.nm]]</code> has more than 2 unique values (other than missing
values) or if <code>bin.nm</code> is not a colname in <code>data</code>. This is a
tradeoff between computational efficiency (FALSE) and more useful error
messages (TRUE).</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>means_diff</code> calculates the mean differences as
<code>data[[vrb.nm]][data[[bin.nm]] == lvl[2], ]</code> -
<code>data[[vrb.nm]][data[[bin.nm]] == lvl[1], ]</code> such that it is group 2 -
group 1. Group 1 corresponds to the first factor level of
<code>data[[bin.nm]]</code> (after being coerced to a factor). Group 2 correspond
to the second factor level of <code>data[[bin.nm]]</code> (after being coerced to a
factor). This was set up to handle dummy coded treatment variables in a
desirable way. For example, if <code>data[[bin.nm]]</code> is a numeric vector with
values <code>0</code> and <code>1</code>, the default factor coersion will have the first
factor level be &quot;0&quot; and the second factor level &quot;1&quot;. This would result will
correspond to 1 - 0. However, if the first factor level of
<code>data[[bin.nm]]</code> is &quot;treatment&quot; and the second factor level is
&quot;control&quot;, the result will correspond to control - treatment. If the opposite
is desired (e.g., treatment - control), this can be reversed within the
function by specifying the <code>lvl</code> argument as
<code>c("control","treatment")</code>. Note, <code>means_diff</code> diverts from
<code>t.test</code> by calculating the mean difference as group 2 - group 1 (as
opposed to the group 1 - group 2 that <code>t.test</code> does). However, group 2 -
group 1 is the convention that <code>psych::cohen.d</code> uses as well.
</p>
<p><code>means_diff</code> calculates the pooled standard deviation in a different way
than <code><a href="psych.html#topic+cohen.d">cohen.d</a></code>. Therefore, the Cohen's d estimates (and
confidence intervals if d.ci.type == &quot;tdist&quot;) differ from those in
<code><a href="psych.html#topic+cohen.d">cohen.d</a></code>. <code>means_diff</code> uses the total degrees of
freedom in the denomenator while <code><a href="psych.html#topic+cohen.d">cohen.d</a></code> uses the total
sample size in the denomenator - based on the notation in McGrath &amp; Meyer
(2006). However, almost every introduction to statistics textbook uses the
total degrees of freedom in the denomenator and that is what makes more sense
to me. See examples.
</p>


<h3>Value</h3>

<p>list of data.frames vectors containing statistical information about
the mean differences (the rownames of each data.frame are <code>vrb.nm</code>):
1) nhst = independent two-samples t-test stat info in a data.frame,
2) desc = descriptive statistics stat info in a data.frame,
3) std = standardized mean difference stat info in a data.frame
</p>
<p>1) nhst = independent two-samples t-test stat info in a data.frame
</p>

<dl>
<dt>est</dt><dd><p>mean difference estimate (i.e., group 2 - group 1)</p>
</dd>
<dt>se</dt><dd><p>standard error</p>
</dd>
<dt>t</dt><dd><p>t-value</p>
</dd>
<dt>df</dt><dd><p>degrees of freedom</p>
</dd>
<dt>p</dt><dd><p>two-sided p-value</p>
</dd>
<dt>lwr</dt><dd><p>lower bound of the confidence interval</p>
</dd>
<dt>upr</dt><dd><p>upper bound of the confidence interval</p>
</dd>
</dl>

<p>2) desc = descriptive statistics stat info in a data.frame
</p>

<dl>
<dt>mean_'lvl[2]'</dt><dd><p>mean of group 2</p>
</dd>
<dt>mean_'lvl[1]'</dt><dd><p>mean of group 1</p>
</dd>
<dt>sd_'lvl[2]'</dt><dd><p>standard deviation of group 2</p>
</dd>
<dt>sd_'lvl[1]'</dt><dd><p>standard deviation of group 1</p>
</dd>
<dt>n_'lvl[2]'</dt><dd><p>sample size of group 2</p>
</dd>
<dt>n_'lvl[1]'</dt><dd><p>sample size of group 1</p>
</dd>
</dl>

<p>3) std = standardized mean difference stat info in a data.frame
</p>

<dl>
<dt>d_est</dt><dd><p>Cohen's d estimate</p>
</dd>
<dt>d_se</dt><dd><p>Cohen's d standard error</p>
</dd>
<dt>d_lwr</dt><dd><p>Cohen's d lower bound of the confidence interval</p>
</dd>
<dt>d_upr</dt><dd><p>Cohen's d upper bound of the confidence interval</p>
</dd>
</dl>



<h3>References</h3>

<p>McGrath, R. E., &amp; Meyer, G. J. (2006). When effect sizes disagree: the case of
r and d. Psychological Methods, 11(4), 386-401.
</p>
<p>Viechtbauer, W. (2007). Approximate confidence intervals for standardized
effect sizes in the two-independent and two-dependent samples design.
Journal of Educational and Behavioral Statistics, 32(1), 39-60.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+means_diff">means_diff</a></code> for independent two-sample t-test of a single variable,
<code><a href="stats.html#topic+t.test">t.test</a></code> the workhorse for <code>mean_diff</code>,
<code><a href="psych.html#topic+cohen.d">cohen.d</a></code> for another standardized mean difference function,
<code><a href="#topic+means_change">means_change</a></code> for dependent two-sample t-tests,
<code><a href="#topic+means_test">means_test</a></code> for one-sample t-tests,
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# independent two-samples t-tests
means_diff(data = mtcars, vrb.nm = c("mpg","cyl","disp"), bin.nm = "vs")
means_diff(data = mtcars, vrb.nm = c("mpg","cyl","disp"), bin.nm = "vs",
   d.ci.type = "classic")
means_diff(data = mtcars, vrb.nm = c("mpg","cyl","disp"), bin.nm = "vs",
   lvl = c("1","0")) # signs are reversed
means_diff(data = mtcars, vrb.nm = c("mpg","cyl","disp"), bin.nm = "vs",
   lvl = c(1,0)) # can provide numeric levels for dummy variables

# compare to psych::cohen.d()
means_diff(data = mtcars, vrb.nm = c("mpg","cyl","disp"), bin.nm = "vs",
   d.ci.type = "tdist")
tmp_nm &lt;- c("mpg","cyl","disp","vs") # so that Roxygen2 doesn't freak out
cohend_obj &lt;- psych::cohen.d(mtcars[tmp_nm], group = "vs")
as.data.frame(cohend_obj[["cohen.d"]]) # different estimate of cohen's d
   # of course, this also leads to different confidence interval bounds as well

# same as intercept-only regression when var.equal = TRUE
means_diff(data = mtcars, vrb.nm = "mpg", bin.nm = "vs")
lm_obj &lt;- lm(mpg ~ vs, data = mtcars)
coef(summary(lm_obj))

# if levels are not unique values in data[[bin.nm]]
## Not run: 
means_diff(data = mtcars, vrb.nm = c("mpg","cyl","disp"), bin.nm = "vs",
   lvl = c("zero", "1")) # an error message is returned
means_diff(data = mtcars, vrb.nm = c("mpg","cyl","disp"), bin.nm = "vs",
   lvl = c("0", "one")) # an error message is returned

## End(Not run)

</code></pre>

<hr>
<h2 id='means_test'>Test for Multiple Sample Means Against Mu (one-sample t-tests)</h2><span id='topic+means_test'></span>

<h3>Description</h3>

<p><code>means_test</code> computes sample means and compares them against specified
population <code>mu</code> values. These are sometimes referred to as one-sample
t-tests. It provides the same results as <code><a href="stats.html#topic+t.test">t.test</a></code>, but
provides the confidence intervals for the mean differences from mu rather
than the mean itself. The function also calculates the descriptive statistics
and the standardized mean differences (i.e., Cohen's d) based on the sample
standard deviations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>means_test(
  data,
  vrb.nm,
  mu = 0,
  d.ci.type = "tdist",
  ci.level = 0.95,
  check = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="means_test_+3A_data">data</code></td>
<td>
<p>data.frame or data.</p>
</td></tr>
<tr><td><code id="means_test_+3A_vrb.nm">vrb.nm</code></td>
<td>
<p>character vector of colnames specifying the variables in
<code>data</code> to conduct the one-sample t-tests for.</p>
</td></tr>
<tr><td><code id="means_test_+3A_mu">mu</code></td>
<td>
<p>numeric vector of length = <code>length(vrb.nm)</code> or length 1
specifying the population mean values to compare the sample means against.
The order of the values should be the same as the order in <code>vrb.nm</code>.
When length 1, the same population mean value is used for all the
variables.</p>
</td></tr>
<tr><td><code id="means_test_+3A_d.ci.type">d.ci.type</code></td>
<td>
<p>character vector with length 1 of specifying the type of
confidence intervals to compute for the standardized mean differences
(i.e., Cohen's d). There are currently two options: 1. &quot;tdist&quot; which
calculates the confidence intervals based on the t-distribution using the
function <code><a href="psych.html#topic+cohen.d.ci">cohen.d.ci</a></code>. No standard error is calculated
for this option and NA is returned for &quot;d_se&quot; in the return object. 2.
&quot;classic&quot; which calculates the confidence intervals of Cohen's d based on
the confidence interval of the mean difference itself. The lower and upper
confidence bounds are divided by the sample standard deviation.
Technically, this confidence interval is biased due to not taking into
account the uncertainty of the standard deviations. No standard error is
calculated for this option and NA is returned for &quot;d_se&quot; in the return
object.</p>
</td></tr>
<tr><td><code id="means_test_+3A_ci.level">ci.level</code></td>
<td>
<p>numeric vector of length 1 specifying the confidence level.
It must be between 0 and 1.</p>
</td></tr>
<tr><td><code id="means_test_+3A_check">check</code></td>
<td>
<p>logical vector of length 1 specifying whether the input
arguments should be checked for errors. For example, checking whether
<code>ci.level</code> is between 0 and 1. This is a tradeoff between
computational efficiency (FALSE) and more useful error messages (TRUE).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list of data.frames containing statistical information about the
sample means (the rownames of the data.frames are <code>vrb.nm</code>): 1)
nhst = one-sample t-test stat info in a data.frame, 2) desc = descriptive
statistics stat info in a data.frame, 3) std = standardized mean difference
stat info in a data.frame
</p>
<p>1) nhst = one-sample t-test stat info in a data.frame
</p>

<dl>
<dt>est</dt><dd><p>mean - mu estimate</p>
</dd>
<dt>se</dt><dd><p>standard error</p>
</dd>
<dt>t</dt><dd><p>t-value</p>
</dd>
<dt>df</dt><dd><p>degrees of freedom</p>
</dd>
<dt>p</dt><dd><p>two-sided p-value</p>
</dd>
<dt>lwr</dt><dd><p>lower bound of the confidence interval</p>
</dd>
<dt>upr</dt><dd><p>upper bound of the confidence interval</p>
</dd>
</dl>

<p>2) desc = descriptive statistics stat info in a data.frame
</p>

<dl>
<dt>mean</dt><dd><p>mean of <code>x</code></p>
</dd>
<dt>mu</dt><dd><p>population value of comparison</p>
</dd>
<dt>sd</dt><dd><p>standard deviation of <code>x</code></p>
</dd>
<dt>n</dt><dd><p>sample size of <code>x</code></p>
</dd>
</dl>

<p>3) std = standardized mean difference stat info in a data.frame
</p>

<dl>
<dt>d_est</dt><dd><p>Cohen's d estimate</p>
</dd>
<dt>d_se</dt><dd><p>Cohen's d standard error</p>
</dd>
<dt>d_lwr</dt><dd><p>Cohen's d lower bound of the confidence interval</p>
</dd>
<dt>d_upr</dt><dd><p>Cohen's d upper bound of the confidence interval</p>
</dd>
</dl>



<h3>See Also</h3>

<p><code><a href="#topic+mean_test">mean_test</a></code> one-sample t-test for a single variable,
<code><a href="stats.html#topic+t.test">t.test</a></code> same results,
<code><a href="#topic+means_diff">means_diff</a></code> independent two-sample t-tests for multiple variables,
<code><a href="#topic+means_change">means_change</a></code> dependent two-sample t-tests for multiple variables,
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# one-sample t-tests
means_test(data = attitude, vrb.nm = names(attitude), mu = 50)
means_test(data = attitude, vrb.nm = c("rating","complaints","privileges"),
   mu = c(60, 55, 50))
means_test(data = attitude, vrb.nm = names(attitude), mu = 50, ci.level = 0.90)
means_test(airquality, vrb.nm = names(airquality)) # different df and n due to missing data

# compare to t.test
means_test(data = attitude, vrb.nm = "rating", mu = 50, ci.level = .99)
t.test(attitude$"rating", mu = 50, conf.level = .99)

# same as intercept-only regression
means_test(data = attitude, vrb.nm = "rating")
lm_obj &lt;- lm(rating ~ 1, data = attitude)
coef(summary(lm_obj))

</code></pre>

<hr>
<h2 id='mode2'>Statistical Mode of a Numeric Vector</h2><span id='topic+mode2'></span>

<h3>Description</h3>

<p><code>mode2</code> calculates the statistical mode - a measure of central tendancy
- of a numeric vector. This is in contrast to <code><a href="base.html#topic+mode">mode</a></code> in base R,
which returns the storage mode of an object. In the case multiple modes
exist, the <code>multiple</code> argument allows the user to specify if they want
the multiple modes returned or just one.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mode2(x, na.rm = FALSE, multiple = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mode2_+3A_x">x</code></td>
<td>
<p>atomic vector</p>
</td></tr>
<tr><td><code id="mode2_+3A_na.rm">na.rm</code></td>
<td>
<p>logical vector of length 1 specifying if missing values should
be removed from <code>x</code> before calculating its frequencies.</p>
</td></tr>
<tr><td><code id="mode2_+3A_multiple">multiple</code></td>
<td>
<p>logical vector of length 1 specifying if multiple modes
should be returned in the case they exist. If multiple modes exist and
<code>multiple</code> = TRUE, the multiple modes will be returned in alphanumeric
order. If multiple modes exist and <code>multiple</code> = TRUE, the first mode
in alphanumeric order will be returned. Note, NA is always last in the
alphanumeric order. If only one mode exists, then the <code>multiple</code>
argument is not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>atomic vector of the same storage mode as <code>x</code> providing the
statistical mode(s).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+freq">freq</a></code>
<code><a href="base.html#topic+table">table</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# ONE MODE
vec &lt;- c(7,8,9,7,8,9,9)
mode2(vec)
mode2(vec, multiple = TRUE)

# TWO MODES
vec &lt;- c(7,8,9,7,8,9,8,9)
mode2(vec)
mode2(vec, multiple = TRUE)

# WITH NA
vec &lt;- c(7,8,9,7,8,9,NA,9)
mode2(vec)
mode2(vec, na.rm = TRUE)
vec &lt;- c(7,8,9,7,8,9,NA,9,NA,NA)
mode2(vec)
mode2(vec, multiple = TRUE)
</code></pre>

<hr>
<h2 id='n_compare'>Test for Equal Frequency of Values (chi-square test of goodness of fit)</h2><span id='topic+n_compare'></span>

<h3>Description</h3>

<p><code>n_compare</code> tests whether all the values for a variable have equal
frequency with a chi-square test of goodness of fit. <code>n_compare</code> does
not currently allow for user-specified unequal frequencies of values; this is
possible with <code><a href="stats.html#topic+chisq.test">chisq.test</a></code>. The function also calculates
the counts and overall percentages for the value frequencies.
<code>prop_test</code> is simply a wrapper for <code><a href="stats.html#topic+chisq.test">chisq.test</a></code> plus
some extra calculations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>n_compare(x, simulate.p.value = FALSE, B = 2000)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="n_compare_+3A_x">x</code></td>
<td>
<p>atomic vector. Probably makes sense to contain relatively few unique
values.</p>
</td></tr>
<tr><td><code id="n_compare_+3A_simulate.p.value">simulate.p.value</code></td>
<td>
<p>logial vector of length 1 specifying whether the
p-value should be based on a Monte Carlo simulation rather than the classic
formula. See <code><a href="stats.html#topic+chisq.test">chisq.test</a></code> for details.</p>
</td></tr>
<tr><td><code id="n_compare_+3A_b">B</code></td>
<td>
<p>integer vector of length 1 specifying how much Monte Carlo
simulations run. Only used if <code>simulate.p.value</code> = TRUE. See
<code><a href="stats.html#topic+chisq.test">chisq.test</a></code> for details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list of numeric vectors containing statistical information about the
frequency comparison: 1) nhst = chi-square test of goodness of fit stat info
in a numeric vector, 2) count = numeric vector of length 3 with table of counts,
3) percent = numeric vector of length 3 with table of overall percentages
</p>
<p>1) nhst = chi-square test of goodness of fit stat info in a numeric vector
</p>

<dl>
<dt>diff_avg</dt><dd><p>average difference in subsample sizes (i.e., |ni - nj|)</p>
</dd>
<dt>se</dt><dd><p>NA (to remind the user there is no standard error for the test)</p>
</dd>
<dt>X2</dt><dd><p>chi-square value</p>
</dd>
<dt>df</dt><dd><p>degrees of freedom (# of unique values = 1)</p>
</dd>
<dt>p</dt><dd><p>two-sided p-value</p>
</dd>
</dl>

<p>2) count = numeric vector of length 3 with table of counts with an additional
element for the total. The names are 1. &quot;n_'lvl[k]'&quot;, 2. &quot;n_'lvl[j]'&quot;, 3.
&quot;n_'lvl[i]'&quot;, ...,  X = &quot;total&quot;
</p>
<p>3) percent = numeric vector of length 3 with table of overall percentages with an additional
element for the total. The names are 1. &quot;n_'lvl[k]'&quot;, 2. &quot;n_'lvl[j]'&quot;, 3.
&quot;n_'lvl[i]'&quot;, ...,  X = &quot;total&quot;
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+chisq.test">chisq.test</a></code> the workhorse for <code>n_compare</code>,
<code><a href="#topic+props_test">props_test</a></code> for multiple dummy variables,
<code><a href="#topic+prop_diff">prop_diff</a></code> for chi-square test of independence,
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
n_compare(mtcars$"cyl")
n_compare(mtcars$"gear")
n_compare(mtcars$"cyl", simulate.p.value = TRUE)

# compare to chisq.test()
n_compare(mtcars$"cyl")
chisq.test(table(mtcars$"cyl"))

</code></pre>

<hr>
<h2 id='ncases'>Number of Cases in Data</h2><span id='topic+ncases'></span>

<h3>Description</h3>

<p><code>ncases</code> counts how many cases in a data.frame there are that have
a specified frequency of observed values across a set of columns. This function
is similar to <code>nrow</code> and is essentially <code>partial.cases</code> + <code>sum</code>. The user
can have <code>ncases</code> return the number of complete cases by calling <code>ov.min = 1</code>,
<code>prop = TRUE</code>, and <code>inclusive = TRUE</code> (the default).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ncases(data, vrb.nm = names(data), ov.min = 1, prop = TRUE, inclusive = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ncases_+3A_data">data</code></td>
<td>
<p>data.frame or matrix of data.</p>
</td></tr>
<tr><td><code id="ncases_+3A_vrb.nm">vrb.nm</code></td>
<td>
<p>a character vector of colnames from <code>data</code> specifying the variables.</p>
</td></tr>
<tr><td><code id="ncases_+3A_ov.min">ov.min</code></td>
<td>
<p>minimum frequency of observed values required per row. If
<code>prop</code> = TRUE, then this is a decimal between 0 and 1. If <code>prop</code> = FALSE,
then this is a integer between 0 and <code>length(vrb.nm)</code>.</p>
</td></tr>
<tr><td><code id="ncases_+3A_prop">prop</code></td>
<td>
<p>logical vector of length 1 specifying whether <code>ov.min</code> should
refer to the proportion of observed values (TRUE) or the count of observed
values (FALSE).</p>
</td></tr>
<tr><td><code id="ncases_+3A_inclusive">inclusive</code></td>
<td>
<p>logical vector of length 1 specifying whether the case should
be included if the frequency of observed values in a row is exactly equal to <code>ov.min</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>integer vector of length 1 providing the nrow in <code>data</code> with the given amount of observed values.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+partial.cases">partial.cases</a></code>
<code><a href="base.html#topic+nrow">nrow</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>vrb_nm &lt;- c("Ozone","Solar.R","Wind")
nrow(airquality[vrb_nm]) # number of cases regardless of missing data
sum(complete.cases(airquality[vrb_nm])) # number of complete cases
ncases(data = airquality, vrb.nm = c("Ozone","Solar.R","Wind"),
   ov.min = 2/3) # number of rows with at least 2 of the 3 variables observed
</code></pre>

<hr>
<h2 id='ncases_by'>Number of Cases in Data by Group</h2><span id='topic+ncases_by'></span>

<h3>Description</h3>

<p><code>ncases_by</code> computes the ncases of a data.frame by group. Through the
use of the <code>ov.min</code>, <code>prop</code>, and <code>inclusive</code> arguments, the
user can specify how many missing values are allowed in a row for it to be
counted. <code>ncases_by</code> is simply a wrapper for <code>ncases</code> +
<code>agg_dfm</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ncases_by(
  data,
  vrb.nm = str2str::pick(names(data), val = grp.nm, not = TRUE),
  grp.nm,
  sep = ".",
  ov.min = 1L,
  prop = TRUE,
  inclusive = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ncases_by_+3A_data">data</code></td>
<td>
<p>data.frame of data.</p>
</td></tr>
<tr><td><code id="ncases_by_+3A_vrb.nm">vrb.nm</code></td>
<td>
<p>character vector of colnames from <code>data</code> specifying the
set of variables to base the ncases on.</p>
</td></tr>
<tr><td><code id="ncases_by_+3A_grp.nm">grp.nm</code></td>
<td>
<p>character vector of colnames from <code>data</code> specifying the
grouping variables.</p>
</td></tr>
<tr><td><code id="ncases_by_+3A_sep">sep</code></td>
<td>
<p>character vector of length 1 specifying what string to use to
separate the groups when naming the return object. <code>sep</code> is only used
if <code>grp.nm</code> has length &gt; 1 (aka multiple grouping variables)</p>
</td></tr>
<tr><td><code id="ncases_by_+3A_ov.min">ov.min</code></td>
<td>
<p>minimum frequency of observed values required per row. If
<code>prop</code> = TRUE, then this is a decimal between 0 and 1. If <code>prop</code>
= FALSE, then this is a integer between 0 and <code>length(vrb.nm)</code>.</p>
</td></tr>
<tr><td><code id="ncases_by_+3A_prop">prop</code></td>
<td>
<p>logical vector of length 1 specifying whether <code>ov.min</code>
should refer to the proportion of observed values (TRUE) or the count of
observed values (FALSE).</p>
</td></tr>
<tr><td><code id="ncases_by_+3A_inclusive">inclusive</code></td>
<td>
<p>logical vector of length 1 specifying whether the case
should be included if the frequency of observed values in a row is exactly
equal to <code>ov.min</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>atomic vector with names = <code>unique(interaction(data[grp.nm], sep
  = sep))</code> and length = <code>length(unique(interaction(data[grp.nm], sep =
  sep)))</code> providing the ncases for each group.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+nrow_by">nrow_by</a></code>
<code><a href="#topic+ncases">ncases</a></code>
<code><a href="#topic+agg_dfm">agg_dfm</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# one grouping variables
tmp_nm &lt;- c("outcome","case","session","trt_time")
dat &lt;- as.data.frame(lmeInfo::Bryant2016)[tmp_nm]
stats_by &lt;- psych::statsBy(dat,
   group = "case") # requires you to include "case" column in dat
ncases_by(data = dat, grp.nm = "case")
dat2 &lt;- as.data.frame(ChickWeight)
ncases_by(data = dat2, grp.nm = "Chick")

# two grouping variables
tmp &lt;- reshape(psych::bfi[1:10, ], varying = 1:25, timevar = "item",
   ids = row.names(psych::bfi)[1:10], direction = "long", sep = "")
tmp_nm &lt;- c("id","item","N","E","C","A","O") # Roxygen runs the whole script
dat3 &lt;- str2str::stack2(tmp[tmp_nm], select.nm = c("N","E","C","A","O"),
   keep.nm = c("id","item"))
ncases_by(dat3, grp.nm = c("id","vrb_names"))

</code></pre>

<hr>
<h2 id='ncases_desc'>Describe Number of Cases in Data by Group</h2><span id='topic+ncases_desc'></span>

<h3>Description</h3>

<p><code>ncases_desc</code> computes descriptive statistics about the number of cases
by group in a data.frame. This is often done in diary studies to obtain
information about compliance for the sample. Through the use of the
<code>ov.min</code>, <code>prop</code>, and <code>inclusive</code> arguments, the user can
specify how many missing values are allowed in a row for it to be counted.
<code>ncases_desc</code> is simply <code>ncases_by</code> + <code>psych::describe</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ncases_desc(
  data,
  vrb.nm = str2str::pick(names(data), val = grp.nm, not = TRUE),
  grp.nm,
  ov.min = 1,
  prop = TRUE,
  inclusive = TRUE,
  interp = FALSE,
  skew = TRUE,
  ranges = TRUE,
  trim = 0.1,
  type = 3,
  quant = c(0.25, 0.75),
  IQR = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ncases_desc_+3A_data">data</code></td>
<td>
<p>data.frame of data.</p>
</td></tr>
<tr><td><code id="ncases_desc_+3A_vrb.nm">vrb.nm</code></td>
<td>
<p>character vector of colnames from <code>data</code> specifying the
set of variables to base the ncases on.</p>
</td></tr>
<tr><td><code id="ncases_desc_+3A_grp.nm">grp.nm</code></td>
<td>
<p>character vector of colnames from <code>data</code> specifying the
grouping variables.</p>
</td></tr>
<tr><td><code id="ncases_desc_+3A_ov.min">ov.min</code></td>
<td>
<p>minimum frequency of observed values required per row. If
<code>prop</code> = TRUE, then this is a decimal between 0 and 1. If <code>prop</code>
= FALSE, then this is a integer between 0 and <code>length(vrb.nm)</code>.</p>
</td></tr>
<tr><td><code id="ncases_desc_+3A_prop">prop</code></td>
<td>
<p>logical vector of length 1 specifying whether <code>ov.min</code>
should refer to the proportion of observed values (TRUE) or the count of
observed values (FALSE).</p>
</td></tr>
<tr><td><code id="ncases_desc_+3A_inclusive">inclusive</code></td>
<td>
<p>logical vector of length 1 specifying whether the case
should be included if the frequency of observed values in a row is exactly
equal to <code>ov.min</code>.</p>
</td></tr>
<tr><td><code id="ncases_desc_+3A_interp">interp</code></td>
<td>
<p>logical vector of length 1 specifying whether the median should
be standard (FALSE) or interpolated (TRUE).</p>
</td></tr>
<tr><td><code id="ncases_desc_+3A_skew">skew</code></td>
<td>
<p>logical vector of length 1 specifying whether skewness and
kurtosis should be calculated (TRUE) or not (FALSE).</p>
</td></tr>
<tr><td><code id="ncases_desc_+3A_ranges">ranges</code></td>
<td>
<p>logical vector of length 1 specifying whether the minimum,
maximum, and range (i.e., maximum - minimum) should be calculated (TRUE) or
not (FALSE). Note, if <code>ranges</code> = FALSE, the trimmed mean and median
absolute deviation is also not computed as per the <code>psych::describe</code>
function behavior.</p>
</td></tr>
<tr><td><code id="ncases_desc_+3A_trim">trim</code></td>
<td>
<p>numeric vector of length 1 specifying the top and bottom
quantiles of data that are to be excluded when calculating the trimmed
mean. For example, the default value of 0.1 means that only data within the
10th - 90th quantiles are used for calculating the trimmed mean.</p>
</td></tr>
<tr><td><code id="ncases_desc_+3A_type">type</code></td>
<td>
<p>numeric vector of length 1 specifying the type of skewness and
kurtosis coefficients to compute. See the details of
<code>psych::describe</code>. The options are 1, 2, or 3.</p>
</td></tr>
<tr><td><code id="ncases_desc_+3A_quant">quant</code></td>
<td>
<p>numeric vector specifying the quantiles to compute. Foe example,
the default value of c(0.25, 0.75) computes the 25th and 75th quantiles of
the group number of cases. If <code>quant</code> = NULL, then no quantiles are
returned.</p>
</td></tr>
<tr><td><code id="ncases_desc_+3A_iqr">IQR</code></td>
<td>
<p>logical vector of length 1 specifying whether to compute the
Interquartile Range (TRUE) or not (FALSE), which is simply the 75th quantil
- 25th quantile.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>numeric vector containing descriptive statistics about number of cases by group.
Note, which elements are returned depends on the arguments. See each argument's description.
</p>

<dl>
<dt>n</dt><dd><p>number of groups</p>
</dd>
<dt>mean</dt><dd><p>mean</p>
</dd>
<dt>sd</dt><dd><p>standard deviation</p>
</dd>
<dt>median</dt><dd><p>median (standard if <code>interp</code> = FALSE, interpolated if <code>interp</code> = TRUE)</p>
</dd>
<dt>trimmed</dt><dd><p>trimmed mean based on <code>trim</code></p>
</dd>
<dt>mad</dt><dd><p>median absolute difference</p>
</dd>
<dt>min</dt><dd><p>minimum</p>
</dd>
<dt>max</dt><dd><p>maximum</p>
</dd>
<dt>range</dt><dd><p>maximum - minumum</p>
</dd>
<dt>skew</dt><dd><p>skewness</p>
</dd>
<dt>kurtosis</dt><dd><p>kurtosis</p>
</dd>
<dt>se</dt><dd><p>standard error of the mean</p>
</dd>
<dt>IQR</dt><dd><p>75th quantile - 25th quantile</p>
</dd>
<dt>QX.XX</dt><dd><p>quantiles, which are named by <code>quant</code> (e.g., 0.25 = &quot;Q0.25&quot;)</p>
</dd>
</dl>



<h3>See Also</h3>

<p><code><a href="#topic+ncases_by">ncases_by</a></code>
<code><a href="psych.html#topic+describe">describe</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tmp_nm &lt;- c("outcome","case","session","trt_time")
dat &lt;- as.data.frame(lmeInfo::Bryant2016)[tmp_nm]
stats_by &lt;- psych::statsBy(dat, group = "case") # doesn't include everything you want
ncases_desc(data = dat, grp.nm = "case")
dat2 &lt;- as.data.frame(ChickWeight)
ncases_desc(data = dat2, grp.nm = "Chick")
ncases_desc(data = dat2, grp.nm = "Chick", trim = .05)
ncases_desc(data = dat2, grp.nm = "Chick", ranges = FALSE)
ncases_desc(data = dat2, grp.nm = "Chick", quant = NULL)
ncases_desc(data = dat2, grp.nm = "Chick", IQR = TRUE)
</code></pre>

<hr>
<h2 id='ncases_ml'>Multilevel Number of Cases</h2><span id='topic+ncases_ml'></span>

<h3>Description</h3>

<p><code>ncases_ml</code> computes the number cases and number of groups in the data
that are at least partially observed, given a specified frequency of observed
values across a set of columns. <code>ncases_ml</code> allows the user to specify
the frequency of columns that need to be observed in order to count the case.
Groups can be excluded if no rows in the data for a group have enough
observed values to be counted as cases. This is simply a combination of
<code>partial.cases</code> + <code>nrow_ml</code>. Note, <code>ncases_ml</code> is essentially
a version of <code><a href="#topic+nrow_ml">nrow_ml</a></code> that accounts for missing data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ncases_ml(
  data,
  vrb.nm = str2str::pick(names(data), val = grp.nm, not = TRUE),
  grp.nm,
  ov.min = 1L,
  prop = TRUE,
  inclusive = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ncases_ml_+3A_data">data</code></td>
<td>
<p>data.frame of data.</p>
</td></tr>
<tr><td><code id="ncases_ml_+3A_vrb.nm">vrb.nm</code></td>
<td>
<p>a character vector of colnames from <code>data</code> specifying the
variables which will be used to determine the partially observed cases.</p>
</td></tr>
<tr><td><code id="ncases_ml_+3A_grp.nm">grp.nm</code></td>
<td>
<p>character vector of colnames from <code>data</code> specifying the
grouping variables.</p>
</td></tr>
<tr><td><code id="ncases_ml_+3A_ov.min">ov.min</code></td>
<td>
<p>minimum frequency of observed values required per row. If
<code>prop</code> = TRUE, then this is a decimal between 0 and 1. If <code>prop</code>
= FALSE, then this is a integer between 0 and <code>length(vrb.nm)</code>.</p>
</td></tr>
<tr><td><code id="ncases_ml_+3A_prop">prop</code></td>
<td>
<p>logical vector of length 1 specifying whether <code>ov.min</code>
should refer to the proportion of observed values (TRUE) or the count of
observed values (FALSE).</p>
</td></tr>
<tr><td><code id="ncases_ml_+3A_inclusive">inclusive</code></td>
<td>
<p>logical vector of length 1 specifying whether the case
should be included if the frequency of observed values in a row is exactly
equal to <code>ov.min</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list with two elements providing the sample sizes (accouning for
missing data). The first element is named &quot;within&quot; and contains the number
of cases in the data. The second element is named &quot;between&quot; and contains
the number of groups in the data. Cases are counted if if the frequency of
observed values is greater than (or equal to, if <code>inclusive</code> = TRUE).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+nrow_ml">nrow_ml</a></code>
<code><a href="#topic+ncases_by">ncases_by</a></code>
<code><a href="#topic+partial.cases">partial.cases</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# NO MISSING DATA

# one grouping variable
ncases_ml(data = as.data.frame(ChickWeight), grp.nm = "Chick")

# multiple grouping variables
ncases_ml(data = mtcars, grp.nm = c("vs","am"))

# YES MISSING DATA

# only within
nrow_ml(data = airquality, grp.nm = "Month")
ncases_ml(data = airquality, grp.nm = "Month")

# both within and between
airquality2 &lt;- airquality
airquality2[airquality2$"Month" == 6, "Ozone"] &lt;- NA
nrow_ml(data = airquality2, grp.nm = "Month")
ncases_ml(data = airquality2, grp.nm = "Month")

</code></pre>

<hr>
<h2 id='ngrp'>Number of Groups in Data</h2><span id='topic+ngrp'></span>

<h3>Description</h3>

<p><code>ngrp</code> computes the number of groups in data given one or more grouping
variables. This is simply a combination of <code>unique.data.frame</code> +
<code>nrow</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ngrp(data, grp.nm)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ngrp_+3A_data">data</code></td>
<td>
<p>data.frame of data.</p>
</td></tr>
<tr><td><code id="ngrp_+3A_grp.nm">grp.nm</code></td>
<td>
<p>character vector of colnames from <code>data</code> specifying the
grouping variables.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>integer vector of length 1 specifying the number of groups.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+nrow_ml">nrow_ml</a></code>
<code><a href="#topic+ncases_ml">ncases_ml</a></code>
<code><a href="#topic+nrow_by">nrow_by</a></code>
<code><a href="#topic+ncases_by">ncases_by</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# one grouping variable
Orthodont2 &lt;- as.data.frame(nlme::Orthodont)
ngrp(Orthodont2, grp.nm = "Subject")
length(unique(Orthodont2$"Subject"))

# two grouping variable
co2 &lt;- as.data.frame(CO2)
ngrp(co2, grp.nm = c("Plant"))
grp_nm &lt;- c("Type","Treatment")
ngrp(co2, grp.nm = grp_nm)
unique.data.frame(co2[grp_nm])

#TODO: how does it handle factor levels with no cases?

</code></pre>

<hr>
<h2 id='nhst'>Null Hypothesis Significance Testing</h2><span id='topic+nhst'></span>

<h3>Description</h3>

<p><code>nhst</code> computes the statistical information for null hypothesis
significance testing (NHST), t-values, p-values, etc., from parameter
estimates, standard errors, and degrees of freedom. If degrees of freedom are
not applicable or available, then <code>df</code> can be set to <code>Inf</code> (the
default) and z-values rather than t-values will be computed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nhst(est, se, df = Inf, ci.level = 0.95, p.value = "two.sided")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nhst_+3A_est">est</code></td>
<td>
<p>numeric vector of parameter estimates.</p>
</td></tr>
<tr><td><code id="nhst_+3A_se">se</code></td>
<td>
<p>numeric vector of standard errors. Must be the same length as
<code>est</code>.</p>
</td></tr>
<tr><td><code id="nhst_+3A_df">df</code></td>
<td>
<p>numeric vector of degrees of freedom. Must be length of 1 or have
same length as <code>est</code> and <code>se</code>. If degrees of freedom are not
applicable or available, then <code>df</code> can be set to <code>Inf</code> (the
default) and z-values rather than t-values will be computed. Note,
<code>df</code> can be non-integers with decimals.</p>
</td></tr>
<tr><td><code id="nhst_+3A_ci.level">ci.level</code></td>
<td>
<p>double vector of length 1 specifying the confidence level.
Must be between 0 and 1 - or can be NULL in which case no confidence
intervals are computed and the return object does not have the columns
&quot;lwr&quot; or &quot;upr&quot;.</p>
</td></tr>
<tr><td><code id="nhst_+3A_p.value">p.value</code></td>
<td>
<p>character vector of length 1 specifying the type of p-values
to compute. The options are 1) &quot;two.sided&quot; which computed non-directional,
two-tailed p-values, 2) &quot;less&quot;, which computes negative-directional,
one-tailed p-values, or 3) &quot;greater&quot;, which computes positive-directional,
one-tailed p-values.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data.frame with nrow equal to the lengths of <code>est</code> and
<code>se</code>. The rownames are taken from <code>est</code>, unless <code>est</code> does not
have any names and then the rownames are taken from the names of <code>se</code>.
If neither have names, then the rownames are automatic (i.e.,
<code>1:nrow()</code>). The columns are the following:
</p>

<dl>
<dt>est</dt><dd><p>parameter estimates</p>
</dd>
<dt>se</dt><dd><p>standard errors</p>
</dd>
<dt>t</dt><dd><p>t-values (z-values if df = Inf)</p>
</dd>
<dt>df</dt><dd><p>degrees of freedom</p>
</dd>
<dt>p</dt><dd><p>p-values</p>
</dd>
<dt>lwr</dt><dd><p>lower bound of the confidence intervals (excluded if <code>ci.level =  NULL</code>)</p>
</dd>
<dt>upr</dt><dd><p>upper bound of the confidence intervals (excluded if <code>ci.level =  NULL</code>)</p>
</dd>
</dl>



<h3>See Also</h3>

<p><code><a href="#topic+confint2.default">confint2.default</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
est &lt;- colMeans(attitude)
se &lt;- apply(X = str2str::d2m(attitude), MARGIN = 2, FUN = function(vec)
   sqrt(var(vec) / length(vec)))
df &lt;- nrow(attitude) - 1
nhst(est = est, se = se, df = df)
nhst(est = est, se = se) # default is df = Inf resulting in z-values
nhst(est = est, se = se, df = df, ci.level = NULL) # no "lwr" or "upr" columns
nhst(est = est, se = se, df = df, ci.level = 0.99)

</code></pre>

<hr>
<h2 id='nom2dum'>Nominal Variable to Dummy Variables</h2><span id='topic+nom2dum'></span>

<h3>Description</h3>

<p><code>nom2dum</code> converts a nominal variable into a set of dummy variables.
There is one dummy variable for each unique value in the nominal variable.
Note, base R does this recoding internally through the
<code>model.matrix.default</code> function, but it is used in the context of
regression-like models and it is not clear how to simplify it for general use
cases outside that context.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nom2dum(nom, yes = 1L, no = 0L, prefix = "", rtn.fct = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nom2dum_+3A_nom">nom</code></td>
<td>
<p>character vector (or any atomic vector, including factors, which
will be then coerced to a character vector) specifying the nominal
variable.</p>
</td></tr>
<tr><td><code id="nom2dum_+3A_yes">yes</code></td>
<td>
<p>atomic vector of length 1 specifying what unique value should
represent rows when the nominal category of interest is present. For a
traditional dummy variable this value would be 1.</p>
</td></tr>
<tr><td><code id="nom2dum_+3A_no">no</code></td>
<td>
<p>atomic vector of length 1 specifying what unique value should
represent rows when the nominal category of interest is absent. For a
traditional dummy variable this value would be 0.</p>
</td></tr>
<tr><td><code id="nom2dum_+3A_prefix">prefix</code></td>
<td>
<p>character vector of length 1 specifying the string that should
be appended to the beginning of each colname in the return object.</p>
</td></tr>
<tr><td><code id="nom2dum_+3A_rtn.fct">rtn.fct</code></td>
<td>
<p>logical vector of length 1 specifying whether the columns of
the return object should be factors where the first level is <code>no</code> and
the second level is <code>yes</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note, that <code>yes</code> and <code>no</code> are assumed to be the same typeof. If
they are not, then the columns in the return object will be coerced to the
most complex typeof (i.e., most to least: character, double, integer,
logical).
</p>


<h3>Value</h3>

<p>data.frame of dummy columns with colnames specified by
<code>paste0(prefix, unique(nom))</code> and rownames specified by
<code>names(nom)</code> or default <code>data.frame</code> rownames (i.e.,
c(&quot;1&quot;,&quot;2&quot;,&quot;3&quot;, etc.) if <code>names(nom)</code> is <code>NULL</code>.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+model.matrix.default">model.matrix.default</a></code>
<code><a href="#topic+dum2nom">dum2nom</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>nom2dum(infert$"education") # default
nom2dum(infert$"education", prefix = "edu_") # use of the `prefix` argument
nom2dum(nom = infert$"education", yes = "one", no = "zero",
   rtn.fct = TRUE) # returns factor columns
</code></pre>

<hr>
<h2 id='nrow_by'>Number of Rows in Data by Group</h2><span id='topic+nrow_by'></span>

<h3>Description</h3>

<p><code>nrow_by</code> computes the nrow of a data.frame by group. <code>nrow_by</code> is
simply a wrapper for <code>nrow</code> + <code>agg_dfm</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nrow_by(data, grp.nm, sep = ".")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nrow_by_+3A_data">data</code></td>
<td>
<p>data.frame of data.</p>
</td></tr>
<tr><td><code id="nrow_by_+3A_grp.nm">grp.nm</code></td>
<td>
<p>character vector of colnames from <code>data</code> specifying the
grouping variables.</p>
</td></tr>
<tr><td><code id="nrow_by_+3A_sep">sep</code></td>
<td>
<p>character vector of length 1 specifying what string to use to
separate the groups when naming the return object. <code>sep</code> is only used
if <code>grp.nm</code> has length &gt; 1 (aka multiple grouping variables)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>atomic vector with names = <code>unique(interaction(data[grp.nm], sep
  = sep))</code> and length = <code>length(unique(interaction(data[grp.nm], sep =
  sep)))</code> providing the nrow for each group.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ncases_by">ncases_by</a></code>
<code><a href="base.html#topic+nrow">nrow</a></code>
<code><a href="#topic+agg_dfm">agg_dfm</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# one grouping variables
tmp_nm &lt;- c("outcome","case","session","trt_time")
dat &lt;- as.data.frame(lmeInfo::Bryant2016)[tmp_nm]
stats_by &lt;- psych::statsBy(dat,
   group = "case") # requires you to include "case" column in dat
nrow_by(data = dat, grp.nm = "case")
dat2 &lt;- as.data.frame(ChickWeight)
nrow_by(data = dat2, grp.nm = "Chick")

# two grouping variables
tmp &lt;- reshape(psych::bfi[1:10, ], varying = 1:25, timevar = "item",
   ids = row.names(psych::bfi)[1:10], direction = "long", sep = "")
tmp_nm &lt;- c("id","item","N","E","C","A","O") # Roxygen runs the whole script
dat3 &lt;- str2str::stack2(tmp[tmp_nm], select.nm = c("N","E","C","A","O"),
   keep.nm = c("id","item"))
nrow_by(dat3, grp.nm = c("id","vrb_names"))

</code></pre>

<hr>
<h2 id='nrow_ml'>Multilevel Number of Rows</h2><span id='topic+nrow_ml'></span>

<h3>Description</h3>

<p><code>nrow_ml</code> computes the number rows in the data as well as the number of
groups in the data. This corresponds to the within-group sample size and
between-group sample size (ignoring any missing data). This is simply a
combination of <code>nrow</code> + <code>ngrp</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nrow_ml(data, grp.nm)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nrow_ml_+3A_data">data</code></td>
<td>
<p>data.frame of data.</p>
</td></tr>
<tr><td><code id="nrow_ml_+3A_grp.nm">grp.nm</code></td>
<td>
<p>character vector of colnames from <code>data</code> specifying the
grouping variables.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list with two elements providing the sample sizes (ignoring missing
data). The first element is named &quot;within&quot; and contains the number of rows
in the data. The second element is named &quot;between&quot; and contains the number
of groups in the data.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ncases_ml">ncases_ml</a></code>
<code><a href="#topic+nrow_by">nrow_by</a></code>
<code><a href="#topic+ncases_by">ncases_by</a></code>
<code><a href="#topic+ngrp">ngrp</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# one grouping variable
nrow_ml(data = as.data.frame(ChickWeight), grp.nm = "Chick")

# multiple grouping variables
nrow_ml(data = mtcars, grp.nm = c("vs","am"))

</code></pre>

<hr>
<h2 id='partial.cases'>Find Partial Cases</h2><span id='topic+partial.cases'></span>

<h3>Description</h3>

<p><code>partial.cases</code> indicates which cases are at least partially observed,
given a specified frequency of observed values across a set of columns. This
function builds off <code><a href="stats.html#topic+complete.cases">complete.cases</a></code>. While
<code>complete.cases</code> requires completely observed cases,
<code>partial.cases</code> allows the user to specify the frequency of columns
required to be observed. The default arguments are equal to
<code>complete.cases</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>partial.cases(data, vrb.nm, ov.min = 1, prop = TRUE, inclusive = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="partial.cases_+3A_data">data</code></td>
<td>
<p>data.frame or matrix of data.</p>
</td></tr>
<tr><td><code id="partial.cases_+3A_vrb.nm">vrb.nm</code></td>
<td>
<p>a character vector of colnames from <code>data</code> specifying the
variables which will be used to determine the partially observed cases.</p>
</td></tr>
<tr><td><code id="partial.cases_+3A_ov.min">ov.min</code></td>
<td>
<p>minimum frequency of observed values required per row. If
<code>prop</code> = TRUE, then this is a decimal between 0 and 1. If <code>prop</code>
= FALSE, then this is a integer between 0 and <code>length(vrb.nm)</code>.</p>
</td></tr>
<tr><td><code id="partial.cases_+3A_prop">prop</code></td>
<td>
<p>logical vector of length 1 specifying whether <code>ov.min</code>
should refer to the proportion of observed values (TRUE) or the count of
observed values (FALSE).</p>
</td></tr>
<tr><td><code id="partial.cases_+3A_inclusive">inclusive</code></td>
<td>
<p>logical vector of length 1 specifying whether the case
should be included if the frequency of observed values in a row is exactly
equal to <code>ov.min</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>logical vector of length = <code>nrow(data)</code> with names =
<code>rownames(data)</code> specifying if the frequency of observed values is
greater than (or equal to, if <code>inclusive</code> = TRUE) <code>ov.min</code>.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+complete.cases">complete.cases</a></code>
<code><a href="#topic+rowNA">rowNA</a></code>
<code><a href="#topic+ncases">ncases</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>cases2keep &lt;- partial.cases(data = airquality,
   vrb.nm = c("Ozone","Solar.R","Wind"), ov.min = .66)
airquality2 &lt;- airquality[cases2keep, ] # all cases with 2/3 variables observed
cases2keep &lt;- partial.cases(data = airquality,
   vrb.nm = c("Ozone","Solar.R","Wind"), ov.min = 1, prop = TRUE, inclusive = TRUE)
complete_cases &lt;- complete.cases(airquality)
identical(x = unname(cases2keep),
   y = complete_cases) # partial.cases(ov.min = 1, prop = TRUE,
   # inclusive = TRUE) = complete.cases()
</code></pre>

<hr>
<h2 id='pomp'>Recode a Numeric Vector to Percentage of Maximum Possible (POMP) Units</h2><span id='topic+pomp'></span>

<h3>Description</h3>

<p><code>pomp</code> recodes a numeric vector to percentage of maximum possible (POMP)
units. This can be useful when data is measured with arbitrary units (e.g.,
Likert scale).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pomp(x, mini, maxi, relative = FALSE, unit = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pomp_+3A_x">x</code></td>
<td>
<p>numeric vector.</p>
</td></tr>
<tr><td><code id="pomp_+3A_mini">mini</code></td>
<td>
<p>numeric vector of length 1 specifying the minimum numeric value
possible.</p>
</td></tr>
<tr><td><code id="pomp_+3A_maxi">maxi</code></td>
<td>
<p>numeric vector of length 1 specifying the maximum numeric value
possible.</p>
</td></tr>
<tr><td><code id="pomp_+3A_relative">relative</code></td>
<td>
<p>logical vector of length 1 specifying whether relative POMP
scores (rather than absolute POMP scores) should be created. If TRUE, then
the <code>mini</code> and <code>maxi</code> arguments are ignored. See details for the
distinction between absolute and relative POMP scores.</p>
</td></tr>
<tr><td><code id="pomp_+3A_unit">unit</code></td>
<td>
<p>numeric vector of length 1 specifying how many percentage points
is desired for the units. Traditionally, POMP scores use <code>unit</code> = 1
(default) such that one unit is one percentage point. However, another
option is to use <code>unit</code> = 100 such that one unit is all 100 percentage
points (i.e., proportion of maximum possible). This argument also gives the
flexibility of specifying units in between 1 and 100 percentage points. For
example, <code>unit</code> = 50 would mean that one unit represents going from
low (i.e., 25th percentile) to high (i.e., 75th percentile) on the
variable.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>There are too common approaches to POMP scores: 1) absolute POMP units where
the minimum and maximum are the smallest/largest values possible from the
measurement instrument (e.g., 1 to 7 on a Likert scale) and 2) relative POMP
units where the minimum and maximum are the smallest/largest values observed
in the data (e.g., 1.3 to 6.8 on a Likert scale). Both will be correlated
perfectly with the original units as they are each linear transformations.
</p>


<h3>Value</h3>

<p>numeric vector from recoding <code>x</code> to percentage of maximum
possible (pomp) with units specified by <code>unit</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pomps">pomps</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>vec &lt;- psych::bfi[[1]]
pomp(x = vec, mini = 1, maxi = 6) # absolute POMP units
pomp(x = vec, relative = TRUE) # relative POMP units
pomp(x = vec, mini = 1, maxi = 6, unit = 100) # unit = 100
pomp(x = vec, mini = 1, maxi = 6, unit = 50) # unit = 50
</code></pre>

<hr>
<h2 id='pomps'>Recode Numeric Data to Percentage of Maximum Possible (POMP) Units</h2><span id='topic+pomps'></span>

<h3>Description</h3>

<p><code>pomps</code> recodes numeric data to percentage of maximum possible (POMP)
units. This can be useful when data is measured with arbitrary units (e.g.,
Likert scale).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pomps(
  data,
  vrb.nm,
  mini,
  maxi,
  relative = FALSE,
  unit = 1,
  suffix = paste0("_p", unit)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pomps_+3A_data">data</code></td>
<td>
<p>data.frame of data.</p>
</td></tr>
<tr><td><code id="pomps_+3A_vrb.nm">vrb.nm</code></td>
<td>
<p>character vector of colnames from <code>data</code> specifying the
variables.</p>
</td></tr>
<tr><td><code id="pomps_+3A_mini">mini</code></td>
<td>
<p>numeric vector of length 1 specifying the minimum numeric value
possible. Note, this is assumed to be the same for each variable.</p>
</td></tr>
<tr><td><code id="pomps_+3A_maxi">maxi</code></td>
<td>
<p>numeric vector of length 1 specifying the maximum numeric value
possible. Note, this is assumed to be the same for each variable.</p>
</td></tr>
<tr><td><code id="pomps_+3A_relative">relative</code></td>
<td>
<p>logical vector of length 1 specifying whether relative POMP
scores (rather than absolute POMP scores) should be created. If TRUE, then
the <code>mini</code> and <code>maxi</code> arguments are ignored. See details for the
distinction between absolute and relative POMP scores.</p>
</td></tr>
<tr><td><code id="pomps_+3A_unit">unit</code></td>
<td>
<p>numeric vector of length 1 specifying how many percentage points
is desired for the units. Traditionally, POMP scores use <code>unit</code> = 1
(default) such that one unit is one percentage point. However, another
option is to use <code>unit</code> = 100 such that one unit is all 100 percentage
points (i.e., proportion of maximum possible). This argument also gives the
flexibility of specifying units in between 1 and 100 percentage points. For
example, <code>unit</code> = 50 would mean that one unit represents going from
low (i.e., 25th percentile) to high (i.e., 75th percentile) on the
variable.</p>
</td></tr>
<tr><td><code id="pomps_+3A_suffix">suffix</code></td>
<td>
<p>character vector of length 1 specifying the string to add to
the end of the column names in the return object.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>There are too common approaches to POMP scores: 1) absolute POMP units where
the minimum and maximum are the smallest/largest values possible from the
measurement instrument (e.g., 1 to 7 on a Likert scale) and 2) relative POMP
units where the minimum and maximum are the smallest/largest values observed
in the data (e.g., 1.3 to 6.8 on a Likert scale). Both will be correlated
perfectly with the original units as they are each linear transformations.
</p>


<h3>Value</h3>

<p>data.frame of variables recoded to percentage of maximum possible
(pomp) with units specified by <code>unit</code> and names specified by
<code>paste0(vrb.nm, suffix)</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pomp">pomp</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>vrb_nm &lt;- names(psych::bfi)[grepl(pattern = "A", x = names(psych::bfi))]
pomps(data = psych::bfi, vrb.nm = vrb_nm, min = 1, max = 6) # absolute POMP units
pomps(data = psych::bfi, vrb.nm = vrb_nm, relative = TRUE) # relative POMP units
pomps(data = psych::bfi, vrb.nm = vrb_nm, min = 1, max = 6, unit = 100) # unit = 100
pomps(data = psych::bfi, vrb.nm = vrb_nm, min = 1, max = 6, unit = 50) # unit = 50
pomps(data = psych::bfi, vrb.nm = vrb_nm, min = 1, max = 6, suffix = "_pomp")
</code></pre>

<hr>
<h2 id='prop_compare'>Proportion Comparisons for a Single Variable across 3+ Independent Groups
(Chi-square Test of Independence)</h2><span id='topic+prop_compare'></span>

<h3>Description</h3>

<p><code>prop_compare</code> tests for proportion differences across 3+ independent
groups with a chi-square test of independence. The function also calculates
the descriptive statistics for each group, Cramer's V and its confidence
interval as a standardized effect size, and can provide the X by 2
contingency tables. <code>prop_compare</code> is simply a wrapper for
<code><a href="stats.html#topic+prop.test">prop.test</a></code> plus some extra calculations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prop_compare(
  x,
  nom,
  lvl = levels(as.factor(nom)),
  yates = TRUE,
  ci.level = 0.95,
  rtn.table = TRUE,
  check = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prop_compare_+3A_x">x</code></td>
<td>
<p>numeric vector that only has values of 0 or 1 (or missing values),
otherwise known as a dummy variable.</p>
</td></tr>
<tr><td><code id="prop_compare_+3A_nom">nom</code></td>
<td>
<p>atomic vector that takes on three or more unordered values (or
missing values), otherwise known as a nominal variable.</p>
</td></tr>
<tr><td><code id="prop_compare_+3A_lvl">lvl</code></td>
<td>
<p>character vector with length 2 specifying the unique values for
the two groups. If <code>nom</code> is a factor, then <code>lvl</code> should be the
factor levels rather than the underlying integer codes. This argument
allows you to specify order of the proportions in the return object.</p>
</td></tr>
<tr><td><code id="prop_compare_+3A_yates">yates</code></td>
<td>
<p>logical vector of length 1 specifying whether the Yate's
continuity correction should be applied for small samples. See
<code><a href="stats.html#topic+chisq.test">chisq.test</a></code> for details.</p>
</td></tr>
<tr><td><code id="prop_compare_+3A_ci.level">ci.level</code></td>
<td>
<p>numeric vector of length 1 specifying the confidence level.
<code>ci.level</code> must range from 0 to 1.</p>
</td></tr>
<tr><td><code id="prop_compare_+3A_rtn.table">rtn.table</code></td>
<td>
<p>logical vector of lengh 1 specifying whether the return
object should include the X by 2 contingency table of counts with totals
and the X by 2 overall percentages table. If TRUE, then the last two
elements of the return object are &quot;count&quot; containing a matrix of counts and
&quot;percent&quot; containing a matrix of overall percentages.</p>
</td></tr>
<tr><td><code id="prop_compare_+3A_check">check</code></td>
<td>
<p>logical vector of length 1 specifying whether the input
arguments should be checked for errors. For example, if <code>nom</code> has
length different than the length of <code>x</code>. This is a tradeoff between
computational efficiency (FALSE) and more useful error messages (TRUE).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The confidence interval for Cramer's V is calculated with fisher's r to z
transformation as Cramer's V is a kind of multiple correlation coefficient.
Cramer's V is transformed to fisher's z units, a symmetric confidence
interval for fisher's z is calculated, and then the lower and upper bounds
are back-transformed to Cramer's V units.
</p>


<h3>Value</h3>

<p>list of numeric vectors containing statistical information about the
proportion comparisons: 1) nhst = chi-square test of independence stat info
in a numeric vector, 2) desc = descriptive statistics stat info in a
numeric vector, 3) std = standardized effect size and its confidence
interval in a numeric vector, 4) count = numeric matrix with dim =
<code>[X+1, 3]</code> of the X by 2 contingency table of counts with an
additional row and column for totals (if <code>rtn.table</code> = TRUE), 5)
percent = numeric matrix with dim = <code>[X+1, 3]</code> of the X by 2
contingency table of overall percentages with an additional row and column
for totals (if <code>rtn.table</code> = TRUE).
</p>
<p>1) nhst = chi-square test of independence stat info in a numeric vector
</p>

<dl>
<dt>est</dt><dd><p>average proportion difference absolute value (i.e., |group j - group i|)</p>
</dd>
<dt>se</dt><dd><p>NA (to remind the user there is no standard error for the test)</p>
</dd>
<dt>X2</dt><dd><p>chi-square value</p>
</dd>
<dt>df</dt><dd><p>degrees of freedom (of the nominal variable)</p>
</dd>
<dt>p</dt><dd><p>two-sided p-value</p>
</dd>
</dl>

<p>2) desc = descriptive statistics stat info in a numeric vector (note there
could be more than 3 groups - groups i, j, and k are just provided as an example):
</p>

<dl>
<dt>prop_'lvl[k]'</dt><dd><p>proportion of group k</p>
</dd>
<dt>prop_'lvl[j]'</dt><dd><p>proportion of group j</p>
</dd>
<dt>prop_'lvl[i]'</dt><dd><p>proportion of group i</p>
</dd>
<dt>sd_'lvl[k]'</dt><dd><p>standard deviation of group k</p>
</dd>
<dt>sd_'lvl[j]'</dt><dd><p>standard deviation of group j</p>
</dd>
<dt>sd_'lvl[i]'</dt><dd><p>standard deviation of group i</p>
</dd>
<dt>n_'lvl[k]'</dt><dd><p>sample size of group k</p>
</dd>
<dt>n_'lvl[j]'</dt><dd><p>sample size of group j</p>
</dd>
<dt>n_'lvl[i]'</dt><dd><p>sample size of group i</p>
</dd>
</dl>

<p>3) std = standardized effect size and its confidence interval in a numeric vector
</p>

<dl>
<dt>cramer</dt><dd><p>Cramer's V estimate</p>
</dd>
<dt>lwr</dt><dd><p>lower bound of Cramer's V confidence interval</p>
</dd>
<dt>upr</dt><dd><p>upper bound of Cramer's V confidence interval</p>
</dd>
</dl>

<p>4) count = numeric matrix with dim = <code>[X+1, 3]</code> of the X by 2
contingency table of counts with an additional row and column for totals (if
<code>rtn.table</code> = TRUE).
</p>
<p>The 3+ unique observed values of <code>nom</code> - plus the total - are the rows
and the two unique observed values of <code>x</code> (i.e., 0 and 1) - plus the
total - are the columns. The dimlabels are &quot;nom&quot; for the rows and &quot;x&quot; for the
columns. The rownames are 1. 'lvl[i]', 2. 'lvl[j]', 3. 'lvl[k]', 4. &quot;total&quot;.
The colnames are 1. &quot;0&quot;, 2. &quot;1&quot;, 3. &quot;total&quot;.
</p>
<p>5) percent = numeric matrix with dim = <code>[X+1, 3]</code> of the X by 2
contingency table of overall percentages with an additional row and column
for totals (if <code>rtn.table</code> = TRUE).
</p>
<p>The 3+ unique observed values of <code>nom</code> - plus the total - are the rows
and the two unique observed values of <code>x</code> (i.e., 0 and 1) - plus the
total - are the columns. The dimlabels are &quot;nom&quot; for the rows and &quot;x&quot; for the
columns. The rownames are 1. 'lvl[i]', 2. 'lvl[j]', 3. 'lvl[k]', 4. &quot;total&quot;.
The rownames are 1. &quot;0&quot;, 2. &quot;1&quot;, 3. &quot;total&quot;.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+prop.test">prop.test</a></code> the workhorse for <code>prop_compare</code>,
<code><a href="#topic+props_compare">props_compare</a></code> for multiple dummy variables,
<code><a href="#topic+prop_diff">prop_diff</a></code> for only 2 independent groups (aka binary variable),
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
tmp &lt;- replicate(n = 10, expr = mtcars, simplify = FALSE)
mtcars2 &lt;- str2str::ld2d(tmp)
mtcars2$"cyl_fct" &lt;- car::recode(mtcars2$"cyl",
   recodes = "4='four'; 6='six'; 8='eight'", as.factor = TRUE)
prop_compare(x = mtcars2$"am", nom = mtcars2$"cyl_fct")
prop_compare(x = mtcars2$"am", nom = mtcars2$"cyl_fct",
   lvl = c("four","six","eight")) # specify order of levels in return object

# more than 3 groups
prop_compare(x = ifelse(airquality$"Wind" &gt;= 10, yes = 1, no = 0), nom = airquality$"Month")
prop_compare(x = ifelse(airquality$"Wind" &gt;= 10, yes = 1, no = 0), nom = airquality$"Month",
   rtn.table = FALSE) # no contingency tables

</code></pre>

<hr>
<h2 id='prop_diff'>Proportion Difference for a Single Variable across Two Independent Groups
(Chi-square Test of Independence)</h2><span id='topic+prop_diff'></span>

<h3>Description</h3>

<p><code>prop_diff</code> tests for proportion differences across two independent
groups with a chi-square test of independence. The function also calculates
the descriptive statistics for each group, various standardized effect sizes
(e.g., Cramer's V), and can provide the 2x2 contingency tables.
<code>prop_diff</code> is simply a wrapper for <code><a href="stats.html#topic+prop.test">prop.test</a></code> plus
some extra calculations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prop_diff(
  x,
  bin,
  lvl = levels(as.factor(bin)),
  yates = TRUE,
  zero.cell = 0.05,
  smooth = TRUE,
  ci.level = 0.95,
  rtn.table = TRUE,
  check = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prop_diff_+3A_x">x</code></td>
<td>
<p>numeric vector that only has values of 0 or 1 (or missing values),
otherwise known as a dummy variable.</p>
</td></tr>
<tr><td><code id="prop_diff_+3A_bin">bin</code></td>
<td>
<p>atomic vector that only takes on two values (or missing values),
otherwise known as a binary variable.</p>
</td></tr>
<tr><td><code id="prop_diff_+3A_lvl">lvl</code></td>
<td>
<p>character vector with length 2 specifying the unique values for
the two groups. If <code>bin</code> is a factor, then <code>lvl</code> should be the
factor levels rather than the underlying integer codes. This argument
allows you to specify the direction of the prop difference.
<code>prop_diff</code> calculates the prop difference as <code>x[ bin == lvl[2]
]</code> - <code>x[ bin == lvl[1] ]</code> such that it is group 2 - group 1. By
changing which group is group 1 vs. group 2, the direction of the prop
difference can be changed. See details.</p>
</td></tr>
<tr><td><code id="prop_diff_+3A_yates">yates</code></td>
<td>
<p>logical vector of length 1 specifying whether the Yate's
continuity correction should be applied for small samples. See
<code><a href="stats.html#topic+chisq.test">chisq.test</a></code> for details.</p>
</td></tr>
<tr><td><code id="prop_diff_+3A_zero.cell">zero.cell</code></td>
<td>
<p>numeric vector of length 1 specifying what value to impute
for zero cell counts in the 2x2 contingency table when computing the
tetrachoric correlation. See <code><a href="psych.html#topic+tetrachoric">tetrachoric</a></code> for details.</p>
</td></tr>
<tr><td><code id="prop_diff_+3A_smooth">smooth</code></td>
<td>
<p>logical vector of length 1 specifying whether a smoothing
algorithm should be applied when estimating the tetrachoric correlation.
See <code><a href="psych.html#topic+tetrachoric">tetrachoric</a></code> for details.</p>
</td></tr>
<tr><td><code id="prop_diff_+3A_ci.level">ci.level</code></td>
<td>
<p>numeric vector of length 1 specifying the confidence level.
<code>ci.level</code> must range from 0 to 1.</p>
</td></tr>
<tr><td><code id="prop_diff_+3A_rtn.table">rtn.table</code></td>
<td>
<p>logical vector of lengh 1 specifying whether the return
object should include the 2x2 contingency table of counts with totals and
the 2x2 overall percentages table. If TRUE, then the last two elements of
the return object are &quot;count&quot; containing a matrix of counts and &quot;percent&quot;
containing a matrix of overall percentages.</p>
</td></tr>
<tr><td><code id="prop_diff_+3A_check">check</code></td>
<td>
<p>logical vector of length 1 specifying whether the input
arguments should be checked for errors. For example, if <code>bin</code> has more
than 2 unique values (other than missing values) or if <code>bin</code> has
length different than the length of <code>x</code>. This is a tradeoff between
computational efficiency (FALSE) and more useful error messages (TRUE).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list of numeric vectors containing statistical information about the
mean difference: 1) nhst = chi-square test of independence stat info in a numeric vector,
2) desc = descriptive statistics stat info in a numeric vector, 3) std = various
standardized effect sizes in a numeric vector, 4) count = numeric matrix with
dim = <code>[3, 3]</code> of the 2x2 contingency table of counts with an additional
row and column for totals (if <code>rtn.table</code> = TRUE), 5) percent = numeric
matrix with dim = <code>[3, 3]</code> of the 2x2 contingency table of overall percentages
with an additional row and column for totals (if <code>rtn.table</code> = TRUE)
</p>
<p>1) nhst = chi-square test of independence stat info in a numeric vector
</p>

<dl>
<dt>est</dt><dd><p>mean difference estimate (i.e., group 2 - group 1)</p>
</dd>
<dt>se</dt><dd><p>NA (to remind the user there is no standard error for the test)</p>
</dd>
<dt>X2</dt><dd><p>chi-square value</p>
</dd>
<dt>df</dt><dd><p>degrees of freedom (will always be 1)</p>
</dd>
<dt>p</dt><dd><p>two-sided p-value</p>
</dd>
<dt>lwr</dt><dd><p>lower bound of the confidence interval</p>
</dd>
<dt>upr</dt><dd><p>upper bound of the confidence interval</p>
</dd>
</dl>

<p>2) desc = descriptive statistics stat info in a numeric vector
</p>

<dl>
<dt>prop_'lvl[2]'</dt><dd><p>proportion of group 2</p>
</dd>
<dt>prop_'lvl[1]'</dt><dd><p>proportion of group 1</p>
</dd>
<dt>sd_'lvl[2]'</dt><dd><p>standard deviation of group 2</p>
</dd>
<dt>sd_'lvl[1]'</dt><dd><p>standard deviation of group 1</p>
</dd>
<dt>n_'lvl[2]'</dt><dd><p>sample size of group 2</p>
</dd>
<dt>n_'lvl[1]'</dt><dd><p>sample size of group 1</p>
</dd>
</dl>

<p>3) std = various standardized effect sizes in a numeric vector
</p>

<dl>
<dt>cramer</dt><dd><p>Cramer's V estimate</p>
</dd>
<dt>h</dt><dd><p>Cohen's h estimate</p>
</dd>
<dt>phi</dt><dd><p>Phi coefficient estimate</p>
</dd>
<dt>yule</dt><dd><p>Yule coefficient estimate</p>
</dd>
<dt>tetra</dt><dd><p>Tetrachoric correlation estimate</p>
</dd>
<dt>OR</dt><dd><p>odds ratio estimate</p>
</dd>
<dt>RR</dt><dd><p>risk ratio estimate calculated as (i.e., group 2 / group 1).
Note this value will often differ when recoding variables (as it should).</p>
</dd>
</dl>

<p>4) count = numeric matrix with dim = <code>[3, 3]</code> of the 2x2 contingency table of
counts with an additional row and column for totals (if <code>rtn.table</code> = TRUE).
</p>
<p>The two unique observed values of <code>x</code> (i.e., 0 and 1) - plus the
total - are the rows and the two unique observed values of <code>bin</code> - plus
the total - are the columns. The dimlabels are &quot;bin&quot; for the rows and &quot;x&quot; for
the columns. The rownames are 1. &quot;0&quot;, 2. &quot;1&quot;, 3. &quot;total&quot;. The colnames are 1.
'lvl[1]', 2. 'lvl[2]', 3. &quot;total&quot;
</p>
<p>5) percent = numeric matrix with dim = <code>[3, 3]</code> of the 2x2 contingency table of overall percentages with an additional
row and column for totals (if <code>rtn.table</code> = TRUE).
</p>
<p>The two unique observed values of <code>x</code> (i.e., 0 and 1) - plus the total -
are the rows and the two unique observed values of <code>bin</code> - plus the total -
are the columns. The dimlabels are &quot;bin&quot; for the rows and &quot;x&quot; for the columns.
The rownames are 1. &quot;0&quot;, 2. &quot;1&quot;, 3. &quot;total&quot;. The colnames are 1. 'lvl[1]',
2. 'lvl[2]', 3. &quot;total&quot;
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+prop.test">prop.test</a></code> the workhorse for <code>prop_diff</code>,
<code><a href="#topic+props_diff">props_diff</a></code> for multiple dummy variables,
<code><a href="psych.html#topic+phi">phi</a></code> for another phi coefficient function
<code><a href="psych.html#topic+Yule">Yule</a></code> for another yule coefficient function
<code><a href="psych.html#topic+tetrachoric">tetrachoric</a></code> for another tetrachoric coefficient function
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# chi-square test of independence
# x = "am", bin = "vs"
mtcars2 &lt;- mtcars
mtcars2$"vs_bin" &lt;- ifelse(mtcars$"vs" == 1, yes = "yes", no = "no")
agg(mtcars2$"am", grp = mtcars2$"vs_bin", rep = FALSE, fun = mean)
prop_diff(x = mtcars2$"am", bin = mtcars2$"vs_bin")
prop_diff(x = mtcars2$"am", bin = mtcars2$"vs")

# using \code{lvl} argument
prop_diff(x = mtcars2$"am", bin = mtcars2$"vs_bin")
prop_diff(x = mtcars2$"am", bin = mtcars2$"vs_bin",
   lvl = c("yes","no")) # reverses the direction of the effect
prop_diff(x = mtcars2$"am", bin = mtcars2$"vs",
   lvl = c(1, 0)) # levels don't have to be character

# recoding the variables
prop_diff(x = mtcars2$"am", bin = ifelse(mtcars2$"vs_bin" == "yes",
   yes = "no", no = "yes")) # reverses the direction of the effect
prop_diff(x = ifelse(mtcars2$"am" == 1, yes = 0, no = 1),
   bin = mtcars2$"vs") # reverses the direction of the effect
prop_diff(x = ifelse(mtcars2$"am" == 1, yes = 0, no = 1),
   bin = ifelse(mtcars2$"vs_bin" == "yes",
      yes = "no", no = "yes")) # double reverse means same direction of the effect

# compare to stats::prop.test
# x = "am", bin = "vs_bin" (binary as the rows; dummy as the columns)
tmp &lt;- c("vs_bin","am") # b/c Roxygen2 will cause problems
table_obj &lt;- table(mtcars2[tmp])
row_order &lt;- nrow(table_obj):1
col_order &lt;- ncol(table_obj):1
table_obj4prop &lt;- table_obj[row_order, col_order]
prop.test(table_obj4prop)

# compare to stats:chisq.test
chisq.test(x = mtcars2$"am", y = mtcars2$"vs_bin")

# compare to psych::phi
cor(mtcars2$"am", mtcars$"vs")
psych::phi(table_obj, digits = 7)

# compare to psych::yule()
psych::Yule(table_obj)

# compare to psych::tetrachoric
psych::tetrachoric(table_obj)
# Note, I couldn't find a case where psych::tetrachoric() failed to compute
psych::tetrachoric(table_obj4prop)

# different than single logistic regression
summary(glm(am ~ vs, data = mtcars, family = binomial(link = "logit")))

</code></pre>

<hr>
<h2 id='prop_test'>Test for Sample Proportion Against Pi (chi-square test of goodness of fit)</h2><span id='topic+prop_test'></span>

<h3>Description</h3>

<p><code>prop_test</code> tests for a sample proportion difference from a population
proportion with a chi-square test of goodness of fit. The default is that the
goodness of fit is consistent with a population proportion Pi of 0.50. The
function also calculates the descriptive statistics, various standardized
effect sizes (e.g., Cramer's V), and can provide the 1x2 contingency tables.
<code>prop_test</code> is simply a wrapper for <code><a href="stats.html#topic+prop.test">prop.test</a></code> plus
some extra calculations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prop_test(
  x,
  pi = 0.5,
  yates = TRUE,
  ci.level = 0.95,
  rtn.table = TRUE,
  check = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prop_test_+3A_x">x</code></td>
<td>
<p>numeric vector that only has values of 0 or 1 (or missing values),
otherwise known as a dummy variable.</p>
</td></tr>
<tr><td><code id="prop_test_+3A_pi">pi</code></td>
<td>
<p>numeric vector of length 1 specifying the population proportion
value to compare the sample proportion against.</p>
</td></tr>
<tr><td><code id="prop_test_+3A_yates">yates</code></td>
<td>
<p>logical vector of length 1 specifying whether the Yate's
continuity correction should be applied for small samples. See
<code><a href="stats.html#topic+chisq.test">chisq.test</a></code> for details.</p>
</td></tr>
<tr><td><code id="prop_test_+3A_ci.level">ci.level</code></td>
<td>
<p>numeric vector of length 1 specifying the confidence level.
<code>ci.level</code> must range from 0 to 1.</p>
</td></tr>
<tr><td><code id="prop_test_+3A_rtn.table">rtn.table</code></td>
<td>
<p>logical vector of lengh 1 specifying whether the return
object should include the 1x2 contingency table of counts with totals and
the 1x2 overall percentages table. If TRUE, then the last two elements of
the return object are &quot;count&quot; containing a vector of counts and &quot;percent&quot;
containing a vector of overall percentages.</p>
</td></tr>
<tr><td><code id="prop_test_+3A_check">check</code></td>
<td>
<p>logical vector of length 1 specifying whether the input
arguments should be checked for errors. For example, if <code>x</code> is a dummy
variable that only takes on value of 0 or 1 (or missing values). This is a
tradeoff between computational efficiency (FALSE) and more useful error
messages (TRUE).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list of numeric vectors containing statistical information about the
proportion difference from pi: 1) nhst = chi-square test of goodness of fit stat
info in a numeric vector, 2) desc = descriptive statistics stat info in a
numeric vector, 3) std = various standardized effect sizes in a numeric vector,
4) count = numeric vector of length 3 with table of counts with an additional
element for the total (if <code>rtn.table</code> = TRUE), 5) percent = numeric vector
of length 3 with table of overall percentages with an element for the total
(if <code>rtn.table</code> = TRUE)
</p>
<p>1) nhst = chi-square test of goodness of fit stat info in a numeric vector
</p>

<dl>
<dt>est</dt><dd><p>proportion difference estimate (i.e., sample proportion - pi)</p>
</dd>
<dt>se</dt><dd><p>NA (to remind the user there is no standard error for the test)</p>
</dd>
<dt>X2</dt><dd><p>chi-square value</p>
</dd>
<dt>df</dt><dd><p>degrees of freedom (will always be 1)</p>
</dd>
<dt>p</dt><dd><p>two-sided p-value</p>
</dd>
</dl>

<p>2) desc = descriptive statistics stat info in a numeric vector
</p>

<dl>
<dt>prop</dt><dd><p>sample proportion</p>
</dd>
<dt>pi</dt><dd><p>popularion proportion provided by the user (or 0.50 by default)</p>
</dd>
<dt>sd</dt><dd><p>standard deviation</p>
</dd>
<dt>n</dt><dd><p>sample size</p>
</dd>
<dt>lwr</dt><dd><p>lower bound of the confidence interval of the sample proportion itself</p>
</dd>
<dt>upr</dt><dd><p>upper bound of the confidence interval of the sample proportion itself</p>
</dd>
</dl>

<p>3) std = various standardized effect sizes in a numeric vector
</p>

<dl>
<dt>cramer</dt><dd><p>Cramer's V estimate</p>
</dd>
<dt>h</dt><dd><p>Cohen's h estimate</p>
</dd>
</dl>

<p>4) count = numeric vector of length 3 with table of counts with an additional
element for the total (if <code>rtn.table</code> = TRUE). The names are 1. &quot;0&quot;, 2.
&quot;1&quot;, 3. &quot;total&quot;
</p>
<p>5) percent = numeric vector of length 3 with table of overall percentages with
an element for the total (if <code>rtn.table</code> = TRUE). The names are 1. &quot;0&quot;, 2.
&quot;1&quot;, 3. &quot;total&quot;
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+prop.test">prop.test</a></code> the workhorse for <code>prop_test</code>,
<code><a href="#topic+props_test">props_test</a></code> for multiple dummy variables,
<code><a href="#topic+prop_diff">prop_diff</a></code> for chi-square test of independence,
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# chi-square test of goodness of fit
table(mtcars$"am")
prop_test(mtcars$"am")
prop_test(ifelse(mtcars$"am" == 1, yes = 0, no = 1))

# different than intercept only logistic regression
summary(glm(am ~ 1, data = mtcars, family = binomial(link = "logit")))

# error from non-dummy variable
## Not run: 
prop_test(ifelse(mtcars$"am" == 1, yes = "1", no = "0"))
prop_test(ifelse(mtcars$"am" == 1, yes = 2, no = 1))

## End(Not run)

</code></pre>

<hr>
<h2 id='props_compare'>Proportion Comparisons for Multiple Variables across 3+ Independent Groups
(Chi-square Tests of Independence)</h2><span id='topic+props_compare'></span>

<h3>Description</h3>

<p><code>prop_compare</code> tests for proportion differences across 3+ independent
groups with chi-square tests of independence. The function also calculates
the descriptive statistics for each group, Cramer's V and its confidence
interval as a standardized effect size, and can provide the X by 2
contingency tables. <code>prop_compare</code> is simply a wrapper for
<code><a href="stats.html#topic+prop.test">prop.test</a></code> plus some extra calculations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>props_compare(
  data,
  vrb.nm,
  nom.nm,
  lvl = levels(as.factor(data[[nom.nm]])),
  yates = TRUE,
  ci.level = 0.95,
  rtn.table = TRUE,
  check = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="props_compare_+3A_data">data</code></td>
<td>
<p>data.frame of data.</p>
</td></tr>
<tr><td><code id="props_compare_+3A_vrb.nm">vrb.nm</code></td>
<td>
<p>character vector of colnames from <code>data</code> specifying the
dummy variables, in other words, variables that only have values of 0 or 1
(or missing values).</p>
</td></tr>
<tr><td><code id="props_compare_+3A_nom.nm">nom.nm</code></td>
<td>
<p>character vector of length 1 specifying the colname in
<code>data</code> containing a nominal variable that takes on three or more
unordered values (or missing values).</p>
</td></tr>
<tr><td><code id="props_compare_+3A_lvl">lvl</code></td>
<td>
<p>character vector with length 3+ specifying the unique values for
the 3+ independent groups. If <code>nom</code> is a factor, then <code>lvl</code>
should be the factor levels rather than the underlying integer codes. This
argument allows you to specify order of the proportions in the return
object.</p>
</td></tr>
<tr><td><code id="props_compare_+3A_yates">yates</code></td>
<td>
<p>logical vector of length 1 specifying whether the Yate's
continuity correction should be applied for small samples. See
<code><a href="stats.html#topic+chisq.test">chisq.test</a></code> for details.</p>
</td></tr>
<tr><td><code id="props_compare_+3A_ci.level">ci.level</code></td>
<td>
<p>numeric vector of length 1 specifying the confidence level.
<code>ci.level</code> must range from 0 to 1.</p>
</td></tr>
<tr><td><code id="props_compare_+3A_rtn.table">rtn.table</code></td>
<td>
<p>logical vector of lengh 1 specifying whether the return
object should include the X by 2 contingency table of counts with totals
for each dummy variable and the X by 2 overall percentages table with
totals for each dummy variable. If TRUE, then the last two elements of the
return object are &quot;count&quot; containing an array of counts  and &quot;percent&quot;
containing an array of overall percentages.</p>
</td></tr>
<tr><td><code id="props_compare_+3A_check">check</code></td>
<td>
<p>logical vector of length 1 specifying whether the input
arguments should be checked for errors. For example, if <code>lvl</code> has
values that are not present in <code>data[[nom.nm]]</code>. This is a tradeoff
between computational efficiency (FALSE) and more useful error messages
(TRUE).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The confidence interval for Cramer's V is calculated with fisher's r to z
transformation as Cramer's V is a kind of multiple correlation coefficient.
Cramer's V is transformed to fisher's z units, a symmetric confidence
interval for fisher's z is calculated, and then the lower and upper bounds
are back-transformed to Cramer's V units.
</p>


<h3>Value</h3>

<p>list of data.frames containing statistical information about the
proportion comparisons: 1) nhst = chi-square test of independence stat info
in a data.frame, 2) desc = descriptive statistics stat info in a data.frame
(note there could be more than 3 groups - groups i, j, and k are just
provided as an example), 3) std = standardized effect size and its
confidence interval in a data.frame, 4) count = numeric array with dim =
<code>[X+1, 3, length(vrb.nm)]</code> of the X by 2 contingency table of counts
for each dummy variable with an additional row and column for totals (if
<code>rtn.table</code> = TRUE), 5) percent = numeric array with dim = <code>[X+1,
  3, length(vrb.nm)]</code> of the X by 2 contingency table of overall percentages
for each dummy variable with an additional row and column for totals (if
<code>rtn.table</code> = TRUE).
</p>
<p>1) nhst = chi-square test of independence stat info in a data.frame
</p>

<dl>
<dt>est</dt><dd><p>average proportion difference absolute value (i.e., |group j - group i|)</p>
</dd>
<dt>se</dt><dd><p>NA (to remind the user there is no standard error for the test)</p>
</dd>
<dt>X2</dt><dd><p>chi-square value</p>
</dd>
<dt>df</dt><dd><p>degrees of freedom (of the nominal variable)</p>
</dd>
<dt>p</dt><dd><p>two-sided p-value</p>
</dd>
</dl>

<p>2) desc = descriptive statistics stat info in a data.frame (note there
could be more than 3 groups - groups i, j, and k are just provided as an example):
</p>

<dl>
<dt>prop_'lvl[k]'</dt><dd><p>proportion of group k</p>
</dd>
<dt>prop_'lvl[j]'</dt><dd><p>proportion of group j</p>
</dd>
<dt>prop_'lvl[i]'</dt><dd><p>proportion of group i</p>
</dd>
<dt>sd_'lvl[k]'</dt><dd><p>standard deviation of group k</p>
</dd>
<dt>sd_'lvl[j]'</dt><dd><p>standard deviation of group j</p>
</dd>
<dt>sd_'lvl[i]'</dt><dd><p>standard deviation of group i</p>
</dd>
<dt>n_'lvl[k]'</dt><dd><p>sample size of group k</p>
</dd>
<dt>n_'lvl[j]'</dt><dd><p>sample size of group j</p>
</dd>
<dt>n_'lvl[i]'</dt><dd><p>sample size of group i</p>
</dd>
</dl>

<p>3) std = standardized effect size and its confidence interval in a data.frame
</p>

<dl>
<dt>cramer</dt><dd><p>Cramer's V estimate</p>
</dd>
<dt>lwr</dt><dd><p>lower bound of Cramer's V confidence interval</p>
</dd>
<dt>upr</dt><dd><p>upper bound of Cramer's V confidence interval</p>
</dd>
</dl>

<p>4) count = numeric array with dim = <code>[X+1, 3, length(vrb.nm)]</code> of the X
by 2 contingency table of counts for each dummy variable with an additional
row and column for totals (if <code>rtn.table</code> = TRUE).
</p>
<p>The 3+ unique observed values of <code>data[[nom.nm]]</code> - plus the total - are
the rows and the two unique observed values of <code>data[[vrb.nm]]</code> (i.e., 0
and 1) - plus the total - are the columns. The variables in
<code>data[vrb.nm]</code> are the layers. The dimlabels are &quot;nom&quot; for the rows and
&quot;x&quot; for the columns and &quot;vrb&quot; for the layers. The rownames are 1. 'lvl[i]',
2. 'lvl[j]', 3. 'lvl[k]', 4. &quot;total&quot;. The colnames are 1. &quot;0&quot;, 2. &quot;1&quot;, 3.
&quot;total&quot;. The laynames are <code>vrb.nm</code>.
</p>
<p>5) percent = numeric array with dim = <code>[X+1, 3, length(vrb.nm)]</code> of the
X by 2 contingency table of overall percentages for each dummy variable with
an additional row and column for totals (if <code>rtn.table</code> = TRUE).
</p>
<p>The 3+ unique observed values of <code>data[[nom.nm]]</code> - plus the total - are
the rows and the two unique observed values of <code>data[[vrb.nm]]</code> (i.e., 0
and 1) - plus the total - are the columns. The variables in
<code>data[vrb.nm]</code> are the layers. The dimlabels are &quot;nom&quot; for the rows, &quot;x&quot;
for the columns, and &quot;vrb&quot; for the layers. The rownames are 1. 'lvl[i]', 2.
'lvl[j]', 3. 'lvl[k]', 4. &quot;total&quot;. The colnames are 1. &quot;0&quot;, 2. &quot;1&quot;, 3.
&quot;total&quot;. The laynames are <code>vrb.nm</code>.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+prop.test">prop.test</a></code> the workhorse for <code>prop_compare</code>,
<code><a href="#topic+prop_compare">prop_compare</a></code> for a single dummy variable,
<code><a href="#topic+props_diff">props_diff</a></code> for only 2 independent groups (aka binary variable),
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# rtn.table = TRUE (default)

# multiple variables
tmp &lt;- replicate(n = 10, expr = mtcars, simplify = FALSE)
mtcars2 &lt;- str2str::ld2d(tmp)
mtcars2$"gear_dum" &lt;- ifelse(mtcars2$"gear" &gt; 3, yes = 1L, no = 0L)
mtcars2$"carb_dum" &lt;- ifelse(mtcars2$"carb" &gt; 3, yes = 1L, no = 0L)
vrb_nm &lt;- c("am","gear_dum","carb_dum") # dummy variables
lapply(X = vrb_nm, FUN = function(nm) {
   tmp &lt;- c("cyl", nm)
   table(mtcars2[tmp])
})
props_compare(data = mtcars2, vrb.nm = c("am","gear_dum","carb_dum"), nom.nm = "cyl")

# single variable
props_compare(mtcars2, vrb.nm = "am", nom.nm = "cyl")

# rtn.table = FALSE (no "count" or "percent" list elements)

# multiple variables
props_compare(data = mtcars2, vrb.nm = c("am","gear_dum","carb_dum"), nom.nm = "cyl",
   rtn.table = FALSE)

# single variable
props_compare(mtcars2, vrb.nm = "am", nom.nm = "cyl",
   rtn.table = FALSE)

# more than 3 groups
airquality2 &lt;- airquality
airquality2$"Wind_dum" &lt;- ifelse(airquality$"Wind" &gt;= 10, yes = 1, no = 0)
airquality2$"Solar.R_dum" &lt;- ifelse(airquality$"Solar.R" &gt;= 100, yes = 1, no = 0)
props_compare(airquality2, vrb.nm = c("Wind_dum","Solar.R_dum"), nom.nm = "Month")
props_compare(airquality2, vrb.nm = "Wind_dum", nom.nm = "Month")

</code></pre>

<hr>
<h2 id='props_diff'>Proportion Difference of Multiple Variables Across Two Independent Groups
(Chi-square Tests of Independence)</h2><span id='topic+props_diff'></span>

<h3>Description</h3>

<p><code>props_diff</code> tests the proportion difference of multiple variables
across two independent groups with chi-square tests of independence. The
function also calculates the descriptive statistics for each group, various
standardized effect sizes (e.g., Cramer's V), and can provide the 2x2
contingency tables. <code>props_diff</code> is simply a wrapper for
<code><a href="stats.html#topic+prop.test">prop.test</a></code> plus some extra calculations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>props_diff(
  data,
  vrb.nm,
  bin.nm,
  lvl = levels(as.factor(data[[bin.nm]])),
  yates = TRUE,
  zero.cell = 0.05,
  smooth = TRUE,
  ci.level = 0.95,
  rtn.table = TRUE,
  check = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="props_diff_+3A_data">data</code></td>
<td>
<p>data.frame of data.</p>
</td></tr>
<tr><td><code id="props_diff_+3A_vrb.nm">vrb.nm</code></td>
<td>
<p>character vector specifying the colnames in <code>data</code> for the
variables. Since we are testing proportions, the variables must be dummy
codes such that they only have values of 0 or 1 (or missing values).</p>
</td></tr>
<tr><td><code id="props_diff_+3A_bin.nm">bin.nm</code></td>
<td>
<p>character vector of length 1 specifying the colname in <code>data</code>
for the binary variable that only takes on two values (or missing values),
specifying the two independent groups.</p>
</td></tr>
<tr><td><code id="props_diff_+3A_lvl">lvl</code></td>
<td>
<p>character vector with length 2 specifying the unique values for
the two groups. If <code>bin</code> is a factor, then <code>lvl</code> should be the
factor levels rather than the underlying integer codes. This argument
allows you to specify the direction of the prop difference.
<code>prop_diff</code> calculates the prop differences as <code>x[ bin == lvl[2]
]</code> - <code>x[ bin == lvl[1] ]</code> such that it is group 2 - group 1. By
changing which group is group 1 vs. group 2, the direction of the prop
differences can be changed. See details of <code><a href="#topic+prop_diff">prop_diff</a></code>.</p>
</td></tr>
<tr><td><code id="props_diff_+3A_yates">yates</code></td>
<td>
<p>logical vector of length 1 specifying whether the Yate's
continuity correction should be applied for small samples. See
<code><a href="stats.html#topic+chisq.test">chisq.test</a></code> for details.</p>
</td></tr>
<tr><td><code id="props_diff_+3A_zero.cell">zero.cell</code></td>
<td>
<p>numeric vector of length 1 specifying what value to impute
for zero cell counts in the 2x2 contingency table when computing the
tetrachoric correlations. See <code><a href="psych.html#topic+tetrachoric">tetrachoric</a></code> for details.</p>
</td></tr>
<tr><td><code id="props_diff_+3A_smooth">smooth</code></td>
<td>
<p>logical vector of length 1 specifying whether a smoothing
algorithm should be applied when estimating the tetrachoric correlations.
See <code><a href="psych.html#topic+tetrachoric">tetrachoric</a></code> for details.</p>
</td></tr>
<tr><td><code id="props_diff_+3A_ci.level">ci.level</code></td>
<td>
<p>numeric vector of length 1 specifying the confidence level.
<code>ci.level</code> must range from 0 to 1.</p>
</td></tr>
<tr><td><code id="props_diff_+3A_rtn.table">rtn.table</code></td>
<td>
<p>logical vector of lengh 1 specifying whether the return
object should include the 2x2 contingency table of counts with totals and
the 2x2 overall percentages table. If TRUE, then the last two elements of
the return object are &quot;count&quot; containing a 3D array of counts and &quot;percent&quot;
containing a 3D array of overall percentages.</p>
</td></tr>
<tr><td><code id="props_diff_+3A_check">check</code></td>
<td>
<p>logical vector of length 1 specifying whether the input
arguments should be checked for errors. For example, if
<code>data[[bin.nm]]</code> has more than 2 unique values (other than missing
values). This is a tradeoff between computational efficiency (FALSE) and
more useful error messages (TRUE).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list of data.frames containing statistical information about the prop
differences (the rownames of each data.frame are <code>vrb.nm</code>): 1)
chisqtest = chi-square tests of independence stat info in a data.frame, 2)
describes = descriptive statistics stat info in a data.frame, 3) effects =
various standardized effect sizes in a data.frame, 4) count = numeric 3D
array with dim = <code>[3, 3, length(vrb.nm)]</code> of the 2x2 contingency
tables of counts with additional rows and columns for totals (if
<code>rtn.table</code> = TRUE), 5) percent = numeric 3D array with dim =
<code>[3, 3, length(vrb.nm)]</code> of the 2x2 contingency tables of overall
percentages with additional rows and columns for totals (if
<code>rtn.table</code> = TRUE).
</p>
<p>1) chisqtest = chi-square tests of independence stat info in a data.frame
</p>

<dl>
<dt>est</dt><dd><p>mean difference estimate (i.e., group 2 - group 1)</p>
</dd>
<dt>se</dt><dd><p>NA (to remind the user there is no standard error for the test)</p>
</dd>
<dt>X2</dt><dd><p>chi-square value</p>
</dd>
<dt>df</dt><dd><p>degrees of freedom (will always be 1)</p>
</dd>
<dt>p</dt><dd><p>two-sided p-value</p>
</dd>
<dt>lwr</dt><dd><p>lower bound of the confidence interval</p>
</dd>
<dt>upr</dt><dd><p>upper bound of the confidence interval</p>
</dd>
</dl>

<p>2) describes = descriptive statistics stat info in a data.frame
</p>

<dl>
<dt>prop_'lvl[2]'</dt><dd><p>proportion of group 2</p>
</dd>
<dt>prop_'lvl[1]'</dt><dd><p>proportion of group 1</p>
</dd>
<dt>sd_'lvl[2]'</dt><dd><p>standard deviation of group 2</p>
</dd>
<dt>sd_'lvl[1]'</dt><dd><p>standard deviation of group 1</p>
</dd>
<dt>n_'lvl[2]'</dt><dd><p>sample size of group 2</p>
</dd>
<dt>n_'lvl[1]'</dt><dd><p>sample size of group 1</p>
</dd>
</dl>

<p>3) effects = various standardized effect sizes in a data.frame
</p>

<dl>
<dt>cramer</dt><dd><p>Cramer's V estimate</p>
</dd>
<dt>h</dt><dd><p>Cohen's h estimate</p>
</dd>
<dt>phi</dt><dd><p>Phi coefficient estimate</p>
</dd>
<dt>yule</dt><dd><p>Yule coefficient estimate</p>
</dd>
<dt>tetra</dt><dd><p>Tetrachoric correlation estimate</p>
</dd>
<dt>OR</dt><dd><p>odds ratio estimate</p>
</dd>
<dt>RR</dt><dd><p>risk ratio estimate calculated as (i.e., group 2 / group 1).
Note this value will often differ when recoding variables (as it should).</p>
</dd>
</dl>

<p>4) count = numeric 3D array with dim = <code>[3, 3, length(vrb.nm)]</code> of the
2x2 contingency tables of counts with additional rows and columns for totals
(if <code>rtn.table</code> = TRUE).
</p>
<p>The two unique observed values of <code>data[vrb.nm]</code> (i.e., 0 and 1) -
plus the total - are the rows and the two unique observed values of
<code>data[[bin.nm]]</code> - plus the total - are the columns. The variables
themselves as the layers (i.e., 3rd dimension of the array). The dimlabels
are &quot;bin&quot; for the rows, &quot;x&quot; for the columns, and &quot;vrb&quot; for the layers. The
rownames are 1. &quot;0&quot;, 2. &quot;1&quot;, 3. &quot;total&quot;. The colnames are 1. 'lvl[1]', 2.
'lvl[2]', 3. &quot;total&quot;. The laynames are <code>vrb.nm</code>.
</p>
<p>5) percent = numeric 3D array with dim = <code>[3, 3, length(vrb.nm)]</code> of the
2x2 contingency tables of overall percentages with additional rows and
columns for totals (if <code>rtn.table</code> = TRUE).
</p>
<p>The two unique observed values of <code>data[vrb.nm]</code> (i.e., 0 and 1) -
plus the total - are the rows and the two unique observed values of
<code>data[[bin]]</code> - plus the total - are the columns. The variables
themselves as the layers (i.e., 3rd dimension of the array). The dimlabels
are &quot;bin&quot; for the rows, &quot;x&quot; for the columns, and &quot;vrb&quot; for the layers. The
rownames are 1. &quot;0&quot;, 2. &quot;1&quot;, 3. &quot;total&quot;. The colnames are 1. 'lvl[1]', 2.
'lvl[2]', 3. &quot;total&quot;. The laynames are <code>vrb.nm</code>.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+prop.test">prop.test</a></code> the workhorse for <code>props_diff</code>,
<code><a href="#topic+prop_diff">prop_diff</a></code> for a single dummy variable,
<code><a href="psych.html#topic+phi">phi</a></code> for another phi coefficient function
<code><a href="psych.html#topic+Yule">Yule</a></code> for another yule coefficient function
<code><a href="psych.html#topic+tetrachoric">tetrachoric</a></code> for another tetrachoric coefficient function
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# rtn.table = TRUE (default)

# multiple variables
mtcars2 &lt;- mtcars
mtcars2$"vs_bin" &lt;- ifelse(mtcars$"vs" == 1, yes = "yes", no = "no")
mtcars2$"gear_dum" &lt;- ifelse(mtcars2$"gear" &gt; 3, yes = 1L, no = 0L)
mtcars2$"carb_dum" &lt;- ifelse(mtcars2$"carb" &gt; 3, yes = 1L, no = 0L)
vrb_nm &lt;- c("am","gear_dum","carb_dum") # dummy variables
lapply(X = vrb_nm, FUN = function(nm) {
   tmp &lt;- c("vs_bin", nm)
   table(mtcars2[tmp])
})
props_diff(data = mtcars2, vrb.nm = c("am","gear_dum","carb_dum"), bin.nm = "vs_bin")

# single variable
props_diff(mtcars2, vrb.nm = "am", bin.nm = "vs_bin")

# rtn.table = FALSE (no "count" or "percent" list elements)

# multiple variables
props_diff(data = mtcars2, vrb.nm = c("am","gear_dum","carb_dum"), bin.nm = "vs",
   rtn.table = FALSE)

# single variable
props_diff(mtcars, vrb.nm = "am", bin.nm = "vs",
   rtn.table = FALSE)

</code></pre>

<hr>
<h2 id='props_test'>Test for Multiple Sample Proportion Against Pi (Chi-square Tests of Goodness
of Fit)</h2><span id='topic+props_test'></span>

<h3>Description</h3>

<p><code>props_test</code> tests for multiple sample proportion difference from
population proportions with chi-square tests of goodness of fit. The default
is that the goodness of fit is consistent with a population proportion Pi of
0.50. The function also calculates the descriptive statistics, various
standardized effect sizes (e.g., Cramer's V), and can provide the 1x2
contingency tables. <code>props_test</code> is simply a wrapper for
<code><a href="stats.html#topic+prop.test">prop.test</a></code> plus some extra calculations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>props_test(
  data,
  dum.nm,
  pi = 0.5,
  yates = TRUE,
  ci.level = 0.95,
  rtn.table = TRUE,
  check = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="props_test_+3A_data">data</code></td>
<td>
<p>data.frame of data.</p>
</td></tr>
<tr><td><code id="props_test_+3A_dum.nm">dum.nm</code></td>
<td>
<p>character vector of length 1 specifying the colnames in
<code>data</code> of the variables used to calculate the proportions. The
variables must only have values of 0 or 1 (or missing values), or be
otherwise known as dummy variables. See <code><a href="str2str.html#topic+is.dummy">is.dummy</a></code>.</p>
</td></tr>
<tr><td><code id="props_test_+3A_pi">pi</code></td>
<td>
<p>numeric vector of length = <code>length(dum.nm)</code> or length 1
specifying the population proportion values to compare the sample
proportions against. The order of the values should be the same as the
order in <code>dum.nm</code>. When length 1, the same population proportion value
is used for all the variables.</p>
</td></tr>
<tr><td><code id="props_test_+3A_yates">yates</code></td>
<td>
<p>logical vector of length 1 specifying whether the Yate's
continuity correction should be applied for small samples. See
<code><a href="stats.html#topic+chisq.test">chisq.test</a></code> for details.</p>
</td></tr>
<tr><td><code id="props_test_+3A_ci.level">ci.level</code></td>
<td>
<p>numeric vector of length 1 specifying the confidence level.
<code>ci.level</code> must range from 0 to 1.</p>
</td></tr>
<tr><td><code id="props_test_+3A_rtn.table">rtn.table</code></td>
<td>
<p>logical vector of lengh 1 specifying whether the return
object should include the rbinded 1x2 contingency table of counts with
totals and the rbinded 1x2 overall percentages table. If TRUE, then the
last two elements of the return object are &quot;count&quot; containing a data.frame
of counts and &quot;percent&quot; containing a data.frame of overall percentages.</p>
</td></tr>
<tr><td><code id="props_test_+3A_check">check</code></td>
<td>
<p>logical vector of length 1 specifying whether the input
arguments should be checked for errors. For example, if <code>data[dum.nm]</code>
are all dummy variables that only take on values of 0 or 1 (or missing
values). This is a tradeoff between computational efficiency (FALSE) and
more useful error messages (TRUE).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list of data.frames containing statistical information about the
proportion differences from pi: 1) nhst = chi-square test of goodness of fit
stat info in a data.frame, 2) desc = descriptive statistics stat info in a
data.frame, 3) std = various standardized effect sizes in a data.frame,
4) count = data.frame containing the rbinded 1x2 tables of counts with an additional
column for the total (if <code>rtn.table</code> = TRUE), 5) percent = data.frame
containing the rbinded 1x2 tables of overall percentages with an additional
column for the total (if <code>rtn.table</code> = TRUE)
</p>
<p>1) nhst = chi-square test of goodness of fit stat info in a data.frame
</p>

<dl>
<dt>est</dt><dd><p>proportion difference estimate (i.e., sample proportion - pi)</p>
</dd>
<dt>se</dt><dd><p>NA (to remind the user there is no standard error for the test)</p>
</dd>
<dt>X2</dt><dd><p>chi-square value</p>
</dd>
<dt>df</dt><dd><p>degrees of freedom (will always be 1)</p>
</dd>
<dt>p</dt><dd><p>two-sided p-value</p>
</dd>
</dl>

<p>2) desc = descriptive statistics stat info in a data.frame
</p>

<dl>
<dt>prop</dt><dd><p>sample proportion</p>
</dd>
<dt>pi</dt><dd><p>popularion proportion provided by the user (or 0.50 by default)</p>
</dd>
<dt>sd</dt><dd><p>standard deviation</p>
</dd>
<dt>n</dt><dd><p>sample size</p>
</dd>
<dt>lwr</dt><dd><p>lower bound of the confidence interval of the sample proportion itself</p>
</dd>
<dt>upr</dt><dd><p>upper bound of the confidence interval of the sample proportion itself</p>
</dd>
</dl>

<p>3) std = various standardized effect sizes in a data.frame
</p>

<dl>
<dt>cramer</dt><dd><p>Cramer's V estimate</p>
</dd>
<dt>h</dt><dd><p>Cohen's h estimate</p>
</dd>
</dl>

<p>4) count = data.frame containing the rbinded 1x2 tables of counts with an additional
column for the total (if <code>rtn.table</code> = TRUE). The colnames are 1.
&quot;0&quot;, 2. &quot;1&quot;, 3. &quot;total&quot;
</p>
<p>5) percent = data.frame containing the rbinded 1x2 tables of overall percentages
with an additional column for the total (if <code>rtn.table</code> = TRUE). The
colnames are 1. &quot;0&quot;, 2. &quot;1&quot;, 3. &quot;total&quot;
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+prop.test">prop.test</a></code> the workhorse for <code>prop_test</code>,
<code><a href="#topic+prop_test">prop_test</a></code> for a single dummy variables,
<code><a href="#topic+props_diff">props_diff</a></code> for chi-square tests of independence,
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# multiple variables
mtcars2 &lt;- mtcars
mtcars2$"gear_dum" &lt;- ifelse(mtcars2$"gear" &gt; 3, yes = 1L, no = 0L)
mtcars2$"carb_dum" &lt;- ifelse(mtcars2$"carb" &gt; 3, yes = 1L, no = 0L)
vrb_nm &lt;- c("am","gear_dum","carb_dum") # dummy variables
lapply(X = vrb_nm, FUN = function(nm) {
   table(mtcars2[nm])
})
props_test(data = mtcars2, dum.nm = c("am","gear_dum","carb_dum"))
props_test(data = mtcars2, dum.nm = c("am","gear_dum","carb_dum"),
   rtn.table = FALSE)

# single variable
props_test(data = mtcars2, dum.nm = "am")
props_test(data = mtcars2, dum.nm = "am", rtn.table = FALSE)

# error from non-dummy variables
## Not run: 
props_test(data = mtcars2, dum.nm = c("am","gear","carb"))

## End(Not run)

</code></pre>

<hr>
<h2 id='quest-package'>Pre-processing Questionnaire Data</h2><span id='topic+quest'></span><span id='topic+quest-package'></span>

<h3>Description</h3>

<p><code>quest</code> is a package for pre-processing questionnaire data
to get it ready for statistical modeling. It contains functions for
investigating missing data (e.g., <code><a href="#topic+rowNA">rowNA</a></code>), reshaping data
(e.g., <code><a href="#topic+wide2long">wide2long</a></code>), validating responses (e.g.,
<code><a href="#topic+revalids">revalids</a></code>), recoding variables (e.g., <code><a href="#topic+recodes">recodes</a></code>),
scoring (e.g., <code><a href="#topic+scores">scores</a></code>), centering (e.g.,
<code><a href="#topic+centers">centers</a></code>), aggregating (e.g., <code><a href="#topic+aggs">aggs</a></code>), shifting
(e.g., <code><a href="#topic+shifts">shifts</a></code>), etc. Functions whose first phrases end with
an <code>s</code> are vectorized versions of their functions without an <code>s</code>
at the end of the first phrase. For example, <code>center</code> inputs a
(atomic) vector and outputs a atomic vector to center and/or scale a single
variable; <code>centers</code> inputs a data.frame and outputs a data.frame to
center and/or scale multiple variables. Functions that end in <code>_by</code>
are calculated by group. For example, <code>center</code> does grand-mean
centering while <code>center_by</code> does group-mean centering. Putting the two
together, <code>centers_by</code> inputs a data.frame and outputs a data.frame to
center and/or scale multiple variables by group. Functions that end in
<code>_ml</code> calculate a &quot;multilevel&quot; result with a within-group result and
between-group result. Functions that end in <code>_if</code> are calculated
dependent on the frequency of observed values (aka amount of missing data).
The <code>quest</code> package uses the <code>str2str</code> package internally to
convert R objects from one structure to another. See <code>str2str</code>
for details.
</p>


<h3>Types of functions</h3>

<p>There are three main types of functions. 1)
Helper functions that primarily exist to save a few lines of code and are
primarily for convenience (e.g., <code><a href="#topic+vecNA">vecNA</a></code>). 2) Functions for
wrangling questionnaire data (e.g., <code><a href="#topic+nom2dum">nom2dum</a></code>,
<code><a href="#topic+reverses">reverses</a></code>). 3) Functions for preliminary statistical
calculation (e.g., <code><a href="#topic+means_diff">means_diff</a></code>, <code><a href="#topic+corp_by">corp_by</a></code>).
</p>


<h3>Abbreviations</h3>

<p>See the table below
</p>

<dl>
<dt>vrb</dt><dd><p>variable</p>
</dd>
<dt>grp</dt><dd><p>group</p>
</dd>
<dt>nm</dt><dd><p>names</p>
</dd>
<dt>NA</dt><dd><p>missing values</p>
</dd>
<dt>ov</dt><dd><p>observed values</p>
</dd>
<dt>prop</dt><dd><p>proportion</p>
</dd>
<dt>sep</dt><dd><p>separator</p>
</dd>
<dt>cor</dt><dd><p>correlations</p>
</dd>
<dt>id</dt><dd><p>identifier</p>
</dd>
<dt>rtn</dt><dd><p>return</p>
</dd>
<dt>fun</dt><dd><p>function</p>
</dd>
<dt>dfm</dt><dd><p>data.frame</p>
</dd>
<dt>fct</dt><dd><p>factor</p>
</dd>
<dt>nom</dt><dd><p>nominal variable</p>
</dd>
<dt>bin</dt><dd><p>binary variable</p>
</dd>
<dt>dum</dt><dd><p>dummy variable</p>
</dd>
<dt>pomp</dt><dd><p>percentage of maximum possible</p>
</dd>
<dt>std</dt><dd><p>standardize</p>
</dd>
<dt>wth</dt><dd><p>within-groups</p>
</dd>
<dt>btw</dt><dd><p>between-groups</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: David Disabato <a href="mailto:ddisab01@gmail.com">ddisab01@gmail.com</a> (<a href="https://orcid.org/0000-0001-7094-4996">ORCID</a>)
</p>

<hr>
<h2 id='recode2other'>Recode Unique Values in a Character Vector to 0ther (or NA)</h2><span id='topic+recode2other'></span>

<h3>Description</h3>

<p><code>recode2other</code> recodes multiple unique values in a character vector to
the same new value (e.g., &quot;other&quot;, NA_character_). It's primary use is to
recode based on the minimum frequency of the unique values so that low
frequency values can be combined into the same category; however, it also
allows for recoding particular unique values given by the user (see details).
This function is a wrapper for <code>car::recode</code>, which can handle general
recoding of character vectors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>recode2other(
  x,
  freq.min,
  prop = FALSE,
  inclusive = TRUE,
  other.nm = "other",
  extra.nm = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="recode2other_+3A_x">x</code></td>
<td>
<p>character vector. If not a character vector, it will be coarced to
one via <code>as.character</code>.</p>
</td></tr>
<tr><td><code id="recode2other_+3A_freq.min">freq.min</code></td>
<td>
<p>numeric vector of length 1 specifying the minimum frequency
of a unique value to keep it unchanged and consequentially recode any
unique values with frequencues less than (or equal to) it.</p>
</td></tr>
<tr><td><code id="recode2other_+3A_prop">prop</code></td>
<td>
<p>logical vector of length 1 specifying if <code>freq.min</code> provides
the frequency as a count (FALSE) or proportion (TRUE).</p>
</td></tr>
<tr><td><code id="recode2other_+3A_inclusive">inclusive</code></td>
<td>
<p>logical vector of length 1 specifying whether the frequency
of a unique value exactly equal to <code>freq.min</code> should be kept unchanged
(and not recoded to <code>other.nm</code>).</p>
</td></tr>
<tr><td><code id="recode2other_+3A_other.nm">other.nm</code></td>
<td>
<p>character vector of length 1 specifying what value the other
unique values should be recoded to. This can be NA_character_ resulting in
recoding to a missing value.</p>
</td></tr>
<tr><td><code id="recode2other_+3A_extra.nm">extra.nm</code></td>
<td>
<p>character vector specifying extra unique values that should
be recoded to <code>other.nm</code> that are not included based on the minimum
frequency from the combination of <code>freq.min</code>, <code>prop</code>,
<code>inclusive</code>. The default is NULL, meaning no extra unique values are
recoded.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>extra.nm</code> argument allows for <code>recode2other</code> to be used as
simpler function that just recodes particular unique values to the same new
value (although arguably this is easier to do using <code>car::recode</code>
directly). To do so set <code>freq.min = 0</code> and provide the unique values to
<code>extra.nm</code>. Note, that the current version of this function does not
allow for NA_character_ to be included in <code>extra.nm</code> as it will end up
treating it as &quot;NA&quot; (see examples).
</p>


<h3>Value</h3>

<p>character vector of the same length as <code>x</code> with unique values
with frequency less than <code>freq.nm</code> recoded to <code>other.nm</code> as well
as any unique values in <code>extra.nm</code>. While the current version of the
function allows for recoding *to* NA values via <code>other.nm</code>, it does
not allow for recoding *from* NA values via <code>extra.nm</code> (see examples).
</p>


<h3>See Also</h3>

<p><code><a href="car.html#topic+recode">recode</a></code>
<code><a href="base.html#topic+ifelse">ifelse</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# based on minimum frequency unique values
state_region &lt;- as.character(state.region)
recode2other(state_region, freq.min = 13) # freq.min as a count
recode2other(state_region, freq.min = 0.26, prop = TRUE) # freq.min as a proportion
recode2other(state_region, freq.min = 13, other.nm = "_blank_")
recode2other(state_region, freq.min = 13,
   other.nm = NA) # allows for other.nm to be NA
recode2other(state_region, freq.min = 13,
   extra.nm = "South") # add an extra unique value to recode
recode2other(state_region, freq.min = 13,
   inclusive = FALSE) # recodes "West" to "other"

# based on user given unique values
recode2other(state_region, freq.min = 0,
   extra.nm = c("South","West")) # recodes manually rather than by freq.min
# current version does NOT allow for NA to be a unique value that is converted to other
state_region2 &lt;- c(NA, state_region, NA)
recode2other(state_region2, freq.min = 13) # NA remains in the character vector
recode2other(state_region2, freq.min = 0,
   extra.nm = c("South","West",NA)) # NA remains in the character vector

</code></pre>

<hr>
<h2 id='recodes'>Recode Data</h2><span id='topic+recodes'></span>

<h3>Description</h3>

<p><code>recodes</code> recodes data based on specified recodes using the
<code>car::recode</code> function. This can be used for numeric or character
(including factors) data. See <code><a href="car.html#topic+recode">recode</a></code> for details. The
<code>levels</code> argument from <code>car::recode</code> is excluded because there is
no easy way to vectorize it when only a subset of the variables are factors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>recodes(data, vrb.nm, recodes, suffix = "_r", as.factor, as.numeric = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="recodes_+3A_data">data</code></td>
<td>
<p>data.frame of data.</p>
</td></tr>
<tr><td><code id="recodes_+3A_vrb.nm">vrb.nm</code></td>
<td>
<p>character vector of colnames from <code>data</code> specifying the
variables.</p>
</td></tr>
<tr><td><code id="recodes_+3A_recodes">recodes</code></td>
<td>
<p>character vector of length 1 specifying the recodes. See
details of <code><a href="car.html#topic+recode">recode</a></code> for how to use this argument.</p>
</td></tr>
<tr><td><code id="recodes_+3A_suffix">suffix</code></td>
<td>
<p>character vector of length 1 specifying the string to add to
the end of the colnames in the return object.</p>
</td></tr>
<tr><td><code id="recodes_+3A_as.factor">as.factor</code></td>
<td>
<p>logical vector of length 1 specifying if the recoded columns
should be returned as factors. The default depends on the column in
<code>data[vrb.nm]</code>. If the column is a factor, then <code>as.factor</code> =
TRUE for that column. If the column is not a factor, then <code>as.factor</code>
= FALSE for that column. Any non-default, specified value for this argument
will result in <code>as.factor</code> being universally applied to all columns in
<code>data[vrb.nm]</code>.</p>
</td></tr>
<tr><td><code id="recodes_+3A_as.numeric">as.numeric</code></td>
<td>
<p>logical vector of length 1 specifying if the recoded
columns should be returned as numeric vectors when possible. This can be
useful when having character vectors converted to numeric, such that
numbers with typeof character (e.g., &quot;1&quot;) will be coerced to typeof numeric
(e.g., 1). Note, this argument has no effect on columns in
<code>data[vrb.nm]</code> which are typeof character and have letters in their
values (e.g., &quot;1a&quot;). Note, this argument is often not needed as you can
directly recode to a numeric by excluding quotes from the number in the
<code>recodes</code> argument.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data.frame of recoded variables with colnames specified by
<code>paste0(vrb.nm, suffix)</code>. In general, the columns of the data.frame
are the same typeof as those in <code>data</code> except for instances when
<code>as.factor</code> and/or <code>as.numeric</code> change the typeof.
</p>


<h3>See Also</h3>

<p><code><a href="car.html#topic+recode">recode</a></code>
<code><a href="#topic+reverses">reverses</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>recodes(data = psych::bfi, vrb.nm = c("A1","C4","C5","E1","E2","O2","O5"),
   recodes = "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
re_codes &lt;- "'Quebec' = 'canada'; 'Mississippi' = 'usa'; 'nonchilled' = 'no'; 'chilled' = 'yes'"
recodes(data = CO2, vrb.nm = c("Type","Treatment"), recodes = re_codes,
   as.factor = FALSE) # convert from factors to characters
</code></pre>

<hr>
<h2 id='renames'>Rename Data Columns from a Codebook</h2><span id='topic+renames'></span>

<h3>Description</h3>

<p><code>renames</code> renames columns in a data.frame from a codebook. The codebook is
assumed to be a list of data.frames containing the old and new column names.
See details for how the codebook should be structured. The idea is that the
codebook has been imported as an excel workbook with different sets of column
renaming information in different workbook sheets. This function is simply a wrapper
for <code>plyr::rename</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>renames(
  data,
  codebook,
  old = 1L,
  new = 2L,
  warn_missing = TRUE,
  warn_duplicated = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="renames_+3A_data">data</code></td>
<td>
<p>data.frame of data.</p>
</td></tr>
<tr><td><code id="renames_+3A_codebook">codebook</code></td>
<td>
<p>list of data.frames containing the old and new column names.</p>
</td></tr>
<tr><td><code id="renames_+3A_old">old</code></td>
<td>
<p>numeric vector or character vector of length 1 specifying the
position or name of the column in the <code>codebook</code> data.frames that
contains the old column names present in <code>data</code>.</p>
</td></tr>
<tr><td><code id="renames_+3A_new">new</code></td>
<td>
<p>numeric vector or character vector of length 1 specifying the
position or name of the column in the <code>codebook</code> data.frames that
contains the new column names to rename to in <code>data</code>.</p>
</td></tr>
<tr><td><code id="renames_+3A_warn_missing">warn_missing</code></td>
<td>
<p>logical vector of length 1 specifying whether <code>renames</code>
should return a warning if any old names in <code>codebook</code> are not present in
<code>data</code>.</p>
</td></tr>
<tr><td><code id="renames_+3A_warn_duplicated">warn_duplicated</code></td>
<td>
<p>logical vector of length 1 specifying whether <code>renames</code>
should return a warning if the renaming process results in duplicate column names
in the return object.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>codebook</code> is a list of data.frames where one column refers to the old names
and another column refers to the new names. Therefore, each row of the data.frames
refers to a column in <code>data</code>. The position or names of the columns in the
<code>codebook</code> data.frames that contain the old (i.e., <code>old</code>) and new
(i.e., <code>new</code>) <code>data</code> columns must be the same for each data.frame in
<code>codebook</code>.
</p>


<h3>Value</h3>

<p>data.frame identical to <code>data</code> except that the old names in
<code>codebook</code> have been replaced by the new names in <code>codebook</code>.
</p>


<h3>See Also</h3>

<p><code><a href="plyr.html#topic+rename">rename</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>code_book &lt;- list(
   data.frame("old" = c("rating","complaints"), "new" = c("RATING","COMPLAINTS")),
   data.frame("old" = c("privileges","learning"), "new" = c("PRIVILEGES","LEARNING"))
)
renames(data = attitude, codebook = code_book, old = "old", new = "new")
</code></pre>

<hr>
<h2 id='reorders'>Reorder Levels of Factor Data</h2><span id='topic+reorders'></span>

<h3>Description</h3>

<p><code>reorders</code> re-orders the levels of factor data. The factors are columns
in a data.frame where the same reordering scheme is desired. This is often
useful before using factor data in a statistical analysis (e.g., <code>lm</code>)
or a graph (e.g., <code>ggplot</code>). It is essentially a vectorized version of
<code>reorder.default</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>reorders(data, fct.nm, ord.nm = NULL, fun, ..., suffix = "_r")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="reorders_+3A_data">data</code></td>
<td>
<p>data.frame of data.</p>
</td></tr>
<tr><td><code id="reorders_+3A_fct.nm">fct.nm</code></td>
<td>
<p>character vector of colnames in <code>data</code> that specify the
factor columns. If any of the columns specified by <code>fct.nm</code> are not
factors, then an error is returned.</p>
</td></tr>
<tr><td><code id="reorders_+3A_ord.nm">ord.nm</code></td>
<td>
<p>character vector of length 1 or <code>NULL</code>. If a character
vector of length 1, it is a colname in <code>data</code> specifying the column in
<code>data</code> that will be used in conjunction with <code>fun</code> to re-order
the factor columns. If <code>NULL</code> (default), it is assumed that each
factor column itself will be used in conjunction with <code>fun</code> to
re-order the factor columns.</p>
</td></tr>
<tr><td><code id="reorders_+3A_fun">fun</code></td>
<td>
<p>function that will be used to re-order the factor columns. The
function is expected to input an atomic vector of length =
<code>nrow(data)</code> and return an atomic vector of length 1. <code>fun</code> is
applied to <code>data[[ord.nm]]</code> if <code>ord.nm</code> is a character vector of
length 1 or applied to each column in <code>data[fct.nm]</code> if <code>ord.nm</code>
= <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="reorders_+3A_...">...</code></td>
<td>
<p>additional named arguments used by <code>fun</code>. For example, if
<code>fun</code> is <code>mean</code>, the user might specify an argument <code>na.rm =
TRUE</code> to set the <code>na.rm</code> argument in the <code>mean</code> function.</p>
</td></tr>
<tr><td><code id="reorders_+3A_suffix">suffix</code></td>
<td>
<p>character vector of length 1 specifying the string that will be
appended to the end of the colnames in the return object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data.frame of re-ordered factor columns with colnames =
<code>paste0(fct.nm, suffix)</code>.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+reorder.default">reorder.default</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# factor vector
reorder(x = state.region, X = state.region,
   FUN = length) # least frequent to most frequent
reorder(x = state.region, X = state.region,
   FUN = function(vec) {-1 * length(vec)}) # most frequent to least frequent

# data.frame of factors
infert_fct &lt;- infert
fct_nm &lt;- c("education","parity","induced","case","spontaneous")
infert_fct[fct_nm] &lt;- lapply(X = infert[fct_nm], FUN = as.factor)
x &lt;- reorders(data = infert_fct, fct.nm = fct_nm,
   fun = length) # least frequent to most frequent
lapply(X = x, FUN = levels)
y &lt;- reorders(data = infert_fct, fct.nm = fct_nm,
   fun = function(vec) {-1 * length(vec)}) # most frequent to least frequent
lapply(X = y, FUN = levels)
# ord.nm specified as a different column in data.frame
z &lt;- reorders(data = infert_fct, fct.nm = fct_nm, ord.nm = "pooled.stratum",
   fun = mean) # category with highest mean for pooled.stratum to
   # category with lowest mean for pooled.stratum
lapply(X = z, FUN = levels)

</code></pre>

<hr>
<h2 id='revalid'>Recode Invalid Values from a Vector</h2><span id='topic+revalid'></span>

<h3>Description</h3>

<p><code>revalid</code> recodes invalid data to specified values. For example,
sometimes invalid values are present in a vector of data (e.g., age = -1).
This function allows you to specify which values are possible and will then
recode any impossible values to <code>undefined</code>. This function is a useful
wrapper for the function <code>car::recode</code>, tailored for the specific use of
recoding invalid values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>revalid(x, valid, undefined = NA)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="revalid_+3A_x">x</code></td>
<td>
<p>atomic vector.</p>
</td></tr>
<tr><td><code id="revalid_+3A_valid">valid</code></td>
<td>
<p>atomic vector of valid values for <code>x</code>.</p>
</td></tr>
<tr><td><code id="revalid_+3A_undefined">undefined</code></td>
<td>
<p>atomic vector of length 1 specifying what the invalid values
should be recoded to.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>atomic vector with the same typeof as <code>x</code> where any values not
present in <code>valid</code> have been recoded to <code>undefined</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+revalids">revalids</a></code>
<code><a href="#topic+valid_test">valid_test</a></code>
<code><a href="#topic+valids_test">valids_test</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>revalid(x = attitude[[1]], valid = 25:75, undefined = NA) # numeric vector
revalid(x = as.character(ToothGrowth[["supp"]]), valid = c('VC'),
   undefined = NA) # character vector
revalid(x = ToothGrowth[["supp"]], valid = c('VC'),
   undefined = NA) # factor
</code></pre>

<hr>
<h2 id='revalids'>Recode Invalid Values from Data</h2><span id='topic+revalids'></span>

<h3>Description</h3>

<p><code>revalids</code> recodes invalid data to specified values. For example,
sometimes invalid values are present in a vector of data (e.g., age = -1).
This function allows you to specify which values are possible and will then
recode any impossible values to <code>undefined</code>. <code>revalids</code> is simply a
vectorized version of <code>revalid</code> to more easily revalid multiple columns
of a data.frame at the same time.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>revalids(data, vrb.nm, valid, undefined = NA, suffix = "_v")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="revalids_+3A_data">data</code></td>
<td>
<p>data.frame of data.</p>
</td></tr>
<tr><td><code id="revalids_+3A_vrb.nm">vrb.nm</code></td>
<td>
<p>character vector of colnames from <code>data</code> specifying the
variables.</p>
</td></tr>
<tr><td><code id="revalids_+3A_valid">valid</code></td>
<td>
<p>atomic vector of valid values for the data. Note, the valid
values must be the same for each variable.</p>
</td></tr>
<tr><td><code id="revalids_+3A_undefined">undefined</code></td>
<td>
<p>atomic vector of length 1 specifying what the invalid values
should be recoded to.</p>
</td></tr>
<tr><td><code id="revalids_+3A_suffix">suffix</code></td>
<td>
<p>character vector of length 1 specifying the string to add to
the end of the colnames in the return object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data.frame of recoded variables where any values not present in
<code>valid</code> have been recoded to <code>undefined</code> with colnames specified
by <code>paste0(vrb.nm, suffix)</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+revalid">revalid</a></code>
<code><a href="#topic+valids_test">valids_test</a></code>
<code><a href="#topic+valid_test">valid_test</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>revalids(data = attitude, vrb.nm = names(attitude),
   valid = 25:75) # numeric data
revalids(data = as.data.frame(CO2), vrb.nm = c("Type","Treatment"),
   valid = c('Quebec','nonchilled')) # factors
</code></pre>

<hr>
<h2 id='reverse'>Reverse Code a Numeric Vector</h2><span id='topic+reverse'></span>

<h3>Description</h3>

<p><code>reverse</code> reverse codes a numeric vector based on minimum and maximum
values. For example, say numerical values of response options can range from
1 to 4. The function will change 1 to 4, 2 to 3, 3 to 2, and 4 to 1. If there
are an odd number of response options, the middle in the sequence will be
unchanged.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>reverse(x, mini, maxi)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="reverse_+3A_x">x</code></td>
<td>
<p>numeric vector.</p>
</td></tr>
<tr><td><code id="reverse_+3A_mini">mini</code></td>
<td>
<p>numeric vector of length 1 specifying the minimum numeric value.</p>
</td></tr>
<tr><td><code id="reverse_+3A_maxi">maxi</code></td>
<td>
<p>numeric vector of length 1 specifying the maximum numeric value.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>numeric vector that correlates exactly -1 with <code>x</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+reverses">reverses</a></code>
<code><a href="psych.html#topic+reverse.code">reverse.code</a></code>
<code><a href="car.html#topic+recode">recode</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- psych::bfi[[1]]
head(x, n = 15)
y &lt;- reverse(x = psych::bfi[[1]], min = 1, max = 6)
head(y, n = 15)
cor(x, y, use = "complete.obs")
</code></pre>

<hr>
<h2 id='reverses'>Reverse Code Numeric Data</h2><span id='topic+reverses'></span>

<h3>Description</h3>

<p><code>reverses</code> reverse codes numeric data based on minimum and maximum
values. For example, say numerical values of response options can range from
1 to 4. The function will change 1 to 4, 2 to 3, 3 to 2, and 4 to 1. If there
are an odd number of response options, the middle in the sequence will be
unchanged.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>reverses(data, vrb.nm, mini, maxi, suffix = "_r")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="reverses_+3A_data">data</code></td>
<td>
<p>data.frame of data.</p>
</td></tr>
<tr><td><code id="reverses_+3A_vrb.nm">vrb.nm</code></td>
<td>
<p>character vector of colnames from <code>data</code> specifying the
variables.</p>
</td></tr>
<tr><td><code id="reverses_+3A_mini">mini</code></td>
<td>
<p>numeric vector of length 1 specifying the minimum numeric value.</p>
</td></tr>
<tr><td><code id="reverses_+3A_maxi">maxi</code></td>
<td>
<p>numeric vector of length 1 specifying the maximum numeric value.</p>
</td></tr>
<tr><td><code id="reverses_+3A_suffix">suffix</code></td>
<td>
<p>character vector of length 1 specifying the string to add to
the end of the colnames in the return object.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>reverses</code> is simply a vectorized version of <code>reverse</code> to more
easily reverse code multiple columns of a data.frame at the same time.
</p>


<h3>Value</h3>

<p>data.frame of reverse coded variables with colnames specified by
<code>paste0(vrb.nm, suffix)</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+reverse">reverse</a></code>
<code><a href="psych.html#topic+reverse.code">reverse.code</a></code>
<code><a href="#topic+recodes">recodes</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tmp &lt;- !(is.element(el = names(psych::bfi) , set = c("gender","education","age")))
vrb_nm &lt;- names(psych::bfi)[tmp]
reverses(data = psych::bfi, vrb.nm = vrb_nm, mini = 1, maxi = 6)
</code></pre>

<hr>
<h2 id='rowMeans_if'>Row Means Conditional on Frequency of Observed Values</h2><span id='topic+rowMeans_if'></span>

<h3>Description</h3>

<p><code>rowMean_if</code> calculates the mean of every row in a numeric or logical
matrix conditional on the frequency of observed data. If the frequency of
observed values in that row is less than (or equal to) that specified by
<code>ov.min</code>, then NA is returned for that row.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rowMeans_if(x, ov.min = 1, prop = TRUE, inclusive = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rowMeans_if_+3A_x">x</code></td>
<td>
<p>numeric or logical matrix. If not a matrix, it will be coerced to
one.</p>
</td></tr>
<tr><td><code id="rowMeans_if_+3A_ov.min">ov.min</code></td>
<td>
<p>minimum frequency of observed values required per row. If
<code>prop</code> = TRUE, then this is a decimal between 0 and 1. If <code>prop</code>
= FALSE, then this is a integer between 0 and <code>ncol(x)</code>.</p>
</td></tr>
<tr><td><code id="rowMeans_if_+3A_prop">prop</code></td>
<td>
<p>logical vector of length 1 specifying whether <code>ov.min</code>
should refer to the proportion of observed values (TRUE) or the count of
observed values (FALSE).</p>
</td></tr>
<tr><td><code id="rowMeans_if_+3A_inclusive">inclusive</code></td>
<td>
<p>logical vector of length 1 specifying whether the mean
should be calculated if the frequency of observed values in a row is
exactly equal to <code>ov.min</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Conceptually this function does: <code>apply(X = x, MARGIN = 1, FUN =
mean_if, ov.min = ov.min, prop = prop, inclusive = inclusive)</code>. But for
computational efficiency purposes it does not because then the observed
values conditioning would not be vectorized. Instead, it uses <code>rowMeans</code>
and then inserts NAs for rows that have too few observed values
</p>


<h3>Value</h3>

<p>numeric vector of length = <code>nrow(x)</code> with names =
<code>rownames(x)</code> providing the mean of each row or NA depending on the
frequency of observed values.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rowSums_if">rowSums_if</a></code>
<code><a href="#topic+colMeans_if">colMeans_if</a></code>
<code><a href="#topic+colSums_if">colSums_if</a></code>
<code><a href="Matrix.html#topic+rowMeans">rowMeans</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>rowMeans_if(airquality)
rowMeans_if(x = airquality, ov.min = 5, prop = FALSE)
</code></pre>

<hr>
<h2 id='rowNA'>Frequency of Missing Values by Row</h2><span id='topic+rowNA'></span>

<h3>Description</h3>

<p><code>rowNA</code> compute the frequency of missing values in a matrix by row. This
function essentially does <code>apply(X = x, MARGIN = 1, FUN = vecNA)</code>. It is
also used by other functions in the quest package related to missing values
(e.g., <code><a href="#topic+rowMeans_if">rowMeans_if</a></code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rowNA(x, prop = FALSE, ov = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rowNA_+3A_x">x</code></td>
<td>
<p>matrix with any typeof. If not a matrix, it will be coerced to a
matrix via <code>as.matrix</code>. The argument <code>rownames.force</code> is set to
TRUE to allow for rownames to carry over for non-matrix objects (e.g.,
data.frames).</p>
</td></tr>
<tr><td><code id="rowNA_+3A_prop">prop</code></td>
<td>
<p>logical vector of length 1 specifying whether the frequency of
missing values should be returned as a proportion (TRUE) or a count
(FALSE).</p>
</td></tr>
<tr><td><code id="rowNA_+3A_ov">ov</code></td>
<td>
<p>logical vector of length 1 specifying whether the frequency of
observed values (TRUE) should be returned rather than the frequency of
missing values (FALSE).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>numeric vector of length = <code>nrow(x)</code>, and names =
<code>rownames(x)</code>, providing the frequency of missing values (or observed
values if <code>ov</code> = TRUE) per row. If <code>prop</code> = TRUE, the
values will range from 0 to 1. If <code>prop</code> = FALSE, the values will
range from 1 to <code>ncol(x)</code>.
</p>


<h3>See Also</h3>

<p><code><a href="Matrix.html#topic+is.na">is.na</a></code>
<code><a href="#topic+vecNA">vecNA</a></code>
<code><a href="#topic+colNA">colNA</a></code>
<code><a href="#topic+rowsNA">rowsNA</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>rowNA(as.matrix(airquality)) # count of missing values
rowNA(as.data.frame(airquality)) # with rownames
rowNA(as.matrix(airquality), prop = TRUE) # proportion of missing values
rowNA(as.matrix(airquality), ov = TRUE) # count of observed values
rowNA(as.data.frame(airquality), prop = TRUE, ov = TRUE) # proportion of observed values
</code></pre>

<hr>
<h2 id='rowsNA'>Frequency of Multiple Sets of Missing Values by Row</h2><span id='topic+rowsNA'></span>

<h3>Description</h3>

<p><code>rowsNA</code> computes the frequency of missing values for multiple sets of
columns from a data.frame. The arguments <code>prop</code> and <code>ov</code> allow the
user to specify if they want to sum or mean the missing values as well as
compute the frequency of observed values rather than missing values. This
function is essentially a vectorized version of <code>rowNA</code> that inputs and
outputs a data.frame.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rowsNA(data, vrb.nm.list, prop = FALSE, ov = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rowsNA_+3A_data">data</code></td>
<td>
<p>data.frame of data.</p>
</td></tr>
<tr><td><code id="rowsNA_+3A_vrb.nm.list">vrb.nm.list</code></td>
<td>
<p>list where each element is a character vector of colnames
in <code>data</code> specifying the variables for that set of columns. The names
of <code>vrb.nm.list</code> will be the colnames of the return object.</p>
</td></tr>
<tr><td><code id="rowsNA_+3A_prop">prop</code></td>
<td>
<p>logical vector of length 1 specifying whether the frequency of
missing values should be returned as a proportion (TRUE) or a count
(FALSE).</p>
</td></tr>
<tr><td><code id="rowsNA_+3A_ov">ov</code></td>
<td>
<p>logical vector of length 1 specifying whether the frequency of
observed values (TRUE) should be returned rather than the frequency of
missing values (FALSE).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data.frame with the frequency of missing values (or observed values
if <code>ov</code> = TRUE) for each set of variables. The names are specified by
<code>names(vrb.nm.list)</code>; if <code>vrb.nm.list</code> does not have any names,
then the first element from <code>vrb.nm.list[[i]]</code> is used.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rowNA">rowNA</a></code>
<code><a href="#topic+colNA">colNA</a></code>
<code><a href="#topic+vecNA">vecNA</a></code>
<code><a href="Matrix.html#topic+is.na">is.na</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>vrb_list &lt;- lapply(X = c("O","C","E","A","N"), FUN = function(chr) {
   tmp &lt;- grepl(pattern = chr, x = names(psych::bfi))
   names(psych::bfi)[tmp]
})
rowsNA(data = psych::bfi,
   vrb.nm.list = vrb_list) # names set to first elements in `vrb.nm.list`[[i]]
names(vrb_list) &lt;- paste0(c("O","C","E","A","N"), "_m")
rowsNA(data = psych::bfi, vrb.nm.list = vrb_list) # names set to names(`vrb.nm.list`)
</code></pre>

<hr>
<h2 id='rowSums_if'>Row Sums Conditional on Frequency of Observed Values</h2><span id='topic+rowSums_if'></span>

<h3>Description</h3>

<p><code>rowSums_if</code> calculates the sum of every row in a numeric or logical
matrix conditional on the frequency of observed data. If the frequency of
observed values in that row is less than (or equal to) that specified by
<code>ov.min</code>, then NA is returned for that row. It also has the option to
return a value other than 0 (e.g., NA) when all rows are NA, which differs
from <code>rowSums(x, na.rm = TRUE)</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rowSums_if(
  x,
  ov.min = 1,
  prop = TRUE,
  inclusive = TRUE,
  impute = TRUE,
  allNA = NA_real_
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rowSums_if_+3A_x">x</code></td>
<td>
<p>numeric or logical matrix. If not a matrix, it will be coerced to
one.</p>
</td></tr>
<tr><td><code id="rowSums_if_+3A_ov.min">ov.min</code></td>
<td>
<p>minimum frequency of observed values required per row. If
<code>prop</code> = TRUE, then this is a decimal between 0 and 1. If <code>prop</code>
= FALSE, then this is a integer between 0 and <code>ncol(x)</code>.</p>
</td></tr>
<tr><td><code id="rowSums_if_+3A_prop">prop</code></td>
<td>
<p>logical vector of length 1 specifying whether <code>ov.min</code>
should refer to the proportion of observed values (TRUE) or the count of
observed values (FALSE).</p>
</td></tr>
<tr><td><code id="rowSums_if_+3A_inclusive">inclusive</code></td>
<td>
<p>logical vector of length 1 specifying whether the sum should
be calculated if the frequency of observed values in a row is exactly equal
to <code>ov.min</code>.</p>
</td></tr>
<tr><td><code id="rowSums_if_+3A_impute">impute</code></td>
<td>
<p>logical vector of length 1 specifying if missing values should
be imputed with the mean of observed values of <code>x[i, ]</code>. If TRUE
(default), this will make sums over the same columns with different amounts
of observed data comparable.</p>
</td></tr>
<tr><td><code id="rowSums_if_+3A_allna">allNA</code></td>
<td>
<p>numeric vector of length 1 specifying what value should be
returned for rows that are all NA. This is most applicable when
<code>ov.min = 0</code> and <code>inclusive = TRUE</code>. The default is NA, which
differs from <code>rowSums</code> with <code>na.rm = TRUE</code> where 0 is returned.
Note, the value is overwritten by NA if the frequency of observed values in
that row is less than (or equal to) that specified by <code>ov.min</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Conceptually this function is doing: <code>apply(X = x, MARGIN = 1, FUN =
sum_if, ov.min = ov.min, prop = prop, inclusive = inclusive)</code>. But for
computational efficiency purposes it does not because then the observed
values conditioning would not be vectorized. Instead, it uses <code>rowSums</code>
and then inserts NAs for rows that have too few observed values.
</p>


<h3>Value</h3>

<p>numeric vector of length = <code>nrow(x)</code> with names =
<code>rownames(x)</code> providing the sum of each row or NA (or <code>allNA</code>)
depending on the frequency of observed values.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rowMeans_if">rowMeans_if</a></code>
<code><a href="#topic+colSums_if">colSums_if</a></code>
<code><a href="#topic+colMeans_if">colMeans_if</a></code>
<code><a href="Matrix.html#topic+rowSums">rowSums</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>rowSums_if(airquality)
rowSums_if(x = airquality, ov.min = 5, prop = FALSE)
x &lt;- data.frame("x" = c(1, 1, NA), "y" = c(2, NA, NA), "z" = c(NA, NA, NA))
rowSums_if(x)
rowSums_if(x, ov.min = 0)
rowSums_if(x, ov.min = 0, allNA = 0)
identical(x = rowSums(x, na.rm = TRUE),
   y = unname(rowSums_if(x, impute = FALSE, ov.min = 0, allNA = 0))) # identical to
   # rowSums(x, na.rm = TRUE)
</code></pre>

<hr>
<h2 id='score'>Observed Unweighted Scoring of a Set of Variables/Items</h2><span id='topic+score'></span>

<h3>Description</h3>

<p><code>score</code> calculates observed unweighted scores across a set of variables/items.
If a row's frequency of observed data is less than (or equal to)
<code>ov.min</code>, then NA is returned for that row. <code>data[vrb.nm]</code> is
coerced to a matrix before scoring. If the coercion leads to a character
matrix, an error is returned.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>score(
  data,
  vrb.nm,
  avg = TRUE,
  ov.min = 1,
  prop = TRUE,
  inclusive = TRUE,
  impute = TRUE,
  std = FALSE,
  std.data = std,
  std.score = std
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="score_+3A_data">data</code></td>
<td>
<p>data.frame or numeric/logical matrix</p>
</td></tr>
<tr><td><code id="score_+3A_vrb.nm">vrb.nm</code></td>
<td>
<p>character vector of colnames in <code>data</code> specifying the set
of variables/items.</p>
</td></tr>
<tr><td><code id="score_+3A_avg">avg</code></td>
<td>
<p>logical vector of length 1 specifying whether mean scores (TRUE)
or sum scores (FALSE) should be created.</p>
</td></tr>
<tr><td><code id="score_+3A_ov.min">ov.min</code></td>
<td>
<p>minimum frequency of observed values required per row. If
<code>prop</code> = TRUE, then this is a decimal between 0 and 1. If <code>prop</code>
= FALSE, then this is a integer between 0 and <code>length(vrb.nm)</code>.</p>
</td></tr>
<tr><td><code id="score_+3A_prop">prop</code></td>
<td>
<p>logical vector of length 1 specifying whether <code>ov.min</code>
should refer to the proportion of observed values (TRUE) or the count of
observed values (FALSE).</p>
</td></tr>
<tr><td><code id="score_+3A_inclusive">inclusive</code></td>
<td>
<p>logical vector of length 1 specifying whether the score
should be calculated (rather than NA) if the frequency of observed values
in a row is exactly equal to <code>ov.min</code>.</p>
</td></tr>
<tr><td><code id="score_+3A_impute">impute</code></td>
<td>
<p>logical vector of length 1 specifying if missing values should
be imputed with the mean of observed values from each row of
<code>data[vrb.nm]</code> (i.e., row mean imputation). If TRUE (default), this
will make sums over the same rows with different frequencies of missing
values comparable. Note, this argument is only used when <code>avg</code> = FALSE
since when <code>avg</code> = TRUE row mean imputation is always done implicitly.</p>
</td></tr>
<tr><td><code id="score_+3A_std">std</code></td>
<td>
<p>logical vector of length 1 specifying whether 1)
<code>data[vrb.nm]</code> should be standardized before scoring and 2) the score
standardized after creation. This argument is for convenience as these two
standardization processes are often used together. However, this argument
will be overwritten by any non-default value for <code>std.data</code> and
<code>std.score</code>.</p>
</td></tr>
<tr><td><code id="score_+3A_std.data">std.data</code></td>
<td>
<p>logical vector of length 1 specifying whether
<code>data[vrb.nm]</code> should be standardized before scoring.</p>
</td></tr>
<tr><td><code id="score_+3A_std.score">std.score</code></td>
<td>
<p>logical vector of length 1 specifying whether the score
should be standardized after creation.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>numeric vector of the mean/sum of each row or <code>NA</code> if the
frequency of observed values is less than (or equal to) <code>ov.min</code>. The
names are the rownames of <code>data</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+scores">scores</a></code>
<code><a href="#topic+rowMeans_if">rowMeans_if</a></code>
<code><a href="#topic+rowSums_if">rowSums_if</a></code>
<code><a href="psych.html#topic+scoreItems">scoreItems</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>score(data = attitude, vrb.nm = c("complaints","privileges","learning","raises"))
score(data = attitude, vrb.nm = c("complaints","privileges","learning","raises"),
   std = TRUE) # standardized scoring
score(data = airquality, vrb.nm = c("Ozone","Solar.R","Temp"),
   ov.min = 0.75) # conditional on observed values
</code></pre>

<hr>
<h2 id='scores'>Observed Unweighted Scoring of Multiple Sets of Variables/Items</h2><span id='topic+scores'></span>

<h3>Description</h3>

<p><code>scores</code> calculates observed unweighted scores across multiple sets of
variables/items. If a row's frequency of observed data is less than (or equal
to) <code>ov.min</code>, then NA is returned for that row. Each set of
variables/items are coerced to a matrix before scoring. If the coercion leads
to a character matrix, an error is returned. This can be tested with
<code>lapply(X = vrb.nm.list, FUN = function(nm)
is.character(as.matrix(data[nm])))</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>scores(
  data,
  vrb.nm.list,
  avg = TRUE,
  ov.min = 1,
  prop = TRUE,
  inclusive = TRUE,
  impute = TRUE,
  std = FALSE,
  std.data = std,
  std.score = std
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="scores_+3A_data">data</code></td>
<td>
<p>data.frame or numeric/logical matrix</p>
</td></tr>
<tr><td><code id="scores_+3A_vrb.nm.list">vrb.nm.list</code></td>
<td>
<p>list where each element is a character vector of colnames
in <code>data</code> specifying the variables/items for that score. The names  of
<code>vrb.nm.list</code> will be the names of the scores in the return object.</p>
</td></tr>
<tr><td><code id="scores_+3A_avg">avg</code></td>
<td>
<p>logical vector of length 1 specifying whether mean scores (TRUE)
or sum scores (FALSE) should be created.</p>
</td></tr>
<tr><td><code id="scores_+3A_ov.min">ov.min</code></td>
<td>
<p>minimum frequency of observed values required per row. If
<code>prop</code> = TRUE, then this is a decimal between 0 and 1. If <code>prop</code>
= FALSE, then this is a integer between 0 and
<code>length(vrb.nm.list[[i]])</code>.</p>
</td></tr>
<tr><td><code id="scores_+3A_prop">prop</code></td>
<td>
<p>logical vector of length 1 specifying whether <code>ov.min</code>
should refer to the proportion of observed values (TRUE) or the count of
observed values (FALSE). If the multiple sets of variables/items contain
different numbers of variables, it probably makes the most sense to use the
proportion of observed values (TRUE).</p>
</td></tr>
<tr><td><code id="scores_+3A_inclusive">inclusive</code></td>
<td>
<p>logical vector of length 1 specifying whether the scores
should be calculated (rather than NA) if the frequency of observed values
in a row is exactly equal to <code>ov.min</code>.</p>
</td></tr>
<tr><td><code id="scores_+3A_impute">impute</code></td>
<td>
<p>logical vector of length 1 specifying if missing values should
be imputed with the mean of observed values from each row of
<code>data[vrb.nm.list[[i]] ]</code> (i.e., row mean imputation). If TRUE
(default), this will make sums over the same rows with different
frequencies of missing values comparable. Note, this argument is only used
when <code>avg</code> = FALSE since when <code>avg</code> = TRUE row mean imputation is
always done implicitly.</p>
</td></tr>
<tr><td><code id="scores_+3A_std">std</code></td>
<td>
<p>logical vector of length 1 specifying whether 1) the variables
should be standardized before scoring and 2) the score standardized after
creation. This argument is for convenience as these two standardization
processes are often used together. However, this argument will be
overwritten by any non-default value for <code>std.data</code> and
<code>std.score</code>.</p>
</td></tr>
<tr><td><code id="scores_+3A_std.data">std.data</code></td>
<td>
<p>logical vector of length 1 specifying whether the
variables/items should be standardized before scoring.</p>
</td></tr>
<tr><td><code id="scores_+3A_std.score">std.score</code></td>
<td>
<p>logical vector of length 1 specifying whether the scores
should be standardized after creation.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data.frame of mean/sum scores with <code>NA</code> for any row with the
frequency of observed values less than (or equal to) <code>ov.min</code>. The
colnames are specified by <code>names(vrb.nm.list)</code> and rownames by
<code>row.names(data)</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+score">score</a></code>
<code><a href="#topic+rowMeans_if">rowMeans_if</a></code>
<code><a href="#topic+rowSums_if">rowSums_if</a></code>
<code><a href="psych.html#topic+scoreItems">scoreItems</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>list_colnames &lt;- list("first" = c("rating","complaints","privileges"),
    "second" = c("learning","raises","critical"))
scores(data = attitude, vrb.nm.list = list_colnames)
list_colnames &lt;- list("first" = c("Ozone","Wind"),
    "second" = c("Solar.R","Temp"))
scores(data = airquality, vrb.nm.list = list_colnames, ov.min = .50,
   inclusive = FALSE) # scoring conditional on observed values
</code></pre>

<hr>
<h2 id='shift'>Shift a Vector (i.e., lag/lead)</h2><span id='topic+shift'></span>

<h3>Description</h3>

<p><code>shift</code> shifts elements of a vector right (<code>n</code> &lt; 0) for lags or
left (<code>n</code> &gt; 0) for leads replacing the undefined data with a
user-defined value (e.g., NA). The number of elements shifted is equal to
<code>abs(n)</code>. It is assumed that <code>x</code> is already sorted by time such
that the first element is earliest in time and the last element is the latest
in time.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>shift(x, n, undefined = NA)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="shift_+3A_x">x</code></td>
<td>
<p>atomic vector or list vector.</p>
</td></tr>
<tr><td><code id="shift_+3A_n">n</code></td>
<td>
<p>integer vector with length 1. Specifies the direction and magnitude
of the shift. See details.</p>
</td></tr>
<tr><td><code id="shift_+3A_undefined">undefined</code></td>
<td>
<p>atomic vector with length 1 (probably makes sense to be the
same typeof as <code>x</code>). Specifies what to insert for undefined values
after the shifting takes place. See details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>n</code> is negative, then <code>shift</code> inserts <code>undefined</code> into the
first <code>abs(n)</code> elements of <code>x</code>, shifting all other values of
<code>x</code> to the right <code>abs(n)</code> positions, and then dropping the last
<code>abs(n)</code> elements of <code>x</code> to preserve the original length of
<code>x</code>. If <code>n</code> is positive, then <code>shift</code> drops the first
<code>abs(n)</code> elements of <code>x</code>, shifting all other values of <code>x</code>
left <code>abs(n)</code> positions, and then inserts <code>undefined</code> into the last
<code>abs(n)</code> elements of <code>x</code> to preserve the original length of
<code>x</code>. If <code>n</code> is zero, then <code>shift</code> simply returns <code>x</code>.
</p>
<p>It is recommended to use <code>L</code> when specifying <code>n</code> to prevent
problems with floating point numbers. <code>shift</code> tries to circumvent this
issue by a call to <code>round</code> within <code>shift</code> if <code>n</code> is not an
integer; however that is not a complete fail safe. The problem is that
<code>as.integer(n)</code> implicit in <code>shift</code> truncates rather than rounds.
</p>


<h3>Value</h3>

<p>an atomic vector of the same length as <code>x</code> that is shifted. If
<code>x</code> and <code>undefined</code> are different typeofs, then the return will
be coerced to the more complex typeof (i.e., complex to simple: character,
double, integer, logical).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+shifts">shifts</a></code>
<code><a href="#topic+shift_by">shift_by</a></code>
<code><a href="#topic+shifts_by">shifts_by</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>shift(x = attitude[[1]], n = -1L) # use L to prevent problems with floating point numbers
shift(x = attitude[[1]], n = -2L) # can specify any integer up to the length of `x`
shift(x = attitude[[1]], n = +1L) # can specify negative or positive integers
shift(x = attitude[[1]], n = +2L, undefined = -999) # user-specified indefined value
shift(x = setNames(object = letters, nm = LETTERS), n = 3L) # names are kept
</code></pre>

<hr>
<h2 id='shift_by'>Shift a Vector (i.e., lag/lead) by Group</h2><span id='topic+shift_by'></span>

<h3>Description</h3>

<p><code>shift_by</code> shifts elements of a vector right (<code>n</code> &lt; 0) for lags or
left (<code>n</code> &gt; 0) for leads by group, replacing the undefined data with a
user-defined value (e.g., NA). The number of elements shifted is equal to
<code>abs(n)</code>. It is assumed that <code>x</code> is already sorted within each
group by time such that the first element for that group is earliest in time
and the last element for that group is the latest in time.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>shift_by(x, grp, n, undefined = NA)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="shift_by_+3A_x">x</code></td>
<td>
<p>atomic vector or list vector.</p>
</td></tr>
<tr><td><code id="shift_by_+3A_grp">grp</code></td>
<td>
<p>list of atomic vector(s) and/or factor(s) (e.g., data.frame),
which each have same length as <code>x</code>. It can also be an atomic vector or
factor, which will then be made the first element of a list internally.</p>
</td></tr>
<tr><td><code id="shift_by_+3A_n">n</code></td>
<td>
<p>integer vector with length 1. Specifies the direction and magnitude
of the shift. See details.</p>
</td></tr>
<tr><td><code id="shift_by_+3A_undefined">undefined</code></td>
<td>
<p>atomic vector with length 1 (probably makes sense to be the
same typeof as <code>x</code>). Specifies what to insert for undefined values
after the shifting takes place. See details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>n</code> is negative, then <code>shift_by</code> inserts <code>undefined</code> into the
first <code>abs(n)</code> elements of <code>x</code> for each group, shifting all other
values of <code>x</code> to the right <code>abs(n)</code> positions, and then dropping
the last <code>abs(n)</code> elements of <code>x</code> to preserve the original length
of each group. If <code>n</code> is positive, then <code>shift_by</code> drops the first
<code>abs(n)</code> elements of <code>x</code> for each group, shifting all other values
of <code>x</code> left <code>abs(n)</code> positions, and then inserts <code>undefined</code>
into the last <code>abs(n)</code> elements of <code>x</code> to preserve the original
length of each group. If <code>n</code> is zero, then <code>shift_by</code> simply returns
<code>x</code>.
</p>
<p>It is recommended to use <code>L</code> when specifying <code>n</code> to prevent
problems with floating point numbers. <code>shift_by</code> tries to circumvent this
issue by a call to <code>round</code> within <code>shift_by</code> if <code>n</code> is not an
integer; however that is not a complete fail safe. The problem is that
<code>as.integer(n)</code> implicit in <code>shift_by</code> truncates rather than rounds.
</p>


<h3>Value</h3>

<p>an atomic vector of the same length as <code>x</code> that is shifted by
group. If <code>x</code> and <code>undefined</code> are different typeofs, then the
return will be coerced to the most complex typeof (i.e., complex to simple:
character, double, integer, logical).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+shifts_by">shifts_by</a></code>
<code><a href="#topic+shift">shift</a></code>
<code><a href="#topic+shifts">shifts</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>shift_by(x = ChickWeight[["Time"]], grp = ChickWeight[["Chick"]], n = -1L)
tmp_nm &lt;- c("vs","am") # b/c Roxygen2 doesn't like c() in a []
shift_by(x = mtcars[["disp"]], grp = mtcars[tmp_nm], n = 1L)
tmp_nm &lt;- c("Type","Treatment") # b/c Roxygen2 doesn't like c() in a []
shift_by(x = as.data.frame(CO2)[["uptake"]], grp = as.data.frame(CO2)[tmp_nm],
   n = 2L) # multiple grouping vectors
</code></pre>

<hr>
<h2 id='shifts'>Shift Data (i.e., lag/lead)</h2><span id='topic+shifts'></span>

<h3>Description</h3>

<p><code>shifts</code> shifts rows of data down (<code>n</code> &lt; 0) for lags or up (<code>n</code>
&gt; 0) for leads replacing the undefined data with a user-defined value (e.g.,
NA). The number of rows shifted is equal to <code>abs(n)</code>. It is assumed that
<code>data[vrb.nm]</code> is already sorted by time such that the first row is
earliest in time and the last row is the latest in time.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>shifts(data, vrb.nm, n, undefined = NA, suffix)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="shifts_+3A_data">data</code></td>
<td>
<p>data.frame of data.</p>
</td></tr>
<tr><td><code id="shifts_+3A_vrb.nm">vrb.nm</code></td>
<td>
<p>character vector of colnames from <code>data</code> specifying the
variables.</p>
</td></tr>
<tr><td><code id="shifts_+3A_n">n</code></td>
<td>
<p>integer vector of length 1. Specifies the direction and magnitude of
the shift. See details.</p>
</td></tr>
<tr><td><code id="shifts_+3A_undefined">undefined</code></td>
<td>
<p>atomic vector of length 1 (probably makes sense to be the
same typeof as the vectors in <code>data[vrb.nm]</code>). Specifies what to
insert for undefined values after the shifting takes place. See details.</p>
</td></tr>
<tr><td><code id="shifts_+3A_suffix">suffix</code></td>
<td>
<p>character vector of length 1 specifying the string to append to
the end of the colnames of the return object. The default depends on the
<code>n</code> argument: 1) if <code>n</code> &lt; 0, then <code>suffix</code> =
<code>paste0("_g", -n)</code>, 2) if <code>n</code> &gt; 0, then <code>suffix</code> =
<code>paste0("_d", +n)</code>, 3) if <code>n</code> = 0, then <code>suffix</code> = &quot;&quot;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>n</code> is negative, then <code>shifts</code> inserts <code>undefined</code> into the
first <code>abs(n)</code> rows of <code>data[vrb.nm]</code>, shifting all other rows of
<code>x</code> down <code>abs(n)</code> positions, and then dropping the last
<code>abs(n)</code> row of <code>data[vrb.nm]</code> to preserve the original nrow of
<code>data</code>. If <code>n</code> is positive, then <code>shifts</code> drops the first
<code>abs(n)</code> rows of <code>x</code>, shifting all other rows of
<code>data[vrb.nm]</code> up <code>abs(n)</code> positions, and then inserts
<code>undefined</code> into the last <code>abs(n)</code> rows of <code>x</code> to preserve the
original length of <code>data</code>. If <code>n</code> is zero, then <code>shifts</code> simply
returns <code>data[vrb.nm]</code>.
</p>
<p>It is recommended to use <code>L</code> when specifying <code>n</code> to prevent
problems with floating point numbers. <code>shifts</code> tries to circumvent this
issue by a call to <code>round</code> within <code>shifts</code> if <code>n</code> is not an
integer; however that is not a complete fail safe. The problem is that
<code>as.integer(n)</code> implicit in <code>shifts</code> truncates rather than rounds.
</p>


<h3>Value</h3>

<p>data.frame of shifted data with colnames specified by <code>suffix</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+shift">shift</a></code>
<code><a href="#topic+shifts_by">shifts_by</a></code>
<code><a href="#topic+shift_by">shift_by</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>shifts(data = attitude, vrb.nm = colnames(attitude), n = -1L)
shifts(data = mtcars, vrb.nm = colnames(mtcars), n = 2L)
</code></pre>

<hr>
<h2 id='shifts_by'>Shift Data (i.e., lag/lead) by Group</h2><span id='topic+shifts_by'></span>

<h3>Description</h3>

<p><code>shifts_by</code> shifts rows of data down (<code>n</code> &lt; 0) for lags or up (<code>n</code>
&gt; 0) for leads replacing the undefined data with a user-defined value (e.g.,
NA). The number of rows shifted is equal to <code>abs(n)</code>. It is assumed that
<code>data[vrb.nm]</code> is already sorted within each group by time such that the
first row for that group is earliest in time and the last row for that group
is the latest in time. The groups can be specified by multiple columns in
<code>data</code> (e.g., <code>grp.nm</code> with length &gt; 1), and <code>interaction</code>
will be implicitly called to create the groups.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>shifts_by(data, vrb.nm, grp.nm, n, undefined = NA, suffix)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="shifts_by_+3A_data">data</code></td>
<td>
<p>data.frame of data.</p>
</td></tr>
<tr><td><code id="shifts_by_+3A_vrb.nm">vrb.nm</code></td>
<td>
<p>character vector of colnames from <code>data</code> specifying the
variables.</p>
</td></tr>
<tr><td><code id="shifts_by_+3A_grp.nm">grp.nm</code></td>
<td>
<p>character vector of colnames from <code>data</code> specifying the
groups.</p>
</td></tr>
<tr><td><code id="shifts_by_+3A_n">n</code></td>
<td>
<p>integer vector of length 1. Specifies the direction and magnitude of
the shift. See details.</p>
</td></tr>
<tr><td><code id="shifts_by_+3A_undefined">undefined</code></td>
<td>
<p>atomic vector of length 1 (probably makes sense to be the
same typeof as the vectors in <code>data[vrb.nm]</code>). Specifies what to
insert for undefined values after the shifting takes place. See details.</p>
</td></tr>
<tr><td><code id="shifts_by_+3A_suffix">suffix</code></td>
<td>
<p>character vector of length 1 specifying the string to append to
the end of the colnames of the return object. The default depends on the
<code>n</code> argument: 1) if <code>n</code> &lt; 0, then <code>suffix</code> =
<code>paste0("_gw", -n)</code>, 2) if <code>n</code> &gt; 0, then <code>suffix</code> =
<code>paste0("_dw", +n)</code>, 3) if <code>n</code> = 0, then <code>suffix</code> = &quot;&quot;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>n</code> is negative, then <code>shifts_by</code> inserts <code>undefined</code> into
the first <code>abs(n)</code> rows of <code>data[vrb.nm]</code> for each group, shifting
all other rows of <code>x</code> down <code>abs(n)</code> positions, and then dropping
the last <code>abs(n)</code> row of <code>data[vrb.nm]</code> to preserve the original
nrow of each group. If <code>n</code> is positive, then <code>shifts_by</code> drops the
first <code>abs(n)</code> rows of <code>x</code> for each group, shifting all other rows
of <code>data[vrb.nm]</code> up <code>abs(n)</code> positions, and then inserts
<code>undefined</code> into the last <code>abs(n)</code> rows of <code>x</code> to preserve the
original length of each group. If <code>n</code> is zero, then <code>shifts_by</code>
simply returns <code>data[vrb.nm]</code>.
</p>
<p>It is recommended to use <code>L</code> when specifying <code>n</code> to prevent
problems with floating point numbers. <code>shifts_by</code> tries to circumvent
this issue by a call to <code>round</code> within <code>shifts_by</code> if <code>n</code> is
not an integer; however that is not a complete fail safe. The problem is that
<code>as.integer(n)</code> implicit in <code>shifts_by</code> truncates rather than
rounds.
</p>


<h3>Value</h3>

<p>data.frame of shifted data by group with colnames specified by
<code>suffix</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+shift_by">shift_by</a></code>
<code><a href="#topic+shifts">shifts</a></code>
<code><a href="#topic+shift">shift</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>shifts_by(data = ChickWeight, vrb.nm = c("weight","Time"), grp.nm = "Chick", n = -1L)
shifts_by(data = mtcars, vrb.nm = c("disp","mpg"), grp.nm = c("vs","am"), n = 1L)
shifts_by(data = as.data.frame(CO2), vrb.nm = c("conc","uptake"),
   grp.nm = c("Type","Treatment"), n = 2L) # multiple grouping columns
</code></pre>

<hr>
<h2 id='sum_if'>Sum Conditional on Minimum Frequency of Observed Values</h2><span id='topic+sum_if'></span>

<h3>Description</h3>

<p><code>sum_if</code> calculates the sum of a numeric or logical vector conditional
on a specified minimum frequency of observed values. If the amount of
observed data is less than (or equal to) <code>ov.min</code>, then <code>NA</code> is
returned rather than the sum.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sum_if(x, impute = TRUE, ov.min = 1, prop = TRUE, inclusive = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sum_if_+3A_x">x</code></td>
<td>
<p>numeric or logical vector.</p>
</td></tr>
<tr><td><code id="sum_if_+3A_impute">impute</code></td>
<td>
<p>logical vector of length 1 specifying if missing values should
be imputed with the mean of observed values of <code>x</code>. If TRUE (default),
this will make sums over the same vectors with different amounts of missing
data comparable.</p>
</td></tr>
<tr><td><code id="sum_if_+3A_ov.min">ov.min</code></td>
<td>
<p>minimum frequency of observed values required. If <code>prop</code> =
TRUE, then this is a decimal between 0 and 1. If <code>prop</code> = FALSE, then
this is a integer between 0 and <code>length(x)</code>.</p>
</td></tr>
<tr><td><code id="sum_if_+3A_prop">prop</code></td>
<td>
<p>logical vector of length 1 specifying whether <code>ov.min</code>
should refer to the proportion of observed values (TRUE) or the count of
observed values (FALSE).</p>
</td></tr>
<tr><td><code id="sum_if_+3A_inclusive">inclusive</code></td>
<td>
<p>logical vector of length 1 specifying whether the sum should
be calculated (rather than NA) if the frequency of observed values is
exactly equal to <code>ov.min</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>numeric vector of length 1 providing the sum of <code>x</code> or <code>NA</code>
conditional on if the frequency of observed data is greater than (or equal
to) <code>ov.min</code>.
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+sum">sum</a></code>
<code><a href="#topic+mean_if">mean_if</a></code>
<code><a href="#topic+make.fun_if">make.fun_if</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>sum_if(x = airquality[[1]], ov.min = .75) # proportion of observed values
sum_if(x = airquality[[1]], ov.min = 116,
   prop = FALSE) # count of observe values
sum_if(x = airquality[[1]], ov.min = 116, prop = FALSE,
   inclusive = FALSE) # not include ov.min value itself
sum_if(x = c(TRUE, NA, FALSE, NA),
   ov.min = .50) # works with logical vectors as well as numeric
</code></pre>

<hr>
<h2 id='summary_ucfa'>Summary of a Unidimensional Confirmatory Factor Analysis</h2><span id='topic+summary_ucfa'></span>

<h3>Description</h3>

<p><code>summary_ucfa</code> provides a summary of a unidimensional confirmatory
factor analysis on a set of variables/items. Unidimensional meaning a
one-factor model where all variables/items load on that factor. The function
is a wrapper for <code><a href="lavaan.html#topic+cfa">cfa</a></code> and returns a list with four
vectors/matrices: 1) model info, 2) fit measures, 3) factor loadings, 4)
covariance/correlation residuals. For details on all the
<code><a href="lavaan.html#topic+cfa">cfa</a></code> arguments see <code><a href="lavaan.html#topic+lavOptions">lavOptions</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>summary_ucfa(
  data,
  vrb.nm,
  std.ov = FALSE,
  std.lv = TRUE,
  ordered = FALSE,
  meanstructure = TRUE,
  estimator = "ML",
  se = "standard",
  test = "standard",
  missing = "fiml",
  fit.measures = c("chisq", "df", "tli", "cfi", "rmsea", "srmr"),
  std.load = TRUE,
  resid.type = "cor.bollen",
  add.class = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary_ucfa_+3A_data">data</code></td>
<td>
<p>data.frame of data.</p>
</td></tr>
<tr><td><code id="summary_ucfa_+3A_vrb.nm">vrb.nm</code></td>
<td>
<p>character vector of colnames from <code>data</code> providing the
variables/items</p>
</td></tr>
<tr><td><code id="summary_ucfa_+3A_std.ov">std.ov</code></td>
<td>
<p>logical vector of length 1 specifying if the variables/items
should be standardized</p>
</td></tr>
<tr><td><code id="summary_ucfa_+3A_std.lv">std.lv</code></td>
<td>
<p>logical vector of length 1 specifying if the latent factor
should be standardized resulting in all factor loadings being estimated. If
FALSE, then the first variable/item in <code>data[vrb.nm]</code> is fixed to a
factor loading of 1.</p>
</td></tr>
<tr><td><code id="summary_ucfa_+3A_ordered">ordered</code></td>
<td>
<p>logical vector of length 1 specifying if the variables/items
should be treated as ordered categorical items where polychoric
correlations are used.</p>
</td></tr>
<tr><td><code id="summary_ucfa_+3A_meanstructure">meanstructure</code></td>
<td>
<p>logical vector of length 1 specifying if the mean
structure of the factor model should be estimated. This would be the
variable/item intercepts (and latent factor mean if <code>std.lv</code> = FALSE).
Note, this must be true to use Full Information Maximum Likelihood (FIML)
to handle missing data via <code>missing</code> = &quot;fiml&quot;.</p>
</td></tr>
<tr><td><code id="summary_ucfa_+3A_estimator">estimator</code></td>
<td>
<p>character vector of length 1 specifying the estimator to use
for parameter estimation. Popular options are 1) &quot;ML&quot; = maximum likelihood
estimation based on the multivariate normal distribution, 2) &quot;DWLS&quot; =
diagonally weighted least squares which uses the diagnonal of the weight
matrix, 3) &quot;WLS&quot; for weighted least squares whiches uses the full weight
matrix (often results in computational problems), 4) &quot;ULS&quot; for unweighted
least squares that doesn't use a weight matrix. &quot;DWLS&quot;, &quot;WLS&quot;, and &quot;ULS&quot;
can each be used with ordered categorical items when <code>ordered</code> = TRUE.</p>
</td></tr>
<tr><td><code id="summary_ucfa_+3A_se">se</code></td>
<td>
<p>character vector of length 1 specifying how standard errors should
be calculated. Popular options are 1) &quot;standard&quot; for conventional standard
errors from inverting the information matrix, 2) &quot;robust.sem&quot; for robust
standard errors, 3) &quot;robust.huber.white&quot; for sandwich standard errors.</p>
</td></tr>
<tr><td><code id="summary_ucfa_+3A_test">test</code></td>
<td>
<p>character vector of length 1 specifying how the omnibus test
statistic should be calculated. Popular options are 1) &quot;standard&quot; for the
conventional chi-square statistic, 2) &quot;Satorra-Bentler&quot; for the
Satorra-Bentler test statistic, 3) &quot;Yaun.Bentler.Mplus&quot; for the version of
the Yuan-Bentler test statistic that Mplus uses, 4) &quot;mean.var.adjusted&quot; for
a mean and variance adjusted test statistic, 5) &quot;scaled.shifted&quot; for the
version of the mean and variance adjusted test statistic Mplus uses.</p>
</td></tr>
<tr><td><code id="summary_ucfa_+3A_missing">missing</code></td>
<td>
<p>character vector of length 1 specifying how to handle missing
data. Popular options are 1) &quot;fiml&quot; = Full Information Maximum Likelihood
(FIML), 2) &quot;pairwise&quot; = pairwise deletion, 3) &quot;listwise&quot; = listwise
deletion.</p>
</td></tr>
<tr><td><code id="summary_ucfa_+3A_fit.measures">fit.measures</code></td>
<td>
<p>character vector specifying which model fit indices to
include in the return object. The default option includes the chi-square
test statistic (&quot;chisq&quot;), degrees of freedom (&quot;df&quot;), tucker-lewis index
(&quot;tli&quot;), comparative fit index (&quot;cfi&quot;), root mean square error of
approximation (&quot;rmsea&quot;), and standardized root mean residual (&quot;srmr&quot;).
Note, if using robust corrections for <code>se</code> and <code>test</code>, you will
probably want to call the scaled versions of model fit indices (e.g.,
&quot;chisq.scaled&quot;). See <code><a href="lavaan.html#topic+fitMeasures">fitMeasures</a></code> for details.</p>
</td></tr>
<tr><td><code id="summary_ucfa_+3A_std.load">std.load</code></td>
<td>
<p>logical vector of length 1 specifying whether the factor
loadings included in the return object should be standardized (TRUE) or not
(FALSE).</p>
</td></tr>
<tr><td><code id="summary_ucfa_+3A_resid.type">resid.type</code></td>
<td>
<p>character vector of length 1 specifying the type of
covariance/correlation residuals to include in the return object. Popular
options are 1) &quot;raw&quot; for conventional covariance residuals, 2) &quot;cor.bollen&quot;
for conventional correlation residuals, 3) &quot;cor.bentler&quot; for correlation
residuals that standardizes the model-implied covariance matrix with the
observed variances, 4) &quot;standardized&quot; for conventional z-scores of the
covariance residuals.</p>
</td></tr>
<tr><td><code id="summary_ucfa_+3A_add.class">add.class</code></td>
<td>
<p>logical vector of length 1 specifying whether the lavaan
classes should be added to the returned vectors/matrices (TRUE) or not
(FALSE). These classes do not change the underlying vector/matrix and only
affect printing.</p>
</td></tr>
<tr><td><code id="summary_ucfa_+3A_...">...</code></td>
<td>
<p>any other named arguments available in the
<code><a href="lavaan.html#topic+cfa">cfa</a></code> function. See <code><a href="lavaan.html#topic+lavOptions">lavOptions</a></code>
for the list of arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list of vectors/matrices providing statistical information about
the unidimensional confirmatory factor analysis. If <code>add.class</code> = TRUE,
then the elements have lavaan classes which affect printing (except for the
first &quot;model_info&quot; element which always is just an integer vector). The four
elements are:
</p>

<dl>
<dt>model_info</dt><dd><p>integer vector providing model information. The first element
&quot;converged&quot; is 1 if the model converged and 0 if not. The second element
&quot;admissible&quot; is 1 if the model is admissible (e.g., no negative variances)
and 0 if not. The third element &quot;nobs&quot; is the number of observations used
in the analysis. The fourth element &quot;npar&quot; is the number of parameter estimates.</p>
</dd>
<dt>fit_measures</dt><dd><p>double vector providing model fit indices. The number
and names of the fit indices is determined by the <code>fit.measures</code> argument.</p>
</dd>
<dt>factor_load</dt><dd><p>1-column double matrix providing factor loadings. The colname
is &quot;latent&quot; and the rownames are the <code>vrb.nm</code> argument.</p>
</dd>
<dt>cov_resid</dt><dd><p>covariance/correlation residuals for the model. Note, even
though the name has &quot;cov&quot; in it, the residuals can be &quot;cor&quot; if the argument
<code>resid.type</code> = &quot;cor.bollen&quot; or &quot;cor.bentler&quot;.</p>
</dd>
</dl>



<h3>See Also</h3>

<p><code><a href="#topic+ucfa">ucfa</a></code>
<code><a href="lavaan.html#topic+cfa">cfa</a></code>
<code><a href="lavaan.html#topic+lavaan">lavaan</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# types of models
dat &lt;- psych::bfi[1:250, 16:20] # nueroticism items
summary_ucfa(data = dat, vrb.nm = names(dat)) # default
summary_ucfa(data = dat, vrb.nm = names(dat), estimator = "ML", # MLR
   se = "robust.huber.white", test = "yuan.bentler.mplus", missing = "fiml",
   fit.measures = c("chisq.scaled","df.scaled","tli.scaled","cfi.scaled",
      "rmsea.scaled","srmr"))
summary_ucfa(data = dat, vrb.nm = names(dat), estimator = "ML", # MLM
   se = "robust.sem", test = "satorra.bentler", missing = "listwise",
   fit.measures = c("chisq.scaled","df.scaled","tli.scaled","cfi.scaled",
      "rmsea.scaled","srmr"))
summary_ucfa(data = dat, vrb.nm = names(dat), ordered = TRUE, estimator = "DWLS", # WLSMV
   se = "robust", test = "scaled.shifted", missing = "listwise",
   fit.measures = c("chisq.scaled","df.scaled","tli.scaled","cfi.scaled",
      "rmsea.scaled","wrmr"))

# types of info
dat &lt;- psych::bfi[1:250, 16:20] # nueroticism items
w &lt;- summary_ucfa(data = dat, vrb.nm = names(dat))
x &lt;- summary_ucfa(data = dat, vrb.nm = names(dat), add.class = FALSE)
y &lt;- summary_ucfa(data = dat, vrb.nm = names(dat),
   std.load = FALSE, resid.type = "raw")
z &lt;- summary_ucfa(data = dat, vrb.nm = names(dat),
   std.load = FALSE, resid.type = "raw", add.class = FALSE)
lapply(w, class)
lapply(x, class)
lapply(y, class)
lapply(z, class)

</code></pre>

<hr>
<h2 id='tapply2'>Apply a Function to a (Atomic) Vector by Group</h2><span id='topic+tapply2'></span>

<h3>Description</h3>

<p><code>tapply2</code> applies a function to a (atomic) vector by group and is an
alternative to the base R function <code><a href="base.html#topic+tapply">tapply</a></code>. The function is
apart of the split-apply-combine type of function discussed in the
<code>plyr</code> R package and is somewhat similar to <code><a href="plyr.html#topic+dlply">dlply</a></code>.
It splits up one (atomic) vector <code>.x</code>into a (atomic) vector for each
group in <code>.grp</code>, applies a function <code>.fun</code> to each (atomic) vector,
and then returns the results as a list with names equal to the group values
<code>unique(interaction(.grp.nm, sep = .sep))</code>. <code>tapply2</code> is simply
<code>split.default</code> + <code>lapply</code>. Similar to <code>dlply</code>, The arguments
all start with <code>.</code> so that they do not conflict with arguments from the
function <code>.fun</code>. If you want to apply a function a data.frame rather
than a (atomic) vector, then use <code><a href="#topic+by2">by2</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tapply2(.x, .grp, .sep = ".", .fun, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tapply2_+3A_.x">.x</code></td>
<td>
<p>atomic vector</p>
</td></tr>
<tr><td><code id="tapply2_+3A_.grp">.grp</code></td>
<td>
<p>list of atomic vector(s) and/or factor(s) (e.g., data.frame)
containing the groups. They should each have same length as <code>.x</code>. It
can also be an atomic vector or factor, which will then be made the first
element of a list internally.</p>
</td></tr>
<tr><td><code id="tapply2_+3A_.sep">.sep</code></td>
<td>
<p>character vector of length 1 specifying the string to combine the
group values together with. <code>.sep</code> is only used if there are multiple
grouping variables (i.e., <code>.grp</code> is a list with multiple elements).</p>
</td></tr>
<tr><td><code id="tapply2_+3A_.fun">.fun</code></td>
<td>
<p>function to apply to <code>.x</code> for each group.</p>
</td></tr>
<tr><td><code id="tapply2_+3A_...">...</code></td>
<td>
<p>additional named arguments to pass to <code>.fun</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list of objects containing the return object of <code>.fun</code> for each
group. The names are the unique combinations of the grouping variables
(i.e., <code>unique(interaction(.grp, sep = .sep))</code>).
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+tapply">tapply</a></code>
<code><a href="#topic+by2">by2</a></code>
<code><a href="plyr.html#topic+dlply">dlply</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# one grouping variable
tapply2(mtcars$"cyl", .grp = mtcars$"vs", .fun = median, na.rm = TRUE)

# two grouping variables
grp_nm &lt;- c("vs","am") # Roxygen runs the whole script if I put a c() in a []
x &lt;- tapply2(mtcars$"cyl", .grp = mtcars[grp_nm], .fun = median, na.rm = TRUE)
print(x)
str(x)

# compare to tapply
grp_nm &lt;- c("vs","am") # Roxygen runs the whole script if I put a c() in a []
y &lt;- tapply(mtcars$"cyl", INDEX = mtcars[grp_nm],
   FUN = median, na.rm = TRUE, simplify = FALSE)
print(y)
str(y) # has dimnames rather than names
</code></pre>

<hr>
<h2 id='ucfa'>Unidimensional Confirmatory Factor Analysis</h2><span id='topic+ucfa'></span>

<h3>Description</h3>

<p><code>ucfa</code> conducts a unidimensional confirmatory factor analysis on a set
of variables/items. Unidimensional meaning a one-factor model where all
variables/items load on that factor. The function is a wrapper for
<code><a href="lavaan.html#topic+cfa">cfa</a></code> and returns an object of class &quot;lavaan&quot;:
<code>lavaan</code>. This then allows the user to extract
statistical information from the object (e.g.,
<code><a href="lavaan.html#topic+lavInspect">lavInspect</a></code>). For details on all the arguments see
<code><a href="lavaan.html#topic+lavOptions">lavOptions</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ucfa(
  data,
  vrb.nm,
  std.ov = FALSE,
  std.lv = TRUE,
  ordered = FALSE,
  meanstructure = TRUE,
  estimator = "ML",
  se = "standard",
  test = "standard",
  missing = "fiml",
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ucfa_+3A_data">data</code></td>
<td>
<p>data.frame of data.</p>
</td></tr>
<tr><td><code id="ucfa_+3A_vrb.nm">vrb.nm</code></td>
<td>
<p>character vector of colnames from <code>data</code> providing the
variables/items</p>
</td></tr>
<tr><td><code id="ucfa_+3A_std.ov">std.ov</code></td>
<td>
<p>logical vector of length 1 specifying if the variables/items
should be standardized</p>
</td></tr>
<tr><td><code id="ucfa_+3A_std.lv">std.lv</code></td>
<td>
<p>logical vector of length 1 specifying if the latent factor
should be standardized resulting in all factor loadings being estimated. If
FALSE, then the first variable/item in <code>data[vrb.nm]</code> is fixed to a
factor loading of 1.</p>
</td></tr>
<tr><td><code id="ucfa_+3A_ordered">ordered</code></td>
<td>
<p>logical vector of length 1 specifying if the variables/items
should be treated as ordered categorical items where polychoric
correlations are used.</p>
</td></tr>
<tr><td><code id="ucfa_+3A_meanstructure">meanstructure</code></td>
<td>
<p>logical vector of length 1 specifying if the mean
structure of the factor model should be estimated. This would be the
variable/item intercepts (and latent factor mean if <code>std.lv</code> = FALSE).
Note, this must be true to use Full Information Maximum Likelihood (FIML)
to handle missing data via <code>missing</code> = &quot;fiml&quot;.</p>
</td></tr>
<tr><td><code id="ucfa_+3A_estimator">estimator</code></td>
<td>
<p>character vector of length 1 specifying the estimator to use
for parameter estimation. Popular options are 1) &quot;ML&quot; = maximum likelihood
estimation based on the multivariate normal distribution, 2) &quot;DWLS&quot; =
diagonally weighted least squares which uses the diagnonal of the weight
matrix, 3) &quot;WLS&quot; for weighted least squares whiches uses the full weight
matrix (often results in computational problems), 4) &quot;ULS&quot; for unweighted
least squares that doesn't use a weight matrix. &quot;DWLS&quot;, &quot;WLS&quot;, and &quot;ULS&quot;
can each be used with ordered categorical items when <code>ordered</code> = TRUE.</p>
</td></tr>
<tr><td><code id="ucfa_+3A_se">se</code></td>
<td>
<p>character vector of length 1 specifying how standard errors should
be calculated. Popular options are 1) &quot;standard&quot; for conventional standard
errors from inverting the information matrix, 2) &quot;robust.sem&quot; for robust
standard errors, 3) &quot;robust.huber.white&quot; for sandwich standard errors.</p>
</td></tr>
<tr><td><code id="ucfa_+3A_test">test</code></td>
<td>
<p>character vector of length 1 specifying how the omnibus test
statistic should be calculated. Popular options are 1) &quot;standard&quot; for the
conventional chi-square statistic, 2) &quot;Satorra-Bentler&quot; for the
Satorra-Bentler test statistic, 3) &quot;Yaun.Bentler.Mplus&quot; for the version of
the Yuan-Bentler test statistic that Mplus uses, 4) &quot;mean.var.adjusted&quot; for
a mean and variance adjusted test statistic, 5) &quot;scaled.shifted&quot; for the
version of the mean and variance adjusted test statistic Mplus uses.</p>
</td></tr>
<tr><td><code id="ucfa_+3A_missing">missing</code></td>
<td>
<p>character vector of length 1 specifying how to handle missing
data. Popular options are 1) &quot;fiml&quot; = Full Information Maximum Likelihood
(FIML), 2) &quot;pairwise&quot; = pairwise deletion, 3) &quot;listwise&quot; = listwise
deletion.</p>
</td></tr>
<tr><td><code id="ucfa_+3A_...">...</code></td>
<td>
<p>any other named arguments available in the
<code><a href="lavaan.html#topic+cfa">cfa</a></code> function. See <code><a href="lavaan.html#topic+lavOptions">lavOptions</a></code>
for the list of arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>object of class &quot;lavaan&quot; <code>lavaan</code>
providing the return object from a call to <code><a href="lavaan.html#topic+cfa">cfa</a></code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+summary_ucfa">summary_ucfa</a></code>
<code><a href="lavaan.html#topic+cfa">cfa</a></code>
<code><a href="lavaan.html#topic+lavaan">lavaan</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
dat &lt;- psych::bfi[1:250, 16:20] # nueroticism items
ucfa(data = dat, vrb.nm = names(dat))
ucfa(data = dat, vrb.nm = names(dat), std.ov = TRUE)
ucfa(data = dat, vrb.nm = names(dat), meanstructure = FALSE, missing = "pairwise")
ucfa(data = dat, vrb.nm = names(dat), estimator = "ML", # MLR
   se = "robust.huber.white", test = "yuan.bentler.mplus", missing = "fiml")
ucfa(data = dat, vrb.nm = names(dat), estimator = "ML", # MLM
   se = "robust.sem", test = "satorra.bentler", missing = "listwise")
ucfa(data = dat, vrb.nm = names(dat), ordered = TRUE, estimator = "DWLS", # WLSMV
   se = "robust", test = "scaled.shifted", missing = "listwise")

</code></pre>

<hr>
<h2 id='valid_test'>Test for Invalid Elements in a Vector</h2><span id='topic+valid_test'></span>

<h3>Description</h3>

<p><code>valid_test</code> tests whether a vector has any invalid elements. Valid
values are specified by <code>valid</code>. If the vector <code>x</code> has any values
other than <code>valid</code>, then FALSE is returned; If the vector <code>x</code> only
has values in <code>valid</code>, then TRUE is returned. This function can be
useful for checking data after manual human entry.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>valid_test(x, valid, na.rm = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="valid_test_+3A_x">x</code></td>
<td>
<p>atomic vector or list vector.</p>
</td></tr>
<tr><td><code id="valid_test_+3A_valid">valid</code></td>
<td>
<p>atomic vector or list vector of valid values.</p>
</td></tr>
<tr><td><code id="valid_test_+3A_na.rm">na.rm</code></td>
<td>
<p>logical vector of length 1 specifying whether NA should be
ignored from the validity test. If TRUE (default), then any NAs are treated
as valid.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>logical vector of length 1 specifying whether all elements in
<code>x</code> are valid values. If FALSE, then (at least one) invalid values are
present.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+valids_test">valids_test</a></code>
<code><a href="#topic+revalid">revalid</a></code>
<code><a href="#topic+revalids">revalids</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>valid_test(x = psych::bfi[[1]], valid = 1:6) # return TRUE
valid_test(x = psych::bfi[[1]], valid = 0:5) # 6 is not present in `valid`
valid_test(x = psych::bfi[[1]], valid = 1:6,
   na.rm = FALSE) # NA is not present in `valid`
</code></pre>

<hr>
<h2 id='valids_test'>Test for Invalid Elements in Data</h2><span id='topic+valids_test'></span>

<h3>Description</h3>

<p><code>Valid.test</code> tests whether data has any invalid elements. Valid values
are specified by <code>valid</code>. Each variable is tested independently. If the
variable in <code>data[vrb.nm]</code> has any values other than <code>valid</code>, then
FALSE is returned for that variable; If the variable in <code>data[vrb.nm]</code>
only has values in <code>valid</code>, then TRUE is returned for that variable.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>valids_test(data, vrb.nm, valid, na.rm = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="valids_test_+3A_data">data</code></td>
<td>
<p>data.frame of data.</p>
</td></tr>
<tr><td><code id="valids_test_+3A_vrb.nm">vrb.nm</code></td>
<td>
<p>character vector of colnames from <code>data</code> specifying the
variables</p>
</td></tr>
<tr><td><code id="valids_test_+3A_valid">valid</code></td>
<td>
<p>atomic vector or list vector of valid values.</p>
</td></tr>
<tr><td><code id="valids_test_+3A_na.rm">na.rm</code></td>
<td>
<p>logical vector of length 1 specifying whether NA should be
ignored from the validity test. If TRUE (default), then any NAs are treated
as valid.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>logical vector with length = <code>length(vrb.nm)</code> and names =
<code>vrb.nm</code> specifying whether all elements in each variable of
<code>data[vrb.nm]</code> are valid. If FALSE, then (at least one) invalid values
are present in that variable of <code>data[vrb.nm]</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+valid_test">valid_test</a></code>
<code><a href="#topic+revalids">revalids</a></code>
<code><a href="#topic+revalid">revalid</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>valids_test(data = psych::bfi, vrb.nm = names(psych::bfi)[1:25],
   valid = 1:6) # return TRUE
valids_test(data = psych::bfi, vrb.nm = names(psych::bfi)[1:25],
   valid = 0:5) # 6 is not present in `valid`
valids_test(data = psych::bfi, vrb.nm = names(psych::bfi)[1:25],
   valid = 1:6, na.rm = FALSE) # NA is not present in `valid`
valids_test(data = ToothGrowth, vrb.nm = c("supp","dose"),
   valid = list("VC", "OJ", 0.5, 1.0, 2.0)) # list vector as `valid` to allow for
   # elements of different typeof
</code></pre>

<hr>
<h2 id='vecNA'>Frequency of Missing Values in a Vector</h2><span id='topic+vecNA'></span>

<h3>Description</h3>

<p><code>vecNA</code> computes the frequency of missing values in an atomic vector.
<code>vecNA</code> is essentially a wrapper for <code>sum</code> or <code>mean</code> +
<code>is.na</code> or <code>!is.na</code> and can be useful for functional programming
(e.g., <code>lapply(FUN = vecNA)</code>). It is also used by other functions in the
quest package related to missing values (e.g., <code><a href="#topic+mean_if">mean_if</a></code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vecNA(x, prop = FALSE, ov = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vecNA_+3A_x">x</code></td>
<td>
<p>atomic vector or list vector. If not a vector, it will be coerced to
a vector via <code><a href="base.html#topic+as.vector">as.vector</a></code>.</p>
</td></tr>
<tr><td><code id="vecNA_+3A_prop">prop</code></td>
<td>
<p>logical vector of length 1 specifying whether the frequency of
missing values should be returned as a proportion (TRUE) or a count
(FALSE).</p>
</td></tr>
<tr><td><code id="vecNA_+3A_ov">ov</code></td>
<td>
<p>logical vector of length 1 specifying whether the frequency of
observed values (TRUE) should be returned rather than the frequency of
missing values (FALSE).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>numeric vector of length 1 providing the frequency of missing values
(or observed values if <code>ov</code> = TRUE). If <code>prop</code> = TRUE, the value
will range from 0 to 1. If <code>prop</code> = FALSE, the value will range from 1
to <code>length(x)</code>.
</p>


<h3>See Also</h3>

<p><code><a href="Matrix.html#topic+is.na">is.na</a></code>
<code><a href="#topic+rowNA">rowNA</a></code>
<code><a href="#topic+colNA">colNA</a></code>
<code><a href="#topic+rowsNA">rowsNA</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>vecNA(airquality[[1]]) # count of missing values
vecNA(airquality[[1]], prop = TRUE) # proportion of missing values
vecNA(airquality[[1]], ov = TRUE) # count of observed values
vecNA(airquality[[1]], prop = TRUE, ov = TRUE) # proportion of observed values
</code></pre>

<hr>
<h2 id='wide2long'>Reshape Multiple Sets of Variables From Wide to Long</h2><span id='topic+wide2long'></span>

<h3>Description</h3>

<p><code>wide2long</code> reshapes data from wide to long. This if often necessary to
do with multilevel data where multiple sets of variables in the wide format
seek to be reshaped to multiple rows in the long format. If only one set of
variables needs to be reshaped, then you can use
<code><a href="str2str.html#topic+stack2">stack2</a></code> or <code><a href="reshape.html#topic+melt.data.frame">melt.data.frame</a></code> - but that
does not work for *multiple* sets of variables. See details for more
information.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>wide2long(
  data,
  vrb.nm.list,
  grp.nm = NULL,
  sep = ".",
  rtn.obs.nm = "obs",
  order.by.grp = TRUE,
  keep.attr = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="wide2long_+3A_data">data</code></td>
<td>
<p>data.frame of multilevel data in the wide format.</p>
</td></tr>
<tr><td><code id="wide2long_+3A_vrb.nm.list">vrb.nm.list</code></td>
<td>
<p>A unique argument for the <code>quest</code> package such that
it can take on different types of inputs. The conventional use is to
provide a list of character vectors specifying each set of colnames to be
reshaped. In longitudinal panel data, each list element would contain a
score with multiple timepoints. The advanced use is to provide a single
character vector specifying the colnames to be reshaped (not organized by
sets). See details.</p>
</td></tr>
<tr><td><code id="wide2long_+3A_grp.nm">grp.nm</code></td>
<td>
<p>character vector specifying the colnames in <code>data</code>
corresponding to the groups. Because <code>data</code> is in the wide format,
<code>data[grp.nm]</code> must have unique rows (aka groups); if this is not the
case, an error is returned. <code>grp.nm</code> can be NULL, in which case the
rownames of <code>data</code> will be used. In longitudinal panel data this
variable would be the participant ID variable.</p>
</td></tr>
<tr><td><code id="wide2long_+3A_sep">sep</code></td>
<td>
<p>character vector of length 1 specifying the string in the column
names provided by <code>vrb.nm.list</code> that separates out the name prefix
from the number suffix. If <code>sep</code> = &quot;&quot;, then that implies there is no
string separating the name prefix and the number suffix (e.g., &quot;outcome1&quot;).</p>
</td></tr>
<tr><td><code id="wide2long_+3A_rtn.obs.nm">rtn.obs.nm</code></td>
<td>
<p>character vector of length 1 specifying the new colname in
the return object indicating which observation within each group the row
refers to. In longitudinal panel data, this would be the returned time
variable.</p>
</td></tr>
<tr><td><code id="wide2long_+3A_order.by.grp">order.by.grp</code></td>
<td>
<p>logical vector of length 1 specifying whether to sort the
return object first by <code>grp.nm</code> and then <code>obs.nm</code> (TRUE) or by
<code>obs.nm</code> and then <code>grp.nm</code> (FALSE).</p>
</td></tr>
<tr><td><code id="wide2long_+3A_keep.attr">keep.attr</code></td>
<td>
<p>logical vector of length 1 specifying whether to keep the
&quot;reshapeLong&quot; attribute (from <code><a href="stats.html#topic+reshape">reshape</a></code>) in the return
object.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>wide2long</code> uses <code>reshape(direction = "long")</code> to reshape the data.
It attempts to streamline the task of reshaping wide to long as the
<code>reshape</code> arguments can be confusing because the same arguments are used
for wide vs. long reshaping. See <code><a href="stats.html#topic+reshape">reshape</a></code> if you are
curious.
</p>
<p>IF <code>vrb.nm.list</code> IS A LIST OF CHARACTER VECTORS: The conventional use of
<code>vrb.nm.list</code> is to provide a list of character vectors, which specify
each set of variables to be reshaped. For example, if <code>data</code> contains
data from a longitudinal panel study with the same scores at different waves,
then there might be a column for each score at each wave. <code>vrb.nm.list</code>
would then contain an element for each score with each element containing a
character vector of the colnames for that score at each wave (see examples).
The names of the list elements would then be the colnames in the return
object for those scores.
</p>
<p>IF <code>vrb.nm.list</code> IS A CHARACTER VECTOR: The advanced use of
<code>vrb.nm.list</code> is to provide a single character vector, which specify the
variables to be reshaped (not organized by sets). In this case (i.e., if
<code>vrb.nm.list</code> is not a list), then <code>wide2long</code> (really
<code><a href="stats.html#topic+reshape">reshape</a></code>) will attempt to guess which colnames go
together as a set. It is assumed the following column naming scheme has been
used: 1) have the same name prefix for columns within a set, 2) have the same
number suffixes for each set of columns, 3) use, *and only use*, <code>sep</code>
in the colnames to separate the name prefix and the number suffix. For
example, the name prefixes might be &quot;predictor&quot; and &quot;outcome&quot; while the
number suffixes might be &quot;0&quot;, &quot;1&quot;, and &quot;2&quot;, and the separator might be &quot;.&quot;,
resulting in column names such as &quot;outcome.1&quot;. The name prefix could include
separators other than <code>sep</code> (e.g., &quot;outcome_item.1&quot;), but it cannot
include <code>sep</code> (e.g., &quot;outcome.item.1&quot;). So &quot;outcome_item1.1&quot; could be
acceptable, but &quot;outcome.item1.1&quot; would not.
</p>


<h3>Value</h3>

<p>data.frame with nrow equal to <code>nrow(data) *
  length(vrb.nm.list[[1]])</code> if <code>vrb.nm.list</code> is a list (i.e.,
conventional use) or <code>nrow(data)</code> * number of unique number suffixes
in <code>vrb.nm.list</code> if <code>vrb.nm.list</code> is not a list (i.e., advanced
use). The columns will be in the following order: 1) <code>grp.nm</code> of the
groups, 2) <code>rtn.obs.nm</code> of the observation labels, 3) the reshaped
columns, 4) the additional columns that were not reshaped and instead
repeated. How the returned data.frame is sorted depends on
<code>order.by.grp</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+long2wide">long2wide</a></code>
<code><a href="stats.html#topic+reshape">reshape</a></code>
<code><a href="str2str.html#topic+stack2">stack2</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# SINGLE GROUPING VARIABLE
dat_wide &lt;- data.frame(
   x_1.1 = runif(5L),
   x_2.1 = runif(5L),
   x_3.1 = runif(5L),
   x_4.1 = runif(5L),
   x_1.2 = runif(5L),
   x_2.2 = runif(5L),
   x_3.2 = runif(5L),
   x_4.2 = runif(5L),
   x_1.3 = runif(5L),
   x_2.3 = runif(5L),
   x_3.3 = runif(5L),
   x_4.3 = runif(5L),
   y_1.1 = runif(5L),
   y_2.1 = runif(5L),
   y_1.2 = runif(5L),
   y_2.2 = runif(5L),
   y_1.3 = runif(5L),
   y_2.3 = runif(5L))
row.names(dat_wide) &lt;- letters[1:5]
print(dat_wide)

# vrb.nm.list = list of character vectors (conventional use)
vrb_pat &lt;- c("x_1","x_2","x_3","x_4","y_1","y_2")
vrb_nm_list &lt;- lapply(X = setNames(vrb_pat, nm = vrb_pat), FUN = function(pat) {
   str2str::pick(x = names(dat_wide), val = pat, pat = TRUE)})
# without `grp.nm`
z1 &lt;- wide2long(dat_wide, vrb.nm = vrb_nm_list)
# with `grp.nm`
dat_wide$"ID" &lt;- letters[1:5]
z2 &lt;- wide2long(dat_wide, vrb.nm = vrb_nm_list, grp.nm = "ID")
dat_wide$"ID" &lt;- NULL

# vrb.nm.list = character vector + guessing (advanced use)
vrb_nm &lt;- str2str::pick(x = names(dat_wide), val = "ID", not = TRUE)
# without `grp.nm`
z3 &lt;- wide2long(dat_wide, vrb.nm.list = vrb_nm)
# with `grp.nm`
dat_wide$"ID" &lt;- letters[1:5]
z4 &lt;- wide2long(dat_wide, vrb.nm = vrb_nm, grp.nm = "ID")
dat_wide$"ID" &lt;- NULL

# comparisons
head(z1); head(z3); head(z2); head(z4)
all.equal(z1, z3)
all.equal(z2, z4)
# keeping the reshapeLong attributes
z7 &lt;- wide2long(dat_wide, vrb.nm = vrb_nm_list, keep.attr = TRUE)
attributes(z7)

# MULTIPLE GROUPING VARIABLES
bfi2 &lt;- psych::bfi
bfi2$"person" &lt;- unlist(lapply(X = 1:400, FUN = rep.int, times = 7))
bfi2$"day" &lt;- rep.int(1:7, times = 400L)
head(bfi2, n = 15)

# vrb.nm.list = list of character vectors (conventional use)
vrb_pat &lt;- c("A","C","E","N","O")
vrb_nm_list &lt;- lapply(X = setNames(vrb_pat, nm = vrb_pat), FUN = function(pat) {
   str2str::pick(x = names(bfi2), val = pat, pat = TRUE)})
z5 &lt;- wide2long(bfi2, vrb.nm.list = vrb_nm_list, grp = c("person","day"),
   rtn.obs.nm = "item")

# vrb.nm.list = character vector + guessing (advanced use)
vrb_nm &lt;- str2str::pick(x = names(bfi2),
   val = c("person","day","gender","education","age"), not = TRUE)
z6 &lt;- wide2long(bfi2, vrb.nm.list = vrb_nm, grp = c("person","day"),
   sep = "", rtn.obs.nm = "item") # need sep = "" because no character separating
   # scale name and item number
all.equal(z5, z6)

</code></pre>

<hr>
<h2 id='winsor'>Winsorize a Numeric Vector</h2><span id='topic+winsor'></span>

<h3>Description</h3>

<p><code>winsor</code> winsorizes a numeric vector by recoding extreme values as a user-identified boundary value, which is defined by z-score units. The <code>to.na</code>
argument provides the option of recoding the extreme values as missing.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>winsor(x, z.min = -3, z.max = 3, rtn.int = FALSE, to.na = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="winsor_+3A_x">x</code></td>
<td>
<p>numeric vector</p>
</td></tr>
<tr><td><code id="winsor_+3A_z.min">z.min</code></td>
<td>
<p>numeric vector of length 1 specifying the lower boundary value
in z-score units.</p>
</td></tr>
<tr><td><code id="winsor_+3A_z.max">z.max</code></td>
<td>
<p>numeric vector of length 1 specifying the upper boundary value
in z-score units.</p>
</td></tr>
<tr><td><code id="winsor_+3A_rtn.int">rtn.int</code></td>
<td>
<p>logical vector of length 1 specifying whether the recoded values
should be rounded to the nearest integer. This can be useful when working with
count data and decimal values are impossible.</p>
</td></tr>
<tr><td><code id="winsor_+3A_to.na">to.na</code></td>
<td>
<p>logical vector of length 1 specifying whether the extreme values
should be recoded to NA rather than winsorized to the boundary values.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note, the psych package also has a function called <code>winsor</code>, which offers
the option to winsorize a numeric vector by quantiles rather than z-scores. If you have both the quest package and the psych
package attached in your current R session (e.g., using <code>library</code>),
depending on which package you attached first, R might default to using the
<code>winsor</code> function in either the quest package or the psych package. One
way to deal with this issue is to explicitly call which package you want to
use the <code>winsor</code> package from. You can do this using the <code>::</code>
function in base R where the package name comes before the <code>::</code> and the
function names comes after it (e.g., <code>quest::winsor</code>).
</p>


<h3>Value</h3>

<p>numeric vector of the same length as <code>x</code> with extreme values
recoded as either the boundary values or NA.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+winsors">winsors</a></code>
<code><a href="psych.html#topic+winsor">winsor</a></code> # psych package
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# winsorize
table(quakes$"stations")
new &lt;- winsor(quakes$"stations")
table(new)

# recode as NA
vecNA(quakes$"stations")
new &lt;- winsor(quakes$"stations", to.na = TRUE)
vecNA(new)

# rtn.int = TRUE
winsor(x = cars[[1]], z.min = -2, z.max = 2, rtn.int = FALSE)
winsor(x = cars[[1]], z.min = -2, z.max = 2, rtn.int = TRUE)
</code></pre>

<hr>
<h2 id='winsors'>Winsorize Numeric Data</h2><span id='topic+winsors'></span>

<h3>Description</h3>

<p><code>winsors</code> winsorizes numeric data by recoding extreme values as a user
identified boundary value, which is defined by z-score units. The <code>to.na</code>
argument provides the option of recoding the extreme values as missing.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>winsors(
  data,
  vrb.nm,
  z.min = -3,
  z.max = 3,
  rtn.int = FALSE,
  to.na = FALSE,
  suffix = "_win"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="winsors_+3A_data">data</code></td>
<td>
<p>data.frame of data.</p>
</td></tr>
<tr><td><code id="winsors_+3A_vrb.nm">vrb.nm</code></td>
<td>
<p>character vector of colnames from <code>data</code> specifying the variables.</p>
</td></tr>
<tr><td><code id="winsors_+3A_z.min">z.min</code></td>
<td>
<p>numeric vector of length 1 specifying the lower boundary value
in z-score units.</p>
</td></tr>
<tr><td><code id="winsors_+3A_z.max">z.max</code></td>
<td>
<p>numeric vector of length 1 specifying the upper boundary value
in z-score units.</p>
</td></tr>
<tr><td><code id="winsors_+3A_rtn.int">rtn.int</code></td>
<td>
<p>logical vector of length 1 specifying whether the recoded values
should be rounded to the nearest integer. This can be useful when working with
count data and decimal values are impossible.</p>
</td></tr>
<tr><td><code id="winsors_+3A_to.na">to.na</code></td>
<td>
<p>logical vector of length 1 specifying whether the extreme values
should be recoded to NA rather than winsorized to the boundary values.</p>
</td></tr>
<tr><td><code id="winsors_+3A_suffix">suffix</code></td>
<td>
<p>character vector of length 1 specifying the string to append
to the end of the colnames in the return object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data.frame of winsorized data with extreme values recoded as either
the boundary values or NA and colnames = <code>paste0(vrb.nm, suffix)</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+winsor">winsor</a></code>
<code><a href="psych.html#topic+winsor">winsor</a></code> # psych package
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# winsorize
lapply(X = quakes[c("mag","stations")], FUN = table)
new &lt;- winsors(quakes, vrb.nm = names(quakes))
lapply(X = new, FUN = table)

# recode as NA
vecNA(quakes)
new &lt;- winsors(quakes, vrb.nm = names(quakes), to.na = TRUE)
vecNA(new)

# rtn.int = TRUE
winsors(data = cars, vrb.nm = names(cars), z.min = -2, z.max = 2, rtn.int = FALSE)
winsors(data = cars, vrb.nm = names(cars), z.min = -2, z.max = 2, rtn.int = TRUE)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
