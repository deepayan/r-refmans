<!DOCTYPE html><html><head><title>Help for package mboost</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {mboost}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#baselearners'><p> Base-learners for Gradient Boosting</p></a></li>
<li><a href='#blackboost'><p> Gradient Boosting with Regression Trees</p></a></li>
<li><a href='#boost_control'><p> Control Hyper-parameters for Boosting Algorithms</p></a></li>
<li><a href='#boost_family-class'><p>Class &quot;boost_family&quot;: Gradient Boosting Family</p></a></li>
<li><a href='#confint.mboost'>
<p>Pointwise Bootstrap Confidence Intervals</p></a></li>
<li><a href='#cvrisk'><p> Cross-Validation</p></a></li>
<li><a href='#Family'><p> Gradient Boosting Families</p></a></li>
<li><a href='#FP'><p> Fractional Polynomials</p></a></li>
<li><a href='#glmboost'><p> Gradient Boosting with Component-wise Linear Models</p></a></li>
<li><a href='#IPCweights'><p> Inverse Probability of Censoring Weights</p></a></li>
<li><a href='#mboost'><p> Gradient Boosting for Additive Models</p></a></li>
<li><a href='#mboost_fit'><p> Model-based Gradient Boosting</p></a></li>
<li><a href='#mboost_intern'>
<p>Call internal functions.</p></a></li>
<li><a href='#mboost-package'>
<p>mboost: Model-Based Boosting</p></a></li>
<li><a href='#methods'><p> Methods for Gradient Boosting Objects</p></a></li>
<li><a href='#plot'>
<p>Plot effect estimates of boosting models</p></a></li>
<li><a href='#stabsel'>
<p>Stability Selection</p></a></li>
<li><a href='#survFit'><p> Survival Curves for a Cox Proportional Hazards Model</p></a></li>
<li><a href='#varimp'><p> Variable Importance</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Model-Based Boosting</td>
</tr>
<tr>
<td>Version:</td>
<td>2.9-9</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-12-07</td>
</tr>
<tr>
<td>Description:</td>
<td>Functional gradient descent algorithm
  (boosting) for optimizing general risk functions utilizing
  component-wise (penalised) least squares estimates or regression
  trees as base-learners for fitting generalized linear, additive
  and interaction models to potentially high-dimensional data.
  Models and algorithms are described in &lt;<a href="https://doi.org/10.1214%2F07-STS242">doi:10.1214/07-STS242</a>&gt;,
  a hands-on tutorial is available from &lt;<a href="https://doi.org/10.1007%2Fs00180-012-0382-5">doi:10.1007/s00180-012-0382-5</a>&gt;.
  The package allows user-specified loss functions and base-learners.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.2.0), methods, stats, parallel, stabs (&ge; 0.5-0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>Matrix, survival (&ge; 3.2-10), splines, lattice, nnls,
quadprog, utils, graphics, grDevices, partykit (&ge; 1.2-1)</td>
</tr>
<tr>
<td>Suggests:</td>
<td>TH.data, MASS, fields, BayesX, gbm, mlbench, RColorBrewer,
rpart (&ge; 4.0-3), randomForest, nnet, testthat (&ge; 0.10.0),
kangar00</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/boost-R/mboost/issues">https://github.com/boost-R/mboost/issues</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/boost-R/mboost">https://github.com/boost-R/mboost</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-12-07 15:19:38 UTC; hothorn</td>
</tr>
<tr>
<td>Author:</td>
<td>Torsten Hothorn <a href="https://orcid.org/0000-0001-8301-0471"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [cre, aut],
  Peter Buehlmann <a href="https://orcid.org/0000-0002-1782-6015"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut],
  Thomas Kneib <a href="https://orcid.org/0000-0003-3390-0972"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut],
  Matthias Schmid <a href="https://orcid.org/0000-0002-0788-0317"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut],
  Benjamin Hofner <a href="https://orcid.org/0000-0003-2810-3186"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut],
  Fabian Otto-Sobotka
    <a href="https://orcid.org/0000-0002-9874-1311"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [ctb],
  Fabian Scheipl <a href="https://orcid.org/0000-0001-8172-3603"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [ctb],
  Andreas Mayr <a href="https://orcid.org/0000-0001-7106-9732"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Torsten Hothorn &lt;Torsten.Hothorn@R-project.org&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-12-07 17:40:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='baselearners'> Base-learners for Gradient Boosting </h2><span id='topic+baselearners'></span><span id='topic+baselearner'></span><span id='topic+base-learner'></span><span id='topic+bols'></span><span id='topic+bbs'></span><span id='topic+bspatial'></span><span id='topic+brad'></span><span id='topic+bkernel'></span><span id='topic+brandom'></span><span id='topic+btree'></span><span id='topic+bmono'></span><span id='topic+bmrf'></span><span id='topic+buser'></span><span id='topic+bns'></span><span id='topic+bss'></span><span id='topic++25+2B+25'></span><span id='topic++25X+25'></span><span id='topic++25O+25'></span>

<h3>Description</h3>

<p>Base-learners for fitting base-models in the generic implementation of
component-wise gradient boosting in function <code>mboost</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## linear base-learner
bols(..., by = NULL, index = NULL, intercept = TRUE, df = NULL,
     lambda = 0, contrasts.arg = "contr.treatment")

## smooth P-spline base-learner
bbs(..., by = NULL, index = NULL, knots = 20, boundary.knots = NULL,
    degree = 3, differences = 2, df = 4, lambda = NULL, center = FALSE,
    cyclic = FALSE, constraint = c("none", "increasing", "decreasing"),
    deriv = 0)

## bivariate P-spline base-learner
bspatial(..., df = 6)

## radial basis functions base-learner
brad(..., by = NULL, index = NULL, knots = 100, df = 4, lambda = NULL,
     covFun = fields::stationary.cov,
     args = list(Covariance="Matern", smoothness = 1.5, theta=NULL))
     
## (genetic) pathway-based kernel base-learner
bkernel(..., df = 4, lambda = NULL, kernel = c("lin", "sia", "net"),
        pathway = NULL, knots = NULL, args = list())

## random effects base-learner
brandom(..., by = NULL, index = NULL, df = 4, lambda = NULL,
        contrasts.arg = "contr.dummy")

## tree based base-learner
btree(..., by = NULL, nmax = Inf, tree_controls = partykit::ctree_control(
    teststat = "quad", testtype = "Teststatistic", 
    mincriterion = 0, minsplit = 10, minbucket = 4,
    maxdepth = 1, saveinfo = FALSE))

## constrained effects base-learner
bmono(...,
      constraint = c("increasing", "decreasing", "convex", "concave",
                     "none", "positive", "negative"),
      type = c("quad.prog", "iterative"),
      by = NULL, index = NULL, knots = 20, boundary.knots = NULL,
      degree = 3, differences = 2, df = 4, lambda = NULL,
      lambda2 = 1e6, niter=10, intercept = TRUE,
      contrasts.arg = "contr.treatment",
      boundary.constraints = FALSE,
      cons.arg = list(lambda = 1e+06, n = NULL, diff_order = NULL))

## Markov random field base-learner
bmrf(..., by = NULL, index = NULL, bnd = NULL, df = 4, lambda = NULL,
    center = FALSE)

## user-specified base-learner
buser(X, K = NULL, by = NULL, index = NULL, df = 4, lambda = NULL)

## combining single base-learners to form new,
## more complex base-learners
bl1 %+% bl2
bl1 %X% bl2
bl1 %O% bl2
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="baselearners_+3A_...">...</code></td>
<td>
<p> one or more predictor variables or one matrix or data
frame of predictor variables. For smooth base-learners,
the number of predictor variables and the number of
columns in the data frame / matrix must be less than or
equal to 2. If a matrix (with at least 2 columns) is
given to <code>bols</code> or <code>brandom</code>, it is directly
used as the design matrix. Especially, no intercept term
is added regardless of argument <code>intercept</code>.
If the argument has only one column, it is simplified
to a vector and an intercept is added or not
according to the argmuent <code>intercept</code>.</p>
</td></tr>
<tr><td><code id="baselearners_+3A_by">by</code></td>
<td>
<p> an optional variable defining varying coefficients,
either a factor or numeric variable.
If <code>by</code> is a factor, the coding is determined by
the global <code>options("contrasts")</code> or as specified
&quot;locally&quot; for the factor (see <code><a href="stats.html#topic+contrasts">contrasts</a></code>). Per
default treatment coding is used. Note that the main
effect needs to be specified in a separate base-learner.
<code>btree</code> currently only allows binary factors.</p>
</td></tr>
<tr><td><code id="baselearners_+3A_index">index</code></td>
<td>
<p> a vector of integers for expanding the variables in
<code>...</code>. For example, <code>bols(x, index = index)</code> is equal to
<code>bols(x[index])</code>, where <code>index</code> is an integer of length
greater or equal to <code>length(x)</code>.</p>
</td></tr>
<tr><td><code id="baselearners_+3A_df">df</code></td>
<td>
<p> trace of the hat matrix for the base-learner defining the
base-learner complexity. Low values of <code>df</code> correspond to a
large amount of smoothing and thus to &quot;weaker&quot; base-learners.
Certain restrictions have to be kept for the specification of
<code>df</code> since most of the base-learners rely on penalization
approaches with a non-trivial null space. For example, for P-splines
fitted with <code>bbs</code>, <code>df</code> has to be larger than the order of
differences employed in the construction of the penalty term.
However, when option <code>center != FALSE</code>, the effect is centered
around its unpenalized part and therefore any positive number is
admissible for <code>df</code>. For details on the computation of degrees
of freedom see section &lsquo;Global Options&rsquo;.</p>
</td></tr>
<tr><td><code id="baselearners_+3A_lambda">lambda</code></td>
<td>
<p> smoothing penalty, computed from <code>df</code> when
<code>df</code> is specified. For details on the computation of degrees
of freedom see section &lsquo;Global Options&rsquo;.</p>
</td></tr>
<tr><td><code id="baselearners_+3A_knots">knots</code></td>
<td>
<p> either the number of knots or a vector of the positions
of the interior knots (for more details see below). For multiple
predictor variables, <code>knots</code> may be a named list where the
names in the list are the variable names.</p>
</td></tr>
<tr><td><code id="baselearners_+3A_boundary.knots">boundary.knots</code></td>
<td>
<p> boundary points at which to anchor the B-spline basis
(default the range of the data). A vector (of length 2)
for the lower and the upper boundary knot can be specified.This is
only advised for <code>bbs(..., cyclic = TRUE)</code>, where the boundary
knots specify the points at which the cyclic function should be joined. In
analogy to <code>knots</code> a names list can be specified. </p>
</td></tr>
<tr><td><code id="baselearners_+3A_degree">degree</code></td>
<td>
<p> degree of the regression spline.</p>
</td></tr>
<tr><td><code id="baselearners_+3A_differences">differences</code></td>
<td>
<p> a non-negative integer, typically 1, 2 or 3. If <code>differences</code> =
<em>k</em>, <em>k</em>-th-order differences are used as
a penalty (<em>0</em>-th order differences specify a
ridge penalty).</p>
</td></tr>
<tr><td><code id="baselearners_+3A_intercept">intercept</code></td>
<td>
<p> if <code>intercept = TRUE</code> an intercept is added to
the design matrix of a linear base-learner. If
<code>intercept = FALSE</code>, continuous covariates
should be (mean-) centered.</p>
</td></tr>
<tr><td><code id="baselearners_+3A_center">center</code></td>
<td>
<p> if <code>center != FALSE</code> the corresponding effect is
re-parameterized such that the unpenalized part of the fit is subtracted and
only the deviation effect is fitted. The unpenalized, parametric part has then
to be included in separate base-learners using <code>bols</code> (see the examples below).
There are two possible ways to re-parameterization;
<code>center = "differenceMatrix"</code> is based on the difference matrix
(the default for <code>bbs</code> with one covariate only)
and <code>center = "spectralDecomp"</code> uses a spectral decomposition
of the penalty matrix (see Fahrmeir et al., 2004, Section 2.3 for details).
The latter option is the default (and currently only option) for <code>bbs</code>
with multiple covariates or <code>bmrf</code>.</p>
</td></tr>
<tr><td><code id="baselearners_+3A_cyclic">cyclic</code></td>
<td>
<p> if <code>cyclic = TRUE</code> the fitted values coincide at the boundaries
(useful for cyclic covariates such as day time etc.). 
For details see Hofner et al. (2016).</p>
</td></tr>
<tr><td><code id="baselearners_+3A_covfun">covFun</code></td>
<td>
<p> the covariance function (i.e. radial basis)
needed to compute the basis functions. Per
default <code><a href="fields.html#topic+stationary.cov">stationary.cov</a></code> function
(from package <code>fields</code>) is used. </p>
</td></tr>
<tr><td><code id="baselearners_+3A_args">args</code></td>
<td>
<p> a named list of arguments to be passed to
<code>cov.function</code>. Thus strongly dependent on the
specified <code>cov.function</code>.</p>
</td></tr>
<tr><td><code id="baselearners_+3A_kernel">kernel</code></td>
<td>
<p>one of <code>"lin"</code> (linear kernel), <code>"sia"</code> (size adjusted kernel), 
or <code>"net"</code> (network kernel). For details see 
<code><a href="kangar00.html#topic+calc_kernel">calc_kernel</a></code></p>
</td></tr>
<tr><td><code id="baselearners_+3A_pathway">pathway</code></td>
<td>
<p>name of pathway; Pathway needs to be contained in the GWAS data set.</p>
</td></tr>
<tr><td><code id="baselearners_+3A_contrasts.arg">contrasts.arg</code></td>
<td>
<p> a named list of characters suitable for input to
the <code><a href="stats.html#topic+contrasts">contrasts</a></code> replacement function, or the contrast
matrix itself, see <code><a href="stats.html#topic+model.matrix">model.matrix</a></code>, or a single character
string (or contrast matrix) which is then used as contrasts for all
factors in this base-learner (with the exception of factors in
<code>by</code>). See also example below for setting contrasts. Note that
a special <code>contrasts.arg</code> exists in package <code>mboost</code>,
namely &quot;contr.dummy&quot;. This contrast is used per default in
<code>brandom</code> and can also be used in <code>bols</code>. It leads to a
dummy coding as returned by <code>model.matrix(~ x - 1)</code> were the
intercept is implicitly included but each factor level gets a
seperate effect estimate (see example below).</p>
</td></tr>
<tr><td><code id="baselearners_+3A_nmax">nmax</code></td>
<td>
<p>integer, maximal number of
bins in the predictor variables. Use <code>Inf</code> to switch-off binning.</p>
</td></tr>
<tr><td><code id="baselearners_+3A_tree_controls">tree_controls</code></td>
<td>
<p> an object of class <code>"TreeControl"</code>, which can be
obtained using <code><a href="partykit.html#topic+ctree_control">ctree_control</a></code>.
Defines hyper-parameters for the trees which are used as base-learners,
stumps are fitted by default. By default, stumps and thus
additive models are fitted.</p>
</td></tr>
<tr><td><code id="baselearners_+3A_constraint">constraint</code></td>
<td>
<p>type of constraint to be used. For <code>bmono</code>, 
the constraint can be either monotonic <code>"increasing"</code> (default), 
<code>"decreasing"</code>, or <code>"convex"</code> or <code>"concave"</code>. 
Additionally, <code>"none"</code> can  be used to specify unconstrained P-splines. 
This is especially of interest in conjunction with <code>boundary.constraints = TRUE</code>.
For <code>bbs</code>, the constraint can be <code>"none"</code>, monotonic <code>"increasing"</code>, or
<code>"decreasing"</code>. In general it is advisable to use <code>bmono</code> to fit 
monotonic splines.</p>
</td></tr>
<tr><td><code id="baselearners_+3A_type">type</code></td>
<td>

<p>determines how the constrained least squares problem should be
solved. If <code>type = "quad.prog"</code>, a numeric quadratic
programming method (Goldfarb and Idnani, 1982, 1983) is used
(see <code><a href="quadprog.html#topic+solve.QP">solve.QP</a></code> in package <span class="pkg">quadprog</span>). If
<code>type = "iterative"</code>, the iterative procedure described in
Hofner et al. (2011b) is used. The quadratic programming approach is
usually much faster than the iterative approach. For details see
Hofner et al. (2016).
</p>
</td></tr>
<tr><td><code id="baselearners_+3A_lambda2">lambda2</code></td>
<td>
<p> penalty parameter for the (monotonicity) constraint. </p>
</td></tr>
<tr><td><code id="baselearners_+3A_niter">niter</code></td>
<td>
<p> maximum number of iterations used to compute constraint
estimates. Increase this number if a warning is displayed. </p>
</td></tr>
<tr><td><code id="baselearners_+3A_boundary.constraints">boundary.constraints</code></td>
<td>
<p> a logical indicating whether additional
constraints on the boundaries of the spline should be applied
(default: FALSE). This is still experimental.</p>
</td></tr>
<tr><td><code id="baselearners_+3A_cons.arg">cons.arg</code></td>
<td>
<p> a named list with additional arguments for boundary
constraints. The element <code>lambda</code> specifies the
penalty parameter that is used for the additional boundary
constraint. The element <code>n</code> specifies the number of knots to be
subject to the constraint and  can be either a scalar (use same
number of constrained knots on each side) or a vector. Per default
10% of the knots on each side are used. The element
<code>diff_order</code> can be used to specify the order of the boundary
penalty: 1 (constant; default for monotonically constrained effects)
or 2 (linear; default for all other effects).</p>
</td></tr>
<tr><td><code id="baselearners_+3A_bnd">bnd</code></td>
<td>

<p>Object of class <code>bnd</code>, in which the boundaries of a map are
defined and from which neighborhood relations can be constructed. See
<code><a href="BayesX.html#topic+read.bnd">read.bnd</a></code>. If a boundary object is not
available, the neighborhood matrix can also be given directly. </p>
</td></tr>
<tr><td><code id="baselearners_+3A_x">X</code></td>
<td>
<p> design matrix as it should be used in the penalized least
squares estimation. Effect modifiers do not need to be included here
(<code>by</code> can be used for convenience). </p>
</td></tr>
<tr><td><code id="baselearners_+3A_k">K</code></td>
<td>
<p> penalty matrix as it should be used in the penalized least
squares estimation. If <code>NULL</code> (default), unpenalized estimation
is used. </p>
</td></tr>
<tr><td><code id="baselearners_+3A_deriv">deriv</code></td>
<td>
<p>an integer; the derivative of the spline of the given order
at the data is computed, defaults to zero. Note that this
argument is only used to set up the design matrix and
cannot be used in the fitting process.</p>
</td></tr>
<tr><td><code id="baselearners_+3A_bl1">bl1</code></td>
<td>
<p>a linear base-learner or a list of linear base-learners.</p>
</td></tr>
<tr><td><code id="baselearners_+3A_bl2">bl2</code></td>
<td>
<p>a linear base-learner or a list of linear base-learners.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>bols</code> refers to linear base-learners (potentially estimated with
a ridge penalty), while <code>bbs</code> provide penalized regression
splines. <code>bspatial</code> fits bivariate surfaces and <code>brandom</code>
defines random effects base-learners. In combination with option
<code>by</code>, these base-learners can be turned into varying coefficient
terms. The linear base-learners are fitted using Ridge Regression
where the penalty parameter <code>lambda</code> is either computed from
<code>df</code> (default for <code>bbs</code>, <code>bspatial</code>, and
<code>brandom</code>) or specified directly (<code>lambda = 0</code> means no
penalization as default for <code>bols</code>).
</p>
<p>In <code>bols(x)</code>, <code>x</code> may be a numeric vector or factor.
Alternatively, <code>x</code> can be a data frame containing numeric or
factor variables. In this case, or when multiple predictor variables
are specified, e.g., using <code>bols(x1, x2)</code>, the model is
equivalent to <code>lm(y ~ ., data = x)</code> or <code>lm(y ~ x1 + x2)</code>,
respectively. By default, an intercept term is added to the
corresponding design matrix (which can be omitted using
<code>intercept = FALSE</code>). It is <em>strongly</em> advised to (mean-)
center continuous covariates, if no intercept is used in <code>bols</code>
(see Hofner et al., 2011a). If <code>x</code> is a matrix, it is directly used
as the design matrix and no further preprocessing (such as addition of
an intercept) is conducted. When <code>df</code> (or <code>lambda</code>) is
given, a ridge estimator with <code>df</code> degrees of freedom (see
section &lsquo;Global Options&rsquo;) is used as base-learner. Note that
all variables are treated as a group, i.e., they enter the model
together if the corresponding base-learner is selected. For ordinal
variables, a ridge penalty for the differences of the adjacent
categories (Gertheiss and Tutz 2009, Hofner et al. 2011a) is applied.
</p>
<p>With <code>bbs</code>, the P-spline approach of Eilers and Marx (1996) is
used. P-splines use a squared <em>k</em>-th-order difference penalty
which can be interpreted as an approximation of the integrated squared
<em>k</em>-th derivative of the spline. In <code>bbs</code> the argument
<code>knots</code> specifies either the number of (equidistant) <em>interior</em>
knots to be used for the regression spline fit or a vector including
the positions of the <em>interior</em> knots. Additionally,
<code>boundary.knots</code> can be specified. However, this is only advised
if one uses cyclic constraints, where the <code>boundary.knots</code>
specify the points where the function is joined (e.g.,
<code>boundary.knots = c(0, 2 * pi)</code> for angles as in a sine function
or <code>boundary.knots = c(0, 24)</code> for hours during the day). For
details on cylcic splines in the context of boosting see Hofner et
al. (2016). 
</p>
<p><code>bspatial</code> implements bivariate tensor product P-splines for the
estimation of either spatial effects or interaction surfaces. Note
that <code>bspatial(x, y)</code> is equivalent to <code>bbs(x, y, df = 6)</code>.
For possible arguments and defaults see there. The penalty term is
constructed based on bivariate extensions of the univariate penalties
in <code>x</code> and <code>y</code> directions, see Kneib, Hothorn and Tutz
(2009) for details. Note that the dimensions of the penalty matrix
increase (quickly) with the number of knots with strong impact on
computational time. Thus, both should not be chosen to large.
Different knots for <code>x</code> and <code>y</code> can be specified by a named
list.
</p>
<p><code>brandom(x)</code> specifies a random effects base-learner based on a
factor variable <code>x</code> that defines the grouping structure of the
data set. For each level of <code>x</code>, a separate random intercept is
fitted, where the random effects variance is governed by the
specification of the degrees of freedom <code>df</code> or <code>lambda</code>
(see section &lsquo;Global Options&rsquo;). Note that <code>brandom(...)</code>
is essentially a wrapper to <code>bols(..., df = 4, contrasts.arg =
  "contr.dummy")</code>, i.e., a wrapper that utilizes ridge-penalized
categorical effects. For possible arguments and defaults see <code>bols</code>.
</p>
<p>For all linear base-learners the amount of smoothing is determined by
the trace of the hat matrix, as indicated by <code>df</code>.
</p>
<p>If <code>by</code> is specified as an additional argument, a varying
coefficients term is estimated, where <code>by</code> is the interaction
variable and the effect modifier is given by either <code>x</code> or
<code>x</code> and <code>y</code> (specified via <code>...</code>). If <code>bbs</code> is
used, this corresponds to the classical situation of varying
coefficients, where the effect of <code>by</code> varies over the co-domain
of <code>x</code>. In case of <code>bspatial</code> as base-learner, the effect of
<code>by</code> varies with respect to both <code>x</code> and <code>y</code>, i.e. an
interaction surface between <code>x</code> and <code>y</code> is specified as
effect modifier. For <code>brandom</code> specification of <code>by</code> leads
to the estimation of random slopes for covariate <code>by</code> with
grouping structure defined by factor <code>x</code> instead of a simple
random intercept. In <code>bbs</code>, <code>bspatial</code> and <code>brandom</code>
the computation of the smoothing parameter <code>lambda</code> for given
<code>df</code>, or vice versa, might become (numerically) instable if the
values of the interaction variable <code>by</code> become too large. In this
case, we recommend to rescale the interaction covariate e.g. by
dividing by <code>max(abs(by))</code>. If <code>bbs</code> or <code>bspatial</code> is
specified with an factor variable <code>by</code> with more than two
factors, the degrees of freedom are shared for the complete
base-learner (i.e., spread over all factor levels). Note that the null
space (see next paragraph) increases, as a separate null space for
each factor level is present. Thus, the minimum degrees of freedom
increase with increasing number of levels of <code>by</code> (if
<code>center = FALSE</code>).
</p>
<p>For <code>bbs</code> and <code>bspatial</code>, option <code>center != FALSE</code> requests that
the fitted effect is centered around its parametric, unpenalized part
(the so called null space). For example, with second order difference
penalty, a linear effect of <code>x</code> remains unpenalized by <code>bbs</code>
and therefore the degrees of freedom for the base-learner have to be
larger than two. To avoid this restriction, option <code>center =
  TRUE</code> subtracts the unpenalized linear effect from the fit, allowing
to specify any positive number as <code>df</code>. Note that in this case
the linear effect <code>x</code> should generally be specified as an
additional base-learner <code>bols(x)</code>. For <code>bspatial</code> and, for
example, second order differences, a linear effect of <code>x</code>
(<code>bols(x)</code>), a linear effect of <code>y</code> (<code>bols(y)</code>), and
their interaction (<code>bols(x*y)</code>) are subtracted from the effect
and have to be added separately to the model equation. More details on
centering can be found in Kneib, Hothorn and Tutz (2009) and Fahrmeir,
Kneib and Lang (2004). We strongly recommend to consult the latter reference
before using this option.
</p>
<p><code>brad(x)</code> specifies penalized radial basis functions as used in
Kriging. If <code>knots</code> is used to specify the number of knots, the
function <code><a href="fields.html#topic+cover.design">cover.design</a></code> is used to specify the
location of the knots such that they minimize a geometric
space-filling criterion. Furthermore, knots can be specified directly
via a matrix. The <code>cov.function</code> allows to specify the
radial basis functions. Per default, the flexible Matern correlation
function is used. This is specified using <code>cov.function =
  stationary.cov</code> with <code>Covariance = "Matern"</code> specified via
<code>args</code>. If an effective range <code>theta</code> is applicable for the
correlation function (e.g., the Matern family) the user can specify
this value. Per default (if <code>theta = NULL</code>) the effective range is
chosen as <code class="reqn">\theta = max(||x_i - x_j||)/c</code> such that the correlation function
</p>
<p style="text-align: center;"><code class="reqn">\rho(c; \theta = 1) = \varepsilon,</code>
</p>

<p>where <code class="reqn">\varepsilon = 0.001</code>.
</p>
<p><code>bmrf</code> builds a base of a Markov random field consisting of
several regions with a neighborhood structure. The input variable is
the observed region. The penalty matrix is either construed from a
boundary object or must be given directly via the option <code>bnd</code>.
In that case the <code>dimnames</code> of the matrix have to be the region
names, on the diagonal the number of neighbors have to be given for
each region, and for each neighborhood relation the value in the
matrix has to be -1, else 0. With a boundary object at hand, the
fitted or predicted values can be directly plotted into the map using
<code><a href="BayesX.html#topic+drawmap">drawmap</a></code>.
</p>
<p><code>bkernel</code> can be used to fit linear (<code>kernel = "lin"</code>), 
size-adjusted (<code>kernel = "sia"</code>) or network (<code>kernel = "net"</code>)
kernels based on genetic pathways for genome-wide assosiation studies. 
For details see Friedrichs et al. (2017) and check the associated package
<a href="https://CRAN.R-project.org/package=kangar00"><span class="pkg">kangar00</span></a>.
</p>
<p><code>buser(X, K)</code> specifies a base-learner with user-specified design
matrix <code>X</code> and penalty matrix <code>K</code>, where <code>X</code> and
<code>K</code> are used to minimize a (penalized) least squares
criterion with quadratic penalty. This can be used to easily specify
base-learners that are not implemented (yet). See examples
below for details how <code>buser</code> can be used to mimic existing
base-learners. Note that for predictions you need to set up the
design matrix for the new data manually.
</p>
<p>For a categorical covariate with non-observed categories
<code>bols(x)</code> and <code>brandom(x)</code> both assign a zero effect
to these categories. However, the non-observed categories must be
listed in <code>levels(x)</code>. Thus, predictions are possible
for new observations if they correspond to this category.
</p>
<p>By default, all linear base-learners include an intercept term (which can
be removed using <code>intercept = FALSE</code> for <code>bols</code>). In this case, 
the respective covariate should be mean centered (if continuous) and an 
explicit global intercept term should be added to <code>gamboost</code> 
via <code>bols</code> (see example below). With <code>bols(x, intercept = FALSE)</code> 
with categorical covariate <code>x</code> a separate effect for each group 
(mean effect) is estimated (see examples for resulting design matrices).
</p>
<p>Smooth estimates with constraints can be computed using the
base-learner <code>bmono()</code> which specifies P-spline base-learners
with an additional asymmetric penalty enforcing monotonicity or
convexity/concavity (see and Eilers, 2005). For more details in the
boosting context and monotonic effects of ordinal factors see Hofner,
Mueller and Hothorn (2011b). The quadratic-programming based algorithm
is described in Hofner et al. (2016). Alternative monotonicity
constraints are implemented via T-splines in <code>bbs()</code> (Beliakov,
2000). In general it is advisable to use <code>bmono</code> to fit monotonic splines 
as T-splines show undesirable behaviour if the observed data deviates 
from monotonicty.
</p>
<p>Two or more linear base-learners can be joined using <code>%+%</code>. A
tensor product of two or more linear base-learners is returned by
<code>%X%</code>. When the design matrix can be written as the Kronecker
product of two matrices <code>X = kronecker(X2, X1)</code>, then <code>bl1
  %O% bl2</code> with design matrices X1 and X2, respectively, can be used
to efficiently compute Ridge-estimates following Currie, Durban,
Eilers (2006). In all cases the overall degrees of freedom of the
combined base-learner increase (additive or multiplicative,
respectively). These three features are experimental and for expert
use only.
</p>
<p><code>btree</code> fits a stump to one or more variables. Note that
<code><a href="#topic+blackboost">blackboost</a></code> is more efficient for boosting stumps. For
further references see Hothorn, Hornik, Zeileis (2006) and Hothorn et
al. (2010).
</p>
<p>Note that the base-learners <code>bns</code> and <code>bss</code> are deprecated
(and no longer available). Please use <code>bbs</code> instead, which
results in qualitatively the same models but is computationally much
more attractive.
</p>


<h3>Value</h3>

<p>An object of class <code>blg</code> (base-learner generator) with a
<code>dpp</code> function.
</p>
<p>The call of <code>dpp</code> returns an object of class
<code>bl</code> (base-learner) with a <code>fit</code> function. The call to
<code>fit</code> finally returns an object of class <code>bm</code> (base-model).
</p>


<h3>Global Options</h3>

<p>Three global options affect the base-learners:
</p>

<dl>
<dt><code>options("mboost_useMatrix")</code></dt><dd><p> defaulting to <code>TRUE</code>
indicates that the base-learner may use sparse matrix techniques
for its computations. This reduces the memory consumption but
might (for smaller sample sizes) require more computing time.</p>
</dd>
<dt><code>options("mboost_indexmin")</code></dt><dd><p>is an integer that
specifies the minimum sample size needed to optimize model fitting
by automatically taking ties into account (default = 10000).</p>
</dd>
<dt><code>options("mboost_dftraceS")</code></dt><dd><p><code>FALSE</code> by default,
indicating how the degrees of freedom should be computed. Per
default </p>
<p style="text-align: center;"><code class="reqn">\mathrm{df}(\lambda) = \mathrm{trace}(2S -
      S^{\top}S),</code>
</p>
<p> with smoother matrix
<code class="reqn">S = X(X^{\top}X + \lambda K)^{-1} X</code> is used (see Hofner et al., 2011a). If <code>TRUE</code>, the
trace of the smoother matrix <code class="reqn">\mathrm{df}(\lambda) =
      \mathrm{trace}(S)</code> is used as degrees of freedom.
</p>
<p>Note that these formulae specify the relation of <code>df</code> and
<code>lambda</code> as the smoother matrix <code class="reqn">S</code> depends only on
<code class="reqn">\lambda</code> (and the (fixed) design matrix <code class="reqn">X</code>, the (fixed)
penalty matrix <code class="reqn">K</code>).</p>
</dd>
</dl>



<h3>References</h3>

<p>Iain D. Currie, Maria Durban, and Paul H. C. Eilers (2006),
Generalized linear array models with applications to
multidimensional smoothing. <em>Journal of the Royal
Statistical Society, Series B&ndash;Statistical Methodology</em>,
<b>68</b>(2), 259&ndash;280.
</p>
<p>Paul H. C. Eilers (2005), Unimodal smoothing. <em>Journal of
Chemometrics</em>, <b>19</b>, 317&ndash;328.
</p>
<p>Paul H. C. Eilers and Brian D. Marx (1996), Flexible smoothing with B-splines
and penalties. <em>Statistical Science</em>, <b>11</b>(2), 89-121.
</p>
<p>Ludwig Fahrmeir, Thomas Kneib and Stefan Lang (2004), Penalized structured
additive regression for space-time data: a Bayesian perspective.
<em>Statistica Sinica</em>, <b>14</b>, 731-761.
</p>
<p>Jan Gertheiss and Gerhard Tutz (2009), Penalized regression with ordinal
predictors, <em>International Statistical Review</em>, <b>77</b>(3), 345&ndash;365.
</p>
<p>D. Goldfarb and A. Idnani (1982),  Dual and Primal-Dual Methods
for Solving Strictly Convex Quadratic Programs.  In J. P. Hennart
(ed.), Numerical Analysis, Springer-Verlag, Berlin, pp. 226-239.
</p>
<p>D. Goldfarb and A. Idnani (1983),  A numerically stable dual
method for solving strictly convex quadratic programs.
<em>Mathematical Programming</em>, <b>27</b>, 1&ndash;33.
</p>
<p>S. Friedrichs, J. Manitz, P. Burger, C.I. Amos, A. Risch, J.C. Chang-Claude, 
H.E. Wichmann, T. Kneib, H. Bickeboeller, and B. Hofner (2017), 
Pathway-Based Kernel Boosting for the Analysis of Genome-Wide Association Studies.
<em>Computational and Mathematical Methods in Medicine</em>. 2017(6742763), 1-17. 
<a href="https://doi.org/10.1155/2017/6742763">doi:10.1155/2017/6742763</a>.
</p>
<p>Benjamin Hofner, Torsten Hothorn, Thomas Kneib, and Matthias Schmid (2011a),
A framework for unbiased model selection based on boosting.
<em>Journal of Computational and Graphical Statistics</em>, <b>20</b>, 956&ndash;971.
</p>
<p>Benjamin Hofner, Joerg Mueller, and Torsten Hothorn (2011b),
Monotonicity-Constrained Species Distribution Models.
<em>Ecology</em>, <b>92</b>, 1895&ndash;1901.
</p>
<p>Benjamin Hofner, Thomas Kneib and Torsten Hothorn (2016), 
A Unified Framework of Constrained Regression. 
<em>Statistics &amp; Computing</em>, <b>26</b>, 1&ndash;14.
</p>
<p>Thomas Kneib, Torsten Hothorn and Gerhard Tutz (2009), Variable
selection and model choice in geoadditive regression models,
<em>Biometrics</em>, <b>65</b>(2), 626&ndash;634.
</p>
<p>Torsten Hothorn, Kurt Hornik, Achim Zeileis (2006), Unbiased recursive
partitioning: A conditional inference framework. <em>Journal of
Computational and Graphical Statistics</em>, <b>15</b>, 651&ndash;674.
</p>
<p>Torsten Hothorn, Peter Buehlmann, Thomas Kneib, Matthias Schmid and
Benjamin Hofner (2010), Model-based Boosting 2.0, <em>Journal of
Machine Learning Research</em>, <b>11</b>, 2109&ndash;2113.
</p>
<p>G. M. Beliakov (2000), Shape Preserving Approximation using Least Squares
Splines, <em>Approximation Theory and its Applications</em>,
<b>16</b>(4), 80&ndash;98.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mboost">mboost</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>
  set.seed(290875)

  n &lt;- 100
  x1 &lt;- rnorm(n)
  x2 &lt;- rnorm(n) + 0.25 * x1
  x3 &lt;- as.factor(sample(0:1, 100, replace = TRUE))
  x4 &lt;- gl(4, 25)
  y &lt;- 3 * sin(x1) + x2^2 + rnorm(n)
  weights &lt;- drop(rmultinom(1, n, rep.int(1, n) / n))

  ### set up base-learners
  spline1 &lt;- bbs(x1, knots = 20, df = 4)
  extract(spline1, "design")[1:10, 1:10]
  extract(spline1, "penalty")
  knots.x2 &lt;- quantile(x2, c(0.25, 0.5, 0.75))
  spline2 &lt;- bbs(x2, knots = knots.x2, df = 5)
  ols3 &lt;- bols(x3)
  extract(ols3)
  ols4 &lt;- bols(x4)

  ### compute base-models
  drop(ols3$dpp(weights)$fit(y)$model) ## same as:
  coef(lm(y ~ x3, weights = weights))

  drop(ols4$dpp(weights)$fit(y)$model) ## same as:
  coef(lm(y ~ x4, weights = weights))

  ### fit model, component-wise
  mod1 &lt;- mboost_fit(list(spline1, spline2, ols3, ols4), y, weights)

  ### more convenient formula interface
  mod2 &lt;- mboost(y ~ bbs(x1, knots = 20, df = 4) +
                     bbs(x2, knots = knots.x2, df = 5) +
                     bols(x3) + bols(x4), weights = weights)
  all.equal(coef(mod1), coef(mod2))


  ### grouped linear effects
  # center x1 and x2 first
  x1 &lt;- scale(x1, center = TRUE, scale = FALSE)
  x2 &lt;- scale(x2, center = TRUE, scale = FALSE)
  model &lt;- gamboost(y ~ bols(x1, x2, intercept = FALSE) +
                        bols(x1, intercept = FALSE) +
                        bols(x2, intercept = FALSE),
                        control = boost_control(mstop = 50))
  coef(model, which = 1)   # one base-learner for x1 and x2
  coef(model, which = 2:3) # two separate base-learners for x1 and x2
                           # zero because they were (not yet) selected.

  ### example for bspatial
  x1 &lt;- runif(250,-pi,pi)
  x2 &lt;- runif(250,-pi,pi)

  y &lt;- sin(x1) * sin(x2) + rnorm(250, sd = 0.4)

  spline3 &lt;- bspatial(x1, x2, knots = 12)
  Xmat &lt;- extract(spline3, "design")
  ## 12 inner knots + 4 boundary knots = 16 knots per direction
  ## THUS: 16 * 16 = 256 columns
  dim(Xmat)
  extract(spline3, "penalty")[1:10, 1:10]

  ## specify number of knots separately
  form1 &lt;- y ~ bspatial(x1, x2, knots = list(x1 = 12, x2 = 14))

  ## decompose spatial effect into parametric part and
  ## deviation with one df
  form2 &lt;- y ~ bols(x1) + bols(x2) + bols(x1, by = x2, intercept = FALSE) +
               bspatial(x1, x2, knots = 12, center = TRUE, df = 1)

  mod1 &lt;- gamboost(form1)
  ## Not run: 
  plot(mod1)
  
## End(Not run)

  mod2 &lt;- gamboost(form2)
  ## automated plot function:
  ## Not run: 
  plot(mod2)
  
## End(Not run)
  ## plot sum of linear and smooth effects:
  library("lattice")
  df &lt;- expand.grid(x1 = unique(x1), x2 = unique(x2))
  df$pred &lt;- predict(mod2, newdata = df)
  ## Not run: 
  levelplot(pred ~ x1 * x2, data = df)
  
## End(Not run)

  ## specify radial basis function base-learner for spatial effect
  ## and use data-adaptive effective range (theta = NULL, see 'args')
  form3 &lt;- y ~ brad(x1, x2)
  ## Now use different settings, e.g. 50 knots and theta fixed to 0.4
  ## (not really a good setting)
  form4 &lt;- y ~ brad(x1, x2, knots = 50, args = list(theta = 0.4))

  mod3 &lt;- gamboost(form3)
  ## Not run: 
  plot(mod3)
  
## End(Not run)
  dim(extract(mod3, what = "design", which = "brad")[[1]])
  knots &lt;- attr(extract(mod3, what = "design", which = "brad")[[1]], "knots")

  mod4 &lt;- gamboost(form4)
  dim(extract(mod4, what = "design", which = "brad")[[1]])
  ## Not run: 
  plot(mod4)
  
## End(Not run)

  ### random intercept
  id &lt;- factor(rep(1:10, each = 5))
  raneff &lt;- brandom(id)
  extract(raneff, "design")
  extract(raneff, "penalty")

  ## random intercept with non-observed category
  set.seed(1907)
  y &lt;- rnorm(50, mean = rep(rnorm(10), each = 5), sd = 0.1)
  plot(y ~ id)
  # category 10 not observed
  obs &lt;- c(rep(1, 45), rep(0, 5))
  model &lt;- gamboost(y ~ brandom(id), weights = obs)
  coef(model)
  fitted(model)[46:50] # just the grand mean as usual for
                       # random effects models


  ### random slope
  z &lt;- runif(50)
  raneff &lt;- brandom(id, by = z)
  extract(raneff, "design")
  extract(raneff, "penalty")

  ### specify simple interaction model (with main effect)
  n &lt;- 210
  x &lt;- rnorm(n)
  X &lt;- model.matrix(~ x)
  z &lt;- gl(3, n/3)
  Z &lt;- model.matrix(~z)
  beta &lt;- list(c(0,1), c(-3,4), c(2, -4))
  y &lt;- rnorm(length(x), mean = (X * Z[,1]) %*% beta[[1]] +
                               (X * Z[,2]) %*% beta[[2]] +
                               (X * Z[,3]) %*% beta[[3]])
  plot(y ~ x, col = z)
  ## specify main effect and interaction
  mod_glm &lt;- gamboost(y ~ bols(x) + bols(x, by = z),
                  control = boost_control(mstop = 100))
  nd &lt;- data.frame(x, z)
  nd &lt;- nd[order(x),]
  nd$pred_glm &lt;- predict(mod_glm, newdata = nd)
  for (i in seq(along = levels(z)))
      with(nd[nd$z == i,], lines(x, pred_glm, col = z))
  mod_gam &lt;- gamboost(y ~ bbs(x) + bbs(x, by = z, df = 8),
                      control = boost_control(mstop = 100))
  nd$pred_gam &lt;- predict(mod_gam, newdata = nd)
  for (i in seq(along = levels(z)))
      with(nd[nd$z == i,], lines(x, pred_gam, col = z, lty = "dashed"))
  ### convenience function for plotting
  ## Not run: 
  par(mfrow = c(1,3))
  plot(mod_gam)
  
## End(Not run)


  ### remove intercept from base-learner
  ### and add explicit intercept to the model
  tmpdata &lt;- data.frame(x = 1:100, y = rnorm(1:100), int = rep(1, 100))
  mod &lt;- gamboost(y ~ bols(int, intercept = FALSE) +
                      bols(x, intercept = FALSE),
                  data = tmpdata,
                  control = boost_control(mstop = 1000))
  cf &lt;- unlist(coef(mod))
  ## add offset
  cf[1] &lt;- cf[1] + mod$offset
  signif(cf, 3)
  signif(coef(lm(y ~ x, data = tmpdata)), 3)

  ### much quicker and better with (mean-) centering
  tmpdata$x_center &lt;- tmpdata$x - mean(tmpdata$x)
  mod_center &lt;- gamboost(y ~ bols(int, intercept = FALSE) +
                             bols(x_center, intercept = FALSE),
                         data = tmpdata,
                         control = boost_control(mstop = 100))
  cf_center &lt;- unlist(coef(mod_center, which=1:2))
  ## due to the shift in x direction we need to subtract
  ## beta_1 * mean(x) to get the correct intercept
  cf_center[1] &lt;- cf_center[1] + mod_center$offset -
                  cf_center[2] * mean(tmpdata$x)
  signif(cf_center, 3)
  signif(coef(lm(y ~ x, data = tmpdata)), 3)

## Not run: ############################################################
## Do not run and check these examples automatically as
## they take some time

  ### large data set with ties
  nunique &lt;- 100
  xindex &lt;- sample(1:nunique, 1000000, replace = TRUE)
  x &lt;- runif(nunique)
  y &lt;- rnorm(length(xindex))
  w &lt;- rep.int(1, length(xindex))

  ### brute force computations
  op &lt;- options()
  options(mboost_indexmin = Inf, mboost_useMatrix = FALSE)
  ## data pre-processing
  b1 &lt;- bbs(x[xindex])$dpp(w)
  ## model fitting
  c1 &lt;- b1$fit(y)$model
  options(op)

  ### automatic search for ties, faster
  b2 &lt;- bbs(x[xindex])$dpp(w)
  c2 &lt;- b2$fit(y)$model

  ### manual specification of ties, even faster
  b3 &lt;- bbs(x, index = xindex)$dpp(w)
  c3 &lt;- b3$fit(y)$model

  all.equal(c1, c2)
  all.equal(c1, c3)

## End(Not run and test)

## End(Not run)

  ### cyclic P-splines
  set.seed(781)
  x &lt;- runif(200, 0,(2*pi))
  y &lt;- rnorm(200, mean=sin(x), sd=0.2)
  newX &lt;- seq(0,2*pi, length=100)
  ### model without cyclic constraints
  mod &lt;- gamboost(y ~ bbs(x, knots = 20))
  ### model with cyclic constraints
  mod_cyclic &lt;- gamboost(y ~ bbs(x, cyclic=TRUE, knots = 20,
                                 boundary.knots=c(0, 2*pi)))
  par(mfrow = c(1,2))
  plot(x,y, main="bbs (non-cyclic)", cex=0.5)
  lines(newX, sin(newX), lty="dotted")
  lines(newX + 2 * pi, sin(newX), lty="dashed")
  lines(newX, predict(mod, data.frame(x = newX)),
        col="red", lwd = 1.5)
  lines(newX + 2 * pi, predict(mod, data.frame(x = newX)),
        col="blue", lwd=1.5)
  plot(x,y, main="bbs (cyclic)", cex=0.5)
  lines(newX, sin(newX), lty="dotted")
  lines(newX + 2 * pi, sin(newX), lty="dashed")
  lines(newX, predict(mod_cyclic, data.frame(x = newX)),
        col="red", lwd = 1.5)
  lines(newX + 2 * pi, predict(mod_cyclic, data.frame(x = newX)),
        col="blue", lwd = 1.5)

  ### use buser() to mimic p-spline base-learner:
  set.seed(1907)
  x &lt;- rnorm(100)
  y &lt;- rnorm(100, mean = x^2, sd = 0.1)
  mod1 &lt;- gamboost(y ~ bbs(x))
  ## now extract design and penalty matrix
  X &lt;- extract(bbs(x), "design")
  K &lt;- extract(bbs(x), "penalty")
  ## use X and K in buser()
  mod2 &lt;- gamboost(y ~ buser(X, K))
  max(abs(predict(mod1) - predict(mod2)))  # same results

  ### use buser() to mimic penalized ordinal base-learner:
  z &lt;- as.ordered(sample(1:3, 100, replace=TRUE))
  y &lt;- rnorm(100, mean = as.numeric(z), sd = 0.1)
  X &lt;- extract(bols(z))
  K &lt;- extract(bols(z), "penalty")
  index &lt;- extract(bols(z), "index")
  mod1 &lt;- gamboost(y ~  buser(X, K, df = 1, index = index))
  mod2 &lt;- gamboost(y ~  bols(z, df = 1))
  max(abs(predict(mod1) - predict(mod2)))  # same results

  ### kronecker product for matrix-valued responses
  data("volcano", package = "datasets")
  layout(matrix(1:2, ncol = 2))

  ## estimate mean of image treating image as matrix
  image(volcano, main = "data")
  x1 &lt;- 1:nrow(volcano)
  x2 &lt;- 1:ncol(volcano)

  vol &lt;- as.vector(volcano)
  mod &lt;- mboost(vol ~ bbs(x1, df = 3, knots = 10)%O%
                      bbs(x2, df = 3, knots = 10),
                      control = boost_control(nu = 0.25))
  mod[250]

  volf &lt;- matrix(fitted(mod), nrow = nrow(volcano))
  image(volf, main = "fitted")

## Not run: ############################################################
## Do not run and check these examples automatically as
## they take some time

  ## the old-fashioned way, a waste of space and time
  x &lt;- expand.grid(x1, x2)
  modx &lt;- mboost(vol ~ bbs(Var2, df = 3, knots = 10) %X%
                       bbs(Var1, df = 3, knots = 10), data = x,
                       control = boost_control(nu = 0.25))
  modx[250]

  max(abs(fitted(mod) - fitted(modx)))

## End(Not run and test)

## End(Not run)

  ### setting contrasts via contrasts.arg
  x &lt;- as.factor(sample(1:4, 100, replace = TRUE))

  ## compute base-learners with different reference categories
  BL1 &lt;- bols(x, contrasts.arg = contr.treatment(4, base = 1)) # default
  BL2 &lt;- bols(x, contrasts.arg = contr.treatment(4, base = 2))
  ## compute 'sum to zero contrasts' using character string
  BL3 &lt;- bols(x, contrasts.arg = "contr.sum")

  ## extract model matrices to check if it works
  extract(BL1)
  extract(BL2)
  extract(BL3)

  ### setting contrasts using named lists in contrasts.arg
  x2 &lt;- as.factor(sample(1:4, 100, replace = TRUE))

  BL4 &lt;- bols(x, x2,
              contrasts.arg = list(x = contr.treatment(4, base = 2),
                                   x2 = "contr.helmert"))
  extract(BL4)

  ### using special contrast: "contr.dummy":
  BL5 &lt;- bols(x, contrasts.arg = "contr.dummy")
  extract(BL5)
</code></pre>

<hr>
<h2 id='blackboost'> Gradient Boosting with Regression Trees </h2><span id='topic+blackboost'></span>

<h3>Description</h3>

<p>Gradient boosting for optimizing arbitrary loss functions where regression
trees are utilized as base-learners.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>blackboost(formula, data = list(),
           weights = NULL, na.action = na.pass,
           offset = NULL, family = Gaussian(), 
           control = boost_control(),
           oobweights = NULL,
           tree_controls = partykit::ctree_control(
               teststat = "quad",
               testtype = "Teststatistic",
               mincriterion = 0,
               minsplit = 10, 
               minbucket = 4,
               maxdepth = 2, 
               saveinfo = FALSE),
           ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="blackboost_+3A_formula">formula</code></td>
<td>
<p> a symbolic description of the model to be fit. </p>
</td></tr>
<tr><td><code id="blackboost_+3A_data">data</code></td>
<td>
<p> a data frame containing the variables in the model. </p>
</td></tr>
<tr><td><code id="blackboost_+3A_weights">weights</code></td>
<td>
<p> an optional vector of weights to be used in the fitting
process. </p>
</td></tr>
<tr><td><code id="blackboost_+3A_na.action">na.action</code></td>
<td>
<p>a function which indicates what should happen when the data
contain <code>NA</code>s. </p>
</td></tr>
<tr><td><code id="blackboost_+3A_offset">offset</code></td>
<td>
<p> a numeric vector to be used as offset (optional).</p>
</td></tr>
<tr><td><code id="blackboost_+3A_family">family</code></td>
<td>
<p>a <code><a href="#topic+Family">Family</a></code> object.</p>
</td></tr>
<tr><td><code id="blackboost_+3A_control">control</code></td>
<td>
<p> a list of parameters controlling the algorithm. For
more details see <code><a href="#topic+boost_control">boost_control</a></code>. </p>
</td></tr>
<tr><td><code id="blackboost_+3A_oobweights">oobweights</code></td>
<td>
<p> an additional vector of out-of-bag weights, which is
used for the out-of-bag risk (i.e., if <code>boost_control(risk =
      "oobag")</code>). This argument is also used internally by
<code>cvrisk</code>. </p>
</td></tr>    
<tr><td><code id="blackboost_+3A_tree_controls">tree_controls</code></td>
<td>
<p> an object of class <code>"TreeControl"</code>, which
can be obtained using <code><a href="partykit.html#topic+ctree_control">ctree_control</a></code>. Defines
hyper-parameters for the trees which are used as base-learners. It
is wise to make sure to understand the consequences of altering any
of its arguments. By default, two-way interactions (but not deeper
trees) are fitted.</p>
</td></tr>
<tr><td><code id="blackboost_+3A_...">...</code></td>
<td>
<p>  additional arguments passed to <code><a href="#topic+mboost_fit">mboost_fit</a></code>,
including <code>weights</code>, <code>offset</code>, <code>family</code> and
<code>control</code>. For default values see <code><a href="#topic+mboost_fit">mboost_fit</a></code>. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function implements the &lsquo;classical&rsquo;
gradient boosting utilizing regression trees as base-learners.
Essentially, the same algorithm is implemented in package
<code><a href="gbm.html#topic+gbm">gbm</a></code>. The
main difference is that arbitrary loss functions to be optimized
can be specified via the <code>family</code> argument to <code>blackboost</code> whereas
<code><a href="gbm.html#topic+gbm">gbm</a></code> uses hard-coded loss functions.
Moreover, the base-learners (conditional
inference trees, see <code><a href="partykit.html#topic+ctree">ctree</a></code>) are a little bit more flexible.
</p>
<p>The regression fit is a black box prediction machine and thus
hardly interpretable.
</p>
<p>Partial dependency plots are not yet available; see example section for
plotting of additive tree models.
</p>


<h3>Value</h3>

<p>An object of class <code>mboost</code> with <code><a href="base.html#topic+print">print</a></code>
and <code><a href="stats.html#topic+predict">predict</a></code> methods being available.
</p>


<h3>References</h3>

<p>Peter Buehlmann and Torsten Hothorn (2007),
Boosting algorithms: regularization, prediction and model fitting.
<em>Statistical Science</em>, <b>22</b>(4), 477&ndash;505.
</p>
<p>Torsten Hothorn, Kurt Hornik and Achim Zeileis (2006). Unbiased recursive
partitioning: A conditional inference framework. <em>Journal of
Computational and Graphical Statistics</em>, <b>15</b>(3), 651&ndash;674.
</p>
<p>Yoav Freund and Robert E. Schapire (1996),
Experiments with a new boosting algorithm.
In <em>Machine Learning: Proc. Thirteenth International Conference</em>,
148&ndash;156.
</p>
<p>Jerome H. Friedman (2001),
Greedy function approximation: A gradient boosting machine.
<em>The Annals of Statistics</em>, <b>29</b>, 1189&ndash;1232.
</p>
<p>Greg Ridgeway (1999), The state of boosting.
<em>Computing Science and Statistics</em>, <b>31</b>,
172&ndash;181.
</p>


<h3>See Also</h3>

<p>See <code><a href="#topic+mboost_fit">mboost_fit</a></code> for the generic boosting function, 
<code><a href="#topic+glmboost">glmboost</a></code> for boosted linear models, and
<code><a href="#topic+gamboost">gamboost</a></code> for boosted additive models. 
</p>
<p>See <code><a href="#topic+baselearners">baselearners</a></code> for possible base-learners. 
</p>
<p>See <code><a href="#topic+cvrisk">cvrisk</a></code> for cross-validated stopping iteration. 
</p>
<p>Furthermore see <code><a href="#topic+boost_control">boost_control</a></code>, <code><a href="#topic+Family">Family</a></code> and
<code><a href="#topic+methods">methods</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
### a simple two-dimensional example: cars data
cars.gb &lt;- blackboost(dist ~ speed, data = cars,
                      control = boost_control(mstop = 50))
cars.gb

### plot fit
plot(dist ~ speed, data = cars)
lines(cars$speed, predict(cars.gb), col = "red")

### set up and plot additive tree model
if (require("partykit")) {
    ctrl &lt;- ctree_control(maxdepth = 3)
    viris &lt;- subset(iris, Species != "setosa")
    viris$Species &lt;- viris$Species[, drop = TRUE]
    imod &lt;- mboost(Species ~ btree(Sepal.Length, tree_controls = ctrl) +
                             btree(Sepal.Width, tree_controls = ctrl) +
                             btree(Petal.Length, tree_controls = ctrl) +
                             btree(Petal.Width, tree_controls = ctrl),
                   data = viris, family = Binomial())[500]
    layout(matrix(1:4, ncol = 2))
    plot(imod)
}
</code></pre>

<hr>
<h2 id='boost_control'> Control Hyper-parameters for Boosting Algorithms </h2><span id='topic+boost_control'></span>

<h3>Description</h3>

<p>Definition of the initial number of boosting iterations, step size
and other hyper-parameters for boosting algorithms.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>boost_control(mstop = 100, nu = 0.1,
              risk = c("inbag", "oobag", "none"), stopintern = FALSE,
              center = TRUE, trace = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="boost_control_+3A_mstop">mstop</code></td>
<td>
<p> an integer giving the number of initial boosting iterations. 
If <code>mstop = 0</code>, the offset model is returned.</p>
</td></tr>
<tr><td><code id="boost_control_+3A_nu">nu</code></td>
<td>
<p> a double (between 0 and 1) defining the step size or shrinkage parameter.
The default is probably too large for many applications
with <code>family = Poisson()</code> and a smaller value is better.</p>
</td></tr>
<tr><td><code id="boost_control_+3A_risk">risk</code></td>
<td>
<p> a character indicating how the empirical risk should be
computed for each boosting iteration. <code>inbag</code> leads to
risks computed for the learning sample (i.e., all non-zero weights),
<code>oobag</code> to risks based on the out-of-bag (all observations with
zero weights) and <code>none</code> to no risk computations at all.</p>
</td></tr>
<tr><td><code id="boost_control_+3A_stopintern">stopintern</code></td>
<td>
<p> a logical that defines if the boosting algorithm stops internally
when the out-of-bag risk in one iteration is larger than the 
out-of-bag risk in the iteration before. Can also be a positive
number giving the risk difference that needs to be exceeded.</p>
</td></tr>
<tr><td><code id="boost_control_+3A_center">center</code></td>
<td>
<p> deprecated. A logical indicating if the numerical covariates should be mean
centered before fitting. Only implemented for
<code><a href="#topic+glmboost">glmboost</a></code>. In <code><a href="#topic+blackboost">blackboost</a></code>
centering is not needed. In <code><a href="#topic+gamboost">gamboost</a></code>
centering is only needed if <code><a href="#topic+bols">bols</a></code>
base-learners are specified without intercept. In this
case centering of the covariates is essential and should be done
manually (at the moment). Will be removed in favour of
a corresponding argument in <code>glmboost</code> in the future
(and gives a warning).</p>
</td></tr>
<tr><td><code id="boost_control_+3A_trace">trace</code></td>
<td>
<p> a logical triggering printout of status information during
the fitting process.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Objects returned by this function specify hyper-parameters of the
boosting algorithms implemented in <code><a href="#topic+glmboost">glmboost</a></code>,
<code><a href="#topic+gamboost">gamboost</a></code> and <code><a href="#topic+blackboost">blackboost</a></code>
(via the <code>control</code> argument).
</p>


<h3>Value</h3>

<p>An object of class <code>boost_control</code>, a list.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mboost">mboost</a></code></p>

<hr>
<h2 id='boost_family-class'>Class &quot;boost_family&quot;: Gradient Boosting Family</h2><span id='topic+boost_family-class'></span><span id='topic+show+2Cboost_family-method'></span>

<h3>Description</h3>

<p>Objects of class <code>boost_family</code> define negative gradients of
loss functions to be optimized.</p>


<h3>Objects from the Class</h3>

<p>Objects can be created by calls of the form <code><a href="#topic+Family">Family</a>(...)</code>
</p>


<h3>Slots</h3>


<dl>
<dt><code>ngradient</code>:</dt><dd><p> a function with arguments <code>y</code> and <code>f</code>
implementing the <em>negative</em> gradient of
the <code>loss</code> function. </p>
</dd>
<dt><code>risk</code>:</dt><dd><p> a risk function with arguments <code>y</code>, <code>f</code> and <code>w</code>,
the weighted mean of the loss function by default. </p>
</dd>
<dt><code>offset</code>:</dt><dd><p> a function with argument <code>y</code> and <code>w</code> (weights)
for computing a <em>scalar</em> offset. </p>
</dd>
<dt><code>weights</code>:</dt><dd><p> a logical indicating if weights are allowed. </p>
</dd>
<dt><code>check_y</code>:</dt><dd><p> a function for checking the class / mode of a response variable.</p>
</dd>
<dt><code>nuisance</code>:</dt><dd><p> a function for extracting nuisance parameters.</p>
</dd>
<dt><code>response</code>:</dt><dd><p>inverse link function of a GLM or any other transformation
on the scale of the response.</p>
</dd>
<dt><code>rclass</code>:</dt><dd><p>function to derive class predictions from conditional class
probabilities (for models with factor response variable).</p>
</dd>
<dt><code>name</code>:</dt><dd><p> a character giving the name of the loss function
for pretty printing. </p>
</dd>
<dt><code>charloss</code>:</dt><dd><p> a character, the deparsed loss function.</p>
</dd>
</dl>



<h3>See Also</h3>

<p><code><a href="#topic+Family">Family</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>
    Laplace()

</code></pre>

<hr>
<h2 id='confint.mboost'>
Pointwise Bootstrap Confidence Intervals
</h2><span id='topic+confint.mboost'></span><span id='topic+confint.glmboost'></span><span id='topic+plot.mboost.ci'></span><span id='topic+lines.mboost.ci'></span><span id='topic+print.glmboost.ci'></span>

<h3>Description</h3>

<p>Compute and display pointwise confidence intervals
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'mboost'
confint(object, parm = NULL, level = 0.95, B = 1000,
        B.mstop = 25, newdata = NULL, which = parm,
        papply = ifelse(B.mstop == 0, mclapply, lapply),
        cvrisk_options = list(), ...)
## S3 method for class 'mboost.ci'
plot(x, which, level = x$level, ylim = NULL, type = "l", col = "black",
     ci.col = rgb(170, 170, 170, alpha = 85, maxColorValue = 255),
     raw = FALSE, print_levelplot = TRUE,...)
## S3 method for class 'mboost.ci'
lines(x, which, level = x$level,
     col = rgb(170, 170, 170, alpha = 85, maxColorValue = 255),
     raw = FALSE, ...)


## S3 method for class 'glmboost'
confint(object, parm = NULL, level = 0.95,
        B = 1000, B.mstop = 25, which = parm, ...)
## S3 method for class 'glmboost.ci'
print(x, which = NULL, level = x$level, pe = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="confint.mboost_+3A_object">object</code></td>
<td>

<p>a fitted model object of class <code>glmboost</code>, <code>gamboost</code> or
<code>mboost</code> for which the confidence intervals should be computed.
</p>
</td></tr>
<tr><td><code id="confint.mboost_+3A_parm">parm</code>, <code id="confint.mboost_+3A_which">which</code></td>
<td>

<p>a subset of base-learners to take into account for computing
confidence intervals. See <code><a href="#topic+mboost_methods">mboost_methods</a></code> for details.
<code>parm</code> is just a synonyme for <code>which</code> to be in line with
the generic <code>confint</code> function. Preferably use <code>which</code>.
</p>
</td></tr>
<tr><td><code id="confint.mboost_+3A_level">level</code></td>
<td>

<p>the confidence level required.
</p>
</td></tr>
<tr><td><code id="confint.mboost_+3A_b">B</code></td>
<td>

<p>number of outer bootstrap replicates used to compute the empirical
bootstrap confidence intervals.
</p>
</td></tr>
<tr><td><code id="confint.mboost_+3A_b.mstop">B.mstop</code></td>
<td>

<p>number of inner bootstrap replicates used to determine the optimal
mstop on each of the <code>B</code> bootstrap samples.
</p>
</td></tr>
<tr><td><code id="confint.mboost_+3A_newdata">newdata</code></td>
<td>

<p>optionally, a data frame on which to compute the predictions for the
confidence intervals.
</p>
</td></tr>
<tr><td><code id="confint.mboost_+3A_papply">papply</code></td>
<td>

<p>(parallel) apply function for the outer bootstrap, defaults to
<code><a href="parallel.html#topic+mclapply">mclapply</a></code> if no inner bootstrap is used to
determine the optimal stopping iteration. For details see
argument <code>papply</code> in <code><a href="#topic+cvrisk">cvrisk</a></code>. Be careful with your
computing resources if you use parallel computing for both, the
inner and the outer bootstrap.
</p>
</td></tr>
<tr><td><code id="confint.mboost_+3A_cvrisk_options">cvrisk_options</code></td>
<td>

<p>(optionally) specify a named list with arguments to the inner
bootstrap. For example use <code>cvrisk_options = list(mc.cores =
      2)</code> to specify that the <code><a href="parallel.html#topic+mclapply">mclapply</a></code> function within
<code><a href="#topic+cvrisk">cvrisk</a></code> uses 2 cores to compute the optimal
<code>mstop</code>.
</p>
</td></tr>
<tr><td><code id="confint.mboost_+3A_x">x</code></td>
<td>

<p>a confidence interval object.
</p>
</td></tr>
<tr><td><code id="confint.mboost_+3A_ylim">ylim</code></td>
<td>

<p>limits of the y scale. Per default computed from the data to plot.
</p>
</td></tr>
<tr><td><code id="confint.mboost_+3A_type">type</code></td>
<td>

<p>type of graphic for the point estimate, i.e., for the predicted
function. Per default a line is plotted.
</p>
</td></tr>
<tr><td><code id="confint.mboost_+3A_col">col</code></td>
<td>

<p>color of the point estimate, i.e., for the predicted function.
</p>
</td></tr>
<tr><td><code id="confint.mboost_+3A_ci.col">ci.col</code></td>
<td>

<p>color of the confidence interval.
</p>
</td></tr>
<tr><td><code id="confint.mboost_+3A_raw">raw</code></td>
<td>

<p>logical, should the raw function estimates or the derived confidence
estimates be plotted?
</p>
</td></tr>
<tr><td><code id="confint.mboost_+3A_print_levelplot">print_levelplot</code></td>
<td>

<p>logical, should the <span class="pkg">lattice</span> <code><a href="lattice.html#topic+levelplot">levelplot</a></code> be printed
or simply returned for further modifications. This argument is only
considered if bivariate effect estimates are plotted. If
<code>print_levelplot</code> is set to <code>FALSE</code>, a list with objects
<code>mean</code>, <code>lowerPI</code> and <code>upperPI</code> is returned
containing the three <code><a href="lattice.html#topic+levelplot">levelplot</a></code> objects.
</p>
</td></tr>
<tr><td><code id="confint.mboost_+3A_pe">pe</code></td>
<td>

<p>logical, should the point estimtate (PE) be also returned?
</p>
</td></tr>
<tr><td><code id="confint.mboost_+3A_...">...</code></td>
<td>

<p>additional arguments to the outer bootstrap such as <code>mc.cores</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Use a nested boostrap approach to compute pointwise confidence
intervals for the predicted partial functions or regression
parameters. The approach is further described in Hofner et al. (2016).
</p>


<h3>Value</h3>

<p>An object of class <code>glmboost.ci</code> or <code>mboost.ci</code> with special
<code>print</code> and/or <code>plot</code> functions.
</p>


<h3>Author(s)</h3>

<p>Benjamin Hofner &lt;benjamin.hofner@pei.de&gt;
</p>


<h3>References</h3>

<p>Benjamin Hofner, Thomas Kneib and Torsten Hothorn (2016), 
A Unified Framework of Constrained Regression. 
<em>Statistics &amp; Computing</em>, <b>26</b>, 1&ndash;14.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cvrisk">cvrisk</a></code> for crossvalidation approaches and
<code><a href="#topic+mboost_methods">mboost_methods</a></code> for other methods.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
############################################################
## Do not run these examples automatically as they take
## some time (~ 30 seconds depending on the system)

### a simple linear example
set.seed(1907)
data &lt;- data.frame(x1 = rnorm(100), x2 = rnorm(100),
                   z = factor(sample(1:3, 100, replace = TRUE)))
data$y &lt;- rnorm(100, mean = data$x1 - data$x2 - 1 * (data$z == 2) +
                            1 * (data$z == 3), sd = 0.1)
linmod &lt;- glmboost(y ~ x1 + x2 + z, data = data,
                   control = boost_control(mstop = 200))

## compute confidence interval from 10 samples. Usually one should use
## at least 1000 samples.
CI &lt;- confint(linmod, B = 10, level = 0.9)
CI

## to compute a confidence interval for another level simply change the
## level in the print function:
print(CI, level = 0.8)
## or print a subset (with point estimates):
print(CI, level = 0.8, pe = TRUE, which = "z")

### a simple smooth example
set.seed(1907)
data &lt;- data.frame(x1 = rnorm(100), x2 = rnorm(100))
data$y &lt;- rnorm(100, mean = data$x1^2 - sin(data$x2), sd = 0.1)
gam &lt;- gamboost(y ~ x1 + x2, data = data,
                control = boost_control(mstop = 200))

## compute confidence interval from 10 samples. Usually one should use
## at least 1000 samples.
CI_gam &lt;- confint(gam, B = 10, level = 0.9)

par(mfrow = c(1, 2))
plot(CI_gam, which = 1)
plot(CI_gam, which = 2)
## to compute a confidence interval for another level simply change the
## level in the plot or lines function:
lines(CI_gam, which = 2, level = 0.8)

## End(Not run)
</code></pre>

<hr>
<h2 id='cvrisk'> Cross-Validation </h2><span id='topic+cvrisk'></span><span id='topic+cvrisk.mboost'></span><span id='topic+print.cvrisk'></span><span id='topic+plot.cvrisk'></span><span id='topic+cv'></span>

<h3>Description</h3>

<p>Cross-validated estimation of the empirical risk for hyper-parameter selection.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'mboost'
cvrisk(object, folds = cv(model.weights(object)),
       grid = 0:mstop(object),
       papply = mclapply,
       fun = NULL, mc.preschedule = FALSE, ...)
cv(weights, type = c("bootstrap", "kfold", "subsampling"),
   B = ifelse(type == "kfold", 10, 25), prob = 0.5, strata = NULL)

## Plot cross-valiation results   
## S3 method for class 'cvrisk'
plot(x, 
     xlab = "Number of boosting iterations", ylab = attr(x, "risk"),
     ylim = range(x), main = attr(x, "type"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cvrisk_+3A_object">object</code></td>
<td>
<p> an object of class <code>mboost</code>.</p>
</td></tr>
<tr><td><code id="cvrisk_+3A_folds">folds</code></td>
<td>
<p> a weight matrix with number of rows equal to the number
of observations. The number of columns corresponds to
the number of cross-validation runs. Can be computed
using function <code>cv</code> and defaults to 25 bootstrap samples.</p>
</td></tr>
<tr><td><code id="cvrisk_+3A_grid">grid</code></td>
<td>
<p> a vector of stopping parameters the empirical risk
is to be evaluated for. </p>
</td></tr>
<tr><td><code id="cvrisk_+3A_papply">papply</code></td>
<td>

<p>(parallel) apply function, defaults to  <code><a href="parallel.html#topic+mclapply">mclapply</a></code>.
Alternatively, <code><a href="parallel.html#topic+parLapply">parLapply</a></code> can be used. In the
latter case, usually more setup is needed (see example for some
details). To run <code>cvrisk</code> sequentially (i.e. not in parallel),
one can use <code><a href="base.html#topic+lapply">lapply</a></code>.
</p>
</td></tr>
<tr><td><code id="cvrisk_+3A_fun">fun</code></td>
<td>
<p> if <code>fun</code> is NULL, the out-of-sample risk is returned. <code>fun</code>,
as a function of <code>object</code>, may extract any other characteristic
of the cross-validated models. These are returned as is.</p>
</td></tr>
<tr><td><code id="cvrisk_+3A_mc.preschedule">mc.preschedule</code></td>
<td>

<p>preschedule tasks if are parallelized using <code><a href="parallel.html#topic+mclapply">mclapply</a></code>
(default: <code>FALSE</code>)? For details see <code><a href="parallel.html#topic+mclapply">mclapply</a></code>.
</p>
</td></tr>
<tr><td><code id="cvrisk_+3A_weights">weights</code></td>
<td>
<p> a numeric vector of weights for the model to be cross-validated.</p>
</td></tr>
<tr><td><code id="cvrisk_+3A_type">type</code></td>
<td>
<p> character argument for specifying the cross-validation
method. Currently (stratified) bootstrap, k-fold cross-validation
and subsampling are implemented.</p>
</td></tr>
<tr><td><code id="cvrisk_+3A_b">B</code></td>
<td>
<p> number of folds, per default 25 for <code>bootstrap</code> and
<code>subsampling</code> and 10 for <code>kfold</code>.</p>
</td></tr>
<tr><td><code id="cvrisk_+3A_prob">prob</code></td>
<td>
<p> percentage of observations to be included in the learning samples
for subsampling.</p>
</td></tr>
<tr><td><code id="cvrisk_+3A_strata">strata</code></td>
<td>
<p> a factor of the same length as <code>weights</code> for stratification.</p>
</td></tr>
<tr><td><code id="cvrisk_+3A_x">x</code></td>
<td>
<p> an object of class <code>cvrisk</code>.</p>
</td></tr>
<tr><td><code id="cvrisk_+3A_xlab">xlab</code>, <code id="cvrisk_+3A_ylab">ylab</code></td>
<td>
<p>axis labels.</p>
</td></tr>
<tr><td><code id="cvrisk_+3A_ylim">ylim</code></td>
<td>
<p>limits of y-axis.</p>
</td></tr>
<tr><td><code id="cvrisk_+3A_main">main</code></td>
<td>
<p>main title of graphic.</p>
</td></tr>
<tr><td><code id="cvrisk_+3A_...">...</code></td>
<td>
<p>additional arguments passed to <code><a href="parallel.html#topic+mclapply">mclapply</a></code> or 
<code><a href="#topic+plot">plot</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The number of boosting iterations is a hyper-parameter of the
boosting algorithms implemented in this package. Honest,
i.e., cross-validated, estimates of the empirical risk
for different stopping parameters <code>mstop</code> are computed by
this function which can be utilized to choose an appropriate
number of boosting iterations to be applied.
</p>
<p>Different forms of cross-validation can be applied, for example
10-fold cross-validation or bootstrapping. The weights (zero weights
correspond to test cases) are defined via the <code>folds</code> matrix.
</p>
<p><code>cvrisk</code> runs in parallel on OSes where forking is possible
(i.e., not on Windows) and multiple cores/processors are available.
The scheduling
can be changed by the corresponding arguments of
<code><a href="parallel.html#topic+mclapply">mclapply</a></code> (via the dot arguments).
</p>
<p>The function <code>cv</code> can be used to build an appropriate
weight matrix to be used with <code>cvrisk</code>. If <code>strata</code> is defined
sampling is performed in each stratum separately thus preserving
the distribution of the <code>strata</code> variable in each fold.
</p>
<p>There exist various functions to display and work with 
cross-validation results. One can <code>print</code> and <code>plot</code> (see above)
results and extract the optimal iteration via <code><a href="#topic+mstop">mstop</a></code>.
</p>


<h3>Value</h3>

<p>An object of class <code>cvrisk</code> (when <code>fun</code> wasn't specified), basically a matrix
containing estimates of the empirical risk for a varying number
of bootstrap iterations. <code>plot</code> and <code>print</code> methods
are available as well as a <code>mstop</code> method.
</p>


<h3>References</h3>

<p>Torsten Hothorn, Friedrich Leisch, Achim Zeileis and Kurt Hornik (2006),
The design and analysis of benchmark experiments.
<em>Journal of Computational and Graphical Statistics</em>, <b>14</b>(3),
675&ndash;699.
</p>
<p>Andreas Mayr, Benjamin Hofner, and Matthias Schmid (2012). The
importance of knowing when to stop - a sequential stopping rule for
component-wise gradient boosting. <em>Methods of Information in
Medicine</em>, <b>51</b>, 178&ndash;186. <br />
DOI: <a href="https://doi.org/10.3414/ME11-02-0030">doi:10.3414/ME11-02-0030</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+AIC.mboost">AIC.mboost</a></code> for
<code>AIC</code> based selection of the stopping iteration. Use <code>mstop</code>
to extract the optimal stopping iteration from <code>cvrisk</code>
object.</p>


<h3>Examples</h3>

<pre><code class='language-R'>
  data("bodyfat", package = "TH.data")

  ### fit linear model to data
  model &lt;- glmboost(DEXfat ~ ., data = bodyfat, center = TRUE)

  ### AIC-based selection of number of boosting iterations
  maic &lt;- AIC(model)
  maic

  ### inspect coefficient path and AIC-based stopping criterion
  par(mai = par("mai") * c(1, 1, 1, 1.8))
  plot(model)
  abline(v = mstop(maic), col = "lightgray")

  ### 10-fold cross-validation
  cv10f &lt;- cv(model.weights(model), type = "kfold")
  cvm &lt;- cvrisk(model, folds = cv10f, papply = lapply)
  print(cvm)
  mstop(cvm)
  plot(cvm)

  ### 25 bootstrap iterations (manually)
  set.seed(290875)
  n &lt;- nrow(bodyfat)
  bs25 &lt;- rmultinom(25, n, rep(1, n)/n)
  cvm &lt;- cvrisk(model, folds = bs25, papply = lapply)
  print(cvm)
  mstop(cvm)
  plot(cvm)

  ### same by default
  set.seed(290875)
  cvrisk(model, papply = lapply)

  ### 25 bootstrap iterations (using cv)
  set.seed(290875)
  bs25_2 &lt;- cv(model.weights(model), type="bootstrap")
  all(bs25 == bs25_2)

## Not run: 
############################################################
## Do not run this example automatically as it takes
## some time (~ 5 seconds depending on the system)

  ### trees
  blackbox &lt;- blackboost(DEXfat ~ ., data = bodyfat)
  cvtree &lt;- cvrisk(blackbox, papply = lapply)
  plot(cvtree)
  
## End(Not run this automatically)  

## End(Not run)


### cvrisk in parallel modes:

## Not run: 
## at least not automatically

## parallel::mclapply() which is used here for parallelization only runs 
## on unix systems (here we use 2 cores)

    cvrisk(model, mc.cores = 2)

## infrastructure needs to be set up in advance

    cl &lt;- makeCluster(25) # e.g. to run cvrisk on 25 nodes via PVM
    myApply &lt;- function(X, FUN, ...) {
      myFun &lt;- function(...) {
          library("mboost") # load mboost on nodes
          FUN(...)
      }
      ## further set up steps as required
      parLapply(cl = cl, X, myFun, ...)
    }
    cvrisk(model, papply = myApply)
    stopCluster(cl)

## End(Not run)

</code></pre>

<hr>
<h2 id='Family'> Gradient Boosting Families </h2><span id='topic+Family'></span><span id='topic+AdaExp'></span><span id='topic+Binomial'></span><span id='topic+GaussClass'></span><span id='topic+GaussReg'></span><span id='topic+Gaussian'></span><span id='topic+Huber'></span><span id='topic+Laplace'></span><span id='topic+Poisson'></span><span id='topic+GammaReg'></span><span id='topic+CoxPH'></span><span id='topic+QuantReg'></span><span id='topic+ExpectReg'></span><span id='topic+NBinomial'></span><span id='topic+PropOdds'></span><span id='topic+Weibull'></span><span id='topic+Loglog'></span><span id='topic+Lognormal'></span><span id='topic+AUC'></span><span id='topic+Gehan'></span><span id='topic+Hurdle'></span><span id='topic+Multinomial'></span><span id='topic+Cindex'></span><span id='topic+RCG'></span>

<h3>Description</h3>

<p><code>boost_family</code> objects provide a convenient way to specify loss functions
and corresponding risk functions to be optimized by one of the boosting
algorithms implemented in this package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Family(ngradient, loss = NULL, risk = NULL,
       offset = function(y, w)
           optimize(risk, interval = range(y),
                    y = y, w = w)$minimum,
       check_y = function(y) y,
       weights = c("any", "none", "zeroone", "case"),
       nuisance = function() return(NA),
       name = "user-specified", fW = NULL,
       response = function(f) NA,
       rclass = function(f) NA)
AdaExp()
AUC()
Binomial(type = c("adaboost", "glm"),
         link = c("logit", "probit", "cloglog", "cauchit", "log"), ...)
GaussClass()
GaussReg()
Gaussian()
Huber(d = NULL)
Laplace()
Poisson()
GammaReg(nuirange = c(0, 100))
CoxPH()
QuantReg(tau = 0.5, qoffset = 0.5)
ExpectReg(tau = 0.5)
NBinomial(nuirange = c(0, 100))
PropOdds(nuirange = c(-0.5, -1), offrange = c(-5, 5))
Weibull(nuirange = c(0, 100))
Loglog(nuirange = c(0, 100))
Lognormal(nuirange = c(0, 100))
Gehan()
Hurdle(nuirange = c(0, 100))
Multinomial()
Cindex(sigma = 0.1, ipcw = 1)
RCG(nuirange = c(0, 1), offrange = c(-5, 5))

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Family_+3A_ngradient">ngradient</code></td>
<td>
<p> a function with arguments <code>y</code>, <code>f</code> and <code>w</code> implementing the
<em>negative</em> gradient of the <code>loss</code> function (which is to be minimized). </p>
</td></tr>
<tr><td><code id="Family_+3A_loss">loss</code></td>
<td>
<p> an optional loss function with arguments <code>y</code> and <code>f</code>. </p>
</td></tr>
<tr><td><code id="Family_+3A_risk">risk</code></td>
<td>
<p> an optional risk function with arguments <code>y</code>, <code>f</code> and <code>w</code> to be minimized (!),
the weighted mean of the loss function by default. </p>
</td></tr>
<tr><td><code id="Family_+3A_offset">offset</code></td>
<td>
<p> a function with argument <code>y</code> and <code>w</code> (weights)
for computing a <em>scalar</em> offset. </p>
</td></tr>
<tr><td><code id="Family_+3A_fw">fW</code></td>
<td>
<p> transformation of the fit for the diagonal weights matrix for an
approximation of the boosting hat matrix for loss functions other than
squared error.</p>
</td></tr>
<tr><td><code id="Family_+3A_response">response</code></td>
<td>
<p> inverse link function of a GLM or any other transformation
on the scale of the response.</p>
</td></tr>
<tr><td><code id="Family_+3A_rclass">rclass</code></td>
<td>
<p> function to derive class predictions from conditional class
probabilities (for models with factor response variable).</p>
</td></tr>
<tr><td><code id="Family_+3A_check_y">check_y</code></td>
<td>
<p> a function for checking and transforming
the class / mode of a response variable.</p>
</td></tr>
<tr><td><code id="Family_+3A_nuisance">nuisance</code></td>
<td>
<p> a function for extracting nuisance parameters from the family.</p>
</td></tr>
<tr><td><code id="Family_+3A_weights">weights</code></td>
<td>
<p> a character indicating what type of weights are
allowed. These can be either arbitrary (non-negative) weights
<code>"any"</code>, only zero and one weights <code>"zeroone"</code>,
(non-negative) interger weights <code>"case"</code>, or no weights are
allowed <code>"none"</code>.</p>
</td></tr>
<tr><td><code id="Family_+3A_name">name</code></td>
<td>
<p> a character giving the name of the loss function for pretty printing. </p>
</td></tr>
<tr><td><code id="Family_+3A_type">type</code></td>
<td>
<p> which parameterization of <code>Binomial</code> shoule be used?</p>
</td></tr></table>
<p> b
</p>
<table>
<tr><td><code id="Family_+3A_link">link</code></td>
<td>
<p> link function. For possible values see Usage section.</p>
</td></tr>
<tr><td><code id="Family_+3A_d">d</code></td>
<td>
<p> delta parameter for Huber loss function. If omitted, it is chosen adaptively.</p>
</td></tr>
<tr><td><code id="Family_+3A_tau">tau</code></td>
<td>
<p> the quantile or expectile to be estimated, a number strictly between 0 and 1.</p>
</td></tr>
<tr><td><code id="Family_+3A_qoffset">qoffset</code></td>
<td>
<p> quantile of response distribution to be used as offset, i.e., 
starting values for the intercept. Per default the median of the response is used,
which is in general a good choice (see Fenske et al. 2011, for details).</p>
</td></tr>
<tr><td><code id="Family_+3A_nuirange">nuirange</code></td>
<td>
<p> a vector containing the end-points of the interval to be
searched for the minimum risk w.r.t. the nuisance parameter.
In case of <code>PropOdds</code>, the starting values for
the nuisance parameters. </p>
</td></tr>
<tr><td><code id="Family_+3A_offrange">offrange</code></td>
<td>
<p> interval to search in for offset.</p>
</td></tr>
<tr><td><code id="Family_+3A_sigma">sigma</code></td>
<td>
<p>smoothness parameter for sigmoid functions inside <code>Cindex</code>.</p>
</td></tr>
<tr><td><code id="Family_+3A_ipcw">ipcw</code></td>
<td>
<p>vector containing inverse probability of censoring weights for all observations. If omitted, it is estimated inside <code>Cindex</code> family.</p>
</td></tr>
<tr><td><code id="Family_+3A_...">...</code></td>
<td>
<p> additional arguments to link functions.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The boosting algorithm implemented in <code><a href="#topic+mboost">mboost</a></code> minimizes the
(weighted) empirical risk function <code>risk(y, f, w)</code> with respect to <code>f</code>.
By default, the risk function is the weighted sum of the loss function <code>loss(y, f)</code>
but can be chosen arbitrarily. The <code>ngradient(y, f)</code> function is the negative
gradient of <code>loss(y, f)</code> with respect to <code>f</code>.
</p>
<p>Pre-fabricated functions for the most commonly used loss functions are
available as well. Buehlmann and Hothorn (2007) give a detailed
overview of the available loss functions. An updated overview can 
be found in Hofner et al (2014). 
</p>
<p>The <code>offset</code> function returns the population minimizers evaluated at the 
response, i.e., <code class="reqn">1/2 \log(p / (1 - p))</code> for <code>Binomial()</code> or <code>AdaExp()</code>
and <code class="reqn">(\sum w_i)^{-1} \sum w_i y_i</code> for <code>Gaussian()</code> and the
median for <code>Huber()</code> and <code>Laplace()</code>. The offset is used as starting 
value for the boosting algorithm.
</p>
<p>Note that all families are functions and thus need to be specified either with
empty brackets (e.g., <code>family = Gaussian()</code> for Gaussian regression) or 
with additional arguments if these are supported by the respective family 
(e.g., <code>family = QuantReg(tau = 0.2)</code> for quantile regression for the 
20% quantile).
</p>
<p><strong>A short summary of the available families is given in the following paragraphs:</strong>
</p>
<p><code>AdaExp()</code>, <code>Binomial()</code> and <code>AUC()</code> implement
families for binary classification. <code>AdaExp()</code> uses the
exponential loss, which essentially leads to the AdaBoost algorithm
of Freund and Schapire (1996). <code>Binomial()</code> implements the
negative binomial log-likelihood of a logistic regression model
as loss function. Thus, using <code>Binomial</code> family closely corresponds
to fitting a logistic model. Alternative link functions
can be specified.
</p>
<p>However, the coefficients resulting from boosting with family
<code>Binomial(link = "logit")</code> are <code class="reqn">1/2</code> of the coefficients of a logit model
obtained via <code><a href="stats.html#topic+glm">glm</a></code>.  Buehlmann and Hothorn (2007) argue that the
family <code>Binomial</code> is the preferred choice for binary
classification. For binary classification problems the response
<code>y</code> has to be a <code>factor</code>. Internally <code>y</code> is re-coded
to <code class="reqn">-1</code> and <code class="reqn">+1</code> (Buehlmann and Hothorn 2007). 
</p>
<p><code>Binomial(type = "glm")</code> is an alternative to <code>Binomial()</code> leading to 
coefficients of the same size as coefficients from a classical logit 
model via <code><a href="stats.html#topic+glm">glm</a></code>. Additionally, it works not only with a 
two-level factor but also with a two-column matrix containing the number 
of successes and number of failures (again, similar to <code><a href="stats.html#topic+glm">glm</a></code>). 
</p>
<p><code>AUC()</code> uses <code class="reqn">1-AUC(y, f)</code> as the loss function.
The area under the ROC curve (AUC) is defined as
<code class="reqn">AUC = (n_{-1} n_1)^{-1} \sum_{i: y_i = 1} \sum_{j: y_j = -1} I(f_i &gt; f_j)</code>.
Since this is not differentiable in <code>f</code>, we approximate the jump function
<code class="reqn">I((f_i - f_j) &gt; 0)</code> by the distribution function of the triangular
distribution on <code class="reqn">[-1, 1]</code> with mean <code class="reqn">0</code>, similar to the logistic
distribution approximation used in Ma and Huang (2005).
</p>
<p><code>Gaussian()</code> is the default family in <code><a href="#topic+mboost">mboost</a></code>. It
implements <code class="reqn">L_2</code>Boosting for continuous response. Note
that families <code>GaussReg()</code> and <code>GaussClass()</code> (for regression
and classification) are deprecated now.
<code>Huber()</code> implements a robust version for boosting with
continuous response, where the Huber-loss is used. <code>Laplace()</code>
implements another strategy for continuous outcomes and uses the
<code class="reqn">L_1</code>-loss instead of the <code class="reqn">L_2</code>-loss as used by
<code>Gaussian()</code>.
</p>
<p><code>Poisson()</code> implements a family for fitting count data with
boosting methods. The implemented loss function is the negative
Poisson log-likelihood. Note that the natural link function
<code class="reqn">\log(\mu) = \eta</code> is assumed. The default step-site <code>nu = 0.1</code>
is probably too large for this family (leading to
infinite residuals) and smaller values are more appropriate.
</p>
<p><code>GammaReg()</code> implements a family for fitting nonnegative response
variables. The implemented loss function is the negative Gamma
log-likelihood with logarithmic link function (instead of the natural
link).
</p>
<p><code>CoxPH()</code> implements the negative partial log-likelihood for Cox
models. Hence, survival models can be boosted using this family.
</p>
<p><code>QuantReg()</code> implements boosting for quantile regression, which is
introduced in Fenske et al. (2009). <code>ExpectReg</code> works in analogy,
only for expectiles, which were introduced to regression by Newey and Powell (1987).
</p>
<p>Families with an additional scale parameter can be used for fitting
models as well: <code>PropOdds()</code> leads to proportional odds models
for ordinal outcome variables (Schmid et al., 2011). When using this
family, an ordered set of threshold parameters is re-estimated in each
boosting iteration. An example is given below which also shows how to
obtain the thresholds. <code>NBinomial()</code> leads to regression models with
a negative binomial conditional distribution of the response.
<code>Weibull()</code>, <code>Loglog()</code>, and <code>Lognormal()</code> implement
the negative log-likelihood functions of accelerated failure time
models with Weibull, log-logistic, and lognormal distributed outcomes,
respectively. Hence, parametric survival models can be boosted using
these families. For details see Schmid and Hothorn (2008) and Schmid
et al. (2010).
</p>
<p><code>Gehan()</code> implements rank-based estimation of survival data in an
accelerated failure time model. The loss function is defined as the sum
of the pairwise absolute differences of residuals. The response needs to
be defined as <code>Surv(y, delta)</code>, where <code>y</code> is the observed survial
time (subject to censoring) and <code>delta</code> is the non-censoring indicator
(see <code><a href="survival.html#topic+Surv">Surv</a></code> for details). For details on <code>Gehan()</code> see
Johnson and Long (2011).
</p>
<p><code>Cindex()</code> optimizes the concordance-index for survival data (often denoted
as Harrell's C or C-index). The concordance index evaluates the rank-based
concordance probability between the model and the outcome. The C-index measures
whether large values of the model are associated with short survival times  and
vice versa. The interpretation is similar to the AUC: A C-index of 1 represents a
perfect discrimination while a C-index of 0.5 will be achieved by a completely
non-informative marker. The <code>Cindex()</code> family is based on an estimator by
Uno et al. (2011), which incorporates inverse probability of censoring weighting
<code>ipcw</code>. To make the estimator differentiable, sigmoid functions are applied;
the corresponding smoothness can be controlled via <code>sigma</code>. For details on
<code>Cindex()</code> see Mayr and Schmid (2014).
</p>
<p>Hurdle models for zero-inflated count data can be fitted by using a combination
of the <code>Binomial()</code> and <code>Hurdle()</code> families. While the <code>Binomial()</code>
family allows for fitting the zero-generating process of the Hurdle model,
<code>Hurdle()</code> fits a negative binomial regression model to the non-zero
counts. Note that the specification of the Hurdle model allows for using
<code>Binomial()</code> and <code>Hurdle()</code> independently of each other.
</p>
<p>Linear or additive multinomial logit models can be fitted using
<code>Multinomial()</code>; although is family requires some extra effort for
model specification (see example).  More specifically, the predictor must
be in the form of a linear array model (see <code><a href="#topic++25O+25">%O%</a></code>).  Note
that this family does not work with tree-based base-learners at the
moment. The class corresponding to the last level of the factor coding
of the response is used as reference class.
</p>
<p><code>RCG()</code> implements the ratio of correlated gammas (RCG) model proposed
by Weinhold et al. (2016).
</p>


<h3>Value</h3>

<p>An object of class <code>boost_family</code>.
</p>


<h3>Warning</h3>

<p>The coefficients resulting from boosting with family
<code>Binomial(link = "logit")</code> are <code class="reqn">1/2</code> of the coefficients of a logit model
obtained via <code><a href="stats.html#topic+glm">glm</a></code> (see above). 
</p>
<p>For <code>AUC()</code>, variables should be centered and scaled and observations with weight &gt; 0 must not contain missing values.
The estimated coefficients for <code>AUC()</code> have no probabilistic interpretation.
</p>


<h3>Author(s)</h3>

<p><code>ExpectReg()</code> was donated by Fabian Sobotka.
<code>AUC()</code> was donated by Fabian Scheipl.
</p>


<h3>References</h3>

<p>Peter Buehlmann and Torsten Hothorn (2007),
Boosting algorithms: regularization, prediction and model fitting.
<em>Statistical Science</em>, <b>22</b>(4), 477&ndash;505.
</p>
<p>Nora Fenske, Thomas Kneib, and Torsten Hothorn (2011),
Identifying risk factors for severe childhood malnutrition by
boosting additive quantile regression.
<em>Journal of the American Statistical Association</em>, <b>106</b>:494-510.
</p>
<p>Yoav Freund and Robert E. Schapire (1996),
Experiments with a new boosting algorithm.
In <em>Machine Learning: Proc. Thirteenth International Conference</em>,
148&ndash;156.
</p>
<p>Shuangge Ma and Jian Huang (2005), Regularized ROC method for
disease classification and biomarker selection with microarray
data. <em>Bioinformatics</em>, <b>21</b>(24), 4356&ndash;4362.
</p>
<p>Andreas Mayr and Matthias Schmid (2014). 
Boosting the concordance index for survival data &ndash; a unified 
framework to derive and evaluate biomarker combination.     
<em>PloS ONE</em>, <b>9</b>(1):84483.
</p>
<p>Whitney K. Newey and James L. Powell (1987),
Asymmetric least squares estimation and testing.
<em>Econometrika</em>, <b>55</b>, 819&ndash;847.
</p>
<p>Matthias Schmid and Torsten Hothorn (2008),
Flexible boosting of accelerated failure time models.
<em>BMC Bioinformatics</em>, <b>9</b>(269).
</p>
<p>Matthias Schmid, Sergej Potapov, Annette Pfahlberg,
and Torsten Hothorn (2010). Estimation and regularization techniques for
regression models with multidimensional prediction functions.
<em>Statistics and Computing</em>, <b>20</b>, 139&ndash;150.
</p>
<p>Schmid, M., T. Hothorn, K. O. Maloney, D. E. Weller and S. Potapov
(2011): Geoadditive regression modeling of stream biological
condition. <em>Environmental and Ecological Statistics</em>,
<b>18</b>(4), 709&ndash;733.
</p>
<p>Uno H, Cai T, Pencina MJ, D Agostino RB and Wei LJ (2011). 
On the C-statistics for evaluating overall adequacy of risk prediction 
procedures with censored survival data. 
<em>Statistics in Medicine</em>, <b>30</b>(10), 1105&ndash;17.
</p>
<p>Benjamin Hofner, Andreas Mayr, Nikolay Robinzonov and Matthias Schmid
(2014). Model-based Boosting in R: A Hands-on Tutorial Using the R
Package mboost. <em>Computational Statistics</em>, <b>29</b>, 3&ndash;35.<br />
<a href="https://doi.org/10.1007/s00180-012-0382-5">doi:10.1007/s00180-012-0382-5</a>
</p>
<p>Available as vignette via: <code>vignette(package = "mboost", "mboost_tutorial")</code>
</p>
<p>Brent A. Johnson and Qi Long (2011) Survival ensembles by the sum of pairwise
differences with application to lung cancer microarray studies.
<em>Annals of Applied Statistics</em>, <b>5</b>, 1081&ndash;1101.
</p>
<p>Weinhold, L., S. Pechlivanis, S. Wahl, P. Hoffmann and M. Schmid (2016) A Statistical Model for the
Analysis of Bounded Response Variables in DNA Methylation Studies.
<em>BMC Bioinformatics</em>. 2016; 17: 480. <a href="https://doi.org/10.1186/s12859-016-1347-4">doi:10.1186/s12859-016-1347-4</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mboost">mboost</a></code> for the usage of <code>Family</code>s. See
<code><a href="#topic+boost_family-class">boost_family-class</a></code> for objects resulting from a call to <code>Family</code>. </p>


<h3>Examples</h3>

<pre><code class='language-R'>### Define a new family
MyGaussian &lt;- function(){
       Family(ngradient = function(y, f, w = 1) y - f,
       loss = function(y, f) (y - f)^2,
       name = "My Gauss Variant")
}
# Now use the new family
data(bodyfat, package = "TH.data")
mod &lt;- mboost(DEXfat ~ ., data = bodyfat, family = MyGaussian())
# N.B. that the family needs to be called with empty brackets


### Proportional odds model
data(iris)
iris$Species &lt;- factor(iris$Species, ordered = TRUE)
if (require("MASS")) {
    (mod.polr &lt;- polr(Species  ~ Sepal.Length, data = iris))
}
mod.PropOdds &lt;- glmboost(Species  ~ Sepal.Length, data = iris,
                         family = PropOdds(nuirange = c(-0.5, 3)))
mstop(mod.PropOdds) &lt;- 1000
## thresholds are treated as nuisance parameters, to extract these use
nuisance(mod.PropOdds)
## effect estimate
coef(mod.PropOdds)["Sepal.Length"]
## make thresholds comparable to a model without intercept
nuisance(mod.PropOdds) - coef(mod.PropOdds)["(Intercept)"] -
    attr(coef(mod.PropOdds), "offset")

### Multinomial logit model via a linear array model
## One needs to convert the data to a list
myiris &lt;- as.list(iris)
## ... and define a dummy vector with one factor level less
## than the outcome, which is used as reference category.
myiris$class &lt;- factor(levels(iris$Species)[-nlevels(iris$Species)])
## Now fit the linear array model
mlm &lt;- mboost(Species ~ bols(Sepal.Length, df = 2) %O%
                        bols(class, df = 2, contrasts.arg = "contr.dummy"),
              data = myiris,
              family = Multinomial())
coef(mlm) ## one should use more boosting iterations.
head(round(pred &lt;- predict(mlm, type = "response"), 2))

## Prediction with new data:
newdata &lt;- as.list(iris[1,])
## One always needs to keep the dummy vector class as above!
newdata$class &lt;- factor(levels(iris$Species)[-nlevels(iris$Species)])
pred2 &lt;- predict(mlm, type = "response", newdata = newdata)
## check results
pred[1, ]
pred2

## Not run: ############################################################
## Do not run and check these examples automatically as
## they take some time

## Compare results with nnet::multinom
if (require("nnet")) {
    mlmn &lt;- multinom(Species ~ Sepal.Length, data = iris)
    max(abs(fitted(mlm[1000], type = "response") -
            fitted(mlmn, type = "prob")))
}

## End(Not run and test)

## End(Not run)


### Example for RCG model
## generate covariate values
set.seed(12345)
x1 &lt;- rnorm(500)
x2 &lt;- rnorm(500)
## generate linear predictors
zetaM &lt;- 0.1 + 0.3 * x1 - 0.5 * x2 
zetaU &lt;- 0.1 - 0.1 * x1 + 0.2 * x2
## generate beta values
M &lt;- rgamma(500, shape = 2, rate = exp(zetaM))
U &lt;- rgamma(500, shape = 2, rate = exp(zetaU))
y &lt;- M / (M + U)

## fit RCG model
data &lt;- data.frame(y, x1, x2)
RCGmodel &lt;- glmboost(y ~ x1 + x2, data = data, family = RCG(),
                     control = boost_control(mstop = 1000,
                     trace = TRUE, nu = 0.01))
## true coefficients: gamma = (0.0, 0.4, -0.7),
##                    alpha (= shape) = 2,
##                    rho = 0
## compare to coefficient estimates
coef(RCGmodel)
nuisance(RCGmodel)

## compute downstream tests 
## (only suitable without early stopping, i.e., if likelihood based model converged)
downstream.test(RCGmodel)

## compute conditional expectations
predictions &lt;- predict(RCGmodel, type = "response")
plot(predictions, y)
abline(0,1)

</code></pre>

<hr>
<h2 id='FP'> Fractional Polynomials </h2><span id='topic+FP'></span>

<h3>Description</h3>

<p>Fractional polynomials transformation for continuous covariates.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>FP(x, p = c(-2, -1, -0.5, 0.5, 1, 2, 3), scaling = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="FP_+3A_x">x</code></td>
<td>
<p> a numeric vector. </p>
</td></tr>
<tr><td><code id="FP_+3A_p">p</code></td>
<td>
<p> all powers of <code>x</code> to be included. </p>
</td></tr>
<tr><td><code id="FP_+3A_scaling">scaling</code></td>
<td>
<p> a logical indicating if the measurements are scaled prior to model fitting.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A fractional polynomial refers to a model
<code class="reqn">\sum_{j = 1}^k (\beta_j x^{p_j} + \gamma_j x^{p_j} \log(x)) +
    \beta_{k + 1} \log(x)  + \gamma_{k + 1} \log(x)^2</code>,
where the degree of the fractional polynomial is the number of non-zero regression coefficients
<code class="reqn">\beta</code> and <code class="reqn">\gamma</code>.
</p>


<h3>Value</h3>

<p>A matrix including all powers <code>p</code> of <code>x</code>,
all powers <code>p</code> of <code>log(x)</code>, and <code>log(x)</code>.
</p>


<h3>References</h3>

<p>Willi Sauerbrei and Patrick Royston (1999), Building multivariable prognostic and
diagnostic models: transformation of the predictors by using fractional polynomials.
<em>Journal of the Royal Statistical Society A</em>, <b>162</b>, 71&ndash;94.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gamboost">gamboost</a></code> to fit smooth models, <code><a href="#topic+bbs">bbs</a></code>
for P-spline base-learners</p>


<h3>Examples</h3>

<pre><code class='language-R'>
    data("bodyfat", package = "TH.data")
    tbodyfat &lt;- bodyfat

    ### map covariates into [1, 2]
    indep &lt;- names(tbodyfat)[-2]
    tbodyfat[indep] &lt;- lapply(bodyfat[indep], function(x) {
        x &lt;- x - min(x)
        x / max(x) + 1
    })

    ### generate formula
    fpfm &lt;- as.formula(paste("DEXfat ~ ",
        paste("FP(", indep, ", scaling = FALSE)", collapse = "+")))
    fpfm

    ### fit linear model
    bf_fp &lt;- glmboost(fpfm, data = tbodyfat,
                      control = boost_control(mstop = 3000))

    ### when to stop
    mstop(aic &lt;- AIC(bf_fp))
    plot(aic)

    ### coefficients
    cf &lt;- coef(bf_fp[mstop(aic)])
    length(cf)
    cf[abs(cf) &gt; 0]

</code></pre>

<hr>
<h2 id='glmboost'> Gradient Boosting with Component-wise Linear Models </h2><span id='topic+glmboost'></span><span id='topic+glmboost.formula'></span><span id='topic+glmboost.matrix'></span><span id='topic+glmboost.default'></span>

<h3>Description</h3>

<p>Gradient boosting for optimizing arbitrary loss functions where component-wise
linear models are utilized as base-learners.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'formula'
glmboost(formula, data = list(), weights = NULL,
          offset = NULL, family = Gaussian(),
          na.action = na.pass, contrasts.arg = NULL,
          center = TRUE, control = boost_control(), oobweights = NULL, ...)
## S3 method for class 'matrix'
glmboost(x, y, center = TRUE, weights = NULL,
          offset = NULL, family = Gaussian(),
          na.action = na.pass, control = boost_control(), oobweights = NULL, ...)
## Default S3 method:
glmboost(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="glmboost_+3A_formula">formula</code></td>
<td>
<p> a symbolic description of the model to be fit. </p>
</td></tr>
<tr><td><code id="glmboost_+3A_data">data</code></td>
<td>
<p> a data frame containing the variables in the model. </p>
</td></tr>
<tr><td><code id="glmboost_+3A_weights">weights</code></td>
<td>
<p> an optional vector of weights to be used in the fitting
process. </p>
</td></tr>
<tr><td><code id="glmboost_+3A_offset">offset</code></td>
<td>
<p> a numeric vector to be used as offset (optional).</p>
</td></tr>
<tr><td><code id="glmboost_+3A_family">family</code></td>
<td>
<p>a <code><a href="#topic+Family">Family</a></code> object.</p>
</td></tr>
<tr><td><code id="glmboost_+3A_na.action">na.action</code></td>
<td>
<p>a function which indicates what should happen when the data
contain <code>NA</code>s.</p>
</td></tr>
<tr><td><code id="glmboost_+3A_contrasts.arg">contrasts.arg</code></td>
<td>
<p>a list, whose entries are contrasts suitable for input
to the <code>contrasts</code> replacement function and whose names are
the names of columns of <code>data</code> containing factors.
See <code><a href="stats.html#topic+model.matrix.default">model.matrix.default</a></code>.</p>
</td></tr>
<tr><td><code id="glmboost_+3A_center">center</code></td>
<td>
<p>logical indicating of the predictor variables are centered before fitting.</p>
</td></tr>
<tr><td><code id="glmboost_+3A_control">control</code></td>
<td>
<p> a list of parameters controlling the algorithm. For
more details see <code><a href="#topic+boost_control">boost_control</a></code>. </p>
</td></tr>
<tr><td><code id="glmboost_+3A_oobweights">oobweights</code></td>
<td>
<p> an additional vector of out-of-bag weights, which is
used for the out-of-bag risk (i.e., if <code>boost_control(risk =
      "oobag")</code>). This argument is also used internally by
<code>cvrisk</code>. </p>
</td></tr>    
<tr><td><code id="glmboost_+3A_x">x</code></td>
<td>
<p> design matrix. Sparse matrices of class <code>Matrix</code> can be used as well.</p>
</td></tr>
<tr><td><code id="glmboost_+3A_y">y</code></td>
<td>
<p> vector of responses. </p>
</td></tr>
<tr><td><code id="glmboost_+3A_...">...</code></td>
<td>
<p> additional arguments passed to <code><a href="#topic+mboost_fit">mboost_fit</a></code>; currently none.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A (generalized) linear model is fitted using a boosting algorithm based on component-wise
univariate linear models. The fit, i.e., the regression coefficients, can be
interpreted in the usual way. The methodology is described in
Buehlmann and Yu (2003), Buehlmann (2006), and Buehlmann and Hothorn (2007).
Examples and further details are given in Hofner et al (2014).
</p>


<h3>Value</h3>

<p>An object of class <code>glmboost</code> with <code><a href="base.html#topic+print">print</a></code>, <code><a href="stats.html#topic+coef">coef</a></code>,
<code><a href="stats.html#topic+AIC">AIC</a></code> and <code><a href="stats.html#topic+predict">predict</a></code> methods being available.
For inputs with longer variable names, you might want to change
<code>par("mai")</code> before calling the <code>plot</code> method of <code>glmboost</code>
objects visualizing the coefficients path.
</p>


<h3>References</h3>

<p>Peter Buehlmann and Bin Yu (2003),
Boosting with the L2 loss: regression and classification.
<em>Journal of the American Statistical Association</em>, <b>98</b>,
324&ndash;339.
</p>
<p>Peter Buehlmann (2006), Boosting for high-dimensional linear models.
<em>The Annals of Statistics</em>, <b>34</b>(2), 559&ndash;583.
</p>
<p>Peter Buehlmann and Torsten Hothorn (2007),
Boosting algorithms: regularization, prediction and model fitting.
<em>Statistical Science</em>, <b>22</b>(4), 477&ndash;505.
</p>
<p>Torsten Hothorn, Peter Buehlmann, Thomas Kneib, Mattthias Schmid and
Benjamin Hofner (2010), Model-based Boosting 2.0. <em>Journal of
Machine Learning Research</em>, <b>11</b>, 2109&ndash;2113.
</p>
<p>Benjamin Hofner, Andreas Mayr, Nikolay Robinzonov and Matthias Schmid
(2014). Model-based Boosting in R: A Hands-on Tutorial Using the R
Package mboost. <em>Computational Statistics</em>, <b>29</b>, 3&ndash;35.<br />
<a href="https://doi.org/10.1007/s00180-012-0382-5">doi:10.1007/s00180-012-0382-5</a>
</p>
<p>Available as vignette via: <code>vignette(package = "mboost", "mboost_tutorial")</code>
</p>


<h3>See Also</h3>

<p>See <code><a href="#topic+mboost_fit">mboost_fit</a></code> for the generic boosting function, 
<code><a href="#topic+gamboost">gamboost</a></code> for boosted additive models, and
<code><a href="#topic+blackboost">blackboost</a></code> for boosted trees. 
</p>
<p>See <code><a href="#topic+baselearners">baselearners</a></code> for possible base-learners. 
</p>
<p>See <code><a href="#topic+cvrisk">cvrisk</a></code> for cross-validated stopping iteration. 
</p>
<p>Furthermore see <code><a href="#topic+boost_control">boost_control</a></code>, <code><a href="#topic+Family">Family</a></code> and
<code><a href="#topic+methods">methods</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
    ### a simple two-dimensional example: cars data
    cars.gb &lt;- glmboost(dist ~ speed, data = cars,
                        control = boost_control(mstop = 2000),
                        center = FALSE)
    cars.gb

    ### coefficients should coincide
    cf &lt;- coef(cars.gb, off2int = TRUE)     ## add offset to intercept
    coef(cars.gb) + c(cars.gb$offset, 0)    ## add offset to intercept (by hand)
    signif(cf, 3)
    signif(coef(lm(dist ~ speed, data = cars)), 3)
    ## almost converged. With higher mstop the results get even better

    ### now we center the design matrix for
    ### much quicker "convergence"
    cars.gb_centered &lt;- glmboost(dist ~ speed, data = cars,
                                 control = boost_control(mstop = 2000),
                                 center = TRUE)

    ## plot coefficient paths of glmboost
    par(mfrow=c(1,2), mai = par("mai") * c(1, 1, 1, 2.5))
    plot(cars.gb, main = "without centering")
    plot(cars.gb_centered, main = "with centering")

    ### alternative loss function: absolute loss
    cars.gbl &lt;- glmboost(dist ~ speed, data = cars,
                         control = boost_control(mstop = 1000),
                         family = Laplace())
    cars.gbl
    coef(cars.gbl, off2int = TRUE)

    ### plot fit
    par(mfrow = c(1,1))
    plot(dist ~ speed, data = cars)
    lines(cars$speed, predict(cars.gb), col = "red")     ## quadratic loss
    lines(cars$speed, predict(cars.gbl), col = "green")  ## absolute loss

    ### Huber loss with adaptive choice of delta
    cars.gbh &lt;- glmboost(dist ~ speed, data = cars,
                         control = boost_control(mstop = 1000),
                         family = Huber())

    lines(cars$speed, predict(cars.gbh), col = "blue")   ## Huber loss
    legend("topleft", col = c("red", "green", "blue"), lty = 1,
           legend = c("Gaussian", "Laplace", "Huber"), bty = "n")

    ### sparse high-dimensional example that makes use of the matrix
    ### interface of glmboost and uses the matrix representation from
    ### package Matrix
    library("Matrix")
    n &lt;- 100
    p &lt;- 10000
    ptrue &lt;- 10
    X &lt;- Matrix(0, nrow = n, ncol = p)
    X[sample(1:(n * p), floor(n * p / 20))] &lt;- runif(floor(n * p / 20))
    beta &lt;- numeric(p)
    beta[sample(1:p, ptrue)] &lt;- 10
    y &lt;- drop(X %*% beta + rnorm(n, sd = 0.1))
    mod &lt;- glmboost(y = y, x = X, center = TRUE) ### mstop needs tuning
    coef(mod, which = which(beta &gt; 0))

</code></pre>

<hr>
<h2 id='IPCweights'> Inverse Probability of Censoring Weights </h2><span id='topic+IPCweights'></span>

<h3>Description</h3>

<p>Compute weights for censored regression via the inverted 
probability of censoring principle.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>IPCweights(x, maxweight = 5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="IPCweights_+3A_x">x</code></td>
<td>
<p> an object of class <code>Surv</code>.</p>
</td></tr>
<tr><td><code id="IPCweights_+3A_maxweight">maxweight</code></td>
<td>
<p> the maximal value of the returned weights. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>Inverse probability of censoring weights are one possibility to fit
models formulated in the <em>full data world</em> in the presence of censoring,
i.e., the <em>observed data world</em>, see van der Laan and Robins (2003) for
the underlying theory and Hothorn et al. (2006) for an application to
survival analysis.
</p>


<h3>Value</h3>

<p>A vector of numeric weights. 
</p>


<h3>References</h3>

 
<p>Mark J. van der Laan and James M. Robins (2003), 
<em>Unified Methods for Censored Longitudinal Data and Causality</em>, 
Springer, New York.
</p>
<p>Torsten Hothorn, Peter Buehlmann, Sandrine Dudoit,
Annette Molinaro and Mark J. van der Laan (2006), Survival ensembles.
<em>Biostatistics</em> <b>7</b>(3), 355&ndash;373.
</p>
<p>Peter Buehlmann and Torsten Hothorn (2007),
Boosting algorithms: regularization, prediction and model fitting.
<em>Statistical Science</em>, <b>22</b>(4), 477&ndash;505.
</p>

<hr>
<h2 id='mboost'> Gradient Boosting for Additive Models </h2><span id='topic+mboost'></span><span id='topic+gamboost'></span>

<h3>Description</h3>

<p>Gradient boosting for optimizing arbitrary loss functions, where component-wise
arbitrary base-learners, e.g., smoothing procedures,  are utilized as additive
base-learners.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mboost(formula, data = list(), na.action = na.omit, weights = NULL, 
       offset = NULL, family = Gaussian(), control = boost_control(),
       oobweights = NULL, baselearner = c("bbs", "bols", "btree", "bss", "bns"), 
       ...)

gamboost(formula, data = list(), na.action = na.omit, weights = NULL, 
         offset = NULL, family = Gaussian(), control = boost_control(),
         oobweights = NULL, baselearner = c("bbs", "bols", "btree", "bss", "bns"),
         dfbase = 4, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mboost_+3A_formula">formula</code></td>
<td>
<p> a symbolic description of the model to be fit. </p>
</td></tr>
<tr><td><code id="mboost_+3A_data">data</code></td>
<td>
<p> a data frame containing the variables in the model. </p>
</td></tr>
<tr><td><code id="mboost_+3A_na.action">na.action</code></td>
<td>
<p>a function which indicates what should happen when
the data contain <code>NA</code>s.</p>
</td></tr>
<tr><td><code id="mboost_+3A_weights">weights</code></td>
<td>
<p> (optional) a numeric vector of weights to be used in 
the fitting process.</p>
</td></tr>
<tr><td><code id="mboost_+3A_offset">offset</code></td>
<td>
<p> a numeric vector to be used as offset (optional).</p>
</td></tr>
<tr><td><code id="mboost_+3A_family">family</code></td>
<td>
<p>a <code><a href="#topic+Family">Family</a></code> object.</p>
</td></tr>
<tr><td><code id="mboost_+3A_control">control</code></td>
<td>
<p> a list of parameters controlling the algorithm. For
more details see <code><a href="#topic+boost_control">boost_control</a></code>. </p>
</td></tr>
<tr><td><code id="mboost_+3A_oobweights">oobweights</code></td>
<td>
<p> an additional vector of out-of-bag weights, which is
used for the out-of-bag risk (i.e., if <code>boost_control(risk =
      "oobag")</code>). This argument is also used internally by
<code>cvrisk</code>. </p>
</td></tr>
<tr><td><code id="mboost_+3A_baselearner">baselearner</code></td>
<td>
<p> a character specifying the component-wise base
learner to be used: <code><a href="#topic+bbs">bbs</a></code> means P-splines with a
B-spline basis (see Schmid and Hothorn 2008), <code><a href="#topic+bols">bols</a></code>
linear models and <code><a href="#topic+btree">btree</a></code> boosts stumps. 
<code>bss</code> and <code>bns</code> are deprecated.
Component-wise smoothing splines have been considered in Buehlmann
and Yu (2003) and Schmid and Hothorn (2008) investigate P-splines
with a B-spline basis. Kneib, Hothorn and Tutz (2009) also utilize
P-splines with a B-spline basis, supplement them with their
bivariate tensor product version to estimate interaction surfaces
and spatial effects and also consider random effects base
learners.</p>
</td></tr>
<tr><td><code id="mboost_+3A_dfbase">dfbase</code></td>
<td>
<p> a single integer giving the degrees of freedom for P-spline 
base-learners (<code><a href="#topic+bbs">bbs</a></code>) globally. </p>
</td></tr>
<tr><td><code id="mboost_+3A_...">...</code></td>
<td>
<p> additional arguments passed to <code><a href="#topic+mboost_fit">mboost_fit</a></code>; currently none.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A (generalized) additive model is fitted using a boosting algorithm based on
component-wise base-learners. 
</p>
<p>The base-learners can either be specified via the <code>formula</code> object or via 
the <code>baselearner</code> argument. The latter argument is the default base-learner 
which is used for all variables in the formula, whithout explicit base-learner 
specification (i.e., if the base-learners are explicitly specified in <code>formula</code>, 
the <code>baselearner</code> argument will be ignored for this variable). 
</p>
<p>Of note, <code>"bss"</code> and <code>"bns"</code> are deprecated and only in the list for 
backward compatibility.
</p>
<p>Note that more base-learners (i.e., in addition to the ones provided
via <code>baselearner</code>) can be specified in <code>formula</code>. See 
<code><a href="#topic+baselearners">baselearners</a></code> for details.
</p>
<p>The only difference when calling <code>mboost</code> and <code>gamboost</code> is that the
latter function allows one to specify default degrees of freedom for smooth 
effects specified via <code>baselearner = "bbs"</code>. In all other cases, 
degrees of freedom need to be set manually via a specific definition of the 
corresponding base-learner.
</p>


<h3>Value</h3>

<p>An object of class <code>mboost</code> with <code><a href="base.html#topic+print">print</a></code>,
<code><a href="stats.html#topic+AIC">AIC</a></code>, <code><a href="#topic+plot">plot</a></code> and <code><a href="stats.html#topic+predict">predict</a></code>
methods being available.
</p>


<h3>References</h3>

<p>Peter Buehlmann and Bin Yu (2003),
Boosting with the L2 loss: regression and classification.
<em>Journal of the American Statistical Association</em>, <b>98</b>,
324&ndash;339.
</p>
<p>Peter Buehlmann and Torsten Hothorn (2007),
Boosting algorithms: regularization, prediction and model fitting.
<em>Statistical Science</em>, <b>22</b>(4), 477&ndash;505.
</p>
<p>Thomas Kneib, Torsten Hothorn and Gerhard Tutz (2009), Variable selection and
model choice in geoadditive regression models, <em>Biometrics</em>, <b>65</b>(2),
626&ndash;634.
</p>
<p>Matthias Schmid and Torsten Hothorn (2008),
Boosting additive models using component-wise P-splines as
base-learners. <em>Computational Statistics &amp; Data Analysis</em>,
<b>53</b>(2), 298&ndash;311.
</p>
<p>Torsten Hothorn, Peter Buehlmann, Thomas Kneib, Mattthias Schmid
and Benjamin Hofner (2010),
Model-based Boosting 2.0.
<em>Journal of Machine Learning Research</em>, <b>11</b>, 2109 &ndash; 2113.
</p>
<p>Benjamin Hofner, Andreas Mayr, Nikolay Robinzonov and Matthias Schmid
(2014). Model-based Boosting in R: A Hands-on Tutorial Using the R
Package mboost. <em>Computational Statistics</em>, <b>29</b>, 3&ndash;35.<br />
<a href="https://doi.org/10.1007/s00180-012-0382-5">doi:10.1007/s00180-012-0382-5</a>
</p>
<p>Available as vignette via: <code>vignette(package = "mboost", "mboost_tutorial")</code>
</p>


<h3>See Also</h3>

<p>See <code><a href="#topic+mboost_fit">mboost_fit</a></code> for the generic boosting function, 
<code><a href="#topic+glmboost">glmboost</a></code> for boosted linear models, and
<code><a href="#topic+blackboost">blackboost</a></code> for boosted trees. 
</p>
<p>See <code><a href="#topic+baselearners">baselearners</a></code> for possible base-learners. 
</p>
<p>See <code><a href="#topic+cvrisk">cvrisk</a></code> for cross-validated stopping iteration. 
</p>
<p>Furthermore see <code><a href="#topic+boost_control">boost_control</a></code>, <code><a href="#topic+Family">Family</a></code> and
<code><a href="#topic+methods">methods</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
    ### a simple two-dimensional example: cars data
    cars.gb &lt;- gamboost(dist ~ speed, data = cars, dfbase = 4,
                        control = boost_control(mstop = 50))
    cars.gb
    AIC(cars.gb, method = "corrected")

    ### plot fit for mstop = 1, ..., 50
    plot(dist ~ speed, data = cars)
    tmp &lt;- sapply(1:mstop(AIC(cars.gb)), function(i)
        lines(cars$speed, predict(cars.gb[i]), col = "red"))
    lines(cars$speed, predict(smooth.spline(cars$speed, cars$dist),
                              cars$speed)$y, col = "green")

    ### artificial example: sinus transformation
    x &lt;- sort(runif(100)) * 10
    y &lt;- sin(x) + rnorm(length(x), sd = 0.25)
    plot(x, y)
    ### linear model
    lines(x, fitted(lm(y ~ sin(x) - 1)), col = "red")
    ### GAM
    lines(x, fitted(gamboost(y ~ x,
                    control = boost_control(mstop = 500))),
          col = "green")

</code></pre>

<hr>
<h2 id='mboost_fit'> Model-based Gradient Boosting </h2><span id='topic+mboost_fit'></span>

<h3>Description</h3>

<p>Work-horse for gradient boosting for optimizing arbitrary loss functions, 
where component-wise models are utilized as base-learners. Usually, this function 
is not called directly by the user.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mboost_fit(blg, response, weights = rep(1, NROW(response)), offset = NULL,
           family = Gaussian(), control = boost_control(), oobweights =
           as.numeric(weights == 0))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mboost_fit_+3A_blg">blg</code></td>
<td>
<p> a list of objects of elements of class <code>blg</code>, as returned by 
all base-learners.</p>
</td></tr>
<tr><td><code id="mboost_fit_+3A_response">response</code></td>
<td>
<p> the response variable.</p>
</td></tr>
<tr><td><code id="mboost_fit_+3A_weights">weights</code></td>
<td>
<p> (optional) a numeric vector of weights to be used in 
the fitting process.</p>
</td></tr>
<tr><td><code id="mboost_fit_+3A_offset">offset</code></td>
<td>
<p> a numeric vector to be used as offset (optional).</p>
</td></tr>
<tr><td><code id="mboost_fit_+3A_family">family</code></td>
<td>
<p>a <code><a href="#topic+Family">Family</a></code> object.</p>
</td></tr>
<tr><td><code id="mboost_fit_+3A_control">control</code></td>
<td>
<p> a list of parameters controlling the algorithm. For
more details see <code><a href="#topic+boost_control">boost_control</a></code>. </p>
</td></tr>
<tr><td><code id="mboost_fit_+3A_oobweights">oobweights</code></td>
<td>
<p> an additional vector of out-of-bag weights, which is
used for the out-of-bag risk (i.e., if <code>boost_control(risk =
      "oobag")</code>). This argument is also used internally by
<code>cvrisk</code>. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function implements component-wise functional gradient boosting in
a generic way. This function is the main work horse and used as back-end by
all boosting algorithms in a unified way. Usually, this function is not
called directly.  Note that the more convenient modelling interfaces 
<code><a href="#topic+gamboost">gamboost</a></code>, <code><a href="#topic+glmboost">glmboost</a></code> and <code><a href="#topic+blackboost">blackboost</a></code> 
all call <code>mboost_fit</code>.
</p>
<p>Basically, the algorithm is initialized with a function
for computing the negative gradient of the loss function (via its
<code>family</code> argument) and one or more base-learners (given as
<code>blg</code>). Usually <code>blg</code> and <code>response</code> are computed in
the functions <code><a href="#topic+gamboost">gamboost</a></code>, <code><a href="#topic+glmboost">glmboost</a></code>,
<code><a href="#topic+blackboost">blackboost</a></code> or <code><a href="#topic+mboost">mboost</a></code>. See there for details 
on the specification of base-learners.
</p>
<p>The algorithm minimized the in-sample empirical risk defined as
the weighted sum (by <code>weights</code>) of the loss function (corresponding
to the negative gradient) evaluated at the data.
</p>
<p>The structure of the model is determined by the structure
of the base-learners. If more than one base-learner is given,
the model is additive in these components.
</p>
<p>Base-learners can be specified via a formula interface
(function <code>mboost</code>) or as a list of objects of class <code>bl</code>,
see, e.g., <code><a href="#topic+bols">bols</a></code>.
</p>
<p><code>oobweights</code> is a vector used internally by <code>cvrisk</code>. When carrying
out cross-validation to determine the optimal stopping iteration of a boosting
model, the default value of <code>oobweights</code> (out-of-bag weights) assures
that the cross-validated risk is computed using the same observation weights
as those used for fitting the boosting model. It is strongly recommended to
leave this argument unspecified.
</p>


<h3>Value</h3>

<p>An object of class <code>mboost</code> with <code><a href="base.html#topic+print">print</a></code>,
<code><a href="stats.html#topic+AIC">AIC</a></code>, <code><a href="#topic+plot">plot</a></code> and <code><a href="stats.html#topic+predict">predict</a></code>
methods being available.
</p>


<h3>References</h3>

<p>Peter Buehlmann and Bin Yu (2003),
Boosting with the L2 loss: regression and classification.
<em>Journal of the American Statistical Association</em>, <b>98</b>,
324&ndash;339.
</p>
<p>Peter Buehlmann and Torsten Hothorn (2007),
Boosting algorithms: regularization, prediction and model fitting.
<em>Statistical Science</em>, <b>22</b>(4), 477&ndash;505.
</p>
<p>Torsten Hothorn, Peter Buehlmann, Thomas Kneib, Mattthias Schmid and
Benjamin Hofner (2010), Model-based Boosting 2.0. <em>Journal of
Machine Learning Research</em>, <b>11</b>, 2109&ndash;2113.
</p>
<p>Yoav Freund and Robert E. Schapire (1996),
Experiments with a new boosting algorithm.
In <em>Machine Learning: Proc. Thirteenth International Conference</em>,
148&ndash;156.
</p>
<p>Jerome H. Friedman (2001),
Greedy function approximation: A gradient boosting machine.
<em>The Annals of Statistics</em>, <b>29</b>, 1189&ndash;1232.
</p>
<p>Benjamin Hofner, Andreas Mayr, Nikolay Robinzonov and Matthias Schmid
(2014). Model-based Boosting in R: A Hands-on Tutorial Using the R
Package mboost. <em>Computational Statistics</em>, <b>29</b>, 3&ndash;35.<br />
<a href="https://doi.org/10.1007/s00180-012-0382-5">doi:10.1007/s00180-012-0382-5</a>
</p>
<p>Available as vignette via: <code>vignette(package = "mboost", "mboost_tutorial")</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+glmboost">glmboost</a></code> for boosted linear models and
<code><a href="#topic+blackboost">blackboost</a></code> for boosted trees. See e.g. <code><a href="#topic+bbs">bbs</a></code>
for possible base-learners. See <code><a href="#topic+cvrisk">cvrisk</a></code> for
cross-validated stopping iteration. Furthermore see
<code><a href="#topic+boost_control">boost_control</a></code>, <code><a href="#topic+Family">Family</a></code> and
<code><a href="#topic+methods">methods</a></code>. </p>


<h3>Examples</h3>

<pre><code class='language-R'>  data("bodyfat", package = "TH.data")

  ### formula interface: additive Gaussian model with
  ### a non-linear step-function in `age', a linear function in `waistcirc'
  ### and a smooth non-linear smooth function in `hipcirc'
  mod &lt;- mboost(DEXfat ~ btree(age) + bols(waistcirc) + bbs(hipcirc),
                data = bodyfat)
  layout(matrix(1:6, nc = 3, byrow = TRUE))
  plot(mod, main = "formula")

  ### the same
  with(bodyfat,
       mod &lt;- mboost_fit(list(btree(age), bols(waistcirc), bbs(hipcirc)),
                         response = DEXfat))
  plot(mod, main = "base-learner")
</code></pre>

<hr>
<h2 id='mboost_intern'>
Call internal functions.
</h2><span id='topic+mboost_intern'></span>

<h3>Description</h3>

<p>Call one of the internal mboost functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mboost_intern(..., fun = c("df2lambda", "hyper_bbs", "hyper_ols",
                  "bl_lin", "bl_lin_matrix",
                  "Complete.cases", "get_index", "isMATRIX",
                  "cbs", "bsplines", "model.frame.blg", 
                  "check_newdata", "do_trace", "rescale_weights", "nnls2D"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mboost_intern_+3A_...">...</code></td>
<td>
<p> Arguments to <code>fun</code>. </p>
</td></tr>
<tr><td><code id="mboost_intern_+3A_fun">fun</code></td>
<td>
<p> The name on an internal mboost function. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function must not be called under any circumstances.
</p>

<hr>
<h2 id='mboost-package'>
mboost: Model-Based Boosting
</h2><span id='topic+mboost-package'></span><span id='topic+mboost_package'></span><span id='topic+package_mboost'></span><span id='topic+package-mboost'></span>

<h3>Description</h3>

<p>Functional gradient descent algorithm
(boosting) for optimizing general risk functions utilizing
component-wise (penalized) least squares estimates or regression
trees as base-learners for fitting generalized linear, additive
and interaction models to potentially high-dimensional data.
</p>


<h3>Details</h3>

<p>This package is intended for modern regression modeling and stands
in-between classical generalized linear and additive models, as for example
implemented by <code><a href="stats.html#topic+lm">lm</a></code>, <code><a href="stats.html#topic+glm">glm</a></code>, or <code><a href="mgcv.html#topic+gam">gam</a></code>,
and machine-learning approaches for complex interactions models,
most prominently represented by <code><a href="gbm.html#topic+gbm">gbm</a></code> and
<code><a href="randomForest.html#topic+randomForest">randomForest</a></code>.
</p>
<p>All functionality in this package is based on the generic
implementation of the optimization algorithm  (function
<code><a href="#topic+mboost_fit">mboost_fit</a></code>) that allows for fitting linear, additive,
and interaction models (and mixtures of those) in low and
high dimensions. The response may be numeric, binary, ordered,
censored or count data.
</p>
<p>Both theory and applications are discussed by Buehlmann and Hothorn (2007).
UseRs without a basic knowledge of boosting methods are asked
to read this introduction before analyzing data using this package.
The examples presented in this paper are available as package vignette
<code>mboost_illustrations</code>.
</p>
<p>Note that the model fitting procedures in this package DO NOT automatically
determine an appropriate model complexity. This task is the responsibility
of the data analyst.
</p>
<p>A description of novel features that were introduced in version 2.0 is
given in Hothorn et. al (2010).
</p>
<p>Hofner et al. (2014) present a comprehensive hands-on tutorial for using the
package <code>mboost</code>, which is also available as
<code>vignette(package = "mboost", "mboost_tutorial")</code>.
</p>
<p>Ben Taieba and Hyndman (2013) used this package for fitting their model in the
Kaggle Global Energy Forecasting Competition 2012. The corresponding research
paper is a good starting point when you plan to analyze your data using
<code>mboost</code>.
</p>


<h3>NEWS in 2.9-series</h3>

<p>Series 2.9 provides a new family (<code>RCG</code>), uses <code>partykit::ctree</code> 
instead of <code>party::ctree</code> to be more flexible, allows for multivariate 
negative gradients, and leave-one-out crossvalidation. Further minor changes were 
introduces and quite some bugs were fixed. 
</p>
<p><b>For more details and other changes see</b><br /><code>news(Version &gt;= "2.9-0", package  = "mboost")</code>
</p>


<h3>NEWS in 2.8-series</h3>

<p>Series 2.8 allows to fit models with zero boosting steps (i.e., models containing 
only the offset). Furthermore, cross-validation can now also select a model
without base-learners. In a <code><a href="#topic+Binomial">Binomial</a></code> family one can now specifiy
links via <code>make.link</code>. With <code>Binomial(type = "glm")</code> an alternative 
implementation of <code>Binomial</code> models is now existing and defines the model
along the lines of the <code>glm</code> implementation. Additionally, it works not only with a 
two-level factor but also with a two-column matrix containing the number of 
successes and number of failures. Finally, a new base-learner <code><a href="#topic+bkernel">bkernel</a></code> for
kernel boosting was added. The references were updated and a lot of bugs fixed. 
</p>
<p><b>For more details and other changes see</b><br /><code>news(Version &gt;= "2.8-0", package  = "mboost")</code>
</p>


<h3>NEWS in 2.7-series</h3>

<p>Series 2.7 provides a new family (<code>Cindex</code>), variable importance measures
(<code>varimp</code>) and improved plotting facilities. The manual was updated in 
various places, vignettes were improved and a lot of bugs were fixed. 
</p>
<p><b>For more details and other changes see</b><br /><code>news(Version &gt;= "2.7-0", package  = "mboost")</code>
</p>


<h3>NEWS in 2.6-series</h3>

<p>Series 2.6 includes a lot of bug fixes and improvements. Most notably,
the development of the package is now hosted entirely on github in the
project <a href="https://github.com/boost-R/mboost/">boost-R/mboost</a>.
Furthermore, the package is now maintained by Benjamin Hofner.
</p>
<p><b>For more details and other changes see</b><br /><code>news(Version &gt;= "2.6-0", package  = "mboost")</code>
</p>


<h3>NEWS in 2.5-series</h3>

<p>Crossvaliation does not stop on errors in single folds anymore an was
sped up by setting <code>mc.preschedule = FALSE</code> if parallel
computations via <code><a href="parallel.html#topic+mclapply">mclapply</a></code> are used. The
<code><a href="#topic+plot.mboost">plot.mboost</a></code> function is now documented. Values outside
the boundary knots are now better handeled (forbidden during fitting,
while linear extrapolation is used for prediction). Further perfomance
improvements and a lot of bug fixes have been added.
</p>
<p><b>For more details and other changes see</b><br /><code>news(Version &gt;= "2.5-0", package  = "mboost")</code>
</p>


<h3>NEWS in 2.4-series</h3>

<p>Bootstrap confidence intervals have been implemented in the novel
<code><a href="#topic+confint.mboost">confint</a></code> function. The stability
selection procedure has now been moved to a stand-alone package called
<span class="pkg">stabs</span>, which now also implements an iterface to use stability
selection with other fitting functions. A generic function for
<code>"mboost"</code> models is implemented in <span class="pkg">mboost</span>.
</p>
<p><b>For more details and other changes see</b><br /><code>news(Version &gt;= "2.4-0", package  = "mboost")</code>
</p>


<h3>NEWS in 2.3-series</h3>

<p>The stability selection procedure has been completely rewritten and
improved. The code base is now extensively tested. New options allow
for a less conservative error control.
</p>
<p>Constrained effects can now be fitted using quadratic programming
methods using the option <code>type = "quad.prog"</code> (default) for
highly improved speed. Additionally, new constraints have been added.
</p>
<p>Other important changes include:
</p>

<ul>
<li><p> A new replacement function <code>mstop(mod) &lt;- i</code> as an alternative to
<code>mod[i]</code> was added (as suggested by Achim Zeileis).
</p>
</li>
<li><p> We added new families <code>Hurdle</code> and <code>Multinomial</code>.
</p>
</li>
<li><p> We added a new argument <code>stopintern</code> for internal stopping
(based on out-of-bag data) during fitting to <code>boost_control</code>.
</p>
</li></ul>

<p><b>For more details and other changes see</b><br /><code>news(Version &gt;= "2.3-0", package  = "mboost")</code>
</p>


<h3>NEWS in 2.2-series</h3>

<p>Starting from version 2.2, the default for the degrees of freedom has
changed. Now the degrees of freedom are (per default) defined as
</p>
<p style="text-align: center;"><code class="reqn">\mathrm{df}(\lambda) = \mathrm{trace}(2S -
  S^{\top}S),</code>
</p>
<p> with smoother matrix
<code class="reqn">S = X(X^{\top}X + \lambda K)^{-1} X</code> (see Hofner et al., 2011). Earlier versions used the trace of the
smoother matrix <code class="reqn">\mathrm{df}(\lambda) = \mathrm{trace}(S)</code> as
degrees of freedom. One can change the old definition using
<code>options(mboost_dftraceS = TRUE)</code> (see also B. Hofner et al.,
2011 and <code><a href="#topic+bols">bols</a></code>).
</p>
<p>Other important changes include:
</p>

<ul>
<li><p> We switched from packages <code>multicore</code> and <code>snow</code> to
<code>parallel</code>
</p>
</li>
<li><p> We changed the behavior of <code>bols(x, intercept = FALSE)</code>
when <code>x</code> is a factor: now the intercept is simply dropped from
the design matrix and the coding can be specified as usually for
factors. Additionally, a new contrast is introduced:
<code>"contr.dummy"</code> (see <code><a href="#topic+bols">bols</a></code> for details).
</p>
</li>
<li><p> We changed the computation of B-spline basis at the
boundaries; B-splines now also use equidistant knots in the
boundaries (per default).
</p>
</li></ul>

<p><b>For more details and other changes see</b><br /><code>news(Version &gt;= "2.2-0" &amp; Version &lt; "2.3-0", package  = "mboost")</code>
</p>


<h3>NEWS in 2.1-series</h3>

<p>In the 2.1 series, we added multiple new base-learners including
<code><a href="#topic+bmono">bmono</a></code> (monotonic effects), <code><a href="#topic+brad">brad</a></code> (radial
basis functions) and <code><a href="#topic+bmrf">bmrf</a></code> (Markov random fields), and
extended <code><a href="#topic+bbs">bbs</a></code> to incorporate cyclic splines (via argument
<code>cyclic = TRUE</code>). We also changed the default <code>df</code> for
<code><a href="#topic+bspatial">bspatial</a></code> to <code>6</code>.
</p>
<p>Starting from this version, we now also automatically center the
variables in <code><a href="#topic+glmboost">glmboost</a></code> (argument <code>center = TRUE</code>).
</p>
<p><b>For more details and other changes see</b><br /><code>news(Version &gt;= "2.1-0" &amp; Version &lt; "2.2-0", package  = "mboost")</code>
</p>


<h3>NEWS in 2.0-series</h3>

<p>Version 2.0 comes with new features, is faster and more accurate
in some aspects. In addition, some changes to the user interface
were necessary: Subsetting <code>mboost</code> objects changes the object.
At each time, a model is associated with a number of boosting iterations
which can be changed (increased or decreased) using the subset operator.
</p>
<p>The <code>center</code> argument in <code><a href="#topic+bols">bols</a></code> was renamed
to <code>intercept</code>. Argument <code>z</code> renamed to <code>by</code>.
</p>
<p>The base-learners <code>bns</code> and <code>bss</code> are deprecated
and replaced by <code>bbs</code> (which results in qualitatively the
same models but is computationally much more attractive).
</p>
<p>New features include new families (for example for ordinal regression)
and the <code>which</code> argument to the <code>coef</code> and <code>predict</code>
methods for selecting interesting base-learners. Predict
methods are much faster now.
</p>
<p>The memory consumption could be reduced considerably,
thanks to sparse matrix technology in package <code>Matrix</code>.
Resampling procedures run automatically in parallel
on OSes where parallelization via package <code>parallel</code> is available.
</p>
<p>The most important advancement is a generic implementation
of the optimizer in function <code><a href="#topic+mboost_fit">mboost_fit</a></code>.
</p>
<p><b>For more details and other changes see</b><br /><code>news(Version &gt;= "2.0-0" &amp; Version &lt; "2.1-0", package  = "mboost")</code>
</p>


<h3>Author(s)</h3>

<p>Torsten Hothorn,Peter Buehlmann, Thomas Kneib, Matthias Schmid and
Benjamin Hofner &lt;<a href="mailto:Benjamin.Hofner@pei.de">Benjamin.Hofner@pei.de</a>&gt;
</p>


<h3>References</h3>

<p>Peter Buehlmann and Torsten Hothorn (2007),
Boosting algorithms: regularization, prediction and model fitting.
<em>Statistical Science</em>, <b>22</b>(4), 477&ndash;505. <br />
<a href="https://doi.org/10.1214/07-STS242">doi:10.1214/07-STS242</a>
</p>
<p>Torsten Hothorn, Peter Buehlmann, Thomas Kneib, Matthias Schmid and
Benjamin Hofner (2010), Model-based Boosting 2.0. <em>Journal of
Machine Learning Research</em>, <b>11</b>, 2109&ndash;2113. <br />
<a href="https://jmlr.csail.mit.edu/papers/v11/hothorn10a.html">https://jmlr.csail.mit.edu/papers/v11/hothorn10a.html</a>
</p>
<p>Benjamin Hofner, Torsten Hothorn, Thomas Kneib, and Matthias Schmid (2011),
A framework for unbiased model selection based on boosting.
<em>Journal of Computational and Graphical Statistics</em>, <b>20</b>, 956&ndash;971. <br />
<a href="https://doi.org/10.1198/jcgs.2011.09220">doi:10.1198/jcgs.2011.09220</a>
</p>
<p>Benjamin Hofner, Andreas Mayr, Nikolay Robinzonov and Matthias Schmid
(2014). Model-based Boosting in R: A Hands-on Tutorial Using the R
Package mboost. <em>Computational Statistics</em>, <b>29</b>, 3&ndash;35.<br />
<a href="https://doi.org/10.1007/s00180-012-0382-5">doi:10.1007/s00180-012-0382-5</a>
</p>
<p>Available as vignette via: <code>vignette(package = "mboost",
    "mboost_tutorial")</code>
</p>
<p>Souhaib Ben Taieba and Rob J. Hyndman (2014),
A gradient boosting approach to the Kaggle load forecasting competition.
<em>International Journal of Forecasting</em>, <b>30</b>, 382&ndash;394.<br />
<a href="https://doi.org/10.1016/j.ijforecast.2013.07.005">doi:10.1016/j.ijforecast.2013.07.005</a>
</p>


<h3>See Also</h3>

<p>The main fitting functions include:<br />
</p>

<ul>
<li> <p><code><a href="#topic+gamboost">gamboost</a></code> for boosted (generalized) additive models,
</p>
</li>
<li> <p><code><a href="#topic+glmboost">glmboost</a></code> for boosted linear models and
</p>
</li>
<li> <p><code><a href="#topic+blackboost">blackboost</a></code> for boosted trees.
</p>
</li></ul>

<p>Model tuning is done via cross-validation as implemented in <code><a href="#topic+cvrisk">cvrisk</a></code>.<br />
See there for more details and further links.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
  data("bodyfat", package = "TH.data")
  set.seed(290875)

  ### model conditional expectation of DEXfat given
  model &lt;- mboost(DEXfat ~
      bols(age) +                 ### a linear function of age
      btree(hipcirc, waistcirc) + ### a smooth non-linear interaction of
                                  ### hip and waist circumference
      bbs(kneebreadth),           ### a smooth function of kneebreadth
      data = bodyfat, control = boost_control(mstop = 100))

  ### 10-fold cv for assessing `optimal' number of boosting iterations
  cvm &lt;- cvrisk(model, papply = lapply, 
                folds = cv(model.weights(model), type = "kfold"))
  ### probably needs larger initial mstop but the
  ### CRAN team is picky about running times for examples
  plot(cvm)

  ### restrict model to mstop(cvm)
  model[mstop(cvm), return = FALSE]
  mstop(model)

  ### plot age and kneebreadth
  layout(matrix(1:2, nc = 2))
  plot(model, which = c("age", "kneebreadth"))

  ### plot interaction of hip and waist circumference
  attach(bodyfat)
  nd &lt;- expand.grid(hipcirc = h &lt;- seq(from = min(hipcirc),
                                  to = max(hipcirc),
                                  length = 100),
                    waistcirc = w &lt;- seq(from = min(waistcirc),
                                  to = max(waistcirc),
                                  length = 100))
  plot(model, which = 2, newdata = nd)
  detach(bodyfat)

  ### customized plot
  layout(1)
  pr &lt;- predict(model, which = "hip", newdata = nd)
  persp(x = h, y = w, z = matrix(pr, nrow = 100, ncol = 100))

</code></pre>

<hr>
<h2 id='methods'> Methods for Gradient Boosting Objects </h2><span id='topic+mboost_methods'></span><span id='topic+print.glmboost'></span><span id='topic+print.mboost'></span><span id='topic+summary.mboost'></span><span id='topic+coef.mboost'></span><span id='topic+coef.glmboost'></span><span id='topic++5B.mboost'></span><span id='topic+AIC.mboost'></span><span id='topic+mstop'></span><span id='topic+mstop.gbAIC'></span><span id='topic+mstop.mboost'></span><span id='topic+mstop.cvrisk'></span><span id='topic+mstop+3C-'></span><span id='topic+predict.mboost'></span><span id='topic+predict.gamboost'></span><span id='topic+predict.blackboost'></span><span id='topic+predict.glmboost'></span><span id='topic+fitted.mboost'></span><span id='topic+residuals.mboost'></span><span id='topic+resid.mboost'></span><span id='topic+variable.names.glmboost'></span><span id='topic+variable.names.mboost'></span><span id='topic+risk'></span><span id='topic+risk.mboost'></span><span id='topic+extract'></span><span id='topic+extract.mboost'></span><span id='topic+extract.gamboost'></span><span id='topic+extract.glmboost'></span><span id='topic+extract.blackboost'></span><span id='topic+extract.blg'></span><span id='topic+extract.bl_lin'></span><span id='topic+extract.bl_tree'></span><span id='topic+logLik.mboost'></span><span id='topic+hatvalues.gamboost'></span><span id='topic+hatvalues.glmboost'></span><span id='topic+selected'></span><span id='topic+selected.mboost'></span><span id='topic+nuisance'></span><span id='topic+nuisance.mboost'></span><span id='topic+downstream.test'></span>

<h3>Description</h3>

<p>Methods for models fitted by boosting algorithms.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'glmboost'
print(x, ...)
## S3 method for class 'mboost'
print(x, ...)

## S3 method for class 'mboost'
summary(object, ...)

## S3 method for class 'mboost'
coef(object, which = NULL,
    aggregate = c("sum", "cumsum", "none"), ...)
## S3 method for class 'glmboost'
coef(object, which = NULL,
     aggregate = c("sum", "cumsum", "none"), off2int = FALSE, ...)

## S3 method for class 'mboost'
x[i, return = TRUE, ...]
mstop(x) &lt;- value

## S3 method for class 'mboost'
AIC(object, method = c("corrected", "classical", "gMDL"),
    df = c("trace", "actset"), ..., k = 2)

## S3 method for class 'mboost'
mstop(object, ...)
## S3 method for class 'gbAIC'
mstop(object, ...)
## S3 method for class 'cvrisk'
mstop(object, ...)

## S3 method for class 'mboost'
predict(object, newdata = NULL,
        type = c("link", "response", "class"), which = NULL,
        aggregate = c("sum", "cumsum", "none"), ...)
## S3 method for class 'glmboost'
predict(object, newdata = NULL,
        type = c("link", "response", "class"), which = NULL,
        aggregate = c("sum", "cumsum", "none"), ...)

## S3 method for class 'mboost'
fitted(object, ...)

## S3 method for class 'mboost'
residuals(object, ...)
## S3 method for class 'mboost'
resid(object, ...)

## S3 method for class 'glmboost'
variable.names(object, which = NULL, usedonly = FALSE, ...)
## S3 method for class 'mboost'
variable.names(object, which = NULL, usedonly = FALSE, ...)

## S3 method for class 'mboost'
extract(object, what = c("design", "penalty", "lambda", "df",
                         "coefficients", "residuals",
                         "variable.names", "bnames", "offset",
                         "nuisance", "weights", "index", "control"),
        which = NULL, ...)
## S3 method for class 'glmboost'
extract(object, what = c("design", "coefficients", "residuals",
                         "variable.names", "offset",
                         "nuisance", "weights", "control"),
        which = NULL, asmatrix = FALSE, ...)
## S3 method for class 'blg'
extract(object, what = c("design", "penalty", "index"),
        asmatrix = FALSE, expand = FALSE, ...)

## S3 method for class 'mboost'
logLik(object, ...)
## S3 method for class 'gamboost'
hatvalues(model, ...)
## S3 method for class 'glmboost'
hatvalues(model, ...)

## S3 method for class 'mboost'
selected(object, ...)

## S3 method for class 'mboost'
risk(object, ...)

## S3 method for class 'mboost'
nuisance(object)

downstream.test(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="methods_+3A_object">object</code></td>
<td>
<p> objects of class <code>glmboost</code>, <code>gamboost</code>,
<code>blackboost</code> or <code>gbAIC</code>. </p>
</td></tr>
<tr><td><code id="methods_+3A_x">x</code></td>
<td>
<p> objects of class <code>glmboost</code> or <code>gamboost</code>. </p>
</td></tr>
<tr><td><code id="methods_+3A_model">model</code></td>
<td>
<p>objects of class mboost</p>
</td></tr>
<tr><td><code id="methods_+3A_newdata">newdata</code></td>
<td>
<p> optionally, a data frame in which to look for variables with
which to predict. In case the model was fitted using the <code>matrix</code>
interface to <code><a href="#topic+glmboost">glmboost</a></code>, <code>newdata</code> must be a <code>matrix</code>
as well (an error is given otherwise).</p>
</td></tr>
<tr><td><code id="methods_+3A_which">which</code></td>
<td>
<p> a subset of base-learners to take into account for computing
predictions or coefficients. If <code>which</code> is given
(as an integer vector or characters corresponding
to base-learners) a list or matrix is returned.</p>
</td></tr>
<tr><td><code id="methods_+3A_usedonly">usedonly</code></td>
<td>
<p> logical. Indicating whether all variable names should
be returned or only those selected in the boosting algorithm.</p>
</td></tr>
<tr><td><code id="methods_+3A_type">type</code></td>
<td>
<p> the type of prediction required.  The default is on the scale
of the predictors; the alternative <code>"response"</code> is on
the scale of the response variable.  Thus for a
binomial model the default predictions are of log-odds
(probabilities on logit scale) and <code>type = "response"</code> gives
the predicted probabilities.  The <code>"class"</code> option returns
predicted classes.</p>
</td></tr>
<tr><td><code id="methods_+3A_aggregate">aggregate</code></td>
<td>
<p> a character specifying how to aggregate predictions
or coefficients of single base-learners. The default
returns the prediction or coefficient for the final number of
boosting iterations. <code>"cumsum"</code> returns a
list with matrices (one per base-learner) with the
cumulative coefficients for all iterations
simultaneously (in columns). <code>"none"</code> returns a
list of matrices where the <code class="reqn">j</code>th columns of the
respective matrix contains the predictions
of the base-learner of the <code class="reqn">j</code>th boosting
iteration (and zero if the base-learner is not
selected in this iteration).</p>
</td></tr>
<tr><td><code id="methods_+3A_off2int">off2int</code></td>
<td>
<p> logical. Indicating whether the offset should be
added to the intercept (if there is any)
or if the offset is returned as attribute of
the coefficient (default).</p>
</td></tr>
<tr><td><code id="methods_+3A_i">i</code></td>
<td>
<p> integer. Index specifying the model to extract. If <code>i = 0</code>, 
the offset model is returned. If <code>i</code> is smaller than the initial 
<code>mstop</code>, a subset is used. If <code>i</code> is larger than the 
initial <code>mstop</code>, additional boosting steps are performed until 
step <code>i</code> is reached. See details for more information. </p>
</td></tr>
<tr><td><code id="methods_+3A_value">value</code></td>
<td>
<p> integer. See <code>i</code>.</p>
</td></tr>
<tr><td><code id="methods_+3A_return">return</code></td>
<td>
<p> a logical indicating whether the changed object is
returned. </p>
</td></tr>
<tr><td><code id="methods_+3A_method">method</code></td>
<td>
<p> a character specifying if the corrected AIC criterion or
a classical (-2 logLik + k * df) should be computed.</p>
</td></tr>
<tr><td><code id="methods_+3A_df">df</code></td>
<td>
<p> a character specifying how degrees of freedom should be computed:
<code>trace</code> defines degrees of freedom by the trace of the
boosting hat matrix and <code>actset</code> uses the number of
non-zero coefficients for each boosting iteration.</p>
</td></tr>
<tr><td><code id="methods_+3A_k">k</code></td>
<td>
<p>  numeric, the <em>penalty</em> per parameter to be used; the default
<code>k = 2</code> is the classical AIC. Only used when
<code>method = "classical"</code>.</p>
</td></tr>
<tr><td><code id="methods_+3A_what">what</code></td>
<td>
<p> a character specifying the quantities to <code>extract</code>.
Depending on <code>object</code> this can be a subset of
<code>"design"</code> (default; design matrix),
<code>"penalty"</code> (penalty matrix),
<code>"lambda"</code> (smoothing parameter), <code>"df"</code>
(degrees of freedom), <code>"coefficients"</code>,
<code>"residuals"</code>, <code>"variable.names"</code>,
<code>"bnames"</code> (names of the base-learners),
<code>"offset"</code>, <code>"nuisance"</code>, <code>"weights"</code>,
<code>"index"</code> (index of ties used to expand the design
matrix) and <code>"control"</code>. In future versions additional
extractors might be specified.</p>
</td></tr>
<tr><td><code id="methods_+3A_asmatrix">asmatrix</code></td>
<td>
<p> a logical indicating whether the the returned
matrix should be coerced to a matrix (default) or if the
returned object stays as it is (i.e., potentially a
<em>sparse</em> matrix). This option is only applicable if
<code>extract</code> returns matrices, i.e., <code>what = "design"</code> or
<code>what = "penalty"</code>. </p>
</td></tr>
<tr><td><code id="methods_+3A_expand">expand</code></td>
<td>
<p> a logical indicating whether the design matrix should
be expanded (default: <code>FALSE</code>). This is useful if  ties
where taken into account either manually (via argument
<code>index</code> in a base-learner) or automatically for data sets
with many observations. <code>expand = TRUE</code> is equivalent to
<code>extract(B)[extract(B, what = "index"),]</code> for a
base-learner <code>B</code>.</p>
</td></tr>
<tr><td><code id="methods_+3A_...">...</code></td>
<td>
<p> additional arguments passed to callies. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>These functions can be used to extract details from fitted models.
<code>print</code> shows a dense representation of the model fit and
<code>summary</code> gives a more detailed representation.
</p>
<p>The function <code>coef</code> extracts the regression coefficients of a
linear model fitted using the <code><a href="#topic+glmboost">glmboost</a></code> function or an
additive model fitted using the <code><a href="#topic+gamboost">gamboost</a></code>. Per default,
only coefficients of selected base-learners are returned. However, any
desired coefficient can be extracted using the <code>which</code> argument
(see examples for details). Per default, the coefficient of the final
iteration is returned (<code>aggregate = "sum"</code>) but it is also
possible to return the coefficients from all iterations simultaniously
(<code>aggregate = "cumsum"</code>). If <code>aggregate = "none"</code> is
specified, the coefficients of the <em>selected</em> base-learners are
returned (see examples below).
For models fitted via <code><a href="#topic+glmboost">glmboost</a></code> with option <code>center
  = TRUE</code> the intercept is rarely selected. However, it is implicitly
estimated through the centering of the design matrix. In this case the
intercept is always returned except <code>which</code> is specified such
that the intercept is not selected. See examples below.
</p>
<p>The <code>predict</code> function can be used to predict the status of the
response variable for new observations whereas <code>fitted</code> extracts
the regression fit for the observations in the learning sample. For
<code>predict</code> <code>newdata</code> can be specified, otherwise the fitted
values are returned. If <code>which</code> is specified, marginal effects of
the corresponding base-learner(s) are returned. The argument
<code>type</code> can be used to make predictions on the scale of the
<code>link</code> (i.e., the linear predictor <code class="reqn">X\beta</code>),
the <code>response</code> (i.e. <code class="reqn">h(X\beta)</code>, where h is the
response function) or the <code>class</code> (in case of
classification). Furthermore, the predictions can be aggregated
analogously to <code>coef</code> by setting <code>aggregate</code> to either
<code>sum</code> (default; predictions of the final iteration are given),
<code>cumsum</code> (predictions of all iterations are returned
simultaniously) or <code>none</code> (change of prediction in each
iteration). If applicable the <code>offset</code> is added to the predictions.
If marginal predictions are requested the <code>offset</code> is attached
to the object via <code>attr(..., "offset")</code> as adding the offset to
one of the marginal predictions doesn't make much sense.
</p>
<p>The <code>[.mboost</code> function can be used to enhance or restrict a given
boosting model to the specified boosting iteration <code>i</code>. Note that
in both cases the original <code>x</code> will be changed to reduce the
memory footprint. If the boosting model is enhanced by specifying an
index that is larger than the initial <code>mstop</code>, only the missing
<code>i - mstop</code> steps are fitted. If the model is restricted, the
spare steps are not dropped, i.e., if we increase <code>i</code> again,
these boosting steps are immediately available. Alternatively, the
same operation can be done by <code>mstop(x) &lt;- i</code>.
</p>
<p>The <code>residuals</code> function can be used to extract the residuals
(i.e., the negative gradient of the current iteration). <code>resid</code>
is is an alias for <code>residuals</code>.
</p>
<p>Variable names (including those of interaction effects specified via
<code>by</code> in a base-learner) can be extracted using the generic
function <code>variable.names</code>, which has special methods for boosting
objects.
</p>
<p>The generic <code>extract</code> function can be used to extract various
characteristics of a fitted model or a base-learner. Note that the
sometimes a penalty function is returned (e.g. by
<code>extract(bols(x), what = "penalty")</code>) even if the estimation is
unpenalized. However, in this case the penalty paramter <code>lambda</code>
is set to zero. If a matrix is returned by <code>extract</code> one can to
set <code>asmatrix = TRUE</code> if the returned matrix should be coerced to
class <code>matrix</code>. If <code>asmatrix = FALSE</code> one might get a sparse
matrix as implemented in package <code>Matrix</code>. If one requests the
design matrix (<code>what = "design"</code>) <code>expand = TRUE</code> expands
the resulting matrix by taking the duplicates handeled via
<code>index</code> into account.
</p>
<p>The ids of base-learners selected during the fitting process can be
extracted using <code>selected()</code>. The <code>nuisance()</code> method
extracts nuisance parameters from the fit that are handled internally
by the corresponding family object, see
<code>"<a href="#topic+boost_family-class">boost_family</a>"</code>. The <code>risk()</code> function can be
used to extract the computed risk (either the <code>"inbag"</code> risk or
the <code>"oobag"</code> risk, depending on the control argument; see
<code><a href="#topic+boost_control">boost_control</a></code>).
</p>
<p>For (generalized) linear and additive models, the <code>AIC</code> function
can be used to compute both the classical AIC (only available for
<code>familiy = Binomial()</code> and <code>familiy = Poisson()</code>) and
corrected AIC (Hurvich et al., 1998, only available when <code>family
  = Gaussian()</code> was used). Details on the used approximations for the
hat matrix can be found in Buehlmann and Hothorn (2007). The AIC is
useful for the determination of the optimal number of boosting
iterations to be applied (which can be extracted via <code>mstop</code>).
The degrees of freedom are either computed via the trace of the
boosting hat matrix (which is rather slow even for moderate sample
sizes) or the number of variables (non-zero coefficients) that entered
the model so far (faster but only meaningful for linear models fitted
via <code><a href="#topic+gamboost">gamboost</a></code> (see Hastie, 2007)). For a discussion of
the use of AIC based stopping see also Mayr, Hofner and Schmid (2012).
</p>
<p>In addition, the general Minimum Description Length criterion
(Buehlmann and Yu, 2006) can be computed using function <code>AIC</code>.
</p>
<p>Note that <code>logLik</code> and <code>AIC</code> only make sense when the
corresponding <code><a href="#topic+Family">Family</a></code> implements the appropriate loss
function.
</p>
<p><code>downstream.test</code> computes tests for linear models fitted via <code><a href="#topic+glmboost">glmboost</a></code>
with a likelihood based loss function and only suitable without early stopping, i.e., 
if likelihood based model converged. In order to work, the Fisher matrix must
be implemented in the <code><a href="#topic+Family">Family</a></code>; currently this is only the case for 
family <code><a href="#topic+RCG">RCG</a></code>.
</p>


<h3>Warning</h3>

<p>The coefficients resulting from boosting with family
<code>Binomial(link = "logit")</code> are <code class="reqn">1/2</code> of the coefficients of a logit  model obtained via <code><a href="stats.html#topic+glm">glm</a></code> (see <code><a href="#topic+Binomial">Binomial</a></code>).
</p>


<h3>Note</h3>

<p>The <code>[.mboost</code> function changes the original object, i.e.
<code>gbmodel[10]</code> changes <code>gbmodel</code> directly!
</p>


<h3>References</h3>

<p>Benjamin Hofner, Andreas Mayr, Nikolay Robinzonov and Matthias Schmid
(2014). Model-based Boosting in R: A Hands-on Tutorial Using the R
Package mboost. <em>Computational Statistics</em>, <b>29</b>, 3&ndash;35.<br />
<a href="https://doi.org/10.1007/s00180-012-0382-5">doi:10.1007/s00180-012-0382-5</a>
</p>
<p>Clifford M. Hurvich, Jeffrey S. Simonoff and Chih-Ling Tsai (1998),
Smoothing parameter selection in nonparametric regression using
an improved Akaike information criterion.
<em>Journal of the Royal Statistical Society, Series B</em>,
<b>20</b>(2), 271&ndash;293.
</p>
<p>Peter Buehlmann and Torsten Hothorn (2007),
Boosting algorithms: regularization, prediction and model fitting.
<em>Statistical Science</em>, <b>22</b>(4), 477&ndash;505.
</p>
<p>Trevor Hastie (2007), Discussion of &ldquo;Boosting algorithms:
Regularization, prediction and model fitting&rdquo; by Peter Buehlmann and
Torsten Hothorn. <em>Statistical Science</em>, <b>22</b>(4), 505.
</p>
<p>Peter Buehlmann and Bin Yu (2006), Sparse boosting. <em>Journal of
Machine Learning Research</em>, <b>7</b>, 1001&ndash;1024.
</p>
<p>Andreas Mayr, Benjamin Hofner, and Matthias Schmid (2012). The
importance of knowing when to stop - a sequential stopping rule for
component-wise gradient boosting. <em>Methods of Information in
Medicine</em>, <b>51</b>, 178&ndash;186. <br />
DOI: <a href="https://doi.org/10.3414/ME11-02-0030">doi:10.3414/ME11-02-0030</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gamboost">gamboost</a></code>, <code><a href="#topic+glmboost">glmboost</a></code> and
<code><a href="#topic+blackboost">blackboost</a></code> for model fitting.
</p>
<p><code><a href="#topic+plot.mboost">plot.mboost</a></code> for plotting methods.
</p>
<p><code><a href="#topic+cvrisk">cvrisk</a></code> for cross-validated stopping iteration.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
  ### a simple two-dimensional example: cars data
  cars.gb &lt;- glmboost(dist ~ speed, data = cars,
                      control = boost_control(mstop = 2000),
                      center = FALSE)
  cars.gb

  ### initial number of boosting iterations
  mstop(cars.gb)

  ### AIC criterion
  aic &lt;- AIC(cars.gb, method = "corrected")
  aic

  ### extract coefficients for glmboost
  coef(cars.gb)
  coef(cars.gb, off2int = TRUE)        # offset added to intercept
  coef(lm(dist ~ speed, data = cars))  # directly comparable

  cars.gb_centered &lt;- glmboost(dist ~ speed, data = cars,
                               center = TRUE)
  selected(cars.gb_centered)           # intercept never selected
  coef(cars.gb_centered)               # intercept implicitly estimated
                                       # and thus returned
  ## intercept is internally corrected for mean-centering
  - mean(cars$speed) * coef(cars.gb_centered, which="speed") # = intercept
  # not asked for intercept thus not returned
  coef(cars.gb_centered, which="speed")
  # explicitly asked for intercept
  coef(cars.gb_centered, which=c("Intercept", "speed"))

  ### enhance or restrict model
  cars.gb &lt;- gamboost(dist ~ speed, data = cars,
                      control = boost_control(mstop = 100, trace = TRUE))
  cars.gb[10]
  cars.gb[100, return = FALSE] # no refitting required
  cars.gb[150, return = FALSE] # only iterations 101 to 150
                               # are newly fitted

  ### coefficients for optimal number of boosting iterations
  coef(cars.gb[mstop(aic)])
  plot(cars$dist, predict(cars.gb[mstop(aic)]),
       ylim = range(cars$dist))
  abline(a = 0, b = 1)

  ### example for extraction of coefficients
  set.seed(1907)
  n &lt;- 100
  x1 &lt;- rnorm(n)
  x2 &lt;- rnorm(n)
  x3 &lt;- rnorm(n)
  x4 &lt;- rnorm(n)
  int &lt;- rep(1, n)
  y &lt;- 3 * x1^2 - 0.5 * x2 + rnorm(n, sd = 0.1)
  data &lt;- data.frame(y = y, int = int, x1 = x1, x2 = x2, x3 = x3, x4 = x4)

  model &lt;- gamboost(y ~ bols(int, intercept = FALSE) +
                        bbs(x1, center = TRUE, df = 1) +
                        bols(x1, intercept = FALSE) +
                        bols(x2, intercept = FALSE) +
                        bols(x3, intercept = FALSE) +
                        bols(x4, intercept = FALSE),
                    data = data, control = boost_control(mstop = 500))

  coef(model) # standard output (only selected base-learners)
  coef(model,
       which = 1:length(variable.names(model))) # all base-learners
  coef(model, which = "x1") # shows all base-learners for x1

  cf1 &lt;- coef(model, which = c(1,3,4), aggregate = "cumsum")
  tmp &lt;- sapply(cf1, function(x) x)
  matplot(tmp, type = "l", main = "Coefficient Paths")

  cf1_all &lt;- coef(model, aggregate = "cumsum")
  cf1_all &lt;- lapply(cf1_all, function(x) x[, ncol(x)]) # last element
  ## same as coef(model)

  cf2 &lt;- coef(model, aggregate = "none")
  cf2 &lt;- lapply(cf2, rowSums) # same as coef(model)

  ### example continued for extraction of predictions

  yhat &lt;- predict(model) # standard prediction; here same as fitted(model)
  p1 &lt;- predict(model, which = "x1") # marginal effects of x1
  orderX &lt;- order(data$x1)
  ## rowSums needed as p1 is a matrix
  plot(data$x1[orderX], rowSums(p1)[orderX], type = "b")

  ## better: predictions on a equidistant grid
  new_data &lt;- data.frame(x1 = seq(min(data$x1), max(data$x1), length = 100))
  p2 &lt;- predict(model, newdata = new_data, which = "x1")
  lines(new_data$x1, rowSums(p2), col = "red")

  ### extraction of model characteristics
  extract(model, which = "x1")  # design matrices for x1
  extract(model, what = "penalty", which = "x1") # penalty matrices for x1
  extract(model, what = "lambda", which = "x1") # df and corresponding lambda for x1
       ## note that bols(x1, intercept = FALSE) is unpenalized

  extract(model, what = "bnames")  ## name of complete base-learner
  extract(model, what = "variable.names") ## only variable names
  variable.names(model)            ## the same

  ### extract from base-learners
  extract(bbs(x1), what = "design")
  extract(bbs(x1), what = "penalty")
  ## weights and lambda can only be extracted after using dpp
  weights &lt;- rep(1, length(x1))
  extract(bbs(x1)$dpp(weights), what = "lambda")
</code></pre>

<hr>
<h2 id='plot'>
Plot effect estimates of boosting models
</h2><span id='topic+plot'></span><span id='topic+plot.glmboost'></span><span id='topic+plot.mboost'></span><span id='topic+lines.mboost'></span>

<h3>Description</h3>

<p>Plot coefficient plots for <code>glmboost</code> models and partial effect
plots for all other <code>mboost</code> models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
## S3 method for class 'glmboost'
plot(x, main = deparse(x$call), col = NULL,
     off2int = FALSE, ...)

## S3 method for class 'mboost'
plot(x, which = NULL, newdata = NULL,
     type = "b", rug = TRUE, rugcol = "black",
     ylim = NULL, xlab = NULL, ylab = expression(f[partial]),
     add = FALSE, ...)

## S3 method for class 'mboost'
lines(x, which = NULL, type = "l", rug = FALSE, ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_+3A_x">x</code></td>
<td>

<p>object of class <code>glmboost</code> or an object inheriting from
<code>mboost</code> for plotting.
</p>
</td></tr>
<tr><td><code id="plot_+3A_main">main</code></td>
<td>

<p>a title for the plot.
</p>
</td></tr>
<tr><td><code id="plot_+3A_col">col</code></td>
<td>

<p>(a vector of) colors for plotting the lines representing the
coefficient paths.
</p>
</td></tr>
<tr><td><code id="plot_+3A_off2int">off2int</code></td>
<td>

<p>logical indicating whether the offset should be added to the
intercept (if there is any) or if the offset is neglected for
plotting (default).
</p>
</td></tr>
<tr><td><code id="plot_+3A_which">which</code></td>
<td>

<p>a subset of base-learners used for plotting. If <code>which</code> is
given (as an integer vector or characters corresponding
to base-learners) only the corresponding partial effect plots are
depicted. Per default all selected base-learners are plotted.
</p>
</td></tr>
<tr><td><code id="plot_+3A_newdata">newdata</code></td>
<td>

<p>optionally, a data frame in which to look for variables with
which to make predictions that are then plotted. This is especially
useful if the data that was used to fit the model shows some larger
gaps as effect plots are linearly interpolated between observations.
For an example using <code>newdata</code> see below.
</p>
</td></tr>
<tr><td><code id="plot_+3A_type">type</code></td>
<td>

<p>character string giving the type of plot desired. Per default,
points and lines are plotted (<code>"b"</code>). Other useful options are
points (<code>"p"</code>) or lines (<code>"l"</code>). See
<code><a href="graphics.html#topic+plot.default">plot.default</a></code> for details.
</p>
</td></tr>
<tr><td><code id="plot_+3A_rug">rug</code></td>
<td>

<p>logical. Should a rug be added to the x-axis?
</p>
</td></tr>
<tr><td><code id="plot_+3A_rugcol">rugcol</code></td>
<td>

<p>color for the rug.
</p>
</td></tr>
<tr><td><code id="plot_+3A_ylim">ylim</code></td>
<td>

<p>the y limits of the plot.
</p>
</td></tr>
<tr><td><code id="plot_+3A_xlab">xlab</code></td>
<td>

<p>a label for the x axis.
</p>
</td></tr>
<tr><td><code id="plot_+3A_ylab">ylab</code></td>
<td>

<p>a label for the y axis.
</p>
</td></tr>
<tr><td><code id="plot_+3A_add">add</code></td>
<td>

<p>logical. Should the plot be added to the previous plot?
</p>
</td></tr>
<tr><td><code id="plot_+3A_...">...</code></td>
<td>

<p>Additional arguments to the <code>plot</code> functions. E.g. one can
specify the x limits <code>xlim</code> or the color of the plot using
<code>col</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The coefficient paths for <code>glmboost</code> models show how the
coefficient estimates evolve with increasing <code>mstop</code>. Each line
represents one parameter estimate. Parameter estimates are only
depicted when they they are selected at any time in the boosting
model. Parameters that are not selected are droped from the figure
(see example).
</p>
<p>Models specified with <code>gamboost</code> or <code>mboost</code> are plotted as
partial effects. Only the effect of the current bossting iteration is
depicted instead of the coefficient paths as for linear models. The
function <code>lines</code> is just a wrapper to <code>plot(... , add =
    TRUE)</code> where per default the effect is plotted as line and the
<code>rug</code> is set to <code>FALSE</code>.
</p>
<p>Spatial effects can be also plotted using the function <code>plot</code>
for mboost models (using <code>lattice</code> graphics). More complex
effects reuquire manual plotting: One needs to predict the effects on
a disired grid and plot the effect estimates.
</p>


<h3>Value</h3>

<p>A plot of the fitted model.
</p>


<h3>References</h3>

<p>Benjamin Hofner, Andreas Mayr, Nikolay Robinzonov and Matthias Schmid
(2014). Model-based Boosting in R: A Hands-on Tutorial Using the R
Package mboost. <em>Computational Statistics</em>, <b>29</b>, 3&ndash;35.<br />
<a href="https://doi.org/10.1007/s00180-012-0382-5">doi:10.1007/s00180-012-0382-5</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mboost_methods">mboost_methods</a></code> for further methods.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
### a simple example: cars data with one random variable
set.seed(1234)
cars$z &lt;- rnorm(50)

########################################
## Plot linear models
########################################

## fit a linear model
cars.lm &lt;- glmboost(dist ~ speed + z, data = cars)

## plot coefficient paths of glmboost
par(mfrow = c(3, 1), mar = c(4, 4, 4, 8))
plot(cars.lm,
     main = "Coefficient paths (offset not included)")
plot(cars.lm, off2int = TRUE,
     main = "Coefficient paths (offset included in intercept)")

## plot coefficient paths only for the first 15 steps,
## i.e., bevore z is selected
mstop(cars.lm) &lt;- 15
plot(cars.lm, off2int = TRUE, main = "z is not yet selected")


########################################
## Plot additive models; basics
########################################

## fit an additive model
cars.gam &lt;- gamboost(dist ~ speed + z, data = cars)

## plot effects
par(mfrow = c(1, 2), mar = c(4, 4, 0.1, 0.1))
plot(cars.gam)

## use same y-lims
plot(cars.gam, ylim = c(-50, 50))

## plot only the effect of speed
plot(cars.gam, which = "speed")
## as partial matching is used we could also use
plot(cars.gam, which = "sp")


########################################
## More complex plots
########################################

## Let us use more boosting iterations and compare the effects.

## We change the plot type and plot both effects in one figure:
par(mfrow = c(1, 1), mar = c(4, 4, 4, 0.1))
mstop(cars.gam) &lt;- 100
plot(cars.gam, which = 1, col = "red", type = "l", rug = FALSE,
     main = "Compare effect for various models")

## Now the same model with 1000 iterations
mstop(cars.gam) &lt;- 1000
lines(cars.gam, which = 1, col = "grey", lty = "dotted")

## There are some gaps in the data. Use newdata to get a smoother curve:
newdata &lt;- data.frame(speed = seq(min(cars$speed), max(cars$speed),
                                  length = 200))
lines(cars.gam, which = 1, col = "grey", lty = "dashed",
      newdata = newdata)

## The model with 1000 steps seems to overfit the data.
## Usually one should use e.g. cross-validation to tune the model.

## Finally we refit the model using linear effects as comparison
cars.glm &lt;- gamboost(dist ~ speed + z, baselearner = bols, data = cars)
lines(cars.glm, which = 1, col = "black")
## We see that all effects are more or less linear.

## Add a legend
legend("topleft", title = "Model",
       legend = c("... with mstop = 100", "... with mstop = 1000",
         "... with linear effects"),
       lty = c("solid", "dashed", "solid"),
       col = c("red", "grey", "black"))

</code></pre>

<hr>
<h2 id='stabsel'>
Stability Selection
</h2><span id='topic+stabsel'></span><span id='topic+stabsel.mboost'></span><span id='topic+stabsel_parameters.mboost'></span>

<h3>Description</h3>

<p>Selection of influential variables or model components with error control.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## a method to compute stability selection paths for fitted mboost models
## S3 method for class 'mboost'
stabsel(x, cutoff, q, PFER, grid = 0:mstop(x),
        folds = subsample(model.weights(x), B = B),
        B = ifelse(sampling.type == "MB", 100, 50),
        assumption = c("unimodal", "r-concave", "none"),
        sampling.type = c("SS", "MB"),
        papply = mclapply, verbose = TRUE, FWER, eval = TRUE, ...)

## just a wrapper to stabsel(p, ..., eval = FALSE)
## S3 method for class 'mboost'
stabsel_parameters(p, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stabsel_+3A_x">x</code>, <code id="stabsel_+3A_p">p</code></td>
<td>
<p>an fitted model of class <code>"mboost"</code>.</p>
</td></tr>
<tr><td><code id="stabsel_+3A_cutoff">cutoff</code></td>
<td>
<p>cutoff between 0.5 and 1. Preferably a value between 0.6
and 0.9 should be used.</p>
</td></tr>
<tr><td><code id="stabsel_+3A_q">q</code></td>
<td>
<p>number of (unique) selected variables (or groups of variables
depending on the model) that are selected on each subsample.</p>
</td></tr>
<tr><td><code id="stabsel_+3A_pfer">PFER</code></td>
<td>
<p>upper bound for the per-family error rate. This
specifies the amount of falsely selected base-learners, which is
tolerated. See details.</p>
</td></tr>
<tr><td><code id="stabsel_+3A_grid">grid</code></td>
<td>
<p> a numeric vector of the form <code>0:m</code>. See also <code><a href="#topic+cvrisk">cvrisk</a></code>. </p>
</td></tr>   
<tr><td><code id="stabsel_+3A_folds">folds</code></td>
<td>
<p> a weight matrix with number of rows equal to the number
of observations, see <code><a href="#topic+cvrisk">cvrisk</a></code> and
<code><a href="stabs.html#topic+subsample">subsample</a></code>. Usually one should not
change the default here as subsampling with a fraction of <code class="reqn">1/2</code>
is needed for the error bounds to hold. One usage scenario where
specifying the folds by hand might be the case when one has
dependent data (e.g. clusters) and thus wants to draw clusters
(i.e., multiple rows together) not individuals.</p>
</td></tr>
<tr><td><code id="stabsel_+3A_assumption">assumption</code></td>
<td>
<p> Defines the type of assumptions on the
distributions of the selection probabilities and simultaneous
selection probabilities. Only applicable for
<code>sampling.type = "SS"</code>. For <code>sampling.type = "MB"</code> we
always use <code>"none"</code>.</p>
</td></tr>
<tr><td><code id="stabsel_+3A_sampling.type">sampling.type</code></td>
<td>
<p> use sampling scheme of of Shah &amp; Samworth
(2013), i.e., with complementarty pairs (<code>sampling.type = "SS"</code>),
or the original sampling scheme of Meinshausen &amp; Buehlmann (2010).</p>
</td></tr>
<tr><td><code id="stabsel_+3A_b">B</code></td>
<td>
<p> number of subsampling replicates. Per default, we use 50
complementary pairs for the error bounds of Shah &amp; Samworth (2013)
and 100 for the error bound derived in  Meinshausen &amp; Buehlmann
(2010). As we use <code class="reqn">B</code> complementray pairs in the former case
this leads to <code class="reqn">2B</code> subsamples.</p>
</td></tr>
<tr><td><code id="stabsel_+3A_papply">papply</code></td>
<td>
<p> (parallel) apply function, defaults to
<code><a href="parallel.html#topic+mclapply">mclapply</a></code>. Alternatively, <code>parLapply</code>
can be used. In the latter case, usually more setup is needed (see
example of <code><a href="#topic+cvrisk">cvrisk</a></code> for some details).</p>
</td></tr>
<tr><td><code id="stabsel_+3A_verbose">verbose</code></td>
<td>
<p> logical (default: <code>TRUE</code>) that determines wether
<code>warnings</code> should be issued. </p>
</td></tr>
<tr><td><code id="stabsel_+3A_fwer">FWER</code></td>
<td>
<p> deprecated. Only for compatibility with older versions,
use PFER instead.</p>
</td></tr>
<tr><td><code id="stabsel_+3A_eval">eval</code></td>
<td>
<p> logical. Determines whether stability selection is
evaluated (<code>eval = TRUE</code>; default) or if only the parameter
combination is returned.</p>
</td></tr>
<tr><td><code id="stabsel_+3A_...">...</code></td>
<td>
<p> additional arguments to parallel apply methods such as
<code><a href="parallel.html#topic+mclapply">mclapply</a></code> and to <code><a href="#topic+cvrisk">cvrisk</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For details see <code><a href="stabs.html#topic+stabsel">stabsel</a></code> in package <span class="pkg">stabs</span>
and Hofner et al. (2015).
</p>


<h3>Value</h3>

<p>An object of class <code>stabsel</code> with a special <code>print</code> method.
The object has the following elements:
</p>
<table>
<tr><td><code>phat</code></td>
<td>
<p>selection probabilities.</p>
</td></tr>
<tr><td><code>selected</code></td>
<td>
<p>elements with maximal selection probability greater
<code>cutoff</code>.</p>
</td></tr>
<tr><td><code>max</code></td>
<td>
<p>maximum of selection probabilities.</p>
</td></tr>
<tr><td><code>cutoff</code></td>
<td>
<p>cutoff used.</p>
</td></tr>
<tr><td><code>q</code></td>
<td>
<p>average number of selected variables used.</p>
</td></tr>
<tr><td><code>PFER</code></td>
<td>
<p>per-family error rate.</p>
</td></tr>
<tr><td><code>sampling.type</code></td>
<td>
<p>the sampling type used for stability selection.</p>
</td></tr>
<tr><td><code>assumption</code></td>
<td>
<p>the assumptions made on the selection
probabilities.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>the call.</p>
</td></tr>
</table>


<h3>References</h3>

<p>B. Hofner, L. Boccuto and M. Goeker (2015),
Controlling false discoveries in high-dimensional situations: Boosting
with stability selection. <em>BMC Bioinformatics</em>, <b>16:144</b>.
</p>
<p>N. Meinshausen and P. Buehlmann (2010), Stability selection.
<em>Journal of the Royal Statistical Society, Series B</em>,
<b>72</b>, 417&ndash;473.
</p>
<p>R.D. Shah and R.J. Samworth (2013), Variable selection with error
control: another look at stability selection. <em>Journal of the Royal
Statistical Society, Series B</em>, <b>75</b>, 55&ndash;80.
</p>


<h3>See Also</h3>

<p><code><a href="stabs.html#topic+stabsel">stabsel</a></code> and
<code><a href="stabs.html#topic+stabsel_parameters">stabsel_parameters</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  ## make data set available
  data("bodyfat", package = "TH.data")
  ## set seed
  set.seed(1234)

  ### low-dimensional example
  mod &lt;- glmboost(DEXfat ~ ., data = bodyfat)

  ## compute cutoff ahead of running stabsel to see if it is a sensible
  ## parameter choice.
  ##   p = ncol(bodyfat) - 1 (= Outcome) + 1 ( = Intercept)
  stabsel_parameters(q = 3, PFER = 1, p = ncol(bodyfat) - 1 + 1,
                     sampling.type = "MB")
  ## the same:
  stabsel(mod, q = 3, PFER = 1, sampling.type = "MB", eval = FALSE)

## Not run: ############################################################
## Do not run and check these examples automatically as
## they take some time (~ 10 seconds depending on the system)

  ## now run stability selection
  (sbody &lt;- stabsel(mod, q = 3, PFER = 1, sampling.type = "MB"))
  opar &lt;- par(mai = par("mai") * c(1, 1, 1, 2.7))
  plot(sbody)
  par(opar)

  plot(sbody, type = "maxsel", ymargin = 6)

## End(Not run and test)

## End(Not run)
</code></pre>

<hr>
<h2 id='survFit'> Survival Curves for a Cox Proportional Hazards Model </h2><span id='topic+survFit'></span><span id='topic+survFit.mboost'></span><span id='topic+plot.survFit'></span>

<h3>Description</h3>

<p>Computes the predicted survivor function for a Cox proportional hazards model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'mboost'
survFit(object, newdata = NULL, ...)
## S3 method for class 'survFit'
plot(x, xlab = "Time", ylab = "Probability", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="survFit_+3A_object">object</code></td>
<td>
<p> an object of class <code>mboost</code> which is assumed to have a <code><a href="#topic+CoxPH">CoxPH</a></code>
family component. </p>
</td></tr>
<tr><td><code id="survFit_+3A_newdata">newdata</code></td>
<td>
<p> an optional data frame in which to look for variables with
which to predict the survivor function. </p>
</td></tr>
<tr><td><code id="survFit_+3A_x">x</code></td>
<td>
<p> an object of class <code>survFit</code> for plotting. </p>
</td></tr>
<tr><td><code id="survFit_+3A_xlab">xlab</code></td>
<td>
<p> the label of the x axis. </p>
</td></tr>
<tr><td><code id="survFit_+3A_ylab">ylab</code></td>
<td>
<p> the label of the y axis. </p>
</td></tr>
<tr><td><code id="survFit_+3A_...">...</code></td>
<td>
<p> additional arguments passed to callies.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>newdata = NULL</code>, the survivor function of the Cox proportional
hazards model is computed for the mean of the covariates used in the
<code><a href="#topic+blackboost">blackboost</a></code>, <code><a href="#topic+gamboost">gamboost</a></code>, or <code><a href="#topic+glmboost">glmboost</a></code>
call. The Breslow estimator is used for computing the baseline survivor
function. If <code>newdata</code> is a data frame, the <code><a href="stats.html#topic+predict">predict</a></code> method
of <code>object</code>, along with the Breslow estimator, is used for computing the
predicted survivor function for each row in <code>newdata</code>.
</p>


<h3>Value</h3>

<p>An object of class <code>survFit</code> containing the following components:
</p>
<table>
<tr><td><code>surv</code></td>
<td>
<p> the estimated survival probabilities at the time points
given in <code>time</code>. </p>
</td></tr>
<tr><td><code>time</code></td>
<td>
<p> the time points at which the survivor functions are
evaluated. </p>
</td></tr>
<tr><td><code>n.event</code></td>
<td>
<p> the number of events observed at each time point given
in <code>time</code>.</p>
</td></tr>
</table>


<h3>See Also</h3>

 <p><code><a href="#topic+gamboost">gamboost</a></code>, <code><a href="#topic+glmboost">glmboost</a></code> and
<code><a href="#topic+blackboost">blackboost</a></code> for model fitting.</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library("survival")
data("cancer", package = "survival")


fm &lt;- Surv(futime,fustat) ~ age + resid.ds + rx + ecog.ps
fit &lt;- glmboost(fm, data = ovarian, family = CoxPH(),
    control=boost_control(mstop = 500))

S1 &lt;- survFit(fit)
S1
newdata &lt;- ovarian[c(1,3,12),]
S2 &lt;- survFit(fit, newdata = newdata)
S2

plot(S1)
</code></pre>

<hr>
<h2 id='varimp'> Variable Importance </h2><span id='topic+varimp'></span><span id='topic+varimp.mboost'></span><span id='topic+plot.varimp'></span><span id='topic+as.data.frame.varimp'></span>

<h3>Description</h3>

<p>In-bag risk reduction per base-learner as variable importance for boosting.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'mboost'
varimp(object, ...)

## S3 method for class 'varimp'
plot(x, percent = TRUE, type = c("variable", "blearner"), 
  blorder = c("importance", "alphabetical", "rev_alphabetical", "formula"),
  nbars = 10L, maxchar = 20L, xlab = NULL, ylab = NULL, xlim, auto.key, ...)
## S3 method for class 'varimp'
as.data.frame(x, row.names = NULL, optional = FALSE, ...) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="varimp_+3A_object">object</code></td>
<td>
<p> an object of class <code>mboost</code>.</p>
</td></tr>
<tr><td><code id="varimp_+3A_x">x</code></td>
<td>
<p> an object of class <code>varimp</code>.</p>
</td></tr>
<tr><td><code id="varimp_+3A_percent">percent</code></td>
<td>
<p> logical, indicating whether variable importance should be 
specified in percent.</p>
</td></tr>
<tr><td><code id="varimp_+3A_type">type</code></td>
<td>
<p> a character string specifying whether to draw bars for variables
(<code>"variable"</code>, default) or base-learners (<code>"blearner"</code>) in the 
model (no effect for a glmboost object).</p>
</td></tr>
<tr><td><code id="varimp_+3A_blorder">blorder</code></td>
<td>
<p> a character string specifying the order of the base-learners
in the plot. The default <code>"importance"</code> corresponds to the order of the
base-learner importance, <code>"alphabetical"</code> and <code>"rev_alphabetical"</code> 
to alphabetical order, respectively its reverse, and <code>"formula"</code> to 
their order in the model formula.</p>
</td></tr>
<tr><td><code id="varimp_+3A_nbars">nbars</code></td>
<td>
<p> integer, maximum number of bars to be plotted. If <code>nbars</code> 
is exceeded, least important variables / base-learners are summarized as
&quot;other&quot;.</p>
</td></tr>
<tr><td><code id="varimp_+3A_maxchar">maxchar</code></td>
<td>
<p> integer, maximum number of characters in bar labels.</p>
</td></tr>
<tr><td><code id="varimp_+3A_xlab">xlab</code></td>
<td>
<p>text for the x-axis label. If not set (default is <code>NULL</code>) 
x-axis label is generated automatically depending on argument 
<code>percent</code>.</p>
</td></tr>  
<tr><td><code id="varimp_+3A_ylab">ylab</code></td>
<td>
<p>text for the y-axis label. If not set (default is <code>NULL</code>) 
y-axis label is generated automatically depending on argument <code>type</code>.</p>
</td></tr>
<tr><td><code id="varimp_+3A_xlim">xlim</code></td>
<td>
<p> the x limits of the plot. Defaults are from <code>0</code> to total 
reduction, or from <code>0</code> to <code>1</code> for <code>percent = TRUE</code>. (In case 
of negative risk reductions, default limits are from total negative to total
positve reduction, or the latter normalized by the total absolute reduction 
for <code>percent = TRUE</code>.)</p>
</td></tr>
<tr><td><code id="varimp_+3A_auto.key">auto.key</code></td>
<td>
<p> logical, or a list passed to <code>lattice::barchart</code>. By 
default <code>auto.key=TRUE</code> provides automatically generated legends showing 
the underlying base-learners in the stacked barchart 
(<code>type = "variable"</code>). If there is an unique base-learner for each 
variable(-interaction), <code>auto.key = FALSE</code> is default setting. 
For <code>type = "blearner"</code> the argument has no effect at all.</p>
</td></tr> 
<tr><td><code id="varimp_+3A_...">...</code></td>
<td>
<p> additional arguments passed to <code>lattice::barchart</code>.</p>
</td></tr>
<tr><td><code id="varimp_+3A_row.names">row.names</code></td>
<td>
<p> NULL or a character vector giving the row names for the 
data frame. Missing values are not allowed.</p>
</td></tr>
<tr><td><code id="varimp_+3A_optional">optional</code></td>
<td>
<p>logical. If TRUE, setting row names and converting column 
names (to syntactic names: see make.names) is optional.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function extracts the in-bag risk reductions per boosting step of a 
fitted <code>mboost</code> model and accumulates it individually for each base-learner
contained in the model. This quantifies the individual contribution to risk 
reduction of each base-learner and can thus be used to compare the importance 
of different base-learners or variables in the model. Starting from offset only, 
in each boosting step risk reduction is computed as the difference between 
in-bag risk of the current and the previous model and is accounted for the 
base-learner selected in the particular step.
</p>
<p>The results can be plotted in a bar plot either for the base-learners, or the
variables contained in the model. The bars are ordered according to variable 
importance. If their number  exceeds <code>nbars</code> the least important are 
summarized as &quot;other&quot;. If bars are plotted per  variable, all base-learners 
containing the same variable will be accumulated in a stacked bar. This is of 
use for models including for example seperate base-learners for the linear and 
non-linear part of a covariate effect (see <code>?bbs</code> option 
<code>center=TRUE</code>). However, variable interactions are treated as individual 
variables, as their desired handling might depend on context.
</p>
<p>As a comparison the selection frequencies are added to the respective 
base-learner labels in the plot (rounded to three digits). For stacked bars 
they are ordered accordingly.
</p>


<h3>Value</h3>

<p> An object of class <code>varimp</code> with available <code>plot</code> and
<code>as.data.frame</code> methods. 
</p>
<p>Converting a <code>varimp</code> object results in a <code>data.frame</code> containing the 
risk reductions, selection frequencies and the corresponding base-learner and 
variable names as ordered <code>factors</code> (ordered according to their particular 
importance).</p>


<h3>Author(s)</h3>

<p>Tobias Kuehn (<a href="mailto:tobi.kuehn@gmx.de">tobi.kuehn@gmx.de</a>), 
Almond Stoecker (<a href="mailto:almond.stoecker@gmail.com">almond.stoecker@gmail.com</a>)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(iris)
### glmboost with multiple variables and intercept
iris$setosa &lt;- factor(iris$Species == "setosa")
iris_glm &lt;- glmboost(setosa ~ 1 + Sepal.Width + Sepal.Length + Petal.Width +
                         Petal.Length,
                     data = iris, control = boost_control(mstop = 50), 
                     family = Binomial(link = c("logit")))
varimp(iris_glm)
### importance plot with four bars only
plot(varimp(iris_glm), nbars = 4)

### gamboost with multiple variables
iris_gam &lt;- gamboost(Sepal.Width ~ 
                         bols(Sepal.Length, by = setosa) +
                         bbs(Sepal.Length, by = setosa, center = TRUE) + 
                         bols(Petal.Width) +
                         bbs(Petal.Width, center = TRUE) + 
                         bols(Petal.Length) +
                         bbs(Petal.Length, center = TRUE),
                     data = iris)
varimp(iris_gam)
### stacked importance plot with base-learners in rev. alphabetical order
plot(varimp(iris_gam), blorder = "rev_alphabetical")

### similar ggplot
## Not run:  
library(ggplot2)
ggplot(data.frame(varimp(iris_gam)), aes(variable, reduction, fill = blearner)) + 
    geom_bar(stat = "identity") + coord_flip() 
## End(Not run)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
