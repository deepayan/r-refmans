<!DOCTYPE html><html><head><title>Help for package less</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {less}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#abalone'><p>Abalone Data Set</p></a></li>
<li><a href='#BaseEstimator'><p>BaseEstimator</p></a></li>
<li><a href='#check_is_fitted'><p>Fitting Control</p></a></li>
<li><a href='#comparison_plot'><p>Comparison Plot</p></a></li>
<li><a href='#CoverTree'><p>CoverTree - Nearest Neighbor Search</p></a></li>
<li><a href='#DecisionTreeClassifier'><p>DecisionTreeClassifier</p></a></li>
<li><a href='#DecisionTreeRegressor'><p>DecisionTreeRegressor</p></a></li>
<li><a href='#get_functions'><p>Get Functions</p></a></li>
<li><a href='#HierarchicalClustering'><p>Hierarchical Clustering</p></a></li>
<li><a href='#k_fold_cv'><p>k-Fold Cross Validation</p></a></li>
<li><a href='#KDTree'><p>KDTree - Nearest Neighbor Search</p></a></li>
<li><a href='#KMeans'><p>KMeans Clustering</p></a></li>
<li><a href='#KNeighborsClassifier'><p>KNeighborsClassifier</p></a></li>
<li><a href='#KNeighborsRegressor'><p>KNeighborsRegressor</p></a></li>
<li><a href='#laplacian'><p>Laplacian kernel - L1 norm</p></a></li>
<li><a href='#LESSBase'><p>LESSBase</p></a></li>
<li><a href='#LESSBinaryClassifier'><p>LESSBinaryClassifier</p></a></li>
<li><a href='#LESSClassifier'><p>LESSClassifier</p></a></li>
<li><a href='#LESSRegressor'><p>LESSRegressor</p></a></li>
<li><a href='#LinearRegression'><p>LinearRegression</p></a></li>
<li><a href='#prepareDataset'><p>Prepare a Dataset</p></a></li>
<li><a href='#prepareXset'><p>Prepare a Dataset</p></a></li>
<li><a href='#RandomForestClassifier'><p>RandomForestClassifier</p></a></li>
<li><a href='#RandomForestRegressor'><p>RandomForestRegressor</p></a></li>
<li><a href='#rbf'><p>RBF kernel - L2 norm</p></a></li>
<li><a href='#SklearnEstimator'><p>SklearnEstimator</p></a></li>
<li><a href='#SVC'><p>Support Vector Classification</p></a></li>
<li><a href='#SVR'><p>Support Vector Regression</p></a></li>
<li><a href='#synthetic_sine_curve'><p>Synthetic Sine Curve</p></a></li>
<li><a href='#test_timing'><p>Compare Fitting Time</p></a></li>
<li><a href='#train_test_split'><p>Dataset splitting</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Learning with Subset Stacking</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.0</td>
</tr>
<tr>
<td>Description:</td>
<td>"Learning with Subset Stacking" is a supervised learning algorithm that is based on training many local estimators on subsets of a given dataset, and then passing their predictions to a global estimator. You can find the details about LESS in our manuscript at &lt;<a href="https://doi.org/10.48550/arXiv.2112.06251">doi:10.48550/arXiv.2112.06251</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.1</td>
</tr>
<tr>
<td>Imports:</td>
<td>caret, e1071, FNN, MLmetrics, pracma, R6, randomForest, RANN,
rpart, wordspace</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10)</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-09-27 10:19:15 UTC; ozerc</td>
</tr>
<tr>
<td>Author:</td>
<td>Ilker Birbil [aut],
  Burhan Ozer Cavdar [aut, trl, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Burhan Ozer Cavdar &lt;bcavdar17@ku.edu.tr&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-09-27 11:00:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='abalone'>Abalone Data Set</h2><span id='topic+abalone'></span>

<h3>Description</h3>

<p>The number of rings is the value to predict.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>abalone
</code></pre>


<h3>Format</h3>

<p>A dataframe with 4177 rows and 8 variables
</p>

<dl>
<dt>Length</dt><dd><p>Longest shell measurement in mm</p>
</dd>
<dt>Diameter</dt><dd><p>perpendicular to length in mm</p>
</dd>
<dt>Height</dt><dd><p>with meat in shell in mm</p>
</dd>
<dt>Whole weight</dt><dd><p>whole abalone in grams</p>
</dd>
<dt>Shucked weight</dt><dd><p>weight of meat in grams</p>
</dd>
<dt>Viscera weight</dt><dd><p>gut weight (after bleeding) in grams</p>
</dd>
<dt>Shell weight</dt><dd><p>after being dried in grams</p>
</dd>
<dt>Rings</dt><dd><p>+1.5 gives the age in years</p>
</dd>
</dl>



<h3>Source</h3>

<p>Dua, D. and Graff, C. (2019). UCI Machine Learning Repository <a href="http://archive.ics.uci.edu/ml/">http://archive.ics.uci.edu/ml/</a>. Irvine, CA: University of California, School of Information and Computer Science.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(abalone)
</code></pre>

<hr>
<h2 id='BaseEstimator'>BaseEstimator</h2><span id='topic+BaseEstimator'></span>

<h3>Description</h3>

<p>A dummy base R6 class that provides get_all_fields, get_attributes and set_random_state functionalities for estimators
</p>


<h3>Value</h3>

<p>R6 Class of BaseEstimator
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-BaseEstimator-get_all_fields"><code>BaseEstimator$get_all_fields()</code></a>
</p>
</li>
<li> <p><a href="#method-BaseEstimator-get_attributes"><code>BaseEstimator$get_attributes()</code></a>
</p>
</li>
<li> <p><a href="#method-BaseEstimator-set_random_state"><code>BaseEstimator$set_random_state()</code></a>
</p>
</li>
<li> <p><a href="#method-BaseEstimator-clone"><code>BaseEstimator$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-BaseEstimator-get_all_fields"></a>



<h4>Method <code>get_all_fields()</code></h4>

<p>Auxiliary function returning the name of all private and public fields of the self class
</p>


<h5>Usage</h5>

<div class="r"><pre>BaseEstimator$get_all_fields()</pre></div>



<h5>Examples</h5>

<div class="r example copy">
<pre>TestClass &lt;- R6::R6Class(classname = "TestClass",
inherit = BaseEstimator,
private = list(random_state = NULL))
exampleClass &lt;- TestClass$new()
exampleClass$get_all_fields()
</pre>
</div>


<hr>
<a id="method-BaseEstimator-get_attributes"></a>



<h4>Method <code>get_attributes()</code></h4>

<p>Auxiliary function returning the name and values of all private and public fields of the self class
</p>


<h5>Usage</h5>

<div class="r"><pre>BaseEstimator$get_attributes()</pre></div>



<h5>Examples</h5>

<div class="r example copy">
<pre>exampleClass$get_attributes()
</pre>
</div>


<hr>
<a id="method-BaseEstimator-set_random_state"></a>



<h4>Method <code>set_random_state()</code></h4>

<p>Auxiliary function that sets random state attribute of the self class
</p>


<h5>Usage</h5>

<div class="r"><pre>BaseEstimator$set_random_state(random_state)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>random_state</code></dt><dd><p>seed number to be set as random state</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>self
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>exampleClass$set_random_state(2022)
</pre>
</div>


<hr>
<a id="method-BaseEstimator-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>BaseEstimator$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>Examples</h3>

<pre><code class='language-R'>
## ------------------------------------------------
## Method `BaseEstimator$get_all_fields`
## ------------------------------------------------

TestClass &lt;- R6::R6Class(classname = "TestClass",
inherit = BaseEstimator,
private = list(random_state = NULL))
exampleClass &lt;- TestClass$new()
exampleClass$get_all_fields()

## ------------------------------------------------
## Method `BaseEstimator$get_attributes`
## ------------------------------------------------

exampleClass$get_attributes()

## ------------------------------------------------
## Method `BaseEstimator$set_random_state`
## ------------------------------------------------

exampleClass$set_random_state(2022)
</code></pre>

<hr>
<h2 id='check_is_fitted'>Fitting Control</h2><span id='topic+check_is_fitted'></span>

<h3>Description</h3>

<p>Checks if the given estimator is fitted
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_is_fitted(estimator)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_is_fitted_+3A_estimator">estimator</code></td>
<td>
<p>An estimator with is_fitted attribute</p>
</td></tr>
</table>


<h3>Value</h3>

<p>TRUE if the estimator is fitted, FALSE is not
</p>

<hr>
<h2 id='comparison_plot'>Comparison Plot</h2><span id='topic+comparison_plot'></span>

<h3>Description</h3>

<p>Plots the fitted functions obtained with various regressors (using their default values) on the
one-dimensional dataset (X, y).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>comparison_plot(X, y, model_list)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="comparison_plot_+3A_x">X</code></td>
<td>
<p>Predictors</p>
</td></tr>
<tr><td><code id="comparison_plot_+3A_y">y</code></td>
<td>
<p>Response variables</p>
</td></tr>
<tr><td><code id="comparison_plot_+3A_model_list">model_list</code></td>
<td>
<p>List of models to be compared</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>sine_data_list &lt;- less::synthetic_sine_curve()
X_sine &lt;- sine_data_list[[1]]
y_sine &lt;- sine_data_list[[2]]

model_list &lt;- c(DecisionTreeRegressor$new(), LinearRegression$new(), KNeighborsRegressor$new())

comparison_plot(X_sine, y_sine, model_list)
</code></pre>

<hr>
<h2 id='CoverTree'>CoverTree - Nearest Neighbor Search</h2><span id='topic+CoverTree'></span>

<h3>Description</h3>

<p>Wrapper R6 Class of FNN::get.knnx function that can be used for LESSRegressor and LESSClassifier
</p>


<h3>Details</h3>

<p>The cover tree is O(n) space data structure which allows us to answer queries in the same O(log(n)) time as kd tree given a fixed intrinsic dimensionality.
Templated code from <a href="https://hunch.net/~jl/projects/cover_tree/cover_tree.html">https://hunch.net/~jl/projects/cover_tree/cover_tree.html</a> is used.
</p>


<h3>Value</h3>

<p>R6 Class of CoverTree
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-CoverTree-new"><code>CoverTree$new()</code></a>
</p>
</li>
<li> <p><a href="#method-CoverTree-query"><code>CoverTree$query()</code></a>
</p>
</li>
<li> <p><a href="#method-CoverTree-clone"><code>CoverTree$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-CoverTree-new"></a>



<h4>Method <code>new()</code></h4>

<p>Creates a new instance of R6 Class of CoverTree
</p>


<h5>Usage</h5>

<div class="r"><pre>CoverTree$new(X = NULL)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>X</code></dt><dd><p>An <strong>M x d</strong> data.frame or matrix, where each of the <strong>M</strong> rows is a point or a (column) vector (where <strong>d=1</strong>).</p>
</dd>
</dl>

</div>



<h5>Examples</h5>

<div class="r example copy">
<pre>data(abalone)
ct &lt;- CoverTree$new(abalone[1:100,])
</pre>
</div>


<hr>
<a id="method-CoverTree-query"></a>



<h4>Method <code>query()</code></h4>

<p>Finds the p number of near neighbours for each point in an input/output dataset.
</p>


<h5>Usage</h5>

<div class="r"><pre>CoverTree$query(query_X = private$X, k = 1)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>query_X</code></dt><dd><p>A set of <strong>N x d</strong> points that will be queried against <code>X</code>. <strong>d</strong>, the number of columns, must be the same as <code>X</code>.
If missing, defaults to  <code>X</code>.</p>
</dd>
<dt><code>k</code></dt><dd><p>The maximum number of nearest neighbours to compute (deafults to 1).</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>A <code>list</code> of length 2 with elements:</p>

<table>
<tr>
 <td style="text-align: left;">
<code>nn.idx</code> </td><td style="text-align: left;"> A <strong>N x k</strong> integer matrix returning the near neighbour indices. </td>
</tr>
<tr>
 <td style="text-align: left;">
</td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">
<code>nn.dists</code> </td><td style="text-align: left;"> A <strong>N x k</strong> matrix returning the near neighbour Euclidean distances </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>




<h5>Examples</h5>

<div class="r example copy">
<pre>res &lt;- ct$query(abalone[1:3,], k=2)
print(res)
</pre>
</div>


<hr>
<a id="method-CoverTree-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>CoverTree$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>See Also</h3>

<p><code><a href="FNN.html#topic+get.knn">FNN::get.knnx()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## ------------------------------------------------
## Method `CoverTree$new`
## ------------------------------------------------

data(abalone)
ct &lt;- CoverTree$new(abalone[1:100,])

## ------------------------------------------------
## Method `CoverTree$query`
## ------------------------------------------------

res &lt;- ct$query(abalone[1:3,], k=2)
print(res)
</code></pre>

<hr>
<h2 id='DecisionTreeClassifier'>DecisionTreeClassifier</h2><span id='topic+DecisionTreeClassifier'></span>

<h3>Description</h3>

<p>Wrapper R6 Class of rpart::rpart function that can be used for LESSRegressor and LESSClassifier
</p>


<h3>Value</h3>

<p>R6 Class of DecisionTreeClassifier
</p>


<h3>Super classes</h3>

<p><code><a href="#topic+BaseEstimator">less::BaseEstimator</a></code> -&gt; <code><a href="#topic+SklearnEstimator">less::SklearnEstimator</a></code> -&gt; <code>DecisionTreeClassifier</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-DecisionTreeClassifier-new"><code>DecisionTreeClassifier$new()</code></a>
</p>
</li>
<li> <p><a href="#method-DecisionTreeClassifier-fit"><code>DecisionTreeClassifier$fit()</code></a>
</p>
</li>
<li> <p><a href="#method-DecisionTreeClassifier-predict"><code>DecisionTreeClassifier$predict()</code></a>
</p>
</li>
<li> <p><a href="#method-DecisionTreeClassifier-get_estimator_type"><code>DecisionTreeClassifier$get_estimator_type()</code></a>
</p>
</li>
<li> <p><a href="#method-DecisionTreeClassifier-clone"><code>DecisionTreeClassifier$clone()</code></a>
</p>
</li></ul>



<details open><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="less" data-topic="BaseEstimator" data-id="get_all_fields"><a href='../../less/html/BaseEstimator.html#method-BaseEstimator-get_all_fields'><code>less::BaseEstimator$get_all_fields()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="BaseEstimator" data-id="get_attributes"><a href='../../less/html/BaseEstimator.html#method-BaseEstimator-get_attributes'><code>less::BaseEstimator$get_attributes()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="BaseEstimator" data-id="set_random_state"><a href='../../less/html/BaseEstimator.html#method-BaseEstimator-set_random_state'><code>less::BaseEstimator$set_random_state()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="SklearnEstimator" data-id="get_isFitted"><a href='../../less/html/SklearnEstimator.html#method-SklearnEstimator-get_isFitted'><code>less::SklearnEstimator$get_isFitted()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="SklearnEstimator" data-id="get_type"><a href='../../less/html/SklearnEstimator.html#method-SklearnEstimator-get_type'><code>less::SklearnEstimator$get_type()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-DecisionTreeClassifier-new"></a>



<h4>Method <code>new()</code></h4>

<p>Creates a new instance of R6 Class of DecisionTreeClassifier
</p>


<h5>Usage</h5>

<div class="r"><pre>DecisionTreeClassifier$new(
  min_samples_split = 2,
  min_samples_leaf = 1,
  cp = 0.001,
  xval = 10,
  surrogate_style = 0,
  max_depth = 30
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>min_samples_split</code></dt><dd><p>The minimum number of observations that must exist in a node in order for a split to be attempted (defaults to 2).</p>
</dd>
<dt><code>min_samples_leaf</code></dt><dd><p>The minimum number of observations in any terminal (leaf) node (defaults to 1).</p>
</dd>
<dt><code>cp</code></dt><dd><p>Complexity Parameter. Any split that does not decrease the overall lack of fit by a factor of cp is not attempted.
This means that the overall R-squared must increase by cp at each step. The main role of this parameter is
to save computing time by pruning off splits that are obviously not worthwhile. (defaults to 0.001)</p>
</dd>
<dt><code>xval</code></dt><dd><p>Number of cross-validations (defaults to 10)</p>
</dd>
<dt><code>surrogate_style</code></dt><dd><p>Controls the selection of a best surrogate. If set to 0 (default) the program uses the total number of correct
classification for a potential surrogate variable, if set to 1 it uses the percent correct, calculated over the non-missing values of the surrogate.
The first option more severely penalizes covariates with a large number of missing values.</p>
</dd>
<dt><code>max_depth</code></dt><dd><p>The maximum depth of any node of the final tree, with the root node counted as depth 0.
Values greater than 30 will give nonsense results on 32-bit machines.</p>
</dd>
</dl>

</div>



<h5>Examples</h5>

<div class="r example copy">
<pre>dt &lt;- DecisionTreeClassifier$new()
dt &lt;- DecisionTreeClassifier$new(min_samples_split = 10)
dt &lt;- DecisionTreeClassifier$new(min_samples_leaf = 6, cp = 0.01)
</pre>
</div>


<hr>
<a id="method-DecisionTreeClassifier-fit"></a>



<h4>Method <code>fit()</code></h4>

<p>Builds a decision tree regressor from the training set (X, y).
</p>


<h5>Usage</h5>

<div class="r"><pre>DecisionTreeClassifier$fit(X, y)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>X</code></dt><dd><p>2D matrix or dataframe that includes predictors</p>
</dd>
<dt><code>y</code></dt><dd><p>1D vector or (n,1) dimensional matrix/dataframe that includes labels</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>Fitted R6 Class of DecisionTreeClassifier
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>data(iris)
split_list &lt;- train_test_split(iris, test_size =  0.3)
X_train &lt;- split_list[[1]]
X_test &lt;- split_list[[2]]
y_train &lt;- split_list[[3]]
y_test &lt;- split_list[[4]]

dt &lt;- DecisionTreeClassifier$new()
dt$fit(X_train, y_train)
</pre>
</div>


<hr>
<a id="method-DecisionTreeClassifier-predict"></a>



<h4>Method <code>predict()</code></h4>

<p>Predict regression value for X0.
</p>


<h5>Usage</h5>

<div class="r"><pre>DecisionTreeClassifier$predict(X0)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>X0</code></dt><dd><p>2D matrix or dataframe that includes predictors</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>Factor of the predict classes.
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>dt &lt;- DecisionTreeClassifier$new()
dt$fit(X_train, y_train)
preds &lt;- dt$predict(X_test)

dt &lt;- DecisionTreeClassifier$new()
preds &lt;- dt$fit(X_train, y_train)$predict(X_test)

preds &lt;- DecisionTreeClassifier$new()$fit(X_train, y_train)$predict(X_test)
print(caret::confusionMatrix(data=preds, reference = factor(y_test)))
</pre>
</div>


<hr>
<a id="method-DecisionTreeClassifier-get_estimator_type"></a>



<h4>Method <code>get_estimator_type()</code></h4>

<p>Auxiliary function returning the estimator type e.g 'regressor', 'classifier'
</p>


<h5>Usage</h5>

<div class="r"><pre>DecisionTreeClassifier$get_estimator_type()</pre></div>



<h5>Examples</h5>

<div class="r example copy">
<pre>dt$get_estimator_type()
</pre>
</div>


<hr>
<a id="method-DecisionTreeClassifier-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>DecisionTreeClassifier$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>See Also</h3>

<p><code><a href="rpart.html#topic+rpart">rpart::rpart()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## ------------------------------------------------
## Method `DecisionTreeClassifier$new`
## ------------------------------------------------

dt &lt;- DecisionTreeClassifier$new()
dt &lt;- DecisionTreeClassifier$new(min_samples_split = 10)
dt &lt;- DecisionTreeClassifier$new(min_samples_leaf = 6, cp = 0.01)

## ------------------------------------------------
## Method `DecisionTreeClassifier$fit`
## ------------------------------------------------

data(iris)
split_list &lt;- train_test_split(iris, test_size =  0.3)
X_train &lt;- split_list[[1]]
X_test &lt;- split_list[[2]]
y_train &lt;- split_list[[3]]
y_test &lt;- split_list[[4]]

dt &lt;- DecisionTreeClassifier$new()
dt$fit(X_train, y_train)

## ------------------------------------------------
## Method `DecisionTreeClassifier$predict`
## ------------------------------------------------

dt &lt;- DecisionTreeClassifier$new()
dt$fit(X_train, y_train)
preds &lt;- dt$predict(X_test)

dt &lt;- DecisionTreeClassifier$new()
preds &lt;- dt$fit(X_train, y_train)$predict(X_test)

preds &lt;- DecisionTreeClassifier$new()$fit(X_train, y_train)$predict(X_test)
print(caret::confusionMatrix(data=preds, reference = factor(y_test)))

## ------------------------------------------------
## Method `DecisionTreeClassifier$get_estimator_type`
## ------------------------------------------------

dt$get_estimator_type()
</code></pre>

<hr>
<h2 id='DecisionTreeRegressor'>DecisionTreeRegressor</h2><span id='topic+DecisionTreeRegressor'></span>

<h3>Description</h3>

<p>Wrapper R6 Class of rpart::rpart function that can be used for LESSRegressor and LESSClassifier
</p>


<h3>Value</h3>

<p>R6 Class of DecisionTreeRegressor
</p>


<h3>Super classes</h3>

<p><code><a href="#topic+BaseEstimator">less::BaseEstimator</a></code> -&gt; <code><a href="#topic+SklearnEstimator">less::SklearnEstimator</a></code> -&gt; <code>DecisionTreeRegressor</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-DecisionTreeRegressor-new"><code>DecisionTreeRegressor$new()</code></a>
</p>
</li>
<li> <p><a href="#method-DecisionTreeRegressor-fit"><code>DecisionTreeRegressor$fit()</code></a>
</p>
</li>
<li> <p><a href="#method-DecisionTreeRegressor-predict"><code>DecisionTreeRegressor$predict()</code></a>
</p>
</li>
<li> <p><a href="#method-DecisionTreeRegressor-get_estimator_type"><code>DecisionTreeRegressor$get_estimator_type()</code></a>
</p>
</li>
<li> <p><a href="#method-DecisionTreeRegressor-clone"><code>DecisionTreeRegressor$clone()</code></a>
</p>
</li></ul>



<details open><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="less" data-topic="BaseEstimator" data-id="get_all_fields"><a href='../../less/html/BaseEstimator.html#method-BaseEstimator-get_all_fields'><code>less::BaseEstimator$get_all_fields()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="BaseEstimator" data-id="get_attributes"><a href='../../less/html/BaseEstimator.html#method-BaseEstimator-get_attributes'><code>less::BaseEstimator$get_attributes()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="BaseEstimator" data-id="set_random_state"><a href='../../less/html/BaseEstimator.html#method-BaseEstimator-set_random_state'><code>less::BaseEstimator$set_random_state()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="SklearnEstimator" data-id="get_isFitted"><a href='../../less/html/SklearnEstimator.html#method-SklearnEstimator-get_isFitted'><code>less::SklearnEstimator$get_isFitted()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="SklearnEstimator" data-id="get_type"><a href='../../less/html/SklearnEstimator.html#method-SklearnEstimator-get_type'><code>less::SklearnEstimator$get_type()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-DecisionTreeRegressor-new"></a>



<h4>Method <code>new()</code></h4>

<p>Creates a new instance of R6 Class of DecisionTreeRegressor
</p>


<h5>Usage</h5>

<div class="r"><pre>DecisionTreeRegressor$new(
  min_samples_split = 2,
  min_samples_leaf = 1,
  cp = 0.001,
  xval = 10,
  surrogate_style = 0,
  max_depth = 30
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>min_samples_split</code></dt><dd><p>The minimum number of observations that must exist in a node in order for a split to be attempted (defaults to 2).</p>
</dd>
<dt><code>min_samples_leaf</code></dt><dd><p>The minimum number of observations in any terminal (leaf) node (defaults to 1).</p>
</dd>
<dt><code>cp</code></dt><dd><p>Complexity Parameter. Any split that does not decrease the overall lack of fit by a factor of cp is not attempted.
This means that the overall R-squared must increase by cp at each step. The main role of this parameter is
to save computing time by pruning off splits that are obviously not worthwhile. (defaults to 0.001)</p>
</dd>
<dt><code>xval</code></dt><dd><p>Number of cross-validations (defaults to 10)</p>
</dd>
<dt><code>surrogate_style</code></dt><dd><p>Controls the selection of a best surrogate. If set to 0 (default) the program uses the total number of correct
classification for a potential surrogate variable, if set to 1 it uses the percent correct, calculated over the non-missing values of the surrogate.
The first option more severely penalizes covariates with a large number of missing values.</p>
</dd>
<dt><code>max_depth</code></dt><dd><p>The maximum depth of any node of the final tree, with the root node counted as depth 0.
Values greater than 30 will give nonsense results on 32-bit machines.</p>
</dd>
</dl>

</div>



<h5>Examples</h5>

<div class="r example copy">
<pre>dt &lt;- DecisionTreeRegressor$new()
dt &lt;- DecisionTreeRegressor$new(min_samples_split = 10)
dt &lt;- DecisionTreeRegressor$new(min_samples_leaf = 6, cp = 0.01)
</pre>
</div>


<hr>
<a id="method-DecisionTreeRegressor-fit"></a>



<h4>Method <code>fit()</code></h4>

<p>Builds a decision tree regressor from the training set (X, y).
</p>


<h5>Usage</h5>

<div class="r"><pre>DecisionTreeRegressor$fit(X, y)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>X</code></dt><dd><p>2D matrix or dataframe that includes predictors</p>
</dd>
<dt><code>y</code></dt><dd><p>1D vector or (n,1) dimensional matrix/dataframe that includes response variables</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>Fitted R6 Class of DecisionTreeRegressor
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>data(abalone)
split_list &lt;- train_test_split(abalone[1:100,], test_size =  0.3)
X_train &lt;- split_list[[1]]
X_test &lt;- split_list[[2]]
y_train &lt;- split_list[[3]]
y_test &lt;- split_list[[4]]

dt &lt;- DecisionTreeRegressor$new()
dt$fit(X_train, y_train)
</pre>
</div>


<hr>
<a id="method-DecisionTreeRegressor-predict"></a>



<h4>Method <code>predict()</code></h4>

<p>Predict regression value for X0.
</p>


<h5>Usage</h5>

<div class="r"><pre>DecisionTreeRegressor$predict(X0)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>X0</code></dt><dd><p>2D matrix or dataframe that includes predictors</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>The predict values.
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>dt &lt;- DecisionTreeRegressor$new()
dt$fit(X_train, y_train)
preds &lt;- dt$predict(X_test)

dt &lt;- DecisionTreeRegressor$new()
preds &lt;- dt$fit(X_train, y_train)$predict(X_test)

preds &lt;- DecisionTreeRegressor$new()$fit(X_train, y_train)$predict(X_test)
print(head(matrix(c(y_test, preds), ncol = 2, dimnames = (list(NULL, c("True", "Prediction"))))))
</pre>
</div>


<hr>
<a id="method-DecisionTreeRegressor-get_estimator_type"></a>



<h4>Method <code>get_estimator_type()</code></h4>

<p>Auxiliary function returning the estimator type e.g 'regressor', 'classifier'
</p>


<h5>Usage</h5>

<div class="r"><pre>DecisionTreeRegressor$get_estimator_type()</pre></div>



<h5>Examples</h5>

<div class="r example copy">
<pre>dt$get_estimator_type()
</pre>
</div>


<hr>
<a id="method-DecisionTreeRegressor-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>DecisionTreeRegressor$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>See Also</h3>

<p><code><a href="rpart.html#topic+rpart">rpart::rpart()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## ------------------------------------------------
## Method `DecisionTreeRegressor$new`
## ------------------------------------------------

dt &lt;- DecisionTreeRegressor$new()
dt &lt;- DecisionTreeRegressor$new(min_samples_split = 10)
dt &lt;- DecisionTreeRegressor$new(min_samples_leaf = 6, cp = 0.01)

## ------------------------------------------------
## Method `DecisionTreeRegressor$fit`
## ------------------------------------------------

data(abalone)
split_list &lt;- train_test_split(abalone[1:100,], test_size =  0.3)
X_train &lt;- split_list[[1]]
X_test &lt;- split_list[[2]]
y_train &lt;- split_list[[3]]
y_test &lt;- split_list[[4]]

dt &lt;- DecisionTreeRegressor$new()
dt$fit(X_train, y_train)

## ------------------------------------------------
## Method `DecisionTreeRegressor$predict`
## ------------------------------------------------

dt &lt;- DecisionTreeRegressor$new()
dt$fit(X_train, y_train)
preds &lt;- dt$predict(X_test)

dt &lt;- DecisionTreeRegressor$new()
preds &lt;- dt$fit(X_train, y_train)$predict(X_test)

preds &lt;- DecisionTreeRegressor$new()$fit(X_train, y_train)$predict(X_test)
print(head(matrix(c(y_test, preds), ncol = 2, dimnames = (list(NULL, c("True", "Prediction"))))))

## ------------------------------------------------
## Method `DecisionTreeRegressor$get_estimator_type`
## ------------------------------------------------

dt$get_estimator_type()
</code></pre>

<hr>
<h2 id='get_functions'>Get Functions</h2><span id='topic+get_functions'></span>

<h3>Description</h3>

<p>Prints the available regressors, clustering methods, tree functions and helper functions within LESS package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_functions()
</code></pre>


<h3>Examples</h3>

<pre><code class='language-R'>get_functions()
</code></pre>

<hr>
<h2 id='HierarchicalClustering'>Hierarchical Clustering</h2><span id='topic+HierarchicalClustering'></span>

<h3>Description</h3>

<p>Wrapper R6 Class of stats::hclust function that can be used for LESSRegressor and LESSClassifier
</p>


<h3>Value</h3>

<p>R6 Class of HierarchicalClustering
</p>


<h3>Super class</h3>

<p><code><a href="#topic+BaseEstimator">less::BaseEstimator</a></code> -&gt; <code>HierarchicalClustering</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-HierarchicalClustering-new"><code>HierarchicalClustering$new()</code></a>
</p>
</li>
<li> <p><a href="#method-HierarchicalClustering-fit"><code>HierarchicalClustering$fit()</code></a>
</p>
</li>
<li> <p><a href="#method-HierarchicalClustering-get_cluster_centers"><code>HierarchicalClustering$get_cluster_centers()</code></a>
</p>
</li>
<li> <p><a href="#method-HierarchicalClustering-get_labels"><code>HierarchicalClustering$get_labels()</code></a>
</p>
</li>
<li> <p><a href="#method-HierarchicalClustering-clone"><code>HierarchicalClustering$clone()</code></a>
</p>
</li></ul>



<details open><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="less" data-topic="BaseEstimator" data-id="get_all_fields"><a href='../../less/html/BaseEstimator.html#method-BaseEstimator-get_all_fields'><code>less::BaseEstimator$get_all_fields()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="BaseEstimator" data-id="get_attributes"><a href='../../less/html/BaseEstimator.html#method-BaseEstimator-get_attributes'><code>less::BaseEstimator$get_attributes()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="BaseEstimator" data-id="set_random_state"><a href='../../less/html/BaseEstimator.html#method-BaseEstimator-set_random_state'><code>less::BaseEstimator$set_random_state()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-HierarchicalClustering-new"></a>



<h4>Method <code>new()</code></h4>

<p>Creates a new instance of R6 Class of HierarchicalClustering
</p>


<h5>Usage</h5>

<div class="r"><pre>HierarchicalClustering$new(linkage = "ward.D2", n_clusters = 8)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>linkage</code></dt><dd><p>the agglomeration method to be used. This should be (an unambiguous abbreviation of) one of
&quot;ward.D&quot;, &quot;ward.D2&quot;, &quot;single&quot;, &quot;complete&quot;, &quot;average&quot; (= UPGMA), &quot;mcquitty&quot; (= WPGMA), &quot;median&quot; (= WPGMC) or &quot;centroid&quot; (= UPGMC)
(defaults to ward.D2).</p>
</dd>
<dt><code>n_clusters</code></dt><dd><p>the number of clusters (defaults to 8).</p>
</dd>
</dl>

</div>



<h5>Examples</h5>

<div class="r example copy">
<pre>hc &lt;- HierarchicalClustering$new()
hc &lt;- HierarchicalClustering$new(n_clusters = 10)
hc &lt;- HierarchicalClustering$new(n_clusters = 10, linkage = "complete")
</pre>
</div>


<hr>
<a id="method-HierarchicalClustering-fit"></a>



<h4>Method <code>fit()</code></h4>

<p>Perform hierarchical clustering on a data matrix.
</p>


<h5>Usage</h5>

<div class="r"><pre>HierarchicalClustering$fit(X)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>X</code></dt><dd><p>numeric matrix of data, or an object that can be coerced to such a matrix (such as a numeric vector or a data frame with all numeric columns).</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>Fitted R6 class of HierarchicalClustering() that has 'labels' attribute
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>data(abalone)
hc &lt;- HierarchicalClustering$new()
hc$fit(abalone[1:100,])
</pre>
</div>


<hr>
<a id="method-HierarchicalClustering-get_cluster_centers"></a>



<h4>Method <code>get_cluster_centers()</code></h4>

<p>Auxiliary function returning the cluster centers
</p>


<h5>Usage</h5>

<div class="r"><pre>HierarchicalClustering$get_cluster_centers()</pre></div>



<h5>Examples</h5>

<div class="r example copy">
<pre>print(hc$get_cluster_centers())
</pre>
</div>


<hr>
<a id="method-HierarchicalClustering-get_labels"></a>



<h4>Method <code>get_labels()</code></h4>

<p>Auxiliary function returning a vector of integers (from 1:k) indicating the cluster to which each point is allocated.
</p>


<h5>Usage</h5>

<div class="r"><pre>HierarchicalClustering$get_labels()</pre></div>



<h5>Examples</h5>

<div class="r example copy">
<pre>print(hc$get_labels())
</pre>
</div>


<hr>
<a id="method-HierarchicalClustering-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>HierarchicalClustering$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>See Also</h3>

<p><code><a href="stats.html#topic+hclust">stats::hclust()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## ------------------------------------------------
## Method `HierarchicalClustering$new`
## ------------------------------------------------

hc &lt;- HierarchicalClustering$new()
hc &lt;- HierarchicalClustering$new(n_clusters = 10)
hc &lt;- HierarchicalClustering$new(n_clusters = 10, linkage = "complete")

## ------------------------------------------------
## Method `HierarchicalClustering$fit`
## ------------------------------------------------

data(abalone)
hc &lt;- HierarchicalClustering$new()
hc$fit(abalone[1:100,])

## ------------------------------------------------
## Method `HierarchicalClustering$get_cluster_centers`
## ------------------------------------------------

print(hc$get_cluster_centers())

## ------------------------------------------------
## Method `HierarchicalClustering$get_labels`
## ------------------------------------------------

print(hc$get_labels())
</code></pre>

<hr>
<h2 id='k_fold_cv'>k-Fold Cross Validation</h2><span id='topic+k_fold_cv'></span>

<h3>Description</h3>

<p>Applies k-Fold cross validation to the given model on the given data
</p>


<h3>Usage</h3>

<pre><code class='language-R'>k_fold_cv(
  data = NULL,
  model = NULL,
  random_state = NULL,
  k = 5,
  y_index = ncol(data)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="k_fold_cv_+3A_data">data</code></td>
<td>
<p>The dataset to be used</p>
</td></tr>
<tr><td><code id="k_fold_cv_+3A_model">model</code></td>
<td>
<p>A classification or a regression model (from LESS package)</p>
</td></tr>
<tr><td><code id="k_fold_cv_+3A_random_state">random_state</code></td>
<td>
<p>A seed number to get reproducable result</p>
</td></tr>
<tr><td><code id="k_fold_cv_+3A_k">k</code></td>
<td>
<p>Number of splits on the training set (defaults to 5)</p>
</td></tr>
<tr><td><code id="k_fold_cv_+3A_y_index">y_index</code></td>
<td>
<p>Column index of the response variable on the given <strong>data</strong>. Default is the last column.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector consists of metric of the individual folds and the average metric over the folds
</p>


<h3>Examples</h3>

<pre><code class='language-R'>k_fold_cv(data = iris, model = KNeighborsClassifier$new(), k = 3)
</code></pre>

<hr>
<h2 id='KDTree'>KDTree - Nearest Neighbor Search</h2><span id='topic+KDTree'></span>

<h3>Description</h3>

<p>Wrapper R6 Class of RANN::nn2 function that can be used for LESSRegressor and LESSClassifier
</p>


<h3>Value</h3>

<p>R6 Class of KDTree
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-KDTree-new"><code>KDTree$new()</code></a>
</p>
</li>
<li> <p><a href="#method-KDTree-query"><code>KDTree$query()</code></a>
</p>
</li>
<li> <p><a href="#method-KDTree-clone"><code>KDTree$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-KDTree-new"></a>



<h4>Method <code>new()</code></h4>

<p>Creates a new instance of R6 Class of KDTree
</p>


<h5>Usage</h5>

<div class="r"><pre>KDTree$new(X = NULL)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>X</code></dt><dd><p>An <strong>M x d</strong> data.frame or matrix, where each of the <strong>M</strong> rows is a point or a (column) vector (where <strong>d=1</strong>).</p>
</dd>
</dl>

</div>



<h5>Examples</h5>

<div class="r example copy">
<pre>data(abalone)
kdt &lt;- KDTree$new(abalone[1:100,])
</pre>
</div>


<hr>
<a id="method-KDTree-query"></a>



<h4>Method <code>query()</code></h4>

<p>Finds the p number of near neighbours for each point in an input/output dataset. The advantage of the kd-tree is that it runs in O(M log M) time.
</p>


<h5>Usage</h5>

<div class="r"><pre>KDTree$query(query_X = private$X, k = 1)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>query_X</code></dt><dd><p>A set of <strong>N x d</strong> points that will be queried against <code>X</code>. <strong>d</strong>, the number of columns, must be the same as <code>X</code>.
If missing, defaults to  <code>X</code>.</p>
</dd>
<dt><code>k</code></dt><dd><p>The maximum number of nearest neighbours to compute (deafults to 1).</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>A <code>list</code> of length 2 with elements:</p>

<table>
<tr>
 <td style="text-align: left;">
<code>nn.idx</code> </td><td style="text-align: left;"> A <strong>N x k</strong> integer matrix returning the near neighbour indices. </td>
</tr>
<tr>
 <td style="text-align: left;">
</td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">
<code>nn.dists</code> </td><td style="text-align: left;"> A <strong>N x k</strong> matrix returning the near neighbour Euclidean distances </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>




<h5>Examples</h5>

<div class="r example copy">
<pre>res &lt;- kdt$query(abalone[1:3,], k=2)
print(res)
</pre>
</div>


<hr>
<a id="method-KDTree-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>KDTree$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>See Also</h3>

<p><code><a href="RANN.html#topic+nn2">RANN::nn2()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## ------------------------------------------------
## Method `KDTree$new`
## ------------------------------------------------

data(abalone)
kdt &lt;- KDTree$new(abalone[1:100,])

## ------------------------------------------------
## Method `KDTree$query`
## ------------------------------------------------

res &lt;- kdt$query(abalone[1:3,], k=2)
print(res)
</code></pre>

<hr>
<h2 id='KMeans'>KMeans Clustering</h2><span id='topic+KMeans'></span>

<h3>Description</h3>

<p>Wrapper R6 Class of stats::kmeans function that can be used for LESSRegressor and LESSClassifier
</p>


<h3>Value</h3>

<p>R6 Class of KMeans
</p>


<h3>Super class</h3>

<p><code><a href="#topic+BaseEstimator">less::BaseEstimator</a></code> -&gt; <code>KMeans</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-KMeans-new"><code>KMeans$new()</code></a>
</p>
</li>
<li> <p><a href="#method-KMeans-fit"><code>KMeans$fit()</code></a>
</p>
</li>
<li> <p><a href="#method-KMeans-get_cluster_centers"><code>KMeans$get_cluster_centers()</code></a>
</p>
</li>
<li> <p><a href="#method-KMeans-get_labels"><code>KMeans$get_labels()</code></a>
</p>
</li>
<li> <p><a href="#method-KMeans-clone"><code>KMeans$clone()</code></a>
</p>
</li></ul>



<details open><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="less" data-topic="BaseEstimator" data-id="get_all_fields"><a href='../../less/html/BaseEstimator.html#method-BaseEstimator-get_all_fields'><code>less::BaseEstimator$get_all_fields()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="BaseEstimator" data-id="get_attributes"><a href='../../less/html/BaseEstimator.html#method-BaseEstimator-get_attributes'><code>less::BaseEstimator$get_attributes()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="BaseEstimator" data-id="set_random_state"><a href='../../less/html/BaseEstimator.html#method-BaseEstimator-set_random_state'><code>less::BaseEstimator$set_random_state()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-KMeans-new"></a>



<h4>Method <code>new()</code></h4>

<p>Creates a new instance of R6 Class of KMeans
</p>


<h5>Usage</h5>

<div class="r"><pre>KMeans$new(n_clusters = 8, n_init = 10, max_iter = 300, random_state = NULL)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>n_clusters</code></dt><dd><p>the number of clusters. A random set of (distinct) rows in X is chosen as the initial centres (default to 8)</p>
</dd>
<dt><code>n_init</code></dt><dd><p>how many random sets should be chosen? (default to 10)</p>
</dd>
<dt><code>max_iter</code></dt><dd><p>the maximum number of iterations allowed (default to 300).</p>
</dd>
<dt><code>random_state</code></dt><dd><p>seed number to be used for fixing the randomness (default to NULL).</p>
</dd>
</dl>

</div>



<h5>Examples</h5>

<div class="r example copy">
<pre>km &lt;- KMeans$new()
km &lt;- KMeans$new(n_clusters = 10)
km &lt;- KMeans$new(n_clusters = 10, random_state = 100)
</pre>
</div>


<hr>
<a id="method-KMeans-fit"></a>



<h4>Method <code>fit()</code></h4>

<p>Perform k-means clustering on a data matrix.
</p>


<h5>Usage</h5>

<div class="r"><pre>KMeans$fit(X)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>X</code></dt><dd><p>numeric matrix of data, or an object that can be coerced to such a matrix (such as a numeric vector or a data frame with all numeric columns).</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>Fitted R6 class of KMeans() that has 'cluster_centers' and 'labels' attributes
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>data(abalone)
km &lt;- KMeans$new()
km$fit(abalone[1:100,])
</pre>
</div>


<hr>
<a id="method-KMeans-get_cluster_centers"></a>



<h4>Method <code>get_cluster_centers()</code></h4>

<p>Auxiliary function returning the cluster centers
</p>


<h5>Usage</h5>

<div class="r"><pre>KMeans$get_cluster_centers()</pre></div>



<h5>Examples</h5>

<div class="r example copy">
<pre>print(km$get_cluster_centers())
</pre>
</div>


<hr>
<a id="method-KMeans-get_labels"></a>



<h4>Method <code>get_labels()</code></h4>

<p>Auxiliary function returning a vector of integers (from 1:k) indicating the cluster to which each point is allocated.
</p>


<h5>Usage</h5>

<div class="r"><pre>KMeans$get_labels()</pre></div>



<h5>Examples</h5>

<div class="r example copy">
<pre>print(km$get_labels())
</pre>
</div>


<hr>
<a id="method-KMeans-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>KMeans$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>See Also</h3>

<p><code><a href="stats.html#topic+kmeans">stats::kmeans()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## ------------------------------------------------
## Method `KMeans$new`
## ------------------------------------------------

km &lt;- KMeans$new()
km &lt;- KMeans$new(n_clusters = 10)
km &lt;- KMeans$new(n_clusters = 10, random_state = 100)

## ------------------------------------------------
## Method `KMeans$fit`
## ------------------------------------------------

data(abalone)
km &lt;- KMeans$new()
km$fit(abalone[1:100,])

## ------------------------------------------------
## Method `KMeans$get_cluster_centers`
## ------------------------------------------------

print(km$get_cluster_centers())

## ------------------------------------------------
## Method `KMeans$get_labels`
## ------------------------------------------------

print(km$get_labels())
</code></pre>

<hr>
<h2 id='KNeighborsClassifier'>KNeighborsClassifier</h2><span id='topic+KNeighborsClassifier'></span>

<h3>Description</h3>

<p>Wrapper R6 Class of caret::knnreg function that can be used for LESSRegressor and LESSClassifier
</p>


<h3>Value</h3>

<p>R6 Class of KNeighborsClassifier
</p>


<h3>Super classes</h3>

<p><code><a href="#topic+BaseEstimator">less::BaseEstimator</a></code> -&gt; <code><a href="#topic+SklearnEstimator">less::SklearnEstimator</a></code> -&gt; <code>KNeighborsClassifier</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-KNeighborsClassifier-new"><code>KNeighborsClassifier$new()</code></a>
</p>
</li>
<li> <p><a href="#method-KNeighborsClassifier-fit"><code>KNeighborsClassifier$fit()</code></a>
</p>
</li>
<li> <p><a href="#method-KNeighborsClassifier-predict"><code>KNeighborsClassifier$predict()</code></a>
</p>
</li>
<li> <p><a href="#method-KNeighborsClassifier-get_estimator_type"><code>KNeighborsClassifier$get_estimator_type()</code></a>
</p>
</li>
<li> <p><a href="#method-KNeighborsClassifier-clone"><code>KNeighborsClassifier$clone()</code></a>
</p>
</li></ul>



<details open><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="less" data-topic="BaseEstimator" data-id="get_all_fields"><a href='../../less/html/BaseEstimator.html#method-BaseEstimator-get_all_fields'><code>less::BaseEstimator$get_all_fields()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="BaseEstimator" data-id="get_attributes"><a href='../../less/html/BaseEstimator.html#method-BaseEstimator-get_attributes'><code>less::BaseEstimator$get_attributes()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="BaseEstimator" data-id="set_random_state"><a href='../../less/html/BaseEstimator.html#method-BaseEstimator-set_random_state'><code>less::BaseEstimator$set_random_state()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="SklearnEstimator" data-id="get_isFitted"><a href='../../less/html/SklearnEstimator.html#method-SklearnEstimator-get_isFitted'><code>less::SklearnEstimator$get_isFitted()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="SklearnEstimator" data-id="get_type"><a href='../../less/html/SklearnEstimator.html#method-SklearnEstimator-get_type'><code>less::SklearnEstimator$get_type()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-KNeighborsClassifier-new"></a>



<h4>Method <code>new()</code></h4>

<p>Creates a new instance of R6 Class of KNeighborsClassifier
</p>


<h5>Usage</h5>

<div class="r"><pre>KNeighborsClassifier$new(k = 5)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>k</code></dt><dd><p>Number of neighbors considered (defaults to 5).</p>
</dd>
</dl>

</div>



<h5>Examples</h5>

<div class="r example copy">
<pre>knc &lt;- KNeighborsClassifier$new()
knc &lt;- KNeighborsClassifier$new(k = 5)
</pre>
</div>


<hr>
<a id="method-KNeighborsClassifier-fit"></a>



<h4>Method <code>fit()</code></h4>

<p>Fit the k-nearest neighbors regressor from the training set (X, y).
</p>


<h5>Usage</h5>

<div class="r"><pre>KNeighborsClassifier$fit(X, y)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>X</code></dt><dd><p>2D matrix or dataframe that includes predictors</p>
</dd>
<dt><code>y</code></dt><dd><p>1D vector or (n,1) dimensional matrix/dataframe that includes labels</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>Fitted R6 Class of KNeighborsClassifier
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>data(iris)
split_list &lt;- train_test_split(iris, test_size =  0.3)
X_train &lt;- split_list[[1]]
X_test &lt;- split_list[[2]]
y_train &lt;- split_list[[3]]
y_test &lt;- split_list[[4]]

knc &lt;- KNeighborsClassifier$new()
knc$fit(X_train, y_train)
</pre>
</div>


<hr>
<a id="method-KNeighborsClassifier-predict"></a>



<h4>Method <code>predict()</code></h4>

<p>Predict regression value for X0.
</p>


<h5>Usage</h5>

<div class="r"><pre>KNeighborsClassifier$predict(X0)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>X0</code></dt><dd><p>2D matrix or dataframe that includes predictors</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>Factor of the predict classes.
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>knc &lt;- KNeighborsClassifier$new()
knc$fit(X_train, y_train)
preds &lt;- knc$predict(X_test)

knc &lt;- KNeighborsClassifier$new()
preds &lt;- knc$fit(X_train, y_train)$predict(X_test)

preds &lt;- KNeighborsClassifier$new()$fit(X_train, y_train)$predict(X_test)
print(caret::confusionMatrix(data=factor(preds), reference = factor(y_test)))
</pre>
</div>


<hr>
<a id="method-KNeighborsClassifier-get_estimator_type"></a>



<h4>Method <code>get_estimator_type()</code></h4>

<p>Auxiliary function returning the estimator type e.g 'regressor', 'classifier'
</p>


<h5>Usage</h5>

<div class="r"><pre>KNeighborsClassifier$get_estimator_type()</pre></div>



<h5>Examples</h5>

<div class="r example copy">
<pre>knc$get_estimator_type()
</pre>
</div>


<hr>
<a id="method-KNeighborsClassifier-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>KNeighborsClassifier$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>See Also</h3>

<p><code><a href="caret.html#topic+knn3">caret::knn3()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## ------------------------------------------------
## Method `KNeighborsClassifier$new`
## ------------------------------------------------

knc &lt;- KNeighborsClassifier$new()
knc &lt;- KNeighborsClassifier$new(k = 5)

## ------------------------------------------------
## Method `KNeighborsClassifier$fit`
## ------------------------------------------------

data(iris)
split_list &lt;- train_test_split(iris, test_size =  0.3)
X_train &lt;- split_list[[1]]
X_test &lt;- split_list[[2]]
y_train &lt;- split_list[[3]]
y_test &lt;- split_list[[4]]

knc &lt;- KNeighborsClassifier$new()
knc$fit(X_train, y_train)

## ------------------------------------------------
## Method `KNeighborsClassifier$predict`
## ------------------------------------------------

knc &lt;- KNeighborsClassifier$new()
knc$fit(X_train, y_train)
preds &lt;- knc$predict(X_test)

knc &lt;- KNeighborsClassifier$new()
preds &lt;- knc$fit(X_train, y_train)$predict(X_test)

preds &lt;- KNeighborsClassifier$new()$fit(X_train, y_train)$predict(X_test)
print(caret::confusionMatrix(data=factor(preds), reference = factor(y_test)))

## ------------------------------------------------
## Method `KNeighborsClassifier$get_estimator_type`
## ------------------------------------------------

knc$get_estimator_type()
</code></pre>

<hr>
<h2 id='KNeighborsRegressor'>KNeighborsRegressor</h2><span id='topic+KNeighborsRegressor'></span>

<h3>Description</h3>

<p>Wrapper R6 Class of caret::knnreg function that can be used for LESSRegressor and LESSClassifier
</p>


<h3>Value</h3>

<p>R6 Class of KNeighborsRegressor
</p>


<h3>Super classes</h3>

<p><code><a href="#topic+BaseEstimator">less::BaseEstimator</a></code> -&gt; <code><a href="#topic+SklearnEstimator">less::SklearnEstimator</a></code> -&gt; <code>KNeighborsRegressor</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-KNeighborsRegressor-new"><code>KNeighborsRegressor$new()</code></a>
</p>
</li>
<li> <p><a href="#method-KNeighborsRegressor-fit"><code>KNeighborsRegressor$fit()</code></a>
</p>
</li>
<li> <p><a href="#method-KNeighborsRegressor-predict"><code>KNeighborsRegressor$predict()</code></a>
</p>
</li>
<li> <p><a href="#method-KNeighborsRegressor-get_estimator_type"><code>KNeighborsRegressor$get_estimator_type()</code></a>
</p>
</li>
<li> <p><a href="#method-KNeighborsRegressor-clone"><code>KNeighborsRegressor$clone()</code></a>
</p>
</li></ul>



<details open><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="less" data-topic="BaseEstimator" data-id="get_all_fields"><a href='../../less/html/BaseEstimator.html#method-BaseEstimator-get_all_fields'><code>less::BaseEstimator$get_all_fields()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="BaseEstimator" data-id="get_attributes"><a href='../../less/html/BaseEstimator.html#method-BaseEstimator-get_attributes'><code>less::BaseEstimator$get_attributes()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="BaseEstimator" data-id="set_random_state"><a href='../../less/html/BaseEstimator.html#method-BaseEstimator-set_random_state'><code>less::BaseEstimator$set_random_state()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="SklearnEstimator" data-id="get_isFitted"><a href='../../less/html/SklearnEstimator.html#method-SklearnEstimator-get_isFitted'><code>less::SklearnEstimator$get_isFitted()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="SklearnEstimator" data-id="get_type"><a href='../../less/html/SklearnEstimator.html#method-SklearnEstimator-get_type'><code>less::SklearnEstimator$get_type()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-KNeighborsRegressor-new"></a>



<h4>Method <code>new()</code></h4>

<p>Creates a new instance of R6 Class of KNeighborsRegressor
</p>


<h5>Usage</h5>

<div class="r"><pre>KNeighborsRegressor$new(k = 5)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>k</code></dt><dd><p>Number of neighbors considered (defaults to 5).</p>
</dd>
</dl>

</div>



<h5>Examples</h5>

<div class="r example copy">
<pre>knr &lt;- KNeighborsRegressor$new()
knr &lt;- KNeighborsRegressor$new(k = 5)
</pre>
</div>


<hr>
<a id="method-KNeighborsRegressor-fit"></a>



<h4>Method <code>fit()</code></h4>

<p>Fit the k-nearest neighbors regressor from the training set (X, y).
</p>


<h5>Usage</h5>

<div class="r"><pre>KNeighborsRegressor$fit(X, y)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>X</code></dt><dd><p>2D matrix or dataframe that includes predictors</p>
</dd>
<dt><code>y</code></dt><dd><p>1D vector or (n,1) dimensional matrix/dataframe that includes response variables</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>Fitted R6 Class of KNeighborsRegressor
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>data(abalone)
split_list &lt;- train_test_split(abalone[1:100,], test_size =  0.3)
X_train &lt;- split_list[[1]]
X_test &lt;- split_list[[2]]
y_train &lt;- split_list[[3]]
y_test &lt;- split_list[[4]]

knr &lt;- KNeighborsRegressor$new()
knr$fit(X_train, y_train)
</pre>
</div>


<hr>
<a id="method-KNeighborsRegressor-predict"></a>



<h4>Method <code>predict()</code></h4>

<p>Predict regression value for X0.
</p>


<h5>Usage</h5>

<div class="r"><pre>KNeighborsRegressor$predict(X0)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>X0</code></dt><dd><p>2D matrix or dataframe that includes predictors</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>The predict values.
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>knr &lt;- KNeighborsRegressor$new()
knr$fit(X_train, y_train)
preds &lt;- knr$predict(X_test)

knr &lt;- KNeighborsRegressor$new()
preds &lt;- knr$fit(X_train, y_train)$predict(X_test)

preds &lt;- KNeighborsRegressor$new()$fit(X_train, y_train)$predict(X_test)
print(head(matrix(c(y_test, preds), ncol = 2, dimnames = (list(NULL, c("True", "Prediction"))))))
</pre>
</div>


<hr>
<a id="method-KNeighborsRegressor-get_estimator_type"></a>



<h4>Method <code>get_estimator_type()</code></h4>

<p>Auxiliary function returning the estimator type e.g 'regressor', 'classifier'
</p>


<h5>Usage</h5>

<div class="r"><pre>KNeighborsRegressor$get_estimator_type()</pre></div>



<h5>Examples</h5>

<div class="r example copy">
<pre>knr$get_estimator_type()
</pre>
</div>


<hr>
<a id="method-KNeighborsRegressor-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>KNeighborsRegressor$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>See Also</h3>

<p><code><a href="caret.html#topic+knnreg">caret::knnreg()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## ------------------------------------------------
## Method `KNeighborsRegressor$new`
## ------------------------------------------------

knr &lt;- KNeighborsRegressor$new()
knr &lt;- KNeighborsRegressor$new(k = 5)

## ------------------------------------------------
## Method `KNeighborsRegressor$fit`
## ------------------------------------------------

data(abalone)
split_list &lt;- train_test_split(abalone[1:100,], test_size =  0.3)
X_train &lt;- split_list[[1]]
X_test &lt;- split_list[[2]]
y_train &lt;- split_list[[3]]
y_test &lt;- split_list[[4]]

knr &lt;- KNeighborsRegressor$new()
knr$fit(X_train, y_train)

## ------------------------------------------------
## Method `KNeighborsRegressor$predict`
## ------------------------------------------------

knr &lt;- KNeighborsRegressor$new()
knr$fit(X_train, y_train)
preds &lt;- knr$predict(X_test)

knr &lt;- KNeighborsRegressor$new()
preds &lt;- knr$fit(X_train, y_train)$predict(X_test)

preds &lt;- KNeighborsRegressor$new()$fit(X_train, y_train)$predict(X_test)
print(head(matrix(c(y_test, preds), ncol = 2, dimnames = (list(NULL, c("True", "Prediction"))))))

## ------------------------------------------------
## Method `KNeighborsRegressor$get_estimator_type`
## ------------------------------------------------

knr$get_estimator_type()
</code></pre>

<hr>
<h2 id='laplacian'>Laplacian kernel - L1 norm</h2><span id='topic+laplacian'></span>

<h3>Description</h3>

<p>An alternative distance function that can be used in LESS.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>laplacian(data, center, coeff = 0.01)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="laplacian_+3A_data">data</code></td>
<td>
<p>Data that includes points in shape of <strong>(M x d)</strong></p>
</td></tr>
<tr><td><code id="laplacian_+3A_center">center</code></td>
<td>
<p>A constant point in shape of <strong>(1 x d)</strong></p>
</td></tr>
<tr><td><code id="laplacian_+3A_coeff">coeff</code></td>
<td>
<p>Coefficient value for Laplacian kernel</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numeric vector containing the laplacian kernel distance between each point in <strong>data</strong> and <strong>center</strong>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data &lt;- matrix(1:12, nrow=3)
center &lt;- c(2, 7, 1, 3)
distances &lt;- laplacian(data, center)
print(distances)
</code></pre>

<hr>
<h2 id='LESSBase'>LESSBase</h2><span id='topic+LESSBase'></span>

<h3>Description</h3>

<p>The base class for LESSRegressor and LESSClassifier
</p>


<h3>Value</h3>

<p>R6 class of LESSBase
</p>


<h3>Super classes</h3>

<p><code><a href="#topic+BaseEstimator">less::BaseEstimator</a></code> -&gt; <code><a href="#topic+SklearnEstimator">less::SklearnEstimator</a></code> -&gt; <code>LESSBase</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-LESSBase-new"><code>LESSBase$new()</code></a>
</p>
</li>
<li> <p><a href="#method-LESSBase-set_random_state"><code>LESSBase$set_random_state()</code></a>
</p>
</li>
<li> <p><a href="#method-LESSBase-get_n_subsets"><code>LESSBase$get_n_subsets()</code></a>
</p>
</li>
<li> <p><a href="#method-LESSBase-get_n_neighbors"><code>LESSBase$get_n_neighbors()</code></a>
</p>
</li>
<li> <p><a href="#method-LESSBase-get_frac"><code>LESSBase$get_frac()</code></a>
</p>
</li>
<li> <p><a href="#method-LESSBase-get_n_replications"><code>LESSBase$get_n_replications()</code></a>
</p>
</li>
<li> <p><a href="#method-LESSBase-get_d_normalize"><code>LESSBase$get_d_normalize()</code></a>
</p>
</li>
<li> <p><a href="#method-LESSBase-get_scaling"><code>LESSBase$get_scaling()</code></a>
</p>
</li>
<li> <p><a href="#method-LESSBase-get_val_size"><code>LESSBase$get_val_size()</code></a>
</p>
</li>
<li> <p><a href="#method-LESSBase-get_random_state"><code>LESSBase$get_random_state()</code></a>
</p>
</li>
<li> <p><a href="#method-LESSBase-get_isFitted"><code>LESSBase$get_isFitted()</code></a>
</p>
</li>
<li> <p><a href="#method-LESSBase-get_replications"><code>LESSBase$get_replications()</code></a>
</p>
</li>
<li> <p><a href="#method-LESSBase-clone"><code>LESSBase$clone()</code></a>
</p>
</li></ul>



<details open><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="less" data-topic="BaseEstimator" data-id="get_all_fields"><a href='../../less/html/BaseEstimator.html#method-BaseEstimator-get_all_fields'><code>less::BaseEstimator$get_all_fields()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="BaseEstimator" data-id="get_attributes"><a href='../../less/html/BaseEstimator.html#method-BaseEstimator-get_attributes'><code>less::BaseEstimator$get_attributes()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="SklearnEstimator" data-id="fit"><a href='../../less/html/SklearnEstimator.html#method-SklearnEstimator-fit'><code>less::SklearnEstimator$fit()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="SklearnEstimator" data-id="get_type"><a href='../../less/html/SklearnEstimator.html#method-SklearnEstimator-get_type'><code>less::SklearnEstimator$get_type()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="SklearnEstimator" data-id="predict"><a href='../../less/html/SklearnEstimator.html#method-SklearnEstimator-predict'><code>less::SklearnEstimator$predict()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-LESSBase-new"></a>



<h4>Method <code>new()</code></h4>

<p>Creates a new instance of R6 Class of LESSBase
</p>


<h5>Usage</h5>

<div class="r"><pre>LESSBase$new(replications = NULL, scobject = NULL, isFitted = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>replications</code></dt><dd><p>List to store the replications</p>
</dd>
<dt><code>scobject</code></dt><dd><p>Scaling object used for normalization (less::StandardScaler)</p>
</dd>
<dt><code>isFitted</code></dt><dd><p>Flag to check whether LESS is fitted</p>
</dd>
</dl>

</div>


<hr>
<a id="method-LESSBase-set_random_state"></a>



<h4>Method <code>set_random_state()</code></h4>

<p>Auxiliary function that sets random state attribute of the self class
</p>


<h5>Usage</h5>

<div class="r"><pre>LESSBase$set_random_state(random_state)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>random_state</code></dt><dd><p>seed number to be set as random state</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>self
</p>


<hr>
<a id="method-LESSBase-get_n_subsets"></a>



<h4>Method <code>get_n_subsets()</code></h4>

<p>Auxiliary function returning the number of subsets
</p>


<h5>Usage</h5>

<div class="r"><pre>LESSBase$get_n_subsets()</pre></div>


<hr>
<a id="method-LESSBase-get_n_neighbors"></a>



<h4>Method <code>get_n_neighbors()</code></h4>

<p>Auxiliary function returning the number of neighbors
</p>


<h5>Usage</h5>

<div class="r"><pre>LESSBase$get_n_neighbors()</pre></div>


<hr>
<a id="method-LESSBase-get_frac"></a>



<h4>Method <code>get_frac()</code></h4>

<p>Auxiliary function returning the percentage of samples used to set the number of neighbors
</p>


<h5>Usage</h5>

<div class="r"><pre>LESSBase$get_frac()</pre></div>


<hr>
<a id="method-LESSBase-get_n_replications"></a>



<h4>Method <code>get_n_replications()</code></h4>

<p>Auxiliary function returning the number of replications
</p>


<h5>Usage</h5>

<div class="r"><pre>LESSBase$get_n_replications()</pre></div>


<hr>
<a id="method-LESSBase-get_d_normalize"></a>



<h4>Method <code>get_d_normalize()</code></h4>

<p>Auxiliary function returning the flag for normalization
</p>


<h5>Usage</h5>

<div class="r"><pre>LESSBase$get_d_normalize()</pre></div>


<hr>
<a id="method-LESSBase-get_scaling"></a>



<h4>Method <code>get_scaling()</code></h4>

<p>Auxiliary function returning the flag for scaling
</p>


<h5>Usage</h5>

<div class="r"><pre>LESSBase$get_scaling()</pre></div>


<hr>
<a id="method-LESSBase-get_val_size"></a>



<h4>Method <code>get_val_size()</code></h4>

<p>Auxiliary function returning the validation set size
</p>


<h5>Usage</h5>

<div class="r"><pre>LESSBase$get_val_size()</pre></div>


<hr>
<a id="method-LESSBase-get_random_state"></a>



<h4>Method <code>get_random_state()</code></h4>

<p>Auxiliary function returning the random seed
</p>


<h5>Usage</h5>

<div class="r"><pre>LESSBase$get_random_state()</pre></div>


<hr>
<a id="method-LESSBase-get_isFitted"></a>



<h4>Method <code>get_isFitted()</code></h4>

<p>Auxiliary function returning the isFitted flag
</p>


<h5>Usage</h5>

<div class="r"><pre>LESSBase$get_isFitted()</pre></div>


<hr>
<a id="method-LESSBase-get_replications"></a>



<h4>Method <code>get_replications()</code></h4>

<p>Auxiliary function returning the isFitted flag
</p>


<h5>Usage</h5>

<div class="r"><pre>LESSBase$get_replications()</pre></div>


<hr>
<a id="method-LESSBase-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>LESSBase$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>



<hr>
<h2 id='LESSBinaryClassifier'>LESSBinaryClassifier</h2><span id='topic+LESSBinaryClassifier'></span>

<h3>Description</h3>

<p>Auxiliary binary classifier for Learning with Subset Stacking (LESS)
</p>


<h3>Value</h3>

<p>R6 class of LESSBinaryClassifier
</p>


<h3>Super classes</h3>

<p><code><a href="#topic+BaseEstimator">less::BaseEstimator</a></code> -&gt; <code><a href="#topic+SklearnEstimator">less::SklearnEstimator</a></code> -&gt; <code><a href="#topic+LESSBase">less::LESSBase</a></code> -&gt; <code>LESSBinaryClassifier</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-LESSBinaryClassifier-new"><code>LESSBinaryClassifier$new()</code></a>
</p>
</li>
<li> <p><a href="#method-LESSBinaryClassifier-fit"><code>LESSBinaryClassifier$fit()</code></a>
</p>
</li>
<li> <p><a href="#method-LESSBinaryClassifier-predict_proba"><code>LESSBinaryClassifier$predict_proba()</code></a>
</p>
</li>
<li> <p><a href="#method-LESSBinaryClassifier-get_global_estimator"><code>LESSBinaryClassifier$get_global_estimator()</code></a>
</p>
</li>
<li> <p><a href="#method-LESSBinaryClassifier-set_random_state"><code>LESSBinaryClassifier$set_random_state()</code></a>
</p>
</li>
<li> <p><a href="#method-LESSBinaryClassifier-clone"><code>LESSBinaryClassifier$clone()</code></a>
</p>
</li></ul>



<details><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="less" data-topic="BaseEstimator" data-id="get_all_fields"><a href='../../less/html/BaseEstimator.html#method-BaseEstimator-get_all_fields'><code>less::BaseEstimator$get_all_fields()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="BaseEstimator" data-id="get_attributes"><a href='../../less/html/BaseEstimator.html#method-BaseEstimator-get_attributes'><code>less::BaseEstimator$get_attributes()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="SklearnEstimator" data-id="get_type"><a href='../../less/html/SklearnEstimator.html#method-SklearnEstimator-get_type'><code>less::SklearnEstimator$get_type()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="SklearnEstimator" data-id="predict"><a href='../../less/html/SklearnEstimator.html#method-SklearnEstimator-predict'><code>less::SklearnEstimator$predict()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="LESSBase" data-id="get_d_normalize"><a href='../../less/html/LESSBase.html#method-LESSBase-get_d_normalize'><code>less::LESSBase$get_d_normalize()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="LESSBase" data-id="get_frac"><a href='../../less/html/LESSBase.html#method-LESSBase-get_frac'><code>less::LESSBase$get_frac()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="LESSBase" data-id="get_isFitted"><a href='../../less/html/LESSBase.html#method-LESSBase-get_isFitted'><code>less::LESSBase$get_isFitted()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="LESSBase" data-id="get_n_neighbors"><a href='../../less/html/LESSBase.html#method-LESSBase-get_n_neighbors'><code>less::LESSBase$get_n_neighbors()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="LESSBase" data-id="get_n_replications"><a href='../../less/html/LESSBase.html#method-LESSBase-get_n_replications'><code>less::LESSBase$get_n_replications()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="LESSBase" data-id="get_n_subsets"><a href='../../less/html/LESSBase.html#method-LESSBase-get_n_subsets'><code>less::LESSBase$get_n_subsets()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="LESSBase" data-id="get_random_state"><a href='../../less/html/LESSBase.html#method-LESSBase-get_random_state'><code>less::LESSBase$get_random_state()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="LESSBase" data-id="get_replications"><a href='../../less/html/LESSBase.html#method-LESSBase-get_replications'><code>less::LESSBase$get_replications()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="LESSBase" data-id="get_scaling"><a href='../../less/html/LESSBase.html#method-LESSBase-get_scaling'><code>less::LESSBase$get_scaling()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="LESSBase" data-id="get_val_size"><a href='../../less/html/LESSBase.html#method-LESSBase-get_val_size'><code>less::LESSBase$get_val_size()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-LESSBinaryClassifier-new"></a>



<h4>Method <code>new()</code></h4>

<p>Creates a new instance of R6 Class of LESSBinaryClassifier
</p>


<h5>Usage</h5>

<div class="r"><pre>LESSBinaryClassifier$new(
  frac = NULL,
  n_neighbors = NULL,
  n_subsets = NULL,
  n_replications = 20,
  d_normalize = TRUE,
  val_size = NULL,
  random_state = NULL,
  tree_method = function(X) KDTree$new(X),
  cluster_method = NULL,
  local_estimator = LinearRegression$new(),
  global_estimator = DecisionTreeClassifier$new(),
  distance_function = NULL,
  scaling = TRUE,
  warnings = TRUE
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>frac</code></dt><dd><p>fraction of total samples used for the number of neighbors (default is 0.05)</p>
</dd>
<dt><code>n_neighbors</code></dt><dd><p>number of neighbors (default is NULL)</p>
</dd>
<dt><code>n_subsets</code></dt><dd><p>number of subsets (default is NULL)</p>
</dd>
<dt><code>n_replications</code></dt><dd><p>number of replications (default is 20)</p>
</dd>
<dt><code>d_normalize</code></dt><dd><p>distance normalization (default is TRUE)</p>
</dd>
<dt><code>val_size</code></dt><dd><p>percentage of samples used for validation (default is NULL - no validation)</p>
</dd>
<dt><code>random_state</code></dt><dd><p>initialization of the random seed (default is NULL)</p>
</dd>
<dt><code>tree_method</code></dt><dd><p>method used for constructing the nearest neighbor tree, e.g., less::KDTree (default)</p>
</dd>
<dt><code>cluster_method</code></dt><dd><p>method used for clustering the subsets, e.g., less::KMeans (default is NULL)</p>
</dd>
<dt><code>local_estimator</code></dt><dd><p>estimator for the local models (default is less::LinearRegression)</p>
</dd>
<dt><code>global_estimator</code></dt><dd><p>estimator for the global model (default is less::DecisionTreeRegressor)</p>
</dd>
<dt><code>distance_function</code></dt><dd><p>distance function evaluating the distance from a subset to a sample,
e.g., df(subset, sample) which returns a vector of distances (default is RBF(subset, sample, 1.0/n_subsets^2))</p>
</dd>
<dt><code>scaling</code></dt><dd><p>flag to normalize the input data (default is TRUE)</p>
</dd>
<dt><code>warnings</code></dt><dd><p>flag to turn on (TRUE) or off (FALSE) the warnings (default is TRUE)</p>
</dd>
</dl>

</div>


<hr>
<a id="method-LESSBinaryClassifier-fit"></a>



<h4>Method <code>fit()</code></h4>

<p>Dummy fit function that calls the proper method according to validation and clustering parameters
Options are:
</p>

<ul>
<li><p> Default fitting (no validation set, no clustering)
</p>
</li>
<li><p> Fitting with validation set (no clustering)
</p>
</li>
<li><p> Fitting with clustering (no) validation set)
</p>
</li>
<li><p> Fitting with validation set and clustering
</p>
</li></ul>



<h5>Usage</h5>

<div class="r"><pre>LESSBinaryClassifier$fit(X, y)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>X</code></dt><dd><p>2D matrix or dataframe that includes predictors</p>
</dd>
<dt><code>y</code></dt><dd><p>1D vector or (n,1) dimensional matrix/dataframe that includes response variables</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>Fitted R6 Class of LESSBinaryClassifier
</p>


<hr>
<a id="method-LESSBinaryClassifier-predict_proba"></a>



<h4>Method <code>predict_proba()</code></h4>

<p>Prediction probabilities are evaluated for the test samples in X0
</p>


<h5>Usage</h5>

<div class="r"><pre>LESSBinaryClassifier$predict_proba(X0)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>X0</code></dt><dd><p>2D matrix or dataframe that includes predictors</p>
</dd>
</dl>

</div>


<hr>
<a id="method-LESSBinaryClassifier-get_global_estimator"></a>



<h4>Method <code>get_global_estimator()</code></h4>

<p>Auxiliary function returning the global_estimator
</p>


<h5>Usage</h5>

<div class="r"><pre>LESSBinaryClassifier$get_global_estimator()</pre></div>


<hr>
<a id="method-LESSBinaryClassifier-set_random_state"></a>



<h4>Method <code>set_random_state()</code></h4>

<p>Auxiliary function that sets random state attribute of the self class
</p>


<h5>Usage</h5>

<div class="r"><pre>LESSBinaryClassifier$set_random_state(random_state)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>random_state</code></dt><dd><p>seed number to be set as random state</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>self
</p>


<hr>
<a id="method-LESSBinaryClassifier-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>LESSBinaryClassifier$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>



<hr>
<h2 id='LESSClassifier'>LESSClassifier</h2><span id='topic+LESSClassifier'></span>

<h3>Description</h3>

<p>Classifier for Learning with Subset Stacking (LESS)
</p>


<h3>Value</h3>

<p>R6 class of LESSClassifier
</p>


<h3>Super classes</h3>

<p><code><a href="#topic+BaseEstimator">less::BaseEstimator</a></code> -&gt; <code><a href="#topic+SklearnEstimator">less::SklearnEstimator</a></code> -&gt; <code><a href="#topic+LESSBase">less::LESSBase</a></code> -&gt; <code>LESSClassifier</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-LESSClassifier-new"><code>LESSClassifier$new()</code></a>
</p>
</li>
<li> <p><a href="#method-LESSClassifier-fit"><code>LESSClassifier$fit()</code></a>
</p>
</li>
<li> <p><a href="#method-LESSClassifier-predict"><code>LESSClassifier$predict()</code></a>
</p>
</li>
<li> <p><a href="#method-LESSClassifier-get_estimator_type"><code>LESSClassifier$get_estimator_type()</code></a>
</p>
</li>
<li> <p><a href="#method-LESSClassifier-set_random_state"><code>LESSClassifier$set_random_state()</code></a>
</p>
</li>
<li> <p><a href="#method-LESSClassifier-clone"><code>LESSClassifier$clone()</code></a>
</p>
</li></ul>



<details><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="less" data-topic="BaseEstimator" data-id="get_all_fields"><a href='../../less/html/BaseEstimator.html#method-BaseEstimator-get_all_fields'><code>less::BaseEstimator$get_all_fields()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="BaseEstimator" data-id="get_attributes"><a href='../../less/html/BaseEstimator.html#method-BaseEstimator-get_attributes'><code>less::BaseEstimator$get_attributes()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="SklearnEstimator" data-id="get_type"><a href='../../less/html/SklearnEstimator.html#method-SklearnEstimator-get_type'><code>less::SklearnEstimator$get_type()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="LESSBase" data-id="get_d_normalize"><a href='../../less/html/LESSBase.html#method-LESSBase-get_d_normalize'><code>less::LESSBase$get_d_normalize()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="LESSBase" data-id="get_frac"><a href='../../less/html/LESSBase.html#method-LESSBase-get_frac'><code>less::LESSBase$get_frac()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="LESSBase" data-id="get_isFitted"><a href='../../less/html/LESSBase.html#method-LESSBase-get_isFitted'><code>less::LESSBase$get_isFitted()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="LESSBase" data-id="get_n_neighbors"><a href='../../less/html/LESSBase.html#method-LESSBase-get_n_neighbors'><code>less::LESSBase$get_n_neighbors()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="LESSBase" data-id="get_n_replications"><a href='../../less/html/LESSBase.html#method-LESSBase-get_n_replications'><code>less::LESSBase$get_n_replications()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="LESSBase" data-id="get_n_subsets"><a href='../../less/html/LESSBase.html#method-LESSBase-get_n_subsets'><code>less::LESSBase$get_n_subsets()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="LESSBase" data-id="get_random_state"><a href='../../less/html/LESSBase.html#method-LESSBase-get_random_state'><code>less::LESSBase$get_random_state()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="LESSBase" data-id="get_replications"><a href='../../less/html/LESSBase.html#method-LESSBase-get_replications'><code>less::LESSBase$get_replications()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="LESSBase" data-id="get_scaling"><a href='../../less/html/LESSBase.html#method-LESSBase-get_scaling'><code>less::LESSBase$get_scaling()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="LESSBase" data-id="get_val_size"><a href='../../less/html/LESSBase.html#method-LESSBase-get_val_size'><code>less::LESSBase$get_val_size()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-LESSClassifier-new"></a>



<h4>Method <code>new()</code></h4>

<p>Creates a new instance of R6 Class of LESSClassifier
</p>


<h5>Usage</h5>

<div class="r"><pre>LESSClassifier$new(
  frac = NULL,
  n_neighbors = NULL,
  n_subsets = NULL,
  n_replications = 20,
  d_normalize = TRUE,
  val_size = NULL,
  random_state = NULL,
  tree_method = function(X) KDTree$new(X),
  cluster_method = NULL,
  local_estimator = LinearRegression$new(),
  global_estimator = DecisionTreeClassifier$new(),
  distance_function = NULL,
  scaling = TRUE,
  warnings = TRUE,
  multiclass = "ovr"
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>frac</code></dt><dd><p>fraction of total samples used for the number of neighbors (default is 0.05)</p>
</dd>
<dt><code>n_neighbors</code></dt><dd><p>number of neighbors (default is NULL)</p>
</dd>
<dt><code>n_subsets</code></dt><dd><p>number of subsets (default is NULL)</p>
</dd>
<dt><code>n_replications</code></dt><dd><p>number of replications (default is 20)</p>
</dd>
<dt><code>d_normalize</code></dt><dd><p>distance normalization (default is TRUE)</p>
</dd>
<dt><code>val_size</code></dt><dd><p>percentage of samples used for validation (default is NULL - no validation)</p>
</dd>
<dt><code>random_state</code></dt><dd><p>initialization of the random seed (default is NULL)</p>
</dd>
<dt><code>tree_method</code></dt><dd><p>method used for constructing the nearest neighbor tree, e.g., less::KDTree (default)</p>
</dd>
<dt><code>cluster_method</code></dt><dd><p>method used for clustering the subsets, e.g., less::KMeans (default is NULL)</p>
</dd>
<dt><code>local_estimator</code></dt><dd><p>estimator for the local models (default is less::LinearRegression)</p>
</dd>
<dt><code>global_estimator</code></dt><dd><p>estimator for the global model (default is less::DecisionTreeRegressor)</p>
</dd>
<dt><code>distance_function</code></dt><dd><p>distance function evaluating the distance from a subset to a sample,
e.g., df(subset, sample) which returns a vector of distances (default is RBF(subset, sample, 1.0/n_subsets^2))</p>
</dd>
<dt><code>scaling</code></dt><dd><p>flag to normalize the input data (default is TRUE)</p>
</dd>
<dt><code>warnings</code></dt><dd><p>flag to turn on (TRUE) or off (FALSE) the warnings (default is TRUE)</p>
</dd>
<dt><code>multiclass</code></dt><dd><p>available strategies are 'ovr' (one-vs-rest, default), 'ovo' (one-vs-one), 'occ' (output-code-classifier) (default is 'ovr')</p>
</dd>
</dl>

</div>



<h5>Examples</h5>

<div class="r example copy">
<pre>lessclassifier &lt;- LESSClassifier$new()
lessclassifier &lt;- LESSClassifier$new(multiclass = "ovo")
</pre>
</div>


<hr>
<a id="method-LESSClassifier-fit"></a>



<h4>Method <code>fit()</code></h4>

<p>Dummy fit function that calls the fit method of the multiclass strategy
</p>


<h5>Usage</h5>

<div class="r"><pre>LESSClassifier$fit(X, y)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>X</code></dt><dd><p>2D matrix or dataframe that includes predictors</p>
</dd>
<dt><code>y</code></dt><dd><p>1D vector or (n,1) dimensional matrix/dataframe that includes response variables</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>Fitted R6 Class of LESSClassifier
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>data(iris)
set.seed(2022)
shuffled_iris &lt;- iris[sample(1:nrow(iris)),]
split_list &lt;- train_test_split(shuffled_iris[1:10,], test_size =  0.3, random_state = 1)
X_train &lt;- split_list[[1]]
X_test &lt;- split_list[[2]]
y_train &lt;- split_list[[3]]
y_test &lt;- split_list[[4]]

lessclassifier &lt;- LESSClassifier$new()
lessclassifier$fit(X_train, y_train)
</pre>
</div>


<hr>
<a id="method-LESSClassifier-predict"></a>



<h4>Method <code>predict()</code></h4>

<p>Dummy predict function that calls the predict method of the multiclass strategy
</p>


<h5>Usage</h5>

<div class="r"><pre>LESSClassifier$predict(X0)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>X0</code></dt><dd><p>2D matrix or dataframe that includes predictors</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>Predicted values of the given predictors
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>preds &lt;- lessclassifier$predict(X_test)
print(caret::confusionMatrix(data=factor(preds), reference = factor(y_test)))
</pre>
</div>


<hr>
<a id="method-LESSClassifier-get_estimator_type"></a>



<h4>Method <code>get_estimator_type()</code></h4>

<p>Auxiliary function returning the estimator type e.g 'regressor', 'classifier'
</p>


<h5>Usage</h5>

<div class="r"><pre>LESSClassifier$get_estimator_type()</pre></div>



<h5>Examples</h5>

<div class="r example copy">
<pre>lessclassifier$get_estimator_type()
</pre>
</div>


<hr>
<a id="method-LESSClassifier-set_random_state"></a>



<h4>Method <code>set_random_state()</code></h4>

<p>Auxiliary function that sets random state attribute of the self class
</p>


<h5>Usage</h5>

<div class="r"><pre>LESSClassifier$set_random_state(random_state)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>random_state</code></dt><dd><p>seed number to be set as random state</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>self
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>lessclassifier$set_random_state(2022)
</pre>
</div>


<hr>
<a id="method-LESSClassifier-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>LESSClassifier$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>Examples</h3>

<pre><code class='language-R'>
## ------------------------------------------------
## Method `LESSClassifier$new`
## ------------------------------------------------

lessclassifier &lt;- LESSClassifier$new()
lessclassifier &lt;- LESSClassifier$new(multiclass = "ovo")

## ------------------------------------------------
## Method `LESSClassifier$fit`
## ------------------------------------------------

data(iris)
set.seed(2022)
shuffled_iris &lt;- iris[sample(1:nrow(iris)),]
split_list &lt;- train_test_split(shuffled_iris[1:10,], test_size =  0.3, random_state = 1)
X_train &lt;- split_list[[1]]
X_test &lt;- split_list[[2]]
y_train &lt;- split_list[[3]]
y_test &lt;- split_list[[4]]

lessclassifier &lt;- LESSClassifier$new()
lessclassifier$fit(X_train, y_train)

## ------------------------------------------------
## Method `LESSClassifier$predict`
## ------------------------------------------------

preds &lt;- lessclassifier$predict(X_test)
print(caret::confusionMatrix(data=factor(preds), reference = factor(y_test)))

## ------------------------------------------------
## Method `LESSClassifier$get_estimator_type`
## ------------------------------------------------

lessclassifier$get_estimator_type()

## ------------------------------------------------
## Method `LESSClassifier$set_random_state`
## ------------------------------------------------

lessclassifier$set_random_state(2022)
</code></pre>

<hr>
<h2 id='LESSRegressor'>LESSRegressor</h2><span id='topic+LESSRegressor'></span>

<h3>Description</h3>

<p>Regressor for Learning with Subset Stacking (LESS)
</p>


<h3>Value</h3>

<p>R6 class of LESSRegressor
</p>


<h3>Super classes</h3>

<p><code><a href="#topic+BaseEstimator">less::BaseEstimator</a></code> -&gt; <code><a href="#topic+SklearnEstimator">less::SklearnEstimator</a></code> -&gt; <code><a href="#topic+LESSBase">less::LESSBase</a></code> -&gt; <code>LESSRegressor</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-LESSRegressor-new"><code>LESSRegressor$new()</code></a>
</p>
</li>
<li> <p><a href="#method-LESSRegressor-fit"><code>LESSRegressor$fit()</code></a>
</p>
</li>
<li> <p><a href="#method-LESSRegressor-predict"><code>LESSRegressor$predict()</code></a>
</p>
</li>
<li> <p><a href="#method-LESSRegressor-get_estimator_type"><code>LESSRegressor$get_estimator_type()</code></a>
</p>
</li>
<li> <p><a href="#method-LESSRegressor-clone"><code>LESSRegressor$clone()</code></a>
</p>
</li></ul>



<details><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="less" data-topic="BaseEstimator" data-id="get_all_fields"><a href='../../less/html/BaseEstimator.html#method-BaseEstimator-get_all_fields'><code>less::BaseEstimator$get_all_fields()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="BaseEstimator" data-id="get_attributes"><a href='../../less/html/BaseEstimator.html#method-BaseEstimator-get_attributes'><code>less::BaseEstimator$get_attributes()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="SklearnEstimator" data-id="get_type"><a href='../../less/html/SklearnEstimator.html#method-SklearnEstimator-get_type'><code>less::SklearnEstimator$get_type()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="LESSBase" data-id="get_d_normalize"><a href='../../less/html/LESSBase.html#method-LESSBase-get_d_normalize'><code>less::LESSBase$get_d_normalize()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="LESSBase" data-id="get_frac"><a href='../../less/html/LESSBase.html#method-LESSBase-get_frac'><code>less::LESSBase$get_frac()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="LESSBase" data-id="get_isFitted"><a href='../../less/html/LESSBase.html#method-LESSBase-get_isFitted'><code>less::LESSBase$get_isFitted()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="LESSBase" data-id="get_n_neighbors"><a href='../../less/html/LESSBase.html#method-LESSBase-get_n_neighbors'><code>less::LESSBase$get_n_neighbors()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="LESSBase" data-id="get_n_replications"><a href='../../less/html/LESSBase.html#method-LESSBase-get_n_replications'><code>less::LESSBase$get_n_replications()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="LESSBase" data-id="get_n_subsets"><a href='../../less/html/LESSBase.html#method-LESSBase-get_n_subsets'><code>less::LESSBase$get_n_subsets()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="LESSBase" data-id="get_random_state"><a href='../../less/html/LESSBase.html#method-LESSBase-get_random_state'><code>less::LESSBase$get_random_state()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="LESSBase" data-id="get_replications"><a href='../../less/html/LESSBase.html#method-LESSBase-get_replications'><code>less::LESSBase$get_replications()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="LESSBase" data-id="get_scaling"><a href='../../less/html/LESSBase.html#method-LESSBase-get_scaling'><code>less::LESSBase$get_scaling()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="LESSBase" data-id="get_val_size"><a href='../../less/html/LESSBase.html#method-LESSBase-get_val_size'><code>less::LESSBase$get_val_size()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="LESSBase" data-id="set_random_state"><a href='../../less/html/LESSBase.html#method-LESSBase-set_random_state'><code>less::LESSBase$set_random_state()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-LESSRegressor-new"></a>



<h4>Method <code>new()</code></h4>

<p>Creates a new instance of R6 Class of LESSRegressor
</p>


<h5>Usage</h5>

<div class="r"><pre>LESSRegressor$new(
  frac = NULL,
  n_neighbors = NULL,
  n_subsets = NULL,
  n_replications = 20,
  d_normalize = TRUE,
  val_size = NULL,
  random_state = NULL,
  tree_method = function(X) KDTree$new(X),
  cluster_method = NULL,
  local_estimator = LinearRegression$new(),
  global_estimator = DecisionTreeRegressor$new(),
  distance_function = NULL,
  scaling = TRUE,
  warnings = TRUE
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>frac</code></dt><dd><p>fraction of total samples used for the number of neighbors (default is 0.05)</p>
</dd>
<dt><code>n_neighbors</code></dt><dd><p>number of neighbors (default is NULL)</p>
</dd>
<dt><code>n_subsets</code></dt><dd><p>number of subsets (default is NULL)</p>
</dd>
<dt><code>n_replications</code></dt><dd><p>number of replications (default is 20)</p>
</dd>
<dt><code>d_normalize</code></dt><dd><p>distance normalization (default is TRUE)</p>
</dd>
<dt><code>val_size</code></dt><dd><p>percentage of samples used for validation (default is NULL - no validation)</p>
</dd>
<dt><code>random_state</code></dt><dd><p>initialization of the random seed (default is NULL)</p>
</dd>
<dt><code>tree_method</code></dt><dd><p>method used for constructing the nearest neighbor tree, e.g., less::KDTree (default)</p>
</dd>
<dt><code>cluster_method</code></dt><dd><p>method used for clustering the subsets, e.g., less::KMeans (default is NULL)</p>
</dd>
<dt><code>local_estimator</code></dt><dd><p>estimator for the local models (default is less::LinearRegression)</p>
</dd>
<dt><code>global_estimator</code></dt><dd><p>estimator for the global model (default is less::DecisionTreeRegressor)</p>
</dd>
<dt><code>distance_function</code></dt><dd><p>distance function evaluating the distance from a subset to a sample,
e.g., df(subset, sample) which returns a vector of distances (default is RBF(subset, sample, 1.0/n_subsets^2))</p>
</dd>
<dt><code>scaling</code></dt><dd><p>flag to normalize the input data (default is TRUE)</p>
</dd>
<dt><code>warnings</code></dt><dd><p>flag to turn on (TRUE) or off (FALSE) the warnings (default is TRUE)</p>
</dd>
</dl>

</div>



<h5>Examples</h5>

<div class="r example copy">
<pre>lessRegressor &lt;- LESSRegressor$new()
lessRegressor &lt;- LESSRegressor$new(val_size = 0.3)
lessRegressor &lt;- LESSRegressor$new(cluster_method = less::KMeans$new())
lessRegressor &lt;- LESSRegressor$new(val_size = 0.3, cluster_method = less::KMeans$new())
</pre>
</div>


<hr>
<a id="method-LESSRegressor-fit"></a>



<h4>Method <code>fit()</code></h4>

<p>Dummy fit function that calls the proper method according to validation and clustering parameters
Options are:
</p>

<ul>
<li><p> Default fitting (no validation set, no clustering)
</p>
</li>
<li><p> Fitting with validation set (no clustering)
</p>
</li>
<li><p> Fitting with clustering (no) validation set)
</p>
</li>
<li><p> Fitting with validation set and clustering
</p>
</li></ul>



<h5>Usage</h5>

<div class="r"><pre>LESSRegressor$fit(X, y)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>X</code></dt><dd><p>2D matrix or dataframe that includes predictors</p>
</dd>
<dt><code>y</code></dt><dd><p>1D vector or (n,1) dimensional matrix/dataframe that includes response variables</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>Fitted R6 Class of LESSRegressor
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>data(abalone)
split_list &lt;- train_test_split(abalone[1:100,], test_size =  0.3)
X_train &lt;- split_list[[1]]
X_test &lt;- split_list[[2]]
y_train &lt;- split_list[[3]]
y_test &lt;- split_list[[4]]

lessRegressor &lt;- LESSRegressor$new()
lessRegressor$fit(X_train, y_train)
</pre>
</div>


<hr>
<a id="method-LESSRegressor-predict"></a>



<h4>Method <code>predict()</code></h4>

<p>Predictions are evaluated for the test samples in X0
</p>


<h5>Usage</h5>

<div class="r"><pre>LESSRegressor$predict(X0)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>X0</code></dt><dd><p>2D matrix or dataframe that includes predictors</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>Predicted values of the given predictors
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>preds &lt;- lessRegressor$predict(X_test)
print(head(matrix(c(y_test, preds), ncol = 2, dimnames = (list(NULL, c("True", "Prediction"))))))
</pre>
</div>


<hr>
<a id="method-LESSRegressor-get_estimator_type"></a>



<h4>Method <code>get_estimator_type()</code></h4>

<p>Auxiliary function returning the estimator type e.g 'regressor', 'classifier'
</p>


<h5>Usage</h5>

<div class="r"><pre>LESSRegressor$get_estimator_type()</pre></div>



<h5>Examples</h5>

<div class="r example copy">
<pre>lessRegressor$get_estimator_type()
</pre>
</div>


<hr>
<a id="method-LESSRegressor-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>LESSRegressor$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>See Also</h3>

<p><a href="#topic+LESSBase">LESSBase</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## ------------------------------------------------
## Method `LESSRegressor$new`
## ------------------------------------------------

lessRegressor &lt;- LESSRegressor$new()
lessRegressor &lt;- LESSRegressor$new(val_size = 0.3)
lessRegressor &lt;- LESSRegressor$new(cluster_method = less::KMeans$new())
lessRegressor &lt;- LESSRegressor$new(val_size = 0.3, cluster_method = less::KMeans$new())

## ------------------------------------------------
## Method `LESSRegressor$fit`
## ------------------------------------------------

data(abalone)
split_list &lt;- train_test_split(abalone[1:100,], test_size =  0.3)
X_train &lt;- split_list[[1]]
X_test &lt;- split_list[[2]]
y_train &lt;- split_list[[3]]
y_test &lt;- split_list[[4]]

lessRegressor &lt;- LESSRegressor$new()
lessRegressor$fit(X_train, y_train)

## ------------------------------------------------
## Method `LESSRegressor$predict`
## ------------------------------------------------

preds &lt;- lessRegressor$predict(X_test)
print(head(matrix(c(y_test, preds), ncol = 2, dimnames = (list(NULL, c("True", "Prediction"))))))

## ------------------------------------------------
## Method `LESSRegressor$get_estimator_type`
## ------------------------------------------------

lessRegressor$get_estimator_type()
</code></pre>

<hr>
<h2 id='LinearRegression'>LinearRegression</h2><span id='topic+LinearRegression'></span>

<h3>Description</h3>

<p>Wrapper R6 Class of stats::lm function that can be used for LESSRegressor and LESSClassifier
</p>


<h3>Value</h3>

<p>R6 Class of LinearRegression
</p>


<h3>Super classes</h3>

<p><code><a href="#topic+BaseEstimator">less::BaseEstimator</a></code> -&gt; <code><a href="#topic+SklearnEstimator">less::SklearnEstimator</a></code> -&gt; <code>LinearRegression</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-LinearRegression-fit"><code>LinearRegression$fit()</code></a>
</p>
</li>
<li> <p><a href="#method-LinearRegression-predict"><code>LinearRegression$predict()</code></a>
</p>
</li>
<li> <p><a href="#method-LinearRegression-get_estimator_type"><code>LinearRegression$get_estimator_type()</code></a>
</p>
</li>
<li> <p><a href="#method-LinearRegression-clone"><code>LinearRegression$clone()</code></a>
</p>
</li></ul>



<details open><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="less" data-topic="BaseEstimator" data-id="get_all_fields"><a href='../../less/html/BaseEstimator.html#method-BaseEstimator-get_all_fields'><code>less::BaseEstimator$get_all_fields()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="BaseEstimator" data-id="get_attributes"><a href='../../less/html/BaseEstimator.html#method-BaseEstimator-get_attributes'><code>less::BaseEstimator$get_attributes()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="BaseEstimator" data-id="set_random_state"><a href='../../less/html/BaseEstimator.html#method-BaseEstimator-set_random_state'><code>less::BaseEstimator$set_random_state()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="SklearnEstimator" data-id="get_isFitted"><a href='../../less/html/SklearnEstimator.html#method-SklearnEstimator-get_isFitted'><code>less::SklearnEstimator$get_isFitted()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="SklearnEstimator" data-id="get_type"><a href='../../less/html/SklearnEstimator.html#method-SklearnEstimator-get_type'><code>less::SklearnEstimator$get_type()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-LinearRegression-fit"></a>



<h4>Method <code>fit()</code></h4>

<p>Fits a linear model (y ~ X)
</p>


<h5>Usage</h5>

<div class="r"><pre>LinearRegression$fit(X, y)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>X</code></dt><dd><p>2D matrix or dataframe that includes predictors</p>
</dd>
<dt><code>y</code></dt><dd><p>1D vector or (n,1) dimensional matrix/dataframe that includes response variables</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>Fitted R6 Class of LinearRegression
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>data(abalone)
split_list &lt;- train_test_split(abalone[1:100,], test_size =  0.3)
X_train &lt;- split_list[[1]]
X_test &lt;- split_list[[2]]
y_train &lt;- split_list[[3]]
y_test &lt;- split_list[[4]]

lr &lt;- LinearRegression$new()
lr$fit(X_train, y_train)
</pre>
</div>


<hr>
<a id="method-LinearRegression-predict"></a>



<h4>Method <code>predict()</code></h4>

<p>Predict regression value for X.
</p>


<h5>Usage</h5>

<div class="r"><pre>LinearRegression$predict(X0)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>X0</code></dt><dd><p>2D matrix or dataframe that includes predictors</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>The predict values.
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>lr &lt;- LinearRegression$new()
lr$fit(X_train, y_train)
preds &lt;- lr$predict(X_test)

lr &lt;- LinearRegression$new()
preds &lt;- lr$fit(X_train, y_train)$predict(X_test)

preds &lt;- LinearRegression$new()$fit(X_train, y_train)$predict(X_test)
print(head(matrix(c(y_test, preds), ncol = 2, dimnames = (list(NULL, c("True", "Prediction"))))))
</pre>
</div>


<hr>
<a id="method-LinearRegression-get_estimator_type"></a>



<h4>Method <code>get_estimator_type()</code></h4>

<p>Auxiliary function returning the estimator type e.g 'regressor', 'classifier'
</p>


<h5>Usage</h5>

<div class="r"><pre>LinearRegression$get_estimator_type()</pre></div>



<h5>Examples</h5>

<div class="r example copy">
<pre>lr$get_estimator_type()
</pre>
</div>


<hr>
<a id="method-LinearRegression-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>LinearRegression$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>See Also</h3>

<p><code><a href="stats.html#topic+lm">stats::lm()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## ------------------------------------------------
## Method `LinearRegression$fit`
## ------------------------------------------------

data(abalone)
split_list &lt;- train_test_split(abalone[1:100,], test_size =  0.3)
X_train &lt;- split_list[[1]]
X_test &lt;- split_list[[2]]
y_train &lt;- split_list[[3]]
y_test &lt;- split_list[[4]]

lr &lt;- LinearRegression$new()
lr$fit(X_train, y_train)

## ------------------------------------------------
## Method `LinearRegression$predict`
## ------------------------------------------------

lr &lt;- LinearRegression$new()
lr$fit(X_train, y_train)
preds &lt;- lr$predict(X_test)

lr &lt;- LinearRegression$new()
preds &lt;- lr$fit(X_train, y_train)$predict(X_test)

preds &lt;- LinearRegression$new()$fit(X_train, y_train)$predict(X_test)
print(head(matrix(c(y_test, preds), ncol = 2, dimnames = (list(NULL, c("True", "Prediction"))))))

## ------------------------------------------------
## Method `LinearRegression$get_estimator_type`
## ------------------------------------------------

lr$get_estimator_type()
</code></pre>

<hr>
<h2 id='prepareDataset'>Prepare a Dataset</h2><span id='topic+prepareDataset'></span>

<h3>Description</h3>

<p>Takes X and y datasets and merges them into a dataframe with column names (y, X_1, X_2 ...)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prepareDataset(X, y)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prepareDataset_+3A_x">X</code></td>
<td>
<p>Independent variables</p>
</td></tr>
<tr><td><code id="prepareDataset_+3A_y">y</code></td>
<td>
<p>Response variables</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A named dataframe which consists of X and y combined
</p>


<h3>Examples</h3>

<pre><code class='language-R'>X &lt;- matrix(1:20, nrow = 4)
y &lt;- c(5:8)
prepareDataset(X, y)
</code></pre>

<hr>
<h2 id='prepareXset'>Prepare a Dataset</h2><span id='topic+prepareXset'></span>

<h3>Description</h3>

<p>Takes X dataset and convert it into a dataframe with column names (X_1, X_2 ...)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prepareXset(X)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prepareXset_+3A_x">X</code></td>
<td>
<p>Independent variables</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A named dataframe which consists of X
</p>


<h3>Examples</h3>

<pre><code class='language-R'>X &lt;- matrix(1:20, nrow = 4)
prepareXset(X)
</code></pre>

<hr>
<h2 id='RandomForestClassifier'>RandomForestClassifier</h2><span id='topic+RandomForestClassifier'></span>

<h3>Description</h3>

<p>Wrapper R6 Class of randomForest::randomForest function that can be used for LESSRegressor and LESSClassifier
</p>


<h3>Value</h3>

<p>R6 Class of RandomForestClassifier
</p>


<h3>Super classes</h3>

<p><code><a href="#topic+BaseEstimator">less::BaseEstimator</a></code> -&gt; <code><a href="#topic+SklearnEstimator">less::SklearnEstimator</a></code> -&gt; <code>RandomForestClassifier</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-RandomForestClassifier-new"><code>RandomForestClassifier$new()</code></a>
</p>
</li>
<li> <p><a href="#method-RandomForestClassifier-fit"><code>RandomForestClassifier$fit()</code></a>
</p>
</li>
<li> <p><a href="#method-RandomForestClassifier-predict"><code>RandomForestClassifier$predict()</code></a>
</p>
</li>
<li> <p><a href="#method-RandomForestClassifier-get_estimator_type"><code>RandomForestClassifier$get_estimator_type()</code></a>
</p>
</li>
<li> <p><a href="#method-RandomForestClassifier-clone"><code>RandomForestClassifier$clone()</code></a>
</p>
</li></ul>



<details open><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="less" data-topic="BaseEstimator" data-id="get_all_fields"><a href='../../less/html/BaseEstimator.html#method-BaseEstimator-get_all_fields'><code>less::BaseEstimator$get_all_fields()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="BaseEstimator" data-id="get_attributes"><a href='../../less/html/BaseEstimator.html#method-BaseEstimator-get_attributes'><code>less::BaseEstimator$get_attributes()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="BaseEstimator" data-id="set_random_state"><a href='../../less/html/BaseEstimator.html#method-BaseEstimator-set_random_state'><code>less::BaseEstimator$set_random_state()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="SklearnEstimator" data-id="get_isFitted"><a href='../../less/html/SklearnEstimator.html#method-SklearnEstimator-get_isFitted'><code>less::SklearnEstimator$get_isFitted()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="SklearnEstimator" data-id="get_type"><a href='../../less/html/SklearnEstimator.html#method-SklearnEstimator-get_type'><code>less::SklearnEstimator$get_type()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-RandomForestClassifier-new"></a>



<h4>Method <code>new()</code></h4>

<p>Creates a new instance of R6 Class of RandomForestClassifier
</p>


<h5>Usage</h5>

<div class="r"><pre>RandomForestClassifier$new(
  n_estimators = 100,
  random_state = NULL,
  min_samples_leaf = 1,
  max_leaf_nodes = NULL
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>n_estimators</code></dt><dd><p>Number of trees to grow. This should not be set to too small a number,
to ensure that every input row gets predicted at least a few times (defaults to 100).</p>
</dd>
<dt><code>random_state</code></dt><dd><p>Seed number to be used for fixing the randomness (default to NULL).</p>
</dd>
<dt><code>min_samples_leaf</code></dt><dd><p>Minimum size of terminal nodes. Setting this number larger causes smaller trees to be grown
(and thus take less time) (defaults to 1)</p>
</dd>
<dt><code>max_leaf_nodes</code></dt><dd><p>Maximum number of terminal nodes trees in the forest can have.
If not given, trees are grown to the maximum possible (subject to limits by nodesize).
If set larger than maximum possible, a warning is issued. (defaults to NULL)</p>
</dd>
</dl>

</div>



<h5>Examples</h5>

<div class="r example copy">
<pre>rf &lt;- RandomForestClassifier$new()
rf &lt;- RandomForestClassifier$new(n_estimators = 500)
rf &lt;- RandomForestClassifier$new(n_estimators = 500, random_state = 100)
</pre>
</div>


<hr>
<a id="method-RandomForestClassifier-fit"></a>



<h4>Method <code>fit()</code></h4>

<p>Builds a random forest regressor from the training set (X, y).
</p>


<h5>Usage</h5>

<div class="r"><pre>RandomForestClassifier$fit(X, y)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>X</code></dt><dd><p>2D matrix or dataframe that includes predictors</p>
</dd>
<dt><code>y</code></dt><dd><p>1D vector or (n,1) dimensional matrix/dataframe that includes labels</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>Fitted R6 Class of RandomForestClassifier
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>data(iris)
split_list &lt;- train_test_split(iris, test_size =  0.3)
X_train &lt;- split_list[[1]]
X_test &lt;- split_list[[2]]
y_train &lt;- split_list[[3]]
y_test &lt;- split_list[[4]]

rf &lt;- RandomForestClassifier$new()
rf$fit(X_train, y_train)
</pre>
</div>


<hr>
<a id="method-RandomForestClassifier-predict"></a>



<h4>Method <code>predict()</code></h4>

<p>Predict regression value for X0.
</p>


<h5>Usage</h5>

<div class="r"><pre>RandomForestClassifier$predict(X0)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>X0</code></dt><dd><p>2D matrix or dataframe that includes predictors</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>Factor of the predict classes.
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>rf &lt;- RandomForestClassifier$new()
rf$fit(X_train, y_train)
preds &lt;- rf$predict(X_test)

rf &lt;- RandomForestClassifier$new()
preds &lt;- rf$fit(X_train, y_train)$predict(X_test)

preds &lt;- RandomForestClassifier$new()$fit(X_train, y_train)$predict(X_test)
print(caret::confusionMatrix(data=preds, reference = factor(y_test)))
</pre>
</div>


<hr>
<a id="method-RandomForestClassifier-get_estimator_type"></a>



<h4>Method <code>get_estimator_type()</code></h4>

<p>Auxiliary function returning the estimator type e.g 'regressor', 'classifier'
</p>


<h5>Usage</h5>

<div class="r"><pre>RandomForestClassifier$get_estimator_type()</pre></div>



<h5>Examples</h5>

<div class="r example copy">
<pre>rf$get_estimator_type()
</pre>
</div>


<hr>
<a id="method-RandomForestClassifier-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>RandomForestClassifier$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>See Also</h3>

<p><code><a href="randomForest.html#topic+randomForest">randomForest::randomForest()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## ------------------------------------------------
## Method `RandomForestClassifier$new`
## ------------------------------------------------

rf &lt;- RandomForestClassifier$new()
rf &lt;- RandomForestClassifier$new(n_estimators = 500)
rf &lt;- RandomForestClassifier$new(n_estimators = 500, random_state = 100)

## ------------------------------------------------
## Method `RandomForestClassifier$fit`
## ------------------------------------------------

data(iris)
split_list &lt;- train_test_split(iris, test_size =  0.3)
X_train &lt;- split_list[[1]]
X_test &lt;- split_list[[2]]
y_train &lt;- split_list[[3]]
y_test &lt;- split_list[[4]]

rf &lt;- RandomForestClassifier$new()
rf$fit(X_train, y_train)

## ------------------------------------------------
## Method `RandomForestClassifier$predict`
## ------------------------------------------------

rf &lt;- RandomForestClassifier$new()
rf$fit(X_train, y_train)
preds &lt;- rf$predict(X_test)

rf &lt;- RandomForestClassifier$new()
preds &lt;- rf$fit(X_train, y_train)$predict(X_test)

preds &lt;- RandomForestClassifier$new()$fit(X_train, y_train)$predict(X_test)
print(caret::confusionMatrix(data=preds, reference = factor(y_test)))

## ------------------------------------------------
## Method `RandomForestClassifier$get_estimator_type`
## ------------------------------------------------

rf$get_estimator_type()
</code></pre>

<hr>
<h2 id='RandomForestRegressor'>RandomForestRegressor</h2><span id='topic+RandomForestRegressor'></span>

<h3>Description</h3>

<p>Wrapper R6 Class of randomForest::randomForest function that can be used for LESSRegressor and LESSClassifier
</p>


<h3>Value</h3>

<p>R6 Class of RandomForestRegressor
</p>


<h3>Super classes</h3>

<p><code><a href="#topic+BaseEstimator">less::BaseEstimator</a></code> -&gt; <code><a href="#topic+SklearnEstimator">less::SklearnEstimator</a></code> -&gt; <code>RandomForestRegressor</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-RandomForestRegressor-new"><code>RandomForestRegressor$new()</code></a>
</p>
</li>
<li> <p><a href="#method-RandomForestRegressor-fit"><code>RandomForestRegressor$fit()</code></a>
</p>
</li>
<li> <p><a href="#method-RandomForestRegressor-predict"><code>RandomForestRegressor$predict()</code></a>
</p>
</li>
<li> <p><a href="#method-RandomForestRegressor-get_estimator_type"><code>RandomForestRegressor$get_estimator_type()</code></a>
</p>
</li>
<li> <p><a href="#method-RandomForestRegressor-clone"><code>RandomForestRegressor$clone()</code></a>
</p>
</li></ul>



<details open><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="less" data-topic="BaseEstimator" data-id="get_all_fields"><a href='../../less/html/BaseEstimator.html#method-BaseEstimator-get_all_fields'><code>less::BaseEstimator$get_all_fields()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="BaseEstimator" data-id="get_attributes"><a href='../../less/html/BaseEstimator.html#method-BaseEstimator-get_attributes'><code>less::BaseEstimator$get_attributes()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="BaseEstimator" data-id="set_random_state"><a href='../../less/html/BaseEstimator.html#method-BaseEstimator-set_random_state'><code>less::BaseEstimator$set_random_state()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="SklearnEstimator" data-id="get_isFitted"><a href='../../less/html/SklearnEstimator.html#method-SklearnEstimator-get_isFitted'><code>less::SklearnEstimator$get_isFitted()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="SklearnEstimator" data-id="get_type"><a href='../../less/html/SklearnEstimator.html#method-SklearnEstimator-get_type'><code>less::SklearnEstimator$get_type()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-RandomForestRegressor-new"></a>



<h4>Method <code>new()</code></h4>

<p>Creates a new instance of R6 Class of RandomForestRegressor
</p>


<h5>Usage</h5>

<div class="r"><pre>RandomForestRegressor$new(
  n_estimators = 100,
  random_state = NULL,
  min_samples_leaf = 1,
  max_leaf_nodes = NULL
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>n_estimators</code></dt><dd><p>Number of trees to grow. This should not be set to too small a number,
to ensure that every input row gets predicted at least a few times (defaults to 100).</p>
</dd>
<dt><code>random_state</code></dt><dd><p>Seed number to be used for fixing the randomness (default to NULL).</p>
</dd>
<dt><code>min_samples_leaf</code></dt><dd><p>Minimum size of terminal nodes. Setting this number larger causes smaller trees to be grown
(and thus take less time) (defaults to 1)</p>
</dd>
<dt><code>max_leaf_nodes</code></dt><dd><p>Maximum number of terminal nodes trees in the forest can have.
If not given, trees are grown to the maximum possible (subject to limits by nodesize).
If set larger than maximum possible, a warning is issued. (defaults to NULL)</p>
</dd>
</dl>

</div>



<h5>Examples</h5>

<div class="r example copy">
<pre>rf &lt;- RandomForestRegressor$new()
rf &lt;- RandomForestRegressor$new(n_estimators = 500)
rf &lt;- RandomForestRegressor$new(n_estimators = 500, random_state = 100)
</pre>
</div>


<hr>
<a id="method-RandomForestRegressor-fit"></a>



<h4>Method <code>fit()</code></h4>

<p>Builds a random forest regressor from the training set (X, y).
</p>


<h5>Usage</h5>

<div class="r"><pre>RandomForestRegressor$fit(X, y)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>X</code></dt><dd><p>2D matrix or dataframe that includes predictors</p>
</dd>
<dt><code>y</code></dt><dd><p>1D vector or (n,1) dimensional matrix/dataframe that includes response variables</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>Fitted R6 Class of RandomForestRegressor
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>data(abalone)
split_list &lt;- train_test_split(abalone[1:100,], test_size =  0.3)
X_train &lt;- split_list[[1]]
X_test &lt;- split_list[[2]]
y_train &lt;- split_list[[3]]
y_test &lt;- split_list[[4]]

rf &lt;- RandomForestRegressor$new()
rf$fit(X_train, y_train)
</pre>
</div>


<hr>
<a id="method-RandomForestRegressor-predict"></a>



<h4>Method <code>predict()</code></h4>

<p>Predict regression value for X0.
</p>


<h5>Usage</h5>

<div class="r"><pre>RandomForestRegressor$predict(X0)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>X0</code></dt><dd><p>2D matrix or dataframe that includes predictors</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>The predict values.
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>preds &lt;- rf$predict(X_test)
print(head(matrix(c(y_test, preds), ncol = 2, dimnames = (list(NULL, c("True", "Prediction"))))))
</pre>
</div>


<hr>
<a id="method-RandomForestRegressor-get_estimator_type"></a>



<h4>Method <code>get_estimator_type()</code></h4>

<p>Auxiliary function returning the estimator type e.g 'regressor', 'classifier'
</p>


<h5>Usage</h5>

<div class="r"><pre>RandomForestRegressor$get_estimator_type()</pre></div>



<h5>Examples</h5>

<div class="r example copy">
<pre>rf$get_estimator_type()
</pre>
</div>


<hr>
<a id="method-RandomForestRegressor-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>RandomForestRegressor$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>See Also</h3>

<p><code><a href="randomForest.html#topic+randomForest">randomForest::randomForest()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## ------------------------------------------------
## Method `RandomForestRegressor$new`
## ------------------------------------------------

rf &lt;- RandomForestRegressor$new()
rf &lt;- RandomForestRegressor$new(n_estimators = 500)
rf &lt;- RandomForestRegressor$new(n_estimators = 500, random_state = 100)

## ------------------------------------------------
## Method `RandomForestRegressor$fit`
## ------------------------------------------------

data(abalone)
split_list &lt;- train_test_split(abalone[1:100,], test_size =  0.3)
X_train &lt;- split_list[[1]]
X_test &lt;- split_list[[2]]
y_train &lt;- split_list[[3]]
y_test &lt;- split_list[[4]]

rf &lt;- RandomForestRegressor$new()
rf$fit(X_train, y_train)

## ------------------------------------------------
## Method `RandomForestRegressor$predict`
## ------------------------------------------------

preds &lt;- rf$predict(X_test)
print(head(matrix(c(y_test, preds), ncol = 2, dimnames = (list(NULL, c("True", "Prediction"))))))

## ------------------------------------------------
## Method `RandomForestRegressor$get_estimator_type`
## ------------------------------------------------

rf$get_estimator_type()
</code></pre>

<hr>
<h2 id='rbf'>RBF kernel - L2 norm</h2><span id='topic+rbf'></span>

<h3>Description</h3>

<p>The default distance function in LESS.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rbf(data, center, coeff = 0.01)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rbf_+3A_data">data</code></td>
<td>
<p>Data that includes points in shape of <strong>(M x d)</strong></p>
</td></tr>
<tr><td><code id="rbf_+3A_center">center</code></td>
<td>
<p>A constant point in shape of <strong>(1 x d)</strong></p>
</td></tr>
<tr><td><code id="rbf_+3A_coeff">coeff</code></td>
<td>
<p>Coefficient value for RBF kernel</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numeric vector containing the Radial basis function kernel distance between each point in <strong>data</strong> and <strong>center</strong>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data &lt;- matrix(1:12, nrow=3)
center &lt;- c(2, 7, 1, 3)
distances &lt;- rbf(data, center)
print(distances)
</code></pre>

<hr>
<h2 id='SklearnEstimator'>SklearnEstimator</h2><span id='topic+SklearnEstimator'></span>

<h3>Description</h3>

<p>A dummy base R6 class that includes fit, predict functions for estimators
</p>


<h3>Value</h3>

<p>R6 Class of SklearnEstimator
</p>


<h3>Super class</h3>

<p><code><a href="#topic+BaseEstimator">less::BaseEstimator</a></code> -&gt; <code>SklearnEstimator</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-SklearnEstimator-fit"><code>SklearnEstimator$fit()</code></a>
</p>
</li>
<li> <p><a href="#method-SklearnEstimator-predict"><code>SklearnEstimator$predict()</code></a>
</p>
</li>
<li> <p><a href="#method-SklearnEstimator-get_type"><code>SklearnEstimator$get_type()</code></a>
</p>
</li>
<li> <p><a href="#method-SklearnEstimator-get_isFitted"><code>SklearnEstimator$get_isFitted()</code></a>
</p>
</li>
<li> <p><a href="#method-SklearnEstimator-clone"><code>SklearnEstimator$clone()</code></a>
</p>
</li></ul>



<details open><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="less" data-topic="BaseEstimator" data-id="get_all_fields"><a href='../../less/html/BaseEstimator.html#method-BaseEstimator-get_all_fields'><code>less::BaseEstimator$get_all_fields()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="BaseEstimator" data-id="get_attributes"><a href='../../less/html/BaseEstimator.html#method-BaseEstimator-get_attributes'><code>less::BaseEstimator$get_attributes()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="BaseEstimator" data-id="set_random_state"><a href='../../less/html/BaseEstimator.html#method-BaseEstimator-set_random_state'><code>less::BaseEstimator$set_random_state()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-SklearnEstimator-fit"></a>



<h4>Method <code>fit()</code></h4>

<p>Dummy fit function
</p>


<h5>Usage</h5>

<div class="r"><pre>SklearnEstimator$fit()</pre></div>



<h5>Examples</h5>

<div class="r example copy">
<pre>sklearn &lt;- SklearnEstimator$new()
sklearn$fit()
</pre>
</div>


<hr>
<a id="method-SklearnEstimator-predict"></a>



<h4>Method <code>predict()</code></h4>

<p>Dummy predict function
</p>


<h5>Usage</h5>

<div class="r"><pre>SklearnEstimator$predict()</pre></div>



<h5>Examples</h5>

<div class="r example copy">
<pre>sklearn$predict()
</pre>
</div>


<hr>
<a id="method-SklearnEstimator-get_type"></a>



<h4>Method <code>get_type()</code></h4>

<p>Auxiliary function returning the type of the class e.g 'estimator'
</p>


<h5>Usage</h5>

<div class="r"><pre>SklearnEstimator$get_type()</pre></div>



<h5>Examples</h5>

<div class="r example copy">
<pre>sklearn$get_type()
</pre>
</div>


<hr>
<a id="method-SklearnEstimator-get_isFitted"></a>



<h4>Method <code>get_isFitted()</code></h4>

<p>Auxiliary function returning the isFitted flag
</p>


<h5>Usage</h5>

<div class="r"><pre>SklearnEstimator$get_isFitted()</pre></div>



<h5>Examples</h5>

<div class="r example copy">
<pre>sklearn$get_isFitted()
</pre>
</div>


<hr>
<a id="method-SklearnEstimator-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>SklearnEstimator$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>Examples</h3>

<pre><code class='language-R'>
## ------------------------------------------------
## Method `SklearnEstimator$fit`
## ------------------------------------------------

sklearn &lt;- SklearnEstimator$new()
sklearn$fit()

## ------------------------------------------------
## Method `SklearnEstimator$predict`
## ------------------------------------------------

sklearn$predict()

## ------------------------------------------------
## Method `SklearnEstimator$get_type`
## ------------------------------------------------

sklearn$get_type()

## ------------------------------------------------
## Method `SklearnEstimator$get_isFitted`
## ------------------------------------------------

sklearn$get_isFitted()
</code></pre>

<hr>
<h2 id='SVC'>Support Vector Classification</h2><span id='topic+SVC'></span>

<h3>Description</h3>

<p>Wrapper R6 Class of e1071::svm function that can be used for LESSRegressor and LESSClassifier
</p>


<h3>Value</h3>

<p>R6 Class of SVC
</p>


<h3>Super classes</h3>

<p><code><a href="#topic+BaseEstimator">less::BaseEstimator</a></code> -&gt; <code><a href="#topic+SklearnEstimator">less::SklearnEstimator</a></code> -&gt; <code>SVC</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-SVC-new"><code>SVC$new()</code></a>
</p>
</li>
<li> <p><a href="#method-SVC-fit"><code>SVC$fit()</code></a>
</p>
</li>
<li> <p><a href="#method-SVC-predict"><code>SVC$predict()</code></a>
</p>
</li>
<li> <p><a href="#method-SVC-get_estimator_type"><code>SVC$get_estimator_type()</code></a>
</p>
</li>
<li> <p><a href="#method-SVC-clone"><code>SVC$clone()</code></a>
</p>
</li></ul>



<details open><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="less" data-topic="BaseEstimator" data-id="get_all_fields"><a href='../../less/html/BaseEstimator.html#method-BaseEstimator-get_all_fields'><code>less::BaseEstimator$get_all_fields()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="BaseEstimator" data-id="get_attributes"><a href='../../less/html/BaseEstimator.html#method-BaseEstimator-get_attributes'><code>less::BaseEstimator$get_attributes()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="BaseEstimator" data-id="set_random_state"><a href='../../less/html/BaseEstimator.html#method-BaseEstimator-set_random_state'><code>less::BaseEstimator$set_random_state()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="SklearnEstimator" data-id="get_isFitted"><a href='../../less/html/SklearnEstimator.html#method-SklearnEstimator-get_isFitted'><code>less::SklearnEstimator$get_isFitted()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="SklearnEstimator" data-id="get_type"><a href='../../less/html/SklearnEstimator.html#method-SklearnEstimator-get_type'><code>less::SklearnEstimator$get_type()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-SVC-new"></a>



<h4>Method <code>new()</code></h4>

<p>Creates a new instance of R6 Class of SVC
</p>


<h5>Usage</h5>

<div class="r"><pre>SVC$new(
  scale = TRUE,
  kernel = "radial",
  degree = 3,
  gamma = NULL,
  coef0 = 0,
  cost = 1,
  cache_size = 40,
  tolerance = 0.001,
  epsilon = 0.1,
  shrinking = TRUE,
  cross = 0,
  probability = FALSE,
  fitted = TRUE
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>scale</code></dt><dd><p>A logical vector indicating the variables to be scaled. If scale is of length 1, the value is recycled as many times as needed.
Per default, data are scaled internally (both x and y variables) to zero mean and unit variance.
The center and scale values are returned and used for later predictions (default: TRUE)</p>
</dd>
<dt><code>kernel</code></dt><dd><p>The kernel used in training and predicting. Possible values are: &quot;linear&quot;, &quot;polynomial&quot;, &quot;radial&quot;, &quot;sigmoid&quot; (default is &quot;radial&quot;)</p>
</dd>
<dt><code>degree</code></dt><dd><p>Parameter needed for kernel of type polynomial (default: 3)</p>
</dd>
<dt><code>gamma</code></dt><dd><p>Parameter needed for all kernels except linear (default: 1/(data dimension))</p>
</dd>
<dt><code>coef0</code></dt><dd><p>Parameter needed for kernels of type polynomial and sigmoid (default: 0)</p>
</dd>
<dt><code>cost</code></dt><dd><p>Cost of constraints violation (default: 1)it is the C-constant of the regularization term in the Lagrange formulation (default: 1)</p>
</dd>
<dt><code>cache_size</code></dt><dd><p>Cache memory in MB (default: 40)</p>
</dd>
<dt><code>tolerance</code></dt><dd><p>Tolerance of termination criterion (default: 0.001)</p>
</dd>
<dt><code>epsilon</code></dt><dd><p>Epsilon in the insensitive-loss function (default: 0.1)</p>
</dd>
<dt><code>shrinking</code></dt><dd><p>Option whether to use the shrinking-heuristics (default: TRUE)</p>
</dd>
<dt><code>cross</code></dt><dd><p>If a integer value k&gt;0 is specified, a k-fold cross validation on the training data is performed to assess the quality of the model:
the accuracy rate for classification and the Mean Squared Error for regression (default: 0)</p>
</dd>
<dt><code>probability</code></dt><dd><p>Logical indicating whether the model should allow for probability predictions (default: FALSE)</p>
</dd>
<dt><code>fitted</code></dt><dd><p>Logical indicating whether the fitted values should be computed and included in the model or not (default: TRUE)</p>
</dd>
</dl>

</div>



<h5>Examples</h5>

<div class="r example copy">
<pre>svc &lt;- SVC$new()
svc &lt;- SVC$new(kernel = "polynomial")
</pre>
</div>


<hr>
<a id="method-SVC-fit"></a>



<h4>Method <code>fit()</code></h4>

<p>Fit the SVM model from the training set (X, y).
</p>


<h5>Usage</h5>

<div class="r"><pre>SVC$fit(X, y)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>X</code></dt><dd><p>2D matrix or dataframe that includes predictors</p>
</dd>
<dt><code>y</code></dt><dd><p>1D vector or (n,1) dimensional matrix/dataframe that includes labels</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>Fitted R6 Class of SVC
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>data(iris)
split_list &lt;- train_test_split(iris, test_size =  0.3)
X_train &lt;- split_list[[1]]
X_test &lt;- split_list[[2]]
y_train &lt;- split_list[[3]]
y_test &lt;- split_list[[4]]

svc &lt;- SVC$new()
svc$fit(X_train, y_train)
</pre>
</div>


<hr>
<a id="method-SVC-predict"></a>



<h4>Method <code>predict()</code></h4>

<p>Predict regression value for X0.
</p>


<h5>Usage</h5>

<div class="r"><pre>SVC$predict(X0)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>X0</code></dt><dd><p>2D matrix or dataframe that includes predictors</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>Factor of the predict classes.
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>svc &lt;- SVC$new()
svc$fit(X_train, y_train)
preds &lt;- svc$predict(X_test)

svc &lt;- SVC$new()
preds &lt;- svc$fit(X_train, y_train)$predict(X_test)

preds &lt;- SVC$new()$fit(X_train, y_train)$predict(X_test)
print(caret::confusionMatrix(data=preds, reference = factor(y_test)))
</pre>
</div>


<hr>
<a id="method-SVC-get_estimator_type"></a>



<h4>Method <code>get_estimator_type()</code></h4>

<p>Auxiliary function returning the estimator type e.g 'regressor', 'classifier'
</p>


<h5>Usage</h5>

<div class="r"><pre>SVC$get_estimator_type()</pre></div>



<h5>Examples</h5>

<div class="r example copy">
<pre>svc$get_estimator_type()
</pre>
</div>


<hr>
<a id="method-SVC-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>SVC$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>See Also</h3>

<p><code><a href="e1071.html#topic+svm">e1071::svm()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## ------------------------------------------------
## Method `SVC$new`
## ------------------------------------------------

svc &lt;- SVC$new()
svc &lt;- SVC$new(kernel = "polynomial")

## ------------------------------------------------
## Method `SVC$fit`
## ------------------------------------------------

data(iris)
split_list &lt;- train_test_split(iris, test_size =  0.3)
X_train &lt;- split_list[[1]]
X_test &lt;- split_list[[2]]
y_train &lt;- split_list[[3]]
y_test &lt;- split_list[[4]]

svc &lt;- SVC$new()
svc$fit(X_train, y_train)

## ------------------------------------------------
## Method `SVC$predict`
## ------------------------------------------------

svc &lt;- SVC$new()
svc$fit(X_train, y_train)
preds &lt;- svc$predict(X_test)

svc &lt;- SVC$new()
preds &lt;- svc$fit(X_train, y_train)$predict(X_test)

preds &lt;- SVC$new()$fit(X_train, y_train)$predict(X_test)
print(caret::confusionMatrix(data=preds, reference = factor(y_test)))

## ------------------------------------------------
## Method `SVC$get_estimator_type`
## ------------------------------------------------

svc$get_estimator_type()
</code></pre>

<hr>
<h2 id='SVR'>Support Vector Regression</h2><span id='topic+SVR'></span>

<h3>Description</h3>

<p>Wrapper R6 Class of e1071::svm function that can be used for LESSRegressor and LESSClassifier
</p>


<h3>Value</h3>

<p>R6 Class of SVR
</p>


<h3>Super classes</h3>

<p><code><a href="#topic+BaseEstimator">less::BaseEstimator</a></code> -&gt; <code><a href="#topic+SklearnEstimator">less::SklearnEstimator</a></code> -&gt; <code>SVR</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-SVR-new"><code>SVR$new()</code></a>
</p>
</li>
<li> <p><a href="#method-SVR-fit"><code>SVR$fit()</code></a>
</p>
</li>
<li> <p><a href="#method-SVR-predict"><code>SVR$predict()</code></a>
</p>
</li>
<li> <p><a href="#method-SVR-get_estimator_type"><code>SVR$get_estimator_type()</code></a>
</p>
</li>
<li> <p><a href="#method-SVR-clone"><code>SVR$clone()</code></a>
</p>
</li></ul>



<details open><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="less" data-topic="BaseEstimator" data-id="get_all_fields"><a href='../../less/html/BaseEstimator.html#method-BaseEstimator-get_all_fields'><code>less::BaseEstimator$get_all_fields()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="BaseEstimator" data-id="get_attributes"><a href='../../less/html/BaseEstimator.html#method-BaseEstimator-get_attributes'><code>less::BaseEstimator$get_attributes()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="BaseEstimator" data-id="set_random_state"><a href='../../less/html/BaseEstimator.html#method-BaseEstimator-set_random_state'><code>less::BaseEstimator$set_random_state()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="SklearnEstimator" data-id="get_isFitted"><a href='../../less/html/SklearnEstimator.html#method-SklearnEstimator-get_isFitted'><code>less::SklearnEstimator$get_isFitted()</code></a></span></li>
<li><span class="pkg-link" data-pkg="less" data-topic="SklearnEstimator" data-id="get_type"><a href='../../less/html/SklearnEstimator.html#method-SklearnEstimator-get_type'><code>less::SklearnEstimator$get_type()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-SVR-new"></a>



<h4>Method <code>new()</code></h4>

<p>Creates a new instance of R6 Class of SVR
</p>


<h5>Usage</h5>

<div class="r"><pre>SVR$new(
  scale = TRUE,
  kernel = "radial",
  degree = 3,
  gamma = NULL,
  coef0 = 0,
  cost = 1,
  cache_size = 40,
  tolerance = 0.001,
  epsilon = 0.1,
  shrinking = TRUE,
  cross = 0,
  probability = FALSE,
  fitted = TRUE
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>scale</code></dt><dd><p>A logical vector indicating the variables to be scaled. If scale is of length 1, the value is recycled as many times as needed.
Per default, data are scaled internally (both x and y variables) to zero mean and unit variance.
The center and scale values are returned and used for later predictions (default: TRUE)</p>
</dd>
<dt><code>kernel</code></dt><dd><p>The kernel used in training and predicting. Possible values are: &quot;linear&quot;, &quot;polynomial&quot;, &quot;radial&quot;, &quot;sigmoid&quot; (default is &quot;radial&quot;)</p>
</dd>
<dt><code>degree</code></dt><dd><p>Parameter needed for kernel of type polynomial (default: 3)</p>
</dd>
<dt><code>gamma</code></dt><dd><p>Parameter needed for all kernels except linear (default: 1/(data dimension))</p>
</dd>
<dt><code>coef0</code></dt><dd><p>Parameter needed for kernels of type polynomial and sigmoid (default: 0)</p>
</dd>
<dt><code>cost</code></dt><dd><p>Cost of constraints violation (default: 1)it is the C-constant of the regularization term in the Lagrange formulation (default: 1)</p>
</dd>
<dt><code>cache_size</code></dt><dd><p>Cache memory in MB (default: 40)</p>
</dd>
<dt><code>tolerance</code></dt><dd><p>Tolerance of termination criterion (default: 0.001)</p>
</dd>
<dt><code>epsilon</code></dt><dd><p>Epsilon in the insensitive-loss function (default: 0.1)</p>
</dd>
<dt><code>shrinking</code></dt><dd><p>Option whether to use the shrinking-heuristics (default: TRUE)</p>
</dd>
<dt><code>cross</code></dt><dd><p>If a integer value k&gt;0 is specified, a k-fold cross validation on the training data is performed to assess the quality of the model:
the accuracy rate for classification and the Mean Squared Error for regression (default: 0)</p>
</dd>
<dt><code>probability</code></dt><dd><p>Logical indicating whether the model should allow for probability predictions (default: FALSE)</p>
</dd>
<dt><code>fitted</code></dt><dd><p>Logical indicating whether the fitted values should be computed and included in the model or not (default: TRUE)</p>
</dd>
</dl>

</div>



<h5>Examples</h5>

<div class="r example copy">
<pre>svr &lt;- SVR$new()
svr &lt;- SVR$new(kernel = "polynomial")
</pre>
</div>


<hr>
<a id="method-SVR-fit"></a>



<h4>Method <code>fit()</code></h4>

<p>Fit the SVM model from the training set (X, y).
</p>


<h5>Usage</h5>

<div class="r"><pre>SVR$fit(X, y)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>X</code></dt><dd><p>2D matrix or dataframe that includes predictors</p>
</dd>
<dt><code>y</code></dt><dd><p>1D vector or (n,1) dimensional matrix/dataframe that includes response variables</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>Fitted R6 Class of SVR
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>data(abalone)
split_list &lt;- train_test_split(abalone[1:100,], test_size =  0.3)
X_train &lt;- split_list[[1]]
X_test &lt;- split_list[[2]]
y_train &lt;- split_list[[3]]
y_test &lt;- split_list[[4]]

svr &lt;- SVR$new()
svr$fit(X_train, y_train)
</pre>
</div>


<hr>
<a id="method-SVR-predict"></a>



<h4>Method <code>predict()</code></h4>

<p>Predict regression value for X0.
</p>


<h5>Usage</h5>

<div class="r"><pre>SVR$predict(X0)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>X0</code></dt><dd><p>2D matrix or dataframe that includes predictors</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>The predict values.
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>svr &lt;- SVR$new()
svr$fit(X_train, y_train)
preds &lt;- svr$predict(X_test)

svr &lt;- SVR$new()
preds &lt;- svr$fit(X_train, y_train)$predict(X_test)

preds &lt;- SVR$new()$fit(X_train, y_train)$predict(X_test)
print(head(matrix(c(y_test, preds), ncol = 2, dimnames = (list(NULL, c("True", "Prediction"))))))
</pre>
</div>


<hr>
<a id="method-SVR-get_estimator_type"></a>



<h4>Method <code>get_estimator_type()</code></h4>

<p>Auxiliary function returning the estimator type e.g 'regressor', 'classifier'
</p>


<h5>Usage</h5>

<div class="r"><pre>SVR$get_estimator_type()</pre></div>



<h5>Examples</h5>

<div class="r example copy">
<pre>svr$get_estimator_type()
</pre>
</div>


<hr>
<a id="method-SVR-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>SVR$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>See Also</h3>

<p><code><a href="e1071.html#topic+svm">e1071::svm()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## ------------------------------------------------
## Method `SVR$new`
## ------------------------------------------------

svr &lt;- SVR$new()
svr &lt;- SVR$new(kernel = "polynomial")

## ------------------------------------------------
## Method `SVR$fit`
## ------------------------------------------------

data(abalone)
split_list &lt;- train_test_split(abalone[1:100,], test_size =  0.3)
X_train &lt;- split_list[[1]]
X_test &lt;- split_list[[2]]
y_train &lt;- split_list[[3]]
y_test &lt;- split_list[[4]]

svr &lt;- SVR$new()
svr$fit(X_train, y_train)

## ------------------------------------------------
## Method `SVR$predict`
## ------------------------------------------------

svr &lt;- SVR$new()
svr$fit(X_train, y_train)
preds &lt;- svr$predict(X_test)

svr &lt;- SVR$new()
preds &lt;- svr$fit(X_train, y_train)$predict(X_test)

preds &lt;- SVR$new()$fit(X_train, y_train)$predict(X_test)
print(head(matrix(c(y_test, preds), ncol = 2, dimnames = (list(NULL, c("True", "Prediction"))))))

## ------------------------------------------------
## Method `SVR$get_estimator_type`
## ------------------------------------------------

svr$get_estimator_type()
</code></pre>

<hr>
<h2 id='synthetic_sine_curve'>Synthetic Sine Curve</h2><span id='topic+synthetic_sine_curve'></span>

<h3>Description</h3>

<p>A simple function to generate n_samples from sine curve in the range (-10, 10) with some amplitude.
The function returns the dataset (X, y), and plots the function (curve) along with the dataset (circles)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>synthetic_sine_curve(n_samples = 200)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="synthetic_sine_curve_+3A_n_samples">n_samples</code></td>
<td>
<p>Number of data points to be generated</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>sine_data_list &lt;- synthetic_sine_curve()
X_sine &lt;- sine_data_list[[1]]
y_sine &lt;- sine_data_list[[2]]
</code></pre>

<hr>
<h2 id='test_timing'>Compare Fitting Time</h2><span id='topic+test_timing'></span>

<h3>Description</h3>

<p>Plots a histogram chart which shows the fitting time obtained from various regressors/classifiers (using their default values) on the
given dataset (X, y).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>test_timing(type = 1, X, y)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="test_timing_+3A_type">type</code></td>
<td>
<p>1 to compare regressors, 2 for comparing classifiers</p>
</td></tr>
<tr><td><code id="test_timing_+3A_x">X</code></td>
<td>
<p>Predictors</p>
</td></tr>
<tr><td><code id="test_timing_+3A_y">y</code></td>
<td>
<p>Response variables</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>X &lt;- matrix(sample(100, 20), nrow = 10)
y &lt;- sample(100, 10)
test_timing(1, X, y)
</code></pre>

<hr>
<h2 id='train_test_split'>Dataset splitting</h2><span id='topic+train_test_split'></span>

<h3>Description</h3>

<p>Split dataframes or matrices into random train and test subsets. Takes the column at the <strong>y_index</strong> of <strong>data</strong> as response variable <strong>(y)</strong>
and the rest as the independent variables <strong>(X)</strong>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>train_test_split(
  data,
  test_size = 0.3,
  random_state = NULL,
  y_index = ncol(data)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="train_test_split_+3A_data">data</code></td>
<td>
<p>Dataset that is going to be split</p>
</td></tr>
<tr><td><code id="train_test_split_+3A_test_size">test_size</code></td>
<td>
<p>Represents the proportion of the dataset to include in the test split.
Should be between 0.0 and 1.0 (defaults to 0.3)</p>
</td></tr>
<tr><td><code id="train_test_split_+3A_random_state">random_state</code></td>
<td>
<p>Controls the shuffling applied to the data before applying the split.
Pass an int for reproducible output across multiple function calls (defaults to NULL)</p>
</td></tr>
<tr><td><code id="train_test_split_+3A_y_index">y_index</code></td>
<td>
<p>Corresponding column index of the response variable <strong>y</strong> (defaults to last column of <strong>data</strong>)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>list</code> of length 4 with elements:</p>

<table>
<tr>
 <td style="text-align: left;">
<code>X_train</code> </td><td style="text-align: left;"> Training input variables  </td>
</tr>
<tr>
 <td style="text-align: left;">
</td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">
<code>X_test</code> </td><td style="text-align: left;"> Test input variables </td>
</tr>
<tr>
 <td style="text-align: left;">
</td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">
<code>y_train</code> </td><td style="text-align: left;"> Training response variables   </td>
</tr>
<tr>
 <td style="text-align: left;">
</td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">
<code>y_test</code> </td><td style="text-align: left;"> Test response variables </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>



<h3>Examples</h3>

<pre><code class='language-R'>data(abalone)
split_list &lt;- train_test_split(abalone, test_size =  0.3)
X_train &lt;- split_list[[1]]
X_test &lt;- split_list[[2]]
y_train &lt;- split_list[[3]]
y_test &lt;- split_list[[4]]

print(head(X_train))
print(head(X_test))
print(head(y_train))
print(head(y_test))
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
