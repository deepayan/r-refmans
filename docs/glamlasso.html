<!DOCTYPE html><html><head><title>Help for package glamlasso</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {glamlasso}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#glamlasso'><p>Penalization in Large Scale Generalized Linear Array Models</p></a></li>
<li><a href='#glamlasso_internal'><p>Internal glamlasso Functions</p></a></li>
<li><a href='#glamlassoRR'><p>Penalized reduced rank regression  in a GLAM</p></a></li>
<li><a href='#glamlassoS'><p>Penalization in Large Scale Generalized Linear Array Models</p></a></li>
<li><a href='#objective'><p>Compute objective values</p></a></li>
<li><a href='#predict.glamlasso'><p>Make Prediction From a glamlasso Object</p></a></li>
<li><a href='#print.glamlasso'><p>Print Function for objects of Class glamlasso</p></a></li>
<li><a href='#RH'><p>The Rotated H-transform of a 3d Array by a Matrix</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Penalization in Large Scale Generalized Linear Array Models</td>
</tr>
<tr>
<td>Version:</td>
<td>3.0.1</td>
</tr>
<tr>
<td>Date:</td>
<td>2021-05-10</td>
</tr>
<tr>
<td>Author:</td>
<td>Adam Lund</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Adam Lund &lt;adam.lund@math.ku.dk&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Efficient design matrix free  lasso penalized estimation in large scale 2 and 3-dimensional generalized linear array model framework. The procedure is based on the gdpg algorithm from Lund et al. (2017) &lt;<a href="https://doi.org/10.1080%2F10618600.2017.1279548">doi:10.1080/10618600.2017.1279548</a>&gt;. Currently Lasso or Smoothly Clipped Absolute Deviation (SCAD) penalized estimation is possible for the following models: The Gaussian model with identity link, the Binomial model with logit link, the Poisson model with log link and the Gamma model with log link. It is also possible to include a  component in the model with non-tensor design e.g an intercept. Also provided are functions, glamlassoRR() and glamlassoS(), fitting special cases of GLAMs. </td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp (&ge; 0.11.2)</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2021-05-13 17:15:49 UTC; adam</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2021-05-16 22:30:06 UTC</td>
</tr>
</table>
<hr>
<h2 id='glamlasso'>Penalization in Large Scale Generalized Linear Array Models</h2><span id='topic+glamlasso'></span>

<h3>Description</h3>

<p>Efficient design matrix free procedure for fitting large scale penalized  2 or 3-dimensional
generalized linear array models (GLAM). It is also possible to fit an additional non-tensor structured component 
- e.g an intercept - however this can reduce the computational efficiency of the procedure substantially. 
Currently the LASSO penalty and the SCAD penalty are both implemented. Furthermore,
the Gaussian model with identity link,  the Binomial model with logit link, the Poisson model
with log link and the Gamma model with log link is currently implemented. The underlying algorithm combines gradient descent and proximal gradient (gdpg algorithm), see  <cite>Lund et al., 2017</cite>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>glamlasso(X, 
          Y, 
          Z = NULL,
          family = "gaussian",
          penalty = "lasso",
          intercept = FALSE,
          weights = NULL,
          betainit = NULL,
          alphainit = NULL,
          nlambda = 100,
          lambdaminratio = 1e-04,
          lambda = NULL,
          penaltyfactor = NULL,
          penaltyfactoralpha = NULL,
          reltolinner = 1e-07,
          reltolouter = 1e-04,
          maxiter = 15000,
          steps = 1,
          maxiterinner = 3000,
          maxiterouter = 25,
          btinnermax = 100,
          btoutermax = 100,
          iwls = "exact",
          nu = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="glamlasso_+3A_x">X</code></td>
<td>
<p>A list containing the tensor components (2 or 3) of the tensor design matrix.
These are  matrices of sizes <code class="reqn">n_i   \times p_i</code>.</p>
</td></tr>
<tr><td><code id="glamlasso_+3A_y">Y</code></td>
<td>
<p>The response values, an array of size <code class="reqn">n_1 \times\cdots\times n_d</code>. For option 
<code>family = "binomial"</code> this array must contain the proportion of successes and the 
number of trials is then specified as <code>weights</code> (see below).</p>
</td></tr>
<tr><td><code id="glamlasso_+3A_z">Z</code></td>
<td>
<p>The non tensor structrured part of the design matrix. A matrix of size <code class="reqn">n_1 \cdots n_d\times q</code>. 
Is set to <code>NULL</code> as default.</p>
</td></tr>
<tr><td><code id="glamlasso_+3A_family">family</code></td>
<td>
<p>A string specifying the model family (essentially the response distribution). Possible values 
are <code>"gaussian", "binomial", "poisson", "gamma"</code>.</p>
</td></tr>
<tr><td><code id="glamlasso_+3A_penalty">penalty</code></td>
<td>
<p>A string specifying the penalty. Possible values 
are <code>"lasso", "scad"</code>.</p>
</td></tr>
<tr><td><code id="glamlasso_+3A_intercept">intercept</code></td>
<td>
<p>Logical variable indicating if the model includes an intercept. 
When <code>intercept = TRUE</code> the first coulmn in the non-tensor design component <code>Z</code> is all 1s.
Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="glamlasso_+3A_weights">weights</code></td>
<td>
<p>Observation weights, an array of size <code class="reqn">n_1 \times \cdots \times n_d</code>. For option 
<code>family = "binomial"</code> this array must contain the number of trials and must be provided.</p>
</td></tr>
<tr><td><code id="glamlasso_+3A_betainit">betainit</code></td>
<td>
<p>The initial parameter values. Default is NULL in which case all parameters are initialized at zero.</p>
</td></tr>
<tr><td><code id="glamlasso_+3A_alphainit">alphainit</code></td>
<td>
<p>A <code class="reqn">q\times 1</code> vector containing the initial parameter values for the non-tensor parameter. 
Default is NULL in which case all parameters are initialized at 0.</p>
</td></tr>
<tr><td><code id="glamlasso_+3A_nlambda">nlambda</code></td>
<td>
<p>The number of <code>lambda</code> values.</p>
</td></tr>
<tr><td><code id="glamlasso_+3A_lambdaminratio">lambdaminratio</code></td>
<td>
<p>The smallest value for <code>lambda</code>, given as a fraction of 
<code class="reqn">\lambda_{max}</code>; the (data derived) smallest value for which all coefficients are zero.</p>
</td></tr>
<tr><td><code id="glamlasso_+3A_lambda">lambda</code></td>
<td>
<p>The sequence of penalty parameters for the regularization path.</p>
</td></tr>
<tr><td><code id="glamlasso_+3A_penaltyfactor">penaltyfactor</code></td>
<td>
<p>An array of size <code class="reqn">p_1 \times \cdots \times p_d</code>. Is multiplied 
with each element in <code>lambda</code> to allow differential shrinkage on the coefficients.</p>
</td></tr>
<tr><td><code id="glamlasso_+3A_penaltyfactoralpha">penaltyfactoralpha</code></td>
<td>
<p>A <code class="reqn">q \times 1</code> vector multiplied with each element in <code>lambda</code> to allow differential 
shrinkage on the non-tensor coefficients.</p>
</td></tr>
<tr><td><code id="glamlasso_+3A_reltolinner">reltolinner</code></td>
<td>
<p>The convergence tolerance for the inner loop</p>
</td></tr>
<tr><td><code id="glamlasso_+3A_reltolouter">reltolouter</code></td>
<td>
<p>The convergence tolerance for the outer loop.</p>
</td></tr>
<tr><td><code id="glamlasso_+3A_maxiter">maxiter</code></td>
<td>
<p>The maximum number of inner iterations allowed for each <code>lambda</code>
value, when  summing over all outer iterations for said <code>lambda</code>.</p>
</td></tr>
<tr><td><code id="glamlasso_+3A_steps">steps</code></td>
<td>
<p>The number of steps used in the multi-step adaptive lasso algorithm for non-convex penalties. Automatically set to 1 when <code>penalty = "lasso"</code>.</p>
</td></tr>
<tr><td><code id="glamlasso_+3A_maxiterinner">maxiterinner</code></td>
<td>
<p>The maximum number of inner iterations allowed for each outer iteration.</p>
</td></tr>
<tr><td><code id="glamlasso_+3A_maxiterouter">maxiterouter</code></td>
<td>
<p>The maximum number of outer iterations allowed for each lambda.</p>
</td></tr>
<tr><td><code id="glamlasso_+3A_btinnermax">btinnermax</code></td>
<td>
<p>Maximum number of backtracking steps allowed in each inner iteration. Default is <code>btinnermax = 100</code>.</p>
</td></tr>
<tr><td><code id="glamlasso_+3A_btoutermax">btoutermax</code></td>
<td>
<p>Maximum number of backtracking steps allowed in each outer iteration. Default is <code>btoutermax = 100</code>.</p>
</td></tr>
<tr><td><code id="glamlasso_+3A_iwls">iwls</code></td>
<td>
<p>A string indicating whether to use the exact iwls weight matrix or use a kronecker structured approximation to it.</p>
</td></tr>
<tr><td><code id="glamlasso_+3A_nu">nu</code></td>
<td>
<p>A number between 0 and 1 that controls the step size <code class="reqn">\delta</code> in the proximal algorithm (inner loop) by 
scaling the upper bound <code class="reqn">\hat{L}_h</code> on the Lipschitz constant <code class="reqn">L_h</code> (see <cite>Lund et al., 2017</cite>). 
For <code>nu = 1</code> backtracking never occurs and the proximal step size is always <code class="reqn">\delta = 1 / \hat{L}_h</code>. 
For <code>nu = 0</code> backtracking always occurs and the proximal step size is initially <code class="reqn">\delta = 1</code>. 
For <code>0 &lt; nu &lt; 1</code> the proximal step size is initially <code class="reqn">\delta = 1/(\nu\hat{L}_h)</code> and backtracking 
is only employed if the objective function does not decrease. A <code>nu</code> close  to 0 gives large step 
sizes and presumably more backtracking in the inner loop. The default is <code>nu = 1</code> and the option is only 
used if <code>iwls = "exact"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Consider a (two component) generalized linear model (GLM) 
</p>
<p style="text-align: center;"><code class="reqn">g(\mu) = X\beta  + Z\alpha =: \eta.</code>
</p>

<p>Here <code class="reqn">g</code> is a link function, <code class="reqn">\mu</code> is a <code class="reqn">n\times 1</code> vector 
containing the mean of the response variable  <code class="reqn">Y</code>, <code class="reqn">Z</code> is a 
<code class="reqn">n\times q</code> matrix and  <code class="reqn">X</code> a <code class="reqn">n\times p</code> matrix  with tensor structure 
</p>
<p style="text-align: center;"><code class="reqn">X =  X_d\otimes\cdots\otimes X_1,</code>
</p>

<p>where <code class="reqn">X_1,\ldots,X_d</code> are the marginal <code class="reqn">n_i\times p_i</code> design 
matrices (tensor factors) such that <code class="reqn">p = p_1\cdots p_d</code> and  
<code class="reqn">n=n_1\cdots n_d</code>. Then <code class="reqn">\beta</code> is the <code class="reqn">p\times 1</code> parameter associated 
with the tensor component <code class="reqn">X</code> and <code class="reqn">\alpha</code> the <code class="reqn">q\times 1</code> 
parameter associated with the non-tensor component <code class="reqn">Z</code>, e.g. the intercept. 
</p>
<p>Using the generalized linear array model (GLAM) framework the model equation is
</p>
<p style="text-align: center;"><code class="reqn">g(\mu) = \textrm{vec}(\rho(X_d,\rho(X_{d-1},\ldots,\rho(X_1,B)))) + Z\alpha ,</code>
</p>

<p>where <code class="reqn">\rho</code> is the so called rotated <code class="reqn">H</code>-transform and <code class="reqn">B</code> is the
array version of <code class="reqn">\beta</code>. See <cite>Currie et al., 2006</cite> for more details.
</p>
<p>The  log-likelihood is a function of  <code class="reqn">\theta  :=(\beta,\alpha)</code> through 
the linear predictor <code class="reqn">\eta</code> i.e. <code class="reqn">\theta \mapsto l(\eta(\theta))</code>.
In the usual exponential family framework this can be expressed as
</p>
<p style="text-align: center;"><code class="reqn">l(\eta(\theta )) = \sum_{i = 1}^n a_i \frac{y_i \vartheta(\eta_i(\theta)) - b(\vartheta(\eta_i(\theta  )))}{\psi}+c(y_i,\psi)</code>
</p>
 
<p>where <code class="reqn">\vartheta</code>, the canonical parameter map,  is linked to the  linear
predictor via the identity <code class="reqn">\eta(\theta) = g(b'(\vartheta))</code> with <code class="reqn">b</code>
the cumulant function. Here <code class="reqn">a_i \ge 0, i = 1,\ldots,n</code> are observation
weights and <code class="reqn">\psi</code> is the dispersion parameter.
</p>
<p>For <code class="reqn">d = 3</code> or <code class="reqn">d = 2</code>, using only the marginal matrices <code class="reqn">X_1,X_2,\ldots</code>, 
the function <code>glamlasso</code> solves the penalized estimation problem 
</p>
<p style="text-align: center;"><code class="reqn">\min_{\theta} -l(\eta(\theta)) + \lambda J (\theta),</code>
</p>
 
<p>for <code class="reqn">J</code> either the LASSO or SCAD penalty function,  in the GLAM setup for 
a sequence of penalty parameters <code class="reqn">\lambda&gt;0</code>. The underlying algorithm is 
based on an outer gradient descent loop and an inner proximal gradient based 
loop. We note that if <code class="reqn">J</code> is not convex, as with the SCAD penalty, we use
the multiple step adaptive lasso procedure to loop over the inner proximal 
algorithm, see <cite>Lund et al., 2017</cite> for more details.
</p>
<p>Note that the package is optimized towards solving the estimation problem, 
for <code class="reqn">\alpha = 0</code>. For <code class="reqn">\alpha \neq 0</code> the user incurs a  potentially 
substantial computational cost. Especially it is not advisable to inlcude a 
very large non-tensor component  in the model (large <code class="reqn">q</code>) and even  
adding an intecept to the model (<code class="reqn">q=1</code>) will result in a reduction of 
computational efficiency.
</p>


<h3>Value</h3>

<p>An object with S3 Class 'glamlasso'. 
</p>
<table>
<tr><td><code>spec</code></td>
<td>
<p>A string indicating the GLAM dimension (<code class="reqn">d = 2, 3</code>), the model 
family and the penalty.</p>
</td></tr>  
<tr><td><code>beta</code></td>
<td>
<p>A <code class="reqn">p_1\cdots p_d \times</code> <code>nlambda</code> matrix containing the 
estimates of the parameters for the tensor structured part of the model 
(<code>beta</code>) for  each <code>lambda</code>-value.</p>
</td></tr>
<tr><td><code>alpha</code></td>
<td>
<p>A <code class="reqn">q \times</code> <code>nlambda</code> matrix containing the estimates 
of the parameters for the non tensor structured part of the model <code>alpha</code> 
for each <code>lambda</code>-value. If <code>intercept = TRUE</code> the first row 
contains the intercept estimate for each <code>lambda</code>-value.</p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>
<p>A vector containing the sequence of penalty values used in the 
estimation procedure.</p>
</td></tr>
<tr><td><code>df</code></td>
<td>
<p>The number of nonzero coefficients for each value of <code>lambda</code>.</p>
</td></tr>	
<tr><td><code>dimcoef</code></td>
<td>
<p>A vector giving the dimension of the model coefficient array 
<code class="reqn">\beta</code>.</p>
</td></tr>
<tr><td><code>dimobs</code></td>
<td>
<p>A vector giving the dimension of the observation (response) 
array <code>Y</code>.</p>
</td></tr>
<tr><td><code>Iter</code></td>
<td>
<p>A list with 4 items:  
<code>bt_iter_inner</code>  is total number of backtracking steps performed in the 
inner loop, <code>bt_enter_inner</code> is the number of times the backtracking is 
initiated in  the inner loop, <code>bt_iter_outer</code> is total number of 
backtracking steps performed in the  outer loop, and <code>iter_mat</code> is a 
<code>nlambda</code> <code class="reqn">\times</code> <code>maxiterouter</code> matrix containing the  number 
of inner iterations for each <code>lambda</code> value and each outer iteration and  
<code>iter</code> is total number of iterations i.e. <code>sum(Iter)</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Adam Lund
</p>
<p>Maintainer: Adam Lund, <a href="mailto:adam.lund@math.ku.dk">adam.lund@math.ku.dk</a>
</p>


<h3>References</h3>

<p>Lund, A., M. Vincent, and N. R. Hansen (2017). Penalized estimation in 
large-scale generalized linear array models. 
<em>Journal of Computational and Graphical Statistics</em>, 26, 3, 709-724.  url = https://doi.org/10.1080/10618600.2017.1279548.
</p>
<p>Currie, I. D., M. Durban, and P. H. C. Eilers (2006). Generalized linear
array models with applications to multidimensional smoothing. 
<em>Journal of the Royal Statistical Society. Series B</em>. 68, 259-280. url = http://dx.doi.org/10.1111/j.1467-9868.2006.00543.x.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
##size of example 
n1 &lt;- 65; n2 &lt;- 26; n3 &lt;- 13; p1 &lt;- 12; p2 &lt;- 6; p3 &lt;- 4

##marginal design matrices (tensor components)
X1 &lt;- matrix(rnorm(n1 * p1), n1, p1) 
X2 &lt;- matrix(rnorm(n2 * p2), n2, p2) 
X3 &lt;- matrix(rnorm(n3 * p3), n3, p3) 
X &lt;- list(X1, X2, X3)

##############gaussian example 
Beta &lt;- array(rnorm(p1 * p2 * p3) * rbinom(p1 * p2 * p3, 1, 0.1), c(p1 , p2, p3))
Mu &lt;- RH(X3, RH(X2, RH(X1, Beta)))
Y &lt;- array(rnorm(n1 * n2 * n3, Mu), c(n1, n2, n3))

system.time(fit &lt;- glamlasso(X, Y))

modelno &lt;- length(fit$lambda)
plot(c(Beta), type = "h", ylim = range(Beta, fit$coef[, modelno]))
points(c(Beta))
lines(fit$coef[ , modelno], col = "red", type = "h")

###with non tensor design component Z
q &lt;- 5
alpha &lt;- matrix(rnorm(q)) * rbinom(q, 1, 0.5)
Z &lt;- matrix(rnorm(n1 * n2 * n3 * q), n1 * n2 *n3, q) 
Y &lt;- array(rnorm(n1 * n2 * n3, Mu + array(Z %*% alpha, c(n1, n2, n3))), c(n1, n2, n3))
system.time(fit &lt;- glamlasso(X, Y, Z))

modelno &lt;- length(fit$lambda)
oldmfrow &lt;- par()$mfrow
par(mfrow = c(1, 2))
plot(c(Beta), type = "l", ylim = range(Beta, fit$coef[, modelno]))
points(c(Beta))
lines(fit$coef[ , modelno], col = "red")
plot(c(alpha), type = "h", ylim = range(Beta, fit$alpha[, modelno]))
points(c(alpha))
lines(fit$alpha[ , modelno], col = "red", type = "h")
par(mfrow = oldmfrow)

################ poisson example
Beta &lt;- array(rnorm(p1 * p2 * p3, 0, 0.1) * rbinom(p1 * p2 * p3, 1, 0.1), c(p1 , p2, p3))
Mu &lt;- RH(X3, RH(X2, RH(X1, Beta)))
Y &lt;- array(rpois(n1 * n2 * n3, exp(Mu)), dim = c(n1, n2, n3))
system.time(fit &lt;- glamlasso(X, Y, family = "poisson", nu = 0.1))

modelno &lt;- length(fit$lambda)
plot(c(Beta), type = "h", ylim = range(Beta, fit$coef[, modelno]))
points(c(Beta))
lines(fit$coef[ , modelno], col = "red", type = "h")

###with non tensor design component Z
q &lt;- 5
alpha &lt;- matrix(rnorm(q)) * rbinom(q, 1, 0.5)
Z &lt;- matrix(rnorm(n1 * n2 * n3 * q), n1 * n2 *n3, q) 
Y &lt;- array(rpois(n1 * n2 * n3, exp(Mu + array(Z %*% alpha, c(n1, n2, n3)))), dim = c(n1, n2, n3))
system.time(fit &lt;- glamlasso(X, Y, Z, family = "poisson", nu = 0.1))

modelno &lt;- length(fit$lambda)
oldmfrow &lt;- par()$mfrow
par(mfrow = c(1, 2))
plot(c(Beta), type = "l", ylim = range(Beta, fit$coef[, modelno]))
points(c(Beta))
lines(fit$coef[ , modelno], col = "red")
plot(c(alpha), type = "h", ylim = range(Beta, fit$alpha[, modelno]))
points(c(alpha))
lines(fit$alpha[ , modelno], col = "red", type = "h")
par(mfrow = oldmfrow)

</code></pre>

<hr>
<h2 id='glamlasso_internal'>Internal glamlasso Functions</h2><span id='topic+glamlasso_internal'></span><span id='topic+mu'></span><span id='topic+gdpg'></span><span id='topic+getobj'></span>

<h3>Description</h3>

<p>Internal glamlasso functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mu(x, fam)
</code></pre>


<h3>Details</h3>

<p>These functions are not intended for use by users.
</p>


<h3>Author(s)</h3>

<p>Adam Lund
</p>

<hr>
<h2 id='glamlassoRR'>Penalized reduced rank regression  in a GLAM</h2><span id='topic+glamlassoRR'></span>

<h3>Description</h3>

<p>Efficient design matrix free procedure for fitting large scale penalized reduced rank
regressions in a 3-dimensional generalized linear array model. To obtain a factorization of the parameter array, 
the <code>glamlassoRR</code> function performes a block relaxation scheme within the gdpg algorithm, see <cite>Lund and Hansen, 2018</cite>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>glamlassoRR(X, 
            Y, 
            Z = NULL,
            family = "gaussian",
            penalty = "lasso",
            intercept = FALSE,
            weights = NULL,
            betainit = NULL,
            alphainit = NULL,
            nlambda = 100,
            lambdaminratio = 1e-04,
            lambda = NULL,
            penaltyfactor = NULL,
            penaltyfactoralpha = NULL,
            reltolinner = 1e-07,
            reltolouter = 1e-04,
            reltolalt = 1e-04,
            maxiter = 15000,
            steps = 1,
            maxiterinner = 3000,
            maxiterouter = 25,
            maxalt = 10,
            btinnermax = 100,
            btoutermax  = 100,
            iwls = "exact",
            nu = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="glamlassoRR_+3A_x">X</code></td>
<td>
<p>A list containing the 3 tensor components of the tensor design matrix. These are  matrices of sizes <code class="reqn">n_i   \times p_i</code>.</p>
</td></tr>
<tr><td><code id="glamlassoRR_+3A_y">Y</code></td>
<td>
<p>The response values, an array of size <code class="reqn">n_1 \times n_2\times n_3</code>. For option 
<code>family = "binomial"</code> this array must contain the proportion of successes and the 
number of trials is then specified as <code>weights</code> (see below).</p>
</td></tr>
<tr><td><code id="glamlassoRR_+3A_z">Z</code></td>
<td>
<p>The non tensor structrured part of the design matrix. A matrix of size <code class="reqn">n_1 n_2 n_3\times q</code>. 
Is set to <code>NULL</code> as default.</p>
</td></tr>
<tr><td><code id="glamlassoRR_+3A_family">family</code></td>
<td>
<p>A string specifying the model family (essentially the response distribution). Possible values 
are <code>"gaussian", "binomial", "poisson", "gamma"</code>.</p>
</td></tr>
<tr><td><code id="glamlassoRR_+3A_penalty">penalty</code></td>
<td>
<p>A string specifying the penalty. Possible values are <code>"lasso", "scad"</code>.</p>
</td></tr>
<tr><td><code id="glamlassoRR_+3A_intercept">intercept</code></td>
<td>
<p>Logical variable indicating if the model includes an intercept.  When <code>intercept = TRUE</code> the first 
coulmn in the non-tensor design component <code>Z</code> is all 1s. Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="glamlassoRR_+3A_weights">weights</code></td>
<td>
<p>Observation weights, an array of size <code class="reqn">n_1 \times \cdots \times n_d</code>. For option 
<code>family = "binomial"</code> this array must contain the number of trials and must be provided.</p>
</td></tr>
<tr><td><code id="glamlassoRR_+3A_betainit">betainit</code></td>
<td>
<p>A list (length 2) containing the initial parameter values for each of the parameter factors. 
Default is NULL in which case all parameters are initialized at 0.01.</p>
</td></tr>
<tr><td><code id="glamlassoRR_+3A_alphainit">alphainit</code></td>
<td>
<p>A <code class="reqn">q\times 1</code> vector containing the initial parameter values for the non-tensor parameter. 
Default is NULL in which case all parameters are initialized at 0.</p>
</td></tr>
<tr><td><code id="glamlassoRR_+3A_nlambda">nlambda</code></td>
<td>
<p>The number of <code>lambda</code> values.</p>
</td></tr>
<tr><td><code id="glamlassoRR_+3A_lambdaminratio">lambdaminratio</code></td>
<td>
<p>The smallest value for <code>lambda</code>, given as a fraction of 
<code class="reqn">\lambda_{max}</code>; the (data derived) smallest value for which all coefficients are zero.</p>
</td></tr>
<tr><td><code id="glamlassoRR_+3A_lambda">lambda</code></td>
<td>
<p>The sequence of penalty parameters for the regularization path.</p>
</td></tr>
<tr><td><code id="glamlassoRR_+3A_penaltyfactor">penaltyfactor</code></td>
<td>
<p>A list of length two containing an array of size <code class="reqn">p_1 \times  p_2</code> and a <code class="reqn">p_3 \times  1</code> vector.
Multiplied  with each element in <code>lambda</code> to allow differential shrinkage on the (tensor) coefficients blocks.</p>
</td></tr>
<tr><td><code id="glamlassoRR_+3A_penaltyfactoralpha">penaltyfactoralpha</code></td>
<td>
<p>A <code class="reqn">q \times 1</code> vector multiplied with each element in <code>lambda</code> to allow differential shrinkage on the non-tensor coefficients.</p>
</td></tr>
<tr><td><code id="glamlassoRR_+3A_reltolinner">reltolinner</code></td>
<td>
<p>The convergence tolerance for the inner loop</p>
</td></tr>
<tr><td><code id="glamlassoRR_+3A_reltolouter">reltolouter</code></td>
<td>
<p>The convergence tolerance for the outer loop.</p>
</td></tr>
<tr><td><code id="glamlassoRR_+3A_reltolalt">reltolalt</code></td>
<td>
<p>The convergence tolerance for the alternation loop over the two parameter blocks.</p>
</td></tr>
<tr><td><code id="glamlassoRR_+3A_maxiter">maxiter</code></td>
<td>
<p>The maximum number of inner iterations allowed for each <code>lambda</code>
value, when  summing over all outer iterations for said <code>lambda</code>.</p>
</td></tr>
<tr><td><code id="glamlassoRR_+3A_steps">steps</code></td>
<td>
<p>The number of steps used in the multi-step adaptive lasso algorithm for non-convex penalties. Automatically set to 1 when <code>penalty = "lasso"</code>.</p>
</td></tr>
<tr><td><code id="glamlassoRR_+3A_maxiterinner">maxiterinner</code></td>
<td>
<p>The maximum number of inner iterations allowed for each outer iteration.</p>
</td></tr>
<tr><td><code id="glamlassoRR_+3A_maxiterouter">maxiterouter</code></td>
<td>
<p>The maximum number of outer iterations allowed for each lambda.</p>
</td></tr>
<tr><td><code id="glamlassoRR_+3A_maxalt">maxalt</code></td>
<td>
<p>The maximum number of  alternations over parameter blocks.</p>
</td></tr>
<tr><td><code id="glamlassoRR_+3A_btinnermax">btinnermax</code></td>
<td>
<p>Maximum number of backtracking steps allowed in each inner iteration. Default is <code>btinnermax = 100</code>.</p>
</td></tr>
<tr><td><code id="glamlassoRR_+3A_btoutermax">btoutermax</code></td>
<td>
<p>Maximum number of backtracking steps allowed in each outer iteration. Default is <code>btoutermax = 100</code>.</p>
</td></tr>
<tr><td><code id="glamlassoRR_+3A_iwls">iwls</code></td>
<td>
<p>A string indicating whether to use the exact iwls weight matrix or use a tensor structured approximation to it.</p>
</td></tr>
<tr><td><code id="glamlassoRR_+3A_nu">nu</code></td>
<td>
<p>A number between 0 and 1 that controls the step size <code class="reqn">\delta</code> in the proximal algorithm (inner loop) by 
scaling the upper bound <code class="reqn">\hat{L}_h</code> on the Lipschitz constant <code class="reqn">L_h</code> (see <cite>Lund et al., 2017</cite>). 
For <code>nu = 1</code> backtracking never occurs and the proximal step size is always <code class="reqn">\delta = 1 / \hat{L}_h</code>. 
For <code>nu = 0</code> backtracking always occurs and the proximal step size is initially <code class="reqn">\delta = 1</code>. 
For <code>0 &lt; nu &lt; 1</code> the proximal step size is initially <code class="reqn">\delta = 1/(\nu\hat{L}_h)</code> and backtracking 
is only employed if the objective function does not decrease. A <code>nu</code> close  to 0 gives large step 
sizes and presumably more backtracking in the inner loop. The default is <code>nu = 1</code> and the option is only 
used if <code>iwls = "exact"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Given the setting from <code><a href="#topic+glamlasso">glamlasso</a></code> we place a reduced rank
restriction on the <code class="reqn">p_1\times p_2\times p _3</code> parameter array <code class="reqn">B</code> 
given by
</p>
<p style="text-align: center;"><code class="reqn">B=(B_{i,j,k})_{i,j,k} = (\gamma_{k}\kappa_{i,j})_{i,j,k}, \ \ \ \gamma_k,\kappa_{i,j}\in \mathcal{R}.</code>
</p>
  
<p>The  <code>glamlassoRR</code> function  solves the PMLE problem by combining a 
block relaxation scheme with the gdpg algorithm. This scheme alternates 
between  optimizing over the first parameter block <code class="reqn">\kappa=(\kappa_{i,j})_{i,j}</code> 
and  the second block <code class="reqn">\gamma=(\gamma_k)_k</code> while fixing the second resp. 
first block. 
</p>
<p>Note that the individual parameter blocks are only identified up to a 
multiplicative constant. Also note that the algorithm is sensitive to
inital values <code>betainit</code> which can prevent convergence.
</p>


<h3>Value</h3>

<p>An object with S3 Class &quot;glamlasso&quot;. 
</p>
<table>
<tr><td><code>spec</code></td>
<td>
<p>A string indicating the model family and the penalty.</p>
</td></tr>  
<tr><td><code>coef12</code></td>
<td>
<p>A <code class="reqn">p_1 p_2 \times</code> <code>nlambda</code> matrix containing the 
estimates of the first model coefficient factor  (<code class="reqn">\kappa</code>) for each 
<code>lambda</code>-value.</p>
</td></tr>
<tr><td><code>coef3</code></td>
<td>
<p>A <code class="reqn">p_3 \times</code> <code>nlambda</code> matrix containing the 
estimates of the second model coefficient factor  (<code class="reqn">\gamma</code>) for each 
<code>lambda</code>-value.</p>
</td></tr>
<tr><td><code>alpha</code></td>
<td>
<p>A <code class="reqn">q \times</code> <code>nlambda</code> matrix containing the estimates 
of the parameters for the non tensor structured part of the model 
(<code>alpha</code>) for each <code>lambda</code>-value. If <code>intercept = TRUE</code> the 
first row contains the intercept estimate for each <code>lambda</code>-value.</p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>
<p>A vector containing the sequence of penalty values used in the 
estimation procedure.</p>
</td></tr>
<tr><td><code>df</code></td>
<td>
<p>The number of nonzero coefficients for each value of <code>lambda</code>.</p>
</td></tr>	
<tr><td><code>dimcoef</code></td>
<td>
<p>A vector giving the dimension of the model coefficient array 
<code class="reqn">\beta</code>.</p>
</td></tr>
<tr><td><code>dimobs</code></td>
<td>
<p>A vector giving the dimension of the observation (response) 
array <code>Y</code>.</p>
</td></tr>
<tr><td><code>Iter</code></td>
<td>
<p>A list with 4 items: <code>bt_iter_inner</code>  is total number of 
backtracking steps performed in the inner loop, <code>bt_enter_inner</code> is the 
number of times the backtracking is initiated in the inner loop, 
<code>bt_iter_outer</code> is total number of backtracking steps performed in the 
outer loop, and <code>iter_mat</code> is a <code>nlambda</code> <code class="reqn">\times</code> 
<code>maxiterouter</code> matrix containing the  number of inner iterations for 
each <code>lambda</code> value and each outer iteration and  <code>iter</code> is total 
number of iterations i.e. <code>sum(Iter)</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Adam Lund
</p>
<p>Maintainer: Adam Lund, <a href="mailto:adam.lund@math.ku.dk">adam.lund@math.ku.dk</a>
</p>


<h3>References</h3>

<p>Lund, A., M. Vincent, and N. R. Hansen (2017). Penalized estimation in 
large-scale generalized linear array models. 
<em>Journal of Computational and Graphical Statistics</em>, 26, 3, 709-724.  url = https://doi.org/10.1080/10618600.2017.1279548.
</p>
<p>Lund, A. and N. R. Hansen (2019). Sparse Network  Estimation for  Dynamical Spatio-temporal Array Models. 
<em>Journal of Multivariate Analysis</em>, 174. url = https://doi.org/10.1016/j.jmva.2019.104532.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> 
##size of example 
n1 &lt;- 65; n2 &lt;- 26; n3 &lt;- 13; p1 &lt;- 12; p2 &lt;- 6; p3 &lt;- 4

##marginal design matrices (tensor components)
X1 &lt;- matrix(rnorm(n1 * p1), n1, p1) 
X2 &lt;- matrix(rnorm(n2 * p2), n2, p2) 
X3 &lt;- matrix(rnorm(n3 * p3), n3, p3) 
X &lt;- list(X1, X2, X3)
Beta12 &lt;- matrix(rnorm(p1 * p2), p1, p2) * matrix(rbinom(p1 * p2, 1, 0.5), p1, p2)
Beta3 &lt;- matrix(rnorm(p3) * rbinom(p3, 1, 0.5), p3, 1)
Beta &lt;- outer(Beta12, c(Beta3))
Mu &lt;- RH(X3, RH(X2, RH(X1, Beta)))
Y &lt;- array(rnorm(n1 * n2 * n3, Mu), dim = c(n1, n2, n3))  

system.time(fit &lt;- glamlassoRR(X, Y))

modelno  &lt;- length(fit$lambda)
oldmfrow &lt;- par()$mfrow
par(mfrow = c(1, 3))
plot(c(Beta), type = "h")
points(c(Beta))
lines(c(outer(fit$coef12[, modelno], c(fit$coef3[, modelno]))), col = "red", type = "h")
plot(c(Beta12), ylim = range(Beta12, fit$coef12[, modelno]), type = "h")
points(c(Beta12))
lines(fit$coef12[, modelno], col = "red", type = "h")
plot(c(Beta3), ylim = range(Beta3, fit$coef3[, modelno]), type = "h")
points(c(Beta3))
lines(fit$coef3[, modelno], col = "red", type = "h")
par(mfrow = oldmfrow)

###with non tensor design component Z
q &lt;- 5
alpha &lt;- matrix(rnorm(q)) * rbinom(q, 1, 0.5)
Z &lt;- matrix(rnorm(n1 * n2 * n3 * q), n1 * n2 * n3, q) 
Y &lt;- array(rnorm(n1 * n2 * n3, Mu + array(Z %*% alpha, c(n1, n2, n3))), c(n1, n2, n3))
system.time(fit &lt;- glamlassoRR(X, Y, Z))

modelno &lt;- length(fit$lambda)
oldmfrow &lt;- par()$mfrow
par(mfrow = c(2, 2))
plot(c(Beta), type = "h")
points(c(Beta))
lines(c(outer(fit$coef12[, modelno], c(fit$coef3[, modelno]))), col = "red", type = "h")
plot(c(Beta12), ylim = range(Beta12,fit$coef12[, modelno]), type = "h")
points(c(Beta12))
lines(fit$coef12[, modelno], col = "red", type = "h")
plot(c(Beta3), ylim = range(Beta3, fit$coef3[, modelno]), type = "h")
points(c(Beta3))
lines(fit$coef3[, modelno], col = "red", type = "h")
plot(c(alpha), ylim = range(alpha, fit$alpha[, modelno]), type = "h")
points(c(alpha))
lines(fit$alpha[, modelno], col = "red", type = "h")
par(mfrow = oldmfrow)

################ poisson example
set.seed(7954) ## for this seed the algorithm fails to converge for default initial values!!
set.seed(42)
##size of example 
n1 &lt;- 65; n2 &lt;- 26; n3 &lt;- 13; p1 &lt;- 12; p2 &lt;- 6; p3 &lt;- 4

##marginal design matrices (tensor components)
X1 &lt;- matrix(rnorm(n1 * p1), n1, p1) 
X2 &lt;- matrix(rnorm(n2 * p2), n2, p2) 
X3 &lt;- matrix(rnorm(n3 * p3), n3, p3) 
X &lt;- list(X1, X2, X3)

Beta12 &lt;- matrix(rnorm(p1 * p2, 0, 0.5) * rbinom(p1 * p2, 1, 0.1), p1, p2) 
Beta3 &lt;-  matrix(rnorm(p3, 0, 0.5) * rbinom(p3, 1, 0.5), p3, 1)
Beta &lt;- outer(Beta12, c(Beta3))
Mu &lt;- RH(X3, RH(X2, RH(X1, Beta)))
Y &lt;- array(rpois(n1 * n2 * n3, exp(Mu)), dim = c(n1, n2, n3))
system.time(fit &lt;- glamlassoRR(X, Y ,family = "poisson"))
modelno &lt;- length(fit$lambda)
oldmfrow &lt;- par()$mfrow
par(mfrow = c(1, 3))
plot(c(Beta), type = "h")
points(c(Beta))
lines(c(outer(fit$coef12[, modelno], c(fit$coef3[, modelno]))), col = "red", type = "h")
plot(c(Beta12), ylim = range(Beta12, fit$coef12[, modelno]), type = "h")
points(c(Beta12))
lines(fit$coef12[, modelno], col = "red", type = "h")
plot(c(Beta3), ylim = range(Beta3, fit$coef3[, modelno]), type = "h")
points(c(Beta3))
lines(fit$coef3[, modelno], col = "red", type = "h")
par(mfrow = oldmfrow)


</code></pre>

<hr>
<h2 id='glamlassoS'>Penalization in Large Scale Generalized Linear Array Models</h2><span id='topic+glamlassoS'></span>

<h3>Description</h3>

<p>Efficient design matrix free procedure for fitting   a special case of  a generalized linear  model 
with  array structured response and partially tensor structured covariates.  See <cite>Lund and Hansen, 2019</cite> for an application of this special purpose function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>glamlassoS(X, 
           Y,
           V, 
           Z = NULL,
           family = "gaussian",
           penalty = "lasso",
           intercept = FALSE,
           weights = NULL,
           betainit = NULL,
           alphainit = NULL,
           nlambda = 100,
           lambdaminratio = 1e-04,
           lambda = NULL,
           penaltyfactor = NULL,
           penaltyfactoralpha = NULL,
           reltolinner = 1e-07,
           reltolouter = 1e-04,
           maxiter = 15000,
           steps = 1,
           maxiterinner = 3000,
           maxiterouter = 25,
           btinnermax = 100,
           btoutermax = 100,
           iwls = "exact",
           nu = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="glamlassoS_+3A_x">X</code></td>
<td>
<p>A list containing the tensor components (2 or 3) of the tensor design matrix.
These are  matrices of sizes <code class="reqn">n_i   \times p_i</code>.</p>
</td></tr>
<tr><td><code id="glamlassoS_+3A_y">Y</code></td>
<td>
<p>The response values, an array of size <code class="reqn">n_1 \times\cdots\times n_d</code>. For option 
<code>family = "binomial"</code> this array must contain the proportion of successes and the 
number of trials is then specified as <code>weights</code> (see below).</p>
</td></tr>
<tr><td><code id="glamlassoS_+3A_v">V</code></td>
<td>
<p>The weight values, an array of size <code class="reqn">n_1 \times\cdots\times n_d</code>.</p>
</td></tr>
<tr><td><code id="glamlassoS_+3A_z">Z</code></td>
<td>
<p>The non tensor structrured part of the design matrix. A matrix of size <code class="reqn">n_1 \cdots n_d\times q</code>. 
Is set to <code>NULL</code> as default.</p>
</td></tr>
<tr><td><code id="glamlassoS_+3A_family">family</code></td>
<td>
<p>A string specifying the model family (essentially the response distribution). Possible values 
are <code>"gaussian", "binomial", "poisson", "gamma"</code>.</p>
</td></tr>
<tr><td><code id="glamlassoS_+3A_penalty">penalty</code></td>
<td>
<p>A string specifying the penalty. Possible values 
are <code>"lasso", "scad"</code>.</p>
</td></tr>
<tr><td><code id="glamlassoS_+3A_intercept">intercept</code></td>
<td>
<p>Logical variable indicating if the model includes an intercept. 
When <code>intercept = TRUE</code> the first coulmn in the non-tensor design component <code>Z</code> is all 1s.
Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="glamlassoS_+3A_weights">weights</code></td>
<td>
<p>Observation weights, an array of size <code class="reqn">n_1 \times \cdots \times n_d</code>. For option 
<code>family = "binomial"</code> this array must contain the number of trials and must be provided.</p>
</td></tr>
<tr><td><code id="glamlassoS_+3A_betainit">betainit</code></td>
<td>
<p>The initial parameter values. Default is NULL in which case all parameters are initialized at zero.</p>
</td></tr>
<tr><td><code id="glamlassoS_+3A_alphainit">alphainit</code></td>
<td>
<p>A <code class="reqn">q\times 1</code> vector containing the initial parameter values for the non-tensor parameter. 
Default is NULL in which case all parameters are initialized at 0.</p>
</td></tr>
<tr><td><code id="glamlassoS_+3A_nlambda">nlambda</code></td>
<td>
<p>The number of <code>lambda</code> values.</p>
</td></tr>
<tr><td><code id="glamlassoS_+3A_lambdaminratio">lambdaminratio</code></td>
<td>
<p>The smallest value for <code>lambda</code>, given as a fraction of 
<code class="reqn">\lambda_{max}</code>; the (data derived) smallest value for which all coefficients are zero.</p>
</td></tr>
<tr><td><code id="glamlassoS_+3A_lambda">lambda</code></td>
<td>
<p>The sequence of penalty parameters for the regularization path.</p>
</td></tr>
<tr><td><code id="glamlassoS_+3A_penaltyfactor">penaltyfactor</code></td>
<td>
<p>An array of size <code class="reqn">p_1 \times \cdots \times p_d</code>. Is multiplied 
with each element in <code>lambda</code> to allow differential shrinkage on the coefficients.</p>
</td></tr>
<tr><td><code id="glamlassoS_+3A_penaltyfactoralpha">penaltyfactoralpha</code></td>
<td>
<p>A <code class="reqn">q \times 1</code> vector multiplied with each element in <code>lambda</code> to allow differential 
shrinkage on the non-tensor coefficients.</p>
</td></tr>
<tr><td><code id="glamlassoS_+3A_reltolinner">reltolinner</code></td>
<td>
<p>The convergence tolerance for the inner loop</p>
</td></tr>
<tr><td><code id="glamlassoS_+3A_reltolouter">reltolouter</code></td>
<td>
<p>The convergence tolerance for the outer loop.</p>
</td></tr>
<tr><td><code id="glamlassoS_+3A_maxiter">maxiter</code></td>
<td>
<p>The maximum number of inner iterations allowed for each <code>lambda</code>
value, when  summing over all outer iterations for said <code>lambda</code>.</p>
</td></tr>
<tr><td><code id="glamlassoS_+3A_steps">steps</code></td>
<td>
<p>The number of steps used in the multi-step adaptive lasso algorithm for non-convex penalties. Automatically set to 1 when <code>penalty = "lasso"</code>.</p>
</td></tr>
<tr><td><code id="glamlassoS_+3A_maxiterinner">maxiterinner</code></td>
<td>
<p>The maximum number of inner iterations allowed for each outer iteration.</p>
</td></tr>
<tr><td><code id="glamlassoS_+3A_maxiterouter">maxiterouter</code></td>
<td>
<p>The maximum number of outer iterations allowed for each lambda.</p>
</td></tr>
<tr><td><code id="glamlassoS_+3A_btinnermax">btinnermax</code></td>
<td>
<p>Maximum number of backtracking steps allowed in each inner iteration. Default is <code>btinnermax = 100</code>.</p>
</td></tr>
<tr><td><code id="glamlassoS_+3A_btoutermax">btoutermax</code></td>
<td>
<p>Maximum number of backtracking steps allowed in each outer iteration. Default is <code>btoutermax = 100</code>.</p>
</td></tr>
<tr><td><code id="glamlassoS_+3A_iwls">iwls</code></td>
<td>
<p>A string indicating whether to use the exact iwls weight matrix or use a kronecker structured approximation to it.</p>
</td></tr>
<tr><td><code id="glamlassoS_+3A_nu">nu</code></td>
<td>
<p>A number between 0 and 1 that controls the step size <code class="reqn">\delta</code> in the proximal algorithm (inner loop) by 
scaling the upper bound <code class="reqn">\hat{L}_h</code> on the Lipschitz constant <code class="reqn">L_h</code> (see <cite>Lund et al., 2017</cite>). 
For <code>nu = 1</code> backtracking never occurs and the proximal step size is always <code class="reqn">\delta = 1 / \hat{L}_h</code>. 
For <code>nu = 0</code> backtracking always occurs and the proximal step size is initially <code class="reqn">\delta = 1</code>. 
For <code>0 &lt; nu &lt; 1</code> the proximal step size is initially <code class="reqn">\delta = 1/(\nu\hat{L}_h)</code> and backtracking 
is only employed if the objective function does not decrease. A <code>nu</code> close  to 0 gives large step 
sizes and presumably more backtracking in the inner loop. The default is <code>nu = 1</code> and the option is only 
used if <code>iwls = "exact"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Given the setting from <code><a href="#topic+glamlasso">glamlasso</a></code> we consider a model
where the tensor design component is only partially tensor structured as
</p>
<p style="text-align: center;"><code class="reqn">X = [V_1X_2^\top\otimes X_1^\top,\ldots,V_{n_3}X_2^\top\otimes X_1^\top]^\top.</code>
</p>
 
<p>Here  <code class="reqn">X_i</code> is a <code class="reqn">n_i\times p_i</code> matrix  for <code class="reqn">i=1,2</code> and  <code class="reqn">V_i</code> is a <code class="reqn">n_1n_2\times n_1n_2</code> diagonal matrix for <code class="reqn">i=1,\ldots,n_3</code>.
</p>
<p>Letting <code class="reqn">Y</code> denote the  <code class="reqn">n_1\times n_2\times n_3</code> response array and <code class="reqn">V</code> the  <code class="reqn">n_1\times n_2\times n_3</code>  weight array containing the diagonals of the <code class="reqn">V_i</code>s, 
the function <code>glamlassoS</code> solves the PMLE problem using  <code class="reqn">Y, V, X_1, X_2</code> and the non-tensor component <code class="reqn">Z</code> as input.
</p>


<h3>Value</h3>

<p>An object with S3 Class &quot;glamlasso&quot;. 
</p>
<table>
<tr><td><code>spec</code></td>
<td>
<p>A string indicating the model family and the penalty.</p>
</td></tr>  
<tr><td><code>beta</code></td>
<td>
<p>A <code class="reqn">p_1\cdots p_d \times</code> <code>nlambda</code> matrix containing the 
estimates of the parameters for the tensor structured part of the model 
(<code>beta</code>) for each <code>lambda</code>-value.</p>
</td></tr>
<tr><td><code>alpha</code></td>
<td>
<p>A <code class="reqn">q \times</code> <code>nlambda</code> matrix containing the estimates 
of the parameters for the non tensor structured part of the model 
(<code>alpha</code>) for each <code>lambda</code>-value. If <code>intercept = TRUE</code> the 
first row contains the intercept estimate for each <code>lambda</code>-value.</p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>
<p>A vector containing the sequence of penalty values used in the 
estimation procedure.</p>
</td></tr>
<tr><td><code>df</code></td>
<td>
<p>The number of nonzero coefficients for each value of <code>lambda</code>.</p>
</td></tr>	
<tr><td><code>dimcoef</code></td>
<td>
<p>A vector giving the dimension of the model coefficient array 
<code class="reqn">\beta</code>.</p>
</td></tr>
<tr><td><code>dimobs</code></td>
<td>
<p>A vector giving the dimension of the observation (response) 
array <code>Y</code>.</p>
</td></tr>
<tr><td><code>Iter</code></td>
<td>
<p>A list with 4 items:  <code>bt_iter_inner</code>  is total number of 
backtracking steps performed in the inner loop, <code>bt_enter_inner</code> is the 
number of times the backtracking is initiated in the inner loop,
<code>bt_iter_outer</code> is total number of backtracking steps performed in the 
outer loop, and <code>iter_mat</code> is a <code>nlambda</code> <code class="reqn">\times</code> 
<code>maxiterouter</code> matrix containing the  number of inner iterations for 
each <code>lambda</code> value and each outer iteration and  <code>iter</code> is total 
number of iterations i.e. <code>sum(Iter)</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Adam Lund
</p>
<p>Maintainer: Adam Lund, <a href="mailto:adam.lund@math.ku.dk">adam.lund@math.ku.dk</a>
</p>


<h3>References</h3>

<p>Lund, A., M. Vincent, and N. R. Hansen (2017). Penalized estimation in 
large-scale generalized linear array models. 
<em>Journal of Computational and Graphical Statistics</em>, 26, 3, 709-724.  url = https://doi.org/10.1080/10618600.2017.1279548.
</p>
<p>Lund, A. and N. R. Hansen (2019). Sparse Network  Estimation for  Dynamical Spatio-temporal Array Models. 
<em>Journal of Multivariate Analysis</em>, 174. url = https://doi.org/10.1016/j.jmva.2019.104532.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
##size of example
n1 &lt;- 65; n2 &lt;- 26; n3 &lt;- 13; p1 &lt;- 13; p2 &lt;- 5; 

##marginal design matrices (tensor components)
X1 &lt;- matrix(rnorm(n1 * p1), n1, p1)
X2 &lt;- matrix(rnorm(n2 * p2), n2, p2)
X &lt;- list(X1, X2)
V &lt;- array(rnorm(n3 * n2 * n1), c(n1, n2, n3))

##gaussian example
Beta &lt;- array(rnorm(p1 * p2) * rbinom(p1 * p2, 1, 0.1), c(p1 , p2))
Mu &lt;- V * array(RH(X2, RH(X1, Beta)), c(n1, n2, n3))
Y &lt;- array(rnorm(n1 * n2 * n3, Mu), c(n1, n2, n3))
system.time(fit &lt;- glamlassoS(X, Y, V))

modelno &lt;- length(fit$lambda)
plot(c(Beta), ylim = range(Beta, fit$coef[, modelno]), type = "h")
points(c(Beta))
lines(c(fit$coef[, modelno]), col = "red", type = "h")

###with non tensor design component Z
q &lt;- 5
alpha &lt;- matrix(rnorm(q)) * rbinom(q, 1, 0.5)
Z &lt;- matrix(rnorm(n1 * n2 * n3 * q), n1 * n2 *n3, q) 
Y &lt;- array(rnorm(n1 * n2 * n3, Mu + array(Z %*% alpha, c(n1, n2, n3))), c(n1, n2, n3))
system.time(fit &lt;- glamlassoS(X, Y, V , Z))

modelno &lt;- length(fit$lambda)
oldmfrow &lt;- par()$mfrow
par(mfrow = c(1, 2))
plot(c(Beta), type="h", ylim = range(Beta, fit$coef[, modelno]))
points(c(Beta))
lines(fit$coef[ , modelno], col = "red", type = "h")
plot(c(alpha), type = "h", ylim = range(alpha, fit$alpha[, modelno]))
points(c(alpha))
lines(fit$alpha[ , modelno], col = "red", type = "h")
par(mfrow = oldmfrow)

################ poisson example
Beta &lt;- matrix(rnorm(p1 * p2, 0, 0.1) * rbinom(p1 * p2, 1, 0.1), p1 , p2)
Mu &lt;- V * array(RH(X2, RH(X1, Beta)), c(n1, n2, n3))
Y &lt;- array(rpois(n1 * n2 * n3, exp(Mu)), dim = c(n1, n2, n3))
system.time(fit &lt;- glamlassoS(X, Y, V, family = "poisson", nu = 0.1))

modelno &lt;- length(fit$lambda)
plot(c(Beta), type = "h", ylim = range(Beta, fit$coef[, modelno]))
points(c(Beta))
lines(fit$coef[ , modelno], col = "red", type = "h")

</code></pre>

<hr>
<h2 id='objective'>Compute objective values</h2><span id='topic+objective'></span><span id='topic+glamlasso_objective'></span>

<h3>Description</h3>

<p>Computes the objective values of the penalized log-likelihood problem
for the models implemented in the package glamlasso.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>objective(Y, 
          Weights, 
          X, 
          Beta, 
          lambda,
          penalty.factor, 
          family,
          penalty)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="objective_+3A_y">Y</code></td>
<td>
<p>The response values, an array of size <code class="reqn">n_1 \times \cdots \times n_d</code>.</p>
</td></tr>
<tr><td><code id="objective_+3A_weights">Weights</code></td>
<td>
<p>Observation weights, an array of size <code class="reqn">n_1 \times \cdots \times n_d</code>.</p>
</td></tr>
<tr><td><code id="objective_+3A_x">X</code></td>
<td>
<p>A list containing the tensor components of the tensor design matrix, each of size
<code class="reqn">n_i \times p_i</code>.</p>
</td></tr>
<tr><td><code id="objective_+3A_beta">Beta</code></td>
<td>
<p>A coefficient matrix of size <code class="reqn">p_1\cdots p_d \times </code><code>nlambda</code>.</p>
</td></tr>
<tr><td><code id="objective_+3A_lambda">lambda</code></td>
<td>
<p>The sequence of penalty parameters for the regularization path.</p>
</td></tr>
<tr><td><code id="objective_+3A_penalty.factor">penalty.factor</code></td>
<td>
<p>An array of size  <code class="reqn">p_1 \times \cdots \times p_d</code>. Is multiplied with each 
element in  <code>lambda</code> to allow differential shrinkage on the coefficients.</p>
</td></tr>
<tr><td><code id="objective_+3A_family">family</code></td>
<td>
<p>A string specifying the model family (essentially the response distribution).</p>
</td></tr>
<tr><td><code id="objective_+3A_penalty">penalty</code></td>
<td>
<p>A string specifying the penalty.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of length <code>length(lambda)</code> containing the objective values for each <code>lambda</code> value.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
n1 &lt;- 65; n2 &lt;- 26; n3 &lt;- 13; p1 &lt;- 13; p2 &lt;- 5; p3 &lt;- 4
X1 &lt;- matrix(rnorm(n1 * p1), n1, p1) 
X2 &lt;- matrix(rnorm(n2 * p2), n2, p2) 
X3 &lt;- matrix(rnorm(n3 * p3), n3, p3) 
Beta &lt;- array(rnorm(p1 * p2 * p3) * rbinom(p1 * p2 * p3, 1, 0.1), c(p1 , p2, p3))
mu &lt;- RH(X3, RH(X2, RH(X1, Beta)))
Y &lt;- array(rnorm(n1 * n2 * n3, mu), dim = c(n1, n2, n3))
fit &lt;- glamlasso(list(X1, X2, X3), Y, family = "gaussian", penalty = "lasso", iwls = "exact")
objfit &lt;- objective(Y, NULL, list(X1, X2, X3), fit$coef, fit$lambda, NULL, fit$family)
plot(objfit, type = "l")

## End(Not run)
</code></pre>

<hr>
<h2 id='predict.glamlasso'>Make Prediction From a glamlasso Object</h2><span id='topic+predict.glamlasso'></span>

<h3>Description</h3>

<p>Given new covariate data this function computes the linear predictors 
based on the estimated model coefficients in an object produced by the function <code>glamlasso</code>. Note that the 
data can be supplied in two different formats: i) as a <code class="reqn">n' \times p</code> matrix (<code class="reqn">p</code> is the number of model 
coefficients and <code class="reqn">n'</code> is the number of new data points) or ii) as a list of two or three matrices each of 
size <code class="reqn">n_i' \times p_i, i = 1, 2, 3</code> (<code class="reqn">n_i'</code> is the number of new marginal data points in the <code class="reqn">i</code>th dimension).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'glamlasso'
predict(object, x = NULL, X = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.glamlasso_+3A_object">object</code></td>
<td>
<p>An object of Class glamlasso, produced with <code>glamlasso</code>.</p>
</td></tr>
<tr><td><code id="predict.glamlasso_+3A_x">x</code></td>
<td>
<p>a matrix of size <code class="reqn">n' \times p</code> with <code class="reqn">n'</code> is the number of new data points.</p>
</td></tr>
<tr><td><code id="predict.glamlasso_+3A_x">X</code></td>
<td>
<p>A list containing the data matrices each of size <code class="reqn">n'_{i} \times p_i</code>,
where <code class="reqn">n'_{i}</code> is the number of new data points in  the <code class="reqn">i</code>th dimension.</p>
</td></tr>
<tr><td><code id="predict.glamlasso_+3A_...">...</code></td>
<td>
<p>ignored</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of length <code>nlambda</code> containing the linear predictors for each  model. If 
new covariate data is supplied in one <code class="reqn">n' \times p</code> matrix <code>x</code> each 
item  is a vector of length <code class="reqn">n'</code>. If the data is supplied as a list of 
matrices each of size <code class="reqn">n'_{i} \times p_i</code>,  each item is an array of size <code class="reqn">n'_1 \times \cdots \times n'_d</code>, with <code class="reqn">d\in \{2,3\}</code>.
</p>


<h3>Author(s)</h3>

<p>Adam Lund
</p>


<h3>Examples</h3>

<pre><code class='language-R'> 
n1 &lt;- 65; n2 &lt;- 26; n3 &lt;- 13; p1 &lt;- 13; p2 &lt;- 5; p3 &lt;- 4
X1 &lt;- matrix(rnorm(n1 * p1), n1, p1) 
X2 &lt;- matrix(rnorm(n2 * p2), n2, p2) 
X3 &lt;- matrix(rnorm(n3 * p3), n3, p3) 
Beta &lt;- array(rnorm(p1 * p2 * p3) * rbinom(p1 * p2 * p3, 1, 0.1), c(p1 , p2, p3))
mu &lt;- RH(X3, RH(X2, RH(X1, Beta)))
Y &lt;- array(rnorm(n1 * n2 * n3, mu), dim = c(n1, n2, n3))
fit &lt;- glamlasso(list(X1, X2, X3), Y)

##new data in matrix form
x &lt;- matrix(rnorm(p1 * p2 * p3), nrow = 1)
predict(fit, x = x)[[100]]

##new data in tensor component form
X1 &lt;- matrix(rnorm(p1), nrow = 1)
X2 &lt;- matrix(rnorm(p2), nrow = 1)
X3 &lt;- matrix(rnorm(p3), nrow = 1)
predict(fit, X = list(X1, X2, X3))[[100]]

</code></pre>

<hr>
<h2 id='print.glamlasso'>Print Function for objects of Class glamlasso</h2><span id='topic+print.glamlasso'></span>

<h3>Description</h3>

<p>This function will print some information about the glamlasso object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'glamlasso'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.glamlasso_+3A_x">x</code></td>
<td>
<p>A glamlasso object</p>
</td></tr>
<tr><td><code id="print.glamlasso_+3A_...">...</code></td>
<td>
<p>ignored</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For the call that produced the object x a two-column data.frame 
with columns <code>Df</code> and <code>lambda</code> is created. The <code>Df</code> column is 
the number of nonzero coefficients and <code>lambda</code> is the sequence of 
penalty parameters.
</p>


<h3>Value</h3>

<p>Prints the data.frame described under Details.
</p>


<h3>Author(s)</h3>

<p>Adam Lund
</p>


<h3>Examples</h3>

<pre><code class='language-R'> 
n1 &lt;- 65; n2 &lt;- 26; n3 &lt;- 13; p1 &lt;- 13; p2 &lt;- 5; p3 &lt;- 4
X1 &lt;- matrix(rnorm(n1 * p1), n1, p1) 
X2 &lt;- matrix(rnorm(n2 * p2), n2, p2) 
X3 &lt;- matrix(rnorm(n3 * p3), n3, p3) 
Beta &lt;- array(rnorm(p1 * p2 * p3) * rbinom(p1 * p2 * p3, 1, 0.1), c(p1 , p2, p3))
mu &lt;- RH(X3, RH(X2, RH(X1, Beta)))
Y &lt;- array(rnorm(n1 * n2 * n3, mu), dim = c(n1, n2, n3))
fit &lt;- glamlasso(list(X1, X2, X3), Y)
fit

</code></pre>

<hr>
<h2 id='RH'>The Rotated H-transform of a 3d Array by a Matrix</h2><span id='topic+RH'></span><span id='topic+glamlasso_RH'></span><span id='topic+Rotate'></span><span id='topic+H'></span>

<h3>Description</h3>

<p>This function is an implementation of the <code class="reqn">\rho</code>-operator found in 
<cite>Currie et al 2006</cite>. It forms the basis of the GLAM arithmetic.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>RH(M, A)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="RH_+3A_m">M</code></td>
<td>
<p>a <code class="reqn">n \times p_1</code> matrix.</p>
</td></tr>
<tr><td><code id="RH_+3A_a">A</code></td>
<td>
<p>a 3d array of size <code class="reqn">p_1 \times p_2 \times p_3</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For details see <cite>Currie et al 2006</cite>. Note that this particular implementation 
is not used in the optimization routines underlying the glamlasso procedure.
</p>


<h3>Value</h3>

<p>A 3d array of size <code class="reqn">p_2 \times p_3 \times n</code>.
</p>


<h3>Author(s)</h3>

<p>Adam Lund
</p>


<h3>References</h3>

<p>Currie, I. D., M. Durban, and P. H. C. Eilers (2006). Generalized linear
array models with applications to multidimensional
smoothing. <em>Journal of the Royal Statistical Society. Series B</em>. 68, 259-280.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
