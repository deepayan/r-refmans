<!DOCTYPE html><html><head><title>Help for package BART</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {BART}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#abart'><p>AFT BART for time-to-event outcomes</p></a></li>
<li><a href='#ACTG175'><p>AIDS Clinical Trials Group Study 175</p></a></li>
<li><a href='#alligator'><p> American alligator Food Choice</p></a></li>
<li><a href='#arq'><p>NHANES 2009-2010 Arthritis Questionnaire</p></a></li>
<li><a href='#BART-package'><p>Bayesian Additive Regression Trees</p></a></li>
<li><a href='#bartModelMatrix'><p>Create a matrix out of a vector or data.frame</p></a></li>
<li><a href='#bladder'><p>Bladder Cancer Recurrences</p></a></li>
<li><a href='#crisk.bart'><p>BART for competing risks</p></a></li>
<li><a href='#crisk.pre.bart'><p>Data construction for competing risks with BART</p></a></li>
<li><a href='#crisk2.bart'><p>BART for competing risks</p></a></li>
<li><a href='#draw_lambda_i'><p>Testing truncated Normal sampling</p></a></li>
<li><a href='#gbart'><p>Generalized BART for continuous and binary outcomes</p></a></li>
<li><a href='#gewekediag'><p>Geweke's convergence diagnostic</p></a></li>
<li><a href='#lbart'><p>Logit BART for dichotomous outcomes with Logistic latents</p></a></li>
<li><a href='#leukemia'><p>Bone marrow transplantation for leukemia and multi-state models</p></a></li>
<li><a href='#lung'><p>NCCTG Lung Cancer Data</p></a></li>
<li><a href='#mbart'><p>Multinomial BART for categorical outcomes with fewer categories</p></a></li>
<li><a href='#mbart2'><p>Multinomial BART for categorical outcomes with more categories</p></a></li>
<li><a href='#mc.cores.openmp'><p>Detecting OpenMP</p></a></li>
<li><a href='#mc.crisk.pwbart'><p>Predicting new observations with a previously fitted BART model</p></a></li>
<li><a href='#mc.crisk2.pwbart'><p>Predicting new observations with a previously fitted BART model</p></a></li>
<li><a href='#mc.lbart'><p>Logit BART for dichotomous outcomes with Logistic latents and parallel computation</p></a></li>
<li><a href='#mc.pbart'><p>Probit BART for dichotomous outcomes with Normal latents and parallel computation</p></a></li>
<li><a href='#mc.surv.pwbart'><p>Predicting new observations with a previously fitted BART model</p></a></li>
<li><a href='#mc.wbart'><p>BART for continuous outcomes with parallel computation</p></a></li>
<li><a href='#mc.wbart.gse'><p>Global SE variable selection for BART with parallel computation</p></a></li>
<li><a href='#pbart'><p>Probit BART for dichotomous outcomes with Normal latents</p></a></li>
<li><a href='#predict.crisk2bart'><p>Predicting new observations with a previously fitted BART model</p></a></li>
<li><a href='#predict.criskbart'><p>Predicting new observations with a previously fitted BART model</p></a></li>
<li><a href='#predict.lbart'><p>Predicting new observations with a previously fitted BART model</p></a></li>
<li><a href='#predict.mbart'><p>Predicting new observations with a previously fitted BART model</p></a></li>
<li><a href='#predict.pbart'><p>Predicting new observations with a previously fitted BART model</p></a></li>
<li><a href='#predict.recurbart'><p>Predicting new observations with a previously fitted BART model</p></a></li>
<li><a href='#predict.survbart'><p>Predicting new observations with a previously fitted BART model</p></a></li>
<li><a href='#predict.wbart'><p>Predicting new observations with a previously fitted BART model</p></a></li>
<li><a href='#pwbart'><p>Predicting new observations with a previously fitted BART model</p></a></li>
<li><a href='#recur.bart'><p>BART for recurrent events</p></a></li>
<li><a href='#recur.pre.bart'><p>Data construction for recurrent events with BART</p></a></li>
<li><a href='#rs.pbart'><p>BART for dichotomous outcomes with parallel computation and</p>
stratified random sampling</a></li>
<li><a href='#rtgamma'><p>Testing truncated Gamma sampling</p></a></li>
<li><a href='#rtnorm'><p>Testing truncated Normal sampling</p></a></li>
<li><a href='#spectrum0ar'><p>Estimate spectral density at zero</p></a></li>
<li><a href='#srstepwise'><p>Stepwise Variable Selection Procedure for survreg</p></a></li>
<li><a href='#stratrs'><p>Perform stratified random sampling to balance outcomes</p></a></li>
<li><a href='#surv.bart'><p>Survival analysis with BART</p></a></li>
<li><a href='#surv.pre.bart'><p>Data construction for survival analysis with BART</p></a></li>
<li><a href='#transplant'><p>Liver transplant waiting list</p></a></li>
<li><a href='#wbart'><p>BART for continuous outcomes</p></a></li>
<li><a href='#xdm20.test'>
<p>A data set used in example of <code>recur.bart</code>.</p></a></li>
<li><a href='#xdm20.train'>
<p>A real data example for <code>recur.bart</code>.</p></a></li>
<li><a href='#ydm20.train'>
<p>A data set used in example of <code>recur.bart</code>.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Bayesian Additive Regression Trees</td>
</tr>
<tr>
<td>Version:</td>
<td>2.9.6</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-01-09</td>
</tr>
<tr>
<td>Author:</td>
<td>Robert McCulloch [aut],
  Rodney Sparapani [aut, cre],
  Robert Gramacy [ctb],
  Matthew Pratola [ctb],
  Charles Spanbauer [ctb],
  Martyn Plummer [ctb],
  Nicky Best [ctb],
  Kate Cowles [ctb],
  Karen Vines [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Rodney Sparapani &lt;rsparapa@mcw.edu&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Bayesian Additive Regression Trees (BART) provide flexible nonparametric modeling of covariates for continuous, binary, categorical and time-to-event outcomes.  For more information see Sparapani, Spanbauer and McCulloch &lt;<a href="https://doi.org/10.18637%2Fjss.v097.i01">doi:10.18637/jss.v097.i01</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.6), nlme, nnet, survival</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp (&ge; 0.12.3), parallel, tools</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp</td>
</tr>
<tr>
<td>Suggests:</td>
<td>MASS, knitr, rmarkdown</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-01-09 17:57:57 UTC; rsparapa</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-01-09 23:43:08 UTC</td>
</tr>
</table>
<hr>
<h2 id='abart'>AFT BART for time-to-event outcomes</h2><span id='topic+abart'></span><span id='topic+mc.abart'></span>

<h3>Description</h3>

<p>BART is a Bayesian &ldquo;sum-of-trees&rdquo; model.<br />
For a numeric response <code class="reqn">y</code>, we have
<code class="reqn">y = f(x) + \epsilon</code>,
where <code class="reqn">\epsilon \sim N(0,\sigma^2)</code>.<br />
</p>
<p><code class="reqn">f</code> is the sum of many tree models.
The goal is to have very flexible inference for the uknown
function <code class="reqn">f</code>.
</p>
<p>In the spirit of &ldquo;ensemble models&rdquo;,
each tree is constrained by a prior to be a weak learner
so that it contributes a small amount to the overall fit.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>abart(
      x.train, times, delta,
      x.test=matrix(0,0,0), K=100,
      type='abart', ntype=1,
      sparse=FALSE, theta=0, omega=1,
      a=0.5, b=1, augment=FALSE, rho=NULL,
      xinfo=matrix(0,0,0), usequants=FALSE,
      rm.const=TRUE,
      sigest=NA, sigdf=3, sigquant=0.90,
      k=2, power=2, base=0.95,
      
      lambda=NA, tau.num=c(NA, 3, 6)[ntype], 
      offset=NULL, w=rep(1, length(times)),
      ntree=c(200L, 50L, 50L)[ntype], numcut=100L,
      
      ndpost=1000L, nskip=100L, 
      keepevery=c(1L, 10L, 10L)[ntype],
      printevery=100L, transposed=FALSE,
      mc.cores = 1L, ## mc.abart only
      nice = 19L,    ## mc.abart only
      seed = 99L     ## mc.abart only
)

mc.abart(
         x.train, times, delta,
         x.test=matrix(0,0,0), K=100,
         type='abart', ntype=1,
         sparse=FALSE, theta=0, omega=1,
         a=0.5, b=1, augment=FALSE, rho=NULL,
         xinfo=matrix(0,0,0), usequants=FALSE,
         rm.const=TRUE,
         sigest=NA, sigdf=3, sigquant=0.90,
         k=2, power=2, base=0.95,
         
         lambda=NA, tau.num=c(NA, 3, 6)[ntype], 
         offset=NULL, w=rep(1, length(times)),
         
         ntree=c(200L, 50L, 50L)[ntype], numcut=100L,
         ndpost=1000L, nskip=100L, 
         keepevery=c(1L, 10L, 10L)[ntype],
         printevery=100L, transposed=FALSE,
         mc.cores = 2L, nice = 19L, seed = 99L
)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="abart_+3A_x.train">x.train</code></td>
<td>
<p> Explanatory variables for training (in sample)
data.<br /> May be a matrix or a data frame, with (as usual) rows
corresponding to observations and columns to variables.<br /> If a
variable is a factor in a data frame, it is replaced with dummies.
Note that <code class="reqn">q</code> dummies are created if <code class="reqn">q&gt;2</code> and one dummy
created if <code class="reqn">q=2</code> where <code class="reqn">q</code> is the number of levels of the
factor.  <code>abart</code> will generate draws of <code class="reqn">f(x)</code> for each
<code class="reqn">x</code> which is a row of <code>x.train</code>.  </p>
</td></tr>
<tr><td><code id="abart_+3A_times">times</code></td>
<td>

<p>The time of event or right-censoring.<br />
If <code>y.train</code> is <code>NULL</code>, then <code>times</code> (and <code>delta</code>)
must be provided.
</p>
</td></tr>
<tr><td><code id="abart_+3A_delta">delta</code></td>
<td>

<p>The event indicator: 1 is an event while 0 is censored.<br />
If <code>y.train</code> is <code>NULL</code>, then <code>delta</code> (and <code>times</code>)
must be provided.
</p>
</td></tr>
<tr><td><code id="abart_+3A_x.test">x.test</code></td>
<td>
<p> Explanatory variables for test (out of sample)
data. Should have same structure as <code>x.train</code>.
<code>abart</code> will generate draws of <code class="reqn">f(x)</code> for each <code class="reqn">x</code> which
is a row of <code>x.test</code>.  </p>
</td></tr>
<tr><td><code id="abart_+3A_k">K</code></td>
<td>

<p>If provided, then coarsen <code>times</code> per the quantiles
<code class="reqn">1/K, 2/K, ..., K/K</code>.
</p>
</td></tr>
<tr><td><code id="abart_+3A_type">type</code></td>
<td>
<p> You can use this argument to specify the type of fit.
<code>'abart'</code> for AFT BART. </p>
</td></tr>
<tr><td><code id="abart_+3A_ntype">ntype</code></td>
<td>
<p> The integer equivalent of <code>type</code> where
<code>'abart'</code> is 1.</p>
</td></tr>
<tr><td><code id="abart_+3A_sparse">sparse</code></td>
<td>
<p>Whether to perform variable selection based on a
sparse Dirichlet prior rather than simply uniform; see Linero 2016.</p>
</td></tr>
<tr><td><code id="abart_+3A_theta">theta</code></td>
<td>
<p>Set <code class="reqn">theta</code> parameter; zero means random.</p>
</td></tr>
<tr><td><code id="abart_+3A_omega">omega</code></td>
<td>
<p>Set <code class="reqn">omega</code> parameter; zero means random.</p>
</td></tr>
<tr><td><code id="abart_+3A_a">a</code></td>
<td>
<p>Sparse parameter for <code class="reqn">Beta(a, b)</code> prior:
<code class="reqn">0.5&lt;=a&lt;=1</code> where lower values inducing more sparsity.</p>
</td></tr>
<tr><td><code id="abart_+3A_b">b</code></td>
<td>
<p>Sparse parameter for <code class="reqn">Beta(a, b)</code> prior; typically,
<code class="reqn">b=1</code>.</p>
</td></tr>
<tr><td><code id="abart_+3A_rho">rho</code></td>
<td>
<p>Sparse parameter: typically <code class="reqn">rho=p</code> where <code class="reqn">p</code> is the
number of covariates under consideration.</p>
</td></tr>
<tr><td><code id="abart_+3A_augment">augment</code></td>
<td>
<p>Whether data augmentation is to be performed in sparse
variable selection.</p>
</td></tr>
<tr><td><code id="abart_+3A_xinfo">xinfo</code></td>
<td>
<p> You can provide the cutpoints to BART or let BART
choose them for you.  To provide them, use the <code>xinfo</code>
argument to specify a list (matrix) where the items (rows) are the
covariates and the contents of the items (columns) are the
cutpoints.  </p>
</td></tr>
<tr><td><code id="abart_+3A_usequants">usequants</code></td>
<td>
<p> If <code>usequants=FALSE</code>, then the
cutpoints in <code>xinfo</code> are generated uniformly; otherwise,
if <code>TRUE</code>, uniform quantiles are used for the cutpoints. </p>
</td></tr>
<tr><td><code id="abart_+3A_rm.const">rm.const</code></td>
<td>
<p> Whether or not to remove constant variables.</p>
</td></tr>
<tr><td><code id="abart_+3A_sigest">sigest</code></td>
<td>
<p> The prior for the error variance
(<code class="reqn">sigma^2</code>) is inverted chi-squared (the standard
conditionally conjugate prior).  The prior is specified by choosing
the degrees of freedom, a rough estimate of the corresponding
standard deviation and a quantile to put this rough estimate at.  If
<code>sigest=NA</code> then the rough estimate will be the usual least squares
estimator.  Otherwise the supplied value will be used.
Not used if <code class="reqn">y</code> is binary.
</p>
</td></tr>
<tr><td><code id="abart_+3A_sigdf">sigdf</code></td>
<td>

<p>Degrees of freedom for error variance prior.
Not used if <code class="reqn">y</code> is binary.
</p>
</td></tr>
<tr><td><code id="abart_+3A_sigquant">sigquant</code></td>
<td>
<p> The quantile of the prior that the rough estimate
(see <code>sigest</code>) is placed at.  The closer the quantile is to 1, the more
aggresive the fit will be as you are putting more prior weight on
error standard deviations (<code class="reqn">sigma</code>) less than the rough
estimate.  Not used if <code class="reqn">y</code> is binary.  </p>
</td></tr>
<tr><td><code id="abart_+3A_k">k</code></td>
<td>
<p> For numeric <code class="reqn">y</code>, <code>k</code> is the number of prior
standard deviations <code class="reqn">E(Y|x) = f(x)</code> is away from +/-0.5.  For
binary <code class="reqn">y</code>, <code>k</code> is the number of prior standard deviations
<code class="reqn">f(x)</code> is away from +/-3.  The bigger <code>k</code> is, the more
conservative the fitting will be.  </p>
</td></tr>
<tr><td><code id="abart_+3A_power">power</code></td>
<td>

<p>Power parameter for tree prior.
</p>
</td></tr>
<tr><td><code id="abart_+3A_base">base</code></td>
<td>

<p>Base parameter for tree prior.
</p>
</td></tr>



<tr><td><code id="abart_+3A_lambda">lambda</code></td>
<td>

<p>The scale of the prior for the variance.  Not used if <code class="reqn">y</code> is binary.
</p>
</td></tr>
<tr><td><code id="abart_+3A_tau.num">tau.num</code></td>
<td>
<p> The numerator in the <code>tau</code> definition, i.e.,
<code>tau=tau.num/(k*sqrt(ntree))</code>. </p>
</td></tr>



<tr><td><code id="abart_+3A_offset">offset</code></td>
<td>
<p> Continous BART operates on <code>y.train</code> centered by
<code>offset</code> which defaults to <code>mean(y.train)</code>.  With binary
BART, the centering is <code class="reqn">P(Y=1 | x) = F(f(x) + offset)</code> where
<code>offset</code> defaults to <code>F^{-1}(mean(y.train))</code>.  You can use
the <code>offset</code> parameter to over-ride these defaults.</p>
</td></tr>
<tr><td><code id="abart_+3A_w">w</code></td>
<td>
<p> Vector of weights which multiply the standard deviation.
Not used if <code class="reqn">y</code> is binary.  </p>
</td></tr>
<tr><td><code id="abart_+3A_ntree">ntree</code></td>
<td>

<p>The number of trees in the sum.
</p>
</td></tr>
<tr><td><code id="abart_+3A_numcut">numcut</code></td>
<td>
<p> The number of possible values of <code class="reqn">c</code> (see
<code>usequants</code>).  If a single number if given, this is used for all
variables.  Otherwise a vector with length equal to
<code>ncol(x.train)</code> is required, where the <code class="reqn">i^{th}</code>
element gives the number of <code class="reqn">c</code> used for the <code class="reqn">i^{th}</code>
variable in <code>x.train</code>.  If usequants is false, numcut equally
spaced cutoffs are used covering the range of values in the
corresponding column of <code>x.train</code>.  If <code>usequants</code> is true, then
<code class="reqn">min(numcut, the number of unique values in the corresponding
   columns of x.train - 1)</code> values are used.  </p>
</td></tr>
<tr><td><code id="abart_+3A_ndpost">ndpost</code></td>
<td>

<p>The number of posterior draws returned.
</p>
</td></tr>
<tr><td><code id="abart_+3A_nskip">nskip</code></td>
<td>

<p>Number of MCMC iterations to be treated as burn in.
</p>
</td></tr>
<tr><td><code id="abart_+3A_printevery">printevery</code></td>
<td>

<p>As the MCMC runs, a message is printed every printevery draws.
</p>
</td></tr>
<tr><td><code id="abart_+3A_keepevery">keepevery</code></td>
<td>

<p>Every keepevery draw is kept to be returned to the user.<br />




</p>
</td></tr>
<tr><td><code id="abart_+3A_transposed">transposed</code></td>
<td>

<p>When running <code>abart</code> in parallel, it is more memory-efficient
to transpose <code>x.train</code> and <code>x.test</code>, if any, prior to
calling <code>mc.abart</code>.
</p>
</td></tr>
<tr><td><code id="abart_+3A_seed">seed</code></td>
<td>

<p>Setting the seed required for reproducible MCMC.
</p>
</td></tr>
<tr><td><code id="abart_+3A_mc.cores">mc.cores</code></td>
<td>

<p>Number of cores to employ in parallel.
</p>
</td></tr>
<tr><td><code id="abart_+3A_nice">nice</code></td>
<td>

<p>Set the job niceness.  The default
niceness is 19: niceness goes from 0 (highest) to 19 (lowest).
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>BART is a Bayesian MCMC method.
At each MCMC interation, we produce a draw from the joint posterior
<code class="reqn">(f,\sigma) | (x,y)</code> in the numeric <code class="reqn">y</code> case
and just <code class="reqn">f</code> in the binary <code class="reqn">y</code> case.
</p>
<p>Thus, unlike a lot of other modelling methods in R, we do not produce
a single model object from which fits and summaries may be extracted.
The output consists of values <code class="reqn">f^*(x)</code> (and
<code class="reqn">\sigma^*</code> in the numeric case) where * denotes a
particular draw.  The <code class="reqn">x</code> is either a row from the training data,
<code>x.train</code> or the test data, <code>x.test</code>.  </p>


<h3>Value</h3>







<p><code>abart</code> returns an object of type <code>abart</code> which is
essentially a list. 
In the numeric <code class="reqn">y</code> case, the list has components:
</p>
<table>
<tr><td><code>yhat.train</code></td>
<td>

<p>A matrix with ndpost rows and nrow(x.train) columns.
Each row corresponds to a draw <code class="reqn">f^*</code> from the posterior of <code class="reqn">f</code>
and each column corresponds to a row of x.train.
The <code class="reqn">(i,j)</code> value is <code class="reqn">f^*(x)</code> for the <code class="reqn">i^{th}</code> kept draw of <code class="reqn">f</code>
and the <code class="reqn">j^{th}</code> row of x.train.<br />
Burn-in is dropped.
</p>
</td></tr>
<tr><td><code>yhat.test</code></td>
<td>
<p>Same as yhat.train but now the x's are the rows of the test data.</p>
</td></tr>
<tr><td><code>yhat.train.mean</code></td>
<td>
<p>train data fits = mean of yhat.train columns.</p>
</td></tr>
<tr><td><code>yhat.test.mean</code></td>
<td>
<p>test data fits = mean of yhat.test columns.</p>
</td></tr>
<tr><td><code>sigma</code></td>
<td>
<p>post burn in draws of sigma, length = ndpost.</p>
</td></tr>
<tr><td><code>first.sigma</code></td>
<td>
<p>burn-in draws of sigma.</p>
</td></tr>
<tr><td><code>varcount</code></td>
<td>
<p>a matrix with ndpost rows and nrow(x.train) columns.
Each row is for a draw. For each variable (corresponding to the columns),
the total count of the number of times
that variable is used in a tree decision rule (over all trees) is given.</p>
</td></tr>
<tr><td><code>sigest</code></td>
<td>

<p>The rough error standard deviation (<code class="reqn">\sigma</code>) used in the prior.
</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+wbart">wbart</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
N = 1000
P = 5       #number of covariates
M = 8

set.seed(12)
x.train=matrix(runif(N*P, -2, 2), N, P)
mu = x.train[ , 1]^3
y=rnorm(N, mu)
offset=mean(y)
T=exp(y)
C=rexp(N, 0.05)
delta=(T&lt;C)*1
table(delta)/N
times=(T*delta+C*(1-delta))

##test BART with token run to ensure installation works
set.seed(99)
post1 = abart(x.train, times, delta, nskip=5, ndpost=10)

## Not run: 

post1 = mc.abart(x.train, times, delta,
                 mc.cores=M, seed=99)
post2 = mc.abart(x.train, times, delta, offset=offset,
                 mc.cores=M, seed=99)

Z=8

plot(mu, post1$yhat.train.mean, asp=1,
     xlim=c(-Z, Z), ylim=c(-Z, Z))
abline(a=0, b=1)

plot(mu, post2$yhat.train.mean, asp=1,
     xlim=c(-Z, Z), ylim=c(-Z, Z))
abline(a=0, b=1)

plot(post1$yhat.train.mean, post2$yhat.train.mean, asp=1,
     xlim=c(-Z, Z), ylim=c(-Z, Z))
abline(a=0, b=1)


## End(Not run)

</code></pre>

<hr>
<h2 id='ACTG175'>AIDS Clinical Trials Group Study 175</h2><span id='topic+ACTG175'></span>

<h3>Description</h3>

<p>ACTG 175 was a randomized clinical trial to compare monotherapy with zidovudine or didanosine with 
combination therapy with zidovudine and didanosine or zidovudine and zalcitabine in adults infected with the human 
immunodeficiency virus type I whose CD4 T cell counts were between 200 and 500 per cubic millimeter.</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(ACTG175)</code></pre>


<h3>Format</h3>

<p>A data frame with 2139 observations on the following 27 variables:
</p>

<dl>
<dt><code>pidnum</code></dt><dd><p>patien ID number</p>
</dd>
<dt><code>age</code></dt><dd><p>age in years at baseline</p>
</dd>
<dt><code>wtkg</code></dt><dd><p>weight in kg at baseline</p>
</dd>
<dt><code>hemo</code></dt><dd><p>hemophilia (0=no, 1=yes)</p>
</dd>
<dt><code>homo</code></dt><dd><p>homosexual activity (0=no, 1=yes)</p>
</dd>
<dt><code>drugs</code></dt><dd><p>history of intravenous drug use (0=no, 1=yes)</p>
</dd>
<dt><code>karnof</code></dt><dd><p>Karnofsky score (on a scale of 0-100)</p>
</dd>
<dt><code>oprior</code></dt><dd><p>non-zidovudine antiretroviral therapy prior to initiation of study treatment (0=no, 1=yes)</p>
</dd>
<dt><code>z30</code></dt><dd><p>zidovudine use in the 30 days prior to treatment initiation (0=no, 1=yes)</p>
</dd>
<dt><code>zprior</code></dt><dd><p>zidovudine use prior to treatment initiation (0=no, 1=yes)</p>
</dd>
<dt><code>preanti</code></dt><dd><p>number of days of previously received antiretroviral therapy</p>
</dd>
<dt><code>race</code></dt><dd><p>race (0=white, 1=non-white)</p>
</dd>
<dt><code>gender</code></dt><dd><p>gender (0=female, 1=male)</p>
</dd>
<dt><code>str2</code></dt><dd><p>antiretroviral history (0=naive, 1=experienced)</p>
</dd>
<dt><code>strat</code></dt><dd><p>antiretroviral history stratification
(1='antiretroviral naive', 2='&gt; 1 but &lt;= 52 weeks of prior
antiretroviral therapy', 3='&gt; 52 weeks')</p>
</dd>
<dt><code>symptom</code></dt><dd><p>symptomatic indicator (0=asymptomatic, 1=symptomatic)</p>
</dd>
<dt><code>treat</code></dt><dd><p>treatment indicator (0=zidovudine only, 1=other therapies)</p>
</dd>
<dt><code>offtrt</code></dt><dd><p>indicator of off-treatment before 96+/-5 weeks (0=no,1=yes)</p>
</dd>
<dt><code>cd40</code></dt><dd><p>CD4 T cell count at baseline</p>
</dd>
<dt><code>cd420</code></dt><dd><p>CD4 T cell count at 20+/-5 weeks</p>
</dd>
<dt><code>cd496</code></dt><dd><p>CD4 T cell count at 96+/-5 weeks (=<code>NA</code> if missing)</p>
</dd>
<dt><code>r</code></dt><dd><p>missing CD4 T cell count at 96+/-5 weeks (0=missing, 1=observed)</p>
</dd>
<dt><code>cd80</code></dt><dd><p>CD8 T cell count at baseline</p>
</dd>
<dt><code>cd820</code></dt><dd><p>CD8 T cell count at 20+/-5 weeks</p>
</dd>
<dt><code>cens</code></dt><dd><p>indicator of observing the event in <code>days</code></p>
</dd>
<dt><code>days</code></dt><dd><p>number of days until the first occurrence of: (i) a decline in CD4 T cell count of at least 50
(ii) an event indicating progression to AIDS, or (iii) death.</p>
</dd>
<dt><code>arms</code></dt><dd><p>treatment arm (0=zidovudine, 1=zidovudine and didanosine, 2=zidovudine and zalcitabine, 
3=didanosine).</p>
</dd>
</dl>



<h3>Details</h3>

<p>The variable <code>days</code> contains right-censored time-to-event
observations. The data set includes the following post-randomization
covariates: CD4 and CD8 T cell count at 20+/-5 weeks and the indicator
of whether or not the patient was taken off-treatment before
96+/-5 weeks.</p>


<h3>References</h3>

<p>Hammer SM, et al. (1996)
A trial comparing nucleoside monotherapy with combination therapy in 
HIV-infected adults with CD4 cell counts from 200 to 500 per cubic millimeter.
<em>New England Journal of Medicine</em> <b>335</b>, 1081-1090.
</p>

<hr>
<h2 id='alligator'> American alligator Food Choice </h2><span id='topic+alligator'></span>

<h3>Description</h3>

<p> In 1985, American alligators were harvested by hunters
from August 26 to September 30 in peninsular Florida from lakes Oklawaha
(Putnam County), George (Putnam and Volusia counties), Hancock (Polk
County) and Trafford (Collier County). Lake, length and sex were
recorded for each alligator. Stomachs from a sample of alligators
1.09-3.89m long were frozen prior to analysis. After thawing, stomach
contents were removed and separated and food items were identified and
tallied. Volumes were determined by water displacement. The stomach
contents of 219 alligators were classified into five categories of
primary food choice: Fish
(the most common primary food choice), Invertebrate (snails, insects,
crayfish, etc.), Reptile (turtles, alligators), Bird, and Other
(amphibians, plants, household pets, stones, and other debris).  </p>


<h3>Usage</h3>

<pre><code class='language-R'>data(alligator)</code></pre>


<h3>Format</h3>

<p>A data frame with 80 observations on the following 5 variables.
</p>

<dl>
<dt><code>lake</code></dt><dd><p>a factor with levels <code>George</code> <code>Hancock</code> <code>Oklawaha</code> <code>Trafford</code></p>
</dd>
<dt><code>sex</code></dt><dd><p>a factor with levels <code>female</code> <code>male</code></p>
</dd>
<dt><code>size</code></dt><dd><p>alligator size, a factor with levels <code>large</code> (&gt;2.3m) <code>small</code> (&lt;=2.3m)</p>
</dd>
<dt><code>food</code></dt><dd><p>primary food choice, a factor with levels <code>bird</code> <code>fish</code> <code>invert</code> <code>other</code> <code>reptile</code></p>
</dd>
<dt><code>count</code></dt><dd><p>cell frequency, a numeric vector</p>
</dd>
</dl>



<h3>Details</h3>

<p> The table contains a fair number of 0 counts.  <code>food</code> is
the response variable.  <code>fish</code> is the most frequent choice, and
often taken as a baseline category in multinomial response models.  </p>


<h3>Source</h3>

 
<p>Agresti, A. (2002).
<em>Categorical Data Analysis</em>,
New York: Wiley, 2nd Ed., Table 7.1
</p>


<h3>References</h3>

<p>Delany MF, Linda SB, Moore CT (1999).
&quot;Diet and condition of American alligators in 4 Florida lakes.&quot;
In <em>Proceedings of the Annual Conference of the Southeastern
Association of Fish and Wildlife Agencies</em>, <b>53</b>,
375&ndash;389. 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(alligator)
     
## nnet::multinom Multinomial logit model fit with neural nets
fit &lt;- multinom(food ~ lake+size+sex, data=alligator, weights=count)

summary(fit$fitted.values)
## 1=bird, 2=fish, 3=invert, 4=other, 5=reptile

(L=length(alligator$count))
(N=sum(alligator$count))
y.train=integer(N)
x.train=matrix(nrow=N, ncol=3)
x.test=matrix(nrow=L, ncol=3)
k=1
for(i in 1:L) {
    x.test[i, ]=as.integer(
        c(alligator$lake[i], alligator$size[i], alligator$sex[i]))
    if(alligator$count[i]&gt;0)
        for(j in 1:alligator$count[i]) {
            y.train[k]=as.integer(alligator$food[i])
            x.train[k, ]=as.integer(
                c(alligator$lake[i], alligator$size[i], alligator$sex[i]))
            k=k+1
        }
}
table(y.train)
##test mbart with token run to ensure installation works
set.seed(99)
check = mbart(x.train, y.train, nskip=1, ndpost=1)

## Not run: 
set.seed(99)
check = mbart(x.train, y.train, nskip=1, ndpost=1)
post=mbart(x.train, y.train, x.test)

##post=mc.mbart(x.train, y.train, x.test, mc.cores=8, seed=99)
##check=predict(post, x.test, mc.cores=8)
##print(cor(post$prob.test.mean, check$prob.test.mean)^2)

par(mfrow=c(3, 2))
K=5
for(j in 1:5) {
    h=seq(j, L*K, K)
    print(cor(fit$fitted.values[ , j], post$prob.test.mean[h])^2)
    plot(fit$fitted.values[ , j], post$prob.test.mean[h],
         xlim=0:1, ylim=0:1,
         xlab=paste0('NN: Est. Prob. j=', j),
         ylab=paste0('BART: Est. Prob. j=', j))
    abline(a=0, b=1)
}
par(mfrow=c(1, 1))

L=16
x.test=matrix(nrow=L, ncol=3)
k=1
for(size in 1:2)
    for(sex in 1:2)
        for(lake in 1:4) {
            x.test[k, ]=c(lake, size, sex)
            k=k+1
        }
x.test

## two sizes: 1=large: &gt;2.3m, 2=small: &lt;=2.3m
pred=predict(post, x.test)
##pred=predict(post, x.test, mc.cores=8)
ndpost=nrow(pred$prob.test)

size.test=matrix(nrow=ndpost, ncol=K*2)
for(i in 1:K) {
    j=seq(i, L*K/2, K) ## large
    size.test[ , i]=apply(pred$prob.test[ , j], 1, mean)
    j=j+L*K/2 ## small
    size.test[ , i+K]=apply(pred$prob.test[ , j], 1, mean)
}
size.test.mean=apply(size.test, 2, mean)
size.test.025=apply(size.test, 2, quantile, probs=0.025)
size.test.975=apply(size.test, 2, quantile, probs=0.975)

plot(factor(1:K, labels=c('bird', 'fish', 'invert', 'other', 'reptile')),
     rep(1, K), col=1:K, type='n', lwd=1, lty=0,
             xlim=c(1, K), ylim=c(0, 0.5), ylab='Prob.',
     sub="Multinomial BART\nFriedman's partial dependence function")
points(1:K, size.test.mean[1:K+K], col=1)
lines(1:K, size.test.025[1:K+K], col=1, lty=2)
lines(1:K, size.test.975[1:K+K], col=1, lty=2)
points(1:K, size.test.mean[1:K], col=2)
lines(1:K, size.test.025[1:K], col=2, lty=2)
lines(1:K, size.test.975[1:K], col=2, lty=2)
## legend('topright', legend=c('Small', 'Large'),
##        pch=1, col=1:2)


## End(Not run)
</code></pre>

<hr>
<h2 id='arq'>NHANES 2009-2010 Arthritis Questionnaire</h2><span id='topic+arq'></span>

<h3>Description</h3>

<p>This data set was created from the National Health and Nutrition Examination Survey (NHANES)
2009-2010 Arthritis Questionnaire.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(arq)
</code></pre>


<h3>Details</h3>

<p> We have two outcomes of interest.  Chronic neck pain: Yes
<code>arq010a=1</code> vs.\ No <code>arq010a=0</code>.  Chronic lower-back/buttock
pain: Yes <code>arq010de=1</code> vs.\ No <code>arq010de=0</code>.  <code>seqn</code> is
a unique survey respondent identifier.  <code>wtint2yr</code> is the survey
sampling weight.  <code>riagendr</code> is gender: 1 for males, 2 for
females.  <code>ridageyr</code> is age in years.  There are several
anthropometric measurements: <code>bmxwt</code>, weight in kg; <code>bmxht</code>,
height in cm; <code>bmxbmi</code>, body mass index in kg/<code class="reqn">m^2</code>; and
<code>bmxwaist</code>, waist circumference in cm.  The data was subsetted
to ensure non-missing values of these variables.
</p>


<h3>References</h3>

<p>National Health and Nutrition Examination Survey (NHANES)
2009-2010 Arthritis Questionnaire.
<a href="https://wwwn.cdc.gov/nchs/nhanes/2009-2010/ARQ_F.htm">https://wwwn.cdc.gov/nchs/nhanes/2009-2010/ARQ_F.htm</a>
</p>

<hr>
<h2 id='BART-package'>Bayesian Additive Regression Trees</h2><span id='topic+BART-package'></span><span id='topic+BART'></span>

<h3>Description</h3>

<p>To avoid duplication, the main references that this package relies upon appear here only. For more information see Sparapani, Spanbauer and McCulloch &lt;doi:10.18637/jss.v097.i01&gt;. 
</p>


<h3>References</h3>

<p>Sparapani R., Spanbauer C. and McCulloch R. (2021)
Nonparametric Machine Learning and Efficient Computation with Bayesian Additive Regression Trees: The BART R Package.
<em>JSS</em>, <b>97</b>, 1-66.
&lt;doi:10.18637/jss.v097.i01&gt;.
</p>
<p>Chipman H., George E. and McCulloch R. (1998)
Bayesian CART Model Search.
<em>JASA</em>, <b>93</b>, 935-948.
&lt;doi:10.1080/01621459.1998.10473750&gt;.
</p>
<p>Chipman H., George E., and McCulloch R. (2010)
Bayesian Additive Regression Trees.
<em>Annals of Applied Statistics</em>, <b>4</b>, 266-298.
&lt;doi:10.1214/09-AOAS285&gt;.
</p>
<p>Sparapani R., Logan B., McCulloch R. and Laud P. (2016)
Nonparametric Survival Analysis Using Bayesian Additive Regression Trees (BART).
<em>Statistics in Medicine</em>, <b>35</b>, 2741-2753.
&lt;doi:10.1002/sim.6893&gt;.
</p>
<p>Sparapani R., Logan B., McCulloch R. and Laud P. (2020)
Nonparametric Competing Risks Analysis Using Bayesian Additive Regression Trees (BART).
<em>SMMR</em>, <b>29</b>, 57-77.
&lt;doi:10.1177/0962280218822140&gt;.
</p>
<p>Sparapani R., Rein L., Tarima S., Jackson T. and Meurer J. (2020)
Non-Parametric Recurrent Events Analysis with BART and an Application to the Hospital Admissions of Patients with Diabetes.
<em>Biostatistics</em>, <b>21</b>, 69-85.
&lt;doi:10.1093/biostatistics/kxy032&gt;.
</p>
<p>Gramacy R. and Polson N. (2012)
Simulation-based regularized logistic regression.
<em>Bayesian Analysis</em>, <b>7</b>, 567-590.
&lt;doi:10.1214/12-ba719&gt;.
</p>
<p>Albert J. and Chib S. (1993)
Bayesian Analysis of Binary and Polychotomous Response Data.
<em>JASA</em>, <b>88</b>, 669-679.
&lt;doi:10.1080/01621459.1993.10476321&gt;.
</p>
<p>De Waal T., Pannekoek J. and Scholtus S. (2011)
Handbook of statistical data editing and imputation.
John Wiley &amp; Sons, Hoboken, NJ.
</p>
<p>Friedman J. (1991)
Multivariate adaptive regression splines.
<em>Annals of Statistics</em>, <b>19</b>, 1-67.
</p>
<p>Friedman J. (2001)
Greedy Function Approximation: A Gradient Boosting Machine.
<em>Annals of Statistics</em>, <b>29</b>, 1189-1232. 
</p>
<p>Holmes C. and Held L. (2006)
Bayesian auxiliary variable models for binary and multinomial regression.
<em>Bayesian Analysis</em>, <b>1</b>, 145-168.
&lt;doi:10.1214/06-ba105&gt;.
</p>
<p>Linero A. (2018)
Bayesian regression trees for high dimensional prediction and variable selection.
<em>JASA</em>, <b>113</b>, 626-636.
&lt;doi:10.1080/01621459.2016.1264957&gt;.
</p>

<hr>
<h2 id='bartModelMatrix'>Create a matrix out of a vector or data.frame</h2><span id='topic+bartModelMatrix'></span>

<h3>Description</h3>

<p>The external BART functions operate on matrices in memory.  Therefore,
if the user submits a vector or data.frame, then this function converts
it to a matrix.  Also, it determines the number of cutpoints necessary
for each column when asked to do so.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bartModelMatrix(X, numcut=0L, usequants=FALSE, type=7,
                rm.const=FALSE, cont=FALSE, xinfo=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bartModelMatrix_+3A_x">X</code></td>
<td>
<p>A vector or data.frame to create the matrix from. </p>
</td></tr>
<tr><td><code id="bartModelMatrix_+3A_numcut">numcut</code></td>
<td>
<p>The maximum number of cutpoints to consider.
If <code>numcut=0</code>, then just return a matrix; otherwise,
return a list containing a matrix <code>X</code>, a vector <code>numcut</code>
and a list <code>xinfo</code>. </p>
</td></tr>
<tr><td><code id="bartModelMatrix_+3A_usequants">usequants</code></td>
<td>
<p> If <code>usequants</code> is <code>FALSE</code>, then the
cutpoints in <code>xinfo</code> are generated uniformly; otherwise,
if <code>TRUE</code>, then quantiles are used for the cutpoints. </p>
</td></tr>
<tr><td><code id="bartModelMatrix_+3A_type">type</code></td>
<td>
<p> Determines which quantile algorithm is employed.</p>
</td></tr>
<tr><td><code id="bartModelMatrix_+3A_rm.const">rm.const</code></td>
<td>
<p> Whether or not to remove constant variables.</p>
</td></tr>
<tr><td><code id="bartModelMatrix_+3A_cont">cont</code></td>
<td>
<p> Whether or not to assume all variables are continuous.</p>
</td></tr>
<tr><td><code id="bartModelMatrix_+3A_xinfo">xinfo</code></td>
<td>
<p> You can provide the cutpoints to BART or let BART
choose them for you.  To provide them, use the <code>xinfo</code>
argument to specify a list (matrix) where the items (rows) are the
covariates and the contents of the items (columns) are the
cutpoints.  </p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="nnet.html#topic+class.ind">class.ind</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
set.seed(99)

a &lt;- rbinom(10, 4, 0.4)

table(a)

x &lt;- runif(10)

df &lt;- data.frame(a=factor(a), x=x)

b &lt;- bartModelMatrix(df)

b

b &lt;- bartModelMatrix(df, numcut=9)

b

b &lt;- bartModelMatrix(df, numcut=9, usequants=TRUE)

b

## Not run: 
    f &lt;- bartModelMatrix(as.character(a))

## End(Not run)
</code></pre>

<hr>
<h2 id='bladder'>Bladder Cancer Recurrences</h2><span id='topic+bladder'></span><span id='topic+bladder1'></span><span id='topic+bladder2'></span>

<h3>Description</h3>

<p>This interesting example is from a clinical trial conducted by the
Veterans Administration Cooperative Urological Research Group.  This
data on recurrence of bladder cancer has been used by many to
demonstrate methodology for recurrent events modelling.  In this study,
all patients had superficial bladder tumors when they entered the
trial. These tumors were removed transurethrally and patients were
randomly assigned to one of three treatments: placebo, thiotepa or
pyridoxine (vitamin B6). Many patients had multiple recurrences of
tumors during the study and new tumors were removed at each visit.  For
each patient, their recurrence time, if any, was measured from the
beginning of treatment.
</p>
<p>bladder is the data set that appears most commonly in the literature.
It uses only the 85 subjects with nonzero follow-up who were assigned to
either thiotepa or placebo and only the first four recurrences for any
patient.  The status variable is 1 for recurrence and 0 for everything
else (including death for any reason).  The data set is laid out in the
competing risks format of the paper by Wei, Lin, and Weissfeld (WLW).
</p>
<p>bladder1 is the full data set from the study. It contains all three
treatment arms and all recurrences for 118 subjects; the maximum
observed number of recurrences is 9.
</p>
<p>bladder2 uses the same subset of subjects as bladder, but formated in
the (start, stop] or Anderson-Gill (AG) style.  Note that in
transforming from the WLW to the AG style data set there is a quite
common programming mistake that leads to extra follow-up time for 12
subjects: all those with follow-up beyond their fourth recurrence.  Over
this extended time these subjects are by definition not at risk for
another event in the WLW data set.
</p>


<h3>Format</h3>

<p>bladder
</p>

<table>
<tr>
 <td style="text-align: left;">
    id:</td><td style="text-align: left;"> Patient id</td>
</tr>
<tr>
 <td style="text-align: left;">
    rx:</td><td style="text-align: left;"> Treatment 1=placebo  2=thiotepa</td>
</tr>
<tr>
 <td style="text-align: left;">
    number:</td><td style="text-align: left;"> Initial number of tumours (8=8 or more)</td>
</tr>
<tr>
 <td style="text-align: left;">
    size:</td><td style="text-align: left;"> size (cm) of largest initial tumour</td>
</tr>
<tr>
 <td style="text-align: left;">
    stop:</td><td style="text-align: left;"> recurrence or censoring time</td>
</tr>
<tr>
 <td style="text-align: left;">
    enum:</td><td style="text-align: left;"> which recurrence (up to 4)</td>
</tr>
<tr>
 <td style="text-align: left;">
  </td>
</tr>

</table>

<p>bladder1
</p>

<table>
<tr>
 <td style="text-align: left;">
    id:</td><td style="text-align: left;"> Patient id</td>
</tr>
<tr>
 <td style="text-align: left;">
    treatment:</td><td style="text-align: left;"> Placebo, pyridoxine (vitamin B6), or thiotepa</td>
</tr>
<tr>
 <td style="text-align: left;">
    number:</td><td style="text-align: left;"> Initial number of tumours (8=8 or more)</td>
</tr>
<tr>
 <td style="text-align: left;">
    size:</td><td style="text-align: left;"> Size (cm) of largest initial tumour</td>
</tr>
<tr>
 <td style="text-align: left;">
    recur:</td><td style="text-align: left;"> Number of recurrences </td>
</tr>
<tr>
 <td style="text-align: left;">
    start,stop:</td><td style="text-align: left;"> The start and end time of each time interval</td>
</tr>
<tr>
 <td style="text-align: left;">
    status:</td><td style="text-align: left;"> End of interval code, 0=censored, 1=recurrence, </td>
</tr>
<tr>
 <td style="text-align: left;">
           </td><td style="text-align: left;"> 2=death from bladder disease, 3=death other/unknown cause</td>
</tr>
<tr>
 <td style="text-align: left;">
    rtumor:</td><td style="text-align: left;"> Number of tumors found at the time of a recurrence</td>
</tr>
<tr>
 <td style="text-align: left;">
    rsize:</td><td style="text-align: left;"> Size of largest tumor at a recurrence</td>
</tr>
<tr>
 <td style="text-align: left;">
    enum:</td><td style="text-align: left;"> Event number (observation number within patient)</td>
</tr>
<tr>
 <td style="text-align: left;">
  </td>
</tr>

</table>

<p>bladder2 
</p>

<table>
<tr>
 <td style="text-align: left;">
    id:</td><td style="text-align: left;"> Patient id</td>
</tr>
<tr>
 <td style="text-align: left;">
    rx:</td><td style="text-align: left;"> Treatment 1=placebo  2=thiotepa</td>
</tr>
<tr>
 <td style="text-align: left;">
    number:</td><td style="text-align: left;"> Initial number of tumours (8=8 or more)</td>
</tr>
<tr>
 <td style="text-align: left;">
    size:</td><td style="text-align: left;"> size (cm) of largest initial tumour</td>
</tr>
<tr>
 <td style="text-align: left;">
    start:</td><td style="text-align: left;"> start of interval (0 or previous recurrence time)</td>
</tr>
<tr>
 <td style="text-align: left;">
    stop:</td><td style="text-align: left;"> recurrence or censoring time</td>
</tr>
<tr>
 <td style="text-align: left;">
    enum:</td><td style="text-align: left;"> which recurrence (up to 4)</td>
</tr>
<tr>
 <td style="text-align: left;">
  </td>
</tr>

</table>



<h3>References</h3>

<p>Byar, DP (1980),
&quot;The Veterans Administration Study of Chemoprophylaxis for Recurrent Stage I Bladder Tumors: Comparisons of Placebo, Pyridoxine, and Topical Thiotepa,&quot;
in <em>Bladder Tumors and Other Topics in Urological Oncology</em>, eds.
M Pavone-Macaluso, PH Smith, and F Edsmyn, New York: Plenum, pp. 363-370.
</p>
<p>Andrews DF, Hertzberg AM (1985), 
DATA: A Collection of Problems from Many Fields for the Student and Research Worker, New York: Springer-Verlag.
</p>
<p>LJ Wei, DY Lin, L Weissfeld (1989),
Regression analysis of multivariate incomplete failure time data by
modeling marginal distributions.
<em>Journal of the American Statistical Association</em>,
<b>84</b>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  data(bladder)
</code></pre>

<hr>
<h2 id='crisk.bart'>BART for competing risks</h2><span id='topic+crisk.bart'></span><span id='topic+mc.crisk.bart'></span>

<h3>Description</h3>

<p>Here we have implemented a simple and direct approach to utilize BART
for competing risks that is very flexible, and is akin to discrete-time
survival analysis.  Following the capabilities of BART, we allow for
maximum flexibility in modeling the dependence of competing failure times on
covariates.  In particular, we do not impose proportional hazards.
</p>
<p>To elaborate, consider data in the form: <code class="reqn">(s_i, \delta_i,
{x}_i)</code> where <code class="reqn">s_i</code> is the event time;
<code class="reqn">\delta_i</code> is an indicator distinguishing events,
<code class="reqn">\delta_i=h</code> due to cause <code class="reqn">h in {1, 2}</code>, from
right-censoring, <code class="reqn">\delta_i=0</code>; <code class="reqn">{x}_i</code> is a vector of
covariates; and <code class="reqn">i=1, ..., N</code> indexes subjects.
</p>
<p>We denote the <code class="reqn">K</code> distinct event/censoring times by
<code class="reqn">0&lt;t_{(1)}&lt;...&lt;t_{(K)}&lt;\infty</code> thus
taking <code class="reqn">t_{(j)}</code> to be the <code class="reqn">j^{th}</code> order statistic
among distinct observation times and, for convenience,
<code class="reqn">t_{(0)}=0</code>. Now consider event indicators for cause
<code class="reqn">h</code>: <code class="reqn">y_{hij}</code> for each subject <code class="reqn">i</code> at each distinct
time <code class="reqn">t_{(j)}</code> up to and including the subject's last
observation time <code class="reqn">s_i=t_{(n_i)}</code> with <code class="reqn">n_i=\arg \max_j
[t_{(j)}\leq s_i]</code> for cause 1, but only up to
<code class="reqn">n_i-y_{1ij}</code> for cause 2.



</p>
<p>We then denote by <code class="reqn">p_{hij}</code> the probability of an event at
time <code class="reqn">t_{(j)}</code> conditional on no previous event. We now write
the model for <code class="reqn">y_{hij}</code> as a nonparametric probit (or
logistic) regression of <code class="reqn">y_{hij}</code> on the time
<code class="reqn">t_{(j)}</code> and the covariates <code class="reqn">{x}_{hi}</code>, and then
utilize BART for binary responses.  Specifically, <code class="reqn"> y_{hij}\ =\
I[\delta_i=h] I[s_i=t_{(j)}],\ j=1, ..., n_i-I[h=2]y_{1ij}</code>.  Therefore, we have
<code class="reqn">p_{hij} = F(mu_{hij}),\ mu_{hij} = mu_h+f_h(t_{(j)},
{x}_{hi})</code> where
<code class="reqn">F</code> denotes the Normal (or Logistic) cdf.  As in the binary response
case, <code class="reqn">f_h</code> is the sum of many tree models.  Finally, based on
these probabilities, <code class="reqn">p_{hij}</code>, we can construct targets of
inference such as the cumulative incidence functions.  </p>


<h3>Usage</h3>

<pre><code class='language-R'>
crisk.bart(x.train=matrix(0,0,0), y.train=NULL,
           x.train2=x.train, y.train2=NULL,
           times=NULL, delta=NULL, K=NULL,
           x.test=matrix(0,0,0), x.test2=x.test, cond=NULL,
           sparse=FALSE, theta=0, omega=1,
           a=0.5, b=1, augment=FALSE,
           rho=NULL, rho2=NULL,
           xinfo=matrix(0,0,0), xinfo2=matrix(0,0,0),
           usequants=FALSE, 
           rm.const=TRUE, type='pbart',
           ntype=as.integer(
               factor(type, levels=c('wbart', 'pbart', 'lbart'))),
           k=2, power=2, base=0.95,
           offset=NULL, offset2=NULL,
           tau.num=c(NA, 3, 6)[ntype], 
           
           ntree=50, numcut=100, ndpost=1000, nskip=250,
           keepevery = 10L,
           
           
           
           printevery=100L, 
           
           id=NULL,    ## crisk.bart only
           seed=99,    ## mc.crisk.bart only
           mc.cores=2, ## mc.crisk.bart only
           nice=19L    ## mc.crisk.bart only
          )

mc.crisk.bart(x.train=matrix(0,0,0), y.train=NULL,
              x.train2=x.train, y.train2=NULL,
              times=NULL, delta=NULL, K=NULL,
              x.test=matrix(0,0,0), x.test2=x.test, cond=NULL,
              sparse=FALSE, theta=0, omega=1,
              a=0.5, b=1, augment=FALSE,
              rho=NULL, rho2=NULL,
              xinfo=matrix(0,0,0), xinfo2=matrix(0,0,0),
              usequants=FALSE, 
              rm.const=TRUE, type='pbart',
              ntype=as.integer(
                  factor(type, levels=c('wbart', 'pbart', 'lbart'))),
              k=2, power=2, base=0.95,
              offset=NULL, offset2=NULL,
              tau.num=c(NA, 3, 6)[ntype], 
              
              ntree=50, numcut=100, ndpost=1000, nskip=250,
              keepevery = 10L,
              
              
              
              printevery=100L, 
              
              id=NULL,    ## crisk.bart only
              seed=99,    ## mc.crisk.bart only
              mc.cores=2, ## mc.crisk.bart only
              nice=19L    ## mc.crisk.bart only
             )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="crisk.bart_+3A_x.train">x.train</code></td>
<td>
<p> Covariates for training (in sample) data of cause 1.<br />
Must be a data.frame or a matrix with rows corresponding to
observations and columns to variables.<br /> <code>crisk.bart</code> will
generate draws of <code class="reqn">f_1(t, x)</code> for each <code class="reqn">x</code> which is
a row of <code>x.train</code> (note that the definition of <code>x.train</code> is
dependent on whether <code>y.train</code> has been specified; see below).  </p>
</td></tr>
<tr><td><code id="crisk.bart_+3A_y.train">y.train</code></td>
<td>
<p> Cause 1 binary response for training (in sample)
data.<br /> If <code>y.train</code> is <code>NULL</code>, then <code>y.train</code>
(<code>x.train</code> and <code>x.test</code>, if specified) are generated by a
call to <code>crisk.pre.bart</code> (which require that <code>times</code> and
<code>delta</code> be provided: see below); otherwise, <code>y.train</code>
(<code>x.train</code> and <code>x.test</code>, if specified) are utilized as
given assuming that the data construction has already been performed.
</p>
</td></tr>
<tr><td><code id="crisk.bart_+3A_x.train2">x.train2</code></td>
<td>
<p> Covariates for training (in sample)
data of cause 2. Similar to <code>x.train</code> above.</p>
</td></tr>
<tr><td><code id="crisk.bart_+3A_y.train2">y.train2</code></td>
<td>

<p>Cause 2 binary response for training (in sample) data, i.e., failure
from any cause besides the cause of interest which is cause 1.
Similar to <code>y.train</code> above.
</p>
</td></tr>
<tr><td><code id="crisk.bart_+3A_times">times</code></td>
<td>

<p>The time of event or right-censoring, <code class="reqn">s_i</code>.<br />
If <code>y.train</code> is <code>NULL</code>, then <code>times</code> (and <code>delta</code>)
must be provided.
</p>
</td></tr>
<tr><td><code id="crisk.bart_+3A_delta">delta</code></td>
<td>

<p>The event indicator: 1 for cause 1, 2 for cause 2 and 0 is censored.<br />
If <code>y.train</code> is <code>NULL</code>, then <code>delta</code> (and <code>times</code>)
must be provided.
</p>
</td></tr>
<tr><td><code id="crisk.bart_+3A_k">K</code></td>
<td>

<p>If provided, then coarsen <code>times</code> per the quantiles
<code class="reqn">1/K, 2/K, ..., K/K</code>.
</p>
</td></tr>
<tr><td><code id="crisk.bart_+3A_x.test">x.test</code></td>
<td>

<p>Covariates for test (out of sample) data of cause 1.<br />
Must be a data.frame or a matrix and have the same structure as
<code>x.train</code>.<br />
<code>crisk.bart</code> will generate draws of
<code class="reqn">f_1(t, x)</code> for each <code class="reqn">x</code> which is a row of
<code>x.test</code>.
</p>
</td></tr>
<tr><td><code id="crisk.bart_+3A_x.test2">x.test2</code></td>
<td>

<p>Covariates for test (out of sample) data of cause 2.
Similar to <code>x.test</code> above.</p>
</td></tr>
<tr><td><code id="crisk.bart_+3A_cond">cond</code></td>
<td>
<p>A vector of indices for <code>y.train2</code> indicating 
subjects who did not suffer a cause 1 event and, therefore, are eligible
for cause 2.</p>
</td></tr>
<tr><td><code id="crisk.bart_+3A_sparse">sparse</code></td>
<td>
<p>Whether to perform variable selection based on a
sparse Dirichlet prior; see Linero 2016.</p>
</td></tr>
<tr><td><code id="crisk.bart_+3A_theta">theta</code></td>
<td>
<p>Set <code class="reqn">theta</code> parameter; zero means random.</p>
</td></tr>
<tr><td><code id="crisk.bart_+3A_omega">omega</code></td>
<td>
<p>Set <code class="reqn">omega</code> parameter; zero means random.</p>
</td></tr>
<tr><td><code id="crisk.bart_+3A_a">a</code></td>
<td>
<p>Sparse parameter for <code class="reqn">Beta(a, b)</code> prior:
<code class="reqn">0.5&lt;=a&lt;=1</code> where lower values inducing more sparsity.</p>
</td></tr>
<tr><td><code id="crisk.bart_+3A_b">b</code></td>
<td>
<p>Sparse parameter for <code class="reqn">Beta(a, b)</code> prior; typically,
<code>b=1</code>.</p>
</td></tr>
<tr><td><code id="crisk.bart_+3A_rho">rho</code></td>
<td>
<p>Sparse parameter: typically <code>rho=p</code> where <code>p</code> is the
number of covariates in <code>x.train</code>.</p>
</td></tr>
<tr><td><code id="crisk.bart_+3A_rho2">rho2</code></td>
<td>
<p>Sparse parameter: typically <code>rho2=p</code> where <code>p</code> is the
number of covariates  in <code>x.train2</code>.</p>
</td></tr>
<tr><td><code id="crisk.bart_+3A_augment">augment</code></td>
<td>
<p>Whether data augmentation is to be performed in sparse
variable selection.</p>
</td></tr>
<tr><td><code id="crisk.bart_+3A_xinfo">xinfo</code></td>
<td>
<p> You can provide the cutpoints to BART or let BART
choose them for you.  To provide them, use the <code>xinfo</code>
argument to specify a list (matrix) where the items (rows) are the
covariates and the contents of the items (columns) are the
cutpoints.  </p>
</td></tr>
<tr><td><code id="crisk.bart_+3A_xinfo2">xinfo2</code></td>
<td>
<p> Cause 2 cutpoints. </p>
</td></tr>
<tr><td><code id="crisk.bart_+3A_usequants">usequants</code></td>
<td>
<p> If <code>usequants=FALSE</code>, then the
cutpoints in <code>xinfo</code> are generated uniformly; otherwise,
if <code>TRUE</code>, uniform quantiles are used for the cutpoints. </p>
</td></tr>

<tr><td><code id="crisk.bart_+3A_rm.const">rm.const</code></td>
<td>
<p> Whether or not to remove constant variables.</p>
</td></tr>
<tr><td><code id="crisk.bart_+3A_type">type</code></td>
<td>
<p> Whether to employ probit BART via Albert-Chib,
<code>'pbart'</code>, or logistic BART by Holmes-Held, <code>'lbart'</code>. </p>
</td></tr>
<tr><td><code id="crisk.bart_+3A_ntype">ntype</code></td>
<td>
<p> The integer equivalent of <code>type</code> where
<code>'wbart'</code> is 1, <code>'pbart'</code> is 2 and
<code>'lbart'</code> is 3.</p>
</td></tr>
<tr><td><code id="crisk.bart_+3A_k">k</code></td>
<td>
<p> k is the number of prior standard deviations
<code class="reqn">f_h(t, x)</code>
is away from +/-3.  The bigger k is, the more conservative the
fitting will be.  </p>
</td></tr>
<tr><td><code id="crisk.bart_+3A_power">power</code></td>
<td>

<p>Power parameter for tree prior.
</p>
</td></tr>
<tr><td><code id="crisk.bart_+3A_base">base</code></td>
<td>

<p>Base parameter for tree prior.
</p>
</td></tr>
<tr><td><code id="crisk.bart_+3A_offset">offset</code></td>
<td>

<p>Cause 1 binary offset.
</p>
</td></tr>
<tr><td><code id="crisk.bart_+3A_offset2">offset2</code></td>
<td>

<p>Cause 2 binary offset.
</p>
</td></tr>
<tr><td><code id="crisk.bart_+3A_tau.num">tau.num</code></td>
<td>
<p> The numerator in the <code>tau</code> definition. </p>
</td></tr>   

<tr><td><code id="crisk.bart_+3A_ntree">ntree</code></td>
<td>

<p>The number of trees in the sum.
</p>
</td></tr>
<tr><td><code id="crisk.bart_+3A_numcut">numcut</code></td>
<td>
<p> The number of possible values of cutpoints (see
<code>usequants</code>).  If a single number if given, this is used for all
variables.  Otherwise a vector with length equal to
<code>ncol(x.train)</code> is required, where the <code class="reqn">i^{th}</code>
element gives the number of cutpoints used for the <code class="reqn">i^{th}</code>
variable in <code>x.train</code>.  If <code>usequants</code> is <code>FALSE</code>,
<code>numcut</code> equally spaced cutoffs are used covering the range of
values in the corresponding column of <code>x.train</code>.  If
<code>usequants</code> is <code>TRUE</code>, then <code>min(numcut, the number of
   unique values in the corresponding columns of x.train - 1)</code> cutpoint
values are used.  </p>
</td></tr>
<tr><td><code id="crisk.bart_+3A_ndpost">ndpost</code></td>
<td>

<p>The number of posterior draws returned.
</p>
</td></tr>
<tr><td><code id="crisk.bart_+3A_nskip">nskip</code></td>
<td>

<p>Number of MCMC iterations to be treated as burn in.
</p>
</td></tr>
<tr><td><code id="crisk.bart_+3A_keepevery">keepevery</code></td>
<td>

<p>Every <code>keepevery</code> draw is kept to be returned to the user.
</p>
</td></tr>












<tr><td><code id="crisk.bart_+3A_printevery">printevery</code></td>
<td>

<p>As the MCMC runs, a message is printed every <code>printevery</code> draws.
</p>
</td></tr>








<tr><td><code id="crisk.bart_+3A_id">id</code></td>
<td>

<p><code>crisk.bart</code> only: unique identifier added to returned list.
</p>
</td></tr>
<tr><td><code id="crisk.bart_+3A_seed">seed</code></td>
<td>

<p><code>mc.crisk.bart</code> only: seed required for reproducible MCMC.
</p>
</td></tr>
<tr><td><code id="crisk.bart_+3A_mc.cores">mc.cores</code></td>
<td>

<p><code>mc.crisk.bart</code> only: number of cores to employ in parallel.
</p>
</td></tr>
<tr><td><code id="crisk.bart_+3A_nice">nice</code></td>
<td>
 <p><code>mc.crisk.bart</code> only: set the job niceness.  The
default niceness is 19: niceness goes from 0 (highest priority) to
19 (lowest priority).  </p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>crisk.bart</code> returns an object of type <code>criskbart</code> which is
essentially a list.  Besides the items listed
below, the list has <code>offset</code>, <code>offset2</code>,
<code>times</code> which are the unique times, <code>K</code>
which is the number of unique times, <code>tx.train</code> and
<code>tx.test</code>, if any.
</p>
<table>
<tr><td><code>yhat.train</code></td>
<td>
<p> A matrix with <code>ndpost</code> rows and
<code>nrow(x.train)</code> columns.  Each row corresponds to a draw
<code class="reqn">f^*_1</code> from the posterior of <code class="reqn">f_1</code> and each column
corresponds to a row of <code>x.train</code>.  The <code class="reqn">(i,j)</code> value is
<code class="reqn">f^*_1(t, x)</code> for the <code class="reqn">i^{th}</code> kept draw of
<code class="reqn">f_1</code> and the <code class="reqn">j^{th}</code> row of <code>x.train</code>.
Burn-in is dropped.  </p>
</td></tr>
<tr><td><code>yhat.test</code></td>
<td>
<p>Same as <code>yhat.train</code> but now the x's are the rows of the test data.</p>
</td></tr>
<tr><td><code>surv.test</code></td>
<td>
<p>test data fits for the survival function, <code class="reqn">S(t, x)</code>.</p>
</td></tr>
<tr><td><code>surv.test.mean</code></td>
<td>
<p>mean of <code>surv.test</code> over the posterior samples.</p>
</td></tr>
<tr><td><code>prob.test</code></td>
<td>
<p>The probability of suffering cause 1.</p>
</td></tr>
<tr><td><code>prob.test2</code></td>
<td>
<p>The probability of suffering cause 2.</p>
</td></tr>
<tr><td><code>cif.test</code></td>
<td>
<p>The cumulative incidence function of cause 1,
<code class="reqn">F_1(t, x)</code>.</p>
</td></tr>
<tr><td><code>cif.test2</code></td>
<td>
<p>The cumulative incidence function of cause 2,
<code class="reqn">F_2(t, x)</code>.</p>
</td></tr>


<tr><td><code>cif.test.mean</code></td>
<td>
<p>mean of <code>cif.test</code> columns for cause 1.</p>
</td></tr>
<tr><td><code>cif.test2.mean</code></td>
<td>
<p>mean of <code>cif.test2</code> columns for cause 2.</p>
</td></tr>
<tr><td><code>varcount</code></td>
<td>
<p>a matrix with <code>ndpost</code> rows and
<code>nrow(x.train)</code> columns.
Each row is for a draw. For each variable (corresponding to the columns),
the total count of the number of times
this variable is used for cause 1 in a tree decision rule (over all trees) is given.</p>
</td></tr>
<tr><td><code>varcount2</code></td>
<td>
<p> For each variable the total count of the number of times
this variable is used for cause 2 in a tree decision rule is given.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+crisk.pre.bart">crisk.pre.bart</a></code>, <code><a href="#topic+predict.criskbart">predict.criskbart</a></code>,
<code><a href="#topic+mc.crisk.pwbart">mc.crisk.pwbart</a></code>, <code><a href="#topic+crisk2.bart">crisk2.bart</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(transplant)

pfit &lt;- survfit(Surv(futime, event) ~ abo, transplant)

# competing risks for type O
plot(pfit[4,], xscale=7, xmax=735, col=1:3, lwd=2, ylim=c(0, 1),
       xlab='t (weeks)', ylab='Aalen-Johansen (AJ) CI(t)')
    legend(450, .4, c("Death", "Transplant", "Withdrawal"), col=1:3, lwd=2)
## plot(pfit[4,], xscale=30.5, xmax=735, col=1:3, lwd=2, ylim=c(0, 1),
##        xlab='t (months)', ylab='Aalen-Johansen (AJ) CI(t)')
##     legend(450, .4, c("Death", "Transplant", "Withdrawal"), col=1:3, lwd=2)

delta &lt;- (as.numeric(transplant$event)-1)
## recode so that delta=1 is cause of interest; delta=2 otherwise
delta[delta==1] &lt;- 4
delta[delta==2] &lt;- 1
delta[delta&gt;1] &lt;- 2
table(delta, transplant$event)

times &lt;- pmax(1, ceiling(transplant$futime/7)) ## weeks
##times &lt;- pmax(1, ceiling(transplant$futime/30.5)) ## months
table(times)

typeO &lt;- 1*(transplant$abo=='O')
typeA &lt;- 1*(transplant$abo=='A')
typeB &lt;- 1*(transplant$abo=='B')
typeAB &lt;- 1*(transplant$abo=='AB')
table(typeA, typeO)

x.train &lt;- cbind(typeO, typeA, typeB, typeAB)

x.test &lt;- cbind(1, 0, 0, 0)
dimnames(x.test)[[2]] &lt;- dimnames(x.train)[[2]]

##test BART with token run to ensure installation works
set.seed(99)
post &lt;- crisk.bart(x.train=x.train, times=times, delta=delta,
                   x.test=x.test, nskip=1, ndpost=1, keepevery=1)

## Not run: 

## run one long MCMC chain in one process
## set.seed(99)
## post &lt;- crisk.bart(x.train=x.train, times=times, delta=delta, x.test=x.test)

## in the interest of time, consider speeding it up by parallel processing
## run "mc.cores" number of shorter MCMC chains in parallel processes
post &lt;- mc.crisk.bart(x.train=x.train, times=times, delta=delta,
                      x.test=x.test, seed=99, mc.cores=8)

K &lt;- post$K

typeO.cif.mean &lt;- apply(post$cif.test, 2, mean)
typeO.cif.025 &lt;- apply(post$cif.test, 2, quantile, probs=0.025)
typeO.cif.975 &lt;- apply(post$cif.test, 2, quantile, probs=0.975)

plot(pfit[4,], xscale=7, xmax=735, col=1:3, lwd=2, ylim=c(0, 0.8),
       xlab='t (weeks)', ylab='CI(t)')
points(c(0, post$times)*7, c(0, typeO.cif.mean), col=4, type='s', lwd=2)
points(c(0, post$times)*7, c(0, typeO.cif.025), col=4, type='s', lwd=2, lty=2)
points(c(0, post$times)*7, c(0, typeO.cif.975), col=4, type='s', lwd=2, lty=2)
     legend(450, .4, c("Transplant(BART)", "Transplant(AJ)",
                       "Death(AJ)", "Withdrawal(AJ)"),
            col=c(4, 2, 1, 3), lwd=2)
##dev.copy2pdf(file='../vignettes/figures/liver-BART.pdf')
## plot(pfit[4,], xscale=30.5, xmax=735, col=1:3, lwd=2, ylim=c(0, 0.8),
##        xlab='t (months)', ylab='CI(t)')
## points(c(0, post$times)*30.5, c(0, typeO.cif.mean), col=4, type='s', lwd=2)
## points(c(0, post$times)*30.5, c(0, typeO.cif.025), col=4, type='s', lwd=2, lty=2)
## points(c(0, post$times)*30.5, c(0, typeO.cif.975), col=4, type='s', lwd=2, lty=2)
##      legend(450, .4, c("Transplant(BART)", "Transplant(AJ)",
##                        "Death(AJ)", "Withdrawal(AJ)"),
##             col=c(4, 2, 1, 3), lwd=2)


## End(Not run)
</code></pre>

<hr>
<h2 id='crisk.pre.bart'>Data construction for competing risks with BART</h2><span id='topic+crisk.pre.bart'></span>

<h3>Description</h3>

<p>Competing risks contained in <code class="reqn">(t, \delta, x)</code> must be translated to data
suitable for the BART competing risks model; see <code>crisk.bart</code> for more details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>crisk.pre.bart( times, delta, x.train=NULL, x.test=NULL,
                x.train2=x.train, x.test2=x.test, K=NULL )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="crisk.pre.bart_+3A_times">times</code></td>
<td>

<p>The time of event or right-censoring.<br />
</p>
</td></tr>
<tr><td><code id="crisk.pre.bart_+3A_delta">delta</code></td>
<td>

<p>The event indicator: 1 is a cause 1 event, 2 a cause 2 while 0 is censored.<br />
</p>
</td></tr>
<tr><td><code id="crisk.pre.bart_+3A_x.train">x.train</code></td>
<td>

<p>Explanatory variables for training (in sample) data of cause 1.<br />
If provided, must be a matrix
with (as usual) rows corresponding to observations and columns to variables.<br />
</p>
</td></tr>
<tr><td><code id="crisk.pre.bart_+3A_x.test">x.test</code></td>
<td>

<p>Explanatory variables for test (out of sample) data of cause 1.<br />
If provided, must be a matrix and have the same structure as x.train.<br />
</p>
</td></tr>
<tr><td><code id="crisk.pre.bart_+3A_x.train2">x.train2</code></td>
<td>

<p>Explanatory variables for training (in sample) data of cause 2.<br />
If provided, must be a matrix
with (as usual) rows corresponding to observations and columns to variables.<br />
</p>
</td></tr>
<tr><td><code id="crisk.pre.bart_+3A_x.test2">x.test2</code></td>
<td>

<p>Explanatory variables for test (out of sample) data of cause 2.<br />
If provided, must be a matrix and have the same structure as x.train.<br />
</p>
</td></tr>
<tr><td><code id="crisk.pre.bart_+3A_k">K</code></td>
<td>

<p>If provided, then coarsen <code>times</code> per the quantiles
<code class="reqn">1/K, 2/K, ..., K/K</code>.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>surv.pre.bart</code> returns a list.
Besides the items listed below, the list has
a <code>times</code> component giving the unique times and <code>K</code> which is the number of
unique times.
</p>
<table>
<tr><td><code>y.train</code></td>
<td>
<p>A vector of binary responses for cause 1.</p>
</td></tr>
<tr><td><code>y.train2</code></td>
<td>
<p>A vector of binary responses for cause 2.</p>
</td></tr>
<tr><td><code>cond</code></td>
<td>
<p>A vector of indices of <code>y.train</code> indicating censored subjects.</p>
</td></tr>
<tr><td><code>binaryOffset</code></td>
<td>
<p>The binary offset for <code>y.train</code>.</p>
</td></tr>
<tr><td><code>binaryOffset2</code></td>
<td>
<p>The binary offset for <code>y.train2</code>.</p>
</td></tr>
<tr><td><code>tx.train</code></td>
<td>
<p>A matrix with rows consisting of time and the
covariates of the training data for cause 1.</p>
</td></tr>
<tr><td><code>tx.train2</code></td>
<td>
<p>A matrix with rows consisting of time and the
covariates of the training data for cause 2.</p>
</td></tr>
<tr><td><code>tx.test</code></td>
<td>
<p>A matrix with rows consisting of time and the
covariates of the test data, if any, for cause 1.</p>
</td></tr>
<tr><td><code>tx.test2</code></td>
<td>
<p>A matrix with rows consisting of time and the
covariates of the test data, if any, for cause 2.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+crisk.bart">crisk.bart</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(transplant)

delta &lt;- (as.numeric(transplant$event)-1)

delta[delta==1] &lt;- 4
delta[delta==2] &lt;- 1
delta[delta&gt;1] &lt;- 2
table(delta, transplant$event)

table(1+floor(transplant$futime/30.5)) ## months
times &lt;- 1+floor(transplant$futime/30.5)

typeO &lt;- 1*(transplant$abo=='O')
typeA &lt;- 1*(transplant$abo=='A')
typeB &lt;- 1*(transplant$abo=='B')
typeAB &lt;- 1*(transplant$abo=='AB')
table(typeA, typeO)

x.train &lt;- cbind(typeO, typeA, typeB, typeAB)

N &lt;- nrow(x.train)

x.test &lt;- x.train

x.test[1:N, 1:4] &lt;- matrix(c(1, 0, 0, 0), nrow=N, ncol=4, byrow=TRUE)

pre &lt;- crisk.pre.bart(x.train=x.train, times=times, delta=delta, x.test=x.test)

</code></pre>

<hr>
<h2 id='crisk2.bart'>BART for competing risks</h2><span id='topic+crisk2.bart'></span><span id='topic+mc.crisk2.bart'></span>

<h3>Description</h3>

<p>Here we have implemented another approach to utilize BART
for competing risks that is very flexible, and is akin to discrete-time
survival analysis.  Following the capabilities of BART, we allow for
maximum flexibility in modeling the dependence of competing failure times on
covariates.  In particular, we do not impose proportional hazards.
</p>
<p>Similar to <code>crisk.bart</code>, we utilize two BART models, yet they are
two different BART models than previously considered.  First, given an
event of either cause occurred, we employ a typical binary BART model to
discriminate between cause 1 and 2.  Next, we proceed as if it were a typical
survival analysis with BART for an absorbing event from either cause.
</p>
<p>To elaborate, consider data in the form: <code class="reqn">(s_i, \delta_i,
{x}_i)</code> where <code class="reqn">s_i</code> is the event time;
<code class="reqn">\delta_i</code> is an indicator distinguishing events,
<code class="reqn">\delta_i=h</code> due to cause <code class="reqn">h in {1, 2}</code>, from
right-censoring, <code class="reqn">\delta_i=0</code>; <code class="reqn">{x}_i</code> is a vector of
covariates; and <code class="reqn">i=1, ..., N</code> indexes subjects.
We denote the <code class="reqn">K</code> distinct event/censoring times by
<code class="reqn">0&lt;t_{(1)}&lt;...&lt;t_{(K)}&lt;\infty</code> thus
taking <code class="reqn">t_{(j)}</code> to be the <code class="reqn">j^{th}</code> order statistic
among distinct observation times and, for convenience,
<code class="reqn">t_{(0)}=0</code>. 



</p>
<p>First, consider event indicators for an event from either cause:
<code class="reqn">y_{1ij}</code> for each subject <code class="reqn">i</code> at each distinct time
<code class="reqn">t_{(j)}</code> up to and including the subject's last observation
time <code class="reqn">s_i=t_{(n_i)}</code> with <code class="reqn">n_i=\arg \max_j [t_{(j)}\leq
s_i]</code>.  We denote by <code class="reqn">p_{1ij}</code> the
probability of an event at time <code class="reqn">t_{(j)}</code> conditional on no
previous event. We now write the model for <code class="reqn">y_{1ij}</code> as a
nonparametric probit (or logistic) regression of <code class="reqn">y_{1ij}</code> on
the time <code class="reqn">t_{(j)}</code> and the covariates <code class="reqn">{x}_{1i}</code>,
and then utilize BART for binary responses.  Specifically, <code class="reqn">
y_{1ij}\ =\ I[\delta_i&gt;0] I[s_i=t_{(j)}],\ j=1, ..., n_i</code>.  Therefore, we have <code class="reqn">p_{1ij} =
F(mu_{1ij}),\ mu_{1ij} = mu_1+f_1(t_{(j)}, {x}_{1i})</code> where <code class="reqn">F</code> denotes the Normal (or
Logistic) cdf.  
</p>
<p>Next, we denote by <code class="reqn">p_{2i}</code> the probability of a cause 1
event at time <code class="reqn">s_i</code> conditional on an event having
occurred.  We now write the model for <code class="reqn">y_{2i}</code> as a
nonparametric probit (or logistic) regression of <code class="reqn">y_{2i}</code> on
the time <code class="reqn">s_i</code> and the covariates <code class="reqn">{x}_{2i}</code>,
via BART for binary responses.  Specifically, <code class="reqn">
y_{2i}\ =\ I[\delta_i=1]</code>.  Therefore, we
have <code class="reqn">p_{2i} = F(mu_{2i}),\ mu_{2i} = mu_2+f_2(s_i,
{x}_{2i})</code> where
<code class="reqn">F</code> denotes the Normal (or Logistic) cdf.  Although, we modeled
<code class="reqn">p_{2i}</code> at the time of an event, <code class="reqn">s_i</code>, we can
estimate this probability at any other time points on the grid via
<code class="reqn">p(t_{(j)}, x_2)=F( mu_2+f_2(t_{(j)}, {x}_2))</code>.
Finally, based on these probabilities,
<code class="reqn">p_{hij}</code>, we can construct targets of inference such as the
cumulative incidence functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
crisk2.bart(x.train=matrix(0,0,0), y.train=NULL,
           x.train2=x.train, y.train2=NULL,
           times=NULL, delta=NULL, K=NULL,
           x.test=matrix(0,0,0), x.test2=x.test, 
           sparse=FALSE, theta=0, omega=1,
           a=0.5, b=1, augment=FALSE,
           rho=NULL, rho2=NULL,
           xinfo=matrix(0,0,0), xinfo2=matrix(0,0,0),
           usequants=FALSE, 
           rm.const=TRUE, type='pbart',
           ntype=as.integer(
               factor(type, levels=c('wbart', 'pbart', 'lbart'))),
           k=2, power=2, base=0.95,
           offset=NULL, offset2=NULL,
           tau.num=c(NA, 3, 6)[ntype],
           
           ntree=50, numcut=100, ndpost=1000, nskip=250,
           keepevery = 10L,
           
           
           
           
           printevery=100L, 
           
           id=NULL,    ## crisk2.bart only
           seed=99,    ## mc.crisk2.bart only
           mc.cores=2, ## mc.crisk2.bart only
           nice=19L    ## mc.crisk2.bart only
          )

mc.crisk2.bart(x.train=matrix(0,0,0), y.train=NULL,
              x.train2=x.train, y.train2=NULL,
              times=NULL, delta=NULL, K=NULL,
              x.test=matrix(0,0,0), x.test2=x.test, 
              sparse=FALSE, theta=0, omega=1,
              a=0.5, b=1, augment=FALSE,
              rho=NULL, rho2=NULL,
              xinfo=matrix(0,0,0), xinfo2=matrix(0,0,0),
              usequants=FALSE, 
              rm.const=TRUE, type='pbart',
              ntype=as.integer(
                  factor(type, levels=c('wbart', 'pbart', 'lbart'))),
              k=2, power=2, base=0.95,
              offset=NULL, offset2=NULL,
              tau.num=c(NA, 3, 6)[ntype],
              
              ntree=50, numcut=100, ndpost=1000, nskip=250,
              keepevery = 10L,
              
              
              
              printevery=100L, 
              
              id=NULL,    ## crisk2.bart only
              seed=99,    ## mc.crisk2.bart only
              mc.cores=2, ## mc.crisk2.bart only
              nice=19L    ## mc.crisk2.bart only
             )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="crisk2.bart_+3A_x.train">x.train</code></td>
<td>
<p> Covariates for training (in sample) data for an event.<br />
Must be a data.frame or a matrix with rows corresponding to
observations and columns to variables.<br /> <code>crisk2.bart</code> will
generate draws of <code class="reqn">f_1(t, x)</code> for each <code class="reqn">x</code> which is
a row of <code>x.train</code> (note that the definition of <code>x.train</code> is
dependent on whether <code>y.train</code> has been specified; see below).  </p>
</td></tr>
<tr><td><code id="crisk2.bart_+3A_y.train">y.train</code></td>
<td>
<p> Event binary response for training (in sample)
data.<br /> If <code>y.train</code> is <code>NULL</code>, then <code>y.train</code>
(<code>x.train</code> and <code>x.test</code>, if specified) are generated by a
call to <code>surv.pre.bart</code> (which require that <code>times</code> and
<code>delta</code> be provided: see below); otherwise, <code>y.train</code>
(<code>x.train</code> and <code>x.test</code>, if specified) are utilized as
given assuming that the data construction has already been performed.
</p>
</td></tr>
<tr><td><code id="crisk2.bart_+3A_x.train2">x.train2</code></td>
<td>
<p> Covariates for training (in sample)
data of for a cause 1 event. Similar to <code>x.train</code> above.</p>
</td></tr>
<tr><td><code id="crisk2.bart_+3A_y.train2">y.train2</code></td>
<td>

<p>Cause 1 event binary response for training (in sample) data.
Similar to <code>y.train</code> above.
</p>
</td></tr>
<tr><td><code id="crisk2.bart_+3A_times">times</code></td>
<td>

<p>The time of event or right-censoring, <code class="reqn">s_i</code>.<br />
If <code>y.train</code> is <code>NULL</code>, then <code>times</code> (and <code>delta</code>)
must be provided.
</p>
</td></tr>
<tr><td><code id="crisk2.bart_+3A_delta">delta</code></td>
<td>

<p>The event indicator: 1 for cause 1, 2 for cause 2 and 0 is censored.<br />
If <code>y.train</code> is <code>NULL</code>, then <code>delta</code> (and <code>times</code>)
must be provided.
</p>
</td></tr>
<tr><td><code id="crisk2.bart_+3A_k">K</code></td>
<td>

<p>If provided, then coarsen <code>times</code> per the quantiles
<code class="reqn">1/K, 2/K, ..., K/K</code>.
</p>
</td></tr>
<tr><td><code id="crisk2.bart_+3A_x.test">x.test</code></td>
<td>

<p>Covariates for test (out of sample) data of an event.<br />
Must be a data.frame or a matrix and have the same structure as
<code>x.train</code>.<br />
<code>crisk2.bart</code> will generate draws of
<code class="reqn">f_1(t, x)</code> for each <code class="reqn">x</code> which is a row of
<code>x.test</code>.
</p>
</td></tr>
<tr><td><code id="crisk2.bart_+3A_x.test2">x.test2</code></td>
<td>

<p>Covariates for test (out of sample) data of a cause 1 event.
Similar to <code>x.test</code> above.</p>
</td></tr>
<tr><td><code id="crisk2.bart_+3A_sparse">sparse</code></td>
<td>
<p>Whether to perform variable selection based on a
sparse Dirichlet prior; see Linero 2016.</p>
</td></tr>
<tr><td><code id="crisk2.bart_+3A_theta">theta</code></td>
<td>
<p>Set <code class="reqn">theta</code> parameter; zero means random.</p>
</td></tr>
<tr><td><code id="crisk2.bart_+3A_omega">omega</code></td>
<td>
<p>Set <code class="reqn">omega</code> parameter; zero means random.</p>
</td></tr>
<tr><td><code id="crisk2.bart_+3A_a">a</code></td>
<td>
<p>Sparse parameter for <code class="reqn">Beta(a, b)</code> prior:
<code class="reqn">0.5&lt;=a&lt;=1</code> where lower values inducing more sparsity.</p>
</td></tr>
<tr><td><code id="crisk2.bart_+3A_b">b</code></td>
<td>
<p>Sparse parameter for <code class="reqn">Beta(a, b)</code> prior; typically,
<code>b=1</code>.</p>
</td></tr>
<tr><td><code id="crisk2.bart_+3A_rho">rho</code></td>
<td>
<p>Sparse parameter: typically <code>rho=p</code> where <code>p</code> is the
number of covariates in <code>x.train</code>.</p>
</td></tr>
<tr><td><code id="crisk2.bart_+3A_rho2">rho2</code></td>
<td>
<p>Sparse parameter: typically <code>rho2=p</code> where <code>p</code> is the
number of covariates  in <code>x.train2</code>.</p>
</td></tr>
<tr><td><code id="crisk2.bart_+3A_augment">augment</code></td>
<td>
<p>Whether data augmentation is to be performed in sparse
variable selection.</p>
</td></tr>
<tr><td><code id="crisk2.bart_+3A_xinfo">xinfo</code></td>
<td>
<p> You can provide the cutpoints to BART or let BART
choose them for you.  To provide them, use the <code>xinfo</code>
argument to specify a list (matrix) where the items (rows) are the
covariates and the contents of the items (columns) are the
cutpoints.  </p>
</td></tr>
<tr><td><code id="crisk2.bart_+3A_xinfo2">xinfo2</code></td>
<td>
<p> Cause 2 cutpoints. </p>
</td></tr>
<tr><td><code id="crisk2.bart_+3A_usequants">usequants</code></td>
<td>
<p> If <code>usequants=FALSE</code>, then the
cutpoints in <code>xinfo</code> are generated uniformly; otherwise,
if <code>TRUE</code>, uniform quantiles are used for the cutpoints. </p>
</td></tr>

<tr><td><code id="crisk2.bart_+3A_rm.const">rm.const</code></td>
<td>
<p> Whether or not to remove constant variables.</p>
</td></tr>
<tr><td><code id="crisk2.bart_+3A_type">type</code></td>
<td>
<p> Whether to employ probit BART via Albert-Chib,
<code>'pbart'</code>, or logistic BART by Holmes-Held, <code>'lbart'</code>. </p>
</td></tr>
<tr><td><code id="crisk2.bart_+3A_ntype">ntype</code></td>
<td>
<p> The integer equivalent of <code>type</code> where
<code>'wbart'</code> is 1, <code>'pbart'</code> is 2 and
<code>'lbart'</code> is 3.</p>
</td></tr>
<tr><td><code id="crisk2.bart_+3A_k">k</code></td>
<td>
<p> k is the number of prior standard deviations
<code class="reqn">f_h(t, x)</code>
is away from +/-3.  The bigger k is, the more conservative the
fitting will be.  </p>
</td></tr>
<tr><td><code id="crisk2.bart_+3A_power">power</code></td>
<td>

<p>Power parameter for tree prior.
</p>
</td></tr>
<tr><td><code id="crisk2.bart_+3A_base">base</code></td>
<td>

<p>Base parameter for tree prior.
</p>
</td></tr>
<tr><td><code id="crisk2.bart_+3A_offset">offset</code></td>
<td>

<p>Cause 1 binary offset.
</p>
</td></tr>
<tr><td><code id="crisk2.bart_+3A_offset2">offset2</code></td>
<td>

<p>Cause 2 binary offset.
</p>
</td></tr>
<tr><td><code id="crisk2.bart_+3A_tau.num">tau.num</code></td>
<td>
<p> The numerator in the <code>tau</code> definition. </p>
</td></tr>
<tr><td><code id="crisk2.bart_+3A_ntree">ntree</code></td>
<td>

<p>The number of trees in the sum.
</p>
</td></tr>
<tr><td><code id="crisk2.bart_+3A_numcut">numcut</code></td>
<td>
<p> The number of possible values of cutpoints (see
<code>usequants</code>).  If a single number if given, this is used for all
variables.  Otherwise a vector with length equal to
<code>ncol(x.train)</code> is required, where the <code class="reqn">i^{th}</code>
element gives the number of cutpoints used for the <code class="reqn">i^{th}</code>
variable in <code>x.train</code>.  If <code>usequants</code> is <code>FALSE</code>,
<code>numcut</code> equally spaced cutoffs are used covering the range of
values in the corresponding column of <code>x.train</code>.  If
<code>usequants</code> is <code>TRUE</code>, then <code>min(numcut, the number of
   unique values in the corresponding columns of x.train - 1)</code> cutpoint
values are used.  </p>
</td></tr>
<tr><td><code id="crisk2.bart_+3A_ndpost">ndpost</code></td>
<td>

<p>The number of posterior draws returned.
</p>
</td></tr>
<tr><td><code id="crisk2.bart_+3A_nskip">nskip</code></td>
<td>

<p>Number of MCMC iterations to be treated as burn in.
</p>
</td></tr>
<tr><td><code id="crisk2.bart_+3A_keepevery">keepevery</code></td>
<td>

<p>Every <code>keepevery</code> draw is kept to be returned to the user.
</p>
</td></tr>












<tr><td><code id="crisk2.bart_+3A_printevery">printevery</code></td>
<td>

<p>As the MCMC runs, a message is printed every <code>printevery</code> draws.
</p>
</td></tr>








<tr><td><code id="crisk2.bart_+3A_id">id</code></td>
<td>

<p><code>crisk2.bart</code> only: unique identifier added to returned list.
</p>
</td></tr>
<tr><td><code id="crisk2.bart_+3A_seed">seed</code></td>
<td>

<p><code>mc.crisk2.bart</code> only: seed required for reproducible MCMC.
</p>
</td></tr>
<tr><td><code id="crisk2.bart_+3A_mc.cores">mc.cores</code></td>
<td>

<p><code>mc.crisk2.bart</code> only: number of cores to employ in parallel.
</p>
</td></tr>
<tr><td><code id="crisk2.bart_+3A_nice">nice</code></td>
<td>
 <p><code>mc.crisk2.bart</code> only: set the job niceness.  The
default niceness is 19: niceness goes from 0 (highest priority) to
19 (lowest priority).  </p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>crisk2.bart</code> returns an object of type <code>crisk2bart</code> which is
essentially a list.  Besides the items listed
below, the list has <code>offset</code>, <code>offset2</code>,
<code>times</code> which are the unique times, <code>K</code>
which is the number of unique times, <code>tx.train</code> and
<code>tx.test</code>, if any.
</p>
<table>
<tr><td><code>yhat.train</code></td>
<td>
<p> A matrix with <code>ndpost</code> rows and
<code>nrow(x.train)</code> columns.  Each row corresponds to a draw
<code class="reqn">f^*_1</code> from the posterior of <code class="reqn">f_1</code> and each column
corresponds to a row of <code>x.train</code>.  The <code class="reqn">(i,j)</code> value is
<code class="reqn">f^*_1(t, x)</code> for the <code class="reqn">i^{th}</code> kept draw of
<code class="reqn">f_1</code> and the <code class="reqn">j^{th}</code> row of <code>x.train</code>.
Burn-in is dropped.  </p>
</td></tr>
<tr><td><code>yhat.test</code></td>
<td>
<p>Same as <code>yhat.train</code> but now the x's are the rows of the test data.</p>
</td></tr>
<tr><td><code>surv.test</code></td>
<td>
<p>test data fits for the survival function, <code class="reqn">S(t, x)</code>.</p>
</td></tr>
<tr><td><code>surv.test.mean</code></td>
<td>
<p>mean of <code>surv.test</code> over the posterior samples.</p>
</td></tr>
<tr><td><code>prob.test</code></td>
<td>
<p>The probability of suffering an event.</p>
</td></tr>
<tr><td><code>prob.test2</code></td>
<td>
<p>The probability of suffering a cause 1 event.</p>
</td></tr>
<tr><td><code>cif.test</code></td>
<td>
<p>The cumulative incidence function of cause 1,
<code class="reqn">F_1(t, x)</code>.</p>
</td></tr>
<tr><td><code>cif.test2</code></td>
<td>
<p>The cumulative incidence function of cause 2,
<code class="reqn">F_2(t, x)</code>.</p>
</td></tr>


<tr><td><code>cif.test.mean</code></td>
<td>
<p>mean of <code>cif.test</code> columns for cause 1.</p>
</td></tr>
<tr><td><code>cif.test2.mean</code></td>
<td>
<p>mean of <code>cif.test2</code> columns for cause 2.</p>
</td></tr>
<tr><td><code>varcount</code></td>
<td>
<p>a matrix with <code>ndpost</code> rows and
<code>nrow(x.train)</code> columns.
Each row is for a draw. For each variable (corresponding to the columns),
the total count of the number of times
this variable is used for an event in a tree decision rule (over all trees) is given.</p>
</td></tr>
<tr><td><code>varcount2</code></td>
<td>
<p> For each variable the total count of the number of times
this variable is used for a cause 1 event in a tree decision rule is given.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+surv.pre.bart">surv.pre.bart</a></code>, <code><a href="#topic+predict.crisk2bart">predict.crisk2bart</a></code>,
<code><a href="#topic+mc.crisk2.pwbart">mc.crisk2.pwbart</a></code>, <code><a href="#topic+crisk.bart">crisk.bart</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(transplant)

pfit &lt;- survfit(Surv(futime, event) ~ abo, transplant)

# competing risks for type O
plot(pfit[4,], xscale=7, xmax=735, col=1:3, lwd=2, ylim=c(0, 1),
       xlab='t (weeks)', ylab='Aalen-Johansen (AJ) CI(t)')
    legend(450, .4, c("Death", "Transplant", "Withdrawal"), col=1:3, lwd=2)
## plot(pfit[4,], xscale=30.5, xmax=735, col=1:3, lwd=2, ylim=c(0, 1),
##        xlab='t (months)', ylab='Aalen-Johansen (AJ) CI(t)')
##     legend(450, .4, c("Death", "Transplant", "Withdrawal"), col=1:3, lwd=2)

delta &lt;- (as.numeric(transplant$event)-1)
## recode so that delta=1 is cause of interest; delta=2 otherwise
delta[delta==1] &lt;- 4
delta[delta==2] &lt;- 1
delta[delta&gt;1] &lt;- 2
table(delta, transplant$event)

times &lt;- pmax(1, ceiling(transplant$futime/7)) ## weeks
##times &lt;- pmax(1, ceiling(transplant$futime/30.5)) ## months
table(times)

typeO &lt;- 1*(transplant$abo=='O')
typeA &lt;- 1*(transplant$abo=='A')
typeB &lt;- 1*(transplant$abo=='B')
typeAB &lt;- 1*(transplant$abo=='AB')
table(typeA, typeO)

x.train &lt;- cbind(typeO, typeA, typeB, typeAB)

x.test &lt;- cbind(1, 0, 0, 0)
dimnames(x.test)[[2]] &lt;- dimnames(x.train)[[2]]

##test BART with token run to ensure installation works
set.seed(99)
post &lt;- crisk2.bart(x.train=x.train, times=times, delta=delta,
                   x.test=x.test, nskip=1, ndpost=1, keepevery=1)

## Not run: 

## run one long MCMC chain in one process
## set.seed(99)
## post &lt;- crisk2.bart(x.train=x.train, times=times, delta=delta, x.test=x.test)

## in the interest of time, consider speeding it up by parallel processing
## run "mc.cores" number of shorter MCMC chains in parallel processes
post &lt;- mc.crisk2.bart(x.train=x.train, times=times, delta=delta,
                      x.test=x.test, seed=99, mc.cores=8)

K &lt;- post$K

typeO.cif.mean &lt;- apply(post$cif.test, 2, mean)
typeO.cif.025 &lt;- apply(post$cif.test, 2, quantile, probs=0.025)
typeO.cif.975 &lt;- apply(post$cif.test, 2, quantile, probs=0.975)

plot(pfit[4,], xscale=7, xmax=735, col=1:3, lwd=2, ylim=c(0, 0.8),
       xlab='t (weeks)', ylab='CI(t)')
points(c(0, post$times)*7, c(0, typeO.cif.mean), col=4, type='s', lwd=2)
points(c(0, post$times)*7, c(0, typeO.cif.025), col=4, type='s', lwd=2, lty=2)
points(c(0, post$times)*7, c(0, typeO.cif.975), col=4, type='s', lwd=2, lty=2)
     legend(450, .4, c("Transplant(BART)", "Transplant(AJ)",
                       "Death(AJ)", "Withdrawal(AJ)"),
            col=c(4, 2, 1, 3), lwd=2)
##dev.copy2pdf(file='../vignettes/figures/liver-BART.pdf')
## plot(pfit[4,], xscale=30.5, xmax=735, col=1:3, lwd=2, ylim=c(0, 0.8),
##        xlab='t (months)', ylab='CI(t)')
## points(c(0, post$times)*30.5, c(0, typeO.cif.mean), col=4, type='s', lwd=2)
## points(c(0, post$times)*30.5, c(0, typeO.cif.025), col=4, type='s', lwd=2, lty=2)
## points(c(0, post$times)*30.5, c(0, typeO.cif.975), col=4, type='s', lwd=2, lty=2)
##      legend(450, .4, c("Transplant(BART)", "Transplant(AJ)",
##                        "Death(AJ)", "Withdrawal(AJ)"),
##             col=c(4, 2, 1, 3), lwd=2)


## End(Not run)
</code></pre>

<hr>
<h2 id='draw_lambda_i'>Testing truncated Normal sampling</h2><span id='topic+draw_lambda_i'></span>

<h3>Description</h3>

<p> Truncated Normal latents with non-unit variance
are necessary for logistic BART.  </p>


<h3>Usage</h3>

<pre><code class='language-R'>draw_lambda_i(lambda, mean, kmax=1000, thin=1)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="draw_lambda_i_+3A_lambda">lambda</code></td>
<td>
<p> Previous value of lambda.</p>
</td></tr>
<tr><td><code id="draw_lambda_i_+3A_mean">mean</code></td>
<td>
<p> Mean of truncated Normal. </p>
</td></tr>
<tr><td><code id="draw_lambda_i_+3A_kmax">kmax</code></td>
<td>
<p> The number of terms in the mixture. </p>
</td></tr>
<tr><td><code id="draw_lambda_i_+3A_thin">thin</code></td>
<td>
<p> The thinning parameter. </p>
</td></tr>
</table>


<h3>Value</h3>

<p> Returns the variance for a truncated Normal, i.e., <code class="reqn">N(mean,
  lambda)I(tau, infinity)</code>.  </p>


<h3>See Also</h3>

<p><code><a href="#topic+rtnorm">rtnorm</a>, <a href="#topic+lbart">lbart</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
set.seed(12)

draw_lambda_i(1, 2)
rtnorm(1, 2, sqrt(6.773462), 6)
draw_lambda_i(6.773462, 2)

</code></pre>

<hr>
<h2 id='gbart'>Generalized BART for continuous and binary outcomes</h2><span id='topic+gbart'></span><span id='topic+mc.gbart'></span>

<h3>Description</h3>

<p>BART is a Bayesian &ldquo;sum-of-trees&rdquo; model.<br />
For a numeric response <code class="reqn">y</code>, we have
<code class="reqn">y = f(x) + \epsilon</code>,
where <code class="reqn">\epsilon \sim N(0,\sigma^2)</code>.<br />
</p>
<p><code class="reqn">f</code> is the sum of many tree models.
The goal is to have very flexible inference for the uknown
function <code class="reqn">f</code>.
</p>
<p>In the spirit of &ldquo;ensemble models&rdquo;,
each tree is constrained by a prior to be a weak learner
so that it contributes a small amount to the overall fit.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gbart(
      x.train, y.train,
      x.test=matrix(0,0,0), type='wbart',
      ntype=as.integer(
          factor(type, levels=c('wbart', 'pbart', 'lbart'))),
      sparse=FALSE, theta=0, omega=1,
      a=0.5, b=1, augment=FALSE, rho=NULL,
      xinfo=matrix(0,0,0), usequants=FALSE,
      rm.const=TRUE,
      sigest=NA, sigdf=3, sigquant=0.90,
      k=2, power=2, base=0.95,
      
      lambda=NA, tau.num=c(NA, 3, 6)[ntype], 
      offset=NULL, w=rep(1, length(y.train)),
      ntree=c(200L, 50L, 50L)[ntype], numcut=100L,
      
      ndpost=1000L, nskip=100L, 
      keepevery=c(1L, 10L, 10L)[ntype],
      printevery=100L, transposed=FALSE,
      hostname=FALSE,
      mc.cores = 1L, ## mc.gbart only
      nice = 19L,    ## mc.gbart only
      seed = 99L     ## mc.gbart only
)

mc.gbart(
         x.train, y.train,
         x.test=matrix(0,0,0), type='wbart',
         ntype=as.integer(
             factor(type, levels=c('wbart', 'pbart', 'lbart'))),
         sparse=FALSE, theta=0, omega=1,
         a=0.5, b=1, augment=FALSE, rho=NULL,
         xinfo=matrix(0,0,0), usequants=FALSE,
         rm.const=TRUE,
         sigest=NA, sigdf=3, sigquant=0.90,
         k=2, power=2, base=0.95,
         
         lambda=NA, tau.num=c(NA, 3, 6)[ntype], 
         offset=NULL, w=rep(1, length(y.train)),
         
         ntree=c(200L, 50L, 50L)[ntype], numcut=100L,
         ndpost=1000L, nskip=100L, 
         keepevery=c(1L, 10L, 10L)[ntype],
         printevery=100L, transposed=FALSE,
         hostname=FALSE,
         mc.cores = 2L, nice = 19L, seed = 99L
)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gbart_+3A_x.train">x.train</code></td>
<td>
<p> Explanatory variables for training (in sample)
data.<br /> May be a matrix or a data frame, with (as usual) rows
corresponding to observations and columns to variables.<br /> If a
variable is a factor in a data frame, it is replaced with dummies.
Note that <code class="reqn">q</code> dummies are created if <code class="reqn">q&gt;2</code> and one dummy
created if <code class="reqn">q=2</code> where <code class="reqn">q</code> is the number of levels of the
factor.  <code>gbart</code> will generate draws of <code class="reqn">f(x)</code> for each
<code class="reqn">x</code> which is a row of <code>x.train</code>.  </p>
</td></tr>
<tr><td><code id="gbart_+3A_y.train">y.train</code></td>
<td>

<p>Continuous or binary dependent variable for training (in sample) data.<br />
If <code class="reqn">y</code> is numeric, then a continuous BART model is fit (Normal errors).<br />
If <code class="reqn">y</code> is binary (has only 0's and 1's), then a binary BART model
with a probit link is fit by default: you can over-ride the default via the
argument <code>type</code> to specify a logit BART model.
</p>
</td></tr>
<tr><td><code id="gbart_+3A_x.test">x.test</code></td>
<td>
<p> Explanatory variables for test (out of sample)
data. Should have same structure as <code>x.train</code>.
<code>gbart</code> will generate draws of <code class="reqn">f(x)</code> for each <code class="reqn">x</code> which
is a row of <code>x.test</code>.  </p>
</td></tr>
<tr><td><code id="gbart_+3A_type">type</code></td>
<td>
<p> You can use this argument to specify the type of fit.
<code>'wbart'</code> for continuous BART, <code>'pbart'</code> for probit BART or
<code>'lbart'</code> for logit BART. </p>
</td></tr>
<tr><td><code id="gbart_+3A_ntype">ntype</code></td>
<td>
<p> The integer equivalent of <code>type</code> where
<code>'wbart'</code> is 1, <code>'pbart'</code> is 2 and
<code>'lbart'</code> is 3.</p>
</td></tr>
<tr><td><code id="gbart_+3A_sparse">sparse</code></td>
<td>
<p>Whether to perform variable selection based on a
sparse Dirichlet prior rather than simply uniform; see Linero 2016.</p>
</td></tr>
<tr><td><code id="gbart_+3A_theta">theta</code></td>
<td>
<p>Set <code class="reqn">theta</code> parameter; zero means random.</p>
</td></tr>
<tr><td><code id="gbart_+3A_omega">omega</code></td>
<td>
<p>Set <code class="reqn">omega</code> parameter; zero means random.</p>
</td></tr>
<tr><td><code id="gbart_+3A_a">a</code></td>
<td>
<p>Sparse parameter for <code class="reqn">Beta(a, b)</code> prior:
<code class="reqn">0.5&lt;=a&lt;=1</code> where lower values inducing more sparsity.</p>
</td></tr>
<tr><td><code id="gbart_+3A_b">b</code></td>
<td>
<p>Sparse parameter for <code class="reqn">Beta(a, b)</code> prior; typically,
<code class="reqn">b=1</code>.</p>
</td></tr>
<tr><td><code id="gbart_+3A_rho">rho</code></td>
<td>
<p>Sparse parameter: typically <code class="reqn">rho=p</code> where <code class="reqn">p</code> is the
number of covariates under consideration.</p>
</td></tr>
<tr><td><code id="gbart_+3A_augment">augment</code></td>
<td>
<p>Whether data augmentation is to be performed in sparse
variable selection.</p>
</td></tr>
<tr><td><code id="gbart_+3A_xinfo">xinfo</code></td>
<td>
<p> You can provide the cutpoints to BART or let BART
choose them for you.  To provide them, use the <code>xinfo</code>
argument to specify a list (matrix) where the items (rows) are the
covariates and the contents of the items (columns) are the
cutpoints.  </p>
</td></tr>
<tr><td><code id="gbart_+3A_usequants">usequants</code></td>
<td>
<p> If <code>usequants=FALSE</code>, then the
cutpoints in <code>xinfo</code> are generated uniformly; otherwise,
if <code>TRUE</code>, uniform quantiles are used for the cutpoints. </p>
</td></tr>
<tr><td><code id="gbart_+3A_rm.const">rm.const</code></td>
<td>
<p> Whether or not to remove constant variables.</p>
</td></tr>
<tr><td><code id="gbart_+3A_sigest">sigest</code></td>
<td>
<p> The prior for the error variance
(<code class="reqn">sigma^2</code>) is inverted chi-squared (the standard
conditionally conjugate prior).  The prior is specified by choosing
the degrees of freedom, a rough estimate of the corresponding
standard deviation and a quantile to put this rough estimate at.  If
<code>sigest=NA</code> then the rough estimate will be the usual least squares
estimator.  Otherwise the supplied value will be used.
Not used if <code class="reqn">y</code> is binary.
</p>
</td></tr>
<tr><td><code id="gbart_+3A_sigdf">sigdf</code></td>
<td>

<p>Degrees of freedom for error variance prior.
Not used if <code class="reqn">y</code> is binary.
</p>
</td></tr>
<tr><td><code id="gbart_+3A_sigquant">sigquant</code></td>
<td>
<p> The quantile of the prior that the rough estimate
(see <code>sigest</code>) is placed at.  The closer the quantile is to 1, the more
aggresive the fit will be as you are putting more prior weight on
error standard deviations (<code class="reqn">sigma</code>) less than the rough
estimate.  Not used if <code class="reqn">y</code> is binary.  </p>
</td></tr>
<tr><td><code id="gbart_+3A_k">k</code></td>
<td>
<p> For numeric <code class="reqn">y</code>, <code>k</code> is the number of prior
standard deviations <code class="reqn">E(Y|x) = f(x)</code> is away from +/-0.5.  For
binary <code class="reqn">y</code>, <code>k</code> is the number of prior standard deviations
<code class="reqn">f(x)</code> is away from +/-3.  The bigger <code>k</code> is, the more
conservative the fitting will be.  </p>
</td></tr>
<tr><td><code id="gbart_+3A_power">power</code></td>
<td>

<p>Power parameter for tree prior.
</p>
</td></tr>
<tr><td><code id="gbart_+3A_base">base</code></td>
<td>

<p>Base parameter for tree prior.
</p>
</td></tr>



<tr><td><code id="gbart_+3A_lambda">lambda</code></td>
<td>

<p>The scale of the prior for the variance.  If <code>lambda</code> is zero,
then the variance is to be considered fixed and known at the given
value of <code>sigest</code>.  Not used if <code class="reqn">y</code> is binary.
</p>
</td></tr>
<tr><td><code id="gbart_+3A_tau.num">tau.num</code></td>
<td>
<p> The numerator in the <code>tau</code> definition, i.e.,
<code>tau=tau.num/(k*sqrt(ntree))</code>. </p>
</td></tr>



<tr><td><code id="gbart_+3A_offset">offset</code></td>
<td>
<p> Continous BART operates on <code>y.train</code> centered by
<code>offset</code> which defaults to <code>mean(y.train)</code>.  With binary
BART, the centering is <code class="reqn">P(Y=1 | x) = F(f(x) + offset)</code> where
<code>offset</code> defaults to <code>F^{-1}(mean(y.train))</code>.  You can use
the <code>offset</code> parameter to over-ride these defaults.</p>
</td></tr>
<tr><td><code id="gbart_+3A_w">w</code></td>
<td>
<p> Vector of weights which multiply the standard deviation.
Not used if <code class="reqn">y</code> is binary.  </p>
</td></tr>
<tr><td><code id="gbart_+3A_ntree">ntree</code></td>
<td>

<p>The number of trees in the sum.
</p>
</td></tr>
<tr><td><code id="gbart_+3A_numcut">numcut</code></td>
<td>
<p> The number of possible values of <code class="reqn">c</code> (see
<code>usequants</code>).  If a single number if given, this is used for all
variables.  Otherwise a vector with length equal to
<code>ncol(x.train)</code> is required, where the <code class="reqn">i^{th}</code>
element gives the number of <code class="reqn">c</code> used for the <code class="reqn">i^{th}</code>
variable in <code>x.train</code>.  If usequants is false, numcut equally
spaced cutoffs are used covering the range of values in the
corresponding column of <code>x.train</code>.  If <code>usequants</code> is true, then
<code class="reqn">min(numcut, the number of unique values in the corresponding
   columns of x.train - 1)</code> values are used.  </p>
</td></tr>
<tr><td><code id="gbart_+3A_ndpost">ndpost</code></td>
<td>

<p>The number of posterior draws returned.
</p>
</td></tr>
<tr><td><code id="gbart_+3A_nskip">nskip</code></td>
<td>

<p>Number of MCMC iterations to be treated as burn in.
</p>
</td></tr>
<tr><td><code id="gbart_+3A_printevery">printevery</code></td>
<td>

<p>As the MCMC runs, a message is printed every printevery draws.
</p>
</td></tr>
<tr><td><code id="gbart_+3A_keepevery">keepevery</code></td>
<td>

<p>Every keepevery draw is kept to be returned to the user.<br />




</p>
</td></tr>
<tr><td><code id="gbart_+3A_transposed">transposed</code></td>
<td>

<p>When running <code>gbart</code> in parallel, it is more memory-efficient
to transpose <code>x.train</code> and <code>x.test</code>, if any, prior to
calling <code>mc.gbart</code>.
</p>
</td></tr>
<tr><td><code id="gbart_+3A_hostname">hostname</code></td>
<td>

<p>When running on a cluster occasionally it is useful
to track on which node each chain is running; to do so
set this argument to <code>TRUE</code>.
</p>
</td></tr>
<tr><td><code id="gbart_+3A_seed">seed</code></td>
<td>

<p>Setting the seed required for reproducible MCMC.
</p>
</td></tr>
<tr><td><code id="gbart_+3A_mc.cores">mc.cores</code></td>
<td>

<p>Number of cores to employ in parallel.
</p>
</td></tr>
<tr><td><code id="gbart_+3A_nice">nice</code></td>
<td>

<p>Set the job niceness.  The default
niceness is 19: niceness goes from 0 (highest) to 19 (lowest).
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>BART is a Bayesian MCMC method.
At each MCMC interation, we produce a draw from the joint posterior
<code class="reqn">(f,\sigma) | (x,y)</code> in the numeric <code class="reqn">y</code> case
and just <code class="reqn">f</code> in the binary <code class="reqn">y</code> case.
</p>
<p>Thus, unlike a lot of other modelling methods in R, we do not produce
a single model object from which fits and summaries may be extracted.
The output consists of values <code class="reqn">f^*(x)</code> (and
<code class="reqn">\sigma^*</code> in the numeric case) where * denotes a
particular draw.  The <code class="reqn">x</code> is either a row from the training data,
<code>x.train</code> or the test data, <code>x.test</code>.
</p>
<p>For <code>x.train</code>/<code>x.test</code> with missing data elements, <code>gbart</code>
will singly impute them with hot decking. For one or more missing
covariates, record-level hot-decking imputation <cite>deWaPann11</cite> is
employed that is biased towards the null, i.e., nonmissing values
from another record are randomly selected regardless of the
outcome. Since <code>mc.gbart</code> runs multiple <code>gbart</code> threads in
parallel, <code>mc.gbart</code> performs multiple imputation with hot
decking, i.e., a separate imputation for each thread.  This
record-level hot-decking imputation is biased towards the null, i.e.,
nonmissing values from another record are randomly selected
regardless of <code>y.train</code>.
</p>


<h3>Value</h3>







<p><code>gbart</code> returns an object of type <code>gbart</code> which is
essentially a list. 
In the numeric <code class="reqn">y</code> case, the list has components:
</p>
<table>
<tr><td><code>yhat.train</code></td>
<td>

<p>A matrix with ndpost rows and nrow(x.train) columns.
Each row corresponds to a draw <code class="reqn">f^*</code> from the posterior of <code class="reqn">f</code>
and each column corresponds to a row of x.train.
The <code class="reqn">(i,j)</code> value is <code class="reqn">f^*(x)</code> for the <code class="reqn">i^{th}</code> kept draw of <code class="reqn">f</code>
and the <code class="reqn">j^{th}</code> row of x.train.<br />
Burn-in is dropped.
</p>
</td></tr>
<tr><td><code>yhat.test</code></td>
<td>
<p>Same as yhat.train but now the x's are the rows of the test data.</p>
</td></tr>
<tr><td><code>yhat.train.mean</code></td>
<td>
<p>train data fits = mean of yhat.train columns.</p>
</td></tr>
<tr><td><code>yhat.test.mean</code></td>
<td>
<p>test data fits = mean of yhat.test columns.</p>
</td></tr>
<tr><td><code>sigma</code></td>
<td>
<p>post burn in draws of sigma, length = ndpost.</p>
</td></tr>
<tr><td><code>first.sigma</code></td>
<td>
<p>burn-in draws of sigma.</p>
</td></tr>
<tr><td><code>varcount</code></td>
<td>
<p>a matrix with ndpost rows and nrow(x.train) columns.
Each row is for a draw. For each variable (corresponding to the columns),
the total count of the number of times
that variable is used in a tree decision rule (over all trees) is given.</p>
</td></tr>
<tr><td><code>sigest</code></td>
<td>

<p>The rough error standard deviation (<code class="reqn">\sigma</code>) used in the prior.
</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+pbart">pbart</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>##simulate data (example from Friedman MARS paper)
f = function(x){
10*sin(pi*x[,1]*x[,2]) + 20*(x[,3]-.5)^2+10*x[,4]+5*x[,5]
}
sigma = 1.0  #y = f(x) + sigma*z , z~N(0,1)
n = 100      #number of observations
set.seed(99)
x=matrix(runif(n*10),n,10) #10 variables, only first 5 matter
Ey = f(x)
y=Ey+sigma*rnorm(n)
lmFit = lm(y~.,data.frame(x,y)) #compare lm fit to BART later

##test BART with token run to ensure installation works
set.seed(99)
bartFit = wbart(x,y,nskip=5,ndpost=5)

## Not run: 
##run BART
set.seed(99)
bartFit = wbart(x,y)

##compare BART fit to linear matter and truth = Ey
fitmat = cbind(y,Ey,lmFit$fitted,bartFit$yhat.train.mean)
colnames(fitmat) = c('y','Ey','lm','bart')
print(cor(fitmat))

## End(Not run)
</code></pre>

<hr>
<h2 id='gewekediag'>Geweke's convergence diagnostic</h2><span id='topic+gewekediag'></span>

<h3>Description</h3>

<p>Geweke (1992) proposed a convergence diagnostic for Markov chains
based on a test for equality of the means of the first and last part
of a Markov chain (by default the first 10% and the last 50%).  If the
samples are drawn from the stationary distribution of the chain, the two
means are equal and Geweke's statistic has an asymptotically standard
normal distribution.
</p>
<p>The test statistic is a standard Z-score: the difference between the
two sample means divided by its estimated standard error.  The standard
error is estimated from the spectral density at zero and so takes into
account any autocorrelation.
</p>
<p>The Z-score is calculated under the assumption that the two parts of
the chain are asymptotically independent, which requires that the sum
of <code>frac1</code> and <code>frac2</code> be strictly less than 1.
</p>
<p>Adapted from the <code>geweke.diag</code> function of
the coda package which passes <code>mcmc</code> objects as arguments
rather than matrices.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gewekediag(x, frac1=0.1, frac2=0.5)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gewekediag_+3A_x">x</code></td>
<td>
<p>Matrix of MCMC chains: the rows are the samples and
the columns are different &quot;parameters&quot;.  For BART, generally, the
columns are estimates of <code class="reqn">f</code>.  For <code>pbart</code>, they are
different subjects.  For <code>surv.bart</code>, they are different subjects
at a grid of times.</p>
</td></tr>
<tr><td><code id="gewekediag_+3A_frac1">frac1</code></td>
<td>
<p>fraction to use from beginning of chain</p>
</td></tr>
<tr><td><code id="gewekediag_+3A_frac2">frac2</code></td>
<td>
<p>fraction to use from end of chain</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Z-scores for a test of equality of means
between the first and last parts of the chain. A separate
statistic is calculated for each variable in each chain.
</p>


<h3>References</h3>

<p>Geweke J. (1992)
Evaluating the Accuracy of Sampling-Based Approaches to the Calculation of Posterior Moments.
In JM Bernado, JO Berger, AP Dawid, AFM Smith (eds.), Bayesian Statistics 4, pp. 169-193.
Oxford University Press, Oxford.
</p>
<p>Plummer M., Best N., Cowles K. and Vines K. (2006)
CODA: Convergence Diagnosis and Output Analysis for MCMC.
R News, vol 6, 7-11.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+spectrum0ar">spectrum0ar</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## load survival package for the advanced lung cancer example
data(lung)

group &lt;- -which(is.na(lung[ , 7])) ## remove missing row for ph.karno
times &lt;- lung[group, 2]   ##lung$time
delta &lt;- lung[group, 3]-1 ##lung$status: 1=censored, 2=dead
                          ##delta: 0=censored, 1=dead

## this study reports time in days rather than months like other studies
## coarsening from days to months will reduce the computational burden
times &lt;- ceiling(times/30)

summary(times)
table(delta)

x.train &lt;- as.matrix(lung[group, c(4, 5, 7)]) ## matrix of observed covariates

## lung$age:        Age in years
## lung$sex:        Male=1 Female=2
## lung$ph.karno:   Karnofsky performance score (dead=0:normal=100:by=10)
##                  rated by physician

dimnames(x.train)[[2]] &lt;- c('age(yr)', 'M(1):F(2)', 'ph.karno(0:100:10)')

summary(x.train[ , 1])
table(x.train[ , 2])
table(x.train[ , 3])

x.test &lt;- matrix(nrow=84, ncol=3) ## matrix of covariate scenarios

dimnames(x.test)[[2]] &lt;- dimnames(x.train)[[2]]

i &lt;- 1

for(age in 5*(9:15)) for(sex in 1:2) for(ph.karno in 10*(5:10)) {
    x.test[i, ] &lt;- c(age, sex, ph.karno)
    i &lt;- i+1
}

## Not run: 
    set.seed(99)
    post &lt;- surv.bart(x.train=x.train, times=times, delta=delta, x.test=x.test)
    ## in the interest of time, consider speeding it up by parallel processing
    ## run "mc.cores" number of shorter MCMC chains in parallel processes
    ## post &lt;- mc.surv.bart(x.train=x.train, times=times, delta=delta,
    ##                      x.test=x.test, mc.cores=8, seed=99)

    N &lt;- nrow(x.test)

    K &lt;- post$K
    ## select 10 lung cancer patients uniformly spread out over the data set
    h &lt;- seq(1, N*K, floor(N/10)*K)

    for(i in h) {
        post.mcmc &lt;- post$yhat.test[ , (i-1)+1:K]
        z &lt;- gewekediag(post.mcmc)$z
        y &lt;- max(c(4, abs(z)))

        ## plot the z scores vs. time for each patient
        if(i==1) plot(post$times, z, ylim=c(-y, y), type='l',
                      xlab='t', ylab='z')
        else lines(post$times, z, type='l')
    }
    ## add two-sided alpha=0.05 critical value lines
    lines(post$times, rep(-1.96, K), type='l', lty=2)
    lines(post$times, rep( 1.96, K), type='l', lty=2)


## End(Not run)

</code></pre>

<hr>
<h2 id='lbart'>Logit BART for dichotomous outcomes with Logistic latents</h2><span id='topic+lbart'></span>

<h3>Description</h3>

<p>BART is a Bayesian &ldquo;sum-of-trees&rdquo; model.<br />
For numeric response <code class="reqn">y</code>, we have
<code class="reqn">y = f(x) + \epsilon</code>,
where <code class="reqn">\epsilon \sim Log(0, 1)</code>.<br />
For a binary response <code class="reqn">y</code>, <code class="reqn">P(Y=1 | x) = F(f(x))</code>, where <code class="reqn">F</code>
denotes the standard Logistic CDF (logit link).
</p>
<p>In both cases, <code class="reqn">f</code> is the sum of many tree models.
The goal is to have very flexible inference for the uknown
function <code class="reqn">f</code>.
</p>
<p>In the spirit of &ldquo;ensemble models&rdquo;,
each tree is constrained by a prior to be a weak learner
so that it contributes a
small amount to the overall fit.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lbart(
   x.train, y.train, x.test=matrix(0.0,0,0),
   sparse=FALSE, a=0.5, b=1, augment=FALSE, rho=NULL,
   xinfo=matrix(0.0,0,0), usequants=FALSE,
   cont=FALSE, rm.const=TRUE, tau.interval=0.95,
   k=2.0, power=2.0, base=.95, 
   binaryOffset=NULL,
   ntree=200L, numcut=100L,
   ndpost=1000L, nskip=100L,
   keepevery=1L,
   nkeeptrain=ndpost, nkeeptest=ndpost,
   
   nkeeptreedraws=ndpost,
   printevery=100L, transposed=FALSE 
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lbart_+3A_x.train">x.train</code></td>
<td>

<p>Explanatory variables for training (in sample) data.<br />
May be a matrix or a data frame,
with (as usual) rows corresponding to observations and columns to variables.<br />
If a variable is a factor in a data frame, it is replaced with dummies.
Note that q dummies are created if q&gt;2 and
one dummy is created if q=2, where q is the number of levels of the factor.
<code>lbart</code> will generate draws of <code class="reqn">f(x)</code> for each <code class="reqn">x</code>
which is a row of x.train.
</p>
</td></tr>
<tr><td><code id="lbart_+3A_y.train">y.train</code></td>
<td>

<p>Binary dependent variable for training (in sample) data.<br />



</p>
</td></tr>
<tr><td><code id="lbart_+3A_x.test">x.test</code></td>
<td>

<p>Explanatory variables for test (out of sample) data.<br />
Should have same structure as x.train.<br />
<code>lbart</code> will generate draws of <code class="reqn">f(x)</code> for each <code class="reqn">x</code> which is a row of x.test.
</p>
</td></tr>
<tr><td><code id="lbart_+3A_sparse">sparse</code></td>
<td>
<p>Whether to perform variable selection based on a
sparse Dirichlet prior rather than simply uniform; see Linero 2016.</p>
</td></tr>
<tr><td><code id="lbart_+3A_a">a</code></td>
<td>
<p>Sparse parameter for <code class="reqn">Beta(a, b)</code> prior:
<code class="reqn">0.5&lt;=a&lt;=1</code> where lower values inducing more sparsity.</p>
</td></tr>
<tr><td><code id="lbart_+3A_b">b</code></td>
<td>
<p>Sparse parameter for <code class="reqn">Beta(a, b)</code> prior; typically,
<code class="reqn">b=1</code>.</p>
</td></tr>
<tr><td><code id="lbart_+3A_rho">rho</code></td>
<td>
<p>Sparse parameter: typically <code class="reqn">rho=p</code> where <code class="reqn">p</code> is the
number of covariates under consideration.</p>
</td></tr>
<tr><td><code id="lbart_+3A_augment">augment</code></td>
<td>
<p>Whether data augmentation is to be performed in sparse
variable selection.</p>
</td></tr>
<tr><td><code id="lbart_+3A_xinfo">xinfo</code></td>
<td>
<p> You can provide the cutpoints to BART or let BART
choose them for you.  To provide them, use the <code>xinfo</code>
argument to specify a list (matrix) where the items (rows) are the
covariates and the contents of the items (columns) are the
cutpoints.  </p>
</td></tr>
<tr><td><code id="lbart_+3A_usequants">usequants</code></td>
<td>
<p> If <code>usequants=FALSE</code>, then the
cutpoints in <code>xinfo</code> are generated uniformly; otherwise,
if <code>TRUE</code>, uniform quantiles are used for the cutpoints. </p>
</td></tr>
<tr><td><code id="lbart_+3A_cont">cont</code></td>
<td>
<p> Whether or not to assume all variables are continuous.</p>
</td></tr>
<tr><td><code id="lbart_+3A_rm.const">rm.const</code></td>
<td>
<p> Whether or not to remove constant variables.</p>
</td></tr>
<tr><td><code id="lbart_+3A_tau.interval">tau.interval</code></td>
<td>

<p>The width of the interval to scale the variance for the terminal
leaf values.</p>
</td></tr>
<tr><td><code id="lbart_+3A_k">k</code></td>
<td>

<p>For numeric y,
k is the number of prior standard deviations <code class="reqn">E(Y|x) = f(x)</code> is away from +/-.5.
For binary y,
k is the number of prior standard deviations <code class="reqn">f(x)</code> is away from +/-3.
In both cases, the bigger k is, the more conservative the fitting will be.
</p>
</td></tr>
<tr><td><code id="lbart_+3A_power">power</code></td>
<td>

<p>Power parameter for tree prior.
</p>
</td></tr>
<tr><td><code id="lbart_+3A_base">base</code></td>
<td>

<p>Base parameter for tree prior.
</p>
</td></tr>
<tr><td><code id="lbart_+3A_binaryoffset">binaryOffset</code></td>
<td>

<p>Used for binary <code class="reqn">y</code>.<br />
The model is <code class="reqn">P(Y=1 | x) = F(f(x) + binaryOffset)</code>.<br />


</p>
</td></tr>






<tr><td><code id="lbart_+3A_ntree">ntree</code></td>
<td>

<p>The number of trees in the sum.
</p>
</td></tr>
<tr><td><code id="lbart_+3A_numcut">numcut</code></td>
<td>

<p>The number of possible values of c (see usequants).
If a single number if given, this is used for all variables.
Otherwise a vector with length equal to ncol(x.train) is required,
where the <code class="reqn">i^{th}</code> element gives the number of c used for
the <code class="reqn">i^{th}</code> variable in x.train.
If usequants is false, numcut equally spaced cutoffs
are used covering the range of values in the corresponding
column of x.train.  If usequants is true, then  min(numcut, the number of unique values in the
corresponding columns of x.train - 1) c values are used.
</p>
</td></tr>
<tr><td><code id="lbart_+3A_ndpost">ndpost</code></td>
<td>

<p>The number of posterior draws returned.
</p>
</td></tr>
<tr><td><code id="lbart_+3A_nskip">nskip</code></td>
<td>

<p>Number of MCMC iterations to be treated as burn in.
</p>
</td></tr>
<tr><td><code id="lbart_+3A_nkeeptrain">nkeeptrain</code></td>
<td>

<p>Number of MCMC iterations to be returned for train data.
</p>
</td></tr>
<tr><td><code id="lbart_+3A_nkeeptest">nkeeptest</code></td>
<td>

<p>Number of MCMC iterations to be returned for test data.
</p>
</td></tr>



<tr><td><code id="lbart_+3A_nkeeptreedraws">nkeeptreedraws</code></td>
<td>

<p>Number of MCMC iterations to be returned for tree draws.
</p>
</td></tr>
<tr><td><code id="lbart_+3A_keepevery">keepevery</code></td>
<td>

<p>Every keepevery draw is kept to be returned to the user.
</p>
</td></tr>
<tr><td><code id="lbart_+3A_printevery">printevery</code></td>
<td>

<p>As the MCMC runs, a message is printed every printevery draws.
</p>
</td></tr>







<tr><td><code id="lbart_+3A_transposed">transposed</code></td>
<td>

<p>When running <code>lbart</code> in parallel, it is more memory-efficient
to transpose <code>x.train</code> and <code>x.test</code>, if any, prior to
calling <code>mc.lbart</code>.
</p>
</td></tr>





</table>


<h3>Details</h3>

<p>BART is an Bayesian MCMC method.
At each MCMC interation, we produce a draw from the joint posterior
<code class="reqn">f | (x,y)</code> in the numeric <code class="reqn">y</code> case
and just <code class="reqn">f</code> in the binary <code class="reqn">y</code> case.
</p>
<p>Thus, unlike a lot of other modelling methods in R, we do not produce a single model object
from which fits and summaries may be extracted.  The output consists of values
<code class="reqn">f^*(x)</code>

where * denotes a particular draw.
The <code class="reqn">x</code> is either a row from the training data (x.train) or the test data (x.test).
</p>


<h3>Value</h3>

<p><code>lbart</code> returns an object of type <code>lbart</code> which is
essentially a list. 
</p>
<table>
<tr><td><code>yhat.train</code></td>
<td>

<p>A matrix with ndpost rows and nrow(x.train) columns.
Each row corresponds to a draw <code class="reqn">f^*</code> from the posterior of <code class="reqn">f</code>
and each column corresponds to a row of x.train.
The <code class="reqn">(i,j)</code> value is <code class="reqn">f^*(x)</code> for the <code class="reqn">i^{th}</code> kept draw of <code class="reqn">f</code>
and the <code class="reqn">j^{th}</code> row of x.train.<br />
Burn-in is dropped.
</p>
</td></tr>
<tr><td><code>yhat.test</code></td>
<td>
<p>Same as yhat.train but now the x's are the rows of the test data.</p>
</td></tr>
<tr><td><code>yhat.train.mean</code></td>
<td>
<p>train data fits = mean of yhat.train columns.</p>
</td></tr>
<tr><td><code>yhat.test.mean</code></td>
<td>
<p>test data fits = mean of yhat.test columns.</p>
</td></tr>
<tr><td><code>varcount</code></td>
<td>
<p>a matrix with ndpost rows and nrow(x.train) columns.
Each row is for a draw. For each variable (corresponding to the columns),
the total count of the number of times
that variable is used in a tree decision rule (over all trees) is given.</p>
</td></tr>
</table>
<p>In addition, the list
has a <code>binaryOffset</code> giving the value used.
</p>
<p>Note that in the binary <code class="reqn">y</code>, case yhat.train and yhat.test are
<code class="reqn">f(x) + binaryOffset</code>.
If you want draws of the probability
<code class="reqn">P(Y=1 | x)</code> you need to apply the Logistic CDF (<code>plogis</code>)
to these values.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+wbart">wbart</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(ACTG175)

## exclude those who do not have CD4 count at 96 weeks
ex &lt;- is.na(ACTG175$cd496)
table(ex)

## inclusion criteria are CD4 counts between 200 and 500
ACTG175$cd40 &lt;- min(500, max(250, ACTG175$cd40))

## calculate relative CD4 decline
y &lt;- ((ACTG175$cd496-ACTG175$cd40)/ACTG175$cd40)[!ex]
summary(y)

## 0=failure, 1=success
y &lt;- 1*(y &gt; -0.5)

## summarize CD4 outcomes
table(y, ACTG175$arms[!ex])

table(y, ACTG175$arms[!ex])/
    matrix(table(ACTG175$arms[!ex]), nrow=2, ncol=4, byrow=TRUE)

## drop unneeded and unwanted variables
## 1: 'pidnum' patient ID number
##14: 'str2' which will be handled by strat1 below
##15: 'strat' which will be handled by strat1-strat3 below
##17: 'treat' handled by arm0-arm3 below
##18: 'offtrt' indicator of off-treatment before 96 weeks
##20: 'cd420' CD4 T cell count at 20 weeks
##21: 'cd496' CD4 T cell count at 96 weeks
##22: 'r' missing CD4 T cell count at 96 weeks
##24: 'cd820' CD8 T cell count at 20 weeks
##25: 'cens' indicator of observing the event in days
##26: 'days' number of days until the primary endpoint
##27: 'arms' handled by arm0-arm3 below
train &lt;- as.matrix(ACTG175)[!ex, -c(1, 14:15, 17, 18, 20:22, 24:27)]
train &lt;- cbind(1*(ACTG175$strat[!ex]==1), 1*(ACTG175$strat[!ex]==2),
               1*(ACTG175$strat[!ex]==3), train)
dimnames(train)[[2]][1:3] &lt;- paste0('strat', 1:3)
train &lt;- cbind(1*(ACTG175$arms[!ex]==0), 1*(ACTG175$arms[!ex]==1),
               1*(ACTG175$arms[!ex]==2), 1*(ACTG175$arms[!ex]==3), train)
dimnames(train)[[2]][1:4] &lt;- paste0('arm', 0:3)

N &lt;- nrow(train)

test0 &lt;- train; test0[ , 1:4] &lt;- 0; test0[ , 1] &lt;- 1
test1 &lt;- train; test1[ , 1:4] &lt;- 0; test1[ , 2] &lt;- 1
test2 &lt;- train; test2[ , 1:4] &lt;- 0; test2[ , 3] &lt;- 1
test3 &lt;- train; test3[ , 1:4] &lt;- 0; test3[ , 4] &lt;- 1

test &lt;- rbind(test0, test1, test2, test3)

##test BART with token run to ensure installation works
## set.seed(21)
## post &lt;- lbart(train, y, test, nskip=5, ndpost=5)

## Not run: 
set.seed(21)
post &lt;- lbart(train, y, test)

## turn z-scores into probabilities
post$prob.test &lt;- plogis(post$yhat.test)

## average over the posterior samples
post$prob.test.mean &lt;- apply(post$prob.test, 2, mean)

## place estimates for arms 0-3 next to each other for convenience
itr &lt;- cbind(post$prob.test.mean[(1:N)], post$prob.test.mean[N+(1:N)],
             post$prob.test.mean[2*N+(1:N)], post$prob.test.mean[3*N+(1:N)])

## find the BART ITR for each patient
itr.pick &lt;- integer(N)
for(i in 1:N) itr.pick[i] &lt;- which(itr[i, ]==max(itr[i, ]))-1

## arms 0 and 3 (monotherapy) are never chosen
table(itr.pick)

## do arms 1 and 2 show treatment heterogeneity?
diff. &lt;- apply(post$prob.test[ , 2*N+(1:N)]-post$prob.test[ , N+(1:N)], 2, mean)
plot(sort(diff.), type='h', main='ACTG175 trial: 50% CD4 decline from baseline at 96 weeks',
     xlab='Arm 2 (1) Preferable to the Right (Left)', ylab='Prob.Diff.: Arms 2 - 1')

library(rpart)
library(rpart.plot)

## make data frame for nicer names in the plot
var &lt;- as.data.frame(train[ , -(1:4)])

dss &lt;- rpart(diff. ~ var$age+var$gender+var$race+var$wtkg+var$cd40+var$cd80+
                   var$karnof+var$symptom+var$hemo+var$homo+var$drugs+var$z30+
                   var$zprior+var$oprior+var$strat1+var$strat2+var$strat3,
               method='anova', control=rpart.control(cp=0.1))
rpart.plot(dss, type=3, extra=101)

## if strat1==1 (antiretroviral naive), then arm 2 is better
## otherwise, arm 1
print(dss)

all0 &lt;- apply(post$prob.test[ , (1:N)], 1, mean)
all1 &lt;- apply(post$prob.test[ , N+(1:N)], 1, mean)
all2 &lt;- apply(post$prob.test[ , 2*N+(1:N)], 1, mean)
all3 &lt;- apply(post$prob.test[ , 3*N+(1:N)], 1, mean)

## BART ITR
BART.itr &lt;- apply(post$prob.test[ , c(N+which(itr.pick==1), 2*N+which(itr.pick==2))], 1, mean)

test &lt;- train
test[ , 1:4] &lt;- 0
test[test[ , 5]==0, 2] &lt;- 1
test[test[ , 5]==1, 3] &lt;- 1

## BART ITR simple
BART.itr.simp &lt;- pwbart(test, post$treedraws)
BART.itr.simp &lt;- apply(plogis(BART.itr.simp), 1, mean)

plot(density(BART.itr), xlab='Value', xlim=c(0.475, 0.775), lwd=2,
     main='ACTG175 trial: 50% CD4 decline from baseline at 96 weeks')
lines(density(BART.itr.simp), col='brown', lwd=2)
lines(density(all0), col='green', lwd=2)
lines(density(all1), col='red', lwd=2)
lines(density(all2), col='blue', lwd=2)
lines(density(all3), col='yellow', lwd=2)
legend('topleft', legend=c('All Arm 0 (ZDV only)', 'All Arm 1 (ZDV+DDI)',
                           'All Arm 2 (ZDV+DDC)', 'All Arm 3 (DDI only)',
                           'BART ITR simple', 'BART ITR'),
       col=c('green', 'red', 'blue', 'yellow', 'brown', 'black'), lty=1, lwd=2)


## End(Not run)
</code></pre>

<hr>
<h2 id='leukemia'>Bone marrow transplantation for leukemia and multi-state models</h2><span id='topic+leukemia'></span>

<h3>Description</h3>

<p>137 patients with acute myelocytic leukemia (AML) and acute
lymphoblastic leukemia (ALL) were given oral busulfan (Bu) 4 mg/kg on
each of 4 days and intravenous cyclophosphamide (Cy) 60 mg/kg on each
of 2 days (BuCy2) followed by allogeneic bone marrow transplantation
from an HLA-identical or one antigen disparate sibling.</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(leukemia)</code></pre>


<h3>Format</h3>

<p>A data frame with 137 subjects on the following 22 variables.
</p>

<dl>
<dt><code>G</code></dt><dd><p>Disease Group (1=ALL, 2=AML Low Risk in first
remission, 3=AML High Risk not in first remission)</p>
</dd>
<dt><code>TD</code></dt><dd><p>Time To Death Or On Study Time</p>
</dd>
<dt><code>TB</code></dt><dd><p>Disease Free Survival Time (Time To Relapse, Death Or End Of Study)</p>
</dd>
<dt><code>D</code></dt><dd><p>Death Indicator (0=Alive, 1=Dead)</p>
</dd>
<dt><code>R</code></dt><dd><p>Relapse Indicator (0=Disease Free, 1=Relapsed)</p>
</dd>
<dt><code>B</code></dt><dd><p>Disease Free Survival Indicator (0=Alive and Disease
Free, 1=Dead or Relapsed)</p>
</dd>
<dt><code>TA</code></dt><dd><p>Time To Acute Graft-Versus-Host Disease (GVHD)</p>
</dd>
<dt><code>A</code></dt><dd><p>Acute GVHD Indicator (0=Never Developed Acute GVHD, 1=Developed Acute GVHD)</p>
</dd>
<dt><code>TC</code></dt><dd><p>Time To Chronic Graft-Versus-Host Disease (GVHD)</p>
</dd>
<dt><code>C</code></dt><dd><p>Chronic GVHD Indicator (0=Never Developed Chronic GVHD, 1=Developed Chronic GVHD)</p>
</dd>
<dt><code>TP</code></dt><dd><p>Time of Platelets Returning to Normal Levels</p>
</dd>
<dt><code>P</code></dt><dd><p>Platelet Recovery Indicator (0=Platelets Never Returned
to Normal, 1=Platelets Returned To Normal)</p>
</dd>
<dt><code>X1</code></dt><dd><p>Patient Age In Years</p>
</dd>
<dt><code>X2</code></dt><dd><p>Donor Age In Years</p>
</dd>
<dt><code>X3</code></dt><dd><p>Patient Gender (0=female, 1=male)</p>
</dd>
<dt><code>X4</code></dt><dd><p>Donor Gender (0=female, 1=male)</p>
</dd>
<dt><code>X5</code></dt><dd><p>Patient Cytomegalovirus (CMV) Immune Status (0=CMV Negative, 1=CMV Positive)</p>
</dd>
<dt><code>X6</code></dt><dd><p>Donor Cytomegalovirus (CMV) Immune Status (0=CMV Negative, 1=CMV Positive)</p>
</dd>
<dt><code>X7</code></dt><dd><p>Waiting Time to Transplant In Days</p>
</dd>
<dt><code>X8</code></dt><dd><p>AML Patients with Elevated Risk By
French-American-British (FAB) Classification (0=Not AML/Elevated, 1=FAB M4 Or M5 with AML)</p>
</dd>
<dt><code>X9</code></dt><dd><p>Hospital (1=The Ohio State University in Columbus,
2=Alfred in Melbourne, 3=St. Vincent in Sydney, 4=Hahnemann University
in Philadelphia)</p>
</dd>
<dt><code>X10</code></dt><dd><p>Methotrexate Used as a Graft-Versus-Host Disease Prophylactic
(0=No, 1=Yes)</p>
</dd>
</dl>



<h3>Source</h3>

<p>Klein J. and Moeschberger M.L. (2003)
<em>Survival Analysis: Techniques for Censored and Truncated Data</em>,  
New York: Springer-Verlag, 2nd Ed., Section 1.3.

</p>


<h3>References</h3>

<p>Copelan E., Biggs J., Thompson J., Crilley P., Szer J., Klein, J., Kapoor N., Avalos, B., Cunningham I. and Atkinson, K. (1991)
&quot;Treatment for acute myelocytic leukemia with allogeneic bone marrow transplantation following preparation with BuCy2&quot;.
<em>Blood</em>, <b>78(3)</b>, pp.838-843.
</p>

<hr>
<h2 id='lung'>NCCTG Lung Cancer Data</h2><span id='topic+cancer'></span><span id='topic+lung'></span>

<h3>Description</h3>

<p>Survival in patients with advanced lung cancer from the North
Central Cancer Treatment Group.  Performance
scores rate how well the patient can perform usual daily activities.
</p>


<h3>Format</h3>


<table>
<tr>
 <td style="text-align: left;">
    inst:</td><td style="text-align: left;"> Institution code</td>
</tr>
<tr>
 <td style="text-align: left;">
    time:</td><td style="text-align: left;"> Survival time in days</td>
</tr>
<tr>
 <td style="text-align: left;">
    status:</td><td style="text-align: left;"> censoring status 1=censored, 2=dead</td>
</tr>
<tr>
 <td style="text-align: left;">
    age:</td><td style="text-align: left;"> Age in years</td>
</tr>
<tr>
 <td style="text-align: left;">
    sex:</td><td style="text-align: left;">  Male=1 Female=2</td>
</tr>
<tr>
 <td style="text-align: left;">
    ph.ecog:</td><td style="text-align: left;"> ECOG performance score (0=good 5=dead)</td>
</tr>
<tr>
 <td style="text-align: left;">
    ph.karno:</td><td style="text-align: left;"> Karnofsky performance score (bad=0-good=100) rated by physician</td>
</tr>
<tr>
 <td style="text-align: left;">
    pat.karno:</td><td style="text-align: left;"> Karnofsky performance score as rated by patient</td>
</tr>
<tr>
 <td style="text-align: left;">
    meal.cal:</td><td style="text-align: left;"> Calories consumed at meals</td>
</tr>
<tr>
 <td style="text-align: left;">
    wt.loss:</td><td style="text-align: left;"> Weight loss in last six months</td>
</tr>
<tr>
 <td style="text-align: left;">
  </td>
</tr>

</table>



<h3>Source</h3>

<p>Terry Therneau</p>


<h3>References</h3>

<p>Loprinzi CL. Laurie JA. Wieand HS. Krook JE. Novotny PJ.
Kugler JW. Bartel J. Law M. Bateman M. Klatt NE. et al.
Prospective evaluation of prognostic variables from patient-completed
questionnaires. North Central Cancer Treatment Group.
Journal of Clinical Oncology. 12(3):601-7, 1994. </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(lung)
</code></pre>

<hr>
<h2 id='mbart'>Multinomial BART for categorical outcomes with fewer categories</h2><span id='topic+mbart'></span><span id='topic+mc.mbart'></span>

<h3>Description</h3>

<p>BART is a Bayesian &ldquo;sum-of-trees&rdquo; model.<br />
For numeric response <code class="reqn">y</code>, we have
<code class="reqn">y = f(x) +\epsilon</code>,
where <code class="reqn">\epsilon \sim N(0, 1)</code>.<br />
For a multinomial response <code class="reqn">y</code>, <code class="reqn">P(Y=y | x) = F(f(x))</code>,
where <code class="reqn">F</code> denotes the standard Normal CDF (probit link) or the
standard Logistic CDF (logit link).
</p>
<p>In both cases, <code class="reqn">f</code> is the sum of many tree models.
The goal is to have very flexible inference for the uknown
function <code class="reqn">f</code>.
</p>
<p>In the spirit of &ldquo;ensemble models&rdquo;,
each tree is constrained by a prior to be a weak learner
so that it contributes a
small amount to the overall fit.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mbart(
      x.train, y.train,
      x.test=matrix(0,0,0), type='pbart',
      ntype=as.integer(
          factor(type,
                 levels=c('wbart', 'pbart', 'lbart'))),
      sparse=FALSE, theta=0, omega=1,
      a=0.5, b=1, augment=FALSE, rho=NULL,
      xinfo=matrix(0,0,0), usequants=FALSE,
      rm.const=TRUE,
      k=2, power=2, base=0.95,
      tau.num=c(NA, 3, 6)[ntype],
      offset=NULL, 
      ntree=c(200L, 50L, 50L)[ntype], numcut=100L,
      ndpost=1000L, nskip=100L,
      keepevery=c(1L, 10L, 10L)[ntype],
      printevery=100L, transposed=FALSE,
      hostname=FALSE,
      mc.cores = 2L, ## mc.bart only
      nice = 19L,    ## mc.bart only
      seed = 99L     ## mc.bart only
     )

mc.mbart(
         x.train, y.train,
         x.test=matrix(0,0,0), type='pbart',
         ntype=as.integer(
             factor(type,
                    levels=c('wbart', 'pbart', 'lbart'))),
         sparse=FALSE, theta=0, omega=1,
         a=0.5, b=1, augment=FALSE, rho=NULL,
         xinfo=matrix(0,0,0), usequants=FALSE,
         rm.const=TRUE,
         k=2, power=2, base=0.95,
         tau.num=c(NA, 3, 6)[ntype],
         offset=NULL, 
         ntree=c(200L, 50L, 50L)[ntype], numcut=100L,
         ndpost=1000L, nskip=100L,
         keepevery=c(1L, 10L, 10L)[ntype],
         printevery=100L, transposed=FALSE,
         hostname=FALSE,
         mc.cores = 2L, ## mc.bart only
         nice = 19L,    ## mc.bart only
         seed = 99L     ## mc.bart only
        )

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mbart_+3A_x.train">x.train</code></td>
<td>

<p>Explanatory variables for training (in sample) data.<br />
May be a matrix or a data frame,
with (as usual) rows corresponding to observations and columns to variables.<br />
If a variable is a factor in a data frame, it is replaced with dummies.
Note that q dummies are created if q&gt;2 and
one dummy is created if q=2, where q is the number of levels of the factor.
<code>mbart</code> will generate draws of <code class="reqn">f(x)</code> for each <code class="reqn">x</code>
which is a row of <code>x.train</code>.
</p>
</td></tr>
<tr><td><code id="mbart_+3A_y.train">y.train</code></td>
<td>

<p>Categorical dependent variable for training (in sample) data.<br />



</p>
</td></tr>
<tr><td><code id="mbart_+3A_x.test">x.test</code></td>
<td>

<p>Explanatory variables for test (out of sample) data.<br />
Should have same structure as <code>x.train</code>.<br />
<code>mbart</code> will generate draws of <code class="reqn">f(x)</code> for each <code class="reqn">x</code> which is a row of <code>x.test</code>.
</p>
</td></tr>
<tr><td><code id="mbart_+3A_type">type</code></td>
<td>
<p> You can use this argument to specify the type of fit.
<code>'pbart'</code> for probit BART or <code>'lbart'</code> for logit BART. </p>
</td></tr>
<tr><td><code id="mbart_+3A_ntype">ntype</code></td>
<td>
<p> The integer equivalent of <code>type</code> where
<code>'pbart'</code> is 2 and <code>'lbart'</code> is 3.</p>
</td></tr>
<tr><td><code id="mbart_+3A_sparse">sparse</code></td>
<td>
<p>Whether to perform variable selection based on a
sparse Dirichlet prior rather than simply uniform; see Linero
2016.</p>
</td></tr>
<tr><td><code id="mbart_+3A_theta">theta</code></td>
<td>
<p>Set <code class="reqn">theta</code> parameter; zero means random.</p>
</td></tr>
<tr><td><code id="mbart_+3A_omega">omega</code></td>
<td>
<p>Set <code class="reqn">omega</code> parameter; zero means random.</p>
</td></tr>
<tr><td><code id="mbart_+3A_a">a</code></td>
<td>
<p>Sparse parameter for <code class="reqn">Beta(a, b)</code> prior:
<code class="reqn">0.5&lt;=a&lt;=1</code> where lower values inducing more sparsity.</p>
</td></tr>
<tr><td><code id="mbart_+3A_b">b</code></td>
<td>
<p>Sparse parameter for <code class="reqn">Beta(a, b)</code> prior; typically,
<code class="reqn">b=1</code>.</p>
</td></tr>
<tr><td><code id="mbart_+3A_rho">rho</code></td>
<td>
<p>Sparse parameter: typically <code class="reqn">rho=p</code> where <code class="reqn">p</code> is the
number of covariates under consideration.</p>
</td></tr>
<tr><td><code id="mbart_+3A_augment">augment</code></td>
<td>
<p>Whether data augmentation is to be performed in sparse
variable selection.</p>
</td></tr>
<tr><td><code id="mbart_+3A_xinfo">xinfo</code></td>
<td>
<p> You can provide the cutpoints to BART or let BART
choose them for you.  To provide them, use the <code>xinfo</code>
argument to specify a list (matrix) where the items (rows) are the
covariates and the contents of the items (columns) are the
cutpoints.  </p>
</td></tr>
<tr><td><code id="mbart_+3A_usequants">usequants</code></td>
<td>
<p> If <code>usequants=FALSE</code>, then the
cutpoints in <code>xinfo</code> are generated uniformly; otherwise,
if <code>TRUE</code>, uniform quantiles are used for the cutpoints. </p>
</td></tr>
<tr><td><code id="mbart_+3A_rm.const">rm.const</code></td>
<td>
<p> Whether or not to remove constant variables.</p>
</td></tr>



<tr><td><code id="mbart_+3A_k">k</code></td>
<td>




<p>For categorical <code>y.train</code>,
k is the number of prior standard deviations <code class="reqn">f(x)</code> is away from +/-3.

</p>
</td></tr>
<tr><td><code id="mbart_+3A_power">power</code></td>
<td>

<p>Power parameter for tree prior.
</p>
</td></tr>
<tr><td><code id="mbart_+3A_base">base</code></td>
<td>

<p>Base parameter for tree prior.
</p>
</td></tr>
<tr><td><code id="mbart_+3A_tau.num">tau.num</code></td>
<td>
<p> The numerator in the <code>tau</code> definition, i.e.,
<code>tau=tau.num/(k*sqrt(ntree))</code>. </p>
</td></tr>
<tr><td><code id="mbart_+3A_offset">offset</code></td>
<td>
<p> With Multinomial
BART, the centering is <code class="reqn">P(yj=1 | x) = F(fj(x) + offset[j])</code> where
<code>offset</code> defaults to <code>F^{-1}(mean(y.train))</code>.  You can use
the <code>offset</code> parameter to over-ride these defaults.</p>
</td></tr>
<tr><td><code id="mbart_+3A_ntree">ntree</code></td>
<td>

<p>The number of trees in the sum.
</p>
</td></tr>
<tr><td><code id="mbart_+3A_numcut">numcut</code></td>
<td>

<p>The number of possible values of c (see usequants).
If a single number if given, this is used for all variables.
Otherwise a vector with length equal to ncol(x.train) is required,
where the <code class="reqn">i^{th}</code> element gives the number of c used for
the <code class="reqn">i^{th}</code> variable in x.train.
If usequants is false, numcut equally spaced cutoffs
are used covering the range of values in the corresponding
column of x.train.  If usequants is true, then  min(numcut, the number of unique values in the
corresponding columns of x.train - 1) c values are used.
</p>
</td></tr>
<tr><td><code id="mbart_+3A_ndpost">ndpost</code></td>
<td>

<p>The number of posterior draws returned.
</p>
</td></tr>
<tr><td><code id="mbart_+3A_nskip">nskip</code></td>
<td>

<p>Number of MCMC iterations to be treated as burn in.
</p>
</td></tr>
<tr><td><code id="mbart_+3A_keepevery">keepevery</code></td>
<td>

<p>Every keepevery draw is kept to be returned to the user.
</p>
</td></tr>
<tr><td><code id="mbart_+3A_printevery">printevery</code></td>
<td>

<p>As the MCMC runs, a message is printed every printevery draws.
</p>
</td></tr>







<tr><td><code id="mbart_+3A_transposed">transposed</code></td>
<td>

<p>When running <code>mbart</code> in parallel, it is more memory-efficient
to transpose <code>x.train</code> and <code>x.test</code>, if any, prior to
calling <code>mc.mbart</code>.
</p>
</td></tr>
<tr><td><code id="mbart_+3A_hostname">hostname</code></td>
<td>

<p>When running on a cluster occasionally it is useful
to track on which node each chain is running; to do so
set this argument to <code>TRUE</code>.
</p>
</td></tr>
<tr><td><code id="mbart_+3A_seed">seed</code></td>
<td>

<p>Setting the seed required for reproducible MCMC.
</p>
</td></tr>
<tr><td><code id="mbart_+3A_mc.cores">mc.cores</code></td>
<td>

<p>Number of cores to employ in parallel.
</p>
</td></tr>
<tr><td><code id="mbart_+3A_nice">nice</code></td>
<td>

<p>Set the job niceness.  The default
niceness is 19: niceness goes from 0 (highest) to 19 (lowest).
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>BART is an Bayesian MCMC method.
At each MCMC interation, we produce a draw from 


<code class="reqn">f</code> in the categorical <code class="reqn">y</code> case.
</p>
<p>Thus, unlike a lot of other modelling methods in R, we do not produce
a single model object from which fits and summaries may be extracted.
The output consists of values <code class="reqn">f^*(x)</code>

where * denotes a particular draw.
The <code class="reqn">x</code> is either a row from the training data (x.train).
</p>


<h3>Value</h3>

<p><code>mbart</code> returns an object of type <code>mbart</code> which is
essentially a list. 
</p>
<table>
<tr><td><code>yhat.train</code></td>
<td>
<p> A matrix with <code>ndpost</code> rows and
<code>nrow(x.train)*K</code> columns.  Each row corresponds to a draw
<code class="reqn">f^*</code> from the posterior of <code class="reqn">f</code> and each column
corresponds to an estimate for a row of <code>x.train</code>.  For the
<code>i</code>th row of <code>x.train</code>, we provide the corresponding
<code>(i-1)*K+j</code>th column of <code>yhat.train</code> where
<code>j=1,...,K</code> indexes the categories.<br />
Burn-in is dropped.  </p>
</td></tr>
<tr><td><code>yhat.train.mean</code></td>
<td>
<p>train data fits = mean of <code>yhat.train</code>
columns.</p>
</td></tr>
<tr><td><code>varcount</code></td>
<td>
<p>a matrix with <code>ndpost</code> rows and
<code>nrow(x.train)</code> columns.  Each row is for a draw. For each
variable (corresponding to the columns), the total count of the
number of times that variable is used in a tree decision rule (over
all trees) is given.</p>
</td></tr>
</table>
<p>In addition, the list
has a <code>offset</code> vector giving the value used.
</p>
<p>Note that in the multinomial <code class="reqn">y</code> case <code>yhat.train</code> is 
<code class="reqn">f(x) + offset[j]</code>.



</p>


<h3>See Also</h3>

<p><code><a href="#topic+gbart">gbart</a></code>, <code><a href="#topic+alligator">alligator</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
N=500
set.seed(12)
x1=runif(N)
x2=runif(N, max=1-x1)
x3=1-x1-x2
x.train=cbind(x1, x2, x3)
y.train=0
for(i in 1:N)
    y.train[i]=sum((1:3)*rmultinom(1, 1, x.train[i, ]))
table(y.train)/N

##test mbart with token run to ensure installation works
set.seed(99)
post = mbart(x.train, y.train, nskip=1, ndpost=1)

## Not run: 
set.seed(99)
post=mbart(x.train, y.train, x.train)
##mc.post=mbart(x.train, y.train, x.test, mc.cores=8, seed=99)

K=3
i=seq(1, N*K, K)-1
for(j in 1:K)
    print(cor(x.train[ , j], post$prob.test.mean[i+j])^2)


## End(Not run)
</code></pre>

<hr>
<h2 id='mbart2'>Multinomial BART for categorical outcomes with more categories</h2><span id='topic+mbart2'></span><span id='topic+mc.mbart2'></span>

<h3>Description</h3>

<p>BART is a Bayesian &ldquo;sum-of-trees&rdquo; model.<br />
For numeric response <code class="reqn">y</code>, we have
<code class="reqn">y = f(x) +\epsilon</code>,
where <code class="reqn">\epsilon \sim N(0, 1)</code>.<br />
For a multinomial response <code class="reqn">y</code>, <code class="reqn">P(Y=y | x) = F(f(x))</code>,
where <code class="reqn">F</code> denotes the standard Normal CDF (probit link) or the
standard Logistic CDF (logit link).
</p>
<p>In both cases, <code class="reqn">f</code> is the sum of many tree models.
The goal is to have very flexible inference for the uknown
function <code class="reqn">f</code>.
</p>
<p>In the spirit of &ldquo;ensemble models&rdquo;,
each tree is constrained by a prior to be a weak learner
so that it contributes a
small amount to the overall fit.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mbart2(
      x.train, y.train,
      x.test=matrix(0,0,0), type='lbart',
      ntype=as.integer(
          factor(type,
                 levels=c('wbart', 'pbart', 'lbart'))),
      sparse=FALSE, theta=0, omega=1,
      a=0.5, b=1, augment=FALSE, rho=NULL,
      xinfo=matrix(0,0,0), usequants=FALSE,
      rm.const=TRUE,
      k=2, power=2, base=0.95,
      tau.num=c(NA, 3, 6)[ntype],
      offset=NULL, 
      ntree=c(200L, 50L, 50L)[ntype], numcut=100L,
      ndpost=1000L, nskip=100L,
      keepevery=c(1L, 10L, 10L)[ntype],
      printevery=100L, transposed=FALSE,
      hostname=FALSE,
      mc.cores = 2L, ## mc.bart only
      nice = 19L,    ## mc.bart only
      seed = 99L     ## mc.bart only
     )

mc.mbart2(
         x.train, y.train,
         x.test=matrix(0,0,0), type='lbart',
         ntype=as.integer(
             factor(type,
                    levels=c('wbart', 'pbart', 'lbart'))),
         sparse=FALSE, theta=0, omega=1,
         a=0.5, b=1, augment=FALSE, rho=NULL,
         xinfo=matrix(0,0,0), usequants=FALSE,
         rm.const=TRUE,
         k=2, power=2, base=0.95,
         tau.num=c(NA, 3, 6)[ntype],
         offset=NULL, 
         ntree=c(200L, 50L, 50L)[ntype], numcut=100L,
         ndpost=1000L, nskip=100L,
         keepevery=c(1L, 10L, 10L)[ntype],
         printevery=100L, transposed=FALSE,
         hostname=FALSE,
         mc.cores = 2L, ## mc.bart only
         nice = 19L,    ## mc.bart only
         seed = 99L     ## mc.bart only
        )

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mbart2_+3A_x.train">x.train</code></td>
<td>

<p>Explanatory variables for training (in sample) data.<br />
May be a matrix or a data frame,
with (as usual) rows corresponding to observations and columns to variables.<br />
If a variable is a factor in a data frame, it is replaced with dummies.
Note that q dummies are created if q&gt;2 and
one dummy is created if q=2, where q is the number of levels of the factor.
<code>mbart2</code> will generate draws of <code class="reqn">f(x)</code> for each <code class="reqn">x</code>
which is a row of <code>x.train</code>.
</p>
</td></tr>
<tr><td><code id="mbart2_+3A_y.train">y.train</code></td>
<td>

<p>Categorical dependent variable for training (in sample) data.<br />



</p>
</td></tr>
<tr><td><code id="mbart2_+3A_x.test">x.test</code></td>
<td>

<p>Explanatory variables for test (out of sample) data.<br />
Should have same structure as <code>x.train</code>.<br />
<code>mbart2</code> will generate draws of <code class="reqn">f(x)</code> for each <code class="reqn">x</code> which is a row of <code>x.test</code>.
</p>
</td></tr>
<tr><td><code id="mbart2_+3A_type">type</code></td>
<td>
<p> You can use this argument to specify the type of fit.
<code>'pbart'</code> for probit BART or <code>'lbart'</code> for logit BART. </p>
</td></tr>
<tr><td><code id="mbart2_+3A_ntype">ntype</code></td>
<td>
<p> The integer equivalent of <code>type</code> where
<code>'pbart'</code> is 2 and <code>'lbart'</code> is 3.</p>
</td></tr>
<tr><td><code id="mbart2_+3A_sparse">sparse</code></td>
<td>
<p>Whether to perform variable selection based on a
sparse Dirichlet prior rather than simply uniform; see Linero
2016.</p>
</td></tr>
<tr><td><code id="mbart2_+3A_theta">theta</code></td>
<td>
<p>Set <code class="reqn">theta</code> parameter; zero means random.</p>
</td></tr>
<tr><td><code id="mbart2_+3A_omega">omega</code></td>
<td>
<p>Set <code class="reqn">omega</code> parameter; zero means random.</p>
</td></tr>
<tr><td><code id="mbart2_+3A_a">a</code></td>
<td>
<p>Sparse parameter for <code class="reqn">Beta(a, b)</code> prior:
<code class="reqn">0.5&lt;=a&lt;=1</code> where lower values inducing more sparsity.</p>
</td></tr>
<tr><td><code id="mbart2_+3A_b">b</code></td>
<td>
<p>Sparse parameter for <code class="reqn">Beta(a, b)</code> prior; typically,
<code class="reqn">b=1</code>.</p>
</td></tr>
<tr><td><code id="mbart2_+3A_rho">rho</code></td>
<td>
<p>Sparse parameter: typically <code class="reqn">rho=p</code> where <code class="reqn">p</code> is the
number of covariates under consideration.</p>
</td></tr>
<tr><td><code id="mbart2_+3A_augment">augment</code></td>
<td>
<p>Whether data augmentation is to be performed in sparse
variable selection.</p>
</td></tr>
<tr><td><code id="mbart2_+3A_xinfo">xinfo</code></td>
<td>
<p> You can provide the cutpoints to BART or let BART
choose them for you.  To provide them, use the <code>xinfo</code>
argument to specify a list (matrix) where the items (rows) are the
covariates and the contents of the items (columns) are the
cutpoints.  </p>
</td></tr>
<tr><td><code id="mbart2_+3A_usequants">usequants</code></td>
<td>
<p> If <code>usequants=FALSE</code>, then the
cutpoints in <code>xinfo</code> are generated uniformly; otherwise,
if <code>TRUE</code>, uniform quantiles are used for the cutpoints. </p>
</td></tr>
<tr><td><code id="mbart2_+3A_rm.const">rm.const</code></td>
<td>
<p> Whether or not to remove constant variables.</p>
</td></tr>



<tr><td><code id="mbart2_+3A_k">k</code></td>
<td>




<p>For categorical <code>y.train</code>,
k is the number of prior standard deviations <code class="reqn">f(x)</code> is away from +/-3.

</p>
</td></tr>
<tr><td><code id="mbart2_+3A_power">power</code></td>
<td>

<p>Power parameter for tree prior.
</p>
</td></tr>
<tr><td><code id="mbart2_+3A_base">base</code></td>
<td>

<p>Base parameter for tree prior.
</p>
</td></tr>
<tr><td><code id="mbart2_+3A_tau.num">tau.num</code></td>
<td>
<p> The numerator in the <code>tau</code> definition, i.e.,
<code>tau=tau.num/(k*sqrt(ntree))</code>. </p>
</td></tr>
<tr><td><code id="mbart2_+3A_offset">offset</code></td>
<td>
<p> With Multinomial
BART, the centering is <code class="reqn">P(yj=1 | x) = F(fj(x) + offset[j])</code> where
<code>offset</code> defaults to <code>F^{-1}(mean(y.train))</code>.  You can use
the <code>offset</code> parameter to over-ride these defaults.</p>
</td></tr>
<tr><td><code id="mbart2_+3A_ntree">ntree</code></td>
<td>

<p>The number of trees in the sum.
</p>
</td></tr>
<tr><td><code id="mbart2_+3A_numcut">numcut</code></td>
<td>

<p>The number of possible values of c (see usequants).
If a single number if given, this is used for all variables.
Otherwise a vector with length equal to ncol(x.train) is required,
where the <code class="reqn">i^{th}</code> element gives the number of c used for
the <code class="reqn">i^{th}</code> variable in x.train.
If usequants is false, numcut equally spaced cutoffs
are used covering the range of values in the corresponding
column of x.train.  If usequants is true, then  min(numcut, the number of unique values in the
corresponding columns of x.train - 1) c values are used.
</p>
</td></tr>
<tr><td><code id="mbart2_+3A_ndpost">ndpost</code></td>
<td>

<p>The number of posterior draws returned.
</p>
</td></tr>
<tr><td><code id="mbart2_+3A_nskip">nskip</code></td>
<td>

<p>Number of MCMC iterations to be treated as burn in.
</p>
</td></tr>
<tr><td><code id="mbart2_+3A_keepevery">keepevery</code></td>
<td>

<p>Every keepevery draw is kept to be returned to the user.
</p>
</td></tr>
<tr><td><code id="mbart2_+3A_printevery">printevery</code></td>
<td>

<p>As the MCMC runs, a message is printed every printevery draws.
</p>
</td></tr>







<tr><td><code id="mbart2_+3A_transposed">transposed</code></td>
<td>

<p>When running <code>mbart2</code> in parallel, it is more memory-efficient
to transpose <code>x.train</code> and <code>x.test</code>, if any, prior to
calling <code>mc.mbart2</code>.
</p>
</td></tr>
<tr><td><code id="mbart2_+3A_hostname">hostname</code></td>
<td>

<p>When running on a cluster occasionally it is useful
to track on which node each chain is running; to do so
set this argument to <code>TRUE</code>.
</p>
</td></tr>
<tr><td><code id="mbart2_+3A_seed">seed</code></td>
<td>

<p>Setting the seed required for reproducible MCMC.
</p>
</td></tr>
<tr><td><code id="mbart2_+3A_mc.cores">mc.cores</code></td>
<td>

<p>Number of cores to employ in parallel.
</p>
</td></tr>
<tr><td><code id="mbart2_+3A_nice">nice</code></td>
<td>

<p>Set the job niceness.  The default
niceness is 19: niceness goes from 0 (highest) to 19 (lowest).
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>BART is an Bayesian MCMC method.
At each MCMC interation, we produce a draw from 


<code class="reqn">f</code> in the categorical <code class="reqn">y</code> case.
</p>
<p>Thus, unlike a lot of other modelling methods in R, we do not produce
a single model object from which fits and summaries may be extracted.
The output consists of values <code class="reqn">f^*(x)</code>

where * denotes a particular draw.
The <code class="reqn">x</code> is either a row from the training data (x.train).
</p>


<h3>Value</h3>

<p><code>mbart2</code> returns an object of type <code>mbart2</code> which is
essentially a list. 
</p>
<table>
<tr><td><code>yhat.train</code></td>
<td>
<p> A matrix with <code>ndpost</code> rows and
<code>nrow(x.train)*K</code> columns.  Each row corresponds to a draw
<code class="reqn">f^*</code> from the posterior of <code class="reqn">f</code> and each column
corresponds to an estimate for a row of <code>x.train</code>.  For the
<code>i</code>th row of <code>x.train</code>, we provide the corresponding
<code>(i-1)*K+j</code>th column of <code>yhat.train</code> where
<code>j=1,...,K</code> indexes the categories.<br />
Burn-in is dropped.  </p>
</td></tr>
<tr><td><code>yhat.train.mean</code></td>
<td>
<p>train data fits = mean of <code>yhat.train</code>
columns.</p>
</td></tr>
<tr><td><code>varcount</code></td>
<td>
<p>a matrix with <code>ndpost</code> rows and
<code>nrow(x.train)</code> columns.  Each row is for a draw. For each
variable (corresponding to the columns), the total count of the
number of times that variable is used in a tree decision rule (over
all trees) is given.</p>
</td></tr>
</table>
<p>In addition, the list
has a <code>offset</code> vector giving the value used.
</p>
<p>Note that in the multinomial <code class="reqn">y</code> case <code>yhat.train</code> is 
<code class="reqn">f(x) + offset[j]</code>.



</p>


<h3>See Also</h3>

<p><code><a href="#topic+gbart">gbart</a></code>, <code><a href="#topic+alligator">alligator</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
N=500
set.seed(12)
x1=runif(N)
x2=runif(N, max=1-x1)
x3=1-x1-x2
x.train=cbind(x1, x2, x3)
y.train=0
for(i in 1:N)
    y.train[i]=sum((1:3)*rmultinom(1, 1, x.train[i, ]))
table(y.train)/N

##test mbart2 with token run to ensure installation works
set.seed(99)
post = mbart2(x.train, y.train, nskip=1, ndpost=1)

## Not run: 
set.seed(99)
post=mbart2(x.train, y.train, x.train)
##mc.post=mbart2(x.train, y.train, x.test, mc.cores=8, seed=99)

K=3
i=seq(1, N*K, K)-1
for(j in 1:K)
    print(cor(x.train[ , j], post$prob.test.mean[i+j])^2)


## End(Not run)
</code></pre>

<hr>
<h2 id='mc.cores.openmp'>Detecting OpenMP</h2><span id='topic+mc.cores.openmp'></span>

<h3>Description</h3>

<p> This package was designed for OpenMP.  For example, the
<code>pwbart</code> function can use OpenMP or the parallel R package for
multi-threading.  On UNIX/Unix-like systems, OpenMP, if available, is
discovered at install time; for the details, see the
<code>configure.ac</code> file which can be found in the source version of
this package.  However, we know of no GPL licensed code available to
detect OpenMP on Windows (for Artistic licensed OpenMP detection code
on Windows, see the Bioconductor R package rGADEM).  To determine
whether OpenMP is available at run time, we provide the function
documented here.  </p>


<h3>Usage</h3>

<pre><code class='language-R'>mc.cores.openmp()
</code></pre>


<h3>Value</h3>

<p>Returns a zero when OpenMP is not available, otherwise, an integer
greater than zero when OpenMP is available (returns one unless
you are running in a multi-threaded process).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pwbart">pwbart</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
mc.cores.openmp()

</code></pre>

<hr>
<h2 id='mc.crisk.pwbart'>Predicting new observations with a previously fitted BART model</h2><span id='topic+mc.crisk.pwbart'></span>

<h3>Description</h3>

<p>BART is a Bayesian &ldquo;sum-of-trees&rdquo; model.<br />
For a numeric response <code class="reqn">y</code>, we have
<code class="reqn">y = f(x) + \epsilon</code>,
where <code class="reqn">\epsilon \sim N(0,\sigma^2)</code>.<br />
</p>
<p><code class="reqn">f</code> is the sum of many tree models.
The goal is to have very flexible inference for the uknown
function <code class="reqn">f</code>.
</p>
<p>In the spirit of &ldquo;ensemble models&rdquo;,
each tree is constrained by a prior to be a weak learner
so that it contributes a
small amount to the overall fit.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mc.crisk.pwbart( x.test, x.test2,
                 treedraws, treedraws2,
                 binaryOffset=0, binaryOffset2=0,
                 mc.cores=2L, type='pbart',
                 transposed=FALSE, nice=19L
               )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mc.crisk.pwbart_+3A_x.test">x.test</code></td>
<td>

<p>Matrix of covariates to predict <code class="reqn">y</code> for cause 1.
</p>
</td></tr>
<tr><td><code id="mc.crisk.pwbart_+3A_x.test2">x.test2</code></td>
<td>

<p>Matrix of covariates to predict <code class="reqn">y</code> for cause 2.
</p>
</td></tr>
<tr><td><code id="mc.crisk.pwbart_+3A_treedraws">treedraws</code></td>
<td>

<p><code>$treedraws</code> for cause 1.
</p>
</td></tr>
<tr><td><code id="mc.crisk.pwbart_+3A_treedraws2">treedraws2</code></td>
<td>

<p><code>$treedraws</code> for cause 2.
</p>
</td></tr>
<tr><td><code id="mc.crisk.pwbart_+3A_binaryoffset">binaryOffset</code></td>
<td>

<p>Mean to add on to <code class="reqn">y</code> prediction for cause 1.
</p>
</td></tr>
<tr><td><code id="mc.crisk.pwbart_+3A_binaryoffset2">binaryOffset2</code></td>
<td>

<p>Mean to add on to <code class="reqn">y</code> prediction for cause 2.
</p>
</td></tr>
<tr><td><code id="mc.crisk.pwbart_+3A_mc.cores">mc.cores</code></td>
<td>

<p>Number of threads to utilize.
</p>
</td></tr>
<tr><td><code id="mc.crisk.pwbart_+3A_type">type</code></td>
<td>
<p> Whether to employ Albert-Chib, <code>'pbart'</code>, or
Holmes-Held, <code>'lbart'</code>. </p>
</td></tr>
<tr><td><code id="mc.crisk.pwbart_+3A_transposed">transposed</code></td>
<td>

<p>When running <code>pwbart</code> or <code>mc.pwbart</code> in parallel, it is more memory-efficient
to transpose <code>x.test</code> prior to calling the internal versions of these functions.
</p>
</td></tr>
<tr><td><code id="mc.crisk.pwbart_+3A_nice">nice</code></td>
<td>

<p>Set the job niceness.  The default
niceness is 19: niceness goes from 0 (highest) to 19 (lowest).
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>BART is an Bayesian MCMC method.
At each MCMC interation, we produce a draw from the joint posterior
<code class="reqn">(f,\sigma) | (x,y)</code> in the numeric <code class="reqn">y</code> case
and just <code class="reqn">f</code> in the binary <code class="reqn">y</code> case.
</p>
<p>Thus, unlike a lot of other modelling methods in R, we do not produce a single model object
from which fits and summaries may be extracted.  The output consists of values
<code class="reqn">f^*(x)</code> (and <code class="reqn">\sigma^*</code> in the numeric case) where * denotes a particular draw.
The <code class="reqn">x</code> is either a row from the training data (x.train) or the test data (x.test).
</p>


<h3>Value</h3>

<p>Returns an object of type <code>criskbart</code> which is essentially a list with components:
</p>
<table>
<tr><td><code>yhat.test</code></td>
<td>

<p>A matrix with ndpost rows and nrow(x.test) columns.
Each row corresponds to a draw <code class="reqn">f^*</code> from the posterior of <code class="reqn">f</code>
and each column corresponds to a row of x.train.
The <code class="reqn">(i,j)</code> value is <code class="reqn">f^*(x)</code> for the <code class="reqn">i^{th}</code> kept draw of <code class="reqn">f</code>
and the <code class="reqn">j^{th}</code> row of x.train.<br />
Burn-in is dropped.
</p>
</td></tr>
<tr><td><code>surv.test</code></td>
<td>
<p>test data fits for survival probability.</p>
</td></tr>
<tr><td><code>surv.test.mean</code></td>
<td>
<p>mean of <code>surv.test</code> over the posterior samples.</p>
</td></tr>
<tr><td><code>prob.test</code></td>
<td>
<p>The probability of suffering cause 1 which is
occasionally useful, e.g., in calculating the concordance.</p>
</td></tr>
<tr><td><code>prob.test2</code></td>
<td>
<p>The probability of suffering cause 2 which is
occasionally useful, e.g., in calculating the concordance.</p>
</td></tr>
<tr><td><code>cif.test</code></td>
<td>
<p>The cumulative incidence function of cause 1,
<code class="reqn">F_1(t, x)</code>, where x's are the rows of the test data.</p>
</td></tr>
<tr><td><code>cif.test2</code></td>
<td>
<p>The cumulative incidence function of cause 2,
<code class="reqn">F_2(t, x)</code>, where x's are the rows of the test data.</p>
</td></tr>

<tr><td><code>yhat.test.mean</code></td>
<td>
<p>test data fits = mean of yhat.test columns.</p>
</td></tr>
<tr><td><code>cif.test.mean</code></td>
<td>
<p>mean of <code>cif.test</code> columns for cause 1.</p>
</td></tr>
<tr><td><code>cif.test2.mean</code></td>
<td>
<p>mean of <code>cif.test2</code> columns for cause 2.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+pwbart">pwbart</a></code>, <code><a href="#topic+crisk.bart">crisk.bart</a></code>, <code><a href="#topic+mc.crisk.bart">mc.crisk.bart</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(transplant)

delta &lt;- (as.numeric(transplant$event)-1)
## recode so that delta=1 is cause of interest; delta=2 otherwise
delta[delta==1] &lt;- 4
delta[delta==2] &lt;- 1
delta[delta&gt;1] &lt;- 2
table(delta, transplant$event)

times &lt;- pmax(1, ceiling(transplant$futime/7)) ## weeks
##times &lt;- pmax(1, ceiling(transplant$futime/30.5)) ## months
table(times)

typeO &lt;- 1*(transplant$abo=='O')
typeA &lt;- 1*(transplant$abo=='A')
typeB &lt;- 1*(transplant$abo=='B')
typeAB &lt;- 1*(transplant$abo=='AB')
table(typeA, typeO)

x.train &lt;- cbind(typeO, typeA, typeB, typeAB)

x.test &lt;- cbind(1, 0, 0, 0)
dimnames(x.test)[[2]] &lt;- dimnames(x.train)[[2]]

## parallel::mcparallel/mccollect do not exist on windows
if(.Platform$OS.type=='unix') {
##test BART with token run to ensure installation works
        post &lt;- mc.crisk.bart(x.train=x.train, times=times, delta=delta,
                               seed=99, mc.cores=2, nskip=5, ndpost=5,
                               keepevery=1)

        pre &lt;- surv.pre.bart(x.train=x.train, x.test=x.test,
                             times=times, delta=delta)

        K &lt;- post$K

        pred &lt;- mc.crisk.pwbart(pre$tx.test, pre$tx.test,
                                post$treedraws, post$treedraws2,
                                post$binaryOffset, post$binaryOffset2)
}

## Not run: 

## run one long MCMC chain in one process
## set.seed(99)
## post &lt;- crisk.bart(x.train=x.train, times=times, delta=delta, x.test=x.test)

## in the interest of time, consider speeding it up by parallel processing
## run "mc.cores" number of shorter MCMC chains in parallel processes
post &lt;- mc.crisk.bart(x.train=x.train,
                       times=times, delta=delta,
                       x.test=x.test, seed=99, mc.cores=8)

check &lt;- mc.crisk.pwbart(post$tx.test, post$tx.test,
                          post$treedraws, post$treedraws2,
                          post$binaryOffset,
                          post$binaryOffset2, mc.cores=8)
## check &lt;- predict(post, newdata=post$tx.test, newdata2=post$tx.test2,
##                  mc.cores=8)

print(c(post$surv.test.mean[1], check$surv.test.mean[1],
        post$surv.test.mean[1]-check$surv.test.mean[1]), digits=22)

print(all(round(post$surv.test.mean, digits=9)==
    round(check$surv.test.mean, digits=9)))

print(c(post$cif.test.mean[1], check$cif.test.mean[1],
        post$cif.test.mean[1]-check$cif.test.mean[1]), digits=22)

print(all(round(post$cif.test.mean, digits=9)==
    round(check$cif.test.mean, digits=9)))

print(c(post$cif.test2.mean[1], check$cif.test2.mean[1],
        post$cif.test2.mean[1]-check$cif.test2.mean[1]), digits=22)

print(all(round(post$cif.test2.mean, digits=9)==
    round(check$cif.test2.mean, digits=9)))


## End(Not run)
</code></pre>

<hr>
<h2 id='mc.crisk2.pwbart'>Predicting new observations with a previously fitted BART model</h2><span id='topic+mc.crisk2.pwbart'></span>

<h3>Description</h3>

<p>BART is a Bayesian &ldquo;sum-of-trees&rdquo; model.<br />
For a numeric response <code class="reqn">y</code>, we have
<code class="reqn">y = f(x) + \epsilon</code>,
where <code class="reqn">\epsilon \sim N(0,\sigma^2)</code>.<br />
</p>
<p><code class="reqn">f</code> is the sum of many tree models.
The goal is to have very flexible inference for the uknown
function <code class="reqn">f</code>.
</p>
<p>In the spirit of &ldquo;ensemble models&rdquo;,
each tree is constrained by a prior to be a weak learner
so that it contributes a
small amount to the overall fit.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mc.crisk2.pwbart( x.test, x.test2,
                 treedraws, treedraws2,
                 binaryOffset=0, binaryOffset2=0,
                 mc.cores=2L, type='pbart',
                 transposed=FALSE, nice=19L
               )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mc.crisk2.pwbart_+3A_x.test">x.test</code></td>
<td>

<p>Matrix of covariates to predict <code class="reqn">y</code> for cause 1.
</p>
</td></tr>
<tr><td><code id="mc.crisk2.pwbart_+3A_x.test2">x.test2</code></td>
<td>

<p>Matrix of covariates to predict <code class="reqn">y</code> for cause 2.
</p>
</td></tr>
<tr><td><code id="mc.crisk2.pwbart_+3A_treedraws">treedraws</code></td>
<td>

<p><code>$treedraws</code> for cause 1.
</p>
</td></tr>
<tr><td><code id="mc.crisk2.pwbart_+3A_treedraws2">treedraws2</code></td>
<td>

<p><code>$treedraws</code> for cause 2.
</p>
</td></tr>
<tr><td><code id="mc.crisk2.pwbart_+3A_binaryoffset">binaryOffset</code></td>
<td>

<p>Mean to add on to <code class="reqn">y</code> prediction for cause 1.
</p>
</td></tr>
<tr><td><code id="mc.crisk2.pwbart_+3A_binaryoffset2">binaryOffset2</code></td>
<td>

<p>Mean to add on to <code class="reqn">y</code> prediction for cause 2.
</p>
</td></tr>
<tr><td><code id="mc.crisk2.pwbart_+3A_mc.cores">mc.cores</code></td>
<td>

<p>Number of threads to utilize.
</p>
</td></tr>
<tr><td><code id="mc.crisk2.pwbart_+3A_type">type</code></td>
<td>
<p> Whether to employ Albert-Chib, <code>'pbart'</code>, or
Holmes-Held, <code>'lbart'</code>. </p>
</td></tr>
<tr><td><code id="mc.crisk2.pwbart_+3A_transposed">transposed</code></td>
<td>

<p>When running <code>pwbart</code> or <code>mc.pwbart</code> in parallel, it is more memory-efficient
to transpose <code>x.test</code> prior to calling the internal versions of these functions.
</p>
</td></tr>
<tr><td><code id="mc.crisk2.pwbart_+3A_nice">nice</code></td>
<td>

<p>Set the job niceness.  The default
niceness is 19: niceness goes from 0 (highest) to 19 (lowest).
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>BART is an Bayesian MCMC method.
At each MCMC interation, we produce a draw from the joint posterior
<code class="reqn">(f,\sigma) | (x,y)</code> in the numeric <code class="reqn">y</code> case
and just <code class="reqn">f</code> in the binary <code class="reqn">y</code> case.
</p>
<p>Thus, unlike a lot of other modelling methods in R, we do not produce a single model object
from which fits and summaries may be extracted.  The output consists of values
<code class="reqn">f^*(x)</code> (and <code class="reqn">\sigma^*</code> in the numeric case) where * denotes a particular draw.
The <code class="reqn">x</code> is either a row from the training data (x.train) or the test data (x.test).
</p>


<h3>Value</h3>

<p>Returns an object of type <code>crisk2bart</code> which is essentially a list with components:
</p>
<table>
<tr><td><code>yhat.test</code></td>
<td>

<p>A matrix with ndpost rows and nrow(x.test) columns.
Each row corresponds to a draw <code class="reqn">f^*</code> from the posterior of <code class="reqn">f</code>
and each column corresponds to a row of x.train.
The <code class="reqn">(i,j)</code> value is <code class="reqn">f^*(x)</code> for the <code class="reqn">i^{th}</code> kept draw of <code class="reqn">f</code>
and the <code class="reqn">j^{th}</code> row of x.train.<br />
Burn-in is dropped.
</p>
</td></tr>
<tr><td><code>surv.test</code></td>
<td>
<p>test data fits for survival probability.</p>
</td></tr>
<tr><td><code>surv.test.mean</code></td>
<td>
<p>mean of <code>surv.test</code> over the posterior samples.</p>
</td></tr>
<tr><td><code>prob.test</code></td>
<td>
<p>The probability of suffering cause 1 which is
occasionally useful, e.g., in calculating the concordance.</p>
</td></tr>
<tr><td><code>prob.test2</code></td>
<td>
<p>The probability of suffering cause 2 which is
occasionally useful, e.g., in calculating the concordance.</p>
</td></tr>
<tr><td><code>cif.test</code></td>
<td>
<p>The cumulative incidence function of cause 1,
<code class="reqn">F_1(t, x)</code>, where x's are the rows of the test data.</p>
</td></tr>
<tr><td><code>cif.test2</code></td>
<td>
<p>The cumulative incidence function of cause 2,
<code class="reqn">F_2(t, x)</code>, where x's are the rows of the test data.</p>
</td></tr>

<tr><td><code>yhat.test.mean</code></td>
<td>
<p>test data fits = mean of yhat.test columns.</p>
</td></tr>
<tr><td><code>cif.test.mean</code></td>
<td>
<p>mean of <code>cif.test</code> columns for cause 1.</p>
</td></tr>
<tr><td><code>cif.test2.mean</code></td>
<td>
<p>mean of <code>cif.test2</code> columns for cause 2.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+pwbart">pwbart</a></code>, <code><a href="#topic+crisk2.bart">crisk2.bart</a></code>, <code><a href="#topic+mc.crisk2.bart">mc.crisk2.bart</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(transplant)

delta &lt;- (as.numeric(transplant$event)-1)
## recode so that delta=1 is cause of interest; delta=2 otherwise
delta[delta==1] &lt;- 4
delta[delta==2] &lt;- 1
delta[delta&gt;1] &lt;- 2
table(delta, transplant$event)

times &lt;- pmax(1, ceiling(transplant$futime/7)) ## weeks
##times &lt;- pmax(1, ceiling(transplant$futime/30.5)) ## months
table(times)

typeO &lt;- 1*(transplant$abo=='O')
typeA &lt;- 1*(transplant$abo=='A')
typeB &lt;- 1*(transplant$abo=='B')
typeAB &lt;- 1*(transplant$abo=='AB')
table(typeA, typeO)

x.train &lt;- cbind(typeO, typeA, typeB, typeAB)

x.test &lt;- cbind(1, 0, 0, 0)
dimnames(x.test)[[2]] &lt;- dimnames(x.train)[[2]]

## parallel::mcparallel/mccollect do not exist on windows
if(.Platform$OS.type=='unix') {
##test BART with token run to ensure installation works
        post &lt;- mc.crisk2.bart(x.train=x.train, times=times, delta=delta,
                               seed=99, mc.cores=2, nskip=5, ndpost=5,
                               keepevery=1)

        pre &lt;- surv.pre.bart(x.train=x.train, x.test=x.test,
                             times=times, delta=delta)

        K &lt;- post$K

        pred &lt;- mc.crisk2.pwbart(pre$tx.test, pre$tx.test,
                                post$treedraws, post$treedraws2,
                                post$binaryOffset, post$binaryOffset2)
}

## Not run: 

## run one long MCMC chain in one process
## set.seed(99)
## post &lt;- crisk2.bart(x.train=x.train, times=times, delta=delta, x.test=x.test)

## in the interest of time, consider speeding it up by parallel processing
## run "mc.cores" number of shorter MCMC chains in parallel processes
post &lt;- mc.crisk2.bart(x.train=x.train,
                       times=times, delta=delta,
                       x.test=x.test, seed=99, mc.cores=8)

check &lt;- mc.crisk2.pwbart(post$tx.test, post$tx.test,
                          post$treedraws, post$treedraws2,
                          post$binaryOffset,
                          post$binaryOffset2, mc.cores=8)
## check &lt;- predict(post, newdata=post$tx.test, newdata2=post$tx.test2,
##                  mc.cores=8)

print(c(post$surv.test.mean[1], check$surv.test.mean[1],
        post$surv.test.mean[1]-check$surv.test.mean[1]), digits=22)

print(all(round(post$surv.test.mean, digits=9)==
    round(check$surv.test.mean, digits=9)))

print(c(post$cif.test.mean[1], check$cif.test.mean[1],
        post$cif.test.mean[1]-check$cif.test.mean[1]), digits=22)

print(all(round(post$cif.test.mean, digits=9)==
    round(check$cif.test.mean, digits=9)))

print(c(post$cif.test2.mean[1], check$cif.test2.mean[1],
        post$cif.test2.mean[1]-check$cif.test2.mean[1]), digits=22)

print(all(round(post$cif.test2.mean, digits=9)==
    round(check$cif.test2.mean, digits=9)))



## End(Not run)
</code></pre>

<hr>
<h2 id='mc.lbart'>Logit BART for dichotomous outcomes with Logistic latents and parallel computation</h2><span id='topic+mc.lbart'></span>

<h3>Description</h3>

<p>BART is a Bayesian &ldquo;sum-of-trees&rdquo; model.<br />
For numeric response <code class="reqn">y</code>, we have
<code class="reqn">y = f(x) + \epsilon</code>,
where <code class="reqn">\epsilon \sim Log(0, 1)</code>.<br />
For a binary response <code class="reqn">y</code>, <code class="reqn">P(Y=1 | x) = F(f(x))</code>, where <code class="reqn">F</code>
denotes the standard Logistic CDF (logit link).
</p>
<p>In both cases, <code class="reqn">f</code> is the sum of many tree models.
The goal is to have very flexible inference for the uknown
function <code class="reqn">f</code>.
</p>
<p>In the spirit of &ldquo;ensemble models&rdquo;,
each tree is constrained by a prior to be a weak learner
so that it contributes a
small amount to the overall fit.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mc.lbart(
   x.train, y.train, x.test=matrix(0.0,0,0),
   sparse=FALSE, a=0.5, b=1, augment=FALSE, rho=NULL,
   xinfo=matrix(0.0,0,0), usequants=FALSE,
   cont=FALSE, rm.const=TRUE, tau.interval=0.95,
   k=2.0, power=2.0, base=.95,
   binaryOffset=NULL,
   ntree=50L, numcut=100L,
   ndpost=1000L, nskip=100L,
   keepevery=1L, printevery=100,
   keeptrainfits=TRUE, transposed=FALSE,
   
   mc.cores = 2L, nice = 19L,
   seed = 99L
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mc.lbart_+3A_x.train">x.train</code></td>
<td>

<p>Explanatory variables for training (in sample) data.<br />
May be a matrix or a data frame,
with (as usual) rows corresponding to observations and columns to variables.<br />
If a variable is a factor in a data frame, it is replaced with dummies.
Note that q dummies are created if q&gt;2 and
one dummy is created if q=2, where q is the number of levels of the factor.
<code>lbart</code> will generate draws of <code class="reqn">f(x)</code> for each <code class="reqn">x</code>
which is a row of x.train.
</p>
</td></tr>
<tr><td><code id="mc.lbart_+3A_y.train">y.train</code></td>
<td>

<p>Dependent variable for training (in sample) data.<br />
If y is numeric a continous response model is fit (normal errors).<br />
If y is a factor (or just has values 0 and 1) then a binary response model
with a logit link is fit.
</p>
</td></tr>
<tr><td><code id="mc.lbart_+3A_x.test">x.test</code></td>
<td>

<p>Explanatory variables for test (out of sample) data.<br />
Should have same structure as x.train.<br />
<code>lbart</code> will generate draws of <code class="reqn">f(x)</code> for each <code class="reqn">x</code> which is a row of x.test.
</p>
</td></tr>
<tr><td><code id="mc.lbart_+3A_sparse">sparse</code></td>
<td>
<p>Whether to perform variable selection based on a
sparse Dirichlet prior rather than simply uniform; see Linero 2016.</p>
</td></tr>
<tr><td><code id="mc.lbart_+3A_a">a</code></td>
<td>
<p>Sparse parameter for <code class="reqn">Beta(a, b)</code> prior:
<code class="reqn">0.5&lt;=a&lt;=1</code> where lower values inducing more sparsity.</p>
</td></tr>
<tr><td><code id="mc.lbart_+3A_b">b</code></td>
<td>
<p>Sparse parameter for <code class="reqn">Beta(a, b)</code> prior; typically,
<code class="reqn">b=1</code>.</p>
</td></tr>
<tr><td><code id="mc.lbart_+3A_rho">rho</code></td>
<td>
<p>Sparse parameter: typically <code class="reqn">rho=p</code> where <code class="reqn">p</code> is the
number of covariates under consideration.</p>
</td></tr>
<tr><td><code id="mc.lbart_+3A_augment">augment</code></td>
<td>
<p>Whether data augmentation is to be performed in sparse
variable selection.</p>
</td></tr>
<tr><td><code id="mc.lbart_+3A_xinfo">xinfo</code></td>
<td>
<p> You can provide the cutpoints to BART or let BART
choose them for you.  To provide them, use the <code>xinfo</code>
argument to specify a list (matrix) where the items (rows) are the
covariates and the contents of the items (columns) are the
cutpoints.  </p>
</td></tr>
<tr><td><code id="mc.lbart_+3A_usequants">usequants</code></td>
<td>
<p> If <code>usequants=FALSE</code>, then the
cutpoints in <code>xinfo</code> are generated uniformly; otherwise,
if <code>TRUE</code>, uniform quantiles are used for the cutpoints. </p>
</td></tr>
<tr><td><code id="mc.lbart_+3A_cont">cont</code></td>
<td>
<p> Whether or not to assume all variables are continuous.</p>
</td></tr>
<tr><td><code id="mc.lbart_+3A_rm.const">rm.const</code></td>
<td>
<p> Whether or not to remove constant variables.</p>
</td></tr>  
<tr><td><code id="mc.lbart_+3A_tau.interval">tau.interval</code></td>
<td>

<p>The width of the interval to scale the variance for the terminal
leaf values.</p>
</td></tr>
<tr><td><code id="mc.lbart_+3A_k">k</code></td>
<td>

<p>For numeric y,
k is the number of prior standard deviations <code class="reqn">E(Y|x) = f(x)</code> is away from +/-.5.
For binary y,
k is the number of prior standard deviations <code class="reqn">f(x)</code> is away from +/-3.
In both cases, the bigger k is, the more conservative the fitting will be.
</p>
</td></tr>
<tr><td><code id="mc.lbart_+3A_power">power</code></td>
<td>

<p>Power parameter for tree prior.
</p>
</td></tr>
<tr><td><code id="mc.lbart_+3A_base">base</code></td>
<td>

<p>Base parameter for tree prior.
</p>
</td></tr>
<tr><td><code id="mc.lbart_+3A_binaryoffset">binaryOffset</code></td>
<td>

<p>Used for binary <code class="reqn">y</code>.<br />
The model is <code class="reqn">P(Y=1 | x) = F(f(x) + binaryOffset)</code>.<br />


</p>
</td></tr>
<tr><td><code id="mc.lbart_+3A_ntree">ntree</code></td>
<td>

<p>The number of trees in the sum.
</p>
</td></tr>
<tr><td><code id="mc.lbart_+3A_numcut">numcut</code></td>
<td>

<p>The number of possible values of c (see usequants).
If a single number if given, this is used for all variables.
Otherwise a vector with length equal to ncol(x.train) is required,
where the <code class="reqn">i^{th}</code> element gives the number of c used for
the <code class="reqn">i^{th}</code> variable in x.train.
If usequants is false, numcut equally spaced cutoffs
are used covering the range of values in the corresponding
column of x.train.  If usequants is true, then  min(numcut, the number of unique values in the
corresponding columns of x.train - 1) c values are used.
</p>
</td></tr>
<tr><td><code id="mc.lbart_+3A_ndpost">ndpost</code></td>
<td>

<p>The number of posterior draws returned.
</p>
</td></tr>
<tr><td><code id="mc.lbart_+3A_nskip">nskip</code></td>
<td>

<p>Number of MCMC iterations to be treated as burn in.
</p>
</td></tr>
<tr><td><code id="mc.lbart_+3A_keepevery">keepevery</code></td>
<td>

<p>Every keepevery draw is kept to be returned to the user.
</p>
</td></tr>
<tr><td><code id="mc.lbart_+3A_printevery">printevery</code></td>
<td>

<p>As the MCMC runs, a message is printed every printevery draws.
</p>
</td></tr>
<tr><td><code id="mc.lbart_+3A_keeptrainfits">keeptrainfits</code></td>
<td>

<p>Whether to keep <code>yhat.train</code> or not.
</p>
</td></tr>
<tr><td><code id="mc.lbart_+3A_transposed">transposed</code></td>
<td>

<p>When running <code>lbart</code> in parallel, it is more memory-efficient
to transpose <code>x.train</code> and <code>x.test</code>, if any, prior to
calling <code>mc.lbart</code>.
</p>
</td></tr>





<tr><td><code id="mc.lbart_+3A_seed">seed</code></td>
<td>

<p>Setting the seed required for reproducible MCMC.
</p>
</td></tr>
<tr><td><code id="mc.lbart_+3A_mc.cores">mc.cores</code></td>
<td>

<p>Number of cores to employ in parallel.
</p>
</td></tr>
<tr><td><code id="mc.lbart_+3A_nice">nice</code></td>
<td>

<p>Set the job niceness.  The default
niceness is 19: niceness goes from 0 (highest) to 19 (lowest).
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>BART is an Bayesian MCMC method.
At each MCMC interation, we produce a draw from the joint posterior
<code class="reqn">(f,\sigma) | (x,y)</code> in the numeric <code class="reqn">y</code> case
and just <code class="reqn">f</code> in the binary <code class="reqn">y</code> case.
</p>
<p>Thus, unlike a lot of other modelling methods in R, we do not produce a single model object
from which fits and summaries may be extracted.  The output consists of values
<code class="reqn">f^*(x)</code> (and <code class="reqn">\sigma^*</code> in the numeric case) where * denotes a particular draw.
The <code class="reqn">x</code> is either a row from the training data (x.train) or the test data (x.test).
</p>


<h3>Value</h3>

<p><code>mc.lbart</code> returns an object of type <code>lbart</code> which is
essentially a list. 
</p>
<table>
<tr><td><code>yhat.train</code></td>
<td>

<p>A matrix with ndpost rows and nrow(x.train) columns.
Each row corresponds to a draw <code class="reqn">f^*</code> from the posterior of <code class="reqn">f</code>
and each column corresponds to a row of x.train.
The <code class="reqn">(i,j)</code> value is <code class="reqn">f^*(x)</code> for the <code class="reqn">i^{th}</code> kept draw of <code class="reqn">f</code>
and the <code class="reqn">j^{th}</code> row of x.train.<br />
Burn-in is dropped.
</p>
</td></tr>
<tr><td><code>yhat.test</code></td>
<td>
<p>Same as yhat.train but now the x's are the rows of the test data.</p>
</td></tr>
<tr><td><code>yhat.train.mean</code></td>
<td>
<p>train data fits = mean of yhat.train columns.</p>
</td></tr>
<tr><td><code>yhat.test.mean</code></td>
<td>
<p>test data fits = mean of yhat.test columns.</p>
</td></tr>
<tr><td><code>varcount</code></td>
<td>
<p>a matrix with ndpost rows and nrow(x.train) columns.
Each row is for a draw. For each variable (corresponding to the columns),
the total count of the number of times
that variable is used in a tree decision rule (over all trees) is given.</p>
</td></tr>
</table>
<p>In addition, the list
has a <code>binaryOffset</code> giving the value used.
</p>
<p>Note that in the binary <code class="reqn">y</code>, case yhat.train and yhat.test are
<code class="reqn">f(x) + binaryOffset</code>.  If you want draws of the probability
<code class="reqn">P(Y=1 | x)</code> you need to apply the Logistic cdf (<code>plogis</code>)
to these values.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+lbart">lbart</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(99)
n=5000
x = sort(-2+4*runif(n))
X=matrix(x,ncol=1)
f = function(x) {return((1/2)*x^3)}
FL = function(x) {return(exp(x)/(1+exp(x)))}
pv = FL(f(x))
y = rbinom(n,1,pv)
np=100
xp=-2+4*(1:np)/np
Xp=matrix(xp,ncol=1)

## parallel::mcparallel/mccollect do not exist on windows
## if(.Platform$OS.type=='unix') {
## ##test BART with token run to ensure installation works
##     mf = mc.lbart(X, y, nskip=5, ndpost=5, mc.cores=1, seed=99)
## }

## Not run: 
set.seed(99)
pf = lbart(X,y,Xp)

plot(f(Xp), pf$yhat.test.mean, xlim=c(-4, 4), ylim=c(-4, 4),
     xlab='True f(x)', ylab='BART f(x)')
lines(c(-4, 4), c(-4, 4))

mf = mc.lbart(X,y,Xp, mc.cores=4, seed=99)

plot(f(Xp), mf$yhat.test.mean, xlim=c(-4, 4), ylim=c(-4, 4),
     xlab='True f(x)', ylab='BART f(x)')
lines(c(-4, 4), c(-4, 4))

par(mfrow=c(2,2))

plot(range(xp),range(pf$yhat.test),xlab='x',ylab='f(x)',type='n')
lines(x,f(x),col='blue',lwd=2)
lines(xp,apply(pf$yhat.test,2,mean),col='red')
qpl = apply(pf$yhat.test,2,quantile,probs=c(.025,.975))
lines(xp,qpl[1,],col='green',lty=1)
lines(xp,qpl[2,],col='green',lty=1)
title(main='BART::lbart f(x) with 0.95 intervals')

plot(range(xp),range(mf$yhat.test),xlab='x',ylab='f(x)',type='n')
lines(x,f(x),col='blue',lwd=2)
lines(xp,apply(mf$yhat.test,2,mean),col='red')
qpl = apply(mf$yhat.test,2,quantile,probs=c(.025,.975))
lines(xp,qpl[1,],col='green',lty=1)
lines(xp,qpl[2,],col='green',lty=1)
title(main='BART::mc.lbart f(x) with 0.95 intervals')

plot(pf$yhat.test.mean,apply(mf$yhat.test,2,mean),xlab='BART::lbart',ylab='BART::mc.lbart')
abline(0,1,col='red')
title(main="BART::lbart f(x) vs. BART::mc.lbart f(x)")

## End(Not run)
</code></pre>

<hr>
<h2 id='mc.pbart'>Probit BART for dichotomous outcomes with Normal latents and parallel computation</h2><span id='topic+mc.pbart'></span>

<h3>Description</h3>

<p>BART is a Bayesian &ldquo;sum-of-trees&rdquo; model.<br />



For a binary response <code class="reqn">y</code>, <code class="reqn">P(Y=1 | x) = F(f(x))</code>, where <code class="reqn">F</code>
denotes the standard normal cdf (probit link).
</p>
<p>In both cases, <code class="reqn">f</code> is the sum of many tree models.
The goal is to have very flexible inference for the uknown
function <code class="reqn">f</code>.
</p>
<p>In the spirit of &ldquo;ensemble models&rdquo;,
each tree is constrained by a prior to be a weak learner
so that it contributes a
small amount to the overall fit.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mc.pbart(
   x.train, y.train, x.test=matrix(0.0,0,0),
   sparse=FALSE, theta=0, omega=1,
   a=0.5, b=1, augment=FALSE, rho=NULL,
   xinfo=matrix(0.0,0,0), usequants=FALSE,
   cont=FALSE, rm.const=TRUE,
   k=2.0, power=2.0, base=.95,
   binaryOffset=NULL,
   ntree=50L, numcut=100L,
   ndpost=1000L, nskip=100L,
   keepevery=1L, printevery=100,
   keeptrainfits=TRUE, transposed=FALSE,

   mc.cores = 2L, nice = 19L,
   seed = 99L
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mc.pbart_+3A_x.train">x.train</code></td>
<td>

<p>Explanatory variables for training (in sample) data.<br />
May be a matrix or a data frame,
with (as usual) rows corresponding to observations and columns to variables.<br />
If a variable is a factor in a data frame, it is replaced with dummies.
Note that q dummies are created if q&gt;2 and
one dummy is created if q=2, where q is the number of levels of the factor.
<code>pbart</code> will generate draws of <code class="reqn">f(x)</code> for each <code class="reqn">x</code>
which is a row of x.train.
</p>
</td></tr>
<tr><td><code id="mc.pbart_+3A_y.train">y.train</code></td>
<td>

<p>Binary dependent variable for training (in sample) data.<br />



</p>
</td></tr>
<tr><td><code id="mc.pbart_+3A_x.test">x.test</code></td>
<td>

<p>Explanatory variables for test (out of sample) data.<br />
Should have same structure as x.train.<br />
<code>pbart</code> will generate draws of <code class="reqn">f(x)</code> for each <code class="reqn">x</code> which is a row of x.test.
</p>
</td></tr>
<tr><td><code id="mc.pbart_+3A_sparse">sparse</code></td>
<td>
<p>Whether to perform variable selection based on a
sparse Dirichlet prior rather than simply uniform; see Linero 2016.</p>
</td></tr>
<tr><td><code id="mc.pbart_+3A_theta">theta</code></td>
<td>
<p>Set <code class="reqn">theta</code> parameter; zero means random.</p>
</td></tr>
<tr><td><code id="mc.pbart_+3A_omega">omega</code></td>
<td>
<p>Set <code class="reqn">omega</code> parameter; zero means random.</p>
</td></tr>
<tr><td><code id="mc.pbart_+3A_a">a</code></td>
<td>
<p>Sparse parameter for <code class="reqn">Beta(a, b)</code> prior:
<code class="reqn">0.5&lt;=a&lt;=1</code> where lower values inducing more sparsity.</p>
</td></tr>
<tr><td><code id="mc.pbart_+3A_b">b</code></td>
<td>
<p>Sparse parameter for <code class="reqn">Beta(a, b)</code> prior; typically,
<code class="reqn">b=1</code>.</p>
</td></tr>
<tr><td><code id="mc.pbart_+3A_rho">rho</code></td>
<td>
<p>Sparse parameter: typically <code class="reqn">rho=p</code> where <code class="reqn">p</code> is the
number of covariates under consideration.</p>
</td></tr>
<tr><td><code id="mc.pbart_+3A_augment">augment</code></td>
<td>
<p>Whether data augmentation is to be performed in sparse
variable selection.</p>
</td></tr>
<tr><td><code id="mc.pbart_+3A_xinfo">xinfo</code></td>
<td>
<p> You can provide the cutpoints to BART or let BART
choose them for you.  To provide them, use the <code>xinfo</code>
argument to specify a list (matrix) where the items (rows) are the
covariates and the contents of the items (columns) are the
cutpoints.  </p>
</td></tr>
<tr><td><code id="mc.pbart_+3A_usequants">usequants</code></td>
<td>
<p> If <code>usequants=FALSE</code>, then the
cutpoints in <code>xinfo</code> are generated uniformly; otherwise,
if <code>TRUE</code>, uniform quantiles are used for the cutpoints. </p>
</td></tr>
<tr><td><code id="mc.pbart_+3A_cont">cont</code></td>
<td>
<p> Whether or not to assume all variables are continuous.</p>
</td></tr>
<tr><td><code id="mc.pbart_+3A_rm.const">rm.const</code></td>
<td>
<p> Whether or not to remove constant variables.</p>
</td></tr>  
<tr><td><code id="mc.pbart_+3A_k">k</code></td>
<td>




<p>For binary y,
k is the number of prior standard deviations <code class="reqn">f(x)</code> is away from +/-3.

The bigger k is, the more conservative the fitting will be.
</p>
</td></tr>
<tr><td><code id="mc.pbart_+3A_power">power</code></td>
<td>

<p>Power parameter for tree prior.
</p>
</td></tr>
<tr><td><code id="mc.pbart_+3A_base">base</code></td>
<td>

<p>Base parameter for tree prior.
</p>
</td></tr>
<tr><td><code id="mc.pbart_+3A_binaryoffset">binaryOffset</code></td>
<td>

<p>Used for binary <code class="reqn">y</code>.<br />
The model is <code class="reqn">P(Y=1 | x) = F(f(x) + binaryOffset)</code>.<br />


</p>
</td></tr>
<tr><td><code id="mc.pbart_+3A_ntree">ntree</code></td>
<td>

<p>The number of trees in the sum.
</p>
</td></tr>
<tr><td><code id="mc.pbart_+3A_numcut">numcut</code></td>
<td>

<p>The number of possible values of c (see usequants).
If a single number if given, this is used for all variables.
Otherwise a vector with length equal to ncol(x.train) is required,
where the <code class="reqn">i^{th}</code> element gives the number of c used for
the <code class="reqn">i^{th}</code> variable in x.train.
If usequants is false, numcut equally spaced cutoffs
are used covering the range of values in the corresponding
column of x.train.  If usequants is true, then  min(numcut, the number of unique values in the
corresponding columns of x.train - 1) c values are used.
</p>
</td></tr>
<tr><td><code id="mc.pbart_+3A_ndpost">ndpost</code></td>
<td>

<p>The number of posterior draws returned.
</p>
</td></tr>
<tr><td><code id="mc.pbart_+3A_nskip">nskip</code></td>
<td>

<p>Number of MCMC iterations to be treated as burn in.
</p>
</td></tr>
<tr><td><code id="mc.pbart_+3A_keepevery">keepevery</code></td>
<td>

<p>Every keepevery draw is kept to be returned to the user.
</p>
</td></tr>
<tr><td><code id="mc.pbart_+3A_printevery">printevery</code></td>
<td>

<p>As the MCMC runs, a message is printed every printevery draws.
</p>
</td></tr>
<tr><td><code id="mc.pbart_+3A_keeptrainfits">keeptrainfits</code></td>
<td>

<p>Whether to keep <code>yhat.train</code> or not.
</p>
</td></tr>
<tr><td><code id="mc.pbart_+3A_transposed">transposed</code></td>
<td>

<p>When running <code>pbart</code> in parallel, it is more memory-efficient
to transpose <code>x.train</code> and <code>x.test</code>, if any, prior to
calling <code>mc.pbart</code>.
</p>
</td></tr>





<tr><td><code id="mc.pbart_+3A_seed">seed</code></td>
<td>

<p>Setting the seed required for reproducible MCMC.
</p>
</td></tr>
<tr><td><code id="mc.pbart_+3A_mc.cores">mc.cores</code></td>
<td>

<p>Number of cores to employ in parallel.
</p>
</td></tr>
<tr><td><code id="mc.pbart_+3A_nice">nice</code></td>
<td>

<p>Set the job niceness.  The default
niceness is 19: niceness goes from 0 (highest) to 19 (lowest).
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>BART is an Bayesian MCMC method.
At each MCMC interation, we produce a draw from 


<code class="reqn">f</code> in the binary <code class="reqn">y</code> case.
</p>
<p>Thus, unlike a lot of other modelling methods in R, we do not produce a single model object
from which fits and summaries may be extracted.  The output consists of values
<code class="reqn">f^*(x)</code> 
where * denotes a particular draw.
The <code class="reqn">x</code> is either a row from the training data (x.train) or the test data (x.test).
</p>


<h3>Value</h3>

<p><code>mc.pbart</code> returns an object of type <code>pbart</code> which is
essentially a list. 
</p>
<table>
<tr><td><code>yhat.train</code></td>
<td>

<p>A matrix with ndpost rows and nrow(x.train) columns.
Each row corresponds to a draw <code class="reqn">f^*</code> from the posterior of <code class="reqn">f</code>
and each column corresponds to a row of x.train.
The <code class="reqn">(i,j)</code> value is <code class="reqn">f^*(x)</code> for the <code class="reqn">i^{th}</code> kept draw of <code class="reqn">f</code>
and the <code class="reqn">j^{th}</code> row of x.train.<br />
Burn-in is dropped.
</p>
</td></tr>
<tr><td><code>yhat.test</code></td>
<td>
<p>Same as yhat.train but now the x's are the rows of the test data.</p>
</td></tr>


<tr><td><code>varcount</code></td>
<td>
<p>a matrix with ndpost rows and nrow(x.train) columns.
Each row is for a draw. For each variable (corresponding to the columns),
the total count of the number of times
that variable is used in a tree decision rule (over all trees) is given.</p>
</td></tr>
</table>
<p>In addition the list
has a binaryOffset component giving the value used.
</p>
<p>Note that in the binary <code class="reqn">y</code>, case yhat.train and yhat.test are
<code class="reqn">f(x)</code> + binaryOffset.  If you want draws of the probability
<code class="reqn">P(Y=1 | x)</code> you need to apply the normal cdf (<code>pnorm</code>)
to these values.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pbart">pbart</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(99)
n=5000
x = sort(-2+4*runif(n))
X=matrix(x,ncol=1)
f = function(x) {return((1/2)*x^3)}
FL = function(x) {return(exp(x)/(1+exp(x)))}
pv = FL(f(x))
y = rbinom(n,1,pv)
np=100
xp=-2+4*(1:np)/np
Xp=matrix(xp,ncol=1)

## parallel::mcparallel/mccollect do not exist on windows
if(.Platform$OS.type=='unix') {
##test BART with token run to ensure installation works
    mf = mc.pbart(X, y, nskip=5, ndpost=5, mc.cores=1, seed=99)
}

## Not run: 
set.seed(99)
pf = pbart(X,y,Xp)

## plot(f(Xp), pf$yhat.test.mean, xlim=c(-4, 4), ylim=c(-4, 4),
##      xlab='True f(x)', ylab='BART f(x)')
## lines(c(-4, 4), c(-4, 4))

mf = mc.pbart(X,y,Xp, mc.cores=4, seed=99)

## plot(f(Xp), mf$yhat.test.mean, xlim=c(-4, 4), ylim=c(-4, 4),
##      xlab='True f(x)', ylab='BART f(x)')
## lines(c(-4, 4), c(-4, 4))

par(mfrow=c(2,2))

plot(range(xp),range(pf$yhat.test),xlab='x',ylab='f(x)',type='n')
lines(x,f(x),col='blue',lwd=2)
lines(xp,apply(pf$yhat.test,2,mean),col='red')
qpl = apply(pf$yhat.test,2,quantile,probs=c(.025,.975))
lines(xp,qpl[1,],col='green',lty=1)
lines(xp,qpl[2,],col='green',lty=1)
title(main='BART::pbart f(x) with 0.95 intervals')

plot(range(xp),range(mf$yhat.test),xlab='x',ylab='f(x)',type='n')
lines(x,f(x),col='blue',lwd=2)
lines(xp,apply(mf$yhat.test,2,mean),col='red')
qpl = apply(mf$yhat.test,2,quantile,probs=c(.025,.975))
lines(xp,qpl[1,],col='green',lty=1)
lines(xp,qpl[2,],col='green',lty=1)
title(main='BART::mc.pbart f(x) with 0.95 intervals')

## plot(pf$yhat.test.mean,apply(mf$yhat.test,2,mean),xlab='BART::pbart',ylab='BART::mc.pbart')
## abline(0,1,col='red')
## title(main="BART::pbart f(x) vs. BART::mc.pbart f(x)")

## End(Not run)
</code></pre>

<hr>
<h2 id='mc.surv.pwbart'>Predicting new observations with a previously fitted BART model</h2><span id='topic+surv.pwbart'></span><span id='topic+mc.surv.pwbart'></span><span id='topic+recur.pwbart'></span><span id='topic+mc.recur.pwbart'></span>

<h3>Description</h3>

<p>BART is a Bayesian &ldquo;sum-of-trees&rdquo; model.<br />
For a numeric response <code class="reqn">y</code>, we have
<code class="reqn">y = f(x) + \epsilon</code>,
where <code class="reqn">\epsilon \sim N(0,\sigma^2)</code>.<br />
</p>
<p><code class="reqn">f</code> is the sum of many tree models.
The goal is to have very flexible inference for the uknown
function <code class="reqn">f</code>.
</p>
<p>In the spirit of &ldquo;ensemble models&rdquo;,
each tree is constrained by a prior to be a weak learner
so that it contributes a
small amount to the overall fit.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>surv.pwbart(
                x.test,
                treedraws,
                binaryOffset=0,
                mc.cores=1L,
                type='pbart',
                transposed=FALSE, nice=19L
              )

mc.surv.pwbart(
                x.test,
                treedraws,
                binaryOffset=0,
                mc.cores=2L,
                type='pbart',
                transposed=FALSE, nice=19L
              )

mc.recur.pwbart(
                x.test,
                treedraws,
                binaryOffset=0,
                mc.cores=2L, 
                type='pbart',
                transposed=FALSE, nice=19L
               )

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mc.surv.pwbart_+3A_x.test">x.test</code></td>
<td>

<p>Matrix of covariates to predict <code class="reqn">y</code> for.
</p>
</td></tr>
<tr><td><code id="mc.surv.pwbart_+3A_binaryoffset">binaryOffset</code></td>
<td>

<p>Mean to add on to <code class="reqn">y</code> prediction.
</p>
</td></tr>
<tr><td><code id="mc.surv.pwbart_+3A_treedraws">treedraws</code></td>
<td>

<p><code>$treedraws</code> returned from <code>surv.bart</code>,
<code>mc.surv.bart</code>, <code>recur.bart</code> or <code>mc.recur.bart</code>.
</p>
</td></tr>
<tr><td><code id="mc.surv.pwbart_+3A_mc.cores">mc.cores</code></td>
<td>

<p>Number of threads to utilize.
</p>
</td></tr>
<tr><td><code id="mc.surv.pwbart_+3A_type">type</code></td>
<td>
<p> Whether to employ Albert-Chib, <code>'pbart'</code>, or
Holmes-Held, <code>'lbart'</code>. </p>
</td></tr>
<tr><td><code id="mc.surv.pwbart_+3A_transposed">transposed</code></td>
<td>

<p>When running <code>pwbart</code> or <code>mc.pwbart</code> in parallel, it is more memory-efficient
to transpose <code>x.test</code> prior to calling the internal versions of these functions.
</p>
</td></tr>
<tr><td><code id="mc.surv.pwbart_+3A_nice">nice</code></td>
<td>

<p>Set the job niceness.  The default
niceness is 19: niceness goes from 0 (highest) to 19 (lowest).
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>BART is an Bayesian MCMC method.
At each MCMC interation, we produce a draw from the joint posterior
<code class="reqn">(f,\sigma) | (x,y)</code> in the numeric <code class="reqn">y</code> case
and just <code class="reqn">f</code> in the binary <code class="reqn">y</code> case.
</p>
<p>Thus, unlike a lot of other modelling methods in R, we do not produce a single model object
from which fits and summaries may be extracted.  The output consists of values
<code class="reqn">f^*(x)</code> (and <code class="reqn">\sigma^*</code> in the numeric case) where * denotes a particular draw.
The <code class="reqn">x</code> is either a row from the training data (x.train) or the test data (x.test).
</p>


<h3>Value</h3>

<p>Returns an object of type <code>survbart</code> which is essentially a list with components:
</p>
<table>
<tr><td><code>yhat.test</code></td>
<td>

<p>A matrix with ndpost rows and nrow(x.test) columns.
Each row corresponds to a draw <code class="reqn">f^*</code> from the posterior of <code class="reqn">f</code>
and each column corresponds to a row of x.train.
The <code class="reqn">(i,j)</code> value is <code class="reqn">f^*(x)</code> for the <code class="reqn">i^{th}</code> kept draw of <code class="reqn">f</code>
and the <code class="reqn">j^{th}</code> row of x.train.<br />
Burn-in is dropped.
</p>
</td></tr>


<tr><td><code>surv.test</code></td>
<td>
<p>test data fits for survival probability: not
available for <code>mc.recur.pwbart</code>.</p>
</td></tr>
<tr><td><code>surv.test.mean</code></td>
<td>
<p>mean of <code>surv.test</code> over the posterior samples: not
available for <code>mc.recur.pwbart</code>.</p>
</td></tr>
<tr><td><code>haz.test</code></td>
<td>
<p>test data fits for hazard: available for
<code>mc.recur.pwbart</code> only.</p>
</td></tr>
<tr><td><code>haz.test.mean</code></td>
<td>
<p>mean of <code>haz.test</code> over the posterior samples:
available for <code>mc.recur.pwbart</code> only.</p>
</td></tr>
<tr><td><code>cum.test</code></td>
<td>
<p>test data fits for cumulative hazard: available for
<code>mc.recur.pwbart</code> only.</p>
</td></tr>
<tr><td><code>cum.test.mean</code></td>
<td>
<p>mean of <code>cum.test</code> over the posterior samples:
available for <code>mc.recur.pwbart</code> only.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+pwbart">pwbart</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## load the advanced lung cancer example
data(lung)

group &lt;- -which(is.na(lung[ , 7])) ## remove missing row for ph.karno
times &lt;- lung[group, 2]   ##lung$time
delta &lt;- lung[group, 3]-1 ##lung$status: 1=censored, 2=dead
                          ##delta: 0=censored, 1=dead

## this study reports time in days rather than months like other studies
## coarsening from days to months will reduce the computational burden
times &lt;- ceiling(times/30)

summary(times)
table(delta)

x.train &lt;- as.matrix(lung[group, c(4, 5, 7)]) ## matrix of observed covariates

## lung$age:        Age in years
## lung$sex:        Male=1 Female=2
## lung$ph.karno:   Karnofsky performance score (dead=0:normal=100:by=10)
##                  rated by physician

dimnames(x.train)[[2]] &lt;- c('age(yr)', 'M(1):F(2)', 'ph.karno(0:100:10)')

summary(x.train[ , 1])
table(x.train[ , 2])
table(x.train[ , 3])

x.test &lt;- matrix(nrow=84, ncol=3) ## matrix of covariate scenarios

dimnames(x.test)[[2]] &lt;- dimnames(x.train)[[2]]

i &lt;- 1

for(age in 5*(9:15)) for(sex in 1:2) for(ph.karno in 10*(5:10)) {
    x.test[i, ] &lt;- c(age, sex, ph.karno)
    i &lt;- i+1
}

## this x.test is relatively small, but often you will want to
## predict for a large x.test matrix which may cause problems
## due to consumption of RAM so we can predict separately

## mcparallel/mccollect do not exist on windows
if(.Platform$OS.type=='unix') {
##test BART with token run to ensure installation works
    set.seed(99)
    post &lt;- surv.bart(x.train=x.train, times=times, delta=delta, nskip=5, ndpost=5, keepevery=1)

    pre &lt;- surv.pre.bart(x.train=x.train, times=times, delta=delta, x.test=x.test)

    pred &lt;- mc.surv.pwbart(pre$tx.test, post$treedraws, post$binaryOffset)
}

## Not run: 
## run one long MCMC chain in one process
set.seed(99)
post &lt;- surv.bart(x.train=x.train, times=times, delta=delta)

## run "mc.cores" number of shorter MCMC chains in parallel processes
## post &lt;- mc.surv.bart(x.train=x.train, times=times, delta=delta,
##                      mc.cores=8, seed=99)

pre &lt;- surv.pre.bart(x.train=x.train, times=times, delta=delta, x.test=x.test)

pred &lt;- surv.pwbart(pre$tx.test, post$treedraws, post$binaryOffset)

## let's look at some survival curves
## first, a younger group with a healthier KPS
## age 50 with KPS=90: males and females
## males: row 17, females: row 23
x.test[c(17, 23), ]

low.risk.males &lt;- 16*post$K+1:post$K ## K=unique times including censoring
low.risk.females &lt;- 22*post$K+1:post$K

plot(post$times, pred$surv.test.mean[low.risk.males], type='s', col='blue',
     main='Age 50 with KPS=90', xlab='t', ylab='S(t)', ylim=c(0, 1))
points(post$times, pred$surv.test.mean[low.risk.females], type='s', col='red')


## End(Not run)
</code></pre>

<hr>
<h2 id='mc.wbart'>BART for continuous outcomes with parallel computation</h2><span id='topic+mc.wbart'></span>

<h3>Description</h3>

<p>BART is a Bayesian &ldquo;sum-of-trees&rdquo; model.<br />
For numeric response <code class="reqn">y</code>, we have
<code class="reqn">y = f(x) + \epsilon</code>,
where <code class="reqn">\epsilon \sim N(0,\sigma^2)</code>.<br />


</p>
<p>In both cases, <code class="reqn">f</code> is the sum of many tree models.
The goal is to have very flexible inference for the uknown
function <code class="reqn">f</code>.
</p>
<p>In the spirit of &ldquo;ensemble models&rdquo;,
each tree is constrained by a prior to be a weak learner
so that it contributes a
small amount to the overall fit.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mc.wbart(
   x.train, y.train, x.test=matrix(0.0,0,0),
   sparse=FALSE, theta=0, omega=1,
   a=0.5, b=1, augment=FALSE, rho=NULL,
   xinfo=matrix(0.0,0,0), usequants=FALSE,
   cont=FALSE, rm.const=TRUE,
   sigest=NA, sigdf=3, sigquant=0.90,
   k=2.0, power=2.0, base=.95,
   sigmaf=NA, lambda=NA, fmean=mean(y.train),
   w=rep(1,length(y.train)),
   ntree=200L, numcut=100L,
   ndpost=1000L, nskip=100L,
   keepevery=1L, printevery=100,
   keeptrainfits=TRUE, transposed=FALSE,
   
   mc.cores = 2L, nice = 19L,
   seed = 99L
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mc.wbart_+3A_x.train">x.train</code></td>
<td>

<p>Explanatory variables for training (in sample) data.<br />
May be a matrix or a data frame,
with (as usual) rows corresponding to observations and columns to variables.<br />
If a variable is a factor in a data frame, it is replaced with dummies.
Note that q dummies are created if q&gt;2 and
one dummy is created if q=2, where q is the number of levels of the factor.
<code>wbart</code> will generate draws of <code class="reqn">f(x)</code> for each <code class="reqn">x</code>
which is a row of x.train.
</p>
</td></tr>
<tr><td><code id="mc.wbart_+3A_y.train">y.train</code></td>
<td>

<p>Dependent variable for training (in sample) data.<br />
If y is numeric a continous response model is fit (normal errors).<br />


</p>
</td></tr>
<tr><td><code id="mc.wbart_+3A_x.test">x.test</code></td>
<td>

<p>Explanatory variables for test (out of sample) data.<br />
Should have same structure as x.train.<br />
<code>wbart</code> will generate draws of <code class="reqn">f(x)</code> for each <code class="reqn">x</code> which is a row of x.test.
</p>
</td></tr>
<tr><td><code id="mc.wbart_+3A_sparse">sparse</code></td>
<td>
<p>Whether to perform variable selection based on a
sparse Dirichlet prior rather than simply uniform; see Linero 2016.</p>
</td></tr>
<tr><td><code id="mc.wbart_+3A_theta">theta</code></td>
<td>
<p>Set <code class="reqn">theta</code> parameter; zero means random.</p>
</td></tr>
<tr><td><code id="mc.wbart_+3A_omega">omega</code></td>
<td>
<p>Set <code class="reqn">omega</code> parameter; zero means random.</p>
</td></tr>
<tr><td><code id="mc.wbart_+3A_a">a</code></td>
<td>
<p>Sparse parameter for <code class="reqn">Beta(a, b)</code> prior:
<code class="reqn">0.5&lt;=a&lt;=1</code> where lower values inducing more sparsity.</p>
</td></tr>
<tr><td><code id="mc.wbart_+3A_b">b</code></td>
<td>
<p>Sparse parameter for <code class="reqn">Beta(a, b)</code> prior; typically,
<code class="reqn">b=1</code>.</p>
</td></tr>
<tr><td><code id="mc.wbart_+3A_rho">rho</code></td>
<td>
<p>Sparse parameter: typically <code class="reqn">rho=p</code> where <code class="reqn">p</code> is the
number of covariates under consideration.</p>
</td></tr>
<tr><td><code id="mc.wbart_+3A_augment">augment</code></td>
<td>
<p>Whether data augmentation is to be performed in sparse
variable selection.</p>
</td></tr>
<tr><td><code id="mc.wbart_+3A_xinfo">xinfo</code></td>
<td>
<p> You can provide the cutpoints to BART or let BART
choose them for you.  To provide them, use the <code>xinfo</code>
argument to specify a list (matrix) where the items (rows) are the
covariates and the contents of the items (columns) are the
cutpoints.  </p>
</td></tr>
<tr><td><code id="mc.wbart_+3A_usequants">usequants</code></td>
<td>
<p> If <code>usequants=FALSE</code>, then the
cutpoints in <code>xinfo</code> are generated uniformly; otherwise,
if <code>TRUE</code>, uniform quantiles are used for the cutpoints. </p>
</td></tr>
<tr><td><code id="mc.wbart_+3A_cont">cont</code></td>
<td>
<p> Whether or not to assume all variables are continuous.</p>
</td></tr>
<tr><td><code id="mc.wbart_+3A_rm.const">rm.const</code></td>
<td>
<p> Whether or not to remove constant variables.</p>
</td></tr>
<tr><td><code id="mc.wbart_+3A_sigest">sigest</code></td>
<td>

<p>The prior for the error variance  (<code class="reqn">\sigma^2</code>) is inverted chi-squared
(the standard conditionally conjugate prior).
The prior is specified by choosing the degrees of freedom, a rough estimate of the
corresponding standard deviation and a quantile to put this rough estimate at.
If sigest=NA then the rough estimate will be the usual least squares estimator.
Otherwise the supplied value will be used.

</p>
</td></tr>
<tr><td><code id="mc.wbart_+3A_sigdf">sigdf</code></td>
<td>

<p>Degrees of freedom for error variance prior.

</p>
</td></tr>
<tr><td><code id="mc.wbart_+3A_sigquant">sigquant</code></td>
<td>

<p>The quantile of the prior that the rough estimate (see sigest) is placed at.
The closer the quantile is to 1,
the more aggresive the fit will be as you are putting more prior weight
on error standard deviations (<code class="reqn">\sigma</code>) less than the rough estimate.

</p>
</td></tr>
<tr><td><code id="mc.wbart_+3A_k">k</code></td>
<td>

<p>For numeric y,
k is the number of prior standard deviations <code class="reqn">E(Y|x) = f(x)</code> is away from +/-.5.



The bigger k is, the more conservative the fitting will be.
</p>
</td></tr>
<tr><td><code id="mc.wbart_+3A_power">power</code></td>
<td>

<p>Power parameter for tree prior.
</p>
</td></tr>
<tr><td><code id="mc.wbart_+3A_base">base</code></td>
<td>

<p>Base parameter for tree prior.
</p>
</td></tr>
<tr><td><code id="mc.wbart_+3A_sigmaf">sigmaf</code></td>
<td>

<p>The SD of f.
</p>
</td></tr>
<tr><td><code id="mc.wbart_+3A_lambda">lambda</code></td>
<td>

<p>The scale of the prior for the variance.
</p>
</td></tr>
<tr><td><code id="mc.wbart_+3A_fmean">fmean</code></td>
<td>

<p>BART operates on <code>y.train</code> centered by <code>fmean</code>.
</p>
</td></tr>
<tr><td><code id="mc.wbart_+3A_w">w</code></td>
<td>

<p>Vector of weights which multiply the variance.
</p>
</td></tr>
<tr><td><code id="mc.wbart_+3A_ntree">ntree</code></td>
<td>

<p>The number of trees in the sum.
</p>
</td></tr>
<tr><td><code id="mc.wbart_+3A_numcut">numcut</code></td>
<td>

<p>The number of possible values of c (see usequants).
If a single number if given, this is used for all variables.
Otherwise a vector with length equal to ncol(x.train) is required,
where the <code class="reqn">i^{th}</code> element gives the number of c used for
the <code class="reqn">i^{th}</code> variable in x.train.
If usequants is false, numcut equally spaced cutoffs
are used covering the range of values in the corresponding
column of x.train.  If usequants is true, then  min(numcut, the number of unique values in the
corresponding columns of x.train - 1) c values are used.
</p>
</td></tr>
<tr><td><code id="mc.wbart_+3A_ndpost">ndpost</code></td>
<td>

<p>The number of posterior draws returned.
</p>
</td></tr>
<tr><td><code id="mc.wbart_+3A_nskip">nskip</code></td>
<td>

<p>Number of MCMC iterations to be treated as burn in.
</p>
</td></tr>
<tr><td><code id="mc.wbart_+3A_keepevery">keepevery</code></td>
<td>

<p>Every keepevery draw is kept to be returned to the user.
</p>
</td></tr>
<tr><td><code id="mc.wbart_+3A_printevery">printevery</code></td>
<td>

<p>As the MCMC runs, a message is printed every printevery draws.
</p>
</td></tr>
<tr><td><code id="mc.wbart_+3A_keeptrainfits">keeptrainfits</code></td>
<td>

<p>Whether to keep <code>yhat.train</code> or not.
</p>
</td></tr>
<tr><td><code id="mc.wbart_+3A_transposed">transposed</code></td>
<td>

<p>When running <code>wbart</code> in parallel, it is more memory-efficient
to transpose <code>x.train</code> and <code>x.test</code>, if any, prior to
calling <code>mc.wbart</code>.
</p>
</td></tr>





<tr><td><code id="mc.wbart_+3A_seed">seed</code></td>
<td>

<p>Setting the seed required for reproducible MCMC.
</p>
</td></tr>
<tr><td><code id="mc.wbart_+3A_mc.cores">mc.cores</code></td>
<td>

<p>Number of cores to employ in parallel.
</p>
</td></tr>
<tr><td><code id="mc.wbart_+3A_nice">nice</code></td>
<td>

<p>Set the job niceness.  The default
niceness is 19: niceness goes from 0 (highest) to 19 (lowest).
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>BART is an Bayesian MCMC method.
At each MCMC interation, we produce a draw from the joint posterior
<code class="reqn">(f,\sigma) | (x,y)</code> in the numeric <code class="reqn">y</code> case.

</p>
<p>Thus, unlike a lot of other modelling methods in R, we do not produce a single model object
from which fits and summaries may be extracted.  The output consists of values
<code class="reqn">f^*(x)</code> (and <code class="reqn">\sigma^*</code> in the numeric case) where * denotes a particular draw.
The <code class="reqn">x</code> is either a row from the training data (x.train) or the test data (x.test).
</p>


<h3>Value</h3>

<p><code>mc.wbart</code> returns an object of type <code>wbart</code> which is
essentially a list. 
</p>
<table>
<tr><td><code>yhat.train</code></td>
<td>

<p>A matrix with ndpost rows and nrow(x.train) columns.
Each row corresponds to a draw <code class="reqn">f^*</code> from the posterior of <code class="reqn">f</code>
and each column corresponds to a row of x.train.
The <code class="reqn">(i,j)</code> value is <code class="reqn">f^*(x)</code> for the <code class="reqn">i^{th}</code> kept draw of <code class="reqn">f</code>
and the <code class="reqn">j^{th}</code> row of x.train.<br />
Burn-in is dropped.
</p>
</td></tr>
<tr><td><code>yhat.test</code></td>
<td>
<p>Same as yhat.train but now the x's are the rows of the test data.</p>
</td></tr>
<tr><td><code>yhat.train.mean</code></td>
<td>
<p>train data fits = mean of yhat.train columns.</p>
</td></tr>
<tr><td><code>yhat.test.mean</code></td>
<td>
<p>test data fits = mean of yhat.test columns.</p>
</td></tr>
<tr><td><code>varcount</code></td>
<td>
<p>a matrix with ndpost rows and nrow(x.train) columns.
Each row is for a draw. For each variable (corresponding to the columns),
the total count of the number of times
that variable is used in a tree decision rule (over all trees) is given.</p>
</td></tr>






</table>


<h3>See Also</h3>

<p><code><a href="#topic+wbart">wbart</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>##simulate data (example from Friedman MARS paper)
f = function(x){
10*sin(pi*x[,1]*x[,2]) + 20*(x[,3]-.5)^2+10*x[,4]+5*x[,5]
}
sigma = 1.0  #y = f(x) + sigma*z , z~N(0,1)
n = 100      #number of observations
set.seed(99)
x=matrix(runif(n*10),n,10) #10 variables, only first 5 matter
Ey = f(x)
y=Ey+sigma*rnorm(n)
lmFit = lm(y~.,data.frame(x,y)) #compare lm fit to BART later

## parallel::mcparallel/mccollect do not exist on windows
if(.Platform$OS.type=='unix') {
##test BART with token run to ensure installation works
    bartFit = mc.wbart(x,y,mc.cores=2,seed=99,nskip=5,ndpost=5)
}

## Not run: 
##run BART
bartFit = mc.wbart(x,y,mc.cores=5,seed=99)
##compare BART fit to linear matter and truth = Ey
fitmat = cbind(y,Ey,lmFit$fitted,bartFit$yhat.train.mean)
colnames(fitmat) = c('y','Ey','lm','bart')
print(cor(fitmat))

## End(Not run)
</code></pre>

<hr>
<h2 id='mc.wbart.gse'>Global SE variable selection for BART with parallel computation</h2><span id='topic+mc.wbart.gse'></span>

<h3>Description</h3>

<p>Here we implement the global SE method for variable selection in
nonparametric survival analysis with BART.  Unfortunately, the method is
very computationally intensive so we present some trade-offs below. </p>


<h3>Usage</h3>

<pre><code class='language-R'>mc.wbart.gse( x.train, y.train,
              P=50L, R=5L, ntree=20L, numcut=100L, C=1, alpha=0.05,
              k=2.0, power=2.0, base=0.95,
              ndpost=2000L, nskip=100L,
              printevery=100L, keepevery=1L, keeptrainfits=FALSE,
              seed=99L, mc.cores=2L, nice=19L 
              )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mc.wbart.gse_+3A_x.train">x.train</code></td>
<td>
<p> Explanatory variables for training (in sample)
data.<br /> Must be a matrix with (as usual) rows corresponding to
observations and columns to variables.<br /> <code>surv.bart</code> will generate
draws of <code class="reqn">f(t, x)</code> for each <code class="reqn">x</code> which is a row of x.train.  </p>
</td></tr>
<tr><td><code id="mc.wbart.gse_+3A_y.train">y.train</code></td>
<td>

<p>The continuous outcome.
</p>
</td></tr>
<tr><td><code id="mc.wbart.gse_+3A_p">P</code></td>
<td>

<p>The number of permutations: typically 50 or 100.
</p>
</td></tr>
<tr><td><code id="mc.wbart.gse_+3A_r">R</code></td>
<td>

<p>The number of replicates: typically 5 or 10.
</p>
</td></tr>   
<tr><td><code id="mc.wbart.gse_+3A_ntree">ntree</code></td>
<td>

<p>The number of trees.  In variable selection,
the number of trees is smaller than what might
be used for the best fit.
</p>
</td></tr>
<tr><td><code id="mc.wbart.gse_+3A_numcut">numcut</code></td>
<td>

<p>The number of possible values of c (see usequants).
If a single number if given, this is used for all variables.
Otherwise a vector with length equal to ncol(x.train) is required,
where the <code class="reqn">i^{th}</code> element gives the number of c used for
the <code class="reqn">i^{th}</code> variable in x.train.
If usequants is false, numcut equally spaced cutoffs
are used covering the range of values in the corresponding
column of x.train.  If usequants is true, then  min(numcut, the number of unique values in the
corresponding columns of x.train - 1) c values are used.
</p>
</td></tr>
<tr><td><code id="mc.wbart.gse_+3A_c">C</code></td>
<td>

<p>The starting value for the multiple of SE.  You should not need to
change this except in rare circumstances.
</p>
</td></tr>
<tr><td><code id="mc.wbart.gse_+3A_alpha">alpha</code></td>
<td>

<p>The global SE method relies on simultaneous 1-<code>alpha</code> coverage
across the permutations for all predictor variables.
</p>
</td></tr>
<tr><td><code id="mc.wbart.gse_+3A_k">k</code></td>
<td>

<p>k is the number of prior standard deviations <code class="reqn">f(t, x)</code> is away from +/-3.
The bigger k is, the more conservative the fitting will be.
</p>
</td></tr>
<tr><td><code id="mc.wbart.gse_+3A_power">power</code></td>
<td>

<p>Power parameter for tree prior.
</p>
</td></tr>
<tr><td><code id="mc.wbart.gse_+3A_base">base</code></td>
<td>

<p>Base parameter for tree prior.
</p>
</td></tr>
<tr><td><code id="mc.wbart.gse_+3A_ndpost">ndpost</code></td>
<td>

<p>The number of posterior draws after burn in.  In the global SE
method, generally, the method is repeated several times to
establish the variable count probabilities.  However, we take the
alternative approach of simply running the MCMC chain longer which
should result in the same stabilization of the estimates.  Therefore,
the number of posterior draws in variable selection should be set to a larger value than
would be typically anticipated for fitting. 
</p>
</td></tr>
<tr><td><code id="mc.wbart.gse_+3A_nskip">nskip</code></td>
<td>

<p>Number of MCMC iterations to be treated as burn in.
</p>
</td></tr>
<tr><td><code id="mc.wbart.gse_+3A_printevery">printevery</code></td>
<td>

<p>As the MCMC runs, a message is printed every printevery draws.
</p>
</td></tr>
<tr><td><code id="mc.wbart.gse_+3A_keepevery">keepevery</code></td>
<td>

<p>Every <code>keepevery</code> draw is kept.
</p>
</td></tr>
<tr><td><code id="mc.wbart.gse_+3A_keeptrainfits">keeptrainfits</code></td>
<td>

<p>If <code>TRUE</code> the draws of <code class="reqn">f(t, x)</code> for <code class="reqn">x</code> = rows of
x.train are generated.
</p>
</td></tr>
<tr><td><code id="mc.wbart.gse_+3A_seed">seed</code></td>
<td>

<p><code>seed</code> required for reproducible MCMC.
</p>
</td></tr>
<tr><td><code id="mc.wbart.gse_+3A_mc.cores">mc.cores</code></td>
<td>

<p>Number of cores to employ in parallel.
</p>
</td></tr>
<tr><td><code id="mc.wbart.gse_+3A_nice">nice</code></td>
<td>

<p>Set the job priority.  The default
priority is 19: priorities go from 0 (highest) to 19 (lowest).
</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>mc.wbart.gse</code> returns a list.
</p>


<h3>References</h3>

<p>Bleich, J., Kapelner, A., George, E.I., and Jensen, S.T. (2014).
Variable selection for BART: an application to gene regulation.
<em>The Annals of Applied Statistics</em>, <b>8:1750-81</b>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mc.wbart">mc.wbart</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

library(ElemStatLearn)

data(phoneme)

x.train &lt;- matrix(NA, nrow=4509, ncol=257)
    
dimnames(x.train)[[2]] &lt;- c(paste0('x.', 1:256), 'speaker')
    
x.train[ , 257] &lt;- as.numeric(phoneme$speaker)

for(j in 1:256) x.train[ , j] &lt;- as.numeric(phoneme[ , paste0('x.', j)])

gse &lt;- mc.wbart.gse(x.train, as.numeric(phoneme$g), mc.cores=5, seed=99)

## important variables
dimnames(x.train)[[2]][gse$which]
    

## End(Not run)
</code></pre>

<hr>
<h2 id='pbart'>Probit BART for dichotomous outcomes with Normal latents</h2><span id='topic+pbart'></span>

<h3>Description</h3>

<p>BART is a Bayesian &ldquo;sum-of-trees&rdquo; model.<br />



For a binary response <code class="reqn">y</code>, <code class="reqn">P(Y=1 | x) = F(f(x))</code>, where <code class="reqn">F</code>
denotes the standard Normal CDF (probit link).
</p>
<p>In both cases, <code class="reqn">f</code> is the sum of many tree models.
The goal is to have very flexible inference for the uknown
function <code class="reqn">f</code>.
</p>
<p>In the spirit of &ldquo;ensemble models&rdquo;,
each tree is constrained by a prior to be a weak learner
so that it contributes a
small amount to the overall fit.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pbart(
   x.train, y.train, x.test=matrix(0.0,0,0),
   sparse=FALSE, theta=0, omega=1,
   a=0.5, b=1, augment=FALSE, rho=NULL,
   xinfo=matrix(0.0,0,0), usequants=FALSE,
   cont=FALSE, rm.const=TRUE,
   k=2.0, power=2.0, base=.95,
   binaryOffset=NULL, 
   ntree=50L, numcut=100L,
   ndpost=1000L, nskip=100L, keepevery=1L,
   nkeeptrain=ndpost, nkeeptest=ndpost,

   nkeeptreedraws=ndpost,
   printevery=100L, transposed=FALSE 
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pbart_+3A_x.train">x.train</code></td>
<td>

<p>Explanatory variables for training (in sample) data.<br />
May be a matrix or a data frame,
with (as usual) rows corresponding to observations and columns to variables.<br />
If a variable is a factor in a data frame, it is replaced with dummies.
Note that q dummies are created if q&gt;2 and
one dummy is created if q=2, where q is the number of levels of the factor.
<code>pbart</code> will generate draws of <code class="reqn">f(x)</code> for each <code class="reqn">x</code>
which is a row of x.train.
</p>
</td></tr>
<tr><td><code id="pbart_+3A_y.train">y.train</code></td>
<td>

<p>Binary dependent variable for training (in sample) data.<br />



</p>
</td></tr>
<tr><td><code id="pbart_+3A_x.test">x.test</code></td>
<td>

<p>Explanatory variables for test (out of sample) data.<br />
Should have same structure as x.train.<br />
<code>pbart</code> will generate draws of <code class="reqn">f(x)</code> for each <code class="reqn">x</code> which is a row of x.test.
</p>
</td></tr>
<tr><td><code id="pbart_+3A_sparse">sparse</code></td>
<td>
<p>Whether to perform variable selection based on a
sparse Dirichlet prior rather than simply uniform; see Linero 2016.</p>
</td></tr>
<tr><td><code id="pbart_+3A_theta">theta</code></td>
<td>
<p>Set <code class="reqn">theta</code> parameter; zero means random.</p>
</td></tr>
<tr><td><code id="pbart_+3A_omega">omega</code></td>
<td>
<p>Set <code class="reqn">omega</code> parameter; zero means random.</p>
</td></tr>
<tr><td><code id="pbart_+3A_a">a</code></td>
<td>
<p>Sparse parameter for <code class="reqn">Beta(a, b)</code> prior:
<code class="reqn">0.5&lt;=a&lt;=1</code> where lower values inducing more sparsity.</p>
</td></tr>
<tr><td><code id="pbart_+3A_b">b</code></td>
<td>
<p>Sparse parameter for <code class="reqn">Beta(a, b)</code> prior; typically,
<code class="reqn">b=1</code>.</p>
</td></tr>
<tr><td><code id="pbart_+3A_rho">rho</code></td>
<td>
<p>Sparse parameter: typically <code class="reqn">rho=p</code> where <code class="reqn">p</code> is the
number of covariates under consideration.</p>
</td></tr>
<tr><td><code id="pbart_+3A_augment">augment</code></td>
<td>
<p>Whether data augmentation is to be performed in sparse
variable selection.</p>
</td></tr>
<tr><td><code id="pbart_+3A_xinfo">xinfo</code></td>
<td>
<p> You can provide the cutpoints to BART or let BART
choose them for you.  To provide them, use the <code>xinfo</code>
argument to specify a list (matrix) where the items (rows) are the
covariates and the contents of the items (columns) are the
cutpoints.  </p>
</td></tr>
<tr><td><code id="pbart_+3A_usequants">usequants</code></td>
<td>
<p> If <code>usequants=FALSE</code>, then the
cutpoints in <code>xinfo</code> are generated uniformly; otherwise,
if <code>TRUE</code>, uniform quantiles are used for the cutpoints. </p>
</td></tr>
<tr><td><code id="pbart_+3A_cont">cont</code></td>
<td>
<p> Whether or not to assume all variables are continuous.</p>
</td></tr>
<tr><td><code id="pbart_+3A_rm.const">rm.const</code></td>
<td>
<p> Whether or not to remove constant variables.</p>
</td></tr>
<tr><td><code id="pbart_+3A_k">k</code></td>
<td>




<p>For binary y,
k is the number of prior standard deviations <code class="reqn">f(x)</code> is away from +/-3.

The bigger k is, the more conservative the fitting will be.
</p>
</td></tr>
<tr><td><code id="pbart_+3A_power">power</code></td>
<td>

<p>Power parameter for tree prior.
</p>
</td></tr>
<tr><td><code id="pbart_+3A_base">base</code></td>
<td>

<p>Base parameter for tree prior.
</p>
</td></tr>
<tr><td><code id="pbart_+3A_binaryoffset">binaryOffset</code></td>
<td>

<p>Used for binary <code class="reqn">y</code>.<br />
The model is <code class="reqn">P(Y=1 | x) = F(f(x) + binaryOffset)</code>.<br />


</p>
</td></tr>





<tr><td><code id="pbart_+3A_ntree">ntree</code></td>
<td>

<p>The number of trees in the sum.
</p>
</td></tr>
<tr><td><code id="pbart_+3A_numcut">numcut</code></td>
<td>

<p>The number of possible values of c (see usequants).
If a single number if given, this is used for all variables.
Otherwise a vector with length equal to ncol(x.train) is required,
where the <code class="reqn">i^{th}</code> element gives the number of c used for
the <code class="reqn">i^{th}</code> variable in x.train.
If usequants is false, numcut equally spaced cutoffs
are used covering the range of values in the corresponding
column of x.train.  If usequants is true, then  min(numcut, the number of unique values in the
corresponding columns of x.train - 1) c values are used.
</p>
</td></tr>
<tr><td><code id="pbart_+3A_ndpost">ndpost</code></td>
<td>

<p>The number of posterior draws returned.
</p>
</td></tr>
<tr><td><code id="pbart_+3A_nskip">nskip</code></td>
<td>

<p>Number of MCMC iterations to be treated as burn in.
</p>
</td></tr>
<tr><td><code id="pbart_+3A_nkeeptrain">nkeeptrain</code></td>
<td>

<p>Number of MCMC iterations to be returned for train data.
</p>
</td></tr>
<tr><td><code id="pbart_+3A_nkeeptest">nkeeptest</code></td>
<td>

<p>Number of MCMC iterations to be returned for test data.
</p>
</td></tr>



<tr><td><code id="pbart_+3A_nkeeptreedraws">nkeeptreedraws</code></td>
<td>

<p>Number of MCMC iterations to be returned for tree draws.
</p>
</td></tr>
<tr><td><code id="pbart_+3A_keepevery">keepevery</code></td>
<td>

<p>Every keepevery draw is kept to be returned to the user.
</p>
</td></tr>
<tr><td><code id="pbart_+3A_printevery">printevery</code></td>
<td>

<p>As the MCMC runs, a message is printed every printevery draws.
</p>
</td></tr>







<tr><td><code id="pbart_+3A_transposed">transposed</code></td>
<td>

<p>When running <code>pbart</code> in parallel, it is more memory-efficient
to transpose <code>x.train</code> and <code>x.test</code>, if any, prior to
calling <code>mc.pbart</code>.
</p>
</td></tr>





</table>


<h3>Details</h3>

<p>BART is an Bayesian MCMC method.
At each MCMC interation, we produce a draw from 


<code class="reqn">f</code> in the binary <code class="reqn">y</code> case.
</p>
<p>Thus, unlike a lot of other modelling methods in R, we do not produce a single model object
from which fits and summaries may be extracted.  The output consists of values
<code class="reqn">f^*(x)</code>

where * denotes a particular draw.
The <code class="reqn">x</code> is either a row from the training data (x.train) or the test data (x.test).
</p>


<h3>Value</h3>

<p><code>pbart</code> returns an object of type <code>pbart</code> which is
essentially a list. 
</p>
<table>
<tr><td><code>yhat.train</code></td>
<td>

<p>A matrix with ndpost rows and nrow(x.train) columns.
Each row corresponds to a draw <code class="reqn">f^*</code> from the posterior of <code class="reqn">f</code>
and each column corresponds to a row of x.train.
The <code class="reqn">(i,j)</code> value is <code class="reqn">f^*(x)</code> for the <code class="reqn">i^{th}</code> kept draw of <code class="reqn">f</code>
and the <code class="reqn">j^{th}</code> row of x.train.<br />
Burn-in is dropped.
</p>
</td></tr>
<tr><td><code>yhat.test</code></td>
<td>
<p>Same as yhat.train but now the x's are the rows of the test data.</p>
</td></tr>


<tr><td><code>varcount</code></td>
<td>
<p>a matrix with ndpost rows and nrow(x.train) columns.
Each row is for a draw. For each variable (corresponding to the columns),
the total count of the number of times
that variable is used in a tree decision rule (over all trees) is given.</p>
</td></tr>
</table>
<p>In addition the list
has a binaryOffset component giving the value used.
</p>
<p>Note that in the binary <code class="reqn">y</code>, case yhat.train and yhat.test are
<code class="reqn">f(x)</code> + binaryOffset.  If you want draws of the probability
<code class="reqn">P(Y=1 | x)</code> you need to apply the Normal CDF (<code>pnorm</code>)
to these values.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+wbart">wbart</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(ACTG175)

## exclude those who do not have CD4 count at 96 weeks
ex &lt;- is.na(ACTG175$cd496)
table(ex)

## inclusion criteria are CD4 counts between 200 and 500
ACTG175$cd40 &lt;- min(500, max(250, ACTG175$cd40))

## calculate relative CD4 decline
y &lt;- ((ACTG175$cd496-ACTG175$cd40)/ACTG175$cd40)[!ex]
summary(y)

## 0=failure, 1=success
y &lt;- 1*(y &gt; -0.5)

## summarize CD4 outcomes
table(y, ACTG175$arms[!ex])

table(y, ACTG175$arms[!ex])/
    matrix(table(ACTG175$arms[!ex]), nrow=2, ncol=4, byrow=TRUE)

## drop unneeded and unwanted variables
## 1: 'pidnum' patient ID number
##14: 'str2' which will be handled by strat1 below
##15: 'strat' which will be handled by strat1-strat3 below
##17: 'treat' handled by arm0-arm3 below
##18: 'offtrt' indicator of off-treatment before 96 weeks
##20: 'cd420' CD4 T cell count at 20 weeks
##21: 'cd496' CD4 T cell count at 96 weeks
##22: 'r' missing CD4 T cell count at 96 weeks
##24: 'cd820' CD8 T cell count at 20 weeks
##25: 'cens' indicator of observing the event in days
##26: 'days' number of days until the primary endpoint
##27: 'arms' handled by arm0-arm3 below
train &lt;- as.matrix(ACTG175)[!ex, -c(1, 14:15, 17, 18, 20:22, 24:27)]
train &lt;- cbind(1*(ACTG175$strat[!ex]==1), 1*(ACTG175$strat[!ex]==2),
               1*(ACTG175$strat[!ex]==3), train)
dimnames(train)[[2]][1:3] &lt;- paste0('strat', 1:3)
train &lt;- cbind(1*(ACTG175$arms[!ex]==0), 1*(ACTG175$arms[!ex]==1),
               1*(ACTG175$arms[!ex]==2), 1*(ACTG175$arms[!ex]==3), train)
dimnames(train)[[2]][1:4] &lt;- paste0('arm', 0:3)

N &lt;- nrow(train)

test0 &lt;- train; test0[ , 1:4] &lt;- 0; test0[ , 1] &lt;- 1
test1 &lt;- train; test1[ , 1:4] &lt;- 0; test1[ , 2] &lt;- 1
test2 &lt;- train; test2[ , 1:4] &lt;- 0; test2[ , 3] &lt;- 1
test3 &lt;- train; test3[ , 1:4] &lt;- 0; test3[ , 4] &lt;- 1

test &lt;- rbind(test0, test1, test2, test3)

##test BART with token run to ensure installation works
set.seed(21)
post &lt;- pbart(train, y, test, nskip=5, ndpost=5)

## Not run: 
set.seed(21)
post &lt;- pbart(train, y, test)

## turn z-scores into probabilities
post$prob.test &lt;- pnorm(post$yhat.test)

## average over the posterior samples
post$prob.test.mean &lt;- apply(post$prob.test, 2, mean)

## place estimates for arms 0-3 next to each other for convenience
itr &lt;- cbind(post$prob.test.mean[(1:N)], post$prob.test.mean[N+(1:N)],
             post$prob.test.mean[2*N+(1:N)], post$prob.test.mean[3*N+(1:N)])

## find the BART ITR for each patient
itr.pick &lt;- integer(N)
for(i in 1:N) itr.pick[i] &lt;- which(itr[i, ]==max(itr[i, ]))-1

## arms 0 and 3 (monotherapy) are never chosen
table(itr.pick)

## do arms 1 and 2 show treatment heterogeneity?
diff. &lt;- apply(post$prob.test[ , 2*N+(1:N)]-post$prob.test[ , N+(1:N)], 2, mean)
plot(sort(diff.), type='h', main='ACTG175 trial: 50% CD4 decline from baseline at 96 weeks',
     xlab='Arm 2 (1) Preferable to the Right (Left)', ylab='Prob.Diff.: Arms 2 - 1')

library(rpart)
library(rpart.plot)

## make data frame for nicer names in the plot
var &lt;- as.data.frame(train[ , -(1:4)])

dss &lt;- rpart(diff. ~ var$age+var$gender+var$race+var$wtkg+var$cd40+var$cd80+
                   var$karnof+var$symptom+var$hemo+var$homo+var$drugs+var$z30+
                   var$zprior+var$oprior+var$strat1+var$strat2+var$strat3,
               method='anova', control=rpart.control(cp=0.1))
rpart.plot(dss, type=3, extra=101)

## if strat1==1 (antiretroviral naive), then arm 2 is better
## otherwise, arm 1
print(dss)

all0 &lt;- apply(post$prob.test[ , (1:N)], 1, mean)
all1 &lt;- apply(post$prob.test[ , N+(1:N)], 1, mean)
all2 &lt;- apply(post$prob.test[ , 2*N+(1:N)], 1, mean)
all3 &lt;- apply(post$prob.test[ , 3*N+(1:N)], 1, mean)

## BART ITR
BART.itr &lt;- apply(post$prob.test[ , c(N+which(itr.pick==1), 2*N+which(itr.pick==2))], 1, mean)

test &lt;- train
test[ , 1:4] &lt;- 0
test[test[ , 5]==0, 2] &lt;- 1
test[test[ , 5]==1, 3] &lt;- 1

## BART ITR simple
BART.itr.simp &lt;- pwbart(test, post$treedraws)
BART.itr.simp &lt;- apply(pnorm(BART.itr.simp), 1, mean)

plot(density(BART.itr), xlab='Value', xlim=c(0.475, 0.775), lwd=2,
     main='ACTG175 trial: 50% CD4 decline from baseline at 96 weeks')
lines(density(BART.itr.simp), col='brown', lwd=2)
lines(density(all0), col='green', lwd=2)
lines(density(all1), col='red', lwd=2)
lines(density(all2), col='blue', lwd=2)
lines(density(all3), col='yellow', lwd=2)
legend('topleft', legend=c('All Arm 0 (ZDV only)', 'All Arm 1 (ZDV+DDI)',
                           'All Arm 2 (ZDV+DDC)', 'All Arm 3 (DDI only)',
                           'BART ITR simple', 'BART ITR'),
       col=c('green', 'red', 'blue', 'yellow', 'brown', 'black'), lty=1, lwd=2)


## End(Not run)
</code></pre>

<hr>
<h2 id='predict.crisk2bart'>Predicting new observations with a previously fitted BART model</h2><span id='topic+predict.crisk2bart'></span>

<h3>Description</h3>

<p>BART is a Bayesian &ldquo;sum-of-trees&rdquo; model.<br />
For a numeric response <code class="reqn">y</code>, we have
<code class="reqn">y = f(x) + \epsilon</code>,
where <code class="reqn">\epsilon \sim N(0,\sigma^2)</code>.<br />
</p>
<p><code class="reqn">f</code> is the sum of many tree models.
The goal is to have very flexible inference for the uknown
function <code class="reqn">f</code>.
</p>
<p>In the spirit of &ldquo;ensemble models&rdquo;,
each tree is constrained by a prior to be a weak learner
so that it contributes a
small amount to the overall fit.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'crisk2bart'
predict(object, newdata, newdata2, mc.cores=1, openmp=(mc.cores.openmp()&gt;0), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.crisk2bart_+3A_object">object</code></td>
<td>

<p><code>object</code> returned from previous BART fit with <code>crisk2.bart</code>
or <code>mc.crisk2.bart</code>.
</p>
</td></tr>
<tr><td><code id="predict.crisk2bart_+3A_newdata">newdata</code></td>
<td>

<p>Matrix of covariates to predict the distribution of <code class="reqn">t1</code>.
</p>
</td></tr>
<tr><td><code id="predict.crisk2bart_+3A_newdata2">newdata2</code></td>
<td>

<p>Matrix of covariates to predict the distribution of <code class="reqn">t2</code>.
</p>
</td></tr>
<tr><td><code id="predict.crisk2bart_+3A_mc.cores">mc.cores</code></td>
<td>

<p>Number of threads to utilize.
</p>
</td></tr>
<tr><td><code id="predict.crisk2bart_+3A_openmp">openmp</code></td>
<td>

<p>Logical value dictating whether OpenMP is utilized for parallel
processing.  Of course, this depends on whether OpenMP is available
on your system which, by default, is verified with <code>mc.cores.openmp</code>.
</p>
</td></tr>
<tr><td><code id="predict.crisk2bart_+3A_...">...</code></td>
<td>

<p>Other arguments which will be passed on to <code>pwbart</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>BART is an Bayesian MCMC method.
At each MCMC interation, we produce a draw from the joint posterior
<code class="reqn">(f,\sigma) | (x,y)</code> in the numeric <code class="reqn">y</code> case
and just <code class="reqn">f</code> in the binary <code class="reqn">y</code> case.
</p>
<p>Thus, unlike a lot of other modelling methods in R, we do not produce a single model object
from which fits and summaries may be extracted.  The output consists of values
<code class="reqn">f^*(x)</code> (and <code class="reqn">\sigma^*</code> in the numeric case) where * denotes a particular draw.
The <code class="reqn">x</code> is either a row from the training data (x.train) or the test data (x.test).
</p>


<h3>Value</h3>

<p>Returns an object of type <code>crisk2bart</code> with predictions
corresponding to <code>newdata</code> and <code>newdata2</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+crisk2.bart">crisk2.bart</a></code>, <code><a href="#topic+mc.crisk2.bart">mc.crisk2.bart</a></code>, <code><a href="#topic+mc.crisk2.pwbart">mc.crisk2.pwbart</a></code>, <code><a href="#topic+mc.cores.openmp">mc.cores.openmp</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(transplant)

delta &lt;- (as.numeric(transplant$event)-1)
## recode so that delta=1 is cause of interest; delta=2 otherwise
delta[delta==1] &lt;- 4
delta[delta==2] &lt;- 1
delta[delta&gt;1] &lt;- 2
table(delta, transplant$event)

times &lt;- pmax(1, ceiling(transplant$futime/7)) ## weeks
##times &lt;- pmax(1, ceiling(transplant$futime/30.5)) ## months
table(times)

typeO &lt;- 1*(transplant$abo=='O')
typeA &lt;- 1*(transplant$abo=='A')
typeB &lt;- 1*(transplant$abo=='B')
typeAB &lt;- 1*(transplant$abo=='AB')
table(typeA, typeO)

x.train &lt;- cbind(typeO, typeA, typeB, typeAB)

x.test &lt;- cbind(1, 0, 0, 0)
dimnames(x.test)[[2]] &lt;- dimnames(x.train)[[2]]

## parallel::mcparallel/mccollect do not exist on windows
if(.Platform$OS.type=='unix') {
##test BART with token run to ensure installation works
        post &lt;- mc.crisk2.bart(x.train=x.train, times=times, delta=delta,
                               seed=99, mc.cores=2, nskip=5, ndpost=5,
                               keepevery=1)

        pre &lt;- surv.pre.bart(x.train=x.train, x.test=x.test,
                             times=times, delta=delta)

        K &lt;- post$K

        pred &lt;- mc.crisk2.pwbart(pre$tx.test, pre$tx.test,
                                post$treedraws, post$treedraws2,
                                post$binaryOffset, post$binaryOffset2)
}

## Not run: 

## run one long MCMC chain in one process
## set.seed(99)
## post &lt;- crisk2.bart(x.train=x.train, times=times, delta=delta, x.test=x.test)

## in the interest of time, consider speeding it up by parallel processing
## run "mc.cores" number of shorter MCMC chains in parallel processes
post &lt;- mc.crisk2.bart(x.train=x.train,
                       times=times, delta=delta,
                       x.test=x.test, seed=99, mc.cores=8)

## check &lt;- mc.crisk2.pwbart(post$tx.test, post$tx.test,
##                           post$treedraws, post$treedraws2,
##                           post$binaryOffset,
##                           post$binaryOffset2, mc.cores=8)
check &lt;- predict(post, newdata=post$tx.test, newdata2=post$tx.test2,
                 mc.cores=8)

print(c(post$surv.test.mean[1], check$surv.test.mean[1],
        post$surv.test.mean[1]-check$surv.test.mean[1]), digits=22)

print(all(round(post$surv.test.mean, digits=9)==
    round(check$surv.test.mean, digits=9)))

print(c(post$cif.test.mean[1], check$cif.test.mean[1],
        post$cif.test.mean[1]-check$cif.test.mean[1]), digits=22)

print(all(round(post$cif.test.mean, digits=9)==
    round(check$cif.test.mean, digits=9)))

print(c(post$cif.test2.mean[1], check$cif.test2.mean[1],
        post$cif.test2.mean[1]-check$cif.test2.mean[1]), digits=22)

print(all(round(post$cif.test2.mean, digits=9)==
    round(check$cif.test2.mean, digits=9)))


## End(Not run)
</code></pre>

<hr>
<h2 id='predict.criskbart'>Predicting new observations with a previously fitted BART model</h2><span id='topic+predict.criskbart'></span>

<h3>Description</h3>

<p>BART is a Bayesian &ldquo;sum-of-trees&rdquo; model.<br />
For a numeric response <code class="reqn">y</code>, we have
<code class="reqn">y = f(x) + \epsilon</code>,
where <code class="reqn">\epsilon \sim N(0,\sigma^2)</code>.<br />
</p>
<p><code class="reqn">f</code> is the sum of many tree models.
The goal is to have very flexible inference for the uknown
function <code class="reqn">f</code>.
</p>
<p>In the spirit of &ldquo;ensemble models&rdquo;,
each tree is constrained by a prior to be a weak learner
so that it contributes a
small amount to the overall fit.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'criskbart'
predict(object, newdata, newdata2, mc.cores=1, openmp=(mc.cores.openmp()&gt;0), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.criskbart_+3A_object">object</code></td>
<td>

<p><code>object</code> returned from previous BART fit with <code>crisk.bart</code>
or <code>mc.crisk.bart</code>.
</p>
</td></tr>
<tr><td><code id="predict.criskbart_+3A_newdata">newdata</code></td>
<td>

<p>Matrix of covariates to predict the distribution of <code class="reqn">t1</code>.
</p>
</td></tr>
<tr><td><code id="predict.criskbart_+3A_newdata2">newdata2</code></td>
<td>

<p>Matrix of covariates to predict the distribution of <code class="reqn">t2</code>.
</p>
</td></tr>
<tr><td><code id="predict.criskbart_+3A_mc.cores">mc.cores</code></td>
<td>

<p>Number of threads to utilize.
</p>
</td></tr>
<tr><td><code id="predict.criskbart_+3A_openmp">openmp</code></td>
<td>

<p>Logical value dictating whether OpenMP is utilized for parallel
processing.  Of course, this depends on whether OpenMP is available
on your system which, by default, is verified with <code>mc.cores.openmp</code>.
</p>
</td></tr>
<tr><td><code id="predict.criskbart_+3A_...">...</code></td>
<td>

<p>Other arguments which will be passed on to <code>pwbart</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>BART is an Bayesian MCMC method.
At each MCMC interation, we produce a draw from the joint posterior
<code class="reqn">(f,\sigma) | (x,y)</code> in the numeric <code class="reqn">y</code> case
and just <code class="reqn">f</code> in the binary <code class="reqn">y</code> case.
</p>
<p>Thus, unlike a lot of other modelling methods in R, we do not produce a single model object
from which fits and summaries may be extracted.  The output consists of values
<code class="reqn">f^*(x)</code> (and <code class="reqn">\sigma^*</code> in the numeric case) where * denotes a particular draw.
The <code class="reqn">x</code> is either a row from the training data (x.train) or the test data (x.test).
</p>


<h3>Value</h3>

<p>Returns an object of type <code>criskbart</code> with predictions
corresponding to <code>newdata</code> and <code>newdata2</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+crisk.bart">crisk.bart</a></code>, <code><a href="#topic+mc.crisk.bart">mc.crisk.bart</a></code>, <code><a href="#topic+mc.crisk.pwbart">mc.crisk.pwbart</a></code>, <code><a href="#topic+mc.cores.openmp">mc.cores.openmp</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(transplant)

delta &lt;- (as.numeric(transplant$event)-1)
## recode so that delta=1 is cause of interest; delta=2 otherwise
delta[delta==1] &lt;- 4
delta[delta==2] &lt;- 1
delta[delta&gt;1] &lt;- 2
table(delta, transplant$event)

times &lt;- pmax(1, ceiling(transplant$futime/7)) ## weeks
##times &lt;- pmax(1, ceiling(transplant$futime/30.5)) ## months
table(times)

typeO &lt;- 1*(transplant$abo=='O')
typeA &lt;- 1*(transplant$abo=='A')
typeB &lt;- 1*(transplant$abo=='B')
typeAB &lt;- 1*(transplant$abo=='AB')
table(typeA, typeO)

x.train &lt;- cbind(typeO, typeA, typeB, typeAB)

x.test &lt;- cbind(1, 0, 0, 0)
dimnames(x.test)[[2]] &lt;- dimnames(x.train)[[2]]

## parallel::mcparallel/mccollect do not exist on windows
if(.Platform$OS.type=='unix') {
##test BART with token run to ensure installation works
        post &lt;- mc.crisk.bart(x.train=x.train, times=times, delta=delta,
                               seed=99, mc.cores=2, nskip=5, ndpost=5,
                               keepevery=1)

        pre &lt;- surv.pre.bart(x.train=x.train, x.test=x.test,
                             times=times, delta=delta)

        K &lt;- post$K

        pred &lt;- mc.crisk.pwbart(pre$tx.test, pre$tx.test,
                                post$treedraws, post$treedraws2,
                                post$binaryOffset, post$binaryOffset2)
}

## Not run: 

## run one long MCMC chain in one process
## set.seed(99)
## post &lt;- crisk.bart(x.train=x.train, times=times, delta=delta, x.test=x.test)

## in the interest of time, consider speeding it up by parallel processing
## run "mc.cores" number of shorter MCMC chains in parallel processes
post &lt;- mc.crisk.bart(x.train=x.train,
                       times=times, delta=delta,
                       x.test=x.test, seed=99, mc.cores=8)

## check &lt;- mc.crisk.pwbart(post$tx.test, post$tx.test,
##                           post$treedraws, post$treedraws2,
##                           post$binaryOffset,
##                           post$binaryOffset2, mc.cores=8)
check &lt;- predict(post, newdata=post$tx.test, newdata2=post$tx.test2,
                 mc.cores=8)

print(c(post$surv.test.mean[1], check$surv.test.mean[1],
        post$surv.test.mean[1]-check$surv.test.mean[1]), digits=22)

print(all(round(post$surv.test.mean, digits=9)==
    round(check$surv.test.mean, digits=9)))

print(c(post$cif.test.mean[1], check$cif.test.mean[1],
        post$cif.test.mean[1]-check$cif.test.mean[1]), digits=22)

print(all(round(post$cif.test.mean, digits=9)==
    round(check$cif.test.mean, digits=9)))

print(c(post$cif.test2.mean[1], check$cif.test2.mean[1],
        post$cif.test2.mean[1]-check$cif.test2.mean[1]), digits=22)

print(all(round(post$cif.test2.mean, digits=9)==
    round(check$cif.test2.mean, digits=9)))


## End(Not run)
</code></pre>

<hr>
<h2 id='predict.lbart'>Predicting new observations with a previously fitted BART model</h2><span id='topic+predict.lbart'></span>

<h3>Description</h3>

<p>BART is a Bayesian &ldquo;sum-of-trees&rdquo; model.<br />
For a numeric response <code class="reqn">y</code>, we have
<code class="reqn">y = f(x) + \epsilon</code>,
where <code class="reqn">\epsilon \sim N(0,\sigma^2)</code>.<br />
</p>
<p><code class="reqn">f</code> is the sum of many tree models.
The goal is to have very flexible inference for the uknown
function <code class="reqn">f</code>.
</p>
<p>In the spirit of &ldquo;ensemble models&rdquo;,
each tree is constrained by a prior to be a weak learner
so that it contributes a
small amount to the overall fit.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'lbart'
predict(object, newdata, mc.cores=1, openmp=(mc.cores.openmp()&gt;0), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.lbart_+3A_object">object</code></td>
<td>

<p><code>object</code> returned from previous BART fit with <code>surv.bart</code>
or <code>mc.surv.bart</code>.
</p>
</td></tr>
<tr><td><code id="predict.lbart_+3A_newdata">newdata</code></td>
<td>

<p>Matrix of covariates to predict the distribution of <code class="reqn">t</code>.
</p>
</td></tr>
<tr><td><code id="predict.lbart_+3A_mc.cores">mc.cores</code></td>
<td>

<p>Number of threads to utilize.
</p>
</td></tr>
<tr><td><code id="predict.lbart_+3A_openmp">openmp</code></td>
<td>

<p>Logical value dictating whether OpenMP is utilized for parallel
processing.  Of course, this depends on whether OpenMP is available
on your system which, by default, is verified with <code>mc.cores.openmp</code>.
</p>
</td></tr>
<tr><td><code id="predict.lbart_+3A_...">...</code></td>
<td>

<p>Other arguments which will be passed on to <code>pwbart</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>BART is an Bayesian MCMC method.
At each MCMC interation, we produce a draw from the joint posterior
<code class="reqn">(f,\sigma) | (x,y)</code> in the numeric <code class="reqn">y</code> case
and just <code class="reqn">f</code> in the binary <code class="reqn">y</code> case.
</p>
<p>Thus, unlike a lot of other modelling methods in R, we do not produce a single model object
from which fits and summaries may be extracted.  The output consists of values
<code class="reqn">f^*(x)</code> (and <code class="reqn">\sigma^*</code> in the numeric case) where * denotes a particular draw.
The <code class="reqn">x</code> is either a row from the training data (x.train) or the test data (x.test).
</p>


<h3>Value</h3>

<p>Returns an object of type <code>lbart</code> with predictions corresponding to <code>newdata</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+surv.bart">surv.bart</a></code>, <code><a href="#topic+mc.surv.bart">mc.surv.bart</a></code>, <code><a href="#topic+surv.pwbart">surv.pwbart</a></code>, <code><a href="#topic+mc.surv.pwbart">mc.surv.pwbart</a></code>, <code><a href="#topic+mc.cores.openmp">mc.cores.openmp</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## load the advanced lung cancer example
data(lung)

group &lt;- -which(is.na(lung[ , 7])) ## remove missing row for ph.karno
times &lt;- lung[group, 2]   ##lung$time
delta &lt;- lung[group, 3]-1 ##lung$status: 1=censored, 2=dead
                          ##delta: 0=censored, 1=dead

## this study reports time in days rather than months like other studies
## coarsening from days to months will reduce the computational burden
times &lt;- ceiling(times/30)

summary(times)
table(delta)

x.train &lt;- as.matrix(lung[group, c(4, 5, 7)]) ## matrix of observed covariates

## lung$age:        Age in years
## lung$sex:        Male=1 Female=2
## lung$ph.karno:   Karnofsky performance score (dead=0:normal=100:by=10)
##                  rated by physician

dimnames(x.train)[[2]] &lt;- c('age(yr)', 'M(1):F(2)', 'ph.karno(0:100:10)')

summary(x.train[ , 1])
table(x.train[ , 2])
table(x.train[ , 3])

x.test &lt;- matrix(nrow=84, ncol=3) ## matrix of covariate scenarios

dimnames(x.test)[[2]] &lt;- dimnames(x.train)[[2]]

i &lt;- 1

for(age in 5*(9:15)) for(sex in 1:2) for(ph.karno in 10*(5:10)) {
    x.test[i, ] &lt;- c(age, sex, ph.karno)
    i &lt;- i+1
}

## this x.test is relatively small, but often you will want to
## predict for a large x.test matrix which may cause problems
## due to consumption of RAM so we can predict separately

## mcparallel/mccollect do not exist on windows
if(.Platform$OS.type=='unix') {
##test BART with token run to ensure installation works
    set.seed(99)
    post &lt;- surv.bart(x.train=x.train, times=times, delta=delta, nskip=5, ndpost=5, keepevery=1)

    pre &lt;- surv.pre.bart(x.train=x.train, times=times, delta=delta, x.test=x.test)

    pred &lt;- predict(post, pre$tx.test)
    ##pred. &lt;- surv.pwbart(pre$tx.test, post$treedraws, post$binaryOffset)
}

## Not run: 
## run one long MCMC chain in one process
set.seed(99)
post &lt;- surv.bart(x.train=x.train, times=times, delta=delta)

## run "mc.cores" number of shorter MCMC chains in parallel processes
## post &lt;- mc.surv.bart(x.train=x.train, times=times, delta=delta,
##                      mc.cores=5, seed=99)

pre &lt;- surv.pre.bart(x.train=x.train, times=times, delta=delta, x.test=x.test)

pred &lt;- predict(post, pre$tx.test)

## let's look at some survival curves
## first, a younger group with a healthier KPS
## age 50 with KPS=90: males and females
## males: row 17, females: row 23
x.test[c(17, 23), ]

low.risk.males &lt;- 16*post$K+1:post$K ## K=unique times including censoring
low.risk.females &lt;- 22*post$K+1:post$K

plot(post$times, pred$surv.test.mean[low.risk.males], type='s', col='blue',
     main='Age 50 with KPS=90', xlab='t', ylab='S(t)', ylim=c(0, 1))
points(post$times, pred$surv.test.mean[low.risk.females], type='s', col='red')


## End(Not run)
</code></pre>

<hr>
<h2 id='predict.mbart'>Predicting new observations with a previously fitted BART model</h2><span id='topic+predict.mbart'></span><span id='topic+predict.mbart2'></span>

<h3>Description</h3>

<p>BART is a Bayesian &ldquo;sum-of-trees&rdquo; model.<br />
For a numeric response <code class="reqn">y</code>, we have
<code class="reqn">y = f(x) + \epsilon</code>,
where <code class="reqn">\epsilon \sim N(0,\sigma^2)</code>.<br />
</p>
<p><code class="reqn">f</code> is the sum of many tree models.
The goal is to have very flexible inference for the uknown
function <code class="reqn">f</code>.
</p>
<p>In the spirit of &ldquo;ensemble models&rdquo;,
each tree is constrained by a prior to be a weak learner
so that it contributes a
small amount to the overall fit.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'mbart'
predict(object, newdata, mc.cores=1, openmp=(mc.cores.openmp()&gt;0), ...)
## S3 method for class 'mbart2'
predict(object, newdata, mc.cores=1, openmp=(mc.cores.openmp()&gt;0), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.mbart_+3A_object">object</code></td>
<td>

<p><code>object</code> returned from previous BART fit with <code>mbart</code>
or <code>mbart2</code>.
</p>
</td></tr>
<tr><td><code id="predict.mbart_+3A_newdata">newdata</code></td>
<td>

<p>Matrix of covariates to predict the distribution of <code class="reqn">t</code>.
</p>
</td></tr>
<tr><td><code id="predict.mbart_+3A_mc.cores">mc.cores</code></td>
<td>

<p>Number of threads to utilize.
</p>
</td></tr>
<tr><td><code id="predict.mbart_+3A_openmp">openmp</code></td>
<td>

<p>Logical value dictating whether OpenMP is utilized for parallel
processing.  Of course, this depends on whether OpenMP is available
on your system which, by default, is verified with <code>mc.cores.openmp</code>.
</p>
</td></tr>
<tr><td><code id="predict.mbart_+3A_...">...</code></td>
<td>

<p>Other arguments which will be passed on to <code>pwbart</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>BART is an Bayesian MCMC method.
At each MCMC interation, we produce a draw from the joint posterior
<code class="reqn">(f,\sigma) | (x,y)</code> in the numeric <code class="reqn">y</code> case
and just <code class="reqn">f</code> in the binary <code class="reqn">y</code> case.
</p>
<p>Thus, unlike a lot of other modelling methods in R, we do not produce a single model object
from which fits and summaries may be extracted.  The output consists of values
<code class="reqn">f^*(x)</code> (and <code class="reqn">\sigma^*</code> in the numeric case) where * denotes a particular draw.
The <code class="reqn">x</code> is either a row from the training data (x.train) or the test data (x.test).
</p>


<h3>Value</h3>

<p>Returns an object of type <code>mbart</code> with predictions corresponding to <code>newdata</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mbart">mbart</a></code>, <code><a href="#topic+mbart2">mbart2</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## load the advanced lung cancer example
data(lung)

group &lt;- -which(is.na(lung[ , 7])) ## remove missing row for ph.karno
times &lt;- lung[group, 2]   ##lung$time
delta &lt;- lung[group, 3]-1 ##lung$status: 1=censored, 2=dead
                          ##delta: 0=censored, 1=dead

## this study reports time in days rather than months like other studies
## coarsening from days to months will reduce the computational burden
times &lt;- ceiling(times/30)

summary(times)
table(delta)

x.train &lt;- as.matrix(lung[group, c(4, 5, 7)]) ## matrix of observed covariates

## lung$age:        Age in years
## lung$sex:        Male=1 Female=2
## lung$ph.karno:   Karnofsky performance score (dead=0:normal=100:by=10)
##                  rated by physician

dimnames(x.train)[[2]] &lt;- c('age(yr)', 'M(1):F(2)', 'ph.karno(0:100:10)')

summary(x.train[ , 1])
table(x.train[ , 2])
table(x.train[ , 3])

x.test &lt;- matrix(nrow=84, ncol=3) ## matrix of covariate scenarios

dimnames(x.test)[[2]] &lt;- dimnames(x.train)[[2]]

i &lt;- 1

for(age in 5*(9:15)) for(sex in 1:2) for(ph.karno in 10*(5:10)) {
    x.test[i, ] &lt;- c(age, sex, ph.karno)
    i &lt;- i+1
}

## this x.test is relatively small, but often you will want to
## predict for a large x.test matrix which may cause problems
## due to consumption of RAM so we can predict separately

## mcparallel/mccollect do not exist on windows
if(.Platform$OS.type=='unix') {
##test BART with token run to ensure installation works
    set.seed(99)
    post &lt;- surv.bart(x.train=x.train, times=times, delta=delta, nskip=5, ndpost=5, keepevery=1)

    pre &lt;- surv.pre.bart(x.train=x.train, times=times, delta=delta, x.test=x.test)

    pred &lt;- predict(post, pre$tx.test)
    ##pred. &lt;- surv.pwbart(pre$tx.test, post$treedraws, post$binaryOffset)
}

## Not run: 
## run one long MCMC chain in one process
set.seed(99)
post &lt;- surv.bart(x.train=x.train, times=times, delta=delta)

## run "mc.cores" number of shorter MCMC chains in parallel processes
## post &lt;- mc.surv.bart(x.train=x.train, times=times, delta=delta,
##                      mc.cores=5, seed=99)

pre &lt;- surv.pre.bart(x.train=x.train, times=times, delta=delta, x.test=x.test)

pred &lt;- predict(post, pre$tx.test)

## let's look at some survival curves
## first, a younger group with a healthier KPS
## age 50 with KPS=90: males and females
## males: row 17, females: row 23
x.test[c(17, 23), ]

low.risk.males &lt;- 16*post$K+1:post$K ## K=unique times including censoring
low.risk.females &lt;- 22*post$K+1:post$K

plot(post$times, pred$surv.test.mean[low.risk.males], type='s', col='blue',
     main='Age 50 with KPS=90', xlab='t', ylab='S(t)', ylim=c(0, 1))
points(post$times, pred$surv.test.mean[low.risk.females], type='s', col='red')


## End(Not run)
</code></pre>

<hr>
<h2 id='predict.pbart'>Predicting new observations with a previously fitted BART model</h2><span id='topic+predict.pbart'></span>

<h3>Description</h3>

<p>BART is a Bayesian &ldquo;sum-of-trees&rdquo; model.<br />
For a numeric response <code class="reqn">y</code>, we have
<code class="reqn">y = f(x) + \epsilon</code>,
where <code class="reqn">\epsilon \sim N(0,\sigma^2)</code>.<br />
</p>
<p><code class="reqn">f</code> is the sum of many tree models.
The goal is to have very flexible inference for the uknown
function <code class="reqn">f</code>.
</p>
<p>In the spirit of &ldquo;ensemble models&rdquo;,
each tree is constrained by a prior to be a weak learner
so that it contributes a
small amount to the overall fit.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'pbart'
predict(object, newdata, mc.cores=1, openmp=(mc.cores.openmp()&gt;0), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.pbart_+3A_object">object</code></td>
<td>

<p><code>object</code> returned from previous BART fit with <code>surv.bart</code>
or <code>mc.surv.bart</code>.
</p>
</td></tr>
<tr><td><code id="predict.pbart_+3A_newdata">newdata</code></td>
<td>

<p>Matrix of covariates to predict the distribution of <code class="reqn">t</code>.
</p>
</td></tr>
<tr><td><code id="predict.pbart_+3A_mc.cores">mc.cores</code></td>
<td>

<p>Number of threads to utilize.
</p>
</td></tr>
<tr><td><code id="predict.pbart_+3A_openmp">openmp</code></td>
<td>

<p>Logical value dictating whether OpenMP is utilized for parallel
processing.  Of course, this depends on whether OpenMP is available
on your system which, by default, is verified with <code>mc.cores.openmp</code>.
</p>
</td></tr>
<tr><td><code id="predict.pbart_+3A_...">...</code></td>
<td>

<p>Other arguments which will be passed on to <code>pwbart</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>BART is an Bayesian MCMC method.
At each MCMC interation, we produce a draw from the joint posterior
<code class="reqn">(f,\sigma) | (x,y)</code> in the numeric <code class="reqn">y</code> case
and just <code class="reqn">f</code> in the binary <code class="reqn">y</code> case.
</p>
<p>Thus, unlike a lot of other modelling methods in R, we do not produce a single model object
from which fits and summaries may be extracted.  The output consists of values
<code class="reqn">f^*(x)</code> (and <code class="reqn">\sigma^*</code> in the numeric case) where * denotes a particular draw.
The <code class="reqn">x</code> is either a row from the training data (x.train) or the test data (x.test).
</p>


<h3>Value</h3>

<p>Returns an object of type <code>pbart</code> with predictions corresponding to <code>newdata</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+surv.bart">surv.bart</a></code>, <code><a href="#topic+mc.surv.bart">mc.surv.bart</a></code>, <code><a href="#topic+surv.pwbart">surv.pwbart</a></code>, <code><a href="#topic+mc.surv.pwbart">mc.surv.pwbart</a></code>, <code><a href="#topic+mc.cores.openmp">mc.cores.openmp</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## load the advanced lung cancer example
data(lung)

group &lt;- -which(is.na(lung[ , 7])) ## remove missing row for ph.karno
times &lt;- lung[group, 2]   ##lung$time
delta &lt;- lung[group, 3]-1 ##lung$status: 1=censored, 2=dead
                          ##delta: 0=censored, 1=dead

## this study reports time in days rather than months like other studies
## coarsening from days to months will reduce the computational burden
times &lt;- ceiling(times/30)

summary(times)
table(delta)

x.train &lt;- as.matrix(lung[group, c(4, 5, 7)]) ## matrix of observed covariates

## lung$age:        Age in years
## lung$sex:        Male=1 Female=2
## lung$ph.karno:   Karnofsky performance score (dead=0:normal=100:by=10)
##                  rated by physician

dimnames(x.train)[[2]] &lt;- c('age(yr)', 'M(1):F(2)', 'ph.karno(0:100:10)')

summary(x.train[ , 1])
table(x.train[ , 2])
table(x.train[ , 3])

x.test &lt;- matrix(nrow=84, ncol=3) ## matrix of covariate scenarios

dimnames(x.test)[[2]] &lt;- dimnames(x.train)[[2]]

i &lt;- 1

for(age in 5*(9:15)) for(sex in 1:2) for(ph.karno in 10*(5:10)) {
    x.test[i, ] &lt;- c(age, sex, ph.karno)
    i &lt;- i+1
}

## this x.test is relatively small, but often you will want to
## predict for a large x.test matrix which may cause problems
## due to consumption of RAM so we can predict separately

## mcparallel/mccollect do not exist on windows
if(.Platform$OS.type=='unix') {
##test BART with token run to ensure installation works
    set.seed(99)
    post &lt;- surv.bart(x.train=x.train, times=times, delta=delta, nskip=5, ndpost=5, keepevery=1)

    pre &lt;- surv.pre.bart(x.train=x.train, times=times, delta=delta, x.test=x.test)

    pred &lt;- predict(post, pre$tx.test)
    ##pred. &lt;- surv.pwbart(pre$tx.test, post$treedraws, post$binaryOffset)
}

## Not run: 
## run one long MCMC chain in one process
set.seed(99)
post &lt;- surv.bart(x.train=x.train, times=times, delta=delta)

## run "mc.cores" number of shorter MCMC chains in parallel processes
## post &lt;- mc.surv.bart(x.train=x.train, times=times, delta=delta,
##                      mc.cores=5, seed=99)

pre &lt;- surv.pre.bart(x.train=x.train, times=times, delta=delta, x.test=x.test)

pred &lt;- predict(post, pre$tx.test)

## let's look at some survival curves
## first, a younger group with a healthier KPS
## age 50 with KPS=90: males and females
## males: row 17, females: row 23
x.test[c(17, 23), ]

low.risk.males &lt;- 16*post$K+1:post$K ## K=unique times including censoring
low.risk.females &lt;- 22*post$K+1:post$K

plot(post$times, pred$surv.test.mean[low.risk.males], type='s', col='blue',
     main='Age 50 with KPS=90', xlab='t', ylab='S(t)', ylim=c(0, 1))
points(post$times, pred$surv.test.mean[low.risk.females], type='s', col='red')


## End(Not run)
</code></pre>

<hr>
<h2 id='predict.recurbart'>Predicting new observations with a previously fitted BART model</h2><span id='topic+predict.recurbart'></span>

<h3>Description</h3>

<p>BART is a Bayesian &ldquo;sum-of-trees&rdquo; model.<br />
For a numeric response <code class="reqn">y</code>, we have
<code class="reqn">y = f(x) + \epsilon</code>,
where <code class="reqn">\epsilon \sim N(0,\sigma^2)</code>.<br />
</p>
<p><code class="reqn">f</code> is the sum of many tree models.
The goal is to have very flexible inference for the uknown
function <code class="reqn">f</code>.
</p>
<p>In the spirit of &ldquo;ensemble models&rdquo;,
each tree is constrained by a prior to be a weak learner
so that it contributes a
small amount to the overall fit.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'recurbart'
predict(object, newdata, mc.cores=1, openmp=(mc.cores.openmp()&gt;0), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.recurbart_+3A_object">object</code></td>
<td>

<p><code>object</code> returned from previous BART fit with <code>recur.bart</code>
or <code>mc.recur.bart</code>.
</p>
</td></tr>
<tr><td><code id="predict.recurbart_+3A_newdata">newdata</code></td>
<td>

<p>Matrix of covariates to predict the distribution of <code class="reqn">t</code>.
</p>
</td></tr>
<tr><td><code id="predict.recurbart_+3A_mc.cores">mc.cores</code></td>
<td>

<p>Number of threads to utilize.
</p>
</td></tr>
<tr><td><code id="predict.recurbart_+3A_openmp">openmp</code></td>
<td>

<p>Logical value dictating whether OpenMP is utilized for parallel
processing.  Of course, this depends on whether OpenMP is available
on your system which, by default, is verified with <code>mc.cores.openmp</code>.
</p>
</td></tr>
<tr><td><code id="predict.recurbart_+3A_...">...</code></td>
<td>

<p>Other arguments which will be passed on to <code>pwbart</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>BART is an Bayesian MCMC method.
At each MCMC interation, we produce a draw from the joint posterior
<code class="reqn">(f,\sigma) | (x,y)</code> in the numeric <code class="reqn">y</code> case
and just <code class="reqn">f</code> in the binary <code class="reqn">y</code> case.
</p>
<p>Thus, unlike a lot of other modelling methods in R, we do not produce a single model object
from which fits and summaries may be extracted.  The output consists of values
<code class="reqn">f^*(x)</code> (and <code class="reqn">\sigma^*</code> in the numeric case) where * denotes a particular draw.
The <code class="reqn">x</code> is either a row from the training data (x.train) or the test data (x.test).
</p>


<h3>Value</h3>

<p>Returns an object of type <code>recurbart</code> with predictions corresponding to <code>newdata</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+recur.bart">recur.bart</a></code>, <code><a href="#topic+mc.recur.bart">mc.recur.bart</a></code>, <code><a href="#topic+recur.pwbart">recur.pwbart</a></code>, <code><a href="#topic+mc.recur.pwbart">mc.recur.pwbart</a></code>, <code><a href="#topic+mc.cores.openmp">mc.cores.openmp</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## load 20 percent random sample
data(xdm20.train)
data(xdm20.test)
data(ydm20.train)

##test BART with token run to ensure installation works
## with current technology even a token run will violate CRAN policy
## set.seed(99)
## post &lt;- recur.bart(x.train=xdm20.train, y.train=ydm20.train,
##                    nskip=1, ndpost=1, keepevery=1)

## Not run: 
set.seed(99)
post &lt;- recur.bart(x.train=xdm20.train, y.train=ydm20.train)
## larger data sets can take some time so, if parallel processing
## is available, submit this statement instead
## post &lt;- mc.recur.bart(x.train=xdm20.train, y.train=ydm20.train,
##                      mc.cores=8, seed=99)

require(rpart)
require(rpart.plot)

dss &lt;- rpart(post$yhat.train.mean~xdm20.train)

rpart.plot(dss)
## for the 20 percent sample, notice that the top splits
## involve cci_pvd and n
## for the full data set, notice that all splits
## involve ca, cci_pud, cci_pvd, ins270 and n
## (except one at the bottom involving a small group)

## compare patients treated with insulin (ins270=1) vs
## not treated with insulin (ins270=0)
N.train &lt;- 50
N.test &lt;- 50
K &lt;- post$K ## 798 unique time points

## only testing set, i.e., remove training set
xdm20.test. &lt;- xdm20.test[N.train*K+(1:(N.test*K)), ]
xdm20.test. &lt;- rbind(xdm20.test., xdm20.test.)
xdm20.test.[ , 'ins270'] &lt;- rep(0:1, each=N.test*K)

## multiple threads will be utilized if available
pred &lt;- predict(post, xdm20.test., mc.cores=8)

## create Friedman's partial dependence function for the
## intensity/hazard by time and ins270
NK.test &lt;- N.test*K
M &lt;- nrow(pred$haz.test) ## number of MCMC samples, typically 1000

RI &lt;- matrix(0, M, K)

for(i in 1:N.test)
    RI &lt;- RI+(pred$haz.test[ , (N.test+i-1)*K+1:K]/
              pred$haz.test[ , (i-1)*K+1:K])/N.test

RI.lo &lt;- apply(RI, 2, quantile, probs=0.025)
RI.mu &lt;- apply(RI, 2, mean)
RI.hi &lt;- apply(RI, 2, quantile, probs=0.975)

plot(post$times, RI.hi, type='l', lty=2, log='y',
     ylim=c(min(RI.lo, 1/RI.hi), max(1/RI.lo, RI.hi)),
     xlab='t', ylab='RI(t, x)',
     sub='insulin(ins270=1) vs. no insulin(ins270=0)',
     main='Relative intensity of hospital admissions for diabetics')
lines(post$times, RI.mu)
lines(post$times, RI.lo, lty=2)
lines(post$times, rep(1, K), col='darkgray')

## RI for insulin therapy seems fairly constant with time
mean(RI.mu)


## End(Not run)
</code></pre>

<hr>
<h2 id='predict.survbart'>Predicting new observations with a previously fitted BART model</h2><span id='topic+predict.survbart'></span>

<h3>Description</h3>

<p>BART is a Bayesian &ldquo;sum-of-trees&rdquo; model.<br />
For a numeric response <code class="reqn">y</code>, we have
<code class="reqn">y = f(x) + \epsilon</code>,
where <code class="reqn">\epsilon \sim N(0,\sigma^2)</code>.<br />
</p>
<p><code class="reqn">f</code> is the sum of many tree models.
The goal is to have very flexible inference for the uknown
function <code class="reqn">f</code>.
</p>
<p>In the spirit of &ldquo;ensemble models&rdquo;,
each tree is constrained by a prior to be a weak learner
so that it contributes a
small amount to the overall fit.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'survbart'
predict(object, newdata, mc.cores=1, openmp=(mc.cores.openmp()&gt;0), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.survbart_+3A_object">object</code></td>
<td>

<p><code>object</code> returned from previous BART fit with <code>surv.bart</code>
or <code>mc.surv.bart</code>.
</p>
</td></tr>
<tr><td><code id="predict.survbart_+3A_newdata">newdata</code></td>
<td>

<p>Matrix of covariates to predict the distribution of <code class="reqn">t</code>.
</p>
</td></tr>
<tr><td><code id="predict.survbart_+3A_mc.cores">mc.cores</code></td>
<td>

<p>Number of threads to utilize.
</p>
</td></tr>
<tr><td><code id="predict.survbart_+3A_openmp">openmp</code></td>
<td>

<p>Logical value dictating whether OpenMP is utilized for parallel
processing.  Of course, this depends on whether OpenMP is available
on your system which, by default, is verified with <code>mc.cores.openmp</code>.
</p>
</td></tr>
<tr><td><code id="predict.survbart_+3A_...">...</code></td>
<td>

<p>Other arguments which will be passed on to <code>pwbart</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>BART is an Bayesian MCMC method.
At each MCMC interation, we produce a draw from the joint posterior
<code class="reqn">(f,\sigma) | (x,y)</code> in the numeric <code class="reqn">y</code> case
and just <code class="reqn">f</code> in the binary <code class="reqn">y</code> case.
</p>
<p>Thus, unlike a lot of other modelling methods in R, we do not produce a single model object
from which fits and summaries may be extracted.  The output consists of values
<code class="reqn">f^*(x)</code> (and <code class="reqn">\sigma^*</code> in the numeric case) where * denotes a particular draw.
The <code class="reqn">x</code> is either a row from the training data (x.train) or the test data (x.test).
</p>


<h3>Value</h3>

<p>Returns an object of type <code>survbart</code> with predictions corresponding to <code>newdata</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+surv.bart">surv.bart</a></code>, <code><a href="#topic+mc.surv.bart">mc.surv.bart</a></code>, <code><a href="#topic+surv.pwbart">surv.pwbart</a></code>, <code><a href="#topic+mc.surv.pwbart">mc.surv.pwbart</a></code>, <code><a href="#topic+mc.cores.openmp">mc.cores.openmp</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## load the advanced lung cancer example
data(lung)

group &lt;- -which(is.na(lung[ , 7])) ## remove missing row for ph.karno
times &lt;- lung[group, 2]   ##lung$time
delta &lt;- lung[group, 3]-1 ##lung$status: 1=censored, 2=dead
                          ##delta: 0=censored, 1=dead

## this study reports time in days rather than months like other studies
## coarsening from days to months will reduce the computational burden
times &lt;- ceiling(times/30)

summary(times)
table(delta)

x.train &lt;- as.matrix(lung[group, c(4, 5, 7)]) ## matrix of observed covariates

## lung$age:        Age in years
## lung$sex:        Male=1 Female=2
## lung$ph.karno:   Karnofsky performance score (dead=0:normal=100:by=10)
##                  rated by physician

dimnames(x.train)[[2]] &lt;- c('age(yr)', 'M(1):F(2)', 'ph.karno(0:100:10)')

summary(x.train[ , 1])
table(x.train[ , 2])
table(x.train[ , 3])

x.test &lt;- matrix(nrow=84, ncol=3) ## matrix of covariate scenarios

dimnames(x.test)[[2]] &lt;- dimnames(x.train)[[2]]

i &lt;- 1

for(age in 5*(9:15)) for(sex in 1:2) for(ph.karno in 10*(5:10)) {
    x.test[i, ] &lt;- c(age, sex, ph.karno)
    i &lt;- i+1
}

## this x.test is relatively small, but often you will want to
## predict for a large x.test matrix which may cause problems
## due to consumption of RAM so we can predict separately

## mcparallel/mccollect do not exist on windows
if(.Platform$OS.type=='unix') {
##test BART with token run to ensure installation works
    set.seed(99)
    post &lt;- surv.bart(x.train=x.train, times=times, delta=delta, nskip=5, ndpost=5, keepevery=1)

    pre &lt;- surv.pre.bart(x.train=x.train, times=times, delta=delta, x.test=x.test)

    pred &lt;- predict(post, pre$tx.test)
    ##pred. &lt;- surv.pwbart(pre$tx.test, post$treedraws, post$binaryOffset)
}

## Not run: 
## run one long MCMC chain in one process
set.seed(99)
post &lt;- surv.bart(x.train=x.train, times=times, delta=delta)

## run "mc.cores" number of shorter MCMC chains in parallel processes
## post &lt;- mc.surv.bart(x.train=x.train, times=times, delta=delta,
##                      mc.cores=5, seed=99)

pre &lt;- surv.pre.bart(x.train=x.train, times=times, delta=delta, x.test=x.test)

pred &lt;- predict(post, pre$tx.test)

## let's look at some survival curves
## first, a younger group with a healthier KPS
## age 50 with KPS=90: males and females
## males: row 17, females: row 23
x.test[c(17, 23), ]

low.risk.males &lt;- 16*post$K+1:post$K ## K=unique times including censoring
low.risk.females &lt;- 22*post$K+1:post$K

plot(post$times, pred$surv.test.mean[low.risk.males], type='s', col='blue',
     main='Age 50 with KPS=90', xlab='t', ylab='S(t)', ylim=c(0, 1))
points(post$times, pred$surv.test.mean[low.risk.females], type='s', col='red')


## End(Not run)
</code></pre>

<hr>
<h2 id='predict.wbart'>Predicting new observations with a previously fitted BART model</h2><span id='topic+predict.wbart'></span>

<h3>Description</h3>

<p>BART is a Bayesian &ldquo;sum-of-trees&rdquo; model.<br />
For a numeric response <code class="reqn">y</code>, we have
<code class="reqn">y = f(x) + \epsilon</code>,
where <code class="reqn">\epsilon \sim N(0,\sigma^2)</code>.<br />
</p>
<p><code class="reqn">f</code> is the sum of many tree models.
The goal is to have very flexible inference for the uknown
function <code class="reqn">f</code>.
</p>
<p>In the spirit of &ldquo;ensemble models&rdquo;,
each tree is constrained by a prior to be a weak learner
so that it contributes a
small amount to the overall fit.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'wbart'
predict(object, newdata, mc.cores=1, openmp=(mc.cores.openmp()&gt;0), ...)



</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.wbart_+3A_object">object</code></td>
<td>

<p><code>object</code> returned from previous BART fit.
</p>
</td></tr>
<tr><td><code id="predict.wbart_+3A_newdata">newdata</code></td>
<td>

<p>Matrix of covariates to predict <code class="reqn">y</code> for.
</p>
</td></tr>
<tr><td><code id="predict.wbart_+3A_mc.cores">mc.cores</code></td>
<td>

<p>Number of threads to utilize.
</p>
</td></tr>
<tr><td><code id="predict.wbart_+3A_openmp">openmp</code></td>
<td>

<p>Logical value dictating whether OpenMP is utilized for parallel
processing.  Of course, this depends on whether OpenMP is available
on your system which, by default, is verified with <code>mc.cores.openmp</code>.
</p>
</td></tr>
<tr><td><code id="predict.wbart_+3A_...">...</code></td>
<td>

<p>Other arguments which will be passed on to <code>pwbart</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>BART is an Bayesian MCMC method.
At each MCMC interation, we produce a draw from the joint posterior
<code class="reqn">(f,\sigma) | (x,y)</code> in the numeric <code class="reqn">y</code> case
and just <code class="reqn">f</code> in the binary <code class="reqn">y</code> case.
</p>
<p>Thus, unlike a lot of other modelling methods in R, we do not produce a single model object
from which fits and summaries may be extracted.  The output consists of values
<code class="reqn">f^*(x)</code> (and <code class="reqn">\sigma^*</code> in the numeric case) where * denotes a particular draw.
The <code class="reqn">x</code> is either a row from the training data (x.train) or the test data (x.test).
</p>


<h3>Value</h3>

<p>Returns a matrix of predictions corresponding to <code>newdata</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+wbart">wbart</a></code>, <code><a href="#topic+mc.wbart">mc.wbart</a></code>,



<code><a href="#topic+pwbart">pwbart</a></code>, <code><a href="#topic+mc.pwbart">mc.pwbart</a></code>,
<code><a href="#topic+mc.cores.openmp">mc.cores.openmp</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>##simulate data (example from Friedman MARS paper)
f = function(x){
10*sin(pi*x[,1]*x[,2]) + 20*(x[,3]-.5)^2+10*x[,4]+5*x[,5]
}
sigma = 1.0  #y = f(x) + sigma*z , z~N(0,1)
n = 100      #number of observations
set.seed(99)
x=matrix(runif(n*10),n,10) #10 variables, only first 5 matter
y=f(x)

##test BART with token run to ensure installation works
set.seed(99)
post = wbart(x,y,nskip=5,ndpost=5)
x.test = matrix(runif(500*10),500,10)

## Not run: 
##run BART
set.seed(99)
post = wbart(x,y)
x.test = matrix(runif(500*10),500,10)
pred = predict(post, x.test, mu=mean(y))

plot(apply(pred, 2, mean), f(x.test))


## End(Not run)
</code></pre>

<hr>
<h2 id='pwbart'>Predicting new observations with a previously fitted BART model</h2><span id='topic+pwbart'></span><span id='topic+mc.pwbart'></span>

<h3>Description</h3>

<p>BART is a Bayesian &ldquo;sum-of-trees&rdquo; model.<br />
For a numeric response <code class="reqn">y</code>, we have
<code class="reqn">y = f(x) + \epsilon</code>,
where <code class="reqn">\epsilon \sim N(0,\sigma^2)</code>.<br />
</p>
<p><code class="reqn">f</code> is the sum of many tree models.
The goal is to have very flexible inference for the uknown
function <code class="reqn">f</code>.
</p>
<p>In the spirit of &ldquo;ensemble models&rdquo;,
each tree is constrained by a prior to be a weak learner
so that it contributes a
small amount to the overall fit.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pwbart( x.test, treedraws, mu=0, mc.cores=1L, transposed=FALSE,
        dodraws=TRUE,
        nice=19L ## mc.pwbart only
      )

mc.pwbart( x.test, treedraws, mu=0, mc.cores=2L, transposed=FALSE,
           dodraws=TRUE,
           nice=19L ## mc.pwbart only
         )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pwbart_+3A_x.test">x.test</code></td>
<td>

<p>Matrix of covariates to predict <code class="reqn">y</code> for.
</p>
</td></tr>
<tr><td><code id="pwbart_+3A_treedraws">treedraws</code></td>
<td>

<p><code>$treedraws</code> returned from <code>wbart</code> or <code>pbart</code>.
</p>
</td></tr>
<tr><td><code id="pwbart_+3A_mu">mu</code></td>
<td>

<p>Mean to add on to <code class="reqn">y</code> prediction.
</p>
</td></tr>
<tr><td><code id="pwbart_+3A_mc.cores">mc.cores</code></td>
<td>

<p>Number of threads to utilize.
</p>
</td></tr>
<tr><td><code id="pwbart_+3A_transposed">transposed</code></td>
<td>

<p>When running <code>pwbart</code> or <code>mc.pwbart</code> in parallel, it is more memory-efficient
to transpose <code>x.test</code> prior to calling the internal versions of these functions.
</p>
</td></tr>
<tr><td><code id="pwbart_+3A_dodraws">dodraws</code></td>
<td>

<p>Whether to return the draws themselves (the default), or whether to
return the mean of the draws as specified by <code>dodraws=FALSE</code>.
</p>
</td></tr>
<tr><td><code id="pwbart_+3A_nice">nice</code></td>
<td>

<p>Set the job niceness.  The default
niceness is 19: niceness goes from 0 (highest) to 19 (lowest).
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>BART is an Bayesian MCMC method.
At each MCMC interation, we produce a draw from the joint posterior
<code class="reqn">(f,\sigma) | (x,y)</code> in the numeric <code class="reqn">y</code> case
and just <code class="reqn">f</code> in the binary <code class="reqn">y</code> case.
</p>
<p>Thus, unlike a lot of other modelling methods in R, we do not produce a single model object
from which fits and summaries may be extracted.  The output consists of values
<code class="reqn">f^*(x)</code> (and <code class="reqn">\sigma^*</code> in the numeric case) where * denotes a particular draw.
The <code class="reqn">x</code> is either a row from the training data (x.train) or the test data (x.test).
</p>


<h3>Value</h3>

<p>Returns a matrix of predictions corresponding to <code>x.test</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+wbart">wbart</a></code>
<code><a href="#topic+predict.wbart">predict.wbart</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>##simulate data (example from Friedman MARS paper)
f = function(x){
10*sin(pi*x[,1]*x[,2]) + 20*(x[,3]-.5)^2+10*x[,4]+5*x[,5]
}
sigma = 1.0  #y = f(x) + sigma*z , z~N(0,1)
n = 100      #number of observations
set.seed(99)
x=matrix(runif(n*10),n,10) #10 variables, only first 5 matter
y=f(x)

##test BART with token run to ensure installation works
set.seed(99)
post = wbart(x,y,nskip=5,ndpost=5)
x.test = matrix(runif(500*10),500,10)

## Not run: 
##run BART
set.seed(99)
post = wbart(x,y)
x.test = matrix(runif(500*10),500,10)
pred = pwbart(post$treedraws, x.test, mu=mean(y))

plot(apply(pred, 2, mean), f(x.test))


## End(Not run)
</code></pre>

<hr>
<h2 id='recur.bart'>BART for recurrent events</h2><span id='topic+recur.bart'></span><span id='topic+mc.recur.bart'></span>

<h3>Description</h3>

<p>Here we have implemented a simple and direct approach to utilize BART in
survival analysis that is very flexible, and is akin to discrete-time
survival analysis.  Following the capabilities of BART, we allow for
maximum flexibility in modeling the dependence of survival times on
covariates.  In particular, we do not impose proportional hazards.
</p>
<p>To elaborate, consider data in the usual form:
<code class="reqn">(t_i, \delta_i, {x}_i)</code> where <code class="reqn">t_i</code> is the event time,
<code class="reqn">\delta_i</code> is an indicator distinguishing events
(<code class="reqn">\delta=1</code>) from right-censoring
(<code class="reqn">\delta=0</code>), <code class="reqn">{x}_i</code> is a vector of covariates, and
<code class="reqn">i=1, ..., N</code>
indexes subjects.
</p>
<p>We denote the <code class="reqn">K</code> distinct event/censoring times by
<code class="reqn">0&lt;t_{(1)}&lt;...&lt;t_{(K)}&lt;\infty</code> thus
taking <code class="reqn">t_{(j)}</code> to be the <code class="reqn">j^{th}</code> order
statistic among distinct observation times and, for convenience,
<code class="reqn">t_{(0)}=0</code>. Now consider event indicators <code class="reqn">y_{ij}</code>
for each subject <code class="reqn">i</code> at each distinct time <code class="reqn">t_{(j)}</code>
up to and including the subject's observation time
<code class="reqn">t_i=t_{(n_i)}</code> with
<code class="reqn">n_i=\sum_j I[t_{(j)}\leq t_i]</code>.
This means <code class="reqn">y_{ij}=0</code> if <code class="reqn">j&lt;n_i</code> and
<code class="reqn">y_{in_i}=\delta_i</code>.
</p>
<p>We then denote by <code class="reqn">p_{ij}</code> the probability
of an event at time <code class="reqn">t_{(j)}</code> conditional on no previous event. We
now write the model for <code class="reqn">y_{ij}</code> as a nonparametric probit
regression of <code class="reqn">y_{ij}</code> on the time <code class="reqn">t_{(j)}</code> and the covariates
<code class="reqn">{x}_i</code>, and then utilize BART for binary responses.  Specifically,
<code class="reqn"> y_{ij}\ =\ \delta_i I[t_i=t_{(j)}],\ j=1, ..., n_i </code>; we have
<code class="reqn">p_{ij} = F(\mu_{ij}),\ \mu_{ij} = \mu_0+f(t_{(j)}, {x}_i)</code> where <code class="reqn">F</code> denotes the standard normal cdf (probit link).
As in the binary
response case, <code class="reqn">f</code> is the sum of many tree models.  </p>


<h3>Usage</h3>

<pre><code class='language-R'>
recur.bart(x.train=matrix(0,0,0),
           y.train=NULL, times=NULL, delta=NULL,
           x.test=matrix(0,0,0), x.test.nogrid=FALSE,
           sparse=FALSE, theta=0, omega=1,
           a=0.5, b=1, augment=FALSE, rho=NULL,
           xinfo=matrix(0,0,0), usequants=FALSE,
           
           rm.const=TRUE, type='pbart',
           ntype=as.integer(
               factor(type, levels=c('wbart', 'pbart', 'lbart'))),
           k=2, power=2, base=0.95,
           offset=NULL, tau.num=c(NA, 3, 6)[ntype], 
           ntree=50, numcut = 100L, ndpost=1000, nskip=250,
           keepevery=10, 
           
           
           printevery = 100L, 
           keeptrainfits = TRUE,
           seed=99,    ## mc.recur.bart only
           mc.cores=2, ## mc.recur.bart only
           nice=19L    ## mc.recur.bart only
         )

mc.recur.bart(x.train=matrix(0,0,0),
              y.train=NULL, times=NULL, delta=NULL,
              x.test=matrix(0,0,0), x.test.nogrid=FALSE,
              sparse=FALSE, theta=0, omega=1,
              a=0.5, b=1, augment=FALSE, rho=NULL,
              xinfo=matrix(0,0,0), usequants=FALSE,
              
              rm.const=TRUE, type='pbart',
              ntype=as.integer(
                  factor(type, levels=c('wbart', 'pbart', 'lbart'))),
              k=2, power=2, base=0.95,
              offset=NULL, tau.num=c(NA, 3, 6)[ntype], 
              ntree=50, numcut = 100L, ndpost=1000, nskip=250,
              keepevery=10, 
              
              
              printevery = 100L, 
              keeptrainfits = TRUE,
              seed=99,    ## mc.recur.bart only
              mc.cores=2, ## mc.recur.bart only
              nice=19L    ## mc.recur.bart only
            )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="recur.bart_+3A_x.train">x.train</code></td>
<td>
<p> Explanatory variables for training (in sample)
data.<br /> Must be a matrix with (as usual) rows corresponding to
observations and columns to variables.<br /> <code>recur.bart</code> will generate
draws of <code class="reqn">f(t, x)</code> for each <code class="reqn">x</code> which is a row of x.train (note
that the definition of <code>x.train</code> is dependent on whether
<code>y.train</code> has been specified; see below).  </p>
</td></tr>
<tr><td><code id="recur.bart_+3A_y.train">y.train</code></td>
<td>

<p>Binary response dependent variable for training (in sample) data.<br />
If <code>y.train</code> is <code>NULL</code>, then <code>y.train</code> (<code>x.train</code> and
<code>x.test</code>, if specified) are generated
by a call to <code>recur.pre.bart</code> (which require that <code>times</code> and <code>delta</code> be
provided: see below); otherwise, <code>y.train</code> (<code>x.train</code> and
<code>x.test</code>, if specified)
are utilized as given assuming that the data construction has already been performed.
</p>
</td></tr>
<tr><td><code id="recur.bart_+3A_times">times</code></td>
<td>

<p>The time of event or right-censoring.<br />
If <code>y.train</code> is <code>NULL</code>, then <code>times</code> (and <code>delta</code>)
must be provided.
</p>
</td></tr>
<tr><td><code id="recur.bart_+3A_delta">delta</code></td>
<td>

<p>The event indicator: 1 is an event while 0 is censored.<br />
If <code>y.train</code> is <code>NULL</code>, then <code>delta</code> (and <code>times</code>)
must be provided.
</p>
</td></tr>
<tr><td><code id="recur.bart_+3A_x.test">x.test</code></td>
<td>

<p>Explanatory variables for test (out of sample) data.<br />
Must be a matrix and have the same structure as x.train.<br />
<code>recur.bart</code> will generate draws of <code class="reqn">f(t, x)</code> for each <code class="reqn">x</code> which is a row of x.test.
</p>
</td></tr>
<tr><td><code id="recur.bart_+3A_x.test.nogrid">x.test.nogrid</code></td>
<td>

<p>Occasionally, you do not need the entire time grid for
<code>x.test</code>.  If so, then for performance reasons, you can set this argument to <code>TRUE</code>.
</p>
</td></tr>
<tr><td><code id="recur.bart_+3A_sparse">sparse</code></td>
<td>
<p>Whether to perform variable selection based on a
sparse Dirichlet prior rather than simply uniform; see Linero 2016.</p>
</td></tr>
<tr><td><code id="recur.bart_+3A_theta">theta</code></td>
<td>
<p>Set <code class="reqn">theta</code> parameter; zero means random.</p>
</td></tr>
<tr><td><code id="recur.bart_+3A_omega">omega</code></td>
<td>
<p>Set <code class="reqn">omega</code> parameter; zero means random.</p>
</td></tr>
<tr><td><code id="recur.bart_+3A_a">a</code></td>
<td>
<p>Sparse parameter for <code class="reqn">Beta(a, b)</code> prior:
<code class="reqn">0.5&lt;=a&lt;=1</code> where lower values inducing more sparsity.</p>
</td></tr>
<tr><td><code id="recur.bart_+3A_b">b</code></td>
<td>
<p>Sparse parameter for <code class="reqn">Beta(a, b)</code> prior; typically,
<code class="reqn">b=1</code>.</p>
</td></tr>
<tr><td><code id="recur.bart_+3A_rho">rho</code></td>
<td>
<p>Sparse parameter: typically <code class="reqn">rho=p</code> where <code class="reqn">p</code> is the
number of covariates under consideration.</p>
</td></tr>
<tr><td><code id="recur.bart_+3A_augment">augment</code></td>
<td>
<p>Whether data augmentation is to be performed in sparse
variable selection.</p>
</td></tr>
<tr><td><code id="recur.bart_+3A_xinfo">xinfo</code></td>
<td>
<p> You can provide the cutpoints to BART or let BART
choose them for you.  To provide them, use the <code>xinfo</code>
argument to specify a list (matrix) where the items (rows) are the
covariates and the contents of the items (columns) are the
cutpoints.  </p>
</td></tr>
<tr><td><code id="recur.bart_+3A_usequants">usequants</code></td>
<td>
<p> If <code>usequants=FALSE</code>, then the
cutpoints in <code>xinfo</code> are generated uniformly; otherwise,
if <code>TRUE</code>, uniform quantiles are used for the cutpoints. </p>
</td></tr>

<tr><td><code id="recur.bart_+3A_rm.const">rm.const</code></td>
<td>
<p> Whether or not to remove constant variables.</p>
</td></tr>
<tr><td><code id="recur.bart_+3A_type">type</code></td>
<td>
<p> Whether to employ Albert-Chib, <code>'pbart'</code>, or
Holmes-Held, <code>'lbart'</code>. </p>
</td></tr>
<tr><td><code id="recur.bart_+3A_ntype">ntype</code></td>
<td>
<p> The integer equivalent of <code>type</code> where
<code>'wbart'</code> is 1, <code>'pbart'</code> is 2 and
<code>'lbart'</code> is 3.</p>
</td></tr>
<tr><td><code id="recur.bart_+3A_k">k</code></td>
<td>

<p>k is the number of prior standard deviations <code class="reqn">f(t, x)</code> is away from +/-3.
The bigger k is, the more conservative the fitting will be.
</p>
</td></tr>
<tr><td><code id="recur.bart_+3A_power">power</code></td>
<td>

<p>Power parameter for tree prior.
</p>
</td></tr>
<tr><td><code id="recur.bart_+3A_base">base</code></td>
<td>

<p>Base parameter for tree prior.
</p>
</td></tr>
<tr><td><code id="recur.bart_+3A_offset">offset</code></td>
<td>
<p> With binary
BART, the centering is <code class="reqn">P(Y=1 | x) = F(f(x) + offset)</code> where
<code>offset</code> defaults to <code>F^{-1}(mean(y.train))</code>.  You can use
the <code>offset</code> parameter to over-ride these defaults.</p>
</td></tr>
<tr><td><code id="recur.bart_+3A_tau.num">tau.num</code></td>
<td>
<p> The numerator in the <code>tau</code> definition, i.e.,
<code>tau=tau.num/(k*sqrt(ntree))</code>. </p>
</td></tr>   



















<tr><td><code id="recur.bart_+3A_ntree">ntree</code></td>
<td>

<p>The number of trees in the sum.
</p>
</td></tr>
<tr><td><code id="recur.bart_+3A_numcut">numcut</code></td>
<td>

<p>The number of possible values of c (see usequants).
If a single number if given, this is used for all variables.
Otherwise a vector with length equal to ncol(x.train) is required,
where the <code class="reqn">i^{th}</code> element gives the number of c used for
the <code class="reqn">i^{th}</code> variable in x.train.
If usequants is false, numcut equally spaced cutoffs
are used covering the range of values in the corresponding
column of x.train.  If usequants is true, then  min(numcut, the number of unique values in the
corresponding columns of x.train - 1) c values are used.
</p>
</td></tr>
<tr><td><code id="recur.bart_+3A_ndpost">ndpost</code></td>
<td>

<p>The number of posterior draws returned.
</p>
</td></tr>
<tr><td><code id="recur.bart_+3A_nskip">nskip</code></td>
<td>

<p>Number of MCMC iterations to be treated as burn in.
</p>
</td></tr>












<tr><td><code id="recur.bart_+3A_keepevery">keepevery</code></td>
<td>

<p>Every keepevery draw is kept to be returned to the user.
</p>
</td></tr>
<tr><td><code id="recur.bart_+3A_printevery">printevery</code></td>
<td>

<p>As the MCMC runs, a message is printed every printevery draws.
</p>
</td></tr>







<tr><td><code id="recur.bart_+3A_keeptrainfits">keeptrainfits</code></td>
<td>

<p>Whether to keep <code>yhat.train</code> or not.
</p>
</td></tr>





<tr><td><code id="recur.bart_+3A_seed">seed</code></td>
<td>

<p><code>mc.recur.bart</code> only: seed required for reproducible MCMC.
</p>
</td></tr>
<tr><td><code id="recur.bart_+3A_mc.cores">mc.cores</code></td>
<td>

<p><code>mc.recur.bart</code> only: number of cores to employ in parallel.
</p>
</td></tr>
<tr><td><code id="recur.bart_+3A_nice">nice</code></td>
<td>

<p><code>mc.recur.bart</code> only: set the job niceness.  The default
niceness is 19: niceness goes from 0 (highest) to 19 (lowest).
</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>recur.bart</code> returns an object of type <code>recurbart</code> which is
essentially a list.  Besides the items listed
below, the list has a <code>binaryOffset</code> component giving the value
used, a <code>times</code> component giving the unique times, <code>K</code>
which is the number of unique times, <code>tx.train</code> and
<code>tx.test</code>, if any.
</p>
<table>
<tr><td><code>yhat.train</code></td>
<td>

<p>A matrix with ndpost rows and nrow(x.train) columns.
Each row corresponds to a draw <code class="reqn">f^*</code> from the posterior of <code class="reqn">f</code>
and each column corresponds to a row of x.train.
The <code class="reqn">(i,j)</code> value is <code class="reqn">f^*(t, x)</code> for the <code class="reqn">i^{th}</code> kept draw of <code class="reqn">f</code>
and the <code class="reqn">j^{th}</code> row of x.train.<br />
Burn-in is dropped.
</p>
</td></tr>
<tr><td><code>haz.train</code></td>
<td>
<p>The hazard function, <code class="reqn">h(t|x)</code>, where x's are the
rows of the training data.</p>
</td></tr>
<tr><td><code>cum.train</code></td>
<td>
<p>The cumulative hazard function, <code class="reqn">h(t|x)</code>, where x's are the
rows of the training data.</p>
</td></tr>
<tr><td><code>yhat.test</code></td>
<td>
<p>Same as yhat.train but now the x's are the rows of the test data.</p>
</td></tr>
<tr><td><code>haz.test</code></td>
<td>
<p>The hazard function, <code class="reqn">h(t|x)</code>, where x's are the
rows of the test data.</p>
</td></tr>
<tr><td><code>cum.test</code></td>
<td>
<p>The cumulative hazard function, <code class="reqn">h(t|x)</code>, where x's are the
rows of the test data.</p>
</td></tr>
<tr><td><code>varcount</code></td>
<td>
<p>a matrix with ndpost rows and nrow(x.train) columns.
Each row is for a draw. For each variable (corresponding to the columns),
the total count of the number of times
that variable is used in a tree decision rule (over all trees) is given.</p>
</td></tr>
</table>
<p>Note that yhat.train and yhat.test are
<code class="reqn">f(t, x)</code> + <code>binaryOffset</code>.  If you want draws of the probability
<code class="reqn">P(Y=1 | t, x)</code> you need to apply the normal cdf (<code>pnorm</code>)
to these values.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+recur.pre.bart">recur.pre.bart</a></code>, <code><a href="#topic+predict.recurbart">predict.recurbart</a></code>,
<code><a href="#topic+recur.pwbart">recur.pwbart</a></code>, <code><a href="#topic+mc.recur.pwbart">mc.recur.pwbart</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## load 20 percent random sample
data(xdm20.train)
data(xdm20.test)
data(ydm20.train)

##test BART with token run to ensure installation works
## with current technology even a token run will violate CRAN policy
## set.seed(99)
## post &lt;- recur.bart(x.train=xdm20.train, y.train=ydm20.train,
##                    nskip=1, ndpost=1, keepevery=1)

## Not run: 

## set.seed(99)
## post &lt;- recur.bart(x.train=xdm20.train, y.train=ydm20.train,
##                    keeptrainfits=TRUE)

## larger data sets can take some time so, if parallel processing
## is available, submit this statement instead
post &lt;- mc.recur.bart(x.train=xdm20.train, y.train=ydm20.train,
                      keeptrainfits=TRUE, mc.cores=8, seed=99)

require(rpart)
require(rpart.plot)

post$yhat.train.mean &lt;- apply(post$yhat.train, 2, mean)
dss &lt;- rpart(post$yhat.train.mean~xdm20.train)

rpart.plot(dss)
## for the 20 percent sample, notice that the top splits
## involve cci_pvd and n
## for the full data set, notice that all splits
## involve ca, cci_pud, cci_pvd, ins270 and n
## (except one at the bottom involving a small group)

## compare patients treated with insulin (ins270=1) vs
## not treated with insulin (ins270=0)
N &lt;- 50 ## 50 training patients and 50 validation patients
K &lt;- post$K ## 798 unique time points
NK &lt;- 50*K

## only testing set, i.e., remove training set
xdm20.test. &lt;- xdm20.test[NK+1:NK, post$rm.const]
xdm20.test. &lt;- rbind(xdm20.test., xdm20.test.)
xdm20.test.[ , 'ins270'] &lt;- rep(0:1, each=NK)

## multiple threads will be utilized if available
pred &lt;- predict(post, xdm20.test., mc.cores=8)

## create Friedman's partial dependence function for the
## relative intensity for ins270 by time
M &lt;- nrow(pred$haz.test) ## number of MCMC samples
RI &lt;- matrix(0, M, K)
for(j in 1:K) {
    h &lt;- seq(j, NK, by=K)
    RI[ , j] &lt;- apply(pred$haz.test[ , h+NK]/
                      pred$haz.test[ , h], 1, mean)
}

RI.lo &lt;- apply(RI, 2, quantile, probs=0.025)
RI.mu &lt;- apply(RI, 2, mean)
RI.hi &lt;- apply(RI, 2, quantile, probs=0.975)

plot(post$times, RI.hi, type='l', lty=2, log='y',
     ylim=c(min(RI.lo, 1/RI.hi), max(1/RI.lo, RI.hi)),
     xlab='t', ylab='RI(t, x)',
     sub='insulin(ins270=1) vs. no insulin(ins270=0)',
     main='Relative intensity of hospital admissions for diabetics')
lines(post$times, RI.mu)
lines(post$times, RI.lo, lty=2)
lines(post$times, rep(1, K), col='darkgray')

## RI for insulin therapy seems fairly constant with time
mean(RI.mu)


## End(Not run)
</code></pre>

<hr>
<h2 id='recur.pre.bart'>Data construction for recurrent events with BART</h2><span id='topic+recur.pre.bart'></span>

<h3>Description</h3>

<p>Recurrent event data contained in <code class="reqn">(t_1,\delta_1, ..., t_k,\delta_k, x)</code> must be translated to data
suitable for the BART model; see <code>recur.bart</code> for more details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>recur.pre.bart( times, delta, x.train=NULL, tstop=NULL, last.value=TRUE )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="recur.pre.bart_+3A_times">times</code></td>
<td>

<p>Matrix of time to event or right-censoring.<br />
</p>
</td></tr>
<tr><td><code id="recur.pre.bart_+3A_delta">delta</code></td>
<td>

<p>Matrix of event indicators: 1 is an event while 0 is censored.<br />
</p>
</td></tr>
<tr><td><code id="recur.pre.bart_+3A_x.train">x.train</code></td>
<td>

<p>Explanatory variables for training (in sample) data.<br />
If provided, must be a matrix
with (as usual) rows corresponding to observations and columns to variables.<br />
</p>
</td></tr>
<tr><td><code id="recur.pre.bart_+3A_tstop">tstop</code></td>
<td>
<p> For non-instantaneous events, this the matrix of event
stop times, i.e., between <code>times[i, j]</code> and <code>tstop[i, j]</code>
subject <code>i</code> is not in the risk set for a recurrent event.
N.B. This is NOT for counting process notation. <br /> </p>
</td></tr>
<tr><td><code id="recur.pre.bart_+3A_last.value">last.value</code></td>
<td>
<p>If <code>last.value=TRUE</code>, then the sojourn time,
<code>v</code>, and the number of previous events, <code>N</code>, are carried
forward assuming that no new events occur beyond censoring.
If <code>last.value=FALSE</code>, then these variables are coded <code>NA</code>
for easy identification allowing replacement with the desired values.
</p>
</td></tr>























</table>


<h3>Value</h3>

<p><code>recur.pre.bart</code> returns a list.
Besides the items listed below, the list has
a <code>times</code> component giving the unique times and <code>K</code> which is the number of
unique times.
</p>
<table>
<tr><td><code>y.train</code></td>
<td>
<p>A vector of binary responses.</p>
</td></tr>
<tr><td><code>tx.train</code></td>
<td>
<p>A matrix with the rows of the training data.</p>
</td></tr>
<tr><td><code>tx.test</code></td>
<td>
<p>Generated from <code>x.train</code> (see discussion above included in
the argument <code>last.value</code>).</p>
</td></tr>






</table>


<h3>See Also</h3>

<p><code><a href="#topic+recur.bart">recur.bart</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(bladder)
subset &lt;- -which(bladder1$stop==0)
bladder0 &lt;- bladder1[subset, ]
id &lt;- unique(sort(bladder0$id))
N &lt;- length(id)
L &lt;- max(bladder0$enum)

times &lt;- matrix(0, nrow=N, ncol=L)
dimnames(times)[[1]] &lt;- paste0(id)

delta &lt;- matrix(0, nrow=N, ncol=L)
dimnames(delta)[[1]] &lt;- paste0(id)

x.train &lt;- matrix(NA, nrow=N, ncol=3+2*L) ## add time-dependent cols too
dimnames(x.train)[[1]] &lt;- paste0(id)
dimnames(x.train)[[2]] &lt;- c('Pl', 'B6', 'Th', rep(c('number', 'size'), L))

for(i in 1:N) {
    h &lt;- id[i]

    for(j in 1:L) {
        k &lt;- which(bladder0$id==h &amp; bladder0$enum==j)

        if(length(k)==1) {
            times[i, j] &lt;- bladder0$stop[k]
            delta[i, j] &lt;- (bladder0$status[k]==1)*1

            if(j==1) {
                x.train[i, 1] &lt;- as.numeric(bladder0$treatment[k])==1
                x.train[i, 2] &lt;- as.numeric(bladder0$treatment[k])==2
                x.train[i, 3] &lt;- as.numeric(bladder0$treatment[k])==3
                x.train[i, 4] &lt;- bladder0$number[k]
                x.train[i, 5] &lt;- bladder0$size[k]
            }
            else if(delta[i, j]==1) {
                if(bladder0$rtumor[k]!='.')
                    x.train[i, 2*j+2] &lt;- as.numeric(bladder0$rtumor[k])
                if(bladder0$rsize[k]!='.')
                    x.train[i, 2*j+3] &lt;- as.numeric(bladder0$rsize[k])
            }
        }
    }
}

pre &lt;- recur.pre.bart(times=times, delta=delta, x.train=x.train)

J &lt;- nrow(pre$tx.train)
for(j in 1:J) {
    if(pre$tx.train[j, 3]&gt;0) {
        pre$tx.train[j, 7] &lt;- pre$tx.train[j, 7+pre$tx.train[j, 3]*2]
        pre$tx.train[j, 8] &lt;- pre$tx.train[j, 8+pre$tx.train[j, 3]*2]
    }
}
pre$tx.train &lt;- pre$tx.train[ , 1:8]

K &lt;- pre$K
NK &lt;- N*K
for(j in 1:NK) {
    if(pre$tx.test[j, 3]&gt;0) {
        pre$tx.test[j, 7] &lt;- pre$tx.test[j, 7+pre$tx.test[j, 3]*2]
        pre$tx.test[j, 8] &lt;- pre$tx.test[j, 8+pre$tx.test[j, 3]*2]
    }
}
pre$tx.test &lt;- pre$tx.test[ , 1:8]


## in bladder1 both number and size are recorded as integers
## from 1 to 8 however they are often missing for recurrences
## at baseline there are no missing and 1 is the mode of both
pre$tx.train[which(is.na(pre$tx.train[ , 7])), 7] &lt;- 1
pre$tx.train[which(is.na(pre$tx.train[ , 8])), 8] &lt;- 1
pre$tx.test[which(is.na(pre$tx.test[ , 7])), 7] &lt;- 1
pre$tx.test[which(is.na(pre$tx.test[ , 8])), 8] &lt;- 1

## it is a good idea to explore more sophisticated methods
## such as imputing the missing data with Sequential BART
## Xu, Daniels and Winterstein.  Sequential BART for imputation of missing
## covariates.  Biostatistics 2016 doi: 10.1093/biostatistics/kxw009
## http://biostatistics.oxfordjournals.org/content/early/2016/03/15/biostatistics.kxw009/suppl/DC1
## https://cran.r-project.org/package=sbart
## library(sbart)
## set.seed(21)
## train &lt;- seqBART(xx=pre$tx.train, yy=NULL, datatype=rep(0, 6),
##                type=0, numskip=20, burn=1000)
## coarsen the imputed data same way as observed example data
## train$imputed5[which(train$imputed5[ , 7]&lt;1), 7] &lt;- 1
## train$imputed5[which(train$imputed5[ , 7]&gt;8), 7] &lt;- 8
## train$imputed5[ , 7] &lt;- round(train$imputed5[ , 7])
## train$imputed5[which(train$imputed5[ , 8]&lt;1), 8] &lt;- 1
## train$imputed5[which(train$imputed5[ , 8]&gt;8), 8] &lt;- 8
## train$imputed5[ , 8] &lt;- round(train$imputed5[ , 8])

## for Friedman's partial dependence, we need to estimate the whole cohort
## at each treatment assignment (and, average over those)
pre$tx.test &lt;- rbind(pre$tx.test, pre$tx.test, pre$tx.test)
pre$tx.test[ , 4] &lt;- c(rep(1, NK), rep(0, 2*NK))          ## Pl
pre$tx.test[ , 5] &lt;- c(rep(0, NK), rep(1, NK), rep(0, NK))## B6
pre$tx.test[ , 6] &lt;- c(rep(0, 2*NK), rep(1, NK))          ## Th

## Not run: 
## set.seed(99)
## post &lt;- recur.bart(y.train=pre$y.train, x.train=pre$tx.train, x.test=pre$tx.test)
## depending on your performance, you may want to run in parallel if available
post &lt;- mc.recur.bart(y.train=pre$y.train, x.train=pre$tx.train,
                      x.test=pre$tx.test, mc.cores=8, seed=99)

M &lt;- nrow(post$yhat.test)
RI.B6.Pl &lt;- matrix(0, nrow=M, ncol=K)
RI.Th.Pl &lt;- matrix(0, nrow=M, ncol=K)
RI.Th.B6 &lt;- matrix(0, nrow=M, ncol=K)

for(j in 1:K) {
    h &lt;- seq(j, NK, K)
    RI.B6.Pl[ , j] &lt;- apply(post$prob.test[ , h+NK]/
                            post$prob.test[ , h], 1, mean)
    RI.Th.Pl[ , j] &lt;- apply(post$prob.test[ , h+2*NK]/
                            post$prob.test[ , h], 1, mean)
    RI.Th.B6[ , j] &lt;- apply(post$prob.test[ , h+2*NK]/
                            post$prob.test[ , h+NK], 1, mean)
}

RI.B6.Pl.mu &lt;- apply(RI.B6.Pl, 2, mean)
RI.B6.Pl.025 &lt;- apply(RI.B6.Pl, 2, quantile, probs=0.025)
RI.B6.Pl.975 &lt;- apply(RI.B6.Pl, 2, quantile, probs=0.975)

RI.Th.Pl.mu &lt;- apply(RI.Th.Pl, 2, mean)
RI.Th.Pl.025 &lt;- apply(RI.Th.Pl, 2, quantile, probs=0.025)
RI.Th.Pl.975 &lt;- apply(RI.Th.Pl, 2, quantile, probs=0.975)

RI.Th.B6.mu &lt;- apply(RI.Th.B6, 2, mean)
RI.Th.B6.025 &lt;- apply(RI.Th.B6, 2, quantile, probs=0.025)
RI.Th.B6.975 &lt;- apply(RI.Th.B6, 2, quantile, probs=0.975)

plot(post$times, RI.Th.Pl.mu, col='blue',
     log='y', main='Bladder cancer ex: Thiotepa vs. Placebo',
     type='l', ylim=c(0.1, 10), ylab='RI(t)', xlab='t (months)')
lines(post$times, RI.Th.Pl.025, col='red')
lines(post$times, RI.Th.Pl.975, col='red')
abline(h=1)

plot(post$times, RI.B6.Pl.mu, col='blue',
     log='y', main='Bladder cancer ex: Vitamin B6 vs. Placebo',
     type='l', ylim=c(0.1, 10), ylab='RI(t)', xlab='t (months)')
lines(post$times, RI.B6.Pl.025, col='red')
lines(post$times, RI.B6.Pl.975, col='red')
abline(h=1)

plot(post$times, RI.Th.B6.mu, col='blue',
     log='y', main='Bladder cancer ex: Thiotepa vs. Vitamin B6',
     type='l', ylim=c(0.1, 10), ylab='RI(t)', xlab='t (months)')
lines(post$times, RI.Th.B6.025, col='red')
lines(post$times, RI.Th.B6.975, col='red')
abline(h=1)


## End(Not run)
</code></pre>

<hr>
<h2 id='rs.pbart'>BART for dichotomous outcomes with parallel computation and
stratified random sampling</h2><span id='topic+rs.pbart'></span>

<h3>Description</h3>

<p>BART is a Bayesian &ldquo;sum-of-trees&rdquo; model.<br />
For numeric response <code class="reqn">y</code>, we have
<code class="reqn">y = f(x) + \epsilon</code>,
where <code class="reqn">\epsilon \sim N(0,\sigma^2)</code>.<br />
For a binary response <code class="reqn">y</code>, <code class="reqn">P(Y=1 | x) = F(f(x))</code>, where <code class="reqn">F</code>
denotes the standard normal cdf (probit link).
</p>
<p>In both cases, <code class="reqn">f</code> is the sum of many tree models.
The goal is to have very flexible inference for the uknown
function <code class="reqn">f</code>.
</p>
<p>In the spirit of &ldquo;ensemble models&rdquo;,
each tree is constrained by a prior to be a weak learner
so that it contributes a
small amount to the overall fit.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rs.pbart(
   x.train, y.train, x.test=matrix(0.0,0,0),
   C=floor(length(y.train)/2000),
   k=2.0, power=2.0, base=.95,
   binaryOffset=0,
   ntree=50L, numcut=100L,
   ndpost=1000L, nskip=100L,
   keepevery=1L, printevery=100,
   keeptrainfits=FALSE, transposed=FALSE,
   
   mc.cores = 2L, nice = 19L,
   seed = 99L
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rs.pbart_+3A_x.train">x.train</code></td>
<td>

<p>Explanatory variables for training (in sample) data.<br />
May be a matrix or a data frame,
with (as usual) rows corresponding to observations and columns to variables.<br />
If a variable is a factor in a data frame, it is replaced with dummies.
Note that q dummies are created if q&gt;2 and
one dummy is created if q=2, where q is the number of levels of the factor.
<code>pbart</code> will generate draws of <code class="reqn">f(x)</code> for each <code class="reqn">x</code>
which is a row of x.train.
</p>
</td></tr>
<tr><td><code id="rs.pbart_+3A_y.train">y.train</code></td>
<td>

<p>Dependent variable for training (in sample) data.<br />
If y is numeric a continous response model is fit (normal errors).<br />
If y is a factor (or just has values 0 and 1) then a binary response model
with a probit link is fit.
</p>
</td></tr>
<tr><td><code id="rs.pbart_+3A_x.test">x.test</code></td>
<td>

<p>Explanatory variables for test (out of sample) data.<br />
Should have same structure as x.train.<br />
<code>pbart</code> will generate draws of <code class="reqn">f(x)</code> for each <code class="reqn">x</code> which is a row of x.test.
</p>
</td></tr>
<tr><td><code id="rs.pbart_+3A_c">C</code></td>
<td>

<p>The number of shards to break the data into and analyze separately.
</p>
</td></tr>
<tr><td><code id="rs.pbart_+3A_k">k</code></td>
<td>

<p>For binary y,
k is the number of prior standard deviations <code class="reqn">f(x)</code> is away from +/-3.
In both cases, the bigger k is, the more conservative the fitting will be.
</p>
</td></tr>
<tr><td><code id="rs.pbart_+3A_power">power</code></td>
<td>

<p>Power parameter for tree prior.
</p>
</td></tr>
<tr><td><code id="rs.pbart_+3A_base">base</code></td>
<td>

<p>Base parameter for tree prior.
</p>
</td></tr>
<tr><td><code id="rs.pbart_+3A_binaryoffset">binaryOffset</code></td>
<td>

<p>Used for binary <code class="reqn">y</code>.<br />
The model is <code class="reqn">P(Y=1 | x) = F(f(x) + binaryOffset)</code>.<br />
The idea is that <code class="reqn">f</code> is shrunk towards 0, so the offset allows you to shrink towards
a probability other than .5.
</p>
</td></tr>
<tr><td><code id="rs.pbart_+3A_ntree">ntree</code></td>
<td>

<p>The number of trees in the sum.
</p>
</td></tr>
<tr><td><code id="rs.pbart_+3A_numcut">numcut</code></td>
<td>

<p>The number of possible values of c (see usequants).
If a single number if given, this is used for all variables.
Otherwise a vector with length equal to ncol(x.train) is required,
where the <code class="reqn">i^{th}</code> element gives the number of c used for
the <code class="reqn">i^{th}</code> variable in x.train.
If usequants is false, numcut equally spaced cutoffs
are used covering the range of values in the corresponding
column of x.train.  If usequants is true, then  min(numcut, the number of unique values in the
corresponding columns of x.train - 1) c values are used.
</p>
</td></tr>
<tr><td><code id="rs.pbart_+3A_ndpost">ndpost</code></td>
<td>

<p>The number of posterior draws returned.
</p>
</td></tr>
<tr><td><code id="rs.pbart_+3A_nskip">nskip</code></td>
<td>

<p>Number of MCMC iterations to be treated as burn in.
</p>
</td></tr>
<tr><td><code id="rs.pbart_+3A_keepevery">keepevery</code></td>
<td>

<p>Every keepevery draw is kept to be returned to the user.
</p>
</td></tr>
<tr><td><code id="rs.pbart_+3A_printevery">printevery</code></td>
<td>

<p>As the MCMC runs, a message is printed every printevery draws.
</p>
</td></tr>
<tr><td><code id="rs.pbart_+3A_keeptrainfits">keeptrainfits</code></td>
<td>

<p>Whether to keep <code>yhat.train</code> or not.
</p>
</td></tr>
<tr><td><code id="rs.pbart_+3A_transposed">transposed</code></td>
<td>

<p>When running <code>pbart</code> in parallel, it is more memory-efficient
to transpose <code>x.train</code> and <code>x.test</code>, if any, prior to
calling <code>mc.pbart</code>.
</p>
</td></tr>





<tr><td><code id="rs.pbart_+3A_seed">seed</code></td>
<td>

<p>Setting the seed required for reproducible MCMC.
</p>
</td></tr>
<tr><td><code id="rs.pbart_+3A_mc.cores">mc.cores</code></td>
<td>

<p>Number of cores to employ in parallel.
</p>
</td></tr>
<tr><td><code id="rs.pbart_+3A_nice">nice</code></td>
<td>

<p>Set the job niceness.  The default
niceness is 19: niceness goes from 0 (highest) to 19 (lowest).
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>BART is an Bayesian MCMC method.
At each MCMC interation, we produce a draw from the joint posterior
<code class="reqn">(f,\sigma) | (x,y)</code> in the numeric <code class="reqn">y</code> case
and just <code class="reqn">f</code> in the binary <code class="reqn">y</code> case.
</p>
<p>Thus, unlike a lot of other modelling methods in R, we do not produce a single model object
from which fits and summaries may be extracted.  The output consists of values
<code class="reqn">f^*(x)</code> (and <code class="reqn">\sigma^*</code> in the numeric case) where * denotes a particular draw.
The <code class="reqn">x</code> is either a row from the training data (x.train) or the test data (x.test).
</p>


<h3>Value</h3>

<p><code>rs.pbart</code> returns an object of type <code>pbart</code> which is
essentially a list. 
</p>
<table>
<tr><td><code>yhat.shard</code></td>
<td>

<p>Estimates generated from the individual shards rather than from the
whole.  This object is only useful for assessing convergence.
</p>
<p>A matrix with ndpost rows and nrow(x.train) columns.
Each row corresponds to a draw <code class="reqn">f^*</code> from the posterior of <code class="reqn">f</code>
and each column corresponds to a row of x.train.
The <code class="reqn">(i,j)</code> value is <code class="reqn">f^*(x)</code> for the <code class="reqn">i^{th}</code> kept draw of <code class="reqn">f</code>
and the <code class="reqn">j^{th}</code> row of x.train.<br />
Burn-in is dropped.
</p>
</td></tr>
<tr><td><code>yhat.train</code></td>
<td>

<p>Estimates generated from the whole if <code>keeptrainfits=TRUE</code>.
</p>
<p>A matrix with ndpost rows and nrow(x.train) columns.
Each row corresponds to a draw <code class="reqn">f^*</code> from the posterior of <code class="reqn">f</code>
and each column corresponds to a row of x.train.
The <code class="reqn">(i,j)</code> value is <code class="reqn">f^*(x)</code> for the <code class="reqn">i^{th}</code> kept draw of <code class="reqn">f</code>
and the <code class="reqn">j^{th}</code> row of x.train.<br />
Burn-in is dropped.
</p>
</td></tr>
<tr><td><code>yhat.test</code></td>
<td>

<p>Estimates generated from the whole if <code>x.test</code> is provided.
</p>
<p>Same as yhat.train but now the x's are the rows of the test data.</p>
</td></tr>


<tr><td><code>varcount</code></td>
<td>
<p>a matrix with ndpost rows and nrow(x.train) columns.
Each row is for a draw. For each variable (corresponding to the columns),
the total count of the number of times
that variable is used in a tree decision rule (over all trees) is given.</p>
</td></tr>
</table>
<p>In addition the list
has a binaryOffset component giving the value used.
</p>
<p>Note that in the binary <code class="reqn">y</code>, case yhat.train and yhat.test are
<code class="reqn">f(x)</code> + binaryOffset.  If you want draws of the probability
<code class="reqn">P(Y=1 | x)</code> you need to apply the normal cdf (<code>pnorm</code>)
to these values.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mc.pbart">mc.pbart</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
##simulate from Friedman's five-dimensional test function
##Friedman JH. Multivariate adaptive regression splines
##(with discussion and a rejoinder by the author).
##Annals of Statistics 1991; 19:1-67.

f = function(x) #only the first 5 matter
    sin(pi*x[ , 1]*x[ , 2]) + 2*(x[ , 3]-.5)^2+x[ , 4]+0.5*x[ , 5]-1.5

sigma = 1.0  #y = f(x) + sigma*z where z~N(0, 1)
k = 50       #number of covariates
thin = 25
ndpost = 2500
nskip = 100
C = 10
m = 10
n = 10000

set.seed(12)
x.train=matrix(runif(n*k), n, k)
Ey.train = f(x.train)
y.train=(Ey.train+sigma*rnorm(n)&gt;0)*1
table(y.train)/n

x &lt;- x.train
x4 &lt;- seq(0, 1, length.out=m)

for(i in 1:m) {
    x[ , 4] &lt;- x4[i]

    if(i==1) x.test &lt;- x
    else x.test &lt;- rbind(x.test, x)
}

## parallel::mcparallel/mccollect do not exist on windows
if(.Platform$OS.type=='unix') {
##test BART with token run to ensure installation works
    post = rs.pbart(x.train, y.train, 
                C=C, mc.cores=4, keepevery=1,
                seed=99, ndpost=1, nskip=1)
}

## Not run: 
post = rs.pbart(x.train, y.train, x.test=x.test,
                C=C, mc.cores=8, keepevery=thin,
                seed=99, ndpost=ndpost, nskip=nskip)
str(post)

par(mfrow=c(2, 2))

M &lt;- nrow(post$yhat.test)
pred &lt;- matrix(nrow=M, ncol=10)

for(i in 1:m) {
    h &lt;- (i-1)*n+1:n
    pred[ , i] &lt;- apply(pnorm(post$yhat.test[ , h]), 1, mean)
}

pred &lt;- apply(pred, 2, mean)

plot(x4, qnorm(pred), xlab=expression(x[4]),
     ylab='partial dependence function', type='l')

i &lt;- floor(seq(1, n, length.out=10))
j &lt;- seq(-0.5, 0.4, length.out=10)
for(h in 1:10) {
    auto.corr &lt;- acf(post$yhat.shard[ , i[h]], plot=FALSE)
    if(h==1) {
        max.lag &lt;- max(auto.corr$lag[ , 1, 1])
        plot(1:max.lag+j[h], auto.corr$acf[1+(1:max.lag), 1, 1],
             type='h', xlim=c(0, max.lag+1), ylim=c(-1, 1),
             ylab='auto-correlation', xlab='lag')
    }
    else 
        lines(1:max.lag+j[h], auto.corr$acf[1+(1:max.lag), 1, 1],
              type='h', col=h)
}

for(j in 1:10) {
    if(j==1)
        plot(pnorm(post$yhat.shard[ , i[j]]),
             type='l', ylim=c(0, 1),
             sub=paste0('N:', n, ', k:', k),
             ylab=expression(Phi(f(x))), xlab='m')
    else
        lines(pnorm(post$yhat.shard[ , i[j]]),
              type='l', col=j)
}

geweke &lt;- gewekediag(post$yhat.shard)

j &lt;- -10^(log10(n)-1)
plot(geweke$z, pch='.', cex=2, ylab='z', xlab='i',
     sub=paste0('N:', n, ', k:', k),
     xlim=c(j, n), ylim=c(-5, 5))
lines(1:n, rep(-1.96, n), type='l', col=6)
lines(1:n, rep(+1.96, n), type='l', col=6)
lines(1:n, rep(-2.576, n), type='l', col=5)
lines(1:n, rep(+2.576, n), type='l', col=5)
lines(1:n, rep(-3.291, n), type='l', col=4)
lines(1:n, rep(+3.291, n), type='l', col=4)
lines(1:n, rep(-3.891, n), type='l', col=3)
lines(1:n, rep(+3.891, n), type='l', col=3)
lines(1:n, rep(-4.417, n), type='l', col=2)
lines(1:n, rep(+4.417, n), type='l', col=2)
text(c(1, 1), c(-1.96, 1.96), pos=2, cex=0.6, labels='0.95')
text(c(1, 1), c(-2.576, 2.576), pos=2, cex=0.6, labels='0.99')
text(c(1, 1), c(-3.291, 3.291), pos=2, cex=0.6, labels='0.999')
text(c(1, 1), c(-3.891, 3.891), pos=2, cex=0.6, labels='0.9999')
text(c(1, 1), c(-4.417, 4.417), pos=2, cex=0.6, labels='0.99999')

par(mfrow=c(1, 1))

##dev.copy2pdf(file='geweke.rs.pbart.pdf')

## End(Not run)
</code></pre>

<hr>
<h2 id='rtgamma'>Testing truncated Gamma sampling</h2><span id='topic+rtgamma'></span>

<h3>Description</h3>

<p> Truncated Gamma draws are needed for the standard
deviation of the random effects Gibbs conditional.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rtgamma(n, shape, rate, a)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rtgamma_+3A_n">n</code></td>
<td>
<p> Number of samples. </p>
</td></tr>
<tr><td><code id="rtgamma_+3A_shape">shape</code></td>
<td>
<p> Sampling from a truncated Gamma where
<code class="reqn">E[x]=shape/rate</code>.</p>
</td></tr>
<tr><td><code id="rtgamma_+3A_rate">rate</code></td>
<td>
<p> This parameter is the inverse of the scale
which is an alternative representation for the Gamma distribution. </p>
</td></tr>
<tr><td><code id="rtgamma_+3A_a">a</code></td>
<td>
<p> The truncation point, i.e., <code class="reqn">a&lt;x</code>. </p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns <code>n</code> truncated Gamma, i.e., <code class="reqn">Gam(shape, rate)I(a, infinity)</code>.
</p>


<h3>References</h3>

<p>Gentle J. (2013)
Random number generation and Monte Carlo methods.
Springer, New York, NY.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
set.seed(12)
rtgamma(1, 3, 1, 4)
rtgamma(1, 3, 1, 4)

a=rtgamma(10000, 10, 2, 1)
mean(a)
min(a)

</code></pre>

<hr>
<h2 id='rtnorm'>Testing truncated Normal sampling</h2><span id='topic+rtnorm'></span>

<h3>Description</h3>

<p> Truncated Normal latents are necessary to transform a
binary BART into a continuous BART.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rtnorm(n, mean, sd, tau)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rtnorm_+3A_n">n</code></td>
<td>
<p> Number of samples. </p>
</td></tr>
<tr><td><code id="rtnorm_+3A_mean">mean</code></td>
<td>
<p> Mean. </p>
</td></tr>
<tr><td><code id="rtnorm_+3A_sd">sd</code></td>
<td>
<p> Standard deviation. </p>
</td></tr>
<tr><td><code id="rtnorm_+3A_tau">tau</code></td>
<td>
<p> Truncation point. </p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns <code>n</code> truncated Normals, i.e., <code class="reqn">N(mean, sd)I(tau, infinity)</code>.
</p>


<h3>References</h3>

<p>Robert C. (1995)
Simulation of truncated normal variables.
<em>Statistics and computing</em>, <b>5(2)</b>, 121&ndash;125.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pbart">pbart</a>, <a href="#topic+lbart">lbart</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
set.seed(12)

rtnorm(1, 0, 1, 3)
rtnorm(1, 0, 1, 3)


</code></pre>

<hr>
<h2 id='spectrum0ar'>Estimate spectral density at zero</h2><span id='topic+spectrum0ar'></span>

<h3>Description</h3>

<p>The spectral density at frequency zero is estimated by fitting an
autoregressive model.  <code>spectrum0(x)/length(x)</code> estimates the
variance of <code>mean(x)</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spectrum0ar(x)
</code></pre>


<h3>Arguments</h3>

 <table>
<tr><td><code id="spectrum0ar_+3A_x">x</code></td>
<td>
<p>Matrix of MCMC chains: the rows are the samples and
the columns are different &quot;parameters&quot;.  For BART, generally, the
columns are estimates of <code class="reqn">f</code>.  For <code>pbart</code>, they are
different subjects.  For <code>surv.bart</code>, they are different subjects
at a grid of times.
</p>
</td></tr></table>


<h3>Details</h3>

<p>The <code>ar()</code> function to fit an autoregressive model to the time
series x. For multivariate time series, separate models are fitted for
each column. The value of the spectral density at zero is then given
by a well-known formula.  Adapted from the <code>spectrum0.ar</code> function of
the coda package which passes <code>mcmc</code> objects as arguments
rather than matrices.
</p>


<h3>Value</h3>

<p>A list with the following values
</p>
<table>
<tr><td><code>spec</code></td>
<td>
<p>The predicted value of the spectral density at frequency zero.</p>
</td></tr>
<tr><td><code>order</code></td>
<td>
<p>The order of the fitted model</p>
</td></tr>
</table>


<h3>References</h3>

<p>Martyn Plummer, Nicky Best, Kate Cowles and Karen Vines (2006). 
CODA: Convergence Diagnosis and Output Analysis for MCMC, R News, vol 6,
7-11.
</p>
<p>BW Silverman (1986).
Density estimation for statistics and data analysis.
Chapman and Hall, London.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gewekediag">gewekediag</a></code>
</p>

<hr>
<h2 id='srstepwise'>Stepwise Variable Selection Procedure for survreg</h2><span id='topic+srstepwise'></span>

<h3>Description</h3>

<p>This stepwise variable selection procedure can be applied to obtain the
best candidates for a <code>survreg</code> fit.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>srstepwise(x, times, delta, sle = 0.15, sls = 0.15, dist='lognormal')
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="srstepwise_+3A_x">x</code></td>
<td>
<p>Matrix of variables to consider.</p>
</td></tr>
<tr><td><code id="srstepwise_+3A_times">times</code></td>
<td>
<p>The time to an event, if any.</p>
</td></tr>
<tr><td><code id="srstepwise_+3A_delta">delta</code></td>
<td>
<p>The event indicator: 1 for event, 0 for no event.</p>
</td></tr>
<tr><td><code id="srstepwise_+3A_sle">sle</code></td>
<td>
<p>The chosen significance level for entering.</p>
</td></tr>
<tr><td><code id="srstepwise_+3A_sls">sls</code></td>
<td>
<p>The chosen significance level for staying.</p>
</td></tr>
<tr><td><code id="srstepwise_+3A_dist">dist</code></td>
<td>
<p>The distribution to be used by <code>survreg</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Unfortunately, no stepwise procedure exists for <code>survreg</code> models.
Therefore, we provide this brute force method.
</p>


<h3>Value</h3>

<p>Returns a list of indices of variables which have entered and stayed.
</p>


<h3>See Also</h3>

<p><a href="#topic+lung">lung</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
names. &lt;- names(lung)[-(2:3)]
status1 &lt;- ifelse(lung$status==2,1,0)
X &lt;- as.matrix(lung)[ , names.]
vars=srstepwise(X, lung$time, status1)
print(names.[vars])

</code></pre>

<hr>
<h2 id='stratrs'>Perform stratified random sampling to balance outcomes</h2><span id='topic+stratrs'></span>

<h3>Description</h3>

<p>This function is used to perform stratified random
sampling to balance outcomes among the shards.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stratrs(y, C=5, P=0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stratrs_+3A_y">y</code></td>
<td>
<p>The binary/categorical/continuous outcome.</p>
</td></tr>
<tr><td><code id="stratrs_+3A_c">C</code></td>
<td>
<p>The number of shards to break the data set into.</p>
</td></tr>
<tr><td><code id="stratrs_+3A_p">P</code></td>
<td>
<p>For continuous data, we break the range into
P segments via the quantiles.  Specifying, P=20 seems to
work reasonably well.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>To perform BART with large data sets, random sampling is employed
to break the data into <code>C</code> shards.  Each shard should be
balanced with respect to the outcome.  For binary/categorical
outcomes, stratified random sampling is employed with this function.
</p>


<h3>Value</h3>

<p>A vector is returned with each element assigned to a shard.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rs.pbart">rs.pbart</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(12)
x &lt;- rbinom(25000, 1, 0.1)
a &lt;- stratrs(x)
table(a, x)
z &lt;- pmin(rpois(25000, 0.8), 5)
b &lt;- stratrs(z)
table(b, z)
</code></pre>

<hr>
<h2 id='surv.bart'>Survival analysis with BART</h2><span id='topic+surv.bart'></span><span id='topic+mc.surv.bart'></span>

<h3>Description</h3>

<p>Here we have implemented a simple and direct approach to utilize BART in
survival analysis that is very flexible, and is akin to discrete-time
survival analysis.  Following the capabilities of BART, we allow for
maximum flexibility in modeling the dependence of survival times on
covariates.  In particular, we do not impose proportional hazards.
</p>
<p>To elaborate, consider data in the usual form:
<code class="reqn">(t_i, \delta_i, {x}_i)</code> where <code class="reqn">t_i</code> is the event time,
<code class="reqn">\delta_i</code> is an indicator distinguishing events
(<code class="reqn">\delta=1</code>) from right-censoring
(<code class="reqn">\delta=0</code>), <code class="reqn">{x}_i</code> is a vector of covariates, and
<code class="reqn">i=1, ..., N</code>
indexes subjects.
</p>
<p>We denote the <code class="reqn">K</code> distinct event/censoring times by
<code class="reqn">0&lt;t_{(1)}&lt;...&lt;t_{(K)}&lt;\infty</code> thus
taking <code class="reqn">t_{(j)}</code> to be the <code class="reqn">j^{th}</code> order
statistic among distinct observation times and, for convenience,
<code class="reqn">t_{(0)}=0</code>. Now consider event indicators <code class="reqn">y_{ij}</code>
for each subject <code class="reqn">i</code> at each distinct time <code class="reqn">t_{(j)}</code>
up to and including the subject's observation time
<code class="reqn">t_i=t_{(n_i)}</code> with
<code class="reqn">n_i=\sum_j I[t_{(j)}\leq t_i]</code>.
This means <code class="reqn">y_{ij}=0</code> if <code class="reqn">j&lt;n_i</code> and
<code class="reqn">y_{in_i}=\delta_i</code>.
</p>
<p>We then denote by <code class="reqn">p_{ij}</code> the probability
of an event at time <code class="reqn">t_{(j)}</code> conditional on no previous event. We
now write the model for <code class="reqn">y_{ij}</code> as a nonparametric probit
regression of <code class="reqn">y_{ij}</code> on the time <code class="reqn">t_{(j)}</code> and the covariates
<code class="reqn">{x}_i</code>, and then utilize BART for binary responses.  Specifically,
<code class="reqn"> y_{ij}\ =\ \delta_i I[t_i=t_{(j)}],\ j=1, ..., n_i </code>; we have
<code class="reqn">p_{ij} = F(\mu_{ij}),\ \mu_{ij} = \mu_0+f(t_{(j)}, {x}_i)</code> where <code class="reqn">F</code> denotes the standard normal cdf (probit link).
As in the binary
response case, <code class="reqn">f</code> is the sum of many tree models.  </p>


<h3>Usage</h3>

<pre><code class='language-R'>
surv.bart( x.train=matrix(0,0,0),
           y.train=NULL, times=NULL, delta=NULL,
           x.test=matrix(0,0,0),
           K=NULL, events=NULL, ztimes=NULL, zdelta=NULL,
           sparse=FALSE, theta=0, omega=1,
           a=0.5, b=1, augment=FALSE, rho=NULL,
           xinfo=matrix(0,0,0), usequants=FALSE,
           
           rm.const=TRUE, type='pbart',
           ntype=as.integer(
               factor(type, levels=c('wbart', 'pbart', 'lbart'))),
           k=2, power=2, base=.95,
           offset=NULL, tau.num=c(NA, 3, 6)[ntype], 
           ntree=50, numcut=100, ndpost=1000, nskip=250,
           keepevery = 10L,
           
           
           
           
           printevery=100L, 
           
           id=NULL,    ## surv.bart only
           seed=99,    ## mc.surv.bart only
           mc.cores=2, ## mc.surv.bart only
           nice=19L    ## mc.surv.bart only
         )

mc.surv.bart( x.train=matrix(0,0,0),
              y.train=NULL, times=NULL, delta=NULL, 
              x.test=matrix(0,0,0), 
              K=NULL, events=NULL, ztimes=NULL, zdelta=NULL,
              sparse=FALSE, theta=0, omega=1,
              a=0.5, b=1, augment=FALSE, rho=NULL,
              xinfo=matrix(0,0,0), usequants=FALSE,
              
              rm.const=TRUE, type='pbart',
              ntype=as.integer(
                  factor(type, levels=c('wbart', 'pbart', 'lbart'))),
              k=2, power=2, base=.95,
              offset=NULL, tau.num=c(NA, 3, 6)[ntype], 
              ntree=50, numcut=100, ndpost=1000, nskip=250,
              keepevery = 10L,
              
              
              
              
              printevery=100L, 
              
              id=NULL,    ## surv.bart only
              seed=99,    ## mc.surv.bart only
              mc.cores=2, ## mc.surv.bart only
              nice=19L    ## mc.surv.bart only
            )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="surv.bart_+3A_x.train">x.train</code></td>
<td>
<p> Explanatory variables for training (in sample)
data.<br /> Must be a matrix with (as usual) rows corresponding to
observations and columns to variables.<br /> <code>surv.bart</code> will generate
draws of <code class="reqn">f(t, x)</code> for each <code class="reqn">x</code> which is a row of x.train (note
that the definition of <code>x.train</code> is dependent on whether
<code>y.train</code> has been specified; see below).  </p>
</td></tr>
<tr><td><code id="surv.bart_+3A_y.train">y.train</code></td>
<td>

<p>Binary response dependent variable for training (in sample) data.<br />
If <code>y.train</code> is <code>NULL</code>, then <code>y.train</code> (<code>x.train</code> and
<code>x.test</code>, if specified) are generated
by a call to <code>surv.pre.bart</code> (which require that <code>times</code> and <code>delta</code> be
provided: see below); otherwise, <code>y.train</code> (<code>x.train</code> and
<code>x.test</code>, if specified)
are utilized as given assuming that the data construction has already been performed.
</p>
</td></tr>
<tr><td><code id="surv.bart_+3A_times">times</code></td>
<td>

<p>The time of event or right-censoring.<br />
If <code>y.train</code> is <code>NULL</code>, then <code>times</code> (and <code>delta</code>)
must be provided.
</p>
</td></tr>
<tr><td><code id="surv.bart_+3A_delta">delta</code></td>
<td>

<p>The event indicator: 1 is an event while 0 is censored.<br />
If <code>y.train</code> is <code>NULL</code>, then <code>delta</code> (and <code>times</code>)
must be provided.
</p>
</td></tr>
<tr><td><code id="surv.bart_+3A_x.test">x.test</code></td>
<td>

<p>Explanatory variables for test (out of sample) data.<br />
Must be a matrix and have the same structure as x.train.<br />
<code>surv.bart</code> will generate draws of <code class="reqn">f(t, x)</code> for each <code class="reqn">x</code> which is a row of x.test.
</p>
</td></tr>
<tr><td><code id="surv.bart_+3A_k">K</code></td>
<td>

<p>If provided, then coarsen <code>times</code> per the quantiles
<code class="reqn">1/K, 2/K, ..., K/K</code>.
</p>
</td></tr>
<tr><td><code id="surv.bart_+3A_events">events</code></td>
<td>

<p>If provided, then use for the grid of time points.
</p>
</td></tr>
<tr><td><code id="surv.bart_+3A_ztimes">ztimes</code></td>
<td>

<p>If provided, then these columns of <code>x.train</code> (and
<code>x.test</code> if any) are the times for time-dependent covariates.
They will be transformed into time-dependent covariate sojourn times.
</p>
</td></tr>
<tr><td><code id="surv.bart_+3A_zdelta">zdelta</code></td>
<td>

<p>If provided, then these columns of <code>x.train</code> (and
<code>x.test</code> if any) are the delta for time-dependent covariates.
They will be transformed into time-dependent covariate binary events.
</p>
</td></tr>
<tr><td><code id="surv.bart_+3A_sparse">sparse</code></td>
<td>
<p>Whether to perform variable selection based on a
sparse Dirichlet prior rather than simply uniform; see Linero 2016.</p>
</td></tr>
<tr><td><code id="surv.bart_+3A_theta">theta</code></td>
<td>
<p>Set <code class="reqn">theta</code> parameter; zero means random.</p>
</td></tr>
<tr><td><code id="surv.bart_+3A_omega">omega</code></td>
<td>
<p>Set <code class="reqn">omega</code> parameter; zero means random.</p>
</td></tr>
<tr><td><code id="surv.bart_+3A_a">a</code></td>
<td>
<p>Sparse parameter for <code class="reqn">Beta(a, b)</code> prior:
<code class="reqn">0.5&lt;=a&lt;=1</code> where lower values inducing more sparsity.</p>
</td></tr>
<tr><td><code id="surv.bart_+3A_b">b</code></td>
<td>
<p>Sparse parameter for <code class="reqn">Beta(a, b)</code> prior; typically,
<code class="reqn">b=1</code>.</p>
</td></tr>
<tr><td><code id="surv.bart_+3A_rho">rho</code></td>
<td>
<p>Sparse parameter: typically <code class="reqn">rho=p</code> where <code class="reqn">p</code> is the
number of covariates under consideration.</p>
</td></tr>
<tr><td><code id="surv.bart_+3A_augment">augment</code></td>
<td>
<p>Whether data augmentation is to be performed in sparse
variable selection.</p>
</td></tr>
<tr><td><code id="surv.bart_+3A_xinfo">xinfo</code></td>
<td>
<p> You can provide the cutpoints to BART or let BART
choose them for you.  To provide them, use the <code>xinfo</code>
argument to specify a list (matrix) where the items (rows) are the
covariates and the contents of the items (columns) are the
cutpoints.  </p>
</td></tr>
<tr><td><code id="surv.bart_+3A_usequants">usequants</code></td>
<td>
<p> If <code>usequants=FALSE</code>, then the
cutpoints in <code>xinfo</code> are generated uniformly; otherwise,
if <code>TRUE</code>, uniform quantiles are used for the cutpoints. </p>
</td></tr>

<tr><td><code id="surv.bart_+3A_rm.const">rm.const</code></td>
<td>
<p> Whether or not to remove constant variables.</p>
</td></tr>
<tr><td><code id="surv.bart_+3A_type">type</code></td>
<td>
<p> Whether to employ Albert-Chib, <code>'pbart'</code>, or
Holmes-Held, <code>'lbart'</code>. </p>
</td></tr>
<tr><td><code id="surv.bart_+3A_ntype">ntype</code></td>
<td>
<p> The integer equivalent of <code>type</code> where
<code>'wbart'</code> is 1, <code>'pbart'</code> is 2 and
<code>'lbart'</code> is 3.</p>
</td></tr>
<tr><td><code id="surv.bart_+3A_k">k</code></td>
<td>

<p>k is the number of prior standard deviations <code class="reqn">f(t, x)</code> is away from +/-3.
The bigger k is, the more conservative the fitting will be.
</p>
</td></tr>
<tr><td><code id="surv.bart_+3A_power">power</code></td>
<td>

<p>Power parameter for tree prior.
</p>
</td></tr>
<tr><td><code id="surv.bart_+3A_base">base</code></td>
<td>

<p>Base parameter for tree prior.
</p>
</td></tr>
<tr><td><code id="surv.bart_+3A_offset">offset</code></td>
<td>
<p>  With binary
BART, the centering is <code class="reqn">P(Y=1 | x) = F(f(x) + offset)</code> where
<code>offset</code> defaults to <code>F^{-1}(mean(y.train))</code>.  You can use
the <code>offset</code> parameter to over-ride these defaults.</p>
</td></tr>
<tr><td><code id="surv.bart_+3A_tau.num">tau.num</code></td>
<td>
<p> The numerator in the <code>tau</code> definition, i.e.,
<code>tau=tau.num/(k*sqrt(ntree))</code>. </p>
</td></tr>   



















<tr><td><code id="surv.bart_+3A_ntree">ntree</code></td>
<td>

<p>The number of trees in the sum.
</p>
</td></tr>
<tr><td><code id="surv.bart_+3A_ndpost">ndpost</code></td>
<td>

<p>The number of posterior draws returned.
</p>
</td></tr>
<tr><td><code id="surv.bart_+3A_nskip">nskip</code></td>
<td>

<p>Number of MCMC iterations to be treated as burn in.
</p>
</td></tr>
<tr><td><code id="surv.bart_+3A_printevery">printevery</code></td>
<td>

<p>As the MCMC runs, a message is printed every printevery draws.
</p>
</td></tr>
<tr><td><code id="surv.bart_+3A_keepevery">keepevery</code></td>
<td>

<p>Every keepevery draw is kept to be returned to the user.<br />
A &ldquo;draw&rdquo; will consist of values <code class="reqn">f^*(t, x)</code>
at <code class="reqn">x</code> = rows from the train(optionally) and test data, where <code class="reqn">f^*</code> denotes
the current draw of <code class="reqn">f</code>.
</p>
</td></tr>



<tr><td><code id="surv.bart_+3A_numcut">numcut</code></td>
<td>

<p>The number of possible values of c (see usequants).
If a single number if given, this is used for all variables.
Otherwise a vector with length equal to ncol(x.train) is required,
where the <code class="reqn">i^{th}</code> element gives the number of c used for
the <code class="reqn">i^{th}</code> variable in x.train.
If usequants is false, numcut equally spaced cutoffs
are used covering the range of values in the corresponding
column of x.train.  If usequants is true, then  min(numcut, the number of unique values in the
corresponding columns of x.train - 1) c values are used.
</p>
</td></tr>

















<tr><td><code id="surv.bart_+3A_id">id</code></td>
<td>

<p><code>surv.bart</code> only: unique identifier added to returned list.
</p>
</td></tr>
<tr><td><code id="surv.bart_+3A_seed">seed</code></td>
<td>

<p><code>mc.surv.bart</code> only: seed required for reproducible MCMC.
</p>
</td></tr>
<tr><td><code id="surv.bart_+3A_mc.cores">mc.cores</code></td>
<td>

<p><code>mc.surv.bart</code> only: number of cores to employ in parallel.
</p>
</td></tr>
<tr><td><code id="surv.bart_+3A_nice">nice</code></td>
<td>

<p><code>mc.surv.bart</code> only: set the job niceness.  The default
niceness is 19: niceness goes from 0 (highest) to 19 (lowest).
</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>surv.bart</code> returns an object of type <code>survbart</code> which is
essentially a list.  Besides the items listed
below, the list has a <code>binaryOffset</code> component giving the value
used, a <code>times</code> component giving the unique times, <code>K</code>
which is the number of unique times, <code>tx.train</code> and
<code>tx.test</code>, if any.
</p>
<table>
<tr><td><code>yhat.train</code></td>
<td>

<p>A matrix with ndpost rows and nrow(x.train) columns.
Each row corresponds to a draw <code class="reqn">f^*</code> from the posterior of <code class="reqn">f</code>
and each column corresponds to a row of x.train.
The <code class="reqn">(i,j)</code> value is <code class="reqn">f^*(t, x)</code> for the <code class="reqn">i^{th}</code> kept draw of <code class="reqn">f</code>
and the <code class="reqn">j^{th}</code> row of x.train.<br />
Burn-in is dropped.
</p>
</td></tr>
<tr><td><code>yhat.test</code></td>
<td>
<p>Same as yhat.train but now the x's are the rows of the test data.</p>
</td></tr>
<tr><td><code>surv.test</code></td>
<td>
<p>The survival function, <code class="reqn">S(t|x)</code>, where x's are the rows of the test data.</p>
</td></tr>
<tr><td><code>yhat.train.mean</code></td>
<td>
<p>train data fits = mean of yhat.train columns.</p>
</td></tr>
<tr><td><code>yhat.test.mean</code></td>
<td>
<p>test data fits = mean of yhat.test columns.</p>
</td></tr>
<tr><td><code>surv.test.mean</code></td>
<td>
<p>mean of surv.test columns.</p>
</td></tr>
<tr><td><code>varcount</code></td>
<td>
<p>a matrix with ndpost rows and nrow(x.train) columns.
Each row is for a draw. For each variable (corresponding to the columns),
the total count of the number of times
that variable is used in a tree decision rule (over all trees) is given.</p>
</td></tr>
</table>
<p>Note that yhat.train and yhat.test are
<code class="reqn">f(t, x)</code> + <code>binaryOffset</code>.  If you want draws of the probability
<code class="reqn">P(Y=1 | t, x)</code> you need to apply the normal cdf (<code>pnorm</code>)
to these values.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+surv.pre.bart">surv.pre.bart</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## load survival package for the advanced lung cancer example
data(lung)

N &lt;- length(lung$status)

table(lung$ph.karno, lung$pat.karno)

## if physician's KPS unavailable, then use the patient's
h &lt;- which(is.na(lung$ph.karno))
lung$ph.karno[h] &lt;- lung$pat.karno[h]

times &lt;- lung$time
delta &lt;- lung$status-1 ##lung$status: 1=censored, 2=dead
##delta: 0=censored, 1=dead

## this study reports time in days rather than weeks or months
## coarsening from days to weeks or months will reduce the computational burden
##times &lt;- ceiling(times/30)
times &lt;- ceiling(times/7)  ## weeks

table(times)
table(delta)

## matrix of observed covariates
x.train &lt;- cbind(lung$sex, lung$age, lung$ph.karno)

## lung$sex:        Male=1 Female=2
## lung$age:        Age in years
## lung$ph.karno:   Karnofsky performance score (dead=0:normal=100:by=10)
##                  rated by physician

dimnames(x.train)[[2]] &lt;- c('M(1):F(2)', 'age(39:82)', 'ph.karno(50:100:10)')

table(x.train[ , 1])
summary(x.train[ , 2])
table(x.train[ , 3])

##test BART with token run to ensure installation works
set.seed(99)
post &lt;- surv.bart(x.train=x.train, times=times, delta=delta,
                  nskip=1, ndpost=1, keepevery=1)

## Not run: 
## run one long MCMC chain in one process
## set.seed(99)
## post &lt;- surv.bart(x.train=x.train, times=times, delta=delta, x.test=x.test)

## in the interest of time, consider speeding it up by parallel processing
## run "mc.cores" number of shorter MCMC chains in parallel processes
post &lt;- mc.surv.bart(x.train=x.train, times=times, delta=delta,
                     mc.cores=8, seed=99)

pre &lt;- surv.pre.bart(times=times, delta=delta, x.train=x.train,
                     x.test=x.train)

K &lt;- pre$K
M &lt;- nrow(post$yhat.train)

pre$tx.test &lt;- rbind(pre$tx.test, pre$tx.test)
pre$tx.test[ , 2] &lt;- c(rep(1, N*K), rep(2, N*K))
## sex pushed to col 2, since time is always in col 1

pred &lt;- predict(post, newdata=pre$tx.test, mc.cores=8)

pd &lt;- matrix(nrow=M, ncol=2*K)

for(j in 1:K) {
    h &lt;- seq(j, N*K, by=K)
    pd[ , j] &lt;- apply(pred$surv.test[ , h], 1, mean)
    pd[ , j+K] &lt;- apply(pred$surv.test[ , h+N*K], 1, mean)
}

pd.mu  &lt;- apply(pd, 2, mean)
pd.025 &lt;- apply(pd, 2, quantile, probs=0.025)
pd.975 &lt;- apply(pd, 2, quantile, probs=0.975)

males &lt;- 1:K
females &lt;- males+K

plot(c(0, pre$times), c(1, pd.mu[males]), type='s', col='blue',
     ylim=0:1, ylab='S(t, x)', xlab='t (weeks)',
     main=paste('Advanced Lung Cancer ex. (BART::lung)',
                "Friedman's partial dependence function",
                'Male (blue) vs. Female (red)', sep='\n'))
lines(c(0, pre$times), c(1, pd.025[males]), col='blue', type='s', lty=2)
lines(c(0, pre$times), c(1, pd.975[males]), col='blue', type='s', lty=2)
lines(c(0, pre$times), c(1, pd.mu[females]), col='red', type='s')
lines(c(0, pre$times), c(1, pd.025[females]), col='red', type='s', lty=2)
lines(c(0, pre$times), c(1, pd.975[females]), col='red', type='s', lty=2)



## End(Not run)
</code></pre>

<hr>
<h2 id='surv.pre.bart'>Data construction for survival analysis with BART</h2><span id='topic+surv.pre.bart'></span>

<h3>Description</h3>

<p>Survival data contained in <code class="reqn">(t,\delta, x)</code> must be translated to data
suitable for the BART survival analysis model; see <code>surv.bart</code> for more details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>surv.pre.bart( times, delta, x.train=NULL, x.test=NULL,
               K=NULL, events=NULL, ztimes=NULL, zdelta=NULL )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="surv.pre.bart_+3A_times">times</code></td>
<td>

<p>The time of event or right-censoring.<br />
</p>
</td></tr>
<tr><td><code id="surv.pre.bart_+3A_delta">delta</code></td>
<td>

<p>The event indicator: 1 is an event while 0 is censored.<br />
</p>
</td></tr>
<tr><td><code id="surv.pre.bart_+3A_x.train">x.train</code></td>
<td>

<p>Explanatory variables for training (in sample) data.<br />
If provided, must be a matrix
with (as usual) rows corresponding to observations and columns to variables.<br />
</p>
</td></tr>
<tr><td><code id="surv.pre.bart_+3A_x.test">x.test</code></td>
<td>

<p>Explanatory variables for test (out of sample) data.<br />
If provided, must be a matrix and have the same structure as x.train.<br />
</p>
</td></tr>
<tr><td><code id="surv.pre.bart_+3A_k">K</code></td>
<td>

<p>If provided, then coarsen <code>times</code> per the quantiles
<code class="reqn">1/K, 2/K, ..., K/K</code>.
</p>
</td></tr>
<tr><td><code id="surv.pre.bart_+3A_events">events</code></td>
<td>

<p>If provided, then use for the grid of time points.
</p>
</td></tr>
<tr><td><code id="surv.pre.bart_+3A_ztimes">ztimes</code></td>
<td>

<p>If provided, then these columns of <code>x.train</code> (and
<code>x.test</code> if any) are the times for time-dependent covariates.
They will be transformed into time-dependent covariate sojourn times.
</p>
</td></tr>
<tr><td><code id="surv.pre.bart_+3A_zdelta">zdelta</code></td>
<td>

<p>If provided, then these columns of <code>x.train</code> (and
<code>x.test</code> if any) are the delta for time-dependent covariates.
They will be transformed into time-dependent covariate binary events.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>surv.pre.bart</code> returns a list.
Besides the items listed below, the list has
a <code>times</code> component giving the unique times and <code>K</code> which is the number of
unique times.
</p>
<table>
<tr><td><code>y.train</code></td>
<td>
<p>A vector of binary responses.</p>
</td></tr>
<tr><td><code>tx.train</code></td>
<td>
<p>A matrix with rows consisting of time and the covariates of the training data.</p>
</td></tr>
<tr><td><code>tx.test</code></td>
<td>
<p>A matrix with rows consisting of time and the
covariates of the test data, if any.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+surv.bart">surv.bart</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## load the advanced lung cancer example
data(lung)

group &lt;- -which(is.na(lung[ , 7])) ## remove missing row for ph.karno
times &lt;- lung[group, 2]   ##lung$time
delta &lt;- lung[group, 3]-1 ##lung$status: 1=censored, 2=dead
                          ##delta: 0=censored, 1=dead

summary(times)
table(delta)

x.train &lt;- as.matrix(lung[group, c(4, 5, 7)]) ## matrix of observed covariates
## lung$age:        Age in years
## lung$sex:        Male=1 Female=2
## lung$ph.karno:   Karnofsky performance score (dead=0:normal=100:by=10)
##                  rated by physician

dimnames(x.train)[[2]] &lt;- c('age(yr)', 'M(1):F(2)', 'ph.karno(0:100:10)')

summary(x.train[ , 1])
table(x.train[ , 2])
table(x.train[ , 3])

x.test &lt;- matrix(nrow=84, ncol=3) ## matrix of covariate scenarios

dimnames(x.test)[[2]] &lt;- dimnames(x.train)[[2]]

i &lt;- 1

for(age in 5*(9:15)) for(sex in 1:2) for(ph.karno in 10*(5:10)) {
    x.test[i, ] &lt;- c(age, sex, ph.karno)
    i &lt;- i+1
}

pre &lt;- surv.pre.bart(times=times, delta=delta, x.train=x.train, x.test=x.test)
str(pre)
    
</code></pre>

<hr>
<h2 id='transplant'>Liver transplant waiting list</h2><span id='topic+transplant'></span>

<h3>Description</h3>

<p>Subjects on a liver transplant waiting list from 1990-1999, and their
disposition: received a transplant, died while waiting, withdrew from
the list, or censored. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("transplant")</code></pre>


<h3>Format</h3>

<p>A data frame with 815 observations on the following 6 variables.
</p>

<dl>
<dt><code>age</code></dt><dd><p>age at addition to the waiting list</p>
</dd>
<dt><code>sex</code></dt><dd><p><code>m</code> or <code>f</code></p>
</dd>
<dt><code>abo</code></dt><dd><p>blood type: <code>A</code>, <code>B</code>, <code>AB</code>  or <code>O</code></p>
</dd>
<dt><code>year</code></dt><dd><p>year in which they entered the waiting list</p>
</dd>
<dt><code>futime</code></dt><dd><p>time from entry to final disposition</p>
</dd>
<dt><code>event</code></dt><dd><p>final disposition: <code>censored</code>,
<code>death</code>,
<code>ltx</code> or <code>withdraw</code></p>
</dd>
</dl>



<h3>Details</h3>

<p>This represents the transplant experience in a particular region,
over a time period in which liver transplant became much more widely
recognized as a viable treatment modality.
The number of liver transplants rises over the period, but the number of
subjects added to the liver transplant waiting list grew much faster.
Important questions addressed by the data are the change in waiting
time, who waits, and whether there was an consequent increase in deaths
while on the list.
</p>
<p>Blood type is an important consideration.  Donor livers from subjects
with blood type O can be used by patients with A, B, AB or O blood
types, whereas a donor liver from the other types will only be
transplanted to a matching recipient.

Thus type O subjects on the waiting list are at a disadvantage, since
the pool of competitors is larger for type O donor livers.
</p>
<p>This data is of historical interest and provides a useful example of
competing risks, but it has little relevance to current
practice.  Liver allocation policies have evolved and now depend
directly on each individual patient's risk and need, assessments of which are
regularly updated while a patient is on the waiting list.
The overall organ shortage remains acute, however.
</p>


<h3>References</h3>

<p>Kim WR, Therneau TM, Benson JT, Kremers WK, Rosen CB, Gores GJ, Dickson
ER. 
Deaths on the liver transplant waiting list: An analysis of competing risks. 
Hepatology 2006 Feb; 43(2):345-51.
</p>

<hr>
<h2 id='wbart'>BART for continuous outcomes</h2><span id='topic+wbart'></span>

<h3>Description</h3>

<p>BART is a Bayesian &ldquo;sum-of-trees&rdquo; model.<br />
For a numeric response <code class="reqn">y</code>, we have
<code class="reqn">y = f(x) + \epsilon</code>,
where <code class="reqn">\epsilon \sim N(0,\sigma^2)</code>.<br />
</p>
<p><code class="reqn">f</code> is the sum of many tree models.
The goal is to have very flexible inference for the uknown
function <code class="reqn">f</code>.
</p>
<p>In the spirit of &ldquo;ensemble models&rdquo;,
each tree is constrained by a prior to be a weak learner
so that it contributes a
small amount to the overall fit.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>wbart(
   x.train, y.train, x.test=matrix(0.0,0,0),
   sparse=FALSE, theta=0, omega=1,
   a=0.5, b=1, augment=FALSE, rho=NULL,
   xinfo=matrix(0.0,0,0), usequants=FALSE,
   cont=FALSE, rm.const=TRUE,
   sigest=NA, sigdf=3, sigquant=.90,
   k=2.0, power=2.0, base=.95,
   sigmaf=NA, lambda=NA,
   fmean=mean(y.train), w=rep(1,length(y.train)),
   ntree=200L, numcut=100L,
   ndpost=1000L, nskip=100L, keepevery=1L,
   nkeeptrain=ndpost, nkeeptest=ndpost,
   nkeeptestmean=ndpost, nkeeptreedraws=ndpost,
   printevery=100L, transposed=FALSE 
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="wbart_+3A_x.train">x.train</code></td>
<td>

<p>Explanatory variables for training (in sample) data.<br />
May be a matrix or a data frame,
with (as usual) rows corresponding to observations and columns to variables.<br />
If a variable is a factor in a data frame, it is replaced with dummies.
Note that q dummies are created if q&gt;2 and
one dummy is created if q=2, where q is the number of levels of the factor.

<code>wbart</code> will generate draws of <code class="reqn">f(x)</code> for each <code class="reqn">x</code>
which is a row of <code>x.train</code>.
</p>
</td></tr>
<tr><td><code id="wbart_+3A_y.train">y.train</code></td>
<td>

<p>Continuous dependent variable for training (in sample) data.<br />



</p>
</td></tr>
<tr><td><code id="wbart_+3A_x.test">x.test</code></td>
<td>

<p>Explanatory variables for test (out of sample) data.<br />
Should have same structure as x.train.<br />
<code>wbart</code> will generate draws of <code class="reqn">f(x)</code> for each <code class="reqn">x</code> which is a row of x.test.
</p>
</td></tr>
<tr><td><code id="wbart_+3A_sparse">sparse</code></td>
<td>
<p>Whether to perform variable selection based on a
sparse Dirichlet prior rather than simply uniform; see Linero 2016.</p>
</td></tr>
<tr><td><code id="wbart_+3A_theta">theta</code></td>
<td>
<p>Set <code class="reqn">theta</code> parameter; zero means random.</p>
</td></tr>
<tr><td><code id="wbart_+3A_omega">omega</code></td>
<td>
<p>Set <code class="reqn">omega</code> parameter; zero means random.</p>
</td></tr>
<tr><td><code id="wbart_+3A_a">a</code></td>
<td>
<p>Sparse parameter for <code class="reqn">Beta(a, b)</code> prior:
<code class="reqn">0.5&lt;=a&lt;=1</code> where lower values inducing more sparsity.</p>
</td></tr>
<tr><td><code id="wbart_+3A_b">b</code></td>
<td>
<p>Sparse parameter for <code class="reqn">Beta(a, b)</code> prior; typically,
<code class="reqn">b=1</code>.</p>
</td></tr>
<tr><td><code id="wbart_+3A_rho">rho</code></td>
<td>
<p>Sparse parameter: typically <code class="reqn">rho=p</code> where <code class="reqn">p</code> is the
number of covariates under consideration.</p>
</td></tr>
<tr><td><code id="wbart_+3A_augment">augment</code></td>
<td>
<p>Whether data augmentation is to be performed in sparse
variable selection.</p>
</td></tr>
<tr><td><code id="wbart_+3A_xinfo">xinfo</code></td>
<td>
<p> You can provide the cutpoints to BART or let BART
choose them for you.  To provide them, use the <code>xinfo</code>
argument to specify a list (matrix) where the items (rows) are the
covariates and the contents of the items (columns) are the
cutpoints.  </p>
</td></tr>
<tr><td><code id="wbart_+3A_usequants">usequants</code></td>
<td>
<p> If <code>usequants=FALSE</code>, then the
cutpoints in <code>xinfo</code> are generated uniformly; otherwise,
if <code>TRUE</code>, uniform quantiles are used for the cutpoints. </p>
</td></tr>
<tr><td><code id="wbart_+3A_cont">cont</code></td>
<td>
<p> Whether or not to assume all variables are continuous.</p>
</td></tr>
<tr><td><code id="wbart_+3A_rm.const">rm.const</code></td>
<td>
<p> Whether or not to remove constant variables.</p>
</td></tr>
<tr><td><code id="wbart_+3A_sigest">sigest</code></td>
<td>

<p>The prior for the error variance  (<code class="reqn">\sigma^2</code>) is inverted chi-squared
(the standard conditionally conjugate prior).
The prior is specified by choosing the degrees of freedom, a rough estimate of the
corresponding standard deviation and a quantile to put this rough estimate at.
If sigest=NA then the rough estimate will be the usual least squares estimator.
Otherwise the supplied value will be used.

</p>
</td></tr>
<tr><td><code id="wbart_+3A_sigdf">sigdf</code></td>
<td>

<p>Degrees of freedom for error variance prior.

</p>
</td></tr>
<tr><td><code id="wbart_+3A_sigquant">sigquant</code></td>
<td>

<p>The quantile of the prior that the rough estimate (see sigest) is placed at.
The closer the quantile is to 1,
the more aggresive the fit will be as you are putting more prior weight
on error standard deviations (<code class="reqn">\sigma</code>) less than the rough estimate.

</p>
</td></tr>
<tr><td><code id="wbart_+3A_k">k</code></td>
<td>

<p>For numeric y,
k is the number of prior standard deviations <code class="reqn">E(Y|x) = f(x)</code> is away from +/-.5.

k is the number of prior standard deviations <code class="reqn">f(x)</code> is away from +/-3.

The bigger k is, the more conservative the fitting will be.
</p>
</td></tr>
<tr><td><code id="wbart_+3A_power">power</code></td>
<td>

<p>Power parameter for tree prior.
</p>
</td></tr>
<tr><td><code id="wbart_+3A_base">base</code></td>
<td>

<p>Base parameter for tree prior.
</p>
</td></tr>
<tr><td><code id="wbart_+3A_sigmaf">sigmaf</code></td>
<td>

<p>The SD of f.
</p>
</td></tr>
<tr><td><code id="wbart_+3A_lambda">lambda</code></td>
<td>

<p>The scale of the prior for the variance.
</p>
</td></tr>
<tr><td><code id="wbart_+3A_fmean">fmean</code></td>
<td>

<p>BART operates on <code>y.train</code> centered by <code>fmean</code>.
</p>
</td></tr>
<tr><td><code id="wbart_+3A_w">w</code></td>
<td>

<p>Vector of weights which multiply the standard deviation.
</p>
</td></tr>
<tr><td><code id="wbart_+3A_ntree">ntree</code></td>
<td>

<p>The number of trees in the sum.
</p>
</td></tr>
<tr><td><code id="wbart_+3A_numcut">numcut</code></td>
<td>

<p>The number of possible values of c (see usequants).
If a single number if given, this is used for all variables.
Otherwise a vector with length equal to ncol(x.train) is required,
where the <code class="reqn">i^{th}</code> element gives the number of c used for
the <code class="reqn">i^{th}</code> variable in x.train.
If usequants is false, numcut equally spaced cutoffs
are used covering the range of values in the corresponding
column of x.train.  If usequants is true, then  min(numcut, the number of unique values in the
corresponding columns of x.train - 1) c values are used.
</p>
</td></tr>
<tr><td><code id="wbart_+3A_ndpost">ndpost</code></td>
<td>

<p>The number of posterior draws returned.
</p>
</td></tr>
<tr><td><code id="wbart_+3A_nskip">nskip</code></td>
<td>

<p>Number of MCMC iterations to be treated as burn in.
</p>
</td></tr>
<tr><td><code id="wbart_+3A_nkeeptrain">nkeeptrain</code></td>
<td>

<p>Number of MCMC iterations to be returned for train data.
</p>
</td></tr>
<tr><td><code id="wbart_+3A_nkeeptest">nkeeptest</code></td>
<td>

<p>Number of MCMC iterations to be returned for test data.
</p>
</td></tr>
<tr><td><code id="wbart_+3A_nkeeptestmean">nkeeptestmean</code></td>
<td>

<p>Number of MCMC iterations to be returned for test mean.
</p>
</td></tr>
<tr><td><code id="wbart_+3A_nkeeptreedraws">nkeeptreedraws</code></td>
<td>

<p>Number of MCMC iterations to be returned for tree draws.
</p>
</td></tr>
<tr><td><code id="wbart_+3A_printevery">printevery</code></td>
<td>

<p>As the MCMC runs, a message is printed every printevery draws.
</p>
</td></tr>
<tr><td><code id="wbart_+3A_keepevery">keepevery</code></td>
<td>

<p>Every keepevery draw is kept to be returned to the user.<br />




</p>
</td></tr>
<tr><td><code id="wbart_+3A_transposed">transposed</code></td>
<td>

<p>When running <code>wbart</code> in parallel, it is more memory-efficient
to transpose <code>x.train</code> and <code>x.test</code>, if any, prior to
calling <code>mc.wbart</code>.
</p>
</td></tr>





</table>


<h3>Details</h3>

<p>BART is an Bayesian MCMC method.
At each MCMC interation, we produce a draw from the joint posterior
<code class="reqn">(f,\sigma) | (x,y)</code> in the numeric <code class="reqn">y</code> case.

</p>
<p>Thus, unlike a lot of other modelling methods in R, we do not produce a single model object
from which fits and summaries may be extracted.  The output consists of values
<code class="reqn">f^*(x)</code> (and <code class="reqn">\sigma^*</code> in the numeric case) where * denotes a particular draw.
The <code class="reqn">x</code> is either a row from the training data (x.train) or the test data (x.test).
</p>


<h3>Value</h3>







<p><code>wbart</code> returns an object of type <code>wbart</code> which is
essentially a list. 
In the numeric <code class="reqn">y</code> case, the list has components:
</p>
<table>
<tr><td><code>yhat.train</code></td>
<td>

<p>A matrix with ndpost rows and nrow(x.train) columns.
Each row corresponds to a draw <code class="reqn">f^*</code> from the posterior of <code class="reqn">f</code>
and each column corresponds to a row of x.train.
The <code class="reqn">(i,j)</code> value is <code class="reqn">f^*(x)</code> for the <code class="reqn">i^{th}</code> kept draw of <code class="reqn">f</code>
and the <code class="reqn">j^{th}</code> row of x.train.<br />
Burn-in is dropped.
</p>
</td></tr>
<tr><td><code>yhat.test</code></td>
<td>
<p>Same as yhat.train but now the x's are the rows of the test data.</p>
</td></tr>
<tr><td><code>yhat.train.mean</code></td>
<td>
<p>train data fits = mean of yhat.train columns.</p>
</td></tr>
<tr><td><code>yhat.test.mean</code></td>
<td>
<p>test data fits = mean of yhat.test columns.</p>
</td></tr>
<tr><td><code>sigma</code></td>
<td>
<p>post burn in draws of sigma, length = ndpost.</p>
</td></tr>
<tr><td><code>first.sigma</code></td>
<td>
<p>burn-in draws of sigma.</p>
</td></tr>
<tr><td><code>varcount</code></td>
<td>
<p>a matrix with ndpost rows and nrow(x.train) columns.
Each row is for a draw. For each variable (corresponding to the columns),
the total count of the number of times
that variable is used in a tree decision rule (over all trees) is given.</p>
</td></tr>
<tr><td><code>sigest</code></td>
<td>

<p>The rough error standard deviation (<code class="reqn">\sigma</code>) used in the prior.
</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+pbart">pbart</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>##simulate data (example from Friedman MARS paper)
f = function(x){
10*sin(pi*x[,1]*x[,2]) + 20*(x[,3]-.5)^2+10*x[,4]+5*x[,5]
}
sigma = 1.0  #y = f(x) + sigma*z , z~N(0,1)
n = 100      #number of observations
set.seed(99)
x=matrix(runif(n*10),n,10) #10 variables, only first 5 matter
Ey = f(x)
y=Ey+sigma*rnorm(n)
lmFit = lm(y~.,data.frame(x,y)) #compare lm fit to BART later

##test BART with token run to ensure installation works
set.seed(99)
bartFit = wbart(x,y,nskip=5,ndpost=5)

## Not run: 
##run BART
set.seed(99)
bartFit = wbart(x,y)

##compare BART fit to linear matter and truth = Ey
fitmat = cbind(y,Ey,lmFit$fitted,bartFit$yhat.train.mean)
colnames(fitmat) = c('y','Ey','lm','bart')
print(cor(fitmat))

## End(Not run)
</code></pre>

<hr>
<h2 id='xdm20.test'>
A data set used in example of <code>recur.bart</code>. 
</h2><span id='topic+xdm20.test'></span>

<h3>Description</h3>

<p>A matrix containing a 20% random sample of the testing set for a real data
example of recurrent events survival analysis.  There are 100 patients in
the cohort: 50 in the training set and 50 in the testing set.  See the
Reference below (and the References therein) for more detailed
information; a brief synopsis follows.
</p>
<p><code>xdm20.test</code> contains both the training set and the testing set.
There are 798 unique time points so there are 50*798=39900 rows of the
training set followed by 50*798=39900 rows of the testing set.  For
patient's who died prior to the end of follow-up, their external factors
are last value carried forward.  Therefore, we can use <code>xdm20.test</code>
to estimate the cumulative hazard for all patients for all time points.
</p>
<p>The full data set, <code>xdm.test</code>, can be obtained online at
<a href="https://www.mcw.edu/-/media/MCW/Departments/Biostatistics/tr064zip.zip">https://www.mcw.edu/-/media/MCW/Departments/Biostatistics/tr064zip.zip</a>

There are 488 patients in the full cohort: 235 in the training set and 253 in
the testing set.  
</p>
<p><code>xdm.test</code> contains both the training set and the testing set.
There are 798 unique time points so there are 235*798=187530 rows of the
training set followed by 253*798=201894 rows of the testing set.  For
patient's who died prior to the end of follow-up, their external factors
are last value carried forward.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(xdm20.test)</code></pre>


<h3>References</h3>

<p>Sparapani, Rein, Tarima, Jackson, Meurer (2020).  Non-parametric recurrent
events analysis with BART and an application to the hospital admissions
of patients with diabetes. <em>Biostatistics</em> doi:10.1093/biostatistics/kxy032





</p>


<h3>See Also</h3>

<p>xdm20.train</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(xdm20.test)
head(xdm20.test[ , 1:10])
</code></pre>

<hr>
<h2 id='xdm20.train'>
A real data example for <code>recur.bart</code>. 
</h2><span id='topic+xdm20.train'></span>

<h3>Description</h3>

<p>A matrix containing a 20% random sample of the training set for a real
data example of recurrent events survival analysis.  There are 100
patients in the cohort: 50 in the training set and 50 in the testing
set.  The full data set, <code>xdm.train</code>, can be obtained online at
<a href="https://www.mcw.edu/-/media/MCW/Departments/Biostatistics/tr064zip.zip">https://www.mcw.edu/-/media/MCW/Departments/Biostatistics/tr064zip.zip</a>

There are 488 patients in the full cohort: 235 in the training set and
253 in the testing set.  See the Reference below (and the References
therein) for more detailed information; a brief synopsis follows.
</p>
<p>We explored the hospital admissions for a cohort of patients with
diabetes cared for by the Froedtert and Medical College of Wisconsin
health network.  These patients were identified via their Electronic
Health Records (EHR) which include vital signs, diagnoses, procedures,
laboratory values, pharmacy orders and billing data.  This human
subjects research and de-identified data release was approved by the
Medical College of Wisconsin and Froedtert Hospital joint
Institutional Review Board.  To maintain patient privacy, roughly one
fourth of patients were randomly sampled for inclusion as well as other
de-identification procedures.
</p>
<p>We identified likely incident diabetes mellitus type 2 patients by
tabulating their first diagnosis code of primary diabetes (ICD-9 codes
250.x0 and 250.x2) in 2006 or 2007, i.e., no such codes were found for
these patients prior to 2006 for as far back as each patient's records
go which is variable.  We restricted the population to adults aged 21 to
90 by 01/01/2008.  Among the patients treated in this health system, the
vast majority were racially self-identified as either white or black so
our inclusion criteria is restricted to these groups.  Since our
interest is in patients with primary diabetes, we excluded those
patients who were diagnosed with either secondary diabetes or
gestational diabetes.
</p>
<p>For this cohort, we identified every hospital admission between
01/01/2008 and 12/31/2012.  For convenience, follow-up begins on
01/01/2008, rather than from each patient's actual incident diagnosis
date which varied over the preceding 2 years.  Following all patients
concurrently allows us to temporally adapt, via our model, for
seasonal/epidemic hospital admissions such as the H1N1 influenza
outbreak in the US from April to June 2009.
</p>
<p>We investigated the following risk factors: gender, race, age, insurance
status (commercial, government or other), diabetes therapy (insulin,
metformin and/or sulfonylurea), health care charges, relative value
units (RVU), vital signs, laboratory values, comorbidity/complication
diagnoses and procedures/surgeries (we will refer to vital signs and
laboratory values collectively as signs; and comorbidity/complication
diagnoses and procedures/surgeries collectively as conditions).  In
total, we considered 85 covariates of which 82 are external factors as
described above and three are temporal factors: time, <code class="reqn">t</code>, the
counting process, <code class="reqn">N_i(t-)</code>, and the sojourn time, <code class="reqn">v_i(t)</code>.
Among these potential predictors only gender, race and age are
time-independent.  The rest are defined as last value carried forward.
</p>
<p>For insulin, metformin and sulfonylurea, we only had access to
prescription orders (rather than prescription fills) and self-reported
current status of prescription therapy during clinic office visits.
Since, generally, orders are only required after every three fills, and
each fill can be for up to 90 days, we define insulin, metformin and
sulfonylurea as binary indicators which are one if there exists an order
or current status indication within the prior 270 days; otherwise zero.
</p>
<p>Health care charges and relative value units (RVU) are measures related
to the services and procedures delivered.  However, they are so closely
related that recent charges/RVUs are of no practical value in this
analysis.  For example, just prior to a patient's hospital admission on
a non-emergent basis, they often have a series of diagnostic tests and
imaging.  Similarly, for an emergent admission, the patient is often
seen in the emergency department just prior to admission where similar
services are conducted.  We do not consider these charges/RVUs
predictive of an admission because we are interested in identifying
preventive opportunities.  Therefore, we investigate charges/RVUs that
are the sum total of the following moving windows of days prior to any
given date: 31 to 90, 91 to 180, 181 to 300.
</p>
<p>For many patients, some signs were not available for a given date so
they were imputed; similarly, if a sign was not observed within the last
180 days, then it was imputed (except for height which never expires,
weight extended to 365 days and body mass index which is a deterministic
function of the two).  We utilized the Sequential BART missing
imputation method.  However, instead of creating several imputed data
sets, we imputed a new sign at each date when it was missing, i.e., in
order to properly address uncertainty within one data set, a new value
was imputed for each date that it was missing and never carried forward.
</p>
<p>Conditions are binary indicators which are zero until the date of the
first coding and then they are one from then on.  Based on clinical
rationale, we identified 26 conditions (23 comorbidities and 3
procedures/surgeries) which are potential risk factors for a hospital
admission many of which are possible complications of diabetes; besides
clinical merit, these conditions were chosen since they are present in
more than just a few subjects so that they may be informative.
Similarly, we employed 15 general conditions which are the Charlson
diagnoses and 18 general conditions from the RxRisk adult diagnoses
which are defined by prescription orders.  Seven conditions are a
composite of diagnosis codes and prescription orders.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(xdm20.train)</code></pre>


<h3>References</h3>

<p>Sparapani, Rein, Tarima, Jackson, Meurer (2020).  Non-parametric recurrent
events analysis with BART and an application to the hospital admissions
of patients with diabetes. <em>Biostatistics</em> doi:10.1093/biostatistics/kxy032





</p>


<h3>See Also</h3>

<p>xdm20.test</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(xdm20.train)
head(xdm20.train[ , 1:10])
</code></pre>

<hr>
<h2 id='ydm20.train'>
A data set used in example of <code>recur.bart</code>. 
</h2><span id='topic+ydm20.train'></span><span id='topic+ydm20.test'></span>

<h3>Description</h3>

<p>Two vectors containing the training and testing set outcomes for a 20%
random sample for a real data example of recurrent events survival
analysis.  There are 100 patients in the cohort: 50 in the training set
and 50 in the testing set.   See the Reference below (and the References
therein) for more detailed information; a brief synopsis follows.
</p>
<p><code>ydm20.train</code> contains the training set only.  <code>ydm20.test</code> is
provided for completeness; it contains both the training set and the
testing set.  There are 798 unique time points so there are 50*798=39900
rows of the training set followed by 50*798=39900 rows of the testing
set.
</p>
<p>The full data sets, <code>ydm.train</code> and <code>ydm.test</code>, can be
obtained online at
<a href="https://www.mcw.edu/-/media/MCW/Departments/Biostatistics/tr064zip.zip">https://www.mcw.edu/-/media/MCW/Departments/Biostatistics/tr064zip.zip</a>

There are 488 patients in the full cohort: 235 in the training set and
253 in the testing set.
</p>
<p><code>ydm.train</code> contains the training set only.  <code>ydm.test</code>
contains both the training set and the testing set.  There are 798
unique time points so there are 235*798=187530 rows of the training set
followed by 253*798=201894 rows of the testing set. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(ydm20.train)
data(ydm20.test)
</code></pre>


<h3>References</h3>

<p>Sparapani, Rein, Tarima, Jackson, Meurer (2020).  Non-parametric recurrent
events analysis with BART and an application to the hospital admissions
of patients with diabetes. <em>Biostatistics</em> doi:10.1093/biostatistics/kxy032





</p>


<h3>See Also</h3>

<p>xdm20.train</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(ydm20.train)
data(ydm20.test)
table(ydm20.train)
table(ydm20.test)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
