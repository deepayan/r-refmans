<!DOCTYPE html><html lang="en"><head><title>Help for package tensorBSS</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {tensorBSS}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#tensorBSS-package'>
<p>Blind Source Separation Methods for Tensor-Valued Observations</p></a></li>
<li><a href='#ggtaugplot'>
<p>Augmentation plot for each mode of an object of class taug using ggplot2</p></a></li>
<li><a href='#ggtladleplot'>
<p>Ladle plot for each mode of an object of class tladle using ggplot2</p></a></li>
<li><a href='#k_tJADE'>
<p>k-tJADE for Tensor-Valued Observations</p></a></li>
<li><a href='#mFlatten'>
<p>Flattening an Array Along One Mode</p></a></li>
<li><a href='#mModeAutoCovariance'><p>The m-Mode Autocovariance Matrix</p></a></li>
<li><a href='#mModeCovariance'><p>The m-Mode Covariance Matrix</p></a></li>
<li><a href='#plot.tbss'><p>Plot an Object of the Class tbss</p></a></li>
<li><a href='#print.taug'>
<p>Printing an object of class taug</p></a></li>
<li><a href='#print.tladle'>
<p>Printing an object of class tladle</p></a></li>
<li><a href='#selectComponents'><p>Select the Most Informative Components</p></a></li>
<li><a href='#tensorBoot'>
<p>Bootstrapping or Permuting a Data Tensor</p></a></li>
<li><a href='#tensorCentering'><p>Center an Array of Observations</p></a></li>
<li><a href='#tensorStandardize'><p>Standardize an Observation Array</p></a></li>
<li><a href='#tensorTransform'><p>Linear Transformation of Tensors from mth Mode</p></a></li>
<li><a href='#tensorTransform2'>
<p>Linear Transformations of Tensors from Several Modes</p></a></li>
<li><a href='#tensorVectorize'><p>Vectorize an Observation Tensor</p></a></li>
<li><a href='#tFOBI'><p>FOBI for Tensor-Valued Observations</p></a></li>
<li><a href='#tgFOBI'><p>gFOBI for Tensor-Valued Time Series</p></a></li>
<li><a href='#tgJADE'><p>gJADE for Tensor-Valued Time Series</p></a></li>
<li><a href='#tJADE'>
<p>tJADE for Tensor-Valued Observations</p></a></li>
<li><a href='#tMD'><p>Minimum Distance Index of a Kronecker Product</p></a></li>
<li><a href='#tNSS.JD'>
<p>NSS-JD Method for Tensor-Valued Time Series</p></a></li>
<li><a href='#tNSS.SD'>
<p>NSS-SD Method for Tensor-Valued Time Series</p></a></li>
<li><a href='#tNSS.TD.JD'>
<p>TNSS-TD-JD Method for Tensor-Valued Time Series</p></a></li>
<li><a href='#tPCA'><p>PCA for Tensor-Valued Observations</p></a></li>
<li><a href='#tPCAaug'>
<p>Order Determination for Tensorial PCA Using Augmentation</p></a></li>
<li><a href='#tPCAladle'>
<p>Ladle Estimate for tPCA</p></a></li>
<li><a href='#tPP'><p>Projection pursuit for Tensor-Valued Observations</p></a></li>
<li><a href='#tSIR'><p>SIR for Tensor-Valued Observations</p></a></li>
<li><a href='#tSOBI'><p>SOBI for Tensor-Valued Time Series</p></a></li>
<li><a href='#tTUCKER'>
<p>Tucker (2) Transformation for a Tensor</p></a></li>
<li><a href='#zip.test'><p> Handwritten Digit Recognition Data</p></a></li>
<li><a href='#zip.train'><p> Handwritten Digit Recognition Data</p></a></li>
<li><a href='#zip2image'><p> function to convert row of zip file to format used by image()</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Blind Source Separation Methods for Tensor-Valued Observations</td>
</tr>
<tr>
<td>Version:</td>
<td>0.3.9</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-09-12</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Joni Virta &lt;joni.virta@outlook.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Contains several utility functions for manipulating tensor-valued data (centering, multiplication from a single mode etc.) and the implementations of the following blind source separation methods for tensor-valued data: 'tPCA', 'tFOBI', 'tJADE', k-tJADE', 'tgFOBI', 'tgJADE', 'tSOBI', 'tNSS.SD', 'tNSS.JD', 'tNSS.TD.JD', 'tPP' and 'tTUCKER'.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp (&ge; 0.12.3), tensor, tsBSS, ICtest, ggplot2, abind</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo</td>
</tr>
<tr>
<td>Depends:</td>
<td>JADE, R (&ge; 2.10), fICA</td>
</tr>
<tr>
<td>Suggests:</td>
<td>stochvol</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-09-12 12:11:42 UTC; jomivi</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-09-12 16:40:02 UTC</td>
</tr>
<tr>
<td>Author:</td>
<td>Joni Virta <a href="https://orcid.org/0000-0001-7330-8434"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut,
    cre],
  Christoph L. Koesner
    <a href="https://orcid.org/0000-0002-2061-8022"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut],
  Bing Li [aut],
  Klaus Nordhausen <a href="https://orcid.org/0000-0002-3758-8501"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut],
  Hannu Oja [aut],
  Una Radojicic <a href="https://orcid.org/0000-0003-0329-0595"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut]</td>
</tr>
</table>
<hr>
<h2 id='tensorBSS-package'>
Blind Source Separation Methods for Tensor-Valued Observations
</h2><span id='topic+tensorBSS-package'></span><span id='topic+tensorBSS'></span>

<h3>Description</h3>

<p>Contains several utility functions for manipulating tensor-valued data (centering, multiplication from a single mode etc.) and 
the implementations of the following blind source separation methods for tensor-valued data: &lsquo;tPCA&rsquo;, &lsquo;tFOBI&rsquo;, &lsquo;tJADE&rsquo;, &lsquo;k-tJADE&rsquo;, &lsquo;tgFOBI&rsquo;, &lsquo;tgJADE&rsquo;, &lsquo;tSOBI&rsquo;, &lsquo;tNSS.SD&rsquo;, &lsquo;tNSS.JD&rsquo;, &lsquo;tNSS.TD.JD&rsquo;, &lsquo;tPP&rsquo; and &lsquo;tTUCKER&rsquo;.
</p>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> tensorBSS</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 0.3.9</td>
</tr>
<tr>
 <td style="text-align: left;">
Date: </td><td style="text-align: left;"> 2024-09-12</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL (&gt;= 2)</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>



<h3>Author(s)</h3>

<p>Joni Virta, Christoph Koesner, Bing Li, Klaus Nordhausen, Hannu Oja and Una Radojicic
</p>
<p>Maintainer: Joni Virta &lt;joni.virta@outlook.com&gt;
</p>


<h3>References</h3>

<p><cite>Virta, J., Taskinen, S. and Nordhausen, K. (2016), Applying fully tensorial ICA to fMRI data, Signal Processing in Medicine and Biology Symposium (SPMB), 2016 IEEE, <a href="https://doi.org/10.1109/SPMB.2016.7846858">doi:10.1109/SPMB.2016.7846858</a></cite> 
</p>
<p><cite>Virta, J., Li, B., Nordhausen, K. and Oja, H., (2017), Independent component analysis for tensor-valued data, Journal of Multivariate Analysis, <a href="https://doi.org/10.1016/j.jmva.2017.09.008">doi:10.1016/j.jmva.2017.09.008</a></cite>
</p>
<p><cite>Virta, J. and Nordhausen, K., (2017), Blind source separation of tensor-valued time series. Signal Processing 141, 204-216, <a href="https://doi.org/10.1016/j.sigpro.2017.06.008">doi:10.1016/j.sigpro.2017.06.008</a></cite>  
</p>
<p><cite>Virta J., Nordhausen K. (2017): Blind source separation for nonstationary tensor-valued time series, 2017 IEEE 27th International Workshop on Machine Learning for Signal Processing (MLSP), <a href="https://doi.org/10.1109/MLSP.2017.8168122">doi:10.1109/MLSP.2017.8168122</a></cite>
</p>
<p><cite>Virta J., Li B., Nordhausen K., Oja H. (2018): JADE for tensor-valued observations, Journal of Computational and Graphical Statistics, 27, 628 - 637, <a href="https://doi.org/10.1080/10618600.2017.1407324">doi:10.1080/10618600.2017.1407324</a></cite> 
</p>
<p><cite>Virta J., Lietzen N., Ilmonen P., Nordhausen K. (2021): Fast tensorial JADE, Scandinavian Journal of Statistics, 48, 164-187, <a href="https://doi.org/10.1111/sjos.12445">doi:10.1111/sjos.12445</a></cite>
</p>
<p><cite>Radojicic, U., Lietzen, N., Nordhausen, K. and Virta, J. (2021): Dimension estimation in two-dimensional PCA. In S. Loncaric, T. Petkovic and D. Petrinovic (editors) &quot;Proceedings of the 12 International Symposium on Image and Signal Processing and Analysis (ISPA 2021)&quot;, 16-22. <a href="https://doi.org/10.1109/ISPA52656.2021.9552114">doi:10.1109/ISPA52656.2021.9552114</a>.</cite>
</p>
<p><cite>Radojicic, U., Lietzen, N., Nordhausen, K. and Virta, J. (2022): Order determination for tensor-valued observations using data augmentation. &lt;arXiv:2207.10423&gt;.</cite>
</p>
<p><cite>Koesner, C, Nordhausen, K. and Virta, J. (2019), Estimating the signal tensor dimension using tensorial PCA. Manuscript.</cite>
</p>

<hr>
<h2 id='ggtaugplot'>
Augmentation plot for each mode of an object of class taug using ggplot2
</h2><span id='topic+ggtaugplot'></span>

<h3>Description</h3>

<p>The augmentation plot is a measure for deciding about the number of interesting components. Of interest for the augmentation plot, which is quite similar to the ladle plot, is the minimum.
The function offers, however, also the possibility to plot other criterion values that combined make up the actual criterion. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ggtaugplot(x, crit = "gn", type = "l", scales = "free", position = "horizontal",
  ylab = crit, xlab = "component", main = deparse(substitute(x)),  ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ggtaugplot_+3A_x">x</code></td>
<td>
<p>an object of class taug.</p>
</td></tr>
<tr><td><code id="ggtaugplot_+3A_crit">crit</code></td>
<td>
<p>the criterion to be plotted, options are <code>"gn"</code>, <code>"fn"</code>, <code>"phin"</code> and <code>"lambda"</code>.</p>
</td></tr>
<tr><td><code id="ggtaugplot_+3A_type">type</code></td>
<td>
<p>plotting type, either lines <code>l</code> or points <code>p</code>.</p>
</td></tr>
<tr><td><code id="ggtaugplot_+3A_position">position</code></td>
<td>
<p>placement of augmentation plots for separate modes, options are <code>"horizontal"</code> and <code>"vertical"</code>.</p>
</td></tr>
<tr><td><code id="ggtaugplot_+3A_scales">scales</code></td>
<td>
<p>determines whether the x- and y-axis scales are shared or allowed to vary freely across the subplots. The options are: both axes are free (the default, <code>"free"</code>), x-axis is free (<code>"free_x"</code>), y-axis is free (<code>"free_y"</code>), neither is free (<code>"fixed"</code>).</p>
</td></tr>
<tr><td><code id="ggtaugplot_+3A_ylab">ylab</code></td>
<td>
<p>default ylab value.</p>
</td></tr>
<tr><td><code id="ggtaugplot_+3A_xlab">xlab</code></td>
<td>
<p>default xlab value.</p>
</td></tr>
<tr><td><code id="ggtaugplot_+3A_main">main</code></td>
<td>
<p>default title.</p>
</td></tr>
<tr><td><code id="ggtaugplot_+3A_...">...</code></td>
<td>
<p>other arguments for the plotting functions.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The main criterion of the augmentation criterion is the scaled sum of the eigenvalues and the measure of variation of the eigenvectors up to the component of interest.
</p>
<p>The sum is denoted <code>"gn"</code> and the individual parts are <code>"fn"</code> for the measure of the eigenvector variation and <code>"phin"</code> for the scaled eigenvalues.
The last option <code>"lambda"</code> corresponds to the unscaled eigenvalues yielding then a screeplot.
</p>
<p>The plot is drawn separately for each mode of the data.
</p>


<h3>Author(s)</h3>

<p>Klaus Nordhausen, Joni Virta, Una Radojicic
</p>


<h3>References</h3>

<p><cite>Radojicic, U., Lietzen, N., Nordhausen, K. and Virta, J. (2021), On order determinaton in 2D PCA. Manuscript.</cite>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+tPCAaug">tPCAaug</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(ICtest)


# matrix-variate example
n &lt;- 200
sig &lt;- 0.6

Z &lt;- rbind(sqrt(0.7)*rt(n,df=5)*sqrt(3/5),
           sqrt(0.3)*runif(n,-sqrt(3),sqrt(3)),
           sqrt(0.3)*(rchisq(n,df=3)-3)/sqrt(6),
           sqrt(0.9)*(rexp(n)-1),
           sqrt(0.1)*rlogis(n,0,sqrt(3)/pi),
           sqrt(0.5)*(rbeta(n,2,2)-0.5)*sqrt(20)
)

dim(Z) &lt;- c(3, 2, n)

U1 &lt;- rorth(12)[,1:3]
U2 &lt;- rorth(8)[,1:2]
U &lt;- list(U1=U1, U2=U2)
Y &lt;- tensorTransform2(Z,U,1:2)
EPS &lt;- array(rnorm(12*8*n, mean=0, sd=sig), dim=c(12,8,n))
X &lt;- Y + EPS


TEST &lt;- tPCAaug(X)
TEST
ggtaugplot(TEST)

# higher order tensor example

Z2 &lt;- rnorm(n*3*2*4*6)

dim(Z2) &lt;- c(3,2,4,6,n)

U2.1 &lt;- rorth(10)[ ,1:3]
U2.2 &lt;- rorth(8)[ ,1:2]
U2.3 &lt;- rorth(5)[ ,1:4]
U2.4 &lt;- rorth(15)[ ,1:6]

U2 &lt;- list(U1 = U2.1, U2 = U2.2, U3 = U2.3, U4 = U2.4)
Y2 &lt;- tensorTransform2(Z2, U2, 1:4)
EPS2 &lt;- array(rnorm(10*8*5*15*n, mean=0, sd=sig), dim=c(10, 8, 5, 15, n))
X2 &lt;- Y2 + EPS2


TEST2 &lt;- tPCAaug(X2)
ggtaugplot(TEST2, crit = "lambda", position = "vertical",
 scales = "free_x")
</code></pre>

<hr>
<h2 id='ggtladleplot'>
Ladle plot for each mode of an object of class tladle using ggplot2
</h2><span id='topic+ggtladleplot'></span>

<h3>Description</h3>

<p>The ladle plot is a measure for deciding about the number of interesting components. Of interest for the ladle criterion is the minimum.
The function here offers however also to plot other criterion values which are part of the actual ladle criterion. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ggtladleplot(x, crit = "gn", type = "l", scales = "free", 
  position = "horizontal", ylab = crit,  
  xlab = "component", main = deparse(substitute(x)), ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ggtladleplot_+3A_x">x</code></td>
<td>
<p>an object of class ladle.</p>
</td></tr>
<tr><td><code id="ggtladleplot_+3A_crit">crit</code></td>
<td>
<p>the criterion to be plotted, options are <code>"gn"</code>, <code>"fn"</code>, <code>"phin"</code> and <code>"lambda"</code>.</p>
</td></tr>
<tr><td><code id="ggtladleplot_+3A_type">type</code></td>
<td>
<p>plotting type, either lines <code>l</code> or points <code>p</code>.</p>
</td></tr>
<tr><td><code id="ggtladleplot_+3A_position">position</code></td>
<td>
<p>placement of augmentation plots for separate modes, options are <code>"horizontal"</code> and <code>"vertical"</code>.</p>
</td></tr>
<tr><td><code id="ggtladleplot_+3A_scales">scales</code></td>
<td>
<p>determines whether the x- and y-axis scales are shared or allowed to vary freely across the subplots. The options are: both axes are free (the default, <code>"free"</code>), x-axis is free (<code>"free_x"</code>), y-axis is free (<code>"free_y"</code>), neither is free (<code>"fixed"</code>).</p>
</td></tr>
<tr><td><code id="ggtladleplot_+3A_ylab">ylab</code></td>
<td>
<p>default ylab value.</p>
</td></tr>
<tr><td><code id="ggtladleplot_+3A_xlab">xlab</code></td>
<td>
<p>default xlab value.</p>
</td></tr>
<tr><td><code id="ggtladleplot_+3A_main">main</code></td>
<td>
<p>default title.</p>
</td></tr>
<tr><td><code id="ggtladleplot_+3A_...">...</code></td>
<td>
<p>other arguments for the plotting functions.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The main criterion of the ladle is the scaled sum of the eigenvalues and the measure of variation of the eigenvectors up to the component of interest.
</p>
<p>The sum is denoted <code>"gn"</code> and the individual parts are <code>"fn"</code> for the measure of the eigenvector variation and <code>"phin"</code> for the scaled eigenvalues.
The last option <code>"lambda"</code> corresponds to the unscaled eigenvalues yielding then a screeplot.
</p>
<p>The plot is drawn separately for each mode of the data.
</p>


<h3>Author(s)</h3>

<p>Klaus Nordhausen, Joni Virta
</p>


<h3>References</h3>

<p><cite>Koesner, C, Nordhausen, K. and Virta, J. (2019), Estimating the signal tensor dimension using tensorial PCA. Manuscript.</cite>
</p>
<p><cite>Luo, W. and Li, B. (2016), Combining Eigenvalues and Variation of Eigenvectors for Order Determination, Biometrika, 103. 875&ndash;887. &lt;doi:10.1093/biomet/asw051&gt;</cite>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+tPCAladle">tPCAladle</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(ICtest)
n &lt;- 500
sig &lt;- 0.6

Z &lt;- rbind(sqrt(0.7)*rt(n,df=5)*sqrt(3/5),
           sqrt(0.3)*runif(n,-sqrt(3),sqrt(3)),
           sqrt(0.3)*(rchisq(n,df=3)-3)/sqrt(6),
           sqrt(0.9)*(rexp(n)-1),
           sqrt(0.1)*rlogis(n,0,sqrt(3)/pi),
           sqrt(0.5)*(rbeta(n,2,2)-0.5)*sqrt(20)
)

dim(Z) &lt;- c(3, 2, n)

U1 &lt;- rorth(12)[,1:3]
U2 &lt;- rorth(8)[,1:2]
U &lt;- list(U1=U1, U2=U2)
Y &lt;- tensorTransform2(Z,U,1:2)
EPS &lt;- array(rnorm(12*8*n, mean=0, sd=sig), dim=c(12,8,n))
X &lt;- Y + EPS


TEST &lt;- tPCAladle(X, n.boot = 100)
TEST
ggtladleplot(TEST, crit = "gn")
ggtladleplot(TEST, crit = "fn")
ggtladleplot(TEST, crit = "phin")
ggtladleplot(TEST, crit = "lambda")
</code></pre>

<hr>
<h2 id='k_tJADE'>
k-tJADE for Tensor-Valued Observations 
</h2><span id='topic+k_tJADE'></span>

<h3>Description</h3>

<p>Computes the faster &ldquo;k&rdquo;-version of tensorial JADE in an independent component model. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>k_tJADE(x, k = NULL, maxiter = 100, eps = 1e-06)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="k_tJADE_+3A_x">x</code></td>
<td>
<p>Numeric array of an order at least two. It is assumed that the last dimension corresponds to the sampling units.</p>
</td></tr>
<tr><td><code id="k_tJADE_+3A_k">k</code></td>
<td>
<p>A vector with one less element than dimensions in <code>x</code>. The elements of <code>k</code> give upper bounds for cumulant matrix indices we diagonalize in each mode. Lower values mean faster computation times. The default value <code>NULL</code> puts k equal to 1 in each mode (the fastest choice).</p>
</td></tr>
<tr><td><code id="k_tJADE_+3A_maxiter">maxiter</code></td>
<td>
<p>Maximum number of iterations. Passed on to <code><a href="JADE.html#topic+rjd">rjd</a></code>.</p>
</td></tr>
<tr><td><code id="k_tJADE_+3A_eps">eps</code></td>
<td>
<p>Convergence tolerance. Passed on to <code><a href="JADE.html#topic+rjd">rjd</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>It is assumed that <code class="reqn">S</code> is a tensor (array) of size <code class="reqn">p_1 \times p_2 \times \ldots \times p_r</code> with mutually independent elements and measured on <code class="reqn">N</code> units. The tensor independent component model further assumes that the tensors S are mixed from each mode
<code class="reqn">m</code> by the mixing matrix <code class="reqn">A_m</code>, <code class="reqn">m = 1, \ldots, r</code>, yielding the observed data <code class="reqn">X</code>. In R the sample of <code class="reqn">X</code> is saved as an <code><a href="base.html#topic+array">array</a></code> of dimensions
<code class="reqn">p_1, p_2, \ldots, p_r, N</code>.
</p>
<p><code>k_tJADE</code> recovers then based on <code>x</code> the underlying independent components <code class="reqn">S</code> by estimating the <code class="reqn">r</code> unmixing matrices 
<code class="reqn">W_1, \ldots, W_r</code> using fourth joint moments at the same time in a more efficient way than <code><a href="#topic+tFOBI">tFOBI</a></code> but also in fewer numbers than <code><a href="#topic+tJADE">tJADE</a></code>. <code>k_tJADE</code> diagonalizes in each mode only those cumulant matrices <code class="reqn">C^{ij}</code> for which <code class="reqn">|i - j| &lt; k_m</code>.
</p>
<p>If <code>x</code> is a matrix, that is, <code class="reqn">r = 1</code>, the method reduces to JADE and the function calls <code><a href="JADE.html#topic+k_JADE">k_JADE</a></code>.
</p>


<h3>Value</h3>

<p>A list with class 'tbss', inheriting from class 'bss', containing the following components: 
</p>
<table role = "presentation">
<tr><td><code>S</code></td>
<td>
<p>Array of the same size as x containing the independent components.</p>
</td></tr>
<tr><td><code>W</code></td>
<td>
<p>List containing all the unmixing matrices</p>
</td></tr>
<tr><td><code>Xmu</code></td>
<td>
<p>The data location.</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>The used vector of <code class="reqn">k</code>-values.</p>
</td></tr>
<tr><td><code>datatype</code></td>
<td>
<p>Character string with value &quot;iid&quot;. Relevant for <code><a href="#topic+plot.tbss">plot.tbss</a></code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Joni Virta
</p>


<h3>References</h3>

<p><cite>Miettinen, J., Nordhausen, K., Oja, H. and Taskinen, S. (2013), Fast Equivariant JADE, 
In the Proceedings of <em>38th IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2013)</em>, 6153&ndash;6157, <a href="https://doi.org/10.1109/ICASSP.2013.6638847">doi:10.1109/ICASSP.2013.6638847</a></cite>
</p>
<p><cite>Virta J., Li B., Nordhausen K., Oja H. (2018): JADE for tensor-valued observations, Journal of Computational and Graphical Statistics, 27, 628-637, <a href="https://doi.org/10.1080/10618600.2017.1407324">doi:10.1080/10618600.2017.1407324</a></cite> 
</p>
<p><cite>Virta J., Lietzen N., Ilmonen P., Nordhausen K. (2021): Fast tensorial JADE, Scandinavian Journal of Statistics, 48, 164-187, <a href="https://doi.org/10.1111/sjos.12445">doi:10.1111/sjos.12445</a></cite>
</p>


<h3>See Also</h3>

<p><code><a href="JADE.html#topic+k_JADE">k_JADE</a></code>, <code><a href="#topic+tJADE">tJADE</a></code>, <code><a href="JADE.html#topic+JADE">JADE</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 1000
S &lt;- t(cbind(rexp(n)-1,
             rnorm(n),
             runif(n, -sqrt(3), sqrt(3)),
             rt(n,5)*sqrt(0.6),
             (rchisq(n,1)-1)/sqrt(2),
             (rchisq(n,2)-2)/sqrt(4)))
             
dim(S) &lt;- c(3, 2, n)

A1 &lt;- matrix(rnorm(9), 3, 3)
A2 &lt;- matrix(rnorm(4), 2, 2)

X &lt;- tensorTransform(S, A1, 1)
X &lt;- tensorTransform(X, A2, 2)


k_tjade &lt;- k_tJADE(X)

MD(k_tjade$W[[1]], A1)
MD(k_tjade$W[[2]], A2) 
tMD(k_tjade$W, list(A1, A2))


k_tjade &lt;- k_tJADE(X, k = c(2, 1))

MD(k_tjade$W[[1]], A1)
MD(k_tjade$W[[2]], A2) 
tMD(k_tjade$W, list(A1, A2))
</code></pre>

<hr>
<h2 id='mFlatten'>
Flattening an Array Along One Mode
</h2><span id='topic+mFlatten'></span>

<h3>Description</h3>

<p>Reshapes a higher order array (tensor) into a matrix with a process known as m-mode flattening or matricization.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mFlatten(x, m)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mFlatten_+3A_x">x</code></td>
<td>

<p>an <code class="reqn">(r + 1)</code>-dimensional array with <code class="reqn">r \geq 2</code>. The final mode is understood to correspond to the observations (i.e., its length is usually the sample size n). 
</p>
</td></tr>
<tr><td><code id="mFlatten_+3A_m">m</code></td>
<td>

<p>an integer between <code class="reqn">1</code> and <code class="reqn">r</code> signifying the mode along which the array should be flattened. Note that the flattening cannot be done w.r.t. the final <code class="reqn">(r + 1)</code>th mode.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If the original tensor <code>x</code> has the size <code class="reqn">p_1 \times \cdots \times p_r \times n</code>, then <code>mFlatten(x, m)</code> returns tensor of size <code class="reqn">p_m \times p_1 \cdots p_{m - 1} p_{m + 1} \cdots p_r \times n</code> obtained by gathering all <code class="reqn">m</code>-mode vectors of <code>x</code> into a wide matrix (an <code class="reqn">m</code>-mode vector of <code>x</code> is any vector of length <code class="reqn">p_m</code> obtained by varying the <code class="reqn">m</code>th index and holding the other indices constant).
</p>


<h3>Value</h3>

<p>The <code class="reqn">m</code>-mode flattened 3rd order tensor of size <code class="reqn">p_m \times p_1 \cdots p_{m - 1} p_{m + 1} \cdots p_r \times n</code>.
</p>


<h3>Author(s)</h3>

<p>Joni Virta
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 10
x &lt;- t(cbind(rnorm(n, mean = 0),
             rnorm(n, mean = 1),
             rnorm(n, mean = 2),
             rnorm(n, mean = 3),
             rnorm(n, mean = 4),
             rnorm(n, mean = 5)))
             
dim(x) &lt;- c(3, 2, n)

dim(mFlatten(x, 1))
dim(mFlatten(x, 2))
</code></pre>

<hr>
<h2 id='mModeAutoCovariance'>The m-Mode Autocovariance Matrix</h2><span id='topic+mModeAutoCovariance'></span>

<h3>Description</h3>

<p>Estimates the m-mode autocovariance matrix from an array of array-valued observations with the specified lag.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mModeAutoCovariance(x, m, lag, center = TRUE, normalize = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mModeAutoCovariance_+3A_x">x</code></td>
<td>
<p>Array of order higher than two with the last dimension corresponding to the sampling units.</p>
</td></tr>
<tr><td><code id="mModeAutoCovariance_+3A_m">m</code></td>
<td>
<p>The mode with respect to which the autocovariance matrix is to be computed.</p>
</td></tr>
<tr><td><code id="mModeAutoCovariance_+3A_lag">lag</code></td>
<td>
<p>The lag with respect to which the autocovariance matrix is to be computed.</p>
</td></tr>
<tr><td><code id="mModeAutoCovariance_+3A_center">center</code></td>
<td>
<p>Logical, indicating whether the observations should be centered prior to computing the autocovariance matrix. Default is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="mModeAutoCovariance_+3A_normalize">normalize</code></td>
<td>
<p>Logical, indicating whether the resulting matrix is divided by <code>p_1 ... p_{m-1} p_{m+1} ... p_r</code> or not. Default is <code>TRUE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The m-mode autocovariance matrix provides a higher order analogy for the ordinary autocovariance matrix of a random vector and is computed for a random tensor <code class="reqn">X_t</code> of size <code class="reqn">p_1 \times p_2 \times \ldots \times p_r</code> as <code class="reqn">Cov_{m \tau}(X_t) = E(X_t^{(m)} X_{t+\tau}^{(m)T})/(p_1 \ldots p_{m-1} p_{m+1} \ldots p_r)</code>, where <code class="reqn">X_t^{(m)}</code> is the centered <code class="reqn">m</code>-flattening of <code class="reqn">X_t</code> and <code class="reqn">\tau</code> is the desired <code>lag</code>. The algorithm computes the estimate of this based on the sample <code>x</code>.
</p>


<h3>Value</h3>

<p>The <code>m</code>-mode autocovariance matrix of <code>x</code> with respect to <code>lag</code> having the size <code class="reqn">p_m \times p_m</code>.
</p>


<h3>Author(s)</h3>

<p>Joni Virta
</p>


<h3>References</h3>

<p><cite>Virta, J. and Nordhausen, K., (2017), Blind source separation of tensor-valued time series, Signal Processing, 141, 204-216, <a href="https://doi.org/10.1016/j.sigpro.2017.06.008">doi:10.1016/j.sigpro.2017.06.008</a></cite> 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mModeCovariance">mModeCovariance</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 1000
S &lt;- t(cbind(as.vector(arima.sim(n = n, list(ar = 0.9))),
             as.vector(arima.sim(n = n, list(ar = -0.9))),
             as.vector(arima.sim(n = n, list(ma = c(0.5, -0.5)))),
             as.vector(arima.sim(n = n, list(ar = c(-0.5, -0.3)))),
             as.vector(arima.sim(n = n, list(ar = c(0.5, -0.3, 0.1, -0.1), ma=c(0.7, -0.3)))),
             as.vector(arima.sim(n = n, list(ar = c(-0.7, 0.1), ma = c(0.9, 0.3, 0.1, -0.1))))))
dim(S) &lt;- c(3, 2, n)

mModeAutoCovariance(S, m = 1, lag = 1)
mModeAutoCovariance(S, m = 1, lag = 4)
</code></pre>

<hr>
<h2 id='mModeCovariance'>The m-Mode Covariance Matrix</h2><span id='topic+mModeCovariance'></span>

<h3>Description</h3>

<p>Estimates the m-mode covariance matrix from an array of array-valued observations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mModeCovariance(x, m, center = TRUE, normalize = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mModeCovariance_+3A_x">x</code></td>
<td>
<p>Array of order higher than two with the last dimension corresponding to the sampling units.</p>
</td></tr>
<tr><td><code id="mModeCovariance_+3A_m">m</code></td>
<td>
<p>The mode with respect to which the covariance matrix is to be computed.</p>
</td></tr>
<tr><td><code id="mModeCovariance_+3A_center">center</code></td>
<td>
<p>Logical, indicating whether the observations should be centered prior to computing the covariance matrix. Default is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="mModeCovariance_+3A_normalize">normalize</code></td>
<td>
<p>Logical, indicating whether the resulting matrix is divided by <code>p_1 ... p_{m-1} p_{m+1} ... p_r</code> or not. Default is <code>TRUE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The m-mode covariance matrix provides a higher order analogy for the ordinary covariance matrix of a random vector and is computed for a random tensor <code class="reqn">X</code> of size <code class="reqn">p_1 \times p_2 \times \ldots \times p_r</code> as <code class="reqn">Cov_m(X) = E(X^{(m)} X^{(m)T})/(p_1 \ldots p_{m-1} p_{m+1} \ldots p_r)</code>, where <code class="reqn">X^{(m)}</code> is the centered <code class="reqn">m</code>-flattening of <code class="reqn">X</code>. The algorithm computes the estimate of this based on the sample <code>x</code>.
</p>


<h3>Value</h3>

<p>The <code>m</code>-mode covariance matrix of <code>x</code> having the size <code class="reqn">p_m \times p_m</code>.
</p>


<h3>Author(s)</h3>

<p>Joni Virta
</p>


<h3>References</h3>

<p><cite>Virta, J., Li, B., Nordhausen, K. and Oja, H., (2017), Independent component analysis for tensor-valued data, Journal of Multivariate Analysis, <a href="https://doi.org/10.1016/j.jmva.2017.09.008">doi:10.1016/j.jmva.2017.09.008</a></cite>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mModeAutoCovariance">mModeAutoCovariance</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Generate sample data.
n &lt;- 100
x &lt;- t(cbind(rnorm(n, mean = 0),
             rnorm(n, mean = 1),
             rnorm(n, mean = 2),
             rnorm(n, mean = 3),
             rnorm(n, mean = 4),
             rnorm(n, mean = 5)))
             
dim(x) &lt;- c(3, 2, n)

# The m-mode covariance matrices of the first and second modes
mModeCovariance(x, 1)
mModeCovariance(x, 2)
</code></pre>

<hr>
<h2 id='plot.tbss'>Plot an Object of the Class tbss</h2><span id='topic+plot.tbss'></span>

<h3>Description</h3>

<p>Plots the most interesting components (in the sense of extreme kurtosis) obtained by a tensor blind source separation method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'tbss'
plot(x, first = 2, last = 2, datatype = NULL, 
     main = "The components with most extreme kurtoses", ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.tbss_+3A_x">x</code></td>
<td>
<p>Object of class tbss.</p>
</td></tr>
<tr><td><code id="plot.tbss_+3A_first">first</code></td>
<td>
<p>Number of components with maximal kurtosis to be selected.<br /> See <code><a href="#topic+selectComponents">selectComponents</a></code> for details.</p>
</td></tr>
<tr><td><code id="plot.tbss_+3A_last">last</code></td>
<td>
<p>Number of components with minimal kurtosis to be selected.<br /> See <code><a href="#topic+selectComponents">selectComponents</a></code> for details.</p>
</td></tr>
<tr><td><code id="plot.tbss_+3A_main">main</code></td>
<td>
<p>The title of the plot.</p>
</td></tr>
<tr><td><code id="plot.tbss_+3A_datatype">datatype</code></td>
<td>
<p>Parameter for choosing the type of plot, either <code>NULL</code>, <code>"iid"</code> or <code>"ts"</code>. The default <code>NULL</code> means the value from the tbss object <code>x</code> is taken.</p>
</td></tr>
<tr><td><code id="plot.tbss_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed to the plotting functions, see details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function <code>plot.tbss</code> first selects the most interesting components using <code><a href="#topic+selectComponents">selectComponents</a></code> and then plots them either as a matrix of scatter plots using <code><a href="graphics.html#topic+pairs">pairs</a></code> (<code>datatype</code> = &quot;iid&quot;) or as a time series plot using <code><a href="stats.html#topic+plot.ts">plot.ts</a></code> (<code>datatype</code> = &quot;ts&quot;). 
Note that for <code><a href="#topic+tSOBI">tSOBI</a></code> this criterion might not necessarily be meaningful as the method is based on second moments only.
</p>


<h3>Author(s)</h3>

<p>Joni Virta
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(zip.train)
x &lt;- zip.train

rows &lt;- which(x[, 1] == 0 | x[, 1] == 1)
x0 &lt;- x[rows, 2:257]
y0 &lt;- x[rows, 1] + 1

x0 &lt;- t(x0)
dim(x0) &lt;- c(16, 16, 2199)

tfobi &lt;- tFOBI(x0)
plot(tfobi, col=y0)

if(require("stochvol")){
  n &lt;- 1000
  S &lt;- t(cbind(svsim(n, mu = -10, phi = 0.98, sigma = 0.2, nu = Inf)$y,
               svsim(n, mu = -5, phi = -0.98, sigma = 0.2, nu = 10)$y,
               svsim(n, mu = -10, phi = 0.70, sigma = 0.7, nu = Inf)$y,
               svsim(n, mu = -5, phi = -0.70, sigma = 0.7, nu = 10)$y,
               svsim(n, mu = -9, phi = 0.20, sigma = 0.01, nu = Inf)$y,
               svsim(n, mu = -9, phi = -0.20, sigma = 0.01, nu = 10)$y))
  dim(S) &lt;- c(3, 2, n)

  A1 &lt;- matrix(rnorm(9), 3, 3)
  A2 &lt;- matrix(rnorm(4), 2, 2)

  X &lt;- tensorTransform(S, A1, 1)
  X &lt;- tensorTransform(X, A2, 2)

  tgfobi &lt;- tgFOBI(X)
  plot(tgfobi, 1, 1)
}
</code></pre>

<hr>
<h2 id='print.taug'>
Printing an object of class taug
</h2><span id='topic+print.taug'></span>

<h3>Description</h3>

<p>Prints an object of class taug.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'taug'
## S3 method for class 'taug'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.taug_+3A_x">x</code></td>
<td>
<p>object of class taug.</p>
</td></tr>
<tr><td><code id="print.taug_+3A_...">...</code></td>
<td>
<p>further arguments to be passed to or from methods.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Klaus Nordhausen
</p>

<hr>
<h2 id='print.tladle'>
Printing an object of class tladle
</h2><span id='topic+print.tladle'></span>

<h3>Description</h3>

<p>Prints an object of class tladle.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'tladle'
## S3 method for class 'tladle'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.tladle_+3A_x">x</code></td>
<td>
<p>object of class tladle.</p>
</td></tr>
<tr><td><code id="print.tladle_+3A_...">...</code></td>
<td>
<p>further arguments to be passed to or from methods.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Klaus Nordhausen
</p>

<hr>
<h2 id='selectComponents'>Select the Most Informative Components</h2><span id='topic+selectComponents'></span>

<h3>Description</h3>

<p>Takes an array of observations as an input and outputs a subset of the components having the most extreme kurtoses.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>selectComponents(x, first = 2, last = 2)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="selectComponents_+3A_x">x</code></td>
<td>
<p>Numeric array of an order at least two. It is assumed that the last dimension corresponds to the sampling units.</p>
</td></tr>
<tr><td><code id="selectComponents_+3A_first">first</code></td>
<td>
<p>Number of components with maximal kurtosis to be selected. Can equal zero but the total number of components selected must be at least two.</p>
</td></tr>
<tr><td><code id="selectComponents_+3A_last">last</code></td>
<td>
<p>Number of components with minimal kurtosis to be selected. Can equal zero but the total number of components selected must be at least two.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In independent component analysis (ICA) the components having the most extreme kurtoses are often thought to be also the most informative. With this viewpoint in mind the function <code>selectComponents</code> selects from <code>x</code> <code>first</code> components having the highest kurtosis and <code>last</code> components having the lowest kurtoses and outputs them as a standard data matrix for further analysis.
</p>


<h3>Value</h3>

<p>Data matrix with rows corresponding to the observations and the columns correponding to the <code>first</code> + <code>last</code> selected components in decreasing order with respect to kurtosis. The names of the components in the output matrix correspond to the indices of the components in the original array <code>x</code>.
</p>


<h3>Author(s)</h3>

<p>Joni Virta
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(zip.train)
x &lt;- zip.train

rows &lt;- which(x[, 1] == 0 | x[, 1] == 1)
x0 &lt;- x[rows, 2:257]

x0 &lt;- t(x0)
dim(x0) &lt;- c(16, 16, 2199)

tfobi &lt;- tFOBI(x0)
comp &lt;- selectComponents(tfobi$S)
head(comp)
</code></pre>

<hr>
<h2 id='tensorBoot'>
Bootstrapping or Permuting a Data Tensor
</h2><span id='topic+tensorBoot'></span>

<h3>Description</h3>

<p>The function takes bootstrap samples or permutes its content along the last dimension of the tensor.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tensorBoot(x, replace = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tensorBoot_+3A_x">x</code></td>
<td>
<p>Array of an order of at least two with the last dimension corresponding to the sampling units.</p>
</td></tr>
<tr><td><code id="tensorBoot_+3A_replace">replace</code></td>
<td>
<p>Logical. Should sampling be performed with or without replacement.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Assume an array of dimension <code class="reqn">r+1</code>, where the last dimension represents the <code class="reqn">n</code> sampling units and the first <code class="reqn">r</code> dimensions the data per unit. The function then returns an array of the same dimension as <code>x</code> where either <code class="reqn">n</code> bootstraps samples are selected or the units are permuted.
</p>


<h3>Value</h3>

<p>The bootstrapped or permuted samples in an array with the same dimension as <code>x</code>.
</p>


<h3>Author(s)</h3>

<p>Christoph Koesner
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- array(1:50, c(2, 5, 5))
x
tensorBoot(x)
tensorBoot(x, replace = FALSE)

x &lt;- array(1:100, c(2, 5, 2, 5))
x
tensorBoot(x)
</code></pre>

<hr>
<h2 id='tensorCentering'>Center an Array of Observations</h2><span id='topic+tensorCentering'></span>

<h3>Description</h3>

<p>Centers an array of array-valued observations by substracting a location array (the mean array by default) from each observation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tensorCentering(x, location = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tensorCentering_+3A_x">x</code></td>
<td>
<p>Array of order at least two with the last dimension corresponding to the sampling units.</p>
</td></tr>
<tr><td><code id="tensorCentering_+3A_location">location</code></td>
<td>
<p>The location to be used in the centering. Either <code>NULL</code>, defaulting to the mean array, or a user-specified <code class="reqn">p_1 \times p_2 \times \ldots \times p_r</code>-dimensional array.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Centers a <code class="reqn">p_1 \times p_2 \times \ldots \times p_r \times n</code>-dimensional array by substracting the <code class="reqn">p_1 \times p_2 \times \ldots \times p_r</code>-dimensional <code>location</code> from each of the observed arrays.
</p>


<h3>Value</h3>

<p>Array of centered observations with the same dimensions as the input array. The used location is returned as attribute <code>"location"</code>.
</p>


<h3>Author(s)</h3>

<p>Joni Virta</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Generate sample data.
n &lt;- 1000
x &lt;- t(cbind(rnorm(n, mean = 0),
             rnorm(n, mean = 1),
             rnorm(n, mean = 2),
             rnorm(n, mean = 3),
             rnorm(n, mean = 4),
             rnorm(n, mean = 5)))
             
dim(x) &lt;- c(3, 2, n)

## Centered data
xcen &lt;- tensorCentering(x)

## Check the means of individual cells
apply(xcen, 1:2, mean)
</code></pre>

<hr>
<h2 id='tensorStandardize'>Standardize an Observation Array</h2><span id='topic+tensorStandardize'></span>

<h3>Description</h3>

<p>Standardizes an array of array-valued observations simultaneously from each mode. The method can be seen as a higher-order analogy for the regular multivariate standardization of random vectors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tensorStandardize(x, location = NULL, scatter = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tensorStandardize_+3A_x">x</code></td>
<td>
<p>Array of an order higher than two with the last dimension corresponding to the sampling units.</p>
</td></tr>
<tr><td><code id="tensorStandardize_+3A_location">location</code></td>
<td>
<p>The location to be used in the standardizing. Either <code>NULL</code>, defaulting to the mean array, or a user-specified <code class="reqn">p_1 \times p_2 \times \ldots \times p_r</code>-dimensional array.</p>
</td></tr>
<tr><td><code id="tensorStandardize_+3A_scatter">scatter</code></td>
<td>
<p>The scatter matrices to be used in the standardizing. Either <code>NULL</code>, defaulting to the m-mode covariance matrices, or a user-specified list of length <code>r</code> of <code class="reqn">p_1 \times p_1, \ldots , p_r \times p_r</code>-dimensional symmetric positive definite matrices.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The algorithm first centers the <code class="reqn">n</code> observed tensors <code class="reqn">X_i</code> using <code>location</code> (either the sample mean, or a user-specified location). Then, if <code>scatter = NULL</code>, it estimates the <code class="reqn">m</code>th mode covariance matrix <code class="reqn">Cov_m(X) = E(X^{(m)} X^{(m)T})/(p_1 \ldots p_{m-1} p_{m+1} \ldots p_r)</code>, where <code class="reqn">X^{(m)}</code> is the centered <code class="reqn">m</code>-flattening of <code class="reqn">X</code>, for each mode, and transforms the observations with the inverse square roots of the covariance matrices from the corresponding modes. If, instead, the user has specified a non-<code>NULL</code> value for <code>scatter</code>, the inverse square roots of those matrices are used to transform the centered data.
</p>


<h3>Value</h3>

<p>A list containing the following components: 
</p>
<table role = "presentation">
<tr><td><code>x</code></td>
<td>
<p>Array of the same size as <code>x</code> containing the standardized observations. The used location and scatters are returned as attributes <code>"location"</code> and <code>"scatter"</code>.</p>
</td></tr>
<tr><td><code>S</code></td>
<td>
<p>List containing inverse square roots of the covariance matrices of different modes.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Joni Virta
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Generate sample data.
n &lt;- 100
x &lt;- t(cbind(rnorm(n, mean = 0),
             rnorm(n, mean = 1),
             rnorm(n, mean = 2),
             rnorm(n, mean = 3),
             rnorm(n, mean = 4),
             rnorm(n, mean = 5)))
             
dim(x) &lt;- c(3, 2, n)

# Standardize
z &lt;- tensorStandardize(x)$x

# The m-mode covariance matrices of the standardized tensors
mModeCovariance(z, 1)
mModeCovariance(z, 2)
</code></pre>

<hr>
<h2 id='tensorTransform'>Linear Transformation of Tensors from mth Mode</h2><span id='topic+tensorTransform'></span>

<h3>Description</h3>

<p>Applies a linear transformation to the mth mode of each individual tensor in an array of tensors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tensorTransform(x, A, m)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tensorTransform_+3A_x">x</code></td>
<td>
<p>Array of an order at least two with the last dimension corresponding to the sampling units.</p>
</td></tr>
<tr><td><code id="tensorTransform_+3A_a">A</code></td>
<td>
<p>Matrix corresponding to the desired linear transformation with the number of columns equal to the size of the <code>m</code>th dimension of <code>x</code>.</p>
</td></tr>
<tr><td><code id="tensorTransform_+3A_m">m</code></td>
<td>
<p>The mode from which the linear transform is to be applied.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Applies the linear transformation given by the matrix <code class="reqn">A</code> of size <code class="reqn">q_m \times p_m</code> to the <code class="reqn">m</code>th mode of each of the <code class="reqn">n</code> observed tensors <code class="reqn">X_i</code> in the given <code class="reqn">p_1 \times p_2 \times \ldots \times p_r \times n</code>-dimensional array <code>x</code>. This is equivalent to separately applying the linear transformation given by <code class="reqn">A</code> to each <code class="reqn">m</code>-mode vector of each <code class="reqn">X_i</code>. 
</p>


<h3>Value</h3>

<p>Array of size <code class="reqn">p_1 \times p_2 \times \ldots \times p_{m-1} \times q_m \times p_{m+1} \times \ldots \times p_r \times n</code>
</p>


<h3>Author(s)</h3>

<p>Joni Virta
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Generate sample data.
n &lt;- 10
x &lt;- t(cbind(rnorm(n, mean = 0),
             rnorm(n, mean = 1),
             rnorm(n, mean = 2),
             rnorm(n, mean = 3),
             rnorm(n, mean = 4),
             rnorm(n, mean = 5)))

dim(x) &lt;- c(3, 2, n)

# Transform from the second mode
A &lt;- matrix(c(2, 1, 0, 3), 2, 2)
z &lt;- tensorTransform(x, A, 2)

# Compare
z[, , 1]
x[, , 1]%*%t(A)
</code></pre>

<hr>
<h2 id='tensorTransform2'>
Linear Transformations of Tensors from Several Modes
</h2><span id='topic+tensorTransform2'></span>

<h3>Description</h3>

<p>Applies a linear transformation to user selected modes of each individual tensor in an array of tensors. The function is a generalization of <code><a href="#topic+tensorTransform">tensorTransform</a></code> which only transforms one specific mode.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tensorTransform2(x, A, mode, transpose = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tensorTransform2_+3A_x">x</code></td>
<td>
<p>Array of order r+1 &gt;= 2 where the last dimension corresponds to the sampling units.</p>
</td></tr>
<tr><td><code id="tensorTransform2_+3A_a">A</code></td>
<td>
<p>A list of r matrices to apply linearly to the corresponding mode.</p>
</td></tr>
<tr><td><code id="tensorTransform2_+3A_mode">mode</code></td>
<td>
<p>subsetting vector indicating which modes should be linearly transformed by multiplying them with the corresponding matrices from <code>A</code>.</p>
</td></tr>
<tr><td><code id="tensorTransform2_+3A_transpose">transpose</code></td>
<td>
<p>logical. Should the matrices in <code>A</code> be transposed before the mode wise transformations or not.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For the modes <code class="reqn">i_1,\ldots,i_k</code>, specified via <code>mode</code>, the function applies the linear transformation given by the matrix <code class="reqn">A^{i_j}</code> of size <code class="reqn">q_{i_j} \times p_{i_j}</code> to the <code class="reqn">i_j</code>th mode of each of the <code class="reqn">n</code> observed tensors <code class="reqn">X_{i_j}</code> in the given <code class="reqn">p_1 \times p_2 \times \ldots \times p_r \times n</code>-dimensional array <code>x</code>. 
</p>


<h3>Value</h3>

<p>Array with r+1 dimensions where the dimensions specfied via <code>mode</code> are transformed.
</p>


<h3>Author(s)</h3>

<p>Klaus Nordhausen
</p>


<h3>See Also</h3>

<p><code><a href="#topic+tensorTransform">tensorTransform</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 5
x &lt;- array(rnorm(5*6*7), dim = c(7, 6, 5))
A1 &lt;- matrix(runif(14), ncol = 7)
A2 &lt;- matrix(rexp(18), ncol = 6)
A  &lt;- list(A1 = A1, A2 = A2)
At &lt;- list(tA1 = t(A1), tA2 = t(A2))

x1 &lt;- tensorTransform2(x, A, 1)
x2 &lt;- tensorTransform2(x, A, -2)
x3 &lt;- tensorTransform(x, A1, 1)
x1 == x2
x1 == x3
x4 &lt;- tensorTransform2(x,At,-2, TRUE)
x1 == x4
x5 &lt;- tensorTransform2(x, A, 1:2)
</code></pre>

<hr>
<h2 id='tensorVectorize'>Vectorize an Observation Tensor</h2><span id='topic+tensorVectorize'></span>

<h3>Description</h3>

<p>Vectorizes an array of array-valued observations into a matrix so that each column of the matrix corresponds to a single observational unit.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tensorVectorize(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tensorVectorize_+3A_x">x</code></td>
<td>
<p>Array of an order at least two with the last dimension corresponding to the sampling units.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Vectorizes a <code class="reqn">p_1 \times p_2 \times \ldots \times p_r \times n</code>-dimensional array into a <code class="reqn">p_1 p_2 \ldots p_r \times n</code>-dimensional matrix, each column of which then corresponds to a single observational unit. The vectorization is done so that the <code class="reqn">r</code>th index goes through its cycle the fastest and the first index the slowest.
</p>
<p>Note that the output is a matrix of the size &quot;number of variables&quot; x &quot;number of observations&quot;, that is, a transpose of the standard format for a data matrix. 
</p>


<h3>Value</h3>

<p>Matrix whose columns contain the vectorized observed tensors.
</p>


<h3>Author(s)</h3>

<p>Joni Virta
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Generate sample data.
n &lt;- 100
x &lt;- t(cbind(rnorm(n, mean = 0),
             rnorm(n, mean = 1),
             rnorm(n, mean = 2),
             rnorm(n, mean = 3),
             rnorm(n, mean = 4),
             rnorm(n, mean = 5)))
             
dim(x) &lt;- c(3, 2, n)

# Matrix of vectorized observations.
vecx &lt;- tensorVectorize(x)

# The covariance matrix of individual tensor elements
cov(t(vecx))
</code></pre>

<hr>
<h2 id='tFOBI'>FOBI for Tensor-Valued Observations 
</h2><span id='topic+tFOBI'></span>

<h3>Description</h3>

<p>Computes the tensorial FOBI in an independent component model. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tFOBI(x, norm = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tFOBI_+3A_x">x</code></td>
<td>
<p>Numeric array of an order at least two. It is assumed that the last dimension corresponds to the sampling units.</p>
</td></tr>
<tr><td><code id="tFOBI_+3A_norm">norm</code></td>
<td>
<p>A Boolean vector with number of entries equal to the number of modes in a single observation. The elements tell which modes use the &ldquo;normed&rdquo; version of tensorial FOBI. If <code>NULL</code> then all modes use the non-normed version.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>It is assumed that <code class="reqn">S</code> is a tensor (array) of size <code class="reqn">p_1 \times p_2 \times \ldots \times p_r</code> with mutually independent elements and measured on <code class="reqn">N</code> units. The tensor independent component model further assumes that the tensors S are mixed from each mode
<code class="reqn">m</code> by the mixing matrix <code class="reqn">A_m</code>, <code class="reqn">m = 1, \ldots, r</code>, yielding the observed data <code class="reqn">X</code>. In R the sample of <code class="reqn">X</code> is saved as an <code><a href="base.html#topic+array">array</a></code> of dimensions
<code class="reqn">p_1, p_2, \ldots, p_r, N</code>.
</p>
<p><code>tFOBI</code> recovers then based on <code>x</code> the underlying independent components <code class="reqn">S</code> by estimating the <code class="reqn">r</code> unmixing matrices 
<code class="reqn">W_1, \ldots, W_r</code> using fourth joint moments. 
</p>
<p>The unmixing can in each mode be done in two ways, using a &ldquo;non-normed&rdquo; or &ldquo;normed&rdquo; method and this is controlled by the argument <code>norm</code>. The authors advocate the general use of non-normed version, see the reference below for their comparison.
</p>
<p>If <code>x</code> is a matrix, that is, <code class="reqn">r = 1</code>, the method reduces to FOBI and the function calls <code><a href="JADE.html#topic+FOBI">FOBI</a></code>.
</p>
<p>For a generalization for tensor-valued time series see <code><a href="#topic+tgFOBI">tgFOBI</a></code>.
</p>


<h3>Value</h3>

<p>A list with class 'tbss', inheriting from class 'bss', containing the following components: 
</p>
<table role = "presentation">
<tr><td><code>S</code></td>
<td>
<p>Array of the same size as x containing the independent components.</p>
</td></tr>
<tr><td><code>W</code></td>
<td>
<p>List containing all the unmixing matrices.</p>
</td></tr>
<tr><td><code>norm</code></td>
<td>
<p>The vector indicating which modes used the &ldquo;normed&rdquo; version.</p>
</td></tr>
<tr><td><code>Xmu</code></td>
<td>
<p>The data location.</p>
</td></tr>
<tr><td><code>datatype</code></td>
<td>
<p>Character string with value &quot;iid&quot;. Relevant for <code><a href="#topic+plot.tbss">plot.tbss</a></code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Joni Virta
</p>


<h3>References</h3>

<p><cite>Virta, J., Li, B., Nordhausen, K. and Oja, H., (2017), Independent component analysis for tensor-valued data, Journal of Multivariate Analysis, <a href="https://doi.org/10.1016/j.jmva.2017.09.008">doi:10.1016/j.jmva.2017.09.008</a></cite>
</p>


<h3>See Also</h3>

<p><code><a href="JADE.html#topic+FOBI">FOBI</a></code>, <code><a href="#topic+tgFOBI">tgFOBI</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 1000
S &lt;- t(cbind(rexp(n)-1,
             rnorm(n),
             runif(n, -sqrt(3), sqrt(3)),
             rt(n,5)*sqrt(0.6),
             (rchisq(n,1)-1)/sqrt(2),
             (rchisq(n,2)-2)/sqrt(4)))
             
dim(S) &lt;- c(3, 2, n)

A1 &lt;- matrix(rnorm(9), 3, 3)
A2 &lt;- matrix(rnorm(4), 2, 2)

X &lt;- tensorTransform(S, A1, 1)
X &lt;- tensorTransform(X, A2, 2)

tfobi &lt;- tFOBI(X)

MD(tfobi$W[[1]], A1)
MD(tfobi$W[[2]], A2) 
tMD(tfobi$W, list(A1, A2))

# Digit data example

data(zip.train)
x &lt;- zip.train

rows &lt;- which(x[, 1] == 0 | x[, 1] == 1)
x0 &lt;- x[rows, 2:257]
y0 &lt;- x[rows, 1] + 1

x0 &lt;- t(x0)
dim(x0) &lt;- c(16, 16, 2199)

tfobi &lt;- tFOBI(x0)
plot(tfobi, col=y0)
</code></pre>

<hr>
<h2 id='tgFOBI'>gFOBI for Tensor-Valued Time Series</h2><span id='topic+tgFOBI'></span>

<h3>Description</h3>

<p>Computes the tensorial gFOBI for time series where at each time point a tensor of order <code class="reqn">r</code> is observed. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tgFOBI(x, lags = 0:12, maxiter = 100, eps = 1e-06)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tgFOBI_+3A_x">x</code></td>
<td>
<p>Numeric array of an order at least two. It is assumed that the last dimension corresponds to the time.</p>
</td></tr>
<tr><td><code id="tgFOBI_+3A_lags">lags</code></td>
<td>
<p>Vector of integers. Defines the lags used for the computations of the autocovariances.</p>
</td></tr>
<tr><td><code id="tgFOBI_+3A_maxiter">maxiter</code></td>
<td>
<p>Maximum number of iterations. Passed on to <code><a href="JADE.html#topic+rjd">rjd</a></code>.</p>
</td></tr>
<tr><td><code id="tgFOBI_+3A_eps">eps</code></td>
<td>
<p>Convergence tolerance. Passed on to <code><a href="JADE.html#topic+rjd">rjd</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>It is assumed that <code class="reqn">S</code> is a tensor (array) of size <code class="reqn">p_1 \times p_2 \times \ldots \times p_r</code> measured at time points <code class="reqn">1, \ldots, T</code>.
The assumption is that the elements of <code class="reqn">S</code> are mutually independent, centered and weakly stationary time series and are mixed from each mode
<code class="reqn">m</code> by the mixing matrix <code class="reqn">A_m</code>, <code class="reqn">m = 1, \ldots, r</code>, yielding the observed time series <code class="reqn">X</code>. In R the sample of <code class="reqn">X</code> is saved as an <code><a href="base.html#topic+array">array</a></code> of dimensions
<code class="reqn">p_1, p_2, \ldots, p_r, T</code>.
</p>
<p><code>tgFOBI</code> recovers then based on <code>x</code> the underlying independent time series <code class="reqn">S</code> by estimating the <code class="reqn">r</code> unmixing matrices 
<code class="reqn">W_1, \ldots, W_r</code> using the lagged fourth joint moments specified by <code>lags</code>. This reliance on higher order moments makes the method especially suited for stochastic volatility models.
</p>
<p>If <code>x</code> is a matrix, that is, <code class="reqn">r = 1</code>, the method reduces to gFOBI and the function calls <code><a href="tsBSS.html#topic+gFOBI">gFOBI</a></code>. 
</p>
<p>If <code>lags = 0</code> the method reduces to <code><a href="#topic+tFOBI">tFOBI</a></code>.
</p>


<h3>Value</h3>

<p>A list with class 'tbss', inheriting from class 'bss', containing the following components: 
</p>
<table role = "presentation">
<tr><td><code>S</code></td>
<td>
<p>Array of the same size as x containing the estimated uncorrelated sources.</p>
</td></tr>
<tr><td><code>W</code></td>
<td>
<p>List containing all the unmixing matrices</p>
</td></tr>
<tr><td><code>Xmu</code></td>
<td>
<p>The data location.</p>
</td></tr>
<tr><td><code>datatype</code></td>
<td>
<p>Character string with value &quot;ts&quot;. Relevant for <code><a href="#topic+plot.tbss">plot.tbss</a></code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Joni Virta
</p>


<h3>References</h3>

<p><cite>Virta, J. and Nordhausen, K., (2017), Blind source separation of tensor-valued time series. Signal Processing 141, 204-216, <a href="https://doi.org/10.1016/j.sigpro.2017.06.008">doi:10.1016/j.sigpro.2017.06.008</a></cite>
</p>


<h3>See Also</h3>

<p><code><a href="tsBSS.html#topic+gFOBI">gFOBI</a></code>, <code><a href="JADE.html#topic+rjd">rjd</a></code>, <code><a href="#topic+tFOBI">tFOBI</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if(require("stochvol")){
  n &lt;- 1000
  S &lt;- t(cbind(svsim(n, mu = -10, phi = 0.98, sigma = 0.2, nu = Inf)$y,
               svsim(n, mu = -5, phi = -0.98, sigma = 0.2, nu = 10)$y,
               svsim(n, mu = -10, phi = 0.70, sigma = 0.7, nu = Inf)$y,
               svsim(n, mu = -5, phi = -0.70, sigma = 0.7, nu = 10)$y,
               svsim(n, mu = -9, phi = 0.20, sigma = 0.01, nu = Inf)$y,
               svsim(n, mu = -9, phi = -0.20, sigma = 0.01, nu = 10)$y))
  dim(S) &lt;- c(3, 2, n)
  
  A1 &lt;- matrix(rnorm(9), 3, 3)
  A2 &lt;- matrix(rnorm(4), 2, 2)
  
  X &lt;- tensorTransform(S, A1, 1)
  X &lt;- tensorTransform(X, A2, 2)
  
  tgfobi &lt;- tgFOBI(X)
  
  MD(tgfobi$W[[1]], A1)
  MD(tgfobi$W[[2]], A2) 
  tMD(tgfobi$W, list(A1, A2))
}
</code></pre>

<hr>
<h2 id='tgJADE'>gJADE for Tensor-Valued Time Series</h2><span id='topic+tgJADE'></span>

<h3>Description</h3>

<p>Computes the tensorial gJADE for time series where at each time point a tensor of order <code class="reqn">r</code> is observed. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tgJADE(x, lags = 0:12, maxiter = 100, eps = 1e-06)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tgJADE_+3A_x">x</code></td>
<td>
<p>Numeric array of an order at least two. It is assumed that the last dimension corresponds to the time.</p>
</td></tr>
<tr><td><code id="tgJADE_+3A_lags">lags</code></td>
<td>
<p>Vector of integers. Defines the lags used for the computations of the autocovariances.</p>
</td></tr>
<tr><td><code id="tgJADE_+3A_maxiter">maxiter</code></td>
<td>
<p>Maximum number of iterations. Passed on to <code><a href="JADE.html#topic+rjd">rjd</a></code>.</p>
</td></tr>
<tr><td><code id="tgJADE_+3A_eps">eps</code></td>
<td>
<p>Convergence tolerance. Passed on to <code><a href="JADE.html#topic+rjd">rjd</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>It is assumed that <code class="reqn">S</code> is a tensor (array) of size <code class="reqn">p_1 \times p_2 \times \ldots \times p_r</code> measured at time points <code class="reqn">1, \ldots, T</code>.
The assumption is that the elements of <code class="reqn">S</code> are mutually independent, centered and weakly stationary time series and are mixed from each mode
<code class="reqn">m</code> by the mixing matrix <code class="reqn">A_m</code>, <code class="reqn">m = 1, \ldots, r</code>, yielding the observed time series <code class="reqn">X</code>. In R the sample of <code class="reqn">X</code> is saved as an <code><a href="base.html#topic+array">array</a></code> of dimensions
<code class="reqn">p_1, p_2, \ldots, p_r, T</code>.
</p>
<p><code>tgJADE</code> recovers then based on <code>x</code> the underlying independent time series <code class="reqn">S</code> by estimating the <code class="reqn">r</code> unmixing matrices 
<code class="reqn">W_1, \ldots, W_r</code> using the lagged fourth joint moments specified by <code>lags</code>. This reliance on higher order moments makes the method especially suited for stochastic volatility models.
</p>
<p>If <code>x</code> is a matrix, that is, <code class="reqn">r = 1</code>, the method reduces to gJADE and the function calls <code><a href="tsBSS.html#topic+gJADE">gJADE</a></code>.
</p>
<p>If <code>lags = 0</code> the method reduces to <code><a href="#topic+tJADE">tJADE</a></code>.
</p>


<h3>Value</h3>

<p>A list with class 'tbss', inheriting from class 'bss', containing the following components: 
</p>
<table role = "presentation">
<tr><td><code>S</code></td>
<td>
<p>Array of the same size as x containing the estimated uncorrelated sources.</p>
</td></tr>
<tr><td><code>W</code></td>
<td>
<p>List containing all the unmixing matrices</p>
</td></tr>
<tr><td><code>Xmu</code></td>
<td>
<p>The data location.</p>
</td></tr>
<tr><td><code>datatype</code></td>
<td>
<p>Character string with value &quot;ts&quot;. Relevant for <code><a href="#topic+plot.tbss">plot.tbss</a></code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Joni Virta
</p>


<h3>References</h3>

<p><cite>Virta, J. and Nordhausen, K., (2017), Blind source separation of tensor-valued time series. Signal Processing 141, 204-216, <a href="https://doi.org/10.1016/j.sigpro.2017.06.008">doi:10.1016/j.sigpro.2017.06.008</a></cite>  
</p>


<h3>See Also</h3>

<p><code><a href="tsBSS.html#topic+gJADE">gJADE</a></code>, <code><a href="JADE.html#topic+rjd">rjd</a></code>, <code><a href="#topic+tJADE">tJADE</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library("stochvol")
n &lt;- 1000
S &lt;- t(cbind(svsim(n, mu = -10, phi = 0.98, sigma = 0.2, nu = Inf)$y,
             svsim(n, mu = -5, phi = -0.98, sigma = 0.2, nu = 10)$y,
             svsim(n, mu = -10, phi = 0.70, sigma = 0.7, nu = Inf)$y,
             svsim(n, mu = -5, phi = -0.70, sigma = 0.7, nu = 10)$y,
             svsim(n, mu = -9, phi = 0.20, sigma = 0.01, nu = Inf)$y,
             svsim(n, mu = -9, phi = -0.20, sigma = 0.01, nu = 10)$y))
dim(S) &lt;- c(3, 2, n)

A1 &lt;- matrix(rnorm(9), 3, 3)
A2 &lt;- matrix(rnorm(4), 2, 2)

X &lt;- tensorTransform(S, A1, 1)
X &lt;- tensorTransform(X, A2, 2)

tgjade &lt;- tgJADE(X)

MD(tgjade$W[[1]], A1)
MD(tgjade$W[[2]], A2) 
tMD(tgjade$W, list(A1, A2))

</code></pre>

<hr>
<h2 id='tJADE'>
tJADE for Tensor-Valued Observations 
</h2><span id='topic+tJADE'></span>

<h3>Description</h3>

<p>Computes the tensorial JADE in an independent component model. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tJADE(x, maxiter = 100, eps = 1e-06)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tJADE_+3A_x">x</code></td>
<td>
<p>Numeric array of an order at least two. It is assumed that the last dimension corresponds to the sampling units.</p>
</td></tr>
<tr><td><code id="tJADE_+3A_maxiter">maxiter</code></td>
<td>
<p>Maximum number of iterations. Passed on to <code><a href="JADE.html#topic+rjd">rjd</a></code>.</p>
</td></tr>
<tr><td><code id="tJADE_+3A_eps">eps</code></td>
<td>
<p>Convergence tolerance. Passed on to <code><a href="JADE.html#topic+rjd">rjd</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>It is assumed that <code class="reqn">S</code> is a tensor (array) of size <code class="reqn">p_1 \times p_2 \times \ldots \times p_r</code> with mutually independent elements and measured on <code class="reqn">N</code> units. The tensor independent component model further assumes that the tensors S are mixed from each mode
<code class="reqn">m</code> by the mixing matrix <code class="reqn">A_m</code>, <code class="reqn">m = 1, \ldots, r</code>, yielding the observed data <code class="reqn">X</code>. In R the sample of <code class="reqn">X</code> is saved as an <code><a href="base.html#topic+array">array</a></code> of dimensions
<code class="reqn">p_1, p_2, \ldots, p_r, N</code>.
</p>
<p><code>tJADE</code> recovers then based on <code>x</code> the underlying independent components <code class="reqn">S</code> by estimating the <code class="reqn">r</code> unmixing matrices 
<code class="reqn">W_1, \ldots, W_r</code> using fourth joint moments in a more efficient way than <code><a href="#topic+tFOBI">tFOBI</a></code>. 
</p>
<p>If <code>x</code> is a matrix, that is, <code class="reqn">r = 1</code>, the method reduces to JADE and the function calls <code><a href="JADE.html#topic+JADE">JADE</a></code>.
</p>
<p>For a generalization for tensor-valued time series see <code><a href="#topic+tgJADE">tgJADE</a></code>.
</p>


<h3>Value</h3>

<p>A list with class 'tbss', inheriting from class 'bss', containing the following components: 
</p>
<table role = "presentation">
<tr><td><code>S</code></td>
<td>
<p>Array of the same size as x containing the independent components.</p>
</td></tr>
<tr><td><code>W</code></td>
<td>
<p>List containing all the unmixing matrices</p>
</td></tr>
<tr><td><code>Xmu</code></td>
<td>
<p>The data location.</p>
</td></tr>
<tr><td><code>datatype</code></td>
<td>
<p>Character string with value &quot;iid&quot;. Relevant for <code><a href="#topic+plot.tbss">plot.tbss</a></code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Joni Virta
</p>


<h3>References</h3>

<p><cite>Virta J., Li B., Nordhausen K., Oja H. (2018): JADE for tensor-valued observations, Journal of Computational and Graphical Statistics, Volume 27, p. 628 - 637, <a href="https://doi.org/10.1080/10618600.2017.1407324">doi:10.1080/10618600.2017.1407324</a></cite> 
</p>


<h3>See Also</h3>

<p><code><a href="JADE.html#topic+JADE">JADE</a></code>, <code><a href="#topic+tgJADE">tgJADE</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 1000
S &lt;- t(cbind(rexp(n)-1,
             rnorm(n),
             runif(n, -sqrt(3), sqrt(3)),
             rt(n,5)*sqrt(0.6),
             (rchisq(n,1)-1)/sqrt(2),
             (rchisq(n,2)-2)/sqrt(4)))
             
dim(S) &lt;- c(3, 2, n)

A1 &lt;- matrix(rnorm(9), 3, 3)
A2 &lt;- matrix(rnorm(4), 2, 2)

X &lt;- tensorTransform(S, A1, 1)
X &lt;- tensorTransform(X, A2, 2)

tjade &lt;- tJADE(X)

MD(tjade$W[[1]], A1)
MD(tjade$W[[2]], A2) 
tMD(tjade$W, list(A1, A2))

## Not run: 
# Digit data example
# Running will take a few minutes

data(zip.train)
x &lt;- zip.train

rows &lt;- which(x[, 1] == 0 | x[, 1] == 1)
x0 &lt;- x[rows, 2:257]
y0 &lt;- x[rows, 1] + 1

x0 &lt;- t(x0)
dim(x0) &lt;- c(16, 16, 2199)

tjade &lt;- tJADE(x0)
plot(tjade, col=y0)

## End(Not run)

</code></pre>

<hr>
<h2 id='tMD'>Minimum Distance Index of a Kronecker Product
</h2><span id='topic+tMD'></span>

<h3>Description</h3>

<p>A shortcut function for computing the minimum distance index of a tensorial ICA estimate on the Kronecker product &ldquo;scale&rdquo; (the vectorized space).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tMD(W.hat, A)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tMD_+3A_w.hat">W.hat</code></td>
<td>
<p>A list of <code>r</code> unmixing matrix estimates, W_1, W_2, ..., W_r.
</p>
</td></tr>
<tr><td><code id="tMD_+3A_a">A</code></td>
<td>
<p>A list of <code>r</code> mixing matrices, A_1, A_2, ..., A_r.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function computes the minimum distance index between <code>W.hat[[r]] %x%  ... %x% W.hat[[1]]</code> and <code>A[[r]] %x%  ... %x% A[[1]]</code>. The index is useful for comparing the performance of a tensor-valued ICA method to that of a method using first vectorization and then some vector-valued ICA method.
</p>


<h3>Value</h3>

<p>The value of the MD index of the Kronecker product.
</p>


<h3>Author(s)</h3>

<p>Joni Virta
</p>


<h3>References</h3>

<p><cite>Ilmonen, P., Nordhausen, K., Oja, H. and Ollila, E. (2010), A New Performance Index for ICA: Properties, Computation and Asymptotic Analysis. In Vigneron, V., Zarzoso, V., Moreau, E., Gribonval, R. and Vincent, E. (editors) Latent Variable Analysis and Signal Separation, 229-236, Springer.</cite>
</p>
<p><cite>Virta, J., Li, B., Nordhausen, K. and Oja, H., (2017), Independent component analysis for tensor-valued data, Journal of Multivariate Analysis, <a href="https://doi.org/10.1016/j.jmva.2017.09.008">doi:10.1016/j.jmva.2017.09.008</a></cite>
</p>


<h3>See Also</h3>

<p><code><a href="JADE.html#topic+MD">MD</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 1000
S &lt;- t(cbind(rexp(n)-1,
             rnorm(n),
             runif(n, -sqrt(3), sqrt(3)),
             rt(n,5)*sqrt(0.6),
             (rchisq(n,1)-1)/sqrt(2),
             (rchisq(n,2)-2)/sqrt(4)))

dim(S) &lt;- c(3, 2, n)

A1 &lt;- matrix(rnorm(9), 3, 3)
A2 &lt;- matrix(rnorm(4), 2, 2)

X &lt;- tensorTransform(S, A1, 1)
X &lt;- tensorTransform(X, A2, 2)

tfobi &lt;- tFOBI(X)

MD(tfobi$W[[2]] %x% tfobi$W[[1]], A2 %x% A1)
tMD(list(tfobi$W[[2]]), list(A2))
</code></pre>

<hr>
<h2 id='tNSS.JD'>
NSS-JD Method for Tensor-Valued Time Series
</h2><span id='topic+tNSS.JD'></span>

<h3>Description</h3>

<p>Estimates the non-stationary sources of a tensor-valued time series using separation information contained in several time intervals.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tNSS.JD(x, K = 12, n.cuts = NULL, eps = 1e-06, maxiter = 100, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tNSS.JD_+3A_x">x</code></td>
<td>
<p>Numeric array of an order at least two. It is assumed that the last dimension corresponds to the sampling units.</p>
</td></tr>
<tr><td><code id="tNSS.JD_+3A_k">K</code></td>
<td>
<p>The number of equisized intervals into which the time range is divided. If the parameter <code>n.cuts</code> is non-<code>NULL</code> it takes preference over this argument.</p>
</td></tr>
<tr><td><code id="tNSS.JD_+3A_n.cuts">n.cuts</code></td>
<td>
<p>Either a interval cutoffs (the cutoffs are used to define the two intervals that are open below and closed above, e.g. <code class="reqn">(a, b]</code>) or <code>NULL</code> (the parameter <code>K</code> is used to define the the amount of intervals).</p>
</td></tr>
<tr><td><code id="tNSS.JD_+3A_eps">eps</code></td>
<td>
<p>Convergence tolerance for <code><a href="JADE.html#topic+rjd">rjd</a></code>.</p>
</td></tr>
<tr><td><code id="tNSS.JD_+3A_maxiter">maxiter</code></td>
<td>
<p>Maximum number of iterations for <code><a href="JADE.html#topic+rjd">rjd</a></code>.</p>
</td></tr>
<tr><td><code id="tNSS.JD_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed to or from methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Assume that the observed tensor-valued time series comes from a tensorial BSS model where the sources have constant means over time but the component variances change in time. Then TNSS-JD first standardizes the series from all modes and then estimates the non-stationary sources by dividing the time scale into <code>K</code> intervals and jointly diagonalizing the covariance matrices of the <code>K</code> intervals within each mode.
</p>


<h3>Value</h3>

<p>A list with class 'tbss', inheriting from class 'bss', containing the following components: 
</p>
<table role = "presentation">
<tr><td><code>S</code></td>
<td>
<p>Array of the same size as x containing the independent components.</p>
</td></tr>
<tr><td><code>W</code></td>
<td>
<p>List containing all the unmixing matrices.</p>
</td></tr>
<tr><td><code>K</code></td>
<td>
<p>The number of intervals.</p>
</td></tr>
<tr><td><code>n.cuts</code></td>
<td>
<p>The interval cutoffs.</p>
</td></tr>
<tr><td><code>Xmu</code></td>
<td>
<p>The data location.</p>
</td></tr>
<tr><td><code>datatype</code></td>
<td>
<p>Character string with value &quot;ts&quot;. Relevant for <code><a href="#topic+plot.tbss">plot.tbss</a></code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Joni Virta
</p>


<h3>References</h3>

<p><cite>Virta J., Nordhausen K. (2017): Blind source separation for nonstationary tensor-valued time series, 2017 IEEE 27th International Workshop on Machine Learning for Signal Processing (MLSP), <a href="https://doi.org/10.1109/MLSP.2017.8168122">doi:10.1109/MLSP.2017.8168122</a></cite>
</p>


<h3>See Also</h3>

<p><code><a href="JADE.html#topic+NSS.SD">NSS.SD</a></code>, <code><a href="JADE.html#topic+NSS.JD">NSS.JD</a></code>, <code><a href="JADE.html#topic+NSS.TD.JD">NSS.TD.JD</a></code>, <code>tNSS.SD</code>, <code>tNSS.TD.JD</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Create innovation series with block-wise changing variances
n1 &lt;- 200
n2 &lt;- 500
n3 &lt;- 300
n &lt;- n1 + n2 + n3
innov1 &lt;- c(rnorm(n1, 0, 1), rnorm(n2, 0, 3), rnorm(n3, 0, 5))
innov2 &lt;- c(rnorm(n1, 0, 1), rnorm(n2, 0, 5), rnorm(n3, 0, 3))
innov3 &lt;- c(rnorm(n1, 0, 5), rnorm(n2, 0, 3), rnorm(n3, 0, 1))
innov4 &lt;- c(rnorm(n1, 0, 5), rnorm(n2, 0, 1), rnorm(n3, 0, 3))

# Generate the observations
vecx &lt;- cbind(as.vector(arima.sim(n = n, list(ar = 0.8), innov = innov1)),
              as.vector(arima.sim(n = n, list(ar = c(0.5, 0.1)), innov = innov2)),
              as.vector(arima.sim(n = n, list(ma = -0.7), innov = innov3)),
              as.vector(arima.sim(n = n, list(ar = 0.5, ma = -0.5), innov = innov4)))
             
# Vector to tensor
tenx &lt;- t(vecx)
dim(tenx) &lt;- c(2, 2, n)

# Run TNSS-JD
res &lt;- tNSS.JD(tenx, K = 6)
res$W

res &lt;- tNSS.JD(tenx, K = 12)
res$W
</code></pre>

<hr>
<h2 id='tNSS.SD'>
NSS-SD Method for Tensor-Valued Time Series
</h2><span id='topic+tNSS.SD'></span>

<h3>Description</h3>

<p>Estimates the non-stationary sources of a tensor-valued time series using separation information contained in two time intervals.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tNSS.SD(x, n.cuts = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tNSS.SD_+3A_x">x</code></td>
<td>
<p>Numeric array of an order at least two. It is assumed that the last dimension corresponds to the sampling units.</p>
</td></tr>
<tr><td><code id="tNSS.SD_+3A_n.cuts">n.cuts</code></td>
<td>
<p>Either a 3-vector of interval cutoffs (the cutoffs are used to define the two intervals that are open below and closed above, e.g. <code class="reqn">(a, b]</code>) or <code>NULL</code> (the time range is sliced into two parts of equal size).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Assume that the observed tensor-valued time series comes from a tensorial BSS model where the sources have constant means over time but the component variances change in time. Then TNSS-SD estimates the non-stationary sources by dividing the time scale into two intervals and jointly diagonalizing the covariance matrices of the two intervals within each mode.
</p>


<h3>Value</h3>

<p>A list with class 'tbss', inheriting from class 'bss', containing the following components: 
</p>
<table role = "presentation">
<tr><td><code>S</code></td>
<td>
<p>Array of the same size as x containing the independent components.</p>
</td></tr>
<tr><td><code>W</code></td>
<td>
<p>List containing all the unmixing matrices.</p>
</td></tr>
<tr><td><code>EV</code></td>
<td>
<p>Eigenvalues obtained from the joint diagonalization.</p>
</td></tr>
<tr><td><code>n.cuts</code></td>
<td>
<p>The interval cutoffs.</p>
</td></tr>
<tr><td><code>Xmu</code></td>
<td>
<p>The data location.</p>
</td></tr>
<tr><td><code>datatype</code></td>
<td>
<p>Character string with value &quot;ts&quot;. Relevant for <code><a href="#topic+plot.tbss">plot.tbss</a></code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Joni Virta
</p>


<h3>References</h3>

<p><cite>Virta J., Nordhausen K. (2017): Blind source separation for nonstationary tensor-valued time series, 2017 IEEE 27th International Workshop on Machine Learning for Signal Processing (MLSP), <a href="https://doi.org/10.1109/MLSP.2017.8168122">doi:10.1109/MLSP.2017.8168122</a></cite>
</p>


<h3>See Also</h3>

<p><code><a href="JADE.html#topic+NSS.SD">NSS.SD</a></code>, <code><a href="JADE.html#topic+NSS.JD">NSS.JD</a></code>, <code><a href="JADE.html#topic+NSS.TD.JD">NSS.TD.JD</a></code>, <code>tNSS.JD</code>, <code>tNSS.TD.JD</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Create innovation series with block-wise changing variances

# 9 smooth variance structures
var_1 &lt;- function(n){
  t &lt;- 1:n
  return(1 + cos((2*pi*t)/n)*sin((2*150*t)/(n*pi)))
}

var_2 &lt;- function(n){
  t &lt;- 1:n
  return(1 + sin((2*pi*t)/n)*cos((2*150*t)/(n*pi)))
}

var_3 &lt;- function(n){
  t &lt;- 1:n
  return(0.5 + 8*exp((n+1)^2/(4*t*(t - n - 1))))
}

var_4 &lt;- function(n){
  t &lt;- 1:n
  return(3.443 - 8*exp((n+1)^2/(4*t*(t - n - 1))))
}

var_5 &lt;- function(n){
  t &lt;- 1:n
  return(0.5 + 0.5*gamma(10)/(gamma(7)*gamma(3))*(t/(n + 1))^(7 - 1)*(1 - t/(n + 1))^(3 - 1))
}

var_6 &lt;- function(n){
  t &lt;- 1:n
  res &lt;- var_5(n)
  return(rev(res))
}

var_7 &lt;- function(n){
  t &lt;- 1:n
  return(0.2+2*t/(n + 1))
}

var_8 &lt;- function(n){
  t &lt;- 1:n
  return(0.2+2*(n + 1 - t)/(n + 1))
}

var_9 &lt;- function(n){
  t &lt;- 1:n
  return(1.5 + cos(4*pi*t/n))
}


# Innovation series
n &lt;- 1000

innov1 &lt;- c(rnorm(n, 0, sqrt(var_1(n))))
innov2 &lt;- c(rnorm(n, 0, sqrt(var_2(n))))
innov3 &lt;- c(rnorm(n, 0, sqrt(var_3(n))))
innov4 &lt;- c(rnorm(n, 0, sqrt(var_4(n))))
innov5 &lt;- c(rnorm(n, 0, sqrt(var_5(n))))
innov6 &lt;- c(rnorm(n, 0, sqrt(var_6(n))))
innov7 &lt;- c(rnorm(n, 0, sqrt(var_7(n))))
innov8 &lt;- c(rnorm(n, 0, sqrt(var_8(n))))
innov9 &lt;- c(rnorm(n, 0, sqrt(var_9(n))))

# Generate the observations
vecx &lt;- cbind(as.vector(arima.sim(n = n, list(ar = 0.9), innov = innov1)),
              as.vector(arima.sim(n = n, list(ar = c(0, 0.2, 0.1, -0.1, 0.7)), 
              innov = innov2)),
              as.vector(arima.sim(n = n, list(ar = c(0.5, 0.3, -0.2, 0.1)), 
              innov = innov3)),
              as.vector(arima.sim(n = n, list(ma = -0.5), innov = innov4)),
              as.vector(arima.sim(n = n, list(ma = c(0.1, 0.1, 0.3, 0.5, 0.8)), 
              innov = innov5)),
              as.vector(arima.sim(n = n, list(ma = c(0.5, -0.5, 0.5)), innov = innov6)),
              as.vector(arima.sim(n = n, list(ar = c(-0.5, -0.3), ma = c(-0.2, 0.1)), 
              innov = innov7)),
              as.vector(arima.sim(n = n, list(ar = c(0, -0.1, -0.2, 0.5), ma = c(0, 0.1, 0.1, 0.6)),
              innov = innov8)),
              as.vector(arima.sim(n = n, list(ar = c(0.8), ma = c(0.7, 0.6, 0.5, 0.1)),
              innov = innov9)))


# Vector to tensor
tenx &lt;- t(vecx)
dim(tenx) &lt;- c(3, 3, n)


# Run TNSS-SD
res &lt;- tNSS.SD(tenx)
res$W
</code></pre>

<hr>
<h2 id='tNSS.TD.JD'>
TNSS-TD-JD Method for Tensor-Valued Time Series
</h2><span id='topic+tNSS.TD.JD'></span>

<h3>Description</h3>

<p>Estimates the non-stationary sources of a tensor-valued time series using separation information contained in several time intervals and lags.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tNSS.TD.JD(x, K = 12, lags = 0:12, n.cuts = NULL, eps = 1e-06, maxiter = 100, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tNSS.TD.JD_+3A_x">x</code></td>
<td>
<p>Numeric array of an order at least two. It is assumed that the last dimension corresponds to the sampling units.</p>
</td></tr>
<tr><td><code id="tNSS.TD.JD_+3A_k">K</code></td>
<td>
<p>The number of equisized intervals into which the time range is divided. If the parameter <code>n.cuts</code> is non-<code>NULL</code> it takes preference over this argument.</p>
</td></tr>
<tr><td><code id="tNSS.TD.JD_+3A_lags">lags</code></td>
<td>
<p>The lag set for the autocovariance matrices.</p>
</td></tr>
<tr><td><code id="tNSS.TD.JD_+3A_n.cuts">n.cuts</code></td>
<td>
<p>Either a interval cutoffs (the cutoffs are used to define the two intervals that are open below and closed above, e.g. <code class="reqn">(a, b]</code>) or <code>NULL</code> (the parameter <code>K</code> is used to define the the amount of intervals).</p>
</td></tr>
<tr><td><code id="tNSS.TD.JD_+3A_eps">eps</code></td>
<td>
<p>Convergence tolerance for <code><a href="JADE.html#topic+rjd">rjd</a></code>.</p>
</td></tr>
<tr><td><code id="tNSS.TD.JD_+3A_maxiter">maxiter</code></td>
<td>
<p>Maximum number of iterations for <code><a href="JADE.html#topic+rjd">rjd</a></code>.</p>
</td></tr>
<tr><td><code id="tNSS.TD.JD_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed to or from methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Assume that the observed tensor-valued time series comes from a tensorial BSS model where the sources have constant means over time but the component variances change in time. Then TNSS-TD-JD first standardizes the series from all modes and then estimates the non-stationary sources by dividing the time scale into <code>K</code> intervals and jointly diagonalizing the autocovariance matrices (specified by <code>lags</code>) of the <code>K</code> intervals within each mode.
</p>


<h3>Value</h3>

<p>A list with class 'tbss', inheriting from class 'bss', containing the following components: 
</p>
<table role = "presentation">
<tr><td><code>S</code></td>
<td>
<p>Array of the same size as x containing the independent components.</p>
</td></tr>
<tr><td><code>W</code></td>
<td>
<p>List containing all the unmixing matrices.</p>
</td></tr>
<tr><td><code>K</code></td>
<td>
<p>The number of intervals.</p>
</td></tr>
<tr><td><code>lags</code></td>
<td>
<p>The lag set.</p>
</td></tr>
<tr><td><code>n.cuts</code></td>
<td>
<p>The interval cutoffs.</p>
</td></tr>
<tr><td><code>Xmu</code></td>
<td>
<p>The data location.</p>
</td></tr>
<tr><td><code>datatype</code></td>
<td>
<p>Character string with value &quot;ts&quot;. Relevant for <code><a href="#topic+plot.tbss">plot.tbss</a></code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Joni Virta
</p>


<h3>References</h3>

<p><cite>Virta J., Nordhausen K. (2017): Blind source separation for nonstationary tensor-valued time series, 2017 IEEE 27th International Workshop on Machine Learning for Signal Processing (MLSP), <a href="https://doi.org/10.1109/MLSP.2017.8168122">doi:10.1109/MLSP.2017.8168122</a></cite>
</p>


<h3>See Also</h3>

<p><code><a href="JADE.html#topic+NSS.SD">NSS.SD</a></code>, <code><a href="JADE.html#topic+NSS.JD">NSS.JD</a></code>, <code><a href="JADE.html#topic+NSS.TD.JD">NSS.TD.JD</a></code>, <code>tNSS.SD</code>, <code>tNSS.JD</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Create innovation series with block-wise changing variances
n1 &lt;- 200
n2 &lt;- 500
n3 &lt;- 300
n &lt;- n1 + n2 + n3
innov1 &lt;- c(rnorm(n1, 0, 1), rnorm(n2, 0, 3), rnorm(n3, 0, 5))
innov2 &lt;- c(rnorm(n1, 0, 1), rnorm(n2, 0, 5), rnorm(n3, 0, 3))
innov3 &lt;- c(rnorm(n1, 0, 5), rnorm(n2, 0, 3), rnorm(n3, 0, 1))
innov4 &lt;- c(rnorm(n1, 0, 5), rnorm(n2, 0, 1), rnorm(n3, 0, 3))

# Generate the observations
vecx &lt;- cbind(as.vector(arima.sim(n = n, list(ar = 0.8), innov = innov1)),
              as.vector(arima.sim(n = n, list(ar = c(0.5, 0.1)), innov = innov2)),
              as.vector(arima.sim(n = n, list(ma = -0.7), innov = innov3)),
              as.vector(arima.sim(n = n, list(ar = 0.5, ma = -0.5), innov = innov4)))
             
# Vector to tensor
tenx &lt;- t(vecx)
dim(tenx) &lt;- c(2, 2, n)

# Run TNSS-TD-JD
res &lt;- tNSS.TD.JD(tenx)
res$W

res &lt;- tNSS.TD.JD(tenx, K = 6, lags = 0:6)
res$W
</code></pre>

<hr>
<h2 id='tPCA'>PCA for Tensor-Valued Observations</h2><span id='topic+tPCA'></span>

<h3>Description</h3>

<p>Computes the tensorial principal components. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tPCA(x, p = NULL, d = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tPCA_+3A_x">x</code></td>
<td>
<p>Numeric array of an order at least three. It is assumed that the last dimension corresponds to the sampling units.</p>
</td></tr>
<tr><td><code id="tPCA_+3A_p">p</code></td>
<td>
<p>A vector of the percentages of variation per each mode the principal components should explain.</p>
</td></tr>
<tr><td><code id="tPCA_+3A_d">d</code></td>
<td>
<p>A vector of the exact number of components retained per each mode. At most one of this and the previous argument should be supplied.</p>
</td></tr> 
</table>


<h3>Details</h3>

<p>The observed tensors (array) <code class="reqn">X</code> of size <code class="reqn">p_1 \times p_2 \times \ldots \times p_r</code> measured on <code class="reqn">N</code> units are projected from each mode on the eigenspaces of the <code class="reqn">m</code>-mode covariance matrices of the corresponding modes. As in regular PCA, by retaining only some subsets of these projections (indices) with respective sizes <code class="reqn">d_1, d_2, ... d_r</code>, a dimension reduction can be carried out, resulting into observations tensors of size <code class="reqn">d_1 \times d_2 \times \ldots \times d_r</code>. In R the sample of <code class="reqn">X</code> is saved as an <code><a href="base.html#topic+array">array</a></code> of dimensions
<code class="reqn">p_1, p_2, \ldots, p_r, N</code>.
</p>


<h3>Value</h3>

<p>A list containing the following components: 
</p>
<table role = "presentation">
<tr><td><code>S</code></td>
<td>
<p>Array of the same size as x containing the principal components.</p>
</td></tr>
<tr><td><code>U</code></td>
<td>
<p>List containing the rotation matrices</p>
</td></tr>
<tr><td><code>D</code></td>
<td>
<p>List containing the amounts of variance explained by each index in each mode.</p>
</td></tr>
<tr><td><code>p_comp</code></td>
<td>
<p>The percentages of variation per each mode that the principal components explain.</p>
</td></tr>
<tr><td><code>Xmu</code></td>
<td>
<p>The data location.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Joni Virta
</p>


<h3>References</h3>

<p><cite>Virta, J., Taskinen, S. and Nordhausen, K. (2016), Applying fully tensorial ICA to fMRI data, Signal Processing in Medicine and Biology Symposium (SPMB), 2016 IEEE,<br /> <a href="https://doi.org/10.1109/SPMB.2016.7846858">doi:10.1109/SPMB.2016.7846858</a></cite> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Digit data example

data(zip.train)
x &lt;- zip.train

rows &lt;- which(x[, 1] == 0 | x[, 1] == 1)
x0 &lt;- x[rows, 2:257]
y0 &lt;- x[rows, 1] + 1

x0 &lt;- t(x0)
dim(x0) &lt;- c(16, 16, 2199)


tpca &lt;- tPCA(x0, d = c(2, 2))
pairs(t(apply(tpca$S, 3, c)), col=y0)
</code></pre>

<hr>
<h2 id='tPCAaug'>
Order Determination for Tensorial PCA Using Augmentation 
</h2><span id='topic+tPCAaug'></span>

<h3>Description</h3>

<p>In a tensorial PCA context the dimensions of a core tensor are estimated based on augmentation of additional noise components. Information from both eigenvectors and eigenvalues are then used to obtain the dimension estimates.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tPCAaug(x, noise = "median", naug = 1, nrep = 1, 
  sigma2 = NULL, alpha = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tPCAaug_+3A_x">x</code></td>
<td>

<p>array of an order at least three with the last dimension corresponding to the sampling units.
</p>
</td></tr>
<tr><td><code id="tPCAaug_+3A_noise">noise</code></td>
<td>

<p>specifies how to estimate the noise variance. Can be one of  
<code>"median"</code>, <code>"quantile"</code>, <code>"last"</code>, <code>"known"</code>. Default is <code>"median"</code>. 
See details for further information.
</p>
</td></tr>
<tr><td><code id="tPCAaug_+3A_naug">naug</code></td>
<td>

<p>number of augmented variables in each mode.  Default is 1.
</p>
</td></tr>
<tr><td><code id="tPCAaug_+3A_nrep">nrep</code></td>
<td>

<p>number of repetitions for the augmentation. Default is 1.
</p>
</td></tr>
<tr><td><code id="tPCAaug_+3A_sigma2">sigma2</code></td>
<td>

<p>if <code>noise = "known"</code> the value of the noise variance.
</p>
</td></tr>
<tr><td><code id="tPCAaug_+3A_alpha">alpha</code></td>
<td>

<p>if <code>noise = "quantile"</code> this specifies the quantile to be used.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For simplicity details are given for matrix valued observations.
</p>
<p>Assume having a sample of <code class="reqn">p_1 \times p_2</code> matrix-valued observations which are realizations of the model
<code class="reqn">X = U_L Z U_R'+ N</code>,
where <code class="reqn">U_L</code> and <code class="reqn">U_R</code> are matrices with orthonormal columns, <code class="reqn">Z</code> is the random, zero mean <code class="reqn">k_1 \times k_2</code> core matrix with <code class="reqn">k_1 \leq p_1</code> and <code class="reqn">k_2 \leq p_2</code>. 
<code class="reqn">N</code> is <code class="reqn">p_1 \times p_2</code> matrix-variate noise that follows a matrix variate spherical distribution with <code class="reqn">E(N) = 0</code> and <code class="reqn">E(N N') = \sigma^2 I_{p_1}</code> and is independent from <code class="reqn">Z</code>. The goal is to estimate <code class="reqn">k_1</code> and <code class="reqn">k_2</code>. For that purpose the eigenvalues and eigenvectors of the left and right covariances are used. To evaluate the variation in the eigenvectors, in each mode the matrix <code class="reqn">X</code> is augmented with <code>naug</code> normally distributed components appropriately scaled by noise standard deviation. The procedure can be repeated <code>nrep</code> times to reduce random variation in the estimates. 
</p>
<p>The procedure needs an estimate of the noise variance and four options are available via the argument <code>noise</code>:
</p>

<ol>
<li> <p><code>noise = "median"</code>: Assumes that at least half of components are noise and uses thus the median of the pooled and scaled eigenvalues as an estimate.
</p>
</li>
<li> <p><code>noise = "quantile"</code>: Assumes that at least  100 <code>alpha</code> % of the components are noise and uses the mean of the lower <code>alpha</code> quantile of the pooled and scaled eigenvalues from all modes as an estimate.
</p>
</li>
<li> <p><code>noise = "last"</code>: Uses the pooled information from all modes and then the smallest eigenvalue as estimate.
</p>
</li>
<li> <p><code>noise = "known"</code>: Assumes the error variance is known and needs to be provided via <code>sigma2</code>. 
</p>
</li></ol>



<h3>Value</h3>

<p>A list of class 'taug' inheriting from class 'tladle' and containing:
</p>
<table role = "presentation">
<tr><td><code>U</code></td>
<td>
<p>list containing the modewise rotation matrices.</p>
</td></tr>
<tr><td><code>D</code></td>
<td>
<p>list containing the modewise eigenvalues.</p>
</td></tr>
<tr><td><code>S</code></td>
<td>
<p>array of the same size as <code>x</code> containing the principal components.</p>
</td></tr>
<tr><td><code>ResMode</code></td>
<td>
<p>a list with the modewise results which are lists containing:
</p>

<dl>
<dt>mode</dt><dd><p>label for the mode.</p>
</dd>
<dt>k</dt><dd><p>the order estimated for that mode.</p>
</dd>
<dt>fn</dt><dd><p>vector giving the measures of variation of the eigenvectors.</p>
</dd>
<dt>phin</dt><dd><p>normalized eigenvalues.</p>
</dd>
<dt>lambda</dt><dd><p>the unnormalized eigenvalues used to compute phin.</p>
</dd>
<dt>gn</dt><dd><p>the main criterion augmented order estimator.</p>
</dd>
<dt>comp</dt><dd><p>vector from 0 to the number of dimensions to be evaluated.</p>
</dd>
</dl>
</td></tr>
<tr><td><code>xmu</code></td>
<td>
<p>the data location</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>string with the name of the input data</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>string <code>tPCA</code>.</p>
</td></tr>
<tr><td><code>Sigma2</code></td>
<td>
<p>estimate of standardized sigma2 from the model described above or the standardized provided value. Sigma2 is the estimate for the variance of individual entries of <code>N</code>.</p>
</td></tr>
<tr><td><code>AllSigHat2</code></td>
<td>
<p>vector of noise variances used for each mode.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Klaus Nordhausen, Una Radojicic
</p>


<h3>References</h3>

<p><cite>Radojicic, U., Lietzen, N., Nordhausen, K. and Virta, J. (2021): Dimension Estimation in Two-Dimensional PCA. In S. Loncaric, T. Petkovic and D. Petrinovic (editors) &quot;Proceedings of the 12 International Symposium on Image and Signal Processing and Analysis (ISPA 2021)&quot;, 16-22. <a href="https://doi.org/10.1109/ISPA52656.2021.9552114">doi:10.1109/ISPA52656.2021.9552114</a>.</cite>
</p>
<p><cite>Radojicic, U., Lietzen, N., Nordhausen, K. and Virta, J. (2022): Order Determination for Tensor-valued Observations Using Data Augmentation. &lt;arXiv:2207.10423&gt;.</cite>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+tPCA">tPCA</a></code>, <code><a href="#topic+tPCAladle">tPCAladle</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(ICtest)


# matrix-variate example
n &lt;- 200
sig &lt;- 0.6

Z &lt;- rbind(sqrt(0.7)*rt(n,df=5)*sqrt(3/5),
           sqrt(0.3)*runif(n,-sqrt(3),sqrt(3)),
           sqrt(0.3)*(rchisq(n,df=3)-3)/sqrt(6),
           sqrt(0.9)*(rexp(n)-1),
           sqrt(0.1)*rlogis(n,0,sqrt(3)/pi),
           sqrt(0.5)*(rbeta(n,2,2)-0.5)*sqrt(20)
)

dim(Z) &lt;- c(3, 2, n)

U1 &lt;- rorth(12)[,1:3]
U2 &lt;- rorth(8)[,1:2]
U &lt;- list(U1=U1, U2=U2)
Y &lt;- tensorTransform2(Z,U,1:2)
EPS &lt;- array(rnorm(12*8*n, mean=0, sd=sig), dim=c(12,8,n))
X &lt;- Y + EPS


TEST &lt;- tPCAaug(X)
# Dimension should be 3 and 2 and (close to) sigma2 0.36
TEST

# Noise variance in i-th mode is equal to Sigma2 multiplied by the product 
# of number of colums of all modes except i-th one

TEST$Sigma2*c(8,12)
# This is returned as
TEST$AllSigHat2

# higher order tensor example

Z2 &lt;- rnorm(n*3*2*4*10)

dim(Z2) &lt;- c(3,2,4,10,n)

U2.1 &lt;- rorth(12)[ ,1:3]
U2.2 &lt;- rorth(8)[ ,1:2]
U2.3 &lt;- rorth(5)[ ,1:4]
U2.4 &lt;- rorth(20)[ ,1:10]

U2 &lt;- list(U1 = U2.1, U2 = U2.2, U3 = U2.3, U4 = U2.4)
Y2 &lt;- tensorTransform2(Z2, U2, 1:4)
EPS2 &lt;- array(rnorm(12*8*5*20*n, mean=0, sd=sig), dim=c(12, 8, 5, 20, n))
X2 &lt;- Y2 + EPS2


TEST2 &lt;- tPCAaug(X2, noise = "quantile", alpha =0.3)
TEST2
</code></pre>

<hr>
<h2 id='tPCAladle'>
Ladle Estimate for tPCA
</h2><span id='topic+tPCAladle'></span>

<h3>Description</h3>

<p>For r-dimensional tensors, the Ladle estimate for tPCA assumes that for a given mode <code class="reqn">m</code>, the last <code class="reqn">p_m - k_m</code> modewise eigenvalues are equal. Combining information from the eigenvalues and eigenvectors of the m-mode covariance matrix the ladle estimator yields estimates for <code class="reqn">k_1,...,k_r</code>.</p>


<h3>Usage</h3>

<pre><code class='language-R'>tPCAladle(x, n.boot = 200, ncomp = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tPCAladle_+3A_x">x</code></td>
<td>
<p>array of an order at least two with the last dimension corresponding to the sampling units.</p>
</td></tr>
<tr><td><code id="tPCAladle_+3A_n.boot">n.boot</code></td>
<td>
<p>number of bootstrapping samples to be used.</p>
</td></tr>
<tr><td><code id="tPCAladle_+3A_ncomp">ncomp</code></td>
<td>
<p>vector giving the number of components among which the ladle estimator is to be searched for each mode. The default follows the recommendation of Luo and Li 2016.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The model here assumes that the eigenvalues of the m-mode covariance matrix are of the form <code class="reqn">\lambda_{1,m} \geq ... \geq \lambda_{k_m,m} &gt; \lambda_{k_m+1,m} =  ... = \lambda_{p_m,m}</code>
and the goal is to estimate the value of <code class="reqn">k_m</code> for all modes. The ladle estimate for this purpose combines the values of the 
scaled eigenvalues and the variation of the eigenvectors based on bootstrapping. The idea there is that for distinct eigenvales the variation of the eigenvectors
is small and for equal eigenvalues the corresponding eigenvectors have large variation.
</p>
<p>This measure is then computed assuming <code class="reqn">k_m</code>=0,..., <code>ncomp[m]</code> and the ladle estimate for <code class="reqn">k_m</code> is the value where the measure takes its minimum. 
</p>


<h3>Value</h3>

<p>A list of class 'tladle' containing:
</p>
<table role = "presentation">
<tr><td><code>U</code></td>
<td>
<p>list containing the modewise rotation matrices.</p>
</td></tr>
<tr><td><code>D</code></td>
<td>
<p>list containing the modewise eigenvalues.</p>
</td></tr>
<tr><td><code>S</code></td>
<td>
<p>array of the same size as <code>x</code> containing the principal components.</p>
</td></tr>
<tr><td><code>ResMode</code></td>
<td>
<p>a list with the modewise results which are lists containing:
</p>

<dl>
<dt>mode</dt><dd><p>label for the mode.</p>
</dd>
<dt>k</dt><dd><p>the estimated value of k.</p>
</dd>
<dt>fn</dt><dd><p>vector giving the measures of variation of the eigenvectors using the bootstrapped eigenvectors for the different number of components.</p>
</dd>
<dt>phin</dt><dd><p>normalized eigenvalues.</p>
</dd>
<dt>lambda</dt><dd><p>the unnormalized eigenvalues used to compute phin.</p>
</dd>
<dt>gn</dt><dd><p>the main criterion for the ladle estimate - the sum of fn and phin. k is the value where gn takes its minimum.</p>
</dd>
<dt>comp</dt><dd><p>vector from 0 to the number of dimensions to be evaluated.</p>
</dd>
</dl>
</td></tr>
<tr><td><code>xmu</code></td>
<td>
<p>the data location</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>string with the name of the input data</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>string <code>tPCA</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Klaus Nordhausen
</p>


<h3>References</h3>

<p><cite>Koesner, C, Nordhausen, K. and Virta, J. (2019), Estimating the signal tensor dimension using tensorial PCA. Manuscript.</cite>
</p>
<p><cite>Luo, W. and Li, B. (2016), Combining Eigenvalues and Variation of Eigenvectors for Order Determination, Biometrika, 103, 875&ndash;887. &lt;doi:10.1093/biomet/asw051&gt;</cite>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+tPCA">tPCA</a></code>, <code><a href="#topic+ggtladleplot">ggtladleplot</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(ICtest)
n &lt;- 200
sig &lt;- 0.6

Z &lt;- rbind(sqrt(0.7)*rt(n,df=5)*sqrt(3/5),
           sqrt(0.3)*runif(n,-sqrt(3),sqrt(3)),
           sqrt(0.3)*(rchisq(n,df=3)-3)/sqrt(6),
           sqrt(0.9)*(rexp(n)-1),
           sqrt(0.1)*rlogis(n,0,sqrt(3)/pi),
           sqrt(0.5)*(rbeta(n,2,2)-0.5)*sqrt(20)
)

dim(Z) &lt;- c(3, 2, n)

U1 &lt;- rorth(12)[,1:3]
U2 &lt;- rorth(8)[,1:2]
U &lt;- list(U1=U1, U2=U2)
Y &lt;- tensorTransform2(Z,U,1:2)
EPS &lt;- array(rnorm(12*8*n, mean=0, sd=sig), dim=c(12,8,n))
X &lt;- Y + EPS


TEST &lt;- tPCAladle(X)
TEST
ggtladleplot(TEST)
</code></pre>

<hr>
<h2 id='tPP'>Projection pursuit for Tensor-Valued Observations</h2><span id='topic+tPP'></span>

<h3>Description</h3>

<p>Applies mode-wise projection pursuit to tensorial data with respect to the chosen measure of interestingness. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  tPP(x, nl = "pow3", eps = 1e-6, maxiter = 100)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tPP_+3A_x">x</code></td>
<td>
<p>Numeric array of an order at least three. It is assumed that the last dimension corresponds to the sampling units.</p>
</td></tr>
<tr><td><code id="tPP_+3A_nl">nl</code></td>
<td>
<p>The chosen measure of interestingness/objective function. Current choices include <code>pow3</code> (default) and <code>skew</code>, see the details below</p>
</td></tr>
<tr><td><code id="tPP_+3A_eps">eps</code></td>
<td>
<p>The convergence tolerance of the iterative algortihm.</p>
</td></tr>
<tr><td><code id="tPP_+3A_maxiter">maxiter</code></td>
<td>
<p>The maximum number of iterations.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The observed tensors (arrays) <code class="reqn">X</code> of size <code class="reqn">p_1 \times p_2 \times \ldots \times p_r</code> measured on <code class="reqn">N</code> units are standardized from each mode and then projected mode-wise onto the directions that maximize the <code class="reqn">L_2</code>-norm of the vector of the values <code class="reqn">E[G(u_k^T X X^T u_k)] - E[G(c^2)]</code>, where <code class="reqn">G</code> is the chosen objective function and <code class="reqn">c^2</code> obeys the chi-squared distribution with <code class="reqn">q</code> degress of freedom. Currently the function allows the choices <code class="reqn">G(x) = x^2</code> (<code>pow3</code>) and <code class="reqn">G(x) = x \sqrt x</code> (<code>skew</code>), which correspond roughly to the maximization of kurtosis and skewness, respectively. The algorithm is the multilinear extension of FastICA, where the names of the objective functions also come from.
</p>


<h3>Value</h3>

<p>A list with class 'tbss', inheriting from class 'bss', containing the following components: 
</p>
<table role = "presentation">
<tr><td><code>S</code></td>
<td>
<p>Array of the same size as x containing the estimated components.</p>
</td></tr>
<tr><td><code>W</code></td>
<td>
<p>List containing all the unmixing matrices.</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>The numbers of iteration used per mode.</p>
</td></tr>
<tr><td><code>Xmu</code></td>
<td>
<p>The data location.</p>
</td></tr>
<tr><td><code>datatype</code></td>
<td>
<p>Character string with value &quot;iid&quot;. Relevant for <code><a href="#topic+plot.tbss">plot.tbss</a></code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Joni Virta
</p>


<h3>References</h3>

<p><cite>Nordhausen, K. and Virta, J. (2018), Tensorial projection pursuit, Manuscript in preparation.</cite>
</p>
<p><cite>Hyvarinen, A. (1999) Fast and robust fixed-point algorithms for independent component analysis, IEEE transactions on Neural Networks 10.3: 626-634.</cite>
</p>


<h3>See Also</h3>

<p><code><a href="fICA.html#topic+fICA">fICA</a></code>, <code><a href="ICtest.html#topic+NGPP">NGPP</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 1000
S &lt;- t(cbind(rexp(n)-1,
             rnorm(n),
             runif(n, -sqrt(3), sqrt(3)),
             rt(n,5)*sqrt(0.6),
             (rchisq(n,1)-1)/sqrt(2),
             (rchisq(n,2)-2)/sqrt(4)))
             
dim(S) &lt;- c(3, 2, n)

A1 &lt;- matrix(rnorm(9), 3, 3)
A2 &lt;- matrix(rnorm(4), 2, 2)

X &lt;- tensorTransform(S, A1, 1)
X &lt;- tensorTransform(X, A2, 2)

tpp &lt;- tPP(X)

MD(tpp$W[[1]], A1)
MD(tpp$W[[2]], A2) 
tMD(tpp$W, list(A1, A2))

</code></pre>

<hr>
<h2 id='tSIR'>SIR for Tensor-Valued Observations 
</h2><span id='topic+tSIR'></span>

<h3>Description</h3>

<p>Computes the tensorial SIR. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  tSIR(x, y, h = 10, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tSIR_+3A_x">x</code></td>
<td>
<p>Numeric array of an order at least three. It is assumed that the last dimension corresponds to the sampling units.
</p>
</td></tr>
<tr><td><code id="tSIR_+3A_y">y</code></td>
<td>

<p>A numeric or factor response vector.
</p>
</td></tr>
<tr><td><code id="tSIR_+3A_h">h</code></td>
<td>

<p>The number of slices. If <code>y</code> is a factor the number of factor levels is automatically used as the number of slices.
</p>
</td></tr>
<tr><td><code id="tSIR_+3A_...">...</code></td>
<td>

<p>Arguments passed on to <code><a href="stats.html#topic+quantile">quantile</a></code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Computes the mode-wise sliced inverse regression (SIR) estimators for a tensor-valued data set and a univariate response variable.
</p>


<h3>Value</h3>

<p>A list with class 'tbss', inheriting from class 'bss', containing the following components: 
</p>
<table role = "presentation">
<tr><td><code>S</code></td>
<td>
<p>Array of the same size as x containing the predictors.</p>
</td></tr>
<tr><td><code>W</code></td>
<td>
<p>List containing all the unmixing matrices.</p>
</td></tr>
<tr><td><code>Xmu</code></td>
<td>
<p>The data location.</p>
</td></tr>
<tr><td><code>datatype</code></td>
<td>
<p>Character string with value &quot;iid&quot;. Relevant for <code><a href="#topic+plot.tbss">plot.tbss</a></code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Joni Virta, Klaus Nordhausen
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(zip.train)
x &lt;- zip.train

rows &lt;- which(x[, 1] == 0 | x[, 1] == 3)
x0 &lt;- x[rows, 2:257]
y0 &lt;- as.factor(x[rows, 1])

x0 &lt;- t(x0)
dim(x0) &lt;- c(16, 16, length(y0))


res &lt;- tSIR(x0, y0)
plot(res$S[1, 1, ], res$S[1, 2, ], col = y0)
</code></pre>

<hr>
<h2 id='tSOBI'>SOBI for Tensor-Valued Time Series
</h2><span id='topic+tSOBI'></span>

<h3>Description</h3>

<p>Computes the tensorial SOBI for time series where at each time point a tensor of order <code class="reqn">r</code> is observed. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tSOBI(x, lags = 1:12, maxiter = 100, eps = 1e-06)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tSOBI_+3A_x">x</code></td>
<td>
<p>Numeric array of an order at least two. It is assumed that the last dimension corresponds to the time.</p>
</td></tr>
<tr><td><code id="tSOBI_+3A_lags">lags</code></td>
<td>
<p>Vector of integers. Defines the lags used for the computations of the autocovariances.</p>
</td></tr>
<tr><td><code id="tSOBI_+3A_maxiter">maxiter</code></td>
<td>
<p>Maximum number of iterations. Passed on to <code><a href="JADE.html#topic+rjd">rjd</a></code>.</p>
</td></tr>
<tr><td><code id="tSOBI_+3A_eps">eps</code></td>
<td>
<p>Convergence tolerance. Passed on to <code><a href="JADE.html#topic+rjd">rjd</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>It is assumed that <code class="reqn">S</code> is a tensor (array) of size <code class="reqn">p_1 \times p_2 \times \ldots \times p_r</code> measured at time points <code class="reqn">1, \ldots, T</code>.
The assumption is that the elements of <code class="reqn">S</code> are uncorrelated, centered and weakly stationary time series and are mixed from each mode
<code class="reqn">m</code> by the mixing matrix <code class="reqn">A_m</code>, <code class="reqn">m = 1, \ldots, r</code>, yielding the observed time series <code class="reqn">X</code>. In R the sample of <code class="reqn">X</code> is saved as an <code><a href="base.html#topic+array">array</a></code> of dimensions
<code class="reqn">p_1, p_2, \ldots, p_r, T</code>.
</p>
<p><code>tSOBI</code> recovers then based on <code>x</code> the underlying uncorrelated time series <code class="reqn">S</code> by estimating the <code class="reqn">r</code> unmixing matrices 
<code class="reqn">W_1, \ldots, W_r</code> using the lagged joint autocovariances specified by <code>lags</code>.
</p>
<p>If <code>x</code> is a matrix, that is, <code class="reqn">r = 1</code>, the method reduces to SOBI and the function calls <code><a href="JADE.html#topic+SOBI">SOBI</a></code>.
</p>


<h3>Value</h3>

<p>A list with class 'tbss', inheriting from class 'bss', containing the following components: 
</p>
<table role = "presentation">
<tr><td><code>S</code></td>
<td>
<p>Array of the same size as x containing the estimated uncorrelated sources.</p>
</td></tr>
<tr><td><code>W</code></td>
<td>
<p>List containing all the unmixing matrices</p>
</td></tr>
<tr><td><code>Xmu</code></td>
<td>
<p>The data location.</p>
</td></tr>
<tr><td><code>datatype</code></td>
<td>
<p>Character string with value &quot;ts&quot;. Relevant for <code><a href="#topic+plot.tbss">plot.tbss</a></code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Joni Virta
</p>


<h3>References</h3>

<p><cite>Virta, J. and Nordhausen, K., (2017), Blind source separation of tensor-valued time series. Signal Processing 141, 204-216, <a href="https://doi.org/10.1016/j.sigpro.2017.06.008">doi:10.1016/j.sigpro.2017.06.008</a></cite> 
</p>


<h3>See Also</h3>

<p><code><a href="JADE.html#topic+SOBI">SOBI</a></code>, <code><a href="JADE.html#topic+rjd">rjd</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 1000
S &lt;- t(cbind(as.vector(arima.sim(n = n, list(ar = 0.9))),
             as.vector(arima.sim(n = n, list(ar = -0.9))),
             as.vector(arima.sim(n = n, list(ma = c(0.5, -0.5)))),
             as.vector(arima.sim(n = n, list(ar = c(-0.5, -0.3)))),
             as.vector(arima.sim(n = n, list(ar = c(0.5, -0.3, 0.1, -0.1), ma=c(0.7, -0.3)))),
             as.vector(arima.sim(n = n, list(ar = c(-0.7, 0.1), ma = c(0.9, 0.3, 0.1, -0.1))))))
dim(S) &lt;- c(3, 2, n)

A1 &lt;- matrix(rnorm(9), 3, 3)
A2 &lt;- matrix(rnorm(4), 2, 2)

X &lt;- tensorTransform(S, A1, 1)
X &lt;- tensorTransform(X, A2, 2)

tsobi &lt;- tSOBI(X)

MD(tsobi$W[[1]], A1)
MD(tsobi$W[[2]], A2) 
tMD(tsobi$W, list(A1, A2))

</code></pre>

<hr>
<h2 id='tTUCKER'>
Tucker (2) Transformation for a Tensor
</h2><span id='topic+tTUCKER'></span>

<h3>Description</h3>

<p>This is a Tucker (2) transformation of a data tensor where the sampling dimension is uncompressed. The transfromation is known also under many different names like multilinear principal components analysis or generalized low rank approximation of matrices if the tensorial data is matrixvalued.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tTUCKER(x, ranks, maxiter = 1000, eps = 1e-06)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tTUCKER_+3A_x">x</code></td>
<td>
<p>array with <code class="reqn">r+1</code> dimensions where the last dimension corresponds to the sampling units.</p>
</td></tr>
<tr><td><code id="tTUCKER_+3A_ranks">ranks</code></td>
<td>
<p>vector of length r giving the dimensions of the compressed core tensor.</p>
</td></tr>
<tr><td><code id="tTUCKER_+3A_maxiter">maxiter</code></td>
<td>
<p>maximum number of iterations for the algorithm.</p>
</td></tr>
<tr><td><code id="tTUCKER_+3A_eps">eps</code></td>
<td>
<p>convergence tolerance.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>As initial solution <code><a href="#topic+tPCA">tPCA</a></code> is used and iterated using an alternating least squares (ALS) approach, known also as higher order orthogonal iteration (HOOI).
</p>


<h3>Value</h3>

<p>A list containing the following components: 
</p>
<table role = "presentation">
<tr><td><code>S</code></td>
<td>
<p>array of the compressed tensor.</p>
</td></tr>
<tr><td><code>U</code></td>
<td>
<p>list containing the rotation matrices.</p>
</td></tr>
<tr><td><code>Xmu</code></td>
<td>
<p>the data location.</p>
</td></tr>
<tr><td><code>norm2xc</code></td>
<td>
<p>squared norm of the original data tensor after centering.</p>
</td></tr>
<tr><td><code>norm2rxc</code></td>
<td>
<p>squared norm of the reconstructed (centered) data tensor.</p>
</td></tr>
<tr><td><code>norm2ratio</code></td>
<td>
<p>the ratio norm2rxc/norm2xc.</p>
</td></tr>
<tr><td><code>mEV</code></td>
<td>
<p>list containing the eigenvalues from the m-mode covariance matrix when all but the relevant mode have be compressed.</p>
</td></tr>
<tr><td><code>tPCA</code></td>
<td>
<p>The output from <code><a href="#topic+tPCA">tPCA</a></code> which was used as initial value.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Klaus Nordhausen
</p>


<h3>References</h3>

<p><cite>Lu, H., Plataniotis, K. and  Venetsanopoulos, A. (2008), MPCA: Multilinear principal component analysis of tensor objects, IEEE Transactions on Neural Networks, 19, 18-39. <a href="https://doi.org/10.1109/TNN.2007.901277">doi:10.1109/TNN.2007.901277</a></cite>
</p>
<p><cite>Lietzen, N., Nordhausen, K. and Virta, J. (2019), Statistical analysis of second-order tensor decompositions, manuscript.</cite> 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+tPCA">tPCA</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(zip.train)
x &lt;- zip.train

rows &lt;- which(x[, 1] == 0 | x[, 1] == 1)
x0 &lt;- x[rows, 2:257]
y0 &lt;- x[rows, 1] + 1

x0 &lt;- t(x0)
dim(x0) &lt;- c(16, 16, 2199)

tucker &lt;-  tTUCKER(x0, ranks = c(2, 2), eps=1e-03)
pairs(t(apply(tucker$S, 3, c)), col=y0)

# To approximate the original data one uses then
x0r &lt;- tensorTransform2(tucker$S, tucker$U)


</code></pre>

<hr>
<h2 id='zip.test'> Handwritten Digit Recognition Data  </h2><span id='topic+zip.test'></span>

<h3>Description</h3>

<p>This .RD-file and the corresponding data set are originally from the R-package ElemStatLearn which has now been removed from CRAN.
</p>
<p>This example is a character recognition task: classification of handwritten 
numerals. This problem captured the attention of the machine learning 
and neural network community for many years, and has remained a benchmark problem 
in the field. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(zip.test)</code></pre>


<h3>Format</h3>

<p>The format is:
num [1:2007, 1:257] 9 6 3 6 6 0 0 0 6 9 ...
</p>


<h3>Details</h3>

<p>Normalized handwritten digits, automatically
scanned from envelopes by the U.S. Postal Service. The original
scanned digits are binary and of different sizes and orientations; the
images  here have been deslanted and size normalized, resulting
in 16 x 16 grayscale images (Le Cun et al., 1990).
</p>
<p>The data are in two gzipped files, and each line consists of the digit
id (0-9) followed by the 256 grayscale values.
</p>
<p>There are 7291 training observations and 2007 test observations,
distributed as follows:
0    1   2   3   4   5   6   7   8   9 Total
Train 1194 1005 731 658 652 556 664 645 542 644 7291
Test  359  264 198 166 200 160 170 147 166 177 2007
</p>
<p>or as proportions:
0    1   2    3    4    5    6    7    8    9 
Train 0.16 0.14 0.1 0.09 0.09 0.08 0.09 0.09 0.07 0.09
Test 0.18 0.13 0.1 0.08 0.10 0.08 0.08 0.07 0.08 0.09
</p>
<p>The test set is notoriously &quot;difficult&quot;, and a 2.5
excellent. These data were kindly made available by the neural network
group at AT&amp;T research labs (thanks to Yann Le Cunn).
</p>


<h3>References</h3>

<p><cite>Kjetil B Halvorsen (package maintainer) (2019), R-package ElemStatLearn: Data Sets, Functions and Examples from the Book: &quot;The Elements
of Statistical Learning, Data Mining, Inference, and Prediction&quot; by Trevor Hastie, Robert Tibshirani and Jerome Friedman</cite>
</p>

<hr>
<h2 id='zip.train'> Handwritten Digit Recognition Data  </h2><span id='topic+zip.train'></span>

<h3>Description</h3>

<p>This .RD-file and the corresponding data set are originally from the R-package ElemStatLearn which has now been removed from CRAN.
</p>
<p>This example is a character recognition task: classification of handwritten 
numerals. This problem captured the attention of the machine learning 
and neural network community for many years, and has remained a benchmark problem 
in the field. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(zip.train)</code></pre>


<h3>Format</h3>

<p>The format is:
num [1:7291, 1:257] 6 5 4 7 3 6 3 1 0 1 ...
</p>


<h3>Details</h3>

<p>Normalized handwritten digits, automatically
scanned from envelopes by the U.S. Postal Service. The original
scanned digits are binary and of different sizes and orientations; the
images  here have been deslanted and size normalized, resulting
in 16 x 16 grayscale images (Le Cun et al., 1990).
</p>
<p>The data are in two gzipped files, and each line consists of the digit
id (0-9) followed by the 256 grayscale values.
</p>
<p>There are 7291 training observations and 2007 test observations,
distributed as follows:
0    1   2   3   4   5   6   7   8   9 Total
Train 1194 1005 731 658 652 556 664 645 542 644 7291
Test  359  264 198 166 200 160 170 147 166 177 2007
</p>
<p>or as proportions:
0    1   2    3    4    5    6    7    8    9 
Train 0.16 0.14 0.1 0.09 0.09 0.08 0.09 0.09 0.07 0.09
Test 0.18 0.13 0.1 0.08 0.10 0.08 0.08 0.07 0.08 0.09
</p>
<p>The test set is notoriously &quot;difficult&quot;, and a 2.5
excellent. These data were kindly made available by the neural network
group at AT&amp;T research labs (thanks to Yann Le Cunn).
</p>


<h3>References</h3>

<p><cite>Kjetil B Halvorsen (package maintainer) (2019), R-package ElemStatLearn: Data Sets, Functions and Examples from the Book: &quot;The Elements
of Statistical Learning, Data Mining, Inference, and Prediction&quot; by Trevor Hastie, Robert Tibshirani and Jerome Friedman</cite>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(zip.train
)
findRows &lt;- function(zip, n) {
 # Find  n (random) rows with zip representing 0,1,2,...,9
 res &lt;- vector(length=10, mode="list")
 names(res) &lt;- 0:9
 ind &lt;- zip[,1]
 for (j in 0:9) {
    res[[j+1]] &lt;- sample( which(ind==j), n ) }
 return(res) }

# Making a plot like that on page 4:

digits &lt;- vector(length=10, mode="list")
names(digits) &lt;- 0:9
rows &lt;- findRows(zip.train, 6)
for (j in 0:9) {
    digits[[j+1]] &lt;- do.call("cbind", lapply(as.list(rows[[j+1]]), 
                       function(x) zip2image(zip.train, x)) )
}
im &lt;- do.call("rbind", digits)
image(im, col=gray(256:0/256), zlim=c(0,1), xlab="", ylab="" )    
</code></pre>

<hr>
<h2 id='zip2image'> function to convert row of zip file to format used by image() </h2><span id='topic+zip2image'></span>

<h3>Description</h3>

<p>This .RD-file and the corresponding function are originally from the R-package ElemStatLearn which has now been removed from CRAN.
</p>
<p>This is a utility function converting zip.train/zip.test data 
to format useful for plotting with the function <code><a href="graphics.html#topic+image">image</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>zip2image(zip, line)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="zip2image_+3A_zip">zip</code></td>
<td>
 <p><code><a href="#topic+zip.train">zip.train</a></code> or <code><a href="#topic+zip.test">zip.test</a></code>.</p>
</td></tr>
<tr><td><code id="zip2image_+3A_line">line</code></td>
<td>
<p> row of matrix to take </p>
</td></tr>
</table>


<h3>Value</h3>

<p>16 x 16 matrix suitable as argument for <code><a href="graphics.html#topic+image">image</a></code>.
</p>


<h3>Author(s)</h3>

<p> Kjetil Halvorsen </p>


<h3>References</h3>

<p><cite>Kjetil B Halvorsen (package maintainer) (2019), R-package ElemStatLearn: Data Sets, Functions and Examples from the Book: &quot;The Elements
of Statistical Learning, Data Mining, Inference, and Prediction&quot; by Trevor Hastie, Robert Tibshirani and Jerome Friedman</cite>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## See example section of help file for zip.train
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
