<!DOCTYPE html><html><head><title>Help for package rchemo</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {rchemo}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#aggmean'><p>Centers of classes</p></a></li>
<li><a href='#aicplsr'><p>AIC and Cp for Univariate PLSR Models</p></a></li>
<li><a href='#asdgap'><p>asdgap</p></a></li>
<li><a href='#blockscal'><p>Block autoscaling</p></a></li>
<li><a href='#cassav'><p>cassav</p></a></li>
<li><a href='#cglsr'><p>CG Least Squares Models</p></a></li>
<li><a href='#checkdupl'><p>Duplicated rows in datasets</p></a></li>
<li><a href='#checkna'><p>Find and count NA values in a dataset</p></a></li>
<li><a href='#covsel'><p>CovSel</p></a></li>
<li><a href='#dderiv'><p>Derivation by finite difference</p></a></li>
<li><a href='#detrend'><p>Polynomial de-trend transformation</p></a></li>
<li><a href='#dfplsr_cg'><p>Degrees of freedom of Univariate PLSR Models</p></a></li>
<li><a href='#dkplsr'><p>Direct KPLSR Models</p></a></li>
<li><a href='#dkrr'><p>Direct KRR Models</p></a></li>
<li><a href='#dmnorm'><p>Multivariate normal probability density</p></a></li>
<li><a href='#dtagg'><p>Summary statistics of data subsets</p></a></li>
<li><a href='#dummy'><p>Table of dummy variables</p></a></li>
<li><a href='#eposvd'><p>External parameter orthogonalization (EPO)</p></a></li>
<li><a href='#euclsq'><p>Matrix of distances</p></a></li>
<li><a href='#fda'><p>Factorial discriminant analysis</p></a></li>
<li><a href='#forages'><p>forages</p></a></li>
<li><a href='#getknn'><p>KNN selection</p></a></li>
<li><a href='#gridcv'><p>Cross-validation</p></a></li>
<li><a href='#gridscore'><p>Tuning of predictive models on a validation dataset</p></a></li>
<li><a href='#headm'><p>Display of the first part of a data set</p></a></li>
<li><a href='#interpl'><p>Resampling of spectra by interpolation methods</p></a></li>
<li><a href='#knnda'><p>KNN-DA</p></a></li>
<li><a href='#knnr'><p>KNN-R</p></a></li>
<li><a href='#kpca'><p>KPCA</p></a></li>
<li><a href='#kplsr'><p>KPLSR Models</p></a></li>
<li><a href='#kplsrda'><p>KPLSR-DA models</p></a></li>
<li><a href='#krbf'><p>Kernel functions</p></a></li>
<li><a href='#krr'><p>KRR (LS-SVMR)</p></a></li>
<li><a href='#krrda'><p>KRR-DA models</p></a></li>
<li><a href='#lda'><p>LDA and QDA</p></a></li>
<li><a href='#lmr'><p>Linear regression models</p></a></li>
<li><a href='#lmrda'><p>LMR-DA models</p></a></li>
<li><a href='#locw'><p>Locally weighted models</p></a></li>
<li><a href='#lwplsr'><p>KNN-LWPLSR</p></a></li>
<li><a href='#lwplsr_agg'><p>Aggregation of KNN-LWPLSR models with different numbers of LVs</p></a></li>
<li><a href='#lwplsrda'><p>KNN-LWPLS-DA Models</p></a></li>
<li><a href='#lwplsrda_agg'><p>Aggregation of KNN-LWPLSDA models with different numbers of LVs</p></a></li>
<li><a href='#matW'><p>Between and within covariance matrices</p></a></li>
<li><a href='#mavg'><p>Smoothing by moving average</p></a></li>
<li><a href='#mbplsr'><p>multi-block PLSR algorithms</p></a></li>
<li><a href='#mbplsrda'><p>multi-block PLSDA models</p></a></li>
<li><a href='#mse'><p>Residuals and prediction error rates</p></a></li>
<li><a href='#octane'><p>octane</p></a></li>
<li><a href='#odis'><p>Orthogonal distances from a PCA or PLS score space</p></a></li>
<li><a href='#orthog'><p>Orthogonalization of a matrix to another matrix</p></a></li>
<li><a href='#ozone'><p>ozone</p></a></li>
<li><a href='#pcasvd'><p>PCA algorithms</p></a></li>
<li><a href='#pinv'><p>Moore-Penrose pseudo-inverse of a matrix</p></a></li>
<li><a href='#plotjit'><p>Jittered plot</p></a></li>
<li><a href='#plotscore'><p>Plotting errors rates</p></a></li>
<li><a href='#plotsp'><p>Plotting spectra</p></a></li>
<li><a href='#plotxna'><p>Plotting Missing Data in a Matrix</p></a></li>
<li><a href='#plotxy'><p>2-d scatter plot</p></a></li>
<li><a href='#plskern'><p>PLSR algorithms</p></a></li>
<li><a href='#plsr_agg'><p>PLSR with aggregation of latent variables</p></a></li>
<li><a href='#plsrda'><p>PLSDA models</p></a></li>
<li><a href='#plsrda_agg'><p>PLSDA with aggregation of latent variables</p></a></li>
<li><a href='#rmgap'><p>Removing vertical gaps in spectra</p></a></li>
<li><a href='#rr'><p>Linear Ridge Regression</p></a></li>
<li><a href='#rrda'><p>RR-DA models</p></a></li>
<li><a href='#sampcla'><p>Within-class sampling</p></a></li>
<li><a href='#sampdp'><p>Duplex sampling</p></a></li>
<li><a href='#sampks'><p>Kennard-Stone sampling</p></a></li>
<li><a href='#savgol'><p>Savitzky-Golay smoothing</p></a></li>
<li><a href='#scordis'><p>Score distances (SD) in a PCA or PLS score space</p></a></li>
<li><a href='#segmkf'><p>Segments for cross-validation</p></a></li>
<li><a href='#selwold'><p>Heuristic selection of the dimension of a latent variable model with the Wold's criterion</p></a></li>
<li><a href='#snv'><p>Standard normal variate transformation (SNV)</p></a></li>
<li><a href='#sopls'><p>Block dimension reduction by SO-PLS</p></a></li>
<li><a href='#soplsrda'><p>Block dimension reduction by SO-PLS-DA</p></a></li>
<li><a href='#sourcedir'><p>Source R functions in a directory</p></a></li>
<li><a href='#summ'><p>Description of the quantitative variables of a data set</p></a></li>
<li><a href='#svmr'><p>SVM Regression and Discrimination</p></a></li>
<li><a href='#transform'><p>Generic transform function</p></a></li>
<li><a href='#vip'><p>Variable Importance in Projection (VIP)</p></a></li>
<li><a href='#wdist'><p>Distance-based weights</p></a></li>
<li><a href='#xfit'><p>Matrix fitting from a PCA or PLS model</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Dimension Reduction, Regression and Discrimination for
Chemometrics</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1-2</td>
</tr>
<tr>
<td>Description:</td>
<td>Data exploration and prediction with focus on high dimensional data and chemometrics.
    The package was initially designed about partial least squares regression and discrimination models and variants, in particular locally weighted PLS models (LWPLS). Then, it has been expanded to many other methods for analyzing high dimensional data.
    The name 'rchemo' comes from the fact that the package is orientated to chemometrics, but most of the provided methods are fully generic to other domains.
    Functions such as transform(), predict(), coef() and summary() are available. Tuning the predictive models is facilitated by generic functions gridscore() (validation dataset) and gridcv() (cross-validation). Faster versions are also available for models based on latent variables (LVs) (gridscorelv() and gridcvlv()) and ridge regularization (gridscorelb() and gridcvlb()).</td>
</tr>
<tr>
<td>Imports:</td>
<td>stats, graphics, grDevices, data.table, FNN, signal, e1071</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.0)</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/ChemHouse-group/rchemo/">https://github.com/ChemHouse-group/rchemo/</a></td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>LazyData:</td>
<td>yes</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-06-05 18:15:03 UTC; mbrandolini</td>
</tr>
<tr>
<td>Author:</td>
<td>Marion Brandolini-Bunlon [aut, cre],
  Benoit Jaillais [aut],
  Jean-Michel Roger [aut],
  Matthieu Lesnoff [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Marion Brandolini-Bunlon &lt;marion.brandolini-bunlon@inrae.fr&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-06-05 18:30:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='aggmean'>Centers of classes</h2><span id='topic+aggmean'></span>

<h3>Description</h3>

<p>Calculation of the centers (means) of classes of row observations of a data set. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>aggmean(X, y = NULL)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="aggmean_+3A_x">X</code></td>
<td>
<p>Data (<code class="reqn">n, p</code>) for which are calculated the centers (column-wise means).</p>
</td></tr>
<tr><td><code id="aggmean_+3A_y">y</code></td>
<td>
<p>Class membership (<code class="reqn">n, 1</code>) of the row of <code>X</code>. Default to <code>NULL</code> (all the rows of are considered).</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>ct</code></td>
<td>
<p>centers (column-wise means)</p>
</td></tr>
<tr><td><code>lev</code></td>
<td>
<p>classes</p>
</td></tr>
<tr><td><code>ni</code></td>
<td>
<p>number of observations in each per class</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
n &lt;- 8 ; p &lt;- 6
X &lt;- matrix(rnorm(n * p, mean = 10), ncol = p, byrow = TRUE)
y &lt;- sample(1:2, size = n, replace = TRUE)
aggmean(X, y)

data(forages)
Xtrain &lt;- forages$Xtrain
ytrain &lt;- forages$ytrain
table(ytrain)
u &lt;- aggmean(Xtrain, ytrain)$ct
headm(u)
plotsp(u, col = 1:4, main = "Means")
x &lt;- Xtrain[1:20, ]
plotsp(x, ylab = "Absorbance", col = "grey")
u &lt;- aggmean(x)$ct
plotsp(u, col = "red", add = TRUE, lwd = 2)

</code></pre>

<hr>
<h2 id='aicplsr'>AIC and Cp for Univariate PLSR Models</h2><span id='topic+aicplsr'></span>

<h3>Description</h3>

<p>Computation of the AIC and Mallows's <code class="reqn">Cp</code> criteria for univariate PLSR models (Lesnoff et al. 2021). This function may receive modifications in the future (work in progress).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
aicplsr(
    X, y, nlv, algo = NULL,
    meth = c("cg", "div", "cov"),
    correct = TRUE, B = 50, 
    print = FALSE, ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="aicplsr_+3A_x">X</code></td>
<td>
<p>A <code class="reqn">n x p</code> matrix or data frame of training observations.</p>
</td></tr>
<tr><td><code id="aicplsr_+3A_y">y</code></td>
<td>
<p>A vector of length <code class="reqn">n</code> of training responses. </p>
</td></tr>
<tr><td><code id="aicplsr_+3A_nlv">nlv</code></td>
<td>
<p>The maximal number of latent variables (LVs) to consider in the model.</p>
</td></tr>
<tr><td><code id="aicplsr_+3A_algo">algo</code></td>
<td>
<p>a PLS algorithm. Default to  <code>NULL</code> (<code><a href="#topic+plskern">plskern</a></code> is used).</p>
</td></tr>
<tr><td><code id="aicplsr_+3A_meth">meth</code></td>
<td>
<p>Method used for estimating <code class="reqn">df</code>. Possible values are <code>"cg"</code> (<code><a href="#topic+dfplsr_cg">dfplsr_cg</a></code>), <code>"cov"</code> (<code><a href="#topic+dfplsr_cov">dfplsr_cov</a></code>)or <code>"div"</code> (<code><a href="#topic+dfplsr_div">dfplsr_div</a></code>).</p>
</td></tr>
<tr><td><code id="aicplsr_+3A_correct">correct</code></td>
<td>
<p>Logical. If <code>TRUE</code> (default), the AICc corection is applied to the criteria.</p>
</td></tr>
<tr><td><code id="aicplsr_+3A_b">B</code></td>
<td>
<p>For <code>meth = "div"</code>: the number of observations in the data receiving perturbation (maximum is <code class="reqn">n</code>; see <code><a href="#topic+dfplsr_cov">dfplsr_cov</a></code>). For <code>meth = "cov"</code>: the number of bootstrap replications (see <code><a href="#topic+dfplsr_cov">dfplsr_cov</a></code>).</p>
</td></tr>
<tr><td><code id="aicplsr_+3A_print">print</code></td>
<td>
<p>Logical. If <code>TRUE</code>, fitting information are printed.</p>
</td></tr>
<tr><td><code id="aicplsr_+3A_...">...</code></td>
<td>
<p>Optionnal arguments to pass in <code>algo</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For a model with <code class="reqn">a</code> latent variables (LVs), function <code>aicplsr</code> calculates <code class="reqn">AIC</code> and <code class="reqn">Cp</code> by:
</p>
<p><code class="reqn">AIC(a) = n * log(SSR(a)) + 2 * (df(a) + 1)</code>  
</p>
<p><code class="reqn">Cp(a) = SSR(a) / n + 2 * df(a) * s2 / n</code>  
</p>
<p>where <code class="reqn">SSR</code> is the sum of squared residuals for the current evaluated model, <code class="reqn">df(a)</code> the estimated PLSR model complexity (i.e. nb. model's degrees of freedom), <code class="reqn">s2</code> an estimate of the irreductible error variance (computed from a low biased model) and <code class="reqn">n</code> the number of training observations.  
</p>
<p>By default (argument <code>correct</code>), the small sample size correction (so-called AICc) is applied to AIC and Cp for deucing the bias. 
</p>
<p>The functions returns two estimates of Cp (<code class="reqn">cp1</code> and <code class="reqn">cp2</code>), each corresponding to a different estimate of <code class="reqn">s2</code>.
</p>
<p>The model complexity  <code class="reqn">df</code> can be computed from three methods (argument <code>meth</code>).
</p>


<h3>Value</h3>

<table>
<tr><td><code>crit</code></td>
<td>
<p>dataframe with <code class="reqn">n</code>, and the etimated criteria (<code class="reqn">df</code>, <code class="reqn">ct</code>, <code class="reqn">ssr</code>, <code class="reqn">aic</code>, <code class="reqn">cp1, cp2</code>) for 0 to <code class="reqn">nlv</code> latent variables in the model.</p>
</td></tr>
<tr><td><code>delta</code></td>
<td>
<p>dataframe with the differences between the estimated values of <code class="reqn">aic</code>, <code class="reqn">cp1</code> and <code class="reqn">cp2</code>, and those of the model with the lowest estimated values of <code class="reqn">aic</code>, <code class="reqn">cp1</code> and <code class="reqn">cp2</code>, for models with 0 to <code class="reqn">nlv</code> latent variables</p>
</td></tr>
<tr><td><code>opt</code></td>
<td>
<p>vector with the optimal number of latent variables in the model (i.e. minimizing aic, cp1 and cp2 values)</p>
</td></tr>
</table>


<h3>References</h3>

<p>Burnham, K.P., Anderson, D.R., 2002. Model selection and multimodel inference: a practical informationtheoretic approach, 2nd ed. Springer, New York, NY, USA.
</p>
<p>Burnham, K.P., Anderson, D.R., 2004. Multimodel Inference: Understanding AIC and BIC in Model
Selection. Sociological Methods &amp; Research 33, 261-304. https://doi.org/10.1177/0049124104268644
</p>
<p>Efron, B., 2004. The Estimation of Prediction Error. Journal of the American Statistical Association 99,
619-632. https://doi.org/10.1198/016214504000000692
</p>
<p>Eubank, R.L., 1999. Nonparametric Regression and Spline Smoothing, 2nd ed, Statistics: Textbooks
and Monographs. Marcel Dekker, Inc., New York, USA.
</p>
<p>Hastie, T., Tibshirani, R.J., 1990. Generalized Additive Models, Monographs on statistics and applied
probablity. Chapman and Hall/CRC, New York, USA.
</p>
<p>Hastie, T., Tibshirani, R., Friedman, J., 2009. The elements of statistical learning: data mining,
inference, and prediction, 2nd ed. Springer, NewYork.
</p>
<p>Hastie, T., Tibshirani, R., Wainwright, M., 2015. Statistical Learning with Sparsity: The Lasso and
Generalizations. CRC Press
</p>
<p>Hurvich, C.M., Tsai, C.-L., 1989. Regression and Time Series Model Selection in Small Samples. Biometrika
76, 297. https://doi.org/10.2307/2336663
</p>
<p>Lesnoff, M., Roger, J.M., Rutledge, D.N., Submitted. Monte Carlo methods for estimating Mallows's Cp and AIC criteria for PLSR models. Illustration on agronomic spectroscopic NIR data. Journal of Chemometrics.
</p>
<p>Mallows, C.L., 1973. Some Comments on Cp. Technometrics 15, 661-675.
https://doi.org/10.1080/00401706.1973.10489103
</p>
<p>Ye, J., 1998. On Measuring and Correcting the Effects of Data Mining and Model Selection. Journal of
the American Statistical Association 93, 120-131. https://doi.org/10.1080/01621459.1998.10474094
</p>
<p>Zuccaro, C., 1992. Mallows'Cp Statistic and Model Selection in Multiple Linear Regression. International Journal of Market Research. 34, 1-10. https://doi.org/10.1177/147078539203400204
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(cassav)

Xtrain &lt;- cassav$Xtrain
ytrain &lt;- cassav$ytrain

nlv &lt;- 25
res &lt;- aicplsr(Xtrain, ytrain, nlv = nlv)
names(res)
headm(res$crit)

z &lt;- res$crit
oldpar &lt;- par(mfrow = c(1, 1))
par(mfrow = c(1, 4))
plot(z$df[-1])
plot(z$aic[-1], type = "b", main = "AIC")
plot(z$cp1[-1], type = "b", main = "Cp1")
plot(z$cp2[-1], type = "b", main = "Cp2")
par(oldpar)

</code></pre>

<hr>
<h2 id='asdgap'>asdgap</h2><span id='topic+asdgap'></span>

<h3>Description</h3>

<p>ASD NIRS dataset, with gaps in the spectra at wawelengths = 1000 and 1800 nm.</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(asdgap)</code></pre>


<h3>Format</h3>

<p>A list with 1 element: the data frame <code class="reqn">X</code> with 5 spectra and 2151 variables.
</p>


<h3>References</h3>

<p>Thanks to J.-F. Roger (Inrae, France) and M. Ecarnot (Inrae, France) for the method.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(asdgap)
names(asdgap)
X &lt;- asdgap$X

numcol &lt;- which(colnames(X) == "1000" | colnames(X) == "1800")
numcol
plotsp(X, lwd = 1.5)
abline(v = as.numeric(colnames(X)[1]) + numcol - 1, col = "grey", lty = 3)

</code></pre>

<hr>
<h2 id='blockscal'>Block autoscaling</h2><span id='topic+blockscal'></span><span id='topic+mblocks'></span><span id='topic+hconcat'></span>

<h3>Description</h3>

<p>Functions managing blocks of data.
</p>
<p>- <code>blockscal</code>: Autoscales a list of blocks (i.e. sets of columns) of a training X-data, and eventually the blocks of new X-data. The scaling factor (computed on the training) is the &quot;norm&quot; of the block, i.e. the square root of the sum of the variances of each column of the block. 
</p>
<p>- <code>mblocks</code>: Makes a list of blocks from X-data.
</p>
<p>- <code>hconcat</code>: Concatenates horizontally the blocks of a list. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
blockscal(Xtrain, X = NULL, weights = NULL)

mblocks(X, blocks)

hconcat(X)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="blockscal_+3A_xtrain">Xtrain</code></td>
<td>
<p>A list of blocks of training X-data</p>
</td></tr>
<tr><td><code id="blockscal_+3A_x">X</code></td>
<td>
<p>For <code>blockscal</code>: A list of blocks of new X-data. For <code>mblocks</code>: X-data. For <code>hconcat</code>: a list of blocks of X-data.</p>
</td></tr>
<tr><td><code id="blockscal_+3A_blocks">blocks</code></td>
<td>
<p>A list (of same length as the number of blocks) giving the column numbers in <code>X</code>.</p>
</td></tr>
<tr><td><code id="blockscal_+3A_weights">weights</code></td>
<td>
<p>Weights (<code class="reqn">n, 1</code>) to apply to the training observations. Internally, weights are &quot;normalized&quot; to sum to 1. Default to <code>NULL</code> (weights are set to <code class="reqn">1 / n</code>).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>For <code>mblocks</code>: a list of blocks of X-data.
</p>
<p>For <code>hconcat</code>: a matrix concatenating a list of data blocks.
</p>
<p>For <code>blockscal</code>: 
</p>
<table>
<tr><td><code>Xtrain</code></td>
<td>
<p>A list of blocks of training X-data, after block autoscaling.</p>
</td></tr>
<tr><td><code>X</code></td>
<td>
<p>A list of blocks of new X-data, after block autoscaling.</p>
</td></tr>  
<tr><td><code>disp</code></td>
<td>
<p>The scaling factor (computed on the training).</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The second example is equivalent to MB-PLSR
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
n &lt;- 10 ; p &lt;- 10
Xtrain &lt;- matrix(rnorm(n * p), ncol = p)
ytrain &lt;- rnorm(n)
m &lt;- 2

Xtest &lt;- matrix(rnorm(m * p), ncol = p)
colnames(Xtest) &lt;- paste("v", 1:p, sep = "")
Xtrain
Xtest

blocks &lt;- list(1:2, 4, 6:8)
zXtrain &lt;- mblocks(Xtrain, blocks = blocks)
zXtest &lt;- mblocks(Xtest, blocks = blocks)

zXtrain
blockscal(zXtrain, zXtest)

res &lt;- blockscal(zXtrain, zXtest)
hconcat(res$Xtrain)
hconcat(res$X)

## example of equivalence with MB-PLSR

n &lt;- 10 ; p &lt;- 10
Xtrain &lt;- matrix(rnorm(n * p), ncol = p)
ytrain &lt;- rnorm(n)
m &lt;- 2

Xtest &lt;- matrix(rnorm(m * p), ncol = p)
colnames(Xtest) &lt;- paste("v", 1:p, sep = "")
Xtrain
Xtest

blocks &lt;- list(1:2, 4, 6:8)
X1 &lt;- mblocks(Xtrain, blocks = blocks)
X1 &lt;- lapply(1:length(X1), function(x) scale(X1[[x]]))
res &lt;- blockscal(X1)
zXtrain &lt;- hconcat(res$Xtrain)

nlv &lt;- 3
fm &lt;- plskern(zXtrain, ytrain, nlv = nlv)

</code></pre>

<hr>
<h2 id='cassav'>cassav</h2><span id='topic+cassav'></span>

<h3>Description</h3>

<p>A NIRS dataset (absorbance) describing the concentration of a natural pigment in samples of tropical shrubs. Spectra were recorded from 400 to 2498 nm at 2 nm intervals.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(cassav)</code></pre>


<h3>Format</h3>

<p>A list with the following components: 
</p>

<p>For the reference (calibration) data:
</p>
<dl>
<dt><code>Xtrain</code></dt><dd><p>A matrix whose rows are the NIR absorbance spectra (= log10(1 / Reflectance)).</p>
</dd>
<dt><code>ytrain</code></dt><dd><p>A vector of the response variable (pigment concentration).</p>
</dd>
<dt><code>year</code></dt><dd><p>A vector of the year of data collection (2009 to 2012; the test set correponds to year 2013).</p>
</dd>
</dl>
<p>For the test data:
</p>
<dl>
<dt><code>Xtest</code></dt><dd><p>A matrix whose rows are the NIR absorbance spectra (= log10(1 / Reflectance)).</p>
</dd>
<dt><code>ytest</code></dt><dd><p>A vector of the response variable (pigment concentration).</p>
</dd>
</dl>



<h3>References</h3>

<p>Davrieux, F., Dufour, D., Dardenne, P., Belalcazar, J., Pizarro, M., Luna, J., Londono, L., Jaramillo, A., Sanchez, T., Morante, N., Calle, F., Becerra Lopez-Lavalle, L., Ceballos, H., 2016. LOCAL regression algorithm improves near infrared spectroscopy predictions when the target constituent evolves in breeding populations. Journal of Near Infrared Spectroscopy 24, 109. https://doi.org/10.1255/jnirs.1213
</p>
<p>CIAT Cassava Project (Colombia), CIRAD Qualisud Research Unit, and funded mainly by the CGIAR Research Program on Roots, Tubers and Bananas (RTB) with support from CGIAR Trust Fund contributors (https://www.cgiar.org/funders/).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(cassav)
str(cassav)

</code></pre>

<hr>
<h2 id='cglsr'>CG Least Squares Models</h2><span id='topic+cglsr'></span><span id='topic+coef.Cglsr'></span><span id='topic+predict.Cglsr'></span>

<h3>Description</h3>

<p>Conjugate gradient algorithm (CG) for the normal equations (CGLS algorithm 7.4.1, Bjorck 1996, p.289)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
cglsr(X, y, nlv, reorth = TRUE, filt = FALSE)

## S3 method for class 'Cglsr'
coef(object, ..., nlv = NULL)  

## S3 method for class 'Cglsr'
predict(object, X, ..., nlv = NULL)  

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cglsr_+3A_x">X</code></td>
<td>
<p>For the main function: Training X-data (<code class="reqn">n, p</code>). &mdash; For auxiliary functions: New X-data (<code class="reqn">m, p</code>) to consider. </p>
</td></tr>
<tr><td><code id="cglsr_+3A_y">y</code></td>
<td>
<p>Univariate training Y-data (<code class="reqn">n, 1</code>).</p>
</td></tr>
<tr><td><code id="cglsr_+3A_nlv">nlv</code></td>
<td>
<p>The number(s) of CG iterations.</p>
</td></tr>
<tr><td><code id="cglsr_+3A_reorth">reorth</code></td>
<td>
<p>Logical. If <code>TRUE</code>, a Gram-Schmidt reorthogonalization of the normal equation residual vectors is done.</p>
</td></tr>
<tr><td><code id="cglsr_+3A_filt">filt</code></td>
<td>
<p>Logical. If <code>TRUE</code>, the filter factors are computed (output <code>F</code>).</p>
</td></tr>
<tr><td><code id="cglsr_+3A_object">object</code></td>
<td>
<p>For auxiliary functions: A fitted model, output of a call to the main functions.</p>
</td></tr>
<tr><td><code id="cglsr_+3A_...">...</code></td>
<td>
<p>For auxiliary functions: Optional arguments. Not used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The code for re-orthogonalization (Hansen 1998) and filter factors (Vogel 1987, Hansen 1998) computations is a transcription (with few adaptations) of the matlab function 'cgls' (Saunders et al. https://web.stanford.edu/group/SOL/software/cgls/; Hansen 2008).
</p>
<p>The filter factors can be used to compute the model complexity of CGLSR and PLSR models (see <code><a href="#topic+dfplsr_cg">dfplsr_cg</a></code>).
</p>
<p>Data <code class="reqn">X</code> and <code class="reqn">y</code> are internally centered. 
</p>
<p>Missing values are not allowed.
</p>


<h3>Value</h3>

	
<p>For <code>cglsr</code>: 
</p>
<table>
<tr><td><code>B</code></td>
<td>
<p>matrix with the model coefficients for the fix nlv.</p>
</td></tr>
<tr><td><code>gnew</code></td>
<td>
<p>squared norm of the s vector</p>
</td></tr>
<tr><td><code>xmeans</code></td>
<td>
<p>variable means for the training X-data</p>
</td></tr>
<tr><td><code>ymeans</code></td>
<td>
<p>variable means for the training Y-data</p>
</td></tr>
<tr><td><code>F</code></td>
<td>
<p>If <code>filt = TRUE</code>, the filter factors</p>
</td></tr>
</table>
<p>For <code>coef.Cglsr</code> : 
</p>
<table>
<tr><td><code>int</code></td>
<td>
<p>intercept value.</p>
</td></tr>
<tr><td><code>B</code></td>
<td>
<p>matrix with the model coefficients.</p>
</td></tr>
</table>
<p>For <code>predict.Cglsr</code> : 
</p>
<table>
<tr><td><code>pred</code></td>
<td>
<p>list of matrices, with the predicted values for each number <code>nlv</code> of CG iterations</p>
</td></tr>
</table>


<h3>References</h3>

<p>Bjorck, A., 1996. Numerical Methods for Least Squares Problems, Other Titles in Applied Mathematics. 
Society for Industrial and Applied Mathematics. https://doi.org/10.1137/1.9781611971484
</p>
<p>Hansen, P.C., 1998. Rank-Deficient and Discrete Ill-Posed Problems, Mathematical Modeling and Computation. 
Society for Industrial and Applied Mathematics. https://doi.org/10.1137/1.9780898719697
</p>
<p>Hansen, P.C., 2008. Regularization Tools version 4.0 for Matlab 7.3. 
Numer Algor 46, 189-194. https://doi.org/10.1007/s11075-007-9136-9
</p>
<p>Manne R. Analysis of two partial-least-squares algorithms for multivariate calibration. Chemometrics Intell.
Lab. Syst. 1987; 2: 187-197.
</p>
<p>Phatak A, De Hoog F. Exploiting the connection between
PLS, Lanczos methods and conjugate gradients: alternative proofs of some properties of PLS. J. Chemometrics
2002; 16: 361-367.
</p>
<p>Vogel, C. R.,  &quot;Solving ill-conditioned linear systems using the conjugate gradient method&quot;, 
Report, Dept. of Mathematical Sciences, Montana State University, 1987.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
z &lt;- ozone$X
u &lt;- which(!is.na(rowSums(z)))
X &lt;- z[u, -4]
y &lt;- z[u, 4]
dim(X)
headm(X)
Xtest &lt;- X[1:2, ]
ytest &lt;- y[1:2]

nlv &lt;- 10
fm &lt;- cglsr(X, y, nlv = nlv)

coef(fm)
coef(fm, nlv = 1)

predict(fm, Xtest)
predict(fm, Xtest, nlv = 1:3)

pred &lt;- predict(fm, Xtest)$pred
msep(pred, ytest)

cglsr(X, y, nlv = 5, filt = TRUE)$F


</code></pre>

<hr>
<h2 id='checkdupl'>Duplicated rows in datasets</h2><span id='topic+checkdupl'></span>

<h3>Description</h3>

<p>Finding and removing duplicated row observations in datasets.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
checkdupl(X, Y = NULL, digits = NULL)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="checkdupl_+3A_x">X</code></td>
<td>
<p>A dataset.</p>
</td></tr>
<tr><td><code id="checkdupl_+3A_y">Y</code></td>
<td>
<p>A dataset compared to <code>X</code>.</p>
</td></tr>
<tr><td><code id="checkdupl_+3A_digits">digits</code></td>
<td>
<p>The number of digits when rounding the data before the duplication test. Default to <code>NULL</code> (no rounding.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a dataframe with the row numbers in the first and second datasets that are identical, and the values of the variables.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
X1 &lt;- matrix(c(1:5, 1:5, c(1, 2, 7, 4, 8)), nrow = 3, byrow = TRUE)
dimnames(X1) &lt;- list(1:3, c("v1", "v2", "v3", "v4", "v5"))

X2 &lt;- matrix(c(6:10, 1:5, c(1, 2, 7, 6, 12)), nrow = 3, byrow = TRUE)
dimnames(X2) &lt;- list(1:3, c("v1", "v2", "v3", "v4", "v5"))

X1
X2

checkdupl(X1, X2)

checkdupl(X1)

checkdupl(matrix(rnorm(20), nrow = 5))

res &lt;- checkdupl(X1)
s &lt;- unique(res$rownum2)
zX1 &lt;- X1[-s, ]
zX1

</code></pre>

<hr>
<h2 id='checkna'>Find and count NA values in a dataset</h2><span id='topic+checkna'></span>

<h3>Description</h3>

<p>Find and count NA values in each row observation of a dataset.</p>


<h3>Usage</h3>

<pre><code class='language-R'>checkna(X)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="checkna_+3A_x">X</code></td>
<td>
<p>A dataset.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame summarizing the numbers of NA by rows.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
X &lt;- data.frame(
  v1 = c(NA, rnorm(9)), 
  v2 = c(NA, rnorm(8), NA),
  v3 = c(NA, NA, NA, rnorm(7))
)
X

checkna(X)

</code></pre>

<hr>
<h2 id='covsel'>CovSel</h2><span id='topic+covsel'></span>

<h3>Description</h3>

<p>Variable selection for high-dimensionnal data with the COVSEL method (Roger et al. 2011).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
covsel(X, Y, nvar = NULL, scaly = TRUE, weights = NULL)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="covsel_+3A_x">X</code></td>
<td>
<p>X-data (<code class="reqn">n, p</code>).</p>
</td></tr>
<tr><td><code id="covsel_+3A_y">Y</code></td>
<td>
<p>Y-data (<code class="reqn">n, q</code>).</p>
</td></tr>
<tr><td><code id="covsel_+3A_nvar">nvar</code></td>
<td>
<p>Number of variables to select in <code>X</code>.</p>
</td></tr>
<tr><td><code id="covsel_+3A_scaly">scaly</code></td>
<td>
<p>If <code>TRUE</code> (default), each column of <code class="reqn">Y</code> is scaled by its standard deviation.</p>
</td></tr>
<tr><td><code id="covsel_+3A_weights">weights</code></td>
<td>
<p>Weights (<code class="reqn">n, 1</code>) to apply to the training observations. Internally, weights are &quot;normalized&quot; to sum to 1. Default to <code>NULL</code> (weights are set to <code class="reqn">1 / n</code>).</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>sel</code></td>
<td>
<p>A dataframe where variable <code>sel</code> shows the column numbers of the variables selected in <code class="reqn">X</code>.</p>
</td></tr>
<tr><td><code>weights</code></td>
<td>
<p>The weights used for the row observations.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Roger, J.M., Palagos, B., Bertrand, D., Fernandez-Ahumada, E., 2011. CovSel: Variable selection for highly multivariate and multi-response calibration: Application to IR spectroscopy. Chem. Lab. Int. Syst. 106, 216-223. 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
n &lt;- 6 ; p &lt;- 4
X &lt;- matrix(rnorm(n * p), ncol = p)
Y &lt;- matrix(rnorm(n * 2), ncol = 2)

covsel(X, Y, nvar = 3)

</code></pre>

<hr>
<h2 id='dderiv'>Derivation by finite difference</h2><span id='topic+dderiv'></span>

<h3>Description</h3>

<p>Calculation of the first derivatives, by finite differences, of the row observations (e.g. spectra) of a dataset. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dderiv(X, n = 5, ts = 1)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dderiv_+3A_x">X</code></td>
<td>
<p>X-data (<code class="reqn">n, p</code>).</p>
</td></tr>
<tr><td><code id="dderiv_+3A_n">n</code></td>
<td>
<p>The number of points (i.e. columns of <code>X</code>) defining the window over wich is calculate each finite difference. The derivation is calculated for the point at the center of the window. Therefore, <code>n</code> must be an odd integer, and be higher or equal to 3.</p>
</td></tr>
<tr><td><code id="dderiv_+3A_ts">ts</code></td>
<td>
<p>A scaling factor for the finite differences (by default, <code>ts</code> = 1.)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix of the transformed data.</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(cassav)

X &lt;- cassav$Xtest

n &lt;- 15
Xp_derivate1 &lt;- dderiv(X, n = n)
Xp_derivate2 &lt;- dderiv(dderiv(X, n), n)

oldpar &lt;- par(mfrow = c(1, 1))
par(mfrow = c(1, 2))
plotsp(X, main = "Signal")
plotsp(Xp_derivate1, main = "Corrected signal")
abline(h = 0, lty = 2, col = "grey")
par(oldpar)

</code></pre>

<hr>
<h2 id='detrend'>Polynomial de-trend transformation</h2><span id='topic+detrend'></span>

<h3>Description</h3>

<p>Polynomial de-trend transformation of row observations (e.g. spectra) of a dataset. The function fits an orthogonal polynom of a given degree to each observation and returns the residuals.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>detrend(X, degree = 1)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="detrend_+3A_x">X</code></td>
<td>
<p>X-data (<code class="reqn">n, p</code>).</p>
</td></tr>
<tr><td><code id="detrend_+3A_degree">degree</code></td>
<td>
<p>Degree of the polynom.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>detrend</code> uses function <code><a href="stats.html#topic+poly">poly</a></code> of package <code>stats</code>.</p>


<h3>Value</h3>

<p>A matrix of the transformed data.</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(cassav)

X &lt;- cassav$Xtest

degree &lt;- 1
Xp &lt;- detrend(X, degree = degree)

oldpar &lt;- par(mfrow = c(1, 1))
par(mfrow = c(1, 2))
plotsp(X, main = "Signal")
plotsp(Xp, main = "Corrected signal")
abline(h = 0, lty = 2, col = "grey")
par(oldpar)

</code></pre>

<hr>
<h2 id='dfplsr_cg'>Degrees of freedom of Univariate PLSR Models</h2><span id='topic+dfplsr_cg'></span><span id='topic+dfplsr_cov'></span><span id='topic+dfplsr_div'></span>

<h3>Description</h3>

<p>Computation of the model complexity <code class="reqn">df</code> (number of degrees of freedom) of univariate PLSR models (with intercept). See Lesnoff et al. 2021 for an illustration.
</p>
<p>(1) Estimation from the CGLSR algorithm (Hansen, 1998).
</p>
<p>- <code>dfplsr_cov</code>
</p>
<p>(2) Monte Carlo estimation (Ye, 1998 and Efron, 2004). Details in relation with the functions are given in Lesnoff et al. 2021.
</p>
<p>- <code>dfplsr_cov</code>: The covariances are computed  by parametric bootstrap (Efron, 2004, Eq. 2.16). The residual variance <code class="reqn">sigma^2</code> is estimated from a low-biased model.
</p>
<p>- <code>dfplsr_div</code>: The divergencies <code class="reqn">dy_fit/dy</code> are computed by perturbation analysis(Ye, 1998 and Efron, 2004). This is a Stein unbiased risk estimation (SURE) of <code class="reqn">df</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
dfplsr_cg(X, y, nlv, reorth = TRUE)

dfplsr_cov(
    X, y, nlv, algo = NULL,
    maxlv = 50, B = 30, print = FALSE, ...)

dfplsr_div(
    X, y, nlv, algo = NULL,
    eps = 1e-2, B = 30, print = FALSE, ...) 

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dfplsr_cg_+3A_x">X</code></td>
<td>
<p>A <code class="reqn">n x p</code> matrix or data frame of training observations.</p>
</td></tr>
<tr><td><code id="dfplsr_cg_+3A_y">y</code></td>
<td>
<p>A vector of length <code class="reqn">n</code> of training responses.</p>
</td></tr>
<tr><td><code id="dfplsr_cg_+3A_nlv">nlv</code></td>
<td>
<p>The maximal number of latent variables (LVs) to consider in the model.</p>
</td></tr>
<tr><td><code id="dfplsr_cg_+3A_reorth">reorth</code></td>
<td>
<p>For <code>dfplsr_cg</code>: Logical. If <code>TRUE</code>, a Gram-Schmidt reorthogonalization of the normal equation residual vectors is done.</p>
</td></tr>
<tr><td><code id="dfplsr_cg_+3A_algo">algo</code></td>
<td>
<p>a PLS algorithm. Default to  <code>NULL</code> (<code><a href="#topic+plskern">plskern</a></code> is used).</p>
</td></tr>
<tr><td><code id="dfplsr_cg_+3A_maxlv">maxlv</code></td>
<td>
<p>For <code>dfplsr_cov</code>: dDmension of the PLSR model (nb. LVs) used for parametric bootstrap.</p>
</td></tr>
<tr><td><code id="dfplsr_cg_+3A_eps">eps</code></td>
<td>
<p>For <code>dfplsr_div</code>: The <code class="reqn">epsilon</code> quantity used for scaling the perturbation analysis.</p>
</td></tr>
<tr><td><code id="dfplsr_cg_+3A_b">B</code></td>
<td>
<p>For <code>dfplsr_cov</code>: Number of bootstrap replications. For <code>dfplsr_div</code>: number of observations in the data receiving perturbation (the maximum is <code class="reqn">n</code>).</p>
</td></tr>
<tr><td><code id="dfplsr_cg_+3A_print">print</code></td>
<td>
<p>Logical. If <code>TRUE</code>, fitting information are printed.</p>
</td></tr>
<tr><td><code id="dfplsr_cg_+3A_...">...</code></td>
<td>
<p>Optionnal arguments to pass in the function defined in <code>algo</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Missing values are not allowed.
</p>
<p>The example below reproduces the numerical illustration given by Kramer &amp; Sugiyama 2011 on the Ozone data (Fig. 1, center).
The <code class="reqn">pls.model</code> function from the R package &quot;plsdof&quot; v0.2-9 (Kramer &amp; Braun 2019) is used for <code class="reqn">df</code> calculations (<code class="reqn">df.kramer</code>), and automatically scales the X matrix before PLS. The example scales also X for consistency when using the other functions.
</p>
<p>For the Monte Carlo estimations, <code>B</code> Should be increased for more stability
</p>


<h3>Value</h3>

<p>A list of outputs :
</p>
<table>
<tr><td><code>df</code></td>
<td>
<p>vector with the model complexity for the models with <code class="reqn">a = 0, 1, ..., nlv</code> components.</p>
</td></tr>
<tr><td><code>cov</code></td>
<td>
<p>For <code>dfplsr_cov</code>: vector with covariances, computed  by parametric bootstrap.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Efron, B., 2004. The Estimation of Prediction Error. Journal of the American Statistical Association 99,
619-632. https://doi.org/10.1198/016214504000000692
</p>
<p>Hastie, T., Tibshirani, R.J., 1990. Generalized Additive Models, Monographs on statistics and applied
probablity. Chapman and Hall/CRC, New York, USA.
</p>
<p>Hastie, T., Tibshirani, R., Friedman, J., 2009. The elements of statistical learning: data mining,
inference, and prediction, 2nd ed. Springer, NewYork.
</p>
<p>Hastie, T., Tibshirani, R., Wainwright, M., 2015. Statistical Learning with Sparsity: The Lasso and
Generalizations. CRC Press
</p>
<p>Kramer, N., Braun, M.L., 2007. Kernelizing PLS, degrees of freedom, and efficient model selection, in: Proceedings of the 24th International Conference on Machine Learning, ICML 07. Association for Computing Machinery, New York, NY, USA, pp. 441-448. https://doi.org/10.1145/1273496.1273552
</p>
<p>Kramer, N., Sugiyama, M., 2011. The Degrees of Freedom of Partial Least Squares Regression. Journal of the American Statistical Association 106, 697-705. https://doi.org/10.1198/jasa.2011.tm10107
</p>
<p>Kramer, N., Braun, M. L. 2019. plsdof: Degrees of Freedom and Statistical Inference for Partial Least Squares Regression. R package version 0.2-9. https://cran.r-project.org
</p>
<p>Lesnoff, M., Roger, J.M., Rutledge, D.N., 2021. Monte Carlo methods for estimating Mallow's Cp and AIC criteria for PLSR models. Illustration on agronomic spectroscopic NIR data. Journal of Chemometrics, 35(10), e3369. https://doi.org/10.1002/cem.3369
</p>
<p>Stein, C.M., 1981. Estimation of the Mean of a Multivariate Normal
Distribution. The Annals of Statistics 9, 1135-1151.
</p>
<p>Ye, J., 1998. On Measuring and Correcting the Effects of Data Mining and Model Selection. Journal of
the American Statistical Association 93, 120-131. https://doi.org/10.1080/01621459.1998.10474094
</p>
<p>Zou, H., Hastie, T., Tibshirani, R., 2007. On the degrees of freedom of the lasso. The Annals of
Statistics 35, 2173-2192. https://doi.org/10.1214/009053607000000127
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## EXAMPLE 1

data(ozone)

z &lt;- ozone$X
u &lt;- which(!is.na(rowSums(z)))
X &lt;- z[u, -4]
y &lt;- z[u, 4]
dim(X)

Xs &lt;- scale(X)

nlv &lt;- 12
res &lt;- dfplsr_cg(Xs, y, nlv = nlv)

df.kramer &lt;- c(1.000000, 3.712373, 6.456417, 11.633565, 12.156760, 11.715101, 12.349716,
  12.192682, 13.000000, 13.000000, 13.000000, 13.000000, 13.000000)

znlv &lt;- 0:nlv
plot(znlv, res$df, type = "l", col = "red",
     ylim = c(0, 15),
     xlab = "Nb components", ylab = "df")
lines(znlv, znlv + 1, col = "grey40")
points(znlv, df.kramer, pch = 16)
abline(h = 1, lty = 2, col = "grey")
legend("bottomright", legend=c("dfplsr_cg","Naive df","df.kramer"), col=c("red","grey40","black"),
lty=c(1,1,0), pch=c(NA,NA,16), bty="n")

## EXAMPLE 2

data(ozone)

z &lt;- ozone$X
u &lt;- which(!is.na(rowSums(z)))
X &lt;- z[u, -4]
y &lt;- z[u, 4]
dim(X)

Xs &lt;- scale(X)

nlv &lt;- 12
B &lt;- 50 
u &lt;- dfplsr_cov(Xs, y, nlv = nlv, B = B)
v &lt;- dfplsr_div(Xs, y, nlv = nlv, B = B)

df.kramer &lt;- c(1.000000, 3.712373, 6.456417, 11.633565, 12.156760, 11.715101, 12.349716,
  12.192682, 13.000000, 13.000000, 13.000000, 13.000000, 13.000000)

znlv &lt;- 0:nlv
plot(znlv, u$df, type = "l", col = "red",
     ylim = c(0, 15),
     xlab = "Nb components", ylab = "df")
lines(znlv, v$df, col = "blue")                 
lines(znlv, znlv + 1, col = "grey40")
points(znlv, df.kramer, pch = 16)
abline(h = 1, lty = 2, col = "grey")
legend("bottomright", legend=c("dfplsr_cov","dfplsr_div","Naive df","df.kramer"), 
col=c("blue","red","grey40","black"),
lty=c(1,1,1,0), pch=c(NA,NA,NA,16), bty="n")

</code></pre>

<hr>
<h2 id='dkplsr'>Direct KPLSR Models</h2><span id='topic+dkplsr'></span><span id='topic+transform.Dkpls'></span><span id='topic+coef.Dkpls'></span><span id='topic+predict.Dkplsr'></span>

<h3>Description</h3>

<p>Direct kernel PLSR (DKPLSR) (Bennett &amp; Embrechts 2003). The method builds kernel Gram matrices and then runs a usual PLSR algorithm on them. This is faster (but not equivalent) to the &quot;true&quot; NIPALS KPLSR algorithm such as described in Rosipal &amp; Trejo (2001). 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
dkplsr(X, Y, weights = NULL, nlv, kern = "krbf", ...)

## S3 method for class 'Dkpls'
transform(object, X, ..., nlv = NULL)  

## S3 method for class 'Dkpls'
coef(object, ..., nlv = NULL)  

## S3 method for class 'Dkplsr'
predict(object, X, ..., nlv = NULL)  

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dkplsr_+3A_x">X</code></td>
<td>
<p>For the main function:  Matrix with the training X-data (<code class="reqn">n, p</code>). &mdash; For auxiliary functions: A matrix with new X-data (<code class="reqn">m, p</code>) to consider.</p>
</td></tr>
<tr><td><code id="dkplsr_+3A_y">Y</code></td>
<td>
<p>Matrix with the training Y-data (<code class="reqn">n, q</code>).</p>
</td></tr>
<tr><td><code id="dkplsr_+3A_weights">weights</code></td>
<td>
<p>vector of weights (<code class="reqn">n, 1</code>) to apply to the training observations. Internally, weights are &quot;normalized&quot; to sum to 1. Default to <code>NULL</code> (weights are set to <code class="reqn">1 / n</code>).</p>
</td></tr>
<tr><td><code id="dkplsr_+3A_nlv">nlv</code></td>
<td>
<p>For the main function: The number(s) of LVs to calculate. &mdash; For auxiliary functions: The number(s) of LVs to consider.</p>
</td></tr>
<tr><td><code id="dkplsr_+3A_kern">kern</code></td>
<td>
<p>Name of the function defining the considered kernel for building the Gram matrix. See <code><a href="#topic+krbf">krbf</a></code> for syntax, and other available kernel functions (<code>krbf</code>, <code>kpol</code>, <code>ktanh</code>).</p>
</td></tr>
<tr><td><code id="dkplsr_+3A_...">...</code></td>
<td>
<p>Optional arguments to pass in the kernel function defined in <code>kern</code> (e.g. <code>gamma</code> for <code><a href="#topic+krbf">krbf</a></code>, <code>gamma</code> and <code>coef0</code> for <code>ktanh</code>, <code>gamma</code> and <code>coef0</code> and <code>degree</code> for <code>kpol</code>).</p>
</td></tr>
<tr><td><code id="dkplsr_+3A_object">object</code></td>
<td>
<p>For auxiliary functions: A fitted model, output of a call to the main function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>For <code>dkplsr</code>: 
</p>
<table>
<tr><td><code>X</code></td>
<td>
<p>Matrix with the training X-data (<code class="reqn">n, p</code>).</p>
</td></tr>
<tr><td><code>fm</code></td>
<td>
<p>List with the outputs of the PLSR ((<code>T</code>): the X-score matrix (n,nlv); (<code>P</code>): the X-loadings matrix (p,nlv); (<code>R</code>): The PLS projection matrix (p,nlv); (<code>W</code>): The X-loading weights matrix (p,nlv); (<code>C</code>): The Y-loading weights matrix; (<code>TT</code>): the X-score normalization factor; (<code>xmeans</code>): the centering vector of X (p,1); (<code>ymeans</code>): the centering vector of Y (q,1); (<code>weights</code>): the weights vector of X-variables (p,1); (<code>U</code>): intermediate output.</p>
</td></tr>
<tr><td><code>K</code></td>
<td>
<p>kernel Gram matrix</p>
</td></tr> 
<tr><td><code>kern</code></td>
<td>
<p>kernel function</p>
</td></tr> 
<tr><td><code>dots</code></td>
<td>
<p>Optional arguments passed in the kernel function</p>
</td></tr>
</table>
<p>For <code>transform.Dkplsr</code> : A matrix (<code class="reqn">m, nlv</code>) with the projection of the new X-data on the X-scores
</p>
<p>For <code>predict.Dkplsr</code>: 
</p>
<table>
<tr><td><code>pred</code></td>
<td>
<p>A list of matrices (<code class="reqn">m, q</code>) with the Y predicted values for the new X-data</p>
</td></tr>
<tr><td><code>K</code></td>
<td>
<p>kernel Gram matrix (<code class="reqn">m, nlv</code>), with values for the new X-data</p>
</td></tr>
</table>
<p>For <code>coef.Dkplsr</code>: 
</p>
<table>
<tr><td><code>int</code></td>
<td>
<p>matrix (1,nlv) with the intercepts</p>
</td></tr>
<tr><td><code>B</code></td>
<td>
<p>matrix (n,nlv) with the coefficients</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The second example concerns the fitting of the function sinc(x) described in Rosipal &amp; Trejo 2001 p. 105-106
</p>


<h3>References</h3>

<p>Bennett, K.P., Embrechts, M.J., 2003. An optimization perspective on kernel partial least squares regression, in: Advances in Learning Theory: Methods, Models and Applications, NATO Science Series III: Computer &amp; Systems Sciences. IOS Press Amsterdam, pp. 227-250.
</p>
<p>Rosipal, R., Trejo, L.J., 2001. Kernel Partial Least Squares Regression in Reproducing Kernel Hilbert Space. Journal of Machine Learning Research 2, 97-123.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## EXAMPLE 1

n &lt;- 6 ; p &lt;- 4
Xtrain &lt;- matrix(rnorm(n * p), ncol = p)
ytrain &lt;- rnorm(n)
Ytrain &lt;- cbind(y1 = ytrain, y2 = 100 * ytrain)
m &lt;- 3
Xtest &lt;- Xtrain[1:m, , drop = FALSE] 
Ytest &lt;- Ytrain[1:m, , drop = FALSE] ; ytest &lt;- Ytest[1:m, 1]

nlv &lt;- 2
fm &lt;- dkplsr(Xtrain, Ytrain, nlv = nlv, kern = "krbf", gamma = .8)
transform(fm, Xtest)
transform(fm, Xtest, nlv = 1)
coef(fm)
coef(fm, nlv = 1)

predict(fm, Xtest)
predict(fm, Xtest, nlv = 0:nlv)$pred

pred &lt;- predict(fm, Xtest)$pred
msep(pred, Ytest)

nlv &lt;- 2
fm &lt;- dkplsr(Xtrain, Ytrain, nlv = nlv, kern = "kpol", degree = 2, coef0 = 10)
predict(fm, Xtest, nlv = nlv)

## EXAMPLE 2

x &lt;- seq(-10, 10, by = .2)
x[x == 0] &lt;- 1e-5
n &lt;- length(x)
zy &lt;- sin(abs(x)) / abs(x)
y &lt;- zy + rnorm(n, 0, .2)
plot(x, y, type = "p")
lines(x, zy, lty = 2)
X &lt;- matrix(x, ncol = 1)

nlv &lt;- 3
fm &lt;- dkplsr(X, y, nlv = nlv)
pred &lt;- predict(fm, X)$pred
plot(X, y, type = "p")
lines(X, zy, lty = 2)
lines(X, pred, col = "red")

</code></pre>

<hr>
<h2 id='dkrr'>Direct KRR Models</h2><span id='topic+dkrr'></span><span id='topic+coef.Dkrr'></span><span id='topic+predict.Dkrr'></span>

<h3>Description</h3>

<p>Direct kernel ridge regression (DKRR), following the same approcah as for DKPLSR (Bennett &amp; Embrechts 2003). The method builds kernel Gram matrices and then runs a RR algorithm on them. This is not equivalent to the &quot;true&quot; KRR (= LS-SVM) algorithm. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
dkrr(X, Y, weights = NULL, lb = 1e-2, kern = "krbf", ...)

## S3 method for class 'Dkrr'
coef(object, ..., lb = NULL)  

## S3 method for class 'Dkrr'
predict(object, X, ..., lb = NULL)  

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dkrr_+3A_x">X</code></td>
<td>
<p>For the main function: Training X-data (<code class="reqn">n, p</code>). &mdash; For the auxiliary functions: New X-data (<code class="reqn">m, p</code>) to consider.</p>
</td></tr>
<tr><td><code id="dkrr_+3A_y">Y</code></td>
<td>
<p>Training Y-data (<code class="reqn">n, q</code>).</p>
</td></tr>
<tr><td><code id="dkrr_+3A_weights">weights</code></td>
<td>
<p>Weights (<code class="reqn">n, 1</code>) to apply to the training observations. Internally, weights are &quot;normalized&quot; to sum to 1. Default to <code>NULL</code> (weights are set to <code class="reqn">1 / n</code>).</p>
</td></tr>
<tr><td><code id="dkrr_+3A_lb">lb</code></td>
<td>
<p>A value of regularization parameter <code class="reqn">lambda</code>.</p>
</td></tr>
<tr><td><code id="dkrr_+3A_kern">kern</code></td>
<td>
<p>Name of the function defining the considered kernel for building the Gram matrix. See <code><a href="#topic+krbf">krbf</a></code> for syntax, and other available kernel functions.</p>
</td></tr>
<tr><td><code id="dkrr_+3A_...">...</code></td>
<td>
<p>Optional arguments to pass in the kernel function defined in <code>kern</code> (e.g. <code>gamma</code> for <code><a href="#topic+krbf">krbf</a></code>).</p>
</td></tr>
<tr><td><code id="dkrr_+3A_object">object</code></td>
<td>
<p>For the auxiliary functions: A fitted model, output of a call to the main function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>For <code>dkrr</code>: 
</p>
<table>
<tr><td><code>X</code></td>
<td>
<p>Matrix with the training X-data (<code class="reqn">n, p</code>).</p>
</td></tr>
<tr><td><code>fm</code></td>
<td>
<p>List with the outputs of the RR ((<code>V</code>): eigenvector matrix of the correlation matrix (n,n); (<code>TtDY</code>): intermediate output; (<code>sv</code>): singular values of the matrix (1,n); (<code>lb</code>): value of regularization parameter <code class="reqn">lambda</code>; (<code>xmeans</code>): the centering vector of X (p,1); (<code>ymeans</code>): the centering vector of Y (q,1); (<code>weights</code>): the weights vector of X-variables (p,1)</p>
</td></tr>
<tr><td><code>K</code></td>
<td>
<p>kernel Gram matrix</p>
</td></tr> 
<tr><td><code>kern</code></td>
<td>
<p>kernel function</p>
</td></tr> 
<tr><td><code>dots</code></td>
<td>
<p>Optional arguments passed in the kernel function</p>
</td></tr>
</table>
<p>For <code>predict.Dkrr</code>: 
</p>
<table>
<tr><td><code>pred</code></td>
<td>
<p>A list of matrices (<code class="reqn">m, q</code>) with the Y predicted values for the new X-data</p>
</td></tr>
<tr><td><code>K</code></td>
<td>
<p>kernel Gram matrix (<code class="reqn">m, nlv</code>), with values for the new X-data</p>
</td></tr>
</table>
<p>For <code>coef.Dkrr</code>: 
</p>
<table>
<tr><td><code>int</code></td>
<td>
<p>matrix (1,nlv) with the intercepts</p>
</td></tr>
<tr><td><code>B</code></td>
<td>
<p>matrix (n,nlv) with the coefficients</p>
</td></tr>
<tr><td><code>df</code></td>
<td>
<p>model complexity (number of degrees of freedom)</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The second example concerns the fitting of the function sinc(x) described in Rosipal &amp; Trejo 2001 p. 105-106
</p>


<h3>References</h3>

<p>Bennett, K.P., Embrechts, M.J., 2003. An optimization perspective on kernel partial least squares regression, in: Advances in Learning Theory: Methods, Models and Applications, NATO Science Series III: Computer &amp; Systems Sciences. IOS Press Amsterdam, pp. 227-250.
</p>
<p>Rosipal, R., Trejo, L.J., 2001. Kernel Partial Least Squares Regression in Reproducing Kernel Hilbert Space. Journal of Machine Learning Research 2, 97-123.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## EXAMPLE 1

n &lt;- 6 ; p &lt;- 4
Xtrain &lt;- matrix(rnorm(n * p), ncol = p)
ytrain &lt;- rnorm(n)
Ytrain &lt;- cbind(y1 = ytrain, y2 = 100 * ytrain)
m &lt;- 3
Xtest &lt;- Xtrain[1:m, , drop = FALSE] 
Ytest &lt;- Ytrain[1:m, , drop = FALSE] ; ytest &lt;- Ytest[1:m, 1]

lb &lt;- 2
fm &lt;- dkrr(Xtrain, Ytrain, lb = lb, kern = "krbf", gamma = .8)
coef(fm)
coef(fm, lb = .6)
predict(fm, Xtest)
predict(fm, Xtest, lb = c(0.1, .8))

pred &lt;- predict(fm, Xtest)$pred
msep(pred, Ytest)

lb &lt;- 2
fm &lt;- dkrr(Xtrain, Ytrain, lb = lb, kern = "kpol", degree = 2, coef0 = 10)
predict(fm, Xtest)

## EXAMPLE 1

x &lt;- seq(-10, 10, by = .2)
x[x == 0] &lt;- 1e-5
n &lt;- length(x)
zy &lt;- sin(abs(x)) / abs(x)
y &lt;- zy + rnorm(n, 0, .2)
plot(x, y, type = "p")
lines(x, zy, lty = 2)
X &lt;- matrix(x, ncol = 1)

fm &lt;- dkrr(X, y, lb = .01, gamma = .5)
pred &lt;- predict(fm, X)$pred
plot(X, y, type = "p")
lines(X, zy, lty = 2)
lines(X, pred, col = "red")

</code></pre>

<hr>
<h2 id='dmnorm'>Multivariate normal probability density</h2><span id='topic+dmnorm'></span><span id='topic+predict.Dmnorm'></span>

<h3>Description</h3>

<p>Prediction of the normal probability density of multivariate observations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
dmnorm(X = NULL, mu = NULL, sigma = NULL)

## S3 method for class 'Dmnorm'
predict(object, X, ...)
  
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dmnorm_+3A_x">X</code></td>
<td>
<p>For the main function: Training data (<code class="reqn">n, p</code>) used for estimating the mean and the covariance matrix population (if <code>mu</code> or/and <code>sigma</code> are not provided). &mdash; For the auxiliary functions: New data (<code class="reqn">m, p</code>) for which the density has to be predicted.</p>
</td></tr>
<tr><td><code id="dmnorm_+3A_mu">mu</code></td>
<td>
<p>The mean (<code class="reqn">p, 1</code>) of the normal distribution. If <code>NULL</code> (default), <code class="reqn">mu</code> is estimated by the column-wise mean of the training data.</p>
</td></tr>
<tr><td><code id="dmnorm_+3A_sigma">sigma</code></td>
<td>
<p>The covariance matrix (<code class="reqn">p x p</code>) of the normal distribution. If <code>NULL</code> (default), <code class="reqn">sigma</code> is estimated by the empirical covariance matrix (denominator <code class="reqn">n - 1</code>) of the training data.</p>
</td></tr>
<tr><td><code id="dmnorm_+3A_object">object</code></td>
<td>
<p>For the auxiliary functions: A result of a call to <code>dmnorm</code>.</p>
</td></tr>
<tr><td><code id="dmnorm_+3A_...">...</code></td>
<td>
<p>For the auxiliary functions: Optional arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>For <code>dmnorm</code>:
</p>
<table>
<tr><td><code>mu</code></td>
<td>
<p>means of the X variables</p>
</td></tr>
<tr><td><code>Uinv</code></td>
<td>
<p>inverse of the Cholesky decomposition of the covariance matrix</p>
</td></tr>
<tr><td><code>det</code></td>
<td>
<p>squared determinant of the Cholesky decomposition of the covariance matrix</p>
</td></tr>
</table>
<p>For <code>predict</code>: 
</p>
<table>
<tr><td><code>pred</code></td>
<td>
<p>Prediction of the normal probability density of new multivariate observations</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
data(iris)

X &lt;- iris[, 1:2]

Xtrain &lt;- X[1:40, ]
Xtest &lt;- X[40:50, ]

fm &lt;- dmnorm(Xtrain)
fm

k &lt;- 50
x1 &lt;- seq(min(Xtrain[, 1]), max(Xtrain[, 1]), length.out = k)
x2 &lt;- seq(min(Xtrain[, 2]), max(Xtrain[, 2]), length.out = k)
zX &lt;- expand.grid(x1, x2)
pred &lt;- predict(fm, zX)$pred
contour(x1, x2, matrix(pred, nrow = 50))

points(Xtest, col = "red", pch = 16)

</code></pre>

<hr>
<h2 id='dtagg'>Summary statistics of data subsets</h2><span id='topic+dtagg'></span>

<h3>Description</h3>

<p>Faster alternative to <code><a href="stats.html#topic+aggregate">aggregate</a></code> to calculate a summary statistic over data subsets.  <code>dtagg</code> uses function <code><a href="data.table.html#topic+data.table">data.table</a></code> of package <code>data.table</code>. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
dtagg(formula, data, FUN = mean, ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dtagg_+3A_formula">formula</code></td>
<td>
<p>A left and right-hand-sides formula defing the variable and the aggregation levels on which is calculated the statistic.</p>
</td></tr>
<tr><td><code id="dtagg_+3A_data">data</code></td>
<td>
<p>A dataframe.</p>
</td></tr>
<tr><td><code id="dtagg_+3A_fun">FUN</code></td>
<td>
<p>Function defining the statistic to compute (default to <code>mean</code>).</p>
</td></tr>
<tr><td><code id="dtagg_+3A_...">...</code></td>
<td>
<p>Eventual additional arguments to pass through <code>FUN</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A dataframe, with the values of the agregation level(s) and the corresponding computed statistic value.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
dat &lt;- data.frame(matrix(rnorm(2 * 100), ncol = 2))
names(dat) &lt;- c("y1", "y2")
dat$typ1 &lt;- sample(1:2, size = nrow(dat), TRUE)
dat$typ2 &lt;- sample(1:3, size = nrow(dat), TRUE)

headm(dat)

dtagg(y1 ~ 1, data = dat)

dtagg(y1 ~ typ1 + typ2, data = dat)

dtagg(y1 ~ typ1 + typ2, data = dat, trim = .2)

</code></pre>

<hr>
<h2 id='dummy'>Table of dummy variables</h2><span id='topic+dummy'></span>

<h3>Description</h3>

<p>The function builds a table of dummy variables from a qualitative variable. A binary (i.e. 0/1) variable is created for each level of the qualitative variable. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dummy(y)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dummy_+3A_y">y</code></td>
<td>
<p>A qualitative variable.</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>Y</code></td>
<td>
<p>A matrix of dummy variables (i.e. binary variables), each representing a given level of the qualitative variable.</p>
</td></tr>
<tr><td><code>lev</code></td>
<td>
<p>levels of the qualitative variable.</p>
</td></tr>
<tr><td><code>ni</code></td>
<td>
<p>number of observations per level of the qualitative variable.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
y &lt;- c(1, 1, 3, 2, 3)
dummy(y)

y &lt;- c("B", "a", "B")
dummy(y)
dummy(as.factor(y))

</code></pre>

<hr>
<h2 id='eposvd'>External parameter orthogonalization (EPO)</h2><span id='topic+eposvd'></span>

<h3>Description</h3>

<p>Pre-processing a X-dataset by external parameter orthogonalization (EPO; Roger et al 2003). The objective is to remove from a dataset X <code class="reqn">(n, p)</code> some &quot;detrimental&quot; information (e.g. humidity effect) represented by a dataset <code class="reqn">D (m, p)</code>. 
</p>
<p>EPO consists in orthogonalizing the row observations of <code class="reqn">X</code> to the detrimental sub-space defined by the first <code class="reqn">nlv</code> non-centered PCA loadings vectors of <code class="reqn">D</code>. 
</p>
<p>Function <code>eposvd</code> uses a SVD factorization of <code class="reqn">D</code> and returns <code class="reqn">M (p, p)</code> the orthogonalization matrix, and <code class="reqn">P</code> the considered loading vectors of <code class="reqn">D</code>. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>eposvd(D, nlv)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="eposvd_+3A_d">D</code></td>
<td>
<p>A dataset  <code class="reqn">(m, p)</code> containing detrimental information.</p>
</td></tr>
<tr><td><code id="eposvd_+3A_nlv">nlv</code></td>
<td>
<p>The number of first loadings vectors of <code class="reqn">D</code> considered for the orthogonalization.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The data corrected from the detrimental information <code class="reqn">D</code> can be computed by <code class="reqn">Xcorrected = X * M</code>.
Rows of the corrected matrix Xcorr are orthogonal to the loadings vectors (columns of P): <code class="reqn">Xcorr * P</code>.
</p>


<h3>Value</h3>

<table>
<tr><td><code>M</code></td>
<td>
<p>orthogonalization matrix.</p>
</td></tr>
<tr><td><code>P</code></td>
<td>
<p>detrimental directions matrix (p, nlv) (loadings of D = columns of P).</p>
</td></tr>
</table>


<h3>References</h3>

<p>Roger, J.-M., Chauchard, F., Bellon-Maurel, V., 2003. EPO-PLS external parameter orthogonalisation of PLS application to temperature-independent measurement of sugar content of intact fruits. Chemometrics and Intelligent Laboratory Systems 66, 191-204. https://doi.org/10.1016/S0169-7439(03)00051-0
</p>
<p>Roger, J.-M., Boulet, J.-C., 2018. A review of orthogonal projections for calibration. Journal of Chemometrics 32, e3045. https://doi.org/10.1002/cem.3045
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
n &lt;- 4 ; p &lt;- 8 
X &lt;- matrix(rnorm(n * p), ncol = p)
m &lt;- 3
D &lt;- matrix(rnorm(m * p), ncol = p)

nlv &lt;- 2
res &lt;- eposvd(D, nlv = nlv)
M &lt;- res$M
P &lt;- res$P
M
P

</code></pre>

<hr>
<h2 id='euclsq'>Matrix of distances</h2><span id='topic+euclsq'></span><span id='topic+euclsq_mu'></span><span id='topic+mahsq'></span><span id='topic+mahsq_mu'></span>

<h3>Description</h3>

<p>&mdash;&ndash; Matrix (<code class="reqn">n, m</code>) of distances between row observations of two datasets <code class="reqn">X</code> (<code class="reqn">n, p</code>) and Y (<code class="reqn">m, p</code>)
</p>
<p>- <code>euclsq</code>: Squared Euclidean distance
</p>
<p>- <code>mahsq</code>: Squared Mahalanobis distance
</p>
<p>&mdash;&ndash; Matrix (<code class="reqn">n, 1</code>) of distances between row observations of a dataset <code class="reqn">X</code> (<code class="reqn">n, p</code>) and a vector <code class="reqn">p</code> (<code class="reqn">n</code>)
</p>
<p>- <code>euclsq_mu</code>: Squared Euclidean distance
</p>
<p>- <code>mahsq_mu</code>: Squared Euclidean distance
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
euclsq(X, Y = NULL)

euclsq_mu(X, mu)

mahsq(X, Y = NULL, Uinv = NULL)

mahsq_mu(X, mu, Uinv = NULL)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="euclsq_+3A_x">X</code></td>
<td>
<p>X-data (<code class="reqn">n, p</code>). </p>
</td></tr>
<tr><td><code id="euclsq_+3A_y">Y</code></td>
<td>
<p>Data (<code class="reqn">m, p</code>) compared to <code>X</code>. If <code>NULL</code> (default), <code>Y</code> is set equal to <code>X</code>.</p>
</td></tr>
<tr><td><code id="euclsq_+3A_mu">mu</code></td>
<td>
<p>Vector (<code class="reqn">p</code>) compared to <code>X</code>.</p>
</td></tr>
<tr><td><code id="euclsq_+3A_uinv">Uinv</code></td>
<td>
<p>For Mahalanobis distance. The inverse of a Choleski factorization matrix of the covariance matrix of <code>X</code>. If <code>NULL</code> (default), <code>Uinv</code> is calculated internally.</p>
</td></tr> 
</table>


<h3>Value</h3>

<p>A distance matrix.</p>


<h3>Examples</h3>

<pre><code class='language-R'>
n &lt;- 5 ; p &lt;- 3
X &lt;- matrix(rnorm(n * p), ncol = p)

euclsq(X)
as.matrix(stats::dist(X)^2)
euclsq(X, X)

Y &lt;- X[c(1, 3), ]
euclsq(X, Y)
euclsq_mu(X, Y[2, ])

i &lt;- 3
euclsq(X, X[i, , drop = FALSE])
euclsq_mu(X, X[i, ])

S &lt;- cov(X) * (n - 1) / n
i &lt;- 3
mahsq(X)[i, , drop = FALSE]
stats::mahalanobis(X, X[i, ], S)

mahsq(X)
Y &lt;- X[c(1, 3), ]
mahsq(X, Y)

</code></pre>

<hr>
<h2 id='fda'>Factorial discriminant analysis</h2><span id='topic+fda'></span><span id='topic+fdasvd'></span><span id='topic+transform.Fda'></span><span id='topic+summary.Fda'></span>

<h3>Description</h3>

<p>Factorial discriminant analysis (FDA). The functions maximize the compromise <code class="reqn">p'Bp / p'Wp</code>, i.e. <code class="reqn">max p'Bp</code> with constraint <code class="reqn">p'Wp = 1</code>. Vectors <code class="reqn">p</code> are the linear discrimant coefficients &quot;LD&quot;.
</p>
<p>- <code>fda</code>: Eigen factorization of <code class="reqn">W^(-1)B</code>
</p>
<p>- <code>fdasvd</code>: Weighted SVD factorization of the matrix of the class centers.
</p>
<p>If <code class="reqn">W</code> is singular, W^(-1) is replaced by a MP pseudo-inverse.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
fda(X, y, nlv = NULL)

fdasvd(X, y, nlv = NULL)

## S3 method for class 'Fda'
transform(object, X, ..., nlv = NULL) 

## S3 method for class 'Fda'
summary(object, ...) 

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fda_+3A_x">X</code></td>
<td>
<p>For the main functions: Training X-data (<code class="reqn">n, p</code>).&mdash; For the auxiliary functions: New X-data (<code class="reqn">m, p</code>) to consider.</p>
</td></tr>
<tr><td><code id="fda_+3A_y">y</code></td>
<td>
<p>Training class membership (<code class="reqn">n</code>). <b>Note:</b> If <code>y</code> is a factor, it is replaced by a character vector.</p>
</td></tr>
<tr><td><code id="fda_+3A_nlv">nlv</code></td>
<td>
<p>For the main functions: The number(s) of LVs to calculate. &mdash; For the auxiliary functions: The number(s) of LVs to consider.</p>
</td></tr>
<tr><td><code id="fda_+3A_object">object</code></td>
<td>
<p>For the auxiliary functions: A fitted model, output of a call to the main function.</p>
</td></tr>
<tr><td><code id="fda_+3A_...">...</code></td>
<td>
<p>For the auxiliary functions: Optional arguments. Not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>For <code>fda</code> and <code>fdasvd</code>:
</p>
<table>
<tr><td><code>T</code></td>
<td>
<p>X-scores matrix (n,nlv).</p>
</td></tr>
<tr><td><code>P</code></td>
<td>
<p>X-loadings matrix (p,nlv) = coefficients of the linear discriminant function = &quot;LD&quot; of function lda of package MASS.</p>
</td></tr>
<tr><td><code>Tcenters</code></td>
<td>
<p>projection of the class centers in the score space.</p>
</td></tr>
<tr><td><code>eig</code></td>
<td>
<p>vector of eigen values</p>
</td></tr>
<tr><td><code>sstot</code></td>
<td>
<p>total variance</p>
</td></tr>
<tr><td><code>W</code></td>
<td>
<p>unbiased within covariance matrix</p>
</td></tr>
<tr><td><code>xmeans</code></td>
<td>
<p>means of the X variables</p>
</td></tr>
<tr><td><code>lev</code></td>
<td>
<p>y levels</p>
</td></tr>
<tr><td><code>ni</code></td>
<td>
<p>number of observations per level of the y variable</p>
</td></tr>
</table>
<p>For <code>transform.Fda</code>: scores of the new X-data in the model.
</p>
<p>For <code>summary.Fda</code>:
</p>
<table>
<tr><td><code>explvar</code></td>
<td>
<p>Explained variance by PCA of the class centers in transformed scale.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Saporta G., 2011. Probabilités analyse des données et statistique. Editions Technip, Paris, France.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(iris)

X &lt;- iris[, 1:4]
y &lt;- iris[, 5]
table(y)

fm &lt;- fda(X, y)
headm(fm$T)

transform(fm, X[1:3, ])

summary(fm)
plotxy(fm$T, group = y, ellipse = TRUE, 
    zeroes = TRUE, pch = 16, cex = 1.5, ncol = 2)
points(fm$Tcenters, pch = 8, col = "blue", cex = 1.5)

</code></pre>

<hr>
<h2 id='forages'>forages</h2><span id='topic+forages'></span>

<h3>Description</h3>

<p>A NIRS dataset (pre-processed absorbance) describing the class membership of forages. Spectra were recorded from 1100 to 2498 nm at 2 nm intervals.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(forages)</code></pre>


<h3>Format</h3>

<p>A list with 4 components: <code>Xtrain</code>, <code>ytrain</code>, <code>Xtest</code>, <code>ytest</code>.
</p>

<p>For the reference (calibration) data:
</p>
<dl>
<dt><code>Xtrain</code></dt><dd><p>A matrix whose rows are the pre-processed NIR absorbance spectra (= log10(1 / Reflectance)).</p>
</dd>
<dt><code>ytrain</code></dt><dd><p>A vector of the response variable (class membership).</p>
</dd>
</dl>
<p>For the test data:
</p>
<dl>
<dt><code>Xtest</code></dt><dd><p>A matrix whose rows are the pre-processed NIR absorbance spectra (= log10(1 / Reflectance)).</p>
</dd>
<dt><code>ytest</code></dt><dd><p>A vector of the response variable (class membership).</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>
data(forages)
str(forages)

</code></pre>

<hr>
<h2 id='getknn'>KNN selection</h2><span id='topic+getknn'></span>

<h3>Description</h3>

<p>Function <code>getknn</code> selects the <code class="reqn">k</code> nearest neighbours of each row observation of a new data set (= query) within a training data set, based on a dissimilarity measure. 
</p>
<p><code>getknn</code> uses function <code><a href="FNN.html#topic+get.knnx">get.knnx</a></code> of package <code>FNN</code> (Beygelzimer et al.) available on CRAN.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
getknn(Xtrain, X, k = NULL, diss = c("eucl", "mahal"), 
  algorithm = "brute", list = TRUE)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="getknn_+3A_xtrain">Xtrain</code></td>
<td>
<p>Training X-data (<code class="reqn">n, p</code>).</p>
</td></tr>
<tr><td><code id="getknn_+3A_x">X</code></td>
<td>
<p>New X-data (<code class="reqn">m, p</code>) to consider.</p>
</td></tr>
<tr><td><code id="getknn_+3A_k">k</code></td>
<td>
<p>The number of nearest neighbors to select in <code>Xtrain</code> for each observation of <code>X</code>.</p>
</td></tr>
<tr><td><code id="getknn_+3A_diss">diss</code></td>
<td>
<p>The type of dissimilarity used. Possible values are &quot;eucl&quot; (default; Euclidean distance) or &quot;mahal&quot; (Mahalanobis distance).</p>
</td></tr>
<tr><td><code id="getknn_+3A_algorithm">algorithm</code></td>
<td>
<p>Search algorithm used for Euclidean and Mahalanobis distances. Default to <code>"brute"</code>. See <code><a href="FNN.html#topic+get.knnx">get.knnx</a></code>.</p>
</td></tr>
<tr><td><code id="getknn_+3A_list">list</code></td>
<td>
<p>If <code>TRUE</code> (default), a list format is also returned for the outputs.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of outputs, such as:
</p>
<table>
<tr><td><code>nn</code></td>
<td>
<p>A dataframe (<code class="reqn">m x k</code>) with the indexes of the neighbors.</p>
</td></tr>
<tr><td><code>d</code></td>
<td>
<p>A dataframe (<code class="reqn">m x k</code>) with the dissimilarities between the neighbors and the new observations.
</p>
</td></tr>
<tr><td><code>listnn</code></td>
<td>
<p>Same as <code>$nn</code> but in a list format.</p>
</td></tr>
<tr><td><code>listd</code></td>
<td>
<p>Same as <code>$d</code> but in a list format.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
n &lt;- 10
p &lt;- 4
X &lt;- matrix(rnorm(n * p), ncol = p)
Xtrain &lt;- X
Xtest &lt;- X[c(1, 3), ]
m &lt;- nrow(Xtest)

k &lt;- 3
getknn(Xtrain, Xtest, k = k)

fm &lt;- pcasvd(Xtrain, nlv = 2)
Ttrain &lt;- fm$T
Ttest &lt;- transform(fm, Xtest)
getknn(Ttrain, Ttest, k = k, diss = "mahal")

</code></pre>

<hr>
<h2 id='gridcv'>Cross-validation</h2><span id='topic+gridcv'></span><span id='topic+gridcvlv'></span><span id='topic+gridcvlb'></span>

<h3>Description</h3>

<p>Functions for cross-validating predictive models.
</p>
<p>The functions return &quot;scores&quot; (average error rates) of predictions for a given model and a grid of parameter values, calculated from a cross-validation process. 
</p>
<p>- <code>gridcv</code>: Can be used for any model. 
</p>
<p>- <code>gridcvlv</code>: Specific to models using regularization by latent variables (LVs) (e.g. PLSR). Much faster than <code>gridcv</code>.
</p>
<p>- <code>gridcvlb</code>: Specific to models using ridge regularization (e.g. RR). Much faster than <code>gridcv</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
gridcv(X, Y, segm, score, fun, pars, verb = TRUE)

gridcvlv(X, Y, segm, score, fun, nlv, pars = NULL, verb = TRUE)

gridcvlb(X, Y, segm, score, fun, lb, pars = NULL, verb = TRUE)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gridcv_+3A_x">X</code></td>
<td>
<p>Training X-data (<code class="reqn">n, p</code>), or list of training X-data.</p>
</td></tr>
<tr><td><code id="gridcv_+3A_y">Y</code></td>
<td>
<p>Training Y-data (<code class="reqn">n, q</code>).</p>
</td></tr>
<tr><td><code id="gridcv_+3A_segm">segm</code></td>
<td>
<p>CV segments, typically output of <code><a href="#topic+segmkf">segmkf</a></code> or <code><a href="#topic+segmts">segmts</a></code>.</p>
</td></tr>
<tr><td><code id="gridcv_+3A_score">score</code></td>
<td>
<p>A function calculating a prediction score (e.g. <code><a href="#topic+msep">msep</a></code>).</p>
</td></tr>
<tr><td><code id="gridcv_+3A_fun">fun</code></td>
<td>
<p>A function corresponding to the predictive model.</p>
</td></tr>
<tr><td><code id="gridcv_+3A_nlv">nlv</code></td>
<td>
<p>For <code>gridcvlv</code>. A vector of numbers of LVs.</p>
</td></tr>
<tr><td><code id="gridcv_+3A_lb">lb</code></td>
<td>
<p>For <code>gridcvlb</code>. A vector of ridge regulariation parameters.</p>
</td></tr>
<tr><td><code id="gridcv_+3A_pars">pars</code></td>
<td>
<p>A list of named vectors.  Each vector must correspond to an argument of the model function and gives the parameter values to consider for this argument. (see details)</p>
</td></tr>
<tr><td><code id="gridcv_+3A_verb">verb</code></td>
<td>
<p>Logical. If <code>TRUE</code>, fitting information are printed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Argument <code>pars</code> (the grid) must be a list of named vectors, each vector corresponding to an argument of the model function and giving the parameter values to consider for this argument. This list can eventually be built with function <code><a href="#topic+mpars">mpars</a></code>, which returns all the combinations of the input parameters, see the examples. 
</p>
<p>For <code>gridcvlv</code>, <code>pars</code> must not contain <code>nlv</code> (nb. LVs), and for <code>gridcvlb</code>, <code>lb</code> (regularization parameter <code class="reqn">lambda</code>).
</p>


<h3>Value</h3>

<p>Dataframes with the prediction scores for the grid.</p>


<h3>Note</h3>

<p>Examples are given:
- with PLSR, using gridcv and gridcvlv (much faster)
- with PLSLDA, using gridcv and gridcvlv (much faster)
- with RR, using gridcv and gridcvlb (much faster) 
- with KRR, using gridcv and gridcvlb (much faster)  
- with LWPLSR, using gridcvlv 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## EXAMPLE WITH PLSR

n &lt;- 50 ; p &lt;- 8
X &lt;- matrix(rnorm(n * p), ncol = p)
y &lt;- rnorm(n)
Y &lt;- cbind(y, 10 * rnorm(n))

K = 3
segm &lt;- segmkf(n = n, K = K, nrep = 1)
segm

nlv &lt;- 5
pars &lt;- mpars(nlv = 1:nlv)
pars
gridcv(
    X, Y, segm,
    score = msep, 
    fun = plskern, 
    pars = pars, verb = TRUE)

gridcvlv(
    X, Y, segm, 
    score = msep, 
    fun = plskern, 
    nlv = 0:nlv, verb = TRUE)
    
## EXAMPLE WITH PLSLDA

n &lt;- 50 ; p &lt;- 8
X &lt;- matrix(rnorm(n * p), ncol = p, byrow = TRUE)
y &lt;- sample(c(1, 4, 10), size = n, replace = TRUE)

K = 3
segm &lt;- segmkf(n = n, K = K, nrep = 1)
segm

nlv &lt;- 5
pars &lt;- mpars(nlv = 1:nlv, prior = c("unif", "prop"))
pars
gridcv(
    X, y, segm, 
    score = err, 
    fun = plslda,
    pars = pars, verb = TRUE)

pars &lt;- mpars(prior = c("unif", "prop"))
pars
gridcvlv(
    X, y, segm, 
    score = err, 
    fun = plslda,
    nlv = 1:nlv, pars = pars, verb = TRUE)

## EXAMPLE WITH RR

n &lt;- 50 ; p &lt;- 8
X &lt;- matrix(rnorm(n * p), ncol = p)
y &lt;- rnorm(n)
Y &lt;- cbind(y, 10 * rnorm(n))

K = 3
segm &lt;- segmkf(n = n, K = K, nrep = 1)
segm

lb &lt;- c(.1, 1)
pars &lt;- mpars(lb = lb)
pars
gridcv(
    X, Y, segm, 
    score = msep, 
    fun = rr, 
    pars = pars, verb = TRUE)

gridcvlb(
    X, Y, segm, 
    score = msep, 
    fun = rr, 
    lb = lb, verb = TRUE)

## EXAMPLE WITH KRR

n &lt;- 50 ; p &lt;- 8
X &lt;- matrix(rnorm(n * p), ncol = p)
y &lt;- rnorm(n)
Y &lt;- cbind(y, 10 * rnorm(n))

K = 3
segm &lt;- segmkf(n = n, K = K, nrep = 1)
segm

lb &lt;- c(.1, 1)
gamma &lt;- 10^(-1:1)
pars &lt;- mpars(lb = lb, gamma = gamma)
pars
gridcv(
    X, Y, segm, 
    score = msep, 
    fun = krr, 
    pars = pars, verb = TRUE)

pars &lt;- mpars(gamma = gamma)
gridcvlb(
    X, Y, segm, 
    score = msep, 
    fun = krr, 
    lb = lb, pars = pars, verb = TRUE)

## EXAMPLE WITH LWPLSR

n &lt;- 50 ; p &lt;- 8
X &lt;- matrix(rnorm(n * p), ncol = p)
y &lt;- rnorm(n)
Y &lt;- cbind(y, 10 * rnorm(n))

K = 3
segm &lt;- segmkf(n = n, K = K, nrep = 1)
segm

nlvdis &lt;- 5
h &lt;- c(1, Inf)
k &lt;- c(10, 20)
nlv &lt;- 5
pars &lt;- mpars(nlvdis = nlvdis, diss = "mahal",
              h = h, k = k)
pars
res &lt;- gridcvlv(
    X, Y, segm, 
    score = msep, 
    fun = lwplsr, 
    nlv = 0:nlv, pars = pars, verb = TRUE)
res

</code></pre>

<hr>
<h2 id='gridscore'>Tuning of predictive models on a validation dataset</h2><span id='topic+gridscore'></span><span id='topic+gridscorelv'></span><span id='topic+gridscorelb'></span><span id='topic+mpars'></span>

<h3>Description</h3>

<p>Functions for tuning predictive models on a validation set.
</p>
<p>The functions return &quot;scores&quot; (average error rates) of predictions for a given model and a grid of parameter values, calculated on a validation dataset. 
</p>
<p>- <code>gridscore</code>: Can be used for any model.
</p>
<p>- <code>gridscorelv</code>: Specific to models using regularization by latent variables (LVs) (e.g. PLSR). Much faster than <code>gridscore</code>.
</p>
<p>- <code>gridscorelb</code>: Specific to models using ridge regularization (e.g. RR). Much faster than <code>gridscore</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
gridscore(Xtrain, Ytrain, X, Y, score, fun, pars, verb = FALSE)

gridscorelv(Xtrain, Ytrain, X, Y, score, fun, nlv, pars = NULL, verb = FALSE)

gridscorelb(Xtrain, Ytrain, X, Y, score, fun, lb, pars = NULL, verb = FALSE)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gridscore_+3A_xtrain">Xtrain</code></td>
<td>
<p>Training X-data (<code class="reqn">n, p</code>).</p>
</td></tr>
<tr><td><code id="gridscore_+3A_ytrain">Ytrain</code></td>
<td>
<p>Training Y-data (<code class="reqn">n, q</code>).</p>
</td></tr>
<tr><td><code id="gridscore_+3A_x">X</code></td>
<td>
<p>Validation X-data (<code class="reqn">n, p</code>).</p>
</td></tr>
<tr><td><code id="gridscore_+3A_y">Y</code></td>
<td>
<p>Validation Y-data (<code class="reqn">n, q</code>).</p>
</td></tr>
<tr><td><code id="gridscore_+3A_score">score</code></td>
<td>
<p>A function calculating a prediction score (e.g. <code><a href="#topic+msep">msep</a></code>).</p>
</td></tr>
<tr><td><code id="gridscore_+3A_fun">fun</code></td>
<td>
<p>A function corresponding to the predictive model.</p>
</td></tr>
<tr><td><code id="gridscore_+3A_nlv">nlv</code></td>
<td>
<p>For <code>gridscorelv</code>. A vector of numbers of LVs.</p>
</td></tr>
<tr><td><code id="gridscore_+3A_lb">lb</code></td>
<td>
<p>For <code>gridscorelb</code>. A vector of ridge regulariation parameters.</p>
</td></tr>
<tr><td><code id="gridscore_+3A_pars">pars</code></td>
<td>
<p>A list of named vectors.  Each vector must correspond to an argument of the model function and gives the parameter values to consider for this argument. (see details)</p>
</td></tr>
<tr><td><code id="gridscore_+3A_verb">verb</code></td>
<td>
<p>Logical. If <code>TRUE</code>, fitting information are printed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Argument <code>pars</code> (the grid) must be a list of named vectors, each vector corresponding to an argument of the model function and giving the parameter values to consider for this argument. This list can eventually be built with function <code>mpars</code>, which returns all the combinations of the input parameters, see the examples. 
</p>
<p>For <code>gridscorelv</code>, <code>pars</code> must not contain <code>nlv</code> (nb. LVs), and for <code>gridscorelb</code>, <code>lb</code> (regularization parameter <code class="reqn">lambda</code>).
</p>


<h3>Value</h3>

<p>A dataframe with the prediction scores for the grid.</p>


<h3>Note</h3>

<p>Examples are given:
- with PLSR, using gridscore and gridscorelv (much faster)
- with PLSLDA, using gridscore and gridscorelv (much faster)
- with RR, using gridscore and gridscorelb (much faster)
- with KRR, using gridscore and gridscorelb (much faster) 
- with LWPLSR, using gridscorelv 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## EXAMPLE WITH PLSR

n &lt;- 50 ; p &lt;- 8
Xtrain &lt;- matrix(rnorm(n * p), ncol = p, byrow = TRUE)
ytrain &lt;- rnorm(n)
Ytrain &lt;- cbind(ytrain, 10 * rnorm(n))
m &lt;- 3
Xtest &lt;- Xtrain[1:m, ] 
Ytest &lt;- Ytrain[1:m, ] ; ytest &lt;- Ytest[, 1]

nlv &lt;- 5
pars &lt;- mpars(nlv = 1:nlv)
pars
gridscore(
    Xtrain, Ytrain, Xtest, Ytest, 
    score = msep, 
    fun = plskern, 
    pars = pars, verb = TRUE
    )

gridscorelv(
    Xtrain, Ytrain, Xtest, Ytest, 
    score = msep, 
    fun = plskern, 
    nlv = 0:nlv, verb = TRUE
    )

fm &lt;- plskern(Xtrain, Ytrain, nlv = nlv)
pred &lt;- predict(fm, Xtest)$pred
msep(pred, Ytest)

## EXAMPLE WITH PLSLDA

n &lt;- 50 ; p &lt;- 8
X &lt;- matrix(rnorm(n * p), ncol = p, byrow = TRUE)
y &lt;- sample(c(1, 4, 10), size = n, replace = TRUE)
Xtrain &lt;- X ; ytrain &lt;- y
m &lt;- 5
Xtest &lt;- X[1:m, ] ; ytest &lt;- y[1:m]

nlv &lt;- 5
pars &lt;- mpars(nlv = 1:nlv, prior = c("unif", "prop"))
pars
gridscore(
    Xtrain, ytrain, Xtest, ytest, 
    score = err, 
    fun = plslda,
    pars = pars, verb = TRUE
    )

fm &lt;- plslda(Xtrain, ytrain, nlv = nlv)
pred &lt;- predict(fm, Xtest)$pred
err(pred, ytest)

pars &lt;- mpars(prior = c("unif", "prop"))
pars
gridscorelv(
    Xtrain, ytrain, Xtest, ytest, 
    score = err, 
    fun = plslda,
    nlv = 1:nlv, pars = pars, verb = TRUE
    )
    
## EXAMPLE WITH RR

n &lt;- 50 ; p &lt;- 8
Xtrain &lt;- matrix(rnorm(n * p), ncol = p, byrow = TRUE)
ytrain &lt;- rnorm(n)
Ytrain &lt;- cbind(ytrain, 10 * rnorm(n))
m &lt;- 3
Xtest &lt;- Xtrain[1:m, ] 
Ytest &lt;- Ytrain[1:m, ] ; ytest &lt;- Ytest[, 1]

lb &lt;- c(.1, 1)
pars &lt;- mpars(lb = lb)
pars
gridscore(
    Xtrain, Ytrain, Xtest, Ytest, 
    score = msep, 
    fun = rr, 
    pars = pars, verb = TRUE
    )

gridscorelb(
    Xtrain, Ytrain, Xtest, Ytest, 
    score = msep, 
    fun = rr, 
    lb = lb, verb = TRUE
    )

## EXAMPLE WITH KRR

n &lt;- 50 ; p &lt;- 8
Xtrain &lt;- matrix(rnorm(n * p), ncol = p, byrow = TRUE)
ytrain &lt;- rnorm(n)
Ytrain &lt;- cbind(ytrain, 10 * rnorm(n))
m &lt;- 3
Xtest &lt;- Xtrain[1:m, ] 
Ytest &lt;- Ytrain[1:m, ] ; ytest &lt;- Ytest[, 1]

lb &lt;- c(.1, 1)
gamma &lt;- 10^(-1:1)
pars &lt;- mpars(lb = lb, gamma = gamma)
pars
gridscore(
    Xtrain, Ytrain, Xtest, Ytest, 
    score = msep, 
    fun = krr, 
    pars = pars, verb = TRUE
    )

pars &lt;- mpars(gamma = gamma)
gridscorelb(
    Xtrain, Ytrain, Xtest, Ytest, 
    score = msep, 
    fun = krr, 
    lb = lb, pars = pars, verb = TRUE
    )
    
## EXAMPLE WITH LWPLSR

n &lt;- 50 ; p &lt;- 8
Xtrain &lt;- matrix(rnorm(n * p), ncol = p, byrow = TRUE)
ytrain &lt;- rnorm(n)
Ytrain &lt;- cbind(ytrain, 10 * rnorm(n))
m &lt;- 3
Xtest &lt;- Xtrain[1:m, ] 
Ytest &lt;- Ytrain[1:m, ] ; ytest &lt;- Ytest[, 1]

nlvdis &lt;- 5
h &lt;- c(1, Inf)
k &lt;- c(10, 20)
nlv &lt;- 5
pars &lt;- mpars(nlvdis = nlvdis, diss = "mahal",
    h = h, k = k)
pars
res &lt;- gridscorelv(
    Xtrain, Ytrain, Xtest, Ytest, 
    score = msep, 
    fun = lwplsr, 
    nlv = 0:nlv, pars = pars, verb = TRUE
    )
res

</code></pre>

<hr>
<h2 id='headm'>Display of the first part of a data set</h2><span id='topic+headm'></span>

<h3>Description</h3>

<p>Function <code>headm</code> displays the first part and the dimension of a data set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
headm(X)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="headm_+3A_x">X</code></td>
<td>
<p>A matrix or dataframe.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>first 6 rows and columns of a dataset, number of rows, number of columns, dataset class.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
n &lt;- 1000
p &lt;- 200
X &lt;- matrix(rnorm(n * p), nrow = n)

headm(X)

</code></pre>

<hr>
<h2 id='interpl'>Resampling of spectra by interpolation methods</h2><span id='topic+interpl'></span>

<h3>Description</h3>

<p>Resampling of signals by interpolation methods, including linear, spline, and cubic interpolation. The function uses <code><a href="signal.html#topic+interp1">interp1</a></code> of package <code>signal</code> available on the CRAN.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
interpl(X, w, meth = "cubic", ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="interpl_+3A_x">X</code></td>
<td>
<p>X-data (<code class="reqn">n x p</code>). For the interpolation, the column names of <code>X</code> are taken as numeric values, <code class="reqn">w0</code>. If they are not numeric or missing, they are automatically set to <code>w0 = 1:p</code>.</p>
</td></tr>
<tr><td><code id="interpl_+3A_w">w</code></td>
<td>
<p>A vector of the values where to interpolate (typically within the range of <code class="reqn">w0</code>).</p>
</td></tr>
<tr><td><code id="interpl_+3A_meth">meth</code></td>
<td>
<p>The method of interpolation. See <code><a href="signal.html#topic+interp1">interp1</a></code>.</p>
</td></tr>
<tr><td><code id="interpl_+3A_...">...</code></td>
<td>
<p>Optional arguments to pass in function <code><a href="stats.html#topic+splinefun">splinefun</a></code> if <code>meth = "spline"</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix of the interpolated signals.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(cassav)

X &lt;- cassav$Xtest
headm(X)

w &lt;- seq(500, 2400, length = 10)
zX &lt;- interpl(X, w, meth = "spline")
headm(zX)
plotsp(zX)

</code></pre>

<hr>
<h2 id='knnda'>KNN-DA</h2><span id='topic+knnda'></span><span id='topic+predict.Knnda'></span>

<h3>Description</h3>

<p>KNN weighted discrimination. For each new observation to predict, a number of <code class="reqn">k</code> nearest neighbors is selected and the prediction is calculated by the most frequent class in <code class="reqn">y</code> in this neighborhood.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
knnda(X, y,
    nlvdis, diss = c("eucl", "mahal"),
    h, k)

## S3 method for class 'Knnda'
predict(object, X, ...)  

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="knnda_+3A_x">X</code></td>
<td>
<p>For the main function: Training X-data (<code class="reqn">n, p</code>). &mdash; For the auxiliary functions: New X-data (<code class="reqn">m, p</code>) to consider.</p>
</td></tr>
<tr><td><code id="knnda_+3A_y">y</code></td>
<td>
<p>Training class membership (<code class="reqn">n</code>). <b>Note:</b> If <code>y</code> is a factor, it is replaced by a character vector.</p>
</td></tr>
<tr><td><code id="knnda_+3A_nlvdis">nlvdis</code></td>
<td>
<p>The number of LVs to consider in the global PLS used for the dimension reduction before calculating the dissimilarities. If <code>nlvdis = 0</code>, there is no dimension reduction. (see details)</p>
</td></tr>
<tr><td><code id="knnda_+3A_diss">diss</code></td>
<td>
<p>The type of dissimilarity used for defining the neighbors. Possible values are &quot;eucl&quot; (default; Euclidean distance), &quot;mahal&quot; (Mahalanobis distance), or &quot;correlation&quot;. Correlation dissimilarities are calculated by sqrt(.5 * (1 - rho)).</p>
</td></tr>
<tr><td><code id="knnda_+3A_h">h</code></td>
<td>
<p>A scale scalar defining the shape of the weight function. Lower is <code class="reqn">h</code>, sharper is the function. See <code><a href="#topic+wdist">wdist</a></code>.</p>
</td></tr>
<tr><td><code id="knnda_+3A_k">k</code></td>
<td>
<p>The number of nearest neighbors to select for each observation to predict.</p>
</td></tr>
<tr><td><code id="knnda_+3A_object">object</code></td>
<td>
<p>For the auxiliary functions: A fitted model, output of a call to the main function.</p>
</td></tr>
<tr><td><code id="knnda_+3A_...">...</code></td>
<td>
<p>For the auxiliary functions: Optional arguments. Not used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In function <code>knnda</code>, the dissimilarities used for computing the neighborhood and the weights can be calculated from the original X-data or after a dimension reduction (argument <code>nlvdis</code>). In the last case, global PLS scores are computed from <code class="reqn">(X, Y)</code> and the dissimilarities are calculated on these scores. For high dimension X-data, the dimension reduction is in general required for using the Mahalanobis distance.   
</p>


<h3>Value</h3>

<p>For <code>knnda</code>:list with input arguments.
</p>
<p>For <code>predict.Knnda</code>:
</p>
<table>
<tr><td><code>pred</code></td>
<td>
<p>prediction calculated for each observation by the most frequent class in <code class="reqn">y</code> in its neighborhood.</p>
</td></tr>
<tr><td><code>listnn</code></td>
<td>
<p>list with the neighbors used for each observation to be predicted</p>
</td></tr>
<tr><td><code>listd</code></td>
<td>
<p>list with the distances to the neighbors used for each observation to be predicted</p>
</td></tr>
<tr><td><code>listw</code></td>
<td>
<p>list with the weights attributed to the neighbors used for each observation to be predicted</p>
</td></tr>
</table>


<h3>References</h3>

<p>Venables, W. N. and Ripley, B. D. (2002) Modern Applied Statistics with S. Fourth edition. Springer.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
n &lt;- 50 ; p &lt;- 8
Xtrain &lt;- matrix(rnorm(n * p), ncol = p)
ytrain &lt;- sample(c(1, 4, 10), size = n, replace = TRUE)

m &lt;- 5
Xtest &lt;- Xtrain[1:m, ] ; ytest &lt;- ytrain[1:m]

nlvdis &lt;- 5 ; diss &lt;- "mahal"
h &lt;- 2 ; k &lt;- 10
fm &lt;- knnda(
    Xtrain, ytrain, 
    nlvdis = nlvdis, diss = diss,
    h = h, k = k
    )
res &lt;- predict(fm, Xtest)
names(res)
res$pred
err(res$pred, ytest)

</code></pre>

<hr>
<h2 id='knnr'>KNN-R</h2><span id='topic+knnr'></span><span id='topic+predict.Knnr'></span>

<h3>Description</h3>

<p>KNN weighted regression. For each new observation to predict, a number of <code class="reqn">k</code> nearest neighbors is selected and the prediction is calculated by the average (eventually weighted) of the response <code class="reqn">Y</code> over this neighborhood.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
knnr(X, Y,
    nlvdis, diss = c("eucl", "mahal"),
    h, k)

## S3 method for class 'Knnr'
predict(object, X, ...)  

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="knnr_+3A_x">X</code></td>
<td>
<p>For the main function: Training X-data (<code class="reqn">n, p</code>). &mdash; For the auxiliary functions: New X-data (<code class="reqn">m, p</code>) to consider.</p>
</td></tr>
<tr><td><code id="knnr_+3A_y">Y</code></td>
<td>
<p>Training Y-data (<code class="reqn">n, q</code>).</p>
</td></tr>
<tr><td><code id="knnr_+3A_nlvdis">nlvdis</code></td>
<td>
<p>The number of LVs to consider in the global PLS used for the dimension reduction before calculating the dissimilarities. If <code>nlvdis = 0</code>, there is no dimension reduction. (see details)</p>
</td></tr>
<tr><td><code id="knnr_+3A_diss">diss</code></td>
<td>
<p>The type of dissimilarity used for defining the neighbors. Possible values are &quot;eucl&quot; (default; Euclidean distance), &quot;mahal&quot; (Mahalanobis distance), or &quot;correlation&quot;. Correlation dissimilarities are calculated by sqrt(.5 * (1 - rho)).</p>
</td></tr>
<tr><td><code id="knnr_+3A_h">h</code></td>
<td>
<p>A scale scalar defining the shape of the weight function. Lower is <code class="reqn">h</code>, sharper is the function. See <code><a href="#topic+wdist">wdist</a></code>.</p>
</td></tr>
<tr><td><code id="knnr_+3A_k">k</code></td>
<td>
<p>The number of nearest neighbors to select for each observation to predict.</p>
</td></tr>
<tr><td><code id="knnr_+3A_object">object</code></td>
<td>
<p>&mdash; For the auxiliary functions: A fitted model, output of a call to the main function.</p>
</td></tr>
<tr><td><code id="knnr_+3A_...">...</code></td>
<td>
<p>&mdash; For the auxiliary functions: Optional arguments. Not used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In function <code>knnr</code>, the dissimilarities used for computing the neighborhood and the weights can be calculated from the original X-data or after a dimension reduction (argument <code>nlvdis</code>). In the last case, global PLS scores are computed from <code class="reqn">(X, Y)</code> and the dissimilarities are calculated on these scores. For high dimension X-data, the dimension reduction is in general required for using the Mahalanobis distance.   
</p>


<h3>Value</h3>

<p>For <code>knnr</code>:list with input arguments.
</p>
<p>For <code>predict.Knnr</code>:
</p>
<table>
<tr><td><code>pred</code></td>
<td>
<p>prediction calculated for each observation by the average (eventually weighted) of the response <code class="reqn">Y</code> over its neighborhood.</p>
</td></tr>
<tr><td><code>listnn</code></td>
<td>
<p>list with the neighbors used for each observation to be predicted</p>
</td></tr>
<tr><td><code>listd</code></td>
<td>
<p>list with the distances to the neighbors used for each observation to be predicted</p>
</td></tr>
<tr><td><code>listw</code></td>
<td>
<p>list with the weights attributed to the neighbors used for each observation to be predicted</p>
</td></tr>
</table>


<h3>References</h3>

<p>Venables, W. N. and Ripley, B. D. (2002) Modern Applied Statistics with S. Fourth edition. Springer.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
n &lt;- 30 ; p &lt;- 10
Xtrain &lt;- matrix(rnorm(n * p), ncol = p)
ytrain &lt;- rnorm(n)
Ytrain &lt;- cbind(ytrain, 100 * ytrain)
m &lt;- 4
Xtest &lt;- matrix(rnorm(m * p), ncol = p)
ytest &lt;- rnorm(m)
Ytest &lt;- cbind(ytest, 10 * ytest)

nlvdis &lt;- 5 ; diss &lt;- "mahal"
h &lt;- 2 ; k &lt;- 10
fm &lt;- knnr(
    Xtrain, Ytrain, 
    nlvdis = nlvdis, diss = diss,
    h = h, k = k)
res &lt;- predict(fm, Xtest)
names(res)
res$pred
msep(res$pred, Ytest)

</code></pre>

<hr>
<h2 id='kpca'>KPCA</h2><span id='topic+kpca'></span><span id='topic+transform.Kpca'></span><span id='topic+summary.Kpca'></span>

<h3>Description</h3>

<p>Kernel PCA (Scholkopf et al. 1997, Scholkopf &amp; Smola 2002, Tipping 2001) by SVD factorization of the weighted Gram matrix <code class="reqn">D^(1/2) * Phi(X) * Phi(X)' * D^(1/2)</code>. <code class="reqn">D</code> is a (<code class="reqn">n, n</code>) diagonal matrix of weights for the observations (rows of <code class="reqn">X</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
kpca(X, weights = NULL, nlv, kern = "krbf", ...)

## S3 method for class 'Kpca'
transform(object, X, ..., nlv = NULL)  

## S3 method for class 'Kpca'
summary(object, ...)  

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kpca_+3A_x">X</code></td>
<td>
<p>For the main function: Training X-data (<code class="reqn">n, p</code>). &mdash; For the auxiliary functions: New X-data (<code class="reqn">m, p</code>) to consider.</p>
</td></tr>
<tr><td><code id="kpca_+3A_weights">weights</code></td>
<td>
<p>Weights (<code class="reqn">n, 1</code>) to apply to the training observations. Internally, weights are &quot;normalized&quot; to sum to 1. Default to <code>NULL</code> (weights are set to <code class="reqn">1 / n</code>).</p>
</td></tr>
<tr><td><code id="kpca_+3A_nlv">nlv</code></td>
<td>
<p>The number of PCs to calculate.</p>
</td></tr>
<tr><td><code id="kpca_+3A_kern">kern</code></td>
<td>
<p>Name of the function defining the considered kernel for building the Gram matrix. See <code><a href="#topic+krbf">krbf</a></code> for syntax, and other available kernel functions.</p>
</td></tr>
<tr><td><code id="kpca_+3A_...">...</code></td>
<td>
<p>Optional arguments to pass in the kernel function defined in <code>kern</code> (e.g. <code>gamma</code> for <code><a href="#topic+krbf">krbf</a></code>).</p>
</td></tr>
<tr><td><code id="kpca_+3A_object">object</code></td>
<td>
<p>&mdash; For the auxiliary functions: A fitted model, output of a call to the main functions.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>For <code>kpca</code>:
</p>
<table>
<tr><td><code>X</code></td>
<td>
<p>Training X-data (<code class="reqn">n, p</code>).</p>
</td></tr>
<tr><td><code>Kt</code></td>
<td>
<p>Gram matrix</p>
</td></tr>
<tr><td><code>T</code></td>
<td>
<p>X-scores matrix.</p>
</td></tr>
<tr><td><code>P</code></td>
<td>
<p>X-loadings matrix.</p>
</td></tr>
<tr><td><code>sv</code></td>
<td>
<p>vector of singular values</p>
</td></tr>
<tr><td><code>eig</code></td>
<td>
<p>vector of eigenvalues.</p>
</td></tr>
<tr><td><code>weights</code></td>
<td>
<p>vector of observation weights.</p>
</td></tr>
<tr><td><code>kern</code></td>
<td>
<p>kern function.</p>
</td></tr>
<tr><td><code>dots</code></td>
<td>
<p>Optional arguments.</p>
</td></tr>
</table>
<p>For <code>transform.Kpca</code>: X-scores matrix for new X-data.
</p>
<p>For <code>summary.Kpca</code>: 
</p>
<table>
<tr><td><code>explvar</code></td>
<td>
<p>explained variance matrix.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Scholkopf, B., Smola, A., Muller, K.-R., 1997. Kernel principal component analysis, in: Gerstner, W., Germond, A., Hasler, M., Nicoud, J.-D. (Eds.), Artificial Neural Networks - ICANN 97, Lecture Notes in Computer Science. Springer, Berlin, Heidelberg, pp. 583-588. https://doi.org/10.1007/BFb0020217
</p>
<p>Scholkopf, B., Smola, A.J., 2002. Learning with kernels: support vector machines, regularization, optimization, and beyond, Adaptive computation and machine learning. MIT Press, Cambridge, Mass.
</p>
<p>Tipping, M.E., 2001. Sparse kernel principal component analysis. Advances in neural information processing systems, MIT Press. http://papers.nips.cc/paper/1791-sparse-kernel-principal-component-analysis.pdf
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## EXAMPLE 1

n &lt;- 5 ; p &lt;- 4
X &lt;- matrix(rnorm(n * p), ncol = p)

nlv &lt;- 3
kpca(X, nlv = nlv, kern = "krbf")

fm &lt;- kpca(X, nlv = nlv, kern = "krbf", gamma = .6)
fm$T
transform(fm, X[1:2, ])
transform(fm, X[1:2, ], nlv = 1)
summary(fm)

## EXAMPLE 2

n &lt;- 5 ; p &lt;- 4
X &lt;- matrix(rnorm(n * p), ncol = p)
nlv &lt;- 3
pcasvd(X, nlv = nlv)$T
kpca(X, nlv = nlv, kern = "kpol")$T

</code></pre>

<hr>
<h2 id='kplsr'>KPLSR Models</h2><span id='topic+kplsr'></span><span id='topic+transform.Kplsr'></span><span id='topic+coef.Kplsr'></span><span id='topic+predict.Kplsr'></span>

<h3>Description</h3>

<p>NIPALS Kernel PLSR algorithm described in Rosipal &amp; Trejo (2001). 
</p>
<p>The algorithm is slow for <code class="reqn">n &gt;= 500</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
kplsr(X, Y, weights = NULL, nlv, kern = "krbf",
     tol = .Machine$double.eps^0.5, maxit = 100, ...)

## S3 method for class 'Kplsr'
transform(object, X, ..., nlv = NULL)  

## S3 method for class 'Kplsr'
coef(object, ..., nlv = NULL)  

## S3 method for class 'Kplsr'
predict(object, X, ..., nlv = NULL)  

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kplsr_+3A_x">X</code></td>
<td>
<p>For the main function: Training X-data (<code class="reqn">n, p</code>). &mdash; For the auxiliary functions: New X-data (<code class="reqn">m, p</code>) to consider.</p>
</td></tr>
<tr><td><code id="kplsr_+3A_y">Y</code></td>
<td>
<p>Training Y-data (<code class="reqn">n, q</code>).</p>
</td></tr>
<tr><td><code id="kplsr_+3A_weights">weights</code></td>
<td>
<p>Weights (<code class="reqn">n, 1</code>) to apply to the training observations. Internally, weights are &quot;normalized&quot; to sum to 1. Default to <code>NULL</code> (weights are set to <code class="reqn">1 / n</code>).</p>
</td></tr>
<tr><td><code id="kplsr_+3A_nlv">nlv</code></td>
<td>
<p>The number(s) of LVs to calculate. &mdash; For the auxiliary functions: The number(s) of LVs to consider.</p>
</td></tr>
<tr><td><code id="kplsr_+3A_kern">kern</code></td>
<td>
<p>Name of the function defining the considered kernel for building the Gram matrix. See <code><a href="#topic+krbf">krbf</a></code> for syntax, and other available kernel functions.</p>
</td></tr>
<tr><td><code id="kplsr_+3A_tol">tol</code></td>
<td>
<p>Tolerance level for stopping the NIPALS iterations.</p>
</td></tr>
<tr><td><code id="kplsr_+3A_maxit">maxit</code></td>
<td>
<p>Maximum number of NIPALS iterations.</p>
</td></tr>
<tr><td><code id="kplsr_+3A_...">...</code></td>
<td>
<p>Optional arguments to pass in the kernel function defined in <code>kern</code> (e.g. <code>gamma</code> for <code><a href="#topic+krbf">krbf</a></code>).</p>
</td></tr>
<tr><td><code id="kplsr_+3A_object">object</code></td>
<td>
<p>For the auxiliary functions: A fitted model, output of a call to the main function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>For <code>kplsr</code>:
</p>
<table>
<tr><td><code>X</code></td>
<td>
<p>Training X-data (<code class="reqn">n, p</code>).</p>
</td></tr>
<tr><td><code>Kt</code></td>
<td>
<p>Gram matrix</p>
</td></tr>
<tr><td><code>T</code></td>
<td>
<p>X-scores matrix.</p>
</td></tr>
<tr><td><code>C</code></td>
<td>
<p>The Y-loading weights matrix.</p>
</td></tr>
<tr><td><code>U</code></td>
<td>
<p>intermediate output.</p>
</td></tr>
<tr><td><code>R</code></td>
<td>
<p>The PLS projection matrix (p,nlv).</p>
</td></tr>
<tr><td><code>ymeans</code></td>
<td>
<p>the centering vector of Y (q,1).</p>
</td></tr>
<tr><td><code>weights</code></td>
<td>
<p>vector of observation weights.</p>
</td></tr>
<tr><td><code>kern</code></td>
<td>
<p>kern function.</p>
</td></tr>
<tr><td><code>dots</code></td>
<td>
<p>Optional arguments.</p>
</td></tr>
</table>
<p>For <code>transform.Kplsr</code>: X-scores matrix for new X-data.
</p>
<p>For <code>coef.Kplsr</code>:
</p>
<table>
<tr><td><code>int</code></td>
<td>
<p>intercept values matrix.</p>
</td></tr>
<tr><td><code>beta</code></td>
<td>
<p>beta coefficient matrix.</p>
</td></tr>
</table>
<p>For <code>predict.Kplsr</code>: 
</p>
<table>
<tr><td><code>pred</code></td>
<td>
<p>predicted values matrix for new X-data.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The second example concerns the fitting of the function sinc(x) described in Rosipal &amp; Trejo 2001 p. 105-106
</p>


<h3>References</h3>

<p>Rosipal, R., Trejo, L.J., 2001. Kernel Partial Least Squares Regression in Reproducing Kernel Hilbert Space. Journal of Machine Learning Research 2, 97-123.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## EXAMPLE 1

n &lt;- 6 ; p &lt;- 4
Xtrain &lt;- matrix(rnorm(n * p), ncol = p)
ytrain &lt;- rnorm(n)
Ytrain &lt;- cbind(y1 = ytrain, y2 = 100 * ytrain)
m &lt;- 3
Xtest &lt;- Xtrain[1:m, , drop = FALSE] 
Ytest &lt;- Ytrain[1:m, , drop = FALSE] ; ytest &lt;- Ytest[1:m, 1]

nlv &lt;- 2
fm &lt;- kplsr(Xtrain, Ytrain, nlv = nlv, kern = "krbf", gamma = .8)
transform(fm, Xtest)
transform(fm, Xtest, nlv = 1)
coef(fm)
coef(fm, nlv = 1)

predict(fm, Xtest)
predict(fm, Xtest, nlv = 0:nlv)$pred

pred &lt;- predict(fm, Xtest)$pred
msep(pred, Ytest)

nlv &lt;- 2
fm &lt;- kplsr(Xtrain, Ytrain, nlv = nlv, kern = "kpol", degree = 2, coef0 = 10)
predict(fm, Xtest, nlv = nlv)

## EXAMPLE 2

x &lt;- seq(-10, 10, by = .2)
x[x == 0] &lt;- 1e-5
n &lt;- length(x)
zy &lt;- sin(abs(x)) / abs(x)
y &lt;- zy + rnorm(n, 0, .2)
plot(x, y, type = "p")
lines(x, zy, lty = 2)
X &lt;- matrix(x, ncol = 1)

nlv &lt;- 2
fm &lt;- kplsr(X, y, nlv = nlv)
pred &lt;- predict(fm, X)$pred
plot(X, y, type = "p")
lines(X, zy, lty = 2)
lines(X, pred, col = "red")

</code></pre>

<hr>
<h2 id='kplsrda'>KPLSR-DA models</h2><span id='topic+kplsrda'></span><span id='topic+predict.Kplsrda'></span>

<h3>Description</h3>

<p>Discrimination (DA) based on kernel PLSR (KPLSR)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
kplsrda(X, y, weights = NULL, nlv, kern = "krbf", ...)

## S3 method for class 'Kplsrda'
predict(object, X, ..., nlv = NULL) 

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kplsrda_+3A_x">X</code></td>
<td>
<p>For main function: Training X-data (<code class="reqn">n, p</code>). &mdash; For auxiliary function: New X-data (<code class="reqn">m, p</code>) to consider.</p>
</td></tr>
<tr><td><code id="kplsrda_+3A_y">y</code></td>
<td>
<p>Training class membership (<code class="reqn">n</code>). <b>Note:</b> If <code>y</code> is a factor, it is replaced by a character vector.</p>
</td></tr>
<tr><td><code id="kplsrda_+3A_weights">weights</code></td>
<td>
<p>Weights (<code class="reqn">n</code>) to apply to the training observations for the PLS2. Internally, weights are &quot;normalized&quot; to sum to 1. Default to <code>NULL</code> (weights are set to <code class="reqn">1 / n</code>).</p>
</td></tr>
<tr><td><code id="kplsrda_+3A_nlv">nlv</code></td>
<td>
<p>For main function: The number(s) of LVs to calculate. &mdash; For auxiliary function: The number(s) of LVs to consider.</p>
</td></tr>
<tr><td><code id="kplsrda_+3A_kern">kern</code></td>
<td>
<p>Name of the function defining the considered kernel for building the Gram matrix. See <code><a href="#topic+krbf">krbf</a></code> for syntax, and other available kernel functions.</p>
</td></tr>
<tr><td><code id="kplsrda_+3A_...">...</code></td>
<td>
<p>Optional arguments to pass in the kernel function defined in <code>kern</code> (e.g. <code>gamma</code> for <code><a href="#topic+krbf">krbf</a></code>).</p>
</td></tr>
<tr><td><code id="kplsrda_+3A_object">object</code></td>
<td>
<p>For auxiliary function: A fitted model, output of a call to the main functions.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The training variable <code class="reqn">y</code> (univariate class membership) is transformed to a dummy table containing <code class="reqn">nclas</code> columns, where <code class="reqn">nclas</code> is the number of classes present in <code class="reqn">y</code>. Each column is a dummy variable (0/1). Then, a kernel PLSR (KPLSR) is run on the <code class="reqn">X-</code>data and the dummy table, returning predictions of the dummy variables. For a given observation, the final prediction is the class corresponding to the dummy variable for which the prediction is the highest.
</p>


<h3>Value</h3>

<p>For <code>kplsrda</code>: 
</p>
<table>
<tr><td><code>fm</code></td>
<td>
<p>list with the kplsrda model: (<code>X</code>): the training X-data (<code class="reqn">n, p</code>); (<code>Kt</code>): the Gram matrix; (<code>T</code>): X-scores matrix; (<code>C</code>): The Y-loading weights matrix; (<code>U</code>): intermediate output; (<code>R</code>): The PLS projection matrix (p,nlv); (<code>ymeans</code>): the centering vector of Y (q,1); (<code>weights</code>): vector of observation weights; (<code>kern</code>): kern function; (<code>dots</code>): Optional arguments.</p>
</td></tr>
<tr><td><code>lev</code></td>
<td>
<p>y levels</p>
</td></tr>
<tr><td><code>ni</code></td>
<td>
<p>number of observations by level of y</p>
</td></tr>
</table>
<p>For <code>predict.Kplsrda</code>: 
</p>
<table>
<tr><td><code>pred</code></td>
<td>
<p>predicted class for each observation</p>
</td></tr>
<tr><td><code>posterior</code></td>
<td>
<p>calculated probability of belonging to a class for each observation</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
n &lt;- 50 ; p &lt;- 8
Xtrain &lt;- matrix(rnorm(n * p), ncol = p)
ytrain &lt;- sample(c(1, 4, 10), size = n, replace = TRUE)
m &lt;- 5
Xtest &lt;- Xtrain[1:m, ] ; ytest &lt;- ytrain[1:m]

nlv &lt;- 2
fm &lt;- kplsrda(Xtrain, ytrain, nlv = nlv)
names(fm)
predict(fm, Xtest)

pred &lt;- predict(fm, Xtest)$pred
err(pred, ytest)

predict(fm, Xtest, nlv = 0:nlv)$posterior
predict(fm, Xtest, nlv = 0)$posterior

predict(fm, Xtest, nlv = 0:nlv)$pred
predict(fm, Xtest, nlv = 0)$pred

</code></pre>

<hr>
<h2 id='krbf'>Kernel functions</h2><span id='topic+krbf'></span><span id='topic+kpol'></span><span id='topic+ktanh'></span>

<h3>Description</h3>

<p>Building Gram matrices for different kernels (e.g. Scholkopf &amp; Smola 2002).
</p>
<p>- radial basis:    exp(-gamma * |x - y|^2)
</p>
<p>- polynomial:      (gamma * x' * y + coef0)^degree
</p>
<p>- sigmoid:        tanh(gamma * x' * y + coef0)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
krbf(X, Y = NULL, gamma = 1)

kpol(X, Y = NULL, degree = 1, gamma = 1, coef0 = 0)

ktanh(X, Y = NULL, gamma = 1, coef0 = 0)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="krbf_+3A_x">X</code></td>
<td>
<p>Dataset (<code class="reqn">n, p</code>).</p>
</td></tr>
<tr><td><code id="krbf_+3A_y">Y</code></td>
<td>
<p>Dataset (<code class="reqn">m, p</code>). The resulting Gram matrix <code class="reqn">K(X, Y)</code> has dimensionnality (<code class="reqn">n, m</code>). If <code>NULL</code> (default), <code>Y</code> is set equal to <code>X</code>.</p>
</td></tr>
<tr><td><code id="krbf_+3A_gamma">gamma</code></td>
<td>
<p>value of the gamma parameter in the kernel calculation.</p>
</td></tr>
<tr><td><code id="krbf_+3A_degree">degree</code></td>
<td>
<p>For <code>kpol</code>: value of the degree parameter in the polynomial kernel calculation.</p>
</td></tr>
<tr><td><code id="krbf_+3A_coef0">coef0</code></td>
<td>
<p>For <code>kpol</code> and <code>ktanh</code>: value of the coef0 parameter in the polynomial or sigmoid kernel calculation.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Gram matrix</p>


<h3>References</h3>

<p>Scholkopf, B., Smola, A.J., 2002. Learning with kernels: support vector machines, regularization, optimization, and beyond, Adaptive computation and machine learning. MIT Press, Cambridge, Mass.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
n &lt;- 5 ; p &lt;- 3
Xtrain &lt;- matrix(rnorm(n * p), ncol = p)
Xtest &lt;- Xtrain[1:2, , drop = FALSE] 

gamma &lt;- .8
krbf(Xtrain, gamma = gamma)

krbf(Xtest, Xtrain, gamma = gamma)
exp(-.5 * euclsq(Xtest, Xtrain) / gamma^2)

kpol(Xtrain, degree = 2, gamma = .5, coef0 = 1)

</code></pre>

<hr>
<h2 id='krr'>KRR (LS-SVMR)</h2><span id='topic+krr'></span><span id='topic+coef.Krr'></span><span id='topic+predict.Krr'></span>

<h3>Description</h3>

<p>Kernel ridge regression models (KRR = LS-SVMR) (Suykens et al. 2000, Bennett &amp; Embrechts 2003, Krell 2018). 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
krr(X, Y, weights = NULL, lb = 1e-2, kern = "krbf", ...)

## S3 method for class 'Krr'
coef(object, ..., lb = NULL)  

## S3 method for class 'Krr'
predict(object, X, ..., lb = NULL)  

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="krr_+3A_x">X</code></td>
<td>
<p>For main function: Training X-data (<code class="reqn">n, p</code>). &mdash; For auxiliary function: New X-data (<code class="reqn">m, p</code>) to consider.</p>
</td></tr>
<tr><td><code id="krr_+3A_y">Y</code></td>
<td>
<p>Training Y-data (<code class="reqn">n, q</code>).</p>
</td></tr>
<tr><td><code id="krr_+3A_weights">weights</code></td>
<td>
<p>Weights (<code class="reqn">n, 1</code>) to apply to the training observations. Internally, weights are &quot;normalized&quot; to sum to 1. Default to <code>NULL</code> (weights are set to <code class="reqn">1 / n</code>).</p>
</td></tr>
<tr><td><code id="krr_+3A_lb">lb</code></td>
<td>
<p>A value of regularization parameter <code class="reqn">lambda</code>. If <code>lb = 0</code>, a pseudo-inverse is used in the RR.</p>
</td></tr> 
<tr><td><code id="krr_+3A_kern">kern</code></td>
<td>
<p>Name of the function defining the considered kernel for building the Gram matrix. See <code><a href="#topic+krbf">krbf</a></code> for syntax, and other available kernel functions.</p>
</td></tr>
<tr><td><code id="krr_+3A_...">...</code></td>
<td>
<p>Optional arguments to pass in the kernel function defined in <code>kern</code> (e.g. <code>gamma</code> for <code><a href="#topic+krbf">krbf</a></code>).</p>
</td></tr>
<tr><td><code id="krr_+3A_object">object</code></td>
<td>
<p>&mdash; For auxiliary function: A fitted model, output of a call to the main function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>For <code>krr</code>: 
</p>
<table>
<tr><td><code>X</code></td>
<td>
<p>Training X-data (<code class="reqn">n, p</code>).</p>
</td></tr>
<tr><td><code>K</code></td>
<td>
<p>Gram matrix</p>
</td></tr>
<tr><td><code>Kt</code></td>
<td>
<p>Gram matrix</p>
</td></tr>
<tr><td><code>U</code></td>
<td>
<p>intermediate output.</p>
</td></tr>
<tr><td><code>UtDY</code></td>
<td>
<p>intermediate output.</p>
</td></tr>
<tr><td><code>sv</code></td>
<td>
<p>singular values of the matrix (1,n)</p>
</td></tr>
<tr><td><code>lb</code></td>
<td>
<p>value of regularization parameter <code class="reqn">lambda</code></p>
</td></tr>
<tr><td><code>ymeans</code></td>
<td>
<p>the centering vector of Y (q,1)</p>
</td></tr>
<tr><td><code>weights</code></td>
<td>
<p>the weights vector of X-variables (p,1)</p>
</td></tr>
<tr><td><code>kern</code></td>
<td>
<p>kern function.</p>
</td></tr>
<tr><td><code>dots</code></td>
<td>
<p>Optional arguments.</p>
</td></tr>
</table>
<p>For <code>coef.Krr</code>:
</p>
<table>
<tr><td><code>int</code></td>
<td>
<p>matrix (1,nlv) with the intercepts</p>
</td></tr>
<tr><td><code>alpha</code></td>
<td>
<p>matrix (n,nlv) with the coefficients</p>
</td></tr>
<tr><td><code>df</code></td>
<td>
<p>model complexity (number of degrees of freedom)</p>
</td></tr>
</table>
<p>For <code>predict.Krr</code>:
</p>
<table>
<tr><td><code>pred</code></td>
<td>
<p>A list of matrices (<code class="reqn">m, q</code>) with the Y predicted values for the new X-data</p>
</td></tr>
</table>


<h3>Note</h3>

<p>KRR is close to the particular SVMR setting the <code class="reqn">epsilon</code> coefficient to zero (no marges excluding observations). The difference is that a L2-norm optimization is done, instead L1 in SVM.   
</p>
<p>The second example concerns the fitting of the function sinc(x) described in Rosipal &amp; Trejo 2001 p. 105-106
</p>


<h3>References</h3>

<p>Bennett, K.P., Embrechts, M.J., 2003. An optimization perspective on kernel partial least squares regression, in: Advances in Learning Theory: Methods, Models and Applications, NATO Science Series III: Computer &amp; Systems Sciences. IOS Press Amsterdam, pp. 227-250.
</p>
<p>Cawley, G.C., Talbot, N.L.C., 2002. Reduced Rank Kernel Ridge Regression. Neural Processing Letters 16, 293-302. https://doi.org/10.1023/A:1021798002258
</p>
<p>Krell, M.M., 2018. Generalizing, Decoding, and Optimizing Support Vector Machine Classification. arXiv:1801.04929.
</p>
<p>Saunders, C., Gammerman, A., Vovk, V., 1998. Ridge Regression Learning Algorithm in Dual Variables, in: In Proceedings of the 15th International Conference on Machine Learning. Morgan Kaufmann, pp. 515-521.
</p>
<p>Suykens, J.A.K., Lukas, L., Vandewalle, J., 2000. Sparse approximation using least squares support vector machines. 2000 IEEE International Symposium on Circuits and Systems. Emerging Technologies for the 21st Century. Proceedings (IEEE Cat No.00CH36353). https://doi.org/10.1109/ISCAS.2000.856439
</p>
<p>Welling, M., n.d. Kernel ridge regression. Department of Computer Science, University of Toronto, Toronto, Canada. https://www.ics.uci.edu/~welling/classnotes/papers_class/Kernel-Ridge.pdf
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## EXAMPLE 1

n &lt;- 6 ; p &lt;- 4
Xtrain &lt;- matrix(rnorm(n * p), ncol = p)
ytrain &lt;- rnorm(n)
Ytrain &lt;- cbind(y1 = ytrain, y2 = 100 * ytrain)
m &lt;- 3
Xtest &lt;- Xtrain[1:m, , drop = FALSE] 
Ytest &lt;- Ytrain[1:m, , drop = FALSE] ; ytest &lt;- Ytest[1:m, 1]

lb &lt;- 2
fm &lt;- krr(Xtrain, Ytrain, lb = lb, kern = "krbf", gamma = .8)
coef(fm)
coef(fm, lb = .6)
predict(fm, Xtest)
predict(fm, Xtest, lb = c(0.1, .6))

pred &lt;- predict(fm, Xtest)$pred
msep(pred, Ytest)

lb &lt;- 2
fm &lt;- krr(Xtrain, Ytrain, lb = lb, kern = "kpol", degree = 2, coef0 = 10)
predict(fm, Xtest)

## EXAMPLE 2

x &lt;- seq(-10, 10, by = .2)
x[x == 0] &lt;- 1e-5
n &lt;- length(x)
zy &lt;- sin(abs(x)) / abs(x)
y &lt;- zy + rnorm(n, 0, .2)
plot(x, y, type = "p")
lines(x, zy, lty = 2)
X &lt;- matrix(x, ncol = 1)

fm &lt;- krr(X, y, lb = .1, gamma = .5)
pred &lt;- predict(fm, X)$pred
plot(X, y, type = "p")
lines(X, zy, lty = 2)
lines(X, pred, col = "red")

</code></pre>

<hr>
<h2 id='krrda'>KRR-DA models</h2><span id='topic+krrda'></span><span id='topic+predict.Krrda'></span>

<h3>Description</h3>

<p>Discrimination (DA) based on kernel ridge regression (KRR).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
krrda(X, y, weights = NULL, lb = 1e-5, kern = "krbf", ...)

## S3 method for class 'Krrda'
predict(object, X, ..., lb = NULL) 

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="krrda_+3A_x">X</code></td>
<td>
<p>For main function: Training X-data (<code class="reqn">n, p</code>). &mdash; For auxiliary function: New X-data (<code class="reqn">m, p</code>) to consider.</p>
</td></tr>
<tr><td><code id="krrda_+3A_y">y</code></td>
<td>
<p>Training class membership (<code class="reqn">n</code>). <b>Note:</b> If <code>y</code> is a factor, it is replaced by a character vector.</p>
</td></tr>
<tr><td><code id="krrda_+3A_weights">weights</code></td>
<td>
<p>Weights (<code class="reqn">n</code>) to apply to the training observations for the PLS2. Internally, weights are &quot;normalized&quot; to sum to 1. Default to <code>NULL</code> (weights are set to <code class="reqn">1 / n</code>).</p>
</td></tr>
<tr><td><code id="krrda_+3A_lb">lb</code></td>
<td>
<p>A value of regularization parameter <code class="reqn">lambda</code>. If <code>lb = 0</code>, a pseudo-inverse is used in the RR.</p>
</td></tr>
<tr><td><code id="krrda_+3A_kern">kern</code></td>
<td>
<p>Name of the function defining the considered kernel for building the Gram matrix. See <code><a href="#topic+krbf">krbf</a></code> for syntax, and other available kernel functions.</p>
</td></tr>
<tr><td><code id="krrda_+3A_...">...</code></td>
<td>
<p>Optional arguments to pass in the kernel function defined in <code>kern</code> (e.g. <code>gamma</code> for <code><a href="#topic+krbf">krbf</a></code>).</p>
</td></tr>
<tr><td><code id="krrda_+3A_object">object</code></td>
<td>
<p>&mdash; For auxiliary function: A fitted model, output of a call to the main functions.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The training variable <code class="reqn">y</code> (univariate class membership) is transformed to a dummy table containing <code class="reqn">nclas</code> columns, where <code class="reqn">nclas</code> is the number of classes present in <code class="reqn">y</code>. Each column is a dummy variable (0/1). Then, a kernel ridge regression (KRR) is run on the <code class="reqn">X-</code>data and the dummy table, returning predictions of the dummy variables. For a given observation, the final prediction is the class corresponding to the dummy variable for which the prediction is the highest.
</p>


<h3>Value</h3>

<p>For <code>krrda</code>: 
</p>
<table>
<tr><td><code>fm</code></td>
<td>
<p>List with the outputs of the RR ((<code>X</code>): Training X-data (<code class="reqn">n, p</code>); (<code>K</code>): Gram matrix; (<code>Kt</code>): Gram matrix; (<code>U</code>): intermediate output; (<code>UtDY</code>): intermediate output; (<code>sv</code>): singular values of the matrix (1,n); (<code>lb</code>): value of regularization parameter <code class="reqn">lambda</code>; (<code>ymeans</code>): the centering vector of Y (q,1); (<code>weights</code>): the weights vector of X-variables (p,1); (<code>kern</code>): kern function; (<code>dots</code>): Optional arguments.</p>
</td></tr>
<tr><td><code>lev</code></td>
<td>
<p>y levels</p>
</td></tr>
<tr><td><code>ni</code></td>
<td>
<p>number of observations by level of y</p>
</td></tr>
</table>
<p>For <code>predict.Krrda</code>: 
</p>
<table>
<tr><td><code>pred</code></td>
<td>
<p>matrix or list of matrices (if lb is a vector), with predicted class for each observation</p>
</td></tr>
<tr><td><code>posterior</code></td>
<td>
<p>matrix or list of matrices (if lb is a vector), calculated probability of belonging to a class for each observation</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
n &lt;- 50 ; p &lt;- 8
Xtrain &lt;- matrix(rnorm(n * p), ncol = p)
ytrain &lt;- sample(c(1, 4, 10), size = n, replace = TRUE)

m &lt;- 5
Xtest &lt;- Xtrain[1:m, ] ; ytest &lt;- ytrain[1:m]

lb &lt;- 1
fm &lt;- krrda(Xtrain, ytrain, lb = lb)
names(fm)
predict(fm, Xtest)

pred &lt;- predict(fm, Xtest)$pred
err(pred, ytest)

predict(fm, Xtest, lb = 0:2)
predict(fm, Xtest, lb = 0)

</code></pre>

<hr>
<h2 id='lda'>LDA and QDA</h2><span id='topic+lda'></span><span id='topic+qda'></span><span id='topic+predict.Lda'></span><span id='topic+predict.Qda'></span>

<h3>Description</h3>

<p>Probabilistic (parametric) linear and quadratic discriminant analysis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
lda(X, y, prior = c("unif", "prop"))

qda(X, y, prior = c("unif", "prop"))
  
## S3 method for class 'Lda'
predict(object, X, ...)  
## S3 method for class 'Qda'
predict(object, X, ...)  

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lda_+3A_x">X</code></td>
<td>
<p>For the main functions: Training X-data (<code class="reqn">n, p</code>). &mdash; For the auxiliary functions: New X-data (<code class="reqn">m, p</code>) to consider.</p>
</td></tr>
<tr><td><code id="lda_+3A_y">y</code></td>
<td>
<p>Training class membership (<code class="reqn">n</code>). <b>Note:</b> If <code>y</code> is a factor, it is replaced by a character vector.</p>
</td></tr>
<tr><td><code id="lda_+3A_prior">prior</code></td>
<td>
<p>The prior probabilities of the classes. Possible values are &quot;unif&quot; (default; probabilities are set equal for all the classes) or &quot;prop&quot; (probabilities are set equal to the observed proportions of the classes in <code>y</code>).</p>
</td></tr>
<tr><td><code id="lda_+3A_object">object</code></td>
<td>
<p>For the auxiliary functions: A fitted model, output of a call to the main functions.</p>
</td></tr>
<tr><td><code id="lda_+3A_...">...</code></td>
<td>
<p>For the auxiliary functions: Optional arguments. Not used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For each observation to predict, the posterior probability to belong to a given class is estimated using the Bayes' formula, assuming priors (proportional or uniform) and a multivariate Normal distribution for the dependent variables <code class="reqn">X</code>. The prediction is the class with the highest posterior probability.
</p>
<p>LDA assumes homogeneous <code class="reqn">X-</code>covariance matrices for the classes while QDA assumes different covariance matrices. The functions use <code><a href="#topic+dmnorm">dmnorm</a></code> for estimating the multivariate Normal densities. 
</p>


<h3>Value</h3>

<p>For <code>lda</code> and <code>qda</code>:
</p>
<table>
<tr><td><code>ct</code></td>
<td>
<p>centers (column-wise means) for classes of observations.</p>
</td></tr>
<tr><td><code>W</code></td>
<td>
<p>unbiased within covariance matrices for classes of observations.</p>
</td></tr>
<tr><td><code>wprior</code></td>
<td>
<p>prior probabilities of the classes.</p>
</td></tr>
<tr><td><code>lev</code></td>
<td>
<p>y levels.</p>
</td></tr>
<tr><td><code>ni</code></td>
<td>
<p>number of observations by level of y.</p>
</td></tr>
</table>
<p>For <code>predict.Lda</code> and <code>predict.Qda</code>:
</p>
<table>
<tr><td><code>pred</code></td>
<td>
<p>predicted classes of observations.</p>
</td></tr>
<tr><td><code>ds</code></td>
<td>
<p>Prediction of the normal probability density.</p>
</td></tr>
<tr><td><code>posterior</code></td>
<td>
<p>posterior probabilities of the classes.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Saporta, G., 2011. Probabilités analyse des données et statistique. Editions Technip, Paris, France.
</p>
<p>Venables, W. N. and Ripley, B. D. (2002) Modern Applied Statistics with S. Fourth edition. Springer.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## EXAMPLE 1

data(iris)

X &lt;- iris[, 1:4]
y &lt;- iris[, 5]
N &lt;- nrow(X)

nTest &lt;- round(.25 * N)
nTraining &lt;- N - nTest
s &lt;- sample(1:N, nTest)
Xtrain &lt;- X[-s, ]
ytrain &lt;- y[-s]
Xtest &lt;- X[s, ]
ytest &lt;- y[s]

prior &lt;- "unif"

fm &lt;- lda(Xtrain, ytrain, prior = prior)
res &lt;- predict(fm, Xtest)
names(res)

headm(res$pred)
headm(res$ds)
headm(res$posterior)

err(res$pred, ytest)

## EXAMPLE 2

data(iris)

X &lt;- iris[, 1:4]
y &lt;- iris[, 5]
N &lt;- nrow(X)

nTest &lt;- round(.25 * N)
nTraining &lt;- N - nTest
s &lt;- sample(1:N, nTest)
Xtrain &lt;- X[-s, ]
ytrain &lt;- y[-s]
Xtest &lt;- X[s, ]
ytest &lt;- y[s]

prior &lt;- "prop"

fm &lt;- lda(Xtrain, ytrain, prior = prior)
res &lt;- predict(fm, Xtest)
names(res)

headm(res$pred)
headm(res$ds)
headm(res$posterior)

err(res$pred, ytest)

</code></pre>

<hr>
<h2 id='lmr'>Linear regression models</h2><span id='topic+lmr'></span><span id='topic+coef.Lmr'></span><span id='topic+predict.Lmr'></span>

<h3>Description</h3>

<p>Linear regression models (uses function <code><a href="stats.html#topic+lm">lm</a></code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
lmr(X, Y, weights = NULL)

## S3 method for class 'Lmr'
coef(object, ...) 

## S3 method for class 'Lmr'
predict(object, X, ...)  

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lmr_+3A_x">X</code></td>
<td>
<p>For the main function: Training X-data (<code class="reqn">n, p</code>). &mdash; For the auxiliary functions: New X-data (<code class="reqn">m, p</code>) to consider.</p>
</td></tr>
<tr><td><code id="lmr_+3A_y">Y</code></td>
<td>
<p>Training Y-data (<code class="reqn">n, q</code>).</p>
</td></tr>
<tr><td><code id="lmr_+3A_weights">weights</code></td>
<td>
<p>Weights (<code class="reqn">n, 1</code>) to apply to the training observations. Internally, weights are &quot;normalized&quot; to sum to 1. Default to <code>NULL</code> (weights are set to <code class="reqn">1 / n</code>).</p>
</td></tr>
<tr><td><code id="lmr_+3A_object">object</code></td>
<td>
<p>For the auxiliary functions:A fitted model, output of a call to the main functions.</p>
</td></tr>
<tr><td><code id="lmr_+3A_...">...</code></td>
<td>
<p>For the auxiliary functions: Optional arguments. Not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>For <code>lmr</code>:
</p>
<table>
<tr><td><code>coefficients</code></td>
<td>
<p>coefficient matrix.</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p>residual matrix.</p>
</td></tr>
<tr><td><code>effects</code></td>
<td>
<p>component relating to the linear fit, for use by extractor functions.</p>
</td></tr>
<tr><td><code>rank</code></td>
<td>
<p>the numeric rank of the fitted linear model.</p>
</td></tr>
<tr><td><code>fitted.values</code></td>
<td>
<p>the fitted mean values.</p>
</td></tr>
<tr><td><code>assign</code></td>
<td>
<p>component relating to the linear fit, for use by extractor functions.</p>
</td></tr>
<tr><td><code>qr</code></td>
<td>
<p>component relating to the linear fit, for use by extractor functions.</p>
</td></tr>
<tr><td><code>df.residual</code></td>
<td>
<p>the residual degrees of freedom.</p>
</td></tr>
<tr><td><code>xlevels</code></td>
<td>
<p>(only where relevant) a record of the levels of the factors used in fitting.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>the matched call.</p>
</td></tr>
<tr><td><code>terms</code></td>
<td>
<p>the terms object used.</p>
</td></tr>
<tr><td><code>model</code></td>
<td>
<p>the model frame used.</p>
</td></tr>
</table>
<p>For <code>coef.Lmr</code>: 
</p>
<table>
<tr><td><code>int</code></td>
<td>
<p>matrix (1,nlv) with the intercepts</p>
</td></tr>
<tr><td><code>B</code></td>
<td>
<p>matrix (n,nlv) with the coefficients</p>
</td></tr>
</table>
<p>For <code>predict.Lmr</code>: 
</p>
<table>
<tr><td><code>pred</code></td>
<td>
<p>A list of matrices (<code class="reqn">m, q</code>) with the Y predicted values for the new X-data</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
n &lt;- 8 ; p &lt;- 3
X &lt;- matrix(rnorm(n * p, mean = 10), ncol = p, byrow = TRUE)
y &lt;- rnorm(n)
Y &lt;- cbind(y, rnorm(n))
Xtrain &lt;- X[1:6, ] ; Ytrain &lt;- Y[1:6, ]
Xtest &lt;- X[7:8, ] ; Ytest &lt;- Y[7:8, ]

fm &lt;- lmr(Xtrain, Ytrain)
coef(fm)

predict(fm, Xtest)

pred &lt;- predict(fm, Xtest)$pred
msep(pred, Ytest)

</code></pre>

<hr>
<h2 id='lmrda'>LMR-DA models</h2><span id='topic+lmrda'></span><span id='topic+predict.Lmrda'></span>

<h3>Description</h3>

<p>Discrimination (DA) based on linear regression (LMR).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
lmrda(X, y, weights = NULL)

## S3 method for class 'Lmrda'
predict(object, X, ...) 

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lmrda_+3A_x">X</code></td>
<td>
<p>For the main function: Training X-data (<code class="reqn">n, p</code>). &mdash; For the auxiliary function: New X-data (<code class="reqn">m, p</code>) to consider.</p>
</td></tr>
<tr><td><code id="lmrda_+3A_y">y</code></td>
<td>
<p>Training class membership (<code class="reqn">n</code>). <b>Note:</b> If <code>y</code> is a factor, it is replaced by a character vector.</p>
</td></tr>
<tr><td><code id="lmrda_+3A_weights">weights</code></td>
<td>
<p>Weights (<code class="reqn">n</code>) to apply to the training observations for the PLS2. Internally, weights are &quot;normalized&quot; to sum to 1. Default to <code>NULL</code> (weights are set to <code class="reqn">1 / n</code>).</p>
</td></tr>
<tr><td><code id="lmrda_+3A_object">object</code></td>
<td>
<p>For the auxiliary function: A fitted model, output of a call to the main functions.</p>
</td></tr>
<tr><td><code id="lmrda_+3A_...">...</code></td>
<td>
<p>For the auxiliary function: Optional arguments. Not used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The training variable <code class="reqn">y</code> (univariate class membership) is transformed to a dummy table containing <code class="reqn">nclas</code> columns, where <code class="reqn">nclas</code> is the number of classes present in <code class="reqn">y</code>. Each column is a dummy variable (0/1). Then, a linear regression model (LMR) is run on the <code class="reqn">X-</code>data and the dummy table, returning predictions of the dummy variables. For a given observation, the final prediction is the class corresponding to the dummy variable for which the prediction is the highest.
</p>


<h3>Value</h3>

<p>For <code>lrmda</code>:
</p>
<table>
<tr><td><code>fm</code></td>
<td>
<p>List with the outputs((<code>coefficients</code>): coefficient matrix; (<code>residuals</code>): residual matrix; (<code>fitted.values</code>): the fitted mean values; (<code>effects</code>): component relating to the linear fit, for use by extractor functions; (<code>weights</code>): Weights (<code class="reqn">n</code>) applied to the training observations for the PLS2; (<code>rank</code>): the numeric rank of the fitted linear model; (<code>assign</code>): component relating to the linear fit, for use by extractor functions; (<code>qr</code>): component relating to the linear fit, for use by extractor functions; (<code>df.residual</code>): the residual degrees of freedom;  (<code>xlevels</code>): (only where relevant) a record of the levels of the factors used in fitting; (<code>call</code>): the matched call; (<code>terms</code>): the terms object used; (<code>model</code>): the model frame used).</p>
</td></tr>
<tr><td><code>lev</code></td>
<td>
<p>y levels.</p>
</td></tr>
<tr><td><code>ni</code></td>
<td>
<p>number of observations by level of y.</p>
</td></tr>
</table>
<p>For <code>predict.Lrmda</code>:
</p>
<table>
<tr><td><code>pred</code></td>
<td>
<p>predicted classes of observations.</p>
</td></tr>
<tr><td><code>posterior</code></td>
<td>
<p>posterior probability of belonging to a class for each observation.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
n &lt;- 50 ; p &lt;- 8
Xtrain &lt;- matrix(rnorm(n * p), ncol = p)
ytrain &lt;- sample(c(1, 4, 10), size = n, replace = TRUE)
m &lt;- 5
Xtest &lt;- Xtrain[1:m, ] ; ytest &lt;- ytrain[1:m]

fm &lt;- lmrda(Xtrain, ytrain)
names(fm)
predict(fm, Xtest)

coef(fm$fm)

pred &lt;- predict(fm, Xtest)$pred
err(pred, ytest)

</code></pre>

<hr>
<h2 id='locw'>Locally weighted models</h2><span id='topic+locw'></span><span id='topic+locwlv'></span>

<h3>Description</h3>

<p><code>locw</code> and <code>locwlv</code> are generic working functions returning predictions of KNN locally weighted (LW) models. One specific (= local) model is fitted for each observation to predict, and a prediction is returned. See the wrapper <code><a href="#topic+lwplsr">lwplsr</a></code> (KNN-LWPLSR) for an example of use. 
</p>
<p>In KNN-LW models, the prediction is built from two sequential steps, therafter referred to as <code class="reqn">weighting "1"</code> and <code class="reqn">weighting "2"</code>, respectively. For each new observation to predict, the two steps are as follow:
</p>
<p>- <code class="reqn">Weighting "1"</code>. The <code class="reqn">k</code> nearest neighbors (in the training data set) are selected and the prediction model is fitted (in the next step) only on this neighborhood. It is equivalent to give a weight = 1 to the neighbors, and a weight = 0 to the other training observations, which corresponds to a binary weighting.
</p>
<p>- <code class="reqn">Weighting "2"</code>. Each of the <code class="reqn">k</code> nearest neighbors eventually receives a weight (different from the usual <code class="reqn">1/k</code>) before fitting the model. The weight depend from the dissimilarity (preliminary calculated) between the observation and the neighbor. This corresponds to a within-neighborhood weighting.
</p>
<p>The prediction model used in step <code class="reqn">"2"</code> has to be defined in a function specified in argument <code>fun</code>. If there are <code class="reqn">m</code> new observations to predict, a list of <code class="reqn">m</code> vectors defining the <code class="reqn">m</code> neighborhoods has to be provided (argument <code>listnn</code>). Each of the <code class="reqn">m</code> vectors contains the indexes of the nearest neighbors in the training set. The <code class="reqn">m</code> vectors are not necessary of same length, i.e. the neighborhood size can vary between observations to predict. If there is a weighting in step <code class="reqn">"2"</code>, a list of <code class="reqn">m</code> vectors of weights have to be provided  (argument <code>listw</code>). Then  <code>locw</code> fits the model successively for each of the <code class="reqn">m</code> neighborhoods, and returns the corresponding <code class="reqn">m</code> predictions.
</p>
<p>Function  <code>locwlv</code> is dedicated to prediction models based on latent variables (LVs) calculations, such as PLSR. It is much faster and recommended.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
locw(Xtrain, Ytrain, X, listnn, listw = NULL, fun, verb = FALSE, ...)

locwlv(Xtrain, Ytrain, X, listnn, listw = NULL, fun, nlv, verb = FALSE, ...)
  
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="locw_+3A_xtrain">Xtrain</code></td>
<td>
<p>Training X-data (<code class="reqn">n, p</code>).</p>
</td></tr>
<tr><td><code id="locw_+3A_ytrain">Ytrain</code></td>
<td>
<p>Training Y-data (<code class="reqn">n, q</code>).</p>
</td></tr>
<tr><td><code id="locw_+3A_x">X</code></td>
<td>
<p>New X-data (<code class="reqn">m, p</code>) to predict.</p>
</td></tr>
<tr><td><code id="locw_+3A_listnn">listnn</code></td>
<td>
<p>A list of <code class="reqn">m</code> vectors defining weighting &quot;1&quot;. Component <code class="reqn">i</code> of this list is a vector (of length between 1 and <code class="reqn">n</code>) of indexes. These indexes define the training observations that are the nearest neighbors of new observation <code class="reqn">i</code>. Typically, <code>listnn</code> can be built from <code><a href="#topic+getknn">getknn</a></code>, but any other list of length <code class="reqn">m</code> can be provided. The <code class="reqn">m</code> vectors can have equal length (i.e. the <code class="reqn">m</code> neighborhoods are of equal size) or not (the number of neighbors varies between the observations to predict).</p>
</td></tr>
<tr><td><code id="locw_+3A_listw">listw</code></td>
<td>
<p>A list of <code class="reqn">m</code> vectors defining weighting &quot;2&quot;. Component <code class="reqn">i</code> of this list is a vector (that must have the same length as component <code class="reqn">i</code> of <code>listnn</code>) of the weights given to the nearest neighbors when the prediction model is fitted. Internally, weights are &quot;normalized&quot; to sum to 1 in each component. Default to <code>NULL</code> (weights are set to <code class="reqn">1 / k</code> where <code class="reqn">k</code>is the size of the neihborhodd).</p>
</td></tr>
<tr><td><code id="locw_+3A_fun">fun</code></td>
<td>
<p>A function corresponding to the prediction model to fit on the <code class="reqn">m</code> neighborhoods.</p>
</td></tr>
<tr><td><code id="locw_+3A_nlv">nlv</code></td>
<td>
<p>For <code>locwlv</code> : The number of LVs to calculate.</p>
</td></tr>
<tr><td><code id="locw_+3A_verb">verb</code></td>
<td>
<p>Logical. If <code>TRUE</code>, fitting information are printed.</p>
</td></tr>
<tr><td><code id="locw_+3A_...">...</code></td>
<td>
<p>Optional arguments to pass in function <code>fun</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>pred</code></td>
<td>
<p>matrix or list of matrices (if <code>nlv</code> is a vector), with predictions</p>
</td></tr>
</table>


<h3>References</h3>

<p>Lesnoff M, Metz M, Roger J-M. Comparison of locally weighted PLS strategies for regression and discrimination on agronomic NIR data. Journal of Chemometrics. 2020;n/a(n/a):e3209. doi:10.1002/cem.3209.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
n &lt;- 50 ; p &lt;- 30
Xtrain &lt;- matrix(rnorm(n * p), ncol = p, byrow = TRUE)
ytrain &lt;- rnorm(n)
Ytrain &lt;- cbind(ytrain, 100 * ytrain)
m &lt;- 4
Xtest &lt;- matrix(rnorm(m * p), ncol = p, byrow = TRUE)
ytest &lt;- rnorm(m)
Ytest &lt;- cbind(ytest, 10 * ytest)

k &lt;- 5
z &lt;- getknn(Xtrain, Xtest, k = k)
listnn &lt;- z$listnn
listd &lt;- z$listd
listnn
listd

listw &lt;- lapply(listd, wdist, h = 2)
listw

nlv &lt;- 2  
locw(Xtrain, Ytrain, Xtest, 
     listnn = listnn, fun = plskern, nlv = nlv)
locw(Xtrain, Ytrain, Xtest, 
     listnn = listnn, listw = listw, fun = plskern, nlv = nlv)

locwlv(Xtrain, Ytrain, Xtest, 
     listnn = listnn, listw = listw, fun = plskern, nlv = nlv)
locwlv(Xtrain, Ytrain, Xtest, 
     listnn = listnn, listw = listw, fun = plskern, nlv = 0:nlv)

</code></pre>

<hr>
<h2 id='lwplsr'>KNN-LWPLSR</h2><span id='topic+lwplsr'></span><span id='topic+predict.Lwplsr'></span>

<h3>Description</h3>

<p>Function <code>lwplsr</code> fits KNN-LWPLSR models described in Lesnoff et al. (2020). The function uses functions <code><a href="#topic+getknn">getknn</a></code>, <code><a href="#topic+locw">locw</a></code> and PLSR functions. See the code for details. Many variants of such pipelines can be build using <code><a href="#topic+locw">locw</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
lwplsr(X, Y,
    nlvdis, diss = c("eucl", "mahal"),
    h, k,
    nlv,
    cri = 4,
    verb = FALSE)

## S3 method for class 'Lwplsr'
predict(object, X, ..., nlv = NULL)  

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lwplsr_+3A_x">X</code></td>
<td>
<p>&mdash; For the main function: Training X-data (<code class="reqn">n, p</code>). &mdash; For the auxiliary function: New X-data (<code class="reqn">m, p</code>) to consider.</p>
</td></tr>
<tr><td><code id="lwplsr_+3A_y">Y</code></td>
<td>
<p>Training Y-data (<code class="reqn">n, q</code>).</p>
</td></tr>
<tr><td><code id="lwplsr_+3A_nlvdis">nlvdis</code></td>
<td>
<p>The number of LVs to consider in the global PLS used for the dimension reduction before calculating the dissimilarities (see details). If <code>nlvdis = 0</code>, there is no dimension reduction.</p>
</td></tr>
<tr><td><code id="lwplsr_+3A_diss">diss</code></td>
<td>
<p>The type of dissimilarity used for defining the neighbors. Possible values are &quot;eucl&quot; (default; Euclidean distance), &quot;mahal&quot; (Mahalanobis distance), or &quot;correlation&quot;. Correlation dissimilarities are calculated by sqrt(.5 * (1 - rho)).</p>
</td></tr>
<tr><td><code id="lwplsr_+3A_h">h</code></td>
<td>
<p>A scale scalar defining the shape of the weight function. Lower is <code class="reqn">h</code>, sharper is the function. See <code><a href="#topic+wdist">wdist</a></code>.</p>
</td></tr>
<tr><td><code id="lwplsr_+3A_k">k</code></td>
<td>
<p>The number of nearest neighbors to select for each observation to predict.</p>
</td></tr>
<tr><td><code id="lwplsr_+3A_nlv">nlv</code></td>
<td>
<p>The number(s) of LVs to calculate in the local PLSR models.</p>
</td></tr>
<tr><td><code id="lwplsr_+3A_cri">cri</code></td>
<td>
<p>Argument <code>cri</code> in function <code><a href="#topic+wdist">wdist</a></code>.</p>
</td></tr>
<tr><td><code id="lwplsr_+3A_verb">verb</code></td>
<td>
<p>Logical. If <code>TRUE</code>, fitting information are printed.</p>
</td></tr>
<tr><td><code id="lwplsr_+3A_object">object</code></td>
<td>
<p>&mdash; For the auxiliary function: A fitted model, output of a call to the main function.</p>
</td></tr>
<tr><td><code id="lwplsr_+3A_...">...</code></td>
<td>
<p>&mdash; For the auxiliary function: Optional arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>- LWPLSR: This is a particular case of &quot;weighted PLSR&quot; (WPLSR) (e.g. Schaal et al. 2002). In WPLSR, a priori weights, different from the usual <code class="reqn">1/n</code> (standard PLSR), are given to the <code class="reqn">n</code> training observations. These weights are used for calculating (i) the PLS scores and loadings and (ii) the regression model of the response(s) over the scores (by weighted least squares). LWPLSR is a particular case of WPLSR. &quot;L&quot; comes from &quot;localized&quot;: the weights are defined from dissimilarities (e.g. distances) between the new observation to predict and the training observations. By definition of LWPLSR, the weights, and therefore the fitted WPLSR model, change for each new observation to predict.
</p>
<p>- KNN-LWPLSR: Basic versions of LWPLSR (e.g. Sicard &amp; Sabatier 2006, Kim et al 2011) use, for each observation to predict, all the <code class="reqn">n</code> training observation. This can be very time consuming, in particular for large <code class="reqn">n</code>. A faster and often more efficient strategy is to preliminary select, in the training set, a number of <code class="reqn">k</code> nearest neighbors to the observation to predict (this is referred to as <code class="reqn">"weighting 1"</code> in function <code><a href="#topic+locw">locw</a></code>) and then to apply LWPLSR only to this pre-selected neighborhood (this is referred to as<code class="reqn">weighting "2"</code> in <code><a href="#topic+locw">locw</a></code>). This strategy corresponds to KNN-LWPLSR. 
</p>
<p>In function <code>lwplsr</code>, the dissimilarities used for computing the weights can be calculated from the original X-data or after a dimension reduction (argument <code>nlvdis</code>). In the last case, global PLS scores are computed from <code class="reqn">(X, Y)</code> and the dissimilarities are calculated on these scores. For high dimension X-data, the dimension reduction is in general required for using the Mahalanobis distance.   
</p>


<h3>Value</h3>

<p>For <code>lwplsr</code>: object of class <code>Lwplsr</code>
</p>
<p>For <code>predict.Lwplsr</code>: 
</p>
<table>
<tr><td><code>pred</code></td>
<td>
<p>prediction calculated for each observation</p>
</td></tr>
<tr><td><code>listnn</code></td>
<td>
<p>list with the neighbors used for each observation to be predicted</p>
</td></tr>
<tr><td><code>listd</code></td>
<td>
<p>list with the distances to the neighbors used for each observation to be predicted</p>
</td></tr>
<tr><td><code>listw</code></td>
<td>
<p>list with the weights attributed to the neighbors used for each observation to be predicted</p>
</td></tr>
</table>


<h3>References</h3>

<p>Kim, S., Kano, M., Nakagawa, H., Hasebe, S., 2011. Estimation of active pharmaceutical ingredients content using locally weighted partial least squares and statistical wavelength selection. Int. J. Pharm., 421, 269-274.
</p>
<p>Lesnoff, M., Metz, M., Roger, J.-M., 2020. Comparison of locally weighted PLS strategies for regression and discrimination on agronomic NIR data. Journal of Chemometrics, e3209. https://doi.org/10.1002/cem.3209
</p>
<p>Schaal, S., Atkeson, C., Vijayamakumar, S. 2002. Scalable techniques from nonparametric statistics for the real time robot learning. Applied Intell., 17, 49-60.
</p>
<p>Sicard, E. Sabatier, R., 2006. Theoretical framework for local PLS1 regression and application to a rainfall data set. Comput. Stat. Data Anal., 51, 1393-1410.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
n &lt;- 30 ; p &lt;- 10
Xtrain &lt;- matrix(rnorm(n * p), ncol = p)
ytrain &lt;- rnorm(n)
Ytrain &lt;- cbind(ytrain, 100 * ytrain)
m &lt;- 4
Xtest &lt;- matrix(rnorm(m * p), ncol = p)
ytest &lt;- rnorm(m)
Ytest &lt;- cbind(ytest, 10 * ytest)

nlvdis &lt;- 5 ; diss &lt;- "mahal"
h &lt;- 2 ; k &lt;- 10
nlv &lt;- 2  
fm &lt;- lwplsr(
    Xtrain, Ytrain, 
    nlvdis = nlvdis, diss = diss,
    h = h, k = k,
    nlv = nlv)
res &lt;- predict(fm, Xtest)
names(res)
res$pred
msep(res$pred, Ytest)

res &lt;- predict(fm, Xtest, nlv = 0:2)
res$pred

</code></pre>

<hr>
<h2 id='lwplsr_agg'>Aggregation of KNN-LWPLSR models with different numbers of LVs</h2><span id='topic+lwplsr_agg'></span><span id='topic+predict.Lwplsr_agg'></span>

<h3>Description</h3>

<p>Ensemblist method where the predictions are calculated by averaging the predictions of KNN-LWPLSR models (<code><a href="#topic+lwplsr">lwplsr</a></code>) built with different numbers of latent variables (LVs). 
</p>
<p>For instance, if argument <code>nlv</code> is set to <code>nlv = "5:10"</code>, the prediction for a new observation is the simple average of the predictions returned by the models with 5 LVS, 6 LVs, ... 10 LVs, respectively. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
lwplsr_agg(
    X, Y,
    nlvdis, diss = c("eucl", "mahal"),
    h, k,
    nlv,
    cri = 4,
    verb = FALSE
    )

## S3 method for class 'Lwplsr_agg'
predict(object, X, ...)  

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lwplsr_agg_+3A_x">X</code></td>
<td>
<p>For the main function: Training X-data (<code class="reqn">n, p</code>). &mdash; For the auxiliary function: New X-data (<code class="reqn">m, p</code>) to consider.</p>
</td></tr>
<tr><td><code id="lwplsr_agg_+3A_y">Y</code></td>
<td>
<p>Training Y-data (<code class="reqn">n, q</code>).</p>
</td></tr>
<tr><td><code id="lwplsr_agg_+3A_nlvdis">nlvdis</code></td>
<td>
<p>The number of LVs to consider in the global PLS used for the dimension reduction before calculating the dissimilarities. If <code>nlvdis = 0</code>, there is no dimension reduction.</p>
</td></tr>
<tr><td><code id="lwplsr_agg_+3A_diss">diss</code></td>
<td>
<p>The type of dissimilarity used for defining the neighbors. Possible values are &quot;eucl&quot; (default; Euclidean distance), &quot;mahal&quot; (Mahalanobis distance), or &quot;correlation&quot;. Correlation dissimilarities are calculated by sqrt(.5 * (1 - rho)).</p>
</td></tr>
<tr><td><code id="lwplsr_agg_+3A_h">h</code></td>
<td>
<p>A scale scalar defining the shape of the weight function. Lower is <code class="reqn">h</code>, sharper is the function. See <code><a href="#topic+wdist">wdist</a></code>.</p>
</td></tr>
<tr><td><code id="lwplsr_agg_+3A_k">k</code></td>
<td>
<p>The number of nearest neighbors to select for each observation to predict.</p>
</td></tr>
<tr><td><code id="lwplsr_agg_+3A_nlv">nlv</code></td>
<td>
<p>A character string such as &quot;5:20&quot; defining the range of the numbers of LVs to consider (here: the models with nb LVS = 5, 6, ..., 20 are averaged). Syntax such as &quot;10&quot; is also allowed (here: correponds to the single model with 10 LVs).</p>
</td></tr>
<tr><td><code id="lwplsr_agg_+3A_cri">cri</code></td>
<td>
<p>Argument <code>cri</code> in function <code><a href="#topic+wdist">wdist</a></code>.</p>
</td></tr>
<tr><td><code id="lwplsr_agg_+3A_verb">verb</code></td>
<td>
<p>Logical. If <code>TRUE</code>, fitting information are printed.</p>
</td></tr>
<tr><td><code id="lwplsr_agg_+3A_object">object</code></td>
<td>
<p>For the auxiliary function: A fitted model, output of a call to the main function.</p>
</td></tr>
<tr><td><code id="lwplsr_agg_+3A_...">...</code></td>
<td>
<p>For the auxiliary function: Optional arguments. Not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>For <code>lwplsr_agg</code>: object of class <code>Lwplsr_agg</code>
</p>
<p>For <code>predict.Lwplsr_agg</code>:
</p>
<table>
<tr><td><code>pred</code></td>
<td>
<p>prediction calculated for each observation, which is the most occurent level (vote) over the predictions returned by the models with different numbers of LVS respectively</p>
</td></tr>
<tr><td><code>listnn</code></td>
<td>
<p>list with the neighbors used for each observation to be predicted</p>
</td></tr>
<tr><td><code>listd</code></td>
<td>
<p>list with the distances to the neighbors used for each observation to be predicted</p>
</td></tr>
<tr><td><code>listw</code></td>
<td>
<p>list with the weights attributed to the neighbors used for each observation to be predicted</p>
</td></tr>
</table>


<h3>Note</h3>

<p>In the examples, <code>gridscore</code> and <code>gricv</code> have been used as there is no sense to use <code>gridscorelv</code> and <code>gricvlv</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## EXAMPLE 1

n &lt;- 30 ; p &lt;- 10
Xtrain &lt;- matrix(rnorm(n * p), ncol = p)
ytrain &lt;- rnorm(n)
Ytrain &lt;- cbind(ytrain, 100 * ytrain)
m &lt;- 4
Xtest &lt;- matrix(rnorm(m * p), ncol = p)
ytest &lt;- rnorm(m)
Ytest &lt;- cbind(ytest, 10 * ytest)

nlvdis &lt;- 5 ; diss &lt;- "mahal"
h &lt;- 2 ; k &lt;- 10
nlv &lt;- "2:6" 
fm &lt;- lwplsr_agg(
    Xtrain, Ytrain, 
    nlvdis = nlvdis, diss = diss,
    h = h, k = k,
    nlv = nlv)
names(fm)
res &lt;- predict(fm, Xtest)
names(res)
res$pred
msep(res$pred, Ytest)

## EXAMPLE 2

n &lt;- 30 ; p &lt;- 10
Xtrain &lt;- matrix(rnorm(n * p), ncol = p)
ytrain &lt;- rnorm(n)
Ytrain &lt;- cbind(ytrain, 100 * ytrain)
m &lt;- 4
Xtest &lt;- matrix(rnorm(m * p), ncol = p)
ytest &lt;- rnorm(m)
Ytest &lt;- cbind(ytest, 10 * ytest)

nlvdis &lt;- 5 ; diss &lt;- "mahal"
h &lt;- c(2, Inf)
k &lt;- c(10, 20)
nlv &lt;- c("1:3", "2:5")
pars &lt;- mpars(nlvdis = nlvdis, diss = diss,
    h = h, k = k, nlv = nlv)
pars
res &lt;- gridscore(
    Xtrain, Ytrain, Xtest, Ytest, 
    score = msep, 
    fun = lwplsr_agg, 
    pars = pars)
res

## EXAMPLE 3

n &lt;- 30 ; p &lt;- 10
Xtrain &lt;- matrix(rnorm(n * p), ncol = p)
ytrain &lt;- rnorm(n)
Ytrain &lt;- cbind(ytrain, 100 * ytrain)
m &lt;- 4
Xtest &lt;- matrix(rnorm(m * p), ncol = p)
ytest &lt;- rnorm(m)
Ytest &lt;- cbind(ytest, 10 * ytest)

K = 3
segm &lt;- segmkf(n = n, K = K, nrep = 1)
segm
res &lt;- gridcv(
    Xtrain, Ytrain, 
    segm, score = msep, 
    fun = lwplsr_agg, 
    pars = pars,
    verb = TRUE)
res

</code></pre>

<hr>
<h2 id='lwplsrda'>KNN-LWPLS-DA Models</h2><span id='topic+lwplsrda'></span><span id='topic+lwplslda'></span><span id='topic+lwplsqda'></span><span id='topic+predict.Lwplsrda'></span><span id='topic+predict.Lwplsprobda'></span>

<h3>Description</h3>

<p>- <code>lwplsrda</code>: KNN-LWPLSRDA models. This is the same methodology as for <code><a href="#topic+lwplsr">lwplsr</a></code> except that PLSR is replaced by PLSRDA (<code><a href="#topic+plsrda">plsrda</a></code>). See the help page of <code><a href="#topic+lwplsr">lwplsr</a></code> for details.
</p>
<p>- <code>lwplslda</code> and <code>lwplsqda</code>: Same as above, but PLSRDA is replaced by either PLSLDA (<code><a href="#topic+plslda">plslda</a></code>) or PLSQDA ((<code><a href="#topic+plsqda">plsqda</a></code>), respecively.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
lwplsrda(
    X, y,
    nlvdis, diss = c("eucl", "mahal"),
    h, k,
    nlv,
    cri = 4,
    verb = FALSE
    )

lwplslda(
    X, y,
    nlvdis, diss = c("eucl", "mahal"),
    h, k,
    nlv,
    prior = c("unif", "prop"),
    cri = 4,
    verb = FALSE
    ) 

lwplsqda(
    X, y,
    nlvdis, diss = c("eucl", "mahal"),
    h, k,
    nlv,
    prior = c("unif", "prop"),
    cri = 4,
    verb = FALSE
    ) 

## S3 method for class 'Lwplsrda'
predict(object, X, ..., nlv = NULL)  

## S3 method for class 'Lwplsprobda'
predict(object, X, ..., nlv = NULL)  

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lwplsrda_+3A_x">X</code></td>
<td>
<p>For the main functions: Training X-data (<code class="reqn">n, p</code>). &mdash; For the auxiliary functions: New X-data (<code class="reqn">m, p</code>) to consider.</p>
</td></tr>
<tr><td><code id="lwplsrda_+3A_y">y</code></td>
<td>
<p>Training class membership (<code class="reqn">n</code>). <b>Note:</b> If <code>y</code> is a factor, it is replaced by a character vector.</p>
</td></tr>
<tr><td><code id="lwplsrda_+3A_nlvdis">nlvdis</code></td>
<td>
<p>The number of LVs to consider in the global PLS used for the dimension reduction before calculating the dissimilarities. If <code>nlvdis = 0</code>, there is no dimension reduction.</p>
</td></tr>
<tr><td><code id="lwplsrda_+3A_diss">diss</code></td>
<td>
<p>The type of dissimilarity used for defining the neighbors. Possible values are &quot;eucl&quot; (default; Euclidean distance), &quot;mahal&quot; (Mahalanobis distance), or &quot;correlation&quot;. Correlation dissimilarities are calculated by sqrt(.5 * (1 - rho)).</p>
</td></tr>
<tr><td><code id="lwplsrda_+3A_h">h</code></td>
<td>
<p>A scale scalar defining the shape of the weight function. Lower is <code class="reqn">h</code>, sharper is the function. See <code><a href="#topic+wdist">wdist</a></code>.</p>
</td></tr>
<tr><td><code id="lwplsrda_+3A_k">k</code></td>
<td>
<p>The number of nearest neighbors to select for each observation to predict.</p>
</td></tr>
<tr><td><code id="lwplsrda_+3A_nlv">nlv</code></td>
<td>
<p>The number(s) of LVs to calculate in the local PLSDA models.</p>
</td></tr>
<tr><td><code id="lwplsrda_+3A_prior">prior</code></td>
<td>
<p>The prior probabilities of the classes. Possible values are &quot;unif&quot; (default; probabilities are set equal for all the classes) or &quot;prop&quot; (probabilities are set equal to the observed proportions of the classes in <code>y</code>).</p>
</td></tr>
<tr><td><code id="lwplsrda_+3A_cri">cri</code></td>
<td>
<p>Argument <code>cri</code> in function <code><a href="#topic+wdist">wdist</a></code>.</p>
</td></tr>
<tr><td><code id="lwplsrda_+3A_verb">verb</code></td>
<td>
<p>Logical. If <code>TRUE</code>, fitting information are printed.</p>
</td></tr>
<tr><td><code id="lwplsrda_+3A_object">object</code></td>
<td>
<p>For the auxiliary functions: A fitted model, output of a call to the main function.</p>
</td></tr>
<tr><td><code id="lwplsrda_+3A_...">...</code></td>
<td>
<p>For the auxiliary functions: Optional arguments. Not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>For <code>lwplsrda</code>, <code>lwplslda</code>, <code>lwplsqda</code>: object of class <code>Lwplsrda</code> or <code>Lwplsprobda</code>,
</p>
<p>For <code>predict.Lwplsrda</code>, <code>predict.Lwplsprobda</code> :
</p>
<table>
<tr><td><code>pred</code></td>
<td>
<p>class predicted for each observation</p>
</td></tr>
<tr><td><code>listnn</code></td>
<td>
<p>list with the neighbors used for each observation to be predicted</p>
</td></tr>
<tr><td><code>listd</code></td>
<td>
<p>list with the distances to the neighbors used for each observation to be predicted</p>
</td></tr>
<tr><td><code>listw</code></td>
<td>
<p>list with the weights attributed to the neighbors used for each observation to be predicted</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
n &lt;- 50 ; p &lt;- 7
Xtrain &lt;- matrix(rnorm(n * p), ncol = p)
ytrain &lt;- sample(c(1, 4, 10), size = n, replace = TRUE)
m &lt;- 4
Xtest &lt;- matrix(rnorm(m * p), ncol = p)
ytest &lt;- sample(c(1, 4, 10), size = m, replace = TRUE)

nlvdis &lt;- 5 ; diss &lt;- "mahal"
h &lt;- 2 ; k &lt;- 10
nlv &lt;- 2  
fm &lt;- lwplsrda(
    Xtrain, ytrain, 
    nlvdis = nlvdis, diss = diss,
    h = h, k = k,
    nlv = nlv
    )
res &lt;- predict(fm, Xtest)
res$pred
res$listnn
err(res$pred, ytest)

res &lt;- predict(fm, Xtest, nlv = 0:2)
res$pred


</code></pre>

<hr>
<h2 id='lwplsrda_agg'>Aggregation of KNN-LWPLSDA models with different numbers of LVs</h2><span id='topic+lwplsrda_agg'></span><span id='topic+lwplslda_agg'></span><span id='topic+lwplsqda_agg'></span><span id='topic+predict.Lwplsrda_agg'></span><span id='topic+predict.Lwplsprobda_agg'></span>

<h3>Description</h3>

<p>Ensemblist method where the predictions are calculated by &quot;averaging&quot; the predictions of KNN-LWPLSDA models built with different numbers of latent variables (LVs). 
</p>
<p>For instance, if argument <code>nlv</code> is set to <code>nlv = "5:10"</code>, the prediction for a new observation is the most occurent level (vote) over the predictions returned by the models with 5 LVS, 6 LVs, ... 10 LVs, respectively.
</p>
<p>- <code>lwplsrda_agg</code>: use <code><a href="#topic+plsrda">plsrda</a></code>.
</p>
<p>- <code>lwplslda_agg</code>: use <code><a href="#topic+plslda">plslda</a></code>.
</p>
<p>- <code>lwplsqda_agg</code>: use <code><a href="#topic+plsqda">plsqda</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
lwplsrda_agg(
    X, y,
    nlvdis, diss = c("eucl", "mahal"),
    h, k,
    nlv,
    cri = 4,
    verb = FALSE
    ) 

lwplslda_agg(
    X, y,
    nlvdis, diss = c("eucl", "mahal"),
    h, k,
    nlv, 
    prior = c("unif", "prop"),
    cri = 4,
    verb = FALSE
    ) 

lwplsqda_agg(
    X, y,
    nlvdis, diss = c("eucl", "mahal"),
    h, k,
    nlv, 
    prior = c("unif", "prop"),
    cri = 4,
    verb = FALSE
    ) 

## S3 method for class 'Lwplsrda_agg'
predict(object, X, ...)

## S3 method for class 'Lwplsprobda_agg'
predict(object, X, ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lwplsrda_agg_+3A_x">X</code></td>
<td>
<p>For the main functions: Training X-data (<code class="reqn">n, p</code>). &mdash; For the auxiliary functions: New X-data (<code class="reqn">m, p</code>) to consider.</p>
</td></tr>
<tr><td><code id="lwplsrda_agg_+3A_y">y</code></td>
<td>
<p>Training class membership (<code class="reqn">n</code>). <b>Note:</b> If <code>y</code> is a factor, it is replaced by a character vector.</p>
</td></tr>
<tr><td><code id="lwplsrda_agg_+3A_nlvdis">nlvdis</code></td>
<td>
<p>The number of LVs to consider in the global PLS used for the dimension reduction before calculating the dissimilarities. If <code>nlvdis = 0</code>, there is no dimension reduction.</p>
</td></tr>
<tr><td><code id="lwplsrda_agg_+3A_diss">diss</code></td>
<td>
<p>The type of dissimilarity used for defining the neighbors. Possible values are &quot;eucl&quot; (default; Euclidean distance), &quot;mahal&quot; (Mahalanobis distance), or &quot;correlation&quot;. Correlation dissimilarities are calculated by sqrt(.5 * (1 - rho)).</p>
</td></tr>
<tr><td><code id="lwplsrda_agg_+3A_h">h</code></td>
<td>
<p>A scale scalar defining the shape of the weight function. Lower is <code class="reqn">h</code>, sharper is the function. See <code><a href="#topic+wdist">wdist</a></code>.</p>
</td></tr>
<tr><td><code id="lwplsrda_agg_+3A_k">k</code></td>
<td>
<p>The number of nearest neighbors to select for each observation to predict.</p>
</td></tr>
<tr><td><code id="lwplsrda_agg_+3A_nlv">nlv</code></td>
<td>
<p>A character string such as &quot;5:20&quot; defining the range of the numbers of LVs to consider (here: the models with nb LVS = 5, 6, ..., 20 are averaged). Syntax such as &quot;10&quot; is also allowed (here: correponds to the single model with 10 LVs).</p>
</td></tr>
<tr><td><code id="lwplsrda_agg_+3A_prior">prior</code></td>
<td>
<p>For <code>lwplslda_agg</code> and <code>lwplsqda_agg</code>: The prior probabilities of the classes. Possible values are &quot;unif&quot; (default; probabilities are set equal for all the classes) or &quot;prop&quot; (probabilities are set equal to the observed proportions of the classes in <code>y</code>).</p>
</td></tr>
<tr><td><code id="lwplsrda_agg_+3A_cri">cri</code></td>
<td>
<p>Argument <code>cri</code> in function <code><a href="#topic+wdist">wdist</a></code>.</p>
</td></tr>
<tr><td><code id="lwplsrda_agg_+3A_verb">verb</code></td>
<td>
<p>Logical. If <code>TRUE</code>, fitting information are printed.</p>
</td></tr>
<tr><td><code id="lwplsrda_agg_+3A_object">object</code></td>
<td>
<p>For the auxiliary functions: A fitted model, output of a call to the main function.</p>
</td></tr>
<tr><td><code id="lwplsrda_agg_+3A_...">...</code></td>
<td>
<p>For the auxiliary functions: Optional arguments. Not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>For <code>lwplsrda_agg</code>, <code>lwplslda_agg</code> and <code>lwplsqda_agg</code>: object of class <code>lwplsrda_agg</code>, <code>lwplslda_agg</code> or <code>lwplsqda_agg</code>
</p>
<p>For <code>predict.Lwplsrda_agg</code> and <code>predict.Lwplsprobda_agg</code>: 
</p>
<table>
<tr><td><code>pred</code></td>
<td>
<p>prediction calculated for each observation, which is the most occurent level (vote) over the predictions returned by the models with different numbers of LVS respectively</p>
</td></tr>
<tr><td><code>listnn</code></td>
<td>
<p>list with the neighbors used for each observation to be predicted</p>
</td></tr>
<tr><td><code>listd</code></td>
<td>
<p>list with the distances to the neighbors used for each observation to be predicted</p>
</td></tr>
<tr><td><code>listw</code></td>
<td>
<p>list with the weights attributed to the neighbors used for each observation to be predicted</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The first example concerns KNN-LWPLSRDA-AGG.
The second example concerns KNN-LWPLSLDA-AGG. 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## KNN-LWPLSRDA-AGG

n &lt;- 40 ; p &lt;- 7
X &lt;- matrix(rnorm(n * p), ncol = p, byrow = TRUE)
y &lt;- sample(c(1, 4, 10), size = n, replace = TRUE)

Xtrain &lt;- X ; ytrain &lt;- y
m &lt;- 5
Xtest &lt;- X[1:m, ] ; ytest &lt;- y[1:m]

nlvdis &lt;- 5 ; diss &lt;- "mahal"
h &lt;- 2 ; k &lt;- 10
nlv &lt;- "2:4" 
fm &lt;- lwplsrda_agg(
    Xtrain, ytrain, 
    nlvdis = nlvdis, diss = diss,
    h = h, k = k,
    nlv = nlv)
res &lt;- predict(fm, Xtest)
res$pred
res$listnn


nlvdis &lt;- 5 ; diss &lt;- "mahal"
h &lt;- c(2, Inf)
k &lt;- c(10, 15)
nlv &lt;- c("1:3", "2:4")
pars &lt;- mpars(nlvdis = nlvdis, diss = diss,
              h = h, k = k, nlv = nlv)
pars

res &lt;- gridscore(
    Xtrain, ytrain, Xtest, ytest, 
    score = err, 
    fun = lwplsrda_agg, 
    pars = pars)
res

segm &lt;- segmkf(n = n, K = 3, nrep = 1)
res &lt;- gridcv(
    Xtrain, ytrain, 
    segm, score = err, 
    fun = lwplsrda_agg, 
    pars = pars,
    verb = TRUE)
names(res)
res$val


## KNN-LWPLSLDA-AGG

n &lt;- 40 ; p &lt;- 7
X &lt;- matrix(rnorm(n * p), ncol = p, byrow = TRUE)
y &lt;- sample(c(1, 4, 10), size = n, replace = TRUE)

Xtrain &lt;- X ; ytrain &lt;- y
m &lt;- 5
Xtest &lt;- X[1:m, ] ; ytest &lt;- y[1:m]

nlvdis &lt;- 5 ; diss &lt;- "mahal"
h &lt;- 2 ; k &lt;- 10
nlv &lt;- "2:4" 
fm &lt;- lwplslda_agg(
    Xtrain, ytrain, 
    nlvdis = nlvdis, diss = diss,
    h = h, k = k,
    nlv = nlv, prior = "prop")
res &lt;- predict(fm, Xtest)
res$pred
res$listnn

nlvdis &lt;- 5 ; diss &lt;- "mahal"
h &lt;- c(2, Inf)
k &lt;- c(10, 15)
nlv &lt;- c("1:3", "2:4")
pars &lt;- mpars(nlvdis = nlvdis, diss = diss,
              h = h, k = k, nlv = nlv, 
              prior = c("unif", "prop"))
pars

res &lt;- gridscore(
    Xtrain, ytrain, Xtest, ytest, 
    score = err, 
    fun = lwplslda_agg, 
    pars = pars)
res

segm &lt;- segmkf(n = n, K = 3, nrep = 1)
res &lt;- gridcv(
    Xtrain, ytrain, 
    segm, score = err, 
    fun = lwplslda_agg, 
    pars = pars,
    verb = TRUE)
names(res)
res$val

</code></pre>

<hr>
<h2 id='matW'>Between and within covariance matrices</h2><span id='topic+matW'></span><span id='topic+matB'></span>

<h3>Description</h3>

<p>Calculation of within (<code>matW</code>) and between (<code>matB</code>) covariance matrices for classes of observations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
matW(X, y)

matB(X, y)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="matW_+3A_x">X</code></td>
<td>
<p>Data (<code class="reqn">n, p</code>) on whch are calculated the covariances.</p>
</td></tr>
<tr><td><code id="matW_+3A_y">y</code></td>
<td>
<p>Class membership (<code class="reqn">n, 1</code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The denominator in the variance calculations is <code class="reqn">n</code>.</p>


<h3>Value</h3>

<p>For (<code>matW</code>):
</p>
<table>
<tr><td><code>W</code></td>
<td>
<p>within covariance matrix.</p>
</td></tr>
<tr><td><code>Wi</code></td>
<td>
<p>list of covariance matrices for each class.</p>
</td></tr>
<tr><td><code>lev</code></td>
<td>
<p>classes</p>
</td></tr>
<tr><td><code>ni</code></td>
<td>
<p>number of observations in each per class</p>
</td></tr>
</table>
<p>For (<code>matB</code>):
</p>
<table>
<tr><td><code>B</code></td>
<td>
<p>between covariance matrix.</p>
</td></tr>
<tr><td><code>ct</code></td>
<td>
<p>matrix of class centers.</p>
</td></tr>
<tr><td><code>lev</code></td>
<td>
<p>classes</p>
</td></tr>
<tr><td><code>ni</code></td>
<td>
<p>number of observations in each per class</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
n &lt;- 8 ; p &lt;- 3
X &lt;- matrix(rnorm(n * p), ncol = p)
y &lt;- sample(1:2, size = n, replace = TRUE)
X
y

matW(X, y)

matB(X, y)

matW(X, y)$W + matB(X, y)$B
(n - 1) / n * cov(X)

</code></pre>

<hr>
<h2 id='mavg'>Smoothing by moving average</h2><span id='topic+mavg'></span>

<h3>Description</h3>

<p>Smoothing, by moving average, of the row observations (e.g. spectra) of a dataset.</p>


<h3>Usage</h3>

<pre><code class='language-R'>mavg(X, n = 5)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mavg_+3A_x">X</code></td>
<td>
<p>X-data (<code class="reqn">n, p</code>).</p>
</td></tr>
<tr><td><code id="mavg_+3A_n">n</code></td>
<td>
<p>The number of points (i.e. columns of <code>X</code>) defining the window over wich is calculate each average. The smoothing is calculated for the point at the center of the window. Therefore, <code>n</code> must be an odd integer, and be higher or equal to 3.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix of the transformed data.</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(cassav)

X &lt;- cassav$Xtest
headm(X)

Xp &lt;- mavg(X, n = 11)
headm(Xp)

oldpar &lt;- par(mfrow = c(1, 1))
par(mfrow = c(1, 2))
plotsp(X, main = "Signal")
plotsp(Xp, main = "Corrected signal")
abline(h = 0, lty = 2, col = "grey")
par(oldpar)

</code></pre>

<hr>
<h2 id='mbplsr'>multi-block PLSR algorithms</h2><span id='topic+mbplsr'></span><span id='topic+transform.Mbplsr'></span><span id='topic+summary.Mbplsr'></span><span id='topic+coef.Mbplsr'></span><span id='topic+predict.Mbplsr'></span>

<h3>Description</h3>

<p>Algorithm fitting a multi-block PLS1 or PLS2 model between dependent variables <code class="reqn">Xlist</code> and responses <code class="reqn">Y</code>, based on the &quot;Improved kernel algorithm #1&quot; proposed by Dayal and MacGregor (1997). 
</p>
<p>For weighted versions, see for instance Schaal et al. 2002, Siccard &amp; Sabatier 2006, Kim et al. 2011 and Lesnoff et al. 2020.
</p>
<p><b>Auxiliary functions</b>
</p>
<p><code>transform</code> Calculates the LVs for any new matrix <code class="reqn">X</code> from the model.
</p>
<p><code>summary</code> returns summary information for the model.
</p>
<p><code>coef</code> Calculates b-coefficients from the model, adjuted for raw data.
</p>
<p><code>predict</code> Calculates the predictions for any new matrix <code class="reqn">X</code> from the model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
mbplsr(Xlist, Y, blockscaling = TRUE, weights = NULL, nlv, 
Xscaling = c("none", "pareto", "sd")[1], Yscaling = c("none", "pareto", "sd")[1])

## S3 method for class 'Mbplsr'
transform(object, X, ..., nlv = NULL)  

## S3 method for class 'Mbplsr'
summary(object, X, ...)  

## S3 method for class 'Mbplsr'
coef(object, ..., nlv = NULL) 

## S3 method for class 'Mbplsr'
predict(object, X, ..., nlv = NULL)  

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mbplsr_+3A_xlist">Xlist</code></td>
<td>
<p>For the main function: list of training X-data (<code class="reqn">n</code>rows).</p>
</td></tr>
<tr><td><code id="mbplsr_+3A_x">X</code></td>
<td>
<p>For the auxiliary functions: list of new X-data, with the same variables than the training X-data.</p>
</td></tr>
<tr><td><code id="mbplsr_+3A_y">Y</code></td>
<td>
<p>Training Y-data (<code class="reqn">n, q</code>).</p>
</td></tr>
<tr><td><code id="mbplsr_+3A_blockscaling">blockscaling</code></td>
<td>
<p>logical. If TRUE, the scaling factor (computed on the training) is the &quot;norm&quot; of the block, i.e. the square root of the sum of the variances of each column of the block.</p>
</td></tr>
<tr><td><code id="mbplsr_+3A_weights">weights</code></td>
<td>
<p>Weights (<code class="reqn">n, 1</code>) to apply to the training observations. Internally, weights are &quot;normalized&quot; to sum to 1. Default to <code>NULL</code> (weights are set to <code class="reqn">1 / n</code>).</p>
</td></tr>
<tr><td><code id="mbplsr_+3A_nlv">nlv</code></td>
<td>
<p>For the main functions: The number(s) of LVs to calculate. &mdash; For the auxiliary functions: The number(s) of LVs to consider.</p>
</td></tr>
<tr><td><code id="mbplsr_+3A_xscaling">Xscaling</code></td>
<td>
<p>vector (of length Xlist) of variable scaling for each datablock, among &quot;none&quot; (mean-centering only), &quot;pareto&quot; (mean-centering and pareto scaling), &quot;sd&quot; (mean-centering and unit variance scaling). If &quot;pareto&quot; or &quot;sd&quot;, uncorrected standard deviation is used.</p>
</td></tr>
<tr><td><code id="mbplsr_+3A_yscaling">Yscaling</code></td>
<td>
<p>character. variable scaling for the Y-block, among &quot;none&quot; (mean-centering only), &quot;pareto&quot; (mean-centering and pareto scaling), &quot;sd&quot; (mean-centering and unit variance scaling). If &quot;pareto&quot; or &quot;sd&quot;, uncorrected standard deviation is used.</p>
</td></tr>
<tr><td><code id="mbplsr_+3A_object">object</code></td>
<td>
<p>For the auxiliary functions: A fitted model, output of a call to the main functions.</p>
</td></tr>
<tr><td><code id="mbplsr_+3A_...">...</code></td>
<td>
<p>For the auxiliary functions: Optional arguments. Not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of outputs, such as
</p>
<table>
<tr><td><code>T</code></td>
<td>
<p>The X-score matrix (<code class="reqn">n, nlv</code>).</p>
</td></tr>
<tr><td><code>P</code></td>
<td>
<p>The X-loadings matrix (<code class="reqn">p, nlv</code>).</p>
</td></tr>
<tr><td><code>W</code></td>
<td>
<p>The X-loading weights matrix (<code class="reqn">p, nlv</code>).</p>
</td></tr>
<tr><td><code>C</code></td>
<td>
<p>The Y-loading weights matrix (C = t(Beta), where Beta is the scores regression coefficients matrix).</p>
</td></tr>
<tr><td><code>R</code></td>
<td>
<p>The PLS projection matrix (<code class="reqn">p, nlv</code>).</p>
</td></tr>
<tr><td><code>xmeans</code></td>
<td>
<p>The list of centering vectors of <code class="reqn">Xlist</code>.</p>
</td></tr>
<tr><td><code>ymeans</code></td>
<td>
<p>The centering vector of <code class="reqn">Y</code> (<code class="reqn">q, 1</code>).</p>
</td></tr>
<tr><td><code>xscales</code></td>
<td>
<p>The list of <code class="reqn">Xlist</code> variable standard deviations.</p>
</td></tr>
<tr><td><code>yscales</code></td>
<td>
<p>The vector of <code class="reqn">Y</code> variable standard deviations (<code class="reqn">q, 1</code>).</p>
</td></tr>
<tr><td><code>weights</code></td>
<td>
<p>Weights applied to the training observations.</p>
</td></tr>
<tr><td><code>TT</code></td>
<td>
<p>the X-score normalization factor.</p>
</td></tr>
<tr><td><code>blockscaling</code></td>
<td>
<p>block scaling.</p>
</td></tr>
<tr><td><code>Xnorms</code></td>
<td>
<p>&quot;norm&quot; of each block, i.e. the square root of the sum of the variances of each column of each block, computed on the training, and used as scaling factor</p>
</td></tr></table>
<p>.
</p>
<table>
<tr><td><code>U</code></td>
<td>
<p>intermediate output.</p>
</td></tr>
</table>
<p>For <code>transform.Mbplsr</code>: X-scores matrix for new Xlist-data.
</p>
<p>For <code>summary.Mbplsr</code>:
</p>
<table>
<tr><td><code>explvarx</code></td>
<td>
<p>matrix of explained variances.</p>
</td></tr>
</table>
<p>For <code>coef.Mbplsr</code>: 
</p>
<table>
<tr><td><code>int</code></td>
<td>
<p>matrix (1,nlv) with the intercepts</p>
</td></tr>
<tr><td><code>B</code></td>
<td>
<p>matrix (n,nlv) with the coefficients</p>
</td></tr>
</table>
<p>For <code>predict.Mbplsr</code>: 
</p>
<table>
<tr><td><code>pred</code></td>
<td>
<p>A list of matrices (<code class="reqn">m, q</code>) with the Y predicted values for the new Xlist-data</p>
</td></tr>
</table>


<h3>References</h3>

<p>Andersson, M., 2009. A comparison of nine PLS1 algorithms. Journal of Chemometrics 23, 518-529.
</p>
<p>Dayal, B.S., MacGregor, J.F., 1997. Improved PLS algorithms. Journal of Chemometrics 11, 73-85.
</p>
<p>Hoskuldsson, A., 1988. PLS regression methods. Journal of Chemometrics 2, 211-228. https://doi.org/10.1002/cem.1180020306
</p>
<p>Kim, S., Kano, M., Nakagawa, H., Hasebe, S., 2011. Estimation of active pharmaceutical ingredients content using locally weighted partial least squares and statistical wavelength selection. Int. J. Pharm., 421, 269-274.
</p>
<p>Lesnoff, M., Metz, M., Roger, J.M., 2020. Comparison of locally weighted PLS strategies for regression and discrimination on agronomic NIR Data. Journal of Chemometrics. e3209. https://onlinelibrary.wiley.com/doi/abs/10.1002/cem.3209
</p>
<p>Rannar, S., Lindgren, F., Geladi, P., Wold, S., 1994. A PLS kernel algorithm for data sets with many variables and fewer objects. Part 1: Theory and algorithm. Journal of Chemometrics 8, 111-125. https://doi.org/10.1002/cem.1180080204
</p>
<p>Schaal, S., Atkeson, C., Vijayamakumar, S. 2002. Scalable techniques from nonparametric statistics for the real time robot learning. Applied Intell., 17, 49-60.
</p>
<p>Sicard, E. Sabatier, R., 2006. Theoretical framework for local PLS1 regression and application to a rainfall data set. Comput. Stat. Data Anal., 51, 1393-1410.
</p>
<p>Tenenhaus, M., 1998. La régression PLS: théorie et pratique. Editions Technip, Paris, France.
</p>
<p>Wold, S., Sjostrom, M., Eriksson, l., 2001. PLS-regression: a basic tool for chemometrics. Chem. Int. Lab. Syst., 58, 109-130.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
n &lt;- 10 ; p &lt;- 10
Xtrain &lt;- matrix(rnorm(n * p), ncol = p)
ytrain &lt;- rnorm(n)

m &lt;- 2
Xtest &lt;- matrix(rnorm(m * p), ncol = p)

colnames(Xtrain) &lt;- colnames(Xtest) &lt;- paste("v", 1:p, sep = "")

Xtrain
Xtest

blocks &lt;- list(1:2, 4, 6:8)
X1 &lt;- mblocks(Xtrain, blocks = blocks)
X2 &lt;- mblocks(Xtest, blocks = blocks)

nlv &lt;- 3
fm &lt;- mbplsr(Xlist = X1, Y = ytrain, Xscaling = c("sd","none","none"), 
blockscaling = TRUE, weights = NULL, nlv = nlv)

summary(fm, X1)
coef(fm)
transform(fm, X2)
predict(fm, X2)

</code></pre>

<hr>
<h2 id='mbplsrda'>multi-block PLSDA models</h2><span id='topic+mbplsrda'></span><span id='topic+mbplslda'></span><span id='topic+mbplsqda'></span><span id='topic+predict.Mbplsrda'></span><span id='topic+predict.Mbplsprobda'></span>

<h3>Description</h3>

<p>Multi-block discrimination (DA) based on PLS.
</p>
<p>The training variable <code class="reqn">y</code> (univariate class membership) is firstly transformed to a dummy table containing <code class="reqn">nclas</code> columns, where <code class="reqn">nclas</code> is the number of classes present in <code class="reqn">y</code>. Each column is a dummy variable (0/1). Then, a PLS2 is implemented on the <code class="reqn">X-</code>data and the dummy table, returning latent variables (LVs) that are used as dependent variables in a DA model.
</p>
<p>- <code>mbplsrda</code>: Usual &quot;PLSDA&quot;. A linear regression model predicts the Y-dummy table from the PLS2 LVs. This corresponds to the PLSR2 of the X-data and the Y-dummy table. For a given observation, the final prediction is the class corresponding to the dummy variable for which the prediction is the highest.
</p>
<p>- <code>mbplslda</code> and <code>mbplsqda</code>: Probabilistic LDA and QDA are run over the PLS2 LVs, respectively.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
mbplsrda(Xlist, y, blockscaling = TRUE, weights = NULL, nlv, 
Xscaling = c("none", "pareto", "sd")[1], Yscaling = c("none", "pareto", "sd")[1])

mbplslda(Xlist, y, blockscaling = TRUE, weights = NULL, nlv, prior = c("unif", "prop"),
Xscaling = c("none", "pareto", "sd")[1], Yscaling = c("none", "pareto", "sd")[1])

mbplsqda(Xlist, y, blockscaling = TRUE, weights = NULL, nlv, prior = c("unif", "prop"),
Xscaling = c("none", "pareto", "sd")[1], Yscaling = c("none", "pareto", "sd")[1])

## S3 method for class 'Mbplsrda'
predict(object, X, ..., nlv = NULL) 

## S3 method for class 'Mbplsprobda'
predict(object, X, ..., nlv = NULL) 

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mbplsrda_+3A_xlist">Xlist</code></td>
<td>
<p>For the main functions: list of training X-data (<code class="reqn">n</code>rows).</p>
</td></tr>
<tr><td><code id="mbplsrda_+3A_x">X</code></td>
<td>
<p>For the auxiliary functions: list of new X-data (<code class="reqn">n</code> rows), with the same variables than the training X-data.</p>
</td></tr>
<tr><td><code id="mbplsrda_+3A_y">y</code></td>
<td>
<p>Training class membership (<code class="reqn">n</code>). <b>Note:</b> If <code>y</code> is a factor, it is replaced by a character vector.</p>
</td></tr>
<tr><td><code id="mbplsrda_+3A_blockscaling">blockscaling</code></td>
<td>
<p>logical. If TRUE, the scaling factor (computed on the training) is the &quot;norm&quot; of the block, i.e. the square root of the sum of the variances of each column of the block.</p>
</td></tr>
<tr><td><code id="mbplsrda_+3A_weights">weights</code></td>
<td>
<p>Weights (<code class="reqn">n</code>) to apply to the training observations for the PLS2. Internally, weights are &quot;normalized&quot; to sum to 1. Default to <code>NULL</code> (weights are set to <code class="reqn">1 / n</code>).</p>
</td></tr>
<tr><td><code id="mbplsrda_+3A_nlv">nlv</code></td>
<td>
<p>The number(s) of LVs to calculate.</p>
</td></tr>
<tr><td><code id="mbplsrda_+3A_prior">prior</code></td>
<td>
<p>The prior probabilities of the classes. Possible values are &quot;unif&quot; (default; probabilities are set equal for all the classes) or &quot;prop&quot; (probabilities are set equal to the observed proportions of the classes in <code>y</code>).</p>
</td></tr>
<tr><td><code id="mbplsrda_+3A_xscaling">Xscaling</code></td>
<td>
<p>vector (of length Xlist) of variable scaling for each datablock, among &quot;none&quot; (mean-centering only), &quot;pareto&quot; (mean-centering and pareto scaling), &quot;sd&quot; (mean-centering and unit variance scaling). If &quot;pareto&quot; or &quot;sd&quot;, uncorrected standard deviation is used.</p>
</td></tr>
<tr><td><code id="mbplsrda_+3A_yscaling">Yscaling</code></td>
<td>
<p>character. variable scaling for the Y-block after binary transformation, among &quot;none&quot; (mean-centering only), &quot;pareto&quot; (mean-centering and pareto scaling), &quot;sd&quot; (mean-centering and unit variance scaling). If &quot;pareto&quot; or &quot;sd&quot;, uncorrected standard deviation is used.</p>
</td></tr>
<tr><td><code id="mbplsrda_+3A_object">object</code></td>
<td>
<p>For the auxiliary functions: A fitted model, output of a call to the main functions.</p>
</td></tr>
<tr><td><code id="mbplsrda_+3A_...">...</code></td>
<td>
<p>For the auxiliary functions: Optional arguments. Not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>For <code>mbplsrda</code>:
</p>
<table>
<tr><td><code>fm</code></td>
<td>
<p>list with the MB-PLS model: (<code>T</code>): X-scores matrix; (<code>P</code>): X-loading matrix;(<code>R</code>): The PLS projection matrix (p,nlv); (<code>W</code>): X-loading weights matrix ;(<code>C</code>): The Y-loading weights matrix; (<code>TT</code>): the X-score normalization factor; (<code>xmeans</code>): the centering vector of X (p,1);  (<code>ymeans</code>): the centering vector of Y (q,1); (<code>weights</code>): vector of observation weights; (<code>blockscaling</code>): block scaling; (<code>Xnorms</code>): &quot;norm&quot; of each block; (<code>U</code>): intermediate output.</p>
</td></tr>
<tr><td><code>lev</code></td>
<td>
<p>classes</p>
</td></tr>
<tr><td><code>ni</code></td>
<td>
<p>number of observations in each class</p>
</td></tr>
</table>
<p>For <code>mbplslda</code>, <code>mbplsqda</code>:
</p>
<table>
<tr><td><code>fm</code></td>
<td>
<p>list with 
[[1]] the MB-PLS model: (<code>T</code>): X-scores matrix; (<code>P</code>): X-loading matrix;(<code>R</code>): The PLS projection matrix (p,nlv); (<code>W</code>): X-loading weights matrix ;(<code>C</code>): The Y-loading weights matrix; (<code>TT</code>): the X-score normalization factor; (<code>xmeans</code>): the centering vectors of X;  (<code>ymeans</code>): the centering vector of Y (q,1); (<code>xscales</code>): the scaling vector of X (p,1);  (<code>yscales</code>): the scaling vector of Y (q,1); (<code>weights</code>): vector of observation weights; (<code>blockscaling</code>): block scaling; (<code>Xnorms</code>): &quot;norm&quot; of each block; (<code>U</code>): intermediate output.
[[2]] lda or qda models.</p>
</td></tr>
<tr><td><code>lev</code></td>
<td>
<p>classes</p>
</td></tr>
<tr><td><code>ni</code></td>
<td>
<p>number of observations in each class</p>
</td></tr>
</table>
<p>For <code>predict.Mbplsrda</code>, <code>predict.Mbplsprobda</code>: 
</p>
<table>
<tr><td><code>pred</code></td>
<td>
<p>predicted class for each observation</p>
</td></tr>
<tr><td><code>posterior</code></td>
<td>
<p>calculated probability of belonging to a class for each observation</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The first example concerns MB-PLSDA, and the second one concerns MB-PLS LDA.
<code>fm</code> are PLS1 models, and <code>zfm</code> are PLS2 models.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## EXAMPLE OF MB-PLSDA

n &lt;- 50 ; p &lt;- 8
Xtrain &lt;- matrix(rnorm(n * p), ncol = p)
Xtrainlist &lt;- list(Xtrain[,1:3], Xtrain[,4:8])

ytrain &lt;- sample(c(1, 4, 10), size = n, replace = TRUE)

Xtest &lt;- Xtrain[1:5, ] ; ytest &lt;- ytrain[1:5]
Xtestlist &lt;- list(Xtest[,1:3], Xtest[,4:8])

nlv &lt;- 5
fm &lt;- mbplsrda(Xtrainlist, ytrain, Xscaling = "sd", nlv = nlv)
names(fm)

predict(fm, Xtestlist)
predict(fm, Xtestlist, nlv = 0:2)$pred

pred &lt;- predict(fm, Xtestlist)$pred
err(pred, ytest)

zfm &lt;- fm$fm
transform(zfm, Xtestlist)
transform(zfm, Xtestlist, nlv = 1)
summary(zfm, Xtrainlist)
coef(zfm)
coef(zfm, nlv = 0)
coef(zfm, nlv = 2)

## EXAMPLE OF MB-PLS LDA

n &lt;- 50 ; p &lt;- 8
Xtrain &lt;- matrix(rnorm(n * p), ncol = p)
Xtrainlist &lt;- list(Xtrain[,1:3], Xtrain[,4:8])

ytrain &lt;- sample(c(1, 4, 10), size = n, replace = TRUE)

Xtest &lt;- Xtrain[1:5, ] ; ytest &lt;- ytrain[1:5]
Xtestlist &lt;- list(Xtest[,1:3], Xtest[,4:8])

nlv &lt;- 5
fm &lt;- mbplslda(Xtrainlist, ytrain, Xscaling = "none", nlv = nlv)
predict(fm, Xtestlist)
predict(fm, Xtestlist, nlv = 1:2)$pred

zfm &lt;- fm[[1]][[1]]
class(zfm)
names(zfm)
summary(zfm, Xtrainlist)
transform(zfm, Xtestlist)
coef(zfm)

## EXAMPLE OF MB-PLS QDA

n &lt;- 50 ; p &lt;- 8
Xtrain &lt;- matrix(rnorm(n * p), ncol = p)
Xtrainlist &lt;- list(Xtrain[,1:3], Xtrain[,4:8])

ytrain &lt;- sample(c(1, 4, 10), size = n, replace = TRUE)

Xtest &lt;- Xtrain[1:5, ] ; ytest &lt;- ytrain[1:5]
Xtestlist &lt;- list(Xtest[,1:3], Xtest[,4:8])

nlv &lt;- 5
fm &lt;- mbplsqda(Xtrainlist, ytrain, Xscaling = "none", nlv = nlv)
predict(fm, Xtestlist)
predict(fm, Xtestlist, nlv = 1:2)$pred

zfm &lt;- fm[[1]][[1]]
class(zfm)
names(zfm)
summary(zfm, Xtrainlist)
transform(zfm, Xtestlist)
coef(zfm)

</code></pre>

<hr>
<h2 id='mse'>Residuals and prediction error rates</h2><span id='topic+mse'></span><span id='topic+msep'></span><span id='topic+rmsep'></span><span id='topic+sep'></span><span id='topic+bias'></span><span id='topic+cor2'></span><span id='topic+r2'></span><span id='topic+rpd'></span><span id='topic+rpdr'></span><span id='topic+err'></span><span id='topic+residreg'></span><span id='topic+residcla'></span>

<h3>Description</h3>

<p>Residuals and prediction error rates (MSEP, SEP, etc. or classification error rate)  for models with quantitative or qualitative responses. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
residreg(pred, Y)
residcla(pred, y)

msep(pred, Y)
rmsep(pred, Y)
sep(pred, Y)
bias(pred, Y)
cor2(pred, Y)
r2(pred, Y)
rpd(pred, Y)
rpdr(pred, Y)
mse(pred, Y, digits = 3)

err(pred, y)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mse_+3A_pred">pred</code></td>
<td>
<p>Prediction (<code class="reqn">m, q</code>); output of a function <code>predict</code>.</p>
</td></tr>
<tr><td><code id="mse_+3A_y">Y</code></td>
<td>
<p>Observed response (<code class="reqn">m, q</code>).</p>
</td></tr>
<tr><td><code id="mse_+3A_y">y</code></td>
<td>
<p>Observed response (<code class="reqn">m, 1</code>).</p>
</td></tr>
<tr><td><code id="mse_+3A_digits">digits</code></td>
<td>
<p>Number of digits for the numerical outputs.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The rate <code class="reqn">R2</code> is calculated by <code class="reqn">R2 = 1 - MSEP(current model) / MSEP(null model)</code>, where  <code class="reqn">MSEP = Sum((y_i - pred_i)^2)/n</code> and &quot;null model&quot; is the overall mean of <code class="reqn">y</code>. For predictions over CV or Test sets, and/or for non linear models, it can be different from the square of the correlation coefficient (<code class="reqn">cor2</code>) between the observed values and the predictions. 
</p>
<p>Function <code>sep</code> computes the SEP, referred to as &quot;corrected SEP&quot; (SEP_c) in Bellon et al. 2010. SEP is the standard deviation of the residuals. There is the relation: <code class="reqn">MSEP = BIAS^2 + SEP^2</code>.
</p>
<p>Function <code>rpd</code> computes the ratio of the &quot;deviation&quot; (sqrt of the mean of the squared residuals for the null model when it is defined by the simple average) to the &quot;performance&quot; (sqrt of the mean of the squared residuals for the current model, i.e. RMSEP), i.e. <code class="reqn">RPD = SD / RMSEP = RMSEP(null model) / RMSEP</code> (see eg. Bellon et al. 2010).
</p>
<p>Function <code>rpdr</code> computes a robust RPD. 
</p>


<h3>Value</h3>

<p>Residuals or prediction error rates.
</p>


<h3>References</h3>

<p>Bellon-Maurel, V., Fernandez-Ahumada, E., Palagos, B., Roger, J.-M., McBratney, A., 2010. Critical review of chemometric indicators commonly used for assessing the quality of the prediction of soil attributes by NIR spectroscopy. TrAC Trends in Analytical Chemistry 29, 1073-1081. https://doi.org/10.1016/j.trac.2010.05.006
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## EXAMPLE 1

n &lt;- 6 ; p &lt;- 4
Xtrain &lt;- matrix(rnorm(n * p), ncol = p)
ytrain &lt;- rnorm(n)
Ytrain &lt;- cbind(y1 = ytrain, y2 = 100 * ytrain)
m &lt;- 3
Xtest &lt;- Xtrain[1:m, , drop = FALSE] 
Ytest &lt;- Ytrain[1:m, , drop = FALSE] 
ytest &lt;- Ytest[1:m, 1]
nlv &lt;- 3
fm &lt;- plskern(Xtrain, Ytrain, nlv = nlv)
pred &lt;- predict(fm, Xtest)$pred

residreg(pred, Ytest)
msep(pred, Ytest)
rmsep(pred, Ytest)
sep(pred, Ytest)
bias(pred, Ytest)
cor2(pred, Ytest)
r2(pred, Ytest)
rpd(pred, Ytest)
rpdr(pred, Ytest)
mse(pred, Ytest, digits = 3)

## EXAMPLE 2

n &lt;- 50 ; p &lt;- 8
Xtrain &lt;- matrix(rnorm(n * p), ncol = p)
ytrain &lt;- sample(c(1, 4, 10), size = n, replace = TRUE)
Xtest &lt;- Xtrain[1:5, ]
ytest &lt;- ytrain[1:5]
nlv &lt;- 5
fm &lt;- plsrda(Xtrain, ytrain, nlv = nlv)
pred &lt;- predict(fm, Xtest)$pred

residcla(pred, ytest)
err(pred, ytest)

</code></pre>

<hr>
<h2 id='octane'>octane</h2><span id='topic+octane'></span>

<h3>Description</h3>

<p>Octane dataset.
</p>
<p>Near infrared (NIR) spectra (absorbance) of <code class="reqn">n</code> = 39 gasoline samples over <code class="reqn">p</code> = 226 wavelengths (1102 nm to 1552 nm, step = 2 nm).
</p>
<p>Samples 25, 26, and 36-39 contain added alcohol (outliers).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(octane)</code></pre>


<h3>Format</h3>

<p>A list with 1 component: the matrix <code class="reqn">X</code> with 39 samples and 226 variables.
</p>


<h3>Source</h3>

<p>K.H. Esbensen, S. Schoenkopf and T. Midtgaard Multivariate Analysis in Practice, Trondheim, Norway: Camo, 1994.
</p>
<p>Todorov, V. 2020. rrcov: Robust Location and Scatter Estimation and Robust Multivariate Analysis with High Breakdown. R Package version 1.5-5. https://cran.r-project.org/.
</p>


<h3>References</h3>

<p>M. Hubert, P. J. Rousseeuw, K. Vanden Branden (2005), ROBPCA: a new approach to robust principal components analysis, Technometrics, 47, 64-79.
</p>
<p>P. J. Rousseeuw, M. Debruyne, S. Engelen and M. Hubert (2006), Robustness and Outlier Detection in Chemometrics, Critical Reviews in Analytical Chemistry, 36(3-4), 221-242.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(octane)

X &lt;- octane$X
headm(X)

plotsp(X, xlab = "Wawelength", ylab = "Absorbance")
plotsp(X[c(25:26, 36:39), ], add = TRUE, col = "red")

</code></pre>

<hr>
<h2 id='odis'>Orthogonal distances from a PCA or PLS score space</h2><span id='topic+odis'></span><span id='topic+lodis'></span>

<h3>Description</h3>

<p><code>odis</code> calculates the orthogonal distances (OD = &quot;X-residuals&quot;) for a PCA or PLS model. OD is the Euclidean distance of a row observation to its projection to the score plan (see e.g. Hubert et al. 2005, Van Branden &amp; Hubert 2005, p. 66; Varmuza &amp; Filzmoser, 2009, p. 79).
</p>
<p>A distance cutoff is computed using a moment estimation of the parameters of a Chi-squared distribution for OD^2 (see Nomikos &amp; MacGregor 1995, and Pomerantzev 2008). In the function output, column <code>dstand</code>  is a standardized distance defined as <code class="reqn">OD / cutoff</code>. A value <code>dstand &gt; 1</code> can be considered as extreme.
</p>
<p>The cutoff for detecting extreme OD values is computed using a moment estimation of a Chi-squared distrbution for the squared distance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
odis(
    object, Xtrain, X = NULL, 
    nlv = NULL,
    rob = TRUE, alpha = .01
    )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="odis_+3A_object">object</code></td>
<td>
<p>A fitted model, output of a call to a fitting function.</p>
</td></tr>
<tr><td><code id="odis_+3A_xtrain">Xtrain</code></td>
<td>
<p>Training X-data that was used to fit the model.</p>
</td></tr>
<tr><td><code id="odis_+3A_x">X</code></td>
<td>
<p>New X-data.</p>
</td></tr>
<tr><td><code id="odis_+3A_nlv">nlv</code></td>
<td>
<p>Number of components (PCs or LVs) to consider.</p>
</td></tr>
<tr><td><code id="odis_+3A_rob">rob</code></td>
<td>
<p>Logical. If <code>TRUE</code>, the moment estimation of the distance cutoff is robustified. This can be recommended after robust PCA or PLS on small data sets containing extreme values.</p>
</td></tr>
<tr><td><code id="odis_+3A_alpha">alpha</code></td>
<td>
<p>Risk-<code class="reqn">I</code> level for defining the cutoff detecting extreme values.</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>res.train</code></td>
<td>
<p>matrix with distance and a standardized distance calculated for Xtrain.</p>
</td></tr>
<tr><td><code>res</code></td>
<td>
<p>matrix with distance and a standardized distance calculated for X.</p>
</td></tr>
<tr><td><code>cutoff</code></td>
<td>
<p>distance cutoff computed using a moment estimation of the parameters of a Chi-squared distribution for OD^2.</p>
</td></tr>
</table>


<h3>References</h3>

<p>M. Hubert, P. J. Rousseeuw, K. Vanden Branden (2005). ROBPCA: a new approach to robust principal components analysis. Technometrics, 47, 64-79.
</p>
<p>Nomikos, P., MacGregor, J.F., 1995. Multivariate SPC Charts for Monitoring Batch Processes. null 37, 41-59. https://doi.org/10.1080/00401706.1995.10485888
</p>
<p>Pomerantsev, A.L., 2008. Acceptance areas for multivariate classification derived by projection methods. Journal of Chemometrics 22, 601-609. https://doi.org/10.1002/cem.1147
</p>
<p>K. Vanden Branden, M. Hubert (2005). Robuts classification in high dimension based on the SIMCA method. Chem. Lab. Int. Syst, 79, 10-21. 
</p>
<p>K. Varmuza, P. Filzmoser (2009). Introduction to multivariate statistical analysis in chemometrics. CRC Press, Boca Raton.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
n &lt;- 6 ; p &lt;- 4
Xtrain &lt;- matrix(rnorm(n * p), ncol = p)
ytrain &lt;- rnorm(n)
Xtest &lt;- Xtrain[1:3, , drop = FALSE] 

nlv &lt;- 3
fm &lt;- pcasvd(Xtrain, nlv = nlv)
odis(fm, Xtrain)
odis(fm, Xtrain, nlv = 2)
odis(fm, Xtrain, X = Xtest, nlv = 2)

</code></pre>

<hr>
<h2 id='orthog'>Orthogonalization of a matrix to another matrix</h2><span id='topic+orthog'></span>

<h3>Description</h3>

<p>Function <code>orthog</code> orthogonalizes a matrix <code class="reqn">Y</code> to a matrix <code class="reqn">X</code>. The row observations can be weighted. 
</p>
<p>The function uses function <code><a href="stats.html#topic+lm">lm</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
orthog(X, Y, weights = NULL)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="orthog_+3A_x">X</code></td>
<td>
<p>A <code class="reqn">n x p</code> matrix or data frame.</p>
</td></tr>
<tr><td><code id="orthog_+3A_y">Y</code></td>
<td>
<p>A <code class="reqn">n x q</code> matrix or data frame to orthogonalize to <code class="reqn">X</code>.</p>
</td></tr>
<tr><td><code id="orthog_+3A_weights">weights</code></td>
<td>
<p>A vector of length <code class="reqn">n</code> defining a priori weights to apply to the observations. Internally, weights are &quot;normalized&quot; to sum to 1. Default to <code>NULL</code> (weights are set to <code class="reqn">1 / n</code>).</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>Y</code></td>
<td>
<p>The <code class="reqn">Y</code> matrix orthogonalized to <code class="reqn">X</code>.</p>
</td></tr>
<tr><td><code>b</code></td>
<td>
<p>The regression coefficients used for orthogonalization.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
n &lt;- 8 ; p &lt;- 3
set.seed(1)
X &lt;- matrix(rnorm(n * p, mean = 10), ncol = p, byrow = TRUE)
Y &lt;- matrix(rnorm(n * 2, mean = 10), ncol = 2, byrow = TRUE)
colnames(Y) &lt;- c("y1", "y2")
set.seed(NULL)
X
Y

res &lt;- orthog(X, Y)
res$Y
crossprod(res$Y, X)
res$b

# Same as:
fm &lt;- lm(Y ~ X)
Y - fm$fitted.values
fm$coef

#### WITH WEIGHTS

w &lt;- 1:n
fm &lt;- lm(Y ~ X, weights = w)
Y - fm$fitted.values
fm$coef

res &lt;- orthog(X, Y, weights = w)
res$Y
t(res$Y) 
res$b

</code></pre>

<hr>
<h2 id='ozone'>ozone</h2><span id='topic+ozone'></span>

<h3>Description</h3>

<p>Los Angeles ozone pollution data in 1976
(sources: Breiman &amp; Friedman 1985,  Leisch &amp;  Dimitriadou 2020).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(ozone)</code></pre>


<h3>Format</h3>

<p>A list with 1 component: the matrix <code>X</code> with 366 observations, 13 variables. The variable to predict is V4.
</p>

<dl>
<dt><code>V1</code></dt><dd><p>Month: 1 = January, ..., 12 = December</p>
</dd>
<dt><code>V2</code></dt><dd><p>Day of month</p>
</dd>
<dt><code>V3</code></dt><dd><p>Day of week: 1 = Monday, ..., 7 = Sunday</p>
</dd>
<dt><code>V4</code></dt><dd><p>Daily maximum one-hour-average ozone reading</p>
</dd>
<dt><code>V5</code></dt><dd><p>500 millibar pressure height (m) measured at Vandenberg AFB</p>
</dd>
<dt><code>V6</code></dt><dd><p>Wind speed (mph) at Los Angeles International Airport (LAX)</p>
</dd>
<dt><code>V7</code></dt><dd><p>Humidity (%) at LAX</p>
</dd>
<dt><code>V8</code></dt><dd><p>Temperature (degrees F) measured at Sandburg, CA</p>
</dd>
<dt><code>V9</code></dt><dd><p>Temperature (degrees F) measured at El Monte, CA</p>
</dd>
<dt><code>V10</code></dt><dd><p>Inversion base height (feet) at LAX</p>
</dd>
<dt><code>V11</code></dt><dd><p>Pressure gradient (mm Hg) from LAX to Daggett, CA</p>
</dd>
<dt><code>V12</code></dt><dd><p>Inversion base temperature (degrees F) at LAX</p>
</dd>
<dt><code>V13</code></dt><dd><p>Visibility (miles) measured at LAX</p>
</dd>
</dl>



<h3>Source</h3>

<p>Breiman L., Friedman J.H. 1985. Estimating optimal transformations for multiple regression and correlation, JASA, 80, pp. 580-598.
</p>
<p>Leisch, F. and Dimitriadou, E. (2010). mlbench: Machine Learning Benchmark Problems.
R package version 1.1-6. https://cran.r-project.org/.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(ozone)

z &lt;- ozone$X
head(z)

plotxna(z)

</code></pre>

<hr>
<h2 id='pcasvd'>PCA algorithms</h2><span id='topic+pcasvd'></span><span id='topic+pcaeigen'></span><span id='topic+pcaeigenk'></span><span id='topic+pcanipals'></span><span id='topic+pcanipalsna'></span><span id='topic+pcasph'></span><span id='topic+transform.Pca'></span><span id='topic+summary.Pca'></span>

<h3>Description</h3>

<p>Algorithms fitting a centered weighted PCA of a matrix <code class="reqn">X</code>. 
</p>
<p>Noting <code class="reqn">D</code> a (<code class="reqn">n, n</code>) diagonal matrix of weights for the observations (rows of <code class="reqn">X</code>), the functions consist in:
</p>
<p>- <code>pcasvd</code>: SVD factorization of <code class="reqn">D^(1/2) * X</code>, using function <code><a href="base.html#topic+svd">svd</a></code>. 
</p>
<p>- <code>pcaeigen</code>:Eigen factorization of <code class="reqn">X' * D * X</code>, using function <code><a href="base.html#topic+eigen">eigen</a></code>.
</p>
<p>- <code>pcaeigenk</code>: Eigen factorization of <code class="reqn">D^(1/2) * X * X' D^(1/2)</code>, using function <code><a href="base.html#topic+eigen">eigen</a></code>. This is the &quot;kernel cross-product trick&quot; version of the PCA algorithm (Wu et al. 1997). For wide matrices (<code class="reqn">n &lt;&lt; p</code>) and <code class="reqn">n</code> not too large, this algorithm can be much faster than the others. 
</p>
<p>- <code>pcanipals</code>: Eigen factorization of <code class="reqn">X' * D * X</code> using NIPALS. 
</p>
<p>- <code>pcanipalsna</code>: Eigen factorization of <code class="reqn">X' * D * X</code> using NIPALS allowing missing data in <code class="reqn">X</code>. 
</p>
<p>- <code>pcasph</code>: Robust spherical PCA (Locantore et al. 1990, Maronna 2005, Daszykowski et al. 2007). 
</p>
<p>Function <code>pcanipalsna</code> accepts missing data (<code>NA</code>s) in <code class="reqn">X</code>, unlike the other functions. The part of <code>pcanipalsna</code> accounting specifically for missing missing data is based on the efficient code of K. Wright in the R package <code>nipals</code> (https://cran.r-project.org/web/packages/nipals/index.html).
</p>
<p><b>Gram-Schmidt orthogonalization in the NIPALS algorithm</b>
</p>
<p>The PCA NIPALS is known to generate a loss of orthogonality of the PCs (due to the accumulation of rounding errors in the successive iterations), particularly for large matrices or with high degrees of column collinearity.
</p>
<p>With missing data, orthogonality of loadings is not satisfied neither.
</p>
<p>An approach for coming back to orthogonality (PCs and loadings) is the iterative classical Gram-Schmidt orthogonalization (Lingen 2000, Andrecut 2009, and vignette of R package <code>nipals</code>), referred to as the iterative CGS. It consists in adding a CGS orthorgonalization step in each iteration of the PCs and loadings calculations.
</p>
<p>For the case with missing data, the iterative CGS does not insure that the orthogonalized PCs are centered.
</p>
<p><b>Auxiliary function</b>
</p>
<p><code>transform</code> Calculates the PCs for any new matrix <code class="reqn">X</code> from the model.
</p>
<p><code>summary</code> returns summary information for the model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
pcasvd(X, weights = NULL, nlv)

pcaeigen(X, weights = NULL, nlv)

pcaeigenk(X,weights = NULL,  nlv)

pcanipals(X, weights = NULL, nlv,
    gs = TRUE, 
    tol = .Machine$double.eps^0.5, maxit = 200)

pcanipalsna(X, nlv, 
    gs = TRUE,
    tol = .Machine$double.eps^0.5, maxit = 200)
    
pcasph(X, weights = NULL, nlv)
  
## S3 method for class 'Pca'
transform(object, X, ..., nlv = NULL)  

## S3 method for class 'Pca'
summary(object, X, ...)  
  
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pcasvd_+3A_x">X</code></td>
<td>
<p>For the main functions and auxiliary function <code>summary</code>: Training X-data (<code class="reqn">n, p</code>). &mdash; For the other auxiliary functions: New X-data (<code class="reqn">m, p</code>) to consider.</p>
</td></tr>
<tr><td><code id="pcasvd_+3A_weights">weights</code></td>
<td>
<p>Weights (<code class="reqn">n, 1</code>) to apply to the training observations. Internally, weights are &quot;normalized&quot; to sum to 1. Default to <code>NULL</code> (weights are set to <code class="reqn">1 / n</code>).</p>
</td></tr>
<tr><td><code id="pcasvd_+3A_nlv">nlv</code></td>
<td>
<p>The number of PCs to calculate.</p>
</td></tr>
<tr><td><code id="pcasvd_+3A_object">object</code></td>
<td>
<p>A fitted model, output of a call to the main functions.</p>
</td></tr>
<tr><td><code id="pcasvd_+3A_...">...</code></td>
<td>
<p>Optional arguments.</p>
</td></tr>
</table>
<p><b>Specific for the NIPALS algorithm</b>
</p>
<table>
<tr><td><code id="pcasvd_+3A_gs">gs</code></td>
<td>
<p>Logical indicating if a Gram-Schmidt orthogonalization is implemented or not (default to <code>TRUE</code>).</p>
</td></tr>
<tr><td><code id="pcasvd_+3A_tol">tol</code></td>
<td>
<p>Tolerance for testing convergence of the NIPALS iterations for each PC.</p>
</td></tr>
<tr><td><code id="pcasvd_+3A_maxit">maxit</code></td>
<td>
<p>Maximum number of NIPALS iterations for each PC.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of outputs, such as:
</p>
<table>
<tr><td><code>T</code></td>
<td>
<p>The score matrix (<code class="reqn">n, nlv</code>).</p>
</td></tr>
<tr><td><code>P</code></td>
<td>
<p>The loadings matrix (<code class="reqn">p, nlv</code>).</p>
</td></tr>
<tr><td><code>R</code></td>
<td>
<p>The projection matrix (= <code class="reqn">P</code>) (<code class="reqn">p, nlv</code>).</p>
</td></tr>
<tr><td><code>sv</code></td>
<td>
<p>The singular values (<code class="reqn">min(n, p), 1</code>) except for NIPALS = (<code class="reqn">nlv, 1</code>).</p>
</td></tr>
<tr><td><code>eig</code></td>
<td>
<p>The eigenvalues (<code>= sv^2</code>) (<code class="reqn">min(n, p), 1</code>) except for NIPALS = (<code class="reqn">nlv, 1</code>).</p>
</td></tr>
<tr><td><code>xmeans</code></td>
<td>
<p>The centering vector of <code class="reqn">X</code> (<code class="reqn">p, 1</code>).</p>
</td></tr>
<tr><td><code>niter</code></td>
<td>
<p>Numbers of iterations of the NIPALS.</p>
</td></tr>
<tr><td><code>conv</code></td>
<td>
<p>Logical indicating if the NIPALS converged before reaching the maximal number of iterations.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Andrecut, M., 2009. Parallel GPU Implementation of Iterative PCA Algorithms. Journal of Computational Biology 16, 1593-1599. https://doi.org/10.1089/cmb.2008.0221
</p>
<p>Gabriel, R. K., 2002. Le biplot - Outil d\'exploration de données multidimensionnelles. Journal de la Société Française de la Statistique, 143, 5-55.
</p>
<p>Lingen, F.J., 2000. Efficient Gram-Schmidt orthonormalisation on parallel computers. Communications in Numerical Methods in Engineering 16, 57-66. https://doi.org/10.1002/(SICI)1099-0887(200001)16:1&lt;57::AID-CNM320&gt;3.0.CO;2-I
</p>
<p>Tenenhaus, M., 1998. La régression PLS: théorie et pratique. Editions Technip, Paris, France.
</p>
<p>Wright, K., 2018. Package nipals: Principal Components Analysis using NIPALS with Gram-Schmidt Orthogonalization. https://cran.r-project.org/
</p>
<p>Wu, W., Massart, D.L., de Jong, S., 1997. The kernel PCA algorithms for wide data. Part I: Theory and algorithms. Chemometrics and Intelligent Laboratory Systems 36, 165-172. https://doi.org/10.1016/S0169-7439(97)00010-5
</p>
<p>For Spherical PCA: 
</p>
<p>Daszykowski, M., Kaczmarek, K., Vander Heyden, Y., Walczak, B., 2007. 
Robust statistics in data analysis - A review. Chemometrics and Intelligent 
Laboratory Systems 85, 203-219. https://doi.org/10.1016/j.chemolab.2006.06.016
</p>
<p>Locantore N., Marron J.S., Simpson D.G., Tripoli N., Zhang J.T., Cohen K.L.
Robust principal component analysis for functional data, Test 8 (1999) 1-7
</p>
<p>Maronna, R., 2005. Principal components and orthogonal regression based on 
robust scales, Technometrics, 47:3, 264-273, DOI: 10.1198/004017005000000166
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
n &lt;- 6 ; p &lt;- 4
Xtrain &lt;- matrix(rnorm(n * p), nrow = n)
s &lt;- c(3, 4, 7, 10, 11, 15, 21:24)   
zX &lt;- replace(Xtrain, s, NA)
Xtrain
zX
m &lt;- 2
Xtest &lt;- matrix(rnorm(m * p), nrow = m)

pcasvd(Xtrain, nlv = 3)
pcaeigen(Xtrain, nlv = 3)
pcaeigenk(Xtrain, nlv = 3)
pcanipals(Xtrain, nlv = 3)
pcanipalsna(Xtrain, nlv = 3)
pcanipalsna(zX, nlv = 3)

fm &lt;- pcaeigen(Xtrain, nlv = 3)
fm$T
transform(fm, Xtest)
transform(fm, Xtest, nlv = 2)

pcaeigen(Xtrain, nlv = 3)$T
pcaeigen(Xtrain, nlv = 3, weights = 1:n)$T


Ttrain &lt;- fm$T
Ttest &lt;- transform(fm, Xtest)
T &lt;- rbind(Ttrain, Ttest)
group &lt;- c(rep("Training", nrow(Ttrain)), rep("Test", nrow(Ttest)))
i &lt;- 1
plotxy(T[, i:(i+1)], group = group, pch = 16, zeroes = TRUE, cex = 1.3, main = "scores")

plotxy(fm$P, zeroes = TRUE, label = TRUE, cex = 2, col = "red3", main ="loadings")

summary(fm, Xtrain)
res &lt;- summary(fm, Xtrain)
plotxy(res$cor.circle, zeroes = TRUE, label = TRUE, cex = 2, col = "red3",
    circle = TRUE, ylim = c(-1, 1))

</code></pre>

<hr>
<h2 id='pinv'>Moore-Penrose pseudo-inverse of a matrix</h2><span id='topic+pinv'></span>

<h3>Description</h3>

<p>Calculation of the Moore-Penrose (MP) pseudo-inverse of a matrix <code class="reqn">X</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
pinv(X, tol = sqrt(.Machine$double.eps))

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pinv_+3A_x">X</code></td>
<td>
<p>X-data (<code class="reqn">n, p</code>).</p>
</td></tr>
<tr><td><code id="pinv_+3A_tol">tol</code></td>
<td>
<p>A relative tolerance to detect zero singular values. </p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>Xplus</code></td>
<td>
<p>The MP pseudo-inverse.</p>
</td></tr>
<tr><td><code>sv</code></td>
<td>
<p>singular values.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
n &lt;- 7 ; p &lt;- 4
X &lt;- matrix(rnorm(n * p), ncol = p)
y &lt;- rnorm(n)

pinv(X)

tcrossprod(pinv(X)$Xplus, t(y))
lm(y ~ X - 1)

</code></pre>

<hr>
<h2 id='plotjit'>Jittered plot</h2><span id='topic+plotjit'></span>

<h3>Description</h3>

<p>Plot comparing classes with jittered points (random noise is added to the x-axis values for avoiding overplotting).</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotjit(x, y, group = NULL, 
  jit = 1, col = NULL, alpha.f = .8,
  legend = TRUE, legend.title = NULL, ncol = 1, med = TRUE,
  ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plotjit_+3A_x">x</code></td>
<td>
<p>A vector of length <code class="reqn">n</code> defining the class membership of the observations (x-axis).</p>
</td></tr>
<tr><td><code id="plotjit_+3A_y">y</code></td>
<td>
<p>A vector of length <code class="reqn">n</code> defining the variable to plot (y-axis).</p>
</td></tr>
<tr><td><code id="plotjit_+3A_group">group</code></td>
<td>
<p>A vector of length <code class="reqn">n</code> defining groups of observations to be plotted with different colors (default to <code>NULL</code>).</p>
</td></tr>
<tr><td><code id="plotjit_+3A_jit">jit</code></td>
<td>
<p>Scalar defining the jittering magnitude. Default to 1.</p>
</td></tr>
<tr><td><code id="plotjit_+3A_alpha.f">alpha.f</code></td>
<td>
<p>Scalar modifying the opacity of the points in the graphics; typically in [0,1]. See <code><a href="grDevices.html#topic+adjustcolor">adjustcolor</a></code>.</p>
</td></tr>
<tr><td><code id="plotjit_+3A_col">col</code></td>
<td>
<p>A color, or a vector of colors (of length equal to the number of classes or groups), defining the color(s) of the points.</p>
</td></tr>
<tr><td><code id="plotjit_+3A_legend">legend</code></td>
<td>
<p>Only if there are groups. Logical indicationg is a legend is drawn for groups (Default to <code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="plotjit_+3A_legend.title">legend.title</code></td>
<td>
<p>Character string indicationg a title for the legend.</p>
</td></tr>
<tr><td><code id="plotjit_+3A_ncol">ncol</code></td>
<td>
<p>Number of columns drawn in the legend box.</p>
</td></tr>
<tr><td><code id="plotjit_+3A_med">med</code></td>
<td>
<p>Logical. If <code>TRUE</code> (default), the median of each class is plotted.</p>
</td></tr>
<tr><td><code id="plotjit_+3A_...">...</code></td>
<td>
<p>Other arguments to pass in <code><a href="base.html#topic+plot">plot</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Jittered plot.</p>


<h3>Examples</h3>

<pre><code class='language-R'>
n &lt;- 500
x &lt;- c(rep("A", n), rep("B", n))
y &lt;- c(rnorm(n), rnorm(n, mean = 5, sd = 3))
group &lt;- sample(1:2, size = 2 * n, replace = TRUE)

plotjit(x, y, pch = 16, jit = .5, alpha.f = .5)

plotjit(x, y, pch = 16, jit = .5, alpha.f = .5,
  group = group)


</code></pre>

<hr>
<h2 id='plotscore'>Plotting errors rates</h2><span id='topic+plotscore'></span>

<h3>Description</h3>

<p>Plotting scores of prediction errors (error rates).</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotscore(x, y, group = NULL, 
    col = NULL, steplab = 2, legend = TRUE, legend.title = NULL, ncol = 1, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plotscore_+3A_x">x</code></td>
<td>
<p>Horizontal axis vector (<code class="reqn">n</code>).</p>
</td></tr>
<tr><td><code id="plotscore_+3A_y">y</code></td>
<td>
<p>Vertical axis vector (<code class="reqn">n</code>)</p>
</td></tr>
<tr><td><code id="plotscore_+3A_group">group</code></td>
<td>
<p>Groups of data (<code class="reqn">n</code>) to be plotted with different colors.</p>
</td></tr>
<tr><td><code id="plotscore_+3A_col">col</code></td>
<td>
<p>A color, or a vector of colors (of length equal to the number of groups), defining the color(s) of the groups.</p>
</td></tr>
<tr><td><code id="plotscore_+3A_steplab">steplab</code></td>
<td>
<p>A step for the horizontal axis. Can be <code>NULL</code> (automatic step).</p>
</td></tr>
<tr><td><code id="plotscore_+3A_legend">legend</code></td>
<td>
<p>Only if there are groups. Logical indicationg is a legend is drawn for groups (Default to <code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="plotscore_+3A_legend.title">legend.title</code></td>
<td>
<p>Character string indicationg a title for the legend.</p>
</td></tr>
<tr><td><code id="plotscore_+3A_ncol">ncol</code></td>
<td>
<p>Number of columns drawn in the legend box.</p>
</td></tr>
<tr><td><code id="plotscore_+3A_...">...</code></td>
<td>
<p>Other arguments to pass in function <code><a href="base.html#topic+plot">plot</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A plot.</p>


<h3>Examples</h3>

<pre><code class='language-R'>
n &lt;- 50 ; p &lt;- 20
Xtrain &lt;- matrix(rnorm(n * p), ncol = p, byrow = TRUE)
ytrain &lt;- rnorm(n)
Ytrain &lt;- cbind(ytrain, 10 * rnorm(n))
m &lt;- 3
Xtest &lt;- Xtrain[1:m, ] 
Ytest &lt;- Ytrain[1:m, ] ; ytest &lt;- Ytest[, 1]

nlv &lt;- 15
res &lt;- gridscorelv(
    Xtrain, ytrain, Xtest, ytest, 
    score = msep, 
    fun = plskern, 
    nlv = 0:nlv, verb = TRUE
    )
plotscore(res$nlv, res$y1, 
          main = "MSEP", xlab = "Nb. LVs", ylab = "Value")

nlvdis &lt;- 5
h &lt;- c(1, Inf)
k &lt;- c(10, 20)
nlv &lt;- 15
pars &lt;- mpars(nlvdis = nlvdis, diss = "mahal",
              h = h, k = k)
res &lt;- gridscorelv(
    Xtrain, Ytrain, Xtest, Ytest, 
    score = msep, 
    fun = lwplsr, 
    nlv = 0:nlv, pars = pars, verb = TRUE)
headm(res)
group &lt;- paste("h=", res$h, " k=", res$k, sep = "")
plotscore(res$nlv, res$y1, group = group,
          main = "MSEP", xlab = "Nb. LVs", ylab = "Value")

</code></pre>

<hr>
<h2 id='plotsp'>Plotting spectra</h2><span id='topic+plotsp'></span><span id='topic+plotsp1'></span>

<h3>Description</h3>

<p><code>plotsp</code> plots lines corresponding to the row observations (e.g. spectra) of a data set.
</p>
<p><code>plotsp1</code> plots only one observation per plot (e.g. spectrum by spectrum) by scrolling the rows. After running a <code>plotsp1</code> command, the plots are printed successively by pushing the R console &quot;entry button&quot;, and stopped by entering any character in the R console.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
plotsp(X,
  type = "l", col = NULL, zeroes = FALSE, labels = FALSE, 
  add = FALSE,
  ...)

plotsp1(X, col = NULL, zeroes = FALSE, ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plotsp_+3A_x">X</code></td>
<td>
<p>Data (<code class="reqn">n, p</code>) to plot.</p>
</td></tr>
<tr><td><code id="plotsp_+3A_type">type</code></td>
<td>
<p>1-character string giving the type of plot desired. Default value to <code>"l"</code> (lines). See <code><a href="graphics.html#topic+plot.default">plot.default</a></code> for other options.</p>
</td></tr>
<tr><td><code id="plotsp_+3A_col">col</code></td>
<td>
<p>A color, or a vector of colors (of length n), defining the color(s) of the lines representing the rows.</p>
</td></tr>
<tr><td><code id="plotsp_+3A_zeroes">zeroes</code></td>
<td>
<p>Logical indicationg if an horizontal line is drawn at coordonates (0, 0) (Default to <code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="plotsp_+3A_labels">labels</code></td>
<td>
<p>Logical indicating if the row names of <code>X</code> are plotted (default to <code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="plotsp_+3A_add">add</code></td>
<td>
<p>Logical defining if the frame of the plot is plotted (<code>add = FALSE</code>; default) or not (<code>add = TRUE</code>). This allows to add new observations to a plot without red-building the frame.</p>
</td></tr>
<tr><td><code id="plotsp_+3A_...">...</code></td>
<td>
<p>Other arguments to pass in functions <code><a href="base.html#topic+plot">plot</a></code> or <code><a href="graphics.html#topic+lines">lines</a></code></p>
</td></tr></table>
<p>.
</p>


<h3>Value</h3>

<p>A plot (see examples).
</p>


<h3>Note</h3>

<p>For the first example, see ?hcl.colors and ?hcl.pals, and try with
col &lt;- hcl.colors(n = n, alpha = 1, rev = FALSE, palette = &quot;Green-Orange&quot;)
col &lt;- terrain.colors(n, rev = FALSE)
col &lt;- rainbow(n, rev = FALSE, alpha = .2)
</p>
<p>The second example is with plotsp1 (Scrolling plot of PCA loadings).
After running the code, type Enter in the R console for starting the scrolling, and type any character in the R console 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## EXAMPLE 1

data(cassav)

X &lt;- cassav$Xtest
n &lt;- nrow(X)

plotsp(X)
plotsp(X, col = "grey")
plotsp(X, col = "lightblue", 
  xlim = c(500, 1500),
  xlab = "Wawelength (nm)", ylab = "Absorbance")

col &lt;- hcl.colors(n = n, alpha = 1, rev = FALSE, palette = "Grays")
plotsp(X, col = col)

plotsp(X, col = "grey")
plotsp(X[23, , drop = FALSE], lwd = 2, add = TRUE)
plotsp(X[c(23, 16), ], lwd = 2, add = TRUE)

plotsp(X[5, , drop = FALSE], labels = TRUE)

plotsp(X[c(5, 61), ], labels = TRUE)

col &lt;- hcl.colors(n = n, alpha = 1, rev = FALSE, palette = "Grays")
plotsp(X, col = col)
plotsp(X[5, , drop = FALSE], col = "red", lwd = 2, add = TRUE, labels = TRUE)

## EXAMPLE 2 (Scrolling plot of PCA loadings)

data(cassav)
X &lt;- cassav$Xtest
fm &lt;- pcaeigenk(X, nlv = 20)
P &lt;- fm$P

plotsp1(t(P), ylab = "Value")

</code></pre>

<hr>
<h2 id='plotxna'>Plotting Missing Data in a Matrix</h2><span id='topic+plotxna'></span>

<h3>Description</h3>

<p>Plot the location of missing data in a matrix.</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotxna(X, pch = 16, col = "red", grid = FALSE, asp = 0, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plotxna_+3A_x">X</code></td>
<td>
<p>A data set (<code class="reqn">n x p</code>).</p>
</td></tr>
<tr><td><code id="plotxna_+3A_pch">pch</code></td>
<td>
<p>Type of point. See <code><a href="graphics.html#topic+points">points</a></code>.</p>
</td></tr>
<tr><td><code id="plotxna_+3A_col">col</code></td>
<td>
<p>A color defining the color of the points.</p>
</td></tr>
<tr><td><code id="plotxna_+3A_grid">grid</code></td>
<td>
<p>Logical. If <code>TRUE</code>, a grid is plotted for representing the matrix rows an columns. Default to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="plotxna_+3A_asp">asp</code></td>
<td>
<p>Scalar. Giving the aspect ratio y/x. The value <code>asp = 0</code> is the default in <code><a href="graphics.html#topic+plot.default">plot.default</a></code> (no constraints on the ratio). See <code><a href="graphics.html#topic+plot.default">plot.default</a></code>.</p>
</td></tr>
<tr><td><code id="plotxna_+3A_...">...</code></td>
<td>
<p>Other arguments to pass in functions <code><a href="base.html#topic+plot">plot</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A plot.</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(octane)
X &lt;- octane$X
n &lt;- nrow(X)
p &lt;- ncol(X)
N &lt;- n * p

s &lt;- sample(1:N, size = 50)
zX &lt;- replace(X, s, NA)
plotxna(zX)
plotxna(zX, grid = TRUE, asp = 0)

</code></pre>

<hr>
<h2 id='plotxy'>2-d scatter plot</h2><span id='topic+plotxy'></span>

<h3>Description</h3>

<p>2-dimension scatter plot.</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotxy(X, group = NULL, 
    asp = 0, col = NULL, alpha.f = .8,
    zeroes = FALSE, circle = FALSE, ellipse = FALSE,
    labels = FALSE,
    legend = TRUE, legend.title = NULL, ncol = 1,
    ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plotxy_+3A_x">X</code></td>
<td>
<p>Data (<code class="reqn">n, p</code>) to plot. If <code class="reqn">p &gt; 2</code>, only the first two columns are considered.</p>
</td></tr>
<tr><td><code id="plotxy_+3A_group">group</code></td>
<td>
<p>Groups of observations (<code class="reqn">n</code>) to be plotted with different colors (default to <code>NULL</code>).</p>
</td></tr>
<tr><td><code id="plotxy_+3A_asp">asp</code></td>
<td>
<p>Scalar. Giving the aspect ratio y/x. The value <code>asp = 0</code> is the default in <code><a href="graphics.html#topic+plot.default">plot.default</a></code> (no constraints on the ratio). See <code><a href="graphics.html#topic+plot.default">plot.default</a></code>.</p>
</td></tr>
<tr><td><code id="plotxy_+3A_col">col</code></td>
<td>
<p>A color, or a vector of colors (of length equal to the number of groups), defining the color(s) of the groups.</p>
</td></tr>
<tr><td><code id="plotxy_+3A_alpha.f">alpha.f</code></td>
<td>
<p>Scalar modifying the opacity of the points in the graphics; typically in [0,1]. See <code><a href="grDevices.html#topic+adjustcolor">adjustcolor</a></code>.</p>
</td></tr>
<tr><td><code id="plotxy_+3A_zeroes">zeroes</code></td>
<td>
<p>Logical indicationg if an horizontal and vertical lines are drawn at coordonates (0, 0) (Default to <code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="plotxy_+3A_circle">circle</code></td>
<td>
<p>Not still working. Logical indicating if a correlation circle is plotted  (default to <code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="plotxy_+3A_ellipse">ellipse</code></td>
<td>
<p>Logical indicating if a Gaussian  ellipse is plotted (default to <code>FALSE</code>). If there are groups, an ellipse is drawn for each group.</p>
</td></tr>
<tr><td><code id="plotxy_+3A_labels">labels</code></td>
<td>
<p>Logical indicating if the row names of <code>X</code> (instead of points) are plotted (default to <code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="plotxy_+3A_legend">legend</code></td>
<td>
<p>Only if there are groups. Logical indicationg is a legend is drawn for groups (Default to <code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="plotxy_+3A_legend.title">legend.title</code></td>
<td>
<p>Character string indicationg a title for the legend.</p>
</td></tr>
<tr><td><code id="plotxy_+3A_ncol">ncol</code></td>
<td>
<p>Number of columns drawn in the legend box.</p>
</td></tr>
<tr><td><code id="plotxy_+3A_...">...</code></td>
<td>
<p>Other arguments to pass in functions <code><a href="base.html#topic+plot">plot</a></code>, <code><a href="graphics.html#topic+points">points</a></code>, <code><a href="graphics.html#topic+axis">axis</a></code> and <code><a href="graphics.html#topic+text">text</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A plot.</p>


<h3>Examples</h3>

<pre><code class='language-R'>
n &lt;- 50 ; p &lt;- 10
Xtrain &lt;- matrix(rnorm(n * p), ncol = p)
Xtest &lt;- Xtrain[1:5, ] + .4 

fm &lt;- pcaeigen(Xtrain, nlv = 5)
Ttrain &lt;- fm$T
Ttest &lt;- transform(fm, Xtest)
T &lt;- rbind(Ttrain, Ttest)
group &lt;- c(rep("Training", nrow(Ttrain)), rep("Test", nrow(Ttest)))
i &lt;- 1
plotxy(T[, i:(i+1)], group = group, 
       pch = 16, zeroes = TRUE,
       main = "PCA")
       
plotxy(T[, i:(i+1)], group = group, 
       pch = 16, zeroes = TRUE, asp = 1,
       main = "PCA")

</code></pre>

<hr>
<h2 id='plskern'>PLSR algorithms</h2><span id='topic+plskern'></span><span id='topic+plsnipals'></span><span id='topic+plsrannar'></span><span id='topic+transform.Plsr'></span><span id='topic+summary.Plsr'></span><span id='topic+coef.Plsr'></span><span id='topic+predict.Plsr'></span>

<h3>Description</h3>

<p>Algorithms fitting a PLS1 or PLS2 model between dependent variables <code class="reqn">X</code> and responses <code class="reqn">Y</code>.
</p>
<p>- <code>plskern</code>: &quot;Improved kernel algorithm #1&quot; proposed by Dayal and MacGregor (1997). This algorithm is stable and fast (Andersson 2009), and returns the same results as the NIPALS. 
</p>
<p>- <code>plsnipals</code>: NIPALS algorithm (e.g. Tenenhaus 1998, Wold 2002). In the function, the usual PLS2 NIPALS iterative is replaced by a direct calculation of the weights vector <code class="reqn">w</code> by SVD decomposition of matrix <code class="reqn">X'Y</code> (Hoskuldsson 1988 p.213).  
</p>
<p>- <code>plsrannar</code>: Kernel algorithm proposed by Rannar et al. (1994) for &quot;wide&quot; matrices, i.e.  with low number of rows and very large number of columns (p &gt;&gt; n; e.g. p = 20000). In such a situation, this algorithm is faster than the others (but it becomes much slower in other situations). If the algorithm converges, it returns the same results as the NIPALS (Note: discrepancies can be observed if too many PLS components are requested compared to the low number of observations).
</p>
<p>For weighted versions, see for instance Schaal et al. 2002, Siccard &amp; Sabatier 2006, Kim et al. 2011 and Lesnoff et al. 2020.
</p>
<p><b>Auxiliary functions</b>
</p>
<p><code>transform</code> Calculates the LVs for any new matrix <code class="reqn">X</code> from the model.
</p>
<p><code>summary</code> returns summary information for the model.
</p>
<p><code>coef</code> Calculates b-coefficients from the model.
</p>
<p><code>predict</code> Calculates the predictions for any new matrix <code class="reqn">X</code> from the model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
plskern(X, Y, weights = NULL, nlv, 
Xscaling = c("none", "pareto", "sd")[1], Yscaling = c("none", "pareto", "sd")[1])

plsnipals(X, Y, weights = NULL, nlv, 
Xscaling = c("none", "pareto", "sd")[1], Yscaling = c("none", "pareto", "sd")[1])

plsrannar(X, Y, weights = NULL, nlv, 
Xscaling = c("none", "pareto", "sd")[1], Yscaling = c("none", "pareto", "sd")[1])

## S3 method for class 'Plsr'
transform(object, X, ..., nlv = NULL)  

## S3 method for class 'Plsr'
summary(object, X, ...)  

## S3 method for class 'Plsr'
coef(object, ..., nlv = NULL) 

## S3 method for class 'Plsr'
predict(object, X, ..., nlv = NULL)  

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plskern_+3A_x">X</code></td>
<td>
<p>For the main functions: Training X-data (<code class="reqn">n, p</code>). &mdash; For the auxiliary functions: Training X-data (<code class="reqn">n, p</code>).</p>
</td></tr>
<tr><td><code id="plskern_+3A_y">Y</code></td>
<td>
<p>Training Y-data (<code class="reqn">n, q</code>).</p>
</td></tr>
<tr><td><code id="plskern_+3A_weights">weights</code></td>
<td>
<p>Weights (<code class="reqn">n, 1</code>) to apply to the training observations. Internally, weights are &quot;normalized&quot; to sum to 1. Default to <code>NULL</code> (weights are set to <code class="reqn">1 / n</code>).</p>
</td></tr>
<tr><td><code id="plskern_+3A_nlv">nlv</code></td>
<td>
<p>For the main functions: The number(s) of LVs to calculate. &mdash; For the auxiliary functions: The number(s) of LVs to consider.</p>
</td></tr>
<tr><td><code id="plskern_+3A_xscaling">Xscaling</code></td>
<td>
<p>X variable scaling among &quot;none&quot; (mean-centering only), &quot;pareto&quot; (mean-centering and pareto scaling), &quot;sd&quot; (mean-centering and unit variance scaling). If &quot;pareto&quot; or &quot;sd&quot;, uncorrected standard deviation is used.</p>
</td></tr>
<tr><td><code id="plskern_+3A_yscaling">Yscaling</code></td>
<td>
<p>Y variable scaling among &quot;none&quot; (mean-centering only), &quot;pareto&quot; (mean-centering and pareto scaling), &quot;sd&quot; (mean-centering and unit variance scaling). If &quot;pareto&quot; or &quot;sd&quot;, uncorrected standard deviation is used.</p>
</td></tr>
<tr><td><code id="plskern_+3A_object">object</code></td>
<td>
<p>For the auxiliary functions: A fitted model, output of a call to the main functions.</p>
</td></tr>
<tr><td><code id="plskern_+3A_...">...</code></td>
<td>
<p>For the auxiliary functions: Optional arguments. Not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>For <code>plskern</code>, <code>plsnipals</code>, <code>plsrannar</code>: A list of outputs, such as
</p>
<table>
<tr><td><code>T</code></td>
<td>
<p>The X-score matrix (<code class="reqn">n, nlv</code>).</p>
</td></tr>
<tr><td><code>P</code></td>
<td>
<p>The X-loadings matrix (<code class="reqn">p, nlv</code>).</p>
</td></tr>
<tr><td><code>W</code></td>
<td>
<p>The X-loading weights matrix (<code class="reqn">p, nlv</code>).</p>
</td></tr>
<tr><td><code>C</code></td>
<td>
<p>The Y-loading weights matrix (C = t(Beta), where Beta is the scores regression coefficients matrix).</p>
</td></tr>
<tr><td><code>R</code></td>
<td>
<p>The PLS projection matrix (<code class="reqn">p, nlv</code>).</p>
</td></tr>
<tr><td><code>xmeans</code></td>
<td>
<p>The centering vector of <code class="reqn">X</code> (<code class="reqn">p, 1</code>).</p>
</td></tr>
<tr><td><code>ymeans</code></td>
<td>
<p>The centering vector of <code class="reqn">Y</code> (<code class="reqn">q, 1</code>).</p>
</td></tr>
<tr><td><code>xscales</code></td>
<td>
<p>The vector of <code class="reqn">X</code> variable standard deviations (<code class="reqn">p, 1</code>).</p>
</td></tr>
<tr><td><code>yscales</code></td>
<td>
<p>The vector of <code class="reqn">Y</code> variable standard deviations (<code class="reqn">q, 1</code>).</p>
</td></tr>
<tr><td><code>weights</code></td>
<td>
<p>Weights applied to the training observations.</p>
</td></tr>
<tr><td><code>TT</code></td>
<td>
<p>the X-score normalization factor.</p>
</td></tr>
<tr><td><code>U</code></td>
<td>
<p>intermediate output.</p>
</td></tr>
</table>
<p>For <code>transform.Plsr</code>: X-scores matrix for new X-data.
</p>
<p>For <code>summary.Plsr</code>:
</p>
<table>
<tr><td><code>explvarx</code></td>
<td>
<p>matrix of explained variances.</p>
</td></tr>
</table>
<p>For <code>coef.Plsr</code>: 
</p>
<table>
<tr><td><code>int</code></td>
<td>
<p>matrix (1,nlv) with the intercepts</p>
</td></tr>
<tr><td><code>B</code></td>
<td>
<p>matrix (n,nlv) with the coefficients</p>
</td></tr>
</table>
<p>For <code>predict.Plsr</code>: 
</p>
<table>
<tr><td><code>pred</code></td>
<td>
<p>A list of matrices (<code class="reqn">m, q</code>) with the Y predicted values for the new X-data</p>
</td></tr>
</table>


<h3>References</h3>

<p>Andersson, M., 2009. A comparison of nine PLS1 algorithms. Journal of Chemometrics 23, 518-529.
</p>
<p>Dayal, B.S., MacGregor, J.F., 1997. Improved PLS algorithms. Journal of Chemometrics 11, 73-85.
</p>
<p>Hoskuldsson, A., 1988. PLS regression methods. Journal of Chemometrics 2, 211-228. https://doi.org/10.1002/cem.1180020306
</p>
<p>Kim, S., Kano, M., Nakagawa, H., Hasebe, S., 2011. Estimation of active pharmaceutical ingredients content using locally weighted partial least squares and statistical wavelength selection. Int. J. Pharm., 421, 269-274.
</p>
<p>Lesnoff, M., Metz, M., Roger, J.M., 2020. Comparison of locally weighted PLS strategies for regression and discrimination on agronomic NIR Data. Journal of Chemometrics. e3209. https://onlinelibrary.wiley.com/doi/abs/10.1002/cem.3209
</p>
<p>Rannar, S., Lindgren, F., Geladi, P., Wold, S., 1994. A PLS kernel algorithm for data sets with many variables and fewer objects. Part 1: Theory and algorithm. Journal of Chemometrics 8, 111-125. https://doi.org/10.1002/cem.1180080204
</p>
<p>Schaal, S., Atkeson, C., Vijayamakumar, S. 2002. Scalable techniques from nonparametric statistics for the real time robot learning. Applied Intell., 17, 49-60.
</p>
<p>Sicard, E. Sabatier, R., 2006. Theoretical framework for local PLS1 regression and application to a rainfall data set. Comput. Stat. Data Anal., 51, 1393-1410.
</p>
<p>Tenenhaus, M., 1998. La régression PLS: théorie et pratique. Editions Technip, Paris, France.
</p>
<p>Wold, S., Sjostrom, M., Eriksson, l., 2001. PLS-regression: a basic tool for chemometrics. Chem. Int. Lab. Syst., 58, 109-130.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
n &lt;- 6 ; p &lt;- 4
Xtrain &lt;- matrix(rnorm(n * p), ncol = p)
ytrain &lt;- rnorm(n)
Ytrain &lt;- cbind(y1 = ytrain, y2 = 100 * ytrain)
m &lt;- 3
Xtest &lt;- Xtrain[1:m, , drop = FALSE] 
Ytest &lt;- Ytrain[1:m, , drop = FALSE] ; ytest &lt;- Ytest[1:m, 1]

nlv &lt;- 3
plskern(Xtrain, Ytrain, Xscaling = "sd", nlv = nlv)
plsnipals(Xtrain, Ytrain, Xscaling = "sd", nlv = nlv)
plsrannar(Xtrain, Ytrain, Xscaling = "sd", nlv = nlv)

plskern(Xtrain, Ytrain, Xscaling = "none", nlv = nlv)
plskern(Xtrain, Ytrain, nlv = nlv)$T
plskern(Xtrain, Ytrain, nlv = nlv, weights = 1:n)$T

fm &lt;- plskern(Xtrain, Ytrain, nlv = nlv)
coef(fm)
coef(fm, nlv = 0)
coef(fm, nlv = 1)

fm$T
transform(fm, Xtest)
transform(fm, Xtest, nlv = 1)

summary(fm, Xtrain)

predict(fm, Xtest)
predict(fm, Xtest, nlv = 0:3)

pred &lt;- predict(fm, Xtest)$pred
msep(pred, Ytest)

</code></pre>

<hr>
<h2 id='plsr_agg'>PLSR with aggregation of latent variables</h2><span id='topic+plsr_agg'></span><span id='topic+predict.Plsr_agg'></span>

<h3>Description</h3>

<p>Ensemblist approach where the predictions are calculated by averaging the predictions of PLSR models (<code><a href="#topic+plskern">plskern</a></code>) built with different numbers of latent variables (LVs). 
</p>
<p>For instance, if argument <code>nlv</code> is set to <code>nlv = "5:10"</code>, the prediction for a new observation is the average (without weighting) of the predictions returned by the models with 5 LVS, 6 LVs, ... 10 LVs.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
plsr_agg(X, Y, weights = NULL, nlv)

## S3 method for class 'Plsr_agg'
predict(object, X, ...)  

</code></pre>


<h3>Arguments</h3>

<p>For <code>plsr_agg</code>:
</p>
<table>
<tr><td><code id="plsr_agg_+3A_x">X</code></td>
<td>
<p>For the main function: Training X-data (<code class="reqn">n, p</code>). &mdash; For the auxiliary function: New X-data (<code class="reqn">m, p</code>) to consider.</p>
</td></tr>
<tr><td><code id="plsr_agg_+3A_y">Y</code></td>
<td>
<p>Training Y-data (<code class="reqn">n, q</code>).</p>
</td></tr>
<tr><td><code id="plsr_agg_+3A_weights">weights</code></td>
<td>
<p>Weights (<code class="reqn">n, 1</code>) to apply to the training observations. Internally, weights are &quot;normalized&quot; to sum to 1. Default to <code>NULL</code> (weights are set to <code class="reqn">1 / n</code>).</p>
</td></tr>
<tr><td><code id="plsr_agg_+3A_nlv">nlv</code></td>
<td>
<p>A character string such as &quot;5:20&quot; defining the range of the numbers of LVs to consider (here: the models with nb LVS = 5, 6, ..., 20 are averaged). Syntax such as &quot;10&quot; is also allowed (here: correponds to the single model with 10 LVs).</p>
</td></tr>
<tr><td><code id="plsr_agg_+3A_object">object</code></td>
<td>
<p>For the auxiliary function: A fitted model, output of a call to the main functions.</p>
</td></tr>
<tr><td><code id="plsr_agg_+3A_...">...</code></td>
<td>
<p>For the auxiliary function: Optional arguments. Not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>For <code>plsr_agg</code>:
</p>
<table>
<tr><td><code>fm</code></td>
<td>
<p>list contaning the model: (<code>fm</code>)=(<code>T</code>): X-scores matrix; (<code>P</code>): X-loading matrix;(<code>R</code>): The PLS projection matrix (p,nlv); (<code>W</code>): X-loading weights matrix ;(<code>C</code>): The Y-loading weights matrix; (<code>TT</code>): the X-score normalization factor; (<code>xmeans</code>): the centering vector of X (p,1);  (<code>ymeans</code>): the centering vector of Y (q,1); (<code>weights</code>): vector of observation weights; (<code>U</code>): intermediate output.</p>
</td></tr>
<tr><td><code>nlv</code></td>
<td>
<p>range of the numbers of LVs considered</p>
</td></tr>
</table>
<p>For <code>predict.Plsr_agg</code>:
</p>
<table>
<tr><td><code>pred</code></td>
<td>
<p>Final predictions (after aggregation)</p>
</td></tr>
<tr><td><code>predlv</code></td>
<td>
<p>Intermediate predictions (Per nb. LVs)</p>
</td></tr>
</table>


<h3>Note</h3>

<p>In the example, <code>zfm</code> is the maximal PLSR model, and there is no sense to use gridscorelv or gridcvlv instead of gridscore or gridcv. 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
n &lt;- 20 ; p &lt;- 4
Xtrain &lt;- matrix(rnorm(n * p), ncol = p)
ytrain &lt;- rnorm(n)
Ytrain &lt;- cbind(y1 = ytrain, y2 = 100 * ytrain)
m &lt;- 3
Xtest &lt;- Xtrain[1:m, , drop = FALSE] 
Ytest &lt;- Ytrain[1:m, , drop = FALSE] ; ytest &lt;- Ytest[1:m, 1]

nlv &lt;- "1:3"

fm &lt;- plsr_agg(Xtrain, ytrain, nlv = nlv)
names(fm)

zfm &lt;- fm$fm
class(zfm)
names(zfm)
summary(zfm, Xtrain)


res &lt;- predict(fm, Xtest)
names(res)

res$pred
msep(res$pred, ytest)

res$predlv

pars &lt;- mpars(nlv = c("1:3", "2:5"))
pars
res &lt;- gridscore(
    Xtrain, Ytrain, Xtest, Ytest, 
    score = msep, 
    fun = plsr_agg, 
    pars = pars)
res

K = 3
segm &lt;- segmkf(n = n, K = K, nrep = 1)
segm
res &lt;- gridcv(
    Xtrain, Ytrain, 
    segm, score = msep, 
    fun = plsr_agg, 
    pars = pars,
    verb = TRUE)
res

</code></pre>

<hr>
<h2 id='plsrda'>PLSDA models</h2><span id='topic+plsrda'></span><span id='topic+plslda'></span><span id='topic+plsqda'></span><span id='topic+predict.Plsrda'></span><span id='topic+predict.Plsprobda'></span>

<h3>Description</h3>

<p>Discrimination (DA) based on PLS.
</p>
<p>The training variable <code class="reqn">y</code> (univariate class membership) is firstly transformed to a dummy table containing <code class="reqn">nclas</code> columns, where <code class="reqn">nclas</code> is the number of classes present in <code class="reqn">y</code>. Each column is a dummy variable (0/1). Then, a PLS2 is implemented on the <code class="reqn">X-</code>data and the dummy table, returning latent variables (LVs) that are used as dependent variables in a DA model.
</p>
<p>- <code>plsrda</code>: Usual &quot;PLSDA&quot;. A linear regression model predicts the Y-dummy table from the PLS2 LVs. This corresponds to the PLSR2 of the X-data and the Y-dummy table. For a given observation, the final prediction is the class corresponding to the dummy variable for which the prediction is the highest.
</p>
<p>- <code>plslda</code> and <code>plsqda</code>: Probabilistic LDA and QDA are run over the PLS2 LVs, respectively.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
plsrda(X, y, weights = NULL, nlv, 
Xscaling = c("none","pareto","sd")[1], Yscaling = c("none","pareto","sd")[1])

plslda(X, y, weights = NULL, nlv, prior = c("unif", "prop"), 
Xscaling = c("none","pareto","sd")[1], Yscaling = c("none","pareto","sd")[1])

plsqda(X, y, weights = NULL, nlv, prior = c("unif", "prop"), 
Xscaling = c("none","pareto","sd")[1], Yscaling = c("none","pareto","sd")[1])

## S3 method for class 'Plsrda'
predict(object, X, ..., nlv = NULL) 

## S3 method for class 'Plsprobda'
predict(object, X, ..., nlv = NULL) 

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plsrda_+3A_x">X</code></td>
<td>
<p>For the main functions: Training X-data (<code class="reqn">n, p</code>). &mdash; For the auxiliary functions: New X-data (<code class="reqn">m, p</code>) to consider.</p>
</td></tr>
<tr><td><code id="plsrda_+3A_y">y</code></td>
<td>
<p>Training class membership (<code class="reqn">n</code>). <b>Note:</b> If <code>y</code> is a factor, it is replaced by a character vector.</p>
</td></tr>
<tr><td><code id="plsrda_+3A_weights">weights</code></td>
<td>
<p>Weights (<code class="reqn">n</code>) to apply to the training observations for the PLS2. Internally, weights are &quot;normalized&quot; to sum to 1. Default to <code>NULL</code> (weights are set to <code class="reqn">1 / n</code>).</p>
</td></tr>
<tr><td><code id="plsrda_+3A_nlv">nlv</code></td>
<td>
<p>The number(s) of LVs to calculate.</p>
</td></tr>
<tr><td><code id="plsrda_+3A_prior">prior</code></td>
<td>
<p>The prior probabilities of the classes. Possible values are &quot;unif&quot; (default; probabilities are set equal for all the classes) or &quot;prop&quot; (probabilities are set equal to the observed proportions of the classes in <code>y</code>).</p>
</td></tr>
<tr><td><code id="plsrda_+3A_xscaling">Xscaling</code></td>
<td>
<p>X variable scaling among &quot;none&quot; (mean-centering only), &quot;pareto&quot; (mean-centering and pareto scaling), &quot;sd&quot; (mean-centering and unit variance scaling). If &quot;pareto&quot; or &quot;sd&quot;, uncorrected standard deviation is used.</p>
</td></tr>
<tr><td><code id="plsrda_+3A_yscaling">Yscaling</code></td>
<td>
<p>Y variable scaling, once converted to binary variables, among &quot;none&quot; (mean-centering only), &quot;pareto&quot; (mean-centering and pareto scaling), &quot;sd&quot; (mean-centering and unit variance scaling). If &quot;pareto&quot; or &quot;sd&quot;, uncorrected standard deviation is used.</p>
</td></tr>
<tr><td><code id="plsrda_+3A_object">object</code></td>
<td>
<p>For the auxiliary functions: A fitted model, output of a call to the main functions.</p>
</td></tr>
<tr><td><code id="plsrda_+3A_...">...</code></td>
<td>
<p>For the auxiliary functions: Optional arguments. Not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>For <code>plsrda</code>, <code>plslda</code>, <code>plsqda</code>:
</p>
<table>
<tr><td><code>fm</code></td>
<td>
<p>list with the model: (<code>T</code>): X-scores matrix; (<code>P</code>): X-loading matrix;(<code>R</code>): The PLS projection matrix (p,nlv); (<code>W</code>): X-loading weights matrix ;(<code>C</code>): The Y-loading weights matrix; (<code>TT</code>): the X-score normalization factor; (<code>xmeans</code>): the centering vector of X (p,1);  (<code>ymeans</code>): the centering vector of Y (q,1); (<code>xscales</code>): the scaling vector of X (p,1);  (<code>yscales</code>): the scaling vector of Y (q,1); (<code>weights</code>): vector of observation weights; (<code>U</code>): intermediate output.</p>
</td></tr>
<tr><td><code>lev</code></td>
<td>
<p>classes</p>
</td></tr>
<tr><td><code>ni</code></td>
<td>
<p>number of observations in each class</p>
</td></tr>
</table>
<p>For <code>predict.Plsrda</code>, <code>predict.Plsprobda</code>: 
</p>
<table>
<tr><td><code>pred</code></td>
<td>
<p>predicted class for each observation</p>
</td></tr>
<tr><td><code>posterior</code></td>
<td>
<p>calculated probability of belonging to a class for each observation</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The first example concerns PLSDA, and the second one concerns PLS LDA.
<code>fm</code> are PLS1 models, and <code>zfm</code> are PLS2 models to predict the disjunctive matrix.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## EXAMPLE OF PLSDA

n &lt;- 50 ; p &lt;- 8
Xtrain &lt;- matrix(rnorm(n * p), ncol = p)
ytrain &lt;- sample(c(1, 4, 10), size = n, replace = TRUE)

Xtest &lt;- Xtrain[1:5, ] ; ytest &lt;- ytrain[1:5]

nlv &lt;- 5
fm &lt;- plsrda(Xtrain, ytrain, Xscaling = "sd", nlv = nlv)
names(fm)

predict(fm, Xtest)
predict(fm, Xtest, nlv = 0:2)$pred

pred &lt;- predict(fm, Xtest)$pred
err(pred, ytest)

zfm &lt;- fm$fm
transform(zfm, Xtest)
transform(zfm, Xtest, nlv = 1)
summary(zfm, Xtrain)
coef(zfm)
coef(zfm, nlv = 0)
coef(zfm, nlv = 2)

## EXAMPLE OF PLS LDA

n &lt;- 50 ; p &lt;- 8
Xtrain &lt;- matrix(rnorm(n * p), ncol = p)
ytrain &lt;- sample(c(1, 4, 10), size = n, replace = TRUE)
Xtest &lt;- Xtrain[1:5, ] ; ytest &lt;- ytrain[1:5]

nlv &lt;- 5
fm &lt;- plslda(Xtrain, ytrain, Xscaling = "sd", nlv = nlv)
predict(fm, Xtest)
predict(fm, Xtest, nlv = 1:2)$pred

zfm &lt;- fm$fm[[1]]
class(zfm)
names(zfm)
summary(zfm, Xtrain)
transform(zfm, Xtest[1:2, ])
coef(zfm)

</code></pre>

<hr>
<h2 id='plsrda_agg'>PLSDA with aggregation of latent variables</h2><span id='topic+plsrda_agg'></span><span id='topic+plslda_agg'></span><span id='topic+plsqda_agg'></span><span id='topic+predict.Plsda_agg'></span>

<h3>Description</h3>

<p>Ensemblist approach where the predictions are calculated by &quot;averaging&quot; the predictions of PLSDA models built with different numbers of latent variables (LVs). 
</p>
<p>For instance, if argument <code>nlv</code> is set to <code>nlv = "5:10"</code>, the prediction for a new observation is the most occurent level (vote) over the predictions returned by the models with 5 LVS, 6 LVs, ... 10 LVs.
</p>
<p>- <code>plsrda_agg</code>: use <code><a href="#topic+plsrda">plsrda</a></code>.
</p>
<p>- <code>plslda_agg</code>: use <code><a href="#topic+plslda">plslda</a></code>.
</p>
<p>- <code>plsqda_agg</code>: use <code><a href="#topic+plsqda">plsqda</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
plsrda_agg(X, y, weights = NULL, nlv)

plslda_agg(X, y, weights = NULL, nlv, prior = c("unif", "prop"))

plsqda_agg(X, y, weights = NULL, nlv, prior = c("unif", "prop"))

## S3 method for class 'Plsda_agg'
predict(object, X, ...)  

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plsrda_agg_+3A_x">X</code></td>
<td>
<p>For the main functions: Training X-data (<code class="reqn">n, p</code>). &mdash; For the auxiliary function: New X-data (<code class="reqn">m, p</code>) to consider.</p>
</td></tr>
<tr><td><code id="plsrda_agg_+3A_y">y</code></td>
<td>
<p>Training class membership (<code class="reqn">n</code>). <b>Note:</b> If <code>y</code> is a factor, it is replaced by a character vector.</p>
</td></tr>
<tr><td><code id="plsrda_agg_+3A_weights">weights</code></td>
<td>
<p>Weights (<code class="reqn">n, 1</code>) to apply to the training observations. Internally, weights are &quot;normalized&quot; to sum to 1. Default to <code>NULL</code> (weights are set to <code class="reqn">1 / n</code>).</p>
</td></tr>
<tr><td><code id="plsrda_agg_+3A_nlv">nlv</code></td>
<td>
<p>A character string such as &quot;5:20&quot; defining the range of the numbers of LVs to consider (here: the models with nb LVS = 5, 6, ..., 20 are averaged). Syntax such as &quot;10&quot; is also allowed (here: correponds to the single model with 10 LVs).</p>
</td></tr>
<tr><td><code id="plsrda_agg_+3A_prior">prior</code></td>
<td>
<p>The prior probabilities of the classes. Possible values are &quot;unif&quot; (default; probabilities are set equal for all the classes) or &quot;prop&quot; (probabilities are set equal to the observed proportions of the classes in <code>y</code>).</p>
</td></tr>
<tr><td><code id="plsrda_agg_+3A_object">object</code></td>
<td>
<p>For the auxiliary function: A fitted model, output of a call to the main functions.</p>
</td></tr>
<tr><td><code id="plsrda_agg_+3A_...">...</code></td>
<td>
<p>For the auxiliary function: Optional arguments. Not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>For <code>plsrda_agg</code>, <code>plslda_agg</code> and <code>plsqda_agg</code>:
</p>
<table>
<tr><td><code>fm</code></td>
<td>
<p>list contaning: 
the model((<code>fm</code>)=(<code>T</code>): X-scores matrix; (<code>P</code>): X-loading matrix;(<code>R</code>): The PLS projection matrix (p,nlv); (<code>W</code>): X-loading weights matrix ;(<code>C</code>): The Y-loading weights matrix; (<code>TT</code>): the X-score normalization factor; (<code>xmeans</code>): the centering vector of X (p,1);  (<code>ymeans</code>): the centering vector of Y (q,1); (<code>weights</code>): vector of observation weights; (<code>U</code>): intermediate output),  (<code>lev</code>):classes, (<code>ni</code>):number of observations in each class</p>
</td></tr>
<tr><td><code>nlv</code></td>
<td>
<p>range of the numbers of LVs considered</p>
</td></tr>
</table>
<p>For <code>predict.Plsda_agg</code>:
</p>
<table>
<tr><td><code>pred</code></td>
<td>
<p>Final predictions (after aggregation)</p>
</td></tr>
<tr><td><code>predlv</code></td>
<td>
<p>Intermediate predictions (Per nb. LVs)</p>
</td></tr>
</table>


<h3>Note</h3>

<p>the first example concerns PLSRDA-AGG, and the second one concerns PLSLDA-AGG.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## EXAMPLE OF PLSRDA-AGG

n &lt;- 50 ; p &lt;- 8
Xtrain &lt;- matrix(rnorm(n * p), ncol = p)
ytrain &lt;- sample(c(1, 4, 10, 2), size = n, replace = TRUE)

m &lt;- 5
Xtest &lt;- Xtrain[1:m, ] ; ytest &lt;- ytrain[1:m]

nlv &lt;- "2:5"
fm &lt;- plsrda_agg(Xtrain, ytrain, nlv = nlv)
names(fm)
res &lt;- predict(fm, Xtest)
names(res)
res$pred
err(res$pred, ytest)
res$predlv

pars &lt;- mpars(nlv = c("1:3", "2:5"))
pars

res &lt;- gridscore(
    Xtrain, ytrain, Xtest, ytest, 
    score = err, 
    fun = plsrda_agg, 
    pars = pars)
res

segm &lt;- segmkf(n = n, K = 3, nrep = 1)
res &lt;- gridcv(
    Xtrain, ytrain, 
    segm, score = err, 
    fun = plslda_agg, 
    pars = pars,
    verb = TRUE)
res

## EXAMPLE OF PLSLDA-AGG

n &lt;- 50 ; p &lt;- 8
Xtrain &lt;- matrix(rnorm(n * p), ncol = p)
ytrain &lt;- sample(c(1, 4, 10, 2), size = n, replace = TRUE)
#ytrain &lt;- sample(c("a", "10", "d"), size = n, replace = TRUE)
m &lt;- 5
Xtest &lt;- Xtrain[1:m, ] ; ytest &lt;- ytrain[1:m]

nlv &lt;- "2:5"
fm &lt;- plslda_agg(Xtrain, ytrain, nlv = nlv, prior = "unif")
names(fm)
res &lt;- predict(fm, Xtest)
names(res)
res$pred
err(res$pred, ytest)
res$predlv

pars &lt;- mpars(nlv = c("1:3", "2:5"), prior = c("unif", "prop"))
pars
res &lt;- gridscore(
    Xtrain, ytrain, Xtest, ytest, 
    score = err, 
    fun = plslda_agg, 
    pars = pars)
res

segm &lt;- segmkf(n = n, K = 3, nrep = 1)
res &lt;- gridcv(
    Xtrain, ytrain, 
    segm, score = err, 
    fun = plslda_agg, 
    pars = pars,
    verb = TRUE)
res

</code></pre>

<hr>
<h2 id='rmgap'>Removing vertical gaps in spectra</h2><span id='topic+rmgap'></span>

<h3>Description</h3>

<p>Remove the vertical gaps in spectra (rows of matrix <code class="reqn">X</code>), e.g. for ASD. This is done by extrapolation from simple linear regressions computed on the left side of the gaps.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rmgap(X, indexcol, k = 5)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rmgap_+3A_x">X</code></td>
<td>
<p>A dataset.</p>
</td></tr>
<tr><td><code id="rmgap_+3A_indexcol">indexcol</code></td>
<td>
<p>The column indexes corresponding to the gaps. For instance, if two gaps are observed between indexes 651-652 and between indexes 1451-1452, respectively, then <code>indexcol = c(651, 1451)</code>.</p>
</td></tr>
<tr><td><code id="rmgap_+3A_k">k</code></td>
<td>
<p>The number of columns used on the left side of the gaps for fitting the linear regressions.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The corrected data <code class="reqn">X</code>.</p>


<h3>Note</h3>

<p>In the example, two gaps are at wavelengths 1000-1001 nm and 1800-1801 nm.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(asdgap)
X &lt;- asdgap$X

indexcol &lt;- which(colnames(X) == "1000" | colnames(X) == "1800")
indexcol
plotsp(X, lwd = 1.5)
abline(v = as.numeric(colnames(X)[1]) + indexcol - 1, col = "lightgrey", lty = 3)

zX &lt;- rmgap(X, indexcol = indexcol)
plotsp(zX, lwd = 1.5)
abline(v = as.numeric(colnames(zX)[1]) + indexcol - 1, col = "lightgrey", lty = 3)

</code></pre>

<hr>
<h2 id='rr'>Linear Ridge Regression</h2><span id='topic+rr'></span><span id='topic+coef.Rr'></span><span id='topic+predict.Rr'></span>

<h3>Description</h3>

<p>Fitting linear ridge regression models (RR) (Hoerl &amp; Kennard 1970, Hastie &amp; Tibshirani 2004, Hastie et al 2009, Cule &amp; De Iorio 2012) by SVD factorization.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
rr(X, Y, weights = NULL, lb = 1e-2)

## S3 method for class 'Rr'
coef(object, ..., lb = NULL)  

## S3 method for class 'Rr'
predict(object, X, ..., lb = NULL)  

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rr_+3A_x">X</code></td>
<td>
<p>For the main function: Training X-data (<code class="reqn">n, p</code>). &mdash; For the auxiliary functions: New X-data (<code class="reqn">m, p</code>) to consider.</p>
</td></tr>
<tr><td><code id="rr_+3A_y">Y</code></td>
<td>
<p>Training Y-data (<code class="reqn">n, q</code>).</p>
</td></tr>
<tr><td><code id="rr_+3A_weights">weights</code></td>
<td>
<p>Weights (<code class="reqn">n, 1</code>) to apply to the training observations. Internally, weights are &quot;normalized&quot; to sum to 1. Default to <code>NULL</code> (weights are set to <code class="reqn">1 / n</code>).</p>
</td></tr>
<tr><td><code id="rr_+3A_lb">lb</code></td>
<td>
<p>A value of regularization parameter <code class="reqn">lambda</code>. If <code>lb = 0</code>, a pseudo-inverse is used.</p>
</td></tr>
<tr><td><code id="rr_+3A_object">object</code></td>
<td>
<p>For the auxiliary functions: A fitted model, output of a call to the main function.</p>
</td></tr>
<tr><td><code id="rr_+3A_...">...</code></td>
<td>
<p>&mdash; For the auxiliary functions: Optional arguments. Not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>For <code>rr</code>:
</p>
<table>
<tr><td><code>V</code></td>
<td>
<p>eigenvector matrix of the correlation matrix (n,n).</p>
</td></tr>
<tr><td><code>TtDY</code></td>
<td>
<p>intermediate output.</p>
</td></tr>
<tr><td><code>sv</code></td>
<td>
<p>singular values of the matrix <code class="reqn">(1,n)</code>.</p>
</td></tr>
<tr><td><code>lb</code></td>
<td>
<p>value of regularization parameter <code class="reqn">lambda</code>.</p>
</td></tr>
<tr><td><code>xmeans</code></td>
<td>
<p>the centering vector of X <code class="reqn">(p,1)</code>.</p>
</td></tr>
<tr><td><code>ymeans</code></td>
<td>
<p>the centering vector of Y <code class="reqn">(q,1)</code>.</p>
</td></tr>
<tr><td><code>weights</code></td>
<td>
<p>the weights vector of X-variables <code class="reqn">(p,1)</code>.</p>
</td></tr>
</table>
<p>For <code>coef.Rr</code>:
</p>
<table>
<tr><td><code>int</code></td>
<td>
<p>matrix (1,nlv) with the intercepts</p>
</td></tr>
<tr><td><code>B</code></td>
<td>
<p>matrix (n,nlv) with the coefficients</p>
</td></tr>
<tr><td><code>df</code></td>
<td>
<p>model complexity (number of degrees of freedom)</p>
</td></tr>
</table>
<p>For <code>predict.Rr</code>:
</p>
<table>
<tr><td><code>pred</code></td>
<td>
<p>A list of matrices (<code class="reqn">m, q</code>) with the Y predicted values for the new X-data</p>
</td></tr>
</table>


<h3>References</h3>

<p>Cule, E., De Iorio, M., 2012. A semi-automatic method to guide the choice of ridge parameter in ridge regression. arXiv:1205.0686.
</p>
<p>Hastie, T., Tibshirani, R., 2004. Efficient quadratic regularization for expression arrays. Biostatistics 5, 329-340. https://doi.org/10.1093/biostatistics/kxh010
</p>
<p>Hastie, T., Tibshirani, R., Friedman, J., 2009. The elements of statistical learning: data mining, inference, and prediction, 2nd ed. Springer, New York.
</p>
<p>Hoerl, A.E., Kennard, R.W., 1970. Ridge Regression: Biased Estimation for Nonorthogonal Problems. Technometrics 12, 55-67. https://doi.org/10.1080/00401706.1970.10488634
</p>
<p>Wu, W., Massart, D.L., de Jong, S., 1997. The kernel PCA algorithms for wide data. Part I: Theory and algorithms. Chemometrics and Intelligent Laboratory Systems 36, 165-172. https://doi.org/10.1016/S0169-7439(97)00010-5
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
n &lt;- 6 ; p &lt;- 4
Xtrain &lt;- matrix(rnorm(n * p), ncol = p)
ytrain &lt;- rnorm(n)
Ytrain &lt;- cbind(y1 = ytrain, y2 = 100 * ytrain)
m &lt;- 3
Xtest &lt;- Xtrain[1:m, , drop = FALSE] 
Ytest &lt;- Ytrain[1:m, , drop = FALSE] ; ytest &lt;- Ytest[1:m, 1]

lb &lt;- .1
fm &lt;- rr(Xtrain, Ytrain, lb = lb)
coef(fm)
coef(fm, lb = .8)
predict(fm, Xtest)
predict(fm, Xtest, lb = c(0.1, .8))

pred &lt;- predict(fm, Xtest)$pred
msep(pred, Ytest)

</code></pre>

<hr>
<h2 id='rrda'>RR-DA models</h2><span id='topic+rrda'></span><span id='topic+predict.Rrda'></span>

<h3>Description</h3>

<p>Discrimination (DA) based on ridge regression (RR).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
rrda(X, y, weights = NULL, lb = 1e-5)

## S3 method for class 'Rrda'
predict(object, X, ..., lb = NULL) 

</code></pre>


<h3>Arguments</h3>

<p>For <code>rrda</code>:
</p>
<table>
<tr><td><code id="rrda_+3A_x">X</code></td>
<td>
<p>For the main function: Training X-data (<code class="reqn">n, p</code>). &mdash; For the auxiliary function: New X-data (<code class="reqn">m, p</code>) to consider.</p>
</td></tr>
<tr><td><code id="rrda_+3A_y">y</code></td>
<td>
<p>Training class membership (<code class="reqn">n</code>). <b>Note:</b> If <code>y</code> is a factor, it is replaced by a character vector.</p>
</td></tr>
<tr><td><code id="rrda_+3A_weights">weights</code></td>
<td>
<p>Weights (<code class="reqn">n</code>) to apply to the training observations for the PLS2. Internally, weights are &quot;normalized&quot; to sum to 1. Default to <code>NULL</code> (weights are set to <code class="reqn">1 / n</code>).</p>
</td></tr>
<tr><td><code id="rrda_+3A_lb">lb</code></td>
<td>
<p>A value of regularization parameter <code class="reqn">lambda</code>. If <code>lb = 0</code>, a pseudo-inverse is used in the RR.</p>
</td></tr>
<tr><td><code id="rrda_+3A_object">object</code></td>
<td>
<p>For the auxiliary function: A fitted model, output of a call to the main functions.</p>
</td></tr>
<tr><td><code id="rrda_+3A_...">...</code></td>
<td>
<p>For the auxiliary function: Optional arguments. Not used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The training variable <code class="reqn">y</code> (univariate class membership) is transformed to a dummy table containing <code class="reqn">nclas</code> columns, where <code class="reqn">nclas</code> is the number of classes present in <code class="reqn">y</code>. Each column is a dummy variable (0/1). Then, a ridge regression (RR) is run on the <code class="reqn">X-</code>data and the dummy table, returning predictions of the dummy variables. For a given observation, the final prediction is the class corresponding to the dummy variable for which the prediction is the highest.
</p>


<h3>Value</h3>

<p>For <code>rrda</code>:
</p>
<table>
<tr><td><code>fm</code></td>
<td>
<p>List with the outputs of the RR ((<code>V</code>): eigenvector matrix of the correlation matrix (n,n); (<code>TtDY</code>): intermediate output; (<code>sv</code>): singular values of the matrix <code class="reqn">(1,n)</code>; (<code>lb</code>): value of regularization parameter <code class="reqn">lambda</code>; (<code>xmeans</code>): the centering vector of X <code class="reqn">(p,1)</code>; (<code>ymeans</code>): the centering vector of Y <code class="reqn">(q,1)</code> ; (<code>weights</code>): the weights vector of X-variables <code class="reqn">(p,1)</code>.</p>
</td></tr>
<tr><td><code>lev</code></td>
<td>
<p>classes</p>
</td></tr>
<tr><td><code>ni</code></td>
<td>
<p>number of observations in each class</p>
</td></tr>
</table>
<p>For <code>predict.Rrda</code>:
</p>
<table>
<tr><td><code>pred</code></td>
<td>
<p>matrix or list of matrices (if lb is a vector), with predicted class for each observation</p>
</td></tr>
<tr><td><code>posterior</code></td>
<td>
<p>matrix or list of matrices (if lb is a vector), calculated probability of belonging to a class for each observation</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
n &lt;- 50 ; p &lt;- 8
Xtrain &lt;- matrix(rnorm(n * p), ncol = p)
ytrain &lt;- sample(c(1, 4, 10), size = n, replace = TRUE)

m &lt;- 5
Xtest &lt;- Xtrain[1:m, ] ; ytest &lt;- ytrain[1:m]

lb &lt;- 1
fm &lt;- rrda(Xtrain, ytrain, lb = lb)
predict(fm, Xtest)

pred &lt;- predict(fm, Xtest)$pred
err(pred, ytest)

predict(fm, Xtest, lb = 0:2)
predict(fm, Xtest, lb = 0)

</code></pre>

<hr>
<h2 id='sampcla'>Within-class sampling</h2><span id='topic+sampcla'></span>

<h3>Description</h3>

<p>The function divides a datset in two sets, &quot;train&quot; vs &quot;test&quot;, using a stratified sampling on defined classes.
</p>
<p>If argument <code>y = NULL</code> (default), the sampling is random within each class. If not, the sampling is systematic (regular grid) within each class over the quantitative variable <code class="reqn">y</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
sampcla(x, y = NULL, m)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sampcla_+3A_x">x</code></td>
<td>
<p>A vector (length <code class="reqn">m</code>) defining the class membership of the observations.</p>
</td></tr>
<tr><td><code id="sampcla_+3A_y">y</code></td>
<td>
<p>A vector (length <code class="reqn">m</code>) defining the quantitative variable for the systematic sampling. If <code>NULL</code> (default), the sampling is random within each class.</p>
</td></tr>
<tr><td><code id="sampcla_+3A_m">m</code></td>
<td>
<p>Either an integer defining the equal number of test observation(s) to select per class, or a vector of integers defining the numbers to select for each class. In the last case, vector <code>m</code> must have a length equal to the number of classes present in <code>x</code>, and be ordered in the same way as the ordered class membership.</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>train</code></td>
<td>
<p>Indexes (i.e. position in <code class="reqn">x</code>) of the selected observations, for the training set.</p>
</td></tr>
<tr><td><code>test</code></td>
<td>
<p>Indexes (i.e. position in <code class="reqn">x</code>) of the selected observations, for the test set.</p>
</td></tr>
<tr><td><code>lev</code></td>
<td>
<p>classes</p>
</td></tr>
<tr><td><code>ni</code></td>
<td>
<p>number of observations in each class</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The second example is a representative stratified sampling from an unsupervised clustering.
</p>


<h3>References</h3>

<p>Naes, T., 1987. The design of calibration in near infra-red reflectance analysis by clustering. Journal of Chemometrics 1, 121-134.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## EXAMPLE 1

x &lt;- sample(c(1, 3, 4), size = 20, replace = TRUE)
table(x)

sampcla(x, m = 2)
s &lt;- sampcla(x, m = 2)$test
x[s]

sampcla(x, m = c(1, 2, 1))
s &lt;- sampcla(x, m = c(1, 2, 1))$test
x[s]

y &lt;- rnorm(length(x))
sampcla(x, y, m = 2)
s &lt;- sampcla(x, y, m = 2)$test
x[s]

## EXAMPLE 2

data(cassav)
X &lt;- cassav$Xtrain
y &lt;- cassav$ytrain
N &lt;- nrow(X)

fm &lt;- pcaeigenk(X, nlv = 10)
z &lt;- stats::kmeans(x = fm$T, centers = 3, nstart = 25, iter.max = 50)
x &lt;- z$cluster
z &lt;- table(x)
z
p &lt;- c(z) / N
p

psamp &lt;- .20
m &lt;- round(psamp * N * p)
m

random_sampling &lt;- sampcla(x, m = m)
s &lt;- random_sampling$test
table(x[s])

Systematic_sampling_for_y &lt;- sampcla(x, y, m = m)
s &lt;- Systematic_sampling_for_y$test
table(x[s])

</code></pre>

<hr>
<h2 id='sampdp'>Duplex sampling</h2><span id='topic+sampdp'></span>

<h3>Description</h3>

<p>The function divides the data <code class="reqn">X</code> in two sets, &quot;train&quot; vs &quot;test&quot;, using the Duplex algorithm (Snee, 1977). The two sets are of equal size. If needed, the user can add <code class="reqn">a posteriori</code> the eventual remaining observations (not in &quot;train&quot; nor &quot;test&quot;) to &quot;train&quot;.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
sampdp(X, k, diss = c("eucl", "mahal"))

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sampdp_+3A_x">X</code></td>
<td>
<p>X-data (<code class="reqn">n, p</code>) to be sampled.</p>
</td></tr>
<tr><td><code id="sampdp_+3A_k">k</code></td>
<td>
<p>An integer defining the number of training observations to select. Must be &lt;= <code class="reqn">n / 2</code>.</p>
</td></tr>
<tr><td><code id="sampdp_+3A_diss">diss</code></td>
<td>
<p>The type of dissimilarity used for selecting the observations in the algorithm. Possible values are &quot;eucl&quot; (default; Euclidean distance) or &quot;mahal&quot; (Mahalanobis distance).</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>train</code></td>
<td>
<p>Indexes (i.e. row numbers in <code class="reqn">X</code>) of the selected observations, for the training set.</p>
</td></tr>
<tr><td><code>test</code></td>
<td>
<p>Indexes (i.e. row numbers in <code class="reqn">X</code>) of the selected observations, for the test set.</p>
</td></tr>
<tr><td><code>remain</code></td>
<td>
<p>Indexes (i.e., row numbers in <code class="reqn">X</code>) of the remaining observations.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Kennard, R.W., Stone, L.A., 1969. Computer aided design of experiments. Technometrics, 11(1), 137-148.
</p>
<p>Snee, R.D., 1977. Validation of Regression Models: Methods and Examples. Technometrics 19, 415-428. https://doi.org/10.1080/00401706.1977.10489581
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
n &lt;- 10 ; p &lt;- 3
X &lt;- matrix(rnorm(n * p), ncol = p)

k &lt;- 4
sampdp(X, k = k)
sampdp(X, k = k, diss = "mahal")

</code></pre>

<hr>
<h2 id='sampks'>Kennard-Stone sampling</h2><span id='topic+sampks'></span>

<h3>Description</h3>

<p>The function divides the data <code class="reqn">X</code> in two sets, &quot;train&quot; vs &quot;test&quot;, using the Kennard-Stone (KS) algorithm (Kennard &amp; Stone, 1969). The two sets correspond to two different underlying probability distributions: set &quot;train&quot; has higher dispersion than set &quot;test&quot;.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
sampks(X, k, diss = c("eucl", "mahal"))

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sampks_+3A_x">X</code></td>
<td>
<p>X-data (<code class="reqn">n, p</code>) to be sampled.</p>
</td></tr>
<tr><td><code id="sampks_+3A_k">k</code></td>
<td>
<p>An integer defining the number of training observations to select.</p>
</td></tr>
<tr><td><code id="sampks_+3A_diss">diss</code></td>
<td>
<p>The type of dissimilarity used for selecting the observations in the algorithm. Possible values are &quot;eucl&quot; (default; Euclidean distance) or &quot;mahal&quot; (Mahalanobis distance).</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>train</code></td>
<td>
<p>Indexes (i.e. row numbers in <code class="reqn">X</code>) of the selected observations, for the training set.</p>
</td></tr>
<tr><td><code>test</code></td>
<td>
<p>Indexes (i.e. row numbers in <code class="reqn">X</code>) of the selected observations, for the test set.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Kennard, R.W., Stone, L.A., 1969. Computer aided design of experiments. Technometrics, 11(1), 137-148.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
n &lt;- 10 ; p &lt;- 3
X &lt;- matrix(rnorm(n * p), ncol = p)

k &lt;- 7
sampks(X, k = k)  

n &lt;- 10 ; k &lt;- 25
X &lt;- expand.grid(1:n, 1:n)
X &lt;- X + rnorm(nrow(X) * ncol(X), 0, .1)
s &lt;- sampks(X, k)$train 
plot(X)
points(X[s, ], pch = 19, col = 2, cex = 1.5)

</code></pre>

<hr>
<h2 id='savgol'>Savitzky-Golay smoothing</h2><span id='topic+savgol'></span>

<h3>Description</h3>

<p>Smoothing by derivation, with a Savitzky-Golay filter, of the row observations (e.g. spectra) of a data set .
</p>
<p>The function uses function <code><a href="signal.html#topic+sgolayfilt">sgolayfilt</a></code> of package <code>signal</code> available on the CRAN.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>savgol(X, m, n, p, ts = 1)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="savgol_+3A_x">X</code></td>
<td>
<p>X-data).</p>
</td></tr>
<tr><td><code id="savgol_+3A_m">m</code></td>
<td>
<p>Derivation order.</p>
</td></tr>
<tr><td><code id="savgol_+3A_n">n</code></td>
<td>
<p>Filter length (must be odd), i.e. the number of colums in <code>X</code> defining the filter window.</p>
</td></tr>
<tr><td><code id="savgol_+3A_p">p</code></td>
<td>
<p>Polynomial order.</p>
</td></tr>
<tr><td><code id="savgol_+3A_ts">ts</code></td>
<td>
<p>Scaling factor (e.g. the absolute step between two columns in matrix <code class="reqn">X</code>), see argument <code>ts</code> in function <code><a href="signal.html#topic+sgolayfilt">sgolayfilt</a></code>. This has not impact on the form of the transformed output.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix of the transformed data.</p>


<h3>Examples</h3>

<pre><code class='language-R'>
X &lt;- cassav$Xtest

m &lt;- 1 ; n &lt;- 11 ; p &lt;- 2
Xp &lt;- savgol(X, m, n, p)

oldpar &lt;- par(mfrow = c(1, 1))
par(mfrow = c(1, 2))
plotsp(X, main = "Signal")
plotsp(Xp, main = "Corrected signal")
abline(h = 0, lty = 2, col = "grey")
par(oldpar)

</code></pre>

<hr>
<h2 id='scordis'>Score distances (SD) in a PCA or PLS score space</h2><span id='topic+scordis'></span>

<h3>Description</h3>

<p><code>scordis</code> calculates the score distances (SD) for a PCA or PLS model. SD is the Mahalanobis distance of the projection of a row observation on the score plan to the center of the score space.
</p>
<p>A distance cutoff is computed using a moment estimation of the parameters of a Chi-squared distrbution for SD^2 (see e.g. Pomerantzev 2008). In the function output, column <code>dstand</code>  is a standardized distance defined as <code class="reqn">SD / cutoff</code>. A value <code>dstand &gt; 1</code> can be considered as extreme.
</p>
<p>The Winisi &quot;GH&quot; is also provided (usually considered as extreme if GH &gt; 3).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
scordis(
    object, X = NULL, 
    nlv = NULL,
    rob = TRUE, alpha = .01
    )

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="scordis_+3A_object">object</code></td>
<td>
<p>A fitted model, output of a call to a fitting function (for example from <code>pcasvd</code>, <code>plskern</code>,...).</p>
</td></tr>
<tr><td><code id="scordis_+3A_x">X</code></td>
<td>
<p>New X-data.</p>
</td></tr>
<tr><td><code id="scordis_+3A_nlv">nlv</code></td>
<td>
<p>Number of components (PCs or LVs) to consider.</p>
</td></tr>
<tr><td><code id="scordis_+3A_rob">rob</code></td>
<td>
<p>Logical. If <code>TRUE</code>, the moment estimation of the distance cutoff is robustified. This can be recommended after robust PCA or PLS on small data sets containing extreme values.</p>
</td></tr>
<tr><td><code id="scordis_+3A_alpha">alpha</code></td>
<td>
<p>Risk-<code class="reqn">I</code> level for defining the cutoff detecting extreme values.</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>res.train</code></td>
<td>
<p>matrix with distances, standardized distances and Winisi &quot;GH&quot;, for the training set.</p>
</td></tr>
<tr><td><code>res</code></td>
<td>
<p>matrix with distances, standardized distances and Winisi &quot;GH&quot;, for new X-data if any.</p>
</td></tr>
<tr><td><code>cutoff</code></td>
<td>
<p>cutoff value</p>
</td></tr>
</table>


<h3>References</h3>

<p>M. Hubert, P. J. Rousseeuw, K. Vanden Branden (2005). ROBPCA: a new approach to robust principal components analysis. Technometrics, 47, 64-79.
</p>
<p>Pomerantsev, A.L., 2008. Acceptance areas for multivariate classification derived by projection methods. Journal of Chemometrics 22, 601-609. https://doi.org/10.1002/cem.1147
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
n &lt;- 6 ; p &lt;- 4
Xtrain &lt;- matrix(rnorm(n * p), ncol = p)
ytrain &lt;- rnorm(n)
Xtest &lt;- Xtrain[1:3, , drop = FALSE] 

nlv &lt;- 3
fm &lt;- pcasvd(Xtrain, nlv = nlv)
scordis(fm)
scordis(fm, nlv = 2)
scordis(fm, Xtest, nlv = 2)

</code></pre>

<hr>
<h2 id='segmkf'>Segments for cross-validation</h2><span id='topic+segmkf'></span><span id='topic+segmts'></span>

<h3>Description</h3>

<p>Build segments of observations for K-Fold or &quot;test-set&quot; cross-validation (CV). 
</p>
<p>The CV can eventually be randomly repeated. For each repetition:
</p>
<p>- <b>K-fold CV</b> - Function <code>segmkf</code> returns the <code class="reqn">K</code> segments.
</p>
<p>- <b>Test-set CV</b> - Function <code>segmts</code> returns a segment (of a given length) randomly sampled in the dataset.
</p>
<p><b>CV of blocks</b> 
</p>
<p>Argument <code>y</code> allows sampling <b>blocks of observations</b> instead of observations. This can be required when there are repetitions in the data. In such a situation, CV should account for the repetition level (if not, the error rates are in general highly underestimated). For implementing such a CV, object <code>y</code> must be a a vector (<code class="reqn">n</code>) defining the blocks, in the same order as in the data.
</p>
<p>In any cases (<code>y = NULL</code> or not), the functions return a list of vector(s). Each vector contains the indexes of the observations defining the segment.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
segmkf(n, y = NULL, K = 5, 
    type = c("random", "consecutive", "interleaved"), nrep = 1) 

segmts(n, y = NULL, m, nrep) 

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="segmkf_+3A_n">n</code></td>
<td>
<p>The total number of row observations in the dataset. If <code>y = NULL</code>, the CV is implemented on <code>1:n</code>. If <code>y != NULL</code>, blocks of observations (defined in <code>y</code>) are sampled instead of observations (but indexes of observations are returned).</p>
</td></tr>
<tr><td><code id="segmkf_+3A_y">y</code></td>
<td>
<p>A vector (<code class="reqn">n</code>) defining the blocks. Default to <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="segmkf_+3A_k">K</code></td>
<td>
<p>For <code>segmkf</code>.The number of folds (i.e. segments) in the K-fold CV.</p>
</td></tr>
<tr><td><code id="segmkf_+3A_type">type</code></td>
<td>
<p>For <code>segmkf</code>.The type K-fold CV. Possible values are &quot;random&quot; (default), &quot;consecutive&quot; and &quot;interleaved&quot;.</p>
</td></tr>
<tr><td><code id="segmkf_+3A_m">m</code></td>
<td>
<p>For <code>segmts</code>. If <code>y = NULL</code>, the number of observations in the segment. If not, the number of blocks in the segment.</p>
</td></tr>
<tr><td><code id="segmkf_+3A_nrep">nrep</code></td>
<td>
<p>The number of replications of the repeated CV. Default to <code>nrep = 1</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The segments (lists of indexes).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
Kfold &lt;- segmkf(n = 10, K = 3)

interleavedKfold &lt;- segmkf(n = 10, K = 3, type = "interleaved")

LeaveOneOut &lt;- segmkf(n = 10, K = 10)

RepeatedKfold &lt;- segmkf(n = 10, K = 3, nrep = 2)

repeatedTestSet &lt;- segmts(n = 10, m = 3, nrep = 5)

n &lt;- 10
y &lt;- rep(LETTERS[1:5], 2)
y

Kfold_withBlocks &lt;- segmkf(n = n, y = y, K = 3, nrep = 1)
z &lt;- Kfold_withBlocks 
z
y[z$rep1$segm1]
y[z$rep1$segm2]
y[z$rep1$segm3]

TestSet_withBlocks &lt;- segmts(n = n, y = y, m = 3, nrep = 1)
z &lt;- TestSet_withBlocks
z
y[z$rep1$segm1]

</code></pre>

<hr>
<h2 id='selwold'>Heuristic selection of the dimension of a latent variable model with the Wold's criterion</h2><span id='topic+selwold'></span>

<h3>Description</h3>

<p>The function helps selecting the dimensionnality of latent variable (LV) models (e.g. PLSR) using the &quot;Wold criterion&quot;. 
</p>
<p>The criterion is the &quot;precision gain ratio&quot; <code class="reqn">R = 1 - r(a+1) / r(a)</code> where <code class="reqn">r</code> is an observed error rate quantifying the model performance (msep, classification error rate, etc.) and <code class="reqn">a</code> the model dimensionnality (= nb. LVs). It can also represent other indicators such as the eigenvalues of a PCA.
</p>
<p><code class="reqn">R</code> is the relative gain in efficiency after a new LV is added to the model. The iterations continue until <code class="reqn">R</code> becomes lower than a threshold value <code class="reqn">alpha</code>. By default and only as an indication, the default <code class="reqn">alpha = .05</code> is set in the function, but the user should set any other value depending on his data and parcimony objective.
</p>
<p>In the original article, Wold (1978; see also Bro et al. 2008) used the ratio of <b>cross-validated</b> over <b>training</b> residual sums of squares, i.e. PRESS over SSR. Instead, <code>selwold</code> compares values of consistent nature (the successive values in the input vector <code class="reqn">r</code>), e.g. PRESS only . For instance, <code class="reqn">r</code> was set to PRESS values in Li et al. (2002) and Andries et al. (2011), which is equivalent to the &quot;punish factor&quot; described in Westad &amp; Martens (2000).
</p>
<p>The ratio <code class="reqn">R</code> is often erratic, making difficult the dimensionnaly selection. Function <code>selwold</code> proposes to calculate a smoothing of <code class="reqn">R</code> (argument <code class="reqn">smooth</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>selwold(
    r, indx = seq(length(r)), 
    smooth = TRUE, f = 1/3,
    alpha = .05, digits = 3,
    plot = TRUE,
    xlab = "Index", ylab = "Value", main = "r",
    ...
    )
  </code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="selwold_+3A_r">r</code></td>
<td>
<p>Vector of a given error rate (<code class="reqn">n</code>) or any other indicator.</p>
</td></tr>
<tr><td><code id="selwold_+3A_indx">indx</code></td>
<td>
<p>Vector of indexes (<code class="reqn">n</code>), typically the nb. of Lvs.</p>
</td></tr>
<tr><td><code id="selwold_+3A_smooth">smooth</code></td>
<td>
<p>Logical. If <code>TRUE</code> (default), the selection is done on the smoothed <code class="reqn">R</code>.</p>
</td></tr>
<tr><td><code id="selwold_+3A_f">f</code></td>
<td>
<p>Window for smoothing <code class="reqn">R</code> with function <code><a href="stats.html#topic+lowess">lowess</a></code>.</p>
</td></tr>
<tr><td><code id="selwold_+3A_alpha">alpha</code></td>
<td>
<p>Proportion <code class="reqn">alpha</code> used as threshold for <code class="reqn">R</code>.</p>
</td></tr>
<tr><td><code id="selwold_+3A_digits">digits</code></td>
<td>
<p>Number of digits for <code class="reqn">R</code>.</p>
</td></tr>
<tr><td><code id="selwold_+3A_plot">plot</code></td>
<td>
<p>Logical. If <code>TRUE</code> (default), results are plotted.</p>
</td></tr>
<tr><td><code id="selwold_+3A_xlab">xlab</code></td>
<td>
<p>x-axis label of the plot of <code class="reqn">r</code> (left-side in the graphic window).</p>
</td></tr>
<tr><td><code id="selwold_+3A_ylab">ylab</code></td>
<td>
<p>y-axis label of the plot of <code class="reqn">r</code> (left-side in the graphic window).</p>
</td></tr>
<tr><td><code id="selwold_+3A_main">main</code></td>
<td>
<p>Title of the plot of <code class="reqn">r</code> (left-side in the graphic window).</p>
</td></tr>
<tr><td><code id="selwold_+3A_...">...</code></td>
<td>
<p>Other arguments to pass in function <code><a href="stats.html#topic+lowess">lowess</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>res</code></td>
<td>
<p>matrix with for each number of Lvs: <code class="reqn">r</code>, the observed error rate quantifying the model performance; <code class="reqn">diff</code>, the difference between <code class="reqn">r(a+1)</code> and <code class="reqn">r(a)</code> ; <code class="reqn">R</code>, the relative gain in efficiency after a new LV is added to the model; <code class="reqn">Rs</code>, smoothing of <code class="reqn">R</code>.</p>
</td></tr>
<tr><td><code>opt</code></td>
<td>
<p>The index of the minimum for <code class="reqn">r</code>.</p>
</td></tr>
<tr><td><code>sel</code></td>
<td>
<p>The index of the selection from the <code class="reqn">R</code> (or smoothed <code class="reqn">R</code>) threshold.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Andries, J.P.M., Vander Heyden, Y., Buydens, L.M.C., 2011. Improved variable reduction in partial least squares modelling based on Predictive-Property-Ranked Variables and adaptation of partial least squares complexity. Analytica Chimica Acta 705, 292-305. https://doi.org/10.1016/j.aca.2011.06.037
</p>
<p>Bro, R., Kjeldahl, K., Smilde, A.K., Kiers, H.A.L., 2008. Cross-validation of component models: A critical look at current methods. Anal Bioanal Chem 390, 1241-1251. https://doi.org/10.1007/s00216-007-1790-1
</p>
<p>Li, B., Morris, J., Martin, E.B., 2002. Model selection for partial least squares regression. Chemometrics and Intelligent Laboratory Systems 64, 79-89. https://doi.org/10.1016/S0169-7439(02)00051-5
</p>
<p>Westad, F., Martens, H., 2000. Variable Selection in near Infrared Spectroscopy Based on Significance Testing in Partial Least Squares Regression. J. Near Infrared Spectrosc., JNIRS 8, 117-124.
</p>
<p>Wold S. Cross-Validatory Estimation of the Number of Components in Factor and Principal Components Models. Technometrics. 1978;20(4):397-405
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(cassav)

Xtrain &lt;- cassav$Xtrain
ytrain &lt;- cassav$ytrain
X &lt;- cassav$Xtest
y &lt;- cassav$ytest

nlv &lt;- 20
res &lt;- gridscorelv(
    Xtrain, ytrain, X, y, 
    score = msep, fun = plskern, 
    nlv = 0:nlv
    )
selwold(res$y1, res$nlv, f = 2/3)

</code></pre>

<hr>
<h2 id='snv'>Standard normal variate transformation (SNV)</h2><span id='topic+snv'></span>

<h3>Description</h3>

<p>SNV transformation of the row observations (e.g. spectra) of a dataset. By default, each observation is centered on its mean and divided by its standard deviation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>snv(X, center = TRUE, scale = TRUE)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="snv_+3A_x">X</code></td>
<td>
<p>X-data (<code class="reqn">n, p</code>).</p>
</td></tr>
<tr><td><code id="snv_+3A_center">center</code></td>
<td>
<p>Logical. If <code>TRUE</code> (default), the centering in the SNV is done.</p>
</td></tr>
<tr><td><code id="snv_+3A_scale">scale</code></td>
<td>
<p>Logical. If <code>TRUE</code> (default), the scaling in the SNV is done.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix of the transformed data.</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(cassav)

X &lt;- cassav$Xtest

Xp &lt;- snv(X)

oldpar &lt;- par(mfrow = c(1, 1))
par(mfrow = c(1, 2))
plotsp(X, main = "Signal")
plotsp(Xp, main = "Corrected signal")
abline(h = 0, lty = 2, col = "grey")
par(oldpar)

</code></pre>

<hr>
<h2 id='sopls'>Block dimension reduction by SO-PLS</h2><span id='topic+soplsr'></span><span id='topic+transform.Soplsr'></span><span id='topic+predict.Soplsr'></span><span id='topic+soplsrcv'></span>

<h3>Description</h3>

<p>Function <code>soplsr</code> implements dimension reductions of pre-selected blocks of variables (= set of columns) of a reference (= training) matrix, by sequential orthogonalization-PLS (said &quot;SO-PLS&quot;). 
</p>
<p>Function <code>soplsrcv</code> perfoms repeteated cross-validation of an SO-PLS model in order to choose the optimal lv combination from the different blocks.
</p>
<p>SO-PLS is described for instance in Menichelli et al. (2014), Biancolillo et al. (2015) and Biancolillo (2016). 
</p>
<p>The block reduction consists in calculating latent variables (= scores) for each block, each block being sequentially orthogonalized to the information computed from the previous blocks.
</p>
<p>The function allows giving a priori weights to the rows of the reference matrix in the calculations.
</p>
<p><b>Auxiliary functions</b>
</p>
<p><code>transform</code> Calculates the LVs for any new matrices list <code class="reqn">Xlist</code> from the model.
</p>
<p><code>predict</code> Calculates the predictions for any new matrices list <code class="reqn">Xlist</code> from the model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
soplsr(Xlist, Y, Xscaling = c("none", "pareto", "sd")[1], 
Yscaling = c("none", "pareto", "sd")[1], weights = NULL, nlv)

soplsrcv(Xlist, Y, Xscaling = c("none", "pareto", "sd")[1], 
Yscaling = c("none", "pareto", "sd")[1], weights = NULL, nlvlist = list(), 
nbrep = 30, cvmethod = "kfolds", seed = 123, samplingk = NULL, nfolds = 7, 
optimisation = c("global","sequential")[1], 
selection = c("localmin","globalmin","1std")[1], majorityvote = FALSE)


## S3 method for class 'Soplsr'
transform(object, X, ...)  

## S3 method for class 'Soplsr'
predict(object, X, ...)  

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sopls_+3A_xlist">Xlist</code></td>
<td>
<p>A list of matrices or data frames of reference (= training) observations.</p>
</td></tr>
<tr><td><code id="sopls_+3A_x">X</code></td>
<td>
<p>For the auxiliary functions: list of new X-data, with the same variables than the training X-data.</p>
</td></tr>
<tr><td><code id="sopls_+3A_y">Y</code></td>
<td>
<p>A <code class="reqn">n x q</code> matrix or data frame, or a vector of length <code class="reqn">n</code>, of reference (= training) responses.</p>
</td></tr>
<tr><td><code id="sopls_+3A_xscaling">Xscaling</code></td>
<td>
<p>vector (of length Xlist) of variable scaling for each datablock, among &quot;none&quot; (mean-centering only), &quot;pareto&quot; (mean-centering and pareto scaling), &quot;sd&quot; (mean-centering and unit variance scaling). If &quot;pareto&quot; or &quot;sd&quot;, uncorrected standard deviation is used.</p>
</td></tr>
<tr><td><code id="sopls_+3A_yscaling">Yscaling</code></td>
<td>
<p>variable scaling for the Y-block, among &quot;none&quot; (mean-centering only), &quot;pareto&quot; (mean-centering and pareto scaling), &quot;sd&quot; (mean-centering and unit variance scaling). If &quot;pareto&quot; or &quot;sd&quot;, uncorrected standard deviation is used.</p>
</td></tr>
<tr><td><code id="sopls_+3A_weights">weights</code></td>
<td>
<p>a priori weights to the rows of the reference matrix in the calculations.</p>
</td></tr>
<tr><td><code id="sopls_+3A_nlv">nlv</code></td>
<td>
<p>A vector of same length as the number of blocks defining the number of scores to calculate for each block, or a single number. In this last case, the same number of scores is used for all the blocks.</p>
</td></tr>
<tr><td><code id="sopls_+3A_nlvlist">nlvlist</code></td>
<td>
<p>A list of same length as the number of X-blocks. Each component of the list gives the number of PLS components of the corresponding X-block to test.</p>
</td></tr>
<tr><td><code id="sopls_+3A_nbrep">nbrep</code></td>
<td>
<p>An integer, setting the number of CV repetitions. Default value is 30.</p>
</td></tr>
<tr><td><code id="sopls_+3A_cvmethod">cvmethod</code></td>
<td>
<p>&quot;kfolds&quot; for k-folds cross-validation, or &quot;loo&quot; for leave-one-out.</p>
</td></tr>
<tr><td><code id="sopls_+3A_seed">seed</code></td>
<td>
<p>a numeric. Seed used for the repeated resampling, and if cvmethod is &quot;kfolds&quot; and samplingk is not NULL.</p>
</td></tr>
<tr><td><code id="sopls_+3A_samplingk">samplingk</code></td>
<td>
<p>A vector of length n. The elements are the values of a qualitative variable used for stratified partition creation. If NULL, the first observation is set in the first fold, the second observation in the second fold, etc...</p>
</td></tr>
<tr><td><code id="sopls_+3A_nfolds">nfolds</code></td>
<td>
<p>An integer, setting the number of partitions to create. Default value is 7.</p>
</td></tr>
<tr><td><code id="sopls_+3A_optimisation">optimisation</code></td>
<td>
<p>&quot;global&quot; or &quot;sequential&quot; optimisation of the number of components. If &quot;sequential&quot;, the optimal lv number is found for the first X-block, then for the 2nd one, etc...</p>
</td></tr>
<tr><td><code id="sopls_+3A_selection">selection</code></td>
<td>
<p>a character indicating the selection method to use to choose the optimal combination of components, among &quot;localmin&quot;,&quot;globalmin&quot;,&quot;1std&quot;. If &quot;localmin&quot;: the optimal combination corresponds to the first local minimum of the mean CV rmse. If &quot;globalmin&quot; : the optimal combination corresponds to the minimum mean CV rmse. If &quot;1std&quot; (one standard errror rule): it corresponds to the first combination after which the mean cross-validated rmse does not decrease significantly.</p>
</td></tr>
<tr><td><code id="sopls_+3A_majorityvote">majorityvote</code></td>
<td>
<p>only if optimisation is &quot;global&quot; or one X-block. If majorityvote is TRUE, the optimal combination is chosen for each Y variable, with the chosen selection, before a majority vote. If majorityvote is &quot;FALSE, the optimal combination is simply chosen with the chosen selection.</p>
</td></tr>
<tr><td><code id="sopls_+3A_object">object</code></td>
<td>
<p>For the auxiliary functions: A fitted model, output of a call to the main functions.</p>
</td></tr>
<tr><td><code id="sopls_+3A_...">...</code></td>
<td>
<p>For the auxiliary functions: Optional arguments. Not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>For <code>soplsr</code>:
</p>
<table>
<tr><td><code>fm</code></td>
<td>
<p>A list of the plsr models.</p>
</td></tr>
<tr><td><code>T</code></td>
<td>
<p>A matrix with the concatenated scores calculated from the X-blocks.</p>
</td></tr>
<tr><td><code>pred</code></td>
<td>
<p>A matrice <code class="reqn">n x q</code> with the calculated fitted values.</p>
</td></tr>
<tr><td><code>xmeans</code></td>
<td>
<p>list of vectors of X-mean values.</p>
</td></tr>
<tr><td><code>ymeans</code></td>
<td>
<p>vector of Y-mean values.</p>
</td></tr>
<tr><td><code>xscales</code></td>
<td>
<p>list of vectors of X-scaling values.</p>
</td></tr>
<tr><td><code>yscales</code></td>
<td>
<p>vector of Y-scaling values.</p>
</td></tr>
<tr><td><code>b</code></td>
<td>
<p>A list of X-loading weights, used in the orthogonalization step.</p>
</td></tr>
<tr><td><code>weights</code></td>
<td>
<p>Weights applied to the training observations.</p>
</td></tr>
<tr><td><code>nlv</code></td>
<td>
<p>vector of numbers of latent variables from each X-block.</p>
</td></tr>
</table>
<p>For <code>transform.Soplsr</code>:  the LVs calculated for the new matrices list <code class="reqn">Xlist</code> from the model.
</p>
<p>For <code>predict.Soplsr</code>: predicted values for each observation
</p>
<p>For <code>soplsrcv</code>:
</p>
<table>
<tr><td><code>lvcombi</code></td>
<td>
<p>matrix or list of matrices, of tested component combinations.</p>
</td></tr>
<tr><td><code>optimcombi</code></td>
<td>
<p>the number of PLS components of each X-block allowing the optimisation of the mean rmseCV.</p>
</td></tr>
<tr><td><code>rmseCV_byY</code></td>
<td>
<p>matrix or list of matrices of mean and sd of cross-validated RMSE in the model for each combination and each response variable.</p>
</td></tr>
<tr><td><code>ExplVarCV_byY</code></td>
<td>
<p>matrix or list of matrices of mean and sd of cross-validated explained variances in the model for each combination and each response variable.</p>
</td></tr>
<tr><td><code>rmseCV</code></td>
<td>
<p>matrix or list of matrices of mean and sd of cross-validated RMSE in the model for each combination and response variables.</p>
</td></tr>
<tr><td><code>ExplVarCV</code></td>
<td>
<p>matrix or list of matrices of mean and sd of cross-validated explained variances in the model for each combination and response variables.</p>
</td></tr>
</table>


<h3>References</h3>

<p>- Biancolillo et al. , 2015. Combining SO-PLS and linear discriminant analysis for
multi-block classification. Chemometrics and Intelligent Laboratory Systems, 141, 58-67.
</p>
<p>- Biancolillo, A. 2016. Method development in the area of multi-block analysis focused on food analysis. PhD. University of copenhagen.
</p>
<p>- Menichelli et al., 2014. SO-PLS as an exploratory tool for path modelling. Food Quality and Preference, 36, 122-134.
</p>
<p>- Tenenhaus, M., 1998. La régression PLS: théorie et pratique. Editions Technip, Paris, France.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
N &lt;- 10 ; p &lt;- 12
set.seed(1)
X &lt;- matrix(rnorm(N * p, mean = 10), ncol = p, byrow = TRUE)
Y &lt;- matrix(rnorm(N * 2, mean = 10), ncol = 2, byrow = TRUE)
colnames(X) &lt;- paste("varx", 1:ncol(X), sep = "")
colnames(Y) &lt;- paste("vary", 1:ncol(Y), sep = "")
rownames(X) &lt;- rownames(Y) &lt;- paste("obs", 1:nrow(X), sep = "")
set.seed(NULL)
X
Y

n &lt;- nrow(X)

X_list &lt;- list(X[,1:4], X[,5:7], X[,9:ncol(X)])
X_list_2 &lt;- list(X[1:2,1:4], X[1:2,5:7], X[1:2,9:ncol(X)])

soplsrcv(X_list, Y, Xscaling = c("none", "pareto", "sd")[1], 
Yscaling = c("none", "pareto", "sd")[1], weights = NULL, 
nlvlist=list(0:1, 1:2, 0:1), nbrep=1, cvmethod="loo", seed = 123, samplingk=NULL,
optimisation="global", selection="localmin", majorityvote=FALSE)


ncomp &lt;- 2
fm &lt;- soplsr(X_list, Y, nlv = ncomp)
transform(fm, X_list_2)
predict(fm, X_list_2)

mse(predict(fm, X_list), Y)

# VIP calculation based on the proportion of Y-variance explained by the components
vip(fm$fm[[1]], X_list[[1]], Y = NULL, nlv = ncomp)
vip(fm$fm[[2]], X_list[[2]], Y = NULL, nlv = ncomp)
vip(fm$fm[[3]], X_list[[3]], Y = NULL, nlv = ncomp)

ncomp &lt;- c(2, 0, 3)
fm &lt;- soplsr(X_list, Y, nlv = ncomp)
transform(fm, X_list_2)
predict(fm, X_list_2)
mse(predict(fm, X_list), Y)

ncomp &lt;- 0
fm &lt;- soplsr(X_list, Y, nlv = ncomp)
transform(fm, X_list_2)
predict(fm, X_list_2)

ncomp &lt;- 2
weights &lt;- rep(1 / n, n)
#w &lt;- 1:n
fm &lt;- soplsr(X_list, Y, Xscaling = c("sd","pareto","none"), nlv = ncomp, weights = weights)
transform(fm, X_list_2)
predict(fm, X_list_2)

</code></pre>

<hr>
<h2 id='soplsrda'>Block dimension reduction by SO-PLS-DA</h2><span id='topic+soplsrda'></span><span id='topic+soplslda'></span><span id='topic+soplsqda'></span><span id='topic+transform.Soplsrda'></span><span id='topic+transform.Soplsprobda'></span><span id='topic+predict.Soplsrda'></span><span id='topic+predict.Soplsprobda'></span><span id='topic+soplsrdacv'></span><span id='topic+soplsldacv'></span><span id='topic+soplsqdacv'></span>

<h3>Description</h3>

<p>Function <code>soplsrda</code> implements dimension reductions of pre-selected blocks of variables (= set of columns) of a reference (= training) matrix, by sequential orthogonalization-PLS (said &quot;SO-PLS&quot;) in a context of discrimination. 
</p>
<p>Function <code>soplsrdacv</code> perfoms repeteated cross-validation of an SO-PLS-RDA model in order to choose the optimal lv combination from the different blocks.
</p>
<p>The block reduction consists in calculating latent variables (= scores) for each block, each block being sequentially orthogonalized to the information computed from the previous blocks.
</p>
<p>The function allows giving a priori weights to the rows of the reference matrix in the calculations.
</p>
<p>In<code>soplslda</code> and <code>soplsqda</code>, probabilistic LDA and QDA are run over the PLS2 LVs, respectively.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
soplsrda(Xlist, y, Xscaling = c("none", "pareto", "sd")[1], 
Yscaling = c("none", "pareto", "sd")[1], weights = NULL, nlv)

soplslda(Xlist, y, Xscaling = c("none", "pareto", "sd")[1], 
Yscaling = c("none", "pareto", "sd")[1], weights = NULL, nlv, 
prior = c("unif", "prop"))

soplsqda(Xlist, y, Xscaling = c("none", "pareto", "sd")[1], 
Yscaling = c("none", "pareto", "sd")[1], weights = NULL, nlv, 
prior = c("unif", "prop"))

soplsrdacv(Xlist, y, Xscaling = c("none", "pareto", "sd")[1], 
Yscaling = c("none", "pareto", "sd")[1], weights = NULL, nlvlist=list(), 
nbrep=30, cvmethod="kfolds", seed = 123, samplingk = NULL, nfolds = 7, 
optimisation = c("global","sequential")[1], 
criterion = c("err","rmse")[1], selection = c("localmin","globalmin","1std")[1])

soplsldacv(Xlist, y, Xscaling = c("none", "pareto", "sd")[1], 
Yscaling = c("none", "pareto", "sd")[1], weights = NULL, nlvlist=list(), 
prior = c("unif", "prop"), nbrep = 30, cvmethod = "kfolds", seed = 123, samplingk = NULL, 
nfolds = 7, optimisation = c("global","sequential")[1], 
criterion = c("err","rmse")[1], selection = c("localmin","globalmin","1std")[1])

soplsqdacv(Xlist, y, Xscaling = c("none", "pareto", "sd")[1], 
Yscaling = c("none", "pareto", "sd")[1], weights = NULL, nlvlist = list(), 
prior = c("unif", "prop"), nbrep = 30, cvmethod = "kfolds", seed = 123, samplingk = NULL, 
nfolds = 7, optimisation = c("global","sequential")[1], 
criterion = c("err","rmse")[1], selection = c("localmin","globalmin","1std")[1])

## S3 method for class 'Soplsrda'
transform(object, X, ...) 

## S3 method for class 'Soplsprobda'
transform(object, X, ...) 

## S3 method for class 'Soplsrda'
predict(object, X, ...) 

## S3 method for class 'Soplsprobda'
predict(object, X, ...) 

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="soplsrda_+3A_xlist">Xlist</code></td>
<td>
<p>For the main functions: A list of matrices or data frames of reference (= training) observations.</p>
</td></tr>
<tr><td><code id="soplsrda_+3A_x">X</code></td>
<td>
<p>For the auxiliary functions: list of new X-data, with the same variables than the training X-data.</p>
</td></tr>
<tr><td><code id="soplsrda_+3A_y">y</code></td>
<td>
<p>Training class membership (<code class="reqn">n</code>). <b>Note:</b> If <code>y</code> is a factor, it is replaced by a character vector.</p>
</td></tr>
<tr><td><code id="soplsrda_+3A_xscaling">Xscaling</code></td>
<td>
<p>vector (of length Xlist) of variable scaling for each datablock, among &quot;none&quot; (mean-centering only), &quot;pareto&quot; (mean-centering and pareto scaling), &quot;sd&quot; (mean-centering and unit variance scaling). If &quot;pareto&quot; or &quot;sd&quot;, uncorrected standard deviation is used.</p>
</td></tr>
<tr><td><code id="soplsrda_+3A_yscaling">Yscaling</code></td>
<td>
<p>variable scaling for the Y-block, among &quot;none&quot; (mean-centering only), &quot;pareto&quot; (mean-centering and pareto scaling), &quot;sd&quot; (mean-centering and unit variance scaling). If &quot;pareto&quot; or &quot;sd&quot;, uncorrected standard deviation is used.</p>
</td></tr>
<tr><td><code id="soplsrda_+3A_weights">weights</code></td>
<td>
<p>a priori weights to the rows of the reference matrix in the calculations.</p>
</td></tr>
<tr><td><code id="soplsrda_+3A_nlv">nlv</code></td>
<td>
<p>A vector of same length as the number of blocks defining the number of scores to calculate for each block, or a single number. In this last case, the same number of scores is used for all the blocks.</p>
</td></tr>
<tr><td><code id="soplsrda_+3A_nlvlist">nlvlist</code></td>
<td>
<p>A list of same length as the number of X-blocks. Each component of the list gives the number of PLS components of the corresponding X-block to test.</p>
</td></tr>
<tr><td><code id="soplsrda_+3A_nbrep">nbrep</code></td>
<td>
<p>An integer, setting the number of CV repetitions. Default value is 30.</p>
</td></tr>
<tr><td><code id="soplsrda_+3A_cvmethod">cvmethod</code></td>
<td>
<p>&quot;kfolds&quot; for k-folds cross-validation, or &quot;loo&quot; for leave-one-out.</p>
</td></tr>
<tr><td><code id="soplsrda_+3A_seed">seed</code></td>
<td>
<p>a numeric. Seed used for the repeated resampling, and if cvmethod is &quot;kfolds&quot; and samplingk is not NULL.</p>
</td></tr>
<tr><td><code id="soplsrda_+3A_samplingk">samplingk</code></td>
<td>
<p>A vector of length n. The elements are the values of a qualitative variable used for stratified partition creation. If NULL, the first observation is set in the first fold, the second observation in the second fold, etc...</p>
</td></tr>
<tr><td><code id="soplsrda_+3A_nfolds">nfolds</code></td>
<td>
<p>An integer, setting the number of partitions to create. Default value is 7.</p>
</td></tr>
<tr><td><code id="soplsrda_+3A_optimisation">optimisation</code></td>
<td>
<p>&quot;global&quot; or &quot;sequential&quot; optimisation of the number of components. If &quot;sequential&quot;, the optimal lv number is found for the first X-block, then for the 2nd one, etc...</p>
</td></tr>
<tr><td><code id="soplsrda_+3A_criterion">criterion</code></td>
<td>
<p>optimisation criterion among &quot;rmse&quot; and &quot;err&quot; (for classification error rate)</p>
</td></tr>
<tr><td><code id="soplsrda_+3A_selection">selection</code></td>
<td>
<p>a character indicating the selection method to use to choose the optimal combination of components, among &quot;localmin&quot;,&quot;globalmin&quot;,&quot;1std&quot;. If &quot;localmin&quot;: the optimal combination corresponds to the first local minimum of the mean CV rmse or error rate. If &quot;globalmin&quot; : the optimal combination corresponds to the minimum mean CV rmse or error rate. If &quot;1std&quot; (one standard error rule) : it corresponds to the first combination after which the mean cross-validated rmse or error rate does not decrease significantly.</p>
</td></tr>
<tr><td><code id="soplsrda_+3A_prior">prior</code></td>
<td>
<p>The prior probabilities of the classes. Possible values are &quot;unif&quot; (default; probabilities are set equal for all the classes) or &quot;prop&quot; (probabilities are set equal to the observed proportions of the classes in <code>y</code>).</p>
</td></tr>
<tr><td><code id="soplsrda_+3A_object">object</code></td>
<td>
<p>For the auxiliary functions: A fitted model, output of a call to the main functions.</p>
</td></tr>
<tr><td><code id="soplsrda_+3A_...">...</code></td>
<td>
<p>For the auxiliary functions: Optional arguments. Not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>For <code>soplsrda</code>, <code>soplslda</code>, <code>soplsqda</code>:
</p>
<table>
<tr><td><code>fm</code></td>
<td>
<p>list with the PLS models: (<code>T</code>): X-scores matrix; (<code>P</code>): X-loading matrix;(<code>R</code>): The PLS projection matrix (p,nlv); (<code>W</code>): X-loading weights matrix ;(<code>C</code>): The Y-loading weights matrix; (<code>TT</code>): the X-score normalization factor; (<code>xmeans</code>): the centering vector of X (p,1);  (<code>ymeans</code>): the centering vector of Y (q,1); (<code>weights</code>): vector of observation weights; (<code>Xscales</code>): X scaling values; (<code>Yscales</code>): Y scaling values; (<code>U</code>): intermediate output.</p>
</td></tr>
<tr><td><code>lev</code></td>
<td>
<p>classes</p>
</td></tr>
<tr><td><code>ni</code></td>
<td>
<p>number of observations in each class</p>
</td></tr>
</table>
<p>For <code>transform.Soplsrda</code>, <code>transform.Soplsprobda</code>:  the LVs Calculated for the new matrices list <code class="reqn">Xlist</code> from the model.
</p>
<p>For <code>predict.Soplsrda</code>, <code>predict.Soplsprobda</code>: 
</p>
<table>
<tr><td><code>pred</code></td>
<td>
<p>predicted class for each observation</p>
</td></tr>
<tr><td><code>posterior</code></td>
<td>
<p>calculated probability of belonging to a class for each observation</p>
</td></tr>
</table>
<p>For <code>soplsrdacv</code>, <code>soplsldacv</code>, <code>soplsqdacv</code>:
</p>
<table>
<tr><td><code>lvcombi</code></td>
<td>
<p>matrix or list of matrices, of tested component combinations.</p>
</td></tr>
<tr><td><code>optimCombiLine</code></td>
<td>
<p>number of the combination line corresponding to the optimal one. In the case of a sequential optimisation, it is the number of the combination line in the model with all the X-blocks.</p>
</td></tr>
<tr><td><code>optimcombi</code></td>
<td>
<p>the number of PLS components of each X-block allowing the optimisation of the mean rmseCV.</p>
</td></tr>
<tr><td><code>optimExplVarCV</code></td>
<td>
<p>cross-validated explained variance for the optimal soplsda model.</p>
</td></tr>
<tr><td><code>rmseCV</code></td>
<td>
<p>matrix or list of matrices of mean and sd of cross-validated rmse in the model for each combination and response variables.</p>
</td></tr>
<tr><td><code>ExplVarCV</code></td>
<td>
<p>matrix or list of matrices of mean and sd of cross-validated explained variances in the model for each combination and response variables.</p>
</td></tr>
<tr><td><code>errCV</code></td>
<td>
<p>matrix or list of matrices of mean and sd of cross-validated classification error rates in the model for each combination and response variables.</p>
</td></tr>
</table>


<h3>References</h3>

<p>- Biancolillo et al. , 2015. Combining SO-PLS and linear discriminant analysis for
multi-block classification. Chemometrics and Intelligent Laboratory Systems, 141, 58-67.
</p>
<p>- Biancolillo, A. 2016. Method development in the area of multi-block analysis focused on food analysis. PhD. University of copenhagen.
</p>
<p>- Menichelli et al., 2014. SO-PLS as an exploratory tool for path modelling. Food Quality and Preference, 36, 122-134.
</p>
<p>- Tenenhaus, M., 1998. La régression PLS: théorie et pratique. Editions Technip, Paris, France.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
N &lt;- 10 ; p &lt;- 12
set.seed(1)
X &lt;- matrix(rnorm(N * p, mean = 10), ncol = p, byrow = TRUE)
y &lt;- matrix(sample(c("1", "4", "10"), size = N, replace = TRUE), ncol=1)
colnames(X) &lt;- paste("x", 1:ncol(X), sep = "")
set.seed(NULL)

n &lt;- nrow(X)

X_list &lt;- list(X[,1:4], X[,5:7], X[,9:ncol(X)])
X_list_2 &lt;- list(X[1:2,1:4], X[1:2,5:7], X[1:2,9:ncol(X)])

# EXEMPLE WITH SO-PLS-RDA
soplsrdacv(X_list, y, Xscaling = c("none", "pareto", "sd")[1], 
Yscaling = c("none", "pareto", "sd")[1], weights = NULL,
nlvlist=list(0:1, 1:2, 0:1), nbrep=1, cvmethod="loo", seed = 123, 
samplingk = NULL, nfolds = 3, optimisation = "global", 
criterion = c("err","rmse")[1], selection = "localmin")

ncomp &lt;- 2
fm &lt;- soplsrda(X_list, y, nlv = ncomp)
predict(fm,X_list_2)
transform(fm,X_list_2)

ncomp &lt;- c(2, 0, 3)
fm &lt;- soplsrda(X_list, y, nlv = ncomp)
predict(fm,X_list_2)
transform(fm,X_list_2)

ncomp &lt;- 0
fm &lt;- soplsrda(X_list, y, nlv = ncomp)
predict(fm,X_list_2)
transform(fm,X_list_2)

# EXEMPLE WITH SO-PLS-LDA
ncomp &lt;- 2
weights &lt;- rep(1 / n, n)
#w &lt;- 1:n
soplslda(X_list, y, Xscaling = "none", nlv = ncomp, weights = weights)
soplslda(X_list, y, Xscaling = "pareto", nlv = ncomp, weights = weights)
soplslda(X_list, y, Xscaling = "sd", nlv = ncomp, weights = weights)

fm &lt;- soplslda(X_list, y, Xscaling = c("none","pareto","sd"), nlv = ncomp, weights = weights)
predict(fm,X_list_2)
transform(fm,X_list_2)

</code></pre>

<hr>
<h2 id='sourcedir'>Source R functions in a directory</h2><span id='topic+sourcedir'></span>

<h3>Description</h3>

<p>Source all the R functions contained in a directory.</p>


<h3>Usage</h3>

<pre><code class='language-R'>sourcedir(path, trace = TRUE, ...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sourcedir_+3A_path">path</code></td>
<td>
<p>A character vector of full path names; the default corresponds to the working directory, <code>getwd()</code>.</p>
</td></tr>
<tr><td><code id="sourcedir_+3A_trace">trace</code></td>
<td>
<p>Logical. Default to <code>TRUE</code>. See the code.</p>
</td></tr>
<tr><td><code id="sourcedir_+3A_...">...</code></td>
<td>
<p>Additional arguments to pass in the function <code><a href="base.html#topic+list.files">list.files</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Sourcing.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
path &lt;- "D:/Users/Fun"
sourcedir(path, FALSE)


</code></pre>

<hr>
<h2 id='summ'>Description of the quantitative variables of a data set</h2><span id='topic+summ'></span>

<h3>Description</h3>

<p>Displays summary statistics for each quantitative column of the data set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>summ(X, nam = NULL, digits = 3)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summ_+3A_x">X</code></td>
<td>
<p>A matrix or data frame containing the variables to summarize.</p>
</td></tr>
<tr><td><code id="summ_+3A_nam">nam</code></td>
<td>
<p>Names of the variables to summarize (vector of character strings). Default to <code>NULL</code> (all the columns are considered).</p>
</td></tr>
<tr><td><code id="summ_+3A_digits">digits</code></td>
<td>
<p>Number of digits for the numerical outputs.</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>tab</code></td>
<td>
<p>A dataframe of summary statistics : <code class="reqn">NbVal</code>, <code class="reqn">Mean</code>, <code class="reqn">Min.</code>, <code class="reqn">Max.</code>, <code class="reqn">Stdev</code>, <code class="reqn">Median</code>, <code class="reqn">X1st.Qu.</code>, <code class="reqn">X3rd.Qu.</code>, <code class="reqn">NbNA</code></p>
</td></tr>
<tr><td><code>ntot</code></td>
<td>
<p>number of observations</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
dat &lt;- data.frame(
  v1 = rnorm(10),
  v2 = c(NA, rnorm(8), NA),
  v3 = c(NA, NA, NA, rnorm(7))
  )
dat

summ(dat)
summ(dat, nam = c("v1", "v3"))

</code></pre>

<hr>
<h2 id='svmr'>SVM Regression and Discrimination</h2><span id='topic+svmr'></span><span id='topic+svmda'></span><span id='topic+predict.Svm'></span><span id='topic+summary.Svm'></span>

<h3>Description</h3>

<p>SVM models with Gaussian (RBF) kernel
</p>
<p><code>svmr</code>: SVM regression (SVMR).
</p>
<p><code>svmda</code>: SVM discrimination (SVMC).
</p>
<p>The functions uses function <code><a href="e1071.html#topic+svm">svm</a></code> of package <code>e1071</code> (Meyer et al. 2021) available on CRAN (e1071 uses the tool box LIVSIM; Chang &amp; Lin, http://www.csie.ntu.edu.tw/~cjlin/libsvm). 
</p>
<p>The SVM models are fitted with parameterization <code class="reqn">'C'</code>, not the <code class="reqn">'nu'</code> parameterization. 
</p>
<p>The RBF kernel is defined by: exp(-gamma * |x - y|^2).
</p>
<p>For tuning the model, usual preliminary ranges are for instance:
</p>
<p>- cost = 10^(-5:15)
</p>
<p>- epsilon = seq(.1, .3, by = .1)
</p>
<p>- gamma = 10^(-6:3)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
svmr(X, y, cost = 1, epsilon = .1, gamma = 1, scale = FALSE)

svmda(X, y, cost = 1, epsilon = .1, gamma = 1, scale = FALSE)

## S3 method for class 'Svm'
predict(object, X, ...)  

## S3 method for class 'Svm'
summary(object, ...)  

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="svmr_+3A_x">X</code></td>
<td>
<p>For the main functions: Training X-data (<code class="reqn">n, p</code>). &mdash; For the auxiliary functions: New X-data (<code class="reqn">m, p</code>) to consider.</p>
</td></tr>
<tr><td><code id="svmr_+3A_y">y</code></td>
<td>
<p>Training Y-data (<code class="reqn">n</code>).</p>
</td></tr>
<tr><td><code id="svmr_+3A_cost">cost</code></td>
<td>
<p>The cost of constraints violation <code class="reqn">cost</code> parameter. See <code><a href="e1071.html#topic+svm">svm</a></code>.</p>
</td></tr>
<tr><td><code id="svmr_+3A_epsilon">epsilon</code></td>
<td>
<p>The <code class="reqn">epsilon</code> parameter in the insensitive-loss function. See <code><a href="e1071.html#topic+svm">svm</a></code>.</p>
</td></tr>
<tr><td><code id="svmr_+3A_gamma">gamma</code></td>
<td>
<p>The <code class="reqn">gamma</code> parameter in the RBF kernel.</p>
</td></tr>
<tr><td><code id="svmr_+3A_scale">scale</code></td>
<td>
<p>Logical. If <code>TRUE</code>, <code>X</code> and <code>Y</code> are scaled internally.</p>
</td></tr>
<tr><td><code id="svmr_+3A_object">object</code></td>
<td>
<p>For the auxiliary functions: A fitted model, output of a call to the main function.</p>
</td></tr>
<tr><td><code id="svmr_+3A_...">...</code></td>
<td>
<p>For the auxiliary functions: Optional arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>For <code>svmr</code> and <code>svmda</code>:
</p>
<table>
<tr><td><code>fm</code></td>
<td>
<p>list of outputs such as:
<code>call</code>; <code>type</code>; <code>kernel</code>; <code>cost</code>; <code>degree</code>; <code>gamma</code>; <code>coef0</code>; <code>nu</code>; <code>epsilon</code>; <code>sparse</code>; <code>scaled</code>; <code>x.scale</code>; <code>y.scale</code>; <code>nclasses</code>; <code>levels</code>; <code>tot.nSV</code>; <code>nSV</code>; <code>labels</code>; <code>SV</code>: The resulting support vectors (possibly scaled); <code>index</code>: The index of the resulting support vectors in the data matrix. Note that this index refers to the preprocessed data (after the possible effect of na.omit and subset); <code>rho</code>: The negative intercept; <code>compprob</code>; <code>probA, probB</code>: numeric vectors of length k(k-1)/2, k number of classes, containing the parameters of the logistic distributions fitted to the decision values of the binary classifiers (1 / (1 + exp(a x + b))); <code>sigma</code>: In case of a probabilistic regression model, the scale parameter of the hypothesized (zero-mean) laplace distribution estimated by maximum likelihood; <code>coefs</code>: The corresponding coefficients times the training labels; <code>na.action</code>; <code>fitted</code>; <code>decision.values</code>; <code>residuals</code>; <code>isnum</code>.</p>
</td></tr>
</table>
<p>For <code>predict.Svm</code>:
</p>
<table>
<tr><td><code>pred</code></td>
<td>
<p>predictions for each observation.</p>
</td></tr>
</table>
<p>For <code>summary.Svm</code>:display of call, parameters, and number of support vectors
</p>


<h3>Note</h3>

<p>The first example illustrates SVMR.
The second one is the example of fitting the function sinc(x) described in Rosipal &amp; Trejo 2001 p. 105-106.
The third one illustrates SVMC.
</p>


<h3>References</h3>

<p>Meyer, M. 2021 Support Vector Machines - The Interface to libsvm in package e1071. FH Technikum Wien, Austria, David.Meyer@R-Project.org. https://cran.r-project.org/web/packages/e1071/vignettes/svmdoc.pdf
</p>
<p>Chang, cost.-cost. &amp; Lin, cost.-J. (2001). LIBSVM: a library for support vector machines. Software available at http://www.csie.ntu.edu.tw/~cjlin/libsvm. Detailed documentation (algorithms, formulae, . . . ) can be found in http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.ps.gz
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## EXAMPLE 1 (SVMR)

n &lt;- 50 ; p &lt;- 4
Xtrain &lt;- matrix(rnorm(n * p), ncol = p)
ytrain &lt;- rnorm(n)
m &lt;- 3
Xtest &lt;- Xtrain[1:m, , drop = FALSE] 
ytest &lt;- ytrain[1:m]

fm &lt;- svmr(Xtrain, ytrain)
predict(fm, Xtest)

pred &lt;- predict(fm, Xtest)$pred
msep(pred, ytest)

summary(fm)

## EXAMPLE 2 

x &lt;- seq(-10, 10, by = .2)
x[x == 0] &lt;- 1e-5
n &lt;- length(x)
zy &lt;- sin(abs(x)) / abs(x)
y &lt;- zy + rnorm(n, 0, .2)
plot(x, y, type = "p")
lines(x, zy, lty = 2)
X &lt;- matrix(x, ncol = 1)

fm &lt;- svmr(X, y, gamma = .5)
pred &lt;- predict(fm, X)$pred
plot(X, y, type = "p")
lines(X, zy, lty = 2)
lines(X, pred, col = "red")

## EXAMPLE 3 (SVMC)

n &lt;- 50 ; p &lt;- 8
Xtrain &lt;- matrix(rnorm(n * p), ncol = p)
ytrain &lt;- sample(c("a", "10", "d"), size = n, replace = TRUE)
m &lt;- 5
Xtest &lt;- Xtrain[1:m, ] ; ytest &lt;- ytrain[1:m]

cost &lt;- 100 ; epsilon &lt;- .1 ; gamma &lt;- 1 
fm &lt;- svmda(Xtrain, ytrain,
    cost = cost, epsilon = epsilon, gamma = gamma)
predict(fm, Xtest)

pred &lt;- predict(fm, Xtest)$pred
err(pred, ytest)

summary(fm)

</code></pre>

<hr>
<h2 id='transform'>Generic transform function</h2><span id='topic+transform'></span>

<h3>Description</h3>

<p>Transformation of the X-data by a fitted model.</p>


<h3>Usage</h3>

<pre><code class='language-R'>
transform(object, X, ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="transform_+3A_object">object</code></td>
<td>
<p>A fitted model, output of a call to a fitting function.</p>
</td></tr>
<tr><td><code id="transform_+3A_x">X</code></td>
<td>
<p>New X-data to consider.</p>
</td></tr>
<tr><td><code id="transform_+3A_...">...</code></td>
<td>
<p>Optional arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the transformed X-data
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## EXAMPLE 1

n &lt;- 6 ; p &lt;- 4
X &lt;- matrix(rnorm(n * p), ncol = p)
y &lt;- rnorm(n)

fm &lt;- pcaeigen(X, nlv = 3)

fm$T
transform(fm, X[1:2, ], nlv = 2)

## EXAMPLE 2

n &lt;- 6 ; p &lt;- 4
X &lt;- matrix(rnorm(n * p), ncol = p)
y &lt;- rnorm(n)

fm &lt;- plskern(X, y, nlv = 3)
fm$T
transform(fm, X[1:2, ], nlv = 2)

</code></pre>

<hr>
<h2 id='vip'>Variable Importance in Projection (VIP)</h2><span id='topic+vip'></span>

<h3>Description</h3>

<p><code>vip</code> calculates the Variable Importance in Projection (VIP) for a PLS model. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
vip(object, X, Y = NULL, nlv = NULL)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vip_+3A_object">object</code></td>
<td>
<p>A fitted model, output of a call to a fitting function among <code>plskern</code>, <code>plsnipals</code>, <code>plsrannar</code>, <code>plsrda</code>, <code>plslda</code>), <code>plsqda</code>).</p>
</td></tr>
<tr><td><code id="vip_+3A_x">X</code></td>
<td>
<p>X-data involved in the fitted model</p>
</td></tr>
<tr><td><code id="vip_+3A_y">Y</code></td>
<td>
<p>Y-data involved in the fitted model.
If <code>Y</code> is NULL (default value), the VIP calculation is based on the proportion of Y-variance explained by the components, as proposed by Mehmood et al (2012, 2020).
If <code>Y</code> is not NULL, the VIP calculation is based on the redundancy, as proposed by Tenenhaus (1998).</p>
</td></tr>
<tr><td><code id="vip_+3A_nlv">nlv</code></td>
<td>
<p>Number of components (LVs) to consider.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>matrix (<code class="reqn">(q,nlv)</code>) with VIP values, for models with 1 to nlv latent variables.
</p>


<h3>References</h3>

<p>Mehmood, T.,Liland, K.H.,Snipen, L.,Sæbø, S., 2012. A review of variable selection methods in Partial Least Squares Regression. Chemometrics and Intelligent Laboratory Systems, 118, 62-69. 
</p>
<p>Mehmood, T., Sæbø, S.,Liland, K.H., 2020. Comparison of variable selection methods in partial least squares regression. Journal of Chemometrics, 34, e3226.
</p>
<p>Tenenhaus, M., 1998. La régression PLS: théorie et pratique. Editions Technip, Paris, France.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## EXAMPLE OF PLS

n &lt;- 50 ; p &lt;- 4
Xtrain &lt;- matrix(rnorm(n * p), ncol = p)
ytrain &lt;- rnorm(n)
Ytrain &lt;- cbind(y1 = ytrain, y2 = 100 * ytrain)
m &lt;- 3
Xtest &lt;- Xtrain[1:m, , drop = FALSE] 
Ytest &lt;- Ytrain[1:m, , drop = FALSE] ; ytest &lt;- Ytest[1:m, 1]

nlv &lt;- 3
fm &lt;- plskern(Xtrain, Ytrain, nlv = nlv)
vip(fm, Xtrain, Ytrain, nlv = nlv)
vip(fm, Xtrain, nlv = nlv)

fm &lt;- plskern(Xtrain, ytrain, nlv = nlv)
vip(fm, Xtrain, ytrain, nlv = nlv)
vip(fm, Xtrain, nlv = nlv)

## EXAMPLE OF PLSDA

n &lt;- 50 ; p &lt;- 8
Xtrain &lt;- matrix(rnorm(n * p), ncol = p)
ytrain &lt;- sample(c("1", "4", "10"), size = n, replace = TRUE)

Xtest &lt;- Xtrain[1:5, ] ; ytest &lt;- ytrain[1:5]

nlv &lt;- 5
fm &lt;- plsrda(Xtrain, ytrain, nlv = nlv)
vip(fm, Xtrain, ytrain, nlv = nlv)

</code></pre>

<hr>
<h2 id='wdist'>Distance-based weights</h2><span id='topic+wdist'></span>

<h3>Description</h3>

<p>Calculation of weights from a vector of distances using a decreasing inverse exponential function.
</p>
<p>Let <code class="reqn">d</code> be a vector of distances. 
</p>
<p>1- Preliminary weights are calculated by <code class="reqn">w = exp(-d / (h * mad(d)))</code> , where <code class="reqn">h</code> is a scalar &gt; 0 (scale factor).  
</p>
<p>2- The weights corresponding to distances higher than <code class="reqn">median(d) + cri * mad(d)</code>, where <code class="reqn">cri</code> is a scalar &gt; 0, are set to zero. This step is used for removing outliers. 
</p>
<p>3- Finally, the weights are &quot;normalized&quot; between 0 and 1 by <code class="reqn">w = w / max(w)</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>wdist(d, h, cri = 4, squared = FALSE)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="wdist_+3A_d">d</code></td>
<td>
<p>A vector of distances.</p>
</td></tr>
<tr><td><code id="wdist_+3A_h">h</code></td>
<td>
<p>A scaling factor (positive scalar). Lower is <code class="reqn">h</code>, sharper is the decreasing function. See the examples.</p>
</td></tr>
<tr><td><code id="wdist_+3A_cri">cri</code></td>
<td>
<p>A positive scalar used for defining outliers in the distances vector.</p>
</td></tr>
<tr><td><code id="wdist_+3A_squared">squared</code></td>
<td>
<p>Logical. If <code>TRUE</code>, distances <code class="reqn">d</code> are replaced by the squared distances in the decreasing function, which corresponds to a Gaussian (RBF) kernel function. Default to <code class="reqn">FALSE)</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of weights.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
x1 &lt;- sqrt(rchisq(n = 100, df = 10))
x2 &lt;- sqrt(rchisq(n = 10, df = 40))
d &lt;- c(x1, x2)
h &lt;- 2 ; cri &lt;- 3
w &lt;- wdist(d, h = h, cri = cri)

oldpar &lt;- par(mfrow = c(1, 1))
par(mfrow = c(2, 2))
plot(d)
hist(d, n = 50)
plot(w, ylim = c(0, 1)) ; abline(h = 1, lty = 2)
plot(d, w, ylim = c(0, 1)) ; abline(h = 1, lty = 2)
par(oldpar)

d &lt;- seq(0, 15, by = .5)
h &lt;- c(.5, 1, 1.5, 2.5, 5, 10, Inf)
for(i in 1:length(h)) {
  w &lt;- wdist(d, h = h[i])
  z &lt;- data.frame(d = d, w = w, h = rep(h[i], length(d)))
  if(i == 1) res &lt;- z else res &lt;- rbind(res, z)
  }
res$h &lt;- as.factor(res$h)
headm(res)
plotxy(res[, c("d", "w")], asp = 0, group = res$h, pch = 16)

</code></pre>

<hr>
<h2 id='xfit'>Matrix fitting from a PCA or PLS model</h2><span id='topic+xfit'></span><span id='topic+xfit.Pca'></span><span id='topic+xfit.Plsr'></span><span id='topic+xresid'></span>

<h3>Description</h3>

<p>Function <code>xfit</code> calculates an approximate of matrix <code class="reqn">X</code> (<code class="reqn">X_fit</code>) from a PCA or PLS fitted on <code class="reqn">X</code>.
</p>
<p>Function <code>xresid</code> calculates the residual matrix <code class="reqn">E = X - X_fit</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
xfit(object, X, ...)

## S3 method for class 'Pca'
xfit(object, X, ..., nlv = NULL) 

## S3 method for class 'Plsr'
xfit(object, X, ..., nlv = NULL) 

xresid(object, X, ..., nlv = NULL)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="xfit_+3A_object">object</code></td>
<td>
<p>A fitted model, output of a call to a fitting function.</p>
</td></tr>
<tr><td><code id="xfit_+3A_x">X</code></td>
<td>
<p>The X-data that was used to fit the model <code>object</code>.</p>
</td></tr>
<tr><td><code id="xfit_+3A_nlv">nlv</code></td>
<td>
<p>Number of components (PCs or LVs) to consider.</p>
</td></tr>
<tr><td><code id="xfit_+3A_...">...</code></td>
<td>
<p>Optional arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>For <code>xfit</code>:matrix of fitted values.
</p>
<p>For <code>xresid</code>:matrix of residuals.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
n &lt;- 6 ; p &lt;- 4
X &lt;- matrix(rnorm(n * p), ncol = p)
y &lt;- rnorm(n)

nlv &lt;- 3
fm &lt;- pcasvd(X, nlv = nlv)
xfit(fm, X)
xfit(fm, X, nlv = 1)
xfit(fm, X, nlv = 0)

X - xfit(fm, X)
xresid(fm, X)

X - xfit(fm, X, nlv = 1)
xresid(fm, X, nlv = 1)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
