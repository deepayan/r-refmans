<!DOCTYPE html><html lang="en"><head><title>Help for package pemultinom</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {pemultinom}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#cv.pemultinom'><p>Fit a multinomial regression model with Lasso penalty.</p></a></li>
<li><a href='#debiased_lasso'><p>Doing statistical inference on l1-penalized multinomial regression via debiased Lasso (or desparisified Lasso).</p></a></li>
<li><a href='#predict_pemultinom'><p>Make predictions on new predictors based on fitted l1-penalized multinomial regression model.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>L1-Penalized Multinomial Regression with Statistical Inference</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.1</td>
</tr>
<tr>
<td>Description:</td>
<td>We aim for fitting a multinomial regression model with Lasso penalty and doing statistical inference (calculating confidence intervals of coefficients and p-values for individual variables). It implements 1) the coordinate descent algorithm to fit an l1-penalized multinomial regression model (parameterized with a reference level); 2) the debiasing approach to obtain the inference results, which is described in "Tian, Y., Rusinek, H., Masurkar, A. V., &amp; Feng, Y. (2024). L1‐Penalized Multinomial Regression: Estimation, Inference, and Prediction, With an Application to Risk Factor Identification for Different Dementia Subtypes. Statistics in Medicine, 43(30), 5711-5747."</td>
</tr>
<tr>
<td>Imports:</td>
<td>foreach, doParallel, stats, Rcpp, nnet, magrittr, utils,
lpSolve</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.2</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2025-02-27 21:17:32 UTC; yetian</td>
</tr>
<tr>
<td>Author:</td>
<td>Ye Tian [aut, cre],
  Henry Rusinek [aut],
  Arjun V. Masurkar [aut],
  Yang Feng [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Ye Tian &lt;ye.t@columbia.edu&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2025-02-28 09:40:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='cv.pemultinom'>Fit a multinomial regression model with Lasso penalty.</h2><span id='topic+cv.pemultinom'></span>

<h3>Description</h3>

<p>Fit a multinomial regression model with Lasso penalty. This function implements the l1-penalized multinomial regression model (parameterized with a reference level). A cross-validation procedure is applied to choose the tuning parameter. See Tian et al. (2024) for details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cv.pemultinom(
  x,
  y,
  ref = NULL,
  nfolds = 5,
  nlambda = 100,
  max_iter = 200,
  tol = 0.001,
  ncores = 1,
  standardized = TRUE,
  weights = NULL,
  info = TRUE,
  lambda_min_ratio = 0.01
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cv.pemultinom_+3A_x">x</code></td>
<td>
<p>the design/predictor matrix, each row of which is an observation vector.</p>
</td></tr>
<tr><td><code id="cv.pemultinom_+3A_y">y</code></td>
<td>
<p>the response variable. Can be of one type from factor/integer/character.</p>
</td></tr>
<tr><td><code id="cv.pemultinom_+3A_ref">ref</code></td>
<td>
<p>the reference level. Default = NULL, which sets the reference level to the last category (sorted by alphabetical order)</p>
</td></tr>
<tr><td><code id="cv.pemultinom_+3A_nfolds">nfolds</code></td>
<td>
<p>the number of cross-validation folds to use. Default = 5.</p>
</td></tr>
<tr><td><code id="cv.pemultinom_+3A_nlambda">nlambda</code></td>
<td>
<p>the number of penalty parameter candidates. Default = 100.</p>
</td></tr>
<tr><td><code id="cv.pemultinom_+3A_max_iter">max_iter</code></td>
<td>
<p>the maximum iteration rounds in each iteration of the coordinate descent algorithm. Default = 200.</p>
</td></tr>
<tr><td><code id="cv.pemultinom_+3A_tol">tol</code></td>
<td>
<p>convergence threshold (tolerance level) for coordinate descent. Default = 1e-3.</p>
</td></tr>
<tr><td><code id="cv.pemultinom_+3A_ncores">ncores</code></td>
<td>
<p>the number of cores to use for parallel computing. Default = 1.</p>
</td></tr>
<tr><td><code id="cv.pemultinom_+3A_standardized">standardized</code></td>
<td>
<p>logical flag for x variable standardization, prior to fitting the model sequence. Default = TRUE. Note that the fitting results will be translated to the original scale before output.</p>
</td></tr>
<tr><td><code id="cv.pemultinom_+3A_weights">weights</code></td>
<td>
<p>observation weights. Should be a vector of non-negative numbers of length n (the number of observations). Default = NULL, which sets equal weights for all observations.</p>
</td></tr>
<tr><td><code id="cv.pemultinom_+3A_info">info</code></td>
<td>
<p>whether to print the information or not. Default = TRUE.</p>
</td></tr>
<tr><td><code id="cv.pemultinom_+3A_lambda_min_ratio">lambda_min_ratio</code></td>
<td>
<p>the ratio between lambda.min and lambda.max, where lambda.max is automatically determined by the code and the lambda sequence will be determined by 'exp(seq(log(lambda.max), log(lambda.min), len = nlambda))'.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with the following components.
</p>
<table role = "presentation">
<tr><td><code>beta.list</code></td>
<td>
<p>the estimates of coefficients. It is a list of which the k-th component is the contrast coefficient between class k and the reference class corresponding to different lambda values. The j-th column of each list component corresponds to the j-th lambda value.</p>
</td></tr>
<tr><td><code>beta.1se</code></td>
<td>
<p>the coefficient estimate corresponding to lambda.1se. It is a matrix, and the k-th column is the contrast coefficient between class k and the reference class.</p>
</td></tr>
<tr><td><code>beta.min</code></td>
<td>
<p>the coefficient estimate corresponding to lambda.min. It is a matrix, and the k-th column is the contrast coefficient between class k and the reference class.</p>
</td></tr>
<tr><td><code>lambda.1se</code></td>
<td>
<p>the largest value of lambda such that error is within 1 standard error of the minimum. See Chapter 2.3 of Hastie et al. (2015) for more details.</p>
</td></tr>
<tr><td><code>lambda.min</code></td>
<td>
<p>the value of lambda that gives minimum cvm.</p>
</td></tr>
<tr><td><code>cvm</code></td>
<td>
<p>the weights in objective function.</p>
</td></tr>
<tr><td><code>cvsd</code></td>
<td>
<p>the estimated marginal probability for each class.</p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>
<p>lambda values considered in the cross-validation process.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Hastie, T., Tibshirani, R., &amp; Wainwright, M. (2015). Statistical learning with sparsity. Monographs on statistics and applied probability, 143.
</p>
<p>Tian, Y., Rusinek, H., Masurkar, A. V., &amp; Feng, Y. (2024). L1‐Penalized Multinomial Regression: Estimation, Inference, and Prediction, With an Application to Risk Factor Identification for Different Dementia Subtypes. Statistics in Medicine, 43(30), 5711-5747.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+debiased_lasso">debiased_lasso</a></code>, <code><a href="#topic+predict_pemultinom">predict_pemultinom</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># generate data from a logistic regression model with n = 100, p = 50, and K = 3
set.seed(0, kind = "L'Ecuyer-CMRG")
n &lt;- 100
p &lt;- 50
K &lt;- 3

Sigma &lt;- outer(1:p, 1:p, function(x,y) {
  0.9^(abs(x-y))
})
R &lt;- chol(Sigma)
s &lt;- 3
beta_coef &lt;- matrix(0, nrow = p+1, ncol = K-1)
beta_coef[1+1:s, 1] &lt;- c(3, 3, 3)
beta_coef[1+1:s+s, 2] &lt;- c(3, 3, 3)

x &lt;- matrix(rnorm(n*p), ncol = p) %*% R
y &lt;- sapply(1:n, function(j){
  prob_i &lt;- c(sapply(1:(K-1), function(k){
    exp(sum(x[j, ]*beta_coef[-1, k]))
  }), 1)
  prob_i &lt;- prob_i/sum(prob_i)
  sample(1:K, size = 1, replace = TRUE, prob = prob_i)
})

# fit the l1-penalized multinomial regression model
fit &lt;- cv.pemultinom(x, y, ncores = 2)
beta &lt;- fit$beta.min

# generate test data from the same model
x.test &lt;- matrix(rnorm(n*p), ncol = p) %*% R
y.test &lt;- sapply(1:n, function(j){
  prob_i &lt;- c(sapply(1:(K-1), function(k){
    exp(sum(x.test[j, ]*beta_coef[-1, k]))
  }), 1)
  prob_i &lt;- prob_i/sum(prob_i)
  sample(1:K, size = 1, replace = TRUE, prob = prob_i)
})

# predict labels of test data and calculate the misclassification error rate (using beta.min)
ypred.min &lt;- predict_pemultinom(fit$beta.min, ref = 3, xnew = x.test, type = "class")
mean(ypred.min != y.test)

# predict labels of test data and calculate the misclassification error rate (using beta.1se)
ypred.1se &lt;- predict_pemultinom(fit$beta.1se, ref = 3, xnew = x.test, type = "class")
mean(ypred.1se != y.test)

# predict posterior probabilities of test data
ypred.prob &lt;- predict_pemultinom(fit$beta.min, ref = 3, xnew = x.test, type = "prob")
</code></pre>

<hr>
<h2 id='debiased_lasso'>Doing statistical inference on l1-penalized multinomial regression via debiased Lasso (or desparisified Lasso).</h2><span id='topic+debiased_lasso'></span>

<h3>Description</h3>

<p>Doing statistical inference on l1-penalized multinomial regression via debiased Lasso (or desparisified Lasso). This function implements the algorithm described in Tian et al. (2024), which is an extension of the original debiased Lasso (Van de Geer et al. 2014; Zhang and Zhang. 2014; Javanmard, A. and Montanari, A. (2014)) to the multinomial case.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>debiased_lasso(
  x,
  y,
  ref = NULL,
  beta = NULL,
  nfolds = 5,
  ncores = 1,
  nlambda = 50,
  max_iter = 200,
  tol = 0.001,
  lambda.choice = "lambda.min",
  alpha = 0.05,
  method = c("nodewise_reg", "LP"),
  info = TRUE,
  LP.parameter = list(eta = NULL, split.ratio = 0.5),
  lambda_min_ratio = 0.01
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="debiased_lasso_+3A_x">x</code></td>
<td>
<p>the design/predictor matrix, each row of which is an observation vector.</p>
</td></tr>
<tr><td><code id="debiased_lasso_+3A_y">y</code></td>
<td>
<p>the response variable. Can be of one type from factor/integer/character.</p>
</td></tr>
<tr><td><code id="debiased_lasso_+3A_ref">ref</code></td>
<td>
<p>the reference level. Default = NULL, which sets the same reference level as used in obtaining <code>beta</code>. Even when the user inputs <code>ref</code> manually, it should be always the same reference level as used in obtaining <code>beta</code>.</p>
</td></tr>
<tr><td><code id="debiased_lasso_+3A_beta">beta</code></td>
<td>
<p>the beta estimate from l1-penalized multinomial regression. Should be in the same format as <code>beta.min</code> or <code>beta.1se</code> in output of function <code><a href="#topic+cv.pemultinom">cv.pemultinom</a></code>. The user is recommended to directly pass <code>beta.min</code> or <code>beta.1se</code> from the output of function <code><a href="#topic+cv.pemultinom">cv.pemultinom</a></code> to parameter <code>beta</code>.</p>
</td></tr>
<tr><td><code id="debiased_lasso_+3A_nfolds">nfolds</code></td>
<td>
<p>the number of cross-validation folds to use. Cross-validation is used to determine the best tuning parameter lambda in the nodewise regression, i.e., Algorithm 2 in Tian et al. (2024). Default = 5.</p>
</td></tr>
<tr><td><code id="debiased_lasso_+3A_ncores">ncores</code></td>
<td>
<p>the number of cores to use for parallel computing. Default = 1.</p>
</td></tr>
<tr><td><code id="debiased_lasso_+3A_nlambda">nlambda</code></td>
<td>
<p>the number of penalty parameter candidates in the cross-validation procedure. Cross-validation is used to determine the best tuning parameter lambda in the nodewise regression, i.e., Algorithm 2 in Tian et al. (2024). Default = 100.</p>
</td></tr>
<tr><td><code id="debiased_lasso_+3A_max_iter">max_iter</code></td>
<td>
<p>the maximum iteration rounds in each iteration of the coordinate descent algorithm. Default = 200.</p>
</td></tr>
<tr><td><code id="debiased_lasso_+3A_tol">tol</code></td>
<td>
<p>convergence threshold (tolerance level) for coordinate descent. Default = 1e-3.</p>
</td></tr>
<tr><td><code id="debiased_lasso_+3A_lambda.choice">lambda.choice</code></td>
<td>
<p>the choice of the tuning parameter lambda used in the nodewise regression. It can only be either &quot;lambda.min&quot; or &quot;lambda.1se&quot;. The interpretation is the same as in the 'cv.pemultinom' function. Default = &quot;lambda.min&quot;.</p>
</td></tr>
<tr><td><code id="debiased_lasso_+3A_alpha">alpha</code></td>
<td>
<p>significance level used in the output confidence interval. Has to be a number between 0 and 1. Default = 0.05.</p>
</td></tr>
<tr><td><code id="debiased_lasso_+3A_method">method</code></td>
<td>
<p>which method to estimate the Hessian inverse matrix. Can be either &quot;nodewise_reg&quot; or &quot;LP&quot;.
</p>

<ul>
<li><p> nodewise_reg: the method presented in the main text of Tian et al. (2024).
</p>
</li>
<li><p> LP: the method presented in Section A of the supplements of Tian et al. (2024). Warning: This method is not well implemented as 'eta' parameter has to be set as fixed and currently cannot be automatically tuned.
</p>
</li></ul>
</td></tr>
<tr><td><code id="debiased_lasso_+3A_info">info</code></td>
<td>
<p>whether to print the information or not. Default = TRUE.</p>
</td></tr>
<tr><td><code id="debiased_lasso_+3A_lp.parameter">LP.parameter</code></td>
<td>
<p>If <code>method</code> = 'LP', then this parameter will be used. It has to be a list of two components like 'list(eta = NULL, split.ratio = 0.5)'. Here is the interpretation:
</p>

<ul>
<li><p> eta: This is the parameter used in the LP method which is the righthand side of the contraints. Default 'eta' = 'NULL', which will be set as <code class="reqn">2\sqrt{\log(p)/n}</code> where p is the number of features and n is the total sample size.
</p>
</li>
<li><p> split.ratio: The split ratio used in the LP method. The data will be splitted into two parts by class based on 'split.ratio'. The first part will be used to fit the Lasso estimator, and the second part will be used to fit the Hessian inverse through the LP method. Default 'split.ratio' = 0.5.
</p>
</li></ul>
</td></tr>
<tr><td><code id="debiased_lasso_+3A_lambda_min_ratio">lambda_min_ratio</code></td>
<td>
<p>the ratio between lambda.min and lambda.max used for the Lasso problem in the nodewise regression (for estimating the Hessian inverse), where lambda.max is automatically determined by the code and the lambda sequence will be determined by 'exp(seq(log(lambda.max), log(lambda.min), len = nlambda))'.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of data frames, each of which contains the inference results for each class (v.s. the reference class). In the data frame, each row represents a variable. The columns include:
</p>
<table role = "presentation">
<tr><td><code>beta</code></td>
<td>
<p>the debiased point estimate of the coefficient</p>
</td></tr>
<tr><td><code>p_value</code></td>
<td>
<p>p value of each variable</p>
</td></tr>
<tr><td><code>CI_lower</code></td>
<td>
<p>lower endpoint of the confidence interval for each coefficient</p>
</td></tr>
<tr><td><code>CI_upper</code></td>
<td>
<p>upper endpoint of the confidence interval for each coefficient</p>
</td></tr>
<tr><td><code>std_dev</code></td>
<td>
<p>the estimated standard deviation of each component of beta estimate</p>
</td></tr>
</table>


<h3>References</h3>

<p>Tian, Y., Rusinek, H., Masurkar, A. V., &amp; Feng, Y. (2024). L1‐Penalized Multinomial Regression: Estimation, Inference, and Prediction, With an Application to Risk Factor Identification for Different Dementia Subtypes. Statistics in Medicine, 43(30), 5711-5747.
</p>
<p>Van de Geer, S., Bühlmann, P., Ritov, Y. A., &amp; Dezeure, R. (2014). On asymptotically optimal confidence regions and tests for high-dimensional models. The Annals of Statistics, 42(3), 1166-1202.
</p>
<p>Zhang, C. H., &amp; Zhang, S. S. (2014). Confidence intervals for low dimensional parameters in high dimensional linear models. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 76(1), 217-242.
</p>
<p>Javanmard, A., &amp; Montanari, A. (2014). Confidence intervals and hypothesis testing for high-dimensional regression. The Journal of Machine Learning Research, 15(1), 2869-2909.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cv.pemultinom">cv.pemultinom</a></code>, <code><a href="#topic+predict_pemultinom">predict_pemultinom</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# generate data from a logistic regression model with n = 100, p = 50, and K = 3
set.seed(0, kind = "L'Ecuyer-CMRG")
n &lt;- 100
p &lt;- 50
K &lt;- 3

Sigma &lt;- outer(1:p, 1:p, function(x,y) {
  0.9^(abs(x-y))
})
R &lt;- chol(Sigma)
s &lt;- 3
beta_coef &lt;- matrix(0, nrow = p+1, ncol = K-1)
beta_coef[1+1:s, 1] &lt;- c(3, 3, 3)
beta_coef[1+1:s+s, 2] &lt;- c(3, 3, 3)

x &lt;- matrix(rnorm(n*p), ncol = p) %*% R
y &lt;- sapply(1:n, function(j){
  prob_i &lt;- c(sapply(1:(K-1), function(k){
    exp(sum(x[j, ]*beta_coef[-1, k]))
  }), 1)
  prob_i &lt;- prob_i/sum(prob_i)
  sample(1:K, size = 1, replace = TRUE, prob = prob_i)
})

# fit the l1-penalized multinomial regression model
fit &lt;- cv.pemultinom(x, y, ncores = 2)
beta &lt;- fit$beta.min

# run the debiasing approach
fit_debiased &lt;- debiased_lasso(x, y, beta = beta, ncores = 2)

</code></pre>

<hr>
<h2 id='predict_pemultinom'>Make predictions on new predictors based on fitted l1-penalized multinomial regression model.</h2><span id='topic+predict_pemultinom'></span>

<h3>Description</h3>

<p>Make predictions on new predictors based on fitted l1-penalized multinomial regression model, by using the fitted beta.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>predict_pemultinom(beta, ref, xnew, type = c("class", "prob"))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict_pemultinom_+3A_beta">beta</code></td>
<td>
<p>the beta estimate from l1-penalized multinomial regression. Should be in the same format as <code>beta.min</code> or <code>beta.1se</code> in output of function <code><a href="#topic+cv.pemultinom">cv.pemultinom</a></code>. The user is recommended to directly pass <code>beta.min</code> or <code>beta.1se</code> from the output of function <code><a href="#topic+cv.pemultinom">cv.pemultinom</a></code> to parameter <code>beta</code>.</p>
</td></tr>
<tr><td><code id="predict_pemultinom_+3A_ref">ref</code></td>
<td>
<p>the reference level, which should be the same reference level as used in obtaining <code>beta</code>. An input is required.</p>
</td></tr>
<tr><td><code id="predict_pemultinom_+3A_xnew">xnew</code></td>
<td>
<p>new observations to predict labels for. Should be a matrix or a data frame, where each row and column represents an observation and predictor, respectively.</p>
</td></tr>
<tr><td><code id="predict_pemultinom_+3A_type">type</code></td>
<td>
<p>the type of prediction output. Can be 'class' or 'prob'. Default = 'class'.
</p>

<ul>
<li><p> class: the predicted class/label.
</p>
</li>
<li><p> prob: the predicted probability for each class.
</p>
</li></ul>
</td></tr>
</table>


<h3>Value</h3>

<p>When <code>type</code> = 'class', return a vector. When <code>type</code> = 'prob', return a matrix where each row and column represent an observation and a probability of that class, respectively. Default = 'class'.
</p>


<h3>References</h3>

<p>Tian, Y., Rusinek, H., Masurkar, A. V., &amp; Feng, Y. (2023). L1-penalized Multinomial Regression: Estimation, inference, and prediction, with an application to risk factor identification for different dementia subtypes. arXiv preprint arXiv:2302.02310.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cv.pemultinom">cv.pemultinom</a></code>, <code><a href="#topic+debiased_lasso">debiased_lasso</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># generate training data from Model 1 in Tian et al. (2023) with n = 50 and p = 50
set.seed(1, kind = "L'Ecuyer-CMRG")
n &lt;- 50
p &lt;- 50
K &lt;- 3

Sigma &lt;- outer(1:p, 1:p, function(x,y) {
  0.9^(abs(x-y))
})
R &lt;- chol(Sigma)
s &lt;- 3
beta_coef &lt;- matrix(0, nrow = p+1, ncol = K-1)
beta_coef[1+1:s, 1] &lt;- c(1.5, 1.5, 1.5)
beta_coef[1+1:s+s, 2] &lt;- c(1.5, 1.5, 1.5)

x &lt;- matrix(rnorm(n*p), ncol = p) %*% R
y &lt;- sapply(1:n, function(j){
  prob_i &lt;- c(sapply(1:(K-1), function(k){
    exp(sum(x[j, ]*beta_coef[-1, k]))
  }), 1)
  prob_i &lt;- prob_i/sum(prob_i)
  sample(1:K, size = 1, replace = TRUE, prob = prob_i)
})

# fit the l1-penalized multinomial regression model
fit &lt;- cv.pemultinom(x, y, ncores = 2)

# generate test data from the same model
x.test &lt;- matrix(rnorm(n*p), ncol = p) %*% R
y.test &lt;- sapply(1:n, function(j){
  prob_i &lt;- c(sapply(1:(K-1), function(k){
    exp(sum(x.test[j, ]*beta_coef[-1, k]))
  }), 1)
  prob_i &lt;- prob_i/sum(prob_i)
  sample(1:K, size = 1, replace = TRUE, prob = prob_i)
})

# predict labels of test data and calculate the misclassification error rate (using beta.min)
ypred.min &lt;- predict_pemultinom(fit$beta.min, ref = 3, xnew = x.test, type = "class")
mean(ypred.min != y.test)

# predict labels of test data and calculate the misclassification error rate (using beta.1se)
ypred.1se &lt;- predict_pemultinom(fit$beta.1se, ref = 3, xnew = x.test, type = "class")
mean(ypred.1se != y.test)

# predict posterior probabilities of test data
ypred.prob &lt;- predict_pemultinom(fit$beta.min, ref = 3, xnew = x.test, type = "prob")
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
