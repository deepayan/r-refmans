<!DOCTYPE html><html><head><title>Help for package imageseg</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {imageseg}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#imageseg-package'>
<p>Overview of the imageseg package</p></a></li>
<li><a href='#dataAugmentation'><p>Data augmentation: rotating and mirroring images, and adjusting colors</p></a></li>
<li><a href='#findValidRegion'><p>Subset image to valid (informative) region</p></a></li>
<li><a href='#imageSegmentation'><p>Model predictions from images based on TensorFlow model</p></a></li>
<li><a href='#imagesToKerasInput'><p>Convert magick images in tibble to array for keras</p></a></li>
<li><a href='#loadImages'><p>Load image files with magick</p></a></li>
<li><a href='#loadModel'><p>Load TensorFlow model from hdf5 file</p></a></li>
<li><a href='#resizeImages'><p>Resize and save images</p></a></li>
<li><a href='#u_net'><p>Create a U-Net architecture</p></a></li>
<li><a href='#u_net_plusplus'><p>Create a U-Net++ architecture</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Deep Learning Models for Image Segmentation</td>
</tr>
<tr>
<td>Version:</td>
<td>0.5.0</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Juergen Niedballa &lt;niedballa@izw-berlin.de&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>A general-purpose workflow for image segmentation using TensorFlow models based on the U-Net architecture by Ronneberger et al. (2015) &lt;<a href="https://doi.org/10.48550/arXiv.1505.04597">doi:10.48550/arXiv.1505.04597</a>&gt; and the U-Net++ architecture by Zhou et al. (2018) &lt;<a href="https://doi.org/10.48550/arXiv.1807.10165">doi:10.48550/arXiv.1807.10165</a>&gt;. We provide pre-trained models for assessing canopy density and understory vegetation density from vegetation photos. In addition, the package provides a workflow for easily creating model input and model architectures for general-purpose image segmentation based on grayscale or color images, both for binary and multi-class image segmentation.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/EcoDynIZW/imageseg/issues">https://github.com/EcoDynIZW/imageseg/issues</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Imports:</td>
<td>grDevices, keras, magick, magrittr, methods, purrr, stats,
tibble, foreach, parallel, doParallel, dplyr</td>
</tr>
<tr>
<td>Suggests:</td>
<td>R.rsp, testthat</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>R.rsp</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.2</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-05-29 10:38:56 UTC; Juergen</td>
</tr>
<tr>
<td>Author:</td>
<td>Juergen Niedballa <a href="https://orcid.org/0000-0002-9187-2116"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre],
  Jan Axtner <a href="https://orcid.org/0000-0003-1269-5586"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut],
  Leibniz Institute for Zoo and Wildlife Research [cph]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-05-29 22:40:12 UTC</td>
</tr>
</table>
<hr>
<h2 id='imageseg-package'>
Overview of the imageseg package
</h2><span id='topic+imageseg-package'></span><span id='topic+imageseg'></span>

<h3>Description</h3>

<p>This package provides a streamlined workflow for image segmentation using deep learning models based on the U-Net architecture by Ronneberger (2015) and the U-Net++ architecture by Zhou et al. (2018). Image segmentation is the labelling of each pixel in a images with class labels. Models are convolutional neural networks implemented in <span class="pkg">keras</span> using a TensorFlow backend. The workflow supports grayscale and color images as input, and binary or multi-class output.
</p>
<p>We provide pre-trained models for two forest structural metrics: canopy density and understory vegetation density. These trained models were trained with large and diverse training data sets, allowing for robust inferences. The package workflow is implemented in a few function, allowing for simple predictions on your own images without specialist knowledge of convolutional neural networks.
</p>
<p>If you have training data available, you can also create and train your own models, or continue model training on the pre-trained models.
</p>
<p>The workflow implemented here can also be used for other image segmentation tasks, e.g. in the cell biology or for medical images. We provide two examples in the package vignette (bacteria detection in darkfield microscopy from color images, breast cancer detection in grayscale ultrasound images).
</p>


<h3>Functions for model predictions</h3>

<p>The following functions are used to perform image segmentation on your images. They resize images, load them into R, convert them to model input, load the model and perform predictions. The functions are given in the order they would typically be run. See the vignette for complete examples.
</p>

<table>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+findValidRegion">findValidRegion</a></code> </td><td style="text-align: left;"> Subset image to valid (informative) region (optional) </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+resizeImages">resizeImages</a></code> </td><td style="text-align: left;"> Resize and save images </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+loadImages">loadImages</a></code> </td><td style="text-align: left;">  Load image files with magick </td>
</tr>
<tr>
 <td style="text-align: left;">

<code><a href="#topic+imagesToKerasInput">imagesToKerasInput</a></code> </td><td style="text-align: left;"> Convert magick images to array for keras </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+loadModel">loadModel</a></code> </td><td style="text-align: left;"> Load TensorFlow model from hdf5 file </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+imageSegmentation">imageSegmentation</a></code> </td><td style="text-align: left;"> Model predictions from images based on TensorFlow model </td>
</tr>
<tr>
 <td style="text-align: left;">

</td>
</tr>

</table>



<h3>Functions for model training</h3>

<p>This function assist in creating models in keras based on the U-Net architecture. See the vignette for complete examples.
</p>

<table>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+dataAugmentation">dataAugmentation</a></code> </td><td style="text-align: left;"> Rotating and mirroring images, and modulating colors </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+u_net">u_net</a></code>  </td><td style="text-align: left;"> Create a U-Net architecture </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+u_net_plusplus">u_net_plusplus</a></code>  </td><td style="text-align: left;"> Create a U-Net++ architecture </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>



<h3>Download pre-trained models for forest structural metrics</h3>

<p>Links to both pre-trained models (canopy and understory), example classifications and all training data used can be found in the GitHub readme under:
</p>
<p><a href="https://github.com/EcoDynIZW/imageseg">https://github.com/EcoDynIZW/imageseg</a>
</p>


<h3>Vignette</h3>

<p>The package contains a pdf vignette demonstrating the workflow for predictions and model training using various examples. It covers installation and setup, model predictions and training the forest structural models, and two more general applications of image segmentation (multi-class image segmentation of RGB microscopy images, and single-class image segmentation of grayscale ultrasound breast scan images). See <code>browseVignettes(package = "imageseg")</code>.
</p>


<h3>Author(s)</h3>

<p>Juergen Niedballa, Jan Axtner
</p>
<p><b>Maintainer</b>: Juergen Niedballa &lt;niedballa@izw-berlin.de&gt;
</p>


<h3>References</h3>

<p>Ronneberger O., Fischer P., Brox T. (2015) U-Net: Convolutional Networks for Biomedical Image Segmentation. In: Navab N., Hornegger J., Wells W., Frangi A. (eds) Medical Image Computing and Computer-Assisted Intervention â€“ MICCAI 2015. MICCAI 2015. Lecture Notes in Computer Science, vol 9351. Springer, Cham.
doi: <a href="https://doi.org/10.1007/978-3-319-24574-4_28">10.1007/978-3-319-24574-4_28</a>
</p>
<p>Zhou, Z., Rahman Siddiquee, M. M., Tajbakhsh, N., &amp; Liang, J. (2018). Unet++: A nested u-net architecture for medical image segmentation. In Deep learning in medical image analysis and multimodal learning for clinical decision support (pp. 3-11). Springer, Cham.
doi: <a href="https://doi.org/10.48550/arXiv.1807.10165">10.48550/arXiv.1807.10165</a>
</p>


<h3>See Also</h3>

<p><span class="pkg">keras</span>
<span class="pkg">tensorflow</span>
<span class="pkg">magick</span>
</p>

<hr>
<h2 id='dataAugmentation'>Data augmentation: rotating and mirroring images, and adjusting colors</h2><span id='topic+dataAugmentation'></span>

<h3>Description</h3>

<p>Rotate and/or mirror images to create augmented training data. Optionally, apply a random shift in brightness, saturation and hue to a certain percentage of the images
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dataAugmentation(
  images,
  subset = NULL,
  rotation_angles = 0,
  flip = FALSE,
  flop = FALSE,
  brightness_shift_lim = c(90, 110),
  saturation_shift_lim = c(95, 105),
  hue_shift_lim = c(80, 120),
  fraction_random_BSH = 0
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dataAugmentation_+3A_images">images</code></td>
<td>
<p>list. Output of <code>loadImages</code>. List with two items ($info: data frame with information about images, $img: tibble containing magick images)</p>
</td></tr>
<tr><td><code id="dataAugmentation_+3A_subset">subset</code></td>
<td>
<p>integer. Indices of images to process. Can be useful for only processing subsets of images (e.g. training images, not test/validation images).</p>
</td></tr>
<tr><td><code id="dataAugmentation_+3A_rotation_angles">rotation_angles</code></td>
<td>
<p>integer. Angles in which to rotate images using <code><a href="magick.html#topic+image_rotate">image_rotate</a></code>)?</p>
</td></tr>
<tr><td><code id="dataAugmentation_+3A_flip">flip</code></td>
<td>
<p>logical. mirror along horizontal axis (turn images upside-down using <code><a href="magick.html#topic+image_flip">image_flip</a></code>)?</p>
</td></tr>
<tr><td><code id="dataAugmentation_+3A_flop">flop</code></td>
<td>
<p>mirror along vertical axis (switch left and right) using <code><a href="magick.html#topic+image_flop">image_flop</a></code>)?</p>
</td></tr>
<tr><td><code id="dataAugmentation_+3A_brightness_shift_lim">brightness_shift_lim</code></td>
<td>
<p>numeric. Lower and upper limits for argument <code>brightness</code> in <code><a href="magick.html#topic+image_modulate">image_modulate</a></code></p>
</td></tr>
<tr><td><code id="dataAugmentation_+3A_saturation_shift_lim">saturation_shift_lim</code></td>
<td>
<p>numeric. Lower and upper limits for argument <code>saturation</code> in <code><a href="magick.html#topic+image_modulate">image_modulate</a></code></p>
</td></tr>
<tr><td><code id="dataAugmentation_+3A_hue_shift_lim">hue_shift_lim</code></td>
<td>
<p>numeric. Lower and upper limits for argument <code>hue</code> in <code><a href="magick.html#topic+image_modulate">image_modulate</a></code></p>
</td></tr>
<tr><td><code id="dataAugmentation_+3A_fraction_random_bsh">fraction_random_BSH</code></td>
<td>
<p>numeric. Fraction of images to apply random brightness / saturation / hue shifts to (between 0 and 1)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For creating training data for canopy, rotation and mirroring in both axes is appropriate. For understory vegetation density, only flop images (don't flip), and don't apply a hue shift since recognition of the orange flysheet is color-critical.
</p>


<h3>Value</h3>

<p>A list with 2 elements: $info, a data frame with information about the images, and $img, a tibble with magick images
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Example 1: Canopy
wd_images_can &lt;- system.file("images/canopy/resized",
                             package = "imageseg")

images_can &lt;- loadImages(imageDir = wd_images_can)

images_can_aug &lt;- dataAugmentation(images = images_can,
                                   rotation_angles = c(0, 90, 180, 270),
                                   flip = TRUE,
                                   flop = TRUE)
images_can_aug

</code></pre>

<hr>
<h2 id='findValidRegion'>Subset image to valid (informative) region</h2><span id='topic+findValidRegion'></span>

<h3>Description</h3>

<p>Load images, find and crop valid (informative) region. This function removes black borders from images, and is suitable for restricting hemispherical (fisheye) images to the actual informative region in the image center.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>findValidRegion(image, fileName, threshold = 0.1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="findValidRegion_+3A_image">image</code></td>
<td>
<p>magick image</p>
</td></tr>
<tr><td><code id="findValidRegion_+3A_filename">fileName</code></td>
<td>
<p>file name of image to load</p>
</td></tr>
<tr><td><code id="findValidRegion_+3A_threshold">threshold</code></td>
<td>
<p>numeric. Minimum range (max - min) of grayscale values for rows/columns to be included in the output image.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Images are converted to grayscale according to the formula in Li et al. (2020). L = 0.30R + 0.59G + 0.11B
We use a default threshold of 10, but it can be adjusted freely.
</p>
<p>This function can optionally be called inside <code><a href="#topic+resizeImages">resizeImages</a></code> to crop each image to the informative region before resizing to the dimensions expected by the models. It is not recommended though since it may return different crop masks for images and their masks.
</p>


<h3>Value</h3>

<p>A list with 3 items:
</p>
<p>* img: the magick image.
* geometry_area: a geometry string that can be used as argument <code>geometry</code> in <code><a href="magick.html#topic+image_crop">image_crop</a></code>.
* geometry_area_df: a data frame containing the information from geometry_area (can be useful for finding consensus are to crop from many images)
</p>


<h3>Warning</h3>

<p>Depending on the quality of the photographic equipment used, supposedly dark regions of images may be affected by stray light and light diffraction. This will be especially prevalend when using fisheye adapters, e.g. for smartphones. In such cases, the function will not provide reliable output. Adjusting 'threshold' may help to a degree, but needs to be set on a case-to-case basis for individual images. In such cases it might be easier to instead use e.g. GIMP to measure out the valid image region.
</p>


<h3>References</h3>

<p>Li, Kexin, et al. &quot;A New Method for Forest Canopy Hemispherical Photography Segmentation Based on Deep Learning.&quot; Forests 11.12 (2020): 1366.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
wd_images_can &lt;- system.file("images/canopy/raw",
                              package = "imageseg")
lf &lt;- list.files(wd_images_can, full.names = TRUE)
img &lt;- findValidRegion(fileName = lf[1])
img

# finding consensus among multiple images
## Not run: 
wd_with_many_images &lt;- "..."
lf &lt;- list.files(wd_with_many_images)
test &lt;- lapply(lf, findValidRegion)
# combine geometry_area_df from many images
geometry_areas_df &lt;- do.call(rbind, lapply(test, FUN = function(x) x$geometry_area_df))
# summary to decide on suitable values
summary(geometry_areas_df)

## End(Not run)
</code></pre>

<hr>
<h2 id='imageSegmentation'>Model predictions from images based on TensorFlow model</h2><span id='topic+imageSegmentation'></span>

<h3>Description</h3>

<p>This function uses a pre-trained TensorFlow model to create predictions from input data. It was mainly designed to predict canopy cover and understory vegetation density from forest habitat photographs using the pre-trained models we provide.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>imageSegmentation(
  model,
  x,
  dirOutput,
  dirExamples,
  subsetArea,
  threshold = 0.5,
  returnInput = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="imageSegmentation_+3A_model">model</code></td>
<td>
<p>trained model to use in predictions</p>
</td></tr>
<tr><td><code id="imageSegmentation_+3A_x">x</code></td>
<td>
<p>array of images as model input (can be created with <code><a href="#topic+imagesToKerasInput">imagesToKerasInput</a></code>)</p>
</td></tr>
<tr><td><code id="imageSegmentation_+3A_diroutput">dirOutput</code></td>
<td>
<p>character. Directory to save output images to (optional)</p>
</td></tr>
<tr><td><code id="imageSegmentation_+3A_direxamples">dirExamples</code></td>
<td>
<p>character. Directory to save example classification to (optional)</p>
</td></tr>
<tr><td><code id="imageSegmentation_+3A_subsetarea">subsetArea</code></td>
<td>
<p>If &quot;circle&quot;, vegetation density will be calculated for a circular area in the center of the predicted images. Can also be a number between 0 and 1 (to scale the circle relative to the image dimensions), or a matrix of 0 and 1 in the same dimensions as images in x.</p>
</td></tr>
<tr><td><code id="imageSegmentation_+3A_threshold">threshold</code></td>
<td>
<p>numeric value at which to split binary predictions. Can be useful to only return high-confidence pixels in predictions. It is not relevant for multi-class predictions.</p>
</td></tr>
<tr><td><code id="imageSegmentation_+3A_returninput">returnInput</code></td>
<td>
<p>logical. If <code>dirOutput</code> is defined, save input images alongside output?</p>
</td></tr>
</table>


<h3>Details</h3>

<p>By default, vegetation density will be calculated across the entire input images. If canopy images are hemispherical and have black areas in the corner that should be ignored, set <code>subsetArea</code> to &quot;circle&quot;. If the relevant section of the images is smaller than the image frame, give a number between 0 and 1 (indicating how big the circle is, relative to the image dimensions).
Alternatively, provide a custom matrix of 0 and 1 in the same dimensions as the input images in x. 0 indicates areas to ignore in the vegetation calculations, 1 is included. <code>subsetArea = "circle"</code> only works if input images in x are square.
</p>
<p>The canopy density models predicts sky and the understory vegetation density model predicts the red flysheet The percentage of these is equivalent to openness (canopy openness or understory openness). This value is in the column &quot;predicted&quot;.
</p>
<p>The interpretation of openness depends on context:
</p>

<ul>
<li><p> Canopy Cover images: openness = Gap Fraction and Fraction Soil
</p>
</li>
<li><p> Hemispherical canopy images: openness = Canopy openness and site openness (in flat terrain)
</p>
</li></ul>

<p>See e.g. Gonsamo et al. (2013) for more details.
</p>
<p>Generally speaking, &quot;predicted&quot; is the percentage of the image that is 1 in the binary prediction.
</p>
<p>The column &quot;not_predicted&quot; is the opposite (1-predicted). It is thus equivalent to vegetation density in the two vegetation models.
</p>
<p>Depending on the context, &quot;not_predicted&quot; can for example mean: canopy cover, canopy closure, understory vegetation density.
In canopy cover images, the vegetation density corresponds to canopy cover. In hemispherical images, vegetation density corresponds to canopy closure.
</p>


<h3>Value</h3>

<p>A list. The type and number of list items depends on the classification. For binary classifications (1 prediction class), the following list items are returned:
</p>

<ul>
<li><p> image (input images)
</p>
</li>
<li><p> prediction (model prediction)
</p>
</li>
<li><p> prediction_binary (binary prediction, only 0 or 1)
</p>
</li>
<li><p> examples (images with their image segmentation results)
</p>
</li>
<li><p> summary (data frame with fraction of image predicted)
</p>
</li>
<li><p> mask (an image showing the area for which summary statistics were calculated (in white, only if <code>subsetArea</code> is defined)
</p>
</li></ul>

<p>in multi-class models:
</p>

<ul>
<li><p> image (input images)
</p>
</li>
<li><p> prediction_most_likely (the class with the highest probability, coded in grayscale)
</p>
</li>
<li><p> class1 - classX: for each class, the predicted probabilities
</p>
</li>
<li><p> examples (images with their image segmentation results)
</p>
</li>
<li><p> summary (data frame with fraction of image covered by vegetation (black)).
</p>
</li>
<li><p> mask (an image showing the area for which summary statistics were calculated (in white, only if <code>subsetArea</code> is defined)
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 

# Example 1: Canopy
wd_images &lt;- system.file("images/canopy/resized",
                         package = "imageseg")
images &lt;- loadImages(imageDir = wd_images)
x &lt;- imagesToKerasInput(images)

wd_model_can &lt;- "C:/Path/To/Model"      # change this
filename_model_can &lt;- "imageseg_canopy_model.hdf5"
model_can &lt;- loadModel(file.path(wd_model_can, filename_model_can))


results_can &lt;- imageSegmentation(model = model_can,
                                 x = x)
results_can$image
results_can$prediction
results_can$prediction_binary
results_can$vegetation


# Example 2: Understory
wd_images_us &lt;- system.file("images/understory/resized",
                             package = "imageseg")
images_us &lt;- loadImages(imageDir = wd_images_us)
x &lt;- imagesToKerasInput(images_us)

# note, here we just specify the complete path, not separate by directory and file name as above
model_file_us &lt;- "C:/Path/To/Model/imageseg_understory_model.hdf5"
model_us &lt;- loadModel(model_file_us)

results_us &lt;- imageSegmentation(model = model_us,
                                x = x)
results_us$image
results_us$prediction
results_us$prediction_binary
results_us$vegetation


## End(Not run)

</code></pre>

<hr>
<h2 id='imagesToKerasInput'>Convert magick images in tibble to array for keras</h2><span id='topic+imagesToKerasInput'></span>

<h3>Description</h3>

<p>This function converts a tibble of images into input for TensorFlow models in keras. Specifically, images are converted to 4D arrays (image, height, width, channels). It can process color images and masks (for model training).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>imagesToKerasInput(
  images,
  subset = NULL,
  type = NULL,
  grayscale = NULL,
  n_class = 1,
  max = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="imagesToKerasInput_+3A_images">images</code></td>
<td>
<p>list. Output of <code>loadImages</code> or <code>dataAugmentation</code>. List with two items ($info: data frame with information about images, $img: tibble containing magick images)</p>
</td></tr>
<tr><td><code id="imagesToKerasInput_+3A_subset">subset</code></td>
<td>
<p>integer. Indices of images to process. Can be useful for only processing subsets of images (e.g. training images, not test/validation images).</p>
</td></tr>
<tr><td><code id="imagesToKerasInput_+3A_type">type</code></td>
<td>
<p>character. Can be &quot;image&quot; or &quot;mask&quot; and will set color channels of array accordingly (optional).</p>
</td></tr>
<tr><td><code id="imagesToKerasInput_+3A_grayscale">grayscale</code></td>
<td>
<p>logical. Defines color channels of images: 1 if codeTRUE, 3 if <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="imagesToKerasInput_+3A_n_class">n_class</code></td>
<td>
<p>For mask images, how many classes do they contain? (note that binary classifications like the canopy model have one class only)</p>
</td></tr>
<tr><td><code id="imagesToKerasInput_+3A_max">max</code></td>
<td>
<p>integer. Maximum value of output color values range. Can be 1 or 255.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function will try to infer the colorspace from images, but if the colorspaces are inconsistent one has to define 'colorspace'.
<code>type = "image"</code> can have either colorspace &quot;sRGB&quot; or &quot;Gray&quot;, masks are always &quot;Gray&quot;. color images have three color channels in the arrays, grayscale images have one color channel.
<code>n_class</code> is only relevant for masks. It determines the dimensions of the output. The default 1 is the (binary case). Higher values are for multi-class cases. If n_class is 2 or larger, keras::to_categorical() will be applied, and the <code><a href="#topic+u_net">u_net</a></code> model will use softmax instead of sigmoid activation in the final layer.
</p>
<p>By default, color value range will be 0-1. Alternatively, set <code>max</code> to 255 to create color value range 0-255 (e.g. to create input for Habitat-Net models).
</p>


<h3>Value</h3>

<p>An array with the following dimensions: image, height, width, channels
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Example 1: Canopy

# images
wd_images_can &lt;- system.file("images/canopy/resized",
                             package = "imageseg")
images_can &lt;- loadImages(imageDir = wd_images_can)
x &lt;- imagesToKerasInput(images_can)
str(x)   # a 4D array with an attribute data frame

# masks

wd_mask_can &lt;- system.file("images/canopy/masks",
                             package = "imageseg")
masks_can &lt;- loadImages(imageDir = wd_mask_can)
y &lt;- imagesToKerasInput(masks_can, type = "mask", grayscale = TRUE)
str(y)   # a 4D array with an attribute data frame

# Example 2: Understory
wd_images_us &lt;- system.file("images/understory/resized",
                             package = "imageseg")
images_us &lt;- loadImages(imageDir = wd_images_us)
x &lt;- imagesToKerasInput(images_us)
str(x)   # a 4D array, with an attribute data frame

</code></pre>

<hr>
<h2 id='loadImages'>Load image files with magick</h2><span id='topic+loadImages'></span>

<h3>Description</h3>

<p>This function loads images from disk to R, where one can inspect them and then pass them on to <code><a href="#topic+imagesToKerasInput">imagesToKerasInput</a></code>, which converts them to input for keras (TensorFlow) models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>loadImages(
  imageDir,
  fileNames,
  pattern,
  patternInclude = TRUE,
  imageFormats = c("JPG|TIF|PNG|JPEG|TIFF")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="loadImages_+3A_imagedir">imageDir</code></td>
<td>
<p>character. Directory containing the images to load</p>
</td></tr>
<tr><td><code id="loadImages_+3A_filenames">fileNames</code></td>
<td>
<p>character. File names to load (they will still be filtered by <code>pattern</code>, if defined)</p>
</td></tr>
<tr><td><code id="loadImages_+3A_pattern">pattern</code></td>
<td>
<p>character. Pattern to search in file names</p>
</td></tr>
<tr><td><code id="loadImages_+3A_patterninclude">patternInclude</code></td>
<td>
<p>logical. Include images with pattern in file names (TRUE) or exclude (FALSE)</p>
</td></tr>
<tr><td><code id="loadImages_+3A_imageformats">imageFormats</code></td>
<td>
<p>character. Image file formats to read.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with 2 slots: &quot;img&quot; contains images as a tibble, &quot;info&quot; contains basic information about the images.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Example 1: Canopy
wd_images_can &lt;- system.file("images/canopy/resized",
                             package = "imageseg")

images_can &lt;- loadImages(imageDir = wd_images_can)
images_can

# Example 2: Understory
wd_images_us &lt;- system.file("images/understory/resized",
                             package = "imageseg")
images_us &lt;- loadImages(imageDir = wd_images_us)
images_us

</code></pre>

<hr>
<h2 id='loadModel'>Load TensorFlow model from hdf5 file</h2><span id='topic+loadModel'></span>

<h3>Description</h3>

<p>Load TensorFlow model from hdf5 file
</p>


<h3>Usage</h3>

<pre><code class='language-R'>loadModel(modelFile, restoreCustomObjects = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="loadModel_+3A_modelfile">modelFile</code></td>
<td>
<p>character. File name of the .hdf5 model file to load</p>
</td></tr>
<tr><td><code id="loadModel_+3A_restorecustomobjects">restoreCustomObjects</code></td>
<td>
<p>logical. Restore custom objects (loss function &amp; dice coefficient) used in training of habitat models</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Loads a trained TensorFlow model from a hdf5 file, and (optionally) restores custom objects.
</p>


<h3>Value</h3>

<p>keras model
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Canopy model
wd_model_can &lt;- "C:/Path/To/Model"      # change this
filename_model_can &lt;- "imageseg_canopy_model.hdf5"
model_can &lt;- loadModel(file.path(wd_model_can, filename_model_can))

# Understory model
# note, here we just specify the complete path, not separate by directory and file name as above
model_file_us &lt;- "C:/Path/To/Model/imageseg_understory_model.hdf5"
model_us &lt;- loadModel(model_file_us)

## End(Not run)
</code></pre>

<hr>
<h2 id='resizeImages'>Resize and save images</h2><span id='topic+resizeImages'></span>

<h3>Description</h3>

<p>Resize and save images
</p>


<h3>Usage</h3>

<pre><code class='language-R'>resizeImages(
  imageDir,
  fileNames,
  pattern,
  patternInclude = TRUE,
  type,
  dimensions,
  validRegion,
  preserveAspect = TRUE,
  filter = NULL,
  colorspace,
  binary,
  gravity = "Center",
  imageFormats = c("JPG|TIF|PNG|JPEG|TIFF"),
  outDir,
  cores = 1,
  compression = "Lossless"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="resizeImages_+3A_imagedir">imageDir</code></td>
<td>
<p>Character. Directory containing raw images</p>
</td></tr>
<tr><td><code id="resizeImages_+3A_filenames">fileNames</code></td>
<td>
<p>character. File names to load (they will still be filtered by <code>pattern</code>, if defined)</p>
</td></tr>
<tr><td><code id="resizeImages_+3A_pattern">pattern</code></td>
<td>
<p>character. Pattern to search in file names</p>
</td></tr>
<tr><td><code id="resizeImages_+3A_patterninclude">patternInclude</code></td>
<td>
<p>logical. Include images with pattern in file names (TRUE) or exclude (FALSE)</p>
</td></tr>
<tr><td><code id="resizeImages_+3A_type">type</code></td>
<td>
<p>character. &quot;canopy&quot; or &quot;understory&quot;. Will set image dimensions accordingly to predefined c(256, 256) or c(160, 256), respectively (optional). Alternatively, use <code>dimensions</code>.</p>
</td></tr>
<tr><td><code id="resizeImages_+3A_dimensions">dimensions</code></td>
<td>
<p>integer. image dimensions provides as c(width, height) in pixels. If specified, overrides <code>type</code></p>
</td></tr>
<tr><td><code id="resizeImages_+3A_validregion">validRegion</code></td>
<td>
<p>character. If defined, use string as argument <code>geometry</code> in <code><a href="magick.html#topic+image_crop">image_crop</a></code> (output of <code><a href="magick.html#topic+geometry_area">geometry_area</a></code>), which will crop all images to the same region before resizing (optional). If undefined, don't crop.</p>
</td></tr>
<tr><td><code id="resizeImages_+3A_preserveaspect">preserveAspect</code></td>
<td>
<p>logical. If TRUE, images will be cropped to aspect ratio of output before resizing (thus preserving original aspect ratio, but losing parts of the image). If FALSE, images will be simply resized from their input size to the desired output (not preserving aspect ratio).</p>
</td></tr>
<tr><td><code id="resizeImages_+3A_filter">filter</code></td>
<td>
<p>character. Resampling filter. Passed to argument <code>filter</code> in <code><a href="magick.html#topic+image_resize">image_resize</a></code>. See <code>magick::filter_types()</code> for available options. Default is LanczosFilter.</p>
</td></tr>
<tr><td><code id="resizeImages_+3A_colorspace">colorspace</code></td>
<td>
<p>character. If defined, image will be converted to the requested colorspace. If undefined, colorspace will remain unchanged. Must be a valid argument to  <code>magick::colorspace_types()</code>. In practice, only &quot;sRGB&quot; and &quot;Gray&quot; will be relevant.</p>
</td></tr>
<tr><td><code id="resizeImages_+3A_binary">binary</code></td>
<td>
<p>logical. If colorspace is &quot;Gray&quot;, make the output binary?</p>
</td></tr>
<tr><td><code id="resizeImages_+3A_gravity">gravity</code></td>
<td>
<p>if preserveAspect = TRUE and images need to be cropped, the <code>gravity</code> argument to use in <code><a href="magick.html#topic+image_crop">image_crop</a></code>.</p>
</td></tr>
<tr><td><code id="resizeImages_+3A_imageformats">imageFormats</code></td>
<td>
<p>character. Image file formats to read.</p>
</td></tr>
<tr><td><code id="resizeImages_+3A_outdir">outDir</code></td>
<td>
<p>character. Directory to save resized images in.</p>
</td></tr>
<tr><td><code id="resizeImages_+3A_cores">cores</code></td>
<td>
<p>integer. Number of cores to use for parallel processing</p>
</td></tr>
<tr><td><code id="resizeImages_+3A_compression">compression</code></td>
<td>
<p>character. Compression type to use in <code><a href="magick.html#topic+image_write">image_write</a></code>. See <code><a href="magick.html#topic+compress_types">compress_types</a></code>. By default, &quot;Lossless&quot; for grayscale images, &quot;Undefined&quot; for color images.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Resizing is done by <code><a href="magick.html#topic+image_resize">image_resize</a></code> and will ensure that the resized images have exactly the desired dimensions.
</p>
<p>If <code>preserveAspect = TRUE</code>, input images will first be cropped to the maximum area with the aspect ratio of the desired output (1:1 (square) for <code>type = "canopy"</code>, 5:8 for <code>type = "understory"</code>), by default in the center of the input image (argument <code>gravity</code>). This will usually lead to the loss of parts of the image, but the remaining part of the image is not deformed compared to the original.
Alternatively, if <code>preserveAspect = FALSE</code>, input images will be resized to the requested dimensions without cropping (thus no loss of part of the image), but the aspect ratio changes. If aspect ratio changes too strongly it may negatively affect model performance.
</p>
<p>Resizing is done using &quot;!&quot; in the geometry syntax. See <code><a href="magick.html#topic+geometry">geometry</a></code> for details.
</p>
<p>compression = &quot;Lossless&quot; is used to ensure no compression artefacts in saved images (which would for example introduce grayscale values in black/white images). If small file sizes are important, you can change it to save compressed images.
</p>


<h3>Value</h3>

<p>No R output, only resized images are saved on disk
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Example 1: Canopy
wd_can &lt;- system.file("images/canopy/raw",
                      package = "imageseg")

wd_out_can &lt;- file.path(tempdir(), "canopy", "resized")
resizeImages(imageDir = wd_can,
             type = "canopy",
             outDir = wd_out_can)

filename_resized &lt;- list.files(wd_out_can, full.names = TRUE)

# check output
img_can &lt;- magick::image_read(filename_resized)
img_can

# Example 2: Understory
wd_us &lt;- system.file("images/understory/raw",
                      package = "imageseg")
wd_out_us &lt;- file.path(tempdir(), "understory", "resized")

# note, these are png images
resizeImages(imageDir = wd_us,
             type = "understory",
             outDir = wd_out_us)

filename_resized &lt;- list.files(wd_out_us, full.names = TRUE)

# check output
img_us &lt;- magick::image_read(filename_resized)
img_us

</code></pre>

<hr>
<h2 id='u_net'>Create a U-Net architecture</h2><span id='topic+u_net'></span>

<h3>Description</h3>

<p>Create a U-Net architecture
</p>


<h3>Usage</h3>

<pre><code class='language-R'>u_net(
  net_h,
  net_w,
  grayscale = FALSE,
  layers_per_block = 2,
  blocks = 4,
  n_class = 1,
  filters = 16,
  dropout = 0,
  batch_normalization = TRUE,
  kernel_initializer = "he_normal"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="u_net_+3A_net_h">net_h</code></td>
<td>
<p>Input layer height.</p>
</td></tr>
<tr><td><code id="u_net_+3A_net_w">net_w</code></td>
<td>
<p>Input layer width.</p>
</td></tr>
<tr><td><code id="u_net_+3A_grayscale">grayscale</code></td>
<td>
<p>Defines input layer color channels - 1 if 'TRUE', 3 if 'FALSE'.</p>
</td></tr>
<tr><td><code id="u_net_+3A_layers_per_block">layers_per_block</code></td>
<td>
<p>Number of convolutional layers per block (can be 2 or 3)</p>
</td></tr>
<tr><td><code id="u_net_+3A_blocks">blocks</code></td>
<td>
<p>Number of blocks in the model.</p>
</td></tr>
<tr><td><code id="u_net_+3A_n_class">n_class</code></td>
<td>
<p>Number of classes.</p>
</td></tr>
<tr><td><code id="u_net_+3A_filters">filters</code></td>
<td>
<p>Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution).</p>
</td></tr>
<tr><td><code id="u_net_+3A_dropout">dropout</code></td>
<td>
<p>Dropout rate (between 0 and 1).</p>
</td></tr>
<tr><td><code id="u_net_+3A_batch_normalization">batch_normalization</code></td>
<td>
<p>Should batch normalization be used in the block?</p>
</td></tr>
<tr><td><code id="u_net_+3A_kernel_initializer">kernel_initializer</code></td>
<td>
<p>Initializer for the kernel weights matrix.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function creates a U-Net model architecture according to user input. It allows flexibility regarding input, output and the hidden layers. 
See the package vignette for examples.
</p>
<p>The function was adapted and slightly modified from the <code>u_net()</code> function in the platypus package (<a href="https://github.com/maju116/platypus/blob/master/R/u_net.R">https://github.com/maju116/platypus/blob/master/R/u_net.R</a>).
</p>
<p>Differences compared to platypus implementation:
</p>

<ul>
<li><p> added argument: layers_per_block (can be 2 or 3) 
</p>
</li>
<li><p> kernel size in layer_conv_2d_transpose is 2, not 3.
</p>
</li>
<li><p> dropout layers are only included if user specifies dropout &gt; 0
</p>
</li>
<li><p> n_class = 1 by default (sufficient for binary classification used for vegetation model, e.g. sky or not sky)
</p>
</li>
<li><p> automatic choice of activation of output layer: &quot;sigmoid&quot; if n_class = 1, otherwise &quot;softmax&quot;
</p>
</li>
<li><p> allows non-square input images (e.g. 160x256 used in understory vegetation density model)
</p>
</li></ul>



<h3>Value</h3>

<p>U-Net model.
</p>
<p>A keras model as returned by <code><a href="keras.html#topic+keras_model">keras_model</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# U-Net model for 256x256 pixel RGB input images with a single output class
# this model was used for canopy density

model &lt;- u_net(net_h = 256, 
net_w = 256, 
grayscale = FALSE,
filters = 32,
blocks = 4,
layers_per_block = 2
)

# several arguments above were not necessary because they were kept at their default. 
# Below is the same model, but shorter:

model &lt;- u_net(net_h = 256, 
net_w = 256, 
filters = 32
)

model


## End(Not run)

</code></pre>

<hr>
<h2 id='u_net_plusplus'>Create a U-Net++ architecture</h2><span id='topic+u_net_plusplus'></span>

<h3>Description</h3>

<p>Create a U-Net++ architecture.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>u_net_plusplus(
  net_h,
  net_w,
  grayscale = FALSE,
  blocks = 4,
  n_class = 1,
  filters = 16,
  kernel_initializer = "he_normal"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="u_net_plusplus_+3A_net_h">net_h</code></td>
<td>
<p>Input layer height.</p>
</td></tr>
<tr><td><code id="u_net_plusplus_+3A_net_w">net_w</code></td>
<td>
<p>Input layer width.</p>
</td></tr>
<tr><td><code id="u_net_plusplus_+3A_grayscale">grayscale</code></td>
<td>
<p>Defines input layer color channels - 1 if 'TRUE', 3 if 'FALSE'.</p>
</td></tr>
<tr><td><code id="u_net_plusplus_+3A_blocks">blocks</code></td>
<td>
<p>Number of blocks in the model.</p>
</td></tr>
<tr><td><code id="u_net_plusplus_+3A_n_class">n_class</code></td>
<td>
<p>Number of classes.</p>
</td></tr>
<tr><td><code id="u_net_plusplus_+3A_filters">filters</code></td>
<td>
<p>Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution).</p>
</td></tr>
<tr><td><code id="u_net_plusplus_+3A_kernel_initializer">kernel_initializer</code></td>
<td>
<p>Initializer for the kernel weights matrix.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function was ported to R from Python code in https://github.com/albertsokol/pneumothorax-detection-unet/blob/master/models.py. For more details, see https://github.com/MrGiovanni/UNetPlusPlus.
</p>


<h3>Value</h3>

<p>U-Net++ model.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# U-Net++ model for 256x256 pixel RGB input images with a single output class

model &lt;- u_net_plusplus(net_h = 256, 
net_w = 256, 
filters = 32,
blocks = 3 
)
 
model


## End(Not run)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
