<!DOCTYPE html><html><head><title>Help for package penalizedSVM</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {penalizedSVM}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#.plot.EPSGO.parms'><p>Plot Interval Search Plot Visited Points and the Q Values.</p></a></li>
<li><a href='#EPSGO'><p>  Fits  SVM with variable selection using penalties.</p></a></li>
<li><a href='#findgacv.scad'><p> Calculate Generalized Approximate Cross Validation Error Estimation for SCAD SVM model</p></a></li>
<li><a href='#lpsvm'><p>Fit L1-norm SVM</p></a></li>
<li><a href='#penalizedSVM-package'>
<p>Feature Selection SVM using Penalty Functions</p></a></li>
<li><a href='#penaltySVM-internal'><p>Internal penaltySVM objects</p></a></li>
<li><a href='#predict'><p> Predict Method for Feature Selection SVM</p></a></li>
<li><a href='#print'><p> Print Function for FS SVM</p></a></li>
<li><a href='#scadsvc'><p> Fit SCAD SVM model</p></a></li>
<li><a href='#sim.data'><p> Simulation of microarray data</p></a></li>
<li><a href='#sortmat'><p>Sort matrix or data frame</p></a></li>
<li><a href='#svmfs'><p>  Fits  SVM with variable selection using penalties.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Feature Selection SVM using Penalty Functions</td>
</tr>
<tr>
<td>Version:</td>
<td>1.1.4</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-03-23</td>
</tr>
<tr>
<td>Depends:</td>
<td>e1071, mlegp, MASS</td>
</tr>
<tr>
<td>Imports:</td>
<td>corpcor, statmod, tgp</td>
</tr>
<tr>
<td>Author:</td>
<td>Natalia Becker, Wiebke Werft, Axel Benner</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Frederic Bertrand &lt;frederic.bertrand@utt.fr&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Support Vector Machine (SVM) classification with simultaneous feature selection using penalty
        functions is implemented. The smoothly clipped absolute deviation (SCAD),
        'L1-norm', 'Elastic Net' ('L1-norm' and 'L2-norm') and 'Elastic
        SCAD' (SCAD and 'L2-norm') penalties are available. The tuning
        parameters can be found using either a fixed grid or a interval
        search.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>yes</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>6.0.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-03-23 14:30:06 UTC; fbertran</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-03-23 15:10:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='.plot.EPSGO.parms'>Plot Interval Search Plot Visited Points and the Q Values. </h2><span id='topic+.plot.EPSGO.parms'></span>

<h3>Description</h3>

<p>For interval search plot visited points and the Q values (=Ytrain) exclude: for D=1 make  an additional plot: skip values for empty model, for example: Ytrain.exclude=10^16.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.plot.EPSGO.parms(Xtrain, Ytrain,bounds, Ytrain.exclude=10^16, plot.name=NULL )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".plot.EPSGO.parms_+3A_xtrain">Xtrain</code></td>
<td>
<p>X points to train</p>
</td></tr>
<tr><td><code id=".plot.EPSGO.parms_+3A_ytrain">Ytrain</code></td>
<td>
<p>Y points to train</p>
</td></tr>
<tr><td><code id=".plot.EPSGO.parms_+3A_bounds">bounds</code></td>
<td>
<p> bounds for parameters, see examples</p>
</td></tr>
<tr><td><code id=".plot.EPSGO.parms_+3A_ytrain.exclude">Ytrain.exclude</code></td>
<td>
<p>If exclude for Ytrain exists, skip those  points. Defaults to 10^16.</p>
</td></tr>
<tr><td><code id=".plot.EPSGO.parms_+3A_plot.name">plot.name</code></td>
<td>
<p>Defaults to <code>NULL</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>The code source was adopted from MATLAB originals, special thanks to Holger Froehlich.
</p>


<h3>Value</h3>

<p>None, only graphs are created.
</p>


<h3>Author(s)</h3>

<p>Natalia Becker<br />
<a href="mailto:natalie_becker@gmx.de">natalie_becker@gmx.de</a>
</p>


<h3>References</h3>

<p>Froehlich, H. and Zell, A. (2005) &quot;Effcient parameter selection for support vector
machines in classification and regression via model-based global optimization&quot;
<em>In Proc. Int. Joint Conf. Neural Networks,  1431-1438 </em>.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+svmfs">svmfs</a></code>   </p>


<h3>Examples</h3>

<pre><code class='language-R'>
	
	seed &lt;- 123
			
	train&lt;-sim.data(n = 200, ng = 100, nsg = 10, corr=FALSE, seed=seed )
	print(str(train)) 
			
	Q.func&lt;- ".calc.scad"
	
	bounds=t(data.frame(log2lambda1=c(-10, 10)))
							colnames(bounds)&lt;-c("lower", "upper")	
			
	print("start interval search")
	# computation intensive; 
	# for demostration reasons only for the first 100 features 
	# and only for 10 iterations maxIter=10, default maxIter=700
	system.time(fit&lt;-EPSGO(Q.func, bounds=bounds, parms.coding="log2", fminlower=0, 
		 show='none', N=21,  maxevals=500, 
		 pdf.name=NULL,  seed=seed,  
		 verbose=FALSE,
		 # Q.func specific parameters:
		 x.svm=t(train$x)[,1:100], y.svm=train$y,
		 inner.val.method="cv",
		 cross.inner=5, maxIter=10 ))
									 
	print(paste("minimal 5-fold cv error:", fit$fmin, "by log2(lambda1)=", fit$xmin))
		
	print(" all lambdas with the same minimum? ")
	print(fit$ points.fmin) 
			
	print(paste(fit$neval, "visited points"))
			
			
	print(" overview: over all visitied points in tuning parameter space 
				with corresponding cv errors")
	print(data.frame(Xtrain=fit$Xtrain, cv.error=fit$Ytrain))

	# create  3 plots om one screen: 
	# 1st plot: distribution of initial points in tuning parameter space
	# 2nd plot: visited lambda points vs. cv errors
	# 3rd plot: the same as the 2nd plot, Ytrain.exclude points are excluded. 
	#    The value cv.error = 10^16 stays for the cv error for an empty model ! 
	.plot.EPSGO.parms (fit$Xtrain, fit$Ytrain,bound=bounds, 
				Ytrain.exclude=10^16, plot.name=NULL )
	 # end of \donttest
</code></pre>

<hr>
<h2 id='EPSGO'>  Fits  SVM with variable selection using penalties. </h2><span id='topic+EPSGO'></span><span id='topic+Direct'></span><span id='topic+ExpImprovement'></span>

<h3>Description</h3>

<p>Fits  SVM with feature selection  using penalties SCAD and 1 norm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>EPSGO(Q.func, bounds,	parms.coding="none", fminlower=0, flag.find.one.min =FALSE,
			show=c("none", "final", "all"), N= NULL, maxevals = 500,  
	    pdf.name=NULL,  pdf.width=12,  pdf.height=12,   my.mfrow=c(1,1), 
	    verbose=TRUE, seed=123,  ...  )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="EPSGO_+3A_q.func">Q.func</code></td>
<td>
<p> name of the function to be  minimized. </p>
</td></tr>
<tr><td><code id="EPSGO_+3A_bounds">bounds</code></td>
<td>
<p> bounds for parameters, see examples</p>
</td></tr>
<tr><td><code id="EPSGO_+3A_parms.coding">parms.coding</code></td>
<td>
<p> parmeters coding: none  or log2, default: none.  </p>
</td></tr>
<tr><td><code id="EPSGO_+3A_fminlower">fminlower</code></td>
<td>
<p> minimal value for the function Q.func, default is 0.     </p>
</td></tr>
<tr><td><code id="EPSGO_+3A_flag.find.one.min">flag.find.one.min</code></td>
<td>
<p>  do you want to find one min value and stop? Default: FALSE </p>
</td></tr>
<tr><td><code id="EPSGO_+3A_show">show</code></td>
<td>
<p>  show plots of  DIRECT algorithm:    none, final iteration, all iterations. Default: none  </p>
</td></tr>
<tr><td><code id="EPSGO_+3A_n">N</code></td>
<td>
<p> define the number of start points, see details. </p>
</td></tr>
<tr><td><code id="EPSGO_+3A_maxevals">maxevals</code></td>
<td>
<p> the maximum number of DIRECT function evaluations, default: 500.   </p>
</td></tr>
<tr><td><code id="EPSGO_+3A_pdf.name">pdf.name</code></td>
<td>
<p>pdf name      </p>
</td></tr>   
<tr><td><code id="EPSGO_+3A_pdf.width">pdf.width</code></td>
<td>
<p> default 12 </p>
</td></tr>
<tr><td><code id="EPSGO_+3A_pdf.height">pdf.height</code></td>
<td>
<p> default 12 </p>
</td></tr>
<tr><td><code id="EPSGO_+3A_my.mfrow">my.mfrow</code></td>
<td>
<p> default c(1,1) </p>
</td></tr>
<tr><td><code id="EPSGO_+3A_verbose">verbose</code></td>
<td>
<p> verbose? default TRUE. </p>
</td></tr>
<tr><td><code id="EPSGO_+3A_seed">seed</code></td>
<td>
<p> seed </p>
</td></tr>
<tr><td><code id="EPSGO_+3A_...">...</code></td>
<td>
<p> additional argument(s) </p>
</td></tr>
</table>


<h3>Details</h3>

<p>if the number of start points (N)  is not defined by the user, it will be defined dependent on the dimensionality of the parameter space.
N=10D+1, where  D is the number of parameters, but for high dimensional parameter space with more than 6 dimensions,  
the initial set is restricted to 65. However for one-dimensional parameter space the N is set to 21 due to stability reasons.
</p>
<p>The idea of EPSGO (Efficient Parameter Selection via Global Optimization): Beginning
from an intial Latin hypercube sampling containing N starting points we train
an Online GP, look for the point with the maximal expected 	improvement, sample there and update the Gaussian Process(GP). Thereby
it is not so important that GP really correctly 	models the error surface of the SVM in parameter space, but
that it can give a us information about potentially interesting 	points in parameter space where we should sample next.
We continue with sampling points until some convergence criterion is met.
</p>
<p>DIRECT is a sampling algorithm which requires no knowledge of the objective function gradient.
Instead, the algorithm samples points in the domain, and uses the information it has obtained to decide where to
search next. The DIRECT algorithm will globally converge to the maximal value of the objective function. The name
DIRECT comes from the shortening of the phrase 'DIviding RECTangles', which describes the way the algorithm moves
towards the optimum.  
</p>
<p>The code source was adopted from MATLAB originals, special thanks to Holger Froehlich.
</p>


<h3>Value</h3>

<table>
<tr><td><code>fmin</code></td>
<td>
<p>minimal value of Q.func on the interval defined by bounds. </p>
</td></tr>
<tr><td><code>xmin</code></td>
<td>
<p>coreesponding parameters for the minimum</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>number of iterations</p>
</td></tr>
<tr><td><code>neval</code></td>
<td>
<p>  number of visited points </p>
</td></tr>
<tr><td><code>maxevals</code></td>
<td>
<p>  the maximum number of DIRECT function evaluations </p>
</td></tr>
<tr><td><code>seed</code></td>
<td>
<p>  seed</p>
</td></tr>
<tr><td><code>bounds</code></td>
<td>
<p> bounds for parameters</p>
</td></tr>
<tr><td><code>Q.func</code></td>
<td>
<p>  name of the function to be  minimized. </p>
</td></tr>
<tr><td><code>points.fmin</code></td>
<td>
<p>  the set of points with the same fmin </p>
</td></tr>
<tr><td><code>Xtrain</code></td>
<td>
<p>  visited points </p>
</td></tr>
<tr><td><code>Ytrain</code></td>
<td>
<p>  the output of Q.func at visited points Xtrain </p>
</td></tr>
<tr><td><code>gp.seed</code></td>
<td>
<p> seed for Gaussian Process </p>
</td></tr>
<tr><td><code>model.list</code></td>
<td>
<p> detailed information of the search process </p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Natalia Becker<br />
<a href="mailto:natalie_becker@gmx.de">natalie_becker@gmx.de</a>
</p>


<h3>References</h3>

<p>Froehlich, H. and Zell, A. (2005) &quot;Effcient parameter selection for support vector
machines in classification and regression via model-based global optimization&quot;
<em>In Proc. Int. Joint Conf. Neural Networks,  1431-1438 </em>.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+svmfs">svmfs</a></code>   </p>


<h3>Examples</h3>

<pre><code class='language-R'>
	
	seed &lt;- 123
			
	train&lt;-sim.data(n = 200, ng = 100, nsg = 10, corr=FALSE, seed=seed )
	print(str(train)) 
			
	Q.func&lt;- ".calc.scad"
	
	bounds=t(data.frame(log2lambda1=c(-10, 10)))
							colnames(bounds)&lt;-c("lower", "upper")	
			
	print("start interval search")
	# computation intensive; 
	# for demostration reasons only for the first 100 features 
	# and only for 10 iterations maxIter=10, default maxIter=700
	system.time(fit&lt;-EPSGO(Q.func, bounds=bounds, parms.coding="log2", fminlower=0, 
		 show='none', N=21,  maxevals=500, 
		 pdf.name=NULL,  seed=seed,  
		 verbose=FALSE,
		 # Q.func specific parameters:
		 x.svm=t(train$x)[,1:100], y.svm=train$y,
		 inner.val.method="cv",
		 cross.inner=5, maxIter=10 ))
									 
	print(paste("minimal 5-fold cv error:", fit$fmin, "by log2(lambda1)=", fit$xmin))
		
	print(" all lambdas with the same minimum? ")
	print(fit$ points.fmin) 
			
	print(paste(fit$neval, "visited points"))
			
			
	print(" overview: over all visitied points in tuning parameter space 
				with corresponding cv errors")
	print(data.frame(Xtrain=fit$Xtrain, cv.error=fit$Ytrain))

	# create  3 plots om one screen: 
	# 1st plot: distribution of initial points in tuning parameter space
	# 2nd plot: visited lambda points vs. cv errors
	# 3rd plot: the same as the 2nd plot, Ytrain.exclude points are excluded. 
	#    The value cv.error = 10^16 stays for the cv error for an empty model ! 
	.plot.EPSGO.parms (fit$Xtrain, fit$Ytrain,bound=bounds, 
				Ytrain.exclude=10^16, plot.name=NULL )
	 # end of \donttest
</code></pre>

<hr>
<h2 id='findgacv.scad'> Calculate Generalized Approximate Cross Validation Error Estimation for SCAD SVM model</h2><span id='topic+findgacv.scad'></span>

<h3>Description</h3>

<p>calculate generalized approximate cross validation error (GACV) estimation 
for SCAD SVM model</p>


<h3>Usage</h3>

<pre><code class='language-R'>findgacv.scad(y, model)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="findgacv.scad_+3A_y">y</code></td>
<td>
<p> vector of class labels (only for 2 classes) </p>
</td></tr>
<tr><td><code id="findgacv.scad_+3A_model">model</code></td>
<td>
<p> list, describing  SCAD SVM model, produced by function scadsvc </p>
</td></tr>
</table>


<h3>Value</h3>

<p>returns the GACV value
</p>


<h3>Author(s)</h3>

<p>Natalia Becker<br />
<a href="mailto:natalie_becker@gmx.de">natalie_becker@gmx.de</a>
</p>


<h3>References</h3>

<p>Zhang, H. H., Ahn, J., Lin, X. and Park, C. (2006). <em> Gene selection using
support vector machines with nonconvex penalty.</em> Bioinformatics, <b>22</b>, pp. 88-95. 
</p>
<p>Wahba G., Lin, Y. and Zhang, H. (2000). <em>GACV for support vector machines, or, another way
to look at margin-like quantities, in A. J. Smola, P. Bartlett, B. Schoelkopf and D. Schurmans
(eds)</em>, Advances in Large Margin Classifiers, MIT Press, pp. 297-309.
</p>


<h3>See Also</h3>

  <p><code><a href="#topic+scadsvc">scadsvc</a></code>,  <code><a href="#topic+predict.penSVM">predict.penSVM</a></code>, <code><a href="#topic+sim.data">sim.data</a></code>     </p>


<h3>Examples</h3>

<pre><code class='language-R'>
# simulate data
train&lt;-sim.data(n = 200, ng = 100, nsg = 10, corr=FALSE, seed=12)
print(str(train)) 
	
# train data	
ff &lt;- scadsvc(as.matrix(t(train$x)), y=train$y, lambda=0.01)
print(str(ff))

# estimate gacv error
(gacv&lt;- findgacv.scad(train$y, model=ff))

</code></pre>

<hr>
<h2 id='lpsvm'>Fit L1-norm SVM </h2><span id='topic+lpsvm'></span>

<h3>Description</h3>

<p>SVM with variable selection (clone selection) using L1-norm penalty. 
( a fast Newton algorithm NLPSVM from Fung and Mangasarian ) 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lpsvm(A, d, k = 5, nu = 0, output = 1, delta = 10^-3, epsi = 10^-4, 
seed = 123, maxIter=700)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lpsvm_+3A_a">A</code></td>
<td>
<p> n-by-d data matrix to train (n chips/patients, d clones/genes).  </p>
</td></tr>
<tr><td><code id="lpsvm_+3A_d">d</code></td>
<td>
<p> vector of class labels  -1 or 1's (for n chips/patiens ). </p>
</td></tr>
<tr><td><code id="lpsvm_+3A_k">k</code></td>
<td>
<p>  k-fold for cv, default k=5.  </p>
</td></tr>
<tr><td><code id="lpsvm_+3A_nu">nu</code></td>
<td>
<p> weighted parameter, 1 - easy estimation,
0  - hard estimation, any other value - used as nu by the algorithm.
Default : 0. </p>
</td></tr>
<tr><td><code id="lpsvm_+3A_output">output</code></td>
<td>
<p> 0 - no output, 1 - produce output, default is 0. </p>
</td></tr>
<tr><td><code id="lpsvm_+3A_delta">delta</code></td>
<td>
<p> some small value, default: <code class="reqn">10^-3</code>. </p>
</td></tr>
<tr><td><code id="lpsvm_+3A_epsi">epsi</code></td>
<td>
<p> tuning parameter.  </p>
</td></tr>
<tr><td><code id="lpsvm_+3A_seed">seed</code></td>
<td>
<p> seed. </p>
</td></tr>
<tr><td><code id="lpsvm_+3A_maxiter">maxIter</code></td>
<td>
<p> maximal iterations, default: 700. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>k: k-fold for cv, is a way to divide the data set into test and training set.<br />
if k = 0: simply run the algorithm without any correctness
calculation, this is the default. <br />
if k = 1: run the algorithm and calculate correctness on
the whole data set. <br />
if k = any value less than the number of rows in the data set:
divide up the data set into test and training
using k-fold method. <br />
if k = number of rows in the data set: use the 'leave one out' (loo) method
</p>


<h3>Value</h3>

<p>a list of 
</p>
<table>
<tr><td><code>w</code></td>
<td>
<p> coefficients of the hyperplane </p>
</td></tr>
<tr><td><code>b</code></td>
<td>
<p> intercept of the hyperplane</p>
</td></tr>
<tr><td><code>xind</code></td>
<td>
<p> the index of the selected features (genes) in the data matrix. </p>
</td></tr>
<tr><td><code>epsi</code></td>
<td>
<p> optimal tuning parameter epsilon  </p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p> number of iterations </p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>  k-fold for cv</p>
</td></tr>
<tr><td><code>trainCorr</code></td>
<td>
<p> for cv: average train correctness </p>
</td></tr>
<tr><td><code>testCorr</code></td>
<td>
<p> for cv: average test correctness </p>
</td></tr>
<tr><td><code>nu</code></td>
<td>
<p> weighted parameter </p>
</td></tr>
</table>


<h3>Note</h3>

<p> Adapted from MATLAB code  http://www.cs.wisc.edu/dmi/svm/lpsvm/
</p>


<h3>Author(s)</h3>

<p> Natalia Becker </p>


<h3>References</h3>

 
<p>Fung, G. and Mangasarian, O. L. (2004). A feature selection newton method for 
support vector machine classification. <em>Computational Optimization and Applications Journal 28(2)  pp. 185-202</em>. 
</p>


<h3>See Also</h3>

  <p><code><a href="#topic+sim.data">sim.data</a></code>  </p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(123)
train&lt;-sim.data(n = 20, ng = 100, nsg = 10, corr=FALSE, seed=12)
print(str(train)) 
	
# train data	
model &lt;- lpsvm(A=t(train$x), d=train$y, k=5, nu=0,output=0, delta=10^-3, epsi=0.001, seed=12)
print(model)


</code></pre>

<hr>
<h2 id='penalizedSVM-package'>
Feature Selection SVM using Penalty Functions  
</h2><span id='topic+penalizedSVM-package'></span><span id='topic+penalizedSVM'></span>

<h3>Description</h3>

<p>Feature Selection SVM using penalty functions. The smoothly clipped absolute deviation (SCAD) and L1-norm penalties are 
availible up to now. Other functions will be implemented in the near feature.  
</p>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> penaltySVM</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 1.1.3</td>
</tr>
<tr>
 <td style="text-align: left;">
Date: </td><td style="text-align: left;"> 2022-05-02</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL-2</td>
</tr>
<tr>
 <td style="text-align: left;">
LazyLoad: </td><td style="text-align: left;"> yes</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p>The main function is <code><a href="#topic+svmfs">svmfs</a>, see the documentation file with examples</code>
</p>


<h3>Author(s)</h3>

<p> Natalia Becker, Axel Benner, Wiebke Werft 
</p>
<p>Maintainer: Frederic Bertrand (<a href="mailto:frederic.bertrand@utt.fr">frederic.bertrand@utt.fr</a>)
</p>


<h3>References</h3>

<p>Zhang, H. H., Ahn, J., Lin, X. and Park, C. (2006). <em>Gene selection using
support vector machines with nonconvex penalty.</em> Bioinformatics, <b>22</b>, pp. 88-95. 
</p>
<p>Fung, G. and Mangasarian, O. L. (2004). <em> A feature selection newton method for 
support vector machine classification.</em> Computational Optimization and Applications Journal ,<b>28.2</b> , pp. 185-202.
</p>

<hr>
<h2 id='penaltySVM-internal'>Internal penaltySVM objects</h2><span id='topic+.core'></span><span id='topic+.calc.mult.inv_Q_mat2'></span><span id='topic+.correctness'></span><span id='topic+.create.covariance.matrix'></span><span id='topic+.EstNuLong'></span><span id='topic+.EstNuShort'></span><span id='topic+.extend.to.quad.matrix'></span><span id='topic+.find.inverse'></span><span id='topic+.required'></span><span id='topic+.run.cv'></span><span id='topic+.whatiscorrect'></span>

<h3>Description</h3>

<p>Internal penaltySVM objects.</p>


<h3>Details</h3>

<p>These are not to be called by the user.</p>

<hr>
<h2 id='predict'> Predict Method for Feature Selection SVM      </h2><span id='topic+predict.penSVM'></span>

<h3>Description</h3>

<p>This function predicts values based upon a model trained by svm. 
If class assigment is provided, confusion table, missclassification table,
sensitivity and specificity are calculated.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'penSVM'
predict(object, newdata, newdata.labels = NULL, labels.universe=NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict_+3A_object">object</code></td>
<td>
<p> Object of class &quot;penSVM&quot;, created by 'svmfs'</p>
</td></tr>
<tr><td><code id="predict_+3A_newdata">newdata</code></td>
<td>
<p> A matrix containing the new input data, samples in rows, features in columns </p>
</td></tr>
<tr><td><code id="predict_+3A_newdata.labels">newdata.labels</code></td>
<td>
<p> optional, new data class labels  </p>
</td></tr>
<tr><td><code id="predict_+3A_labels.universe">labels.universe</code></td>
<td>
<p> important for models produced by loocv: all possible labels in the particular data set  </p>
</td></tr>
<tr><td><code id="predict_+3A_...">...</code></td>
<td>
<p> additional argument(s) </p>
</td></tr>
</table>


<h3>Value</h3>

<p>returns a list of prediction values for classes
</p>
<table>
<tr><td><code>pred.class</code></td>
<td>
<p>predicted class</p>
</td></tr>
<tr><td><code>tab</code></td>
<td>
<p>confusion table</p>
</td></tr>
<tr><td><code>error</code></td>
<td>
<p>missclassification error</p>
</td></tr>
<tr><td><code>sensitivity</code></td>
<td>
<p>sensitivity</p>
</td></tr>
<tr><td><code>specificity</code></td>
<td>
<p>specificity</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Natalia Becker </p>


<h3>See Also</h3>

 <p><a href="e1071.html#topic+svm">svm</a>, <code><a href="#topic+svmfs">svmfs</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>
seed&lt;- 123
train&lt;-sim.data(n = 200, ng = 100, nsg = 10, corr=FALSE, seed=seed )
print(str(train)) 

#train standard svm
my.svm&lt;-svm(x=t(train$x), y=train$y, kernel="linear")

# test with other data
test&lt;- sim.data(n = 200, ng = 100, nsg = 10, seed=(seed+1) )

# Check accuracy standard SVM
my.pred &lt;-ifelse( predict(my.svm, t(test$x)) &gt;0,1,-1)
# Check accuracy:
table(my.pred, test$y)

## Not run: # define set values of tuning parameter lambda1 for SCAD 
lambda1.scad &lt;- c (seq(0.01 ,0.05, .01),  seq(0.1,0.5, 0.2), 1 ) 
# for presentation don't check  all lambdas : time consuming! 
# computation intensive; for demostration reasons only for the first 100 features
# and only for 10 Iterations maxIter=10, default maxIter=700

system.time(fit.scad&lt;- svmfs(x=t(train$x)[,1:100],y=train$y, fs.method="scad", cross.outer= 0,
	grid.search = "discrete",  lambda1.set=lambda1.scad[1:3], show="none",
	parms.coding = "none", maxIter=10,
	inner.val.method = "cv", cross.inner= 5, seed=seed, verbose=FALSE))


# SCAD 
test.error.scad&lt;-predict(fit.scad, newdata=t(test$x)[,1:100],newdata.labels=test$y )
 # Check accuracy SCAD SVM  	
print(test.error.scad$tab)	

## End(Not run)

#########################################
## analog for 1-norm SVM
#epsi.set&lt;-vector(); for (num in (1:9)) epsi.set&lt;-sort(c(epsi.set, c(num*10^seq(-5, -1, 1 ))) )
#lambda1.1norm &lt;- 	epsi.set[c(3,5)] # 2 params
#
## train 1norm SVM
# norm1.fix&lt;- svmfs(t(train$x), y=train$y, fs.method="1norm", 
#		cross.outer= 0, grid.search = "discrete",  
#		lambda1.set=lambda1.1norm, show="none",
#		parms.coding = "none",
#		maxIter = 700, inner.val.method = "cv", cross.inner= 5,
#		seed=seed, verbose=FALSE ) 	
#	
#	print(norm1.fix)   
#
## L1-norm SVM
#test.error.1norm&lt;-predict(norm1.fix, newdata=t(test$x),newdata.labels=test$y )
# # Check accuracy L1-norm SVM  	
#print(test.error.1norm$tab)	
</code></pre>

<hr>
<h2 id='print'> Print Function for FS SVM</h2><span id='topic+print.penSVM'></span>

<h3>Description</h3>

<p>Print Function for FS SVM
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'penSVM'
print(x,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print_+3A_x">x</code></td>
<td>
<p> model trained by scad or 1norm svm of class PenSVM </p>
</td></tr>
<tr><td><code id="print_+3A_...">...</code></td>
<td>
<p> additional argument(s) </p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Natalia Becker </p>


<h3>See Also</h3>

 <p><a href="e1071.html#topic+svm">svm</a>, <code><a href="#topic+svmfs">svmfs</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>
seed&lt;- 123
train&lt;-sim.data(n = 20, ng = 100, nsg = 10, corr=FALSE, seed=seed )
print(str(train)) 

# for presentation don't check  all lambdas : time consuming! 
model &lt;- scadsvc(as.matrix(t(train$x)), y=train$y, lambda=0.05)
print(str(model))

print(model)
</code></pre>

<hr>
<h2 id='scadsvc'> Fit SCAD SVM model  </h2><span id='topic+scadsvc'></span>

<h3>Description</h3>

<p>SVM with variable selection (clone selection) using SCAD penalty.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>scadsvc(lambda1 = 0.01, x, y, a = 3.7, tol= 10^(-4), class.weights= NULL,
 seed=123, maxIter=700, verbose=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="scadsvc_+3A_lambda1">lambda1</code></td>
<td>
<p> tuning parameter in SCAD function (default : 0.01)  </p>
</td></tr>
<tr><td><code id="scadsvc_+3A_x">x</code></td>
<td>
<p> n-by-d data matrix to train (n chips/patients, d clones/genes) </p>
</td></tr>
<tr><td><code id="scadsvc_+3A_y">y</code></td>
<td>
<p> vector of class labels  -1 or 1\'s (for n chips/patiens ) </p>
</td></tr>
<tr><td><code id="scadsvc_+3A_a">a</code></td>
<td>
<p> tuning parameter in scad function (default: 3.7) </p>
</td></tr>
<tr><td><code id="scadsvc_+3A_tol">tol</code></td>
<td>
<p> the cut-off value to be taken as 0 </p>
</td></tr>
<tr><td><code id="scadsvc_+3A_class.weights">class.weights</code></td>
<td>
<p> a named vector of weights for the different classes, 
used for asymetric class sizes. 
Not all factor levels have to be supplied (default weight: 1).
All components have to be named. (default: NULL)</p>
</td></tr>
<tr><td><code id="scadsvc_+3A_seed">seed</code></td>
<td>
<p> seed </p>
</td></tr>
<tr><td><code id="scadsvc_+3A_maxiter">maxIter</code></td>
<td>
<p> maximal iteration, default: 700 </p>
</td></tr>
<tr><td><code id="scadsvc_+3A_verbose">verbose</code></td>
<td>
<p> verbose, default: TRUE </p>
</td></tr>
</table>


<h3>Details</h3>

<p>Adopted from Matlab code: http://www4.stat.ncsu.edu/~hzhang/software.html
</p>


<h3>Value</h3>

<table>
<tr><td><code>w</code></td>
<td>
<p>coefficients of the hyperplane.</p>
</td></tr>
<tr><td><code>b</code></td>
<td>
<p>intercept of the hyperplane.</p>
</td></tr>
<tr><td><code>xind</code></td>
<td>
<p>the index of the selected features (genes) in the data matrix. </p>
</td></tr>
<tr><td><code>xqx</code></td>
<td>
<p>internal calculations product <code class="reqn">xqx = 0.5 * x1 * inv_Q * t(x1)</code>, see code for more details.  </p>
</td></tr>
<tr><td><code>fitted</code></td>
<td>
<p>fit of hyperplane f(x) for all _training_ samples with reduced set of features. </p>
</td></tr>
<tr><td><code>index</code></td>
<td>
<p>the index of the resulting support vectors in the data matrix.</p>
</td></tr> 
<tr><td><code>type</code></td>
<td>
<p>type of svm, from svm function.  </p>
</td></tr>
<tr><td><code>lambda1</code></td>
<td>
<p>optimal lambda1.  </p>
</td></tr>
<tr><td><code>gacv</code></td>
<td>
<p>corresponding gacv.</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p> nuber of iterations. </p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Axel Benner </p>


<h3>References</h3>

<p>Zhang, H. H., Ahn, J., Lin, X. and Park, C. (2006). <em> Gene selection using
support vector machines with nonconvex penalty. Bioinformatics, <b>22</b>, pp. 88-95</em>. 
</p>


<h3>See Also</h3>

  <p><code><a href="#topic+findgacv.scad">findgacv.scad</a></code>,  <code><a href="#topic+predict.penSVM">predict.penSVM</a></code>, <code><a href="#topic+sim.data">sim.data</a></code>  </p>


<h3>Examples</h3>

<pre><code class='language-R'>

# simulate data
train&lt;-sim.data(n = 200, ng = 100, nsg = 10, corr=FALSE, seed=12)
print(str(train)) 
	
# train data	
model &lt;- scadsvc(as.matrix(t(train$x)), y=train$y, lambda=0.01)
print(str(model))

print(model)

</code></pre>

<hr>
<h2 id='sim.data'> Simulation of microarray data </h2><span id='topic+sim.data'></span>

<h3>Description</h3>

<p>Simulation of 'n' samples. Each sample has 'sg' genes,  only 'nsg' of them are called significant and 
have influence on class labels. All other '(ng - nsg)' genes are called ballanced.
All gene ratios are drawn from a multivariate normal distribution. 
There is a posibility to create blocks of highly correlated genes.     
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sim.data(n = 256, ng = 1000, nsg = 100,
		 p.n.ratio = 0.5, 
		 sg.pos.factor= 1, sg.neg.factor= -1,
		 # correlation info:
		 corr = FALSE, corr.factor = 0.8,
		 # block info:
		 blocks = FALSE, n.blocks = 6, nsg.block = 1, ng.block = 5, 
		 seed = 123, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sim.data_+3A_n">n</code></td>
<td>
<p> number of samples, logistic regression works well if <code class="reqn">n&gt;200</code>!</p>
</td></tr>
<tr><td><code id="sim.data_+3A_ng">ng</code></td>
<td>
<p> number of genes</p>
</td></tr>
<tr><td><code id="sim.data_+3A_nsg">nsg</code></td>
<td>
<p> number of significant genes</p>
</td></tr>
<tr><td><code id="sim.data_+3A_p.n.ratio">p.n.ratio</code></td>
<td>
<p> ratio between positive and negative significant genes (default 0.5)</p>
</td></tr> 
<tr><td><code id="sim.data_+3A_sg.pos.factor">sg.pos.factor</code></td>
<td>
<p> impact factor of <b>positive</b> significant genes on the classifaction, default:  1  </p>
</td></tr> 
<tr><td><code id="sim.data_+3A_sg.neg.factor">sg.neg.factor</code></td>
<td>
<p> impact factor of <b>negative</b> significant genes on the classifaction,default:  -1  </p>
</td></tr> 
<tr><td><code id="sim.data_+3A_corr">corr</code></td>
<td>
<p> are the genes correalted to each other? (default FALSE). see Details</p>
</td></tr>
<tr><td><code id="sim.data_+3A_corr.factor">corr.factor</code></td>
<td>
<p> correlation factorfor genes, between 0 and 1 (default 0.8)   </p>
</td></tr>
<tr><td><code id="sim.data_+3A_blocks">blocks</code></td>
<td>
<p> are blocks of highly correlated genes are allowed? (default FALSE)</p>
</td></tr>
<tr><td><code id="sim.data_+3A_n.blocks">n.blocks</code></td>
<td>
<p> number of blocks </p>
</td></tr>
<tr><td><code id="sim.data_+3A_nsg.block">nsg.block</code></td>
<td>
<p> number of significant genes per block</p>
</td></tr>
<tr><td><code id="sim.data_+3A_ng.block">ng.block</code></td>
<td>
<p> number of genes per block</p>
</td></tr>
<tr><td><code id="sim.data_+3A_seed">seed</code></td>
<td>
<p> seed</p>
</td></tr>
<tr><td><code id="sim.data_+3A_...">...</code></td>
<td>
<p> additional argument(s) </p>
</td></tr>
</table>


<h3>Details</h3>

<p>If no blockes (n.blocks=0 or blocks=FALSE) are defined and corr=TRUE
create covarance matrix for all genes! with decrease of correlation :  <code class="reqn">cov(i,j)=cov(j,i)= corr.factor^(i-j)</code>
</p>


<h3>Value</h3>

<table>
<tr><td><code>x</code></td>
<td>
<p>matrix of simulated data. Genes in rows and samples in columns</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>named vector of class labels</p>
</td></tr>
<tr><td><code>seed</code></td>
<td>
<p>seed</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Wiebke Werft, Natalia Becker </p>


<h3>See Also</h3>

 <p><code><a href="MASS.html#topic+mvrnorm">mvrnorm</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>
my.seed&lt;-123

# 1. simulate 20 samples, with 100 genes in each. Only the first two genes
# have an impact on the class labels.
# All genes are assumed to be i.i.d. 
train&lt;-sim.data(n = 20, ng = 100, nsg = 3, corr=FALSE, seed=my.seed )
print(str(train)) 

# 2. change the proportion between positive and negative significant genes 
#(from 0.5 to 0.8)
train&lt;-sim.data(n = 20, ng = 100, nsg = 10, p.n.ratio = 0.8,  seed=my.seed )
rownames(train$x)[1:15]
# [1] "pos1" "pos2" "pos3" "pos4" "pos5" "pos6" "pos7" "pos8" 
# [2] "neg1" "neg2" "bal1" "bal2" "bal3" "bal4" "bal5"

# 3. assume to have correlation for positive significant genes, 
# negative significant genes and 'balanced' genes separatly. 
train&lt;-sim.data(n = 20, ng = 100, nsg = 10, corr=TRUE, seed=my.seed )
#cor(t(train$x[1:15,]))

# 4. add 6 blocks of 5 genes each and only one significant gene per block.
# all genes in the block are correlated with constant correlation factor
#  corr.factor=0.8 		
train&lt;-sim.data(n = 20, ng = 100, nsg = 6, corr=TRUE, corr.factor=0.8,
			 blocks=TRUE, n.blocks=6, nsg.block=1, ng.block=5, seed=my.seed )
print(str(train)) 
# first block
#cor(t(train$x[1:5,]))
# second block
#cor(t(train$x[6:10,]))

</code></pre>

<hr>
<h2 id='sortmat'>Sort matrix or data frame  </h2><span id='topic+sortmat'></span>

<h3>Description</h3>

<p> A   useful function for ranking. 
Sort matrix or dataframe 'Mat', by column(s) 'Sort' in decrising or increasing order.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sortmat (Mat, Sort, decreasing=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sortmat_+3A_mat">Mat</code></td>
<td>
<p> a matrix  or a data frame  </p>
</td></tr>
<tr><td><code id="sortmat_+3A_sort">Sort</code></td>
<td>
<p> Sort is a number ! </p>
</td></tr>
<tr><td><code id="sortmat_+3A_decreasing">decreasing</code></td>
<td>
<p>   in decreasing order? default: FALSE   </p>
</td></tr>
</table>


<h3>Value</h3>

<p>sorted matrix or data frame
</p>


<h3>Author(s)</h3>

<p> found in world wide web: http://tolstoy.newcastle.edu.au/R/help/99b/0668.html</p>


<h3>Examples</h3>

<pre><code class='language-R'>
m &lt;- matrix(c(9:5, c(1, 4, 3, 3, 5), c(1, 2, 4, 3, 5)), ncol = 3, byrow = FALSE)

print( m)
#    [,1] [,2] [,3]
#[1,]    9    1    1
#[2,]    8    4    2
#[3,]    7    3    4
#[4,]    6    3    3
#[5,]    5    5    5

# sort first according to the second column then if equal according to the third column 
print(m1 &lt;- sortmat(Mat = m, Sort = c(2, 3)))
#     [,1] [,2] [,3]
#[1,]    9    1    1
#[2,]    6    3    3
#[3,]    7    3    4
#[4,]    8    4    2
#[5,]    5    5    5


# sort first according to the third (!)  column then if equal according
# to the second column 
print(m2 &lt;- sortmat(Mat = m, Sort = c(3, 2)))
#     [,1] [,2] [,3]
#[1,]    9    1    1
#[2,]    8    4    2
#[3,]    6    3    3
#[4,]    7    3    4
#[5,]    5    5    5

# Note m1 and m2 are not equal!!!!
all(m1==m2) #FALSE


# in decreasing order
print(m3 &lt;- sortmat(Mat = m, Sort = c(2, 3), decreasing=TRUE))
#     [,1] [,2] [,3]
#[1,]    5    5    5
#[2,]    8    4    2
#[3,]    7    3    4
#[4,]    6    3    3
#[5,]    9    1    1

</code></pre>

<hr>
<h2 id='svmfs'>  Fits  SVM with variable selection using penalties.</h2><span id='topic+svmfs'></span><span id='topic+svmfs.default'></span><span id='topic+DrHSVM'></span><span id='topic+scad_L2.svc'></span>

<h3>Description</h3>

<p>Fits  SVM with variable selection (clone selection) using penalties SCAD,  L1 norm, Elastic Net (L1 + L2 norms) and ELastic SCAD (SCAD + L1 norm). 
Additionally tuning parameter search is presented by two approcaches: fixed grid or interval search.
NOTE: The name of the function has been changed: svmfs instead of svm.fs! 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## Default S3 method:
svmfs(x,y,
				fs.method = c("scad", "1norm", "scad+L2", "DrHSVM"),
				grid.search=c("interval","discrete"),
				lambda1.set=NULL,  
				lambda2.set=NULL,
				bounds=NULL, 
				parms.coding= c("log2","none"),
				maxevals=500, 
				inner.val.method = c("cv", "gacv"),
				cross.inner= 5,
				show= c("none", "final"),
				calc.class.weights=FALSE,
				class.weights=NULL, 
				seed=123, 
				maxIter=700, 
				verbose=TRUE,
				...) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="svmfs_+3A_x">x</code></td>
<td>
<p> input matrix with genes in columns and samples in rows!  </p>
</td></tr>
<tr><td><code id="svmfs_+3A_y">y</code></td>
<td>
<p> numerical vector of class labels, -1 , 1 </p>
</td></tr>
<tr><td><code id="svmfs_+3A_fs.method">fs.method</code></td>
<td>
<p> feature selection method. Availible 'scad',  '1norm' for 1-norm,  &quot;DrHSVM&quot; for Elastic Net 
and &quot;scad+L2&quot; for Elastic SCAD </p>
</td></tr>
<tr><td><code id="svmfs_+3A_grid.search">grid.search</code></td>
<td>
<p> chose the search method for tuning lambda1,2: 'interval' or 'discrete', default: 'interval'</p>
</td></tr>
<tr><td><code id="svmfs_+3A_lambda1.set">lambda1.set</code></td>
<td>
<p> for fixed grid search: fixed grid for lambda1, default: NULL  </p>
</td></tr>
<tr><td><code id="svmfs_+3A_lambda2.set">lambda2.set</code></td>
<td>
<p> for fixed grid search: fixed grid for lambda2, default: NULL  </p>
</td></tr>
<tr><td><code id="svmfs_+3A_bounds">bounds</code></td>
<td>
<p> for interval grid search: fixed grid for lambda2, default: NULL  </p>
</td></tr>
<tr><td><code id="svmfs_+3A_parms.coding">parms.coding</code></td>
<td>
<p> for interval grid search: parms.coding: none or log2  , default: log2  </p>
</td></tr>
<tr><td><code id="svmfs_+3A_maxevals">maxevals</code></td>
<td>
<p> the maximum number of DIRECT function evaluations, default: 500.   </p>
</td></tr>

<tr><td><code id="svmfs_+3A_calc.class.weights">calc.class.weights</code></td>
<td>
<p> calculate class.weights for SVM, default: FALSE </p>
</td></tr>
<tr><td><code id="svmfs_+3A_class.weights">class.weights</code></td>
<td>
<p> a named vector of weights for the different
classes, used for asymetric class sizes. Not all factor levels have
to be supplied (default weight: 1). All components have to be named.  </p>
</td></tr>
<tr><td><code id="svmfs_+3A_inner.val.method">inner.val.method</code></td>
<td>
<p> method for the inner validation: cross validation, gacv , default cv  </p>
</td></tr>
<tr><td><code id="svmfs_+3A_cross.inner">cross.inner</code></td>
<td>
<p> 'cross.inner'-fold cv, default: 5 </p>
</td></tr>
<tr><td><code id="svmfs_+3A_show">show</code></td>
<td>
<p> for interval search: show plots of  DIRECT algorithm:    none, final iteration, all iterations. Default: none  </p>
</td></tr>
<tr><td><code id="svmfs_+3A_seed">seed</code></td>
<td>
<p> seed </p>
</td></tr>
<tr><td><code id="svmfs_+3A_maxiter">maxIter</code></td>
<td>
<p> maximal iteration, default: 700 </p>
</td></tr>
<tr><td><code id="svmfs_+3A_verbose">verbose</code></td>
<td>
<p> verbose?, default: TRUE </p>
</td></tr>
<tr><td><code id="svmfs_+3A_...">...</code></td>
<td>
<p> additional argument(s) </p>
</td></tr>
</table>


<h3>Details</h3>

<p>The goodness of the model is highly correlated with the choice of tuning parameter lambda.
Therefore the model is trained with different lambdas and the best model with optimal tuning parameter 
is used in futher analysises.
For very small lamdas is recomended to use maxIter, otherweise the algorithms is slow or might not converge.
</p>
<p>The Feature Selection methods are using different techniques for finding optimal tunung parameters
By SCAD SVM Generalized approximate cross validation (gacv) error is calculated for each pre-defined tuning parameter. 
</p>
<p>By L1-norm SVM the cross validation (default 5-fold) missclassification error is calculated for each lambda.  
After training and cross validation, the optimal lambda with minimal missclassification error is choosen,
and a final model with optimal lambda is created for the whole data set.    
</p>


<h3>Value</h3>

<table>
<tr><td><code>classes</code></td>
<td>
<p> vector of class labels as input 'y' </p>
</td></tr>
<tr><td><code>sample.names</code></td>
<td>
<p>sample names</p>
</td></tr>
<tr><td><code>class.method</code></td>
<td>
<p>feature selection method</p>
</td></tr>

<tr><td><code>seed</code></td>
<td>
<p> seed</p>
</td></tr>
<tr><td><code>model</code></td>
<td>
<p> final model 
</p>

<ul>
<li><p> w - coefficients of the hyperplane 
</p>
</li>
<li><p> b - intercept of the hyperplane
</p>
</li>
<li><p> xind -   the index of the selected features (genes) in the data matrix.
</p>
</li>
<li><p> index - the index of the resulting support vectors in the data matrix. 
</p>
</li>
<li><p> type - type of svm, from svm function 
</p>
</li>
<li><p> lam.opt  - optimal lambda  
</p>
</li>
<li><p> gacv - corresponding gacv 
</p>
</li></ul>

</td></tr>
</table>


<h3>Author(s)</h3>

<p>Natalia Becker<br />
<a href="mailto:natalie_becker@gmx.de">natalie_becker@gmx.de</a>
</p>


<h3>References</h3>

 
<p>Becker, N., Werft, W., Toedt, G., Lichter, P. and Benner, A.(2009) PenalizedSVM: a R-package for feature selection SVM classification, 
Bioinformatics, 25(13),p 1711-1712
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+predict.penSVM">predict.penSVM</a></code>,   <code><a href="e1071.html#topic+svm">svm</a></code> (in package <span class="pkg">e1071</span>) </p>


<h3>Examples</h3>

<pre><code class='language-R'>


		
		seed&lt;- 123
		
		train&lt;-sim.data(n = 200, ng = 100, nsg = 10, corr=FALSE, seed=seed )
		print(str(train)) 
		
		
		### Fixed grid ####
		
		# train SCAD SVM ####################
		# define set values of tuning parameter lambda1 for SCAD 
		lambda1.scad &lt;- c (seq(0.01 ,0.05, .01),  seq(0.1,0.5, 0.2), 1 ) 
		# for presentation don't check  all lambdas : time consuming! 
		lambda1.scad&lt;-lambda1.scad[2:3]
		# 
		# train SCAD SVM
		
		# computation intensive; for demostration reasons only for the first 100 features 
		# and only for 10 Iterations maxIter=10, default maxIter=700
		system.time(scad.fix&lt;- svmfs(t(train$x)[,1:100], y=train$y, fs.method="scad", 
  		cross.outer= 0, grid.search = "discrete",  
  		lambda1.set=lambda1.scad,
  		parms.coding = "none", show="none",
  		maxIter = 10, inner.val.method = "cv", cross.inner= 5,
  		seed=seed, verbose=FALSE) 	)
			
		print(scad.fix)
			
		# train 1NORM SVM 	################	
		# define set values of tuning parameter lambda1 for 1norm
		#epsi.set&lt;-vector(); for (num in (1:9)) epsi.set&lt;-sort(c(epsi.set,
		#    c(num*10^seq(-5, -1, 1 ))) )
		## for presentation don't check  all lambdas : time consuming! 
		#lambda1.1norm &lt;- 	epsi.set[c(3,5)] # 2 params
		#
		### train 1norm SVM
		## time consuming: for presentation only for the first 100 features    
		#norm1.fix&lt;- svmfs(t(train$x)[,1:100], y=train$y, fs.method="1norm", 
		#			cross.outer= 0, grid.search = "discrete",  
		#			lambda1.set=lambda1.1norm,
		#			parms.coding = "none", show="none",
		#			maxIter = 700, inner.val.method = "cv", cross.inner= 5,
		#			seed=seed, verbose=FALSE ) 	
		#	
		#	print(norm1.fix)   
		
		### Interval  search  ####
		
		
		seed &lt;- 123
		
		train&lt;-sim.data(n = 200, ng = 100, nsg = 10, corr=FALSE, seed=seed )
		print(str(train)) 
		
		
		test&lt;-sim.data(n = 200, ng = 100, nsg = 10, corr=FALSE, seed=seed+1 )
		print(str(test)) 
		
				
		bounds=t(data.frame(log2lambda1=c(-10, 10)))
						colnames(bounds)&lt;-c("lower", "upper")	
		
		# computation intensive; for demostration reasons only for the first 100 features 
		# and only for 10 Iterations maxIter=10, default maxIter=700
		print("start interval search")
			system.time( scad&lt;- svmfs(t(train$x)[,1:100], y=train$y,
			 fs.method="scad", bounds=bounds, 
			 cross.outer= 0, grid.search = "interval",  maxIter = 10, 
			 inner.val.method = "cv", cross.inner= 5, maxevals=500,
			 seed=seed, parms.coding = "log2", show="none", verbose=FALSE ) )
		print("scad final model")
		print(str(scad$model))
				
		(scad.5cv.test&lt;-predict.penSVM(scad, t(test$x)[,1:100], newdata.labels=test$y)   )
		
		
		print(paste("minimal 5-fold cv error:", scad$model$fit.info$fmin, 
		"by log2(lambda1)=", scad$model$fit.info$xmin))
		
		print(" all lambdas with the same minimum? ")
		print(scad$model$fit.info$ points.fmin) 
		
		print(paste(scad$model$fit.info$neval, "visited points"))
		
		
		print(" overview: over all visitied points in tuning parameter space 
		with corresponding cv errors")
		print(data.frame(Xtrain=scad$model$fit.info$Xtrain, 
					cv.error=scad$model$fit.info$Ytrain))
		# 						 
		
		# create  3 plots on one screen: 
		# 1st plot: distribution of initial points in tuning parameter space
		# 2nd plot: visited lambda points vs. cv errors
		# 3rd plot: the same as the 2nd plot, Ytrain.exclude points are excluded. 
		# The value cv.error = 10^16 stays for the cv error for an empty model ! 
		.plot.EPSGO.parms (scad$model$fit.info$Xtrain, scad$model$fit.info$Ytrain,
				bound=bounds, Ytrain.exclude=10^16, plot.name=NULL )
		
 # end of \donttest

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
