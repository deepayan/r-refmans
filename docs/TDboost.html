<!DOCTYPE html><html><head><title>Help for package TDboost</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {TDboost}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#FHT'><p>Simulation data generated based from the FHT model used in Yang, Y., Qian, W. and Zou, H. (2013).</p></a></li>
<li><a href='#plot.TDboost'><p> Marginal plots of fitted TDboost objects</p></a></li>
<li><a href='#predict.TDboost'><p> Predict method for TDboost Model Fits</p></a></li>
<li><a href='#relative.influence'><p> Methods for estimating relative influence</p></a></li>
<li><a href='#summary.TDboost'><p>Summary of a TDboost object</p></a></li>
<li><a href='#TDboost'><p>TDboost Tweedie Regression Modeling</p></a></li>
<li><a href='#TDboost.object'><p>TDboost Tweedie Regression Model Object</p></a></li>
<li><a href='#TDboost.perf'><p>TDboost performance</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>A Boosted Tweedie Compound Poisson Model</td>
</tr>
<tr>
<td>Version:</td>
<td>1.5</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-01-17</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.12.0), lattice</td>
</tr>
<tr>
<td>Description:</td>
<td>An implementation of a boosted Tweedie compound Poisson model proposed by Yang, Y., Qian, W. and Zou, H. (2018) &lt;<a href="https://doi.org/10.1080%2F07350015.2016.1200981">doi:10.1080/07350015.2016.1200981</a>&gt;. It is capable of fitting a flexible nonlinear Tweedie compound Poisson model (or a gamma model) and capturing high-order interactions among predictors. This package is based on the 'gbm' package originally developed by Greg Ridgeway.</td>
</tr>
<tr>
<td>LazyData:</td>
<td>yes</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-01-18 04:14:20 UTC; jiaozi</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-01-18 10:00:14 UTC</td>
</tr>
<tr>
<td>Author:</td>
<td>Yi Yang [aut, cre] (http://www.math.mcgill.ca/yyang/),
  Hui Zou [aut] (http://users.stat.umn.edu/~zouxx019/),
  Wei Qian [aut] (https://sites.google.com/a/udel.edu/weiqian/),
  Greg Ridgeway [ctb, cph]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Yi Yang &lt;yi.yang6@mcgill.ca&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
</table>
<hr>
<h2 id='FHT'>Simulation data generated based from the FHT model used in Yang, Y., Qian, W. and Zou, H. (2013).</h2><span id='topic+FHT'></span><span id='topic+data1'></span><span id='topic+data2'></span>

<h3>Description</h3>

<p>There are two data sets, one for training and the other for testing. The training data set has n = 200 observations and p = 6 predictors. The testing data set has n = 20 observations and p = 6 predictors. See details in Friedman et al. (2010).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(FHT)</code></pre>


<h3>Format</h3>

<p>Two data frames both contain the following columns:
</p>

<dl>
<dt>X1-X6</dt><dd><p>predictor columns</p>
</dd>
<dt>Y</dt><dd><p>response variable</p>
</dd>
</dl>



<h3>References</h3>

<p>Yang, Y., Qian, W. and Zou, H. (2013), &ldquo;A Boosted Tweedie Compound Poisson Model for Insurance Premium&rdquo; Preprint.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(FHT)
</code></pre>

<hr>
<h2 id='plot.TDboost'> Marginal plots of fitted TDboost objects </h2><span id='topic+plot.TDboost'></span>

<h3>Description</h3>

<p>Plots the marginal effect of the selected variables by &quot;integrating&quot; out the other variables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'TDboost'
plot(x,
     i.var = 1,
     n.trees = x$n.trees,
     continuous.resolution = 100,
     return.grid = FALSE,
     ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.TDboost_+3A_x">x</code></td>
<td>
<p> a <code><a href="#topic+TDboost.object">TDboost.object</a></code> fitted using a call to <code><a href="#topic+TDboost">TDboost</a></code></p>
</td></tr>
<tr><td><code id="plot.TDboost_+3A_i.var">i.var</code></td>
<td>
<p>a vector of indices or the names of the variables to plot. If
using indices, the variables are indexed in the same order that they appear
in the initial <code>TDboost</code> formula.
If <code>length(i.var)</code> is between 1 and 3 then <code>plot.TDboost</code> produces the plots. Otherwise,
<code>plot.TDboost</code> returns only the grid of evaluation points and their average predictions</p>
</td></tr>
<tr><td><code id="plot.TDboost_+3A_n.trees">n.trees</code></td>
<td>
<p> the number of trees used to generate the plot. Only the first
<code>n.trees</code> trees will be used</p>
</td></tr>
<tr><td><code id="plot.TDboost_+3A_continuous.resolution">continuous.resolution</code></td>
<td>
<p> The number of equally space points at which to
evaluate continuous predictors </p>
</td></tr>
<tr><td><code id="plot.TDboost_+3A_return.grid">return.grid</code></td>
<td>
<p> if <code>TRUE</code> then <code>plot.TDboost</code> produces no graphics and only returns
the grid of evaluation points and their average predictions. This is useful for
customizing the graphics for special variable types or for dimensions greater
than 3 </p>
</td></tr>
<tr><td><code id="plot.TDboost_+3A_...">...</code></td>
<td>
<p> other arguments passed to the plot function </p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>plot.TDboost</code> produces low dimensional projections of the
<code><a href="#topic+TDboost.object">TDboost.object</a></code> by integrating out the variables not included in the
<code>i.var</code> argument. The function selects a grid of points and uses the
weighted tree traversal method described in Friedman (2001) to do the
integration. Based on the variable types included in the projection,
<code>plot.TDboost</code> selects an appropriate display choosing amongst line plots,
contour plots, and <code><a href="lattice.html#topic+lattice">lattice</a></code> plots. If the default graphics
are not sufficient the user may set <code>return.grid=TRUE</code>, store the result
of the function, and develop another graphic display more appropriate to the
particular example.
</p>


<h3>Value</h3>

<p>Nothing unless <code>return.grid</code> is true then <code>plot.TDboost</code> produces no
graphics and only returns the grid of evaluation points and their average
predictions.
</p>


<h3>Author(s)</h3>

<p>Yi Yang <a href="mailto:yi.yang6@mcgill.ca">yi.yang6@mcgill.ca</a>, Wei Qian <a href="mailto:wxqsma@rit.edu">wxqsma@rit.edu</a> and Hui Zou <a href="mailto:hzou@stat.umn.edu">hzou@stat.umn.edu</a></p>


<h3>References</h3>

<p>Yang, Y., Qian, W. and Zou, H. (2013), &ldquo;A Boosted Tweedie Compound Poisson Model for Insurance Premium&rdquo; Preprint.
</p>
<p>G. Ridgeway (1999). &ldquo;The state of boosting,&rdquo; <em>Computing Science and
Statistics</em> 31:172-181.
</p>
<p>J.H. Friedman (2001). &quot;Greedy Function Approximation: A Gradient Boosting
Machine,&quot; Annals of Statistics 29(4).
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+TDboost">TDboost</a></code>, <code><a href="#topic+TDboost.object">TDboost.object</a></code>, <code><a href="graphics.html#topic+plot">plot</a></code> </p>

<hr>
<h2 id='predict.TDboost'> Predict method for TDboost Model Fits </h2><span id='topic+predict.TDboost'></span>

<h3>Description</h3>

<p>Predicted values based on an TDboost Tweedie regression model object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'TDboost'
predict(object,
        newdata,
        n.trees,
        single.tree=FALSE,
		  type=c("response","link"),
        ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.TDboost_+3A_object">object</code></td>
<td>
<p> Object of class inheriting from (<code><a href="#topic+TDboost.object">TDboost.object</a></code>) </p>
</td></tr>
<tr><td><code id="predict.TDboost_+3A_newdata">newdata</code></td>
<td>
<p> Data frame of observations for which to make predictions </p>
</td></tr>
<tr><td><code id="predict.TDboost_+3A_n.trees">n.trees</code></td>
<td>
<p> Number of trees used in the prediction. <code>n.trees</code> may
be a vector in which case predictions are returned for each
iteration specified</p>
</td></tr>
<tr><td><code id="predict.TDboost_+3A_single.tree">single.tree</code></td>
<td>
<p>If <code>single.tree=TRUE</code> then <code>predict.TDboost</code> returns
only the predictions from tree(s) <code>n.trees</code></p>
</td></tr> 
<tr><td><code id="predict.TDboost_+3A_type">type</code></td>
<td>

<p>type of prediction required. </p>

<ul>
<li><p> Type <code>"response"</code> gives predicted response mu(x) = E(Y|X=x) for the regression problems. It is the default.
</p>
</li>
<li><p> Type <code>"link"</code> gives the linear predictors x*b = log(mu(x)) = log(E(Y|X=x)) for the regression problems.</p>
</li></ul>
</td></tr>
<tr><td><code id="predict.TDboost_+3A_...">...</code></td>
<td>
<p> further arguments passed to or from other methods </p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>predict.TDboost</code> produces predicted values for each observation in <code>newdata</code> using the the first <code>n.trees</code> iterations of the boosting sequence. If <code>n.trees</code> is a vector than the result is a matrix with each column representing the predictions from TDboost models with <code>n.trees[1]</code> iterations, <code>n.trees[2]</code> iterations, and so on.
</p>
<p>The predictions from <code>TDboost</code> do not include the offset term. The user may add the value of the offset to the predicted value if desired.
</p>
<p>If <code>object</code> was fit using <code><a href="#topic+TDboost.fit">TDboost.fit</a></code> there will be no
<code>Terms</code> component. Therefore, the user has greater responsibility to make
sure that <code>newdata</code> is of the same format (order and number of variables)
as the one originally used to fit the model.
</p>


<h3>Value</h3>

<p>Returns a vector of predictions. By default the predictions are on the scale of f(x).
</p>


<h3>Author(s)</h3>

<p>Yi Yang <a href="mailto:yi.yang6@mcgill.ca">yi.yang6@mcgill.ca</a>, Wei Qian <a href="mailto:wxqsma@rit.edu">wxqsma@rit.edu</a> and Hui Zou <a href="mailto:hzou@stat.umn.edu">hzou@stat.umn.edu</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+TDboost">TDboost</a></code>, <code><a href="#topic+TDboost.object">TDboost.object</a></code>
</p>

<hr>
<h2 id='relative.influence'> Methods for estimating relative influence </h2><span id='topic+relative.influence'></span><span id='topic+permutation.test.TDboost'></span><span id='topic+TDboost.loss'></span>

<h3>Description</h3>

<p>Helper functions for computing the relative influence of each variable in the TDboost object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>relative.influence(object, n.trees)
permutation.test.TDboost(object, n.trees)
TDboost.loss(y,f,w,offset,dist,baseline)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="relative.influence_+3A_object">object</code></td>
<td>
<p>a <code>TDboost</code> object created from an initial call to <code><a href="#topic+TDboost">TDboost</a></code>.</p>
</td></tr>
<tr><td><code id="relative.influence_+3A_n.trees">n.trees</code></td>
<td>
<p> the number of trees to use for computations.</p>
</td></tr>
<tr><td><code id="relative.influence_+3A_y">y</code>, <code id="relative.influence_+3A_f">f</code>, <code id="relative.influence_+3A_w">w</code>, <code id="relative.influence_+3A_offset">offset</code>, <code id="relative.influence_+3A_dist">dist</code>, <code id="relative.influence_+3A_baseline">baseline</code></td>
<td>
<p>For <code>TDboost.loss</code>: These components are the
outcome, predicted value, observation weight, offset, distribution, and comparison
loss function, respectively.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is not intended for end-user use. These functions offer the different
methods for computing the relative influence in <code><a href="#topic+summary.TDboost">summary.TDboost</a></code>.
<code>TDboost.loss</code> is a helper function for <code>permutation.test.TDboost</code>.
</p>


<h3>Value</h3>

<p>Returns an unprocessed vector of estimated relative influences.
</p>


<h3>Author(s)</h3>

<p>Yi Yang <a href="mailto:yi.yang6@mcgill.ca">yi.yang6@mcgill.ca</a>, Wei Qian <a href="mailto:wxqsma@rit.edu">wxqsma@rit.edu</a> and Hui Zou <a href="mailto:hzou@stat.umn.edu">hzou@stat.umn.edu</a></p>


<h3>References</h3>

<p>Yang, Y., Qian, W. and Zou, H. (2013), &ldquo;A Boosted Tweedie Compound Poisson Model for Insurance Premium&rdquo; Preprint.
</p>
<p>G. Ridgeway (1999). &ldquo;The state of boosting,&rdquo; <em>Computing Science and
Statistics</em> 31:172-181.
</p>
<p>J.H. Friedman (2001). &quot;Greedy Function Approximation: A Gradient Boosting
Machine,&quot; Annals of Statistics 29(5):1189-1232.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+summary.TDboost">summary.TDboost</a></code> </p>

<hr>
<h2 id='summary.TDboost'>Summary of a TDboost object </h2><span id='topic+summary.TDboost'></span>

<h3>Description</h3>

<p>Computes the relative influence of each variable in the TDboost object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'TDboost'
summary(object,
        cBars=length(object$var.names),
        n.trees=object$n.trees,
        plotit=TRUE,
        order=TRUE,
        method=relative.influence,
        normalize=TRUE,
        ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.TDboost_+3A_object">object</code></td>
<td>
<p>a <code>TDboost</code> object created from an initial call to
<code><a href="#topic+TDboost">TDboost</a></code>.</p>
</td></tr>
<tr><td><code id="summary.TDboost_+3A_cbars">cBars</code></td>
<td>
<p> the number of bars to plot. If <code>order=TRUE</code> the only the
variables with the <code>cBars</code> largest relative influence will appear in the
barplot. If <code>order=FALSE</code> then the first <code>cBars</code> variables will
appear in the plot. In either case, the function will return the relative
influence of all of the variables.</p>
</td></tr>
<tr><td><code id="summary.TDboost_+3A_n.trees">n.trees</code></td>
<td>
<p> the number of trees used to generate the plot. Only the first
<code>n.trees</code> trees will be used.</p>
</td></tr>
<tr><td><code id="summary.TDboost_+3A_plotit">plotit</code></td>
<td>
<p> an indicator as to whether the plot is generated. </p>
</td></tr>
<tr><td><code id="summary.TDboost_+3A_order">order</code></td>
<td>
<p> an indicator as to whether the plotted and/or returned relative
influences are sorted. </p>
</td></tr>
<tr><td><code id="summary.TDboost_+3A_method">method</code></td>
<td>
<p> The function used to compute the relative influence.
<code><a href="#topic+relative.influence">relative.influence</a></code> is the default and is the same as that
described in Friedman (2001). The other current (and experimental) choice is
<code><a href="#topic+permutation.test.TDboost">permutation.test.TDboost</a></code>. This method randomly permutes each predictor
variable at a time and computes the associated reduction in predictive
performance. This is similar to the variable importance measures Breiman uses
for random forests, but <code>TDboost</code> currently computes using the entire training
dataset (not the out-of-bag observations.</p>
</td></tr>
<tr><td><code id="summary.TDboost_+3A_normalize">normalize</code></td>
<td>
<p> if <code>FALSE</code> then <code>summary.TDboost</code> returns the 
unnormalized influence. </p>
</td></tr>
<tr><td><code id="summary.TDboost_+3A_...">...</code></td>
<td>
<p> other arguments passed to the plot function. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>This returns the reduction attributable to each variable in sum of squared error in 
predicting the gradient on each iteration. It describes the relative influence 
of each variable in reducing the loss function. See the references below for 
exact details on the computation.
</p>


<h3>Value</h3>

<p>Returns a data frame where the first component is the variable name and the
second is the computed relative influence, normalized to sum to 100.
</p>


<h3>Author(s)</h3>

<p>Yi Yang <a href="mailto:yi.yang6@mcgill.ca">yi.yang6@mcgill.ca</a>, Wei Qian <a href="mailto:wxqsma@rit.edu">wxqsma@rit.edu</a> and Hui Zou <a href="mailto:hzou@stat.umn.edu">hzou@stat.umn.edu</a></p>


<h3>References</h3>

<p>Yang, Y., Qian, W. and Zou, H. (2013), &ldquo;A Boosted Tweedie Compound Poisson Model for Insurance Premium&rdquo; Preprint.
</p>
<p>G. Ridgeway (1999). &ldquo;The state of boosting,&rdquo; <em>Computing Science and
Statistics</em> 31:172-181.
</p>
<p>J.H. Friedman (2001). &quot;Greedy Function Approximation: A Gradient Boosting
Machine,&quot; Annals of Statistics 29(5):1189-1232.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+TDboost">TDboost</a></code> </p>

<hr>
<h2 id='TDboost'>TDboost Tweedie Regression Modeling</h2><span id='topic+TDboost'></span><span id='topic+TDboost.more'></span><span id='topic+TDboost.fit'></span>

<h3>Description</h3>

<p>Fits TDboost Tweedie Regression models.</p>


<h3>Usage</h3>

<pre><code class='language-R'>TDboost(formula = formula(data),
    distribution = list(name="EDM",alpha=1.5),
    data = list(),
    weights,
    var.monotone = NULL,
    n.trees = 100,
    interaction.depth = 1,
    n.minobsinnode = 10,
    shrinkage = 0.001,
    bag.fraction = 0.5,
    train.fraction = 1.0,
    cv.folds=0,
    keep.data = TRUE,
    verbose = TRUE)

TDboost.fit(x,y,
        offset = NULL,
        misc = NULL,
        distribution = list(name="EDM",alpha=1.5),
        w = NULL,
        var.monotone = NULL,
        n.trees = 100,
        interaction.depth = 1,
        n.minobsinnode = 10,
        shrinkage = 0.001,
        bag.fraction = 0.5,
        train.fraction = 1.0,
        keep.data = TRUE,
        verbose = TRUE,
        var.names = NULL,
        response.name = NULL)

TDboost.more(object,
         n.new.trees = 100,
         data = NULL,
         weights = NULL,
         offset = NULL,
         verbose = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="TDboost_+3A_formula">formula</code></td>
<td>
<p>a symbolic description of the model to be fit. The formula may 
include an offset term (e.g. y~offset(n)+x). If <code>keep.data=FALSE</code> in 
the initial call to <code>TDboost</code> then it is the user's responsibility to 
resupply the offset to <code><a href="#topic+TDboost.more">TDboost.more</a></code>.</p>
</td></tr>
<tr><td><code id="TDboost_+3A_distribution">distribution</code></td>
<td>
<p>a list with a component <code>name</code> specifying the distribution 
and any additional parameters needed. Tweedie regression is available and <code>distribution</code> must a list of the form 
<code>list(name="EDM",alpha=1.5)</code> where <code>alpha</code> is the index parameter that must be in (1,2]. 
When <code>alpha=2</code>, the distribution reduces to gamma.
The current version's Tweedie regression methods do 
not handle non-constant weights and will stop.</p>
</td></tr>
<tr><td><code id="TDboost_+3A_data">data</code></td>
<td>
<p>an optional data frame containing the variables in the model. By
default the variables are taken from <code>environment(formula)</code>, typically 
the environment from which <code>TDboost</code> is called. If <code>keep.data=TRUE</code> in 
the initial call to <code>TDboost</code> then <code>TDboost</code> stores a copy with the 
object. If <code>keep.data=FALSE</code> then subsequent calls to 
<code><a href="#topic+TDboost.more">TDboost.more</a></code> must resupply the same dataset. It becomes the user's 
responsibility to resupply the same data at this point.</p>
</td></tr>
<tr><td><code id="TDboost_+3A_weights">weights</code></td>
<td>
<p>an optional vector of weights to be used in the fitting process. 
Must be positive but do not need to be normalized. If <code>keep.data=FALSE</code> 
in the initial call to <code>TDboost</code> then it is the user's responsibility to 
resupply the weights to <code><a href="#topic+TDboost.more">TDboost.more</a></code>.</p>
</td></tr>
<tr><td><code id="TDboost_+3A_var.monotone">var.monotone</code></td>
<td>
<p>an optional vector, the same length as the number of
predictors, indicating which variables have a monotone increasing (+1),
decreasing (-1), or arbitrary (0) relationship with the outcome.</p>
</td></tr>
<tr><td><code id="TDboost_+3A_n.trees">n.trees</code></td>
<td>
<p>the total number of trees to fit. This is equivalent to the
number of iterations and the number of basis functions in the additive
expansion.</p>
</td></tr>
<tr><td><code id="TDboost_+3A_cv.folds">cv.folds</code></td>
<td>
<p>Number of cross-validation folds to perform. If <code>cv.folds</code>&gt;1 then
<code>TDboost</code>, in addition to the usual fit, will perform a cross-validation, calculate
an estimate of generalization error returned in <code>cv.error</code>.</p>
</td></tr>
<tr><td><code id="TDboost_+3A_interaction.depth">interaction.depth</code></td>
<td>
<p>The maximum depth of variable interactions. 1 implies
an additive model, 2 implies a model with up to 2-way interactions, etc.</p>
</td></tr>
<tr><td><code id="TDboost_+3A_n.minobsinnode">n.minobsinnode</code></td>
<td>
<p>minimum number of observations in the trees terminal
nodes. Note that this is the actual number of observations not the total
weight.</p>
</td></tr>
<tr><td><code id="TDboost_+3A_shrinkage">shrinkage</code></td>
<td>
<p>a shrinkage parameter applied to each tree in the expansion.
Also known as the learning rate or step-size reduction.</p>
</td></tr>
<tr><td><code id="TDboost_+3A_bag.fraction">bag.fraction</code></td>
<td>
<p>the fraction of the training set observations randomly
selected to propose the next tree in the expansion. This introduces randomnesses
into the model fit. If <code>bag.fraction</code>&lt;1 then running the same model twice
will result in similar but different fits. <code>TDboost</code> uses the R random number
generator so <code>set.seed</code> can ensure that the model can be
reconstructed. Preferably, the user can save the returned
<code><a href="#topic+TDboost.object">TDboost.object</a></code> using <code><a href="base.html#topic+save">save</a></code>.</p>
</td></tr>
<tr><td><code id="TDboost_+3A_train.fraction">train.fraction</code></td>
<td>
<p>The first <code>train.fraction * nrows(data)</code>
observations are used to fit the <code>TDboost</code> and the remainder are used for
computing out-of-sample estimates of the loss function.</p>
</td></tr>
<tr><td><code id="TDboost_+3A_keep.data">keep.data</code></td>
<td>
<p>a logical variable indicating whether to keep the data and
an index of the data stored with the object. Keeping the data and index makes
subsequent calls to <code><a href="#topic+TDboost.more">TDboost.more</a></code> faster at the cost of storing an
extra copy of the dataset.</p>
</td></tr>
<tr><td><code id="TDboost_+3A_object">object</code></td>
<td>
<p>a <code>TDboost</code> object created from an initial call to
<code><a href="#topic+TDboost">TDboost</a></code>.</p>
</td></tr>
<tr><td><code id="TDboost_+3A_n.new.trees">n.new.trees</code></td>
<td>
<p>the number of additional trees to add to <code>object</code>.</p>
</td></tr>
<tr><td><code id="TDboost_+3A_verbose">verbose</code></td>
<td>
<p>If TRUE, TDboost will print out progress and performance indicators.
If this option is left unspecified for TDboost.more then it uses <code>verbose</code> from
<code>object</code>.</p>
</td></tr>
<tr><td><code id="TDboost_+3A_x">x</code>, <code id="TDboost_+3A_y">y</code></td>
<td>
<p>For <code>TDboost.fit</code>: <code>x</code> is a data frame or data matrix containing the
predictor variables and <code>y</code> is the vector of outcomes. The number of rows
in <code>x</code> must be the same as the length of <code>y</code>.</p>
</td></tr>
<tr><td><code id="TDboost_+3A_offset">offset</code></td>
<td>
<p>a vector of values for the offset</p>
</td></tr>
<tr><td><code id="TDboost_+3A_misc">misc</code></td>
<td>
<p>For <code>TDboost.fit</code>: <code>misc</code> is an R object that is simply passed on to
the TDboost engine.</p>
</td></tr>
<tr><td><code id="TDboost_+3A_w">w</code></td>
<td>
<p>For <code>TDboost.fit</code>: <code>w</code> is a vector of weights of the same
length as the <code>y</code>.</p>
</td></tr>
<tr><td><code id="TDboost_+3A_var.names">var.names</code></td>
<td>
<p>For <code>TDboost.fit</code>: A vector of strings of length equal to the
number of columns of <code>x</code> containing the names of the predictor variables.</p>
</td></tr>
<tr><td><code id="TDboost_+3A_response.name">response.name</code></td>
<td>
<p>For <code>TDboost.fit</code>: A character string label for the response
variable.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This package implements a regression tree based gradient boosting estimator for nonparametric multiple Tweedie regression. The code is a modified version of <code>gbm</code> library originally written by Greg Ridgeway.
</p>
<p>Boosting is the process of iteratively adding basis functions in a greedy
fashion so that each additional basis function further reduces the selected
loss function. This implementation closely follows Friedman's Gradient
Boosting Machine (Friedman, 2001).
</p>
<p>In addition to many of the features documented in the Gradient Boosting Machine,
<code>TDboost</code> offers additional features including the out-of-bag estimator for
the optimal number of iterations, the ability to store and manipulate the
resulting <code>TDboost</code> object.
</p>
<p><code>TDboost.fit</code> provides the link between R and the C++ TDboost engine. <code>TDboost</code>
is a front-end to <code>TDboost.fit</code> that uses the familiar R modeling formulas.
However, <code><a href="stats.html#topic+model.frame">model.frame</a></code> is very slow if there are many
predictor variables. For power-users with many variables use <code>TDboost.fit</code>.
For general practice <code>TDboost</code> is preferable.
</p>


<h3>Value</h3>

<p><code>TDboost</code>, <code>TDboost.fit</code>, and <code>TDboost.more</code> return a
<code><a href="#topic+TDboost.object">TDboost.object</a></code>.
</p>


<h3>Author(s)</h3>

<p>Yi Yang <a href="mailto:yi.yang6@mcgill.ca">yi.yang6@mcgill.ca</a>, Wei Qian <a href="mailto:wxqsma@rit.edu">wxqsma@rit.edu</a> and Hui Zou <a href="mailto:hzou@stat.umn.edu">hzou@stat.umn.edu</a></p>


<h3>References</h3>

<p>Yang, Y., Qian, W. and Zou, H. (2013), &ldquo;A Boosted Tweedie Compound Poisson Model for Insurance Premium&rdquo; Preprint.
</p>
<p>G. Ridgeway (1999). &ldquo;The state of boosting,&rdquo; <em>Computing Science and
Statistics</em> 31:172-181.
</p>
<p>J.H. Friedman (2001). &ldquo;Greedy Function Approximation: A Gradient Boosting
Machine,&rdquo; <em>Annals of Statistics</em> 29(5):1189-1232.
</p>
<p>J.H. Friedman (2002). &ldquo;Stochastic Gradient Boosting,&rdquo; <em>Computational Statistics
and Data Analysis</em> 38(4):367-378.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+TDboost.object">TDboost.object</a></code>,
<code><a href="#topic+TDboost.perf">TDboost.perf</a></code>,
<code><a href="#topic+plot.TDboost">plot.TDboost</a></code>,
<code><a href="#topic+predict.TDboost">predict.TDboost</a></code>,
<code><a href="#topic+summary.TDboost">summary.TDboost</a></code>,
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(FHT)
# training on data1
TDboost1 &lt;- TDboost(Y~X1+X2+X3+X4+X5+X6,         # formula
    data=data1,                   # dataset
    var.monotone=c(0,0,0,0,0,0), # -1: monotone decrease,
                                 # +1: monotone increase,
                                 #  0: no monotone restrictions
    distribution=list(name="EDM",alpha=1.5),
                                 # specify Tweedie index parameter
    n.trees=3000,                # number of trees
    shrinkage=0.005,             # shrinkage or learning rate,
                                 # 0.001 to 0.1 usually work
    interaction.depth=3,         # 1: additive model, 2: two-way interactions, etc.
    bag.fraction = 0.5,          # subsampling fraction, 0.5 is probably best
    train.fraction = 0.5,        # fraction of data for training,
                                 # first train.fraction*N used for training
    n.minobsinnode = 10,         # minimum total weight needed in each node
    cv.folds = 5,                # do 5-fold cross-validation
    keep.data=TRUE,              # keep a copy of the dataset with the object
    verbose=TRUE)                # print out progress

# print out the optimal iteration number M
best.iter &lt;- TDboost.perf(TDboost1,method="test")
print(best.iter)

# check performance using 5-fold cross-validation
best.iter &lt;- TDboost.perf(TDboost1,method="cv")
print(best.iter)

# plot the performance
# plot variable influence
summary(TDboost1,n.trees=1)         # based on the first tree
summary(TDboost1,n.trees=best.iter) # based on the estimated best number of trees

# making prediction on data2
f.predict &lt;- predict.TDboost(TDboost1,data2,best.iter)

# least squares error
print(sum((data2$Y-f.predict)^2))

# create marginal plots
# plot variable X1 after "best" iterations
plot.TDboost(TDboost1,1,best.iter)
# contour plot of variables 1 and 3 after "best" iterations
plot.TDboost(TDboost1,c(1,3),best.iter)

# do another 20 iterations
TDboost2 &lt;- TDboost.more(TDboost1,20,
                 verbose=FALSE) # stop printing detailed progress

# fit a gamma model (when alpha = 2.0)
data2 &lt;- data1[data1$Y!=0,]
TDboost3 &lt;- TDboost(Y~X1+X2+X3+X4+X5+X6,         # formula
    data=data2,                   # dataset
    distribution=list(name="EDM",alpha=2.0),
    n.trees=3000, 				 # number of trees
    train.fraction = 0.5,        # fraction of data for training,
    verbose=TRUE)                # print out progress
best.iter2 &lt;- TDboost.perf(TDboost3,method="test")

</code></pre>

<hr>
<h2 id='TDboost.object'>TDboost Tweedie Regression Model Object</h2><span id='topic+TDboost.object'></span>

<h3>Description</h3>

<p>These are objects representing fitted <code>TDboost</code>s.</p>


<h3>Value</h3>

<table>
<tr><td><code>initF</code></td>
<td>
<p>the &quot;intercept&quot; term, the initial predicted value to which trees
make adjustments</p>
</td></tr>
<tr><td><code>fit</code></td>
<td>
<p>a vector containing the fitted values on the scale of regression
function</p>
</td></tr>
<tr><td><code>train.error</code></td>
<td>
<p>a vector of length equal to the number of fitted trees
containing the value of the loss function for each boosting iteration
evaluated on the training data</p>
</td></tr>
<tr><td><code>valid.error</code></td>
<td>
<p>a vector of length equal to the number of fitted trees
containing the value of the loss function for each boosting iteration
evaluated on the validation data</p>
</td></tr>
<tr><td><code>cv.error</code></td>
<td>
<p>if <code>cv.folds</code>&lt;2 this component is NULL. Otherwise, this 
component is a vector of length equal to the number of fitted trees
containing a cross-validated estimate of the loss function for each boosting 
iteration</p>
</td></tr>
<tr><td><code>oobag.improve</code></td>
<td>
<p>a vector of length equal to the number of fitted trees
containing an out-of-bag estimate of the marginal reduction in the expected
value of the loss function. The out-of-bag estimate uses only the training
data and is useful for estimating the optimal number of boosting iterations.
See <code><a href="#topic+TDboost.perf">TDboost.perf</a></code></p>
</td></tr>
<tr><td><code>trees</code></td>
<td>
<p>a list containing the tree structures.</p>
</td></tr>
<tr><td><code>c.splits</code></td>
<td>
<p>a list of all the categorical splits in the collection of
trees. If the <code>trees[[i]]</code> component of a <code>TDboost</code> object describes a
categorical split then the splitting value will refer to a component of
<code>c.splits</code>. That component of <code>c.splits</code> will be a vector of length
equal to the number of levels in the categorical split variable. -1 indicates
left, +1 indicates right, and 0 indicates that the level was not present in the
training data</p>
</td></tr>
</table>


<h3>Structure</h3>

<p>The following components must be included in a legitimate <code>TDboost</code> object.</p>


<h3>Author(s)</h3>

<p>Yi Yang <a href="mailto:yi.yang6@mcgill.ca">yi.yang6@mcgill.ca</a>, Wei Qian <a href="mailto:wxqsma@rit.edu">wxqsma@rit.edu</a> and Hui Zou <a href="mailto:hzou@stat.umn.edu">hzou@stat.umn.edu</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+TDboost">TDboost</a></code>
</p>

<hr>
<h2 id='TDboost.perf'>TDboost performance</h2><span id='topic+TDboost.perf'></span>

<h3>Description</h3>

<p>Estimates the optimal number of boosting iterations for a <code>TDboost</code> object and
optionally plots various performance measures
</p>


<h3>Usage</h3>

<pre><code class='language-R'>TDboost.perf(object, 
         plot.it = TRUE, 
         oobag.curve = FALSE, 
         overlay = TRUE, 
         method)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="TDboost.perf_+3A_object">object</code></td>
<td>
<p>a <code><a href="#topic+TDboost.object">TDboost.object</a></code> created from an initial call to 
<code><a href="#topic+TDboost">TDboost</a></code>.</p>
</td></tr>
<tr><td><code id="TDboost.perf_+3A_plot.it">plot.it</code></td>
<td>
<p>an indicator of whether or not to plot the performance measures.
Setting <code>plot.it=TRUE</code> creates two plots. The first plot plots 
<code>object$train.error</code> (in black) and <code>object$valid.error</code> (in red) 
versus the iteration number. The scale of the error measurement, shown on the 
left vertical axis, depends on the <code>distribution</code> argument used in the 
initial call to <code><a href="#topic+TDboost">TDboost</a></code>.</p>
</td></tr>
<tr><td><code id="TDboost.perf_+3A_oobag.curve">oobag.curve</code></td>
<td>
<p>indicates whether to plot the out-of-bag performance measures
in a second plot.</p>
</td></tr>
<tr><td><code id="TDboost.perf_+3A_overlay">overlay</code></td>
<td>
<p>if TRUE and oobag.curve=TRUE then a right y-axis is added to the 
training and test error plot and the estimated cumulative improvement in the loss 
function is plotted versus the iteration number.</p>
</td></tr>
<tr><td><code id="TDboost.perf_+3A_method">method</code></td>
<td>
<p>indicate the method used to estimate the optimal number
of boosting iterations. <code>method="OOB"</code> computes the out-of-bag
estimate and <code>method="test"</code> uses the test (or validation) dataset 
to compute an out-of-sample estimate. <code>method="cv"</code> extracts the 
optimal number of iterations using cross-validation if <code>TDboost</code> was called
with <code>cv.folds</code>&gt;1</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>TDboost.perf</code> returns the estimated optimal number of iterations. The method 
of computation depends on the <code>method</code> argument.</p>


<h3>Author(s)</h3>

<p>Yi Yang <a href="mailto:yi.yang6@mcgill.ca">yi.yang6@mcgill.ca</a>, Wei Qian <a href="mailto:wxqsma@rit.edu">wxqsma@rit.edu</a> and Hui Zou <a href="mailto:hzou@stat.umn.edu">hzou@stat.umn.edu</a></p>


<h3>References</h3>

<p>Yang, Y., Qian, W. and Zou, H. (2013), &ldquo;A Boosted Tweedie Compound Poisson Model for Insurance Premium&rdquo; Preprint.
</p>
<p>G. Ridgeway (1999). &ldquo;The state of boosting,&rdquo; <em>Computing Science and
Statistics</em> 31:172-181.
</p>
<p>G. Ridgeway (2003). &quot;A note on out-of-bag estimation for estimating the optimal
number of boosting iterations,&quot; Working paper.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+TDboost">TDboost</a></code>, <code><a href="#topic+TDboost.object">TDboost.object</a></code></p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
