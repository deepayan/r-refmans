<!DOCTYPE html><html lang="en"><head><title>Help for package QLearning</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {QLearning}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#qlearn'><p> qlearn</p></a></li>
<li><a href='#qlearningaction'><p> qlearningaction</p></a></li>
<li><a href='#qlearningupdate'><p> qlearningupdate</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Reinforcement Learning using the Q Learning Algorithm</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.1</td>
</tr>
<tr>
<td>Author:</td>
<td>Liam Bressler</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Liam Bressler &lt;liam.bressler@yale.edu&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Implements Q-Learning, a model-free form of reinforcement
        learning, described in work by Strehl, Li, Wiewiora, Langford &amp;
        Littman (2006) &lt;<a href="https://doi.org/10.1145%2F1143844.1143955">doi:10.1145/1143844.1143955</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GNU General Public License]</td>
</tr>
<tr>
<td>LazyData:</td>
<td>TRUE</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>6.0.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2017-09-20 21:39:02 UTC; liambressler</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2017-09-21 07:59:42 UTC</td>
</tr>
</table>
<hr>
<h2 id='qlearn'> qlearn
</h2><span id='topic+qlearn'></span>

<h3>Description</h3>

<p>Input a <em>game</em> that has variables <em>statevars</em> (which the player can keep track of). The player can perform any of <em>possibleactions</em>. The output matrix will give the expected value of each action (column) in each state (row).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>qlearn(game, statevars, possibleactions, playername="P1",
  numiter=1000, prevstrategy=NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="qlearn_+3A_game">game</code></td>
<td>
<p>Name of the game to be played/learned.</p>
</td></tr>
<tr><td><code id="qlearn_+3A_statevars">statevars</code></td>
<td>
<p>A vector of the states to be monitored inside <em>game</em>. These are the conditions under which we the player has to make his decision.</p>
</td></tr>
<tr><td><code id="qlearn_+3A_possibleactions">possibleactions</code></td>
<td>
<p>A vector of the names of the possible actions inside <em>game</em>. This should be a list of every possible action that can be taken, regardless of state.</p>
</td></tr>
<tr><td><code id="qlearn_+3A_playername">playername</code></td>
<td>
<p>The name of the variable that holds the name for the player's action inside <em>game</em>. See <strong>Details</strong>.</p>
</td></tr>
<tr><td><code id="qlearn_+3A_numiter">numiter</code></td>
<td>
<p>Number of iterations of <em>game</em>. Defaults to 50.</p>
</td></tr>
<tr><td><code id="qlearn_+3A_prevstrategy">prevstrategy</code></td>
<td>
<p>Reward matrix returned by a previous <em>qlearn</em> function; serves as a starting point. Defaults to a blank reward matrix.</p>
</td></tr>
<tr><td><code id="qlearn_+3A_...">...</code></td>
<td>
<p>Additional arguments to be passed to <em>game</em>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>At some point in game, there must be a line of the format
</p>
<p><code>playername &lt;- 'Choose'</code>
</p>
<p>where playername is substituted with the paramater &quot;playername&quot;. This line should be at the point where the user wants to have the player choose an action. Since playername defaults to &quot;P1&quot;, it sufficient to put the line:
</p>
<p><code>P1 &lt;- 'Choose'</code>
</p>
<p>somewhere in the function.
</p>


<h3>Value</h3>

<p>A matrix describing the expected reward values of performing a certain action (columns) in a certain state (rows).
</p>


<h3>Note</h3>

<p>Contact at liam.bressler@yale.edu
</p>


<h3>Author(s)</h3>

<p>Liam Bressler
</p>


<h3>References</h3>

<p>http://labressler.github.io/analytics
</p>


<h3>Examples</h3>

<pre><code class='language-R'>cardgame &lt;- function()
{
  playercards &lt;- sample(1:8,4) #distribute the cards, we're player one
  ourcard &lt;- playercards[1] #our card
  playertotals &lt;- rep(-1,4) #including the antes
  playersinpot &lt;- vector()
  for (player in 2:4) #other 3 players go first
  {
    if (playercards[player]&gt;=2)
    {
      playertotals[player] &lt;- (-3)
      playersinpot &lt;- append(playersinpot,player)
    }
  }
  #the next line is where we want to choose our action
  player1 &lt;- 'Choose'
  if (player1=="Call")
  {
    playertotals[1] &lt;- (-3)
    playersinpot &lt;- append(playersinpot,1)
  }
  potsize &lt;- -1*(sum(playertotals)) #the amount in the pot is how much the players put in
  playercards[!(1:4 %in% playersinpot)] &lt;- 0 #get rid of everyone who folded
  winner &lt;- which.max(playercards) #winner is the person with the highest card who didn't fold
  playertotals[winner] &lt;- playertotals[winner]+potsize
  return(playertotals[1]) #return how much we won
}

strat &lt;- qlearn(game="cardgame",statevars="ourcard",possibleactions=c("Call","Fold"),
  playername="player1",numiter=25000) #make sure each function and variable name is a string

strat



</code></pre>

<hr>
<h2 id='qlearningaction'> qlearningaction
</h2><span id='topic+qlearningaction'></span>

<h3>Description</h3>

<p>This repository implements <a href="http://artint.info/html/ArtInt_265.html">Q-Learning</a>, a model-free form of reinforcement learning in R.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>qlearningaction(q, currentstate, exploration=.5)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="qlearningaction_+3A_q">q</code></td>
<td>
<p>Input state/action matrix.</p>
</td></tr>
<tr><td><code id="qlearningaction_+3A_currentstate">currentstate</code></td>
<td>
<p>Current state of the game. Does not have to match any of the state for <em>q</em>.</p>
</td></tr>
<tr><td><code id="qlearningaction_+3A_exploration">exploration</code></td>
<td>
<p>The probability of choosing a random state, rather than the one with the highest EV. Default 0.5.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For internal use for <em>qlearn</em>.
</p>


<h3>Value</h3>

<p>An action to take, taken from the possible actions of <em>q</em>.
</p>


<h3>Note</h3>

<p>Contact at liam.bressler@yale.edu
</p>


<h3>Author(s)</h3>

<p>Liam Bressler
</p>


<h3>References</h3>

<p>http://labressler.github.io/analytics
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
cardgame &lt;- function()
{
  playercards &lt;- sample(1:8,4) #distribute the cards, we're player one
  ourcard &lt;- playercards[1] #our card
  playertotals &lt;- rep(-1,4) #including the antes
  playersinpot &lt;- vector()
  for (player in 2:4) #other 3 players go first
  {
    if (playercards[player]&gt;=2)
    {
      playertotals[player] &lt;- (-3)
      playersinpot &lt;- append(playersinpot,player)
    }
  }
  #the next line is where we want to choose our action
  player1 &lt;- 'Choose'
  if (player1=="Call")
  {
    playertotals[1] &lt;- (-3)
    playersinpot &lt;- append(playersinpot,1)
  }
  potsize &lt;- -1*(sum(playertotals)) #the amount in the pot is how much the players put in
  playercards[!(1:4 %in% playersinpot)] &lt;- 0 #get rid of everyone who folded
  winner &lt;- which.max(playercards) #winner is the person with the highest card who didn't fold
  playertotals[winner] &lt;- playertotals[winner]+potsize
  return(playertotals[1]) #return how much we won
}

strat &lt;- qlearn(game="cardgame",statevars="ourcard",possibleactions=c("Call","Fold"),
  playername="player1",numiter=25000) #make sure each function and variable name is a string

qlearningaction(strat,3,exploration=.75)
#Pick an action to perform when we have the 3 card, with high exploration
</code></pre>

<hr>
<h2 id='qlearningupdate'> qlearningupdate
</h2><span id='topic+qlearningupdate'></span>

<h3>Description</h3>

<p>This repository implements <a href="http://artint.info/html/ArtInt_265.html">Q-Learning</a>, a model-free form of reinforcement learning in R.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>qlearningupdate(q, currentstate, currentaction, currentreward, nextstate=NULL,
  rewardcount=.5, gamma=.25)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="qlearningupdate_+3A_q">q</code></td>
<td>
<p>Input state/action matrix.</p>
</td></tr>
<tr><td><code id="qlearningupdate_+3A_currentstate">currentstate</code></td>
<td>
<p>Current state of the game. Does not have to match any of the state for <em>q</em>.</p>
</td></tr>
<tr><td><code id="qlearningupdate_+3A_currentaction">currentaction</code></td>
<td>
<p>Action to take.</p>
</td></tr>
<tr><td><code id="qlearningupdate_+3A_currentreward">currentreward</code></td>
<td>
<p>Reward for <em>currentaction</em> in current iteration.</p>
</td></tr>
<tr><td><code id="qlearningupdate_+3A_nextstate">nextstate</code></td>
<td>
<p>State that the game is in after taking <em>currentaction</em>.</p>
</td></tr>
<tr><td><code id="qlearningupdate_+3A_rewardcount">rewardcount</code></td>
<td>
<p>Regularization constant for reward.</p>
</td></tr>
<tr><td><code id="qlearningupdate_+3A_gamma">gamma</code></td>
<td>
<p>Learning rate constant for Q-Learning.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For internal use for <em>qlearn</em>.
</p>


<h3>Value</h3>

<p>An updated state/action matrix.
</p>


<h3>Note</h3>

<p>Contact at liam.bressler@yale.edu
</p>


<h3>Author(s)</h3>

<p>Liam Bressler
</p>


<h3>References</h3>

<p>http://labressler.github.io/analytics
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
cardgame &lt;- function()
{
  playercards &lt;- sample(1:8,4) #distribute the cards, we're player one
  ourcard &lt;- playercards[1] #our card
  playertotals &lt;- rep(-1,4) #including the antes
  playersinpot &lt;- vector()
  for (player in 2:4) #other 3 players go first
  {
    if (playercards[player]&gt;=2)
    {
      playertotals[player] &lt;- (-3)
      playersinpot &lt;- append(playersinpot,player)
    }
  }
  #the next line is where we want to choose our action
  player1 &lt;- 'Choose'
  if (player1=="Call")
  {
    playertotals[1] &lt;- (-3)
    playersinpot &lt;- append(playersinpot,1)
  }
  potsize &lt;- -1*(sum(playertotals)) #the amount in the pot is how much the players put in
  playercards[!(1:4 %in% playersinpot)] &lt;- 0 #get rid of everyone who folded
  winner &lt;- which.max(playercards) #winner is the person with the highest card who didn't fold
  playertotals[winner] &lt;- playertotals[winner]+potsize
  return(playertotals[1]) #return how much we won
}

strat &lt;- qlearn(game="cardgame",statevars="ourcard",possibleactions=c("Call","Fold"),
  playername="player1",numiter=25000) #make sure each function and variable name is a string

strat &lt;- qlearningupdate(strat,currentstate=7,currentaction="Call",currentreward=5)
#Update the matrix after an example when we call with the 7 card as our state, winning 5 chips
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
