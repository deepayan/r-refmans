<!DOCTYPE html><html><head><title>Help for package TensorClustering</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {TensorClustering}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#DEEM'><p>Doubly-enhanced EM algorithm</p></a></li>
<li><a href='#TEMM'><p>Fit the Tensor Envelope Mixture Model (TEMM)</p></a></li>
<li><a href='#TGMM'><p>Fit the Tensor Gaussian Mixture Model (TGMM)</p></a></li>
<li><a href='#tune_K'><p>Select the number of clusters <code>K</code> in DEEM</p></a></li>
<li><a href='#tune_lamb'><p>Parameter tuning in enhanced E-step in DEEM</p></a></li>
<li><a href='#tune_u_joint'><p>Tuning envelope dimension jointly by BIC in TEMM.</p></a></li>
<li><a href='#tune_u_sep'><p>Tuning envelope dimension separately by BIC in TEMM.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Model-Based Tensor Clustering</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.2</td>
</tr>
<tr>
<td>Description:</td>
<td>Performs model-based tensor clustering methods including Tensor Gaussian Mixture Model (TGMM), Tensor Envelope Mixture Model (TEMM) by Deng and Zhang (2021) &lt;<a href="https://doi.org/10.1111%2Fbiom.13486">doi:10.1111/biom.13486</a>&gt;, Doubly-Enhanced EM (DEEM) algorithm by Mai, Zhang, Pan and Deng (2021) &lt;<a href="https://doi.org/10.1080%2F01621459.2021.1904959">doi:10.1080/01621459.2021.1904959</a>&gt;. </td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>false</td>
</tr>
<tr>
<td>Imports:</td>
<td>MASS, tensr, rTensor, TRES, abind, combinat, pracma, stats,
mvtnorm, Rcpp</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.1</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2021-06-26 04:52:38 UTC; azuryee</td>
</tr>
<tr>
<td>Author:</td>
<td>Kai Deng [aut, cre],
  Yuqing Pan [aut],
  Xin Zhang [aut],
  Qing Mai [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Kai Deng &lt;kd18h@stat.fsu.edu&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2021-06-26 05:40:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='DEEM'>Doubly-enhanced EM algorithm</h2><span id='topic+DEEM'></span>

<h3>Description</h3>

<p>Doubly-enhanced EM algorithm for tensor clustering</p>


<h3>Usage</h3>

<pre><code class='language-R'>DEEM(X, nclass, niter = 100, lambda = NULL, dfmax = n, pmax = nvars, pf = rep(1, nvars),
eps = 1e-04, maxit = 1e+05, sml = 1e-06, verbose = FALSE, ceps = 0.1,
initial = TRUE, vec_x = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="DEEM_+3A_x">X</code></td>
<td>
<p>Input tensor (or matrix) list of length <code class="reqn">n</code>, where <code class="reqn">n</code> is the number of observations. Each element of the list is a tensor or matrix. The order of tensor can be any positive integer not less than 2.</p>
</td></tr>
<tr><td><code id="DEEM_+3A_nclass">nclass</code></td>
<td>
<p>Number of clusters.</p>
</td></tr>
<tr><td><code id="DEEM_+3A_niter">niter</code></td>
<td>
<p>Maximum iteration times for EM algorithm. Default value is 100.</p>
</td></tr>
<tr><td><code id="DEEM_+3A_lambda">lambda</code></td>
<td>
<p>A user-specified <code>lambda</code> value. <code>lambda</code> is the weight of L1 penalty and a smaller <code>lambda</code> allows more variables to be nonzero</p>
</td></tr>
<tr><td><code id="DEEM_+3A_dfmax">dfmax</code></td>
<td>
<p>The maximum number of selected variables in the model. Default is the number of observations <code>n</code>.</p>
</td></tr>
<tr><td><code id="DEEM_+3A_pmax">pmax</code></td>
<td>
<p>The maximum number of potential selected variables during iteration. In middle step, the algorithm can select at most <code>pmax</code> variables and then shrink part of them such that the number of final selected variables is less than <code>dfmax</code>.</p>
</td></tr>
<tr><td><code id="DEEM_+3A_pf">pf</code></td>
<td>
<p>Weight of lasso penalty. Default is a vector of value <code>1</code> and length <code>p</code>, representing L1 penalty of length <code class="reqn">p</code>. Can be modified to use adaptive lasso penalty.</p>
</td></tr>
<tr><td><code id="DEEM_+3A_eps">eps</code></td>
<td>
<p>Convergence threshold for coordinate descent difference between iterations. Default value is <code>1e-04</code>.</p>
</td></tr>
<tr><td><code id="DEEM_+3A_maxit">maxit</code></td>
<td>
<p>Maximum iteration times for coordinate descent for all lambda. Default value is <code>1e+05</code>.</p>
</td></tr>
<tr><td><code id="DEEM_+3A_sml">sml</code></td>
<td>
<p>Threshold for ratio of loss function change after each iteration to old loss function value. Default value is <code>1e-06</code>.</p>
</td></tr>
<tr><td><code id="DEEM_+3A_verbose">verbose</code></td>
<td>
<p>Indicates whether print out lambda during iteration or not. Default value is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="DEEM_+3A_ceps">ceps</code></td>
<td>
<p>Convergence threshold for cluster mean difference between iterations. Default value is <code>1</code>.</p>
</td></tr>
<tr><td><code id="DEEM_+3A_initial">initial</code></td>
<td>
<p>Whether to initialize algorithm with K-means clustering. Default value is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="DEEM_+3A_vec_x">vec_x</code></td>
<td>
<p>Vectorized tensor data. Default value is <code>NULL</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code><a href="#topic+DEEM">DEEM</a></code> function implements the Doubly-Enhanced EM algorithm (DEEM) for tensor clustering. The observations <code class="reqn">\mathbf{X}_i</code> are assumed to be following the tensor normal mixture model (TNMM) with common covariances across different clusters:
</p>
<p style="text-align: center;"><code class="reqn">
\mathbf{X}_i\sim\sum_{k=1}^K\pi_k \mathrm{TN}(\bm{\mu}_k;\bm{\Sigma}_1,\ldots,\bm{\Sigma}_M),\quad i=1,\dots,n,
</code>
</p>

<p>where <code class="reqn">0&lt;\pi_k&lt;1</code> is the prior probability for <code class="reqn">\mathbf{X}</code> to be in the <code class="reqn">k</code>-th cluster such that <code class="reqn">\sum_{k=1}^{K}\pi_k=1</code>, <code class="reqn">\bm{\mu}_k</code> is the cluster mean of the <code class="reqn">k</code>-th cluster and <code class="reqn">\bm{\Sigma}_1,\ldots,\bm{\Sigma}_M)</code> are the common covariances across different clusters. Under the TNMM framework, the optimal clustering rule can be showed as
</p>
<p style="text-align: center;"><code class="reqn">
\widehat{Y}^{opt}=\arg\max_k\{\log\pi_k+\langle\mathbf{X}-(\bm{\mu}_1+\bm{\mu}_k)/2,\mathbf{B}_k\rangle\},
</code>
</p>

<p>where <code class="reqn">\mathbf{B}_k=[\![\bm{\mu}_k-\bm{\mu}_1;\bm{\Sigma}_1^{-1},\ldots,\bm{\Sigma}_M^{-1}]\!]</code>. In the enhanced E-step, <code><a href="#topic+DEEM">DEEM</a></code> imposes sparsity directly on the optimal clustering rule as a flexible alternative to popular low-rank assumptions on tensor coefficients <code class="reqn">\mathbf{B}_k</code> as 
</p>
<p style="text-align: center;"><code class="reqn">
\min_{\mathbf{B}_2,\dots,\mathbf{B}_K}\bigg[\sum_{k=2}^K(\langle\mathbf{B}_k,[\![\mathbf{B}_k,\widehat{\bm{\Sigma}}_1^{(t)},\ldots,\widehat{\bm{\Sigma}}_M^{(t)}]\!]\rangle-2\langle\mathbf{B}_k,\widehat{\bm{\mu}}_k^{(t)}-\widehat{\bm{\mu}}_1^{(t)}\rangle) +\lambda^{(t+1)}\sum_{\mathcal{J}}\sqrt{\sum_{k=2}^Kb_{k,\mathcal{J}}^2}\bigg],
</code>
</p>

<p>where <code class="reqn">\lambda^{(t+1)}</code> is a tuning parameter. In the enhanced M-step, <code><a href="#topic+DEEM">DEEM</a></code> employs a new estimator for the tensor correlation structure, which facilitates both the computation and the theoretical studies.</p>


<h3>Value</h3>

<table>
<tr><td><code>pi</code></td>
<td>
<p>A vector of estimated prior probabilities for clusters.</p>
</td></tr>
<tr><td><code>mu</code></td>
<td>
<p>A list of estimated cluster means.</p>
</td></tr>
<tr><td><code>sigma</code></td>
<td>
<p>A list of estimated covariance matrices.</p>
</td></tr>
<tr><td><code>gamma</code></td>
<td>
<p>A <code>n</code> by <code>nclass</code> matrix of estimated membership weights.</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>A vector of estimated labels.</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>Number of iterations until convergence.</p>
</td></tr>
<tr><td><code>df</code></td>
<td>
<p>Average zero elements in beta over iterations.</p>
</td></tr>
<tr><td><code>beta</code></td>
<td>
<p>A matrix of vectorized <code>B_k</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Kai Deng, Yuqing Pan, Xin Zhang and Qing Mai</p>


<h3>References</h3>

<p>Mai, Q., Zhang, X., Pan, Y. and Deng, K. (2021). A Doubly-Enhanced EM Algorithm for Model-Based Tensor Clustering. <em>Journal of the American Statistical Association</em>.</p>


<h3>See Also</h3>

<p><code><a href="#topic+tune_lamb">tune_lamb</a></code>, <code><a href="#topic+tune_K">tune_K</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>dimen = c(5,5,5)
nvars = prod(dimen)
K = 2
n = 100
sigma = array(list(),3)

sigma[[1]] = sigma[[2]] = sigma[[3]] = diag(5)

B2=array(0,dim=dimen)
B2[1:3,1,1]=2

y = c(rep(1,50),rep(2,50))
M = array(list(),K)
M[[1]] = array(0,dim=dimen)
M[[2]] = B2

vec_x=matrix(rnorm(n*prod(dimen)),ncol=n)
X=array(list(),n)
for (i in 1:n){
  X[[i]] = array(vec_x[,i],dim=dimen)
  X[[i]] = M[[y[i]]] + X[[i]]
}

myfit = DEEM(X, nclass=2, lambda=0.05)
</code></pre>

<hr>
<h2 id='TEMM'>Fit the Tensor Envelope Mixture Model (TEMM)</h2><span id='topic+TEMM'></span>

<h3>Description</h3>

<p>Fit the Tensor Envelope Mixture Model (TEMM)</p>


<h3>Usage</h3>

<pre><code class='language-R'>TEMM(Xn, u, K, initial = "kmeans", iter.max = 500, 
stop = 1e-3, trueY = NULL, print = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="TEMM_+3A_xn">Xn</code></td>
<td>
<p>The tensor for clustering, should be array type, the last dimension is the sample size <code>n</code>.</p>
</td></tr>
<tr><td><code id="TEMM_+3A_u">u</code></td>
<td>
<p>A vector of envelope dimension</p>
</td></tr>
<tr><td><code id="TEMM_+3A_k">K</code></td>
<td>
<p>Number of clusters, greater than or equal to <code>2</code>.</p>
</td></tr>
<tr><td><code id="TEMM_+3A_initial">initial</code></td>
<td>
<p>Initialization meth0d for the regularized EM algorithm. Default value is &quot;kmeans&quot;.</p>
</td></tr>
<tr><td><code id="TEMM_+3A_iter.max">iter.max</code></td>
<td>
<p>Maximum number of iterations. Default value is <code>500</code>.</p>
</td></tr>
<tr><td><code id="TEMM_+3A_stop">stop</code></td>
<td>
<p>Convergence threshold of relative change in cluster means. Default value is <code>1e-3</code>.</p>
</td></tr>
<tr><td><code id="TEMM_+3A_truey">trueY</code></td>
<td>
<p>A vector of true cluster labels of each observation. Default value is NULL.</p>
</td></tr>
<tr><td><code id="TEMM_+3A_print">print</code></td>
<td>
<p>Whether to print information including current iteration number, relative change in cluster means 
and clustering error (<code>%</code>) in each iteration.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code><a href="#topic+TEMM">TEMM</a></code> function fits the Tensor Envelope Mixture Model (TEMM) through a subspace-regularized EM algorithm. For mode <code class="reqn">m</code>, let <code class="reqn">(\bm{\Gamma}_m,\bm{\Gamma}_{0m})\in R^{p_m\times p_m}</code> be an orthogonal matrix where <code class="reqn">\bm{\Gamma}_{m}\in R^{p_{m}\times u_{m}}</code>, <code class="reqn">u_{m}\leq p_{m}</code>, represents the material part. Specifically, the material part <code class="reqn">\mathbf{X}_{\star,m}=\mathbf{X}\times_{m}\bm{\Gamma}_{m}^{T}</code> follows a tensor normal mixture distribution, while the immaterial part <code class="reqn">\mathbf{X}_{\circ,m}=\mathbf{X}\times_{m}\bm{\Gamma}_{0m}^{T}</code> is unimodal, independent of the material part and hence can be eliminated without loss of  clustering information. Dimension reduction is achieved by focusing on the material part <code class="reqn">\mathbf{X}_{\star,m}=\mathbf{X}\times_{m}\bm{\Gamma}_{m}^{T}</code>. Collectively, the joint reduction from each mode is
</p>
<p style="text-align: center;"><code class="reqn">
\mathbf{X}_{\star}=[\![\mathbf{X};\bm{\Gamma}_{1}^{T},\dots,\bm{\Gamma}_{M}^{T}]\!]\sim\sum_{k=1}^{K}\pi_{k}\mathrm{TN}(\bm{\alpha}_{k};\bm{\Omega}_{1},\dots,\bm{\Omega}_{M}),\quad \mathbf{X}_{\star}\perp\!\!\!\perp\mathbf{X}_{\circ,m},
</code>
</p>

<p>where <code class="reqn">\bm{\alpha}_{k}\in R^{u_{1}\times\cdots\times u_{M}}</code> and <code class="reqn">\bm{\Omega}_m\in R^{u_m\times u_m}</code> are the dimension-reduced clustering parameters and <code class="reqn">\mathbf{X}_{\circ,m}</code> does not vary with cluster index <code class="reqn">Y</code>. In the E-step, the membership weights are evaluated as
</p>
<p style="text-align: center;"><code class="reqn">
\widehat{\eta}_{ik}^{(s)}=\frac{\widehat{\pi}_{k}^{(s-1)}f_{k}(\mathbf{X}_i;\widehat{\bm{\theta}}^{(s-1)})}{\sum_{k=1}^{K}\widehat{\pi}_{k}^{(s-1)}f_{k}(\mathbf{X}_i;\widehat{\bm{\theta}}^{(s-1)})},
</code>
</p>

<p>where <code class="reqn">f_k</code> denotes the conditional probability density function of <code class="reqn">\mathbf{X}_i</code> within the <code class="reqn">k</code>-th cluster. In the subspace-regularized M-step, the envelope subspace is iteratively estimated through a Grassmann manifold optimization that minimize the following log-likelihood-based objective function:
</p>
<p style="text-align: center;"><code class="reqn">
G_m^{(s)}(\bm{\Gamma}_m) = \log|\bm{\Gamma}_m^T \mathbf{M}_m^{(s)} \bm{\Gamma}_m|+\log|\bm{\Gamma}_m^T (\mathbf{N}_m^{(s)})^{-1} \bm{\Gamma}_m|,
</code>
</p>

<p>where <code class="reqn">\mathbf{M}_{m}^{(s)}</code> and <code class="reqn">\mathbf{N}_{m}^{(s)}</code> are given by
</p>
<p style="text-align: center;"><code class="reqn">
\mathbf{M}_m^{(s)} = \frac{1}{np_{-m}}\sum_{i=1}^{n} \sum_{k=1}^{K}\widehat{\eta}_{ik}^{(s)} (\bm{\epsilon}_{ik}^{(s)})_{(m)}(\widehat{\bm{\Sigma}}_{-m}^{(s-1)})^{-1}  (\bm{\epsilon}_{ik}^{(s)})_{(m)}^T,
</code>
</p>

<p style="text-align: center;"><code class="reqn">
\mathbf{N}_m^{(s)} = \frac{1}{np_{-m}}\sum_{i=1}^{n} (\mathbf{X}_i)_{(m)}(\widehat{\bm{\Sigma}}_{-m}^{(s-1)})^{-1}(\mathbf{X}_i)_{(m)}^T.
</code>
</p>

<p>The intermediate estimators <code class="reqn">\mathbf{M}_{m}^{(s)}</code> can be viewed the mode-<code class="reqn">m</code> conditional variation estimate of <code class="reqn">\mathbf{X}\mid Y</code> and <code class="reqn">\mathbf{N}_{m}^{(s)}</code> is the mode-<code class="reqn">m</code> marginal variation estimate of <code class="reqn">\mathbf{X}</code>.
</p>


<h3>Value</h3>

<table>
<tr><td><code>id</code></td>
<td>
<p>A vector of estimated labels.</p>
</td></tr>
<tr><td><code>pi</code></td>
<td>
<p>A vector of estimated prior probabilities for clusters.</p>
</td></tr>
<tr><td><code>eta</code></td>
<td>
<p>A <code>n</code> by <code>K</code> matrix of estimated membership weights.</p>
</td></tr>
<tr><td><code>Mu.est</code></td>
<td>
<p>A list of estimated cluster means.</p>
</td></tr>
<tr><td><code>SIG.est</code></td>
<td>
<p>A list of estimated covariance matrices.</p>
</td></tr>
<tr><td><code>Mm</code></td>
<td>
<p>Estimation of <code>Mm</code> defined in paper.</p>
</td></tr>
<tr><td><code>Nm</code></td>
<td>
<p>Estimation of <code>Nm</code> defined in paper.</p>
</td></tr>
<tr><td><code>Gamma.est</code></td>
<td>
<p>A list of estimated envelope basis.</p>
</td></tr>
<tr><td><code>PGamma.est</code></td>
<td>
<p>A list of envelope projection matrices.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Kai Deng, Yuqing Pan, Xin Zhang and Qing Mai</p>


<h3>References</h3>

<p>Deng, K. and Zhang, X. (2021). Tensor Envelope Mixture Model for Simultaneous Clustering and Multiway Dimension Reduction. <em>Biometrics</em>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+TGMM">TGMM</a></code>, <code><a href="#topic+tune_u_sep">tune_u_sep</a></code>, <code><a href="#topic+tune_u_joint">tune_u_joint</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>  A = array(c(rep(1,20),rep(2,20))+rnorm(40),dim=c(2,2,10))
  myfit = TEMM(A,u=c(2,2),K=2)
</code></pre>

<hr>
<h2 id='TGMM'>Fit the Tensor Gaussian Mixture Model (TGMM)</h2><span id='topic+TGMM'></span>

<h3>Description</h3>

<p>Fit the Tensor Gaussian Mixture Model (TGMM)</p>


<h3>Usage</h3>

<pre><code class='language-R'>TGMM(Xn, K, shape = "shared", initial = "kmeans", 
iter.max = 500, stop = 1e-3, trueY = NULL, print = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="TGMM_+3A_xn">Xn</code></td>
<td>
<p>The tensor for clustering, should be array type, the last dimension is the sample size <code>n</code>.</p>
</td></tr>
<tr><td><code id="TGMM_+3A_k">K</code></td>
<td>
<p>Number of clusters, greater than or equal to <code>2</code>.</p>
</td></tr>
<tr><td><code id="TGMM_+3A_shape">shape</code></td>
<td>
<p>&quot;shared&quot; if assume common covariance across mixtures, &quot;distinct&quot; if allow different covariance structures. Default value is &quot;shared&quot;.</p>
</td></tr>
<tr><td><code id="TGMM_+3A_initial">initial</code></td>
<td>
<p>Initialization meth0d for the regularized EM algorithm. Default value is &quot;kmeans&quot;.</p>
</td></tr>
<tr><td><code id="TGMM_+3A_iter.max">iter.max</code></td>
<td>
<p>Maximum number of iterations. Default value is <code>500</code>.</p>
</td></tr>
<tr><td><code id="TGMM_+3A_stop">stop</code></td>
<td>
<p>Convergence threshold of relative change in cluster means. Default value is <code>1e-3</code>.</p>
</td></tr>
<tr><td><code id="TGMM_+3A_truey">trueY</code></td>
<td>
<p>A vector of true cluster labels of each observation. Default value is NULL.</p>
</td></tr>
<tr><td><code id="TGMM_+3A_print">print</code></td>
<td>
<p>Whether to print information including current iteration number, relative change in cluster means 
and clustering error (<code>%</code>) in each iteration.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code><a href="#topic+TGMM">TGMM</a></code> function fits the Tensor Gaussian Mixture Model (TGMM) through the classical EM algorithm. TGMM assumes the following tensor normal mixture distribution of M-way tensor data <code class="reqn">\mathbf{X}</code>:
</p>
<p style="text-align: center;"><code class="reqn">
\mathbf{X}\sim\sum_{k=1}^K\pi_k \mathrm{TN}(\bm{\mu}_k,\mathcal{M}_k),\quad i=1,\dots,n,
</code>
</p>

<p>where <code class="reqn">0&lt;\pi_k&lt;1</code> is the prior probability for <code class="reqn">\mathbf{X}</code> to be in the <code class="reqn">k</code>-th cluster such that <code class="reqn">\sum_{k=1}^{K}\pi_k=1</code>, <code class="reqn">\bm{\mu}_k</code> is the mean of the <code class="reqn">k</code>-th cluster, <code class="reqn">\mathcal{M}_k \equiv \{\bm{\Sigma}_{km}, m=1,\dots,M\}</code> is the set of covariances of the <code class="reqn">k</code>-th cluster. If <code class="reqn">\mathcal{M}_k</code>'s are the same for <code class="reqn">k=1,\dots,K</code>, call <code><a href="#topic+TGMM">TGMM</a></code> with argument <code>shape="shared"</code>.
</p>


<h3>Value</h3>

<table>
<tr><td><code>id</code></td>
<td>
<p>A vector of estimated labels.</p>
</td></tr>
<tr><td><code>pi</code></td>
<td>
<p>A vector of estimated prior probabilities for clusters.</p>
</td></tr>
<tr><td><code>eta</code></td>
<td>
<p>A <code>n</code> by <code>K</code> matrix of estimated membership weights.</p>
</td></tr>
<tr><td><code>Mu.est</code></td>
<td>
<p>A list of estimated cluster means.</p>
</td></tr>
<tr><td><code>SIG.est</code></td>
<td>
<p>A list of estimated covariance matrices.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Kai Deng, Yuqing Pan, Xin Zhang and Qing Mai</p>


<h3>References</h3>

<p>Deng, K. and Zhang, X. (2021). Tensor Envelope Mixture Model for Simultaneous Clustering and Multiway Dimension Reduction. <em>Biometrics</em>.
</p>
<p>Tait, P. A. and McNicholas, P. D. (2019). Clustering higher order data: Finite mixtures of multidimensional arrays. <em>arXiv:1907.08566</em>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+TEMM">TEMM</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>  A = array(c(rep(1,20),rep(2,20))+rnorm(40),dim=c(2,2,10))
  myfit = TGMM(A,K=2,shape="shared")
</code></pre>

<hr>
<h2 id='tune_K'>Select the number of clusters <code>K</code> in DEEM</h2><span id='topic+tune_K'></span>

<h3>Description</h3>

<p>Select the number of clusters <code>K</code> along with tuning parameter <code>lambda</code> through BIC in DEEM.</p>


<h3>Usage</h3>

<pre><code class='language-R'>tune_K(X, seqK, seqlamb, initial = TRUE, vec_x = NULL)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tune_K_+3A_x">X</code></td>
<td>
<p>Input tensor (or matrix) list of length <code class="reqn">n</code>, where <code class="reqn">n</code> is the number of observations. Each element of the list is a tensor or matrix. The order of tensor can be any positive integer not less than 2.</p>
</td></tr>
<tr><td><code id="tune_K_+3A_seqk">seqK</code></td>
<td>
<p>A sequence of user-specified number of clusters.</p>
</td></tr>
<tr><td><code id="tune_K_+3A_seqlamb">seqlamb</code></td>
<td>
<p>A sequence of user-specified <code>lambda</code> values. <code>lambda</code> is the weight of L1 penalty and a smaller <code>lambda</code> allows more variables to be nonzero</p>
</td></tr>
<tr><td><code id="tune_K_+3A_initial">initial</code></td>
<td>
<p>Whether to initialize algorithm with K-means clustering. Default value is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="tune_K_+3A_vec_x">vec_x</code></td>
<td>
<p>Vectorized tensor data. Default value is <code>NULL</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code><a href="#topic+tune_K">tune_K</a></code> function runs <code><a href="#topic+tune_lamb">tune_lamb</a></code> function <code>length(seqK)</code> times to choose the tuning parameter <code class="reqn">\lambda</code> and number of clusters <code class="reqn">K</code> simultaneously. Let <code class="reqn">\widehat{\bm{\theta}}^{\{\lambda,K\}}</code> be the output of <code><a href="#topic+DEEM">DEEM</a></code> with the tuning parameter and number of clusters fixed at <code class="reqn">\lambda</code> and <code class="reqn">K</code> respectively, <code><a href="#topic+tune_K">tune_K</a></code> looks for the values of <code class="reqn">\lambda</code> and <code class="reqn">K</code> that minimizes
</p>
<p style="text-align: center;"><code class="reqn">\mathrm{BIC}(\lambda,K)=-2\sum_{i=1}^n\log(\sum_{k=1}^K\widehat{\pi}^{\{\lambda,K\}}_kf_k(\mathbf{X}_i;\widehat{\bm{\theta}}_k^{\{\lambda,K\}}))+\log(n)\cdot |\widehat{\mathcal{D}}^{\{\lambda,K\}}|,</code>
</p>

<p>where <code class="reqn">\widehat{\mathcal{D}}^{\{\lambda,K\}}=\{(k, {\mathcal{J}}): \widehat b_{k,{\mathcal{J}}}^{\lambda} \neq 0 \}</code> is the set of nonzero elements in <code class="reqn">\widehat{\bm{B}}_2^{\{\lambda,K\}},\ldots,\widehat{\bm{B}}_K^{\{\lambda,K\}}</code>. The <code><a href="#topic+tune_K">tune_K</a></code> function intrinsically selects the initial point and return the optimal estimated labels.
</p>


<h3>Value</h3>

<table>
<tr><td><code>opt_K</code></td>
<td>
<p>Selected number of clusters that leads to optimal BIC.</p>
</td></tr>
<tr><td><code>opt_lamb</code></td>
<td>
<p>Tuned <code>lambda</code> that leads to optimal BIC.</p>
</td></tr>
<tr><td><code>Krank</code></td>
<td>
<p>A selection summary.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Kai Deng, Yuqing Pan, Xin Zhang and Qing Mai</p>


<h3>References</h3>

<p>Mai, Q., Zhang, X., Pan, Y. and Deng, K. (2021). A Doubly-Enhanced EM Algorithm for Model-Based Tensor Clustering. <em>Journal of the American Statistical Association</em>.</p>


<h3>See Also</h3>

<p><code><a href="#topic+DEEM">DEEM</a></code>, <code><a href="#topic+tune_lamb">tune_lamb</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>
dimen = c(5,5,5)
nvars = prod(dimen)
K = 2
n = 100
sigma = array(list(),3)

sigma[[1]] = sigma[[2]] = sigma[[3]] = diag(5)

B2=array(0,dim=dimen)
B2[1:3,1,1]=2

y = c(rep(1,50),rep(2,50))
M = array(list(),K)
M[[1]] = array(0,dim=dimen)
M[[2]] = B2

vec_x=matrix(rnorm(n*prod(dimen)),ncol=n)
X=array(list(),n)
for (i in 1:n){
  X[[i]] = array(vec_x[,i],dim=dimen)
  X[[i]] = M[[y[i]]] + X[[i]]
}

mytune = tune_K(X, seqK=2:4, seqlamb=seq(0.01,0.1,by=0.01))

</code></pre>

<hr>
<h2 id='tune_lamb'>Parameter tuning in enhanced E-step in DEEM</h2><span id='topic+tune_lamb'></span>

<h3>Description</h3>

<p>Perform parameter tuning through BIC in DEEM.</p>


<h3>Usage</h3>

<pre><code class='language-R'>tune_lamb(X, K, seqlamb, initial = TRUE, vec_x = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tune_lamb_+3A_x">X</code></td>
<td>
<p>Input tensor (or matrix) list of length <code class="reqn">n</code>, where <code class="reqn">n</code> is the number of observations. Each element of the list is a tensor or matrix. The order of tensor can be any positive integer not less than 2.</p>
</td></tr>
<tr><td><code id="tune_lamb_+3A_k">K</code></td>
<td>
<p>Number of clusters.</p>
</td></tr>
<tr><td><code id="tune_lamb_+3A_seqlamb">seqlamb</code></td>
<td>
<p>A sequence of user-specified <code>lambda</code> values. <code>lambda</code> is the weight of L1 penalty and a smaller <code>lambda</code> allows more variables to be nonzero</p>
</td></tr>
<tr><td><code id="tune_lamb_+3A_initial">initial</code></td>
<td>
<p>Whether to initialize algorithm with K-means clustering. Default value is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="tune_lamb_+3A_vec_x">vec_x</code></td>
<td>
<p>Vectorized tensor data. Default value is <code>NULL</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code><a href="#topic+tune_lamb">tune_lamb</a></code> function adopts a BIC-type criterion to select the tuning parameter <code class="reqn">\lambda</code> in the enhanced E-step. Let <code class="reqn">\widehat{\bm{\theta}}^{\lambda}</code> be the output of <code><a href="#topic+DEEM">DEEM</a></code> with the tuning parameter fixed at <code class="reqn">\lambda</code>, <code><a href="#topic+tune_lamb">tune_lamb</a></code> looks for the value of <code class="reqn">\lambda</code> that minimizes
</p>
<p style="text-align: center;"><code class="reqn">\mathrm{BIC}(\lambda)=-2\sum_{i=1}^n\log(\sum_{k=1}^K\widehat{\pi}^{\lambda}_kf_k(\mathbf{X}_i;\widehat{\bm{\theta}}_k^{\lambda}))+\log(n)\cdot |\widehat{\mathcal{D}}^{\lambda}|,</code>
</p>

<p>where <code class="reqn">\widehat{\mathcal{D}}^{\lambda}=\{(k, {\mathcal{J}}): \widehat b_{k,{\mathcal{J}}}^{\lambda} \neq 0 \}</code> is the set of nonzero elements in <code class="reqn">\widehat{\bm{B}}_2^{\lambda},\ldots,\widehat{\bm{B}}_K^{\lambda}</code>. The <code><a href="#topic+tune_lamb">tune_lamb</a></code> function intrinsically selects the initial point and return the optimal estimated labels.
</p>


<h3>Value</h3>

<table>
<tr><td><code>opt_lamb</code></td>
<td>
<p>Tuned <code>lambda</code> that leads to optimal BIC.</p>
</td></tr>
<tr><td><code>opt_bic</code></td>
<td>
<p>BIC value.</p>
</td></tr>
<tr><td><code>opt_y</code></td>
<td>
<p>Estimated labels fitted by DEEM with tuned <code>lambda</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Kai Deng, Yuqing Pan, Xin Zhang and Qing Mai</p>


<h3>References</h3>

<p>Mai, Q., Zhang, X., Pan, Y. and Deng, K. (2021). A Doubly-Enhanced EM Algorithm for Model-Based Tensor Clustering. <em>Journal of the American Statistical Association</em>.</p>


<h3>See Also</h3>

<p><code><a href="#topic+DEEM">DEEM</a></code>, <code><a href="#topic+tune_K">tune_K</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>
dimen = c(5,5,5)
nvars = prod(dimen)
K = 2
n = 100
sigma = array(list(),3)

sigma[[1]] = sigma[[2]] = sigma[[3]] = diag(5)

B2=array(0,dim=dimen)
B2[1:3,1,1]=2

y = c(rep(1,50),rep(2,50))
M = array(list(),K)
M[[1]] = array(0,dim=dimen)
M[[2]] = B2

vec_x=matrix(rnorm(n*prod(dimen)),ncol=n)
X=array(list(),n)
for (i in 1:n){
  X[[i]] = array(vec_x[,i],dim=dimen)
  X[[i]] = M[[y[i]]] + X[[i]]
}

mytune = tune_lamb(X, K=2, seqlamb=seq(0.01,0.1,by=0.01))

</code></pre>

<hr>
<h2 id='tune_u_joint'>Tuning envelope dimension jointly by BIC in TEMM.</h2><span id='topic+tune_u_joint'></span>

<h3>Description</h3>

<p>Tuning envelope dimension jointly by BIC in TEMM.</p>


<h3>Usage</h3>

<pre><code class='language-R'>tune_u_joint(u_candi, K, X, iter.max = 500, stop = 0.001, trueY = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tune_u_joint_+3A_u_candi">u_candi</code></td>
<td>
<p>A list of length <code>M</code> containing candidate envelope dimension for each mode.</p>
</td></tr>
<tr><td><code id="tune_u_joint_+3A_k">K</code></td>
<td>
<p>Number of clusters, greater than or equal to <code>2</code>.</p>
</td></tr>
<tr><td><code id="tune_u_joint_+3A_x">X</code></td>
<td>
<p>The tensor for clustering, should be array type, the last dimension is the sample size <code>n</code>.</p>
</td></tr>
<tr><td><code id="tune_u_joint_+3A_iter.max">iter.max</code></td>
<td>
<p>Maximum number of iterations. Default value is <code>500</code>.</p>
</td></tr>
<tr><td><code id="tune_u_joint_+3A_stop">stop</code></td>
<td>
<p>Convergence threshold of relative change in cluster means. Default value is <code>1e-3</code>.</p>
</td></tr>
<tr><td><code id="tune_u_joint_+3A_truey">trueY</code></td>
<td>
<p>A vector of true cluster labels of each observation. Default value is NULL.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code><a href="#topic+tune_u_joint">tune_u_joint</a></code> function searches over all the combinations of <code class="reqn">u\equiv(u_1,\dots,u_M)</code> in the neighborhood of <code class="reqn">\widetilde{u}</code>, <code class="reqn">\mathcal{N}(\widetilde u)=\{u:\ \max(1,\widetilde u_m-2) \leq u_m \leq \min(\widetilde u_m+2,p_m),\ m=1,\dots,M\}</code>, that minimizes	
</p>
<p style="text-align: center;"><code class="reqn">\mathrm{BIC}(u) = -2\sum_{i=1}^{n}\log(\sum_{k=1}^{K}\widehat{\pi}_k^u f_k(\mathbf{X}_i;\widehat{\bm{\theta}}^u)) + \log(n)\cdot K_u.</code>
</p>

<p>In the above BIC, <code class="reqn">K_u=(K-1)\prod_{m=1}^M u_m + \sum_{m=1}^{M}p_m(p_m+1)/2</code> is the total number of parameters in TEMM, <code class="reqn">\widehat{\pi}_k^u</code> and <code class="reqn">\widehat{\bm{\theta}}^{u}</code> are the estimated parameters with envelope dimension fixed at <code class="reqn">u</code>. The <code><a href="#topic+tune_u_joint">tune_u_joint</a></code> function intrinsically selects the initial point and return the optimal estimated labels.
</p>


<h3>Value</h3>

<table>
<tr><td><code>opt.u</code></td>
<td>
<p>Optimal envelope dimension selected.</p>
</td></tr>
<tr><td><code>opt.id</code></td>
<td>
<p>Estimated labels fitted by TEMM with the optimal envelope dimension.</p>
</td></tr>
<tr><td><code>opt.Mu</code></td>
<td>
<p>Estimated cluster means fitted by TEMM with the optimal envelope dimension.</p>
</td></tr>
<tr><td><code>bic</code></td>
<td>
<p>BIC value.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Kai Deng, Yuqing Pan, Xin Zhang and Qing Mai</p>


<h3>References</h3>

<p>Deng, K. and Zhang, X. (2021). Tensor Envelope Mixture Model for Simultaneous Clustering and Multiway Dimension Reduction. <em>Biometrics</em>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+TEMM">TEMM</a></code>, <code><a href="#topic+tune_u_sep">tune_u_sep</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>  A = array(c(rep(1,20),rep(2,20))+rnorm(40),dim=c(2,2,10))
  mytune = tune_u_joint(u_candi=list(1:2,1:2),K=2,A)
</code></pre>

<hr>
<h2 id='tune_u_sep'>Tuning envelope dimension separately by BIC in TEMM.</h2><span id='topic+tune_u_sep'></span>

<h3>Description</h3>

<p>Tuning envelope dimension separately by BIC in TEMM.</p>


<h3>Usage</h3>

<pre><code class='language-R'>tune_u_sep(m, u_candi, K, X, C = 1, oneD = TRUE, 
iter.max = 500, stop = 0.001, trueY = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tune_u_sep_+3A_m">m</code></td>
<td>
<p>The tensor mode to be tuned, can take value in <code>1,...,M</code>.</p>
</td></tr>
<tr><td><code id="tune_u_sep_+3A_u_candi">u_candi</code></td>
<td>
<p>A vector of candidate envelope dimension.</p>
</td></tr>
<tr><td><code id="tune_u_sep_+3A_k">K</code></td>
<td>
<p>Number of clusters, greater than or equal to <code>2</code>.</p>
</td></tr>
<tr><td><code id="tune_u_sep_+3A_x">X</code></td>
<td>
<p>The tensor for clustering, should be array type, the last dimension is the sample size <code>n</code>.</p>
</td></tr>
<tr><td><code id="tune_u_sep_+3A_c">C</code></td>
<td>
<p>Constant in separate BIC criterion. Default value is <code>1</code>.</p>
</td></tr>
<tr><td><code id="tune_u_sep_+3A_oned">oneD</code></td>
<td>
<p>Whether to apply 1D-BIC tuning. Default value is TRUE.</p>
</td></tr>
<tr><td><code id="tune_u_sep_+3A_iter.max">iter.max</code></td>
<td>
<p>Maximum number of iterations. Default value is <code>500</code>.</p>
</td></tr>
<tr><td><code id="tune_u_sep_+3A_stop">stop</code></td>
<td>
<p>Convergence threshold of relative change in cluster means. Default value is <code>1e-3</code>.</p>
</td></tr>
<tr><td><code id="tune_u_sep_+3A_truey">trueY</code></td>
<td>
<p>A vector of true cluster labels of each observation. Default value is NULL.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For tensor mode <code class="reqn">m=1,\dots,M</code>, the <code><a href="#topic+tune_u_sep">tune_u_sep</a></code> function selects the envelope dimension <code class="reqn">\widetilde{u}_m</code> by minimizing the following BIC-type criterion over the set <code class="reqn">\{0,1,\dots,p_m\}</code>,
</p>
<p style="text-align: center;"><code class="reqn">\mathrm{BIC}_m(u_m) = \log|\bm{\Gamma}_m^T \widehat{\mathbf{M}}_m \bm{\Gamma}_m|+\log|\bm{\Gamma}_{m}^T \widehat{\mathbf{N}}_m^{-1} \bm{\Gamma}_{m}| + C \cdot u_m \log(n)/n.</code>
</p>

<p>This separate selection over each mode <code class="reqn">m</code> is less sensitive to the complex interrelationships of each mode of the tensor. The default constant <code class="reqn">C</code> is set as <code class="reqn">1</code> as suggested by Zhang and Mai (2018).
</p>


<h3>Value</h3>

<table>
<tr><td><code>opt.u</code></td>
<td>
<p>Optimal envelope dimension selected.</p>
</td></tr>
<tr><td><code>bic</code></td>
<td>
<p>BIC value.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Kai Deng, Yuqing Pan, Xin Zhang and Qing Mai</p>


<h3>References</h3>

<p>Deng, K. and Zhang, X. (2021). Tensor Envelope Mixture Model for Simultaneous Clustering and Multiway Dimension Reduction. <em>Biometrics</em>.
</p>
<p>Zhang, X. and Mai, Q. (2018). Model-free envelope dimension selection. <em>Electronic Journal of Statistics</em> 12, 2193-2216.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+TEMM">TEMM</a></code>, <code><a href="#topic+tune_u_joint">tune_u_joint</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>  A = array(c(rep(1,20),rep(2,20))+rnorm(40),dim=c(2,2,10))
  mytune = tune_u_sep(1,1:2,K=2,A)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
