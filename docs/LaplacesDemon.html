<!DOCTYPE html><html><head><title>Help for package LaplacesDemon</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {LaplacesDemon}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#LaplacesDemon-package'>
<p>Complete Environment for Bayesian Inference</p></a></li>
<li><a href='#ABB'><p>Approximate Bayesian Bootstrap</p></a></li>
<li><a href='#AcceptanceRate'><p>Acceptance Rate</p></a></li>
<li><a href='#as.covar'><p>Proposal Covariance</p></a></li>
<li><a href='#as.initial.values'><p>Initial Values</p></a></li>
<li><a href='#as.parm.names'><p>Parameter Names</p></a></li>
<li><a href='#as.ppc'><p>As Posterior Predictive Check</p></a></li>
<li><a href='#BayesFactor'><p>Bayes Factor</p></a></li>
<li><a href='#BayesianBootstrap'><p>The Bayesian Bootstrap</p></a></li>
<li><a href='#BayesTheorem'><p>Bayes' Theorem</p></a></li>
<li><a href='#BigData'><p>Big Data</p></a></li>
<li><a href='#Blocks'><p>Blocks</p></a></li>
<li><a href='#BMK.Diagnostic'><p>BMK Convergence Diagnostic</p></a></li>
<li><a href='#burnin'><p>Burn-in</p></a></li>
<li><a href='#caterpillar.plot'><p>Caterpillar Plot</p></a></li>
<li><a href='#CenterScale'><p>Centering and Scaling</p></a></li>
<li><a href='#Combine'><p>Combine Demonoid Objects</p></a></li>
<li><a href='#cond.plot'><p>Conditional Plots</p></a></li>
<li><a href='#Consort'><p>Consort with Laplace's Demon</p></a></li>
<li><a href='#CSF'><p>Cumulative Sample Function</p></a></li>
<li><a href='#data.demonchoice'><p>Demon Choice Data Set</p></a></li>
<li><a href='#data.demonfx'><p>Demon FX Data Set</p></a></li>
<li><a href='#data.demonsessions'><p>Demon Sessions Data Set</p></a></li>
<li><a href='#data.demonsnacks'><p>Demon Snacks Data Set</p></a></li>
<li><a href='#data.demontexas'><p>Demon Space-Time Data Set</p></a></li>
<li><a href='#de.Finetti.Game'><p>de Finetti's Game</p></a></li>
<li><a href='#deburn'><p>De-Burn</p></a></li>
<li><a href='#dist.Asymmetric.Laplace'><p>Asymmetric Laplace Distribution: Univariate</p></a></li>
<li><a href='#dist.Asymmetric.Log.Laplace'><p>Asymmetric Log-Laplace Distribution</p></a></li>
<li><a href='#dist.Asymmetric.Multivariate.Laplace'><p>Asymmetric Multivariate Laplace Distribution</p></a></li>
<li><a href='#dist.Bernoulli'><p>Bernoulli Distribution</p></a></li>
<li><a href='#dist.Categorical'><p>Categorical Distribution</p></a></li>
<li><a href='#dist.ContinuousRelaxation'><p>Continuous Relaxation of a Markov Random Field Distribution</p></a></li>
<li><a href='#dist.Dirichlet'><p>Dirichlet Distribution</p></a></li>
<li><a href='#dist.Generalized.Pareto'><p>Generalized Pareto Distribution</p></a></li>
<li><a href='#dist.Generalized.Poisson'><p>Generalized Poisson Distribution</p></a></li>
<li><a href='#dist.HalfCauchy'><p>Half-Cauchy Distribution</p></a></li>
<li><a href='#dist.HalfNormal'><p>Half-Normal Distribution</p></a></li>
<li><a href='#dist.Halft'><p>Half-t Distribution</p></a></li>
<li><a href='#dist.Horseshoe'><p>Horseshoe Distribution</p></a></li>
<li><a href='#dist.HuangWand'><p>Huang-Wand Distribution</p></a></li>
<li><a href='#dist.Inverse.Beta'><p>Inverse Beta Distribution</p></a></li>
<li><a href='#dist.Inverse.ChiSquare'><p>(Scaled) Inverse Chi-Squared Distribution</p></a></li>
<li><a href='#dist.Inverse.Gamma'><p>Inverse Gamma Distribution</p></a></li>
<li><a href='#dist.Inverse.Gaussian'><p>Inverse Gaussian Distribution</p></a></li>
<li><a href='#dist.Inverse.Matrix.Gamma'><p>Inverse Matrix Gamma Distribution</p></a></li>
<li><a href='#dist.Inverse.Wishart'><p>Inverse Wishart Distribution</p></a></li>
<li><a href='#dist.Inverse.Wishart.Cholesky'><p>Inverse Wishart Distribution: Cholesky Parameterization</p></a></li>
<li><a href='#dist.Laplace'><p>Laplace Distribution: Univariate Symmetric</p></a></li>
<li><a href='#dist.Laplace.Mixture'><p>Mixture of Laplace Distributions</p></a></li>
<li><a href='#dist.Laplace.Precision'><p>Laplace Distribution: Precision Parameterization</p></a></li>
<li><a href='#dist.LASSO'><p>LASSO Distribution</p></a></li>
<li><a href='#dist.Log.Laplace'><p>Log-Laplace Distribution: Univariate Symmetric</p></a></li>
<li><a href='#dist.Log.Normal.Precision'><p>Log-Normal Distribution: Precision Parameterization</p></a></li>
<li><a href='#dist.Matrix.Gamma'><p>Matrix Gamma Distribution</p></a></li>
<li><a href='#dist.Matrix.Normal'><p>Matrix Normal Distribution</p></a></li>
<li><a href='#dist.Multivariate.Cauchy'><p>Multivariate Cauchy Distribution</p></a></li>
<li><a href='#dist.Multivariate.Cauchy.Cholesky'><p>Multivariate Cauchy Distribution: Cholesky Parameterization</p></a></li>
<li><a href='#dist.Multivariate.Cauchy.Precision'><p>Multivariate Cauchy Distribution: Precision Parameterization</p></a></li>
<li><a href='#dist.Multivariate.Cauchy.Precision.Cholesky'><p>Multivariate Cauchy Distribution: Precision-Cholesky Parameterization</p></a></li>
<li><a href='#dist.Multivariate.Laplace'><p>Multivariate Laplace Distribution</p></a></li>
<li><a href='#dist.Multivariate.Laplace.Cholesky'><p>Multivariate Laplace Distribution: Cholesky Parameterization</p></a></li>
<li><a href='#dist.Multivariate.Normal'><p>Multivariate Normal Distribution</p></a></li>
<li><a href='#dist.Multivariate.Normal.Cholesky'><p>Multivariate Normal Distribution: Cholesky Parameterization</p></a></li>
<li><a href='#dist.Multivariate.Normal.Precision'><p>Multivariate Normal Distribution: Precision Parameterization</p></a></li>
<li><a href='#dist.Multivariate.Normal.Precision.Cholesky'><p>Multivariate Normal Distribution: Precision-Cholesky Parameterization</p></a></li>
<li><a href='#dist.Multivariate.Polya'><p>Multivariate Polya Distribution</p></a></li>
<li><a href='#dist.Multivariate.Power.Exponential'><p>Multivariate Power Exponential Distribution</p></a></li>
<li><a href='#dist.Multivariate.Power.Exponential.Cholesky'><p>Multivariate Power Exponential Distribution: Cholesky Parameterization</p></a></li>
<li><a href='#dist.Multivariate.t'><p>Multivariate t Distribution</p></a></li>
<li><a href='#dist.Multivariate.t.Cholesky'><p>Multivariate t Distribution: Cholesky Parameterization</p></a></li>
<li><a href='#dist.Multivariate.t.Precision'><p>Multivariate t Distribution: Precision Parameterization</p></a></li>
<li><a href='#dist.Multivariate.t.Precision.Cholesky'><p>Multivariate t Distribution: Precision-Cholesky Parameterization</p></a></li>
<li><a href='#dist.Normal.Inverse.Wishart'><p>Normal-Inverse-Wishart Distribution</p></a></li>
<li><a href='#dist.Normal.Laplace'><p>Normal-Laplace Distribution: Univariate Asymmetric</p></a></li>
<li><a href='#dist.Normal.Mixture'><p>Mixture of Normal Distributions</p></a></li>
<li><a href='#dist.Normal.Precision'><p>Normal Distribution: Precision Parameterization</p></a></li>
<li><a href='#dist.Normal.Variance'><p>Normal Distribution: Variance Parameterization</p></a></li>
<li><a href='#dist.Normal.Wishart'><p>Normal-Wishart Distribution</p></a></li>
<li><a href='#dist.Pareto'><p>Pareto Distribution</p></a></li>
<li><a href='#dist.Power.Exponential'><p>Power Exponential Distribution: Univariate Symmetric</p></a></li>
<li><a href='#dist.Scaled.Inverse.Wishart'><p>Scaled Inverse Wishart Distribution</p></a></li>
<li><a href='#dist.Skew.Discrete.Laplace'><p>Skew Discrete Laplace Distribution: Univariate</p></a></li>
<li><a href='#dist.Skew.Laplace'><p>Skew-Laplace Distribution: Univariate</p></a></li>
<li><a href='#dist.Stick'><p>Truncated Stick-Breaking Prior Distribution</p></a></li>
<li><a href='#dist.Student.t'><p>Student t Distribution: Univariate</p></a></li>
<li><a href='#dist.Student.t.Precision'><p>Student t Distribution: Precision Parameterization</p></a></li>
<li><a href='#dist.Truncated'><p>Truncated Distributions</p></a></li>
<li><a href='#dist.Wishart'><p>Wishart Distribution</p></a></li>
<li><a href='#dist.Wishart.Cholesky'><p>Wishart Distribution: Cholesky Parameterization</p></a></li>
<li><a href='#dist.YangBerger'><p>Yang-Berger Distribution</p></a></li>
<li><a href='#dist.Zellner'><p>Hyperprior-g Prior and Zellner's g-Prior</p></a></li>
<li><a href='#Elicitation'><p>Prior Elicitation</p></a></li>
<li><a href='#ESS'><p>Effective Sample Size due to Autocorrelation</p></a></li>
<li><a href='#Gelfand.Diagnostic'><p>Gelfand's Convergence Diagnostic</p></a></li>
<li><a href='#Gelman.Diagnostic'><p>Gelman and Rubin's MCMC Convergence Diagnostic</p></a></li>
<li><a href='#Geweke.Diagnostic'><p>Geweke's Convergence Diagnostic</p></a></li>
<li><a href='#GIV'><p>Generate Initial Values</p></a></li>
<li><a href='#Hangartner.Diagnostic'><p>Hangartner's Convergence Diagnostic</p></a></li>
<li><a href='#Heidelberger.Diagnostic'><p>Heidelberger and Welch's MCMC Convergence Diagnostic</p></a></li>
<li><a href='#hpc_server'><p>Server Listening</p></a></li>
<li><a href='#IAT'><p>Integrated Autocorrelation Time</p></a></li>
<li><a href='#Importance'><p>Variable Importance</p></a></li>
<li><a href='#interval'><p>Constrain to Interval</p></a></li>
<li><a href='#is.appeased'><p>Appeased</p></a></li>
<li><a href='#is.bayesian'><p>Logical Check of a Bayesian Model</p></a></li>
<li><a href='#is.class'><p>Logical Check of Classes</p></a></li>
<li><a href='#is.constant'><p>Logical Check of a Constant</p></a></li>
<li><a href='#is.constrained'><p>Logical Check of Constraints</p></a></li>
<li><a href='#is.data'><p>Logical Check of Data</p></a></li>
<li><a href='#is.model'><p>Logical Check of a Model</p></a></li>
<li><a href='#is.proper'><p>Logical Check of Propriety</p></a></li>
<li><a href='#is.stationary'><p>Logical Check of Stationarity</p></a></li>
<li><a href='#IterativeQuadrature'><p>Iterative Quadrature</p></a></li>
<li><a href='#joint.density.plot'><p>Joint Density Plot</p></a></li>
<li><a href='#joint.pr.plot'><p>Joint Probability Region Plot</p></a></li>
<li><a href='#Juxtapose'><p>Juxtapose MCMC Algorithm Inefficiency</p></a></li>
<li><a href='#KLD'><p>Kullback-Leibler Divergence (KLD)</p></a></li>
<li><a href='#KS.Diagnostic'><p>Kolmogorov-Smirnov Convergence Diagnostic</p></a></li>
<li><a href='#LaplaceApproximation'><p>Laplace Approximation</p></a></li>
<li><a href='#LaplacesDemon'><p>Laplace's Demon</p></a></li>
<li><a href='#LaplacesDemon.RAM'><p>LaplacesDemon RAM Estimate</p></a></li>
<li><a href='#Levene.Test'><p>Levene's Test</p></a></li>
<li><a href='#LML'><p>Logarithm of the Marginal Likelihood</p></a></li>
<li><a href='#log-log'><p>The log-log and complementary log-log functions</p></a></li>
<li><a href='#logit'><p>The logit and inverse-logit functions</p></a></li>
<li><a href='#LossMatrix'><p>Loss Matrix</p></a></li>
<li><a href='#LPL.interval'><p>Lowest Posterior Loss Interval</p></a></li>
<li><a href='#Math'><p>Math Utility Functions</p></a></li>
<li><a href='#Matrices'><p>Matrix Utility Functions</p></a></li>
<li><a href='#MCSE'><p>Monte Carlo Standard Error</p></a></li>
<li><a href='#MinnesotaPrior'><p>Minnesota Prior</p></a></li>
<li><a href='#MISS'><p>Multiple Imputation Sequential Sampling</p></a></li>
<li><a href='#Mode'><p>The Mode(s) of a Vector</p></a></li>
<li><a href='#Model.Specification.Time'><p>Model Specification Time</p></a></li>
<li><a href='#p.interval'><p>Probability Interval</p></a></li>
<li><a href='#plot.bmk'><p>Plot Hellinger Distances</p></a></li>
<li><a href='#plot.demonoid'><p>Plot samples from the output of Laplace's Demon</p></a></li>
<li><a href='#plot.demonoid.ppc'><p>Plots of Posterior Predictive Checks</p></a></li>
<li><a href='#plot.importance'><p>Plot Variable Importance</p></a></li>
<li><a href='#plot.iterquad'><p>Plot the output of <code>IterativeQuadrature</code></p></a></li>
<li><a href='#plot.iterquad.ppc'><p>Plots of Posterior Predictive Checks</p></a></li>
<li><a href='#plot.juxtapose'><p>Plot MCMC Juxtaposition</p></a></li>
<li><a href='#plot.laplace'><p>Plot the output of <code>LaplaceApproximation</code></p></a></li>
<li><a href='#plot.laplace.ppc'><p>Plots of Posterior Predictive Checks</p></a></li>
<li><a href='#plot.miss'><p>Plot samples from the output of MISS</p></a></li>
<li><a href='#plot.pmc'><p>Plot samples from the output of PMC</p></a></li>
<li><a href='#plot.pmc.ppc'><p>Plots of Posterior Predictive Checks</p></a></li>
<li><a href='#plot.vb'><p>Plot the output of <code>VariationalBayes</code></p></a></li>
<li><a href='#plot.vb.ppc'><p>Plots of Posterior Predictive Checks</p></a></li>
<li><a href='#plotMatrix'><p>Plot a Numerical Matrix</p></a></li>
<li><a href='#plotSamples'><p>Plot Samples</p></a></li>
<li><a href='#PMC'><p>Population Monte Carlo</p></a></li>
<li><a href='#PMC.RAM'><p>PMC RAM Estimate</p></a></li>
<li><a href='#PosteriorChecks'><p>Posterior Checks</p></a></li>
<li><a href='#Precision'><p>Precision</p></a></li>
<li><a href='#predict.demonoid'><p>Posterior Predictive Checks</p></a></li>
<li><a href='#predict.iterquad'><p>Posterior Predictive Checks</p></a></li>
<li><a href='#predict.laplace'><p>Posterior Predictive Checks</p></a></li>
<li><a href='#predict.pmc'><p>Posterior Predictive Checks</p></a></li>
<li><a href='#predict.vb'><p>Posterior Predictive Checks</p></a></li>
<li><a href='#print.demonoid'><p>Print an object of class <code>demonoid</code> to the screen.</p></a></li>
<li><a href='#print.heidelberger'><p>Print an object of class <code>heidelberger</code> to the screen.</p></a></li>
<li><a href='#print.iterquad'><p>Print an object of class <code>iterquad</code> to the screen.</p></a></li>
<li><a href='#print.laplace'><p>Print an object of class <code>laplace</code> to the screen.</p></a></li>
<li><a href='#print.miss'><p>Print an object of class <code>miss</code> to the screen.</p></a></li>
<li><a href='#print.pmc'><p>Print an object of class <code>pmc</code> to the screen.</p></a></li>
<li><a href='#print.raftery'><p>Print an object of class <code>raftery</code> to the screen.</p></a></li>
<li><a href='#print.vb'><p>Print an object of class <code>vb</code> to the screen.</p></a></li>
<li><a href='#Raftery.Diagnostic'><p>Raftery and Lewis's diagnostic</p></a></li>
<li><a href='#RejectionSampling'><p>Rejection Sampling</p></a></li>
<li><a href='#SensitivityAnalysis'><p>Sensitivity Analysis</p></a></li>
<li><a href='#SIR'><p>Sampling Importance Resampling</p></a></li>
<li><a href='#Stick'><p>Truncated Stick-Breaking</p></a></li>
<li><a href='#summary.demonoid.ppc'><p>Posterior Predictive Check Summary</p></a></li>
<li><a href='#summary.iterquad.ppc'><p>Posterior Predictive Check Summary</p></a></li>
<li><a href='#summary.laplace.ppc'><p>Posterior Predictive Check Summary</p></a></li>
<li><a href='#summary.miss'><p>MISS Summary</p></a></li>
<li><a href='#summary.pmc.ppc'><p>Posterior Predictive Check Summary</p></a></li>
<li><a href='#summary.vb.ppc'><p>Posterior Predictive Check Summary</p></a></li>
<li><a href='#Thin'><p>Thin</p></a></li>
<li><a href='#Validate'><p>Holdout Validation</p></a></li>
<li><a href='#VariationalBayes'><p>Variational Bayes</p></a></li>
<li><a href='#WAIC'><p>Widely Applicable Information Criterion</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Version:</td>
<td>16.1.6</td>
</tr>
<tr>
<td>Title:</td>
<td>Complete Environment for Bayesian Inference</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.0.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>parallel, grDevices, graphics, stats, utils</td>
</tr>
<tr>
<td>Suggests:</td>
<td>KernSmooth</td>
</tr>
<tr>
<td>ByteCompile:</td>
<td>TRUE</td>
</tr>
<tr>
<td>Description:</td>
<td>Provides a complete environment for Bayesian inference using a variety of different samplers (see ?LaplacesDemon for an overview).</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/LaplacesDemonR/LaplacesDemon">https://github.com/LaplacesDemonR/LaplacesDemon</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/LaplacesDemonR/LaplacesDemon/issues">https://github.com/LaplacesDemonR/LaplacesDemon/issues</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2021-07-09 13:02:27 UTC; henrik</td>
</tr>
<tr>
<td>Author:</td>
<td>Byron Hall [aut],
  Martina Hall [aut],
  Statisticat, LLC [aut],
  Eric Brown [ctb],
  Richard Hermanson [ctb],
  Emmanuel Charpentier [ctb],
  Daniel Heck [ctb],
  Stephane Laurent [ctb],
  Quentin F. Gronau [ctb],
  Henrik Singmann [cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Henrik Singmann &lt;singmann+LaplacesDemon@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2021-07-09 14:00:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='LaplacesDemon-package'>
Complete Environment for Bayesian Inference
</h2><span id='topic+LaplacesDemon-package'></span><span id='topic+.colVars'></span><span id='topic+.iqagh'></span><span id='topic+.iqaghsg'></span><span id='topic+.iqcagh'></span><span id='topic+.laaga'></span><span id='topic+.labfgs'></span><span id='topic+.labhhh'></span><span id='topic+.lacg'></span><span id='topic+.ladfp'></span><span id='topic+.lahar'></span><span id='topic+.lahj'></span><span id='topic+.lalbfgs'></span><span id='topic+.lalm'></span><span id='topic+.lanm'></span><span id='topic+.lanr'></span><span id='topic+.lapso'></span><span id='topic+.larprop'></span><span id='topic+.lasgd'></span><span id='topic+.lasoma'></span><span id='topic+.laspg'></span><span id='topic+.lasr1'></span><span id='topic+.latr'></span><span id='topic+.mcmcadmg'></span><span id='topic+.mcmcafss'></span><span id='topic+.mcmcagg'></span><span id='topic+.mcmcahmc'></span><span id='topic+.mcmcaies'></span><span id='topic+.mcmcam'></span><span id='topic+.mcmcamm'></span><span id='topic+.mcmcamm.b'></span><span id='topic+.mcmcamwg'></span><span id='topic+.mcmccharm'></span><span id='topic+.mcmcdemc'></span><span id='topic+.mcmcdram'></span><span id='topic+.mcmcdrm'></span><span id='topic+.mcmcess'></span><span id='topic+.mcmcgibbs'></span><span id='topic+.mcmcgg'></span><span id='topic+.mcmcggcp'></span><span id='topic+.mcmcggcpp'></span><span id='topic+.mcmcggdp'></span><span id='topic+.mcmcggdpp'></span><span id='topic+.mcmcharm'></span><span id='topic+.mcmchmc'></span><span id='topic+.mcmchmcda'></span><span id='topic+.mcmcim'></span><span id='topic+.mcmcinca'></span><span id='topic+.mcmcmala'></span><span id='topic+.mcmcmcmcmc'></span><span id='topic+.mcmcmtm'></span><span id='topic+.mcmcmwg'></span><span id='topic+.mcmcnuts'></span><span id='topic+.mcmcohss'></span><span id='topic+.mcmcram'></span><span id='topic+.mcmcrdmh'></span><span id='topic+.mcmcrefractive'></span><span id='topic+.mcmcrj'></span><span id='topic+.mcmcrss'></span><span id='topic+.mcmcrwm'></span><span id='topic+.mcmcsamwg'></span><span id='topic+.mcmcsgld'></span><span id='topic+.mcmcslice'></span><span id='topic+.mcmcsmwg'></span><span id='topic+.mcmcthmc'></span><span id='topic+.mcmctwalk'></span><span id='topic+.mcmcuess'></span><span id='topic+.mcmcusamwg'></span><span id='topic+.mcmcusmwg'></span><span id='topic+.rowVars'></span><span id='topic+.vbsalimans2'></span>

<h3>Description</h3>

<p>Provides a complete environment for Bayesian inference using a variety of different samplers (see ?LaplacesDemon for an overview).
</p>


<h3>Details</h3>

<p>The DESCRIPTION file:
</p>

<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> LaplacesDemon</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 16.1.6</td>
</tr>
<tr>
 <td style="text-align: left;">
Title: </td><td style="text-align: left;"> Complete Environment for Bayesian Inference</td>
</tr>
<tr>
 <td style="text-align: left;">
Authors@R: </td><td style="text-align: left;"> c(person("Byron", "Hall", role = "aut"),
    person("Martina", "Hall", role = "aut"),
    person(family="Statisticat, LLC", role = "aut"),
    person(given="Eric", family="Brown", role = "ctb"),
    person(given="Richard", family="Hermanson", role = "ctb"),
    person(given="Emmanuel", family="Charpentier", role = "ctb"),
    person(given="Daniel", family="Heck", role = "ctb"),
    person(given="Stephane", family="Laurent", role = "ctb"),
    person(given="Quentin F.", family="Gronau", role = "ctb"),
    person(given="Henrik", family="Singmann", 
    email="singmann+LaplacesDemon@gmail.com", role="cre"))</td>
</tr>
<tr>
 <td style="text-align: left;">
Depends: </td><td style="text-align: left;"> R (&gt;= 3.0.0)</td>
</tr>
<tr>
 <td style="text-align: left;">
Imports: </td><td style="text-align: left;"> parallel, grDevices, graphics, stats, utils</td>
</tr>
<tr>
 <td style="text-align: left;">
Suggests: </td><td style="text-align: left;"> KernSmooth</td>
</tr>
<tr>
 <td style="text-align: left;">
ByteCompile: </td><td style="text-align: left;"> TRUE</td>
</tr>
<tr>
 <td style="text-align: left;">
Description: </td><td style="text-align: left;"> Provides a complete environment for Bayesian inference using a variety of different samplers (see ?LaplacesDemon for an overview).</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> MIT + file LICENSE</td>
</tr>
<tr>
 <td style="text-align: left;">
URL: </td><td style="text-align: left;"> https://github.com/LaplacesDemonR/LaplacesDemon</td>
</tr>
<tr>
 <td style="text-align: left;">
BugReports: </td><td style="text-align: left;"> https://github.com/LaplacesDemonR/LaplacesDemon/issues</td>
</tr>
<tr>
 <td style="text-align: left;">
Author: </td><td style="text-align: left;"> Byron Hall [aut],
  Martina Hall [aut],
  Statisticat, LLC [aut],
  Eric Brown [ctb],
  Richard Hermanson [ctb],
  Emmanuel Charpentier [ctb],
  Daniel Heck [ctb],
  Stephane Laurent [ctb],
  Quentin F. Gronau [ctb],
  Henrik Singmann [cre]</td>
</tr>
<tr>
 <td style="text-align: left;">
Maintainer: </td><td style="text-align: left;"> Henrik Singmann &lt;singmann+LaplacesDemon@gmail.com&gt;</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>


<p>Index of help topics:
</p>
<pre>
ABB                     Approximate Bayesian Bootstrap
AcceptanceRate          Acceptance Rate
BMK.Diagnostic          BMK Convergence Diagnostic
BayesFactor             Bayes Factor
BayesTheorem            Bayes' Theorem
BayesianBootstrap       The Bayesian Bootstrap
BigData                 Big Data
Blocks                  Blocks
CSF                     Cumulative Sample Function
CenterScale             Centering and Scaling
Combine                 Combine Demonoid Objects
Consort                 Consort with Laplace's Demon
Cov2Prec                Precision
ESS                     Effective Sample Size due to Autocorrelation
GIV                     Generate Initial Values
GaussHermiteQuadRule    Math Utility Functions
Gelfand.Diagnostic      Gelfand's Convergence Diagnostic
Gelman.Diagnostic       Gelman and Rubin's MCMC Convergence Diagnostic
Geweke.Diagnostic       Geweke's Convergence Diagnostic
Hangartner.Diagnostic   Hangartner's Convergence Diagnostic
Heidelberger.Diagnostic
                        Heidelberger and Welch's MCMC Convergence
                        Diagnostic
IAT                     Integrated Autocorrelation Time
Importance              Variable Importance
IterativeQuadrature     Iterative Quadrature
Juxtapose               Juxtapose MCMC Algorithm Inefficiency
KLD                     Kullback-Leibler Divergence (KLD)
KS.Diagnostic           Kolmogorov-Smirnov Convergence Diagnostic
LML                     Logarithm of the Marginal Likelihood
LPL.interval            Lowest Posterior Loss Interval
LaplaceApproximation    Laplace Approximation
LaplacesDemon           Laplace's Demon
LaplacesDemon-package   Complete Environment for Bayesian Inference
LaplacesDemon.RAM       LaplacesDemon RAM Estimate
Levene.Test             Levene's Test
LossMatrix              Loss Matrix
MCSE                    Monte Carlo Standard Error
MISS                    Multiple Imputation Sequential Sampling
MinnesotaPrior          Minnesota Prior
Mode                    The Mode(s) of a Vector
Model.Spec.Time         Model Specification Time
PMC                     Population Monte Carlo
PMC.RAM                 PMC RAM Estimate
PosteriorChecks         Posterior Checks
Raftery.Diagnostic      Raftery and Lewis's diagnostic
RejectionSampling       Rejection Sampling
SIR                     Sampling Importance Resampling
SensitivityAnalysis     Sensitivity Analysis
Stick                   Truncated Stick-Breaking
Thin                    Thin
Validate                Holdout Validation
VariationalBayes        Variational Bayes
WAIC                    Widely Applicable Information Criterion
as.covar                Proposal Covariance
as.indicator.matrix     Matrix Utility Functions
as.initial.values       Initial Values
as.parm.names           Parameter Names
as.ppc                  As Posterior Predictive Check
burnin                  Burn-in
caterpillar.plot        Caterpillar Plot
cloglog                 The log-log and complementary log-log functions
cond.plot               Conditional Plots
dStick                  Truncated Stick-Breaking Prior Distribution
dalaplace               Asymmetric Laplace Distribution: Univariate
dallaplace              Asymmetric Log-Laplace Distribution
daml                    Asymmetric Multivariate Laplace Distribution
dbern                   Bernoulli Distribution
dcat                    Categorical Distribution
dcrmrf                  Continuous Relaxation of a Markov Random Field
                        Distribution
ddirichlet              Dirichlet Distribution
de.Finetti.Game         de Finetti's Game
deburn                  De-Burn
delicit                 Prior Elicitation
demonchoice             Demon Choice Data Set
demonfx                 Demon FX Data Set
demonsessions           Demon Sessions Data Set
demonsnacks             Demon Snacks Data Set
demontexas              Demon Space-Time Data Set
dgpd                    Generalized Pareto Distribution
dgpois                  Generalized Poisson Distribution
dhalfcauchy             Half-Cauchy Distribution
dhalfnorm               Half-Normal Distribution
dhalft                  Half-t Distribution
dhs                     Horseshoe Distribution
dhuangwand              Huang-Wand Distribution
dhyperg                 Hyperprior-g Prior and Zellner's g-Prior
dinvbeta                Inverse Beta Distribution
dinvchisq               (Scaled) Inverse Chi-Squared Distribution
dinvgamma               Inverse Gamma Distribution
dinvgaussian            Inverse Gaussian Distribution
dinvmatrixgamma         Inverse Matrix Gamma Distribution
dinvwishart             Inverse Wishart Distribution
dinvwishartc            Inverse Wishart Distribution: Cholesky
                        Parameterization
dlaplace                Laplace Distribution: Univariate Symmetric
dlaplacem               Mixture of Laplace Distributions
dlaplacep               Laplace Distribution: Precision
                        Parameterization
dlasso                  LASSO Distribution
dllaplace               Log-Laplace Distribution: Univariate Symmetric
dlnormp                 Log-Normal Distribution: Precision
                        Parameterization
dmatrixgamma            Matrix Gamma Distribution
dmatrixnorm             Matrix Normal Distribution
dmvc                    Multivariate Cauchy Distribution
dmvcc                   Multivariate Cauchy Distribution: Cholesky
                        Parameterization
dmvcp                   Multivariate Cauchy Distribution: Precision
                        Parameterization
dmvcpc                  Multivariate Cauchy Distribution:
                        Precision-Cholesky Parameterization
dmvl                    Multivariate Laplace Distribution
dmvlc                   Multivariate Laplace Distribution: Cholesky
                        Parameterization
dmvn                    Multivariate Normal Distribution
dmvnc                   Multivariate Normal Distribution: Cholesky
                        Parameterization
dmvnp                   Multivariate Normal Distribution: Precision
                        Parameterization
dmvnpc                  Multivariate Normal Distribution:
                        Precision-Cholesky Parameterization
dmvpe                   Multivariate Power Exponential Distribution
dmvpec                  Multivariate Power Exponential Distribution:
                        Cholesky Parameterization
dmvpolya                Multivariate Polya Distribution
dmvt                    Multivariate t Distribution
dmvtc                   Multivariate t Distribution: Cholesky
                        Parameterization
dmvtp                   Multivariate t Distribution: Precision
                        Parameterization
dmvtpc                  Multivariate t Distribution: Precision-Cholesky
                        Parameterization
dnorminvwishart         Normal-Inverse-Wishart Distribution
dnormlaplace            Normal-Laplace Distribution: Univariate
                        Asymmetric
dnormm                  Mixture of Normal Distributions
dnormp                  Normal Distribution: Precision Parameterization
dnormv                  Normal Distribution: Variance Parameterization
dnormwishart            Normal-Wishart Distribution
dpareto                 Pareto Distribution
dpe                     Power Exponential Distribution: Univariate
                        Symmetric
dsdlaplace              Skew Discrete Laplace Distribution: Univariate
dsiw                    Scaled Inverse Wishart Distribution
dslaplace               Skew-Laplace Distribution: Univariate
dst                     Student t Distribution: Univariate
dstp                    Student t Distribution: Precision
                        Parameterization
dtrunc                  Truncated Distributions
dwishart                Wishart Distribution
dwishartc               Wishart Distribution: Cholesky Parameterization
dyangberger             Yang-Berger Distribution
interval                Constrain to Interval
is.appeased             Appeased
is.bayesfactor          Logical Check of Classes
is.bayesian             Logical Check of a Bayesian Model
is.constant             Logical Check of a Constant
is.constrained          Logical Check of Constraints
is.data                 Logical Check of Data
is.model                Logical Check of a Model
is.proper               Logical Check of Propriety
is.stationary           Logical Check of Stationarity
joint.density.plot      Joint Density Plot
joint.pr.plot           Joint Probability Region Plot
logit                   The logit and inverse-logit functions
p.interval              Probability Interval
plot.bmk                Plot Hellinger Distances
plot.demonoid           Plot samples from the output of Laplace's Demon
plot.demonoid.ppc       Plots of Posterior Predictive Checks
plot.importance         Plot Variable Importance
plot.iterquad           Plot the output of 'IterativeQuadrature'
plot.iterquad.ppc       Plots of Posterior Predictive Checks
plot.juxtapose          Plot MCMC Juxtaposition
plot.laplace            Plot the output of 'LaplaceApproximation'
plot.laplace.ppc        Plots of Posterior Predictive Checks
plot.miss               Plot samples from the output of MISS
plot.pmc                Plot samples from the output of PMC
plot.pmc.ppc            Plots of Posterior Predictive Checks
plot.vb                 Plot the output of 'VariationalBayes'
plot.vb.ppc             Plots of Posterior Predictive Checks
plotMatrix              Plot a Numerical Matrix
plotSamples             Plot Samples
predict.demonoid        Posterior Predictive Checks
predict.iterquad        Posterior Predictive Checks
predict.laplace         Posterior Predictive Checks
predict.pmc             Posterior Predictive Checks
predict.vb              Posterior Predictive Checks
print.demonoid          Print an object of class 'demonoid' to the
                        screen.
print.heidelberger      Print an object of class 'heidelberger' to the
                        screen.
print.iterquad          Print an object of class 'iterquad' to the
                        screen.
print.laplace           Print an object of class 'laplace' to the
                        screen.
print.miss              Print an object of class 'miss' to the screen.
print.pmc               Print an object of class 'pmc' to the screen.
print.raftery           Print an object of class 'raftery' to the
                        screen.
print.vb                Print an object of class 'vb' to the screen.
server_Listening        Server Listening
summary.demonoid.ppc    Posterior Predictive Check Summary
summary.iterquad.ppc    Posterior Predictive Check Summary
summary.laplace.ppc     Posterior Predictive Check Summary
summary.miss            MISS Summary
summary.pmc.ppc         Posterior Predictive Check Summary
summary.vb.ppc          Posterior Predictive Check Summary
</pre>
<p>The goal of LaplacesDemon, often referred to as LD, is to provide a
complete and self-contained Bayesian environment within R. For
example, this package includes dozens of MCMC algorithms, Laplace
Approximation, iterative quadrature, variational Bayes,
parallelization, big data, PMC, over 100 examples in the &ldquo;Examples&rdquo;
vignette, dozens of additional probability distributions, numerous
MCMC diagnostics, Bayes factors, posterior predictive checks, a
variety of plots, elicitation, parameter and variable importance,
Bayesian forms of test statistics (such as Durbin-Watson, Jarque-Bera,
etc.), validation, and numerous additional utility functions, such as
functions for multimodality, matrices, or timing your model
specification. Other vignettes include an introduction to Bayesian
inference, as well as a tutorial.
</p>
<p>No further development of this package is currently being done as the original
maintainer has stopped working on the package. Contributions to this
package are welcome at <a href="https://github.com/LaplacesDemonR/LaplacesDemon">https://github.com/LaplacesDemonR/LaplacesDemon</a>.
</p>
<p>The main function in this package is the <code>LaplacesDemon</code>
function, and the best place to start is probably with the
LaplacesDemon Tutorial vignette.
</p>


<h3>Author(s)</h3>

<p>NA
</p>
<p>Maintainer: NA
</p>

<hr>
<h2 id='ABB'>Approximate Bayesian Bootstrap</h2><span id='topic+ABB'></span>

<h3>Description</h3>

<p>This function performs multiple imputation (MI) with the Approximate
Bayesian Bootstrap (ABB) of Rubin and Schenker (1986).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ABB(X, K=1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ABB_+3A_x">X</code></td>
<td>
<p>This is a vector or matrix of data that must include both
observed and missing values. When <code>X</code> is a matrix, missing
values must occur somewhere in the set, but are not required to
occur in each variable.</p>
</td></tr>
<tr><td><code id="ABB_+3A_k">K</code></td>
<td>
<p>This is the number of imputations.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Approximate Bayesian Bootstrap (ABB) is a modified form of the
<code><a href="#topic+BayesianBootstrap">BayesianBootstrap</a></code> (Rubin, 1981) that is used for
multiple imputation (MI). Imputation is a family of statistical
methods for replacing missing values with estimates. Introduced by
Rubin and Schenker (1986) and Rubin (1987), MI is a family of
imputation methods that includes multiple estimates, and therefore
includes variability of the estimates.
</p>
<p>The data, <code class="reqn">\textbf{X}</code>, are assumed to be independent and
identically distributed (IID), contain both observed and missing
values, and its missing values are assumed to be ignorable (meaning
enough information is available in the data that the missingness
mechanism can be ignored, if the information is used properly) and
Missing Completely At Random (MCAR). When <code>ABB</code> is used in
conjunction with a propensity score (described below), missing values
may be Missing At Random (MAR).
</p>
<p><code>ABB</code> does not add auxiliary information, but performs imputation
with two sampling (with replacement) steps. First,
<code class="reqn">\textbf{X}^\star_{obs}</code> is sampled from
<code class="reqn">\textbf{X}_{obs}</code>. Then,
<code class="reqn">\textbf{X}^\star_{mis}</code> is sampled from
<code class="reqn">\textbf{X}^\star_{obs}</code>. The result is a sample of
the posterior predictive distribution of
<code class="reqn">(\textbf{X}_{mis}|\textbf{X}_{obs})</code>. The first
sampling step is also known as hotdeck imputation, and the second
sampling step changes the variance. Since auxiliary information is not
included, <code>ABB</code> is appropriate for missing values that are
ignorable and MCAR.
</p>
<p>Auxiliary information may be included in the process of imputation by
introducing a propensity score (Rosenbaum and Rubin, 1983; Rosenbaum
and Rubin, 1984), which is an estimate of the probability of
missingness. The propensity score is often the result of a binary
logit model, where missingness is predicted as a function of other
variables. The propensity scores are discretized into quantile-based
groups, usually quintiles. Each quintile must have both observed and
missing values. <code>ABB</code> is applied to each quintile. This is called
within-class imputation. It is assumed that the missing mechanism
depends only on the variables used to estimate the propensity score.
</p>
<p>With <code class="reqn">K=1</code>, <code>ABB</code> may be used in MCMC, such as in
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>, more commonly along with a propensity
score for missingness. MI is performed, despite <code class="reqn">K=1</code>, because
imputation occurs at each MCMC iteration. The practical advantage of
this form of imputation is the ease with which it may be
implemented. For example, full-likelihood imputation should perform
better, but requires a chain to be updated for each missing value.
</p>
<p>An example of a limitation of <code>ABB</code> with propensity scores is to
consider imputing missing values of income from age in a context where
age and income have a positive relationship, and where the highest
incomes are missing systematically. <code>ABB</code> with propensity scores
should impute these highest missing incomes given the highest observed
ages, but is unable to infer beyond the observed data.
</p>
<p>ABB has been extended (Parzen et al., 2005) to reduce bias, by
introducing a correction factor that is applied to the MI variance
estimate. This correction may be applied to output from <code>ABB</code>.
</p>


<h3>Value</h3>

<p>This function returns a list with <code class="reqn">K</code> components, one for each set
of imputations. Each component contains a vector of imputations equal
in length to the number of missing values in the data.
</p>
<p><code>ABB</code> does not currently return the mean of the imputations, or
the between-imputation variance or within-imputation variance.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>References</h3>

<p>Parzen, M., Lipsitz, S.R., and Fitzmaurice, G.M. (2005). &quot;A Note on
Reducing the Bias of the Approximate Bayesian Bootstrap Imputation
Variance Estimator&quot;. <em>Biometrika</em>, 92, 4, p. 971&ndash;974.
</p>
<p>Rosenbaum, P.R. and Rubin, D.B. (1983). &quot;The Central Role of the
Propensity Score in Observational Studies for Causal
Effects&quot;. <em>Biometrika</em>, 70, p. 41&ndash;55.
</p>
<p>Rosenbaum, P.R. and Rubin, D.B. (1984). &quot;Reducing Bias in
Observational Studies Using Subclassification in the Propensity
Score&quot;. <em>Journal of the American Statistical Association</em>, 79,
p. 516&ndash;524.
</p>
<p>Rubin, D.B. (1981). &quot;The Bayesian Bootstrap&quot;. <em>Annals of
Statistics</em>, 9, p. 130&ndash;134.
</p>
<p>Rubin, D.B. (1987). &quot;Multiple Imputation for Nonresponse in
Surveys&quot;. John Wiley and Sons: New York, NY.
</p>
<p>Rubin, D.B. and Schenker, N. (1986). &quot;Multiple Imputation for Interval
Estimation from Simple Random Samples with Ignorable
Nonresponse&quot;. <em>Journal of the American Statistical Association</em>,
81, p. 366&ndash;374.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+BayesianBootstrap">BayesianBootstrap</a></code>,
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>, and
<code><a href="#topic+MISS">MISS</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)

### Create Data
J &lt;- 10 #Number of variables
m &lt;- 20 #Number of missings
N &lt;- 50 #Number of records
mu &lt;- runif(J, 0, 100)
sigma &lt;- runif(J, 0, 100)
X &lt;- matrix(0, N, J)
for (j in 1:J) X[,j] &lt;- rnorm(N, mu[j], sigma[j])

### Create Missing Values
M1 &lt;- rep(0, N*J)
M2 &lt;- sample(N*J, m)
M1[M2] &lt;- 1
M &lt;- matrix(M1, N, J)
X &lt;- ifelse(M == 1, NA, X)

### Approximate Bayesian Bootstrap
imp &lt;- ABB(X, K=1)

### Replace Missing Values in X (when K=1)
X.imp &lt;- X
X.imp[which(is.na(X.imp))] &lt;- unlist(imp)
X.imp
</code></pre>

<hr>
<h2 id='AcceptanceRate'>Acceptance Rate</h2><span id='topic+AcceptanceRate'></span>

<h3>Description</h3>

<p>The <code>Acceptance.Rate</code> function calculates the acceptance rate per
chain from a matrix of posterior MCMC samples.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>AcceptanceRate(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="AcceptanceRate_+3A_x">x</code></td>
<td>
<p>This required argument accepts a <code class="reqn">S \times J</code>
numeric matrix of <code class="reqn">S</code> posterior samples for <code class="reqn">J</code> variables,
such as <code>Posterior1</code> or <code>Posterior2</code> from an object of
class <code>demonoid</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The acceptance rate of an MCMC algorithm is the percentage of
iterations in which the proposals were accepted.
</p>
<p>Optimal Acceptance Rates<br />
The optimal acceptance rate varies with the number of parameters and
by algorithm. Algorithms with componentwise Gaussian proposals have an
optimal acceptance rate of 0.44, regardless of the number of
parameters. Algorithms that update with multivariate Gaussian
proposals tend to have an optimal acceptance rate that ranges from
0.44 for one parameter (one IID Gaussian target distribution) to 0.234
for an infinite number of parameters (IID Gaussian target
distributions), and 0.234 is approached quickly as the number of
parameters increases. The AHMC, HMC, and THMC algorithms have an
optimal acceptance rate of 0.67, except with the algorithm
specification <code>L=1</code>, where the optimal acceptance rate is
0.574. The target acceptance rate is specified in HMCDA and NUTS, and
the recommended rate is 0.65 and 0.60 respectively. Some algorithms
have an acceptance rate of 1, such as AGG, ESS, GG, GS (MISS only),
SGLD, or Slice.
</p>
<p>Global and Local Acceptance Rates<br />
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code> reports the global acceptance rate for the
un-thinned chains. However, componentwise algorithms make a proposal
per parameter, and therefore have a local acceptance rate for each
parameter. Since only the global acceptance rate is reported, the
<code>AcceptanceRate</code> function may be used to calculate the local
acceptance rates from a matrix of un-thinned posterior samples.
</p>
<p>Thinning<br />
Thinned samples tend to have higher local acceptance rates than
un-thinned samples. With enough samples and enough thinning, local
acceptance rates approach 1. Local acceptance rates do not need to
approach the optimal acceptance rates above. Conversely, local
acceptance rates do not need to approach 1, because too much
information may possibly be discarded by thinning. For more
information on thinning, see the <code><a href="#topic+Thin">Thin</a></code> function.
</p>
<p>Diagnostics<br />
The <code>AcceptanceRate</code> function may be used to calculate local
acceptance rates on a matrix of thinned or un-thinned samples. Any
chain with a local acceptance rate that is an outlier may be studied
for reasons that may cause the outlier. A local acceptance rate
outlier does not violate theory and is often acceptable, but may
indicate a potential problem. Only some of the many potential problems
include: identifiability, model misspecification, multicollinearity,
multimodality, choice of prior distributions, or becoming trapped in a
low-probability space. The solution to local acceptance rate outliers
tends to be either changing the MCMC algorithm or re-specifying the
model or priors. For example, an MCMC algorithm that makes
multivariate Gaussian proposals for a large number of parameters may
have low global and local acceptance rates when far from the target
distributions.
</p>


<h3>Value</h3>

<p>The <code>AcceptanceRate</code> function returns a vector of acceptance
rates, one for each chain.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>,
<code><a href="#topic+MISS">MISS</a></code>,
<code><a href="#topic+PosteriorChecks">PosteriorChecks</a></code>, and
<code><a href="#topic+Thin">Thin</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
AcceptanceRate(matrix(rnorm(5000),1000,5))
</code></pre>

<hr>
<h2 id='as.covar'>Proposal Covariance</h2><span id='topic+as.covar'></span>

<h3>Description</h3>

<p>This function returns the most recent covariance matrix or a list of
blocking covariance matrices from an object of class <code>demonoid</code>,
the most recent covariance matrix from <code>iterquad</code>,
<code>laplace</code>, or <code>vb</code>, the most recent covariance matrix from
the chain with the lowest deviance in an object of class
<code>demonoid.hpc</code>, and a number of covariance matrices of an object
of class <code>pmc</code> equal to the number of mixture components. The
returned covariance matrix or matrices are intended to be the initial
proposal covariance matrix or matrices for future updates. A variance
vector from an object of class <code>demonoid</code> or <code>demonoid.hpc</code>
is converted to a covariance matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as.covar(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as.covar_+3A_x">x</code></td>
<td>
<p>This is an object of class <code>demonoid</code>,
<code>demonoid.hpc</code>, <code>iterquad</code>, <code>laplace</code>, <code>pmc</code>, or
<code>vb</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Unless it is known beforehand how many iterations are required for
iterative quadrature, Laplace Approximation, or Variational Bayes to
converge, MCMC to appear converged, or the normalized perplexity to
stabilize in PMC, multiple updates are necessary. An additional
update, however, should not begin with the same proposal covariance
matrix or matrices as the original update, because it will have to
repeat the work already accomplished. For this reason, the
<code>as.covar</code> function may be used at the end of an update to change
the previous initial values to the latest values.
</p>
<p>The <code>as.covar</code> function is most helpful with objects of class
<code>pmc</code> that have multiple mixture components. For more
information, see <code><a href="#topic+PMC">PMC</a></code>.
</p>


<h3>Value</h3>

<p>The returned value is a matrix (or array in the case of PMC with
multiple mixture components) of the latest observed or proposal
covariance, which may now be used as an initial proposal covariance
matrix or matrices for a future update.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code>,
<code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>,
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>,
<code><a href="#topic+LaplacesDemon.hpc">LaplacesDemon.hpc</a></code>,
<code><a href="#topic+PMC">PMC</a></code>, and
<code><a href="#topic+VariationalBayes">VariationalBayes</a></code>.
</p>

<hr>
<h2 id='as.initial.values'>Initial Values</h2><span id='topic+as.initial.values'></span>

<h3>Description</h3>

<p>This function returns the most recent posterior samples from an object
of class <code>demonoid</code> or <code>demonoid.hpc</code>, the posterior means
of an object of class <code>iterquad</code>, the posterior modes of an
object of class <code>laplace</code> or <code>vb</code>, the posterior means of an
object of class <code>pmc</code> with one mixture component, or the latest
means of the importance sampling distribution of an object of class
<code>pmc</code> with multiple mixture components. The returned values are
intended to be the initial values for future updates.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as.initial.values(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as.initial.values_+3A_x">x</code></td>
<td>
<p>This is an object of class <code>demonoid</code>,
<code>demonoid.hpc</code>, <code>iterquad</code>, <code>laplace</code>, <code>pmc</code>, or
<code>vb</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Unless it is known beforehand how many iterations are required for
<code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code>, <code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>,
or <code><a href="#topic+VariationalBayes">VariationalBayes</a></code> to converge, MCMC in
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code> to appear converged, or the normalized
perplexity to stabilize in <code><a href="#topic+PMC">PMC</a></code>, multiple updates are
necessary. An additional update, however, should not begin with the
same initial values as the original update, because it will have to
repeat the work already accomplished. For this reason, the
<code>as.initial.values</code> function may be used at the end of an update
to change the previous initial values to the latest values.
</p>
<p>When using <code><a href="#topic+LaplacesDemon.hpc">LaplacesDemon.hpc</a></code>, <code>as.initial.values</code>
should be used when the output is of class <code>demonoid.hpc</code>, before
the <code><a href="#topic+Combine">Combine</a></code> function is used to combine the multiple
chains for use with <code><a href="#topic+Consort">Consort</a></code> and other functions, because
the <code><a href="#topic+Combine">Combine</a></code> function returns an object of class
<code>demonoid</code>, and the number of chains will become unknown. The
<code><a href="#topic+Consort">Consort</a></code> function may suggest using
<code>as.initial.values</code>, but when applied to an object of class
<code>demonoid</code>, it will return the latest values as if there were
only one chain.
</p>


<h3>Value</h3>

<p>The returned value is a vector (or matrix in the case of an object of
class <code>demonoid.hpc</code>, or <code>pmc</code> with multiple mixture
components) of the latest values, which may now be used as initial
values for a future update.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+Combine">Combine</a></code>,
<code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code>,
<code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>,
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>,
<code><a href="#topic+LaplacesDemon.hpc">LaplacesDemon.hpc</a></code>,
<code><a href="#topic+PMC">PMC</a></code>, and
<code><a href="#topic+VariationalBayes">VariationalBayes</a></code>.
</p>

<hr>
<h2 id='as.parm.names'>Parameter Names</h2><span id='topic+as.parm.names'></span>

<h3>Description</h3>

<p>This function creates a vector of parameter names from a list of
parameters, and the list may contain any combination of scalars,
vectors, matrices, upper-triangular matrices, and arrays.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as.parm.names(x, uppertri=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as.parm.names_+3A_x">x</code></td>
<td>
<p>This required argument is a list of named parameters. The
list may contain scalars, vectors, matrices, and arrays. The value
of the named parameters does not matter here, though they are
usually set to zero. However, if a missing value occurs, then the
associated element is omitted in the output.</p>
</td></tr>
<tr><td><code id="as.parm.names_+3A_uppertri">uppertri</code></td>
<td>
<p>This optional argument must be a vector with a length
equal to the number of named parameters. Each element in
<code>uppertri</code> must be either a 0 or 1, where a 1 indicates that an
upper triangular matrix will be used for the associated element in
the vector of named parameters. Each element of <code>uppertri</code> is
associated with a named parameter. The <code>uppertri</code> argument does
not function with arrays.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Each <code>model</code> function for <code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code>,
<code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>, <code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>,
<code><a href="#topic+PMC">PMC</a></code>, or <code><a href="#topic+VariationalBayes">VariationalBayes</a></code> requires a vector
of parameters (specified at first as <code>Initial.Values</code>) and a list
of data. One component in the list of data must be named
<code>parm.names</code>. Each element of <code>parm.names</code> is a name
associated with the corresponding parameter in <code>Initial.Values</code>.
</p>
<p>The <code>parm.names</code> vector is easy to program explicitly for a simple
model, but can require considerably more programming effort for more
complicated models. The <code>as.parm.names</code> function is a utility
function designed to minimize programming by the user.
</p>
<p>For example, a simple model may only require <code>parm.names &lt;-
  c("alpha", "beta[1]", "beta[2]", "sigma")</code>. A more complicated model
may contain hundreds of parameters that are a combination of scalars,
vectors, matrices, upper-triangular matrices, and arrays, and is the
reason for the <code>as.parm.names</code> function. The code for the above
is <code>as.parm.names(list(alpha=0, beta=rep(0,2), sigma=0))</code>.
</p>
<p>In the case of an upper-triangular matrix, simply pass the full matrix
to <code>as.parm.names</code> and indicate that only the upper-triangular
will be used via the <code>uppertri</code> argument. For example,
<code>as.parm.names(list(beta=rep(0,J),U=diag(K)), uppertri=c(0,1))</code>
creates parameter names for a vector of <code class="reqn">\beta</code> parameters of
length <code class="reqn">J</code> and an upper-triangular matrix <code class="reqn">\textbf{U}</code> of
dimension <code class="reqn">K</code>.
</p>
<p>Numerous examples may be found in the accompanying &ldquo;Examples&rdquo;
vignette.
</p>


<h3>Value</h3>

<p>This function returns a vector of parameter names.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code>
<code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>,
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>,
<code><a href="#topic+PMC">PMC</a></code>, and
<code><a href="#topic+VariationalBayes">VariationalBayes</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
N &lt;- 100
J &lt;- 5
y &lt;- rnorm(N,0,1)
X &lt;- matrix(runif(N*J,-2,2),N,J)
S &lt;- diag(J)
T &lt;- diag(2)
mon.names &lt;- c("LP","sigma")
parm.names &lt;- as.parm.names(list(log.sigma=0, beta=rep(0,J), S=diag(J),
     T=diag(2)), uppertri=c(0,0,0,1))
MyData &lt;- list(J=J, N=N, S=S, T=T, X=X, mon.names=mon.names,
     parm.names=parm.names, y=y)
MyData
</code></pre>

<hr>
<h2 id='as.ppc'>As Posterior Predictive Check</h2><span id='topic+as.ppc'></span>

<h3>Description</h3>

<p>This function converts an object of class <code>demonoid.val</code> to an
object of class <code>demonoid.ppc</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as.ppc(x, set=3)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as.ppc_+3A_x">x</code></td>
<td>
<p>This is an object of class <code>demonoid.val</code>.</p>
</td></tr>
<tr><td><code id="as.ppc_+3A_set">set</code></td>
<td>
<p>This is an integer that indicates which list component is
to be used. When <code>set=1</code>, the modeled data set is used. When
<code>set=2</code>, the validation data set is used. When <code>set=3</code>,
both data sets are used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>After using the <code><a href="#topic+Validate">Validate</a></code> function for holdout
validation, it is often suggested to perform posterior predictive
checks. The <code>as.ppc</code> function converts the output object of
<code><a href="#topic+Validate">Validate</a></code>, which is an object of class
<code>demonoid.val</code>, to an object of class <code>demonoid.ppc</code>. The
returned object is the same as if it were created with the
<code><a href="#topic+predict.demonoid">predict.demonoid</a></code> function, rather than the
<code><a href="#topic+Validate">Validate</a></code> function.
</p>
<p>After this conversion, the user may use posterior predictive checks,
as usual, with the <code><a href="#topic+summary.demonoid.ppc">summary.demonoid.ppc</a></code> function.
</p>


<h3>Value</h3>

<p>The returned object is an object of class <code>demonoid.ppc</code>.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.demonoid">predict.demonoid</a></code>,
<code><a href="#topic+summary.demonoid.ppc">summary.demonoid.ppc</a></code>, and
<code><a href="#topic+Validate">Validate</a></code>.
</p>

<hr>
<h2 id='BayesFactor'>Bayes Factor</h2><span id='topic+BayesFactor'></span>

<h3>Description</h3>

<p>This function calculates Bayes factors for two or more fitted objects
of class <code>demonoid</code>, <code>iterquad</code>, <code>laplace</code>, <code>pmc</code>,
or <code>vb</code> that were estimated respectively with the
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>, <code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code>,
<code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>, <code><a href="#topic+PMC">PMC</a></code>, or
<code><a href="#topic+VariationalBayes">VariationalBayes</a></code> functions, and indicates the strength
of evidence in favor of the hypothesis (that each model,
<code class="reqn">\mathcal{M}_i</code>, is better than another model,
<code class="reqn">\mathcal{M}_j</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>BayesFactor(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="BayesFactor_+3A_x">x</code></td>
<td>
<p>This is a list of two or more fitted objects of class
<code>demonoid</code>, <code>iterquad</code>, <code>laplace</code>, <code>pmc</code>, or
<code>vb</code>. The components are named in order beginning with model 1,
<code>M1</code>, and <code class="reqn">k</code> models are usually represented as
<code class="reqn">\mathcal{M}_1,\dots,\mathcal{M}_k</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Introduced by Harold Jeffreys, a 'Bayes factor' is a Bayesian
alternative to frequentist hypothesis testing that is most often used
for the comparison of multiple models by hypothesis testing, usually
to determine which model better fits the data (Jeffreys, 1961). Bayes
factors are notoriously difficult to compute, and the Bayes factor is
only defined when the marginal density of <code class="reqn">\textbf{y}</code> under
each model is proper (see <code><a href="#topic+is.proper">is.proper</a></code>). However, the Bayes
factor is easy to approximate with the Laplace-Metropolis estimator
(Lewis and Raftery, 1997) and other methods of approximating the
logarithm of the marginal likelihood (for more information, see
<code><a href="#topic+LML">LML</a></code>).
</p>
<p>Hypothesis testing with Bayes factors is more robust than frequentist
hypothesis testing, since the Bayesian form avoids model selection
bias, evaluates evidence in favor of the null hypothesis, includes
model uncertainty, and allows non-nested models to be compared (though
of course the model must have the same dependent variable). Also,
frequentist significance tests become biased in favor of rejecting the
null hypothesis with sufficiently large sample size.
</p>
<p>The Bayes factor for comparing two models may be approximated as the
ratio of the marginal likelihood of the data in model 1 and model
2. Formally, the Bayes factor in this case is
</p>
<p style="text-align: center;"><code class="reqn">B =
  \frac{p(\textbf{y}|\mathcal{M}_1)}{p(\textbf{y}|\mathcal{M}_2)} =
  \frac{\int
    p(\textbf{y}|\Theta_1,\mathcal{M}_1)p(\Theta_1|\mathcal{M}_1)d\Theta_1}{\int
    p(\textbf{y}|\Theta_2,\mathcal{M}_2)p(\Theta_2|\mathcal{M}_2)d\Theta_2}</code>
</p>

<p>where <code class="reqn">p(\textbf{y}|\mathcal{M}_1)</code> is the marginal
likelihood of the data in model 1.
</p>
<p>The <code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code>,
<code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>, <code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>,
<code><a href="#topic+PMC">PMC</a></code>, and <code><a href="#topic+VariationalBayes">VariationalBayes</a></code> functions each
return the <code><a href="#topic+LML">LML</a></code>, the approximate logarithm of the
marginal likelihood of the data, in each fitted object of class
<code>iterquad</code>, <code>laplace</code>, <code>demonoid</code>, <code>pmc</code>, or
<code>vb</code>. The <code>BayesFactor</code> function calculates matrix <code>B</code>,
a matrix of Bayes factors, where each element of matrix <code>B</code> is a
comparison of two models. Each Bayes factor is calculated as the
exponentiated difference of <code>LML</code> of model 1
(<code class="reqn">\mathcal{M}_1</code>) and <code>LML</code> of model 2
(<code class="reqn">\mathcal{M}_2</code>), and the hypothesis for each element of
matrix <code>B</code> is that the model associated with the row is greater
than the model associated with the column. For example, element
<code>B[3,2]</code> is the Bayes factor that model 3 is greater than model
2. The 'Strength of Evidence' aids in the interpretation (Jeffreys,
1961).
</p>
<p>A table for the interpretation of the strength of evidence for Bayes
factors is available at
<a href="https://web.archive.org/web/20150214194051/http://www.bayesian-inference.com/bayesfactors">https://web.archive.org/web/20150214194051/http://www.bayesian-inference.com/bayesfactors</a>.
</p>
<p>Each Bayes factor, <code>B</code>, is the posterior odds in favor of the
hypothesis divided by the prior odds in favor of the hypothesis, where
the hypothesis is usually
<code class="reqn">\mathcal{M}_1 &gt; \mathcal{M}_2</code>. For example, when
<code>B[3,2]=2</code>, the data favor <code class="reqn">\mathcal{M}_3</code> over
<code class="reqn">\mathcal{M}_2</code> with 2:1 odds.
</p>
<p>It is also popular to consider the natural logarithm of the Bayes
factor. The scale of the logged Bayes factor is the same above and
below one, which is more appropriate for visual comparisons. For
example, when comparing two Bayes factors at 0.5 and 2, the logarithm
of these Bayes factors is -0.69 and 0.69.
</p>
<p>Gelman finds Bayes factors generally to be irrelevant, because they
compute the relative probabilities of the models conditional on one
of them being true. Gelman prefers approaches that measure the
distance of the data to each of the approximate models (Gelman et al.,
2004, p. 180), such as with posterior predictive checks (see the
<code><a href="#topic+predict.iterquad">predict.iterquad</a></code> function regarding iterative
quadrature, <code><a href="#topic+predict.laplace">predict.laplace</a></code> function in the context of
Laplace Approximation, <code><a href="#topic+predict.demonoid">predict.demonoid</a></code> function in the
context of MCMC, <code><a href="#topic+predict.pmc">predict.pmc</a></code> function in the context
of PMC, or <code><a href="#topic+predict.vb">predict.vb</a></code> function in the context of
Variational Bayes). Kass et al. (1995) asserts this can be done
without assuming one model is the true model.
</p>


<h3>Value</h3>

<p><code>BayesFactor</code> returns an object of class <code>bayesfactor</code> that
is a list with the following components:
</p>
<table>
<tr><td><code>B</code></td>
<td>
<p>This is a matrix of Bayes factors.</p>
</td></tr>
<tr><td><code>Hypothesis</code></td>
<td>

<p>This is the hypothesis, and is stated as 'row &gt; column', indicating
that the model associated with the row of an element in matrix
<code>B</code> is greater than the model associated with the column of
that element.</p>
</td></tr>
<tr><td><code>Strength.of.Evidence</code></td>
<td>

<p>This is the strength of evidence in favor of the hypothesis.</p>
</td></tr>
<tr><td><code>Posterior.Probability</code></td>
<td>

<p>This is a vector of the posterior probability of each model, given
flat priors.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Statisticat, LLC.</p>


<h3>References</h3>

<p>Gelman, A., Carlin, J., Stern, H., and Rubin, D. (2004). &quot;Bayesian
Data Analysis, Texts in Statistical Science, 2nd ed.&quot;. Chapman and
Hall, London.
</p>
<p>Jeffreys, H. (1961). &quot;Theory of Probability, Third Edition&quot;. Oxford
University Press: Oxford, England.
</p>
<p>Kass, R.E. and Raftery, A.E. (1995). &quot;Bayes Factors&quot;. <em>Journal
of the American Statistical Association</em>, 90(430), p. 773&ndash;795.
</p>
<p>Lewis, S.M. and Raftery, A.E. (1997). &quot;Estimating Bayes Factors via
Posterior Simulation with the Laplace-Metropolis Estimator&quot;.
<em>Journal of the American Statistical Association</em>, 92,
p. 648&ndash;655.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+is.bayesfactor">is.bayesfactor</a></code>,
<code><a href="#topic+is.proper">is.proper</a></code>,
<code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code>,
<code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>,
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>,
<code><a href="#topic+LML">LML</a></code>,
<code><a href="#topic+PMC">PMC</a></code>,
<code><a href="#topic+predict.demonoid">predict.demonoid</a></code>,
<code><a href="#topic+predict.iterquad">predict.iterquad</a></code>,
<code><a href="#topic+predict.laplace">predict.laplace</a></code>,
<code><a href="#topic+predict.pmc">predict.pmc</a></code>,
<code><a href="#topic+predict.vb">predict.vb</a></code>, and
<code><a href="#topic+VariationalBayes">VariationalBayes</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># The following example fits a model as Fit1, then adds a predictor, and
# fits another model, Fit2. The two models are compared with Bayes
# factors.

library(LaplacesDemon)

##############################  Demon Data  ###############################
data(demonsnacks)
J &lt;- 2
y &lt;- log(demonsnacks$Calories)
X &lt;- cbind(1, as.matrix(log(demonsnacks[,10]+1)))
X[,2] &lt;- CenterScale(X[,2])

#########################  Data List Preparation  #########################
mon.names &lt;- "LP"
parm.names &lt;- as.parm.names(list(beta=rep(0,J), sigma=0))
pos.beta &lt;- grep("beta", parm.names)
pos.sigma &lt;- grep("sigma", parm.names)
PGF &lt;- function(Data) {
     beta &lt;- rnorm(Data$J)
     sigma &lt;- runif(1)
     return(c(beta, sigma))
     }
MyData &lt;- list(J=J, PGF=PGF, X=X, mon.names=mon.names,
     parm.names=parm.names, pos.beta=pos.beta, pos.sigma=pos.sigma, y=y)

##########################  Model Specification  ##########################
Model &lt;- function(parm, Data)
     {
     ### Parameters
     beta &lt;- parm[Data$pos.beta]
     sigma &lt;- interval(parm[Data$pos.sigma], 1e-100, Inf)
     parm[Data$pos.sigma] &lt;- sigma
     ### Log-Prior
     beta.prior &lt;- sum(dnormv(beta, 0, 1000, log=TRUE))
     sigma.prior &lt;- dhalfcauchy(sigma, 25, log=TRUE)
     ### Log-Likelihood
     mu &lt;- tcrossprod(Data$X, t(beta))
     LL &lt;- sum(dnorm(Data$y, mu, sigma, log=TRUE))
     ### Log-Posterior
     LP &lt;- LL + beta.prior + sigma.prior
     Modelout &lt;- list(LP=LP, Dev=-2*LL, Monitor=LP,
          yhat=rnorm(length(mu), mu, sigma), parm=parm)
     return(Modelout)
     }

############################  Initial Values  #############################
Initial.Values &lt;- GIV(Model, MyData, PGF=TRUE)

########################  Laplace Approximation  ##########################
Fit1 &lt;- LaplaceApproximation(Model, Initial.Values, Data=MyData,
     Iterations=10000)
Fit1

##############################  Demon Data  ###############################
data(demonsnacks)
J &lt;- 3
y &lt;- log(demonsnacks$Calories)
X &lt;- cbind(1, as.matrix(demonsnacks[,c(7,8)]))
X[,2] &lt;- CenterScale(X[,2])
X[,3] &lt;- CenterScale(X[,3])

#########################  Data List Preparation  #########################
mon.names &lt;- c("sigma","mu[1]")
parm.names &lt;- as.parm.names(list(beta=rep(0,J), sigma=0))
pos.beta &lt;- grep("beta", parm.names)
pos.sigma &lt;- grep("sigma", parm.names)
PGF &lt;- function(Data) return(c(rnormv(Data$J,0,10), rhalfcauchy(1,5)))
MyData &lt;- list(J=J, PGF=PGF, X=X, mon.names=mon.names,
     parm.names=parm.names, pos.beta=pos.beta, pos.sigma=pos.sigma, y=y)

############################  Initial Values  #############################
Initial.Values &lt;- GIV(Model, MyData, PGF=TRUE)

########################  Laplace Approximation  ##########################
Fit2 &lt;- LaplaceApproximation(Model, Initial.Values, Data=MyData,
     Iterations=10000)
Fit2

#############################  Bayes Factor  ##############################
Model.list &lt;- list(M1=Fit1, M2=Fit2)
BayesFactor(Model.list)
</code></pre>

<hr>
<h2 id='BayesianBootstrap'>The Bayesian Bootstrap</h2><span id='topic+BayesianBootstrap'></span>

<h3>Description</h3>

<p>This function performs the Bayesian bootstrap of Rubin (1981),
returning either bootstrapped weights or statistics.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>BayesianBootstrap(X, n=1000, Method="weights", Status=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="BayesianBootstrap_+3A_x">X</code></td>
<td>
<p>This is a vector or matrix of data. When a matrix is
supplied, sampling is based on the first column.</p>
</td></tr>
<tr><td><code id="BayesianBootstrap_+3A_n">n</code></td>
<td>
<p>This is the number of bootstrapped replications.</p>
</td></tr>
<tr><td><code id="BayesianBootstrap_+3A_method">Method</code></td>
<td>
<p>When <code>Method="weights"</code> (which is the default),
a matrix of row weights is returned. Otherwise, a function is
accepted. The function specifies the statistic to be
bootstrapped. The first argument of the function should be a matrix
of data, and the second argument should be a vector of weights.</p>
</td></tr>
<tr><td><code id="BayesianBootstrap_+3A_status">Status</code></td>
<td>
<p>This determines the periodicity of status messages. When
<code>Status=100</code>, for example, a status message is displayed every
100 replications. Otherwise, <code>Status</code> defaults to <code>NULL</code>,
and status messages are not displayed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The term, &lsquo;bootstrap&rsquo;, comes from the German novel <em>Adventures of
Baron Munchausen</em> by Rudolph Raspe, in which the hero saves himself
from drowning by pulling on his own bootstraps. The idea of the
statistical bootstrap is to evaluate properties of an estimator
through the empirical, rather than theoretical, CDF.
</p>
<p>Rubin (1981) introduced the Bayesian bootstrap. In contrast to the
frequentist bootstrap which simulates the sampling distribution of a
statistic estimating a parameter, the Bayesian bootstrap simulates the
posterior distribution.
</p>
<p>The data, <code class="reqn">\textbf{X}</code>, are assumed to be independent and
identically distributed (IID), and to be a representative sample of
the larger (bootstrapped) population. Given that the data has <code class="reqn">N</code>
rows in one bootstrap replication, the row weights are sampled from a
Dirichlet distribution with all <code class="reqn">N</code> concentration parameters equal
to <code class="reqn">1</code> (a uniform distribution over an open standard <code class="reqn">N-1</code>
simplex). The distributions of a parameter inferred from considering
many samples of weights are interpretable as posterior distributions
on that parameter.
</p>
<p>The Bayesian bootstrap is useful for estimating marginal posterior
covariance and standard deviations for the posterior modes of
<code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>, especially when the model
dimension (the number of parameters) is large enough that estimating
the <code><a href="#topic+Hessian">Hessian</a></code> matrix of second partial derivatives is too
computationally demanding.
</p>
<p>Just as with the frequentist bootstrap, inappropriate use of the
Bayesian bootstrap can lead to inappropriate inferences. The Bayesian
bootstrap violates the likelihood principle, because the evaluation of
a statistic of interest depends on data sets other than the observed
data set. For more information on the likelihood principle, see
<a href="https://web.archive.org/web/20150213002158/http://www.bayesian-inference.com/likelihood#likelihoodprinciple">https://web.archive.org/web/20150213002158/http://www.bayesian-inference.com/likelihood#likelihoodprinciple</a>.
</p>
<p>The <code>BayesianBootstrap</code> function has many uses, including
creating test statistics on the population data given the observed
data (supported here), imputation (with this variation:
<code><a href="#topic+ABB">ABB</a></code>), validation, and more.
</p>


<h3>Value</h3>

<p>When <code>Method="weights"</code>, this function returns a
<code class="reqn">N \times n</code> matrix of weights, where the number of rows
<code class="reqn">N</code> is equal to the number of rows in <code>X</code>.
</p>
<p>For statistics, a matrix or array is returned, depending on the number
of dimensions. The replicates are indexed by row in a matrix or in
the first dimension of the array.
</p>


<h3>Author(s)</h3>

<p>Bogumil Kaminski, <a href="mailto:bkamins@sgh.waw.pl">bkamins@sgh.waw.pl</a> and
Statisticat, LLC.</p>


<h3>References</h3>

<p>Rubin, D.B. (1981). &quot;The Bayesian Bootstrap&quot;. <em>The Annals of
Statistics</em>, 9(1), p. 130&ndash;134.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ABB">ABB</a></code>,
<code><a href="#topic+Hessian">Hessian</a></code>,
<code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>, and
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)

#Example 1: Samples
x &lt;- 1:2
BB &lt;- BayesianBootstrap(X=x, n=100, Method="weights"); BB

#Example 2: Mean, Univariate
x &lt;- 1:2
BB &lt;- BayesianBootstrap(X=x, n=100, Method=weighted.mean); BB

#Example 3: Mean, Multivariate
data(demonsnacks)
BB &lt;- BayesianBootstrap(X=demonsnacks, n=100,
     Method=function(x,w) apply(x, 2, weighted.mean, w=w)); BB

#Example 4: Correlation
dye &lt;- c(1.15, 1.70, 1.42, 1.38, 2.80, 4.70, 4.80, 1.41, 3.90)
efp &lt;- c(1.38, 1.72, 1.59, 1.47, 1.66, 3.45, 3.87, 1.31, 3.75)
X &lt;- matrix(c(dye,efp), length(dye), 2)
colnames(X) &lt;- c("dye","efp")
BB &lt;- BayesianBootstrap(X=X, n=100,
     Method=function(x,w) cov.wt(x, w, cor=TRUE)$cor); BB

#Example 5: Marginal Posterior Covariance
#The following example is commented out due to package build time.
#To run the following example, use the code from the examples in
#the LaplaceApproximation function for the data, model specification
#function, and initial values. Then perform the Laplace
#Approximation as below (with CovEst="Identity" and sir=FALSE) until
#convergence, set the latest initial values, then use the Bayesian
#bootstrap on the data, run the Laplace Approximation again to
#convergence, save the posterior modes, and repeat until S samples
#of the posterior modes are collected. Finally, calculate the
#parameter covariance or standard deviation.

#Fit &lt;- LaplaceApproximation(Model, Initial.Values, Data=MyData,
#     Iterations=1000, Method="SPG",  CovEst="Identity", sir=FALSE)
#Initial.Values &lt;- as.initial.values(Fit)
#S &lt;- 100 #Number of bootstrapped sets of posterior modes (parameters)
#Z &lt;- rbind(Fit$Summary1[,1]) #Bootstrapped parameters collected here
#N &lt;- nrow(MyData$X) #Number of records
#MyData.B &lt;- MyData
#for (s in 1:S) {
#     cat("\nIter:", s, "\n")
#     BB &lt;- BayesianBootstrap(MyData$y, n=N)
#     z &lt;- apply(BB, 2, function(x) sample.int(N, size=1, prob=x))
#     MyData.B$y &lt;- MyData$y[z]
#     MyData.B$X &lt;- MyData$X[z,]
#     Fit &lt;- LaplaceApproximation(Model, Initial.Values, Data=MyData.B,
#          Iterations=1000, Method="SPG", CovEst="Identity", sir=FALSE)
#     Z &lt;- rbind(Z, Fit$Summary1[,1])}
#cov(Z) #Bootstrapped marginal posterior covariance
#sqrt(diag(cov(Z))) #Bootstrapped marginal posterior standard deviations
</code></pre>

<hr>
<h2 id='BayesTheorem'>Bayes' Theorem</h2><span id='topic+BayesTheorem'></span>

<h3>Description</h3>

<p>Bayes' theorem shows the relation between two conditional
probabilities that are the reverse of each other. This theorem is
named after Reverend Thomas Bayes (1702-1761), and is also referred to
as Bayes' law or Bayes' rule (Bayes and Price, 1763). Bayes' theorem
expresses the conditional probability, or &lsquo;posterior probability&rsquo;, of
an event <code class="reqn">A</code> after <code class="reqn">B</code> is observed in terms of the 'prior
probability' of <code class="reqn">A</code>, prior probability of <code class="reqn">B</code>, and the
conditional probability of <code class="reqn">B</code> given <code class="reqn">A</code>. Bayes' theorem is
valid in all common interpretations of probability. This function
provides one of several forms of calculations that are possible with
Bayes' theorem.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>BayesTheorem(PrA, PrBA)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="BayesTheorem_+3A_pra">PrA</code></td>
<td>
<p>This required argument is the prior probability of <code class="reqn">A</code>,
or <code class="reqn">\Pr(A)</code>.</p>
</td></tr>
<tr><td><code id="BayesTheorem_+3A_prba">PrBA</code></td>
<td>
<p>This required argument is the conditional probability of
<code class="reqn">B</code> given <code class="reqn">A</code> or <code class="reqn">\Pr(B | A)</code>, and is known
as the data, evidence, or likelihood.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Bayes' theorem provides an expression for the conditional probability
of <code class="reqn">A</code> given <code class="reqn">B</code>, which is equal to
</p>
<p style="text-align: center;"><code class="reqn">\Pr(A | B) = \frac{\Pr(B | A)\Pr(A)}{\Pr(B)}</code>
</p>

<p>For example, suppose one asks the question: what is the probability of
going to Hell, conditional on consorting (or given that a person
consorts) with Laplace's Demon. By replacing <code class="reqn">A</code> with <code class="reqn">Hell</code>
and <code class="reqn">B</code> with <code class="reqn">Consort</code>, the question becomes
</p>
<p style="text-align: center;"><code class="reqn">\Pr(\mathrm{Hell} | \mathrm{Consort}) =
  \frac{\Pr(\mathrm{Consort} |
  \mathrm{Hell})\Pr(\mathrm{Hell})}{\Pr(\mathrm{Consort})}</code>
</p>

<p>Note that a common fallacy is to assume that <code class="reqn">\Pr(A | B) = \Pr(B |
    A)</code>, which is called the conditional
probability fallacy.
</p>
<p>Another way to state Bayes' theorem (and this is the form in the
provided function) is
</p>
<p style="text-align: center;"><code class="reqn">\Pr(A_i | B) = \frac{\Pr(B | A_i)\Pr(A_i)}{\Pr(B | A_i)\Pr(A_i)
  +\dots+ \Pr(B | A_n)\Pr(A_n)}</code>
</p>

<p>Let's examine our <em>burning</em> question, by replacing
<code class="reqn">A_i</code> with Hell or Heaven, and replacing <code class="reqn">B</code> with
Consort
</p>

<ul>
<li> <p><code class="reqn">\Pr(A_1) = \Pr(\mathrm{Hell})</code>
</p>
</li>
<li> <p><code class="reqn">\Pr(A_2) = \Pr(\mathrm{Heaven})</code>
</p>
</li>
<li> <p><code class="reqn">\Pr(B) = \Pr(\mathrm{Consort})</code>
</p>
</li>
<li> <p><code class="reqn">\Pr(A_1 | B) = \Pr(\mathrm{Hell} |
    \mathrm{Consort})</code>
</p>
</li>
<li> <p><code class="reqn">\Pr(A_2 | B) = \Pr(\mathrm{Heaven} |
    \mathrm{Consort})</code>
</p>
</li>
<li> <p><code class="reqn">\Pr(B | A_1) = \Pr(\mathrm{Consort} |
    \mathrm{Hell})</code>
</p>
</li>
<li> <p><code class="reqn">\Pr(B | A_2) = \Pr(\mathrm{Consort} |
    \mathrm{Heaven})</code>
</p>
</li></ul>

<p>Laplace's Demon was conjured and asked for some data. He was glad to
oblige.
</p>

<ul>
<li><p> 6 people consorted out of 9 who went to Hell.
</p>
</li>
<li><p> 5 people consorted out of 7 who went to Heaven.
</p>
</li>
<li><p> 75% of the population goes to Hell.
</p>
</li>
<li><p> 25% of the population goes to Heaven.
</p>
</li></ul>

<p>Now, Bayes' theorem is applied to the data. Four pieces are worked out
as follows
</p>

<ul>
<li> <p><code class="reqn">\Pr(\mathrm{Consort} | \mathrm{Hell}) = 6/9 =
    0.666</code>
</p>
</li>
<li> <p><code class="reqn">\Pr(\mathrm{Consort} | \mathrm{Heaven}) = 5/7 =
    0.714</code>
</p>
</li>
<li> <p><code class="reqn">\Pr(\mathrm{Hell}) = 0.75</code>
</p>
</li>
<li> <p><code class="reqn">\Pr(\mathrm{Heaven}) = 0.25</code>
</p>
</li></ul>

<p>Finally, the desired conditional probability <code class="reqn">\Pr(\mathrm{Hell} |
    \mathrm{Consort})</code> is calculated using Bayes'
theorem
</p>

<ul>
<li> <p><code class="reqn">\Pr(\mathrm{Hell} | \mathrm{Consort}) =
    \frac{0.666(0.75)}{0.666(0.75) + 0.714(0.25)}</code>
</p>
</li>
<li> <p><code class="reqn">\Pr(\mathrm{Hell} | \mathrm{Consort}) = 0.737</code>
</p>
</li></ul>

<p>The probability of someone consorting with Laplace's Demon and going
to Hell is 73.7%, which is less than the prevalence of 75% in the
population. According to these findings, consorting with Laplace's
Demon does not increase the probability of going to Hell.
</p>
<p>For an introduction to model-based Bayesian inference, see the
accompanying vignette entitled &ldquo;Bayesian Inference&rdquo; or
<a href="https://web.archive.org/web/20150206004608/http://www.bayesian-inference.com/bayesian">https://web.archive.org/web/20150206004608/http://www.bayesian-inference.com/bayesian</a>.
</p>


<h3>Value</h3>

<p>The <code>BayesTheorem</code> function returns the conditional probability
of <code class="reqn">A</code> given <code class="reqn">B</code>, known in Bayesian inference as the
posterior. The returned object is of class <code>bayestheorem</code>.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC.</p>


<h3>References</h3>

<p>Bayes, T. and Price, R. (1763). &quot;An Essay Towards Solving a Problem in
the Doctrine of Chances&quot;. By the late Rev. Mr. Bayes, communicated by
Mr. Price, in a letter to John Canton, M.A. and F.R.S.
<em>Philosophical Transactions of the Royal Statistical Society of
London</em>, 53, p. 370&ndash;418.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code>,
<code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>,
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>,
<code><a href="#topic+PMC">PMC</a></code>, and
<code><a href="#topic+VariationalBayes">VariationalBayes</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Pr(Hell|Consort) =
PrA &lt;- c(0.75,0.25)
PrBA &lt;- c(6/9, 5/7)
BayesTheorem(PrA, PrBA)
</code></pre>

<hr>
<h2 id='BigData'>Big Data</h2><span id='topic+BigData'></span>

<h3>Description</h3>

<p>This function enables Bayesian inference with data that is too large
for computer memory (RAM) with the simplest method: reading in batches
of data (where each batch is a section of rows), applying a function
to the batch, and combining the results.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>BigData(file, nrow, ncol, size=1, Method="add", CPUs=1, Type="PSOCK",
FUN, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="BigData_+3A_file">file</code></td>
<td>
<p>This required argument accepts a path and filename that
must refer to a .csv file, and that must contain only a numeric
matrix without a header, row names, or column names.</p>
</td></tr>
<tr><td><code id="BigData_+3A_nrow">nrow</code></td>
<td>
<p>This required argument accepts a scalar integer that
indicates the number of rows in the big data matrix.</p>
</td></tr>
<tr><td><code id="BigData_+3A_ncol">ncol</code></td>
<td>
<p>This required argument accepts a scalar integer that
indicates the number of columns in the big data matrix.</p>
</td></tr>
<tr><td><code id="BigData_+3A_size">size</code></td>
<td>
<p>This argument accepts a scalar integer that specifies the
number of rows of each batch. The last batch is not required to have
the same number of rows as the other batches. The largest possible
size, and therefore the fewest number of batches, should be
preferred.</p>
</td></tr>
<tr><td><code id="BigData_+3A_method">Method</code></td>
<td>
<p>This argument accepts a scalar string, defaults to
&quot;add&quot;, and alternatively accepts &quot;rbind&quot;. When
<code>Method="rbind"</code>, the user-specified function <code>FUN</code> is
applied to each batch, and results are combined together by rows.
For example, if calculating <code class="reqn">\mu = \textbf{X}\beta</code> in,
say, 10 batches, then the output column vector <code class="reqn">\mu</code> is equal
to the number of rows of the big data set.</p>
</td></tr>
<tr><td><code id="BigData_+3A_cpus">CPUs</code></td>
<td>
<p>This argument accepts an integer that specifies the number
of central processing units (CPUs) of the multicore computer or
computer cluster. This argument defaults to <code>CPUs=1</code>, in which
parallel processing does not occur.</p>
</td></tr>
<tr><td><code id="BigData_+3A_type">Type</code></td>
<td>
<p>This argument specifies the type of parallel processing to
perform, accepting either <code>Type="PSOCK"</code> or
<code>Type="MPI"</code>.</p>
</td></tr>
<tr><td><code id="BigData_+3A_fun">FUN</code></td>
<td>
<p>This required argument accepts a user-specified function
that will be performed on each batch. The first argument in the
function must be the data.</p>
</td></tr>
<tr><td><code id="BigData_+3A_...">...</code></td>
<td>
<p>Additional arguments are used within the user-specified
function. Additional arguments often refer to parameters.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Big data is defined loosely here as data that is too large for
computer memory (RAM). The <code>BigData</code> function uses the
split-apply-combine strategy with a big data set. The unmanageable
big data set is split into smaller, manageable pieces (batches),
a function is applied to each batch, and results are combined.
</p>
<p>Each iteration, the <code>BigData</code> function opens a connection to a
big data set and keeps the connection open while the <code>scan</code>
function reads in each batch of data (elsewhere, batches are often
referred to chunks). A user-specified function is applied to each
batch of data, the results are combined together, the connection is
closed, and the results are returned.
</p>
<p>As an introductory example, suppose a statistician updates a linear
regression model, but the design matrix <code class="reqn">\textbf{X}</code> is too
large for computer memory. Suppose the design matrix has 100 million
rows, and the statistician specifies <code>size=1e6</code>. The statistician
combines dependent variable <code class="reqn">\textbf{y}</code> with design matrix
<code class="reqn">\textbf{X}</code>. Each iteration in <code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code>,
<code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>, <code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>,
<code><a href="#topic+PMC">PMC</a></code>, or <code><a href="#topic+VariationalBayes">VariationalBayes</a></code>, the
<code>BigData</code> function sequentially reads in one million rows of the
combined data <code class="reqn">\textbf{X}</code>, calculates expectation vector
<code class="reqn">\mu</code>, and finally returns the sum of the log-likelihood. The sum
of the log-likelihood is added together for all batches, and returned.
</p>
<p>There are many limitations with this function.
</p>
<p>This function is not fast, in the sense that the entire big data set
is processed in batches, each iteration. With iterative methods, this
may perform well, albeit slowly.
</p>
<p>There are many functions that cannot be performed on batches, though
most models in the Examples vignette may easily be updated with big
data.
</p>
<p>Large matrices of samples are unaddressed, only the data.
</p>
<p>Although many (but not all) models may be estimated, many additional
functions in this package will not work when applied after the model
has updated. Instead, a batch or random sample of data (see the
<code><a href="#topic+read.matrix">read.matrix</a></code> function for sampling from big data) should
be used in the usual way, in the <code>Data</code> argument, and the
<code>Model</code> function coded in the usual way without the
<code>BigData</code> function.
</p>
<p>Parallel processing may be performed when the user specifies
<code>CPUs</code> to be greater than one, implying that the specified number
of CPUs exists and is available. Parallelization may be performed on a
multicore computer or a computer cluster. Either a Simple Network of
Workstations (SNOW) or Message Passing Interface (MPI) is used. Each
call to <code>BigData</code> establishes and closes the parallelization,
which is costly, and unfortunately results in copious output to the
console. With small data sets, parallel processing may be slower, due
to computer network communication. With larger data sets, the user
should experience a faster run-time.
</p>
<p>There have been several alternative approaches suggested for big data.
</p>
<p>Huang and Gelman (2005) propose that the user creates batches by
sampling from big data, updating a separate Bayesian model on each
batch, and combining the results into a consensus posterior. This
many-mini-model approach may be faster when feasible, because multiple
models may be updated in parallel, say one per CPU. Such results will
work with all functions in this package. With the many-mini-model
approach, several methods are proposed for combining posterior samples
from batch-level models, such as by using a normal approximation,
updating from prior to posterior sequentially (the posterior from the
last batch becomes the prior of the next batch), sample from the full
posterior via importance sampling from the batched posteriors, and
more.
</p>
<p>Scott et al. (2013) propose a method that they call Consensus Monte
Carlo, which consists of breaking the data down into chunks, calling
each chunk a shard, and use a many-mini-model approach as well, but
propose their own method of weighting the posteriors back together.
</p>
<p>Balakrishnan and Madigan (2006) introduced a Sequential Monte Carlo
(SMC) sampler, a refinement of an earlier proposal, that was designed
for big data. It makes one pass through the massive data set, after an
initial MCMC estimation on a small sample. Each particle is updated
for each record, resulting in numerous evaluations per record.
</p>
<p>Welling and Teh (2011) proposed a new class of MCMC sampler in which
only a random sample of big data is used each iteration. The
stochastic gradient Langevin dynamics (SGLD) algorithm is available
in the <code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code> function.
</p>
<p>An important alternative to consider is using the <code>ff</code> package,
where &quot;ff&quot; stands for fast access file. The <code>ff</code> package has been
tested successfully with updating a model in <code>LaplacesDemon</code>.
Once the big data set, say <code class="reqn">\textbf{X}</code>, is an object of
class <code>ff_matrix</code>, simply include it in the list of data as
usual, and modify the <code>Model</code> specification function
appropriately. For example, change <code>mu &lt;- tcrossprod(X, t(beta))</code>
to <code>mu &lt;- tcrossprod(X[], t(beta))</code>. The <code>ff</code> package is
not included as a dependency in the <code>LaplacesDemon</code> package, so
it must be installed and activated.
</p>


<h3>Value</h3>

<p>The <code>BigData</code> function returns output that is the result of
performing a user-specified function on batches of big data. Output is
a matrix, and may have one or more column vectors.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>References</h3>

<p>Balakrishnan, S. and Madigan, D. (2006). &quot;A One-Pass Sequential Monte
Carlo Method for Bayesian Analysis of Massive Datasets&quot;.
<em>Bayesian Analysis</em>, 1(2), p. 345&ndash;362.
</p>
<p>Huang, Z. and Gelman, A. (2005) &quot;Sampling for Bayesian Computation
with Large Datasets&quot;. <em>SSRN eLibrary</em>.
</p>
<p>Scott, S.L., Blocker, A.W. and Bonassi, F.V. (2013). &quot;Bayes and Big
Data: The Consensus Monte Carlo Algorithm&quot;. In <em>Bayes 250</em>.
</p>
<p>Welling, M. and Teh, Y.W. (2011). &quot;Bayesian Learning via Stochastic
Gradient Langevin Dynamics&quot;. <em>Proceedings of the 28th
International Conference on Machine Learning (ICML)</em>, p. 681&ndash;688.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code>,
<code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>,
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>,
<code><a href="#topic+LaplacesDemon.RAM">LaplacesDemon.RAM</a></code>,
<code><a href="#topic+PMC">PMC</a></code>,
<code><a href="#topic+PMC.RAM">PMC.RAM</a></code>,
<code><a href="#topic+read.matrix">read.matrix</a></code>, and
<code><a href="#topic+VariationalBayes">VariationalBayes</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### Below is an example of a linear regression model specification
### function in which BigData reads in a batch of 1,000 records of
### Data$N records from a data set that is too large to fully open
### in memory. The example simulates on 10,000 records, which is
### not big data; it's just a toy example. The data set is file X.csv,
### and the first column of matrix X is the dependent variable y. The
### user supplies a function to BigData along with parameters beta and
### sigma. When each batch of 1,000 records is read in,
### mu = XB is calculated, and then the LL is calculated as
### y ~ N(mu, sigma^2). These results are added together from all
### batches, and returned as LL.

library(LaplacesDemon)
N &lt;- 10000
J &lt;- 10 #Number of predictors, including the intercept
X &lt;- matrix(1,N,J)
for (j in 2:J) {X[,j] &lt;- rnorm(N,runif(1,-3,3),runif(1,0.1,1))}
beta.orig &lt;- runif(J,-3,3)
e &lt;- rnorm(N,0,0.1)
y &lt;- as.vector(tcrossprod(beta.orig, X) + e)
mon.names &lt;- c("LP","sigma")
parm.names &lt;- as.parm.names(list(beta=rep(0,J), log.sigma=0))
PGF &lt;- function(Data) return(c(rnormv(Data$J,0,0.01),
     log(rhalfcauchy(1,1))))
MyData &lt;- list(J=J, PGF=PGF, N=N, mon.names=mon.names,
     parm.names=parm.names) #Notice that X and y are not included here
filename &lt;- tempfile("X.csv")  
write.table(cbind(y,X), filename, sep=",", row.names=FALSE,
  col.names=FALSE)

Model &lt;- function(parm, Data)
     {
     ### Parameters
     beta &lt;- parm[1:Data$J]
     sigma &lt;- exp(parm[Data$J+1])
     ### Log(Prior Densities)
     beta.prior &lt;- sum(dnormv(beta, 0, 1000, log=TRUE))
     sigma.prior &lt;- dhalfcauchy(sigma, 25, log=TRUE)
     ### Log-Likelihood
     LL &lt;- BigData(file=filename, nrow=Data$N, ncol=Data$J+1, size=1000,
          Method="add", CPUs=1, Type="PSOCK",
          FUN=function(x, beta, sigma) sum(dnorm(x[,1], tcrossprod(x[,-1],
               t(beta)), sigma, log=TRUE)), beta, sigma)
     ### Log-Posterior
     LP &lt;- LL + beta.prior + sigma.prior
     Modelout &lt;- list(LP=LP, Dev=-2*LL, Monitor=c(LP,sigma),
          yhat=0,#rnorm(length(mu), mu, sigma),
          parm=parm)
     return(Modelout)
     }

### From here, the user may update the model as usual.
</code></pre>

<hr>
<h2 id='Blocks'>Blocks</h2><span id='topic+Blocks'></span>

<h3>Description</h3>

<p>The <code>Blocks</code> function returns a list of <code class="reqn">N</code> blocks of
parameters, for use with some MCMC algorithms in the
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code> function. Blocks may be created either
sequentially, or from a hierarchical clustering of the posterior
correlation matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Blocks(Initial.Values, N, PostCor=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Blocks_+3A_initial.values">Initial.Values</code></td>
<td>
<p>This required argument is a vector of initial
values.</p>
</td></tr>
<tr><td><code id="Blocks_+3A_n">N</code></td>
<td>
<p>This optional argument indicates the desired number of
blocks. If omitted, then the truncated square root of the number of
initial values is used. If a posterior correlation matrix is
supplied to <code>PostCor</code>, then <code>N</code> may be a scalar, or
have length two. If <code>N</code> has length two, then the first element
indicates the minimum number of blocks, and the second element
indicates the maximum number of blocks, and the number of blocks
is the maximum of the mean silhouette width for each hierarchical
cluster solution.</p>
</td></tr>
<tr><td><code id="Blocks_+3A_postcor">PostCor</code></td>
<td>
<p>This optional argument defaults to <code>NULL</code>, in
which case sequential blocking is performed. If a posterior
correlation matrix is supplied, then blocks are created based on
hierarchical clustering.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Usually, there is more than one target distribution in MCMC, in which
case it must be determined whether it is best to sample from target
distributions individually, in groups, or all at once. Blockwise
sampling (also called block updating) refers to splitting a
multivariate vector into groups called blocks, and each block is
sampled separately. A block may contain one or more parameters.
</p>
<p>Parameters are usually grouped into blocks such that parameters within
a block are as correlated as possible, and parameters between blocks
are as independent as possible. This strategy retains as much of the
parameter correlation as possible for blockwise sampling, as opposed
to componentwise sampling where parameter correlation is ignored.
The <code><a href="#topic+PosteriorChecks">PosteriorChecks</a></code> function can be used on the output
of previous runs to find highly correlated parameters. See examples
below.
</p>
<p>Advantages of blockwise sampling are that a different MCMC algorithm
may be used for each block (or parameter, for that matter), creating a
more specialized approach (though different algorithms by block are
not supported here), the acceptance of a newly proposed state is
likely to be higher than sampling from all target distributions at
once in high dimensions, and large proposal covariance matrices can
be reduced in size, which is most helpful again in high dimensions.
</p>
<p>Disadvantages of blockwise sampling are that correlations probably
exist between parameters between blocks, and each block is updated
while holding the other blocks constant, ignoring these correlations
of parameters between blocks. Without simultaneously taking
everything into account, the algorithm may converge slowly or never
arrive at the proper solution. However, there are instances when it
may be best when everything is not taken into account at once, such
as in state-space models. Also, as the number of blocks increases,
more computation is required, which slows the algorithm. In general,
blockwise sampling allows a more specialized approach at the expense
of accuracy, generalization, and speed. Blockwise sampling is offered
in the following algorithms: Adaptive-Mixture Metropolis (AMM),
Adaptive Metropolis-within-Gibbs (AMWG), Automated Factor Slice
Sampler (AFSS), Elliptical Slice Sampler (ESS), Hit-And-Run Metropolis
(HARM), Metropolis-within-Gibbs (MWG), Random-Walk Metropolis (RWM),
Robust Adaptive Metropolis (RAM), Slice Sampler (Slice), and the
Univariate Eigenvector Slice Sampler (UESS).
</p>
<p>Large-dimensional models often require blockwise sampling. For
example, with thousands of parameters, a componentwise algorithm
must evaluate the model specification function once per parameter per
iteration, resulting in an algorithm that may take longer than is
acceptable to produce samples. Algorithms that require derivatives,
such as the family of Hamiltonian Monte Carlo (HMC), require even more
evaluations of the model specification function per iteration, and
quickly become too costly in large dimensions. Finally, algorithms
with multivariate proposals often have difficulty producing an
accepted proposal in large-dimensional models. The most practical
solution is to group parameters into <code class="reqn">N</code> blocks, and each
iteration the algorithm evaluates the model specification function
<code class="reqn">N</code> times, each with a reduced set of parameters.
</p>
<p>The <code>Blocks</code> function performs either a sequential assignment of
parameters to blocks when posterior correlation is not supplied, or
uses hierarchical clustering to create blocks based on posterior
correlation. If posterior correlation is supplied, then the user may
specify a range of the number of blocks to consider, and the optimal
number of blocks is considered to be the maximum of the mean
silhouette width of each hierarchical clustering. Silhouette width
is calculated as per the <code>cluster</code> package. Hierarchical
clustering is performed on the distance matrix calculated from the
dissimilarity matrix (1 - abs(PostCor)) of the posterior correlation
matrix. With sequential assignment, the number of parameters per
block is approximately equal. With hierarchical clustering, the
number of parameters per block may vary widely. Creating blocks from
hierarchical clustering performs well in practice, though there are
many alternative methods the user may consider outside of this
function, such as using factor analysis, model-based clustering, or
other methods.
</p>
<p>Aside from sequentially-assigned blocks, or blocks based on posterior
correlation, it is also common to group parameters with similar uses,
such as putting regression effects parameters into one block, and
autocorrelation parameters into another block. Another popular way to
group parameters into blocks is by time-period for some time-series
models. These alternative blocking strategies are unsupported in the
<code>Blocks</code> function, and best left to user discretion.
</p>
<p>Some MCMC algorithms that accept blocked parameters also require
blocked variance-covariance matrices. The <code>Blocks</code> function
does not return these matrices, because it may not be necessary,
or when it is, the user may prefer identity matrices, scaled identity
matrices, or matrices with explicitly-defined elements.
</p>
<p>If the user is looking for a place to begin with blockwise sampling,
then the recommended, default approach (when blocked parameters by
time-period are not desired in a time-series) is to begin with a
trial run of the adaptive, unblocked HARM algorithm (since covariance
matrices are not required) for the purposes of obtaining a posterior
correlation matrix. Next, create blocks with the <code>Blocks</code>
function based on the posterior correlation matrix obtained from the
trial run. Finally, run the desired, blocked algorithm with the newly
created blocks (and possibly user-specified covariance matrices),
beginning where the trial run ended.
</p>
<p>If hierarchical clustering is used, then it is important to note that
hierarchical clustering has no idea that the user intends to perform
blockwise sampling in MCMC. If hierarchical clustering returns
numerous small blocks, then the user may consider combining some or
all of those blocks. For example, if several 1-parameter blocks are
returned, then blockwise sampling will equal componentwise sampling
for those blocks, which will iterate slower. Conversely, if
hierarchical clustering returns one or more big blocks, each with
enough parameters that multivariate sampling will have difficulty
getting an accepted proposal, or an accepted proposal that moves
more than a small amount, then the user may consider subdividing
these big blocks into smaller, more manageable blocks, though with
the understanding that more posterior correlation is unaccounted for.
</p>


<h3>Value</h3>

<p>The <code>Blocks</code> function returns an object of class <code>blocks</code>,
which is a list. Each component of the list is a block of parameters,
and parameters are indicated by their position in the initial values
vector.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code> and
<code><a href="#topic+PosteriorChecks">PosteriorChecks</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)

### Create the default number of sequentially assigned blocks:
Initial.Values &lt;- rep(0,1000)
MyBlocks &lt;- Blocks(Initial.Values)
MyBlocks

### Or, a pre-specified number of sequentially assigned blocks:
#Initial.Values &lt;- rep(0,1000)
#MyBlocks &lt;- Blocks(Initial.Values, N=20)

### If scaled diagonal covariance matrices are desired:
#VarCov &lt;- list()
#for (i in 1:length(MyBlocks))
#  VarCov[[i]] &lt;- diag(length(MyBlocks[[i]]))*2.38^2/length(MyBlocks[[i]])

### Or, determine the number of blocks in the range of 2 to 50 from
### hierarchical clustering on the posterior correlation matrix of an
### object, say called Fit, output from LaplacesDemon:
#MyBlocks &lt;- Blocks(Initial.Values, N=c(2,50),
#  PostCor=cor(Fit$Posterior1))
#lapply(MyBlocks, length) #See the number of parameters per block

### Or, create a pre-specified number of blocks from hierarchical
### clustering on the posterior correlation matrix of an object,
### say called Fit, output from LaplacesDemon:
#MyBlocks &lt;- Blocks(Initial.Values, N=20, PostCor=cor(Fit$Posterior1))

### Posterior correlation from a previous trial run could be obtained
### with either method below (though cor() will be fastest because
### additional checks are not calculated for the parameters):
#rho &lt;- cor(Fit$Posterior1)
#rho &lt;- PosteriorChecks(Fit)$Posterior.Correlation
</code></pre>

<hr>
<h2 id='BMK.Diagnostic'>BMK Convergence Diagnostic</h2><span id='topic+BMK.Diagnostic'></span>

<h3>Description</h3>

<p>Given a matrix of posterior samples from MCMC, the
<code>BMK.Diagnostic</code> function calculates Hellinger distances between
consecutive batches for each chain. This is useful for monitoring
convergence of MCMC chains.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>BMK.Diagnostic(X, batches=10)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="BMK.Diagnostic_+3A_x">X</code></td>
<td>
<p>This required argument accepts a matrix of posterior
samples or an object of class <code>demonoid</code>, in which case it uses
the posterior samples in <code>X$Posterior1</code>.</p>
</td></tr>
<tr><td><code id="BMK.Diagnostic_+3A_batches">batches</code></td>
<td>
<p>This is the number of batches on which the convergence
diagnostic will be calculated. The <code>batches</code> argument defaults
to 10.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Hellinger distance is used to quantify dissimilarity between two
probability distributions. It is based on the Hellinger integral,
introduced by Hellinger (1909). Traditionally, Hellinger distance is
bound to the interval [0,1], though another popular form occurs in
the interval [0,<code class="reqn">\sqrt{2}</code>]. A higher value of Hellinger
distance is associated with more dissimilarity between the
distributions.
</p>
<p>Convergence is assumed when Hellinger distances are below a threshold,
indicating that posterior samples are similar between consecutive
batches. If all Hellinger distances beyond a given batch of samples is
below the threshold, then <code>burnin</code> is suggested to occur
immediately before the first batch of satisfactory Hellinger
distances.
</p>
<p>As an aid to interpretation, consider a matrix of 1,000 posterior
samples from three chains: <code>beta[1]</code>, <code>beta[2]</code>, and
<code>beta[3]</code>. With 10 batches, the column names are: 100, 200,
..., 900. A Hellinger distance for the chain <code>beta[1]</code> at 100
is the Hellinger distance between two batches: samples 1-100, and
samples 101:200.
</p>
<p>A benefit to using <code>BMK.Diagnostic</code> is that the resulting
Hellinger distances may easily be plotted with the <code>plotMatrix</code>
function, allowing the user to see quickly which consecutive batches
of which chains were dissimilar. This makes it easier to find
problematic chains.
</p>
<p>The <code>BMK.Diagnostic</code> is calculated automatically in the
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code> function, and is one of the criteria in
the <code><a href="#topic+Consort">Consort</a></code> function regarding the recommendation of
when to stop updating the Markov chain Monte Carlo (MCMC) sampler in
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>.
</p>
<p>For more information on the related topics of burn-in and
stationarity, see the <code>burnin</code> and <code><a href="#topic+is.stationary">is.stationary</a></code>
functions, and the accompanying vignettes.
</p>


<h3>Value</h3>

<p>The <code>BMK.Diagnostic</code> function returns an object of class
<code>bmk</code> that is a <code class="reqn">J \times B</code> matrix of Hellinger
distances between consecutive batches for <code class="reqn">J</code> parameters of
posterior samples. The number of columns, <code class="reqn">B</code> is equal to the
number of batches minus one.
</p>
<p>The <code>BMK.Diagnostic</code> function is similar to the
<code>bmkconverge</code> function in package BMK.
</p>


<h3>References</h3>

<p>Boone, E.L., Merrick, J.R. and Krachey, M.J. (2013). &quot;A Hellinger
Distance Approach to MCMC Diagnostics&quot;. <em>Journal of Statistical
Computation and Simulation</em>, in press.
</p>
<p>Hellinger, E. (1909). &quot;Neue Begrundung der Theorie quadratischer
Formen von unendlichvielen Veranderlichen&quot; (in German). <em>Journal
fur die reine und angewandte Mathematik</em>, 136, p. 210&ndash;271.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+burnin">burnin</a></code>,
<code><a href="#topic+Consort">Consort</a></code>,
<code><a href="#topic+is.stationary">is.stationary</a></code>, and 
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
N &lt;- 1000 #Number of posterior samples
J &lt;- 10 #Number of parameters
Theta &lt;- matrix(runif(N*J),N,J)
colnames(Theta) &lt;- paste("beta[", 1:J, "]", sep="")
for (i in 2:N) {Theta[i,1] &lt;- Theta[i-1,1] + rnorm(1)}
HD &lt;- BMK.Diagnostic(Theta, batches=10)
plot(HD, title="Hellinger distance between batches")
</code></pre>

<hr>
<h2 id='burnin'>Burn-in</h2><span id='topic+burnin'></span>

<h3>Description</h3>

<p>The <code>burnin</code> function estimates the duration of burn-in in
iterations for one or more Markov chains. &ldquo;Burn-in&rdquo; refers to the
initial portion of a Markov chain that is not stationary and is still
affected by its initial value.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>burnin(x, method="BMK")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="burnin_+3A_x">x</code></td>
<td>
<p>This is a vector or matrix of posterior samples for which a
the number of burn-in iterations will be estimated.</p>
</td></tr>
<tr><td><code id="burnin_+3A_method">method</code></td>
<td>
<p>This argument defaults to <code>"BMK"</code>, in which case
stationarity is estimated with the <code><a href="#topic+BMK.Diagnostic">BMK.Diagnostic</a></code>
function. Alternatively, the <code><a href="#topic+Geweke.Diagnostic">Geweke.Diagnostic</a></code>
function may be used when <code>method="Geweke"</code> or the
<code><a href="#topic+KS.Diagnostic">KS.Diagnostic</a></code> function may be used when
<code>method="KS"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Burn-in is a colloquial term for the initial iterations in a Markov
chain prior to its convergence to the target distribution. During
burn-in, the chain is not considered to have &ldquo;forgotten&rdquo; its initial
value.
</p>
<p>Burn-in is not a theoretical part of MCMC, but its use is the norm
because of the need to limit the number of posterior samples due to
computer memory. If burn-in were retained rather than discarded, then
more posterior samples would have to be retained. If a Markov chain
starts anywhere close to the center of its target distribution, then
burn-in iterations do not need to be discarded.
</p>
<p>In the <code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code> function, stationarity is estimated
with the <code><a href="#topic+BMK.Diagnostic">BMK.Diagnostic</a></code> function on all thinned
posterior samples of each chain, beginning at cumulative 10% intervals
relative to the total number of samples, and the lowest number in
which all chains are stationary is considered the burn-in.
</p>
<p>The term, &ldquo;burn-in&rdquo;, originated in electronics regarding the initial
testing of component failure at the factory to eliminate initial
failures (Geyer, 2011). Although &ldquo;burn-in' has been the standard term
for decades, some are referring to these as &ldquo;warm-up&rdquo; iterations.
</p>


<h3>Value</h3>

<p>The <code>burnin</code> function returns a vector equal in length to the
number of MCMC chains in <code>x</code>, and each element indicates the
maximum iteration in burn-in.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>References</h3>

<p>Geyer, C.J. (2011). &quot;Introduction to Markov Chain Monte Carlo&quot;. In
S Brooks, A Gelman, G Jones, and M Xiao-Li (eds.), &quot;Handbook of
Markov Chain Monte Carlo&quot;, p. 3&ndash;48. Chapman and Hall, Boca Raton, FL.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+BMK.Diagnostic">BMK.Diagnostic</a></code>,
<code><a href="#topic+deburn">deburn</a></code>,
<code><a href="#topic+Geweke.Diagnostic">Geweke.Diagnostic</a></code>,
<code><a href="#topic+KS.Diagnostic">KS.Diagnostic</a></code>, and
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
x &lt;- rnorm(1000)
burnin(x)
</code></pre>

<hr>
<h2 id='caterpillar.plot'>Caterpillar Plot</h2><span id='topic+caterpillar.plot'></span>

<h3>Description</h3>

<p>A caterpillar plot is a horizontal plot of 3 quantiles of selected
distributions. This may be used to produce a caterpillar plot of
posterior samples (parameters and monitored variables) from an object
either of class <code>demonoid</code>, <code>demonoid.hpc</code>, <code>iterquad</code>,
<code>laplace</code>, <code>pmc</code>, <code>vb</code>, or a matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>caterpillar.plot(x, Parms=NULL, Title=NULL)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="caterpillar.plot_+3A_x">x</code></td>
<td>

<p>This required argument is an object of class <code>demonoid</code>,
codedemonoid.hpc, <code>iterquad</code>, <code>laplace</code>, <code>pmc</code>,
<code>vb</code>, or a <code class="reqn">S \times J</code> matrix of <code class="reqn">S</code> samples
and <code class="reqn">J</code> variables. For an object of class <code>demonoid</code>, the
distributions of the stationary posterior summary (<code>Summary2</code>)
will be attempted first, and if missing, then the parameters of all
posterior samples (<code>Summary1</code>) will be plotted. For an object
of class <code>demonoid.hpc</code>, stationarity may differ by chain, so
all posterior samples (<code>Summary1</code>) are used. For an object of
class <code>laplace</code> or <code>vb</code>, the distributions in the
posterior summary, <code>Summary</code>, are plotted according to the
posterior draws, sampled with sampling importance resampling in the
<code><a href="#topic+SIR">SIR</a></code> function. When a generic matrix is supplied,
unimodal 95% HPD intervals are estimated with the
<code><a href="#topic+p.interval">p.interval</a></code> function.</p>
</td></tr>
<tr><td><code id="caterpillar.plot_+3A_parms">Parms</code></td>
<td>

<p>This argument accepts a vector of quoted strings to be matched for
selecting parameters and monitored variables for plotting (though
all parameters are selected when a generic matrix is supplied). This
argument defaults to <code>NULL</code> and selects every parameter for
plotting. Each quoted string is matched to one or more parameter
names with the <code>grep</code> function. For example, if the user specifies
<code>Parms=c("eta", "tau")</code>, and if the parameter names are
beta[1], beta[2], eta[1], eta[2], and tau, then all parameters will
be selected, because the string <code>eta</code> is within <code>beta</code>.
Since <code>grep</code> is used, string matching uses regular
expressions, so beware of meta-characters, though these are
acceptable: &quot;.&quot;, &quot;[&quot;, and &quot;]&quot;.</p>
</td></tr>
<tr><td><code id="caterpillar.plot_+3A_title">Title</code></td>
<td>

<p>This argument accepts a title for the plot.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Caterpillar plots are popular plots in Bayesian inference for
summarizing the quantiles of posterior samples. A caterpillar plot is
similar to a horizontal boxplot, though without quartiles, making it
easier for the user to study more distributions in a single plot. The
following quantiles are plotted as a line for each parameter: 0.025 and
0.975, with the exception of a generic matrix, where unimodal 95% HPD
intervals are estimated (for more information, see
<code><a href="#topic+p.interval">p.interval</a></code>). A vertical, gray line is included at zero.
For all but class <code>demonoid.hpc</code>, the median appears as a black
dot, and the quantile line is black. For class <code>demonoid.hpc</code>, the
color of the median and quantile line differs by chain; the first
chain is black and additional chains appear beneath.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code>,
<code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>,
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>,
<code><a href="#topic+LaplacesDemon.hpc">LaplacesDemon.hpc</a></code>,
<code><a href="#topic+PMC">PMC</a></code>,
<code><a href="#topic+p.interval">p.interval</a></code>,
<code><a href="#topic+SIR">SIR</a></code>, and
<code><a href="#topic+VariationalBayes">VariationalBayes</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#An example is provided in the LaplacesDemon function.</code></pre>

<hr>
<h2 id='CenterScale'>Centering and Scaling</h2><span id='topic+CenterScale'></span>

<h3>Description</h3>

<p>This function either centers and scales a continuous variable and
provides options for binary variables, or returns an untransformed
variable from a centered and scaled variable.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CenterScale(x, Binary="none", Inverse=FALSE, mu, sigma, Range, Min)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CenterScale_+3A_x">x</code></td>
<td>
<p>This is a vector to be centered and scaled, or to be
untransformed if <code>Inverse=TRUE</code>.</p>
</td></tr>
<tr><td><code id="CenterScale_+3A_binary">Binary</code></td>
<td>
<p>This argument indicates how binary variables will be
treated, and defaults to <code>"none"</code>, which keeps the original
scale, or transforms the variable to the 0-1 range, if not already
there. With <code>"center"</code>, it will center the binary variable by
subtracting the mean. With <code>"center0"</code>, it centers the binary
variable at zero, recoding a 0 to -0.5, and a 1 to 0.5. Finally,
<code>"centerscale"</code> will center and scale the binary variable,
subtracting the mean and dividing by two standard deviations.</p>
</td></tr>
<tr><td><code id="CenterScale_+3A_inverse">Inverse</code></td>
<td>
<p>Logical. If <code>TRUE</code>, then a centered and scaled
variable <code>x</code> will be transformed to its original, un-centered
and un-scaled state. This defaults to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="CenterScale_+3A_mu">mu</code>, <code id="CenterScale_+3A_sigma">sigma</code>, <code id="CenterScale_+3A_range">Range</code>, <code id="CenterScale_+3A_min">Min</code></td>
<td>
<p>These arguments are required only when
<code>Inverse=TRUE</code>, where <code>mu</code> is the mean, <code>sigma</code> is
the standard deviation, <code>Range</code> is the range, and <code>Min</code> is
the minimum of the original <code>x</code>. <code>Range</code> and <code>Min</code>
are used only when <code>Binary="none"</code> or <code>Binary="center0"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Gelman (2008) recommends centering and scaling continuous predictors
to facilitate MCMC convergence and enable comparisons between
coefficients of centered and scaled continuous predictors with
coefficients of untransformed binary predictors. A continuous
predictor is centered and scaled as follows: <code>x.cs &lt;- (x -
    mean(x)) / (2*sd(x))</code>. This is an improvement over the usual
practice of standardizing predictors, which is <code>x.z &lt;- (x -
    mean(x)) / sd(x)</code>, where coefficients cannot be validly compared
between binary and continuous predictors.
</p>
<p>In MCMC, such as in <code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>, a centered and
scaled predictor often results in a higher effective sample size
(<code><a href="#topic+ESS">ESS</a></code>), and therefore the chain mixes better. Centering
and scaling is a method of re-parameterization to improve mixing.
</p>
<p>Griffin and Brown (2013) also assert that the user may not want to
scale predictors that are measured on the same scale, since scaling
in this case may increase noisy, low signals. In this case, centering
(without scaling) is recommended. To center a predictor, subtract its
mean.
</p>


<h3>Value</h3>

<p>The <code>CenterScale</code> function returns a centered and scaled vector,
or the untransformed vector.
</p>


<h3>References</h3>

<p>Gelman, A. (2008). &quot;Scaling Regression Inputs by Dividing by Two Standard
Devations&quot;. <em>Statistics in Medicine</em>, 27, p. 2865&ndash;2873.
</p>
<p>Griffin, J.E. and Brown, P.J. (2013) &quot;Some Priors for Sparse
Regression Modelling&quot;. <em>Bayesian Analysis</em>, 8(3), p. 691&ndash;702.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ESS">ESS</a></code>,
<code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code>,
<code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>,
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>, and
<code><a href="#topic+PMC">PMC</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### See the LaplacesDemon function for an example in use.
library(LaplacesDemon)
x &lt;- rnorm(100,10,1)
x.cs &lt;- CenterScale(x)
x.orig &lt;- CenterScale(x.cs, Inverse=TRUE, mu=mean(x), sigma=sd(x))
</code></pre>

<hr>
<h2 id='Combine'>Combine Demonoid Objects</h2><span id='topic+Combine'></span>

<h3>Description</h3>

<p>This function combines objects of class <code>demonoid</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Combine(x, Data, Thinning=1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Combine_+3A_x">x</code></td>
<td>
<p>This is a list of objects of class <code>demonoid</code>, and this
list may be an object of class <code>demonoid.hpc</code>.</p>
</td></tr>
<tr><td><code id="Combine_+3A_data">Data</code></td>
<td>
<p>This is the data, and must be identical to the data used
to create the <code>demonoid</code> objects with
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>.</p>
</td></tr>
<tr><td><code id="Combine_+3A_thinning">Thinning</code></td>
<td>
<p>This is the amount of thinning to apply to the
posterior samples after appending them together. <code>Thinning</code>
defaults to 1, in which case all samples are retained. For example,
in the case of, say, <code>Thinning=10</code>, then only every 10th sample
would be retained. When combining parallel chains, <code>Thinning</code>
is often left to its default. When combining consecutive updates,
<code>Thinning</code> is usually applied, with the value equal to the
number of objects of class <code>demonoid</code>. For more information on
thinning, see the <code>Thin</code> function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The purpose of the <code>Combine</code> function is to enable a user to
combine objects of class <code>demonoid</code> for one of three
reasons. First, parallel chains from <code><a href="#topic+LaplacesDemon.hpc">LaplacesDemon.hpc</a></code>
may be combined after convergence is assessed with
<code><a href="#topic+Gelman.Diagnostic">Gelman.Diagnostic</a></code>. Second, consecutive updates of single
chains from <code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code> or parallel chains from
<code><a href="#topic+LaplacesDemon.hpc">LaplacesDemon.hpc</a></code> may be combined when the computer has
insufficient random-access memory (RAM) for the user to update once 
with enough iterations. Third, consecutive single-chain or
parallel-chain updates may be combined when it seems that the
logarithm of the joint posterior distribution, <code>LP</code>, seems to be
oscillating up and down, which is described in more detail below.
</p>
<p>The most common use regards the combination of parallel chains output
from <code><a href="#topic+LaplacesDemon.hpc">LaplacesDemon.hpc</a></code>. Typically, a user with parallel
chains examines them graphically with the
<code><a href="#topic+caterpillar.plot">caterpillar.plot</a></code> and <code>plot</code> (actually,
<code><a href="#topic+plot.demonoid">plot.demonoid</a></code>) functions, and assesses convergence
with the <code><a href="#topic+Gelman.Diagnostic">Gelman.Diagnostic</a></code> function. Thereafter, the
parallel chain output in the object of class <code>demonoid.hpc</code>
should be combined into a single object of class <code>demonoid</code>,
before doing posterior predictive checks and making inferences. In
this case, the <code>Thinning</code> argument usually is recommended to
remain at its default.
</p>
<p>It is also common with a high-dimensional model (a model with a large
number of parameters) to need more posterior samples than allowed by
the random-access memory (RAM) of the computer. In this case, it is
best to use the <code><a href="#topic+LaplacesDemon.RAM">LaplacesDemon.RAM</a></code> function to estimate
the amount of RAM that a given model will require with a given number
of iterations, and then update <code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code> almost as
much as RAM allows, and save the output object of class
<code>demonoid</code>. Then, the user is advised to continue onward with a
consecutive update (after using <code><a href="#topic+as.initial.values">as.initial.values</a></code> and
anything else appropriate to prepare for the consecutive
update). Suppose a user desires to update a gigantic model with
thousands of parameters, and with the aid of
<code><a href="#topic+LaplacesDemon.RAM">LaplacesDemon.RAM</a></code>, estimates that they can safely update
only 100,000 iterations, and that 150,000 iterations would exceed RAM
and crash the computer. The patient user can update several
consecutive models, each with retaining only 1,000 thinned posterior
samples, and combine them later with the <code>Combine</code> function, by
placing multiple objects into a list, as described below. In this way,
it is possible for a user to update models that otherwise far exceed
computer RAM.
</p>
<p>Less commonly, multiple updates of single-chain objects should be
combined into a single object of class <code>demonoid</code>. This is most
useful in complicated models that are run for large numbers of
iterations, where it may be suspected that stationarity has been
achieved, but that thinning is insufficient, and the samples may be
combined and thinned. If followed, then these suggestions may continue
seemingly to infinity, and the unnormalized logarithm of the joint
posterior density, <code>LP</code>, may seem to oscillate, sometimes
improving and getting higher, and getting lower during other updates.
For this purpose, the prior covariance matrix of the last model is
retained (rather than combining them). This may be an unpleasant
surprise for combining parallel updates, so be aware of it.
</p>
<p>In these cases, which usually involve complicated models with high
autocorrelation in the chains, the user may opt to use parallel
processing with the <code><a href="#topic+LaplacesDemon.hpc">LaplacesDemon.hpc</a></code> function, or may
use the <code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code> function as follows. The user
should save (meaning, not overwrite) each object of class
<code>demonoid</code>, place multiple objects into a list, and use the
<code>Combine</code> function to combine these objects.
</p>
<p>For example, suppose a user names the object Fit, as in the
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code> example. Now, rather than overwriting
object Fit, object Fit is renamed, after updating a million
iterations, to Fit1. As suggested by <code><a href="#topic+Consort">Consort</a></code>, another
million iterations are used, but now to create object Fit2. Further
suppose this user specified <code>Thinning=1000</code> in
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>, meaning that the million iterations are
thinned by 1,000, so only 1,000 iterations are retained in each
object, Fit1 and Fit2. In this case, <code>Combine</code> combines the
information in Fit1 and Fit2, and returns an object the user names
Fit3. Fit3 has only 1,000 iterations, which is the result of appending
the iterations in Fit1 and Fit2, and thinning by 2. If 2,000,000
iterations were updated from the beginning, and were thinned by 2,000,
then the same information exists now in Fit3. The <code><a href="#topic+Consort">Consort</a></code>
function can now be applied to Fit3, to see if stationarity is found.
If not, then more objects of class <code>demonoid</code> can be collected and
combined.
</p>


<h3>Value</h3>

<p>This function returns an object of class <code>demonoid</code>. For more
information on an object of class <code>demonoid</code>, see the
<code>LaplacesDemon</code> function.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+caterpillar.plot">caterpillar.plot</a></code>,
<code><a href="#topic+Gelman.Diagnostic">Gelman.Diagnostic</a></code>,
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>, 
<code><a href="#topic+LaplacesDemon.hpc">LaplacesDemon.hpc</a></code>, and
<code><a href="#topic+Thin">Thin</a></code>.</p>

<hr>
<h2 id='cond.plot'>Conditional Plots</h2><span id='topic+cond.plot'></span>

<h3>Description</h3>

<p>This function provides several styles of conditional plots with base
graphics.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cond.plot(x, y, z, Style="smoothscatter")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cond.plot_+3A_x">x</code></td>
<td>
<p>This required argument accepts a numeric vector.</p>
</td></tr>
<tr><td><code id="cond.plot_+3A_y">y</code></td>
<td>
<p>This argument accepts a numeric vector, and is only used with
some styles.</p>
</td></tr>
<tr><td><code id="cond.plot_+3A_z">z</code></td>
<td>
<p>This required argument accepts a discrete vector.</p>
</td></tr>
<tr><td><code id="cond.plot_+3A_style">Style</code></td>
<td>
<p>This argument specifies the style of plot, and accepts
&quot;boxplot&quot;, &quot;densover&quot; (density overlay), &quot;hist&quot;, &quot;scatter&quot;, or
&quot;smoothscatter&quot;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>cond.plot</code> function provides simple conditional plots with
base graphics.  All plot styles are conditional upon <code>z</code>. Up to
nine conditional plots are produced in a panel.
</p>
<p>Plots include:
</p>
<p>boxplot: y ~ x | z
densover: f(x | z)
hist: x | z
scatter: x, y | z
smoothscatter: x, y | z
</p>
<p>The <code>cond.plot</code> function is not intended to try to compete with
some of the better graphics packages, but merely to provide simple
functionality.
</p>


<h3>Value</h3>

<p>Conditional plots are returned.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+joint.density.plot">joint.density.plot</a></code> and
<code><a href="#topic+joint.pr.plot">joint.pr.plot</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
x &lt;- rnorm(1000)
y &lt;- runif(1000)
z &lt;- rcat(1000, rep(1/4,4))
cond.plot(x, y, z, Style="smoothscatter")
</code></pre>

<hr>
<h2 id='Consort'>Consort with Laplace's Demon</h2><span id='topic+Consort'></span>

<h3>Description</h3>

<p>This may be used to consort with Laplace's Demon regarding an object
of class <code>demonoid</code>. Laplace's Demon will offer suggestions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Consort(object)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Consort_+3A_object">object</code></td>
<td>
<p>This required argument is an object of class
<code>demonoid</code>. For more information, see the
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code> function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>First, <code>Consort</code> calls <code>print.demonoid</code>, which prints most
of the components to the screen from the supplied object of class
<code>demonoid</code>.
</p>
<p>Second, Laplace's Demon considers a combination of five conditions
when making the largest part of its suggestion. These conditions are:
the algorithm, acceptance rate, MCSE, ESS, and stationarity. Other
things are considered as well, such as the recommended thinning value
is used to suggest a new number of iterations, how fast the algorithm
is expected to be, and if the condition of diminishing adaptation
(also called the vanishing adaptation condition) was met (for an
adaptive algorithm). Diminishing adaptation occurs only when the
absolute value of the proposed variances trends downward (toward zero)
over the course of all adaptations. When an algorithm is adaptive and
it does not have diminishing adaptations, the <code>Consort</code> function
will suggest a different adaptive algorithm. The <code>Periodicity</code>
argument is suggested to be set equal to the value of
<code>Rec.Thinning</code>.
</p>
<p>Appeasement applies only when all parameters are continuous.The
<code><a href="#topic+Hangartner.Diagnostic">Hangartner.Diagnostic</a></code> should be considered for discrete
parameters.
</p>
<p>Appeasement Conditions
</p>

<ul>
<li><p> Algorithm: The final algorithm must be non-adaptive, so that
the Markov property holds. This is conservative. A user may have
an adaptive (non-final) algorithm in which adaptations in the latest
update are stationary, or no longer diminishing. Laplace's Demon is
unaware of previous updates, and conservatively interprets this as
failing to meet the condition of diminishing adaptation, when the
output may be satisfactory. On the other hand, if the adaptive
algorithm has essentially stopped adapting, and if there is a
non-adaptive version, then the user should consider switching to
the non-adaptive algorithm. User discretion is advised.
</p>
</li>
<li><p> Acceptance Rate: The acceptance rate is considered
satisfactory if it is within the interval [15%,50%] for most
algorithms. Some algorithms have different recommended intervals.
</p>
</li>
<li><p> MCSE: The Monte Carlo Standard Error (MCSE) is considered
satisfactory for each target distribution if it is less than 6.27%
of the standard deviation of the target distribution. This allows
the true mean to be within 5% of the area under a Gaussian
distribution around the estimated mean. The <code><a href="#topic+MCSE">MCSE</a></code>
function is used. Toft et al. (2007) propose a stricter criterion of
5%. The criterion of 6.27% for this stopping rule is arbitrary,
and may be too lenient or strict, depending on the needs of the
user. Nonetheless, it has performed well, and this type of stopping
rule has been observed to perform better than MCMC convergence
diagnostics (Flegal et al., 2008).
</p>
</li>
<li><p> ESS: The effective sample size (ESS) is considered
satisfactory for each target distribution if it is at least 100,
which is usually enough to describe 95% probability intervals (see
<code><a href="#topic+p.interval">p.interval</a></code> and <code><a href="#topic+LPL.interval">LPL.interval</a></code> for more
information). The <code><a href="#topic+ESS">ESS</a></code> function is used. When this
criterion is unmet, the name of the worst mixing chain in Summary1
appears.
</p>
</li>
<li><p> Stationarity: Each target distribution is considered
satisfactory if it is estimated to be stationary with the
<code><a href="#topic+BMK.Diagnostic">BMK.Diagnostic</a></code> function.
</p>
</li></ul>

<p>Bear in mind that the MCSE, ESS, and stationarity criteria are all
univariate measures applied to each marginal posterior
distribution. Multivariate forms are not included. By chance alone due
to multiple independent tests, 5% of these diagnostics should
indicate non-convergence when 'convergence' exists. In contrast, even
one non-convergent nuisance parameter is associated with
non-convergence in all other parameters. Assessing convergence is
difficult.
</p>
<p>If all five conditions are satisfactory, then Laplace's Demon is
appeased. Otherwise, Laplace's Demon will suggest and supply R
code that is ready to be copy/pasted and executed.
</p>
<p>To visualize the MCSE-based stopping rule, run the following code:
</p>
<p><code>x &lt;- seq(from=-3, to=3, by=0.1);</code>
<code>plot(x, dnorm(x,0,1), type="l");</code>
<code>abline(v=-0.0627); abline(v=0.0627);</code>
<code>abline(v=2*-0.0627, col="red"); abline(v=2*0.0627, col="red")</code>
</p>
<p>The black vertical lines show the standard error, and the red vertical
lines show the 95% interval.
</p>
<p>If the user has an object of class <code>demonoid.hpc</code>, then the
<code>Consort</code> function may be still be applied, but a particular
chain in the object must be specified as a component in a list. For
example, with an object called <code>Fit</code> and a goal of consorting
over the second chain, the code would be: <code>Consort(Fit[[2]])</code>.
</p>
<p>The Demonic Suggestion is usually very helpful, but should not be
followed blindly. Do not let it replace critical thinking. For
example, <code>Consort</code> may find that diminishing adaptation is unmet,
and recommend a different algorithm. However, the user may be
convinced that the current algorithm is best, and believe instead that
MCMC found a local solution, and is leaving it to find the global
solution, in which case adaptations may increase again. Diminishing
adaptation may have occurred in a previous run, and is not found in
the current run because adaptation is essentially finished. If either
of these is true, then it may be best to ignore the newly suggested
algorithm, and continue with the current algorithm. The suggested code
may be helpful, but it is merely a suggestion.
</p>
<p>If achieving the appeasement of Laplace's Demon is difficult, consider
ignoring the MCSE criterion and terminate when all other criteria have
been met, placing special emphasis on ESS.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>References</h3>

<p>Flegal, J.M., Haran, M., and Jones, G.L. (2008). &quot;Markov chain Monte
Carlo: Can We Trust the Third Significant Figure?&quot;. <em>Statistical
Science</em>, 23, p. 250&ndash;260.
</p>
<p>Toft, N., Innocent, G., Gettinby, G., and Reid, S. (2007). &quot;Assessing
the Convergence of Markov Chain Monte Carlo Methods: An Example from
Evaluation of Diagnostic Tests in Absence of a Gold Standard&quot;.
<em>Preventive Veterinary Medicine</em>, 79, p. 244&ndash;256.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+BMK.Diagnostic">BMK.Diagnostic</a></code>,
<code><a href="#topic+ESS">ESS</a></code>,
<code><a href="#topic+Hangartner.Diagnostic">Hangartner.Diagnostic</a></code>,
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>,
<code><a href="#topic+LaplacesDemon.hpc">LaplacesDemon.hpc</a></code>,
<code><a href="#topic+LPL.interval">LPL.interval</a></code>,
<code><a href="#topic+MCSE">MCSE</a></code>, and
<code><a href="#topic+p.interval">p.interval</a></code>.
</p>

<hr>
<h2 id='CSF'>Cumulative Sample Function</h2><span id='topic+CSF'></span>

<h3>Description</h3>

<p>The Cumulative Sample Function (CSF) is a visual MCMC diagnostic in
which the user may select a measure (such as a variable, summary
statistic, or other diagnostic), and observe a plot of how the measure
changes over cumulative posterior samples from MCMC, such as the
output of <code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>. This may be considered to be a
generalized extension of the <code>cumuplot</code> in the coda package,
which is a more restrictive form of the cusum diagnostic introduced by
Yu and Myckland (1998).
</p>
<p>Yu and Myckland (1998) suggest that CSF plots should be examined after
traditional trace plots seem convergent, and assert that faster mixing
chains (which are more desirable) result in CSF plots that are more
&lsquo;hairy&rsquo; (as opposed to smooth), though this is subjective and has been
debated. The <code>LaplacesDemon</code> package neither supports nor
contradicts the suggestion of mixing and &lsquo;hairiness&rsquo;, but suggests
that CSF plots may be used to provide additional information about a
chain. For example, a user may decide on a practical
<code><a href="#topic+burnin">burnin</a></code> given when a conditional mean obtains a certain
standard error.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CSF(x, name, method="Quantiles", quantiles=c(0.025,0.500,0.975), output=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CSF_+3A_x">x</code></td>
<td>
<p>This is a vector of posterior samples from MCMC.</p>
</td></tr>
<tr><td><code id="CSF_+3A_name">name</code></td>
<td>
<p>This is an optional name for vector <code>x</code>, and is input
as a quoted string, such as <code>name="theta"</code>.</p>
</td></tr>
<tr><td><code id="CSF_+3A_method">method</code></td>
<td>
<p>This is a measure that will be observed over the course
of cumulative samples of <code>x</code>. It defaults to
<code>method="Quantiles"</code>, and optional methods include: <code>"ESS"</code>,
<code>"Geweke.Diagnostic"</code>, <code>"HPD"</code>, <code>"is.stationary"</code>,
<code>"Kurtosis"</code>, <code>"MCSE"</code>, <code>"MCSE.bm"</code>,
<code>"MCSE.sv"</code>, <code>"Mean"</code>, <code>"Mode"</code>, <code>"N.Modes"</code>,
<code>"Precision"</code>, <code>"Quantiles"</code>, and <code>"Skewness"</code>.</p>
</td></tr>
<tr><td><code id="CSF_+3A_quantiles">quantiles</code></td>
<td>
<p>This optional argument applies only when
<code>method="Quantiles"</code>, in which case this vector indicates the
probabilities that will be observed. It defaults to the median and
95% probability interval bounds (see <code><a href="#topic+p.interval">p.interval</a></code> for
more information).</p>
</td></tr>
<tr><td><code id="CSF_+3A_output">output</code></td>
<td>
<p>Logical. If <code>output=TRUE</code>, then the results of the
measure over the course of the cumulative samples will be output as
an object, either a vector or matrix, depending on the <code>method</code>
argument. The <code>output</code> argument defaults to <code>FALSE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When <code>method="ESS"</code>, the effective sample size (ESS) is observed
as a function of the cumulative samples of <code>x</code>. For more
information, see the <code><a href="#topic+ESS">ESS</a></code> function.
</p>
<p>When <code>method="Geweke.Diagnostic"</code>, the Z-score output of the
Geweke diagnostic is observed as a function of the cumulative samples
of <code>x</code>. For more information, see the
<code><a href="#topic+Geweke.Diagnostic">Geweke.Diagnostic</a></code> function.
</p>
<p>When <code>method="HPD"</code>, the Highest Posterior Density (HPD) interval
is observed as a function of the cumulative samples of <code>x</code>. For
more information, see the <code><a href="#topic+p.interval">p.interval</a></code> function.
</p>
<p>When <code>method="is.stationary"</code>, stationarity is logically
tested and the result is observed as a function of the cumulative
samples of <code>x</code>. For more information, see the
<code><a href="#topic+is.stationary">is.stationary</a></code> function.
</p>
<p>When <code>method="Kurtosis"</code>, kurtosis is observed as a function of
the cumulative samples of <code>x</code>.
</p>
<p>When <code>method="MCSE"</code>, the Monte Carlo Standard Error (MCSE)
estimated with the <code>IMPS</code> method is observed as a function of
the cumulative samples of <code>x</code>. For more information, see the
<code><a href="#topic+MCSE">MCSE</a></code> function.
</p>
<p>When <code>method="MCSE.bm"</code>, the Monte Carlo Standard Error (MCSE)
estimated with the <code>batch.means</code> method is observed as a
function of the cumulative samples of <code>x</code>. For more information,
see the <code><a href="#topic+MCSE">MCSE</a></code> function.
</p>
<p>When <code>method="MCSE.sv"</code>, the Monte Carlo Standard Error (MCSE)
estimated with the <code>sample.variance</code> method is observed as a
function of the cumulative samples of <code>x</code>. For more information,
see the <code><a href="#topic+MCSE">MCSE</a></code> function.
</p>
<p>When <code>method="Mean"</code>, the mean is observed as a function of
the cumulative samples of <code>x</code>.
</p>
<p>When <code>method="Mode"</code>, the estimated mode is observed as a
function of the cumulative samples of <code>x</code>. For more information,
see the <code><a href="#topic+Mode">Mode</a></code> function.
</p>
<p>When <code>method="N.Modes"</code>, the estimated number of modes is
observed as a function of the cumulative samples of <code>x</code>. For
more information, see the <code><a href="#topic+Modes">Modes</a></code> function.
</p>
<p>When <code>method="Precision"</code>, the precision (inverse variance) is
observed as a function of the cumulative samples of <code>x</code>.
</p>
<p>When <code>method="Quantiles"</code>, the quantiles selected with the
<code>quantiles</code> argument are observed as a function of the
cumulative samples of <code>x</code>.
</p>
<p>When <code>method="Skewness"</code>, skewness is observed as a function of
the cumulative samples of <code>x</code>.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>References</h3>

<p>Yu, B. and Myckland, P. (1997). &quot;Looking at Markov Samplers through
Cusum Path Plots: A Simple Diagnostic Idea&quot;. <em>Statistics and
Computing</em>, 8(3), p. 275&ndash;286. 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+burnin">burnin</a></code>,
<code><a href="#topic+ESS">ESS</a></code>,
<code><a href="#topic+Geweke.Diagnostic">Geweke.Diagnostic</a></code>,
<code><a href="#topic+is.stationary">is.stationary</a></code>,
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>,
<code><a href="#topic+MCSE">MCSE</a></code>,
<code><a href="#topic+Mode">Mode</a></code>,
<code><a href="#topic+Modes">Modes</a></code>, and
<code><a href="#topic+p.interval">p.interval</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#Commented-out because of run-time for package builds
#library(LaplacesDemon)
#x &lt;- rnorm(1000)
#CSF(x, method="ESS")
#CSF(x, method="Geweke.Diagnostic")
#CSF(x, method="HPD")
#CSF(x, method="is.stationary")
#CSF(x, method="Kurtosis")
#CSF(x, method="MCSE")
#CSF(x, method="MCSE.bm")
#CSF(x, method="MCSE.sv")
#CSF(x, method="Mean")
#CSF(x, method="Mode")
#CSF(x, method="N.Modes")
#CSF(x, method="Precision")
#CSF(x, method="Quantiles")
#CSF(x, method="Skewness")
</code></pre>

<hr>
<h2 id='data.demonchoice'>Demon Choice Data Set</h2><span id='topic+demonchoice'></span>

<h3>Description</h3>

<p>This data set is for discrete choice models and consists of the choice
of commuting route to school: arterial, two-lane, or freeway. There
were 151 Pennsylvania commuters who started from a residential complex
in State College, PA, and commute to downtown State College.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(demonchoice)</code></pre>


<h3>Format</h3>

<p>This data frame contains 151 rows of individual choices and 9
columns. The following data dictionary describes each variable or
column.
</p>

<dl>
<dt><code>Choice</code></dt><dd><p>This is the route choice: four-lane arterial
(35 MPH speed limit), two-lane highway (35 MPH speed limit, with
one lane in each direction), or a limited-access four-lane freeway
(55 MPH speed liimit.)</p>
</dd>
<dt><code>HH.Income</code></dt><dd><p>This is an ordinal variable of annual
household income of the commuter in USD. There are four
categories: 1 is less than 20,000 USD, 2 is 20,000-29,999 USD, 3
is 30,000-39,999 USD, and 4 is 40,000 USD or greater.</p>
</dd>
<dt><code>Vehicle.Age</code></dt><dd><p>This is the age in years of the vehicle of
the commuter.</p>
</dd>
<dt><code>Stop.Signs.Arterial</code></dt><dd><p>This is the number of stop signs
along the arterial route.</p>
</dd>
<dt><code>Stop.Signs.Two.Lane</code></dt><dd><p>This is the number of stop signs
along the two-lane route.</p>
</dd>
<dt><code>Stop.Signs.Freeway</code></dt><dd><p>This is the number of stop signs
along the freeway route.</p>
</dd>
<dt><code>Distance.Arterial</code></dt><dd><p>This is distance in miles of the
arterial route.</p>
</dd>
<dt><code>Distance.Two.Lane</code></dt><dd><p>This is the distance in miles of
the two-lane route.</p>
</dd>
<dt><code>Distance.Freeway</code></dt><dd><p>This is the distance in miles of
the freeway route.</p>
</dd>
</dl>



<h3>Source</h3>

<p>Washington, S., Congdon, P., Karlaftis, M., and Mannering,
F. (2009). &quot;Bayesian Multinomial Logit: Theory and Route Choice
Example&quot;. Transportation Research Record, 2136, p. 28&ndash;36.</p>

<hr>
<h2 id='data.demonfx'>Demon FX Data Set</h2><span id='topic+demonfx'></span>

<h3>Description</h3>

<p>This data set consists of daily currency pair prices from 2010 through
2014. Each currency pair has a close, high, and low price.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(demonfx)</code></pre>


<h3>Format</h3>

<p>This data frame contains 1,301 rows as time-periods (with row names) and
39 columns of currency pair prices. The following data dictionary describes
each time-series or column.
</p>

<dl>
<dt><code>EURUSD.Close</code></dt><dd><p>This is the currency pair closing price.</p>
</dd>
<dt><code>EURUSD.High</code></dt><dd><p>This is the currency pair high price.</p>
</dd>
<dt><code>EURUSD.Low</code></dt><dd><p>This is the currency pair low price.</p>
</dd>
<dt><code>USDJPY.Close</code></dt><dd><p>This is the currency pair closing price.</p>
</dd>
<dt><code>USDJPY.High</code></dt><dd><p>This is the currency pair high price.</p>
</dd>
<dt><code>USDJPY.Low</code></dt><dd><p>This is the currency pair low price.</p>
</dd>
<dt><code>USDCHF.Close</code></dt><dd><p>This is the currency pair closing price.</p>
</dd>
<dt><code>USDCHF.High</code></dt><dd><p>This is the currency pair high price.</p>
</dd>
<dt><code>USDCHF.Low</code></dt><dd><p>This is the currency pair low price.</p>
</dd>
<dt><code>GBPUSD.Close</code></dt><dd><p>This is the currency pair closing price.</p>
</dd>
<dt><code>GBPUSD.High</code></dt><dd><p>This is the currency pair high price.</p>
</dd>
<dt><code>GBPUSD.Low</code></dt><dd><p>This is the currency pair low price.</p>
</dd>
<dt><code>USDCAD.Close</code></dt><dd><p>This is the currency pair closing price.</p>
</dd>
<dt><code>USDCAD.High</code></dt><dd><p>This is the currency pair high price.</p>
</dd>
<dt><code>USDCAD.Low</code></dt><dd><p>This is the currency pair low price.</p>
</dd>
<dt><code>EURGBP.Close</code></dt><dd><p>This is the currency pair closing price.</p>
</dd>
<dt><code>EURGBP.High</code></dt><dd><p>This is the currency pair high price.</p>
</dd>
<dt><code>EURGBP.Low</code></dt><dd><p>This is the currency pair low price.</p>
</dd>
<dt><code>EURJPY.Close</code></dt><dd><p>This is the currency pair closing price.</p>
</dd>
<dt><code>EURJPY.High</code></dt><dd><p>This is the currency pair high price.</p>
</dd>
<dt><code>EURJPY.Low</code></dt><dd><p>This is the currency pair low price.</p>
</dd>
<dt><code>EURCHF.Close</code></dt><dd><p>This is the currency pair closing price.</p>
</dd>
<dt><code>EURCHF.High</code></dt><dd><p>This is the currency pair high price.</p>
</dd>
<dt><code>EURCHF.Low</code></dt><dd><p>This is the currency pair low price.</p>
</dd>
<dt><code>AUDUSD.Close</code></dt><dd><p>This is the currency pair closing price.</p>
</dd>
<dt><code>AUDUSD.High</code></dt><dd><p>This is the currency pair high price.</p>
</dd>
<dt><code>AUDUSD.Low</code></dt><dd><p>This is the currency pair low price.</p>
</dd>
<dt><code>GBPJPY.Close</code></dt><dd><p>This is the currency pair closing price.</p>
</dd>
<dt><code>GBPJPY.High</code></dt><dd><p>This is the currency pair high price.</p>
</dd>
<dt><code>GBPJPY.Low</code></dt><dd><p>This is the currency pair low price.</p>
</dd>
<dt><code>CHFJPY.Close</code></dt><dd><p>This is the currency pair closing price.</p>
</dd>
<dt><code>CHFJPY.High</code></dt><dd><p>This is the currency pair high price.</p>
</dd>
<dt><code>CHFJPY.Low</code></dt><dd><p>This is the currency pair low price.</p>
</dd>
<dt><code>GBPCHF.Close</code></dt><dd><p>This is the currency pair closing price.</p>
</dd>
<dt><code>GBPCHF.High</code></dt><dd><p>This is the currency pair high price.</p>
</dd>
<dt><code>GBPCHF.Low</code></dt><dd><p>This is the currency pair low price.</p>
</dd>
<dt><code>NZDUSD.Close</code></dt><dd><p>This is the currency pair closing price.</p>
</dd>
<dt><code>NZDUSD.High</code></dt><dd><p>This is the currency pair high price.</p>
</dd>
<dt><code>NZDUSD.Low</code></dt><dd><p>This is the currency pair low price.</p>
</dd>
</dl>



<h3>Source</h3>

<p><a href="https://www.global-view.com/forex-trading-tools/forex-history/index.html">https://www.global-view.com/forex-trading-tools/forex-history/index.html</a></p>

<hr>
<h2 id='data.demonsessions'>Demon Sessions Data Set</h2><span id='topic+demonsessions'></span>

<h3>Description</h3>

<p>These are the monthly number of user sessions at
<a href="https://web.archive.org/web/20141224051720/http://www.bayesian-inference.com/index">https://web.archive.org/web/20141224051720/http://www.bayesian-inference.com/index</a> by continent. Additional data
may be added in the future.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(demonsessions)</code></pre>


<h3>Format</h3>

<p>This data frame contains 26 rows (with row names) and 6 columns. The
following data dictionary describes each variable or column.
</p>

<dl>
<dt><code>Africa</code></dt><dd><p>This is the African continent.</p>
</dd>
<dt><code>Americas</code></dt><dd><p>This is North and South America.</p>
</dd>
<dt><code>Asia</code></dt><dd><p>This is the Asian continent.</p>
</dd>
<dt><code>Europe</code></dt><dd><p>This is Europe as a continent.</p>
</dd>
<dt><code>Oceania</code></dt><dd><p>This is Oceania, such as Australia.</p>
</dd>
<dt><code>Not.Set</code></dt><dd><p>This includes sessions in which the continent
was not set, or is unknown.</p>
</dd>
</dl>



<h3>Source</h3>

<p><a href="https://web.archive.org/web/20141224051720/http://www.bayesian-inference.com/index">https://web.archive.org/web/20141224051720/http://www.bayesian-inference.com/index</a></p>

<hr>
<h2 id='data.demonsnacks'>Demon Snacks Data Set</h2><span id='topic+demonsnacks'></span>

<h3>Description</h3>

<p>Late one night, after witnessing Laplace's Demon in action, I followed
him back to what seemed to be his lair. Minutes later, he left again.
I snuck inside and saw something labeled 'Demon Snacks'. Hurriedly, I
recorded the 39 items, each with a name and 10 nutritional attributes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(demonsnacks)</code></pre>


<h3>Format</h3>

<p>This data frame contains 39 rows (with row names) and 10 columns. The
following data dictionary describes each variable or column.
</p>

<dl>
<dt><code>Serving.Size</code></dt><dd><p>This is serving size in grams.</p>
</dd>
<dt><code>Calories</code></dt><dd><p>This is the number of calories.</p>
</dd>
<dt><code>Total.Fat</code></dt><dd><p>This is total fat in grams.</p>
</dd>
<dt><code>Saturated.Fat</code></dt><dd><p>This is saturated fat in grams.</p>
</dd>
<dt><code>Cholesterol</code></dt><dd><p>This is cholesterol in milligrams.</p>
</dd>
<dt><code>Sodium</code></dt><dd><p>This is sodium in milligrams.</p>
</dd>
<dt><code>Total.Carbohydrate</code></dt><dd><p>This is the total carbohydrates in grams.</p>
</dd>
<dt><code>Dietary.Fiber</code></dt><dd><p>This is dietary fiber in grams.</p>
</dd>
<dt><code>Sugars</code></dt><dd><p>This is sugar in grams.</p>
</dd>
<dt><code>Protein</code></dt><dd><p>This is protein in grams.</p>
</dd>
</dl>



<h3>Source</h3>

<p>This data was obtained from the lair of Laplace's Demon!</p>

<hr>
<h2 id='data.demontexas'>Demon Space-Time Data Set</h2><span id='topic+demontexas'></span>

<h3>Description</h3>

<p>This data set is for space-time models that require latitude and
longitude, or coordinates. This data set consists of the minimum,
mean, and maximum temperatures in Texas for 13 months.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(demontexas)</code></pre>


<h3>Format</h3>

<p>This data frame contains 369 rows of sites in Texas and 43
columns. The following data dictionary describes each variable or
column.
</p>

<dl>
<dt><code>Elevation</code></dt><dd><p>This is the elevation of the site.</p>
</dd>
<dt><code>Latitude</code></dt><dd><p>This is the latitude of the site.</p>
</dd>
<dt><code>Longitude</code></dt><dd><p>This is the longitude of the site.</p>
</dd>
<dt><code>Gulf</code></dt><dd><p>This is a gulf indicator of the site.</p>
</dd>
<dt><code>Max1</code></dt><dd><p>This is the maximum temperature in month 1.</p>
</dd>
<dt><code>Max2</code></dt><dd><p>This is the maximum temperature in month 2.</p>
</dd>
<dt><code>Max3</code></dt><dd><p>This is the maximum temperature in month 3.</p>
</dd>
<dt><code>Max4</code></dt><dd><p>This is the maximum temperature in month 4.</p>
</dd>
<dt><code>Max5</code></dt><dd><p>This is the maximum temperature in month 5.</p>
</dd>
<dt><code>Max6</code></dt><dd><p>This is the maximum temperature in month 6.</p>
</dd>
<dt><code>Max7</code></dt><dd><p>This is the maximum temperature in month 7.</p>
</dd>
<dt><code>Max8</code></dt><dd><p>This is the maximum temperature in month 8.</p>
</dd>
<dt><code>Max9</code></dt><dd><p>This is the maximum temperature in month 9.</p>
</dd>
<dt><code>Max10</code></dt><dd><p>This is the maximum temperature in month 10.</p>
</dd>
<dt><code>Max11</code></dt><dd><p>This is the maximum temperature in month 11.</p>
</dd>
<dt><code>Max12</code></dt><dd><p>This is the maximum temperature in month 12.</p>
</dd>
<dt><code>Max13</code></dt><dd><p>This is the maximum temperature in month 13.</p>
</dd>
<dt><code>Mean1</code></dt><dd><p>This is the mean temperature in month 1.</p>
</dd>
<dt><code>Mean2</code></dt><dd><p>This is the mean temperature in month 2.</p>
</dd>
<dt><code>Mean3</code></dt><dd><p>This is the mean temperature in month 3.</p>
</dd>
<dt><code>Mean4</code></dt><dd><p>This is the mean temperature in month 4.</p>
</dd>
<dt><code>Mean5</code></dt><dd><p>This is the mean temperature in month 5.</p>
</dd>
<dt><code>Mean6</code></dt><dd><p>This is the mean temperature in month 6.</p>
</dd>
<dt><code>Mean7</code></dt><dd><p>This is the mean temperature in month 7.</p>
</dd>
<dt><code>Mean8</code></dt><dd><p>This is the mean temperature in month 8.</p>
</dd>
<dt><code>Mean9</code></dt><dd><p>This is the mean temperature in month 9.</p>
</dd>
<dt><code>Mean10</code></dt><dd><p>This is the mean temperature in month 10.</p>
</dd>
<dt><code>Mean11</code></dt><dd><p>This is the mean temperature in month 11.</p>
</dd>
<dt><code>Mean12</code></dt><dd><p>This is the mean temperature in month 12.</p>
</dd>
<dt><code>Mean13</code></dt><dd><p>This is the mean temperature in month 13.</p>
</dd>
<dt><code>Min1</code></dt><dd><p>This is the minimum temperature in month 1.</p>
</dd>
<dt><code>Min2</code></dt><dd><p>This is the minimum temperature in month 2.</p>
</dd>
<dt><code>Min3</code></dt><dd><p>This is the minimum temperature in month 3.</p>
</dd>
<dt><code>Min4</code></dt><dd><p>This is the minimum temperature in month 4.</p>
</dd>
<dt><code>Min5</code></dt><dd><p>This is the minimum temperature in month 5.</p>
</dd>
<dt><code>Min6</code></dt><dd><p>This is the minimum temperature in month 6.</p>
</dd>
<dt><code>Min7</code></dt><dd><p>This is the minimum temperature in month 7.</p>
</dd>
<dt><code>Min8</code></dt><dd><p>This is the minimum temperature in month 8.</p>
</dd>
<dt><code>Min9</code></dt><dd><p>This is the minimum temperature in month 9.</p>
</dd>
<dt><code>Min10</code></dt><dd><p>This is the minimum temperature in month 10.</p>
</dd>
<dt><code>Min11</code></dt><dd><p>This is the minimum temperature in month 11.</p>
</dd>
<dt><code>Min12</code></dt><dd><p>This is the minimum temperature in month 12.</p>
</dd>
<dt><code>Min13</code></dt><dd><p>This is the minimum temperature in month 13.</p>
</dd>
</dl>



<h3>Source</h3>

<p><a href="http://www.stat.ufl.edu/~winner/datasets.html">http://www.stat.ufl.edu/~winner/datasets.html</a></p>

<hr>
<h2 id='de.Finetti.Game'>de Finetti's Game</h2><span id='topic+de.Finetti.Game'></span>

<h3>Description</h3>

<p>The <code>de.Finetti.Game</code> function estimates the interval of a
subjective probability regarding a possible event in the near future.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>de.Finetti.Game(width)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="de.Finetti.Game_+3A_width">width</code></td>
<td>
<p>This is the maximum acceptable width of the interval for
the returned subjective probability. The user must specify a width
between 0 and 1.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is a variation on the game introduced by de Finetti,
who is one of the main developers of subjective probability,
along with Ramsey and Savage. In the original context, de Finetti
proposed a gamble regarding life on Mars one billion years ago.
</p>
<p>The frequentist interpretation of probability defines the probability
of an event as the limit of its relative frequency in a large number
of trials. Frequentist inference is undefined, for example, when there
are no trials from which to calculate a probability. By defining
probability relative to frequencies of physical events, frequentists
attempt to objectify probability. However, de Finetti asserts that the
frequentist (or objective) interpretation always reduces to a subjective
interpretation of probability, because probability is a human
construct and does not exist independently of humans in
nature. Therefore, probability is a degree of belief, and is called
subjective or personal probability.
</p>


<h3>Value</h3>

<p>The <code>de.Finetti.Game</code> function returns a vector of length
two. The respective elements are the lower and upper bounds of the
subjective probability of the participant regarding the possible event
in the near future.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+elicit">elicit</a></code>
</p>

<hr>
<h2 id='deburn'>De-Burn</h2><span id='topic+deburn'></span>

<h3>Description</h3>

<p>The <code>deburn</code> function discards or removes a user-specified number
of burn-in iterations from an object of class <code>demonoid</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>deburn(x, BurnIn=0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="deburn_+3A_x">x</code></td>
<td>
<p>This is an object of class <code>demonoid</code>.</p>
</td></tr>
<tr><td><code id="deburn_+3A_burnin">BurnIn</code></td>
<td>
<p>This argument defaults to <code>BurnIn=0</code>, and accepts
an integer that indicates the number of iterations to discard as
burn-in.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Documentation for the <code><a href="#topic+burnin">burnin</a></code> function provides an
introduction to the concept of burn-in as it relates to Markov chains.
</p>
<p>The <code>deburn</code> function discards a number of the first posterior
samples, as specified by the <code>BurnIn</code> argument. Stationarity is
not checked, because it is assumed the user has a reason for using the
<code>deburn</code> function, rather than using the results from the object
of class <code>demonoid</code>. Therefore, the posterior samples in
<code>Posterior1</code> and <code>Posterior2</code> are identical, as are
<code>Summary1</code> and <code>Summary2</code>.
</p>


<h3>Value</h3>

<p>The <code>deburn</code> function returns an object of class <code>demonoid</code>.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+burnin">burnin</a></code> and
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### Assuming the user has Fit which is an object of class demonoid:
#library(LaplacesDemon)
#Fit2 &lt;- deburn(Fit, BurnIn=100)
</code></pre>

<hr>
<h2 id='dist.Asymmetric.Laplace'>Asymmetric Laplace Distribution: Univariate</h2><span id='topic+dalaplace'></span><span id='topic+palaplace'></span><span id='topic+qalaplace'></span><span id='topic+ralaplace'></span>

<h3>Description</h3>

<p>These functions provide the density, distribution function, quantile
function, and random generation for the univariate, asymmetric Laplace
distribution with location parameter <code>location</code>, scale parameter
<code>scale</code>, and asymmetry or skewness parameter <code>kappa</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dalaplace(x, location=0, scale=1, kappa=1, log=FALSE)
palaplace(q, location=0, scale=1, kappa=1)
qalaplace(p, location=0, scale=1, kappa=1)
ralaplace(n, location=0, scale=1, kappa=1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.Asymmetric.Laplace_+3A_x">x</code>, <code id="dist.Asymmetric.Laplace_+3A_q">q</code></td>
<td>
<p>These are each a vector of quantiles.</p>
</td></tr>
<tr><td><code id="dist.Asymmetric.Laplace_+3A_p">p</code></td>
<td>
<p>This is a vector of probabilities.</p>
</td></tr>
<tr><td><code id="dist.Asymmetric.Laplace_+3A_n">n</code></td>
<td>
<p>This is the number of observations, which must be a positive
integer that has length 1.</p>
</td></tr>
<tr><td><code id="dist.Asymmetric.Laplace_+3A_location">location</code></td>
<td>
<p>This is the location parameter <code class="reqn">\mu</code>.</p>
</td></tr>
<tr><td><code id="dist.Asymmetric.Laplace_+3A_scale">scale</code></td>
<td>
<p>This is the scale parameter <code class="reqn">\lambda</code>, which
must be positive.</p>
</td></tr>
<tr><td><code id="dist.Asymmetric.Laplace_+3A_kappa">kappa</code></td>
<td>
<p>This is the asymmetry or skewness parameter
<code class="reqn">\kappa</code>, which must be positive.</p>
</td></tr>
<tr><td><code id="dist.Asymmetric.Laplace_+3A_log">log</code></td>
<td>
<p>Logical. If <code>log=TRUE</code>, then the logarithm of the
density is returned.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> Application: Continuous Univariate
</p>
</li>
<li><p> Density: <code class="reqn">p(\theta) = \frac{\kappa \sqrt{2}}{\lambda
      (1+\kappa^2)} \exp(-|\theta-\mu| \frac{\sqrt{2}}{\lambda}
    \kappa^{|\theta-\mu|} |\theta-\mu|)</code>
</p>
</li>
<li><p> Inventor: Kotz, Kozubowski, and Podgorski (2001)
</p>
</li>
<li><p> Notation 1: <code class="reqn">\theta \sim \mathcal{AL}(\mu, \lambda,
    \kappa)</code>
</p>
</li>
<li><p> Notation 2: <code class="reqn">p(\theta) = \mathcal{AL}(\theta | \mu, \lambda,
  \kappa)</code>
</p>
</li>
<li><p> Parameter 1: location parameter <code class="reqn">\mu</code>
</p>
</li>
<li><p> Parameter 2: scale parameter <code class="reqn">\lambda &gt; 0</code>
</p>
</li>
<li><p> Parameter 3: skewness parameter <code class="reqn">\kappa &gt; 0</code>
</p>
</li>
<li><p> Mean: <code class="reqn">E(\theta) = \mu + \lambda \frac{1/\kappa - \kappa}{\sqrt{2}}</code>
</p>
</li>
<li><p> Variance: <code class="reqn">var(\theta) = \lambda^2 \frac{1 + \kappa^4}{2 \kappa^2}</code>
</p>
</li>
<li><p> Mode: <code class="reqn">mode(\theta) = \mu</code>
</p>
</li></ul>

<p>The asymmetric Laplace of Kotz, Kozubowski, and Podgorski (2001), also
referred to as AL, is an extension of the univariate, symmetric Laplace
distribution to allow for skewness. It is parameterized according to
three parameters: location parameter <code class="reqn">\mu</code>, scale parameter
<code class="reqn">\lambda</code>, and asymmetry or skewness parameter
<code class="reqn">\kappa</code>. The special case of <code class="reqn">\kappa=1</code> is the
symmetric Laplace distribution. Values of <code class="reqn">\kappa</code> in the
intervals <code class="reqn">(0, 1)</code> and <code class="reqn">(1, \infty)</code>,
correspond to positive (right) and negative (left) skewness,
respectively. The AL distribution is leptokurtic, and its kurtosis
ranges from 3 to 6 as <code class="reqn">\kappa</code> ranges from 1 to infinity. The
skewness of the AL has been useful in engineering and finance. As an
example, the AL distribution has been used as a replacement for
Gaussian-distributed GARCH residuals. There is also an extension to the
asymmetric multivariate Laplace distribution.
</p>
<p>The asymmetric Laplace distribution is demonstrated in Kozubowski and
Podgorski (2001) to be well-suited for financial modeling, specifically
with currency exchange rates.
</p>
<p>These functions are similar to those in the <code>VGAM</code> package.
</p>


<h3>Value</h3>

<p><code>dalaplace</code> gives the density,
<code>palaplace</code> gives the distribution function,
<code>qalaplace</code> gives the quantile function, and
<code>ralaplace</code> generates random deviates.
</p>


<h3>References</h3>

<p>Kotz, S., Kozubowski, T.J., and Podgorski, K. (2001). &quot;The Laplace
Distribution and Generalizations: a Revisit with Applications to
Communications, Economics, Engineering, and Finance&quot;. Boston:
Birkhauser.
</p>
<p>Kozubowski, T.J. and Podgorski, K. (2001). &quot;Asymmetric Laplace
Laws and Modeling Financial Data&quot;. <em>Mathematical and Computer
Modelling</em>, 34, p. 1003-1021.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dlaplace">dlaplace</a></code> and
<code><a href="#topic+dallaplace">dallaplace</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
x &lt;- dalaplace(1,0,1,1)
x &lt;- palaplace(1,0,1,1)
x &lt;- qalaplace(0.5,0,1,1)
x &lt;- ralaplace(100,0,1,1)

#Plot Probability Functions
x &lt;- seq(from=-5, to=5, by=0.1)
plot(x, dalaplace(x,0,1,0.5), ylim=c(0,1), type="l", main="Probability Function",
     ylab="density", col="red")
lines(x, dalaplace(x,0,1,1), type="l", col="green")
lines(x, dalaplace(x,0,1,5), type="l", col="blue")
legend(1, 0.9, expression(paste(mu==0, ", ", lambda==1, ", ", kappa==0.5),
     paste(mu==0, ", ", lambda==1, ", ", kappa==1),
     paste(mu==0, ", ", lambda==1, ", ", kappa==5)),
     lty=c(1,1,1), col=c("red","green","blue"))
</code></pre>

<hr>
<h2 id='dist.Asymmetric.Log.Laplace'>Asymmetric Log-Laplace Distribution</h2><span id='topic+dallaplace'></span><span id='topic+pallaplace'></span><span id='topic+qallaplace'></span><span id='topic+rallaplace'></span>

<h3>Description</h3>

<p>These functions provide the density, distribution function, quantile
function, and random generation for the univariate, asymmetric,
log-Laplace distribution with location parameter <code class="reqn">\mu</code>,
scale parameter <code class="reqn">\lambda</code>, and asymmetry or skewness
parameter <code class="reqn">\kappa</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dallaplace(x, location=0, scale=1, kappa=1, log=FALSE)
pallaplace(q, location=0, scale=1, kappa=1)
qallaplace(p, location=0, scale=1, kappa=1)
rallaplace(n, location=0, scale=1, kappa=1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.Asymmetric.Log.Laplace_+3A_x">x</code>, <code id="dist.Asymmetric.Log.Laplace_+3A_q">q</code></td>
<td>
<p>These are each a vector of quantiles.</p>
</td></tr>
<tr><td><code id="dist.Asymmetric.Log.Laplace_+3A_p">p</code></td>
<td>
<p>This is a vector of probabilities.</p>
</td></tr>
<tr><td><code id="dist.Asymmetric.Log.Laplace_+3A_n">n</code></td>
<td>
<p>This is the number of observations, which must be a positive
integer that has length 1.</p>
</td></tr>
<tr><td><code id="dist.Asymmetric.Log.Laplace_+3A_location">location</code></td>
<td>
<p>This is the location parameter <code class="reqn">\mu</code>.</p>
</td></tr>
<tr><td><code id="dist.Asymmetric.Log.Laplace_+3A_scale">scale</code></td>
<td>
<p>This is the scale parameter <code class="reqn">\lambda</code>, which
must be positive.</p>
</td></tr>
<tr><td><code id="dist.Asymmetric.Log.Laplace_+3A_kappa">kappa</code></td>
<td>
<p>This is the asymmetry or skewness parameter
<code class="reqn">\kappa</code>, which must be positive.</p>
</td></tr>
<tr><td><code id="dist.Asymmetric.Log.Laplace_+3A_log">log</code></td>
<td>
<p>Logical. If <code>log=TRUE</code>, then the logarithm of the
density is returned.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> Application: Continuous Univariate
</p>
</li>
<li><p> Density 1: <code class="reqn">p(\theta) = \exp(-\mu)\frac{(\sqrt(2)\kappa / \lambda)(\sqrt(2) / \lambda\kappa)}{(\sqrt(2)\kappa / \lambda)+(\sqrt(2) / (\lambda\kappa))} \exp(-(\frac{\sqrt(2)\kappa}{\lambda})+1), \quad \theta \ge \exp(\mu)</code>
</p>
</li>
<li><p> Density 2: <code class="reqn">p(\theta) = \exp(-\mu) \frac{(\sqrt(2)\kappa / \lambda) (\sqrt(2) / (\lambda\kappa))}{(\sqrt(2)\kappa / \lambda) + (\sqrt(2) / (\lambda\kappa))} \exp(\frac{\sqrt(2)(\log(\theta)-\mu)}{\lambda\kappa} - (\log(\theta)-\mu)), \quad \theta &lt; \exp(\mu)</code>
</p>
</li>
<li><p> Inventor: Pierre-Simon Laplace
</p>
</li>
<li><p> Notation 1: <code class="reqn">\theta \sim \mathcal{ALL}(\mu, \lambda,
    \kappa)</code>
</p>
</li>
<li><p> Notation 2: <code class="reqn">p(\theta) = \mathcal{ALL}(\theta | \mu,
    \lambda, \kappa)</code>
</p>
</li>
<li><p> Parameter 1: location parameter <code class="reqn">\mu</code>
</p>
</li>
<li><p> Parameter 2: scale parameter <code class="reqn">\lambda &gt; 0</code>
</p>
</li>
<li><p> Mean: <code class="reqn">E(\theta) = </code>
</p>
</li>
<li><p> Variance: <code class="reqn">var(\theta) = </code>
</p>
</li>
<li><p> Mode: <code class="reqn">mode(\theta) = </code>
</p>
</li></ul>

<p>The univariate, asymmetric log-Laplace distribution is derived from the
Laplace distribution. Multivariate and symmetric versions also exist.
</p>
<p>These functions are similar to those in the <code>VGAM</code> package.
</p>


<h3>Value</h3>

<p><code>dallaplace</code> gives the density,
<code>pallaplace</code> gives the distribution function,
<code>qallaplace</code> gives the quantile function, and
<code>rallaplace</code> generates random deviates.
</p>


<h3>References</h3>

<p>Kozubowski, T. J. and Podgorski, K. (2003). &quot;Log-Laplace Distributions&quot;.
<em>International Mathematical Journal</em>, 3, p. 467&ndash;495.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dalaplace">dalaplace</a></code>,
<code><a href="stats.html#topic+dexp">dexp</a></code>,
<code><a href="#topic+dlaplace">dlaplace</a></code>,
<code><a href="#topic+dlaplacep">dlaplacep</a></code>,
<code><a href="#topic+dllaplace">dllaplace</a></code>,
<code><a href="#topic+dmvl">dmvl</a></code>,
<code><a href="stats.html#topic+dnorm">dnorm</a></code>,
<code><a href="#topic+dnormp">dnormp</a></code>,
<code><a href="#topic+dnormv">dnormv</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
x &lt;- dallaplace(1,0,1,1)
x &lt;- pallaplace(1,0,1,1)
x &lt;- qallaplace(0.5,0,1,1)
x &lt;- rallaplace(100,0,1,1)

#Plot Probability Functions
x &lt;- seq(from=0.1, to=10, by=0.1)
plot(x, dallaplace(x,0,1,0.5), ylim=c(0,1), type="l", main="Probability Function",
     ylab="density", col="red")
lines(x, dallaplace(x,0,1,1), type="l", col="green")
lines(x, dallaplace(x,0,1,5), type="l", col="blue")
legend(5, 0.9, expression(paste(mu==0, ", ", lambda==1, ", ", kappa==0.5),
     paste(mu==0, ", ", lambda==1, ", ", kappa==1),
     paste(mu==0, ", ", lambda==1, ", ", kappa==5)),
     lty=c(1,1,1), col=c("red","green","blue"))
</code></pre>

<hr>
<h2 id='dist.Asymmetric.Multivariate.Laplace'>Asymmetric Multivariate Laplace Distribution</h2><span id='topic+daml'></span><span id='topic+raml'></span>

<h3>Description</h3>

<p>These functions provide the density and random generation for the
asymmetric multivariate Laplace distribution with location and skew
parameter <code class="reqn">\mu</code> and covariance <code class="reqn">\Sigma</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>daml(x, mu, Sigma, log=FALSE)
raml(n, mu, Sigma)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.Asymmetric.Multivariate.Laplace_+3A_x">x</code></td>
<td>
<p>This is a <code class="reqn">N \times K</code> matrix of data, or a
vector of length <code class="reqn">K</code>.</p>
</td></tr>
<tr><td><code id="dist.Asymmetric.Multivariate.Laplace_+3A_n">n</code></td>
<td>
<p>This is the number of observations, which must be a positive
integer that has length 1.</p>
</td></tr>
<tr><td><code id="dist.Asymmetric.Multivariate.Laplace_+3A_mu">mu</code></td>
<td>
<p>This is the location and skew parameter <code class="reqn">\mu</code>. This
may be a <code class="reqn">N \times K</code> matrix, or a vector of length
<code class="reqn">K</code>.</p>
</td></tr>
<tr><td><code id="dist.Asymmetric.Multivariate.Laplace_+3A_sigma">Sigma</code></td>
<td>
<p>This is the <code class="reqn">K \times K</code> positive-definite
covariance matrix <code class="reqn">\Sigma</code>.</p>
</td></tr>
<tr><td><code id="dist.Asymmetric.Multivariate.Laplace_+3A_log">log</code></td>
<td>
<p>Logical. If <code>log=TRUE</code>, then the logarithm of the
density is returned.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> Application: Continuous Multivariate
</p>
</li>
<li><p> Density: <code class="reqn">p(\theta) =
      \frac{2\exp(\theta\Omega\theta)}{(2\pi)^{k/2}|\Sigma|^0.5}
      \frac{\theta\Omega\theta}{2 + \mu\Omega\mu}^{(2-k)/4}
      K_{(2-k)/2}(\sqrt{(2 + \mu\Omega\mu)(\theta\Omega\theta)})</code>
</p>
</li>
<li><p> Inventor: Kotz, Kozubowski, and Podgorski (2003)
</p>
</li>
<li><p> Notation 1: <code class="reqn">\theta \sim \mathcal{AL}_K(\mu,
      \Sigma)</code>
</p>
</li>
<li><p> Notation 2: <code class="reqn">p(\theta) = \mathcal{AL}_K(\theta | \mu,
      \Sigma)</code>
</p>
</li>
<li><p> Parameter 1: location-skew parameter <code class="reqn">\mu</code>
</p>
</li>
<li><p> Parameter 2: positive-definite covariance matrix
<code class="reqn">\Sigma</code>
</p>
</li>
<li><p> Mean: Unknown
</p>
</li>
<li><p> Variance: Unknown
</p>
</li>
<li><p> Mode: <code class="reqn">mode(\theta) = \mu</code>
</p>
</li></ul>

<p>The asymmetric multivariate Laplace distribution of Kotz, Kozubowski,
and Podgorski (2003) is a multivariate extension of the univariate,
asymmetric Laplace distribution. It is parameterized according to
two parameters: location-skew parameter <code class="reqn">\mu</code> and positive-definite
covariance matrix <code class="reqn">\Sigma</code>. Location and skew occur in the
same parameter. When <code class="reqn">\mu=0</code>, the density is the (symmetric)
multivariate Laplace of Anderson (1992). As each location deviates from
zero, the marginal distribution becomes more skewed. Since location and
skew are combined, it is appropriate for zero-centered variables, such
as a matrix of centered and scaled dependent variables in cluster
analysis, factor analysis, multivariate regression, or multivariate
time-series.
</p>
<p>The asymmetric multivariate Laplace distribution is also discussed
earlier in Kozubowski and Podgorski (2001), and is well-suited for
financial modeling via multivariate regression, specifically with
currency exchange rates. Cajigas and Urga (2005) fit residuals in a
multivariate GARCH model with the asymmetric multivariate Laplace
distribution, regarding stocks and bonds. They find that it
&quot;overwhelmingly outperforms&quot; normality.
</p>


<h3>Value</h3>

<p><code>daml</code> gives the density, and
<code>raml</code> generates random deviates.
</p>


<h3>References</h3>

<p>Anderson, D.N. (1992). &quot;A Multivariate Linnik Distribution&quot;.
<em>Statistical Probability Letters</em>, 14, p. 333&ndash;336.
</p>
<p>Cajigas, J.P. and Urga, G. (2005) &quot;Dynamic Conditional Correlation
Models with Asymmetric Laplace Innovations&quot;. Centre for Economic
Analysis: Cass Business School.
</p>
<p>Kotz, S., Kozubowski, T.J., and Podgorski, K. (2003). &quot;An
Asymmetric Multivariate Laplace Distribution&quot;. Working Paper.
</p>
<p>Kozubowski, T.J. and Podgorski, K. (2001). &quot;Asymmetric Laplace
Laws and Modeling Financial Data&quot;. <em>Mathematical and Computer
Modelling</em>, 34, p. 1003&ndash;1021.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dalaplace">dalaplace</a></code> and
<code><a href="#topic+dmvl">dmvl</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
x &lt;- daml(c(1,2,3), c(0,1,2), diag(3))
X &lt;- raml(1000, c(0,1,2), diag(3))
joint.density.plot(X[,1], X[,2], color=FALSE)
</code></pre>

<hr>
<h2 id='dist.Bernoulli'>Bernoulli Distribution</h2><span id='topic+dbern'></span><span id='topic+pbern'></span><span id='topic+qbern'></span><span id='topic+rbern'></span>

<h3>Description</h3>

<p>These functions provide the density, distribution function, quantile
function, and random generation for the Bernoulli distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dbern(x, prob, log=FALSE)
pbern(q, prob, lower.tail=TRUE, log.p=FALSE)
qbern(p, prob, lower.tail=TRUE, log.p=FALSE)
rbern(n, prob)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.Bernoulli_+3A_x">x</code>, <code id="dist.Bernoulli_+3A_q">q</code></td>
<td>
<p>These are each a vector of quantiles.</p>
</td></tr>
<tr><td><code id="dist.Bernoulli_+3A_p">p</code></td>
<td>
<p>This is a vector of probabilities.</p>
</td></tr>
<tr><td><code id="dist.Bernoulli_+3A_n">n</code></td>
<td>
<p>This is the number of observations. If <code>length(n) &gt; 1</code>,
then the length is taken to be the number required.</p>
</td></tr>
<tr><td><code id="dist.Bernoulli_+3A_prob">prob</code></td>
<td>
<p>This is the probability of success on each trial.</p>
</td></tr>
<tr><td><code id="dist.Bernoulli_+3A_log">log</code>, <code id="dist.Bernoulli_+3A_log.p">log.p</code></td>
<td>
<p>Logical. if <code>TRUE</code>, probabilities <code class="reqn">p</code> are
given as <code class="reqn">\log(p)</code>.</p>
</td></tr>
<tr><td><code id="dist.Bernoulli_+3A_lower.tail">lower.tail</code></td>
<td>
<p>Logical. if <code>TRUE</code> (default), probabilities
are <code class="reqn">Pr[X \le x]</code>, otherwise,
<code class="reqn">Pr[X &gt; x]</code>.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> Application: Continuous Univariate
</p>
</li>
<li><p> Density: <code class="reqn">p(\theta) = {p}^{\theta}
    {(1-p)}^{1-\theta}</code>, <code class="reqn">\theta = 0,1</code>
</p>
</li>
<li><p> Inventor: Jacob Bernoulli
</p>
</li>
<li><p> Notation 1: <code class="reqn">\theta \sim \mathcal{BERN}(p)</code>
</p>
</li>
<li><p> Notation 2: <code class="reqn">p(\theta) = \mathcal{BERN}(\theta | p)</code>
</p>
</li>
<li><p> Parameter 1: probability parameter <code class="reqn">0 \le p \le 1</code>
</p>
</li>
<li><p> Mean: <code class="reqn">E(\theta) = p</code>
</p>
</li>
<li><p> Variance: <code class="reqn">var(\theta) = \frac{p}{1-p}</code>
</p>
</li>
<li><p> Mode: <code class="reqn">mode(\theta) =</code>
</p>
</li></ul>

<p>The Bernoulli distribution is a binomial distribution with
<code class="reqn">n=1</code>, and one instance of a Bernoulli distribution is called a
Bernoulli trial. One coin flip is a Bernoulli trial, for example. The
categorical distribution is the generalization of the Bernoulli
distribution for variables with more than two discrete values. The
beta distribution is the conjugate prior distribution of the Bernoulli
distribution. The geometric distribution is the number of Bernoulli
trials needed to get one success.
</p>


<h3>Value</h3>

<p><code>dbern</code> gives the density,
<code>pbern</code> gives the distribution function,
<code>qbern</code> gives the quantile function, and
<code>rbern</code> generates random deviates.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+dbinom">dbinom</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
dbern(1, 0.7)
rbern(10, 0.5)
</code></pre>

<hr>
<h2 id='dist.Categorical'>Categorical Distribution</h2><span id='topic+dcat'></span><span id='topic+qcat'></span><span id='topic+rcat'></span>

<h3>Description</h3>

<p>This is the density and random deviates function for the categorical
distribution with probabilities parameter <code class="reqn">p</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dcat(x, p, log=FALSE)
qcat(pr, p, lower.tail=TRUE, log.pr=FALSE)
rcat(n, p)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.Categorical_+3A_x">x</code></td>
<td>
<p>This is a vector of discrete data with <code class="reqn">k</code> discrete
categories, and is of length <code class="reqn">n</code>. This function also accepts
<code class="reqn">x</code> after it has been converted to an <code class="reqn">n \times k</code>
indicator matrix, such as with the <code>as.indicator.matrix</code> function.</p>
</td></tr>
<tr><td><code id="dist.Categorical_+3A_n">n</code></td>
<td>
<p>This is the number of observations, which must be a positive
integer that has length 1. When <code>p</code> is supplied to <code>rcat</code>
as a matrix, <code>n</code> must equal the number of rows in <code>p</code>.</p>
</td></tr>
<tr><td><code id="dist.Categorical_+3A_p">p</code></td>
<td>
<p>This is a vector of length <code class="reqn">k</code> or <code class="reqn">n \times k</code>
matrix of probabilities. The <code>qcat</code> function requires a
vector.</p>
</td></tr>
<tr><td><code id="dist.Categorical_+3A_pr">pr</code></td>
<td>
<p>This is a vector of probabilities, or log-probabilities.</p>
</td></tr>
<tr><td><code id="dist.Categorical_+3A_log">log</code></td>
<td>
<p>Logical. If <code>log=TRUE</code>, then the logarithm of the
density is returned.</p>
</td></tr>
<tr><td><code id="dist.Categorical_+3A_log.pr">log.pr</code></td>
<td>
<p>Logical. if <code>TRUE</code>, probabilities <code class="reqn">pr</code> are
given as <code class="reqn">\log(pr)</code>.</p>
</td></tr>
<tr><td><code id="dist.Categorical_+3A_lower.tail">lower.tail</code></td>
<td>
<p>Logical. if <code>TRUE</code> (default), probabilities
are <code class="reqn">Pr[X \le x]</code>, otherwise,
<code class="reqn">Pr[X &gt; x]</code>.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> Application: Discrete Univariate
</p>
</li>
<li><p> Density: <code class="reqn">p(\theta) = \sum \theta p</code>
</p>
</li>
<li><p> Inventor: Unknown (to me, anyway)
</p>
</li>
<li><p> Notation 1: <code class="reqn">\theta \sim \mathcal{CAT}(p)</code>
</p>
</li>
<li><p> Notation 2: <code class="reqn">p(\theta) = \mathcal{CAT}(\theta | p)</code>
</p>
</li>
<li><p> Parameter 1: probabilities <code class="reqn">p</code>
</p>
</li>
<li><p> Mean: <code class="reqn">E(\theta)</code> = Unknown
</p>
</li>
<li><p> Variance: <code class="reqn">var(\theta)</code> = Unknown
</p>
</li>
<li><p> Mode: <code class="reqn">mode(\theta)</code> = Unknown
</p>
</li></ul>

<p>Also called the discrete distribution, the categorical distribution
describes the result of a random event that can take on one of <code class="reqn">k</code>
possible outcomes, with the probability <code class="reqn">p</code> of each outcome
separately specified. The vector <code class="reqn">p</code> of probabilities for each
event must sum to 1. The categorical distribution is often used, for
example, in the multinomial logit model. The conjugate prior is the
Dirichlet distribution.
</p>


<h3>Value</h3>

<p><code>dcat</code> gives the density and
<code>rcat</code> generates random deviates.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+as.indicator.matrix">as.indicator.matrix</a></code>,
<code><a href="#topic+ddirichlet">ddirichlet</a></code>, and
<code><a href="stats.html#topic+dmultinom">dmultinom</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
dcat(x=1, p=c(0.3,0.3,0.4))
rcat(n=10, p=c(0.1,0.3,0.6))
</code></pre>

<hr>
<h2 id='dist.ContinuousRelaxation'>Continuous Relaxation of a Markov Random Field Distribution</h2><span id='topic+dcrmrf'></span><span id='topic+rcrmrf'></span>

<h3>Description</h3>

<p>This is the density function and random generation from the continuous
relaxation of a Markov random field (MRF) distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dcrmrf(x, alpha, Omega, log=FALSE)
rcrmrf(n, alpha, Omega)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.ContinuousRelaxation_+3A_x">x</code></td>
<td>
<p>This is a vector of length <code class="reqn">k</code>.</p>
</td></tr>
<tr><td><code id="dist.ContinuousRelaxation_+3A_n">n</code></td>
<td>
<p>This is the number of random deviates to generate.</p>
</td></tr>
<tr><td><code id="dist.ContinuousRelaxation_+3A_alpha">alpha</code></td>
<td>
<p>This is a vector of length <code class="reqn">k</code> of shape parameters.</p>
</td></tr>
<tr><td><code id="dist.ContinuousRelaxation_+3A_omega">Omega</code></td>
<td>
<p>This is the <code class="reqn">k \times k</code> precision matrix
<code class="reqn">\Omega</code>.</p>
</td></tr>
<tr><td><code id="dist.ContinuousRelaxation_+3A_log">log</code></td>
<td>
<p>Logical. If <code>log=TRUE</code>, then the logarithm of the
density is returned.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> Application: Continuous Multivariate
</p>
</li>
<li><p> Density: </p>
<p style="text-align: center;"><code class="reqn">p(\theta) \propto \exp(-\frac{1}{2} \theta^T
      \Omega^{-1} \theta) \prod_i (1 + \exp(\theta_i +
      alpha_i))</code>
</p>

</li>
<li><p> Inventor: Zhang et al. (2012)
</p>
</li>
<li><p> Notation 1: <code class="reqn">\theta \sim \mathcal{CRMRF}(\alpha,
      \Omega)</code>
</p>
</li>
<li><p> Notation 2: <code class="reqn">p(\theta) = \mathcal{CRMRF}(\theta | \alpha,
      \Omega)</code>
</p>
</li>
<li><p> Parameter 1: shape vector <code class="reqn">\alpha</code>
</p>
</li>
<li><p> Parameter 2: positive-definite <code class="reqn">k \times k</code> matrix
<code class="reqn">\Omega</code>
</p>
</li>
<li><p> Mean: <code class="reqn">E(\theta)</code>
</p>
</li>
<li><p> Variance: <code class="reqn">var(\theta)</code>
</p>
</li>
<li><p> Mode: <code class="reqn">mode(\theta)</code>
</p>
</li></ul>

<p>It is often easier to solve or optimize a problem with continuous
variables rather than a problem that involves discrete variables. A
continuous variable may also have a gradient, contour, and curvature
that may be useful for optimization or sampling. Continuous MCMC
samplers are far more common.
</p>
<p>Zhang et al. (2012) introduced a generalized form of the Gaussian
integral trick from statistical physics to transform a discrete variable
so that it may be estimated with continuous variables. An auxiliary
Gaussian variable is added to a discrete Markov random field (MRF) so
that discrete dependencies cancel out, allowing the discrete variable to
be summed away, and leaving a continuous problem. The resulting
continuous representation of the problem allows the model to be updated
with a continuous MCMC sampler, and may benefit from a MCMC sampler that
uses derivatives. Another advantage of continuous MCMC is that
stationarity of discrete Markov chains is problematic to assess.
</p>
<p>A disadvantage of solving a discrete problem with continuous parameters
is that the continuous solution requires more parameters.
</p>


<h3>Value</h3>

<p><code>dcrmrf</code> gives the density and
<code>rcrmrf</code> generates random deviates.
</p>


<h3>References</h3>

<p>Zhang, Y., Ghahramani, Z., Storkey, A.J., and Sutton, C.A. (2012).
&quot;Continuous Relaxations for Discrete Hamiltonian Monte Carlo&quot;.
<em>Advances in Neural Information Processing Systems</em>, 25,
p. 3203&ndash;3211.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dmvn">dmvn</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
x &lt;- dcrmrf(rnorm(5), rnorm(5), diag(5))
x &lt;- rcrmrf(10, rnorm(5), diag(5))
</code></pre>

<hr>
<h2 id='dist.Dirichlet'>Dirichlet Distribution</h2><span id='topic+ddirichlet'></span><span id='topic+rdirichlet'></span>

<h3>Description</h3>

<p>This is the density function and random generation from the Dirichlet
distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ddirichlet(x, alpha, log=FALSE)
rdirichlet(n, alpha)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.Dirichlet_+3A_x">x</code></td>
<td>
<p>This is a vector containing a single deviate or matrix
containing one random deviate per row. Each vector, or matrix row,
must sum to 1.</p>
</td></tr>
<tr><td><code id="dist.Dirichlet_+3A_n">n</code></td>
<td>
<p>This is the number of random deviates to generate.</p>
</td></tr>
<tr><td><code id="dist.Dirichlet_+3A_alpha">alpha</code></td>
<td>
<p>This is a vector or matrix of shape parameters.</p>
</td></tr>
<tr><td><code id="dist.Dirichlet_+3A_log">log</code></td>
<td>
<p>Logical. If <code>log=TRUE</code>, then the logarithm of the
density is returned.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> Application: Continuous Multivariate
</p>
</li>
<li><p> Density: </p>
<p style="text-align: center;"><code class="reqn">p(\theta) = \frac{\gamma(\alpha_1 + \dots +
	\alpha_k)}{\gamma \alpha_1 \dots \gamma \alpha_k}
      \theta^{(\alpha[1]-1)}_1 \dots \theta^{(\alpha[k]-1)}_k, \quad
      \theta_1, \dots, \theta_k &gt; 0, \quad \sum^k_{j=1} \theta_j = 1</code>
</p>

</li>
<li><p> Inventor: Johann Peter Gustav Lejeune Dirichlet (1805-1859)
</p>
</li>
<li><p> Notation 1: <code class="reqn">\theta \sim</code>
Dirichlet(<code class="reqn">\alpha_1,\dots,\alpha_k</code>)
</p>
</li>
<li><p> Notation 2: <code class="reqn">p(\theta) =</code> Dirichlet(<code class="reqn">\theta
      | \alpha_1,\dots,\alpha_k</code>)
</p>
</li>
<li><p> Notation 3: <code class="reqn">\theta \sim
      \mathcal{DIR}(\alpha_1,\dots,\alpha_k)</code>
</p>
</li>
<li><p> Notation 4: <code class="reqn">p(\theta) = \mathcal{DIR}(\theta |
      \alpha_1,\dots,\alpha_k)</code>
</p>
</li>
<li><p> Parameter: 'prior sample sizes' <code class="reqn">\alpha_j &gt; 0, \alpha_0 =
      \sum^k_{j=1} \alpha_j</code>
</p>
</li>
<li><p> Mean: <code class="reqn">E(\theta_j) =
      \frac{\alpha_j}{\alpha_0}</code>
</p>
</li>
<li><p> Variance: <code class="reqn">var(\theta_j) = \frac{\alpha_j (\alpha_0 -
	\alpha_j)}{\alpha^2_0 (\alpha_0 + 1)}</code>
</p>
</li>
<li><p> Covariance: <code class="reqn">cov(\theta_i, \theta_j) = - \frac{\alpha_i
	\alpha_j}{\alpha^2_0 (\alpha_0 + 1)}</code>
</p>
</li>
<li><p> Mode: <code class="reqn">mode(\theta_j) = \frac{\alpha_j - 1}{\alpha_0 -
	k}</code>
</p>
</li></ul>

<p>The Dirichlet distribution is the multivariate generalization of the
univariate beta distribution. Its probability density function returns
the belief that the probabilities of <code class="reqn">k</code> rival events are
<code class="reqn">\theta_j</code> given that each event has been observed
<code class="reqn">\alpha_j - 1</code> times.
</p>
<p>The Dirichlet distribution is commonly used as a prior distribution in
Bayesian inference. The Dirichlet distribution is the conjugate prior
distribution for the parameters of the categorical and multinomial
distributions.
</p>
<p>A very common special case is the symmetric Dirichlet distribution,
where all of the elements in parameter vector <code class="reqn">\alpha</code> have
the same value. Symmetric Dirichlet distributions are often used as 
vague or weakly informative Dirichlet prior distributions, so that one
component is not favored over another. The single value that is entered
into all elements of <code class="reqn">\alpha</code> is called the concentration
parameter.
</p>


<h3>Value</h3>

<p><code>ddirichlet</code> gives the density and
<code>rdirichlet</code> generates random deviates.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+dbeta">dbeta</a></code>,
<code><a href="#topic+dcat">dcat</a></code>,
<code><a href="#topic+dmvpolya">dmvpolya</a></code>,
<code><a href="stats.html#topic+dmultinom">dmultinom</a></code>, and
<code><a href="#topic+TransitionMatrix">TransitionMatrix</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
x &lt;- ddirichlet(c(.1,.3,.6), c(1,1,1))
x &lt;- rdirichlet(10, c(1,1,1))
</code></pre>

<hr>
<h2 id='dist.Generalized.Pareto'>Generalized Pareto Distribution</h2><span id='topic+dgpd'></span><span id='topic+rgpd'></span>

<h3>Description</h3>

<p>These are the density and random generation functions for the
generalized Pareto distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dgpd(x, mu, sigma, xi, log=FALSE)
rgpd(n, mu, sigma, xi)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.Generalized.Pareto_+3A_x">x</code></td>
<td>
<p>This is a vector of data.</p>
</td></tr>
<tr><td><code id="dist.Generalized.Pareto_+3A_n">n</code></td>
<td>
<p>This is a positive scalar integer, and is the number of
observations to generate randomly.</p>
</td></tr>
<tr><td><code id="dist.Generalized.Pareto_+3A_mu">mu</code></td>
<td>
<p>This is a scalar or vector location parameter
<code class="reqn">\mu</code>. When <code class="reqn">\xi</code> is non-negative,
<code class="reqn">\mu</code> must not be greater than <code class="reqn">\textbf{x}</code>. When
<code class="reqn">\xi</code> is negative, <code class="reqn">\mu</code> must be less than
<code class="reqn">\textbf{x} + \sigma / \xi</code>.</p>
</td></tr>
<tr><td><code id="dist.Generalized.Pareto_+3A_sigma">sigma</code></td>
<td>
<p>This is a positive-only scalar or vector of scale
parameters <code class="reqn">\sigma</code>.</p>
</td></tr>
<tr><td><code id="dist.Generalized.Pareto_+3A_xi">xi</code></td>
<td>
<p>This is a scalar or vector of shape parameters
<code class="reqn">\xi</code>.</p>
</td></tr>
<tr><td><code id="dist.Generalized.Pareto_+3A_log">log</code></td>
<td>
<p>Logical. If <code>log=TRUE</code>, then the logarithm of the
density is returned.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> Application: Continuous Univariate
</p>
</li>
<li><p> Density: <code class="reqn">p(\theta) = \frac{1}{\sigma}(1 +
      \xi\textbf{z})^(-1/\xi + 1)</code> where <code class="reqn">\textbf{z} = \frac{\theta - \mu}{\sigma}</code>
</p>
</li>
<li><p> Inventor: Pickands (1975)
</p>
</li>
<li><p> Notation 1: <code class="reqn">\theta \sim \mathcal{GPD}(\mu, \sigma,
      \xi)</code>
</p>
</li>
<li><p> Notation 2: <code class="reqn">p(\theta) \sim \mathcal{GPD}(\theta |
      \mu, \sigma, \xi)</code>
</p>
</li>
<li><p> Parameter 1: location <code class="reqn">\mu</code>, where <code class="reqn">\mu \le
      \theta</code> when <code class="reqn">\xi \ge 0</code>, and
<code class="reqn">\mu \ge \theta + \sigma / \xi</code>
when <code class="reqn">\xi &lt; 0</code>
</p>
</li>
<li><p> Parameter 2: scale <code class="reqn">\sigma &gt; 0</code>
</p>
</li>
<li><p> Parameter 3: shape <code class="reqn">\xi</code>
</p>
</li>
<li><p> Mean: <code class="reqn">\mu + \frac{\sigma}{1 - \xi}</code> when <code class="reqn">\xi &lt; 1</code>
</p>
</li>
<li><p> Variance: <code class="reqn">\frac{\sigma^2}{(1 - \xi)^2 (1 -
      2\xi)}</code> when <code class="reqn">\xi
      &lt; 0.5</code>
</p>
</li>
<li><p> Mode: 
</p>
</li></ul>

<p>The generalized Pareto distribution (GPD) is a more flexible extension
of the Pareto (<code><a href="#topic+dpareto">dpareto</a></code>) distribution. It is equivalent to
the exponential distribution when both <code class="reqn">\mu = 0</code> and
<code class="reqn">\xi = 0</code>, and it is equivalent to the Pareto
distribution when <code class="reqn">\mu = \sigma / \xi</code> and
<code class="reqn">\xi &gt; 0</code>.
</p>
<p>The GPD is often used to model the tails of another distribution, and
the shape parameter <code class="reqn">\xi</code> relates to
tail-behavior. Distributions with tails that decrease exponentially are
modeled with shape <code class="reqn">\xi = 0</code>. Distributions with tails that
decrease as a polynomial are modeled with a positive shape
parameter. Distributions with finite tails are modeled with a negative
shape parameter.
</p>


<h3>Value</h3>

<p><code>dgpd</code> gives the density, and
<code>rgpd</code> generates random deviates.
</p>


<h3>References</h3>

<p>Pickands J. (1975). &quot;Statistical Inference Using Extreme Order
Statistics&quot;. <em>The Annals of Statistics</em>, 3, p. 119&ndash;131.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dpareto">dpareto</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
x &lt;- dgpd(0,0,1,0,log=TRUE)
x &lt;- rgpd(10,0,1,0)
</code></pre>

<hr>
<h2 id='dist.Generalized.Poisson'>Generalized Poisson Distribution</h2><span id='topic+dgpois'></span>

<h3>Description</h3>

<p>The density function is provided for the univariate, discrete,
generalized Poisson distribution with location parameter
<code class="reqn">\lambda</code> and scale parameter <code class="reqn">\omega</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dgpois(x, lambda=0, omega=0, log=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.Generalized.Poisson_+3A_x">x</code></td>
<td>
<p>This is a vector of quantiles.</p>
</td></tr>
<tr><td><code id="dist.Generalized.Poisson_+3A_lambda">lambda</code></td>
<td>
<p>This is the parameter <code class="reqn">\lambda</code>.</p>
</td></tr>
<tr><td><code id="dist.Generalized.Poisson_+3A_omega">omega</code></td>
<td>
<p>This is the parameter <code class="reqn">\omega</code>, which
should be in the interval [0,1) for positive counts.</p>
</td></tr>
<tr><td><code id="dist.Generalized.Poisson_+3A_log">log</code></td>
<td>
<p>Logical. If <code>log=TRUE</code>, then the logarithm of the
density is returned.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> Application: Discrete Univariate
</p>
</li>
<li><p> Density: <code class="reqn">p(\theta) = (1 - \omega) \lambda \frac{[(1 -
      \omega) \lambda + \omega \theta]^{\theta - 1}}{\theta!} \exp{-[(1
      - \omega) \lambda + \omega \theta]}</code>
</p>
</li>
<li><p> Inventor: Consul (1989) and Ntzoufras et al. (2005)
</p>
</li>
<li><p> Notation 1: <code class="reqn">\theta \sim
    \mathrm{GP}(\lambda,\omega)</code>
</p>
</li>
<li><p> Notation 2: <code class="reqn">p(\theta) = \mathrm{GP}(\theta | \lambda,
    \omega)</code>
</p>
</li>
<li><p> Parameter 1: location parameter <code class="reqn">\lambda</code>
</p>
</li>
<li><p> Parameter 2: scale parameter <code class="reqn">\omega \in [0,1)</code>
</p>
</li>
<li><p> Mean: <code class="reqn">E(\theta) = \lambda</code>
</p>
</li>
<li><p> Variance: <code class="reqn">var(\theta) = \lambda(1 -
    \omega)^{-2}</code>
</p>
</li></ul>

<p>The generalized Poisson distribution (Consul, 1989) is also called the
Lagrangian Poisson distribution. The simple Poisson distribution is a
special case of the generalized Poisson distribution. The generalized
Poisson distribution is used in generalized Poisson regression as an
extension of Poisson regression that accounts for overdispersion.
</p>
<p>The <code>dgpois</code> function is parameterized according to Ntzoufras et
al. (2005), which is easier to interpret and estimates better with MCMC.
</p>
<p>Valid values for omega are in the interval [0,1) for positive counts.
For <code class="reqn">\omega = 0</code>, the generalized Poisson reduces to a
simple Poisson with mean <code class="reqn">\lambda</code>. Note that it is possible
for <code class="reqn">\omega &lt; 0</code>, but this implies underdispersion in
count data, which is uncommon. The <code>dgpois</code> function returns
warnings or errors, so <code class="reqn">\omega</code> should be non-negative here.
</p>
<p>The dispersion index (DI) is a variance-to-mean ratio, and is <code class="reqn">DI =
  (1 - \omega)^{-2}</code>. A simple Poisson has DI=1.
When DI is far from one, the assumption that the variance equals the
mean of a simple Poisson is violated.
</p>


<h3>Value</h3>

<p><code>dgpois</code> gives the density.
</p>


<h3>References</h3>

<p>Consul, P. (1989). '&quot;Generalized Poisson Distribution: Properties and
Applications&quot;. Marcel Decker: New York, NY.
</p>
<p>Ntzoufras, I., Katsis, A., and Karlis, D. (2005). &quot;Bayesian Assessment
of the Distribution of Insurance Claim Counts using Reversible Jump
MCMC&quot;, <em>North American Actuarial Journal</em>, 9, p. 90&ndash;108.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+dnbinom">dnbinom</a></code> and
<code><a href="stats.html#topic+dpois">dpois</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
y &lt;- rpois(100, 5)
lambda &lt;- rpois(100, 5)
x &lt;- dgpois(y, lambda, 0.5)

#Plot Probability Functions
x &lt;- seq(from=0, to=20, by=1)
plot(x, dgpois(x,1,0.5), ylim=c(0,1), type="l", main="Probability Function",
     ylab="density", col="red")
lines(x, dlaplace(x,1,0.6), type="l", col="green")
lines(x, dlaplace(x,1,0.7), type="l", col="blue")
legend(2, 0.9, expression(paste(lambda==1, ", ", omega==0.5),
     paste(lambda==1, ", ", omega==0.6), paste(lambda==1, ", ", omega==0.7)),
     lty=c(1,1,1), col=c("red","green","blue"))
</code></pre>

<hr>
<h2 id='dist.HalfCauchy'>Half-Cauchy Distribution</h2><span id='topic+dhalfcauchy'></span><span id='topic+phalfcauchy'></span><span id='topic+qhalfcauchy'></span><span id='topic+rhalfcauchy'></span>

<h3>Description</h3>

<p>These functions provide the density, distribution function, quantile
function, and random generation for the half-Cauchy distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dhalfcauchy(x, scale=25, log=FALSE)
phalfcauchy(q, scale=25)
qhalfcauchy(p, scale=25)
rhalfcauchy(n, scale=25)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.HalfCauchy_+3A_x">x</code>, <code id="dist.HalfCauchy_+3A_q">q</code></td>
<td>
<p>These are each a vector of quantiles.</p>
</td></tr>
<tr><td><code id="dist.HalfCauchy_+3A_p">p</code></td>
<td>
<p>This is a vector of probabilities.</p>
</td></tr>
<tr><td><code id="dist.HalfCauchy_+3A_n">n</code></td>
<td>
<p>This is the number of observations, which must be a positive
integer that has length 1.</p>
</td></tr>
<tr><td><code id="dist.HalfCauchy_+3A_scale">scale</code></td>
<td>
<p>This is the scale parameter <code class="reqn">\alpha</code>, which
must be positive.</p>
</td></tr>
<tr><td><code id="dist.HalfCauchy_+3A_log">log</code></td>
<td>
<p>Logical. If <code>log=TRUE</code>, then the logarithm of the
density is returned.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> Application: Continuous Univariate
</p>
</li>
<li><p> Density: <code class="reqn">p(\theta) = \frac{2 \alpha}{\pi(\theta^2 +
      \alpha^2)}, \quad \theta &gt; 0</code>
</p>
</li>
<li><p> Inventor: Derived from Cauchy
</p>
</li>
<li><p> Notation 1: <code class="reqn">\theta \sim \mathcal{HC}(\alpha)</code>
</p>
</li>
<li><p> Notation 2: <code class="reqn">p(\theta) = \mathcal{HC}(\theta | \alpha)</code>
</p>
</li>
<li><p> Parameter 1: scale parameter <code class="reqn">\alpha &gt; 0</code>
</p>
</li>
<li><p> Mean: <code class="reqn">E(\theta)</code> = does not exist
</p>
</li>
<li><p> Variance: <code class="reqn">var(\theta)</code> = does not exist
</p>
</li>
<li><p> Mode: <code class="reqn">mode(\theta) = 0</code>
</p>
</li></ul>

<p>The half-Cauchy distribution with scale <code class="reqn">\alpha=25</code> is a
recommended, default, weakly informative prior distribution for a scale
parameter. Otherwise, the scale, <code class="reqn">\alpha</code>, is recommended to
be set to be just a little larger than the expected standard deviation,
as a weakly informative prior distribution on a standard deviation
parameter.
</p>
<p>The Cauchy distribution is known as a pathological distribution because
its mean and variance are undefined, and it does not satisfy the central
limit theorem.
</p>


<h3>Value</h3>

<p><code>dhalfcauchy</code> gives the density,
<code>phalfcauchy</code> gives the distribution function,
<code>qhalfcauchy</code> gives the quantile function, and
<code>rhalfcauchy</code> generates random deviates.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+dcauchy">dcauchy</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
x &lt;- dhalfcauchy(1,25)
x &lt;- phalfcauchy(1,25)
x &lt;- qhalfcauchy(0.5,25)
x &lt;- rhalfcauchy(1,25)

#Plot Probability Functions
x &lt;- seq(from=0, to=20, by=0.1)
plot(x, dhalfcauchy(x,1), ylim=c(0,1), type="l", main="Probability Function",
     ylab="density", col="red")
lines(x, dhalfcauchy(x,5), type="l", col="green")
lines(x, dhalfcauchy(x,10), type="l", col="blue")
legend(2, 0.9, expression(alpha==1, alpha==5, alpha==10),
     lty=c(1,1,1), col=c("red","green","blue"))
</code></pre>

<hr>
<h2 id='dist.HalfNormal'>Half-Normal Distribution</h2><span id='topic+dhalfnorm'></span><span id='topic+phalfnorm'></span><span id='topic+qhalfnorm'></span><span id='topic+rhalfnorm'></span>

<h3>Description</h3>

<p>These functions provide the density, distribution function, quantile
function, and random generation for the half-normal distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dhalfnorm(x, scale=sqrt(pi/2), log=FALSE)
phalfnorm(q, scale=sqrt(pi/2), lower.tail=TRUE, log.p=FALSE)
qhalfnorm(p, scale=sqrt(pi/2), lower.tail=TRUE, log.p=FALSE)
rhalfnorm(n, scale=sqrt(pi/2))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.HalfNormal_+3A_x">x</code>, <code id="dist.HalfNormal_+3A_q">q</code></td>
<td>
<p>These are each a vector of quantiles.</p>
</td></tr>
<tr><td><code id="dist.HalfNormal_+3A_p">p</code></td>
<td>
<p>This is a vector of probabilities.</p>
</td></tr>
<tr><td><code id="dist.HalfNormal_+3A_n">n</code></td>
<td>
<p>This is the number of observations, which must be a positive
integer that has length 1.</p>
</td></tr>
<tr><td><code id="dist.HalfNormal_+3A_scale">scale</code></td>
<td>
<p>This is the scale parameter <code class="reqn">\sigma</code>, which
must be positive.</p>
</td></tr>
<tr><td><code id="dist.HalfNormal_+3A_log">log</code>, <code id="dist.HalfNormal_+3A_log.p">log.p</code></td>
<td>
<p>Logical. If <code>log=TRUE</code>, then the logarithm of the
density or result is returned.</p>
</td></tr>
<tr><td><code id="dist.HalfNormal_+3A_lower.tail">lower.tail</code></td>
<td>
<p>Logical. If <code>lower.tail=TRUE</code> (default),
probabilities are <code class="reqn">Pr[X \le x]</code>, otherwise,
<code class="reqn">Pr[X &gt; x]</code>.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> Application: Continuous Univariate
</p>
</li>
<li><p> Density: <code class="reqn">p(\theta) = \frac{2 \sigma}{\pi}
    \exp(-\frac{\theta^2 \sigma^2}{\pi}), \quad \theta \ge 0</code>
</p>
</li>
<li><p> Inventor: Derived from the normal or Gaussian
</p>
</li>
<li><p> Notation 1: <code class="reqn">\theta \sim \mathcal{HN}(\sigma)</code>
</p>
</li>
<li><p> Notation 2: <code class="reqn">p(\theta) = \mathcal{HN}(\theta | \sigma)</code>
</p>
</li>
<li><p> Parameter 1: scale parameter <code class="reqn">\sigma &gt; 0</code>
</p>
</li>
<li><p> Mean: <code class="reqn">E(\theta) = \frac{1}{\sigma}</code>
</p>
</li>
<li><p> Variance: <code class="reqn">var(\theta) = \frac{\pi-2}{2 \sigma^2}</code>
</p>
</li>
<li><p> Mode: <code class="reqn">mode(\theta) = 0</code>
</p>
</li></ul>

<p>The half-normal distribution is recommended as a weakly informative prior
distribution for a scale parameter that may be useful as an alternative
to the half-Cauchy, half-t, or vague gamma.
</p>


<h3>Value</h3>

<p><code>dhalfnorm</code> gives the density,
<code>phalfnorm</code> gives the distribution function,
<code>qhalfnorm</code> gives the quantile function, and
<code>rhalfnorm</code> generates random deviates.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+dnorm">dnorm</a></code>,
<code><a href="#topic+dnormp">dnormp</a></code>, and
<code><a href="#topic+dnormv">dnormv</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
x &lt;- dhalfnorm(1)
x &lt;- phalfnorm(1)
x &lt;- qhalfnorm(0.5)
x &lt;- rhalfnorm(10)

#Plot Probability Functions
x &lt;- seq(from=0.1, to=20, by=0.1)
plot(x, dhalfnorm(x,0.1), ylim=c(0,1), type="l", main="Probability Function",
     ylab="density", col="red")
lines(x, dhalfnorm(x,0.5), type="l", col="green")
lines(x, dhalfnorm(x,1), type="l", col="blue")
legend(2, 0.9, expression(sigma==0.1, sigma==0.5, sigma==1),
     lty=c(1,1,1), col=c("red","green","blue"))
</code></pre>

<hr>
<h2 id='dist.Halft'>Half-t Distribution</h2><span id='topic+dhalft'></span><span id='topic+phalft'></span><span id='topic+qhalft'></span><span id='topic+rhalft'></span>

<h3>Description</h3>

<p>These functions provide the density, distribution function, quantile function, and random generation for the half-t distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dhalft(x, scale=25, nu=1, log=FALSE)
phalft(q, scale=25, nu=1)
qhalft(p, scale=25, nu=1)
rhalft(n, scale=25, nu=1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.Halft_+3A_x">x</code>, <code id="dist.Halft_+3A_q">q</code></td>
<td>
<p>These are each a vector of quantiles.</p>
</td></tr>
<tr><td><code id="dist.Halft_+3A_p">p</code></td>
<td>
<p>This is a vector of probabilities.</p>
</td></tr>
<tr><td><code id="dist.Halft_+3A_n">n</code></td>
<td>
<p>This is the number of observations, which must be a positive
integer that has length 1.</p>
</td></tr>
<tr><td><code id="dist.Halft_+3A_scale">scale</code></td>
<td>
<p>This is the scale parameter <code class="reqn">\alpha</code>, which
must be positive.</p>
</td></tr>
<tr><td><code id="dist.Halft_+3A_nu">nu</code></td>
<td>
<p>This is the scalar degrees of freedom parameter, which is
usually represented as <code class="reqn">\nu</code>.</p>
</td></tr>
<tr><td><code id="dist.Halft_+3A_log">log</code></td>
<td>
<p>Logical. If <code>log=TRUE</code> then the logarithm of the
density is returned.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> Application: Continuous Univariate
</p>
</li>
<li><p> Density: <code class="reqn">p(\theta) = (1 + \frac{1}{\nu} (\theta /
    \alpha)^2)^{-(\nu+1)/2}, \quad \theta \ge 0</code>
</p>
</li>
<li><p> Inventor: Derived from the Student t
</p>
</li>
<li><p> Notation 1: <code class="reqn">\theta \sim \mathcal{HT}(\alpha, \nu)</code>
</p>
</li>
<li><p> Notation 2: <code class="reqn">p(\theta) = \mathcal{HT}(\theta | \alpha,
    \nu)</code>
</p>
</li>
<li><p> Parameter 1: scale parameter <code class="reqn">\alpha &gt; 0</code>
</p>
</li>
<li><p> Parameter 2: degrees of freedom parameter <code class="reqn">\nu</code>
</p>
</li>
<li><p> Mean: <code class="reqn">E(\theta)</code> = unknown
</p>
</li>
<li><p> Variance: <code class="reqn">var(\theta)</code> = unknown
</p>
</li>
<li><p> Mode: <code class="reqn">mode(\theta) = 0</code>
</p>
</li></ul>

<p>The half-t distribution is derived from the Student t distribution, and
is useful as a weakly informative prior distribution for a scale
parameter. It is more adaptable than the default recommended
half-Cauchy, though it may also be more difficult to estimate due to its
additional degrees of freedom parameter, <code class="reqn">\nu</code>. When
<code class="reqn">\nu=1</code>, the density is proportional to a proper half-Cauchy
distribution. When <code class="reqn">\nu=-1</code>, the density becomes an improper,
uniform prior distribution. For more information on propriety, see
<code>is.proper</code>.
</p>
<p>Wand et al. (2011) demonstrated that the half-t distribution may be
represented as a scale mixture of inverse-gamma distributions. This
representation is useful for conjugacy.
</p>


<h3>Value</h3>

<p><code>dhalft</code> gives the density,
<code>phalft</code> gives the distribution function,
<code>qhalft</code> gives the quantile function, and
<code>rhalft</code> generates random deviates.
</p>


<h3>References</h3>

<p>Wand, M.P., Ormerod, J.T., Padoan, S.A., and Fruhwirth, R. (2011).
&quot;Mean Field Variational Bayes for Elaborate Distributions&quot;.
<em>Bayesian Analysis</em>, 6: p. 847&ndash;900.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dhalfcauchy">dhalfcauchy</a></code>,
<code><a href="#topic+dst">dst</a></code>,
<code><a href="stats.html#topic+dt">dt</a></code>,
<code><a href="stats.html#topic+dunif">dunif</a></code>, and
<code><a href="#topic+is.proper">is.proper</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
x &lt;- dhalft(1,25,1)
x &lt;- phalft(1,25,1)
x &lt;- qhalft(0.5,25,1)
x &lt;- rhalft(10,25,1)

#Plot Probability Functions
x &lt;- seq(from=0.1, to=20, by=0.1)
plot(x, dhalft(x,1,-1), ylim=c(0,1), type="l", main="Probability Function",
     ylab="density", col="red")
lines(x, dhalft(x,1,0.5), type="l", col="green")
lines(x, dhalft(x,1,500), type="l", col="blue")
legend(2, 0.9, expression(paste(alpha==1, ", ", nu==-1),
     paste(alpha==1, ", ", nu==0.5), paste(alpha==1, ", ", nu==500)),
     lty=c(1,1,1), col=c("red","green","blue"))
</code></pre>

<hr>
<h2 id='dist.Horseshoe'>Horseshoe Distribution</h2><span id='topic+dhs'></span><span id='topic+rhs'></span>

<h3>Description</h3>

<p>This is the density function and random generation from the horseshoe
distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dhs(x, lambda, tau, log=FALSE)
rhs(n, lambda, tau)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.Horseshoe_+3A_n">n</code></td>
<td>
<p>This is the number of draws from the distribution.</p>
</td></tr>
<tr><td><code id="dist.Horseshoe_+3A_x">x</code></td>
<td>
<p>This is a location vector at which to evaluate density.</p>
</td></tr>
<tr><td><code id="dist.Horseshoe_+3A_lambda">lambda</code></td>
<td>
<p>This vector is a positive-only local parameter
<code class="reqn">\lambda</code>.</p>
</td></tr>
<tr><td><code id="dist.Horseshoe_+3A_tau">tau</code></td>
<td>
<p>This scalar is a positive-only global parameter
<code class="reqn">\tau</code>.</p>
</td></tr>
<tr><td><code id="dist.Horseshoe_+3A_log">log</code></td>
<td>
<p>Logical. If <code>log=TRUE</code>, then the logarithm of the
density is returned.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> Application: Multivariate Scale Mixture
</p>
</li>
<li><p> Density: (see below)
</p>
</li>
<li><p> Inventor: Carvalho et al. (2008)
</p>
</li>
<li><p> Notation 1: <code class="reqn">\theta \sim \mathcal{HS}(\lambda,
      \tau)</code>
</p>
</li>
<li><p> Notation 2: <code class="reqn">p(\theta) = \mathcal{HS}(\theta | \lambda,
      \tau)</code>
</p>
</li>
<li><p> Parameter 1: local scale <code class="reqn">\lambda &gt; 0</code>
</p>
</li>
<li><p> Parameter 2: global scale <code class="reqn">\tau &gt; 0</code>
</p>
</li>
<li><p> Mean: <code class="reqn">E(\theta)</code>
</p>
</li>
<li><p> Variance: <code class="reqn">var(\theta)</code>
</p>
</li>
<li><p> Mode: <code class="reqn">mode(\theta)</code>
</p>
</li></ul>

<p>The horseshoe distribution (Carvalho et al., 2008) is a heavy-tailed
mixture distribution that can be considered a variance mixture,
and it is in the family of multivariate scale mixtures of normals.
</p>
<p>The horseshoe distribution was proposed as a prior distribution, and
recommended as a default choice for shrinkage priors in the presence of
sparsity. Horseshoe priors are most appropriate in large-p models where
dimension reduction is necessary to avoid overly complex models that
predict poorly, and also perform well in estimating a sparse covariance
matrix via Cholesky decomposition (Carvalho et al., 2009).
</p>
<p>When the number of parameters in variable selection is assumed to be
sparse, meaning that most elements are zero or nearly zero, a horseshoe
prior is a desirable alternative to the Laplace-distributed parameters
in the LASSO, or the parameterization in ridge regression. When the true
value is far from zero, the horseshoe prior leaves the parameter
unshrunk. Yet, the horseshoe prior is accurate in shrinking parameters
that are truly zero or near-zero. Parameters near zero are shrunk more
than parameters far from zero. Therefore, parameters far from zero
experience less shrinkage and are closer to their true values. The
horseshoe prior is valuable in discriminating signal from noise.
</p>
<p>By replacing the Laplace-distributed parameters in LASSO with
horseshoe-distributed parameters and including a global scale, the
result is called horseshoe regression.
</p>


<h3>Value</h3>

<p><code>dhs</code> gives the density and
<code>rhs</code> generates random deviates.
</p>


<h3>References</h3>

<p>Carvalho, C.M., Polson, N.G., and Scott, J.G. (2008). &quot;The Horseshoe
Estimator for Sparse Signals&quot;. <em>Discussion Paper 2008-31</em>. Duke
University Department of Statistical Science.
</p>
<p>Carvalho, C.M., Polson, N.G., and Scott, J.G. (2009). &quot;Handling
Sparsity via the Horseshoe&quot;. <em>Journal of Machine Learning
Research</em>, 5, p. 73&ndash;80.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dlaplace">dlaplace</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
x &lt;- rnorm(100)
lambda &lt;- rhalfcauchy(100, 5)
tau &lt;- 5
x &lt;- dhs(x, lambda, tau, log=TRUE)
x &lt;- rhs(100, lambda=lambda, tau=tau)
plot(density(x))
</code></pre>

<hr>
<h2 id='dist.HuangWand'>Huang-Wand Distribution</h2><span id='topic+dhuangwand'></span><span id='topic+dhuangwandc'></span><span id='topic+rhuangwand'></span><span id='topic+rhuangwandc'></span>

<h3>Description</h3>

<p>These are the density and random generation functions for the
Huang-Wand prior distribution for a covariance matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dhuangwand(x, nu=2, a, A, log=FALSE)
dhuangwandc(x, nu=2, a, A, log=FALSE)
rhuangwand(nu=2, a, A)
rhuangwandc(nu=2, a, A)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.HuangWand_+3A_x">x</code></td>
<td>
<p>This is a <code class="reqn">k \times k</code> positive-definite
covariance matrix <code class="reqn">\Sigma</code> for <code>dhuangwand</code>, or the
Cholesky factor <code class="reqn">\textbf{U}</code> of the covariance matrix for
<code>dhuangwandc</code>.</p>
</td></tr>
<tr><td><code id="dist.HuangWand_+3A_nu">nu</code></td>
<td>
<p>This is a scalar degrees of freedom parameter
<code class="reqn">\nu</code>. The default is <code>nu=2</code>, which is an
uninformative prior, resulting in marginal uniform distributions
on the correlation matrix.</p>
</td></tr>
<tr><td><code id="dist.HuangWand_+3A_a">a</code></td>
<td>
<p>This is a positive-only vector of scale parameters
<code class="reqn">a</code> of length <code class="reqn">k</code>.</p>
</td></tr>
<tr><td><code id="dist.HuangWand_+3A_a">A</code></td>
<td>
<p>This is a positive-only vector of of scale hyperparameters
<code class="reqn">A</code> of length <code class="reqn">k</code>. Larger values result in a more
uninformative prior. A default, uninformative prior is
<code>A=rep(1e6,k)</code>.</p>
</td></tr>
<tr><td><code id="dist.HuangWand_+3A_log">log</code></td>
<td>
<p>Logical. If <code>log=TRUE</code>, then the logarithm of the
density is returned.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> Application: Continuous Multivariate
</p>
</li>
<li><p> Density: <code class="reqn">p(\theta) =
      \mathcal{W}^{-1}_{\nu+k-1}(2 \nu diag(1/a))
      \mathcal{G}^{-1}(1/2, 1/A^2)</code>
</p>
</li>
<li><p> Inventor: Huang and Wand (2013)
</p>
</li>
<li><p> Notation 1: <code class="reqn">\theta \sim \mathcal{HW}_\nu(\textbf{a},
      \textbf{A})</code>
</p>
</li>
<li><p> Notation 2: <code class="reqn">p(\theta) \sim \mathcal{HW}_\nu(\theta |
      \textbf{a}, \textbf{A})</code>
</p>
</li>
<li><p> Parameter 1: degrees of freedom <code class="reqn">\nu</code>
</p>
</li>
<li><p> Parameter 2: scale <code class="reqn">a &gt; 0</code>
</p>
</li>
<li><p> Parameter 3: scale <code class="reqn">A &gt; 0</code>
</p>
</li>
<li><p> Mean: 
</p>
</li>
<li><p> Variance: 
</p>
</li>
<li><p> Mode: 
</p>
</li></ul>

<p>Huang and Wand (2013) proposed a prior distribution for a covariance
matrix that uses a hierarchical inverse Wishart. This is a more flexible
alternative to the inverse Wishart distribution, and the Huang-Wand
prior retains conjugacy. The Cholesky parameterization is also provided
here.
</p>
<p>The Huang-Wand prior distribution alleviates two main limitations of an
inverse Wishart distribution. First, the uncertainty in the diagonal
variances of a covariance matrix that is inverse Wishart distributed is
represented with only one degrees of freedom parameter, which may be too
restrictive. The Huang-Wand prior overcomes this limitation. Second, the
inverse Wishart distribution imposes a dependency between variance and
correlation. The Huang-Wand prior lessens, but does not fully remove,
this dependency.
</p>
<p>The standard deviations of a Huang-Wand distributed covariance matrix
are half-t distributed, as <code class="reqn">\mathcal{HT}(\nu, \textbf{A})</code>. This is in accord with modern assumptions about distributions of
scale parameters, and is also useful for sparse covariance matrices.
</p>
<p>The <code>rhuangwand</code> function allows either <code>a</code> or <code>A</code> to be
missing. When <code>a</code> is missing, the covariance matrix is generated
from the hyperparameters. When <code>A</code> is missing, the covariance
matrix is generated from the parameters.
</p>


<h3>Value</h3>

<p><code>dhuangwand</code> and <code>dhuangwandc</code> give the density, and
<code>rhuangwand</code> and <code>rhuangwandc</code> generate random deviates.
</p>


<h3>References</h3>

<p>Huang, A., Wand, M., et al. (2013), &quot;Simple Marginally Noninformative
Prior Distributions for Covariance Matrices&quot;. <em>Bayesian
Analysis</em>, 8, p. 439&ndash;452.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dhalft">dhalft</a></code> and
<code><a href="#topic+dinvwishart">dinvwishart</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
dhuangwand(diag(3), nu=2, a=runif(3), A=rep(1e6,3), log=TRUE)
rhuangwand(nu=2, A=rep(1e6, 3)) #Missing a
rhuangwand(nu=2, a=runif(3)) #Missing A
</code></pre>

<hr>
<h2 id='dist.Inverse.Beta'>Inverse Beta Distribution</h2><span id='topic+dinvbeta'></span><span id='topic+rinvbeta'></span>

<h3>Description</h3>

<p>This is the density function and random generation from the inverse
beta distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dinvbeta(x, a, b, log=FALSE)
rinvbeta(n, a, b)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.Inverse.Beta_+3A_n">n</code></td>
<td>
<p>This is the number of draws from the distribution.</p>
</td></tr>
<tr><td><code id="dist.Inverse.Beta_+3A_x">x</code></td>
<td>
<p>This is a location vector at which to evaluate density.</p>
</td></tr>
<tr><td><code id="dist.Inverse.Beta_+3A_a">a</code></td>
<td>
<p>This is the scalar shape parameter <code class="reqn">\alpha</code>.</p>
</td></tr>
<tr><td><code id="dist.Inverse.Beta_+3A_b">b</code></td>
<td>
<p>This is the scalar shape parameter <code class="reqn">\beta</code></p>
</td></tr>
<tr><td><code id="dist.Inverse.Beta_+3A_log">log</code></td>
<td>
<p>Logical. If <code>log=TRUE</code>, then the logarithm of the
density is returned.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> Application: Continuous Univariate
</p>
</li>
<li><p> Density: <code class="reqn">p(\theta) = \frac{\theta^{\alpha - 1} (1 +
	\theta)^{-\alpha - \beta}}{\beta(\alpha, \beta)}</code>
</p>
</li>
<li><p> Inventor: Dubey (1970)
</p>
</li>
<li><p> Notation 1: <code class="reqn">\theta \sim \mathcal{B}^{-1}(\alpha, \beta)</code>
</p>
</li>
<li><p> Notation 2: <code class="reqn">p(\theta) = \mathcal{B}^{-1}(\theta | \alpha,
    \beta)</code>
</p>
</li>
<li><p> Parameter 1: shape <code class="reqn">\alpha &gt; 0</code>
</p>
</li>
<li><p> Parameter 2: shape <code class="reqn">\beta &gt; 0</code>
</p>
</li>
<li><p> Mean: <code class="reqn">E(\theta) = \frac{\alpha}{\beta - 1}</code>, for <code class="reqn">\beta &gt; 1</code>
</p>
</li>
<li><p> Variance: <code class="reqn">var(\theta) = \frac{\alpha(\alpha + \beta -
	1)}{(\beta - 1)^2 (\beta - 2)}</code>
</p>
</li>
<li><p> Mode: <code class="reqn">mode(\theta) = \frac{\alpha - 1}{\beta +
	1}</code>
</p>
</li></ul>

<p>The inverse-beta, also called the beta prime distribution, applies to
variables that are continuous and positive. The inverse beta is the
conjugate prior distribution of a parameter of a Bernoulli distribution
expressed in odds.
</p>
<p>The inverse-beta distribution has also been extended to the generalized
beta prime distribution, though it is not (yet) included here.
</p>


<h3>Value</h3>

<p><code>dinvbeta</code> gives the density and
<code>rinvbeta</code> generates random deviates.
</p>


<h3>References</h3>

<p>Dubey, S.D. (1970). &quot;Compound Gamma, Beta and F Distributions&quot;.
<em>Metrika</em>, 16, p. 27&ndash;31.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+dbeta">dbeta</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
x &lt;- dinvbeta(5:10, 2, 3)
x &lt;- rinvbeta(10, 2, 3)

#Plot Probability Functions
x &lt;- seq(from=0.1, to=20, by=0.1)
plot(x, dinvbeta(x,2,2), ylim=c(0,1), type="l", main="Probability Function",
     ylab="density", col="red")
lines(x, dinvbeta(x,2,3), type="l", col="green")
lines(x, dinvbeta(x,3,2), type="l", col="blue")
legend(2, 0.9, expression(paste(alpha==2, ", ", beta==2),
     paste(alpha==2, ", ", beta==3), paste(alpha==3, ", ", beta==2)),
     lty=c(1,1,1), col=c("red","green","blue"))
</code></pre>

<hr>
<h2 id='dist.Inverse.ChiSquare'>(Scaled) Inverse Chi-Squared Distribution</h2><span id='topic+dinvchisq'></span><span id='topic+rinvchisq'></span>

<h3>Description</h3>

<p>This is the density function and random generation for the (scaled)
inverse chi-squared distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dinvchisq(x, df, scale, log=FALSE)
rinvchisq(n, df, scale=1/df)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.Inverse.ChiSquare_+3A_x">x</code></td>
<td>
<p>This is a vector of quantiles.</p>
</td></tr>
<tr><td><code id="dist.Inverse.ChiSquare_+3A_n">n</code></td>
<td>
<p>This is the number of observations. If <code>length(n) &gt; 1</code>,
then the length is taken to be the number required.</p>
</td></tr>
<tr><td><code id="dist.Inverse.ChiSquare_+3A_df">df</code></td>
<td>
<p>This is the degrees of freedom parameter, usually
represented as <code class="reqn">\nu</code>.</p>
</td></tr>
<tr><td><code id="dist.Inverse.ChiSquare_+3A_scale">scale</code></td>
<td>
<p>This is the scale parameter, usually represented as
<code class="reqn">\lambda</code>.</p>
</td></tr>
<tr><td><code id="dist.Inverse.ChiSquare_+3A_log">log</code></td>
<td>
<p>Logical. If <code>log=TRUE</code>, then the logarithm of the
density is returned.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> Application: Continuous Univariate
</p>
</li>
<li><p> Density: </p>
<p style="text-align: center;"><code class="reqn">p(\theta) = \frac{{\nu/2}^{\nu/2}}{\Gamma(\nu/2)}
    \lambda^\nu \frac{1}{\theta}^{\nu/2+1} \exp(-\frac{\nu
      \lambda^2}{2\theta}), \theta \ge 0</code>
</p>

</li>
<li><p> Inventor: Derived from the chi-squared distribution
</p>
</li>
<li><p> Notation 1: <code class="reqn">\theta \sim \chi^{-2}(\nu, \lambda)</code>
</p>
</li>
<li><p> Notation 2: <code class="reqn">p(\theta) = \chi^{-2}(\theta | \nu,
    \lambda)</code>
</p>
</li>
<li><p> Parameter 1: degrees of freedom parameter <code class="reqn">\nu &gt; 0</code>
</p>
</li>
<li><p> Parameter 2: scale parameter <code class="reqn">\lambda</code>
</p>
</li>
<li><p> Mean: <code class="reqn">E(\theta)</code> = unknown
</p>
</li>
<li><p> Variance: <code class="reqn">var(\theta)</code> = unknown
</p>
</li>
<li><p> Mode: <code class="reqn">mode(\theta) = </code>
</p>
</li></ul>

<p>The inverse chi-squared distribution, also called the
inverted chi-square distribution, is the multiplicate inverse of the
chi-squared distribution. If <code class="reqn">x</code> has the chi-squared distribution
with <code class="reqn">\nu</code> degrees of freedom, then <code class="reqn">1 / x</code> has the
inverse chi-squared distribution with <code class="reqn">\nu</code> degrees of freedom,
and <code class="reqn">\nu / x</code> has the inverse chi-squared distribution with
<code class="reqn">\nu</code> degrees of freedom.
</p>
<p>These functions are similar to those in the GeoR package.
</p>


<h3>Value</h3>

<p><code>dinvchisq</code> gives the density and
<code>rinvchisq</code> generates random deviates.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+dchisq">dchisq</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
x &lt;- dinvchisq(1,1,1)
x &lt;- rinvchisq(10,1)

#Plot Probability Functions
x &lt;- seq(from=0.1, to=5, by=0.01)
plot(x, dinvchisq(x,0.5,1), ylim=c(0,1), type="l", main="Probability Function",
     ylab="density", col="red")
lines(x, dinvchisq(x,1,1), type="l", col="green")
lines(x, dinvchisq(x,5,1), type="l", col="blue")
legend(3, 0.9, expression(paste(nu==0.5, ", ", lambda==1),
     paste(nu==1, ", ", lambda==1), paste(nu==5, ", ", lambda==1)),
     lty=c(1,1,1), col=c("red","green","blue"))
</code></pre>

<hr>
<h2 id='dist.Inverse.Gamma'>Inverse Gamma Distribution</h2><span id='topic+dinvgamma'></span><span id='topic+rinvgamma'></span>

<h3>Description</h3>

<p>This is the density function and random generation from the inverse
gamma distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dinvgamma(x, shape=1, scale=1, log=FALSE)
rinvgamma(n, shape=1, scale=1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.Inverse.Gamma_+3A_n">n</code></td>
<td>
<p>This is the number of draws from the distribution.</p>
</td></tr>
<tr><td><code id="dist.Inverse.Gamma_+3A_x">x</code></td>
<td>
<p>This is the scalar location to evaluate density.</p>
</td></tr>
<tr><td><code id="dist.Inverse.Gamma_+3A_shape">shape</code></td>
<td>
<p>This is the scalar shape parameter <code class="reqn">\alpha</code>,
which defaults to one.</p>
</td></tr>
<tr><td><code id="dist.Inverse.Gamma_+3A_scale">scale</code></td>
<td>
<p>This is the scalar scale parameter <code class="reqn">\beta</code>,
which defaults to one.</p>
</td></tr>
<tr><td><code id="dist.Inverse.Gamma_+3A_log">log</code></td>
<td>
<p>Logical. If <code>log=TRUE</code>, then the logarithm of the
density is returned.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> Application: Continuous Univariate
</p>
</li>
<li><p> Density: <code class="reqn">p(\theta) = \frac{\beta^\alpha}{\Gamma(\alpha)}
      \theta^{-(\alpha + 1)} \exp(-\frac{\beta}{\theta}), \quad \theta &gt; 0</code>
</p>
</li>
<li><p> Inventor: Unknown (to me, anyway)
</p>
</li>
<li><p> Notation 1: <code class="reqn">\theta \sim \mathcal{G}^{-1}(\alpha, \beta)</code>
</p>
</li>
<li><p> Notation 2: <code class="reqn">p(\theta) = \mathcal{G}^{-1}(\theta | \alpha,
    \beta)</code>
</p>
</li>
<li><p> Parameter 1: shape <code class="reqn">\alpha &gt; 0</code>
</p>
</li>
<li><p> Parameter 2: scale <code class="reqn">\beta &gt; 0</code>
</p>
</li>
<li><p> Mean: <code class="reqn">E(\theta) = \frac{\beta}{\alpha - 1}</code>, for <code class="reqn">\alpha &gt; 1</code>
</p>
</li>
<li><p> Variance: <code class="reqn">var(\theta) = \frac{\beta^2}{(\alpha - 1)^2
	(\alpha - 2)}, \alpha &gt; 2</code>
</p>
</li>
<li><p> Mode: <code class="reqn">mode(\theta) = \frac{\beta}{\alpha + 1}</code>
</p>
</li></ul>

<p>The inverse-gamma is the conjugate prior distribution for the normal
or Gaussian variance, and has been traditionally specified as a vague
prior in that application. The density is always finite; its integral is
finite if <code class="reqn">\alpha &gt; 0</code>. Prior information decreases as
<code class="reqn">\alpha, \beta \rightarrow 0</code>.
</p>
<p>These functions are similar to those in the <code>MCMCpack</code> package.
</p>


<h3>Value</h3>

<p><code>dinvgamma</code> gives the density and
<code>rinvgamma</code> generates random deviates. The parameterization
is consistent with the Gamma Distribution in the stats package.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+dgamma">dgamma</a></code>,
<code><a href="stats.html#topic+dnorm">dnorm</a></code>,
<code><a href="#topic+dnormp">dnormp</a></code>, and
<code><a href="#topic+dnormv">dnormv</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
x &lt;- dinvgamma(4.3, 1.1)
x &lt;- rinvgamma(10, 3.3)

#Plot Probability Functions
x &lt;- seq(from=0.1, to=20, by=0.1)
plot(x, dinvgamma(x,1,1), ylim=c(0,1), type="l", main="Probability Function",
     ylab="density", col="red")
lines(x, dinvgamma(x,1,0.6), type="l", col="green")
lines(x, dinvgamma(x,0.6,1), type="l", col="blue")
legend(2, 0.9, expression(paste(alpha==1, ", ", beta==1),
     paste(alpha==1, ", ", beta==0.6), paste(alpha==0.6, ", ", beta==1)),
     lty=c(1,1,1), col=c("red","green","blue"))
</code></pre>

<hr>
<h2 id='dist.Inverse.Gaussian'>Inverse Gaussian Distribution</h2><span id='topic+dinvgaussian'></span><span id='topic+rinvgaussian'></span>

<h3>Description</h3>

<p>This is the density function and random generation from the inverse
gaussian distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dinvgaussian(x, mu, lambda, log=FALSE)
rinvgaussian(n, mu, lambda)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.Inverse.Gaussian_+3A_n">n</code></td>
<td>
<p>This is the number of draws from the distribution.</p>
</td></tr>
<tr><td><code id="dist.Inverse.Gaussian_+3A_x">x</code></td>
<td>
<p>This is the scalar location to evaluate density.</p>
</td></tr>
<tr><td><code id="dist.Inverse.Gaussian_+3A_mu">mu</code></td>
<td>
<p>This is the mean parameter, <code class="reqn">\mu</code>.</p>
</td></tr>
<tr><td><code id="dist.Inverse.Gaussian_+3A_lambda">lambda</code></td>
<td>
<p>This is the inverse-variance parameter,
<code class="reqn">\lambda</code>.</p>
</td></tr>
<tr><td><code id="dist.Inverse.Gaussian_+3A_log">log</code></td>
<td>
<p>Logical. If <code>log=TRUE</code>, then the logarithm of the
density is returned.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> Application: Continuous Univariate
</p>
</li>
<li><p> Density: <code class="reqn">p(\theta) = \frac{\lambda}{(2 \pi
	\theta^3)^{1/2}} \exp(-\frac{\lambda (\theta - \mu)^2}{2 \mu^2
	\theta}), \theta &gt; 0</code>
</p>
</li>
<li><p> Inventor: Schrodinger (1915)
</p>
</li>
<li><p> Notation 1: <code class="reqn">\theta \sim \mathcal{N}^{-1}(\mu,
      \lambda)</code>
</p>
</li>
<li><p> Notation 2: <code class="reqn">p(\theta) = \mathcal{N}^{-1}(\theta | \mu,
      \lambda)</code>
</p>
</li>
<li><p> Parameter 1: shape <code class="reqn">\mu &gt; 0</code>
</p>
</li>
<li><p> Parameter 2: scale <code class="reqn">\lambda &gt; 0</code>
</p>
</li>
<li><p> Mean: <code class="reqn">E(\theta) = \mu</code>
</p>
</li>
<li><p> Variance: <code class="reqn">var(\theta) = \frac{\mu^3}{\lambda}</code>
</p>
</li>
<li><p> Mode: <code class="reqn">mode(\theta) = \mu((1 + \frac{9 \mu^2}{4
      \lambda^2})^{1/2} - \frac{3 \mu}{2 \lambda})</code>
</p>
</li></ul>

<p>The inverse-Gaussian distribution, also called the Wald distribution, is
used when modeling dependent variables that are positive and
continuous. When
<code class="reqn">\lambda \rightarrow \infty</code> (or variance
to zero), the inverse-Gaussian distribution becomes similar to a normal
(Gaussian) distribution. The name, inverse-Gaussian, is misleading,
because it is not the inverse of a Gaussian distribution, which is
obvious from the fact that <code class="reqn">\theta</code> must be positive.
</p>


<h3>Value</h3>

<p><code>dinvgaussian</code> gives the density and
<code>rinvgaussian</code> generates random deviates.
</p>


<h3>References</h3>

<p>Schrodinger E. (1915). &quot;Zur Theorie der Fall-und Steigversuche an
Teilchenn mit Brownscher Bewegung&quot;. <em>Physikalische Zeitschrift</em>,
16, p. 289&ndash;295.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+dnorm">dnorm</a></code>,
<code><a href="#topic+dnormp">dnormp</a></code>, and
<code><a href="#topic+dnormv">dnormv</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
x &lt;- dinvgaussian(2, 1, 1)
x &lt;- rinvgaussian(10, 1, 1)

#Plot Probability Functions
x &lt;- seq(from=1, to=20, by=0.1)
plot(x, dinvgaussian(x,1,0.5), ylim=c(0,1), type="l", main="Probability Function",
     ylab="density", col="red")
lines(x, dinvgaussian(x,1,1), type="l", col="green")
lines(x, dinvgaussian(x,1,5), type="l", col="blue")
legend(2, 0.9, expression(paste(mu==1, ", ", sigma==0.5),
     paste(mu==1, ", ", sigma==1), paste(mu==1, ", ", sigma==5)),
     lty=c(1,1,1), col=c("red","green","blue"))
</code></pre>

<hr>
<h2 id='dist.Inverse.Matrix.Gamma'>Inverse Matrix Gamma Distribution</h2><span id='topic+dinvmatrixgamma'></span>

<h3>Description</h3>

<p>This function provides the density for the inverse matrix gamma
distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dinvmatrixgamma(X, alpha, beta, Psi, log=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.Inverse.Matrix.Gamma_+3A_x">X</code></td>
<td>
<p>This is a <code class="reqn">k \times k</code> positive-definite
covariance matrix.</p>
</td></tr>
<tr><td><code id="dist.Inverse.Matrix.Gamma_+3A_alpha">alpha</code></td>
<td>
<p>This is a scalar shape parameter (the degrees of freedom),
<code class="reqn">\alpha</code>.</p>
</td></tr>
<tr><td><code id="dist.Inverse.Matrix.Gamma_+3A_beta">beta</code></td>
<td>
<p>This is a scalar, positive-only scale parameter,
<code class="reqn">\beta</code>.</p>
</td></tr>
<tr><td><code id="dist.Inverse.Matrix.Gamma_+3A_psi">Psi</code></td>
<td>
<p>This is a <code class="reqn">k \times k</code> positive-definite scale
matrix.</p>
</td></tr>
<tr><td><code id="dist.Inverse.Matrix.Gamma_+3A_log">log</code></td>
<td>
<p>Logical. If <code>log=TRUE</code>, then the logarithm of the
density is returned.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> Application: Continuous Multivariate Matrix
</p>
</li>
<li><p> Density: <code class="reqn">p(\theta) = \frac{|\Psi|^\alpha}{\beta^{k
	  \alpha} \Gamma_k(\alpha)}
      |\theta|^{-\alpha-(k+1)/2}\exp(tr(-\frac{1}{\beta}\Psi\theta^{-1}))</code>
</p>
</li>
<li><p> Inventors: Unknown
</p>
</li>
<li><p> Notation 1: <code class="reqn">\theta \sim \mathcal{IMG}_k(\alpha, \beta,
    \Psi)</code>
</p>
</li>
<li><p> Notation 2: <code class="reqn">p(\theta) = \mathcal{IMG}_k(\theta | \alpha,
    \beta, \Psi)</code>
</p>
</li>
<li><p> Parameter 1: shape <code class="reqn">\alpha &gt; 2</code>
</p>
</li>
<li><p> Parameter 2: scale <code class="reqn">\beta &gt; 0</code>
</p>
</li>
<li><p> Parameter 3: positive-definite <code class="reqn">k \times k</code> scale
matrix <code class="reqn">\Psi</code>
</p>
</li>
<li><p> Mean: 
</p>
</li>
<li><p> Variance: 
</p>
</li>
<li><p> Mode: 
</p>
</li></ul>

<p>The inverse matrix gamma (IMG), also called the inverse matrix-variate
gamma, distribution is a generalization of the inverse gamma
distribution to positive-definite matrices. It is a more general and
flexible version of the inverse Wishart distribution
(<code><a href="#topic+dinvwishart">dinvwishart</a></code>), and is a conjugate prior of the covariance
matrix of a multivariate normal distribution (<code><a href="#topic+dmvn">dmvn</a></code>) and
matrix normal distribution (<code><a href="#topic+dmatrixnorm">dmatrixnorm</a></code>).
</p>
<p>The compound distribution resulting from compounding a matrix normal
with an inverse matrix gamma prior over the covariance matrix is a
generalized matrix t-distribution.
</p>
<p>The inverse matrix gamma distribution is identical to the inverse
Wishart distribution when <code class="reqn">\alpha = \nu / 2</code> and
<code class="reqn">\beta = 2</code>.
</p>


<h3>Value</h3>

<p><code>dinvmatrixgamma</code> gives the density.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+dinvgamma">dinvgamma</a></code>
<code><a href="#topic+dmatrixnorm">dmatrixnorm</a></code>,
<code><a href="#topic+dmvn">dmvn</a></code>, and
<code><a href="#topic+dinvwishart">dinvwishart</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
k &lt;- 10
dinvmatrixgamma(X=diag(k), alpha=(k+1)/2, beta=2, Psi=diag(k), log=TRUE)
dinvwishart(Sigma=diag(k), nu=k+1, S=diag(k), log=TRUE)
</code></pre>

<hr>
<h2 id='dist.Inverse.Wishart'>Inverse Wishart Distribution</h2><span id='topic+dinvwishart'></span><span id='topic+rinvwishart'></span>

<h3>Description</h3>

<p>These functions provide the density and random number generation
for the inverse Wishart distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>   dinvwishart(Sigma, nu, S, log=FALSE)
   rinvwishart(nu, S)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.Inverse.Wishart_+3A_sigma">Sigma</code></td>
<td>
<p>This is the symmetric, positive-definite
<code class="reqn">k \times k</code> matrix <code class="reqn">\Sigma</code>.</p>
</td></tr>
<tr><td><code id="dist.Inverse.Wishart_+3A_nu">nu</code></td>
<td>
<p>This is the scalar degrees of freedom, <code class="reqn">\nu</code>.</p>
</td></tr>
<tr><td><code id="dist.Inverse.Wishart_+3A_s">S</code></td>
<td>
<p>This is the symmetric, positive-semidefinite
<code class="reqn">k \times k</code> scale matrix <code class="reqn">\textbf{S}</code>.</p>
</td></tr>
<tr><td><code id="dist.Inverse.Wishart_+3A_log">log</code></td>
<td>
<p>Logical. If <code>log=TRUE</code>, then the logarithm of the
density is returned.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> Application: Continuous Multivariate
</p>
</li>
<li><p> Density: <code class="reqn">p(\theta) = (2^{\nu k/2} \pi^{k(k-1)/4}
      \prod^k_{i=1} \Gamma(\frac{\nu+1-i}{2}))^{-1} |\textbf{S}|^{nu/2}
      |\Omega|^{-(nu-k-1)/2} \exp(-\frac{1}{2} tr(\textbf{S}
      \Omega^{-1}))</code>
</p>
</li>
<li><p> Inventor: John Wishart (1928)
</p>
</li>
<li><p> Notation 1: <code class="reqn">\Sigma \sim
      \mathcal{W}^{-1}_{\nu}(\textbf{S}^{-1})</code>
</p>
</li>
<li><p> Notation 2: <code class="reqn">p(\Sigma) = \mathcal{W}^{-1}_{\nu}(\Sigma |
      \textbf{S}^{-1})</code>
</p>
</li>
<li><p> Parameter 1: degrees of freedom <code class="reqn">\nu</code>
</p>
</li>
<li><p> Parameter 2: symmetric, positive-semidefinite
<code class="reqn">k \times k</code> scale matrix <code class="reqn">\textbf{S}</code>
</p>
</li>
<li><p> Mean: <code class="reqn">E(\Sigma) = \frac{\textbf{S}}{\nu - k -
	1}</code>
</p>
</li>
<li><p> Variance: 
</p>
</li>
<li><p> Mode: <code class="reqn">mode(\Sigma) = \frac{\textbf{S}}{\nu + k +
	1}</code>
</p>
</li></ul>

<p>The inverse Wishart distribution is a probability distribution defined on
real-valued, symmetric, positive-definite matrices, and is used as the
conjugate prior for the covariance matrix, <code class="reqn">\Sigma</code>, of a
multivariate normal distribution. The inverse-Wishart density is always
finite, and the integral is always finite. A degenerate form occurs when
<code class="reqn">\nu &lt; k</code>.
</p>
<p>When applicable, the alternative Cholesky parameterization should be
preferred. For more information, see <code><a href="#topic+dinvwishartc">dinvwishartc</a></code>.
</p>
<p>The inverse Wishart prior lacks flexibility, having only one parameter,
<code class="reqn">\nu</code>, to control the variability for all <code class="reqn">k(k + 1)/2</code>
elements. Popular choices for the scale matrix <code class="reqn">\textbf{S}</code>
include an identity matrix or sample covariance matrix. When the model
sample size is small, the specification of the scale matrix can be
influential.
</p>
<p>The inverse Wishart distribution has a dependency between variance and
correlation, although its relative for a precision matrix (inverse
covariance matrix), the Wishart distribution, does not have this
dependency. This relationship becomes weaker with more degrees of
freedom.
</p>
<p>Due to these limitations (lack of flexibility, and dependence between
variance and correlation), alternative distributions have been
developed. Alternative distributions that are available here include
Huang-Wand (<code><a href="#topic+dhuangwand">dhuangwand</a></code>), inverse matrix gamma
(<code><a href="#topic+dinvmatrixgamma">dinvmatrixgamma</a></code>), Scaled Inverse Wishart
(<code><a href="#topic+dsiw">dsiw</a></code>), and Yang-Berger (<code><a href="#topic+dyangberger">dyangberger</a></code>).
</p>
<p>These functions are parameterized as per Gelman et al. (2004).
</p>


<h3>Value</h3>

<p><code>dinvwishart</code> gives the density and
<code>rinvwishart</code> generates random deviates.
</p>


<h3>References</h3>

  
<p>Gelman, A., Carlin, J., Stern, H., and Rubin, D. (2004). &quot;Bayesian
Data Analysis, Texts in Statistical Science, 2nd ed.&quot;. Chapman and
Hall, London.
</p>
<p>Wishart, J. (1928). &quot;The Generalised Product Moment Distribution in
Samples from a Normal Multivariate Population&quot;. <em>Biometrika</em>,
20A(1-2), p. 32&ndash;52.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dhuangwand">dhuangwand</a></code>,
<code><a href="#topic+dinvmatrixgamma">dinvmatrixgamma</a></code>,
<code><a href="#topic+dinvwishartc">dinvwishartc</a></code>,
<code><a href="#topic+dmvn">dmvn</a></code>,
<code><a href="#topic+dsiw">dsiw</a></code>,
<code><a href="#topic+dwishart">dwishart</a></code>, and
<code><a href="#topic+dyangberger">dyangberger</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
x &lt;- dinvwishart(matrix(c(2,-.3,-.3,4),2,2), 3, matrix(c(1,.1,.1,1),2,2))
x &lt;- rinvwishart(3, matrix(c(1,.1,.1,1),2,2))
</code></pre>

<hr>
<h2 id='dist.Inverse.Wishart.Cholesky'>Inverse Wishart Distribution: Cholesky Parameterization</h2><span id='topic+dinvwishartc'></span><span id='topic+rinvwishartc'></span>

<h3>Description</h3>

<p>These functions provide the density and random number generation
for the inverse Wishart distribution with the Cholesky
parameterization.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>   dinvwishartc(U, nu, S, log=FALSE)
   rinvwishartc(nu, S)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.Inverse.Wishart.Cholesky_+3A_u">U</code></td>
<td>
<p>This is the upper-triangular <code class="reqn">k \times k</code> matrix
for the Cholesky factor <code class="reqn">\textbf{U}</code> of covariance matrix
<code class="reqn">\Sigma</code>.</p>
</td></tr>
<tr><td><code id="dist.Inverse.Wishart.Cholesky_+3A_nu">nu</code></td>
<td>
<p>This is the scalar degrees of freedom, <code class="reqn">\nu</code>.</p>
</td></tr>
<tr><td><code id="dist.Inverse.Wishart.Cholesky_+3A_s">S</code></td>
<td>
<p>This is the symmetric, positive-semidefinite
<code class="reqn">k \times k</code> scale matrix <code class="reqn">\textbf{S}</code>.</p>
</td></tr>
<tr><td><code id="dist.Inverse.Wishart.Cholesky_+3A_log">log</code></td>
<td>
<p>Logical. If <code>log=TRUE</code>, then the logarithm of the
density is returned.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> Application: Continuous Multivariate
</p>
</li>
<li><p> Density: <code class="reqn">p(\theta) = (2^{\nu k/2} \pi^{k(k-1)/4}
      \prod^k_{i=1} \Gamma(\frac{\nu+1-i}{2}))^{-1} |\textbf{S}|^{nu/2}
      |\Omega|^{-(nu-k-1)/2} \exp(-\frac{1}{2} tr(\textbf{S}
      \Omega^{-1}))</code>
</p>
</li>
<li><p> Inventor: John Wishart (1928)
</p>
</li>
<li><p> Notation 1: <code class="reqn">\Sigma \sim
      \mathcal{W}^{-1}_{\nu}(\textbf{S}^{-1})</code>
</p>
</li>
<li><p> Notation 2: <code class="reqn">p(\Sigma) = \mathcal{W}^{-1}_{\nu}(\Sigma |
      \textbf{S}^{-1})</code>
</p>
</li>
<li><p> Parameter 1: degrees of freedom <code class="reqn">\nu</code>
</p>
</li>
<li><p> Parameter 2: symmetric, positive-semidefinite
<code class="reqn">k \times k</code> scale matrix <code class="reqn">\textbf{S}</code>
</p>
</li>
<li><p> Mean: <code class="reqn">E(\Sigma) = \frac{\textbf{S}}{\nu - k -
	1}</code>
</p>
</li>
<li><p> Variance: 
</p>
</li>
<li><p> Mode: <code class="reqn">mode(\Sigma) = \frac{\textbf{S}}{\nu + k +
	1}</code>
</p>
</li></ul>

<p>The inverse Wishart distribution is a probability distribution defined on
real-valued, symmetric, positive-definite matrices, and is used as the
conjugate prior for the covariance matrix, <code class="reqn">\Sigma</code>, of a
multivariate normal distribution. In this parameterization,
<code class="reqn">\Sigma</code> has been decomposed to the upper-triangular Cholesky
factor <code class="reqn">\textbf{U}</code>, as per <code><a href="Matrix.html#topic+chol">chol</a></code>. The
inverse-Wishart density is always finite, and the integral is always
finite. A degenerate form occurs when <code class="reqn">\nu &lt; k</code>.
</p>
<p>In practice, <code class="reqn">\textbf{U}</code> is fully unconstrained for proposals
when its diagonal is log-transformed. The diagonal is exponentiated
after a proposal and before other calculations. Overall, the
Cholesky parameterization is faster than the traditional
parameterization. Compared with <code>dinvwishart</code>, <code>dinvwishartc</code>
must additionally matrix-multiply the Cholesky back to the covariance
matrix, but it does not have to check for or correct the covariance
matrix to positive-semidefiniteness, which overall is slower. Compared
with <code>rinvwishart</code>, <code>rinvwishartc</code> must additionally
calculate a Cholesky decomposition, and is therefore slower.
</p>
<p>The inverse Wishart prior lacks flexibility, having only one parameter,
<code class="reqn">\nu</code>, to control the variability for all <code class="reqn">k(k + 1)/2</code>
elements. Popular choices for the scale matrix <code class="reqn">\textbf{S}</code>
include an identity matrix or sample covariance matrix. When the model
sample size is small, the specification of the scale matrix can be
influential.
</p>
<p>The inverse Wishart distribution has a dependency between variance and
correlation, although its relative for a precision matrix (inverse
covariance matrix), the Wishart distribution, does not have this
dependency. This relationship becomes weaker with more degrees of
freedom.
</p>
<p>Due to these limitations (lack of flexibility, and dependence between
variance and correlation), alternative distributions have been
developed. Alternative distributions that are available here include the
inverse matrix gamma (<code><a href="#topic+dinvmatrixgamma">dinvmatrixgamma</a></code>), Scaled Inverse
Wishart (<code><a href="#topic+dsiw">dsiw</a></code>) and Huang-Wand (<code><a href="#topic+dhuangwand">dhuangwand</a></code>).
Huang-Wand is recommended.
</p>


<h3>Value</h3>

<p><code>dinvwishartc</code> gives the density and
<code>rinvwishartc</code> generates random deviates.
</p>


<h3>References</h3>

<p>Wishart, J. (1928). &quot;The Generalised Product Moment Distribution in
Samples from a Normal Multivariate Population&quot;. <em>Biometrika</em>,
20A(1-2), p. 32&ndash;52.
</p>


<h3>See Also</h3>

<p><code><a href="Matrix.html#topic+chol">chol</a></code>,
<code><a href="#topic+Cov2Prec">Cov2Prec</a></code>,
<code><a href="#topic+dhuangwand">dhuangwand</a></code>,
<code><a href="#topic+dinvmatrixgamma">dinvmatrixgamma</a></code>,
<code><a href="#topic+dmvn">dmvn</a></code>,
<code><a href="#topic+dmvnc">dmvnc</a></code>,
<code><a href="#topic+dmvtc">dmvtc</a></code>,
<code><a href="#topic+dsiw">dsiw</a></code>,
<code><a href="#topic+dwishart">dwishart</a></code>,
<code><a href="#topic+dwishartc">dwishartc</a></code>, and
<code><a href="#topic+dyangbergerc">dyangbergerc</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
Sigma &lt;- matrix(c(2,-.3,-.3,4),2,2)
U &lt;- chol(Sigma)
x &lt;- dinvwishartc(U, 3, matrix(c(1,.1,.1,1),2,2))
x &lt;- rinvwishartc(3, matrix(c(1,.1,.1,1),2,2))
</code></pre>

<hr>
<h2 id='dist.Laplace'>Laplace Distribution: Univariate Symmetric</h2><span id='topic+dlaplace'></span><span id='topic+plaplace'></span><span id='topic+qlaplace'></span><span id='topic+rlaplace'></span>

<h3>Description</h3>

<p>These functions provide the density, distribution function, quantile
function, and random generation for the univariate, symmetric, Laplace
distribution with location parameter <code class="reqn">\mu</code> and scale
parameter <code class="reqn">\lambda</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dlaplace(x, location=0, scale=1, log=FALSE)
plaplace(q, location=0, scale=1)
qlaplace(p, location=0, scale=1)
rlaplace(n, location=0, scale=1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.Laplace_+3A_x">x</code>, <code id="dist.Laplace_+3A_q">q</code></td>
<td>
<p>These are each a vector of quantiles.</p>
</td></tr>
<tr><td><code id="dist.Laplace_+3A_p">p</code></td>
<td>
<p>This is a vector of probabilities.</p>
</td></tr>
<tr><td><code id="dist.Laplace_+3A_n">n</code></td>
<td>
<p>This is the number of observations, which must be a positive
integer that has length 1.</p>
</td></tr>
<tr><td><code id="dist.Laplace_+3A_location">location</code></td>
<td>
<p>This is the location parameter <code class="reqn">\mu</code>.</p>
</td></tr>
<tr><td><code id="dist.Laplace_+3A_scale">scale</code></td>
<td>
<p>This is the scale parameter <code class="reqn">\lambda</code>, which
must be positive.</p>
</td></tr>
<tr><td><code id="dist.Laplace_+3A_log">log</code></td>
<td>
<p>Logical. If <code>log=TRUE</code>, then the logarithm of the
density is returned.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> Application: Continuous Univariate
</p>
</li>
<li><p> Density: <code class="reqn">p(\theta) = \frac{1}{2 \lambda} \exp(-\frac{|\theta - \mu|}{\lambda})</code>
</p>
</li>
<li><p> Inventor: Pierre-Simon Laplace (1774)
</p>
</li>
<li><p> Notation 1: <code class="reqn">\theta \sim
    \mathrm{Laplace}(\mu,\lambda)</code>
</p>
</li>
<li><p> Notation 2: <code class="reqn">\theta \sim \mathcal{L}(\mu, \lambda)</code>
</p>
</li>
<li><p> Notation 3: <code class="reqn">p(\theta) = \mathrm{Laplace}(\theta | \mu,
    \lambda)</code>
</p>
</li>
<li><p> Notation 4: <code class="reqn">p(\theta) = \mathcal{L}(\theta | \mu,
    \lambda)</code>
</p>
</li>
<li><p> Parameter 1: location parameter <code class="reqn">\mu</code>
</p>
</li>
<li><p> Parameter 2: scale parameter <code class="reqn">\lambda &gt; 0</code>
</p>
</li>
<li><p> Mean: <code class="reqn">E(\theta) = \mu</code>
</p>
</li>
<li><p> Variance: <code class="reqn">var(\theta) = 2 \lambda^2</code>
</p>
</li>
<li><p> Mode: <code class="reqn">mode(\theta) = \mu</code>
</p>
</li></ul>

<p>The Laplace distribution (Laplace, 1774) is also called the double
exponential distribution, because it looks like two exponential
distributions back to back with respect to location <code class="reqn">\mu</code>. It is
also called the &ldquo;First Law of Laplace&rdquo;, just as the normal
distribution is referred to as the &ldquo;Second Law of Laplace&rdquo;. The
Laplace distribution is symmetric with respect to <code class="reqn">\mu</code>, though
there are asymmetric versions of the Laplace distribution. The PDF of
the Laplace distribution is reminiscent of the normal distribution;
however, whereas the normal distribution is expressed in terms of the
squared difference from the mean <code class="reqn">\mu</code>, the Laplace density is
expressed in terms of the absolute difference from the mean,
<code class="reqn">\mu</code>. Consequently, the Laplace distribution has fatter
tails than the normal distribution. It has been argued that the Laplace
distribution fits most things in nature better than the normal
distribution.
</p>
<p>There are many extensions to the Laplace distribution, such as the
asymmetric Laplace, asymmetric log-Laplace, Laplace (re-parameterized
for precision), log-Laplace, multivariate Laplace, and skew-Laplace,
among many more.
</p>
<p>These functions are similar to those in the <code>VGAM</code> package.
</p>


<h3>Value</h3>

<p><code>dlaplace</code> gives the density,
<code>plaplace</code> gives the distribution function,
<code>qlaplace</code> gives the quantile function, and
<code>rlaplace</code> generates random deviates.
</p>


<h3>References</h3>

<p>Laplace, P. (1774). &quot;Memoire sur la Probabilite des Causes par les
Evenements.&quot; l'Academie Royale des Sciences, 6, 621&ndash;656. English
translation by S.M. Stigler in 1986 as &quot;Memoir on the Probability
of the Causes of Events&quot; in <em>Statistical Science</em>, 1(3),
p. 359&ndash;378.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dalaplace">dalaplace</a></code>,
<code><a href="#topic+dallaplace">dallaplace</a></code>,
<code><a href="stats.html#topic+dexp">dexp</a></code>,
<code><a href="#topic+dlaplacep">dlaplacep</a></code>,
<code><a href="#topic+dllaplace">dllaplace</a></code>,
<code><a href="#topic+dmvl">dmvl</a></code>,
<code><a href="stats.html#topic+dnorm">dnorm</a></code>,
<code><a href="#topic+dnormp">dnormp</a></code>,
<code><a href="#topic+dnormv">dnormv</a></code>,
<code><a href="#topic+dsdlaplace">dsdlaplace</a></code>, and
<code><a href="#topic+dslaplace">dslaplace</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
x &lt;- dlaplace(1,0,1)
x &lt;- plaplace(1,0,1)
x &lt;- qlaplace(0.5,0,1)
x &lt;- rlaplace(100,0,1)

#Plot Probability Functions
x &lt;- seq(from=-5, to=5, by=0.1)
plot(x, dlaplace(x,0,0.5), ylim=c(0,1), type="l", main="Probability Function",
     ylab="density", col="red")
lines(x, dlaplace(x,0,1), type="l", col="green")
lines(x, dlaplace(x,0,2), type="l", col="blue")
legend(2, 0.9, expression(paste(mu==0, ", ", lambda==0.5),
     paste(mu==0, ", ", lambda==1), paste(mu==0, ", ", lambda==2)),
     lty=c(1,1,1), col=c("red","green","blue"))
</code></pre>

<hr>
<h2 id='dist.Laplace.Mixture'>Mixture of Laplace Distributions</h2><span id='topic+dlaplacem'></span><span id='topic+plaplacem'></span><span id='topic+rlaplacem'></span>

<h3>Description</h3>

<p>These functions provide the density, cumulative, and random generation
for the mixture of univariate Laplace distributions with probability
<code class="reqn">p</code>, location <code class="reqn">\mu</code> and scale <code class="reqn">\sigma</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dlaplacem(x, p, location, scale, log=FALSE)
plaplacem(q, p, location, scale)
rlaplacem(n, p, location, scale)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.Laplace.Mixture_+3A_x">x</code>, <code id="dist.Laplace.Mixture_+3A_q">q</code></td>
<td>
<p>This is vector of values at which the density will be evaluated.</p>
</td></tr>
<tr><td><code id="dist.Laplace.Mixture_+3A_p">p</code></td>
<td>
<p>This is a vector of length <code class="reqn">M</code> of probabilities for <code class="reqn">M</code>
components. The sum of the vector must be one.</p>
</td></tr>
<tr><td><code id="dist.Laplace.Mixture_+3A_n">n</code></td>
<td>
<p>This is the number of observations, which must be a positive
integer that has length 1.</p>
</td></tr>
<tr><td><code id="dist.Laplace.Mixture_+3A_location">location</code></td>
<td>
<p>This is a vector of length <code class="reqn">M</code> that is the location
parameter <code class="reqn">\mu</code>.</p>
</td></tr>
<tr><td><code id="dist.Laplace.Mixture_+3A_scale">scale</code></td>
<td>
<p>This is a vector of length <code class="reqn">M</code> that is the scale
parameter <code class="reqn">\sigma</code>, which must be positive.</p>
</td></tr>
<tr><td><code id="dist.Laplace.Mixture_+3A_log">log</code></td>
<td>
<p>Logical. If <code>TRUE</code>, then the logarithm of the density is
returned.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> Application: Continuous Univariate
</p>
</li>
<li><p> Density: <code class="reqn">p(\theta) = \sum p_i \mathcal{L}(\mu_i,
    \sigma_i)</code>
</p>
</li>
<li><p> Inventor: Unknown
</p>
</li>
<li><p> Notation 1: <code class="reqn">\theta \sim \mathcal{L}(\mu, \sigma)</code>
</p>
</li>
<li><p> Notation 2: <code class="reqn">p(\theta) = \mathcal{L}(\theta | \mu,
    \sigma)</code>
</p>
</li>
<li><p> Parameter 1: location parameters <code class="reqn">\mu</code>
</p>
</li>
<li><p> Parameter 2: scale parameters <code class="reqn">\sigma &gt; 0</code>
</p>
</li>
<li><p> Mean: <code class="reqn">E(\theta) = \sum p_i \mu_i</code>
</p>
</li>
<li><p> Variance: 
</p>
</li>
<li><p> Mode: 
</p>
</li></ul>

<p>A mixture distribution is a probability distribution that is a
combination of other probability distributions, and each distribution is
called a mixture component, or component. A probability (or weight)
exists for each component, and these probabilities sum to one. A
mixture distribution (though not these functions here in particular) may
contain mixture components in which each component is a different
probability distribution. Mixture distributions are very flexible, and
are often used to represent a complex distribution with an unknown
form. When the number of mixture components is unknown, Bayesian
inference is the only sensible approach to estimation.
</p>
<p>A Laplace mixture distribution is a combination of Laplace probability
distributions.
</p>
<p>One of many applications of Laplace mixture distributions is the Laplace
Mixture Model (LMM).
</p>


<h3>Value</h3>

<p><code>dlaplacem</code> gives the density,
<code>plaplacem</code> returns the CDF, and
<code>rlaplacem</code> generates random deviates.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+ddirichlet">ddirichlet</a></code> and
<code><a href="#topic+dlaplace">dlaplace</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
p &lt;- c(0.3,0.3,0.4)
mu &lt;- c(-5, 1, 5)
sigma &lt;- c(1,2,1)
x &lt;- seq(from=-10, to=10, by=0.1)
plot(x, dlaplacem(x, p, mu, sigma, log=FALSE), type="l") #Density
plot(x, plaplacem(x, p, mu, sigma), type="l") #CDF
plot(density(rlaplacem(10000, p, mu, sigma))) #Random Deviates
</code></pre>

<hr>
<h2 id='dist.Laplace.Precision'>Laplace Distribution: Precision Parameterization</h2><span id='topic+dlaplacep'></span><span id='topic+plaplacep'></span><span id='topic+qlaplacep'></span><span id='topic+rlaplacep'></span>

<h3>Description</h3>

<p>These functions provide the density, distribution function, quantile
function, and random generation for the univariate, symmetric, Laplace
distribution with location parameter <code class="reqn">\mu</code> and precision
parameter <code class="reqn">\tau</code>, which is the inverse of the usual scale
parameter, <code class="reqn">\lambda</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dlaplacep(x, mu=0, tau=1, log=FALSE)
plaplacep(q, mu=0, tau=1)
qlaplacep(p, mu=0, tau=1)
rlaplacep(n, mu=0, tau=1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.Laplace.Precision_+3A_x">x</code>, <code id="dist.Laplace.Precision_+3A_q">q</code></td>
<td>
<p>These are each a vector of quantiles.</p>
</td></tr>
<tr><td><code id="dist.Laplace.Precision_+3A_p">p</code></td>
<td>
<p>This is a vector of probabilities.</p>
</td></tr>
<tr><td><code id="dist.Laplace.Precision_+3A_n">n</code></td>
<td>
<p>This is the number of observations, which must be a positive
integer that has length 1.</p>
</td></tr>
<tr><td><code id="dist.Laplace.Precision_+3A_mu">mu</code></td>
<td>
<p>This is the location parameter <code class="reqn">\mu</code>.</p>
</td></tr>
<tr><td><code id="dist.Laplace.Precision_+3A_tau">tau</code></td>
<td>
<p>This is the precision parameter <code class="reqn">\tau</code>, which
must be positive.</p>
</td></tr>
<tr><td><code id="dist.Laplace.Precision_+3A_log">log</code></td>
<td>
<p>Logical. If <code>log=TRUE</code>, then the logarithm of the
density is returned.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> Application: Continuous Univariate
</p>
</li>
<li><p> Density: <code class="reqn">p(\theta) = \frac{\tau}{2} \exp(-\tau |\theta-\mu|)</code>
</p>
</li>
<li><p> Inventor: Pierre-Simon Laplace (1774)
</p>
</li>
<li><p> Notation 1: <code class="reqn">\theta \sim \mathrm{Laplace}(\mu,\tau^{-1})</code>
</p>
</li>
<li><p> Notation 2: <code class="reqn">\theta \sim \mathcal{L}(\mu, \tau^{-1})</code>
</p>
</li>
<li><p> Notation 3: <code class="reqn">p(\theta) =
    \mathrm{Laplace}(\mu,\tau^{-1})</code>
</p>
</li>
<li><p> Notation 4: <code class="reqn">p(\theta) = \mathcal{L}(\theta | \mu,
    \tau^{-1})</code>
</p>
</li>
<li><p> Parameter 1: location parameter <code class="reqn">\mu</code>
</p>
</li>
<li><p> Parameter 2: precision parameter <code class="reqn">\tau &gt; 0</code>
</p>
</li>
<li><p> Mean: <code class="reqn">E(\theta) = \mu</code>
</p>
</li>
<li><p> Variance: <code class="reqn">var(\theta) = 2 \tau^{-2}</code>
</p>
</li>
<li><p> Mode: <code class="reqn">mode(\theta) = \mu</code>
</p>
</li></ul>

<p>The Laplace distribution is also called the double exponential 
distribution, because it looks like two exponential distributions back to
back with respect to location <code class="reqn">\mu</code>. It is also called the
&ldquo;First Law of Laplace&rdquo;, just as the normal distribution is referred to
as the &ldquo;Second Law of Laplace&rdquo;. The Laplace distribution is
symmetric with respect to <code class="reqn">\mu</code>, though there are asymmetric
versions of the Laplace distribution. The PDF of the Laplace
distribution is reminiscent of the normal distribution; however,
whereas the normal distribution is expressed in terms of the squared
difference from the mean <code class="reqn">\mu</code>, the Laplace density is
expressed in terms of the absolute difference from the mean,
<code class="reqn">\mu</code>. Consequently, the Laplace distribution has fatter
tails than the normal distribution. It has been argued that the Laplace
distribution fits most things in nature better than the normal
distribution. Elsewhere, there are a large number of extensions to the
Laplace distribution, including asymmetric versions and
multivariate versions, among many more. These functions provide the
precision parameterization for convenience and familiarity in Bayesian
inference.
</p>


<h3>Value</h3>

<p><code>dlaplacep</code> gives the density,
<code>plaplacep</code> gives the distribution function,
<code>qlaplacep</code> gives the quantile function, and
<code>rlaplacep</code> generates random deviates.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+dalaplace">dalaplace</a></code>,
<code><a href="stats.html#topic+dexp">dexp</a></code>,
<code><a href="#topic+dlaplace">dlaplace</a></code>,
<code><a href="#topic+dmvl">dmvl</a></code>,
<code><a href="stats.html#topic+dnorm">dnorm</a></code>,
<code><a href="#topic+dnormp">dnormp</a></code>, and
<code><a href="#topic+dnormv">dnormv</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
x &lt;- dlaplacep(1,0,1)
x &lt;- plaplacep(1,0,1)
x &lt;- qlaplacep(0.5,0,1)
x &lt;- rlaplacep(100,0,1)

#Plot Probability Functions
x &lt;- seq(from=-5, to=5, by=0.1)
plot(x, dlaplacep(x,0,0.5), ylim=c(0,1), type="l", main="Probability Function",
     ylab="density", col="red")
lines(x, dlaplacep(x,0,1), type="l", col="green")
lines(x, dlaplacep(x,0,2), type="l", col="blue")
legend(2, 0.9, expression(paste(mu==0, ", ", tau==0.5),
     paste(mu==0, ", ", tau==1), paste(mu==0, ", ", tau==2)),
     lty=c(1,1,1), col=c("red","green","blue"))
</code></pre>

<hr>
<h2 id='dist.LASSO'>LASSO Distribution</h2><span id='topic+dlasso'></span><span id='topic+rlasso'></span>

<h3>Description</h3>

<p>These functions provide the density and random generation for the
Bayesian LASSO prior distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dlasso(x, sigma, tau, lambda, a=1, b=1, log=FALSE)
rlasso(n, sigma, tau, lambda, a=1, b=1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.LASSO_+3A_x">x</code></td>
<td>
<p>This is a location vector of length <code class="reqn">J</code> at which to
evaluate density.</p>
</td></tr>
<tr><td><code id="dist.LASSO_+3A_n">n</code></td>
<td>
<p>This is the number of observations, which must be a
positive integer that has length 1.</p>
</td></tr>
<tr><td><code id="dist.LASSO_+3A_sigma">sigma</code></td>
<td>
<p>This is a positive-only scalar hyperparameter
<code class="reqn">\sigma</code>, which is also the residual standard deviation.</p>
</td></tr>
<tr><td><code id="dist.LASSO_+3A_tau">tau</code></td>
<td>
<p>This is a positive-only vector of hyperparameters,
<code class="reqn">\tau</code>, of length <code class="reqn">J</code> regarding local sparsity.</p>
</td></tr>
<tr><td><code id="dist.LASSO_+3A_lambda">lambda</code></td>
<td>
<p>This is a positive-only scalar hyperhyperparameter,
<code class="reqn">\lambda</code>, of global sparsity.</p>
</td></tr>
<tr><td><code id="dist.LASSO_+3A_a">a</code>, <code id="dist.LASSO_+3A_b">b</code></td>
<td>
<p>These are positive-only scalar hyperhyperhyperparameters
for gamma distributed <code class="reqn">\lambda</code>.</p>
</td></tr>
<tr><td><code id="dist.LASSO_+3A_log">log</code></td>
<td>
<p>Logical. If <code>log=TRUE</code>, then the logarithm of the
density is returned.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> Application: Multivariate Scale Mixture
</p>
</li>
<li><p> Density: <code class="reqn">p(\theta) \sim \mathcal{N}_k(0, \sigma^2
      diag(\tau^2))(\frac{1}{sigma^2})
      \mathcal{EXP}(\frac{\lambda^2}{2}) \mathcal{G}(a,b)</code>    
</p>
</li>
<li><p> Inventor: Parks and Casella (2008)
</p>
</li>
<li><p> Notation 1: <code class="reqn">\theta \sim \mathcal{LASSO}(\sigma, \tau,
      \lambda, a, b)</code>
</p>
</li>
<li><p> Notation 2: <code class="reqn">p(\theta) = \mathcal{LASSO}(\theta | \sigma,
      \tau, \lambda, a, b)</code>
</p>
</li>
<li><p> Parameter 1: hyperparameter global scale
<code class="reqn">\sigma &gt; 0</code>
</p>
</li>
<li><p> Parameter 2: hyperparameter local scale
<code class="reqn">\tau &gt; 0</code>
</p>
</li>
<li><p> Parameter 3: hyperhyperparameter global scale
<code class="reqn">\lambda &gt; 0</code>
</p>
</li>
<li><p> Parameter 4: hyperhyperhyperparameter scale <code class="reqn">a &gt; 0</code>
</p>
</li>
<li><p> Parameter 5: hyperhyperhyperparameter scale <code class="reqn">b &gt; 0</code>
</p>
</li>
<li><p> Mean: <code class="reqn">E(\theta)</code>
</p>
</li>
<li><p> Variance: 
</p>
</li>
<li><p> Mode: 
</p>
</li></ul>

<p>The Bayesian LASSO distribution (Parks and Casella, 2008) is a
heavy-tailed mixture distribution that can be considered a variance
mixture, and it is in the family of multivariate scale mixtures of
normals.
</p>
<p>The LASSO distribution was proposed as a prior distribution, as a
Bayesian version of the frequentist LASSO, introduced by Tibshirani
(1996). It is applied as a shrinkage prior in the presence of sparsity
for <code class="reqn">J</code> regression effects. LASSO priors are most appropriate in
large-dimensional models where dimension reduction is necessary to
avoid overly complex models that predict poorly.
</p>
<p>The Bayesian LASSO results in regression effects that are a compromise
between regression effects in the frequentist LASSO and ridge
regression. The Bayesian LASSO applies more shrinkage to weak regression
effects than ridge regression.
</p>
<p>The Bayesian LASSO is an alternative to horseshoe regression and ridge
regression.
</p>


<h3>Value</h3>

<p><code>dlasso</code> gives the density and
<code>rlasso</code> generates random deviates.
</p>


<h3>References</h3>

<p>Park, T. and Casella, G. (2008). &quot;The Bayesian Lasso&quot;. <em>Journal
of the American Statistical Association</em>, 103, p. 672&ndash;680.
</p>
<p>Tibshirani, R. (1996). &quot;Regression Shrinkage and Selection via the
Lasso&quot;. <em>Journal of the Royal Statistical Society</em>, Series B, 58,
p. 267&ndash;288.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dhs">dhs</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
x &lt;- rnorm(100)
sigma &lt;- rhalfcauchy(1, 5)
tau &lt;- rhalfcauchy(100, 5)
lambda &lt;- rhalfcauchy(1, 5)
x &lt;- dlasso(x, sigma, tau, lambda, log=TRUE)
x &lt;- rlasso(length(tau), sigma, tau, lambda)
</code></pre>

<hr>
<h2 id='dist.Log.Laplace'>Log-Laplace Distribution: Univariate Symmetric</h2><span id='topic+dllaplace'></span><span id='topic+pllaplace'></span><span id='topic+qllaplace'></span><span id='topic+rllaplace'></span>

<h3>Description</h3>

<p>These functions provide the density, distribution function, quantile
function, and random generation for the univariate, symmetric,
log-Laplace distribution with location parameter <code>location</code> and
scale parameter <code>scale</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dllaplace(x, location=0, scale=1, log=FALSE)
pllaplace(q, location=0, scale=1)
qllaplace(p, location=0, scale=1)
rllaplace(n, location=0, scale=1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.Log.Laplace_+3A_x">x</code>, <code id="dist.Log.Laplace_+3A_q">q</code></td>
<td>
<p>These are each a vector of quantiles.</p>
</td></tr>
<tr><td><code id="dist.Log.Laplace_+3A_p">p</code></td>
<td>
<p>This is a vector of probabilities.</p>
</td></tr>
<tr><td><code id="dist.Log.Laplace_+3A_n">n</code></td>
<td>
<p>This is the number of observations, which must be a positive
integer that has length 1.</p>
</td></tr>
<tr><td><code id="dist.Log.Laplace_+3A_location">location</code></td>
<td>
<p>This is the location parameter <code class="reqn">\mu</code>.</p>
</td></tr>
<tr><td><code id="dist.Log.Laplace_+3A_scale">scale</code></td>
<td>
<p>This is the scale parameter <code class="reqn">\lambda</code>, which
must be positive.</p>
</td></tr>
<tr><td><code id="dist.Log.Laplace_+3A_log">log</code></td>
<td>
<p>Logical. If <code>log=TRUE</code>, then the logarithm of the
density is returned.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> Application: Continuous Univariate
</p>
</li>
<li><p> Density 1: <code class="reqn">p(\theta) =
    \frac{(\sqrt{2}/\lambda)^2}{2(\sqrt{2}/\lambda)}
    \exp(-(\sqrt{2}/\lambda)(\theta - \mu)), \theta \ge
    \exp(\mu)</code>
</p>
</li>
<li><p> Density 2: <code class="reqn">p(\theta) =
    \frac{(\sqrt{2}/\lambda)^2}{2(\sqrt{2}/\lambda)}
    \exp((\sqrt{2}/\lambda)(\theta - \mu)), \theta &lt; \exp(\mu)</code>
</p>
</li>
<li><p> Inventor: Pierre-Simon Laplace
</p>
</li>
<li><p> Notation 1: <code class="reqn">\theta \sim \mathcal{LL}(\mu, \lambda)</code>
</p>
</li>
<li><p> Notation 2: <code class="reqn">p(\theta) = \mathcal{LL}(\theta | \mu,
    \lambda)</code>
</p>
</li>
<li><p> Parameter 1: location parameter <code class="reqn">\mu</code>
</p>
</li>
<li><p> Parameter 2: scale parameter <code class="reqn">\lambda &gt; 0</code>
</p>
</li>
<li><p> Mean: <code class="reqn">E(\theta) = </code>
</p>
</li>
<li><p> Variance: <code class="reqn">var(\theta) = </code>
</p>
</li>
<li><p> Mode: <code class="reqn">mode(\theta) = </code>
</p>
</li></ul>

<p>The univariate, symmetric log-Laplace distribution is derived from the
Laplace distribution. Multivariate and asymmetric versions also exist.
</p>
<p>These functions are similar to those in the <code>VGAM</code> package.
</p>


<h3>Value</h3>

<p><code>dllaplace</code> gives the density,
<code>pllaplace</code> gives the distribution function,
<code>qllaplace</code> gives the quantile function, and
<code>rllaplace</code> generates random deviates.
</p>


<h3>References</h3>

<p>Kozubowski, T. J. and Podgorski, K. (2003). &quot;Log-Laplace Distributions&quot;.
<em>International Mathematical Journal</em>, 3, p. 467&ndash;495.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dalaplace">dalaplace</a></code>,
<code><a href="#topic+dallaplace">dallaplace</a></code>,
<code><a href="stats.html#topic+dexp">dexp</a></code>,
<code><a href="#topic+dlaplace">dlaplace</a></code>,
<code><a href="#topic+dlaplacep">dlaplacep</a></code>,
<code><a href="#topic+dmvl">dmvl</a></code>,
<code><a href="stats.html#topic+dnorm">dnorm</a></code>,
<code><a href="#topic+dnormp">dnormp</a></code>, and
<code><a href="#topic+dnormv">dnormv</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
x &lt;- dllaplace(1,0,1)
x &lt;- pllaplace(1,0,1)
x &lt;- qllaplace(0.5,0,1)
x &lt;- rllaplace(100,0,1)

#Plot Probability Functions
x &lt;- seq(from=0.1, to=20, by=0.1)
plot(x, dllaplace(x,0,0.1), ylim=c(0,1), type="l", main="Probability Function",
     ylab="density", col="red")
lines(x, dllaplace(x,0,0.5), type="l", col="green")
lines(x, dllaplace(x,0,1.5), type="l", col="blue")
legend(2, 0.9, expression(paste(mu==0, ", ", lambda==0.1),
     paste(mu==0, ", ", lambda==0.5), paste(mu==0, ", ", lambda==1.5)),
     lty=c(1,1,1), col=c("red","green","blue"))
</code></pre>

<hr>
<h2 id='dist.Log.Normal.Precision'>Log-Normal Distribution: Precision Parameterization</h2><span id='topic+dlnormp'></span><span id='topic+plnormp'></span><span id='topic+qlnormp'></span><span id='topic+rlnormp'></span>

<h3>Description</h3>

<p>These functions provide the density, distribution function, quantile
function, and random generation for the univariate log-normal
distribution with mean <code class="reqn">\mu</code> and precision <code class="reqn">\tau</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dlnormp(x, mu, tau=NULL, var=NULL, log=FALSE)
plnormp(q, mu, tau, lower.tail=TRUE, log.p=FALSE)
qlnormp(p, mu, tau, lower.tail=TRUE, log.p=FALSE)
rlnormp(n, mu, tau=NULL, var=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.Log.Normal.Precision_+3A_x">x</code>, <code id="dist.Log.Normal.Precision_+3A_q">q</code></td>
<td>
<p>These are each a vector of quantiles.</p>
</td></tr>
<tr><td><code id="dist.Log.Normal.Precision_+3A_p">p</code></td>
<td>
<p>This is a vector of probabilities.</p>
</td></tr>
<tr><td><code id="dist.Log.Normal.Precision_+3A_n">n</code></td>
<td>
<p>This is the number of observations, which must be a positive
integer that has length 1.</p>
</td></tr>
<tr><td><code id="dist.Log.Normal.Precision_+3A_mu">mu</code></td>
<td>
<p>This is the mean parameter <code class="reqn">\mu</code>.</p>
</td></tr>
<tr><td><code id="dist.Log.Normal.Precision_+3A_tau">tau</code></td>
<td>
<p>This is the precision parameter <code class="reqn">\tau</code>, which
must be positive. Tau and var cannot be used together</p>
</td></tr>
<tr><td><code id="dist.Log.Normal.Precision_+3A_var">var</code></td>
<td>
<p>This is the variance parameter, which must be positive. Tau and var cannot be used together</p>
</td></tr>
<tr><td><code id="dist.Log.Normal.Precision_+3A_log">log</code>, <code id="dist.Log.Normal.Precision_+3A_log.p">log.p</code></td>
<td>
<p>Logical. If <code>TRUE</code>, then probabilities
<code class="reqn">p</code> are given as <code class="reqn">\log(p)</code>.</p>
</td></tr>
<tr><td><code id="dist.Log.Normal.Precision_+3A_lower.tail">lower.tail</code></td>
<td>
<p>Logical. If <code>TRUE</code> (default), then probabilities
are <code class="reqn">Pr[X \le x]</code>, otherwise,
<code class="reqn">Pr[X &gt; x]</code>.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> Application: Continuous Univariate
</p>
</li>
<li><p> Density: <code class="reqn">p(\theta) = \sqrt{\frac{\tau}{2\pi}}
    \frac{1}{\theta} \exp(-\frac{\tau}{2} (\log(\theta -
    \mu))^2)</code>
</p>
</li>
<li><p> Inventor: Carl Friedrich Gauss or Abraham De Moivre
</p>
</li>
<li><p> Notation 1: <code class="reqn">\theta \sim \mathrm{Log-}\mathcal{N}(\mu,
    \tau^{-1})</code>
</p>
</li>
<li><p> Notation 2: <code class="reqn">p(\theta) = \mathrm{Log-}\mathcal{N}(\theta | \mu,
    \tau^{-1})</code>
</p>
</li>
<li><p> Parameter 1: mean parameter <code class="reqn">\mu</code>
</p>
</li>
<li><p> Parameter 2: precision parameter <code class="reqn">\tau &gt; 0</code>
</p>
</li>
<li><p> Mean: <code class="reqn">E(\theta) = \exp(\mu + \tau^{-1} / 2)</code>
</p>
</li>
<li><p> Variance: <code class="reqn">var(\theta) = (\exp(\tau^{-1}) - 1)\exp(2\mu +
    \tau^{-1})</code>
</p>
</li>
<li><p> Mode: <code class="reqn">mode(\theta) = \exp(\mu - \tau^{-1})</code>
</p>
</li></ul>

<p>The log-normal distribution, also called the Galton distribution, is
applied to a variable whose logarithm is normally-distributed. The
distribution is usually parameterized with mean and variance, or in
Bayesian inference, with mean and precision, where precision is the
inverse of the variance. In contrast, <code>Base R</code> parameterizes the
log-normal distribution with the mean and standard deviation. These
functions provide the precision parameterization for convenience and
familiarity.
</p>
<p>A flat distribution is obtained in the limit as
<code class="reqn">\tau \rightarrow 0</code>.
</p>
<p>These functions are similar to those in <code>base R</code>.
</p>


<h3>Value</h3>

<p><code>dlnormp</code> gives the density,
<code>plnormp</code> gives the distribution function,
<code>qlnormp</code> gives the quantile function, and
<code>rlnormp</code> generates random deviates.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+dnorm">dnorm</a></code>,
<code><a href="#topic+dnormp">dnormp</a></code>,
<code><a href="#topic+dnormv">dnormv</a></code>, and
<code><a href="#topic+prec2var">prec2var</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
x &lt;- dlnormp(1,0,1)
x &lt;- plnormp(1,0,1)
x &lt;- qlnormp(0.5,0,1)
x &lt;- rlnormp(100,0,1)

#Plot Probability Functions
x &lt;- seq(from=0.1, to=3, by=0.01)
plot(x, dlnormp(x,0,0.1), ylim=c(0,1), type="l", main="Probability Function",
     ylab="density", col="red")
lines(x, dlnormp(x,0,1), type="l", col="green")
lines(x, dlnormp(x,0,5), type="l", col="blue")
legend(2, 0.9, expression(paste(mu==0, ", ", tau==0.1),
     paste(mu==0, ", ", tau==1), paste(mu==0, ", ", tau==5)),
     lty=c(1,1,1), col=c("red","green","blue"))
</code></pre>

<hr>
<h2 id='dist.Matrix.Gamma'>Matrix Gamma Distribution</h2><span id='topic+dmatrixgamma'></span>

<h3>Description</h3>

<p>This function provides the density for the matrix gamma distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dmatrixgamma(X, alpha, beta, Sigma, log=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.Matrix.Gamma_+3A_x">X</code></td>
<td>
<p>This is a <code class="reqn">k \times k</code> positive-definite precision
matrix.</p>
</td></tr>
<tr><td><code id="dist.Matrix.Gamma_+3A_alpha">alpha</code></td>
<td>
<p>This is a scalar shape parameter (the degrees of freedom),
<code class="reqn">\alpha</code>.</p>
</td></tr>
<tr><td><code id="dist.Matrix.Gamma_+3A_beta">beta</code></td>
<td>
<p>This is a scalar, positive-only scale parameter,
<code class="reqn">\beta</code>.</p>
</td></tr>
<tr><td><code id="dist.Matrix.Gamma_+3A_sigma">Sigma</code></td>
<td>
<p>This is a <code class="reqn">k \times k</code> positive-definite scale
matrix.</p>
</td></tr>
<tr><td><code id="dist.Matrix.Gamma_+3A_log">log</code></td>
<td>
<p>Logical. If <code>log=TRUE</code>, then the logarithm of the
density is returned.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> Application: Continuous Multivariate Matrix
</p>
</li>
<li><p> Density: <code class="reqn">p(\theta) = \frac{|\Sigma|^{-\alpha}}{\beta^{k
	  \alpha} \Gamma_k(\alpha)}
      |\theta|^{\alpha-(k+1)/2}\exp(tr(-\frac{1}{\beta}\Sigma^{-1}\theta))</code>
</p>
</li>
<li><p> Inventors: Unknown
</p>
</li>
<li><p> Notation 1: <code class="reqn">\theta \sim \mathcal{MG}_k(\alpha, \beta,
    \Sigma)</code>
</p>
</li>
<li><p> Notation 2: <code class="reqn">p(\theta) = \mathcal{MG}_k(\theta | \alpha,
    \beta, \Sigma)</code>
</p>
</li>
<li><p> Parameter 1: shape <code class="reqn">\alpha &gt; 2</code>
</p>
</li>
<li><p> Parameter 2: scale <code class="reqn">\beta &gt; 0</code>
</p>
</li>
<li><p> Parameter 3: positive-definite <code class="reqn">k \times k</code> scale matrix <code class="reqn">\Sigma</code>
</p>
</li>
<li><p> Mean: 
</p>
</li>
<li><p> Variance: 
</p>
</li>
<li><p> Mode: 
</p>
</li></ul>

<p>The matrix gamma (MG), also called the matrix-variate gamma,
distribution is a generalization of the gamma distribution to
positive-definite matrices. It is a more general and flexible version of
the Wishart distribution (<code><a href="#topic+dwishart">dwishart</a></code>), and is a conjugate
prior of the precision matrix of a multivariate normal distribution
(<code><a href="#topic+dmvnp">dmvnp</a></code>) and matrix normal distribution
(<code><a href="#topic+dmatrixnorm">dmatrixnorm</a></code>).
</p>
<p>The compound distribution resulting from compounding a matrix normal
with a matrix gamma prior over the precision matrix is a generalized
matrix t-distribution.
</p>
<p>The matrix gamma distribution is identical to the Wishart distribution
when <code class="reqn">\alpha = \nu / 2</code> and
<code class="reqn">\beta = 2</code>.
</p>


<h3>Value</h3>

<p><code>dmatrixgamma</code> gives the density.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+dgamma">dgamma</a></code>
<code><a href="#topic+dmatrixnorm">dmatrixnorm</a></code>,
<code><a href="#topic+dmvnp">dmvnp</a></code>, and
<code><a href="#topic+dwishart">dwishart</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
k &lt;- 10
dmatrixgamma(X=diag(k), alpha=(k+1)/2, beta=2, Sigma=diag(k), log=TRUE)
dwishart(Omega=diag(k), nu=k+1, S=diag(k), log=TRUE)
</code></pre>

<hr>
<h2 id='dist.Matrix.Normal'>Matrix Normal Distribution</h2><span id='topic+dmatrixnorm'></span><span id='topic+rmatrixnorm'></span>

<h3>Description</h3>

<p>These functions provide the density and random number generation for
the matrix normal distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dmatrixnorm(X, M, U, V, log=FALSE) 
rmatrixnorm(M, U, V)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.Matrix.Normal_+3A_x">X</code></td>
<td>
<p>This is data or parameters in the form of a matrix with
<code class="reqn">n</code> rows and <code class="reqn">k</code> columns.</p>
</td></tr>
<tr><td><code id="dist.Matrix.Normal_+3A_m">M</code></td>
<td>
<p>This is mean matrix with <code class="reqn">n</code> rows and <code class="reqn">k</code> columns.</p>
</td></tr>
<tr><td><code id="dist.Matrix.Normal_+3A_u">U</code></td>
<td>
<p>This is a <code class="reqn">n \times n</code> positive-definite scale
matrix.</p>
</td></tr>
<tr><td><code id="dist.Matrix.Normal_+3A_v">V</code></td>
<td>
<p>This is a <code class="reqn">k \times k</code> positive-definite scale
matrix.</p>
</td></tr>
<tr><td><code id="dist.Matrix.Normal_+3A_log">log</code></td>
<td>
<p>Logical. If <code>log=TRUE</code>, then the logarithm of the
density is returned.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> Application: Continuous Multivariate Matrix
</p>
</li>
<li><p> Density: <code class="reqn">p(\theta) =
      \frac{\exp(-0.5tr[V^{-1}(X-M)'U^{-1}(X-M)])}{(2\pi)^{nk/2}|V|^{n/2}|U|^{k/2}}</code>
</p>
</li>
<li><p> Inventors: Unknown
</p>
</li>
<li><p> Notation 1: <code class="reqn">\theta \sim \mathcal{MN}_{n \times k}(M, U,
    V)</code>
</p>
</li>
<li><p> Notation 2: <code class="reqn">p(\theta) = \mathcal{MN}_{n \times k}(\theta |
    M, U, V)</code>
</p>
</li>
<li><p> Parameter 1: location <code class="reqn">n \times k</code> matrix <code class="reqn">M</code>
</p>
</li>
<li><p> Parameter 2: positive-definite <code class="reqn">n \times n</code> scale matrix <code class="reqn">U</code>
</p>
</li>
<li><p> Parameter 3: positive-definite <code class="reqn">k \times k</code> scale matrix <code class="reqn">V</code>
</p>
</li>
<li><p> Mean: <code class="reqn">E(\theta) = M</code>
</p>
</li>
<li><p> Variance: Unknown
</p>
</li>
<li><p> Mode: Unknown
</p>
</li></ul>

<p>The matrix normal distribution is also called the matrix Gaussian,
matrix-variate normal, or matrix-variate Gaussian distribution. It is a
generalization of the multivariate normal distribution to matrix-valued
random variables.
</p>
<p>An example of the use of a matrix normal distribution is multivariate
regression, in which there is a <code class="reqn">j \times k</code> matrix of
regression effects of <code class="reqn">j</code> predictors for <code class="reqn">k</code> dependent
variables. For univariate regression, having only one dependent
variable, the <code class="reqn">j</code> regression effects may be multivariate normally
distributed. For multivariate regression, this multivariate normal
distribution may be extended to a matrix normal distribution to account
for relationships of the regression effects across <code class="reqn">k</code> dependent
variables. In this example, the matrix normal distribution is the
conjugate prior distribution for these regression effects.
</p>
<p>The matrix normal distribution has two covariance matrices, one for the
rows and one for the columns. When <code class="reqn">U</code> is diagonal, the rows are
independent. When <code class="reqn">V</code> is diagonal, the columns are independent.
</p>


<h3>Value</h3>

<p><code>dmatrixnorm</code> gives the density and 
<code>rmatrixnorm</code> generates random deviates.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+dinvmatrixgamma">dinvmatrixgamma</a></code>,
<code><a href="#topic+dmatrixgamma">dmatrixgamma</a></code>, and
<code><a href="#topic+dmvn">dmvn</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
N &lt;- 10
K &lt;- 4
U &lt;- as.positive.definite(matrix(rnorm(N*N),N,N))
V &lt;- as.positive.definite(matrix(rnorm(K*K),K,K))
x &lt;- dmatrixnorm(matrix(0,N,K), matrix(0,N,K), U, V)
X &lt;- rmatrixnorm(matrix(0,N,K), U, V)
joint.density.plot(X[,1], X[,2], color=TRUE)
</code></pre>

<hr>
<h2 id='dist.Multivariate.Cauchy'>Multivariate Cauchy Distribution</h2><span id='topic+dmvc'></span><span id='topic+rmvc'></span>

<h3>Description</h3>

<p>These functions provide the density and random number generation for
the multivariate Cauchy distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dmvc(x, mu, S, log=FALSE)
rmvc(n=1, mu, S)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.Multivariate.Cauchy_+3A_x">x</code></td>
<td>
<p>This is either a vector of length <code class="reqn">k</code> or a matrix with 
a number of columns, <code class="reqn">k</code>, equal to the number of columns in
scale matrix <code class="reqn">\textbf{S}</code>.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.Cauchy_+3A_n">n</code></td>
<td>
<p>This is the number of random draws.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.Cauchy_+3A_mu">mu</code></td>
<td>
<p>This is a numeric vector representing the location parameter,
<code class="reqn">\mu</code> (the mean vector), of the multivariate distribution
It must be of length <code class="reqn">k</code>, as defined above.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.Cauchy_+3A_s">S</code></td>
<td>
<p>This is a <code class="reqn">k \times k</code> positive-definite scale
matrix <code class="reqn">\textbf{S}</code>.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.Cauchy_+3A_log">log</code></td>
<td>
<p>Logical. If <code>log=TRUE</code>, then the logarithm of the
density is returned.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> Application: Continuous Multivariate
</p>
</li>
<li><p> Density: </p>
<p style="text-align: center;"><code class="reqn">p(\theta) =
    \frac{\Gamma[(1+k)/2]}{\Gamma(1/2)1^{k/2}\pi^{k/2}|\Sigma|^{1/2}[1+(\theta-\mu)^{\mathrm{T}}\Sigma^{-1}(\theta-\mu)]^{(1+k)/2}}</code>
</p>

</li>
<li><p> Inventor: Unknown (to me, anyway)
</p>
</li>
<li><p> Notation 1: <code class="reqn">\theta \sim \mathcal{MC}_k(\mu, \Sigma)</code>
</p>
</li>
<li><p> Notation 2: <code class="reqn">p(\theta) = \mathcal{MC}_k(\theta | \mu,
    \Sigma)</code>
</p>
</li>
<li><p> Parameter 1: location vector <code class="reqn">\mu</code>
</p>
</li>
<li><p> Parameter 2: positive-definite <code class="reqn">k \times k</code> scale
matrix <code class="reqn">\Sigma</code>
</p>
</li>
<li><p> Mean: <code class="reqn">E(\theta) = \mu</code>
</p>
</li>
<li><p> Variance: <code class="reqn">var(\theta) = undefined</code>
</p>
</li>
<li><p> Mode: <code class="reqn">mode(\theta) = \mu</code>
</p>
</li></ul>

<p>The multivariate Cauchy distribution is a multidimensional extension of the
one-dimensional or univariate Cauchy distribution. The multivariate
Cauchy distribution is equivalent to a multivariate t distribution with
1 degree of freedom. A random vector is considered to be multivariate
Cauchy-distributed if every linear combination of its components has a
univariate Cauchy distribution.
</p>
<p>The Cauchy distribution is known as a pathological distribution because
its mean and variance are undefined, and it does not satisfy the central
limit theorem.
</p>


<h3>Value</h3>

<p><code>dmvc</code> gives the density and 
<code>rmvc</code> generates random deviates.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+dcauchy">dcauchy</a></code>,
<code><a href="#topic+dinvwishart">dinvwishart</a></code>,
<code><a href="#topic+dmvcp">dmvcp</a></code>,
<code><a href="#topic+dmvt">dmvt</a></code>, and
<code><a href="#topic+dmvtp">dmvtp</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
x &lt;- seq(-2,4,length=21)
y &lt;- 2*x+10
z &lt;- x+cos(y) 
mu &lt;- c(1,12,2)
Sigma &lt;- matrix(c(1,2,0,2,5,0.5,0,0.5,3), 3, 3)
f &lt;- dmvc(cbind(x,y,z), mu, Sigma)

X &lt;- rmvc(1000, rep(0,2), diag(2))
X &lt;- X[rowSums((X &gt;= quantile(X, probs=0.025)) &amp;
     (X &lt;= quantile(X, probs=0.975)))==2,]
joint.density.plot(X[,1], X[,2], color=TRUE)
</code></pre>

<hr>
<h2 id='dist.Multivariate.Cauchy.Cholesky'>Multivariate Cauchy Distribution: Cholesky Parameterization</h2><span id='topic+dmvcc'></span><span id='topic+rmvcc'></span>

<h3>Description</h3>

<p>These functions provide the density and random number generation for
the multivariate Cauchy distribution, given the Cholesky
parameterization.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dmvcc(x, mu, U, log=FALSE)
rmvcc(n=1, mu, U)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.Multivariate.Cauchy.Cholesky_+3A_x">x</code></td>
<td>
<p>This is either a vector of length <code class="reqn">k</code> or a matrix with 
a number of columns, <code class="reqn">k</code>, equal to the number of columns in
scale matrix <code class="reqn">\textbf{S}</code>.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.Cauchy.Cholesky_+3A_n">n</code></td>
<td>
<p>This is the number of random draws.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.Cauchy.Cholesky_+3A_mu">mu</code></td>
<td>
<p>This is a numeric vector representing the location parameter,
<code class="reqn">\mu</code> (the mean vector), of the multivariate distribution
It must be of length <code class="reqn">k</code>, as defined above.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.Cauchy.Cholesky_+3A_u">U</code></td>
<td>
<p>This is the <code class="reqn">k \times k</code> upper-triangular matrix
that is Cholesky factor <code class="reqn">\textbf{U}</code> of the positive-definite
scale matrix <code class="reqn">\textbf{S}</code>.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.Cauchy.Cholesky_+3A_log">log</code></td>
<td>
<p>Logical. If <code>log=TRUE</code>, then the logarithm of the
density is returned.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> Application: Continuous Multivariate
</p>
</li>
<li><p> Density: </p>
<p style="text-align: center;"><code class="reqn">p(\theta) =
    \frac{\Gamma[(1+k)/2]}{\Gamma(1/2)1^{k/2}\pi^{k/2}|\Sigma|^{1/2}[1+(\theta-\mu)^{\mathrm{T}}\Sigma^{-1}(\theta-\mu)]^{(1+k)/2}}</code>
</p>

</li>
<li><p> Inventor: Unknown (to me, anyway)
</p>
</li>
<li><p> Notation 1: <code class="reqn">\theta \sim \mathcal{MC}_k(\mu, \Sigma)</code>
</p>
</li>
<li><p> Notation 2: <code class="reqn">p(\theta) = \mathcal{MC}_k(\theta | \mu,
    \Sigma)</code>
</p>
</li>
<li><p> Parameter 1: location vector <code class="reqn">\mu</code>
</p>
</li>
<li><p> Parameter 2: positive-definite <code class="reqn">k \times k</code> scale
matrix <code class="reqn">\Sigma</code>
</p>
</li>
<li><p> Mean: <code class="reqn">E(\theta) = \mu</code>
</p>
</li>
<li><p> Variance: <code class="reqn">var(\theta) = </code>
</p>
</li>
<li><p> Mode: <code class="reqn">mode(\theta) = \mu</code>
</p>
</li></ul>

<p>The multivariate Cauchy distribution is a multidimensional extension of the
one-dimensional or univariate Cauchy distribution. The multivariate
Cauchy distribution is equivalent to a multivariate t distribution with
1 degree of freedom. A random vector is considered to be multivariate
Cauchy-distributed if every linear combination of its components has a
univariate Cauchy distribution.
</p>
<p>The Cauchy distribution is known as a pathological distribution because
its mean and variance are undefined, and it does not satisfy the central
limit theorem.
</p>
<p>In practice, <code class="reqn">\textbf{U}</code> is fully unconstrained for proposals
when its diagonal is log-transformed. The diagonal is exponentiated
after a proposal and before other calculations. Overall, the Cholesky
parameterization is faster than the traditional parameterization.
Compared with <code>dmvc</code>, <code>dmvcc</code> must additionally
matrix-multiply the Cholesky back to the scake matrix, but it
does not have to check for or correct the scale matrix to
positive-definiteness, which overall is slower. Compared with
<code>rmvc</code>, <code>rmvcc</code> is faster because the Cholesky decomposition
has already been performed.
</p>


<h3>Value</h3>

<p><code>dmvcc</code> gives the density and 
<code>rmvcc</code> generates random deviates.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="Matrix.html#topic+chol">chol</a></code>,
<code><a href="stats.html#topic+dcauchy">dcauchy</a></code>,
<code><a href="#topic+dinvwishartc">dinvwishartc</a></code>,
<code><a href="#topic+dmvcpc">dmvcpc</a></code>,
<code><a href="#topic+dmvtc">dmvtc</a></code>, and
<code><a href="#topic+dmvtpc">dmvtpc</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
x &lt;- seq(-2,4,length=21)
y &lt;- 2*x+10
z &lt;- x+cos(y) 
mu &lt;- c(1,12,2)
Sigma &lt;- matrix(c(1,2,0,2,5,0.5,0,0.5,3), 3, 3)
U &lt;- chol(Sigma)
f &lt;- dmvcc(cbind(x,y,z), mu, U)

X &lt;- rmvcc(1000, rep(0,2), diag(2))
X &lt;- X[rowSums((X &gt;= quantile(X, probs=0.025)) &amp;
     (X &lt;= quantile(X, probs=0.975)))==2,]
joint.density.plot(X[,1], X[,2], color=TRUE)
</code></pre>

<hr>
<h2 id='dist.Multivariate.Cauchy.Precision'>Multivariate Cauchy Distribution: Precision Parameterization</h2><span id='topic+dmvcp'></span><span id='topic+rmvcp'></span>

<h3>Description</h3>

<p>These functions provide the density and random number generation for
the multivariate Cauchy distribution. These functions use the
precision parameterization.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dmvcp(x, mu, Omega, log=FALSE)
rmvcp(n=1, mu, Omega)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.Multivariate.Cauchy.Precision_+3A_x">x</code></td>
<td>
<p>This is either a vector of length <code class="reqn">k</code> or a matrix with 
a number of columns, <code class="reqn">k</code>, equal to the number of columns in
precision matrix <code class="reqn">\Omega</code>.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.Cauchy.Precision_+3A_n">n</code></td>
<td>
<p>This is the number of random draws.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.Cauchy.Precision_+3A_mu">mu</code></td>
<td>
<p>This is a numeric vector representing the location parameter,
<code class="reqn">\mu</code> (the mean vector), of the multivariate
distribution. It must be of length <code class="reqn">k</code>, as defined above.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.Cauchy.Precision_+3A_omega">Omega</code></td>
<td>
<p>This is a <code class="reqn">k \times k</code> positive-definite
precision matrix <code class="reqn">\Omega</code>.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.Cauchy.Precision_+3A_log">log</code></td>
<td>
<p>Logical. If <code>log=TRUE</code>, then the logarithm of the
density is returned.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> Application: Continuous Multivariate
</p>
</li>
<li><p> Density: </p>
<p style="text-align: center;"><code class="reqn">p(\theta) =
    \frac{\Gamma((1+k)/2)}{\Gamma(1/2)1^{k/2}\pi^{k/2}} |\Omega|^{1/2}
    (1 + (\theta-\mu)^T \Omega (\theta-\mu))^{-(1+k)/2}</code>
</p>

</li>
<li><p> Inventor: Unknown (to me, anyway)
</p>
</li>
<li><p> Notation 1: <code class="reqn">\theta \sim \mathcal{MC}_k(\mu,
    \Omega^{-1})</code>
</p>
</li>
<li><p> Notation 2: <code class="reqn">p(\theta) = \mathcal{MC}_k(\theta | \mu,
    \Omega^{-1})</code>
</p>
</li>
<li><p> Parameter 1: location vector <code class="reqn">\mu</code>
</p>
</li>
<li><p> Parameter 2: positive-definite <code class="reqn">k \times k</code> precision
matrix <code class="reqn">\Omega</code>
</p>
</li>
<li><p> Mean: <code class="reqn">E(\theta) = \mu</code>
</p>
</li>
<li><p> Variance: <code class="reqn">var(\theta) = undefined</code>
</p>
</li>
<li><p> Mode: <code class="reqn">mode(\theta) = \mu</code>
</p>
</li></ul>

<p>The multivariate Cauchy distribution is a multidimensional extension
of the one-dimensional or univariate Cauchy distribution. A random
vector is considered to be multivariate Cauchy-distributed if every
linear combination of its components has a univariate Cauchy
distribution. The multivariate Cauchy distribution is equivalent to a
multivariate t distribution with 1 degree of freedom.
</p>
<p>The Cauchy distribution is known as a pathological distribution because
its mean and variance are undefined, and it does not satisfy the central
limit theorem.
</p>
<p>It is usually parameterized with mean and a covariance matrix, or in
Bayesian inference, with mean and a precision matrix, where the
precision matrix is the matrix inverse of the covariance matrix. These
functions provide the precision parameterization for convenience and
familiarity. It is easier to calculate a multivariate Cauchy density
with the precision parameterization, because a matrix inversion can be
avoided.
</p>
<p>This distribution has a mean parameter vector <code class="reqn">\mu</code> of length
<code class="reqn">k</code>, and a <code class="reqn">k \times k</code> precision matrix <code class="reqn">\Omega</code>,
which must be positive-definite.
</p>


<h3>Value</h3>

<p><code>dmvcp</code> gives the density and 
<code>rmvcp</code> generates random deviates.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+dcauchy">dcauchy</a></code>,
<code><a href="#topic+dmvc">dmvc</a></code>,
<code><a href="#topic+dmvt">dmvt</a></code>,
<code><a href="#topic+dmvtp">dmvtp</a></code>, and
<code><a href="#topic+dwishart">dwishart</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
x &lt;- seq(-2,4,length=21)
y &lt;- 2*x+10
z &lt;- x+cos(y) 
mu &lt;- c(1,12,2)
Omega &lt;- matrix(c(1,2,0,2,5,0.5,0,0.5,3), 3, 3)
f &lt;- dmvcp(cbind(x,y,z), mu, Omega)

X &lt;- rmvcp(1000, rep(0,2), diag(2))
X &lt;- X[rowSums((X &gt;= quantile(X, probs=0.025)) &amp;
     (X &lt;= quantile(X, probs=0.975)))==2,]
joint.density.plot(X[,1], X[,2], color=TRUE)
</code></pre>

<hr>
<h2 id='dist.Multivariate.Cauchy.Precision.Cholesky'>Multivariate Cauchy Distribution: Precision-Cholesky Parameterization</h2><span id='topic+dmvcpc'></span><span id='topic+rmvcpc'></span>

<h3>Description</h3>

<p>These functions provide the density and random number generation for
the multivariate Cauchy distribution. These functions use the
precision and Cholesky parameterization.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dmvcpc(x, mu, U, log=FALSE)
rmvcpc(n=1, mu, U)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.Multivariate.Cauchy.Precision.Cholesky_+3A_x">x</code></td>
<td>
<p>This is either a vector of length <code class="reqn">k</code> or a matrix with 
a number of columns, <code class="reqn">k</code>, equal to the number of columns in
precision matrix <code class="reqn">\Omega</code>.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.Cauchy.Precision.Cholesky_+3A_n">n</code></td>
<td>
<p>This is the number of random draws.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.Cauchy.Precision.Cholesky_+3A_mu">mu</code></td>
<td>
<p>This is a numeric vector representing the location parameter,
<code class="reqn">\mu</code> (the mean vector), of the multivariate
distribution. It must be of length <code class="reqn">k</code>, as defined above.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.Cauchy.Precision.Cholesky_+3A_u">U</code></td>
<td>
<p>This is the <code class="reqn">k \times k</code> upper-triangular matrix
that is Cholesky factor <code class="reqn">\textbf{U}</code> of the positive-definite
precision matrix <code class="reqn">\Omega</code>.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.Cauchy.Precision.Cholesky_+3A_log">log</code></td>
<td>
<p>Logical. If <code>log=TRUE</code>, then the logarithm of the
density is returned.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> Application: Continuous Multivariate
</p>
</li>
<li><p> Density: </p>
<p style="text-align: center;"><code class="reqn">p(\theta) =
    \frac{\Gamma((1+k)/2)}{\Gamma(1/2)1^{k/2}\pi^{k/2}} |\Omega|^{1/2}
    (1 + (\theta-\mu)^T \Omega (\theta-\mu))^{-(1+k)/2}</code>
</p>

</li>
<li><p> Inventor: Unknown (to me, anyway)
</p>
</li>
<li><p> Notation 1: <code class="reqn">\theta \sim \mathcal{MC}_k(\mu,
    \Omega^{-1})</code>
</p>
</li>
<li><p> Notation 2: <code class="reqn">p(\theta) = \mathcal{MC}_k(\theta | \mu,
    \Omega^{-1})</code>
</p>
</li>
<li><p> Parameter 1: location vector <code class="reqn">\mu</code>
</p>
</li>
<li><p> Parameter 2: positive-definite <code class="reqn">k \times k</code> precision
matrix <code class="reqn">\Omega</code>
</p>
</li>
<li><p> Mean: <code class="reqn">E(\theta) = \mu</code>
</p>
</li>
<li><p> Variance: <code class="reqn">var(\theta) = </code>
</p>
</li>
<li><p> Mode: <code class="reqn">mode(\theta) = \mu</code>
</p>
</li></ul>

<p>The multivariate Cauchy distribution is a multidimensional extension
of the one-dimensional or univariate Cauchy distribution. A random
vector is considered to be multivariate Cauchy-distributed if every
linear combination of its components has a univariate Cauchy
distribution. The multivariate Cauchy distribution is equivalent to a
multivariate t distribution with 1 degree of freedom.
</p>
<p>The Cauchy distribution is known as a pathological distribution because
its mean and variance are undefined, and it does not satisfy the central
limit theorem.
</p>
<p>It is usually parameterized with mean and a covariance matrix, or in
Bayesian inference, with mean and a precision matrix, where the
precision matrix is the matrix inverse of the covariance matrix. These
functions provide the precision parameterization for convenience and
familiarity. It is easier to calculate a multivariate Cauchy density
with the precision parameterization, because a matrix inversion can be
avoided.
</p>
<p>This distribution has a mean parameter vector <code class="reqn">\mu</code> of length
<code class="reqn">k</code>, and a <code class="reqn">k \times k</code> precision matrix
<code class="reqn">\Omega</code>, which must be positive-definite. The precision
matrix is replaced with the upper-triangular Cholesky factor, as in
<code><a href="Matrix.html#topic+chol">chol</a></code>.
</p>
<p>In practice, <code class="reqn">\textbf{U}</code> is fully unconstrained for proposals
when its diagonal is log-transformed. The diagonal is exponentiated
after a proposal and before other calculations. Overall, Cholesky
parameterization is faster than the traditional parameterization.
Compared with <code>dmvcp</code>, <code>dmvcpc</code> must additionally
matrix-multiply the Cholesky back to the covariance matrix, but it
does not have to check for or correct the precision matrix to
positive-definiteness, which overall is slower. Compared with
<code>rmvcp</code>, <code>rmvcpc</code> is faster because the Cholesky decomposition
has already been performed.
</p>


<h3>Value</h3>

<p><code>dmvcpc</code> gives the density and 
<code>rmvcpc</code> generates random deviates.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="Matrix.html#topic+chol">chol</a></code>,
<code><a href="stats.html#topic+dcauchy">dcauchy</a></code>,
<code><a href="#topic+dmvcc">dmvcc</a></code>,
<code><a href="#topic+dmvtc">dmvtc</a></code>,
<code><a href="#topic+dmvtpc">dmvtpc</a></code>, and
<code><a href="#topic+dwishartc">dwishartc</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
x &lt;- seq(-2,4,length=21)
y &lt;- 2*x+10
z &lt;- x+cos(y) 
mu &lt;- c(1,12,2)
Omega &lt;- matrix(c(1,2,0,2,5,0.5,0,0.5,3), 3, 3)
U &lt;- chol(Omega)
f &lt;- dmvcpc(cbind(x,y,z), mu, U)

X &lt;- rmvcpc(1000, rep(0,2), diag(2))
X &lt;- X[rowSums((X &gt;= quantile(X, probs=0.025)) &amp;
     (X &lt;= quantile(X, probs=0.975)))==2,]
joint.density.plot(X[,1], X[,2], color=TRUE)
</code></pre>

<hr>
<h2 id='dist.Multivariate.Laplace'>Multivariate Laplace Distribution</h2><span id='topic+dmvl'></span><span id='topic+rmvl'></span>

<h3>Description</h3>

<p>These functions provide the density and random number generation
for the multivariate Laplace distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dmvl(x, mu, Sigma, log=FALSE)
rmvl(n, mu, Sigma)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.Multivariate.Laplace_+3A_x">x</code></td>
<td>
<p>This is data or parameters in the form of a vector of length
<code class="reqn">k</code> or a matrix with <code class="reqn">k</code> columns.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.Laplace_+3A_n">n</code></td>
<td>
<p>This is the number of random draws.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.Laplace_+3A_mu">mu</code></td>
<td>
<p>This is mean vector <code class="reqn">\mu</code> with length <code class="reqn">k</code> or
matrix with <code class="reqn">k</code> columns.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.Laplace_+3A_sigma">Sigma</code></td>
<td>
<p>This is the <code class="reqn">k \times k</code> covariance matrix
<code class="reqn">\Sigma</code>.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.Laplace_+3A_log">log</code></td>
<td>
<p>Logical. If <code>log=TRUE</code>, then the logarithm of the
density is returned.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> Application: Continuous Multivariate
</p>
</li>
<li><p> Density: </p>
<p style="text-align: center;"><code class="reqn">p(\theta) = \frac{2}{(2\pi)^{k/2}
      |\Sigma|^{1/2}} \frac{(\pi/(2\sqrt{2(\theta - \mu)^T \Sigma^{-1}
      (\theta - \mu)}))^{1/2} \exp(-\sqrt{2(\theta - \mu)^T \Sigma^{-1}
      (\theta - \mu)})}{\sqrt{((\theta - \mu)^T \Sigma^{-1} (\theta -
      \mu) / 2)}^{k/2-1}}</code>
</p>

</li>
<li><p> Inventor: Fang et al. (1990)
</p>
</li>
<li><p> Notation 1: <code class="reqn">\theta \sim \mathcal{MVL}(\mu, \Sigma)</code>
</p>
</li>
<li><p> Notation 2: <code class="reqn">\theta \sim \mathcal{L}_k(\mu, \Sigma)</code>
</p>
</li>
<li><p> Notation 3: <code class="reqn">p(\theta) = \mathcal{MVL}(\theta | \mu,
    \Sigma)</code>
</p>
</li>
<li><p> Notation 4: <code class="reqn">p(\theta) = \mathcal{L}_k(\theta | \mu,
    \Sigma)</code>
</p>
</li>
<li><p> Parameter 1: location vector <code class="reqn">\mu</code>
</p>
</li>
<li><p> Parameter 2: positive-definite <code class="reqn">k \times k</code>
covariance matrix <code class="reqn">\Sigma</code>
</p>
</li>
<li><p> Mean: <code class="reqn">E(\theta) = \mu</code>
</p>
</li>
<li><p> Variance: <code class="reqn">var(\theta) = \Sigma</code>
</p>
</li>
<li><p> Mode: <code class="reqn">mode(\theta) = \mu</code>
</p>
</li></ul>

<p>The multivariate Laplace distribution is a multidimensional extension of
the one-dimensional or univariate symmetric Laplace distribution. There
are multiple forms of the multivariate Laplace distribution.
</p>
<p>The bivariate case was introduced by Ulrich and Chen (1987), and the
first form in larger dimensions may have been Fang et al. (1990), which
requires a Bessel function. Alternatively, multivariate Laplace was soon
introduced as a special case of a multivariate Linnik distribution
(Anderson, 1992), and later as a special case of the multivariate power
exponential distribution (Fernandez et al., 1995; Ernst, 1998). Bayesian
considerations appear in Haro-Lopez and Smith (1999). Wainwright and
Simoncelli (2000) presented multivariate Laplace as a Gaussian scale
mixture. Kotz et al. (2001) present the distribution formally. Here, the
density is calculated with the asymptotic formula for the Bessel
function as presented in Wang et al. (2008).
</p>
<p>The multivariate Laplace distribution is an attractive alternative to
the multivariate normal distribution due to its wider tails, and remains
a two-parameter distribution (though alternative three-parameter forms
have been introduced as well), unlike the three-parameter multivariate t
distribution, which is often used as a robust alternative to the
multivariate normal distribution.
</p>


<h3>Value</h3>

<p><code>dmvl</code> gives the density, and
<code>rmvl</code> generates random deviates.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>References</h3>

<p>Anderson, D.N. (1992). &quot;A Multivariate Linnik Distribution&quot;.
<em>Statistical Probability Letters</em>, 14, p. 333&ndash;336.
</p>
<p>Eltoft, T., Kim, T., and Lee, T. (2006). &quot;On the Multivariate Laplace
Distribution&quot;. <em>IEEE Signal Processing Letters</em>, 13(5),
p. 300&ndash;303.
</p>
<p>Ernst, M. D. (1998). &quot;A Multivariate Generalized Laplace
Distribution&quot;. <em>Computational Statistics</em>, 13, p. 227&ndash;232.
</p>
<p>Fang, K.T., Kotz, S., and Ng, K.W. (1990). &quot;Symmetric Multivariate and
Related Distributions&quot;. Monographs on Statistics and Probability, 36,
Chapman-Hall, London.
</p>
<p>Fernandez, C., Osiewalski, J. and Steel, M.F.J. (1995). &quot;Modeling and
Inference with v-spherical Distributions&quot;. <em>Journal of the
American Statistical Association</em>, 90, p. 1331&ndash;1340.
</p>
<p>Gomez, E., Gomez-Villegas, M.A., and Marin, J.M. (1998). &quot;A
Multivariate Generalization of the Power Exponential Family of
Distributions&quot;. <em>Communications in Statistics-Theory and
Methods</em>, 27(3), p. 589&ndash;600.
</p>
<p>Haro-Lopez, R.A. and Smith, A.F.M. (1999). &quot;On Robust Bayesian
Analysis for Location and Scale Parameters&quot;. <em>Journal of
Multivariate Analysis</em>, 70, p. 30&ndash;56.
</p>
<p>Kotz., S., Kozubowski, T.J., and Podgorski, K. (2001). &quot;The Laplace
Distribution and Generalizations: A Revisit with Applications to
Communications, Economics, Engineering, and Finance&quot;. Birkhauser:
Boston, MA.
</p>
<p>Ulrich, G. and Chen, C.C. (1987). &quot;A Bivariate Double Exponential
Distribution and its Generalization&quot;. <em>ASA Proceedings on
Statistical Computing</em>, p. 127&ndash;129.
</p>
<p>Wang, D., Zhang, C., and Zhao, X. (2008). &quot;Multivariate Laplace
Filter: A Heavy-Tailed Model for Target Tracking&quot;. <em>Proceedings
of the 19th International Conference on Pattern Recognition</em>: FL.
</p>
<p>Wainwright, M.J. and Simoncelli, E.P. (2000). &quot;Scale Mixtures of
Gaussians and the Statistics of Natural Images&quot;. <em>Advances in
Neural Information Processing Systems</em>, 12, p. 855&ndash;861.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+daml">daml</a></code>,
<code><a href="#topic+dlaplace">dlaplace</a></code>,
<code><a href="#topic+dmvn">dmvn</a></code>,
<code><a href="#topic+dmvnp">dmvnp</a></code>,
<code><a href="#topic+dmvpe">dmvpe</a></code>,
<code><a href="#topic+dmvt">dmvt</a></code>,
<code><a href="stats.html#topic+dnorm">dnorm</a></code>,
<code><a href="#topic+dnormp">dnormp</a></code>, and
<code><a href="#topic+dnormv">dnormv</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
x &lt;- dmvl(c(1,2,3), c(0,1,2), diag(3))
X &lt;- rmvl(1000, c(0,1,2), diag(3))
joint.density.plot(X[,1], X[,2], color=TRUE)
</code></pre>

<hr>
<h2 id='dist.Multivariate.Laplace.Cholesky'>Multivariate Laplace Distribution: Cholesky Parameterization</h2><span id='topic+dmvlc'></span><span id='topic+rmvlc'></span>

<h3>Description</h3>

<p>These functions provide the density and random number generation
for the multivariate Laplace distribution, given the Cholesky
parameterization.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dmvlc(x, mu, U, log=FALSE)
rmvlc(n, mu, U)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.Multivariate.Laplace.Cholesky_+3A_x">x</code></td>
<td>
<p>This is data or parameters in the form of a vector of length
<code class="reqn">k</code> or a matrix with <code class="reqn">k</code> columns.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.Laplace.Cholesky_+3A_n">n</code></td>
<td>
<p>This is the number of random draws.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.Laplace.Cholesky_+3A_mu">mu</code></td>
<td>
<p>This is mean vector <code class="reqn">\mu</code> with length <code class="reqn">k</code> or
matrix with <code class="reqn">k</code> columns.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.Laplace.Cholesky_+3A_u">U</code></td>
<td>
<p>This is the <code class="reqn">k \times k</code> upper-triangular matrix
that is Cholesky factor <code class="reqn">\textbf{U}</code> of covariance matrix
<code class="reqn">\Sigma</code>.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.Laplace.Cholesky_+3A_log">log</code></td>
<td>
<p>Logical. If <code>log=TRUE</code>, then the logarithm of the
density is returned.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> Application: Continuous Multivariate
</p>
</li>
<li><p> Density: </p>
<p style="text-align: center;"><code class="reqn">p(\theta) = \frac{2}{(2\pi)^{k/2}
      |\Sigma|^{1/2}} \frac{(\pi/(2\sqrt{2(\theta - \mu)^T \Sigma^{-1}
      (\theta - \mu)}))^{1/2} \exp(-\sqrt{2(\theta - \mu)^T \Sigma^{-1}
      (\theta - \mu)})}{\sqrt{((\theta - \mu)^T \Sigma^{-1} (\theta -
      \mu) / 2)}^{k/2-1}}</code>
</p>

</li>
<li><p> Inventor: Fang et al. (1990)
</p>
</li>
<li><p> Notation 1: <code class="reqn">\theta \sim \mathcal{MVL}(\mu, \Sigma)</code>
</p>
</li>
<li><p> Notation 2: <code class="reqn">\theta \sim \mathcal{L}_k(\mu, \Sigma)</code>
</p>
</li>
<li><p> Notation 3: <code class="reqn">p(\theta) = \mathcal{MVL}(\theta | \mu,
    \Sigma)</code>
</p>
</li>
<li><p> Notation 4: <code class="reqn">p(\theta) = \mathcal{L}_k(\theta | \mu,
    \Sigma)</code>
</p>
</li>
<li><p> Parameter 1: location vector <code class="reqn">\mu</code>
</p>
</li>
<li><p> Parameter 2: positive-definite <code class="reqn">k \times k</code>
covariance matrix <code class="reqn">\Sigma</code>
</p>
</li>
<li><p> Mean: <code class="reqn">E(\theta) = \mu</code>
</p>
</li>
<li><p> Variance: <code class="reqn">var(\theta) = \Sigma</code>
</p>
</li>
<li><p> Mode: <code class="reqn">mode(\theta) = \mu</code>
</p>
</li></ul>

<p>The multivariate Laplace distribution is a multidimensional extension of
the one-dimensional or univariate symmetric Laplace distribution. There
are multiple forms of the multivariate Laplace distribution.
</p>
<p>The bivariate case was introduced by Ulrich and Chen (1987), and the
first form in larger dimensions may have been Fang et al. (1990), which
requires a Bessel function. Alternatively, multivariate Laplace was soon
introduced as a special case of a multivariate Linnik distribution
(Anderson, 1992), and later as a special case of the multivariate power
exponential distribution (Fernandez et al., 1995; Ernst, 1998). Bayesian
considerations appear in Haro-Lopez and Smith (1999). Wainwright and
Simoncelli (2000) presented multivariate Laplace as a Gaussian scale
mixture. Kotz et al. (2001) present the distribution formally. Here, the
density is calculated with the asymptotic formula for the Bessel
function as presented in Wang et al. (2008).
</p>
<p>The multivariate Laplace distribution is an attractive alternative to
the multivariate normal distribution due to its wider tails, and remains
a two-parameter distribution (though alternative three-parameter forms
have been introduced as well), unlike the three-parameter multivariate t
distribution, which is often used as a robust alternative to the
multivariate normal distribution.
</p>
<p>In practice, <code class="reqn">\textbf{U}</code> is fully unconstrained for proposals
when its diagonal is log-transformed. The diagonal is exponentiated
after a proposal and before other calculations. Overall, the Cholesky
parameterization is faster than the traditional parameterization.
Compared with <code>dmvl</code>, <code>dmvlc</code> must additionally
matrix-multiply the Cholesky back to the covariance matrix, but it
does not have to check for or correct the covariance matrix to
positive-definiteness, which overall is slower. Compared with
<code>rmvl</code>, <code>rmvlc</code> is faster because the Cholesky decomposition
has already been performed.
</p>


<h3>Value</h3>

<p><code>dmvlc</code> gives the density, and
<code>rmvlc</code> generates random deviates.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>References</h3>

<p>Anderson, D.N. (1992). &quot;A Multivariate Linnik Distribution&quot;.
<em>Statistical Probability Letters</em>, 14, p. 333&ndash;336.
</p>
<p>Eltoft, T., Kim, T., and Lee, T. (2006). &quot;On the Multivariate Laplace
Distribution&quot;. <em>IEEE Signal Processing Letters</em>, 13(5),
p. 300&ndash;303.
</p>
<p>Ernst, M. D. (1998). &quot;A Multivariate Generalized Laplace
Distribution&quot;. <em>Computational Statistics</em>, 13, p. 227&ndash;232.
</p>
<p>Fang, K.T., Kotz, S., and Ng, K.W. (1990). &quot;Symmetric Multivariate and
Related Distributions&quot;. Monographs on Statistics and Probability, 36,
Chapman-Hall, London.
</p>
<p>Fernandez, C., Osiewalski, J. and Steel, M.F.J. (1995). &quot;Modeling and
Inference with v-spherical Distributions&quot;. <em>Journal of the
American Statistical Association</em>, 90, p. 1331&ndash;1340.
</p>
<p>Gomez, E., Gomez-Villegas, M.A., and Marin, J.M. (1998). &quot;A
Multivariate Generalization of the Power Exponential Family of
Distributions&quot;. <em>Communications in Statistics-Theory and
Methods</em>, 27(3), p. 589&ndash;600.
</p>
<p>Haro-Lopez, R.A. and Smith, A.F.M. (1999). &quot;On Robust Bayesian
Analysis for Location and Scale Parameters&quot;. <em>Journal of
Multivariate Analysis</em>, 70, p. 30&ndash;56.
</p>
<p>Kotz., S., Kozubowski, T.J., and Podgorski, K. (2001). &quot;The Laplace
Distribution and Generalizations: A Revisit with Applications to
Communications, Economics, Engineering, and Finance&quot;. Birkhauser:
Boston, MA.
</p>
<p>Ulrich, G. and Chen, C.C. (1987). &quot;A Bivariate Double Exponential
Distribution and its Generalization&quot;. <em>ASA Proceedings on
Statistical Computing</em>, p. 127&ndash;129.
</p>
<p>Wang, D., Zhang, C., and Zhao, X. (2008). &quot;Multivariate Laplace
Filter: A Heavy-Tailed Model for Target Tracking&quot;. <em>Proceedings
of the 19th International Conference on Pattern Recognition</em>: FL.
</p>
<p>Wainwright, M.J. and Simoncelli, E.P. (2000). &quot;Scale Mixtures of
Gaussians and the Statistics of Natural Images&quot;. <em>Advances in
Neural Information Processing Systems</em>, 12, p. 855&ndash;861.
</p>


<h3>See Also</h3>

<p><code><a href="Matrix.html#topic+chol">chol</a></code>,
<code><a href="#topic+daml">daml</a></code>,
<code><a href="#topic+dlaplace">dlaplace</a></code>,
<code><a href="#topic+dmvnc">dmvnc</a></code>,
<code><a href="#topic+dmvnpc">dmvnpc</a></code>,
<code><a href="#topic+dmvpec">dmvpec</a></code>,
<code><a href="#topic+dmvtc">dmvtc</a></code>,
<code><a href="stats.html#topic+dnorm">dnorm</a></code>,
<code><a href="#topic+dnormp">dnormp</a></code>, and
<code><a href="#topic+dnormv">dnormv</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
Sigma &lt;- diag(3)
U &lt;- chol(Sigma)
x &lt;- dmvlc(c(1,2,3), c(0,1,2), U)
X &lt;- rmvlc(1000, c(0,1,2), U)
joint.density.plot(X[,1], X[,2], color=TRUE)
</code></pre>

<hr>
<h2 id='dist.Multivariate.Normal'>Multivariate Normal Distribution</h2><span id='topic+dmvn'></span><span id='topic+rmvn'></span>

<h3>Description</h3>

<p>These functions provide the density and random number generation for
the multivariate normal distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dmvn(x, mu, Sigma, log=FALSE) 
rmvn(n=1, mu, Sigma)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.Multivariate.Normal_+3A_x">x</code></td>
<td>
<p>This is data or parameters in the form of a vector of length
<code class="reqn">k</code> or a matrix with <code class="reqn">k</code> columns.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.Normal_+3A_n">n</code></td>
<td>
<p>This is the number of random draws.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.Normal_+3A_mu">mu</code></td>
<td>
<p>This is mean vector <code class="reqn">\mu</code> with length <code class="reqn">k</code> or
matrix with <code class="reqn">k</code> columns.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.Normal_+3A_sigma">Sigma</code></td>
<td>
<p>This is the <code class="reqn">k \times k</code> covariance matrix
<code class="reqn">\Sigma</code>.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.Normal_+3A_log">log</code></td>
<td>
<p>Logical. If <code>log=TRUE</code>, then the logarithm of the
density is returned.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> Application: Continuous Multivariate
</p>
</li>
<li><p> Density: <code class="reqn">p(\theta) = \frac{1}{(2\pi)^{k/2}|\Sigma|^{1/2}}
    \exp(-\frac{1}{2}(\theta - \mu)'\Sigma^{-1}(\theta - \mu))</code>
</p>
</li>
<li><p> Inventors: Robert Adrain (1808), Pierre-Simon Laplace (1812), and Francis Galton (1885)
</p>
</li>
<li><p> Notation 1: <code class="reqn">\theta \sim \mathcal{MVN}(\mu, \Sigma)</code>
</p>
</li>
<li><p> Notation 2: <code class="reqn">\theta \sim \mathcal{N}_k(\mu, \Sigma)</code>
</p>
</li>
<li><p> Notation 3: <code class="reqn">p(\theta) = \mathcal{MVN}(\theta | \mu, \Sigma)</code>
</p>
</li>
<li><p> Notation 4: <code class="reqn">p(\theta) = \mathcal{N}_k(\theta | \mu, \Sigma)</code>
</p>
</li>
<li><p> Parameter 1: location vector <code class="reqn">\mu</code>
</p>
</li>
<li><p> Parameter 2: positive-definite <code class="reqn">k \times k</code> covariance matrix <code class="reqn">\Sigma</code>
</p>
</li>
<li><p> Mean: <code class="reqn">E(\theta) = \mu</code>
</p>
</li>
<li><p> Variance: <code class="reqn">var(\theta) = \Sigma</code>
</p>
</li>
<li><p> Mode: <code class="reqn">mode(\theta) = \mu</code>
</p>
</li></ul>

<p>The multivariate normal distribution, or multivariate Gaussian
distribution, is a multidimensional extension of the one-dimensional
or univariate normal (or Gaussian) distribution. A random vector is
considered to be multivariate normally distributed if every linear
combination of its components has a univariate normal distribution.
This distribution has a mean parameter vector <code class="reqn">\mu</code> of length
<code class="reqn">k</code> and a <code class="reqn">k \times k</code> covariance matrix
<code class="reqn">\Sigma</code>, which must be positive-definite.
</p>
<p>The conjugate prior of the mean vector is another multivariate normal
distribution. The conjugate prior of the covariance matrix is the
inverse Wishart distribution (see <code><a href="#topic+dinvwishart">dinvwishart</a></code>).
</p>
<p>When applicable, the alternative Cholesky parameterization should be
preferred. For more information, see <code><a href="#topic+dmvnc">dmvnc</a></code>.
</p>
<p>For models where the dependent variable, Y, is specified to be
distributed multivariate normal given the model, the Mardia test (see
<code><a href="#topic+plot.demonoid.ppc">plot.demonoid.ppc</a></code>, <code><a href="#topic+plot.laplace.ppc">plot.laplace.ppc</a></code>, or
<code><a href="#topic+plot.pmc.ppc">plot.pmc.ppc</a></code>) may be used to test the residuals.
</p>


<h3>Value</h3>

<p><code>dmvn</code> gives the density and 
<code>rmvn</code> generates random deviates.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+dinvwishart">dinvwishart</a></code>,
<code><a href="#topic+dmatrixnorm">dmatrixnorm</a></code>,
<code><a href="#topic+dmvnc">dmvnc</a></code>,
<code><a href="#topic+dmvnp">dmvnp</a></code>,
<code><a href="stats.html#topic+dnorm">dnorm</a></code>,
<code><a href="#topic+dnormp">dnormp</a></code>,
<code><a href="#topic+dnormv">dnormv</a></code>,
<code><a href="#topic+plot.demonoid.ppc">plot.demonoid.ppc</a></code>,
<code><a href="#topic+plot.laplace.ppc">plot.laplace.ppc</a></code>, and
<code><a href="#topic+plot.pmc.ppc">plot.pmc.ppc</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
x &lt;- dmvn(c(1,2,3), c(0,1,2), diag(3))
X &lt;- rmvn(1000, c(0,1,2), diag(3))
joint.density.plot(X[,1], X[,2], color=TRUE)
</code></pre>

<hr>
<h2 id='dist.Multivariate.Normal.Cholesky'>Multivariate Normal Distribution: Cholesky Parameterization</h2><span id='topic+dmvnc'></span><span id='topic+rmvnc'></span>

<h3>Description</h3>

<p>These functions provide the density and random number generation for
the multivariate normal distribution, given the Cholesky
parameterization.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dmvnc(x, mu, U, log=FALSE) 
rmvnc(n=1, mu, U)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.Multivariate.Normal.Cholesky_+3A_x">x</code></td>
<td>
<p>This is data or parameters in the form of a vector of length
<code class="reqn">k</code> or a matrix with <code class="reqn">k</code> columns.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.Normal.Cholesky_+3A_n">n</code></td>
<td>
<p>This is the number of random draws.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.Normal.Cholesky_+3A_mu">mu</code></td>
<td>
<p>This is mean vector <code class="reqn">\mu</code> with length <code class="reqn">k</code> or
matrix with <code class="reqn">k</code> columns.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.Normal.Cholesky_+3A_u">U</code></td>
<td>
<p>This is the <code class="reqn">k \times k</code> upper-triangular matrix
that is Cholesky factor <code class="reqn">\textbf{U}</code> of covariance matrix
<code class="reqn">\Sigma</code>.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.Normal.Cholesky_+3A_log">log</code></td>
<td>
<p>Logical. If <code>log=TRUE</code>, then the logarithm of the
density is returned.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> Application: Continuous Multivariate
</p>
</li>
<li><p> Density: <code class="reqn">p(\theta) = \frac{1}{(2\pi)^{k/2}|\Sigma|^{1/2}}
    \exp(-\frac{1}{2}(\theta - \mu)'\Sigma^{-1}(\theta - \mu))</code>
</p>
</li>
<li><p> Inventor: Unknown (to me, anyway)
</p>
</li>
<li><p> Notation 1: <code class="reqn">\theta \sim \mathcal{MVN}(\mu, \Sigma)</code>
</p>
</li>
<li><p> Notation 2: <code class="reqn">\theta \sim \mathcal{N}_k(\mu, \Sigma)</code>
</p>
</li>
<li><p> Notation 3: <code class="reqn">p(\theta) = \mathcal{MVN}(\theta | \mu, \Sigma)</code>
</p>
</li>
<li><p> Notation 4: <code class="reqn">p(\theta) = \mathcal{N}_k(\theta | \mu, \Sigma)</code>
</p>
</li>
<li><p> Parameter 1: location vector <code class="reqn">\mu</code>
</p>
</li>
<li><p> Parameter 2: <code class="reqn">k \times k</code> positive-definite matrix <code class="reqn">\Sigma</code>
</p>
</li>
<li><p> Mean: <code class="reqn">E(\theta) = \mu</code>
</p>
</li>
<li><p> Variance: <code class="reqn">var(\theta) = \Sigma</code>
</p>
</li>
<li><p> Mode: <code class="reqn">mode(\theta) = \mu</code>
</p>
</li></ul>

<p>The multivariate normal distribution, or multivariate Gaussian
distribution, is a multidimensional extension of the one-dimensional
or univariate normal (or Gaussian) distribution. A random vector is
considered to be multivariate normally distributed if every linear
combination of its components has a univariate normal distribution.
This distribution has a mean parameter vector <code class="reqn">\mu</code> of length
<code class="reqn">k</code> and an upper-triangular <code class="reqn">k \times k</code> matrix that is
Cholesky factor <code class="reqn">\textbf{U}</code>, as per the <code><a href="Matrix.html#topic+chol">chol</a></code>
function for Cholesky decomposition.
</p>
<p>In practice, <code class="reqn">\textbf{U}</code> is fully unconstrained for proposals
when its diagonal is log-transformed. The diagonal is exponentiated
after a proposal and before other calculations. Overall, the Cholesky
parameterization is faster than the traditional parameterization.
Compared with <code>dmvn</code>, <code>dmvnc</code> must additionally
matrix-multiply the Cholesky back to the covariance matrix, but it
does not have to check for or correct the covariance matrix to
positive-definiteness, which overall is slower. Compared with
<code>rmvn</code>, <code>rmvnc</code> is faster because the Cholesky decomposition
has already been performed.
</p>
<p>For models where the dependent variable, Y, is specified to be
distributed multivariate normal given the model, the Mardia test (see
<code><a href="#topic+plot.demonoid.ppc">plot.demonoid.ppc</a></code>, <code><a href="#topic+plot.laplace.ppc">plot.laplace.ppc</a></code>, or
<code><a href="#topic+plot.pmc.ppc">plot.pmc.ppc</a></code>) may be used to test the residuals.
</p>


<h3>Value</h3>

<p><code>dmvnc</code> gives the density and 
<code>rmvnc</code> generates random deviates.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="Matrix.html#topic+chol">chol</a></code>,
<code><a href="#topic+dinvwishartc">dinvwishartc</a></code>,
<code><a href="#topic+dmvn">dmvn</a></code>,
<code><a href="#topic+dmvnp">dmvnp</a></code>,
<code><a href="#topic+dmvnpc">dmvnpc</a></code>,
<code><a href="stats.html#topic+dnorm">dnorm</a></code>,
<code><a href="#topic+dnormp">dnormp</a></code>,
<code><a href="#topic+dnormv">dnormv</a></code>,
<code><a href="#topic+plot.demonoid.ppc">plot.demonoid.ppc</a></code>,
<code><a href="#topic+plot.laplace.ppc">plot.laplace.ppc</a></code>, and
<code><a href="#topic+plot.pmc.ppc">plot.pmc.ppc</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
Sigma &lt;- diag(3)
U &lt;- chol(Sigma)
x &lt;- dmvnc(c(1,2,3), c(0,1,2), U)
X &lt;- rmvnc(1000, c(0,1,2), U)
joint.density.plot(X[,1], X[,2], color=TRUE)
</code></pre>

<hr>
<h2 id='dist.Multivariate.Normal.Precision'>Multivariate Normal Distribution: Precision Parameterization</h2><span id='topic+dmvnp'></span><span id='topic+rmvnp'></span>

<h3>Description</h3>

<p>These functions provide the density and random number generation for
the multivariate normal distribution, given the precision
parameterization.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dmvnp(x, mu, Omega, log=FALSE) 
rmvnp(n=1, mu, Omega)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.Multivariate.Normal.Precision_+3A_x">x</code></td>
<td>
<p>This is data or parameters in the form of a vector of length
<code class="reqn">k</code> or a matrix with <code class="reqn">k</code> columns.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.Normal.Precision_+3A_n">n</code></td>
<td>
<p>This is the number of random draws.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.Normal.Precision_+3A_mu">mu</code></td>
<td>
<p>This is mean vector <code class="reqn">\mu</code> with length <code class="reqn">k</code> or
matrix with <code class="reqn">k</code> columns.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.Normal.Precision_+3A_omega">Omega</code></td>
<td>
<p>This is the <code class="reqn">k \times k</code> precision matrix
<code class="reqn">\Omega</code>.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.Normal.Precision_+3A_log">log</code></td>
<td>
<p>Logical. If <code>log=TRUE</code>, then the logarithm of the
density is returned.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> Application: Continuous Multivariate
</p>
</li>
<li><p> Density: <code class="reqn">p(\theta) = (2\pi)^{-p/2} |\Omega|^{1/2}
    \exp(-\frac{1}{2} (\theta-\mu)^T \Omega (\theta-\mu))</code>
</p>
</li>
<li><p> Inventor: Unknown (to me, anyway)
</p>
</li>
<li><p> Notation 1: <code class="reqn">\theta \sim \mathcal{MVN}(\mu, \Omega^{-1})</code>
</p>
</li>
<li><p> Notation 2: <code class="reqn">\theta \sim \mathcal{N}_k(\mu, \Omega^{-1})</code>
</p>
</li>
<li><p> Notation 3: <code class="reqn">p(\theta) = \mathcal{MVN}(\theta | \mu, \Omega^{-1})</code>
</p>
</li>
<li><p> Notation 4: <code class="reqn">p(\theta) = \mathcal{N}_k(\theta | \mu, \Omega^{-1})</code>
</p>
</li>
<li><p> Parameter 1: location vector <code class="reqn">\mu</code>
</p>
</li>
<li><p> Parameter 2: positive-definite <code class="reqn">k \times k</code> precision matrix <code class="reqn">\Omega</code>
</p>
</li>
<li><p> Mean: <code class="reqn">E(\theta) = \mu</code>
</p>
</li>
<li><p> Variance: <code class="reqn">var(\theta) = \Omega^{-1}</code>
</p>
</li>
<li><p> Mode: <code class="reqn">mode(\theta) = \mu</code>
</p>
</li></ul>

<p>The multivariate normal distribution, or multivariate Gaussian
distribution, is a multidimensional extension of the one-dimensional
or univariate normal (or Gaussian) distribution. It is usually
parameterized with mean and a covariance matrix, or in Bayesian
inference, with mean and a precision matrix, where the precision matrix
is the matrix inverse of the covariance matrix. These functions
provide the precision parameterization for convenience and
familiarity. It is easier to calculate a multivariate normal density
with the precision parameterization, because a matrix inversion can be
avoided.
</p>
<p>A random vector is considered to be multivariate normally distributed
if every linear combination of its components has a univariate normal
distribution. This distribution has a mean parameter vector
<code class="reqn">\mu</code> of length <code class="reqn">k</code> and a <code class="reqn">k \times k</code>
precision matrix <code class="reqn">\Omega</code>, which must be positive-definite.
</p>
<p>The conjugate prior of the mean vector is another multivariate normal
distribution. The conjugate prior of the precision matrix is the
Wishart distribution (see <code><a href="#topic+dwishart">dwishart</a></code>).
</p>
<p>When applicable, the alternative Cholesky parameterization should be
preferred. For more information, see <code><a href="#topic+dmvnpc">dmvnpc</a></code>.
</p>
<p>For models where the dependent variable, Y, is specified to be
distributed multivariate normal given the model, the Mardia test (see
<code><a href="#topic+plot.demonoid.ppc">plot.demonoid.ppc</a></code>, <code><a href="#topic+plot.laplace.ppc">plot.laplace.ppc</a></code>, or
<code><a href="#topic+plot.pmc.ppc">plot.pmc.ppc</a></code>) may be used to test the residuals.
</p>


<h3>Value</h3>

<p><code>dmvnp</code> gives the density and 
<code>rmvnp</code> generates random deviates.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+dmvn">dmvn</a></code>,
<code><a href="#topic+dmvnc">dmvnc</a></code>,
<code><a href="#topic+dmvnpc">dmvnpc</a></code>,
<code><a href="stats.html#topic+dnorm">dnorm</a></code>,
<code><a href="#topic+dnormp">dnormp</a></code>,
<code><a href="#topic+dnormv">dnormv</a></code>,
<code><a href="#topic+dwishart">dwishart</a></code>,
<code><a href="#topic+plot.demonoid.ppc">plot.demonoid.ppc</a></code>,
<code><a href="#topic+plot.laplace.ppc">plot.laplace.ppc</a></code>, and
<code><a href="#topic+plot.pmc.ppc">plot.pmc.ppc</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
x &lt;- dmvnp(c(1,2,3), c(0,1,2), diag(3))
X &lt;- rmvnp(1000, c(0,1,2), diag(3))
joint.density.plot(X[,1], X[,2], color=TRUE)
</code></pre>

<hr>
<h2 id='dist.Multivariate.Normal.Precision.Cholesky'>Multivariate Normal Distribution: Precision-Cholesky Parameterization</h2><span id='topic+dmvnpc'></span><span id='topic+rmvnpc'></span>

<h3>Description</h3>

<p>These functions provide the density and random number generation for
the multivariate normal distribution, given the precision-Cholesky
parameterization.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dmvnpc(x, mu, U, log=FALSE) 
rmvnpc(n=1, mu, U)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.Multivariate.Normal.Precision.Cholesky_+3A_x">x</code></td>
<td>
<p>This is data or parameters in the form of a vector of length
<code class="reqn">k</code> or a matrix with <code class="reqn">k</code> columns.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.Normal.Precision.Cholesky_+3A_n">n</code></td>
<td>
<p>This is the number of random draws.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.Normal.Precision.Cholesky_+3A_mu">mu</code></td>
<td>
<p>This is mean vector <code class="reqn">\mu</code> with length <code class="reqn">k</code> or
matrix with <code class="reqn">k</code> columns.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.Normal.Precision.Cholesky_+3A_u">U</code></td>
<td>
<p>This is the <code class="reqn">k \times k</code> upper-triangular of the
precision matrix that is Cholesky factor <code class="reqn">\textbf{U}</code> of
precision matrix <code class="reqn">\Omega</code>.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.Normal.Precision.Cholesky_+3A_log">log</code></td>
<td>
<p>Logical. If <code>log=TRUE</code>, then the logarithm of the
density is returned.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> Application: Continuous Multivariate
</p>
</li>
<li><p> Density: <code class="reqn">p(\theta) = (2\pi)^{-p/2} |\Omega|^{1/2}
    \exp(-\frac{1}{2} (\theta-\mu)^T \Omega (\theta-\mu))</code>
</p>
</li>
<li><p> Inventor: Unknown (to me, anyway)
</p>
</li>
<li><p> Notation 1: <code class="reqn">\theta \sim \mathcal{MVN}(\mu, \Omega^{-1})</code>
</p>
</li>
<li><p> Notation 2: <code class="reqn">\theta \sim \mathcal{N}_k(\mu, \Omega^{-1})</code>
</p>
</li>
<li><p> Notation 3: <code class="reqn">p(\theta) = \mathcal{MVN}(\theta | \mu, \Omega^{-1})</code>
</p>
</li>
<li><p> Notation 4: <code class="reqn">p(\theta) = \mathcal{N}_k(\theta | \mu, \Omega^{-1})</code>
</p>
</li>
<li><p> Parameter 1: location vector <code class="reqn">\mu</code>
</p>
</li>
<li><p> Parameter 2: positive-definite <code class="reqn">k \times k</code> precision matrix <code class="reqn">\Omega</code>
</p>
</li>
<li><p> Mean: <code class="reqn">E(\theta) = \mu</code>
</p>
</li>
<li><p> Variance: <code class="reqn">var(\theta) = \Omega^{-1}</code>
</p>
</li>
<li><p> Mode: <code class="reqn">mode(\theta) = \mu</code>
</p>
</li></ul>

<p>The multivariate normal distribution, or multivariate Gaussian
distribution, is a multidimensional extension of the one-dimensional
or univariate normal (or Gaussian) distribution. It is usually
parameterized with mean and a covariance matrix, or in Bayesian
inference, with mean and a precision matrix, where the precision matrix
is the matrix inverse of the covariance matrix. These functions
provide the precision-Cholesky parameterization for convenience and
familiarity. It is easier to calculate a multivariate normal density
with the precision parameterization, because a matrix inversion can be
avoided. The precision matrix is replaced with an upper-triangular
<code class="reqn">k \times k</code> matrix that is Cholesky factor
<code class="reqn">\textbf{U}</code>, as per the <code><a href="Matrix.html#topic+chol">chol</a></code> function for Cholesky
decomposition.
</p>
<p>A random vector is considered to be multivariate normally distributed
if every linear combination of its components has a univariate normal
distribution. This distribution has a mean parameter vector
<code class="reqn">\mu</code> of length <code class="reqn">k</code> and a <code class="reqn">k \times k</code>
precision matrix <code class="reqn">\Omega</code>, which must be positive-definite.
</p>
<p>In practice, <code class="reqn">\textbf{U}</code> is fully unconstrained for proposals
when its diagonal is log-transformed. The diagonal is exponentiated
after a proposal and before other calculations. Overall, Cholesky
parameterization is faster than the traditional parameterization.
Compared with <code>dmvnp</code>, <code>dmvnpc</code> must additionally
matrix-multiply the Cholesky back to the covariance matrix, but it
does not have to check for or correct the precision matrix to
positive-definiteness, which overall is slower. Compared with
<code>rmvnp</code>, <code>rmvnpc</code> is faster because the Cholesky decomposition
has already been performed.
</p>
<p>For models where the dependent variable, Y, is specified to be
distributed multivariate normal given the model, the Mardia test (see
<code><a href="#topic+plot.demonoid.ppc">plot.demonoid.ppc</a></code>, <code><a href="#topic+plot.laplace.ppc">plot.laplace.ppc</a></code>, or
<code><a href="#topic+plot.pmc.ppc">plot.pmc.ppc</a></code>) may be used to test the residuals.
</p>


<h3>Value</h3>

<p><code>dmvnpc</code> gives the density and 
<code>rmvnpc</code> generates random deviates.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="Matrix.html#topic+chol">chol</a></code>,
<code><a href="#topic+dmvn">dmvn</a></code>,
<code><a href="#topic+dmvnc">dmvnc</a></code>,
<code><a href="#topic+dmvnp">dmvnp</a></code>,
<code><a href="stats.html#topic+dnorm">dnorm</a></code>,
<code><a href="#topic+dnormp">dnormp</a></code>,
<code><a href="#topic+dnormv">dnormv</a></code>,
<code><a href="#topic+dwishartc">dwishartc</a></code>,
<code><a href="#topic+plot.demonoid.ppc">plot.demonoid.ppc</a></code>,
<code><a href="#topic+plot.laplace.ppc">plot.laplace.ppc</a></code>, and
<code><a href="#topic+plot.pmc.ppc">plot.pmc.ppc</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
Omega &lt;- diag(3)
U &lt;- chol(Omega)
x &lt;- dmvnpc(c(1,2,3), c(0,1,2), U)
X &lt;- rmvnpc(1000, c(0,1,2), U)
joint.density.plot(X[,1], X[,2], color=TRUE)
</code></pre>

<hr>
<h2 id='dist.Multivariate.Polya'>Multivariate Polya Distribution</h2><span id='topic+dmvpolya'></span><span id='topic+rmvpolya'></span>

<h3>Description</h3>

<p>These functions provide the density and random number generation for
the multivariate Polya distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dmvpolya(x, alpha, log=FALSE)
rmvpolya(n, alpha)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.Multivariate.Polya_+3A_x">x</code></td>
<td>
<p>This is data or parameters in the form of a vector of length
<code class="reqn">k</code>.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.Polya_+3A_n">n</code></td>
<td>
<p>This is the number of random draws to take from the
distribution.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.Polya_+3A_alpha">alpha</code></td>
<td>
<p>This is shape vector <code class="reqn">\alpha</code> with length
<code class="reqn">k</code>.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.Polya_+3A_log">log</code></td>
<td>
<p>Logical. If <code>log=TRUE</code>, then the logarithm of the
density is returned.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> Application: Discrete Multivariate
</p>
</li>
<li><p> Density: </p>
<p style="text-align: center;"><code class="reqn">p(\theta) = \frac{N!}{\prod_k N_k!} \frac{(\sum_k
      \alpha_k - 1)!}{(\sum_k \theta_k + \sum_k \alpha_k - 1)!}
    \frac{\prod (\theta + \alpha - 1)!}{(\alpha - 1)!}</code>
</p>

</li>
<li><p> Inventor: George Polya (1887-1985)
</p>
</li>
<li><p> Notation 1: <code class="reqn">\theta \sim \mathcal{MPO}(\alpha)</code>
</p>
</li>
<li><p> Notation 3: <code class="reqn">p(\theta) = \mathcal{MPO}(\theta |
    \alpha)</code>
</p>
</li>
<li><p> Parameter 1: shape parameter vector <code class="reqn">\alpha</code>
</p>
</li>
<li><p> Mean: <code class="reqn">E(\theta) = </code>
</p>
</li>
<li><p> Variance: <code class="reqn">var(\theta) =</code>
</p>
</li>
<li><p> Mode: <code class="reqn">mode(\theta) = </code>
</p>
</li></ul>

<p>The multivariate Polya distribution is named after George Polya
(1887-1985). It is also called the Dirichlet compound multinomial
distribution or the Dirichlet-multinomial distribution. The multivariate
Polya distribution is a compound probability distribution, where a
probability vector <code class="reqn">p</code> is drawn from a Dirichlet distribution with
parameter vector <code class="reqn">\alpha</code>, and a set of <code class="reqn">N</code> discrete
samples is drawn from the categorical distribution with probability
vector <code class="reqn">p</code> and having <code class="reqn">K</code> discrete categories. The compounding
corresponds to a Polya urn scheme. In document classification, for
example, the distribution is used to represent probabilities over word
counts for different document types. The multivariate Polya distribution
is a multivariate extension of the univariate Beta-binomial distribution.
</p>


<h3>Value</h3>

<p><code>dmvpolya</code> gives the density and <code>rmvpolya</code> generates random
deviates.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+dcat">dcat</a></code>,
<code><a href="#topic+ddirichlet">ddirichlet</a></code>, and
<code><a href="stats.html#topic+dmultinom">dmultinom</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
dmvpolya(x=1:3, alpha=1:3, log=TRUE)
x &lt;- rmvpolya(1000, c(0.1,0.3,0.6))
</code></pre>

<hr>
<h2 id='dist.Multivariate.Power.Exponential'>Multivariate Power Exponential Distribution</h2><span id='topic+dmvpe'></span><span id='topic+rmvpe'></span>

<h3>Description</h3>

<p>These functions provide the density and random number generation for
the multivariate power exponential distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dmvpe(x=c(0,0), mu=c(0,0), Sigma=diag(2), kappa=1, log=FALSE)
rmvpe(n, mu=c(0,0), Sigma=diag(2), kappa=1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.Multivariate.Power.Exponential_+3A_x">x</code></td>
<td>
<p>This is data or parameters in the form of a vector of length
<code class="reqn">k</code> or a matrix with <code class="reqn">k</code> columns.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.Power.Exponential_+3A_n">n</code></td>
<td>
<p>This is the number of random draws.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.Power.Exponential_+3A_mu">mu</code></td>
<td>
<p>This is mean vector <code class="reqn">\mu</code> with length <code class="reqn">k</code> or
matrix with <code class="reqn">k</code> columns.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.Power.Exponential_+3A_sigma">Sigma</code></td>
<td>
<p>This is the <code class="reqn">k \times k</code> covariance matrix
<code class="reqn">\Sigma</code>.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.Power.Exponential_+3A_kappa">kappa</code></td>
<td>
<p>This is the kurtosis parameter, <code class="reqn">\kappa</code>, and
must be positive.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.Power.Exponential_+3A_log">log</code></td>
<td>
<p>Logical. If <code>log=TRUE</code>, then the logarithm of the
density is returned.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> Application: Continuous Multivariate
</p>
</li>
<li><p> Density: </p>
<p style="text-align: center;"><code class="reqn">p(\theta) = \frac{k\Gamma(k/2)}{\pi^{k/2}
      \sqrt{|\Sigma|} \Gamma(1 + k/(2\kappa)) 2^{1 + k/(2\kappa)}}
    \exp(-\frac{1}{2}(\theta-\mu)^T \Sigma
    (\theta-\mu))^{\kappa}</code>
</p>

</li>
<li><p> Inventor: Gomez, Gomez-Villegas, and Marin (1998)
</p>
</li>
<li><p> Notation 1: <code class="reqn">\theta \sim \mathcal{MPE}(\mu, \Sigma,
    \kappa)</code>
</p>
</li>
<li><p> Notation 2: <code class="reqn">\theta \sim \mathcal{PE}_k(\mu, \Sigma,
    \kappa)</code>
</p>
</li>
<li><p> Notation 3: <code class="reqn">p(\theta) = \mathcal{MPE}(\theta | \mu, \Sigma,
    \kappa)</code>
</p>
</li>
<li><p> Notation 4: <code class="reqn">p(\theta) = \mathcal{PE}_k(\theta | \mu,
    \Sigma, \kappa)</code>
</p>
</li>
<li><p> Parameter 1: location vector <code class="reqn">\mu</code>
</p>
</li>
<li><p> Parameter 2: positive-definite <code class="reqn">k \times k</code>
covariance matrix <code class="reqn">\Sigma</code>
</p>
</li>
<li><p> Parameter 3: kurtosis parameter <code class="reqn">\kappa</code>
</p>
</li>
<li><p> Mean: <code class="reqn">E(\theta) = </code>
</p>
</li>
<li><p> Variance: <code class="reqn">var(\theta) =</code>
</p>
</li>
<li><p> Mode: <code class="reqn">mode(\theta) = </code>
</p>
</li></ul>

<p>The multivariate power exponential distribution, or multivariate
exponential power distribution, is a multidimensional extension of
the one-dimensional or univariate power exponential distribution.
Gomez-Villegas (1998) and Sanchez-Manzano et al. (2002) proposed
multivariate and matrix generalizations of the PE family of
distributions and studied their properties in relation to multivariate
Elliptically Contoured (EC) distributions.
</p>
<p>The multivariate power exponential distribution includes the
multivariate normal distribution (<code class="reqn">\kappa = 1</code>) and
multivariate Laplace distribution (<code class="reqn">\kappa = 0.5</code>) as
special cases, depending on the kurtosis or <code class="reqn">\kappa</code>
parameter. A multivariate uniform occurs as
<code class="reqn">\kappa \rightarrow \infty</code>.
</p>
<p>If the goal is to use a multivariate Laplace distribution, the
<code>dmvl</code> function will perform faster and more accurately.
</p>
<p>The <code>rmvpe</code> function is a modified form of the rmvpowerexp function
in the MNM package.
</p>


<h3>Value</h3>

<p><code>dmvpe</code> gives the density and
<code>rmvpe</code> generates random deviates.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>References</h3>

<p>Gomez, E., Gomez-Villegas, M.A., and Marin, J.M. (1998). &quot;A
Multivariate Generalization of the Power Exponential Family of
Distributions&quot;. <em>Communications in Statistics-Theory and
Methods</em>, 27(3), p. 589&ndash;600.
</p>
<p>Sanchez-Manzano, E.G., Gomez-Villegas, M.A., and Marn-Diazaraque,
J.M. (2002). &quot;A Matrix Variate Generalization of the Power Exponential
Family of Distributions&quot;. <em>Communications in Statistics, Part A -
Theory and Methods</em> [Split from: J(CommStat)], 31(12), p. 2167&ndash;2182.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dlaplace">dlaplace</a></code>,
<code><a href="#topic+dmvl">dmvl</a></code>,
<code><a href="#topic+dmvn">dmvn</a></code>,
<code><a href="#topic+dmvnp">dmvnp</a></code>,
<code><a href="stats.html#topic+dnorm">dnorm</a></code>,
<code><a href="#topic+dnormp">dnormp</a></code>,
<code><a href="#topic+dnormv">dnormv</a></code>, and
<code><a href="#topic+dpe">dpe</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
n &lt;- 100
k &lt;- 3
x &lt;- matrix(runif(n*k),n,k)
mu &lt;- matrix(runif(n*k),n,k)
Sigma &lt;- diag(k)
dmvpe(x, mu, Sigma, kappa=1)
X &lt;- rmvpe(n, mu, Sigma, kappa=1)
joint.density.plot(X[,1], X[,2], color=TRUE)
</code></pre>

<hr>
<h2 id='dist.Multivariate.Power.Exponential.Cholesky'>Multivariate Power Exponential Distribution: Cholesky Parameterization</h2><span id='topic+dmvpec'></span><span id='topic+rmvpec'></span>

<h3>Description</h3>

<p>These functions provide the density and random number generation for
the multivariate power exponential distribution, given the Cholesky
parameterization.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dmvpec(x=c(0,0), mu=c(0,0), U, kappa=1, log=FALSE)
rmvpec(n, mu=c(0,0), U, kappa=1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.Multivariate.Power.Exponential.Cholesky_+3A_x">x</code></td>
<td>
<p>This is data or parameters in the form of a vector of length
<code class="reqn">k</code> or a matrix with <code class="reqn">k</code> columns.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.Power.Exponential.Cholesky_+3A_n">n</code></td>
<td>
<p>This is the number of random draws.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.Power.Exponential.Cholesky_+3A_mu">mu</code></td>
<td>
<p>This is mean vector <code class="reqn">\mu</code> with length <code class="reqn">k</code> or
matrix with <code class="reqn">k</code> columns.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.Power.Exponential.Cholesky_+3A_u">U</code></td>
<td>
<p>This is the <code class="reqn">k \times k</code> upper-triangular matrix
that is Cholesky factor <code class="reqn">\textbf{U}</code> of covariance matrix
<code class="reqn">\Sigma</code>.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.Power.Exponential.Cholesky_+3A_kappa">kappa</code></td>
<td>
<p>This is the kurtosis parameter, <code class="reqn">\kappa</code>, and
must be positive.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.Power.Exponential.Cholesky_+3A_log">log</code></td>
<td>
<p>Logical. If <code>log=TRUE</code>, then the logarithm of the
density is returned.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> Application: Continuous Multivariate
</p>
</li>
<li><p> Density: </p>
<p style="text-align: center;"><code class="reqn">p(\theta) = \frac{k\Gamma(k/2)}{\pi^{k/2}
      \sqrt{|\Sigma|} \Gamma(1 + k/(2\kappa)) 2^{1 + k/(2\kappa)}}
    \exp(-\frac{1}{2}(\theta-\mu)^T \Sigma
    (\theta-\mu))^{\kappa}</code>
</p>

</li>
<li><p> Inventor: Gomez, Gomez-Villegas, and Marin (1998)
</p>
</li>
<li><p> Notation 1: <code class="reqn">\theta \sim \mathcal{MPE}(\mu, \Sigma,
    \kappa)</code>
</p>
</li>
<li><p> Notation 2: <code class="reqn">\theta \sim \mathcal{PE}_k(\mu, \Sigma,
    \kappa)</code>
</p>
</li>
<li><p> Notation 3: <code class="reqn">p(\theta) = \mathcal{MPE}(\theta | \mu, \Sigma,
    \kappa)</code>
</p>
</li>
<li><p> Notation 4: <code class="reqn">p(\theta) = \mathcal{PE}_k(\theta | \mu,
    \Sigma, \kappa)</code>
</p>
</li>
<li><p> Parameter 1: location vector <code class="reqn">\mu</code>
</p>
</li>
<li><p> Parameter 2: positive-definite <code class="reqn">k \times k</code>
covariance matrix <code class="reqn">\Sigma</code>
</p>
</li>
<li><p> Parameter 3: kurtosis parameter <code class="reqn">\kappa</code>
</p>
</li>
<li><p> Mean: <code class="reqn">E(\theta) = </code>
</p>
</li>
<li><p> Variance: <code class="reqn">var(\theta) =</code>
</p>
</li>
<li><p> Mode: <code class="reqn">mode(\theta) = </code>
</p>
</li></ul>

<p>The multivariate power exponential distribution, or multivariate
exponential power distribution, is a multidimensional extension of
the one-dimensional or univariate power exponential distribution.
Gomez-Villegas (1998) and Sanchez-Manzano et al. (2002) proposed
multivariate and matrix generalizations of the PE family of
distributions and studied their properties in relation to multivariate
Elliptically Contoured (EC) distributions.
</p>
<p>The multivariate power exponential distribution includes the
multivariate normal distribution (<code class="reqn">\kappa = 1</code>) and
multivariate Laplace distribution (<code class="reqn">\kappa = 0.5</code>) as
special cases, depending on the kurtosis or <code class="reqn">\kappa</code>
parameter. A multivariate uniform occurs as
<code class="reqn">\kappa \rightarrow \infty</code>.
</p>
<p>If the goal is to use a multivariate Laplace distribution, the
<code>dmvlc</code> function will perform faster and more accurately.
</p>
<p>In practice, <code class="reqn">\textbf{U}</code> is fully unconstrained for proposals
when its diagonal is log-transformed. The diagonal is exponentiated
after a proposal and before other calculations. Overall, the Cholesky
parameterization is faster than the traditional parameterization.
Compared with <code>dmvpe</code>, <code>dmvpec</code> must additionally
matrix-multiply the Cholesky back to the covariance matrix, but it
does not have to check for or correct the covariance matrix to
positive-definiteness, which overall is slower. Compared with
<code>rmvpe</code>, <code>rmvpec</code> is faster because the Cholesky decomposition
has already been performed.
</p>
<p>The <code>rmvpec</code> function is a modified form of the rmvpowerexp function
in the MNM package.
</p>


<h3>Value</h3>

<p><code>dmvpec</code> gives the density and
<code>rmvpec</code> generates random deviates.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>References</h3>

<p>Gomez, E., Gomez-Villegas, M.A., and Marin, J.M. (1998). &quot;A
Multivariate Generalization of the Power Exponential Family of
Distributions&quot;. <em>Communications in Statistics-Theory and
Methods</em>, 27(3), p. 589&ndash;600.
</p>
<p>Sanchez-Manzano, E.G., Gomez-Villegas, M.A., and Marn-Diazaraque,
J.M. (2002). &quot;A Matrix Variate Generalization of the Power Exponential
Family of Distributions&quot;. <em>Communications in Statistics, Part A -
Theory and Methods</em> [Split from: J(CommStat)], 31(12), p. 2167&ndash;2182.
</p>


<h3>See Also</h3>

<p><code><a href="Matrix.html#topic+chol">chol</a></code>,
<code><a href="#topic+dlaplace">dlaplace</a></code>,
<code><a href="#topic+dmvlc">dmvlc</a></code>,
<code><a href="#topic+dmvnc">dmvnc</a></code>,
<code><a href="#topic+dmvnpc">dmvnpc</a></code>,
<code><a href="stats.html#topic+dnorm">dnorm</a></code>,
<code><a href="#topic+dnormp">dnormp</a></code>,
<code><a href="#topic+dnormv">dnormv</a></code>, and
<code><a href="#topic+dpe">dpe</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
n &lt;- 100
k &lt;- 3
x &lt;- matrix(runif(n*k),n,k)
mu &lt;- matrix(runif(n*k),n,k)
Sigma &lt;- diag(k)
U &lt;- chol(Sigma)
dmvpec(x, mu, U, kappa=1)
X &lt;- rmvpec(n, mu, U, kappa=1)
joint.density.plot(X[,1], X[,2], color=TRUE)
</code></pre>

<hr>
<h2 id='dist.Multivariate.t'>Multivariate t Distribution</h2><span id='topic+dmvt'></span><span id='topic+rmvt'></span>

<h3>Description</h3>

<p>These functions provide the density and random number generation for
the multivariate t distribution, otherwise called the multivariate
Student distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dmvt(x, mu, S, df=Inf, log=FALSE)
rmvt(n=1, mu, S, df=Inf)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.Multivariate.t_+3A_x">x</code></td>
<td>
<p>This is either a vector of length <code class="reqn">k</code> or a matrix with
a number of columns, <code class="reqn">k</code>, equal to the number of columns in
scale matrix <code class="reqn">\textbf{S}</code>.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.t_+3A_n">n</code></td>
<td>
<p>This is the number of random draws.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.t_+3A_mu">mu</code></td>
<td>
<p>This is a numeric vector or matrix representing the location
parameter,<code class="reqn">\mu</code> (the mean vector), of the multivariate
distribution (equal to the expected value when <code>df &gt; 1</code>,
otherwise represented as <code class="reqn">\nu &gt; 1</code>). When a vector, it
must be of length <code class="reqn">k</code>, or must have <code class="reqn">k</code> columns as a matrix,
as defined above.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.t_+3A_s">S</code></td>
<td>
<p>This is a <code class="reqn">k \times k</code> positive-definite scale
matrix <code class="reqn">\textbf{S}</code>, such that <code>S*df/(df-2)</code> is the
variance-covariance matrix when <code>df &gt; 2</code>. A vector of
length 1 is also allowed (in this case, <code class="reqn">k=1</code> is set).</p>
</td></tr>
<tr><td><code id="dist.Multivariate.t_+3A_df">df</code></td>
<td>
<p>This is the degrees of freedom, and is often represented
with <code class="reqn">\nu</code>.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.t_+3A_log">log</code></td>
<td>
<p>Logical. If <code>log=TRUE</code>, then the logarithm of the
density is returned.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> Application: Continuous Multivariate
</p>
</li>
<li><p> Density: </p>
<p style="text-align: center;"><code class="reqn">p(\theta) =
    \frac{\Gamma[(\nu+k)/2]}{\Gamma(\nu/2)\nu^{k/2}\pi^{k/2}|\Sigma|^{1/2}[1
      + (1/\nu)(\theta-\mu)^{\mathrm{T}} \Sigma^{-1}
      (\theta-\mu)]^{(\nu+k)/2}}</code>
</p>

</li>
<li><p> Inventor: Unknown (to me, anyway)
</p>
</li>
<li><p> Notation 1: <code class="reqn">\theta \sim \mathrm{t}_k(\mu,
    \Sigma, \nu)</code>
</p>
</li>
<li><p> Notation 2: <code class="reqn">p(\theta) = \mathrm{t}_k(\theta | \mu,
    \Sigma, \nu)</code>
</p>
</li>
<li><p> Parameter 1: location vector <code class="reqn">\mu</code>
</p>
</li>
<li><p> Parameter 2: positive-definite <code class="reqn">k \times k</code> scale
matrix <code class="reqn">\Sigma</code>
</p>
</li>
<li><p> Parameter 3: degrees of freedom <code class="reqn">\nu &gt; 0</code> (df in the
functions)
</p>
</li>
<li><p> Mean: <code class="reqn">E(\theta) = \mu</code>, for <code class="reqn">\nu &gt; 1</code>, otherwise undefined
</p>
</li>
<li><p> Variance: <code class="reqn">var(\theta) = \frac{\nu}{\nu - 2}
    \Sigma</code>, for <code class="reqn">\nu &gt; 2</code>
</p>
</li>
<li><p> Mode: <code class="reqn">mode(\theta) = \mu</code>
</p>
</li></ul>

<p>The multivariate t distribution, also called the multivariate Student or
multivariate Student t distribution, is a multidimensional extension of the
one-dimensional or univariate Student t distribution. A random vector is
considered to be multivariate t-distributed if every linear
combination of its components has a univariate Student t-distribution.
This distribution has a mean parameter vector <code class="reqn">\mu</code> of length
<code class="reqn">k</code>, and a <code class="reqn">k \times k</code> scale matrix <code class="reqn">\textbf{S}</code>,
which must be positive-definite. When degrees of freedom
<code class="reqn">\nu=1</code>, this is the multivariate Cauchy distribution.
</p>


<h3>Value</h3>

<p><code>dmvt</code> gives the density and 
<code>rmvt</code> generates random deviates.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+dinvwishart">dinvwishart</a></code>,
<code><a href="#topic+dmvc">dmvc</a></code>,
<code><a href="#topic+dmvcp">dmvcp</a></code>,
<code><a href="#topic+dmvtp">dmvtp</a></code>,
<code><a href="#topic+dst">dst</a></code>,
<code><a href="#topic+dstp">dstp</a></code>, and
<code><a href="stats.html#topic+dt">dt</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
x &lt;- seq(-2,4,length=21)
y &lt;- 2*x+10
z &lt;- x+cos(y) 
mu &lt;- c(1,12,2)
S &lt;- matrix(c(1,2,0,2,5,0.5,0,0.5,3), 3, 3)
df &lt;- 4
f &lt;- dmvt(cbind(x,y,z), mu, S, df)
X &lt;- rmvt(1000, c(0,1,2), S, 5)
joint.density.plot(X[,1], X[,2], color=TRUE)
</code></pre>

<hr>
<h2 id='dist.Multivariate.t.Cholesky'>Multivariate t Distribution: Cholesky Parameterization</h2><span id='topic+dmvtc'></span><span id='topic+rmvtc'></span>

<h3>Description</h3>

<p>These functions provide the density and random number generation for
the multivariate t distribution, otherwise called the multivariate
Student distribution, given the Cholesky parameterization.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dmvtc(x, mu, U, df=Inf, log=FALSE)
rmvtc(n=1, mu, U, df=Inf)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.Multivariate.t.Cholesky_+3A_x">x</code></td>
<td>
<p>This is either a vector of length <code class="reqn">k</code> or a matrix with
a number of columns, <code class="reqn">k</code>, equal to the number of columns in
scale matrix <code class="reqn">\textbf{S}</code>.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.t.Cholesky_+3A_n">n</code></td>
<td>
<p>This is the number of random draws.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.t.Cholesky_+3A_mu">mu</code></td>
<td>
<p>This is a numeric vector or matrix representing the location
parameter,<code class="reqn">\mu</code> (the mean vector), of the multivariate
distribution (equal to the expected value when <code>df &gt; 1</code>,
otherwise represented as <code class="reqn">\nu &gt; 1</code>). When a vector, it
must be of length <code class="reqn">k</code>, or must have <code class="reqn">k</code> columns as a matrix,
as defined above.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.t.Cholesky_+3A_u">U</code></td>
<td>
<p>This is the <code class="reqn">k \times k</code> upper-triangular matrix
that is Cholesky factor <code class="reqn">\textbf{U}</code> of scale matrix
<code class="reqn">\textbf{S}</code>, such that <code>S*df/(df-2)</code> is the
variance-covariance matrix when <code>df &gt; 2</code>.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.t.Cholesky_+3A_df">df</code></td>
<td>
<p>This is the degrees of freedom, and is often represented
with <code class="reqn">\nu</code>.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.t.Cholesky_+3A_log">log</code></td>
<td>
<p>Logical. If <code>log=TRUE</code>, then the logarithm of the
density is returned.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> Application: Continuous Multivariate
</p>
</li>
<li><p> Density: </p>
<p style="text-align: center;"><code class="reqn">p(\theta) =
    \frac{\Gamma[(\nu+k)/2]}{\Gamma(\nu/2)\nu^{k/2}\pi^{k/2}|\Sigma|^{1/2}[1
      + (1/\nu)(\theta-\mu)^{\mathrm{T}} \Sigma^{-1}
      (\theta-\mu)]^{(\nu+k)/2}}</code>
</p>

</li>
<li><p> Inventor: Unknown (to me, anyway)
</p>
</li>
<li><p> Notation 1: <code class="reqn">\theta \sim \mathrm{t}_k(\mu,
    \Sigma, \nu)</code>
</p>
</li>
<li><p> Notation 2: <code class="reqn">p(\theta) = \mathrm{t}_k(\theta | \mu,
    \Sigma, \nu)</code>
</p>
</li>
<li><p> Parameter 1: location vector <code class="reqn">\mu</code>
</p>
</li>
<li><p> Parameter 2: positive-definite <code class="reqn">k \times k</code> scale
matrix <code class="reqn">\Sigma</code>
</p>
</li>
<li><p> Parameter 3: degrees of freedom <code class="reqn">\nu &gt; 0</code> (df in the
functions)
</p>
</li>
<li><p> Mean: <code class="reqn">E(\theta) = \mu</code>, for <code class="reqn">\nu &gt; 1</code>, otherwise undefined
</p>
</li>
<li><p> Variance: <code class="reqn">var(\theta) = \frac{\nu}{\nu - 2}
    \Sigma</code>, for <code class="reqn">\nu &gt; 2</code>
</p>
</li>
<li><p> Mode: <code class="reqn">mode(\theta) = \mu</code>
</p>
</li></ul>

<p>The multivariate t distribution, also called the multivariate Student or
multivariate Student t distribution, is a multidimensional extension of the
one-dimensional or univariate Student t distribution. A random vector is
considered to be multivariate t-distributed if every linear
combination of its components has a univariate Student t-distribution.
This distribution has a mean parameter vector <code class="reqn">\mu</code> of length
<code class="reqn">k</code>, and an upper-triangular <code class="reqn">k \times k</code> matrix that is
Cholesky factor <code class="reqn">\textbf{U}</code>, as per the <code><a href="Matrix.html#topic+chol">chol</a></code>
function for Cholesky decomposition. When degrees of freedom
<code class="reqn">\nu=1</code>, this is the multivariate Cauchy distribution.
</p>
<p>In practice, <code class="reqn">\textbf{U}</code> is fully unconstrained for proposals
when its diagonal is log-transformed. The diagonal is exponentiated
after a proposal and before other calculations. Overall, the Cholesky
parameterization is faster than the traditional parameterization.
Compared with <code>dmvt</code>, <code>dmvtc</code> must additionally
matrix-multiply the Cholesky back to the scale matrix, but it
does not have to check for or correct the scale matrix to
positive-definiteness, which overall is slower. The same is true when
comparing <code>rmvt</code> and <code>rmvtc</code>.
</p>


<h3>Value</h3>

<p><code>dmvtc</code> gives the density and 
<code>rmvtc</code> generates random deviates.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="Matrix.html#topic+chol">chol</a></code>,
<code><a href="#topic+dinvwishartc">dinvwishartc</a></code>,
<code><a href="#topic+dmvc">dmvc</a></code>,
<code><a href="#topic+dmvcp">dmvcp</a></code>,
<code><a href="#topic+dmvtp">dmvtp</a></code>,
<code><a href="#topic+dst">dst</a></code>,
<code><a href="#topic+dstp">dstp</a></code>, and
<code><a href="stats.html#topic+dt">dt</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
x &lt;- seq(-2,4,length=21)
y &lt;- 2*x+10
z &lt;- x+cos(y) 
mu &lt;- c(1,12,2)
S &lt;- matrix(c(1,2,0,2,5,0.5,0,0.5,3), 3, 3)
U &lt;- chol(S)
df &lt;- 4
f &lt;- dmvtc(cbind(x,y,z), mu, U, df)
X &lt;- rmvtc(1000, c(0,1,2), U, 5)
joint.density.plot(X[,1], X[,2], color=TRUE)
</code></pre>

<hr>
<h2 id='dist.Multivariate.t.Precision'>Multivariate t Distribution: Precision Parameterization</h2><span id='topic+dmvtp'></span><span id='topic+rmvtp'></span>

<h3>Description</h3>

<p>These functions provide the density and random number generation for
the multivariate t distribution, otherwise called the multivariate
Student distribution. These functions use the precision
parameterization.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dmvtp(x, mu, Omega, nu=Inf, log=FALSE)
rmvtp(n=1, mu, Omega, nu=Inf)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.Multivariate.t.Precision_+3A_x">x</code></td>
<td>
<p>This is either a vector of length <code class="reqn">k</code> or a matrix with 
a number of columns, <code class="reqn">k</code>, equal to the number of columns in
precision matrix <code class="reqn">\Omega</code>.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.t.Precision_+3A_n">n</code></td>
<td>
<p>This is the number of random draws.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.t.Precision_+3A_mu">mu</code></td>
<td>
<p>This is a numeric vector representing the location parameter,
<code class="reqn">\mu</code> (the mean vector), of the multivariate distribution
(equal to the expected value when <code>df &gt; 1</code>, otherwise
represented as <code class="reqn">\nu &gt; 1</code>). It must be of length
<code class="reqn">k</code>, as defined above.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.t.Precision_+3A_omega">Omega</code></td>
<td>
<p>This is a <code class="reqn">k \times k</code> positive-definite
precision matrix <code class="reqn">\Omega</code>.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.t.Precision_+3A_nu">nu</code></td>
<td>
<p>This is the degrees of freedom <code class="reqn">\nu</code>, which must be
positive.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.t.Precision_+3A_log">log</code></td>
<td>
<p>Logical. If <code>log=TRUE</code>, then the logarithm of the
density is returned.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> Application: Continuous Multivariate
</p>
</li>
<li><p> Density: </p>
<p style="text-align: center;"><code class="reqn">p(\theta) =
    \frac{\Gamma((\nu+k)/2)}{\Gamma(\nu/2)\nu^{k/2}\pi^{k/2}}
    |\Omega|^{1/2} (1 + \frac{1}{\nu} (\theta-\mu)^T \Omega
    (\theta-\mu))^{-(\nu+k)/2}</code>
</p>

</li>
<li><p> Inventor: Unknown (to me, anyway)
</p>
</li>
<li><p> Notation 1: <code class="reqn">\theta \sim \mathrm{t}_k(\mu,
    \Omega^{-1}, \nu)</code>
</p>
</li>
<li><p> Notation 2: <code class="reqn">p(\theta) = \mathrm{t}_k(\theta | \mu,
    \Omega^{-1}, \nu)</code>
</p>
</li>
<li><p> Parameter 1: location vector <code class="reqn">\mu</code>
</p>
</li>
<li><p> Parameter 2: positive-definite <code class="reqn">k \times k</code> precision
matrix <code class="reqn">\Omega</code>
</p>
</li>
<li><p> Parameter 3: degrees of freedom <code class="reqn">\nu &gt; 0</code>
</p>
</li>
<li><p> Mean: <code class="reqn">E(\theta) = \mu</code>, for <code class="reqn">\nu &gt; 1</code>, otherwise undefined
</p>
</li>
<li><p> Variance: <code class="reqn">var(\theta) = \frac{\nu}{\nu - 2}
    \Omega^{-1}</code>, for <code class="reqn">\nu
    &gt; 2</code>
</p>
</li>
<li><p> Mode: <code class="reqn">mode(\theta) = \mu</code>
</p>
</li></ul>

<p>The multivariate t distribution, also called the multivariate Student or
multivariate Student t distribution, is a multidimensional extension of the
one-dimensional or univariate Student t distribution. A random vector is
considered to be multivariate t-distributed if every linear
combination of its components has a univariate Student t-distribution.
</p>
<p>It is usually parameterized with mean and a covariance matrix, or in
Bayesian inference, with mean and a precision matrix, where the
precision matrix is the matrix inverse of the covariance matrix. These
functions provide the precision parameterization for convenience and
familiarity. It is easier to calculate a multivariate t density
with the precision parameterization, because a matrix inversion can be
avoided.
</p>
<p>This distribution has a mean parameter vector <code class="reqn">\mu</code> of length
<code class="reqn">k</code>, and a <code class="reqn">k \times k</code> precision matrix
<code class="reqn">\Omega</code>, which must be positive-definite. When degrees of
freedom <code class="reqn">\nu=1</code>, this is the multivariate Cauchy distribution.
</p>


<h3>Value</h3>

<p><code>dmvtp</code> gives the density and 
<code>rmvtp</code> generates random deviates.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+dwishart">dwishart</a></code>,
<code><a href="#topic+dmvc">dmvc</a></code>,
<code><a href="#topic+dmvcp">dmvcp</a></code>,
<code><a href="#topic+dmvt">dmvt</a></code>,
<code><a href="#topic+dst">dst</a></code>,
<code><a href="#topic+dstp">dstp</a></code>, and
<code><a href="stats.html#topic+dt">dt</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
x &lt;- seq(-2,4,length=21)
y &lt;- 2*x+10
z &lt;- x+cos(y) 
mu &lt;- c(1,12,2)
Omega &lt;- matrix(c(1,2,0,2,5,0.5,0,0.5,3), 3, 3)
nu &lt;- 4
f &lt;- dmvtp(cbind(x,y,z), mu, Omega, nu)
X &lt;- rmvtp(1000, c(0,1,2), diag(3), 5)
joint.density.plot(X[,1], X[,2], color=TRUE)
</code></pre>

<hr>
<h2 id='dist.Multivariate.t.Precision.Cholesky'>Multivariate t Distribution: Precision-Cholesky Parameterization</h2><span id='topic+dmvtpc'></span><span id='topic+rmvtpc'></span>

<h3>Description</h3>

<p>These functions provide the density and random number generation for
the multivariate t distribution, otherwise called the multivariate
Student distribution. These functions use the precision and Cholesky
parameterization.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dmvtpc(x, mu, U, nu=Inf, log=FALSE)
rmvtpc(n=1, mu, U, nu=Inf)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.Multivariate.t.Precision.Cholesky_+3A_x">x</code></td>
<td>
<p>This is either a vector of length <code class="reqn">k</code> or a matrix with 
a number of columns, <code class="reqn">k</code>, equal to the number of columns in
precision matrix <code class="reqn">\Omega</code>.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.t.Precision.Cholesky_+3A_n">n</code></td>
<td>
<p>This is the number of random draws.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.t.Precision.Cholesky_+3A_mu">mu</code></td>
<td>
<p>This is a numeric vector representing the location parameter,
<code class="reqn">\mu</code> (the mean vector), of the multivariate distribution
(equal to the expected value when <code>df &gt; 1</code>, otherwise
represented as <code class="reqn">\nu &gt; 1</code>). It must be of length
<code class="reqn">k</code>, as defined above.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.t.Precision.Cholesky_+3A_u">U</code></td>
<td>
<p>This is a <code class="reqn">k \times k</code> upper-triangular of the
precision matrix that is Cholesky fator <code class="reqn">\textbf{U}</code> of
precision matrix <code class="reqn">\Omega</code>.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.t.Precision.Cholesky_+3A_nu">nu</code></td>
<td>
<p>This is the degrees of freedom <code class="reqn">\nu</code>, which must be
positive.</p>
</td></tr>
<tr><td><code id="dist.Multivariate.t.Precision.Cholesky_+3A_log">log</code></td>
<td>
<p>Logical. If <code>log=TRUE</code>, then the logarithm of the
density is returned.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> Application: Continuous Multivariate
</p>
</li>
<li><p> Density: </p>
<p style="text-align: center;"><code class="reqn">p(\theta) =
    \frac{\Gamma((\nu+k)/2)}{\Gamma(\nu/2)\nu^{k/2}\pi^{k/2}}
    |\Omega|^{1/2} (1 + \frac{1}{\nu} (\theta-\mu)^T \Omega
    (\theta-\mu))^{-(\nu+k)/2}</code>
</p>

</li>
<li><p> Inventor: Unknown (to me, anyway)
</p>
</li>
<li><p> Notation 1: <code class="reqn">\theta \sim \mathrm{t}_k(\mu,
    \Omega^{-1}, \nu)</code>
</p>
</li>
<li><p> Notation 2: <code class="reqn">p(\theta) = \mathrm{t}_k(\theta | \mu,
    \Omega^{-1}, \nu)</code>
</p>
</li>
<li><p> Parameter 1: location vector <code class="reqn">\mu</code>
</p>
</li>
<li><p> Parameter 2: positive-definite <code class="reqn">k \times k</code> precision
matrix <code class="reqn">\Omega</code>
</p>
</li>
<li><p> Parameter 3: degrees of freedom <code class="reqn">\nu &gt; 0</code>
</p>
</li>
<li><p> Mean: <code class="reqn">E(\theta) = \mu</code>, for <code class="reqn">\nu &gt; 1</code>, otherwise undefined
</p>
</li>
<li><p> Variance: <code class="reqn">var(\theta) = \frac{\nu}{\nu - 2}
    \Omega^{-1}</code>, for <code class="reqn">\nu
    &gt; 2</code>
</p>
</li>
<li><p> Mode: <code class="reqn">mode(\theta) = \mu</code>
</p>
</li></ul>

<p>The multivariate t distribution, also called the multivariate Student or
multivariate Student t distribution, is a multidimensional extension of the
one-dimensional or univariate Student t distribution. A random vector is
considered to be multivariate t-distributed if every linear
combination of its components has a univariate Student t-distribution.
</p>
<p>It is usually parameterized with mean and a covariance matrix, or in
Bayesian inference, with mean and a precision matrix, where the
precision matrix is the matrix inverse of the covariance matrix. These
functions provide the precision parameterization for convenience and
familiarity. It is easier to calculate a multivariate t density
with the precision parameterization, because a matrix inversion can be
avoided. The precision matrix is replaced with an upper-triangular
<code class="reqn">k \times k</code> matrix that is Cholesky factor
<code class="reqn">\textbf{U}</code>, as per the <code><a href="Matrix.html#topic+chol">chol</a></code> function for Cholesky
decomposition.
</p>
<p>This distribution has a mean parameter vector <code class="reqn">\mu</code> of length
<code class="reqn">k</code>, and a <code class="reqn">k \times k</code> precision matrix
<code class="reqn">\Omega</code>, which must be positive-definite. When degrees of
freedom <code class="reqn">\nu=1</code>, this is the multivariate Cauchy distribution.
</p>
<p>In practice, <code class="reqn">\textbf{U}</code> is fully unconstrained for proposals
when its diagonal is log-transformed. The diagonal is exponentiated
after a proposal and before other calculations. Overall, the Cholesky
parameterization is faster than the traditional parameterization.
Compared with <code>dmvtp</code>, <code>dmvtpc</code> must additionally
matrix-multiply the Cholesky back to the precision matrix, but it
does not have to check for or correct the precision matrix to
positive-definiteness, which overall is slower. Compared with
<code>rmvtp</code>, <code>rmvtpc</code> is faster because the Cholesky decomposition
has already been performed.
</p>


<h3>Value</h3>

<p><code>dmvtpc</code> gives the density and 
<code>rmvtpc</code> generates random deviates.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="Matrix.html#topic+chol">chol</a></code>,
<code><a href="#topic+dwishartc">dwishartc</a></code>,
<code><a href="#topic+dmvc">dmvc</a></code>,
<code><a href="#topic+dmvcp">dmvcp</a></code>,
<code><a href="#topic+dmvtc">dmvtc</a></code>,
<code><a href="#topic+dst">dst</a></code>,
<code><a href="#topic+dstp">dstp</a></code>, and
<code><a href="stats.html#topic+dt">dt</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
x &lt;- seq(-2,4,length=21)
y &lt;- 2*x+10
z &lt;- x+cos(y) 
mu &lt;- c(1,12,2)
Omega &lt;- matrix(c(1,2,0,2,5,0.5,0,0.5,3), 3, 3)
U &lt;- chol(Omega)
nu &lt;- 4
f &lt;- dmvtpc(cbind(x,y,z), mu, U, nu)
X &lt;- rmvtpc(1000, c(0,1,2), U, 5)
joint.density.plot(X[,1], X[,2], color=TRUE)
</code></pre>

<hr>
<h2 id='dist.Normal.Inverse.Wishart'>Normal-Inverse-Wishart Distribution</h2><span id='topic+dnorminvwishart'></span><span id='topic+rnorminvwishart'></span>

<h3>Description</h3>

<p>These functions provide the density and random number generation for
the normal-inverse-Wishart distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dnorminvwishart(mu, mu0, lambda, Sigma, S, nu, log=FALSE) 
rnorminvwishart(n=1, mu0, lambda, S, nu)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.Normal.Inverse.Wishart_+3A_mu">mu</code></td>
<td>
<p>This is data or parameters in the form of a vector of length
<code class="reqn">k</code> or a matrix with <code class="reqn">k</code> columns.</p>
</td></tr>
<tr><td><code id="dist.Normal.Inverse.Wishart_+3A_mu0">mu0</code></td>
<td>
<p>This is mean vector <code class="reqn">\mu_0</code> with length <code class="reqn">k</code> or
matrix with <code class="reqn">k</code> columns.</p>
</td></tr>
<tr><td><code id="dist.Normal.Inverse.Wishart_+3A_lambda">lambda</code></td>
<td>
<p>This is a positive-only scalar.</p>
</td></tr>
<tr><td><code id="dist.Normal.Inverse.Wishart_+3A_n">n</code></td>
<td>
<p>This is the number of random draws.</p>
</td></tr>
<tr><td><code id="dist.Normal.Inverse.Wishart_+3A_nu">nu</code></td>
<td>
<p>This is the scalar degrees of freedom <code class="reqn">\nu</code>.</p>
</td></tr>
<tr><td><code id="dist.Normal.Inverse.Wishart_+3A_sigma">Sigma</code></td>
<td>
<p>This is a <code class="reqn">k \times k</code> covariance matrix
<code class="reqn">\Sigma</code>.</p>
</td></tr>
<tr><td><code id="dist.Normal.Inverse.Wishart_+3A_s">S</code></td>
<td>
<p>This is the symmetric, positive-semidefinite, <code class="reqn">k \times
      k</code> scale matrix <code class="reqn">\textbf{S}</code>.</p>
</td></tr>
<tr><td><code id="dist.Normal.Inverse.Wishart_+3A_log">log</code></td>
<td>
<p>Logical. If <code>log=TRUE</code>, then the logarithm of the
density is returned.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> Application: Continuous Multivariate
</p>
</li>
<li><p> Density: <code class="reqn">p(\mu, \Sigma) = \mathcal{N}(\mu | \mu_0,
      \frac{1}{\lambda}\Sigma) \mathcal{W}^{-1}(\Sigma | \nu,
      \textbf{S})</code>
</p>
</li>
<li><p> Inventors: Unknown
</p>
</li>
<li><p> Notation 1: <code class="reqn">(\mu, \Sigma) \sim \mathcal{NIW}(\mu_0, \lambda,
    \textbf{S}, \nu)</code>
</p>
</li>
<li><p> Notation 2: <code class="reqn">p(\mu, \Sigma) = \mathcal{NIW}(\mu, \Sigma |
    \mu_0, \lambda, \textbf{S}, \nu)</code>
</p>
</li>
<li><p> Parameter 1: location vector <code class="reqn">\mu_0</code>
</p>
</li>
<li><p> Parameter 2: <code class="reqn">\lambda &gt; 0</code>
</p>
</li>
<li><p> Parameter 3: symmetric, positive-semidefinite
<code class="reqn">k \times k</code> scale matrix <code class="reqn">\textbf{S}</code>
</p>
</li>
<li><p> Parameter 4: degrees of freedom <code class="reqn">\nu \ge k</code>
</p>
</li>
<li><p> Mean: Unknown
</p>
</li>
<li><p> Variance: Unknown
</p>
</li>
<li><p> Mode: Unknown
</p>
</li></ul>

<p>The normal-inverse-Wishart distribution, or Gaussian-inverse-Wishart
distribution, is a multivariate four-parameter continuous probability
distribution. It is the conjugate prior of a multivariate normal
distribution with unknown mean and covariance matrix.
</p>


<h3>Value</h3>

<p><code>dnorminvwishart</code> gives the density and 
<code>rnorminvwishart</code> generates random deviates and returns a list
with two components.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+dmvn">dmvn</a></code> and
<code><a href="#topic+dinvwishart">dinvwishart</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
K &lt;- 3
mu &lt;- rnorm(K)
mu0 &lt;- rnorm(K)
nu &lt;- K + 1
S &lt;- diag(K)
lambda &lt;- runif(1) #Real scalar
Sigma &lt;- as.positive.definite(matrix(rnorm(K^2),K,K))
x &lt;- dnorminvwishart(mu, mu0, lambda, Sigma, S, nu, log=TRUE)
out &lt;- rnorminvwishart(n=10, mu0, lambda, S, nu)
joint.density.plot(out$mu[,1], out$mu[,2], color=TRUE)
</code></pre>

<hr>
<h2 id='dist.Normal.Laplace'>Normal-Laplace Distribution: Univariate Asymmetric</h2><span id='topic+dnormlaplace'></span><span id='topic+rnormlaplace'></span>

<h3>Description</h3>

<p>These functions provide the density and random generation for the
univariate, asymmetric, normal-Laplace distribution with location
parameter <code class="reqn">\mu</code>, scale parameter <code class="reqn">\sigma</code>, and
tail-behavior parameters <code class="reqn">\alpha</code> and <code class="reqn">\beta</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dnormlaplace(x, mu=0, sigma=1, alpha=1, beta=1, log=FALSE)
rnormlaplace(n, mu=0, sigma=1, alpha=1, beta=1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.Normal.Laplace_+3A_x">x</code></td>
<td>
<p>This is a vector of data.</p>
</td></tr>
<tr><td><code id="dist.Normal.Laplace_+3A_n">n</code></td>
<td>
<p>This is the number of observations, which must be a positive
integer that has length 1.</p>
</td></tr>
<tr><td><code id="dist.Normal.Laplace_+3A_mu">mu</code></td>
<td>
<p>This is the location parameter <code class="reqn">\mu</code>.</p>
</td></tr>
<tr><td><code id="dist.Normal.Laplace_+3A_sigma">sigma</code></td>
<td>
<p>This is the scale parameter <code class="reqn">\sigma</code>, which
must be positive.</p>
</td></tr>
<tr><td><code id="dist.Normal.Laplace_+3A_alpha">alpha</code></td>
<td>
<p>This is shape parameter <code class="reqn">\alpha</code> for
left-tail behavior.</p>
</td></tr>
<tr><td><code id="dist.Normal.Laplace_+3A_beta">beta</code></td>
<td>
<p>This is shape parameter <code class="reqn">\beta</code> for
right-tail behavior.</p>
</td></tr>
<tr><td><code id="dist.Normal.Laplace_+3A_log">log</code></td>
<td>
<p>Logical. If <code>log=TRUE</code>, then the logarithm of the
density is returned.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> Application: Continuous Univariate
</p>
</li>
<li><p> Density: <code class="reqn">p(\theta) = \frac{\alpha\beta}{\alpha +
	\beta}\phi\frac{\theta - \mu}{\sigma} [R(\alpha\sigma -
      \frac{\theta - \mu}{\sigma}) + R(\beta\sigma + \frac{\theta -
	\mu}{\sigma})]</code>
</p>
</li>
<li><p> Inventor: Reed (2006)
</p>
</li>
<li><p> Notation 1: <code class="reqn">\theta \sim
    \mathrm{NL}(\mu,\sigma,\alpha,\beta)</code>
</p>
</li>
<li><p> Notation 2: <code class="reqn">p(\theta) = \mathrm{NL}(\theta | \mu,
    \sigma, \alpha, \beta)</code>
</p>
</li>
<li><p> Parameter 1: location parameter <code class="reqn">\mu</code>
</p>
</li>
<li><p> Parameter 2: scale parameter <code class="reqn">\sigma &gt; 0</code>
</p>
</li>
<li><p> Parameter 3: shape parameter <code class="reqn">\alpha &gt; 0</code>
</p>
</li>
<li><p> Parameter 4: shape parameter <code class="reqn">\beta &gt; 0</code>
</p>
</li>
<li><p> Mean:
</p>
</li>
<li><p> Variance:
</p>
</li>
<li><p> Mode:
</p>
</li></ul>

<p>The normal-Laplace (NL) distribution is the convolution of a normal
distribution and a skew-Laplace distribution. When the NL distribution
is symmetric (when <code class="reqn">\alpha = \beta</code>), it behaves
somewhat like the normal distribution in the middle of its range,
somewhat like the Laplace distribution in its tails, and functions
generally between the normal and Laplace distributions. Skewness is
parameterized by including a skew-Laplace component. It may be applied,
for example, to the logarithmic price of a financial instrument.
</p>
<p>Parameters <code class="reqn">\alpha</code> and <code class="reqn">\beta</code> determine the
behavior in the left and right tails, respectively. A small value
corresponds to heaviness in the corresponding tail. As
<code class="reqn">\sigma</code> approaches zero, the NL distribution approaches a
skew-Laplace distribution. As <code class="reqn">\beta</code> approaches infinity,
the NL distribution approaches a normal distribution, though it never
quite reaches it.
</p>


<h3>Value</h3>

<p><code>dnormlaplace</code> gives the density, and
<code>rnormlaplace</code> generates random deviates.
</p>


<h3>References</h3>

<p>Reed, W.J. (2006). &quot;The Normal-Laplace Distribution and Its Relatives&quot;.
In <em>Advances in Distribution Theory, Order Statistics and
Inference</em>, p. 61&ndash;74, Birkhauser, Boston.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dalaplace">dalaplace</a></code>,
<code><a href="#topic+dallaplace">dallaplace</a></code>,
<code><a href="#topic+daml">daml</a></code>,
<code><a href="#topic+dlaplace">dlaplace</a></code>, and
<code><a href="stats.html#topic+dnorm">dnorm</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
x &lt;- dnormlaplace(1,0,1,0.5,2)
x &lt;- rnormlaplace(100,0,1,0.5,2)

#Plot Probability Functions
x &lt;- seq(from=-5, to=5, by=0.1)
plot(x, dlaplace(x,0,0.5), ylim=c(0,1), type="l", main="Probability Function",
     ylab="density", col="red")
lines(x, dlaplace(x,0,1), type="l", col="green")
lines(x, dlaplace(x,0,2), type="l", col="blue")
legend(2, 0.9, expression(paste(mu==0, ", ", lambda==0.5),
     paste(mu==0, ", ", lambda==1), paste(mu==0, ", ", lambda==2)),
     lty=c(1,1,1), col=c("red","green","blue"))
</code></pre>

<hr>
<h2 id='dist.Normal.Mixture'>Mixture of Normal Distributions</h2><span id='topic+dnormm'></span><span id='topic+pnormm'></span><span id='topic+rnormm'></span>

<h3>Description</h3>

<p>These functions provide the density, cumulative, and random generation
for the mixture of univariate normal distributions with probability
<code class="reqn">p</code>, mean <code class="reqn">\mu</code> and standard deviation <code class="reqn">\sigma</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dnormm(x, p, mu, sigma, log=FALSE)
pnormm(q, p, mu, sigma, lower.tail=TRUE, log.p=FALSE)
rnormm(n, p, mu, sigma)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.Normal.Mixture_+3A_x">x</code>, <code id="dist.Normal.Mixture_+3A_q">q</code></td>
<td>
<p>This is vector of values at which the density will be evaluated.</p>
</td></tr>
<tr><td><code id="dist.Normal.Mixture_+3A_p">p</code></td>
<td>
<p>This is a vector of length <code class="reqn">M</code> of probabilities for <code class="reqn">M</code>
components. The sum of the vector must be one.</p>
</td></tr>
<tr><td><code id="dist.Normal.Mixture_+3A_n">n</code></td>
<td>
<p>This is the number of observations, which must be a positive
integer that has length 1.</p>
</td></tr>
<tr><td><code id="dist.Normal.Mixture_+3A_mu">mu</code></td>
<td>
<p>This is a vector of length <code class="reqn">M</code> that is the mean
parameter <code class="reqn">\mu</code>.</p>
</td></tr>
<tr><td><code id="dist.Normal.Mixture_+3A_sigma">sigma</code></td>
<td>
<p>This is a vector of length <code class="reqn">M</code> that is the standard
deviation parameter <code class="reqn">\sigma</code>, which must be positive.</p>
</td></tr>
<tr><td><code id="dist.Normal.Mixture_+3A_lower.tail">lower.tail</code></td>
<td>
<p>Logical. This defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="dist.Normal.Mixture_+3A_log">log</code>, <code id="dist.Normal.Mixture_+3A_log.p">log.p</code></td>
<td>
<p>Logical. If <code>TRUE</code>, then probabilities
<code class="reqn">p</code> are given as <code class="reqn">\log(p)</code>.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> Application: Continuous Univariate
</p>
</li>
<li><p> Density: <code class="reqn">p(\theta) = \sum p_i \mathcal{N}(\mu_i,
    \sigma^2_i)</code>
</p>
</li>
<li><p> Inventor: Unknown
</p>
</li>
<li><p> Notation 1: <code class="reqn">\theta \sim \mathcal{N}(\mu, \sigma^2)</code>
</p>
</li>
<li><p> Notation 2: <code class="reqn">p(\theta) = \mathcal{N}(\theta | \mu,
    \sigma^2)</code>
</p>
</li>
<li><p> Parameter 1: mean parameters <code class="reqn">\mu</code>
</p>
</li>
<li><p> Parameter 2: standard deviation parameters <code class="reqn">\sigma &gt; 0</code>
</p>
</li>
<li><p> Mean: <code class="reqn">E(\theta) = \sum p_i \mu_i</code>
</p>
</li>
<li><p> Variance: <code class="reqn">var(\theta) = \sum p_i \sigma^{0.5}_i</code>
</p>
</li>
<li><p> Mode:
</p>
</li></ul>

<p>A mixture distribution is a probability distribution that is a
combination of other probability distributions, and each distribution is
called a mixture component, or component. A probability (or weight)
exists for each component, and these probabilities sum to one. A
mixture distribution (though not these functions here in particular) may
contain mixture components in which each component is a different
probability distribution. Mixture distributions are very flexible, and
are often used to represent a complex distribution with an unknown
form. When the number of mixture components is unknown, Bayesian
inference is the only sensible approach to estimation.
</p>
<p>A normal mixture, or Gaussian mixture, distribution is a combination of
normal probability distributions.
</p>


<h3>Value</h3>

<p><code>dnormm</code> gives the density,
<code>pnormm</code> returns the CDF, and
<code>rnormm</code> generates random deviates.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+ddirichlet">ddirichlet</a></code> and
<code><a href="stats.html#topic+dnorm">dnorm</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
p &lt;- c(0.3,0.3,0.4)
mu &lt;- c(-5, 1, 5)
sigma &lt;- c(1,2,1)
x &lt;- seq(from=-10, to=10, by=0.1)
plot(x, dnormm(x, p, mu, sigma, log=FALSE), type="l") #Density
plot(x, pnormm(x, p, mu, sigma), type="l") #CDF
plot(density(rnormm(10000, p, mu, sigma))) #Random Deviates
</code></pre>

<hr>
<h2 id='dist.Normal.Precision'>Normal Distribution: Precision Parameterization</h2><span id='topic+dnormp'></span><span id='topic+pnormp'></span><span id='topic+qnormp'></span><span id='topic+rnormp'></span>

<h3>Description</h3>

<p>These functions provide the density, distribution function, quantile
function, and random generation for the univariate normal distribution
with mean <code class="reqn">\mu</code> and precision <code class="reqn">\tau</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dnormp(x, mean=0, prec=1, log=FALSE)
pnormp(q, mean=0, prec=1, lower.tail=TRUE, log.p=FALSE)
qnormp(p, mean=0, prec=1, lower.tail=TRUE, log.p=FALSE)
rnormp(n, mean=0, prec=1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.Normal.Precision_+3A_x">x</code>, <code id="dist.Normal.Precision_+3A_q">q</code></td>
<td>
<p>These are each a vector of quantiles.</p>
</td></tr>
<tr><td><code id="dist.Normal.Precision_+3A_p">p</code></td>
<td>
<p>This is a vector of probabilities.</p>
</td></tr>
<tr><td><code id="dist.Normal.Precision_+3A_n">n</code></td>
<td>
<p>This is the number of observations, which must be a positive
integer that has length 1.</p>
</td></tr>
<tr><td><code id="dist.Normal.Precision_+3A_mean">mean</code></td>
<td>
<p>This is the mean parameter <code class="reqn">\mu</code>.</p>
</td></tr>
<tr><td><code id="dist.Normal.Precision_+3A_prec">prec</code></td>
<td>
<p>This is the precision parameter <code class="reqn">\tau</code>, which
must be positive.</p>
</td></tr>
<tr><td><code id="dist.Normal.Precision_+3A_log">log</code>, <code id="dist.Normal.Precision_+3A_log.p">log.p</code></td>
<td>
<p>Logical. If <code>TRUE</code>, then probabilities
<code class="reqn">p</code> are given as <code class="reqn">\log(p)</code>.</p>
</td></tr>
<tr><td><code id="dist.Normal.Precision_+3A_lower.tail">lower.tail</code></td>
<td>
<p>Logical. If <code>TRUE</code> (default), then probabilities
are <code class="reqn">Pr[X \le x]</code>, otherwise,
<code class="reqn">Pr[X &gt; x]</code>.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> Application: Continuous Univariate
</p>
</li>
<li><p> Density: <code class="reqn">p(\theta) = \sqrt{\frac{\tau}{2\pi}}
    \exp(-\frac{\tau}{2} (\theta-\mu)^2)</code>
</p>
</li>
<li><p> Inventor: Carl Friedrich Gauss or Abraham De Moivre
</p>
</li>
<li><p> Notation 1: <code class="reqn">\theta \sim \mathcal{N}(\mu, \tau^{-1})</code>
</p>
</li>
<li><p> Notation 2: <code class="reqn">p(\theta) = \mathcal{N}(\theta | \mu,
    \tau^{-1})</code>
</p>
</li>
<li><p> Parameter 1: mean parameter <code class="reqn">\mu</code>
</p>
</li>
<li><p> Parameter 2: precision parameter <code class="reqn">\tau &gt; 0</code>
</p>
</li>
<li><p> Mean: <code class="reqn">E(\theta) = \mu</code>
</p>
</li>
<li><p> Variance: <code class="reqn">var(\theta) = \tau^{-1}</code>
</p>
</li>
<li><p> Mode: <code class="reqn">mode(\theta) = \mu</code>
</p>
</li></ul>

<p>The normal distribution, also called the Gaussian distribution and the
Second Law of Laplace, is usually parameterized with mean and variance,
or in Bayesian inference, with mean and precision, where precision is
the inverse of the variance. In contrast, <code>Base R</code> parameterizes
the normal distribution with the mean and standard deviation. These
functions provide the precision parameterization for convenience and
familiarity.
</p>
<p>Some authors attribute credit for the normal distribution to Abraham
de Moivre in 1738. In 1809, Carl Friedrich Gauss published his
monograph &ldquo;Theoria motus corporum coelestium in sectionibus conicis
solem ambientium&rdquo;, in which he introduced the method of least squares,
method of maximum likelihood, and normal distribution, among many other
innovations.
</p>
<p>Gauss, himself, characterized this distribution according to mean and
precision, though his definition of precision differed from the modern
one. The modern Bayesian use of precision <code class="reqn">\tau</code> developed
because it was more straightforward to estimate <code class="reqn">\tau</code> with a
gamma distribution as a conjugate prior, than to estimate
<code class="reqn">\sigma^2</code> with an inverse-gamma distribution as a
conjugate prior.
</p>
<p>Although the normal distribution is very common, it often does not fit
data as well as more robust alternatives with fatter tails, such as the
Laplace or Student t distribution.
</p>
<p>A flat distribution is obtained in the limit as
<code class="reqn">\tau \rightarrow 0</code>.
</p>
<p>For models where the dependent variable, y, is specified to be
normally distributed given the model, the Jarque-Bera test (see
<code><a href="#topic+plot.demonoid.ppc">plot.demonoid.ppc</a></code> or <code><a href="#topic+plot.laplace.ppc">plot.laplace.ppc</a></code>) may
be used to test the residuals.
</p>
<p>These functions are similar to those in <code>base R</code>.
</p>


<h3>Value</h3>

<p><code>dnormp</code> gives the density,
<code>pnormp</code> gives the distribution function,
<code>qnormp</code> gives the quantile function, and
<code>rnormp</code> generates random deviates.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+dlaplace">dlaplace</a></code>,
<code><a href="stats.html#topic+dnorm">dnorm</a></code>,
<code><a href="#topic+dnormv">dnormv</a></code>,
<code><a href="#topic+prec2var">prec2var</a></code>,
<code><a href="#topic+dst">dst</a></code>,
<code><a href="stats.html#topic+dt">dt</a></code>,
<code><a href="#topic+plot.demonoid.ppc">plot.demonoid.ppc</a></code>, and
<code><a href="#topic+plot.laplace.ppc">plot.laplace.ppc</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
x &lt;- dnormp(1,0,1)
x &lt;- pnormp(1,0,1)
x &lt;- qnormp(0.5,0,1)
x &lt;- rnormp(100,0,1)

#Plot Probability Functions
x &lt;- seq(from=-5, to=5, by=0.1)
plot(x, dnormp(x,0,0.5), ylim=c(0,1), type="l", main="Probability Function",
     ylab="density", col="red")
lines(x, dnormp(x,0,1), type="l", col="green")
lines(x, dnormp(x,0,5), type="l", col="blue")
legend(2, 0.9, expression(paste(mu==0, ", ", tau==0.5),
     paste(mu==0, ", ", tau==1), paste(mu==0, ", ", tau==5)),
     lty=c(1,1,1), col=c("red","green","blue"))
</code></pre>

<hr>
<h2 id='dist.Normal.Variance'>Normal Distribution: Variance Parameterization</h2><span id='topic+dnormv'></span><span id='topic+pnormv'></span><span id='topic+qnormv'></span><span id='topic+rnormv'></span>

<h3>Description</h3>

<p>These functions provide the density, distribution function, quantile
function, and random generation for the univariate normal distribution
with mean <code class="reqn">\mu</code> and variance <code class="reqn">\sigma^2</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dnormv(x, mean=0, var=1, log=FALSE)
pnormv(q, mean=0, var=1, lower.tail=TRUE, log.p=FALSE)
qnormv(p, mean=0, var=1, lower.tail=TRUE, log.p=FALSE)
rnormv(n, mean=0, var=1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.Normal.Variance_+3A_x">x</code>, <code id="dist.Normal.Variance_+3A_q">q</code></td>
<td>
<p>These are each a vector of quantiles.</p>
</td></tr>
<tr><td><code id="dist.Normal.Variance_+3A_p">p</code></td>
<td>
<p>This is a vector of probabilities.</p>
</td></tr>
<tr><td><code id="dist.Normal.Variance_+3A_n">n</code></td>
<td>
<p>This is the number of observations, which must be a positive
integer that has length 1.</p>
</td></tr>
<tr><td><code id="dist.Normal.Variance_+3A_mean">mean</code></td>
<td>
<p>This is the mean parameter <code class="reqn">\mu</code>.</p>
</td></tr>
<tr><td><code id="dist.Normal.Variance_+3A_var">var</code></td>
<td>
<p>This is the variance parameter <code class="reqn">\sigma^2</code>, which
must be positive.</p>
</td></tr>
<tr><td><code id="dist.Normal.Variance_+3A_log">log</code>, <code id="dist.Normal.Variance_+3A_log.p">log.p</code></td>
<td>
<p>Logical. If <code>TRUE</code>, then probabilities <code class="reqn">p</code>
are given as <code class="reqn">\log(p)</code>.</p>
</td></tr>
<tr><td><code id="dist.Normal.Variance_+3A_lower.tail">lower.tail</code></td>
<td>
<p>Logical. If <code>TRUE</code> (default), then probabilities
are <code class="reqn">Pr[X \le x]</code>, otherwise,
<code class="reqn">Pr[X &gt; x]</code>.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> Application: Continuous Univariate
</p>
</li>
<li><p> Density: <code class="reqn">p(\theta) = \frac{1}{\sqrt{2\pi\sigma^2}}
    \exp(-\frac{(\theta-\mu)^2}{2\sigma^2})</code>
</p>
</li>
<li><p> Inventor: Carl Friedrich Gauss or Abraham De Moivre
</p>
</li>
<li><p> Notation 1: <code class="reqn">\theta \sim \mathcal{N}(\mu, \sigma^2)</code>
</p>
</li>
<li><p> Notation 2: <code class="reqn">p(\theta) = \mathcal{N}(\theta | \mu,
    \sigma^2)</code>
</p>
</li>
<li><p> Parameter 1: mean parameter <code class="reqn">\mu</code>
</p>
</li>
<li><p> Parameter 2: variance parameter <code class="reqn">\sigma^2 &gt; 0</code>
</p>
</li>
<li><p> Mean: <code class="reqn">E(\theta) = \mu</code>
</p>
</li>
<li><p> Variance: <code class="reqn">var(\theta) = \sigma^2</code>
</p>
</li>
<li><p> Mode: <code class="reqn">mode(\theta) = \mu</code>
</p>
</li></ul>

<p>The normal distribution, also called the Gaussian distribution and the
Second Law of Laplace, is usually parameterized with mean and variance.
<code>Base R</code> uses the mean and standard deviation. These functions
provide the variance parameterization for convenience and familiarity.
For example, it is easier to code <code>dnormv(1,1,1000)</code> than
<code>dnorm(1,1,sqrt(1000))</code>.
</p>
<p>Some authors attribute credit for the normal distribution to Abraham
de Moivre in 1738. In 1809, Carl Friedrich Gauss published his
monograph &ldquo;Theoria motus corporum coelestium in sectionibus conicis
solem ambientium&rdquo;, in which he introduced the method of least squares,
method of maximum likelihood, and normal distribution, among many other
innovations.
</p>
<p>Gauss, himself, characterized this distribution according to mean and
precision, though his definition of precision differed from the modern
one.
</p>
<p>Although the normal distribution is very common, it often does not fit
data as well as more robust alternatives with fatter tails, such as the
Laplace or Student t distribution.
</p>
<p>A flat distribution is obtained in the limit as
<code class="reqn">\sigma^2 \rightarrow \infty</code>.
</p>
<p>For models where the dependent variable, y, is specified to be
normally distributed given the model, the Jarque-Bera test (see
<code><a href="#topic+plot.demonoid.ppc">plot.demonoid.ppc</a></code> or <code><a href="#topic+plot.laplace.ppc">plot.laplace.ppc</a></code>) may
be used to test the residuals.
</p>
<p>These functions are similar to those in <code>base R</code>.
</p>


<h3>Value</h3>

<p><code>dnormv</code> gives the density,
<code>pnormv</code> gives the distribution function,
<code>qnormv</code> gives the quantile function, and
<code>rnormv</code> generates random deviates.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+dlaplace">dlaplace</a></code>,
<code><a href="stats.html#topic+dnorm">dnorm</a></code>,
<code><a href="#topic+dnormp">dnormp</a></code>,
<code><a href="#topic+dst">dst</a></code>,
<code><a href="stats.html#topic+dt">dt</a></code>,
<code><a href="#topic+plot.demonoid.ppc">plot.demonoid.ppc</a></code>, and
<code><a href="#topic+plot.laplace.ppc">plot.laplace.ppc</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
x &lt;- dnormv(1,0,1)
x &lt;- pnormv(1,0,1)
x &lt;- qnormv(0.5,0,1)
x &lt;- rnormv(100,0,1)

#Plot Probability Functions
x &lt;- seq(from=-5, to=5, by=0.1)
plot(x, dnormv(x,0,0.5), ylim=c(0,1), type="l", main="Probability Function",
     ylab="density", col="red")
lines(x, dnormv(x,0,1), type="l", col="green")
lines(x, dnormv(x,0,5), type="l", col="blue")
legend(2, 0.9, expression(paste(mu==0, ", ", sigma^2==0.5),
     paste(mu==0, ", ", sigma^2==1), paste(mu==0, ", ", sigma^2==5)),
     lty=c(1,1,1), col=c("red","green","blue"))
</code></pre>

<hr>
<h2 id='dist.Normal.Wishart'>Normal-Wishart Distribution</h2><span id='topic+dnormwishart'></span><span id='topic+rnormwishart'></span>

<h3>Description</h3>

<p>These functions provide the density and random number generation for
the normal-Wishart distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dnormwishart(mu, mu0, lambda, Omega, S, nu, log=FALSE) 
rnormwishart(n=1, mu0, lambda, S, nu)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.Normal.Wishart_+3A_mu">mu</code></td>
<td>
<p>This is data or parameters in the form of a vector of length
<code class="reqn">k</code> or a matrix with <code class="reqn">k</code> columns.</p>
</td></tr>
<tr><td><code id="dist.Normal.Wishart_+3A_mu0">mu0</code></td>
<td>
<p>This is mean vector <code class="reqn">\mu_0</code> with length <code class="reqn">k</code> or
matrix with <code class="reqn">k</code> columns.</p>
</td></tr>
<tr><td><code id="dist.Normal.Wishart_+3A_lambda">lambda</code></td>
<td>
<p>This is a positive-only scalar.</p>
</td></tr>
<tr><td><code id="dist.Normal.Wishart_+3A_n">n</code></td>
<td>
<p>This is the number of random draws.</p>
</td></tr>
<tr><td><code id="dist.Normal.Wishart_+3A_nu">nu</code></td>
<td>
<p>This is the scalar degrees of freedom <code class="reqn">\nu</code>.</p>
</td></tr>
<tr><td><code id="dist.Normal.Wishart_+3A_omega">Omega</code></td>
<td>
<p>This is a <code class="reqn">k \times k</code> precision matrix
<code class="reqn">\Omega</code>.</p>
</td></tr>
<tr><td><code id="dist.Normal.Wishart_+3A_s">S</code></td>
<td>
<p>This is the symmetric, positive-semidefinite, <code class="reqn">k \times
      k</code> scale matrix <code class="reqn">\textbf{S}</code>.</p>
</td></tr>
<tr><td><code id="dist.Normal.Wishart_+3A_log">log</code></td>
<td>
<p>Logical. If <code>log=TRUE</code>, then the logarithm of the
density is returned.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> Application: Continuous Multivariate
</p>
</li>
<li><p> Density: <code class="reqn">p(\mu, \Omega) = \mathcal{N}(\mu | \mu_0,
      (\lambda\Omega)^{-1}) \mathcal{W}(\Omega | \nu, \textbf{S})</code>
</p>
</li>
<li><p> Inventors: Unknown
</p>
</li>
<li><p> Notation 1: <code class="reqn">(\mu, \Omega) \sim \mathcal{NW}(\mu_0, \lambda,
    \textbf{S}, \nu)</code>
</p>
</li>
<li><p> Notation 2: <code class="reqn">p(\mu, \Omega) = \mathcal{NW}(\mu, \Omega |
    \mu_0, \lambda, \textbf{S}, \nu)</code>
</p>
</li>
<li><p> Parameter 1: location vector <code class="reqn">\mu_0</code>
</p>
</li>
<li><p> Parameter 2: <code class="reqn">\lambda &gt; 0</code>
</p>
</li>
<li><p> Parameter 3: symmetric, positive-semidefinite
<code class="reqn">k \times k</code> scale matrix <code class="reqn">\textbf{S}</code>
</p>
</li>
<li><p> Parameter 4: degrees of freedom <code class="reqn">\nu \ge k</code>
</p>
</li>
<li><p> Mean: Unknown
</p>
</li>
<li><p> Variance: Unknown
</p>
</li>
<li><p> Mode: Unknown
</p>
</li></ul>

<p>The normal-Wishart distribution, or Gaussian-Wishart distribution, is a
multivariate four-parameter continuous probability distribution. It is
the conjugate prior of a multivariate normal distribution with unknown
mean and precision matrix.
</p>


<h3>Value</h3>

<p><code>dnormwishart</code> gives the density and 
<code>rnormwishart</code> generates random deviates and returns a list with
two components.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+dmvnp">dmvnp</a></code> and
<code><a href="#topic+dwishart">dwishart</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
K &lt;- 3
mu &lt;- rnorm(K)
mu0 &lt;- rnorm(K)
nu &lt;- K + 1
S &lt;- diag(K)
lambda &lt;- runif(1) #Real scalar
Omega &lt;- as.positive.definite(matrix(rnorm(K^2),K,K))
x &lt;- dnormwishart(mu, mu0, lambda, Omega, S, nu, log=TRUE)
out &lt;- rnormwishart(n=10, mu0, lambda, S, nu)
joint.density.plot(out$mu[,1], out$mu[,2], color=TRUE)
</code></pre>

<hr>
<h2 id='dist.Pareto'>Pareto Distribution</h2><span id='topic+dpareto'></span><span id='topic+ppareto'></span><span id='topic+qpareto'></span><span id='topic+rpareto'></span>

<h3>Description</h3>

<p>These functions provide the density, distribution function, quantile
function, and random generation for the pareto distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dpareto(x, alpha, log=FALSE)
ppareto(q, alpha)
qpareto(p, alpha)
rpareto(n, alpha)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.Pareto_+3A_x">x</code>, <code id="dist.Pareto_+3A_q">q</code></td>
<td>
<p>These are each a vector of quantiles.</p>
</td></tr>
<tr><td><code id="dist.Pareto_+3A_p">p</code></td>
<td>
<p>This is a vector of probabilities.</p>
</td></tr>
<tr><td><code id="dist.Pareto_+3A_n">n</code></td>
<td>
<p>This is the number of observations, which must be a positive
integer that has length 1.</p>
</td></tr>
<tr><td><code id="dist.Pareto_+3A_alpha">alpha</code></td>
<td>
<p>This is the shape parameter <code class="reqn">\alpha</code>, which
must be positive.</p>
</td></tr>
<tr><td><code id="dist.Pareto_+3A_log">log</code></td>
<td>
<p>Logical. If <code>log=TRUE</code>, then the logarithm of the
density or result is returned.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> Application: Continuous Univariate
</p>
</li>
<li><p> Density: <code class="reqn">p(\theta) =
    \frac{\alpha}{\theta^{\alpha+1}}, \theta \ge 1</code>
</p>
</li>
<li><p> Inventor: Vilfredo Pareto (1848-1923)
</p>
</li>
<li><p> Notation 1: <code class="reqn">\theta \sim \mathcal{PA}(\alpha)</code>
</p>
</li>
<li><p> Notation 2: <code class="reqn">p(\theta) = \mathcal{PA}(\theta |
    \alpha)</code>
</p>
</li>
<li><p> Parameter 1: shape parameter <code class="reqn">\alpha &gt; 0</code>
</p>
</li>
<li><p> Mean: <code class="reqn">E(\theta) = \frac{\alpha}{\alpha - 1}</code>
</p>
</li>
<li><p> Variance: <code class="reqn">var(\theta) =
    \frac{\alpha}{(\alpha-1)^2(\alpha-2)}, \alpha &gt; 2</code>
</p>
</li>
<li><p> Mode: <code class="reqn">mode(\theta) = 1</code>
</p>
</li></ul>

<p>The Pareto distribution, sometimes called the Bradford distribution, is
related to the exponential distribution. The gamma distribution is the
conjugate prior distribution for the shape parameter <code class="reqn">\alpha</code>
in the Pareto distribution. The Pareto distribution is the conjugate
prior distribution for the range parameters of a uniform
distribution. An extension, elsewhere, is the symmetric Pareto
distribution.
</p>


<h3>Value</h3>

<p><code>dpareto</code> gives the density,
<code>ppareto</code> gives the distribution function,
<code>qpareto</code> gives the quantile function, and
<code>rpareto</code> generates random deviates.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+dexp">dexp</a></code>,
<code><a href="stats.html#topic+dlnorm">dlnorm</a></code>,
<code><a href="#topic+dlnormp">dlnormp</a></code>,
<code><a href="stats.html#topic+dnorm">dnorm</a></code>,
<code><a href="#topic+dnormp">dnormp</a></code>,
<code><a href="#topic+dnormv">dnormv</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
x &lt;- dpareto(1,1)
x &lt;- ppareto(0.5,1)
x &lt;- qpareto(0.5,1)
x &lt;- rpareto(10,1)

#Plot Probability Functions
x &lt;- seq(from=1, to=5, by=0.01)
plot(x, dpareto(x,0.1), ylim=c(0,1), type="l", main="Probability Function",
     ylab="density", col="red")
lines(x, dpareto(x,0.5), type="l", col="green")
lines(x, dpareto(x,1), type="l", col="blue")
legend(2, 0.9, expression(alpha==0.1, alpha==0.5, alpha==1),
     lty=c(1,1,1), col=c("red","green","blue"))
</code></pre>

<hr>
<h2 id='dist.Power.Exponential'>Power Exponential Distribution: Univariate Symmetric</h2><span id='topic+dpe'></span><span id='topic+ppe'></span><span id='topic+qpe'></span><span id='topic+rpe'></span>

<h3>Description</h3>

<p>These functions provide the density, distribution function, quantile
function, and random generation for the univariate, symmetric, power
exponential distribution with location parameter <code class="reqn">\mu</code>, scale
parameter <code class="reqn">\sigma</code>, and kurtosis parameter
<code class="reqn">\kappa</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dpe(x, mu=0, sigma=1, kappa=2, log=FALSE)
ppe(q, mu=0, sigma=1, kappa=2, lower.tail=TRUE, log.p=FALSE)
qpe(p, mu=0, sigma=1, kappa=2, lower.tail=TRUE, log.p=FALSE)
rpe(n, mu=0, sigma=1, kappa=2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.Power.Exponential_+3A_x">x</code>, <code id="dist.Power.Exponential_+3A_q">q</code></td>
<td>
<p>These are each a vector of quantiles.</p>
</td></tr>
<tr><td><code id="dist.Power.Exponential_+3A_p">p</code></td>
<td>
<p>This is a vector of probabilities.</p>
</td></tr>
<tr><td><code id="dist.Power.Exponential_+3A_n">n</code></td>
<td>
<p>This is the number of observations, which must be a positive
integer that has length 1.</p>
</td></tr>
<tr><td><code id="dist.Power.Exponential_+3A_mu">mu</code></td>
<td>
<p>This is the location parameter <code class="reqn">\mu</code>.</p>
</td></tr>
<tr><td><code id="dist.Power.Exponential_+3A_sigma">sigma</code></td>
<td>
<p>This is the scale parameter <code class="reqn">\sigma</code>, which
must be positive.</p>
</td></tr>
<tr><td><code id="dist.Power.Exponential_+3A_kappa">kappa</code></td>
<td>
<p>This is the kurtosis parameter <code class="reqn">\kappa</code>, which
must be positive.</p>
</td></tr>
<tr><td><code id="dist.Power.Exponential_+3A_log">log</code>, <code id="dist.Power.Exponential_+3A_log.p">log.p</code></td>
<td>
<p>Logical. If <code>log=TRUE</code>, then the logarithm of the
density or result is returned.</p>
</td></tr>
<tr><td><code id="dist.Power.Exponential_+3A_lower.tail">lower.tail</code></td>
<td>
<p>Logical. If <code>lower.tail=TRUE</code> (default),
probabilities are <code class="reqn">Pr[X \le x]</code>, otherwise,
<code class="reqn">Pr[X &gt; x]</code>.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> Application: Continuous Univariate
</p>
</li>
<li><p> Density: <code class="reqn">p(\theta) = \frac{1}{2 \kappa^{1/\kappa}
    \Gamma(1+\frac{1}{\kappa}) \sigma}
  \exp(-\frac{|\theta-\mu|^{\kappa}}{\kappa \sigma^\kappa})</code>
</p>
</li>
<li><p> Inventor: Subbotin, M.T. (1923)
</p>
</li>
<li><p> Notation 1: <code class="reqn">\theta \sim \mathcal{PE}(\mu, \sigma, \kappa)</code>
</p>
</li>
<li><p> Notation 2: <code class="reqn">p(\theta) = \mathcal{PE}(\theta | \mu,
    \sigma, \kappa)</code>
</p>
</li>
<li><p> Parameter 1: location parameter <code class="reqn">\mu</code>
</p>
</li>
<li><p> Parameter 2: scale parameter <code class="reqn">\sigma &gt; 0</code>
</p>
</li>
<li><p> Parameter 3: kurtosis parameter <code class="reqn">\kappa &gt; 0</code>
</p>
</li>
<li><p> Mean: <code class="reqn">E(\theta) = \mu</code>
</p>
</li>
<li><p> Variance: <code class="reqn">var(\theta) = </code>
</p>
</li>
<li><p> Mode: <code class="reqn">mode(\theta) = \mu</code>
</p>
</li></ul>

<p>The power exponential distribution is also called the exponential power
distribution, generalized error distribution, generalized Gaussian
distribution, and generalized normal distribution. The original form was
introduced by Subbotin (1923) and re-parameterized by Lunetta
(1963). These functions use the more recent parameterization by Lunetta
(1963). A shape parameter, <code class="reqn">\kappa &gt; 0</code>, is added to the
normal distribution. When <code class="reqn">\kappa=1</code>, the power exponential
distribution is the same as the Laplace distribution. When
<code class="reqn">\kappa=2</code>, the power exponential distribution is the same
as the normal distribution. As <code class="reqn">\kappa \rightarrow \infty</code>, this becomes a uniform distribution <code class="reqn">\in (\mu-\sigma,
  \mu+\sigma)</code>. Tails that are heavier than
normal occur when <code class="reqn">\kappa &lt; 2</code>, or lighter than normal
when <code class="reqn">\kappa &gt; 2</code>. This distribution is univariate and
symmetric, and there exist multivariate and asymmetric versions.
</p>
<p>These functions are similar to those in the <code>normalp</code> package.
</p>


<h3>Value</h3>

<p><code>dpe</code> gives the density,
<code>ppe</code> gives the distribution function,
<code>qpe</code> gives the quantile function, and
<code>rpe</code> generates random deviates.
</p>


<h3>References</h3>

<p>Lunetta, G. (1963). &quot;Di una Generalizzazione dello Schema della Curva
Normale&quot;. <em>Annali della Facolt'a di Economia e Commercio di
Palermo</em>, 17, p. 237&ndash;244.
</p>
<p>Subbotin, M.T. (1923). &quot;On the Law of Frequency of Errors&quot;.
<em>Matematicheskii Sbornik</em>, 31, p. 296&ndash;301.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dlaplace">dlaplace</a></code>,
<code><a href="#topic+dlaplacep">dlaplacep</a></code>,
<code><a href="#topic+dmvpe">dmvpe</a></code>,
<code><a href="stats.html#topic+dnorm">dnorm</a></code>,
<code><a href="#topic+dnormp">dnormp</a></code>,
<code><a href="#topic+dnormv">dnormv</a></code>, and
<code><a href="stats.html#topic+dunif">dunif</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
x &lt;- dpe(1,0,1,2)
x &lt;- ppe(1,0,1,2)
x &lt;- qpe(0.5,0,1,2)
x &lt;- rpe(100,0,1,2)

#Plot Probability Functions
x &lt;- seq(from=0.1, to=3, by=0.01)
plot(x, dpe(x,0,1,0.1), ylim=c(0,1), type="l", main="Probability Function",
     ylab="density", col="red")
lines(x, dpe(x,0,1,2), type="l", col="green")
lines(x, dpe(x,0,1,5), type="l", col="blue")
legend(1.5, 0.9, expression(paste(mu==0, ", ", sigma==1, ", ", kappa==0.1),
     paste(mu==0, ", ", sigma==1, ", ", kappa==2),
     paste(mu==0, ", ", sigma==1, ", ", kappa==5)),
     lty=c(1,1,1), col=c("red","green","blue"))
</code></pre>

<hr>
<h2 id='dist.Scaled.Inverse.Wishart'>Scaled Inverse Wishart Distribution</h2><span id='topic+dsiw'></span><span id='topic+rsiw'></span>

<h3>Description</h3>

<p>These functions provide the density and random number generation
for the scaled inverse Wishart distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>   dsiw(Q, nu, S, zeta, mu, delta, log=FALSE)
   rsiw(nu, S, mu, delta)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.Scaled.Inverse.Wishart_+3A_q">Q</code></td>
<td>
<p>This is the symmetric, positive-definite
<code class="reqn">k \times k</code> matrix <code class="reqn">\textbf{Q}</code>.</p>
</td></tr>
<tr><td><code id="dist.Scaled.Inverse.Wishart_+3A_nu">nu</code></td>
<td>
<p>This is the scalar degrees of freedom, <code class="reqn">\nu</code>
regarding <code class="reqn">\textbf{Q}</code>. The default recommendation is
<code>nu=k+1</code>.</p>
</td></tr>
<tr><td><code id="dist.Scaled.Inverse.Wishart_+3A_s">S</code></td>
<td>
<p>This is the symmetric, positive-semidefinite
<code class="reqn">k \times k</code> scale matrix <code class="reqn">\textbf{S}</code> regarding
<code class="reqn">\textbf{Q}</code>. The default recommendation is <code>S=diag(k)</code>.</p>
</td></tr>
<tr><td><code id="dist.Scaled.Inverse.Wishart_+3A_zeta">zeta</code></td>
<td>
<p>This is a positive-only vector of length <code class="reqn">k</code> of
auxiliary scale parameters <code class="reqn">\zeta</code>.</p>
</td></tr>
<tr><td><code id="dist.Scaled.Inverse.Wishart_+3A_mu">mu</code></td>
<td>
<p>This is a vector of length <code class="reqn">k</code> of location
hyperparameters <code class="reqn">\mu</code> regarding <code class="reqn">\zeta</code>.</p>
</td></tr>
<tr><td><code id="dist.Scaled.Inverse.Wishart_+3A_delta">delta</code></td>
<td>
<p>This is a positive-only vector of length <code class="reqn">k</code> of
scale hyperparameters <code class="reqn">\delta</code> regarding
<code class="reqn">\zeta</code>.</p>
</td></tr>
<tr><td><code id="dist.Scaled.Inverse.Wishart_+3A_log">log</code></td>
<td>
<p>Logical. If <code>log=TRUE</code>, then the logarithm of the
density is returned.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> Application: Continuous Multivariate
</p>
</li>
<li><p> Density: (see below)
</p>
</li>
<li><p> Inventor: O'Malley and Zaslavsky (2005)
</p>
</li>
<li><p> Notation 1: <code class="reqn">p(\Sigma) \sim \mathcal{SIW}(\textbf{Q},
      \nu, \textbf{S}, \zeta, \mu, \delta)</code>
</p>
</li>
<li><p> Notation 2: <code class="reqn">p(\Sigma) = \mathcal{SIW}(\Sigma |
      \textbf{Q}, \nu, \textbf{S}, \zeta, \mu, \delta</code>
</p>
</li>
<li><p> Parameter 1: symmetric, positive-definite
<code class="reqn">k \times k</code> matrix <code class="reqn">\textbf{Q}</code>
</p>
</li>
<li><p> Parameter 2: degrees of freedom <code class="reqn">\nu</code>
</p>
</li>
<li><p> Parameter 3: symmetric, positive-semidefinite
<code class="reqn">k \times k</code> scale matrix <code class="reqn">\textbf{S}</code>
</p>
</li>
<li><p> Parameter 4: Auxiliary scale parameter vector
<code class="reqn">\zeta</code>
</p>
</li>
<li><p> Parameter 5: Hyperparameter location vector
<code class="reqn">\mu</code>
</p>
</li>
<li><p> Parameter 6: Hyperparameter scale vector
<code class="reqn">\delta</code>
</p>
</li>
<li><p> Mean: 
</p>
</li>
<li><p> Variance: 
</p>
</li>
<li><p> Mode: 
</p>
</li></ul>

<p>The scaled inverse Wishart (SIW) distribution is a prior probability
distribution for a covariance matrix, and is an alternative to the
inverse Wishart distribution.
</p>
<p>While the inverse Wishart distribution is applied directly to covariance
matrix <code class="reqn">\Sigma</code>, the SIW distribution is applied to a
decomposed matrix <code class="reqn">\textbf{Q}</code> and diagonal scale matrix
<code class="reqn">\zeta</code>. For information on how to apply it to
<code class="reqn">\textbf{Q}</code>, see the example below.
</p>
<p>SIW is more flexible than the inverse Wishart distribution because it
has additional, and some say somewhat redundant, scale parameters. This
makes up for one limitation of the inverse Wishart, namely that all
uncertainty about posterior variances is represented in one parameter.
The SIW prior may somewhat alleviate the dependency in the inverse
Wishart between variances and correlations, though the SIW prior still
retains some of this relationship.
</p>
<p>The Huang-Wand (<code><a href="#topic+dhuangwand">dhuangwand</a></code>) prior is a hierarchical
alternative.
</p>


<h3>Value</h3>

<p><code>dsiw</code> gives the density and
<code>rsiw</code> generates random deviates.
</p>


<h3>References</h3>

<p>O'Malley, A.J. and Zaslavsky, A.M. (2005), &quot;Domain-Level Covariance
Analysis for Survey Data with Structured Nonresponse&quot;.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dhuangwand">dhuangwand</a></code>,
<code><a href="#topic+dinvwishartc">dinvwishartc</a></code>,
<code><a href="#topic+dmvn">dmvn</a></code>, and
<code><a href="#topic+dwishart">dwishart</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
### In the model specification function, input U and zeta, then:
# Q &lt;- t(U) %*% U
# Zeta &lt;- diag(zeta)
# Sigma &lt;- Zeta %*% Q %*% Zeta
# Sigma.prior &lt;- dsiw(Q, nu=Data$K+1, S=diag(Data$K), zeta, mu=0, delta=1)
### Examples
x &lt;- dsiw(diag(3), 4, diag(3), runif(3), rep(0,3), rep(1,3), log=TRUE)
x &lt;- rsiw(4, diag(3), rep(0,3), rep(1,3))
</code></pre>

<hr>
<h2 id='dist.Skew.Discrete.Laplace'>Skew Discrete Laplace Distribution: Univariate</h2><span id='topic+dsdlaplace'></span><span id='topic+psdlaplace'></span><span id='topic+qsdlaplace'></span><span id='topic+rsdlaplace'></span>

<h3>Description</h3>

<p>These functions provide the density, distribution function, quantile
function, and random generation for the univariate, skew discrete
Laplace distribution with parameters <code class="reqn">p</code> and <code class="reqn">q</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dsdlaplace(x, p, q, log=FALSE)
psdlaplace(x, p, q)
qsdlaplace(prob, p, q)
rsdlaplace(n, p, q)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.Skew.Discrete.Laplace_+3A_x">x</code></td>
<td>
<p>This is a vector of data.</p>
</td></tr>
<tr><td><code id="dist.Skew.Discrete.Laplace_+3A_p">p</code></td>
<td>
<p>This is a scalar or vector of parameter <code class="reqn">p \in [0,1]</code>.</p>
</td></tr>
<tr><td><code id="dist.Skew.Discrete.Laplace_+3A_q">q</code></td>
<td>
<p>This is a scalar or vector of parameter <code class="reqn">q \in [0,1]</code>.</p>
</td></tr>
<tr><td><code id="dist.Skew.Discrete.Laplace_+3A_prob">prob</code></td>
<td>
<p>This is a probability scalar or vector.</p>
</td></tr>
<tr><td><code id="dist.Skew.Discrete.Laplace_+3A_n">n</code></td>
<td>
<p>This is the number of observations, which must be a positive
integer that has length 1.</p>
</td></tr>
<tr><td><code id="dist.Skew.Discrete.Laplace_+3A_log">log</code></td>
<td>
<p>Logical. If <code>log=TRUE</code>, then the logarithm of the
density is returned.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> Application: Discrete Univariate
</p>
</li>
<li><p> Density 1: <code class="reqn">p(\theta) = \frac{(1-p)(1-q)}{1-pq}p^\theta;
      \theta=0,1,2,3,\dots</code>
</p>
</li>
<li><p> Density 2: <code class="reqn">p(\theta) = \frac{(1-p)(1-q)}{1-pq}q^{|\theta|};
      x=0,-1,-2,-3,\dots</code>
</p>
</li>
<li><p> Inventor: Kozubowski, T.J. and Inusah, S. (2006)
</p>
</li>
<li><p> Notation 1: <code class="reqn">\theta \sim \mathcal{DL}(p, q)</code>
</p>
</li>
<li><p> Notation 2: <code class="reqn">p(\theta) = \mathcal{DL}(\theta | p,
      q)</code>
</p>
</li>
<li><p> Parameter 1: <code class="reqn">p \in [0,1]</code>
</p>
</li>
<li><p> Parameter 2: <code class="reqn">q \in [0,1]</code>
</p>
</li>
<li><p> Mean 1: <code class="reqn">E(\theta) =
      \frac{1}{1-p}-\frac{1}{1-q}=\frac{p}{1-p}-\frac{q}{1-q}</code>
</p>
</li>
<li><p> Mean 2: <code class="reqn">E(|\theta|) =
      \frac{q(1-p)^2+p(1-q)^2}{(1-qp)(1-q)(1-p)}</code>
</p>
</li>
<li><p> Variance: <code class="reqn">var(\theta) =
      \frac{1}{(1-p)^2(1-q)^2}[\frac{q(1-p)^3(1+q)+p(1-q)^3(1+p)}{1-pq}-(p-q)^2]</code>
</p>
</li>
<li><p> Mode: 
</p>
</li></ul>

<p>This is a discrete form of the skew-Laplace distribution. The symmetric
discrete Laplace distribution occurs when <code class="reqn">p=q</code>. DL(p,0) is a
geometric distribution, and DL(0,q) is a geometric distribution of
non-positive integers. The distribution is degenerate when
DL(0,0). Since the geometric distribution is a discrete analog of the
exponential distribution, the distribution of the difference of two
geometric variables is a discrete Laplace distribution.
</p>
<p>These functions are similar to those in the <code>DiscreteLaplace</code> package.
</p>


<h3>Value</h3>

<p><code>dslaplace</code> gives the density,
<code>pslaplace</code> gives the distribution function,
<code>qslaplace</code> gives the quantile function, and
<code>rslaplace</code> generates random deviates.
</p>


<h3>References</h3>

<p>Kozubowski, T.J. and Inusah, S. (2006). &quot;A Skew Laplace Distribution
on Integers&quot;. <em>AISM</em>, 58, p. 555&ndash;571.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dalaplace">dalaplace</a></code>,
<code><a href="stats.html#topic+dexp">dexp</a></code>,
<code><a href="#topic+dlaplace">dlaplace</a></code>,
<code><a href="#topic+dlaplacep">dlaplacep</a></code>, and
<code><a href="#topic+dslaplace">dslaplace</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
x &lt;- dsdlaplace(1,0.5,0.5)
x &lt;- psdlaplace(1,0.5,0.5)
x &lt;- qsdlaplace(0.5,0.5,0.5)
x &lt;- rsdlaplace(5,0.5,0.5)

#Plot Probability Functions
     x &lt;- c(-3:3)
     plot(x, dsdlaplace(x,0.5,0.5), ylim=c(0,0.6), type="l", main="Probability Function",
          ylab="density", col="red")
     lines(x, dsdlaplace(x,0.3,0.6), type="l", col="green")
     lines(x, dsdlaplace(x,0.9,0.1), type="l", col="blue")
     legend(-2.5, 0.5, expression(paste(p==0.5, ", ", q==0.5),
          paste(p==0.3, ", ", q==0.6),
          paste(p==0.9, ", ", q==0.1)),
          lty=c(1,1,1), col=c("red","green","blue"))
</code></pre>

<hr>
<h2 id='dist.Skew.Laplace'>Skew-Laplace Distribution: Univariate</h2><span id='topic+dslaplace'></span><span id='topic+pslaplace'></span><span id='topic+qslaplace'></span><span id='topic+rslaplace'></span>

<h3>Description</h3>

<p>These functions provide the density, distribution function, quantile
function, and random generation for the univariate, skew-Laplace
distribution with location parameter <code class="reqn">\mu</code>, and two mixture
parameters: <code class="reqn">\alpha</code> and <code class="reqn">\beta</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dslaplace(x, mu, alpha, beta, log=FALSE)
pslaplace(q, mu, alpha, beta)
qslaplace(p, mu, alpha, beta)
rslaplace(n, mu, alpha, beta)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.Skew.Laplace_+3A_x">x</code>, <code id="dist.Skew.Laplace_+3A_q">q</code></td>
<td>
<p>These are each a vector of quantiles.</p>
</td></tr>
<tr><td><code id="dist.Skew.Laplace_+3A_p">p</code></td>
<td>
<p>This is a vector of probabilities.</p>
</td></tr>
<tr><td><code id="dist.Skew.Laplace_+3A_n">n</code></td>
<td>
<p>This is the number of observations, which must be a positive
integer that has length 1.</p>
</td></tr>
<tr><td><code id="dist.Skew.Laplace_+3A_mu">mu</code></td>
<td>
<p>This is the location parameter <code class="reqn">\mu</code>.</p>
</td></tr>
<tr><td><code id="dist.Skew.Laplace_+3A_alpha">alpha</code></td>
<td>
<p>This is a mixture parameter <code class="reqn">\alpha</code>, which
must be positive.</p>
</td></tr>
<tr><td><code id="dist.Skew.Laplace_+3A_beta">beta</code></td>
<td>
<p>This is a mixture parameter <code class="reqn">\beta</code>, which
must be positive.</p>
</td></tr>
<tr><td><code id="dist.Skew.Laplace_+3A_log">log</code></td>
<td>
<p>Logical. If <code>log=TRUE</code>, then the logarithm of the
density is returned.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> Application: Continuous Univariate
</p>
</li>
<li><p> Density 1: <code class="reqn">p(\theta) = \frac{1}{\alpha + \beta}
    \exp(\frac{\theta - \mu}{\alpha}), \theta \le \mu</code>
</p>
</li>
<li><p> Density 2: <code class="reqn">p(\theta) = \frac{1}{\alpha + \beta}
    \exp(\frac{\mu - \theta}{\beta}), \theta &gt; \mu</code>
</p>
</li>
<li><p> Inventor: Fieller, et al. (1992)
</p>
</li>
<li><p> Notation 1: <code class="reqn">\theta \sim \mathcal{SL}(\mu, \alpha, \beta)</code>
</p>
</li>
<li><p> Notation 2: <code class="reqn">p(\theta) = \mathcal{SL}(\theta | \mu,
    \alpha, \beta)</code>
</p>
</li>
<li><p> Parameter 1: location parameter <code class="reqn">\mu</code>
</p>
</li>
<li><p> Parameter 2: mixture parameter <code class="reqn">\alpha &gt; 0</code>
</p>
</li>
<li><p> Parameter 3: mixture parameter <code class="reqn">\beta &gt; 0</code>
</p>
</li>
<li><p> Mean: <code class="reqn">E(\theta) = \mu + \beta - \alpha</code>
</p>
</li>
<li><p> Variance: <code class="reqn">var(\theta) = \alpha^2 + \beta^2</code>
</p>
</li>
<li><p> Mode: <code class="reqn">mode(\theta) = \mu</code>
</p>
</li></ul>

<p>This is the three-parameter general skew-Laplace distribution, which is
an extension of the two-parameter central skew-Laplace distribution. The
general form allows the mode to be shifted along the real line with
parameter <code class="reqn">\mu</code>. In contrast, the central skew-Laplace has mode
zero, and may be reproduced here by setting <code class="reqn">\mu=0</code>.
</p>
<p>The general skew-Laplace distribution is a mixture of a negative
exponential distribution with mean <code class="reqn">\beta</code>, and the negative
of an exponential distribution with mean <code class="reqn">\alpha</code>. The
weights of the positive and negative components are proportional to
their means. The distribution is symmetric when
<code class="reqn">\alpha=\beta</code>, in which case the mean is <code class="reqn">\mu</code>.
</p>
<p>These functions are similar to those in the <code>HyperbolicDist</code> package.
</p>


<h3>Value</h3>

<p><code>dslaplace</code> gives the density,
<code>pslaplace</code> gives the distribution function,
<code>qslaplace</code> gives the quantile function, and
<code>rslaplace</code> generates random deviates.
</p>


<h3>References</h3>

<p>Fieller, N.J., Flenley, E.C., and Olbricht, W. (1992). &quot;Statistics of
Particle Size Data&quot;. <em>Applied Statistics</em>, 41, p. 127&ndash;146.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dalaplace">dalaplace</a></code>,
<code><a href="stats.html#topic+dexp">dexp</a></code>,
<code><a href="#topic+dlaplace">dlaplace</a></code>, 
<code><a href="#topic+dlaplacep">dlaplacep</a></code>, and
<code><a href="#topic+dsdlaplace">dsdlaplace</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
x &lt;- dslaplace(1,0,1,1)
x &lt;- pslaplace(1,0,1,1)
x &lt;- qslaplace(0.5,0,1,1)
x &lt;- rslaplace(100,0,1,1)

#Plot Probability Functions
x &lt;- seq(from=0.1, to=3, by=0.01)
plot(x, dslaplace(x,0,1,1), ylim=c(0,1), type="l", main="Probability Function",
     ylab="density", col="red")
lines(x, dslaplace(x,0,0.5,2), type="l", col="green")
lines(x, dslaplace(x,0,2,0.5), type="l", col="blue")
legend(1.5, 0.9, expression(paste(mu==0, ", ", alpha==1, ", ", beta==1),
     paste(mu==0, ", ", alpha==0.5, ", ", beta==2),
     paste(mu==0, ", ", alpha==2, ", ", beta==0.5)),
     lty=c(1,1,1), col=c("red","green","blue"))
</code></pre>

<hr>
<h2 id='dist.Stick'>Truncated Stick-Breaking Prior Distribution</h2><span id='topic+dStick'></span><span id='topic+rStick'></span>

<h3>Description</h3>

<p>These functions provide the density and random number generation of
the original, truncated stick-breaking (TSB) prior distribution given
<code class="reqn">\theta</code> and <code class="reqn">\gamma</code>, as per Ishwaran and James
(2001).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dStick(theta, gamma, log=FALSE)
rStick(M, gamma)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.Stick_+3A_m">M</code></td>
<td>
<p>This accepts an integer that is equal to one less than the
number of truncated number of possible mixture components
(<code class="reqn">M=1</code>). Unlike most random deviate functions, this is not the
number of random deviates to return.</p>
</td></tr>
<tr><td><code id="dist.Stick_+3A_theta">theta</code></td>
<td>
<p>This is <code class="reqn">\theta</code>, a vector of length
<code class="reqn">M-1</code>, where <code class="reqn">M</code> is the truncated number of possible mixture
components.</p>
</td></tr>
<tr><td><code id="dist.Stick_+3A_gamma">gamma</code></td>
<td>
<p>This is <code class="reqn">\gamma</code>, a scalar, and is usually
gamma-distributed.</p>
</td></tr>
<tr><td><code id="dist.Stick_+3A_log">log</code></td>
<td>
<p>Logical. If <code>log=TRUE</code>, then the logarithm of the
density is returned.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> Application: Discrete Multivariate
</p>
</li>
<li><p> Density: <code class="reqn">p(\pi) = \frac{(1-\theta)^{\beta-1}}{\mathrm{B}(1,\beta)}</code>
</p>
</li>
<li><p> Inventor: Sethuraman, J. (1994)
</p>
</li>
<li><p> Notation 1: <code class="reqn">\pi \sim
    \mathrm{Stick}(\theta,\gamma)</code>
</p>
</li>
<li><p> Notation 2: <code class="reqn">\pi \sim
    \mathrm{GEM}(\theta,\gamma)</code>
</p>
</li>
<li><p> Notation 3: <code class="reqn">p(\pi) = \mathrm{Stick}(\pi | \theta,
    \gamma)</code>
</p>
</li>
<li><p> Notation 4: <code class="reqn">p(\pi) = \mathrm{GEM}(\pi | \theta,
    \gamma)</code>
</p>
</li>
<li><p> Parameter 1: shape parameter <code class="reqn">\theta \in (0,1)</code>
</p>
</li>
<li><p> Parameter 2: shape parameter <code class="reqn">\gamma &gt; 0</code>
</p>
</li>
<li><p> Mean: <code class="reqn">E(\pi) = \frac{1}{1+\gamma}</code>
</p>
</li>
<li><p> Variance: <code class="reqn">var(\pi) = \frac{\gamma}{(1+\gamma)^2
      (\gamma+2)}</code>
</p>
</li>
<li><p> Mode: <code class="reqn">mode(\pi) = 0</code>
</p>
</li></ul>

<p>The original truncated stick-breaking (TSB) prior distribution assigns
each <code class="reqn">\theta</code> to be beta-distributed with parameters
<code class="reqn">\alpha=1</code> and <code class="reqn">\beta=\gamma</code> (Ishwaran and
James, 2001). This distribution is commonly used in truncated Dirichlet
processes (TDPs).
</p>


<h3>Value</h3>

<p><code>dStick</code> gives the density and
<code>rStick</code> generates a random deviate vector of length <code class="reqn">M</code>.
</p>


<h3>References</h3>

<p>Ishwaran, H. and James, L. (2001). &quot;Gibbs Sampling Methods for Stick
Breaking Priors&quot;. <em>Journal of the American Statistical
Association</em>, 96(453), p. 161&ndash;173.
</p>
<p>Sethuraman, J. (1994). &quot;A Constructive Definition of Dirichlet
Priors&quot;. <em>Statistica Sinica</em>, 4, p. 639&ndash;650.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ddirichlet">ddirichlet</a></code>,
<code><a href="#topic+dmvpolya">dmvpolya</a></code>, and
<code><a href="#topic+Stick">Stick</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
dStick(runif(4), 0.1)
rStick(4, 0.1)
</code></pre>

<hr>
<h2 id='dist.Student.t'>Student t Distribution: Univariate</h2><span id='topic+dst'></span><span id='topic+pst'></span><span id='topic+qst'></span><span id='topic+rst'></span>

<h3>Description</h3>

<p>These functions provide the density, distribution function, quantile
function, and random generation for the univariate Student t
distribution with location parameter <code class="reqn">\mu</code>, scale parameter
<code class="reqn">\sigma</code>, and degrees of freedom parameter <code class="reqn">\nu</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dst(x, mu=0, sigma=1, nu=10, log=FALSE)
pst(q, mu=0, sigma=1, nu=10, lower.tail=TRUE, log.p=FALSE)
qst(p, mu=0, sigma=1, nu=10, lower.tail=TRUE, log.p=FALSE)
rst(n, mu=0, sigma=1, nu=10)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.Student.t_+3A_x">x</code>, <code id="dist.Student.t_+3A_q">q</code></td>
<td>
<p>These are each a vector of quantiles.</p>
</td></tr>
<tr><td><code id="dist.Student.t_+3A_p">p</code></td>
<td>
<p>This is a vector of probabilities.</p>
</td></tr>
<tr><td><code id="dist.Student.t_+3A_n">n</code></td>
<td>
<p>This is the number of observations, which must be a positive
integer that has length 1.</p>
</td></tr>
<tr><td><code id="dist.Student.t_+3A_mu">mu</code></td>
<td>
<p>This is the location parameter <code class="reqn">\mu</code>.</p>
</td></tr>
<tr><td><code id="dist.Student.t_+3A_sigma">sigma</code></td>
<td>
<p>This is the scale parameter <code class="reqn">\sigma</code>, which
must be positive.</p>
</td></tr>
<tr><td><code id="dist.Student.t_+3A_nu">nu</code></td>
<td>
<p>This is the degrees of freedom parameter <code class="reqn">\nu</code>,
which must be positive.</p>
</td></tr>
<tr><td><code id="dist.Student.t_+3A_lower.tail">lower.tail</code></td>
<td>
<p>Logical. If <code>lower.tail=TRUE</code>, then
probabilities are <code class="reqn">Pr[X \le x]</code>, otherwise,
<code class="reqn">Pr[X &gt; x]</code>.</p>
</td></tr>
<tr><td><code id="dist.Student.t_+3A_log">log</code>, <code id="dist.Student.t_+3A_log.p">log.p</code></td>
<td>
<p>Logical. If <code>log=TRUE</code>, then the logarithm of the
density or probability is returned.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> Application: Continuous Univariate
</p>
</li>
<li><p> Density: <code class="reqn">p(\theta) =
    \frac{\Gamma[(\nu+1)/2]}{\Gamma(\nu/2)} \sqrt{\nu \pi} \sigma[1 +
    \frac{1}{\nu}[\frac{\theta - \mu}{\sigma}]^2]^{(-\nu + 1)/2}</code>
</p>
</li>
<li><p> Inventor: William Sealy Gosset (1908)
</p>
</li>
<li><p> Notation 1: <code class="reqn">\theta \sim \mathrm{t}(\mu, \sigma, \nu)</code>
</p>
</li>
<li><p> Notation 2: <code class="reqn">p(\theta) = \mathrm{t}(\theta | \mu, \sigma,
    \nu)</code>
</p>
</li>
<li><p> Parameter 1: location parameter <code class="reqn">\mu</code>
</p>
</li>
<li><p> Parameter 2: scale parameter <code class="reqn">\sigma &gt; 0</code>
</p>
</li>
<li><p> Parameter 3: degrees of freedom <code class="reqn">\nu &gt; 0</code>
</p>
</li>
<li><p> Mean: <code class="reqn">E(\theta) = \mu</code>, for <code class="reqn">\nu &gt; 1</code>, otherwise undefined
</p>
</li>
<li><p> Variance: <code class="reqn">var(\theta) = \frac{\nu}{\nu -
      2}\sigma^2</code>, for <code class="reqn">\nu &gt;
    2</code>
</p>
</li>
<li><p> Mode: <code class="reqn">mode(\theta) = \mu</code>
</p>
</li></ul>

<p>The Student t-distribution is often used as an alternative to the normal
distribution as a model for data. It is frequently the case that real
data have heavier tails than the normal distribution allows for. The
classical approach was to identify outliers and exclude or downweight
them in some way. However, it is not always easy to identify outliers
(especially in high dimensions), and the Student t-distribution is a
natural choice of model-form for such data. It provides a parametric
approach to robust statistics.
</p>
<p>The degrees of freedom parameter, <code class="reqn">\nu</code>, controls the kurtosis
of the distribution, and is correlated with the scale parameter
<code class="reqn">\sigma</code>. The likelihood can have multiple local maxima
and, as such, it is often necessary to fix <code class="reqn">\nu</code> at a fairly
low value and estimate the other parameters taking this as given.
Some authors report that values between 3 and 9 are often good
choices, and some authors suggest 5 is often a good choice.
</p>
<p>In the limit <code class="reqn">\nu \rightarrow \infty</code>, the
Student t-distribution approaches
<code class="reqn">\mathcal{N}(\mu, \sigma^2)</code>. The
case of <code class="reqn">\nu = 1</code> is the Cauchy distribution.
</p>
<p>The <code>pst</code> and <code>qst</code> functions are similar to those in the
<code>gamlss.dist</code> package.
</p>


<h3>Value</h3>

<p><code>dst</code> gives the density,
<code>pst</code> gives the distribution function,
<code>qst</code> gives the quantile function, and
<code>rst</code> generates random deviates.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+dcauchy">dcauchy</a></code>,
<code><a href="#topic+dmvt">dmvt</a></code>,
<code><a href="#topic+dmvtp">dmvtp</a></code>,
<code><a href="stats.html#topic+dnorm">dnorm</a></code>,
<code><a href="#topic+dnormp">dnormp</a></code>,
<code><a href="#topic+dnormv">dnormv</a></code>,
<code><a href="#topic+dstp">dstp</a></code>, and
<code><a href="stats.html#topic+dt">dt</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
x &lt;- dst(1,0,1,10)
x &lt;- pst(1,0,1,10)
x &lt;- qst(0.5,0,1,10)
x &lt;- rst(100,0,1,10)

#Plot Probability Functions
x &lt;- seq(from=-5, to=5, by=0.1)
plot(x, dst(x,0,1,0.1), ylim=c(0,1), type="l", main="Probability Function",
     ylab="density", col="red")
lines(x, dst(x,0,1,1), type="l", col="green")
lines(x, dst(x,0,1,10), type="l", col="blue")
legend(1, 0.9, expression(paste(mu==0, ", ", sigma==1, ", ", nu==0.5),
     paste(mu==0, ", ", sigma==1, ", ", nu==1),
     paste(mu==0, ", ", sigma==1, ", ", nu==10)),
     lty=c(1,1,1), col=c("red","green","blue"))
</code></pre>

<hr>
<h2 id='dist.Student.t.Precision'>Student t Distribution: Precision Parameterization</h2><span id='topic+dstp'></span><span id='topic+pstp'></span><span id='topic+qstp'></span><span id='topic+rstp'></span>

<h3>Description</h3>

<p>These functions provide the density, distribution function, quantile
function, and random generation for the univariate Student t
distribution with location parameter <code class="reqn">\mu</code>, precision parameter
<code class="reqn">\tau</code>, and degrees of freedom parameter <code class="reqn">\nu</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dstp(x, mu=0, tau=1, nu=10, log=FALSE)
pstp(q, mu=0, tau=1, nu=10, lower.tail=TRUE, log.p=FALSE)
qstp(p, mu=0, tau=1, nu=10, lower.tail=TRUE, log.p=FALSE)
rstp(n, mu=0, tau=1, nu=10)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.Student.t.Precision_+3A_x">x</code>, <code id="dist.Student.t.Precision_+3A_q">q</code></td>
<td>
<p>These are each a vector of quantiles.</p>
</td></tr>
<tr><td><code id="dist.Student.t.Precision_+3A_p">p</code></td>
<td>
<p>This is a vector of probabilities.</p>
</td></tr>
<tr><td><code id="dist.Student.t.Precision_+3A_n">n</code></td>
<td>
<p>This is the number of observations, which must be a positive
integer that has length 1.</p>
</td></tr>
<tr><td><code id="dist.Student.t.Precision_+3A_mu">mu</code></td>
<td>
<p>This is the location parameter <code class="reqn">\mu</code>.</p>
</td></tr>
<tr><td><code id="dist.Student.t.Precision_+3A_tau">tau</code></td>
<td>
<p>This is the precision parameter <code class="reqn">\tau</code>, which
must be positive.</p>
</td></tr>
<tr><td><code id="dist.Student.t.Precision_+3A_nu">nu</code></td>
<td>
<p>This is the degrees of freedom parameter <code class="reqn">\nu</code>,
which must be positive.</p>
</td></tr>
<tr><td><code id="dist.Student.t.Precision_+3A_lower.tail">lower.tail</code></td>
<td>
<p>Logical. If <code>lower.tail=TRUE</code>, then
probabilities are <code class="reqn">Pr[X \le x]</code>, otherwise,
<code class="reqn">Pr[X &gt; x]</code>.</p>
</td></tr>
<tr><td><code id="dist.Student.t.Precision_+3A_log">log</code>, <code id="dist.Student.t.Precision_+3A_log.p">log.p</code></td>
<td>
<p>Logical. If <code>log=TRUE</code>, then the logarithm of the
density or probability is returned.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> Application: Continuous Univariate
</p>
</li>
<li><p> Density: <code class="reqn">p(\theta) =
    \frac{\Gamma((\nu+1)/2)}{\Gamma(\nu/2)} \sqrt{\frac{\tau}{\nu\pi}} (1
    + \frac{\tau}{\nu} (\theta-\mu)^2)^{-(\nu+1)/2}</code>
</p>
</li>
<li><p> Inventor: William Sealy Gosset (1908)
</p>
</li>
<li><p> Notation 1: <code class="reqn">\theta \sim \mathrm{t}(\mu, \sqrt{\tau^{-1}},
    \nu)</code>
</p>
</li>
<li><p> Notation 2: <code class="reqn">p(\theta) = \mathrm{t}(\theta | \mu,
    \sqrt{\tau^{-1}}, \nu)</code>
</p>
</li>
<li><p> Parameter 1: location parameter <code class="reqn">\mu</code>
</p>
</li>
<li><p> Parameter 2: precision parameter <code class="reqn">\tau &gt; 0</code>
</p>
</li>
<li><p> Parameter 3: degrees of freedom <code class="reqn">\nu &gt; 0</code>
</p>
</li>
<li><p> Mean: <code class="reqn">E(\theta) = \mu</code>, for <code class="reqn">\nu &gt; 1</code>, otherwise undefined
</p>
</li>
<li><p> Variance: <code class="reqn">var(\theta) = \frac{1}{\tau}\frac{\nu}{\nu -
      2}</code>, for <code class="reqn">\nu &gt; 2</code>
</p>
</li>
<li><p> Mode: <code class="reqn">mode(\theta) = \mu</code>
</p>
</li></ul>

<p>The Student t-distribution is often used as an alternative to the normal
distribution as a model for data. It is frequently the case that real
data have heavier tails than the normal distribution allows for. The
classical approach was to identify outliers and exclude or downweight
them in some way. However, it is not always easy to identify outliers
(especially in high dimensions), and the Student t-distribution is a
natural choice of model-form for such data. It provides a parametric
approach to robust statistics.
</p>
<p>The degrees of freedom parameter, <code class="reqn">\nu</code>, controls the kurtosis
of the distribution, and is correlated with the precision parameter
<code class="reqn">\tau</code>. The likelihood can have multiple local maxima
and, as such, it is often necessary to fix <code class="reqn">\nu</code> at a fairly
low value and estimate the other parameters taking this as given.
Some authors report that values between 3 and 9 are often good
choices, and some authors suggest 5 is often a good choice.
</p>
<p>In the limit <code class="reqn">\nu \rightarrow \infty</code>, the
Student t-distribution approaches
<code class="reqn">\mathcal{N}(\mu, \sigma^2)</code>. The
case of <code class="reqn">\nu = 1</code> is the Cauchy distribution.
</p>


<h3>Value</h3>

<p><code>dstp</code> gives the density,
<code>pstp</code> gives the distribution function,
<code>qstp</code> gives the quantile function, and
<code>rstp</code> generates random deviates.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+dcauchy">dcauchy</a></code>,
<code><a href="#topic+dmvt">dmvt</a></code>,
<code><a href="#topic+dmvtp">dmvtp</a></code>,
<code><a href="stats.html#topic+dnorm">dnorm</a></code>,
<code><a href="#topic+dnormp">dnormp</a></code>,
<code><a href="#topic+dnormv">dnormv</a></code>,
<code><a href="#topic+dst">dst</a></code>,
<code><a href="stats.html#topic+dt">dt</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
x &lt;- dstp(1,0,1,10)
x &lt;- pstp(1,0,1,10)
x &lt;- qstp(0.5,0,1,10)
x &lt;- rstp(100,0,1,10)

#Plot Probability Functions
x &lt;- seq(from=-5, to=5, by=0.1)
plot(x, dstp(x,0,1,0.1), ylim=c(0,1), type="l", main="Probability Function",
     ylab="density", col="red")
lines(x, dstp(x,0,1,1), type="l", col="green")
lines(x, dstp(x,0,1,10), type="l", col="blue")
legend(1, 0.9, expression(paste(mu==0, ", ", tau==1, ", ", nu==0.5),
     paste(mu==0, ", ", tau==1, ", ", nu==1),
     paste(mu==0, ", ", tau==1, ", ", nu==10)),
     lty=c(1,1,1), col=c("red","green","blue"))
</code></pre>

<hr>
<h2 id='dist.Truncated'>Truncated Distributions</h2><span id='topic+dtrunc'></span><span id='topic+extrunc'></span><span id='topic+ptrunc'></span><span id='topic+qtrunc'></span><span id='topic+rtrunc'></span><span id='topic+vartrunc'></span>

<h3>Description</h3>

<p>Density, distribution function, quantile function and random
generation for truncated distributions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dtrunc(x, spec, a=-Inf, b=Inf, log=FALSE, ...)
extrunc(spec, a=-Inf, b=Inf, ...)
ptrunc(x, spec, a=-Inf, b=Inf, ...)
qtrunc(p, spec, a=-Inf, b=Inf, ...)
rtrunc(n, spec, a=-Inf, b=Inf, ...)
vartrunc(spec, a=-Inf, b=Inf, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.Truncated_+3A_n">n</code></td>
<td>
<p>This is a the number of random draws for <code>rtrunc</code>.</p>
</td></tr>
<tr><td><code id="dist.Truncated_+3A_p">p</code></td>
<td>
<p>This is a vector of probabilities.</p>
</td></tr>
<tr><td><code id="dist.Truncated_+3A_x">x</code></td>
<td>
<p>This is a vector to be evaluated.</p>
</td></tr>
<tr><td><code id="dist.Truncated_+3A_spec">spec</code></td>
<td>
<p>The base name of a probability distribution is
specified here. For example, to estimate the density of a
truncated normal distribution, enter <code>norm</code>.</p>
</td></tr>
<tr><td><code id="dist.Truncated_+3A_a">a</code></td>
<td>
<p>This is the lower bound of truncation, which defaults
to negative infinity.</p>
</td></tr>
<tr><td><code id="dist.Truncated_+3A_b">b</code></td>
<td>
<p>This is the upper bound of truncation, which defaults
to infinity.</p>
</td></tr>
<tr><td><code id="dist.Truncated_+3A_log">log</code></td>
<td>
<p>Logical. If <code>log=TRUE</code>, then the logarithm of the
density is returned.</p>
</td></tr>
<tr><td><code id="dist.Truncated_+3A_...">...</code></td>
<td>
<p>Additional arguments pertain to the probability
distribution specified in the <code>spec</code> argument.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A truncated distribution is a conditional distribution that results
from a priori restricting the domain of some other probability
distribution. More than merely preventing values outside of truncated
bounds, a proper truncated distribution integrates to one within the
truncated bounds. For more information on propriety, see
<code><a href="#topic+is.proper">is.proper</a></code>. In contrast to a truncated distribution, a
censored distribution occurs when the probability distribution is
still allowed outside of a pre-specified range. Here, distributions
are truncated to the interval <code class="reqn">[a,b]</code>, such as <code class="reqn">p(\theta) \in
  [a,b]</code>.
</p>
<p>The <code>dtrunc</code> function is often used in conjunction with the
<code><a href="#topic+interval">interval</a></code> function to truncate prior probability
distributions in the model specification function for use with these
numerical approximation functions: <code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>,
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>, and <code><a href="#topic+PMC">PMC</a></code>.
</p>
<p>The R code of Nadarajah and Kotz (2006) has been modified to work with
log-densities.
</p>


<h3>Value</h3>

<p><code>dtrunc</code> gives the density,
<code>extrunc</code> gives the expectation,
<code>ptrunc</code> gives the distribution function,
<code>qtrunc</code> gives the quantile function,
<code>rtrunc</code> generates random deviates, and
<code>vartrunc</code> gives the variance of the truncated distribution.
</p>


<h3>References</h3>

<p>Nadarajah, S. and Kotz, S. (2006). &quot;R Programs for Computing Truncated
Distributions&quot;. <em>Journal of Statistical Software</em>, 16,
Code Snippet 2, p. 1&ndash;8.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+interval">interval</a></code>,
<code><a href="#topic+is.proper">is.proper</a></code>,
<code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>,
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>, and
<code><a href="#topic+PMC">PMC</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
x &lt;- seq(-0.5, 0.5, by = 0.1)
y &lt;- dtrunc(x, "norm", a=-0.5, b=0.5, mean=0, sd=2)
</code></pre>

<hr>
<h2 id='dist.Wishart'>Wishart Distribution</h2><span id='topic+dwishart'></span><span id='topic+rwishart'></span>

<h3>Description</h3>

<p>These functions provide the density and random number generation
for the Wishart distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>   dwishart(Omega, nu, S, log=FALSE)
   rwishart(nu, S)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.Wishart_+3A_omega">Omega</code></td>
<td>
<p>This is the symmetric, positive-definite <code class="reqn">k \times
      k</code> matrix <code class="reqn">\Omega</code>.</p>
</td></tr>
<tr><td><code id="dist.Wishart_+3A_nu">nu</code></td>
<td>
<p>This is the scalar degrees of freedom <code class="reqn">\nu</code>.</p>
</td></tr>
<tr><td><code id="dist.Wishart_+3A_s">S</code></td>
<td>
<p>This is the symmetric, positive-semidefinite, <code class="reqn">k \times
      k</code> scale matrix <code class="reqn">\textbf{S}</code>.</p>
</td></tr>
<tr><td><code id="dist.Wishart_+3A_log">log</code></td>
<td>
<p>Logical. If <code>log=TRUE</code>, then the logarithm of the
density is returned.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> Application: Continuous Multivariate
</p>
</li>
<li><p> Density: <code class="reqn">p(\theta) = (2^{\nu k/2} \pi^{k(k-1)/4}
      \prod^k_{i=1} \Gamma(\frac{\nu+1-i}{2}))^{-1} |\textbf{S}|^{-nu/2}
      |\Omega|^{(nu-k-1)/2} \exp(-\frac{1}{2} tr(\textbf{S}^{-1}
      \Omega))</code>
</p>
</li>
<li><p> Inventor: John Wishart (1928)
</p>
</li>
<li><p> Notation 1: <code class="reqn">\Omega \sim
      \mathcal{W}_{\nu}(\textbf{S})</code>
</p>
</li>
<li><p> Notation 2: <code class="reqn">p(\Omega) = \mathcal{W}_{\nu}(\Omega |
      \textbf{S})</code>
</p>
</li>
<li><p> Parameter 1: degrees of freedom <code class="reqn">\nu \ge k</code>
</p>
</li>
<li><p> Parameter 2: symmetric, positive-semidefinite
<code class="reqn">k \times k</code> scale matrix <code class="reqn">\textbf{S}</code>
</p>
</li>
<li><p> Mean: <code class="reqn">E(\Omega) = \nu \textbf{S}</code>
</p>
</li>
<li><p> Variance: <code class="reqn">var(\Omega) = \nu (\textbf{S}^2_{i,j} +
      \textbf{S}_{i,i} \textbf{S}_{j,j})</code>
</p>
</li>
<li><p> Mode: <code class="reqn">mode(\Omega) = (\nu - k - 1)
      \textbf{S}</code>, for <code class="reqn">\nu \ge k + 1</code>
</p>
</li></ul>

<p>The Wishart distribution is a generalization to multiple dimensions of
the chi-square distribution, or, in the case of non-integer degrees of
freedom, of the gamma distribution. However, the Wishart distribution is
not called the multivariate chi-squared distribution because the
marginal distribution of the off-diagonal elements is not chi-squared.
</p>
<p>The Wishart is the conjugate prior distribution for the precision matrix
<code class="reqn">\Omega</code>, the inverse of which (covariance matrix
<code class="reqn">\Sigma</code>) is used in a multivariate normal distribution.
</p>
<p>The integral is finite when <code class="reqn">\nu \ge k</code>, where
<code class="reqn">\nu</code> is the scalar degrees of freedom parameter, and <code class="reqn">k</code> is
the dimension of scale matrix <code class="reqn">\textbf{S}</code>. The density is finite
when <code class="reqn">\nu ge k + 1</code>, which is recommended.
</p>
<p>The degrees of freedom, <code class="reqn">\nu</code>, is equivalent to specifying a
prior sample size, indicating the confidence in <code class="reqn">\textbf{S}</code>,
where <code class="reqn">\textbf{S}</code> is a prior guess at the order of covariance
matrix <code class="reqn">\Sigma</code>. A flat prior distribution is
obtained as <code class="reqn">\nu \rightarrow 0</code>.
</p>
<p>When applicable, the alternative Cholesky parameterization should be
preferred. For more information, see <code><a href="#topic+dwishartc">dwishartc</a></code>.
</p>
<p>The Wishart prior lacks flexibility, having only one parameter,
<code class="reqn">\nu</code>, to control the variability for all <code class="reqn">k(k + 1)/2</code>
elements. Popular choices for the scale matrix <code class="reqn">\textbf{S}</code>
include an identity matrix or sample covariance matrix. When the model
sample size is small, the specification of the scale matrix can be
influential.
</p>
<p>Although the related inverse Wishart distribution has a dependency
between variance and correlation, the Wishart distribution does not
have this dependency.
</p>
<p>The matrix gamma (<code><a href="#topic+dmatrixgamma">dmatrixgamma</a></code>) distribution is a more
general version of the Wishart distribution, and the Yang-Berger
(<code><a href="#topic+dyangberger">dyangberger</a></code>) distribution is an alterative that is a
least informative prior (LIP).
</p>


<h3>Value</h3>

<p><code>dwishart</code> gives the density and
<code>rwishart</code> generates random deviates.
</p>


<h3>References</h3>

<p>Wishart, J. (1928). &quot;The Generalised Product Moment Distribution in
Samples from a Normal Multivariate Population&quot;. <em>Biometrika</em>,
20A(1-2), p. 32&ndash;52.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+dchisq">dchisq</a></code>,
<code><a href="stats.html#topic+dgamma">dgamma</a></code>,
<code><a href="#topic+dinvwishart">dinvwishart</a></code>,
<code><a href="#topic+dmatrixgamma">dmatrixgamma</a></code>,
<code><a href="#topic+dmvnp">dmvnp</a></code>,
<code><a href="#topic+dwishartc">dwishartc</a></code>,
<code><a href="#topic+Prec2Cov">Prec2Cov</a></code>, and
<code><a href="#topic+dyangberger">dyangberger</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
x &lt;- dwishart(matrix(c(2,-.3,-.3,4),2,2), 3, matrix(c(1,.1,.1,1),2,2))
x &lt;- rwishart(3, matrix(c(1,.1,.1,1),2,2))
</code></pre>

<hr>
<h2 id='dist.Wishart.Cholesky'>Wishart Distribution: Cholesky Parameterization</h2><span id='topic+dwishartc'></span><span id='topic+rwishartc'></span>

<h3>Description</h3>

<p>These functions provide the density and random number generation
for the Wishart distribution with the Cholesky parameterization.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>   dwishartc(U, nu, S, log=FALSE)
   rwishartc(nu, S)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.Wishart.Cholesky_+3A_u">U</code></td>
<td>
<p>This is the upper-triangular <code class="reqn">k \times k</code> matrix
for the Cholesky factor <code class="reqn">\textbf{U}</code> of precision matrix
<code class="reqn">\Omega</code>.</p>
</td></tr>
<tr><td><code id="dist.Wishart.Cholesky_+3A_nu">nu</code></td>
<td>
<p>This is the scalar degrees of freedom <code class="reqn">\nu</code>.</p>
</td></tr>
<tr><td><code id="dist.Wishart.Cholesky_+3A_s">S</code></td>
<td>
<p>This is the symmetric, positive-semidefinite, <code class="reqn">k \times
      k</code> scale matrix <code class="reqn">\textbf{S}</code>.</p>
</td></tr>
<tr><td><code id="dist.Wishart.Cholesky_+3A_log">log</code></td>
<td>
<p>Logical. If <code>log=TRUE</code>, then the logarithm of the
density is returned.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> Application: Continuous Multivariate
</p>
</li>
<li><p> Density: <code class="reqn">p(\theta) = (2^{\nu k/2} \pi^{k(k-1)/4}
      \prod^k_{i=1} \Gamma(\frac{\nu+1-i}{2}))^{-1} |\textbf{S}|^{-nu/2}
      |\Omega|^{(nu-k-1)/2} \exp(-\frac{1}{2} tr(\textbf{S}^{-1}
      \Omega))</code>
</p>
</li>
<li><p> Inventor: John Wishart (1928)
</p>
</li>
<li><p> Notation 1: <code class="reqn">\Omega \sim
      \mathcal{W}_{\nu}(\textbf{S})</code>
</p>
</li>
<li><p> Notation 2: <code class="reqn">p(\Omega) = \mathcal{W}_{\nu}(\Omega |
      \textbf{S})</code>
</p>
</li>
<li><p> Parameter 1: degrees of freedom <code class="reqn">\nu \ge k</code>
</p>
</li>
<li><p> Parameter 2: symmetric, positive-semidefinite
<code class="reqn">k \times k</code> scale matrix <code class="reqn">\textbf{S}</code>
</p>
</li>
<li><p> Mean: <code class="reqn">E(\Omega) = \nu \textbf{S}</code>
</p>
</li>
<li><p> Variance: <code class="reqn">var(\Omega) = \nu (\textbf{S}^2_{i,j} +
      \textbf{S}_{i,i} \textbf{S}_{j,j})</code>
</p>
</li>
<li><p> Mode: <code class="reqn">mode(\Omega) = (\nu - k - 1)
      \textbf{S}</code>, for <code class="reqn">\nu \ge k + 1</code>
</p>
</li></ul>

<p>The Wishart distribution is a generalization to multiple dimensions of
the chi-square distribution, or, in the case of non-integer degrees of
freedom, of the gamma distribution. However, the Wishart distribution is
not called the multivariate chi-squared distribution because the
marginal distribution of the off-diagonal elements is not chi-squared.
</p>
<p>The Wishart is the conjugate prior distribution for the precision matrix
<code class="reqn">\Omega</code>, the inverse of which (covariance matrix
<code class="reqn">\Sigma</code>) is used in a multivariate normal distribution. In
this parameterization, <code class="reqn">\Omega</code> has been decomposed to the
upper-triangular Cholesky factor <code class="reqn">\textbf{U}</code>, as per
<code><a href="Matrix.html#topic+chol">chol</a></code>.
</p>
<p>The integral is finite when <code class="reqn">\nu \ge k</code>, where
<code class="reqn">\nu</code> is the scalar degrees of freedom parameter, and <code class="reqn">k</code> is
the dimension of scale matrix <code class="reqn">\textbf{S}</code>. The density is finite
when <code class="reqn">\nu ge k + 1</code>, which is recommended.
</p>
<p>The degrees of freedom, <code class="reqn">\nu</code>, is equivalent to specifying a
prior sample size, indicating the confidence in <code class="reqn">\textbf{S}</code>,
where <code class="reqn">\textbf{S}</code> is a prior guess at the order of covariance
matrix <code class="reqn">\Sigma</code>. A flat prior distribution is
obtained as <code class="reqn">\nu \rightarrow 0</code>.
</p>
<p>In practice, <code class="reqn">\textbf{U}</code> is fully unconstrained for proposals
when its diagonal is log-transformed. The diagonal is exponentiated
after a proposal and before other calculations. Overall, the
Cholesky parameterization is faster than the traditional
parameterization. Compared with <code>dwishart</code>, <code>dwishartc</code>
must additionally matrix-multiply the Cholesky back to the precision
matrix, but it does not have to check for or correct the precision
matrix to positive-semidefiniteness, which overall is slower. Compared
with <code>rwishart</code>, <code>rwishartc</code> must additionally
calculate a Cholesky decomposition, and is therefore slower.
</p>
<p>The Wishart prior lacks flexibility, having only one parameter,
<code class="reqn">\nu</code>, to control the variability for all <code class="reqn">k(k + 1)/2</code>
elements. Popular choices for the scale matrix <code class="reqn">\textbf{S}</code>
include an identity matrix or sample covariance matrix. When the model
sample size is small, the specification of the scale matrix can be
influential.
</p>
<p>Although the related inverse Wishart distribution has a dependency
between variance and correlation, the Wishart distribution does not
have this dependency.
</p>
<p>The matrix gamma (<code><a href="#topic+dmatrixgamma">dmatrixgamma</a></code>) distribution is a more
general version of the Wishart distribution, and the Yang-Berger
(<code><a href="#topic+dyangberger">dyangberger</a></code>) distribution is an alterative that is a
least informative prior (LIP).
</p>


<h3>Value</h3>

<p><code>dwishartc</code> gives the density and
<code>rwishartc</code> generates random deviates.
</p>


<h3>References</h3>

<p>Wishart, J. (1928). &quot;The Generalised Product Moment Distribution in
Samples from a Normal Multivariate Population&quot;. <em>Biometrika</em>,
20A(1-2), p. 32&ndash;52.
</p>


<h3>See Also</h3>

<p><code><a href="Matrix.html#topic+chol">chol</a></code>,
<code><a href="stats.html#topic+dchisq">dchisq</a></code>,
<code><a href="stats.html#topic+dgamma">dgamma</a></code>,
<code><a href="#topic+dinvwishart">dinvwishart</a></code>,
<code><a href="#topic+dinvwishartc">dinvwishartc</a></code>,
<code><a href="#topic+dmatrixgamma">dmatrixgamma</a></code>,
<code><a href="#topic+dmvnp">dmvnp</a></code>,
<code><a href="#topic+dmvnpc">dmvnpc</a></code>,
<code><a href="#topic+Prec2Cov">Prec2Cov</a></code>, and
<code><a href="#topic+dyangbergerc">dyangbergerc</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
Omega &lt;- matrix(c(2,-.3,-.3,4),2,2)
U &lt;- chol(Omega)
x &lt;- dwishartc(U, 3, matrix(c(1,.1,.1,1),2,2))
x &lt;- rwishartc(3, matrix(c(1,.1,.1,1),2,2))
</code></pre>

<hr>
<h2 id='dist.YangBerger'>Yang-Berger Distribution</h2><span id='topic+dyangberger'></span><span id='topic+dyangbergerc'></span>

<h3>Description</h3>

<p>This is the density function for the Yang-Berger prior distribution
for a covariance matrix or precision matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dyangberger(x, log=FALSE)
dyangbergerc(x, log=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.YangBerger_+3A_x">x</code></td>
<td>
<p>This is the <code class="reqn">k \times k</code> positive-definite
covariance matrix or precision matrix for <code>dyangberger</code> or the
Cholesky factor <code class="reqn">\textbf{U}</code> of the covariance matrix or
precision matrix for <code>dyangbergerc</code>.</p>
</td></tr>
<tr><td><code id="dist.YangBerger_+3A_log">log</code></td>
<td>
<p>Logical. If <code>log=TRUE</code>, then the logarithm of the
density is returned.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> Application: Continuous Multivariate
</p>
</li>
<li><p> Density: <code class="reqn">p(\theta) = \frac{1}{|\theta|^{\prod (d_j -
      d_{j-1})}}</code>,
where <code class="reqn">d</code> are increasing eigenvalues. See equation 13 in
Yang and Berger (1994).
</p>
</li>
<li><p> Inventor: Yang and Berger (1994)
</p>
</li>
<li><p> Notation 1: <code class="reqn">\theta \sim \mathcal{YB}</code>
</p>
</li>
<li><p> Mean: 
</p>
</li>
<li><p> Variance: 
</p>
</li>
<li><p> Mode: 
</p>
</li></ul>

<p>Yang and Berger (1994) derived a least informative prior (LIP) for a
covariance matrix or precision matrix. The Yang-Berger (YB) distribution
does not have any parameters. It is a reference prior for objective
Bayesian inference. The Cholesky parameterization is also provided here.
</p>
<p>The YB prior distribution results in a proper posterior. It involves an
eigendecomposition of the covariance matrix or precision matrix. It is
difficult to interpret a model that uses the YB prior, due to a lack of
intuition regarding the relationship between eigenvalues and
correlations.
</p>
<p>Compared to Jeffreys prior for a covariance matrix, this reference prior
encourages equal eigenvalues, and therefore results in a covariance
matrix or precision matrix with a better shrinkage of its
eigenstructure.
</p>


<h3>Value</h3>

<p><code>dyangberger</code> and <code>dyangbergerc</code> give the density.
</p>


<h3>References</h3>

<p>Yang, R. and Berger, J.O. (1994). &quot;Estimation of a Covariance Matrix
using the Reference Prior&quot;. <em>Annals of Statistics</em>, 2,
p. 1195-1211.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dinvwishart">dinvwishart</a></code> and
<code><a href="#topic+dwishart">dwishart</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
X &lt;- matrix(c(1,0.8,0.8,1), 2, 2)
dyangberger(X, log=TRUE)
</code></pre>

<hr>
<h2 id='dist.Zellner'>Hyperprior-g Prior and Zellner's g-Prior</h2><span id='topic+dhyperg'></span><span id='topic+dzellner'></span><span id='topic+rzellner'></span>

<h3>Description</h3>

<p>These functions provide the density of the hyper-g prior (Liang et
al., 2008), and both the density and random generation of Zellner's
g-prior (Zellner, 1986).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dhyperg(g, alpha=3, log=FALSE)
dzellner(beta, g, sigma, X, log=FALSE)
rzellner(n, g, sigma, X)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.Zellner_+3A_alpha">alpha</code></td>
<td>
<p>This is a positive scale hyperhyperparameter that is
proper when <code class="reqn">\alpha &gt; 2</code>. The default is
<code>alpha=3</code>.</p>
</td></tr>
<tr><td><code id="dist.Zellner_+3A_beta">beta</code></td>
<td>
<p>This is regression effects <code class="reqn">\beta</code>, a vector of
length <code class="reqn">J</code>.</p>
</td></tr>
<tr><td><code id="dist.Zellner_+3A_g">g</code></td>
<td>
<p>This is hyperparameter <code class="reqn">g</code>, a positive scalar.</p>
</td></tr>
<tr><td><code id="dist.Zellner_+3A_n">n</code></td>
<td>
<p>This is the number of random deviates to generate.</p>
</td></tr>
<tr><td><code id="dist.Zellner_+3A_sigma">sigma</code></td>
<td>
<p>This is the residual standard deviation
<code class="reqn">\sigma</code>, a positive scalar.</p>
</td></tr>
<tr><td><code id="dist.Zellner_+3A_x">X</code></td>
<td>
<p>This is a full-rank <code class="reqn">N \times J</code> design matrix
<code class="reqn">\textbf{X}</code> for <code class="reqn">N</code> records and <code class="reqn">J</code> predictors,
where <code class="reqn">J+1 &lt; N</code>. Zellner's g-prior has been extended (elsewhere)
via singular value decomposition (SVD) to the case where
<code class="reqn">J &gt; N</code>.</p>
</td></tr>
<tr><td><code id="dist.Zellner_+3A_log">log</code></td>
<td>
<p>Logical. If <code>log=TRUE</code>, then the logarithm of the
density is returned.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> Application: Continuous Multivariate
</p>
</li>
<li><p> Density: <code class="reqn">p(\theta) = \frac{1}{(2\pi)^{J/2}|(g
      \sigma^2(\textbf{X}^T \textbf{X})^{-1})^{-1}|^{1/2}}
    \exp(-\frac{1}{2}(\theta - \mu)'(g \sigma^2(\textbf{X}^T
    \textbf{X})^{-1})^{-1}(\theta - \mu))</code>
</p>
</li>
<li><p> Inventor: Zellner, A. (1986)
</p>
</li>
<li><p> Notation 1: <code class="reqn">\theta \sim
    \mathrm{N}_J(0, g \sigma^2(\textbf{X}^T \textbf{X})^{-1})</code>
</p>
</li>
<li><p> Notation 2: <code class="reqn">p(\theta) = \mathrm{N}_J(\theta | g, \sigma^2,
    \textbf{X})</code>
</p>
</li>
<li><p> Parameter 1: location parameter <code class="reqn">\beta</code>
</p>
</li>
<li><p> Parameter 2: scale parameter <code class="reqn">g &gt; 0</code>
</p>
</li>
<li><p> Parameter 3: scale parameter <code class="reqn">\sigma^2 &gt; 0</code>
</p>
</li>
<li><p> Mean:
</p>
</li>
<li><p> Variance:
</p>
</li>
<li><p> Mode:
</p>
</li></ul>

<p>Zellner's g-prior is a popular, data-dependent, elliptical, improper,
least-informative prior distribution on regression effects
<code class="reqn">\beta</code> in a Gaussian regression model. It is a particular
form in the conjugate Normal-Gamma family. Zellner's g-prior is also
used for estimating Bayes factors (for hypothesis testing) with a
simpler form, as well as in model selection and variable selection. The
marginal posterior distribution of regression effects <code class="reqn">\beta</code>
is multivariate t.
</p>
<p>One of many nice properties of Zellner's g-prior is that it adapts
automatically to near-collinearity between different
predictors. Zellner's g-prior puts most of its prior mass in the
direction that causes the regression coefficients of correlated
predictors to be smoothed away from each other. When coupled with model
selection, Zellner's g-prior discourages highly collinear predictors
from entering the models simultaneously by inducing a negative
correlation between the coefficients. However, when it is desirable for
collinear predictors to enter simultaneously, a modification has been
proposed (though not included here) in which
<code class="reqn">(\textbf{X}^T \textbf{X})^{-1}</code> is replaced with
<code class="reqn">(\textbf{X}^T \textbf{X})^\lambda</code>. For more
information, see Krishna et al. (2009).
</p>
<p>For variable selection, large values of <code class="reqn">g</code>, with a prior mean of
zero for <code class="reqn">\beta</code>, encourage models with few, large
coefficients. Conversely, small values of <code class="reqn">g</code> encourage saturated
models with many, small coefficients.
</p>
<p>The design matrix <code class="reqn">\textbf{X}</code> is converted to Fisher's
information matrix, which is used as a covariance matrix for
<code class="reqn">\beta</code>. This is computationally efficient, because each
element of the covariance matrix does not need to be estimated as a
parameter. When <code class="reqn">\textbf{X}</code> is nearly singular, regression
effects <code class="reqn">\beta</code> may be poorly estimated.
</p>
<p>Hyperparameter <code class="reqn">g</code> acts as an inverse relative prior sample size, or
as a dimensionality penalty. Zellner (1986) recommended that a
hyperprior distribution is assigned to <code class="reqn">g</code> so that it is estimated
from the data, although in practice <code class="reqn">g</code> has often been fixed, usually
to <code class="reqn">N</code> when no information is available, since it has the
interpretation of adding prior information equivalent to one
observation. A variety of hyperpriors have been suggested for <code class="reqn">g</code>,
such as in Bove and Held (2011), Liang et al. (2008), and Maruyama and
George (2011). <code class="reqn">g</code> becomes diffuse as it approaches infinity, and
the Bayes factor approaches zero. The hyper-g prior of Liang et al.
(2008) is proper when <code class="reqn">\alpha &gt; 2</code>, and any value in
the interval <code class="reqn">(2,4]</code> may be reasonable.
</p>


<h3>Value</h3>

<p><code>dhyperg</code> gives the density of the hyper-g prior of Liang et
al. (2008), <code>dzellner</code> gives the density of Zellner's g-prior,
and <code>rzellner</code> generates random deviates.
</p>


<h3>References</h3>

<p>Bove, D.S. and Held, L. (2011). &quot;Hyper-g Priors for Generalized
Linear Models&quot;. <em>Bayesian Analysis</em>, 6(3), p. 387&ndash;410.
</p>
<p>Krishna, A., Bondell, H.D., and Ghosh, S.K. (2009). &quot;Bayesian Variable
Selection Using an Adaptive Powered Correlation Prior&quot;. <em>Journal
of Statistical Planning Inference</em>, 139(8), p. 2665-2674..
</p>
<p>Liang, F., Paulo, R., Molina, G., Clyde, M.A., and Berger,
J.O. (2008). &quot;Mixtures of g Priors for Bayesian Variable
Selection&quot;. <em>Journal of the American Statistical Association</em>,
103, p. 410&ndash;423.
</p>
<p>Maruyama, Y. and George, E.I. (2011). &quot;Fully Bayes Factors with a
Generalised g-Prior&quot;. <em>Annals of Statistics</em>, 39, p. 2740&ndash;2765.
</p>
<p>Zellner, A. (1986). &quot;On Assessing Prior Distributions and Bayesian
Regression Analysis with g-Prior Distributions&quot;. In <em>Bayesian
Inference and Decision Techniques: Essays in Honor of Bruno de
Finetti</em>, p. 233&ndash;243. Elsevier: Amsterdam, North Holland.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+BayesFactor">BayesFactor</a></code> and
<code><a href="#topic+dmvt">dmvt</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
set.seed(667)
beta &lt;- rnorm(10)
g &lt;- 100
sigma &lt;- 2
X &lt;- cbind(1,matrix(rnorm(100*9),100,9))
dhyperg(g, alpha=3)
dzellner(beta, g, sigma, X)
rzellner(1, g, sigma, X)
</code></pre>

<hr>
<h2 id='Elicitation'>Prior Elicitation</h2><span id='topic+delicit'></span><span id='topic+elicit'></span>

<h3>Description</h3>

<p>Prior elicitation is the act of inducing personal opinion to be
expressed by the probabilities the person associates with an event
(Savage, 1971). The <code>elicit</code> function elicits personal opinion
and the <code>delicit</code> function estimates probability density to be
used with model specification in the
<code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code>, <code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>,
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>, <code><a href="#topic+LaplacesDemon.hpc">LaplacesDemon.hpc</a></code>,
<code><a href="#topic+PMC">PMC</a></code>, or <code><a href="#topic+VariationalBayes">VariationalBayes</a></code> functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>delicit(theta, x, a=-Inf, b=Inf, log=FALSE)
elicit(n, cats, cat.names, show.plot=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Elicitation_+3A_theta">theta</code></td>
<td>
<p>This is a scalar or vector of parameters for which the
density is estimated with respect to the kernel density estimate of
<code>x</code>.</p>
</td></tr>
<tr><td><code id="Elicitation_+3A_x">x</code></td>
<td>
<p>This is the elicited vector.</p>
</td></tr>
<tr><td><code id="Elicitation_+3A_a">a</code></td>
<td>
<p>This is an optional lower bound for support.</p>
</td></tr>
<tr><td><code id="Elicitation_+3A_b">b</code></td>
<td>
<p>This is an optional upper bound for support.</p>
</td></tr>
<tr><td><code id="Elicitation_+3A_log">log</code></td>
<td>
<p>Logical. If <code>log=TRUE</code>, then the logarithm of the
density is returned.</p>
</td></tr>
<tr><td><code id="Elicitation_+3A_n">n</code></td>
<td>
<p>This is the number of chips.</p>
</td></tr>
<tr><td><code id="Elicitation_+3A_cats">cats</code></td>
<td>
<p>This is a vector of <code class="reqn">k</code> categories, bins, or
intervals. When the variable is continuous, the mid-point of each
category is used. For example, if the continuous interval [0,1] has
5 equal-sized categories, then <code>cats=c(0.1,0.3,0.5,0.7,0.9)</code>.</p>
</td></tr>
<tr><td><code id="Elicitation_+3A_cat.names">cat.names</code></td>
<td>
<p>This is a vector of category names. For example, if
the continuous interval [0,1] has 5 equal-sized categories, then one
way or naming the categories may be <code>cat.names=c("0:&lt;.2",
      ".2:&lt;.4", ".4:&lt;.6", ".6:&lt;.8", ".8:1")</code>.</p>
</td></tr>
<tr><td><code id="Elicitation_+3A_show.plot">show.plot</code></td>
<td>
<p>Logical. If <code>show.plot=TRUE</code>, then a barplot is
shown after each allocation of chips.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>elicit</code> function elicits a univariate, discrete,
non-conjugate, informative, prior probability distribution by
offering a number of chips (specified as <code>n</code> by the statistician)
for the user to allocate into categories specified by the
statistician. The results of multiple elicitations (meaning, with
multiple people), each the output of <code>elicit</code>, may be combined
with the <code>c</code> function in base R.
</p>
<p>This discrete distribution is included with the data for
a model and supplied to a model specification function, where in turn
it is supplied to the <code>delicit</code> function, which estimates the
density at the current value of the prior distribution,
<code class="reqn">p(\theta)</code>. The prior distribution may be either
continuous or discrete, will be proper, and may have bounded support
(constrained to an interval).
</p>
<p>For a minimal example, a statistician elicits the prior probability
distribution for a regression effect, <code class="reqn">\beta</code>. Non-statisticians
would not be asked about expected parameters, but could be asked about
how much <code class="reqn">\textbf{y}</code> would be expected to change given a
one-unit change in <code class="reqn">\textbf{x}</code>. After consulting with others
who have prior knowledge, the support does not need to be bounded,
and their guesses at the range result in the statistician creating
5 catgories from the interval [-1,4], where each interval has a width
of one. The statistician schedules time with 3 people, and each person
participates when the statistician runs the following R code:
</p>
<p><code>x &lt;- elicit(n=10, cats=c(-0.5, 0.5, 1.5, 2.5, 3.5),
    cat.names=c("-1:&lt;0", "0:&lt;1", "1:&lt;2", "2:&lt;3", "3:4"), show.plot=TRUE)</code>
</p>
<p>Each of the 3 participants receives 10 chips to allocate among the
5 categories according to personal beliefs in the probability of the
regression effect. When the statistician and each participant accept
their elicited distribution, all 3 vectors are combined into one
vector. In the model form, the prior is expressed as
</p>
<p style="text-align: center;"><code class="reqn">p(\beta) \sim \mathcal{EL}</code>
</p>

<p>and the code for the model specification is
</p>
<p><code>elicit.prior &lt;- delicit(beta, x, log=TRUE)</code>
</p>
<p>This method is easily extended to priors that are multivariate,
correlated, or conditional.
</p>
<p>As an alternative, Hahn (2006) also used a categorical approach,
eliciting judgements about the relative likelihood of each category,
and then minimizes the KLD (for more information on KLD, see the
<code><a href="#topic+KLD">KLD</a></code> function).
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>References</h3>

<p>Hahn, E.D. (2006). &quot;Re-examining Informative Prior Elicitation Through
the Lens of Markov chain Monte Carlo Methods&quot;. <em>Journal of the
Royal Statistical Society</em>, A 169 (1), p. 37&ndash;48.
</p>
<p>Savage, L.J. (1971). &quot;Elicitation of Personal Probabilities and
Expectations&quot;. <em>Journal of the American Statistical Association</em>,
66(336), p. 783&ndash;801.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+de.Finetti.Game">de.Finetti.Game</a></code>,
<code><a href="#topic+KLD">KLD</a></code>,
<code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code>,
<code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>,
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>,
<code><a href="#topic+LaplacesDemon.hpc">LaplacesDemon.hpc</a></code>,
<code><a href="#topic+PMC">PMC</a></code>, and
<code><a href="#topic+VariationalBayes">VariationalBayes</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
x &lt;- c(1,2,2,3,3,3,4,7,8,8,9,10) #Elicited with elicit function
theta &lt;- seq(from=-5,to=15,by=.1)
plot(theta, delicit(theta,x), type="l", xlab=expression(theta),
     ylab=expression("p(" * theta * ")"))
</code></pre>

<hr>
<h2 id='ESS'>Effective Sample Size due to Autocorrelation</h2><span id='topic+ESS'></span>

<h3>Description</h3>

<p>This function may be used to estimate the effective sample size (ESS)
(not to be confused with Elliptical Slice Sampling) of a continuous
target distribution, where the sample size is reduced by
autocorrelation. ESS is a measure of how well each continuous chain is
mixing.
</p>
<p>ESS is a univariate function that is often applied to each continuous,
marginal posterior distribution. A multivariate form is not
included. By chance alone due to multiple independent tests, 5% of
the continuous parameters may indicate that ESS is below a user
threshold of acceptability, such as 100, even when above the
threshold. Assessing convergence is difficult.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ESS(x)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ESS_+3A_x">x</code></td>
<td>
<p>This required argument is a vector or matrix of posterior
samples.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Effective Sample Size (ESS) was recommended by Radford Neal in the
panel discussion of Kass et al. (1998). When a continuous, marginal
posterior distribution is sampled with a Markov chain Monte Carlo
(MCMC) algorithm, there is usually autocorrelation present in the
samples. More autocorrelation is associated with less posterior
sampled information, because the information in the samples is
autocorrelated, or put another way, successive samples are not
independent from earlier samples. This reduces the effective sample
size of, and precision in representing, the continuous, marginal
posterior distribution. <code>ESS</code> is one of the criteria in the
<code><a href="#topic+Consort">Consort</a></code> function, where stopping the MCMC updates is
not recommended until <code>ESS</code> <code class="reqn">\ge 100</code>. Although the need
for precision of each modeler differs with each model, it is often
a good goal to obtain <code>ESS</code> <code class="reqn">= 1000</code>.
</p>
<p><code>ESS</code> is related to the integrated autocorrelation time (see
<code><a href="#topic+IAT">IAT</a></code> for more information).
</p>
<p>ESS is usually defined as
</p>
<p style="text-align: center;"><code class="reqn">\mathrm{ESS}(\theta) = \frac{S}{1 + 2 \sum^{\infty}_{k=1} \rho_k
    (\theta)},</code>
</p>

<p>where <code class="reqn">S</code> is the number of posterior samples,
<code class="reqn">\rho_k</code> is the autocorrelation at lag <code class="reqn">k</code>, and
<code class="reqn">\theta</code> is the vector of marginal posterior samples. The
infinite sum is often truncated at lag <code class="reqn">k</code> when
<code class="reqn">\rho_k (\theta) &lt; 0.05</code>. Just as with the
<code>effectiveSize</code> function in the <code>coda</code> package, the
<code>AIC</code> argument in the <code>ar</code> function is used to estimate the
order.
</p>
<p>ESS is a measure of how well each continuous chain is mixing, and a
continuous chain mixes better when in the target distribution. This
does not imply that a poorly mixing chain still searching for its
target distribution will suddenly mix well after finding it, though
mixing should improve. A poorly mixing continuous chain does not
necessarily indicate problems. A smaller ESS is often due to
correlated parameters, and is commonly found with scale
parameters. Posterior correlation may be obtained from the
<code><a href="#topic+PosteriorChecks">PosteriorChecks</a></code> function, and plotted with the 
<code><a href="#topic+plotMatrix">plotMatrix</a></code> function. Common remedies for poor mixing
include re-parameterizing the model or trying a different MCMC
algorithm that better handles correlated parameters. Slow mixing is
indicative of an inefficiency in which a continuous chain takes longer
to find its target distribution, and once found, takes longer to
explore it. Therefore, slow mixing results in a longer required
run-time to find and adequately represent the continuous target
distribution, and increases the chance that the user may make
inferences from a less than adequate representation of the continuous
target distribution.
</p>
<p>There are many methods of re-parameterization to improve mixing. It
is helpful when predictors are centered and scaled, such as with the
<code><a href="#topic+CenterScale">CenterScale</a></code> function. Parameters for predictors are
often assigned prior distributions that are independent per parameter,
in which case an exchangeable prior distribution or a multivariate
prior distribution may help. If a parameter with poor mixing is
bounded with the <code><a href="#topic+interval">interval</a></code> function, then
transforming it to the real line (such as with a log transformation
for a scale parameter) is often helpful, since constraining a
parameter to an interval often reduces ESS. Another method is to
re-parameterize so that one or more latent variables represent the
process that results in slow mixing. Such re-parameterization uses
data augmentation.
</p>
<p>This is numerically the same as the <code>effectiveSize</code> function in
the <code>coda</code> package, but programmed to accept a simple vector or
matrix so it does not require an <code>mcmc</code> or <code>mcmc.list</code>
object, and the result is bound to be less than or equal to the
original number of samples.
</p>


<h3>Value</h3>

<p>A vector is returned, and each element is the effective sample size
(ESS) for a corresponding column of <code>x</code>, after autocorrelation has
been taken into account.
</p>


<h3>References</h3>

<p>Kass, R.E., Carlin, B.P., Gelman, A., and Neal, R. (1998). &quot;Markov
Chain Monte Carlo in Practice: A Roundtable Discussion&quot;. <em>The
American Statistician</em>, 52, p. 93&ndash;100. 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+CenterScale">CenterScale</a></code>,
<code><a href="#topic+Consort">Consort</a></code>,
<code><a href="#topic+IAT">IAT</a></code>,
<code><a href="#topic+interval">interval</a></code>,
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>,
<code><a href="#topic+plotMatrix">plotMatrix</a></code>, and
<code><a href="#topic+PosteriorChecks">PosteriorChecks</a></code>.
</p>

<hr>
<h2 id='Gelfand.Diagnostic'>Gelfand's Convergence Diagnostic</h2><span id='topic+Gelfand.Diagnostic'></span>

<h3>Description</h3>

<p>Gelfand et al. (1990) proposed a convergence diagnostic for Markov
chains. The <code>Gelfand.Diagnostic</code> function is an interpretation of
Gelfand's &ldquo;thick felt-tip pen&rdquo; MCMC convergence diagnostic. This
diagnostic plots a series of kernel density plots at <code class="reqn">k</code>
intervals of cumulative samples. Given a vector of <code class="reqn">S</code> samples
from a marginal posterior distribution, <code class="reqn">\theta</code>, multiple
kernel density lines are plotted together, where each includes samples
from a different interval. It is assumed that <code><a href="#topic+burnin">burnin</a></code>
iterations have been discarded.
</p>
<p>Gelfand et al. (1990) assert that convergence is violated when the
plotted lines are farther apart than the width of a thick, felt-tip
pen. This depends on the size of the plot, and, of course, the
pen. The estimated width of a &ldquo;thick felt-tip pen&rdquo; is included as a
black, vertical line. The pen in <code>Gelfand.Diagnostic</code> is included
for historical reasons. This diagnostic requires numerous samples.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Gelfand.Diagnostic(x, k=3, pen=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Gelfand.Diagnostic_+3A_x">x</code></td>
<td>
<p>This required argument is a vector of marginal posterior
samples, such as selected from the output of
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>.</p>
</td></tr>
<tr><td><code id="Gelfand.Diagnostic_+3A_k">k</code></td>
<td>
<p>This argument specifies the number <code class="reqn">k</code> of kernel
density plots given cumulative intervals of samples. This argument
defaults to <code class="reqn">k=3</code>.</p>
</td></tr>
<tr><td><code id="Gelfand.Diagnostic_+3A_pen">pen</code></td>
<td>
<p>Logical. This argument defaults to <code>pen=FALSE</code>. When
<code>pen=TRUE</code>, the thick felt-tip pen is included as a black,
vertical line.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>Gelfand.Diagnostic</code> returns a plot.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>References</h3>

<p>Gelfand, A.E., Hills, S., Racine-Poon, A., and Smith,
A.F.M. (1990). &quot;Illustration of Bayesian Inference in Normal Data
Models Using Gibbs Sampling&quot;. <em>Journal of the American
Statistical Association</em>, 85, p. 972&ndash;985.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+burnin">burnin</a></code> and
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
x &lt;- rnorm(1000)
Gelfand.Diagnostic(x)
</code></pre>

<hr>
<h2 id='Gelman.Diagnostic'>Gelman and Rubin's MCMC Convergence Diagnostic</h2><span id='topic+Gelman.Diagnostic'></span>

<h3>Description</h3>

<p>Gelman and Rubin (1992) proposed a general approach to monitoring
convergence of MCMC output in which <code class="reqn">m &gt; 1</code> parallel chains are
updated with initial values that are overdispersed relative to each
target distribution, which must be normally distributed. Convergence
is diagnosed when the chains have &lsquo;forgotten&rsquo; their initial values,
and the output from all chains is indistinguishable. The
<code>Gelman.Diagnostic</code> function makes a comparison of within-chain
and between-chain variances, and is similar to a classical analysis of
variance. A large deviation between these two variances indicates
non-convergence.
</p>
<p>This diagnostic is popular as a stopping rule, though it requires
parallel chains. The <code><a href="#topic+LaplacesDemon.hpc">LaplacesDemon.hpc</a></code> function is an
extension of <code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code> to enable parallel chains.
As an alternative, the popular single-chain stopping rule is based on
<code><a href="#topic+MCSE">MCSE</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Gelman.Diagnostic(x, confidence=0.95, transform=FALSE)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Gelman.Diagnostic_+3A_x">x</code></td>
<td>
<p>This required argument accepts an object of class
<code>demonoid.hpc</code>, or a list of multiple objects of class
<code>demonoid</code>, where the number of components in the list 
is the number of chains.</p>
</td></tr>
<tr><td><code id="Gelman.Diagnostic_+3A_confidence">confidence</code></td>
<td>
<p>This is the coverage probability of the confidence
interval for the potential scale reduction factor (PSRF).</p>
</td></tr>
<tr><td><code id="Gelman.Diagnostic_+3A_transform">transform</code></td>
<td>
<p>Logical. If <code>TRUE</code>, then marginal posterior
distributions in <code>x</code> may be transformed to improve the
normality of the distribution, which is assumed. A log-transform is
applied to marginal posterior distributions in the interval <code class="reqn">(0,
      \infty]</code>, or a logit-transform is applied to marginal
posterior distributions in the interval <code class="reqn">(0,1)</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>To use the <code>Gelman.Diagnostic</code> function, the user must first have
multiple MCMC chains for the same model, and three chains is usually
sufficient. The easiest way to obtain multiple chains is with the
<code><a href="#topic+LaplacesDemon.hpc">LaplacesDemon.hpc</a></code> function.
</p>
<p>Although the <code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code> function does not
simultaneously update multiple MCMC chains, it is easy enough to
obtain multiple chains, and if the computer has multiple processors
(which is common), then multiple chains may be obtained simultaneously
as follows. The model file may be opened in separate, concurrent R
sessions, and it is recommended that a maximum number of sessions is
equal to the number of processors, minus one. Each session constitutes
its own chain, and the code is identical, except the initial values
should be randomized with the <code><a href="#topic+GIV">GIV</a></code> function so the chains
begin in different places. The resulting object of class
<code>demonoid</code> for each chain is saved, all objects are read into one
session, put into a list, and passed to the <code>Gelman.Diagnostic</code>
function.
</p>
<p>Initial values must be overdispersed with respect to each target
distribution, though these distributions are unknown in the
beginning. Since the <code>Gelman.Diagnostic</code> function relies heavily
on overdispersion with respect to the target distribution, the user
should consider using MCMC twice, first to estimate the target
distributions, and secondly to overdisperse initial values with
respect to them. This may help identify multimodal target
distributions. If multiple modes are found, it remain possible that
more modes exist. When multiple modes are found, and if chains are
combined with the <code><a href="#topic+Combine">Combine</a></code> function, each mode is
probably not represented in a proportion correct to the distribution.
</p>
<p>The &lsquo;potential scale reduction factor&rsquo; (PSRF) is an estimated factor
by which the scale of the current distribution for the target
distribution might be reduced if the simulations were continued for
an infinite number of iterations. Each PSRF declines to 1 as the
number of iterations approaches infinity. PSRF is also often
represented as R-hat. PSRF is calculated for each marginal posterior
distribution in <code>x</code>, together with upper and lower confidence
limits. Approximate convergence is diagnosed when the upper limit is
close to 1. The recommended proximity of each PSRF to 1 varies with
each problem, but a general goal is to achieve PSRF &lt; 1.1. PSRF is an
estimate of how much narrower the posterior might become with an
infinite number of iterations. When PSRF = 1.1, for example, it may be
interpreted as a potential reduction of 10% in posterior interval
width, given infinite iterations. The multivariate form bounds above
the potential scale reduction factor for any linear combination of the
(possibly transformed) variables.
</p>
<p>The confidence limits are based on the assumption that the
target distribution is stationary and normally distributed. The
<code>transform</code> argument may be used to improve the normal
approximation.
</p>
<p>A large PSRF indicates that the between-chain variance is
substantially greater than the within-chain variance, so that longer
simulation is needed. If a PSRF is close to 1, then the associated
chains are likely to have converged to one target distribution. A
large PSRF (perhaps generally when a PSRF &gt; 1.2) indicates convergence
failure, and can indicate the presence of a multimodal marginal
posterior distribution in which different chains may have converged
to different local modes (see <code><a href="#topic+is.multimodal">is.multimodal</a></code>), or the
need to update the associated chains longer, because burn-in (see
<code><a href="#topic+burnin">burnin</a></code>) has yet to be completed.
</p>
<p>The <code>Gelman.Diagnostic</code> is essentially the same as the
<code>gelman.diag</code> function in the <code>coda</code> package, but here it is 
programmed to work with objects of class <code>demonoid</code>.
</p>
<p>There are two ways to estimate the variance of the stationary
distribution: the mean of the empirical variance within each chain,
<code class="reqn">W</code>, and the empirical variance from all chains combined, which
can be expressed as
</p>
<p style="text-align: center;"><code class="reqn"> \widehat{\sigma}^2 = \frac{(n-1) W}{n} +
    \frac{B}{n}</code>
</p>
 
<p>where <code class="reqn">n</code> is the number of iterations and <code class="reqn">B/n</code> is the
empirical between-chain variance.
</p>
<p>If the chains have converged, then both estimates are
unbiased. Otherwise the first method will <em>underestimate</em> the
variance, since the individual chains have not had time to range all
over the stationary distribution, and the second method will
<em>overestimate</em> the variance, since the initial values were
chosen to be overdispersed (and this assumes the target distribution
is known, see above).
</p>
<p>This convergence diagnostic is based on the assumption that each
target distribution is normal. A Bayesian probability interval (see
<code><a href="#topic+p.interval">p.interval</a></code>) can be constructed using a t-distribution
with mean
</p>
<p style="text-align: center;"><code class="reqn">\widehat{\mu}=\mbox{Sample mean of all chains combined,}</code>
</p>

<p>variance
</p>
<p style="text-align: center;"><code class="reqn">\widehat{V} = \widehat{\sigma}^2 + \frac{B}{mn},</code>
</p>

<p>and degrees of freedom estimated by the method of moments
</p>
<p style="text-align: center;"><code class="reqn">d = \frac{2\widehat{V}^2}{\mbox{Var}(\widehat{V})}</code>
</p>

<p>Use of the t-distribution accounts for the fact that the mean and
variance of the posterior distribution are estimated. The convergence
diagnostic itself is
</p>
<p style="text-align: center;"><code class="reqn">R=\sqrt{\frac{(d+3) \widehat{V}}{(d+1)W}}</code>
</p>

<p>Values substantially above 1 indicate lack of convergence. If the
chains have not converged, then Bayesian probability intervals based
on the t-distribution are too wide, and have the potential to shrink
by this factor if the MCMC run is continued.
</p>
<p>The multivariate version of Gelman and Rubin's diagnostic was proposed
by Brooks and Gelman (1998). Unlike the univariate proportional scale
reduction factor, the multivariate version does not include an
adjustment for the estimated number of degrees of freedom.
</p>


<h3>Value</h3>

<p>A list is returned with the following components:
</p>
<table>
<tr><td><code>PSRF</code></td>
<td>
<p>This is a list containing the point-estimates of the
potential scale reduction factor (labelled <code>Point Est.</code>) and
the associated upper confidence limits (labelled <code>Upper C.I.</code>).</p>
</td></tr>
<tr><td><code>MPSRF</code></td>
<td>
<p>This is the point-estimate of the multivariate potential
scale reduction factor.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Brooks, S.P. and Gelman, A. (1998). &quot;General Methods for Monitoring
Convergence of Iterative Simulations&quot;. <em>Journal of Computational
and Graphical Statistics</em>, 7, p. 434&ndash;455.
</p>
<p>Gelman, A. and Rubin, D.B. (1992). &quot;Inference from Iterative Simulation
using Multiple Sequences&quot;. <em>Statistical Science</em>, 7, p. 457&ndash;511.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Combine">Combine</a></code>,
<code><a href="#topic+GIV">GIV</a></code>,
<code><a href="#topic+is.multimodal">is.multimodal</a></code>,
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>,
<code><a href="#topic+LaplacesDemon.hpc">LaplacesDemon.hpc</a></code>,
<code><a href="#topic+MCSE">MCSE</a></code>, and
<code><a href="#topic+p.interval">p.interval</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#library(LaplacesDemon)
###After updating multiple chains with LaplacesDemon.hpc, do:
#Gelman.Diagnostic(Fit)
</code></pre>

<hr>
<h2 id='Geweke.Diagnostic'>Geweke's Convergence Diagnostic</h2><span id='topic+Geweke.Diagnostic'></span>

<h3>Description</h3>

<p>Geweke (1992) proposed a convergence diagnostic for Markov chains.
This diagnostic is based on a test for equality of the means of the
first and last part of a Markov chain (by default the first 10% and
the last 50%). If the samples are drawn from a stationary
distribution of the chain, then the two means are equal and Geweke's
statistic has an asymptotically standard normal distribution.
</p>
<p>The test statistic is a standard Z-score: the difference between the
two sample means divided by its estimated standard error. The
standard error is estimated from the spectral density at zero, and so
takes into account any autocorrelation.
</p>
<p>The Z-score is calculated under the assumption that the two parts of
the chain are asymptotically independent.
</p>
<p>The <code>Geweke.Diagnostic</code> is a univariate diagnostic that is
usually applied to each marginal posterior distribution. A
multivariate form is not included. By chance alone due to multiple
independent tests, 5% of the marginal posterior distributions should
appear non-stationary when stationarity exists. Assessing multivariate
convergence is difficult.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Geweke.Diagnostic(x)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Geweke.Diagnostic_+3A_x">x</code></td>
<td>
<p>This required argument is a vector or matrix of posterior
samples, such as from the output of the <code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>
function. Each column vector in a matrix is a chain to be assessed.
A minimum of 100 samples are required.</p>
</td></tr>
</table>


<h3>Details</h3>

  
<p>The <code>Geweke.Diagnostic</code> is essentially the same as the
<code>geweke.diag</code> function in the <code>coda</code> package, but
programmed to accept a simple vector or matrix, so it does not require
an <code>mcmc</code> object.
</p>


<h3>Value</h3>

<p>A vector is returned, in which each element is a Z-score for a test of
equality that compares the means of the first and last parts of each
chain supplied as <code>x</code> to <code>Geweke.Diagnostic</code>.
</p>


<h3>References</h3>

<p>Geweke, J. (1992). &quot;Evaluating the Accuracy of Sampling-Based
Approaches to Calculating Posterior Moments&quot;. In
<em>Bayesian Statistics 4</em> (ed JM Bernardo, JO Berger, AP Dawid,
and AFM Smith). Clarendon Press, Oxford, UK.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+burnin">burnin</a></code>,
<code><a href="#topic+is.stationary">is.stationary</a></code>, and 
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
Geweke.Diagnostic(rnorm(100))
Geweke.Diagnostic(matrix(rnorm(100),10,10))
</code></pre>

<hr>
<h2 id='GIV'>Generate Initial Values</h2><span id='topic+GIV'></span>

<h3>Description</h3>

<p>The <code>GIV</code> function generates initial values for use with the
<code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code>, <code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>,
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>, <code><a href="#topic+PMC">PMC</a></code>, and
<code><a href="#topic+VariationalBayes">VariationalBayes</a></code> functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>GIV(Model, Data, n=1000, PGF=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="GIV_+3A_model">Model</code></td>
<td>
<p>This required argument is a model specification
function. For more information, see <code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>.</p>
</td></tr>
<tr><td><code id="GIV_+3A_data">Data</code></td>
<td>
<p>This required argument is a list of data. For more
information, see <code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>.</p>
</td></tr>
<tr><td><code id="GIV_+3A_n">n</code></td>
<td>
<p>This is the number of attempts to generate acceptable
initial values.</p>
</td></tr>
<tr><td><code id="GIV_+3A_pgf">PGF</code></td>
<td>
<p>Logical. When <code>TRUE</code>, a Parameter-Generating Function
(PGF) is required to be in <code>Data</code>, and <code>GIV</code> will generate
initial values according to the user-specified PGF. This argument
defaults to <code>FALSE</code>, in which case initial values are generated
randomly without respect to a user-specified function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Initial values are required for optimization or sampling algorithms. A
user may specify initial values, or use the <code>GIV</code> function for
random generation. Initial values determined by the user may fail to
produce a finite posterior in complicated models, and the <code>GIV</code>
function is here to help.
</p>
<p><code>GIV</code> has several uses. First, the
<code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code>, <code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>,
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>, and <code><a href="#topic+VariationalBayes">VariationalBayes</a></code>
functions use <code>GIV</code> internally if unacceptable initial values are
discovered. Second, the user may use <code>GIV</code> when developing their
model specification function, <code>Model</code>, to check for potential
problems. Third, the user may prefer to randomly generate acceptable
initial values. Lastly, <code>GIV</code> is recommended when running
multiple or parallel chains with the <code><a href="#topic+LaplacesDemon.hpc">LaplacesDemon.hpc</a></code>
function (such as for later use with the <code>Gelman.Diagnostic</code>) for
dispersed starting locations. For dispersed starting locations,
<code>GIV</code> should be run once for each parallel chain, and the results
should be stored per row in a matrix of initial values. For more
information, see the <code>LaplacesDemon.hpc</code> documentation for
initial values.
</p>
<p>It is strongly recommended that the user specifies a
Parameter-Generating Function (PGF), and includes this function in the
list of data. Although the PGF may be specified according to the prior
distributions (possibly considered as a Prior-Generating Function), it
is often specified with a more restricted range. For example, if a
user has a model with the following prior distributions
</p>
<p style="text-align: center;"><code class="reqn">\beta_j \sim \mathcal{N}(0, 1000), j=1,\dots,5</code>
</p>

<p style="text-align: center;"><code class="reqn">\sigma \sim \mathcal{HC}(25)</code>
</p>

<p>then the PGF, given the prior distributions, is
</p>
<p><code>PGF &lt;- function(Data) return(c(rnormv(5,0,1000),rhalfcauchy(1,25)))</code>
</p>
<p>However, the user may not want to begin with initial values that could
be so far from zero (as determined by the variance of 1000), and may
instead prefer
</p>
<p><code>PGF &lt;- function(Data) return(c(rnormv(5,0,10),rhalfcauchy(1,5)))</code>
</p>
<p>When <code>PGF=FALSE</code>, initial values are attempted to be constrained
to the interval <code class="reqn">[-100,100]</code>. This is done to prevent numeric
overflows with parameters that are exponentiated within the model
specification function. First, <code>GIV</code> passes the upper and lower
bounds of this interval to the model, and any changed parameters are
noted.
</p>
<p>At this point, it is hoped that a non-finite posterior is not
found. If found, then the remainder of the process is random and
without the previous bounds. This can be particularly problematic in
the case of, say, initial values that are the elements of a matrix
that must be positive-definite, especially with large matrices. If a
random solution is not found, then <code>GIV</code> will fail.
</p>
<p>If the posterior is finite and <code>PGF=FALSE</code>, then initial values
are randomly generated with a normal proposal and a small variance at
the center of the returned range of each parameter. As <code>GIV</code>
fails to find acceptable initial values, the algorithm iterates toward
its maximum number of iterations, <code>n</code>. In each iteration, the
variance increases for the proposal.
</p>
<p>Initial values are considered acceptable only when the first two
returned components of <code>Model</code> (which are <code>LP</code> and
<code>Dev</code>) are finite, and when initial values do not change through
constraints, as returned in the fifth component of the list:
<code>parm</code>.
</p>
<p>If <code>GIV</code> fails to return acceptable initial values, then it is
best to study the model specification function. When the model is
complicated, here is a suggestion. Remove the log-likelihood,
<code>LL</code>, from the equation that calculates the logarithm of the
unnormalized joint posterior density, <code>LP</code>. For example, convert
<code>LP &lt;- LL + beta.prior</code> to <code>LP &lt;- beta.prior</code>. Now, maximize
<code>LP</code>, which is merely the set of prior densities, with any
optimization algorithm. Replace <code>LL</code>, and run the model with
initial values that are in regions of high prior density (preferably
with <code>PGF=TRUE</code>. If this fails, then the model specification
should be studied closely, because a non-finite posterior should
(especially) never be associated with regions of high prior density.
</p>


<h3>Value</h3>

<p>The <code>GIV</code> function returns a vector equal in length to the
number of parameters, and each element is an initial value for the
associated parameter in <code>Data$parm.names</code>. When <code>GIV</code> fails
to find acceptable initial values, each returned element is <code>NA</code>.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+as.initial.values">as.initial.values</a></code>,
<code><a href="#topic+Gelman.Diagnostic">Gelman.Diagnostic</a></code>,
<code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code>,
<code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>,
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>,
<code><a href="#topic+LaplacesDemon.hpc">LaplacesDemon.hpc</a></code>,
<code><a href="#topic+PMC">PMC</a></code>, and
<code><a href="#topic+VariationalBayes">VariationalBayes</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)

##############################  Demon Data  ###############################
data(demonsnacks)
y &lt;- log(demonsnacks$Calories)
X &lt;- cbind(1, as.matrix(log(demonsnacks[,c(1,4,10)]+1)))
J &lt;- ncol(X)
for (j in 2:J) X[,j] &lt;- CenterScale(X[,j])

#########################  Data List Preparation  #########################
mon.names &lt;- c("LP","sigma")
parm.names &lt;- as.parm.names(list(beta=rep(0,J), sigma=0))
pos.beta &lt;- grep("beta", parm.names)
pos.sigma &lt;- grep("sigma", parm.names)
PGF &lt;- function(Data) {
     beta &lt;- rnorm(Data$J)
     sigma &lt;- runif(1)
     return(c(beta, sigma))
     }
MyData &lt;- list(J=J, PGF=PGF, X=X, mon.names=mon.names,
     parm.names=parm.names, pos.beta=pos.beta, pos.sigma=pos.sigma, y=y)

##########################  Model Specification  ##########################
Model &lt;- function(parm, Data)
     {
     ### Parameters
     beta &lt;- parm[Data$pos.beta]
     sigma &lt;- interval(parm[Data$pos.sigma], 1e-100, Inf)
     parm[Data$pos.sigma] &lt;- sigma
     ### Log-Prior
     beta.prior &lt;- sum(dnormv(beta, 0, 1000, log=TRUE))
     sigma.prior &lt;- dhalfcauchy(sigma, 25, log=TRUE)
     ### Log-Likelihood
     mu &lt;- tcrossprod(Data$X, t(beta))
     LL &lt;- sum(dnorm(Data$y, mu, sigma, log=TRUE))
     ### Log-Posterior
     LP &lt;- LL + beta.prior + sigma.prior
     Modelout &lt;- list(LP=LP, Dev=-2*LL, Monitor=LP,
          yhat=rnorm(length(mu), mu, sigma), parm=parm)
     return(Modelout)
     }

########################  Generate Initial Values  ########################
Initial.Values &lt;- GIV(Model, MyData, PGF=TRUE)
</code></pre>

<hr>
<h2 id='Hangartner.Diagnostic'>Hangartner's Convergence Diagnostic</h2><span id='topic+Hangartner.Diagnostic'></span>

<h3>Description</h3>

<p>Hangartner et al. (2011) proposed a convergence diagnostic for
discrete Markov chains. A simple Pearson's Chi-squared test for
two or more non-overlapping periods of a discrete Markov chain
is a reliable diagnostic of convergence. It does not rely upon the
estimation of spectral density, on suspect normality assumptions, or
determining overdispersion within a small number of outcomes, all of
which can be problematic with discrete measures. A discrete Markov
chain is split into two or more non-overlapping windows. Two windows
are recommended, and results may be sensitive to the number of
selected windows, as well as sample size. As such, a user may try
several window configurations before concluding there is no evidence
of non-convergence.
</p>
<p>As the number of discrete events in the sample space increases, this
diagnostic becomes less appropriate and standard diagnostics become
more appropriate.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Hangartner.Diagnostic(x, J=2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Hangartner.Diagnostic_+3A_x">x</code></td>
<td>
<p>This required argument is a vector of marginal posterior
samples of a discrete Markov chain, such as selected from the output
of <code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>.</p>
</td></tr>
<tr><td><code id="Hangartner.Diagnostic_+3A_j">J</code></td>
<td>
<p>This argument specifies the number <code class="reqn">J</code> of windows to be
used, and defaults to <code class="reqn">J=2</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>Hangartner.Diagnostic</code> returns an object of class
<code>hangartner</code>, including the output from a Pearson's Chi-squared
test. A frequentist p-value less than or equal to 0.05 is usually
considered to be indicative of non-convergence.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>References</h3>

<p>Hangartner, D., Gill, J., and Cranmer, S., (2011). &quot;An MCMC Diagnostic
for Purely Discrete Parameters&quot;. Paper presented at the annual meeting
of the Southern Political Science Association, Hotel InterContinental,
New Orleans, Louisiana Online.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code> and
<code><a href="#topic+TransitionMatrix">TransitionMatrix</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
N &lt;- 1000
K &lt;- 3
x &lt;- rcat(N, rep(1/K,K))
hd &lt;- Hangartner.Diagnostic(x, J=2)
hd
</code></pre>

<hr>
<h2 id='Heidelberger.Diagnostic'>Heidelberger and Welch's MCMC Convergence Diagnostic</h2><span id='topic+Heidelberger.Diagnostic'></span>

<h3>Description</h3>

<p>Heidelberger and Welch (1981; 1983) proposed a two-part MCMC
convergence diagnostic that calculates a test statistic (based on the
Cramer-von Mises test statistic) to accept or reject the null
hypothesis that the Markov chain is from a stationary distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Heidelberger.Diagnostic(x, eps=0.1, pvalue=0.05)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Heidelberger.Diagnostic_+3A_x">x</code></td>
<td>
<p>This required argument accepts an object of class
<code>demonoid</code>. It attempts to use <code>Posterior2</code>, but when this
is missing it uses <code>Posterior1</code>.</p>
</td></tr>
<tr><td><code id="Heidelberger.Diagnostic_+3A_eps">eps</code></td>
<td>
<p>This argument specifies the target value for the ratio of
halfwidth to sample mean.</p>
</td></tr>
<tr><td><code id="Heidelberger.Diagnostic_+3A_pvalue">pvalue</code></td>
<td>
<p>This argument specifies the level of statistical
significance.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Heidelberg and Welch MCMC convergence diagnostic consists of two
parts:
</p>
<p>First Part
1. Generate a chain of <code class="reqn">N</code> iterations and define an alpha level.
2. Calculate the test statistic on the whole chain. Accept or reject
the null hypothesis that the chain is from a stationary distribution.
3. If the null hypothesis is rejected, then discard the first 10% of
the chain. Calculate the test statistic and accept or reject the null
hypothesis.
4. If the null hypothesis is rejected, then discard the next 10% and
calculate the test statistic.
5. Repeat until the null hypothesis is accepted or 50% of the chain
is discarded. If the test still rejects the null hypothesis, then
the chain fails the test and needs to be run longer.
</p>
<p>Second Part
If the chain passes the first part of the diagnostic, then the part of
the chain that was not discarded from the first part is used to test
the second part.
</p>
<p>The halfwidth test calculates half the width of the (1 - alpha)%
probability interval (credible interval) around the mean.
</p>
<p>If the ratio of the halfwidth and the mean is lower than <code>eps</code>,
then the chain passes the halfwidth test. Otherwise, the chain fails
the halfwidth test and must be updated for more iterations until
sufficient accuracy is obtained. In order to avoid problems caused by
sequential testing, the test should not be repeated too frequently.
Heidelberger and Welch (1981) suggest increasing the run length by a
factor I &gt; 1.5, each time, so that estimate has the same, reasonably
large, proportion of new data.
</p>
<p>The Heidelberger and Welch MCMC convergence diagnostic conducts
multiple hypothesis tests. The number of potentially wrong results
increases with the number of non-independent hypothesis tests
conducted.
</p>
<p>The <code>Heidelberger.Diagnostic</code> is a univariate diagnostic that is
usually applied to each marginal posterior distribution. A
multivariate form is not included. By chance alone due to multiple
independent tests, 5% of the marginal posterior distributions should
appear non-stationary when stationarity exists. Assessing multivariate
convergence is difficult.
</p>


<h3>Value</h3>

<p>The <code>Heidelberger.Diagnostic</code> function returns an object of class
<code>heidelberger</code>. This object is a <code class="reqn">J \times 6</code>
matrix, and it is intended to be summarized with the
<code><a href="#topic+print.heidelberger">print.heidelberger</a></code> function. Nonetheless, this object of
class <code>heidelberger</code> has <code class="reqn">J</code> rows, each of which corresponds
to a Markov chain. The column names are <code>stest</code>, <code>start</code>,
<code>pvalue</code>, <code>htest</code>, <code>mean</code>, and <code>halfwidth</code>. The
<code>stest</code> column indicates convergence with a one, and
non-convergence with a zero, regarding the stationarity test. When
non-convergence is indicated, the remaining columns have missing
values. The <code>start</code> column indicates the starting iteration, and
the <code>pvalue</code> column shows the p-value associated with the first
test. The <code>htest</code> column indicates convergence for the halfwidth
test. The <code>mean</code> and <code>halfwidth</code> columns report the mean and
halfwidth.
</p>


<h3>Note</h3>

<p>The <code>Heidelberger.Diagnostic</code> function was adapted from the
<code>heidel.diag</code> function in the coda package.
</p>


<h3>References</h3>

<p>Heidelberger, P. and Welch, P.D. (1981). &quot;A Spectral Method for
Confidence Interval Generation and Run Length Control in Simulations&quot;.
<em>Comm. ACM.</em>, 24, p. 233&ndash;245.
</p>
<p>Heidelberger, P. and Welch, P.D. (1983). &quot;Simulation Run Length Control
in the Presence of an Initial Transient&quot;. <em>Opns Res.</em>, 31,
p. 1109&ndash;1144.
</p>
<p>Schruben, L.W. (1982). &quot;Detecting Initialization Bias in Simulation
Experiments&quot;. <em>Opns. Res.</em>, 30, p. 569&ndash;590.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+burnin">burnin</a></code>,
<code><a href="#topic+is.stationary">is.stationary</a></code>,
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>, and
<code><a href="#topic+print.heidelberger">print.heidelberger</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#library(LaplacesDemon)
###After updating with LaplacesDemon, do:
#hd &lt;- Heidelberger.Diagnostic(Fit)
#print(hd)
</code></pre>

<hr>
<h2 id='hpc_server'>Server Listening</h2><span id='topic+server_Listening'></span>

<h3>Description</h3>

<p>This function is not intended to be called directly by the user. It is
an internal-only function to prevent cluster problems while using the
<code>INCA</code> algorithm in the <code>LaplacesDemon.hpc</code> function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>server_Listening(n=2, port=19009)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hpc_server_+3A_n">n</code></td>
<td>
<p>This is the number of CPUs. For more information, see
<code><a href="#topic+LaplacesDemon.hpc">LaplacesDemon.hpc</a></code>.</p>
</td></tr>
<tr><td><code id="hpc_server_+3A_port">port</code></td>
<td>
<p>This is a port for server listening, and defaults to
port <code>19009</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For the <code>INCA</code> algorithm, a server has been built into the
<code>LaplacesDemon.hpc</code> function. The server exchanges information
between processes, and has been designed to be portable. The
<code>server_Listening</code> function is run as a separate process via the
<code>system</code> function, when <code>INCA</code> is selected in
<code>LaplacesDemon.hpc</code>.
</p>
<p>Socket connections and the <code>serialize</code> function are used as per
the <span class="pkg">Snow</span> package to update a single proposal covariance matrix
given all parallel chains. The sockets are opened/closed in each
process with a small random sleep time to avoid collisions during
connections to the internal server of
<code>LaplacesDemon.hpc</code>. Blocking sockets are used to synchronize
processes.
</p>


<h3>Author(s)</h3>

<p>Silvere Vialet-Chabrand <a href="mailto:silvere@vialet-chabrand.com">silvere@vialet-chabrand.com</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code> and
<code><a href="#topic+LaplacesDemon.hpc">LaplacesDemon.hpc</a></code>.
</p>

<hr>
<h2 id='IAT'>Integrated Autocorrelation Time</h2><span id='topic+IAT'></span>

<h3>Description</h3>

<p>The <code>IAT</code> function estimates integrated autocorrelation time,
which is the computational inefficiency of a continuous chain or MCMC
sampler. IAT is also called the IACT, ACT, autocorrelation time,
autocovariance time, correlation time, or inefficiency factor. A lower
value of <code>IAT</code> is better. <code>IAT</code> is a MCMC diagnostic that is
an estimate of the number of iterations, on average, for an
independent sample to be drawn, given a continuous chain or Markov
chain. Put another way, <code>IAT</code> is the number of correlated samples
with the same variance-reducing power as one independent sample.
</p>
<p>IAT is a univariate function. A multivariate form is not included.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>IAT(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="IAT_+3A_x">x</code></td>
<td>
<p>This requried argument is a vector of samples from a chain.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>IAT</code> is a MCMC diagnostic that is often used to compare
continuous chains of MCMC samplers for computational inefficiency,
where the sampler with the lowest <code>IAT</code>s is the most efficient
sampler. Otherwise, chains may be compared within a model, such as
with the output of <code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code> to learn about the
inefficiency of the continuous chain. For more information on
comparing MCMC algorithmic inefficiency, see the
<code><a href="#topic+Juxtapose">Juxtapose</a></code> function.
</p>
<p><code>IAT</code> is also estimated in the <code><a href="#topic+PosteriorChecks">PosteriorChecks</a></code>
function. <code>IAT</code> is usually applied to a stationary, continuous
chain after discarding burn-in iterations (see <code><a href="#topic+burnin">burnin</a></code>
for more information). The <code>IAT</code> of a continuous chain correlates
with the variability of the mean of the chain, and relates to
Effective Sample Size (<code><a href="#topic+ESS">ESS</a></code>) and Monte Carlo Standard
Error (<code><a href="#topic+MCSE">MCSE</a></code>).
</p>
<p><code>IAT</code> and <code><a href="#topic+ESS">ESS</a></code> are inversely related, though not
perfectly, because each is estimated a little differently. Given
<code class="reqn">N</code> samples and taking autocorrelation into account,
<code><a href="#topic+ESS">ESS</a></code> estimates a reduced number of <code class="reqn">M</code> samples.
Conversely, <code>IAT</code> estimates the number of autocorrelated samples,
on average, required to produce one independently drawn sample.
</p>
<p>The <code>IAT</code> function is similar to the <code>IAT</code> function in the
<code>Rtwalk</code> package of Christen and Fox (2010), which is currently
unavailabe on CRAN.
</p>


<h3>Value</h3>

<p>The <code>IAT</code> function returns the integrated autocorrelation time of
a chain.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>References</h3>

<p>Christen, J.A. and Fox, C. (2010). &quot;A General Purpose Sampling
Algorithm for Continuous Distributions (the t-walk)&quot;. <em>Bayesian
Analysis</em>, 5(2), p. 263&ndash;282.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+burnin">burnin</a></code>,
<code><a href="methods.html#topic+Compare">Compare</a></code>,
<code><a href="#topic+ESS">ESS</a></code>,
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>,
<code><a href="#topic+MCSE">MCSE</a></code>, and
<code><a href="#topic+PosteriorChecks">PosteriorChecks</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
theta &lt;- rnorm(100)
IAT(theta)
</code></pre>

<hr>
<h2 id='Importance'>Variable Importance</h2><span id='topic+Importance'></span>

<h3>Description</h3>

<p>The <code>Importance</code> function considers variable importance (or
predictor importance) to be the effect that the variable has on
replicates <code class="reqn">\textbf{y}^{rep}</code> (or
<code class="reqn">\textbf{Y}^{rep}</code>) when the variable is removed from the
model by setting it equal to zero. Here, variable importance is
considered in terms of the comparison of posterior predictive
checks. This may be considered to be a form of sensitivity analysis,
and can be useful for model revision, variable selection, and model
interpretation.
</p>
<p>Currently, this function only tests the variable importance of design
matrix <code class="reqn">\textbf{X}</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Importance(object, Model, Data, Categorical=FALSE, Discrep, d=0, CPUs=1,
  Type="PSOCK")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Importance_+3A_object">object</code></td>
<td>
<p>An object of class <code>demonoid</code>, <code>iterquad</code>,
<code>laplace</code>, <code>pmc</code>, or <code>vb</code> is required.</p>
</td></tr>
<tr><td><code id="Importance_+3A_model">Model</code></td>
<td>
<p>The model specification function is required.</p>
</td></tr>
<tr><td><code id="Importance_+3A_data">Data</code></td>
<td>
<p>A data set in a list is required. The dependent variable
is required to be named either <code>y</code> or <code>Y</code>. The
<code>Importance</code> function will sequentially remove each column
vector in <code>X</code>, so <code>X</code> is required to be in data set
<code>Data</code>.</p>
</td></tr>
<tr><td><code id="Importance_+3A_categorical">Categorical</code></td>
<td>
<p>Logical. If <code>TRUE</code>, then <code>y</code> and
<code>yhat</code> are considered to be categorical (such as y=0 or y=1),
rather than continuous. This defaults to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="Importance_+3A_discrep">Discrep</code></td>
<td>
<p>This optional argument allows a discrepancy statistic to
be included. For more information on discrepancy statistics, see
<code><a href="#topic+summary.demonoid.ppc">summary.demonoid.ppc</a></code>.</p>
</td></tr>
<tr><td><code id="Importance_+3A_d">d</code></td>
<td>
<p>This is an optional integer to be used with the
<code>Discrep</code> argument above, and it defaults to <code>d=0</code>. For
more information on discrepancy, see
<code><a href="#topic+summary.demonoid.ppc">summary.demonoid.ppc</a></code>.</p>
</td></tr>
<tr><td><code id="Importance_+3A_cpus">CPUs</code></td>
<td>
<p>This argument accepts an integer that specifies the number
of central processing units (CPUs) of the multicore computer or
computer cluster. This argument defaults to <code>CPUs=1</code>, in which
parallel processing does not occur.</p>
</td></tr>
<tr><td><code id="Importance_+3A_type">Type</code></td>
<td>
<p>This argument specifies the type of parallel processing to
perform, accepting either <code>Type="PSOCK"</code> or
<code>Type="MPI"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Variable importance is defined here as the impact of each
variable (predictor, or column vector) in design matrix
<code class="reqn">\textbf{X}</code> on <code class="reqn">\textbf{y}^{rep}</code> (or
<code class="reqn">\textbf{Y}^{rep}</code>), when the variable is removed.
</p>
<p>First, the full model is predicted with the
<code><a href="#topic+predict.demonoid">predict.demonoid</a></code>, <code><a href="#topic+predict.iterquad">predict.iterquad</a></code>,
<code><a href="#topic+predict.laplace">predict.laplace</a></code>, <code><a href="#topic+predict.pmc">predict.pmc</a></code>, or
<code><a href="#topic+predict.vb">predict.vb</a></code> function, and summarized with the
<code><a href="#topic+summary.demonoid.ppc">summary.demonoid.ppc</a></code>,
<code><a href="#topic+summary.iterquad.ppc">summary.iterquad.ppc</a></code>, <code><a href="#topic+summary.laplace.ppc">summary.laplace.ppc</a></code>,
<code><a href="#topic+summary.pmc.ppc">summary.pmc.ppc</a></code>, or <code><a href="#topic+summary.vb.ppc">summary.vb.ppc</a></code>
function, respectively. The results are stored in the first row of the
output. Each successive row in the output corresponds to the
application of <code>predict</code> and <code>summary</code> functions, but with
each variable in design matrix <code class="reqn">\textbf{X}</code> being set to zero
and effectively removed. The results show the impact of sequentially
removing each predictor.
</p>
<p>The criterion for variable importance may differ from model to
model. As a default, BPIC is recommended. The Bayesian Predictive
Information Criterion (BPIC) was introduced by Ando (2007). BPIC is a
variation of the Deviance Information Criterion (DIC) that has been
modified for predictive distributions. For more information on DIC
(Spiegelhalter et al., 2002), see the accompanying vignette entitled
&quot;Bayesian Inference&quot;. <code class="reqn">BPIC = Dbar + 2pD</code>.
</p>
<p>With BPIC, variable importance has a positive relationship, such that
larger values indicate a more important variable, because removing
that variable resulted in a worse fit to the data. The best model
has the lowest BPIC.
</p>
<p>In a model in which the dependent variable is not categorical, it is
also recommended to consider the L-criterion (Laud and Ibrahim, 1995),
provided that sample size is small enough that it does not result in
<code>Inf</code>. For more information on the L-criterion, see the
accompanying vignette entitled &quot;Bayesian Inference&quot;.
</p>
<p>With the L-criterion, variable importance has a positive relationship, 
such that larger values indicate a more important variable, because
removing that variable resulted in a worse fit to the data. Ibrahim
(1995) recommended considering the model with the lowest
L-criterion, say as <code class="reqn">L_1</code>, and the model with the closest
L-criterion, say as <code class="reqn">L_2</code>, and creating a comparison score
as <code class="reqn">\phi = (L_2-L_1)/S_L</code>, where
<code>S.L</code> is from the <code class="reqn">L_1</code> model. If the comparison score,
<code class="reqn">\phi</code> is less than 2, then <code class="reqn">L_2</code> is within 2
standard deviations of <code class="reqn">L_1</code>, and is the recommended cut-off
for model choice.
</p>
<p>The <code>Importance</code> function may suggest that a model fits the data
better with a variable removed. In which case, the user may
choose to leave the variable in the model (perhaps the model is
misspecified without the variable), investigate and possibly
re-specify the relationship between the independent and dependent
variable(s), or remove the variable and update the model again.
</p>
<p>In contrast to variable importance, the <code><a href="#topic+PosteriorChecks">PosteriorChecks</a></code>
function calculates parameter importance, which is the probability
that each parameter's marginal posterior distribution is greater than
zero, where an important parameter does not include zero in its
probability interval (see <code><a href="#topic+p.interval">p.interval</a></code>). Parameter
importance and variable importance may disagree, and both should be
studied.
</p>
<p>The <code>Importance</code> function tends to indicate that a model fits the
data better when variables are removed that have parameters with
marginal posterior distributions that include 0 in the 95%
probability interval (variables associated with lower parameter
importance).
</p>
<p>Often, in complicated models, it is difficult to assess variable
importance by examining the marginal posterior distribution of the
associated parameter(s). Consider polynomial regression, in which each
variable may have multiple parameters.
</p>
<p>The information provided by the <code>Importance</code> function may be used
for model revision, or reporting the relative importance of variables.
</p>
<p>The <code><a href="#topic+plot.importance">plot.importance</a></code> function is available to plot the
output of the <code>Importance</code> function according to BPIC, predictive
concordance (Gelfand, 1996), the selected discrepancy statistic
(Gelman et al., 1996), or the L-criterion.
</p>
<p>Parallel processing may be performed when the user specifies
<code>CPUs</code> to be greater than one, implying that the specified number
of CPUs exists and is available. Parallelization may be performed on a
multicore computer or a computer cluster. Either a Simple Network of
Workstations (SNOW) or Message Passing Interface is used (MPI). With
small data sets and few samples, parallel processing may be slower,
due to computer network communication. With larger data sets and more
samples, the user should experience a faster run-time.
</p>


<h3>Value</h3>

<p><code>Importance</code> returns an object of class <code>importance</code>, which
is a matrix with a number of rows equal to the number of columns in
design matrix <code class="reqn">\textbf{X}</code> + 1 (including the full model), and
4 columns, which are BPIC, Concordance (or Mean.Lift if categorical),
Discrep, and L-criterion. Each row represents a model with a predictor
in <code class="reqn">\textbf{X}</code> removed (except for the first row, which is the
full model), and the resulting posterior predictive checks. For
non-categorical dependent variables, an attribute is returned with the
object, and the attribute is a vector of <code>S.L</code>, the calibration
number of the L-criterion.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>References</h3>

<p>Ando, T. (2007). &quot;Bayesian Predictive Information Criterion for
the Evaluation of Hierarchical Bayesian and Empirical Bayes Models&quot;.
<em>Biometrika</em>, 94(2), p. 443&ndash;458.
</p>
<p>Gelfand, A. (1996). &quot;Model Determination Using Sampling Based
Methods&quot;. In Gilks, W., Richardson, S., Spiegehalter, D., Chapter 9 in
Markov Chain Monte Carlo in Practice. Chapman and Hall: Boca Raton, FL.
</p>
<p>Laud, P.W. and Ibrahim, J.G. (1995). &quot;Predictive Model
Selection&quot;. <em>Journal of the Royal Statistical Society</em>, B 57,
p. 247&ndash;262.
</p>
<p>Spiegelhalter, D.J., Best, N.G., Carlin, B.P., and van der Linde, A.
(2002). &quot;Bayesian Measures of Model Complexity and Fit (with
Discussion)&quot;. <em>Journal of the Royal Statistical Society</em>, B 64,
p. 583&ndash;639.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+is.importance">is.importance</a></code>,
<code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code>,
<code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>,
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>,
<code><a href="#topic+PMC">PMC</a></code>,
<code><a href="#topic+plot.importance">plot.importance</a></code>,
<code><a href="#topic+PosteriorChecks">PosteriorChecks</a></code>,
<code><a href="#topic+p.interval">p.interval</a></code>,
<code><a href="#topic+predict.demonoid">predict.demonoid</a></code>,
<code><a href="#topic+predict.iterquad">predict.iterquad</a></code>,
<code><a href="#topic+predict.laplace">predict.laplace</a></code>,
<code><a href="#topic+predict.pmc">predict.pmc</a></code>,
<code><a href="#topic+predict.vb">predict.vb</a></code>,
<code><a href="#topic+summary.demonoid.ppc">summary.demonoid.ppc</a></code>,
<code><a href="#topic+summary.iterquad.ppc">summary.iterquad.ppc</a></code>,
<code><a href="#topic+summary.laplace.ppc">summary.laplace.ppc</a></code>,
<code><a href="#topic+summary.pmc.ppc">summary.pmc.ppc</a></code>,
<code><a href="#topic+summary.vb.ppc">summary.vb.ppc</a></code>, and
<code><a href="#topic+VariationalBayes">VariationalBayes</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#First, update the model with the LaplacesDemon function, such as
#the example with linear regression, creating an object called Fit.
#Then
#Importance(Fit, Model, MyData, Discrep="Chi-Square", CPUs=1)
</code></pre>

<hr>
<h2 id='interval'>Constrain to Interval</h2><span id='topic+interval'></span>

<h3>Description</h3>

<p>This function constrains the value(s) of a scalar, vector, matrix, or
array to a specified interval, <code class="reqn">[a,b]</code>. In Bayesian inference, it
is often used both to truncate a parameter to an interval, such as
<code class="reqn">p(\theta) \in [a,b]</code>. The <code>interval</code>
function is often used in conjunction with the <code><a href="#topic+dtrunc">dtrunc</a></code> 
function to truncate the prior probability distribution associated
with the constrained parameter. While <code><a href="#topic+dtrunc">dtrunc</a></code> prevents
assigning density outside of its interval and re-estimates density
within the interval, the <code>interval</code> function is used to prevent
the parameter from moving outside of the interval in the first place.
</p>
<p>After the parameter is constrained to an interval in
<code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code>, <code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>,
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>, <code><a href="#topic+PMC">PMC</a></code>, or
<code><a href="#topic+VariationalBayes">VariationalBayes</a></code>, the constrained parameter should be
updated back into the <code>parm</code> vector, so the algorithm knows it
has been constrained.
</p>
<p>This is unrelated to the probability interval (see
<code><a href="#topic+p.interval">p.interval</a></code> and <code><a href="#topic+LPL.interval">LPL.interval</a></code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>interval(x, a=-Inf, b=Inf, reflect=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="interval_+3A_x">x</code></td>
<td>
<p>This required argument is a scalar, vector, matrix or array,
and its elements will be constrained to the interval
[<code>a</code>,<code>b</code>].</p>
</td></tr>
<tr><td><code id="interval_+3A_a">a</code></td>
<td>
<p>This optional argument allows the specification of the lower
bound of the interval, and defaults to <code>-Inf</code>.</p>
</td></tr>
<tr><td><code id="interval_+3A_b">b</code></td>
<td>
<p>This optional argument allows the specification of the upper
bound of the interval, and defaults to <code>Inf</code>.</p>
</td></tr>
<tr><td><code id="interval_+3A_reflect">reflect</code></td>
<td>
<p>Logical. When <code>TRUE</code>, a value outside of the
constrained interval is reflected or bounced back into the
interval. When <code>FALSE</code>, a value outside of the interval is
assigned the nearest boundary of the interval. This argument defaults
to <code>TRUE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>It is common for a parameter to be constrained to an interval. The
<code>interval</code> function provides two methods of constraining
proposals. The default is to reflect an out-of-bounds proposal off of
the boundaries until the proposal is within the specified
interval. This is rare in the literature but works very well in
practice. The other method does not reflect off of boundaries, but
sets the value equal to the violated boundary. This is also rare in
the literature and is not generally recommended.
</p>
<p>If the <code>interval</code> function is unacceptable, then there are
several alternatives.
</p>
<p>It is common to re-parameterize by transforming the constrained
parameter to the real line. For example, a positive-only scale
parameter may be log-transformed. A parameter that is re-parameterized
to the real line often mixes better in MCMC, exhibiting a higher
effective sample size (<code><a href="#topic+ESS">ESS</a></code>), and each evaluation of the
model specification function is faster as well. However, without a
hard constraint, it remains possible for the transformed parameter
still become problematic, such as a log-transformed scale parameter
that reaches negative infinity. This is much more common in the
literature.
</p>
<p>Another method is to allow the parameters to move outside of the
desired, constrained interval in MCMC during the model update, and
when the model update is finished, to discard any samples outside of
the constraint boundaries. This is a method of rejecting unacceptable
proposals in regions of zero probability. However, it is possible for
parameters to remain outside of acceptable bounds long enough to be
problematic.
</p>
<p>In <code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>, the Gibbs sampler allows more control
in the FC function, where a user can customize how constraints are
handled.
</p>


<h3>Value</h3>

<p>The <code>interval</code> function returns a scalar, vector, matrix, or
array in accord with its argument, <code>x</code>. Each element is
constrained to the interval [<code>a</code>,<code>b</code>].
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+dtrunc">dtrunc</a></code>,
<code><a href="#topic+ESS">ESS</a></code>,
<code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code>,
<code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>,
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>,
<code><a href="#topic+LPL.interval">LPL.interval</a></code>,
<code><a href="#topic+PMC">PMC</a></code>,
<code><a href="#topic+p.interval">p.interval</a></code>,
<code><a href="#topic+VariationalBayes">VariationalBayes</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#See the Examples vignette for numerous examples.
library(LaplacesDemon)
x &lt;- 2
interval(x,0,1)
X &lt;- matrix(runif(25,-2,2),5,5)
interval(X,-1,1)
</code></pre>

<hr>
<h2 id='is.appeased'>Appeased</h2><span id='topic+is.appeased'></span>

<h3>Description</h3>

<p>This function returns <code>TRUE</code> if Laplace's Demon is appeased by the
object of class <code>demonoid</code>, and <code>FALSE</code> otherwise. If
appeased, then the object passes several tests that indicate potential
convergence of the Markov chains.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.appeased(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="is.appeased_+3A_x">x</code></td>
<td>
<p>This is an object of class <code>demonoid</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>After updating a model with the <code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code> function,
an output object is created. The output object is of class
<code>demonoid</code>. The object may be passed to the <code><a href="#topic+Consort">Consort</a></code>
function, which will apply several criteria regarding the potential
convergence of its Markov chains. If all criteria are met, then
Laplace's Demon is appeased. Otherwise, Laplace's Demon suggests R
code to be copy/pasted and executed. The <code><a href="#topic+Consort">Consort</a></code>
function prints a large amount of information to the screen. The
<code>is.appeased</code> function may be applied as an alternative, though
it only informs the user as to whether or not Laplace's Demon was
appeased, as <code>TRUE</code> or <code>FALSE</code>.
</p>


<h3>Value</h3>

<p>The <code>is.appeased</code> function returns a logical value indicating
whether or not the supplied object passes several potential Markov
chain convergence criteria. If the object passes all criteria, then
Laplace's Demon is appeased, and the logical value returned is
<code>TRUE</code>.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+Consort">Consort</a></code> and
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>.
</p>

<hr>
<h2 id='is.bayesian'>Logical Check of a Bayesian Model</h2><span id='topic+is.bayesian'></span>

<h3>Description</h3>

<p>This function provides a logical test of whether or not a <code>Model</code>
specification function is Bayesian.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.bayesian(Model, Initial.Values, Data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="is.bayesian_+3A_model">Model</code></td>
<td>
<p>This is a model specification function. For more
information, see the <code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code> function.</p>
</td></tr>
<tr><td><code id="is.bayesian_+3A_initial.values">Initial.Values</code></td>
<td>
<p>This is a vector of initial values, or current
parameter values. For more information, see the
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code> function.</p>
</td></tr>
<tr><td><code id="is.bayesian_+3A_data">Data</code></td>
<td>
<p>This is a list of data. For more information, see the
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code> function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function tests whether or not a model is Bayesian by comparing
the first two returned arguments: the logarithm of the unnormalized
joint posterior density (<code>LP</code>) and deviance (<code>Dev</code>). The
deviance (D) is
</p>
<p style="text-align: center;"><code class="reqn">\mathrm{D} = -2 \mathrm{LL}</code>
</p>
<p>,
</p>
<p>where LL is the log-likelihood. Consequently,
</p>
<p style="text-align: center;"><code class="reqn">\mathrm{LL} = \mathrm{D} / -2</code>
</p>
<p>,
</p>
<p>and LP is the sum of LL and prior probability densities. If LP = LL,
then the model is not Bayesian, because prior densities are absent.
</p>


<h3>Value</h3>

<p>The <code>is.bayesian</code> function returns a logical value of <code>TRUE</code>
when the model is Bayesian, and <code>FALSE</code> otherwise.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>.
</p>

<hr>
<h2 id='is.class'>Logical Check of Classes</h2><span id='topic+is.bayesfactor'></span><span id='topic+is.blocks'></span><span id='topic+is.bmk'></span><span id='topic+is.demonoid'></span><span id='topic+is.demonoid.hpc'></span><span id='topic+is.demonoid.ppc'></span><span id='topic+is.demonoid.val'></span><span id='topic+is.hangartner'></span><span id='topic+is.heidelberger'></span><span id='topic+is.importance'></span><span id='topic+is.iterquad'></span><span id='topic+is.iterquad.ppc'></span><span id='topic+is.juxtapose'></span><span id='topic+is.laplace'></span><span id='topic+is.laplace.ppc'></span><span id='topic+is.miss'></span><span id='topic+is.pmc'></span><span id='topic+is.pmc.ppc'></span><span id='topic+is.pmc.val'></span><span id='topic+is.posteriorchecks'></span><span id='topic+is.raftery'></span><span id='topic+is.rejection'></span><span id='topic+is.sensitivity'></span><span id='topic+is.vb'></span><span id='topic+is.vb.ppc'></span>

<h3>Description</h3>

<p>These functions each provide a logical test of the class of an object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.bayesfactor(x)
is.blocks(x)
is.bmk(x)
is.demonoid(x)
is.demonoid.hpc(x)
is.demonoid.ppc(x)
is.demonoid.val(x)
is.hangartner(x)
is.heidelberger(x)
is.importance(x)
is.iterquad(x)
is.iterquad.ppc(x)
is.juxtapose(x)
is.laplace(x)
is.laplace.ppc(x)
is.miss(x)
is.pmc(x)
is.pmc.ppc(x)
is.pmc.val(x)
is.posteriorchecks(x)
is.raftery(x)
is.rejection(x)
is.sensitivity(x)
is.vb(x)
is.vb.ppc(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="is.class_+3A_x">x</code></td>
<td>
<p>This is an object that will be subjected to a logical test of
its class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Functions in Laplace's Demon often assigns a class to an output
object. For example, after updating a model with the
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code> or <code><a href="#topic+LaplacesDemon.hpc">LaplacesDemon.hpc</a></code>
function, an output object is created. The output object is of class
<code>demonoid</code> or <code>demonoid.hpc</code>, respectively. Likewise, after
passing a model to the <code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code> function, an
output object is created, and it is of class <code>laplace</code>. The class
of these and other objects may be logically tested.
</p>
<p>By assigning a class to an output object, the package is able to
discern which other functions are appropriate for it. For example,
after updating a model with <code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>, which creates
an object of class <code>demonoid</code>, the user may desire to plot its
output. Since it is assigned a class, the user may use the generic
<code>plot</code> function, which internally selects the
<code><a href="#topic+plot.demonoid">plot.demonoid</a></code> function, which differs from
<code><a href="#topic+plot.laplace">plot.laplace</a></code> for objects of class <code>laplace</code>.
</p>
<p>For more information on object classes, see the <code><a href="base.html#topic+class">class</a></code>
function.
</p>


<h3>Value</h3>

<p>The <code>is.bayesfactor</code> function returns a logical value indicating
whether or not the supplied object is of class <code>bayesfactor</code>.
</p>
<p>The <code>is.blocks</code> function returns a logical value indicating
whether or not the supplied object is of class <code>blocks</code>.
</p>
<p>The <code>is.bmk</code> function returns a logical value indicating
whether or not the supplied object is of class <code>bmk</code>.
</p>
<p>The <code>is.demonoid</code> function returns a logical value indicating
whether or not the supplied object is of class <code>demonoid</code>.
</p>
<p>The <code>is.demonoid.hpc</code> function returns a logical value indicating
whether or not the supplied object is of class <code>demonoid.hpc</code>.
</p>
<p>The <code>is.demonoid.ppc</code> function returns a logical value indicating
whether or not the supplied object is of class <code>demonoid.ppc</code>.
</p>
<p>The <code>is.demonoid.val</code> function returns a logical value indicating
whether or not the supplied object is of class <code>demonoid.val</code>.
</p>
<p>The <code>is.hangartner</code> function returns a logical value indicating
whether or not the supplied object is of class <code>hangartner</code>.
</p>
<p>The <code>is.heidelberger</code> function returns a logical value indicating
whether or not the supplied object is of class <code>heidelberger</code>.
</p>
<p>The <code>is.importance</code> function returns a logical value indicating
whether or not the supplied object is of class <code>importance</code>.
</p>
<p>The <code>is.iterquad</code> function returns a logical value indicating
whether or not the supplied object is of class <code>iterquad</code>.
</p>
<p>The <code>is.iterquad.ppc</code> function returns a logical value indicating
whether or not the supplied object is of class <code>iterquad.ppc</code>.
</p>
<p>The <code>is.juxtapose</code> function returns a logical value indicating
whether or not the supplied object is of class <code>juxtapose</code>.
</p>
<p>The <code>is.laplace</code> function returns a logical value indicating
whether or not the supplied object is of class <code>laplace</code>.
</p>
<p>The <code>is.laplace.ppc</code> function returns a logical value indicating
whether or not the supplied object is of class <code>laplace.ppc</code>.
</p>
<p>The <code>is.miss</code> function returns a logical value indicating
whether or not the supplied object is of class <code>miss</code>.
</p>
<p>The <code>is.pmc</code> function returns a logical value indicating
whether or not the supplied object is of class <code>pmc</code>.
</p>
<p>The <code>is.pmc.ppc</code> function returns a logical value indicating
whether or not the supplied object is of class <code>pmc.ppc</code>.
</p>
<p>The <code>is.pmc.val</code> function returns a logical value indicating
whether or not the supplied object is of class <code>pmc.val</code>.
</p>
<p>The <code>is.posteriorchecks</code> function returns a logical value
indicating whether or not the supplied object is of class
<code>posteriorchecks</code>.
</p>
<p>The <code>is.raftery</code> function returns a logical value indicating
whether or not the supplied object is of class <code>raftery</code>.
</p>
<p>The <code>is.rejection</code> function returns a logical value indicating
whether or not the supplied object is of class <code>rejection</code>.
</p>
<p>The <code>is.sensitivity</code> function returns a logical value indicating
whether or not the supplied object is of class <code>sensitivity</code>.
</p>
<p>The <code>is.vb</code> function returns a logical value indicating
whether or not the supplied object is of class <code>vb</code>.
</p>
<p>The <code>is.vb.ppc</code> function returns a logical value indicating
whether or not the supplied object is of class <code>vb.ppc</code>.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+BayesFactor">BayesFactor</a></code>,
<code><a href="#topic+Blocks">Blocks</a></code>,
<code><a href="#topic+BMK.Diagnostic">BMK.Diagnostic</a></code>,
<code><a href="base.html#topic+class">class</a></code>,
<code><a href="#topic+Hangartner.Diagnostic">Hangartner.Diagnostic</a></code>,
<code><a href="#topic+Heidelberger.Diagnostic">Heidelberger.Diagnostic</a></code>,
<code><a href="#topic+Importance">Importance</a></code>,
<code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code>,
<code><a href="#topic+Juxtapose">Juxtapose</a></code>,
<code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>,
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>,
<code><a href="#topic+LaplacesDemon.hpc">LaplacesDemon.hpc</a></code>,
<code><a href="#topic+MISS">MISS</a></code>,
<code><a href="#topic+PMC">PMC</a></code>,
<code><a href="#topic+PosteriorChecks">PosteriorChecks</a></code>,
<code><a href="#topic+predict.demonoid">predict.demonoid</a></code>,
<code><a href="#topic+predict.laplace">predict.laplace</a></code>,
<code><a href="#topic+predict.pmc">predict.pmc</a></code>,
<code><a href="#topic+predict.vb">predict.vb</a></code>,
<code><a href="#topic+Raftery.Diagnostic">Raftery.Diagnostic</a></code>,
<code><a href="#topic+RejectionSampling">RejectionSampling</a></code>,
<code><a href="#topic+SensitivityAnalysis">SensitivityAnalysis</a></code>,
<code><a href="#topic+Validate">Validate</a></code>, and
<code><a href="#topic+VariationalBayes">VariationalBayes</a></code>.
</p>

<hr>
<h2 id='is.constant'>Logical Check of a Constant</h2><span id='topic+is.constant'></span>

<h3>Description</h3>

<p>This function provides a logical test of whether or not a vector is a
constant.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.constant(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="is.constant_+3A_x">x</code></td>
<td>
<p>This is a vector.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>As opposed to a variable, a constant is a vector in which the elements
contain less than or equal to one unique value.
</p>


<h3>Value</h3>

<p>The <code>is.constant</code> function returns a logical result, reporting
<code>TRUE</code> when a vector is a constant, or <code>FALSE</code> otherwise.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+unique">unique</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
is.constant(rep(1,10)) #TRUE
is.constant(1:10) #FALSE
</code></pre>

<hr>
<h2 id='is.constrained'>Logical Check of Constraints</h2><span id='topic+is.constrained'></span>

<h3>Description</h3>

<p>This function provides a logical test of constraints for each initial
value or parameter for a model specification, given data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.constrained(Model, Initial.Values, Data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="is.constrained_+3A_model">Model</code></td>
<td>
<p>This is a model specification function. For more
information, see the <code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code> function.</p>
</td></tr>
<tr><td><code id="is.constrained_+3A_initial.values">Initial.Values</code></td>
<td>
<p>This is a vector of initial values, or current
parameter values. For more information, see the
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code> function.</p>
</td></tr>
<tr><td><code id="is.constrained_+3A_data">Data</code></td>
<td>
<p>This is a list of data. For more information, see the
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code> function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is useful for testing whether or not initial values
changed due to constraints when being passed through a <code>Model</code>
specification function. If any initial value changes, then the
constrained values that are ouput in the fifth component of the
<code>Model</code> specification are suitable as initial values, not the
tested initial values.
</p>
<p>A parameter may be constrained and this function may not discover the
constraint, since the discovery depends on the initial values and
whether or not they change as they are passed through the model.
</p>


<h3>Value</h3>

<p>The <code>is.constrained</code> function returns a logical vector, equal in
length to the number of initial values. Each element receives
<code>TRUE</code> if the corresponding initial value changed due to a
constraint, or <code>FALSE</code> if it did not.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>.
</p>

<hr>
<h2 id='is.data'>Logical Check of Data</h2><span id='topic+is.data'></span>

<h3>Description</h3>

<p>This function provides a logical test of whether or not a given list
of data meets minimum criteria to be considered data for
<code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code>, <code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>,
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>, <code><a href="#topic+PMC">PMC</a></code>, or
<code><a href="#topic+VariationalBayes">VariationalBayes</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.data(Data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="is.data_+3A_data">Data</code></td>
<td>
<p>This is a list of data. For more information, see the
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code> function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is useful for testing whether or not a list of data
meets minimum criteria to be considered data in this package. The
minimum requirements are that <code>Data</code> is a list, and it contains
<code>mon.names</code> and <code>parm.names</code>.
</p>
<p>This function is not extensive. For example, it does not match the
length of <code>parm.names</code> with the length of <code>Initial.Values</code>,
or compare the length of <code>mon.names</code> to the number of monitored
variables output from the <code>Model</code> specification
function. Additional checks are conducted in
<code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code>, <code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>,
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>, <code><a href="#topic+PMC">PMC</a></code>, and
<code><a href="#topic+VariationalBayes">VariationalBayes</a></code>.
</p>


<h3>Value</h3>

<p>The <code>is.data</code> function returns a logical value. It returns
<code>TRUE</code> if <code>Data</code> meets minimum requirements to be considered
data in this package, and <code>FALSE</code> otherwise.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code>
<code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>,
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>,
<code><a href="#topic+PMC">PMC</a></code>, and
<code><a href="#topic+VariationalBayes">VariationalBayes</a></code>.
</p>

<hr>
<h2 id='is.model'>Logical Check of a Model</h2><span id='topic+is.model'></span>

<h3>Description</h3>

<p>This function provides a logical test of whether or not a <code>Model</code>
specification function meets mininum requirements to be considered as
such.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.model(Model, Initial.Values, Data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="is.model_+3A_model">Model</code></td>
<td>
<p>This is a model specification function. For more
information, see the <code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code> function.</p>
</td></tr>
<tr><td><code id="is.model_+3A_initial.values">Initial.Values</code></td>
<td>
<p>This is a vector of initial values, or current
parameter values. For more information, see the
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code> function.</p>
</td></tr>
<tr><td><code id="is.model_+3A_data">Data</code></td>
<td>
<p>This is a list of data. For more information, see the
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code> function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function tests for minimum criteria for <code>Model</code> to be
considered a model specification function. Specifically, it tests:
</p>

<ul>
<li> <p><code>Model</code> must be a function
</p>
</li>
<li> <p><code>Model</code> must execute without errors
</p>
</li>
<li> <p><code>Model</code> must return a list
</p>
</li>
<li> <p><code>Model</code> must have five components in the list
</p>
</li>
<li><p> The first component must be named LP and have length 1
</p>
</li>
<li><p> The second component must be named Dev and have length 1
</p>
</li>
<li><p> The third component must be named Monitor
</p>
</li>
<li><p> The lengths of Monitor and mon.names must be equal
</p>
</li>
<li><p> The fourth component must be named yhat
</p>
</li>
<li><p> The fifth component must be named parm
</p>
</li>
<li><p> The lengths of parm and parm.names must be equal
</p>
</li></ul>

<p>This function is not extensive, and checks only for these minimum
criteria. Additional checks are conducted in
<code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code>, <code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>,
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>, <code><a href="#topic+PMC">PMC</a></code>, and
<code><a href="#topic+VariationalBayes">VariationalBayes</a></code>.
</p>


<h3>Value</h3>

<p>The <code>is.model</code> function returns a logical value of <code>TRUE</code>
when <code>Model</code> meets minimum criteria of a model specification
function, and <code>FALSE</code> otherwise.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code>,
<code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>,
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>,
<code><a href="#topic+PMC">PMC</a></code>, and
<code><a href="#topic+VariationalBayes">VariationalBayes</a></code>.
</p>

<hr>
<h2 id='is.proper'>Logical Check of Propriety</h2><span id='topic+is.proper'></span>

<h3>Description</h3>

<p>This function provides a logical check of the propriety of a
univariate prior probability distribution or the joint posterior
distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.proper(f, a, b, tol=1e-5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="is.proper_+3A_f">f</code></td>
<td>
<p>This is either a probability density function or an object of
class <code>demonoid</code>, <code>laplace</code>, <code>pmc</code>, or <code>vb</code>.</p>
</td></tr>
<tr><td><code id="is.proper_+3A_a">a</code></td>
<td>
<p>This is the lower limit of integration, and may be negative
infinity.</p>
</td></tr>
<tr><td><code id="is.proper_+3A_b">b</code></td>
<td>
<p>This is the upper limit of integration, and may be positive
infinity.</p>
</td></tr>
<tr><td><code id="is.proper_+3A_tol">tol</code></td>
<td>
<p>This is the tolerance, and indicates the allowable
difference from one.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A proper probability distribution is a probability distribution that
integrates to one, and an improper probability distribution does not
integrate to one. If a probability distribution integrates to any
positive and finite value other than one, then it is an improper
distribution, but is merely unnormalized. An unnormalized distribution
may be multiplied by a constant so that it integrates to one.
</p>
<p>In Bayesian inference, the posterior probability distribution should
be proper. An improper prior distribution can cause an improper
posterior distribution. When the posterior distribution is improper,
inferences are invalid, it is non-integrable, and Bayes factors cannot
be used (though there are exceptions).
</p>
<p>To avoid these problems, it is suggested that the prior probability
distribution should be proper, though it is possible to use an
improper prior distribution and have it result in a proper posterior
distribution.
</p>
<p>To check the propriety of a univariate prior probability distribution,
create a function <code>f</code>. For example, to check the propriety of a
vague normal distribution, such as
</p>
<p style="text-align: center;"><code class="reqn">\theta \sim \mathcal{N}(0,1000)</code>
</p>

<p>the function is <code>function(x){dnormv(x,0,1000)}</code>. Next, set the
lower and upper limits of integration, <code>a</code> and
<code>b</code>. Internally, this function calls <code>integrate</code> from base
R, which uses adaptive quadrature. By using <code class="reqn">f(x)</code> as shorthand
for the specified function, <code>is.proper</code> will check to see if the
area of the following integral is one:
</p>
<p style="text-align: center;"><code class="reqn">\int^b_a f(x)dx</code>
</p>

<p>Multivariate prior probability distributions currently cannot be
checked for approximate propriety. This is currently unavailable in
this package.
</p>
<p>To check the propriety of the joint posterior distribution, the only
argument to be supplied is an object of class <code>demonoid</code>,
<code>iterquad</code>, <code>laplace</code>, <code>pmc</code>, or <code>vb</code>. The
<code>is.proper</code> function checks the logarithm of the marginal
likelihood (see <code><a href="#topic+LML">LML</a></code>) for a finite value, and returns
<code>TRUE</code> when the LML is finite. This indicates that the marginal
likelihood is finite for all observed <code class="reqn">\textbf{y}</code> in the model
data set. This implies:
</p>
<p style="text-align: center;"><code class="reqn">\int p(\theta|\textbf{y})p(\theta)d\theta &lt; \infty</code>
</p>

<p>If the object is of class <code>demonoid</code> and the algorithm was
adaptive, or if the object is of class <code>iterquad</code>,
<code>laplace</code>, or <code>vb</code> and the algorithm did not converge, then
<code>is.proper</code> will return <code>FALSE</code> because LML was not
estimated. In this case, it is possible for the joint posterior to be
proper, but <code>is.proper</code> will be unable to determine propriety
without the estimate of LML. If desired, the <code><a href="#topic+LML">LML</a></code> may be
estimated by the user, and if it is finite, then the joint posterior
distribution is proper.
</p>


<h3>Value</h3>

<p>The <code>is.proper</code> function returns a logical value indicating
whether or not the univariate prior or joint posterior probability
distribution integrates to one within its specified limits.
<code>TRUE</code> is returned for a proper univariate probability
distribution.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+dnormv">dnormv</a></code>,
<code><a href="stats.html#topic+integrate">integrate</a></code>,
<code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code>,
<code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>,
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>,
<code><a href="#topic+LML">LML</a></code>,
<code><a href="#topic+PMC">PMC</a></code>, and
<code><a href="#topic+VariationalBayes">VariationalBayes</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
### Prior Probability Distribution
is.proper(function(x) {dnormv(x,0,1000)}, -Inf, Inf) #x ~ N(0,1000)
is.proper(function(x) {dhalfcauchy(x,25)}, 0, Inf) #x ~ HC(25)
is.proper(function(x) {dunif(x,0,1)}, 0, 1) #x ~ U(0,1)
is.proper(function(x) {dunif(x,-Inf,Inf)}, -Inf, Inf) #x ~ U(-Inf,Inf)
### Joint Posterior Distribution
##This assumes that Fit is an object of class demonoid, iterquad,
##  laplace, or pmc
#is.proper(Fit)
</code></pre>

<hr>
<h2 id='is.stationary'>Logical Check of Stationarity</h2><span id='topic+is.stationary'></span>

<h3>Description</h3>

<p>This function returns <code>TRUE</code> if the object is stationary
according to the <code><a href="#topic+Geweke.Diagnostic">Geweke.Diagnostic</a></code> function, and
<code>FALSE</code> otherwise.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.stationary(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="is.stationary_+3A_x">x</code></td>
<td>
<p>This is a vector, matrix, or object of class
<code>demonoid</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Stationarity, here, refers to the limiting distribution in a Markov
chain. A series of samples from a Markov chain, in which each sample
is the result of an iteration of a Markov chain Monte Carlo (MCMC)
algorithm, is analyzed for stationarity, meaning whether or not the
samples trend or its moments change across iterations. A stationary
posterior distribution is an equilibrium distribution, and assessing
stationarity is an important diagnostic toward inferring Markov chain
convergence.
</p>
<p>In the cases of a matrix or an object of class <code>demonoid</code>, all
Markov chains (as column vectors) must be stationary for
<code>is.stationary</code> to return <code>TRUE</code>.
</p>
<p>Alternative ways to assess stationarity of chains are to use the
<code><a href="#topic+BMK.Diagnostic">BMK.Diagnostic</a></code> or <code><a href="#topic+Heidelberger.Diagnostic">Heidelberger.Diagnostic</a></code>
functions.
</p>


<h3>Value</h3>

<p><code>is.stationary</code> returns a logical value indicating whether or not
the supplied object is stationary according to the
<code><a href="#topic+Geweke.Diagnostic">Geweke.Diagnostic</a></code> function.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+BMK.Diagnostic">BMK.Diagnostic</a></code>,
<code><a href="#topic+Geweke.Diagnostic">Geweke.Diagnostic</a></code>,
<code><a href="#topic+Heidelberger.Diagnostic">Heidelberger.Diagnostic</a></code>, and
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
is.stationary(rnorm(100))
is.stationary(matrix(rnorm(100),10,10))
</code></pre>

<hr>
<h2 id='IterativeQuadrature'>Iterative Quadrature</h2><span id='topic+IterativeQuadrature'></span>

<h3>Description</h3>

<p>The <code>IterativeQuadrature</code> function iteratively approximates the
first two moments of marginal posterior distributions of a Bayesian
model with deterministic integration.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>IterativeQuadrature(Model, parm, Data, Covar=NULL, Iterations=100,
     Algorithm="CAGH", Specs=NULL, Samples=1000, sir=TRUE,
     Stop.Tolerance=c(1e-5,1e-15), CPUs=1, Type="PSOCK")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="IterativeQuadrature_+3A_model">Model</code></td>
<td>
<p>This required argument receives the model from a
user-defined function. The user-defined function is where the model
is specified. <code>IterativeQuadrature</code> passes two arguments to
the model function, <code>parms</code> and <code>Data</code>. For more
information, see the <code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code> function and
&ldquo;LaplacesDemon Tutorial&rdquo; vignette.</p>
</td></tr>
<tr><td><code id="IterativeQuadrature_+3A_parm">parm</code></td>
<td>
<p>This argument requires a vector of initial values equal in
length to the number of parameters. <code>IterativeQuadrature</code> will
attempt to approximate these initial values for the parameters as
means (or posterior modes) of normal integrals. The
<code><a href="#topic+GIV">GIV</a></code> function may be used to randomly generate initial
values. Parameters must be continuous.</p>
</td></tr>
<tr><td><code id="IterativeQuadrature_+3A_data">Data</code></td>
<td>
<p>This required argument accepts a list of data. The list of
data must include <code>mon.names</code> which contains monitored variable
names, and <code>parm.names</code> which contains parameter
names.</p>
</td></tr>
<tr><td><code id="IterativeQuadrature_+3A_covar">Covar</code></td>
<td>
<p>This argument accepts a <code class="reqn">J \times J</code>
covariance matrix for <code class="reqn">J</code> initial values. When a covariance
matrix is not supplied, a scaled identity matrix is used.</p>
</td></tr>
<tr><td><code id="IterativeQuadrature_+3A_iterations">Iterations</code></td>
<td>
<p>This argument accepts an integer that determines the
number of iterations that <code>IterativeQuadrature</code> will attempt
to approximate the posterior with normal
integrals. <code>Iterations</code> defaults to 100.
<code>IterativeQuadrature</code> will stop before this number of
iterations if the tolerance is less than or equal to the
<code>Stop.Tolerance</code> criterion. The required amount of computer
memory increases with <code>Iterations</code>. If computer memory is
exceeded, then all will be lost.</p>
</td></tr>
<tr><td><code id="IterativeQuadrature_+3A_algorithm">Algorithm</code></td>
<td>
<p>This optional argument accepts a quoted string that
specifies the iterative quadrature algorithm. The default
method is <code>Method="CAGH"</code>. Options include <code>"AGHSG"</code> for
Adaptive Gauss-Hermite Sparse Grid, and <code>"CAGH"</code> for
Componentwise Adaptive Gaussian-Hermite.</p>
</td></tr>
<tr><td><code id="IterativeQuadrature_+3A_specs">Specs</code></td>
<td>
<p>This argument accepts a list of specifications for an
algorithm.</p>
</td></tr>
<tr><td><code id="IterativeQuadrature_+3A_samples">Samples</code></td>
<td>
<p>This argument indicates the number of posterior samples
to be taken with sampling importance resampling via the
<code><a href="#topic+SIR">SIR</a></code> function, which occurs only when
<code>sir=TRUE</code>. Note that the number of samples should increase
with the number and intercorrelations of the parameters.</p>
</td></tr>
<tr><td><code id="IterativeQuadrature_+3A_sir">sir</code></td>
<td>
<p>This logical argument indicates whether or not Sampling
Importance Resampling (SIR) is conducted via the <code><a href="#topic+SIR">SIR</a></code>
function to draw independent posterior samples. This argument
defaults to <code>TRUE</code>. Even when <code>TRUE</code>, posterior samples
are drawn only when <code>IterativeQuadrature</code> has
converged. Posterior samples are required for many other functions,
including <code>plot.iterquad</code> and <code>predict.iterquad</code>. Less
time can be spent on sampling by increasing <code>CPUs</code>, if
available, which parallelizes the sampling.</p>
</td></tr>
<tr><td><code id="IterativeQuadrature_+3A_stop.tolerance">Stop.Tolerance</code></td>
<td>
<p>This argument accepts a vector of two positive
numbers, and defaults to <code>1e-5,1e-15</code>. Tolerance is calculated
each iteration, and the criteria varies by algorithm. The algorithm
is considered to have converged to the user-specified
<code>Stop.Tolerance</code> when the tolerance is less than or equal to
the value of <code>Stop.Tolerance</code>, and the algorithm terminates at
the end of the current iteration. Unless stated otherwise, the
first element is the stop tolerance for the change in <code class="reqn">\mu</code>,
the second element is the stop tolerance for the change in mean
integration error, and the first tolerance must be met before the
second tolerance is considered.</p>
</td></tr>
<tr><td><code id="IterativeQuadrature_+3A_cpus">CPUs</code></td>
<td>
<p>This argument accepts an integer that specifies the number
of central processing units (CPUs) of the multicore computer or
computer cluster. This argument defaults to <code>CPUs=1</code>, in which
parallel processing does not occur. When multiple CPUs are
specified, model function evaluations are parallelized across the
nodes, and sampling with <code><a href="#topic+SIR">SIR</a></code> is parallelized when
<code>sir=TRUE</code>.</p>
</td></tr>
<tr><td><code id="IterativeQuadrature_+3A_type">Type</code></td>
<td>
<p>This argument specifies the type of parallel processing to
perform, accepting either <code>Type="PSOCK"</code> or
<code>Type="MPI"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Quadrature is a historical term in mathematics that means determining
area. Mathematicians of ancient Greece, according to the Pythagorean
doctrine, understood determination of area of a figure as the process
of geometrically constructing a square having the same area
(squaring). Thus the name quadrature for this process.
</p>
<p>In medieval Europe, quadrature meant the calculation of area by any
method. With the invention of integral calculus, quadrature has been
applied to the computation of a univariate definite
integral. Numerical integration is a broad family of algorithms for
calculating the numerical value of a definite integral. Numerical
quadrature is a synonym for quadrature applied to one-dimensional
integrals. Multivariate quadrature, also called cubature, is the
application of quadrature to multidimensional integrals.
</p>
<p>A quadrature rule is an approximation of the definite integral of a
function, usually stated as a weighted sum of function values at
specified points within the domain of integration. The specified
points are referred to as abscissae, abscissas, integration points, or
nodes, and have associated weights. The calculation of the nodes and
weights of the quadrature rule differs by the type of
quadrature. There are numerous types of quadrature
algorithms. Bayesian forms of quadrature usually use Gauss-Hermite
quadrature (Naylor and Smith, 1982), and placing a Gaussian Process on
the function is a common extension (O'Hagan, 1991; Rasmussen and
Ghahramani, 2003) that is called &lsquo;Bayesian Quadrature&rsquo;. Often, these
and other forms of quadrature are also referred to as model-based
integration.
</p>
<p>Gauss-Hermite quadrature uses Hermite polynomials to calculate the
rule. However, there are two versions of Hermite polynomials, which
result in different kernels in different fields. In physics, the
kernel is <code>exp(-x^2)</code>, while in probability the kernel is
<code>exp(-x^2/2)</code>. The weights are a normal density. If the
parameters of the normal distribution, <code class="reqn">\mu</code> and
<code class="reqn">\sigma^2</code>, are estimated from data, then it is referred
to as adaptive Gauss-Hermite quadrature, and the parameters are the
conditional mean and conditional variance. Outside of Gauss-Hermite
quadrature, adaptive quadrature implies that a difficult range in the
integrand is subdivided with more points until it is
well-approximated. Gauss-Hermite quadrature performs well when the
integrand is smooth, and assumes normality or multivariate normality.
Adaptive Gauss-Hermite quadrature has been demonstrated to outperform
Gauss-Hermite quadrature in speed and accuracy.
</p>
<p>A goal in quadrature is to minimize integration error, which is the
error between the evaluations and the weights of the rule. Therefore,
a goal in Bayesian Gauss-Hermite quadrature is to minimize integration
error while approximating a marginal posterior distribution that is
assumed to be smooth and normally-distributed. This minimization often
occurs by increasing the number of nodes until a change in mean
integration error is below a tolerance, rather than minimizing
integration error itself, since the target may be only approximately
normally distributed, or minimizing the sum of integration error,
which would change with the number of nodes.
</p>
<p>To approximate integrals in multiple dimensions, one approach applies
<code class="reqn">N</code> nodes of a univariate quadrature rule to multiple dimensions
(using the <code><a href="#topic+GaussHermiteCubeRule">GaussHermiteCubeRule</a></code> function for example)
via the product rule, which results in many more multivariate nodes.
This requires the number of function evaluations to grow exponentially
as dimension increases. Multidimensional quadrature is usually limited
to less than ten dimensions, both due to the number of nodes required,
and because the accuracy of multidimensional quadrature algorithms
decreases as the dimension increases. Three methods may overcome this
curse of dimensionality in varying degrees: componentwise quadrature,
sparse grids, and Monte Carlo.
</p>
<p>Componentwise quadrature is the iterative application of univariate
quadrature to each parameter. It is applicable with high-dimensional
models, but sacrifices the ability to calculate the conditional
covariance matrix, and calculates only the variance of each parameter.
</p>
<p>Sparse grids were originally developed by Smolyak for
multidimensional quadrature. A sparse grid is based on a
one-dimensional quadrature rule. Only a subset of the nodes from the
product rule is included, and the weights are appropriately rescaled.
Although a sparse grid is more efficient because it reduces the
number of nodes to achieve the same accuracy, the user must contend
with increasing the accuracy of the grid, and it remains inapplicable
to high-dimensional integrals.
</p>
<p>Monte Carlo is a large family of sampling-based algorithms. O'Hagan
(1987) asserts that Monte Carlo is frequentist, inefficient, regards
irrelevant information, and disregards relevant information.
Quadrature, he maintains (O'Hagan, 1992), is the most Bayesian
approach, and also the most efficient. In high dimensions, he
concedes, a popular subset of Monte Carlo algorithms is currently the
best for cheap model function evaluations. These algorithms are called
Markov chain Monte Carlo (MCMC). High-dimensional models with
expensive model evaluation functions, however, are not well-suited to
MCMC. A large number of MCMC algorithms is available in the
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code> function.
</p>
<p>Following are some reasons to consider iterative quadrature rather
than MCMC. Once an MCMC sampler finds equilibrium, it must then draw
enough samples to represent all targets. Iterative quadrature does not
need to continue drawing samples. Multivariate quadrature is
consistently reported as more efficient than MCMC when its assumptions
hold, though multivariate quadrature is limited to small dimensions.
High-dimensional models therefore default to MCMC, between the two.
Componentwise quadrature algorithms like CAGH, however, may also be
more efficient with cloc-time than MCMC in high dimensions, especially
against componentwise MCMC algorithms. Another reason to consider
iterative quadrature are that assessing convergence in MCMC is a
difficult topic, but not for iterative quadrature. A user of
iterative quadrature does not have to contend with effective sample
size and autocorrelation, assessing stationarity, acceptance rates,
diminishing adaptation, etc. Stochastic sampling in MCMC is less
efficient when samples occur in close proximity (such as when highly
autocorrelated), whereas in quadrature the nodes are spread out by
design.
</p>
<p>In general, the conditional means and conditional variances progress
smoothly to the target in multidimensional quadrature. For
componentwise quadrature, movement to the target is not smooth, and
often resembles a Markov chain or optimization algorithm.
</p>
<p>Iterative quadrature is often applied after
<code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code> to obtain a more reliable
estimate of parameter variance or covariance than the negative inverse
of the <code><a href="#topic+Hessian">Hessian</a></code> matrix of second derivatives, which is
suitable only when the contours of the logarithm of the unnormalized
joint posterior density are approximately ellipsoidal (Naylor and
Smith, 1982, p. 224).
</p>
<p>When <code>Algorithm="AGH"</code>, the Naylor and Smith (1982) algorithm
is used. The AGH algorithm uses multivariate quadrature with the
physicist's (not the probabilist's) kernel.
</p>
<p>There are four algorithm specifications: <code>N</code> is the number of
univariate nodes, <code>Nmax</code> is the maximum number of univariate
nodes, <code>Packages</code> accepts any package required for the model
function when parallelized, and <code>Dyn.libs</code> accepts dynamic
libraries for parallelization, if required. The number of univariate
nodes begins at <code class="reqn">N</code> and increases by one each iteration. The
number of multivariate nodes grows quickly with <code class="reqn">N</code>. Naylor and
Smith (1982) recommend beginning with as few nodes as <code class="reqn">N=3</code>. Any
of the following events will cause <code class="reqn">N</code> to increase by 1 when
<code class="reqn">N</code> is less than <code>Nmax</code>:
</p>

<ul>
<li><p> All LP weights are zero (and non-finite weights are set to zero)
</p>
</li>
<li> <p><code class="reqn">\mu</code> does not result in an increase in LP
</p>
</li>
<li><p> All elements in <code class="reqn">\Sigma</code> are not finite
</p>
</li>
<li><p> The square root of the sum of the squared changes in <code class="reqn">\mu</code>
is less than or equal to the <code>Stop.Tolerance</code>
</p>
</li></ul>

<p>Tolerance includes two metrics: change in mean integration error and
change in parameters. Including the change in parameters for tolerance
was not mentioned in Naylor and Smith (1982).
</p>
<p>Naylor and Smith (1982) consider a transformation due to correlation.
This is not included here.
</p>
<p>The AGH algorithm does not currently handle constrained parameters,
such as with the <code><a href="#topic+interval">interval</a></code> function. If a parameter is
constrained and changes during a model evaluation, this changes the
node and the multivariate weight. This is currently not corrected.
</p>
<p>An advantage of AGH over componentwise adaptive quadrature is that
AGH estimates covariance, where a componentwise algorithm ignores
it. A disadvantage of AGH over a componentwise algorithm is that
the number of nodes increases so quickly with dimension, that AGH is
limited to small-dimensional models.
</p>
<p>When <code>Algorithm="AGHSG"</code>, the Naylor and Smith (1982) algorithm
is applied to a sparse grid, rather than a traditional multivariate
quadrature rule. This is identical to the AGH algorithm above, except
that a sparse grid replaces the multivariate quadrature rule.
</p>
<p>The sparse grid reduces the number of nodes. The cost of reducing the
number of nodes is that the user must consider the accuracy, <code class="reqn">K</code>.
</p>
<p>There are four algorithm specifications: <code>K</code> is the accuracy (as a
positive integer), <code>Kmax</code> is the maximum accuracy,
<code>Packages</code> accepts any package required for the model function
when parallelized, and <code>Dyn.libs</code> accepts dynamic libraries for
parallelization, if required. These arguments represent accuracy
rather than the number of univariate nodes, but otherwise are similar
to the AGH algorithm.
</p>
<p>When <code>Algorithm="CAGH"</code>, a componentwise version of the adaptive
Gauss-Hermite quadrature of Naylor and Smith (1982) is used. Each
iteration, each marginal posterior distribution is approximated
sequentially, in a random order, with univariate quadrature. The
conditional mean and conditional variance are also approximated each
iteration, making it an adaptive algorithm.
</p>
<p>There are four algorithm specifications: <code>N</code> is the number of
nodes, <code>Nmax</code> is the maximum number of nodes, <code>Packages</code>
accepts any package required for the model function when parallelized,
and <code>Dyn.libs</code> accepts dynamic libraries for parallelization, if
required. The number of nodes begins at <code class="reqn">N</code>. All parameters have
the same number of nodes. Any of the following events will cause
<code class="reqn">N</code> to increase by 1 when <code class="reqn">N</code> is less than <code>Nmax</code>, and
these conditions refer to all parameters (not individually):
</p>

<ul>
<li><p> Any LP weights are not finite
</p>
</li>
<li><p> All LP weights are zero
</p>
</li>
<li> <p><code class="reqn">\mu</code> does not result in an increase in LP
</p>
</li>
<li><p> The square root of the sum of the squared changes in <code class="reqn">\mu</code>
is less than or equal to the <code>Stop.Tolerance</code>
</p>
</li></ul>

<p>It is recommended to begin with <code>N=3</code> and set <code>Nmax</code> between
10 and 100. As long as CAGH does not experience problematic weights,
and as long as CAGH is improving LP with <code class="reqn">\mu</code>, the number of nodes
does not increase. When CAGH becomes either universally problematic or
universally stable, then <code class="reqn">N</code> slowly increases until the sum of
both the mean integration error and the sum of the squared changes in
<code class="reqn">\mu</code> is less than the <code>Stop.Tolerance</code> for two consecutive
iterations.
</p>
<p>If the highest LP occurs at the lowest or highest node, then the value
at that node becomes the conditional mean, rather than calculating it
from all weighted samples; this facilitates movement when the current
integral is poorly centered toward a well-centered integral. If all
weights are zero, then a random proposal is generated with a small
variance.
</p>
<p>Tolerance includes two metrics: change in mean integration error and
change in parameters, as the square root of the sum of the squared
differences.
</p>
<p>When a parameter constraint is encountered, the node and weight of the
quadrature rule is recalculated.
</p>
<p>An advantage of CAGH over multidimensional adaptive quadrature is that
CAGH may be applied in large dimensions. Disadvantages of CAGH are
that only variance, not covariance, is estimated, and ignoring
covariance may be problematic.
</p>


<h3>Value</h3>

<p><code>IterativeQuadrature</code> returns an object of class <code>iterquad</code>
that is a list with the following components:
</p>
<table>
<tr><td><code>Algorithm</code></td>
<td>
<p>This is the name of the iterative quadrature
algorithm.</p>
</td></tr>
<tr><td><code>Call</code></td>
<td>
<p>This is the matched call of <code>IterativeQuadrature</code>.</p>
</td></tr>
<tr><td><code>Converged</code></td>
<td>
<p>This is a logical indicator of whether or not
<code>IterativeQuadrature</code> converged within the specified
<code>Iterations</code> according to the supplied <code>Stop.Tolerance</code>
criterion. Convergence does not indicate that the global maximum has
been found, but only that the tolerance was less than or equal to
the <code>Stop.Tolerance</code> criteria.</p>
</td></tr>
<tr><td><code>Covar</code></td>
<td>
<p>This is the estimated covariance matrix. The <code>Covar</code>
matrix may be scaled and input into the <code>Covar</code> argument of the
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code> or <code><a href="#topic+PMC">PMC</a></code> function for
further estimation. To scale this matrix for use with Laplace's
Demon or PMC, multiply it by <code class="reqn">2.38^2/d</code>, where <code class="reqn">d</code> is the
number of initial values.</p>
</td></tr>
<tr><td><code>Deviance</code></td>
<td>
<p>This is a vector of the iterative history of the
deviance in the <code>IterativeQuadrature</code> function, as it sought
convergence.</p>
</td></tr>
<tr><td><code>History</code></td>
<td>
<p>This is a matrix of the iterative history of the
parameters in the <code>IterativeQuadrature</code> function, as it sought
convergence.</p>
</td></tr>
<tr><td><code>Initial.Values</code></td>
<td>
<p>This is the vector of initial values that was
originally given to <code>IterativeQuadrature</code> in the <code>parm</code>
argument.</p>
</td></tr>
<tr><td><code>LML</code></td>
<td>
<p>This is an approximation of the logarithm of the marginal
likelihood of the data (see the <code><a href="#topic+LML">LML</a></code> function for more
information). When the model has converged and <code>sir=TRUE</code>, the
NSIS method is used. When the model has converged and
<code>sir=FALSE</code>, the LME method is used. This is the
logarithmic form of equation 4 in Lewis and Raftery (1997). As a
rough estimate of Kass and Raftery (1995), the LME-based LML is
worrisome when the sample size of the data is less than five times
the number of parameters, and <code>LML</code> should be adequate in most
problems when the sample size of the data exceeds twenty times the
number of parameters (p. 778). The LME is inappropriate with
hierarchical models. However <code>LML</code> is estimated, it is useful
for comparing multiple models with the <code>BayesFactor</code> function.</p>
</td></tr>
<tr><td><code>LP.Final</code></td>
<td>
<p>This reports the final scalar value for the logarithm
of the unnormalized joint posterior density.</p>
</td></tr>
<tr><td><code>LP.Initial</code></td>
<td>
<p>This reports the initial scalar value for the
logarithm of the unnormalized joint posterior density.</p>
</td></tr>
<tr><td><code>LPw</code></td>
<td>
<p>This is the latest matrix of the logarithm of the
unnormalized joint posterior density. It is weighted and normalized
so that each column sums to one.</p>
</td></tr>
<tr><td><code>M</code></td>
<td>
<p>This is the final <code class="reqn">N \times J</code> matrix of
quadrature weights that have been corrected for non-standard normal
distributions, where <code class="reqn">N</code> is the number of nodes and <code class="reqn">J</code>
is the number of parameters.</p>
</td></tr>
<tr><td><code>Minutes</code></td>
<td>
<p>This is the number of minutes that
<code>IterativeQuadrature</code> was running, and this includes the
initial checks as well as drawing posterior samples and creating
summaries.</p>
</td></tr>
<tr><td><code>Monitor</code></td>
<td>
<p>When <code>sir=TRUE</code>, a number of independent
posterior samples equal to <code>Samples</code> is taken, and the draws
are stored here as a matrix. The rows of the matrix are the samples,
and the columns are the monitored variables.</p>
</td></tr>
<tr><td><code>N</code></td>
<td>
<p>This is the final number of nodes.</p>
</td></tr>
<tr><td><code>Posterior</code></td>
<td>
<p>When <code>sir=TRUE</code>, a number of independent
posterior samples equal to <code>Samples</code> is taken, and the draws
are stored here as a matrix. The rows of the matrix are the samples,
and the columns are the parameters.</p>
</td></tr>
<tr><td><code>Summary1</code></td>
<td>
<p>This is a summary matrix that summarizes the
point-estimated posterior means. Uncertainty around the posterior
means is estimated from the covariance matrix. Rows are parameters.
The following columns are included: Mean, SD (Standard Deviation),
LB (Lower Bound), and UB (Upper Bound). The bounds constitute a 95%
probability interval.</p>
</td></tr>
<tr><td><code>Summary2</code></td>
<td>
<p>This is a summary matrix that summarizes the
posterior samples drawn with sampling importance resampling
(<code><a href="#topic+SIR">SIR</a></code>) when <code>sir=TRUE</code>, given the point-estimated
posterior modes and the covariance matrix. Rows are parameters. The
following columns are included: Mean, SD (Standard Deviation),
LB (Lower Bound), and UB (Upper Bound). The bounds constitute a 95%
probability interval.</p>
</td></tr>
<tr><td><code>Tolerance.Final</code></td>
<td>
<p>This is the last <code>Tolerance</code> of the
<code>LaplaceApproxiation</code> algorithm.</p>
</td></tr>
<tr><td><code>Tolerance.Stop</code></td>
<td>
<p>This is the <code>Stop.Tolerance</code> criteria.</p>
</td></tr>
<tr><td><code>Z</code></td>
<td>
<p>This is the final <code class="reqn">N \times J</code> matrix of the
conditional mean, where <code class="reqn">N</code> is the number of nodes and <code class="reqn">J</code>
is the number of parameters.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Statisticat, LLC <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>References</h3>

<p>Naylor, J.C. and Smith, A.F.M. (1982). &quot;Applications of a Method for
the Efficient Computation of Posterior Distributions&quot;. <em>Applied
Statistics</em>, 31(3), p. 214&ndash;225.
</p>
<p>O'Hagan, A. (1987). &quot;Monte Carlo is Fundamentally Unsound&quot;. <em>The
Statistician</em>, 36, p. 247&ndash;249.
</p>
<p>O'Hagan, A. (1991). &quot;Bayes-Hermite Quadrature&quot;. <em>Journal of
Statistical Planning and Inference</em>, 29, p. 245&ndash;260.
</p>
<p>O'Hagan, A. (1992). &quot;Some Bayesian Numerical Analysis&quot;. In Bernardo,
J.M., Berger, J.O., David, A.P., and Smith, A.F.M., editors,
<em>Bayesian Statistics</em>, 4, p. 356&ndash;363, Oxford University Press.
</p>
<p>Rasmussen, C.E. and Ghahramani, Z. (2003). &quot;Bayesian Monte Carlo&quot;. In
Becker, S. and Obermayer, K., editors, <em>Advances in Neural
Information Processing Systems</em>, 15, MIT Press, Cambridge, MA.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+GaussHermiteCubeRule">GaussHermiteCubeRule</a></code>,
<code><a href="#topic+GaussHermiteQuadRule">GaussHermiteQuadRule</a></code>,
<code><a href="#topic+GIV">GIV</a></code>,
<code><a href="#topic+Hermite">Hermite</a></code>,
<code><a href="#topic+Hessian">Hessian</a></code>,
<code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>,
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>,
<code><a href="#topic+LML">LML</a></code>,
<code><a href="#topic+PMC">PMC</a></code>,
<code><a href="#topic+SIR">SIR</a></code>, and
<code><a href="#topic+SparseGrid">SparseGrid</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># The accompanying Examples vignette is a compendium of examples.
####################  Load the LaplacesDemon Library  #####################
library(LaplacesDemon)

##############################  Demon Data  ###############################
data(demonsnacks)
y &lt;- log(demonsnacks$Calories)
X &lt;- cbind(1, as.matrix(log(demonsnacks[,10]+1)))
J &lt;- ncol(X)
for (j in 2:J) X[,j] &lt;- CenterScale(X[,j])

#########################  Data List Preparation  #########################
mon.names &lt;- "mu[1]"
parm.names &lt;- as.parm.names(list(beta=rep(0,J), sigma=0))
pos.beta &lt;- grep("beta", parm.names)
pos.sigma &lt;- grep("sigma", parm.names)
PGF &lt;- function(Data) {
     beta &lt;- rnorm(Data$J)
     sigma &lt;- runif(1)
     return(c(beta, sigma))
     }
MyData &lt;- list(J=J, PGF=PGF, X=X, mon.names=mon.names,
     parm.names=parm.names, pos.beta=pos.beta, pos.sigma=pos.sigma, y=y)

##########################  Model Specification  ##########################
Model &lt;- function(parm, Data)
     {
     ### Parameters
     beta &lt;- parm[Data$pos.beta]
     sigma &lt;- interval(parm[Data$pos.sigma], 1e-100, Inf)
     parm[Data$pos.sigma] &lt;- sigma
     ### Log-Prior
     beta.prior &lt;- sum(dnormv(beta, 0, 1000, log=TRUE))
     sigma.prior &lt;- dhalfcauchy(sigma, 25, log=TRUE)
     ### Log-Likelihood
     mu &lt;- tcrossprod(Data$X, t(beta))
     LL &lt;- sum(dnorm(Data$y, mu, sigma, log=TRUE))
     ### Log-Posterior
     LP &lt;- LL + beta.prior + sigma.prior
     Modelout &lt;- list(LP=LP, Dev=-2*LL, Monitor=mu[1],
          yhat=rnorm(length(mu), mu, sigma), parm=parm)
     return(Modelout)
     }

############################  Initial Values  #############################
#Initial.Values &lt;- GIV(Model, MyData, PGF=TRUE)
Initial.Values &lt;- rep(0,J+1)

#########################  Adaptive Gauss-Hermite  ########################
#Fit &lt;- IterativeQuadrature(Model, Initial.Values, MyData, Covar=NULL,
#     Iterations=100, Algorithm="AGH",
#     Specs=list(N=5, Nmax=7, Packages=NULL, Dyn.libs=NULL), CPUs=1)

##################  Adaptive Gauss-Hermite Sparse Grid  ###################
#Fit &lt;- IterativeQuadrature(Model, Initial.Values, MyData, Covar=NULL,
#     Iterations=100, Algorithm="AGHSG",
#     Specs=list(K=5, Kmax=7, Packages=NULL, Dyn.libs=NULL), CPUs=1)

#################  Componentwise Adaptive Gauss-Hermite  ##################
#Fit &lt;- IterativeQuadrature(Model, Initial.Values, MyData, Covar=NULL,
#     Iterations=100, Algorithm="CAGH",
#     Specs=list(N=3, Nmax=10, Packages=NULL, Dyn.libs=NULL), CPUs=1)

#Fit
#print(Fit)
#PosteriorChecks(Fit)
#caterpillar.plot(Fit, Parms="beta")
#plot(Fit, MyData, PDF=FALSE)
#Pred &lt;- predict(Fit, Model, MyData, CPUs=1)
#summary(Pred, Discrep="Chi-Square")
#plot(Pred, Style="Covariates", Data=MyData)
#plot(Pred, Style="Density", Rows=1:9)
#plot(Pred, Style="Fitted")
#plot(Pred, Style="Jarque-Bera")
#plot(Pred, Style="Predictive Quantiles")
#plot(Pred, Style="Residual Density")
#plot(Pred, Style="Residuals")
#Levene.Test(Pred)
#Importance(Fit, Model, MyData, Discrep="Chi-Square")

#End
</code></pre>

<hr>
<h2 id='joint.density.plot'>Joint Density Plot</h2><span id='topic+joint.density.plot'></span>

<h3>Description</h3>

<p>This function plots the joint kernel density from samples of two
marginal posterior distributions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>joint.density.plot(x, y, Title=NULL, contour=TRUE, color=FALSE, Trace=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="joint.density.plot_+3A_x">x</code>, <code id="joint.density.plot_+3A_y">y</code></td>
<td>
<p>These are vectors consisting of samples from two marginal
posterior distributions, such as those output by
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code> in components <code>Posterior1</code> (all
samples) or <code>Posterior2</code> (stationary samples).</p>
</td></tr>
<tr><td><code id="joint.density.plot_+3A_title">Title</code></td>
<td>
<p>This is the title of the joint posterior density plot.</p>
</td></tr>
<tr><td><code id="joint.density.plot_+3A_contour">contour</code></td>
<td>
<p>This logical argument indicates whether or not contour
lines will be added to the plot. <code>contour</code> defaults to
<code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="joint.density.plot_+3A_color">color</code></td>
<td>
<p>This logical argument indicates whether or not color will
be added to the plot. <code>color</code> defaults to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="joint.density.plot_+3A_trace">Trace</code></td>
<td>
<p>This argument defaults to <code>NULL</code>, in which case it
does not trace the exploration of the joint density. To trace the
exploration of the joint density, specify <code>Trace</code> with the
beginning and ending iteration or sample. For example, to view the
trace of the first ten iterations or samples, specify
<code>Trace=c(1,10)</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function produces either a bivariate scatterplot that may have
kernel density contour lines added, or a bivariate plot with kernel 
density-influenced colors, which may also have kernel density contour
lines added. A joint density plot may be more informative than two
univariate density plots.
</p>
<p>The <code>Trace</code> argument allows the user to view the exploration of
the joint density, such as from MCMC chain output. An efficient
algorithm jumps to random points of the joint density, and an
inefficient algorithm explores more slowly. The initial point of the
trace (which is the first element passed to <code>Trace</code>) is plotted
with a green dot. The user should consider plotting the joint density of
the two marginal posterior distributions with the highest
<code><a href="#topic+IAT">IAT</a></code>, as identified with the
<code><a href="#topic+PosteriorChecks">PosteriorChecks</a></code> function, since these are the two least
efficient MCMC chains. Different sequences of iterations may be
plotted. This &lsquo;joint trace plot&rsquo; may show behavior of the MCMC
algorithm to the user.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+IAT">IAT</a></code>,
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>, and
<code><a href="#topic+PosteriorChecks">PosteriorChecks</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
X &lt;- rmvn(1000, runif(2), diag(2))
joint.density.plot(X[,1], X[,2], Title="Joint Density Plot",
     contour=TRUE, color=FALSE)
joint.density.plot(X[,1], X[,2], Title="Joint Density Plot",
     contour=FALSE, color=TRUE)
joint.density.plot(X[,1], X[,2], Title="Joint Density Plot",
     contour=TRUE, color=TRUE)
joint.density.plot(X[,1], X[,2], Title="Joint Trace Plot",
     contour=FALSE, color=TRUE, Trace=c(1,10))
</code></pre>

<hr>
<h2 id='joint.pr.plot'>Joint Probability Region Plot</h2><span id='topic+joint.pr.plot'></span>

<h3>Description</h3>

<p>Given two vectors, the <code>joint.pr.plot</code> function creates a
scatterplot with ellipses of probability regions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>joint.pr.plot(x, y, quantiles=c(0.25,0.50,0.75,0.95))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="joint.pr.plot_+3A_x">x</code></td>
<td>
<p>This required argument is a vector.</p>
</td></tr>
<tr><td><code id="joint.pr.plot_+3A_y">y</code></td>
<td>
<p>This required argument is a vector.</p>
</td></tr>
<tr><td><code id="joint.pr.plot_+3A_quantiles">quantiles</code></td>
<td>
<p>These are the quantiles for which probability regions
are estimated with ellipses. The center of the ellipse is plotted by
default. The 0.95 quantile creates a probability region that
contains approximately 95% of the data or samples of <code>x</code> and
<code>y</code>. By default, four quantiles are included.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A probability region is also commonly called a credible region. For
more information on probability regions, see <code><a href="#topic+p.interval">p.interval</a></code>.
</p>
<p>Joint probability regions are plotted only for two variables, and the
regions are estimated with functions modified from the <code>car</code>
package. The internal ellipse functions assume bivariate normality.
</p>
<p>This function is often used to plot posterior distributions of
samples, such as from the <code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code> function.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code> and
<code><a href="#topic+p.interval">p.interval</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
x &lt;- rnorm(100)
y &lt;- rnorm(100)
joint.pr.plot(x, y)
</code></pre>

<hr>
<h2 id='Juxtapose'>Juxtapose MCMC Algorithm Inefficiency</h2><span id='topic+Juxtapose'></span>

<h3>Description</h3>

<p>This function gives a side-by-side comparison of (or juxtaposes) the
inefficiency of MCMC algorithms in <code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code> for
applied use, and is a valuable tool for selecting what is likely to be
the least inefficient algorithm for the user's current model, prior to
updating the final, intended model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Juxtapose(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Juxtapose_+3A_x">x</code></td>
<td>
<p>This is a list of multiple components. Each component must
be an object of class <code>demonoid</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Laplace's Demon recommends using the <code>Juxtapose</code> function on the
user's model (or most likely a simplified version of it) with a
smaller, simulated data set to select the least inefficient MCMC
algorithm before using real data and updating the model for numerous
iterations. The least inefficient MCMC algorithm differs for different
models and data sets. Using <code>Juxtapose</code> in this way does not
guarantee that the selected algorithm will remain the best choice with
real data, but it should be better than otherwise selecting an
algorithm.
</p>
<p>The user must make a decision regarding their model and data. The more
similar the model and data is to the final, intended model and data,
the more appropriate will be the results of the <code>Juxtapose</code>
function. However, if the full model and data are used, then the user
may as well instead skip using <code>Juxtapose</code> and proceed directly
to <code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>. Replacing the actual data set with a
smaller, simulated set is fairly straightforward, but the
decision-making will most likely focus on what is the best way to
reduce the full model specification. A simple approach may be to
merely reduce the number of predictors. However, complicated models
may have several components that slow down estimation time, and extend
the amount of time until global stationarity is estimated. Laplace's
Demon offers no guidance here, and leaves it in the realm of user
discretion.
</p>
<p>First, the user should simulate a smaller data set, and if best,
reduce the model specification. Next, the user must select candidate
algorithms. Then, the user must update each algorithm with
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code> for numerous iterations, with the goal of
achieving stationarity for all parameters early in the
iterations. Each update should begin with the same model specification
function, vector of initial values, and data. Each output object of
class <code>demonoid</code> should be renamed. An example follows.
</p>
<p>Suppose a user considers three candidate algorithms for their model:
AMWG, NUTS, and twalk. The user updates each model, saving the model
that used the AMWG algorithm as, say, <code>Fit1</code>, the NUTS model as
<code>Fit2</code>, and the twalk model as <code>Fit3</code>.
</p>
<p>Next, the output model objects are put in a list and passed to the
<code>Juxtapose</code> function. See the example below.
</p>
<p>The <code>Juxtapose</code> function uses an internal version of the
<code><a href="#topic+IAT">IAT</a></code>, which is a slightly modified version of that found
in the <code>SamplerCompare</code> package. The <code>Juxtapose</code> function
returns an object of class <code>juxtapose</code>. It is a matrix in which
each row is a result and each column is an algorithm.
</p>
<p>The rows are:
</p>

<ul>
<li> <p><code>iter.min</code>: This is the iterations per minute.
</p>
</li>
<li> <p><code>t.iter.min</code>: This is the thinned iterations per minute.
</p>
</li>
<li> <p><code>prop.stat</code>: This is the proportion of iterations that
were stationary.
</p>
</li>
<li> <p><code>IAT.025</code>: This is the 2.5% quantile of the integrated
autocorrelation time of the worst parameter, estimated only on
samples when all parameters are estimated to be globally stationary.
</p>
</li>
<li> <p><code>IAT.500</code>: This is the median integrated autocorrelation
time of the worst parameter, estimated only on samples when all
parameters are estimated to be globally stationary.
</p>
</li>
<li> <p><code>IAT.975</code>: This is the 97.5% quantile of the integrated
autocorrelation time of the worst parameter, estimated only on
samples when all parameters are estimated to be globally stationary.
</p>
</li>
<li> <p><code>ISM.025</code>: This is the 2.5% quantile of the number of
independent samples per minute.
</p>
</li>
<li> <p><code>ISM.500</code>: This is the median of the number of the
independent samples per minute. The least inefficient MCMC algorithm
has the highest <code>ISM.500</code>.
</p>
</li>
<li> <p><code>ISM.975</code>: This is the 97.5% quantile of the number of
the independent samples per minute.
</p>
</li></ul>

<p>As for calculating <code class="reqn">ISM</code>, let <code class="reqn">TIM</code> be the observed number of
thinned iterations per minute, <code class="reqn">PS</code> be the percent of iterations
in which all parameters were estimated to be globally stationary, and
<code class="reqn">IAT_q</code> be a quantile from a simulated distribution of the
integrated autocorrelation time among the parameters.
</p>
<p style="text-align: center;"><code class="reqn">ISM = \frac{PS \times TIM}{IAT_q}</code>
</p>

<p>There are various ways to measure the inefficiency of MCMC
samplers. <code><a href="#topic+IAT">IAT</a></code> is used perhaps most often. As with the
<code>SamplerCompare</code> package, Laplace's Demon uses the worst
parameter, in terms of <code><a href="#topic+IAT">IAT</a></code>. Often, the number of
evaluations or number of parameters is considered. The
<code>Juxtapose</code> function, instead considers the final criterion of
MCMC efficiency, in an applied context, to be <code>ISM</code>, or the
number of Independent (thinned) Samples per Minute. The algorithm with
the highest <code>ISM.500</code> is the best, or least inefficient,
algorithm with respect to its worst <code><a href="#topic+IAT">IAT</a></code>, the proportion
of iterations required to seem to have global stationarity, and the
number of (thinned) iterations per minute.
</p>
<p>A disadvantage of using time is that it will differ by computer, and
is less likely to be reported in a journal. The advantage, though, is
that it is more meaningful to a user. Increases in the number of
evaluations, parameters, and time should all correlate well, but time
may enlighten a user as to expected run-time given the model just
studied, even though the real data set will most likely be larger than
the simulated data used initially. NUTS is an example of a sampler in
which the number of evaluations varies per iteration. For an
alternative approach, see Thompson (2010).
</p>
<p>The <code>Juxtapose</code> function also adjusts <code>ISM</code> by
<code>prop.stat</code>, the proportion of the iterations in which all chains
were estimated to be stationary. This adjustment is weighted by
burn-in iterations, penalizing an algorithm that took longer to
achieve global stationarity. The goal, again, is to assist the user in
selecting the least inefficient MCMC algorithm in an applied setting.
</p>
<p>The <code>Juxtapose</code> function has many other potential uses than those
described above. One additional use of the <code>Juxtapose</code> function is
to compare inefficiencies within a single algorithm in which
algorithmic specifications varied with different model
updates. Another use is to investigate parallel chains in an object of
class <code>demonoid.hpc</code>, as returned from the
<code><a href="#topic+LaplacesDemon.hpc">LaplacesDemon.hpc</a></code> function. Yet another use is to
compare the effects of small changes to a model specification
function, such as with priors, or due to an increase in the amount of
simulated data.
</p>
<p>An object of class <code>juxtapose</code> may be plotted with the
<code><a href="#topic+plot.juxtapose">plot.juxtapose</a></code> function, which displays <code>ISM</code> by
default, or optionally <code>IAT</code>. For more information, see the
<code><a href="#topic+plot.juxtapose">plot.juxtapose</a></code> function.
</p>
<p>Independent samples per minute, calculated as <code><a href="#topic+ESS">ESS</a></code>
divided by minutes of run-time, are also available by parameter in the
<code><a href="#topic+PosteriorChecks">PosteriorChecks</a></code> function.
</p>


<h3>Value</h3>

<p>This function returns an object of class <code>juxtapose</code>. It is a
<code class="reqn">9 \times J</code> matrix with nine results for <code class="reqn">J</code> MCMC
algorithms.
</p>


<h3>References</h3>

<p>Thompson, M. (2010). &quot;Graphical Comparison of MCMC Performance&quot;. ArXiv
e-prints, eprint 1011.4458.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+IAT">IAT</a></code>,
<code><a href="#topic+is.juxtapose">is.juxtapose</a></code>,
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>,
<code><a href="#topic+LaplacesDemon.hpc">LaplacesDemon.hpc</a></code>,
<code><a href="#topic+plot.juxtapose">plot.juxtapose</a></code>, and
<code><a href="#topic+PosteriorChecks">PosteriorChecks</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### Update three demonoid objects, each from different MCMC algorithms.
### Suppose Fit1 was updated with AFSS, Fit2 with AMWG, and
### Fit3 with NUTS. Then, compare the inefficiencies:
#Juxt &lt;- Juxtapose(list(Fit1=Fit1, Fit2=Fit2, Fit3=Fit3)); Juxt
#plot(Juxt, Style="ISM")
</code></pre>

<hr>
<h2 id='KLD'>Kullback-Leibler Divergence (KLD)</h2><span id='topic+KLD'></span>

<h3>Description</h3>

<p>This function calculates the Kullback-Leibler divergence (KLD) between
two probability distributions, and has many uses, such as in lowest
posterior loss probability intervals, posterior predictive checks,
prior elicitation, reference priors, and Variational Bayes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>KLD(px, py, base)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="KLD_+3A_px">px</code></td>
<td>
<p>This is a required vector of probability densities,
considered as <code class="reqn">p(\textbf{x})</code>. Log-densities are also
accepted, in which case both <code>px</code> and <code>py</code> must be
log-densities.</p>
</td></tr>
<tr><td><code id="KLD_+3A_py">py</code></td>
<td>
<p>This is a required vector of probability densities,
considered as <code class="reqn">p(\textbf{y})</code>. Log-densities are also
accepted, in which case both <code>px</code> and <code>py</code> must be
log-densities.</p>
</td></tr>
<tr><td><code id="KLD_+3A_base">base</code></td>
<td>
<p>This optional argument specifies the logarithmic base,
which defaults to <code>base=exp(1)</code> (or <code class="reqn">e</code>) and represents
information in natural units (nats), where <code>base=2</code> represents
information in binary units (bits).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Kullback-Leibler divergence (KLD) is known by many names, some of
which are Kullback-Leibler distance, K-L, and logarithmic divergence.
KLD is an asymmetric measure of the difference, distance, or direct
divergence between two probability distributions
<code class="reqn">p(\textbf{y})</code> and <code class="reqn">p(\textbf{x})</code> (Kullback and
Leibler, 1951). Mathematically, however, KLD is not a distance,
because of its asymmetry.
</p>
<p>Here, <code class="reqn">p(\textbf{y})</code> represents the
&ldquo;true&rdquo; distribution of data, observations, or theoretical
distribution, and <code class="reqn">p(\textbf{x})</code> represents a theory,
model, or approximation of <code class="reqn">p(\textbf{y})</code>.
</p>
<p>For probability distributions <code class="reqn">p(\textbf{y})</code> and
<code class="reqn">p(\textbf{x})</code> that are discrete (whether the underlying
distribution is continuous or discrete, the observations themselves
are always discrete, such as from <code class="reqn">i=1,\dots,N</code>),
</p>
<p style="text-align: center;"><code class="reqn">\mathrm{KLD}[p(\textbf{y}) || p(\textbf{x})] = \sum^N_i
  p(\textbf{y}_i)
  \log\frac{p(\textbf{y}_i)}{p(\textbf{x}_i)}</code>
</p>

<p>In Bayesian inference, KLD can be used as a measure of the information
gain in moving from a prior distribution, <code class="reqn">p(\theta)</code>,
to a posterior distribution, <code class="reqn">p(\theta | \textbf{y})</code>. As such, KLD is the basis of reference priors and lowest
posterior loss intervals (<code><a href="#topic+LPL.interval">LPL.interval</a></code>), such as in
Berger, Bernardo, and Sun (2009) and Bernardo (2005). The intrinsic
discrepancy was introduced by Bernardo and Rueda (2002). For more
information on the intrinsic discrepancy, see
<code><a href="#topic+LPL.interval">LPL.interval</a></code>.
</p>


<h3>Value</h3>

<p><code>KLD</code> returns a list with the following components:
</p>
<table>
<tr><td><code>KLD.px.py</code></td>
<td>
<p>This is <code class="reqn">\mathrm{KLD}_i[p(\textbf{x}_i) ||
      p(\textbf{y}_i)]</code>.</p>
</td></tr>
<tr><td><code>KLD.py.px</code></td>
<td>
<p>This is <code class="reqn">\mathrm{KLD}_i[p(\textbf{y}_i) ||
      p(\textbf{x}_i)]</code>.</p>
</td></tr>
<tr><td><code>mean.KLD</code></td>
<td>
<p>This is the mean of the two components above. This is
the expected posterior loss in <code><a href="#topic+LPL.interval">LPL.interval</a></code>.</p>
</td></tr>
<tr><td><code>sum.KLD.px.py</code></td>
<td>
<p>This is <code class="reqn">\mathrm{KLD}[p(\textbf{x}) ||
      p(\textbf{y})]</code>. This is a directed
divergence.</p>
</td></tr>
<tr><td><code>sum.KLD.py.px</code></td>
<td>
<p>This is <code class="reqn">\mathrm{KLD}[p(\textbf{y}) ||
      p(\textbf{x})]</code>. This is a directed divergence.</p>
</td></tr>
<tr><td><code>mean.sum.KLD</code></td>
<td>
<p>This is the mean of the two components above.</p>
</td></tr>
<tr><td><code>intrinsic.discrepancy</code></td>
<td>
<p>This is minimum of the two directed
divergences.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>References</h3>

<p>Berger, J.O., Bernardo, J.M., and Sun, D. (2009). &quot;The Formal
Definition of Reference Priors&quot;. <em>The Annals of Statistics</em>,
37(2), p. 905&ndash;938.
</p>
<p>Bernardo, J.M. and Rueda, R. (2002). &quot;Bayesian Hypothesis Testing: A
Reference Approach&quot;. <em>International Statistical Review</em>, 70,
p. 351&ndash;372.
</p>
<p>Bernardo, J.M. (2005). &quot;Intrinsic Credible Regions: An Objective
Bayesian Approach to Interval Estimation&quot;. <em>Sociedad de
Estadistica e Investigacion Operativa</em>, 14(2), p. 317&ndash;384.
</p>
<p>Kullback, S. and Leibler, R.A. (1951). &quot;On Information and
Sufficiency&quot;. <em>The Annals of Mathematical Statistics</em>, 22(1),
p. 79&ndash;86.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+LPL.interval">LPL.interval</a></code> and
<code><a href="#topic+VariationalBayes">VariationalBayes</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
px &lt;- dnorm(runif(100),0,1)
py &lt;- dnorm(runif(100),0.1,0.9)
KLD(px,py)
</code></pre>

<hr>
<h2 id='KS.Diagnostic'>Kolmogorov-Smirnov Convergence Diagnostic</h2><span id='topic+KS.Diagnostic'></span>

<h3>Description</h3>

<p>The Kolmogorov-Smirnov test is a nonparametric test of stationarity
that has been applied as an MCMC diagnostic (Brooks et al, 2003), such
as to the posterior samples from the <code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>
function. The first and last halves of the chain are compared. This
test assumes IID, which is violated in the presence of
autocorrelation.
</p>
<p>The <code>KS.Diagnostic</code> is a univariate diagnostic that is usually
applied to each marginal posterior distribution. A multivariate form
is not included. By chance alone due to multiple independent tests,
5% of the marginal posterior distributions should appear
non-stationary when stationarity exists. Assessing multivariate 
convergence is difficult.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>KS.Diagnostic(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="KS.Diagnostic_+3A_x">x</code></td>
<td>
<p>This is a vector of posterior samples for which a
Kolmogorov-Smirnov test will be applied that compares the first and
last halves for stationarity.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>There are two main approaches to using the Kolmogorov-Smirnov test as
an MCMC diagnostic. There is a version of the test that has
been adapted to account for autocorrelation (and is not included
here). Otherwise, the chain is thinned enough that autocorrelation is
not present or is minimized, in which case the two-sample
Kolmogorov-Smirnov test is applied. The CDFs of both samples are
compared. The <code>ks.test</code> function in base R is used.
</p>
<p>The advantage of the Kolmogorov-Smirnov test is that it is easier and
faster to calculate. The disadvantages are that autocorrelation biases
results, and the test is generally biased on the conservative side
(indicating stationarity when it should not).
</p>


<h3>Value</h3>

<p>The <code>KS.Diagnostic</code> function returns a frequentist p-value, and
stationarity is indicated when p &gt; 0.05.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>References</h3>

<p>Brooks, S.P., Giudici, P., and Philippe, A. (2003). &quot;Nonparametric
Convergence Assessment for MCMC Model Selection&quot;. <em>Journal of
Computational and Graphical Statistics</em>. 12(1), p. 1&ndash;22.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+is.stationary">is.stationary</a></code>,
<code><a href="stats.html#topic+ks.test">ks.test</a></code>, and
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
x &lt;- rnorm(1000)
KS.Diagnostic(x)
</code></pre>

<hr>
<h2 id='LaplaceApproximation'>Laplace Approximation</h2><span id='topic+LaplaceApproximation'></span>

<h3>Description</h3>

<p>The <code>LaplaceApproximation</code> function deterministically maximizes
the logarithm of the unnormalized joint posterior density with one of
several optimization algorithms. The goal of Laplace Approximation is
to estimate the posterior mode and variance of each parameter. This
function is useful for optimizing initial values and estimating a
covariance matrix to be input into the
<code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code>, <code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>,
<code><a href="#topic+PMC">PMC</a></code>, or <code><a href="#topic+VariationalBayes">VariationalBayes</a></code> function, or
sometimes for model estimation in its own right.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>LaplaceApproximation(Model, parm, Data, Interval=1.0E-6,
     Iterations=100, Method="SPG", Samples=1000, CovEst="Hessian",
     sir=TRUE, Stop.Tolerance=1.0E-5, CPUs=1, Type="PSOCK")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="LaplaceApproximation_+3A_model">Model</code></td>
<td>
<p>This required argument receives the model from a
user-defined function. The user-defined function is where the model
is specified. <code>LaplaceApproximation</code> passes two arguments to
the model function, <code>parms</code> and <code>Data</code>. For more
information, see the <code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code> function and
&ldquo;LaplacesDemon Tutorial&rdquo; vignette.</p>
</td></tr>
<tr><td><code id="LaplaceApproximation_+3A_parm">parm</code></td>
<td>
<p>This argument requires a vector of initial values equal in
length to the number of parameters. <code>LaplaceApproximation</code> will
attempt to optimize these initial values for the parameters, where
the optimized values are the posterior modes, for later use with the
<code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code>, <code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>,
<code><a href="#topic+PMC">PMC</a></code>, or the <code><a href="#topic+VariationalBayes">VariationalBayes</a></code> function.
The <code><a href="#topic+GIV">GIV</a></code> function may be used to randomly generate
initial values. Parameters must be continuous.</p>
</td></tr>
<tr><td><code id="LaplaceApproximation_+3A_data">Data</code></td>
<td>
<p>This required argument accepts a list of data. The list of
data must include <code>mon.names</code> which contains monitored variable
names, and <code>parm.names</code> which contains parameter
names. <code>LaplaceApproximation</code> must be able to determine the
sample size of the data, and will look for a scalar sample size
variable <code>n</code> or <code>N</code>. If not found, it will look for
variable <code>y</code> or <code>Y</code>, and attempt to take its number of
rows as sample size. <code>LaplaceApproximation</code> needs to determine
sample size due to the asymptotic nature of this method. Sample size
should be at least <code class="reqn">\sqrt{J}</code> with <code class="reqn">J</code> exchangeable
parameters.</p>
</td></tr>
<tr><td><code id="LaplaceApproximation_+3A_interval">Interval</code></td>
<td>
<p>This argument receives an interval for estimating
approximate gradients. The logarithm of the unnormalized joint
posterior density of the Bayesian model is evaluated at the current
parameter value, and again at the current parameter value plus this
interval.</p>
</td></tr>
<tr><td><code id="LaplaceApproximation_+3A_iterations">Iterations</code></td>
<td>
<p>This argument accepts an integer that determines the
number of iterations that <code>LaplaceApproximation</code> will attempt
to maximize the logarithm of the unnormalized joint posterior
density. <code>Iterations</code> defaults to 100.
<code>LaplaceApproximation</code> will stop before this number of
iterations if the tolerance is less than or equal to the
<code>Stop.Tolerance</code> criterion. The required amount of computer
memory increases with <code>Iterations</code>. If computer memory is
exceeded, then all will be lost.</p>
</td></tr>
<tr><td><code id="LaplaceApproximation_+3A_method">Method</code></td>
<td>
<p>This optional argument accepts a quoted string that
specifies the method used for Laplace Approximation. The default
method is <code>Method="SPG"</code>. Options include <code>"AGA"</code> for
adaptive gradient ascent, <code>"BFGS"</code> for the
Broyden-Fletcher-Goldfarb-Shanno algorithm, <code>"BHHH"</code> for the
algorithm of Berndt et al., <code>"CG"</code> for conjugate gradient,
<code>"DFP"</code> for the Davidon-Fletcher-Powell algorithm,
<code>"HAR"</code> for adaptive hit-and-run, <code>"HJ"</code> for
Hooke-Jeeves, <code>"LBFGS"</code> for limited-memory BFGS, <code>"LM"</code>
for Levenberg-Marquardt, <code>"NM"</code> for Nelder-Mead, <code>"NR"</code>
for Newton-Raphson, <code>"PSO"</code> for Particle Swarm Optimization,
<code>"Rprop"</code> for resilient backpropagation, <code>"SGD"</code> for
Stochastic Gradient Descent, <code>"SOMA"</code> for the Self-Organizing
Migration Algorithm, <code>"SPG"</code> for Spectral Projected Gradient,
<code>"SR1"</code> for Symmetric Rank-One, and <code>"TR"</code> for Trust
Region.</p>
</td></tr>
<tr><td><code id="LaplaceApproximation_+3A_samples">Samples</code></td>
<td>
<p>This argument indicates the number of posterior samples
to be taken with sampling importance resampling via the
<code><a href="#topic+SIR">SIR</a></code> function, which occurs only when
<code>sir=TRUE</code>. Note that the number of samples should increase
with the number and intercorrelations of the parameters.</p>
</td></tr>
<tr><td><code id="LaplaceApproximation_+3A_covest">CovEst</code></td>
<td>
<p>This argument accepts a quoted string that indicates how
the covariance matrix is estimated after the model finishes. This
covariance matrix is used to obtain the standard deviation of each
parameter, and may also be used for posterior sampling via Sampling
Importance Resampling (SIR) (see the <code>sir</code> argument below), if
converged. By default, the covariance matrix is approximated as the
negative inverse of the <code>"Hessian"</code> matrix of second
derivatives, estimated with Richardson extrapolation. Alternatives
include <code>CovEst="Identity"</code>, <code>CovEst="OPG"</code>, or
<code>CovEst="Sandwich"</code>. When <code>CovEst="Identity"</code>, the
covariance matrix is not estimated, and is merely assigned an
identity matrix. When <code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code> is
performed internally by <code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>, an identity
matrix is returned and scaled. When <code>CovEst="OPG"</code>, the
covariance matrix is approximated with the inverse of the sum of the
outer products of the gradient, which requires <code>X</code>, and either
<code>y</code> or <code>Y</code> in the list of data. For OPG, a partial
derivative is taken for each row in <code>X</code>, and each element in
<code>y</code> or row in <code>Y</code>. Therefore, this requires <code class="reqn">N + NJ</code>
model evaluations for a data set with <code class="reqn">N</code> records and <code class="reqn">J</code>
variables. The OPG method is an asymptotic approximation of
the Hessian, and usually requires fewer calculations with a small
data set, or more with large data sets. Both methods require a
matrix inversion, which becomes costly as dimension grows. The
Richardson-based Hessian method is more accurate, but requires more
calculation in large dimensions. An alternative approach to
obtaining covariance is to use the <code><a href="#topic+BayesianBootstrap">BayesianBootstrap</a></code>
on the data, or sample the posterior with iterative quadrature
(<code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code>), MCMC
(<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>), or <code><a href="#topic+VariationalBayes">VariationalBayes</a></code>.</p>
</td></tr>
<tr><td><code id="LaplaceApproximation_+3A_sir">sir</code></td>
<td>
<p>This logical argument indicates whether or not Sampling
Importance Resampling (SIR) is conducted via the <code><a href="#topic+SIR">SIR</a></code>
function to draw independent posterior samples. This argument
defaults to <code>TRUE</code>. Even when <code>TRUE</code>, posterior samples
are drawn only when <code>LaplaceApproximation</code> has
converged. Posterior samples are required for many other functions,
including <code>plot.laplace</code> and <code>predict.laplace</code>. The only
time that it is advantageous for <code>sir=FALSE</code> is when
<code>LaplaceApproximation</code> is used to help the initial values for
<code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code>, <code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>,
<code><a href="#topic+PMC">PMC</a></code>, or <code><a href="#topic+VariationalBayes">VariationalBayes</a></code>, and it is
unnecessary for time to be spent on sampling. Less time can be spent
on sampling by increasing <code>CPUs</code>, which parallelizes the
sampling.</p>
</td></tr>
<tr><td><code id="LaplaceApproximation_+3A_stop.tolerance">Stop.Tolerance</code></td>
<td>
<p>This argument accepts any positive number and
defaults to 1.0E-5. Tolerance is calculated each iteration, and the
criteria varies by algorithm. The algorithm is considered to have
converged to the user-specified <code>Stop.Tolerance</code> when the
tolerance is less than or equal to the value of
<code>Stop.Tolerance</code>, and the algorithm terminates at the end of
the current iteration. Often, multiple criteria are used, in
which case the maximum of all criteria becomes the tolerance. For
example, when partial derivatives are taken, it is commonly required
that the Euclidean norm of the partial derivatives is a criterion,
and another common criterion is the Euclidean norm of the
differences between the current and previous parameter values.
Several algorithms have other, specific tolerances.</p>
</td></tr>
<tr><td><code id="LaplaceApproximation_+3A_cpus">CPUs</code></td>
<td>
<p>This argument accepts an integer that specifies the number
of central processing units (CPUs) of the multicore computer or
computer cluster. This argument defaults to <code>CPUs=1</code>, in which
parallel processing does not occur. Parallelization occurs only for
sampling with <code><a href="#topic+SIR">SIR</a></code> when <code>sir=TRUE</code>.</p>
</td></tr>
<tr><td><code id="LaplaceApproximation_+3A_type">Type</code></td>
<td>
<p>This argument specifies the type of parallel processing to
perform, accepting either <code>Type="PSOCK"</code> or
<code>Type="MPI"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Laplace Approximation or Laplace Method is a family of asymptotic
techniques used to approximate integrals. Laplace's method accurately
approximates unimodal posterior moments and marginal posterior
distributions in many cases. Since it is not applicable in
all cases, it is recommended here that Laplace Approximation is used
cautiously in its own right, or preferably, it is used before MCMC.
</p>
<p>After introducing the Laplace Approximation (Laplace, 1774,
p. 366&ndash;367), a proof was published later (Laplace, 1814) as part of
a mathematical system of inductive reasoning based on probability.
Laplace used this method to approximate posterior moments.
</p>
<p>Since its introduction, the Laplace Approximation has been applied
successfully in many disciplines. In the 1980s, the Laplace
Approximation experienced renewed interest, especially in statistics,
and some improvements in its implementation were introduced (Tierney
et al., 1986; Tierney et al., 1989). Only since the 1980s has the
Laplace Approximation been seriously considered by statisticians in
practical applications.
</p>
<p>There are many variations of Laplace Approximation, with an effort
toward replacing Markov chain Monte Carlo (MCMC) algorithms as the
dominant form of numerical approximation in Bayesian inference. The
run-time of Laplace Approximation is a little longer than Maximum
Likelihood Estimation (MLE), usually shorter than variational Bayes,
and much shorter than MCMC (Azevedo and Shachter, 1994).
</p>
<p>The speed of Laplace Approximation depends on the optimization
algorithm selected, and typically involves many evaluations of the
objective function per iteration (where an MCMC algorithm with a
multivariate proposal usually evaluates once per iteration), making
many MCMC algorithms faster per iteration. The attractiveness
of Laplace Approximation is that it typically improves the objective
function better than iterative quadrature, MCMC, and PMC when the
parameters are in low-probability regions. Laplace Approximation is
also typically faster than MCMC and PMC because it is seeking
point-estimates, rather than attempting to represent the target
distribution with enough simulation draws. Laplace Approximation
extends MLE, but shares similar limitations, such as its asymptotic
nature with respect to sample size and that marginal posterior
distributions are Gaussian. Bernardo and Smith (2000) note that
Laplace Approximation is an attractive family of numerical
approximation algorithms, and will continue to develop.
</p>
<p><code>LaplaceApproximation</code> seeks a global maximum of the logarithm of
the unnormalized joint posterior density. The approach differs by
<code>Method</code>. The <code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code> function uses the
<code>LaplaceApproximation</code> algorithm to optimize initial values and
save time for the user.
</p>
<p>Most optimization algorithms assume that the logarithm of the
unnormalized joint posterior density is defined and differentiable.
Some methods calculate an approximate gradient for each initial value
as the difference in the logarithm of the unnormalized joint posterior
density due to a slight increase in the parameter.
</p>
<p>When <code>Method="AGA"</code>, the direction and distance for each
parameter is proposed based on an approximate truncated gradient and
an adaptive step size. The step size parameter, which is often plural
and called rate parameters in other literature, is adapted each
iteration with the univariate version of the Robbins-Monro stochastic
approximation in Garthwaite (2010). The step size shrinks when a
proposal is rejected and expands when a proposal is accepted.
</p>
<p>Gradient ascent is criticized for sometimes being relatively slow when
close to the maximum, and its asymptotic rate of convergence is
inferior to other methods. However, compared to other popular
optimization algorithms such as Newton-Raphson, an advantage of the
gradient ascent is that it works in infinite dimensions, requiring
only sufficient computer memory. Although Newton-Raphson converges in
fewer iterations, calculating the inverse of the negative Hessian
matrix of second-derivatives is more computationally expensive and
subject to singularities. Therefore, gradient ascent takes longer to
converge, but is more generalizable.
</p>
<p>When <code>Method="BFGS"</code>, the BFGS algorithm is used, which was
proposed by Broyden (1970), Fletcher (1970), Goldfarb (1970), and
Shanno (1970), independently. BFGS may be the most efficient and
popular quasi-Newton optimiziation algorithm. As a quasi-Newton
algorithm, the Hessian matrix is approximated using rank-one updates
specified by (approximate) gradient evaluations. Since BFGS is very
popular, there are many variations of it. This is a version by Nash
that has been adapted from the Rvmmin package, and is used in the
<code>optim</code> function of base R. The approximate Hessian is not
guaranteed to converge to the Hessian. When BFGS is used, the
approximate Hessian is not used to calculate the final covariance
matrix.
</p>
<p>When <code>Method="BHHH"</code>, the algorithm of Berndt et al. (1974) is
used, which is commonly pronounced B-triple H. The BHHH algorithm is a
quasi-Newton method that includes a step-size parameter, partial
derivatives, and an approximation of a covariance matrix that is
calculated as the inverse of the sum of the outer product of the
gradient (OPG), calculated from each record. The OPG method becomes
more costly with data sets with more records. Since partial
derivatives must be calculated per record of data, the list of data
has special requirements with this method, and must include design
matrix <code>X</code>, and dependent variable <code>y</code> or <code>Y</code>. Records
must be row-wise. An advantage of BHHH over NR (see below) is that
the covariance matrix is necessarily positive definite, and gauranteed
to provide an increase in LP each iteration (given a small enough
step-size), even in convex areas. The covariance matrix is better
approximated with larger data sample sizes, and when closer to the
maximum of LP. Disadvantages of BHHH include that it can give small
increases in LP, especially when far from the maximum or when LP is
highly non-quadratic.
</p>
<p>When <code>Method="CG"</code>, a nonlinear conjugate gradient algorithm is
used. CG uses partial derivatives, but does not use the Hessian matrix
or any approximation of it. CG usually requires more iterations to
reach convergence than other algorithms that use the Hessian or an
approximation. However, since the Hessian becomes computationally
expensive as the dimension of the model grows, CG is applicable to
large dimensional models when <code>CovEst="Hessian"</code> is avoided.
CG was originally developed by Hestenes and Stiefel (1952), though
this version is adapted from the <code>Rcgminu</code> function in package
Rcgmin.
</p>
<p>When <code>Method="DFP"</code>, the Davidon-Fletcher-Powell algorithm is
used. DFP was the first popular, multidimensional, quasi-Newton
optimization algorithm. The DFP update of an approximate Hessian
matrix maintains symmetry and positive-definiteness. The approximate
Hessian is not guaranteed to converge to the Hessian. When DFP is
used, the approximate Hessian is not used to calculate the final
covariance matrix. Although DFP is very effective, it was superseded
by the BFGS algorithm.
</p>
<p>When <code>Method="HAR"</code>, a hit-and-run algorithm with a multivariate
proposal and adaptive length is used. The length parameter is adapted
each iteration with the univariate version of the Robbins-Monro
stochastic approximation in Garthwaite (2010). The length shrinks when
a proposal is rejected and expands when a proposal is accepted. This
is the same algorithm as the HARM or Hit-And-Run Metropolis MCMC
algorithm with adaptive length, except that a Metropolis step is not
used.
</p>
<p>When <code>Method="HJ"</code>, the Hooke-Jeeves (1961) algorithm is used.
This was adapted from the <code>HJK</code> algorithm in package dfoptim.
Hooke-Jeeves is a derivative-free, direct search method. Each iteration
involves two steps: an exploratory move and a pattern move. The
exploratory move explores local behavior, and the pattern move takes
advantage of pattern direction. It is sometimes described as a
hill-climbing algorithm. If the solution improves, it accepts the
move, and otherwise rejects it. Step size decreases with each
iteration. The decreasing step size can trap it in local maxima, where
it gets stuck and convergences erroneously. Users are encouraged to
attempt again after what seems to be convergence, starting from the
latest point. Although getting stuck at local maxima can be
problematic, the Hooke-Jeeves algorithm is also attractive because it
is simple, fast, does not depend on derivatives, and is otherwise
relatively robust.
</p>
<p>When <code>Method="LBFGS"</code>, the limited-memory BFGS
(Broyden-Fletcher-Goldfarb-Shanno) algorithm is called in
<code>optim</code>, once per iteration.
</p>
<p>When <code>Method="LM"</code>, the Levenberg-Marquardt algorithm (Levenberg,
1944; Marquardt, 1963) is used. Also known as the Levenberg-Marquardt
Algorithm (LMA) or the Damped Least-Squares (DLS) method, LM is a
trust region (not to be confused with TR below) quasi-Newton
optimization algorithm that provides minimizes nonlinear least
squares, and has been adapted here to maximize LP. LM uses partial
derivatives and approximates the Hessian with outer-products. It is
suitable for nonlinear optimization up to a few hundred parameters,
but loses its efficiency in larger problems due to matrix inversion.
LM is considered between the Gauss-Newton algorithm and gradient
descent. When far from the solution, LM moves slowly like gradient
descent, but is guaranteed to converge. When LM is close to the
solution, LM becomes a damped Gauss-Newton method. This was adapted
from the <code>lsqnonlin</code> algorithm in package pracma.
</p>
<p>When <code>Method="NM"</code>, the Nelder-Mead (1965) algorithm is
used. This was adapted from the <code>nelder_mead</code> function in package
pracma. Nelder-Mead is a derivative-free, direct search method that is
known to become inefficient in large-dimensional problems. As the
dimension increases, the search direction becomes increasingly
orthogonal to the steepest ascent (usually descent)
direction. However, in smaller dimensions, it is a popular algorithm.
At each iteration, three steps are taken to improve a simplex:
reflection, extension, and contraction.
</p>
<p>When <code>Method="NR"</code>, the Newton-Raphson optimization algorithm,
also known as Newton's Method, is used. Newton-Raphson uses
derivatives and a Hessian matrix. The algorithm is included for its
historical significance, but is known to be problematic when starting
values are far from the targets, and calculating and inverting the
Hessian matrix can be computationally expensive. As programmed here,
when the Hessian is problematic, it tries to use only the derivatives,
and when that fails, a jitter is applied. Newton-Raphson should not
be the first choice of the user, and BFGS should always be preferred.
</p>
<p>When <code>Method="PSO"</code>, the Standard Particle Swarm Optimization
2007 algorithm is used. A swarm of particles is moved according
to velocity, neighborhood, and the best previous solution. The
neighborhood for each particle is a set of informing particles. PSO
is derivative-free. PSO has been adapted from the <code>psoptim</code>
function in package pso.
</p>
<p>When <code>Method="Rprop"</code>, the approximate gradient is taken for each
parameter in each iteration, and its sign is compared to the
approximate gradient in the previous iteration. A weight element in a
weight vector is associated with each approximate gradient. A weight
element is multiplied by 1.2 when the sign does not change, or by 0.5
if the sign changes. The weight vector is the step size, and is
constrained to the interval [0.001, 50], and initial weights are
0.0125. This is the resilient backpropagation algorithm, which is
often denoted as the &ldquo;Rprop-&rdquo; algorithm of Riedmiller (1994).
</p>
<p>When <code>Method="SGD"</code>, a stochastic gradient descent algorithm is
used that is designed only for big data, and gained popularity after
successful use in the NetFlix competition. This algorithm has special
requirements for the <code>Model</code> specification function and the
<code>Data</code> list. See the &ldquo;LaplacesDemon Tutorial&rdquo; vignette for more
information.
</p>
<p>When <code>Method="SOMA"</code>, a population of ten particles or
individuals moves in the direction of the best particle, the leader.
The leader does not move in each iteration, and a line-search is used
for each non-leader, up to three times the difference in parameter
values between each non-leader and leader. This algorithm is
derivative-free and often considered in the family of evolution
algorithms. Numerous model evaluations are performed per non-leader
per iteration. This algorithm was adapted from package soma.
</p>
<p>When <code>Method="SPG"</code>, a Spectral Projected Gradient algorithm
is used. SPG is a non-monotone algorithm that is suitable for
high-dimensional models. The approximate gradient is used, but the
Hessian matrix is not. When used with large models,
<code>CovEst="Hessian"</code> should be avoided. SPG has been adapted from
the <code>spg</code> function in package BB.
</p>
<p>When <code>Method="SR1"</code>, the Symmetric Rank-One (SR1) algorithm is
used. SR1 is a quasi-Newton algorithm, and the Hessian matrix is
approximated, often without being positive-definite. At the posterior
modes, the true Hessian is usually positive-definite, but this is
often not the case during optimization when the parameters have not
yet reached the posterior modes. Other restrictions, including
constraints, often result in the true Hessian being indefinite at the
solution. For these reasons, SR1 often outperforms BFGS. The
approximate Hessian is not guaranteed to converge to the Hessian. When
SR1 is used, the approximate Hessian is not used to calculate the
final covariance matrix.
</p>
<p>When <code>Method="TR"</code>, the Trust Region algorithm of Nocedal and
Wright (1999) is used. The TR algorithm attempts to reach its
objective in the fewest number of iterations, is therefore very
efficient, as well as safe. The efficiency of TR is attractive when
model evaluations are expensive. The Hessian is approximated each
iteration, making TR best suited to models with small to medium
dimensions, say up to a few hundred parameters. TR has been adapted
from the <code>trust</code> function in package trust.
</p>


<h3>Value</h3>

<p><code>LaplaceApproximation</code> returns an object of class <code>laplace</code>
that is a list with the following components:
</p>
<table>
<tr><td><code>Call</code></td>
<td>
<p>This is the matched call of <code>LaplaceApproximation</code>.</p>
</td></tr>
<tr><td><code>Converged</code></td>
<td>
<p>This is a logical indicator of whether or not
<code>LaplaceApproximation</code> converged within the specified
<code>Iterations</code> according to the supplied <code>Stop.Tolerance</code>
criterion. Convergence does not indicate that the global maximum has
been found, but only that the tolerance was less than or equal to
the <code>Stop.Tolerance</code> criterion.</p>
</td></tr>
<tr><td><code>Covar</code></td>
<td>
<p>This covariance matrix is estimated according to the
<code>CovEst</code> argument. The <code>Covar</code> matrix may be scaled and
input into the <code>Covar</code> argument of the
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code> or <code><a href="#topic+PMC">PMC</a></code> function for
further estimation, or the diagonal of this matrix may be used
to represent the posterior variance of the parameters, provided the
algorithm converged and matrix inversion was successful. To scale
this matrix for use with Laplace's Demon or PMC, multiply it by
<code class="reqn">2.38^2/d</code>, where <code class="reqn">d</code> is the number of initial values.</p>
</td></tr>
<tr><td><code>Deviance</code></td>
<td>
<p>This is a vector of the iterative history of the
deviance in the <code>LaplaceApproximation</code> function, as it sought
convergence.</p>
</td></tr>
<tr><td><code>History</code></td>
<td>
<p>This is a matrix of the iterative history of the
parameters in the <code>LaplaceApproximation</code> function, as it sought
convergence.</p>
</td></tr>
<tr><td><code>Initial.Values</code></td>
<td>
<p>This is the vector of initial values that was
originally given to <code>LaplaceApproximation</code> in the <code>parm</code>
argument.</p>
</td></tr>
<tr><td><code>LML</code></td>
<td>
<p>This is an approximation of the logarithm of the marginal
likelihood of the data (see the <code><a href="#topic+LML">LML</a></code> function for more
information). When the model has converged and <code>sir=TRUE</code>, the
NSIS method is used. When the model has converged and
<code>sir=FALSE</code>, the LME method is used. This is the
logarithmic form of equation 4 in Lewis and Raftery (1997). As a
rough estimate of Kass and Raftery (1995), the LME-based LML is
worrisome when the sample size of the data is less than five times
the number of parameters, and <code>LML</code> should be adequate in most
problems when the sample size of the data exceeds twenty times the
number of parameters (p. 778). The LME is inappropriate with
hierarchical models. However <code>LML</code> is estimated, it is useful
for comparing multiple models with the <code>BayesFactor</code> function.</p>
</td></tr>
<tr><td><code>LP.Final</code></td>
<td>
<p>This reports the final scalar value for the logarithm
of the unnormalized joint posterior density.</p>
</td></tr>
<tr><td><code>LP.Initial</code></td>
<td>
<p>This reports the initial scalar value for the
logarithm of the unnormalized joint posterior density.</p>
</td></tr>
<tr><td><code>Minutes</code></td>
<td>
<p>This is the number of minutes that
<code>LaplaceApproximation</code> was running, and this includes the
initial checks as well as drawing posterior samples and creating
summaries.</p>
</td></tr>
<tr><td><code>Monitor</code></td>
<td>
<p>When <code>sir=TRUE</code>, a number of independent
posterior samples equal to <code>Samples</code> is taken, and the draws
are stored here as a matrix. The rows of the matrix are the samples,
and the columns are the monitored variables.</p>
</td></tr>
<tr><td><code>Posterior</code></td>
<td>
<p>When <code>sir=TRUE</code>, a number of independent
posterior samples equal to <code>Samples</code> is taken, and the draws
are stored here as a matrix. The rows of the matrix are the samples,
and the columns are the parameters.</p>
</td></tr>
<tr><td><code>Step.Size.Final</code></td>
<td>
<p>This is the final, scalar <code>Step.Size</code>
value at the end of the <code>LaplaceApproximation</code> algorithm.</p>
</td></tr>
<tr><td><code>Step.Size.Initial</code></td>
<td>
<p>This is the initial, scalar <code>Step.Size</code>.</p>
</td></tr>
<tr><td><code>Summary1</code></td>
<td>
<p>This is a summary matrix that summarizes the
point-estimated posterior modes. Uncertainty around the posterior
modes is estimated from the covariance matrix. Rows are parameters.
The following columns are included: Mode, SD (Standard Deviation),
LB (Lower Bound), and UB (Upper Bound). The bounds constitute a 95%
probability interval.</p>
</td></tr>
<tr><td><code>Summary2</code></td>
<td>
<p>This is a summary matrix that summarizes the
posterior samples drawn with sampling importance resampling
(<code><a href="#topic+SIR">SIR</a></code>) when <code>sir=TRUE</code>, given the point-estimated
posterior modes and the covariance matrix. Rows are parameters. The
following columns are included: Mode, SD (Standard Deviation),
LB (Lower Bound), and UB (Upper Bound). The bounds constitute a 95%
probability interval.</p>
</td></tr>
<tr><td><code>Tolerance.Final</code></td>
<td>
<p>This is the last <code>Tolerance</code> of the
<code>LaplaceApproximation</code> algorithm.</p>
</td></tr>
<tr><td><code>Tolerance.Stop</code></td>
<td>
<p>This is the <code>Stop.Tolerance</code> criterion.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Statisticat, LLC <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>References</h3>

<p>Azevedo-Filho, A. and Shachter, R. (1994). &quot;Laplace's Method
Approximations for Probabilistic Inference in Belief Networks with
Continuous Variables&quot;. In &quot;Uncertainty in Artificial Intelligence&quot;,
Mantaras, R. and Poole, D., Morgan Kauffman, San Francisco, CA,
p. 28&ndash;36.
</p>
<p>Bernardo, J.M. and Smith, A.F.M. (2000). &quot;Bayesian Theory&quot;. John
Wiley \&amp; Sons: West Sussex, England.
</p>
<p>Berndt, E., Hall, B., Hall, R., and Hausman, J. (1974), &quot;Estimation
and Inference in Nonlinear Structural Models&quot;. <em>Annals of
Economic and Social Measurement</em>, 3, p. 653&ndash;665.
</p>
<p>Broyden, C.G. (1970). &quot;The Convergence of a Class of Double Rank
Minimization Algorithms: 2. The New Algorithm&quot;. Journal of the
Institute of Mathematics and its Applications, 6, p.76&ndash;90.
</p>
<p>Fletcher, R. (1970). &quot;A New Approach to Variable Metric Algorithms&quot;.
Computer Journal, 13(3), p. 317&ndash;322.
</p>
<p>Garthwaite, P., Fan, Y., and Sisson, S. (2010). &quot;Adaptive Optimal
Scaling of Metropolis-Hastings Algorithms Using the Robbins-Monro
Process.&quot;
</p>
<p>Goldfarb, D. (1970). &quot;A Family of Variable Metric Methods Derived
by Variational Means&quot;. Mathematics of Computation, 24(109), p. 23&ndash;26.
</p>
<p>Hestenes, M.R. and Stiefel, E. (1952). &quot;Methods of Conjugate Gradients
for Solving Linear Systems&quot;. <em>Journal of Research of the National
Bureau of Standards</em>, 49(6), p. 409&ndash;436.
</p>
<p>Hooke, R. and Jeeves, T.A. (1961). &quot;'Direct Search' Solution of
Numerical and Statistical Problems&quot;. <em>Journal of the Association
for Computing Machinery</em>, 8(2), p. 212&ndash;229. 
</p>
<p>Kass, R.E. and Raftery, A.E. (1995). &quot;Bayes Factors&quot;. <em>Journal
of the American Statistical Association</em>, 90(430), p. 773&ndash;795.
</p>
<p>Laplace, P. (1774). &quot;Memoire sur la Probabilite des Causes par les
Evenements.&quot; l'Academie Royale des Sciences, 6, 621&ndash;656. English
translation by S.M. Stigler in 1986 as &quot;Memoir on the Probability
of the Causes of Events&quot; in Statistical Science, 1(3), 359&ndash;378.
</p>
<p>Laplace, P. (1814). &quot;Essai Philosophique sur les Probabilites.&quot;
English translation in Truscott, F.W. and Emory, F.L. (2007) from
(1902) as &quot;A Philosophical Essay on Probabilities&quot;. ISBN
1602063281, translated from the French 6th ed. (1840).
</p>
<p>Levenberg, K. (1944). &quot;A Method for the Solution of Certain
Non-Linear Problems in Least Squares&quot;. <em>Quarterly of Applied
Mathematics</em>, 2, p. 164&ndash;168.
</p>
<p>Lewis, S.M. and Raftery, A.E. (1997). &quot;Estimating Bayes Factors via
Posterior Simulation with the Laplace-Metropolis
Estimator&quot;. <em>Journal of the American Statistical Association</em>,
92, p. 648&ndash;655.
</p>
<p>Marquardt, D. (1963). &quot;An Algorithm for Least-Squares Estimation of
Nonlinear Parameters&quot;. <em>SIAM Journal on Applied Mathematics</em>,
11(2), p. 431&ndash;441.
</p>
<p>Nelder, J.A. and Mead, R. (1965). &quot;A Simplex Method for Function
Minimization&quot;. <em>The Computer Journal</em>, 7(4), p. 308&ndash;313.
</p>
<p>Nocedal, J. and Wright, S.J. (1999). &quot;Numerical Optimization&quot;.
Springer-Verlag.
</p>
<p>Riedmiller, M. (1994). &quot;Advanced Supervised Learning in Multi-Layer
Perceptrons - From Backpropagation to Adaptive Learning
Algorithms&quot;. <em>Computer Standards and Interfaces</em>, 16,
p. 265&ndash;278.
</p>
<p>Shanno, D.F. (1970). &quot;Conditioning of quasi-Newton Methods for
Function Minimization&quot;. Mathematics of Computation, 24(111),
p. 647&ndash;650.
</p>
<p>Tierney, L. and Kadane, J.B. (1986). &quot;Accurate Approximations for
Posterior Moments and Marginal Densities&quot;. <em>Journal of the
American Statistical Association</em>, 81(393), p. 82&ndash;86.
</p>
<p>Tierney, L., Kass. R., and Kadane, J.B. (1989). &quot;Fully Exponential
Laplace Approximations to Expectations and Variances of Nonpositive
Functions&quot;. <em>Journal of the American Statistical Association</em>,
84(407), p. 710&ndash;716.
</p>
<p>Zelinka, I. (2004). &quot;SOMA - Self Organizing Migrating Algorithm&quot;. In:
Onwubolu G.C. and Babu, B.V., editors. &quot;New Optimization
Techniques in Engineering&quot;. Springer: Berlin, Germany.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+BayesFactor">BayesFactor</a></code>,
<code><a href="#topic+BayesianBootstrap">BayesianBootstrap</a></code>,
<code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code>,
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>,
<code><a href="#topic+GIV">GIV</a></code>,
<code><a href="#topic+LML">LML</a></code>,
<code><a href="stats.html#topic+optim">optim</a></code>,
<code><a href="#topic+PMC">PMC</a></code>,
<code><a href="#topic+SIR">SIR</a></code>, and
<code><a href="#topic+VariationalBayes">VariationalBayes</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># The accompanying Examples vignette is a compendium of examples.
####################  Load the LaplacesDemon Library  #####################
library(LaplacesDemon)

##############################  Demon Data  ###############################
data(demonsnacks)
y &lt;- log(demonsnacks$Calories)
X &lt;- cbind(1, as.matrix(log(demonsnacks[,10]+1)))
J &lt;- ncol(X)
for (j in 2:J) X[,j] &lt;- CenterScale(X[,j])

#########################  Data List Preparation  #########################
mon.names &lt;- "mu[1]"
parm.names &lt;- as.parm.names(list(beta=rep(0,J), sigma=0))
pos.beta &lt;- grep("beta", parm.names)
pos.sigma &lt;- grep("sigma", parm.names)
PGF &lt;- function(Data) {
     beta &lt;- rnorm(Data$J)
     sigma &lt;- runif(1)
     return(c(beta, sigma))
     }
MyData &lt;- list(J=J, PGF=PGF, X=X, mon.names=mon.names,
     parm.names=parm.names, pos.beta=pos.beta, pos.sigma=pos.sigma, y=y)

##########################  Model Specification  ##########################
Model &lt;- function(parm, Data)
     {
     ### Parameters
     beta &lt;- parm[Data$pos.beta]
     sigma &lt;- interval(parm[Data$pos.sigma], 1e-100, Inf)
     parm[Data$pos.sigma] &lt;- sigma
     ### Log-Prior
     beta.prior &lt;- sum(dnormv(beta, 0, 1000, log=TRUE))
     sigma.prior &lt;- dhalfcauchy(sigma, 25, log=TRUE)
     ### Log-Likelihood
     mu &lt;- tcrossprod(Data$X, t(beta))
     LL &lt;- sum(dnorm(Data$y, mu, sigma, log=TRUE))
     ### Log-Posterior
     LP &lt;- LL + beta.prior + sigma.prior
     Modelout &lt;- list(LP=LP, Dev=-2*LL, Monitor=mu[1],
          yhat=rnorm(length(mu), mu, sigma), parm=parm)
     return(Modelout)
     }

############################  Initial Values  #############################
#Initial.Values &lt;- GIV(Model, MyData, PGF=TRUE)
Initial.Values &lt;- rep(0,J+1)

Fit &lt;- LaplaceApproximation(Model, Initial.Values, Data=MyData,
     Iterations=100, Method="NM", CPUs=1)
Fit
print(Fit)
#PosteriorChecks(Fit)
#caterpillar.plot(Fit, Parms="beta")
#plot(Fit, MyData, PDF=FALSE)
#Pred &lt;- predict(Fit, Model, MyData, CPUs=1)
#summary(Pred, Discrep="Chi-Square")
#plot(Pred, Style="Covariates", Data=MyData)
#plot(Pred, Style="Density", Rows=1:9)
#plot(Pred, Style="Fitted")
#plot(Pred, Style="Jarque-Bera")
#plot(Pred, Style="Predictive Quantiles")
#plot(Pred, Style="Residual Density")
#plot(Pred, Style="Residuals")
#Levene.Test(Pred)
#Importance(Fit, Model, MyData, Discrep="Chi-Square")

#Fit$Covar is scaled (2.38^2/d) and submitted to LaplacesDemon as Covar.
#Fit$Summary[,1] is submitted to LaplacesDemon as Initial.Values.
#End
</code></pre>

<hr>
<h2 id='LaplacesDemon'>Laplace's Demon</h2><span id='topic+LaplacesDemon'></span><span id='topic+LaplacesDemon.hpc'></span>

<h3>Description</h3>

<p>The <code>LaplacesDemon</code> function is the main function of Laplace's
Demon. Given data, a model specification, and initial values,
<code>LaplacesDemon</code> maximizes the logarithm of the unnormalized joint
posterior density with MCMC and provides samples of the marginal
posterior distributions, deviance, and other monitored variables.
</p>
<p>The <code>LaplacesDemon.hpc</code> function extends <code>LaplacesDemon</code> to
parallel chains for multicore or cluster high performance computing.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>LaplacesDemon(Model, Data, Initial.Values, Covar=NULL, Iterations=10000,
     Status=100, Thinning=10, Algorithm="MWG", Specs=list(B=NULL),
     Debug=list(DB.chol=FALSE, DB.eigen=FALSE, DB.MCSE=FALSE,
     DB.Model=TRUE), LogFile="", ...)
LaplacesDemon.hpc(Model, Data, Initial.Values, Covar=NULL,
     Iterations=10000, Status=100, Thinning=10, Algorithm="MWG",
     Specs=list(B=NULL), Debug=list(DB.chol=FALSE, DB.eigen=FALSE,
     DB.MCSE=FALSE, DB.Model=TRUE), LogFile="", Chains=2, CPUs=2,
     Type="PSOCK", Packages=NULL, Dyn.libs=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="LaplacesDemon_+3A_model">Model</code></td>
<td>
<p>This required argument receives the model from a
user-defined function that must be named Model. The user-defined
function is where the model is specified. <code>LaplacesDemon</code>
passes two arguments to the model function, <code>parms</code> and
<code>Data</code>, and receives five arguments from the model function:
<code>LP</code> (the logarithm of the unnormalized joint posterior),
<code>Dev</code> (the deviance), <code>Monitor</code> (the monitored variables),
<code>yhat</code> (the variables for posterior predictive checks), and
<code>parm</code>, the vector of parameters, which may be constrained in
the model function. More information on the Model specification
function may be found in the &quot;LaplacesDemon Tutorial&quot; vignette, and
the <code><a href="#topic+is.model">is.model</a></code> function. Many examples of model
specification functions may be found in the &quot;Examples&quot; vignette.</p>
</td></tr>
<tr><td><code id="LaplacesDemon_+3A_data">Data</code></td>
<td>
<p>This required argument accepts a list of data. The list of
data must contain <code>mon.names</code> which contains monitored variable
names, and must contain <code>parm.names</code> which contains parameter
names. The <code><a href="#topic+as.parm.names">as.parm.names</a></code> function may be helpful for
preparing the data, and the <code><a href="#topic+is.data">is.data</a></code> function may be
helpful for checking data.</p>
</td></tr>
<tr><td><code id="LaplacesDemon_+3A_initial.values">Initial.Values</code></td>
<td>
<p>For <code>LaplacesDemon</code>, this argument requires
a vector of initial values equal in length to the number of
parameters. For <code>LaplacesDemon.hpc</code>, this argument also accepts
a vector, in which case the same initial values will be applied to
all parallel chains, or the argument accepts a matrix in which each
row is a parallel chain and the number of columns is equal in length
to the number of parameters. When a matrix is supplied for
<code>LaplacesDemon.hpc</code>, each parallel chain begins with its own
initial values that are preferably dispersed. For both
<code>LaplacesDemon</code> and <code>LaplacesDemon.hpc</code>, each initial
value will be the starting point for an adaptive chain or a
non-adaptive Markov chain of a parameter. Parameters are assumed to
be continuous, unless specified to be discrete (see <code>dparm</code>
below), which is not accepted by all algorithms (see
<code><a href="#topic+dcrmrf">dcrmrf</a></code> for an alternative). If all initial values are
set to zero, then Laplace's Demon will attempt to optimize the
initial values with the <code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>
function. After Laplace's Demon finishes updating, it may be desired
to continue updating from where it left off. To continue, this
argument should receive the last iteration of the previous update.
For example, if the output object is called Fit, then
<code>Initial.Values=as.initial.values(Fit)</code>. Initial values may be
generated randomly with the <code><a href="#topic+GIV">GIV</a></code> function.</p>
</td></tr>
<tr><td><code id="LaplacesDemon_+3A_covar">Covar</code></td>
<td>
<p>This argument defaults to <code>NULL</code>, but may otherwise
accept a <code class="reqn">K \times K</code> proposal covariance matrix
(where <code class="reqn">K</code> is the number of dimensions or parameters), a
variance vector, or a list of covariance matrices (for blockwise
sampling in some algorithms). When the model is updated for the
first time and prior variance or covariance is unknown, then
<code>Covar=NULL</code> should be used. Some algorithms require
covariance, some only require variance, and some require neither.
Laplace's Demon automatically converts the user input to the
required form. Once Laplace's Demon has finished updating, it may
be desired to continue updating where it left off, in which case
the proposal covariance matrix from the last run can be input into
the next run. The covariance matrix may also be input from the
<code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code> function, if used.</p>
</td></tr>
<tr><td><code id="LaplacesDemon_+3A_iterations">Iterations</code></td>
<td>
<p>This required argument accepts integers larger than
10, and determines the number of iterations that Laplace's Demon
will update the parameters while searching for target
distributions. The required amount of computer memory will increase
with <code>Iterations</code>. If computer memory is exceeded, then all
will be lost. The <code><a href="#topic+Combine">Combine</a></code> function can be used later
to combine multiple updates.</p>
</td></tr>
<tr><td><code id="LaplacesDemon_+3A_status">Status</code></td>
<td>
<p>This argument accepts an integer between 1 and the
number of iterations, and indicates how often, in iterations, the
user would like the status printed to the screen or log
file. Usually, the following is reported: the number of iterations,
the proposal type (for example, multivariate or componentwise, or
mixture, or subset), and LP. For example, if a model is updated for
1,000 iterations and <code>Status=200</code>, then a status message will
be printed at the following iterations: 200, 400, 600, 800, and
1,000.</p>
</td></tr>
<tr><td><code id="LaplacesDemon_+3A_thinning">Thinning</code></td>
<td>
<p>This argument accepts integers between 1 and the
number of iterations, and indicates that every nth iteration will be
retained, while the other iterations are discarded. If
<code>Thinning=5</code>, then every 5th iteration will be
retained. Thinning is performed to reduce autocorrelation and the
number of marginal posterior samples.</p>
</td></tr>
<tr><td><code id="LaplacesDemon_+3A_algorithm">Algorithm</code></td>
<td>
<p>This argument accepts the abbreviated name of the
MCMC algorithm, which must appear in quotes. A list of MCMC
algorithms appears below in the Details section, and the
abbreviated name is in parenthesis.</p>
</td></tr>
<tr><td><code id="LaplacesDemon_+3A_specs">Specs</code></td>
<td>
<p>This argument defaults to <code>NULL</code>, and accepts a list
of specifications for the MCMC algorithm declared in the
<code>Algorithm</code> argument. The specifications associated with each
algorithm may be seen below in the examples, must appear in the
order shown, and are described in the details section below.</p>
</td></tr>
<tr><td><code id="LaplacesDemon_+3A_debug">Debug</code></td>
<td>
<p>This argument accepts a list of logical scalars that
control whether or not errors or warnings are reported due to a
<code>try</code> function or non-finite values. List components include
<code>DB.chol</code> regarding <code>chol</code>, <code>DB.eigen</code> regarding
<code>eigen</code>, <code>DB.MCSE</code> regarding <code><a href="#topic+MCSE">MCSE</a></code>, and
<code>DB.Model</code> regarding the Model specification function. Errors
and warnings should be investigated, but do not necessarily indicate
a faulty Model specification function or a bug in the software. For
example, a sampler may make a proposal that would result in a matrix
that is not positive definite, when it should be. This kind of error
or warning is acceptable, provided the sampler handles it correctly
by rejecting the proposal, and provided the Model specification
function is not causing the issue. Oftentimes, blockwise sampling
with carefully chosen blocks will mostly or completely eliminate
errors or warnings that occur otherwise in larger, multivariate
proposals. Similarly, debugged componentwise algorithms tend to
provide more information than multivariate algorithms, since
usually the parameter and both its current and proposed values may
be reported. If confident in the Model specification function, and
errors or warnings are produced frequently that are acceptable,
then consider setting <code>DB.Model=FALSE</code> for cleaner output and
faster sampling. If the Model specification function is not faulty
and there is a bug in <code>LaplacesDemon</code>, then please report it
with a bug description and reproducible code on 
<a href="https://github.com/LaplacesDemonR/LaplacesDemon/issues">https://github.com/LaplacesDemonR/LaplacesDemon/issues</a>.</p>
</td></tr>
<tr><td><code id="LaplacesDemon_+3A_logfile">LogFile</code></td>
<td>
<p>This argument is used to specify a log file name in
quotes in the working directory as a destination, rather than the
console, for the output messages of <code>cat</code> and <code>stop</code>
commands. It is helpful to assign a log file name when
using multiple cores, such as with <code>LaplacesDemon.hpc</code>. Doing
so allows the user to check the progress in the log. A number of log
files are created, one for each chain, and one for the overall
process.</p>
</td></tr>
<tr><td><code id="LaplacesDemon_+3A_chains">Chains</code></td>
<td>
<p>This argument is required only for
<code>LaplacesDemon.hpc</code>, and indicates the number of parallel
chains.</p>
</td></tr>
<tr><td><code id="LaplacesDemon_+3A_cpus">CPUs</code></td>
<td>
<p>This argument is required for parallel independent or
interactive chains in <code>LaplacesDemon</code> or
<code>LaplacesDemon.hpc</code>, and indicates the number of central
processing units (CPUs) of the computer or cluster. For example,
when a user has a quad-core computer, <code>CPUs=4</code>.</p>
</td></tr>
<tr><td><code id="LaplacesDemon_+3A_type">Type</code></td>
<td>
<p>This argument defaults to <code>"PSOCK"</code> and uses the
Simple Network of Workstations (SNOW) for parallelization.
Alternatively, <code>Type="MPI"</code> may be specified to use Message
Passing Interface (MPI) for parallelization.</p>
</td></tr>
<tr><td><code id="LaplacesDemon_+3A_packages">Packages</code></td>
<td>
<p>This optional argument is for use with parallel
independent or interacting chains, and defaults to <code>NULL</code>. This
argument accepts a vector of package names to load into each
parallel chain. If the <code>Model</code> specification depends on any
packages, then these package names need to be in this vector.</p>
</td></tr>
<tr><td><code id="LaplacesDemon_+3A_dyn.libs">Dyn.libs</code></td>
<td>
<p>This optional argument is for use with parallel
independent or interacting chain, and defaults to <code>NULL</code>. This
argument accepts a vector of the names of dynamic link libraries
(shared objects) to load into each parallel chain. The libraries
must be located in the working directory.</p>
</td></tr>
<tr><td><code id="LaplacesDemon_+3A_...">...</code></td>
<td>
<p>Additional arguments are unused.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>LaplacesDemon</code> offers numerous MCMC algorithms for numerical
approximation in Bayesian inference. The algorithms are
</p>

<ul>
<li><p> Adaptive Directional Metropolis-within-Gibbs (ADMG)
</p>
</li>
<li><p> Adaptive Griddy-Gibbs (AGG)
</p>
</li>
<li><p> Adaptive Hamiltonian Monte Carlo (AHMC)
</p>
</li>
<li><p> Adaptive Metropolis (AM)
</p>
</li>
<li><p> Adaptive Metropolis-within-Gibbs (AMWG)
</p>
</li>
<li><p> Adaptive-Mixture Metropolis (AMM)
</p>
</li>
<li><p> Affine-Invariant Ensemble Sampler (AIES)
</p>
</li>
<li><p> Componentwise Hit-And-Run Metropolis (CHARM)
</p>
</li>
<li><p> Delayed Rejection Adaptive Metropolis (DRAM)
</p>
</li>
<li><p> Delayed Rejection Metropolis (DRM)
</p>
</li>
<li><p> Differential Evolution Markov Chain (DEMC)
</p>
</li>
<li><p> Elliptical Slice Sampler (ESS)
</p>
</li>
<li><p> Gibbs Sampler (Gibbs)
</p>
</li>
<li><p> Griddy-Gibbs (GG)
</p>
</li>
<li><p> Hamiltonian Monte Carlo (HMC)
</p>
</li>
<li><p> Hamiltonian Monte Carlo with Dual-Averaging (HMCDA)
</p>
</li>
<li><p> Hit-And-Run Metropolis (HARM)
</p>
</li>
<li><p> Independence Metropolis (IM)
</p>
</li>
<li><p> Interchain Adaptation (INCA)
</p>
</li>
<li><p> Metropolis-Adjusted Langevin Algorithm (MALA)
</p>
</li>
<li><p> Metropolis-Coupled Markov Chain Monte Carlo (MCMCMC)
</p>
</li>
<li><p> Metropolis-within-Gibbs (MWG)
</p>
</li>
<li><p> Multiple-Try Metropolis (MTM)
</p>
</li>
<li><p> No-U-Turn Sampler (NUTS)
</p>
</li>
<li><p> Oblique Hyperrectangle Slice Sampler (OHSS)
</p>
</li>
<li><p> Preconditioned Crank-Nicolson (pCN)
</p>
</li>
<li><p> Random Dive Metropolis-Hastings (RDMH)
</p>
</li>
<li><p> Random-Walk Metropolis (RWM)
</p>
</li>
<li><p> Reflective Slice Sampler (RSS)
</p>
</li>
<li><p> Refractive Sampler (Refractive)
</p>
</li>
<li><p> Reversible-Jump (RJ)
</p>
</li>
<li><p> Robust Adaptive Metropolis (RAM)
</p>
</li>
<li><p> Sequential Adaptive Metropolis-within-Gibbs (SAMWG)
</p>
</li>
<li><p> Sequential Metropolis-within-Gibbs (SMWG)
</p>
</li>
<li><p> Slice Sampler (Slice)
</p>
</li>
<li><p> Stochastic Gradient Langevin Dynamics (SGLD)
</p>
</li>
<li><p> Tempered Hamiltonian Monte Carlo (THMC)
</p>
</li>
<li><p> t-walk (twalk)
</p>
</li>
<li><p> Univariate Eigenvector Slice Sampler (UESS)
</p>
</li>
<li><p> Updating Sequential Adaptive Metropolis-within-Gibbs (USAMWG)
</p>
</li>
<li><p> Updating Sequential Metropolis-within-Gibbs (USMWG)
</p>
</li></ul>

<p>It is a goal for the documentation in the <span class="pkg">LaplacesDemon</span> to be
extensive. However, details of MCMC algorithms are best explored
online at <a href="https://web.archive.org/web/20150206014000/http://www.bayesian-inference.com/mcmc">https://web.archive.org/web/20150206014000/http://www.bayesian-inference.com/mcmc</a>, as well
as in the &quot;LaplacesDemon Tutorial&quot; vignette, and the &quot;Bayesian
Inference&quot; vignette. Algorithm specifications (<code>Specs</code>) are
listed below:
</p>

<ul>
<li> <p><code>A</code> is used in AFSS, HMCDA, MALA, NUTS, OHSS, and UESS. In
MALA, it is the maximum acceptable value of the Euclidean norm of
the adaptive parameters mu and sigma, and the Frobenius norm of the
covariance matrix. In AFSS, HMCDA, NUTS, OHSS, and UESS, it is the
number of initial, adaptive iterations to be discarded as burn-in.
</p>
</li>
<li> <p><code>Adaptive</code> is the iteration in which adaptation begins,
and is used in AM, AMM, DRAM, INCA, and Refractive. Most of these
algorithms adapt according to an observed covariance matrix, and
should sample before beginning to adapt.
</p>
</li>
<li> <p><code>alpha.star</code> is the target acceptance rate in MALA and
RAM, and is optional in CHARM and HARM. The recommended value for
multivariate proposals is <code>alpha.star=0.234</code>, for componentwise
proposals is <code>alpha.star=0.44</code>, and for MALA is
<code>alpha.star=0.574</code>.
</p>
</li>
<li> <p><code>at</code> affects the traverse move in twalk. <code>at=6</code> is
recommended. It helps when some parameters are highly correlated,
and the correlation structure may change through the
state-space. The traverse move is associated with an acceptance rate
that decreases as the number of parameters increases, and is the
reason that <code>n1</code> is used to select a subset of parameters each
iteration. If adjusted, it is recommended to stay in the interval
[2,10].
</p>
</li>
<li> <p><code>aw</code> affects the walk move in twalk, and <code>aw=1.5</code> is
recommended. If adjusted, it is recommended to stay in the
interval [0.3,2].
</p>
</li>
<li> <p><code>beta</code> is a scale parameter for AIES, and defaults to 2,
or an autoregressive parameter for pCN.
</p>
</li>
<li> <p><code>bin.n</code> is the scalar size parameter for a binomial prior
distribution of model size for the RJ algorithm.
</p>
</li>
<li> <p><code>bin.p</code> is the scalar probability parameter for a
binomial prior distribution of model size for the RJ algorithm.
</p>
</li>
<li> <p><code>B</code> is a list of blocked parameters. Each component of
the list represents a block of parameters, and contains a vector in
which each element is the position of the associated parameter in
parm.names. This function is optional in the AFSS, AMM, AMWG, ESS,
HARM, MWG, RAM, RWM, Slice, and UESS algorithms. For more
information on blockwise sampling, see the <code><a href="#topic+Blocks">Blocks</a></code>
function.
</p>
</li>
<li> <p><code>Begin</code> indicates the time-period in which to begin
updating (filtering or predicting) in the USAMWG and USMWG
algorithms.
</p>
</li>
<li> <p><code>Bounds</code> is used in the Slice algorithm. It is a vector
of length two with the lower and upper boundary of the slice. For
continuous parameters, it is often set to negative and positive
infinity, while for discrete parameters it is set to the minimum
and maximum discrete values to be sampled. When blocks are used,
this must be supplied as a list with the same number of list
components as the number of blocks.
</p>
</li>
<li> <p><code>delta</code> is used in HMCDA, MALA, and NUTS. In HMCDA and
NUTS, it is the target acceptance rate, and the recommended value is
0.65 in HMCDA and 0.6 in NUTS. In MALA, it is a constant in the
bounded drift function, may be in the interval [1e-10,1000], and 1
is the default.
</p>
</li>
<li> <p><code>Dist</code> is the proposal distribution in RAM, and may
either be <code>Dist="t"</code> for t-distributed or <code>Dist="N"</code> for
normally-distributed.
</p>
</li>
<li> <p><code>dparm</code> accepts a vector of integers that indicate
discrete parameters. This argument is for use with the AGG or GG
algorithm.
</p>
</li>
<li> <p><code>Dyn</code> is a <code class="reqn">T \times K</code> matrix of dynamic
parameters, where <code class="reqn">T</code> is the number of time-periods and <code class="reqn">K</code>
is the number of dynamic parameters. <code>Dyn</code> is used by SAMWG,
SMWG, USAMWG, and USMWG. Non-dynamic parameters are updated first in
each sampler iteration, then dynamic parameters are updated in a
random order in each time-period, and sequentially by time-period.
</p>
</li>
<li> <p><code>epsilon</code> is used in AHMC, HMC, HMCDA, MALA, NUTS, SGLD,
and THMC. It is the step-size in all algorithms except MALA. It is
a vector equal in length to the number of parameters in AHMC, HMC,
and THMC. It is a scalar in HMCDA and NUTS. It is either a scalar
or a vector equal in length to the number of iterations in SGLD.
When <code>epsilon=NULL</code> in HMCDA or NUTS (only), a reasonable
initial value is found. In MALA, it is a vector of length two. The
first element is the acceptable minimum of adaptive scale sigma, and
the second element is added to the diagonal of the covariance matrix
for regularization.
</p>
</li>
<li> <p><code>FC</code> is used in Gibbs and accepts a function that
receives two arguments: the vector of all parameters and the list of
data (similar to the Model specification function). FC must return
the updated vector of all parameters. The user specifies FC to
calculate the full conditional distribution of one or more
parameters.
</p>
</li>
<li> <p><code>file</code> is the quoted name of a numeric matrix of data,
without headers, for SGLD. The big data set must be a .csv
file. This matrix has <code>Nr</code> rows and <code>Nc</code> columns. Each
iteration, SGLD will randomly select a block of rows, where the
number of rows is specified by the <code>size</code> argument.
</p>
</li>
<li> <p><code>Fit</code> is an object of class <code>demonoid</code> in the USAMWG
and USMWG algorithms. Posterior samples before the time-period
specified in the <code>Begin</code> argument are not updated, and are used
instead from <code>Fit</code>.
</p>
</li>
<li> <p><code>gamma</code> controls the step size in DEMC or the decay of
adaptation in MALA and RAM. In DEMC, it is positive and defaults to
<code class="reqn">2.38 / \sqrt{2J}</code> when <code>NULL</code>, where
<code class="reqn">J</code> is the length of initial values. For RAM, it is in the
interval (0.5,1], and 0.66 is recommended. For MALA, it is in the
interval (1,<code>Iterations</code>), and defaults to 1.
</p>
</li>
<li> <p><code>Grid</code> accepts either a vector or a list of vectors of
evenly-spaced points on a grid for the AGG or GG algorithm. When the
argument is a vector, the same grid is applied to all
parameters. When the argument is a list, each component in the list
has a grid that is applied to the corresponding parameter. The
algorithm will evaluate each continuous parameter at the latest
value plus each point in the grid, or each discrete parameter (see
<code>dparm</code>) at each grid point (which should be each discrete
value).
</p>
</li>
<li> <p><code>K</code> is a scalar number of proposals in MTM.
</p>
</li>
<li> <p><code>L</code> is a scalar number of leapfrog steps in AHMC, HMC, and
THMC. When <code>L=1</code>, the algorithm reduces to Langevin Monte Carlo
(LMC).
</p>
</li>
<li> <p><code>lambda</code> is used in HMCDA and MCMCMC. In HMCDA, it is a
scalar trajectory length. In MCMCMC, it is either a scalar that
controls temperature spacing, or a vector of temperature spacings.
</p>
</li>
<li> <p><code>Lmax</code> is a scalar maximum for <code>L</code> (see above) in
HMCDA and NUTS.
</p>
</li>
<li> <p><code>m</code> is used in the AFSS, AHMC, HMC, Refractive, RSS, Slice,
THMC, and UESS algorithms. In AHMC, HMC, and THMC, it is a
<code class="reqn">J \times J</code> mass matrix for <code class="reqn">J</code> initial values. In
AFSS and UESS, it is a scalar, and is the maximum number of steps
for creating the slice interval. In Refractive and RSS, it is a
scalar, and is the number of steps to take per iteration. In Slice,
it is either a scalar or a list with as many list components as
blocks. It must be an integer in [1,Inf], and indicates the maximum
number of steps for creating the slice interval.
</p>
</li>
<li> <p><code>mu</code> is a vector that is equal in length to the initial
values. This vector will be used as the mean of the proposal
distribution, and is usually the posterior mode of a
previously-updated <code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>.
</p>
</li>
<li> <p><code>MWG</code> is used in Gibbs to specify a vector of parameters
that are to receive Metropolis-within-Gibbs updates. Each element is
an integer that indicates the parameter.
</p>
</li>
<li> <p><code>Nc</code> is either the number of (un-parallelized) parallel
chains in DEMC (and must be at least 3) or the number of columns of
big data in SGLD.
</p>
</li>
<li> <p><code>Nr</code> is the number of rows of big data in SGLD.
</p>
</li>
<li> <p><code>n</code> is the number of previous iterations in ADMG, AFSS,
AMM, AMWG, OHSS, RAM, and UESS.
</p>
</li>
<li> <p><code>n1</code> affects the size of the subset of each set of points
to adjust, and is used in twalk. It relates to the number of
parameters, and <code>n1=4</code> is recommended. If adjusted, it is
recommended to stay in the interval [2,20].
</p>
</li>
<li> <p><code>parm.p</code> is a vector of probabilities for parameter
selection in the RJ algorithm, and must be equal in length to
the number of initial values.
</p>
</li>
<li> <p><code>r</code> is a scalar used in the Refractive algorithm to
indicate the ratio between r1 and r2.
</p>
</li>
<li> <p><code>Periodicity</code> specifies how often in iterations the
adaptive algorithm should adapt, and is used by AHMC, AM, AMM, AMWG,
DRAM, INCA, SAMWG, and USAMWG. If <code>Periodicity=10</code>, then the
algorithm adapts every 10th iteration. A higher <code>Periodicity</code>
is associated with an algorithm that runs faster, because it does
not have to calculate adaptation as often, though the algorithm
adapts less often to the target distributions, so it is a
trade-off. It is recommended to use the lowest value that runs
fast enough to suit the user, or provide sufficient adaptation.
</p>
</li>
<li> <p><code>selectable</code> is a vector of indicators of whether or not
a parameter is selectable for variable selection in the RJ
algorithm. Non-selectable parameters are assigned a zero, and are
always in the model. Selectable parameters are assigned a one. This
vector must be equal in length to the number of initial values.
</p>
</li>
<li> <p><code>selected</code> is a vector of indicators of whether or not
each parameter is selected when the RJ algorithm begins, and
must be equal in length to the number of initial values.
</p>
</li>
<li> <p><code>SIV</code> stands for secondary initial values and is used by
twalk. <code>SIV</code> must be the same length as <code>Initial.Values</code>,
and each element of these two vectors must be unique from each
other, both before and after being passed to the <code>Model</code>
function. <code>SIV</code> defaults to <code>NULL</code>, in which case values
are generated with <code><a href="#topic+GIV">GIV</a></code>.
</p>
</li>
<li> <p><code>size</code> is the number of rows of big data to be read into
SGLD each iteration.
</p>
</li>
<li> <p><code>smax</code> is the maximum allowable tuning parameter sigma,
the standard deviation of the conditional distribution, in the AGG
algorithm.
</p>
</li>
<li> <p><code>Temperature</code> is used in the THMC algorithm to heat up
the momentum in the first half of the leapfrog steps, and then cool
down the momentum in the last half. <code>Temperature</code> must be
positive. When greater than 1, THMC should explore more diffuse
distributions, and may be helpful with multimodal distributions.
</p>
</li>
<li> <p><code>Type</code> is used in the Slice algorithm. It is either a
scalar or a list with the same number of list components as blocks.
This accepts <code>"Continuous"</code> for continuous parameters,
<code>"Nominal"</code> for discrete parameters that are unordered, and
<code>"Ordinal"</code> for discrete parameters that are ordered.
</p>
</li>
<li> <p><code>w</code> is used in AFSS, AMM, DEMC, Refractive, RSS, and
Slice. It is a mixture weight for both the AMM and DEMC algorithms,
and in these algorithms it is in the interval (0,1]. For AMM, it is
recommended to use <code>w=0.05</code>, as per Roberts and Rosenthal
(2009). The two mixture components in AMM are adaptive multivariate
and static/symmetric univariate proposals. The mixture is determined
at each iteration with mixture weight <code>w</code>. In the AMM
algorithm, a higher value of <code>w</code> is associated with more
static/symmetric univariate proposals, and a lower <code>w</code> is
associated with more adaptive multivariate proposals. AMM will be
unable to include the multivariate mixture component until it has
accumulated some history, and models with more parameters will take
longer to be able to use adaptive multivariate proposals. In DEMC,
it indicates the probability that each iteration uses a snooker
update, rather than a projection update, and the recommended default
is <code>w=0.1</code>. In the Refractive algorithm, <code>w</code> is a scalar
step size parameter. In AFSS, RSS, and the Slice algorithms, this is
a step size interval for creating the slice interval. In AFSS and
RSS, a scalar or vector equal in length the number of initial values
is accepted. In Slice, a scalar or a list with a number of list
components equal to the number of blocks is accepted.
</p>
</li>
<li> <p><code>Z</code> accepts a <code class="reqn">T \times J</code> matrix or <code class="reqn">T
    \times J \times Nc</code> array of thinned samples for <code class="reqn">T</code>
thinned iterations, <code class="reqn">J</code> parameters, and <code class="reqn">Nc</code> chains for
DEMC. <code>Z</code> defaults to <code>NULL</code>. The matrix of thinned
posterior samples from a previous run may be used, in which case the
samples are copied across the chains.
</p>
</li></ul>



<h3>Value</h3>

<p><code>LaplacesDemon</code> returns an object of class <code>demonoid</code>, and
<code>LaplacesDemon.hpc</code> returns an object of class
<code>demonoid.hpc</code> that is a list of objects of class
<code>demonoid</code>, where the number of components in the list
is the number of parallel chains. Each object of class <code>demonoid</code> 
is a list with the following components:
</p>
<table>
<tr><td><code>Acceptance.Rate</code></td>
<td>
<p>This is the acceptance rate of the MCMC
algorithm, indicating the percentage of iterations in which the
proposals were accepted. For more information on acceptance rates,
see the <code><a href="#topic+AcceptanceRate">AcceptanceRate</a></code> function.</p>
</td></tr>
<tr><td><code>Algorithm</code></td>
<td>
<p>This reports the specific algorithm used.</p>
</td></tr>
<tr><td><code>Call</code></td>
<td>
<p>This is the matched call of <code>LaplacesDemon</code>.</p>
</td></tr>
<tr><td><code>Covar</code></td>
<td>
<p>This stores the <code class="reqn">K \times K</code> proposal
covariance matrix (where <code class="reqn">K</code> is the dimension or number of
parameters), variance vector, or list of covariance matrices.
If variance or covariance is used for adaptation, then this
covariance is returned. Otherwise, the variance of the samples of
each parameter is returned. If the model is updated in the future,
then this vector, matrix, or list can be used to start the next
update where the last update left off. Only the diagonal of this
matrix is reported in the associated <code>print</code> function.</p>
</td></tr>
<tr><td><code>CovarDHis</code></td>
<td>
<p>This <code class="reqn">N \times K</code> matrix stores the
diagonal of the proposal covariance matrix of each adaptation in
each of <code class="reqn">N</code> rows for <code class="reqn">K</code> dimensions, where the dimension is
the number of parameters or length of the initial values vector. The
proposal covariance matrix should change less over time. An
exception is that the AHMC algorithm stores an algorithm
specification here, which is not the diagonal of the proposal
covariance matrix.</p>
</td></tr>
<tr><td><code>Deviance</code></td>
<td>
<p>This is a vector of the deviance of the model, with a
length equal to the number of thinned samples that were retained.
Deviance is useful for considering model fit, and is equal to the
sum of the log-likelihood for all rows in the data set, which is
then multiplied by negative two.</p>
</td></tr>
<tr><td><code>DIC1</code></td>
<td>
<p>This is a vector of three values: Dbar, pD, and DIC. Dbar
is the mean deviance, pD is a measure of model complexity indicating
the effective number of parameters, and DIC is the Deviance
Information Criterion, which is a model fit statistic that is the
sum of Dbar and pD. <code>DIC1</code> is calculated over all retained
samples. Note that pD is calculated as <code>var(Deviance)/2</code> as in
Gelman et al. (2004).</p>
</td></tr>
<tr><td><code>DIC2</code></td>
<td>
<p>This is identical to <code>DIC1</code> above, except that it is
calculated over only the samples that were considered by the
<code>BMK.Diagnostic</code> to be stationary for all parameters. If
stationarity (or a lack of trend) is not estimated for all
parameters, then <code>DIC2</code> is set to missing values.</p>
</td></tr>
<tr><td><code>Initial.Values</code></td>
<td>
<p>This is the vector of <code>Initial.Values</code>,
which may have been optimized with the
<code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code> or
<code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code> function.</p>
</td></tr>
<tr><td><code>Iterations</code></td>
<td>
<p>This reports the number of <code>Iterations</code> for
updating.</p>
</td></tr>
<tr><td><code>LML</code></td>
<td>
<p>This is an approximation of the logarithm of the marginal
likelihood of the data (see the <code><a href="#topic+LML">LML</a></code> function for more
information). <code>LML</code> is estimated only with stationary samples,
and only with a non-adaptive algorithm, including Adaptive
Griddy-Gibbs (AGG), Affine-Invariant Ensemble Sampler (AIES),
Componentwise Hit-And-Run (CHARM), Delayed Rejection Metropolis
(DRM), Elliptical Slice Sampling (ESS), Gibbs Sampler (Gibbs),
Griddy-Gibbs (GG), Hamiltonian Monte Carlo (HMC), Hit-And-Run
Metropolis (HARM), Independence Metropolis (IM), Metropolis-Coupled
Markov Chain Monte Carlo (MCMCMC), Metropolis-within-Gibbs (MWG),
Multiple-Try Metropolis, No-U-Turn Sampler (NUTS), Random Dive
Metropolis-Hastings (RDMH), Random-Walk Metropolis (RWM), Reflective
Slice Sampler (RSS), Refractive Sampler (Refractive),
Reversible-Jump (RJ), Sequential Metropolis-within-Gibbs (SMWG),
Slice Sampler (Slice), Stochastic Gradient Langevin Dynamics (SGLD),
Tempered Hamiltonian Monte Carlo (THMC), or t-walk (twalk).
<code>LML</code> is estimated with nonparametric self-normalized
importance sampling (NSIS), given LL and the marginal posterior
samples of the parameters. <code>LML</code> is useful for comparing
multiple models with the <code><a href="#topic+BayesFactor">BayesFactor</a></code> function.</p>
</td></tr>
<tr><td><code>Minutes</code></td>
<td>
<p>This indicates the number of minutes that
<code>LaplacesDemon</code> was running, and includes the initial checks as
well as time it took the <code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>
function, assessing stationarity, effective sample size (ESS), and
creating summaries.</p>
</td></tr>
<tr><td><code>Model</code></td>
<td>
<p>This contains the model specification <code>Model</code>.</p>
</td></tr>
<tr><td><code>Monitor</code></td>
<td>
<p>This is a vector or matrix of one or more monitored
variables, which are variables that were specified in the
<code>Model</code> function to be observed as chains (or Markov chains,
if <code>Adaptive=0</code>), but that were not deviance or parameters.</p>
</td></tr>
<tr><td><code>Parameters</code></td>
<td>
<p>This reports the number of parameters.</p>
</td></tr>
<tr><td><code>Posterior1</code></td>
<td>
<p>This is a matrix of marginal posterior distributions
composed of thinned samples, with a number of rows equal to the
number of thinned samples and a number of columns equal to the
number of parameters. This matrix includes all thinned samples.</p>
</td></tr>
<tr><td><code>Posterior2</code></td>
<td>
<p>This is a matrix equal to <code>Posterior1</code>, except
that rows are included only if stationarity (a lack of trend) is
indicated by the <code><a href="#topic+BMK.Diagnostic">BMK.Diagnostic</a></code> for all parameters.
If stationarity did not occur, then this matrix is missing.</p>
</td></tr>
<tr><td><code>Rec.BurnIn.Thinned</code></td>
<td>
<p>This is the recommended burn-in for the
thinned samples, where the value indicates the first row that was
stationary across all parameters, and previous rows are discarded
as burn-in. Samples considered as burn-in are discarded because they
do not represent the target distribution and have not adequately
forgotten the initial value of the chain (or Markov chain, if
<code>Adaptive=0</code>).</p>
</td></tr>
<tr><td><code>Rec.BurnIn.UnThinned</code></td>
<td>
<p>This is the recommended burn-in for all
samples, in case thinning will not be necessary.</p>
</td></tr>
<tr><td><code>Rec.Thinning</code></td>
<td>
<p>This is the recommended value for the
<code>Thinning</code> argument according to the autocorrelation in the
thinned samples, and it is limited to the interval [1,1000].</p>
</td></tr>
<tr><td><code>Specs</code></td>
<td>
<p>This is an optional list of algorithm specifications.</p>
</td></tr>
<tr><td><code>Status</code></td>
<td>
<p>This is the value in the <code>Status</code> argument.</p>
</td></tr>
<tr><td><code>Summary1</code></td>
<td>
<p>This is a matrix that summarizes the marginal
posterior distributions of the parameters, deviance, and monitored
variables over all samples in <code>Posterior1</code>. The following
summary statistics are included: mean, standard deviation, MCSE
(Monte Carlo Standard Error), ESS is the effective sample size due
to autocorrelation, and finally the 2.5%, 50%, and 97.5%
quantiles are reported. MCSE is essentially a standard deviation
around the marginal posterior mean that is due to uncertainty
associated with using MCMC. The acceptable size of the MCSE
depends on the acceptable uncertainty associated around the
marginal posterior mean. Laplace's Demon prefers to continue
updating until each MCSE is less than 6.27% of each marginal
posterior standard deviation (see the <code><a href="#topic+MCSE">MCSE</a></code> and
<code><a href="#topic+Consort">Consort</a></code> functions). The default <code>IMPS</code> method
is used. Next, the desired precision of ESS depends on the user's
goal, and Laplace's Demon prefers to continue until each ESS is at
least 100, which should be enough to describe 95% boundaries of an
approximately Gaussian distribution (see the <code><a href="#topic+ESS">ESS</a></code> for
more information).</p>
</td></tr>
<tr><td><code>Summary2</code></td>
<td>
<p>This matrix is identical to the matrix in
<code>Summary1</code>, except that it is calculated only on the
stationary samples found in <code>Posterior2</code>. If universal
stationarity was not estimated for the parameters, then this matrix
is set to missing values.</p>
</td></tr>
<tr><td><code>Thinned.Samples</code></td>
<td>
<p>This is the number of thinned samples that
were retained.</p>
</td></tr>
<tr><td><code>Thinning</code></td>
<td>
<p>This is the value of the <code>Thinning</code> argument.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Statisticat, LLC.,
Silvere Vialet-Chabrand <a href="mailto:silvere@vialet-chabrand.com">silvere@vialet-chabrand.com</a></p>


<h3>References</h3>

<p>Atchade, Y.F. (2006). &quot;An Adaptive Version for the Metropolis Adjusted
Langevin Algorithm with a Truncated Drift&quot;. <em>Methodology and
Computing in Applied Probability</em>, 8, p. 235&ndash;254.
</p>
<p>Bai, Y. (2009). &quot;An Adaptive Directional Metropolis-within-Gibbs
Algorithm&quot;. Technical Report in Department of Statistics at the
University of Toronto.
</p>
<p>Beskos, A., Roberts, G.O., Stuart, A.M., and Voss, J. (2008). &quot;MCMC
Methods for Diffusion Bridges&quot;. Stoch. Dyn., 8, p. 319&ndash;350.
</p>
<p>Boyles, L.B. and Welling, M. (2012). &quot;Refractive Sampling&quot;.
</p>
<p>Craiu, R.V., Rosenthal, J., and Yang, C. (2009). &quot;Learn From Thy
Neighbor: Parallel-Chain and Regional Adaptive MCMC&quot;. <em>Journal
of the American Statistical Assocation</em>, 104(488), p. 1454&ndash;1466.
</p>
<p>Christen, J.A. and Fox, C. (2010). &quot;A General Purpose Sampling
Algorithm for Continuous Distributions (the t-walk)&quot;. <em>Bayesian
Analysis</em>, 5(2), p. 263&ndash;282.
</p>
<p>Dutta, S. (2012). &quot;Multiplicative Random Walk Metropolis-Hastings
on the Real Line&quot;. <em>Sankhya B</em>, 74(2), p. 315&ndash;342.
</p>
<p>Duane, S., Kennedy, A.D., Pendleton, B.J., and Roweth, D. (1987).
&quot;Hybrid Monte Carlo&quot;. <em>Physics Letters</em>, B, 195, p. 216&ndash;222.
</p>
<p>Gelman, A., Carlin, J., Stern, H., and Rubin, D. (2004). &quot;Bayesian
Data Analysis, Texts in Statistical Science, 2nd ed.&quot;. Chapman and
Hall, London.
</p>
<p>Geman, S. and Geman, D. (1984). &quot;Stochastic Relaxation, Gibbs
Distributions, and the Bayesian Restoration of Images&quot;. <em>IEEE
Transactions on Pattern Analysis and Machine Intelligence</em>, 6(6),
p. 721&ndash;741.
</p>
<p>Geyer, C.J. (1991). &quot;Markov Chain Monte Carlo Maximum Likelihood&quot;. In
Keramidas, E.M. Computing Science and Statistics: Proceedings of the
23rd Symposium of the Interface. Fairfax Station VA: Interface
Foundation. p. 156&ndash;163.
</p>
<p>Goodman J, and Weare, J. (2010). &quot;Ensemble Samplers with Affine
Invariance&quot;. <em>Communications in Applied Mathematics and
Computational Science</em>, 5(1), p. 65&ndash;80.
</p>
<p>Green, P.J. (1995). &quot;Reversible Jump Markov Chain Monte Carlo
Computation and Bayesian Model Determination&quot;. <em>Biometrika</em>,
82, p. 711&ndash;732.
</p>
<p>Haario, H., Laine, M., Mira, A., and Saksman, E. (2006). &quot;DRAM:
Efficient Adaptive MCMC&quot;. <em>Statistical Computing</em>, 16,
p. 339&ndash;354.
</p>
<p>Haario, H., Saksman, E., and Tamminen, J. (2001). &quot;An Adaptive
Metropolis Algorithm&quot;. <em>Bernoulli</em>, 7, p. 223&ndash;242.
</p>
<p>Hoffman, M.D. and Gelman. A. (2012). &quot;The No-U-Turn Sampler:
Adaptively Setting Path Lengths in Hamiltonian Monte
Carlo&quot;. <em>Journal of Machine Learning Research</em>, p. 1&ndash;30.
</p>
<p>Kass, R.E. and Raftery, A.E. (1995). &quot;Bayes Factors&quot;. <em>Journal
of the American Statistical Association</em>, 90(430), p. 773&ndash;795.
</p>
<p>Lewis, S.M. and Raftery, A.E. (1997). &quot;Estimating Bayes Factors via
Posterior Simulation with the Laplace-Metropolis Estimator&quot;.
<em>Journal of the American Statistical Association</em>, 92,
p. 648&ndash;655.
</p>
<p>Liu, J., Liang, F., and Wong, W. (2000). &quot;The Multiple-Try Method and
Local Optimization in Metropolis Sampling&quot;. <em>Journal of the
American Statistical Association</em>, 95, p. 121&ndash;134.
</p>
<p>Metropolis, N., Rosenbluth, A.W., Rosenbluth, M.N., and Teller,
E. (1953). &quot;Equation of State Calculations by Fast Computing
Machines&quot;. <em>Journal of Chemical Physics</em>, 21, p. 1087&ndash;1092.
</p>
<p>Mira, A. (2001). &quot;On Metropolis-Hastings Algorithms with Delayed
Rejection&quot;. <em>Metron</em>, Vol. LIX, n. 3-4, p. 231&ndash;241.
</p>
<p>Murray, I., Adams, R.P., and MacKay, D.J. (2010). &quot;Elliptical Slice
Sampling&quot;. <em>Journal of Machine Learning Research</em>, 9,
p. 541&ndash;548.
</p>
<p>Neal, R.M. (2003). &quot;Slice Sampling&quot; (with discussion). <em>Annals of
Statistics</em>, 31(3), p. 705&ndash;767.
</p>
<p>Ritter, C. and Tanner, M. (1992), &quot;Facilitating the Gibbs Sampler: the
Gibbs Stopper and the Griddy-Gibbs Sampler&quot;, <em>Journal of the
American Statistical Association</em>, 87, p. 861&ndash;868.
</p>
<p>Roberts, G.O. and Rosenthal, J.S. (2009). &quot;Examples of Adaptive
MCMC&quot;. <em>Computational Statistics and Data Analysis</em>, 18,
p. 349&ndash;367.
</p>
<p>Roberts, G.O. and Tweedie, R.L. (1996). &quot;Exponential Convergence of
Langevin Distributions and Their Discrete Approximations&quot;.
<em>Bernoulli</em>, 2(4), p. 341&ndash;363.
</p>
<p>Rosenthal, J.S. (2007). &quot;AMCMC: An R interface for adaptive MCMC&quot;.
<em>Computational Statistics and Data Analysis</em>, 51, p. 5467&ndash;5470.
</p>
<p>Smith, R.L. (1984). &quot;Efficient Monte Carlo Procedures for Generating
Points Uniformly Distributed Over Bounded Region&quot;. <em>Operations
Research</em>, 32, p. 1296&ndash;1308.
</p>
<p>Ter Braak, C.J.F. and Vrugt, J.A. (2008). &quot;Differential Evolution
Markov Chain with Snooker Updater and Fewer Chains&quot;, <em>Statistics
and Computing</em>, 18(4), p. 435&ndash;446.
</p>
<p>Tibbits, M., Groendyke, C., Haran, M., Liechty, J. (2014). &quot;Automated
Factor Slice Sampling&quot;. <em>Journal of Computational and Graphical
Statistics</em>, 23(2), p. 543&ndash;563.
</p>
<p>Thompson, M.D. (2011). &quot;Slice Sampling with Multivariate Steps&quot;.
<a href="http://hdl.handle.net/1807/31955">http://hdl.handle.net/1807/31955</a>
</p>
<p>Vihola, M. (2011). &quot;Robust Adaptive Metropolis Algorithm with Coerced
Acceptance Rate&quot;. <em>Statistics and Computing</em>. Springer,
Netherlands.
</p>
<p>Welling, M. and Teh, Y.W. (2011). &quot;Bayesian Learning via Stochastic
Gradient Langevin Dynamics&quot;. <em>Proceedings of the 28th
International Conference on Machine Learning (ICML)</em>, p. 681&ndash;688.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+AcceptanceRate">AcceptanceRate</a></code>,
<code><a href="#topic+as.initial.values">as.initial.values</a></code>,
<code><a href="#topic+as.parm.names">as.parm.names</a></code>,
<code><a href="#topic+BayesFactor">BayesFactor</a></code>,
<code><a href="#topic+Blocks">Blocks</a></code>,
<code><a href="#topic+BMK.Diagnostic">BMK.Diagnostic</a></code>,
<code><a href="#topic+Combine">Combine</a></code>,
<code><a href="#topic+Consort">Consort</a></code>,
<code><a href="#topic+dcrmrf">dcrmrf</a></code>,
<code><a href="#topic+ESS">ESS</a></code>,
<code><a href="#topic+GIV">GIV</a></code>,
<code><a href="#topic+is.data">is.data</a></code>,
<code><a href="#topic+is.model">is.model</a></code>,
<code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code>,
<code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>,
<code><a href="#topic+LaplacesDemon.RAM">LaplacesDemon.RAM</a></code>,
<code><a href="#topic+LML">LML</a></code>, and
<code><a href="#topic+MCSE">MCSE</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># The accompanying Examples vignette is a compendium of examples.
####################  Load the LaplacesDemon Library  #####################
library(LaplacesDemon)

##############################  Demon Data  ###############################
data(demonsnacks)
y &lt;- log(demonsnacks$Calories)
X &lt;- cbind(1, as.matrix(log(demonsnacks[,c(1,4,10)]+1)))
J &lt;- ncol(X)
for (j in 2:J) X[,j] &lt;- CenterScale(X[,j])

#########################  Data List Preparation  #########################
mon.names &lt;- "LP"
parm.names &lt;- as.parm.names(list(beta=rep(0,J), sigma=0))
pos.beta &lt;- grep("beta", parm.names)
pos.sigma &lt;- grep("sigma", parm.names)
PGF &lt;- function(Data) {
     beta &lt;- rnorm(Data$J)
     sigma &lt;- runif(1)
     return(c(beta, sigma))
     }
MyData &lt;- list(J=J, PGF=PGF, X=X, mon.names=mon.names,
     parm.names=parm.names, pos.beta=pos.beta, pos.sigma=pos.sigma, y=y)

##########################  Model Specification  ##########################
Model &lt;- function(parm, Data)
     {
     ### Parameters
     beta &lt;- parm[Data$pos.beta]
     sigma &lt;- interval(parm[Data$pos.sigma], 1e-100, Inf)
     parm[Data$pos.sigma] &lt;- sigma
     ### Log-Prior
     beta.prior &lt;- sum(dnormv(beta, 0, 1000, log=TRUE))
     sigma.prior &lt;- dhalfcauchy(sigma, 25, log=TRUE)
     ### Log-Likelihood
     mu &lt;- tcrossprod(Data$X, t(beta))
     LL &lt;- sum(dnorm(Data$y, mu, sigma, log=TRUE))
     ### Log-Posterior
     LP &lt;- LL + beta.prior + sigma.prior
     Modelout &lt;- list(LP=LP, Dev=-2*LL, Monitor=LP,
          yhat=rnorm(length(mu), mu, sigma), parm=parm)
     return(Modelout)
     }
#library(compiler)
#Model &lt;- cmpfun(Model) #Consider byte-compiling for more speed

set.seed(666)

############################  Initial Values  #############################
Initial.Values &lt;- GIV(Model, MyData, PGF=TRUE)

###########################################################################
# Examples of MCMC Algorithms                                             #
###########################################################################

####################  Automated Factor Slice Sampler  #####################
Fit &lt;- LaplacesDemon(Model, Data=MyData, Initial.Values,
     Covar=NULL, Iterations=1000, Status=100, Thinning=1,
     Algorithm="AFSS", Specs=list(A=Inf, B=NULL, m=100, n=0, w=1))
Fit
print(Fit)
#Consort(Fit)
#plot(BMK.Diagnostic(Fit))
#PosteriorChecks(Fit)
#caterpillar.plot(Fit, Parms="beta")
#BurnIn &lt;- Fit$Rec.BurnIn.Thinned
#plot(Fit, BurnIn, MyData, PDF=FALSE)
#Pred &lt;- predict(Fit, Model, MyData, CPUs=1)
#summary(Pred, Discrep="Chi-Square")
#plot(Pred, Style="Covariates", Data=MyData)
#plot(Pred, Style="Density", Rows=1:9)
#plot(Pred, Style="ECDF")
#plot(Pred, Style="Fitted")
#plot(Pred, Style="Jarque-Bera")
#plot(Pred, Style="Predictive Quantiles")
#plot(Pred, Style="Residual Density")
#plot(Pred, Style="Residuals")
#Levene.Test(Pred)
#Importance(Fit, Model, MyData, Discrep="Chi-Square")

#############  Adaptive Directional Metropolis-within-Gibbs  ##############
#Fit &lt;- LaplacesDemon(Model, Data=MyData, Initial.Values,
#     Covar=NULL, Iterations=1000, Status=100, Thinning=1,
#     Algorithm="ADMG", Specs=list(n=0, Periodicity=50))

########################  Adaptive Griddy-Gibbs  ##########################
#Fit &lt;- LaplacesDemon(Model, Data=MyData, Initial.Values,
#     Covar=NULL, Iterations=1000, Status=100, Thinning=1,
#     Algorithm="AGG", Specs=list(Grid=GaussHermiteQuadRule(3)$nodes,
#     dparm=NULL, smax=Inf, CPUs=1, Packages=NULL, Dyn.libs=NULL))

##################  Adaptive Hamiltonian Monte Carlo  #####################
#Fit &lt;- LaplacesDemon(Model, Data=MyData, Initial.Values,
#     Covar=NULL, Iterations=1000, Status=100, Thinning=1,
#     Algorithm="AHMC", Specs=list(epsilon=0.02, L=2, m=NULL,
#     Periodicity=10))

##########################  Adaptive Metropolis  ##########################
#Fit &lt;- LaplacesDemon(Model, Data=MyData, Initial.Values,
#     Covar=NULL, Iterations=1000, Status=100, Thinning=1,
#     Algorithm="AM", Specs=list(Adaptive=500, Periodicity=10))

###################  Adaptive Metropolis-within-Gibbs  ####################
#Fit &lt;- LaplacesDemon(Model, Data=MyData, Initial.Values,
#     Covar=NULL, Iterations=1000, Status=100, Thinning=1,
#     Algorithm="AMWG", Specs=list(B=NULL, n=0, Periodicity=50))

######################  Adaptive-Mixture Metropolis  ######################
#Fit &lt;- LaplacesDemon(Model, Data=MyData, Initial.Values,
#     Covar=NULL, Iterations=1000, Status=100, Thinning=1,
#     Algorithm="AMM", Specs=list(Adaptive=500, B=NULL, n=0,
#     Periodicity=10, w=0.05))

###################  Affine-Invariant Ensemble Sampler  ###################
#Fit &lt;- LaplacesDemon(Model, Data=MyData, Initial.Values,
#     Covar=NULL, Iterations=1000, Status=100, Thinning=1,
#     Algorithm="AIES", Specs=list(Nc=2*length(Initial.Values), Z=NULL,
#     beta=2, CPUs=1, Packages=NULL, Dyn.libs=NULL))

#################  Componentwise Hit-And-Run Metropolis  ##################
#Fit &lt;- LaplacesDemon(Model, Data=MyData, Initial.Values,
#     Covar=NULL, Iterations=1000, Status=100, Thinning=1,
#     Algorithm="CHARM", Specs=NULL)

###########  Componentwise Hit-And-Run (Adaptive) Metropolis  #############
#Fit &lt;- LaplacesDemon(Model, Data=MyData, Initial.Values,
#     Covar=NULL, Iterations=1000, Status=100, Thinning=1,
#     Algorithm="CHARM", Specs=list(alpha.star=0.44))

#################  Delayed Rejection Adaptive Metropolis  #################
#Fit &lt;- LaplacesDemon(Model, Data=MyData, Initial.Values,
#     Covar=NULL, Iterations=1000, Status=100, Thinning=1,
#     Algorithm="DRAM", Specs=list(Adaptive=500, Periodicity=10))

#####################  Delayed Rejection Metropolis  ######################
#Fit &lt;- LaplacesDemon(Model, Data=MyData, Initial.Values,
#     Covar=NULL, Iterations=1000, Status=100, Thinning=1,
#     Algorithm="DRM", Specs=NULL)

##################  Differential Evolution Markov Chain  ##################
#Fit &lt;- LaplacesDemon(Model, Data=MyData, Initial.Values,
#     Covar=NULL, Iterations=1000, Status=100, Thinning=1,
#     Algorithm="DEMC", Specs=list(Nc=3, Z=NULL, gamma=NULL, w=0.1))

#######################  Elliptical Slice Sampler  ########################
#Fit &lt;- LaplacesDemon(Model, Data=MyData, Initial.Values,
#     Covar=NULL, Iterations=1000, Status=100, Thinning=1,
#     Algorithm="ESS", Specs=list(B=NULL))

#############################  Gibbs Sampler  #############################
### NOTE: Unlike the other samplers, Gibbs requires specifying a
### function (FC) that draws from full conditionals.
#FC &lt;- function(parm, Data)
#     {
#     ### Parameters
#     beta &lt;- parm[Data$pos.beta]
#     sigma &lt;- interval(parm[Data$pos.sigma], 1e-100, Inf)
#     sigma2 &lt;- sigma*sigma
#     ### Hyperparameters
#     betamu &lt;- rep(0,length(beta))
#     betaprec &lt;- diag(length(beta))/1000
#     ### Update beta
#     XX &lt;- crossprod(Data$X)
#     Xy &lt;- crossprod(Data$X, Data$y)
#     IR &lt;- backsolve(chol(XX/sigma2 + betaprec), diag(length(beta)))
#     btilde &lt;- crossprod(t(IR)) %*% (Xy/sigma2 + betaprec %*% betamu)
#     beta &lt;- btilde + IR %*% rnorm(length(beta))
#     return(c(beta,sigma))
#     }
##library(compiler)
##FC &lt;- cmpfun(FC) #Consider byte-compiling for more speed
#Fit &lt;- LaplacesDemon(Model, Data=MyData, Initial.Values,
#     Covar=NULL, Iterations=1000, Status=100, Thinning=1,
#     Algorithm="Gibbs", Specs=list(FC=FC, MWG=pos.sigma))


#############################  Griddy-Gibbs  ##############################
#Fit &lt;- LaplacesDemon(Model, Data=MyData, Initial.Values,
#     Covar=NULL, Iterations=1000, Status=100, Thinning=1,
#     Algorithm="GG", Specs=list(Grid=seq(from=-0.1, to=0.1, len=5),
#     dparm=NULL, CPUs=1, Packages=NULL, Dyn.libs=NULL))

#######################  Hamiltonian Monte Carlo  #########################
#Fit &lt;- LaplacesDemon(Model, Data=MyData, Initial.Values,
#     Covar=NULL, Iterations=1000, Status=100, Thinning=1,
#     Algorithm="HMC", Specs=list(epsilon=0.001, L=2, m=NULL))

#############  Hamiltonian Monte Carlo with Dual-Averaging  ###############
#Fit &lt;- LaplacesDemon(Model, Data=MyData, Initial.Values,
#     Covar=NULL, Iterations=1000, Status=1, Thinning=1,
#     Algorithm="HMCDA", Specs=list(A=500, delta=0.65, epsilon=NULL,
#     Lmax=1000, lambda=0.1))

#######################  Hit-And-Run Metropolis  ##########################
#Fit &lt;- LaplacesDemon(Model, Data=MyData, Initial.Values,
#     Covar=NULL, Iterations=1000, Status=100, Thinning=1,
#     Algorithm="HARM", Specs=NULL)

##################  Hit-And-Run (Adaptive) Metropolis  ####################
#Fit &lt;- LaplacesDemon(Model, Data=MyData, Initial.Values,
#     Covar=NULL, Iterations=1000, Status=100, Thinning=1,
#     Algorithm="HARM", Specs=list(alpha.star=0.234, B=NULL))

########################  Independence Metropolis  ########################
### Note: the mu and Covar arguments are populated from a previous Laplace
### Approximation.
#Fit &lt;- LaplacesDemon(Model, Data=MyData, Initial.Values,
#     Covar=Fit$Covar, Iterations=1000, Status=100, Thinning=1,
#     Algorithm="IM",
#     Specs=list(mu=Fit$Summary1[1:length(Initial.Values),1]))

#########################  Interchain Adaptation  #########################
#Initial.Values &lt;- rbind(Initial.Values, GIV(Model, MyData, PGF=TRUE))
#Fit &lt;- LaplacesDemon.hpc(Model, Data=MyData, Initial.Values,
#     Covar=NULL, Iterations=1000, Status=100, Thinning=1,
#     Algorithm="INCA", Specs=list(Adaptive=500, Periodicity=10),
#     LogFile="MyLog", Chains=2, CPUs=2, Type="PSOCK", Packages=NULL,
#     Dyn.libs=NULL)

################  Metropolis-Adjusted Langevin Algorithm  #################
#Fit &lt;- LaplacesDemon(Model, Data=MyData, Initial.Values,
#     Covar=NULL, Iterations=1000, Status=100, Thinning=1,
#     Algorithm="MALA", Specs=list(A=1e7, alpha.star=0.574, gamma=1,
#          delta=1, epsilon=c(1e-6,1e-7)))

#############  Metropolis-Coupled Markov Chain Monte Carlo  ###############
#Fit &lt;- LaplacesDemon(Model, Data=MyData, Initial.Values,
#     Covar=NULL, Iterations=1000, Status=100, Thinning=1,
#     Algorithm="MCMCMC", Specs=list(lambda=1, CPUs=2, Packages=NULL,
#     Dyn.libs=NULL))

#######################  Metropolis-within-Gibbs  #########################
#Fit &lt;- LaplacesDemon(Model, Data=MyData, Initial.Values,
#     Covar=NULL, Iterations=1000, Status=100, Thinning=1,
#     Algorithm="MWG", Specs=list(B=NULL))

########################  Multiple-Try Metropolis  ########################
#Fit &lt;- LaplacesDemon(Model, Data=MyData, Initial.Values,
#     Covar=NULL, Iterations=1000, Status=100, Thinning=1,
#     Algorithm="MTM", Specs=list(K=4, CPUs=1, Packages=NULL, Dyn.libs=NULL))

##########################  No-U-Turn Sampler  ############################
#Fit &lt;- LaplacesDemon(Model, Data=MyData, Initial.Values,
#     Covar=NULL, Iterations=1000, Status=1, Thinning=1,
#     Algorithm="NUTS", Specs=list(A=500, delta=0.6, epsilon=NULL,
#     Lmax=Inf))

#################  Oblique Hyperrectangle Slice Sampler  ##################
#Fit &lt;- LaplacesDemon(Model, Data=MyData, Initial.Values,
#     Covar=NULL, Iterations=1000, Status=100, Thinning=1,
#     Algorithm="OHSS", Specs=list(A=Inf, n=0))

#####################  Preconditioned Crank-Nicolson  #####################
#Fit &lt;- LaplacesDemon(Model, Data=MyData, Initial.Values,
#     Covar=NULL, Iterations=1000, Status=100, Thinning=1,
#     Algorithm="pCN", Specs=list(beta=0.1))

######################  Robust Adaptive Metropolis  #######################
#Fit &lt;- LaplacesDemon(Model, Data=MyData, Initial.Values,
#     Covar=NULL, Iterations=1000, Status=100, Thinning=1,
#     Algorithm="RAM", Specs=list(alpha.star=0.234, B=NULL, Dist="N",
#     gamma=0.66, n=0))

###################  Random Dive Metropolis-Hastings  ####################
#Fit &lt;- LaplacesDemon(Model, Data=MyData, Initial.Values,
#     Covar=NULL, Iterations=1000, Status=100, Thinning=1,
#     Algorithm="RDMH", Specs=NULL)

##########################  Refractive Sampler  ###########################
#Fit &lt;- LaplacesDemon(Model, Data=MyData, Initial.Values,
#     Covar=NULL, Iterations=1000, Status=100, Thinning=1,
#     Algorithm="Refractive", Specs=list(Adaptive=1, m=2, w=0.1, r=1.3))

###########################  Reversible-Jump  #############################
#bin.n &lt;- J-1
#bin.p &lt;- 0.2
#parm.p &lt;- c(1, rep(1/(J-1),(J-1)), 1)
#selectable &lt;- c(0, rep(1,J-1), 0)
#Fit &lt;- LaplacesDemon(Model, Data=MyData, Initial.Values,
#     Covar=NULL, Iterations=1000, Status=100, Thinning=1,
#     Algorithm="RJ", Specs=list(bin.n=bin.n, bin.p=bin.p,
#          parm.p=parm.p, selectable=selectable,
#          selected=c(0,rep(1,J-1),0)))

########################  Random-Walk Metropolis  #########################
#Fit &lt;- LaplacesDemon(Model, Data=MyData, Initial.Values,
#     Covar=NULL, Iterations=1000, Status=100, Thinning=1,
#     Algorithm="RWM", Specs=NULL)

########################  Reflective Slice Sampler  #######################
#Fit &lt;- LaplacesDemon(Model, Data=MyData, Initial.Values,
#     Covar=NULL, Iterations=1000, Status=100, Thinning=1,
#     Algorithm="RSS", Specs=list(m=5, w=1e-5))

##############  Sequential Adaptive Metropolis-within-Gibbs  ##############
#NOTE: The SAMWG algorithm is only for state-space models (SSMs)
#Fit &lt;- LaplacesDemon(Model, Data=MyData, Initial.Values,
#     Covar=NULL, Iterations=1000, Status=100, Thinning=1,
#     Algorithm="SAMWG", Specs=list(Dyn=Dyn, Periodicity=50))

##################  Sequential Metropolis-within-Gibbs  ###################
#NOTE: The SMWG algorithm is only for state-space models (SSMs)
#Fit &lt;- LaplacesDemon(Model, Data=MyData, Initial.Values,
#     Covar=NULL, Iterations=1000, Status=100, Thinning=1,
#     Algorithm="SMWG", Specs=list(Dyn=Dyn))

#############################  Slice Sampler  #############################
#Fit &lt;- LaplacesDemon(Model, Data=MyData, Initial.Values,
#     Covar=NULL, Iterations=1000, Status=1, Thinning=1,
#     Algorithm="Slice", Specs=list(B=NULL, Bounds=c(-Inf,Inf), m=100,
#     Type="Continuous", w=1))

#################  Stochastic Gradient Langevin Dynamics  #################
#NOTE: The Data and Model functions must be coded differently for SGLD.
#Fit &lt;- LaplacesDemon(Model, Data=MyData, Initial.Values,
#     Covar=NULL, Iterations=1000, Status=10, Thinning=10,
#     Algorithm="SGLD", Specs=list(epsilon=1e-4, file="X.csv", Nr=1e4,
#     Nc=6, size=10))

###################  Tempered Hamiltonian Monte Carlo  ####################
#Fit &lt;- LaplacesDemon(Model, Data=MyData, Initial.Values,
#     Covar=NULL, Iterations=1000, Status=100, Thinning=1,
#     Algorithm="THMC", Specs=list(epsilon=0.001, L=2, m=NULL,
#     Temperature=2))

###############################  t-walk  #################################
#Fit &lt;- LaplacesDemon(Model, Data=MyData, Initial.Values,
#     Covar=NULL, Iterations=1000, Status=100, Thinning=1,
#     Algorithm="twalk", Specs=list(SIV=NULL, n1=4, at=6, aw=1.5))

#################  Univariate Eigenvector Slice Sampler  #################
#Fit &lt;- LaplacesDemon(Model, Data=MyData, Initial.Values,
#     Covar=NULL, Iterations=1000, Status=100, Thinning=1,
#     Algorithm="UESS", Specs=list(A=Inf, B=NULL, m=100, n=0))

##########  Updating Sequential Adaptive Metropolis-within-Gibbs  #########
#NOTE: The USAMWG algorithm is only for state-space model updating
#Fit &lt;- LaplacesDemon(Model, Data=MyData, Initial.Values, 
#     Covar=NULL, Iterations=100000, Status=100, Thinning=100,
#     Algorithm="USAMWG", Specs=list(Dyn=Dyn, Periodicity=50, Fit=Fit,
#     Begin=T.m))

##############  Updating Sequential Metropolis-within-Gibbs  ##############
#NOTE: The USMWG algorithm is only for state-space model updating
#Fit &lt;- LaplacesDemon(Model, Data=MyData, Initial.Values, 
#     Covar=NULL, Iterations=100000, Status=100, Thinning=100,
#     Algorithm="USMWG", Specs=list(Dyn=Dyn, Fit=Fit, Begin=T.m))

#End
</code></pre>

<hr>
<h2 id='LaplacesDemon.RAM'>LaplacesDemon RAM Estimate</h2><span id='topic+LaplacesDemon.RAM'></span>

<h3>Description</h3>

<p>This function estimates the random-access memory (RAM) required to
update a given model and data with the <code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>
function.
</p>
<p><em>Warning:</em> Unwise use of this function may crash a computer, so
please read the details below.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>LaplacesDemon.RAM(Model, Data, Iterations, Thinning, Algorithm="RWM")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="LaplacesDemon.RAM_+3A_model">Model</code></td>
<td>
<p>This is a model specification function. For more
information, see <code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>.</p>
</td></tr>
<tr><td><code id="LaplacesDemon.RAM_+3A_data">Data</code></td>
<td>
<p>This is a list of Data. For more information, see
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>.</p>
</td></tr>
<tr><td><code id="LaplacesDemon.RAM_+3A_iterations">Iterations</code></td>
<td>
<p>This is the number of iterations for which
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code> would update. For more information,
see <code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>.</p>
</td></tr>
<tr><td><code id="LaplacesDemon.RAM_+3A_thinning">Thinning</code></td>
<td>
<p>This is the amount of thinning applied to the chains
in <code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>.For more information, see
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>.</p>
</td></tr>
<tr><td><code id="LaplacesDemon.RAM_+3A_algorithm">Algorithm</code></td>
<td>
<p>This argument accepts the name of the algorithm as a
string, as entered in <code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>.For more
information, see <code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>LaplacesDemon.RAM</code> function uses the
<code><a href="utils.html#topic+object.size">object.size</a></code> function to estimate the size in MB of RAM
required to update one chain in <code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code> for a
given model and data, and for a number of iterations and specified
thinning. When RAM is exceeded, the computer will crash. This function
can be useful when trying to estimate how many iterations to update a
model without crashing the computer. However, when estimating the
required RAM, <code>LaplacesDemon.RAM</code> actually creates several large
objects, such as <code>post</code> (see below). If too many iterations are
given as an argument to <code>LaplacesDemon.RAM</code>, for example, then it
will crash the computer while trying to estimate the required RAM.
</p>
<p>The best way to use this function is as follows. First, prepare the
model specification and list of data. Second, observe how much RAM the
computer is using at the moment, as well as the maximum available
RAM. The majority of the difference of these two is the amount of RAM
the computer may dedicate to updating the model. Next, use this
function with a small number of iterations (important in some
algorithms), and with few thinned samples (important in all
algorithms). Note the estimated RAM. Increase the number of
iterations and thinned samples, and again note the RAM. Continue to
increase the number of iterations and thinned samples until, say,
arbitrarily within 90% of the above-mentioned difference in RAM.
</p>
<p>The computer operating system uses RAM, as does any other software
running at the moment. R is currently using RAM, and other functions
in the <code>LaplacesDemon</code> package, and any other package that is
currently activated, are using RAM. There are numerous small objects
that are not included in the returned list, that use RAM. For example,
there may be a scalar called <code>alpha</code> for the acceptance
probability, etc.
</p>
<p>One potentially larger object that is not included, and depends on
the algorithm, is a matrix used for estimating <code><a href="#topic+LML">LML</a></code>.
Its use occurs with non-adaptive MCMC algorithms, only with enough
globally stationary samples, and only when the ratio of parameters to
samples is not excessive. If used, then the user should create a
matrix of the appropriate dimensions and use the
<code><a href="utils.html#topic+object.size">object.size</a></code> function to estimate the RAM.
</p>
<p>If the data is too large for RAM, then consider using either the
<code><a href="#topic+BigData">BigData</a></code> function or the SGLD algorithm in
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>.
</p>


<h3>Value</h3>

<p><code>LaplacesDemon.RAM</code> returns a list with several components. Each
component is an estimate in MB for an object. The list has the
following components:
</p>
<table>
<tr><td><code>Covar</code></td>
<td>
<p>This is the estimated size in MB of RAM required for the
covariance matrix, variance vector, or both (some algorithms store
both internally, creating one from the other). Blocked covariance
matrices are not considered at this time.</p>
</td></tr>
<tr><td><code>Data</code></td>
<td>
<p>This is the estimated size in MB of RAM required for the
list of data.</p>
</td></tr>
<tr><td><code>Deviance</code></td>
<td>
<p>This is the estimated size in MB of RAM required for
the deviance vector.</p>
</td></tr>
<tr><td><code>Initial.Values</code></td>
<td>
<p>This is the estimated size in MB of RAM required
for the vector of initial values.</p>
</td></tr>
<tr><td><code>Model</code></td>
<td>
<p>This is the estimated size in MB of RAM required for the
model specification function.</p>
</td></tr>
<tr><td><code>Monitor</code></td>
<td>
<p>This is the estimated size in MB of RAM required for
the <code class="reqn">N \times J</code> matrix <code>Monitor</code>, where <code class="reqn">N</code> is
the number of thinned samples and J is the number of monitored
variables.</p>
</td></tr>
<tr><td><code>post</code></td>
<td>
<p>This is the estimated size in MB of RAM required for a
matrix of posterior samples. This matrix is used in some algorithms,
and is not returned by <code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>.</p>
</td></tr>
<tr><td><code>Posterior1</code></td>
<td>
<p>This is the estimated size in MB of RAM required for
the <code class="reqn">N \times J</code> matrix <code>Posterior1</code>, where <code class="reqn">N</code>
is the number of thinned samples and <code class="reqn">J</code> is the number of
initial values or parameters.</p>
</td></tr>
<tr><td><code>Posterior2</code></td>
<td>
<p>This is the estimated size in MB of RAM required for
the <code class="reqn">N \times J</code> matrix <code>Posterior2</code>, where <code class="reqn">N</code>
is the number of globally stationary thinned samples and <code class="reqn">J</code> is
the number of initial values or parameters. Maximum RAM use is
assumed here, so the same <code class="reqn">N</code> is used, as in <code>Posterior1</code>.</p>
</td></tr>
<tr><td><code>Summary1</code></td>
<td>
<p>This is the estimated size in MB of RAM required for
the summary table of all thinned posterior samples of parameters,
deviance, and monitored variables.</p>
</td></tr>
<tr><td><code>Summary2</code></td>
<td>
<p>This is the estimated size in MB of RAM required for
the summary table of all globally stationary thinned posterior
samples of parameters, deviance, and monitored variables.</p>
</td></tr>
<tr><td><code>Total</code></td>
<td>
<p>This is the estimated size in MB of RAM required in total
to update one chain in <code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code> for a given model
and data, and for a number of iterations and specified thinning.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Statisticat, LLC <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+BigData">BigData</a></code>,
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>,
<code><a href="#topic+LML">LML</a></code>, and
<code><a href="utils.html#topic+object.size">object.size</a></code>.
</p>

<hr>
<h2 id='Levene.Test'>Levene's Test</h2><span id='topic+Levene.Test'></span>

<h3>Description</h3>

<p>The <code>Levene.Test</code> function is a Bayesian form of Levene's test
(Levene, 1960) of equality of variances.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Levene.Test(x, Method="U", G=NULL, Data=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Levene.Test_+3A_x">x</code></td>
<td>
<p>This required argument must be an object of class
<code>demonoid.ppc</code>, <code>iterquad.ppc</code>, <code>laplace.ppc</code>,
<code>pmc.ppc</code>, or <code>vb.ppc</code>.</p>
</td></tr>
<tr><td><code id="Levene.Test_+3A_method">Method</code></td>
<td>
<p>The method defaults to <code>U</code> for a univariate
dependent variable (DV), y. When the DV is multivariate,
<code>Method="C"</code> applies Levene's test to each column associated
in Y. When <code>Method="R"</code>, Levene's test is applied to each
row associated in Y.</p>
</td></tr>
<tr><td><code id="Levene.Test_+3A_g">G</code></td>
<td>
<p>This argument defaults to <code>NULL</code>, or is required to
have the same dimensions as the DV. For example, if the DV is
univariate, then G must have a length equal to y, which is usually
represented with a length of N for the number of records or T for
the number of time-periods. If the DV is multivariate, then
<code>G</code> must be a matrix, like Y, and have the same number of
rows and columns. The purpose of the <code>G</code> argument is to allow
the user to specify each element of y or Y to be in a particular
group, so the variance of the groups can be tested. As such, each
element of <code>G</code> must consist of an integer from one to the
number of groups desired to be tested. The reason this test allows
this degree of specificity is so the user can specify groups, such
as according to covariate levels. By default, 4 groups are
specified, where the first quarter of the records are group 1 and
the last quarter of the records are group 4.</p>
</td></tr>
<tr><td><code id="Levene.Test_+3A_data">Data</code></td>
<td>
<p>This argument is required when the DV is multivariate,
hence when <code>Method="C"</code> or <code>Method="R"</code>. The DV is
required to be named Y.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is a Bayesian form of Levene's test. Levene's test is
used to assess the probability of the equality of residual variances
in different groups. When residual variance does not differ by group,
it is often called homoscedastic (or homoskedastic) residual variance.
Homoskedastic residual variance is a common assumption. An advantage
of Levene's test to other tests of homoskedastic residual variance is
that Levene's test does not require normality of the residuals.
</p>
<p>The <code>Levene.Test</code> function estimates the test statistic,
<code class="reqn">W</code>, as per Levene's test. This Bayesian form, however,
estimates <code class="reqn">W</code> from the observed residuals as
<code class="reqn">W^{obs}</code>, and <code class="reqn">W</code> from residuals that are
replicated from a homoskedastic process as <code class="reqn">W^{rep}</code>.
Further, <code class="reqn">W^{obs}</code> and <code class="reqn">W^{rep}</code> are
estimated for each posterior sample. Finally, the probability that
the distribution of <code class="reqn">W^{obs}</code> is greater than the
distribution of <code class="reqn">W^{rep}</code> is reported (see below).
</p>


<h3>Value</h3>

<p>The <code>Levene.Test</code> function returns a plot (or for multivariate Y,
a series of plots), and a vector with a length equal to the number of
Levene's tests conducted.
</p>
<p>One plot is produced per univariate application of Levene's test. Each
plot shows the test statistic W, both from the observed process
(W.obs as a black density) and the replicated process (W.rep as a red
line). The mean of W.obs is reported, along with its 95% quantile-based
probability interval (see <code><a href="#topic+p.interval">p.interval</a></code>), the probability
<code class="reqn">p(W^{obs} &gt; W^{rep})</code>, and the indicated
results, either homoskedastic or heteroskedastic.
</p>
<p>Each element of the returned vector is the probability
<code class="reqn">p(W^{obs} &gt; W^{rep})</code>. When the probability
is <code class="reqn">p &lt; 0.025</code> or <code class="reqn">p &gt; 0.975</code>,
heteroskedastic variance is indicated. Otherwise, the variances of
the groups are assumed not to differ effectively.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>References</h3>

<p>Levene, H. (1960). &quot;Robust Tests for Equality of Variances&quot;. In
I. Olkins, S. G. Ghurye, W. Hoeffding, W. G. Madow, &amp; H. B. Mann
(Eds.), <em>Contributions to Probability and Statistics</em>,
p. 278&ndash;292. Stanford University Press: Stanford, CA.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code>,
<code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>,
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>,
<code><a href="#topic+PMC">PMC</a></code>,
<code><a href="#topic+p.interval">p.interval</a></code>, and
<code><a href="#topic+VariationalBayes">VariationalBayes</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#First, update the model with IterativeQuadrature, LaplaceApproximation,
#  LaplacesDemon, PMC, or VariationalBayes.
#Then, use the predict function, creating, say, object Pred.
#Finally:
#Levene.Test(Pred)
</code></pre>

<hr>
<h2 id='LML'>Logarithm of the Marginal Likelihood</h2><span id='topic+LML'></span>

<h3>Description</h3>

<p>This function approximates the logarithm of the marginal likelihood
(LML), where the marginal likelihood is also called the integrated
likelihood or the prior predictive distribution of <code class="reqn">\textbf{y}</code>
in Bayesian inference. The marginal likelihood is
</p>
<p style="text-align: center;"><code class="reqn">p(\textbf{y}) = \int p(\textbf{y} | \Theta)p(\Theta) d\Theta</code>
</p>

<p>The prior predictive distribution indicates what <code class="reqn">\textbf{y}</code>
should look like, given the model, before <code class="reqn">\textbf{y}</code> has been
observed. The presence of the marginal likelihood of
<code class="reqn">\textbf{y}</code> normalizes the joint posterior distribution,
<code class="reqn">p(\Theta|\textbf{y})</code>, ensuring it is a proper
distribution and integrates to one (see <code><a href="#topic+is.proper">is.proper</a></code>). The
marginal likelihood is the denominator of Bayes' theorem, and is often
omitted, serving as a constant of proportionality. Several methods of
approximation are available.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>LML(Model=NULL, Data=NULL, Modes=NULL, theta=NULL, LL=NULL, Covar=NULL,
method="NSIS")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="LML_+3A_model">Model</code></td>
<td>
<p>This is the model specification for the model that was
updated either in <code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code>,
<code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>, <code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>,
<code><a href="#topic+LaplacesDemon.hpc">LaplacesDemon.hpc</a></code>, or <code><a href="#topic+VariationalBayes">VariationalBayes</a></code>.
This argument is used only with the <code>LME</code> method.</p>
</td></tr>
<tr><td><code id="LML_+3A_data">Data</code></td>
<td>
<p>This is the list of data passed to the model
specification. This argument is used only with the <code>LME</code>
method.</p>
</td></tr>
<tr><td><code id="LML_+3A_modes">Modes</code></td>
<td>
<p>This is a vector of the posterior modes (or medians, in
the case of MCMC). This argument is used only with the <code>GD</code> or
<code>LME</code> methods.</p>
</td></tr>
<tr><td><code id="LML_+3A_theta">theta</code></td>
<td>
<p>This is a matrix of posterior samples (parameters only),
and is specified only with the <code>GD</code>, <code>HME</code>, or
<code>NSIS</code> methods.</p>
</td></tr>
<tr><td><code id="LML_+3A_ll">LL</code></td>
<td>
<p>This is a vector of MCMC samples of the log-likelihood, and
is specified only with the <code>GD</code>, codeHME, or <code>NSIS</code>
methods.</p>
</td></tr>
<tr><td><code id="LML_+3A_covar">Covar</code></td>
<td>
<p>This argument accepts the covariance matrix of the
posterior modes, and is used only with the <code>GD</code> or <code>LME</code>
methods.</p>
</td></tr>
<tr><td><code id="LML_+3A_method">method</code></td>
<td>
<p>The method may be <code>"GD"</code>, <code>"HME"</code>,
<code>"LME"</code>, or <code>"NSIS"</code>, and defaults to <code>"NSIS"</code>.
<code>"GD"</code> uses the Gelfand-Dey estimator, <code>"HME"</code> uses the
Harmonic Mean Estimator, <code>"LME"</code> uses the Laplace-Metropolis
Estimator, and <code>"NSIS"</code> uses nonparametric self-normalized
importance sampling (NSIS).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Generally, a user of <code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>,
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>, <code><a href="#topic+LaplacesDemon.hpc">LaplacesDemon.hpc</a></code>,
<code><a href="#topic+PMC">PMC</a></code>, or <code><a href="#topic+VariationalBayes">VariationalBayes</a></code> does not need to
use the <code>LML</code> function, because these methods already include
it. However, <code>LML</code> may be called by the user, should the user
desire to estimate the logarithm of the marginal likelihood with a
different method, or with non-stationary chains. The
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code> and <code><a href="#topic+LaplacesDemon.hpc">LaplacesDemon.hpc</a></code>
functions only call <code>LML</code> when all parameters are stationary, and
only with non-adaptive algorithms.
</p>
<p>The <code>GD</code> method, where GD stands for Gelfand-Dey (1994), is a
modification of the harmonic mean estimator (HME) that results in a
more stable estimator of the logarithm of the marginal
likelihood. This method is unbiased, simulation-consistent, and
usually satisfies the Gaussian central limit theorem.
</p>
<p>The <code>HME</code> method, where HME stands for harmonic mean estimator,
of Newton-Raftery (1994) is the easiest, and therefore fastest,
estimation of the logarithm of the marginal likelihood. However, it is
an unreliable estimator and should be avoided, because small
likelihood values can overly influence the estimator, variance is
often infinite, and the Gaussian central limit theorem is usually not
satisfied. It is included here for completeness. There is not a
function in this package that uses this method by default. Given
<code class="reqn">N</code> samples, the estimator is
<code class="reqn">1/[\frac{1}{N} \sum_N \exp(-LL)]</code>.
</p>
<p>The <code>LME</code> method uses the Laplace-Metropolis Estimator (LME), in
which the estimation of the Hessian matrix is approximated
numerically. It is the slowest method here, though it returns an
estimate in more cases than the other methods. The supplied
<code>Model</code> specification must be executed a number of times equal
to <code class="reqn">k^2 \times 4</code>, where <code class="reqn">k</code> is the number of
parameters. In large dimensions, this is very slow. The
Laplace-Metropolis Estimator is inappropriate with hierarchical
models. The <code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code>,
<code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>, and <code><a href="#topic+VariationalBayes">VariationalBayes</a></code>
functions use <code>LME</code> when it has converged and <code>sir=FALSE</code>,
in which case it uses the posterior means or modes, and is itself
Laplace Approximation.
</p>
<p>The Laplace-Metropolis Estimator (LME) is the logarithmic form of
equation 4 in Lewis and Raftery (1997). In a non-hierarchical model,
the marginal likelihood may easily be approximated with the
Laplace-Metropolis Estimator for model <code class="reqn">m</code> as
</p>
<p style="text-align: center;"><code class="reqn">p(\textbf{y}|m) =
  (2\pi)^{d_m/2}|\Sigma_m|^{1/2}p(\textbf{y}|\Theta_m,m)p(\Theta_m|m)</code>
</p>

<p>where <code class="reqn">d</code> is the number of parameters and <code class="reqn">\Sigma</code> is
the inverse of the negative of the approximated Hessian matrix of
second derivatives.
</p>
<p>As a rough estimate of Kass and Raftery (1995), LME is worrisome when
the sample size of the data is less than five times the number of
parameters, and LME should be adequate in most problems when the
sample size of the data exceeds twenty times the number of parameters
(p. 778).
</p>
<p>The <code>NSIS</code> method is essentially the <code>MarginalLikelihood</code>
function in the <code>MargLikArrogance</code> package. After <code>HME</code>,
this is the fastest method available here. The
<code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code>,
<code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>, and <code><a href="#topic+VariationalBayes">VariationalBayes</a></code>
functions use <code>NSIS</code> when converged and <code>sir=TRUE</code>. The
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>, <code><a href="#topic+LaplacesDemon.hpc">LaplacesDemon.hpc</a></code>, and
<code><a href="#topic+PMC">PMC</a></code> functions use <code>NSIS</code>. At least 301 stationary
samples are required, and the number of parameters cannot exceed half
the number of stationary samples.
</p>


<h3>Value</h3>

<p><code>LML</code> returns a list with two components:
</p>
<table>
<tr><td><code>LML</code></td>
<td>

<p>This is an approximation of the logarithm of the marginal
likelihood (LML), which is notoriously difficult to estimate. For this
reason, several methods are provided. The marginal likelihood is
useful when comparing models, such as with Bayes factors in the
<code><a href="#topic+BayesFactor">BayesFactor</a></code> function. When the method fails, <code>NA</code>
is returned, and it is most likely that the joint posterior is
improper (see <code><a href="#topic+is.proper">is.proper</a></code>).</p>
</td></tr>
<tr><td><code>VarCov</code></td>
<td>

<p>This is a variance-covariance matrix, and is the negative inverse of
the Hessian matrix, if estimated. The <code>GD</code>, <code>HME</code>, and
<code>NSIS</code> methods do not estimate <code>VarCov</code>, and return
<code>NA</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>References</h3>

<p>Gelfand, A.E. and Dey, D.K. (1994). &quot;Bayesian Model Choice:
Asymptotics and Exact Calculations&quot;. <em>Journal of the Royal
Statistical Society</em>, Series B 56, p. 501&ndash;514.
</p>
<p>Kass, R.E. and Raftery, A.E. (1995). &quot;Bayes Factors&quot;. <em>Journal
of the American Statistical Association</em>, 90(430), p. 773&ndash;795.
</p>
<p>Lewis, S.M. and Raftery, A.E. (1997). &quot;Estimating Bayes Factors via
Posterior Simulation with the Laplace-Metropolis Estimator&quot;.
<em>Journal of the American Statistical Association</em>, 92,
p. 648&ndash;655.
</p>
<p>Newton, M.A. and Raftery, A.E. (1994). &quot;Approximate Bayesian
Inference by the Weighted Likelihood Bootstrap&quot;. <em>Journal of the
Royal Statistical Society</em>, Series B 3, p. 3&ndash;48.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+BayesFactor">BayesFactor</a></code>,
<code><a href="#topic+is.proper">is.proper</a></code>,
<code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code>,
<code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>,
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>,
<code><a href="#topic+LaplacesDemon.hpc">LaplacesDemon.hpc</a></code>,
<code><a href="#topic+PMC">PMC</a></code>, and
<code><a href="#topic+VariationalBayes">VariationalBayes</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### If a model object were created and called Fit, then:
#
### Applying HME to an object of class demonoid or pmc:
#LML(LL=Fit$Deviance*(-1/2), method="HME")
#
### Applying LME to an object of class demonoid:
#LML(Model, MyData, Modes=apply(Fit$Posterior1, 2, median), method="LME")
#
### Applying NSIS to an object of class demonoid
#LML(theta=Fit$Posterior1, LL=Fit$Deviance*-(1/2), method="NSIS")
#
### Applying LME to an object of class iterquad:
#LML(Model, MyData, Modes=Fit$Summary1[,1], method="LME")
#
### Applying LME to an object of class laplace:
#LML(Model, MyData, Modes=Fit$Summary1[,1], method="LME")
#
### Applying LME to an object of class vb:
#LML(Model, MyData, Modes=Fit$Summary1[,1], method="LME")
</code></pre>

<hr>
<h2 id='log-log'>The log-log and complementary log-log functions</h2><span id='topic+cloglog'></span><span id='topic+invcloglog'></span><span id='topic+invloglog'></span><span id='topic+loglog'></span>

<h3>Description</h3>

<p>The log-log and complementary log-log functions, as well as the
inverse functions, are provided.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cloglog(p)
invcloglog(x)
invloglog(x)
loglog(p)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="log-log_+3A_x">x</code></td>
<td>
<p>This is a vector of real values that will be transformed to
the interval [0,1].</p>
</td></tr>
<tr><td><code id="log-log_+3A_p">p</code></td>
<td>
<p>This is a vector of probabilities p in the interval [0,1]
that will be transformed to the real line.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The logit and probit links are symmetric, because the probabilities
approach zero or one at the same rate. The log-log and complementary
log-log links are asymmetric. Complementary log-log links approach
zero slowly and one quickly. Log-log links approach zero quickly and
one slowly. Either the log-log or complementary log-log link will tend
to fit better than logistic and probit, and are frequently used when
the probability of an event is small or large. A mixture of the two
links, the log-log and complementary log-log is often used, where each
link is weighted. The reason that logit is so prevalent is because
logistic parameters can be interpreted as odds ratios.
</p>


<h3>Value</h3>

<p><code>cloglog</code> returns <code>x</code>,
<code>invcloglog</code> and <code>invloglog</code> return probability <code>p</code>,
and <code>loglog</code> returns <code>x</code>.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
x &lt;- -5:5
p &lt;- invloglog(x)
x &lt;- loglog(p)
</code></pre>

<hr>
<h2 id='logit'>The logit and inverse-logit functions</h2><span id='topic+invlogit'></span><span id='topic+logit'></span>

<h3>Description</h3>

<p>The logit and inverse-logit (also called the logistic function) are
provided.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>invlogit(x)
logit(p)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="logit_+3A_x">x</code></td>
<td>
<p>This object contains real values that will be transformed to
the interval [0,1].</p>
</td></tr>
<tr><td><code id="logit_+3A_p">p</code></td>
<td>
<p>This object contains of probabilities p in the interval [0,1]
that will be transformed to the real line.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>logit</code> function is the inverse of the sigmoid or logistic
function, and transforms a continuous value (usually probability
<code class="reqn">p</code>) in the interval [0,1] to the real line (where it is usually
the logarithm of the odds). The <code>logit</code> function is <code class="reqn">\log(p /
    (1-p))</code>.
</p>
<p>The <code>invlogit</code> function (called either the inverse logit or the
logistic function) transforms a real number (usually the logarithm of
the odds) to a value (usually probability <code class="reqn">p</code>) in the interval
[0,1]. The <code>invlogit</code> function is <code class="reqn">\frac{1}{1 + \exp(-x)}</code>.
</p>
<p>If <code class="reqn">p</code> is a probability, then <code class="reqn">\frac{p}{1-p}</code> is the
corresponding odds, while the <code>logit</code> of <code class="reqn">p</code> is the logarithm
of the odds. The difference between the logits of two probabilities is
the logarithm of the odds ratio. The derivative of probability <code class="reqn">p</code>
in a logistic function (such as <code>invlogit</code>) is: <code class="reqn">\frac{d}{dx}
    = p(1-p)</code>.
</p>
<p>In the LaplacesDemon package, it is common to re-parameterize a model
so that a parameter that should be in an interval can be updated from
the real line by using the <code>logit</code> and <code>invlogit</code> functions,
though the <code><a href="#topic+interval">interval</a></code> function provides an
alternative. For example, consider a parameter <code class="reqn">\theta</code>
that must be in the interval [0,1]. The algorithms in
<code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code>, <code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>,
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>, <code><a href="#topic+PMC">PMC</a></code>, and
<code><a href="#topic+VariationalBayes">VariationalBayes</a></code> are unaware of the desired interval,
and may attempt <code class="reqn">\theta</code> outside of this interval. One
solution is to have the algorithms update <code>logit(theta)</code> rather
than <code>theta</code>. After <code>logit(theta)</code> is manipulated by the
algorithm, it is transformed via <code>invlogit(theta)</code> in the model
specification function, where <code class="reqn">\theta \in [0,1]</code>.
</p>


<h3>Value</h3>

<p><code>invlogit</code> returns probability <code>p</code>, and
<code>logit</code> returns <code>x</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+interval">interval</a></code>,
<code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code>,
<code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>,
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>,
<code><a href="stats.html#topic+plogis">plogis</a></code>,
<code><a href="#topic+PMC">PMC</a></code>,
<code><a href="stats.html#topic+qlogis">qlogis</a></code>, and
<code><a href="#topic+VariationalBayes">VariationalBayes</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
x &lt;- -5:5
p &lt;- invlogit(x)
x &lt;- logit(p)
</code></pre>

<hr>
<h2 id='LossMatrix'>Loss Matrix</h2><span id='topic+LossMatrix'></span>

<h3>Description</h3>

<p>A loss matrix is useful in Bayesian decision theory for selecting the
Bayes action, the optimal Bayesian decision, when there are a discrete
set of possible choices (actions) and a discrete set of possible
outcomes (states of the world). The Bayes action is the action that
minimizes expected loss, which is equivalent to maximizing expected
utility.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>LossMatrix(L, p.theta)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="LossMatrix_+3A_l">L</code></td>
<td>
<p>This required argument accepts a <code class="reqn">S \times A</code>
matrix or <code class="reqn">S \times A \times N</code> array of losses,
where <code class="reqn">S</code> is the number of states of the world, <code class="reqn">A</code> is the
number of actions, and <code class="reqn">N</code> is the number of samples. These
losses have already been estimated, given a personal loss
function. One or more personal losses has already been estimated
for each combination of possible actions
<code class="reqn">a=1,\dots,A</code> and possible states
<code class="reqn">s=1,\dots,S</code>.</p>
</td></tr>
<tr><td><code id="LossMatrix_+3A_p.theta">p.theta</code></td>
<td>
<p>This required argument accepts a
<code class="reqn">S \times A</code> matrix or
<code class="reqn">S \times A \times N</code> array of state prior
probabilities, where <code class="reqn">S</code> is the number of states of the world,
<code class="reqn">A</code> is the number of actions, and <code class="reqn">N</code> is the number of
samples. The sum of each column must equal one.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Bayesian inference is often tied to decision theory (Bernardo and
Smith, 2000), and decision theory has long been considered the
foundations of statistics (Savage, 1954).
</p>
<p>Before using the <code>LossMatrix</code> function, the user should have
already considered all possible actions (choices), states of the world
(outcomes unknown at the time of decision-making), chosen a loss
function <code class="reqn">L(\theta, \alpha)</code>, estimated loss, and
elicited prior probabilities <code class="reqn">p(\theta | x)</code>.
</p>
<p>Although possible actions (choices) for the decision-maker and
possible states (outcomes) may be continuous or discrete, the loss
matrix is used for discrete actions and states. An example of a
continuous action may be that a decision-maker has already decided to
invest, and the remaining, current decision is how much to invest.
Likewise, an example of continuous states of the world (outcomes) may
be how much profit or loss may occur after a given continuous unit of
time.
</p>
<p>The coded example provided below is taken from Berger (1985, p. 6-7)
and described here. The set of possible actions for a decision-maker
is to invest in bond ZZZ or alternatively in bond XXX, as it is called
here. A real-world decision should include a mutually exhaustive list
of actions, such as investing in neither, but perhaps the
decision-maker has already decided to invest and narrowed the options
down to these two bonds.
</p>
<p>The possible states of the world (outcomes unknown at the time of
decision-making) are considered to be two states: either the chosen
bond will not default or it will default. Here, the loss function is
a negative linear identity of money, and hence a loss in element
<code>L[1,1]</code> of -500 is a profit of 500, while a loss in
<code>L[2,1]</code> of 1,000 is a loss of 1,000.
</p>
<p>The decision-maker's dilemma is that bond ZZZ may return a higher
profit than bond XXX, however there is an estimated 10% chance, the
prior probability, that bond ZZZ will default and return a substantial
loss. In contrast, bond XXX is considered to be a sure-thing and 
return a steady but smaller profit. The Bayes action is to choose the
first action and invest in bond ZZZ, because it minimizes expected
loss, even though there is a chance of default.
</p>
<p>A more realistic application of a loss matrix may be to replace the
point-estimates of loss with samples given uncertainty around the
estimated loss, and replace the point-estimates of the prior
probability of each state with samples given the uncertainty of the
probability of each state. The loss function used in the example is
intuitive, but a more popular monetary loss function may be
<code class="reqn">-\log(E(W | R))</code>, the negative log of the
expectation of wealth, given the return. There are many alternative
loss functions.
</p>
<p>Although isolated decision-theoretic problems exist such as the
provided example, decision theory may also be applied to the results
of a probability model (such as from
<code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code>, <code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>,
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>, <code><a href="#topic+PMC">PMC</a></code>), or
<code><a href="#topic+VariationalBayes">VariationalBayes</a></code>, contingent on how
a decision-maker is considering to use the information from the
model. The statistician may pass the results of a model to a client,
who then considers choosing possible actions, given this
information. The statistician should further assist the client with
considering actions, states of the world, then loss functions, and
finally eliciting the client's prior probabilities (such as with the
<code><a href="#topic+elicit">elicit</a></code> function).
</p>
<p>When the outcome is finally observed, the information from this
outcome may be used to refine the priors of the next such decision. In
this way, Bayesian learning occurs.
</p>


<h3>Value</h3>

<p>The <code>LossMatrix</code> function returns a list with two components:
</p>
<table>
<tr><td><code>BayesAction</code></td>
<td>
<p>This is a numeric scalar that indicates the action
that minimizes expected loss.</p>
</td></tr>
<tr><td><code>E.Loss</code></td>
<td>
<p>This is a vector of expected losses, one for each
action.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>References</h3>

<p>Berger, J.O. (1985). &quot;Statistical Decision Theory and Bayesian
Analysis&quot;, Second Edition. Springer: New York, NY.
</p>
<p>Bernardo, J.M. and Smith, A.F.M. (2000). &quot;Bayesian Theory&quot;. John
Wiley \&amp; Sons: West Sussex, England.
</p>
<p>Savage, L.J. (1954). &quot;The Foundations of Statistics&quot;. John Wiley \&amp;
Sons: West Sussex, England.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+elicit">elicit</a></code>,
<code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code>,
<code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>,
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>,
<code><a href="#topic+PMC">PMC</a></code>, and
<code><a href="#topic+VariationalBayes">VariationalBayes</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
### Point-estimated loss and state probabilities
L &lt;- matrix(c(-500,1000,-300,-300), 2, 2)
rownames(L) &lt;- c("s[1]: !Defaults","s[2]: Defaults")
colnames(L) &lt;- c("a[1]: Buy ZZZ", "a[2]: Buy XXX")
L
p.theta &lt;- matrix(c(0.9, 0.1, 1, 0), 2, 2)
Fit &lt;- LossMatrix(L, p.theta)

### Point-estimated loss and samples of state probabilities
L &lt;- matrix(c(-500,1000,-300,-300), 2, 2)
rownames(L) &lt;- c("s[1]: Defaults","s[2]: !Defaults")
colnames(L) &lt;- c("a[1]: Buy ZZZ", "a[2]: Buy XXX")
L
p.theta &lt;- array(runif(4000), dim=c(2,2,1000)) #Random probabilities,
#just for a quick example. And, since they must sum to one:
for (i in 1:1000) {
     p.theta[,,i] &lt;- p.theta[,,i] / matrix(colSums(p.theta[,,i]),
          dim(p.theta)[1], dim(p.theta)[2], byrow=TRUE)}
Fit &lt;- LossMatrix(L, p.theta)
Fit

### Point-estimates of loss may be replaced with samples as well.
</code></pre>

<hr>
<h2 id='LPL.interval'>Lowest Posterior Loss Interval</h2><span id='topic+LPL.interval'></span>

<h3>Description</h3>

<p>This function returns the Lowest Posterior Loss (LPL) interval for one
parameter, given samples from the density of its prior distribution
and samples of the posterior distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>LPL.interval(Prior, Posterior, prob=0.95, plot=FALSE, PDF=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="LPL.interval_+3A_prior">Prior</code></td>
<td>
<p>This is a vector of samples of the prior density.</p>
</td></tr>
<tr><td><code id="LPL.interval_+3A_posterior">Posterior</code></td>
<td>
<p>This is a vector of posterior samples.</p>
</td></tr>
<tr><td><code id="LPL.interval_+3A_prob">prob</code></td>
<td>
<p>This is a numeric scalar in the interval (0,1) giving the
Lowest Posterior Loss (LPL) interval, and defaults to 0.95,
representing a 95% LPL interval.</p>
</td></tr>
<tr><td><code id="LPL.interval_+3A_plot">plot</code></td>
<td>
<p>Logical. When <code>plot=TRUE</code>, two plots are
produced. The top plot shows the expected posterior loss. The LPL
region is shaded black, and the area outside the region is gray. The
bottom plot shows LPL interval of <code class="reqn">\theta</code> on the kernel
density of <code class="reqn">\theta</code>. Again, the LPL region is shaded
black, and the outside area is gray. A vertical, red, dotted line is
added at zero for both plots. The <code>plot</code> argument defaults to
<code>FALSE</code>. The plot treats the distribution as if it were
unimodal; disjoint regions are not estimated here. If multimodality
should result in disjoint regions, then consider using HPD intervals
in the <code><a href="#topic+p.interval">p.interval</a></code> function.</p>
</td></tr>
<tr><td><code id="LPL.interval_+3A_pdf">PDF</code></td>
<td>
<p>Logical. When <code>PDF=TRUE</code>, and only when
<code>plot=TRUE</code>, plots are saved as a .pdf file in the working
directory.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Lowest Posterior Loss (LPL) interval (Bernardo, 2005), or LPLI, is
a probability interval based on intrinsic discrepancy loss between
prior and posterior distributions. The expected posterior loss
is the loss associated with using a particular value
<code class="reqn">\theta_i \in \theta</code> of the parameter as the
unknown true value of <code class="reqn">\theta</code> (Bernardo, 2005). Parameter
values with smaller expected posterior loss should always be
preferred. The LPL interval includes a region in which all parameter
values have smaller expected posterior loss than those outside the
region.
</p>
<p>Although any loss function could be used, the loss function should be
invariant under reparameterization. Any intrinsic loss function is
invariant under reparameterization, but not necessarily invariant
under one-to-one transformations of data <code class="reqn">\textbf{x}</code>. When a
loss function is also invariant under one-to-one transformations, it
is usually also invariant when reduced to a sufficient statistic. Only
an intrinsic loss function that is invariant when reduced to a
sufficient statistic should be considered.
</p>
<p>The intrinsic discrepancy loss is easily a superior loss function to
the overused quadratic loss function, and is more appropriate than
other popular measures, such as Hellinger distance, Kullback-Leibler
divergence (<code><a href="#topic+KLD">KLD</a></code>), and Jeffreys logarithmic
divergence. The intrinsic discrepancy loss is also an
information-theory related divergence measure. Intrinsic discrepancy
loss is a symmetric, non-negative loss function, and is a continuous,
convex function. Intrinsic discrepancy loss was introduced
by Bernardo and Rueda (2002) in a different context: hypothesis
testing. Formally, it is:
</p>
<p style="text-align: center;"><code class="reqn">\delta f(p_2,p_1) = min[\kappa(p_2 | p_1), \kappa(p_1 |
    p_2)]</code>
</p>

<p>where <code class="reqn">\delta</code> is the discrepancy, <code class="reqn">\kappa</code> is
the <code><a href="#topic+KLD">KLD</a></code>, and <code class="reqn">p_1</code> and <code class="reqn">p_2</code> are the
probability distributions. The intrinsic discrepancy loss is the loss
function, and the expected posterior loss is the mean of the directed
divergences.
</p>
<p>The LPL interval is also called an intrinsic credible interval or
intrinsic probability interval, and the area inside the interval is
often called an intrinsic credible region or intrinsic probability
region.
</p>
<p>In practice, whether a reference prior or weakly informative prior
(WIP) is used, the LPL interval is usually very close to the HPD
interval, though the posterior losses may be noticeably different. If
LPL used a zero-one loss function, then the HPD interval would be
produced. An advantage of the LPL interval over HPD interval (see
<code><a href="#topic+p.interval">p.interval</a></code>) is that the LPL interval is invariant to
reparameterization. This is due to the invariant reparameterization
property of reference priors. The quantile-based probability interval
is also invariant to reparameterization. The LPL interval enjoys the
same advantage as the HPD interval does over the quantile-based
probability interval: it does not produce equal tails when
inappropriate.
</p>
<p>Compared with probability intervals, the LPL interval is slightly less
convenient to calculate. Although the prior distribution is specified
within the <code>Model</code> specification function, the user must specify
it for the <code>LPL.interval</code> function as well. A comparison of the
quantile-based probability interval, HPD interval, and LPL interval is
available here: <a href="https://web.archive.org/web/20150214090353/http://www.bayesian-inference.com/credible">https://web.archive.org/web/20150214090353/http://www.bayesian-inference.com/credible</a>.
</p>


<h3>Value</h3>

<p>A matrix is returned with one row and two columns. The row represents
the parameter and the column names are <code>"Lower"</code> and
<code>"Upper"</code>. The elements of the matrix are the lower and upper
bounds of the LPL interval.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC.</p>


<h3>References</h3>

<p>Bernardo, J.M. (2005). &quot;Intrinsic Credible Regions: An Objective
Bayesian Approach to Interval Estimation&quot;. <em>Sociedad de
Estadistica e Investigacion Operativa</em>, 14(2), p. 317&ndash;384.
</p>
<p>Bernardo, J.M. and Rueda, R. (2002). &quot;Bayesian Hypothesis Testing: A
Reference Approach&quot;. <em>International Statistical Review</em>, 70,
p. 351&ndash;372.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+KLD">KLD</a></code>,
<code><a href="#topic+p.interval">p.interval</a></code>,
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>, and
<code><a href="#topic+PMC">PMC</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
#Although LPL is intended to be applied to output from LaplacesDemon or
#PMC, here is an example in which p(theta) ~ N(0,100), and
#p(theta | y) ~ N(1,10), given 1000 samples.
theta &lt;- rnorm(1000,1,10)
LPL.interval(Prior=dnorm(theta,0,100^2), Posterior=theta, prob=0.95,
  plot=TRUE)
#A more practical example follows, but it assumes a model has been
#updated with LaplacesDemon or PMC, the output object is called Fit, and
#that the prior for the third parameter is normally distributed with
#mean 0 and variance 100: 
#temp &lt;- Fit$Posterior2[,3]
#names(temp) &lt;- colnames(Fit$Posterior2)[3]
#LPL.interval(Prior=dnorm(temp,0,100^2), Posterior=temp, prob=0.95,
#     plot=TRUE, PDF=FALSE)
</code></pre>

<hr>
<h2 id='Math'>Math Utility Functions</h2><span id='topic+GaussHermiteQuadRule'></span><span id='topic+Hermite'></span><span id='topic+logadd'></span><span id='topic+partial'></span>

<h3>Description</h3>

<p>These are utility functions for math.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>GaussHermiteQuadRule(N)
Hermite(x, N, prob=TRUE)
logadd(x, add=TRUE)
partial(Model, parm, Data, Interval=1e-6, Method="simple")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Math_+3A_n">N</code></td>
<td>
<p>This required argument accepts a positive integer that
indicates the number of nodes.</p>
</td></tr>
<tr><td><code id="Math_+3A_x">x</code></td>
<td>
<p>This is a numeric vector.</p>
</td></tr>
<tr><td><code id="Math_+3A_add">add</code></td>
<td>
<p>Logical. This defaults to <code>TRUE</code>, in which case
<code class="reqn">\log(x+y)</code> is performed. Otherwise,
<code class="reqn">\log(x-y)</code> is performed.</p>
</td></tr>
<tr><td><code id="Math_+3A_model">Model</code></td>
<td>
<p>This is a model specification function. For more
information, see <code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>.</p>
</td></tr>
<tr><td><code id="Math_+3A_parm">parm</code></td>
<td>
<p>This is a vector parameters.</p>
</td></tr>
<tr><td><code id="Math_+3A_prob">prob</code></td>
<td>
<p>Logical. This defaults to <code>TRUE</code>, which uses the
probabilist's kernel for the Hermite polynomial. Otherwise,
<code>FALSE</code> uses the physicist's kernel.</p>
</td></tr>
<tr><td><code id="Math_+3A_data">Data</code></td>
<td>
<p>This is a list of data. For more information, see
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>.</p>
</td></tr>
<tr><td><code id="Math_+3A_interval">Interval</code></td>
<td>
<p>This is the interval of numeric differencing.</p>
</td></tr>
<tr><td><code id="Math_+3A_method">Method</code></td>
<td>
<p>This accepts a quoted string, and defaults to
&quot;simple&quot;, which is finite-differencing. Alternatively
<code>Method="Richardson"</code> uses Richardson extrapolation, which
is more accurate, but takes longer to calculate. Another method
called automatic differentiation is currently unsupported, but
is even more accurate, and takes even longer to calculate.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>GaussHermiteQuadRule</code> function returns nodes and weights for
univariate Gauss-Hermite quadrature. The nodes and weights are
obtained from a tridiagonal eigenvalue problem. Weights are calculated
from the physicist's (rather than the probabilist's) kernel. This has
been adapted from the GaussHermite function in the pracma package. The
<code><a href="#topic+GaussHermiteCubeRule">GaussHermiteCubeRule</a></code> function is a multivariate version.
This is used in the <code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code> function.
</p>
<p>The <code>Hermite</code> function evaluates a Hermite polynomial of degree
<code class="reqn">N</code> at <code class="reqn">x</code>, using either the probabilist's (<code>prob=TRUE</code>)
or physicist's (<code>prob=FALSE</code>) kernel. This function was adapted
from the <code>hermite</code> function in package EQL.
</p>
<p>The <code>logadd</code> function performs addition (or subtraction) when the
terms are logarithmic. The equations are:
</p>
<p style="text-align: center;"><code class="reqn">\log(x+y) = \log(x) + \log(1 + \exp(\log(y) - \log(x)))</code>
</p>

<p style="text-align: center;"><code class="reqn">\log(x-y) = \log(x) + \log(1 - \exp(\log(y) - \log(x)))</code>
</p>

<p>The <code>partial</code> function estimates partial derivatives of
parameters in a model specification with data, using either
forward finite-differencing or Richardson extrapolation. In calculus,
a partial derivative of a function of several variables is its
derivative with respect to one of those variables, with the others
held constant. Related functions include <code>Jacobian</code> which returns
a matrix of first-order partial derivatives, and <code>Hessian</code>, which
returns a matrix of second-order partial derivatives of the model
specification function with respect to its parameters. The
<code>partial</code> function is not intended to be called by the user, but
is used by other functions. This is essentially the <code>grad</code>
function in the numDeriv package, but defaulting to forward
finite-differencing with a smaller interval.
</p>


<h3>Value</h3>

<p><code>logadd</code> returns the result of <code class="reqn">\log(x+y)</code> or
<code class="reqn">\log(x-y)</code>.
</p>
<p><code>partial</code> returns a vector of partial derivatives.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+GaussHermiteCubeRule">GaussHermiteCubeRule</a></code>,
<code><a href="#topic+Hessian">Hessian</a></code>,
<code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code>,
<code><a href="#topic+Jacobian">Jacobian</a></code>,
<code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>,
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>, and
<code><a href="#topic+VariationalBayes">VariationalBayes</a></code>.
</p>

<hr>
<h2 id='Matrices'>Matrix Utility Functions</h2><span id='topic+as.indicator.matrix'></span><span id='topic+as.inverse'></span><span id='topic+as.parm.matrix'></span><span id='topic+as.positive.definite'></span><span id='topic+as.positive.semidefinite'></span><span id='topic+as.symmetric.matrix'></span><span id='topic+Cov2Cor'></span><span id='topic+CovEstim'></span><span id='topic+GaussHermiteCubeRule'></span><span id='topic+Hessian'></span><span id='topic+is.positive.definite'></span><span id='topic+is.positive.semidefinite'></span><span id='topic+is.square.matrix'></span><span id='topic+is.symmetric.matrix'></span><span id='topic+Jacobian'></span><span id='topic+logdet'></span><span id='topic+lower.triangle'></span><span id='topic+read.matrix'></span><span id='topic+SparseGrid'></span><span id='topic+TransitionMatrix'></span><span id='topic+tr'></span><span id='topic+upper.triangle'></span>

<h3>Description</h3>

<p>These are utility functions for working with matrices.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as.indicator.matrix(x)
as.inverse(x)
as.parm.matrix(x, k, parm, Data, a=-Inf, b=Inf, restrict=FALSE, chol=FALSE)
as.positive.definite(x)
as.positive.semidefinite(x)
as.symmetric.matrix(x, k=NULL)
is.positive.definite(x)
is.positive.semidefinite(x)
is.square.matrix(x)
is.symmetric.matrix(x)
Cov2Cor(Sigma)
CovEstim(Model, parm, Data, Method="Hessian")
GaussHermiteCubeRule(N, dims, rule)
Hessian(Model, parm, Data, Interval=1e-6, Method="Richardson")
Jacobian(Model, parm, Data, Interval=1e-6, Method="simple")
logdet(x)
lower.triangle(x, diag=FALSE)
read.matrix(file, header=FALSE, sep=",", nrow=0, samples=0, size=0, na.rm=FALSE)
SparseGrid(J, K)
TransitionMatrix(theta.y=NULL, y.theta=NULL, p.theta=NULL)
tr(x)
upper.triangle(x, diag=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Matrices_+3A_n">N</code></td>
<td>
<p>This required argument accepts a positive integer that
indicates the number of nodes.</p>
</td></tr>
<tr><td><code id="Matrices_+3A_x">x</code></td>
<td>
<p>This is a matrix (though <code>as.symmetric.matrix</code> also
accepts vectors).</p>
</td></tr>
<tr><td><code id="Matrices_+3A_j">J</code></td>
<td>
<p>This required argument indicates the dimension of the
integral and accepts a positive integer.</p>
</td></tr>
<tr><td><code id="Matrices_+3A_k">k</code></td>
<td>
<p>For <code>as.parm.matrix</code>, this is a required argument,
indicating the dimension of the matrix. For
<code>as.symmetric.matrix</code>, this is an optional argument that
specifies the dimension of the symmetric matrix. This applies
only when <code>x</code> is a vector. It defaults to <code>NULL</code>,
in which case it calculates <code>k &lt;- (-1 + sqrt(1 + 8 *
      length(x)))/ 2</code>.</p>
</td></tr>
<tr><td><code id="Matrices_+3A_k">K</code></td>
<td>
<p>This required argument indicates the accuracy and accepts a
positive integer. Larger values result in many more integration
nodes.</p>
</td></tr>
<tr><td><code id="Matrices_+3A_diag">diag</code></td>
<td>
<p>Logical. If <code>TRUE</code>, then the elements in the main
diagonal are also returned.</p>
</td></tr>
<tr><td><code id="Matrices_+3A_dims">dims</code></td>
<td>
<p>This required argument indicates the dimension of the
integral and accepts a positive integer.</p>
</td></tr>
<tr><td><code id="Matrices_+3A_sigma">Sigma</code></td>
<td>
<p>This is a covariance matrix, <code class="reqn">\Sigma</code>, and may
be entered either as a matrix or vector.</p>
</td></tr>
<tr><td><code id="Matrices_+3A_model">Model</code></td>
<td>
<p>This is a model specification function. For more
information, see <code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>.</p>
</td></tr>
<tr><td><code id="Matrices_+3A_parm">parm</code></td>
<td>
<p>This is a vector of parameters passed to the model
specification.</p>
</td></tr>
<tr><td><code id="Matrices_+3A_data">Data</code></td>
<td>
<p>This is the list of data passed to the model
specification. For more information, see <code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>.</p>
</td></tr>
<tr><td><code id="Matrices_+3A_a">a</code>, <code id="Matrices_+3A_b">b</code></td>
<td>
<p>These optional arguments allow the elements of <code>x</code> to
be bound to the interval <code class="reqn">[a,b]</code>. For example, elements of a
correlation matrix are in the interval <code class="reqn">[-1,1]</code>.</p>
</td></tr>
<tr><td><code id="Matrices_+3A_restrict">restrict</code></td>
<td>
<p>Logical. If <code>TRUE</code>, then <code>x[1,1]</code> is
restricted to 1. This is useful in multinomial probit, for
example. The variable, <code>LaplacesDemonMatrix</code>, is
created in a new environment, <code>LDEnv</code> so <code>as.parm.matrix</code>
can keep track of changes from iteration to iteration.</p>
</td></tr>
<tr><td><code id="Matrices_+3A_rule">rule</code></td>
<td>
<p>This is an optional argument that accepts a univariate
Gauss-Hermite quadrature rule. Usually, this argument is left empty.
A rule may be supplied that differs from the traditional rule, such
as when constraints have been observed, and one or more nodes and
weights were adjusted.</p>
</td></tr>
<tr><td><code id="Matrices_+3A_chol">chol</code></td>
<td>
<p>Logical. If <code>TRUE</code>, then x is an upper-triangular
matrix.</p>
</td></tr>
<tr><td><code id="Matrices_+3A_file">file</code></td>
<td>
<p>This is the name of the file from which the numeric data
matrix will be imported or read.</p>
</td></tr>
<tr><td><code id="Matrices_+3A_header">header</code></td>
<td>
<p>Logical. When <code>TRUE</code>, the first row of the file
must contain names of the columns, and will be converted to the
column names of the numeric matrix. When <code>FALSE</code>, the first row
of the file contains data, not column names.</p>
</td></tr>
<tr><td><code id="Matrices_+3A_interval">Interval</code></td>
<td>
<p>This accepts a small scalar number for precision.</p>
</td></tr>
<tr><td><code id="Matrices_+3A_method">Method</code></td>
<td>
<p>This accepts a quoted string. For <code>Hessian</code>, it
defaults to <code>Method="Richardson"</code>, which uses Richardson
extrapolation. For <code>Jacobian</code>, it defaults to
<code>Method="simple"</code>, which uses finite-differencing. Richardson
Richardson extrapolation is more accurate, but slower to calculate.
Since error due to finite-differencing propagates through to higher
derivatives, finite-differencing should not be used when
approximating a Hessian matrix. Another method called automatic
differentiation is not currently available here, but should be more
accurate, though even slower to calculate. Another popular
alternative is to use the <code><a href="#topic+BayesianBootstrap">BayesianBootstrap</a></code> on the
data. For <code>CovEstim</code>, this accepts <code>Method="Hessian"</code>,
<code>Method="Identity"</code> (which simply assigns an identity matrix),
<code>Method="OPG"</code> (which calculates the sum of outer products of
record-level gradients), or <code>Method="Sandwich"</code>, which is the
sandwich estimator and combines the Hessian and OPG estimates.</p>
</td></tr>
<tr><td><code id="Matrices_+3A_nrow">nrow</code></td>
<td>
<p>This is the number of rows of the numeric matrix, and
defaults to <code>nrow=0</code>. If the number is known, the function will
perform noticeably faster when it does not have to check.</p>
</td></tr>
<tr><td><code id="Matrices_+3A_p.theta">p.theta</code></td>
<td>
<p>This accepts a matrix of prior probabilities for a
transition matrix, and defaults to <code>NULL</code>. If used, each row
must sum to 1.</p>
</td></tr>
<tr><td><code id="Matrices_+3A_samples">samples</code></td>
<td>
<p>This is the number of samples to take from the numeric
matrix. When <code>samples=0</code>, sampling is not performed and the
entire matrix is returned.</p>
</td></tr>
<tr><td><code id="Matrices_+3A_sep">sep</code></td>
<td>
<p>This argument indicates a character with which it will
separate fields when creating column vectors. For example, a
read a comma-separated file (.csv), use <code>sep=","</code>.</p>
</td></tr>
<tr><td><code id="Matrices_+3A_size">size</code></td>
<td>
<p>This is the batch size to be used only when reading a
numeric matrix that is larger than the available computer memory
(RAM), and only when <code>samples</code> is greater than zero. Sampling
of a big data matrix is performed by first determining the records
to keep, and then reading batches, one by one, and keeping the
matching records.</p>
</td></tr>
<tr><td><code id="Matrices_+3A_theta.y">theta.y</code></td>
<td>
<p>This accepts a vector of posterior samples of a
discrete Markov chain, and defaults to <code>NULL</code>. If used, the
order of the samples affects the transition probability.</p>
</td></tr>
<tr><td><code id="Matrices_+3A_na.rm">na.rm</code></td>
<td>
<p>Logical. When <code>TRUE</code>, rows with missing values are
removed from the matrix after it is read. Rather than removing
missing values, the user may consider imputing missing values inside
the model, or before the model with the <code><a href="#topic+MISS">MISS</a></code>
function. Examples of within-model imputation may be found in the
accompanying &quot;Examples&quot; vignette.</p>
</td></tr>
<tr><td><code id="Matrices_+3A_y.theta">y.theta</code></td>
<td>
<p>This accepts a vector of data that are samples of a
discrete distribution, and defaults to <code>NULL</code>. If used, the
order of the samples affects the transition probability.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>as.indicator.matrix</code> function creates an indicator matrix
from a vector. This function is useful for converting a discrete
vector into a matrix in which each column represents one of the
discrete values, and each occurence of that value in the related
column is indicated by a one, and is otherwise filled with
zeroes. This function is similar to the <code>class.ind</code> function in
the nnet package.
</p>
<p>The <code>as.inverse</code> function returns the matrix inverse of
<code>x</code>. The <code>solve</code> function in base R also returns the matrix
inverse, but <code>solve</code> can return a matrix that is not symmetric,
and can fail due to singularities. The <code>as.inverse</code> function
tries to use the <code>solve</code> function to return a matrix inverse, and
when it fails due to a singularity, <code>as.inverse</code> uses eigenvalue
decomposition (in which eigenvalues below a tolerance are replaced
with the tolerance), and coerces the result to a symmetric
matrix. This is similar to the <code>solvcov</code> function in the fpc
package.
</p>
<p>The <code>as.parm.matrix</code> function prepares a correlation, covariance,
or precision matrix in two important ways. First,
<code>as.parm.matrix</code> obtains the parameters for the matrix specified
in the <code>x</code> argument by matching the name of the matrix in the
<code>x</code> argument with any parameters in <code>parm</code>, given the
parameter names in the <code>Data</code> listed in <code>parm.names</code>. These
obtained parameters are organized into a matrix as the elements of the
upper-triangular, including the diagonal. A copy is made, without the
diagonal, and the lower-triangular is filled in, completing the
matrix. Second, <code>as.parm.matrix</code> checks for
positive-definiteness. If matrix <code>x</code> is positive-definite, then
the matrix is stored as a variable called <code>LaplacesDemonMatrix</code>
in a new environment called <code>LDEnv</code>. If matrix <code>x</code> is not
positive-definite, then <code>LaplacesDemonMatrix</code> in <code>LDEnv</code> is
sought as a replacement. If this variable exists, then it is used to
replace the matrix. If not, then the matrix is replaced with an
identity matrix. Back in the model specification, after using
<code>as.parm.matrix</code>, it is recommended that the user also pass the
resulting matrix back into the <code>parm</code> vector, so the sampler or
algorithm knows that the elements of the matrix have changed.
</p>
<p>The <code>as.positive.definite</code> function returns the nearest
positive-definite matrix for a matrix that is square and symmetric
(Higham, 2002). This version is intended only for covariance and
precision matrices, and has been optimized for speed. A more
extensible function is <code>nearPD</code> in the matrixcalc package, which
is also able to work with correlation matrices, and matrices that are
asymmetric.
</p>
<p>The <code>as.positive.semidefinite</code> function iteratively seeks to
return a square, symmetric matrix that is at least
positive-semidefinite, by replacing each negative eigenvalue and
calculating its projection. This is intended only for covariance and
precision matrices. A similar function is <code>makePsd</code> in the RTAQ
package, though it is not iterative, and returns matrices that fail a
logical check with <code>is.positive.semidefinite</code>.
</p>
<p>The <code>as.symmetric.matrix</code> function accepts either a vector or
matrix, and returns a symmetric matrix. In the case of a vector, it
can be either all elements of the matrix, or the lower triangular. In
the case of a <code>x</code> being entered as a matrix, this function
tolerates non-finite values in one triangle (say, the lower), as long
as the corresponding element is finite in the other (say, the upper)
triangle.
</p>
<p>The <code>Cov2Cor</code> function converts a covariance matrix into a
correlation matrix, and accepts the covariance matrix either in matrix
or vector form. This function may be useful inside a model
specification and also with converting posterior draws of the elements
of a covariance matrix to a correlation matrix. <code>Cov2Cor</code> is an
expanded form of the <code>cov2cor</code> function in the <code>stats</code>
package, where <code>Cov2Cor</code> is also able to accept and return a
vectorized matrix.
</p>
<p>The <code>CovEstim</code> function estimates a covariance matrix with one of
several methods. This is mainly used by
<code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>, where the <code>parm</code> argument
receives the posterior modes. See the <code>CovEst</code> argument for
more details.
</p>
<p>The <code>GaussHermiteCubeRule</code> function returns a matrix of nodes and
a vector of weights for a <code>dims</code>-dimensional integral given
<code class="reqn">N</code> univariate nodes. The number of multivariate nodes will differ
from the number of univariate nodes. This function is for use with
multivariate quadrature, often called cubature. This has been adapted
from the <code>multiquad</code> function in the NominalLogisticBiplot
package. The <code><a href="#topic+GaussHermiteQuadRule">GaussHermiteQuadRule</a></code> function is a
univariate version. A customized univariate <code>rule</code> may be
supplied when constraints necessitate that one or more nodes and
weights had to be altered.
</p>
<p>The <code>Hessian</code> returns a symmetric, Hessian matrix, which is a
matrix of second partial derivatives. The estimation of the Hessian
matrix is approximated numerically using Richardson extrapolation by
default. This is a slow function. This function is not intended to be
called by the user, but is made available here. This is essentially
the <code>hessian</code> function from the numDeriv package, adapted to
Laplace's Demon.
</p>
<p>The <code>is.positive.definite</code> function is a logical test of whether
or not a matrix is positive-definite. A <code class="reqn">k \times k</code>
symmetric matrix <code class="reqn">\textbf{X}</code> is positive-definite if all of
its eigenvalues are positive (<code class="reqn">\lambda_i &gt; 0, i \in k</code>). All main-diagonal elements must be positive. The
determinant of a positive-definite matrix is always positive, so a
positive-definite matrix is always nonsingular. Non-symmetric,
positive-definite matrices exist, but are not considered here.
</p>
<p>The <code>is.positive.semidefinite</code> function is a logical test of
whether or not a matrix is positive-semidefinite. A <code class="reqn">k x k</code>
symmetric matrix <code class="reqn">\textbf{X}</code> is positive-semidefinite if all
of its eigenvalues are non-negative (<code class="reqn">\lambda_i \ge 0, i \in
  k</code>).
</p>
<p>The <code>is.square.matrix</code> function is a logical test of whether or
not a matrix is square. A square matrix is a matrix with the same
number of rows and columns, and is usually represented as a <code class="reqn">k
  \times k</code> matrix <code class="reqn">\textbf{X}</code>.
</p>
<p>The <code>is.symmetric.matrix</code> function is a logical test of whether
or not a matrix is symmetric. A symmetric matrix is a square matrix
that is equal to its transpose, <code class="reqn">\textbf{X} = \textbf{X}^T</code>. For example, where <code class="reqn">i</code> indexes rows and <code class="reqn">j</code> indexes
columns, <code class="reqn">\textbf{X}_{i,j} = \textbf{X}_{j,i}</code>. This differs from the <code>isSymmetric</code> function in base R
that is inexact, using <code>all.equal</code>.
</p>
<p>The <code>Jacobian</code> function estimates the Jacobian matrix, which is
a matrix of all first-order partial derivatives of the <code>Model</code>.
The Jacobian matrix is estimated by default with forward
finite-differencing, or optionally with Richardson extrapolation. This
function is not intended to be called by the user, but is made
available here. This is essentially the <code>jacobian</code> function from
the numDeriv package, adapted to LaplacesDemon.
</p>
<p>The <code>logdet</code> function returns the logarithm of the determinant of
a positive-definite matrix via the Cholesky decomposition. The
determinant is a value associated with a square matrix, and was used
historically to <em>determine</em> if a system of linear equations has a
unique solution. The term <em>determinant</em> was introduced by Gauss,
where Laplace referred to it as the resultant. When the determinant is
zero, the matrix is singular and non-invertible; there are either no
solutions or many solutions. A unique solution exists when the
determinant is non-zero. The <code>det</code> function in base R works well
for small matrices, but can return erroneously return zero in larger
matrices. It is better to work with the log-determinant.
</p>
<p>The <code>lower.triangle</code> function returns a vector of the lower
triangular elements of a matrix, and the diagonal is included when
<code>diag=TRUE</code>.
</p>
<p>The <code>read.matrix</code> function is provided here as one of many
convenient ways to read a numeric matrix into R. The most common
method of storing data in R is the data frame, because it is
versatile. For example, a data frame may contain character, factor,
and numeric variables together. For iterative estimation, common in
Bayesian inference, the data frame is much slower than the numeric
matrix. For this reason, the LaplacesDemon package does not use data
frames, and has not traditionally accepted character or factor
data. The <code>read.matrix</code> function returns either an entire numeric
matrix, or row-wise samples from a numeric matrix. Samples may be
taken from a matrix that is too large for available computer memory
(RAM), such as with big data.
</p>
<p>The <code>SparseGrid</code> function returns a sparse grid for a
<code class="reqn">J</code>-dimensional integral with accuracy <code class="reqn">K</code>, given
Gauss-Hermite quadrature rules. A grid of order eqnK provides an
exact result for a polynomial of total order of <code class="reqn">2K - 1</code> or less.
<code>SparseGrid</code> returns a matrix of nodes and a vector of weights.
A sparse grid is more efficient than the full grid in the
<code>GaussHermiteCubeRule</code> function. This has been adapted from the
SparseGrid package.
</p>
<p>The <code>TransitionMatrix</code> function has several uses. A user may
supply a vector of marginal posterior samples of a discrete Markov
chain as <code>theta.y</code>, and an observed posterior transition matrix
is returned. Otherwise, a user may supply data (<code>y.theta</code>) and/or
a prior (<code>p.theta</code>), in which case a posterior transition matrix
is returned. A common row-wise prior is the dirichlet distribution.
Transition probabilities are from row element to column element.
</p>
<p>The <code>tr</code> function returns the trace of a matrix. The trace of a
matrix is the sum of the elements in the main diagonal of a square
matrix. For example, the trace of a <code class="reqn">k \times k</code> matrix
<code class="reqn">\textbf{X}</code>, is <code class="reqn">\sum_{k=1} \textbf{X}_{k,k}</code>.
</p>
<p>The <code>upper.triangle</code> function returns a vector of the lower
triangular elements of a matrix, and the diagonal is included when
<code>diag=TRUE</code>.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>References</h3>

<p>Higham, N.J. (2002). &quot;Computing the Nearest Correlation Matrix - a
Problem from Finance&quot;. <em>IMA Journal of Numerical Analysis</em>, 22,
p. 329&ndash;343.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+BayesianBootstrap">BayesianBootstrap</a></code>,
<code><a href="#topic+Cov2Prec">Cov2Prec</a></code>,
<code><a href="stats.html#topic+cov2cor">cov2cor</a></code>,
<code><a href="#topic+ddirichlet">ddirichlet</a></code>,
<code><a href="#topic+GaussHermiteQuadRule">GaussHermiteQuadRule</a></code>,
<code><a href="Matrix.html#topic+isSymmetric">isSymmetric</a></code>,
<code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>,
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>,
<code><a href="base.html#topic+lower.tri">lower.tri</a></code>,
<code><a href="#topic+MISS">MISS</a></code>,
<code><a href="#topic+Prec2Cov">Prec2Cov</a></code>,
<code><a href="Matrix.html#topic+solve">solve</a></code>, and
<code><a href="base.html#topic+upper.tri">upper.tri</a></code>.
</p>

<hr>
<h2 id='MCSE'>Monte Carlo Standard Error</h2><span id='topic+MCSE'></span><span id='topic+MCSS'></span>

<h3>Description</h3>

<p>Monte Carlo Standard Error (MCSE) is an estimate of the inaccuracy of
Monte Carlo samples, usually regarding the expectation of posterior
samples, <code class="reqn">\mathrm{E}(\theta)</code>, from Monte Carlo or
Markov chain Monte Carlo (MCMC) algorithms, such as with the
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code> or <code><a href="#topic+LaplacesDemon.hpc">LaplacesDemon.hpc</a></code>
functions. MCSE approaches zero as the number of independent posterior
samples approaches infinity. MCSE is essentially a standard deviation
around the posterior mean of the samples,
<code class="reqn">\mathrm{E}(\theta)</code>, due to uncertainty associated with
using an MCMC algorithm, or Monte Carlo methods in general.
</p>
<p>The acceptable size of the MCSE depends on the acceptable uncertainty
associated around the marginal posterior mean,
<code class="reqn">\mathrm{E}(\theta)</code>, and the goal of inference. It has
been argued that MCSE is generally unimportant when the goal of
inference is <code class="reqn">\theta</code> rather than
<code class="reqn">\mathrm{E}(\theta)</code> (Gelman et al., 2004, p. 277), and
that a sufficient <code><a href="#topic+ESS">ESS</a></code> is more important. Others perceive
MCSE to be a vital part of reporting any Bayesian model, and as a
stopping rule (Flegal et al., 2008).
</p>
<p>In <code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>, MCSE is part of the posterior
summaries because it is easy to estimate, and Laplace's Demon prefers
to continue updating until each MCSE is less than 6.27% of its
associated marginal posterior standard deviation (for more information
on this stopping rule, see the <code><a href="#topic+Consort">Consort</a></code> function), since
MCSE has been demonstrated to be an excellent stopping rule.
</p>
<p>Acceptable error may be specified, if known, in the <code>MCSS</code>
(Monte Carlo Sample Size) function to estimate the required number of
posterior samples.
</p>
<p><code>MCSE</code> is a univariate function that is often applied to each
marginal posterior distribution. A multivariate form is not
included. By chance alone due to multiple independent tests, 5% of
the parameters should indicate unacceptable MSCEs, even when
acceptable. Assessing convergence is difficult.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MCSE(x, method="IMPS", batch.size="sqrt", warn=FALSE)
MCSS(x, a)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MCSE_+3A_x">x</code></td>
<td>
<p>This is a vector of posterior samples for which MCSE or MCSS
will be estimated.</p>
</td></tr>
<tr><td><code id="MCSE_+3A_a">a</code></td>
<td>
<p>This is a scalar argument of acceptable error for the mean of
<code>x</code>, and <code>a</code> must be positive. As acceptable error
decreases, the required number of samples increases.</p>
</td></tr>
<tr><td><code id="MCSE_+3A_method">method</code></td>
<td>
<p>This is an optional argument for the method of MCSE
estimation, and defaults to Geyer's <code>"IMPS"</code> method. Optional
methods include <code>"sample.variance"</code> and <code>"batch.mean"</code>.
Note that <code>"batch.mean"</code> is recommended only when the number of
posterior samples is at least 1,000.</p>
</td></tr>
<tr><td><code id="MCSE_+3A_batch.size">batch.size</code></td>
<td>
<p>This is an optional argument that corresponds only
with <code>method="batch.means"</code>, and determines either the size of
the batches (accepting a numerical argument) or the method of
creating the size of batches, which is either <code>"sqrt"</code> or
<code>"cuberoot"</code>, and refers to the length of <code>x</code>. The default
argument is <code>"sqrt"</code>.</p>
</td></tr>
<tr><td><code id="MCSE_+3A_warn">warn</code></td>
<td>
<p>Logical. If <code>warn=TRUE</code>, then a warning is provided
with <code>method="batch.means"</code> whenever posterior sample size is
less than 1,000, or a warning is produced when more autcovariance
is recommended with <code>method="IMPS"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The default method for estimating MCSE is Geyer's Initial Monotone
Positive Sequence (IMPS) estimator (Geyer, 1992), which takes the
asymptotic variance into account and is time-series based. This method
goes by other names, such as Initial Positive Sequence (IPS).
</p>
<p>The simplest method for estimating MCSE is to modify the formula for
standard error, <code class="reqn">\sigma(\textbf{x}) / \sqrt{N}</code>, to account for non-independence in the sequence
<code class="reqn">\textbf{x}</code> of posterior samples. Non-independence is
estimated with the <code>ESS</code> function for Effective Sample Size (see
the <code><a href="#topic+ESS">ESS</a></code> function for more details), where <code class="reqn">M =
  ESS(\textbf{x})</code>, and MCSE is
<code class="reqn">\sigma(\textbf{x}) / \sqrt{M}</code>. Although this
is the fastest and easiest method of estimation, it does not
incorporate an estimate of the asymptotic variance of
<code class="reqn">\textbf{x}</code>.
</p>
<p>The batch means method (Jones et al., 2006; Flegal et al., 2008)
separates elements of <code class="reqn">\textbf{x}</code> into batches and estimates
MCSE as a function of multiple batches. This method is excellent, but
is not recommended when the number of posterior samples is less than
1,000. These journal articles also assert that MCSE is a better
stopping rule than MCMC convergence diagnostics.
</p>
<p>The <code>MCSS</code> function estimates the required number of posterior
samples, given the user-specified acceptable error, posterior samples
<code>x</code>, and the observed variance (rather than asymptotic
variance). Due to the observed variance, this is a rough estimate.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>References</h3>

<p>Flegal, J.M., Haran, M., and Jones, G.L. (2008). &quot;Markov chain Monte
Carlo: Can We Trust the Third Significant Figure?&quot;. <em>Statistical
Science</em>, 23, p. 250&ndash;260.
</p>
<p>Gelman, A., Carlin, J., Stern, H., and Rubin, D. (2004). &quot;Bayesian
Data Analysis, Texts in Statistical Science, 2nd ed.&quot;. Chapman and
Hall, London.
</p>
<p>Geyer, C.J. (1992). &quot;Practical Markov Chain Monte Carlo&quot;.
<em>Statistical Science</em>, 7, 4, p. 473&ndash;483.
</p>
<p>Jones, G.L., Haran, M., Caffo, B.S., and Neath, R. (2006). &quot;Fixed-Width
Output Analysis for Markov chain Monte Carlo&quot;. <em>Journal of the
American Statistical Association</em>, 101(1), p. 1537&ndash;1547.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Consort">Consort</a></code>,
<code><a href="#topic+ESS">ESS</a></code>,
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>, and
<code><a href="#topic+LaplacesDemon.hpc">LaplacesDemon.hpc</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
x &lt;- rnorm(1000)
MCSE(x)
MCSE(x, method="batch.means")
MCSE(x, method="sample.variance")
MCSS(x, a=0.01)
</code></pre>

<hr>
<h2 id='MinnesotaPrior'>Minnesota Prior</h2><span id='topic+MinnesotaPrior'></span>

<h3>Description</h3>

<p>The Minnesota prior, also called the Litterman prior, is a shrinkage
prior for autoregressive parameters in vector autoregressive (VAR)
models. There are many variations of the Minnesota prior. This
Minnesota prior is calculated as presented in Lutkepohl (2005,
p. 225), and returns one or more prior covariance matrices in an
array.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MinnesotaPrior(J, lags=c(1,2), lambda=1, theta=0.5, sigma)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MinnesotaPrior_+3A_j">J</code></td>
<td>
<p>This is the scalar number of time-series in the VAR.</p>
</td></tr>
<tr><td><code id="MinnesotaPrior_+3A_lags">lags</code></td>
<td>
<p>This accepts an integer vector of lags of the
autoregressive parameters. The lags are not required to be
successive.</p>
</td></tr>
<tr><td><code id="MinnesotaPrior_+3A_lambda">lambda</code></td>
<td>
<p>This accepts a scalar, positive-only hyperparameter that
controls how tightly the parameter of the first lag is concentrated
around zero. A smaller value results in smaller diagonal variance.
When equal to zero, the posterior equals the prior and data is not
influential. When equal to infinity, no shrinkage occurs and
posterior expectations are closest to estimates from ordinary least
squares (OLS). It has been asserted that as the number, <code class="reqn">J</code>, of
time-series increases, this hyperparameter should decrease.</p>
</td></tr>
<tr><td><code id="MinnesotaPrior_+3A_theta">theta</code></td>
<td>
<p>This accepts a scalar hyperparameter in the interval
[0,1]. When one, off-diagonal elements have variance similar or
equal to diagonal elements. When zero, off-diagonal elements have
zero variance. A smaller value is associated with less off-diagonal
variance.</p>
</td></tr>
<tr><td><code id="MinnesotaPrior_+3A_sigma">sigma</code></td>
<td>
<p>This accepts a vector of length <code class="reqn">J</code> of residual
standard deviations of the dependent variables given the
expectations.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Minnesota prior was introduced in Doan, Litterman, and Sims
(1984) as a shrinkage prior for autoregressive parameters in vector
autoregressive (VAR) models. The Minnesota prior was reviewed in
Litterman (1986), and numerous variations have been presented since.
This is the version of the Minnesota prior as described in Lutkepohl
(2005, p. 225) for stationary time-series.
</p>
<p>Given one or more <code class="reqn">J \times J</code> matrices of autoregressive
parameters in a VAR model, the user specifies two tuning
hyperparameters for the Minnesota prior: <code>lambda</code> and
<code>theta</code>. Each iteration of the numerical approximation algorithm,
the latest vector of residual standard deviation parameters is
supplied to the <code>MinnesotaPrior</code> function, which then returns an
array that contains one or more prior covariance matrices for the
autoregressive parameters. Multiple prior covariance matrices are
returned when multiple lags are specified. The tuning hyperparameters,
<code>lambda</code> and <code>theta</code>, can be estimated from the data via
hierarchical Bayes.
</p>
<p>It is important to note that the Minnesota prior does not technically
return a covariance matrix, because the matrix is not symmetric, and
therefore not positive-definite. For this reason, a Minnesota prior
covariance matrix should not be supplied as a covariance matrix to a
multivariate normal distribution, such as with the <code><a href="#topic+dmvn">dmvn</a></code>
function, though it would be accepted and then (incorrectly)
converted to a symmetric matrix. Instead, <code><a href="#topic+dnormv">dnormv</a></code> should
be used for element-wise evaluation.
</p>
<p>While the Minnesota prior is used to specify the prior covariance for
VAR autoregressive parameters, prior means are often all set to zero,
or sometimes the first lag is set to an identity matrix.
</p>
<p>An example is provided in the Examples vignette.
</p>


<h3>Value</h3>

<p>This function returns a <code class="reqn">J \times J \times L</code> array
for <code class="reqn">J</code> time-series and <code class="reqn">L</code> lags.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>References</h3>

<p>Doan, T., Litterman, R.B. and Sims, C.A. (1984). &quot;Forecasting and
Conditional Projection using Realistic Prior Distributions&quot;.
<em>Econometric Reviews</em>, 3, p. 1&ndash;144.
</p>
<p>Litterman, R.B. (1986). &quot;Forecasting with Bayesian Vector
Autoregressions - Five Years of Experience&quot;. <em>Journal of
Business &amp; Economic Statistics</em>, 4, p. 25&ndash;38.
</p>
<p>Lutkepohl, H. (2005). &quot;New Introduction to Multiple Time Series
Analysis&quot;. Springer, Germany.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dmvn">dmvn</a></code>,
<code><a href="#topic+dnormv">dnormv</a></code>, and
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>.
</p>

<hr>
<h2 id='MISS'>Multiple Imputation Sequential Sampling</h2><span id='topic+MISS'></span>

<h3>Description</h3>

<p>This function performs multiple imputation (MI) on a numeric matrix
by sequentially sampling variables with missing values, given all
other variables in the data set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MISS(X, Iterations=100, Algorithm="GS", Fit=NULL, verbose=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MISS_+3A_x">X</code></td>
<td>
<p>This required argument accepts a numeric matrix of data that
contains both observed and missing values. Data set
<code class="reqn">\textbf{X}</code> must not have any rows or columns that are
completely missing. <code class="reqn">\textbf{X}</code> must not have any
constants. The user must apply any data transformations appropriate
for these models. Missing values are assumed to be Missing At Random
(MAR).</p>
</td></tr>
<tr><td><code id="MISS_+3A_iterations">Iterations</code></td>
<td>
<p>This is the number of iterations to perform
sequential sampling via MCMC algorithms.</p>
</td></tr>
<tr><td><code id="MISS_+3A_algorithm">Algorithm</code></td>
<td>
<p>The MCMC algorithm defaults to the Gibbs Sampler (GS).</p>
</td></tr>
<tr><td><code id="MISS_+3A_fit">Fit</code></td>
<td>
<p>This optional argument accepts an object of class
<code>miss</code>. When supplied, <code>MISS</code> will continue where it left
off, provided the user does not change the algorithm (different
methods are used with different algortihms, so model parameters will
not match). In short, changing algorithms requires starting from
scratch.</p>
</td></tr>
<tr><td><code id="MISS_+3A_verbose">verbose</code></td>
<td>
<p>Logical. When <code>FALSE</code>, only the iteration prints
to the console. When <code>TRUE</code>, which is the default, both the
iteration and which variable is being imputed are printed to the
console.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Imputation is a family of statistical methods for replacing missing
values with estimates. Introduced by Rubin and Schenker (1986) and
Rubin (1987), Multiple Imputation (MI) is a family of imputation
methods that includes multiple estimates, and therefore includes
variability of the estimates.
</p>
<p>The Multiple Imputation Sequential Sampler (MISS) function performs
MI by determining the type of variable and therefore the sampler for
each variable, and then sequentially progresses through each variable
in the data set that has missing values, updating its prediction of
those missing values given all other variables in the data set each
iteration.
</p>
<p>MI is best performed within a model, where it is called
full-likelihood imputation. Examples may be found in the &quot;Examples&quot;
vignette. However, sometimes it is impractical to impute within a
model when there are numerous missing values and a large number of
parameters are therefore added. As an alternative, MI may be
performed on the data set before the data is passed to the model,
such as in the <code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code>,
<code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>, <code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>, or
<code><a href="#topic+VariationalBayes">VariationalBayes</a></code> function. This is less desirable, but
MISS is available for MCMC-based MI in this case.
</p>
<p>Missing values are initially set to column means for continuous
variables, and are set to one for discrete variables.
</p>
<p>MISS uses the following methods and MCMC algorithms:
</p>
<p>Missing values of continuous variables are estimated with a
ridge-stabilized linear regression Gibbs sampler.
</p>
<p>Missing values of binary variables that have only 0 or 1 for values
are estimated either with a binary robit (t-link logistic
regression model) Gibbs sampler of Albert and Chib (1993).
</p>
<p>Missing values of discrete variables with 3 or more (ordered or
unordered) discrete values are considered continuous.
</p>
<p>In the presence of big data, it is suggested that the user
sequentially read in batches of data that are small enough to be
manageable, and then apply the MISS function to each batch. Each batch
should be representative of the whole, of course.
</p>
<p>It is common for multiple imputation functions to handle variable
transformations. MISS does not transform variables, but imputes what
it gets. For example, if a user has a variable that should be positive
only, then it is recommended here that the user log-transform the
variable, pass the data set to MISS, and when finished, exponentiate
both the observed and imputed values of that variable.
</p>
<p>The <code>CenterScale</code> function should also be considered to speed up
convergence.
</p>
<p>It is hoped that MISS is helpful, though it is not without limitation
and there are numerous alternatives outside of the
<code>LaplacesDemon</code> package. If MISS does not fulfill the needs of
the user, then the following packages are recommended: Amelia, mi, or
mice. MISS emphasizes MCMC more than these alternatives, though MISS is
not as extensive. When a data set does not have a simple structure,
such as merely continuous or binary or unordered discrete, the
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code> function should be considered, where a
user can easily specify complicated structures such as multilevel,
spatial or temporal dependence, and more.
</p>
<p>Matrix inversions are required in the Gibbs sampler. Matrix inversions
become more cumbersome as the number <code class="reqn">J</code> of variables increases.
</p>
<p>If a large number of iterations is used, then the user may consider
studying the imputations for approximate convergence with the
<code><a href="#topic+BMK.Diagnostic">BMK.Diagnostic</a></code> function, by supplying the transpose of
codeFit$Imp. In the presence of numerous missing values, say more
than 100, the user may consider iterating through the study of the
imputations of 100 missing values at a time.
</p>


<h3>Value</h3>

<p>This function returns an object of class <code>miss</code> that is a list
with five components:
</p>
<table>
<tr><td><code>Algorithm</code></td>
<td>
<p>This indicates which algorithm was selected.</p>
</td></tr>
<tr><td><code>Imp</code></td>
<td>
<p>This is a <code class="reqn">M \times T</code> matrix of <code class="reqn">M</code> missing
values and <code class="reqn">T</code> iterations that contains imputations.</p>
</td></tr>
<tr><td><code>parm</code></td>
<td>
<p>This is a list of length <code class="reqn">J</code> for <code class="reqn">J</code> variables,
and each component of the list contains parameters associated with
the prediction of missing values for that variable.</p>
</td></tr>
<tr><td><code>PostMode</code></td>
<td>
<p>This is a vector of posterior modes. If the user
intends to replace missing values in a data set with only one
estimate per missing values (single, not multiple imputation), then
this vector contains these values.</p>
</td></tr>
<tr><td><code>Type</code></td>
<td>
<p>This is a vector of length <code class="reqn">J</code> for <code class="reqn">J</code> variables
that indicates the type of each variable, as MISS will consider it.
When <code>Type=1</code>, the variable is considered to be continuous.
When <code>Type=2</code>, only two discrete values (0 and 1) were found.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Statisticat, LLC <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>References</h3>

<p>Albert, J.H. and Chib, S. (1993). &quot;Bayesian Analysis of Binary and
Polychotomous Response Data&quot;. <em>Journal of the American
Statistical Association</em>, 88(422), p. 669&ndash;679.
</p>
<p>Rubin, D.B. (1987). &quot;Multiple Imputation for Nonresponse in
Surveys&quot;. John Wiley and Sons: New York, NY.
</p>
<p>Rubin, D.B. and Schenker, N. (1986). &quot;Multiple Imputation for Interval
Estimation from Simple Random Samples with Ignorable
Nonresponse&quot;. <em>Journal of the American Statistical Association</em>,
81, p. 366&ndash;374.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ABB">ABB</a></code>,
<code><a href="#topic+BMK.Diagnostic">BMK.Diagnostic</a></code>,
<code><a href="#topic+CenterScale">CenterScale</a></code>,
<code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code>
<code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>,
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>, and
<code><a href="#topic+VariationalBayes">VariationalBayes</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#library(LaplacesDemon)
### Create Data
#N &lt;- 20 #Number of Simulated Records
#J &lt;- 5 #Number of Simulated Variables
#pM &lt;- 0.25 #Percent Missing
#Sigma &lt;- as.positive.definite(matrix(runif(J*J),J,J))
#X &lt;- rmvn(N, rep(0,J), Sigma)
#m &lt;- sample.int(N*J, round(pM*N*J))
#X[m] &lt;- NA
#head(X)

### Begin Multiple Imputation
#Fit &lt;- MISS(X, Iterations=100, Algorithm="GS", verbose=TRUE)
#Fit
#summary(Fit)
#plot(Fit)
#plot(BMK.Diagnostic(t(Fit$Imp)))

### Continue Updating if Necessary
#Fit &lt;- MISS(X, Iterations=100, Algorithm="GS", Fit, verbose=TRUE)
#summary(Fit)
#plot(Fit)
#plot(BMK.Diagnostic(t(Fit$Imp)))

### Replace Missing Values in Data Set with Posterior Modes
#Ximp &lt;- X
#Ximp[which(is.na(X))] &lt;- Fit$PostMode

### Original and Imputed Data Sets
#head(X)
#head(Ximp)
#summary(X)
#summary(Ximp)

### or Multiple Data Sets, say 3
#Ximp &lt;- array(X, dim=c(nrow(X), ncol(X), 3))
#for (i in 1:3) {
#     Xi &lt;- X
#     Xi[which(is.na(X))] &lt;- Fit$Imp[,sample.int(ncol(Fit$Imp), 1)]
#     Ximp[,,i] &lt;- Xi}
#head(X)
#head(Ximp[,,1])
#head(Ximp[,,2])
#head(Ximp[,,3])

#End
</code></pre>

<hr>
<h2 id='Mode'>The Mode(s) of a Vector</h2><span id='topic+is.amodal'></span><span id='topic+is.bimodal'></span><span id='topic+is.multimodal'></span><span id='topic+is.trimodal'></span><span id='topic+is.unimodal'></span><span id='topic+Mode'></span><span id='topic+Modes'></span>

<h3>Description</h3>

<p>The mode is a measure of central tendency. It is the value that occurs
most frequently, or in a continuous probability distribution, it is
the value with the most density. A distribution may have no modes
(such as with a constant, or in a uniform distribution when no value
occurs more frequently than any other), or one or more modes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.amodal(x, min.size=0.1)
is.bimodal(x, min.size=0.1)
is.multimodal(x, min.size=0.1)
is.trimodal(x, min.size=0.1)
is.unimodal(x, min.size=0.1)
Mode(x)
Modes(x, min.size=0.1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Mode_+3A_x">x</code></td>
<td>
<p>This is a vector in which a mode (or modes) will be sought.</p>
</td></tr>
<tr><td><code id="Mode_+3A_min.size">min.size</code></td>
<td>
<p>This is the minimum size that can be considered a
mode, where size means the proportion of the distribution between
areas of increasing kernel density estimates.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>is.amodal</code> function is a logical test of whether or not
<code>x</code> has a mode. If <code>x</code> has a mode, then <code>TRUE</code> is
returned, otherwise <code>FALSE</code>.
</p>
<p>The <code>is.bimodal</code> function is a logical test of whether or not
<code>x</code> has two modes. If <code>x</code> has two modes, then <code>TRUE</code>
is returned, otherwise <code>FALSE</code>.
</p>
<p>The <code>is.multimodal</code> function is a logical test of whether or not
<code>x</code> has multiple modes. If <code>x</code> has multiple modes, then
<code>TRUE</code> is returned, otherwise <code>FALSE</code>.
</p>
<p>The <code>is.trimodal</code> function is a logical test of whether or not
<code>x</code> has three modes. If <code>x</code> has three modes, then <code>TRUE</code>
is returned, otherwise <code>FALSE</code>.
</p>
<p>The <code>is.unimodal</code> function is a logical test of whether or not
<code>x</code> has one mode. If <code>x</code> has one mode, then <code>TRUE</code>
is returned, otherwise <code>FALSE</code>.
</p>
<p>The <code>Mode</code> function returns the most frequent value when <code>x</code>
is discrete. If <code>x</code> is a constant, then it is considered amodal,
and <code>NA</code> is returned. If multiple modes exist, this function
returns only the mode with the highest density, or if two or more
modes have the same density, then it returns the first mode found.
Otherwise, the <code>Mode</code> function returns the value of <code>x</code>
associated with the highest kernel density estimate, or the first
one found if multiple modes have the same density.
</p>
<p>The <code>Modes</code> function is a simple, deterministic function that
differences the kernel density of <code>x</code> and reports a number of
modes equal to half the number of changes in direction, although the
<code>min.size</code> function can be used to reduce the number of modes
returned, and defaults to 0.1, eliminating modes that do not have at
least 10% of the distributional area. The <code>Modes</code> function
returns a list with three components: <code>modes</code>, <code>modes.dens</code>,
and <code>size</code>. The elements in each component are ordered according
to the decreasing density of the modes. The <code>modes</code> component is
a vector of the values of <code>x</code> associated with the modes. The
<code>modes.dens</code> component is a vector of the kernel density
estimates at the modes. The <code>size</code> component is a vector of the
proportion of area underneath each mode.
</p>
<p>The <code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code>,
<code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>, and <code><a href="#topic+VariationalBayes">VariationalBayes</a></code>
functions characterize the marginal posterior distributions by
posterior modes (means) and variance. A related topic is MAP or
maximum <em>a posteriori</em> estimation.
</p>
<p>Otherwise, the results of Bayesian inference tend to report the
posterior mean or median, along with probability intervals (see
<code><a href="#topic+p.interval">p.interval</a></code> and <code><a href="#topic+LPL.interval">LPL.interval</a></code>), rather than
posterior modes. In many types of models, such as mixture models, the
posterior may be multimodal. In such a case, the usual recommendation
is to choose the highest mode if feasible and possible. However, the
highest mode may be uncharacteristic of the majority of the posterior.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code>,
<code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>,
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>,
<code><a href="#topic+LPL.interval">LPL.interval</a></code>,
<code><a href="#topic+p.interval">p.interval</a></code>, and
<code><a href="#topic+VariationalBayes">VariationalBayes</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
### Below are distributions with different numbers of modes.
x &lt;- c(1,1) #Amodal
x &lt;- c(1,2,2,2,3) #Unimodal
x &lt;- c(1,2) #Bimodal
x &lt;- c(1,3,3,3,3,4,4,4,4,4) #min.size affects the answer
x &lt;- c(1,1,3,3,3,3,4,4,4,4,4) #Trimodal

### And for each of the above, the functions below may be applied.
Mode(x)
Modes(x)
is.amodal(x)
is.bimodal(x)
is.multimodal(x)
is.trimodal(x)
is.unimodal(x)
</code></pre>

<hr>
<h2 id='Model.Specification.Time'>Model Specification Time</h2><span id='topic+Model.Spec.Time'></span>

<h3>Description</h3>

<p>The <code>Model.Spec.Time</code> function returns the time in minutes to
evaluate a model specification a number of times, as well as
the evaluations per minute, and componentwise iterations per minute.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Model.Spec.Time(Model, Initial.Values, Data, n=1000)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Model.Specification.Time_+3A_model">Model</code></td>
<td>
<p>This requried argument is a model specification
function. For more information, see <code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>.</p>
</td></tr>
<tr><td><code id="Model.Specification.Time_+3A_initial.values">Initial.Values</code></td>
<td>
<p>This required argument is a vector of initial
values for the parameters.</p>
</td></tr>
<tr><td><code id="Model.Specification.Time_+3A_data">Data</code></td>
<td>
<p>This required argument is a list of data. For more
information, see <code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>.</p>
</td></tr>
<tr><td><code id="Model.Specification.Time_+3A_n">n</code></td>
<td>
<p>This is the number of evaluations of the model specification,
and accuracy increases with <code>n</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The largest single factor to affect the run-time of an algorithm &ndash;
whether it is with <code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code>,
<code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>, <code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>,
<code><a href="#topic+PMC">PMC</a></code>, or <code><a href="#topic+VariationalBayes">VariationalBayes</a></code> &ndash; is the time
that it takes to evaluate the model specification. This has also been
observed in Rosenthal (2007). It is highly recommended that a user of
the <code>LaplacesDemon</code> package should attempt to reduce the run-time
of the model specification, usually by testing alternate forms of code
for speed. This is especially true with big data, such as with the
<code><a href="#topic+BigData">BigData</a></code> function.
</p>
<p>Every function in the LaplacesDemon package is byte-compiled, which is
a recent option in R. This reduces run-time, thanks to Tierney's
compiler package in base R. The model specification, however, is
specified by the user, and should be byte-compiled. The reduction in
run-time may range from mild to dramatic, depending on the model. It
is highly recommended that users concerned with run-time activate the
compiler package and use the <code>cmpfun</code> function, as per the
example below.
</p>
<p>A model specification function that is optimized for speed and
involves many records may result in a model update run-time that is
considerably less than other popular MCMC-based software algorithms
that loop through records, even when those algorithms are coded in
<code>C</code> or other fast languages. For a comparison, see the
&ldquo;Laplace's Demon Tutorial&rdquo; vignette.
</p>
<p>However, if a model specification function in the LaplacesDemon
package is not fully vectorized (contains <code>for</code> loops and
<code>apply</code> functions), then run-time will typically be slower than
other popular MCMC-based software algorithms.
</p>
<p>The speed of calculating the model specification function is
affected by parameter constraints, such as with the
<code><a href="#topic+interval">interval</a></code> function. Parameter constraints may add
considerable variability to the speed of this calculation, and usually
more variation occurs with initial values that are far from the target
distributions.
</p>
<p>Distributions in the <code>LaplacesDemon</code> package usually have logical
checks to ensure correctness. These checks may slow the 
calculation of the density, for example. If the user is confident
these checks are unnecessary for their model, then the user may
copy the code to a new function name and comment-out the checks to
improve speed.
</p>
<p>When speed is of paramount importance, a user is encouraged to
experiment with writing the model specification function in another
language such as in <code>C++</code> with the <code>Rcpp</code> package, and
calling drop-in replacement functions from within the <code>Model</code>
function, or re-writing the model function entirely in <code>C++</code>.
For an introduction to including <code>C++</code> in <span class="pkg">LaplacesDemon</span>,
see <a href="https://web.archive.org/web/20150227225556/http://www.bayesian-inference.com/softwarearticlescppsugar">https://web.archive.org/web/20150227225556/http://www.bayesian-inference.com/softwarearticlescppsugar</a>.
</p>
<p>When a model specification function is computationally expensive,
another approach to reduce run-time may be for the user to write
parallelized code within the model, splitting up difficult tasks and
assigning each to a separate CPU.
</p>
<p>Another use for <code>Model.Spec.Time</code> is to allow the user to make an
informed decision about which MCMC algorithm to select, given the
speed of their model specification. For example, the Adaptive
Metropolis-within-Gibbs (AMWG) of Roberts and Rosenthal (2009) is
currently the adaptive MCMC algorithm of choice in general in many
cases, but this choice is conditional on run-time. While other
MCMC algorithms in <code>LaplacesDemon</code> evaluate the model
specification function once per iteration, componentwise algorithms
such as in the MWG family evaluate it once per parameter per
iteration, significantly increasing run-time per iteration in large
models. The <code>Model.Spec.Time</code> function may forewarn the user of
the associated run-time, and if it should be decided to go with an
alternate MCMC algorithm, such as AMM, in which each element of its
covariance matrix must stabilize for the chains to become
stationary. AMM, for example, will require many more iterations of 
burn-in (for more information, see the <code><a href="#topic+burnin">burnin</a></code> function),
but with numerous iterations, allows more thinning. A general
recommendation may be to use AMWG when
<code>Componentwise.Iters.per.Minute</code> &gt;= 1000, but this is subjective
and best determined by each user for each model. A better decision may
be made by comparing MCMC algorithms with the <code><a href="#topic+Juxtapose">Juxtapose</a></code>
function for a particular model.
</p>
<p>Following are a few common suggestions for increasing the speed of
<code>R</code> code in the model specification function. There are often
exceptions to these suggestions, so case-by-case experimentation is
also suggested.
</p>

<ul>
<li><p> Replace exponents with multiplied terms, such as <code>x^2</code>
with <code>x*x</code>.
</p>
</li>
<li><p> Replace <code>mean(x)</code> with <code>sum(x)/length(x)</code>.
</p>
</li>
<li><p> Replace parentheses (when possible) with curly brackets, such
as <code>x*(a+b)</code> with <code>x*{a+b}</code>.
</p>
</li>
<li><p> Replace <code>tcrossprod(Data$X, t(beta))</code> with
<code>Data$X %*% beta</code> when there are few predictors, and avoid
<code>tcrossprod(beta, Data$X)</code>, which is always slowest.
</p>
</li>
<li><p> Vectorize functions and eliminate <code>apply</code> and <code>for</code>
functions. There are often specialized functions. For example,
<code>max.col(X)</code> is faster than <code>apply(X, 1, which.max)</code>.
</p>
</li></ul>

<p>When seeking speed, things to consider beyond the LaplacesDemon
package are the Basic Linear Algebra System (BLAS) and hardware. The
ATLAS (Automatically Tuned Linear Algebra System) should be worth
installing (and there are several alternatives), but this discussion
is beyond the scope of this documentation. Lastly, the speed at which
the computer can process iterations is limited by its hardware, and
more should be considered than merely the CPU (Central Processing
Unit). Again, though, this is beyond the scope of this documentation.
</p>


<h3>Value</h3>

<p>The <code>Model.Spec.Time</code> function returns a list with three
components:
</p>
<table>
<tr><td><code>Time</code></td>
<td>
<p>This is the time in minutes to evaluate the model
specification <code>n</code> times.</p>
</td></tr>
<tr><td><code>Evals.per.Minute</code></td>
<td>
<p>This is the number of evaluations of the
model specification per minute: <code>n</code> / <code>Time</code>. This is also
the number of iterations per minute in an algorithm that is not
componentwise, where one evaluation occurs per iteration.</p>
</td></tr>
<tr><td><code>Componentwise.Iters.per.Minute</code></td>
<td>
<p>This is the number of iterations
per minute in a componentwise MCMC algorithm, such as AMWG or
MWG. It is the evaluations per minute divided by the number of
parameters, since an evaluation must occur for each parameter, for
each iteration.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Statisticat, LLC.</p>


<h3>References</h3>

<p>Roberts, G.O. and Rosenthal, J.S. (2009). &quot;Examples of Adaptive
MCMC&quot;. <em>Computational Statistics and Data Analysis</em>, 18,
p. 349&ndash;367.
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+.C">.C</a></code>,
<code><a href="base.html#topic+.Fortran">.Fortran</a></code>,
</p>
<p><code><a href="base.html#topic+apply">apply</a></code>,
<code><a href="#topic+BigData">BigData</a></code>,
<code><a href="#topic+interval">interval</a></code>,
<code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code>,
<code><a href="#topic+Juxtapose">Juxtapose</a></code>,
<code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>,
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>,
<code><a href="base.html#topic+max.col">max.col</a></code>,
<code><a href="#topic+PMC">PMC</a></code>,
<code><a href="base.html#topic+system.time">system.time</a></code>, and
<code><a href="#topic+VariationalBayes">VariationalBayes</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># The accompanying Examples vignette is a compendium of examples.
####################  Load the LaplacesDemon Library  #####################
library(LaplacesDemon)

##############################  Demon Data  ###############################
data(demonsnacks)
y &lt;- log(demonsnacks$Calories)
X &lt;- cbind(1, as.matrix(log(demonsnacks[,c(1,4,10)]+1)))
J &lt;- ncol(X)
for (j in 2:J) {X[,j] &lt;- CenterScale(X[,j])}

#########################  Data List Preparation  #########################
mon.names &lt;- "LP"
parm.names &lt;- as.parm.names(list(beta=rep(0,J), sigma=0))
pos.beta &lt;- grep("beta", parm.names)
pos.sigma &lt;- grep("sigma", parm.names)
PGF &lt;- function(Data) return(c(rnormv(Data$J,0,10), rhalfcauchy(1,5)))
MyData &lt;- list(J=J, PGF=PGF, X=X, mon.names=mon.names,
     parm.names=parm.names, pos.beta=pos.beta, pos.sigma=pos.sigma, y=y)

##########################  Model Specification  ##########################
Model &lt;- function(parm, Data)
     {
     ### Parameters
     beta &lt;- parm[Data$pos.beta]
     sigma &lt;- interval(parm[Data$pos.sigma], 1e-100, Inf)
     parm[Data$pos.sigma] &lt;- sigma
     ### Log of Prior Densities
     beta.prior &lt;- sum(dnormv(beta, 0, 1000, log=TRUE))
     sigma.prior &lt;- dhalfcauchy(sigma, 25, log=TRUE)
     ### Log-Likelihood
     mu &lt;- tcrossprod(Data$X, t(beta))
     LL &lt;- sum(dnorm(Data$y, mu, sigma, log=TRUE))
     ### Log-Posterior
     LP &lt;- LL + beta.prior + sigma.prior
     Modelout &lt;- list(LP=LP, Dev=-2*LL, Monitor=LP,
          yhat=rnorm(length(mu), mu, sigma), parm=parm)
     return(Modelout)
     }

set.seed(666)

############################  Initial Values  #############################
Initial.Values &lt;- GIV(Model, MyData, PGF=TRUE)

############################  Model.Spec.Time  ############################
### Not byte-compiled
Model.Spec.Time(Model, Initial.Values, MyData)
### Byte-compiled
library(compiler)
Model &lt;- cmpfun(Model)
Model.Spec.Time(Model, Initial.Values, MyData)
</code></pre>

<hr>
<h2 id='p.interval'>Probability Interval</h2><span id='topic+p.interval'></span>

<h3>Description</h3>

<p>This function returns one or more probability intervals of posterior
samples.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>p.interval(obj, HPD=TRUE, MM=TRUE, prob=0.95, plot=FALSE, PDF=FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="p.interval_+3A_obj">obj</code></td>
<td>
<p>This can be either a vector or matrix of posterior samples,
or an object of class <code>demonoid</code>, <code>iterquad</code>,
<code>laplace</code>, <code>pmc</code>, or <code>vb</code>. If it is an object of
class <code>demonoid</code>, then it will use only stationary posterior
samples and monitored target distributions (automatically discarding
the burn-in; if stationarity does not exist, then it will use all
samples).</p>
</td></tr>
<tr><td><code id="p.interval_+3A_hpd">HPD</code></td>
<td>
<p>Logical. This argument defaults to <code>TRUE</code>, in which
case one or more Highest Posterior Density (HPD) intervals is
returned. When <code>FALSE</code>, one or more quantile-based probability
intervals is returned.</p>
</td></tr>
<tr><td><code id="p.interval_+3A_mm">MM</code></td>
<td>
<p>Logical. This argument defaults to <code>TRUE</code>, in which
case each column vector is checked for multimodality, and if found,
the multimodal form of a Highest Posterior Density (HPD) interval is
additionally estimated, even when <code>HPD=FALSE</code>.</p>
</td></tr>
<tr><td><code id="p.interval_+3A_prob">prob</code></td>
<td>
<p>This is a numeric scalar in the interval (0,1) giving the
target probability interval, and defaults to 0.95, representing a
95% probability interval. A 95% probability interval, for example,
is an interval that contains 95% of a posterior probability
distribution.</p>
</td></tr>
<tr><td><code id="p.interval_+3A_plot">plot</code></td>
<td>
<p>Logical. When <code>plot=TRUE</code>, each kernel density is
plotted and shaded gray, and the area under the curve within the
probability interval is shaded black. If the kernel density is
considered to be multimodal, then up to three intervals are shaded
black. A vertical, red, dotted line is added at zero. The
<code>plot</code> argument defaults to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="p.interval_+3A_pdf">PDF</code></td>
<td>
<p>Logical. When <code>PDF=TRUE</code>, and only when
<code>plot=TRUE</code>, plots are saved as a .pdf file in the working
directory.</p>
</td></tr>
<tr><td><code id="p.interval_+3A_...">...</code></td>
<td>
<p>Additional arguments are unused.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A probability interval, also called a credible interval or Bayesian
confidence interval, is an interval in the domain of a posterior
probability distribution. When generalized to multivariate forms, it
is called a probability region (or credible region), though some
sources refer to a probability region (or credible region) as the
area within the probability interval. Bivariate probability regions
may be plotted with the <code><a href="#topic+joint.pr.plot">joint.pr.plot</a></code> function.
</p>
<p>The <code>p.interval</code> function may return different probability
intervals: a quantile-based probability interval, a unimodal
Highest Posterior Density (HPD) interval, and multimodal HPD
intervals. Another type of probability interval is the Lowest
Posterior Loss (LPL) interval, which is calculated with the
<code><a href="#topic+LPL.interval">LPL.interval</a></code> function.
</p>
<p>The quantile-based probability interval is used most commonly,
possibly because it is simple, the fastest to calculate, invariant
under transformation, and more closely resembles the frequentist
confidence interval. The lower and upper bounds of the
quantile-based probability interval are calculated with the
<code>quantile</code> function. A 95% quantile-based probability interval
reports the values of the posterior probability distribution that
indicate the 2.5% and 97.5% quantiles, which contain the central
95% of the distribution. The quantile-based probability interval is
centered around the median and has equal-sized tails.
</p>
<p>The HPD (highest posterior density) interval is identical to
the quantile-based probability interval when the posterior probability
distribution is unimodal and symmetric. Otherwise, the HPD interval
is the smallest interval, because it is estimated as the interval
that contains the highest posterior density. Unlike the quantile-based
probability interval, the HPD interval could be one-tailed or
two-tailed, whichever is more appropriate. However, unlike the
quantile-based interval, the HPD interval is not invariant to
reparameterization (Bernardo, 2005).
</p>
<p>The unimodal HPD interval is estimated from the empirical CDF of the
sample for each parameter (or deviance or monitored variable) as the
shortest interval for which the difference in the ECDF values of the
end-points is the user-specified probability width. This assumes the
distribution is not severely multimodal.
</p>
<p>As an example, imagine an exponential posterior distribution. A
quantile-based probability interval would report the highest density
region near zero to be outside of its interval. In contrast, the
unimodal HPD interval is recommended for such skewed posterior
distributions.
</p>
<p>When <code>MM=TRUE</code>, the <code><a href="#topic+is.multimodal">is.multimodal</a></code> function is
applied to each column vector after the unimodal interval (either
quantile-based or HPD) is estimated. If multimodality is found, then
multimodal HPD intervals are estimated with kernel density and
printed to the screen as a character string. The original unimodal
intervals are returned in the output matrix, because the matrix is
constrained to have a uniform number of columns per row, and because
multimodal HPD intervals may be disjoint.
</p>
<p>Disjoint multimodal HPD intervals have multiple intervals for one
posterior probability distribution. An example may be when there is a
bimodal, Gaussian distribution with means -10 and 10, variances of 1
and 1, and a 95% probability interval is specified. In this case,
there is not enough density between these two distant modes to have
only one probability interval.
</p>
<p>The user should also consider <code><a href="#topic+LPL.interval">LPL.interval</a></code>, since it is
invariant to reparameterization like the quantile-based probability
interval, but could be one- or two-tailed, whichever is more
appropriate, like the HPD interval. A comparison of the quantile-based
probability interval, HPD interval, and LPL interval is available
here: <a href="https://web.archive.org/web/20150214090353/http://www.bayesian-inference.com/credible">https://web.archive.org/web/20150214090353/http://www.bayesian-inference.com/credible</a>.
</p>


<h3>Value</h3>

<p>A matrix is returned with rows corresponding to the parameters (or
deviance or monitored variables), and columns <code>"Lower"</code> and
<code>"Upper"</code>. The elements of the matrix are the unimodal
probability intervals. The attribute <code>"Probability"</code> is the
user-selected probability width. If <code>MM=TRUE</code> and multimodal
posterior distributions are found, then multimodal HPD intervals are
printed to the screen in a character string.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC</p>


<h3>References</h3>

<p>Bernardo, J.M. (2005). &quot;Intrinsic Credible Regions: An Objective
Bayesian Approach to Interval Estimation&quot;. <em>Sociedad de
Estadistica e Investigacion Operativa</em>, 14(2), p. 317&ndash;384.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+is.multimodal">is.multimodal</a></code>,
<code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code>,
<code><a href="#topic+joint.pr.plot">joint.pr.plot</a></code>,
<code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>,
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>,
<code><a href="#topic+LPL.interval">LPL.interval</a></code>,
<code><a href="#topic+PMC">PMC</a></code>, and
<code><a href="#topic+VariationalBayes">VariationalBayes</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>##First, update the model with the LaplacesDemon function.
##Then
#p.interval(Fit, HPD=TRUE, MM=TRUE, prob=0.95)
</code></pre>

<hr>
<h2 id='plot.bmk'>Plot Hellinger Distances</h2><span id='topic+plot.bmk'></span>

<h3>Description</h3>

<p>This function plots Hellinger distances in an object of class <code>bmk</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'bmk'
plot(x, col=colorRampPalette(c("black","red"))(100),
     title="", PDF=FALSE, Parms=NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.bmk_+3A_x">x</code></td>
<td>
<p>This required argument is an object of class <code>bmk</code>. See
the <code><a href="#topic+BMK.Diagnostic">BMK.Diagnostic</a></code> function for more information.</p>
</td></tr>
<tr><td><code id="plot.bmk_+3A_col">col</code></td>
<td>
<p>This argument specifies the colors of the cells. By
default, the <code>colorRampPalette</code> function colors large Hellinger
distances as <code>red</code>, small as <code>black</code>, and provides 100
color gradations.</p>
</td></tr>
<tr><td><code id="plot.bmk_+3A_title">title</code></td>
<td>
<p>This argument specifies the title of the plot, and the
default does not include a title.</p>
</td></tr>
<tr><td><code id="plot.bmk_+3A_pdf">PDF</code></td>
<td>
<p>Logical. When <code>TRUE</code>, the plot is saved as a .pdf
file.</p>
</td></tr>
<tr><td><code id="plot.bmk_+3A_parms">Parms</code></td>
<td>

<p>This argument accepts a vector of quoted strings to be matched for
selecting parameters for plotting. This argument defaults to
<code>NULL</code> and selects every parameter for plotting. Each quoted
string is matched to one or more parameter names with the
<code>grep</code> function. For example, if the user specifies
<code>Parms=c("eta", "tau")</code>, and if the parameter names
are beta[1], beta[2], eta[1], eta[2], and tau, then all parameters
will be selected, because the string <code>eta</code> is within
<code>beta</code>. Since <code>grep</code> is used, string matching uses
regular expressions, so beware of meta-characters, though these are
acceptable: &quot;.&quot;, &quot;[&quot;, and &quot;]&quot;.</p>
</td></tr>
<tr><td><code id="plot.bmk_+3A_...">...</code></td>
<td>
<p>Additional arguments are unused.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>plot.bmk</code> function plots the Hellinger distances in an
object of class <code>bmk</code>. This is useful for quickly finding
portions of chains with large Hellinger distances, which indicates
non-stationarity and non-convergence.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+BMK.Diagnostic">BMK.Diagnostic</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
N &lt;- 1000 #Number of posterior samples
J &lt;- 10 #Number of parameters
Theta &lt;- matrix(runif(N*J),N,J)
colnames(Theta) &lt;- paste("beta[", 1:J, "]", sep="")
for (i in 2:N) {Theta[i,1] &lt;- Theta[i-1,1] + rnorm(1)}
HD &lt;- BMK.Diagnostic(Theta, batches=10)
plot(HD, title="Hellinger distance between batches")
</code></pre>

<hr>
<h2 id='plot.demonoid'>Plot samples from the output of Laplace's Demon</h2><span id='topic+plot.demonoid'></span><span id='topic+plot.demonoid.hpc'></span>

<h3>Description</h3>

<p>This may be used to plot, or save plots of, samples in an object of
class <code>demonoid</code> or <code>demonoid.hpc</code>. Plots include a trace
plot, density plot, autocorrelation or ACF plot, and if an adaptive
algorithm was used, the absolute difference in the proposal variance,
or the value of epsilon, across adaptations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'demonoid'
plot(x, BurnIn=0, Data, PDF=FALSE, Parms, FileName, ...)
## S3 method for class 'demonoid.hpc'
plot(x, BurnIn=0, Data, PDF=FALSE, Parms, FileName, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.demonoid_+3A_x">x</code></td>
<td>

<p>This required argument is an object of class <code>demonoid</code> or
<code>demonoid.hpc</code>.</p>
</td></tr>
<tr><td><code id="plot.demonoid_+3A_burnin">BurnIn</code></td>
<td>

<p>This argument requires zero or a positive integer that indicates the
number of thinned samples to discard as burn-in for the purposes of
plotting. For more information on burn-in, see <code><a href="#topic+burnin">burnin</a></code>.</p>
</td></tr>
<tr><td><code id="plot.demonoid_+3A_data">Data</code></td>
<td>

<p>This required argument must receive the list of data that was
supplied to <code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code> to create the object of
class <code>demonoid</code>.</p>
</td></tr>
<tr><td><code id="plot.demonoid_+3A_pdf">PDF</code></td>
<td>

<p>This logical argument indicates whether or not the user wants
Laplace's Demon to save the plots as a .pdf file.</p>
</td></tr>
<tr><td><code id="plot.demonoid_+3A_parms">Parms</code></td>
<td>

<p>This argument accepts a vector of quoted strings to be matched for
selecting parameters for plotting. This argument defaults to
<code>NULL</code> and selects every parameter for plotting. Each quoted
string is matched to one or more parameter names with the
<code>grep</code> function. For example, if the user specifies
<code>Parms=c("eta", "tau")</code>, and if the parameter names
are beta[1], beta[2], eta[1], eta[2], and tau, then all parameters
will be selected, because the string <code>eta</code> is within
<code>beta</code>. Since <code>grep</code> is used, string matching uses
regular expressions, so beware of meta-characters, though these are
acceptable: &quot;.&quot;, &quot;[&quot;, and &quot;]&quot;.</p>
</td></tr>
<tr><td><code id="plot.demonoid_+3A_filename">FileName</code></td>
<td>

<p>This argument accepts a string and save the plot under the specified name. If <code>PDF=FALSE</code> this argument in unused. By default, <code>FileName = paste0("laplacesDemon-plot_", format(Sys.time(), "yyyy-mm-dd_h:m:s"), ".pdf")</code>
</p>
</td></tr>
<tr><td><code id="plot.demonoid_+3A_...">...</code></td>
<td>
<p>Additional arguments are unused.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The plots are arranged in a <code class="reqn">3 \times 3</code> matrix. Each row
represents a parameter, the deviance, or a monitored variable. The
left column displays trace plots, the middle column displays kernel
density plots, and the right column displays autocorrelation (ACF)
plots.
</p>
<p>Trace plots show the thinned history of the chain or Markov chain,
with its value in the y-axis moving by thinned sample across the x-axis.
A chain or Markov chain with good properties does not suggest a trend
upward or downward as it progresses across the x-axis (it should
appear stationary), and it should mix well, meaning it should appear
as though random samples are being taken each time from the same
target distribution. Visual inspection of a trace plot cannot
verify convergence, but apparent non-stationarity or poor mixing can
certainly suggest non-convergence. A red, smoothed line also appears
to aid visual inspection.
</p>
<p>Kernel density plots depict the marginal posterior distribution.
Although there is no distributional assumption about this density,
kernel density estimation uses Gaussian basis functions.
</p>
<p>Autocorrelation plots show the autocorrelation or serial correlation
between values of thinned samples at nearby thinned samples. Samples with
autocorrelation do not violate any assumption, but are inefficient
because they reduce the effective sample size (<code><a href="#topic+ESS">ESS</a></code>), and
indicate that the chain is not mixing well, since each value is
influenced by values that are previous and nearby. The x-axis
indicates lags with respect to thinned samples, and the y-axis
represents autocorrelation. The ideal autocorrelation plot shows
perfect correlation at zero lag, and quickly falls to zero
autocorrelation for all other lags.
</p>
<p>If an adaptive algorithm was used, then the distribution of absolute
differences in the proposal variances, or the value of epsilon, is
plotted across adaptations. The proposal variance, or epsilon, should
change less as the adaptive algorithm approaches the target
distributions. The absolute differences in the proposal variance plot
should approach zero. This is called the condition of diminishing
adaptation. If it is not approaching zero, then consider using a
different adaptive MCMC algorithm. The following quantiles are plotted
for absolute changes proposal variance: 0.025, 0.500, and 0.975.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+burnin">burnin</a></code>,
<code><a href="#topic+ESS">ESS</a></code>,
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>, and
<code><a href="#topic+LaplacesDemon.hpc">LaplacesDemon.hpc</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'>### See the LaplacesDemon function for an example.</code></pre>

<hr>
<h2 id='plot.demonoid.ppc'>Plots of Posterior Predictive Checks</h2><span id='topic+plot.demonoid.ppc'></span>

<h3>Description</h3>

<p>This may be used to plot, or save plots of, samples in an object of
class <code>demonoid.ppc</code>. A variety of plots is provided.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'demonoid.ppc'
plot(x, Style=NULL, Data=NULL, Rows=NULL,
     PDF=FALSE, ...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.demonoid.ppc_+3A_x">x</code></td>
<td>

<p>This required argument is an object of class <code>demonoid.ppc</code>.</p>
</td></tr>
<tr><td><code id="plot.demonoid.ppc_+3A_style">Style</code></td>
<td>

<p>This optional argument specifies one of several styles of plots,
and defaults to <code>NULL</code> (which is the same as
<code>"Density"</code>). Styles of plots are indicated in quotes. Optional
styles include <code>"Covariates"</code>, <code>"Covariates, Categorical DV"</code>,
<code>"Density"</code>, <code>"DW"</code>, <code>"DW, Multivariate, C"</code>,
<code>"ECDF"</code>, <code>"Fitted"</code>, <code>"Fitted, Multivariate, C"</code>,
<code>"Fitted, Multivariate, R"</code>, <code>"Jarque-Bera"</code>,
<code>"Jarque-Bera, Multivariate, C"</code>,
<code>"Mardia"</code>, <code>"Predictive Quantiles"</code>,
<code>"Residual Density"</code>, <code>"Residual Density, Multivariate, C"</code>,
<code>"Residual Density, Multivariate, R"</code>,
<code>"Residuals"</code>, <code>"Residuals, Multivariate, C"</code>,
<code>"Residuals, Multivariate, R"</code>, <code>"Space-Time by Space"</code>,
<code>"Space-Time by Time"</code>, <code>"Spatial"</code>,
<code>"Spatial Uncertainty"</code>, <code>"Time-Series"</code>,
<code>"Time-Series, Multivariate, C"</code>, and
<code>"Time-Series, Multivariate, R"</code>. Details are given below.</p>
</td></tr>
<tr><td><code id="plot.demonoid.ppc_+3A_data">Data</code></td>
<td>

<p>This optional argument accepts the data set used when updating the
model. Data is required only with certain plot styles, including
<code>"Covariates"</code>, <code>"Covariates, Categorical DV"</code>,
<code>"DW, Multivariate, C"</code>, <code>"Fitted, Multivariate, C"</code>,
<code>"Fitted, Multivariate, R"</code>,
<code>"Jarque-Bera, Multivariate, C"</code>, <code>"Mardia"</code>,
<code>"Residual Density, Multivariate, C"</code>,
<code>"Residual Density, Multivariate, R"</code>,
<code>"Residuals, Multivariate, C"</code>,
<code>"Residuals, Multivariate, R"</code>,
<code>"Space-Time by Space"</code>, <code>"Space-Time by Time"</code>,
<code>"Spatial"</code>, <code>"Spatial Uncertainty"</code>,
<code>"Time-Series, Multivariate, C"</code>, and
<code>"Time-Series, Multivariate, R"</code>.</p>
</td></tr>
<tr><td><code id="plot.demonoid.ppc_+3A_rows">Rows</code></td>
<td>

<p>This optional argument is for a vector of row numbers that 
specify the records associated by row in the object of class
<code>demonoid.ppc</code>. Only these rows are plotted. The default is to
plot all rows. Some plots do not allow rows to be specified.</p>
</td></tr>
<tr><td><code id="plot.demonoid.ppc_+3A_pdf">PDF</code></td>
<td>

<p>This logical argument indicates whether or not the user wants
Laplace's Demon to save the plots as a .pdf file.</p>
</td></tr>
<tr><td><code id="plot.demonoid.ppc_+3A_...">...</code></td>
<td>
<p>Additional arguments are unused.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function can be used to produce a variety of posterior predictive
plots, and the style of plot is selected with the <code>Style</code>
argument. Below are some notes on the styles of plots.
</p>
<p><code>Covariates</code> requires <code>Data</code> to be specified, and also
requires that the covariates are named <code>X</code> or <code>x</code>. A plot
is produced for each covariate column vector against yhat, and is
appropriate when y is not categorical.
</p>
<p><code>Covariates, Categorical DV</code> requires <code>Data</code> to be
specified, and also requires that the covariates are named <code>X</code> or
<code>x</code>. A plot is produced for each covariate column vector against
yhat, and is appropriate when y is categorical.
</p>
<p><code>Density</code> plots show the kernel density of the posterior
predictive distribution for each selected row of y (all are selected
by default). A vertical red line indicates the position of the
observed y along the x-axis. When the vertical red line is close to
the middle of a normal posterior predictive distribution, then there
is little discrepancy between y and the posterior predictive
distribution. When the vertical red line is in the tail of the
distribution, or outside of the kernel density altogether, then
there is a large discrepancy between y and the posterior predictive
distribution. Large discrepancies may be considered outliers, and
moreover suggest that an improvement in model fit should be
considered.
</p>
<p><code>DW</code> plots the distributions of the Durbin-Watson (DW) test
statistics (Durbin and Watson, 1950), both observed
(<code class="reqn">d^{obs}</code> as a transparent, black density) and replicated
(<code class="reqn">d^{rep}</code> as a transparent, red density). The distribution
of <code class="reqn">d^{obs}</code> is estimated from the model, and
<code class="reqn">d^{rep}</code> is simulated from normal residuals without
autocorrelation, where the number of simulations are the same as the
observed number. This DW test may be applied to the residuals of
univariate time-series models (or otherwise ordered residuals) to
detect first-order autocorrelation. Autocorrelated residuals are not
independent. The DW test is applicable only when the residuals are
normally-distributed, higher-order autocorrelation is not present, and
y is not used also as a lagged predictor. The DW test statistic,
<code class="reqn">d^{obs}</code>, occurs in the interval (0,4), where 0 is
perfect positive autocorrelation, 2 is no autocorrelation, and 4 is
perfect negative autocorrelation. The following summary is reported on
the plot: the mean of <code class="reqn">d^{obs}</code> (and its 95% probability
interval), the probability that <code class="reqn">d^{obs} &gt; d^{rep}</code>, and whether or not autocorrelation is found. Positive
autocorrelation is reported when the observed process is greater than
the replicated process in 2.5% of the samples, and negative
autocorrelation is reported when the observed process is greater than
the replicated process in 97.5% of the samples.
</p>
<p><code>DW, Multivariate, C</code> requires <code>Data</code> to be
specified, and also requires that variable <code>Y</code> exist in the data
set with exactly that name. These plots compare each column-wise
vector of residuals with a univariate Durbin-Watson test, as in
<code>DW</code> above. This plot is appropriate when Y is multivariate, not
categorical, and residuals are desired to be tested column-wise for
first-order autocorrelation.
</p>
<p><code>ECDF</code> (Empirical Cumulative Distribution Function) plots compare
the ECDF of y with three ECDFs of yhat based on the 2.5%, 50%
(median), and 97.5% of its distribution. The ECDF(y) is defined as
the proportion of values less than or equal to y. This plot is
appropriate when y is univariate and at least ordinal.
</p>
<p><code>Fitted</code> plots compare y with the probability interval of its
replicate, and provide loess smoothing. This plot is appropriate when
y is univariate and not categorical.
</p>
<p><code>Fitted, Multivariate, C</code> requires <code>Data</code> to be
specified, and also requires that variable <code>Y</code> exists in the data
set with exactly that name. These plots compare each column-wise
vector of y in Y with its replicates and provide loess smoothing.
This plot is appropriate when Y is multivariate, not categorical, and
desired to be seen column-wise.
</p>
<p><code>Fitted, Multivariate, R</code> requires <code>Data</code> to be
specified, and also requires that variable <code>Y</code> exists in the data
set with exactly that name. These plots compare each row-wise
vector of y in Y with its replicates and provide loess smoothing.
This plot is appropriate when Y is multivariate, not categorical, and
desired to be seen row-wise.
</p>
<p><code>Jarque-Bera</code> plots the distributions of the Jarque-Bera (JB)
test statistics (Jarque and Bera, 1980), both observed
(<code class="reqn">JB^{obs}</code> as a transparent black density) and replicated
(<code class="reqn">JB^{rep}</code> as a transparent red density). The
distribution of <code class="reqn">JB^{obs}</code> is estimated from the model,
and <code class="reqn">JB^{rep}</code> is simulated from normal residuals, where
the number of simulations are the same as the observed number. This
Jarque-Bera test may be applied to the residuals of 
univariate models to test for normality. The Jarque-Bera test does not
test normality per se, but whether or not the distribution has
kurtosis and skewness that match a normal distribution, and is
therefore a test of the moments of a normal distribution. The
following summary is reported on the plot: the mean of
<code class="reqn">JB^{obs}</code> (and its 95% probability interval), the
probability that <code class="reqn">JB^{obs} &gt; JB^{rep}</code>, and
whether or not normality is indicated. Non-normality is reported when
the observed process is greater than the replicated process in either
2.5% or 97.5% of the samples.
</p>
<p><code>Jarque-Bera, Multivariate, C</code> requires <code>Data</code> to be
specified, and also requires that variable <code>Y</code> exist in the data
set with exactly that name. These plots compare each column-wise
vector of residuals with a univariate Jarque-Bera test, as in
<code>Jarque-Bera</code> above. This plot is appropriate when Y is
multivariate, not categorical, and residuals are desired to be
tested column-wise for normality.
</p>
<p><code>Mardia</code> plots the distributions of the skewness (K3) and
kurtosis (K4) test statistics (Mardia, 1970), both observed
(<code class="reqn">K3^{obs}</code> and <code class="reqn">K4^{obs}</code> as transparent
black density) and replicated (<code class="reqn">K3^{rep}</code> and
<code class="reqn">K4^{rep}</code> as transparent red density). The distributions
of <code class="reqn">K3^{obs}</code> and <code class="reqn">K4^{obs}</code> are estimated
from the model, and both <code class="reqn">K3^{rep}</code> <code class="reqn">K4^{rep}</code>
are simulated from multivariate normal residuals, where the number of
simulations are the same as the observed number. This Mardia's test
may be applied to the residuals of multivariate models to test for
multivariate normality. Mardia's test does not test for multivariate
normality per se, but whether or not the distribution has kurtosis and
skewness that match a multivariate normal distribution, and is 
therefore a test of the moments of a multivariate normal
distribution. The following summary is reported on the plots: the
means of <code class="reqn">K3^{obs}</code> and <code class="reqn">K4^{obs}</code> (and
the associated 95% probability intervals), the probabilities that
<code class="reqn">K3^{obs} &gt; K3^{rep}</code> and
<code class="reqn">K4^{obs} &gt; K4^{rep}</code>, and whether or not
multivariate normality is indicated. Non-normality is reported when 
the observed process is greater than the replicated process in either
2.5% or 97.5% of the samples. <code>Mardia</code> requires <code>Data</code> to
be specified, and also requires that variable <code>Y</code> exist in the
data set with exactly that name. <code>Y</code> must be a <code class="reqn">N \times P</code> matrix of <code class="reqn">N</code> records and <code class="reqn">P</code> variables. Source
code was modified from the deprecated package QRMlib.
</p>
<p><code>Predictive Quantiles</code> plots compare y with the predictive
quantile (PQ) of its replicate. This may be useful in looking for
patterns with outliers. Instances outside of the gray lines are
considered outliers.
</p>
<p><code>Residual Density</code> plots the residual density of the median of
the samples. A vertical red line occurs at zero. This plot may be
useful for inspecting a distributional assumption of residual
variance. This plot is appropriate when y is univariate and
continuous.
</p>
<p><code>Residual Density, Multivariate C</code> requires <code>Data</code> to be
specified, and also requires that variable <code>Y</code> exist in the data
set with exactly that name. These are column-wise plots of residual
density, given the median of the samples. These plots may be useful
for inspecting a distributional assumption of residual variance.
This plot is appropriate when Y is multivariate, continuous, and
densities are desired to be seen column-wise.
</p>
<p><code>Residual Density, Multivariate R</code> requires <code>Data</code> to be
specified, and also requires that variable <code>Y</code> exist in the data
set with exactly that name. These are row-wise plots of residual
density, given the median of the samples. These plots may be useful
for inspecting a distributional assumption of residual variance.
This plot is appropriate when Y is multivariate, continuous, and
densities are desired to be seen row-wise.
</p>
<p><code>Residuals</code> plots compare y with its residuals. The probability
interval is plotted as a line. This plot is appropriate when y
is univariate.
</p>
<p><code>Residuals, Multivariate, C</code> requires <code>Data</code> to be
specified, and also requires that variable <code>Y</code> exist in the data
set with exactly that name. These are plots of each column-wise
vector of residuals. The probability interval is plotted as a
line. This plot is appropriate when Y is multivariate, not
categorical, and the residuals are desired to be seen column-wise.
</p>
<p><code>Residuals, Multivariate, R</code> requires <code>Data</code> to be
specified, and also requires that variable <code>Y</code> exist in the data
set with exactly that name. These are plots of each row-wise
vector of residuals. The probability interval is plotted as a
line. This plot is appropriate when Y is multivariate, not
categorical, and the residuals are desired to be seen row-wise.
</p>
<p><code>Space-Time by Space</code> requires <code>Data</code> to be specified, and
also requires that the following variables exist in the data set with
exactly these names: <code>latitude</code>, <code>longitude</code>, <code>S</code>, and
<code>T</code>. These space-time plots compare the S x T matrix Y with the S
x T matrix Yrep, producing one time-series plot per point s in space,
for a total of S plots. Therefore, these are time-series plots for
each point s in space across T time-periods. See <code>Time-Series</code>
plots below.
</p>
<p><code>Space-Time by Time</code> requires <code>Data</code> to be specified, and
also requires that the following variables exist in the data set with
exactly these names: <code>latitude</code>, <code>longitude</code>, <code>S</code>, and
<code>T</code>. These space-time plots compare the S x T matrix Y with the S
x T matrix Yrep, producing one spatial plot per time-period, and T
plots will be produced. See <code>Spatial</code> plots below.
</p>
<p><code>Spatial</code> requires <code>Data</code> to be specified, and also requires
that the following variables exist in the data set with exactly these
names: <code>latitude</code> and <code>longitude</code>. This spatial plot shows
yrep plotted according to its coordinates, and is color-coded so that
higher values of yrep become more red, and lower values become more
yellow.
</p>
<p><code>Spatial Uncertainty</code> requires <code>Data</code> to be specified, and
also requires that the following variables exist in the data set with
exactly these names: <code>latitude</code> and <code>longitude</code>. This
spatial plot shows the probability interval of yrep plotted according
to its coordinates, and is color-coded so that wider probability
intervals become more red, and lower values become more yellow.
</p>
<p><code>Time-Series</code> plots compare y with its replicate, including the
median and probability interval quantiles. This plot is appropriate
when y is univariate and ordered by time.
</p>
<p><code>Time-Series, Multivariate, C</code> requires <code>Data</code> to be
specified, and also requires that variable <code>Y</code> exist in the data
set with exactly that name. These plots compare each column-wise
time-series in Y with its replicate, including the median and
probability interval quantiles. This plot is appropriate when y is
multivariate and each time-series is indexed by column in Y.
</p>
<p><code>Time-Series, Multivariate, R</code> requires <code>Data</code> to be
specified, and also requires that variable <code>Y</code> exist in the data
set with exactly that name. These plots compare each row-wise
time-series in Y with its replicate, including the median and
probability interval quantiles. This plot is appropriate when y is
multivariate and each time-series is indexed by row in Y, such as is
typically true in panel models.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>References</h3>

<p>Durbin, J., and Watson, G.S. (1950). &quot;Testing for Serial Correlation
in Least Squares Regression, I.&quot; <em>Biometrika</em>, 37, p. 409&ndash;428.
</p>
<p>Jarque, C.M. and Bera, A.K. (1980). &quot;Efficient Tests for Normality,
Homoscedasticity and Serial Independence of Regression Residuals&quot;.
<em>Economics Letters</em>, 6(3), p. 255&ndash;259.
</p>
<p>Mardia, K.V. (1970). &quot;Measures of Multivariate Skewness and Kurtosis
with Applications&quot;. <em>Biometrika</em>, 57(3), p. 519&ndash;530.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code> and
<code><a href="#topic+predict.demonoid">predict.demonoid</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### See the LaplacesDemon function for an example.</code></pre>

<hr>
<h2 id='plot.importance'>Plot Variable Importance</h2><span id='topic+plot.importance'></span>

<h3>Description</h3>

<p>This may be used to plot variable importance with BPIC, predictive
concordance, a discrepancy statistic, or the L-criterion regarding an
object of class <code>importance</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'importance'
plot(x, Style="BPIC", ...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.importance_+3A_x">x</code></td>
<td>
<p>This required argument is an object of class
<code>importance</code>.</p>
</td></tr>
<tr><td><code id="plot.importance_+3A_style">Style</code></td>
<td>
<p>When <code>Style="BPIC"</code>, BPIC is shown, and <code>BPIC</code>
is the default. Otherwise, predictive concordance is plotted when
<code>Style="Concordance"</code>, a discrepancy statistic is plotted when
<code>Style="Discrep"</code>, or the L-criterion is plotted when
<code>Style="L-criterion"</code>.</p>
</td></tr>
<tr><td><code id="plot.importance_+3A_...">...</code></td>
<td>
<p>Additional arguments are unused.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The x-axis is either BPIC (Ando, 2007), predictive concordance
(Gelfand, 1996), a discrepancy statistic (Gelman et al., 1996), or the
L-criterion (Laud and Ibrahim, 1995) of the <code><a href="#topic+Importance">Importance</a></code>
function (depending on the <code>Style</code> argument), and variables are
on the y-axis. A more important variable is associated with a dot that
is plotted farther to the right. For more information on variable
importance, see the <code><a href="#topic+Importance">Importance</a></code> function.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>References</h3>

<p>Ando, T. (2007). &quot;Bayesian Predictive Information Criterion for
the Evaluation of Hierarchical Bayesian and Empirical Bayes Models&quot;.
<em>Biometrika</em>, 94(2), p. 443&ndash;458.
</p>
<p>Gelfand, A. (1996). &quot;Model Determination Using Sampling Based
Methods&quot;. In Gilks, W., Richardson, S., Spiegehalter, D., Chapter 9 in
Markov Chain Monte Carlo in Practice. Chapman and Hall: Boca Raton, FL.
</p>
<p>Gelman, A., Meng, X.L., and Stern H. (1996). &quot;Posterior Predictive
Assessment of Model Fitness via Realized Discrepancies&quot;.
<em>Statistica Sinica</em>, 6, p. 733&ndash;807.
</p>
<p>Laud, P.W. and Ibrahim, J.G. (1995). &quot;Predictive Model
Selection&quot;. <em>Journal of the Royal Statistical Society</em>, B 57,
p. 247&ndash;262.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Importance">Importance</a></code></p>

<hr>
<h2 id='plot.iterquad'>Plot the output of <code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code></h2><span id='topic+plot.iterquad'></span>

<h3>Description</h3>

<p>This may be used to plot, or save plots of, the iterated history of
the parameters and, if posterior samples were taken, density plots of
parameters and monitors in an object of class <code>iterquad</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'iterquad'
plot(x, Data, PDF=FALSE, Parms, ...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.iterquad_+3A_x">x</code></td>
<td>

<p>This required argument is an object of class <code>iterquad</code>.</p>
</td></tr>
<tr><td><code id="plot.iterquad_+3A_data">Data</code></td>
<td>

<p>This required argument must receive the list of data that was
supplied to <code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code> to create the object
of class <code>iterquad</code>.</p>
</td></tr>
<tr><td><code id="plot.iterquad_+3A_pdf">PDF</code></td>
<td>

<p>This logical argument indicates whether or not the user wants
Laplace's Demon to save the plots as a .pdf file.</p>
</td></tr>
<tr><td><code id="plot.iterquad_+3A_parms">Parms</code></td>
<td>

<p>This argument accepts a vector of quoted strings to be matched for
selecting parameters for plotting. This argument defaults to
<code>NULL</code> and selects every parameter for plotting. Each quoted
string is matched to one or more parameter names with the
<code>grep</code> function. For example, if the user specifies
<code>Parms=c("eta", "tau")</code>, and if the parameter names
are beta[1], beta[2], eta[1], eta[2], and tau, then all parameters
will be selected, because the string <code>eta</code> is within
<code>beta</code>. Since <code>grep</code> is used, string matching uses
regular expressions, so beware of meta-characters, though these are
acceptable: &quot;.&quot;, &quot;[&quot;, and &quot;]&quot;.</p>
</td></tr>
<tr><td><code id="plot.iterquad_+3A_...">...</code></td>
<td>
<p>Additional arguments are unused.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The plots are arranged in a <code class="reqn">2 \times 2</code> matrix. The
purpose of the iterated history plots is to show how the value of each
parameter and the deviance changed by iteration as the
<code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code> attempted to fit a normal
distribution to the marginal posterior distributions.
</p>
<p>The plots on the right show several densities, described below.
</p>

<ul>
<li><p> The transparent black density is the normalized quadrature
weights for non-standard normal distributions, <code class="reqn">M</code>. For
multivariate quadrature, there are often multiple weights at a given
node, and the average <code class="reqn">M</code> is shown. Vertical black lines
indicate the nodes.
</p>
</li>
<li><p> The transparent red density is the normalized LP weights. For
multivariate quadrature, there are often multiple weights at a given
node, and the average normalized and weighted LP is shown. Vertical
red lines indicate the nodes.
</p>
</li>
<li><p> The transparent green density is the normal density implied
given the conditional mean and conditional variance.
</p>
</li>
<li><p> The transparent blue density is the kernel density estimate
of posterior samples generated with Sampling Importance Resampling.
This is plotted only if the algorithm converged, and if
<code>sir=TRUE</code>.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>### See the IterativeQuadrature function for an example.</code></pre>

<hr>
<h2 id='plot.iterquad.ppc'>Plots of Posterior Predictive Checks</h2><span id='topic+plot.iterquad.ppc'></span>

<h3>Description</h3>

<p>This may be used to plot, or save plots of, samples in an object of
class <code>iterquad.ppc</code>. A variety of plots is provided.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'iterquad.ppc'
plot(x, Style=NULL, Data=NULL,  Rows=NULL,
     PDF=FALSE, ...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.iterquad.ppc_+3A_x">x</code></td>
<td>

<p>This required argument is an object of class <code>iterquad.ppc</code>.</p>
</td></tr>
<tr><td><code id="plot.iterquad.ppc_+3A_style">Style</code></td>
<td>

<p>This optional argument specifies one of several styles of plots, and
defaults to <code>NULL</code> (which is the same as
<code>"Density"</code>). Styles of plots are indicated in quotes. Optional 
styles include <code>"Covariates"</code>, <code>"Covariates, Categorical DV"</code>,
<code>"Density"</code>, <code>"DW"</code>, <code>"DW, Multivariate, C"</code>,
<code>"ECDF"</code>, <code>"Fitted"</code>, <code>"Fitted, Multivariate, C"</code>,
<code>"Fitted, Multivariate, R"</code>, <code>"Jarque-Bera"</code>,
<code>"Jarque-Bera, Multivariate, C"</code>,
<code>"Mardia"</code>, <code>"Predictive Quantiles"</code>,
<code>"Residual Density"</code>, <code>"Residual Density, Multivariate, C"</code>,
<code>"Residual Density, Multivariate, R"</code>,
<code>"Residuals"</code>, <code>"Residuals, Multivariate, C"</code>,
<code>"Residuals, Multivariate, R"</code>, <code>"Space-Time by Space"</code>,
<code>"Space-Time by Time"</code>, <code>"Spatial"</code>,
<code>"Spatial Uncertainty"</code>, <code>"Time-Series"</code>,
<code>"Time-Series, Multivariate, C"</code>, and
<code>"Time-Series, Multivariate, R"</code>. Details are given below.</p>
</td></tr>
<tr><td><code id="plot.iterquad.ppc_+3A_data">Data</code></td>
<td>

<p>This optional argument accepts the data set used when updating the
model. Data is required only with certain plot styles, including
<code>"Covariates"</code>, <code>"Covariates, Categorical DV"</code>,
<code>"DW, Multivariate, C"</code>, <code>"Fitted, Multivariate, C"</code>,
<code>"Fitted, Multivariate, R"</code>,
<code>"Jarque-Bera, Multivariate, C"</code>, <code>"Mardia"</code>,
<code>"Residual Density, Multivariate, C"</code>,
<code>"Residual Density, Multivariate, R"</code>,
<code>"Residuals, Multivariate, C"</code>,
<code>"Residuals, Multivariate, R"</code>,
<code>"Space-Time by Space"</code>, <code>"Space-Time by Time"</code>,
<code>"Spatial"</code>, <code>"Spatial Uncertainty"</code>,
<code>"Time-Series, Multivariate, C"</code>, and
<code>"Time-Series, Multivariate, R"</code>.</p>
</td></tr>
<tr><td><code id="plot.iterquad.ppc_+3A_rows">Rows</code></td>
<td>

<p>This optional argument is for a vector of row numbers that 
specify the records associated by row in the object of class
<code>iterquad.ppc</code>. Only these rows are plotted. The default is to
plot all rows. Some plots do not allow rows to be specified.</p>
</td></tr>
<tr><td><code id="plot.iterquad.ppc_+3A_pdf">PDF</code></td>
<td>

<p>This logical argument indicates whether or not the user wants
Laplace's Demon to save the plots as a .pdf file.</p>
</td></tr>
<tr><td><code id="plot.iterquad.ppc_+3A_...">...</code></td>
<td>
<p>Additional arguments are unused.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function can be used to produce a variety of posterior predictive
plots, and the style of plot is selected with the <code>Style</code>
argument. Below are some notes on the styles of plots.
</p>
<p><code>Covariates</code> requires <code>Data</code> to be specified, and also
requires that the covariates are named <code>X</code> or <code>x</code>. A plot
is produced for each covariate column vector against yhat, and is
appropriate when y is not categorical.
</p>
<p><code>Covariates, Categorical DV</code> requires <code>Data</code> to be
specified, and also requires that the covariates are named <code>X</code> or
<code>x</code>. A plot is produced for each covariate column vector against
yhat, and is appropriate when y is categorical.
</p>
<p><code>Density</code> plots show the kernel density of the posterior
predictive distribution for each selected row of y (all are selected
by default). A vertical red line indicates the position of the
observed y along the x-axis. When the vertical red line is close to
the middle of a normal posterior predictive distribution, then there
is little discrepancy between y and the posterior predictive
distribution. When the vertical red line is in the tail of the
distribution, or outside of the kernel density altogether, then
there is a large discrepancy between y and the posterior predictive
distribution. Large discrepancies may be considered outliers, and
moreover suggest that an improvement in model fit should be
considered.
</p>
<p><code>DW</code> plots the distributions of the Durbin-Watson (DW) test
statistics (Durbin and Watson, 1950), both observed
(<code class="reqn">d^{obs}</code> as a transparent, black density) and replicated
(<code class="reqn">d^{rep}</code> as a transparent, red density). The distribution
of <code class="reqn">d^{obs}</code> is estimated from the model, and
<code class="reqn">d^{rep}</code> is simulated from normal residuals without 
autocorrelation, where the number of simulations are the same as the
observed number. This DW test may be applied to the residuals of
univariate time-series models (or otherwise ordered residuals) to
detect first-order autocorrelation. Autocorrelated residuals are not
independent. The DW test is applicable only when the residuals are
normally-distributed, higher-order autocorrelation is not present, and
y is not used also as a lagged predictor. The DW test statistic,
<code class="reqn">d^{obs}</code>, occurs in the interval (0,4), where 0 is
perfect positive autocorrelation, 2 is no autocorrelation, and 4 is
perfect negative autocorrelation. The following summary is reported on
the plot: the mean of <code class="reqn">d^{obs}</code> (and its 95% probability
interval), the probability that <code class="reqn">d^{obs} &gt; d^{rep}</code>, and whether or not autocorrelation is found. Positive
autocorrelation is reported when the observed process is greater than
the replicated process in 2.5% of the samples, and negative
autocorrelation is reported when the observed process is greater than
the replicated process in 97.5% of the samples.
</p>
<p><code>DW, Multivariate, C</code> requires <code>Data</code> to be
specified, and also requires that variable <code>Y</code> exist in the data
set with exactly that name. These plots compare each column-wise
vector of residuals with a univariate Durbin-Watson test, as in
<code>DW</code> above. This plot is appropriate when Y is multivariate, not
categorical, and residuals are desired to be tested column-wise for
first-order autocorrelation.
</p>
<p><code>ECDF</code> (Empirical Cumulative Distribution Function) plots compare
the ECDF of y with three ECDFs of yhat based on the 2.5%, 50%
(median), and 97.5% of its distribution. The ECDF(y) is defined as
the proportion of values less than or equal to y. This plot is
appropriate when y is univariate and at least ordinal.
</p>
<p><code>Fitted</code> plots compare y with the probability interval of its
replicate, and provide loess smoothing. This plot is appropriate when
y is univariate and not categorical.
</p>
<p><code>Fitted, Multivariate, C</code> requires <code>Data</code> to be
specified, and also requires that variable <code>Y</code> exists in the data
set with exactly that name. These plots compare each column-wise
vector of y in Y with its replicates and provide loess smoothing.
This plot is appropriate when Y is multivariate, not categorical, and
desired to be seen column-wise.
</p>
<p><code>Fitted, Multivariate, R</code> requires <code>Data</code> to be
specified, and also requires that variable <code>Y</code> exists in the data
set with exactly that name. These plots compare each row-wise
vector of y in Y with its replicates and provide loess smoothing.
This plot is appropriate when Y is multivariate, not categorical, and
desired to be seen row-wise.
</p>
<p><code>Jarque-Bera</code> plots the distributions of the Jarque-Bera (JB)
test statistics (Jarque and Bera, 1980), both observed
(<code class="reqn">JB^{obs}</code> as a transparent black density) and replicated
(<code class="reqn">JB^{rep}</code> as a transparent red density). The
distribution of <code class="reqn">JB^{obs}</code> is estimated from the model,
and <code class="reqn">JB^{rep}</code> is simulated from normal residuals, where
the number of simulations are the same as the observed number. This
Jarque-Bera test may be applied to the residuals of 
univariate models to test for normality. The Jarque-Bera test does not
test normality per se, but whether or not the distribution has
kurtosis and skewness that match a normal distribution, and is
therefore a test of the moments of a normal distribution. The
following summary is reported on the plot: the mean of
<code class="reqn">JB^{obs}</code> (and its 95% probability interval), the
probability that <code class="reqn">JB^{obs} &gt; JB^{rep}</code>, and
whether or not normality is indicated. Non-normality is reported when
the observed process is greater than the replicated process in either
2.5% or 97.5% of the samples.
</p>
<p><code>Jarque-Bera, Multivariate, C</code> requires <code>Data</code> to be
specified, and also requires that variable <code>Y</code> exist in the data
set with exactly that name. These plots compare each column-wise
vector of residuals with a univariate Jarque-Bera test, as in
<code>Jarque-Bera</code> above. This plot is appropriate when Y is
multivariate, not categorical, and residuals are desired to be
tested column-wise for normality.
</p>
<p><code>Mardia</code> plots the distributions of the skewness (K3) and
kurtosis (K4) test statistics (Mardia, 1970), both observed
(<code class="reqn">K3^{obs}</code> and <code class="reqn">K4^{obs}</code> as transparent
black density) and replicated (<code class="reqn">K3^{rep}</code> and
<code class="reqn">K4^{rep}</code> as transparent red density). The distributions
of <code class="reqn">K3^{obs}</code> and <code class="reqn">K4^{obs}</code> are estimated
from the model, and both <code class="reqn">K3^{rep}</code> <code class="reqn">K4^{rep}</code>
are simulated from multivariate normal residuals, where the number of
simulations are the same as the observed number. This Mardia's test
may be applied to the residuals of multivariate models to test for
multivariate normality. Mardia's test does not test for multivariate
normality per se, but whether or not the distribution has kurtosis and
skewness that match a multivariate normal distribution, and is 
therefore a test of the moments of a multivariate normal
distribution. The following summary is reported on the plots: the
means of <code class="reqn">K3^{obs}</code> and <code class="reqn">K4^{obs}</code> (and
the associated 95% probability intervals), the probabilities that
<code class="reqn">K3^{obs} &gt; K3^{rep}</code> and
<code class="reqn">K4^{obs} &gt; K4^{rep}</code>, and whether or not
multivariate normality is indicated. Non-normality is reported when 
the observed process is greater than the replicated process in either
2.5% or 97.5% of the samples. <code>Mardia</code> requires <code>Data</code> to
be specified, and also requires that variable <code>Y</code> exist in the
data set with exactly that name. <code>Y</code> must be a <code class="reqn">N \times P</code> matrix of <code class="reqn">N</code> records and <code class="reqn">P</code> variables. Source
code was modified from the deprecated package QRMlib.
</p>
<p><code>Predictive Quantiles</code> plots compare y with the predictive
quantile (PQ) of its replicate. This may be useful in looking for
patterns with outliers. Instances outside of the gray lines are
considered outliers.
</p>
<p><code>Residual Density</code> plots the residual density of the median of
the samples. A vertical red line occurs at zero. This plot may be
useful for inspecting a distributional assumption of residual
variance. This plot is appropriate when y is univariate and
continuous.
</p>
<p><code>Residual Density, Multivariate C</code> requires <code>Data</code> to be
specified, and also requires that variable <code>Y</code> exist in the data
set with exactly that name. These are column-wise plots of residual
density, given the median of the samples. These plots may be useful
for inspecting a distributional assumption of residual variance.
This plot is appropriate when Y is multivariate, continuous, and
densities are desired to be seen column-wise.
</p>
<p><code>Residual Density, Multivariate R</code> requires <code>Data</code> to be
specified, and also requires that variable <code>Y</code> exist in the data
set with exactly that name. These are row-wise plots of residual
density, given the median of the samples. These plots may be useful
for inspecting a distributional assumption of residual variance.
This plot is appropriate when Y is multivariate, continuous, and
densities are desired to be seen row-wise.
</p>
<p><code>Residuals</code> plots compare y with its residuals. The probability
interval is plotted as a line. This plot is appropriate when y
is univariate.
</p>
<p><code>Residuals, Multivariate, C</code> requires <code>Data</code> to be
specified, and also requires that variable <code>Y</code> exist in the data
set with exactly that name. These are plots of each column-wise
vector of residuals. The probability interval is plotted as a
line. This plot is appropriate when Y is multivariate, not
categorical, and the residuals are desired to be seen column-wise.
</p>
<p><code>Residuals, Multivariate, R</code> requires <code>Data</code> to be
specified, and also requires that variable <code>Y</code> exist in the data
set with exactly that name. These are plots of each row-wise
vector of residuals. The probability interval is plotted as a
line. This plot is appropriate when Y is multivariate, not
categorical, and the residuals are desired to be seen row-wise.
</p>
<p><code>Space-Time by Space</code> requires <code>Data</code> to be specified, and
also requires that the following variables exist in the data set with
exactly these names: <code>latitude</code>, <code>longitude</code>, <code>S</code>, and
<code>T</code>. These space-time plots compare the S x T matrix Y with the S
x T matrix Yrep, producing one time-series plot per point s in space,
for a total of S plots. Therefore, these are time-series plots for
each point s in space across T time-periods. See <code>Time-Series</code>
plots below.
</p>
<p><code>Space-Time by Time</code> requires <code>Data</code> to be specified, and
also requires that the following variables exist in the data set with
exactly these names: <code>latitude</code>, <code>longitude</code>, <code>S</code>, and
<code>T</code>. These space-time plots compare the S x T matrix Y with the S
x T matrix Yrep, producing one spatial plot per time-period, and T
plots will be produced. See <code>Spatial</code> plots below.
</p>
<p><code>Spatial</code> requires <code>Data</code> to be specified, and also requires
that the following variables exist in the data set with exactly these
names: <code>latitude</code> and <code>longitude</code>. This spatial plot shows
yrep plotted according to its coordinates, and is color-coded so that
higher values of yrep become more red, and lower values become more
yellow.
</p>
<p><code>Spatial Uncertainty</code> requires <code>Data</code> to be specified, and
also requires that the following variables exist in the data set with
exactly these names: <code>latitude</code> and <code>longitude</code>. This
spatial plot shows the probability interval of yrep plotted according
to its coordinates, and is color-coded so that wider probability
intervals become more red, and lower values become more yellow.
</p>
<p><code>Time-Series</code> plots compare y with its replicate, including the
median and probability interval quantiles. This plot is appropriate
when y is univariate and ordered by time.
</p>
<p><code>Time-Series, Multivariate, C</code> requires <code>Data</code> to be
specified, and also requires that variable <code>Y</code> exist in the data
set with exactly that name. These plots compare each column-wise
time-series in Y with its replicate, including the median and
probability interval quantiles. This plot is appropriate when y is
multivariate and each time-series is indexed by column in Y.
</p>
<p><code>Time-Series, Multivariate, R</code> requires <code>Data</code> to be
specified, and also requires that variable <code>Y</code> exist in the data
set with exactly that name. These plots compare each row-wise
time-series in Y with its replicate, including the median and
probability interval quantiles. This plot is appropriate when y is
multivariate and each time-series is indexed by row in Y, such as is
typically true in panel models.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>References</h3>

<p>Durbin, J., and Watson, G.S. (1950). &quot;Testing for Serial Correlation
in Least Squares Regression, I.&quot; <em>Biometrika</em>, 37, p. 409&ndash;428.
</p>
<p>Jarque, C.M. and Bera, A.K. (1980). &quot;Efficient Tests for Normality,
Homoscedasticity and Serial Independence of Regression Residuals&quot;.
<em>Economics Letters</em>, 6(3), p. 255&ndash;259.
</p>
<p>Mardia, K.V. (1970). &quot;Measures of Multivariate Skewness and Kurtosis
with Applications&quot;. <em>Biometrika</em>, 57(3), p. 519&ndash;530.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code> and
<code><a href="#topic+predict.iterquad">predict.iterquad</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### See the IterativeQuadrature function for an example.</code></pre>

<hr>
<h2 id='plot.juxtapose'>Plot MCMC Juxtaposition</h2><span id='topic+plot.juxtapose'></span>

<h3>Description</h3>

<p>This may be used to plot a juxtaposition of MCMC algorithms according
either to <code><a href="#topic+IAT">IAT</a></code> or ISM (Independent Samples per Minute).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'juxtapose'
plot(x, Style="ISM", ...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.juxtapose_+3A_x">x</code></td>
<td>
<p>This required argument is an object of class
<code>juxtapose</code>.</p>
</td></tr>
<tr><td><code id="plot.juxtapose_+3A_style">Style</code></td>
<td>
<p>This argument accepts either <code>IAT</code> or <code>ISM</code>,
and defaults to <code>ISM</code>.</p>
</td></tr>
<tr><td><code id="plot.juxtapose_+3A_...">...</code></td>
<td>
<p>Additional arguments are unused.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When <code>Style="IAT"</code>, the medians and 95% probability intervals of
the integrated autocorrelation times (IATs) of MCMC algorithms are
displayed in a caterpillar plot. The best, or least inefficient, MCMC
algorithm is the algorithm with the lowest IAT.
</p>
<p>When <code>Style="ISM"</code>, the medians and 95% probability intervals of
the numbers of independent samples per minute (ISM) of MCMC algorithms
are displayed in a caterpillar plot. The best, or least inefficient,
MCMC algorithm is the algorithm with the highest ISM.
</p>
<p>For more information, see the <code><a href="#topic+Juxtapose">Juxtapose</a></code> function.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+Juxtapose">Juxtapose</a></code></p>

<hr>
<h2 id='plot.laplace'>Plot the output of <code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code></h2><span id='topic+plot.laplace'></span>

<h3>Description</h3>

<p>This may be used to plot, or save plots of, the iterated history of
the parameters and, if posterior samples were taken, density plots of
parameters and monitors in an object of class <code>laplace</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'laplace'
plot(x, Data, PDF=FALSE, Parms, ...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.laplace_+3A_x">x</code></td>
<td>

<p>This required argument is an object of class <code>laplace</code>.</p>
</td></tr>
<tr><td><code id="plot.laplace_+3A_data">Data</code></td>
<td>

<p>This required argument must receive the list of data that was
supplied to <code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code> to create the object
of class <code>laplace</code>.</p>
</td></tr>
<tr><td><code id="plot.laplace_+3A_pdf">PDF</code></td>
<td>

<p>This logical argument indicates whether or not the user wants
Laplace's Demon to save the plots as a .pdf file.</p>
</td></tr>
<tr><td><code id="plot.laplace_+3A_parms">Parms</code></td>
<td>

<p>This argument accepts a vector of quoted strings to be matched for
selecting parameters for plotting. This argument defaults to
<code>NULL</code> and selects every parameter for plotting. Each quoted
string is matched to one or more parameter names with the
<code>grep</code> function. For example, if the user specifies
<code>Parms=c("eta", "tau")</code>, and if the parameter names
are beta[1], beta[2], eta[1], eta[2], and tau, then all parameters
will be selected, because the string <code>eta</code> is within
<code>beta</code>. Since <code>grep</code> is used, string matching uses
regular expressions, so beware of meta-characters, though these are
acceptable: &quot;.&quot;, &quot;[&quot;, and &quot;]&quot;.</p>
</td></tr>
<tr><td><code id="plot.laplace_+3A_...">...</code></td>
<td>
<p>Additional arguments are unused.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The plots are arranged in a <code class="reqn">2 \times 2</code> matrix. The
purpose of the iterated history plots is to show how the value of each
parameter and the deviance changed by iteration as the
<code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code> attempted to maximize the logarithm
of the unnormalized joint posterior density. If the algorithm
converged, and if <code>sir=TRUE</code> in
<code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>, then plots are produced of
selected parameters and all monitored variables.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>### See the LaplaceApproximation function for an example.</code></pre>

<hr>
<h2 id='plot.laplace.ppc'>Plots of Posterior Predictive Checks</h2><span id='topic+plot.laplace.ppc'></span>

<h3>Description</h3>

<p>This may be used to plot, or save plots of, samples in an object of
class <code>laplace.ppc</code>. A variety of plots is provided.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'laplace.ppc'
plot(x, Style=NULL, Data=NULL,  Rows=NULL,
     PDF=FALSE, ...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.laplace.ppc_+3A_x">x</code></td>
<td>

<p>This required argument is an object of class <code>laplace.ppc</code>.</p>
</td></tr>
<tr><td><code id="plot.laplace.ppc_+3A_style">Style</code></td>
<td>

<p>This optional argument specifies one of several styles of plots, and
defaults to <code>NULL</code> (which is the same as
<code>"Density"</code>). Styles of plots are indicated in quotes. Optional 
styles include <code>"Covariates"</code>, <code>"Covariates, Categorical DV"</code>,
<code>"Density"</code>, <code>"DW"</code>, <code>"DW, Multivariate, C"</code>,
<code>"ECDF"</code>, <code>"Fitted"</code>, <code>"Fitted, Multivariate, C"</code>,
<code>"Fitted, Multivariate, R"</code>, <code>"Jarque-Bera"</code>,
<code>"Jarque-Bera, Multivariate, C"</code>,
<code>"Mardia"</code>, <code>"Predictive Quantiles"</code>,
<code>"Residual Density"</code>, <code>"Residual Density, Multivariate, C"</code>,
<code>"Residual Density, Multivariate, R"</code>,
<code>"Residuals"</code>, <code>"Residuals, Multivariate, C"</code>,
<code>"Residuals, Multivariate, R"</code>, <code>"Space-Time by Space"</code>,
<code>"Space-Time by Time"</code>, <code>"Spatial"</code>,
<code>"Spatial Uncertainty"</code>, <code>"Time-Series"</code>,
<code>"Time-Series, Multivariate, C"</code>, and
<code>"Time-Series, Multivariate, R"</code>. Details are given below.</p>
</td></tr>
<tr><td><code id="plot.laplace.ppc_+3A_data">Data</code></td>
<td>

<p>This optional argument accepts the data set used when updating the
model. Data is required only with certain plot styles, including
<code>"Covariates"</code>, <code>"Covariates, Categorical DV"</code>,
<code>"DW, Multivariate, C"</code>, <code>"Fitted, Multivariate, C"</code>,
<code>"Fitted, Multivariate, R"</code>,
<code>"Jarque-Bera, Multivariate, C"</code>, <code>"Mardia"</code>,
<code>"Residual Density, Multivariate, C"</code>,
<code>"Residual Density, Multivariate, R"</code>,
<code>"Residuals, Multivariate, C"</code>,
<code>"Residuals, Multivariate, R"</code>,
<code>"Space-Time by Space"</code>, <code>"Space-Time by Time"</code>,
<code>"Spatial"</code>, <code>"Spatial Uncertainty"</code>,
<code>"Time-Series, Multivariate, C"</code>, and
<code>"Time-Series, Multivariate, R"</code>.</p>
</td></tr>
<tr><td><code id="plot.laplace.ppc_+3A_rows">Rows</code></td>
<td>

<p>This optional argument is for a vector of row numbers that 
specify the records associated by row in the object of class
<code>laplace.ppc</code>. Only these rows are plotted. The default is to
plot all rows. Some plots do not allow rows to be specified.</p>
</td></tr>
<tr><td><code id="plot.laplace.ppc_+3A_pdf">PDF</code></td>
<td>

<p>This logical argument indicates whether or not the user wants
Laplace's Demon to save the plots as a .pdf file.</p>
</td></tr>
<tr><td><code id="plot.laplace.ppc_+3A_...">...</code></td>
<td>
<p>Additional arguments are unused.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function can be used to produce a variety of posterior predictive
plots, and the style of plot is selected with the <code>Style</code>
argument. Below are some notes on the styles of plots.
</p>
<p><code>Covariates</code> requires <code>Data</code> to be specified, and also
requires that the covariates are named <code>X</code> or <code>x</code>. A plot
is produced for each covariate column vector against yhat, and is
appropriate when y is not categorical.
</p>
<p><code>Covariates, Categorical DV</code> requires <code>Data</code> to be
specified, and also requires that the covariates are named <code>X</code> or
<code>x</code>. A plot is produced for each covariate column vector against
yhat, and is appropriate when y is categorical.
</p>
<p><code>Density</code> plots show the kernel density of the posterior
predictive distribution for each selected row of y (all are selected
by default). A vertical red line indicates the position of the
observed y along the x-axis. When the vertical red line is close to
the middle of a normal posterior predictive distribution, then there
is little discrepancy between y and the posterior predictive
distribution. When the vertical red line is in the tail of the
distribution, or outside of the kernel density altogether, then
there is a large discrepancy between y and the posterior predictive
distribution. Large discrepancies may be considered outliers, and
moreover suggest that an improvement in model fit should be
considered.
</p>
<p><code>DW</code> plots the distributions of the Durbin-Watson (DW) test
statistics (Durbin and Watson, 1950), both observed
(<code class="reqn">d^{obs}</code> as a transparent, black density) and replicated
(<code class="reqn">d^{rep}</code> as a transparent, red density). The distribution
of <code class="reqn">d^{obs}</code> is estimated from the model, and
<code class="reqn">d^{rep}</code> is simulated from normal residuals without 
autocorrelation, where the number of simulations are the same as the
observed number. This DW test may be applied to the residuals of
univariate time-series models (or otherwise ordered residuals) to
detect first-order autocorrelation. Autocorrelated residuals are not
independent. The DW test is applicable only when the residuals are
normally-distributed, higher-order autocorrelation is not present, and
y is not used also as a lagged predictor. The DW test statistic,
<code class="reqn">d^{obs}</code>, occurs in the interval (0,4), where 0 is
perfect positive autocorrelation, 2 is no autocorrelation, and 4 is
perfect negative autocorrelation. The following summary is reported on
the plot: the mean of <code class="reqn">d^{obs}</code> (and its 95% probability
interval), the probability that <code class="reqn">d^{obs} &gt; d^{rep}</code>, and whether or not autocorrelation is found. Positive
autocorrelation is reported when the observed process is greater than
the replicated process in 2.5% of the samples, and negative
autocorrelation is reported when the observed process is greater than
the replicated process in 97.5% of the samples.
</p>
<p><code>DW, Multivariate, C</code> requires <code>Data</code> to be
specified, and also requires that variable <code>Y</code> exist in the data
set with exactly that name. These plots compare each column-wise
vector of residuals with a univariate Durbin-Watson test, as in
<code>DW</code> above. This plot is appropriate when Y is multivariate, not
categorical, and residuals are desired to be tested column-wise for
first-order autocorrelation.
</p>
<p><code>ECDF</code> (Empirical Cumulative Distribution Function) plots compare
the ECDF of y with three ECDFs of yhat based on the 2.5%, 50%
(median), and 97.5% of its distribution. The ECDF(y) is defined as
the proportion of values less than or equal to y. This plot is
appropriate when y is univariate and at least ordinal.
</p>
<p><code>Fitted</code> plots compare y with the probability interval of its
replicate, and provide loess smoothing. This plot is appropriate when
y is univariate and not categorical.
</p>
<p><code>Fitted, Multivariate, C</code> requires <code>Data</code> to be
specified, and also requires that variable <code>Y</code> exists in the data
set with exactly that name. These plots compare each column-wise
vector of y in Y with its replicates and provide loess smoothing.
This plot is appropriate when Y is multivariate, not categorical, and
desired to be seen column-wise.
</p>
<p><code>Fitted, Multivariate, R</code> requires <code>Data</code> to be
specified, and also requires that variable <code>Y</code> exists in the data
set with exactly that name. These plots compare each row-wise
vector of y in Y with its replicates and provide loess smoothing.
This plot is appropriate when Y is multivariate, not categorical, and
desired to be seen row-wise.
</p>
<p><code>Jarque-Bera</code> plots the distributions of the Jarque-Bera (JB)
test statistics (Jarque and Bera, 1980), both observed
(<code class="reqn">JB^{obs}</code> as a transparent black density) and replicated
(<code class="reqn">JB^{rep}</code> as a transparent red density). The
distribution of <code class="reqn">JB^{obs}</code> is estimated from the model,
and <code class="reqn">JB^{rep}</code> is simulated from normal residuals, where
the number of simulations are the same as the observed number. This
Jarque-Bera test may be applied to the residuals of 
univariate models to test for normality. The Jarque-Bera test does not
test normality per se, but whether or not the distribution has
kurtosis and skewness that match a normal distribution, and is
therefore a test of the moments of a normal distribution. The
following summary is reported on the plot: the mean of
<code class="reqn">JB^{obs}</code> (and its 95% probability interval), the
probability that <code class="reqn">JB^{obs} &gt; JB^{rep}</code>, and
whether or not normality is indicated. Non-normality is reported when
the observed process is greater than the replicated process in either
2.5% or 97.5% of the samples.
</p>
<p><code>Jarque-Bera, Multivariate, C</code> requires <code>Data</code> to be
specified, and also requires that variable <code>Y</code> exist in the data
set with exactly that name. These plots compare each column-wise
vector of residuals with a univariate Jarque-Bera test, as in
<code>Jarque-Bera</code> above. This plot is appropriate when Y is
multivariate, not categorical, and residuals are desired to be
tested column-wise for normality.
</p>
<p><code>Mardia</code> plots the distributions of the skewness (K3) and
kurtosis (K4) test statistics (Mardia, 1970), both observed
(<code class="reqn">K3^{obs}</code> and <code class="reqn">K4^{obs}</code> as transparent
black density) and replicated (<code class="reqn">K3^{rep}</code> and
<code class="reqn">K4^{rep}</code> as transparent red density). The distributions
of <code class="reqn">K3^{obs}</code> and <code class="reqn">K4^{obs}</code> are estimated
from the model, and both <code class="reqn">K3^{rep}</code> <code class="reqn">K4^{rep}</code>
are simulated from multivariate normal residuals, where the number of
simulations are the same as the observed number. This Mardia's test
may be applied to the residuals of multivariate models to test for
multivariate normality. Mardia's test does not test for multivariate
normality per se, but whether or not the distribution has kurtosis and
skewness that match a multivariate normal distribution, and is 
therefore a test of the moments of a multivariate normal
distribution. The following summary is reported on the plots: the
means of <code class="reqn">K3^{obs}</code> and <code class="reqn">K4^{obs}</code> (and
the associated 95% probability intervals), the probabilities that
<code class="reqn">K3^{obs} &gt; K3^{rep}</code> and
<code class="reqn">K4^{obs} &gt; K4^{rep}</code>, and whether or not
multivariate normality is indicated. Non-normality is reported when 
the observed process is greater than the replicated process in either
2.5% or 97.5% of the samples. <code>Mardia</code> requires <code>Data</code> to
be specified, and also requires that variable <code>Y</code> exist in the
data set with exactly that name. <code>Y</code> must be a <code class="reqn">N \times P</code> matrix of <code class="reqn">N</code> records and <code class="reqn">P</code> variables. Source
code was modified from the deprecated package QRMlib.
</p>
<p><code>Predictive Quantiles</code> plots compare y with the predictive
quantile (PQ) of its replicate. This may be useful in looking for
patterns with outliers. Instances outside of the gray lines are
considered outliers.
</p>
<p><code>Residual Density</code> plots the residual density of the median of
the samples. A vertical red line occurs at zero. This plot may be
useful for inspecting a distributional assumption of residual
variance. This plot is appropriate when y is univariate and
continuous.
</p>
<p><code>Residual Density, Multivariate C</code> requires <code>Data</code> to be
specified, and also requires that variable <code>Y</code> exist in the data
set with exactly that name. These are column-wise plots of residual
density, given the median of the samples. These plots may be useful
for inspecting a distributional assumption of residual variance.
This plot is appropriate when Y is multivariate, continuous, and
densities are desired to be seen column-wise.
</p>
<p><code>Residual Density, Multivariate R</code> requires <code>Data</code> to be
specified, and also requires that variable <code>Y</code> exist in the data
set with exactly that name. These are row-wise plots of residual
density, given the median of the samples. These plots may be useful
for inspecting a distributional assumption of residual variance.
This plot is appropriate when Y is multivariate, continuous, and
densities are desired to be seen row-wise.
</p>
<p><code>Residuals</code> plots compare y with its residuals. The probability
interval is plotted as a line. This plot is appropriate when y
is univariate.
</p>
<p><code>Residuals, Multivariate, C</code> requires <code>Data</code> to be
specified, and also requires that variable <code>Y</code> exist in the data
set with exactly that name. These are plots of each column-wise
vector of residuals. The probability interval is plotted as a
line. This plot is appropriate when Y is multivariate, not
categorical, and the residuals are desired to be seen column-wise.
</p>
<p><code>Residuals, Multivariate, R</code> requires <code>Data</code> to be
specified, and also requires that variable <code>Y</code> exist in the data
set with exactly that name. These are plots of each row-wise
vector of residuals. The probability interval is plotted as a
line. This plot is appropriate when Y is multivariate, not
categorical, and the residuals are desired to be seen row-wise.
</p>
<p><code>Space-Time by Space</code> requires <code>Data</code> to be specified, and
also requires that the following variables exist in the data set with
exactly these names: <code>latitude</code>, <code>longitude</code>, <code>S</code>, and
<code>T</code>. These space-time plots compare the S x T matrix Y with the S
x T matrix Yrep, producing one time-series plot per point s in space,
for a total of S plots. Therefore, these are time-series plots for
each point s in space across T time-periods. See <code>Time-Series</code>
plots below.
</p>
<p><code>Space-Time by Time</code> requires <code>Data</code> to be specified, and
also requires that the following variables exist in the data set with
exactly these names: <code>latitude</code>, <code>longitude</code>, <code>S</code>, and
<code>T</code>. These space-time plots compare the S x T matrix Y with the S
x T matrix Yrep, producing one spatial plot per time-period, and T
plots will be produced. See <code>Spatial</code> plots below.
</p>
<p><code>Spatial</code> requires <code>Data</code> to be specified, and also requires
that the following variables exist in the data set with exactly these
names: <code>latitude</code> and <code>longitude</code>. This spatial plot shows
yrep plotted according to its coordinates, and is color-coded so that
higher values of yrep become more red, and lower values become more
yellow.
</p>
<p><code>Spatial Uncertainty</code> requires <code>Data</code> to be specified, and
also requires that the following variables exist in the data set with
exactly these names: <code>latitude</code> and <code>longitude</code>. This
spatial plot shows the probability interval of yrep plotted according
to its coordinates, and is color-coded so that wider probability
intervals become more red, and lower values become more yellow.
</p>
<p><code>Time-Series</code> plots compare y with its replicate, including the
median and probability interval quantiles. This plot is appropriate
when y is univariate and ordered by time.
</p>
<p><code>Time-Series, Multivariate, C</code> requires <code>Data</code> to be
specified, and also requires that variable <code>Y</code> exist in the data
set with exactly that name. These plots compare each column-wise
time-series in Y with its replicate, including the median and
probability interval quantiles. This plot is appropriate when y is
multivariate and each time-series is indexed by column in Y.
</p>
<p><code>Time-Series, Multivariate, R</code> requires <code>Data</code> to be
specified, and also requires that variable <code>Y</code> exist in the data
set with exactly that name. These plots compare each row-wise
time-series in Y with its replicate, including the median and
probability interval quantiles. This plot is appropriate when y is
multivariate and each time-series is indexed by row in Y, such as is
typically true in panel models.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>References</h3>

<p>Durbin, J., and Watson, G.S. (1950). &quot;Testing for Serial Correlation
in Least Squares Regression, I.&quot; <em>Biometrika</em>, 37, p. 409&ndash;428.
</p>
<p>Jarque, C.M. and Bera, A.K. (1980). &quot;Efficient Tests for Normality,
Homoscedasticity and Serial Independence of Regression Residuals&quot;.
<em>Economics Letters</em>, 6(3), p. 255&ndash;259.
</p>
<p>Mardia, K.V. (1970). &quot;Measures of Multivariate Skewness and Kurtosis
with Applications&quot;. <em>Biometrika</em>, 57(3), p. 519&ndash;530.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code> and
<code><a href="#topic+predict.laplace">predict.laplace</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### See the LaplaceApproximation function for an example.</code></pre>

<hr>
<h2 id='plot.miss'>Plot samples from the output of MISS</h2><span id='topic+plot.miss'></span>

<h3>Description</h3>

<p>This may be used to plot, or save plots of, samples in an object of
class <code>miss</code>. Plots include a trace plot, density plot, and
autocorrelation or ACF plot.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'miss'
plot(x, PDF=FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.miss_+3A_x">x</code></td>
<td>

<p>This required argument is an object of class <code>miss</code>.</p>
</td></tr>
<tr><td><code id="plot.miss_+3A_pdf">PDF</code></td>
<td>

<p>This logical argument indicates whether or not the user wants
Laplace's Demon to save the plots as a .pdf file.</p>
</td></tr>
<tr><td><code id="plot.miss_+3A_...">...</code></td>
<td>
<p>Additional arguments are unused.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The plots are arranged in a <code class="reqn">3 \times 3</code> matrix. Each row
represents the predictive distribution of a missing value. The
left column displays trace plots, the middle column displays kernel
density plots, and the right column displays autocorrelation (ACF)
plots.
</p>
<p>Trace plots show the thinned history of the predictive distribution,
with its value in the y-axis moving by iteration across the x-axis.
Simulations of a predictive distribution with good properties do not
suggest a trend upward or downward as it progresses across the x-axis
(it should appear stationary), and it should mix well, meaning it
should appear as though random samples are being taken each time from
the same target distribution. Visual inspection of a trace plot cannot
verify convergence, but apparent non-stationarity or poor mixing can
certainly suggest non-convergence. A red, smoothed line also appears
to aid visual inspection.
</p>
<p>Kernel density plots depict the marginal posterior distribution.
There is no distributional assumption about this density.
</p>
<p>Autocorrelation plots show the autocorrelation or serial correlation
between sampled values at nearby iterations. Samples with
autocorrelation do not violate any assumption, but are inefficient
because they reduce the effective sample size (<code><a href="#topic+ESS">ESS</a></code>), and
indicate that the chain is not mixing well, since each value is
influenced by values that are previous and nearby. The x-axis
indicates lags with respect to samples by iteration, and the y-axis
represents autocorrelation. The ideal autocorrelation plot shows
perfect correlation at zero lag, and quickly falls to zero
autocorrelation for all other lags.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+MISS">MISS</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'>### See the MISS function for an example.</code></pre>

<hr>
<h2 id='plot.pmc'>Plot samples from the output of PMC</h2><span id='topic+plot.pmc'></span>

<h3>Description</h3>

<p>This may be used to plot, or save plots of, samples in an object of
class <code>pmc</code>. Plots include a trace plot and density plot for
parameters, a density plot for deviance and monitored variables, and
convergence plots.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'pmc'
plot(x, BurnIn=0, Data, PDF=FALSE, Parms, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.pmc_+3A_x">x</code></td>
<td>
<p>This required argument is an object of class <code>pmc</code>.</p>
</td></tr>
<tr><td><code id="plot.pmc_+3A_burnin">BurnIn</code></td>
<td>
<p>This argument requires zero or a positive integer that
indicates the number of iterations to discard as burn-in for the
purposes of plotting.</p>
</td></tr>
<tr><td><code id="plot.pmc_+3A_data">Data</code></td>
<td>
<p>This required argument must receive the list of data that
was supplied to <code><a href="#topic+PMC">PMC</a></code> to create the object of class
<code>pmc</code>.</p>
</td></tr>
<tr><td><code id="plot.pmc_+3A_pdf">PDF</code></td>
<td>
<p>This logical argument indicates whether or not the user
wants Laplace's Demon to save the plots as a .pdf file.</p>
</td></tr>
<tr><td><code id="plot.pmc_+3A_parms">Parms</code></td>
<td>
<p>This argument accepts a vector of quoted strings to be
matched for selecting parameters for plotting. This argument
defaults to <code>NULL</code> and selects every parameter for
plotting. Each quoted string is matched to one or more parameter
names with the <code>grep</code> function. For example, if the user
specifies <code>Parms=c("eta", "tau")</code>, and if the parameter names
are beta[1], beta[2], eta[1], eta[2], and tau, then all parameters
will be selected, because the string <code>eta</code> is within
<code>beta</code>. Since <code>grep</code> is used, string matching uses
regular expressions, so beware of meta-characters, though these are
acceptable: &quot;.&quot;, &quot;[&quot;, and &quot;]&quot;.</p>
</td></tr>
<tr><td><code id="plot.pmc_+3A_...">...</code></td>
<td>
<p>Additional arguments are unused.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The plots are arranged in a <code class="reqn">2 \times 2</code> matrix. Each row
represents a parameter, the deviance, or a monitored variable. For
parameters, the left column displays trace plots and the right column
displays kernel density plots.
</p>
<p>Trace plots show the history of the distribution of independent
importance samples. When multiple mixture components are used, each
mixture component has a different color. These plots are unavailable
for the deviance and monitored variables.
</p>
<p>Kernel density plots depict the marginal posterior distribution.
Although there is no distributional assumption about this density,
kernel density estimation uses Gaussian basis functions.
</p>
<p>Following these plots are three plots for convergence. First, ESSN
(red) and perplexity (black) are plotted by iteration. Convergence
occurs when both of these seem to stabilize, and higher is
better. The second plot shows the distribution of the normalized
importance weights by iteration. The third plot appears only when
multiple mixture components are used. The third plot displays the
probabilities of each mixture component by iteration. Although the
last two plots are not formally convergence plots, they are provided
so the user can verify the distribution of importance weights and the
mixture probabilities have become stable.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+ESS">ESS</a></code> and
<code><a href="#topic+PMC">PMC</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'>### See the PMC function for an example.</code></pre>

<hr>
<h2 id='plot.pmc.ppc'>Plots of Posterior Predictive Checks</h2><span id='topic+plot.pmc.ppc'></span>

<h3>Description</h3>

<p>This may be used to plot, or save plots of, samples in an object of
class <code>pmc.ppc</code>. A variety of plots is provided.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'pmc.ppc'
plot(x, Style=NULL, Data=NULL, Rows=NULL,
     PDF=FALSE, ...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.pmc.ppc_+3A_x">x</code></td>
<td>

<p>This required argument is an object of class <code>pmc.ppc</code>.</p>
</td></tr>
<tr><td><code id="plot.pmc.ppc_+3A_style">Style</code></td>
<td>

<p>This optional argument specifies one of several styles of plots,
and defaults to <code>NULL</code> (which is the same as
<code>"Density"</code>). Styles of plots are indicated in quotes. Optional
styles include <code>"Covariates"</code>, <code>"Covariates, Categorical DV"</code>,
<code>"Density"</code>, <code>"DW"</code>, <code>"DW, Multivariate, C"</code>,
<code>"ECDF"</code>, <code>"Fitted"</code>, <code>"Fitted, Multivariate, C"</code>,
<code>"Fitted, Multivariate, R"</code>, <code>"Jarque-Bera"</code>,
<code>"Jarque-Bera, Multivariate, C"</code>,
<code>"Mardia"</code>, <code>"Predictive Quantiles"</code>,
<code>"Residual Density"</code>, <code>"Residual Density, Multivariate, C"</code>,
<code>"Residual Density, Multivariate, R"</code>,
<code>"Residuals"</code>, <code>"Residuals, Multivariate, C"</code>,
<code>"Residuals, Multivariate, R"</code>, <code>"Space-Time by Space"</code>,
<code>"Space-Time by Time"</code>, <code>"Spatial"</code>,
<code>"Spatial Uncertainty"</code>, <code>"Time-Series"</code>,
<code>"Time-Series, Multivariate, C"</code>, and
<code>"Time-Series, Multivariate, R"</code>. Details are given below.</p>
</td></tr>
<tr><td><code id="plot.pmc.ppc_+3A_data">Data</code></td>
<td>

<p>This optional argument accepts the data set used when updating the
model. Data is required only with certain plot styles, including
<code>"Covariates"</code>, <code>"Covariates, Categorical DV"</code>,
<code>"DW, Multivariate, C"</code>, <code>"Fitted, Multivariate, C"</code>,
<code>"Fitted, Multivariate, R"</code>,
<code>"Jarque-Bera, Multivariate, C"</code>, <code>"Mardia"</code>,
<code>"Residual Density, Multivariate, C"</code>,
<code>"Residual Density, Multivariate, R"</code>,
<code>"Residuals, Multivariate, C"</code>,
<code>"Residuals, Multivariate, R"</code>,
<code>"Space-Time by Space"</code>, <code>"Space-Time by Time"</code>,
<code>"Spatial"</code>, <code>"Spatial Uncertainty"</code>,
<code>"Time-Series, Multivariate, C"</code>, and
<code>"Time-Series, Multivariate, R"</code>.</p>
</td></tr>
<tr><td><code id="plot.pmc.ppc_+3A_rows">Rows</code></td>
<td>

<p>This optional argument is for a vector of row numbers that 
specify the records associated by row in the object of class
<code>pmc.ppc</code>. Only these rows are plotted. The default is to
plot all rows. Some plots do not allow rows to be specified.</p>
</td></tr>
<tr><td><code id="plot.pmc.ppc_+3A_pdf">PDF</code></td>
<td>

<p>This logical argument indicates whether or not the user wants
Laplace's Demon to save the plots as a .pdf file.</p>
</td></tr>
<tr><td><code id="plot.pmc.ppc_+3A_...">...</code></td>
<td>
<p>Additional arguments are unused.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function can be used to produce a variety of posterior predictive
plots, and the style of plot is selected with the <code>Style</code>
argument. Below are some notes on the styles of plots.
</p>
<p><code>Covariates</code> requires <code>Data</code> to be specified, and also
requires that the covariates are named <code>X</code> or <code>x</code>. A plot
is produced for each covariate column vector against yhat, and is
appropriate when y is not categorical.
</p>
<p><code>Covariates, Categorical DV</code> requires <code>Data</code> to be
specified, and also requires that the covariates are named <code>X</code> or
<code>x</code>. A plot is produced for each covariate column vector against
yhat, and is appropriate when y is categorical.
</p>
<p><code>Density</code> plots show the kernel density of the posterior
predictive distribution for each selected row of y (all are selected
by default). A vertical red line indicates the position of the
observed y along the x-axis. When the vertical red line is close to
the middle of a normal posterior predictive distribution, then there
is little discrepancy between y and the posterior predictive
distribution. When the vertical red line is in the tail of the
distribution, or outside of the kernel density altogether, then
there is a large discrepancy between y and the posterior predictive
distribution. Large discrepancies may be considered outliers, and
moreover suggest that an improvement in model fit should be
considered.
</p>
<p><code>DW</code> plots the distributions of the Durbin-Watson (DW) test
statistics (Durbin and Watson, 1950), both observed
(<code class="reqn">d^{obs}</code> as a transparent, black density) and replicated
(<code class="reqn">d^{rep}</code> as a transparent, red density). The distribution
of <code class="reqn">d^{obs}</code> is estimated from the model, and
<code class="reqn">d^{rep}</code> is simulated from normal residuals without
autocorrelation, where the number of simulations are the same as the
observed number. This DW test may be applied to the residuals of
univariate time-series models (or otherwise ordered residuals) to
detect first-order autocorrelation. Autocorrelated residuals are not
independent. The DW test is applicable only when the residuals are
normally-distributed, higher-order autocorrelation is not present, and
y is not used also as a lagged predictor. The DW test statistic,
<code class="reqn">d^{obs}</code>, occurs in the interval (0,4), where 0 is
perfect positive autocorrelation, 2 is no autocorrelation, and 4 is
perfect negative autocorrelation. The following summary is reported on
the plot: the mean of <code class="reqn">d^{obs}</code> (and its 95% probability
interval), the probability that <code class="reqn">d^{obs} &gt; d^{rep}</code>, and whether or not autocorrelation is found. Positive
autocorrelation is reported when the observed process is greater than
the replicated process in 2.5% of the samples, and negative
autocorrelation is reported when the observed process is greater than
the replicated process in 97.5% of the samples.
</p>
<p><code>DW, Multivariate, C</code> requires <code>Data</code> to be
specified, and also requires that variable <code>Y</code> exist in the data
set with exactly that name. These plots compare each column-wise
vector of residuals with a univariate Durbin-Watson test, as in
<code>DW</code> above. This plot is appropriate when Y is multivariate, not
categorical, and residuals are desired to be tested column-wise for
first-order autocorrelation.
</p>
<p><code>ECDF</code> (Empirical Cumulative Distribution Function) plots compare
the ECDF of y with three ECDFs of yhat based on the 2.5%, 50%
(median), and 97.5% of its distribution. The ECDF(y) is defined as
the proportion of values less than or equal to y. This plot is
appropriate when y is univariate and at least ordinal.
</p>
<p><code>Fitted</code> plots compare y with the probability interval of its
replicate, and provide loess smoothing. This plot is appropriate when
y is univariate and not categorical.
</p>
<p><code>Fitted, Multivariate, C</code> requires <code>Data</code> to be
specified, and also requires that variable <code>Y</code> exists in the data
set with exactly that name. These plots compare each column-wise
vector of y in Y with its replicates and provide loess smoothing.
This plot is appropriate when Y is multivariate, not categorical, and
desired to be seen column-wise.
</p>
<p><code>Fitted, Multivariate, R</code> requires <code>Data</code> to be
specified, and also requires that variable <code>Y</code> exists in the data
set with exactly that name. These plots compare each row-wise
vector of y in Y with its replicates and provide loess smoothing.
This plot is appropriate when Y is multivariate, not categorical, and
desired to be seen row-wise.
</p>
<p><code>Jarque-Bera</code> plots the distributions of the Jarque-Bera (JB)
test statistics (Jarque and Bera, 1980), both observed
(<code class="reqn">JB^{obs}</code> as a transparent black density) and replicated
(<code class="reqn">JB^{rep}</code> as a transparent red density). The
distribution of <code class="reqn">JB^{obs}</code> is estimated from the model,
and <code class="reqn">JB^{rep}</code> is simulated from normal residuals, where
the number of simulations are the same as the observed number. This
Jarque-Bera test may be applied to the residuals of 
univariate models to test for normality. The Jarque-Bera test does not
test normality per se, but whether or not the distribution has
kurtosis and skewness that match a normal distribution, and is
therefore a test of the moments of a normal distribution. The
following summary is reported on the plot: the mean of
<code class="reqn">JB^{obs}</code> (and its 95% probability interval), the
probability that <code class="reqn">JB^{obs} &gt; JB^{rep}</code>, and
whether or not normality is indicated. Non-normality is reported when
the observed process is greater than the replicated process in either
2.5% or 97.5% of the samples.
</p>
<p><code>Jarque-Bera, Multivariate, C</code> requires <code>Data</code> to be
specified, and also requires that variable <code>Y</code> exist in the data
set with exactly that name. These plots compare each column-wise
vector of residuals with a univariate Jarque-Bera test, as in
<code>Jarque-Bera</code> above. This plot is appropriate when Y is
multivariate, not categorical, and residuals are desired to be
tested column-wise for normality.
</p>
<p><code>Mardia</code> plots the distributions of the skewness (K3) and
kurtosis (K4) test statistics (Mardia, 1970), both observed
(<code class="reqn">K3^{obs}</code> and <code class="reqn">K4^{obs}</code> as transparent
black density) and replicated (<code class="reqn">K3^{rep}</code> and
<code class="reqn">K4^{rep}</code> as transparent red density). The distributions
of <code class="reqn">K3^{obs}</code> and <code class="reqn">K4^{obs}</code> are estimated
from the model, and both <code class="reqn">K3^{rep}</code> <code class="reqn">K4^{rep}</code>
are simulated from multivariate normal residuals, where the number of
simulations are the same as the observed number. This Mardia's test
may be applied to the residuals of multivariate models to test for
multivariate normality. Mardia's test does not test for multivariate
normality per se, but whether or not the distribution has kurtosis and
skewness that match a multivariate normal distribution, and is 
therefore a test of the moments of a multivariate normal
distribution. The following summary is reported on the plots: the
means of <code class="reqn">K3^{obs}</code> and <code class="reqn">K4^{obs}</code> (and
the associated 95% probability intervals), the probabilities that
<code class="reqn">K3^{obs} &gt; K3^{rep}</code> and
<code class="reqn">K4^{obs} &gt; K4^{rep}</code>, and whether or not
multivariate normality is indicated. Non-normality is reported when 
the observed process is greater than the replicated process in either
2.5% or 97.5% of the samples. <code>Mardia</code> requires <code>Data</code> to
be specified, and also requires that variable <code>Y</code> exist in the
data set with exactly that name. <code>Y</code> must be a <code class="reqn">N \times P</code> matrix of <code class="reqn">N</code> records and <code class="reqn">P</code> variables. Source
code was modified from the deprecated package QRMlib.
</p>
<p><code>Predictive Quantiles</code> plots compare y with the predictive
quantile (PQ) of its replicate. This may be useful in looking for
patterns with outliers. Instances outside of the gray lines are
considered outliers.
</p>
<p><code>Residual Density</code> plots the residual density of the median of
the samples. A vertical red line occurs at zero. This plot may be
useful for inspecting a distributional assumption of residual
variance. This plot is appropriate when y is univariate and
continuous.
</p>
<p><code>Residual Density, Multivariate C</code> requires <code>Data</code> to be
specified, and also requires that variable <code>Y</code> exist in the data
set with exactly that name. These are column-wise plots of residual
density, given the median of the samples. These plots may be useful
for inspecting a distributional assumption of residual variance.
This plot is appropriate when Y is multivariate, continuous, and
densities are desired to be seen column-wise.
</p>
<p><code>Residual Density, Multivariate R</code> requires <code>Data</code> to be
specified, and also requires that variable <code>Y</code> exist in the data
set with exactly that name. These are row-wise plots of residual
density, given the median of the samples. These plots may be useful
for inspecting a distributional assumption of residual variance.
This plot is appropriate when Y is multivariate, continuous, and
densities are desired to be seen row-wise.
</p>
<p><code>Residuals</code> plots compare y with its residuals. The probability
interval is plotted as a line. This plot is appropriate when y
is univariate.
</p>
<p><code>Residuals, Multivariate, C</code> requires <code>Data</code> to be
specified, and also requires that variable <code>Y</code> exist in the data
set with exactly that name. These are plots of each column-wise
vector of residuals. The probability interval is plotted as a
line. This plot is appropriate when Y is multivariate, not
categorical, and the residuals are desired to be seen column-wise.
</p>
<p><code>Residuals, Multivariate, R</code> requires <code>Data</code> to be
specified, and also requires that variable <code>Y</code> exist in the data
set with exactly that name. These are plots of each row-wise
vector of residuals. The probability interval is plotted as a
line. This plot is appropriate when Y is multivariate, not
categorical, and the residuals are desired to be seen row-wise.
</p>
<p><code>Space-Time by Space</code> requires <code>Data</code> to be specified, and
also requires that the following variables exist in the data set with
exactly these names: <code>latitude</code>, <code>longitude</code>, <code>S</code>, and
<code>T</code>. These space-time plots compare the S x T matrix Y with the S
x T matrix Yrep, producing one time-series plot per point s in space,
for a total of S plots. Therefore, these are time-series plots for
each point s in space across T time-periods. See <code>Time-Series</code>
plots below.
</p>
<p><code>Space-Time by Time</code> requires <code>Data</code> to be specified, and
also requires that the following variables exist in the data set with
exactly these names: <code>latitude</code>, <code>longitude</code>, <code>S</code>, and
<code>T</code>. These space-time plots compare the S x T matrix Y with the S
x T matrix Yrep, producing one spatial plot per time-period, and T
plots will be produced. See <code>Spatial</code> plots below.
</p>
<p><code>Spatial</code> requires <code>Data</code> to be specified, and also requires
that the following variables exist in the data set with exactly these
names: <code>latitude</code> and <code>longitude</code>. This spatial plot shows
yrep plotted according to its coordinates, and is color-coded so that
higher values of yrep become more red, and lower values become more
yellow.
</p>
<p><code>Spatial Uncertainty</code> requires <code>Data</code> to be specified, and
also requires that the following variables exist in the data set with
exactly these names: <code>latitude</code> and <code>longitude</code>. This
spatial plot shows the probability interval of yrep plotted according
to its coordinates, and is color-coded so that wider probability
intervals become more red, and lower values become more yellow.
</p>
<p><code>Time-Series</code> plots compare y with its replicate, including the
median and probability interval quantiles. This plot is appropriate
when y is univariate and ordered by time.
</p>
<p><code>Time-Series, Multivariate, C</code> requires <code>Data</code> to be
specified, and also requires that variable <code>Y</code> exist in the data
set with exactly that name. These plots compare each column-wise
time-series in Y with its replicate, including the median and
probability interval quantiles. This plot is appropriate when y is
multivariate and each time-series is indexed by column in Y.
</p>
<p><code>Time-Series, Multivariate, R</code> requires <code>Data</code> to be
specified, and also requires that variable <code>Y</code> exist in the data
set with exactly that name. These plots compare each row-wise
time-series in Y with its replicate, including the median and
probability interval quantiles. This plot is appropriate when y is
multivariate and each time-series is indexed by row in Y, such as is
typically true in panel models.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>References</h3>

<p>Durbin, J., and Watson, G.S. (1950). &quot;Testing for Serial Correlation
in Least Squares Regression, I.&quot; <em>Biometrika</em>, 37, p. 409&ndash;428.
</p>
<p>Jarque, C.M. and Bera, A.K. (1980). &quot;Efficient Tests for Normality,
Homoscedasticity and Serial Independence of Regression Residuals&quot;.
<em>Economics Letters</em>, 6(3), p. 255&ndash;259.
</p>
<p>Mardia, K.V. (1970). &quot;Measures of Multivariate Skewness and Kurtosis
with Applications&quot;. <em>Biometrika</em>, 57(3), p. 519&ndash;530.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+PMC">PMC</a></code> and
<code><a href="#topic+predict.pmc">predict.pmc</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### See the PMC function for an example.</code></pre>

<hr>
<h2 id='plot.vb'>Plot the output of <code><a href="#topic+VariationalBayes">VariationalBayes</a></code></h2><span id='topic+plot.vb'></span>

<h3>Description</h3>

<p>This may be used to plot, or save plots of, the iterated history of
the parameters and variances, and if posterior samples were taken,
density plots of parameters and monitors in an object of class
<code>vb</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'vb'
plot(x, Data, PDF=FALSE, Parms, ...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.vb_+3A_x">x</code></td>
<td>

<p>This required argument is an object of class <code>vb</code>.</p>
</td></tr>
<tr><td><code id="plot.vb_+3A_data">Data</code></td>
<td>

<p>This required argument must receive the list of data that was
supplied to <code><a href="#topic+VariationalBayes">VariationalBayes</a></code> to create the object
of class <code>vb</code>.</p>
</td></tr>
<tr><td><code id="plot.vb_+3A_pdf">PDF</code></td>
<td>

<p>This logical argument indicates whether or not the user wants
Laplace's Demon to save the plots as a .pdf file.</p>
</td></tr>
<tr><td><code id="plot.vb_+3A_parms">Parms</code></td>
<td>

<p>This argument accepts a vector of quoted strings to be matched for
selecting parameters for plotting. This argument defaults to
<code>NULL</code> and selects every parameter for plotting. Each quoted
string is matched to one or more parameter names with the
<code>grep</code> function. For example, if the user specifies
<code>Parms=c("eta", "tau")</code>, and if the parameter names
are beta[1], beta[2], eta[1], eta[2], and tau, then all parameters
will be selected, because the string <code>eta</code> is within
<code>beta</code>. Since <code>grep</code> is used, string matching uses
regular expressions, so beware of meta-characters, though these are
acceptable: &quot;.&quot;, &quot;[&quot;, and &quot;]&quot;.</p>
</td></tr>
<tr><td><code id="plot.vb_+3A_...">...</code></td>
<td>
<p>Additional arguments are unused.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The plots are arranged in a <code class="reqn">3 \times 3</code> matrix. The
purpose of the iterated history plots is to show how the value of each
parameter, variance, and the deviance changed by iteration as the
<code><a href="#topic+VariationalBayes">VariationalBayes</a></code> attempted to maximize the logarithm
of the unnormalized joint posterior density. If the algorithm
converged, and if <code>sir=TRUE</code> in
<code><a href="#topic+VariationalBayes">VariationalBayes</a></code>, then plots are produced of
selected parameters and all monitored variables.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+VariationalBayes">VariationalBayes</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>### See the VariationalBayes function for an example.</code></pre>

<hr>
<h2 id='plot.vb.ppc'>Plots of Posterior Predictive Checks</h2><span id='topic+plot.vb.ppc'></span>

<h3>Description</h3>

<p>This may be used to plot, or save plots of, samples in an object of
class <code>vb.ppc</code>. A variety of plots is provided.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'vb.ppc'
plot(x, Style=NULL, Data=NULL,  Rows=NULL,
     PDF=FALSE, ...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.vb.ppc_+3A_x">x</code></td>
<td>

<p>This required argument is an object of class <code>vb.ppc</code>.</p>
</td></tr>
<tr><td><code id="plot.vb.ppc_+3A_style">Style</code></td>
<td>

<p>This optional argument specifies one of several styles of plots, and
defaults to <code>NULL</code> (which is the same as
<code>"Density"</code>). Styles of plots are indicated in quotes. Optional 
styles include <code>"Covariates"</code>, <code>"Covariates, Categorical DV"</code>,
<code>"Density"</code>, <code>"DW"</code>, <code>"DW, Multivariate, C"</code>,
<code>"ECDF"</code>, <code>"Fitted"</code>, <code>"Fitted, Multivariate, C"</code>,
<code>"Fitted, Multivariate, R"</code>, <code>"Jarque-Bera"</code>,
<code>"Jarque-Bera, Multivariate, C"</code>,
<code>"Mardia"</code>, <code>"Predictive Quantiles"</code>,
<code>"Residual Density"</code>, <code>"Residual Density, Multivariate, C"</code>,
<code>"Residual Density, Multivariate, R"</code>,
<code>"Residuals"</code>, <code>"Residuals, Multivariate, C"</code>,
<code>"Residuals, Multivariate, R"</code>, <code>"Space-Time by Space"</code>,
<code>"Space-Time by Time"</code>, <code>"Spatial"</code>,
<code>"Spatial Uncertainty"</code>, <code>"Time-Series"</code>,
<code>"Time-Series, Multivariate, C"</code>, and
<code>"Time-Series, Multivariate, R"</code>. Details are given below.</p>
</td></tr>
<tr><td><code id="plot.vb.ppc_+3A_data">Data</code></td>
<td>

<p>This optional argument accepts the data set used when updating the
model. Data is required only with certain plot styles, including
<code>"Covariates"</code>, <code>"Covariates, Categorical DV"</code>,
<code>"DW, Multivariate, C"</code>, <code>"Fitted, Multivariate, C"</code>,
<code>"Fitted, Multivariate, R"</code>,
<code>"Jarque-Bera, Multivariate, C"</code>, <code>"Mardia"</code>,
<code>"Residual Density, Multivariate, C"</code>,
<code>"Residual Density, Multivariate, R"</code>,
<code>"Residuals, Multivariate, C"</code>,
<code>"Residuals, Multivariate, R"</code>,
<code>"Space-Time by Space"</code>, <code>"Space-Time by Time"</code>,
<code>"Spatial"</code>, <code>"Spatial Uncertainty"</code>,
<code>"Time-Series, Multivariate, C"</code>, and
<code>"Time-Series, Multivariate, R"</code>.</p>
</td></tr>
<tr><td><code id="plot.vb.ppc_+3A_rows">Rows</code></td>
<td>

<p>This optional argument is for a vector of row numbers that 
specify the records associated by row in the object of class
<code>vb.ppc</code>. Only these rows are plotted. The default is to
plot all rows. Some plots do not allow rows to be specified.</p>
</td></tr>
<tr><td><code id="plot.vb.ppc_+3A_pdf">PDF</code></td>
<td>

<p>This logical argument indicates whether or not the user wants
Laplace's Demon to save the plots as a .pdf file.</p>
</td></tr>
<tr><td><code id="plot.vb.ppc_+3A_...">...</code></td>
<td>
<p>Additional arguments are unused.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function can be used to produce a variety of posterior predictive
plots, and the style of plot is selected with the <code>Style</code>
argument. Below are some notes on the styles of plots.
</p>
<p><code>Covariates</code> requires <code>Data</code> to be specified, and also
requires that the covariates are named <code>X</code> or <code>x</code>. A plot
is produced for each covariate column vector against yhat, and is
appropriate when y is not categorical.
</p>
<p><code>Covariates, Categorical DV</code> requires <code>Data</code> to be
specified, and also requires that the covariates are named <code>X</code> or
<code>x</code>. A plot is produced for each covariate column vector against
yhat, and is appropriate when y is categorical.
</p>
<p><code>Density</code> plots show the kernel density of the posterior
predictive distribution for each selected row of y (all are selected
by default). A vertical red line indicates the position of the
observed y along the x-axis. When the vertical red line is close to
the middle of a normal posterior predictive distribution, then there
is little discrepancy between y and the posterior predictive
distribution. When the vertical red line is in the tail of the
distribution, or outside of the kernel density altogether, then
there is a large discrepancy between y and the posterior predictive
distribution. Large discrepancies may be considered outliers, and
moreover suggest that an improvement in model fit should be
considered.
</p>
<p><code>DW</code> plots the distributions of the Durbin-Watson (DW) test
statistics (Durbin and Watson, 1950), both observed
(<code class="reqn">d^{obs}</code> as a transparent, black density) and replicated
(<code class="reqn">d^{rep}</code> as a transparent, red density). The distribution
of <code class="reqn">d^{obs}</code> is estimated from the model, and
<code class="reqn">d^{rep}</code> is simulated from normal residuals without 
autocorrelation, where the number of simulations are the same as the
observed number. This DW test may be applied to the residuals of
univariate time-series models (or otherwise ordered residuals) to
detect first-order autocorrelation. Autocorrelated residuals are not
independent. The DW test is applicable only when the residuals are
normally-distributed, higher-order autocorrelation is not present, and
y is not used also as a lagged predictor. The DW test statistic,
<code class="reqn">d^{obs}</code>, occurs in the interval (0,4), where 0 is
perfect positive autocorrelation, 2 is no autocorrelation, and 4 is
perfect negative autocorrelation. The following summary is reported on
the plot: the mean of <code class="reqn">d^{obs}</code> (and its 95% probability
interval), the probability that <code class="reqn">d^{obs} &gt; d^{rep}</code>, and whether or not autocorrelation is found. Positive
autocorrelation is reported when the observed process is greater than
the replicated process in 2.5% of the samples, and negative
autocorrelation is reported when the observed process is greater than
the replicated process in 97.5% of the samples.
</p>
<p><code>DW, Multivariate, C</code> requires <code>Data</code> to be
specified, and also requires that variable <code>Y</code> exist in the data
set with exactly that name. These plots compare each column-wise
vector of residuals with a univariate Durbin-Watson test, as in
<code>DW</code> above. This plot is appropriate when Y is multivariate, not
categorical, and residuals are desired to be tested column-wise for
first-order autocorrelation.
</p>
<p><code>ECDF</code> (Empirical Cumulative Distribution Function) plots compare
the ECDF of y with three ECDFs of yhat based on the 2.5%, 50%
(median), and 97.5% of its distribution. The ECDF(y) is defined as
the proportion of values less than or equal to y. This plot is
appropriate when y is univariate and at least ordinal.
</p>
<p><code>Fitted</code> plots compare y with the probability interval of its
replicate, and provide loess smoothing. This plot is appropriate when
y is univariate and not categorical.
</p>
<p><code>Fitted, Multivariate, C</code> requires <code>Data</code> to be
specified, and also requires that variable <code>Y</code> exists in the data
set with exactly that name. These plots compare each column-wise
vector of y in Y with its replicates and provide loess smoothing.
This plot is appropriate when Y is multivariate, not categorical, and
desired to be seen column-wise.
</p>
<p><code>Fitted, Multivariate, R</code> requires <code>Data</code> to be
specified, and also requires that variable <code>Y</code> exists in the data
set with exactly that name. These plots compare each row-wise
vector of y in Y with its replicates and provide loess smoothing.
This plot is appropriate when Y is multivariate, not categorical, and
desired to be seen row-wise.
</p>
<p><code>Jarque-Bera</code> plots the distributions of the Jarque-Bera (JB)
test statistics (Jarque and Bera, 1980), both observed
(<code class="reqn">JB^{obs}</code> as a transparent black density) and replicated
(<code class="reqn">JB^{rep}</code> as a transparent red density). The
distribution of <code class="reqn">JB^{obs}</code> is estimated from the model,
and <code class="reqn">JB^{rep}</code> is simulated from normal residuals, where
the number of simulations are the same as the observed number. This
Jarque-Bera test may be applied to the residuals of 
univariate models to test for normality. The Jarque-Bera test does not
test normality per se, but whether or not the distribution has
kurtosis and skewness that match a normal distribution, and is
therefore a test of the moments of a normal distribution. The
following summary is reported on the plot: the mean of
<code class="reqn">JB^{obs}</code> (and its 95% probability interval), the
probability that <code class="reqn">JB^{obs} &gt; JB^{rep}</code>, and
whether or not normality is indicated. Non-normality is reported when
the observed process is greater than the replicated process in either
2.5% or 97.5% of the samples.
</p>
<p><code>Jarque-Bera, Multivariate, C</code> requires <code>Data</code> to be
specified, and also requires that variable <code>Y</code> exist in the data
set with exactly that name. These plots compare each column-wise
vector of residuals with a univariate Jarque-Bera test, as in
<code>Jarque-Bera</code> above. This plot is appropriate when Y is
multivariate, not categorical, and residuals are desired to be
tested column-wise for normality.
</p>
<p><code>Mardia</code> plots the distributions of the skewness (K3) and
kurtosis (K4) test statistics (Mardia, 1970), both observed
(<code class="reqn">K3^{obs}</code> and <code class="reqn">K4^{obs}</code> as transparent
black density) and replicated (<code class="reqn">K3^{rep}</code> and
<code class="reqn">K4^{rep}</code> as transparent red density). The distributions
of <code class="reqn">K3^{obs}</code> and <code class="reqn">K4^{obs}</code> are estimated
from the model, and both <code class="reqn">K3^{rep}</code> <code class="reqn">K4^{rep}</code>
are simulated from multivariate normal residuals, where the number of
simulations are the same as the observed number. This Mardia's test
may be applied to the residuals of multivariate models to test for
multivariate normality. Mardia's test does not test for multivariate
normality per se, but whether or not the distribution has kurtosis and
skewness that match a multivariate normal distribution, and is 
therefore a test of the moments of a multivariate normal
distribution. The following summary is reported on the plots: the
means of <code class="reqn">K3^{obs}</code> and <code class="reqn">K4^{obs}</code> (and
the associated 95% probability intervals), the probabilities that
<code class="reqn">K3^{obs} &gt; K3^{rep}</code> and
<code class="reqn">K4^{obs} &gt; K4^{rep}</code>, and whether or not
multivariate normality is indicated. Non-normality is reported when 
the observed process is greater than the replicated process in either
2.5% or 97.5% of the samples. <code>Mardia</code> requires <code>Data</code> to
be specified, and also requires that variable <code>Y</code> exist in the
data set with exactly that name. <code>Y</code> must be a <code class="reqn">N \times P</code> matrix of <code class="reqn">N</code> records and <code class="reqn">P</code> variables. Source
code was modified from the deprecated package QRMlib.
</p>
<p><code>Predictive Quantiles</code> plots compare y with the predictive
quantile (PQ) of its replicate. This may be useful in looking for
patterns with outliers. Instances outside of the gray lines are
considered outliers.
</p>
<p><code>Residual Density</code> plots the residual density of the median of
the samples. A vertical red line occurs at zero. This plot may be
useful for inspecting a distributional assumption of residual
variance. This plot is appropriate when y is univariate and
continuous.
</p>
<p><code>Residual Density, Multivariate C</code> requires <code>Data</code> to be
specified, and also requires that variable <code>Y</code> exist in the data
set with exactly that name. These are column-wise plots of residual
density, given the median of the samples. These plots may be useful
for inspecting a distributional assumption of residual variance.
This plot is appropriate when Y is multivariate, continuous, and
densities are desired to be seen column-wise.
</p>
<p><code>Residual Density, Multivariate R</code> requires <code>Data</code> to be
specified, and also requires that variable <code>Y</code> exist in the data
set with exactly that name. These are row-wise plots of residual
density, given the median of the samples. These plots may be useful
for inspecting a distributional assumption of residual variance.
This plot is appropriate when Y is multivariate, continuous, and
densities are desired to be seen row-wise.
</p>
<p><code>Residuals</code> plots compare y with its residuals. The probability
interval is plotted as a line. This plot is appropriate when y
is univariate.
</p>
<p><code>Residuals, Multivariate, C</code> requires <code>Data</code> to be
specified, and also requires that variable <code>Y</code> exist in the data
set with exactly that name. These are plots of each column-wise
vector of residuals. The probability interval is plotted as a
line. This plot is appropriate when Y is multivariate, not
categorical, and the residuals are desired to be seen column-wise.
</p>
<p><code>Residuals, Multivariate, R</code> requires <code>Data</code> to be
specified, and also requires that variable <code>Y</code> exist in the data
set with exactly that name. These are plots of each row-wise
vector of residuals. The probability interval is plotted as a
line. This plot is appropriate when Y is multivariate, not
categorical, and the residuals are desired to be seen row-wise.
</p>
<p><code>Space-Time by Space</code> requires <code>Data</code> to be specified, and
also requires that the following variables exist in the data set with
exactly these names: <code>latitude</code>, <code>longitude</code>, <code>S</code>, and
<code>T</code>. These space-time plots compare the S x T matrix Y with the S
x T matrix Yrep, producing one time-series plot per point s in space,
for a total of S plots. Therefore, these are time-series plots for
each point s in space across T time-periods. See <code>Time-Series</code>
plots below.
</p>
<p><code>Space-Time by Time</code> requires <code>Data</code> to be specified, and
also requires that the following variables exist in the data set with
exactly these names: <code>latitude</code>, <code>longitude</code>, <code>S</code>, and
<code>T</code>. These space-time plots compare the S x T matrix Y with the S
x T matrix Yrep, producing one spatial plot per time-period, and T
plots will be produced. See <code>Spatial</code> plots below.
</p>
<p><code>Spatial</code> requires <code>Data</code> to be specified, and also requires
that the following variables exist in the data set with exactly these
names: <code>latitude</code> and <code>longitude</code>. This spatial plot shows
yrep plotted according to its coordinates, and is color-coded so that
higher values of yrep become more red, and lower values become more
yellow.
</p>
<p><code>Spatial Uncertainty</code> requires <code>Data</code> to be specified, and
also requires that the following variables exist in the data set with
exactly these names: <code>latitude</code> and <code>longitude</code>. This
spatial plot shows the probability interval of yrep plotted according
to its coordinates, and is color-coded so that wider probability
intervals become more red, and lower values become more yellow.
</p>
<p><code>Time-Series</code> plots compare y with its replicate, including the
median and probability interval quantiles. This plot is appropriate
when y is univariate and ordered by time.
</p>
<p><code>Time-Series, Multivariate, C</code> requires <code>Data</code> to be
specified, and also requires that variable <code>Y</code> exist in the data
set with exactly that name. These plots compare each column-wise
time-series in Y with its replicate, including the median and
probability interval quantiles. This plot is appropriate when y is
multivariate and each time-series is indexed by column in Y.
</p>
<p><code>Time-Series, Multivariate, R</code> requires <code>Data</code> to be
specified, and also requires that variable <code>Y</code> exist in the data
set with exactly that name. These plots compare each row-wise
time-series in Y with its replicate, including the median and
probability interval quantiles. This plot is appropriate when y is
multivariate and each time-series is indexed by row in Y, such as is
typically true in panel models.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>References</h3>

<p>Durbin, J., and Watson, G.S. (1950). &quot;Testing for Serial Correlation
in Least Squares Regression, I.&quot; <em>Biometrika</em>, 37, p. 409&ndash;428.
</p>
<p>Jarque, C.M. and Bera, A.K. (1980). &quot;Efficient Tests for Normality,
Homoscedasticity and Serial Independence of Regression Residuals&quot;.
<em>Economics Letters</em>, 6(3), p. 255&ndash;259.
</p>
<p>Mardia, K.V. (1970). &quot;Measures of Multivariate Skewness and Kurtosis
with Applications&quot;. <em>Biometrika</em>, 57(3), p. 519&ndash;530.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.vb">predict.vb</a></code> and
<code><a href="#topic+VariationalBayes">VariationalBayes</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### See the VariationalBayes function for an example.</code></pre>

<hr>
<h2 id='plotMatrix'>Plot a Numerical Matrix</h2><span id='topic+plotMatrix'></span>

<h3>Description</h3>

<p>This function plots a numerical matrix, and is often used to plot the
following matrices: correlation, covariance, distance, and
precision.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotMatrix(x, col=colorRampPalette(c("red","black","green"))(100),
     cex=1, circle=TRUE, order=FALSE, zlim=NULL, title="", PDF=FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plotMatrix_+3A_x">x</code></td>
<td>
<p>This required argument is a numerical matrix, or an
object of class <code>bayesfactor</code>, <code>demonoid</code>, <code>iterquad</code>,
<code>laplace</code>, <code>pmc</code>, <code>posteriorchecks</code>, or <code>vb</code>.
See more information below regarding these classes. One component of
a blocked proposal covariance matrix must be pointed to explicitly,
rather than to the object of class <code>demonoid</code>.</p>
</td></tr>
<tr><td><code id="plotMatrix_+3A_col">col</code></td>
<td>
<p>This argument specifies the colors of the circles. By
default, the <code>colorRampPalette</code> function colors strong positive
correlation as <code>green</code>, zero correlation as <code>black</code>, and
strong negative correlation as <code>red</code>, and provides 100 color
gradations.</p>
</td></tr>
<tr><td><code id="plotMatrix_+3A_cex">cex</code></td>
<td>
<p>When <code>circle=TRUE</code>, this argument specifies the size
of the marginal text, the names of the parameters or variables, and
defaults to 1.</p>
</td></tr>
<tr><td><code id="plotMatrix_+3A_circle">circle</code></td>
<td>
<p>Logical. When <code>TRUE</code>, each element in the numeric
matrix is represented with a circle, and a larger circle is assigned
to elements that are farther from zero. Also, when <code>TRUE</code>, the
gradation scale does not appear to the right of the plot.</p>
</td></tr>
<tr><td><code id="plotMatrix_+3A_order">order</code></td>
<td>
<p>Logical. This argument defaults to <code>FALSE</code>, and
presents the parameters or variables in the same order as in the
numeric matrix. When <code>TRUE</code>, the parameters or variables
are ordered using principal components analysis.</p>
</td></tr>
<tr><td><code id="plotMatrix_+3A_zlim">zlim</code></td>
<td>
<p>When <code>circle=FALSE</code>, the gradation scale may be
constrained to an interval by <code>zlim</code>, such as
<code>zlim=c(-1,1)</code>, and only values within the interval are
plotted.</p>
</td></tr>
<tr><td><code id="plotMatrix_+3A_title">title</code></td>
<td>
<p>This argument specifies the title of the plot, and the
default does not include a title. When <code>x</code> is of class
<code>posteriorchecks</code>, the title is changed to <code>Posterior
      Correlation</code>.</p>
</td></tr>
<tr><td><code id="plotMatrix_+3A_pdf">PDF</code></td>
<td>
<p>Logical. When <code>TRUE</code>, the plot is saved as a .pdf
file.</p>
</td></tr>
<tr><td><code id="plotMatrix_+3A_...">...</code></td>
<td>
<p>Additional arguments are unused.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>plotMatrix</code> function produces one of two styles of plots,
depending on the <code>circle</code> argument. A <code class="reqn">K \times K</code>
numeric matrix of <code class="reqn">K</code> parameters or variables is plotted. The plot
is a matrix of the same dimensions, in which each element is colored
(and sized, when <code>circle=TRUE</code>) according to its value.
</p>
<p>Although <code>plotMatrix</code> does not provide the same detail as a
numeric matrix, it is easier to discover elements of interest
according to color (and size when <code>circle=TRUE</code>).
</p>
<p>The <code>plotMatrix</code> function is not inherently Bayesian, and does
not include uncertainty in matrices. Nonetheless, it is included
because it is a useful graphical presentation of a numeric matrices,
and is recommended to be used with the posterior correlation matrix in
an object of class <code>posteriorchecks</code>.
</p>
<p>When <code>x</code> is an object of class <code>bayesfactor</code>, matrix
<code>B</code> is plotted. When <code>x</code> is an object of class
<code>demonoid</code> (if it is a matrix), <code>iterquad</code>, <code>laplace</code>,
<code>pmc</code>, or <code>vb</code>, the covariance matrix <code>Covar</code> is
plotted. When <code>x</code> is an object of class <code>posteriorchecks</code>,
the posterior correlation matrix is plotted.
</p>
<p>This is a modified version of the <code>circle.corr</code> function
of Taiyun Wei.
</p>


<h3>Author(s)</h3>

<p>Taiyun Wei</p>


<h3>See Also</h3>

<p><code><a href="#topic+PosteriorChecks">PosteriorChecks</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
### Although it is most commonly used with an object of class
### posteriorchecks, it is applied here to a different correlation matrix.
data(mtcars)
plotMatrix(cor(mtcars), col=colorRampPalette(c("green","gray10","red"))(100),
     cex=1, circle=FALSE, order=TRUE)
plotMatrix(cor(mtcars), col=colorRampPalette(c("green","gray10","red"))(100),
     cex=1, circle=TRUE, order=TRUE)</code></pre>

<hr>
<h2 id='plotSamples'>Plot Samples</h2><span id='topic+plotSamples'></span>

<h3>Description</h3>

<p>This function provides basic plots that are extended to include
samples.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotSamples(X, Style="KDE", LB=0.025, UB=0.975, Title=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plotSamples_+3A_x">X</code></td>
<td>
<p>This required argument is a <code class="reqn">N \times S</code> numerical
matrix of <code class="reqn">N</code> records and <code class="reqn">S</code> samples.</p>
</td></tr>
<tr><td><code id="plotSamples_+3A_style">Style</code></td>
<td>
<p>This argument accepts the following quoted strings:
&quot;barplot&quot;, &quot;dotchart&quot;, &quot;hist&quot;, &quot;KDE&quot;, or &quot;Time-Series&quot;. It defaults
to <code>Style="KDE"</code>.</p>
</td></tr>
<tr><td><code id="plotSamples_+3A_lb">LB</code></td>
<td>
<p>This argument accepts the lower bound of a probability
interval, which must be in the interval [0,0.5).</p>
</td></tr>
<tr><td><code id="plotSamples_+3A_ub">UB</code></td>
<td>
<p>This argument accepts the upper bound of a probability
interval, which must be in the interval (0.5,1].</p>
</td></tr>
<tr><td><code id="plotSamples_+3A_title">Title</code></td>
<td>
<p>This argument defaults to <code>NULL</code>, and otherwise
accepts a quoted string that will be the title of the plot.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>plotSamples</code> function extends several basic plots from
points to samples. For example, it is common to use the <code>hist</code>
function to plot a histogram from a column vector. However, the user
may desire to plot a histogram of a column vector that was sampled
numerous times, rather than a simple column vector, in which a
(usually 95%) probability interval is also plotted to show the
uncertainty around the sampled median of each bin in the histogram.
</p>
<p>The <code>plotSamples</code> function extends the <code>barplot</code>,
<code>dotchart</code>, and <code>hist</code> functions to include uncertainty due
to samples. The <code>KDE</code> style of plot is added so that a
probability interval is shown around a sampled kernel density estimate
of a distribution, and the <code>Time-Series</code> style of plot is added
so that a probability interval is shown around a sampled univariate
time-series.
</p>
<p>For each style of plot, three quantiles are plotted: the lower bound
(LB), median, and upper bound (UB).
</p>
<p>One of many potential Bayesian applications is to examine the
uncertainty in a predictive distribution.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>Examples</h3>

<pre><code class='language-R'>#library(LaplacesDemon)
#N &lt;- 100
#S &lt;- 100
#X &lt;- matrix(rnorm(N*S),N,S)
#rownames(X) &lt;- 1:100
#plotSamples(X, Style="barplot", LB=0.025, UB=0.975)
#plotSamples(X[1:10,], Style="dotchart", LB=0.025, UB=0.975)
#plotSamples(X, Style="hist", LB=0.025, UB=0.975)
#plotSamples(X, Style="KDE", LB=0.025, UB=0.975)
#plotSamples(X, Style="Time-Series", LB=0.025, UB=0.975)
</code></pre>

<hr>
<h2 id='PMC'>Population Monte Carlo</h2><span id='topic+PMC'></span>

<h3>Description</h3>

<p>The <code>PMC</code> function updates a model with Population Monte Carlo.
Given a model specification, data, and initial values, <code>PMC</code>
maximizes the logarithm of the unnormalized joint
posterior density and provides samples of the marginal
posterior distributions, deviance, and other monitored variables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PMC(Model, Data, Initial.Values, Covar=NULL, Iterations=10, Thinning=1,
alpha=NULL, M=1, N=1000, nu=9, CPUs=1, Type="PSOCK")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="PMC_+3A_model">Model</code></td>
<td>
<p>This is a model specification function. For more
information, see <code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>.</p>
</td></tr>
<tr><td><code id="PMC_+3A_initial.values">Initial.Values</code></td>
<td>
<p>This is either a vector initial values, one for
each of <code class="reqn">K</code> parameters, or in the case of a mixture of <code class="reqn">M</code>
components, this is a <code class="reqn">M \times K</code> matrix of initial
values. If all initial values are zero in this vector, or in the
first row of a matrix, then <code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code> is
used to optimize initial values, in which case all mixture
components receive the same initial values and covariance matrix
from the object of class <code>laplace</code>. Parameters must be
continuous.</p>
</td></tr>
<tr><td><code id="PMC_+3A_data">Data</code></td>
<td>
<p>This is a list of data. For more information, see
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>.</p>
</td></tr>
<tr><td><code id="PMC_+3A_covar">Covar</code></td>
<td>
<p>This is a <code class="reqn">K \times K</code> covariance matrix for
<code class="reqn">K</code> parameters, or for multiple mixture components, this is a
<code class="reqn">K \times K \times M</code> array of <code class="reqn">M</code> covariance
matrices, where <code class="reqn">M</code> is the number of mixture components.
<code>Covar</code> defaults to <code>NULL</code>, in which case a scaled
identity matrix (with the same scale as in
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>) is applied to all mixture components.</p>
</td></tr>
<tr><td><code id="PMC_+3A_iterations">Iterations</code></td>
<td>
<p>This is the number of iterations during which PMC
will update the model. Updating the model for only one iteration is
the same as applying non-adaptive importance sampling.</p>
</td></tr>
<tr><td><code id="PMC_+3A_thinning">Thinning</code></td>
<td>
<p>This is the number by which the posterior is
thinned. To have 1,000 posterior samples with <code>M=3</code> mixture
components and <code>N=10000</code> samples each, <code>Thinning=30</code>. For
more information, see the <code><a href="#topic+Thin">Thin</a></code> function.</p>
</td></tr>
<tr><td><code id="PMC_+3A_alpha">alpha</code></td>
<td>
<p>This is a vector of length <code class="reqn">M</code>, the number of mixture
components. <code class="reqn">\alpha</code> is the probability of each mixture
component. The default value is <code>NULL</code>, which assigns an equal
probability to each component.</p>
</td></tr>
<tr><td><code id="PMC_+3A_m">M</code></td>
<td>
<p>This is the number <code class="reqn">M</code> of multivariate t distribution
mixture components.</p>
</td></tr>
<tr><td><code id="PMC_+3A_n">N</code></td>
<td>
<p>This is the number <code class="reqn">N</code> of samples per mixture
component. The required number of samples increases with the number
<code class="reqn">K</code> of parameters. These samples are also called walkers or
particles.</p>
</td></tr>
<tr><td><code id="PMC_+3A_nu">nu</code></td>
<td>
<p>This is the degrees of freedom parameter <code class="reqn">\nu</code> for
the multivariate t distribution for each mixture component. If a
multivariate normal distribution is preferred, then set
<code class="reqn">\nu &gt; 1e4</code>.</p>
</td></tr>
<tr><td><code id="PMC_+3A_cpus">CPUs</code></td>
<td>
<p>This argument is required for parallel processing, and
and indicates the number of central processing units (CPUs) of the
computer or cluster. For example, when a user has a quad-core
computer, <code>CPUs=4</code>.</p>
</td></tr>
<tr><td><code id="PMC_+3A_type">Type</code></td>
<td>
<p>This argument defaults to <code>"PSOCK"</code> and uses the
Simple Network of Workstations (SNOW) for parallelization.
Alternatively, <code>Type="MPI"</code> may be specified to use Message
Passing Interface (MPI) for parallelization.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>PMC</code> function uses the adaptive importance sampling
algorithm of Wraith et al. (2009), also called Mixture PMC or M-PMC
(Cappe et al., 2008). Iterative adaptive importance sampling was
introduced in the 1980s. Modern PMC was introduced (Cappe et al.,
2004), and extended to multivariate Gaussian or t-distributed mixtures
(Cappe et al., 2008). This version uses a multivariate t distribution
for each mixture component, and also allows a multivariate normal
distribution when the degrees of freedom, <code class="reqn">\nu &gt; 1e4</code>. At each iteration, a mixture distribution is sampled with
importance sampling, and the samples (or populations) are adapted to
improve the importance sampling. Adaptation is a variant of EM
(Expectation-Maximization). The sample is self-normalized, and is an
example of self-normalized importance sampling (SNIS), or
self-importance sampling. The vector <code class="reqn">\alpha</code> contains the
probability of each mixture component. These, as well as multivariate
t distribution mixture parameters (except <code class="reqn">\nu</code>), are adapted
each iteration.
</p>
<p>Advantages of PMC over MCMC include:
</p>

<ul>
<li><p> It is difficult to assess convergence of MCMC chains, and this
is not necessary in PMC (Wraith et al., 2009).  
</p>
</li>
<li><p> MCMC chains have autocorrelation that effectively reduces
posterior samples. PMC produces independent samples that are not
reduced with autocorrelation.
</p>
</li>
<li><p> PMC has been reported to produce samples with less variance
than MCMC.
</p>
</li>
<li><p> It is difficult to parallelize MCMC. Posterior samples from
parallel chains can be pooled when all chains have converged, but
until this occurs, parallelization is unhelpful. PMC, on the other
hand, can parallelize the independent, Monte Carlo samples during each
iteration and reduce run-time as the number of processors
increases. Currently, PMC is not parallelized here.
</p>
</li>
<li><p> The multivariate mixture in PMC can represent a multimodal
posterior, where MCMC with parallel chains may be used to identify a
multimodal posterior, but probably will not yield combined samples
that proportionally represent it.
</p>
</li></ul>

<p>Disadvantages of PMC, compared to MCMC, include:
</p>

<ul>
<li><p> In PMC, the required number of samples at each iteration
increases quickly with respect to an increase in parameters. MCMC is
more suitable for models with large numbers of parameters, and
therefore, MCMC is more generalizable.
</p>
</li>
<li><p> PMC is more sensitive to initial values than MCMC, especially
as the number of parameters increases.
</p>
</li>
<li><p> PMC is more sensitive to the initial covariance matrix (or
matrices for mixture components) than adaptive MCMC. PMC requires more
information about the target distributions before updating. The
covariance matrix from a converged iterative quadrature algorithm,
Laplace Approximation, or Variational Bayes may be required (see
<code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code>, <code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>,
or <code><a href="#topic+VariationalBayes">VariationalBayes</a></code> for more information).
</p>
</li></ul>

<p>Since PMC requires better initial information than iterative
quadrature, Laplace Approximation, MCMC, and Variational Bayes, it is
not recommended to begin updating a model that has little prior
information with PMC, especially when the model has more than a few
parameters. Instead, iterative quadrature, Laplace Approximation,
MCMC, or Variational Bayes should be used. However, once convergence
is found or assumed, it is recommended to attempt to update the model
with PMC, given the latest parameters and convariance matrix from
iterative quadrature, Laplace Approximation, MCMC, or Variational
Bayes. Used in this way, PMC may improve the model fit obtained with
MCMC and should reduce the variance of the marginal posterior
distributions,  which is desirable for predictive modeling.
</p>
<p>Convergence is assessed by observing two outputs: normalized effective
sample size (<code>ESSN</code>) and normalized perplexity
(<code>Perplexity</code>). These are described below. PMC is considered to
have converged when these diagnostics stabilize (Wraith et al., 2009),
or when the normalized perplexity becomes sufficiently close to 1
(Cappe et al., 2008). If they do not stabilize, then it is suggested
to begin PMC again with a larger number <code>N</code> of samples, and
possibly with different initial values and covariance matrix or
matrices. <code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code>,
<code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>, or <code><a href="#topic+VariationalBayes">VariationalBayes</a></code>
may be helpful to provide better starting values for <code>PMC</code>.
</p>
<p>If a message appears that warns about &lsquo;bad weights&rsquo;, then <code>PMC</code>
is attempting to work with an iteration in which importance weights
are problematic. If this occurs in the first iteration, then all
importance weights are set to <code class="reqn">1/N</code>. If this occurs in other
iterations, then the information from the previous iteration is used
instead and different draws are made from that importance
distribution. This may allow <code>PMC</code> to eventually adapt
successfully to the target. If not, the user is advised to begin again
with a larger number <code class="reqn">N</code> of samples, and possibly different
initial values and covariance matrix or matrices, as above. PMC can
experience difficulty when it begins with poor initial conditions.
</p>
<p>The user may combine samples from previous iterations with samples
from the latest iteration for inference, if the algorithm converged
before the last iteration. Currently, a function is not provided for
combining previous samples.
</p>


<h3>Value</h3>

<p>The returned object is an object of class <code>pmc</code> with the
following components:
</p>
<table>
<tr><td><code>alpha</code></td>
<td>
<p>This is a <code class="reqn">M \times T</code> matrix of the
probabilities of mixture components, where <code class="reqn">M</code> is the number of
mixture components and <code class="reqn">T</code> is the number of iterations.</p>
</td></tr>
<tr><td><code>Call</code></td>
<td>
<p>This is the matched call of <code>PMC</code>.</p>
</td></tr>
<tr><td><code>Covar</code></td>
<td>
<p>This stores the <code class="reqn">K \times K \times T \times M</code> proposal covariance matrix in an array, where <code class="reqn">K</code> is
the dimension or number of parameters or initial values, <code class="reqn">T</code> is
the number of iterations, and <code class="reqn">M</code> is the number of mixture
components. If the model is updated in the future, then the latest
covariance matrix for each mixture component can be extracted and
used to start the next update where the last update left off.</p>
</td></tr>
<tr><td><code>Deviance</code></td>
<td>
<p>This is a vector of the deviance of the model, with a
length equal to the number of thinned samples that were retained.
Deviance is useful for considering model fit, and is equal to the
sum of the log-likelihood for all rows in the data set, which is
then multiplied by negative two.</p>
</td></tr>
<tr><td><code>DIC</code></td>
<td>
<p>This is a vector of three values: Dbar, pD, and DIC. Dbar
is the mean deviance, pD is a measure of model complexity indicating
the effective number of parameters, and DIC is the Deviance
Information Criterion, which is a model fit statistic that is the
sum of Dbar and pD. <code>DIC</code> is calculated over the thinned
samples. Note that pD is calculated as <code>var(Deviance)/2</code> as in
Gelman et al. (2004).</p>
</td></tr>
<tr><td><code>ESSN</code></td>
<td>
<p>This is a vector of length <code class="reqn">T</code> that contains the
normalized effective sample size (ESSN) per iteration across <code class="reqn">T</code>
iterations. ESSN is used as a convergence diagnostic. ESSN is
normalized between zero and one, and can be interpreted as the
proportion of samples with non-zero weight. Higher is better.</p>
</td></tr>
<tr><td><code>Initial.Values</code></td>
<td>
<p>This is the vector or matrix of
<code>Initial.Values</code>.</p>
</td></tr>
<tr><td><code>Iterations</code></td>
<td>
<p>This reports the number of <code>Iterations</code> for
updating.</p>
</td></tr>
<tr><td><code>LML</code></td>
<td>
<p>This is an approximation of the logarithm of the marginal
likelihood of the data (see the <code><a href="#topic+LML">LML</a></code> function for more
information). <code>LML</code> is estimated with nonparametric
self-normalized importance sampling (NSIS), given LL and the
marginal posterior samples of the parameters. <code>LML</code> is useful
for comparing multiple models with the <code><a href="#topic+BayesFactor">BayesFactor</a></code>
function.</p>
</td></tr>
<tr><td><code>M</code></td>
<td>
<p>This reports the number of mixture components.</p>
</td></tr>
<tr><td><code>Minutes</code></td>
<td>
<p>This indicates the number of minutes that <code>PMC</code>
was running, and this includes the initial checks as well as time it
took to perform final sampling and create summaries.</p>
</td></tr>
<tr><td><code>Model</code></td>
<td>
<p>This contains the model specification <code>Model</code>.</p>
</td></tr>
<tr><td><code>N</code></td>
<td>
<p>This is the number of un-thinned samples per mixture
component.</p>
</td></tr>
</table>
<p>itemnuThis is the degrees of freedom parameter <code class="reqn">\nu</code> for
each multivariate t distribution in each mixture component.
</p>
<table>
<tr><td><code>Mu</code></td>
<td>
<p>This is a <code class="reqn">T \times K \times M</code> array of
means for the importance sampling distribution across <code class="reqn">T</code>
iterations, <code class="reqn">K</code> parameters, and <code class="reqn">M</code> mixture components.</p>
</td></tr>
<tr><td><code>Monitor</code></td>
<td>
<p>This is a <code class="reqn">S \times J</code> matrix of
thinned samples of monitored variables, where <code class="reqn">S</code> is the number
of thinned samples and <code class="reqn">J</code> is the number of monitored
variables.</p>
</td></tr>
<tr><td><code>Parameters</code></td>
<td>
<p>This reports the number <code class="reqn">K</code> of parameters.</p>
</td></tr>
<tr><td><code>Perplexity</code></td>
<td>
<p>This is a vector of length <code class="reqn">T</code> that contains the
normalized perplexity per iteration across <code class="reqn">T</code> iterations, and
is used as a convergence diagnostic. Perplexity is an approximation
of the negative of the Kullback-Leibler divergence (see
<code><a href="#topic+KLD">KLD</a></code>) between the target and the importance function.
Perplexity is normalized between zero and one, and a higher
normalized perplexity relates to less divergence, so higher is
better. A normalized perplexity that is close to one indicates good
agreement between the target density and the importance
function. This is based on the Shannon entropy of the normalized
importance weights, which is used frequently to measure the quality
of importance samples.</p>
</td></tr>
<tr><td><code>Posterior1</code></td>
<td>
<p>This is an <code class="reqn">N \times K \times T \times M</code> array of un-thinned posterior samples across <code class="reqn">N</code>
samples, <code class="reqn">K</code> parameters, <code class="reqn">T</code> iterations, and <code class="reqn">M</code> mixture
components.</p>
</td></tr>
<tr><td><code>Posterior2</code></td>
<td>
<p>This is a <code class="reqn">S \times K</code> matrix of
thinned posterior samples, where <code class="reqn">S</code> is the number of thinned
samples and <code class="reqn">K</code> is the number of parameters.</p>
</td></tr>
<tr><td><code>Summary</code></td>
<td>
<p>This is a matrix that summarizes the marginal
posterior distributions of the parameters, deviance, and monitored
variables from thinned samples. The following summary statistics are
included: mean, standard deviation, MCSE (Monte Carlo Standard
Error), ESS is the effective sample size due to autocorrelation, and
finally the 2.5%, 50%, and 97.5% quantiles are reported. MCSE is
essentially a standard deviation around the marginal posterior mean
that is due to uncertainty associated with using Monte Carlo
sampling. The acceptable size of the MCSE depends on the acceptable
uncertainty associated around the marginal posterior mean. The
default <code>IMPS</code> method is used. Next, the desired precision of
ESS depends on the user's goal.</p>
</td></tr>
<tr><td><code>Thinned.Samples</code></td>
<td>
<p>This is the number of thinned samples in
<code>Posterior2</code>.</p>
</td></tr>
<tr><td><code>Thinning</code></td>
<td>
<p>This is the amount of thinning requested by the user.</p>
</td></tr>
<tr><td><code>W</code></td>
<td>
<p>This is a <code class="reqn">N \times T</code> matrix of normalized
importance weights, where <code class="reqn">N</code> is the number of un-thinned
samples per mixture component and <code class="reqn">T</code> is the number of
iterations. Computationally, the algorithm uses the logarithm of the
weights.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>References</h3>

<p>Cappe, O., Douc, R., Guillin, A., Marin, J.M., and Robert, C. (2008).
&quot;Adaptive Importance Sampling in General Mixture Classes&quot;.
<em>Statistics and Computing</em>, 18, p. 587&ndash;600.
</p>
<p>Cappe, O., Guillin, A., Marin, J.M., and Robert, C. (2004). &quot;Population 
Monte Carlo&quot;. <em>Journal of Computational and Graphical
Statistics</em>, 13, p. 907&ndash;929.
</p>
<p>Gelman, A., Carlin, J., Stern, H., and Rubin, D. (2004). &quot;Bayesian
Data Analysis, Texts in Statistical Science, 2nd ed.&quot;. Chapman and
Hall, London.
</p>
<p>Wraith, D., Kilbinger, M., Benabed, K., Cappe, O., Cardoso, J.F.,
Fort, G., Prunet, S., and Robert, C.P. (2009). &quot;Estimation of
Cosmological Parameters Using Adaptive Importance Sampling&quot;.
<em>Physical Review D</em>, 80(2), p. 023507.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+BayesFactor">BayesFactor</a></code>,
<code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code>,
<code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>,
<code><a href="#topic+LML">LML</a></code>,
<code><a href="#topic+PMC.RAM">PMC.RAM</a></code>,
<code><a href="#topic+Thin">Thin</a></code>, and
<code><a href="#topic+VariationalBayes">VariationalBayes</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># The accompanying Examples vignette is a compendium of examples.
####################  Load the LaplacesDemon Library  #####################
library(LaplacesDemon)

##############################  Demon Data  ###############################
data(demonsnacks)
y &lt;- log(demonsnacks$Calories)
X &lt;- cbind(1, as.matrix(log(demonsnacks[,c(1,4,10)]+1)))
J &lt;- ncol(X)
for (j in 2:J) X[,j] &lt;- CenterScale(X[,j])

#########################  Data List Preparation  #########################
mon.names &lt;- "LP"
parm.names &lt;- as.parm.names(list(beta=rep(0,J), sigma=0))
pos.beta &lt;- grep("beta", parm.names)
pos.sigma &lt;- grep("sigma", parm.names)
PGF &lt;- function(Data) {
     beta &lt;- rnorm(Data$J)
     sigma &lt;- runif(1)
     return(c(beta, sigma))
     }
MyData &lt;- list(J=J, PGF=PGF, X=X, mon.names=mon.names,
     parm.names=parm.names, pos.beta=pos.beta, pos.sigma=pos.sigma, y=y)

##########################  Model Specification  ##########################
Model &lt;- function(parm, Data)
     {
     ### Parameters
     beta &lt;- parm[Data$pos.beta]
     sigma &lt;- interval(parm[Data$pos.sigma], 1e-100, Inf)
     parm[Data$pos.sigma] &lt;- sigma
     ### Log-Prior
     beta.prior &lt;- sum(dnormv(beta, 0, 1000, log=TRUE))
     sigma.prior &lt;- dhalfcauchy(sigma, 25, log=TRUE)
     ### Log-Likelihood
     mu &lt;- tcrossprod(Data$X, t(beta))
     LL &lt;- sum(dnorm(Data$y, mu, sigma, log=TRUE))
     ### Log-Posterior
     LP &lt;- LL + beta.prior + sigma.prior
     Modelout &lt;- list(LP=LP, Dev=-2*LL, Monitor=LP,
          yhat=rnorm(length(mu), mu, sigma), parm=parm)
     return(Modelout)
     }

set.seed(666)

############################  Initial Values  #############################
Initial.Values &lt;- GIV(Model, MyData, PGF=TRUE)

########################  Population Monte Carlo  #########################
Fit &lt;- PMC(Model, MyData, Initial.Values, Covar=NULL, Iterations=5,
     Thinning=1, alpha=NULL, M=1, N=100, CPUs=1)
Fit
print(Fit)
PosteriorChecks(Fit)
caterpillar.plot(Fit, Parms="beta")
plot(Fit, BurnIn=0, MyData, PDF=FALSE)
Pred &lt;- predict(Fit, Model, MyData, CPUs=1)
summary(Pred, Discrep="Chi-Square")
plot(Pred, Style="Covariates", Data=MyData)
plot(Pred, Style="Density", Rows=1:9)
plot(Pred, Style="ECDF")
plot(Pred, Style="Fitted")
plot(Pred, Style="Jarque-Bera")
plot(Pred, Style="Predictive Quantiles")
plot(Pred, Style="Residual Density")
plot(Pred, Style="Residuals")
Levene.Test(Pred)
Importance(Fit, Model, MyData, Discrep="Chi-Square")

#End
</code></pre>

<hr>
<h2 id='PMC.RAM'>PMC RAM Estimate</h2><span id='topic+PMC.RAM'></span>

<h3>Description</h3>

<p>This function estimates the random-access memory (RAM) required to
update a given model and data with <code><a href="#topic+PMC">PMC</a></code>.
</p>
<p><em>Warning:</em> Unwise use of this function may crash a computer, so
please read the details below.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PMC.RAM(Model, Data, Iterations, Thinning, M, N)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="PMC.RAM_+3A_model">Model</code></td>
<td>
<p>This is a model specification function. For more
information, see <code><a href="#topic+PMC">PMC</a></code>.</p>
</td></tr>
<tr><td><code id="PMC.RAM_+3A_data">Data</code></td>
<td>
<p>This is a list of Data. For more information, see
<code><a href="#topic+PMC">PMC</a></code>.</p>
</td></tr>
<tr><td><code id="PMC.RAM_+3A_iterations">Iterations</code></td>
<td>
<p>This is the number of iterations for which
<code><a href="#topic+PMC">PMC</a></code> would update. For more information,
see <code><a href="#topic+PMC">PMC</a></code>.</p>
</td></tr>
<tr><td><code id="PMC.RAM_+3A_thinning">Thinning</code></td>
<td>
<p>This is the amount of thinning applied to the samples
in <code><a href="#topic+PMC">PMC</a></code>.For more information, see <code><a href="#topic+PMC">PMC</a></code>.</p>
</td></tr>
<tr><td><code id="PMC.RAM_+3A_m">M</code></td>
<td>
<p>This is the number of mixture components in
<code><a href="#topic+PMC">PMC</a></code>.</p>
</td></tr>
<tr><td><code id="PMC.RAM_+3A_n">N</code></td>
<td>
<p>This is the number of samples in <code><a href="#topic+PMC">PMC</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>PMC.RAM</code> function uses the
<code><a href="utils.html#topic+object.size">object.size</a></code> function to estimate the size in MB of RAM
required to update in <code><a href="#topic+PMC">PMC</a></code> for a given model and data,
and for a number of iterations and specified thinning. When RAM is
exceeded, the computer will crash. This function can be useful when
trying to estimate how many samples and iterations to update a
model without crashing the computer. However, when estimating the
required RAM, <code>PMC.RAM</code> actually creates several large
objects, such as <code>post</code> (see below). If too many iterations are
given as an argument to <code>PMC.RAM</code>, for example, then it
will crash the computer while trying to estimate the required RAM.
</p>
<p>The best way to use this function is as follows. First, prepare the
model specification and list of data. Second, observe how much RAM the
computer is using at the moment, as well as the maximum available
RAM. The majority of the difference of these two is the amount of RAM
the computer may dedicate to updating the model. Next, use this
function with a small number of iterations. Note the estimated
RAM. Increase the number of iterations, and again note the
RAM. Continue to increase the number of iterations until, say,
arbitrarily within 90% of the above-mentioned difference in RAM.
</p>
<p>The computer operating system uses RAM, as does any other software
running at the moment. R is currently using RAM, and other functions
in the <code>LaplacesDemon</code> package, and any other package that is
currently activated, are using RAM. There are numerous small objects
that are not included in the returned list, that use RAM. For example,
perplexity is a small vector, etc.
</p>
<p>A potentially large objects that is not included is a matrix used for
estimating <code><a href="#topic+LML">LML</a></code>.
</p>


<h3>Value</h3>

<p><code>PMC.RAM</code> returns a list with several components. Each component
is an estimate in MB for an object. The list has the following
components:
</p>
<table>
<tr><td><code>alpha</code></td>
<td>
<p>This is the estimated size in MB of RAM required for the
matrix of mixture probabilities by iteration.</p>
</td></tr>
<tr><td><code>Covar</code></td>
<td>
<p>This is the estimated size in MB of RAM required for the
covariance matrix or matrices.</p>
</td></tr>
<tr><td><code>Data</code></td>
<td>
<p>This is the estimated size in MB of RAM required for the
list of data.</p>
</td></tr>
<tr><td><code>Deviance</code></td>
<td>
<p>This is the estimated size in MB of RAM required for
the deviance vector before thinning.</p>
</td></tr>
<tr><td><code>Initial.Values</code></td>
<td>
<p>This is the estimated size in MB of RAM required
for the matrix or vector of initial values.</p>
</td></tr>
<tr><td><code>LH</code></td>
<td>
<p>This is the estimated size in MB of RAM required for the
<code class="reqn">N \times T \times M</code> array <code>LH</code>, where <code class="reqn">N</code>
is the number of samples, <code class="reqn">T</code> is the number of iterations, and
<code class="reqn">M</code> is the number of mixture components. The <code>LH</code> array is
not returned by <code><a href="#topic+PMC">PMC</a></code>.</p>
</td></tr>
<tr><td><code>LP</code></td>
<td>
<p>This is the estimated size in MB of RAM required for the
<code class="reqn">N \times T \times M</code> array <code>LP</code>, where <code class="reqn">N</code>
is the number of samples, <code class="reqn">T</code> is the number of iterations, and
<code class="reqn">M</code> is the number of mixture components. The <code>LP</code> array is
not returned by <code><a href="#topic+PMC">PMC</a></code>.</p>
</td></tr>
<tr><td><code>Model</code></td>
<td>
<p>This is the estimated size in MB of RAM required for the
model specification function.</p>
</td></tr>
<tr><td><code>Monitor</code></td>
<td>
<p>This is the estimated size in MB of RAM required for
the <code class="reqn">N \times J</code> matrix <code>Monitor</code>, where <code class="reqn">N</code> is
the number of unthinned samples and J is the number of monitored
variables. Although it is thinned later in the algorithm, the full
matrix is created.</p>
</td></tr>
<tr><td><code>Posterior1</code></td>
<td>
<p>This is the estimated size in MB of RAM required for
the <code class="reqn">N \times J \times T \times M</code> array
<code>Posterior1</code>, where <code class="reqn">N</code> is the number of samples, <code class="reqn">J</code>
is the number of parameters, <code class="reqn">T</code> is the number of iterations,
and <code class="reqn">M</code> is the number of mixture components.</p>
</td></tr>
<tr><td><code>Posterior2</code></td>
<td>
<p>This is the estimated size in MB of RAM required for
the <code class="reqn">N \times J</code> matrix <code>Posterior2</code>, where <code class="reqn">N</code>
is the number of samples and <code class="reqn">J</code> is the number of initial values
or parameters. Although this is thinned later, at one point it is
un-thinned.</p>
</td></tr>
<tr><td><code>Summary</code></td>
<td>
<p>This is the estimated size in MB of RAM required for
the summary table.</p>
</td></tr>
<tr><td><code>W</code></td>
<td>
<p>This is the estimated size in MB of RAM required for the
matrix of importance weights.</p>
</td></tr>
<tr><td><code>Total</code></td>
<td>
<p>This is the estimated size in MB of RAM required in total
to update with <code><a href="#topic+PMC">PMC</a></code> for a given model and data, and for
a number of iterations, specified thinning, mixture components, and
number of samples.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+BigData">BigData</a></code>,
<code><a href="#topic+LML">LML</a></code>,
<code><a href="utils.html#topic+object.size">object.size</a></code>, and
<code><a href="#topic+PMC">PMC</a></code>.
</p>

<hr>
<h2 id='PosteriorChecks'>Posterior Checks</h2><span id='topic+PosteriorChecks'></span>

<h3>Description</h3>

<p>Not to be confused with posterior predictive checks, this function
provides additional information about the marginal posterior
distributions of continuous parameters, such as the probability that
each posterior coefficient of the parameters (referred to generically
as <code class="reqn">\theta</code>), is greater than zero
[<code class="reqn">p(\theta &gt; 0)</code>], the estimated number of modes,
the kurtosis and skewness of the posterior distributions, the burn-in
of each chain (for MCMC only), integrated autocorrelation time,
independent samples per minute, and acceptance rate. A posterior
correlation matrix is provided only for objects of class
<code>demonoid</code> or <code>pmc</code>.
</p>
<p>For discrete parameters, see the <code><a href="#topic+Hangartner.Diagnostic">Hangartner.Diagnostic</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PosteriorChecks(x, Parms)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="PosteriorChecks_+3A_x">x</code></td>
<td>
<p>This required argument accepts an object of class
<code>demonoid</code>, <code>iterquad</code>, <code>laplace</code>, <code>pmc</code>, or
<code>vb</code>.</p>
</td></tr>
<tr><td><code id="PosteriorChecks_+3A_parms">Parms</code></td>
<td>
<p>This argument accepts a vector of quoted strings to be
matched for selecting parameters. This argument defaults to
<code>NULL</code> and selects every parameter. Each quoted string is
matched to one or more parameter names with the <code>grep</code>
function. For example, if the user specifies <code>Parms=c("eta",
      "tau")</code>, and if the parameter names are beta[1], beta[2], eta[1],
eta[2], and tau, then all parameters will be selected, because the
string <code>eta</code> is within <code>beta</code>. Since <code>grep</code> is used,
string matching uses regular expressions, so beware of
meta-characters, though these are acceptable: &quot;.&quot;, &quot;[&quot;, and &quot;]&quot;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>PosteriorChecks</code> is a supplemental function that returns
a list with two components. Following is a summary of popular uses of
the <code>PosteriorChecks</code> function.
</p>
<p>First (and only for MCMC users), the user may be considering the
current MCMC algorithm versus others. In this case, the
<code>PosteriorChecks</code> function is often used to find the two MCMC
chains with the highest <code><a href="#topic+IAT">IAT</a></code>, and these chains are
studied for non-randomness with a joint trace plot, via the
<code><a href="#topic+joint.density.plot">joint.density.plot</a></code> function. The best algorithm has the
chains with the highest independent samples per minute (ISM).
</p>
<p>Posterior correlation may be studied between model updates as well as
after a model seems to have converged. While frequentists consider
multicollinear predictor variables, Bayesians tend to consider
posterior correlation of the parameters. Models with multicollinear
parameters take more iterations to converge. Hierarchical models often
have high posterior correlations. Posterior correlation often
contributes to a lower effective sample size (<code><a href="#topic+ESS">ESS</a></code>).
Common remedies include transforming the predictors,
re-parameterization to reduce posterior correlation, using WIPs
(Weakly-Informative Priors), or selecting a different numerical
approximation algorithm. An example of re-parameterization is to
constrain related parameters to sum to zero. Another approach is to
specify the parameters according to a multivariate distribution that
is assisted by estimating a covariance matrix. Some algorithms are
more robust to posterior correlation than others. For example,
posterior correlation should generally be less problematic for twalk
than AMWG in <code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>. Posterior correlation may be
plotted with the <code><a href="#topic+plotMatrix">plotMatrix</a></code> function, and may be useful
for blocking parameters. For more information on blockwise sampling,
see the <code><a href="#topic+Blocks">Blocks</a></code> function.
</p>
<p>After a user is convinced of the applicability of the current MCMC
algorithm, and that the chains have converged, <code>PosteriorChecks</code>
is often used to identify multimodal marginal posterior distributions
for further study or model re-specification.
</p>
<p>Although many marginal posterior distributions appear normally
distributed, there is no such assumption. Nonetheless, a marginal
posterior distribution tends to be distributed the same as its prior
distribution. If a parameter has a prior specified with a Laplace
distribution, then the marginal posterior distribution tends also to
be Laplace-distributed. In the common case of normality, kurtosis
and skewness may be used to identify discrepancies between the prior
and posterior, and perhaps this should be called a 'prior-posterior
check'.
</p>
<p>Lastly, parameter importance may be considered, in which case it is
recommended to be considered simultaneously with variable importance
from the <code><a href="#topic+Importance">Importance</a></code> function.
</p>


<h3>Value</h3>

<p><code>PosteriorChecks</code> returns an object of class
<code>posteriorchecks</code> that is a list with the following components:
</p>
<table>
<tr><td><code>Posterior.Correlation</code></td>
<td>

<p>This is a correlation matrix of the parameters selected with the
<code>Parms</code> argument. This component is returned as <code>NA</code> for
objects of classes <code>"laplace"</code> or <code>"vb"</code>.</p>
</td></tr>
<tr><td><code>Posterior.Summary</code></td>
<td>
<p>This is a matrix in which each row is a
parameter and there are eight columns: p(theta &gt; 0), N.Modes,
Kurtosis, Skewness, Burn-In, IAT, ISM, and AR. The first column,
p(theta &gt; 0), indicates parameter importance by reporting how much
of the distribution is greater than zero. An important parameter
distribution will have a result at least as extreme as 0.025 or
0.975, and an unimportant parameter distribution is centered at
0.5. This is not the importance of the associated variable relative
to how well the model fits the data. For variable importance, see
the <code><a href="#topic+Importance">Importance</a></code> function. The second column, N.Modes,
is the number of modes, estimated with the <code><a href="#topic+Modes">Modes</a></code>
function. Kurtosis and skewness are useful posterior checks that may
suggest that a posterior distribution is non-normal or does not fit
well with a distributional assumption, assuming a distributional
assumption exists, which it may not. The burn-in is estimated for
each chain (only for objects of class <code>demonoid</code> with the
<code><a href="#topic+burnin">burnin</a></code> function. The integrated autocorrelation
time is estimated with <code><a href="#topic+IAT">IAT</a></code>. The number of independent
samples per minute (ISM) is calculated for objects of class
<code>"demonoid"</code> as <code><a href="#topic+ESS">ESS</a></code> divided by minutes. Lastly,
the local acceptance rate of each MCMC chain is calculated with the
<code><a href="#topic+AcceptanceRate">AcceptanceRate</a></code> function, and is set to 1 for objects
of class <code>iterquad</code>, <code>laplace</code>, <code>pmc</code>, or <code>vb</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+AcceptanceRate">AcceptanceRate</a></code>,
<code><a href="#topic+Blocks">Blocks</a></code>,
<code><a href="#topic+burnin">burnin</a></code>,
<code><a href="#topic+ESS">ESS</a></code>,
<code><a href="#topic+Hangartner.Diagnostic">Hangartner.Diagnostic</a></code>,
<code><a href="#topic+joint.density.plot">joint.density.plot</a></code>,
<code><a href="#topic+IAT">IAT</a></code>,
<code><a href="#topic+Importance">Importance</a></code>,
<code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code>,
<code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>,
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>,
<code><a href="#topic+Modes">Modes</a></code>,
<code><a href="#topic+plotMatrix">plotMatrix</a></code>,
<code><a href="#topic+PMC">PMC</a></code>, and
<code><a href="#topic+VariationalBayes">VariationalBayes</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### See the LaplacesDemon function for an example.</code></pre>

<hr>
<h2 id='Precision'>Precision</h2><span id='topic+Cov2Prec'></span><span id='topic+Prec2Cov'></span><span id='topic+prec2sd'></span><span id='topic+prec2var'></span><span id='topic+sd2prec'></span><span id='topic+sd2var'></span><span id='topic+var2prec'></span><span id='topic+var2sd'></span>

<h3>Description</h3>

<p>Bayesians often use precision rather than variance. These are elementary
utility functions to facilitate conversions between precision,
standard deviation, and variance regarding scalars, vectors, and
matrices, and these functions are designed for those who are new to
Bayesian inference. The names of these functions consist of two
different scale parameters, separated by a &lsquo;2&rsquo;, and capital letters
refer to matrices while lower case letters refer to scalars and
vectors. For example, the <code>Prec2Cov</code> function converts a
precision matrix to a covariance matrix, while the <code>prec2sd</code>
function converts a scalar or vector of precision parameters to
standard deviation parameters.
</p>
<p>The modern Bayesian use of precision developed because it was more
straightforward in a normal distribution to estimate precision
<code class="reqn">\tau</code> with a gamma distribution as a conjugate prior, than
to estimate <code class="reqn">\sigma^2</code> with an inverse-gamma distribution
as a conjugate prior. Today, conjugacy is usually considered to be
merely a convenience, and in this example, a non-conjugate half-Cauchy
prior distribution is recommended as a weakly informative prior
distribution for scale parameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Cov2Prec(Cov)
Prec2Cov(Prec)
prec2sd(prec=1)
prec2var(prec=1)
sd2prec(sd=1)
sd2var(sd=1)
var2prec(var=1)
var2sd(var=1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Precision_+3A_cov">Cov</code></td>
<td>
<p>This is a covariance matrix, usually represented as
<code class="reqn">\Sigma</code>.</p>
</td></tr>
<tr><td><code id="Precision_+3A_prec">Prec</code></td>
<td>
<p>This is a precision matrix, usually represented as
<code class="reqn">\Omega</code>.</p>
</td></tr>
<tr><td><code id="Precision_+3A_prec">prec</code></td>
<td>
<p>This is a precision scalar or vector, usually represented as
<code class="reqn">\tau</code>.</p>
</td></tr>
<tr><td><code id="Precision_+3A_sd">sd</code></td>
<td>
<p>This is a standard deviation scalar or vector, usually
represented as <code class="reqn">\sigma</code>.</p>
</td></tr>
<tr><td><code id="Precision_+3A_var">var</code></td>
<td>
<p>This is a variance scalar or vector, usually
represented as <code class="reqn">\sigma^2</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Bayesians often use precision rather than variance, where
precision is the inverse of the variance. For example, a linear
regression may be represented equivalently as <code class="reqn">\textbf{y} \sim
    \mathcal{N}(\mu, \sigma^2)</code>, or <code class="reqn">\textbf{y}
    \sim \mathcal{N}(\mu, \tau^{-1})</code>, where
<code class="reqn">\sigma^2</code> is the variance, and <code class="reqn">\tau</code> is the
precision, which is the inverse of the variance.</p>


<h3>Value</h3>

<table>
<tr><td><code>Cov2Prec</code></td>
<td>

<p>This returns a precision matrix, <code class="reqn">\Omega</code>, from a
covariance matrix, <code class="reqn">\Sigma</code>, where <code class="reqn">\Omega =
    \Sigma^{-1}</code>.</p>
</td></tr>
<tr><td><code>Prec2Cov</code></td>
<td>

<p>This returns a covariance matrix, <code class="reqn">\Sigma</code>, from a
precision matrix, <code class="reqn">\Omega</code>, where <code class="reqn">\Sigma =
    \Omega^{-1}</code>.</p>
</td></tr>
<tr><td><code>prec2sd</code></td>
<td>

<p>This returns a standard deviation, <code class="reqn">\sigma</code>, from a
precision, <code class="reqn">\tau</code>, where <code class="reqn">\sigma =
    \sqrt{\tau^{-1}}</code>.</p>
</td></tr>
<tr><td><code>prec2var</code></td>
<td>

<p>This returns a variance, <code class="reqn">\sigma^2</code>, from a precision,
<code class="reqn">\tau</code>, where <code class="reqn">\sigma^2 = \tau^{-1}</code>.</p>
</td></tr>
<tr><td><code>sd2prec</code></td>
<td>

<p>This returns a precision, <code class="reqn">\tau</code>, from a standard
deviation, <code class="reqn">\sigma</code>, where <code class="reqn">\tau = \sigma^{-2}</code>.</p>
</td></tr>
<tr><td><code>sd2var</code></td>
<td>

<p>This returns a variance, <code class="reqn">\sigma^2</code>, from a standard
deviation, <code class="reqn">\sigma</code>, where <code class="reqn">\sigma^2 = \sigma
    \sigma</code>.</p>
</td></tr>
<tr><td><code>var2prec</code></td>
<td>

<p>This returns a precision, <code class="reqn">\tau</code>, from a variance,
<code class="reqn">\sigma^2</code>, where <code class="reqn">\tau = \frac{1}{\sigma^2}</code>.</p>
</td></tr>
<tr><td><code>var2sd</code></td>
<td>

<p>This returns a standard deviation, <code class="reqn">\sigma</code>, from a
variance, <code class="reqn">\sigma^2</code>, where <code class="reqn">\sigma =
    \sqrt{\sigma^2}</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+Cov2Cor">Cov2Cor</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
Cov2Prec(matrix(c(1,0.1,0.1,1),2,2))
Prec2Cov(matrix(c(1,0.1,0.1,1),2,2))
prec2sd(0.5)
prec2var(0.5)
sd2prec(1.4142)
sd2var(01.4142)
var2prec(2)
var2sd(2)
</code></pre>

<hr>
<h2 id='predict.demonoid'>Posterior Predictive Checks</h2><span id='topic+predict.demonoid'></span>

<h3>Description</h3>

<p>This may be used to predict either new, unobserved instances of
<code class="reqn">\textbf{y}</code> (called <code class="reqn">\textbf{y}^{new}</code>) or
replicates of <code class="reqn">\textbf{y}</code> (called
<code class="reqn">\textbf{y}^{rep}</code>), and then perform posterior
predictive checks. Either <code class="reqn">\textbf{y}^{new}</code> or
<code class="reqn">\textbf{y}^{rep}</code> is predicted given an object of
class <code>demonoid</code>, the model specification, and data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'demonoid'
predict(object, Model, Data, CPUs=1, Type="PSOCK", ...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.demonoid_+3A_object">object</code></td>
<td>
<p>An object of class <code>demonoid</code> is required.</p>
</td></tr>
<tr><td><code id="predict.demonoid_+3A_model">Model</code></td>
<td>
<p>The model specification function is required.</p>
</td></tr>
<tr><td><code id="predict.demonoid_+3A_data">Data</code></td>
<td>
<p>A data set in a list is required. The dependent variable
is required to be named either <code>y</code> or <code>Y</code>.</p>
</td></tr>
<tr><td><code id="predict.demonoid_+3A_cpus">CPUs</code></td>
<td>
<p>This argument accepts an integer that specifies the number
of central processing units (CPUs) of the multicore computer or
computer cluster. This argument defaults to <code>CPUs=1</code>, in which
parallel processing does not occur.</p>
</td></tr>
<tr><td><code id="predict.demonoid_+3A_type">Type</code></td>
<td>
<p>This argument specifies the type of parallel processing to
perform, accepting either <code>Type="PSOCK"</code> or
<code>Type="MPI"</code>.</p>
</td></tr>
<tr><td><code id="predict.demonoid_+3A_...">...</code></td>
<td>
<p>Additional arguments are unused.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function passes each iteration of marginal posterior samples
along with data to <code>Model</code>, where the fourth component in the
return list is labeled <code>yhat</code>, and is a vector of expectations of
<code class="reqn">\textbf{y}</code>, given the samples, model specification, and
data. Stationary samples are used if detected, otherwise
non-stationary samples will be used. To predict
<code class="reqn">\textbf{y}^{rep}</code>, simply supply the data set used to
estimate the model. To predict <code class="reqn">\textbf{y}^{new}</code>, supply
a new data set instead (though for some model specifications, this
cannot be done, and <code class="reqn">\textbf{y}_{new}</code> must be specified
in the <code>Model</code> function). If the new data set does not have
<code class="reqn">\textbf{y}</code>, then create <code>y</code> in the list and set it
equal to something sensible, such as <code>mean(y)</code> from the
original data set.
</p>
<p>The variable <code>y</code> must be a vector. If instead it is matrix
<code>Y</code>, then it will be converted to vector <code>y</code>. The vectorized
length of <code>y</code> or <code>Y</code> must be equal to the vectorized length
of <code>yhat</code>, the fourth component of the return list of the
<code>Model</code> function.
</p>
<p>Parallel processing may be performed when the user specifies
<code>CPUs</code> to be greater than one, implying that the specified number
of CPUs exists and is available. Parallelization may be performed on a
multicore computer or a computer cluster. Either a Simple Network of
Workstations (SNOW) or Message Passing Interface is used (MPI). With
small data sets and few samples, parallel processing may be slower,
due to computer network communication. With larger data sets and more
samples, the user should experience a faster run-time.
</p>
<p>For more information on posterior predictive checks, see
<a href="https://web.archive.org/web/20150215050702/http://www.bayesian-inference.com/posteriorpredictivechecks">https://web.archive.org/web/20150215050702/http://www.bayesian-inference.com/posteriorpredictivechecks</a>.
</p>


<h3>Value</h3>

<p>This function returns an object of class <code>demonoid.ppc</code> (where
ppc stands for posterior predictive checks). The returned object is
a list with the following components:
</p>
<table>
<tr><td><code>y</code></td>
<td>
<p>This stores the vectorized form of <code class="reqn">\textbf{y}</code>, the
dependent variable.</p>
</td></tr>
<tr><td><code>yhat</code></td>
<td>
<p>This is a <code class="reqn">N \times S</code> matrix, where <code class="reqn">N</code> is
the number of records of <code class="reqn">\textbf{y}</code> and <code class="reqn">S</code> is the
number of posterior samples.</p>
</td></tr>
<tr><td><code>Deviance</code></td>
<td>
<p>This is a vector of predictive deviance.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Statisticat, LLC.</p>


<h3>See Also</h3>

<p><code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>
</p>

<hr>
<h2 id='predict.iterquad'>Posterior Predictive Checks</h2><span id='topic+predict.iterquad'></span>

<h3>Description</h3>

<p>This may be used to predict either new, unobserved instances of
<code class="reqn">\textbf{y}</code> (called <code class="reqn">\textbf{y}^{new}</code>) or
replicates of <code class="reqn">\textbf{y}</code> (called
<code class="reqn">\textbf{y}^{rep}</code>), and then perform posterior
predictive checks. Either <code class="reqn">\textbf{y}^{new}</code> or
<code class="reqn">\textbf{y}^{rep}</code> is predicted given an object of
class <code>iterquad</code>, the model specification, and data. This
function requires that posterior samples were produced with
<code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'iterquad'
predict(object, Model, Data, CPUs=1, Type="PSOCK", ...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.iterquad_+3A_object">object</code></td>
<td>
<p>An object of class <code>iterquad</code> is required.</p>
</td></tr>
<tr><td><code id="predict.iterquad_+3A_model">Model</code></td>
<td>
<p>The model specification function is required.</p>
</td></tr>
<tr><td><code id="predict.iterquad_+3A_data">Data</code></td>
<td>
<p>A data set in a list is required. The dependent
variable is required to be named either <code>y</code> or <code>Y</code>.</p>
</td></tr>
<tr><td><code id="predict.iterquad_+3A_cpus">CPUs</code></td>
<td>
<p>This argument accepts an integer that specifies the number
of central processing units (CPUs) of the multicore computer or
computer cluster. This argument defaults to <code>CPUs=1</code>, in which
parallel processing does not occur.</p>
</td></tr>
<tr><td><code id="predict.iterquad_+3A_type">Type</code></td>
<td>
<p>This argument specifies the type of parallel processing to
perform, accepting either <code>Type="PSOCK"</code> or
<code>Type="MPI"</code>.</p>
</td></tr>
<tr><td><code id="predict.iterquad_+3A_...">...</code></td>
<td>
<p>Additional arguments are unused.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Since iterative quadrature characterizes marginal posterior
distributions with means and variances, and posterior predictive
checks involve samples, the <code>predict.iterquad</code> function requires
the use of independent samples of the marginal posterior
distributions, provided by <code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code> when
<code>sir=TRUE</code>.
</p>
<p>The samples of the marginal posterior distributions of the target
distributions (the parameters) are passed along with the data to the
<code>Model</code> specification and used to draw samples from the deviance
and monitored variables. At the same time, the fourth component in the
returned list, which is labeled <code>yhat</code>, is a vector of
expectations of <code class="reqn">\textbf{y}</code>, given the samples, model
specification, and data. To predict <code class="reqn">\textbf{y}^{rep}</code>,
simply supply the data set used to estimate the model. To predict
<code class="reqn">\textbf{y}^{new}</code>, supply a new data set instead (though
for some model specifications, this cannot be done, and
<code class="reqn">\textbf{y}_{new}</code> must be specified in the <code>Model</code>
function). If the new data set does not have <code class="reqn">\textbf{y}</code>, then
create <code>y</code> in the list and set it equal to something sensible,
such as <code>mean(y)</code> from the original data set.
</p>
<p>The variable <code>y</code> must be a vector. If instead it is matrix
<code>Y</code>, then it will be converted to vector <code>y</code>. The vectorized
length of <code>y</code> or <code>Y</code> must be equal to the vectorized length
of <code>yhat</code>, the fourth component of the returned list of the
<code>Model</code> function.
</p>
<p>Parallel processing may be performed when the user specifies
<code>CPUs</code> to be greater than one, implying that the specified number
of CPUs exists and is available. Parallelization may be performed on a
multicore computer or a computer cluster. Either a Simple Network of
Workstations (SNOW) or Message Passing Interface is used (MPI). With
small data sets and few samples, parallel processing may be slower,
due to computer network communication. With larger data sets and more
samples, the user should experience a faster run-time.
</p>
<p>For more information on posterior predictive checks, see
<a href="https://web.archive.org/web/20150215050702/http://www.bayesian-inference.com/posteriorpredictivechecks">https://web.archive.org/web/20150215050702/http://www.bayesian-inference.com/posteriorpredictivechecks</a>.
</p>


<h3>Value</h3>

<p>This function returns an object of class <code>iterquad.ppc</code> (where
&ldquo;ppc&rdquo; stands for posterior predictive checks). The returned object
is a list with the following components:
</p>
<table>
<tr><td><code>y</code></td>
<td>

<p>This stores <code class="reqn">\textbf{y}</code>, the dependent variable.</p>
</td></tr>
<tr><td><code>yhat</code></td>
<td>

<p>This is a <code class="reqn">N \times S</code> matrix, where <code class="reqn">N</code> is the
number of records of <code class="reqn">\textbf{y}</code> and <code class="reqn">S</code> is the number of 
posterior samples.</p>
</td></tr>
<tr><td><code>Deviance</code></td>
<td>

<p>This is a vector of length <code class="reqn">S</code>, where <code class="reqn">S</code> is the number of
independent posterior samples. Samples are obtained with the
sampling importance resampling algorithm, <code><a href="#topic+SIR">SIR</a></code>.</p>
</td></tr>
<tr><td><code>monitor</code></td>
<td>

<p>This is a <code class="reqn">N \times S</code> matrix, where <code class="reqn">N</code> is the
number of monitored variables and <code class="reqn">S</code> is the number of independent
posterior samples. Samples are obtained with the sampling importance
resampling algorithm, <code><a href="#topic+SIR">SIR</a></code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Statisticat, LLC.</p>


<h3>See Also</h3>

<p><code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code> and
<code><a href="#topic+SIR">SIR</a></code>.
</p>

<hr>
<h2 id='predict.laplace'>Posterior Predictive Checks</h2><span id='topic+predict.laplace'></span>

<h3>Description</h3>

<p>This may be used to predict either new, unobserved instances of
<code class="reqn">\textbf{y}</code> (called <code class="reqn">\textbf{y}^{new}</code>) or
replicates of <code class="reqn">\textbf{y}</code> (called
<code class="reqn">\textbf{y}^{rep}</code>), and then perform posterior
predictive checks. Either <code class="reqn">\textbf{y}^{new}</code> or
<code class="reqn">\textbf{y}^{rep}</code> is predicted given an object of
class <code>laplace</code>, the model specification, and data. This function
requires that posterior samples were produced with
<code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'laplace'
predict(object, Model, Data, CPUs=1, Type="PSOCK", ...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.laplace_+3A_object">object</code></td>
<td>
<p>An object of class <code>laplace</code> is required.</p>
</td></tr>
<tr><td><code id="predict.laplace_+3A_model">Model</code></td>
<td>
<p>The model specification function is required.</p>
</td></tr>
<tr><td><code id="predict.laplace_+3A_data">Data</code></td>
<td>
<p>A data set in a list is required. The dependent
variable is required to be named either <code>y</code> or <code>Y</code>.</p>
</td></tr>
<tr><td><code id="predict.laplace_+3A_cpus">CPUs</code></td>
<td>
<p>This argument accepts an integer that specifies the number
of central processing units (CPUs) of the multicore computer or
computer cluster. This argument defaults to <code>CPUs=1</code>, in which
parallel processing does not occur.</p>
</td></tr>
<tr><td><code id="predict.laplace_+3A_type">Type</code></td>
<td>
<p>This argument specifies the type of parallel processing to
perform, accepting either <code>Type="PSOCK"</code> or
<code>Type="MPI"</code>.</p>
</td></tr>
<tr><td><code id="predict.laplace_+3A_...">...</code></td>
<td>
<p>Additional arguments are unused.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Since Laplace Approximation characterizes marginal posterior
distributions with modes and variances, and posterior predictive
checks involve samples, the <code>predict.laplace</code> function requires
the use of independent samples of the marginal posterior
distributions, provided by <code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code> when
<code>sir=TRUE</code>.
</p>
<p>The samples of the marginal posterior distributions of the target
distributions (the parameters) are passed along with the data to the
<code>Model</code> specification and used to draw samples from the deviance
and monitored variables. At the same time, the fourth component in the
returned list, which is labeled <code>yhat</code>, is a vector of
expectations of <code class="reqn">\textbf{y}</code>, given the samples, model
specification, and data. To predict <code class="reqn">\textbf{y}^{rep}</code>,
simply supply the data set used to estimate the model. To predict
<code class="reqn">\textbf{y}^{new}</code>, supply a new data set instead (though
for some model specifications, this cannot be done, and
<code class="reqn">\textbf{y}_{new}</code> must be specified in the <code>Model</code>
function). If the new data set does not have <code class="reqn">\textbf{y}</code>, then
create <code>y</code> in the list and set it equal to something sensible,
such as <code>mean(y)</code> from the original data set.
</p>
<p>The variable <code>y</code> must be a vector. If instead it is matrix
<code>Y</code>, then it will be converted to vector <code>y</code>. The vectorized
length of <code>y</code> or <code>Y</code> must be equal to the vectorized length
of <code>yhat</code>, the fourth component of the returned list of the
<code>Model</code> function.
</p>
<p>Parallel processing may be performed when the user specifies
<code>CPUs</code> to be greater than one, implying that the specified number
of CPUs exists and is available. Parallelization may be performed on a
multicore computer or a computer cluster. Either a Simple Network of
Workstations (SNOW) or Message Passing Interface is used (MPI). With
small data sets and few samples, parallel processing may be slower,
due to computer network communication. With larger data sets and more
samples, the user should experience a faster run-time.
</p>
<p>For more information on posterior predictive checks, see
<a href="https://web.archive.org/web/20150215050702/http://www.bayesian-inference.com/posteriorpredictivechecks">https://web.archive.org/web/20150215050702/http://www.bayesian-inference.com/posteriorpredictivechecks</a>.
</p>


<h3>Value</h3>

<p>This function returns an object of class <code>laplace.ppc</code> (where
&ldquo;ppc&rdquo; stands for posterior predictive checks). The returned object
is a list with the following components:
</p>
<table>
<tr><td><code>y</code></td>
<td>

<p>This stores <code class="reqn">\textbf{y}</code>, the dependent variable.</p>
</td></tr>
<tr><td><code>yhat</code></td>
<td>

<p>This is a <code class="reqn">N \times S</code> matrix, where <code class="reqn">N</code> is the
number of records of <code class="reqn">\textbf{y}</code> and <code class="reqn">S</code> is the number of 
posterior samples.</p>
</td></tr>
<tr><td><code>Deviance</code></td>
<td>

<p>This is a vector of length <code class="reqn">S</code>, where <code class="reqn">S</code> is the number of
independent posterior samples. Samples are obtained with the
sampling importance resampling algorithm, <code><a href="#topic+SIR">SIR</a></code>.</p>
</td></tr>
<tr><td><code>monitor</code></td>
<td>

<p>This is a <code class="reqn">N \times S</code> matrix, where <code class="reqn">N</code> is the
number of monitored variables and <code class="reqn">S</code> is the number of independent
posterior samples. Samples are obtained with the sampling importance
resampling algorithm, <code><a href="#topic+SIR">SIR</a></code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Statisticat, LLC.</p>


<h3>See Also</h3>

<p><code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code> and
<code><a href="#topic+SIR">SIR</a></code>.
</p>

<hr>
<h2 id='predict.pmc'>Posterior Predictive Checks</h2><span id='topic+predict.pmc'></span>

<h3>Description</h3>

<p>This may be used to predict either new, unobserved instances of
<code class="reqn">\textbf{y}</code> (called <code class="reqn">\textbf{y}^{new}</code>) or
replicates of <code class="reqn">\textbf{y}</code> (called
<code class="reqn">\textbf{y}^{rep}</code>), and then perform posterior
predictive checks. Either <code class="reqn">\textbf{y}^{new}</code> or
<code class="reqn">\textbf{y}^{rep}</code> is predicted given an object of
class <code>demonoid</code>, the model specification, and data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'pmc'
predict(object, Model, Data, CPUs=1, Type="PSOCK", ...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.pmc_+3A_object">object</code></td>
<td>
<p>An object of class <code>pmc</code> is required.</p>
</td></tr>
<tr><td><code id="predict.pmc_+3A_model">Model</code></td>
<td>
<p>The model specification function is required.</p>
</td></tr>
<tr><td><code id="predict.pmc_+3A_data">Data</code></td>
<td>
<p>A data set in a list is required. The dependent variable
is required to be named either <code>y</code> or <code>Y</code>.</p>
</td></tr>
<tr><td><code id="predict.pmc_+3A_cpus">CPUs</code></td>
<td>
<p>This argument accepts an integer that specifies the number
of central processing units (CPUs) of the multicore computer or
computer cluster. This argument defaults to <code>CPUs=1</code>, in which
parallel processing does not occur.</p>
</td></tr>
<tr><td><code id="predict.pmc_+3A_type">Type</code></td>
<td>
<p>This argument specifies the type of parallel processing to
perform, accepting either <code>Type="PSOCK"</code> or
<code>Type="MPI"</code>.</p>
</td></tr>
<tr><td><code id="predict.pmc_+3A_...">...</code></td>
<td>
<p>Additional arguments are unused.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function passes each iteration of marginal posterior samples
along with data to <code>Model</code>, where the fourth component in the
return list is labeled <code>yhat</code>, and is a vector of expectations of
<code class="reqn">\textbf{y}</code>, given the samples, model specification, and
data. Stationary samples are used if detected, otherwise
non-stationary samples will be used. To predict
<code class="reqn">\textbf{y}^{rep}</code>, simply supply the data set used to
estimate the model. To predict <code class="reqn">\textbf{y}^{new}</code>, supply
a new data set instead (though for some model specifications, this
cannot be done, and <code class="reqn">\textbf{y}_{new}</code> must be specified
in the <code>Model</code> function). If the new data set does not have
<code class="reqn">\textbf{y}</code>, then create <code>y</code> in the list and set it
equal to something sensible, such as <code>mean(y)</code> from the
original data set.
</p>
<p>The variable <code>y</code> must be a vector. If instead it is matrix
<code>Y</code>, then it will be converted to vector <code>y</code>. The vectorized
length of <code>y</code> or <code>Y</code> must be equal to the vectorized length
of <code>yhat</code>, the fourth component of the return list of the
<code>Model</code> function.
</p>
<p>Parallel processing may be performed when the user specifies
<code>CPUs</code> to be greater than one, implying that the specified number
of CPUs exists and is available. Parallelization may be performed on a
multicore computer or a computer cluster. Either a Simple Network of
Workstations (SNOW) or Message Passing Interface is used (MPI). With
small data sets and few samples, parallel processing may be slower,
due to computer network communication. With larger data sets and more
samples, the user should experience a faster run-time.
</p>
<p>For more information on posterior predictive checks, see
<a href="https://web.archive.org/web/20150215050702/http://www.bayesian-inference.com/posteriorpredictivechecks">https://web.archive.org/web/20150215050702/http://www.bayesian-inference.com/posteriorpredictivechecks</a>.
</p>


<h3>Value</h3>

<p>This function returns an object of class <code>pmc.ppc</code> (where
ppc stands for posterior predictive checks). The returned object is
a list with the following components:
</p>
<table>
<tr><td><code>y</code></td>
<td>
<p>This stores the vectorized form of <code class="reqn">\textbf{y}</code>, the
dependent variable.</p>
</td></tr>
<tr><td><code>yhat</code></td>
<td>
<p>This is a <code class="reqn">N \times S</code> matrix, where <code class="reqn">N</code> is
the number of records of <code class="reqn">\textbf{y}</code> and <code class="reqn">S</code> is the
number of posterior samples.</p>
</td></tr>
<tr><td><code>Deviance</code></td>
<td>
<p>This is a vector of predictive deviance.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Statisticat, LLC.</p>


<h3>See Also</h3>

<p><code><a href="#topic+PMC">PMC</a></code>
</p>

<hr>
<h2 id='predict.vb'>Posterior Predictive Checks</h2><span id='topic+predict.vb'></span>

<h3>Description</h3>

<p>This may be used to predict either new, unobserved instances of
<code class="reqn">\textbf{y}</code> (called <code class="reqn">\textbf{y}^{new}</code>) or
replicates of <code class="reqn">\textbf{y}</code> (called
<code class="reqn">\textbf{y}^{rep}</code>), and then perform posterior
predictive checks. Either <code class="reqn">\textbf{y}^{new}</code> or
<code class="reqn">\textbf{y}^{rep}</code> is predicted given an object of
class <code>vb</code>, the model specification, and data. This function
requires that posterior samples were produced with
<code><a href="#topic+VariationalBayes">VariationalBayes</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'vb'
predict(object, Model, Data, CPUs=1, Type="PSOCK", ...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.vb_+3A_object">object</code></td>
<td>
<p>An object of class <code>vb</code> is required.</p>
</td></tr>
<tr><td><code id="predict.vb_+3A_model">Model</code></td>
<td>
<p>The model specification function is required.</p>
</td></tr>
<tr><td><code id="predict.vb_+3A_data">Data</code></td>
<td>
<p>A data set in a list is required. The dependent
variable is required to be named either <code>y</code> or <code>Y</code>.</p>
</td></tr>
<tr><td><code id="predict.vb_+3A_cpus">CPUs</code></td>
<td>
<p>This argument accepts an integer that specifies the number
of central processing units (CPUs) of the multicore computer or
computer cluster. This argument defaults to <code>CPUs=1</code>, in which
parallel processing does not occur.</p>
</td></tr>
<tr><td><code id="predict.vb_+3A_type">Type</code></td>
<td>
<p>This argument specifies the type of parallel processing to
perform, accepting either <code>Type="PSOCK"</code> or
<code>Type="MPI"</code>.</p>
</td></tr>
<tr><td><code id="predict.vb_+3A_...">...</code></td>
<td>
<p>Additional arguments are unused.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Since Variational Bayes characterizes marginal posterior
distributions with modes and variances, and posterior predictive
checks involve samples, the <code>predict.vb</code> function requires
the use of independent samples of the marginal posterior
distributions, provided by <code><a href="#topic+VariationalBayes">VariationalBayes</a></code> when
<code>sir=TRUE</code>.
</p>
<p>The samples of the marginal posterior distributions of the target
distributions (the parameters) are passed along with the data to the
<code>Model</code> specification and used to draw samples from the deviance
and monitored variables. At the same time, the fourth component in the
returned list, which is labeled <code>yhat</code>, is a vector of
expectations of <code class="reqn">\textbf{y}</code>, given the samples, model
specification, and data. To predict <code class="reqn">\textbf{y}^{rep}</code>,
simply supply the data set used to estimate the model. To predict
<code class="reqn">\textbf{y}^{new}</code>, supply a new data set instead (though
for some model specifications, this cannot be done, and
<code class="reqn">\textbf{y}_{new}</code> must be specified in the <code>Model</code>
function). If the new data set does not have <code class="reqn">\textbf{y}</code>, then
create <code>y</code> in the list and set it equal to something sensible,
such as <code>mean(y)</code> from the original data set.
</p>
<p>The variable <code>y</code> must be a vector. If instead it is matrix
<code>Y</code>, then it will be converted to vector <code>y</code>. The vectorized
length of <code>y</code> or <code>Y</code> must be equal to the vectorized length
of <code>yhat</code>, the fourth component of the returned list of the
<code>Model</code> function.
</p>
<p>Parallel processing may be performed when the user specifies
<code>CPUs</code> to be greater than one, implying that the specified number
of CPUs exists and is available. Parallelization may be performed on a
multicore computer or a computer cluster. Either a Simple Network of
Workstations (SNOW) or Message Passing Interface is used (MPI). With
small data sets and few samples, parallel processing may be slower,
due to computer network communication. With larger data sets and more
samples, the user should experience a faster run-time.
</p>
<p>For more information on posterior predictive checks, see
<a href="https://web.archive.org/web/20150215050702/http://www.bayesian-inference.com/posteriorpredictivechecks">https://web.archive.org/web/20150215050702/http://www.bayesian-inference.com/posteriorpredictivechecks</a>.
</p>


<h3>Value</h3>

<p>This function returns an object of class <code>vb.ppc</code> (where
&ldquo;ppc&rdquo; stands for posterior predictive checks). The returned object
is a list with the following components:
</p>
<table>
<tr><td><code>y</code></td>
<td>

<p>This stores <code class="reqn">\textbf{y}</code>, the dependent variable.</p>
</td></tr>
<tr><td><code>yhat</code></td>
<td>

<p>This is a <code class="reqn">N \times S</code> matrix, where <code class="reqn">N</code> is the
number of records of <code class="reqn">\textbf{y}</code> and <code class="reqn">S</code> is the number of 
posterior samples.</p>
</td></tr>
<tr><td><code>Deviance</code></td>
<td>

<p>This is a vector of length <code class="reqn">S</code>, where <code class="reqn">S</code> is the number of
independent posterior samples. Samples are obtained with the
sampling importance resampling algorithm, <code><a href="#topic+SIR">SIR</a></code>.</p>
</td></tr>
<tr><td><code>monitor</code></td>
<td>

<p>This is a <code class="reqn">N \times S</code> matrix, where <code class="reqn">N</code> is the
number of monitored variables and <code class="reqn">S</code> is the number of independent
posterior samples. Samples are obtained with the sampling importance
resampling algorithm, <code><a href="#topic+SIR">SIR</a></code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Statisticat, LLC.</p>


<h3>See Also</h3>

<p><code><a href="#topic+SIR">SIR</a></code> and
<code><a href="#topic+VariationalBayes">VariationalBayes</a></code>.
</p>

<hr>
<h2 id='print.demonoid'>Print an object of class <code>demonoid</code> to the screen.</h2><span id='topic+print.demonoid'></span>

<h3>Description</h3>

<p>This may be used to print the contents of an object of class
<code>demonoid</code> to the screen.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'demonoid'
print(x, ...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.demonoid_+3A_x">x</code></td>
<td>
<p>An object of class <code>demonoid</code> is required.</p>
</td></tr>
<tr><td><code id="print.demonoid_+3A_...">...</code></td>
<td>
<p>Additional arguments are unused.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If the user has an object of class <code>demonoid.hpc</code>, then the
<code>print</code> function may still be used by specifying the chain as a
component in a list, such as printing the second chain with
<code>print(Fit[[2]])</code> when the <code>demonoid.hpc</code> object is named
<code>Fit</code>, for example.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+Consort">Consort</a></code>,
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>, and
<code><a href="#topic+LaplacesDemon.hpc">LaplacesDemon.hpc</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### See the LaplacesDemon function for an example.</code></pre>

<hr>
<h2 id='print.heidelberger'>Print an object of class <code>heidelberger</code> to the screen.</h2><span id='topic+print.heidelberger'></span>

<h3>Description</h3>

<p>This may be used to print the contents of an object of class
<code>heidelberger</code> to the screen.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'heidelberger'
print(x, digits=3, ...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.heidelberger_+3A_x">x</code></td>
<td>
<p>An object of class <code>heidelberger</code> is required.</p>
</td></tr>
<tr><td><code id="print.heidelberger_+3A_digits">digits</code></td>
<td>
<p>This is the number of digits to print.</p>
</td></tr>
<tr><td><code id="print.heidelberger_+3A_...">...</code></td>
<td>
<p>Additional arguments are unused.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+Heidelberger.Diagnostic">Heidelberger.Diagnostic</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### See the Heidelberger.Diagnostic function for an example.</code></pre>

<hr>
<h2 id='print.iterquad'>Print an object of class <code>iterquad</code> to the screen.</h2><span id='topic+print.iterquad'></span>

<h3>Description</h3>

<p>This may be used to print the contents of an object of class
<code>iterquad</code> to the screen.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'iterquad'
print(x, ...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.iterquad_+3A_x">x</code></td>
<td>
<p>An object of class <code>iterquad</code> is required.</p>
</td></tr>
<tr><td><code id="print.iterquad_+3A_...">...</code></td>
<td>
<p>Additional arguments are unused.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>### See the IterativeQuadrature function for an example.</code></pre>

<hr>
<h2 id='print.laplace'>Print an object of class <code>laplace</code> to the screen.</h2><span id='topic+print.laplace'></span>

<h3>Description</h3>

<p>This may be used to print the contents of an object of class
<code>laplace</code> to the screen.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'laplace'
print(x, ...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.laplace_+3A_x">x</code></td>
<td>
<p>An object of class <code>laplace</code> is required.</p>
</td></tr>
<tr><td><code id="print.laplace_+3A_...">...</code></td>
<td>
<p>Additional arguments are unused.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>### See the LaplaceApproximation function for an example.</code></pre>

<hr>
<h2 id='print.miss'>Print an object of class <code>miss</code> to the screen.</h2><span id='topic+print.miss'></span>

<h3>Description</h3>

<p>This may be used to print the contents of an object of class
<code>miss</code> to the screen.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'miss'
print(x, ...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.miss_+3A_x">x</code></td>
<td>
<p>An object of class <code>miss</code> is required.</p>
</td></tr>
<tr><td><code id="print.miss_+3A_...">...</code></td>
<td>
<p>Additional arguments are unused.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+MISS">MISS</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### See the MISS function for an example.</code></pre>

<hr>
<h2 id='print.pmc'>Print an object of class <code>pmc</code> to the screen.</h2><span id='topic+print.pmc'></span>

<h3>Description</h3>

<p>This may be used to print the contents of an object of class
<code>pmc</code> to the screen.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'pmc'
print(x, ...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.pmc_+3A_x">x</code></td>
<td>
<p>An object of class <code>pmc</code> is required.</p>
</td></tr>
<tr><td><code id="print.pmc_+3A_...">...</code></td>
<td>
<p>Additional arguments are unused.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+PMC">PMC</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### See the PMC function for an example.</code></pre>

<hr>
<h2 id='print.raftery'>Print an object of class <code>raftery</code> to the screen.</h2><span id='topic+print.raftery'></span>

<h3>Description</h3>

<p>This may be used to print the contents of an object of class
<code>raftery</code> to the screen.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'raftery'
print(x, digits=3, ...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.raftery_+3A_x">x</code></td>
<td>
<p>An object of class <code>raftery</code> is required.</p>
</td></tr>
<tr><td><code id="print.raftery_+3A_digits">digits</code></td>
<td>
<p>This is the number of digits to print.</p>
</td></tr>
<tr><td><code id="print.raftery_+3A_...">...</code></td>
<td>
<p>Additional arguments are unused.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+Raftery.Diagnostic">Raftery.Diagnostic</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### See the Raftery.Diagnostic function for an example.</code></pre>

<hr>
<h2 id='print.vb'>Print an object of class <code>vb</code> to the screen.</h2><span id='topic+print.vb'></span>

<h3>Description</h3>

<p>This may be used to print the contents of an object of class
<code>vb</code> to the screen.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'vb'
print(x, ...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.vb_+3A_x">x</code></td>
<td>
<p>An object of class <code>vb</code> is required.</p>
</td></tr>
<tr><td><code id="print.vb_+3A_...">...</code></td>
<td>
<p>Additional arguments are unused.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+VariationalBayes">VariationalBayes</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>### See the VariationalBayes function for an example.</code></pre>

<hr>
<h2 id='Raftery.Diagnostic'>Raftery and Lewis's diagnostic</h2><span id='topic+Raftery.Diagnostic'></span>

<h3>Description</h3>

<p>Raftery and Lewis (1992) introduced an MCMC diagnostic that estimates
the number of iterations needed for a given level of precision in
posterior samples, as well as estimating burn-in, when quantiles are
the posterior summaries of interest.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Raftery.Diagnostic(x, q=0.025, r=0.005, s=0.95, eps=0.001)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Raftery.Diagnostic_+3A_x">x</code></td>
<td>
<p>This required argument accepts an object of class
<code>demonoid</code>. It attempts to use <code>Posterior2</code>, but when this
is missing it uses <code>Posterior1</code>.</p>
</td></tr>
<tr><td><code id="Raftery.Diagnostic_+3A_q">q</code></td>
<td>
<p>This is the quantile to be estimated.</p>
</td></tr>
<tr><td><code id="Raftery.Diagnostic_+3A_r">r</code></td>
<td>
<p>This is the desired margin of error of the estimate, also
called the accuracy.</p>
</td></tr>
<tr><td><code id="Raftery.Diagnostic_+3A_s">s</code></td>
<td>
<p>This is the probability of obtaining an estimate in the
interval (q-r, q+r).</p>
</td></tr>
<tr><td><code id="Raftery.Diagnostic_+3A_eps">eps</code></td>
<td>
<p>This is the precision required for the estimate of time to
convergence.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In this MCMC diagnostic, a posterior quantile <code class="reqn">q</code> of interest is
specified. Next, an acceptable tolerance <code class="reqn">r</code> is specified for
<code class="reqn">q</code>, which means that it is desired to measure <code class="reqn">q</code> with an
accuracy of +/- <code class="reqn">r</code>. Finally, the user selects a probability
<code class="reqn">s</code>, which is the probability of being within the interval
<code class="reqn">(q-r, q+r)</code>. The <code>Raftery.Diagnostic</code> then estimates the
number <code class="reqn">N</code> of iterations and the number <code class="reqn">M</code> of burn-in
iterations that are necessary to satisfy the specified conditions
regarding quantile <code class="reqn">q</code>.
</p>
<p>The diagnostic was designed to test a short, initial update, in which
the chains were called pilot chains, and the application was later
suggested for iterative use after any update as a general method for
pursuing convergence (Raftery and Lewis, 1996).
</p>
<p>Results of the <code>Raftery.Diagnostic</code> differ depending on the
chosen quantile <code class="reqn">q</code>. Estimates are conservative, so more
iterations are suggested than necessary.
</p>


<h3>Value</h3>

<p>The <code>Raftery.Diagnostic</code> function returns an object of class
<code>raftery</code> that is list. A print method is available for objects
of this class. The list has the following components:
</p>
<table>
<tr><td><code>tspar</code></td>
<td>
<p>These are the time-series parameters of the posterior
samples in <code>x</code>.</p>
</td></tr>
<tr><td><code>params</code></td>
<td>
<p>This is a vector containing the parameters <code>q</code>,
<code>r</code>, and <code>s</code>.</p>
</td></tr>
<tr><td><code>Niters</code></td>
<td>
<p>This is the number of iterations in the posterior
samples in <code>x</code>.</p>
</td></tr>
<tr><td><code>resmatrix</code></td>
<td>
<p>This is a 3-dimensional array containing the
results: <code class="reqn">M</code> is the suggested burn-in, <code class="reqn">N</code> is the suggested
number of iterations, <code class="reqn">Nmin</code> is the suggested number of
iterations based on zero autocorrelation, and
<code class="reqn">I = (M+N)/Nmin</code> is the &quot;dependence factor&quot;. The dependence
factor is interpreted as the proportional increase in the number of
iterations attributable to autocorrelation. Highly autocorrelated
chains (&gt; 5) are worrisome, and may be due to influential initial
values, parameter correlations, or poor mixing.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The <code>Raftery.Diagnostic</code> function was adapted from the
<code>raftery.diag</code> function in the coda package, which was adapted
from the FORTRAN program &lsquo;gibbsit&rsquo;, written by Steven Lewis.
</p>


<h3>References</h3>

<p>Raftery, A.E. and Lewis, S.M. (1992). &quot;How Many Iterations in the
Gibbs Sampler?&quot; In <em>Bayesian Statistics</em>, 4 (J.M. Bernardo, J.O.
Berger, A.P. Dawid and A.F.M. Smith, eds.). Oxford, U.K.: Oxford
University Press, p. 763&ndash;773.
</p>
<p>Raftery, A.E. and Lewis, S.M. (1992). &quot;One Long Run with
Diagnostics: Implementation Strategies for Markov chain Monte Carlo&quot;.
<em>Statistical Science</em>, 7, p. 493&ndash;497.
</p>
<p>Raftery, A.E. and Lewis, S.M. (1996). &quot;Implementing MCMC&quot;.
<em>In</em> Practical Markov Chain Monte Carlo (W.R. Gilks,
D.J. Spiegelhalter and S. Richardson, eds.). Chapman and Hall:
Baton Rouge, FL.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+burnin">burnin</a></code>,
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>,
<code><a href="#topic+print.raftery">print.raftery</a></code>, and
<code><a href="#topic+Thin">Thin</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#library(LaplacesDemon)
###After updating with LaplacesDemon, do:
#rd &lt;- Raftery.Diagnostic(Fit)
#print(rd)
</code></pre>

<hr>
<h2 id='RejectionSampling'>Rejection Sampling</h2><span id='topic+RejectionSampling'></span>

<h3>Description</h3>

<p>The <code>RejectionSampling</code> function implements rejection sampling
of a target density given a proposal density.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>RejectionSampling(Model, Data, mu, S, df=Inf, logc, n=1000, CPUs=1, Type="PSOCK")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="RejectionSampling_+3A_model">Model</code></td>
<td>
<p>This is a model specification function. For more
information, see <code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>.</p>
</td></tr>
<tr><td><code id="RejectionSampling_+3A_data">Data</code></td>
<td>
<p>This is a list of data. For more information, see
<code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>.</p>
</td></tr>
<tr><td><code id="RejectionSampling_+3A_mu">mu</code></td>
<td>
<p>This is a mean vector <code class="reqn">\mu</code> for the multivariate
normal or multivariate t proposal density.</p>
</td></tr>
<tr><td><code id="RejectionSampling_+3A_s">S</code></td>
<td>
<p>This is a convariance matrix <code class="reqn">\Sigma</code> for the
multivariate normal or multivariate t proposal density.</p>
</td></tr>
<tr><td><code id="RejectionSampling_+3A_df">df</code></td>
<td>
<p>This is a scalar degrees of freedom parameter
<code class="reqn">\nu</code>. It defaults to infinity, in which case the
multivariate normal density is used.</p>
</td></tr>
<tr><td><code id="RejectionSampling_+3A_logc">logc</code></td>
<td>
<p>This is the logarithm of the rejection sampling constant.</p>
</td></tr>
<tr><td><code id="RejectionSampling_+3A_n">n</code></td>
<td>
<p>This is the number of independent draws to be simulated from
the proposal density.</p>
</td></tr>
<tr><td><code id="RejectionSampling_+3A_cpus">CPUs</code></td>
<td>
<p>This argument accepts an integer that specifies the number
of central processing units (CPUs) of the multicore computer or
computer cluster. This argument defaults to <code>CPUs=1</code>, in which
parallel processing does not occur.</p>
</td></tr>
<tr><td><code id="RejectionSampling_+3A_type">Type</code></td>
<td>
<p>This argument specifies the type of parallel processing to
perform, accepting either <code>Type="PSOCK"</code> or
<code>Type="MPI"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Rejection sampling (von Neumann, 1951) is a Monte Carlo method for
drawing independent samples from a distribution that is proportional
to the target distribution, <code class="reqn">f(x)</code>, given a sampling distribution,
<code class="reqn">g(x)</code>, from which samples can readily be drawn, and for which
there is a finite constant <code class="reqn">c</code>.
</p>
<p>Here, the target distribution, <code class="reqn">f(x)</code>, is the result of the
<code>Model</code> function. The sampling distribution, <code class="reqn">g(x)</code>, is
either a multivariate normal or multivariate t-distribution. The
parameters of <code class="reqn">g(x)</code> (<code>mu</code>, <code>S</code>, and <code>df</code>) are used
to create random draws, <code class="reqn">\theta</code>, of the sampling
distribution, <code class="reqn">g(x)</code>. These draws, <code class="reqn">\theta</code>, are used
to evaluate the target distribution, <code class="reqn">f(x)</code>, via the <code>Model</code>
specification function. The evaluations of the target distribution,
sampling distribution, and the constant are used to create a
probability of acceptance for each draw, by comparing to a vector of
<code class="reqn">n</code> uniform draws, <code class="reqn">u</code>. Each draw, <code class="reqn">\theta</code> is
accepted if </p>
<p style="text-align: center;"><code class="reqn">u \le \frac{f(\theta|\textbf{y})}{cg(\theta)}</code>
</p>

<p>Before beginning rejection sampling, a goal of the user is to find the
bounding constant, <code class="reqn">c</code>, such that <code class="reqn">f(\theta|\textbf{y}) \le
  cg(\theta)</code> for all
<code class="reqn">\theta</code>. These are all expressed in logarithms, so the
goal is to find <code class="reqn">\log f(\theta|\textbf{y}) - \log g(\theta) \le
  \log(c)</code> for all
<code class="reqn">\theta</code>. This is done by maximizing <code class="reqn">\log
  f(\theta|\textbf{y}) - \log g(\theta)</code> over all <code class="reqn">\theta</code>. By using, say,
<code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code> to find the modes of the
parameters of interest, and using the resultant <code>LP</code>, the mode
of the logarithm of the joint posterior distribution, as
<code class="reqn">\log(c)</code>.
</p>
<p>The <code>RejectionSampling</code> function performs one iteration of
rejection sampling. Rejection sampling is often iterated, then called
the rejection sampling algorithm, until a sufficient number or
proportion of <code class="reqn">\theta</code> is accepted. An efficient
rejection sampling algorithm has a high acceptance rate. However,
rejection sampling becomes less efficient as the model dimension (the
number of parameters) increases.
</p>
<p>Extensions of rejection sampling include Adaptive Rejection
Sampling (ARS) (either derivative-based or derivative-free) and
Adaptive Rejection Metropolis Sampling (ARMS), as in Gilks et
al. (1995). The random-walk Metropolis algorithm (Metropolis et al.,
1953) combined the rejection sampling (a method of Monte Carlo
simulation) of von Neumann (1951) with Markov chains.
</p>
<p>Parallel processing may be performed when the user specifies
<code>CPUs</code> to be greater than one, implying that the specified number
of CPUs exists and is available. Parallelization may be performed on a
multicore computer or a computer cluster. Either a Simple Network of
Workstations (SNOW) or Message Passing Interface (MPI) is used. With
small data sets and few samples, parallel processing may be slower,
due to computer network communication. With larger data sets and more
samples, the user should experience a faster run-time.
</p>
<p>This function is similar to the <code>rejectsampling</code> function in the
<code>LearnBayes</code> package.
</p>


<h3>Value</h3>

<p>The <code>RejectionSampling</code> function returns an object of class
<code>rejection</code>, which is a matrix of accepted, independent,
simulated draws from the target distribution.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>References</h3>

<p>Gilks, W.R., Best, N.G., Tan, K.K.C. (1995). &quot;Adaptive Rejection
Metropolis Sampling within Gibbs Sampling&quot;. Journal of the Royal
Statistical Society. Series C (Applied Statistics), Vol. 44, No. 4,
p. 455&ndash;472.
</p>
<p>Metropolis, N., Rosenbluth, A.W., Rosenbluth, M.N., and Teller,
E. (1953). &quot;Equation of State Calculations by Fast Computing
Machines&quot;. Journal of Chemical Physics, 21, p. 1087-1092.
</p>
<p>von Neumann, J. (1951). &quot;Various Techniques Used in Connection with
Random Digits. Monte Carlo Methods&quot;. National Bureau Standards, 12,
p. 36&ndash;38.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dmvn">dmvn</a></code>,
<code><a href="#topic+dmvt">dmvt</a></code>,
<code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code>,
<code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>,
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>, and
<code><a href="#topic+VariationalBayes">VariationalBayes</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
### Suppose an output object of class laplace is called Fit:
#rs &lt;- RejectionSampling(Model, MyData, mu=Fit$Summary1[,1],
#     S=Fit$Covar, df=Inf, logc=Fit$LP.Final, n=1000)
#rs
</code></pre>

<hr>
<h2 id='SensitivityAnalysis'>Sensitivity Analysis</h2><span id='topic+SensitivityAnalysis'></span>

<h3>Description</h3>

<p>This function performs an elementary sensitivity analysis for two
models regarding marginal posterior distributions and posterior
inferences.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SensitivityAnalysis(Fit1, Fit2, Pred1, Pred2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SensitivityAnalysis_+3A_fit1">Fit1</code></td>
<td>
<p>This argument accepts an object of class <code>demonoid</code>,
<code>iterquad</code>, <code>laplace</code>, <code>pmc</code>, or <code>vb</code>.</p>
</td></tr>
<tr><td><code id="SensitivityAnalysis_+3A_fit2">Fit2</code></td>
<td>
<p>This argument accepts an object of class <code>demonoid</code>,
<code>iterquad</code>, <code>laplace</code>, <code>pmc</code>, or <code>vb</code>.</p>
</td></tr>
<tr><td><code id="SensitivityAnalysis_+3A_pred1">Pred1</code></td>
<td>
<p>This argument accepts an object of class
<code>demonoid.ppc</code>, <code>iterquad.ppc</code>, <code>laplace.ppc</code>,
<code>pmc.ppc</code>, or <code>vb.ppc</code>.</p>
</td></tr>
<tr><td><code id="SensitivityAnalysis_+3A_pred2">Pred2</code></td>
<td>
<p>This argument accepts an object of class
<code>demonoid.ppc</code>, <code>iterquad.ppc</code>, <code>laplace.ppc</code>,
<code>pmc.ppc</code>, or <code>vb.ppc</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Sensitivity analysis is concerned with the influence from changes to the
inputs of a model on the output. Comparing differences resulting from
different prior distributions is the most common application of
sensitivity analysis, though results from different likelihoods may be
compared as well. The outputs of interest are the marginal posterior
distributions and posterior inferences.
</p>
<p>There are many more methods of conducting a sensitivity analysis than
exist in the <code>SensitivityAnalysis</code> function. For more
information, see Oakley and O'Hagan (2004). The <code><a href="#topic+SIR">SIR</a></code>
function is useful for approximating changes in the posterior due to
small changes in prior distributions.
</p>
<p>The <code>SensitivityAnalysis</code> function compares marginal posterior
distributions and posterior predictive distributions. Specifically,
it calculates the probability that each distribution in <code>Fit1</code>
and <code>Pred1</code> is greater than the associated distribution in
<code>Fit2</code> and <code>Pred2</code>, and returns a variance ratio of each
pair of distributions. If the probability is <code class="reqn">0.5</code> that a
distribution is greater than another, or if the variance ratio is
<code class="reqn">1</code>, then no difference is found due to the inputs.
</p>
<p>Additional comparisons and methods are currently outside the scope of
the <code>SensitivityAnalysis</code> function. The <code><a href="#topic+BayesFactor">BayesFactor</a></code>
function may also be considered, as well as comparing posterior
predictive checks resulting from <code><a href="#topic+summary.demonoid.ppc">summary.demonoid.ppc</a></code>,
<code><a href="#topic+summary.iterquad.ppc">summary.iterquad.ppc</a></code>,
<code><a href="#topic+summary.laplace.ppc">summary.laplace.ppc</a></code>, <code><a href="#topic+summary.pmc.ppc">summary.pmc.ppc</a></code>, or
<code><a href="#topic+summary.vb.ppc">summary.vb.ppc</a></code>.
</p>
<p>Regarding marginal posterior distributions, the
<code>SensitivityAnalysis</code> function compares only distributions with
identical parameter names. For example, suppose a statistician
conducts a sensitivity analysis to study differences resulting from
two prior distributions: a normal distribution and a Student t
distribution. These distributions have two and three parameters,
respectively. The statistician has named the parameters <code>beta</code>
and <code>sigma</code> for the normal distribution, while for the Student
t distribution, the parameters are named <code>beta</code>, <code>sigma</code>,
and <code>nu</code>. In this case, the <code>SensitivityAnalysis</code> function
compares the marginal posterior distributions for <code>beta</code> and
<code>sigma</code>, though <code>nu</code> is ignored because it is not in both
models. If the statistician does not want certain parameters compared,
then differing parameter names should be assigned.
</p>
<p>Robust Bayesian analysis is a very similar topic, and often called
simply Bayesian sensitivity analysis. In robust Bayesian analysis, the
robustness of answers from a Bayesian analysis to uncertainty about
the precise details of the analysis is studied. An answer is
considered robust if it does not depend sensitively on the assumptions
and inputs on which it is based. Robust Bayes methods acknowledge that
it is sometimes very difficult to come up with precise distributions
to be used as priors. Likewise the appropriate likelihood function
that should be used for a particular problem may also be in doubt. In
a robust Bayesian analysis, a standard Bayesian analysis is applied to
all possible combinations of prior distributions and likelihood
functions selected from classes of priors and likelihoods considered
empirically plausible by the statistician.
</p>


<h3>Value</h3>

<p>This function returns a list with the following components:
</p>
<table>
<tr><td><code>Posterior</code></td>
<td>
<p>This is a <code class="reqn">J \times 2</code> matrix of <code class="reqn">J</code>
marginal posterior distributions. Column names are &quot;p(Fit1 &gt; Fit2)&quot;
and &quot;var(Fit1) / var(Fit2)&quot;.</p>
</td></tr>
<tr><td><code>Post.Pred.Dist</code></td>
<td>
<p>This is a <code class="reqn">N \times 2</code> matrix of <code class="reqn">N</code>
posterior predictive distributions. Column names are
&quot;p(Pred1 &gt; Pred2)&quot; and &quot;var(Pred1) / var(Pred2)&quot;.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Statisticat, LLC <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>References</h3>

<p>Berger, J.O. (1984). &quot;The Robust Bayesian Viewpoint (with
discussion)&quot;. In J. B. Kadane, editor, Robustness of Bayesian
Analyses, p. 63&ndash;144. North-Holland, Amsterdam.
</p>
<p>Berger, J.O. (1985). &quot;Statistical Decision Theory and Bayesian
Analysis&quot;. Springer-Verlag, New York.
</p>
<p>Berger, J.O. (1994). &quot;An Overview of Robust Bayesian Analysis
(with discussion)&quot;. Test, 3, p. 5&ndash;124.
</p>
<p>Oakley, J. and O'Hagan, A. (2004). &quot;Probabilistic Sensitivity Analysis
of Complex Models: a Bayesian Approach&quot;. <em>Journal of the Royal
Statistical Society, Series B</em>, 66, p. 751&ndash;769.
</p>
<p>Weiss, R. (1995). &quot;An Approach to Bayesian Sensitivity Analysis&quot;.
<em>Journal of the Royal Statistical Society, Series B</em>, 58,
p. 739&ndash;750.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+BayesFactor">BayesFactor</a></code>,
<code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code>,
<code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>,
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>,
<code><a href="#topic+PMC">PMC</a></code>,
<code><a href="#topic+predict.demonoid">predict.demonoid</a></code>,
<code><a href="#topic+predict.iterquad">predict.iterquad</a></code>,
<code><a href="#topic+predict.laplace">predict.laplace</a></code>,
<code><a href="#topic+predict.pmc">predict.pmc</a></code>,
<code><a href="#topic+SIR">SIR</a></code>,
<code><a href="#topic+summary.demonoid.ppc">summary.demonoid.ppc</a></code>,
<code><a href="#topic+summary.iterquad.ppc">summary.iterquad.ppc</a></code>,
<code><a href="#topic+summary.laplace.ppc">summary.laplace.ppc</a></code>,
<code><a href="#topic+summary.pmc.ppc">summary.pmc.ppc</a></code>, and
<code><a href="#topic+VariationalBayes">VariationalBayes</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#sa &lt;- SensitivityAnalysis(Fit1, Fit2, Pred1, Pred2)
#sa
</code></pre>

<hr>
<h2 id='SIR'>Sampling Importance Resampling</h2><span id='topic+SIR'></span>

<h3>Description</h3>

<p>The <code>SIR</code> function performs Sampling Importance Resampling, also
called Sequential Importance Resampling, and uses a multivariate normal
proposal density.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SIR(Model, Data, mu, Sigma, n=1000, CPUs=1, Type="PSOCK")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SIR_+3A_model">Model</code></td>
<td>
<p>This is a model specification function. For more
information, see <code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>.</p>
</td></tr>
<tr><td><code id="SIR_+3A_data">Data</code></td>
<td>
<p>This is a list of data. For more information, see
<code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>.</p>
</td></tr>
<tr><td><code id="SIR_+3A_mu">mu</code></td>
<td>
<p>This is a mean vector, <code class="reqn">\mu</code>, for a multivariate
normal distribution, and is usually the posterior means from an
object of class <code>iterquad</code> (from
<code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code>) or class <code>vb</code> (from
<code><a href="#topic+VariationalBayes">VariationalBayes</a></code>), or the posterior modes from an
object of class <code>laplace</code> (from 
<code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>).</p>
</td></tr>
<tr><td><code id="SIR_+3A_sigma">Sigma</code></td>
<td>
<p>This is a covariance matrix, <code class="reqn">\Sigma</code>, for a
multivariate normal distribution, and is usually the <code>Covar</code>
component of an object of class <code>iterquad</code>, <code>laplace</code>, or
<code>vb</code>.</p>
</td></tr>
<tr><td><code id="SIR_+3A_n">n</code></td>
<td>
<p>This is the number of samples to be drawn from the posterior
distribution.</p>
</td></tr>
<tr><td><code id="SIR_+3A_cpus">CPUs</code></td>
<td>
<p>This argument accepts an integer that specifies the number
of central processing units (CPUs) of the multicore computer or
computer cluster. This argument defaults to <code>CPUs=1</code>, in which
parallel processing does not occur.</p>
</td></tr>
<tr><td><code id="SIR_+3A_type">Type</code></td>
<td>
<p>This argument specifies the type of parallel processing to
perform, accepting either <code>Type="PSOCK"</code> or
<code>Type="MPI"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Sampling Importance Resampling (SIR) was introduced in Gordon, et
al. (1993), and is the original particle filtering algorithm (and this
family of algorithms is also known as Sequential Monte Carlo). A
distribution is approximated with importance weights, which are
approximations to the relative posterior densities of the particles,
and the sum of the weights is one. In this terminology, each sample in
the distribution is a &ldquo;particle&rdquo;. SIR is a sequential or recursive
form of importance sampling. As in importance sampling, the
expectation of a function can be approximated as a weighted
average. The optimal proposal distribution is the target distribution.
</p>
<p>In the <code>LaplacesDemon</code> package, the main use of the <code>SIR</code>
function is to produce posterior samples for iterative quadrature,
Laplace Approximation, or Variational Bayes, and <code>SIR</code> is called
behind-the-scenes by the <code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code>,
<code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>, or <code><a href="#topic+VariationalBayes">VariationalBayes</a></code>
function.
</p>
<p>Iterative quadrature estimates the posterior mean and the associated
covariance matrix. Assuming normality, this output characterizes the
marginal posterior distributions. However, it is often useful to have
posterior samples, in which case the <code>SIR</code> function is used to
draw samples. The number of samples, <code>n</code>, should increase with
the number and intercorrelations of the parameters. Otherwise,
multimodal posterior distributions may occur.
</p>
<p>Laplace Approximation estimates the posterior mode and the associated
covariance matrix. Assuming normality, this output characterizes the
marginal posterior distributions. However, it is often useful to have
posterior samples, in which case the <code>SIR</code> function is used to
draw samples. The number of samples, <code>n</code>, should increase with
the number and intercorrelations of the parameters. Otherwise,
multimodal posterior distributions may occur.
</p>
<p>Variational Bayes estimates both the posterior mean and
variance. Assuming normality, this output characterizes the marginal
posterior distributions. However, it is often useful to have posterior
samples, in which case the <code>SIR</code> function is used to draw
samples. The number of samples, <code>n</code>, should increase with the
number of intercorrelations of the parameters. Otherwise, multimodal
posterior distributions may occur.
</p>
<p>SIR is also commonly used when considering a mild change in a prior
distribution. For example, suppose a model was updated in
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>, and it had a least-informative prior
distribution, but the statistician would like to estimate the impact
of changing to a weakly-informative prior distribution. The change is
made in the model specification function, and the posterior means and
covariance are supplied to the <code>SIR</code> function. The returned
samples are estimates of the posterior, given the different prior
distribution. This is akin to sensitivity analysis (see the
<code>SensitivityAnalysis</code> function).
</p>
<p>In other contexts (for which this function is not designed), SIR is
used with dynamic linear models (DLMs) and state-space models (SSMs)
for state filtering.
</p>
<p>Parallel processing may be performed when the user specifies
<code>CPUs</code> to be greater than one, implying that the specified number
of CPUs exists and is available. Parallelization may be performed on a
multicore computer or a computer cluster. Either a Simple Network of
Workstations (SNOW) or Message Passing Interface (MPI) is used. With
small data sets and few samples, parallel processing may be slower,
due to computer network communication. With larger data sets and more
samples, the user should experience a faster run-time.
</p>
<p>This function was adapted from the <code>sir</code> function in the
<code>LearnBayes</code> package.
</p>


<h3>Value</h3>

<p>The <code>SIR</code> function returns a matrix of samples drawn from the
posterior distribution.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>References</h3>

<p>Gordon, N.J., Salmond, D.J., and Smith, A.F.M. (1993). &quot;Novel Approach
to Nonlinear/Non-Gaussian Bayesian State Estimation&quot;. <em>IEEE
Proceedings F on Radar and Signal Processing</em>, 140(2), p. 107&ndash;113.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dmvn">dmvn</a></code>,
<code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code>,
<code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>,
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>,
<code><a href="#topic+SensitivityAnalysis">SensitivityAnalysis</a></code>, and
<code><a href="#topic+VariationalBayes">VariationalBayes</a></code>.
</p>

<hr>
<h2 id='Stick'>Truncated Stick-Breaking</h2><span id='topic+Stick'></span>

<h3>Description</h3>

<p>The <code>Stick</code> function provides the utility of truncated
stick-breaking regarding the vector
<code class="reqn">\theta</code>. Stick-breaking is commonly referred to as a
stick-breaking process, and is used often in a Dirichlet
process (Sethuraman, 1994). It is commonly associated with
infinite-dimensional mixtures, but in practice, the &lsquo;infinite&rsquo; number
is truncated to a finite number, since it is impossible to estimate an
infinite number of parameters (Ishwaran and James, 2001).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Stick(theta)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Stick_+3A_theta">theta</code></td>
<td>
<p>This required argument, <code class="reqn">\theta</code> is a vector
of length <code class="reqn">(M-1)</code> regarding <code class="reqn">M</code> mixture components.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Dirichlet process (DP) is a stochastic process used in Bayesian
nonparametric modeling, most commonly in DP mixture models, otherwise
known as infinite mixture models. A DP is a distribution over
distributions. Each draw from a DP is itself a discrete
distribution. A DP is an infinite-dimensional generalization of
Dirichlet distributions. It is called a DP because it has
Dirichlet-distributed, finite-dimensional, marginal distributions,
just as the Gaussian process has Gaussian-distributed,
finite-dimensional, marginal distributions. Distributions drawn from a
DP cannot be described using a finite number of parameters, thus the
classification as a nonparametric model. The truncated stick-breaking
(TSB) process is associated with a truncated Dirichlet process (TDP).
</p>
<p>An example of a TSB process is cluster analysis, where the number of
clusters is unknown and treated as mixture components. In such a
model, the TSB process calculates probability vector <code class="reqn">\pi</code>
from <code class="reqn">\theta</code>, given a user-specified maximum number of
clusters to explore as <code class="reqn">C</code>, where <code class="reqn">C</code> is the length of
<code class="reqn">\theta + 1</code>. Vector <code class="reqn">\pi</code> is assigned a TSB
prior distribution (for more information, see <code><a href="#topic+dStick">dStick</a></code>).
</p>
<p>Elsewhere, each element of <code class="reqn">\theta</code> is constrained to the
interval (0,1), and the original TSB form is beta-distributed with the
<code class="reqn">\alpha</code> parameter of the beta distribution constrained
to 1 (Ishwaran and James, 2001). The <code class="reqn">\beta</code> hyperparameter
in the beta distribution is usually gamma-distributed.
</p>
<p>A larger value for a given <code class="reqn">\theta_m</code> is associated
with a higher probability of the associated mixture component,
however, the proportion changes according to the position of the
element in the <code class="reqn">\theta</code> vector.
</p>
<p>A variety of stick-breaking processes exist. For example, rather than
each <code class="reqn">\theta</code> being beta-distributed, there have been other
forms introduced such as logistic and probit, among others.
</p>


<h3>Value</h3>

<p>The <code>Stick</code> function returns a probability vector wherein each
element relates to a mixture component.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>References</h3>

<p>Ishwaran, H. and James, L. (2001). &quot;Gibbs Sampling Methods for Stick
Breaking Priors&quot;. <em>Journal of the American Statistical
Association</em>, 96(453), p. 161&ndash;173.
</p>
<p>Sethuraman, J. (1994). &quot;A Constructive Definition of Dirichlet
Priors&quot;. <em>Statistica Sinica</em>, 4, p. 639&ndash;650.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ddirichlet">ddirichlet</a></code>,
<code><a href="#topic+dmvpolya">dmvpolya</a></code>, and 
<code><a href="#topic+dStick">dStick</a></code>.
</p>

<hr>
<h2 id='summary.demonoid.ppc'>Posterior Predictive Check Summary</h2><span id='topic+summary.demonoid.ppc'></span>

<h3>Description</h3>

<p>This may be used to summarize either new, unobserved instances of
<code class="reqn">\textbf{y}</code> (called <code class="reqn">\textbf{y}^{new}</code>) or
replicates of <code class="reqn">\textbf{y}</code> (called
<code class="reqn">\textbf{y}^{rep}</code>). Either <code class="reqn">\textbf{y}^{new}</code> or
<code class="reqn">\textbf{y}^{rep}</code> is summarized, depending on
<code><a href="#topic+predict.demonoid">predict.demonoid</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'demonoid.ppc'
summary(object, Categorical, Rows,
     Discrep, d, Quiet, ...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.demonoid.ppc_+3A_object">object</code></td>
<td>
<p>An object of class <code>demonoid.ppc</code> is required.</p>
</td></tr>
<tr><td><code id="summary.demonoid.ppc_+3A_categorical">Categorical</code></td>
<td>
<p>Logical. If <code>TRUE</code>, then <code>y</code> and
<code>yhat</code> are considered to be categorical (such as y=0 or y=1),
rather than continuous.</p>
</td></tr>
<tr><td><code id="summary.demonoid.ppc_+3A_rows">Rows</code></td>
<td>
<p>An optional vector of row numbers, for example
<code>c(1:10)</code>. All rows will be estimated, but only these rows will
appear in the summary.</p>
</td></tr>
<tr><td><code id="summary.demonoid.ppc_+3A_discrep">Discrep</code></td>
<td>
<p>A character string indicating a discrepancy
test. <code>Discrep</code> defaults to <code>NULL</code>. Valid character
strings when <code>y</code> is continuous are: <code>"Chi-Square"</code>,
<code>"Chi-Square2 "</code>, <code>"Kurtosis"</code>, <code>"L-criterion"</code>,
<code>"MASE"</code>, <code>"MSE"</code>, <code>"PPL"</code>, <code>"Quadratic Loss"</code>,
<code>"Quadratic Utility"</code>, <code>"RMSE"</code>, <code>"Skewness"</code>,
<code>"max(yhat[i,]) &gt; max(y)"</code>, <code>"mean(yhat[i,]) &gt; mean(y)"</code>,
<code>"mean(yhat[i,] &gt; d)"</code>, <code>"mean(yhat[i,] &gt; mean(y))"</code>,
<code>"min(yhat[i,]) &lt; min(y)"</code>, <code>"round(yhat[i,]) = d"</code>, and
<code>"sd(yhat[i,]) &gt; sd(y)"</code>. Valid character strings when <code>y</code>
is categorical are: <code>"p(yhat[i,] != y[i])"</code>. Kurtosis and
skewness are not discrepancies, but are included here for convenience.</p>
</td></tr>
<tr><td><code id="summary.demonoid.ppc_+3A_d">d</code></td>
<td>
<p>This is an optional integer to be used with the
<code>Discrep</code> argument above, and it defaults to <code>d=0</code>.</p>
</td></tr>
<tr><td><code id="summary.demonoid.ppc_+3A_quiet">Quiet</code></td>
<td>
<p>This logical argument defaults to <code>FALSE</code> and will
print results to the console. When <code>TRUE</code>, results are not
printed.</p>
</td></tr>
<tr><td><code id="summary.demonoid.ppc_+3A_...">...</code></td>
<td>
<p>Additional arguments are unused.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function summarizes an object of class <code>demonoid.ppc</code>, which
consists of posterior predictive checks on either
<code class="reqn">\textbf{y}^{new}</code> or <code class="reqn">\textbf{y}^{rep}</code>,
depending respectively on whether unobserved instances of
<code class="reqn">\textbf{y}</code> or the model sample of <code class="reqn">\textbf{y}</code> was
used in the <code><a href="#topic+predict.demonoid">predict.demonoid</a></code> function.
</p>
<p>The purpose of a posterior predictive check is to assess how well (or
poorly) the model fits the data, or to assess discrepancies between
the model and the data. For more information on posterior predictive
checks, see
<a href="https://web.archive.org/web/20150215050702/http://www.bayesian-inference.com/posteriorpredictivechecks">https://web.archive.org/web/20150215050702/http://www.bayesian-inference.com/posteriorpredictivechecks</a>.
</p>
<p>When <code class="reqn">\textbf{y}</code> is continuous and known, this function
estimates the predictive concordance between <code class="reqn">\textbf{y}</code> and
<code class="reqn">\textbf{y}^{rep}</code> as per Gelfand (1996), and the
predictive quantile (PQ), which is for record-level outlier detection
used to calculate Gelfand's predictive concordance.
</p>
<p>When <code class="reqn">\textbf{y}</code> is categorical and known, this function
estimates the record-level lift, which is
<code>p(yhat[i,] = y[i]) / [p(y = j) / n]</code>, or
the number of correctly predicted samples over the rate of that
category of <code class="reqn">\textbf{y}</code> in vector <code class="reqn">\textbf{y}</code>.
</p>
<p>A discrepancy measure is an approach to studying discrepancies between
the model and data (Gelman et al., 1996). Below is a list of
discrepancy measures, followed by a brief introduction to discrepancy
analysis:
</p>

<ul>
<li><p> The <code>"Chi-Square"</code> discrepancy measure is the chi-square
goodness-of-fit test that is recommended by Gelman. For each record
i=1:N, this returns (y[i] - E(y[i]))^2 / var(yhat[i,]).
</p>
</li>
<li><p> The <code>"Chi-Square2"</code> discrepancy measure returns the
following for each record: Pr(chisq.rep[i,] &gt; chisq.obs[i,]), where
chisq.obs[i,] &lt;- (y[i] - E(y[i]))^2 / E(y[i]), and chisq.rep[i,] &lt;-
(yhat[i,] - E(yhat[i,]))^2 / E(yhat[i,]), and the overall
discrepancy is the percent of records that were outside of the 95%
quantile-based probability interval (see <code><a href="#topic+p.interval">p.interval</a></code>).
</p>
</li>
<li><p> The <code>"Kurtosis"</code> discrepancy measure returns the kurtosis
of <code class="reqn">\textbf{y}^{rep}</code> for each record, and the discrepancy
statistic is the mean for all records. This does not measure
discrepancies between the model and data, and is useful for finding
kurtotic replicate distributions.
</p>
</li>
<li><p> The <code>"L-criterion"</code> discrepancy measure of Laud and Ibrahim
(1995) provides the record-level combination of two components (see
below), and the discrepancy statistic is the sum, <code>L</code>, as well as
a calibration number, <code>S.L</code>. For more information on the
L-criterion, see the accompanying vignette entitled &quot;Bayesian
Inference&quot;.
</p>
</li>
<li><p> The <code>"MASE"</code> (Mean Absolute Scaled Error) is a
discrepancy measure for the accuracy of time-series forecasts,
estimated as <code>(|y - yhat|) / mean(abs(diff(y)))</code>. The discrepancy
statistic is the mean of the record-level values.
</p>
</li>
<li><p> The <code>"MSE"</code> (Mean Squared Error) discrepancy measure
provides the MSE for each record across all replicates, and the
discrepancy statistic is the mean of the record-level MSEs. MSE and
quadratic loss are identical.
</p>
</li>
<li><p> The <code>"PPL"</code> (Posterior Predictive Loss) discrepancy
measure of Gelfand and Ghosh (1998) provides the record-level
combination of two components: one involves the predictive variance
and the other includes the accuracy of the means of the predictive
distribution. The <code>d=0</code> argument applies the following weight to
the accuracy component, which is then added to the variance component:
<code class="reqn">d/(d+1)</code>. For <code class="reqn">\textbf{y}^{new}</code>, use <code class="reqn">d=0</code>. For
<code class="reqn">\textbf{y}^{rep}</code> and model comparison, <code class="reqn">d</code> is
commonly set to 1, 10, or 100000. Larger values of <code class="reqn">d</code> put more
stress on fit and downgrade the precision of the estimates.
</p>
</li>
<li><p> The <code>"Quadratic Loss"</code> discrepancy measure provides the
mean quadratic loss for each record across all replicates, and the
discrepancy statistic is the mean of the record-level mean quadratic
losses. Quadratic loss and MSE are identical, and quadratic loss is
the negative of quadratic utility.
</p>
</li>
<li><p> The <code>"Quadratic Utility"</code> discrepancy measure provides
the mean quadratic utility for each record across all replicates, and
the discrepancy statistic is the mean of the record-level mean
quadratic utilities. Quadratic utility is the negative of quadratic
loss.
</p>
</li>
<li><p> The <code>"RMSE"</code> (Root Mean Squared Error) discrepancy
measure provides the RMSE for each record across all replicates, and
the discrepancy statistic is the mean of the record-level RMSEs.
</p>
</li>
<li><p> The <code>"Skewness"</code> discrepancy measure returns the skewness
of <code class="reqn">\textbf{y}^{rep}</code> for each record, and the discrepancy
statistic is the mean for all records. This does not measure
discrepancies between the model and data, and is useful for finding
skewed replicate distributions.
</p>
</li>
<li><p> The <code>"max(yhat[i,]) &gt; max(y)"</code> discrepancy measure
returns a record-level indicator when a record's maximum
<code class="reqn">\textbf{y}^{rep}_i</code> exceeds the maximum of
<code class="reqn">\textbf{y}</code>. The discrepancy statistic is the mean of the
record-level indicators, reporting the proportion of records with
replications that exceed the maximum of <code class="reqn">\textbf{y}</code>.
</p>
</li>
<li><p> The <code>"mean(yhat[i,]) &gt; mean(y)"</code> discrepancy measure
returns a record-level indicator when the mean of a record's
<code class="reqn">\textbf{y}^{rep}_i</code> is greater than the mean of
<code class="reqn">\textbf{y}</code>. The discrepancy statistic is the mean of the
record-level indicators, reporting the proportion of records with
mean replications that exceed the mean of <code class="reqn">\textbf{y}</code>.
</p>
</li>
<li><p> The <code>"mean(yhat[i,] &gt; d)"</code> discrepancy measure returns a
record-level proportion of <code class="reqn">\textbf{y}^{rep}_i</code> that
exceeds a specified value, <code>d</code>. The discrepancy statistic is the
mean of the record-level proportions.
</p>
</li>
<li><p> The <code>"mean(yhat[i,] &gt; mean(y))"</code> discrepancy measure
returns a record-level proportion of
<code class="reqn">\textbf{y}^{rep}_i</code> that exceeds the mean of
<code class="reqn">\textbf{y}</code>. The discrepancy statistic is the mean of the
record-level proportions.
</p>
</li>
<li><p> The <code>"min(yhat[i,]) &lt; min(y)"</code> discrepancy measure
returns a record-level indicator when a record's minimum
<code class="reqn">\textbf{y}^{rep}_i</code> is less than the minimum of
<code class="reqn">\textbf{y}</code>. The discrepancy statistic is the mean of the
record-level indicators, reporting the proportion of records with
replications less than the minimum of <code class="reqn">\textbf{y}</code>.
</p>
</li>
<li><p> The <code>"round(yhat[i,]) = d"</code> discrepancy measure returns a
record-level proportion of <code class="reqn">\textbf{y}^{rep}_i</code> that,
when rounded, is equal to a specified discrete value, <code>d</code>. The
discrepancy statistic is the mean of the record-level proportions.
</p>
</li>
<li><p> The <code>"sd(yhat[i,]) &gt; sd(y)"</code> discrepancy measure returns a
record-level indicator when the standard deviation of replicates is
larger than the standard deviation of all of <code class="reqn">\textbf{y}</code>. The
discrepancy statistic is the mean of the record-level indicators,
reporting the proportion of records with larger standard deviations
than <code class="reqn">\textbf{y}</code>.
</p>
</li>
<li><p> The <code>"p(yhat[i,] != y[i])"</code> discrepancy measure returns
the record-level probability that <code class="reqn">\textbf{y}^{rep}_i</code>
is not equal to <code class="reqn">\textbf{y}</code>. This is valid when
<code class="reqn">\textbf{y}</code> is categorical and <code>yhat</code> is the predicted
category. The probability is the proportion of replicates.
</p>
</li></ul>

<p>After observing a discrepancy statistic, the user attempts to improve
the model by revising the model to account for discrepancies between
data and the current model. This approach to model revision relies on
an analysis of the discrepancy statistic. Given a discrepancy measure
that is based on model fit, such as the L-criterion, the user may
correlate the record-level discrepancy statistics with the dependent
variable, independent variables, and interactions of independent
variables. The discrepancy statistic should not correlate with the
dependent and independent variables. Interaction variables may be
useful for exploring new relationships that are not in the current
model. Alternatively, a decision tree may be applied to the
record-level discrepancy statistics, given the independent variables,
in an effort to find relationships in the data that may be helpful
in the model. Model revision may involve the addition of a finite
mixture component to account for outliers in discrepancy, or
specifying the model with a distribution that is more robust to
outliers. There are too many suggestions to include here, and
discrepancy analysis varies by model.
</p>


<h3>Value</h3>

<p>This function returns a list with the following components:
</p>
<table>
<tr><td><code>BPIC</code></td>
<td>
<p>The Bayesian Predictive Information Criterion (BPIC) was
introduced by Ando (2007). BPIC is a variation of the Deviance
Information Criterion (DIC) that has been modified for predictive
distributions. For more information on DIC (Spiegelhalter
et al., 2002), see the accompanying vignette entitled &quot;Bayesian
Inference&quot;. <code class="reqn">BPIC = Dbar + 2pD</code>. The goal is to minimize BPIC.</p>
</td></tr>
<tr><td><code>Concordance</code></td>
<td>
<p>This is the percentage of the records of y that are
within the 95% quantile-based probability interval (see
<code><a href="#topic+p.interval">p.interval</a></code>) of <code class="reqn">\textbf{y}^{rep}</code>.
Gelfand's suggested goal is to achieve 95% predictive concordance.
Lower percentages indicate too many outliers and a poor fit of the
model to the data, and higher percentages may suggest
overfitting. Concordance occurs only when <code class="reqn">\textbf{y}</code> is
continuous.</p>
</td></tr>
<tr><td><code>Mean Lift</code></td>
<td>
<p>This is the mean of the record-level lifts, and
occurs only when <code class="reqn">\textbf{y}</code> is specified as categorical
with <code>Categorical=TRUE</code>.</p>
</td></tr>
<tr><td><code>Discrepancy.Statistic</code></td>
<td>
<p>This is only reported if the
<code>Discrep</code> argument receives a valid discrepancy measure as
listed above. The <code>Discrep</code> applies to each record of
<code class="reqn">\textbf{y}</code>, and the <code>Discrepancy.Statistic</code> reports
the results of the discrepancy measure on the entire data set. For
example, if <code>Discrep="min(yhat[i,]) &lt; min(y)"</code>, then the
overall result is the proportion of records in which the minimum
sample of yhat was less than the overall minimum
<code class="reqn">\textbf{y}</code>. This is <code>Pr(min(yhat[i,]) &lt; min(y) | y,
      Theta)</code>, where <code>Theta</code> is the parameter set.</p>
</td></tr>
<tr><td><code>L-criterion</code></td>
<td>
<p>The L-criterion (Laud and Ibrahim, 1995) was
developed for model and variable selection. It is a sum of two
components: one involves the predictive variance and the other
includes the accuracy of the means of the predictive
distribution. The L-criterion measures model performance with a
combination of how close its predictions are to the observed data
and variability of the predictions. Better models have smaller
values of <code>L</code>. <code>L</code> is measured in the same units as the
response variable, and measures how close the data vector
<code class="reqn">\textbf{y}</code> is to the predictive distribution. In
addition to the value of <code>L</code>, there is a value for <code>S.L</code>,
which is the calibration number of <code>L</code>, and is useful in
determining how much of a decrease is necessary between models to be
noteworthy.</p>
</td></tr>
<tr><td><code>Summary</code></td>
<td>
<p>When <code class="reqn">\textbf{y}</code> is continuous, this is a
<code class="reqn">N \times 8</code> matrix, where <code class="reqn">N</code> is the number of
records of <code class="reqn">\textbf{y}</code> and there are 8 columns, as follows:
y, Mean, SD, LB (the 2.5% quantile), Median, UB (the 97.5%
quantile), PQ (the predictive quantile, which is
<code class="reqn">Pr(\textbf{y}^{rep} \ge \textbf{y})</code>), and
Test, which shows the record-level result of a test, if
specified. When <code class="reqn">\textbf{y}</code> is categorical, this matrix has
a number of columns equal to the number of categories of
<code class="reqn">\textbf{y}</code> plus 3, also including <code>y</code>, <code>Lift</code>,
and <code>Discrep</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Statisticat, LLC.</p>


<h3>References</h3>

<p>Ando, T. (2007). &quot;Bayesian Predictive Information Criterion for
the Evaluation of Hierarchical Bayesian and Empirical Bayes Models&quot;.
<em>Biometrika</em>, 94(2), p. 443&ndash;458.
</p>
<p>Gelfand, A. (1996). &quot;Model Determination Using Sampling Based
Methods&quot;. In Gilks, W., Richardson, S., Spiegehalter, D., Chapter 9 in
Markov Chain Monte Carlo in Practice. Chapman and Hall: Boca Raton, FL.
</p>
<p>Gelfand, A. and Ghosh, S. (1998). &quot;Model Choice: A Minimum Posterior
Predictive Loss Approach&quot;. <em>Biometrika</em>, 85, p. 1&ndash;11.
</p>
<p>Gelman, A., Meng, X.L., and Stern H. (1996). &quot;Posterior Predictive
Assessment of Model Fitness via Realized Discrepancies&quot;.
<em>Statistica Sinica</em>, 6, p. 733&ndash;807.
</p>
<p>Laud, P.W. and Ibrahim, J.G. (1995). &quot;Predictive Model
Selection&quot;. <em>Journal of the Royal Statistical Society</em>, B 57,
p. 247&ndash;262.
</p>
<p>Spiegelhalter, D.J., Best, N.G., Carlin, B.P., and van der Linde, A.
(2002). &quot;Bayesian Measures of Model Complexity and Fit (with
Discussion)&quot;. <em>Journal of the Royal Statistical Society</em>, B 64,
p. 583&ndash;639.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>,
<code><a href="#topic+predict.demonoid">predict.demonoid</a></code>, and
<code><a href="#topic+p.interval">p.interval</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### See the LaplacesDemon function for an example.</code></pre>

<hr>
<h2 id='summary.iterquad.ppc'>Posterior Predictive Check Summary</h2><span id='topic+summary.iterquad.ppc'></span>

<h3>Description</h3>

<p>This may be used to summarize either new, unobserved instances of
<code class="reqn">\textbf{y}</code> (called <code class="reqn">\textbf{y}^{new}</code>) or
replicates of <code class="reqn">\textbf{y}</code> (called
<code class="reqn">\textbf{y}^{rep}</code>). Either <code class="reqn">\textbf{y}^{new}</code> or
<code class="reqn">\textbf{y}^{rep}</code> is summarized, depending on
<code><a href="#topic+predict.iterquad">predict.iterquad</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'iterquad.ppc'
summary(object, Categorical, Rows,
     Discrep, d, Quiet, ...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.iterquad.ppc_+3A_object">object</code></td>
<td>
<p>An object of class <code>iterquad.ppc</code> is required.</p>
</td></tr>
<tr><td><code id="summary.iterquad.ppc_+3A_categorical">Categorical</code></td>
<td>
<p>Logical. If <code>TRUE</code>, then <code>y</code> and
<code>yhat</code> are considered to be categorical (such as y=0 or y=1),
rather than continuous.</p>
</td></tr>
<tr><td><code id="summary.iterquad.ppc_+3A_rows">Rows</code></td>
<td>
<p>An optional vector of row numbers, for example
<code>c(1:10)</code>. All rows will be estimated, but only these rows will
appear in the summary.</p>
</td></tr>
<tr><td><code id="summary.iterquad.ppc_+3A_discrep">Discrep</code></td>
<td>
<p>A character string indicating a discrepancy
test. <code>Discrep</code> defaults to <code>NULL</code>. Valid character
strings when <code>y</code> is continuous are: <code>"Chi-Square"</code>,
<code>"Chi-Square2"</code>, <code>"Kurtosis"</code>, <code>"L-criterion"</code>,
<code>"MASE"</code>, <code>"MSE"</code>, <code>"PPL"</code>, <code>"Quadratic Loss"</code>,
<code>"Quadratic Utility"</code>, <code>"RMSE"</code>, <code>"Skewness"</code>,
<code>"max(yhat[i,]) &gt; max(y)"</code>, <code>"mean(yhat[i,]) &gt; mean(y)"</code>,
<code>"mean(yhat[i,] &gt; d)"</code>, <code>"mean(yhat[i,] &gt; mean(y))"</code>,
<code>"min(yhat[i,]) &lt; min(y)"</code>, <code>"round(yhat[i,]) = d"</code>, and
<code>"sd(yhat[i,]) &gt; sd(y)"</code>. Valid character strings when <code>y</code>
is categorical are: <code>"p(yhat[i,] != y[i])"</code>. Kurtosis and
skewness are not discrepancies, but are included here for convenience.</p>
</td></tr>
<tr><td><code id="summary.iterquad.ppc_+3A_d">d</code></td>
<td>
<p>This is an optional integer to be used with the
<code>Discrep</code> argument above, and it defaults to <code>d=0</code>.</p>
</td></tr>
<tr><td><code id="summary.iterquad.ppc_+3A_quiet">Quiet</code></td>
<td>
<p>This logical argument defaults to <code>FALSE</code> and will
print results to the console. When <code>TRUE</code>, results are not
printed.</p>
</td></tr>
<tr><td><code id="summary.iterquad.ppc_+3A_...">...</code></td>
<td>
<p>Additional arguments are unused.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function summarizes an object of class <code>iterquad.ppc</code>, which
consists of posterior predictive checks on either
<code class="reqn">\textbf{y}^{new}</code> or <code class="reqn">\textbf{y}^{rep}</code>,
depending respectively on whether unobserved instances of
<code class="reqn">\textbf{y}</code> or the model sample of <code class="reqn">\textbf{y}</code> was
used in the <code><a href="#topic+predict.iterquad">predict.iterquad</a></code> function. The deviance and
monitored variables are also summarized.
</p>
<p>The purpose of a posterior predictive check is to assess how well (or
poorly) the model fits the data, or to assess discrepancies between
the model and the data. For more information on posterior predictive
checks, see
<a href="https://web.archive.org/web/20150215050702/http://www.bayesian-inference.com/posteriorpredictivechecks">https://web.archive.org/web/20150215050702/http://www.bayesian-inference.com/posteriorpredictivechecks</a>.
</p>
<p>When <code class="reqn">\textbf{y}</code> is continuous and known, this function
estimates the predictive concordance between <code class="reqn">\textbf{y}</code> and
<code class="reqn">\textbf{y}^{rep}</code> as per Gelfand (1996), and the
predictive quantile (PQ), which is for record-level outlier detection
used to calculate Gelfand's predictive concordance.
</p>
<p>When <code class="reqn">\textbf{y}</code> is categorical and known, this function
estimates the record-level lift, which is
<code>p(yhat[i,] = y[i]) / [p(y = j) / n]</code>, or
the number of correctly predicted samples over the rate of that
category of <code class="reqn">\textbf{y}</code> in vector <code class="reqn">\textbf{y}</code>.
</p>
<p>A discrepancy measure is an approach to studying discrepancies between
the model and data (Gelman et al., 1996). Below is a list of
discrepancy measures, followed by a brief introduction to discrepancy
analysis:
</p>

<ul>
<li><p> The <code>"Chi-Square"</code> discrepancy measure is the chi-square
goodness-of-fit test that is recommended by Gelman. For each record
i=1:N, this returns (y[i] - E(y[i]))^2 / var(yhat[i,]).
</p>
</li>
<li><p> The <code>"Chi-Square2"</code> discrepancy measure returns the
following for each record: Pr(chisq.rep[i,] &gt; chisq.obs[i,]), where
chisq.obs[i,] &lt;- (y[i] - E(y[i]))^2 / E(y[i]), and chisq.rep[i,] &lt;-
(yhat[i,] - E(yhat[i,]))^2 / E(yhat[i,]), and the overall
discrepancy is the percent of records that were outside of the 95%
quantile-based probability interval (see <code><a href="#topic+p.interval">p.interval</a></code>).
</p>
</li>
<li><p> The <code>"Kurtosis"</code> discrepancy measure returns the kurtosis
of <code class="reqn">\textbf{y}^{rep}</code> for each record, and the discrepancy
statistic is the mean for all records. This does not measure
discrepancies between the model and data, and is useful for finding
kurtotic replicate distributions.
</p>
</li>
<li><p> The <code>"L-criterion"</code> discrepancy measure of Laud and Ibrahim
(1995) provides the record-level combination of two components (see
below), and the discrepancy statistic is the sum, <code>L</code>, as well as
a calibration number, <code>S.L</code>. For more information on the
L-criterion, see the accompanying vignette entitled &quot;Bayesian
Inference&quot;.
</p>
</li>
<li><p> The <code>"MASE"</code> (Mean Absolute Scaled Error) is a
discrepancy measure for the accuracy of time-series forecasts,
estimated as <code>(|y - yhat|) / mean(abs(diff(y)))</code>. The discrepancy
statistic is the mean of the record-level values.
</p>
</li>
<li><p> The <code>"MSE"</code> (Mean Squared Error) discrepancy measure
provides the MSE for each record across all replicates, and the
discrepancy statistic is the mean of the record-level MSEs. MSE and
quadratic loss are identical.
</p>
</li>
<li><p> The <code>"PPL"</code> (Posterior Predictive Loss) discrepancy
measure of Gelfand and Ghosh (1998) provides the record-level
combination of two components: one involves the predictive variance
and the other includes the accuracy of the means of the predictive
distribution. The <code>d=0</code> argument applies the following weight to
the accuracy component, which is then added to the variance component:
<code class="reqn">d/(d+1)</code>. For <code class="reqn">\textbf{y}^{new}</code>, use <code class="reqn">d=0</code>. For
<code class="reqn">\textbf{y}^{rep}</code> and model comparison, <code class="reqn">d</code> is
commonly set to 1, 10, or 100000. Larger values of <code class="reqn">d</code> put more
stress on fit and downgrade the precision of the estimates.
</p>
</li>
<li><p> The <code>"Quadratic Loss"</code> discrepancy measure provides the
mean quadratic loss for each record across all replicates, and the
discrepancy statistic is the mean of the record-level mean quadratic
losses. Quadratic loss and MSE are identical, and quadratic loss is
the negative of quadratic utility.
</p>
</li>
<li><p> The <code>"Quadratic Utility"</code> discrepancy measure provides
the mean quadratic utility for each record across all replicates, and
the discrepancy statistic is the mean of the record-level mean
quadratic utilities. Quadratic utility is the negative of quadratic
loss.
</p>
</li>
<li><p> The <code>"RMSE"</code> (Root Mean Squared Error) discrepancy
measure provides the RMSE for each record across all replicates, and
the discrepancy statistic is the mean of the record-level RMSEs.
</p>
</li>
<li><p> The <code>"Skewness"</code> discrepancy measure returns the skewness
of <code class="reqn">\textbf{y}^{rep}</code> for each record, and the discrepancy
statistic is the mean for all records. This does not measure
discrepancies between the model and data, and is useful for finding
skewed replicate distributions.
</p>
</li>
<li><p> The <code>"max(yhat[i,]) &gt; max(y)"</code> discrepancy measure
returns a record-level indicator when a record's maximum
<code class="reqn">\textbf{y}^{rep}_i</code> exceeds the maximum of
<code class="reqn">\textbf{y}</code>. The discrepancy statistic is the mean of the
record-level indicators, reporting the proportion of records with
replications that exceed the maximum of <code class="reqn">\textbf{y}</code>.
</p>
</li>
<li><p> The <code>"mean(yhat[i,]) &gt; mean(y)"</code> discrepancy measure
returns a record-level indicator when the mean of a record's
<code class="reqn">\textbf{y}^{rep}_i</code> is greater than the mean of
<code class="reqn">\textbf{y}</code>. The discrepancy statistic is the mean of the
record-level indicators, reporting the proportion of records with
mean replications that exceed the mean of <code class="reqn">\textbf{y}</code>.
</p>
</li>
<li><p> The <code>"mean(yhat[i,] &gt; d)"</code> discrepancy measure returns a
record-level proportion of <code class="reqn">\textbf{y}^{rep}_i</code> that
exceeds a specified value, <code>d</code>. The discrepancy statistic is the
mean of the record-level proportions.
</p>
</li>
<li><p> The <code>"mean(yhat[i,] &gt; mean(y))"</code> discrepancy measure
returns a record-level proportion of
<code class="reqn">\textbf{y}^{rep}_i</code> that exceeds the mean of
<code class="reqn">\textbf{y}</code>. The discrepancy statistic is the mean of the
record-level proportions.
</p>
</li>
<li><p> The <code>"min(yhat[i,]) &lt; min(y)"</code> discrepancy measure
returns a record-level indicator when a record's minimum
<code class="reqn">\textbf{y}^{rep}_i</code> is less than the minimum of
<code class="reqn">\textbf{y}</code>. The discrepancy statistic is the mean of the
record-level indicators, reporting the proportion of records with
replications less than the minimum of <code class="reqn">\textbf{y}</code>.
</p>
</li>
<li><p> The <code>"round(yhat[i,]) = d"</code> discrepancy measure returns a
record-level proportion of <code class="reqn">\textbf{y}^{rep}_i</code> that,
when rounded, is equal to a specified discrete value, <code>d</code>. The
discrepancy statistic is the mean of the record-level proportions.
</p>
</li>
<li><p> The <code>"sd(yhat[i,]) &gt; sd(y)"</code> discrepancy measure returns a
record-level indicator when the standard deviation of replicates is
larger than the standard deviation of all of <code class="reqn">\textbf{y}</code>. The
discrepancy statistic is the mean of the record-level indicators,
reporting the proportion of records with larger standard deviations
than <code class="reqn">\textbf{y}</code>.
</p>
</li>
<li><p> The <code>"p(yhat[i,] != y[i])"</code> discrepancy measure returns
the record-level probability that <code class="reqn">\textbf{y}^{rep}_i</code>
is not equal to <code class="reqn">\textbf{y}</code>. This is valid when
<code class="reqn">\textbf{y}</code> is categorical and <code>yhat</code> is the predicted
category. The probability is the proportion of replicates.
</p>
</li></ul>

<p>After observing a discrepancy statistic, the user attempts to improve
the model by revising the model to account for discrepancies between
data and the current model. This approach to model revision relies on
an analysis of the discrepancy statistic. Given a discrepancy measure
that is based on model fit, such as the L-criterion, the user may
correlate the record-level discrepancy statistics with the dependent
variable, independent variables, and interactions of independent
variables. The discrepancy statistic should not correlate with the
dependent and independent variables. Interaction variables may be
useful for exploring new relationships that are not in the current
model. Alternatively, a decision tree may be applied to the
record-level discrepancy statistics, given the independent variables,
in an effort to find relationships in the data that may be helpful
in the model. Model revision may involve the addition of a finite
mixture component to account for outliers in discrepancy, or
specifying the model with a distribution that is more robust to
outliers. There are too many suggestions to include here, and
discrepancy analysis varies by model.
</p>


<h3>Value</h3>

<p>This function returns a list with the following components:
</p>
<table>
<tr><td><code>BPIC</code></td>
<td>
<p>The Bayesian Predictive Information Criterion (BPIC) was
introduced by Ando (2007). BPIC is a variation of the Deviance
Information Criterion (DIC) that has been modified for predictive
distributions. For more information on DIC (Spiegelhalter
et al., 2002), see the accompanying vignette entitled &quot;Bayesian
Inference&quot;. <code class="reqn">BPIC = Dbar + 2pD</code>. The goal is to minimize BPIC.</p>
</td></tr>
<tr><td><code>Concordance</code></td>
<td>
<p>This is the percentage of the records of y that are
within the 95% quantile-based probability interval (see
<code><a href="#topic+p.interval">p.interval</a></code>) of <code class="reqn">\textbf{y}^{rep}</code>.
Gelfand's suggested goal is to achieve 95% predictive concordance.
Lower percentages indicate too many outliers and a poor fit of the
model to the data, and higher percentages may suggest overfitting.
Concordance occurs only when <code class="reqn">\textbf{y}</code> is continuous.</p>
</td></tr>
<tr><td><code>Mean Lift</code></td>
<td>
<p>This is the mean of the record-level lifts, and
occurs only when <code class="reqn">\textbf{y}</code> is specified as categorical
with <code>Categorical=TRUE</code>.</p>
</td></tr>
<tr><td><code>Discrepancy.Statistic</code></td>
<td>
<p>This is only reported if the
<code>Discrep</code> argument receives a valid discrepancy measure as
listed above. The <code>Discrep</code> applies to each record of
<code class="reqn">\textbf{y}</code>, and the <code>Discrepancy.Statistic</code> reports
the results of the discrepancy measure on the entire data set. For
example, if <code>Discrep="min(yhat[i,]) &lt; min(y)"</code>, then the
overall result is the proportion of records in which the minimum
sample of yhat was less than the overall minimum
<code class="reqn">\textbf{y}</code>. This is <code>Pr(min(yhat[i,]) &lt; min(y) | y,
      Theta)</code>, where <code>Theta</code> is the parameter set.</p>
</td></tr>
<tr><td><code>L-criterion</code></td>
<td>
<p>The L-criterion (Laud and Ibrahim, 1995) was
developed for model and variable selection. It is a sum of two
components: one involves the predictive variance and the other
includes the accuracy of the means of the predictive
distribution. The L-criterion measures model performance with a
combination of how close its predictions are to the observed data
and variability of the predictions. Better models have smaller
values of <code>L</code>. <code>L</code> is measured in the same units as
the response variable, and measures how close the data vector
<code class="reqn">\textbf{y}</code> is to the predictive distribution. In addition
to the value of <code>L</code>, there is a value for <code>S.L</code>, which is
the calibration number of <code>L</code>, and is useful in determining how
much of a decrease is necessary between models to be noteworthy.</p>
</td></tr>
<tr><td><code>Monitor</code></td>
<td>
<p>This is a <code class="reqn">N \times 5</code> matrix, where <code class="reqn">N</code>
is the number of monitored variables and there are 5 columns, as
follows: Mean, SD, LB (the 2.5% quantile), Median, and UB (the
97.5% quantile).</p>
</td></tr>
<tr><td><code>Summary</code></td>
<td>
<p>When <code class="reqn">\textbf{y}</code> is continuous, this is a
<code class="reqn">N \times 8</code> matrix, where <code class="reqn">N</code> is the number of
records of <code class="reqn">\textbf{y}</code> and there are 8 columns, as follows:
y, Mean, SD, LB (the 2.5% quantile), Median, UB (the 97.5%
quantile), PQ (the predictive quantile, which is
<code class="reqn">Pr(\textbf{y}^{rep} \ge \textbf{y})</code>), and
Test, which shows the record-level result of a test, if
specified. When <code class="reqn">\textbf{y}</code> is categorical, this matrix has
a number of columns equal to the number of categories of
<code class="reqn">\textbf{y}</code> plus 3, also including <code>y</code>, <code>Lift</code>,
and <code>Discrep</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Statisticat, LLC.</p>


<h3>References</h3>

<p>Ando, T. (2007). &quot;Bayesian Predictive Information Criterion for
the Evaluation of Hierarchical Bayesian and Empirical Bayes Models&quot;.
<em>Biometrika</em>, 94(2), p. 443&ndash;458.
</p>
<p>Gelfand, A. (1996). &quot;Model Determination Using Sampling Based
Methods&quot;. In Gilks, W., Richardson, S., Spiegehalter, D., Chapter 9 in
Markov Chain Monte Carlo in Practice. Chapman and Hall: Boca Raton, FL.
</p>
<p>Gelfand, A. and Ghosh, S. (1998). &quot;Model Choice: A Minimum Posterior
Predictive Loss Approach&quot;. <em>Biometrika</em>, 85, p. 1&ndash;11.
</p>
<p>Gelman, A., Meng, X.L., and Stern H. (1996). &quot;Posterior Predictive
Assessment of Model Fitness via Realized Discrepancies&quot;.
<em>Statistica Sinica</em>, 6, p. 733&ndash;807.
</p>
<p>Laud, P.W. and Ibrahim, J.G. (1995). &quot;Predictive Model Selection&quot;.
<em>Journal of the Royal Statistical Society</em>, B 57, p. 247&ndash;262.
</p>
<p>Spiegelhalter, D.J., Best, N.G., Carlin, B.P., and van der Linde, A.
(2002). &quot;Bayesian Measures of Model Complexity and Fit (with
Discussion)&quot;. <em>Journal of the Royal Statistical Society</em>, B 64,
p. 583&ndash;639.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code>,
<code><a href="#topic+predict.iterquad">predict.iterquad</a></code>, and
<code><a href="#topic+p.interval">p.interval</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### See the IterativeQuadrature function for an example.</code></pre>

<hr>
<h2 id='summary.laplace.ppc'>Posterior Predictive Check Summary</h2><span id='topic+summary.laplace.ppc'></span>

<h3>Description</h3>

<p>This may be used to summarize either new, unobserved instances of
<code class="reqn">\textbf{y}</code> (called <code class="reqn">\textbf{y}^{new}</code>) or
replicates of <code class="reqn">\textbf{y}</code> (called
<code class="reqn">\textbf{y}^{rep}</code>). Either <code class="reqn">\textbf{y}^{new}</code> or
<code class="reqn">\textbf{y}^{rep}</code> is summarized, depending on
<code><a href="#topic+predict.laplace">predict.laplace</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'laplace.ppc'
summary(object, Categorical, Rows, Discrep,
     d, Quiet, ...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.laplace.ppc_+3A_object">object</code></td>
<td>
<p>An object of class <code>laplace.ppc</code> is required.</p>
</td></tr>
<tr><td><code id="summary.laplace.ppc_+3A_categorical">Categorical</code></td>
<td>
<p>Logical. If <code>TRUE</code>, then <code>y</code> and
<code>yhat</code> are considered to be categorical (such as y=0 or y=1),
rather than continuous.</p>
</td></tr>
<tr><td><code id="summary.laplace.ppc_+3A_rows">Rows</code></td>
<td>
<p>An optional vector of row numbers, for example
<code>c(1:10)</code>. All rows will be estimated, but only these rows will
appear in the summary.</p>
</td></tr>
<tr><td><code id="summary.laplace.ppc_+3A_discrep">Discrep</code></td>
<td>
<p>A character string indicating a discrepancy
test. <code>Discrep</code> defaults to <code>NULL</code>. Valid character
strings when <code>y</code> is continuous are: <code>"Chi-Square"</code>,
<code>"Chi-Square2"</code>, <code>"Kurtosis"</code>, <code>"L-criterion"</code>,
<code>"MASE"</code>, <code>"MSE"</code>, <code>"PPL"</code>, <code>"Quadratic Loss"</code>,
<code>"Quadratic Utility"</code>, <code>"RMSE"</code>, <code>"Skewness"</code>,
<code>"max(yhat[i,]) &gt; max(y)"</code>, <code>"mean(yhat[i,]) &gt; mean(y)"</code>,
<code>"mean(yhat[i,] &gt; d)"</code>, <code>"mean(yhat[i,] &gt; mean(y))"</code>,
<code>"min(yhat[i,]) &lt; min(y)"</code>, <code>"round(yhat[i,]) = d"</code>, and
<code>"sd(yhat[i,]) &gt; sd(y)"</code>. Valid character strings when <code>y</code>
is categorical are: <code>"p(yhat[i,] != y[i])"</code>. Kurtosis and
skewness are not discrepancies, but are included here for convenience.</p>
</td></tr>
<tr><td><code id="summary.laplace.ppc_+3A_d">d</code></td>
<td>
<p>This is an optional integer to be used with the
<code>Discrep</code> argument above, and it defaults to <code>d=0</code>.</p>
</td></tr>
<tr><td><code id="summary.laplace.ppc_+3A_quiet">Quiet</code></td>
<td>
<p>This logical argument defaults to <code>FALSE</code> and will
print results to the console. When <code>TRUE</code>, results are not
printed.</p>
</td></tr>
<tr><td><code id="summary.laplace.ppc_+3A_...">...</code></td>
<td>
<p>Additional arguments are unused.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function summarizes an object of class <code>laplace.ppc</code>, which
consists of posterior predictive checks on either
<code class="reqn">\textbf{y}^{new}</code> or <code class="reqn">\textbf{y}^{rep}</code>,
depending respectively on whether unobserved instances of
<code class="reqn">\textbf{y}</code> or the model sample of <code class="reqn">\textbf{y}</code> was
used in the <code><a href="#topic+predict.laplace">predict.laplace</a></code> function. The deviance and
monitored variables are also summarized.
</p>
<p>The purpose of a posterior predictive check is to assess how well (or
poorly) the model fits the data, or to assess discrepancies between
the model and the data. For more information on posterior predictive
checks, see
<a href="https://web.archive.org/web/20150215050702/http://www.bayesian-inference.com/posteriorpredictivechecks">https://web.archive.org/web/20150215050702/http://www.bayesian-inference.com/posteriorpredictivechecks</a>.
</p>
<p>When <code class="reqn">\textbf{y}</code> is continuous and known, this function
estimates the predictive concordance between <code class="reqn">\textbf{y}</code> and
<code class="reqn">\textbf{y}^{rep}</code> as per Gelfand (1996), and the
predictive quantile (PQ), which is for record-level outlier detection
used to calculate Gelfand's predictive concordance.
</p>
<p>When <code class="reqn">\textbf{y}</code> is categorical and known, this function
estimates the record-level lift, which is
<code>p(yhat[i,] = y[i]) / [p(y = j) / n]</code>, or
the number of correctly predicted samples over the rate of that
category of <code class="reqn">\textbf{y}</code> in vector <code class="reqn">\textbf{y}</code>.
</p>
<p>A discrepancy measure is an approach to studying discrepancies between
the model and data (Gelman et al., 1996). Below is a list of
discrepancy measures, followed by a brief introduction to discrepancy
analysis:
</p>

<ul>
<li><p> The <code>"Chi-Square"</code> discrepancy measure is the chi-square
goodness-of-fit test that is recommended by Gelman. For each record
i=1:N, this returns (y[i] - E(y[i]))^2 / var(yhat[i,]).
</p>
</li>
<li><p> The <code>"Chi-Square2"</code> discrepancy measure returns the
following for each record: Pr(chisq.rep[i,] &gt; chisq.obs[i,]), where
chisq.obs[i,] &lt;- (y[i] - E(y[i]))^2 / E(y[i]), and chisq.rep[i,] &lt;-
(yhat[i,] - E(yhat[i,]))^2 / E(yhat[i,]), and the overall
discrepancy is the percent of records that were outside of the 95%
quantile-based probability interval (see <code><a href="#topic+p.interval">p.interval</a></code>).
</p>
</li>
<li><p> The <code>"Kurtosis"</code> discrepancy measure returns the kurtosis
of <code class="reqn">\textbf{y}^{rep}</code> for each record, and the discrepancy
statistic is the mean for all records. This does not measure
discrepancies between the model and data, and is useful for finding
kurtotic replicate distributions.
</p>
</li>
<li><p> The <code>"L-criterion"</code> discrepancy measure of Laud and Ibrahim
(1995) provides the record-level combination of two components (see
below), and the discrepancy statistic is the sum, <code>L</code>, as well as
a calibration number, <code>S.L</code>. For more information on the
L-criterion, see the accompanying vignette entitled &quot;Bayesian
Inference&quot;.
</p>
</li>
<li><p> The <code>"MASE"</code> (Mean Absolute Scaled Error) is a
discrepancy measure for the accuracy of time-series forecasts,
estimated as <code>(|y - yhat|) / mean(abs(diff(y)))</code>. The discrepancy
statistic is the mean of the record-level values.
</p>
</li>
<li><p> The <code>"MSE"</code> (Mean Squared Error) discrepancy measure
provides the MSE for each record across all replicates, and the
discrepancy statistic is the mean of the record-level MSEs. MSE and
quadratic loss are identical.
</p>
</li>
<li><p> The <code>"PPL"</code> (Posterior Predictive Loss) discrepancy
measure of Gelfand and Ghosh (1998) provides the record-level
combination of two components: one involves the predictive variance
and the other includes the accuracy of the means of the predictive
distribution. The <code>d=0</code> argument applies the following weight to
the accuracy component, which is then added to the variance component:
<code class="reqn">d/(d+1)</code>. For <code class="reqn">\textbf{y}^{new}</code>, use <code class="reqn">d=0</code>. For
<code class="reqn">\textbf{y}^{rep}</code> and model comparison, <code class="reqn">d</code> is
commonly set to 1, 10, or 100000. Larger values of <code class="reqn">d</code> put more
stress on fit and downgrade the precision of the estimates.
</p>
</li>
<li><p> The <code>"Quadratic Loss"</code> discrepancy measure provides the
mean quadratic loss for each record across all replicates, and the
discrepancy statistic is the mean of the record-level mean quadratic
losses. Quadratic loss and MSE are identical, and quadratic loss is
the negative of quadratic utility.
</p>
</li>
<li><p> The <code>"Quadratic Utility"</code> discrepancy measure provides
the mean quadratic utility for each record across all replicates, and
the discrepancy statistic is the mean of the record-level mean
quadratic utilities. Quadratic utility is the negative of quadratic
loss.
</p>
</li>
<li><p> The <code>"RMSE"</code> (Root Mean Squared Error) discrepancy
measure provides the RMSE for each record across all replicates, and
the discrepancy statistic is the mean of the record-level RMSEs.
</p>
</li>
<li><p> The <code>"Skewness"</code> discrepancy measure returns the skewness
of <code class="reqn">\textbf{y}^{rep}</code> for each record, and the discrepancy
statistic is the mean for all records. This does not measure
discrepancies between the model and data, and is useful for finding
skewed replicate distributions.
</p>
</li>
<li><p> The <code>"max(yhat[i,]) &gt; max(y)"</code> discrepancy measure
returns a record-level indicator when a record's maximum
<code class="reqn">\textbf{y}^{rep}_i</code> exceeds the maximum of
<code class="reqn">\textbf{y}</code>. The discrepancy statistic is the mean of the
record-level indicators, reporting the proportion of records with
replications that exceed the maximum of <code class="reqn">\textbf{y}</code>.
</p>
</li>
<li><p> The <code>"mean(yhat[i,]) &gt; mean(y)"</code> discrepancy measure
returns a record-level indicator when the mean of a record's
<code class="reqn">\textbf{y}^{rep}_i</code> is greater than the mean of
<code class="reqn">\textbf{y}</code>. The discrepancy statistic is the mean of the
record-level indicators, reporting the proportion of records with
mean replications that exceed the mean of <code class="reqn">\textbf{y}</code>.
</p>
</li>
<li><p> The <code>"mean(yhat[i,] &gt; d)"</code> discrepancy measure returns a
record-level proportion of <code class="reqn">\textbf{y}^{rep}_i</code> that
exceeds a specified value, <code>d</code>. The discrepancy statistic is the
mean of the record-level proportions.
</p>
</li>
<li><p> The <code>"mean(yhat[i,] &gt; mean(y))"</code> discrepancy measure
returns a record-level proportion of
<code class="reqn">\textbf{y}^{rep}_i</code> that exceeds the mean of
<code class="reqn">\textbf{y}</code>. The discrepancy statistic is the mean of the
record-level proportions.
</p>
</li>
<li><p> The <code>"min(yhat[i,]) &lt; min(y)"</code> discrepancy measure
returns a record-level indicator when a record's minimum
<code class="reqn">\textbf{y}^{rep}_i</code> is less than the minimum of
<code class="reqn">\textbf{y}</code>. The discrepancy statistic is the mean of the
record-level indicators, reporting the proportion of records with
replications less than the minimum of <code class="reqn">\textbf{y}</code>.
</p>
</li>
<li><p> The <code>"round(yhat[i,]) = d"</code> discrepancy measure returns a
record-level proportion of <code class="reqn">\textbf{y}^{rep}_i</code> that,
when rounded, is equal to a specified discrete value, <code>d</code>. The
discrepancy statistic is the mean of the record-level proportions.
</p>
</li>
<li><p> The <code>"sd(yhat[i,]) &gt; sd(y)"</code> discrepancy measure returns a
record-level indicator when the standard deviation of replicates is
larger than the standard deviation of all of <code class="reqn">\textbf{y}</code>. The
discrepancy statistic is the mean of the record-level indicators,
reporting the proportion of records with larger standard deviations
than <code class="reqn">\textbf{y}</code>.
</p>
</li>
<li><p> The <code>"p(yhat[i,] != y[i])"</code> discrepancy measure returns
the record-level probability that <code class="reqn">\textbf{y}^{rep}_i</code>
is not equal to <code class="reqn">\textbf{y}</code>. This is valid when
<code class="reqn">\textbf{y}</code> is categorical and <code>yhat</code> is the predicted
category. The probability is the proportion of replicates.
</p>
</li></ul>

<p>After observing a discrepancy statistic, the user attempts to improve
the model by revising the model to account for discrepancies between
data and the current model. This approach to model revision relies on
an analysis of the discrepancy statistic. Given a discrepancy measure
that is based on model fit, such as the L-criterion, the user may
correlate the record-level discrepancy statistics with the dependent
variable, independent variables, and interactions of independent
variables. The discrepancy statistic should not correlate with the
dependent and independent variables. Interaction variables may be
useful for exploring new relationships that are not in the current
model. Alternatively, a decision tree may be applied to the
record-level discrepancy statistics, given the independent variables,
in an effort to find relationships in the data that may be helpful
in the model. Model revision may involve the addition of a finite
mixture component to account for outliers in discrepancy, or
specifying the model with a distribution that is more robust to
outliers. There are too many suggestions to include here, and
discrepancy analysis varies by model.
</p>


<h3>Value</h3>

<p>This function returns a list with the following components:
</p>
<table>
<tr><td><code>BPIC</code></td>
<td>
<p>The Bayesian Predictive Information Criterion (BPIC) was
introduced by Ando (2007). BPIC is a variation of the Deviance
Information Criterion (DIC) that has been modified for predictive
distributions. For more information on DIC (Spiegelhalter
et al., 2002), see the accompanying vignette entitled &quot;Bayesian
Inference&quot;. <code class="reqn">BPIC = Dbar + 2pD</code>. The goal is to minimize BPIC.</p>
</td></tr>
<tr><td><code>Concordance</code></td>
<td>
<p>This is the percentage of the records of y that are
within the 95% quantile-based probability interval (see
<code><a href="#topic+p.interval">p.interval</a></code>) of <code class="reqn">\textbf{y}^{rep}</code>.
Gelfand's suggested goal is to achieve 95% predictive concordance.
Lower percentages indicate too many outliers and a poor fit of the
model to the data, and higher percentages may suggest overfitting.
Concordance occurs only when <code class="reqn">\textbf{y}</code> is continuous.</p>
</td></tr>
<tr><td><code>Mean Lift</code></td>
<td>
<p>This is the mean of the record-level lifts, and
occurs only when <code class="reqn">\textbf{y}</code> is specified as categorical
with <code>Categorical=TRUE</code>.</p>
</td></tr>
<tr><td><code>Discrepancy.Statistic</code></td>
<td>
<p>This is only reported if the
<code>Discrep</code> argument receives a valid discrepancy measure as
listed above. The <code>Discrep</code> applies to each record of
<code class="reqn">\textbf{y}</code>, and the <code>Discrepancy.Statistic</code> reports
the results of the discrepancy measure on the entire data set. For
example, if <code>Discrep="min(yhat[i,]) &lt; min(y)"</code>, then the
overall result is the proportion of records in which the minimum
sample of yhat was less than the overall minimum
<code class="reqn">\textbf{y}</code>. This is <code>Pr(min(yhat[i,]) &lt; min(y) | y,
      Theta)</code>, where <code>Theta</code> is the parameter set.</p>
</td></tr>
<tr><td><code>L-criterion</code></td>
<td>
<p>The L-criterion (Laud and Ibrahim, 1995) was
developed for model and variable selection. It is a sum of two
components: one involves the predictive variance and the other
includes the accuracy of the means of the predictive
distribution. The L-criterion measures model performance with a
combination of how close its predictions are to the observed data
and variability of the predictions. Better models have smaller
values of <code>L</code>. <code>L</code> is measured in the same units as
the response variable, and measures how close the data vector
<code class="reqn">\textbf{y}</code> is to the predictive distribution. In addition
to the value of <code>L</code>, there is a value for <code>S.L</code>, which is
the calibration number of <code>L</code>, and is useful in determining how
much of a decrease is necessary between models to be noteworthy.</p>
</td></tr>
<tr><td><code>Monitor</code></td>
<td>
<p>This is a <code class="reqn">N \times 5</code> matrix, where <code class="reqn">N</code>
is the number of monitored variables and there are 5 columns, as
follows: Mean, SD, LB (the 2.5% quantile), Median, and UB (the
97.5% quantile).</p>
</td></tr>
<tr><td><code>Summary</code></td>
<td>
<p>When <code class="reqn">\textbf{y}</code> is continuous, this is a
<code class="reqn">N \times 8</code> matrix, where <code class="reqn">N</code> is the number of
records of <code class="reqn">\textbf{y}</code> and there are 8 columns, as follows:
y, Mean, SD, LB (the 2.5% quantile), Median, UB (the 97.5%
quantile), PQ (the predictive quantile, which is
<code class="reqn">Pr(\textbf{y}^{rep} \ge \textbf{y})</code>), and
Test, which shows the record-level result of a test, if
specified. When <code class="reqn">\textbf{y}</code> is categorical, this matrix has
a number of columns equal to the number of categories of
<code class="reqn">\textbf{y}</code> plus 3, also including <code>y</code>, <code>Lift</code>,
and <code>Discrep</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Statisticat, LLC.</p>


<h3>References</h3>

<p>Ando, T. (2007). &quot;Bayesian Predictive Information Criterion for
the Evaluation of Hierarchical Bayesian and Empirical Bayes Models&quot;.
<em>Biometrika</em>, 94(2), p. 443&ndash;458.
</p>
<p>Gelfand, A. (1996). &quot;Model Determination Using Sampling Based
Methods&quot;. In Gilks, W., Richardson, S., Spiegehalter, D., Chapter 9 in
Markov Chain Monte Carlo in Practice. Chapman and Hall: Boca Raton, FL.
</p>
<p>Gelfand, A. and Ghosh, S. (1998). &quot;Model Choice: A Minimum Posterior
Predictive Loss Approach&quot;. <em>Biometrika</em>, 85, p. 1&ndash;11.
</p>
<p>Gelman, A., Meng, X.L., and Stern H. (1996). &quot;Posterior Predictive
Assessment of Model Fitness via Realized Discrepancies&quot;.
<em>Statistica Sinica</em>, 6, p. 733&ndash;807.
</p>
<p>Laud, P.W. and Ibrahim, J.G. (1995). &quot;Predictive Model Selection&quot;.
<em>Journal of the Royal Statistical Society</em>, B 57, p. 247&ndash;262.
</p>
<p>Spiegelhalter, D.J., Best, N.G., Carlin, B.P., and van der Linde, A.
(2002). &quot;Bayesian Measures of Model Complexity and Fit (with
Discussion)&quot;. <em>Journal of the Royal Statistical Society</em>, B 64,
p. 583&ndash;639.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>,
<code><a href="#topic+predict.laplace">predict.laplace</a></code>, and
<code><a href="#topic+p.interval">p.interval</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### See the LaplaceApproximation function for an example.</code></pre>

<hr>
<h2 id='summary.miss'>MISS Summary</h2><span id='topic+summary.miss'></span>

<h3>Description</h3>

<p>This function summarizes posterior predictive distributions from
an object of class <code>miss</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'miss'
summary(object, ...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.miss_+3A_object">object</code></td>
<td>
<p>An object of class <code>miss</code> is required.</p>
</td></tr>
<tr><td><code id="summary.miss_+3A_...">...</code></td>
<td>
<p>Additional arguments are unused.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function summarizes the posterior predictive distributions from
an object of class <code>miss</code>.
</p>


<h3>Value</h3>

<p>This function returns a <code class="reqn">M \times 7</code> matrix, in which each
row is the posterior predictive distribution of one of <code class="reqn">M</code> missing
values. Columns are Mean, SD, MCSE, ESS, LB, Median, and UB.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+MISS">MISS</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### See the MISS function for an example.</code></pre>

<hr>
<h2 id='summary.pmc.ppc'>Posterior Predictive Check Summary</h2><span id='topic+summary.pmc.ppc'></span>

<h3>Description</h3>

<p>This may be used to summarize either new, unobserved instances of
<code class="reqn">\textbf{y}</code> (called <code class="reqn">\textbf{y}^{new}</code>) or
replicates of <code class="reqn">\textbf{y}</code> (called
<code class="reqn">\textbf{y}^{rep}</code>). Either <code class="reqn">\textbf{y}^{new}</code> or
<code class="reqn">\textbf{y}^{rep}</code> is summarized, depending on
<code><a href="#topic+predict.pmc">predict.pmc</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'pmc.ppc'
summary(object, Categorical, Rows,
     Discrep, d, Quiet, ...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.pmc.ppc_+3A_object">object</code></td>
<td>
<p>An object of class <code>pmc.ppc</code> is required.</p>
</td></tr>
<tr><td><code id="summary.pmc.ppc_+3A_categorical">Categorical</code></td>
<td>
<p>Logical. If <code>TRUE</code>, then <code>y</code> and
<code>yhat</code> are considered to be categorical (such as y=0 or y=1),
rather than continuous.</p>
</td></tr>
<tr><td><code id="summary.pmc.ppc_+3A_rows">Rows</code></td>
<td>
<p>An optional vector of row numbers, for example
<code>c(1:10)</code>. All rows will be estimated, but only these rows will
appear in the summary.</p>
</td></tr>
<tr><td><code id="summary.pmc.ppc_+3A_discrep">Discrep</code></td>
<td>
<p>A character string indicating a discrepancy
test. <code>Discrep</code> defaults to <code>NULL</code>. Valid character
strings when <code>y</code> is continuous are: <code>"Chi-Square"</code>,
<code>"Chi-Square2 "</code>, <code>"Kurtosis"</code>, <code>"L-criterion"</code>,
<code>"MASE"</code>, <code>"MSE"</code>, <code>"PPL"</code>, <code>"Quadratic Loss"</code>,
<code>"Quadratic Utility"</code>, <code>"RMSE"</code>, <code>"Skewness"</code>,
<code>"max(yhat[i,]) &gt; max(y)"</code>, <code>"mean(yhat[i,]) &gt; mean(y)"</code>,
<code>"mean(yhat[i,] &gt; d)"</code>, <code>"mean(yhat[i,] &gt; mean(y))"</code>,
<code>"min(yhat[i,]) &lt; min(y)"</code>, <code>"round(yhat[i,]) = d"</code>, and
<code>"sd(yhat[i,]) &gt; sd(y)"</code>. Valid character strings when <code>y</code>
is categorical are: <code>"p(yhat[i,] != y[i])"</code>. Kurtosis and
skewness are not discrepancies, but are included here for convenience.</p>
</td></tr>
<tr><td><code id="summary.pmc.ppc_+3A_d">d</code></td>
<td>
<p>This is an optional integer to be used with the
<code>Discrep</code> argument above, and it defaults to <code>d=0</code>.</p>
</td></tr>
<tr><td><code id="summary.pmc.ppc_+3A_quiet">Quiet</code></td>
<td>
<p>This logical argument defaults to <code>FALSE</code> and will
print results to the console. When <code>TRUE</code>, results are not
printed.</p>
</td></tr>
<tr><td><code id="summary.pmc.ppc_+3A_...">...</code></td>
<td>
<p>Additional arguments are unused.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function summarizes an object of class <code>pmc.ppc</code>, which
consists of posterior predictive checks on either
<code class="reqn">\textbf{y}^{new}</code> or <code class="reqn">\textbf{y}^{rep}</code>,
depending respectively on whether unobserved instances of
<code class="reqn">\textbf{y}</code> or the model sample of <code class="reqn">\textbf{y}</code> was
used in the <code><a href="#topic+predict.demonoid">predict.demonoid</a></code> function.
</p>
<p>The purpose of a posterior predictive check is to assess how well (or
poorly) the model fits the data, or to assess discrepancies between
the model and the data. For more information on posterior predictive
checks, see
<a href="https://web.archive.org/web/20150215050702/http://www.bayesian-inference.com/posteriorpredictivechecks">https://web.archive.org/web/20150215050702/http://www.bayesian-inference.com/posteriorpredictivechecks</a>.
</p>
<p>When <code class="reqn">\textbf{y}</code> is continuous and known, this function
estimates the predictive concordance between <code class="reqn">\textbf{y}</code> and
<code class="reqn">\textbf{y}^{rep}</code> as per Gelfand (1996), and the
predictive quantile (PQ), which is for record-level outlier detection
used to calculate Gelfand's predictive concordance.
</p>
<p>When <code class="reqn">\textbf{y}</code> is categorical and known, this function
estimates the record-level lift, which is
<code>p(yhat[i,] = y[i]) / [p(y = j) / n]</code>, or
the number of correctly predicted samples over the rate of that
category of <code class="reqn">\textbf{y}</code> in vector <code class="reqn">\textbf{y}</code>.
</p>
<p>A discrepancy measure is an approach to studying discrepancies between
the model and data (Gelman et al., 1996). Below is a list of
discrepancy measures, followed by a brief introduction to discrepancy
analysis:
</p>

<ul>
<li><p> The <code>"Chi-Square"</code> discrepancy measure is the chi-square
goodness-of-fit test that is recommended by Gelman. For each record
i=1:N, this returns (y[i] - E(y[i]))^2 / var(yhat[i,]).
</p>
</li>
<li><p> The <code>"Chi-Square2"</code> discrepancy measure returns the
following for each record: Pr(chisq.rep[i,] &gt; chisq.obs[i,]), where
chisq.obs[i,] &lt;- (y[i] - E(y[i]))^2 / E(y[i]), and chisq.rep[i,] &lt;-
(yhat[i,] - E(yhat[i,]))^2 / E(yhat[i,]), and the overall
discrepancy is the percent of records that were outside of the 95%
quantile-based probability interval (see <code><a href="#topic+p.interval">p.interval</a></code>).
</p>
</li>
<li><p> The <code>"Kurtosis"</code> discrepancy measure returns the kurtosis
of <code class="reqn">\textbf{y}^{rep}</code> for each record, and the discrepancy
statistic is the mean for all records. This does not measure
discrepancies between the model and data, and is useful for finding
kurtotic replicate distributions.
</p>
</li>
<li><p> The <code>"L-criterion"</code> discrepancy measure of Laud and Ibrahim
(1995) provides the record-level combination of two components (see
below), and the discrepancy statistic is the sum, <code>L</code>, as well as
a calibration number, <code>S.L</code>. For more information on the
L-criterion, see the accompanying vignette entitled &quot;Bayesian
Inference&quot;.
</p>
</li>
<li><p> The <code>"MASE"</code> (Mean Absolute Scaled Error) is a
discrepancy measure for the accuracy of time-series forecasts,
estimated as <code>(|y - yhat|) / mean(abs(diff(y)))</code>. The discrepancy
statistic is the mean of the record-level values.
</p>
</li>
<li><p> The <code>"MSE"</code> (Mean Squared Error) discrepancy measure
provides the MSE for each record across all replicates, and the
discrepancy statistic is the mean of the record-level MSEs. MSE and
quadratic loss are identical.
</p>
</li>
<li><p> The <code>"PPL"</code> (Posterior Predictive Loss) discrepancy
measure of Gelfand and Ghosh (1998) provides the record-level
combination of two components: one involves the predictive variance
and the other includes the accuracy of the means of the predictive
distribution. The <code>d=0</code> argument applies the following weight to
the accuracy component, which is then added to the variance component:
<code class="reqn">d/(d+1)</code>. For <code class="reqn">\textbf{y}^{new}</code>, use <code class="reqn">d=0</code>. For
<code class="reqn">\textbf{y}^{rep}</code> and model comparison, <code class="reqn">d</code> is
commonly set to 1, 10, or 100000. Larger values of <code class="reqn">d</code> put more
stress on fit and downgrade the precision of the estimates.
</p>
</li>
<li><p> The <code>"Quadratic Loss"</code> discrepancy measure provides the
mean quadratic loss for each record across all replicates, and the
discrepancy statistic is the mean of the record-level mean quadratic
losses. Quadratic loss and MSE are identical, and quadratic loss is
the negative of quadratic utility.
</p>
</li>
<li><p> The <code>"Quadratic Utility"</code> discrepancy measure provides
the mean quadratic utility for each record across all replicates, and
the discrepancy statistic is the mean of the record-level mean
quadratic utilities. Quadratic utility is the negative of quadratic
loss.
</p>
</li>
<li><p> The <code>"RMSE"</code> (Root Mean Squared Error) discrepancy
measure provides the RMSE for each record across all replicates, and
the discrepancy statistic is the mean of the record-level RMSEs.
</p>
</li>
<li><p> The <code>"Skewness"</code> discrepancy measure returns the skewness
of <code class="reqn">\textbf{y}^{rep}</code> for each record, and the discrepancy
statistic is the mean for all records. This does not measure
discrepancies between the model and data, and is useful for finding
skewed replicate distributions.
</p>
</li>
<li><p> The <code>"max(yhat[i,]) &gt; max(y)"</code> discrepancy measure
returns a record-level indicator when a record's maximum
<code class="reqn">\textbf{y}^{rep}_i</code> exceeds the maximum of
<code class="reqn">\textbf{y}</code>. The discrepancy statistic is the mean of the
record-level indicators, reporting the proportion of records with
replications that exceed the maximum of <code class="reqn">\textbf{y}</code>.
</p>
</li>
<li><p> The <code>"mean(yhat[i,]) &gt; mean(y)"</code> discrepancy measure
returns a record-level indicator when the mean of a record's
<code class="reqn">\textbf{y}^{rep}_i</code> is greater than the mean of
<code class="reqn">\textbf{y}</code>. The discrepancy statistic is the mean of the
record-level indicators, reporting the proportion of records with
mean replications that exceed the mean of <code class="reqn">\textbf{y}</code>.
</p>
</li>
<li><p> The <code>"mean(yhat[i,] &gt; d)"</code> discrepancy measure returns a
record-level proportion of <code class="reqn">\textbf{y}^{rep}_i</code> that
exceeds a specified value, <code>d</code>. The discrepancy statistic is the
mean of the record-level proportions.
</p>
</li>
<li><p> The <code>"mean(yhat[i,] &gt; mean(y))"</code> discrepancy measure
returns a record-level proportion of
<code class="reqn">\textbf{y}^{rep}_i</code> that exceeds the mean of
<code class="reqn">\textbf{y}</code>. The discrepancy statistic is the mean of the
record-level proportions.
</p>
</li>
<li><p> The <code>"min(yhat[i,]) &lt; min(y)"</code> discrepancy measure
returns a record-level indicator when a record's minimum
<code class="reqn">\textbf{y}^{rep}_i</code> is less than the minimum of
<code class="reqn">\textbf{y}</code>. The discrepancy statistic is the mean of the
record-level indicators, reporting the proportion of records with
replications less than the minimum of <code class="reqn">\textbf{y}</code>.
</p>
</li>
<li><p> The <code>"round(yhat[i,]) = d"</code> discrepancy measure returns a
record-level proportion of <code class="reqn">\textbf{y}^{rep}_i</code> that,
when rounded, is equal to a specified discrete value, <code>d</code>. The
discrepancy statistic is the mean of the record-level proportions.
</p>
</li>
<li><p> The <code>"sd(yhat[i,]) &gt; sd(y)"</code> discrepancy measure returns a
record-level indicator when the standard deviation of replicates is
larger than the standard deviation of all of <code class="reqn">\textbf{y}</code>. The
discrepancy statistic is the mean of the record-level indicators,
reporting the proportion of records with larger standard deviations
than <code class="reqn">\textbf{y}</code>.
</p>
</li>
<li><p> The <code>"p(yhat[i,] != y[i])"</code> discrepancy measure returns
the record-level probability that <code class="reqn">\textbf{y}^{rep}_i</code>
is not equal to <code class="reqn">\textbf{y}</code>. This is valid when
<code class="reqn">\textbf{y}</code> is categorical and <code>yhat</code> is the predicted
category. The probability is the proportion of replicates.
</p>
</li></ul>

<p>After observing a discrepancy statistic, the user attempts to improve
the model by revising the model to account for discrepancies between
data and the current model. This approach to model revision relies on
an analysis of the discrepancy statistic. Given a discrepancy measure
that is based on model fit, such as the L-criterion, the user may
correlate the record-level discrepancy statistics with the dependent
variable, independent variables, and interactions of independent
variables. The discrepancy statistic should not correlate with the
dependent and independent variables. Interaction variables may be
useful for exploring new relationships that are not in the current
model. Alternatively, a decision tree may be applied to the
record-level discrepancy statistics, given the independent variables,
in an effort to find relationships in the data that may be helpful
in the model. Model revision may involve the addition of a finite
mixture component to account for outliers in discrepancy, or
specifying the model with a distribution that is more robust to
outliers. There are too many suggestions to include here, and
discrepancy analysis varies by model.
</p>


<h3>Value</h3>

<p>This function returns a list with the following components:
</p>
<table>
<tr><td><code>BPIC</code></td>
<td>
<p>The Bayesian Predictive Information Criterion (BPIC) was
introduced by Ando (2007). BPIC is a variation of the Deviance
Information Criterion (DIC) that has been modified for predictive
distributions. For more information on DIC (Spiegelhalter
et al., 2002), see the accompanying vignette entitled &quot;Bayesian
Inference&quot;. <code class="reqn">BPIC = Dbar + 2pD</code>. The goal is to minimize BPIC.</p>
</td></tr>
<tr><td><code>Concordance</code></td>
<td>
<p>This is the percentage of the records of y that are
within the 95% quantile-based probability interval (see
<code><a href="#topic+p.interval">p.interval</a></code>) of <code class="reqn">\textbf{y}^{rep}</code>.
Gelfand's suggested goal is to achieve 95% predictive concordance.
Lower percentages indicate too many outliers and a poor fit of the
model to the data, and higher percentages may suggest
overfitting. Concordance occurs only when <code class="reqn">\textbf{y}</code> is
continuous.</p>
</td></tr>
<tr><td><code>Mean Lift</code></td>
<td>
<p>This is the mean of the record-level lifts, and
occurs only when <code class="reqn">\textbf{y}</code> is specified as categorical
with <code>Categorical=TRUE</code>.</p>
</td></tr>
<tr><td><code>Discrepancy.Statistic</code></td>
<td>
<p>This is only reported if the
<code>Discrep</code> argument receives a valid discrepancy measure as
listed above. The <code>Discrep</code> applies to each record of
<code class="reqn">\textbf{y}</code>, and the <code>Discrepancy.Statistic</code> reports
the results of the discrepancy measure on the entire data set. For
example, if <code>Discrep="min(yhat[i,]) &lt; min(y)"</code>, then the
overall result is the proportion of records in which the minimum
sample of yhat was less than the overall minimum
<code class="reqn">\textbf{y}</code>. This is <code>Pr(min(yhat[i,]) &lt; min(y) | y,
    Theta)</code>, where <code>Theta</code> is the parameter set.</p>
</td></tr>
<tr><td><code>L-criterion</code></td>
<td>
<p>The L-criterion (Laud and Ibrahim, 1995) was
developed for model and variable selection. It is a sum of two
components: one involves the predictive variance and the other
includes the accuracy of the means of the predictive
distribution. The L-criterion measures model performance with a
combination of how close its predictions are to the observed data
and variability of the predictions. Better models have smaller
values of <code>L</code>. <code>L</code> is measured in the same units as the
response variable, and measures how close the data vector
<code class="reqn">\textbf{y}</code> is to the predictive distribution. In addition
to the value of <code>L</code>, there is a value for <code>S.L</code>, which is
the calibration number of <code>L</code>, and is useful in determining how
much of a decrease is necessary between models to be noteworthy.</p>
</td></tr>
<tr><td><code>Summary</code></td>
<td>
<p>When <code class="reqn">\textbf{y}</code> is continuous, this is a
<code class="reqn">N \times 8</code> matrix, where <code class="reqn">N</code> is the number of
records of <code class="reqn">\textbf{y}</code> and there are 8 columns, as follows:
y, Mean, SD, LB (the 2.5% quantile), Median, UB (the 97.5%
quantile), PQ (the predictive quantile, which is
<code class="reqn">Pr(\textbf{y}^{rep} \ge \textbf{y})</code>), and
Test, which shows the record-level result of a test, if
specified. When <code class="reqn">\textbf{y}</code> is categorical, this matrix has
a number of columns equal to the number of categories of
<code class="reqn">\textbf{y}</code> plus 3, also including <code>y</code>, <code>Lift</code>,
and <code>Discrep</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Statisticat, LLC.</p>


<h3>References</h3>

<p>Ando, T. (2007). &quot;Bayesian Predictive Information Criterion for
the Evaluation of Hierarchical Bayesian and Empirical Bayes Models&quot;.
<em>Biometrika</em>, 94(2), p. 443&ndash;458.
</p>
<p>Gelfand, A. (1996). &quot;Model Determination Using Sampling Based
Methods&quot;. In Gilks, W., Richardson, S., Spiegehalter, D., Chapter 9 in
Markov Chain Monte Carlo in Practice. Chapman and Hall: Boca Raton, FL.
</p>
<p>Gelfand, A. and Ghosh, S. (1998). &quot;Model Choice: A Minimum Posterior
Predictive Loss Approach&quot;. <em>Biometrika</em>, 85, p. 1&ndash;11.
</p>
<p>Gelman, A., Meng, X.L., and Stern H. (1996). &quot;Posterior Predictive
Assessment of Model Fitness via Realized Discrepancies&quot;.
<em>Statistica Sinica</em>, 6, p. 733&ndash;807.
</p>
<p>Laud, P.W. and Ibrahim, J.G. (1995). &quot;Predictive Model
Selection&quot;. <em>Journal of the Royal Statistical Society</em>, B 57,
p. 247&ndash;262.
</p>
<p>Spiegelhalter, D.J., Best, N.G., Carlin, B.P., and van der Linde, A.
(2002). &quot;Bayesian Measures of Model Complexity and Fit (with
Discussion)&quot;. <em>Journal of the Royal Statistical Society</em>, B 64,
p. 583&ndash;639.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+PMC">PMC</a></code>,
<code><a href="#topic+predict.pmc">predict.pmc</a></code>, and
<code><a href="#topic+p.interval">p.interval</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### See the PMC function for an example.</code></pre>

<hr>
<h2 id='summary.vb.ppc'>Posterior Predictive Check Summary</h2><span id='topic+summary.vb.ppc'></span>

<h3>Description</h3>

<p>This may be used to summarize either new, unobserved instances of
<code class="reqn">\textbf{y}</code> (called <code class="reqn">\textbf{y}^{new}</code>) or
replicates of <code class="reqn">\textbf{y}</code> (called
<code class="reqn">\textbf{y}^{rep}</code>). Either <code class="reqn">\textbf{y}^{new}</code> or
<code class="reqn">\textbf{y}^{rep}</code> is summarized, depending on
<code><a href="#topic+predict.vb">predict.vb</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'vb.ppc'
summary(object, Categorical, Rows, Discrep,
     d, Quiet, ...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.vb.ppc_+3A_object">object</code></td>
<td>
<p>An object of class <code>vb.ppc</code> is required.</p>
</td></tr>
<tr><td><code id="summary.vb.ppc_+3A_categorical">Categorical</code></td>
<td>
<p>Logical. If <code>TRUE</code>, then <code>y</code> and
<code>yhat</code> are considered to be categorical (such as y=0 or y=1),
rather than continuous.</p>
</td></tr>
<tr><td><code id="summary.vb.ppc_+3A_rows">Rows</code></td>
<td>
<p>An optional vector of row numbers, for example
<code>c(1:10)</code>. All rows will be estimated, but only these rows will
appear in the summary.</p>
</td></tr>
<tr><td><code id="summary.vb.ppc_+3A_discrep">Discrep</code></td>
<td>
<p>A character string indicating a discrepancy
test. <code>Discrep</code> defaults to <code>NULL</code>. Valid character
strings when <code>y</code> is continuous are: <code>"Chi-Square"</code>,
<code>"Chi-Square2"</code>, <code>"Kurtosis"</code>, <code>"L-criterion"</code>,
<code>"MASE"</code>, <code>"MSE"</code>, <code>"PPL"</code>, <code>"Quadratic Loss"</code>,
<code>"Quadratic Utility"</code>, <code>"RMSE"</code>, <code>"Skewness"</code>,
<code>"max(yhat[i,]) &gt; max(y)"</code>, <code>"mean(yhat[i,]) &gt; mean(y)"</code>,
<code>"mean(yhat[i,] &gt; d)"</code>, <code>"mean(yhat[i,] &gt; mean(y))"</code>,
<code>"min(yhat[i,]) &lt; min(y)"</code>, <code>"round(yhat[i,]) = d"</code>, and
<code>"sd(yhat[i,]) &gt; sd(y)"</code>. Valid character strings when <code>y</code>
is categorical are: <code>"p(yhat[i,] != y[i])"</code>. Kurtosis and
skewness are not discrepancies, but are included here for convenience.</p>
</td></tr>
<tr><td><code id="summary.vb.ppc_+3A_d">d</code></td>
<td>
<p>This is an optional integer to be used with the
<code>Discrep</code> argument above, and it defaults to <code>d=0</code>.</p>
</td></tr>
<tr><td><code id="summary.vb.ppc_+3A_quiet">Quiet</code></td>
<td>
<p>This logical argument defaults to <code>FALSE</code> and will
print results to the console. When <code>TRUE</code>, results are not
printed.</p>
</td></tr>
<tr><td><code id="summary.vb.ppc_+3A_...">...</code></td>
<td>
<p>Additional arguments are unused.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function summarizes an object of class <code>vb.ppc</code>, which
consists of posterior predictive checks on either
<code class="reqn">\textbf{y}^{new}</code> or <code class="reqn">\textbf{y}^{rep}</code>,
depending respectively on whether unobserved instances of
<code class="reqn">\textbf{y}</code> or the model sample of <code class="reqn">\textbf{y}</code> was
used in the <code><a href="#topic+predict.vb">predict.vb</a></code> function. The deviance and
monitored variables are also summarized.
</p>
<p>The purpose of a posterior predictive check is to assess how well (or
poorly) the model fits the data, or to assess discrepancies between
the model and the data. For more information on posterior predictive
checks, see
<a href="https://web.archive.org/web/20150215050702/http://www.bayesian-inference.com/posteriorpredictivechecks">https://web.archive.org/web/20150215050702/http://www.bayesian-inference.com/posteriorpredictivechecks</a>.
</p>
<p>When <code class="reqn">\textbf{y}</code> is continuous and known, this function
estimates the predictive concordance between <code class="reqn">\textbf{y}</code> and
<code class="reqn">\textbf{y}^{rep}</code> as per Gelfand (1996), and the
predictive quantile (PQ), which is for record-level outlier detection
used to calculate Gelfand's predictive concordance.
</p>
<p>When <code class="reqn">\textbf{y}</code> is categorical and known, this function
estimates the record-level lift, which is
<code>p(yhat[i,] = y[i]) / [p(y = j) / n]</code>, or
the number of correctly predicted samples over the rate of that
category of <code class="reqn">\textbf{y}</code> in vector <code class="reqn">\textbf{y}</code>.
</p>
<p>A discrepancy measure is an approach to studying discrepancies between
the model and data (Gelman et al., 1996). Below is a list of
discrepancy measures, followed by a brief introduction to discrepancy
analysis:
</p>

<ul>
<li><p> The <code>"Chi-Square"</code> discrepancy measure is the chi-square
goodness-of-fit test that is recommended by Gelman. For each record
i=1:N, this returns (y[i] - E(y[i]))^2 / var(yhat[i,]).
</p>
</li>
<li><p> The <code>"Chi-Square2"</code> discrepancy measure returns the
following for each record: Pr(chisq.rep[i,] &gt; chisq.obs[i,]), where
chisq.obs[i,] &lt;- (y[i] - E(y[i]))^2 / E(y[i]), and chisq.rep[i,] &lt;-
(yhat[i,] - E(yhat[i,]))^2 / E(yhat[i,]), and the overall
discrepancy is the percent of records that were outside of the 95%
quantile-based probability interval (see <code><a href="#topic+p.interval">p.interval</a></code>).
</p>
</li>
<li><p> The <code>"Kurtosis"</code> discrepancy measure returns the kurtosis
of <code class="reqn">\textbf{y}^{rep}</code> for each record, and the discrepancy
statistic is the mean for all records. This does not measure
discrepancies between the model and data, and is useful for finding
kurtotic replicate distributions.
</p>
</li>
<li><p> The <code>"L-criterion"</code> discrepancy measure of Laud and Ibrahim
(1995) provides the record-level combination of two components (see
below), and the discrepancy statistic is the sum, <code>L</code>, as well as
a calibration number, <code>S.L</code>. For more information on the
L-criterion, see the accompanying vignette entitled &quot;Bayesian
Inference&quot;.
</p>
</li>
<li><p> The <code>"MASE"</code> (Mean Absolute Scaled Error) is a
discrepancy measure for the accuracy of time-series forecasts,
estimated as <code>(|y - yhat|) / mean(abs(diff(y)))</code>. The discrepancy
statistic is the mean of the record-level values.
</p>
</li>
<li><p> The <code>"MSE"</code> (Mean Squared Error) discrepancy measure
provides the MSE for each record across all replicates, and the
discrepancy statistic is the mean of the record-level MSEs. MSE and
quadratic loss are identical.
</p>
</li>
<li><p> The <code>"PPL"</code> (Posterior Predictive Loss) discrepancy
measure of Gelfand and Ghosh (1998) provides the record-level
combination of two components: one involves the predictive variance
and the other includes the accuracy of the means of the predictive
distribution. The <code>d=0</code> argument applies the following weight to
the accuracy component, which is then added to the variance component:
<code class="reqn">d/(d+1)</code>. For <code class="reqn">\textbf{y}^{new}</code>, use <code class="reqn">d=0</code>. For
<code class="reqn">\textbf{y}^{rep}</code> and model comparison, <code class="reqn">d</code> is
commonly set to 1, 10, or 100000. Larger values of <code class="reqn">d</code> put more
stress on fit and downgrade the precision of the estimates.
</p>
</li>
<li><p> The <code>"Quadratic Loss"</code> discrepancy measure provides the
mean quadratic loss for each record across all replicates, and the
discrepancy statistic is the mean of the record-level mean quadratic
losses. Quadratic loss and MSE are identical, and quadratic loss is
the negative of quadratic utility.
</p>
</li>
<li><p> The <code>"Quadratic Utility"</code> discrepancy measure provides
the mean quadratic utility for each record across all replicates, and
the discrepancy statistic is the mean of the record-level mean
quadratic utilities. Quadratic utility is the negative of quadratic
loss.
</p>
</li>
<li><p> The <code>"RMSE"</code> (Root Mean Squared Error) discrepancy
measure provides the RMSE for each record across all replicates, and
the discrepancy statistic is the mean of the record-level RMSEs.
</p>
</li>
<li><p> The <code>"Skewness"</code> discrepancy measure returns the skewness
of <code class="reqn">\textbf{y}^{rep}</code> for each record, and the discrepancy
statistic is the mean for all records. This does not measure
discrepancies between the model and data, and is useful for finding
skewed replicate distributions.
</p>
</li>
<li><p> The <code>"max(yhat[i,]) &gt; max(y)"</code> discrepancy measure
returns a record-level indicator when a record's maximum
<code class="reqn">\textbf{y}^{rep}_i</code> exceeds the maximum of
<code class="reqn">\textbf{y}</code>. The discrepancy statistic is the mean of the
record-level indicators, reporting the proportion of records with
replications that exceed the maximum of <code class="reqn">\textbf{y}</code>.
</p>
</li>
<li><p> The <code>"mean(yhat[i,]) &gt; mean(y)"</code> discrepancy measure
returns a record-level indicator when the mean of a record's
<code class="reqn">\textbf{y}^{rep}_i</code> is greater than the mean of
<code class="reqn">\textbf{y}</code>. The discrepancy statistic is the mean of the
record-level indicators, reporting the proportion of records with
mean replications that exceed the mean of <code class="reqn">\textbf{y}</code>.
</p>
</li>
<li><p> The <code>"mean(yhat[i,] &gt; d)"</code> discrepancy measure returns a
record-level proportion of <code class="reqn">\textbf{y}^{rep}_i</code> that
exceeds a specified value, <code>d</code>. The discrepancy statistic is the
mean of the record-level proportions.
</p>
</li>
<li><p> The <code>"mean(yhat[i,] &gt; mean(y))"</code> discrepancy measure
returns a record-level proportion of
<code class="reqn">\textbf{y}^{rep}_i</code> that exceeds the mean of
<code class="reqn">\textbf{y}</code>. The discrepancy statistic is the mean of the
record-level proportions.
</p>
</li>
<li><p> The <code>"min(yhat[i,]) &lt; min(y)"</code> discrepancy measure
returns a record-level indicator when a record's minimum
<code class="reqn">\textbf{y}^{rep}_i</code> is less than the minimum of
<code class="reqn">\textbf{y}</code>. The discrepancy statistic is the mean of the
record-level indicators, reporting the proportion of records with
replications less than the minimum of <code class="reqn">\textbf{y}</code>.
</p>
</li>
<li><p> The <code>"round(yhat[i,]) = d"</code> discrepancy measure returns a
record-level proportion of <code class="reqn">\textbf{y}^{rep}_i</code> that,
when rounded, is equal to a specified discrete value, <code>d</code>. The
discrepancy statistic is the mean of the record-level proportions.
</p>
</li>
<li><p> The <code>"sd(yhat[i,]) &gt; sd(y)"</code> discrepancy measure returns a
record-level indicator when the standard deviation of replicates is
larger than the standard deviation of all of <code class="reqn">\textbf{y}</code>. The
discrepancy statistic is the mean of the record-level indicators,
reporting the proportion of records with larger standard deviations
than <code class="reqn">\textbf{y}</code>.
</p>
</li>
<li><p> The <code>"p(yhat[i,] != y[i])"</code> discrepancy measure returns
the record-level probability that <code class="reqn">\textbf{y}^{rep}_i</code>
is not equal to <code class="reqn">\textbf{y}</code>. This is valid when
<code class="reqn">\textbf{y}</code> is categorical and <code>yhat</code> is the predicted
category. The probability is the proportion of replicates.
</p>
</li></ul>

<p>After observing a discrepancy statistic, the user attempts to improve
the model by revising the model to account for discrepancies between
data and the current model. This approach to model revision relies on
an analysis of the discrepancy statistic. Given a discrepancy measure
that is based on model fit, such as the L-criterion, the user may
correlate the record-level discrepancy statistics with the dependent
variable, independent variables, and interactions of independent
variables. The discrepancy statistic should not correlate with the
dependent and independent variables. Interaction variables may be
useful for exploring new relationships that are not in the current
model. Alternatively, a decision tree may be applied to the
record-level discrepancy statistics, given the independent variables,
in an effort to find relationships in the data that may be helpful
in the model. Model revision may involve the addition of a finite
mixture component to account for outliers in discrepancy, or
specifying the model with a distribution that is more robust to
outliers. There are too many suggestions to include here, and
discrepancy analysis varies by model.
</p>


<h3>Value</h3>

<p>This function returns a list with the following components:
</p>
<table>
<tr><td><code>BPIC</code></td>
<td>
<p>The Bayesian Predictive Information Criterion (BPIC) was
introduced by Ando (2007). BPIC is a variation of the Deviance
Information Criterion (DIC) that has been modified for predictive
distributions. For more information on DIC (Spiegelhalter
et al., 2002), see the accompanying vignette entitled &quot;Bayesian
Inference&quot;. <code class="reqn">BPIC = Dbar + 2pD</code>. The goal is to minimize BPIC.</p>
</td></tr>
<tr><td><code>Concordance</code></td>
<td>
<p>This is the percentage of the records of y that are
within the 95% quantile-based probability interval (see
<code><a href="#topic+p.interval">p.interval</a></code>) of <code class="reqn">\textbf{y}^{rep}</code>.
Gelfand's suggested goal is to achieve 95% predictive concordance.
Lower percentages indicate too many outliers and a poor fit of the
model to the data, and higher percentages may suggest overfitting.
Concordance occurs only when <code class="reqn">\textbf{y}</code> is continuous.</p>
</td></tr>
<tr><td><code>Mean Lift</code></td>
<td>
<p>This is the mean of the record-level lifts, and
occurs only when <code class="reqn">\textbf{y}</code> is specified as categorical
with <code>Categorical=TRUE</code>.</p>
</td></tr>
<tr><td><code>Discrepancy.Statistic</code></td>
<td>
<p>This is only reported if the
<code>Discrep</code> argument receives a valid discrepancy measure as
listed above. The <code>Discrep</code> applies to each record of
<code class="reqn">\textbf{y}</code>, and the <code>Discrepancy.Statistic</code> reports
the results of the discrepancy measure on the entire data set. For
example, if <code>Discrep="min(yhat[i,]) &lt; min(y)"</code>, then the
overall result is the proportion of records in which the minimum
sample of yhat was less than the overall minimum
<code class="reqn">\textbf{y}</code>. This is <code>Pr(min(yhat[i,]) &lt; min(y) | y,
      Theta)</code>, where <code>Theta</code> is the parameter set.</p>
</td></tr>
<tr><td><code>L-criterion</code></td>
<td>
<p>The L-criterion (Laud and Ibrahim, 1995) was
developed for model and variable selection. It is a sum of two
components: one involves the predictive variance and the other
includes the accuracy of the means of the predictive
distribution. The L-criterion measures model performance with a
combination of how close its predictions are to the observed data
and variability of the predictions. Better models have smaller
values of <code>L</code>. <code>L</code> is measured in the same units as
the response variable, and measures how close the data vector
<code class="reqn">\textbf{y}</code> is to the predictive distribution. In addition
to the value of <code>L</code>, there is a value for <code>S.L</code>, which is
the calibration number of <code>L</code>, and is useful in determining how
much of a decrease is necessary between models to be noteworthy.</p>
</td></tr>
<tr><td><code>Monitor</code></td>
<td>
<p>This is a <code class="reqn">N \times 5</code> matrix, where <code class="reqn">N</code>
is the number of monitored variables and there are 5 columns, as
follows: Mean, SD, LB (the 2.5% quantile), Median, and UB (the
97.5% quantile).</p>
</td></tr>
<tr><td><code>Summary</code></td>
<td>
<p>When <code class="reqn">\textbf{y}</code> is continuous, this is a
<code class="reqn">N \times 8</code> matrix, where <code class="reqn">N</code> is the number of
records of <code class="reqn">\textbf{y}</code> and there are 8 columns, as follows:
y, Mean, SD, LB (the 2.5% quantile), Median, UB (the 97.5%
quantile), PQ (the predictive quantile, which is
<code class="reqn">Pr(\textbf{y}^{rep} \ge \textbf{y})</code>), and
Test, which shows the record-level result of a test, if
specified. When <code class="reqn">\textbf{y}</code> is categorical, this matrix has
a number of columns equal to the number of categories of
<code class="reqn">\textbf{y}</code> plus 3, also including <code>y</code>, <code>Lift</code>,
and <code>Discrep</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Statisticat, LLC.</p>


<h3>References</h3>

<p>Ando, T. (2007). &quot;Bayesian Predictive Information Criterion for
the Evaluation of Hierarchical Bayesian and Empirical Bayes Models&quot;.
<em>Biometrika</em>, 94(2), p. 443&ndash;458.
</p>
<p>Gelfand, A. (1996). &quot;Model Determination Using Sampling Based
Methods&quot;. In Gilks, W., Richardson, S., Spiegehalter, D., Chapter 9 in
Markov Chain Monte Carlo in Practice. Chapman and Hall: Boca Raton, FL.
</p>
<p>Gelfand, A. and Ghosh, S. (1998). &quot;Model Choice: A Minimum Posterior
Predictive Loss Approach&quot;. <em>Biometrika</em>, 85, p. 1&ndash;11.
</p>
<p>Gelman, A., Meng, X.L., and Stern H. (1996). &quot;Posterior Predictive
Assessment of Model Fitness via Realized Discrepancies&quot;.
<em>Statistica Sinica</em>, 6, p. 733&ndash;807.
</p>
<p>Laud, P.W. and Ibrahim, J.G. (1995). &quot;Predictive Model Selection&quot;.
<em>Journal of the Royal Statistical Society</em>, B 57, p. 247&ndash;262.
</p>
<p>Spiegelhalter, D.J., Best, N.G., Carlin, B.P., and van der Linde, A.
(2002). &quot;Bayesian Measures of Model Complexity and Fit (with
Discussion)&quot;. <em>Journal of the Royal Statistical Society</em>, B 64,
p. 583&ndash;639.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.vb">predict.vb</a></code>,
<code><a href="#topic+p.interval">p.interval</a></code>, and
<code><a href="#topic+VariationalBayes">VariationalBayes</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### See the VariationalBayes function for an example.</code></pre>

<hr>
<h2 id='Thin'>Thin</h2><span id='topic+Thin'></span>

<h3>Description</h3>

<p>This function reduces the number of posterior samples by retaining
every <code class="reqn">k</code>th sample.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Thin(x, By=1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Thin_+3A_x">x</code></td>
<td>
<p>This is a vector or matrix of posterior samples to be
thinned.</p>
</td></tr>
<tr><td><code id="Thin_+3A_by">By</code></td>
<td>
<p>This argument specifies that every <code class="reqn">k</code>th posterior
sample will be retained, and <code>By</code> defaults to 1, meaning that
thinning will not occur, because every sample will be retained.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A thinned matrix of posterior samples is a matrix in which only every
<code class="reqn">k</code>th posterior sample (or row) in the original matrix is
retained. The act of thinning posterior samples has been criticized as
throwing away information, which is correct. However, it is common
practice to thin posterior samples, usually associated with MCMC such
as <code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>, for two reasons. First, Each chain
(column vector) in a matrix of posterior samples probably has higher
autocorrelation than desired, which reduces the effective sample size
(see <code><a href="#topic+ESS">ESS</a></code> for more information). Therefore, a thinned
matrix usually contains posterior samples that are closer to
independent than an un-thinned matrix. The other reason for the
popularity of thinning is that it a user may not have the
random-access memory (RAM) to store large, un-thinned matrices of
posterior samples.
</p>
<p><code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code> and <code><a href="#topic+PMC">PMC</a></code> automatically thin
posterior samples, deviance samples, and samples of monitored
variables, according to its own user-specified argument. The
<code>Thin</code> function is made available here, should it be necessary to
thin posterior samples outside of objects of class <code>demonoid</code> or
<code>pmc</code>.
</p>


<h3>Value</h3>

<p>The <code>Thin</code> argument returns a thinned matrix. When <code>x</code> is a
vector, the returned object is a matrix with 1 column.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+ESS">ESS</a></code>,
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>, and
<code><a href="#topic+PMC">PMC</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
x &lt;- matrix(runif(100), 10, 10)
Thin(x, By=2)
</code></pre>

<hr>
<h2 id='Validate'>Holdout Validation</h2><span id='topic+Validate'></span>

<h3>Description</h3>

<p>This function performs holdout validation on an object of class
<code>demonoid</code> or <code>pmc</code>, given both a modeled and validation
data set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Validate(object, Model, Data, plot=FALSE, PDF=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Validate_+3A_object">object</code></td>
<td>
<p>This is an object of class <code>demonoid</code> or
<code>pmc</code>.</p>
</td></tr>
<tr><td><code id="Validate_+3A_model">Model</code></td>
<td>
<p>This is a model specification function for
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code> or <code><a href="#topic+PMC">PMC</a></code>.</p>
</td></tr>
<tr><td><code id="Validate_+3A_data">Data</code></td>
<td>
<p>This is a list that contains two lists of data, as
specified for <code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>. The first component in
the list is the list of modeled data, and the second component in
the list is the list of validation data.</p>
</td></tr>
<tr><td><code id="Validate_+3A_plot">plot</code></td>
<td>
<p>Logical. When <code>plot=TRUE</code>, two plots are
displayed. The upper plot shows the density of the modeled deviance
in black and the density of the validation deviance in red. The
lower plot shows the density of the change in deviance in gray. The
<code>plot</code> argument defaults to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="Validate_+3A_pdf">PDF</code></td>
<td>
<p>Logical. When <code>PDF=TRUE</code> (and <code>plot=TRUE</code>), the
plot is saved as a .pdf file. The <code>PDF</code> argument defaults to
<code>FALSE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>There are numerous ways to validate a model. In this context,
validation means to assess the predictive performance of a model on
out-of-sample data. If reasonable, leave-one-out cross-validation
(LOOCV) via the conditional predictive ordinate (CPO) should be
considered when using <code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code> or
<code><a href="#topic+PMC">PMC</a></code>. For more information on CPO, see the accompanying
vignettes entitled &quot;Bayesian Inference&quot; and &quot;Examples&quot;. CPO is
unavailable when using <code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code> or
<code><a href="#topic+VariationalBayes">VariationalBayes</a></code>.
</p>
<p>For <code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code> or
<code><a href="#topic+VariationalBayes">VariationalBayes</a></code>, it is recommended that the user
perform holdout validation by comparing posterior predictive checks,
comparing the differences in the specified discrepancy measure.
</p>
<p>When LOOCV is unreasonable, popular alternatives include k-fold
cross-validation and holdout validation. Although k-fold
cross-validation is not performed explicitly here, the user may
accomplish it with some effort. Of these methods, holdout validation
includes the most bias, but is the most common in applied use, since
only one model is fitted, rather than <code class="reqn">k-1</code> models in k-fold
cross-validation. The <code>Validate</code> function performs holdout
validation.
</p>
<p>For holdout validation, the observed data is sampled randomly into two
data sets of approximately equal size, or three data sets that
consists of two data sets of approximately equal size and a remainder
data set. Of the two data sets approximately equal in size, one is
called the modeled (or training) data set, and the other is called the
validation (or test) data set. The modeled data set is used when
updating the model. After the model is updated, both data sets are
predicted in the <code>Validate</code> function, given the model. Predictive
loss is estimated for the validation data set, relative to the modeled
data set.
</p>
<p>Predictive loss is associated with overfitting, differences between
the model and validation data set, or model misspecification. Bayesian
inference is reputed to be much more robust to overfitting than
frequentist inference.
</p>
<p>There are many ways to measure predictive loss, and within each
approach, there are usually numerous possible loss functions. The
log-likelihood of the model is a popular approximate utility function,
and consequently, the deviance of the model is a popular loss
function.
</p>
<p>A vector of model-level (rather than record-level) deviance
samples is returned with each object of class <code>demonoid</code> or
<code>pmc</code>. The <code>Validate</code> function obtains this vector for each
data set, and then calculates the Bayesian Predictive Information
Criterion (BPIC), as per Ando (2007). BPIC is a variation of the
Deviance Information Criterion (DIC) that has been modified for
predictive distributions. For more information on DIC (Spiegelhalter
et al., 2002), see the accompanying vignette entitled &quot;Bayesian
Inference&quot;. The goal is to minimize BPIC.
</p>
<p>When DIC is applied after the model, such as with a predictive
distribution, it is positively biased, or too small. The bias is due
to the same data <code class="reqn">\textbf{y}</code> being used both to construct the
posterior distributions and to evaluate pD, the penalty term for model
complexity. For example, for validation data set
<code class="reqn">\textbf{y}_{new}</code>, BPIC is:
</p>
<p style="text-align: center;"><code class="reqn">BPIC = -2\mathrm{log}[p(\textbf{y}_{new}|\textbf{y},\Theta)] +
    2pD</code>
</p>

<p>When <code>plot=TRUE</code>, the distributions of the modeled and validation
deviances are plotted above, and the lower plot is the modeled
deviance subtracted from the validation deviance. When positive, this
distribution of the change in deviance is the loss in predictive
deviance associated with moving from the modeled data set to the
validation data set.
</p>
<p>After using the <code>Validate</code> function, the user is encouraged to
perform posterior predictive checks on each data set via the
<code><a href="#topic+summary.demonoid.ppc">summary.demonoid.ppc</a></code> or <code><a href="#topic+summary.pmc.ppc">summary.pmc.ppc</a></code>
function.
</p>


<h3>Value</h3>

<p>This function returns a list with three components. The first two
components are also lists. Each list consists of <code>y</code>,
<code>yhat</code>, and <code>Deviance</code>. The third component is a matrix that
reports the expected deviance, pD, and BPIC. The object is of class
<code>demonoid.val</code> for <code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>, or <code>pmc.val</code>
when associated with <code><a href="#topic+PMC">PMC</a></code>.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>References</h3>

<p>Ando, T. (2007). &quot;Bayesian Predictive Information Criterion for
the Evaluation of Hierarchical Bayesian and Empirical Bayes Models&quot;.
<em>Biometrika</em>, 94(2), p. 443&ndash;458.
</p>
<p>Spiegelhalter, D.J., Best, N.G., Carlin, B.P., and van der Linde, A.
(2002). &quot;Bayesian Measures of Model Complexity and Fit (with
Discussion)&quot;. <em>Journal of the Royal Statistical Society</em>, B 64,
p. 583&ndash;639.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>,
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>,
<code><a href="#topic+PMC">PMC</a></code>, and
<code><a href="#topic+VariationalBayes">VariationalBayes</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(LaplacesDemon)
#Given an object called Fit of class demonoid, a Model specification,
#and a modeled data set (MyData.M) and validation data set (MyData.V):
#Validate(Fit, Model, Data=list(MyData.M=MyData.M, MyData.V=MyData.V))
</code></pre>

<hr>
<h2 id='VariationalBayes'>Variational Bayes</h2><span id='topic+VariationalBayes'></span>

<h3>Description</h3>

<p>The <code>VariationalBayes</code> function is a numerical approximation
method for deterministically estimating the marginal posterior
distributions, target distributions, in a Bayesian model with
approximated distributions by minimizing the Kullback-Leibler
Divergence (<code><a href="#topic+KLD">KLD</a></code>) between the target and its
approximation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>VariationalBayes(Model, parm, Data, Covar=NULL, Interval=1.0E-6,
     Iterations=1000, Method="Salimans2", Samples=1000, sir=TRUE,
     Stop.Tolerance=1.0E-5, CPUs=1, Type="PSOCK")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="VariationalBayes_+3A_model">Model</code></td>
<td>
<p>This required argument receives the model from a
user-defined function. The user-defined function is where the model
is specified. <code>VariationalBayes</code> passes two arguments to
the model function, <code>parms</code> and <code>Data</code>. For more
information, see the <code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code> function and
&ldquo;LaplacesDemon Tutorial&rdquo; vignette.</p>
</td></tr>
<tr><td><code id="VariationalBayes_+3A_parm">parm</code></td>
<td>
<p>This argument requires a vector of initial values equal in
length to the number of parameters. <code>VariationalBayes</code> will
attempt to optimize these initial values for the parameters, where
the optimized values are the posterior means, for later use with the
<code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code>, <code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>, or
<code><a href="#topic+PMC">PMC</a></code> function. The <code><a href="#topic+GIV">GIV</a></code> function may be
used to randomly generate initial values. Parameters must be
continuous.</p>
</td></tr>
<tr><td><code id="VariationalBayes_+3A_data">Data</code></td>
<td>
<p>This required argument accepts a list of data. The list of
data must include <code>mon.names</code> which contains monitored variable
names, and <code>parm.names</code> which contains parameter
names. <code>VariationalBayes</code> must be able to determine the
sample size of the data, and will look for a scalar sample size
variable <code>n</code> or <code>N</code>. If not found, it will look for
variable <code>y</code> or <code>Y</code>, and attempt to take its number of
rows as sample size. <code>VariationalBayes</code> needs to determine
sample size due to the asymptotic nature of this method. Sample size
should be at least <code class="reqn">\sqrt{J}</code> with <code class="reqn">J</code> exchangeable
parameters.</p>
</td></tr>
<tr><td><code id="VariationalBayes_+3A_covar">Covar</code></td>
<td>
<p>This argument defaults to <code>NULL</code>, but may otherwise
accept a <code class="reqn">K \times K</code> covariance matrix (where <code class="reqn">K</code>
is the number of dimensions or parameters) of the parameters. When
the model is updated for the first time and prior variance or
covariance is unknown, then <code>Covar=NULL</code> should be used. Once
<code>VariationalBayes</code> has finished updating, it may be desired
to continue updating where it left off, in which case the covariance
matrix from the last run can be input into the next run.</p>
</td></tr>
<tr><td><code id="VariationalBayes_+3A_interval">Interval</code></td>
<td>
<p>This argument receives an interval for estimating
approximate gradients. The logarithm of the unnormalized joint
posterior density of the Bayesian model is evaluated at the current
parameter value, and again at the current parameter value plus this
interval.</p>
</td></tr>
<tr><td><code id="VariationalBayes_+3A_iterations">Iterations</code></td>
<td>
<p>This argument accepts an integer that determines the
number of iterations that <code>VariationalBayes</code> will attempt
to maximize the logarithm of the unnormalized joint posterior
density. <code>Iterations</code> defaults to 1000.
<code>VariationalBayes</code> will stop before this number of
iterations if the tolerance is less than or equal to the
<code>Stop.Tolerance</code> criterion. The required amount of computer
memory increases with <code>Iterations</code>. If computer memory is
exceeded, then all will be lost.</p>
</td></tr>
<tr><td><code id="VariationalBayes_+3A_method">Method</code></td>
<td>
<p>This optional argument currently accepts only
<code>Salimans2</code>, which is the second algorithm in Salimans and
Knowles (2013).</p>
</td></tr>
<tr><td><code id="VariationalBayes_+3A_samples">Samples</code></td>
<td>
<p>This argument indicates the number of posterior samples
to be taken with sampling importance resampling via the
<code><a href="#topic+SIR">SIR</a></code> function, which occurs only when
<code>sir=TRUE</code>. Note that the number of samples should increase
with the number and intercorrelations of the parameters.</p>
</td></tr>
<tr><td><code id="VariationalBayes_+3A_sir">sir</code></td>
<td>
<p>This logical argument indicates whether or not Sampling
Importance Resampling (SIR) is conducted via the <code><a href="#topic+SIR">SIR</a></code>
function to draw independent posterior samples. This argument
defaults to <code>TRUE</code>. Even when <code>TRUE</code>, posterior samples
are drawn only when <code>VariationalBayes</code> has
converged. Posterior samples are required for many other functions,
including <code>plot.vb</code> and <code>predict.vb</code>. The only
time that it is advantageous for <code>sir=FALSE</code> is when
<code>VariationalBayes</code> is used to help the initial values for
<code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code>, <code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>, or
<code><a href="#topic+PMC">PMC</a></code>, and it is unnecessary for time to be spent on
sampling. Less time can be spent on sampling by increasing
<code>CPUs</code>, which parallelizes the sampling.</p>
</td></tr>
<tr><td><code id="VariationalBayes_+3A_stop.tolerance">Stop.Tolerance</code></td>
<td>
<p>This argument accepts any positive number and
defaults to 1.0E-3. Tolerance is calculated each iteration, and the
criteria varies by algorithm. The algorithm is considered to have
converged to the user-specified <code>Stop.Tolerance</code> when the
tolerance is less than or equal to the value of
<code>Stop.Tolerance</code>, and the algorithm terminates at the end of
the current iteration. Often, multiple criteria are used, in
which case the maximum of all criteria becomes the tolerance. For
example, when partial derivatives are taken, it is commonly required
that the Euclidean norm of the partial derivatives is a criterion,
and another common criterion is the Euclidean norm of the
differences between the current and previous parameter
values. Several algorithms have other, specific tolerances.</p>
</td></tr>
<tr><td><code id="VariationalBayes_+3A_cpus">CPUs</code></td>
<td>
<p>This argument accepts an integer that specifies the number
of central processing units (CPUs) of the multicore computer or
computer cluster. This argument defaults to <code>CPUs=1</code>, in which
parallel processing does not occur. Parallelization occurs only for
sampling with <code><a href="#topic+SIR">SIR</a></code> when <code>sir=TRUE</code>.</p>
</td></tr>
<tr><td><code id="VariationalBayes_+3A_type">Type</code></td>
<td>
<p>This argument specifies the type of parallel processing to
perform, accepting either <code>Type="PSOCK"</code> or
<code>Type="MPI"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Variational Bayes (VB) is a family of numerical approximation
algorithms that is a subset of variational inference algorithms, or
variational methods. Some examples of variational methods include the
mean-field approximation, loopy belief propagation, tree-reweighted
belief propagation, and expectation propagation (EP).
</p>
<p>Variational inference for probabilistic models was introduced in the
field of machine learning, influenced by statistical physics
literature (Saul et al., 1996; Saul and Jordan, 1996; Jaakkola, 1997).
The mean-field methods in Neal and Hinton (1999) led to variational
algorithms.
</p>
<p>Variational inference algorithms were later generalized for conjugate
exponential-family models (Attias, 1999, 2000; Wiegerinck, 2000;
Ghahramani and Beal, 2001; Xing et al., 2003). These algorithms still
require different designs for different model forms. Salimans and
Knowles (2013) introduced general-purpose VB algorithms for Gaussian
posteriors.
</p>
<p>A VB algorithm deterministically estimates the marginal posterior
distributions (target distributions) in a Bayesian model with
approximated distributions by minimizing the Kullback-Leibler
Divergence (<code><a href="#topic+KLD">KLD</a></code>) between the target and its
approximation. The complicated posterior distribution is approximated
with a simpler distribution. The simpler, approximated distribution is
called the variational approximation, or approximation distribution,
of the posterior. The term variational is derived from the calculus of
variations, and regards optimization algorithms that select the best
function (which is a distribution in VB), rather than merely selecting
the best parameters.
</p>
<p>VB algorithms often use Gaussian distributions as approximating
distributions. In this case, both the mean and variance of the
parameters are estimated.
</p>
<p>Usually, a VB algorithm is slower to convergence than a Laplace
Approximation algorithm, and faster to convergence than a Monte Carlo
algorithm such as Markov chain Monte Carlo (MCMC). VB often provides
solutions with comparable accuracy to MCMC in less time. Though Monte
Carlo algorithms provide a numerical approximation to the exact
posterior using a set of samples, VB provides a locally-optimal,
exact analytical solution to an approximation of the posterior. VB is
often more applicable than MCMC to big data or large-dimensional
models.
</p>
<p>Since VB is deterministic, it is asymptotic and subject to the same
limitations with respect to sample size as Laplace Approximation.
However, VB estimates more parameters than Laplace Approximation,
such as when Laplace Approximation optimizes the posterior mode of a
Gaussian distribution, while VB optimizes both the Gaussian mean and
variance.
</p>
<p>Traditionally, VB algorithms required customized equations. The
<code>VariationalBayes</code> function uses general-purpose algorithms. A
general-purpose VB algorithm is less efficient than an algorithm
custom designed for the model form. However, a general-purpose
algorithm is applied consistently and easily to numerous model forms.
</p>
<p>When <code>Method="Salimans2"</code>, the second algorithm of Salimans and
Knowles (2013) is used. This requires the gradient and Hessian, which
is more efficient with a small number of parameters as long as the
posterior is twice differentiable. The step size is constant. This
algorithm is suitable for marginal posterior distributions that are
Gaussian and unimodal. A stochastic approximation algorithm is used
in the context of fixed-form VB, inspired by considering fixed-form VB
to be equivalent to performing a linear regression with the sufficient
statistics of the approximation as independent variables and the
unnormalized logarithm of the joint posterior density as the dependent
variable. The number of requested iterations should be large, since the
step-size decreases for larger requested iterations, and a small
step-size will eventually converge. A large number of requested
iterations results in a smaller step-size and better convergence
properties, so hope for early convergence. However convergence is
checked only in the last half of the iterations after the algorithm
begins to average the mean and variance from the samples of the
stochastic approximation. The history of stochastic samples is
returned.
</p>


<h3>Value</h3>

<p><code>VariationalBayes</code> returns an object of class <code>vb</code>
that is a list with the following components:
</p>
<table>
<tr><td><code>Call</code></td>
<td>
<p>This is the matched call of <code>VariationalBayes</code>.</p>
</td></tr>
<tr><td><code>Converged</code></td>
<td>
<p>This is a logical indicator of whether or not
<code>VariationalBayes</code> converged within the specified
<code>Iterations</code> according to the supplied <code>Stop.Tolerance</code>
criterion. Convergence does not indicate that the global maximum has
been found, but only that the tolerance was less than or equal to
the <code>Stop.Tolerance</code> criterion.</p>
</td></tr>
<tr><td><code>Covar</code></td>
<td>
<p>This is the estimated covariance matrix. The
<code>Covar</code> matrix may be scaled and input into the <code>Covar</code>
argument of the <code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code> or <code><a href="#topic+PMC">PMC</a></code>
function for further estimation, or the diagonal of this matrix may
be used to represent the posterior variance of the parameters,
provided the algorithm converged and matrix inversion was
successful. To scale this matrix for use with Laplace's Demon or
PMC, multiply it by <code class="reqn">2.38^2/d</code>, where <code class="reqn">d</code> is the number
of initial values.</p>
</td></tr>
<tr><td><code>Deviance</code></td>
<td>
<p>This is a vector of the iterative history of the
deviance in the <code>VariationalBayes</code> function, as it sought
convergence.</p>
</td></tr>
<tr><td><code>History</code></td>
<td>
<p>This is an array of the iterative history of the
parameters in the <code>VariationalBayes</code> function, as it sought
convergence. The first matrix is for means and the second matrix is
for variances.</p>
</td></tr>
<tr><td><code>Initial.Values</code></td>
<td>
<p>This is the vector of initial values that was
originally given to <code>VariationalBayes</code> in the <code>parm</code>
argument.</p>
</td></tr>
<tr><td><code>LML</code></td>
<td>
<p>This is an approximation of the logarithm of the marginal
likelihood of the data (see the <code><a href="#topic+LML">LML</a></code> function for more
information). When the model has converged and <code>sir=TRUE</code>, the
NSIS method is used. When the model has converged and
<code>sir=FALSE</code>, the LME method is used. This is the
logarithmic form of equation 4 in Lewis and Raftery (1997). As a
rough estimate of Kass and Raftery (1995), the LME-based LML is
worrisome when the sample size of the data is less than five times
the number of parameters, and <code>LML</code> should be adequate in most
problems when the sample size of the data exceeds twenty times the
number of parameters (p. 778). The LME is inappropriate with
hierarchical models. However <code>LML</code> is estimated, it is useful
for comparing multiple models with the <code>BayesFactor</code> function.</p>
</td></tr>
<tr><td><code>LP.Final</code></td>
<td>
<p>This reports the final scalar value for the logarithm
of the unnormalized joint posterior density.</p>
</td></tr>
<tr><td><code>LP.Initial</code></td>
<td>
<p>This reports the initial scalar value for the
logarithm of the unnormalized joint posterior density.</p>
</td></tr>
<tr><td><code>Minutes</code></td>
<td>
<p>This is the number of minutes that
<code>VariationalBayes</code> was running, and this includes the
initial checks as well as drawing posterior samples and creating
summaries.</p>
</td></tr>
<tr><td><code>Monitor</code></td>
<td>
<p>When <code>sir=TRUE</code>, a number of independent
posterior samples equal to <code>Samples</code> is taken, and the draws
are stored here as a matrix. The rows of the matrix are the samples,
and the columns are the monitored variables.</p>
</td></tr>
<tr><td><code>Posterior</code></td>
<td>
<p>When <code>sir=TRUE</code>, a number of independent
posterior samples equal to <code>Samples</code> is taken, and the draws
are stored here as a matrix. The rows of the matrix are the samples,
and the columns are the parameters.</p>
</td></tr>
<tr><td><code>Step.Size.Final</code></td>
<td>
<p>This is the final, scalar <code>Step.Size</code>
value at the end of the <code>VariationalBayes</code> algorithm.</p>
</td></tr>
<tr><td><code>Step.Size.Initial</code></td>
<td>
<p>This is the initial, scalar <code>Step.Size</code>.</p>
</td></tr>
<tr><td><code>Summary1</code></td>
<td>
<p>This is a summary matrix that summarizes the
point-estimated posterior means and variances. Uncertainty around
the posterior means is estimated from the estimated covariance
matrix. Rows are parameters. The following columns are included:
Mean, SD (Standard Deviation), LB (Lower Bound), and UB (Upper
Bound). The bounds constitute a 95% probability interval.</p>
</td></tr>
<tr><td><code>Summary2</code></td>
<td>
<p>This is a summary matrix that summarizes the
posterior samples drawn with sampling importance resampling
(<code><a href="#topic+SIR">SIR</a></code>) when <code>sir=TRUE</code>, given the point-estimated
posterior means and covariance matrix. Rows are parameters. The
following columns are included: Mean, SD (Standard Deviation),
LB (Lower Bound), and UB (Upper Bound). The bounds constitute a 95%
probability interval.</p>
</td></tr>
<tr><td><code>Tolerance.Final</code></td>
<td>
<p>This is the last <code>Tolerance</code> of the
<code>VariationalBayes</code> algorithm.</p>
</td></tr>
<tr><td><code>Tolerance.Stop</code></td>
<td>
<p>This is the <code>Stop.Tolerance</code> criterion.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Statisticat, LLC <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>References</h3>

<p>Attias, H. (1999). &quot;Inferring Parameters and Structure of Latent
Variable Models by Variational Bayes&quot;. In <em>Uncertainty in
Artificial Intelligence</em>.
</p>
<p>Attias, H. (2000). &quot;A Variational Bayesian Framework for Graphical
Models&quot;. In <em>Neural Information Processing Systems</em>.
</p>
<p>Ghahramani, Z. and Beal, M. (2001). &quot;Propagation Algorithms for
Variational Bayesian Learning&quot;. In <em>Neural Information Processing
Systems</em>, p. 507&ndash;513.
</p>
<p>Jaakkola, T. (1997). &quot;Variational Methods for Inference and Estimation
in Graphical Models&quot;. PhD thesis, Massachusetts Institute of
Technology.
</p>
<p>Salimans, T. and Knowles, D.A. (2013). &quot;Fixed-Form Variational
Posterior Approximation through Stochastic Linear Regression&quot;.
<em>Bayesian Analysis</em>, 8(4), p. 837&ndash;882.
</p>
<p>Neal, R. and Hinton, G. (1999). &quot;A View of the EM Algorithm that
Justifies Incremental, Sparse, and Other Variants&quot;. In Learning in
Graphical Models, p. 355&ndash;368. MIT Press, 1999.
</p>
<p>Saul, L. and Jordan, M. (1996). &quot;Exploiting Tractable Substructures in
Intractable Networks&quot;. <em>Neural Information Processing Systems</em>.
</p>
<p>Saul, L., Jaakkola, T., and Jordan, M. (1996). &quot;Mean Field Theory for
Sigmoid Belief Networks&quot;. <em>Journal of Artificial Intelligence
Research</em>, 4, p. 61&ndash;76.
</p>
<p>Wiegerinck, W. (2000). &quot;Variational Approximations Between Mean Field
Theory and the Junction Tree Algorithm&quot;. In <em>Uncertainty in
Artificial Intelligence</em>.
</p>
<p>Xing, E., Jordan, M., and Russell, S. (2003). &quot;A Generalized Mean
Field Algorithm for Variational Inference in Exponential Families&quot;. In
<em>Uncertainty in Artificial Intelligence</em>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+BayesFactor">BayesFactor</a></code>,
<code><a href="#topic+IterativeQuadrature">IterativeQuadrature</a></code>,
<code><a href="#topic+LaplaceApproximation">LaplaceApproximation</a></code>,
<code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>,
<code><a href="#topic+GIV">GIV</a></code>,
<code><a href="#topic+LML">LML</a></code>,
<code><a href="#topic+PMC">PMC</a></code>, and
<code><a href="#topic+SIR">SIR</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># The accompanying Examples vignette is a compendium of examples.
####################  Load the LaplacesDemon Library  #####################
library(LaplacesDemon)

##############################  Demon Data  ###############################
data(demonsnacks)
y &lt;- log(demonsnacks$Calories)
X &lt;- cbind(1, as.matrix(log(demonsnacks[,10]+1)))
J &lt;- ncol(X)
for (j in 2:J) X[,j] &lt;- CenterScale(X[,j])

#########################  Data List Preparation  #########################
mon.names &lt;- "mu[1]"
parm.names &lt;- as.parm.names(list(beta=rep(0,J), sigma=0))
pos.beta &lt;- grep("beta", parm.names)
pos.sigma &lt;- grep("sigma", parm.names)
PGF &lt;- function(Data) {
     beta &lt;- rnorm(Data$J)
     sigma &lt;- runif(1)
     return(c(beta, sigma))
     }
MyData &lt;- list(J=J, PGF=PGF, X=X, mon.names=mon.names,
     parm.names=parm.names, pos.beta=pos.beta, pos.sigma=pos.sigma, y=y)

##########################  Model Specification  ##########################
Model &lt;- function(parm, Data)
     {
     ### Parameters
     beta &lt;- parm[Data$pos.beta]
     sigma &lt;- interval(parm[Data$pos.sigma], 1e-100, Inf)
     parm[Data$pos.sigma] &lt;- sigma
     ### Log-Prior
     beta.prior &lt;- sum(dnormv(beta, 0, 1000, log=TRUE))
     sigma.prior &lt;- dhalfcauchy(sigma, 25, log=TRUE)
     ### Log-Likelihood
     mu &lt;- tcrossprod(Data$X, t(beta))
     LL &lt;- sum(dnorm(Data$y, mu, sigma, log=TRUE))
     ### Log-Posterior
     LP &lt;- LL + beta.prior + sigma.prior
     Modelout &lt;- list(LP=LP, Dev=-2*LL, Monitor=mu[1],
          yhat=rnorm(length(mu), mu, sigma), parm=parm)
     return(Modelout)
     }

############################  Initial Values  #############################
#Initial.Values &lt;- GIV(Model, MyData, PGF=TRUE)
Initial.Values &lt;- rep(0,J+1)

#Fit &lt;- VariationalBayes(Model, Initial.Values, Data=MyData, Covar=NULL,
#     Iterations=1000, Method="Salimans2", Stop.Tolerance=1e-3, CPUs=1)
#Fit
#print(Fit)
#PosteriorChecks(Fit)
#caterpillar.plot(Fit, Parms="beta")
#plot(Fit, MyData, PDF=FALSE)
#Pred &lt;- predict(Fit, Model, MyData, CPUs=1)
#summary(Pred, Discrep="Chi-Square")
#plot(Pred, Style="Covariates", Data=MyData)
#plot(Pred, Style="Density", Rows=1:9)
#plot(Pred, Style="Fitted")
#plot(Pred, Style="Jarque-Bera")
#plot(Pred, Style="Predictive Quantiles")
#plot(Pred, Style="Residual Density")
#plot(Pred, Style="Residuals")
#Levene.Test(Pred)
#Importance(Fit, Model, MyData, Discrep="Chi-Square")

#Fit$Covar is scaled (2.38^2/d) and submitted to LaplacesDemon as Covar.
#Fit$Summary[,1] is submitted to LaplacesDemon as Initial.Values.
#End
</code></pre>

<hr>
<h2 id='WAIC'>Widely Applicable Information Criterion</h2><span id='topic+WAIC'></span>

<h3>Description</h3>

<p>This function calculates the Widely Applicable Information Criterion
(WAIC), also known as the Widely Available Information Criterion or
the Watanable-Akaike, of Watanabe (2010).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>WAIC(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="WAIC_+3A_x">x</code></td>
<td>
<p>This required argument accepts a <code class="reqn">N \times S</code>
matrix of log-likelihood (LL) for <code class="reqn">N</code> records and <code class="reqn">S</code>
samples.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>WAIC is an extension of the Akaike Information Criterion (AIC) that is
more fully Bayesian than the Deviance Information Criterion (DIC).
</p>
<p>Like DIC, WAIC estimates the effective number of parameters to adjust
for overfitting. Two adjustments have been proposed. pWAIC1 is similar
to pD in the original DIC. In contrast, pWAIC2 is calculated with
variance more similarly to pV, which Gelman proposed for DIC. Gelman
et al. (2014, p.174) recommends pWAIC2 because its results are closer
in practice to the results of leave-one-out cross-validation
(LOO-CV). pWAIC is considered an approximation to the number of
unconstrained and uninformed parameters, where a parameter counts as 1
when estimated without contraint or any prior information, 0 if fully
constrained or all information comes from the prior distribution, or
an intermediate number if both the data and prior are informative,
which is usually the case.
</p>
<p>Gelman et al. (2014, p. 174) scale the WAIC of Watanabe (2010) by a
factor of 2 so that it is comparable to AIC and DIC. WAIC is then
reported as <code class="reqn">-2(lppd - pWAIC)</code>. Gelman et al. (2014) prefer WAIC
to AIC or DIC when feasible, which is less often than AIC or DIC.
The <code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code> function requires the model
specification function to return the model-level deviance, which is
<code class="reqn">-2(LL)</code>, where <code class="reqn">LL</code> is the sum of the record-level
log-likelihood. Therefore, if the user desires to calculate WAIC, then
the record-level log-likelihood must be monitored.
</p>


<h3>Value</h3>

<p>The <code>WAIC</code> argument returns a list with four components:
</p>
<table>
<tr><td><code>WAIC</code></td>
<td>
<p>This is the Widely Applicable Information Criterion
(WAIC), which is <code class="reqn">-2(lppd - pWAIC)</code>.</p>
</td></tr>
<tr><td><code>lppd</code></td>
<td>
<p>This is the log pointwise predictive density. For more
information, see Gelman et al. (2014, p. 168).</p>
</td></tr>
<tr><td><code>pWAIC</code></td>
<td>
<p>This is the effective number of parameters preferred by
Gelman et al. (2014).</p>
</td></tr>
<tr><td><code>pWAIC1</code></td>
<td>
<p>This is the effective number of parameters, is
calculated with an alternate method, and is included here for
completeness. It is not used to calculate WAIC in the <code>WAIC</code>
function.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>References</h3>

<p>Gelman, A., Carlin, J.B., Stern, H.S., Dunson, D.B., Vehtari, A., and
Rubin, D.B. (2014). &quot;Bayesian Data Analysis, 3rd ed.&quot;. CRC Press:
Boca Raton, FL.
</p>
<p>Watanabe, S. (2010). &quot;Asymptotic Equivalence of Bayes Cross Validation
and Widely Applicable Information Criterion in Singular Learning
Theory&quot;. <em>Journal of Machine Learning Research</em>, 11,
p. 3571&ndash;3594.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+LaplacesDemon">LaplacesDemon</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#library(LaplacesDemon)
#N &lt;- 10 #Number of records
#S &lt;- 1000 #Number of samples
#LL &lt;- t(rmvn(S, -70+rnorm(N),
#     as.positive.definite(matrix(rnorm(N*N),N,N))))
#WAIC(LL)
### Compare with DIC:
#Dev &lt;- -2*colSums(LL)
#DIC &lt;- list(DIC=mean(Dev) + var(Dev)/2, Dbar=mean(Dev), pV=var(Dev)/2)
#DIC
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
