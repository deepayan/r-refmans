<!DOCTYPE html><html lang="en"><head><title>Help for package sparktf</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {sparktf}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#spark_read_tfrecord'><p>Read a TFRecord File</p></a></li>
<li><a href='#spark_write_tfrecord'><p>Write a Spark DataFrame to a TFRecord file</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Interface for 'TensorFlow' 'TFRecord' Files with 'Apache Spark'</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.0</td>
</tr>
<tr>
<td>Description:</td>
<td>A 'sparklyr' extension that enables reading and writing 'TensorFlow'
  TFRecord files via 'Apache Spark'.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.apache.org/licenses/LICENSE-2.0">Apache License (&ge; 2.0)</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>TensorFlow (https://www.tensorflow.org/)</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.1.2)</td>
</tr>
<tr>
<td>Imports:</td>
<td>sparklyr (&ge; 1.0)</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>6.1.0</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat, dplyr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2019-02-26 22:12:47 UTC; kevinykuo</td>
</tr>
<tr>
<td>Author:</td>
<td>Kevin Kuo <a href="https://orcid.org/0000-0001-7803-7901"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut,
    cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Kevin Kuo &lt;kevin.kuo@rstudio.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2019-03-05 14:30:03 UTC</td>
</tr>
</table>
<hr>
<h2 id='spark_read_tfrecord'>Read a TFRecord File</h2><span id='topic+spark_read_tfrecord'></span>

<h3>Description</h3>

<p>Read a TFRecord file as a Spark DataFrame.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_read_tfrecord(sc, name = NULL, path = name, schema = NULL,
  record_type = c("Example", "SequenceExample"), overwrite = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_read_tfrecord_+3A_sc">sc</code></td>
<td>
<p>A spark conneciton.</p>
</td></tr>
<tr><td><code id="spark_read_tfrecord_+3A_name">name</code></td>
<td>
<p>The name to assign to the newly generated table or the path to the file. Note that if a path is provided for the 'name' argument
then one cannot specify a name.</p>
</td></tr>
<tr><td><code id="spark_read_tfrecord_+3A_path">path</code></td>
<td>
<p>The path to the file. Needs to be accessible from the cluster. Supports the &quot;hdfs://&quot;, &quot;s3a://&quot; and &quot;file://&quot; protocols.</p>
</td></tr>
<tr><td><code id="spark_read_tfrecord_+3A_schema">schema</code></td>
<td>
<p>(Currently unsupported.) Schema of TensorFlow records.  If not provided, the schema is inferred from TensorFlow records.</p>
</td></tr>
<tr><td><code id="spark_read_tfrecord_+3A_record_type">record_type</code></td>
<td>
<p>Input format of TensorFlow records. By default it is Example.</p>
</td></tr>
<tr><td><code id="spark_read_tfrecord_+3A_overwrite">overwrite</code></td>
<td>
<p>Boolean; overwrite the table with the given name if it already exists?</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
iris_tbl &lt;- copy_to(sc, iris)
data_path &lt;- file.path(tempdir(), "iris")
df1 &lt;- iris_tbl %&gt;%
ft_string_indexer_model(
  "Species", "label",
  labels = c("setosa", "versicolor", "virginica")
)

df1 %&gt;%
spark_write_tfrecord(
  path = data_path,
  write_locality = "local"
)

spark_read_tfrecord(sc, data_path)

## End(Not run)

</code></pre>

<hr>
<h2 id='spark_write_tfrecord'>Write a Spark DataFrame to a TFRecord file</h2><span id='topic+spark_write_tfrecord'></span>

<h3>Description</h3>

<p>Serialize a Spark DataFrame to the TensorFlow TFRecord format for
training or inference.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_write_tfrecord(x, path, record_type = c("Example",
  "SequenceExample"), write_locality = c("distributed", "local"),
  mode = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_write_tfrecord_+3A_x">x</code></td>
<td>
<p>A Spark DataFrame</p>
</td></tr>
<tr><td><code id="spark_write_tfrecord_+3A_path">path</code></td>
<td>
<p>The path to the file. Needs to be accessible from the cluster.
Supports the &quot;hdfs://&quot;, &quot;s3a://&quot;, and &quot;file://&quot; protocols.</p>
</td></tr>
<tr><td><code id="spark_write_tfrecord_+3A_record_type">record_type</code></td>
<td>
<p>Output format of TensorFlow records. One of <code>"Example"</code> and 
<code>"SequenceExample"</code>.</p>
</td></tr>
<tr><td><code id="spark_write_tfrecord_+3A_write_locality">write_locality</code></td>
<td>
<p>Determines whether the TensorFlow records are
written locally on the workers or on a distributed file system. One of
<code>"distributed"</code> and <code>"local"</code>. See Details for more information.</p>
</td></tr>
<tr><td><code id="spark_write_tfrecord_+3A_mode">mode</code></td>
<td>
<p>A <code>character</code> element. Specifies the behavior when data or
table already exists. Supported values include: 'error', 'append', 'overwrite' and
'ignore'. Notice that 'overwrite' will also change the column structure.
</p>
<p>For more details see also <a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes">http://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes</a>
for your version of Spark.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For <code>write_locality = local</code>, each of the workers stores on the
local disk a subset of the data. The subset that is stored on each worker
is determined by the partitioning of the DataFrame. Each of the partitions
is coalesced into a single TFRecord file and written on the node where the
partition lives. This is useful in the context of distributed training, in which
each of the workers gets a subset of the data to work on. When this mode is
activated, the path provided to the writer is interpreted as a base path
that is created on each of the worker nodes, and that will be populated with data
from the DataFrame.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
iris_tbl &lt;- copy_to(sc, iris)
data_path &lt;- file.path(tempdir(), "iris")
df1 &lt;- iris_tbl %&gt;%
ft_string_indexer_model(
  "Species", "label",
  labels = c("setosa", "versicolor", "virginica")
)

df1 %&gt;%
spark_write_tfrecord(
  path = data_path,
  write_locality = "local"
)

## End(Not run)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
