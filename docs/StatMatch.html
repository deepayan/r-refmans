<!DOCTYPE html><html lang="en"><head><title>Help for package StatMatch</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {StatMatch}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#StatMatch-package'>
<p>Statistical Matching or Data Fusion</p></a></li>
<li><a href='#comb.samples'><p>Statistical Matching of data from complex sample surveys</p></a></li>
<li><a href='#comp.cont'><p>Compares two distributions of the same continuous variable</p></a></li>
<li><a href='#comp.prop'><p>Compares two distributions of the same categorical variable</p></a></li>
<li><a href='#create.fused'><p>Creates a matched (synthetic) dataset</p></a></li>
<li><a href='#create.imputed'><p>Fills-in missing values in the recipient dataset with values observed on the donors units</p></a></li>
<li><a href='#fact2dummy'><p>Transforms a categorical variable in a set of dummy variables</p></a></li>
<li><a href='#Fbounds.pred'>
<p>Estimates Frechet bounds for cells in the contingency table crossing two categorical variables observed in distinct samples referred to the same target population.</p></a></li>
<li><a href='#Fbwidths.by.x'><p>Computes the Frechet bounds of cells in a contingency table by considering all the possible subsets of the common variables.</p></a></li>
<li><a href='#Frechet.bounds.cat'><p>Frechet bounds of cells in a contingency table</p></a></li>
<li><a href='#gower.dist'><p>Computes the Gower's Distance</p></a></li>
<li><a href='#harmonize.x'><p>Harmonizes the marginal (joint) distribution of a set of variables observed independently in two sample surveys referred to the same target population</p></a></li>
<li><a href='#mahalanobis.dist'><p>Computes the Mahalanobis Distance</p></a></li>
<li><a href='#maximum.dist'><p>Computes the Maximum  Distance</p></a></li>
<li><a href='#mixed.mtc'><p>Statistical Matching via Mixed Methods</p></a></li>
<li><a href='#NND.hotdeck'><p>Distance Hot Deck method.</p></a></li>
<li><a href='#pBayes'><p>Pseudo-Bayes estimates of cell probabilities</p></a></li>
<li><a href='#plotBounds'><p>Graphical representation of the uncertainty bounds estimated through the <code>Frechet.bounds.cat</code>  function</p></a></li>
<li><a href='#plotCont'><p>graphical comparison of the estimated distributions for the same continuous variable.</p></a></li>
<li><a href='#plotTab'><p>Graphical comparison of the estimated distributions for the same categorical variable.</p></a></li>
<li><a href='#pw.assoc'><p>Pairwise measures between categorical variables</p></a></li>
<li><a href='#RANDwNND.hotdeck'><p>Random Distance hot deck.</p></a></li>
<li><a href='#rankNND.hotdeck'><p> Rank distance hot deck method.</p></a></li>
<li><a href='#rho.bounds'><p>Estimates plausible values of the Pearson's correlation coefficient between two variables observed in distinct samples referred to the same target population.</p></a></li>
<li><a href='#rho.bounds.pred'><p>Estimates plausible values of the Pearson's correlation coefficient between two variables observed in distinct samples referred to the same target population.</p></a></li>
<li><a href='#samp.A'><p>Artificial data set resembling EU&ndash;SILC survey</p></a></li>
<li><a href='#samp.B'><p>Artificial data set resembling EU&ndash;SILC survey</p></a></li>
<li><a href='#samp.C'><p>Artificial data set resembling EU&ndash;SILC survey</p></a></li>
<li><a href='#selMtc.by.unc'><p>Identifies the best combination if matching variables in reducing uncertainty in estimation the contingency table Y vs. Z.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Version:</td>
<td>1.4.3</td>
</tr>
<tr>
<td>Date:</td>
<td>2025-01-08</td>
</tr>
<tr>
<td>Title:</td>
<td>Statistical Matching or Data Fusion</td>
</tr>
<tr>
<td>Author:</td>
<td>Marcello D'Orazio [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Marcello D'Orazio &lt;mdo.statmatch@gmail.com&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.7.0), proxy, survey, lpSolve, ggplot2, dplyr</td>
</tr>
<tr>
<td>Suggests:</td>
<td>Hmisc, MASS, mipfp, clue, RANN, randomForest, nnet, glmnet,
naivebayes</td>
</tr>
<tr>
<td>Description:</td>
<td>Integration of two data sources referred to the same target population which share a number of variables. Some functions can also be used to impute missing values in data sets through hot deck imputation methods. Methods to perform statistical matching when dealing  with data from complex sample surveys are available too.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/marcellodo/StatMatch">https://github.com/marcellodo/StatMatch</a>,
<a href="https://github.com/marcellodo/StatMatch/tree/master/Tutorials_Vignette_OtherDocs">https://github.com/marcellodo/StatMatch/tree/master/Tutorials_Vignette_OtherDocs</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2025-01-08 21:30:09 UTC; marc&amp;gio</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2025-01-08 22:00:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='StatMatch-package'>
Statistical Matching or Data Fusion
</h2><span id='topic+StatMatch-package'></span><span id='topic+StatMatch'></span>

<h3>Description</h3>

<p>Functions to perform statistical matching (aka data fusion), i.e. the integration of two data sources.  Some functions can also be used to impute missing values in data sets through hotdeck imputation methods.
</p>


<h3>Details</h3>

<p><em>Statistical matching</em> (also known as <em>data fusion</em>) aims to integrate two data sources (typically samples) that refer to the same target population and share a number of variables.  The ultimate aim is to study the relationship between variables that are not jointly observed in a single data source.  The integration can be performed at <em>micro</em> level (a <em>synthetic</em> or <em>fused</em> file is the output) or <em>macro</em> level (the aim is to estimate correlation coefficients, regression coefficients, contingency tables, etc.).  
Non-parametric hotdeck imputation methods (random, rank and nearest neighbour) can be used to derive the synthetic data set.  Alternatively, a mixed (parametric-nonparametric) procedure based on predictive mean matching can be used.  Methods are also available for performing statistical matching when dealing with data from complex sample surveys.  Finally, some functions can be used to explore the uncertainty due to the typical matching framework and can help to decide whether or not to proceed with statistical matching at the micro or macro level.  For major details see D'Orazio et al. (2006), D'Orazio (2015), and material available at <a href="https://github.com/marcellodo/StatMatch/tree/master/Tutorials_Vignette_OtherDocs">https://github.com/marcellodo/StatMatch/tree/master/Tutorials_Vignette_OtherDocs</a>.
</p>


<h3>Author(s)</h3>

<p>Marcello D'Orazio
</p>
<p>Maintainer: Marcello D'Orazio &lt;mdo.statmatch@gmail.com&gt;
</p>


<h3>References</h3>

<p>D'Orazio M. (2015) Integration and imputation of survey data in R: the StatMatch package, <em>Romanian Statistical Review</em>, 2/2015, pp. 57&ndash;68
</p>
<p>D'Orazio M., Di Zio M., Scanu M. (2006) <em>Statistical Matching, Theory and Practice</em>. Wiley, Chichester.
</p>

<hr>
<h2 id='comb.samples'>Statistical Matching of data from complex sample surveys</h2><span id='topic+comb.samples'></span>

<h3>Description</h3>

<p>This function allows you to cross-tabulate two categorical variables, Y and Z, observed separately in two independent surveys (Y is collected in survey A and Z is collected in survey B) relating to the same target population.  The two surveys share a number of common variables <b>X</b>.  If a third survey C is available on the same population and collects both Y and Z, these data are used as a source of additional information.
</p>
<p>The statistical adjustment is done by calibrating the survey weights as suggested in Renssen (1998).
</p>
<p>It is also possible to use the function to derive estimates that a unit falls into one of the categories of the target variable (the estimates are based on Linear probability models and are obtained as a by-product of the Renssen method).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>comb.samples(svy.A, svy.B, svy.C=NULL, y.lab, z.lab, form.x, 
              estimation=NULL, micro=FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="comb.samples_+3A_svy.a">svy.A</code></td>
<td>

<p>A <code>svydesign</code> <span class="rlang"><b>R</b></span> object that stores the data collected in the survey A and all the information concerning the corresponding sampling design.  This object can be created by using the function <code><a href="survey.html#topic+svydesign">svydesign</a></code> in the package <span class="pkg">survey</span>.  All the variables specified in <code>form.x</code> and by <code>y.lab</code> must be available in <code>svy.A</code>.
</p>
</td></tr>
<tr><td><code id="comb.samples_+3A_svy.b">svy.B</code></td>
<td>

<p>A <code>svydesign</code> <span class="rlang"><b>R</b></span> object that stores the data collected in the survey B and all the information concerning the corresponding sampling design.  This object can be created by using the function <code><a href="survey.html#topic+svydesign">svydesign</a></code> in the package <span class="pkg">survey</span>.  All the variables specified in <code>form.x</code> and by <code>z.lab</code> must be available in <code>svy.B</code>.
</p>
</td></tr>
<tr><td><code id="comb.samples_+3A_svy.c">svy.C</code></td>
<td>

<p>A <code>svydesign</code> <span class="rlang"><b>R</b></span> object that stores the data collected in the the survey C and all the information concerning the corresponding sampling design.  This object can be created by using the function <code><a href="survey.html#topic+svydesign">svydesign</a></code> in the package <span class="pkg">survey</span>.
</p>
<p>When <code>svy.C=NULL</code> (default), i.e. no auxiliary information is available, the function returns an estimate of the contingency table of Y vs. Z under the Conditional Independence assumption (CIA) (see Details for major information).
</p>
<p>When <code>svy.C</code> is available, if <code>estimation="incomplete"</code> then it must contain at least <code>y.lab</code> and <code>z.lab</code> variables.  On the contrary, when 
<code>estimation="synthetic"</code> all the variables specified in <code>form.x</code>, <code>y.lab</code> and <code>z.lab</code> must be available in <code>svy.C</code>.
</p>
</td></tr>
<tr><td><code id="comb.samples_+3A_y.lab">y.lab</code></td>
<td>

<p>A string providing the name of the Y variable, available in survey A and in survey C (if available).  The Y variable can be a categorical variable (<code>factor</code> in <span class="rlang"><b>R</b></span>) or a continuous one (in this latter case <code>z.lab</code> should be categorical).
</p>
</td></tr>
<tr><td><code id="comb.samples_+3A_z.lab">z.lab</code></td>
<td>

<p>A string providing the name of the Z variable available in survey B and in survey C (if available).  The Z variable can be a categorical variable (<code>factor</code> in <span class="rlang"><b>R</b></span>) or a continuous one (in this latter case <code>y.lab</code> should be categorical).
</p>
</td></tr>
<tr><td><code id="comb.samples_+3A_form.x">form.x</code></td>
<td>

<p>A <span class="rlang"><b>R</b></span> formula specifying which matching variables (subset of the <b>X</b> variables) are collected in all surveys and how they are to be considered when combining samples.  For example, <code>form.x=~x1+x2</code> means that variables x1 and x2 are to be considered marginally, without taking into account their cross-tabulation; only their marginal distribution is considered.  To skip the intercept, the formula must be written as <code>form.x=~x1+x2-1</code>.
</p>
<p>When dealing with categorical variables, <code>form.x=~x1:x2-1</code> means that the joint distribution of the two variables (table of x1 vs. x2) has to be considered.
</p>
<p>To better understand the use of <code>form.x</code>, see <code><a href="stats.html#topic+model.matrix">model.matrix</a></code> (see also <br />
<code><a href="stats.html#topic+formula">formula</a></code>).
</p>
<p>Due to the weight calibration features, it is preferable to work with categorical <b>X</b> variables.  In some cases, the procedure can be successful when a single continuous variable is considered together with one or more categorical variables, but often it may be necessary to categorise the continuous variable (see Details).
</p>
</td></tr>
<tr><td><code id="comb.samples_+3A_estimation">estimation</code></td>
<td>

<p>A string identifying the method to be used to estimate the table of Y vs. Z when data from survey C are available.  As suggested in Renssen (1998), there are two alternative methods: (i) incomplete two-way stratification (<code>estimation="incomplete"</code> or <code>estimation="ITWS"</code>, the default) and (ii) synthetic two-way stratification (<code>estimation="synthetic"</code> or <br /> <code>estimation="STWS"</code>).  In the first case (<code>estimation="incomplete"</code>), only Y and Z variables need to be available in <code>svy.C</code>.  On the other hand, in the case of <br /> <code>estimation="synthetic"</code>, the C survey must contain all the <b>X</b> variables specified by <code>form.x</code>, as well as the Y and Z variables.  See details for more information.
</p>
</td></tr>
<tr><td><code id="comb.samples_+3A_micro">micro</code></td>
<td>

<p>Logical, when <code>TRUE</code> predictions of Z in A and of Y in B are provided.  In particular when Y and Z are both categorical variables it is provided the estimated probability that a unit falls in each of the  categories of the given variable.  These probabilities are estimated as a by-product of the whole procedure by considering Linear Probability Models, as suggested in Renssen (1998) (see Details)
</p>
</td></tr>
<tr><td><code id="comb.samples_+3A_...">...</code></td>
<td>

<p>Further arguments that may be necessary for calibration.  In particular, the argument <code>calfun</code> allows to specify the calibration function: <br />
(i) <code>calfun="linear"</code> for linear calibration (default); <br />
(ii) <code>calfun="raking"</code> to rake the survey weights; and <br />
(iii) <code>calfun="logit"</code> for logit calibration.  See <code><a href="survey.html#topic+calibrate">calibrate</a></code> for major details.
</p>
<p>Note that when <code>calfun="linear"</code> calibration may return negative weights.  Generally speaking, in sample surveys weights are expected to be greater than or equal to 1, i.e. <code>bounds=c(1, Inf)</code>.
</p>
<p>The number of iterations used in calibration can be modified by using the argument <code>maxit</code> (by default <code>maxit=50</code>).
</p>
<p>See <code><a href="survey.html#topic+calibrate">calibrate</a></code> for further details.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>TThis function estimates the Y vs. Z contingency table by performing a series of calibrations of the survey weights.  In practice, the estimation is carried out on data from survey C, using all the information from surveys A and B. If survey C is not available, the table of Y vs. Z is estimated under the assumption of conditional independence (CIA), i.e. <code class="reqn">p(Y,Z)=p(Y|\bold{X}) \times p(Z|\bold{X}) \times p(\bold{X})</code>.
</p>
<p>When data from survey C are available (Renssen, 1998), the table of Y vs. Z can be estimated by: Incomplete Two-Way Stratification (ITWS) or Synthetic Two-Way Stratification (STWS).  In the first case (ITWS) the weights of the units in survey C are calibrated so that the new weights allow to reproduce the marginal distributions of Y estimated on survey A, and that of Z estimated on survey B.  Note that the distribution of the <b>X</b> variables in survey A and in survey B, must be harmonized before performing ITWS (see <code><a href="#topic+harmonize.x">harmonize.x</a></code>).  
</p>
<p>The Synthetic Two-Way Stratification allows to estimate the table of Y vs. Z by considering also the <b>X</b> variables observed in C.  This method consists in correcting the table of Y vs. Z estimated under the CIA according to the relationship between Y and Z observed in survey C (for major details see Renssen, 1998.
</p>
<p>If the argument <code>micro</code> is set to <code>TRUE</code> the function will also return <code>Z.A</code> and <code>Y.B</code>.  The first data.frame has the same rows as <code>svy.A</code> and the number of columns is equal to the number of categories of the Z variable specified by <code>z.lab</code>.  Each row gives the estimated probabilities of taking a value in each category.  The same happens for <code>Y.B</code>, which gives the estimated probabilities of assuming a category of <code>y.lab</code> for each unit in B. The estimated probabilities are obtained by applying the linear probability models (for more details see Renssen, 1998).  Unfortunately, such models may give estimated probabilities less than 0 or greater than 1. Great care should be taken when using such predictions for practical purposes.
</p>


<h3>Value</h3>

<p>A <span class="rlang"><b>R</b></span> list with the results of the calibration procedure according to the input arguments. 
</p>
<table role = "presentation">
<tr><td><code>yz.CIA</code></td>
<td>

<p>The table of Y (<code>y.lab</code>) vs. Z (<code>z.lab</code>) estimated under the Conditional Independence Assumption (CIA).
</p>
</td></tr>
<tr><td><code>cal.C</code></td>
<td>

<p>The survey object <code>svy.C</code> after the calibration.  Only when <code>svy.C</code> is provided.
</p>
</td></tr>
<tr><td><code>yz.est</code></td>
<td>

<p>The table of Y (<code>y.lab</code>) vs. Z (<code>z.lab</code>) estimated under the method specified via <code>estimation</code> argument.  Only when <code>svy.C</code> is provided.
</p>
</td></tr>
<tr><td><code>Z.A</code></td>
<td>

<p>Only when <code>micro=TRUE</code>.  It is a data frame with the same rows as in <code>svy.A</code> and the number of columns is equal to the number of categories of the variable <code>z.lab</code>. Each row provides the estimated probabilities for a unit being in the various categories of <code>z.lab</code>.
</p>
</td></tr>
<tr><td><code>Y.B</code></td>
<td>

<p>Only when <code>micro=TRUE</code>.  It is a data frame with the same rows as in <code>svy.B</code> and the number of columns is equal to the number of categories of <code>y.lab</code>. Each row provides the estimated probabilities for a unit being in the various categories of <code>y.lab</code>.
</p>
</td></tr>
<tr><td><code>call</code></td>
<td>

<p>Stores the call to this function with all the values specified for the various arguments (<code>call=match.call()</code>).
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Marcello D'Orazio <a href="mailto:mdo.statmatch@gmail.com">mdo.statmatch@gmail.com</a> 
</p>


<h3>References</h3>

<p>D'Orazio, M., Di Zio, M. and Scanu, M. (2006). <em>Statistical Matching: Theory and Practice</em>. Wiley, Chichester.
</p>
<p>Renssen, R.H. (1998) &ldquo;Use of Statistical Matching Techniques in Calibration Estimation&rdquo;. <em>Survey Methodology</em>, <b>24</b>, pp. 171&ndash;183.
</p>


<h3>See Also</h3>

 
<p><code><a href="survey.html#topic+calibrate">calibrate</a></code>, <code><a href="survey.html#topic+svydesign">svydesign</a></code>, <code><a href="#topic+harmonize.x">harmonize.x</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(quine, package="MASS") #loads quine from MASS
str(quine)
quine$c.Days &lt;- cut(quine$Days, c(-1, seq(0,20,10),100))
table(quine$c.Days)


# split quine in two subsets

suppressWarnings(RNGversion("3.5.0"))
set.seed(124)
lab.A &lt;- sample(nrow(quine), 70, replace=TRUE)
quine.A &lt;- quine[lab.A, c("Eth","Sex","Age","Lrn")]
quine.B &lt;- quine[-lab.A, c("Eth","Sex","Age","c.Days")]

# create svydesign objects
require(survey)
quine.A$f &lt;- 70/nrow(quine) # sampling fraction
quine.B$f &lt;- (nrow(quine)-70)/nrow(quine)
svy.qA &lt;- svydesign(~1, fpc=~f, data=quine.A)
svy.qB &lt;- svydesign(~1, fpc=~f, data=quine.B)

# Harmonizazion wrt the joint distribution
# of ('Sex' x 'Age' x 'Eth')

# vector of population total known
# estimated from the full data set
# note the formula!
tot.m &lt;- colSums(model.matrix(~Eth:Sex:Age-1, data=quine))
tot.m

out.hz &lt;- harmonize.x(svy.A=svy.qA, svy.B=svy.qB, x.tot=tot.m,
            form.x=~Eth:Sex:Age-1, cal.method="linear")
            
# estimation of 'Lrn' vs. 'c.Days' under the CIA

svy.qA.h &lt;- out.hz$cal.A
svy.qB.h &lt;- out.hz$cal.B

out.1 &lt;- comb.samples(svy.A=svy.qA.h, svy.B=svy.qB.h,
            svy.C=NULL, y.lab="Lrn", z.lab="c.Days",
            form.x=~Eth:Sex:Age-1)

out.1$yz.CIA
addmargins(out.1$yz.CIA)

#
# incomplete two-way stratification

# select a sample C from quine
# and define a survey object

suppressWarnings(RNGversion("3.5.0"))
set.seed(4321)
lab.C &lt;- sample(nrow(quine), 50, replace=TRUE)
quine.C &lt;- quine[lab.C, c("Lrn","c.Days")]
quine.C$f &lt;- 50/nrow(quine) # sampling fraction
svy.qC &lt;- svydesign(~1, fpc=~f, data=quine.C)

# call comb.samples
out.2 &lt;- comb.samples(svy.A=svy.qA.h, svy.B=svy.qB.h,
            svy.C=svy.qC, y.lab="Lrn", z.lab="c.Days",
            form.x=~Eth:Sex:Age-1, estimation="incomplete",
            calfun="linear", maxit=100)

summary(weights(out.2$cal.C))
out.2$yz.est # estimated table of 'Lrn' vs. 'c.Days'
# difference wrt the table 'Lrn' vs. 'c.Days' under CIA
addmargins(out.2$yz.est)-addmargins(out.2$yz.CIA)

# synthetic two-way stratification
# only macro estimation

quine.C &lt;- quine[lab.C, ]
quine.C$f &lt;- 50/nrow(quine) # sampling fraction
svy.qC &lt;- svydesign(~1, fpc=~f, data=quine.C)

out.3 &lt;- comb.samples(svy.A=svy.qA.h, svy.B=svy.qB.h,
            svy.C=svy.qC, y.lab="Lrn", z.lab="c.Days",
            form.x=~Eth:Sex:Age-1, estimation="synthetic",
            calfun="linear",bounds=c(.5,Inf), maxit=100)

summary(weights(out.3$cal.C))

out.3$yz.est # estimated table of 'Lrn' vs. 'c.Days'
# difference wrt the table of 'Lrn' vs. 'c.Days' under CIA
addmargins(out.3$yz.est)-addmargins(out.3$yz.CIA)
# diff wrt the table of 'Lrn' vs. 'c.Days' under incomplete 2ws
addmargins(out.3$yz.est)-addmargins(out.2$yz.CIA)

# synthetic two-way stratification
# with micro predictions

out.4 &lt;- comb.samples(svy.A=svy.qA.h, svy.B=svy.qB.h,
            svy.C=svy.qC, y.lab="Lrn", z.lab="c.Days",
            form.x=~Eth:Sex:Age-1, estimation="synthetic",
            micro=TRUE, calfun="linear",bounds=c(.5,Inf), 
            maxit=100)
            
head(out.4$Z.A)
head(out.4$Y.B)



</code></pre>

<hr>
<h2 id='comp.cont'>Compares two distributions of the same continuous variable</h2><span id='topic+comp.cont'></span>

<h3>Description</h3>

<p>This function estimate the &ldquo;closeness&rdquo; of the distributions of the same continuous variable(s) but estimated from different data sources.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>comp.cont(data.A, data.B, xlab.A, xlab.B = NULL, w.A = NULL, 
          w.B = NULL, ref = FALSE) 
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="comp.cont_+3A_data.a">data.A</code></td>
<td>

<p>A dataframe or matrix containing the variable of interest <code>xlab.A</code> and eventual associated survey weights <code>w.A</code>. 
</p>
</td></tr>
<tr><td><code id="comp.cont_+3A_data.b">data.B</code></td>
<td>
 
<p>A dataframe or matrix containing the variable of interest <code>xlab.B</code> and eventual associated survey weights <code>w.B</code>. 
</p>
</td></tr>
<tr><td><code id="comp.cont_+3A_xlab.a">xlab.A</code></td>
<td>

<p>Character string providing the name of the variable in <code>data.A</code> whose estimated distribution should be compared with that estimated from <code>data.B</code>.
</p>
</td></tr>
<tr><td><code id="comp.cont_+3A_xlab.b">xlab.B</code></td>
<td>

<p>Character string providing the name of the variable in <code>data.B</code> whose distribution should be  compared with that estimated from <code>data.A</code>. If <code>xlab.B=NULL</code> (default) then it assumed <code>xlab.B=xlab.A</code>.
</p>
</td></tr>
<tr><td><code id="comp.cont_+3A_w.a">w.A</code></td>
<td>

<p>Character string providing the name of the optional weighting variable in <code>data.A</code> that, in case, should be used to estimate the distribution of <code>xlab.A</code>
</p>
</td></tr>
<tr><td><code id="comp.cont_+3A_w.b">w.B</code></td>
<td>

<p>Character string providing the name of the optional weighting variable in <code>data.B</code> that, in case, should be used to estimate the distribution of <code>xlab.B</code>
</p>
</td></tr>
<tr><td><code id="comp.cont_+3A_ref">ref</code></td>
<td>

<p>Logical.  When <code>ref = TRUE</code>, the distribution of <code>xlab.B</code> estimated from <code>data.B</code> is considered the reference distribution (true or reliable estimate of distribution).
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function calculates well&ndash;known summary measures (min, Q1, median, mean, Q3, max and sd) estimated from the available data. It also compares the quantiles estimated from <code>data.A</code> with those estimated from <code>data.B</code> and returns the average of the absolute value of the differences and the average of the squared differences. Note that the number of percentiles estimated depends on the minimum between the two sample sizes. Note that the number of estimated percentiles depends on the minimum between the two sample sizes. Only quartiles are calculated if min(n.A, n.B)&lt;=50; quintiles are estimated if min(n.A, n.B)&gt;50 and min(n.A, n.B)&lt;=150; deciles are estimated if min(n. A, n.B)&gt;150 and min(n.A, n.B)&lt;=250; finally, quantiles for <code>probs=seq(from = 0.05,to = 0.95,by = 0.05)</code> are estimated when min(n.A, n.B)&gt;250. If survey weights are available (indicated by <code>w.A</code> and/or <code>w.B</code>), they are used to estimate the quantiles by calling the function <code><a href="Hmisc.html#topic+wtd.quantile">wtd.quantile</a></code> in the package <span class="pkg">Hmisc</span>.
</p>
<p>The dissimilarities between the estimated empirical distribution functions are calculated. The measures considered are the maximum value of the differences, the sum of the absolute values of the minimum and maximum, and the average of the absolute differences. If weights are given, they are used in the estimation of the empirical cumulative distribution function. Note that when <code>ref=TRUE</code> is given, the estimation of the density and the empirical cumulative distribution will be guided by the data in <code>data.B</code>.
</p>
<p>Finally, the total variation distance, the overlap and the Hellinger are calculated on the transformed categorised variable. Note that the breaks to categorise the variable are decided according to the Freedman-Diaconis rule (<code><a href="grDevices.html#topic+nclass">nclass</a></code>) and, in this case, with <code>ref=TRUE</code> the IQR is estimated on <code>data.B</code> alone, whereas with <code>ref=FALSE</code> it is estimated by combining the two data sources. 
If present, the weights are used to estimate the relative frequencies of the categorised variable. 
<em>total variation distance</em>: 
</p>
<p style="text-align: center;"><code class="reqn">\Delta_{AB} = \frac{1}{2} \sum_{j=1}^J \left| p_{A,j} - p_{B,j}  \right|</code>
</p>

<p>where <code class="reqn">p_{s,j}</code> are the relative frequencies (<code class="reqn">0 \leq p_{s,j} \leq 1</code>). The dissimilarity index ranges from 0 (minimum dissimilarity) to 1. The total variation distance comes along with its complement to 1, said &ldquo;overlap&rdquo; between distributions.
</p>
<p>the <em>Hellinger's distance</em>:
</p>
<p style="text-align: center;"><code class="reqn">d_{H,AB} = \sqrt{  \frac{1}{2} \sum_{j=1}^J \left( \sqrt{p_{A,j}} - \sqrt{p_{B,j}}  \right)^2 } </code>
</p>

<p>It is a dissimilarity measure ranging from 0 (distributions are equal) to 1 (max dissimilarity).  It satisfies all the properties of a distance measure (<code class="reqn">0 \leq d_{H,AB} \leq 1</code>;  symmetry and triangle inequality).  
Hellinger's distance is related to the total variation distance, and it is possible to show that:
</p>
<p style="text-align: center;"><code class="reqn">d_{H,AB}^2 \leq  \Delta_{AB} \leq d_{H,AB}\sqrt{2} </code>
</p>



<h3>Value</h3>

<p>A <code>list</code> object with four components. 
</p>
<table role = "presentation">
<tr><td><code>summary</code></td>
<td>
<p>A matrix with summaries of <code>xlab.A</code> estimated on <code>data.A</code> and summaries of <code>xlab.B</code> estimated on <code>data.B</code></p>
</td></tr>
<tr><td><code>diff.Qs</code></td>
<td>
<p> Average of absolute and squared differences between the quantiles of <code>xlab.A</code> estimated on <code>data.A</code> and the corresponding ones of <code>xlab.B</code> estimated on <code>data.B</code></p>
</td></tr>
<tr><td><code>dist.ecdf</code></td>
<td>
<p>Dissimilarity measures between the estimated empirical cumulative distribution functions.</p>
</td></tr>
<tr><td><code>dist.discr</code></td>
<td>
<p>Distance between the distributions after discretization of the target variable.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Marcello D'Orazio <a href="mailto:mdo.statmatch@gmail.com">mdo.statmatch@gmail.com</a> 
</p>


<h3>References</h3>

<p>Bellhouse D.R. and J. E. Stafford (1999). &ldquo;Density Estimation from Complex Surveys&rdquo;. <em>Statistica Sinica</em>, <b>9</b>, 407&ndash;424.
</p>


<h3>See Also</h3>

 
<p><code><a href="#topic+plotCont">plotCont</a></code>
</p>

<hr>
<h2 id='comp.prop'>Compares two distributions of the same categorical variable</h2><span id='topic+comp.prop'></span>

<h3>Description</h3>

<p>This function compares two (estimated) distributions of the same categorical variable(s).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>comp.prop(p1, p2, n1, n2=NULL, ref=FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="comp.prop_+3A_p1">p1</code></td>
<td>

<p>A vector or an array containing relative or absolute frequencies for one or more categorical variables.  Usually it is the output of the function <code><a href="stats.html#topic+xtabs">xtabs</a></code> or <code><a href="base.html#topic+table">table</a></code>. 
</p>
</td></tr>
<tr><td><code id="comp.prop_+3A_p2">p2</code></td>
<td>
 
<p>A vector or an array containing relative or absolute frequencies for one or more categorical variables.  Usually it is the output of the function <code><a href="stats.html#topic+xtabs">xtabs</a></code> or <code><a href="base.html#topic+table">table</a></code>.  If <code>ref = FALSE</code> then <code>p2</code> is a further estimate of the distribution of the categorical variable(s) being considered.  On the contrary (<code>ref = TRUE</code>) it is the 'reference' distribution (the distribution considered true or a reliable estimate).  
</p>
</td></tr>
<tr><td><code id="comp.prop_+3A_n1">n1</code></td>
<td>

<p>The size of the sample on which <code>p1</code> has been estimated.
</p>
</td></tr>
<tr><td><code id="comp.prop_+3A_n2">n2</code></td>
<td>

<p>The size of the sample on which <code>p2</code> has been estimated, required just when <code>ref = FALSE</code> (<code>p2</code> is estimated on another sample and is not the reference distribution).
</p>
</td></tr>
<tr><td><code id="comp.prop_+3A_ref">ref</code></td>
<td>

<p>Logical.  When <code>ref = TRUE</code>, <code>p2</code> is the reference distribution (true or reliable estimate of distribution), on the contrary when <code>ref = FALSE</code> it an estimate of the distribution derived from another sample with sample size <code>n2</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function computes some similarity or dissimilarity measures between marginal (joint) distribution of categorical variables(s).
The following measures are considered:
</p>
<p><em>Dissimilarity index</em> or <em>total variation distance</em>: 
</p>
<p style="text-align: center;"><code class="reqn">\Delta_{12} = \frac{1}{2} \sum_{j=1}^J \left| p_{1,j} - p_{2,j}  \right|</code>
</p>

<p>where <code class="reqn">p_{s,j}</code> are the relative frequencies (<code class="reqn">0 \leq p_{s,j} \leq 1</code>). The dissimilarity index ranges from 0 (minimum dissimilarity) to 1.  It can be interpreted as the smallest fraction of units that need to be reclassified in order to make the distributions equal.  When <code>p2</code> is the reference distribution (true or expected distribution under a given hypothesis) than, following the Agresti's rule of thumb (Agresti 2002, pp. 329&ndash;330) , values of <code class="reqn">\Delta_{12} \leq 0.03</code> denotes that the estimated distribution <code>p1</code> follows the true or expected pattern quite closely.
</p>
<p><em>Overlap</em> between two distributions:
</p>
<p style="text-align: center;"><code class="reqn">O_{12} = \sum_{j=1}^J min(p_{1,j},p_{2,j})  </code>
</p>

<p>It is a measure of similarity which ranges from 0 to 1 (the distributions are equal).  It is worth noting that <code class="reqn">O_{12}=1-\Delta_{12}</code>.
</p>
<p><em>Bhattacharyya coefficient</em>:
</p>
<p style="text-align: center;"><code class="reqn">B_{12} = \sum_{j=1}^J \sqrt{p_{1,j} \times p_{2,j}}  </code>
</p>

<p>It is a measure of similarity and ranges from 0 to 1 (the distributions are equal).
</p>
<p><em>Hellinger's distance</em>:
</p>
<p style="text-align: center;"><code class="reqn">d_{H,12} = \sqrt{1-B_{12}} </code>
</p>

<p>It is a dissimilarity measure ranging from 0 (distributions are equal) to 1 (max dissimilarity).  It satisfies all the properties of a distance measure (<code class="reqn">0 \leq d_{H,12} \leq 1</code>;  symmetry and triangle inequality).  
Hellinger's distance is related to the dissimilarity index, and it is possible to show that:
</p>
<p style="text-align: center;"><code class="reqn">d_{H,12}^2 \leq  \Delta_{12} \leq d_{H,12}\sqrt{2} </code>
</p>

<p>Alongside with those similarity/dissimilarity measures the Pearson's Chi-squared is computed. Two formulas are considered.  When <code>p2</code> is the reference distribution (true or expected under some hypothesis, <code>ref=TRUE</code>):
</p>
<p style="text-align: center;"><code class="reqn"> \chi^2_P = n_1 \sum_{j=1}^J \frac{\left( p_1,j - p_{2,j}\right)^2}{p_{2,j}}  </code>
</p>

<p>When <code>p2</code> is a distribution estimated on a second sample then:
</p>
<p style="text-align: center;"><code class="reqn"> \chi^2_P = \sum_{i=1}^2 \sum_{j=1}^J n_i \frac{\left( p_{i,j} - p_{+,j}\right)^2}{p_{+,j}}  </code>
</p>

<p>where <code class="reqn">p_{+,j}</code> is the expected frequency for category <em>j</em>, obtained as follows:
</p>
<p style="text-align: center;"><code class="reqn"> p_{+,j} = \frac{n_1 p_{1,j} + n_2 p_{2,j}}{n_1+n_2}  </code>
</p>

<p>being <code class="reqn">n_1</code> and <code class="reqn">n_2</code> the sizes of the samples.
</p>
<p>The Chi-Square value can be used to test the hypothesis that two distributions are equal (<code class="reqn">df=J-1</code>).  Unfortunately such a test would not be useful when the distribution are estimated from samples selected from a finite population using complex selection schemes (stratification, clustering, etc.).  In such a case different alternative corrected Chi-square tests are available (cf. Sarndal et al., 1992, Sec. 13.5). One possibility consist in dividing the Pearson's Chi-square test by the <em>generalised design effect</em> of both the surveys.  Its estimation is not straightforward (sampling design variables need to be available). Generally speacking, the generalised design effect is smaller than 1 in the presence of stratified random sampling designs, while it exceeds 1 the presence of a two stage cluster sampling design.  For the purposes of analysis it is reported the value of the generalised design effect <em>g</em> that would determine the acceptance of the null hypothesis (equality of distributions) in the case of <code class="reqn">\alpha = 0.05</code> (<code class="reqn">df = J-1</code>), i.e. values of <em>g</em> such that 
</p>
<p style="text-align: center;"><code class="reqn">   \frac{\chi^2_P}{g} \leq \chi^2_{J-1,0.05} </code>
</p>



<h3>Value</h3>

<p>A <code>list</code> object with two or three components depending on the argument <code>ref</code>. 
</p>
<table role = "presentation">
<tr><td><code>meas</code></td>
<td>
<p>A vector with the measures of similarity/dissimilarity between the distributions: dissimilarity index (<code>"tvd"</code>), overlap (<code>"overlap"</code>), Bhattacharyya coefficient <br /> 
(<code>"Bhatt"</code>) and Hellinger's distance (<code>"Hell"</code>).</p>
</td></tr>
<tr><td><code>chi.sq</code></td>
<td>
<p> A vector with the following values: Pearson's Chi-square (<code>"Pearson"</code>), the degrees of freedom (<code>"df"</code>), the percentile of a Chi-squared distribution (<code>"q0.05"</code>) and the largest admissible value of the generalised design effect that would determine the acceptance of H0 (equality of distributions).</p>
</td></tr>
<tr><td><code>p.exp</code></td>
<td>
<p> When <code>ref=FALSE</code> it is reported the value of the reference distribution <code class="reqn">p_{+,j}</code> estimated used in deriving the Chi-square statistic and also the dissimilarity index.  On the contrary (<code>ref=FALSE</code>)  it is set equal to the argument <code>p2</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Marcello D'Orazio <a href="mailto:mdo.statmatch@gmail.com">mdo.statmatch@gmail.com</a> 
</p>


<h3>References</h3>

<p>Agresti A (2002) <em>Categorical Data Analysis. Second Edition</em>. Wiley, new York.
</p>
<p>Sarndal CE, Swensson B, Wretman JH (1992) <em>Model Assisted Survey Sampling</em>. Springer&ndash;Verlag, New York.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(quine, package="MASS") #loads quine from MASS
str(quine)

# split quine in two subsets
suppressWarnings(RNGversion("3.5.0"))
set.seed(124)
lab.A &lt;- sample(nrow(quine), 70, replace=TRUE)
quine.A &lt;- quine[lab.A, c("Eth","Sex","Age")]
quine.B &lt;- quine[-lab.A, c("Eth","Sex","Age")]

# compare est. distributions from 2 samples
# 1 variable
tt.A &lt;- xtabs(~Age, data=quine.A)
tt.B &lt;- xtabs(~Age, data=quine.B)
comp.prop(p1=tt.A, p2=tt.B, n1=nrow(quine.A), n2=nrow(quine.B), ref=FALSE)

# joint distr. of more variables
tt.A &lt;- xtabs(~Eth+Sex+Age, data=quine.A)
tt.B &lt;- xtabs(~Eth+Sex+Age, data=quine.B)
comp.prop(p1=tt.A, p2=tt.B, n1=nrow(quine.A), n2=nrow(quine.B), ref=FALSE)

# compare est. distr. with a one considered as reference
tt.A &lt;- xtabs(~Eth+Sex+Age, data=quine.A)
tt.all &lt;- xtabs(~Eth+Sex+Age, data=quine)
comp.prop(p1=tt.A, p2=tt.all, n1=nrow(quine.A), n2=NULL, ref=TRUE)


</code></pre>

<hr>
<h2 id='create.fused'>Creates a matched (synthetic) dataset</h2><span id='topic+create.fused'></span>

<h3>Description</h3>

<p>Creates a <em>synthetic</em> data frame after the statistical matching of two data sources at <em>micro</em> level.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create.fused(data.rec, data.don, mtc.ids, 
                z.vars, dup.x=FALSE, match.vars=NULL)  
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="create.fused_+3A_data.rec">data.rec</code></td>
<td>

<p>A matrix or data frame that plays the role of <em>recipient</em> in the statistical matching application. 
</p>
</td></tr>
<tr><td><code id="create.fused_+3A_data.don">data.don</code></td>
<td>

<p>A matrix or data frame that plays the role of <em>donor</em> in the statistical matching application.
</p>
</td></tr>
<tr><td><code id="create.fused_+3A_mtc.ids">mtc.ids</code></td>
<td>

<p>A matrix with two columns.  Each row must contain the name or the index of the recipient record (row) in <code>data.don</code> and the name or the index of the corresponding donor record (row) in <code>data.don</code>.  Note that this type of matrix is returned by the functions <code><a href="#topic+NND.hotdeck">NND.hotdeck</a></code>, <code><a href="#topic+RANDwNND.hotdeck">RANDwNND.hotdeck</a></code>, <code><a href="#topic+rankNND.hotdeck">rankNND.hotdeck</a></code>, and <code><a href="#topic+mixed.mtc">mixed.mtc</a></code>.
</p>
</td></tr>
<tr><td><code id="create.fused_+3A_z.vars">z.vars</code></td>
<td>

<p>A character vector with the names of the variables available only in <code>data.don</code> that should be &ldquo;donated&rdquo; to <code>data.rec</code>.
</p>
</td></tr>
<tr><td><code id="create.fused_+3A_dup.x">dup.x</code></td>
<td>

<p>Logical. When <code>TRUE</code> the values of the matching variables in <code>data.don</code> are also &ldquo;donated&rdquo; to <code>data.rec</code>.  The names of the matching variables have to be specified with the argument <code>match.vars</code>.  To avoid confusion, the matching variables added to <code>data.rec</code> are renamed by adding the suffix &ldquo;don&rdquo;. By default <code>dup.x=FALSE</code>.
</p>
</td></tr>
<tr><td><code id="create.fused_+3A_match.vars">match.vars</code></td>
<td>

<p>A character vector with the names of the matching variables.  It has to be specified only when <code>dup.x=TRUE</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function allows to create the synthetic (or fused) data set after the application of a statistical matching in a <em>micro</em> framework.  For details see D'Orazio <em>et al.</em> (2006).
</p>


<h3>Value</h3>

<p>The data frame <code>data.rec</code> with the <code>z.vars</code> filled in and, when <code>dup.x=TRUE</code>, with the values of the matching variables <code>match.vars</code> observed on the donor records.
</p>


<h3>Author(s)</h3>

<p>Marcello D'Orazio <a href="mailto:mdo.statmatch@gmail.com">mdo.statmatch@gmail.com</a> 
</p>


<h3>References</h3>

<p>D'Orazio, M., Di Zio, M. and Scanu, M. (2006). <em>Statistical Matching: Theory and Practice.</em> Wiley, Chichester.
</p>


<h3>See Also</h3>

 
<p><code><a href="#topic+NND.hotdeck">NND.hotdeck</a></code>
<code><a href="#topic+RANDwNND.hotdeck">RANDwNND.hotdeck</a></code>
<code><a href="#topic+rankNND.hotdeck">rankNND.hotdeck</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
lab &lt;- c(1:15, 51:65, 101:115)
iris.rec &lt;- iris[lab, c(1:3,5)]  # recipient data.frame
iris.don &lt;- iris[-lab, c(1:2,4:5)] # donor data.frame

# Now iris.rec and iris.don have the variables
# "Sepal.Length", "Sepal.Width"  and "Species"
# in common.
#  "Petal.Length" is available only in iris.rec
#  "Petal.Width"  is available only in iris.don

# find the closest donors using NND hot deck;
# distances are computed on "Sepal.Length" and "Sepal.Width"

out.NND &lt;- NND.hotdeck(data.rec=iris.rec, data.don=iris.don,
            match.vars=c("Sepal.Length", "Sepal.Width"), 
            don.class="Species")

# create synthetic data.set, without the 
# duplication of the matching variables

fused.0 &lt;- create.fused(data.rec=iris.rec, data.don=iris.don, 
            mtc.ids=out.NND$mtc.ids, z.vars="Petal.Width")

# create synthetic data.set, with the "duplication" 
# of the matching variables

fused.1 &lt;- create.fused(data.rec=iris.rec, data.don=iris.don,
            mtc.ids=out.NND$mtc.ids, z.vars="Petal.Width",
            dup.x=TRUE, match.vars=c("Sepal.Length", "Sepal.Width"))
</code></pre>

<hr>
<h2 id='create.imputed'>Fills-in missing values in the recipient dataset with values observed on the donors units</h2><span id='topic+create.imputed'></span>

<h3>Description</h3>

<p>Imputes the missing values (NAs) in the recipient dataset with values observed on the donors units after search of donors with NND or random hotdeck.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create.imputed(data.rec, data.don, mtc.ids)  
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="create.imputed_+3A_data.rec">data.rec</code></td>
<td>

<p>A matrix or data frame that has missing values. 
</p>
</td></tr>
<tr><td><code id="create.imputed_+3A_data.don">data.don</code></td>
<td>

<p>A matrix or data frame that is used for donation (imputation).
</p>
</td></tr>
<tr><td><code id="create.imputed_+3A_mtc.ids">mtc.ids</code></td>
<td>

<p>A matrix with two columns.  Each row must contain the name or the index of the recipient record (row) in <code>data.don</code> and the name or the index of the corresponding donor record (row) in <code>data.don</code>.  Note that this type of matrix is returned by the functions <code><a href="#topic+NND.hotdeck">NND.hotdeck</a></code>, <code><a href="#topic+RANDwNND.hotdeck">RANDwNND.hotdeck</a></code>, <code><a href="#topic+rankNND.hotdeck">rankNND.hotdeck</a></code>, and <code><a href="#topic+mixed.mtc">mixed.mtc</a></code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function allows the missing values (NAs) in the recipient to be filled in with values observed in the donor dataset after searching for donors via NND or random hotdeck using the functions available in the package, i.e. <code><a href="#topic+NND.hotdeck">NND.hotdeck</a></code>, <code><a href="#topic+RANDwNND.hotdeck">RANDwNND.hotdeck</a></code>, <code><a href="#topic+rankNND.hotdeck">rankNND.hotdeck</a></code>, and <code><a href="#topic+mixed.mtc">mixed.mtc</a></code>.
If the same record in the recipient dataset has 2 or more NAs, they will all be replaced with the values observed for that unit on the selected donor; this is equivalent to joint hotdeck imputation. </p>


<h3>Value</h3>

<p>The data frame <code>data.rec</code> missing values (NAs) filled in.
</p>


<h3>Author(s)</h3>

<p>Marcello D'Orazio <a href="mailto:mdo.statmatch@gmail.com">mdo.statmatch@gmail.com</a> 
</p>


<h3>References</h3>

<p>D'Orazio, M., Di Zio, M. and Scanu, M. (2006). <em>Statistical Matching: Theory and Practice.</em> Wiley, Chichester.
</p>


<h3>See Also</h3>

 
<p><code><a href="#topic+NND.hotdeck">NND.hotdeck</a></code>
<code><a href="#topic+RANDwNND.hotdeck">RANDwNND.hotdeck</a></code>
<code><a href="#topic+rankNND.hotdeck">rankNND.hotdeck</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# introduce missing values
# in Petal.Length of iris dataset

set.seed(13579)
pos &lt;- sample(x = 1:nrow(iris), size = 15, 
              replace = FALSE)


iris.rec &lt;- iris[pos, ]  # recipient data.frame with missing values
iris.rec[, "Petal.Length"] &lt;- NA

iris.don &lt;- iris[-pos, ] # donor data.frame ALL observed

# find the closest donors using NND hot deck;
# distances are computed on "Petal.Width"
# donors only of the same Specie

out.NND &lt;- NND.hotdeck(data.rec=iris.rec, data.don=iris.don,
                       match.vars=c("Petal.Width"), 
                       don.class="Species")

# impute missing
iris.rec.imp &lt;- create.imputed(data.rec=iris.rec, data.don=iris.don, 
                        mtc.ids=out.NND$mtc.ids)
summary(iris.rec.imp$Petal.Length)

</code></pre>

<hr>
<h2 id='fact2dummy'>Transforms a categorical variable in a set of dummy variables</h2><span id='topic+fact2dummy'></span>

<h3>Description</h3>

<p>Transforms a factor or more factors contained in a data frame in a set of dummy variables, while numeric variables remain unchanged.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fact2dummy(data, all=TRUE, lab="x") 
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="fact2dummy_+3A_data">data</code></td>
<td>

<p>A factor or a data frame that contains one or more factors (columns whose class is &ldquo;factor&rdquo; or &ldquo;ordered&rdquo;) that have to be substituted by the corresponding dummy variables. 
</p>
</td></tr>
<tr><td><code id="fact2dummy_+3A_all">all</code></td>
<td>

<p>Logical. When <code>all=TRUE</code> (default) the output matrix will contain as many dummy variables as the number of the levels of the factor variable. On the contrary, when <code>all=FALSE</code>, the dummy variable related to the last level of the factor is dropped.
</p>
</td></tr>
<tr><td><code id="fact2dummy_+3A_lab">lab</code></td>
<td>

<p>A character string with the name of the variable to be pasted with its levels. This is used only when  <code>data</code> is a factor. By default it is set to &ldquo;x&rdquo;.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function substitutes categorical variables in the input data frame (columns whose class is &ldquo;factor&rdquo; or &ldquo;ordered&rdquo;) with the corresponding dummy variables. Note that if a factor includes a missing values (NA) then all the associated dummies will report an NA in correspondence of the missing observation (row).
</p>


<h3>Value</h3>

<p>A matrix with the dummy variables instead of initial factor variables.
</p>


<h3>Author(s)</h3>

<p>Marcello D'Orazio <a href="mailto:mdo.statmatch@gmail.com">mdo.statmatch@gmail.com</a> 
</p>


<h3>See Also</h3>

 
<p><code><a href="#topic+gower.dist">gower.dist</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
x &lt;- runif(5)
y &lt;- factor(c(1,2,1,2,2))
z &lt;- ordered(c(1,2,3,2,2))
xyz &lt;- data.frame(x,y,z)
fact2dummy(xyz)

fact2dummy(xyz, all=FALSE)


#example with iris data frame
str(iris)
ir.mat &lt;- fact2dummy(iris)
head(ir.mat)

</code></pre>

<hr>
<h2 id='Fbounds.pred'>
Estimates Frechet bounds for cells in the contingency table crossing two categorical variables observed in distinct samples referred to the same target population.</h2><span id='topic+Fbounds.pred'></span>

<h3>Description</h3>

<p>This function assesses the uncertainty in estimating the contingency table crossing <code>y.rec</code> (Y) and <code>z.don</code> (Z) when the two variables are observed in two different samples sharing a number of common predictors.</p>


<h3>Usage</h3>

<pre><code class='language-R'>Fbounds.pred(data.rec, data.don,
             match.vars, y.rec, z.don, pred = "multinom",
             w.rec = NULL, w.don = NULL, type.pred = "random",
             out.pred = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Fbounds.pred_+3A_data.rec">data.rec</code></td>
<td>
<p>dataframe including the Xs (predictors, listed in <code>match.vars</code>) and <code>y.rec</code> (response; target variable in this dataset)</p>
</td></tr>
<tr><td><code id="Fbounds.pred_+3A_data.don">data.don</code></td>
<td>
<p>dataframe including the Xs (predictors, listed in  <code>match.vars</code>) and <code>z.don</code> (response; target variable in this dataset)</p>
</td></tr>
<tr><td><code id="Fbounds.pred_+3A_match.vars">match.vars</code></td>
<td>
<p>vector with the names of the Xs variables to be used as predictors (or set in which select the best predictors with lasso) of respectively <code>y.rec</code> and <code>z.don</code></p>
</td></tr>
<tr><td><code id="Fbounds.pred_+3A_y.rec">y.rec</code></td>
<td>
<p>character indicating the name of Y target variable in <code>data.rec</code>. It should be a factor.</p>
</td></tr>
<tr><td><code id="Fbounds.pred_+3A_z.don">z.don</code></td>
<td>
<p>character indicating the name of Z target variable in <code>data.don</code>. It should be a factor.</p>
</td></tr>
<tr><td><code id="Fbounds.pred_+3A_pred">pred</code></td>
<td>
<p>character specifying the method used to obtain predictions of both Y and Z. Available methods include
<code>pred = "multinom"</code> (default) fits two multinomial models (<span class="pkg">nnet</span> function <code><a href="nnet.html#topic+multinom">multinom</a></code>) to get predictions with Y and Z as response variables and <code>match. vars</code> as predictors; <code>pred = "lasso"</code> uses the lasso method (<span class="rlang"><b>R</b></span> package <span class="pkg">glmnet</span>, function <code><a href="glmnet.html#topic+cv.glmnet">cv.glmnet</a></code>) and cross-validation to select a subset of <code>match.vars</code> that are the best predictors of Y and Z, respectively, and then fits the multinomial models with the selected predictors;
<code>pred = "nb"</code> uses the Naive Bayes classifier to get predictions of Y and Z respectively (<span class="rlang"><b>R</b></span> package <span class="pkg">naivebayes</span> function <code><a href="naivebayes.html#topic+naive_bayes">naive_bayes</a></code>); <code>pred = "rf"</code> fits randomForest to get predictions of both Y and Z (function <code><a href="randomForest.html#topic+randomForest">randomForest</a></code> in <span class="pkg">randomForest</span>).
</p>
</td></tr>
<tr><td><code id="Fbounds.pred_+3A_w.rec">w.rec</code></td>
<td>
<p>name of the variable with the weights of the units in <code>data.rec</code>, if available (default is NULL); the weights, if available, are only used for estimating bounds, not for fitting models.</p>
</td></tr>
<tr><td><code id="Fbounds.pred_+3A_w.don">w.don</code></td>
<td>
<p>name of the variable with the weights of the units in <code>data.don</code>, if available (default is NULL); the weights, if available, are only used for estimating bounds, not for fitting models.</p>
</td></tr>
<tr><td><code id="Fbounds.pred_+3A_type.pred">type.pred</code></td>
<td>
<p>string specifying how to obtain the predictions of Y and Z. By default, the fitted models return conditional probabilities (or scores), then if <code>type.pred = "random"</code> (default), the predicted class of Y (Z) is obtained by a random draw with selection probabilities equal to the estimated conditional probabilities (scores); on the contrary, if <code>type.pred = "mostvoted"</code>, the predicted class is the one with the highest estimated conditional probability (score).</p>
</td></tr>
<tr><td><code id="Fbounds.pred_+3A_out.pred">out.pred</code></td>
<td>
<p> Logical. If TRUE (default is FALSE) returns the input datasets with the estimated conditional probabilities (depending on <code>pred</code> argument), the predicted class for the target variable (Y or Z) in the dataset (depending on <code>type.pred</code> argument) and the true observed class of Y (or Z).
</p>
</td></tr>
<tr><td><code id="Fbounds.pred_+3A_...">...</code></td>
<td>
<p>additional arguments, if needed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function evaluates the uncertainty in estimating the contingency table crossing <code>y.rec</code> (Y) and <code>z.don</code> (Z) when the two variables are observed in two different samples related to the same target population, but the samples share a number of common predictors. The evaluation of the uncertainty is equivalent to estimating the bounds for each cell in the contingency table where Y and Z intersect; the bounds can be unconditional (Frechet property) or conditional on the predictions of both Y and Z provided by the models fitted according to the <code>pred</code> argument. This latter way of working avoids many of the drawbacks of obtaining expectations of conditional bounds when conditioning on many X variables, and allows the inclusion of non-categorical predictors. The final estimation of the conditional bounds is provided by the function <code><a href="#topic+Frechet.bounds.cat">Frechet.bounds.cat</a></code>.
</p>


<h3>Value</h3>

<p>a list with the following components: 
</p>
<p><code>up.rec</code> only when <code>out.pred = TRUE</code> it corresponds to a smaller version of <code>data.rec</code> with the estimated conditional probabilities for both Y and Z (depending on <code>pred</code> argument), the predicted class of Y (depending on <code>type.pred</code> argument),  the predicted class of Z (depending on <code>type.pred</code> argument), the true observed class of Y and the predictors (argument <code>match.vars</code>) (and the weights when <code>w.rec</code> is specified).
</p>
<p><code>up.don</code> only when <code>out.pred = TRUE</code> it corresponds to a smaller version of <code>data.don</code> with the estimated conditional probabilities for both Y and Z (depending on <code>pred</code> argument), the predicted class of Y (depending on <code>type.pred</code> argument), the predicted class of Z (depending on <code>type.pred</code> argument), the true observed class of Z and the predictors (argument <code>match.vars</code>) (and the weights when <code>w.don</code> is specified).
</p>
<p><code>p.xx.ini</code> the estimated relative frequencies in the table crossing predictions of Y and Z; it is estimated after pooling the samples (weighted average of estimates obtained on the separates samples);
</p>
<p><code>p.xy.ini</code> the estimated table crossing Y and the predictions of both Y and Z estimated from <code>data.rec</code> (weights are used if provided with the <code>w.rec</code> argument);
</p>
<p><code>p.xz.ini</code> the estimated table crossing Z and the predictions of both Y and Z estimated from <code>data.don</code> (weights are used if provided with the <code>w.don</code> argument); 
</p>
<p><code>accuracy</code> the estimated accuracy in predicting respectively Y and Z with the chosen method (argument <code>pred</code>) and the available predictors (argument <code>match.vars</code>);
</p>
<p><code>bounds</code> a data.frame whose columns reports the estimated unconditional and conditional bounds for each cell in the contingency table crossing <code>y.rec</code>(Y) and <code>z.don</code> (Z);
</p>
<p><code>uncertainty</code> the uncertainty associated to input data, measured in terms of average width of uncertainty bounds with and without conditioning on the predictions (for further details see <code><a href="#topic+Frechet.bounds.cat">Frechet.bounds.cat</a></code>.
</p>


<h3>Author(s)</h3>

<p>Marcello D'Orazio <a href="mailto:mdo.statmatch@gmail.com">mdo.statmatch@gmail.com</a> 
</p>


<h3>References</h3>

<p>D'Orazio, M., (2024). <em>Is Statistical Matching feasible?</em> Note, <a href="https://www.researchgate.net/publication/387699016_Is_statistical_matching_feasible">https://www.researchgate.net/publication/387699016_Is_statistical_matching_feasible</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Frechet.bounds.cat">Frechet.bounds.cat</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(quine, package="MASS") #loads quine from MASS
str(quine)

# split quine in two subsets
set.seed(223344)
lab.A &lt;- sample(nrow(quine), 70, replace=TRUE)
quine.A &lt;- quine[lab.A, 1:3]
quine.B &lt;- quine[-lab.A, 2:4]

# multinomial model and predictions with most-voted criterion
fbp &lt;- Fbounds.pred(data.rec = quine.A, data.don = quine.B, 
                    match.vars = c("Sex", "Age"), 
                    y.rec = "Eth", z.don = "Lrn", 
                    pred = "multinom", type.pred = "mostvoted")

fbp$p.xx.ini # estimated cross-tab of predictions
fbp$bounds # estimated conditional and unconditional bounds
fbp$uncertainty  # estimated uncertainty about Y*Z

# multinomial model and predictions with randomized criterion
fbp &lt;- Fbounds.pred(data.rec = quine.A, data.don = quine.B, 
                    match.vars = c("Sex", "Age"), 
                    y.rec = "Eth", z.don = "Lrn", 
                    pred = "multinom", type.pred = "random")

fbp$p.xx.ini # estimated cross-tab of predictions
fbp$bounds # estimated conditional and unconditional bounds
fbp$uncertainty  # estimated uncertainty about Y*Z
</code></pre>

<hr>
<h2 id='Fbwidths.by.x'>Computes the Frechet bounds of cells in a contingency table by considering all the possible subsets of the common variables.</h2><span id='topic+Fbwidths.by.x'></span>

<h3>Description</h3>

<p>This function permits to compute the bounds for cell probabilities in the contingency table Y vs. Z starting from the marginal tables (<b>X</b> vs. Y), (<b>X</b> vs. Z) and the joint distribution of the <b>X</b> variables, by considering all the possible subsets of the <b>X</b> variables.  In this manner it is possible to identify which subset of the <b>X</b> variables produces the major reduction of the average width of conditional bounds.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Fbwidths.by.x(tab.x, tab.xy, tab.xz, deal.sparse="discard", 
          nA=NULL, nB=NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Fbwidths.by.x_+3A_tab.x">tab.x</code></td>
<td>

<p>A <span class="rlang"><b>R</b></span> table crossing the <b>X</b> variables.  This table must be obtained by using the function <code><a href="stats.html#topic+xtabs">xtabs</a></code> or <code><a href="base.html#topic+table">table</a></code>, e.g. <br />
<code>tab.x &lt;- xtabs(~x1+x2+x3, data=data.all)</code>.
</p>
</td></tr>
<tr><td><code id="Fbwidths.by.x_+3A_tab.xy">tab.xy</code></td>
<td>

<p>A <span class="rlang"><b>R</b></span> table of <b>X</b> vs. Y variable.  This table must be obtained by using the function <code><a href="stats.html#topic+xtabs">xtabs</a></code> or <code><a href="base.html#topic+table">table</a></code>, e.g. <br />
<code>table.xy &lt;- xtabs(~x1+x2+x3+y, data=data.A)</code>.
</p>
<p>A single categorical Y variables is allowed.  One or more categorical variables can be considered as <b>X</b> variables (common variables).  The same <b>X</b> variables in <code>tab.x</code> must be available in <code>tab.xy</code>.  Moreover, it is assumed that the joint distribution of the <b>X</b> variables computed from <code>tab.xy</code> is equal to <code>tab.x</code>; a warning is produced if this is not true.
</p>
</td></tr>
<tr><td><code id="Fbwidths.by.x_+3A_tab.xz">tab.xz</code></td>
<td>

<p>A <span class="rlang"><b>R</b></span> table of <b>X</b> vs. Z variable.  This table must be obtained by using the function <code><a href="stats.html#topic+xtabs">xtabs</a></code> or <code><a href="base.html#topic+table">table</a></code>, e.g. <br />
<code>tab.xz &lt;- xtabs(~x1+x2+x3+z, data=data.B)</code>.
</p>
<p>A single categorical Z variable is allowed.  One or more categorical variables can be considered as <b>X</b> variables (common variables).  The same <b>X</b> variables in <code>tab.x</code> must be available in <code>tab.xz</code>.  Moreover, it is assumed that the joint distribution of the <b>X</b> variables computed from <code>tab.xz</code> is equal to <code>tab.x</code>; a warning is produced if this is not true.
</p>
</td></tr>
<tr><td><code id="Fbwidths.by.x_+3A_deal.sparse">deal.sparse</code></td>
<td>

<p>Text, how to estimate the cell relative frequencies when dealing with too sparse tables. When <code>deal.sparse="discard"</code> (default) no estimation is performed if <code>tab.xy</code> or <code>tab.xz</code> is too sparse. When <code>deal.sparse="relfreq"</code> the standard estimator (cell count divided by the sample size) is considered. 
Note that here sparseness is measured by number of cells with respect to the sample size; sparse table are those where the number of cells exceeds  the sample size (see Details).
</p>
</td></tr>
<tr><td><code id="Fbwidths.by.x_+3A_na">nA</code></td>
<td>

<p>Integer, sample size of file A used to estimate <code>tab.xy</code>. If  <code>NULL</code>, it is obtained as sum of frequencies in<code>tab.xy</code>.
</p>
</td></tr>
<tr><td><code id="Fbwidths.by.x_+3A_nb">nB</code></td>
<td>

<p>Integer, sample size of file B used to estimate <code>tab.xz</code>. If  <code>NULL</code>, it is obtained as sum of frequencies in<code>tab.xz</code>.
</p>
</td></tr>
<tr><td><code id="Fbwidths.by.x_+3A_...">...</code></td>
<td>

<p>Additional arguments that may be required when deriving an estimate of uncertainty by calling <code><a href="#topic+Frechet.bounds.cat">Frechet.bounds.cat</a></code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function permits to compute the Frechet bounds for the frequencies in the contingency table of Y vs. Z, starting from the conditional distributions P(Y|<b>X</b>) and P(Z|<b>X</b>) (for details see <br />
<code><a href="#topic+Frechet.bounds.cat">Frechet.bounds.cat</a></code>), by considering all the possible subsets of the <b>X</b> variables.  In this manner it is possible to identify the subset of the <b>X</b> variables, with highest association with both Y and Z, that permits to reduce the uncertainty concerning the distribution of Y vs. Z. 
</p>
<p>The uncertainty is measured by the average of the widths of the bounds for the cells in the table Y vs. Z:
</p>
<p style="text-align: center;"><code class="reqn"> \bar{d} = \frac{1}{J \times K} \sum_{j,k} ( p^{(up)}_{Y=j,Z=k} - p^{(low)}_{Y=j,Z=k} )</code>
</p>

<p>For details see <code><a href="#topic+Frechet.bounds.cat">Frechet.bounds.cat</a></code>.
</p>
<p>Provided that uncertainty, measured in terms of <code class="reqn">\bar{d}</code>, tends to reduce when conditioning on a higher number of <b>X</b> variables. Two penalties are introduced to account for the additional number of cells to be estimated when adding a X variable. The first penalty, introduced in D'Orazio et al. (2017), is:
</p>
<p style="text-align: center;"><code class="reqn">g_1=log\left( 1 + \frac{H_{D_m}}{H_{D_Q}} \right) </code>
</p>

<p>Where <code class="reqn">H_{D_m}</code> is the number of cell in the table obtained by crossing the given subset of <b>X</b> variables and the <code class="reqn">H_{D_Q}</code> is the number of cell in the table achieved by crossing all the available <b>X</b> variables. 
A second penalty takes into account the number of cells to estimate with respect to the sample size (D'Orazio et al., 2019). It is obtained as:
</p>
<p style="text-align: center;"><code class="reqn">g_2 = max \left[ \frac{1}{n_A - H_{D_m} \times J}, \frac{1}{n_B - H_{D_m} \times K}  \right]</code>
</p>

<p>with <code class="reqn">n_A &gt; H_{D_m} \times J</code>  and  <code class="reqn">n_B &gt; H_{D_m} \times K</code>. In practice, it is considered the number of cells to estimate compared to the sample size. This criterion is considered to measure sparseness too. In particular, for the purposes of this function, tables are NOT considered sparse when: 
</p>
<p style="text-align: center;"><code class="reqn">min\left[ \frac{n_A}{H_{D_m} \times J}, \frac{n_B}{H_{D_m} \times K} \right] &gt; 1 </code>
</p>

<p>This rule is applied when deciding how to proceed with estimation in case of sparse table (argument <code>deal.sparse</code>). 
Note that sparseness can be measured in different manners. The outputs include also the empty cells in each table (due to statistical zeros or structural zeros) and the Cohen's effect size with respect to the case of uniform distribution of frequencies across cells (the value 1/no.of.cells in every cell):
</p>
<p style="text-align: center;"><code class="reqn">\omega_{eq} = \sqrt{H \sum_{h=1}^{H} (\hat{p}_h - 1/H)^2 } </code>
</p>

<p>values of <code class="reqn">\omega_{eq}</code> jointly with  <code class="reqn">n/H \leq 1</code> usually indicate severe sparseness. 
</p>


<h3>Value</h3>

<p>A list with the estimated bounds for the cells in the table of Y vs. Z for each possible subset of the <b>X</b> variables.  The final component in the list, <code>sum.unc</code>, is a data.frame that summarizes the main results. In particular, it reports the number of <b>X</b> variables (<code>"x.vars"</code>), the number of cells in each of the input tables and the cells with frequency equal to 0 (columns ending with <code>freq0</code> ). Moreover, it reported the value (<code>"av.n"</code>) of the rule used to decide whether we are dealing with a sparse case (see Details) and the Cohen's effect size measured for the table crossing the considered combination of the X variables. 
Finally, it is provided the average width of the uncertainty intervals (<code>"av.width"</code>), the penalty terms g1 and g2 (<code>"penalty1"</code> and <code>"penalty2"</code> respectively), and the penalized average widths (<code>"av.width.pen1"</code> and <code>"av.width.pen2"</code>, where av.width.pen1=av.width+pen1 and av.width.pen2=av.width+pen2).
</p>


<h3>Author(s)</h3>

<p>Marcello D'Orazio <a href="mailto:mdo.statmatch@gmail.com">mdo.statmatch@gmail.com</a> 
</p>


<h3>References</h3>

<p>D'Orazio, M., Di Zio, M. and Scanu, M. (2006). <em>Statistical Matching: Theory and Practice.</em> Wiley, Chichester.
</p>
<p>D'Orazio, M., Di Zio, M. and Scanu, M. (2017). &ldquo;The use of uncertainty to choose matching variables in statistical matching&rdquo;. <em>International Journal of Approximate Reasoning </em>, 90, pp. 433-440.
</p>
<p>D'Orazio, M., Di Zio, M. and Scanu, M. (2019). &ldquo;Auxiliary variable selection in a a statistical matching problem&rdquo;. In Zhang, L.-C. and Chambers, R. L. (eds.) <em>Analysis of Integrated Data</em>, Chapman &amp; Hall/CRC (Forthcoming).
</p>


<h3>See Also</h3>

 
<p><code><a href="#topic+Frechet.bounds.cat">Frechet.bounds.cat</a></code>, <code><a href="#topic+harmonize.x">harmonize.x</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# un-comment to run
#
# data(quine, package="MASS") #loads quine from MASS
# str(quine)
# quine$c.Days &lt;- cut(quine$Days, c(-1, seq(0,50,10),100))
# table(quine$c.Days)
# 
# 
# # split quine in two subsets
# suppressWarnings(RNGversion("3.5.0"))
# set.seed(4567)
# lab.A &lt;- sample(nrow(quine), 70, replace=TRUE)
# quine.A &lt;- quine[lab.A, 1:4]
# quine.B &lt;- quine[-lab.A, c(1:3,6)]
# 
# # compute the tables required by Fbwidths.by.x()
# freq.xA &lt;- xtabs(~Eth+Sex+Age, data=quine.A)
# freq.xB &lt;- xtabs(~Eth+Sex+Age, data=quine.B)
# 
# freq.xy &lt;- xtabs(~Eth+Sex+Age+Lrn, data=quine.A)
# freq.xz &lt;- xtabs(~Eth+Sex+Age+c.Days, data=quine.B)
# 
# # apply Fbwidths.by.x()
# bounds.yz &lt;- Fbwidths.by.x(tab.x=freq.xA+freq.xB, tab.xy=freq.xy,
#                            tab.xz=freq.xz)
# 
# bounds.yz$sum.unc


</code></pre>

<hr>
<h2 id='Frechet.bounds.cat'>Frechet bounds of cells in a contingency table</h2><span id='topic+Frechet.bounds.cat'></span>

<h3>Description</h3>

<p>This function permits to derive the bounds for cell probabilities of the table Y vs. Z starting from the marginal tables (<b>X</b> vs. Y), (<b>X</b> vs. Z) and the joint distribution of the <b>X</b> variables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Frechet.bounds.cat(tab.x, tab.xy, tab.xz, print.f="tables", align.margins = FALSE,
                            tol= 0.001, warn = TRUE) 
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Frechet.bounds.cat_+3A_tab.x">tab.x</code></td>
<td>

<p>A <span class="rlang"><b>R</b></span> table crossing the <b>X</b> variables.  This table must be obtained by using the function <code><a href="stats.html#topic+xtabs">xtabs</a></code> or <code><a href="base.html#topic+table">table</a></code>,  e.g. <br />
<code>tab.x &lt;- xtabs(~x1+x2+x3, data=data.all)</code>. <br />
When <code>tab.x = NULL</code> then only <code>tab.xy</code> and <code>tab.xz</code> must be supplied.
</p>
</td></tr>
<tr><td><code id="Frechet.bounds.cat_+3A_tab.xy">tab.xy</code></td>
<td>

<p>A <span class="rlang"><b>R</b></span> table of <b>X</b> vs. Y variable.  This table must be obtained by using the function <code><a href="stats.html#topic+xtabs">xtabs</a></code> or <code><a href="base.html#topic+table">table</a></code>, e.g. <br />
<code>table.xy &lt;- xtabs(~x1+x2+x3+y, data=data.A)</code>.
</p>
<p>A single categorical Y variable is allowed.  One or more categorical variables can be considered as <b>X</b> variables (common variables).  Obviously, the same <b>X</b> variables in <code>tab.x</code> must be available in <code>tab.xy</code>.  Usually, it is assumed that the joint distribution of the <b>X</b> variables computed from <code>tab.xy</code> is equal to <code>tab.x</code> (a warning appears if any absolute difference is greater than <code>tol</code>). Note that when marginal distribution of  <b>X</b> in <code>tab.xy</code> is not equal to that of <code>tab.x</code> it is possible to ask their alignment (see argument <code>align.margins</code>).
</p>
<p>When <code>tab.x = NULL</code> then <code>tab.xy</code> should be a one&ndash;dimensional table providing the marginal distribution of the Y variable.
</p>
</td></tr>
<tr><td><code id="Frechet.bounds.cat_+3A_tab.xz">tab.xz</code></td>
<td>

<p>A <span class="rlang"><b>R</b></span> table of <b>X</b> vs. Z variable.  This table must be obtained by using the function <code><a href="stats.html#topic+xtabs">xtabs</a></code> or <code><a href="base.html#topic+table">table</a></code>, e.g. <br />
<code>tab.xz &lt;- xtabs(~x1+x2+x3+z, data=data.B)</code>.
</p>
<p>A single categorical Z variable is allowed.  One or more categorical variables can be considered as <b>X</b> variables (common variables).  The same <b>X</b> variables in <code>tab.x</code> must be available in <code>tab.xz</code>.  Usually, it is assumed that the joint distribution of the <b>X</b> variables computed from <code>tab.xz</code> is equal to <code>tab.x</code> (a warning appears if any absolute difference is greater than <code>tol</code>). Note that when marginal distribution of  <b>X</b> in <code>tab.xz</code> is not equal to that of <code>tab.x</code> it is possible to ask their alignment (see argument <code>align.margins</code>).
</p>
<p>When <code>tab.x = NULL</code> then <code>tab.xz</code> should be a one&ndash;dimensional table providing the marginal distribution of the Z variable.
</p>
</td></tr>
<tr><td><code id="Frechet.bounds.cat_+3A_print.f">print.f</code></td>
<td>

<p>A string: when <code>print.f="tables"</code> (default) all the cells' estimates will be saved as tables in a list.  On the contrary, if <code>print.f="data.frame"</code>, they will be saved as columns of a data.frame.
</p>
</td></tr>
<tr><td><code id="Frechet.bounds.cat_+3A_align.margins">align.margins</code></td>
<td>

<p>Logical (default <code>FALSE</code>). When when <code>TRUE</code> the distribution of <b>X</b> variables in <code>tab.xy</code> is aligned with the distribution resulting from <code>tab.x</code>, without affecting the marginal distribution of Y. Similarly, the distribution of <b>X</b> variables in <code>tab.xz</code> is aligned with the distribution resulting from <code>tab.x</code> without affecting the marginal distribution of Z. The alignment is performed by running IPF algorithm as implemented in the function <code><a href="mipfp.html#topic+Estimate">Estimate</a></code> in the package <span class="pkg">mipfp</span>. Note that to avoid lack of convergence due to combinations of Xs encountered in one table but not in the other (statistical 0s), before running IPF a small constant (1e-06) is added to empty cells in <code>tab.xy</code> and <code>tab.xz</code>.
</p>
</td></tr>
<tr><td><code id="Frechet.bounds.cat_+3A_tol">tol</code></td>
<td>

<p>Tolerance used in comparing joint distributions as far as <b>X</b> variables are considered (default <code>tol= 0.001</code>); estimation of cells bounds would require that distribution of <b>X</b> variables computed from <code>tab.xy</code> and <code>tab.xz</code> should be approximately equal to that in <code>tab.x</code>, on contrary  incoherences in estimated cells' bounds could happen. In case of not-coherent marginal distributions it is suggested to get them aligned by setting <code>align.margins=TRUE</code>.
</p>
</td></tr>
<tr><td><code id="Frechet.bounds.cat_+3A_warn">warn</code></td>
<td>

<p>Logical, when <code>TRUE</code> (default) return warnings when marginal distributions of <b>X</b> variables show differences grater than <code>tol</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function permits to compute the expected conditional Frechet bounds for the relative frequencies in the contingency table of Y vs. Z, starting from the distributions P(Y|X), P(Z|X) and P(X).  The expected conditional bounds for the relative frequencies <code class="reqn">p_{j,k}</code> in the table Y vs. Z are:
</p>
<p style="text-align: center;"><code class="reqn">  p^{(low)}_{Y=j,Z=k} = \sum_{i}  p_{X=i}\max (0; p_{Y=j|X=i} + p_{Z=k|X=i} - 1 ) </code>
</p>

<p style="text-align: center;"><code class="reqn"> p^{(up)}_{Y=j,Z=k} = \sum_{i} p_{X=i} \min ( p_{Y=j|X=i};  p_{Z=k|X=i})</code>
</p>

<p>The relative frequencies <code class="reqn">p_{X=i}=n_i/n</code> are computed from the frequencies in <code>tab.x</code>; <br />
the relative frequencies <code class="reqn">p_{Y=j|X=i}=n_{ij}/n_{i+}</code> are derived from <code>tab.xy</code>, <br />
finally, <code class="reqn">p_{Z=k|X=i}=n_{ik}/n_{i+}</code> are derived from <code>tab.xz</code>.
</p>
<p>Estimation requires that all the starting tables share the same marginal distribution of the <b>X</b> variables. 
</p>
<p>This function returns also the unconditional bounds for the relative frequencies in the contingency table of Y vs. Z, i.e. computed also without considering the <b>X</b> variables:
</p>
<p style="text-align: center;"><code class="reqn"> \max\{0; p_{Y=j} + p_{Z=k} - 1\} \leq p_{Y=j,Z=k} \leq \min \{ p_{Y=j}; p_{Z=k}\}</code>
</p>

<p>These bounds represent the unique output when <code>tab.x = NULL</code>.
</p>
<p>Finally, the contingency table of Y vs. Z estimated under the Conditional Independence Assumption (CIA) is obtained by considering:
</p>
<p style="text-align: center;"><code class="reqn"> p_{Y=j,Z=k} = p_{Y=j|X=i} \times p_{Z=k|X=i} \times p_{X=i}.</code>
</p>

<p>When <code>tab.x = NULL</code> then it is also provided the expected table under the assumption of independence between Y and Z:
</p>
<p style="text-align: center;"><code class="reqn"> p_{Y=j,Z=k} = p_{Y=j} \times p_{Z=k}.</code>
</p>

<p>The presence of too many cells with 0s in the input contingency tables is an indication of sparseness; this is an unappealing situation when estimating the cells' relative frequencies needed to derive the bounds; in such cases the corresponding results may be unreliable. A possible alternative way of working consists in estimating the required parameters by considering a pseudo-Bayes estimator (see <code><a href="#topic+pBayes">pBayes</a></code>); in practice the input <code>tab.x</code>, <code>tab.xy</code> and <code>tab.xz</code> should be the ones provided by the <code><a href="#topic+pBayes">pBayes</a></code> function.
</p>


<h3>Value</h3>

<p>When <code>print.f="tables"</code> (default) a list with the following components:
</p>
<table role = "presentation">
<tr><td><code>low.u</code></td>
<td>

<p>The estimated lower bounds for the relative frequencies in the table Y vs. Z without conditioning on the <b>X</b> variables.
</p>
</td></tr>
<tr><td><code>up.u</code></td>
<td>

<p>The estimated upper bounds for the relative frequencies in the table Y vs. Z without conditioning on the <b>X</b> variables.
</p>
</td></tr>
<tr><td><code>CIA</code></td>
<td>

<p>The estimated relative frequencies in the table Y vs. Z under the Conditional Independence Assumption (CIA).
</p>
</td></tr>
<tr><td><code>low.cx</code></td>
<td>

<p>The estimated lower bounds for the relative frequencies in the table Y vs. Z when conditioning on the <b>X</b> variables.
</p>
</td></tr>
<tr><td><code>up.cx</code></td>
<td>

<p>The estimated upper bounds for the relative frequencies in the table Y vs. Z when conditioning on the <b>X</b> variables.
</p>
</td></tr>
<tr><td><code>uncertainty</code></td>
<td>

<p>The uncertainty associated to input data, measured in terms of average width of uncertainty bounds with and without conditioning on the <b>X</b> variables.
</p>
</td></tr>
</table>
<p>When <code>print.f="data.frame"</code> the output list contains just two components: 
</p>
<table role = "presentation">
<tr><td><code>bounds</code></td>
<td>

<p>A data.frame whose columns reports the estimated uncertainty bounds.
</p>
</td></tr>
<tr><td><code>uncertainty</code></td>
<td>

<p>The uncertainty associated to input data, measured in terms of average width of uncertainty bounds with and without conditioning on the <b>X</b> variables.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Marcello D'Orazio <a href="mailto:mdo.statmatch@gmail.com">mdo.statmatch@gmail.com</a> 
</p>


<h3>References</h3>

<p>D'Orazio, M., Di Zio, M. and Scanu, M. (2006) &ldquo;Statistical Matching for Categorical Data: Displaying Uncertainty and Using Logical Constraints&rdquo;, <em>Journal of Official Statistics</em>, 22, pp. 137&ndash;157. 
</p>
<p>D'Orazio, M., Di Zio, M. and Scanu, M. (2006). <em>Statistical Matching: Theory and Practice.</em> Wiley, Chichester.
</p>


<h3>See Also</h3>

 
<p><code><a href="#topic+Fbwidths.by.x">Fbwidths.by.x</a></code>, <code><a href="#topic+harmonize.x">harmonize.x</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(quine, package="MASS") #loads quine from MASS
str(quine)

# split quine in two subsets
suppressWarnings(RNGversion("3.5.0"))
set.seed(7654)
lab.A &lt;- sample(nrow(quine), 70, replace=TRUE)
quine.A &lt;- quine[lab.A, 1:3]
quine.B &lt;- quine[-lab.A, 2:4]

# compute the tables required by Frechet.bounds.cat()
freq.xA &lt;- xtabs(~Sex+Age, data=quine.A)
freq.xB &lt;- xtabs(~Sex+Age, data=quine.B)

freq.xy &lt;- xtabs(~Sex+Age+Eth, data=quine.A)
freq.xz &lt;- xtabs(~Sex+Age+Lrn, data=quine.B)

# apply Frechet.bounds.cat()
bounds.yz &lt;- Frechet.bounds.cat(tab.x=freq.xA+freq.xB, tab.xy=freq.xy,
        tab.xz=freq.xz, print.f="data.frame")
bounds.yz

# harmonize distr. of Sex vs. Age during computations
# in Frechet.bounds.cat()

#compare marg. distribution of Xs in A and B vs. pooled estimate
comp.prop(p1=margin.table(freq.xy,c(1,2)), p2=freq.xA+freq.xB, 
          n1=nrow(quine.A), n2=nrow(quine.A)+nrow(quine.B), ref=TRUE)

comp.prop(p1=margin.table(freq.xz,c(1,2)), p2=freq.xA+freq.xB, 
          n1=nrow(quine.A), n2=nrow(quine.A)+nrow(quine.B), ref=TRUE)

bounds.yz &lt;- Frechet.bounds.cat(tab.x=freq.xA+freq.xB, tab.xy=freq.xy,
        tab.xz=freq.xz, print.f="data.frame", align.margins=TRUE)
bounds.yz

</code></pre>

<hr>
<h2 id='gower.dist'>Computes the Gower's Distance</h2><span id='topic+gower.dist'></span>

<h3>Description</h3>

<p>This function computes the Gower's distance (dissimilarity) between units in a dataset or between observations in two distinct datasets.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gower.dist(data.x, data.y=data.x, rngs=NULL, KR.corr=TRUE, var.weights = NULL, robcb=NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="gower.dist_+3A_data.x">data.x</code></td>
<td>

<p>A matrix or a data frame containing variables that should be used in the computation of the distance.
</p>
<p>Columns of mode <code>numeric</code> will be considered as interval scaled variables; columns of mode <code>character</code> or class <code>factor</code> will be considered as categorical nominal variables; columns of class <code>ordered</code> will be considered as categorical ordinal variables and, columns of mode <code>logical</code> will be considered as binary asymmetric variables (see Details for further information). 
</p>
<p>Missing values (<code>NA</code>) are allowed.
</p>
<p>If only <code>data.x</code> is supplied, the dissimilarities between rows of <code>data.x</code> will be computed. 
</p>
</td></tr>
<tr><td><code id="gower.dist_+3A_data.y">data.y</code></td>
<td>

<p>A numeric matrix or data frame with the same variables, of the same type, as those in <code>data.x</code>. Dissimilarities between rows of <code>data.x</code> and rows of <code>data.y</code> will be computed. If not provided, by default it is assumed equal to <code>data.x</code> and only dissimilarities between rows of <code>data.x</code> will be computed. 
</p>
</td></tr>
<tr><td><code id="gower.dist_+3A_rngs">rngs</code></td>
<td>

<p>A vector with the ranges to scale the variables. Its length must be equal to number of variables in <code>data.x</code>. In correspondence of non-numeric variables, just put 1 or <code>NA</code>. When <code>rngs=NULL</code> (default) the range of a numeric variable is estimated by jointly considering the values for the variable in <code>data.x</code> and those in <code>data.y</code>. Therefore, assuming <code>rngs=NULL</code>, if a variable <code>"X1"</code> is considered:
</p>
<pre>
rngs["X1"] &lt;- max(data.x[,"X1"], data.y[,"X1"]) - 
               min(data.x[,"X1"], data.y[,"X1"])</pre><p>.
</p>
</td></tr>
<tr><td><code id="gower.dist_+3A_kr.corr">KR.corr</code></td>
<td>

<p>When <code>TRUE</code> (default) the extension of the Gower's dissimilarity measure proposed by Kaufman and Rousseeuw (1990) is used. Otherwise, when <br />
<code>KR.corr=FALSE</code>, the Gower's (1971) formula is considered.
</p>
</td></tr>
<tr><td><code id="gower.dist_+3A_var.weights">var.weights</code></td>
<td>

<p>By default (<code>NULL</code>) each variable has the same weight (value 1) when calculating the overall distance (weighted average of distances on single variables; see Details). User can specify different weights for the different variables by providing a numeric value for each of the variables contributing to the distance. In other words, <code>var.weights</code> should be set equal to a numeric vector having length equal to the number of variables considered in calculating distance.  Entered weights are scales to sum up to 1. 
</p>
</td></tr>
<tr><td><code id="gower.dist_+3A_robcb">robcb</code></td>
<td>

<p>By default is (<code>NULL</code>). If <code>robcb="boxp"</code> the scaling of the Manhattan distance is done by using the difference between upper and lower fences of the Boxplot with k=3. In alternative,  <code>robcb="asyboxp"</code> the scaling of the Manhattan distance is done by the difference between upper and lower fences of the modified Boxplot to accocunt for slight skewness. In this case scaled distances greater than 1 are set equal to 1. This option is suggested in the presence of outliers in the continuous variables.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function computes distances between records when variables of different type (categorical and continuous) have been observed. In order to handle different types of variables, the Gower's  dissimilarity coefficient (Gower, 1971) is used. By default (<code>KR.corr=TRUE</code>) the Kaufman and Rousseeuw (1990) extension of the Gower's dissimilarity coefficient is used. 
</p>
<p>The final dissimilarity between the <em>i</em>th and <em>j</em>th unit is obtained as a weighted sum of dissimilarities for each variable:
</p>
<p style="text-align: center;"><code class="reqn">d(i,j) = \frac{\sum_k{\delta_{ijk} d_{ijk} w_k}}{\sum_k{\delta_{ijk} w_k}}</code>
</p>
 
<p>In particular, <code class="reqn">d_{ijk}</code> represents the distance between the <em>i</em>th and <em>j</em>th unit computed considering the <em>k</em>th variable, while <code class="reqn">w_k</code> is the weight assigned to variable <em>k</em> (by default 1 for all the variables, unless different weights are provided by user with argument <code>var.weights</code>). Distance depends on the nature of the variable:
</p>

<ul>
<li> <p><code>logical</code> columns are considered as asymmetric binary variables, for such case <code class="reqn">d_{ijk}=0</code> if <code class="reqn">x_{ik} = x_{jk} = \code{TRUE}</code>, 1 otherwise;
</p>
</li>
<li> <p><code>factor</code> or <code>character</code> columns are considered as categorical nominal variables and <code class="reqn">d_{ijk}=0</code> if <code class="reqn">x_{ik}=x_{jk}</code>, 1 otherwise;
</p>
</li>
<li> <p><code>numeric</code> columns are considered as interval-scaled variables and 
</p>
<p style="text-align: center;"><code class="reqn">d_{ijk}=\frac{\left|x_{ik}-x_{jk}\right|}{R_k}</code>
</p>

<p>being  <code class="reqn">R_k</code> the range of the <em>k</em>th variable. The range is the one supplied with the argument <code>rngs</code>  (<code>rngs[k]</code>) or the one computed on available data (when <code>rngs=NULL</code>);
</p>
</li>
<li> <p><code>ordered</code> columns are considered as categorical ordinal variables and the values are substituted with the corresponding position index, <code class="reqn">r_{ik}</code> in the factor levels. When <code>KR.corr=FALSE</code> these position indexes (that are different from the output of the R function <code><a href="base.html#topic+rank">rank</a></code>) are transformed in the following manner  
</p>
<p style="text-align: center;"><code class="reqn">z_{ik}=\frac{(r_{ik}-1)}{max\left(r_{ik}\right) - 1}</code>
</p>

<p>These new values, <code class="reqn">z_{ik}</code>, are treated as observations of an interval scaled variable.
</p>
</li></ul>

<p>As far as the weight <code class="reqn">\delta_{ijk}</code> is concerned:
</p>

<ul>
<li> <p><code class="reqn">\delta_{ijk}=0</code> if <code class="reqn">x_{ik} = \code{NA}</code> or <code class="reqn">x_{jk} = \code{NA}</code>;
</p>
</li>
<li> <p><code class="reqn">\delta_{ijk}=0</code> if the variable is asymmetric binary  and  <code class="reqn">x_{ik}=x_{jk}=0</code> or  <code class="reqn">x_{ik} = x_{jk} = \code{FALSE}</code>;
</p>
</li>
<li> 
<p><code class="reqn">\delta_{ijk}=1</code> in all the other cases.
</p>
</li></ul>

<p>In practice, <code>NAs</code> and couple of cases with <code class="reqn">x_{ik}=x_{jk}=\code{FALSE}</code> do not contribute to distance computation.
</p>


<h3>Value</h3>

<p>A <code>matrix</code> object with distances between rows of <code>data.x</code> and those of <code>data.y</code>.
</p>


<h3>Author(s)</h3>

<p>Marcello D'Orazio <a href="mailto:mdo.statmatch@gmail.com">mdo.statmatch@gmail.com</a> 
</p>


<h3>References</h3>

<p>Gower, J. C. (1971), &ldquo;A general coefficient of similarity and some of its properties&rdquo;. <em>Biometrics</em>, <b>27</b>, 623&ndash;637.
</p>
<p>Kaufman, L. and Rousseeuw, P.J. (1990), <em>Finding Groups in Data: An Introduction to Cluster Analysis.</em> Wiley, New York.
</p>


<h3>See Also</h3>

 
<p><code><a href="cluster.html#topic+daisy">daisy</a></code>, 
<code><a href="proxy.html#topic+dist">dist</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
x1 &lt;- as.logical(rbinom(10,1,0.5)) 
x2 &lt;- sample(letters, 10, replace=TRUE)
x3 &lt;- rnorm(10)
x4 &lt;- ordered(cut(x3, -4:4, include.lowest=TRUE))
xx &lt;- data.frame(x1, x2, x3, x4, stringsAsFactors = FALSE)

# matrix of distances between observations in xx
dx &lt;- gower.dist(xx)
head(dx)

# matrix of distances between first obs. in xx
# and the remaining ones
gower.dist(data.x=xx[1:6,], data.y=xx[7:10,], var.weights = c(1,2,5,2))

</code></pre>

<hr>
<h2 id='harmonize.x'>Harmonizes the marginal (joint) distribution of a set of variables observed independently in two sample surveys referred to the same target population</h2><span id='topic+harmonize.x'></span>

<h3>Description</h3>

<p>This function permits to harmonize the marginal or the joint distribution of a set of variables observed independently in two sample surveys carried out on the same target population.  This harmonization is carried out by using the calibration of the survey weights of the sample units in both the surveys according to the procedure suggested by Renssen (1998).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>harmonize.x(svy.A, svy.B, form.x, x.tot=NULL, 
                      cal.method="linear", ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="harmonize.x_+3A_svy.a">svy.A</code></td>
<td>

<p>A <code>svydesign</code> <span class="rlang"><b>R</b></span> object that stores the data collected in the the survey A and all the information concerning the corresponding sampling design.  This object can be created by using the function <code><a href="survey.html#topic+svydesign">svydesign</a></code> in the package <span class="pkg">survey</span>.
</p>
</td></tr>
<tr><td><code id="harmonize.x_+3A_svy.b">svy.B</code></td>
<td>

<p>A <code>svydesign</code> <span class="rlang"><b>R</b></span> object that stores the data collected in the the survey B and all the information concerning the corresponding sampling design.  This object can be created by using the function <code><a href="survey.html#topic+svydesign">svydesign</a></code> in the package <span class="pkg">survey</span>.</p>
</td></tr>
<tr><td><code id="harmonize.x_+3A_form.x">form.x</code></td>
<td>

<p>A <span class="rlang"><b>R</b></span> formula specifying which of the variables, common to both the surveys, have to be considered, and how have to be considered.  For instance <br />
<code>form.x=~x1+x2</code> means that the marginal distribution of the variables x1 and x2 have to be harmonized and there is also an &lsquo;Intercept&rsquo;.  In order to skip the intercept the formula has to be written in the following manner <br /> <code>form.x=~x1+x2-1</code>.
</p>
<p>When dealing with categorical variables, the formula <code>form.x=~x1:x2-1</code> means that the harmonization has to be carried out by considering the joint distribution of the two variables (x1 vs. x2).  To better understand how <code>form.x</code> works see <code><a href="stats.html#topic+model.matrix">model.matrix</a></code> (see also <code><a href="stats.html#topic+formula">formula</a></code>).
</p>
<p>Due to weights calibration features, it is preferable to work with categorical <b>X</b> variables.  In some cases, the procedure may be successful when a single continuous variable is considered jointly with one or more categorical variables.  When dealing with several continuous variable it may be preferable to categorize them.
</p>
</td></tr>
<tr><td><code id="harmonize.x_+3A_x.tot">x.tot</code></td>
<td>

<p>A vector or table with known population totals for the <b>X</b> variables.  A vector is required when <code>cal.method="linear"</code> or <code>cal.method="raking"</code>.  The names and the length of the vector depends on the way it is specified the argument <code>form.x</code> (see <code><a href="stats.html#topic+model.matrix">model.matrix</a></code>).  A contingency table is required when <br /> <code>cal.method="poststratify"</code> (for details see <code><a href="survey.html#topic+postStratify">postStratify</a></code>).
</p>
<p>When <code>x.tot</code> is not provided (i.e. <code>x.tot=NULL</code>) then the vector of totals is estimated as a weighted average of the totals estimated on the two surveys.  The weight assigned to the totals estimated from A is <code class="reqn">\lambda = n_A/(n_A+n_B)</code>;  <code class="reqn">1-\lambda</code> is the weight assigned to <b>X</b> totals estimated from survey B (<code class="reqn">n_A</code> and <code class="reqn">n_B</code> are the number of units in A and B respectively).
</p>
</td></tr>
<tr><td><code id="harmonize.x_+3A_cal.method">cal.method</code></td>
<td>

<p>A string that specifies how the calibration of the weights in <code>svy.A</code> and <code>svy.B</code> has to be carried out.  By default linear calibration is performed ( <code>cal.method="linear"</code>).  In particular, the calibration is carried out by mean of the function <code><a href="survey.html#topic+calibrate">calibrate</a></code> in the package <span class="pkg">survey</span>.
</p>
<p>Alternatively, it is possible to rake the origin survey weights by specifying <code>cal.method="raking"</code>.  Finally, it is possible to perform a simple post-stratification by setting <code>cal.method="poststratify"</code>.  Note that in this case the weights adjustments are carried out by considering the function <br />
<code><a href="survey.html#topic+postStratify">postStratify</a></code> in the package <span class="pkg">survey</span>.
</p>
</td></tr>
<tr><td><code id="harmonize.x_+3A_...">...</code></td>
<td>

<p>Further arguments that may be necessary for calibration or post-stratification.  The number of iterations used in calibration can be modified too by using the argument <code>maxit</code> (by default <code>maxit=50</code>).
</p>
<p>See <code><a href="survey.html#topic+calibrate">calibrate</a></code> for further details.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function harmonizes the totals of the <b>X</b> variables, observed in both survey A and survey B, to be equal to given known totals specified via <code>x.tot</code>.  When these totals are not known (<code>x.tot=NULL</code>)  they are estimated by combining the estimates derived from the two separate surveys.  The harmonization is carried out according to a procedure suggested by Renssen (1998) based on calibration of survey weights (for major details on calibration see Sarndal and Lundstrom, 2005).  The procedure is particularly suited to deal with categorical <b>X</b> variables, in this case it permits to harmonize the joint or the marginal distribution of the categorical variables being considered. Note that an incomplete crossing of the <b>X</b> variables can be considered: i.e. harmonisation wrt to the joint distribution of <code class="reqn">X_1 \times X_2</code> and the marginal distribution of <code class="reqn">X_3</code>).
</p>
<p>The calibration procedure may not produce the final result due to convergence problems.  In this case an error message appears.  In order to reach convergence it may be necessary to launch the procedure with less constraints (i.e a reduced number of population totals) by joining adjacent categories or by discarding some variables.  
</p>
<p>In some limited cases, it could be possible to consider both categorical and continuous variables.  In this situation it may happen that calibration is not successful.  In order to reach convergence it may be necessary to categorize the continuous <b>X</b> variables.
</p>
<p>Post-stratification is a special case of calibration; all the weights of the units in a given post-stratum are modified so as to reproduce the known total for that post-stratum.  Post-stratification avoids problems of convergence but, on the other hand, it may produce final weights with a higher variability than those derived from the calibration.
</p>


<h3>Value</h3>

<p>A <span class="rlang"><b>R</b></span> with list the results of calibration procedures carried out on survey A and survey B, respectively.  In particular the following components will be provided:
</p>
<table role = "presentation">
<tr><td><code>cal.A</code></td>
<td>

<p>The survey object <code>svy.A</code> after the calibration; in particular, the weights now are calibrated with respect to the totals of the <b>X</b> variables.
</p>
</td></tr>
<tr><td><code>cal.B</code></td>
<td>

<p>The survey object <code>svy.B</code> after the calibration; in particular, the weights now are calibrated with respect to the totals of the <b>X</b> variables.
</p>
</td></tr>
<tr><td><code>weights.A</code></td>
<td>

<p>The new calibrated weights associated to the the units in <code>svy.A</code>.
</p>
</td></tr>
<tr><td><code>weights.B</code></td>
<td>

<p>The new calibrated weights associated to the the units in <code>svy.B</code>.
</p>
</td></tr>
<tr><td><code>call</code></td>
<td>

<p>Stores the call to this function  with all the values specified for the various arguments (<code>call=match.call()</code>).
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Marcello D'Orazio <a href="mailto:mdo.statmatch@gmail.com">mdo.statmatch@gmail.com</a> 
</p>


<h3>References</h3>

<p>D'Orazio, M., Di Zio, M. and Scanu, M. (2006). <em>Statistical Matching: Theory and Practice</em>. Wiley, Chichester.
</p>
<p>Renssen, R.H. (1998) &ldquo;Use of Statistical Matching Techniques in Calibration Estimation&rdquo;. <em>Survey Methodology</em>, N. <b>24</b>, pp. 171&ndash;183.
</p>
<p>Sarndal, C.E. and Lundstrom, S. (2005) <em>Estimation in Surveys with Nonresponse</em>. Wiley, Chichester.
</p>


<h3>See Also</h3>

 
<p><code><a href="#topic+comb.samples">comb.samples</a></code>, <code><a href="survey.html#topic+calibrate">calibrate</a></code>, <code><a href="survey.html#topic+svydesign">svydesign</a></code>, <code><a href="survey.html#topic+postStratify">postStratify</a></code>, 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(quine, package="MASS") #loads quine from MASS
str(quine)

# split quine in two subsets
suppressWarnings(RNGversion("3.5.0"))
set.seed(7654)
lab.A &lt;- sample(nrow(quine), 70, replace=TRUE)
quine.A &lt;- quine[lab.A, c("Eth","Sex","Age","Lrn")]
quine.B &lt;- quine[-lab.A, c("Eth","Sex","Age","Days")]

# create svydesign objects
require(survey)
quine.A$f &lt;- 70/nrow(quine) # sampling fraction
quine.B$f &lt;- (nrow(quine)-70)/nrow(quine)
svy.qA &lt;- svydesign(~1, fpc=~f, data=quine.A)
svy.qB &lt;- svydesign(~1, fpc=~f, data=quine.B)

#------------------------------------------------------
# example (1)
# Harmonizazion of the distr. of Sex vs. Age
# usign poststratification

# (1.a) known population totals
# the population toatal are computed on the full data frame
tot.sex.age &lt;- xtabs(~Sex+Age, data=quine)
tot.sex.age

out.hz &lt;- harmonize.x(svy.A=svy.qA, svy.B=svy.qB, form.x=~Sex+Age,
          x.tot=tot.sex.age, cal.method="poststratify")

tot.A &lt;- xtabs(out.hz$weights.A~Sex+Age, data=quine.A)
tot.B &lt;- xtabs(out.hz$weights.B~Sex+Age, data=quine.B)

tot.sex.age-tot.A
tot.sex.age-tot.B

# (1.b) unknown population totals (x.tot=NULL)
# the population total is estimated by combining totals from the
 # two surveys

out.hz &lt;- harmonize.x(svy.A=svy.qA, svy.B=svy.qB, form.x=~Sex+Age,
          x.tot=NULL, cal.method="poststratify")

tot.A &lt;- xtabs(out.hz$weights.A~Sex+Age, data=quine.A)
tot.B &lt;- xtabs(out.hz$weights.B~Sex+Age, data=quine.B)

tot.A
tot.A-tot.B

#-----------------------------------------------------
# example (2)
# Harmonizazion wrt the maginal distribution
# of 'Eth', 'Sex' and 'Age'
# using linear calibration

# (2.a) vector of population total known
# estimated from the full data set
# note the formula! only marginal distribution of the
# variables are considered
tot.m &lt;- colSums(model.matrix(~Eth+Sex+Age-1, data=quine))
tot.m

out.hz &lt;- harmonize.x(svy.A=svy.qA, svy.B=svy.qB, x.tot=tot.m,
            form.x=~Eth+Sex+Age-1, cal.method="linear")

summary(out.hz$weights.A) #check for negative weights
summary(out.hz$weights.B) #check for negative weights

tot.m
svytable(formula=~Eth, design=out.hz$cal.A)
svytable(formula=~Eth, design=out.hz$cal.B)

svytable(formula=~Sex, design=out.hz$cal.A)
svytable(formula=~Sex, design=out.hz$cal.B)

# Note: margins are equal but joint distributions are not!
svytable(formula=~Sex+Age, design=out.hz$cal.A)
svytable(formula=~Sex+Age, design=out.hz$cal.B)

# (2.b) vector of population total unknown
out.hz &lt;- harmonize.x(svy.A=svy.qA, svy.B=svy.qB, x.tot=NULL,
            form.x=~Eth+Sex+Age-1, cal.method="linear")
svytable(formula=~Eth, design=out.hz$cal.A)
svytable(formula=~Eth, design=out.hz$cal.B)

svytable(formula=~Sex, design=out.hz$cal.A)
svytable(formula=~Sex, design=out.hz$cal.B)

#-----------------------------------------------------
# example (3)
# Harmonizazion wrt the joint distribution of 'Sex' vs. 'Age'
# and the marginal distribution of 'Eth'
# using raking

# vector of population total known
# estimated from the full data set
# note the formula!
tot.m &lt;- colSums(model.matrix(~Eth+(Sex:Age-1)-1, data=quine))
tot.m

out.hz &lt;- harmonize.x(svy.A=svy.qA, svy.B=svy.qB, x.tot=tot.m,
            form.x=~Eth+(Sex:Age)-1, cal.method="raking")

summary(out.hz$weights.A) #check for negative weights
summary(out.hz$weights.B) #check for negative weights

tot.m
svytable(formula=~Eth, design=out.hz$cal.A)
svytable(formula=~Eth, design=out.hz$cal.B)

svytable(formula=~Sex+Age, design=out.hz$cal.A)
svytable(formula=~Sex+Age, design=out.hz$cal.B)


</code></pre>

<hr>
<h2 id='mahalanobis.dist'>Computes the Mahalanobis Distance</h2><span id='topic+mahalanobis.dist'></span>

<h3>Description</h3>

<p>This function computes the Mahalanobis distance among units in a dataset or between observations in two distinct datasets.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mahalanobis.dist(data.x, data.y = NULL, vc = NULL, rob.vc = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mahalanobis.dist_+3A_data.x">data.x</code></td>
<td>

<p>A matrix or a data frame containing variables that should be used in the computation of the distance between units.  Only continuous variables are allowed.  Missing values (<code>NA</code>) are not allowed.
</p>
<p>When only <code>data.x</code> is supplied, the distances between rows of <code>data.x</code> is computed. 
</p>
</td></tr>
<tr><td><code id="mahalanobis.dist_+3A_data.y">data.y</code></td>
<td>

<p>A numeric matrix or data frame with the same variables, of the same type, as those in <code>data.x</code> (only continuous variables are allowed).  Dissimilarities between rows of <code>data.x</code> and rows of <code>data.y</code> will be computed.  If not provided, by default it is assumed <code>data.y=data.x</code> and only dissimilarities between rows of <code>data.x</code> will be computed. 
</p>
</td></tr>
<tr><td><code id="mahalanobis.dist_+3A_vc">vc</code></td>
<td>

<p>Covariance matrix that should be used in distance computation.  If it is not supplied (<code>vc = NULL</code>) it is estimated from the input data.  In particular, when <code>vc = NULL</code> and only <code>data.x</code> is supplied then the covariance matrix is estimated from <code>data.x</code> (i.e. <code>vc = var(data.x)</code>).  On the contrary when <code>vc = NULL</code> and both <code>data.x</code> and <code>data.y</code> are available then the covariance matrix is estimated on the joined data sets.
</p>
</td></tr>
<tr><td><code id="mahalanobis.dist_+3A_rob.vc">rob.vc</code></td>
<td>

<p>Logical, if TRUE when <code>vc = NULL</code> the covariance matrix is estimated using a robust method by means of the function <code><a href="MASS.html#topic+cov.rob">cov.rob</a></code> in the package <span class="pkg">MASS</span>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Mahalanobis distance is calculated by means of:
</p>
<p style="text-align: center;"><code class="reqn">d(i,j)=\sqrt{(x_i - x_j)^T S^{-1} (x_i - x_j)}</code>
</p>

<p>The covariance matrix <em>S</em> is estimated from the available data when <code>vc=NULL</code>, otherwise the one supplied via the argument <code>vc</code> is used. 
</p>


<h3>Value</h3>

<p>A <code>matrix</code> object with distances among rows of <code>data.x</code> and those of <code>data.y</code>.
</p>


<h3>Author(s)</h3>

<p>Marcello D'Orazio <a href="mailto:mdo.statmatch@gmail.com">mdo.statmatch@gmail.com</a> 
</p>


<h3>References</h3>

<p>Mahalanobis, P C (1936) &ldquo;On the generalised distance in statistics&rdquo;. Proceedings of the National Institute of Sciences of India 2, pp. 49-55.
</p>


<h3>See Also</h3>

 
<p><code><a href="stats.html#topic+mahalanobis">mahalanobis</a></code> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
md1 &lt;- mahalanobis.dist(iris[1:6,1:4])
md2 &lt;- mahalanobis.dist(data.x=iris[1:6,1:4], data.y=iris[51:60, 1:4])

vv &lt;- var(iris[,1:4])
md1a &lt;- mahalanobis.dist(data.x=iris[1:6,1:4], vc=vv)
md2a &lt;- mahalanobis.dist(data.x=iris[1:6,1:4], data.y=iris[51:60, 1:4], vc=vv)

</code></pre>

<hr>
<h2 id='maximum.dist'>Computes the Maximum  Distance</h2><span id='topic+maximum.dist'></span>

<h3>Description</h3>

<p>This function computes the Maximum distance (or <code class="reqn">L^\infty</code> norm) between units in a dataset or between observations in two distinct datasets.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>maximum.dist(data.x, data.y=data.x, rank=FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="maximum.dist_+3A_data.x">data.x</code></td>
<td>

<p>A matrix or a data frame containing variables that should be used in the computation of the distance.  Only continuous variables are allowed.  Missing values (<code>NA</code>) are not allowed.
</p>
<p>When only <code>data.x</code> is supplied, the distances between rows of <code>data.x</code> are computed. 
</p>
</td></tr>
<tr><td><code id="maximum.dist_+3A_data.y">data.y</code></td>
<td>

<p>A numeric matrix or data frame with the same variables, of the same type, as those in <code>data.x</code> (only continuous variables are allowed).  Dissimilarities between rows of <code>data.x</code> and rows of <code>data.y</code> will be computed.  If not provided, by default it is assumed <code>data.y=data.x</code> and only dissimilarities between rows of <code>data.x</code> will be computed. 
</p>
</td></tr>
<tr><td><code id="maximum.dist_+3A_rank">rank</code></td>
<td>

<p>Logical, when <code>TRUE</code> the original values are substituted by their ranks divided by the number of values plus one (following suggestion in Kovar et al. 1988).  This rank transformation permits to remove the effect of different scales on the distance computation.  When computing ranks the tied observations assume the average of their position 
(<code>ties.method = "average"</code> in calling the <code><a href="base.html#topic+rank">rank</a></code> function). 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function computes the <code class="reqn">L^\infty</code> distance also know as <em>minimax</em> distance.  In practice the distance between two records is the maximum of the absolute differences on the available variables:
</p>
<p style="text-align: center;"><code class="reqn">d(i,j) = max \left( \left|x_{1i}-x_{1j} \right|, \left|x_{2i}-x_{2j} \right|,\ldots,\left|x_{Ki}-x_{Kj} \right| \right)</code>
</p>

<p>When <code>rank=TRUE</code> the original values are substituted by their ranks divided by the number of values plus one (following suggestion in Kovar et al. 1988). 
</p>


<h3>Value</h3>

<p>A <code>matrix</code> object with distances between rows of <code>data.x</code> and those of <code>data.y</code>.
</p>


<h3>Author(s)</h3>

<p>Marcello D'Orazio <a href="mailto:mdo.statmatch@gmail.com">mdo.statmatch@gmail.com</a> 
</p>


<h3>References</h3>

<p>Kovar, J.G., MacMillan, J. and Whitridge, P. (1988). &ldquo;Overview and strategy for the Generalized Edit and Imputation System&rdquo;. Statistics Canada, Methodology Branch Working Paper No. BSMD 88-007 E/F.
</p>


<h3>See Also</h3>

 
<p><code><a href="base.html#topic+rank">rank</a></code>, 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
md1 &lt;- maximum.dist(iris[1:10,1:4])
md2 &lt;- maximum.dist(iris[1:10,1:4], rank=TRUE)

md3 &lt;- maximum.dist(data.x=iris[1:50,1:4], data.y=iris[51:100,1:4])
md4 &lt;- maximum.dist(data.x=iris[1:50,1:4], data.y=iris[51:100,1:4], rank=TRUE)

</code></pre>

<hr>
<h2 id='mixed.mtc'>Statistical Matching via Mixed Methods</h2><span id='topic+mixed.mtc'></span>

<h3>Description</h3>

<p>This function implements some mixed methods to perform statistical matching between two data sources. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mixed.mtc(data.rec, data.don, match.vars, y.rec, z.don, method="ML",
           rho.yz=NULL, micro=FALSE, constr.alg="Hungarian") 
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mixed.mtc_+3A_data.rec">data.rec</code></td>
<td>

<p>A matrix or data frame that plays the role of <em>recipient</em> in the statistical matching application. This data set must contain all variables (columns) that should be used in statistical matching, i.e. the variables called by the arguments <br /> <code>match.vars</code> and <code>y.rec</code>. Note that continuous variables are expected, if there are some categorical variables they are re-coded into dummies. Missing values (<code>NA</code>) are not allowed.
</p>
</td></tr>
<tr><td><code id="mixed.mtc_+3A_data.don">data.don</code></td>
<td>

<p>A matrix or data frame that plays the role of <em>donor</em> in the statistical matching application. This data set must contain all the numeric variables (columns) that should be used in statistical matching, i.e. the variables called by the arguments <code>match.vars</code> and <code>z.don</code>. Note that continuous variables are expected, if there are some categorical variables they are re-coded into dummies. Missing values (<code>NA</code>) are not allowed. 
</p>
</td></tr>
<tr><td><code id="mixed.mtc_+3A_match.vars">match.vars</code></td>
<td>

<p>A character vector with the names of the common variables (the columns in both the data frames) to be used as matching variables (<b>X</b>). 
</p>
</td></tr>
<tr><td><code id="mixed.mtc_+3A_y.rec">y.rec</code></td>
<td>

<p>A character vector with the name of the target variable Y that is observed only for units in <code>data.rec</code>. Only one continuous variable is allowed.
</p>
</td></tr>
<tr><td><code id="mixed.mtc_+3A_z.don">z.don</code></td>
<td>

<p>A character vector with the name of the target variable Z that is observed only for units in <code>data.don</code>.  Only one continuous variable is allowed.
</p>
</td></tr>
<tr><td><code id="mixed.mtc_+3A_method">method</code></td>
<td>

<p>A character vector that identifies the method that should be used to estimate the parameters of the regression models: Y vs. <b>X</b> and Z vs. <b>X</b>.  Maximum Likelihood method is used when <code>method="ML"</code> (default); on the contrary, when <code>method="MS"</code> the parameters are estimated according to approach proposed by Moriarity and Scheuren (2001 and 2003).  See Details for further information.
</p>
</td></tr>
<tr><td><code id="mixed.mtc_+3A_rho.yz">rho.yz</code></td>
<td>

<p>A numeric value representing a guess for the correlation between the Y (<code>y.rec</code>) and the Z variable (<code>z.don</code>) that are not jointly observed.  When <code>method="MS"</code> then the argument <code>cor.yz</code> must specify the value of the correlation coefficient <code class="reqn">\rho_{YZ}</code>; on the contrary, when <code>method="ML"</code>, it must specify the <em>partial correlation coefficient</em> between Y and Z given <b>X</b> (<code class="reqn">\rho_{YZ|\bf{X}}</code>). 
</p>
<p>By default (<code>rho.yz=NULL</code>).  In practice, in absence of auxiliary information concerning the correlation coefficient or the partial correlation coefficient, the statistical matching is carried out under the assumption of independence between Y and Z given <b>X</b> (Conditional Independence Assumption, CIA ), i.e.  <code class="reqn">\rho_{YZ|\bf{X}}=0</code>. 
</p>
</td></tr>
<tr><td><code id="mixed.mtc_+3A_micro">micro</code></td>
<td>

<p>Logical.  When <code>micro=FALSE</code> (default) only the parameters' estimates are returned.  On the contrary, when <code>micro=TRUE</code> the function returns also <code>data.rec</code> filled in with the values for the variable Z.  The donors for filling in Z in <code>data.rec</code> are identified using a constrained distance hot deck method.  In this case, the number of units (rows) in <code>data.don</code> must be grater or equal to the number of units (rows) in <code>data.rec</code>.  See next argument and Details for further information.
</p>
</td></tr>
<tr><td><code id="mixed.mtc_+3A_constr.alg">constr.alg</code></td>
<td>

<p>A string that has to be specified when <code>micro=TRUE</code>, in order to solve the transportation problem involved by the constrained distance hot deck method.  Two choices are available: &ldquo;lpSolve&rdquo; and &ldquo;Hungarian&rdquo;.  In the first case, <br />
<code>constr.alg="lpSolve"</code>, the transportation problem is solved by means of the function <code><a href="lpSolve.html#topic+lp.transport">lp.transport</a></code> available in the package <span class="pkg">lpSolve</span>.  When <br /> <code>constr.alg="Hungarian"</code> (default) the transportation problem is solved using the Hungarian method implemented in the function <code><a href="clue.html#topic+solve_LSAP">solve_LSAP</a></code> available in the package <span class="pkg">clue</span> (Hornik, 2012).  Note that Hungarian algorithm is more efficient and requires less processing time.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function implements some mixed methods to perform statistical matching. A mixed method consists of two steps: 
</p>
<p>(1) adoption of a parametric model for the joint distribution of <code class="reqn"> \left( \mathbf{X},Y,Z \right) </code> and estimation of its parameters;
</p>
<p>(2) derivation of a complete &ldquo;synthetic&rdquo; data set (recipient data set filled in with values for the Z variable) using a nonparametric approach.
</p>
<p>In this case, as far as (1) is concerned, it is assumed that  <code class="reqn"> \left( \mathbf{X},Y,Z \right) </code> follows a multivariate normal distribution.  Please note that if some of the <b>X</b> are categorical, then they are recoded into dummies before starting with the estimation.  In such a case, the assumption of multivariate normal distribution may be questionable.  
</p>
<p>The whole procedure is based on the imputation method known as <em>predictive mean matching</em>.  The procedure consists of three steps: 
</p>
<p><b>step 1a)</b> <em>Regression step</em>: the two linear regression models Y vs. <b>X</b> and Z vs. <b>X</b> are considered and their parameters are estimated. 
</p>
<p><b>step 1b)</b> <em>Computation of intermediate values</em>.  For the units in <code>data.rec</code> the following intermediate values are derived:
</p>
<p style="text-align: center;"><code class="reqn"> \tilde{z}_{a} = \hat{\alpha}_{Z} + \hat{\beta}_{Z\bf{X}} \mathbf{x}_a + e_a </code>
</p>

<p>for each <code class="reqn">a=1,\ldots,n_{A}</code>, being <code class="reqn">n_A</code> the number of units in <code>data.rec</code> (rows of <code>data.rec</code>).  Note that, <code class="reqn">e_a</code> is a random draw from the multivariate normal distribution with zero mean and estimated residual variance  <code class="reqn">\hat{\sigma}_{Z|\bf{X}}</code>.
</p>
<p>Similarly, for the units in <code>data.don</code> the following intermediate values are derived:
</p>
<p style="text-align: center;"><code class="reqn"> \tilde{y}_{b} = \hat{\alpha}_{Y} + \hat{\beta}_{Y\bf{X}} \mathbf{x}_b + e_b </code>
</p>

<p>for each <code class="reqn">b=1,\ldots,n_{B}</code>, being <code class="reqn">n_B</code> the number of units in <code>data.don</code> (rows of <code>data.don</code>). <code class="reqn">e_b</code> is a random draw from the multivariate normal distribution with zero mean and estimated residual variance <code class="reqn">\hat{\sigma}_{Y|\bf{X}}</code>.
</p>
<p><b>step 2)</b> <em>Matching step</em>.  For each observation (row) in <code>data.rec</code> a donor is chosen in <code>data.don</code> through a nearest neighbor constrained distance hot deck procedure.  The distances are computed between <code class="reqn">\left( y_a, \tilde{z}_a \right)</code> and <code class="reqn">\left( \tilde{y}_b, z_b \right)</code> using Mahalanobis distance.
</p>
<p>For further details see Sections 2.5.1 and 3.6.1 in D'Orazio <em>et al.</em> (2006).
</p>
<p>In step 1a) the parameters of the regression model can be estimated by means of the Maximum Likelihood method (<code>method="ML"</code>) (see D'Orazio <em>et al.</em>, 2006, pp. 19&ndash;23,73&ndash;75) or, using the Moriarity and Scheuren (2001 and 2003) approach (<code>method="MS"</code>) (see also D'Orazio <em>et al.</em>, 2006, pp. 75&ndash;76).  The two estimation methods are compared in D'Orazio <em>et al.</em> (2005). 
</p>
<p>When <code>method="MS"</code>, if the value specified for the argument <code>rho.yz</code> is not compatible with the other correlation coefficients estimated from the data, then it is substituted with the closest value compatible with the other estimated coefficients.
</p>
<p>When <code>micro=FALSE</code> only the estimation of the parameters is performed (step 1a).  Otherwise, <br /> 
(<code>micro=TRUE</code>) the whole procedure is carried out.
</p>


<h3>Value</h3>

<p>A list with a varying number of components depending on the values of the arguments 
<code>method</code> and <code>rho.yz</code>. 
</p>
<table role = "presentation">
<tr><td><code>mu</code></td>
<td>

<p>The estimated mean vector. 
</p>
</td></tr>
<tr><td><code>vc</code></td>
<td>

<p>The estimated variance&ndash;covariance matrix. 
</p>
</td></tr>
<tr><td><code>cor</code></td>
<td>

<p>The estimated correlation matrix. 
</p>
</td></tr>
<tr><td><code>res.var</code></td>
<td>

<p>A vector with estimates of the residual variances <code class="reqn">\sigma_{Y|Z\bf{X}}</code> and <code class="reqn">\sigma_{Z|Y\bf{X}}</code>. 
</p>
</td></tr>
<tr><td><code>start.prho.yz</code></td>
<td>

<p>It is the initial guess for the partial correlation coefficient <code class="reqn">\rho_{YZ|\bf{X}}</code> passed in input via the <code>rho.yz</code> argument when <code>method="ML"</code>.
</p>
</td></tr>
<tr><td><code>rho.yz</code></td>
<td>

<p>Returned in output only when <code>method="MS"</code>. It is a vector with four values: the initial guess for <code class="reqn">\rho_{YZ}</code>; the lower and upper bounds for <code class="reqn">\hat{\rho}_{YZ}</code> in the statistical matching framework given the correlation coefficients between Y and <b>X</b> and the correlation coefficients between Z and <b>X</b> estimated from the available data; and, finally, the closest admissible value used in computations instead of the initial <code>rho.yz</code> that resulted not coherent with the others correlation coefficients estimated from the available data.
</p>
</td></tr>
<tr><td><code>phi</code></td>
<td>

<p>When <code>method="MS"</code>. Estimates of the <code class="reqn">\phi</code> terms introduced by Moriarity and Scheuren (2001 and 2003). 
</p>
</td></tr>
<tr><td><code>filled.rec</code></td>
<td>

<p>The <code>data.rec</code> filled in with the values of Z. It is returned only when <br /> <code>micro=TRUE</code>.  
</p>
</td></tr>
<tr><td><code>mtc.ids</code></td>
<td>

<p>when <code>micro=TRUE</code>. This is a matrix with the same number of rows of <code>data.rec</code> and two columns. The first column contains the row names of the <code>data.rec</code> and the second column contains the row names of the corresponding donors selected from the <code>data.don</code>. When the input matrices do not contain row names, a numeric matrix with the indexes of the rows is provided.
</p>
</td></tr>
<tr><td><code>dist.rd</code></td>
<td>

<p>A vector with the distances between each recipient unit and the corresponding donor, returned only in case <code>micro=TRUE</code>.
</p>
</td></tr>
<tr><td><code>call</code></td>
<td>

<p>How the function has been called.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Marcello D'Orazio <a href="mailto:mdo.statmatch@gmail.com">mdo.statmatch@gmail.com</a> 
</p>


<h3>References</h3>

<p>D'Orazio, M., Di Zio, M. and Scanu, M. (2005). &ldquo;A comparison among different estimators of regression parameters on statistically matched files through an extensive simulation study&rdquo;, <em>Contributi</em>, <b>2005/10</b>, Istituto Nazionale di Statistica, Rome.
</p>
<p>D'Orazio, M., Di Zio, M. and Scanu, M. (2006). <em>Statistical Matching: Theory and Practice.</em> Wiley, Chichester.
</p>
<p>Hornik K. (2012).  clue: Cluster ensembles.  R package version 0.3-45.  <a href="https://CRAN.R-project.org/package=clue">https://CRAN.R-project.org/package=clue</a>.
</p>
<p>Moriarity, C., and Scheuren, F. (2001). &ldquo;Statistical matching: a paradigm for assessing the uncertainty in the procedure&rdquo;. <em>Journal of Official Statistics</em>, <b>17</b>, 407&ndash;422.
</p>
<p>Moriarity, C., and Scheuren, F. (2003). &ldquo;A note on Rubin's statistical matching using file concatenation with adjusted weights and multiple imputation&rdquo;, <em>Journal of Business and Economic Statistics</em>, <b>21</b>, 65&ndash;73.
</p>


<h3>See Also</h3>

 
<p><code><a href="#topic+NND.hotdeck">NND.hotdeck</a></code>, <code><a href="#topic+mahalanobis.dist">mahalanobis.dist</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# reproduce the statistical matching framework
# starting from the iris data.frame
suppressWarnings(RNGversion("3.5.0"))
set.seed(98765)
pos &lt;- sample(1:150, 50, replace=FALSE)
ir.A &lt;- iris[pos,c(1,3:5)]
ir.B &lt;- iris[-pos, 2:5]

xx &lt;- intersect(colnames(ir.A), colnames(ir.B))
xx  # common variables

# ML estimation method under CIA ((rho_YZ|X=0));
# only parameter estimates (micro=FALSE)
# only continuous matching variables
xx.mtc &lt;- c("Petal.Length", "Petal.Width")
mtc.1 &lt;- mixed.mtc(data.rec=ir.A, data.don=ir.B, match.vars=xx.mtc,
                    y.rec="Sepal.Length", z.don="Sepal.Width")

# estimated correlation matrix
mtc.1$cor 

# ML estimation method under CIA ((rho_YZ|X=0));
# only parameter estimates (micro=FALSE)
# categorical variable 'Species' used as matching variable

xx.mtc &lt;- xx
mtc.2 &lt;- mixed.mtc(data.rec=ir.A, data.don=ir.B, match.vars=xx.mtc,
                    y.rec="Sepal.Length", z.don="Sepal.Width")

# estimated correlation matrix
mtc.2$cor 


# ML estimation method with partial correlation coefficient
# set equal to 0.5 (rho_YZ|X=0.5)
# only parameter estimates (micro=FALSE)

mtc.3 &lt;- mixed.mtc(data.rec=ir.A, data.don=ir.B, match.vars=xx.mtc,
                    y.rec="Sepal.Length", z.don="Sepal.Width",
                    rho.yz=0.5)

# estimated correlation matrix
mtc.3$cor 

# ML estimation method with partial correlation coefficient
# set equal to 0.5 (rho_YZ|X=0.5)
# with imputation step (micro=TRUE)

mtc.4 &lt;- mixed.mtc(data.rec=ir.A, data.don=ir.B, match.vars=xx.mtc,
                    y.rec="Sepal.Length", z.don="Sepal.Width",
                    rho.yz=0.5, micro=TRUE, constr.alg="Hungarian")

# first rows of data.rec filled in with z
head(mtc.4$filled.rec)

#
# Moriarity and Scheuren estimation method under CIA;
# only with parameter estimates (micro=FALSE)
mtc.5 &lt;- mixed.mtc(data.rec=ir.A, data.don=ir.B, match.vars=xx.mtc,
                    y.rec="Sepal.Length", z.don="Sepal.Width",
                    method="MS")

# the starting value of rho.yz and the value used
# in computations
mtc.5$rho.yz

# estimated correlation matrix
mtc.5$cor 

# Moriarity and Scheuren estimation method
# with correlation coefficient set equal to -0.15 (rho_YZ=-0.15)
# with imputation step (micro=TRUE)

mtc.6 &lt;- mixed.mtc(data.rec=ir.A, data.don=ir.B, match.vars=xx.mtc,
                    y.rec="Sepal.Length", z.don="Sepal.Width",
                    method="MS", rho.yz=-0.15, 
                    micro=TRUE, constr.alg="lpSolve")

# the starting value of rho.yz and the value used
# in computations
mtc.6$rho.yz

# estimated correlation matrix
mtc.6$cor

# first rows of data.rec filled in with z imputed values
head(mtc.6$filled.rec)

</code></pre>

<hr>
<h2 id='NND.hotdeck'>Distance Hot Deck method.</h2><span id='topic+NND.hotdeck'></span>

<h3>Description</h3>

<p>This function implements the distance hot deck method to match the records of two data sources that share some variables. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>NND.hotdeck(data.rec, data.don, match.vars, 
             don.class=NULL, dist.fun="Manhattan",
             constrained=FALSE, constr.alg="Hungarian", 
             k=1, keep.t=FALSE, ...) 
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="NND.hotdeck_+3A_data.rec">data.rec</code></td>
<td>

<p>A matrix or data frame that plays the role of <em>recipient</em>.  This data frame must contain the variables (columns) that should be used, directly or indirectly, in the matching application (specified via <code>match.vars</code> and eventually <code>don.class</code>). 
</p>
<p>Missing values (<code>NA</code>) are allowed.
</p>
</td></tr>
<tr><td><code id="NND.hotdeck_+3A_data.don">data.don</code></td>
<td>

<p>A matrix or data frame that plays the role of <em>donor</em>.  The variables (columns) involved, directly or indirectly, in the computation of distance must be the same and of the same type as those in <code>data.rec</code> (specified via <code>match.vars</code> and eventually <code>don.class</code>).   
</p>
</td></tr>
<tr><td><code id="NND.hotdeck_+3A_match.vars">match.vars</code></td>
<td>

<p>A character vector with the names of the matching variables (the columns in both the data frames) that have to be used to compute distances between records (rows) in <code>data.rec</code> and those in <code>data.don</code>.
The variables used in computing distances may contain missing values but only a limited number of distance functions can handle them (see Details for clarifications). 
</p>
</td></tr>
<tr><td><code id="NND.hotdeck_+3A_don.class">don.class</code></td>
<td>

<p>A character vector with the names of the variables (columns in both the data frames) that have to be used to identify the donation classes.  In this case the computation of distances is limited to those units of <code>data.rec</code> and <code>data.doc</code> that belong to the same donation class.  The case of empty donation classes should be avoided.  It would be preferable that variables used to form donation classes are defined as <code>factor</code>.
</p>
<p>The variables chosen for the creation of the donation classes should NOT contain missing values (NAs).
</p>
<p>When not specified (default), no donation classes are used.  This choice may require more memory to store a larger distance matrix and a higher computational effort.
</p>
</td></tr>
<tr><td><code id="NND.hotdeck_+3A_dist.fun">dist.fun</code></td>
<td>

<p>A string with the name of the distance function that has to be used.  The following distances are allowed: &ldquo;Manhattan&rdquo; (aka &ldquo;City block&rdquo;; default), &ldquo;Euclidean&rdquo;, &ldquo;Mahalanobis&rdquo;,&ldquo;exact&rdquo; or &ldquo;exact matching&rdquo;, &ldquo;Gower&rdquo;, &ldquo;minimax&rdquo; or one of the distance functions available in the package <span class="pkg">proxy</span>.  Note that the distance is computed using the function  <code><a href="proxy.html#topic+dist">dist</a></code> of the package <span class="pkg">proxy</span> with the exception of the &ldquo;Gower&rdquo; (see function <code><a href="#topic+gower.dist">gower.dist</a></code> for details), &ldquo;Mahalanobis&rdquo; (function <code><a href="#topic+mahalanobis.dist">mahalanobis.dist</a></code>) and &ldquo;minimax&rdquo; (see <code><a href="#topic+maximum.dist">maximum.dist</a></code>) cases.
</p>
<p>When <code>dist.fun="Manhattan"</code>, <code>"Euclidean"</code>, <code>"Mahalanobis"</code> or <code>"minimax"</code> all the matching variables in <code>data.rec</code> and <code>data.don</code> must be numeric.  When <code>dist.fun="exact"</code> or <code>dist.fun="exact matching"</code>, all the variables in <code>data.rec</code> and <code>data.don</code> will be converted to character and, as far as the distance computation is concerned, they will be treated as categorical nominal variables, i.e. distance is 0 if a couple of units presents the same response category and 1 otherwise. 
</p>
</td></tr>
<tr><td><code id="NND.hotdeck_+3A_constrained">constrained</code></td>
<td>

<p>Logical.  When <code>constrained=FALSE</code> (default) each record in <code>data.don</code> can be used as a donor more than once.  On the contrary, when <br /> <code>constrained=TRUE</code> each record in <code>data.don</code> can be used as a donor only  <code>k</code> times. In this case, the set of donors is selected by solving an optimization problem, whose goal is to minimize the overall matching distance.  See description of the argument <code>constr.alg</code> for details.
</p>
</td></tr>
<tr><td><code id="NND.hotdeck_+3A_constr.alg">constr.alg</code></td>
<td>

<p>A string that has to be specified when <code>constrained=TRUE</code>.  Two choices are available: &ldquo;lpSolve&rdquo; and &ldquo;hungarian&rdquo;.  In the first case, <code>constr.alg="lpSolve"</code>, the optimization problem is solved by means of the function <code><a href="lpSolve.html#topic+lp.transport">lp.transport</a></code> available in the package <span class="pkg">lpSolve</span>.  When <code>constr.alg="hungarian"</code> (default) the problem is solved using the Hungarian method, implemented in function <code><a href="clue.html#topic+solve_LSAP">solve_LSAP</a></code> available in the package <span class="pkg">clue</span>.  Note that Hungarian algorithm is faster and more efficient if compared to <code>constr.alg="lpSolve"</code>  but it allows selecting a donor just once, i.e.  <code>k = 1</code> .
</p>
</td></tr>
<tr><td><code id="NND.hotdeck_+3A_k">k</code></td>
<td>

<p>The number of times that a unit in <code>data.don</code> can be selected as a donor when  <code>constrained=TRUE</code>  (default <code>k = 1</code> ). When <code>k&gt;1</code> then optimization problem can be solved by setting <code>constr.alg="lpSolve"</code>. Hungarian algorithm <br /> (<code>constr.alg="hungarian"</code>) can be used only when  <code>k = 1</code>. 
</p>
</td></tr>
<tr><td><code id="NND.hotdeck_+3A_keep.t">keep.t</code></td>
<td>

<p>Logical, when donation classes are used by setting  <code>keep.t=TRUE</code> prints information on the donation classes being processed (by default <code>keep.t=FALSE</code>).
</p>
</td></tr>
<tr><td><code id="NND.hotdeck_+3A_...">...</code></td>
<td>
 
<p>Additional arguments that may be required by <code><a href="#topic+gower.dist">gower.dist</a></code>, <br /> 
<code><a href="#topic+maximum.dist">maximum.dist</a></code>, or <code><a href="proxy.html#topic+dist">dist</a></code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function finds a donor record in <code>data.don</code> for each record in <code>data.rec</code>.  In the unconstrained case, it searches for the closest donor record according to the chosen distance function.  When for a given recipient record there are more donors available at the minimum distance, one of them is picked at random.
</p>
<p>In the constrained case a donor can be used just a fixed number of times, as specified by the <code>k</code> argument, but the whole set of donors is chosen in order to minimize the overall matching distance.  When <code>k=1</code>  the number of units (rows) in the donor data set has to be larger or equal to the number of units of the recipient data set; when the donation classes are used, this condition must be satisfied in each donation class.  For further details on nearest neighbor distance hot deck refer to Chapter 2 in D'Orazio <em>et al.</em> (2006).
</p>
<p>This function can also be used to impute missing values in a data set using the nearest neighbor distance hot deck.  In this case <code>data.rec</code> is the part of the initial data set that contains missing values on the target variable; on the contrary, <code>data.don</code> is the part of the data set without missing values on it.  See <span class="rlang"><b>R</b></span> code in the Examples for details.
</p>
<p>Please note that only  &ldquo;Gower&rdquo; and &ldquo;minimax&rdquo; distance functions allow for the presence of missing values (<code>NA</code>s) in the variables used in computing distances.  In both the cases when one of the of the observations presents a variable showing an NA, then this variable is excluded from the computation of distance between them. 
</p>


<h3>Value</h3>

<p>A <span class="rlang"><b>R</b></span> list with the following components:
</p>
<table role = "presentation">
<tr><td><code>mtc.ids</code></td>
<td>

<p>A matrix with the same number of rows of <code>data.rec</code> and two columns.  The first column contains the row names of the <code>data.rec</code> and the second column contains the row names of the corresponding donors selected from the <code>data.don</code>.  When the input matrices do not contain row names, a numeric matrix with the indexes of the rows is provided.
</p>
</td></tr>
<tr><td><code>dist.rd</code></td>
<td>

<p>A vector with the distances between each recipient unit and the corresponding donor. 
</p>
</td></tr>
<tr><td><code>noad</code></td>
<td>

<p>When <code>constrained=FALSE</code>, it reports the number of available donors at the minimum distance for each recipient unit. 
</p>
</td></tr>
<tr><td><code>call</code></td>
<td>

<p>How the function has been called.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Marcello D'Orazio <a href="mailto:mdo.statmatch@gmail.com">mdo.statmatch@gmail.com</a> 
</p>


<h3>References</h3>

<p>D'Orazio, M., Di Zio, M. and Scanu, M. (2006). <em>Statistical Matching: Theory and Practice.</em> Wiley, Chichester.
</p>
<p>Hornik K. (2012).  clue: Cluster ensembles.  R package version 0.3-45.  <a href="https://CRAN.R-project.org/package=clue">https://CRAN.R-project.org/package=clue</a>.
</p>
<p>Rodgers, W.L. (1984). &ldquo;An evaluation of statistical matching&rdquo;. <em>Journal of Business and Economic Statistics</em>, <b>2</b>, 91&ndash;102.
</p>
<p>Singh, A.C., Mantel, H., Kinack, M. and Rowe, G. (1993). &ldquo;Statistical matching: use of auxiliary information as an alternative to the conditional independence assumption&rdquo;. <em>Survey Methodology</em>, <b>19</b>, 59&ndash;79.
</p>


<h3>See Also</h3>

 
<p><code><a href="#topic+RANDwNND.hotdeck">RANDwNND.hotdeck</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

# create the classical matching framework
lab &lt;- c(1:15, 51:65, 101:115)
iris.rec &lt;- iris[lab, c(1:3,5)]  # recipient data.frame 
iris.don &lt;- iris[-lab, c(1:2,4:5)] #donor data.frame

# Now iris.rec and iris.don have the variables
# "Sepal.Length", "Sepal.Width"  and "Species"
# in common.
#  "Petal.Length" is available only in iris.rec
#  "Petal.Width"  is available only in iris.don

# Find the closest donors donors computing distance
# on "Sepal.Length" and "Sepal.Width"
# unconstrained case, Euclidean distance

out.NND.1 &lt;- NND.hotdeck(data.rec=iris.rec, data.don=iris.don,
                         match.vars=c("Sepal.Length", "Sepal.Width") )

# create the synthetic data.set:
# fill in "Petal.Width" in iris.rec

fused.1 &lt;- create.fused(data.rec=iris.rec, data.don=iris.don, 
                        mtc.ids=out.NND.1$mtc.ids, z.vars="Petal.Width") 
head(fused.1)

# Find the closest donors computing distance
# on "Sepal.Length", "Sepal.Width" and Species;
# unconstrained case, Gower's distance

out.NND.2 &lt;- NND.hotdeck(data.rec=iris.rec, data.don=iris.don,
                         match.vars=c("Sepal.Length", "Sepal.Width", "Species"), 
                         dist.fun="Gower")


# find the closest donors using "Species" to form donation classes
# and "Sepal.Length" and "Sepal.Width" to compute distance;
# unconstrained case.

out.NND.3 &lt;- NND.hotdeck(data.rec=iris.rec, data.don=iris.don,
                         match.vars=c("Sepal.Length", "Sepal.Width"),
                         don.class="Species")


# find the donors using "Species" to form donation classes
# and "Sepal.Length" and "Sepal.Width" to compute distance;
# constrained case, "Hungarian" algorithm

library(clue)
out.NND.4 &lt;- NND.hotdeck(data.rec=iris.rec, data.don=iris.don,
                         match.vars=c("Sepal.Length", "Sepal.Width"),
                         don.class="Species", constrained=TRUE, 
                         constr.alg="Hungarian")

# Example of Imputation of missing values.
# Introducing missing values in iris
ir.mat &lt;- iris
miss &lt;- rbinom(nrow(iris), 1, 0.3)
ir.mat[miss==1,"Sepal.Length"] &lt;- NA
iris.rec &lt;- ir.mat[miss==1,-1]
iris.don &lt;- ir.mat[miss==0,]

#search for NND donors
imp.NND &lt;- NND.hotdeck(data.rec=iris.rec, data.don=iris.don,
                       match.vars=c("Sepal.Width","Petal.Length", "Petal.Width"),
                       don.class="Species")

# imputing missing values
iris.rec.imp &lt;- create.fused(data.rec=iris.rec, data.don=iris.don, 
                             mtc.ids=imp.NND$mtc.ids, z.vars="Sepal.Length") 

# rebuild the imputed data.frame
final &lt;- rbind(iris.rec.imp, iris.don)
head(final)


</code></pre>

<hr>
<h2 id='pBayes'>Pseudo-Bayes estimates of cell probabilities</h2><span id='topic+pBayes'></span>

<h3>Description</h3>

<p>Estimation of cells counts in contingency tables by means of the pseudo-Bayes estimator.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pBayes(x, method="m.ind", const=NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="pBayes_+3A_x">x</code></td>
<td>

<p>A contingency table with observed cell counts. Typically the output of <code><a href="base.html#topic+table">table</a></code> or <code><a href="stats.html#topic+xtabs">xtabs</a></code>. More in general an R <code>array</code> with the counts.
</p>
</td></tr>
<tr><td><code id="pBayes_+3A_method">method</code></td>
<td>
 
<p>The method for estimating the final cell frequencies. The following options are available:
</p>
<p><code>method = "Jeffreys"</code>, consists in adding 0.5 to each cell before estimation of the relative frequencies. 
</p>
<p><code>method = "minimax"</code>, consists in adding <code class="reqn">\sqrt(n)/c</code> to each cell before estimation of the relative frequencies, being <code class="reqn">n</code> the sum of all the counts and <code class="reqn">c</code> the number of cells in the table.
</p>
<p><code>method = "invcat"</code>, consists in adding <code class="reqn">1/c</code> to each cell before estimation of the relative frequencies.
</p>
<p><code>method = "user"</code>, consists in adding a used defined constant <code class="reqn">a</code> (<code class="reqn">a&gt;0</code>) to each cell before estimation of the relative frequencies. The constant <code class="reqn">a</code> should be passed via the argument <code>const</code>.
</p>
<p><code>method = "m.ind"</code>, the prior guess for the unknown cell probabilities is obtained by considering estimated probabilities under the mutual independence hypothesis. This option is available when dealing with at least two-way contingency tables <br /> (<code>length(dim(x))&gt;=2</code>).
</p>
<p><code>method = "h.assoc"</code>, the prior guess for the unknown cell probabilities is obtained by considering estimated probabilities under the homogeneous association hypothesis. This option is available when dealing with at least two-way contingency tables (<code>length(dim(x))&gt;=2</code>).
</p>
</td></tr>
<tr><td><code id="pBayes_+3A_const">const</code></td>
<td>

<p>Numeric value, a user defined constant <code class="reqn">a</code> (<code class="reqn">a&gt;0</code>) to be added to each cell before estimation of the relative frequencies when <code>method = "user"</code>. As a general rule of thumb, it is preferable to avoid that the sum of constant over all the cells is greater than <code class="reqn">0.20 \times n</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function estimates the frequencies in a contingency table by using the pseudo-Bayes approach. In practice the estimator being considered is a weighted average of the input (observed) cells counts <code class="reqn">n_h</code> and a suitable prior guess, <code class="reqn">\gamma_h</code>, for cells probabilities :
</p>
<p style="text-align: center;"><code class="reqn">\tilde{p}_h = \frac{n}{n+K} \hat{p}_h + \frac{K}{n+K} \gamma_h </code>
</p>

<p><code class="reqn">K</code> depends on the parameters of Dirichlet prior distribution being considered (for major details see Chapter 12 in Bishop et al., 1974).
It is worth noting that with a constant prior guess <code class="reqn">\gamma_h=1/c</code> (<code class="reqn">h=1,2,\cdots, c</code>), then <code class="reqn">K=1</code> and in practice corresponds to adding <code class="reqn">1/c</code> to each cell before estimation of the relative frequencies (<code>method = "invcat"</code>); <code class="reqn">K=c/2</code> when the constant 0.5 is added to each cell (<code>method = "Jeffreys"</code>); finally <code class="reqn">K=\sqrt{n}</code> when the quantity <code class="reqn">\sqrt{n}/c</code> is added to each cell (<code>method = "minimax"</code>). All these cases corresponds to adding a flattening constant; the higher is the value of <code class="reqn">K</code> the more the estimates will be shrinked towards <code class="reqn">\gamma_h=1/c</code> (flattening).
</p>
<p>When <code>method = "m.ind"</code> the prior guess <code class="reqn">\gamma_h</code> is estimated under the hypothesis of mutual independence between the variables crossed in the initial contingency table <code>x</code>, supposed to be at least a two-way table.  In this case the value of <code class="reqn">K</code> is estimated via a data-driven approach by considering
</p>
<p style="text-align: center;"><code class="reqn"> \hat{K} = \frac{1 - \sum_{h} \hat{p}_h^2}{\sum_{h} \left( \hat{\gamma}_h - \hat{p}_h \right)^2 } </code>
</p>

<p>On the contrary, when <code>method = "h.assoc"</code> the prior guess <code class="reqn">\gamma_h</code> is estimated under the hypothesis of homogeneous association between the variables crossed in the initial contingency table <code>x</code>. 
</p>
<p>Please note that when the input table is estimated from sample data where a weight is assigned to each unit, the weights should be used in estimating the input table, but it is suggested to rescale them so that their sum is equal to <em>n</em>, the sample size.
</p>


<h3>Value</h3>

<p>A <code>list</code> object with three components. 
</p>
<table role = "presentation">
<tr><td><code>info</code></td>
<td>
<p>A vector with the sample size <code>"n"</code>, the number of cells (<code>"no.cells"</code>) in <code>x</code>, the average cell frequency (<code>"av.cfr"</code>), the number of cells showing frequencies equal to zero (<code>"no.0s"</code>), the <code>const</code> input argument, the chosen/estimated <code class="reqn">K</code> (<code>"K"</code>) and the relative size of <code class="reqn">K</code>, i.e. <code class="reqn">K/(n+K)</code> (<code>"rel.K"</code>).</p>
</td></tr>
<tr><td><code>prior</code></td>
<td>
<p> A table having the same dimension as <code>x</code> with the considered prior values for the cell frequencies.</p>
</td></tr>
<tr><td><code>pseudoB</code></td>
<td>
<p> A table with having the same dimension as <code>x</code> providing the pseudo-Bayes estimates for the cell frequencies in <code>x</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Marcello D'Orazio <a href="mailto:mdo.statmatch@gmail.com">mdo.statmatch@gmail.com</a> 
</p>


<h3>References</h3>

<p>Bishop Y.M.M., Fienberg, S.E., Holland, P.W. (1974) <em>Discrete Multivariate Analysis: Theory and Practice.</em> The Massachusetts Institute of Technology
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(samp.A, package="StatMatch")
tab &lt;- xtabs(~ area5 + urb + c.age + sex + edu7, data = samp.A)
out.pb &lt;- pBayes(x=tab, method="m.ind")
out.pb$info

out.pb &lt;- pBayes(x=tab, method="h.assoc")
out.pb$info

out.pb &lt;- pBayes(x=tab, method="Jeffreys")
out.pb$info

# usage of weights in estimating the input table
n &lt;- nrow(samp.A)
r.w &lt;- samp.A$ww / sum(samp.A$ww) * n   # rescale weights to sum up to n 
tab.w &lt;- xtabs(r.w ~ area5 + urb + c.age + sex + edu7, data = samp.A)
out.pbw &lt;- pBayes(x=tab.w, method="m.ind")
out.pbw$info

</code></pre>

<hr>
<h2 id='plotBounds'>Graphical representation of the uncertainty bounds estimated through the <code><a href="#topic+Frechet.bounds.cat">Frechet.bounds.cat</a></code>  function</h2><span id='topic+plotBounds'></span>

<h3>Description</h3>

<p>The function uses the output of the function <code><a href="#topic+Frechet.bounds.cat">Frechet.bounds.cat</a></code> to produce a basic graphical representation of the uncertainty bounds related to the contingency table of Y vs. Z.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotBounds(outFB)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plotBounds_+3A_outfb">outFB</code></td>
<td>

<p>the list provided in output from <code><a href="#topic+Frechet.bounds.cat">Frechet.bounds.cat</a></code>. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function represents graphically the uncertainty bounds estimated by the function <code><a href="#topic+Frechet.bounds.cat">Frechet.bounds.cat</a></code> for each relative frequency in the contingency table of Y vs. Z.
the dotted line indicates the width of the bounds estimated without conditioning on the Xs (the size is reported in parenthesis below the line). The full line indicates the width of the estimated bounds conditional on the Xs (expected conditional Frechet bounds for the relative frequencies in the contingency table of Y vs. Z (size reported below the line, not in the the parenthesis).
Not that when the X are not used it is drawn only the width of the unconditional bounds and the size is shown below the line.
</p>
<p>The figure on the top od the line indicated the estimated relative frequency under the assumption of independence between Y and Z conditional on one or more X variables (Conditional Independence Assumption, CIA; for details see help pages of <code><a href="#topic+Frechet.bounds.cat">Frechet.bounds.cat</a></code>), otherwise it corresponds to the estimated relative frequency under the assumption of independence between Y and Z.
</p>


<h3>Value</h3>

<p>The required graphical representation is drawn using standard <span class="pkg">graphics</span> facilities.
</p>


<h3>Author(s)</h3>

<p>Marcello D'Orazio <a href="mailto:mdo.statmatch@gmail.com">mdo.statmatch@gmail.com</a> 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Frechet.bounds.cat">Frechet.bounds.cat</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# # compute the tables required by Frechet.bounds.cat()
# freq.xA &lt;- xtabs(~sex+c.age, data=samp.A)
# freq.xB &lt;- xtabs(~sex+c.age, data=samp.B)
# freq.xy &lt;- xtabs(~sex+c.age+c.neti, data=samp.A)
# freq.xz &lt;- xtabs(~sex+c.age+labour5, data=samp.B)
# 
# # apply Frechet.bounds.cat()
# bounds.yz &lt;- Frechet.bounds.cat(tab.x=freq.xA+freq.xB, tab.xy=freq.xy,
#                                 tab.xz=freq.xz, print.f="data.frame")
# 
# 
# plot.bounds(bounds.yz)


</code></pre>

<hr>
<h2 id='plotCont'>graphical comparison of the estimated distributions for the same continuous variable.</h2><span id='topic+plotCont'></span>

<h3>Description</h3>

<p>Compares graphically the estimated distributions for the same continuous variable using data coming from two different data sources.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotCont(data.A, data.B, xlab.A, xlab.B=NULL, w.A=NULL, w.B=NULL,
         type="density", ref=FALSE) 
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plotCont_+3A_data.a">data.A</code></td>
<td>

<p>A dataframe or matrix containing the variable of interest <code>xlab.A</code> and eventual associated survey weights <code>w.A</code>. 
</p>
</td></tr>
<tr><td><code id="plotCont_+3A_data.b">data.B</code></td>
<td>

<p>A dataframe or matrix containing the variable of interest <code>xlab.B</code> and eventual associated survey weights <code>w.B</code>. 
</p>
</td></tr>
<tr><td><code id="plotCont_+3A_xlab.a">xlab.A</code></td>
<td>

<p>Character string providing the name of the variable in <code>data.A</code> whose distribution should be represented graphically and compared with that estimated from <code>data.B</code>.
</p>
</td></tr>
<tr><td><code id="plotCont_+3A_xlab.b">xlab.B</code></td>
<td>

<p>Character string providing the name of the variable in <code>data.B</code> whose distribution should be represented graphically and compared with that estimated from <code>data.A</code>. If <code>xlab.B=NULL</code> (default) then it assumed <code>xlab.B=xlab.A</code>.
</p>
</td></tr>
<tr><td><code id="plotCont_+3A_w.a">w.A</code></td>
<td>

<p>Character string providing the name of the optional weighting variable in <code>data.A</code> that, in case, should be used to estimate the distribution of <code>xlab.A</code>
</p>
</td></tr>
<tr><td><code id="plotCont_+3A_w.b">w.B</code></td>
<td>

<p>Character string providing the name of the optional weighting variable in <code>data.B</code> that, in case, should be used to estimate the distribution of <code>xlab.B</code>
</p>
</td></tr>
<tr><td><code id="plotCont_+3A_type">type</code></td>
<td>

<p>A character string indicating the type of graphical representation that should be used to compare the estimated distributions of <code>xlab.A</code> and <code>xlab.B</code>. By default (<code>type="density"</code>) density plots are used. Other possible options are &ldquo;ecdf&rdquo;, &ldquo;qqplot&rdquo;, &ldquo;qqshift&rdquo; and &ldquo;hist&rdquo;. See Details for more information.
</p>
</td></tr>
<tr><td><code id="plotCont_+3A_ref">ref</code></td>
<td>

<p>Logical, indicating whether the distribution estimated from <code>data.B</code> should be considered the reference or not. Default <code>ref=FALSE</code>. when Default <code>ref=TRUE</code> the estimation of the histograms, the density and the empirical cumulative distribution function are guided by data in  <code>data.B</code> 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function graphically compares the distribution of the same variable, but estimated from data from two different data sources. The graphical comparison can be performed in several ways. 
With <code>type="hist"</code>, the continuous variable is categorized and the corresponding histograms estimated from <code>data.A</code> and <code>data.B</code> are compared. If present, the weights are used to estimate the relative frequencies. Note that the breaks to categorize the variable are decided according to the Freedman-Diaconis rule (<code><a href="grDevices.html#topic+nclass">nclass</a></code>), and in this case with <code>ref=TRUE</code> the IQR is estimated from <code>data.B</code> alone, while with <code>ref=FALSE</code> it is estimated by joining the two data sources.
</p>
<p>With <code>type="density"</code> the density plots are drawn; when available the weights are used in the estimation of the density based on the histograms (as suggested by Bellhouse and Stafford, 1999). When<code>type="ecdf"</code> the comparison relies on the empirical cumulative distribution function, that can be estimated considering the weights. Note that when <code>ref=TRUE</code> the estimation of the density and the empirical cumulative distribution are guided by the data in  <code>data.B</code>.
</p>
<p>The comparison is based on percentiles with <code>type="qqplot"</code> and <code>type="qqshift"</code>. In the first case, the function draws a scatterplot (red dots) of the estimated percentiles of <code>xlab.A</code> against those of <code>xlab.B</code>; the dashed line indicates the ideal situation of equality of percentiles (points lying on the line). When <code>type="qqshift"</code>, the scatterplot refers to (percentiles.A - percentiles.B) vs. percentiles.B; in this case, the points lying on the horizontal line passing through 0 indicate equality (difference equal to 0). Note that the number of estimated percentiles depends on the minimum between the two sample sizes. Only quartiles are calculated if min(n.A, n.B)&lt;=50; quintiles are estimated if min(n.A, n.B)&gt;50 and min(n.A, n.B)&lt;=150; deciles are estimated if min(n. A, n.B)&gt;150 and min(n.A, n.B)&lt;=250; finally, quantiles for <code>probs=seq(from = 0.05,to = 0.95,by = 0.05)</code> are estimated when min(n.A, n.B)&gt;250. If survey weights are available (indicated by <code>w.A</code> and/or <code>w.B</code>), they are used to estimate the quantiles by calling the function <code><a href="Hmisc.html#topic+wtd.quantile">wtd.quantile</a></code> in the package <span class="pkg">Hmisc</span>.
</p>


<h3>Value</h3>

<p>The required graphical representation is drawn using the <span class="pkg">ggplot2</span> facilities.
</p>


<h3>Author(s)</h3>

<p>Marcello D'Orazio <a href="mailto:mdo.statmatch@gmail.com">mdo.statmatch@gmail.com</a> 
</p>


<h3>References</h3>

<p>Bellhouse D.R. and J. E. Stafford (1999). &ldquo;Density Estimation from Complex Surveys&rdquo;. <em>Statistica Sinica</em>, <b>9</b>, 407&ndash;424.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+comp.cont">comp.cont</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# plotCont(data.A = samp.A, data.B = samp.B, xlab.A="age")
# plotCont(data.A = samp.A, data.B = samp.B, xlab.A="age", w.A = "ww")

</code></pre>

<hr>
<h2 id='plotTab'>Graphical comparison of the estimated distributions for the same categorical variable.</h2><span id='topic+plotTab'></span>

<h3>Description</h3>

<p>Compares graphically the estimated distributions for the same categorical variable(s) using data coming from two different data sources.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotTab(data.A, data.B, xlab.A, xlab.B=NULL, w.A=NULL, w.B=NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plotTab_+3A_data.a">data.A</code></td>
<td>

<p>A dataframe or matrix containing the variable of interest <code>xlab.A</code> and eventual associated survey weights <code>w.A</code>. 
</p>
</td></tr>
<tr><td><code id="plotTab_+3A_data.b">data.B</code></td>
<td>

<p>A dataframe or matrix containing the variable of interest <code>xlab.B</code> and eventual associated survey weights <code>w.B</code>. 
</p>
</td></tr>
<tr><td><code id="plotTab_+3A_xlab.a">xlab.A</code></td>
<td>

<p>Character string providing the name(s) of one or more variables in <code>data.A</code> whose (joint) distribution should be represented graphically and compared with that estimated from <code>data.B</code>.
</p>
</td></tr>
<tr><td><code id="plotTab_+3A_xlab.b">xlab.B</code></td>
<td>

<p>Character string providing the name(s) of one or more variables in <code>data.A</code> whose (joint) distribution should be represented graphically and compared with that estimated from <code>data.A</code>. If <code>xlab.B=NULL</code> (default) then it assumed <code>xlab.B=xlab.A</code>.
</p>
</td></tr>
<tr><td><code id="plotTab_+3A_w.a">w.A</code></td>
<td>

<p>Character string providing the name of the optional weighting variable in <code>data.A</code> that, in case, should be used to estimate the distribution of <code>xlab.A</code>
</p>
</td></tr>
<tr><td><code id="plotTab_+3A_w.b">w.B</code></td>
<td>

<p>Character string providing the name of the optional weighting variable in <code>data.B</code> that, in case, should be used to estimate the distribution of <code>xlab.B</code>
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function compares graphically the (joint) distribution of the same variables but estimated from data coming from two different data sources. The graphical comparison is done using barcharts.
</p>


<h3>Value</h3>

<p>The required graphical representation is drawn using the <span class="pkg">ggplot2</span> facilities.
</p>


<h3>Author(s)</h3>

<p>Marcello D'Orazio <a href="mailto:mdo.statmatch@gmail.com">mdo.statmatch@gmail.com</a> 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+comp.prop">comp.prop</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# plotTab(data.A = samp.A, data.B = samp.B, xlab.A="edu7", w.A = "ww")
# plotTab(data.A = samp.A, data.B = samp.B, xlab.A=c("urb", "sex"), w.A = "ww", w.B="ww")

</code></pre>

<hr>
<h2 id='pw.assoc'>Pairwise measures between categorical variables</h2><span id='topic+pw.assoc'></span>

<h3>Description</h3>

<p>This function computes some association and <em>Proportional Reduction in Error</em> (PRE) measures between a categorical nominal variable and each of the other available predictors (being also categorical variables).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pw.assoc(formula, data, weights=NULL, out.df=FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="pw.assoc_+3A_formula">formula</code></td>
<td>

<p>A formula of the type <code>y~x1+x2</code> where <code>y</code> denotes the name of the categorical variable (a <code>factor</code> in <span class="pkg">R</span>) which plays the role of the dependent variable, while <code>x1</code> and <code>x2</code> are the name of the predictors (both categorical variables).  Numeric variables are not allowed; eventual numerical variables should be categorized (see function <code><a href="base.html#topic+cut">cut</a></code>) before being passed to <code>pw.assoc</code>.
</p>
</td></tr>
<tr><td><code id="pw.assoc_+3A_data">data</code></td>
<td>
 
<p>The data frame which contains the variables called by <code>formula</code>.
</p>
</td></tr>
<tr><td><code id="pw.assoc_+3A_weights">weights</code></td>
<td>

<p>The name of the variable in <code>data</code> which provides the units' weights.  Weights are used to estimate frequencies (a cell frequency is estimated by summing the weights of the units which present the given characteristic).  Default is <code>NULL</code> (no weights available) and each unit counts 1. When case weight are provided, then they are scales so that their sum equals <em>n</em>, the sample size (assumed to be <code>nrow(data)</code>).
</p>
</td></tr>
<tr><td><code id="pw.assoc_+3A_out.df">out.df</code></td>
<td>

<p>Logical. If <code>NULL</code> measures will be organized in a data frame (a column for each measure).
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function computes some association, PRE measures, AIC and BIC for each couple response-predictor that can be created  starting from argument <code>formula</code>. In particular, a two-way contingency table <code class="reqn">X \times Y</code> is built for each available X variable (X in rows and Y in columns); then the following measures are considered.
</p>
<p>Cramer's <em>V</em>: 
</p>
<p style="text-align: center;"><code class="reqn"> V=\sqrt{\frac{\chi^2}{n \times min\left[I-1,J-1\right]} } </code>
</p>

<p><em>n</em> is the sample size, <em>I</em> is the number of rows (categories of X) and <em>J</em> is the number of columns (categories of Y).  Cramer's <em>V</em> ranges from 0 to 1. 
</p>
<p>Bias-corrected Cramer's <em>V</em> (<code class="reqn">V_c</code>) proposed by Bergsma (2013).
</p>
<p>Mutual information:
</p>
<p style="text-align: center;"><code class="reqn"> I(X;Y) = \sum_{i,j} p_{ij} \, log \left( \frac{p_{ij}}{p_{i+} p_{+j}} \right) </code>
</p>
  
<p>equal to 0 in case of independence but with infinite upper bound, i.e.  <code class="reqn">0 \leq I(X;Y) &lt; \infty</code>. In it <code class="reqn">p_{ij}=n_{ij}/n </code>.
</p>
<p>A normalized version of <code class="reqn">I(X;Y)</code>, ranging from 0 (independence) to 1 and not affected by number of categories (<em>I</em> and <em>J</em>):
</p>
<p style="text-align: center;"><code class="reqn">I(X;Y)^* = \frac{I(X;Y)}{min(H_X, H_Y) } </code>
</p>

<p>being <code class="reqn">H_X</code> and <code class="reqn">H_Y</code> the entropy of the variable X and Y, respectively.
</p>
<p>Goodman-Kruskal <code class="reqn">\lambda(Y|X)</code> (i.e. response conditional on the given predictor):
</p>
<p style="text-align: center;"><code class="reqn"> \lambda(Y|X) = \frac{\sum_{i=1}^I max_{j}(p_{ij}) - max_{j}(p_{+j})}{1-max_{j}(p_{+j})} </code>
</p>

<p>It ranges from 0 to 1, and denotes how much the knowledge of the row variable X (predictor) helps in reducing the prediction error of the values of the column variable Y (response).
</p>
<p>Goodman-Kruskal <code class="reqn">\tau(Y|X)</code>:
</p>
<p style="text-align: center;"><code class="reqn"> \tau(Y|X) = \frac{ \sum_{i=1}^I \sum_{j=1}^J p^2_{ij}/p_{i+} - \sum_{j=1}^J p_{+j}^2}{1 -  \sum_{j=1}^J p_{+j}^2} </code>
</p>

<p>It takes values in the interval [0,1] and has the same PRE meaning of the lambda.
</p>
<p>Theil's uncertainty coefficient:
</p>
<p style="text-align: center;"><code class="reqn"> U(Y|X) = \frac{\sum_{i=1}^I \sum_{j=1}^J p_{ij} log(p_{ij}/p_{i+}) - \sum_{j=1}^J p_{+j} log p_{+j}}{- \sum_{j=1}^J p_{+j} log p_{+j}} </code>
</p>

<p>It takes values in the interval [0,1] and measures the reduction of uncertainty in the column variable Y due to knowing the row variable X. Note that the numerator of U(Y|X) is the mutual information I(X;Y)
</p>
<p>It is worth noting that <code class="reqn">\lambda</code>, <code class="reqn">\tau</code> and <em>U</em> can be viewed as measures of the proportional reduction of the variance of the Y variable when passing from its marginal distribution to its conditional distribution given the predictor X, derived from the general expression (cf. Agresti, 2002, p. 56):
</p>
<p style="text-align: center;"><code class="reqn"> \frac{V(Y) - E[V(Y|X)]}{V(Y)}</code>
</p>

<p>They differ in the way of measuring variance, in fact it does not exist a general accepted definition of the variance for a categorical variable.
</p>
<p>Finally, AIC (and BIC) is calculated, as suggested in Sakamoto and Akaike (1977). In particular:
</p>
<p style="text-align: center;"><code class="reqn"> AIC(Y|X) = -2 \sum_{i,j} n_{ij} \, log \left( \frac{n_{ij}}{n_{i+}} \right) + 2I(J - 1) </code>
</p>

<p style="text-align: center;"><code class="reqn"> BIC(Y|X) = -2 \sum_{i,j} n_{ij} \, log \left( \frac{n_{ij}}{n_{i+}} \right) +I(J-1) log(n)    </code>
</p>

<p>being <code class="reqn">I(J-1)</code> the parameters (conditional probabilities) to estimate. Note that the <span class="rlang"><b>R</b></span> package <span class="pkg">catdap</span> provides functions to identify the best subset of predictors based on AIC.
</p>
<p>Please note that the missing values are excluded from the tables and therefore excluded from the estimation of the various measures.
</p>


<h3>Value</h3>

<p>When <code>out.df=FALSE</code> (default) a <code>list</code> object with four components: 
</p>
<table role = "presentation">
<tr><td><code>V</code></td>
<td>
<p>A vector with the estimated Cramer's V for each couple response-predictor.</p>
</td></tr>
<tr><td><code>bcV</code></td>
<td>
<p>A vector with the estimated bias-corrected Cramer's V for each couple response-predictor.</p>
</td></tr>
<tr><td><code>mi</code></td>
<td>
<p>A vector with the estimated mutual information I(X;Y) for each couple response-predictor.</p>
</td></tr>
<tr><td><code>norm.mi</code></td>
<td>
<p>A vector with the normalized mutual information I(X;Y)* for each couple response-predictor.</p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>
<p> A vector with the values of Goodman-Kruscal <code class="reqn">\lambda(Y|X)</code> for each couple response-predictor.</p>
</td></tr>
<tr><td><code>tau</code></td>
<td>
<p> A vector with the values of Goodman-Kruscal <code class="reqn">\tau(Y|X)</code> for each couple response-predictor.</p>
</td></tr>
<tr><td><code>U</code></td>
<td>
<p> A vector with the values of Theil's uncertainty coefficient U(Y|X) for each couple response-predictor.</p>
</td></tr>
<tr><td><code>AIC</code></td>
<td>
<p> A vector with the values of AIC(Y|X) for each couple response-predictor.</p>
</td></tr>
<tr><td><code>BIC</code></td>
<td>
<p> A vector with the values of BIC(Y|X) for each couple response-predictor.</p>
</td></tr>
<tr><td><code>npar</code></td>
<td>
<p> A vector with the number of parameters (conditional probabilities) estimated to calculate AIC and BIC for each couple response-predictor.</p>
</td></tr>
</table>
<p>When <code>out.df=TRUE</code> the output will be a data.frame with a column for each measure. 
</p>


<h3>Author(s)</h3>

<p>Marcello D'Orazio <a href="mailto:mdo.statmatch@gmail.com">mdo.statmatch@gmail.com</a> 
</p>


<h3>References</h3>

<p>Agresti A (2002) <em>Categorical Data Analysis. Second Edition</em>. Wiley, new York.
</p>
<p>Bergsma W (2013) A bias-correction for Cramer's V and Tschuprow's T. <em>Journal of the Korean Statistical Society</em>, 42, 323&ndash;328.
</p>
<p>The Institute of Statistical Mathematics (2018). catdap: Categorical Data  Analysis Program Package. R package version 1.3.4. <a href="https://CRAN.R-project.org/package=catdap">https://CRAN.R-project.org/package=catdap</a>
</p>
<p>Sakamoto Y and Akaike, H (1977) Analysis of Cross-Classified Data by AIC. <em>Ann. Inst. Statist. Math.</em>, 30, 185-197.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(quine, package="MASS") #loads quine from MASS
str(quine)

# how Lrn is response variable
pw.assoc(Lrn~Age+Sex+Eth, data=quine)

# usage of units' weights
quine$ww &lt;- runif(nrow(quine), 1,4) #random gen  1&lt;=weights&lt;=4
pw.assoc(Lrn~Age+Sex+Eth, data=quine, weights="ww")

</code></pre>

<hr>
<h2 id='RANDwNND.hotdeck'>Random Distance hot deck.</h2><span id='topic+RANDwNND.hotdeck'></span>

<h3>Description</h3>

<p>This function implements a variant of the distance hot deck method.  For each recipient record a subset of of the closest donors is retained and then a donor is selected at random.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>RANDwNND.hotdeck(data.rec, data.don, match.vars=NULL, 
                 don.class=NULL, dist.fun="Manhattan", 
                 cut.don="rot", k=NULL, weight.don=NULL, 
                 keep.t=FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="RANDwNND.hotdeck_+3A_data.rec">data.rec</code></td>
<td>

<p>A numeric matrix or data frame that plays the role of <em>recipient</em>.  This data frame must contain the variables (columns), specified via <code>match.vars</code> and <code>don.class</code>, that should be used in the matching. 
</p>
<p>Missing values (<code>NA</code>) are allowed.
</p>
</td></tr>
<tr><td><code id="RANDwNND.hotdeck_+3A_data.don">data.don</code></td>
<td>

<p>A matrix or data frame that plays the role of <em>donor</em>.  This data frame must contain the variables (columns), specified via <code>match.vars</code> and <code>don.class</code>, that should be used in the matching. 
</p>
</td></tr>
<tr><td><code id="RANDwNND.hotdeck_+3A_match.vars">match.vars</code></td>
<td>

<p>A character vector with the names of the variables (the columns in both the data frames) that have to be used to compute distances between records (rows) in <code>data.rec</code> and those in <code>data.don</code>.  When no matching variables are considered (<code>match.vars=NULL</code>) then all the units in the same donation class are considered as possible donors.  Hence one of them is selected at random or with probability proportional to its weight (see argument <code>weight.don</code>).  When <code>match.vars=NULL</code> and the donation classes are not created <br />
(<code>don.class=NULL</code>) then all the available records in the <code>data.don</code> are considered as potential donors.
</p>
</td></tr>
<tr><td><code id="RANDwNND.hotdeck_+3A_don.class">don.class</code></td>
<td>

<p>A character vector with the names of the variables (columns in both the data frames) that have to be used to identify donation classes.  In this case the computation of distances is limited to those units in <code>data.rec</code> and <code>data.doc</code> that belong to the same donation class.  The case of empty donation classes should be avoided.  It would be preferable that variables used to form donation classes are defined as <code>factor</code>.
</p>
<p>When not specified (default), no donation classes are used.  This may result in a heavy computational effort.
</p>
</td></tr>
<tr><td><code id="RANDwNND.hotdeck_+3A_dist.fun">dist.fun</code></td>
<td>

<p>A string with the name of the distance function that has to be used.  The following distances can be used: &ldquo;Manhattan&rdquo; (aka &ldquo;City block&rdquo;; default), &ldquo;Euclidean&rdquo;, &ldquo;Mahalanobis&rdquo;,&ldquo;exact&rdquo; or &ldquo;exact matching&rdquo;, &ldquo;Gower&rdquo;, &ldquo;minimax&rdquo;, &ldquo;difference&rdquo;, or one of the distance functions available in the package <span class="pkg">proxy</span>.  Note that the distances are computed using the function  <code><a href="proxy.html#topic+dist">dist</a></code> of the package <span class="pkg">proxy</span> with the exception of the &ldquo;Gower&rdquo; (see function <code><a href="#topic+gower.dist">gower.dist</a></code> for details), &ldquo;Mahalanobis&rdquo; (function <code><a href="#topic+mahalanobis.dist">mahalanobis.dist</a></code>), &ldquo;minimax&rdquo; (see <code><a href="#topic+maximum.dist">maximum.dist</a></code>) &ldquo;difference&rdquo; case.  Note that <code>dist.fun="difference"</code> computes just the difference between the values of the unique numeric matching variable considered; in practice, it should be used when the subset of the donation classes should be formed by comparing the values of the unique matching variable (for further details see the argument <code>cut.don</code>.
</p>
<p>By setting <code>dist.fun="ANN"</code> or <code>dist.fun="RANN"</code> it is possible to search for the <code>k</code> nearest neighbours for each recipient record by using the the Approximate Nearest Neighbor (ANN) search as implemented in the  function <code><a href="RANN.html#topic+nn2">nn2</a></code> provided by the package <span class="pkg">RANN</span>.
</p>
<p>When <code>dist.fun="Manhattan"</code>, <code>"Euclidean"</code>, <code>"Mahalanobis"</code> or <code>"minimax"</code> all the variables in <code>data.rec</code> and <code>data.don</code> must be numeric.  On the contrary, when <code>dist.fun="exact"</code> or <br />
<code>dist.fun="exact matching"</code>, all the variables in <code>data.rec</code> and <code>data.don</code> will be converted to character and, as far as the distance computation is concerned, they will be treated as categorical nominal variables, i.e. distance is 0 if a couple of units shows the same response category and 1 otherwise. 
</p>
</td></tr>
<tr><td><code id="RANDwNND.hotdeck_+3A_cut.don">cut.don</code></td>
<td>

<p>A character string that, jointly with the argument <code>k</code>, identifies the rule to be used to form the subset of the closest donor records. 
</p>

<ul>
<li> <p><code>cut.don="rot"</code>: (default) then the number of the closest donors to retain is given by <code class="reqn"> \left[ \sqrt{n_{D}} \right]+1</code>; being <code class="reqn"> n_{D} </code> the total number of available donors.  In this case <code>k</code> must not to be specified.
</p>
</li>
<li> <p><code>cut.don="span"</code>: the number of closest donors is determined as the proportion <code>k</code> of all the available donors, i.e. <code class="reqn"> \left[ n_{D} \times k \right] </code>.  Note that, in this case, <code class="reqn"> 0&lt; \code{k} \leq 1 </code>.
</p>
</li>
<li> <p><code>cut.don="exact"</code>: the <code>k</code>th closest donors out of the <code class="reqn">n_{D}</code> are retained.  In this case, <code class="reqn"> 0&lt; \code{k} \leq{ n_{D} } </code>.
</p>
</li>
<li> <p><code>cut.don="min"</code>: the donors at the minimum distance from the recipient are retained.
</p>
</li>
<li> <p><code>cut.don="k.dist"</code>: only the donors whose distance from the recipient is less or equal to the value specified with the argument <code>k</code>.  Note that in this case it is not possible to use <code>dist.fun="ANN"</code>.
</p>
</li>
<li> <p><code>cut.don="lt"</code> or <code>cut.don="&lt;"</code>: only the donors whose value of the matching variable is smaller than the value of the recipient are retained. Note that in this case it is has to be set <code>dist.fun="difference"</code>.
</p>
</li>
<li> <p><code>cut.don="le"</code> or <code>cut.don="&lt;="</code>: only the donors whose value of the matching variable is smaller or equal to the value of the recipient are retained.  Note that in this case it is has to be set <code>dist.fun="difference"</code>.
</p>
</li>
<li> <p><code>cut.don="ge"</code> or <code>cut.don="&gt;="</code>: only the donors whose value of the matching variable is greater or equal to the value of the recipient are retained.  Note that in this case it is has to be set <code>dist.fun="difference"</code>.
</p>
</li>
<li> <p><code>cut.don="gt"</code> or <code>cut.don="&gt;"</code>: only the donors whose value of the matching variable is greater than the value of the recipient are retained. Note that in this case it is has to be set <code>dist.fun="difference"</code>.
</p>
</li></ul>

</td></tr>
<tr><td><code id="RANDwNND.hotdeck_+3A_k">k</code></td>
<td>

<p>Depends on the <code>cut.don</code> argument. 
</p>
</td></tr>
<tr><td><code id="RANDwNND.hotdeck_+3A_weight.don">weight.don</code></td>
<td>
 
<p>A character string providing the name of the variable with the weights associated to the  donor units in <code>data.don</code>.  When this variable is specified, then the selection of a donor among those in the subset of the closest donors is done with probability proportional to its weight (units with larger weight will have a higher chance of being selected).  When <code>weight.don=NULL</code> (default) all the units in the subset of the closest donors will have the same probability of being selected.
</p>
</td></tr>
<tr><td><code id="RANDwNND.hotdeck_+3A_keep.t">keep.t</code></td>
<td>

<p>Logical, when donation classes are used by setting  <code>keep.t=TRUE</code> prints information on the donation classes being processed (by default <code>keep.t=FALSE</code>).
</p>
</td></tr>
<tr><td><code id="RANDwNND.hotdeck_+3A_...">...</code></td>
<td>
 
<p>Additional arguments that may be required by <code><a href="#topic+gower.dist">gower.dist</a></code>, by <br />
<code><a href="#topic+maximum.dist">maximum.dist</a></code>, by <code><a href="proxy.html#topic+dist">dist</a></code> or by <code><a href="RANN.html#topic+nn2">nn2</a></code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function finds a donor record for each record in the recipient data set.  The donor is chosen at random in the subset of available donors.  This procedure is known as <em>random hot deck</em> (cf. Andridge and Little, 2010).  In <code>RANDwNND.hotdeck</code>, the number of closest donors retained to form the subset is determined according to criterion specified with the argument <code>cut.don</code>.
The selection of the donor among those in the subset is carried out with equal probability (<code>weight.don=NULL</code>) or with probability proportional to a weight associated to the donors, specified via the <code>weight.don</code> argument.  This procedure is is known as <em>weighted random hot deck</em> (cf. Andridge and Little, 2010).
</p>
<p>The search for the subset of the closest donors can be speed up by using the Approximate Nearest Neighbor search as implemented in the  function <code><a href="RANN.html#topic+nn2">nn2</a></code> provided by the package <span class="pkg">RANN</span>.  Note that this search can be used in all the cases with the exception of <code>cut.don="k.dist"</code>.
</p>
<p>Note that the same donor can be used more than once.
</p>
<p>This function can also be used to impute missing values in a data set.  In this case <code>data.rec</code> is the part of the initial data set that contains missing values; on the contrary, <code>data.don</code> is the part of the data set without missing values. See <span class="rlang"><b>R</b></span> code in the Examples for details.
</p>


<h3>Value</h3>

<p>A <span class="rlang"><b>R</b></span> list with the following components:
</p>
<table role = "presentation">
<tr><td><code>mtc.ids</code></td>
<td>

<p>A matrix with the same number of rows of <code>data.rec</code> and two columns.  The first column contains the row names of the <code>data.rec</code> and the second column contains the row names of the corresponding donors selected from the <code>data.don</code>.  When the input matrices do not contain row names, then a numeric matrix with the indexes of the rows is provided.
</p>
</td></tr>
<tr><td><code>sum.dist</code></td>
<td>

<p>A matrix with summary statistics concerning the subset of the closest donors.  The first three columns report the minimum, the maximum and the standard deviation of the distances among the recipient record and the donors in the subset of the closest donors, respectively.  The 4th column reports the cutting distance, i.e. the value of the distance such that donors at a higher distance are discarded.  The 5th column reports the distance between the recipient and the donor chosen at random in the subset of the donors.
</p>
</td></tr>
<tr><td><code>noad</code></td>
<td>

<p>For each recipient unit, reports the number of donor records in the subset of closest donors. 
</p>
</td></tr>
<tr><td><code>call</code></td>
<td>

<p>How the function has been called.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Marcello D'Orazio <a href="mailto:mdo.statmatch@gmail.com">mdo.statmatch@gmail.com</a> 
</p>


<h3>References</h3>

<p>Andridge, R.R., and Little, R.J.A. (2010) &ldquo;A Review of Hot Deck Imputation for Survey Non-response&rdquo;. <em>International Statistical Review</em>, <b>78</b>, 40&ndash;64. 
</p>
<p>D'Orazio, M., Di Zio, M. and Scanu, M. (2006). <em>Statistical Matching: Theory and Practice.</em> Wiley, Chichester.
</p>
<p>Rodgers, W.L. (1984). &ldquo;An evaluation of statistical matching&rdquo;. <em>Journal of Business and Economic Statistics</em>, <b>2</b>, 91&ndash;102.
</p>
<p>Singh, A.C., Mantel, H., Kinack, M. and Rowe, G. (1993). &ldquo;Statistical matching: use of auxiliary information as an alternative to the conditional independence assumption&rdquo;. <em>Survey Methodology</em>, <b>19</b>, 59&ndash;79.
</p>


<h3>See Also</h3>

 
<p><code><a href="#topic+NND.hotdeck">NND.hotdeck</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(samp.A, samp.B, package="StatMatch") #loads data sets
?samp.A
?samp.B


# samp.A plays the role of recipient
# samp.B plays the role of donor
# find a donor in the in the same region ("area5") and with the same
# gender ("sex"), then only the closest k=20 donors in terms of 
# "age" are cnsidered and one of them is picked up at random

out.RND.1 &lt;- RANDwNND.hotdeck(data.rec=samp.A, data.don=samp.B,
                              don.class=c("area5", "sex"), dist.fun="ANN",
                              match.vars="age", cut.don="exact", k=20)

# create the synthetic (or fused) data.frame:
# fill in "labour5" in A
fused.1 &lt;- create.fused(data.rec=samp.A, data.don=samp.B,
                        mtc.ids=out.RND.1$mtc.ids, z.vars="labour5")
head(fused.1)

# weights ("ww") are used in selecting the donor in the final step

out.RND.2 &lt;- RANDwNND.hotdeck(data.rec=samp.A, data.don=samp.B,
                              don.class=c("area5", "sex"), dist.fun="ANN",
                              match.vars="age", cut.don="exact", 
                              k=20, weight.don="ww")
fused.2 &lt;- create.fused(data.rec=samp.A, data.don=samp.B,
                        mtc.ids=out.RND.2$mtc.ids, z.vars="labour5")
head(fused.2)

# find a donor in the in the same region ("area5") and with the same
# gender ("sex"), then only the donors with "age" &lt;= to the age of the
# recipient are considered,
# then one of them is picked up at random

out.RND.3 &lt;- RANDwNND.hotdeck(data.rec=samp.A, data.don=samp.B,
                              don.class=c("area5", "sex"), dist.fun="diff",
                              match.vars="age", cut.don="&lt;=")

# create the synthetic (or fused) data.frame:
# fill in "labour5" in A
fused.3 &lt;- create.fused(data.rec=samp.A, data.don=samp.B,
                        mtc.ids=out.RND.3$mtc.ids, z.vars="labour5")
head(fused.3)

# Example of Imputation of missing values
# introducing missing vales in iris
ir.mat &lt;- iris
miss &lt;- rbinom(nrow(iris), 1, 0.3)
ir.mat[miss==1,"Sepal.Length"] &lt;- NA
iris.rec &lt;- ir.mat[miss==1,-1]
iris.don &lt;- ir.mat[miss==0,]

#search for NND donors
imp.RND &lt;- RANDwNND.hotdeck(data.rec=iris.rec, data.don=iris.don,
                            match.vars=c("Sepal.Width","Petal.Length", "Petal.Width"),
                            don.class="Species")

# imputing missing values
iris.rec.imp &lt;- create.fused(data.rec=iris.rec, data.don=iris.don,
                             mtc.ids=imp.RND$mtc.ids, z.vars="Sepal.Length")

# rebuild the imputed data.frame
final &lt;- rbind(iris.rec.imp, iris.don)
head(final)

</code></pre>

<hr>
<h2 id='rankNND.hotdeck'> Rank distance hot deck method.</h2><span id='topic+rankNND.hotdeck'></span>

<h3>Description</h3>

<p>This function implements rank  hot deck distance method.  For each recipient record the closest donors is chosen by considering the distance between the percentage points of the empirical cumulative distribution function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rankNND.hotdeck(data.rec, data.don, var.rec, var.don=var.rec, 
                 don.class=NULL,  weight.rec=NULL, weight.don=NULL,
                 constrained=FALSE, constr.alg="Hungarian",
                 keep.t=FALSE) 
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="rankNND.hotdeck_+3A_data.rec">data.rec</code></td>
<td>

<p>A numeric matrix or data frame that plays the role of <em>recipient</em>.  This data frame must contain the variable <code>var.rec</code> to be used in computing the percentage points of the empirical cumulative distribution function and eventually the variables that identify the donation classes (see argument <code>don.class</code>) and the case weights (see argument <code>weight.rec</code>). 
</p>
<p>Missing values (<code>NA</code>) are not allowed.
</p>
</td></tr>
<tr><td><code id="rankNND.hotdeck_+3A_data.don">data.don</code></td>
<td>

<p>A matrix or data frame that plays the role of <em>donor</em>.  This data frame must contain the variable <code>var.don</code> to be used in computing percentage points of the the empirical cumulative distribution function and eventually the variables that identify the donation classes (see argument <code>don.class</code>) and the case weights (see argument <code>weight.don</code>).  
</p>
</td></tr>
<tr><td><code id="rankNND.hotdeck_+3A_var.rec">var.rec</code></td>
<td>

<p>A character vector with the name of the variable in <code>data.rec</code> that should be ranked. 
</p>
</td></tr>
<tr><td><code id="rankNND.hotdeck_+3A_var.don">var.don</code></td>
<td>

<p>A character vector with the name of the variable <code>data.don</code> that should be ranked.  If not specified, by default <code>var.don=var.rec</code>.
</p>
</td></tr>
<tr><td><code id="rankNND.hotdeck_+3A_don.class">don.class</code></td>
<td>

<p>A character vector with the names of the variables (columns in both the data frames) that identify donation classes.  In each donation class the computation of percentage points is carried out independently.  Then only distances between percentage points of the units in the same donation class are computed.  The case of empty donation classes should be avoided.  It would be preferable that the variables used to form donation classes are defined as <code>factor</code>.
</p>
<p>When not specified (default), no donation classes are used.
</p>
</td></tr>
<tr><td><code id="rankNND.hotdeck_+3A_weight.rec">weight.rec</code></td>
<td>

<p>Eventual name of the variable in <code>data.rec</code> that provides the weights that should be used in computing the the empirical cumulative distribution function for <code>var.rec</code> (see Details).
</p>
</td></tr>
<tr><td><code id="rankNND.hotdeck_+3A_weight.don">weight.don</code></td>
<td>

<p>Eventual name of the variable in <code>data.don</code> that provides the weights that should be used in computing the the empirical cumulative distribution function for <code>var.don</code> (see Details).
</p>
</td></tr>
<tr><td><code id="rankNND.hotdeck_+3A_constrained">constrained</code></td>
<td>

<p>Logical.  When <code>constrained=FALSE</code> (default) each record in <code>data.don</code> can be used as a donor more than once.  On the contrary, when <br /> <code>constrained=TRUE</code> each record in <code>data.don</code> can be used as a donor only once.  In this case, the set of donors is selected by solving a transportation problem, in order to minimize the overall matching distance.  See description of the argument <code>constr.alg</code> for details.
</p>
</td></tr>
<tr><td><code id="rankNND.hotdeck_+3A_constr.alg">constr.alg</code></td>
<td>

<p>A string that has to be specified when <code>constrained=TRUE</code>.  Two choices are available: &ldquo;lpSolve&rdquo; and &ldquo;Hungarian&rdquo;.  In the first case, <code>constr.alg="lpSolve"</code>, the transportation problem is solved by means of the function <code><a href="lpSolve.html#topic+lp.transport">lp.transport</a></code> available in the package <span class="pkg">lpSolve</span>.  When <code>constr.alg="Hungarian"</code> (default) the transportation problem is solved using the Hungarian method, implemented in function <code><a href="clue.html#topic+solve_LSAP">solve_LSAP</a></code> available in the package <span class="pkg">clue</span>.  Note that <br /> <code>constr.alg="Hungarian"</code> is faster and more efficient.
</p>
</td></tr>
<tr><td><code id="rankNND.hotdeck_+3A_keep.t">keep.t</code></td>
<td>

<p>Logical, when donation classes are used by setting  <code>keep.t=TRUE</code> prints information on the donation classes being processed (by default <code>keep.t=FALSE</code>).
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function finds a donor record for each record in the recipient data set.  The chosen donor is the one at the closest distance in terms of empirical cumulative distribution (Singh et al., 1990).  In practice the distance is computed by considering the estimated empirical cumulative distribution for the reference variable (<code>var.rec</code> and <code>var.don</code>) in <code>data.rec</code> and <code>data.don</code>.  The empirical cumulative distribution function is estimated by:
</p>
<p style="text-align: center;"><code class="reqn"> \hat{F}(y) = \frac{1}{n} \sum_{i=1}^{n} I(y_i\leq y) </code>
</p>

<p>being <code class="reqn">I()=1</code> if <code class="reqn">y_i\leq y</code> and 0 otherwise.
</p>
<p>In presence of weights, the empirical cumulative distribution function is estimated by:
</p>
<p style="text-align: center;"><code class="reqn"> \hat{F}(y) =  \frac{\sum_{i=1}^{n} w_i I(y_i\leq y)}{\sum_{i=1}^{n} w_i} </code>
</p>

<p>In the unconstrained case, when there are more donors at the same distance, one of them is chosen at random.
</p>
<p>When the donation class are introduced, then the empirical cumulative distribution function is estimated independently in each donation classes and the search of a recipient is restricted to donors in the same donation class.
</p>
<p>A donor can be chosen more than once. To avoid it set <code>constrained=TRUE</code>. In such a case a donor can be chosen just once and the selection of the donors is carried out by solving a transportation problem with the objective of minimizing the overall matching distance (sum of the distances recipient-donor).
</p>


<h3>Value</h3>

<p>A <span class="rlang"><b>R</b></span> list with the following components:
</p>
<table role = "presentation">
<tr><td><code>mtc.ids</code></td>
<td>

<p>A matrix with the same number of rows of <code>data.rec</code> and two columns.  The first column contains the row names of the <code>data.rec</code> and the second column contains the row names of the corresponding donors selected from the <code>data.don</code>.  When the input matrices do not contain row names, then a numeric matrix with the indexes of the rows is provided.
</p>
</td></tr>
<tr><td><code>dist.rd</code></td>
<td>

<p>A vector with the distances between each recipient unit and the corresponding donor. 
</p>
</td></tr>
<tr><td><code>noad</code></td>
<td>

<p>The number of available donors at the minimum distance for each recipient unit (only in unconstrained case)
</p>
</td></tr>
<tr><td><code>call</code></td>
<td>

<p>How the function has been called.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Marcello D'Orazio <a href="mailto:mdo.statmatch@gmail.com">mdo.statmatch@gmail.com</a> 
</p>


<h3>References</h3>

<p>D'Orazio, M., Di Zio, M. and Scanu, M. (2006). <em>Statistical Matching: Theory and Practice.</em> Wiley, Chichester.
</p>
<p>Singh, A.C., Mantel, H., Kinack, M. and Rowe, G. (1993). &ldquo;Statistical matching: use of auxiliary information as an alternative to the conditional independence assumption&rdquo;. <em>Survey Methodology</em>, <b>19</b>, 59&ndash;79.
</p>


<h3>See Also</h3>

 
<p><code><a href="#topic+NND.hotdeck">NND.hotdeck</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(samp.A, samp.B, package="StatMatch") #loads data sets

# samp.A plays the role of recipient
?samp.A

# samp.B plays the role of donor
?samp.B


# rankNND.hotdeck()
# donation classes formed using "area5"
# ecdf conputed on "age"
# UNCONSTRAINED case
out.1 &lt;- rankNND.hotdeck(data.rec=samp.A, data.don=samp.B, var.rec="age",
                         don.class="area5")
fused.1 &lt;- create.fused(data.rec=samp.A, data.don=samp.B,
                        mtc.ids=out.1$mtc.ids, z.vars="labour5")
head(fused.1)

#  as before but ecdf estimated  using weights
# UNCONSTRAINED case
out.2 &lt;- rankNND.hotdeck(data.rec=samp.A, data.don=samp.B, var.rec="age",
                         don.class="area5",
                         weight.rec="ww", weight.don="ww")
fused.2 &lt;- create.fused(data.rec=samp.A, data.don=samp.B,
                        mtc.ids=out.2$mtc.ids, z.vars="labour5")
head(fused.2)

</code></pre>

<hr>
<h2 id='rho.bounds'>Estimates plausible values of the Pearson's correlation coefficient between two variables observed in distinct samples referred to the same target population.</h2><span id='topic+rho.bounds'></span>

<h3>Description</h3>

<p>This function assesses the uncertainty in estimating the Pearson's correlation coefficient between <code>y.rec</code> (Y) and <code>z.don</code> (Z) when the two variables are observed in two different samples sharing a number of common predictors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rho.bounds(data.rec, data.don,
           match.vars, y.rec, z.don,
           w.rec = NULL, w.don = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="rho.bounds_+3A_data.rec">data.rec</code></td>
<td>
<p>dataframe including the Xs (predictors, listed in <code>match.vars</code>) and <code>y.rec</code> (response; target variable in this dataset).</p>
</td></tr>
<tr><td><code id="rho.bounds_+3A_data.don">data.don</code></td>
<td>
<p>dataframe including the Xs (predictors, listed in  <code>match.vars</code>) and <code>z.don</code> (response; target variable in this dataset).</p>
</td></tr>
<tr><td><code id="rho.bounds_+3A_match.vars">match.vars</code></td>
<td>
<p>vector with the names of the Xs variables to be used, jointly with <code>y.rec</code> and <code>z.don</code>, in estimating the correlation matrix. If <code>match.vars</code> include one or more factor variables these will be replaced with the corresponding dummies before estimating the correlation matrix.</p>
</td></tr>
<tr><td><code id="rho.bounds_+3A_y.rec">y.rec</code></td>
<td>
<p>character indicating the name of Y target variable in <code>data.rec</code>. It should be a numeric variable.</p>
</td></tr>
<tr><td><code id="rho.bounds_+3A_z.don">z.don</code></td>
<td>
<p>character indicating the name of Z target variable in <code>data.don</code>. It should be a numeric variable.</p>
</td></tr>
<tr><td><code id="rho.bounds_+3A_w.rec">w.rec</code></td>
<td>
<p>name of the variable with units' weights in <code>data.rec</code>, if available (default NULL); the weights, if provided, are used in estimating the bounds.</p>
</td></tr>
<tr><td><code id="rho.bounds_+3A_w.don">w.don</code></td>
<td>
<p>name of the variable with units' weights in <code>data.don</code>, if available (default NULL); the weights, if provided, are used in estimating the bounds.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function evaluates the uncertainty in the estimation of the Pearson's correlation coefficient between <code>y.rec</code> (Y) and <code>z.don</code> (Z), when the two variables are observed in two different samples that refer to the same target population, but that share a set of common predictors X (<code>match.vars</code>). The evaluation of the uncertainty corresponds to the estimation of the bounds (lower and upper) of the correlation coefficient between Y and Z, given the available data. The method uses the expressions proposed by Rodgers and DeVol (1982). Note that the correlations between the X variables common to both samples (<code>match.vars</code>) are estimated after pooling the samples. Factor variables, if present in <code>match.vars</code>, are replaced by the corresponding dummies before estimating the correlation; this method suffers from a number of critical problems related to the estimation of biserial correlation and the underlying assumption of a Gaussian distribution. The correlation matrix between Y and Xs is estimated on <code>data.rec</code>, while the correlation matrix between Z and Xs is estimated on <code>data.don</code>; this way of working can in some cases give unreliable estimates due to problems with the samples (usually when they are not representative of the same target population).
</p>


<h3>Value</h3>

<p>A vector with three values: the estimated lower bound for Pearson's correlation coefficient between <code>y.rec</code>(Y) and <code>z.don</code> (Z); the estimated upper bound; and, the mid-point of the interval that corresponds to the estimate Pearson's correlation coefficient under the conditional independence assumption (i.e. the correlations between Y and Z is fully explained by the available X variables <code>match.vars</code>).
</p>


<h3>Author(s)</h3>

<p>Marcello D'Orazio <a href="mailto:mdo.statmatch@gmail.com">mdo.statmatch@gmail.com</a> 
</p>


<h3>References</h3>

<p>D'Orazio, M., (2024). <em>Is Statistical Matching feasible?</em> Note, <a href="https://www.researchgate.net/publication/387699016_Is_statistical_matching_feasible">https://www.researchgate.net/publication/387699016_Is_statistical_matching_feasible</a>.
</p>
<p>Rodgers, W.L. and DeVol E.B. (1982). An evaluation of statistical matching. <em>Report Submitted to the Income Survey Development Program</em>, Dept. of Health and Human Services, Institute for Social Reasearch, University of Michigan.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mixed.mtc">mixed.mtc</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(11335577)
pos &lt;- sample(x = 1:150, size = 60, replace = FALSE)
ir.A &lt;- iris[pos, c(1:3, 5)]
ir.B &lt;- iris[-pos, c(1:2, 4:5)]

intersect(colnames(ir.A), colnames(ir.B)) # shared Xs

# Xs without Species (factor)
out.1 &lt;- rho.bounds(data.rec=ir.A, data.don=ir.B, 
                    match.vars=c("Sepal.Length", "Sepal.Width"),
                   y.rec="Petal.Length", z.don="Petal.Width")
out.1

# Xs with Species (factor)
out.2 &lt;- rho.bounds(data.rec=ir.A, data.don=ir.B, 
                    match.vars=c("Sepal.Length", "Sepal.Width", "Species"),
                    y.rec="Petal.Length", z.don="Petal.Width")
out.2
</code></pre>

<hr>
<h2 id='rho.bounds.pred'>Estimates plausible values of the Pearson's correlation coefficient between two variables observed in distinct samples referred to the same target population.</h2><span id='topic+rho.bounds.pred'></span>

<h3>Description</h3>

<p>This function evaluates the uncertainty in estimating the Pearson's correlation coefficient between <code>y.rec</code> (Y) and <code>z.don</code> (Z) when the two variables are observed in two different samples that share a set of common predictors (Xs). The Xs are used to predict Y and Z respectively, and then the predictions become the input for estimating the uncertainty.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rho.bounds.pred(data.rec, data.don,
                match.vars, y.rec, z.don,
                pred = "lm",
                w.rec = NULL, w.don = NULL, 
								out.pred =FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="rho.bounds.pred_+3A_data.rec">data.rec</code></td>
<td>
<p>dataframe including the Xs (predictors, listed in <code>match.vars</code>) and <code>y.rec</code> (response; target variable in this dataset).</p>
</td></tr>
<tr><td><code id="rho.bounds.pred_+3A_data.don">data.don</code></td>
<td>
<p>dataframe including the Xs (predictors, listed in  <code>match.vars</code>) and <code>z.don</code> (response; target variable in this dataset).</p>
</td></tr>
<tr><td><code id="rho.bounds.pred_+3A_match.vars">match.vars</code></td>
<td>
<p>vector with the names of the Xs variables to be used, as (possible) predictors of respectively <code>y.rec</code> and <code>z.don</code>.</p>
</td></tr>
<tr><td><code id="rho.bounds.pred_+3A_y.rec">y.rec</code></td>
<td>
<p>character indicating the name of Y target variable in <code>data.rec</code>. It should be a numeric variable.</p>
</td></tr>
<tr><td><code id="rho.bounds.pred_+3A_z.don">z.don</code></td>
<td>
<p>character indicating the name of Z target variable in <code>data.don</code>. It should be a numeric variable.</p>
</td></tr>
<tr><td><code id="rho.bounds.pred_+3A_pred">pred</code></td>
<td>
<p>String specifying the method used to obtain predictions of both Y and Z. Available methods include
<code>pred = "lm"</code> (default) fits two linear regression models (function <code>lm</code>) to get predictions with Y and Z as response variables and <code>match.vars</code> as predictors; 
<code>pred = "roblm"</code> (default) fits two robust linear regression models (function <code>rlm</code> in package <span class="pkg">MASS</span>);
<code>pred = "lasso"</code> uses the lasso method (<span class="rlang"><b>R</b></span> package <span class="pkg">glmnet</span>, function <code><a href="glmnet.html#topic+cv.glmnet">cv.glmnet</a></code>) and cross-validation to select a subset of <code>match.vars</code> that are the best predictors of Y (Z) and then obtain the model predictions;
<code>pred = "rf"</code> fits randomForest to get predictions of both Y and Z (function <code><a href="randomForest.html#topic+randomForest">randomForest</a></code> in <span class="pkg">randomForest</span>).</p>
</td></tr>
<tr><td><code id="rho.bounds.pred_+3A_w.rec">w.rec</code></td>
<td>
<p>possible name of the variable with the weights associated to the units in <code>data.rec</code>, if available; the weights are only used in estimating correlations, not in fitting models.</p>
</td></tr>
<tr><td><code id="rho.bounds.pred_+3A_w.don">w.don</code></td>
<td>
<p>possible name of the variable with the weights associated to the units in <code>data.don</code>, if available; the weights are only used in estimating correlations, not in fitting models.</p>
</td></tr>
<tr><td><code id="rho.bounds.pred_+3A_out.pred">out.pred</code></td>
<td>
<p>Logical, when TRUE (default is FALSE) the output includes the input datasets with the predictions of both the target variables.</p>
</td></tr>
<tr><td><code id="rho.bounds.pred_+3A_...">...</code></td>
<td>
<p>addition eventual parameters needed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function evaluates the uncertainty in the estimation of the Pearson's correlation coefficient between <code>y.rec</code> (Y) and <code>z.don</code> (Z), when the two variables are observed in two different samples that refer to the same target population, but that share a set of common predictors X (<code>match.vars</code>). The evaluation of the uncertainty corresponds to the estimation of the bounds (lower and upper) of the correlation coefficient between Y and Z, given the available data. The method uses the expressions proposed by Rodgers and DeVol (1982), but instead of using the Xs <code>match.vars</code> directly, they are replaced by the predictions of both Y and Z provided by the fitted models according to <code>pred</code>. This last way of working avoids the drawbacks encountered when estimating covariances in the presence of several X variables, some of which are categorical (factors) and therefore pose the problem of working with dummies. The final estimation of the bounds is provided by the function <code><a href="#topic+rho.bounds">rho.bounds</a></code>. Note that the correlations between the predictions of both Y and Z are estimated after pooling the samples. Survey weights, if available (arguments <code>w.rec</code> and <code>w.don</code>), are used in estimating the correlations, but not in fitting the models.
</p>


<h3>Value</h3>

<p>a list with the following components:
</p>
<p><code>up.rec</code> only when <code>out.pred = TRUE</code> the output list includes <code>data.rec</code> with the predicted values of both Y and Z;
</p>
<p><code>up.don</code> only when <code>out.pred = TRUE</code> the output list includes <code>data.don</code> with the predicted values of both Y and Z;
</p>
<p><code>corr</code> the estimated correlations between Y (Z) and the corresponding predicted values;
</p>
<p><code>bounds</code> a vector with three values: the estimated lower bound for the Pearson's correlation coefficient between <code>y.rec</code>(Y) and <code>z.don</code> (Z); the estimated upper bound;  and, the mid-point of the interval that corresponds to the estimate Pearson's correlation coefficient under the conditional independence assumption.
</p>


<h3>Author(s)</h3>

<p>Marcello D'Orazio <a href="mailto:mdo.statmatch@gmail.com">mdo.statmatch@gmail.com</a> 
</p>


<h3>References</h3>

<p>D'Orazio, M., (2024). <em>Is Statistical Matching feasible?</em> Note, <a href="https://www.researchgate.net/publication/387699016_Is_statistical_matching_feasible">https://www.researchgate.net/publication/387699016_Is_statistical_matching_feasible</a>.
</p>
<p>Rodgers, W.L. and DeVol E.B. (1982). An evaluation of statistical matching. <em>Report Submitted to the Income Survey Development Program</em>, Dept. of Health and Human Services, Institute for Social Reasearch, University of Michigan.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mixed.mtc">mixed.mtc</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(11335577)
pos &lt;- sample(x = 1:150, size = 60, replace = FALSE)
ir.A &lt;- iris[pos, c(1:3, 5)]
ir.B &lt;- iris[-pos, c(1:2, 4:5)]

intersect(colnames(ir.A), colnames(ir.B)) # shared Xs

op1 &lt;- rho.bounds.pred(data.rec=ir.A, data.don=ir.B, 
                       match.vars=c("Sepal.Length", "Sepal.Width", "Species"),
                       y.rec="Petal.Length", z.don="Petal.Width", 
                       pred = "lm")
op1
op2 &lt;- rho.bounds.pred(data.rec=ir.A, data.don=ir.B, 
                       match.vars=c("Sepal.Length", "Sepal.Width", "Species"),
                       y.rec="Petal.Length", z.don="Petal.Width", 
                       pred = "roblm")
op2
</code></pre>

<hr>
<h2 id='samp.A'>Artificial data set resembling EU&ndash;SILC survey</h2><span id='topic+samp.A'></span>

<h3>Description</h3>

<p>This data set provides a limited number of variables observed at persons levels among those usually collected in the European Union Statistics on Income and Living Conditions Survey (EU&ndash;SILC). The data are artificially generated, just to show the application of the statistical matching techniques implemented in <span class="pkg">StatMatch</span>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(samp.A)
</code></pre>


<h3>Format</h3>

<p>A data frame with 3009 observations and the following variables:
</p>

<dl>
<dt>HH.P.id</dt><dd><p>unique unit identifier of the type <code>aa.bb</code> where <code>aa</code> identifies the Household while <code>bb</code> identifies the household member</p>
</dd>
<dt>area5</dt><dd><p>large geographic area, factor with 5 levels: &lsquo;NE&rsquo;=North&ndash;East, &lsquo;NO&rsquo;=North&ndash;West,
&lsquo;C&rsquo;=center, &lsquo;S&rsquo;=South, &lsquo;I&rsquo;=islands</p>
</dd>
<dt>urb</dt><dd><p>Degree of urbanization, factor with 3 levels: &lsquo;1&rsquo;=densely populated area, &lsquo;2&rsquo;=intermediate area,
&lsquo;3&rsquo;=thinly populated area</p>
</dd>
<dt>hsize</dt><dd><p>integer, size of the household in which the person lives</p>
</dd>
<dt>hsize5</dt><dd><p>factor with 5 levels derived from <code>hsize</code>, where the 5th level &lsquo;&gt;=5&rsquo; denotes 5 and more people in the household</p>
</dd>
<dt>age</dt><dd><p>integer, the person's age</p>
</dd>
<dt>c.age</dt><dd><p>factor, age categorized in 5 classes</p>
</dd>
<dt>sex</dt><dd><p>factor, the person's gender: &lsquo;1&rsquo;=male, &lsquo;2&rsquo;=female</p>
</dd>
<dt>marital</dt><dd><p>factor, the person's marital status: &lsquo;1&rsquo;=never married, &lsquo;2&rsquo;=married, &lsquo;3&rsquo;=other (separated, widowed, divorced)</p>
</dd>
<dt>edu7</dt><dd><p>factor, the person's highest education level attained, follows the ISCED-97 categories:
&lsquo;0&rsquo;=pre&ndash;primary education, &lsquo;1&rsquo;=primary education, &lsquo;2&rsquo;=lower secondary education, &lsquo;3&rsquo;= (upper) secondary education, &lsquo;4&rsquo;= post&ndash;secondary non tertiary education, &lsquo;5&rsquo;=first stage of tertiary education (not leading directly to an advanced research qualification), &lsquo;6&rsquo;=second stage of tertiary education (leading to an advanced research qualification)</p>
</dd>
<dt>n.income</dt><dd><p>numeric, the person's net income in Euros</p>
</dd>
<dt>c.neti</dt><dd><p>factor, the person's net income categorized in 7 classes of thousand of Euros</p>
</dd>
<dt>ww</dt><dd><p>numeric, the unit's weight</p>
</dd>
</dl>



<h3>Details</h3>

<p>Please note that this data set is just for illustrative purposes. The unit's weight do not reflect the Italian population size. The variables included are derived starting from the those usually observed in the EU&ndash;SILC survey. 
</p>


<h3>Source</h3>

<p>This data set is artificially created starting from the EU&ndash;SILC survey structure. 
</p>


<h3>References</h3>

<p><a href="https://ec.europa.eu/eurostat/web/income-and-living-conditions/overview">https://ec.europa.eu/eurostat/web/income-and-living-conditions/overview</a>
</p>

<hr>
<h2 id='samp.B'>Artificial data set resembling EU&ndash;SILC survey</h2><span id='topic+samp.B'></span>

<h3>Description</h3>

<p>This data set provides a limited number of variables observed at persons levels among those usually collected in the European Union Statistics on Income and Living Conditions Survey (EU&ndash;SILC). The data are artificially generated, just to show the application of the statistical matching techniques implemented in <span class="pkg">StatMatch</span>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(samp.B)
</code></pre>


<h3>Format</h3>

<p>A data frame with 6686 observations and the following variables:
</p>

<dl>
<dt>HH.P.id</dt><dd><p>unique unit identifier of the type <code>aa.bb</code> where <code>aa</code> identifies the Household while <code>bb</code> identifies the household member</p>
</dd>
<dt>area5</dt><dd><p>large geographic area, factor with 5 levels: &lsquo;NE&rsquo;=North&ndash;East, &lsquo;NO&rsquo;=North&ndash;West,
&lsquo;C&rsquo;=center, &lsquo;S&rsquo;=South, &lsquo;I&rsquo;=islands</p>
</dd>
<dt>urb</dt><dd><p>Degree of urbanization, factor with 3 levels: &lsquo;1&rsquo;=densely populated area, &lsquo;2&rsquo;=intermediate area,
&lsquo;3&rsquo;=thinly populated area</p>
</dd>
<dt>hsize</dt><dd><p>integer, size of the household in which the person lives</p>
</dd>
<dt>hsize5</dt><dd><p>factor with 5 levels derived from <code>hsize</code>, where the 5th level &lsquo;&gt;=5&rsquo; denotes 5 and more people in the household</p>
</dd>
<dt>age</dt><dd><p>integer, the person's age</p>
</dd>
<dt>c.age</dt><dd><p>factor, age categorized in 5 classes</p>
</dd>
<dt>sex</dt><dd><p>factor, the person's gender: &lsquo;1&rsquo;=male, &lsquo;2&rsquo;=female</p>
</dd>
<dt>marital</dt><dd><p>factor, the person's marital status: &lsquo;1&rsquo;=never married, &lsquo;2&rsquo;=married, &lsquo;3&rsquo;=other (separated, widowed, divorced)</p>
</dd>
<dt>edu7</dt><dd><p>factor, the person's highest education level attained, follows the ISCED-97 categories:
&lsquo;0&rsquo;=pre&ndash;primary education, &lsquo;1&rsquo;=primary education, &lsquo;2&rsquo;=lower secondary education, &lsquo;3&rsquo;= (upper) secondary education, &lsquo;4&rsquo;= post&ndash;secondary non tertiary education, &lsquo;5&rsquo;=first stage of tertiary education (not leading directly to an advanced research qualification), &lsquo;6&rsquo;=second stage of tertiary education (leading to an advanced research qualification)</p>
</dd>
<dt>labour5</dt><dd><p>the person's self&ndash;defined economic status, factor with 5 levels: &lsquo;1&rsquo;=employee working full&ndash;time or part&ndash;time, &lsquo;2&rsquo;=self&ndash;employed working full&ndash;time or part&ndash;time, &lsquo;3&rsquo;=unemployed, &lsquo;4&rsquo;=In retirement or in early retirement or has given up business, &lsquo;5&rsquo;=other status (student, permanent disabled, in compulsory military service, fulfilling domestic tasks, etc.)</p>
</dd>
<dt>ww</dt><dd><p>numeric, the unit's weight</p>
</dd>
</dl>



<h3>Details</h3>

<p>Please note that this data set is just for illustrative purposes. The unit's weight do not reflect the Italian population size. The variables included are derived starting from the those usually observed in the EU&ndash;SILC survey. 
</p>


<h3>Source</h3>

<p>This data set is artificially created starting from the EU&ndash;SILC survey structure. 
</p>


<h3>References</h3>

<p><a href="https://ec.europa.eu/eurostat/web/income-and-living-conditions/overview">https://ec.europa.eu/eurostat/web/income-and-living-conditions/overview</a>
</p>

<hr>
<h2 id='samp.C'>Artificial data set resembling EU&ndash;SILC survey</h2><span id='topic+samp.C'></span>

<h3>Description</h3>

<p>This data set provides a limited number of variables observed at persons levels among those usually collected in the European Union Statistics on Income and Living Conditions Survey (EU&ndash;SILC). The data are artificially generated, just to show the application of the statistical matching techniques implemented in <span class="pkg">StatMatch</span>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(samp.C)
</code></pre>


<h3>Format</h3>

<p>A data frame with 980 observations and the following variables:
</p>

<dl>
<dt>HH.P.id</dt><dd><p>unique unit identifier of the type <code>aa.bb</code> where <code>aa</code> identifies the Household while <code>bb</code> identifies the household member</p>
</dd>
<dt>area5</dt><dd><p>large geographic area, factor with 5 levels: &lsquo;NE&rsquo;=North&ndash;East, &lsquo;NO&rsquo;=North&ndash;West,
&lsquo;C&rsquo;=center, &lsquo;S&rsquo;=South, &lsquo;I&rsquo;=islands</p>
</dd>
<dt>urb</dt><dd><p>Degree of urbanization, factor with 3 levels: &lsquo;1&rsquo;=densely populated area, &lsquo;2&rsquo;=intermediate area,
&lsquo;3&rsquo;=thinly populated area</p>
</dd>
<dt>hsize</dt><dd><p>integer, size of the household in which the person lives</p>
</dd>
<dt>hsize5</dt><dd><p>factor with 5 levels derived from <code>hsize</code>, where the 5th level &lsquo;&gt;=5&rsquo; denotes 5 and more people in the household</p>
</dd>
<dt>age</dt><dd><p>integer, the person's age</p>
</dd>
<dt>c.age</dt><dd><p>factor, age categorized in 5 classes</p>
</dd>
<dt>sex</dt><dd><p>factor, the person's gender: &lsquo;1&rsquo;=male, &lsquo;2&rsquo;=female</p>
</dd>
<dt>marital</dt><dd><p>factor, the person's marital status: &lsquo;1&rsquo;=never married, &lsquo;2&rsquo;=married, &lsquo;3&rsquo;=other (separated, widowed, divorced)</p>
</dd>
<dt>edu7</dt><dd><p>factor, the person's highest education level attained, follows the ISCED-97 categories:
&lsquo;0&rsquo;=pre&ndash;primary education, &lsquo;1&rsquo;=primary education, &lsquo;2&rsquo;=lower secondary education, &lsquo;3&rsquo;= (upper) secondary education, &lsquo;4&rsquo;= post&ndash;secondary non tertiary education, &lsquo;5&rsquo;=first stage of tertiary education (not leading directly to an advanced research qualification), &lsquo;6&rsquo;=second stage of tertiary education (leading to an advanced research qualification)</p>
</dd>
<dt>labour5</dt><dd><p>the person's self&ndash;defined economic status, factor with 5 levels: &lsquo;1&rsquo;=employee working full&ndash;time or part&ndash;time, &lsquo;2&rsquo;=self&ndash;employed working full&ndash;time or part&ndash;time, &lsquo;3&rsquo;=unemployed, &lsquo;4&rsquo;=In retirement or in early retirement or has given up business, &lsquo;5&rsquo;=other status (student, permanent disabled, in compulsory military service, fulfilling domestic tasks, etc.)</p>
</dd>
<dt>n.income</dt><dd><p>numeric, the person's net income in Euros</p>
</dd>
<dt>c.neti</dt><dd><p>factor, the person's net income categorized in 7 classes of thousand of Euros</p>
</dd>
<dt>ww</dt><dd><p>numeric, the unit's weight</p>
</dd>
</dl>



<h3>Details</h3>

<p>Please note that this data set is just for illustrative purposes. The unit's weight do not reflect the Italian population size. The variables included are derived starting from the those usually observed in the EU&ndash;SILC survey. 
</p>


<h3>Source</h3>

<p>This data set is artificially created starting from the EU&ndash;SILC survey structure. 
</p>


<h3>References</h3>

<p><a href="https://ec.europa.eu/eurostat/web/income-and-living-conditions/overview">https://ec.europa.eu/eurostat/web/income-and-living-conditions/overview</a>
</p>

<hr>
<h2 id='selMtc.by.unc'>Identifies the best combination if matching variables in reducing uncertainty in estimation the contingency table Y vs. Z.</h2><span id='topic+selMtc.by.unc'></span>

<h3>Description</h3>

<p>This function identifies the &ldquo;best&rdquo; subset of matching variables in terms of reduction of uncertainty when estimating relative frequencies in the contingency table Y vs. Z. The sequential procedure presented in D'Orazio <em>et al.</em>  (2017 and 2019) is implemented. This procedure avoids exploring all the possible combinations of the available X variables as in <code><a href="#topic+Fbwidths.by.x">Fbwidths.by.x</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>selMtc.by.unc(tab.x, tab.xy, tab.xz, corr.d=2, 
                    nA=NULL, nB=NULL, align.margins=FALSE) 
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="selMtc.by.unc_+3A_tab.x">tab.x</code></td>
<td>

<p>A <span class="rlang"><b>R</b></span> table crossing the <b>X</b> variables.  This table must be obtained by using the function <code><a href="stats.html#topic+xtabs">xtabs</a></code> or <code><a href="base.html#topic+table">table</a></code>, e.g. <br />
<code>tab.x &lt;- xtabs(~x1+x2+x3, data=data.all)</code>. <strong>A minimum number of 3 variables is needed</strong>.
</p>
</td></tr>
<tr><td><code id="selMtc.by.unc_+3A_tab.xy">tab.xy</code></td>
<td>

<p>A <span class="rlang"><b>R</b></span> table of <b>X</b> vs. Y variable.  This table must be obtained by using the function <code><a href="stats.html#topic+xtabs">xtabs</a></code> or <code><a href="base.html#topic+table">table</a></code>, e.g. <br />
<code>table.xy &lt;- xtabs(~x1+x2+x3+y, data=data.A)</code>.
</p>
<p>A single categorical Y variables is allowed.  At least <strong>three</strong> categorical variables should be considered as <b>X</b> variables (common variables).  The same <b>X</b> variables in <code>tab.x</code> must be available in <code>tab.xy</code>.   Usually, it is assumed that the joint distribution of the <b>X</b> variables computed from <code>tab.xy</code> is equal to <code>tab.x</code> (a warning appears if any absolute difference is greater than <code>tol</code>). Note that when the marginal distribution of  <b>X</b> in <code>tab.xy</code> is not equal to that of <code>tab.x</code> it is possible to align them before computations (see argument <code>align.margins</code>).
</p>
</td></tr>
<tr><td><code id="selMtc.by.unc_+3A_tab.xz">tab.xz</code></td>
<td>

<p>A <span class="rlang"><b>R</b></span> table of <b>X</b> vs. Z variable.  This table must be obtained by using the function <code><a href="stats.html#topic+xtabs">xtabs</a></code> or <code><a href="base.html#topic+table">table</a></code>, e.g. <br />
<code>tab.xz &lt;- xtabs(~x1+x2+x3+z, data=data.B)</code>.
</p>
<p>A single categorical Z variable is allowed.  At least <strong>three</strong> categorical variables should be considered as <b>X</b> variables (common variables).  The same <b>X</b> variables in <code>tab.x</code> must be available in <code>tab.xz</code>.  Usually, it is assumed that the joint distribution of the <b>X</b> variables computed from <code>tab.xz</code> is equal to <code>tab.x</code> (a warning appears if any absolute difference is greater than <code>tol</code>). Note that when the marginal distribution of  <b>X</b> in <code>tab.xz</code> is not equal to that of <code>tab.x</code> it is possible to align them before computations (see argument <code>align.margins</code>).
</p>
</td></tr>
<tr><td><code id="selMtc.by.unc_+3A_corr.d">corr.d</code></td>
<td>

<p>Integer, indicates the penalty that should be introduced in estimating the uncertainty by means of the average width of cell bounds. When <code>corr.d=1</code>  the penalty being considered is the one introduced in D'Orazio <em>et al.</em> (2017) (i.e. penalty1 in <code><a href="#topic+Fbwidths.by.x">Fbwidths.by.x</a></code>). When <code>corr.d=2</code> (default) it is considered a penalty suggested in D'Orazio <em>et al.</em> (2019) (indicated as &ldquo;penalty2&rdquo;  in   <code><a href="#topic+Fbwidths.by.x">Fbwidths.by.x</a></code>). Finally, no penalties are considered when <code>corr.d=0</code>.
</p>
</td></tr>
<tr><td><code id="selMtc.by.unc_+3A_na">nA</code></td>
<td>

<p>Integer, sample size of file A used to estimate <code>tab.xy</code>. If  <code>NULL</code> is obtained as sum of frequencies in <code>tab.xy</code>.
</p>
</td></tr>
<tr><td><code id="selMtc.by.unc_+3A_nb">nB</code></td>
<td>

<p>Integer, sample size of file B used to estimate <code>tab.xz</code>. If  <code>NULL</code> is obtained as sum of frequencies in <code>tab.xz</code>.
</p>
</td></tr>
<tr><td><code id="selMtc.by.unc_+3A_align.margins">align.margins</code></td>
<td>

<p>Logical (default <code>FALSE</code>). When when <code>TRUE</code> the distribution of <b>X</b> variables in <code>tab.xy</code> is aligned with the distribution resulting from <code>tab.x</code>, without affecting the marginal distribution of Y. Similarly the distribution of <b>X</b> variables in <code>tab.xz</code> is aligned with the distribution resulting from <code>tab.x</code>, without affecting the marginal distribution of Z. The alignment is performed by running IPF algorithm as implemented in the function <code><a href="mipfp.html#topic+Estimate">Estimate</a></code> in the package <span class="pkg">mipfp</span>. To avoid lack of convergence due to combinations of Xs encountered in one table but not in the other (statistical 0s), before running IPF a small constant (1e-06) is added to empty cells in <code>tab.xy</code> and <code>tab.xz</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function follows the sequential procedure described in D'Orazio <em>et al.</em> (2017, 2019) to identify the combination of common variables most effective in reducing uncertainty when estimating the contingency table Y vs. Z. Initially, the available Xs are ordered according to the reduction of average width of uncertainty bounds when conditioning on each of them. Then in each step one the remaining X variables is added until the table became too sparse; in practice the procedure stops when: 
</p>
<p style="text-align: center;"><code class="reqn">min\left[ \frac{n_A}{H_{D_m} \times J}, \frac{n_B}{H_{D_m} \times K} \right] \leq 1 </code>
</p>
 
<p>For major details see also <code><a href="#topic+Fbwidths.by.x">Fbwidths.by.x</a></code>.
</p>


<h3>Value</h3>

<p>A list with the main outcomes of the procedure.
</p>
<table role = "presentation">
<tr><td><code>ini.ord</code></td>
<td>

<p>Average width of uncertainty bounds when conditioning on each of the available X variables. Variable most effective in reducing uncertainty comes first. The ordering determines the order in which they are entered in the sequential procedure. 
</p>
</td></tr>
<tr><td><code>list.xs</code></td>
<td>

<p>List with the various combinations of the matching variables being considered in each step.
</p>
</td></tr>
<tr><td><code>av.df</code></td>
<td>

<p>Data.frame with all the relevant information for each of combination of X variables. The last row corresponds to the combination of the X variables identified as the best in reducing average width of uncertainty bounds (penalized or not depending on the input argument <code>corr.d</code>). For each combination of X variables the following additional information are reported:  the number of cells (name starts with &ldquo;<code>nc</code>&rdquo;);  the number of empty cells (name starts with &ldquo;<code>nc0</code>&rdquo;; the average relative frequency (name starts with &ldquo;<code>av.crf</code>&rdquo;); sparseness measured as Cohen's effect size with respect to equiprobability (uniform distribution across cells). Finally there are the value of the stopping criterion (&ldquo;<code>min.av</code>&rdquo;), the unconditioned average width of uncertainty bounds (&ldquo;<code>avw</code>&rdquo;), the penalty term  (&ldquo;<code>penalty</code>&rdquo;) and the penalized width  (&ldquo;<code>avw.pen</code>&rdquo;; <code>avw.pen=avw+penalty</code>).
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Marcello D'Orazio <a href="mailto:mdo.statmatch@gmail.com">mdo.statmatch@gmail.com</a> 
</p>


<h3>References</h3>

<p>D'Orazio, M., Di Zio, M. and Scanu, M. (2006). <em>Statistical Matching: Theory and Practice.</em> Wiley, Chichester.
</p>
<p>D'Orazio, M., Di Zio, M. and Scanu, M. (2017). &ldquo;The use of uncertainty to choose matching variables in statistical matching&rdquo;. <em>International Journal of Approximate Reasoning</em>, 90, pp. 433-440.
</p>
<p>D'Orazio, M., Di Zio, M. and Scanu, M. (2019). &ldquo;Auxiliary variable selection in a a statistical matching problem&rdquo;. In Zhang, L.-C. and Chambers, R. L. (eds.) <em>Analysis of Integrated Data</em>, Chapman &amp; Hall/CRC (forthcoming).
</p>


<h3>See Also</h3>

 
<p><code><a href="#topic+Fbwidths.by.x">Fbwidths.by.x</a></code>, <code><a href="#topic+Frechet.bounds.cat">Frechet.bounds.cat</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(quine, package="MASS") #loads quine from MASS
str(quine)
quine$c.Days &lt;- cut(quine$Days, c(-1, seq(0,50,10),100))
table(quine$c.Days)


# split quine in two subsets
suppressWarnings(RNGversion("3.5.0"))
set.seed(1111)
lab.A &lt;- sample(nrow(quine), 70, replace=TRUE)
quine.A &lt;- quine[lab.A, 1:4]
quine.B &lt;- quine[-lab.A, c(1:3,6)]

# compute the tables required by Fbwidths.by.x()
freq.xA &lt;- xtabs(~Eth+Sex+Age, data=quine.A)
freq.xB &lt;- xtabs(~Eth+Sex+Age, data=quine.B)

freq.xy &lt;- xtabs(~Eth+Sex+Age+Lrn, data=quine.A)
freq.xz &lt;- xtabs(~Eth+Sex+Age+c.Days, data=quine.B)

# apply Fbwidths.by.x()
bb &lt;- Fbwidths.by.x(tab.x=freq.xA+freq.xB, 
                           tab.xy=freq.xy,  tab.xz=freq.xz,
                           warn=FALSE)
bb$sum.unc
cc &lt;- selMtc.by.unc(tab.x=freq.xA+freq.xB, 
                           tab.xy=freq.xy,  tab.xz=freq.xz, corr.d=0)
cc$ini.ord
cc$av.df


</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
