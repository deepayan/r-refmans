<!DOCTYPE html><html><head><title>Help for package genieclust</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {genieclust}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#cluster_validity'><p>Internal Cluster Validity Measures</p></a></li>
<li><a href='#compare_partitions'><p>External Cluster Validity Measures and Pairwise Partition Similarity Scores</p></a></li>
<li><a href='#emst_mlpack'><p>Euclidean Minimum Spanning Tree</p></a></li>
<li><a href='#gclust'><p>Hierarchical Clustering Algorithm Genie</p></a></li>
<li><a href='#genieclust-package'><p>The Genie Hierarchical Clustering Algorithm (with Extras)</p></a></li>
<li><a href='#inequality'><p>Inequality Measures</p></a></li>
<li><a href='#mst'><p>Minimum Spanning Tree of the Pairwise Distance Graph</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Fast and Robust Hierarchical Clustering with Noise Points
Detection</td>
</tr>
<tr>
<td>Version:</td>
<td>1.1.5-2</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-10-18</td>
</tr>
<tr>
<td>Description:</td>
<td>A retake on the Genie algorithm
    (Gagolewski, 2021 &lt;<a href="https://doi.org/10.1016%2Fj.softx.2021.100722">doi:10.1016/j.softx.2021.100722</a>&gt;) - a robust
    hierarchical clustering method
    (Gagolewski, Bartoszuk, Cena, 2016 &lt;<a href="https://doi.org/10.1016%2Fj.ins.2016.05.003">doi:10.1016/j.ins.2016.05.003</a>&gt;).
    Now faster and more memory efficient; determining the whole hierarchy
    for datasets of 10M points in low dimensional Euclidean spaces or
    100K points in high-dimensional ones takes only 1-2 minutes.
    Allows clustering with respect to mutual reachability distances
    so that it can act as a noise point detector or a robustified version of
    'HDBSCAN*' (that is able to detect a predefined number of
    clusters and hence it does not dependent on the somewhat
    fragile 'eps' parameter).
    The package also features an implementation of inequality indices
    (the Gini, Bonferroni index), external cluster validity measures
    (e.g., the normalised clustering accuracy and partition similarity scores
    such as the adjusted Rand, Fowlkes-Mallows, adjusted mutual information,
    and the pair sets index),
    and internal cluster validity indices (e.g., the Calinski-Harabasz,
    Davies-Bouldin, Ball-Hall, Silhouette, and generalised Dunn indices).
    See also the 'Python' version of 'genieclust' available on 'PyPI', which
    supports sparse data, more metrics, and even larger datasets.</td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/gagolews/genieclust/issues">https://github.com/gagolews/genieclust/issues</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://genieclust.gagolewski.com/">https://genieclust.gagolewski.com/</a>,
<a href="https://clustering-benchmarks.gagolewski.com/">https://clustering-benchmarks.gagolewski.com/</a>,
<a href="https://github.com/gagolews/genieclust">https://github.com/gagolews/genieclust</a></td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/AGPL-3">AGPL-3</a></td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp (&ge; 1.0.4), stats, utils</td>
</tr>
<tr>
<td>Suggests:</td>
<td>datasets, mlpack</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>OpenMP</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-10-18 08:43:24 UTC; gagolews</td>
</tr>
<tr>
<td>Author:</td>
<td>Marek Gagolewski <a href="https://orcid.org/0000-0003-0637-6028"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre, cph],
  Maciej Bartoszuk [ctb],
  Anna Cena [ctb],
  Peter M. Larsen [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Marek Gagolewski &lt;marek@gagolewski.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-10-18 09:50:06 UTC</td>
</tr>
</table>
<hr>
<h2 id='cluster_validity'>Internal Cluster Validity Measures</h2><span id='topic+cluster_validity'></span><span id='topic+calinski_harabasz_index'></span><span id='topic+dunnowa_index'></span><span id='topic+generalised_dunn_index'></span><span id='topic+negated_ball_hall_index'></span><span id='topic+negated_davies_bouldin_index'></span><span id='topic+negated_wcss_index'></span><span id='topic+silhouette_index'></span><span id='topic+silhouette_w_index'></span><span id='topic+wcnn_index'></span>

<h3>Description</h3>

<p>Implementation of a number of so-called cluster validity indices critically
reviewed in (Gagolewski, Bartoszuk, Cena, 2021). See Section 2
therein and (Gagolewski, 2022) for the respective definitions.
</p>
<p>The greater the index value, the more <em>valid</em> (whatever that means)
the assessed partition. For consistency, the Ball-Hall and
Davies-Bouldin indexes as well as the within-cluster sum of squares (WCSS)
take negative values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calinski_harabasz_index(X, y)

dunnowa_index(
  X,
  y,
  M = 25L,
  owa_numerator = "SMin:5",
  owa_denominator = "Const"
)

generalised_dunn_index(X, y, lowercase_d, uppercase_d)

negated_ball_hall_index(X, y)

negated_davies_bouldin_index(X, y)

negated_wcss_index(X, y)

silhouette_index(X, y)

silhouette_w_index(X, y)

wcnn_index(X, y, M = 25L)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cluster_validity_+3A_x">X</code></td>
<td>
<p>numeric matrix with <code>n</code> rows and <code>d</code> columns,
representing <code>n</code> points in a <code>d</code>-dimensional space</p>
</td></tr>
<tr><td><code id="cluster_validity_+3A_y">y</code></td>
<td>
<p>vector of <code>n</code> integer labels,
representing a partition whose <em>quality</em> is to be
assessed; <code>y[i]</code> is the cluster ID of the <code>i</code>-th point,
<code>X[i, ]</code>; <code>1 &lt;= y[i] &lt;= K</code>, where <code>K</code> is the number
or clusters</p>
</td></tr>
<tr><td><code id="cluster_validity_+3A_m">M</code></td>
<td>
<p>number of nearest neighbours</p>
</td></tr>
<tr><td><code id="cluster_validity_+3A_owa_numerator">owa_numerator</code>, <code id="cluster_validity_+3A_owa_denominator">owa_denominator</code></td>
<td>
<p>single string specifying
the OWA operators to use in the definition of the DuNN index;
one of: <code>"Mean"</code>, <code>"Min"</code>, <code>"Max"</code>, <code>"Const"</code>,
<code>"SMin:D"</code>, <code>"SMax:D"</code>, where <code>D</code> is an integer
defining the degree of smoothness</p>
</td></tr>
<tr><td><code id="cluster_validity_+3A_lowercase_d">lowercase_d</code></td>
<td>
<p>an integer between 1 and 5, denoting
<code class="reqn">d_1</code>, ..., <code class="reqn">d_5</code> in the definition
of the generalised Dunn (Bezdek-Pal) index (numerator:
min, max, and mean pairwise intracluster distance,
distance between cluster centroids,
weighted point-centroid distance, respectively)</p>
</td></tr>
<tr><td><code id="cluster_validity_+3A_uppercase_d">uppercase_d</code></td>
<td>
<p>an integer between 1 and 3, denoting
<code class="reqn">D_1</code>, ..., <code class="reqn">D_3</code> in the definition
of the generalised Dunn (Bezdek-Pal) index (denominator:
max and min pairwise intracluster distance, average point-centroid
distance, respectively)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A single numeric value (the more, the <em>better</em>).
</p>


<h3>Author(s)</h3>

<p><a href="https://www.gagolewski.com/">Marek Gagolewski</a> and other contributors
</p>


<h3>References</h3>

<p>Ball G.H., Hall D.J.,
<em>ISODATA: A novel method of data analysis and pattern classification</em>,
Technical report No. AD699616, Stanford Research Institute, 1965.
</p>
<p>Bezdek J., Pal N., Some new indexes of cluster validity,
<em>IEEE Transactions on Systems, Man, and Cybernetics, Part B</em> 28,
1998, 301-315, <a href="https://doi.org/10.1109/3477.678624">doi:10.1109/3477.678624</a>.
</p>
<p>Calinski T., Harabasz J., A dendrite method for cluster analysis,
<em>Communications in Statistics</em> 3(1), 1974, 1-27,
<a href="https://doi.org/10.1080/03610927408827101">doi:10.1080/03610927408827101</a>.
</p>
<p>Davies D.L., Bouldin D.W.,
A Cluster Separation Measure,
<em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>
PAMI-1 (2), 1979, 224-227, <a href="https://doi.org/10.1109/TPAMI.1979.4766909">doi:10.1109/TPAMI.1979.4766909</a>.
</p>
<p>Dunn J.C., A Fuzzy Relative of the ISODATA Process and Its Use in Detecting
Compact Well-Separated Clusters, <em>Journal of Cybernetics</em> 3(3), 1973,
32-57, <a href="https://doi.org/10.1080/01969727308546046">doi:10.1080/01969727308546046</a>.
</p>
<p>Gagolewski M., Bartoszuk M., Cena A.,
Are cluster validity measures (in)valid?, <em>Information Sciences</em> 581,
620-636, 2021, <a href="https://doi.org/10.1016/j.ins.2021.10.004">doi:10.1016/j.ins.2021.10.004</a>;
preprint: <a href="https://raw.githubusercontent.com/gagolews/bibliography/master/preprints/2021cvi.pdf">https://raw.githubusercontent.com/gagolews/bibliography/master/preprints/2021cvi.pdf</a>.
</p>
<p>Gagolewski M., <em>A Framework for Benchmarking Clustering Algorithms</em>,
2022, <a href="https://clustering-benchmarks.gagolewski.com">https://clustering-benchmarks.gagolewski.com</a>.
</p>
<p>Rousseeuw P.J., Silhouettes: A Graphical Aid to the Interpretation and
Validation of Cluster Analysis, <em>Computational and Applied Mathematics</em>
20, 1987, 53-65, <a href="https://doi.org/10.1016/0377-0427%2887%2990125-7">doi:10.1016/0377-0427(87)90125-7</a>.
</p>


<h3>See Also</h3>

<p>The official online manual of <span class="pkg">genieclust</span> at <a href="https://genieclust.gagolewski.com/">https://genieclust.gagolewski.com/</a>
</p>
<p>Gagolewski M., <span class="pkg">genieclust</span>: Fast and robust hierarchical clustering, <em>SoftwareX</em> 15:100722, 2021, <a href="https://doi.org/10.1016/j.softx.2021.100722">doi:10.1016/j.softx.2021.100722</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>X &lt;- as.matrix(iris[,1:4])
X[,] &lt;- jitter(X)  # otherwise we get a non-unique solution
y &lt;- as.integer(iris[[5]])
calinski_harabasz_index(X, y)  # good
calinski_harabasz_index(X, sample(1:3, nrow(X), replace=TRUE))  # bad

</code></pre>

<hr>
<h2 id='compare_partitions'>External Cluster Validity Measures and Pairwise Partition Similarity Scores</h2><span id='topic+compare_partitions'></span><span id='topic+normalized_clustering_accuracy'></span><span id='topic+normalized_pivoted_accuracy'></span><span id='topic+pair_sets_index'></span><span id='topic+adjusted_rand_score'></span><span id='topic+rand_score'></span><span id='topic+adjusted_fm_score'></span><span id='topic+fm_score'></span><span id='topic+mi_score'></span><span id='topic+normalized_mi_score'></span><span id='topic+adjusted_mi_score'></span><span id='topic+normalized_confusion_matrix'></span><span id='topic+normalizing_permutation'></span>

<h3>Description</h3>

<p>The functions described in this section quantify the similarity between
two label vectors <code>x</code> and <code>y</code> which represent two partitions
of a set of <code class="reqn">n</code> elements into, respectively, <code class="reqn">K</code> and <code class="reqn">L</code>
nonempty and pairwise disjoint subsets.
</p>
<p>For instance, <code>x</code> and <code>y</code> can represent two clusterings
of a dataset with <code class="reqn">n</code> observations specified by two vectors
of labels. The functions described here can be used as external cluster
validity measures, where we assume that <code>x</code> is
a reference (ground-truth) partition whilst <code>y</code> is the vector
of predicted cluster memberships.
</p>
<p>All indices except <code>normalized_clustering_accuracy()</code>
can act as a pairwise partition similarity score: they are symmetric,
i.e., <code>index(x, y) == index(y, x)</code>.
</p>
<p>Each index except <code>mi_score()</code> (which computes the mutual
information score) outputs 1 given two identical partitions.
Note that partitions are always defined up to a permutation (bijection)
of the set of possible labels, e.g., (1, 1, 2, 1) and (4, 4, 2, 4)
represent the same 2-partition.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>normalized_clustering_accuracy(x, y = NULL)

normalized_pivoted_accuracy(x, y = NULL)

pair_sets_index(x, y = NULL, simplified = FALSE, clipped = TRUE)

adjusted_rand_score(x, y = NULL, clipped = FALSE)

rand_score(x, y = NULL)

adjusted_fm_score(x, y = NULL, clipped = FALSE)

fm_score(x, y = NULL)

mi_score(x, y = NULL)

normalized_mi_score(x, y = NULL)

adjusted_mi_score(x, y = NULL, clipped = FALSE)

normalized_confusion_matrix(x, y = NULL)

normalizing_permutation(x, y = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="compare_partitions_+3A_x">x</code></td>
<td>
<p>an integer vector of length n (or an object coercible to)
representing a K-partition of an n-set (e.g., a reference partition),
or a confusion matrix with K rows and L columns
(see <code><a href="base.html#topic+table">table</a>(x, y)</code>)</p>
</td></tr>
<tr><td><code id="compare_partitions_+3A_y">y</code></td>
<td>
<p>an integer vector of length n (or an object coercible to)
representing an L-partition of the same set (e.g., the output of a
clustering algorithm we wish to compare with <code>x</code>),
or NULL (if x is an K*L confusion matrix)</p>
</td></tr>
<tr><td><code id="compare_partitions_+3A_simplified">simplified</code></td>
<td>
<p>whether to assume E=1 in the definition of the pair sets index index,
i.e., use Eq. (20) in (Rezaei, Franti, 2016) instead of Eq. (18)</p>
</td></tr>
<tr><td><code id="compare_partitions_+3A_clipped">clipped</code></td>
<td>
<p>whether the result should be clipped to the unit interval, i.e., [0, 1]</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>normalized_clustering_accuracy()</code> (Gagolewski, 2023)
is an asymmetric external cluster validity measure
which assumes that the label vector <code>x</code> (or rows in the confusion
matrix) represents the reference (ground truth) partition.
It is an average proportion of correctly classified points in each cluster
above the worst case scenario of uniform membership assignment,
with cluster ID matching based on the solution to the maximal linear
sum assignment problem; see <code><a href="#topic+normalized_confusion_matrix">normalized_confusion_matrix</a></code>).
It is given by:
<code class="reqn">\max_\sigma \frac{1}{K} \sum_{j=1}^K \frac{c_{\sigma(j), j}-c_{\sigma(j),\cdot}/K}{c_{\sigma(j),\cdot}-c_{\sigma(j),\cdot}/K}</code>,
where <code class="reqn">C</code> is a confusion matrix with <code class="reqn">K</code> rows and <code class="reqn">L</code> columns,
<code class="reqn">\sigma</code> is a permutation of the set <code class="reqn">\{1,\dots,\max(K,L)\}</code>, and
<code class="reqn">c_{i, \cdot}=c_{i, 1}+...+c_{i, L}</code> is the i-th row sum,
under the assumption that <code class="reqn">c_{i,j}=0</code> for <code class="reqn">i&gt;K</code> or <code class="reqn">j&gt;L</code>
and <code class="reqn">0/0=0</code>.
</p>
<p><code>normalized_pivoted_accuracy()</code> is defined as
<code class="reqn">(\max_\sigma \sum_{j=1}^{\max(K,L)} c_{\sigma(j),j}/n-1/\max(K,L))/(1-1/\max(K,L))</code>,
where <code class="reqn">\sigma</code> is a permutation of the set <code class="reqn">\{1,\dots,\max(K,L)\}</code>,
and <code class="reqn">n</code> is the sum of all elements in <code class="reqn">C</code>.
For non-square matrices, missing rows/columns are assumed
to be filled with 0s.
</p>
<p><code>pair_sets_index()</code> (PSI) was introduced in (Rezaei, Franti, 2016).
The simplified PSI assumes E=1 in the definition of the index,
i.e., uses Eq. (20) in the said paper instead of Eq. (18).
For non-square matrices, missing rows/columns are assumed
to be filled with 0s.
</p>
<p><code>rand_score()</code> gives the Rand score (the &quot;probability&quot; of agreement
between the two partitions) and
<code>adjusted_rand_score()</code> is its version corrected for chance,
see (Hubert, Arabie, 1985): its expected value is 0 given two independent
partitions. Due to the adjustment, the resulting index may be negative
for some inputs.
</p>
<p>Similarly, <code>fm_score()</code> gives the Fowlkes-Mallows (FM) score
and <code>adjusted_fm_score()</code> is its adjusted-for-chance version;
see (Hubert, Arabie, 1985).
</p>
<p><code>mi_score()</code>, <code>adjusted_mi_score()</code> and
<code>normalized_mi_score()</code> are information-theoretic
scores, based on mutual information,
see the definition of <code class="reqn">AMI_{sum}</code> and <code class="reqn">NMI_{sum}</code>
in (Vinh et al., 2010).
</p>
<p><code>normalized_confusion_matrix()</code> computes the confusion matrix
and permutes its rows and columns so that the sum of the elements
of the main diagonal is the largest possible (by solving
the maximal assignment problem).
The function only accepts <code class="reqn">K \leq L</code>.
The reordering of the columns of a confusion matrix can be determined
by calling <code>normalizing_permutation()</code>.
</p>
<p>Also note that the built-in
<code><a href="base.html#topic+table">table</a>()</code> determines the standard confusion matrix.
</p>


<h3>Value</h3>

<p>Each cluster validity measure is a single numeric value.
</p>
<p><code>normalized_confusion_matrix()</code> returns a numeric matrix.
</p>
<p><code>normalizing_permutation()</code> returns a vector of indexes.
</p>


<h3>Author(s)</h3>

<p><a href="https://www.gagolewski.com/">Marek Gagolewski</a> and other contributors
</p>


<h3>References</h3>

<p>Gagolewski M., <em>A Framework for Benchmarking Clustering Algorithms</em>,
2022, <a href="https://clustering-benchmarks.gagolewski.com">https://clustering-benchmarks.gagolewski.com</a>.
</p>
<p>Gagolewski M., Normalised clustering accuracy: An asymmetric external
cluster validity measure, 2023, under review (preprint),
<a href="https://doi.org/10.48550/arXiv.2209.02935">doi:10.48550/arXiv.2209.02935</a>.
</p>
<p>Hubert L., Arabie P., Comparing partitions,
<em>Journal of Classification</em> 2(1), 1985, 193-218, esp. Eqs. (2) and (4).
</p>
<p>Meila M., Heckerman D., An experimental comparison of model-based clustering
methods, <em>Machine Learning</em> 42, 2001, pp. 9-29,
<a href="https://doi.org/10.1023/A%3A1007648401407">doi:10.1023/A:1007648401407</a>.
</p>
<p>Rezaei M., Franti P., Set matching measures for external cluster validity,
<em>IEEE Transactions on Knowledge and Data Mining</em> 28(8), 2016,
2173-2186.
</p>
<p>Steinley D., Properties of the Hubert-Arabie adjusted Rand index,
<em>Psychological Methods</em> 9(3), 2004, pp. 386-396,
<a href="https://doi.org/10.1037/1082-989X.9.3.386">doi:10.1037/1082-989X.9.3.386</a>.
</p>
<p>Vinh N.X., Epps J., Bailey J.,
Information theoretic measures for clusterings comparison:
Variants, properties, normalization and correction for chance,
<em>Journal of Machine Learning Research</em> 11, 2010, 2837-2854.
</p>


<h3>See Also</h3>

<p>The official online manual of <span class="pkg">genieclust</span> at <a href="https://genieclust.gagolewski.com/">https://genieclust.gagolewski.com/</a>
</p>
<p>Gagolewski M., <span class="pkg">genieclust</span>: Fast and robust hierarchical clustering, <em>SoftwareX</em> 15:100722, 2021, <a href="https://doi.org/10.1016/j.softx.2021.100722">doi:10.1016/j.softx.2021.100722</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y_true &lt;- iris[[5]]
y_pred &lt;- kmeans(as.matrix(iris[1:4]), 3)$cluster
normalized_clustering_accuracy(y_true, y_pred)
normalized_pivoted_accuracy(y_true, y_pred)
pair_sets_index(y_true, y_pred)
pair_sets_index(y_true, y_pred, simplified=TRUE)
adjusted_rand_score(y_true, y_pred)
rand_score(table(y_true, y_pred)) # the same
adjusted_fm_score(y_true, y_pred)
fm_score(y_true, y_pred)
mi_score(y_true, y_pred)
normalized_mi_score(y_true, y_pred)
adjusted_mi_score(y_true, y_pred)
normalized_confusion_matrix(y_true, y_pred)
normalizing_permutation(y_true, y_pred)

</code></pre>

<hr>
<h2 id='emst_mlpack'>Euclidean Minimum Spanning Tree</h2><span id='topic+emst_mlpack'></span>

<h3>Description</h3>

<p>Provides access to the implementation of the Dual-Tree Boruvka
algorithm from the <code>mlpack</code> package (if available).
It is based on kd-trees and is fast for (very) low-dimensional
Euclidean spaces. For higher dimensional spaces (say, over 5 features)
or other metrics, use the parallelised Prim-like algorithm implemented
in <code><a href="#topic+mst">mst</a>()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>emst_mlpack(X, leaf_size = 1, naive = FALSE, verbose = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="emst_mlpack_+3A_x">X</code></td>
<td>
<p>a numeric matrix (or an object coercible to one,
e.g., a data frame with numeric-like columns)</p>
</td></tr>
<tr><td><code id="emst_mlpack_+3A_leaf_size">leaf_size</code></td>
<td>
<p>size of leaves in the kd-tree,
controls the trade-off between speed and memory consumption</p>
</td></tr>
<tr><td><code id="emst_mlpack_+3A_naive">naive</code></td>
<td>
<p>logical; whether to use the naive, quadratic-time algorithm</p>
</td></tr>
<tr><td><code id="emst_mlpack_+3A_verbose">verbose</code></td>
<td>
<p>logical; whether to print diagnostic messages</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>mst</code>, see <code><a href="#topic+mst">mst</a>()</code> for details.
</p>


<h3>Author(s)</h3>

<p><a href="https://www.gagolewski.com/">Marek Gagolewski</a> and other contributors
</p>


<h3>References</h3>

<p>March W.B., Ram P., Gray A.G.,
Fast Euclidean Minimum Spanning Tree: Algorithm, Analysis, and Applications,
<em>Proc. ACM SIGKDD'10</em>, 2010, 603-611,
<a href="https://mlpack.org/papers/emst.pdf">https://mlpack.org/papers/emst.pdf</a>.
</p>
<p>Curtin R.R., Edel M., Lozhnikov M., Mentekidis Y., Ghaisas S., Zhang S.,
mlpack 3: A fast, flexible machine learning library,
<em>Journal of Open Source Software</em> 3(26), 2018, 726.
</p>


<h3>See Also</h3>

<p>The official online manual of <span class="pkg">genieclust</span> at <a href="https://genieclust.gagolewski.com/">https://genieclust.gagolewski.com/</a>
</p>
<p>Gagolewski M., <span class="pkg">genieclust</span>: Fast and robust hierarchical clustering, <em>SoftwareX</em> 15:100722, 2021, <a href="https://doi.org/10.1016/j.softx.2021.100722">doi:10.1016/j.softx.2021.100722</a>.
</p>

<hr>
<h2 id='gclust'>Hierarchical Clustering Algorithm Genie</h2><span id='topic+gclust'></span><span id='topic+gclust.default'></span><span id='topic+gclust.dist'></span><span id='topic+gclust.mst'></span><span id='topic+genie'></span><span id='topic+genie.default'></span><span id='topic+genie.dist'></span><span id='topic+genie.mst'></span>

<h3>Description</h3>

<p>A reimplementation of <em>Genie</em> - a robust and outlier resistant
clustering algorithm (see Gagolewski, Bartoszuk, Cena, 2016).
The Genie algorithm is based on a minimum spanning tree (MST) of the
pairwise distance graph of a given point set.
Just like the single linkage, it consumes the edges
of the MST in an increasing order of weights. However, it prevents
the formation of clusters of highly imbalanced sizes; once the Gini index
(see <code><a href="#topic+gini_index">gini_index</a>()</code>) of the cluster size distribution
raises above <code>gini_threshold</code>, a forced merge of a point group
of the smallest size is performed. Its appealing simplicity goes hand
in hand with its usability; Genie often outperforms
other clustering approaches on benchmark data,
such as <a href="https://github.com/gagolews/clustering-benchmarks">https://github.com/gagolews/clustering-benchmarks</a>.
</p>
<p>The clustering can now also be computed with respect to the
mutual reachability distance (based, e.g., on the Euclidean metric),
which is used in the definition of the HDBSCAN* algorithm
(see Campello et al., 2013). If <code>M</code> &gt; 1, then the mutual reachability
distance <code class="reqn">m(i,j)</code> with smoothing factor <code>M</code> is used instead of the
chosen &quot;raw&quot; distance <code class="reqn">d(i,j)</code>. It holds <code class="reqn">m(i,j)=\max(d(i,j), c(i), c(j))</code>,
where <code class="reqn">c(i)</code> is <code class="reqn">d(i,k)</code> with <code class="reqn">k</code> being the
(<code>M</code>-1)-th nearest neighbour of <code class="reqn">i</code>.
This makes &quot;noise&quot; and &quot;boundary&quot; points being &quot;pulled away&quot; from each other.
</p>
<p>The Genie correction together with the smoothing factor <code>M</code> &gt; 1 (note that
<code>M</code> = 2 corresponds to the original distance) gives a robustified version of
the HDBSCAN* algorithm that is able to detect a predefined number of
clusters. Hence it does not dependent on the DBSCAN's somewhat magical
<code>eps</code> parameter or the HDBSCAN's <code>min_cluster_size</code> one.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gclust(d, ...)

## Default S3 method:
gclust(
  d,
  gini_threshold = 0.3,
  distance = c("euclidean", "l2", "manhattan", "cityblock", "l1", "cosine"),
  cast_float32 = TRUE,
  verbose = FALSE,
  ...
)

## S3 method for class 'dist'
gclust(d, gini_threshold = 0.3, verbose = FALSE, ...)

## S3 method for class 'mst'
gclust(d, gini_threshold = 0.3, verbose = FALSE, ...)

genie(d, ...)

## Default S3 method:
genie(
  d,
  k,
  gini_threshold = 0.3,
  distance = c("euclidean", "l2", "manhattan", "cityblock", "l1", "cosine"),
  M = 1L,
  postprocess = c("boundary", "none", "all"),
  detect_noise = M &gt; 1L,
  cast_float32 = TRUE,
  verbose = FALSE,
  ...
)

## S3 method for class 'dist'
genie(
  d,
  k,
  gini_threshold = 0.3,
  M = 1L,
  postprocess = c("boundary", "none", "all"),
  detect_noise = M &gt; 1L,
  verbose = FALSE,
  ...
)

## S3 method for class 'mst'
genie(
  d,
  k,
  gini_threshold = 0.3,
  postprocess = c("boundary", "none", "all"),
  detect_noise = FALSE,
  verbose = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gclust_+3A_d">d</code></td>
<td>
<p>a numeric matrix (or an object coercible to one,
e.g., a data frame with numeric-like columns) or an
object of class <code>dist</code>, see <code><a href="stats.html#topic+dist">dist</a></code>
or an object of class <code>mst</code>, see <code><a href="#topic+mst">mst</a>()</code>.</p>
</td></tr>
<tr><td><code id="gclust_+3A_...">...</code></td>
<td>
<p>further arguments passed to other methods.</p>
</td></tr>
<tr><td><code id="gclust_+3A_gini_threshold">gini_threshold</code></td>
<td>
<p>threshold for the Genie correction, i.e.,
the Gini index of the cluster size distribution;
Threshold of 1.0 disables the correction.
Low thresholds highly penalise the formation of small clusters.</p>
</td></tr>
<tr><td><code id="gclust_+3A_distance">distance</code></td>
<td>
<p>metric used to compute the linkage, one of:
<code>"euclidean"</code> (synonym: <code>"l2"</code>),
<code>"manhattan"</code> (a.k.a. <code>"l1"</code> and <code>"cityblock"</code>),
<code>"cosine"</code>.</p>
</td></tr>
<tr><td><code id="gclust_+3A_cast_float32">cast_float32</code></td>
<td>
<p>logical; whether to compute the distances using 32-bit
instead of 64-bit precision floating-point arithmetic (up to 2x faster).</p>
</td></tr>
<tr><td><code id="gclust_+3A_verbose">verbose</code></td>
<td>
<p>logical; whether to print diagnostic messages
and progress information.</p>
</td></tr>
<tr><td><code id="gclust_+3A_k">k</code></td>
<td>
<p>the desired number of clusters to detect, <code>k</code> = 1 with <code>M</code> &gt; 1
acts as a noise point detector.</p>
</td></tr>
<tr><td><code id="gclust_+3A_m">M</code></td>
<td>
<p>smoothing factor; <code>M</code> &lt;= 2 gives the selected <code>distance</code>;
otherwise, the mutual reachability distance is used.</p>
</td></tr>
<tr><td><code id="gclust_+3A_postprocess">postprocess</code></td>
<td>
<p>one of <code>"boundary"</code> (default), <code>"none"</code>
or <code>"all"</code>;  in effect only if <code>M</code> &gt; 1.
By default, only &quot;boundary&quot; points are merged
with their nearest &quot;core&quot; points (A point is a boundary point if it is
a noise point and it's amongst its adjacent vertex's
<code>M</code>-1 nearest neighbours). To force a classical
k-partition of a data set (with no notion of noise),
choose &quot;all&quot;.</p>
</td></tr>
<tr><td><code id="gclust_+3A_detect_noise">detect_noise</code></td>
<td>
<p>whether the minimum spanning tree's leaves
should be marked as noise points, defaults to <code>TRUE</code> if <code>M</code> &gt; 1
for compatibility with HDBSCAN*.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that, as in the case of all the distance-based methods,
the standardisation of the input features is definitely worth giving a try.
</p>
<p>If <code>d</code> is a numeric matrix or an object of class <code>dist</code>,
<code><a href="#topic+mst">mst</a>()</code> will be called to compute an MST, which generally
takes at most <code class="reqn">O(n^2)</code> time (the algorithm we provide is parallelised,
environment variable <code>OMP_NUM_THREADS</code> controls the number of threads
in use). However, see <code><a href="#topic+emst_mlpack">emst_mlpack</a>()</code> for a very fast alternative
in the case of Euclidean spaces of (very) low dimensionality and <code>M</code> = 1.
</p>
<p>Given an minimum spanning tree, the algorithm runs in <code class="reqn">O(n \sqrt{n})</code> time.
Therefore, if you want to test different <code>gini_threshold</code>s,
(or <code>k</code>s), it is best to explicitly compute the MST first.
</p>
<p>According to the algorithm's original definition,
the resulting partition tree (dendrogram) might violate
the ultrametricity property (merges might occur at levels that
are not increasing w.r.t. a between-cluster distance).
Departures from ultrametricity are corrected by applying
<code>height = rev(cummin(rev(height)))</code>.
</p>


<h3>Value</h3>

<p><code>gclust()</code> computes the whole clustering hierarchy; it
returns a list of class <code>hclust</code>,
see <code><a href="stats.html#topic+hclust">hclust</a></code>. Use <code><a href="stats.html#topic+cutree">cutree</a></code> to obtain
an arbitrary k-partition.
</p>
<p><code>genie()</code> returns a <code>k</code>-partition - a vector with elements in 1,...,k,
whose i-th element denotes the i-th input point's cluster identifier.
Missing values (<code>NA</code>) denote noise points (if <code>detect_noise</code>
is <code>TRUE</code>).
</p>


<h3>Author(s)</h3>

<p><a href="https://www.gagolewski.com/">Marek Gagolewski</a> and other contributors
</p>


<h3>References</h3>

<p>Gagolewski M., Bartoszuk M., Cena A.,
Genie: A new, fast, and outlier-resistant hierarchical clustering algorithm,
<em>Information Sciences</em> 363, 2016, 8-23,
<a href="https://doi.org/10.1016/j.ins.2016.05.003">doi:10.1016/j.ins.2016.05.003</a>.
</p>
<p>Campello R.J.G.B., Moulavi D., Sander J.,
Density-based clustering based on hierarchical density estimates,
<em>Lecture Notes in Computer Science</em> 7819, 2013, 160-172,
<a href="https://doi.org/10.1007/978-3-642-37456-2_14">doi:10.1007/978-3-642-37456-2_14</a>.
</p>
<p>Gagolewski M., Cena A., Bartoszuk M., Brzozowski L.,
<em>Clustering with minimum spanning trees: How good can it be?</em>,
2023, under review (preprint), <a href="https://doi.org/10.48550/arXiv.2303.05679">doi:10.48550/arXiv.2303.05679</a>.
</p>


<h3>See Also</h3>

<p>The official online manual of <span class="pkg">genieclust</span> at <a href="https://genieclust.gagolewski.com/">https://genieclust.gagolewski.com/</a>
</p>
<p>Gagolewski M., <span class="pkg">genieclust</span>: Fast and robust hierarchical clustering, <em>SoftwareX</em> 15:100722, 2021, <a href="https://doi.org/10.1016/j.softx.2021.100722">doi:10.1016/j.softx.2021.100722</a>.
</p>
<p><code><a href="#topic+mst">mst</a>()</code> for the minimum spanning tree routines.
</p>
<p><code><a href="#topic+adjusted_rand_score">adjusted_rand_score</a>()</code> (amongst others) for external
cluster validity measures (partition similarity scores).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library("datasets")
data("iris")
X &lt;- iris[1:4]
h &lt;- gclust(X)
y_pred &lt;- cutree(h, 3)
y_test &lt;- iris[,5]
plot(iris[,2], iris[,3], col=y_pred,
   pch=as.integer(iris[,5]), asp=1, las=1)
adjusted_rand_score(y_test, y_pred)
pair_sets_index(y_test, y_pred)

# Fast for low-dimensional Euclidean spaces:
# h &lt;- gclust(emst_mlpack(X))

</code></pre>

<hr>
<h2 id='genieclust-package'>The Genie Hierarchical Clustering Algorithm (with Extras)</h2><span id='topic+genieclust-package'></span><span id='topic+genieclust'></span>

<h3>Description</h3>

<p>See <code><a href="#topic+genie">genie</a>()</code> for more details.
</p>


<h3>Author(s)</h3>

<p><a href="https://www.gagolewski.com/">Marek Gagolewski</a> and other contributors
</p>


<h3>See Also</h3>

<p>The official online manual of <span class="pkg">genieclust</span> at <a href="https://genieclust.gagolewski.com/">https://genieclust.gagolewski.com/</a>
</p>
<p>Gagolewski M., <span class="pkg">genieclust</span>: Fast and robust hierarchical clustering, <em>SoftwareX</em> 15:100722, 2021, <a href="https://doi.org/10.1016/j.softx.2021.100722">doi:10.1016/j.softx.2021.100722</a>.
</p>

<hr>
<h2 id='inequality'>Inequality Measures</h2><span id='topic+inequality'></span><span id='topic+gini_index'></span><span id='topic+bonferroni_index'></span><span id='topic+devergottini_index'></span>

<h3>Description</h3>

<p><code>gini_index()</code> gives the normalised Gini index,
<code>bonferroni_index()</code> implements the Bonferroni index, and
<code>devergottini_index()</code> implements the De Vergottini index.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gini_index(x)

bonferroni_index(x)

devergottini_index(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="inequality_+3A_x">x</code></td>
<td>
<p>numeric vector of non-negative values</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These indices can be used to quantify the &quot;inequality&quot; of a numeric sample.
They can be conceived as normalised measures of data dispersion.
For constant vectors (perfect equity), the indices yield values of 0.
Vectors with all elements but one equal to 0 (perfect inequality),
are assigned scores of 1.
They follow the Pigou-Dalton principle (are Schur-convex):
setting <code class="reqn">x_i = x_i - h</code> and <code class="reqn">x_j = x_j + h</code> with <code class="reqn">h &gt; 0</code>
and <code class="reqn">x_i - h \geq  x_j + h</code> (taking from the &quot;rich&quot; and
giving to the &quot;poor&quot;) decreases the inequality
</p>
<p>These indices have applications in economics, amongst others.
The Genie clustering algorithm uses the Gini index as a measure
of the inequality of cluster sizes.
</p>
<p>The normalised Gini index is given by:
</p>
<p style="text-align: center;"><code class="reqn">
    G(x_1,\dots,x_n) = \frac{
    \sum_{i=1}^{n} (n-2i+1) x_{\sigma(n-i+1)}
    }{
    (n-1) \sum_{i=1}^n x_i
    },
</code>
</p>

<p>The normalised Bonferroni index is given by:
</p>
<p style="text-align: center;"><code class="reqn">
    B(x_1,\dots,x_n) = \frac{
    \sum_{i=1}^{n}  (n-\sum_{j=1}^i \frac{n}{n-j+1})
         x_{\sigma(n-i+1)}
    }{
    (n-1) \sum_{i=1}^n x_i
    }.
</code>
</p>

<p>The normalised De Vergottini index is given by:
</p>
<p style="text-align: center;"><code class="reqn">
    V(x_1,\dots,x_n) =
    \frac{1}{\sum_{i=2}^n \frac{1}{i}} \left(
       \frac{ \sum_{i=1}^n \left( \sum_{j=i}^{n} \frac{1}{j}\right)
       x_{\sigma(n-i+1)} }{\sum_{i=1}^{n} x_i} - 1
    \right).
</code>
</p>

<p>Here, <code class="reqn">\sigma</code> is an ordering permutation of <code class="reqn">(x_1,\dots,x_n)</code>.
</p>
<p>Time complexity: <code class="reqn">O(n)</code> for sorted (increasingly) data.
Otherwise, the vector will be sorted.
</p>


<h3>Value</h3>

<p>The value of the inequality index, a number in <code class="reqn">[0, 1]</code>.
</p>


<h3>Author(s)</h3>

<p><a href="https://www.gagolewski.com/">Marek Gagolewski</a> and other contributors
</p>


<h3>References</h3>

<p>Bonferroni C., <em>Elementi di Statistica Generale</em>, Libreria Seber,
Firenze, 1930.
</p>
<p>Gagolewski M., Bartoszuk M., Cena A., Genie: A new, fast, and
outlier-resistant hierarchical clustering algorithm,
<em>Information Sciences</em> 363, 2016, pp. 8-23.
<a href="https://doi.org/10.1016/j.ins.2016.05.003">doi:10.1016/j.ins.2016.05.003</a>
</p>
<p>Gini C., <em>Variabilita e Mutabilita</em>,
Tipografia di Paolo Cuppini, Bologna, 1912.
</p>


<h3>See Also</h3>

<p>The official online manual of <span class="pkg">genieclust</span> at <a href="https://genieclust.gagolewski.com/">https://genieclust.gagolewski.com/</a>
</p>
<p>Gagolewski M., <span class="pkg">genieclust</span>: Fast and robust hierarchical clustering, <em>SoftwareX</em> 15:100722, 2021, <a href="https://doi.org/10.1016/j.softx.2021.100722">doi:10.1016/j.softx.2021.100722</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>gini_index(c(2, 2, 2, 2, 2))   # no inequality
gini_index(c(0, 0, 10, 0, 0))  # one has it all
gini_index(c(7, 0, 3, 0, 0))   # give to the poor, take away from the rich
gini_index(c(6, 0, 3, 1, 0))   # (a.k.a. Pigou-Dalton principle)
bonferroni_index(c(2, 2, 2, 2, 2))
bonferroni_index(c(0, 0, 10, 0, 0))
bonferroni_index(c(7, 0, 3, 0, 0))
bonferroni_index(c(6, 0, 3, 1, 0))
devergottini_index(c(2, 2, 2, 2, 2))
devergottini_index(c(0, 0, 10, 0, 0))
devergottini_index(c(7, 0, 3, 0, 0))
devergottini_index(c(6, 0, 3, 1, 0))

</code></pre>

<hr>
<h2 id='mst'>Minimum Spanning Tree of the Pairwise Distance Graph</h2><span id='topic+mst'></span><span id='topic+mst.default'></span><span id='topic+mst.dist'></span>

<h3>Description</h3>

<p>An parallelised implementation of a Jarnik (Prim/Dijkstra)-like
algorithm for determining
a(*) minimum spanning tree (MST) of a complete undirected graph
representing a set of n points
with weights given by a pairwise distance matrix.
</p>
<p>(*) Note that there might be multiple minimum trees spanning a given graph.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mst(d, ...)

## Default S3 method:
mst(
  d,
  distance = c("euclidean", "l2", "manhattan", "cityblock", "l1", "cosine"),
  M = 1L,
  cast_float32 = TRUE,
  verbose = FALSE,
  ...
)

## S3 method for class 'dist'
mst(d, M = 1L, verbose = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mst_+3A_d">d</code></td>
<td>
<p>either a numeric matrix (or an object coercible to one,
e.g., a data frame with numeric-like columns) or an
object of class <code>dist</code>, see <code><a href="stats.html#topic+dist">dist</a></code></p>
</td></tr>
<tr><td><code id="mst_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods</p>
</td></tr>
<tr><td><code id="mst_+3A_distance">distance</code></td>
<td>
<p>metric used to compute the linkage, one of:
<code>"euclidean"</code> (synonym: <code>"l2"</code>),
<code>"manhattan"</code> (a.k.a. <code>"l1"</code> and <code>"cityblock"</code>),
<code>"cosine"</code></p>
</td></tr>
<tr><td><code id="mst_+3A_m">M</code></td>
<td>
<p>smoothing factor; <code>M</code> = 1 gives the selected <code>distance</code>;
otherwise, the mutual reachability distance is used</p>
</td></tr>
<tr><td><code id="mst_+3A_cast_float32">cast_float32</code></td>
<td>
<p>logical; whether to compute the distances using 32-bit
instead of 64-bit precision floating-point arithmetic (up to 2x faster)</p>
</td></tr>
<tr><td><code id="mst_+3A_verbose">verbose</code></td>
<td>
<p>logical; whether to print diagnostic messages
and progress information</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>d</code> is a numeric matrix of size <code class="reqn">n p</code>,
the <code class="reqn">n (n-1)/2</code> distances are computed on the fly, so that <code class="reqn">O(n M)</code>
memory is used.
</p>
<p>The algorithm is parallelised; set the <code>OMP_NUM_THREADS</code> environment
variable <code><a href="base.html#topic+Sys.setenv">Sys.setenv</a></code> to control the number of threads
used.
</p>
<p>Time complexity is <code class="reqn">O(n^2)</code> for the method accepting an object of
class <code>dist</code> and <code class="reqn">O(p n^2)</code> otherwise.
</p>
<p>If <code>M</code> &gt;= 2, then the mutual reachability distance <code class="reqn">m(i,j)</code> with smoothing
factor <code>M</code> (see Campello et al. 2013)
is used instead of the chosen &quot;raw&quot; distance <code class="reqn">d(i,j)</code>.
It holds <code class="reqn">m(i, j)=\max(d(i,j), c(i), c(j))</code>, where <code class="reqn">c(i)</code> is
<code class="reqn">d(i, k)</code> with <code class="reqn">k</code> being the (<code>M</code>-1)-th nearest neighbour of <code class="reqn">i</code>.
This makes &quot;noise&quot; and &quot;boundary&quot; points being &quot;pulled away&quot; from each other.
Genie++ clustering algorithm (see <code><a href="#topic+gclust">gclust</a></code>)
with respect to the mutual reachability distance gains the ability to
identify some observations are noise points.
</p>
<p>Note that the case <code>M</code> = 2 corresponds to the original distance, but we are
determining the 1-nearest neighbours separately as well, which is a bit
suboptimal; you can file a feature request if this makes your data analysis
tasks too slow.
</p>


<h3>Value</h3>

<p>Matrix of class <code>mst</code> with n-1 rows and 3 columns:
<code>from</code>, <code>to</code> and <code>dist</code>. It holds <code>from</code> &lt; <code>to</code>.
Moreover, <code>dist</code> is sorted nondecreasingly.
The i-th row gives the i-th edge of the MST.
<code>(from[i], to[i])</code> defines the vertices (in 1,...,n)
and <code>dist[i]</code> gives the weight, i.e., the
distance between the corresponding points.
</p>
<p>The <code>method</code> attribute gives the name of the distance used.
The <code>Labels</code> attribute gives the labels of all the input points.
</p>
<p>If <code>M</code> &gt; 1, the <code>nn</code> attribute gives the indices of the <code>M</code>-1
nearest neighbours of each point.
</p>


<h3>Author(s)</h3>

<p><a href="https://www.gagolewski.com/">Marek Gagolewski</a> and other contributors
</p>


<h3>References</h3>

<p>Jarnik V., O jistem problemu minimalnim,
<em>Prace Moravske Prirodovedecke Spolecnosti</em> 6, 1930, 57-63.
</p>
<p>Olson C.F., Parallel algorithms for hierarchical clustering,
<em>Parallel Comput.</em> 21, 1995, 1313-1325.
</p>
<p>Prim R., Shortest connection networks and some generalisations,
<em>Bell Syst. Tech. J.</em> 36, 1957, 1389-1401.
</p>
<p>Campello R.J.G.B., Moulavi D., Sander J.,
Density-based clustering based on hierarchical density estimates,
<em>Lecture Notes in Computer Science</em> 7819, 2013, 160-172,
<a href="https://doi.org/10.1007/978-3-642-37456-2_14">doi:10.1007/978-3-642-37456-2_14</a>.
</p>


<h3>See Also</h3>

<p>The official online manual of <span class="pkg">genieclust</span> at <a href="https://genieclust.gagolewski.com/">https://genieclust.gagolewski.com/</a>
</p>
<p>Gagolewski M., <span class="pkg">genieclust</span>: Fast and robust hierarchical clustering, <em>SoftwareX</em> 15:100722, 2021, <a href="https://doi.org/10.1016/j.softx.2021.100722">doi:10.1016/j.softx.2021.100722</a>.
</p>
<p><code><a href="#topic+emst_mlpack">emst_mlpack</a>()</code> for a very fast alternative
in case of (very) low-dimensional Euclidean spaces (and <code>M</code> = 1).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library("datasets")
data("iris")
X &lt;- iris[1:4]
tree &lt;- mst(X)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
