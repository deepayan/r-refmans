<!DOCTYPE html><html><head><title>Help for package loo</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {loo}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#loo-package'><p>Efficient LOO-CV and WAIC for Bayesian models</p></a></li>
<li><a href='#.compute_point_estimate'><p>Compute a point estimate from a draws object</p></a></li>
<li><a href='#.ndraws'><p>The number of posterior draws in a draws object.</p></a></li>
<li><a href='#.thin_draws'><p>Thin a draws object</p></a></li>
<li><a href='#ap_psis'><p>Pareto smoothed importance sampling (PSIS)</p>
using approximate posteriors</a></li>
<li><a href='#compare'><p>Model comparison (deprecated, old version)</p></a></li>
<li><a href='#crps'><p>Continuously ranked probability score</p></a></li>
<li><a href='#E_loo'><p>Compute weighted expectations</p></a></li>
<li><a href='#elpd'><p>Generic (expected) log-predictive density</p></a></li>
<li><a href='#example_loglik_array'><p>Objects to use in examples and tests</p></a></li>
<li><a href='#extract_log_lik'><p>Extract pointwise log-likelihood from a Stan model</p></a></li>
<li><a href='#find_model_names'><p>Find the model names associated with <code>"loo"</code> objects</p></a></li>
<li><a href='#gpdfit'><p>Estimate parameters of the Generalized Pareto distribution</p></a></li>
<li><a href='#importance_sampling'><p>A parent class for different importance sampling methods.</p></a></li>
<li><a href='#kfold-generic'><p>Generic function for K-fold cross-validation for developers</p></a></li>
<li><a href='#kfold-helpers'><p>Helper functions for K-fold cross-validation</p></a></li>
<li><a href='#loo'><p>Efficient approximate leave-one-out cross-validation (LOO)</p></a></li>
<li><a href='#loo_approximate_posterior'><p>Efficient approximate leave-one-out cross-validation (LOO) for posterior</p>
approximations</a></li>
<li><a href='#loo_compare'><p>Model comparison</p></a></li>
<li><a href='#loo_model_weights'><p>Model averaging/weighting via stacking or pseudo-BMA weighting</p></a></li>
<li><a href='#loo_moment_match'><p>Moment matching for efficient approximate leave-one-out cross-validation (LOO)</p></a></li>
<li><a href='#loo_moment_match_split'><p>Split moment matching for efficient approximate leave-one-out cross-validation (LOO)</p></a></li>
<li><a href='#loo_predictive_metric'><p>Estimate leave-one-out predictive performance..</p></a></li>
<li><a href='#loo_subsample'><p>Efficient approximate leave-one-out cross-validation (LOO) using subsampling,</p>
so that less costly and more approximate computation is made for all LOO-fold,
and more costly and accurate computations are made only for m&lt;N LOO-folds.</a></li>
<li><a href='#loo-datasets'><p>Datasets for loo examples and vignettes</p></a></li>
<li><a href='#loo-glossary'><p>LOO package glossary</p></a></li>
<li><a href='#nlist'><p>Named lists</p></a></li>
<li><a href='#nobs.psis_loo_ss'><p>The number of observations in a <code>psis_loo_ss</code> object.</p></a></li>
<li><a href='#obs_idx'><p>Get observation indices used in subsampling</p></a></li>
<li><a href='#old-extractors'><p>Extractor methods</p></a></li>
<li><a href='#parallel_psis_list'><p>Parallel psis list computations</p></a></li>
<li><a href='#pareto-k-diagnostic'><p>Diagnostics for Pareto smoothed importance sampling (PSIS)</p></a></li>
<li><a href='#pointwise'><p>Convenience function for extracting pointwise estimates</p></a></li>
<li><a href='#print_dims'><p>Print dimensions of log-likelihood or log-weights matrix</p></a></li>
<li><a href='#print.loo'><p>Print methods</p></a></li>
<li><a href='#psis'><p>Pareto smoothed importance sampling (PSIS)</p></a></li>
<li><a href='#psis_approximate_posterior'><p>Diagnostics for Laplace and ADVI approximations and Laplace-loo and ADVI-loo</p></a></li>
<li><a href='#psislw'><p>Pareto smoothed importance sampling (deprecated, old version)</p></a></li>
<li><a href='#relative_eff'><p>Convenience function for computing relative efficiencies</p></a></li>
<li><a href='#sis'><p>Standard importance sampling (SIS)</p></a></li>
<li><a href='#tis'><p>Truncated importance sampling (TIS)</p></a></li>
<li><a href='#update.psis_loo_ss'><p>Update <code>psis_loo_ss</code> objects</p></a></li>
<li><a href='#waic'><p>Widely applicable information criterion (WAIC)</p></a></li>
<li><a href='#weights.importance_sampling'><p>Extract importance sampling weights</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Efficient Leave-One-Out Cross-Validation and WAIC for Bayesian
Models</td>
</tr>
<tr>
<td>Version:</td>
<td>2.8.0</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-07-03</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Jonah Gabry &lt;jsg2201@columbia.edu&gt;</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://mc-stan.org/loo/">https://mc-stan.org/loo/</a>, <a href="https://discourse.mc-stan.org">https://discourse.mc-stan.org</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/stan-dev/loo/issues">https://github.com/stan-dev/loo/issues</a></td>
</tr>
<tr>
<td>Description:</td>
<td>Efficient approximate leave-one-out cross-validation (LOO)
    for Bayesian models fit using Markov chain Monte Carlo, as 
    described in Vehtari, Gelman, and Gabry (2017) 
    &lt;<a href="https://doi.org/10.1007%2Fs11222-016-9696-4">doi:10.1007/s11222-016-9696-4</a>&gt;. 
    The approximation uses Pareto smoothed importance sampling (PSIS), 
    a new procedure for regularizing importance weights. 
    As a byproduct of the calculations, we also obtain approximate 
    standard errors for estimated predictive errors and for the comparison 
    of predictive errors between models. The package also provides methods 
    for using stacking and other model weighting techniques to average 
    Bayesian predictive distributions.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>LazyData:</td>
<td>TRUE</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.1.2)</td>
</tr>
<tr>
<td>Imports:</td>
<td>checkmate, matrixStats (&ge; 0.52), parallel, posterior (&ge;
1.5.0), stats</td>
</tr>
<tr>
<td>Suggests:</td>
<td>bayesplot (&ge; 1.7.0), brms (&ge; 2.10.0), ggplot2, graphics,
knitr, rmarkdown, rstan, rstanarm (&ge; 2.19.0), rstantools,
spdep, testthat (&ge; 2.1.0)</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>pandoc (&gt;= 1.12.3), pandoc-citeproc</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-07-03 16:26:26 UTC; jgabry</td>
</tr>
<tr>
<td>Author:</td>
<td>Aki Vehtari [aut],
  Jonah Gabry [cre, aut],
  Måns Magnusson [aut],
  Yuling Yao [aut],
  Paul-Christian Bürkner [aut],
  Topi Paananen [aut],
  Andrew Gelman [aut],
  Ben Goodrich [ctb],
  Juho Piironen [ctb],
  Bruno Nicenboim [ctb],
  Leevi Lindgren [ctb]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-07-03 18:50:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='loo-package'>Efficient LOO-CV and WAIC for Bayesian models</h2><span id='topic+loo-package'></span>

<h3>Description</h3>


<p><img src="../help/figures/stanlogo.png" width="50" alt="mc-stan.org" />

<em>Stan Development Team</em>
</p>
<p>This package implements the methods described in Vehtari, Gelman, and
Gabry (2017), Vehtari, Simpson, Gelman, Yao, and Gabry (2024), and
Yao et al. (2018). To get started see the <strong>loo</strong> package
<a href="https://mc-stan.org/loo/articles/index.html">vignettes</a>, the
<code><a href="#topic+loo">loo()</a></code> function for efficient approximate leave-one-out
cross-validation (LOO-CV), the <code><a href="#topic+psis">psis()</a></code> function for the Pareto
smoothed importance sampling (PSIS) algorithm, or
<code><a href="#topic+loo_model_weights">loo_model_weights()</a></code> for an implementation of Bayesian stacking of
predictive distributions from multiple models.
</p>


<h3>Details</h3>

<p>Leave-one-out cross-validation (LOO-CV) and the widely applicable
information criterion (WAIC) are methods for estimating pointwise
out-of-sample prediction accuracy from a fitted Bayesian model using the
log-likelihood evaluated at the posterior simulations of the parameter
values. LOO-CV and WAIC have various advantages over simpler estimates of
predictive error such as AIC and DIC but are less used in practice because
they involve additional computational steps. This package implements the
fast and stable computations for approximate LOO-CV laid out in Vehtari,
Gelman, and Gabry (2017). From existing posterior simulation draws, we
compute LOO-CV using Pareto smoothed importance sampling (PSIS; Vehtari,
Simpson, Gelman, Yao, and Gabry, 2024), a new procedure for stabilizing
and diagnosing importance weights. As a byproduct of our calculations,
we also obtain approximate standard errors for estimated predictive
errors and for comparing of predictive errors between two models.
</p>
<p>We recommend PSIS-LOO-CV instead of WAIC, because PSIS provides useful
diagnostics and effective sample size and Monte Carlo standard error
estimates.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Jonah Gabry <a href="mailto:jsg2201@columbia.edu">jsg2201@columbia.edu</a>
</p>
<p>Authors:
</p>

<ul>
<li><p> Aki Vehtari <a href="mailto:Aki.Vehtari@aalto.fi">Aki.Vehtari@aalto.fi</a>
</p>
</li>
<li><p> Måns Magnusson
</p>
</li>
<li><p> Yuling Yao
</p>
</li>
<li><p> Paul-Christian Bürkner
</p>
</li>
<li><p> Topi Paananen
</p>
</li>
<li><p> Andrew Gelman
</p>
</li></ul>

<p>Other contributors:
</p>

<ul>
<li><p> Ben Goodrich [contributor]
</p>
</li>
<li><p> Juho Piironen [contributor]
</p>
</li>
<li><p> Bruno Nicenboim [contributor]
</p>
</li>
<li><p> Leevi Lindgren [contributor]
</p>
</li></ul>



<h3>References</h3>

<p>Vehtari, A., Gelman, A., and Gabry, J. (2017). Practical Bayesian model
evaluation using leave-one-out cross-validation and WAIC.
<em>Statistics and Computing</em>. 27(5), 1413&ndash;1432. doi:10.1007/s11222-016-9696-4
(<a href="https://link.springer.com/article/10.1007/s11222-016-9696-4">journal version</a>,
<a href="https://arxiv.org/abs/1507.04544">preprint arXiv:1507.04544</a>).
</p>
<p>Vehtari, A., Simpson, D., Gelman, A., Yao, Y., and Gabry, J. (2024).
Pareto smoothed importance sampling. <em>Journal of Machine Learning Research</em>,
25(72):1-58.
<a href="https://jmlr.org/papers/v25/19-556.html">PDF</a>
</p>
<p>Yao, Y., Vehtari, A., Simpson, D., and Gelman, A. (2018) Using
stacking to average Bayesian predictive distributions.
<em>Bayesian Analysis</em>, advance publication,  doi:10.1214/17-BA1091.
(<a href="https://projecteuclid.org/euclid.ba/1516093227">online</a>).
</p>
<p>Magnusson, M., Riis Andersen, M., Jonasson, J. and Vehtari, A. (2019).
Leave-One-Out Cross-Validation for Large Data.
In <em>Thirty-sixth International Conference on Machine Learning</em>,
PMLR 97:4244-4253.
</p>
<p>Magnusson, M., Riis Andersen, M., Jonasson, J. and Vehtari, A. (2020).
Leave-One-Out Cross-Validation for Model Comparison in Large Data.
In <em>Proceedings of the 23rd International Conference on Artificial
Intelligence and Statistics (AISTATS)</em>, PMLR 108:341-351.
</p>
<p>Epifani, I., MacEachern, S. N., and Peruggia, M. (2008). Case-deletion
importance sampling estimators: Central limit theorems and related results.
<em>Electronic Journal of Statistics</em> <strong>2</strong>, 774-806.
</p>
<p>Gelfand, A. E. (1996). Model determination using sampling-based methods. In
<em>Markov Chain Monte Carlo in Practice</em>, ed. W. R. Gilks, S. Richardson,
D. J. Spiegelhalter, 145-162. London: Chapman and Hall.
</p>
<p>Gelfand, A. E., Dey, D. K., and Chang, H. (1992). Model determination using
predictive distributions with implementation via sampling-based methods. In
<em>Bayesian Statistics 4</em>, ed. J. M. Bernardo, J. O. Berger, A. P. Dawid,
and A. F. M. Smith, 147-167. Oxford University Press.
</p>
<p>Gelman, A., Hwang, J., and Vehtari, A. (2014). Understanding predictive
information criteria for Bayesian models. <em>Statistics and Computing</em>
<strong>24</strong>, 997-1016.
</p>
<p>Ionides, E. L. (2008). Truncated importance sampling. <em>Journal of
Computational and Graphical Statistics</em> <strong>17</strong>, 295-311.
</p>
<p>Koopman, S. J., Shephard, N., and Creal, D. (2009). Testing the assumptions
behind importance sampling. <em>Journal of Econometrics</em> <strong>149</strong>, 2-11.
</p>
<p>Peruggia, M. (1997). On the variability of case-deletion importance sampling
weights in the Bayesian linear model. <em>Journal of the American
Statistical Association</em> <strong>92</strong>, 199-207.
</p>
<p>Stan Development Team (2017). The Stan C++ Library, Version 2.17.0.
<a href="https://mc-stan.org">https://mc-stan.org</a>.
</p>
<p>Stan Development Team (2018). RStan: the R interface to Stan, Version 2.17.3.
<a href="https://mc-stan.org">https://mc-stan.org</a>.
</p>
<p>Watanabe, S. (2010). Asymptotic equivalence of Bayes cross validation and
widely application information criterion in singular learning theory.
<em>Journal of Machine Learning Research</em> <strong>11</strong>, 3571-3594.
</p>
<p>Zhang, J., and Stephens, M. A. (2009). A new and efficient estimation method
for the generalized Pareto distribution. <em>Technometrics</em> <strong>51</strong>,
316-325.
</p>


<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://mc-stan.org/loo/">https://mc-stan.org/loo/</a>
</p>
</li>
<li> <p><a href="https://discourse.mc-stan.org">https://discourse.mc-stan.org</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/stan-dev/loo/issues">https://github.com/stan-dev/loo/issues</a>
</p>
</li></ul>


<hr>
<h2 id='.compute_point_estimate'>Compute a point estimate from a draws object</h2><span id='topic+.compute_point_estimate'></span><span id='topic+.compute_point_estimate.matrix'></span><span id='topic+.compute_point_estimate.default'></span>

<h3>Description</h3>

<p>Compute a point estimate from a draws object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.compute_point_estimate(draws)

## S3 method for class 'matrix'
.compute_point_estimate(draws)

## Default S3 method:
.compute_point_estimate(draws)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".compute_point_estimate_+3A_draws">draws</code></td>
<td>
<p>A draws object with draws from the posterior.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a generic function to compute point estimates from draws
objects. The function is internal and should only be used by developers to
enable <code><a href="#topic+loo_subsample">loo_subsample()</a></code> for arbitrary draws objects.
</p>


<h3>Value</h3>

<p>A 1 by P matrix with point estimates from a draws object.
</p>

<hr>
<h2 id='.ndraws'>The number of posterior draws in a draws object.</h2><span id='topic+.ndraws'></span><span id='topic+.ndraws.matrix'></span><span id='topic+.ndraws.default'></span>

<h3>Description</h3>

<p>The number of posterior draws in a draws object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.ndraws(x)

## S3 method for class 'matrix'
.ndraws(x)

## Default S3 method:
.ndraws(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".ndraws_+3A_x">x</code></td>
<td>
<p>A draws object with posterior draws.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a generic function to return the total number of draws from
an arbitrary draws objects. The function is internal and should only be
used by developers to enable <code><a href="#topic+loo_subsample">loo_subsample()</a></code> for arbitrary draws objects.
</p>


<h3>Value</h3>

<p>An integer with the number of draws.
</p>

<hr>
<h2 id='.thin_draws'>Thin a draws object</h2><span id='topic+.thin_draws'></span><span id='topic+.thin_draws.matrix'></span><span id='topic+.thin_draws.numeric'></span><span id='topic+.thin_draws.default'></span>

<h3>Description</h3>

<p>Thin a draws object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.thin_draws(draws, loo_approximation_draws)

## S3 method for class 'matrix'
.thin_draws(draws, loo_approximation_draws)

## S3 method for class 'numeric'
.thin_draws(draws, loo_approximation_draws)

## Default S3 method:
.thin_draws(draws, loo_approximation_draws)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".thin_draws_+3A_draws">draws</code></td>
<td>
<p>A draws object with posterior draws.</p>
</td></tr>
<tr><td><code id=".thin_draws_+3A_loo_approximation_draws">loo_approximation_draws</code></td>
<td>
<p>The number of posterior draws to return (ie after thinning).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a generic function to thin draws from arbitrary draws
objects. The function is internal and should only be used by developers to
enable <code><a href="#topic+loo_subsample">loo_subsample()</a></code> for arbitrary draws objects.
</p>


<h3>Value</h3>

<p>A thinned draws object.
</p>

<hr>
<h2 id='ap_psis'>Pareto smoothed importance sampling (PSIS)
using approximate posteriors</h2><span id='topic+ap_psis'></span><span id='topic+ap_psis.array'></span><span id='topic+ap_psis.matrix'></span><span id='topic+ap_psis.default'></span>

<h3>Description</h3>

<p>Pareto smoothed importance sampling (PSIS)
using approximate posteriors
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ap_psis(log_ratios, log_p, log_g, ...)

## S3 method for class 'array'
ap_psis(log_ratios, log_p, log_g, ..., cores = getOption("mc.cores", 1))

## S3 method for class 'matrix'
ap_psis(log_ratios, log_p, log_g, ..., cores = getOption("mc.cores", 1))

## Default S3 method:
ap_psis(log_ratios, log_p, log_g, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ap_psis_+3A_log_ratios">log_ratios</code></td>
<td>
<p>The log-likelihood ratios (ie -log_liks)</p>
</td></tr>
<tr><td><code id="ap_psis_+3A_log_p">log_p</code></td>
<td>
<p>The log-posterior (target) evaluated at S samples from the
proposal distribution (g). A vector of length S.</p>
</td></tr>
<tr><td><code id="ap_psis_+3A_log_g">log_g</code></td>
<td>
<p>The log-density (proposal) evaluated at S samples from the
proposal distribution (g). A vector of length S.</p>
</td></tr>
<tr><td><code id="ap_psis_+3A_...">...</code></td>
<td>
<p>Currently not in use.</p>
</td></tr>
<tr><td><code id="ap_psis_+3A_cores">cores</code></td>
<td>
<p>The number of cores to use for parallelization. This defaults to
the option <code>mc.cores</code> which can be set for an entire R session by
<code>options(mc.cores = NUMBER)</code>. The old option <code>loo.cores</code> is now
deprecated but will be given precedence over <code>mc.cores</code> until
<code>loo.cores</code> is removed in a future release. <strong>As of version
2.0.0 the default is now 1 core if <code>mc.cores</code> is not set</strong>, but we
recommend using as many (or close to as many) cores as possible.
</p>

<ul>
<li><p> Note for Windows 10 users: it is <strong>strongly</strong>
<a href="https://github.com/stan-dev/loo/issues/94">recommended</a> to avoid using
the <code>.Rprofile</code> file to set <code>mc.cores</code> (using the <code>cores</code> argument or
setting <code>mc.cores</code> interactively or in a script is fine).
</p>
</li></ul>
</td></tr>
</table>


<h3>Methods (by class)</h3>


<ul>
<li> <p><code>ap_psis(array)</code>: An <code class="reqn">I</code> by <code class="reqn">C</code> by <code class="reqn">N</code> array, where <code class="reqn">I</code>
is the number of MCMC iterations per chain, <code class="reqn">C</code> is the number of
chains, and <code class="reqn">N</code> is the number of data points.
</p>
</li>
<li> <p><code>ap_psis(matrix)</code>: An <code class="reqn">S</code> by <code class="reqn">N</code> matrix, where <code class="reqn">S</code> is the size
of the posterior sample (with all chains merged) and <code class="reqn">N</code> is the number
of data points.
</p>
</li>
<li> <p><code>ap_psis(default)</code>: A vector of length <code class="reqn">S</code> (posterior sample size).
</p>
</li></ul>

<hr>
<h2 id='compare'>Model comparison (deprecated, old version)</h2><span id='topic+compare'></span>

<h3>Description</h3>

<p><strong>This function is deprecated</strong>. Please use the new <code><a href="#topic+loo_compare">loo_compare()</a></code> function
instead.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compare(..., x = list())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="compare_+3A_...">...</code></td>
<td>
<p>At least two objects returned by <code><a href="#topic+loo">loo()</a></code> (or <code><a href="#topic+waic">waic()</a></code>).</p>
</td></tr>
<tr><td><code id="compare_+3A_x">x</code></td>
<td>
<p>A list of at least two objects returned by <code><a href="#topic+loo">loo()</a></code> (or
<code><a href="#topic+waic">waic()</a></code>). This argument can be used as an alternative to
specifying the objects in <code>...</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When comparing two fitted models, we can estimate the difference in their
expected predictive accuracy by the difference in <code>elpd_loo</code> or
<code>elpd_waic</code> (or multiplied by -2, if desired, to be on the
deviance scale).
</p>
<p><em>When that difference, <code>elpd_diff</code>, is positive then the expected
predictive accuracy for the second model is higher. A negative
<code>elpd_diff</code> favors the first model.</em>
</p>
<p>When using <code>compare()</code> with more than two models, the values in the
<code>elpd_diff</code> and <code>se_diff</code> columns of the returned matrix are
computed by making pairwise comparisons between each model and the model
with the best ELPD (i.e., the model in the first row).
Although the <code>elpd_diff</code> column is equal to the difference in
<code>elpd_loo</code>, do not expect the <code>se_diff</code> column to be equal to the
the difference in <code>se_elpd_loo</code>.
</p>
<p>To compute the standard error of the difference in ELPD we use a
paired estimate to take advantage of the fact that the same set of <em>N</em>
data points was used to fit both models. These calculations should be most
useful when <em>N</em> is large, because then non-normality of the
distribution is not such an issue when estimating the uncertainty in these
sums. These standard errors, for all their flaws, should give a better
sense of uncertainty than what is obtained using the current standard
approach of comparing differences of deviances to a Chi-squared
distribution, a practice derived for Gaussian linear models or
asymptotically, and which only applies to nested models in any case.
</p>


<h3>Value</h3>

<p>A vector or matrix with class <code>'compare.loo'</code> that has its own
print method. If exactly two objects are provided in <code>...</code> or
<code>x</code>, then the difference in expected predictive accuracy and the
standard error of the difference are returned. If more than two objects are
provided then a matrix of summary information is returned (see <strong>Details</strong>).
</p>


<h3>References</h3>

<p>Vehtari, A., Gelman, A., and Gabry, J. (2017). Practical Bayesian model
evaluation using leave-one-out cross-validation and WAIC.
<em>Statistics and Computing</em>. 27(5), 1413&ndash;1432. doi:10.1007/s11222-016-9696-4
(<a href="https://link.springer.com/article/10.1007/s11222-016-9696-4">journal version</a>,
<a href="https://arxiv.org/abs/1507.04544">preprint arXiv:1507.04544</a>).
</p>
<p>Vehtari, A., Simpson, D., Gelman, A., Yao, Y., and Gabry, J. (2024).
Pareto smoothed importance sampling. <em>Journal of Machine Learning Research</em>,
25(72):1-58.
<a href="https://jmlr.org/papers/v25/19-556.html">PDF</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
loo1 &lt;- loo(log_lik1)
loo2 &lt;- loo(log_lik2)
print(compare(loo1, loo2), digits = 3)
print(compare(x = list(loo1, loo2)))

waic1 &lt;- waic(log_lik1)
waic2 &lt;- waic(log_lik2)
compare(waic1, waic2)

## End(Not run)

</code></pre>

<hr>
<h2 id='crps'>Continuously ranked probability score</h2><span id='topic+crps'></span><span id='topic+scrps'></span><span id='topic+loo_crps'></span><span id='topic+loo_scrps'></span><span id='topic+crps.matrix'></span><span id='topic+crps.numeric'></span><span id='topic+loo_crps.matrix'></span><span id='topic+scrps.matrix'></span><span id='topic+scrps.numeric'></span><span id='topic+loo_scrps.matrix'></span>

<h3>Description</h3>

<p>The <code>crps()</code> and <code>scrps()</code> functions and their <code style="white-space: pre;">&#8288;loo_*()&#8288;</code> counterparts can be
used to compute the continuously ranked probability score (CRPS) and scaled
CRPS (SCRPS) (see Bolin and Wallin, 2022). CRPS is a proper scoring rule, and
strictly proper when the first moment of the predictive distribution is
finite. Both can be expressed in terms of samples form the predictive
distribution. See e.g. Gneiting and Raftery (2007) for a comprehensive
discussion on CRPS.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>crps(x, ...)

scrps(x, ...)

loo_crps(x, ...)

loo_scrps(x, ...)

## S3 method for class 'matrix'
crps(x, x2, y, ..., permutations = 1)

## S3 method for class 'numeric'
crps(x, x2, y, ..., permutations = 1)

## S3 method for class 'matrix'
loo_crps(
  x,
  x2,
  y,
  log_lik,
  ...,
  permutations = 1,
  r_eff = 1,
  cores = getOption("mc.cores", 1)
)

## S3 method for class 'matrix'
scrps(x, x2, y, ..., permutations = 1)

## S3 method for class 'numeric'
scrps(x, x2, y, ..., permutations = 1)

## S3 method for class 'matrix'
loo_scrps(
  x,
  x2,
  y,
  log_lik,
  ...,
  permutations = 1,
  r_eff = 1,
  cores = getOption("mc.cores", 1)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="crps_+3A_x">x</code></td>
<td>
<p>A <code>S</code> by <code>N</code> matrix (draws by observations), or a vector of length
<code>S</code> when only single observation is provided in <code>y</code>.</p>
</td></tr>
<tr><td><code id="crps_+3A_...">...</code></td>
<td>
<p>Passed on to <code><a href="#topic+E_loo">E_loo()</a></code> in the <code style="white-space: pre;">&#8288;loo_*()&#8288;</code> version of these
functions.</p>
</td></tr>
<tr><td><code id="crps_+3A_x2">x2</code></td>
<td>
<p>Independent draws from the same distribution as draws in <code>x</code>.
Should be of the identical dimension.</p>
</td></tr>
<tr><td><code id="crps_+3A_y">y</code></td>
<td>
<p>A vector of observations or a single value.</p>
</td></tr>
<tr><td><code id="crps_+3A_permutations">permutations</code></td>
<td>
<p>An integer, with default value of 1,  specifying how many
times the expected value of  |X - X'| (<code style="white-space: pre;">&#8288;|x - x2|&#8288;</code>) is computed. The row
order of <code>x2</code> is shuffled as elements <code>x</code> and <code>x2</code> are typically drawn
given the same values of parameters. This happens, e.g., when one calls
<code>posterior_predict()</code> twice for a fitted <span class="pkg">rstanarm</span> or <span class="pkg">brms</span>
model. Generating more permutations is expected to decrease the variance of
the computed expected value.</p>
</td></tr>
<tr><td><code id="crps_+3A_log_lik">log_lik</code></td>
<td>
<p>A log-likelihood matrix the same size as <code>x</code>.</p>
</td></tr>
<tr><td><code id="crps_+3A_r_eff">r_eff</code></td>
<td>
<p>An optional vector of relative effective sample size estimates
containing one element per observation. See <code><a href="#topic+psis">psis()</a></code> for details.</p>
</td></tr>
<tr><td><code id="crps_+3A_cores">cores</code></td>
<td>
<p>The number of cores to use for parallelization of <code style="white-space: pre;">&#8288;[psis()]&#8288;</code>.
See <code><a href="#topic+psis">psis()</a></code> for details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>To compute (S)CRPS, the user needs to provide two sets of draws, <code>x</code> and
<code>x2</code>, from the predictive distribution. This is due to the fact that formulas
used to compute CRPS involve an expectation of the absolute difference of <code>x</code>
and <code>x2</code>, both having the same distribution. See the <code>permutations</code> argument,
as well as Gneiting and Raftery (2007) for details.
</p>


<h3>Value</h3>

<p>A list containing two elements: <code>estimates</code> and <code>pointwise</code>.
The former reports estimator and standard error and latter the pointwise
values.
</p>


<h3>References</h3>

<p>Bolin, D., &amp; Wallin, J. (2022). Local scale invariance and robustness of
proper scoring rules. arXiv. <a href="https://doi.org/10.48550/arXiv.1912.05642">doi:10.48550/arXiv.1912.05642</a>
</p>
<p>Gneiting, T., &amp; Raftery, A. E. (2007). Strictly Proper Scoring Rules,
Prediction, and Estimation. Journal of the American Statistical Association,
102(477), 359–378.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# An example using rstanarm
library(rstanarm)
data("kidiq")
fit &lt;- stan_glm(kid_score ~ mom_hs + mom_iq, data = kidiq)
ypred1 &lt;- posterior_predict(fit)
ypred2 &lt;- posterior_predict(fit)
crps(ypred1, ypred2, y = fit$y)
loo_crps(ypred1, ypred2, y = fit$y, log_lik = log_lik(fit))

## End(Not run)

</code></pre>

<hr>
<h2 id='E_loo'>Compute weighted expectations</h2><span id='topic+E_loo'></span><span id='topic+E_loo.default'></span><span id='topic+E_loo.matrix'></span>

<h3>Description</h3>

<p>The <code>E_loo()</code> function computes weighted expectations (means, variances,
quantiles) using the importance weights obtained from the
<a href="#topic+psis">PSIS</a> smoothing procedure. The expectations estimated by the
<code>E_loo()</code> function assume that the PSIS approximation is working well.
<strong>A small <a href="#topic+pareto-k-diagnostic">Pareto k</a> estimate is necessary,
but not sufficient, for <code>E_loo()</code> to give reliable estimates.</strong> Additional
diagnostic checks for gauging the reliability of the estimates are in
development and will be added in a future release.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>E_loo(x, psis_object, ...)

## Default S3 method:
E_loo(
  x,
  psis_object,
  ...,
  type = c("mean", "variance", "sd", "quantile"),
  probs = NULL,
  log_ratios = NULL
)

## S3 method for class 'matrix'
E_loo(
  x,
  psis_object,
  ...,
  type = c("mean", "variance", "sd", "quantile"),
  probs = NULL,
  log_ratios = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="E_loo_+3A_x">x</code></td>
<td>
<p>A numeric vector or matrix.</p>
</td></tr>
<tr><td><code id="E_loo_+3A_psis_object">psis_object</code></td>
<td>
<p>An object returned by <code><a href="#topic+psis">psis()</a></code>.</p>
</td></tr>
<tr><td><code id="E_loo_+3A_...">...</code></td>
<td>
<p>Arguments passed to individual methods.</p>
</td></tr>
<tr><td><code id="E_loo_+3A_type">type</code></td>
<td>
<p>The type of expectation to compute. The options are
<code>"mean"</code>, <code>"variance"</code>, <code>"sd"</code>, and <code>"quantile"</code>.</p>
</td></tr>
<tr><td><code id="E_loo_+3A_probs">probs</code></td>
<td>
<p>For computing quantiles, a vector of probabilities.</p>
</td></tr>
<tr><td><code id="E_loo_+3A_log_ratios">log_ratios</code></td>
<td>
<p>Optionally, a vector or matrix (the same dimensions as <code>x</code>)
of raw (not smoothed) log ratios. If working with log-likelihood values,
the log ratios are the <strong>negative</strong> of those values. If <code>log_ratios</code> is
specified we are able to compute more accurate <a href="#topic+pareto-k-diagnostic">Pareto k</a>
diagnostics specific to <code>E_loo()</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A named list with the following components:
</p>

<dl>
<dt><code>value</code></dt><dd>
<p>The result of the computation.
</p>
<p>For the matrix method, <code>value</code> is a vector with <code>ncol(x)</code>
elements, with one exception: when <code>type="quantile"</code> and
multiple values are specified in <code>probs</code> the <code>value</code> component of
the returned object is a <code>length(probs)</code> by <code>ncol(x)</code> matrix.
</p>
<p>For the default/vector method the <code>value</code> component is scalar, with
one exception: when <code>type="quantile"</code> and multiple values
are specified in <code>probs</code> the <code>value</code> component is a vector with
<code>length(probs)</code> elements.
</p>
</dd>
<dt><code>pareto_k</code></dt><dd>
<p>Function-specific diagnostic.
</p>
<p>For the matrix method it will be a vector of length <code>ncol(x)</code>
containing estimates of the shape parameter <code class="reqn">k</code> of the
generalized Pareto distribution. For the default/vector method,
the estimate is a scalar. If <code>log_ratios</code> is not specified when
calling <code>E_loo()</code>, the smoothed log-weights are used to estimate
Pareto-k's, which may produce optimistic estimates.
</p>
<p>For <code>type="mean"</code>, <code>type="var"</code>, and <code>type="sd"</code>, the returned Pareto-k is
usually the maximum of the Pareto-k's for the left and right tail of <code class="reqn">hr</code>
and the right tail of <code class="reqn">r</code>, where <code class="reqn">r</code> is the importance ratio and
<code class="reqn">h=x</code> for <code>type="mean"</code> and <code class="reqn">h=x^2</code> for <code>type="var"</code> and <code>type="sd"</code>.
If <code class="reqn">h</code> is binary, constant, or not finite, or if <code>type="quantile"</code>, the
returned Pareto-k is the Pareto-k for the right tail of <code class="reqn">r</code>.
</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>
if (requireNamespace("rstanarm", quietly = TRUE)) {
# Use rstanarm package to quickly fit a model and get both a log-likelihood
# matrix and draws from the posterior predictive distribution
library("rstanarm")

# data from help("lm")
ctl &lt;- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)
trt &lt;- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)
d &lt;- data.frame(
  weight = c(ctl, trt),
  group = gl(2, 10, 20, labels = c("Ctl","Trt"))
)
fit &lt;- stan_glm(weight ~ group, data = d, refresh = 0)
yrep &lt;- posterior_predict(fit)
dim(yrep)

log_ratios &lt;- -1 * log_lik(fit)
dim(log_ratios)

r_eff &lt;- relative_eff(exp(-log_ratios), chain_id = rep(1:4, each = 1000))
psis_object &lt;- psis(log_ratios, r_eff = r_eff, cores = 2)

E_loo(yrep, psis_object, type = "mean")
E_loo(yrep, psis_object, type = "var")
E_loo(yrep, psis_object, type = "sd")
E_loo(yrep, psis_object, type = "quantile", probs = 0.5) # median
E_loo(yrep, psis_object, type = "quantile", probs = c(0.1, 0.9))

# We can get more accurate Pareto k diagnostic if we also provide
# the log_ratios argument
E_loo(yrep, psis_object, type = "mean", log_ratios = log_ratios)
}


</code></pre>

<hr>
<h2 id='elpd'>Generic (expected) log-predictive density</h2><span id='topic+elpd'></span><span id='topic+elpd.array'></span><span id='topic+elpd.matrix'></span>

<h3>Description</h3>

<p>The <code>elpd()</code> methods for arrays and matrices can compute the expected log
pointwise predictive density for a new dataset or the log pointwise
predictive density of the observed data (an overestimate of the elpd).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>elpd(x, ...)

## S3 method for class 'array'
elpd(x, ...)

## S3 method for class 'matrix'
elpd(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="elpd_+3A_x">x</code></td>
<td>
<p>A log-likelihood array or matrix. The <strong>Methods (by class)</strong>
section, below, has detailed descriptions of how to specify the inputs for
each method.</p>
</td></tr>
<tr><td><code id="elpd_+3A_...">...</code></td>
<td>
<p>Currently ignored.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>elpd()</code> function is an S3 generic and methods are provided for
3-D pointwise log-likelihood arrays and matrices.
</p>


<h3>Methods (by class)</h3>


<ul>
<li> <p><code>elpd(array)</code>: An <code class="reqn">I</code> by <code class="reqn">C</code> by <code class="reqn">N</code> array, where <code class="reqn">I</code>
is the number of MCMC iterations per chain, <code class="reqn">C</code> is the number of
chains, and <code class="reqn">N</code> is the number of data points.
</p>
</li>
<li> <p><code>elpd(matrix)</code>: An <code class="reqn">S</code> by <code class="reqn">N</code> matrix, where <code class="reqn">S</code> is the size
of the posterior sample (with all chains merged) and <code class="reqn">N</code> is the number
of data points.
</p>
</li></ul>


<h3>See Also</h3>

<p>The vignette <em>Holdout validation and K-fold cross-validation of Stan
programs with the loo package</em> for demonstrations of using the <code>elpd()</code>
methods.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Calculate the lpd of the observed data
LLarr &lt;- example_loglik_array()
elpd(LLarr)

</code></pre>

<hr>
<h2 id='example_loglik_array'>Objects to use in examples and tests</h2><span id='topic+example_loglik_array'></span><span id='topic+example_loglik_matrix'></span>

<h3>Description</h3>

<p>Example pointwise log-likelihood objects to use in demonstrations and tests.
See the <strong>Value</strong> and <strong>Examples</strong> sections below.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>example_loglik_array()

example_loglik_matrix()
</code></pre>


<h3>Value</h3>

<p><code>example_loglik_array()</code> returns a 500 (draws) x 2 (chains) x 32
(observations) pointwise log-likelihood array.
</p>
<p><code>example_loglik_matrix()</code> returns the same pointwise log-likelihood values
as <code>example_loglik_array()</code> but reshaped into a 1000 (draws*chains) x 32
(observations) matrix.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>LLarr &lt;- example_loglik_array()
(dim_arr &lt;- dim(LLarr))
LLmat &lt;- example_loglik_matrix()
(dim_mat &lt;- dim(LLmat))

all.equal(dim_mat[1], dim_arr[1] * dim_arr[2])
all.equal(dim_mat[2], dim_arr[3])

all.equal(LLarr[, 1, ], LLmat[1:500, ])
all.equal(LLarr[, 2, ], LLmat[501:1000, ])

</code></pre>

<hr>
<h2 id='extract_log_lik'>Extract pointwise log-likelihood from a Stan model</h2><span id='topic+extract_log_lik'></span>

<h3>Description</h3>

<p>Convenience function for extracting the pointwise log-likelihood
matrix or array from a <code>stanfit</code> object from the <span class="pkg">rstan</span> package.
Note: recent versions of <span class="pkg">rstan</span> now include a <code>loo()</code> method for
<code>stanfit</code> objects that handles this internally.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>extract_log_lik(stanfit, parameter_name = "log_lik", merge_chains = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="extract_log_lik_+3A_stanfit">stanfit</code></td>
<td>
<p>A <code>stanfit</code> object (<span class="pkg">rstan</span> package).</p>
</td></tr>
<tr><td><code id="extract_log_lik_+3A_parameter_name">parameter_name</code></td>
<td>
<p>A character string naming the parameter (or generated
quantity) in the Stan model corresponding to the log-likelihood.</p>
</td></tr>
<tr><td><code id="extract_log_lik_+3A_merge_chains">merge_chains</code></td>
<td>
<p>If <code>TRUE</code> (the default), all Markov chains are
merged together (i.e., stacked) and a matrix is returned. If <code>FALSE</code>
they are kept separate and an array is returned.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Stan does not automatically compute and store the log-likelihood. It
is up to the user to incorporate it into the Stan program if it is to be
extracted after fitting the model. In a Stan model, the pointwise log
likelihood can be coded as a vector in the transformed parameters block
(and then summed up in the model block) or it can be coded entirely in the
generated quantities block. We recommend using the generated quantities
block so that the computations are carried out only once per iteration
rather than once per HMC leapfrog step.
</p>
<p>For example, the following is the <code style="white-space: pre;">&#8288;generated quantities&#8288;</code> block for
computing and saving the log-likelihood for a linear regression model with
<code>N</code> data points, outcome <code>y</code>, predictor matrix <code>X</code>,
coefficients <code>beta</code>, and standard deviation <code>sigma</code>:
</p>
<p><code style="white-space: pre;">&#8288;vector[N] log_lik;&#8288;</code>
</p>
<p><code>for (n in 1:N) log_lik[n] = normal_lpdf(y[n] | X[n, ] * beta, sigma);</code>
</p>


<h3>Value</h3>

<p>If <code>merge_chains=TRUE</code>, an <code class="reqn">S</code> by <code class="reqn">N</code> matrix of
(post-warmup) extracted draws, where <code class="reqn">S</code> is the size of the posterior
sample and <code class="reqn">N</code> is the number of data points. If
<code>merge_chains=FALSE</code>, an <code class="reqn">I</code> by <code class="reqn">C</code> by <code class="reqn">N</code> array, where
<code class="reqn">I \times C = S</code>.
</p>


<h3>References</h3>

<p>Stan Development Team (2017). The Stan C++ Library, Version 2.16.0.
<a href="https://mc-stan.org/">https://mc-stan.org/</a>
</p>
<p>Stan Development Team (2017). RStan: the R interface to Stan, Version 2.16.1.
<a href="https://mc-stan.org/">https://mc-stan.org/</a>
</p>

<hr>
<h2 id='find_model_names'>Find the model names associated with <code>"loo"</code> objects</h2><span id='topic+find_model_names'></span>

<h3>Description</h3>

<p>Find the model names associated with <code>"loo"</code> objects
</p>


<h3>Usage</h3>

<pre><code class='language-R'>find_model_names(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="find_model_names_+3A_x">x</code></td>
<td>
<p>List of <code>"loo"</code> objects.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Character vector of model names the same length as <code>x.</code>
</p>

<hr>
<h2 id='gpdfit'>Estimate parameters of the Generalized Pareto distribution</h2><span id='topic+gpdfit'></span>

<h3>Description</h3>

<p>Given a sample <code class="reqn">x</code>, Estimate the parameters <code class="reqn">k</code> and <code class="reqn">\sigma</code> of
the generalized Pareto distribution (GPD), assuming the location parameter is
0. By default the fit uses a prior for <code class="reqn">k</code>, which will stabilize
estimates for very small sample sizes (and low effective sample sizes in the
case of MCMC samples). The weakly informative prior is a Gaussian prior
centered at 0.5.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gpdfit(x, wip = TRUE, min_grid_pts = 30, sort_x = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gpdfit_+3A_x">x</code></td>
<td>
<p>A numeric vector. The sample from which to estimate the parameters.</p>
</td></tr>
<tr><td><code id="gpdfit_+3A_wip">wip</code></td>
<td>
<p>Logical indicating whether to adjust <code class="reqn">k</code> based on a weakly
informative Gaussian prior centered on 0.5. Defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="gpdfit_+3A_min_grid_pts">min_grid_pts</code></td>
<td>
<p>The minimum number of grid points used in the fitting
algorithm. The actual number used is <code>min_grid_pts + floor(sqrt(length(x)))</code>.</p>
</td></tr>
<tr><td><code id="gpdfit_+3A_sort_x">sort_x</code></td>
<td>
<p>If <code>TRUE</code> (the default), the first step in the fitting
algorithm is to sort the elements of <code>x</code>. If <code>x</code> is already
sorted in ascending order then <code>sort_x</code> can be set to <code>FALSE</code> to
skip the initial sorting step.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Here the parameter <code class="reqn">k</code> is the negative of <code class="reqn">k</code> in Zhang &amp;
Stephens (2009).
</p>


<h3>Value</h3>

<p>A named list with components <code>k</code> and <code>sigma</code>.
</p>


<h3>References</h3>

<p>Zhang, J., and Stephens, M. A. (2009). A new and efficient estimation method
for the generalized Pareto distribution. <em>Technometrics</em> <strong>51</strong>, 316-325.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+psis">psis()</a></code>, <a href="#topic+pareto-k-diagnostic">pareto-k-diagnostic</a>
</p>

<hr>
<h2 id='importance_sampling'>A parent class for different importance sampling methods.</h2><span id='topic+importance_sampling'></span><span id='topic+importance_sampling.array'></span><span id='topic+importance_sampling.matrix'></span><span id='topic+importance_sampling.default'></span>

<h3>Description</h3>

<p>A parent class for different importance sampling methods.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>importance_sampling(log_ratios, method, ...)

## S3 method for class 'array'
importance_sampling(
  log_ratios,
  method,
  ...,
  r_eff = 1,
  cores = getOption("mc.cores", 1)
)

## S3 method for class 'matrix'
importance_sampling(
  log_ratios,
  method,
  ...,
  r_eff = 1,
  cores = getOption("mc.cores", 1)
)

## Default S3 method:
importance_sampling(log_ratios, method, ..., r_eff = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="importance_sampling_+3A_log_ratios">log_ratios</code></td>
<td>
<p>An array, matrix, or vector of importance ratios on the log
scale (for PSIS-LOO these are <em>negative</em> log-likelihood values). See the
<strong>Methods (by class)</strong> section below for a detailed description of how
to specify the inputs for each method.</p>
</td></tr>
<tr><td><code id="importance_sampling_+3A_method">method</code></td>
<td>
<p>The importance sampling method to use. The following methods
are implemented:
</p>

<ul>
<li> <p><code><a href="#topic+psis">&quot;psis&quot;</a></code>: Pareto-Smoothed Importance Sampling (PSIS). Default method.
</p>
</li>
<li> <p><code><a href="#topic+tis">&quot;tis&quot;</a></code>: Truncated Importance Sampling (TIS) with truncation at
<code>sqrt(S)</code>, where <code>S</code> is the number of posterior draws.
</p>
</li>
<li> <p><code><a href="#topic+sis">&quot;sis&quot;</a></code>: Standard Importance Sampling (SIS).
</p>
</li></ul>
</td></tr>
<tr><td><code id="importance_sampling_+3A_...">...</code></td>
<td>
<p>Arguments passed on to the various methods.</p>
</td></tr>
<tr><td><code id="importance_sampling_+3A_r_eff">r_eff</code></td>
<td>
<p>Vector of relative effective sample size estimates containing
one element per observation. The values provided should be the relative
effective sample sizes of <code>1/exp(log_ratios)</code> (i.e., <code>1/ratios</code>).
This is related to the relative efficiency of estimating the normalizing
term in self-normalizing importance sampling. If <code>r_eff</code> is not
provided then the reported PSIS effective sample sizes and Monte Carlo
error estimates can be over-optimistic. If the posterior draws are (near)
independent then <code>r_eff=1</code> can be used. <code>r_eff</code> has to be a scalar (same
value is used for all observations) or a vector with length equal to the
number of observations. The default value is 1. See the <code><a href="#topic+relative_eff">relative_eff()</a></code>
helper function for computing <code>r_eff</code>.</p>
</td></tr>
<tr><td><code id="importance_sampling_+3A_cores">cores</code></td>
<td>
<p>The number of cores to use for parallelization. This defaults to
the option <code>mc.cores</code> which can be set for an entire R session by
<code>options(mc.cores = NUMBER)</code>. The old option <code>loo.cores</code> is now
deprecated but will be given precedence over <code>mc.cores</code> until
<code>loo.cores</code> is removed in a future release. <strong>As of version
2.0.0 the default is now 1 core if <code>mc.cores</code> is not set</strong>, but we
recommend using as many (or close to as many) cores as possible.
</p>

<ul>
<li><p> Note for Windows 10 users: it is <strong>strongly</strong>
<a href="https://github.com/stan-dev/loo/issues/94">recommended</a> to avoid using
the <code>.Rprofile</code> file to set <code>mc.cores</code> (using the <code>cores</code> argument or
setting <code>mc.cores</code> interactively or in a script is fine).
</p>
</li></ul>
</td></tr>
</table>

<hr>
<h2 id='kfold-generic'>Generic function for K-fold cross-validation for developers</h2><span id='topic+kfold-generic'></span><span id='topic+kfold'></span><span id='topic+is.kfold'></span>

<h3>Description</h3>

<p>For developers of Bayesian modeling packages, <strong>loo</strong> includes
a generic function <code>kfold()</code> so that methods may be defined for K-fold
CV without name conflicts between packages. See, for example, the
<code>kfold()</code> methods in the <strong>rstanarm</strong> and <strong>brms</strong> packages.
</p>
<p>The <strong>Value</strong> section below describes the objects that <code>kfold()</code>
methods should return in order to be compatible with
<code><a href="#topic+loo_compare">loo_compare()</a></code> and the <strong>loo</strong> package print methods.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kfold(x, ...)

is.kfold(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kfold-generic_+3A_x">x</code></td>
<td>
<p>A fitted model object.</p>
</td></tr>
<tr><td><code id="kfold-generic_+3A_...">...</code></td>
<td>
<p>Arguments to pass to specific methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>For developers defining a <code>kfold()</code> method for a class
<code>"foo"</code>, the <code>kfold.foo()</code> function should return a list with class
<code>c("kfold", "loo")</code> with at least the following named elements:
</p>

<ul>
<li> <p><code>"estimates"</code>: A <code style="white-space: pre;">&#8288;1x2&#8288;</code> matrix containing the ELPD estimate and its
standard error. The matrix must have row name &quot;<code>elpd_kfold</code>&quot; and column
names <code>"Estimate"</code> and <code>"SE"</code>.
</p>
</li>
<li> <p><code>"pointwise"</code>: A <code>Nx1</code> matrix with column name <code>"elpd_kfold"</code> containing
the pointwise contributions for each data point.
</p>
</li></ul>

<p>It is important for the object to have at least these classes and
components so that it is compatible with other functions like
<code><a href="#topic+loo_compare">loo_compare()</a></code> and <code>print()</code> methods.
</p>

<hr>
<h2 id='kfold-helpers'>Helper functions for K-fold cross-validation</h2><span id='topic+kfold-helpers'></span><span id='topic+kfold_split_random'></span><span id='topic+kfold_split_stratified'></span><span id='topic+kfold_split_grouped'></span>

<h3>Description</h3>

<p>These functions can be used to generate indexes for use with
K-fold cross-validation. See the <strong>Details</strong> section for explanations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kfold_split_random(K = 10, N = NULL)

kfold_split_stratified(K = 10, x = NULL)

kfold_split_grouped(K = 10, x = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kfold-helpers_+3A_k">K</code></td>
<td>
<p>The number of folds to use.</p>
</td></tr>
<tr><td><code id="kfold-helpers_+3A_n">N</code></td>
<td>
<p>The number of observations in the data.</p>
</td></tr>
<tr><td><code id="kfold-helpers_+3A_x">x</code></td>
<td>
<p>A discrete variable of length <code>N</code> with at least <code>K</code> levels (unique
values). Will be coerced to a <a href="base.html#topic+factor">factor</a>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>kfold_split_random()</code> splits the data into <code>K</code> groups
of equal size (or roughly equal size).
</p>
<p>For a categorical variable <code>x</code> <code>kfold_split_stratified()</code>
splits the observations into <code>K</code> groups ensuring that relative
category frequencies are approximately preserved.
</p>
<p>For a grouping variable <code>x</code>, <code>kfold_split_grouped()</code> places
all observations in <code>x</code> from the same group/level together in
the same fold. The selection of which groups/levels go into which
fold (relevant when when there are more groups than folds) is
randomized.
</p>


<h3>Value</h3>

<p>An integer vector of length <code>N</code> where each element is an index in <code>1:K</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>ids &lt;- kfold_split_random(K = 5, N = 20)
print(ids)
table(ids)


x &lt;- sample(c(0, 1), size = 200, replace = TRUE, prob = c(0.05, 0.95))
table(x)
ids &lt;- kfold_split_stratified(K = 5, x = x)
print(ids)
table(ids, x)

grp &lt;- gl(n = 50, k = 15, labels = state.name)
length(grp)
head(table(grp))

ids_10 &lt;- kfold_split_grouped(K = 10, x = grp)
(tab_10 &lt;- table(grp, ids_10))
colSums(tab_10)

ids_9 &lt;- kfold_split_grouped(K = 9, x = grp)
(tab_9 &lt;- table(grp, ids_9))
colSums(tab_9)

</code></pre>

<hr>
<h2 id='loo'>Efficient approximate leave-one-out cross-validation (LOO)</h2><span id='topic+loo'></span><span id='topic+loo.array'></span><span id='topic+loo.matrix'></span><span id='topic+loo.function'></span><span id='topic+loo_i'></span><span id='topic+is.loo'></span><span id='topic+is.psis_loo'></span>

<h3>Description</h3>

<p>The <code>loo()</code> methods for arrays, matrices, and functions compute PSIS-LOO
CV, efficient approximate leave-one-out (LOO) cross-validation for Bayesian
models using Pareto smoothed importance sampling (<a href="#topic+psis">PSIS</a>). This is
an implementation of the methods described in Vehtari, Gelman, and Gabry
(2017) and Vehtari, Simpson, Gelman, Yao, and Gabry (2024).
</p>
<p>The <code>loo_i()</code> function enables testing log-likelihood
functions for use with the <code>loo.function()</code> method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>loo(x, ...)

## S3 method for class 'array'
loo(
  x,
  ...,
  r_eff = 1,
  save_psis = FALSE,
  cores = getOption("mc.cores", 1),
  is_method = c("psis", "tis", "sis")
)

## S3 method for class 'matrix'
loo(
  x,
  ...,
  r_eff = 1,
  save_psis = FALSE,
  cores = getOption("mc.cores", 1),
  is_method = c("psis", "tis", "sis")
)

## S3 method for class ''function''
loo(
  x,
  ...,
  data = NULL,
  draws = NULL,
  r_eff = 1,
  save_psis = FALSE,
  cores = getOption("mc.cores", 1),
  is_method = c("psis", "tis", "sis")
)

loo_i(i, llfun, ..., data = NULL, draws = NULL, r_eff = 1, is_method = "psis")

is.loo(x)

is.psis_loo(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="loo_+3A_x">x</code></td>
<td>
<p>A log-likelihood array, matrix, or function. The <strong>Methods (by class)</strong>
section, below, has detailed descriptions of how to specify the inputs for
each method.</p>
</td></tr>
<tr><td><code id="loo_+3A_r_eff">r_eff</code></td>
<td>
<p>Vector of relative effective sample size estimates for the
likelihood (<code>exp(log_lik)</code>) of each observation. This is related to
the relative efficiency of estimating the normalizing term in
self-normalized importance sampling when using posterior draws obtained
with MCMC. If MCMC draws are used and <code>r_eff</code> is not provided then
the reported PSIS effective sample sizes and Monte Carlo error estimates
can be over-optimistic. If the posterior draws are (near) independent then
<code>r_eff=1</code> can be used. <code>r_eff</code> has to be a scalar (same value is used
for all observations) or a vector with length equal to the number of
observations. The default value is 1. See the <code><a href="#topic+relative_eff">relative_eff()</a></code> helper
functions for help computing <code>r_eff</code>.</p>
</td></tr>
<tr><td><code id="loo_+3A_save_psis">save_psis</code></td>
<td>
<p>Should the <code>psis</code> object created internally by <code>loo()</code> be
saved in the returned object? The <code>loo()</code> function calls <code><a href="#topic+psis">psis()</a></code>
internally but by default discards the (potentially large) <code>psis</code> object
after using it to compute the LOO-CV summaries. Setting <code>save_psis=TRUE</code>
will add a <code>psis_object</code> component to the list returned by <code>loo</code>.
This is useful if you plan to use the <code><a href="#topic+E_loo">E_loo()</a></code> function to compute
weighted expectations after running <code>loo</code>. Several functions in the
<span class="pkg">bayesplot</span> package also accept <code>psis</code> objects.</p>
</td></tr>
<tr><td><code id="loo_+3A_cores">cores</code></td>
<td>
<p>The number of cores to use for parallelization. This defaults to
the option <code>mc.cores</code> which can be set for an entire R session by
<code>options(mc.cores = NUMBER)</code>. The old option <code>loo.cores</code> is now
deprecated but will be given precedence over <code>mc.cores</code> until
<code>loo.cores</code> is removed in a future release. <strong>As of version
2.0.0 the default is now 1 core if <code>mc.cores</code> is not set</strong>, but we
recommend using as many (or close to as many) cores as possible.
</p>

<ul>
<li><p> Note for Windows 10 users: it is <strong>strongly</strong>
<a href="https://github.com/stan-dev/loo/issues/94">recommended</a> to avoid using
the <code>.Rprofile</code> file to set <code>mc.cores</code> (using the <code>cores</code> argument or
setting <code>mc.cores</code> interactively or in a script is fine).
</p>
</li></ul>
</td></tr>
<tr><td><code id="loo_+3A_is_method">is_method</code></td>
<td>
<p>The importance sampling method to use. The following methods
are implemented:
</p>

<ul>
<li> <p><code><a href="#topic+psis">&quot;psis&quot;</a></code>: Pareto-Smoothed Importance Sampling (PSIS). Default method.
</p>
</li>
<li> <p><code><a href="#topic+tis">&quot;tis&quot;</a></code>: Truncated Importance Sampling (TIS) with truncation at
<code>sqrt(S)</code>, where <code>S</code> is the number of posterior draws.
</p>
</li>
<li> <p><code><a href="#topic+sis">&quot;sis&quot;</a></code>: Standard Importance Sampling (SIS).
</p>
</li></ul>
</td></tr>
<tr><td><code id="loo_+3A_data">data</code>, <code id="loo_+3A_draws">draws</code>, <code id="loo_+3A_...">...</code></td>
<td>
<p>For the <code>loo.function()</code> method and the <code>loo_i()</code>
function, these are the data, posterior draws, and other arguments to pass
to the log-likelihood function. See the <strong>Methods (by class)</strong> section
below for details on how to specify these arguments.</p>
</td></tr>
<tr><td><code id="loo_+3A_i">i</code></td>
<td>
<p>For <code>loo_i()</code>, an integer in <code>1:N</code>.</p>
</td></tr>
<tr><td><code id="loo_+3A_llfun">llfun</code></td>
<td>
<p>For <code>loo_i()</code>, the same as <code>x</code> for the
<code>loo.function()</code> method. A log-likelihood function as described in the
<strong>Methods (by class)</strong> section.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>loo()</code> function is an S3 generic and methods are provided for
3-D pointwise log-likelihood arrays, pointwise log-likelihood matrices, and
log-likelihood functions. The array and matrix methods are the most
convenient, but for models fit to very large datasets the <code>loo.function()</code>
method is more memory efficient and may be preferable.
</p>


<h3>Value</h3>

<p>The <code>loo()</code> methods return a named list with class
<code>c("psis_loo", "loo")</code> and components:
</p>

<dl>
<dt><code>estimates</code></dt><dd>
<p>A matrix with two columns (<code>Estimate</code>, <code>SE</code>) and three rows (<code>elpd_loo</code>,
<code>p_loo</code>, <code>looic</code>). This contains point estimates and standard errors of the
expected log pointwise predictive density (<code><a href="#topic+loo-glossary">elpd_loo</a></code>), the
effective number of parameters (<code><a href="#topic+loo-glossary">p_loo</a></code>) and the LOO
information criterion <code>looic</code> (which is just <code>-2 * elpd_loo</code>, i.e.,
converted to deviance scale).
</p>
</dd>
<dt><code>pointwise</code></dt><dd>
<p>A matrix with five columns (and number of rows equal to the number of
observations) containing the pointwise contributions of the measures
(<code>elpd_loo</code>, <code>mcse_elpd_loo</code>, <code>p_loo</code>, <code>looic</code>, <code>influence_pareto_k</code>).
in addition to the three measures in <code>estimates</code>, we also report
pointwise values of the Monte Carlo standard error of <code><a href="#topic+loo-glossary">elpd_loo</a></code>
(<code><a href="#topic+loo-glossary">mcse_elpd_loo</a></code>), and statistics describing the influence of
each observation on the posterior distribution (<code>influence_pareto_k</code>).
These are the estimates of the shape parameter <code class="reqn">k</code> of the
generalized Pareto fit to the importance ratios for each leave-one-out
distribution (see the <a href="#topic+pareto-k-diagnostic">pareto-k-diagnostic</a> page for details).
</p>
</dd>
<dt><code>diagnostics</code></dt><dd>
<p>A named list containing two vectors:
</p>

<ul>
<li> <p><code>pareto_k</code>: Importance sampling reliability diagnostics. By default,
these are equal to the <code>influence_pareto_k</code> in <code>pointwise</code>.
Some algorithms can improve importance sampling reliability and
modify these diagnostics. See the <a href="#topic+pareto-k-diagnostic">pareto-k-diagnostic</a> page for details.
</p>
</li>
<li> <p><code>n_eff</code>: PSIS effective sample size estimates.
</p>
</li></ul>

</dd>
<dt><code>psis_object</code></dt><dd>
<p>This component will be <code>NULL</code> unless the <code>save_psis</code> argument is set to
<code>TRUE</code> when calling <code>loo()</code>. In that case <code>psis_object</code> will be the object
of class <code>"psis"</code> that is created when the <code>loo()</code> function calls <code><a href="#topic+psis">psis()</a></code>
internally to do the PSIS procedure.
</p>
</dd>
</dl>

<p>The <code>loo_i()</code> function returns a named list with components
<code>pointwise</code> and <code>diagnostics</code>. These components have the same
structure as the <code>pointwise</code> and <code>diagnostics</code> components of the
object returned by <code>loo()</code> except they contain results for only a single
observation.
</p>


<h3>Methods (by class)</h3>


<ul>
<li> <p><code>loo(array)</code>: An <code class="reqn">I</code> by <code class="reqn">C</code> by <code class="reqn">N</code> array, where <code class="reqn">I</code>
is the number of MCMC iterations per chain, <code class="reqn">C</code> is the number of
chains, and <code class="reqn">N</code> is the number of data points.
</p>
</li>
<li> <p><code>loo(matrix)</code>: An <code class="reqn">S</code> by <code class="reqn">N</code> matrix, where <code class="reqn">S</code> is the size
of the posterior sample (with all chains merged) and <code class="reqn">N</code> is the number
of data points.
</p>
</li>
<li> <p><code>loo(`function`)</code>: A function <code>f()</code> that takes arguments <code>data_i</code> and <code>draws</code> and returns a
vector containing the log-likelihood for a single observation <code>i</code> evaluated
at each posterior draw. The function should be written such that, for each
observation <code>i</code> in <code>1:N</code>, evaluating
</p>
<div class="sourceCode"><pre>f(data_i = data[i,, drop=FALSE], draws = draws)
</pre></div>
<p>results in a vector of length <code>S</code> (size of posterior sample). The
log-likelihood function can also have additional arguments but <code>data_i</code> and
<code>draws</code> are required.
</p>
<p>If using the function method then the arguments <code>data</code> and <code>draws</code> must also
be specified in the call to <code>loo()</code>:
</p>

<ul>
<li> <p><code>data</code>: A data frame or matrix containing the data (e.g.
observed outcome and predictors) needed to compute the pointwise
log-likelihood. For each observation <code>i</code>, the <code>i</code>th row of
<code>data</code> will be passed to the <code>data_i</code> argument of the
log-likelihood function.
</p>
</li>
<li> <p><code>draws</code>: An object containing the posterior draws for any
parameters needed to compute the pointwise log-likelihood. Unlike
<code>data</code>, which is indexed by observation, for each observation the
entire object <code>draws</code> will be passed to the <code>draws</code> argument of
the log-likelihood function.
</p>
</li>
<li><p> The <code>...</code> can be used if your log-likelihood function takes additional
arguments. These arguments are used like the <code>draws</code> argument in that they
are recycled for each observation.
</p>
</li></ul>

</li></ul>


<h3>Defining <code>loo()</code> methods in a package</h3>

<p>Package developers can define
<code>loo()</code> methods for fitted models objects. See the example <code>loo.stanfit()</code>
method in the <strong>Examples</strong> section below for an example of defining a
method that calls <code>loo.array()</code>. The <code>loo.stanreg()</code> method in the
<strong>rstanarm</strong> package is an example of defining a method that calls
<code>loo.function()</code>.
</p>


<h3>References</h3>

<p>Vehtari, A., Gelman, A., and Gabry, J. (2017). Practical Bayesian model
evaluation using leave-one-out cross-validation and WAIC.
<em>Statistics and Computing</em>. 27(5), 1413&ndash;1432. doi:10.1007/s11222-016-9696-4
(<a href="https://link.springer.com/article/10.1007/s11222-016-9696-4">journal version</a>,
<a href="https://arxiv.org/abs/1507.04544">preprint arXiv:1507.04544</a>).
</p>
<p>Vehtari, A., Simpson, D., Gelman, A., Yao, Y., and Gabry, J. (2024).
Pareto smoothed importance sampling. <em>Journal of Machine Learning Research</em>,
25(72):1-58.
<a href="https://jmlr.org/papers/v25/19-556.html">PDF</a>
</p>


<h3>See Also</h3>


<ul>
<li><p> The <strong>loo</strong> package <a href="https://mc-stan.org/loo/articles/index.html">vignettes</a>
for demonstrations.
</p>
</li>
<li><p> The <a href="https://mc-stan.org/loo/articles/online-only/faq.html">FAQ page</a> on
the <strong>loo</strong> website for answers to frequently asked questions.
</p>
</li>
<li> <p><code><a href="#topic+psis">psis()</a></code> for the underlying Pareto Smoothed Importance Sampling (PSIS)
procedure used in the LOO-CV approximation.
</p>
</li>
<li> <p><a href="#topic+pareto-k-diagnostic">pareto-k-diagnostic</a> for convenience functions for looking at diagnostics.
</p>
</li>
<li> <p><code><a href="#topic+loo_compare">loo_compare()</a></code> for model comparison.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>### Array and matrix methods (using example objects included with loo package)
# Array method
LLarr &lt;- example_loglik_array()
rel_n_eff &lt;- relative_eff(exp(LLarr))
loo(LLarr, r_eff = rel_n_eff, cores = 2)

# Matrix method
LLmat &lt;- example_loglik_matrix()
rel_n_eff &lt;- relative_eff(exp(LLmat), chain_id = rep(1:2, each = 500))
loo(LLmat, r_eff = rel_n_eff, cores = 2)


### Using log-likelihood function instead of array or matrix
set.seed(124)

# Simulate data and draw from posterior
N &lt;- 50; K &lt;- 10; S &lt;- 100; a0 &lt;- 3; b0 &lt;- 2
p &lt;- rbeta(1, a0, b0)
y &lt;- rbinom(N, size = K, prob = p)
a &lt;- a0 + sum(y); b &lt;- b0 + N * K - sum(y)
fake_posterior &lt;- as.matrix(rbeta(S, a, b))
dim(fake_posterior) # S x 1
fake_data &lt;- data.frame(y,K)
dim(fake_data) # N x 2

llfun &lt;- function(data_i, draws) {
  # each time called internally within loo the arguments will be equal to:
  # data_i: ith row of fake_data (fake_data[i,, drop=FALSE])
  # draws: entire fake_posterior matrix
  dbinom(data_i$y, size = data_i$K, prob = draws, log = TRUE)
}

# Use the loo_i function to check that llfun works on a single observation
# before running on all obs. For example, using the 3rd obs in the data:
loo_3 &lt;- loo_i(i = 3, llfun = llfun, data = fake_data, draws = fake_posterior)
print(loo_3$pointwise[, "elpd_loo"])

# Use loo.function method (default r_eff=1 is used as this posterior not obtained via MCMC)
loo_with_fn &lt;- loo(llfun, draws = fake_posterior, data = fake_data)

# If we look at the elpd_loo contribution from the 3rd obs it should be the
# same as what we got above with the loo_i function and i=3:
print(loo_with_fn$pointwise[3, "elpd_loo"])
print(loo_3$pointwise[, "elpd_loo"])

# Check that the loo.matrix method gives same answer as loo.function method
log_lik_matrix &lt;- sapply(1:N, function(i) {
  llfun(data_i = fake_data[i,, drop=FALSE], draws = fake_posterior)
})
loo_with_mat &lt;- loo(log_lik_matrix)
all.equal(loo_with_mat$estimates, loo_with_fn$estimates) # should be TRUE!


## Not run: 
### For package developers: defining loo methods

# An example of a possible loo method for 'stanfit' objects (rstan package).
# A similar method is included in the rstan package.
# In order for users to be able to call loo(stanfit) instead of
# loo.stanfit(stanfit) the NAMESPACE needs to be handled appropriately
# (roxygen2 and devtools packages are good for that).
#
loo.stanfit &lt;-
 function(x,
         pars = "log_lik",
         ...,
         save_psis = FALSE,
         cores = getOption("mc.cores", 1)) {
  stopifnot(length(pars) == 1L)
  LLarray &lt;- loo::extract_log_lik(stanfit = x,
                                  parameter_name = pars,
                                  merge_chains = FALSE)
  r_eff &lt;- loo::relative_eff(x = exp(LLarray), cores = cores)
  loo::loo.array(LLarray,
                 r_eff = r_eff,
                 cores = cores,
                 save_psis = save_psis)
}

## End(Not run)


</code></pre>

<hr>
<h2 id='loo_approximate_posterior'>Efficient approximate leave-one-out cross-validation (LOO) for posterior
approximations</h2><span id='topic+loo_approximate_posterior'></span><span id='topic+loo_approximate_posterior.array'></span><span id='topic+loo_approximate_posterior.matrix'></span><span id='topic+loo_approximate_posterior.function'></span>

<h3>Description</h3>

<p>Efficient approximate leave-one-out cross-validation (LOO) for posterior
approximations
</p>


<h3>Usage</h3>

<pre><code class='language-R'>loo_approximate_posterior(x, log_p, log_g, ...)

## S3 method for class 'array'
loo_approximate_posterior(
  x,
  log_p,
  log_g,
  ...,
  save_psis = FALSE,
  cores = getOption("mc.cores", 1)
)

## S3 method for class 'matrix'
loo_approximate_posterior(
  x,
  log_p,
  log_g,
  ...,
  save_psis = FALSE,
  cores = getOption("mc.cores", 1)
)

## S3 method for class ''function''
loo_approximate_posterior(
  x,
  ...,
  data = NULL,
  draws = NULL,
  log_p = NULL,
  log_g = NULL,
  save_psis = FALSE,
  cores = getOption("mc.cores", 1)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="loo_approximate_posterior_+3A_x">x</code></td>
<td>
<p>A log-likelihood array, matrix, or function.
The <strong>Methods (by class)</strong> section, below, has detailed descriptions of how
to specify the inputs for each method.</p>
</td></tr>
<tr><td><code id="loo_approximate_posterior_+3A_log_p">log_p</code></td>
<td>
<p>The log-posterior (target) evaluated at S samples from the
proposal distribution (g). A vector of length S.</p>
</td></tr>
<tr><td><code id="loo_approximate_posterior_+3A_log_g">log_g</code></td>
<td>
<p>The log-density (proposal) evaluated at S samples from the
proposal distribution (g). A vector of length S.</p>
</td></tr>
<tr><td><code id="loo_approximate_posterior_+3A_save_psis">save_psis</code></td>
<td>
<p>Should the <code>"psis"</code> object created internally by
<code>loo_approximate_posterior()</code> be saved in the returned object? See
<code><a href="#topic+loo">loo()</a></code> for details.</p>
</td></tr>
<tr><td><code id="loo_approximate_posterior_+3A_cores">cores</code></td>
<td>
<p>The number of cores to use for parallelization. This defaults to
the option <code>mc.cores</code> which can be set for an entire R session by
<code>options(mc.cores = NUMBER)</code>. The old option <code>loo.cores</code> is now
deprecated but will be given precedence over <code>mc.cores</code> until
<code>loo.cores</code> is removed in a future release. <strong>As of version
2.0.0 the default is now 1 core if <code>mc.cores</code> is not set</strong>, but we
recommend using as many (or close to as many) cores as possible.
</p>

<ul>
<li><p> Note for Windows 10 users: it is <strong>strongly</strong>
<a href="https://github.com/stan-dev/loo/issues/94">recommended</a> to avoid using
the <code>.Rprofile</code> file to set <code>mc.cores</code> (using the <code>cores</code> argument or
setting <code>mc.cores</code> interactively or in a script is fine).
</p>
</li></ul>
</td></tr>
<tr><td><code id="loo_approximate_posterior_+3A_data">data</code>, <code id="loo_approximate_posterior_+3A_draws">draws</code>, <code id="loo_approximate_posterior_+3A_...">...</code></td>
<td>
<p>For the <code>loo_approximate_posterior.function()</code> method,
these are the data, posterior draws, and other arguments to pass to the
log-likelihood function. See the <strong>Methods (by class)</strong> section below for
details on how to specify these arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>loo_approximate_posterior()</code> function is an S3 generic and
methods are provided for 3-D pointwise log-likelihood arrays, pointwise
log-likelihood matrices, and log-likelihood functions. The implementation
works for posterior approximations where it is possible to compute the log
density for the posterior approximation.
</p>


<h3>Value</h3>

<p>The <code>loo_approximate_posterior()</code> methods return a named list with
class <code>c("psis_loo_ap", "psis_loo", "loo")</code>. It has the same structure
as the objects returned by <code><a href="#topic+loo">loo()</a></code> but with the additional slot:
</p>

<dl>
<dt><code>posterior_approximation</code></dt><dd>
<p>A list with two vectors, <code>log_p</code> and <code>log_g</code> of the same length
containing the posterior density and the approximation density
for the individual draws.
</p>
</dd>
</dl>



<h3>Methods (by class)</h3>


<ul>
<li> <p><code>loo_approximate_posterior(array)</code>: An <code class="reqn">I</code> by <code class="reqn">C</code> by <code class="reqn">N</code> array, where <code class="reqn">I</code>
is the number of MCMC iterations per chain, <code class="reqn">C</code> is the number of
chains, and <code class="reqn">N</code> is the number of data points.
</p>
</li>
<li> <p><code>loo_approximate_posterior(matrix)</code>: An <code class="reqn">S</code> by <code class="reqn">N</code> matrix, where <code class="reqn">S</code> is the size
of the posterior sample (with all chains merged) and <code class="reqn">N</code> is the number
of data points.
</p>
</li>
<li> <p><code>loo_approximate_posterior(`function`)</code>: A function <code>f()</code> that takes arguments <code>data_i</code> and <code>draws</code> and returns a
vector containing the log-likelihood for a single observation <code>i</code> evaluated
at each posterior draw. The function should be written such that, for each
observation <code>i</code> in <code>1:N</code>, evaluating
</p>
<div class="sourceCode"><pre>f(data_i = data[i,, drop=FALSE], draws = draws)
</pre></div>
<p>results in a vector of length <code>S</code> (size of posterior sample). The
log-likelihood function can also have additional arguments but <code>data_i</code> and
<code>draws</code> are required.
</p>
<p>If using the function method then the arguments <code>data</code> and <code>draws</code> must also
be specified in the call to <code>loo()</code>:
</p>

<ul>
<li> <p><code>data</code>: A data frame or matrix containing the data (e.g.
observed outcome and predictors) needed to compute the pointwise
log-likelihood. For each observation <code>i</code>, the <code>i</code>th row of
<code>data</code> will be passed to the <code>data_i</code> argument of the
log-likelihood function.
</p>
</li>
<li> <p><code>draws</code>: An object containing the posterior draws for any
parameters needed to compute the pointwise log-likelihood. Unlike
<code>data</code>, which is indexed by observation, for each observation the
entire object <code>draws</code> will be passed to the <code>draws</code> argument of
the log-likelihood function.
</p>
</li>
<li><p> The <code>...</code> can be used if your log-likelihood function takes additional
arguments. These arguments are used like the <code>draws</code> argument in that they
are recycled for each observation.
</p>
</li></ul>

</li></ul>


<h3>References</h3>

<p>Magnusson, M., Riis Andersen, M., Jonasson, J. and Vehtari, A. (2019).
Leave-One-Out Cross-Validation for Large Data.
In <em>Thirty-sixth International Conference on Machine Learning</em>,
PMLR 97:4244-4253.
</p>
<p>Magnusson, M., Riis Andersen, M., Jonasson, J. and Vehtari, A. (2020).
Leave-One-Out Cross-Validation for Model Comparison in Large Data.
In <em>Proceedings of the 23rd International Conference on Artificial
Intelligence and Statistics (AISTATS)</em>, PMLR 108:341-351.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+loo">loo()</a></code>, <code><a href="#topic+psis">psis()</a></code>, <code><a href="#topic+loo_compare">loo_compare()</a></code>
</p>

<hr>
<h2 id='loo_compare'>Model comparison</h2><span id='topic+loo_compare'></span><span id='topic+loo_compare.default'></span><span id='topic+print.compare.loo'></span><span id='topic+print.compare.loo_ss'></span>

<h3>Description</h3>

<p>Compare fitted models based on <a href="#topic+loo-glossary">ELPD</a>.
</p>
<p>By default the print method shows only the most important information. Use
<code>print(..., simplify=FALSE)</code> to print a more detailed summary.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>loo_compare(x, ...)

## Default S3 method:
loo_compare(x, ...)

## S3 method for class 'compare.loo'
print(x, ..., digits = 1, simplify = TRUE)

## S3 method for class 'compare.loo_ss'
print(x, ..., digits = 1, simplify = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="loo_compare_+3A_x">x</code></td>
<td>
<p>An object of class <code>"loo"</code> or a list of such objects. If a list is
used then the list names will be used as the model names in the output. See
<strong>Examples</strong>.</p>
</td></tr>
<tr><td><code id="loo_compare_+3A_...">...</code></td>
<td>
<p>Additional objects of class <code>"loo"</code>, if not passed in as a single
list.</p>
</td></tr>
<tr><td><code id="loo_compare_+3A_digits">digits</code></td>
<td>
<p>For the print method only, the number of digits to use when
printing.</p>
</td></tr>
<tr><td><code id="loo_compare_+3A_simplify">simplify</code></td>
<td>
<p>For the print method only, should only the essential columns
of the summary matrix be printed? The entire matrix is always returned, but
by default only the most important columns are printed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When comparing two fitted models, we can estimate the difference in their
expected predictive accuracy by the difference in
<code><a href="#topic+loo-glossary">elpd_loo</a></code> or <code>elpd_waic</code> (or multiplied by <code class="reqn">-2</code>, if
desired, to be on the deviance scale).
</p>
<p>When using <code>loo_compare()</code>, the returned matrix will have one row per model
and several columns of estimates. The values in the
<code><a href="#topic+loo-glossary">elpd_diff</a></code> and <code><a href="#topic+loo-glossary">se_diff</a></code> columns of the
returned matrix are computed by making pairwise comparisons between each
model and the model with the largest ELPD (the model in the first row). For
this reason the <code>elpd_diff</code> column will always have the value <code>0</code> in the
first row (i.e., the difference between the preferred model and itself) and
negative values in subsequent rows for the remaining models.
</p>
<p>To compute the standard error of the difference in <a href="#topic+loo-glossary">ELPD</a> &mdash;
which should not be expected to equal the difference of the standard errors
&mdash; we use a paired estimate to take advantage of the fact that the same
set of <code class="reqn">N</code> data points was used to fit both models. These calculations
should be most useful when <code class="reqn">N</code> is large, because then non-normality of
the distribution is not such an issue when estimating the uncertainty in
these sums. These standard errors, for all their flaws, should give a
better sense of uncertainty than what is obtained using the current
standard approach of comparing differences of deviances to a Chi-squared
distribution, a practice derived for Gaussian linear models or
asymptotically, and which only applies to nested models in any case.
Sivula et al. (2022) discuss the conditions when the normal
approximation used for SE and <code>se_diff</code> is good.
</p>
<p>If more than <code class="reqn">11</code> models are compared, we internally recompute the model
differences using the median model by ELPD as the baseline model. We then
estimate whether the differences in predictive performance are potentially
due to chance as described by McLatchie and Vehtari (2023). This will flag
a warning if it is deemed that there is a risk of over-fitting due to the
selection process. In that case users are recommended to avoid model
selection based on LOO-CV, and instead to favor model averaging/stacking or
projection predictive inference.
</p>


<h3>Value</h3>

<p>A matrix with class <code>"compare.loo"</code> that has its own
print method. See the <strong>Details</strong> section.
</p>


<h3>References</h3>

<p>Vehtari, A., Gelman, A., and Gabry, J. (2017). Practical Bayesian model
evaluation using leave-one-out cross-validation and WAIC.
<em>Statistics and Computing</em>. 27(5), 1413&ndash;1432. doi:10.1007/s11222-016-9696-4
(<a href="https://link.springer.com/article/10.1007/s11222-016-9696-4">journal version</a>,
<a href="https://arxiv.org/abs/1507.04544">preprint arXiv:1507.04544</a>).
</p>
<p>Vehtari, A., Simpson, D., Gelman, A., Yao, Y., and Gabry, J. (2024).
Pareto smoothed importance sampling. <em>Journal of Machine Learning Research</em>,
25(72):1-58.
<a href="https://jmlr.org/papers/v25/19-556.html">PDF</a>
</p>
<p>Sivula, T, Magnusson, M., Matamoros A. A., and Vehtari, A. (2022).
Uncertainty in Bayesian leave-one-out cross-validation based model
comparison. <a href="https://arxiv.org/abs/2008.10296v3">preprint arXiv:2008.10296v3.</a>.
</p>
<p>McLatchie, Y., and Vehtari, A. (2023).  Efficient estimation and
correction of selection-induced bias with order statistics.
<a href="https://arxiv.org/abs/2309.03742">preprint arXiv:2309.03742</a>
</p>


<h3>See Also</h3>


<ul>
<li><p> The <a href="https://mc-stan.org/loo/articles/online-only/faq.html">FAQ page</a> on
the <strong>loo</strong> website for answers to frequently asked questions.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'># very artificial example, just for demonstration!
LL &lt;- example_loglik_array()
loo1 &lt;- loo(LL)     # should be worst model when compared
loo2 &lt;- loo(LL + 1) # should be second best model when compared
loo3 &lt;- loo(LL + 2) # should be best model when compared

comp &lt;- loo_compare(loo1, loo2, loo3)
print(comp, digits = 2)

# show more details with simplify=FALSE
# (will be the same for all models in this artificial example)
print(comp, simplify = FALSE, digits = 3)

# can use a list of objects with custom names
# will use apple, banana, and cherry, as the names in the output
loo_compare(list("apple" = loo1, "banana" = loo2, "cherry" = loo3))

## Not run: 
# works for waic (and kfold) too
loo_compare(waic(LL), waic(LL - 10))

## End(Not run)

</code></pre>

<hr>
<h2 id='loo_model_weights'>Model averaging/weighting via stacking or pseudo-BMA weighting</h2><span id='topic+loo_model_weights'></span><span id='topic+loo_model_weights.default'></span><span id='topic+stacking_weights'></span><span id='topic+pseudobma_weights'></span>

<h3>Description</h3>

<p>Model averaging via stacking of predictive distributions, pseudo-BMA
weighting or pseudo-BMA+ weighting with the Bayesian bootstrap. See Yao et
al. (2018), Vehtari, Gelman, and Gabry (2017), and Vehtari, Simpson,
Gelman, Yao, and Gabry (2024) for background.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>loo_model_weights(x, ...)

## Default S3 method:
loo_model_weights(
  x,
  ...,
  method = c("stacking", "pseudobma"),
  optim_method = "BFGS",
  optim_control = list(),
  BB = TRUE,
  BB_n = 1000,
  alpha = 1,
  r_eff_list = NULL,
  cores = getOption("mc.cores", 1)
)

stacking_weights(lpd_point, optim_method = "BFGS", optim_control = list())

pseudobma_weights(lpd_point, BB = TRUE, BB_n = 1000, alpha = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="loo_model_weights_+3A_x">x</code></td>
<td>
<p>A list of <code>"psis_loo"</code> objects (objects returned by <code><a href="#topic+loo">loo()</a></code>) or
pointwise log-likelihood matrices or , one for each model. If the list
elements are named the names will be used to label the models in the
results. Each matrix/object should have dimensions <code class="reqn">S</code> by <code class="reqn">N</code>,
where <code class="reqn">S</code> is the size of the posterior sample (with all chains merged)
and <code class="reqn">N</code> is the number of data points. If <code>x</code> is a list of
log-likelihood matrices then <code><a href="#topic+loo">loo()</a></code> is called internally on each matrix.
Currently the <code>loo_model_weights()</code> function is not implemented to be used
with results from K-fold CV, but you can still obtain weights using K-fold
CV results by calling the <code>stacking_weights()</code> function directly.</p>
</td></tr>
<tr><td><code id="loo_model_weights_+3A_...">...</code></td>
<td>
<p>Unused, except for the generic to pass arguments to individual
methods.</p>
</td></tr>
<tr><td><code id="loo_model_weights_+3A_method">method</code></td>
<td>
<p>Either <code>"stacking"</code> (the default) or <code>"pseudobma"</code>, indicating which method
to use for obtaining the weights. <code>"stacking"</code> refers to stacking of
predictive distributions and  <code>"pseudobma"</code> refers to pseudo-BMA+ weighting
(or plain pseudo-BMA weighting if argument <code>BB</code> is <code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="loo_model_weights_+3A_optim_method">optim_method</code></td>
<td>
<p>If <code>method="stacking"</code>, a string passed to the <code>method</code>
argument of <code><a href="stats.html#topic+constrOptim">stats::constrOptim()</a></code> to specify the optimization algorithm.
The default is <code>optim_method="BFGS"</code>, but other options are available (see
<code><a href="stats.html#topic+optim">stats::optim()</a></code>).</p>
</td></tr>
<tr><td><code id="loo_model_weights_+3A_optim_control">optim_control</code></td>
<td>
<p>If <code>method="stacking"</code>, a list of control parameters for
optimization passed to the <code>control</code> argument of <code><a href="stats.html#topic+constrOptim">stats::constrOptim()</a></code>.</p>
</td></tr>
<tr><td><code id="loo_model_weights_+3A_bb">BB</code></td>
<td>
<p>Logical used when <code>"method"</code>=<code>"pseudobma"</code>. If
<code>TRUE</code> (the default), the Bayesian bootstrap will be used to adjust
the pseudo-BMA weighting, which is called pseudo-BMA+ weighting. It helps
regularize the weight away from 0 and 1, so as to reduce the variance.</p>
</td></tr>
<tr><td><code id="loo_model_weights_+3A_bb_n">BB_n</code></td>
<td>
<p>For pseudo-BMA+ weighting only, the number of samples to use for
the Bayesian bootstrap. The default is <code>BB_n=1000</code>.</p>
</td></tr>
<tr><td><code id="loo_model_weights_+3A_alpha">alpha</code></td>
<td>
<p>Positive scalar shape parameter in the Dirichlet distribution
used for the Bayesian bootstrap. The default is <code>alpha=1</code>, which
corresponds to a uniform distribution on the simplex space.</p>
</td></tr>
<tr><td><code id="loo_model_weights_+3A_r_eff_list">r_eff_list</code></td>
<td>
<p>Optionally, a list of relative effective sample size
estimates for the likelihood <code>(exp(log_lik))</code> of each observation in
each model. See <code><a href="#topic+psis">psis()</a></code> and  <code><a href="#topic+relative_eff">relative_eff()</a></code> helper
function for computing <code>r_eff</code>. If <code>x</code> is a list of <code>"psis_loo"</code>
objects then <code>r_eff_list</code> is ignored.</p>
</td></tr>
<tr><td><code id="loo_model_weights_+3A_cores">cores</code></td>
<td>
<p>The number of cores to use for parallelization. This defaults to
the option <code>mc.cores</code> which can be set for an entire R session by
<code>options(mc.cores = NUMBER)</code>. The old option <code>loo.cores</code> is now
deprecated but will be given precedence over <code>mc.cores</code> until
<code>loo.cores</code> is removed in a future release. <strong>As of version
2.0.0 the default is now 1 core if <code>mc.cores</code> is not set</strong>, but we
recommend using as many (or close to as many) cores as possible.
</p>

<ul>
<li><p> Note for Windows 10 users: it is <strong>strongly</strong>
<a href="https://github.com/stan-dev/loo/issues/94">recommended</a> to avoid using
the <code>.Rprofile</code> file to set <code>mc.cores</code> (using the <code>cores</code> argument or
setting <code>mc.cores</code> interactively or in a script is fine).
</p>
</li></ul>
</td></tr>
<tr><td><code id="loo_model_weights_+3A_lpd_point">lpd_point</code></td>
<td>
<p>If calling <code>stacking_weights()</code> or <code>pseudobma_weights()</code>
directly, a matrix of pointwise leave-one-out (or K-fold) log likelihoods
evaluated for different models. It should be a <code class="reqn">N</code> by <code class="reqn">K</code>  matrix
where <code class="reqn">N</code> is sample size and <code class="reqn">K</code> is the number of models. Each
column corresponds to one model. These values can be calculated
approximately using <code><a href="#topic+loo">loo()</a></code> or by running exact leave-one-out or K-fold
cross-validation.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>loo_model_weights()</code> is a wrapper around the <code>stacking_weights()</code> and
<code>pseudobma_weights()</code> functions that implements stacking, pseudo-BMA, and
pseudo-BMA+ weighting for combining multiple predictive distributions. We can
use approximate or exact leave-one-out cross-validation (LOO-CV) or K-fold CV
to estimate the expected log predictive density (ELPD).
</p>
<p>The stacking method (<code>method="stacking"</code>), which is the default for
<code>loo_model_weights()</code>, combines all models by maximizing the leave-one-out
predictive density of the combination distribution. That is, it finds the
optimal linear combining weights for maximizing the leave-one-out log score.
</p>
<p>The pseudo-BMA method (<code>method="pseudobma"</code>) finds the relative weights
proportional to the ELPD of each model. However, when
<code>method="pseudobma"</code>, the default is to also use the Bayesian bootstrap
(<code>BB=TRUE</code>), which corresponds to the pseudo-BMA+ method. The Bayesian
bootstrap  takes into account the uncertainty of finite data points and
regularizes the weights away from the extremes of 0 and 1.
</p>
<p>In general, we recommend stacking for averaging predictive distributions,
while pseudo-BMA+ can serve as a computationally easier alternative.
</p>


<h3>Value</h3>

<p>A numeric vector containing one weight for each model.
</p>


<h3>References</h3>

<p>Vehtari, A., Gelman, A., and Gabry, J. (2017). Practical Bayesian model
evaluation using leave-one-out cross-validation and WAIC.
<em>Statistics and Computing</em>. 27(5), 1413&ndash;1432. doi:10.1007/s11222-016-9696-4
(<a href="https://link.springer.com/article/10.1007/s11222-016-9696-4">journal version</a>,
<a href="https://arxiv.org/abs/1507.04544">preprint arXiv:1507.04544</a>).
</p>
<p>Vehtari, A., Simpson, D., Gelman, A., Yao, Y., and Gabry, J. (2024).
Pareto smoothed importance sampling. <em>Journal of Machine Learning Research</em>,
25(72):1-58.
<a href="https://jmlr.org/papers/v25/19-556.html">PDF</a>
</p>
<p>Yao, Y., Vehtari, A., Simpson, D., and Gelman, A. (2018) Using
stacking to average Bayesian predictive distributions.
<em>Bayesian Analysis</em>, advance publication,  doi:10.1214/17-BA1091.
(<a href="https://projecteuclid.org/euclid.ba/1516093227">online</a>).
</p>


<h3>See Also</h3>


<ul>
<li><p> The <strong>loo</strong> package <a href="https://mc-stan.org/loo/articles/">vignettes</a>, particularly
<a href="https://mc-stan.org/loo/articles/loo2-weights.html">Bayesian Stacking and Pseudo-BMA weights using the <strong>loo</strong> package</a>.
</p>
</li>
<li> <p><code><a href="#topic+loo">loo()</a></code> for details on leave-one-out ELPD estimation.
</p>
</li>
<li> <p><code><a href="stats.html#topic+constrOptim">constrOptim()</a></code> for the choice of optimization methods and control-parameters.
</p>
</li>
<li> <p><code><a href="#topic+relative_eff">relative_eff()</a></code> for computing <code>r_eff</code>.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
### Demonstrating usage after fitting models with RStan
library(rstan)

# generate fake data from N(0,1).
N &lt;- 100
y &lt;- rnorm(N, 0, 1)

# Suppose we have three models: N(-1, sigma), N(0.5, sigma) and N(0.6,sigma).
stan_code &lt;- "
  data {
    int N;
    vector[N] y;
    real mu_fixed;
  }
  parameters {
    real&lt;lower=0&gt; sigma;
  }
  model {
    sigma ~ exponential(1);
    y ~ normal(mu_fixed, sigma);
  }
  generated quantities {
    vector[N] log_lik;
    for (n in 1:N) log_lik[n] = normal_lpdf(y[n]| mu_fixed, sigma);
  }"

mod &lt;- stan_model(model_code = stan_code)
fit1 &lt;- sampling(mod, data=list(N=N, y=y, mu_fixed=-1))
fit2 &lt;- sampling(mod, data=list(N=N, y=y, mu_fixed=0.5))
fit3 &lt;- sampling(mod, data=list(N=N, y=y, mu_fixed=0.6))
model_list &lt;- list(fit1, fit2, fit3)
log_lik_list &lt;- lapply(model_list, extract_log_lik)

# optional but recommended
r_eff_list &lt;- lapply(model_list, function(x) {
  ll_array &lt;- extract_log_lik(x, merge_chains = FALSE)
  relative_eff(exp(ll_array))
})

# stacking method:
wts1 &lt;- loo_model_weights(
  log_lik_list,
  method = "stacking",
  r_eff_list = r_eff_list,
  optim_control = list(reltol=1e-10)
)
print(wts1)

# can also pass a list of psis_loo objects to avoid recomputing loo
loo_list &lt;- lapply(1:length(log_lik_list), function(j) {
  loo(log_lik_list[[j]], r_eff = r_eff_list[[j]])
})

wts2 &lt;- loo_model_weights(
  loo_list,
  method = "stacking",
  optim_control = list(reltol=1e-10)
)
all.equal(wts1, wts2)

# can provide names to be used in the results
loo_model_weights(setNames(loo_list, c("A", "B", "C")))


# pseudo-BMA+ method:
set.seed(1414)
loo_model_weights(loo_list, method = "pseudobma")

# pseudo-BMA method (set BB = FALSE):
loo_model_weights(loo_list, method = "pseudobma", BB = FALSE)

# calling stacking_weights or pseudobma_weights directly
lpd1 &lt;- loo(log_lik_list[[1]], r_eff = r_eff_list[[1]])$pointwise[,1]
lpd2 &lt;- loo(log_lik_list[[2]], r_eff = r_eff_list[[2]])$pointwise[,1]
lpd3 &lt;- loo(log_lik_list[[3]], r_eff = r_eff_list[[3]])$pointwise[,1]
stacking_weights(cbind(lpd1, lpd2, lpd3))
pseudobma_weights(cbind(lpd1, lpd2, lpd3))
pseudobma_weights(cbind(lpd1, lpd2, lpd3), BB = FALSE)

## End(Not run)

</code></pre>

<hr>
<h2 id='loo_moment_match'>Moment matching for efficient approximate leave-one-out cross-validation (LOO)</h2><span id='topic+loo_moment_match'></span><span id='topic+loo_moment_match.default'></span>

<h3>Description</h3>

<p>Moment matching algorithm for updating a loo object when Pareto k estimates
are large.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>loo_moment_match(x, ...)

## Default S3 method:
loo_moment_match(
  x,
  loo,
  post_draws,
  log_lik_i,
  unconstrain_pars,
  log_prob_upars,
  log_lik_i_upars,
  max_iters = 30L,
  k_threshold = NULL,
  split = TRUE,
  cov = TRUE,
  cores = getOption("mc.cores", 1),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="loo_moment_match_+3A_x">x</code></td>
<td>
<p>A fitted model object.</p>
</td></tr>
<tr><td><code id="loo_moment_match_+3A_...">...</code></td>
<td>
<p>Further arguments passed to the custom functions documented above.</p>
</td></tr>
<tr><td><code id="loo_moment_match_+3A_loo">loo</code></td>
<td>
<p>A loo object to be modified.</p>
</td></tr>
<tr><td><code id="loo_moment_match_+3A_post_draws">post_draws</code></td>
<td>
<p>A function the takes <code>x</code> as the first argument and returns
a matrix of posterior draws of the model parameters.</p>
</td></tr>
<tr><td><code id="loo_moment_match_+3A_log_lik_i">log_lik_i</code></td>
<td>
<p>A function that takes <code>x</code> and <code>i</code> and returns a matrix (one
column per chain) or a vector (all chains stacked) of log-likelihood draws
of the <code>i</code>th observation based on the model <code>x</code>. If the draws are obtained
using MCMC, the matrix with MCMC chains separated is preferred.</p>
</td></tr>
<tr><td><code id="loo_moment_match_+3A_unconstrain_pars">unconstrain_pars</code></td>
<td>
<p>A function that takes arguments <code>x</code>, and <code>pars</code> and
returns posterior draws on the unconstrained space based on the posterior
draws on the constrained space passed via <code>pars</code>.</p>
</td></tr>
<tr><td><code id="loo_moment_match_+3A_log_prob_upars">log_prob_upars</code></td>
<td>
<p>A function that takes arguments <code>x</code> and <code>upars</code> and
returns a matrix of log-posterior density values of the unconstrained
posterior draws passed via <code>upars</code>.</p>
</td></tr>
<tr><td><code id="loo_moment_match_+3A_log_lik_i_upars">log_lik_i_upars</code></td>
<td>
<p>A function that takes arguments <code>x</code>, <code>upars</code>, and <code>i</code>
and returns a vector of log-likelihood draws of the <code>i</code>th observation based
on the unconstrained posterior draws passed via <code>upars</code>.</p>
</td></tr>
<tr><td><code id="loo_moment_match_+3A_max_iters">max_iters</code></td>
<td>
<p>Maximum number of moment matching iterations. Usually this
does not need to be modified. If the maximum number of iterations is
reached, there will be a warning, and increasing <code>max_iters</code> may improve
accuracy.</p>
</td></tr>
<tr><td><code id="loo_moment_match_+3A_k_threshold">k_threshold</code></td>
<td>
<p>Threshold value for Pareto k values above which the moment
matching algorithm is used. The default value is <code>1 - 1 / log10(S)</code>,
where <code>S</code> is the sample size.</p>
</td></tr>
<tr><td><code id="loo_moment_match_+3A_split">split</code></td>
<td>
<p>Logical; Indicate whether to do the split transformation or not
at the end of moment matching for each LOO fold.</p>
</td></tr>
<tr><td><code id="loo_moment_match_+3A_cov">cov</code></td>
<td>
<p>Logical; Indicate whether to match the covariance matrix of the
samples or not. If <code>FALSE</code>, only the mean and marginal variances are
matched.</p>
</td></tr>
<tr><td><code id="loo_moment_match_+3A_cores">cores</code></td>
<td>
<p>The number of cores to use for parallelization. This defaults to
the option <code>mc.cores</code> which can be set for an entire R session by
<code>options(mc.cores = NUMBER)</code>. The old option <code>loo.cores</code> is now
deprecated but will be given precedence over <code>mc.cores</code> until
<code>loo.cores</code> is removed in a future release. <strong>As of version
2.0.0 the default is now 1 core if <code>mc.cores</code> is not set</strong>, but we
recommend using as many (or close to as many) cores as possible.
</p>

<ul>
<li><p> Note for Windows 10 users: it is <strong>strongly</strong>
<a href="https://github.com/stan-dev/loo/issues/94">recommended</a> to avoid using
the <code>.Rprofile</code> file to set <code>mc.cores</code> (using the <code>cores</code> argument or
setting <code>mc.cores</code> interactively or in a script is fine).
</p>
</li></ul>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>loo_moment_match()</code> function is an S3 generic and we provide a
default method that takes as arguments user-specified functions
<code>post_draws</code>, <code>log_lik_i</code>, <code>unconstrain_pars</code>, <code>log_prob_upars</code>, and
<code>log_lik_i_upars</code>. All of these functions should take <code>...</code>. as an argument
in addition to those specified for each function.
</p>


<h3>Value</h3>

<p>The <code>loo_moment_match()</code> methods return an updated <code>loo</code> object. The
structure of the updated <code>loo</code> object is similar, but the method also
stores the original Pareto k diagnostic values in the diagnostics field.
</p>


<h3>Methods (by class)</h3>


<ul>
<li> <p><code>loo_moment_match(default)</code>: A default method that takes as arguments a
user-specified model object <code>x</code>, a <code>loo</code> object and user-specified
functions <code>post_draws</code>, <code>log_lik_i</code>, <code>unconstrain_pars</code>, <code>log_prob_upars</code>,
and <code>log_lik_i_upars</code>.
</p>
</li></ul>


<h3>References</h3>

<p>Paananen, T., Piironen, J., Buerkner, P.-C., Vehtari, A. (2021).
Implicitly adaptive importance sampling. <em>Statistics and Computing</em>, 31, 16.
doi:10.1007/s11222-020-09982-2. arXiv preprint arXiv:1906.08850.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+loo">loo()</a></code>, <code><a href="#topic+loo_moment_match_split">loo_moment_match_split()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># See the vignette for loo_moment_match()
</code></pre>

<hr>
<h2 id='loo_moment_match_split'>Split moment matching for efficient approximate leave-one-out cross-validation (LOO)</h2><span id='topic+loo_moment_match_split'></span>

<h3>Description</h3>

<p>A function that computes the split moment matching importance sampling loo.
Takes in the moment matching total transformation, transforms only half
of the draws, and computes a single elpd using multiple importance sampling.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>loo_moment_match_split(
  x,
  upars,
  cov,
  total_shift,
  total_scaling,
  total_mapping,
  i,
  log_prob_upars,
  log_lik_i_upars,
  r_eff_i,
  cores,
  is_method,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="loo_moment_match_split_+3A_x">x</code></td>
<td>
<p>A fitted model object.</p>
</td></tr>
<tr><td><code id="loo_moment_match_split_+3A_upars">upars</code></td>
<td>
<p>A matrix containing the model parameters in unconstrained space
where they can have any real value.</p>
</td></tr>
<tr><td><code id="loo_moment_match_split_+3A_cov">cov</code></td>
<td>
<p>Logical; Indicate whether to match the covariance matrix of the
samples or not. If <code>FALSE</code>, only the mean and marginal variances are
matched.</p>
</td></tr>
<tr><td><code id="loo_moment_match_split_+3A_total_shift">total_shift</code></td>
<td>
<p>A vector representing the total shift made by the moment
matching algorithm.</p>
</td></tr>
<tr><td><code id="loo_moment_match_split_+3A_total_scaling">total_scaling</code></td>
<td>
<p>A vector representing the total scaling of marginal
variance made by the moment matching algorithm.</p>
</td></tr>
<tr><td><code id="loo_moment_match_split_+3A_total_mapping">total_mapping</code></td>
<td>
<p>A vector representing the total covariance
transformation made by the moment matching algorithm.</p>
</td></tr>
<tr><td><code id="loo_moment_match_split_+3A_i">i</code></td>
<td>
<p>Observation index.</p>
</td></tr>
<tr><td><code id="loo_moment_match_split_+3A_log_prob_upars">log_prob_upars</code></td>
<td>
<p>A function that takes arguments <code>x</code> and <code>upars</code> and
returns a matrix of log-posterior density values of the unconstrained
posterior draws passed via <code>upars</code>.</p>
</td></tr>
<tr><td><code id="loo_moment_match_split_+3A_log_lik_i_upars">log_lik_i_upars</code></td>
<td>
<p>A function that takes arguments <code>x</code>, <code>upars</code>, and <code>i</code>
and returns a vector of log-likeliood draws of the <code>i</code>th observation based
on the unconstrained posterior draws passed via <code>upars</code>.</p>
</td></tr>
<tr><td><code id="loo_moment_match_split_+3A_r_eff_i">r_eff_i</code></td>
<td>
<p>MCMC relative effective sample size of the <code>i</code>'th log
likelihood draws.</p>
</td></tr>
<tr><td><code id="loo_moment_match_split_+3A_cores">cores</code></td>
<td>
<p>The number of cores to use for parallelization. This defaults to
the option <code>mc.cores</code> which can be set for an entire R session by
<code>options(mc.cores = NUMBER)</code>. The old option <code>loo.cores</code> is now
deprecated but will be given precedence over <code>mc.cores</code> until
<code>loo.cores</code> is removed in a future release. <strong>As of version
2.0.0 the default is now 1 core if <code>mc.cores</code> is not set</strong>, but we
recommend using as many (or close to as many) cores as possible.
</p>

<ul>
<li><p> Note for Windows 10 users: it is <strong>strongly</strong>
<a href="https://github.com/stan-dev/loo/issues/94">recommended</a> to avoid using
the <code>.Rprofile</code> file to set <code>mc.cores</code> (using the <code>cores</code> argument or
setting <code>mc.cores</code> interactively or in a script is fine).
</p>
</li></ul>
</td></tr>
<tr><td><code id="loo_moment_match_split_+3A_is_method">is_method</code></td>
<td>
<p>The importance sampling method to use. The following methods
are implemented:
</p>

<ul>
<li> <p><code><a href="#topic+psis">&quot;psis&quot;</a></code>: Pareto-Smoothed Importance Sampling (PSIS). Default method.
</p>
</li>
<li> <p><code><a href="#topic+tis">&quot;tis&quot;</a></code>: Truncated Importance Sampling (TIS) with truncation at
<code>sqrt(S)</code>, where <code>S</code> is the number of posterior draws.
</p>
</li>
<li> <p><code><a href="#topic+sis">&quot;sis&quot;</a></code>: Standard Importance Sampling (SIS).
</p>
</li></ul>
</td></tr>
<tr><td><code id="loo_moment_match_split_+3A_...">...</code></td>
<td>
<p>Further arguments passed to the custom functions documented above.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing the updated log-importance weights and
log-likelihood values. Also returns the updated MCMC effective sample size
and the integrand-specific log-importance weights.
</p>


<h3>References</h3>

<p>Paananen, T., Piironen, J., Buerkner, P.-C., Vehtari, A. (2021).
Implicitly adaptive importance sampling. <em>Statistics and Computing</em>, 31, 16.
doi:10.1007/s11222-020-09982-2. arXiv preprint arXiv:1906.08850.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+loo">loo()</a></code>, <code><a href="#topic+loo_moment_match">loo_moment_match()</a></code>
</p>

<hr>
<h2 id='loo_predictive_metric'>Estimate leave-one-out predictive performance..</h2><span id='topic+loo_predictive_metric'></span><span id='topic+loo_predictive_metric.matrix'></span>

<h3>Description</h3>

<p>The <code>loo_predictive_metric()</code> function computes estimates of leave-one-out
predictive metrics given a set of predictions and observations. Currently
supported metrics are mean absolute error, mean squared error and root mean
squared error for continuous predictions and accuracy and balanced accuracy
for binary classification. Predictions are passed on to the <code><a href="#topic+E_loo">E_loo()</a></code>
function, so this function assumes that the PSIS approximation is working
well.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>loo_predictive_metric(x, ...)

## S3 method for class 'matrix'
loo_predictive_metric(
  x,
  y,
  log_lik,
  ...,
  metric = c("mae", "rmse", "mse", "acc", "balanced_acc"),
  r_eff = 1,
  cores = getOption("mc.cores", 1)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="loo_predictive_metric_+3A_x">x</code></td>
<td>
<p>A numeric matrix of predictions.</p>
</td></tr>
<tr><td><code id="loo_predictive_metric_+3A_...">...</code></td>
<td>
<p>Additional arguments passed on to <code><a href="#topic+E_loo">E_loo()</a></code></p>
</td></tr>
<tr><td><code id="loo_predictive_metric_+3A_y">y</code></td>
<td>
<p>A numeric vector of observations. Length should be equal to the
number of rows in <code>x</code>.</p>
</td></tr>
<tr><td><code id="loo_predictive_metric_+3A_log_lik">log_lik</code></td>
<td>
<p>A matrix of pointwise log-likelihoods. Should be of same
dimension as <code>x</code>.</p>
</td></tr>
<tr><td><code id="loo_predictive_metric_+3A_metric">metric</code></td>
<td>
<p>The type of predictive metric to be used. Currently
supported options are <code>"mae"</code>, <code>"rmse"</code> and <code>"mse"</code> for regression and
for binary classification <code>"acc"</code> and <code>"balanced_acc"</code>.
</p>

<dl>
<dt><code>"mae"</code></dt><dd>
<p>Mean absolute error.
</p>
</dd>
<dt><code>"mse"</code></dt><dd>
<p>Mean squared error.
</p>
</dd>
<dt><code>"rmse"</code></dt><dd>
<p>Root mean squared error, given by as the square root of <code>MSE</code>.
</p>
</dd>
<dt><code>"acc"</code></dt><dd>
<p>The proportion of predictions indicating the correct outcome.
</p>
</dd>
<dt><code>"balanced_acc"</code></dt><dd>
<p>Balanced accuracy is given by the average of true positive and true
negative rates.
</p>
</dd>
</dl>
</td></tr>
<tr><td><code id="loo_predictive_metric_+3A_r_eff">r_eff</code></td>
<td>
<p>A Vector of relative effective sample size estimates containing
one element per observation. See <code><a href="#topic+psis">psis()</a></code> for more details.</p>
</td></tr>
<tr><td><code id="loo_predictive_metric_+3A_cores">cores</code></td>
<td>
<p>The number of cores to use for parallelization of <code style="white-space: pre;">&#8288;[psis()]&#8288;</code>.
See <code><a href="#topic+psis">psis()</a></code> for details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with the following components:
</p>

<dl>
<dt><code>estimate</code></dt><dd>
<p>Estimate of the given metric.
</p>
</dd>
<dt><code>se</code></dt><dd>
<p>Standard error of the estimate.
</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>
if (requireNamespace("rstanarm", quietly = TRUE)) {
# Use rstanarm package to quickly fit a model and get both a log-likelihood
# matrix and draws from the posterior predictive distribution
library("rstanarm")

# data from help("lm")
ctl &lt;- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)
trt &lt;- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)
d &lt;- data.frame(
  weight = c(ctl, trt),
  group = gl(2, 10, 20, labels = c("Ctl","Trt"))
)
fit &lt;- stan_glm(weight ~ group, data = d, refresh = 0)
ll &lt;- log_lik(fit)
r_eff &lt;- relative_eff(exp(-ll), chain_id = rep(1:4, each = 1000))

mu_pred &lt;- posterior_epred(fit)
# Leave-one-out mean absolute error of predictions
mae &lt;- loo_predictive_metric(x = mu_pred, y = d$weight, log_lik = ll,
                            pred_error = 'mae', r_eff = r_eff)
# Leave-one-out 90%-quantile of mean absolute error
mae_90q &lt;- loo_predictive_metric(x = mu_pred, y = d$weight, log_lik = ll,
                                pred_error = 'mae', r_eff = r_eff,
                                type = 'quantile', probs = 0.9)
}

</code></pre>

<hr>
<h2 id='loo_subsample'>Efficient approximate leave-one-out cross-validation (LOO) using subsampling,
so that less costly and more approximate computation is made for all LOO-fold,
and more costly and accurate computations are made only for m&lt;N LOO-folds.</h2><span id='topic+loo_subsample'></span><span id='topic+loo_subsample.function'></span>

<h3>Description</h3>

<p>Efficient approximate leave-one-out cross-validation (LOO) using subsampling,
so that less costly and more approximate computation is made for all LOO-fold,
and more costly and accurate computations are made only for m&lt;N LOO-folds.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>loo_subsample(x, ...)

## S3 method for class ''function''
loo_subsample(
  x,
  ...,
  data = NULL,
  draws = NULL,
  observations = 400,
  log_p = NULL,
  log_g = NULL,
  r_eff = 1,
  save_psis = FALSE,
  cores = getOption("mc.cores", 1),
  loo_approximation = "plpd",
  loo_approximation_draws = NULL,
  estimator = "diff_srs",
  llgrad = NULL,
  llhess = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="loo_subsample_+3A_x">x</code></td>
<td>
<p>A function. The <strong>Methods (by class)</strong> section, below, has detailed
descriptions of how to specify the inputs.</p>
</td></tr>
<tr><td><code id="loo_subsample_+3A_data">data</code>, <code id="loo_subsample_+3A_draws">draws</code>, <code id="loo_subsample_+3A_...">...</code></td>
<td>
<p>For <code>loo_subsample.function()</code>, these are the data,
posterior draws, and other arguments to pass to the log-likelihood
function. Note that for some <code>loo_approximation</code>s, the draws will be replaced
by the posteriors summary statistics to compute loo approximations. See
argument <code>loo_approximation</code> for details.</p>
</td></tr>
<tr><td><code id="loo_subsample_+3A_observations">observations</code></td>
<td>
<p>The subsample observations to use. The argument can take
four (4) types of arguments:
</p>

<ul>
<li> <p><code>NULL</code> to use all observations. The algorithm then just uses
standard <code>loo()</code> or <code>loo_approximate_posterior()</code>.
</p>
</li>
<li><p> A single integer to specify the number of observations to be subsampled.
</p>
</li>
<li><p> A vector of integers to provide the indices used to subset the data.
<em>These observations need to be subsampled with the same scheme as given by
the <code>estimator</code> argument</em>.
</p>
</li>
<li><p> A <code>psis_loo_ss</code> object to use the same observations that were used in a
previous call to <code>loo_subsample()</code>.
</p>
</li></ul>
</td></tr>
<tr><td><code id="loo_subsample_+3A_log_p">log_p</code>, <code id="loo_subsample_+3A_log_g">log_g</code></td>
<td>
<p>Should be supplied only if approximate posterior draws are
used. The default (<code>NULL</code>) indicates draws are from &quot;true&quot; posterior (i.e.
using MCMC). If not <code>NULL</code> then they should be specified as described in
<code><a href="#topic+loo_approximate_posterior">loo_approximate_posterior()</a></code>.</p>
</td></tr>
<tr><td><code id="loo_subsample_+3A_r_eff">r_eff</code></td>
<td>
<p>Vector of relative effective sample size estimates for the
likelihood (<code>exp(log_lik)</code>) of each observation. This is related to
the relative efficiency of estimating the normalizing term in
self-normalized importance sampling when using posterior draws obtained
with MCMC. If MCMC draws are used and <code>r_eff</code> is not provided then
the reported PSIS effective sample sizes and Monte Carlo error estimates
can be over-optimistic. If the posterior draws are (near) independent then
<code>r_eff=1</code> can be used. <code>r_eff</code> has to be a scalar (same value is used
for all observations) or a vector with length equal to the number of
observations. The default value is 1. See the <code><a href="#topic+relative_eff">relative_eff()</a></code> helper
functions for help computing <code>r_eff</code>.</p>
</td></tr>
<tr><td><code id="loo_subsample_+3A_save_psis">save_psis</code></td>
<td>
<p>Should the <code>"psis"</code> object created internally by
<code>loo_subsample()</code> be saved in the returned object? See <code><a href="#topic+loo">loo()</a></code> for details.</p>
</td></tr>
<tr><td><code id="loo_subsample_+3A_cores">cores</code></td>
<td>
<p>The number of cores to use for parallelization. This defaults to
the option <code>mc.cores</code> which can be set for an entire R session by
<code>options(mc.cores = NUMBER)</code>. The old option <code>loo.cores</code> is now
deprecated but will be given precedence over <code>mc.cores</code> until
<code>loo.cores</code> is removed in a future release. <strong>As of version
2.0.0 the default is now 1 core if <code>mc.cores</code> is not set</strong>, but we
recommend using as many (or close to as many) cores as possible.
</p>

<ul>
<li><p> Note for Windows 10 users: it is <strong>strongly</strong>
<a href="https://github.com/stan-dev/loo/issues/94">recommended</a> to avoid using
the <code>.Rprofile</code> file to set <code>mc.cores</code> (using the <code>cores</code> argument or
setting <code>mc.cores</code> interactively or in a script is fine).
</p>
</li></ul>
</td></tr>
<tr><td><code id="loo_subsample_+3A_loo_approximation">loo_approximation</code></td>
<td>
<p>What type of approximation of the loo_i's should be used?
The default is <code>"plpd"</code> (the log predictive density using the posterior expectation).
There are six different methods implemented to approximate loo_i's
(see the references for more details):
</p>

<ul>
<li> <p><code>"plpd"</code>: uses the lpd based on point estimates (i.e., <code class="reqn">p(y_i|\hat{\theta})</code>).
</p>
</li>
<li> <p><code>"lpd"</code>: uses the lpds (i,e., <code class="reqn">p(y_i|y)</code>).
</p>
</li>
<li> <p><code>"tis"</code>: uses truncated importance sampling to approximate PSIS-LOO.
</p>
</li>
<li> <p><code>"waic"</code>: uses waic (i.e., <code class="reqn">p(y_i|y) - p_{waic}</code>).
</p>
</li>
<li> <p><code>"waic_grad_marginal"</code>: uses waic approximation using first order delta
method and posterior marginal variances to approximate <code class="reqn">p_{waic}</code> (ie.
<code class="reqn">p(y_i|\hat{\theta})</code>-p_waic_grad_marginal). Requires gradient of
likelihood function.
</p>
</li>
<li> <p><code>"waic_grad"</code>: uses waic approximation using first order delta method and
posterior covariance to approximate <code class="reqn">p_{waic}</code> (ie.
<code class="reqn">p(y_i|\hat{\theta})</code>-p_waic_grad). Requires gradient of likelihood
function.
</p>
</li>
<li> <p><code>"waic_hess"</code>: uses waic approximation using second order delta method and
posterior covariance to approximate <code class="reqn">p_{waic}</code> (ie.
<code class="reqn">p(y_i|\hat{\theta})</code>-p_waic_grad). Requires gradient and Hessian of
likelihood function.
</p>
</li></ul>

<p>As point estimates of <code class="reqn">\hat{\theta}</code>, the posterior expectations
of the parameters are used.</p>
</td></tr>
<tr><td><code id="loo_subsample_+3A_loo_approximation_draws">loo_approximation_draws</code></td>
<td>
<p>The number of posterior draws used when
integrating over the posterior. This is used if <code>loo_approximation</code> is set
to <code>"lpd"</code>, <code>"waic"</code>, or <code>"tis"</code>.</p>
</td></tr>
<tr><td><code id="loo_subsample_+3A_estimator">estimator</code></td>
<td>
<p>How should <code>elpd_loo</code>, <code>p_loo</code> and <code>looic</code> be estimated?
The default is <code>"diff_srs"</code>.
</p>

<ul>
<li> <p><code>"diff_srs"</code>: uses the difference estimator with simple random sampling
without replacement (srs). <code>p_loo</code> is estimated using standard srs.
(Magnusson et al., 2020)
</p>
</li>
<li> <p><code>"hh"</code>: uses the Hansen-Hurwitz estimator with sampling with replacement
proportional to size, where <code>abs</code> of loo_approximation is used as size.
(Magnusson et al., 2019)
</p>
</li>
<li> <p><code>"srs"</code>: uses simple random sampling and ordinary estimation.
</p>
</li></ul>
</td></tr>
<tr><td><code id="loo_subsample_+3A_llgrad">llgrad</code></td>
<td>
<p>The gradient of the log-likelihood. This
is only used when <code>loo_approximation</code> is <code>"waic_grad"</code>,
<code>"waic_grad_marginal"</code>, or <code>"waic_hess"</code>. The default is <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="loo_subsample_+3A_llhess">llhess</code></td>
<td>
<p>The Hessian of the log-likelihood. This is only used
with <code>loo_approximation = "waic_hess"</code>. The default is <code>NULL</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>loo_subsample()</code> function is an S3 generic and a methods is
currently provided for log-likelihood functions. The implementation works
for both MCMC and for posterior approximations where it is possible to
compute the log density for the approximation.
</p>


<h3>Value</h3>

<p><code>loo_subsample()</code> returns a named list with class <code>c("psis_loo_ss", "psis_loo", "loo")</code>. This has the same structure as objects returned by
<code><a href="#topic+loo">loo()</a></code> but with the additional slot:
</p>

<ul>
<li> <p><code>loo_subsampling</code>: A list with two vectors, <code>log_p</code> and <code>log_g</code>, of the
same length containing the posterior density and the approximation density
for the individual draws.
</p>
</li></ul>



<h3>Methods (by class)</h3>


<ul>
<li> <p><code>loo_subsample(`function`)</code>: A function <code>f()</code> that takes arguments <code>data_i</code> and <code>draws</code> and returns a
vector containing the log-likelihood for a single observation <code>i</code> evaluated
at each posterior draw. The function should be written such that, for each
observation <code>i</code> in <code>1:N</code>, evaluating
</p>
<div class="sourceCode"><pre>f(data_i = data[i,, drop=FALSE], draws = draws)
</pre></div>
<p>results in a vector of length <code>S</code> (size of posterior sample). The
log-likelihood function can also have additional arguments but <code>data_i</code> and
<code>draws</code> are required.
</p>
<p>If using the function method then the arguments <code>data</code> and <code>draws</code> must also
be specified in the call to <code>loo()</code>:
</p>

<ul>
<li> <p><code>data</code>: A data frame or matrix containing the data (e.g.
observed outcome and predictors) needed to compute the pointwise
log-likelihood. For each observation <code>i</code>, the <code>i</code>th row of
<code>data</code> will be passed to the <code>data_i</code> argument of the
log-likelihood function.
</p>
</li>
<li> <p><code>draws</code>: An object containing the posterior draws for any
parameters needed to compute the pointwise log-likelihood. Unlike
<code>data</code>, which is indexed by observation, for each observation the
entire object <code>draws</code> will be passed to the <code>draws</code> argument of
the log-likelihood function.
</p>
</li>
<li><p> The <code>...</code> can be used if your log-likelihood function takes additional
arguments. These arguments are used like the <code>draws</code> argument in that they
are recycled for each observation.
</p>
</li></ul>

</li></ul>


<h3>References</h3>

<p>Magnusson, M., Riis Andersen, M., Jonasson, J. and Vehtari, A. (2019).
Leave-One-Out Cross-Validation for Large Data.
In <em>Thirty-sixth International Conference on Machine Learning</em>,
PMLR 97:4244-4253.
</p>
<p>Magnusson, M., Riis Andersen, M., Jonasson, J. and Vehtari, A. (2020).
Leave-One-Out Cross-Validation for Model Comparison in Large Data.
In <em>Proceedings of the 23rd International Conference on Artificial
Intelligence and Statistics (AISTATS)</em>, PMLR 108:341-351.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+loo">loo()</a></code>, <code><a href="#topic+psis">psis()</a></code>, <code><a href="#topic+loo_compare">loo_compare()</a></code>
</p>

<hr>
<h2 id='loo-datasets'>Datasets for loo examples and vignettes</h2><span id='topic+loo-datasets'></span><span id='topic+Kline'></span><span id='topic+milk'></span><span id='topic+voice'></span><span id='topic+voice_loo'></span>

<h3>Description</h3>

<p>Small datasets for use in <strong>loo</strong> examples and vignettes. The <code>Kline</code>
and <code>milk</code> datasets are also included in the <strong>rethinking</strong> package
(McElreath, 2016a), but we include them here as <strong>rethinking</strong> is not
on CRAN.
</p>


<h3>Details</h3>

<p>Currently the data sets included are:
</p>

<ul>
<li> <p><code>Kline</code>:
Small dataset from Kline and Boyd (2010) on tool complexity and demography
in Oceanic islands societies. This data is discussed in detail in
McElreath (2016a,2016b). <a href="https://www.rdocumentation.org/packages/rethinking/versions/1.59/topics/Kline">(Link to variable descriptions)</a>
</p>
</li>
<li> <p><code>milk</code>:
Small dataset from Hinde and Milligan (2011) on primate milk
composition.This data is discussed in detail in McElreath (2016a,2016b).
<a href="https://www.rdocumentation.org/packages/rethinking/versions/1.59/topics/milk">(Link to variable descriptions)</a>
</p>
</li>
<li> <p><code>voice</code>:
Voice rehabilitation data from Tsanas et al. (2014).
</p>
</li></ul>



<h3>References</h3>

<p>Hinde and Milligan. 2011. <em>Evolutionary Anthropology</em> 20:9-23.
</p>
<p>Kline, M.A. and R. Boyd. 2010. <em>Proc R Soc B</em> 277:2559-2564.
</p>
<p>McElreath, R. (2016a). rethinking: Statistical Rethinking book package.
R package version 1.59.
</p>
<p>McElreath, R. (2016b). <em>Statistical rethinking: A Bayesian course with
examples in R and Stan</em>. Chapman &amp; Hall/CRC.
</p>
<p>A. Tsanas, M.A. Little, C. Fox, L.O. Ramig: Objective automatic assessment of
rehabilitative speech treatment in Parkinson's disease, IEEE
Transactions on Neural Systems and Rehabilitation Engineering, Vol. 22, pp.
181-190, January 2014
</p>


<h3>Examples</h3>

<pre><code class='language-R'>str(Kline)
str(milk)

</code></pre>

<hr>
<h2 id='loo-glossary'>LOO package glossary</h2><span id='topic+loo-glossary'></span>

<h3>Description</h3>

<p>The pages provides definitions to key terms. Also see the
<a href="https://mc-stan.org/loo/articles/online-only/faq.html">FAQ page</a> on
the <strong>loo</strong> website for answers to frequently asked questions.
</p>
<p>Note: VGG2017 refers to Vehtari, Gelman, and Gabry (2017). See
<strong>References</strong>, below.
</p>


<h3>ELPD and <code>elpd_loo</code></h3>

<p>The ELPD is the theoretical expected log pointwise predictive density for a new
dataset (Eq 1 in VGG2017), which can be estimated, e.g., using
cross-validation. <code>elpd_loo</code> is the Bayesian LOO estimate of the
expected log pointwise predictive density (Eq 4 in VGG2017) and
is a sum of N individual pointwise log predictive densities. Probability
densities can be smaller or larger than 1, and thus log predictive densities
can be negative or positive. For simplicity the ELPD acronym is used also for
expected log pointwise predictive probabilities for discrete models.
Probabilities are always equal or less than 1, and thus log predictive
probabilities are 0 or negative.
</p>


<h3>Standard error of <code>elpd_loo</code></h3>

<p>As <code>elpd_loo</code> is defined as the sum of N independent components (Eq 4 in
VGG2017), we can compute the standard error by using the standard deviation
of the N components and multiplying by <code>sqrt(N)</code> (Eq 23 in VGG2017).
This standard error is a coarse description of our uncertainty about the
predictive performance for unknown future data. When N is small or there is
severe model misspecification, the current SE estimate is overoptimistic and
the actual SE can even be twice as large. Even for moderate N, when the SE
estimate is an accurate estimate for the scale, it ignores the skewness. When
making model comparisons, the SE of the component-wise (pairwise) differences
should be used instead (see the <code>se_diff</code> section below and Eq 24 in
VGG2017). Sivula et al. (2022) discuss the conditions when the normal
approximation used for SE and <code>se_diff</code> is good.
</p>


<h3>Monte Carlo SE of elpd_loo</h3>

<p>The Monte Carlo standard error is the estimate for the computational accuracy
of MCMC and importance sampling used to compute <code>elpd_loo</code>. Usually this
is negligible compared to the standard describing the uncertainty due to
finite number of observations (Eq 23 in VGG2017).
</p>


<h3><code>p_loo</code> (effective number of parameters)</h3>

<p><code>p_loo</code> is the difference between <code>elpd_loo</code> and the non-cross-validated
log posterior predictive density. It describes how much more difficult it
is to predict future data than the observed data. Asymptotically under
certain regularity conditions, <code>p_loo</code> can be interpreted as the
<em>effective number of parameters</em>. In well behaving cases <code>p_loo &lt; N</code> and
<code>p_loo &lt; p</code>, where <code>p</code> is the total number of parameters in the
model. <code>p_loo &gt; N</code>  or <code>p_loo &gt; p</code> indicates that the model has very
weak predictive capability and may indicate a severe model misspecification.
See below for more on interpreting <code>p_loo</code> when there are warnings
about high Pareto k diagnostic values.
</p>


<h3>Pareto k estimates</h3>

<p>The Pareto <code class="reqn">k</code> estimate is a diagnostic for Pareto smoothed importance
sampling (PSIS), which is used to compute components of <code>elpd_loo</code>. In
importance-sampling LOO the full posterior distribution is used as the
proposal distribution. The Pareto k diagnostic estimates how far an
individual leave-one-out distribution is from the full distribution. If
leaving out an observation changes the posterior too much then importance
sampling is not able to give a reliable estimate. Pareto smoothing stabilizes
importance sampling and guarantees a finite variance estimate at the
cost of some bias.
</p>
<p>The diagnostic threshold for Pareto <code class="reqn">k</code> depends on sample size
<code class="reqn">S</code> (sample size dependent threshold was introduced by Vehtari
et al., 2024, and before that fixed thresholds of 0.5 and 0.7 were
recommended). For simplicity, <code>loo</code> package uses the nominal sample
size <code class="reqn">S</code>  when computing the sample size specific
threshold. This provides an optimistic threshold if the effective
sample size is less than 2200, but even then if ESS/S &gt; 1/2 the difference
is usually negligible. Thinning of MCMC draws can be used to improve
the ratio ESS/S.
</p>

<ul>
<li><p> If <code class="reqn">k &lt; \min(1 - 1 / \log_{10}(S), 0.7)</code>, where <code class="reqn">S</code> is the
sample size, the PSIS estimate and the corresponding Monte
Carlo standard error estimate are reliable.
</p>
</li>
<li><p> If <code class="reqn">1 - 1 / \log_{10}(S) &lt;= k &lt; 0.7</code>, the PSIS estimate and the
corresponding Monte Carlo standard error estimate are not
reliable, but increasing the (effective) sample size <code class="reqn">S</code> above
2200 may help (this will increase the sample size specific
threshold <code class="reqn">(1 - 1 / \log_{10}(2200) &gt; 0.7</code> and then the bias specific
threshold 0.7 dominates).
</p>
</li>
<li><p> If <code class="reqn">0.7 &lt;= k &lt; 1</code>, the PSIS estimate and the corresponding Monte
Carlo standard error have large bias and are not reliable. Increasing
the sample size may reduce the variability in the <code class="reqn">k</code> estimate, which
may also result in a lower <code class="reqn">k</code> estimate.
</p>
</li>
<li><p> If <code class="reqn">k \geq 1</code>, the target distribution is estimated to
have non-finite mean. The PSIS estimate and the corresponding Monte
Carlo standard error are not well defined. Increasing the sample size
may reduce the variability in <code class="reqn">k</code> estimate, which may also result in
a lower <code class="reqn">k</code> estimate.
</p>
</li></ul>

<p>Pareto <code class="reqn">k</code> is also useful as a measure of influence of an
observation.  Highly influential observations have high <code class="reqn">k</code>
values. Very high <code class="reqn">k</code> values often indicate model
misspecification, outliers or mistakes in data processing. See
Section 6 of Gabry et al. (2019) for an example.
</p>


<h4>Interpreting <code>p_loo</code> when Pareto <code>k</code> is large</h4>

<p>If <code class="reqn">k &gt; 0.7</code> then we can also look at
the <code>p_loo</code> estimate for some additional information about the problem:
</p>

<ul>
<li><p> If <code style="white-space: pre;">&#8288;p_loo &lt;&lt; p&#8288;</code> (the total number of parameters in the model),
then the model is likely to be misspecified. Posterior predictive checks
(PPCs) are then likely to also detect the problem. Try using an overdispersed
model, or add more structural information (nonlinearity, mixture model,
etc.).
</p>
</li>
<li><p> If <code>p_loo &lt; p</code> and the number of parameters <code>p</code> is relatively
large compared to the number of observations (e.g., <code>p&gt;N/5</code>), it is
likely that the model is so flexible or the population prior so weak that it’s
difficult to predict the left out observation (even for the true model).
This happens, for example, in the simulated 8 schools (in VGG2017), random
effect models with a few observations per random effect, and Gaussian
processes and spatial models with short correlation lengths.
</p>
</li>
<li><p> If <code>p_loo &gt; p</code>, then the model is likely to be badly misspecified.
If the number of parameters <code style="white-space: pre;">&#8288;p&lt;&lt;N&#8288;</code>, then PPCs are also likely to detect the
problem. See the case study at
<a href="https://avehtari.github.io/modelselection/roaches.html">https://avehtari.github.io/modelselection/roaches.html</a> for an example.
If <code>p</code> is relatively large compared to the number of
observations, say <code>p&gt;N/5</code> (more accurately we should count number of
observations influencing each parameter as in hierarchical models some groups
may have few observations and other groups many), it is possible that PPCs won't
detect the problem.
</p>
</li></ul>




<h3>elpd_diff</h3>

<p><code>elpd_diff</code> is the difference in <code>elpd_loo</code> for two models. If more
than two models are compared, the difference is computed relative to the
model with highest <code>elpd_loo</code>.
</p>


<h3>se_diff</h3>

<p>The standard error of component-wise differences of elpd_loo (Eq 24 in
VGG2017) between two models. This SE is <em>smaller</em> than the SE for
individual models due to correlation (i.e., if some observations are easier
and some more difficult to predict for all models).
</p>


<h3>References</h3>

<p>Vehtari, A., Gelman, A., and Gabry, J. (2017). Practical Bayesian model
evaluation using leave-one-out cross-validation and WAIC.
<em>Statistics and Computing</em>. 27(5), 1413&ndash;1432. doi:10.1007/s11222-016-9696-4
(<a href="https://link.springer.com/article/10.1007/s11222-016-9696-4">journal version</a>,
<a href="https://arxiv.org/abs/1507.04544">preprint arXiv:1507.04544</a>).
</p>
<p>Vehtari, A., Simpson, D., Gelman, A., Yao, Y., and Gabry, J. (2024).
Pareto smoothed importance sampling. <em>Journal of Machine Learning Research</em>,
25(72):1-58.
<a href="https://jmlr.org/papers/v25/19-556.html">PDF</a>
</p>
<p>Sivula, T, Magnusson, M., Matamoros A. A., and Vehtari,
A. (2022).  Uncertainty in Bayesian leave-one-out
cross-validation based model comparison. <a href="https://arxiv.org/abs/2008.10296v3">preprint arXiv:2008.10296v3.</a>.
</p>
<p>Gabry, J. , Simpson, D. , Vehtari, A. , Betancourt, M. and
Gelman, A. (2019), Visualization in Bayesian workflow.
<em>J. R. Stat. Soc. A</em>, 182: 389-402. doi:10.1111/rssa.12378
(<a href="https://rss.onlinelibrary.wiley.com/doi/full/10.1111/rssa.12378">journal version</a>,
<a href="https://arxiv.org/abs/1709.01449">preprint arXiv:1709.01449</a>,
<a href="https://github.com/jgabry/bayes-vis-paper">code on GitHub</a>)
</p>

<hr>
<h2 id='nlist'>Named lists</h2><span id='topic+nlist'></span>

<h3>Description</h3>

<p>Create a named list using specified names or, if names are omitted, using the
names of the objects in the list. The code <code>list(a = a, b = b)</code> becomes
<code>nlist(a,b)</code> and <code>list(a = a, b = 2)</code> becomes <code>nlist(a, b = 2)</code>, etc.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nlist(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nlist_+3A_...">...</code></td>
<td>
<p>Objects to include in the list.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A named list.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# All variables already defined
a &lt;- rnorm(100)
b &lt;- mat.or.vec(10, 3)
nlist(a,b)

# Define some variables in the call and take the rest from the environment
nlist(a, b, veggies = c("lettuce", "spinach"), fruits = c("banana", "papaya"))

</code></pre>

<hr>
<h2 id='nobs.psis_loo_ss'>The number of observations in a <code>psis_loo_ss</code> object.</h2><span id='topic+nobs.psis_loo_ss'></span>

<h3>Description</h3>

<p>The number of observations in a <code>psis_loo_ss</code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'psis_loo_ss'
nobs(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nobs.psis_loo_ss_+3A_object">object</code></td>
<td>
<p>a <code>psis_loo_ss</code> object.</p>
</td></tr>
<tr><td><code id="nobs.psis_loo_ss_+3A_...">...</code></td>
<td>
<p>Currently unused.</p>
</td></tr>
</table>

<hr>
<h2 id='obs_idx'>Get observation indices used in subsampling</h2><span id='topic+obs_idx'></span>

<h3>Description</h3>

<p>Get observation indices used in subsampling
</p>


<h3>Usage</h3>

<pre><code class='language-R'>obs_idx(x, rep = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="obs_idx_+3A_x">x</code></td>
<td>
<p>A <code>psis_loo_ss</code> object.</p>
</td></tr>
<tr><td><code id="obs_idx_+3A_rep">rep</code></td>
<td>
<p>If sampling with replacement is used, an observation can have
multiple samples and these are then repeated in the returned object if
<code>rep=TRUE</code> (e.g., a vector <code>c(1,1,2)</code> indicates that observation 1 has been
subampled two times). If <code>rep=FALSE</code> only the unique indices are returned.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An integer vector.
</p>

<hr>
<h2 id='old-extractors'>Extractor methods</h2><span id='topic+old-extractors'></span><span id='topic++5B.loo'></span><span id='topic++5B+5B.loo'></span><span id='topic++24.loo'></span>

<h3>Description</h3>

<p>These are only defined in order to deprecate with a warning (rather than
remove and break backwards compatibility) the old way of accessing the point
estimates in a <code>"psis_loo"</code> or <code>"psis"</code> object. The new way as of
v2.0.0 is to get them from the <code>"estimates"</code> component of the object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'loo'
x[i]

## S3 method for class 'loo'
x[[i, exact = TRUE]]

## S3 method for class 'loo'
x$name
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="old-extractors_+3A_x">x</code>, <code id="old-extractors_+3A_i">i</code>, <code id="old-extractors_+3A_exact">exact</code>, <code id="old-extractors_+3A_name">name</code></td>
<td>
<p>See <a href="base.html#topic+Extract">Extract</a>.</p>
</td></tr>
</table>

<hr>
<h2 id='parallel_psis_list'>Parallel psis list computations</h2><span id='topic+parallel_psis_list'></span><span id='topic+parallel_importance_sampling_list'></span>

<h3>Description</h3>

<p>Parallel psis list computations
</p>


<h3>Usage</h3>

<pre><code class='language-R'>parallel_psis_list(
  N,
  .loo_i,
  .llfun,
  data,
  draws,
  r_eff,
  save_psis,
  cores,
  ...
)

parallel_importance_sampling_list(
  N,
  .loo_i,
  .llfun,
  data,
  draws,
  r_eff,
  save_psis,
  cores,
  method,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="parallel_psis_list_+3A_n">N</code></td>
<td>
<p>The total number of observations (i.e. <code>nrow(data)</code>).</p>
</td></tr>
<tr><td><code id="parallel_psis_list_+3A_.loo_i">.loo_i</code></td>
<td>
<p>The function used to compute individual loo contributions.</p>
</td></tr>
<tr><td><code id="parallel_psis_list_+3A_.llfun">.llfun</code></td>
<td>
<p>See <code>llfun</code> in <code><a href="#topic+loo.function">loo.function()</a></code>.</p>
</td></tr>
<tr><td><code id="parallel_psis_list_+3A_data">data</code>, <code id="parallel_psis_list_+3A_draws">draws</code>, <code id="parallel_psis_list_+3A_...">...</code></td>
<td>
<p>For the <code>loo.function()</code> method and the <code>loo_i()</code>
function, these are the data, posterior draws, and other arguments to pass
to the log-likelihood function. See the <strong>Methods (by class)</strong> section
below for details on how to specify these arguments.</p>
</td></tr>
<tr><td><code id="parallel_psis_list_+3A_r_eff">r_eff</code></td>
<td>
<p>Vector of relative effective sample size estimates for the
likelihood (<code>exp(log_lik)</code>) of each observation. This is related to
the relative efficiency of estimating the normalizing term in
self-normalized importance sampling when using posterior draws obtained
with MCMC. If MCMC draws are used and <code>r_eff</code> is not provided then
the reported PSIS effective sample sizes and Monte Carlo error estimates
can be over-optimistic. If the posterior draws are (near) independent then
<code>r_eff=1</code> can be used. <code>r_eff</code> has to be a scalar (same value is used
for all observations) or a vector with length equal to the number of
observations. The default value is 1. See the <code><a href="#topic+relative_eff">relative_eff()</a></code> helper
functions for help computing <code>r_eff</code>.</p>
</td></tr>
<tr><td><code id="parallel_psis_list_+3A_save_psis">save_psis</code></td>
<td>
<p>Should the <code>psis</code> object created internally by <code>loo()</code> be
saved in the returned object? The <code>loo()</code> function calls <code><a href="#topic+psis">psis()</a></code>
internally but by default discards the (potentially large) <code>psis</code> object
after using it to compute the LOO-CV summaries. Setting <code>save_psis=TRUE</code>
will add a <code>psis_object</code> component to the list returned by <code>loo</code>.
This is useful if you plan to use the <code><a href="#topic+E_loo">E_loo()</a></code> function to compute
weighted expectations after running <code>loo</code>. Several functions in the
<span class="pkg">bayesplot</span> package also accept <code>psis</code> objects.</p>
</td></tr>
<tr><td><code id="parallel_psis_list_+3A_cores">cores</code></td>
<td>
<p>The number of cores to use for parallelization. This defaults to
the option <code>mc.cores</code> which can be set for an entire R session by
<code>options(mc.cores = NUMBER)</code>. The old option <code>loo.cores</code> is now
deprecated but will be given precedence over <code>mc.cores</code> until
<code>loo.cores</code> is removed in a future release. <strong>As of version
2.0.0 the default is now 1 core if <code>mc.cores</code> is not set</strong>, but we
recommend using as many (or close to as many) cores as possible.
</p>

<ul>
<li><p> Note for Windows 10 users: it is <strong>strongly</strong>
<a href="https://github.com/stan-dev/loo/issues/94">recommended</a> to avoid using
the <code>.Rprofile</code> file to set <code>mc.cores</code> (using the <code>cores</code> argument or
setting <code>mc.cores</code> interactively or in a script is fine).
</p>
</li></ul>
</td></tr>
<tr><td><code id="parallel_psis_list_+3A_method">method</code></td>
<td>
<p>See <code>is_method</code> for <code><a href="#topic+loo">loo()</a></code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Refactored function to handle parallel computations
for psis_list
</p>

<hr>
<h2 id='pareto-k-diagnostic'>Diagnostics for Pareto smoothed importance sampling (PSIS)</h2><span id='topic+pareto-k-diagnostic'></span><span id='topic+pareto_k_table'></span><span id='topic+pareto_k_ids'></span><span id='topic+pareto_k_values'></span><span id='topic+pareto_k_influence_values'></span><span id='topic+psis_n_eff_values'></span><span id='topic+mcse_loo'></span><span id='topic+plot.psis_loo'></span><span id='topic+plot.loo'></span><span id='topic+plot.psis'></span>

<h3>Description</h3>

<p>Print a diagnostic table summarizing the estimated Pareto shape parameters
and PSIS effective sample sizes, find the indexes of observations for which
the estimated Pareto shape parameter <code class="reqn">k</code> is larger than some
<code>threshold</code> value, or plot observation indexes vs. diagnostic estimates.
The <strong>Details</strong> section below provides a brief overview of the
diagnostics, but we recommend consulting Vehtari, Gelman, and Gabry (2017)
and Vehtari, Simpson, Gelman, Yao, and Gabry (2024) for full details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pareto_k_table(x)

pareto_k_ids(x, threshold = NULL)

pareto_k_values(x)

pareto_k_influence_values(x)

psis_n_eff_values(x)

mcse_loo(x, threshold = NULL)

## S3 method for class 'psis_loo'
plot(
  x,
  diagnostic = c("k", "ESS", "n_eff"),
  ...,
  label_points = FALSE,
  main = "PSIS diagnostic plot"
)

## S3 method for class 'psis'
plot(
  x,
  diagnostic = c("k", "ESS", "n_eff"),
  ...,
  label_points = FALSE,
  main = "PSIS diagnostic plot"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pareto-k-diagnostic_+3A_x">x</code></td>
<td>
<p>An object created by <code><a href="#topic+loo">loo()</a></code> or <code><a href="#topic+psis">psis()</a></code>.</p>
</td></tr>
<tr><td><code id="pareto-k-diagnostic_+3A_threshold">threshold</code></td>
<td>
<p>For <code>pareto_k_ids()</code>, <code>threshold</code> is the minimum <code class="reqn">k</code>
value to flag (default is a sample size <code>S</code> dependend threshold
<code>1 - 1 / log10(S)</code>). For <code>mcse_loo()</code>, if any <code class="reqn">k</code> estimates are
greater than <code>threshold</code> the MCSE estimate is returned as <code>NA</code>
See <strong>Details</strong> for the motivation behind these defaults.</p>
</td></tr>
<tr><td><code id="pareto-k-diagnostic_+3A_diagnostic">diagnostic</code></td>
<td>
<p>For the <code>plot</code> method, which diagnostic should be
plotted? The options are <code>"k"</code> for Pareto <code class="reqn">k</code> estimates (the
default), or <code>"ESS"</code> or <code>"n_eff"</code> for PSIS effective sample size estimates.</p>
</td></tr>
<tr><td><code id="pareto-k-diagnostic_+3A_label_points">label_points</code>, <code id="pareto-k-diagnostic_+3A_...">...</code></td>
<td>
<p>For the <code>plot()</code> method, if <code>label_points</code> is
<code>TRUE</code> the observation numbers corresponding to any values of <code class="reqn">k</code>
greater than the diagnostic threshold will be displayed in the plot.
Any arguments specified in <code>...</code> will be passed to <code><a href="graphics.html#topic+text">graphics::text()</a></code>
and can be used to control the appearance of the labels.</p>
</td></tr>
<tr><td><code id="pareto-k-diagnostic_+3A_main">main</code></td>
<td>
<p>For the <code>plot()</code> method, a title for the plot.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The reliability and approximate convergence rate of the PSIS-based
estimates can be assessed using the estimates for the shape
parameter <code class="reqn">k</code> of the generalized Pareto distribution. The
diagnostic threshold for Pareto <code class="reqn">k</code> depends on sample size
<code class="reqn">S</code> (sample size dependent threshold was introduced by Vehtari
et al. (2024), and before that fixed thresholds of 0.5 and 0.7 were
recommended). For simplicity, <code>loo</code> package uses the nominal sample
size <code class="reqn">S</code> when computing the sample size specific
threshold. This provides an optimistic threshold if the effective
sample size is less than 2200, but if MCMC-ESS &gt; S/2 the difference
is usually negligible. Thinning of MCMC draws can be used to
improve the ratio ESS/S.
</p>

<ul>
<li><p> If <code class="reqn">k &lt; min(1 - 1 / log10(S), 0.7)</code>, where <code class="reqn">S</code> is the
sample size, the PSIS estimate and the corresponding Monte Carlo
standard error estimate are reliable.
</p>
</li>
<li><p> If <code class="reqn">1 - 1 / log10(S) &lt;= k &lt; 0.7</code>, the PSIS estimate and the
corresponding Monte Carlo standard error estimate are not
reliable, but increasing the (effective) sample size <code class="reqn">S</code> above
2200 may help (this will increase the sample size specific
threshold <code class="reqn">(1-1/log10(2200)&gt;0.7</code> and then the bias specific
threshold 0.7 dominates).
</p>
</li>
<li><p> If <code class="reqn">0.7 &lt;= k &lt; 1</code>, the PSIS estimate and the corresponding Monte
Carlo standard error have large bias and are not reliable. Increasing
the sample size may reduce the variability in <code class="reqn">k</code> estimate, which
may result in lower <code class="reqn">k</code> estimate, too.
</p>
</li>
<li><p> If <code class="reqn">k \geq 1</code>, the target distribution is estimated to
have a non-finite mean. The PSIS estimate and the corresponding Monte
Carlo standard error are not well defined. Increasing the sample size
may reduce the variability in the <code class="reqn">k</code> estimate, which
may also result in a lower <code class="reqn">k</code> estimate.
</p>
</li></ul>



<h4>What if the estimated tail shape parameter <code class="reqn">k</code>
exceeds the diagnostic threshold?</h4>

<p> Importance sampling is likely to
work less well if the marginal posterior <code class="reqn">p(\theta^s | y)</code> and
LOO posterior <code class="reqn">p(\theta^s | y_{-i})</code> are very different, which
is more likely to happen with a non-robust model and highly
influential observations.  If the estimated tail shape parameter
<code class="reqn">k</code> exceeds the diagnostic threshold, the user should be
warned. (Note: If <code class="reqn">k</code> is greater than the diagnostic threshold
then WAIC is also likely to fail, but WAIC lacks as accurate
diagnostic.)  When using PSIS in the context of approximate LOO-CV,
we recommend one of the following actions:
</p>

<ul>
<li><p> With some additional computations, it is possible to transform
the MCMC draws from the posterior distribution to obtain more
reliable importance sampling estimates. This results in a smaller
shape parameter <code class="reqn">k</code>.  See <code><a href="#topic+loo_moment_match">loo_moment_match()</a></code> and the
vignette <em>Avoiding model refits in leave-one-out cross-validation
with moment matching</em> for an example of this.
</p>
</li>
<li><p> Sampling from a leave-one-out mixture distribution (see the
vignette <em>Mixture IS leave-one-out cross-validation for
high-dimensional Bayesian models</em>), directly from <code class="reqn">p(\theta^s
  | y_{-i})</code> for the problematic observations <code class="reqn">i</code>, or using
<code class="reqn">K</code>-fold cross-validation (see the vignette <em>Holdout
validation and K-fold cross-validation of Stan programs with the
loo package</em>) will generally be more stable.
</p>
</li>
<li><p> Using a model that is more robust to anomalous observations will
generally make approximate LOO-CV more stable.
</p>
</li></ul>




<h4>Observation influence statistics</h4>

<p> The estimated shape parameter
<code class="reqn">k</code> for each observation can be used as a measure of the observation's
influence on posterior distribution of the model. These can be obtained with
<code>pareto_k_influence_values()</code>.
</p>



<h4>Effective sample size and error estimates</h4>

<p> In the case that we
obtain the samples from the proposal distribution via MCMC the <strong>loo</strong>
package also computes estimates for the Monte Carlo error and the effective
sample size for importance sampling, which are more accurate for PSIS than
for IS and TIS (see Vehtari et al (2024) for details). However, the PSIS
effective sample size estimate will be
<strong>over-optimistic when the estimate of <code class="reqn">k</code> is greater than</strong>
<code class="reqn">min(1-1/log10(S), 0.7)</code>, where <code class="reqn">S</code> is the sample size.
</p>



<h3>Value</h3>

<p><code>pareto_k_table()</code> returns an object of class
<code>"pareto_k_table"</code>, which is a matrix with columns <code>"Count"</code>,
<code>"Proportion"</code>, and <code>"Min. n_eff"</code>, and has its own print method.
</p>
<p><code>pareto_k_ids()</code> returns an integer vector indicating which
observations have Pareto <code class="reqn">k</code> estimates above <code>threshold</code>.
</p>
<p><code>pareto_k_values()</code> returns a vector of the estimated Pareto
<code class="reqn">k</code> parameters. These represent the reliability of sampling.
</p>
<p><code>pareto_k_influence_values()</code> returns a vector of the estimated Pareto
<code class="reqn">k</code> parameters. These represent influence of the observations on the
model posterior distribution.
</p>
<p><code>psis_n_eff_values()</code> returns a vector of the estimated PSIS
effective sample sizes.
</p>
<p><code>mcse_loo()</code> returns the Monte Carlo standard error (MCSE)
estimate for PSIS-LOO. MCSE will be NA if any Pareto <code class="reqn">k</code> values are
above <code>threshold</code>.
</p>
<p>The <code>plot()</code> method is called for its side effect and does not
return anything. If <code>x</code> is the result of a call to <code><a href="#topic+loo">loo()</a></code>
or <code><a href="#topic+psis">psis()</a></code> then <code>plot(x, diagnostic)</code> produces a plot of
the estimates of the Pareto shape parameters (<code>diagnostic = "k"</code>) or
estimates of the PSIS effective sample sizes (<code>diagnostic = "ESS"</code>).
</p>


<h3>References</h3>

<p>Vehtari, A., Gelman, A., and Gabry, J. (2017). Practical Bayesian model
evaluation using leave-one-out cross-validation and WAIC.
<em>Statistics and Computing</em>. 27(5), 1413&ndash;1432. doi:10.1007/s11222-016-9696-4
(<a href="https://link.springer.com/article/10.1007/s11222-016-9696-4">journal version</a>,
<a href="https://arxiv.org/abs/1507.04544">preprint arXiv:1507.04544</a>).
</p>
<p>Vehtari, A., Simpson, D., Gelman, A., Yao, Y., and Gabry, J. (2024).
Pareto smoothed importance sampling. <em>Journal of Machine Learning Research</em>,
25(72):1-58.
<a href="https://jmlr.org/papers/v25/19-556.html">PDF</a>
</p>


<h3>See Also</h3>


<ul>
<li> <p><code><a href="#topic+psis">psis()</a></code> for the implementation of the PSIS algorithm.
</p>
</li>
<li><p> The <a href="https://mc-stan.org/loo/articles/online-only/faq.html">FAQ page</a> on
the <strong>loo</strong> website for answers to frequently asked questions.
</p>
</li></ul>


<hr>
<h2 id='pointwise'>Convenience function for extracting pointwise estimates</h2><span id='topic+pointwise'></span><span id='topic+pointwise.loo'></span>

<h3>Description</h3>

<p>Convenience function for extracting pointwise estimates
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pointwise(x, estimate, ...)

## S3 method for class 'loo'
pointwise(x, estimate, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pointwise_+3A_x">x</code></td>
<td>
<p>A <code>loo</code> object, for example one returned by <code><a href="#topic+loo">loo()</a></code>,
<code><a href="#topic+loo_subsample">loo_subsample()</a></code>, <code><a href="#topic+loo_approximate_posterior">loo_approximate_posterior()</a></code>, <code><a href="#topic+loo_moment_match">loo_moment_match()</a></code>, etc.</p>
</td></tr>
<tr><td><code id="pointwise_+3A_estimate">estimate</code></td>
<td>
<p>Which pointwise estimate to return. By default all are
returned. The objects returned by the different functions (<code><a href="#topic+loo">loo()</a></code>,
<code><a href="#topic+loo_subsample">loo_subsample()</a></code>, etc.) have slightly different estimates available.
Typically at a minimum the estimates <code>elpd_loo</code>, <code>looic</code>, <code>mcse_elpd_loo</code>,
<code>p_loo</code>, and <code>influence_pareto_k</code> will be available, but there may be
others.</p>
</td></tr>
<tr><td><code id="pointwise_+3A_...">...</code></td>
<td>
<p>Currently ignored.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of length equal to the number of observations.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- loo(example_loglik_array())
pointwise(x, "elpd_loo")

</code></pre>

<hr>
<h2 id='print_dims'>Print dimensions of log-likelihood or log-weights matrix</h2><span id='topic+print_dims'></span><span id='topic+print_dims.importance_sampling'></span><span id='topic+print_dims.psis_loo'></span><span id='topic+print_dims.importance_sampling_loo'></span><span id='topic+print_dims.waic'></span><span id='topic+print_dims.kfold'></span><span id='topic+print_dims.psis_loo_ss'></span>

<h3>Description</h3>

<p>Print dimensions of log-likelihood or log-weights matrix
</p>


<h3>Usage</h3>

<pre><code class='language-R'>print_dims(x, ...)

## S3 method for class 'importance_sampling'
print_dims(x, ...)

## S3 method for class 'psis_loo'
print_dims(x, ...)

## S3 method for class 'importance_sampling_loo'
print_dims(x, ...)

## S3 method for class 'waic'
print_dims(x, ...)

## S3 method for class 'kfold'
print_dims(x, ...)

## S3 method for class 'psis_loo_ss'
print_dims(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print_dims_+3A_x">x</code></td>
<td>
<p>The object returned by <code><a href="#topic+psis">psis()</a></code>, <code><a href="#topic+loo">loo()</a></code>, or <code><a href="#topic+waic">waic()</a></code>.</p>
</td></tr>
<tr><td><code id="print_dims_+3A_...">...</code></td>
<td>
<p>Ignored.</p>
</td></tr>
</table>

<hr>
<h2 id='print.loo'>Print methods</h2><span id='topic+print.loo'></span><span id='topic+print.waic'></span><span id='topic+print.psis_loo'></span><span id='topic+print.importance_sampling_loo'></span><span id='topic+print.psis_loo_ap'></span><span id='topic+print.psis'></span><span id='topic+print.importance_sampling'></span>

<h3>Description</h3>

<p>Print methods
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'loo'
print(x, digits = 1, ...)

## S3 method for class 'waic'
print(x, digits = 1, ...)

## S3 method for class 'psis_loo'
print(x, digits = 1, plot_k = FALSE, ...)

## S3 method for class 'importance_sampling_loo'
print(x, digits = 1, plot_k = FALSE, ...)

## S3 method for class 'psis_loo_ap'
print(x, digits = 1, plot_k = FALSE, ...)

## S3 method for class 'psis'
print(x, digits = 1, plot_k = FALSE, ...)

## S3 method for class 'importance_sampling'
print(x, digits = 1, plot_k = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.loo_+3A_x">x</code></td>
<td>
<p>An object returned by <code><a href="#topic+loo">loo()</a></code>, <code><a href="#topic+psis">psis()</a></code>, or <code><a href="#topic+waic">waic()</a></code>.</p>
</td></tr>
<tr><td><code id="print.loo_+3A_digits">digits</code></td>
<td>
<p>An integer passed to <code><a href="base.html#topic+Round">base::round()</a></code>.</p>
</td></tr>
<tr><td><code id="print.loo_+3A_...">...</code></td>
<td>
<p>Arguments passed to <code><a href="#topic+plot.psis_loo">plot.psis_loo()</a></code> if <code>plot_k</code> is
<code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="print.loo_+3A_plot_k">plot_k</code></td>
<td>
<p>Logical. If <code>TRUE</code> the estimates of the Pareto shape
parameter <code class="reqn">k</code> are plotted. Ignored if <code>x</code> was generated by
<code><a href="#topic+waic">waic()</a></code>. To just plot <code class="reqn">k</code> without printing use the
<a href="#topic+pareto-k-diagnostic">plot()</a> method for 'loo' objects.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>x</code>, invisibly.
</p>


<h3>See Also</h3>

<p><a href="#topic+pareto-k-diagnostic">pareto-k-diagnostic</a>
</p>

<hr>
<h2 id='psis'>Pareto smoothed importance sampling (PSIS)</h2><span id='topic+psis'></span><span id='topic+psis.array'></span><span id='topic+psis.matrix'></span><span id='topic+psis.default'></span><span id='topic+is.psis'></span><span id='topic+is.sis'></span><span id='topic+is.tis'></span>

<h3>Description</h3>

<p>Implementation of Pareto smoothed importance sampling (PSIS), a method for
stabilizing importance ratios. The version of PSIS implemented here
corresponds to the algorithm presented in Vehtari, Simpson, Gelman, Yao,
and Gabry (2024).
For PSIS diagnostics see the <a href="#topic+pareto-k-diagnostic">pareto-k-diagnostic</a> page.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>psis(log_ratios, ...)

## S3 method for class 'array'
psis(log_ratios, ..., r_eff = 1, cores = getOption("mc.cores", 1))

## S3 method for class 'matrix'
psis(log_ratios, ..., r_eff = 1, cores = getOption("mc.cores", 1))

## Default S3 method:
psis(log_ratios, ..., r_eff = 1)

is.psis(x)

is.sis(x)

is.tis(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="psis_+3A_log_ratios">log_ratios</code></td>
<td>
<p>An array, matrix, or vector of importance ratios on the log
scale (for PSIS-LOO these are <em>negative</em> log-likelihood values). See the
<strong>Methods (by class)</strong> section below for a detailed description of how
to specify the inputs for each method.</p>
</td></tr>
<tr><td><code id="psis_+3A_...">...</code></td>
<td>
<p>Arguments passed on to the various methods.</p>
</td></tr>
<tr><td><code id="psis_+3A_r_eff">r_eff</code></td>
<td>
<p>Vector of relative effective sample size estimates containing
one element per observation. The values provided should be the relative
effective sample sizes of <code>1/exp(log_ratios)</code> (i.e., <code>1/ratios</code>).
This is related to the relative efficiency of estimating the normalizing
term in self-normalizing importance sampling. If <code>r_eff</code> is not
provided then the reported PSIS effective sample sizes and Monte Carlo
error estimates can be over-optimistic. If the posterior draws are (near)
independent then <code>r_eff=1</code> can be used. <code>r_eff</code> has to be a scalar (same
value is used for all observations) or a vector with length equal to the
number of observations. The default value is 1. See the <code><a href="#topic+relative_eff">relative_eff()</a></code>
helper function for computing <code>r_eff</code>.</p>
</td></tr>
<tr><td><code id="psis_+3A_cores">cores</code></td>
<td>
<p>The number of cores to use for parallelization. This defaults to
the option <code>mc.cores</code> which can be set for an entire R session by
<code>options(mc.cores = NUMBER)</code>. The old option <code>loo.cores</code> is now
deprecated but will be given precedence over <code>mc.cores</code> until
<code>loo.cores</code> is removed in a future release. <strong>As of version
2.0.0 the default is now 1 core if <code>mc.cores</code> is not set</strong>, but we
recommend using as many (or close to as many) cores as possible.
</p>

<ul>
<li><p> Note for Windows 10 users: it is <strong>strongly</strong>
<a href="https://github.com/stan-dev/loo/issues/94">recommended</a> to avoid using
the <code>.Rprofile</code> file to set <code>mc.cores</code> (using the <code>cores</code> argument or
setting <code>mc.cores</code> interactively or in a script is fine).
</p>
</li></ul>
</td></tr>
<tr><td><code id="psis_+3A_x">x</code></td>
<td>
<p>For <code>is.psis()</code>, an object to check.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>psis()</code> methods return an object of class <code>"psis"</code>,
which is a named list with the following components:
</p>

<dl>
<dt><code>log_weights</code></dt><dd>
<p>Vector or matrix of smoothed (and truncated) but <em>unnormalized</em> log
weights. To get normalized weights use the
<code><a href="#topic+weights.importance_sampling">weights()</a></code> method provided for objects of
class <code>"psis"</code>.
</p>
</dd>
<dt><code>diagnostics</code></dt><dd>
<p>A named list containing two vectors:
</p>

<ul>
<li> <p><code>pareto_k</code>: Estimates of the shape parameter <code class="reqn">k</code> of the
generalized Pareto distribution. See the <a href="#topic+pareto-k-diagnostic">pareto-k-diagnostic</a>
page for details.
</p>
</li>
<li> <p><code>n_eff</code>: PSIS effective sample size estimates.
</p>
</li></ul>

</dd>
</dl>

<p>Objects of class <code>"psis"</code> also have the following <a href="base.html#topic+attributes">attributes</a>:
</p>

<dl>
<dt><code>norm_const_log</code></dt><dd>
<p>Vector of precomputed values of <code>colLogSumExps(log_weights)</code> that are
used internally by the <code>weights</code> method to normalize the log weights.
</p>
</dd>
<dt><code>tail_len</code></dt><dd>
<p>Vector of tail lengths used for fitting the generalized Pareto distribution.
</p>
</dd>
<dt><code>r_eff</code></dt><dd>
<p>If specified, the user's <code>r_eff</code> argument.
</p>
</dd>
<dt><code>dims</code></dt><dd>
<p>Integer vector of length 2 containing <code>S</code> (posterior sample size)
and <code>N</code> (number of observations).
</p>
</dd>
<dt><code>method</code></dt><dd>
<p>Method used for importance sampling, here <code>psis</code>.
</p>
</dd>
</dl>



<h3>Methods (by class)</h3>


<ul>
<li> <p><code>psis(array)</code>: An <code class="reqn">I</code> by <code class="reqn">C</code> by <code class="reqn">N</code> array, where <code class="reqn">I</code>
is the number of MCMC iterations per chain, <code class="reqn">C</code> is the number of
chains, and <code class="reqn">N</code> is the number of data points.
</p>
</li>
<li> <p><code>psis(matrix)</code>: An <code class="reqn">S</code> by <code class="reqn">N</code> matrix, where <code class="reqn">S</code> is the size
of the posterior sample (with all chains merged) and <code class="reqn">N</code> is the number
of data points.
</p>
</li>
<li> <p><code>psis(default)</code>: A vector of length <code class="reqn">S</code> (posterior sample size).
</p>
</li></ul>


<h3>References</h3>

<p>Vehtari, A., Gelman, A., and Gabry, J. (2017). Practical Bayesian model
evaluation using leave-one-out cross-validation and WAIC.
<em>Statistics and Computing</em>. 27(5), 1413&ndash;1432. doi:10.1007/s11222-016-9696-4
(<a href="https://link.springer.com/article/10.1007/s11222-016-9696-4">journal version</a>,
<a href="https://arxiv.org/abs/1507.04544">preprint arXiv:1507.04544</a>).
</p>
<p>Vehtari, A., Simpson, D., Gelman, A., Yao, Y., and Gabry, J. (2024).
Pareto smoothed importance sampling. <em>Journal of Machine Learning Research</em>,
25(72):1-58.
<a href="https://jmlr.org/papers/v25/19-556.html">PDF</a>
</p>


<h3>See Also</h3>


<ul>
<li> <p><code><a href="#topic+loo">loo()</a></code> for approximate LOO-CV using PSIS.
</p>
</li>
<li> <p><a href="#topic+pareto-k-diagnostic">pareto-k-diagnostic</a> for PSIS diagnostics.
</p>
</li>
<li><p> The <strong>loo</strong> package <a href="https://mc-stan.org/loo/articles/index.html">vignettes</a>
for demonstrations.
</p>
</li>
<li><p> The <a href="https://mc-stan.org/loo/articles/online-only/faq.html">FAQ page</a> on
the <strong>loo</strong> website for answers to frequently asked questions.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>log_ratios &lt;- -1 * example_loglik_array()
r_eff &lt;- relative_eff(exp(-log_ratios))
psis_result &lt;- psis(log_ratios, r_eff = r_eff)
str(psis_result)
plot(psis_result)

# extract smoothed weights
lw &lt;- weights(psis_result) # default args are log=TRUE, normalize=TRUE
ulw &lt;- weights(psis_result, normalize=FALSE) # unnormalized log-weights

w &lt;- weights(psis_result, log=FALSE) # normalized weights (not log-weights)
uw &lt;- weights(psis_result, log=FALSE, normalize = FALSE) # unnormalized weights



</code></pre>

<hr>
<h2 id='psis_approximate_posterior'>Diagnostics for Laplace and ADVI approximations and Laplace-loo and ADVI-loo</h2><span id='topic+psis_approximate_posterior'></span>

<h3>Description</h3>

<p>Diagnostics for Laplace and ADVI approximations and Laplace-loo and ADVI-loo
</p>


<h3>Usage</h3>

<pre><code class='language-R'>psis_approximate_posterior(
  log_p = NULL,
  log_g = NULL,
  log_liks = NULL,
  cores,
  save_psis,
  ...,
  log_q = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="psis_approximate_posterior_+3A_log_p">log_p</code></td>
<td>
<p>The log-posterior (target) evaluated at S samples from the
proposal distribution (g). A vector of length S.</p>
</td></tr>
<tr><td><code id="psis_approximate_posterior_+3A_log_g">log_g</code></td>
<td>
<p>The log-density (proposal) evaluated at S samples from the
proposal distribution (g). A vector of length S.</p>
</td></tr>
<tr><td><code id="psis_approximate_posterior_+3A_log_liks">log_liks</code></td>
<td>
<p>A log-likelihood matrix of size S * N, where N is the number
of observations and S is the number of samples from q. See
<code><a href="#topic+loo.matrix">loo.matrix()</a></code> for details. Default is <code>NULL</code>. Then only the
posterior is evaluated using the k_hat diagnostic.</p>
</td></tr>
<tr><td><code id="psis_approximate_posterior_+3A_cores">cores</code></td>
<td>
<p>The number of cores to use for parallelization. This defaults to
the option <code>mc.cores</code> which can be set for an entire R session by
<code>options(mc.cores = NUMBER)</code>. The old option <code>loo.cores</code> is now
deprecated but will be given precedence over <code>mc.cores</code> until
<code>loo.cores</code> is removed in a future release. <strong>As of version
2.0.0 the default is now 1 core if <code>mc.cores</code> is not set</strong>, but we
recommend using as many (or close to as many) cores as possible.
</p>

<ul>
<li><p> Note for Windows 10 users: it is <strong>strongly</strong>
<a href="https://github.com/stan-dev/loo/issues/94">recommended</a> to avoid using
the <code>.Rprofile</code> file to set <code>mc.cores</code> (using the <code>cores</code> argument or
setting <code>mc.cores</code> interactively or in a script is fine).
</p>
</li></ul>
</td></tr>
<tr><td><code id="psis_approximate_posterior_+3A_save_psis">save_psis</code></td>
<td>
<p>Should the <code>psis</code> object created internally by <code>loo()</code> be
saved in the returned object? The <code>loo()</code> function calls <code><a href="#topic+psis">psis()</a></code>
internally but by default discards the (potentially large) <code>psis</code> object
after using it to compute the LOO-CV summaries. Setting <code>save_psis=TRUE</code>
will add a <code>psis_object</code> component to the list returned by <code>loo</code>.
This is useful if you plan to use the <code><a href="#topic+E_loo">E_loo()</a></code> function to compute
weighted expectations after running <code>loo</code>. Several functions in the
<span class="pkg">bayesplot</span> package also accept <code>psis</code> objects.</p>
</td></tr>
<tr><td><code id="psis_approximate_posterior_+3A_log_q">log_q</code></td>
<td>
<p>Deprecated argument name (the same as log_g).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>If log likelihoods are supplied, the function returns a <code>"loo"</code> object,
otherwise the function returns a <code>"psis"</code> object.
</p>


<h3>References</h3>

<p>Vehtari, A., Gelman, A., and Gabry, J. (2017). Practical Bayesian model
evaluation using leave-one-out cross-validation and WAIC.
<em>Statistics and Computing</em>. 27(5), 1413&ndash;1432. doi:10.1007/s11222-016-9696-4
(<a href="https://link.springer.com/article/10.1007/s11222-016-9696-4">journal version</a>,
<a href="https://arxiv.org/abs/1507.04544">preprint arXiv:1507.04544</a>).
</p>
<p>Vehtari, A., Simpson, D., Gelman, A., Yao, Y., and Gabry, J. (2024).
Pareto smoothed importance sampling. <em>Journal of Machine Learning Research</em>,
25(72):1-58.
<a href="https://jmlr.org/papers/v25/19-556.html">PDF</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+loo">loo()</a></code> and <code><a href="#topic+psis">psis()</a></code>
</p>

<hr>
<h2 id='psislw'>Pareto smoothed importance sampling (deprecated, old version)</h2><span id='topic+psislw'></span>

<h3>Description</h3>

<p>As of version <code style="white-space: pre;">&#8288;2.0.0&#8288;</code> this function is <strong>deprecated</strong>. Please use the
<code><a href="#topic+psis">psis()</a></code> function for the new PSIS algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>psislw(
  lw,
  wcp = 0.2,
  wtrunc = 3/4,
  cores = getOption("mc.cores", 1),
  llfun = NULL,
  llargs = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="psislw_+3A_lw">lw</code></td>
<td>
<p>A matrix or vector of log weights. For computing LOO, <code>lw = -log_lik</code>, the <em>negative</em> of an <code class="reqn">S</code> (simulations) by <code class="reqn">N</code> (data
points) pointwise log-likelihood matrix.</p>
</td></tr>
<tr><td><code id="psislw_+3A_wcp">wcp</code></td>
<td>
<p>The proportion of importance weights to use for the generalized
Pareto fit. The <code>100*wcp</code>\
from which to estimate the parameters of the generalized Pareto
distribution.</p>
</td></tr>
<tr><td><code id="psislw_+3A_wtrunc">wtrunc</code></td>
<td>
<p>For truncating very large weights to <code class="reqn">S</code>^<code>wtrunc</code>. Set
to zero for no truncation.</p>
</td></tr>
<tr><td><code id="psislw_+3A_cores">cores</code></td>
<td>
<p>The number of cores to use for parallelization. This defaults to
the option <code>mc.cores</code> which can be set for an entire R session by
<code>options(mc.cores = NUMBER)</code>, the old option <code>loo.cores</code> is now
deprecated but will be given precedence over <code>mc.cores</code> until it is
removed. <strong>As of version 2.0.0, the default is now 1 core if
<code>mc.cores</code> is not set, but we recommend using as many (or close to as
many) cores as possible.</strong></p>
</td></tr>
<tr><td><code id="psislw_+3A_llfun">llfun</code>, <code id="psislw_+3A_llargs">llargs</code></td>
<td>
<p>See <code><a href="#topic+loo.function">loo.function()</a></code>.</p>
</td></tr>
<tr><td><code id="psislw_+3A_...">...</code></td>
<td>
<p>Ignored when <code>psislw()</code> is called directly. The <code>...</code> is
only used internally when <code>psislw()</code> is called by the <code><a href="#topic+loo">loo()</a></code>
function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A named list with components <code>lw_smooth</code> (modified log weights) and
<code>pareto_k</code> (estimated generalized Pareto shape parameter(s) k).
</p>


<h3>References</h3>

<p>Vehtari, A., Gelman, A., and Gabry, J. (2017). Practical Bayesian model
evaluation using leave-one-out cross-validation and WAIC.
<em>Statistics and Computing</em>. 27(5), 1413&ndash;1432. doi:10.1007/s11222-016-9696-4
(<a href="https://link.springer.com/article/10.1007/s11222-016-9696-4">journal version</a>,
<a href="https://arxiv.org/abs/1507.04544">preprint arXiv:1507.04544</a>).
</p>
<p>Vehtari, A., Simpson, D., Gelman, A., Yao, Y., and Gabry, J. (2024).
Pareto smoothed importance sampling. <em>Journal of Machine Learning Research</em>,
25(72):1-58.
<a href="https://jmlr.org/papers/v25/19-556.html">PDF</a>
</p>


<h3>See Also</h3>

<p><a href="#topic+pareto-k-diagnostic">pareto-k-diagnostic</a> for PSIS diagnostics.
</p>

<hr>
<h2 id='relative_eff'>Convenience function for computing relative efficiencies</h2><span id='topic+relative_eff'></span><span id='topic+relative_eff.default'></span><span id='topic+relative_eff.matrix'></span><span id='topic+relative_eff.array'></span><span id='topic+relative_eff.function'></span><span id='topic+relative_eff.importance_sampling'></span>

<h3>Description</h3>

<p><code>relative_eff()</code> computes the the MCMC effective sample size divided by
the total sample size.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>relative_eff(x, ...)

## Default S3 method:
relative_eff(x, chain_id, ...)

## S3 method for class 'matrix'
relative_eff(x, chain_id, ..., cores = getOption("mc.cores", 1))

## S3 method for class 'array'
relative_eff(x, ..., cores = getOption("mc.cores", 1))

## S3 method for class ''function''
relative_eff(
  x,
  chain_id,
  ...,
  cores = getOption("mc.cores", 1),
  data = NULL,
  draws = NULL
)

## S3 method for class 'importance_sampling'
relative_eff(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="relative_eff_+3A_x">x</code></td>
<td>
<p>A vector, matrix, 3-D array, or function. See the <strong>Methods (by
class)</strong> section below for details on specifying <code>x</code>, but where
&quot;log-likelihood&quot; is mentioned replace it with one of the following
depending on the use case:
</p>

<ul>
<li><p> For use with the <code><a href="#topic+loo">loo()</a></code> function, the values in <code>x</code> (or generated by
<code>x</code>, if a function) should be <strong>likelihood</strong> values
(i.e., <code>exp(log_lik)</code>), not on the log scale.
</p>
</li>
<li><p> For generic use with <code><a href="#topic+psis">psis()</a></code>, the values in <code>x</code> should be the reciprocal
of the importance ratios (i.e., <code>exp(-log_ratios)</code>).
</p>
</li></ul>
</td></tr>
<tr><td><code id="relative_eff_+3A_chain_id">chain_id</code></td>
<td>
<p>A vector of length <code>NROW(x)</code> containing MCMC chain
indexes for each each row of <code>x</code> (if a matrix) or each value in
<code>x</code> (if a vector). No <code>chain_id</code> is needed if <code>x</code> is a 3-D
array. If there are <code>C</code> chains then valid chain indexes are values
in <code>1:C</code>.</p>
</td></tr>
<tr><td><code id="relative_eff_+3A_cores">cores</code></td>
<td>
<p>The number of cores to use for parallelization.</p>
</td></tr>
<tr><td><code id="relative_eff_+3A_data">data</code>, <code id="relative_eff_+3A_draws">draws</code>, <code id="relative_eff_+3A_...">...</code></td>
<td>
<p>Same as for the <code><a href="#topic+loo">loo()</a></code> function method.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of relative effective sample sizes.
</p>


<h3>Methods (by class)</h3>


<ul>
<li> <p><code>relative_eff(default)</code>: A vector of length <code class="reqn">S</code> (posterior sample size).
</p>
</li>
<li> <p><code>relative_eff(matrix)</code>: An <code class="reqn">S</code> by <code class="reqn">N</code> matrix, where <code class="reqn">S</code> is the size
of the posterior sample (with all chains merged) and <code class="reqn">N</code> is the number
of data points.
</p>
</li>
<li> <p><code>relative_eff(array)</code>: An <code class="reqn">I</code> by <code class="reqn">C</code> by <code class="reqn">N</code> array, where <code class="reqn">I</code>
is the number of MCMC iterations per chain, <code class="reqn">C</code> is the number of
chains, and <code class="reqn">N</code> is the number of data points.
</p>
</li>
<li> <p><code>relative_eff(`function`)</code>: A function <code>f()</code> that takes arguments <code>data_i</code> and <code>draws</code> and returns a
vector containing the log-likelihood for a single observation <code>i</code> evaluated
at each posterior draw. The function should be written such that, for each
observation <code>i</code> in <code>1:N</code>, evaluating
</p>
<div class="sourceCode"><pre>f(data_i = data[i,, drop=FALSE], draws = draws)
</pre></div>
<p>results in a vector of length <code>S</code> (size of posterior sample). The
log-likelihood function can also have additional arguments but <code>data_i</code> and
<code>draws</code> are required.
</p>
<p>If using the function method then the arguments <code>data</code> and <code>draws</code> must also
be specified in the call to <code>loo()</code>:
</p>

<ul>
<li> <p><code>data</code>: A data frame or matrix containing the data (e.g.
observed outcome and predictors) needed to compute the pointwise
log-likelihood. For each observation <code>i</code>, the <code>i</code>th row of
<code>data</code> will be passed to the <code>data_i</code> argument of the
log-likelihood function.
</p>
</li>
<li> <p><code>draws</code>: An object containing the posterior draws for any
parameters needed to compute the pointwise log-likelihood. Unlike
<code>data</code>, which is indexed by observation, for each observation the
entire object <code>draws</code> will be passed to the <code>draws</code> argument of
the log-likelihood function.
</p>
</li>
<li><p> The <code>...</code> can be used if your log-likelihood function takes additional
arguments. These arguments are used like the <code>draws</code> argument in that they
are recycled for each observation.
</p>
</li></ul>

</li>
<li> <p><code>relative_eff(importance_sampling)</code>: If <code>x</code> is an object of class <code>"psis"</code>, <code>relative_eff()</code> simply returns
the <code>r_eff</code> attribute of <code>x</code>.
</p>
</li></ul>


<h3>Examples</h3>

<pre><code class='language-R'>LLarr &lt;- example_loglik_array()
LLmat &lt;- example_loglik_matrix()
dim(LLarr)
dim(LLmat)

rel_n_eff_1 &lt;- relative_eff(exp(LLarr))
rel_n_eff_2 &lt;- relative_eff(exp(LLmat), chain_id = rep(1:2, each = 500))
all.equal(rel_n_eff_1, rel_n_eff_2)

</code></pre>

<hr>
<h2 id='sis'>Standard importance sampling (SIS)</h2><span id='topic+sis'></span><span id='topic+sis.array'></span><span id='topic+sis.matrix'></span><span id='topic+sis.default'></span>

<h3>Description</h3>

<p>Implementation of standard importance sampling (SIS).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sis(log_ratios, ...)

## S3 method for class 'array'
sis(log_ratios, ..., r_eff = NULL, cores = getOption("mc.cores", 1))

## S3 method for class 'matrix'
sis(log_ratios, ..., r_eff = NULL, cores = getOption("mc.cores", 1))

## Default S3 method:
sis(log_ratios, ..., r_eff = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sis_+3A_log_ratios">log_ratios</code></td>
<td>
<p>An array, matrix, or vector of importance ratios on the log
scale (for Importance sampling LOO, these are <em>negative</em> log-likelihood
values). See the <strong>Methods (by class)</strong> section below for a detailed
description of how to specify the inputs for each method.</p>
</td></tr>
<tr><td><code id="sis_+3A_...">...</code></td>
<td>
<p>Arguments passed on to the various methods.</p>
</td></tr>
<tr><td><code id="sis_+3A_r_eff">r_eff</code></td>
<td>
<p>Vector of relative effective sample size estimates containing
one element per observation. The values provided should be the relative
effective sample sizes of <code>1/exp(log_ratios)</code> (i.e., <code>1/ratios</code>).
This is related to the relative efficiency of estimating the normalizing
term in self-normalizing importance sampling. See the <code><a href="#topic+relative_eff">relative_eff()</a></code>
helper function for computing <code>r_eff</code>. If using <code>psis</code> with
draws of the <code>log_ratios</code> not obtained from MCMC then the warning
message thrown when not specifying <code>r_eff</code> can be disabled by
setting <code>r_eff</code> to <code>NA</code>.</p>
</td></tr>
<tr><td><code id="sis_+3A_cores">cores</code></td>
<td>
<p>The number of cores to use for parallelization. This defaults to
the option <code>mc.cores</code> which can be set for an entire R session by
<code>options(mc.cores = NUMBER)</code>. The old option <code>loo.cores</code> is now
deprecated but will be given precedence over <code>mc.cores</code> until
<code>loo.cores</code> is removed in a future release. <strong>As of version
2.0.0 the default is now 1 core if <code>mc.cores</code> is not set</strong>, but we
recommend using as many (or close to as many) cores as possible.
</p>

<ul>
<li><p> Note for Windows 10 users: it is <strong>strongly</strong>
<a href="https://github.com/stan-dev/loo/issues/94">recommended</a> to avoid using
the <code>.Rprofile</code> file to set <code>mc.cores</code> (using the <code>cores</code> argument or
setting <code>mc.cores</code> interactively or in a script is fine).
</p>
</li></ul>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>sis()</code> methods return an object of class <code>"sis"</code>,
which is a named list with the following components:
</p>

<dl>
<dt><code>log_weights</code></dt><dd>
<p>Vector or matrix of smoothed but <em>unnormalized</em> log
weights. To get normalized weights use the
<code><a href="#topic+weights.importance_sampling">weights()</a></code> method provided for objects of
class <code>sis</code>.
</p>
</dd>
<dt><code>diagnostics</code></dt><dd>
<p>A named list containing one vector:
</p>

<ul>
<li> <p><code>pareto_k</code>: Not used in <code>sis</code>, all set to 0.
</p>
</li>
<li> <p><code>n_eff</code>: effective sample size estimates.
</p>
</li></ul>

</dd>
</dl>

<p>Objects of class <code>"sis"</code> also have the following <a href="base.html#topic+attributes">attributes</a>:
</p>

<dl>
<dt><code>norm_const_log</code></dt><dd>
<p>Vector of precomputed values of <code>colLogSumExps(log_weights)</code> that are
used internally by the <code>weights</code> method to normalize the log weights.
</p>
</dd>
<dt><code>r_eff</code></dt><dd>
<p>If specified, the user's <code>r_eff</code> argument.
</p>
</dd>
<dt><code>tail_len</code></dt><dd>
<p>Not used for <code>sis</code>.
</p>
</dd>
<dt><code>dims</code></dt><dd>
<p>Integer vector of length 2 containing <code>S</code> (posterior sample size)
and <code>N</code> (number of observations).
</p>
</dd>
<dt><code>method</code></dt><dd>
<p>Method used for importance sampling, here <code>sis</code>.
</p>
</dd>
</dl>



<h3>Methods (by class)</h3>


<ul>
<li> <p><code>sis(array)</code>: An <code class="reqn">I</code> by <code class="reqn">C</code> by <code class="reqn">N</code> array, where <code class="reqn">I</code>
is the number of MCMC iterations per chain, <code class="reqn">C</code> is the number of
chains, and <code class="reqn">N</code> is the number of data points.
</p>
</li>
<li> <p><code>sis(matrix)</code>: An <code class="reqn">S</code> by <code class="reqn">N</code> matrix, where <code class="reqn">S</code> is the size
of the posterior sample (with all chains merged) and <code class="reqn">N</code> is the number
of data points.
</p>
</li>
<li> <p><code>sis(default)</code>: A vector of length <code class="reqn">S</code> (posterior sample size).
</p>
</li></ul>


<h3>References</h3>

<p>Vehtari, A., Gelman, A., and Gabry, J. (2017). Practical Bayesian model
evaluation using leave-one-out cross-validation and WAIC.
<em>Statistics and Computing</em>. 27(5), 1413&ndash;1432. doi:10.1007/s11222-016-9696-4
(<a href="https://link.springer.com/article/10.1007/s11222-016-9696-4">journal version</a>,
<a href="https://arxiv.org/abs/1507.04544">preprint arXiv:1507.04544</a>).
</p>
<p>Vehtari, A., Simpson, D., Gelman, A., Yao, Y., and Gabry, J. (2024).
Pareto smoothed importance sampling. <em>Journal of Machine Learning Research</em>,
25(72):1-58.
<a href="https://jmlr.org/papers/v25/19-556.html">PDF</a>
</p>


<h3>See Also</h3>


<ul>
<li> <p><code><a href="#topic+psis">psis()</a></code> for approximate LOO-CV using PSIS.
</p>
</li>
<li> <p><code><a href="#topic+loo">loo()</a></code> for approximate LOO-CV.
</p>
</li>
<li> <p><a href="#topic+pareto-k-diagnostic">pareto-k-diagnostic</a> for PSIS diagnostics.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>log_ratios &lt;- -1 * example_loglik_array()
r_eff &lt;- relative_eff(exp(-log_ratios))
sis_result &lt;- sis(log_ratios, r_eff = r_eff)
str(sis_result)

# extract smoothed weights
lw &lt;- weights(sis_result) # default args are log=TRUE, normalize=TRUE
ulw &lt;- weights(sis_result, normalize=FALSE) # unnormalized log-weights

w &lt;- weights(sis_result, log=FALSE) # normalized weights (not log-weights)
uw &lt;- weights(sis_result, log=FALSE, normalize = FALSE) # unnormalized weights

</code></pre>

<hr>
<h2 id='tis'>Truncated importance sampling (TIS)</h2><span id='topic+tis'></span><span id='topic+tis.array'></span><span id='topic+tis.matrix'></span><span id='topic+tis.default'></span>

<h3>Description</h3>

<p>Implementation of truncated (self-normalized) importance sampling (TIS),
truncated at S^(1/2) as recommended by Ionides (2008).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tis(log_ratios, ...)

## S3 method for class 'array'
tis(log_ratios, ..., r_eff = 1, cores = getOption("mc.cores", 1))

## S3 method for class 'matrix'
tis(log_ratios, ..., r_eff = 1, cores = getOption("mc.cores", 1))

## Default S3 method:
tis(log_ratios, ..., r_eff = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tis_+3A_log_ratios">log_ratios</code></td>
<td>
<p>An array, matrix, or vector of importance ratios on the log
scale (for Importance sampling LOO, these are <em>negative</em> log-likelihood
values). See the <strong>Methods (by class)</strong> section below for a detailed
description of how to specify the inputs for each method.</p>
</td></tr>
<tr><td><code id="tis_+3A_...">...</code></td>
<td>
<p>Arguments passed on to the various methods.</p>
</td></tr>
<tr><td><code id="tis_+3A_r_eff">r_eff</code></td>
<td>
<p>Vector of relative effective sample size estimates containing
one element per observation. The values provided should be the relative
effective sample sizes of <code>1/exp(log_ratios)</code> (i.e., <code>1/ratios</code>).
This is related to the relative efficiency of estimating the normalizing
term in self-normalizing importance sampling. If <code>r_eff</code> is not
provided then the reported (T)IS effective sample sizes and Monte Carlo
error estimates can be over-optimistic. If the posterior draws are (near)
independent then <code>r_eff=1</code> can be used. <code>r_eff</code> has to be a scalar (same
value is used for all observations) or a vector with length equal to the
number of observations. The default value is 1. See the <code><a href="#topic+relative_eff">relative_eff()</a></code>
helper function for computing <code>r_eff</code>.</p>
</td></tr>
<tr><td><code id="tis_+3A_cores">cores</code></td>
<td>
<p>The number of cores to use for parallelization. This defaults to
the option <code>mc.cores</code> which can be set for an entire R session by
<code>options(mc.cores = NUMBER)</code>. The old option <code>loo.cores</code> is now
deprecated but will be given precedence over <code>mc.cores</code> until
<code>loo.cores</code> is removed in a future release. <strong>As of version
2.0.0 the default is now 1 core if <code>mc.cores</code> is not set</strong>, but we
recommend using as many (or close to as many) cores as possible.
</p>

<ul>
<li><p> Note for Windows 10 users: it is <strong>strongly</strong>
<a href="https://github.com/stan-dev/loo/issues/94">recommended</a> to avoid using
the <code>.Rprofile</code> file to set <code>mc.cores</code> (using the <code>cores</code> argument or
setting <code>mc.cores</code> interactively or in a script is fine).
</p>
</li></ul>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>tis()</code> methods return an object of class <code>"tis"</code>,
which is a named list with the following components:
</p>

<dl>
<dt><code>log_weights</code></dt><dd>
<p>Vector or matrix of smoothed (and truncated) but <em>unnormalized</em> log
weights. To get normalized weights use the
<code><a href="#topic+weights.importance_sampling">weights()</a></code> method provided for objects of
class <code>tis</code>.
</p>
</dd>
<dt><code>diagnostics</code></dt><dd>
<p>A named list containing one vector:
</p>

<ul>
<li> <p><code>pareto_k</code>: Not used in <code>tis</code>, all set to 0.
</p>
</li>
<li> <p><code>n_eff</code>: Effective sample size estimates.
</p>
</li></ul>

</dd>
</dl>

<p>Objects of class <code>"tis"</code> also have the following <a href="base.html#topic+attributes">attributes</a>:
</p>

<dl>
<dt><code>norm_const_log</code></dt><dd>
<p>Vector of precomputed values of <code>colLogSumExps(log_weights)</code> that are
used internally by the <code><a href="stats.html#topic+weights">weights()</a></code>method to normalize the log weights.
</p>
</dd>
<dt><code>r_eff</code></dt><dd>
<p>If specified, the user's <code>r_eff</code> argument.
</p>
</dd>
<dt><code>tail_len</code></dt><dd>
<p>Not used for <code>tis</code>.
</p>
</dd>
<dt><code>dims</code></dt><dd>
<p>Integer vector of length 2 containing <code>S</code> (posterior sample size)
and <code>N</code> (number of observations).
</p>
</dd>
<dt><code>method</code></dt><dd>
<p>Method used for importance sampling, here <code>tis</code>.
</p>
</dd>
</dl>



<h3>Methods (by class)</h3>


<ul>
<li> <p><code>tis(array)</code>: An <code class="reqn">I</code> by <code class="reqn">C</code> by <code class="reqn">N</code> array, where <code class="reqn">I</code>
is the number of MCMC iterations per chain, <code class="reqn">C</code> is the number of
chains, and <code class="reqn">N</code> is the number of data points.
</p>
</li>
<li> <p><code>tis(matrix)</code>: An <code class="reqn">S</code> by <code class="reqn">N</code> matrix, where <code class="reqn">S</code> is the size
of the posterior sample (with all chains merged) and <code class="reqn">N</code> is the number
of data points.
</p>
</li>
<li> <p><code>tis(default)</code>: A vector of length <code class="reqn">S</code> (posterior sample size).
</p>
</li></ul>


<h3>References</h3>

<p>Ionides, Edward L. (2008). Truncated importance sampling.
<em>Journal of Computational and Graphical Statistics</em> 17(2): 295&ndash;311.
</p>


<h3>See Also</h3>


<ul>
<li> <p><code><a href="#topic+psis">psis()</a></code> for approximate LOO-CV using PSIS.
</p>
</li>
<li> <p><code><a href="#topic+loo">loo()</a></code> for approximate LOO-CV.
</p>
</li>
<li> <p><a href="#topic+pareto-k-diagnostic">pareto-k-diagnostic</a> for PSIS diagnostics.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>log_ratios &lt;- -1 * example_loglik_array()
r_eff &lt;- relative_eff(exp(-log_ratios))
tis_result &lt;- tis(log_ratios, r_eff = r_eff)
str(tis_result)

# extract smoothed weights
lw &lt;- weights(tis_result) # default args are log=TRUE, normalize=TRUE
ulw &lt;- weights(tis_result, normalize=FALSE) # unnormalized log-weights

w &lt;- weights(tis_result, log=FALSE) # normalized weights (not log-weights)
uw &lt;- weights(tis_result, log=FALSE, normalize = FALSE) # unnormalized weights

</code></pre>

<hr>
<h2 id='update.psis_loo_ss'>Update <code>psis_loo_ss</code> objects</h2><span id='topic+update.psis_loo_ss'></span>

<h3>Description</h3>

<p>Update <code>psis_loo_ss</code> objects
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'psis_loo_ss'
update(
  object,
  ...,
  data = NULL,
  draws = NULL,
  observations = NULL,
  r_eff = 1,
  cores = getOption("mc.cores", 1),
  loo_approximation = NULL,
  loo_approximation_draws = NULL,
  llgrad = NULL,
  llhess = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="update.psis_loo_ss_+3A_object">object</code></td>
<td>
<p>A <code>psis_loo_ss</code> object to update.</p>
</td></tr>
<tr><td><code id="update.psis_loo_ss_+3A_...">...</code></td>
<td>
<p>Currently not used.</p>
</td></tr>
<tr><td><code id="update.psis_loo_ss_+3A_data">data</code>, <code id="update.psis_loo_ss_+3A_draws">draws</code></td>
<td>
<p>See <code><a href="#topic+loo_subsample.function">loo_subsample.function()</a></code>.</p>
</td></tr>
<tr><td><code id="update.psis_loo_ss_+3A_observations">observations</code></td>
<td>
<p>The subsample observations to use. The argument can take
four (4) types of arguments:
</p>

<ul>
<li> <p><code>NULL</code> to use all observations. The algorithm then just uses
standard <code>loo()</code> or <code>loo_approximate_posterior()</code>.
</p>
</li>
<li><p> A single integer to specify the number of observations to be subsampled.
</p>
</li>
<li><p> A vector of integers to provide the indices used to subset the data.
<em>These observations need to be subsampled with the same scheme as given by
the <code>estimator</code> argument</em>.
</p>
</li>
<li><p> A <code>psis_loo_ss</code> object to use the same observations that were used in a
previous call to <code>loo_subsample()</code>.
</p>
</li></ul>
</td></tr>
<tr><td><code id="update.psis_loo_ss_+3A_r_eff">r_eff</code></td>
<td>
<p>Vector of relative effective sample size estimates for the
likelihood (<code>exp(log_lik)</code>) of each observation. This is related to
the relative efficiency of estimating the normalizing term in
self-normalized importance sampling when using posterior draws obtained
with MCMC. If MCMC draws are used and <code>r_eff</code> is not provided then
the reported PSIS effective sample sizes and Monte Carlo error estimates
can be over-optimistic. If the posterior draws are (near) independent then
<code>r_eff=1</code> can be used. <code>r_eff</code> has to be a scalar (same value is used
for all observations) or a vector with length equal to the number of
observations. The default value is 1. See the <code><a href="#topic+relative_eff">relative_eff()</a></code> helper
functions for help computing <code>r_eff</code>.</p>
</td></tr>
<tr><td><code id="update.psis_loo_ss_+3A_cores">cores</code></td>
<td>
<p>The number of cores to use for parallelization. This defaults to
the option <code>mc.cores</code> which can be set for an entire R session by
<code>options(mc.cores = NUMBER)</code>. The old option <code>loo.cores</code> is now
deprecated but will be given precedence over <code>mc.cores</code> until
<code>loo.cores</code> is removed in a future release. <strong>As of version
2.0.0 the default is now 1 core if <code>mc.cores</code> is not set</strong>, but we
recommend using as many (or close to as many) cores as possible.
</p>

<ul>
<li><p> Note for Windows 10 users: it is <strong>strongly</strong>
<a href="https://github.com/stan-dev/loo/issues/94">recommended</a> to avoid using
the <code>.Rprofile</code> file to set <code>mc.cores</code> (using the <code>cores</code> argument or
setting <code>mc.cores</code> interactively or in a script is fine).
</p>
</li></ul>
</td></tr>
<tr><td><code id="update.psis_loo_ss_+3A_loo_approximation">loo_approximation</code></td>
<td>
<p>What type of approximation of the loo_i's should be used?
The default is <code>"plpd"</code> (the log predictive density using the posterior expectation).
There are six different methods implemented to approximate loo_i's
(see the references for more details):
</p>

<ul>
<li> <p><code>"plpd"</code>: uses the lpd based on point estimates (i.e., <code class="reqn">p(y_i|\hat{\theta})</code>).
</p>
</li>
<li> <p><code>"lpd"</code>: uses the lpds (i,e., <code class="reqn">p(y_i|y)</code>).
</p>
</li>
<li> <p><code>"tis"</code>: uses truncated importance sampling to approximate PSIS-LOO.
</p>
</li>
<li> <p><code>"waic"</code>: uses waic (i.e., <code class="reqn">p(y_i|y) - p_{waic}</code>).
</p>
</li>
<li> <p><code>"waic_grad_marginal"</code>: uses waic approximation using first order delta
method and posterior marginal variances to approximate <code class="reqn">p_{waic}</code> (ie.
<code class="reqn">p(y_i|\hat{\theta})</code>-p_waic_grad_marginal). Requires gradient of
likelihood function.
</p>
</li>
<li> <p><code>"waic_grad"</code>: uses waic approximation using first order delta method and
posterior covariance to approximate <code class="reqn">p_{waic}</code> (ie.
<code class="reqn">p(y_i|\hat{\theta})</code>-p_waic_grad). Requires gradient of likelihood
function.
</p>
</li>
<li> <p><code>"waic_hess"</code>: uses waic approximation using second order delta method and
posterior covariance to approximate <code class="reqn">p_{waic}</code> (ie.
<code class="reqn">p(y_i|\hat{\theta})</code>-p_waic_grad). Requires gradient and Hessian of
likelihood function.
</p>
</li></ul>

<p>As point estimates of <code class="reqn">\hat{\theta}</code>, the posterior expectations
of the parameters are used.</p>
</td></tr>
<tr><td><code id="update.psis_loo_ss_+3A_loo_approximation_draws">loo_approximation_draws</code></td>
<td>
<p>The number of posterior draws used when
integrating over the posterior. This is used if <code>loo_approximation</code> is set
to <code>"lpd"</code>, <code>"waic"</code>, or <code>"tis"</code>.</p>
</td></tr>
<tr><td><code id="update.psis_loo_ss_+3A_llgrad">llgrad</code></td>
<td>
<p>The gradient of the log-likelihood. This
is only used when <code>loo_approximation</code> is <code>"waic_grad"</code>,
<code>"waic_grad_marginal"</code>, or <code>"waic_hess"</code>. The default is <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="update.psis_loo_ss_+3A_llhess">llhess</code></td>
<td>
<p>The Hessian of the log-likelihood. This is only used
with <code>loo_approximation = "waic_hess"</code>. The default is <code>NULL</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>observations</code> is updated then if a vector of indices or a <code>psis_loo_ss</code>
object is supplied the updated object will have exactly the observations
indicated by the vector or <code>psis_loo_ss</code> object. If a single integer is
supplied, new observations will be sampled to reach the supplied sample size.
</p>


<h3>Value</h3>

<p>A <code>psis_loo_ss</code> object.
</p>

<hr>
<h2 id='waic'>Widely applicable information criterion (WAIC)</h2><span id='topic+waic'></span><span id='topic+waic.array'></span><span id='topic+waic.matrix'></span><span id='topic+waic.function'></span><span id='topic+is.waic'></span>

<h3>Description</h3>

<p>The <code>waic()</code> methods can be used to compute WAIC from the pointwise
log-likelihood. However, we recommend LOO-CV using PSIS (as implemented by
the <code><a href="#topic+loo">loo()</a></code> function) because PSIS provides useful diagnostics as well as
effective sample size and Monte Carlo estimates.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>waic(x, ...)

## S3 method for class 'array'
waic(x, ...)

## S3 method for class 'matrix'
waic(x, ...)

## S3 method for class ''function''
waic(x, ..., data = NULL, draws = NULL)

is.waic(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="waic_+3A_x">x</code></td>
<td>
<p>A log-likelihood array, matrix, or function. The <strong>Methods (by class)</strong>
section, below, has detailed descriptions of how to specify the inputs for
each method.</p>
</td></tr>
<tr><td><code id="waic_+3A_draws">draws</code>, <code id="waic_+3A_data">data</code>, <code id="waic_+3A_...">...</code></td>
<td>
<p>For the function method only. See the
<strong>Methods (by class)</strong> section below for details on these arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A named list (of class <code>c("waic", "loo")</code>) with components:
</p>

<dl>
<dt><code>estimates</code></dt><dd>
<p>A matrix with two columns (<code>"Estimate"</code>, <code>"SE"</code>) and three
rows (<code>"elpd_waic"</code>, <code>"p_waic"</code>, <code>"waic"</code>). This contains
point estimates and standard errors of the expected log pointwise predictive
density (<code>elpd_waic</code>), the effective number of parameters
(<code>p_waic</code>) and the information criterion <code>waic</code> (which is just
<code>-2 * elpd_waic</code>, i.e., converted to deviance scale).
</p>
</dd>
<dt><code>pointwise</code></dt><dd>
<p>A matrix with three columns (and number of rows equal to the number of
observations) containing the pointwise contributions of each of the above
measures (<code>elpd_waic</code>, <code>p_waic</code>, <code>waic</code>).
</p>
</dd>
</dl>



<h3>Methods (by class)</h3>


<ul>
<li> <p><code>waic(array)</code>: An <code class="reqn">I</code> by <code class="reqn">C</code> by <code class="reqn">N</code> array, where <code class="reqn">I</code>
is the number of MCMC iterations per chain, <code class="reqn">C</code> is the number of
chains, and <code class="reqn">N</code> is the number of data points.
</p>
</li>
<li> <p><code>waic(matrix)</code>: An <code class="reqn">S</code> by <code class="reqn">N</code> matrix, where <code class="reqn">S</code> is the size
of the posterior sample (with all chains merged) and <code class="reqn">N</code> is the number
of data points.
</p>
</li>
<li> <p><code>waic(`function`)</code>: A function <code>f()</code> that takes arguments <code>data_i</code> and <code>draws</code> and returns a
vector containing the log-likelihood for a single observation <code>i</code> evaluated
at each posterior draw. The function should be written such that, for each
observation <code>i</code> in <code>1:N</code>, evaluating
</p>
<div class="sourceCode"><pre>f(data_i = data[i,, drop=FALSE], draws = draws)
</pre></div>
<p>results in a vector of length <code>S</code> (size of posterior sample). The
log-likelihood function can also have additional arguments but <code>data_i</code> and
<code>draws</code> are required.
</p>
<p>If using the function method then the arguments <code>data</code> and <code>draws</code> must also
be specified in the call to <code>loo()</code>:
</p>

<ul>
<li> <p><code>data</code>: A data frame or matrix containing the data (e.g.
observed outcome and predictors) needed to compute the pointwise
log-likelihood. For each observation <code>i</code>, the <code>i</code>th row of
<code>data</code> will be passed to the <code>data_i</code> argument of the
log-likelihood function.
</p>
</li>
<li> <p><code>draws</code>: An object containing the posterior draws for any
parameters needed to compute the pointwise log-likelihood. Unlike
<code>data</code>, which is indexed by observation, for each observation the
entire object <code>draws</code> will be passed to the <code>draws</code> argument of
the log-likelihood function.
</p>
</li>
<li><p> The <code>...</code> can be used if your log-likelihood function takes additional
arguments. These arguments are used like the <code>draws</code> argument in that they
are recycled for each observation.
</p>
</li></ul>

</li></ul>


<h3>References</h3>

<p>Watanabe, S. (2010). Asymptotic equivalence of Bayes cross validation and
widely application information criterion in singular learning theory.
<em>Journal of Machine Learning Research</em> <strong>11</strong>, 3571-3594.
</p>
<p>Vehtari, A., Gelman, A., and Gabry, J. (2017). Practical Bayesian model
evaluation using leave-one-out cross-validation and WAIC.
<em>Statistics and Computing</em>. 27(5), 1413&ndash;1432. doi:10.1007/s11222-016-9696-4
(<a href="https://link.springer.com/article/10.1007/s11222-016-9696-4">journal version</a>,
<a href="https://arxiv.org/abs/1507.04544">preprint arXiv:1507.04544</a>).
</p>
<p>Vehtari, A., Simpson, D., Gelman, A., Yao, Y., and Gabry, J. (2024).
Pareto smoothed importance sampling. <em>Journal of Machine Learning Research</em>,
25(72):1-58.
<a href="https://jmlr.org/papers/v25/19-556.html">PDF</a>
</p>


<h3>See Also</h3>


<ul>
<li><p> The <strong>loo</strong> package <a href="https://mc-stan.org/loo/articles/">vignettes</a> and
Vehtari, Gelman, and Gabry (2017) and Vehtari, Simpson, Gelman, Yao,
and Gabry (2024) for more details on why we prefer <code>loo()</code> to <code>waic()</code>.
</p>
</li>
<li> <p><code><a href="#topic+loo_compare">loo_compare()</a></code> for comparing models on approximate LOO-CV or WAIC.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>### Array and matrix methods
LLarr &lt;- example_loglik_array()
dim(LLarr)

LLmat &lt;- example_loglik_matrix()
dim(LLmat)

waic_arr &lt;- waic(LLarr)
waic_mat &lt;- waic(LLmat)
identical(waic_arr, waic_mat)


## Not run: 
log_lik1 &lt;- extract_log_lik(stanfit1)
log_lik2 &lt;- extract_log_lik(stanfit2)
(waic1 &lt;- waic(log_lik1))
(waic2 &lt;- waic(log_lik2))
print(compare(waic1, waic2), digits = 2)

## End(Not run)

</code></pre>

<hr>
<h2 id='weights.importance_sampling'>Extract importance sampling weights</h2><span id='topic+weights.importance_sampling'></span>

<h3>Description</h3>

<p>Extract importance sampling weights
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'importance_sampling'
weights(object, ..., log = TRUE, normalize = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="weights.importance_sampling_+3A_object">object</code></td>
<td>
<p>An object returned by <code><a href="#topic+psis">psis()</a></code>, <code><a href="#topic+tis">tis()</a></code>, or <code><a href="#topic+sis">sis()</a></code>.</p>
</td></tr>
<tr><td><code id="weights.importance_sampling_+3A_...">...</code></td>
<td>
<p>Ignored.</p>
</td></tr>
<tr><td><code id="weights.importance_sampling_+3A_log">log</code></td>
<td>
<p>Should the weights be returned on the log scale? Defaults to
<code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="weights.importance_sampling_+3A_normalize">normalize</code></td>
<td>
<p>Should the weights be normalized? Defaults to <code>TRUE</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>weights()</code> method returns an object with the same dimensions as
the <code>log_weights</code> component of <code>object</code>. The <code>normalize</code> and <code>log</code>
arguments control whether the returned weights are normalized and whether
or not to return them on the log scale.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># See the examples at help("psis")

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
