<!DOCTYPE html><html lang="en"><head><title>Help for package ebmc</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {ebmc}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#adam2'><p>Implementation of AdaBoost.M2</p></a></li>
<li><a href='#measure'><p>Calculating Performance Measurement in Class Imbalance Problem</p></a></li>
<li><a href='#predict.modelBag'><p>Predict Method for modelBag Object</p></a></li>
<li><a href='#predict.modelBst'><p>Predict Method for modelBst Object</p></a></li>
<li><a href='#rus'><p>Implementation of RUSBoost</p></a></li>
<li><a href='#sbag'><p>Implementation of SMOTEBagging</p></a></li>
<li><a href='#sbo'><p>Implementation of SMOTEBoost</p></a></li>
<li><a href='#ub'><p>Implementation of UnderBagging</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Ensemble-Based Methods for Class Imbalance Problem</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.1</td>
</tr>
<tr>
<td>Author:</td>
<td>Hsiang Hao, Chen</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>"Hsiang Hao, Chen" &lt;kbman1101@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Four ensemble-based methods (SMOTEBoost, RUSBoost, UnderBagging, and SMOTEBagging) for class imbalance problem are implemented for binary classification. Such methods adopt ensemble methods and data re-sampling techniques to improve model performance in presence of class imbalance problem. One special feature offers the possibility to choose multiple supervised learning algorithms to build weak learners within ensemble models. References: Nitesh V. Chawla, Aleksandar Lazarevic, Lawrence O. Hall, and Kevin W. Bowyer (2003) &lt;<a href="https://doi.org/10.1007%2F978-3-540-39804-2_12">doi:10.1007/978-3-540-39804-2_12</a>&gt;, Chris Seiffert, Taghi M. Khoshgoftaar, Jason Van Hulse, and Amri Napolitano (2010) &lt;<a href="https://doi.org/10.1109%2FTSMCA.2009.2029559">doi:10.1109/TSMCA.2009.2029559</a>&gt;, R. Barandela, J. S. Sanchez, R. M. Valdovinos (2003) &lt;<a href="https://doi.org/10.1007%2Fs10044-003-0192-z">doi:10.1007/s10044-003-0192-z</a>&gt;, Shuo Wang and Xin Yao (2009) &lt;<a href="https://doi.org/10.1109%2FCIDM.2009.4938667">doi:10.1109/CIDM.2009.4938667</a>&gt;, Yoav Freund and Robert E. Schapire (1997) &lt;<a href="https://doi.org/10.1006%2Fjcss.1997.1504">doi:10.1006/jcss.1997.1504</a>&gt;.</td>
</tr>
<tr>
<td>Depends:</td>
<td>methods</td>
</tr>
<tr>
<td>Imports:</td>
<td>e1071, rpart, C50, randomForest, pROC, smotefamily</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-01-08 07:15:04 UTC; kbman</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-01-10 18:12:44 UTC</td>
</tr>
</table>
<hr>
<h2 id='adam2'>Implementation of AdaBoost.M2</h2><span id='topic+adam2'></span>

<h3>Description</h3>

<p>The function implements AdaBoost.M2 for binary classification. It returns a list of weak learners that are built on random under-sampled training-sets, and a vector of error estimations of each weak learner. The weak learners altogether consist the ensemble model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>adam2(formula, data, size, alg, rf.ntree = 50, svm.ker = "radial")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="adam2_+3A_formula">formula</code></td>
<td>
<p>A formula specify predictors and target variable. Target variable should be a factor of 0 and 1. Predictors can be either numerical and categorical.</p>
</td></tr>
<tr><td><code id="adam2_+3A_data">data</code></td>
<td>
<p>A data frame used for training the model, i.e. training set.</p>
</td></tr>
<tr><td><code id="adam2_+3A_size">size</code></td>
<td>
<p>Ensemble size, i.e. number of weak learners in the ensemble model.</p>
</td></tr>
<tr><td><code id="adam2_+3A_alg">alg</code></td>
<td>
<p>The learning algorithm used to train weak learners in the ensemble model. <em>cart</em>, <em>c50</em>, <em>rf</em>, <em>nb</em>, and <em>svm</em> are available. Please see Details for more information.</p>
</td></tr>
<tr><td><code id="adam2_+3A_rf.ntree">rf.ntree</code></td>
<td>
<p>Number of decision trees in each forest of the ensemble model when using <em>rf</em> (Random Forest) as base learner. Integer is required.</p>
</td></tr>
<tr><td><code id="adam2_+3A_svm.ker">svm.ker</code></td>
<td>
<p>Specifying kernel function when using svm as base algorithm. Four options are available: <b>linear</b>, <b>polynomial</b>, <b>radial</b>, and <b>sigmoid</b>. Default is radial. Equivalent to that in e1071::svm().</p>
</td></tr>
</table>


<h3>Details</h3>

<p>AdaBoost.M2 is an extension of AdaBoost. AdaBoost.M2 introduces <em>pseudo-loss</em>, which is a more sophisticated method to estimate error and update instance weight in each iteration compared to AdaBoost and AdaBoost.M1. Although AdaBoost.M2 is originally implemented with decision tree, this function makes it possible to use other learning algorithms for building weak learners.
</p>
<p>Argument <em>alg</em> specifies the learning algorithm used to train weak learners within the ensemble model. Totally five algorithms are implemented: <b>cart</b> (Classification and Regression Tree), <b>c50</b> (C5.0 Decision Tree), <b>rf</b> (Random Forest), <b>nb</b> (Naive Bayes), and <b>svm</b> (Support Vector Machine). When using Random Forest as base learner, the ensemble model is consisted of forests and each forest contains a number of trees.
</p>
<p>The function requires the target varible to be a factor of 0 and 1, where 1 indicates minority while 0 indicates majority instances. Only binary classification is implemented in this version.
</p>
<p>The object class of returned list is defined as <em>modelBst</em>, which can be directly passed to predict() for predicting test instances.
</p>


<h3>Value</h3>

<p>The function returns a list containing two elements:
</p>
<table role = "presentation">
<tr><td><code>weakLearners</code></td>
<td>
<p>A list of weak learners.</p>
</td></tr>
<tr><td><code>errorEstimation</code></td>
<td>
<p>Error estimation of each weak learner. Calculated by using (pseudo_loss + smooth) / (1 - pseudo_loss + smooth). <em>smooth</em> helps prevent error rate = 0 resulted from perfect classfication during trainging iterations. For more information, please see Schapire et al. (1999) Section 4.2.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Freund, Y. and Schapire, R. 1997. A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting. Journal of Computer and System Sciences. 55, pp. 119-139.
</p>
<p>Freund, Y.  and  Schapire,  R.  1996.  Experiments  with  a  new  boosting  algorithm. Machine Learning: In Proceedings of the 13th International Conference. pp. 148-156
</p>
<p>Schapire, R. and Singer, Y. 1999. Improved Boosting Algorithms Using Confidence-rated Predictions. Machine Learning. 37(3). pp. 297-336.
</p>
<p>Galar, M., Fernandez, A., Barrenechea, E., Bustince,  H., and Herrera, F. 2012. A Review  on  Ensembles  for  the  Class  Imbalance  Problem:  Bagging-,  Boosting-,  and Hybrid-Based Approaches. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews). 42(4), pp. 463-484.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("iris")
iris &lt;- iris[1:70, ]
iris$Species &lt;- factor(iris$Species, levels = c("setosa", "versicolor"), labels = c("0", "1"))
model1 &lt;- adam2(Species ~ ., data = iris, size = 10, alg = "c50")
model2 &lt;- adam2(Species ~ ., data = iris, size = 20, alg = "rf", rf.ntree = 100)
model3 &lt;- adam2(Species ~ ., data = iris, size = 40, alg = "svm", svm.ker = "sigmoid")
</code></pre>

<hr>
<h2 id='measure'>Calculating Performance Measurement in Class Imbalance Problem</h2><span id='topic+measure'></span>

<h3>Description</h3>

<p>The function is an interation of multiple performance measurements that can be used to assess model performance in class imbalance problem. Totally six measurements are included.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>measure(label, probability, metric, threshold = 0.5)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="measure_+3A_label">label</code></td>
<td>
<p>A vector of actual labels of target variable in test set.</p>
</td></tr>
<tr><td><code id="measure_+3A_probability">probability</code></td>
<td>
<p>A vector of probability estimated by the model.</p>
</td></tr>
<tr><td><code id="measure_+3A_metric">metric</code></td>
<td>
<p>Measurement used for assessing model performance. <b>auc</b>, <b>gmean</b>, <b>tpr</b>, <b>tnr</b>, <b>f</b>, and <b>acc</b> are available. Please see Details for more information.</p>
</td></tr>
<tr><td><code id="measure_+3A_threshold">threshold</code></td>
<td>
<p>Probability threshold for determining the class of instances. A numerical value ranging from 0 to 1. Default is 0.5</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function integrates six common measurements. It uses pROC::roc() and pROC::auc() to calculate <b>auc</b> (Area Under Curve), while calculates other measurements without dependency on other package: <b>gmean</b> (Geometric Mean), <b>tpr</b> (True Positive Rate), <b>tnr</b> (True Negative Rate),and <b>f</b> (F-Measure).
</p>
<p><b>acc</b> (Accuracy) is also included for any possible use, although such measurement can be misleading when the classes of test set is highly imbalanced.
</p>
<p><em>threshold</em> is the probability cutoff for determing the predicted class of instances. For AUC, users do not need to specify threshold because AUC is not affected by the probability cutoff. However, the threshold is required for other five measurements.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("iris")
iris &lt;- iris[1:70, ]
iris$Species &lt;- factor(iris$Species, levels = c("setosa", "versicolor"), labels = c("0", "1"))

# Creat training and test set
samp &lt;- sample(nrow(iris), nrow(iris) * 0.7)
train &lt;- iris[samp, ]
test &lt;- iris[-samp, ]

# Model building and prediction
model &lt;- rus(Species ~ ., data = train, size = 10, alg = "c50")
prob &lt;- predict(model, newdata = test, type = "prob")

# Calculate measurements
auc &lt;- measure(label = test$Species, probability = prob, metric = "auc")
gmean &lt;- measure(label = test$Species, probability = prob, metric = "gmean", threshold = 0.5)
</code></pre>

<hr>
<h2 id='predict.modelBag'>Predict Method for modelBag Object</h2><span id='topic+predict.modelBag'></span>

<h3>Description</h3>

<p>Predicting instances in test set using modelBag object
</p>


<h3>Usage</h3>

<pre><code class='language-R'> ## S3 method for class 'modelBag'
predict(object, newdata, type = "prob", ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.modelBag_+3A_object">object</code></td>
<td>
<p>A object of <em>modelBag</em> class.</p>
</td></tr>
<tr><td><code id="predict.modelBag_+3A_newdata">newdata</code></td>
<td>
<p>A <em>data frame</em> object containing new instances.</p>
</td></tr>
<tr><td><code id="predict.modelBag_+3A_type">type</code></td>
<td>
<p>Types of output, which can be <b>prob</b> (probability) and <b>class</b> (predicted label). Default is prob.</p>
</td></tr>
<tr><td><code id="predict.modelBag_+3A_...">...</code></td>
<td>
<p>Not used currently.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Two type of output can be selected:
</p>
<table role = "presentation">
<tr><td><code>prob</code></td>
<td>
<p>Estimated probability of being a minority instance (i.e. 1). The probability is averaged by using an equal-weight majority vote by all weak learners.</p>
</td></tr>
<tr><td><code>class</code></td>
<td>
<p>Predicted class of the instance. Instances of probability larger than 0.5 are predicted as 1, otherwise 0.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>data("iris")
iris &lt;- iris[1:70, ]
iris$Species &lt;- factor(iris$Species, levels = c("setosa", "versicolor"), labels = c("0", "1"))
samp &lt;- sample(nrow(iris), nrow(iris) * 0.7)
train &lt;- iris[samp, ]
test &lt;- iris[-samp, ]
model &lt;- ub(Species ~ ., data = train, size = 10, alg = "c50") # Build UnderBagging model
prob &lt;- predict(model, newdata = test, type = "prob") # return probability estimation
pred &lt;- predict(model, newdata = test, type = "class") # return predicted class
</code></pre>

<hr>
<h2 id='predict.modelBst'>Predict Method for modelBst Object</h2><span id='topic+predict.modelBst'></span>

<h3>Description</h3>

<p>Predicting instances in test set using modelBst object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  ## S3 method for class 'modelBst'
predict(object, newdata, type = "prob", ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.modelBst_+3A_object">object</code></td>
<td>
<p>A object of <em>modelBst</em> class.</p>
</td></tr>
<tr><td><code id="predict.modelBst_+3A_newdata">newdata</code></td>
<td>
<p>A <em>data frame</em> object containing new instances.</p>
</td></tr>
<tr><td><code id="predict.modelBst_+3A_type">type</code></td>
<td>
<p>Types of output, which can be <b>prob</b> (probability) and <b>class</b> (predicted label). Default is prob.</p>
</td></tr>
<tr><td><code id="predict.modelBst_+3A_...">...</code></td>
<td>
<p>Not used currently.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Two type of output can be selected:
</p>
<table role = "presentation">
<tr><td><code>prob</code></td>
<td>
<p>Estimated probability of being a minority instance (i.e. 1). The probability is averaged by using a majority vote by all weak learners, weighted by error estimation.</p>
</td></tr>
<tr><td><code>class</code></td>
<td>
<p>Predicted class of the instance. Instances of probability larger than 0.5 are predicted as 1, otherwise 0.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>data("iris")
iris &lt;- iris[1:70, ]
iris$Species &lt;- factor(iris$Species, levels = c("setosa", "versicolor"), labels = c("0", "1"))
samp &lt;- sample(nrow(iris), nrow(iris) * 0.7)
train &lt;- iris[samp, ]
test &lt;- iris[-samp, ]
model &lt;- rus(Species ~ ., data = train, size = 10, alg = "c50") # Build RUSBoost model
prob &lt;- predict(model, newdata = test, type = "prob") # return probability estimation
pred &lt;- predict(model, newdata = test, type = "class") # return predicted class
</code></pre>

<hr>
<h2 id='rus'>Implementation of RUSBoost</h2><span id='topic+rus'></span>

<h3>Description</h3>

<p>The function implements RUSBoost for binary classification. It returns a list of weak learners that are built on random under-sampled training-sets, and a vector of error estimations of each weak learner. The weak learners altogether consist the ensemble model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rus(formula, data, size, alg, ir = 1, rf.ntree = 50, svm.ker = "radial")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="rus_+3A_formula">formula</code></td>
<td>
<p>A formula specify predictors and target variable. Target variable should be a factor of 0 and 1. Predictors can be either numerical and categorical.</p>
</td></tr>
<tr><td><code id="rus_+3A_data">data</code></td>
<td>
<p>A data frame used for training the model, i.e. training set.</p>
</td></tr>
<tr><td><code id="rus_+3A_size">size</code></td>
<td>
<p>Ensemble size, i.e. number of weak learners in the ensemble model.</p>
</td></tr>
<tr><td><code id="rus_+3A_alg">alg</code></td>
<td>
<p>The learning algorithm used to train weak learners in the ensemble model. <em>cart</em>, <em>c50</em>, <em>rf</em>, <em>nb</em>, and <em>svm</em> are available. Please see Details for more information.</p>
</td></tr>
<tr><td><code id="rus_+3A_ir">ir</code></td>
<td>
<p>Imbalance ratio. Specifying how many times the under-sampled majority instances are over minority instances. Interger is not required and so such as ir = 1.5 is allowed.</p>
</td></tr>
<tr><td><code id="rus_+3A_rf.ntree">rf.ntree</code></td>
<td>
<p>Number of decision trees in each forest of the ensemble model when using <em>rf</em> (Random Forest) as base learner. Integer is required.</p>
</td></tr>
<tr><td><code id="rus_+3A_svm.ker">svm.ker</code></td>
<td>
<p>Specifying kernel function when using svm as base algorithm. Four options are available: <b>linear</b>, <b>polynomial</b>, <b>radial</b>, and <b>sigmoid</b>. Default is radial. Equivalent to that in e1071::svm().</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Based on AdaBoost.M2, RUSBoost uses random under-sampling to reduce majority instances in each iteration of training weak learners. A 1:1 under-sampling ratio (i.e. equal numbers of majority and minority instances) is set as default.
</p>
<p>The function requires the target varible to be a factor of 0 and 1, where 1 indicates minority while 0 indicates majority instances. Only binary classification is implemented in this version.
</p>
<p>Argument <em>alg</em> specifies the learning algorithm used to train weak learners within the ensemble model. Totally five algorithms are implemented: <b>cart</b> (Classification and Regression Tree), <b>c50</b> (C5.0 Decision Tree), <b>rf</b> (Random Forest), <b>nb</b> (Naive Bayes), and <b>svm</b> (Support Vector Machine). When using Random Forest as base learner, the ensemble model is consisted of forests and each forest contains a number of trees.
</p>
<p><em>ir</em> refers to the intended imbalance ratio of training sets for manipulation. With ir = 1 (default), the numbers of majority and minority instances are equal after class rebalancing. With ir = 2, the number of majority instances is twice of that of minority instances. Interger is not required and so such as ir = 1.5 is allowed.
</p>
<p>The object class of returned list is defined as <em>modelBst</em>, which can be directly passed to predict() for predicting test instances.
</p>


<h3>Value</h3>

<p>The function returns a list containing two elements:
</p>
<table role = "presentation">
<tr><td><code>weakLearners</code></td>
<td>
<p>A list of weak learners.</p>
</td></tr>
<tr><td><code>errorEstimation</code></td>
<td>
<p>Error estimation of each weak learner. Calculated by using (pseudo_loss + smooth) / (1 - pseudo_loss + smooth). <em>smooth</em> helps prevent error rate = 0 resulted from perfect classfication during trainging iterations. For more information, please see Schapire et al. (1999) Section 4.2.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Seiffert,  C.,  Khoshgoftaar, T.,  Hulse,  J.,  and  Napolitano, A.  2010.  RUSBoost: A Hybrid Approach to Alleviating Class Imbalance. IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans. 40(1), pp. 185-197.
</p>
<p>Galar, M., Fernandez, A., Barrenechea, E., Bustince,  H., and Herrera, F. 2012. A Review  on  Ensembles  for  the  Class  Imbalance  Problem:  Bagging-,  Boosting-,  and Hybrid-Based Approaches. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews). 42(4), pp. 463-484.
</p>
<p>Freund, Y. and Schapire, R. 1997. A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting. Journal of Computer and System Sciences. 55, pp. 119-139.
</p>
<p>Freund, Y.  and  Schapire,  R.  1996.  Experiments  with  a  new  boosting  algorithm. Machine Learning: In Proceedings of the 13th International Conference. pp. 148-156
</p>
<p>Schapire, R. and Singer, Y. 1999. Improved Boosting Algorithms Using Confidence-rated Predictions. Machine Learning. 37(3). pp. 297-336.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("iris")
iris &lt;- iris[1:70, ]
iris$Species &lt;- factor(iris$Species, levels = c("setosa", "versicolor"), labels = c("0", "1"))
model1 &lt;- rus(Species ~ ., data = iris, size = 10, alg = "c50", ir = 1)
model2 &lt;- rus(Species ~ ., data = iris, size = 20, alg = "rf", ir = 1, rf.ntree = 100)
model3 &lt;- rus(Species ~ ., data = iris, size = 40, alg = "svm", ir = 1, svm.ker = "sigmoid")
</code></pre>

<hr>
<h2 id='sbag'>Implementation of SMOTEBagging</h2><span id='topic+sbag'></span>

<h3>Description</h3>

<p>The function implements SMOTEBagging for binary classification. It returns a list of weak learners that are built on training-sets manipulated by SMOTE and random over-sampling. They together consist the ensemble model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sbag(formula, data, size, alg, smote.k = 5, rf.ntree = 50, svm.ker = "radial")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sbag_+3A_formula">formula</code></td>
<td>
<p>A formula specify predictors and target variable. Target variable should be a factor of 0 and 1. Predictors can be either numerical and categorical.</p>
</td></tr>
<tr><td><code id="sbag_+3A_data">data</code></td>
<td>
<p>A data frame used for training the model, i.e. training set.</p>
</td></tr>
<tr><td><code id="sbag_+3A_size">size</code></td>
<td>
<p>Ensemble size, i.e. number of weak learners in the ensemble model.</p>
</td></tr>
<tr><td><code id="sbag_+3A_alg">alg</code></td>
<td>
<p>The learning algorithm used to train weak learners in the ensemble model. <em>cart</em>, <em>c50</em>, <em>rf</em>, <em>nb</em>, and <em>svm</em> are available. Please see Details for more information.</p>
</td></tr>
<tr><td><code id="sbag_+3A_smote.k">smote.k</code></td>
<td>
<p>Number of k applied in SMOTE algorithm. Default is 5.</p>
</td></tr>
<tr><td><code id="sbag_+3A_rf.ntree">rf.ntree</code></td>
<td>
<p>Number of decision trees in each forest of the ensemble model when using <em>rf</em> (Random Forest) as base learner. Integer is required.</p>
</td></tr>
<tr><td><code id="sbag_+3A_svm.ker">svm.ker</code></td>
<td>
<p>Specifying kernel function when using svm as base algorithm. Four options are available: <b>linear</b>, <b>polynomial</b>, <b>radial</b>, and <b>sigmoid</b>. Default is radial. Equivalent to that in e1071::svm().</p>
</td></tr>
</table>


<h3>Details</h3>

<p>SMOTEBagging uses both SMOTE (Synthetic Minority Over-sampling TEchnique) and random over-sampling to increase minority instances in each bag of Bagging in order to rebalance class distribution. The manipulated training sets contain equal numbers of majority and minority instances, but the proportions of minority instances from SMOTE and random over-sampling vary for different bags, determined by an assigned re-sampling rate <em>a</em>. The re-sampling rate <em>a</em> is always the multiple of 10, and the function automatically generates a vector of <em>a</em>, therefore users do not need to self-define.
</p>
<p>The function requires the target varible to be a factor of 0 and 1, where 1 indicates minority while 0 indicates majority instances. Only binary classification is implemented in this version.
</p>
<p>Argument <em>alg</em> specifies the learning algorithm used to train weak learners within the ensemble model. Totally five algorithms are implemented: <b>cart</b> (Classification and Regression Tree), <b>c50</b> (C5.0 Decision Tree), <b>rf</b> (Random Forest), <b>nb</b> (Naive Bayes), and <b>svm</b> (Support Vector Machine). When using Random Forest as base learner, the ensemble model is consisted of forests and each forest contains a number of trees.
</p>
<p>The object class of returned list is defined as <em>modelBag</em>, which can be directly passed to predict() for predicting test instances.
</p>


<h3>References</h3>

<p>Wang, S. and Yao, X. 2009. Diversity Analysis on Imbalanced Data Sets by Using Ensemble Models. IEEE Symposium on Computational Intelligence and Data Mining, CIDM '09.
</p>
<p>Galar, M., Fernandez, A., Barrenechea, E., Bustince,  H., and Herrera, F. 2012. A Review  on  Ensembles  for  the  Class  Imbalance  Problem:  Bagging-,  Boosting-,  and Hybrid-Based Approaches. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews). 42(4), pp. 463-484.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("iris")
iris &lt;- iris[1:70, ]
iris$Species &lt;- factor(iris$Species, levels = c("setosa", "versicolor"), labels = c("0", "1"))
model1 &lt;- sbag(Species ~ ., data = iris, size = 10, alg = "c50")
model2 &lt;- sbag(Species ~ ., data = iris, size = 20, alg = "rf", rf.ntree = 100)
model3 &lt;- sbag(Species ~ ., data = iris, size = 40, alg = "svm", svm.ker = "sigmoid")
</code></pre>

<hr>
<h2 id='sbo'>Implementation of SMOTEBoost</h2><span id='topic+sbo'></span>

<h3>Description</h3>

<p>The function implements SMOTEBoost for binary classification. It returns a list of weak learners that are built on SMOTE-manipulated training-sets, and a vector of error estimations of each weak learner. The weak learners altogether consist the ensemble model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sbo(formula, data, size, alg, over = 100, smote.k = 5, rf.ntree = 50, svm.ker = "radial")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sbo_+3A_formula">formula</code></td>
<td>
<p>A formula specify predictors and target variable. Target variable should be a factor of 0 and 1. Predictors can be either numerical and categorical.</p>
</td></tr>
<tr><td><code id="sbo_+3A_data">data</code></td>
<td>
<p>A data frame used for training the model, i.e. training set.</p>
</td></tr>
<tr><td><code id="sbo_+3A_size">size</code></td>
<td>
<p>Ensemble size, i.e. number of weak learners in the ensemble model.</p>
</td></tr>
<tr><td><code id="sbo_+3A_alg">alg</code></td>
<td>
<p>The learning algorithm used to train weak learners in the ensemble model. <em>cart</em>, <em>c50</em>, <em>rf</em>, <em>nb</em>, and <em>svm</em> are available. Please see Details for more information.</p>
</td></tr>
<tr><td><code id="sbo_+3A_over">over</code></td>
<td>
<p>Specifying over-sampling rate of SMOTE. Only multiple of 100 is acceptable.</p>
</td></tr>
<tr><td><code id="sbo_+3A_smote.k">smote.k</code></td>
<td>
<p>Number of k applied in SMOTE algorithm. Default is 5.</p>
</td></tr>
<tr><td><code id="sbo_+3A_rf.ntree">rf.ntree</code></td>
<td>
<p>Number of decision trees in each forest of the ensemble model when using <em>rf</em> (Random Forest) as base learner. Integer is required.</p>
</td></tr>
<tr><td><code id="sbo_+3A_svm.ker">svm.ker</code></td>
<td>
<p>Specifying kernel function when using svm as base algorithm. Four options are available: <b>linear</b>, <b>polynomial</b>, <b>radial</b>, and <b>sigmoid</b>. Default is radial. Equivalent to that in e1071::svm().</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Based on AdaBoost.M2, SMOTEBoost uses SMOTE (Synthetic Minority Over-sampling TEchnique) to increase minority instances in each iteration of training weak learners. An over-sampling rate of SMOTE can be defined by users with argument <em>over</em>.
</p>
<p>The function requires the target varible to be a factor of 0 and 1, where 1 indicates minority while 0 indicates majority instances. Only binary classification is implemented in this version.
</p>
<p>Argument <em>alg</em> specifies the learning algorithm used to train weak learners within the ensemble model. Totally five algorithms are implemented: <b>cart</b> (Classification and Regression Tree), <b>c50</b> (C5.0 Decision Tree), <b>rf</b> (Random Forest), <b>nb</b> (Naive Bayes), and <b>svm</b> (Support Vector Machine). When using Random Forest as base learner, the ensemble model is consisted of forests and each forest contains a number of trees.
</p>
<p>The object class of returned list is defined as <em>modelBst</em>, which can be directly passed to predict() for predicting test instances.
</p>


<h3>Value</h3>

<p>The function returns a list containing two elements:
</p>
<table role = "presentation">
<tr><td><code>weakLearners</code></td>
<td>
<p>A list of weak learners.</p>
</td></tr>
<tr><td><code>errorEstimation</code></td>
<td>
<p>Error estimation of each weak learner. Calculated by using (pseudo_loss + smooth) / (1 - pseudo_loss + smooth). <em>smooth</em> helps prevent error rate = 0 resulted from perfect classfication during trainging iterations. For more information, please see Schapire et al. (1999) Section 4.2.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Chawla, N., Lazarevic, A., Hall, L., and Bowyer, K. 2003. SMOTEBoost: Improving Prediction of the Minority Class in Boosting. In Proceedings European Conference on Principles of Data Mining and Knowledge Discovery. pp. 107-119
</p>
<p>Galar, M., Fernandez, A., Barrenechea, E., Bustince,  H., and Herrera, F. 2012. A Review  on  Ensembles  for  the  Class  Imbalance  Problem:  Bagging-,  Boosting-,  and Hybrid-Based Approaches. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews). 42(4), pp. 463-484.
</p>
<p>Freund, Y. and Schapire, R. 1997. A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting. Journal of Computer and System Sciences. 55, pp. 119-139.
</p>
<p>Freund, Y.  and  Schapire,  R.  1996.  Experiments  with  a  new  boosting  algorithm. Machine Learning: In Proceedings of the 13th International Conference. pp. 148-156
</p>
<p>Schapire, R. and Singer, Y. 1999. Improved Boosting Algorithms Using Confidence-rated Predictions. Machine Learning. 37(3). pp. 297-336.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("iris")
iris &lt;- iris[1:70, ]
iris$Species &lt;- factor(iris$Species, levels = c("setosa", "versicolor"), labels = c("0", "1"))
model1 &lt;- sbo(Species ~ ., data = iris, size = 10, over = 100, alg = "c50")
model2 &lt;- sbo(Species ~ ., data = iris, size = 20, over = 200, alg = "rf", rf.ntree = 100)
model3 &lt;- sbo(Species ~ ., data = iris, size = 40, over = 300, alg = "svm", svm.ker = "sigmoid")
</code></pre>

<hr>
<h2 id='ub'>Implementation of UnderBagging</h2><span id='topic+ub'></span>

<h3>Description</h3>

<p>The function implements UnderBagging for binary classification. It returns a list of weak learners that are built on random under-sampled training-sets. They together consist the ensemble model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ub(formula, data, size, alg, ir = 1, rf.ntree = 50, svm.ker = "radial")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ub_+3A_formula">formula</code></td>
<td>
<p>A formula specify predictors and target variable. Target variable should be a factor of 0 and 1. Predictors can be either numerical and categorical.</p>
</td></tr>
<tr><td><code id="ub_+3A_data">data</code></td>
<td>
<p>A data frame used for training the model, i.e. training set.</p>
</td></tr>
<tr><td><code id="ub_+3A_size">size</code></td>
<td>
<p>Ensemble size, i.e. number of weak learners in the ensemble model.</p>
</td></tr>
<tr><td><code id="ub_+3A_alg">alg</code></td>
<td>
<p>The learning algorithm used to train weak learners in the ensemble model. <em>cart</em>, <em>c50</em>, <em>rf</em>, <em>nb</em>, and <em>svm</em> are available. Please see Details for more information.</p>
</td></tr>
<tr><td><code id="ub_+3A_ir">ir</code></td>
<td>
<p>Imbalance ratio. Specifying how many times the under-sampled majority instances are over minority instances. Interger is not required and so such as ir = 1.5 is allowed.</p>
</td></tr>
<tr><td><code id="ub_+3A_rf.ntree">rf.ntree</code></td>
<td>
<p>Number of decision trees in each forest of the ensemble model when using <em>rf</em> (Random Forest) as base learner. Integer is required.</p>
</td></tr>
<tr><td><code id="ub_+3A_svm.ker">svm.ker</code></td>
<td>
<p>Specifying kernel function when using svm as base algorithm. Four options are available: <b>linear</b>, <b>polynomial</b>, <b>radial</b>, and <b>sigmoid</b>. Default is radial. Equivalent to that in e1071::svm().</p>
</td></tr>
</table>


<h3>Details</h3>

<p>UnderBagging uses random under-sampling to reduce majority instances in each bag of Bagging in order to rebalance class distribution. A 1:1 under-sampling ratio (i.e. equal numbers of majority and minority instances) is set as default.
</p>
<p>The function requires the target varible to be a factor of 0 and 1, where 1 indicates minority while 0 indicates majority instances. Only binary classification is implemented in this version.
</p>
<p>Argument <em>alg</em> specifies the learning algorithm used to train weak learners within the ensemble model. Totally five algorithms are implemented: <b>cart</b> (Classification and Regression Tree), <b>c50</b> (C5.0 Decision Tree), <b>rf</b> (Random Forest), <b>nb</b> (Naive Bayes), and <b>svm</b> (Support Vector Machine). When using Random Forest as base learner, the ensemble model is consisted of forests and each forest contains a number of trees.
</p>
<p><em>ir</em> refers to the intended imbalance ratio of training sets for manipulation. With ir = 1 (default), the numbers of majority and minority instances are equal after class rebalancing. With ir = 2, the number of majority instances is twice of that of minority instances. Interger is not required and so such as ir = 1.5 is allowed.
</p>
<p>The object class of returned list is defined as <em>modelBag</em>, which can be directly passed to predict() for predicting test instances.
</p>


<h3>References</h3>

<p>Barandela,  R.,  Sanchez,  J.,  and  Valdovinos,  R.  2003.  New  Applications  of Ensembles of Classifiers. Pattern Analysis and Applications. 6(3), pp. 245-256.
</p>
<p>Galar, M., Fernandez, A., Barrenechea, E., Bustince,  H., and Herrera, F. 2012. A Review  on  Ensembles  for  the  Class  Imbalance  Problem:  Bagging-,  Boosting-,  and Hybrid-Based Approaches. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews). 42(4), pp. 463-484.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("iris")
iris &lt;- iris[1:70, ]
iris$Species &lt;- factor(iris$Species, levels = c("setosa", "versicolor"), labels = c("0", "1"))
model1 &lt;- ub(Species ~ ., data = iris, size = 10, alg = "c50", ir = 1)
model2 &lt;- ub(Species ~ ., data = iris, size = 20, alg = "rf", ir = 1, rf.ntree = 100)
model3 &lt;- ub(Species ~ ., data = iris, size = 40, alg = "svm", ir = 1, svm.ker = "sigmoid")
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
