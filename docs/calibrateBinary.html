<!DOCTYPE html><html lang="en"><head><title>Help for package calibrateBinary</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {calibrateBinary}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#calibrateBinary'><p>Calibration for Binary Outputs</p></a></li>
<li><a href='#cv.KLR'><p>K-fold cross-validation for Kernel Logistic Regression</p></a></li>
<li><a href='#KLR'><p>Kernel Logistic Regression</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Calibration for Computer Experiments with Binary Responses</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1</td>
</tr>
<tr>
<td>Date:</td>
<td>2018-07-16</td>
</tr>
<tr>
<td>Author:</td>
<td>Chih-Li Sung</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Chih-Li Sung &lt;iamdfchile@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Performs the calibration procedure proposed by Sung et al. (2018+) &lt;<a href="https://doi.org/10.48550/arXiv.1806.01453">doi:10.48550/arXiv.1806.01453</a>&gt;. This calibration method is particularly useful when the outputs of both computer and physical experiments are binary and the estimation for the calibration parameters is of interest.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>5.0.1</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.14.1)</td>
</tr>
<tr>
<td>Imports:</td>
<td>GPfit, gelnet, kernlab, randtoolbox</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2018-07-16 05:40:08 UTC; apple</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2018-07-20 15:00:07 UTC</td>
</tr>
</table>
<hr>
<h2 id='calibrateBinary'>Calibration for Binary Outputs</h2><span id='topic+calibrateBinary'></span>

<h3>Description</h3>

<p>The function performs the L2 calibration method for binary outputs.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calibrateBinary(Xp, yp, Xs1, Xs2, ys, K = 5, lambda = seq(0.001, 0.1,
  0.005), kernel = c("matern", "exponential")[1], nu = 1.5, power = 1.95,
  rho = seq(0.05, 0.5, 0.05), sigma = seq(100, 20, -1), lower, upper,
  verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="calibrateBinary_+3A_xp">Xp</code></td>
<td>
<p>a design matrix with dimension <code>np</code> by <code>d</code>.</p>
</td></tr>
<tr><td><code id="calibrateBinary_+3A_yp">yp</code></td>
<td>
<p>a response vector with length <code>np</code>. The values in the vector are 0 or 1.</p>
</td></tr>
<tr><td><code id="calibrateBinary_+3A_xs1">Xs1</code></td>
<td>
<p>a design matrix with dimension <code>ns</code> by <code>d</code>. These columns should one-by-one correspond to the columns of <code>Xp</code>.</p>
</td></tr>
<tr><td><code id="calibrateBinary_+3A_xs2">Xs2</code></td>
<td>
<p>a design matrix with dimension <code>ns</code> by <code>q</code>.</p>
</td></tr>
<tr><td><code id="calibrateBinary_+3A_ys">ys</code></td>
<td>
<p>a response vector with length <code>ns</code>. The values in the vector are 0 or 1.</p>
</td></tr>
<tr><td><code id="calibrateBinary_+3A_k">K</code></td>
<td>
<p>a positive integer specifying the number of folds for fitting kernel logistic regression and generalized Gaussian process. The default is 5.</p>
</td></tr>
<tr><td><code id="calibrateBinary_+3A_lambda">lambda</code></td>
<td>
<p>a vector specifying lambda values at which CV curve will be computed for fitting kernel logistic regression. See <code><a href="#topic+cv.KLR">cv.KLR</a></code>.</p>
</td></tr>
<tr><td><code id="calibrateBinary_+3A_kernel">kernel</code></td>
<td>
<p>input for fitting kernel logistic regression. See <code><a href="#topic+KLR">KLR</a></code>.</p>
</td></tr>
<tr><td><code id="calibrateBinary_+3A_nu">nu</code></td>
<td>
<p>input for fitting kernel logistic regression. See <code><a href="#topic+KLR">KLR</a></code>.</p>
</td></tr>
<tr><td><code id="calibrateBinary_+3A_power">power</code></td>
<td>
<p>input for fitting kernel logistic regression. See <code><a href="#topic+KLR">KLR</a></code>.</p>
</td></tr>
<tr><td><code id="calibrateBinary_+3A_rho">rho</code></td>
<td>
<p><code>rho</code> value at which CV curve will be computed for fitting kernel logistic regression. See <code><a href="#topic+KLR">KLR</a></code>.</p>
</td></tr>
<tr><td><code id="calibrateBinary_+3A_sigma">sigma</code></td>
<td>
<p>a vector specifying values of the tuning parameter <code class="reqn">\sigma</code> at which CV curve will be computed for fitting generalized Gaussian process. See Details.</p>
</td></tr>
<tr><td><code id="calibrateBinary_+3A_lower">lower</code></td>
<td>
<p>a vector of size <code>p+q</code> specifying lower bounds of the input space for <code>rbind(Xp,Xs1)</code> and <code>Xs2</code>.</p>
</td></tr>
<tr><td><code id="calibrateBinary_+3A_upper">upper</code></td>
<td>
<p>a vector of size <code>p+q</code> specifying upper bounds of the input space for <code>rbind(Xp,Xs1)</code> and <code>Xs2</code>.</p>
</td></tr>
<tr><td><code id="calibrateBinary_+3A_verbose">verbose</code></td>
<td>
<p>logical. If <code>TRUE</code>, additional diagnostics are printed. The default is <code>TRUE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function performs the L2 calibration method for computer experiments with binary outputs. The input and ouput of physical data are assigned to <code>Xp</code> and <code>yp</code>, and the input and output of computer data are assigned to <code>cbind(Xs1,Xs2)</code> and <code>ys</code>. Note here we separate the input of computer data by <code>Xs1</code> and <code>Xs2</code>, where <code>Xs1</code> is the shared input with <code>Xp</code> and <code>Xs2</code> is the calibration input. The idea of L2 calibration is to find the calibration parameter that minimizes the discrepancy measured by the L2 distance between the underlying probability functions in the physical and computer data. That is, </p>
<p style="text-align: center;"><code class="reqn">\hat{\theta}=\arg\min_{\theta}\|\hat{\eta}(\cdot)-\hat{p}(\cdot,\theta)\|_{L_2(\Omega)},</code>
</p>
<p> where <code class="reqn">\hat{\eta}(x)</code> is the fitted probability function for physical data, and <code class="reqn">\hat{p}(x,\theta)</code> is the fitted probability function for computer data. In this L2 calibration framework, <code class="reqn">\hat{\eta}(x)</code> is fitted by the kernel logistic regression using the input <code>Xp</code> and the output <code>yp</code>. The tuning parameter <code class="reqn">\lambda</code> for the kernel logistic regression can be chosen by k-fold cross-validation, where k is assigned by <code>K</code>. The choices of the tuning parameter are given by the vector <code>lambda</code>. The kernel function for the kernel logistic regression can be given by <code>kernel</code>, where Matern kernel or power exponential kernel can be chosen. The arguments <code>power</code>, <code>nu</code>, <code>rho</code> are the tuning parameters in the kernel functions. See <code><a href="#topic+KLR">KLR</a></code>. For computer data, the probability function <code class="reqn">\hat{p}(x,\theta)</code> is fitted by the Bayesian Gaussian process in Williams and Barber (1998) using the input <code>cbind(Xs1,Xs2)</code> and the output <code>ys</code>, where the Gaussian correlation function, </p>
<p style="text-align: center;"><code class="reqn">R_{\sigma}(\mathbf{x}_i,\mathbf{x}_j)=\exp\{-\sum^{d}_{l=1}\sigma(x_{il}-x_{jl})^2 \},</code>
</p>
<p> is used here. The vector <code>sigma</code> is the choices of the tuning parameter <code class="reqn">\sigma</code>, and it will be chosen by k-fold cross-validation. More details can be seen in Sung et al. (unpublished). The arguments <code>lower</code> and <code>upper</code> are lower and upper bounds of the input space, which will be used in scaling the inputs and optimization for <code class="reqn">\theta</code>. If they are not given, the default is the range of each column of <code>rbind(Xp,Xs1)</code>, and <code>Xs2</code>.
</p>


<h3>Value</h3>

<p>a matrix with number of columns <code>q+1</code>. The first <code>q</code> columns are the local (the first row is the global) minimal solutions which are the potential estimates of calibration parameters, and the <code>(q+1)</code>-th column is the corresponding L2 distance.
</p>


<h3>Author(s)</h3>

<p>Chih-Li Sung &lt;iamdfchile@gmail.com&gt;
</p>


<h3>See Also</h3>

<p><code><a href="#topic+KLR">KLR</a></code> for performing a kernel logistic regression with given <code>lambda</code> and <code>rho</code>. <code><a href="#topic+cv.KLR">cv.KLR</a></code> for performing cross-validation to estimate the tuning parameters.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(calibrateBinary)

set.seed(1)
#####   data from physical experiment   #####
np &lt;- 10
xp &lt;- seq(0,1,length.out = np)
eta_fun &lt;- function(x) exp(exp(-0.5*x)*cos(3.5*pi*x)-1) # true probability function
eta_x &lt;- eta_fun(xp)
yp &lt;- rep(0,np)
for(i in 1:np) yp[i] &lt;- rbinom(1,1, eta_x[i])

#####   data from computer experiment   #####
ns &lt;- 20
xs &lt;- matrix(runif(ns*2), ncol=2)  # the first column corresponds to the column of xp
p_xtheta &lt;- function(x,theta) {
     # true probability function
     exp(exp(-0.5*x)*cos(3.5*pi*x)-1) - abs(theta-0.3) *exp(-0.5*x)*cos(3.5*pi*x)
}
ys &lt;- rep(0,ns)
for(i in 1:ns) ys[i] &lt;- rbinom(1,1, p_xtheta(xs[i,1],xs[i,2]))

#####    check the true parameter    #####
curve(eta_fun, lwd=2, lty=2, from=0, to=1)
curve(p_xtheta(x,0.3), add=TRUE, col=4)   # true value = 0.3: L2 dist = 0
curve(p_xtheta(x,0.9), add=TRUE, col=3)   # other value

##### calibration: true parameter is 0.3 #####

calibrate.result &lt;- calibrateBinary(xp, yp, xs[,1], xs[,2], ys)
print(calibrate.result)

</code></pre>

<hr>
<h2 id='cv.KLR'>K-fold cross-validation for Kernel Logistic Regression</h2><span id='topic+cv.KLR'></span>

<h3>Description</h3>

<p>The function performs k-fold cross validation for kernel logistic regression to estimate tuning parameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cv.KLR(X, y, K = 5, lambda = seq(0.001, 0.2, 0.005), kernel = c("matern",
  "exponential")[1], nu = 1.5, power = 1.95, rho = seq(0.05, 0.5, 0.05))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cv.KLR_+3A_x">X</code></td>
<td>
<p>input for <code>KLR</code>.</p>
</td></tr>
<tr><td><code id="cv.KLR_+3A_y">y</code></td>
<td>
<p>input for <code>KLR</code>.</p>
</td></tr>
<tr><td><code id="cv.KLR_+3A_k">K</code></td>
<td>
<p>a positive integer specifying the number of folds. The default is 5.</p>
</td></tr>
<tr><td><code id="cv.KLR_+3A_lambda">lambda</code></td>
<td>
<p>a vector specifying lambda values at which CV curve will be computed.</p>
</td></tr>
<tr><td><code id="cv.KLR_+3A_kernel">kernel</code></td>
<td>
<p>input for <code>KLR</code>.</p>
</td></tr>
<tr><td><code id="cv.KLR_+3A_nu">nu</code></td>
<td>
<p>input for <code>KLR</code>.</p>
</td></tr>
<tr><td><code id="cv.KLR_+3A_power">power</code></td>
<td>
<p>input for <code>KLR</code>.</p>
</td></tr>
<tr><td><code id="cv.KLR_+3A_rho">rho</code></td>
<td>
<p>rho value at which CV curve will be computed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function performs the k-fold cross-valibration for a kernel logistic regression. The CV curve is computed at the values of the tuning parameters assigned by <code>lambda</code> and <code>rho</code>. The number of fold is given by <code>K</code>.
</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>lambda</code></td>
<td>
<p>value of <code>lambda</code> that gives minimum CV error.</p>
</td></tr>
<tr><td><code>rho</code></td>
<td>
<p>value of <code>rho</code> that gives minimum CV error.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Chih-Li Sung &lt;iamdfchile@gmail.com&gt;
</p>


<h3>See Also</h3>

<p><code><a href="#topic+KLR">KLR</a></code> for performing a kernel logistic regression with given <code>lambda</code> and <code>rho</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(calibrateBinary)

set.seed(1)
np &lt;- 10
xp &lt;- seq(0,1,length.out = np)
eta_fun &lt;- function(x) exp(exp(-0.5*x)*cos(3.5*pi*x)-1) # true probability function
eta_x &lt;- eta_fun(xp)
yp &lt;- rep(0,np)
for(i in 1:np) yp[i] &lt;- rbinom(1,1, eta_x[i])

x.test &lt;- seq(0,1,0.001)
etahat &lt;- KLR(xp,yp,x.test)

plot(xp,yp)
curve(eta_fun, col = "blue", lty = 2, add = TRUE)
lines(x.test, etahat, col = 2)

#####   cross-validation with K=5    #####
##### to determine the parameter rho #####

cv.out &lt;- cv.KLR(xp,yp,K=5)
print(cv.out)

etahat.cv &lt;- KLR(xp,yp,x.test,lambda=cv.out$lambda,rho=cv.out$rho)

plot(xp,yp)
curve(eta_fun, col = "blue", lty = 2, add = TRUE)
lines(x.test, etahat, col = 2)
lines(x.test, etahat.cv, col = 3)

</code></pre>

<hr>
<h2 id='KLR'>Kernel Logistic Regression</h2><span id='topic+KLR'></span>

<h3>Description</h3>

<p>The function performs a kernel logistic regression for binary outputs.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>KLR(X, y, xnew, lambda = 0.01, kernel = c("matern", "exponential")[1],
  nu = 1.5, power = 1.95, rho = 0.1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="KLR_+3A_x">X</code></td>
<td>
<p>a design matrix with dimension <code>n</code> by <code>d</code>.</p>
</td></tr>
<tr><td><code id="KLR_+3A_y">y</code></td>
<td>
<p>a response vector with length <code>n</code>. The values in the vector are 0 or 1.</p>
</td></tr>
<tr><td><code id="KLR_+3A_xnew">xnew</code></td>
<td>
<p>a testing matrix with dimension <code>n_new</code> by <code>d</code> in which each row corresponds to a predictive location.</p>
</td></tr>
<tr><td><code id="KLR_+3A_lambda">lambda</code></td>
<td>
<p>a positive value specifing the tuning parameter for KLR. The default is 0.01.</p>
</td></tr>
<tr><td><code id="KLR_+3A_kernel">kernel</code></td>
<td>
<p>&quot;matern&quot; or &quot;exponential&quot; which specifies the matern kernel or power exponential kernel. The default is &quot;matern&quot;.</p>
</td></tr>
<tr><td><code id="KLR_+3A_nu">nu</code></td>
<td>
<p>a positive value specifying the order of matern kernel if <code>kernel</code> == &quot;matern&quot;. The default is 1.5 if matern kernel is chosen.</p>
</td></tr>
<tr><td><code id="KLR_+3A_power">power</code></td>
<td>
<p>a positive value (between 1.0 and 2.0) specifying the power of power exponential kernel if <code>kernel</code> == &quot;exponential&quot;. The default is 1.95 if power exponential kernel is chosen.</p>
</td></tr>
<tr><td><code id="KLR_+3A_rho">rho</code></td>
<td>
<p>a positive value specifying the scale parameter of matern and power exponential kernels. The default is 0.1.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function performs a kernel logistic regression, where the kernel can be assigned to Matern kernel or power exponential kernel by the argument <code>kernel</code>. The arguments <code>power</code> and <code>rho</code> are the tuning parameters in the power exponential kernel function, and <code>nu</code> and <code>rho</code> are the tuning parameters in the Matern kernel function. The power exponential kernel has the form </p>
<p style="text-align: center;"><code class="reqn">K_{ij}=\exp(-\frac{\sum_{k}{|x_{ik}-x_{jk}|^{power}}}{rho}),</code>
</p>
<p> and the Matern kernel has the form </p>
<p style="text-align: center;"><code class="reqn">K_{ij}=\prod_{k}\frac{1}{\Gamma(nu)2^{nu-1}}(2\sqrt{nu}\frac{|x_{ik}-x_{jk}|}{rho})^{nu} \kappa(2\sqrt{nu}\frac{|x_{ik}-x_{jk}|}{rho}).</code>
</p>
<p> The argument <code>lambda</code> is the tuning parameter for the function smoothness.
</p>


<h3>Value</h3>

<p>Predictive probabilities at given locations <code>xnew</code>.
</p>


<h3>Author(s)</h3>

<p>Chih-Li Sung &lt;iamdfchile@gmail.com&gt;
</p>


<h3>References</h3>

<p>Zhu, J. and Hastie, T. (2005). Kernel logistic regression and the import vector machine. Journal of Computational and Graphical Statistics, 14(1), 185-205.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cv.KLR">cv.KLR</a></code> for performing cross-validation to choose the tuning parameters.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(calibrateBinary)

set.seed(1)
np &lt;- 10
xp &lt;- seq(0,1,length.out = np)
eta_fun &lt;- function(x) exp(exp(-0.5*x)*cos(3.5*pi*x)-1) # true probability function
eta_x &lt;- eta_fun(xp)
yp &lt;- rep(0,np)
for(i in 1:np) yp[i] &lt;- rbinom(1,1, eta_x[i])

x.test &lt;- seq(0,1,0.001)
etahat &lt;- KLR(xp,yp,x.test)

plot(xp,yp)
curve(eta_fun, col = "blue", lty = 2, add = TRUE)
lines(x.test, etahat, col = 2)

#####   cross-validation with K=5    #####
##### to determine the parameter rho #####

cv.out &lt;- cv.KLR(xp,yp,K=5)
print(cv.out)

etahat.cv &lt;- KLR(xp,yp,x.test,lambda=cv.out$lambda,rho=cv.out$rho)

plot(xp,yp)
curve(eta_fun, col = "blue", lty = 2, add = TRUE)
lines(x.test, etahat, col = 2)
lines(x.test, etahat.cv, col = 3)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
