<!DOCTYPE html><html><head><title>Help for package selectiveInference</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {selectiveInference}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#debiasingMatrix'>
<p>Find an approximate inverse of a non-negative definite matrix.</p></a></li>
<li><a href='#estimateSigma'>
<p>Estimate the noise standard deviation in regression</p></a></li>
<li><a href='#factorDesign'><p>Expand a data frame with factors to form a design matrix with the full binary encoding of each factor.</p></a></li>
<li><a href='#fixedLassoInf'>
<p>Inference for the lasso, with a fixed lambda</p></a></li>
<li><a href='#forwardStop'>
<p>ForwardStop rule for sequential p-values</p></a></li>
<li><a href='#fs'>
<p>Forward stepwise regression</p></a></li>
<li><a href='#fsInf'>
<p>Selective inference for forward stepwise regression</p></a></li>
<li><a href='#groupfs'><p>Select a model with forward stepwise.</p></a></li>
<li><a href='#groupfsInf'><p>Compute selective p-values for a model fitted by <code>groupfs</code>.</p></a></li>
<li><a href='#lar'>
<p>Least angle regression</p></a></li>
<li><a href='#larInf'>
<p>Selective inference for least angle regression</p></a></li>
<li><a href='#manyMeans'>
<p>Selective inference for many normal means</p></a></li>
<li><a href='#plot.fs'>
<p>Plot function for forward stepwise regression</p></a></li>
<li><a href='#plot.lar'>
<p>Plot function for least angle regression</p></a></li>
<li><a href='#predict.fs'>
<p>Prediction and coefficient functions for forward stepwise</p>
regression</a></li>
<li><a href='#predict.groupfs'><p>Prediction and coefficient functions for <code>groupfs</code>.</p></a></li>
<li><a href='#predict.lar'>
<p>Prediction and coefficient functions for least angle regression</p></a></li>
<li><a href='#randomizedLasso'>
<p>Inference for the randomized lasso, with a fixed lambda</p></a></li>
<li><a href='#randomizedLassoInf'>
<p>Inference for the randomized lasso, with a fixed lambda</p></a></li>
<li><a href='#ROSI'>
<p>Relevant One-step Selective Inference for the LASSO</p></a></li>
<li><a href='#scaleGroups'><p>Center and scale design matrix by groups</p></a></li>
<li><a href='#selectiveInference'>
<p>Tools for selective inference</p></a></li>
<li><a href='#selectiveInference-internal'><p>Internal PMA functions</p></a></li>
<li><a href='#TG.interval'>
<p>Truncated Gaussian confidence interval.</p></a></li>
<li><a href='#TG.limits'>
<p>Truncation limits and standard deviation.</p></a></li>
<li><a href='#TG.pvalue'>
<p>Truncated Gaussian p-value.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Tools for Post-Selection Inference</td>
</tr>
<tr>
<td>Version:</td>
<td>1.2.5</td>
</tr>
<tr>
<td>Date:</td>
<td>2019-09-04</td>
</tr>
<tr>
<td>Author:</td>
<td>Ryan Tibshirani, Rob Tibshirani, Jonathan Taylor,
    Joshua Loftus, Stephen Reid, Jelena Markovic</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Rob Tibshirani &lt;tibs@stanford.edu&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>glmnet, intervals, survival, adaptMCMC, MASS</td>
</tr>
<tr>
<td>Suggests:</td>
<td>Rmpfr</td>
</tr>
<tr>
<td>Description:</td>
<td>New tools for post-selection inference, for use with forward
    stepwise regression, least angle regression, the lasso, and the many means
    problem. The lasso function implements Gaussian, logistic and Cox survival
    models.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>5.0.1</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2019-09-05 23:44:03 UTC; jonathantaylor</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2019-09-07 07:00:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='debiasingMatrix'>
Find an approximate inverse of a non-negative definite matrix.
</h2><span id='topic+debiasingMatrix'></span>

<h3>Description</h3>

<p>Find some rows of an approximate inverse of a non-negative definite 
symmetric matrix by solving optimization problem described 
in Javanmard and Montanari (2013). Can be used for approximate 
Newton step from some consistent estimator (such as the LASSO)
to find a debiased solution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>debiasingMatrix(Xinfo, 
                is_wide,			
                nsample, 
                rows, 
		verbose=FALSE, 
		bound=NULL,            
   		linesearch=TRUE,    
   		scaling_factor=1.5, 
		max_active=NULL,    
		max_try=10,         
		warn_kkt=FALSE,     
		max_iter=50,       
		kkt_stop=TRUE,
		parameter_stop=TRUE,
		objective_stop=TRUE,
                kkt_tol=1.e-4,      
		parameter_tol=1.e-4,
		objective_tol=1.e-4)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="debiasingMatrix_+3A_xinfo">Xinfo</code></td>
<td>

<p>Either a non-negative definite matrix S=t(X) 
is_wide is TRUE, then Xinfo should be X, otherwise it should be S.
</p>
</td></tr>
<tr><td><code id="debiasingMatrix_+3A_is_wide">is_wide</code></td>
<td>

<p>Are we solving for rows of the debiasing matrix assuming it is 
a wide matrix so that Xinfo=X and the non-negative definite
matrix of interest is t(X) 
</p>
</td></tr>
<tr><td><code id="debiasingMatrix_+3A_nsample">nsample</code></td>
<td>

<p>Number of samples used in forming the cross-covariance matrix.
Used for default value of the bound parameter.
</p>
</td></tr>
<tr><td><code id="debiasingMatrix_+3A_rows">rows</code></td>
<td>

<p>Which rows of the approximate inverse to compute.
</p>
</td></tr>      
<tr><td><code id="debiasingMatrix_+3A_verbose">verbose</code></td>
<td>

<p>Print out progress as rows are being computed.
</p>
</td></tr> 
<tr><td><code id="debiasingMatrix_+3A_bound">bound</code></td>
<td>

<p>Initial bound parameter for each row. Will be changed
if linesearch is TRUE.
</p>
</td></tr> 
<tr><td><code id="debiasingMatrix_+3A_linesearch">linesearch</code></td>
<td>

<p>Run a line search to find as small as possible a bound parameter for each row?
</p>
</td></tr>
<tr><td><code id="debiasingMatrix_+3A_scaling_factor">scaling_factor</code></td>
<td>

<p>In the linesearch, the bound parameter is either multiplied or divided by this
factor at each step.
</p>
</td></tr>
<tr><td><code id="debiasingMatrix_+3A_max_active">max_active</code></td>
<td>

<p>How large an active set to consider in solving the problem with coordinate descent.
Defaults to max(50, 0.3*nsample).
</p>
</td></tr>
<tr><td><code id="debiasingMatrix_+3A_max_try">max_try</code></td>
<td>

<p>How many tries in the linesearch.
</p>
</td></tr>
<tr><td><code id="debiasingMatrix_+3A_warn_kkt">warn_kkt</code></td>
<td>

<p>Warn if the problem does not seem to be feasible after running the coordinate
descent algorithm.
</p>
</td></tr>
<tr><td><code id="debiasingMatrix_+3A_max_iter">max_iter</code></td>
<td>

<p>How many full iterations to run of the coordinate descent for each
value of the bound parameter.
</p>
</td></tr>
<tr><td><code id="debiasingMatrix_+3A_kkt_stop">kkt_stop</code></td>
<td>

<p>If TRUE, check to stop coordinate descent when KKT conditions are approximately satisfied.
</p>
</td></tr>
<tr><td><code id="debiasingMatrix_+3A_parameter_stop">parameter_stop</code></td>
<td>

<p>If TRUE, check to stop coordinate descent based on relative convergence of parameter vector,
checked at geometrically spaced iterations 2^k.
</p>
</td></tr>
<tr><td><code id="debiasingMatrix_+3A_objective_stop">objective_stop</code></td>
<td>

<p>If TRUE, check to stop coordinate descent based on relative decrease of objective value,
checked at geometrically spaced iterations 2^k.
</p>
</td></tr>
<tr><td><code id="debiasingMatrix_+3A_kkt_tol">kkt_tol</code></td>
<td>

<p>Tolerance value for assessing whether KKT conditions for solving the
dual problem and feasibility of the original problem.
</p>
</td></tr>
<tr><td><code id="debiasingMatrix_+3A_parameter_tol">parameter_tol</code></td>
<td>

<p>Tolerance value for assessing convergence of the problem using relative
convergence of the parameter.
</p>
</td></tr>
<tr><td><code id="debiasingMatrix_+3A_objective_tol">objective_tol</code></td>
<td>

<p>Tolerance value for assessing convergence of the problem using relative
decrease of the objective.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function computes an approximate inverse
as described in Javanmard and Montanari (2013), specifically
display (4). The problem is solved by considering a dual
problem which has an objective similar to a LASSO problem and is solvable
by coordinate descent. For some values of bound the original
problem may not be feasible, in which case the dual problem has no solution. 
An attempt to detect this is made by stopping when the active set grows quite
large, determined by max_active.
</p>


<h3>Value</h3>

  
<table>
<tr><td><code>M</code></td>
<td>
<p>Rows of approximate inverse of Sigma.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Ryan Tibshirani, Rob Tibshirani, Jonathan Taylor, Joshua Loftus, Stephen Reid</p>


<h3>References</h3>

<p>Adel Javanmard and Andrea Montanari (2013). 
Confidence Intervals and Hypothesis Testing for High-Dimensional Regression. Arxiv: 1306.3171
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
set.seed(10)
n = 50
p = 100
X = matrix(rnorm(n * p), n, p)
S = t(X) %*% X / n
M = debiasingMatrix(S, FALSE, n, c(1,3,5))
M2 = debiasingMatrix(X, TRUE, n, c(1,3,5))
max(M - M2)
</code></pre>

<hr>
<h2 id='estimateSigma'>
Estimate the noise standard deviation in regression 
</h2><span id='topic+estimateSigma'></span>

<h3>Description</h3>

<p>Estimates the standard deviation of the noise, for use in the selectiveInference 
package
</p>


<h3>Usage</h3>

<pre><code class='language-R'>estimateSigma(x, y, intercept=TRUE, standardize=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="estimateSigma_+3A_x">x</code></td>
<td>

<p>Matrix of predictors (n by p)
</p>
</td></tr>
<tr><td><code id="estimateSigma_+3A_y">y</code></td>
<td>

<p>Vector of outcomes (length n)
</p>
</td></tr>
<tr><td><code id="estimateSigma_+3A_intercept">intercept</code></td>
<td>
<p>Should glmnet be run with an intercept? Default is TRUE</p>
</td></tr>
<tr><td><code id="estimateSigma_+3A_standardize">standardize</code></td>
<td>
<p>Should glmnet be run with standardized predictors? Default is TRUE</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function estimates the standard deviation of the noise, in a linear regresion setting.
A lasso regression is fit, using cross-validation to estimate the tuning parameter lambda.
With sample size n, yhat equal to the predicted values and df being the number of nonzero 
coefficients from the lasso fit, the estimate of sigma is <code>sqrt(sum((y-yhat)^2) / (n-df-1))</code>.
Important: if you are using glmnet to compute the lasso estimate, be sure to use the settings
for the &quot;intercept&quot; and &quot;standardize&quot; arguments in glmnet and estimateSigma. Same applies to fs 
or lar, where the argument for standardization is called &quot;normalize&quot;.
</p>


<h3>Value</h3>

<table>
<tr><td><code>sigmahat</code></td>
<td>
<p>The estimate of sigma</p>
</td></tr>
<tr><td><code>df</code></td>
<td>
<p>The degrees of freedom of lasso fit used</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Ryan Tibshirani, Rob Tibshirani, Jonathan Taylor, Joshua Loftus, Stephen Reid</p>


<h3>References</h3>

<p>Stephen Reid, Jerome Friedman, and Rob Tibshirani (2014). 
A study of error variance estimation in lasso regression. arXiv:1311.5274.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(33)
n = 50
p = 10
sigma = 1
x = matrix(rnorm(n*p),n,p)
beta = c(3,2,rep(0,p-2))
y = x%*%beta + sigma*rnorm(n)

# run forward stepwise
fsfit = fs(x,y)

# estimate sigma
sigmahat = estimateSigma(x,y)$sigmahat

# run sequential inference with estimated sigma
out = fsInf(fsfit,sigma=sigmahat)
out
</code></pre>

<hr>
<h2 id='factorDesign'>Expand a data frame with factors to form a design matrix with the full binary encoding of each factor.</h2><span id='topic+factorDesign'></span>

<h3>Description</h3>

<p>When using <code><a href="#topic+groupfs">groupfs</a></code> with factor variables call this function first to create a design matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>factorDesign(df)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="factorDesign_+3A_df">df</code></td>
<td>
<p>Data frame containing some columns which are <code>factors</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List containing
</p>

<dl>
<dt>x</dt><dd><p>Design matrix, the first columns contain any numeric variables from the original date frame.</p>
</dd>
<dt>index</dt><dd><p>Group membership indicator for expanded matrix.</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
fd = factorDesign(warpbreaks)
y = rnorm(nrow(fd$x))
fit = groupfs(fd$x, y, fd$index, maxsteps=2, intercept=F)
pvals = groupfsInf(fit)

## End(Not run)
</code></pre>

<hr>
<h2 id='fixedLassoInf'>
Inference for the lasso, with a fixed lambda 
</h2><span id='topic+fixedLassoInf'></span>

<h3>Description</h3>

<p>Compute p-values and confidence intervals for the lasso estimate, at a 
fixed value of the tuning parameter lambda
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fixedLassoInf(x, 
              y, 
              beta, 
              lambda, 
              family = c("gaussian", "binomial", "cox"),
              intercept=TRUE, 
              add.targets=NULL, 
              status=NULL, 
              sigma=NULL, 
              alpha=0.1,
              type=c("partial","full"), 
              tol.beta=1e-5, 
              tol.kkt=0.1,
              gridrange=c(-100,100), 
              bits=NULL, 
              verbose=FALSE, 
              linesearch.try=10) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fixedLassoInf_+3A_x">x</code></td>
<td>

<p>Matrix of predictors (n by p); 
</p>
</td></tr>
<tr><td><code id="fixedLassoInf_+3A_y">y</code></td>
<td>

<p>Vector of outcomes (length n)
</p>
</td></tr>
<tr><td><code id="fixedLassoInf_+3A_beta">beta</code></td>
<td>

<p>Estimated lasso coefficients (e.g., from glmnet). This is of length p
(so the intercept is not included as the first component).
</p>
<p>Be careful! This function uses the &quot;standard&quot; lasso objective
</p>
<p style="text-align: center;"><code class="reqn">
    1/2 \|y - x \beta\|_2^2 + \lambda \|\beta\|_1.
  </code>
</p>

<p>In contrast, glmnet multiplies the first term by a factor of 1/n.
So after running glmnet, to extract the beta corresponding to a value lambda, 
you need to use <code>beta = coef(obj, s=lambda/n)[-1]</code>,
where obj is the object returned by glmnet (and [-1] removes the intercept,
which glmnet always puts in the first component)
</p>
</td></tr>      
<tr><td><code id="fixedLassoInf_+3A_lambda">lambda</code></td>
<td>

<p>Value of lambda used to compute beta. See the above warning
</p>
</td></tr> 
<tr><td><code id="fixedLassoInf_+3A_family">family</code></td>
<td>
<p>Response type: &quot;gaussian&quot; (default), &quot;binomial&quot;, or 
&quot;cox&quot; (for censored survival data) </p>
</td></tr>
<tr><td><code id="fixedLassoInf_+3A_sigma">sigma</code></td>
<td>

<p>Estimate of error standard deviation. If NULL (default), this is estimated 
using the mean squared residual of the full least squares fit when n &gt;= 2p, and 
using the standard deviation of y when n &lt; 2p. In the latter case, the user 
should use <code><a href="#topic+estimateSigma">estimateSigma</a></code> function for a more accurate estimate.
Not used for family= &quot;binomial&quot;, or  &quot;cox&quot;
</p>
</td></tr>
<tr><td><code id="fixedLassoInf_+3A_alpha">alpha</code></td>
<td>

<p>Significance level for confidence intervals (target is miscoverage alpha/2 in each tail)
</p>
</td></tr>
<tr><td><code id="fixedLassoInf_+3A_intercept">intercept</code></td>
<td>

<p>Was the lasso problem solved (e.g., by glmnet) with an intercept in the model? 
Default is TRUE. Must be TRUE for &quot;binomial&quot; family. Not used for 'cox&quot; family, where no intercept is assumed.
</p>
</td></tr>
<tr><td><code id="fixedLassoInf_+3A_add.targets">add.targets</code></td>
<td>
<p>Optional vector of predictors to be included as targets of inference, regardless of whether or not they are selected by the lasso. Default is NULL.</p>
</td></tr>
<tr><td><code id="fixedLassoInf_+3A_status">status</code></td>
<td>
<p>Censoring status for Cox model; 1=failurem 0=censored</p>
</td></tr>
<tr><td><code id="fixedLassoInf_+3A_type">type</code></td>
<td>
<p>Contrast type for p-values and confidence intervals: default is
&quot;partial&quot;&mdash;meaning that the contrasts tested are the partial population 
regression coefficients, within the active set of predictors; the alternative is
&quot;full&quot;&mdash;meaning that the full population regression coefficients are tested.
The latter does not make sense when p &gt; n.</p>
</td></tr>
<tr><td><code id="fixedLassoInf_+3A_tol.beta">tol.beta</code></td>
<td>

<p>Tolerance for determining if a coefficient is zero
</p>
</td></tr>
<tr><td><code id="fixedLassoInf_+3A_tol.kkt">tol.kkt</code></td>
<td>

<p>Tolerance for determining if an entry of the subgradient is zero 
</p>
</td></tr>
<tr><td><code id="fixedLassoInf_+3A_gridrange">gridrange</code></td>
<td>

<p>Grid range for constructing confidence intervals, on the standardized scale
</p>
</td></tr>
<tr><td><code id="fixedLassoInf_+3A_bits">bits</code></td>
<td>

<p>Number of bits to be used for p-value and confidence interval calculations. Default is 
NULL, in which case standard floating point calculations are performed. When not NULL, 
multiple precision floating point calculations are performed with the specified number 
of bits, using the R package <code>Rmpfr</code> (if this package is not installed, then a 
warning is thrown, and standard floating point calculations are pursued).
Note: standard double precision uses 53 bits
so, e.g., a choice of 200 bits uses about 4 times double precision. The confidence
interval computation is sometimes numerically challenging, and the extra precision can be 
helpful (though computationally more costly). In particular, extra precision might be tried 
if the values in the output columns of <code>tailarea</code> differ noticeably from alpha/2.
</p>
</td></tr>
<tr><td><code id="fixedLassoInf_+3A_verbose">verbose</code></td>
<td>

<p>Print out progress along the way? Default is FALSE
</p>
</td></tr>
<tr><td><code id="fixedLassoInf_+3A_linesearch.try">linesearch.try</code></td>
<td>

<p>When running type=&quot;full&quot; (i.e. debiased LASSO) how many attempts in the line search?
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function computes selective p-values and confidence intervals for the lasso,
given a fixed value of the tuning parameter lambda. 
Three different response types are supported:  gaussian, binomial and Cox.
The confidence interval construction involves numerical search and can be fragile:
if the observed statistic is too close to either end of the truncation interval
(vlo and vup, see references), then one or possibly both endpoints of the interval of 
desired coverage cannot be computed, and default to +/- Inf. The output <code>tailarea</code> 
gives the achieved Gaussian tail areas for the reported intervals&mdash;these should be close 
to alpha/2, and can be used for error-checking purposes.
</p>
<p>Important!: Before running glmnet (or some other lasso-solver)  x should be centered, that is x &lt;- scale(X,TRUE,FALSE).
In addition, if standardization of the predictors is desired, x should be scaled as well:  x &lt;- scale(x,TRUE,TRUE).
Then when running glmnet, set standardize=F. See example below.
</p>
<p>The penalty.factor facility in glmmet&ndash; allowing different penalties lambda for each predictor,
is not yet implemented in fixedLassoInf. However you can finesse this&mdash; see the example below. One caveat- using this approach, a penalty factor of zero (forcing a predictor in)
is not allowed.
</p>
<p>Note that the coefficients and standard errors reported are unregularized.
Eg for the Gaussian, they are the usual least squares estimates and standard errors
for the model fit to the active set from the lasso.
</p>


<h3>Value</h3>

  
<table>
<tr><td><code>type</code></td>
<td>
<p>Type of coefficients tested (partial or full)</p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>
<p>Value of tuning parameter lambda used</p>
</td></tr>
<tr><td><code>pv</code></td>
<td>
<p>One-sided P-values for active variables, uses the fact we have conditioned on the sign.</p>
</td></tr>
<tr><td><code>ci</code></td>
<td>
<p>Confidence intervals</p>
</td></tr>
<tr><td><code>tailarea</code></td>
<td>
<p>Realized tail areas (lower and upper) for each confidence interval</p>
</td></tr>
<tr><td><code>vlo</code></td>
<td>
<p>Lower truncation limits for statistics</p>
</td></tr>
<tr><td><code>vup</code></td>
<td>
<p>Upper truncation limits for statistics</p>
</td></tr>
<tr><td><code>vmat</code></td>
<td>
<p>Linear contrasts that define the observed statistics</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>Vector of outcomes</p>
</td></tr>
<tr><td><code>vars</code></td>
<td>
<p>Variables in active set</p>
</td></tr>
<tr><td><code>sign</code></td>
<td>
<p>Signs of active coefficients</p>
</td></tr>
<tr><td><code>alpha</code></td>
<td>
<p>Desired coverage (alpha/2 in each tail)</p>
</td></tr>
<tr><td><code>sigma</code></td>
<td>
<p>Value of error standard deviation (sigma) used</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>The call to fixedLassoInf</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Ryan Tibshirani, Rob Tibshirani, Jonathan Taylor, Joshua Loftus, Stephen Reid</p>


<h3>References</h3>

<p>Jason Lee, Dennis Sun, Yuekai Sun, and Jonathan Taylor (2013). 
Exact post-selection inference, with application to the lasso. arXiv:1311.6238.
</p>
<p>Jonathan Taylor and Robert Tibshirani (2016) Post-selection inference for L1-penalized likelihood models.
arXiv:1602.07358
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(43)
n = 50
p = 10
sigma = 1

x = matrix(rnorm(n*p),n,p)
x = scale(x,TRUE,TRUE)

beta = c(3,2,rep(0,p-2))
y = x%*%beta + sigma*rnorm(n)

# first run glmnet
gfit = glmnet(x,y,standardize=FALSE)

# extract coef for a given lambda; note the 1/n factor!
# (and we don't save the intercept term)
lambda = .8
beta = coef(gfit, x=x, y=y, s=lambda/n, exact=TRUE)[-1]

# compute fixed lambda p-values and selection intervals
out = fixedLassoInf(x,y,beta,lambda,sigma=sigma)
out


## as above, but use lar function instead to get initial 
## lasso fit (should get same results)
lfit = lar(x,y,normalize=FALSE)
beta = coef(lfit, s=lambda, mode="lambda")
out2 = fixedLassoInf(x, y, beta, lambda, sigma=sigma)
out2

## mimic different penalty factors by first scaling x
 set.seed(43)
n = 50
p = 10
sigma = 1

x = matrix(rnorm(n*p),n,p)
x=scale(x,TRUE,TRUE)

beta = c(3,2,rep(0,p-2))
y = x%*%beta + sigma*rnorm(n)
pf=c(rep(1,7),rep(.1,3))  #define penalty factors
pf=p*pf/sum(pf)   # penalty factors should be rescaled so they sum to p
xs=scale(x,FALSE,pf) #scale cols of x by penalty factors
# first run glmnet
gfit = glmnet(xs, y, standardize=FALSE)

# extract coef for a given lambda; note the 1/n factor!
# (and we don't save the intercept term)
lambda = .8
beta_hat = coef(gfit, x=xs, y=y, s=lambda/n, exact=TRUE)[-1]

# compute fixed lambda p-values and selection intervals
out = fixedLassoInf(xs,y,beta_hat,lambda,sigma=sigma)

#rescale conf points to undo the penalty factor
out$ci=t(scale(t(out$ci),FALSE,pf[out$vars]))
out

#logistic model
set.seed(43)

n = 50
p = 10
sigma = 1

x = matrix(rnorm(n*p),n,p)
x=scale(x,TRUE,TRUE)

beta = c(3,2,rep(0,p-2))
y = x%*%beta + sigma*rnorm(n)
y=1*(y&gt;mean(y))
# first run glmnet
gfit = glmnet(x,y,standardize=FALSE,family="binomial")

# extract coef for a given lambda; note the 1/n factor!
# (and here  we DO  include the intercept term)
lambda = .8
beta_hat = coef(gfit, x=x, y=y, s=lambda/n, exact=TRUE)

# compute fixed lambda p-values and selection intervals
out = fixedLassoInf(x,y,beta_hat,lambda,family="binomial")
out


# Cox model

set.seed(43)
n = 50
p = 10
sigma = 1

x = matrix(rnorm(n*p), n, p)
x = scale(x, TRUE, TRUE)

beta = c(3,2,rep(0,p-2))
tim = as.vector(x%*%beta + sigma*rnorm(n))
tim= tim-min(tim)+1
status=sample(c(0,1),size=n,replace=TRUE)
# first run glmnet


y = Surv(tim,status)
gfit = glmnet(x, y, standardize=FALSE, family="cox")

# extract coef for a given lambda; note the 1/n factor!

lambda = 1.5
beta_hat = as.numeric(coef(gfit, x=x, y=y, s=lambda/n, exact=TRUE))

# compute fixed lambda p-values and selection intervals
out = fixedLassoInf(x, tim, beta_hat, lambda, status=status, family="cox")
out

# Debiased lasso or "full"

n = 50
p = 100
sigma = 1

x = matrix(rnorm(n*p),n,p)
x = scale(x,TRUE,TRUE)

beta = c(3,2,rep(0,p-2))
y = x%*%beta + sigma*rnorm(n)

# first run glmnet
gfit = glmnet(x, y, standardize=FALSE, intercept=FALSE)

# extract coef for a given lambda; note the 1/n factor!
# (and we don't save the intercept term)
lambda = 2.8
beta = coef(gfit, x=x, y=y, s=lambda/n, exact=TRUE)[-1]

# compute fixed lambda p-values and selection intervals
out = fixedLassoInf(x, y, beta, lambda, sigma=sigma, type='full', intercept=FALSE)
out

# When n &gt; p and "full" we use the full inverse
# instead of Javanmard and Montanari's approximate inverse

n = 200
p = 50
sigma = 1

x = matrix(rnorm(n*p),n,p)
x = scale(x,TRUE,TRUE)

beta = c(3,2,rep(0,p-2))
y = x%*%beta + sigma*rnorm(n)

# first run glmnet
gfit = glmnet(x, y, standardize=FALSE, intercept=FALSE)

# extract coef for a given lambda; note the 1/n factor!
# (and we don't save the intercept term)
lambda = 2.8
beta = coef(gfit, x=x, y=y, s=lambda/n, exact=TRUE)[-1]

# compute fixed lambda p-values and selection intervals
out = fixedLassoInf(x, y, beta, lambda, sigma=sigma, type='full', intercept=FALSE)
out

</code></pre>

<hr>
<h2 id='forwardStop'>
ForwardStop rule for sequential p-values
</h2><span id='topic+forwardStop'></span>

<h3>Description</h3>

<p>Computes the ForwardStop sequential stopping rule of G'Sell et al (2014)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>forwardStop(pv, alpha=0.1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="forwardStop_+3A_pv">pv</code></td>
<td>

<p>Vector of **sequential** p-values, for example from fsInf or larInf
</p>
</td></tr>
<tr><td><code id="forwardStop_+3A_alpha">alpha</code></td>
<td>

<p>Desired type FDR level (between 0 and 1)
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Computes the ForwardStop sequential stopping rule of G'Sell et al (2014).
Guarantees FDR control at the level alpha, for independent p-values.
</p>


<h3>Value</h3>

<p>Step number for sequential stop.
</p>


<h3>Author(s)</h3>

<p>Ryan Tibshirani, Rob Tibshirani, Jonathan Taylor, Joshua Loftus, Stephen Reid</p>


<h3>References</h3>

<p>Max Grazier G'Sell, Stefan Wager, Alexandra Chouldechova, and Rob Tibshirani (2014).
Sequential selection procedures and Fflse Discovery Rate Control. arXiv:1309.5352.
To appear in Journal of the Royal Statistical Society: Series B.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(33)
n = 50
p = 10
sigma = 1
x = matrix(rnorm(n*p),n,p)
beta = c(3,2,rep(0,p-2))
y = x%*%beta + sigma*rnorm(n)

# run forward stepwise
fsfit = fs(x,y)

# compute sequential p-values and confidence intervals
# (sigma estimated from full model)
out = fsInf(fsfit)
out

# estimate optimal stopping point
forwardStop(out$pv, alpha=.10)
</code></pre>

<hr>
<h2 id='fs'>
Forward stepwise regression
</h2><span id='topic+fs'></span>

<h3>Description</h3>

<p>This function implements forward stepwise regression, for use in the 
selectiveInference package
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fs(x, y, maxsteps=2000, intercept=TRUE, normalize=TRUE, verbose=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fs_+3A_x">x</code></td>
<td>

<p>Matrix of predictors (n by p)
</p>
</td></tr>
<tr><td><code id="fs_+3A_y">y</code></td>
<td>

<p>Vector of outcomes (length n)
</p>
</td></tr>
<tr><td><code id="fs_+3A_maxsteps">maxsteps</code></td>
<td>

<p>Maximum number of steps to take 
</p>
</td></tr>
<tr><td><code id="fs_+3A_intercept">intercept</code></td>
<td>
<p>Should an intercept be included on the model? Default is TRUE</p>
</td></tr>
<tr><td><code id="fs_+3A_normalize">normalize</code></td>
<td>
<p>Should the predictors be normalized? Default is TRUE. (Note:
this argument has no real effect on model selection since forward stepwise is 
scale invariant already; however, it is included for completeness, and to match
the interface for the <code>lar</code> function)
</p>
</td></tr>
<tr><td><code id="fs_+3A_verbose">verbose</code></td>
<td>
<p>Print out progress along the way? Default is FALSE</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function implements forward stepwise regression, adding the predictor at each 
step that maximizes the absolute correlation between the predictors&mdash;once 
orthogonalized with respect to the current model&mdash;and the residual. This entry
criterion is standard, and is equivalent to choosing the variable that achieves 
the biggest drop in RSS at each step; it is used, e.g., by the <code>step</code> function
in R. Note that, for example, the <code>lars</code> package implements a stepwise option 
(with type=&quot;step&quot;), but uses a (mildly) different entry criterion, based on maximal
absolute correlation between the original (non-orthogonalized) predictors and the 
residual. 
</p>


<h3>Value</h3>

<table>
<tr><td><code>action</code></td>
<td>
<p>Vector of predictors in order of entry</p>
</td></tr>
<tr><td><code>sign</code></td>
<td>
<p>Signs of coefficients of predictors, upon entry</p>
</td></tr>
<tr><td><code>df</code></td>
<td>
<p>Degrees of freedom of each active model</p>
</td></tr>
<tr><td><code>beta</code></td>
<td>
<p>Matrix of regression coefficients for each model along the path,
one column per model</p>
</td></tr>
<tr><td><code>completepath</code></td>
<td>
<p>Was the complete stepwise path computed?</p>
</td></tr>
<tr><td><code>bls</code></td>
<td>
<p>If completepath is TRUE, the full least squares coefficients</p>
</td></tr>
<tr><td><code>Gamma</code></td>
<td>
<p>Matrix that captures the polyhedral selection at each step</p>
</td></tr>
<tr><td><code>nk</code></td>
<td>
<p>Number of polyhedral constraints at each step in path</p>
</td></tr>
<tr><td><code>vreg</code></td>
<td>
<p>Matrix of linear contrasts that gives coefficients of variables
to enter along the path</p>
</td></tr>
<tr><td><code>x</code></td>
<td>
<p>Matrix of predictors used</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>Vector of outcomes used</p>
</td></tr>
<tr><td><code>bx</code></td>
<td>
<p>Vector of column means of original x</p>
</td></tr>
<tr><td><code>by</code></td>
<td>
<p>Mean of original y</p>
</td></tr>
<tr><td><code>sx</code></td>
<td>
<p>Norm of each column of original x</p>
</td></tr>
<tr><td><code>intercept</code></td>
<td>
<p>Was an intercept included?</p>
</td></tr>
<tr><td><code>normalize</code></td>
<td>
<p>Were the predictors normalized?</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>The call to fs</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Ryan Tibshirani, Rob Tibshirani, Jonathan Taylor, Joshua Loftus, Stephen Reid</p>


<h3>See Also</h3>

<p><code><a href="#topic+fsInf">fsInf</a></code>, <code><a href="#topic+predict.fs">predict.fs</a></code>,<code><a href="#topic+coef.fs">coef.fs</a></code>, <code><a href="#topic+plot.fs">plot.fs</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(33)
n = 50
p = 10
sigma = 1
x = matrix(rnorm(n*p),n,p)
beta = c(3,2,rep(0,p-2))
y = x%*%beta + sigma*rnorm(n)

# run forward stepwise, plot results
fsfit = fs(x,y)
plot(fsfit)

# compute sequential p-values and confidence intervals
# (sigma estimated from full model)
out = fsInf(fsfit)
out
</code></pre>

<hr>
<h2 id='fsInf'>
Selective inference for forward stepwise regression
</h2><span id='topic+fsInf'></span>

<h3>Description</h3>

<p>Computes p-values and confidence intervals for forward 
stepwise regression
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fsInf(obj, sigma=NULL, alpha=0.1, k=NULL, type=c("active","all","aic"), 
      gridrange=c(-100,100), bits=NULL, mult=2, ntimes=2, verbose=FALSE) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fsInf_+3A_obj">obj</code></td>
<td>

<p>Object returned by <code><a href="#topic+fs">fs</a></code> function
</p>
</td></tr>
<tr><td><code id="fsInf_+3A_sigma">sigma</code></td>
<td>

<p>Estimate of error standard deviation. If NULL (default), this is estimated 
using the mean squared residual of the full least squares fit when n &gt;= 2p, and 
using the standard deviation of y when n &lt; 2p. In the latter case, the user 
should use <code><a href="#topic+estimateSigma">estimateSigma</a></code> function for a more accurate estimate
</p>
</td></tr>
<tr><td><code id="fsInf_+3A_alpha">alpha</code></td>
<td>

<p>Significance level for confidence intervals (target is miscoverage alpha/2 in each tail)
</p>
</td></tr>
<tr><td><code id="fsInf_+3A_k">k</code></td>
<td>

<p>See &quot;type&quot; argument below. Default is NULL, in which case k is taken to be the
the number of steps computed in the forward stepwise path
</p>
</td></tr>
<tr><td><code id="fsInf_+3A_type">type</code></td>
<td>

<p>Type of analysis desired: with &quot;active&quot; (default), p-values and confidence intervals are 
computed for each predictor as it is entered into the active step, all the way through
k steps; with &quot;all&quot;, p-values and confidence intervals are computed for all variables in 
the active model after k steps; with &quot;aic&quot;, the number of steps k is first estimated using 
a modified AIC criterion, and then the same type of analysis as in &quot;all&quot; is carried out for 
this particular value of k.  
</p>
<p>Note that the AIC scheme is defined to choose a number of steps k after which the AIC criterion 
increases <code>ntimes</code> in a row, where <code>ntimes</code> can be specified by the user (see below). 
Under this definition, the AIC selection event is characterizable as a polyhedral set, and hence
the extra conditioning can be taken into account exactly. Also note that an analogous BIC scheme
can be specified through the <code>mult</code> argument (see below)
</p>
</td></tr>
<tr><td><code id="fsInf_+3A_gridrange">gridrange</code></td>
<td>

<p>Grid range for constructing confidence intervals, on the standardized scale
</p>
</td></tr>
<tr><td><code id="fsInf_+3A_bits">bits</code></td>
<td>

<p>Number of bits to be used for p-value and confidence interval calculations. Default is 
NULL, in which case standard floating point calculations are performed. When not NULL, 
multiple precision floating point calculations are performed with the specified number 
of bits, using the R package <code>Rmpfr</code> (if this package is not installed, then a 
warning is thrown, and standard floating point calculations are pursued).
Note: standard double precision uses 53 bits
so, e.g., a choice of 200 bits uses about 4 times double precision. The confidence
interval computation is sometimes numerically challenging, and the extra precision can be 
helpful (though computationally more costly). In particular, extra precision might be tried 
if the values in the output columns of <code>tailarea</code> differ noticeably from alpha/2.
</p>
</td></tr>
<tr><td><code id="fsInf_+3A_mult">mult</code></td>
<td>
<p>Multiplier for the AIC-style penalty. Hence a value of 2 (default) 
gives AIC, whereas a value of log(n) would give BIC</p>
</td></tr>
<tr><td><code id="fsInf_+3A_ntimes">ntimes</code></td>
<td>
<p>Number of steps for which AIC-style criterion has to increase before 
minimizing point is declared</p>
</td></tr>
<tr><td><code id="fsInf_+3A_verbose">verbose</code></td>
<td>
<p>Print out progress along the way? Default is FALSE</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function computes selective p-values and confidence intervals (selection intervals)
for forward stepwise regression. The default is to report the results for 
each predictor after its entry into the model. See the &quot;type&quot; argument for other options.
The confidence interval construction involves numerical search and can be fragile:
if the observed statistic is too close to either end of the truncation interval
(vlo and vup, see references), then one or possibly both endpoints of the interval of 
desired coverage cannot be computed, and default to +/- Inf. The output <code>tailarea</code> 
gives the achieved Gaussian tail areas for the reported intervals&mdash;these should be close 
to alpha/2, and can be used for error-checking purposes.
</p>


<h3>Value</h3>

  
<table>
<tr><td><code>type</code></td>
<td>
<p>Type of analysis (active, all, or aic)</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>Value of k specified in call</p>
</td></tr>
<tr><td><code>khat</code></td>
<td>
<p>When type is &quot;active&quot;, this is an estimated stopping point
declared by <code><a href="#topic+forwardStop">forwardStop</a></code>; when type is &quot;aic&quot;, this is the
value chosen by the modified AIC scheme</p>
</td></tr>
<tr><td><code>pv</code></td>
<td>
<p>One sided P-values for active variables, uses the sign that a variable entered the model with.</p>
</td></tr>
<tr><td><code>ci</code></td>
<td>
<p>Confidence intervals</p>
</td></tr>
<tr><td><code>tailarea</code></td>
<td>
<p>Realized tail areas (lower and upper) for each confidence interval</p>
</td></tr>
<tr><td><code>vlo</code></td>
<td>
<p>Lower truncation limits for statistics</p>
</td></tr>
<tr><td><code>vup</code></td>
<td>
<p>Upper truncation limits for statistics</p>
</td></tr>
<tr><td><code>vmat</code></td>
<td>
<p>Linear contrasts that define the observed statistics</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>Vector of outcomes</p>
</td></tr>
<tr><td><code>vars</code></td>
<td>
<p>Variables in active set</p>
</td></tr>
<tr><td><code>sign</code></td>
<td>
<p>Signs of active coefficients</p>
</td></tr>
<tr><td><code>alpha</code></td>
<td>
<p>Desired coverage (alpha/2 in each tail)</p>
</td></tr>
<tr><td><code>sigma</code></td>
<td>
<p>Value of error standard deviation (sigma) used</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>The call to fsInf</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Ryan Tibshirani, Rob Tibshirani, Jonathan Taylor, Joshua Loftus, Stephen Reid</p>


<h3>References</h3>

<p>Ryan Tibshirani, Jonathan Taylor, Richard Lockhart, and Rob Tibshirani (2014).
Exact post-selection inference for sequential regression procedures. arXiv:1401.3889.
</p>
<p>Joshua Loftus and Jonathan Taylor (2014). A significance test for forward stepwise
model selection. arXiv:1405.3920.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fs">fs</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(33)
n = 50
p = 10
sigma = 1
x = matrix(rnorm(n*p),n,p)
beta = c(3,2,rep(0,p-2))
y = x%*%beta + sigma*rnorm(n)

# run forward stepwise
fsfit = fs(x,y)

# compute sequential p-values and confidence intervals
# (sigma estimated from full model)
out.seq = fsInf(fsfit)
out.seq

# compute p-values and confidence intervals after AIC stopping
out.aic = fsInf(fsfit,type="aic")
out.aic

# compute p-values and confidence intervals after 5 fixed steps
out.fix = fsInf(fsfit,type="all",k=5)
out.fix
</code></pre>

<hr>
<h2 id='groupfs'>Select a model with forward stepwise.</h2><span id='topic+groupfs'></span>

<h3>Description</h3>

<p>This function implements forward selection of linear models almost identically to <code><a href="stats.html#topic+step">step</a></code> with <code>direction = "forward"</code>. The reason this is a separate function from <code><a href="#topic+fs">fs</a></code> is that groups of variables (e.g. dummies encoding levels of a categorical variable) must be handled differently in the selective inference framework.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>groupfs(x, y, index, maxsteps, sigma = NULL, k = 2, intercept = TRUE,
  center = TRUE, normalize = TRUE, aicstop = 0, verbose = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="groupfs_+3A_x">x</code></td>
<td>
<p>Matrix of predictors (n by p).</p>
</td></tr>
<tr><td><code id="groupfs_+3A_y">y</code></td>
<td>
<p>Vector of outcomes (length n).</p>
</td></tr>
<tr><td><code id="groupfs_+3A_index">index</code></td>
<td>
<p>Group membership indicator of length p. Check that <code>sort(unique(index)) = 1:G</code> where <code>G</code> is the number of distinct groups.</p>
</td></tr>
<tr><td><code id="groupfs_+3A_maxsteps">maxsteps</code></td>
<td>
<p>Maximum number of steps for forward stepwise.</p>
</td></tr>
<tr><td><code id="groupfs_+3A_sigma">sigma</code></td>
<td>
<p>Estimate of error standard deviation for use in AIC criterion. This determines the relative scale between RSS and the degrees of freedom penalty. Default is NULL corresponding to unknown sigma. When NULL, <code>link{groupfsInf}</code> performs truncated F inference instead of truncated <code class="reqn">\chi</code>. See <code><a href="stats.html#topic+extractAIC">extractAIC</a></code> for details on the AIC criterion.</p>
</td></tr>
<tr><td><code id="groupfs_+3A_k">k</code></td>
<td>
<p>Multiplier of model size penalty, the default is <code>k = 2</code> for AIC. Use <code>k = log(n)</code> for BIC, or <code>k = 2log(p)</code> for RIC (best for high dimensions, when <code class="reqn">p &gt; n</code>). If <code class="reqn">G &lt; p</code> then RIC may be too restrictive and it would be better to use <code>log(G) &lt; k &lt; 2log(p)</code>.</p>
</td></tr>
<tr><td><code id="groupfs_+3A_intercept">intercept</code></td>
<td>
<p>Should an intercept be included in the model? Default is TRUE. Does not count as a step.</p>
</td></tr>
<tr><td><code id="groupfs_+3A_center">center</code></td>
<td>
<p>Should the columns of the design matrix be centered? Default is TRUE.</p>
</td></tr>
<tr><td><code id="groupfs_+3A_normalize">normalize</code></td>
<td>
<p>Should the design matrix be normalized? Default is TRUE.</p>
</td></tr>
<tr><td><code id="groupfs_+3A_aicstop">aicstop</code></td>
<td>
<p>Early stopping if AIC increases. Default is 0 corresponding to no early stopping. Positive integer values specify the number of times the AIC is allowed to increase in a row, e.g. with <code>aicstop = 2</code> the algorithm will stop if the AIC criterion increases for 2 steps in a row. The default of <code><a href="stats.html#topic+step">step</a></code> corresponds to <code>aicstop = 1</code>.</p>
</td></tr>
<tr><td><code id="groupfs_+3A_verbose">verbose</code></td>
<td>
<p>Print out progress along the way? Default is FALSE.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class &quot;groupfs&quot; containing information about the sequence of models in the forward stepwise algorithm. Call the function <code><a href="#topic+groupfsInf">groupfsInf</a></code> on this object to compute selective p-values.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+groupfsInf">groupfsInf</a></code>, <code><a href="#topic+factorDesign">factorDesign</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x = matrix(rnorm(20*40), nrow=20)
index = sort(rep(1:20, 2))
y = rnorm(20) + 2 * x[,1] - x[,4]
fit = groupfs(x, y, index, maxsteps = 5)
out = groupfsInf(fit)
out
</code></pre>

<hr>
<h2 id='groupfsInf'>Compute selective p-values for a model fitted by <code>groupfs</code>.</h2><span id='topic+groupfsInf'></span>

<h3>Description</h3>

<p>Computes p-values for each group of variables in a model fitted by <code><a href="#topic+groupfs">groupfs</a></code>. These p-values adjust for selection by truncating the usual <code class="reqn">\chi^2</code> statistics to the regions implied by the model selection event. If the <code>sigma</code> to <code><a href="#topic+groupfs">groupfs</a></code> was NULL then groupfsInf uses truncated <code class="reqn">F</code> statistics instead of truncated <code class="reqn">\chi</code>. The <code>sigma</code> argument to groupfsInf allows users to override and use <code class="reqn">\chi</code>, but this is not recommended unless <code class="reqn">\sigma</code> can be estimated well (i.e. <code class="reqn">n &gt; p</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>groupfsInf(obj, sigma = NULL, verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="groupfsInf_+3A_obj">obj</code></td>
<td>
<p>Object returned by <code><a href="#topic+groupfs">groupfs</a></code> function</p>
</td></tr>
<tr><td><code id="groupfsInf_+3A_sigma">sigma</code></td>
<td>
<p>Estimate of error standard deviation. Default is NULL and in this case groupfsInf uses the value of sigma specified to <code><a href="#topic+groupfs">groupfs</a></code>.</p>
</td></tr>
<tr><td><code id="groupfsInf_+3A_verbose">verbose</code></td>
<td>
<p>Print out progress along the way? Default is TRUE.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class &quot;groupfsInf&quot; containing selective p-values for the fitted model <code>obj</code>. For comparison with <code><a href="#topic+fsInf">fsInf</a></code>, note that the option <code>type = "active"</code> is not available.
</p>

<dl>
<dt>vars</dt><dd><p>Labels of the active groups in the order they were included.</p>
</dd>
<dt>pv</dt><dd><p>Selective p-values computed from appropriate truncated distributions.</p>
</dd>
<dt>sigma</dt><dd><p>Estimate of error variance used in computing p-values.</p>
</dd>
<dt>TC or TF</dt><dd><p>Observed value of truncated <code class="reqn">\chi</code> or <code class="reqn">F</code>.</p>
</dd>
<dt>df</dt><dd><p>Rank of group of variables when it was added to the model.</p>
</dd>
<dt>support</dt><dd><p>List of intervals defining the truncation region of the corresponding statistic.</p>
</dd>
</dl>


<hr>
<h2 id='lar'>
Least angle regression
</h2><span id='topic+lar'></span>

<h3>Description</h3>

<p>This function implements least angle regression, for use in the 
selectiveInference package
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lar(x, y, maxsteps=2000, minlam=0, intercept=TRUE, normalize=TRUE,
    verbose=FALSE) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lar_+3A_x">x</code></td>
<td>

<p>Matrix of predictors (n by p)
</p>
</td></tr>
<tr><td><code id="lar_+3A_y">y</code></td>
<td>

<p>Vector of outcomes (length n)
</p>
</td></tr>
<tr><td><code id="lar_+3A_maxsteps">maxsteps</code></td>
<td>

<p>Maximum number of steps to take 
</p>
</td></tr>
<tr><td><code id="lar_+3A_minlam">minlam</code></td>
<td>

<p>Minimum value of lambda to consider
</p>
</td></tr>
<tr><td><code id="lar_+3A_intercept">intercept</code></td>
<td>
<p>Should an intercept be included on the model? Default is TRUE</p>
</td></tr>
<tr><td><code id="lar_+3A_normalize">normalize</code></td>
<td>
<p>Should the predictors be normalized? Default is TRUE</p>
</td></tr>
<tr><td><code id="lar_+3A_verbose">verbose</code></td>
<td>
<p>Print out progress along the way? Default is FALSE</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The least angle regression algorithm is described in detail by Efron et al. (2002). 
This function should match (in terms of its output) that from the <code>lars</code> package,
but returns additional information (namely, the polyhedral constraints) needed for the
selective inference calculations.
</p>


<h3>Value</h3>

<table>
<tr><td><code>lambda</code></td>
<td>
<p>Values of lambda (knots) visited along the path</p>
</td></tr>
<tr><td><code>action</code></td>
<td>
<p>Vector of predictors in order of entry</p>
</td></tr>
<tr><td><code>sign</code></td>
<td>
<p>Signs of coefficients of predictors, upon entry</p>
</td></tr>
<tr><td><code>df</code></td>
<td>
<p>Degrees of freedom of each active model</p>
</td></tr>
<tr><td><code>beta</code></td>
<td>
<p>Matrix of regression coefficients for each model along the path,
one model per column</p>
</td></tr>
<tr><td><code>completepath</code></td>
<td>
<p>Was the complete stepwise path computed?</p>
</td></tr>
<tr><td><code>bls</code></td>
<td>
<p>If completepath is TRUE, the full least squares coefficients</p>
</td></tr>
<tr><td><code>Gamma</code></td>
<td>
<p>Matrix that captures the polyhedral selection at each step</p>
</td></tr>
<tr><td><code>nk</code></td>
<td>
<p>Number of polyhedral constraints at each step in path</p>
</td></tr>
<tr><td><code>vreg</code></td>
<td>
<p>Matrix of linear contrasts that gives coefficients of variables
to enter along the path</p>
</td></tr>     
<tr><td><code>mp</code></td>
<td>
<p>Value of M+ (for internal use with the spacing test)</p>
</td></tr>	 
<tr><td><code>x</code></td>
<td>
<p>Matrix of predictors used</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>Vector of outcomes used</p>
</td></tr>
<tr><td><code>bx</code></td>
<td>
<p>Vector of column means of original x</p>
</td></tr>
<tr><td><code>by</code></td>
<td>
<p>Mean of original y</p>
</td></tr>
<tr><td><code>sx</code></td>
<td>
<p>Norm of each column of original x</p>
</td></tr>
<tr><td><code>intercept</code></td>
<td>
<p>Was an intercept included?</p>
</td></tr>
<tr><td><code>normalize</code></td>
<td>
<p>Were the predictors normalized?</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>The call to lar</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Ryan Tibshirani, Rob Tibshirani, Jonathan Taylor, Max G'Sell, Joshua Loftus, Stephen Reid</p>


<h3>References</h3>

<p>Brad Efron, Trevor Hastie, Iain Johnstone, and Rob Tibshirani (2002).
Least angle regression. Annals of Statistics (with discussion).
</p>
<p>See also the descriptions in Trevor Hastie, Rob Tibshirani, and 
Jerome Friedman (2002, 2009). Elements of Statistical Learning.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+larInf">larInf</a></code>, <code><a href="#topic+predict.lar">predict.lar</a></code>, <code><a href="#topic+coef.lar">coef.lar</a></code>, <code><a href="#topic+plot.lar">plot.lar</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(43)
n = 50
p = 10
sigma = 1
x = matrix(rnorm(n*p),n,p)
beta = c(3,2,rep(0,p-2))
y = x%*%beta + sigma*rnorm(n)

# run LAR, plot results
larfit = lar(x,y)
plot(larfit)

# compute sequential p-values and confidence intervals
# (sigma estimated from full model)
out = larInf(larfit)
out                                    
</code></pre>

<hr>
<h2 id='larInf'>
Selective inference for least angle regression 
</h2><span id='topic+larInf'></span>

<h3>Description</h3>

<p>Computes p-values and confidence intervals for least 
angle regression
</p>


<h3>Usage</h3>

<pre><code class='language-R'>larInf(obj, sigma=NULL, alpha=0.1, k=NULL, type=c("active","all","aic"), 
       gridrange=c(-100,100), bits=NULL, mult=2, ntimes=2, verbose=FALSE) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="larInf_+3A_obj">obj</code></td>
<td>

<p>Object returned by <code>lar</code> function (not the <code>lars</code> function!)
</p>
</td></tr>
<tr><td><code id="larInf_+3A_sigma">sigma</code></td>
<td>

<p>Estimate of error standard deviation. If NULL (default), this is estimated 
using the mean squared residual of the full least squares fit when n &gt;= 2p, and 
using the standard deviation of y when n &lt; 2p. In the latter case, the user 
should use <code><a href="#topic+estimateSigma">estimateSigma</a></code> function for a more accurate estimate
</p>
</td></tr>
<tr><td><code id="larInf_+3A_alpha">alpha</code></td>
<td>

<p>Significance level for confidence intervals (target is miscoverage alpha/2 in each tail)
</p>
</td></tr>
<tr><td><code id="larInf_+3A_k">k</code></td>
<td>

<p>See &quot;type&quot; argument below. Default is NULL, in which case k is taken to be the
the number of steps computed in the least angle regression path
</p>
</td></tr>
<tr><td><code id="larInf_+3A_type">type</code></td>
<td>

<p>Type of analysis desired: with &quot;active&quot; (default), p-values and confidence intervals are 
computed for each predictor as it is entered into the active step, all the way through
k steps; with &quot;all&quot;, p-values and confidence intervals are computed for all variables in 
the active model after k steps; with &quot;aic&quot;, the number of steps k is first estimated using 
a modified AIC criterion, and then the same type of analysis as in &quot;all&quot; is carried out for 
this particular value of k.  
</p>
<p>Note that the AIC scheme is defined to choose a number of steps k after which the AIC criterion 
increases <code>ntimes</code> in a row, where <code>ntimes</code> can be specified by the user (see below). 
Under this definition, the AIC selection event is characterizable as a polyhedral set, and hence
the extra conditioning can be taken into account exactly. Also note that an analogous BIC scheme
can be specified through the <code>mult</code> argument (see below)
</p>
</td></tr>
<tr><td><code id="larInf_+3A_gridrange">gridrange</code></td>
<td>

<p>Grid range for constructing confidence intervals, on the standardized scale
</p>
</td></tr>
<tr><td><code id="larInf_+3A_bits">bits</code></td>
<td>

<p>Number of bits to be used for p-value and confidence interval calculations. Default is 
NULL, in which case standard floating point calculations are performed. When not NULL, 
multiple precision floating point calculations are performed with the specified number 
of bits, using the R package <code>Rmpfr</code> (if this package is not installed, then a 
warning is thrown, and standard floating point calculations are pursued).
Note: standard double precision uses 53 bits
so, e.g., a choice of 200 bits uses about 4 times double precision. The confidence
interval computation is sometimes numerically challenging, and the extra precision can be 
helpful (though computationally more costly). In particular, extra precision might be tried 
if the values in the output columns of <code>tailarea</code> differ noticeably from alpha/2.
</p>
</td></tr>
<tr><td><code id="larInf_+3A_mult">mult</code></td>
<td>
<p>Multiplier for the AIC-style penalty. Hence a value of 2 (default) 
gives AIC, whereas a value of log(n) would give BIC</p>
</td></tr>
<tr><td><code id="larInf_+3A_ntimes">ntimes</code></td>
<td>
<p>Number of steps for which AIC-style criterion has to increase before 
minimizing point is declared</p>
</td></tr>
<tr><td><code id="larInf_+3A_verbose">verbose</code></td>
<td>
<p>Print out progress along the way? Default is FALSE</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function computes selective p-values and confidence intervals (selection intervals)
for least angle regression. The default is to report the results for 
each predictor after its entry into the model. See the &quot;type&quot; argument for other options.
The confidence interval construction involves numerical search and can be fragile:
if the observed statistic is too close to either end of the truncation interval
(vlo and vup, see references), then one or possibly both endpoints of the interval of 
desired coverage cannot be computed, and default to +/- Inf. The output <code>tailarea</code> 
gives the achieved Gaussian tail areas for the reported intervals&mdash;these should be close 
to alpha/2, and can be used for error-checking purposes.
</p>


<h3>Value</h3>

<table>
<tr><td><code>type</code></td>
<td>
<p>Type of analysis (active, all, or aic)</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>Value of k specified in call</p>
</td></tr>
<tr><td><code>khat</code></td>
<td>
<p>When type is &quot;active&quot;, this is an estimated stopping point
declared by <code><a href="#topic+forwardStop">forwardStop</a></code>; when type is &quot;aic&quot;, this is the
value chosen by the modified AIC scheme</p>
</td></tr>
<tr><td><code>pv</code></td>
<td>
<p>P-values for active variables</p>
</td></tr>
<tr><td><code>ci</code></td>
<td>
<p>Confidence intervals</p>
</td></tr>
<tr><td><code>tailarea</code></td>
<td>
<p>Realized tail areas (lower and upper) for each confidence interval</p>
</td></tr>
<tr><td><code>vlo</code></td>
<td>
<p>Lower truncation limits for statistics</p>
</td></tr>
<tr><td><code>vup</code></td>
<td>
<p>Upper truncation limits for statistics</p>
</td></tr>
<tr><td><code>vmat</code></td>
<td>
<p>Linear contrasts that define the observed statistics</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>Vector of outcomes</p>
</td></tr>
<tr><td><code>pv.spacing</code></td>
<td>
<p>P-values from the spacing test (here M+ is used)</p>
</td></tr>
<tr><td><code>pv.modspac</code></td>
<td>
<p>P-values from the modified form of the spacing test 
(here M+ is replaced by the next knot)</p>
</td></tr>
<tr><td><code>pv.covtest</code></td>
<td>
<p>P-values from covariance test</p>
</td></tr>
<tr><td><code>vars</code></td>
<td>
<p>Variables in active set</p>
</td></tr>
<tr><td><code>sign</code></td>
<td>
<p>Signs of active coefficients</p>
</td></tr>
<tr><td><code>alpha</code></td>
<td>
<p>Desired coverage (alpha/2 in each tail)</p>
</td></tr>
<tr><td><code>sigma</code></td>
<td>
<p>Value of error standard deviation (sigma) used</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>The call to larInf</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Ryan Tibshirani, Rob Tibshirani, Jonathan Taylor, Joshua Loftus, Stephen Reid</p>


<h3>References</h3>

<p>Ryan Tibshirani, Jonathan Taylor, Richard Lockhart, and Rob Tibshirani (2014).
Exact post-selection inference for sequential regression procedures. arXiv:1401.3889.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+lar">lar</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(43)
n = 50
p = 10
sigma = 1
x = matrix(rnorm(n*p),n,p)
beta = c(3,2,rep(0,p-2))
y = x%*%beta + sigma*rnorm(n)

# run LAR
larfit = lar(x,y)

# compute sequential p-values and confidence intervals
# (sigma estimated from full model)
out.seq = larInf(larfit)
out.seq

# compute p-values and confidence intervals after AIC stopping
out.aic = larInf(larfit,type="aic")
out.aic

# compute p-values and confidence intervals after 5 fixed steps
out.fix = larInf(larfit,type="all",k=5)
out.fix
</code></pre>

<hr>
<h2 id='manyMeans'>
Selective inference for many normal means
</h2><span id='topic+manyMeans'></span>

<h3>Description</h3>

<p>Computes p-values and confidence intervals for the largest k
among many normal means
</p>


<h3>Usage</h3>

<pre><code class='language-R'>manyMeans(y, alpha=0.1, bh.q=NULL, k=NULL, sigma=1, verbose=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="manyMeans_+3A_y">y</code></td>
<td>
<p>Vector of outcomes (length n)</p>
</td></tr>
<tr><td><code id="manyMeans_+3A_alpha">alpha</code></td>
<td>

<p>Significance level for confidence intervals (target is miscoverage alpha/2 in each tail)
</p>
</td></tr>
<tr><td><code id="manyMeans_+3A_bh.q">bh.q</code></td>
<td>
<p>q parameter for BH(q) procedure</p>
</td></tr>
<tr><td><code id="manyMeans_+3A_k">k</code></td>
<td>
<p>Number of means to consider</p>
</td></tr>
<tr><td><code id="manyMeans_+3A_sigma">sigma</code></td>
<td>
<p>Estimate of error standard deviation</p>
</td></tr>
<tr><td><code id="manyMeans_+3A_verbose">verbose</code></td>
<td>
<p>Print out progress along the way? Default is FALSE</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function compute p-values and confidence intervals for the largest k 
among many normal means. One can specify a fixed number of means k to consider, 
or choose the number to consider via the BH rule.
</p>


<h3>Value</h3>

  
<table>
<tr><td><code>mu.hat</code></td>
<td>
<p> Vector of length n containing the estimated signal sizes.
If a sample element is not selected, then its signal size estimate is 0</p>
</td></tr>
<tr><td><code>selected.set</code></td>
<td>
<p>Indices of the  vector y of the sample elements that 
were selected by the procedure (either BH(q) or top-K). Labelled &quot;Selind&quot; in output table.</p>
</td></tr>
<tr><td><code>pv</code></td>
<td>
<p>P-values for selected signals</p>
</td></tr>
<tr><td><code>ci</code></td>
<td>
<p>Confidence intervals</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>Method used to choose number of means</p>
</td></tr>
<tr><td><code>sigma</code></td>
<td>
<p>Value of error standard deviation (sigma) used</p>
</td></tr>
<tr><td><code>bh.q</code></td>
<td>
<p>BH q-value used</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>Desired number of means</p>
</td></tr>
<tr><td><code>threshold</code></td>
<td>
<p>Computed cutoff</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>The call to manyMeans</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Ryan Tibshirani, Rob Tibshirani, Jonathan Taylor, Joshua Loftus, Stephen Reid</p>


<h3>References</h3>

<p>Stephen Reid, Jonathan Taylor, and Rob Tibshirani (2014).
Post-selection point and interval estimation of signal sizes in Gaussian samples.
arXiv:1405.3340.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(12345)
n = 100 
mu = c(rep(3,floor(n/5)), rep(0,n-floor(n/5))) 
y = mu + rnorm(n)
out = manyMeans(y, bh.q=0.1)
out
</code></pre>

<hr>
<h2 id='plot.fs'>
Plot function for forward stepwise regression
</h2><span id='topic+plot.fs'></span>

<h3>Description</h3>

<p>Plot coefficient profiles along the forward stepwise  path
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'fs'
 plot(x, breaks=TRUE, omit.zeros=TRUE, var.labels=TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.fs_+3A_x">x</code></td>
<td>

<p>Object returned by a call to <code>fs</code> function
</p>
</td></tr>
<tr><td><code id="plot.fs_+3A_breaks">breaks</code></td>
<td>
<p>Should vertical lines be drawn at each break point in the piecewise
linear coefficient paths? Default is TRUE</p>
</td></tr>
<tr><td><code id="plot.fs_+3A_omit.zeros">omit.zeros</code></td>
<td>
<p>Should segments of the coefficients paths that are equal to 
zero be omitted (to avoid clutter in the figure)? Default is TRUE</p>
</td></tr>
<tr><td><code id="plot.fs_+3A_var.labels">var.labels</code></td>
<td>
<p>Should paths be labelled with corresponding variable numbers? 
Default is TRUE</p>
</td></tr>
<tr><td><code id="plot.fs_+3A_...">...</code></td>
<td>
<p>Additional arguments for plotting</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Ryan Tibshirani, Rob Tibshirani, Jonathan Taylor, Joshua Loftus, Stephen Reid</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(33)
n = 50
p = 10
sigma = 1
x = matrix(rnorm(n*p),n,p)
beta = c(3,2,rep(0,p-2))
y = x%*%beta + sigma*rnorm(n)

# run forward stepwise, plot results
fsfit = fs(x,y)
plot(fsfit)
</code></pre>

<hr>
<h2 id='plot.lar'>
Plot function for least angle regression
</h2><span id='topic+plot.lar'></span>

<h3>Description</h3>

<p>Plot coefficient profiles along the LAR path
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'lar'
plot(x, xvar=c("norm","step","lambda"), breaks=TRUE, 
                   omit.zeros=TRUE, var.labels=TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.lar_+3A_x">x</code></td>
<td>

<p>Object returned by a call to <code>lar</code> function
(not the <code>lars</code> function!)
</p>
</td></tr>
<tr><td><code id="plot.lar_+3A_xvar">xvar</code></td>
<td>
<p>Either &quot;norm&quot; or &quot;step&quot; or &quot;lambda&quot;, determining what is plotted 
on the x-axis</p>
</td></tr>
<tr><td><code id="plot.lar_+3A_breaks">breaks</code></td>
<td>
<p>Should vertical lines be drawn at each break point in the piecewise
linear coefficient paths? Default is TRUE</p>
</td></tr>
<tr><td><code id="plot.lar_+3A_omit.zeros">omit.zeros</code></td>
<td>
<p>Should segments of the coefficients paths that are equal to 
zero be omitted (to avoid clutter in the figure)? Default is TRUE</p>
</td></tr>
<tr><td><code id="plot.lar_+3A_var.labels">var.labels</code></td>
<td>
<p>Should paths be labelled with corresponding variable numbers? 
Default is TRUE</p>
</td></tr>
<tr><td><code id="plot.lar_+3A_...">...</code></td>
<td>
<p>Additional arguments for plotting</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Ryan Tibshirani, Rob Tibshirani, Jonathan Taylor, Joshua Loftus, Stephen Reid</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(43)
n = 50
p = 10
sigma = 1
x = matrix(rnorm(n*p),n,p)
beta = c(3,2,rep(0,p-2))
y = x%*%beta + sigma*rnorm(n)

# run LAR, plot results
larfit = lar(x,y)
plot(larfit)
</code></pre>

<hr>
<h2 id='predict.fs'>
Prediction and coefficient functions for forward stepwise
regression
</h2><span id='topic+predict.fs'></span><span id='topic+coef.fs'></span>

<h3>Description</h3>

<p>Make predictions or extract coefficients from a forward stepwise object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'fs'
predict(object, newx, s, ...)
## S3 method for class 'fs'
coef(object, s, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.fs_+3A_object">object</code></td>
<td>

<p>Object returned by a call to <code>fs</code> function
</p>
</td></tr>
<tr><td><code id="predict.fs_+3A_newx">newx</code></td>
<td>

<p>Matrix of x values at which the predictions are desired. If NULL,
the x values from forward stepwise fitting are used
</p>
</td></tr>
<tr><td><code id="predict.fs_+3A_s">s</code></td>
<td>

<p>Step number(s) at which predictions or coefficients are desired
</p>
</td></tr>
<tr><td><code id="predict.fs_+3A_...">...</code></td>
<td>
<p>Additional arguments</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Either a vector/matrix of predictions, or a vector/matrix of coefficients.
</p>


<h3>Author(s)</h3>

<p>Ryan Tibshirani, Rob Tibshirani, Jonathan Taylor, Joshua Loftus, Stephen Reid</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(33)
n = 200
p = 20
sigma = 1
x = matrix(rnorm(n*p),n,p)
beta = c(rep(3,10),rep(0,p-10))
y = x%*%beta + sigma*rnorm(n)

# run forward stepwise and predict functions
obj = fs(x,y)
fit = predict(obj,x,s=3)
</code></pre>

<hr>
<h2 id='predict.groupfs'>Prediction and coefficient functions for <code><a href="#topic+groupfs">groupfs</a></code>.</h2><span id='topic+predict.groupfs'></span>

<h3>Description</h3>

<p>Make predictions or extract coefficients from a groupfs forward stepwise object.</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'groupfs'
predict(object, newx)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.groupfs_+3A_object">object</code></td>
<td>
<p>Object returned by a call to <code><a href="#topic+groupfs">groupfs</a></code>.</p>
</td></tr>
<tr><td><code id="predict.groupfs_+3A_newx">newx</code></td>
<td>
<p>Matrix of x values at which the predictions are desired. If NULL, the x values from groupfs fitting are used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of predictions or a vector of coefficients.
</p>

<hr>
<h2 id='predict.lar'>
Prediction and coefficient functions for least angle regression
</h2><span id='topic+predict.lar'></span><span id='topic+coef.lar'></span>

<h3>Description</h3>

<p>Make predictions or extract coefficients from a least angle regression object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'lar'
predict(object, newx, s, mode=c("step","lambda"), ...)
## S3 method for class 'lar'
coef(object, s, mode=c("step","lambda"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.lar_+3A_object">object</code></td>
<td>

<p>Object returned by a call to <code>lar</code> function
(not the <code>lars</code> function!)
</p>
</td></tr>
<tr><td><code id="predict.lar_+3A_newx">newx</code></td>
<td>

<p>Matrix of x values at which the predictions are desired. If NULL,
the x values from least angle regression fitting are used
</p>
</td></tr>
<tr><td><code id="predict.lar_+3A_s">s</code></td>
<td>

<p>Step number(s) or lambda value(s) at which predictions or coefficients 
are desired
</p>
</td></tr>
<tr><td><code id="predict.lar_+3A_mode">mode</code></td>
<td>
<p>Either &quot;step&quot; or &quot;lambda&quot;, determining the role of s (above)</p>
</td></tr>
<tr><td><code id="predict.lar_+3A_...">...</code></td>
<td>
<p>Additional arguments</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Either a vector/matrix of predictions, or a vector/matrix of coefficients.
</p>


<h3>Author(s)</h3>

<p>Ryan Tibshirani, Rob Tibshirani, Jonathan Taylor, Joshua Loftus, Stephen Reid</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(33)
n = 200
p = 20
sigma = 1
x = matrix(rnorm(n*p),n,p)
beta = c(rep(3,10),rep(0,p-10))
y = x%*%beta + sigma*rnorm(n)

# run lar and predict functions
obj = lar(x,y)
fit = predict(obj,x,s=3)
</code></pre>

<hr>
<h2 id='randomizedLasso'>
Inference for the randomized lasso, with a fixed lambda 
</h2><span id='topic+randomizedLasso'></span>

<h3>Description</h3>

<p>Solve a randomly perturbed LASSO problem.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>randomizedLasso(X, 
                y, 
                lam, 
                family=c("gaussian", "binomial"),
                noise_scale=NULL, 
                ridge_term=NULL, 
                max_iter=100,       
                kkt_tol=1.e-4,      
                parameter_tol=1.e-8,
                objective_tol=1.e-8,
                objective_stop=FALSE,
                kkt_stop=TRUE,
                parameter_stop=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="randomizedLasso_+3A_x">X</code></td>
<td>

<p>Matrix of predictors (n by p); 
</p>
</td></tr>
<tr><td><code id="randomizedLasso_+3A_y">y</code></td>
<td>

<p>Vector of outcomes (length n)
</p>
</td></tr>
<tr><td><code id="randomizedLasso_+3A_lam">lam</code></td>
<td>

<p>Value of lambda used to compute beta. See the above warning
Be careful! This function uses the &quot;standard&quot; lasso objective
</p>
<p style="text-align: center;"><code class="reqn">
    1/2 \|y - x \beta\|_2^2 + \lambda \|\beta\|_1.
  </code>
</p>

<p>In contrast, glmnet multiplies the first term by a factor of 1/n.
So after running glmnet, to extract the beta corresponding to a value lambda, 
you need to use <code>beta = coef(obj, s=lambda/n)[-1]</code>,
where obj is the object returned by glmnet (and [-1] removes the intercept,
which glmnet always puts in the first component)
</p>
</td></tr> 
<tr><td><code id="randomizedLasso_+3A_family">family</code></td>
<td>

<p>Response type: &quot;gaussian&quot; (default), &quot;binomial&quot;.
</p>
</td></tr>
<tr><td><code id="randomizedLasso_+3A_noise_scale">noise_scale</code></td>
<td>

<p>Scale of Gaussian noise added to objective. Default is 
0.5 * sd(y) times the sqrt of the mean of the trace of X^TX.
</p>
</td></tr>
<tr><td><code id="randomizedLasso_+3A_ridge_term">ridge_term</code></td>
<td>

<p>A small &quot;elastic net&quot; or ridge penalty is added to ensure
the randomized problem has a solution. 
0.5 * sd(y) times the sqrt of the mean of the trace of X^TX divided by
sqrt(n).
</p>
</td></tr>
<tr><td><code id="randomizedLasso_+3A_max_iter">max_iter</code></td>
<td>

<p>How many rounds of updates used of coordinate descent in solving randomized
LASSO.
</p>
</td></tr>
<tr><td><code id="randomizedLasso_+3A_kkt_tol">kkt_tol</code></td>
<td>

<p>Tolerance for checking convergence based on KKT conditions.
</p>
</td></tr>
<tr><td><code id="randomizedLasso_+3A_parameter_tol">parameter_tol</code></td>
<td>

<p>Tolerance for checking convergence based on convergence
of parameters.
</p>
</td></tr>
<tr><td><code id="randomizedLasso_+3A_objective_tol">objective_tol</code></td>
<td>

<p>Tolerance for checking convergence based on convergence
of objective value.
</p>
</td></tr>
<tr><td><code id="randomizedLasso_+3A_kkt_stop">kkt_stop</code></td>
<td>

<p>Should we use KKT check to determine when to stop?
</p>
</td></tr>
<tr><td><code id="randomizedLasso_+3A_parameter_stop">parameter_stop</code></td>
<td>

<p>Should we use convergence of parameters to determine when to stop?
</p>
</td></tr>
<tr><td><code id="randomizedLasso_+3A_objective_stop">objective_stop</code></td>
<td>

<p>Should we use convergence of objective value to determine when to stop?
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For <code>family="gaussian"</code> this function uses the &quot;standard&quot; lasso objective
</p>
<p style="text-align: center;"><code class="reqn">
    1/2 \|y - x \beta\|_2^2 + \lambda \|\beta\|_1
  </code>
</p>

<p>and adds a term 
</p>
<p style="text-align: center;"><code class="reqn">
    - \omega^T\beta + \frac{\epsilon}{2} \|\beta\|^2_2
  </code>
</p>

<p>where omega is drawn from IID normals with standard deviation
<code>noise_scale</code> and epsilon given by <code>ridge_term</code>. 
See below for default values of <code>noise_scale</code> and <code>ridge_term</code>.
</p>
<p>For <code>family="binomial"</code>, the squared error loss is replaced by the
negative of the logistic log-likelihood.
</p>


<h3>Value</h3>

  
<table>
<tr><td><code>X</code></td>
<td>
<p>Design matrix.</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>Response vector.</p>
</td></tr>
<tr><td><code>lam</code></td>
<td>
<p>Vector of penalty parameters.</p>
</td></tr>
<tr><td><code>family</code></td>
<td>
<p>Family: &quot;gaussian&quot; or &quot;binomial&quot;.</p>
</td></tr>
<tr><td><code>active_set</code></td>
<td>
<p>Set of non-zero coefficients in randomized solution that were penalized. Integers from 1:p.</p>
</td></tr>
<tr><td><code>inactive_set</code></td>
<td>
<p>Set of zero coefficients in randomized solution. Integers from 1:p.</p>
</td></tr>
<tr><td><code>unpenalized_set</code></td>
<td>
<p>Set of non-zero coefficients in randomized solution that were not penalized. Integers from 1:p.</p>
</td></tr>
<tr><td><code>sign_soln</code></td>
<td>
<p>The sign pattern of the randomized solution.</p>
</td></tr>
<tr><td><code>full_law</code></td>
<td>
<p>List describing sampling parameters for conditional law of all optimization variables given the data in the LASSO problem.</p>
</td></tr>
<tr><td><code>conditional_law</code></td>
<td>
<p>List describing sampling parameters for conditional law of only the scaling variables given the data and the observed subgradient in the LASSO problem.</p>
</td></tr>
<tr><td><code>internal_transform</code></td>
<td>
<p>Affine transformation describing relationship between internal representation of the data and the data compontent of score of the likelihood at the unregularized MLE based on the sign_vector (a.k.a. relaxed LASSO).</p>
</td></tr>
<tr><td><code>observed_raw</code></td>
<td>
<p>Data component of the score at the unregularized MLE.</p>
</td></tr>
<tr><td><code>noise_scale</code></td>
<td>
<p>SD of Gaussian noise used to draw the perturbed objective.</p>
</td></tr>
<tr><td><code>soln</code></td>
<td>
<p>The randomized solution. Inference is made conditional on its sign vector (so no more snooping of this value is formally permitted.) 
If <code>condition_subgrad == TRUE</code> when sampling, then we may snoop on the observed subgradient.</p>
</td></tr>
<tr><td><code>perturb</code></td>
<td>
<p>The random vector in the linear term added to the objective.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Jelena Markovic, Jonathan Taylor</p>


<h3>References</h3>

<p>Xiaoying Tian, and Jonathan Taylor (2015).
Selective inference with a randomized response. arxiv.org:1507.06739
</p>
<p>Xiaoying Tian, Snigdha Panigrahi, Jelena Markovic, Nan Bi and Jonathan Taylor (2016).
Selective inference after solving a convex problem. 
arxiv:1609.05609
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(43)
n = 50
p = 10
sigma = 0.2
lam = 0.5

X = matrix(rnorm(n*p), n, p)
X = scale(X, TRUE, TRUE) / sqrt(n-1)

beta = c(3,2,rep(0,p-2))
y = X%*%beta + sigma*rnorm(n)

result = randomizedLasso(X, y, lam)

</code></pre>

<hr>
<h2 id='randomizedLassoInf'>
Inference for the randomized lasso, with a fixed lambda 
</h2><span id='topic+randomizedLassoInf'></span>

<h3>Description</h3>

<p>Compute p-values and confidence intervals based on selecting
an active set with the randomized lasso, at a 
fixed value of the tuning parameter lambda and using Gaussian
randomization.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>randomizedLassoInf(rand_lasso_soln, 
		   targets=NULL,
                   level=0.9,
                   sampler=c("norejection", "adaptMCMC"),
                   nsample=10000,
                   burnin=2000,
                   opt_samples=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="randomizedLassoInf_+3A_rand_lasso_soln">rand_lasso_soln</code></td>
<td>

<p>A randomized lasso solution as returned by <code>randomizedLasso</code>.
</p>
</td></tr>
<tr><td><code id="randomizedLassoInf_+3A_targets">targets</code></td>
<td>

<p>If not NULL, should be a list with entries <code>observed_target, cov_target, crosscov_target_internal</code>.
The <code>observed_target</code> should be (pre-selection) asymptotically Gaussian around targeted
parameters. The quantity <code>cov_target</code> should be an estimate of the (pre-selection) covariance
of <code>observed_target</code>. Finally, <code>crosscov_target_internal</code> should be an estimate of
the (pre-selection) covariance of <code>observed_target</code> and the internal representation of the
data of the LASSO. For both <code>"gaussian"</code> and <code>"binomial"</code>, this is the vector
</p>
<p style="text-align: center;"><code class="reqn">
\hat{\beta}_{E,MLE}, X_{-E}^T(y - \mu(X_E\hat{\beta}_{E,MLE}))
  </code>
</p>
 
<p>For example, this cross-covariance could be estimated by jointly bootstrapping the target 
of interest and the above vector.
</p>
</td></tr>
<tr><td><code id="randomizedLassoInf_+3A_level">level</code></td>
<td>

<p>Level for confidence intervals.
</p>
</td></tr>
<tr><td><code id="randomizedLassoInf_+3A_sampler">sampler</code></td>
<td>

<p>Which sampler to use &ndash; default is a no-rejection sampler. Otherwise
use MCMC from the adaptMCMC package.
</p>
</td></tr>
<tr><td><code id="randomizedLassoInf_+3A_nsample">nsample</code></td>
<td>

<p>Number of samples of optimization variables to sample.
</p>
</td></tr>
<tr><td><code id="randomizedLassoInf_+3A_burnin">burnin</code></td>
<td>

<p>How many samples of optimization variable to discard (should be less than nsample).
</p>
</td></tr>
<tr><td><code id="randomizedLassoInf_+3A_opt_samples">opt_samples</code></td>
<td>

<p>Optional sample of optimization variables. If not NULL then no MCMC will be run.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function computes selective p-values and confidence intervals for a
randomized version of the lasso,
given a fixed value of the tuning parameter lambda. 
</p>


<h3>Value</h3>

  
<table>
<tr><td><code>targets</code></td>
<td>
<p>A list with entries <code>observed_target, cov_target, crosscov_target_internal</code>. See argument description above.</p>
</td></tr>
<tr><td><code>pvalues</code></td>
<td>
<p>P-values testing hypotheses that each specific target is 0.</p>
</td></tr>
<tr><td><code>ci</code></td>
<td>
<p>Confidence interval for parameters determined by <code>targets</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Jelena Markovic, Jonathan Taylor</p>


<h3>References</h3>

<p>Jelena Markovic and Jonathan Taylor (2016).
Bootstrap inference after using multiple queries for model selection. arxiv.org:1612.07811
</p>
<p>Xiaoying Tian and Jonathan Taylor (2015).
Selective inference with a randomized response. arxiv.org:1507.06739
</p>
<p>Xiaoying Tian, Snigdha Panigrahi, Jelena Markovic, Nan Bi and Jonathan Taylor (2016).
Selective inference after solving a convex problem. 
arxiv.org:1609.05609
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(43)
n = 50
p = 10
sigma = 0.2
lam = 0.5

X = matrix(rnorm(n*p), n, p)
X = scale(X, TRUE, TRUE) / sqrt(n-1)

beta = c(3,2,rep(0,p-2))
y = X%*%beta + sigma*rnorm(n)

result = randomizedLasso(X, y, lam)
inf_result = randomizedLassoInf(result)
</code></pre>

<hr>
<h2 id='ROSI'>
Relevant One-step Selective Inference for the LASSO
</h2><span id='topic+ROSI'></span>

<h3>Description</h3>

<p>Compute p-values and confidence intervals for the lasso estimate, at a 
fixed value of the tuning parameter lambda using the &quot;relevant&quot;
conditioning event of arxiv.org/1801.09037. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ROSI(X, 
     y, 
     soln, 
     lambda, 
     penalty_factor=NULL, 
     dispersion=1,
     family=c('gaussian', 'binomial'), 
     solver=c('QP', 'glmnet'),
     construct_ci=TRUE, 
     debiasing_method=c("JM", "BN"),
     verbose=FALSE,
     level=0.9,
     use_debiased=TRUE) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ROSI_+3A_x">X</code></td>
<td>

<p>Matrix of predictors (n by p); 
</p>
</td></tr>
<tr><td><code id="ROSI_+3A_y">y</code></td>
<td>

<p>Vector of outcomes (length n)
</p>
</td></tr>
<tr><td><code id="ROSI_+3A_soln">soln</code></td>
<td>

<p>Estimated lasso coefficients (e.g., from glmnet). This is of length p
(so the intercept is not included as the first component).
</p>
<p>Be careful! This function uses the &quot;standard&quot; lasso objective
</p>
<p style="text-align: center;"><code class="reqn">
    1/2 \|y - X \beta\|_2^2 + \lambda \|\beta\|_1.
  </code>
</p>

<p>In contrast, glmnet multiplies the first term by a factor of 1/n.
So after running glmnet, to extract the beta corresponding to a value lambda, 
you need to use <code>beta = coef(obj, s=lambda/n)[-1]</code>,
where obj is the object returned by glmnet (and [-1] removes the intercept,
which glmnet always puts in the first component)
</p>
</td></tr>      
<tr><td><code id="ROSI_+3A_lambda">lambda</code></td>
<td>

<p>Value of lambda used to compute beta. See the above warning
</p>
</td></tr> 
<tr><td><code id="ROSI_+3A_penalty_factor">penalty_factor</code></td>
<td>

<p>Penalty factor as used by glmnet.
Actual penalty used in solving the problem is 
</p>
<p style="text-align: center;"><code class="reqn">
\lambda \cdot \sum_{i=1}^p f_i |\beta_i|
</code>
</p>

<p>with f being the penalty_factor. Defaults to vector of 1s.
</p>
</td></tr>
<tr><td><code id="ROSI_+3A_dispersion">dispersion</code></td>
<td>

<p>Estimate of dispersion in the GLM. Can be taken to be
1 for logisitic and should be an estimate of the error variance
for the Gaussian.
</p>
</td></tr>
<tr><td><code id="ROSI_+3A_family">family</code></td>
<td>

<p>Family used for likelihood.
</p>
</td></tr>
<tr><td><code id="ROSI_+3A_solver">solver</code></td>
<td>

<p>Solver used to solve restricted problems needed to find truncation set.
Each active variable requires solving a new LASSO problem obtained
by zeroing out one coordinate of original problem. The &quot;QP&quot; choice
uses coordinate descent for a specific value of lambda, rather than glmnet which
would solve for a new path each time.
</p>
</td></tr>
<tr><td><code id="ROSI_+3A_construct_ci">construct_ci</code></td>
<td>

<p>Report confidence intervals or just p-values?
</p>
</td></tr>
<tr><td><code id="ROSI_+3A_debiasing_method">debiasing_method</code></td>
<td>

<p>Which method should be used for debiasing? Choices are &quot;JM&quot; (Javanmard, Montanari)
or &quot;BN&quot; (method described in arxiv.org/1703.03282).
</p>
</td></tr>
<tr><td><code id="ROSI_+3A_verbose">verbose</code></td>
<td>

<p>Print out progress along the way? Default is FALSE.
</p>
</td></tr>
<tr><td><code id="ROSI_+3A_level">level</code></td>
<td>

<p>Confidence level for intervals.
</p>
</td></tr>
<tr><td><code id="ROSI_+3A_use_debiased">use_debiased</code></td>
<td>

<p>Use the debiased estimate of the parameter or not. When FALSE, this is the
method desribed in arxiv.org/1801.09037. The default TRUE often
produces noticably shorter intervals and more powerful tests when
p is comparable to n. Ignored when n&lt;p and set to TRUE.
Also note that with &quot;BN&quot; as debiasing method and n &gt; p, this agrees with
method in arxiv.org/1801.09037.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>???
</p>


<h3>Value</h3>

  
<table>
<tr><td><code>active_set</code></td>
<td>
<p>Active set of LASSO.</p>
</td></tr>
<tr><td><code>pvalues</code></td>
<td>
<p>Two-sided P-values for active variables.</p>
</td></tr>
<tr><td><code>intervals</code></td>
<td>
<p>Confidence intervals</p>
</td></tr>
<tr><td><code>estimate</code></td>
<td>
<p>Relaxed (i.e. unshrunk) selected estimates.</p>
</td></tr>
<tr><td><code>std_err</code></td>
<td>
<p>Standard error of relaxed estimates (pre-selection).</p>
</td></tr>
<tr><td><code>dispersion</code></td>
<td>
<p>Dispersion parameter.</p>
</td></tr>
<tr><td><code>lower_trunc</code></td>
<td>
<p>Lower truncation point. The estimates should be outside the interval formed by the lower and upper truncation poitns.</p>
</td></tr>
<tr><td><code>upper_trunc</code></td>
<td>
<p>Lower truncation point. The estimates should be outside the interval formed by the lower and upper truncation poitns.</p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>
<p>Value of tuning parameter lambda used.</p>
</td></tr>
<tr><td><code>penalty_factor</code></td>
<td>
<p>Penalty factor used for solving problem.</p>
</td></tr>
<tr><td><code>level</code></td>
<td>
<p>Confidence level.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>The call to fixedLassoInf.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Jelena Markovic, Jonathan Taylor</p>


<h3>References</h3>

<p>Keli Liu, Jelena Markovic, Robert Tibshirani. More powerful post-selection 
inference, with application to the Lasso. arXiv:1801.09037
</p>
<p>Tom Boot, Didier Nibbering. Inference in high-dimensional linear regression models.
arXiv:1703.03282
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

library(selectiveInference)
library(glmnet)
set.seed(43)

n = 100
p = 200
s = 2
sigma = 1

x = matrix(rnorm(n*p),n,p)
x = scale(x,TRUE,TRUE)

beta = c(rep(10, s), rep(0,p-s)) / sqrt(n)
y = x %*% beta + sigma*rnorm(n)

# first run glmnet
gfit = glmnet(x,y,standardize=FALSE)

# extract coef for a given lambda; note the 1/n factor!
# (and we don't save the intercept term)
lambda = 4 * sqrt(n)
lambda_glmnet = 4 / sqrt(n)
beta = selectiveInference:::solve_problem_glmnet(x, 
                                                 y, 
                                                 lambda_glmnet, 
                                                 penalty_factor=rep(1, p),
                                                 family="gaussian")
# compute fixed lambda p-values and selection intervals
out = ROSI(x,
           y,
           beta,
           lambda,
           dispersion=sigma^2)
out

# an alternate approximate inverse from Boot and Nibbering

out = ROSI(x,
           y,
           beta,
           lambda,
           dispersion=sigma^2,
           debiasing_method="BN")
out

</code></pre>

<hr>
<h2 id='scaleGroups'>Center and scale design matrix by groups</h2><span id='topic+scaleGroups'></span>

<h3>Description</h3>

<p>For internal use by <code><a href="#topic+groupfs">groupfs</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>scaleGroups(x, index, center = TRUE, normalize = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="scaleGroups_+3A_x">x</code></td>
<td>
<p>Design matrix.</p>
</td></tr>
<tr><td><code id="scaleGroups_+3A_index">index</code></td>
<td>
<p>Group membership indicator of length p.</p>
</td></tr>
<tr><td><code id="scaleGroups_+3A_center">center</code></td>
<td>
<p>Center groups, default is TRUE.</p>
</td></tr>
<tr><td><code id="scaleGroups_+3A_normalize">normalize</code></td>
<td>
<p>Scale groups by Frobenius norm, default is TRUE.</p>
</td></tr>
</table>


<h3>Value</h3>


<dl>
<dt>x</dt><dd><p>Optionally centered/scaled design matrix.</p>
</dd>
<dt>xm</dt><dd><p>Means of groups in original design matrix.</p>
</dd>
<dt>xs</dt><dd><p>Frobenius norms of groups in original design matrix.</p>
</dd>
</dl>


<hr>
<h2 id='selectiveInference'>
Tools for selective inference
</h2><span id='topic+selectiveInference'></span>

<h3>Description</h3>

<p>Functions to perform post-selection inference for forward
stepwise regression, least angle regression, the lasso and the 
many normal means problem. The lasso function also supports logistic regression and the Cox model.
</p>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> selectiveInference</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL-2</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p>This package provides tools for inference after selection, in forward stepwise 
regression, least angle regression, the lasso, and the many normal means problem. 
The functions compute p-values and selection intervals that properly account for 
the inherent selection carried out by the procedure. These have exact finite sample 
type I error and coverage under Gaussian errors. For the logistic and Cox familes (fixedLassoInf),
the coverage is asymptotically valid
</p>
<p>This R package was developed as part of the selective inference software project
in Python and R:
</p>
<p><a href="https://github.com/selective-inference">https://github.com/selective-inference</a>
</p>
<p>Some of the R code in this work is a modification of Python code from this
repository. Here is the current selective inference software team:
</p>
<p>Yuval Benjamini, 
Leonard Blier, 
Will Fithian, 
Jason Lee, 
Joshua Loftus,
Joshua Loftus, Stephen Reid,
Dennis Sun,
Yuekai Sun,
Jonathan Taylor,
Xiaoying Tian,
Ryan Tibshirani,
Rob Tibshirani
</p>
<p>The main functions included in the package are:
<code><a href="#topic+fs">fs</a></code>,
<code><a href="#topic+fsInf">fsInf</a></code>,
<code><a href="#topic+lar">lar</a></code>,
<code><a href="#topic+larInf">larInf</a></code>,
<code><a href="#topic+fixedLassoInf">fixedLassoInf</a></code>,
<code><a href="#topic+manyMeans">manyMeans</a></code>
</p>


<h3>Author(s)</h3>

<p>Ryan Tibshirani, Rob Tibshirani, Jonathan Taylor, Joshua Loftus, Stephen Reid
</p>
<p>Maintainer: Rob Tibshirani &lt;tibs@stanford.edu&gt;
</p>


<h3>References</h3>

<p>Ryan Tibshirani, Jonathan Taylor, Richard Lockhart, and Rob Tibshirani (2014).
Exact post-selection inference for sequential regression procedures. arXiv:1401.3889.
</p>
<p>Jason Lee, Dennis Sun, Yuekai Sun, and Jonathan Taylor (2013). 
Exact post-selection inference, with application to the lasso. arXiv:1311.6238.
</p>
<p>Stephen Reid, Jonathan Taylor, and Rob Tibshirani (2014).
Post-selection point and interval estimation of signal sizes in Gaussian samples.
arXiv:1405.3340.
</p>
<p>Jonathan Taylor and Robert Tibshirani (2016) Post-selection inference for L1-penalized likelihood models.
arXiv:1602.07358
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(33)
n = 50
p = 10
sigma = 1
x = matrix(rnorm(n*p),n,p)
beta = c(3,2,rep(0,p-2))
y = x%*%beta + sigma*rnorm(n)

# run forward stepwise
fsfit = fs(x,y)

# compute sequential p-values and confidence intervals
# (sigma estimated from full model)
out.seq = fsInf(fsfit)
out.seq

# compute p-values and confidence intervals after AIC stopping
out.aic = fsInf(fsfit,type="aic")
out.aic

# compute p-values and confidence intervals after 5 fixed steps
out.fix = fsInf(fsfit,type="all",k=5)
out.fix

## NOT RUN---lasso at fixed lambda- Gaussian family
## first run glmnet
# gfit = glmnet(x,y)

## extract coef for a given lambda; note the 1/n factor!
## (and we don't save the intercept term)
# lambda = .1
# beta = coef(gfit, s=lambda/n, exact=TRUE)[-1]

## compute fixed lambda p-values and selection intervals
# out = fixedLassoInf(x,y,beta,lambda,sigma=sigma)
# out


#lasso at fixed lambda- logistic family
#set.seed(43)
  #   n = 50
  #   p = 10
  #   sigma = 1
     
 #    x = matrix(rnorm(n*p),n,p)
     x=scale(x,TRUE,TRUE)
  #   
#     beta = c(3,2,rep(0,p-2))
 #    y = x%*%beta + sigma*rnorm(n)
 #    y=1*(y&gt;mean(y))
     # first run glmnet
 #    gfit = glmnet(x,y,standardize=FALSE,family="binomial")
     
     # extract coef for a given lambda; note the 1/n factor!
     # (and here  we DO  include the intercept term)
 #    lambda = .8
 #    beta = coef(gfit, s=lambda/n, exact=TRUE)
     
 #    # compute fixed lambda p-values and selection intervals
 #    out = fixedLassoInf(x,y,beta,lambda,family="binomial")
 #    out

##lasso at fixed lambda- Cox family
#set.seed(43)
#     n = 50
 #    p = 10
 #    sigma = 1
     
 #    x = matrix(rnorm(n*p),n,p)
  #   x=scale(x,TRUE,TRUE)
     
 #    beta = c(3,2,rep(0,p-2))
 #    tim = as.vector(x%*%beta + sigma*rnorm(n))
  #   tim= tim-min(tim)+1
#status=sample(c(0,1),size=n,replace=T)
     # first run glmnet
   #  gfit = glmnet(x,Surv(tim,status),standardize=FALSE,family="cox")
     # extract coef for a given lambda; note the 1/n factor!
   
  #   lambda = 1.5
  #   beta = as.numeric(coef(gfit, s=lambda/n, exact=TRUE))
     
     # compute fixed lambda p-values and selection intervals
   #  out = fixedLassoInf(x,tim,beta,lambda,status=status,family="cox")
   #  out
## NOT RUN---many normal means
# set.seed(12345)
# n = 100 
# mu = c(rep(3,floor(n/5)), rep(0,n-floor(n/5))) 
# y = mu + rnorm(n)
# out = manyMeans(y, bh.q=0.1)
# out

## NOT RUN---forward stepwise with groups
# set.seed(1)
# n = 20
# p = 40
# x = matrix(rnorm(n*p), nrow=n)
# index = sort(rep(1:(p/2), 2))
# y = rnorm(n) + 2 * x[,1] - x[,4]
# fit = groupfs(x, y, index, maxsteps = 5)
# out = groupfsInf(fit)
# out

## NOT RUN---estimation of sigma for use in fsInf
## (or larInf or fixedLassoInf)
# set.seed(33)
# n = 50
# p = 10
# sigma = 1
# x = matrix(rnorm(n*p),n,p)
# beta = c(3,2,rep(0,p-2))
# y = x%*%beta + sigma*rnorm(n)

## run forward stepwise
# fsfit = fs(x,y)

## estimate sigma
# sigmahat = estimateSigma(x,y)$sigmahat

## run sequential inference with estimated sigma
# out = fsInf(fit,sigma=sigmahat)
# out
</code></pre>

<hr>
<h2 id='selectiveInference-internal'>Internal PMA functions</h2><span id='topic+print.fixedLassoInf'></span><span id='topic+print.fs'></span><span id='topic+print.fsInf'></span><span id='topic+print.larInf'></span><span id='topic+print.lar'></span><span id='topic+print.manyMeans'></span><span id='topic+print.ROSI'></span>

<h3>Description</h3>

<p>Internal selectiveInference functions</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'fs'
print(x, ...)
## S3 method for class 'fsInf'
print(x, tailarea = TRUE, ...)
## S3 method for class 'lar'
print(x,...)
## S3 method for class 'larInf'
print(x, tailarea = TRUE, ...)
## S3 method for class 'fixedLassoInf'
print(x, tailarea = TRUE, ...)
## S3 method for class 'manyMeans'
print(x, ...)
## S3 method for class 'ROSI'
print(x, ...)
</code></pre>


<h3>Author(s)</h3>

<p>Ryan Tibshirani, Rob Tibshirani, Jonathan Taylor, Joshua Loftus, Stephen Reid</p>

<hr>
<h2 id='TG.interval'>
Truncated Gaussian confidence interval.
</h2><span id='topic+TG.interval'></span>

<h3>Description</h3>

<p>Compute truncated Gaussian interval of Lee et al. (2016) with
arbitrary affine selection and covariance.
Z should satisfy A 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>TG.interval(Z, A, b, eta, Sigma=NULL, alpha=0.1, 
	   gridrange=c(-100,100),
           gridpts=100, 
           griddepth=2, 
           flip=FALSE, 
           bits=NULL) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="TG.interval_+3A_z">Z</code></td>
<td>

<p>Observed data (assumed to follow N(mu, Sigma) with sum(eta*mu)=null_value)
</p>
</td></tr>
<tr><td><code id="TG.interval_+3A_a">A</code></td>
<td>

<p>Matrix specifiying affine inequalities AZ &lt;= b
</p>
</td></tr>
<tr><td><code id="TG.interval_+3A_b">b</code></td>
<td>

<p>Offsets in the affine inequalities AZ &lt;= b.
</p>
</td></tr>      
<tr><td><code id="TG.interval_+3A_eta">eta</code></td>
<td>

<p>Determines the target sum(eta*mu) and estimate sum(eta*Z).
</p>
</td></tr> 
<tr><td><code id="TG.interval_+3A_sigma">Sigma</code></td>
<td>

<p>Covariance matrix of Z. Defaults to identity.
</p>
</td></tr> 
<tr><td><code id="TG.interval_+3A_alpha">alpha</code></td>
<td>

<p>Significance level for confidence intervals (target is miscoverage alpha/2 in each tail)
</p>
</td></tr>
<tr><td><code id="TG.interval_+3A_gridrange">gridrange</code></td>
<td>

<p>Grid range for constructing confidence intervals, on the standardized scale.
</p>
</td></tr>
<tr><td><code id="TG.interval_+3A_gridpts">gridpts</code></td>
<td>

<p>???????
</p>
</td></tr>
<tr><td><code id="TG.interval_+3A_griddepth">griddepth</code></td>
<td>

<p>???????
</p>
</td></tr>
<tr><td><code id="TG.interval_+3A_flip">flip</code></td>
<td>

<p>???????
</p>
</td></tr>
<tr><td><code id="TG.interval_+3A_bits">bits</code></td>
<td>

<p>Number of bits to be used for p-value and confidence interval calculations. Default is 
NULL, in which case standard floating point calculations are performed. When not NULL, 
multiple precision floating point calculations are performed with the specified number 
of bits, using the R package <code>Rmpfr</code> (if this package is not installed, then a 
warning is thrown, and standard floating point calculations are pursued).
Note: standard double precision uses 53 bits
so, e.g., a choice of 200 bits uses about 4 times double precision. The confidence
interval computation is sometimes numerically challenging, and the extra precision can be 
helpful (though computationally more costly). In particular, extra precision might be tried 
if the values in the output columns of <code>tailarea</code> differ noticeably from alpha/2.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function computes selective confidence intervals based on the polyhedral
lemma of Lee et al. (2016). 
</p>


<h3>Value</h3>

  
<table>
<tr><td><code>int</code></td>
<td>
<p>Selective confidence interval.</p>
</td></tr>
<tr><td><code>tailarea</code></td>
<td>
<p>Realized tail areas (lower and upper) for each confidence interval.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Ryan Tibshirani, Rob Tibshirani, Jonathan Taylor, Joshua Loftus, Stephen Reid</p>


<h3>References</h3>

<p>Jason Lee, Dennis Sun, Yuekai Sun, and Jonathan Taylor (2016). 
Exact post-selection inference, with application to the lasso. Annals of Statistics, 44(3), 907-927.
</p>
<p>Jonathan Taylor and Robert Tibshirani (2017) Post-selection inference for math L1-penalized likelihood models.
Canadian Journal of Statistics, xx, 1-21. (Volume still not posted)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>A = diag(5)
b = rep(1, 5)
Z = rep(0, 5)
Sigma = diag(5)
eta = as.numeric(c(1, 1, 0, 0, 0))
TG.interval(Z, A, b, eta, Sigma)
</code></pre>

<hr>
<h2 id='TG.limits'>
Truncation limits and standard deviation.
</h2><span id='topic+TG.limits'></span>

<h3>Description</h3>

<p>Compute truncated limits and SD for use in computing
p-values or confidence intervals of Lee et al. (2016).
Z should satisfy A 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>TG.limits(Z, A, b, eta, Sigma)  
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="TG.limits_+3A_z">Z</code></td>
<td>

<p>Observed data (assumed to follow N(mu, Sigma) with sum(eta*mu)=null_value)
</p>
</td></tr>
<tr><td><code id="TG.limits_+3A_a">A</code></td>
<td>

<p>Matrix specifiying affine inequalities AZ &lt;= b
</p>
</td></tr>
<tr><td><code id="TG.limits_+3A_b">b</code></td>
<td>

<p>Offsets in the affine inequalities AZ &lt;= b.
</p>
</td></tr>      
<tr><td><code id="TG.limits_+3A_eta">eta</code></td>
<td>

<p>Determines the target sum(eta*mu) and estimate sum(eta*Z).
</p>
</td></tr> 
<tr><td><code id="TG.limits_+3A_sigma">Sigma</code></td>
<td>

<p>Covariance matrix of Z. Defaults to identity.
</p>
</td></tr> 
</table>


<h3>Details</h3>

<p>This function computes the limits of truncation and the implied
standard deviation in the polyhedral lemma of Lee et al. (2016). 
</p>


<h3>Value</h3>

  
<table>
<tr><td><code>vlo</code></td>
<td>
<p>Lower truncation limits for statistic</p>
</td></tr>
<tr><td><code>vup</code></td>
<td>
<p>Upper truncation limits for statistic</p>
</td></tr>
<tr><td><code>sd</code></td>
<td>
<p>Standard error of sum(eta*Z)</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Ryan Tibshirani, Rob Tibshirani, Jonathan Taylor, Joshua Loftus, Stephen Reid</p>


<h3>References</h3>

<p>Jason Lee, Dennis Sun, Yuekai Sun, and Jonathan Taylor (2016). 
Exact post-selection inference, with application to the lasso. Annals of Statistics, 44(3), 907-927.
</p>
<p>Jonathan Taylor and Robert Tibshirani (2017) Post-selection inference for math L1-penalized likelihood models.
Canadian Journal of Statistics, xx, 1-21. (Volume still not posted)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>A = diag(5)
b = rep(1, 5)
Z = rep(0, 5)
Sigma = diag(5)
eta = as.numeric(c(1, 1, 0, 0, 0))
TG.limits(Z, A, b, eta, Sigma)
</code></pre>

<hr>
<h2 id='TG.pvalue'>
Truncated Gaussian p-value.
</h2><span id='topic+TG.pvalue'></span>

<h3>Description</h3>

<p>Compute truncated Gaussian p-value of Lee et al. (2016) with
arbitrary affine selection and covariance.
Z should satisfy A 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>TG.pvalue(Z, A, b, eta, Sigma, null_value=0, bits=NULL)  
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="TG.pvalue_+3A_z">Z</code></td>
<td>

<p>Observed data (assumed to follow N(mu, Sigma) with sum(eta*mu)=null_value)
</p>
</td></tr>
<tr><td><code id="TG.pvalue_+3A_a">A</code></td>
<td>

<p>Matrix specifiying affine inequalities AZ &lt;= b
</p>
</td></tr>
<tr><td><code id="TG.pvalue_+3A_b">b</code></td>
<td>

<p>Offsets in the affine inequalities AZ &lt;= b.
</p>
</td></tr>      
<tr><td><code id="TG.pvalue_+3A_eta">eta</code></td>
<td>

<p>Determines the target sum(eta*mu) and estimate sum(eta*Z).
</p>
</td></tr> 
<tr><td><code id="TG.pvalue_+3A_sigma">Sigma</code></td>
<td>

<p>Covariance matrix of Z. Defaults to identity.
</p>
</td></tr> 
<tr><td><code id="TG.pvalue_+3A_null_value">null_value</code></td>
<td>
<p>Hypothesized value of sum(eta*mu) under the null.
</p>
</td></tr>
<tr><td><code id="TG.pvalue_+3A_bits">bits</code></td>
<td>

<p>Number of bits to be used for p-value and confidence interval calculations. Default is 
NULL, in which case standard floating point calculations are performed. When not NULL, 
multiple precision floating point calculations are performed with the specified number 
of bits, using the R package <code>Rmpfr</code> (if this package is not installed, then a 
warning is thrown, and standard floating point calculations are pursued).
Note: standard double precision uses 53 bits
so, e.g., a choice of 200 bits uses about 4 times double precision. The confidence
interval computation is sometimes numerically challenging, and the extra precision can be 
helpful (though computationally more costly). In particular, extra precision might be tried 
if the values in the output columns of <code>tailarea</code> differ noticeably from alpha/2.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function computes selective p-values based on the polyhedral
lemma of Lee et al. (2016). 
</p>


<h3>Value</h3>

  
<table>
<tr><td><code>pv</code></td>
<td>
<p>One-sided P-values for active variables, uses the fact we have conditioned on the sign.</p>
</td></tr>
<tr><td><code>vlo</code></td>
<td>
<p>Lower truncation limits for statistic</p>
</td></tr>
<tr><td><code>vup</code></td>
<td>
<p>Upper truncation limits for statistic</p>
</td></tr>
<tr><td><code>sd</code></td>
<td>
<p>Standard error of sum(eta*Z)</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Ryan Tibshirani, Rob Tibshirani, Jonathan Taylor, Joshua Loftus, Stephen Reid</p>


<h3>References</h3>

<p>Jason Lee, Dennis Sun, Yuekai Sun, and Jonathan Taylor (2016). 
Exact post-selection inference, with application to the lasso. Annals of Statistics, 44(3), 907-927.
</p>
<p>Jonathan Taylor and Robert Tibshirani (2017) Post-selection inference for math L1-penalized likelihood models.
Canadian Journal of Statistics, xx, 1-21. (Volume still not posted)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
A = diag(5)
b = rep(1, 5)
Z = rep(0, 5)
Sigma = diag(5)
eta = as.numeric(c(1, 1, 0, 0, 0))
TG.pvalue(Z, A, b, eta, Sigma)
TG.pvalue(Z, A, b, eta, Sigma, null_value=1)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
