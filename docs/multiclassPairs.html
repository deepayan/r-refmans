<!DOCTYPE html><html><head><title>Help for package multiclassPairs</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {multiclassPairs}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#do_dunn_test'>
<p>internal function</p></a></li>
<li><a href='#filter_genes_TSP'>
<p>Filter genes/features for multiclass one-vs-rest classifier downstream training</p></a></li>
<li><a href='#group_TSP'>
<p>Internal function: for grouping labels for one-vs-rest usage</p></a></li>
<li><a href='#optimize_RF'>
<p>Optimize parameters to be used in training the final RF model</p></a></li>
<li><a href='#plot_binary_RF'>
<p>Plot binary rule-based heatmaps</p></a></li>
<li><a href='#plot_binary_TSP'>
<p>Plot binary rule-based heatmaps</p></a></li>
<li><a href='#predict_one_vs_rest_TSP'>
<p>Predict sample class based on one-vs-rest multiclass top score pairs classifier</p></a></li>
<li><a href='#predict_RF'>
<p>Predict sample class based on gene pair-based random forest classifier</p></a></li>
<li><a href='#print-methods'><p>Methods for Function <code>print</code> in Package <span class="pkg">multiclassPairs</span></p></a></li>
<li><a href='#proximity_matrix_RF'>
<p>Plot binary rule-based heatmaps</p></a></li>
<li><a href='#ReadData'>
<p>Function for preparing data object</p></a></li>
<li><a href='#sort_genes_RF'>
<p>Sort genes/features for pair-based random forest classifier downstream steps</p></a></li>
<li><a href='#sort_rules_RF'>
<p>Create and sort feature/gene pairs for pair-based random forest classifier training step</p></a></li>
<li><a href='#summary_genes_RF'>
<p>Summarize sorted genes to rules</p></a></li>
<li><a href='#train_one_vs_rest_TSP'>
<p>Build multiclass rule-based classifier as one-vs-rest scheme</p></a></li>
<li><a href='#train_RF'>
<p>Train pair-based random forest model</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Build MultiClass Pair-Based Classifiers using TSPs or RF</td>
</tr>
<tr>
<td>Version:</td>
<td>0.4.3</td>
</tr>
<tr>
<td>Author:</td>
<td>Nour-al-dain Marzouka</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Nour-al-dain Marzouka &lt;Nour-al-dain.Marzouka@med.lu.se&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>A toolbox to train a single sample classifier that uses in-sample feature relationships. The relationships are represented as feature1 &lt; feature2 (e.g. gene1 &lt; gene2). We provide two options to go with. First is based on 'switchBox' package which uses Top-score pairs algorithm. Second is a novel implementation based on random forest algorithm. For simple problems we recommend to use one-vs-rest using TSP option due to its simplicity and for being easy to interpret.  For complex problems RF performs better.  Both lines filter the features first then combine the filtered features to make the list of all the possible rules (i.e. rule1: feature1 &lt; feature2, rule2: feature1 &lt; feature3, etc...).  Then the list of rules will be filtered and the most important and informative rules will be kept. The informative rules will be assembled in an one-vs-rest model or in an RF model.  We provide a detailed description with each function in this package to explain the filtration and training methodology in each line. Reference: Marzouka &amp; Eriksson (2021) &lt;<a href="https://doi.org/10.1093%2Fbioinformatics%2Fbtab088">doi:10.1093/bioinformatics/btab088</a>&gt;.</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/NourMarzouka/multiclassPairs">https://github.com/NourMarzouka/multiclassPairs</a></td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>biocViews:</td>
<td>Classification</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.0.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>methods, utils, stats, graphics, grDevices, ranger, Boruta,
dunn.test, caret, e1071, rdist</td>
</tr>
<tr>
<td>Suggests:</td>
<td>BiocManager, Biobase, switchBox, knitr, rmarkdown, BiocStyle,
leukemiasEset, qpdf</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.1</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2021-05-16 20:32:52 UTC; no5462ma</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2021-05-16 22:30:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='do_dunn_test'>
internal function
</h2><span id='topic+do_dunn_test'></span>

<h3>Description</h3>

<p>It loops dunn.test from dunn.test package to perform pairwise
comparison between groups for each gene in the data
</p>


<h3>Author(s)</h3>

<p>Nour-al-dain Marzouka &lt;nour-al-dain.marzouka at med.lu.se&gt;
</p>

<hr>
<h2 id='filter_genes_TSP'>
Filter genes/features for multiclass one-vs-rest classifier downstream training
</h2><span id='topic+filter_genes_TSP'></span>

<h3>Description</h3>

<p><code>filter_genes_TSP</code> filters genes/features prior of downstream training steps that will involve top scores pairs using switchBox package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>filter_genes_TSP(data_object,
                filter = c("one_vs_one", "one_vs_rest"),
                platform_wise = FALSE,
                featureNo = 1000,
                UpDown = TRUE,
                verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="filter_genes_TSP_+3A_data_object">data_object</code></td>
<td>

<p>data object generated by ReadData function. Object contains the data and labels.
</p>
</td></tr>
<tr><td><code id="filter_genes_TSP_+3A_filter">filter</code></td>
<td>

<p>a character indicating the method of comparison to be used in the filtering.
Should be &quot;one_vs_one&quot; or &quot;one_vs_rest&quot;.
</p>
</td></tr>
<tr><td><code id="filter_genes_TSP_+3A_platform_wise">platform_wise</code></td>
<td>

<p>a logical indicating if the comparisons will be done in each platform alone then the features should be ranked high in all platforms to be selected. If TRUE then the data object should contain platform vector to be used in splitting samples based on the platform before the comparisons</p>
</td></tr>
<tr><td><code id="filter_genes_TSP_+3A_featureno">featureNo</code></td>
<td>

<p>an integer indicating the number of features to be returned after the filtering per class
</p>
</td></tr>
<tr><td><code id="filter_genes_TSP_+3A_updown">UpDown</code></td>
<td>

<p>a logical value indicating whether an equal number of up and down genes should be considered. If FALSE then the number of features will be collected regardless of the portion of the up genes and down genes
</p>
</td></tr>
<tr><td><code id="filter_genes_TSP_+3A_verbose">verbose</code></td>
<td>

<p>a logical value indicating whether processing messages will be printed or not. Default is TRUE.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Input data will be ranked (rank the features inside each sample) before appling the comparison methods. Sufficient number of returned features is recommended if large number of rules is used in the downstream training steps.
</p>
<p>For one vs rest comparisons, Wilcoxon test will be performed through SWAP.Filter.Wilcoxon function from switchBox package. For one vs one comparisons, dunn test will be performed through dunn.test function from dunn.test package, and this method is recommended in case of class imbalance or if the classes are so close to each other in their properties.
</p>
<p>P-values from dunn test are ranked in each one vs one comparison then the ranks are combined, the selected genes should be ranked at the top in all comparisons.
</p>
<p>If platform-wise is TRUE, then the gene that is ranked high in all comparisons and in all platforms will be selected.
</p>
<p>For example, if we have five classes (i.e. C1-C5), and dunn.test was performed, then C1 will have four comparison agains C2-C5 (so four lists of p-values), pvalues will be ranked (smaller number means smaller p.value) in each list, and the gene that is ranked (5,5,5,5) will be prioritized over the gene that is ranked (1,1,1,6), and in case we have two platforms and we truned the platform-wise to TRUE then we will have 8 lists of p-values, and the top genes will be selected in the same way. And this is apply also on the platform-wise one-vs-rest comparison. So in brief, the lowest rank in the different list will determine the position of the gene in the output.
</p>
<p>Other p-value combining methods could be added in the future.
</p>


<h3>Value</h3>

<p><code>OnevsrestScheme_genes_TSP</code> object that contains the names of the top filtered features for each class
</p>


<h3>Author(s)</h3>

<p>Nour-al-dain Marzouka &lt;nour-al-dain.marzouka at med.lu.se&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'># random data
Data &lt;- matrix(runif(10000), nrow=100, ncol=100,
               dimnames = list(paste0("G",1:100), paste0("S",1:100)))

# labels
L &lt;- sample(x = c("A","B","C"), size = 100, replace = TRUE)

# study/platform
P &lt;- sample(c("P1","P2"), size = 100, replace = TRUE)

object &lt;- ReadData(Data = Data,
             Labels = L,
                   Platform = P)

# not to run
# switchBox package from Bioconductor is needed
# Visit their website or install switchBox package using:
# if(!requireNamespace("switchBox", quietly = TRUE)){
#       if (!requireNamespace('BiocManager', quietly = TRUE)) {
#       install.packages('BiocManager')
#      }
#      BiocManager::install('switchBox')", call. = FALSE)
#  }

# filtered_genes &lt;- filter_genes_TSP(data_object = object,
#                                  filter = "one_vs_rest",
#                                  platform_wise = FALSE,
#                                  featureNo = 10,
#                                  UpDown = TRUE,
#                                  verbose = FALSE)

# training
# classifier &lt;- train_one_vs_rest_TSP(data_object = object,
#                              filtered_genes = filtered_genes,
#                              k_range = 10:50,
#                              include_pivot = FALSE,
#                              one_vs_one_scores = FALSE,
#                              platform_wise_scores = FALSE,
#                              seed = 1234,
#                              verbose = FALSE)

# results &lt;- predict_one_vs_rest_TSP(classifier = classifier,
#                                   Data = object,
#                                   tolerate_missed_genes = TRUE,
#                                   weighted_votes = TRUE,
#                                   verbose = FALSE)

# Confusion Matrix and Statistics on training data
#  caret::confusionMatrix(data = factor(results$max_score, levels = unique(L)),
#                         reference = factor(L, levels = unique(L)),
#                         mode="everything")

# plot_binary_TSP(Data = object, classes=c("A","B","C"),
#                 classifier = classifier,
#                 prediction = results,
#                 title = "Test")
</code></pre>

<hr>
<h2 id='group_TSP'>
Internal function: for grouping labels for one-vs-rest usage</h2><span id='topic+group_TSP'></span>

<h3>Description</h3>

<p>Used to convert labels to factor to be used by switchBox package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>group_TSP(label, my_group)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="group_TSP_+3A_label">label</code></td>
<td>

<p>a vector indicating multi classes</p>
</td></tr>
<tr><td><code id="group_TSP_+3A_my_group">my_group</code></td>
<td>

<p>character indicate the wanted class
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a factor contains two levels one is the wanted class and the other is &quot;rest&quot; that represent any other class other than the wanted class
</p>


<h3>Author(s)</h3>

<p>Nour-al-dain Marzouka &lt;nour-al-dain.marzouka at med.lu.se&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>L &lt;- sample(x = c("A","B","C","D"), size = 1000, replace = TRUE)

</code></pre>

<hr>
<h2 id='optimize_RF'>
Optimize parameters to be used in training the final RF model</h2><span id='topic+optimize_RF'></span>

<h3>Description</h3>

<p>optimize_RF takes a different sets of parameters to be used as training parameters. optimize_RF passes each set of the parameters to train_RF function, then optimize_RF returns the accuracies and related measurements (i.e. number of used genes and rules) for each trained RF model based on each set of parameters. Accuracies can be calculated based on the training data or by applying the trained RF model on another testing data.</p>


<h3>Usage</h3>

<pre><code class='language-R'>optimize_RF(data_object,
            sorted_rules_RF,
            parameters,
            overall = c("Accuracy", "Kappa", "AccuracyLower",
                        "AccuracyUpper", "AccuracyNull", "AccuracyPValue",
                        "McnemarPValue")[1:2],
            byclass = c("Sensitivity", "Specificity",
                        "Pos Pred Value", "Neg Pred Value",
                        "Precision", "Recall", "F1", "Prevalence",
                        "Detection Rate", "Detection Prevalence",
                        "Balanced Accuracy" )[c(11)],
            seed = 123456,
            test_object = NULL,
            impute = TRUE,
            impute_reject = 0.67,
            verbose = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="optimize_RF_+3A_data_object">data_object</code></td>
<td>

<p>Data object with labels generated by ReadData function</p>
</td></tr>
<tr><td><code id="optimize_RF_+3A_sorted_rules_rf">sorted_rules_RF</code></td>
<td>

<p>sorted rules object generated by sort_rules_RF function</p>
</td></tr>
<tr><td><code id="optimize_RF_+3A_parameters">parameters</code></td>
<td>

<p>a dataframe with the variables that the RF model will be trained based on. Column names should match arguments used in train_RF function. Each row represents one trial (model), e.g. a dataframe with 10 rows means you want to check the performance of 10 different RF models based on 10 different set of parameters.</p>
</td></tr>
<tr><td><code id="optimize_RF_+3A_overall">overall</code></td>
<td>

<p>a vector with the names of the overall performance measurements to be reported in the summary table in results. It can be one or more of these measurements: &quot;Accuracy&quot;, &quot;Kappa&quot;, &quot;AccuracyLower&quot;, &quot;AccuracyUpper&quot;, &quot;AccuracyNull&quot;, &quot;AccuracyPValue&quot;, &quot;McnemarPValue&quot;. Default is c(&quot;Accuracy&quot;, &quot;Kappa&quot;). These masurements based on confusionMatrix function output in caret package.</p>
</td></tr>
<tr><td><code id="optimize_RF_+3A_byclass">byclass</code></td>
<td>

<p>a vector with the names of the performance measurements for individual classes to be reported in the summary table in results. It can be one or more of these measurements: &quot;Sensitivity&quot;, &quot;Specificity&quot;, &quot;Pos Pred Value&quot;, &quot;Neg Pred Value&quot;, &quot;Precision&quot;, &quot;Recall&quot;, &quot;F1&quot;, &quot;Prevalence&quot;, &quot;Detection Rate&quot;, &quot;Detection Prevalence&quot;, &quot;Balanced Accuracy&quot;. Default is &quot;Balanced Accuracy&quot;. These masurements based on confusionMatrix function output in caret package.</p>
</td></tr>
<tr><td><code id="optimize_RF_+3A_seed">seed</code></td>
<td>

<p>seed to be used in the training process for reproducibility.</p>
</td></tr>
<tr><td><code id="optimize_RF_+3A_test_object">test_object</code></td>
<td>

<p>data object with labels generated by ReadData to be used as testing data. If this object is provided then the accuracies and performance results will be based on this object not the training data.</p>
</td></tr>
<tr><td><code id="optimize_RF_+3A_impute">impute</code></td>
<td>

<p>logical to be passed to predict_RF when test_object is used. To impute missed genes and NA values in test_object. Default is TRUE.</p>
</td></tr>
<tr><td><code id="optimize_RF_+3A_impute_reject">impute_reject</code></td>
<td>

<p>a number between 0 and 1 to be passed to predict_RF when test_object is used. It indicate the threshold of the missed rules in the sample. Based on this threshold the sample will be rejected (i.e. skipped) and the missed rules will not be imputed in this sample. Default is 0.67.</p>
</td></tr>
<tr><td><code id="optimize_RF_+3A_verbose">verbose</code></td>
<td>

<p>a logical value indicating whether processing messages will be printed or not. Default is FALSE.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>optimize_RF</code> helps the user to optimize parameters to be used in <code>train_RF</code> function for a given training dataset.
</p>


<h3>Value</h3>

<p>return optimize_RF_output object which is a list caintains:
</p>
<table>
<tr><td><code>summary</code></td>
<td>
<p>dataframe contains the input parameters, number of genes and rules in the model, and the selected overall and by class performance measurements. Each trials (i.e. set of parameters) as on row.</p>
</td></tr>
<tr><td><code>confusionMatrix</code></td>
<td>
<p>list of confusionMatrix objects generated by caret package, which contains the fulloverall and by class performance for each trial</p>
</td></tr>
<tr><td><code>errors</code></td>
<td>
<p>list of errors generated by trials</p>
</td></tr>
<tr><td><code>calls</code></td>
<td>
<p>the call which used to generate this object.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Nour-al-dain Marzouka &lt;nour-al-dain.marzouka at med.lu.se&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'># generate random data
Data &lt;- matrix(runif(8000), nrow=100, ncol=80,
               dimnames = list(paste0("G",1:100), paste0("S",1:80)))

# generate random labels
L &lt;- sample(x = c("A","B","C","D"), size = 80, replace = TRUE)

# generate random platform labels
P &lt;- sample(c("P1","P2","P3"), size = 80, replace = TRUE)

# create data object
object &lt;- ReadData(Data = Data,
                   Labels = L,
                   Platform = P,
                   verbose = FALSE)

# sort genes
genes_RF &lt;- sort_genes_RF(data_object = object,
                          seed=123456, verbose = FALSE)

# to get an idea of how many genes we will use
# and how many rules will be generated
# summary_genes_RF(sorted_genes_RF = genes_RF,
#                  genes_altogether = c(10,20,50,100,150,200),
#                  genes_one_vs_rest = c(10,20,50,100,150,200))

# creat and sort rules
# rules_RF &lt;- sort_rules_RF(data_object = object,
#                           sorted_genes_RF = genes_RF,
#                           genes_altogether = 100,
#                           genes_one_vs_rest = 100,
#                           seed=123456,
#                           verbose = FALSE)

# parameters &lt;- data.frame(
#   gene_repetition=c(3,2,1),
#   rules_one_vs_rest=0,
#   rules_altogether=c(2,3,10),
#   run_boruta=c(FALSE,"produce_error",FALSE),
#   plot_boruta = FALSE,
#   num.trees=c(100,200,300),
#   stringsAsFactors = FALSE)
# parameters

# Or you can use expand.grid to generate dataframe with all parameter combinations
# parameters &lt;- expand.grid(
#   gene_repetition=c(3,2,1),
#   rules_one_vs_rest=0,
#   rules_altogether=c(2,3,10),
#   num.trees=c(100,500,1000),
#   stringsAsFactors = FALSE)
# parameters


# test &lt;- optimize_RF(data_object = object,
#                     sorted_rules_RF = rules_RF,
#                     test_object = NULL,
#                     overall = c("Accuracy"),
#                     byclass = NULL, verbose = FALSE,
#                     parameters = parameters)
# test
# test$summary[which.max(test$summary$Accuracy),]
#
# # train the final model
# # it is preferred to increase the number of trees and rules in case you have
# # large number of samples and features
# # for quick example, we have small number of trees and rules here
# # based on the optimize_RF results we will select the parameters
# RF_classifier &lt;- train_RF(data_object = object,
#                           gene_repetition = 1,
#                           rules_altogether = 0,
#                           rules_one_vs_rest = 10,
#                           run_boruta = FALSE,
#                           plot_boruta = FALSE,
#                           probability = TRUE,
#                           num.trees = 300,
#                           sorted_rules_RF = rules_RF,
#                           boruta_args = list(),
#                           verbose = TRUE)
#
# # training accuracy
# # get the prediction labels
# # if the classifier trained using probability	= FALSE
# training_pred &lt;- RF_classifier$RF_scheme$RF_classifier$predictions
# if (is.factor(training_pred)) {
#   x &lt;- as.character(training_pred)
# }
#
# # if the classifier trained using probability	= TRUE
# if (is.matrix(training_pred)) {
#   x &lt;- colnames(training_pred)[max.col(training_pred)]
# }
#
# # training accuracy
# caret::confusionMatrix(data =factor(x),
#                 reference = factor(object$data$Labels),
#                 mode = "everything")

# not to run
# visualize the binary rules in training dataset
# plot_binary_RF(Data = object,
#                classifier = RF_classifier,
#                prediction = NULL, as_training = TRUE,
#                show_scores = TRUE,
#                top_anno = "ref",
#                show_predictions = TRUE,
#                title = "Training data")

# not to run
# Extract and plot the proximity matrix from the classifier for the training data
# it takes long time for large data
# proximity_mat &lt;- proximity_matrix_RF(object = object,
#                       classifier = RF_classifier,
#                       plot=TRUE,
#                       return_matrix=TRUE,
#                       title = "Test",
#                       cluster_cols = TRUE)

# not to run
# predict
# test_object # any test data
# results &lt;- predict_RF(classifier = RF_classifier, impute = TRUE,
#                       Data = test_object)
#
# # visualize the binary rules in training dataset
# plot_binary_RF(Data = test_object,
#                classifier = RF_classifier,
#                prediction = results, as_training = FALSE,
#                show_scores = TRUE,
#                top_anno = "ref",
#                show_predictions = TRUE,
#                title = "Test data")
</code></pre>

<hr>
<h2 id='plot_binary_RF'>
Plot binary rule-based heatmaps
</h2><span id='topic+plot_binary_RF'></span>

<h3>Description</h3>

<p><code>plot_binary_RF</code> Plot binary heatmaps for datasets based on rule-based random forest classifier
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  plot_binary_RF(Data,
                  classifier,
                  ref              = NULL,
                  prediction       = NULL,
                  as_training      = FALSE,
                  platform         = NULL,
                  classes          = NULL,
                  platforms_ord    = NULL,
                  top_anno         = c("ref", "prediction", "platform")[1],
                  title            = "",
                  binary_col       = c("white", "black", "gray"),
                  ref_col          = NULL,
                  pred_col         = NULL,
                  platform_col     = NULL,
                  show_ref         = TRUE,
                  show_predictions = TRUE,
                  show_platform    = TRUE,
                  show_scores      = TRUE,
                  show_rule_name   = TRUE,
                  legend           = TRUE,
                  cluster_cols     = TRUE,
                  cluster_rows     = TRUE,
                  anno_height      = 0.03,
                  score_height     = 0.03,
                  margin           = c(0, 5, 0, 5))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_binary_RF_+3A_data">Data</code></td>
<td>

<p>a matrix or a dataframe, samples as columns and row as features/genes. Can also take ExpressionSet, or data_object generated by ReadData function.
</p>
</td></tr>
<tr><td><code id="plot_binary_RF_+3A_classifier">classifier</code></td>
<td>

<p>Classifier as a rule_based_RandomForest object, generated by <code>train_RF</code> function
</p>
</td></tr>
<tr><td><code id="plot_binary_RF_+3A_ref">ref</code></td>
<td>

<p>Optional vector with the reference labels. Ref labels in data_object will be used if not ref input provided. For ExpressionSet, the name of the ref variable in the pheno data can be used.
</p>
</td></tr>
<tr><td><code id="plot_binary_RF_+3A_prediction">prediction</code></td>
<td>

<p>Optional. &quot;ranger.prediction&quot; object for the class scores generated by <code>predict_RF</code> function.
</p>
</td></tr>
<tr><td><code id="plot_binary_RF_+3A_as_training">as_training</code></td>
<td>

<p>Logical indicates if the plot is for the training data. It means the predictions will be extracted from the classifier itself and any prediction object will be ignored. If TRUE, then the training data/object should be used for Data argument.
</p>
</td></tr>
<tr><td><code id="plot_binary_RF_+3A_platform">platform</code></td>
<td>

<p>Optional vector with the platform/study labels or any additional annotation. Platform labels in data_object will be used if no platform input is provided. For ExpressionSet, the name of the variable in the pheno data can be used.
</p>
</td></tr>
<tr><td><code id="plot_binary_RF_+3A_classes">classes</code></td>
<td>

<p>Optional vector with class names. This will determine which classes will be plotted and in which order. It is not recommended to use both &quot;classes&quot; and &quot;platforms_ord&quot; arguments together to avoid sample order conflict and may result in an improper plotting for samples.
</p>
</td></tr>
<tr><td><code id="plot_binary_RF_+3A_platforms_ord">platforms_ord</code></td>
<td>

<p>Optional vector with the platform/study names. This will determine which platform/study will be plotted and in which order. This will be used when top_anno=&quot;platform&quot;. It is not recommended to use both &quot;classes&quot; and &quot;platforms_ord&quot; arguments together.
</p>
</td></tr>
<tr><td><code id="plot_binary_RF_+3A_top_anno">top_anno</code></td>
<td>

<p>Determine the top annotation level. Samples will be grouped based on the top_anno. Input can be one of three options: &quot;ref&quot;, &quot;prediction&quot;, &quot;platform&quot;. Default is &quot;ref&quot;.
</p>
</td></tr>
<tr><td><code id="plot_binary_RF_+3A_title">title</code></td>
<td>

<p>Character input as a title for the whole heatmap. Default is &quot;&quot;.
</p>
</td></tr>
<tr><td><code id="plot_binary_RF_+3A_binary_col">binary_col</code></td>
<td>

<p>Vector determines the colors of the binary heatmap. Default is c(&quot;white&quot;, &quot;black&quot;, &quot;gray&quot;). First color for the color when the rule is false in the sample. Second color for the color when the rule is true. Third color is for NAs.
</p>
</td></tr>
<tr><td><code id="plot_binary_RF_+3A_ref_col">ref_col</code></td>
<td>

<p>Optional named vector determines the colors of classes for the reference labels. Default is NULL. Vector names should match with the ref labels.
</p>
</td></tr>
<tr><td><code id="plot_binary_RF_+3A_pred_col">pred_col</code></td>
<td>

<p>Optional named vector determines the colors of classes for the prediction labels. Default is NULL. Vector names should match with the prediction labels in the prediction labels.
</p>
</td></tr>
<tr><td><code id="plot_binary_RF_+3A_platform_col">platform_col</code></td>
<td>

<p>Optional named vector determines the colors of platforms/study labels. Default is NULL. Vector names should match with the platforms/study labels.
</p>
</td></tr>
<tr><td><code id="plot_binary_RF_+3A_show_ref">show_ref</code></td>
<td>

<p>Logical. Determines if the ref labels will be plotted or not. If the top_anno argument is &quot;ref&quot; then show_ref will be ignored and ref labels will be plotted.
</p>
</td></tr>
<tr><td><code id="plot_binary_RF_+3A_show_predictions">show_predictions</code></td>
<td>

<p>Logical. Determines if the prediction labels will be plotted or not. If the top_anno argument is &quot;prediction&quot; then show_predictions will be ignored and predictions will be plotted.
</p>
</td></tr>
<tr><td><code id="plot_binary_RF_+3A_show_platform">show_platform</code></td>
<td>

<p>Logical. Determines if the platform/study labels will be plotted or not. If the top_anno argument is &quot;platform&quot; then show_platform will be ignored and platforms will be plotted.
</p>
</td></tr>
<tr><td><code id="plot_binary_RF_+3A_show_scores">show_scores</code></td>
<td>

<p>Logical. Determines if the prediction scores will be plotted or not. To visualize scores, the classifier should be trained with probability=TRUE otherwise show_scores will be turned FALSE automatically.
</p>
</td></tr>
<tr><td><code id="plot_binary_RF_+3A_show_rule_name">show_rule_name</code></td>
<td>

<p>Logical. Determines if the rule names will be plotted on the left side of the heatmapp or not.
</p>
</td></tr>
<tr><td><code id="plot_binary_RF_+3A_legend">legend</code></td>
<td>

<p>Logical. Determines if a legend will be plotted under the heatmap.
</p>
</td></tr>
<tr><td><code id="plot_binary_RF_+3A_cluster_cols">cluster_cols</code></td>
<td>

<p>Logical. Clustering the samples in each class (i.e. not all samples in the cohort) based on the binary rules for that class. If top_anno is &quot;platform&quot; then the rules from all classes are used to cluster the samples in each platform.
</p>
</td></tr>
<tr><td><code id="plot_binary_RF_+3A_cluster_rows">cluster_rows</code></td>
<td>

<p>Logical. Clustering the rules in each class.
</p>
</td></tr>
<tr><td><code id="plot_binary_RF_+3A_anno_height">anno_height</code></td>
<td>

<p>Determines the height of the annotations. It is recommended not to go out of this range 0.01&lt;height&lt;0.1. Default is 0.03.
</p>
</td></tr>
<tr><td><code id="plot_binary_RF_+3A_score_height">score_height</code></td>
<td>

<p>Determines the height of the score bars. It is recommended not to go out of this range 0.01&lt;height&lt;0.1. Default is 0.03.
</p>
</td></tr>
<tr><td><code id="plot_binary_RF_+3A_margin">margin</code></td>
<td>

<p>Determines the margins of the heatmap. Default is c(0, 5, 0, 5).
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>returns a heatmap plot for the binary rule
</p>


<h3>Author(s)</h3>

<p>Nour-al-dain Marzouka &lt;nour-al-dain.marzouka at med.lu.se&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'># generate random data
Data &lt;- matrix(runif(8000), nrow=100, ncol=80,
               dimnames = list(paste0("G",1:100), paste0("S",1:80)))

# generate random labels
L &lt;- sample(x = c("A","B","C","D"), size = 80, replace = TRUE)

# generate random platform labels
P &lt;- sample(c("P1","P2","P3"), size = 80, replace = TRUE)

# create data object
object &lt;- ReadData(Data = Data,
                   Labels = L,
                   Platform = P,
                   verbose = FALSE)

# sort genes
genes_RF &lt;- sort_genes_RF(data_object = object,
                          seed=123456, verbose = FALSE)

# to get an idea of how many genes we will use
# and how many rules will be generated
# summary_genes_RF(sorted_genes_RF = genes_RF,
#                  genes_altogether = c(10,20,50,100,150,200),
#                  genes_one_vs_rest = c(10,20,50,100,150,200))

# creat and sort rules
# rules_RF &lt;- sort_rules_RF(data_object = object,
#                           sorted_genes_RF = genes_RF,
#                           genes_altogether = 100,
#                           genes_one_vs_rest = 100,
#                           seed=123456,
#                           verbose = FALSE)

# parameters &lt;- data.frame(
#   gene_repetition=c(3,2,1),
#   rules_one_vs_rest=0,
#   rules_altogether=c(2,3,10),
#   run_boruta=c(FALSE,"produce_error",FALSE),
#   plot_boruta = FALSE,
#   num.trees=c(100,200,300),
#   stringsAsFactors = FALSE)
# parameters

# Or you can use expand.grid to generate dataframe with all parameter combinations
# parameters &lt;- expand.grid(
#   gene_repetition=c(3,2,1),
#   rules_one_vs_rest=0,
#   rules_altogether=c(2,3,10),
#   num.trees=c(100,500,1000),
#   stringsAsFactors = FALSE)
# parameters


# test &lt;- optimize_RF(data_object = object,
#                     sorted_rules_RF = rules_RF,
#                     test_object = NULL,
#                     overall = c("Accuracy"),
#                     byclass = NULL, verbose = FALSE,
#                     parameters = parameters)
# test
# test$summary[which.max(test$summary$Accuracy),]
#
# # train the final model
# # it is preferred to increase the number of trees and rules in case you have
# # large number of samples and features
# # for quick example, we have small number of trees and rules here
# # based on the optimize_RF results we will select the parameters
# RF_classifier &lt;- train_RF(data_object = object,
#                           gene_repetition = 1,
#                           rules_altogether = 0,
#                           rules_one_vs_rest = 10,
#                           run_boruta = FALSE,
#                           plot_boruta = FALSE,
#                           probability = TRUE,
#                           num.trees = 300,
#                           sorted_rules_RF = rules_RF,
#                           boruta_args = list(),
#                           verbose = TRUE)
#
# # training accuracy
# # get the prediction labels
# # if the classifier trained using probability	= FALSE
# training_pred &lt;- RF_classifier$RF_scheme$RF_classifier$predictions
# if (is.factor(training_pred)) {
#   x &lt;- as.character(training_pred)
# }
#
# # if the classifier trained using probability	= TRUE
# if (is.matrix(training_pred)) {
#   x &lt;- colnames(training_pred)[max.col(training_pred)]
# }
#
# # training accuracy
# caret::confusionMatrix(data =factor(x),
#                 reference = factor(object$data$Labels),
#                 mode = "everything")

# not to run
# visualize the binary rules in training dataset
# plot_binary_RF(Data = object,
#                classifier = RF_classifier,
#                prediction = NULL, as_training = TRUE,
#                show_scores = TRUE,
#                top_anno = "ref",
#                show_predictions = TRUE,
#                title = "Training data")

# not to run
# Extract and plot the proximity matrix from the classifier for the training data
# it takes long time for large data
# proximity_mat &lt;- proximity_matrix_RF(object = object,
#                       classifier = RF_classifier,
#                       plot=TRUE,
#                       return_matrix=TRUE,
#                       title = "Test",
#                       cluster_cols = TRUE)

# not to run
# predict
# test_object # any test data
# results &lt;- predict_RF(classifier = RF_classifier, impute = TRUE,
#                       Data = test_object)
#
# # visualize the binary rules in training dataset
# plot_binary_RF(Data = test_object,
#                classifier = RF_classifier,
#                prediction = results, as_training = FALSE,
#                show_scores = TRUE,
#                top_anno = "ref",
#                show_predictions = TRUE,
#                title = "Test data")</code></pre>

<hr>
<h2 id='plot_binary_TSP'>
Plot binary rule-based heatmaps
</h2><span id='topic+plot_binary_TSP'></span>

<h3>Description</h3>

<p><code>plot_binary_TSP</code>
Plot binary heatmaps for datasets based on one-vs-rest multiclass top score pairs classifier
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_binary_TSP(Data,
                classifier,
                ref              = NULL,
                prediction       = NULL,
                platform         = NULL,
                classes          = NULL,
                platforms_ord    = NULL,
                top_anno         = c("ref", "prediction", "platform")[1],
                title            = "",
                binary_col       = c("white", "black", "gray"),
                ref_col          = NULL,
                pred_col         = NULL,
                platform_col     = NULL,
                show_ref         = TRUE,
                show_predictions = TRUE,
                show_platform    = TRUE,
                show_scores      = TRUE,
                show_rule_name   = TRUE,
                legend           = TRUE,
                cluster_cols     = TRUE,
                cluster_rows     = TRUE,
                anno_height      = 0.03,
                score_height     = 0.03,
                margin           = c(0, 5, 0, 5))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_binary_TSP_+3A_data">Data</code></td>
<td>

<p>a matrix, dataframe, where samples as columns and row as features/genes. Can also take ExpressionSet, or data_object generated by ReadData function.
</p>
</td></tr>
<tr><td><code id="plot_binary_TSP_+3A_classifier">classifier</code></td>
<td>

<p>classifier as a OnevsrestScheme_TSP object, generated by train_one_vs_rest_TSP function
</p>
</td></tr>
<tr><td><code id="plot_binary_TSP_+3A_ref">ref</code></td>
<td>

<p>Optional vector with the reference labels. Ref labels in data_object will be used if not ref input provided. For ExpressionSet, the name of the variable in the pheno data.
</p>
</td></tr>
<tr><td><code id="plot_binary_TSP_+3A_prediction">prediction</code></td>
<td>

<p>Optional dataframe with class &quot;OneVsRestTSP prediction&quot; generated by predict_one_vs_rest_TSP function with the scores and the predicted labels.
</p>
</td></tr>
<tr><td><code id="plot_binary_TSP_+3A_platform">platform</code></td>
<td>

<p>Optional vector with the platform/study labels or any additional annotation. Platform labels in data_object will be used if no platform input provided. For ExpressionSet, the name of the variable in the pheno data.
</p>
</td></tr>
<tr><td><code id="plot_binary_TSP_+3A_classes">classes</code></td>
<td>

<p>Optional vector with the class names. This will determine which classes will be plotted and in which order. It is not recommended to use both &quot;classes&quot; and &quot;platforms_ord&quot; arguments together.
</p>
</td></tr>
<tr><td><code id="plot_binary_TSP_+3A_platforms_ord">platforms_ord</code></td>
<td>

<p>Optional vector with the platform/study names. This will determine which platform/study will be plotted and in which order. This will be used when top_anno=&quot;platform&quot;. It is not recommended to use both &quot;classes&quot; and &quot;platforms_ord&quot; arguments together.
</p>
</td></tr>
<tr><td><code id="plot_binary_TSP_+3A_top_anno">top_anno</code></td>
<td>

<p>Determine the top annotation level. Samples will be grouped based on the top_anno. Input can be one of three options: &quot;ref&quot;, &quot;prediction&quot;, &quot;platform&quot;. Default is &quot;ref&quot;.
</p>
</td></tr>
<tr><td><code id="plot_binary_TSP_+3A_title">title</code></td>
<td>

<p>Carachter input as a title for the whole heatmap. Default is &quot;&quot;.
</p>
</td></tr>
<tr><td><code id="plot_binary_TSP_+3A_binary_col">binary_col</code></td>
<td>

<p>vector determines the colors of the binary heatmap. Default is c(&quot;white&quot;, &quot;black&quot;, &quot;gray&quot;). First color for the color when the rule is false in the sample. Second color for the color when the rule is true. Third color is for NAs.
</p>
</td></tr>
<tr><td><code id="plot_binary_TSP_+3A_ref_col">ref_col</code></td>
<td>

<p>optional named vector determines the colors of classes for the reference labels. Default is NULL. Vector names should match with the ref labels.
</p>
</td></tr>
<tr><td><code id="plot_binary_TSP_+3A_pred_col">pred_col</code></td>
<td>

<p>optional named vector determines the colors of classes for the prediction labels. Default is NULL. Vector names should match with the prediction labels in the prediction labels.
</p>
</td></tr>
<tr><td><code id="plot_binary_TSP_+3A_platform_col">platform_col</code></td>
<td>

<p>optional named vector determines the colors of platforms/study labels. Default is NULL. Vector names should match with the platforms/study labels.
</p>
</td></tr>
<tr><td><code id="plot_binary_TSP_+3A_show_ref">show_ref</code></td>
<td>

<p>logical. Determines if the ref labels will be plotted or not. If the top_anno argument is &quot;ref&quot; then show_ref will be ignored.
</p>
</td></tr>
<tr><td><code id="plot_binary_TSP_+3A_show_predictions">show_predictions</code></td>
<td>

<p>logical. Determines if the prediction labels will be plotted or not. If the top_anno argument is &quot;prediction&quot; then show_predictions will be ignored.
</p>
</td></tr>
<tr><td><code id="plot_binary_TSP_+3A_show_platform">show_platform</code></td>
<td>

<p>logical. Determines if the platform/study labels will be plotted or not. If the top_anno argument is &quot;platform&quot; then show_platform will be ignored.
</p>
</td></tr>
<tr><td><code id="plot_binary_TSP_+3A_show_scores">show_scores</code></td>
<td>

<p>logical. Determines if the prediction scores will be plotted or not.
</p>
</td></tr>
<tr><td><code id="plot_binary_TSP_+3A_show_rule_name">show_rule_name</code></td>
<td>

<p>logical. Determines if the rule names will be plotted on the left side of the heatmapp or not.
</p>
</td></tr>
<tr><td><code id="plot_binary_TSP_+3A_legend">legend</code></td>
<td>

<p>logical. Determines if a legend will be plotted under the heatmap.
</p>
</td></tr>
<tr><td><code id="plot_binary_TSP_+3A_cluster_cols">cluster_cols</code></td>
<td>

<p>logical. Clustering the samples in each class (i.e. not all samples in the cohort) based on the binary rules for that class. If top_anno is &quot;platform&quot; then the rules from all classes are used to cluster the samples in each platform.
</p>
</td></tr>
<tr><td><code id="plot_binary_TSP_+3A_cluster_rows">cluster_rows</code></td>
<td>

<p>logical. Clustering the rules in each class.
</p>
</td></tr>
<tr><td><code id="plot_binary_TSP_+3A_anno_height">anno_height</code></td>
<td>

<p>Determines the height of the annotations. It is recommended not to go out of this range 0.01&lt;height&lt;0.1. Default is 0.03.
</p>
</td></tr>
<tr><td><code id="plot_binary_TSP_+3A_score_height">score_height</code></td>
<td>

<p>Determines the height of the score bars. It is recommended not to go out of this range 0.01&lt;height&lt;0.1. Default is 0.03.
</p>
</td></tr>
<tr><td><code id="plot_binary_TSP_+3A_margin">margin</code></td>
<td>

<p>Determines the margins of the heatmap. Default is c(0, 5, 0, 5).
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>returns a heatmap plot for the binary rule
</p>


<h3>Author(s)</h3>

<p>Nour-al-dain Marzouka &lt;nour-al-dain.marzouka at med.lu.se&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'># random data
Data &lt;- matrix(runif(10000), nrow=100, ncol=100,
               dimnames = list(paste0("G",1:100), paste0("S",1:100)))

# labels
L &lt;- sample(x = c("A","B","C"), size = 100, replace = TRUE)

# study/platform
P &lt;- sample(c("P1","P2"), size = 100, replace = TRUE)

object &lt;- ReadData(Data = Data,
             Labels = L,
                   Platform = P)

# not to run
# switchBox package from Bioconductor is needed
# Visit their website or install switchBox package using:
# if(!requireNamespace("switchBox", quietly = TRUE)){
#       if (!requireNamespace('BiocManager', quietly = TRUE)) {
#       install.packages('BiocManager')
#      }
#      BiocManager::install('switchBox')", call. = FALSE)
#  }

#filtered_genes &lt;- filter_genes_TSP(data_object = object,
#                                   filter = "one_vs_rest",
#                                  platform_wise = FALSE,
#                                  featureNo = 10,
#                                  UpDown = TRUE,
#                                  verbose = FALSE)

# training
# classifier &lt;- train_one_vs_rest_TSP(data_object = object,
#                              filtered_genes = filtered_genes,
#                              k_range = 2:50,
#                              include_pivot = FALSE,
#                              one_vs_one_scores = FALSE,
#                              platform_wise_scores = FALSE,
#                              seed = 1234,
#                              verbose = FALSE)

# results &lt;- predict_one_vs_rest_TSP(classifier = classifier,
#                                   Data = object,
#                                   tolerate_missed_genes = TRUE,
#                                   weighted_votes = TRUE,
#                                   verbose = FALSE)

# Confusion Matrix and Statistics on training data
#  caret::confusionMatrix(data = factor(results$max_score, levels = unique(L)),
#                         reference = factor(L, levels = unique(L)),
#                         mode="everything")

# plot_binary_TSP(Data = object, classes=c("A","B","C"),
#                 classifier = classifier,
#                 prediction = results,
#                 title = "Test")
</code></pre>

<hr>
<h2 id='predict_one_vs_rest_TSP'>
Predict sample class based on one-vs-rest multiclass top score pairs classifier
</h2><span id='topic+predict_one_vs_rest_TSP'></span>

<h3>Description</h3>

<p><code>predict_one_vs_rest_TSP</code> predicts sample class based on one-vs-rest multiclass top score pairs classifier classifier
</p>


<h3>Usage</h3>

<pre><code class='language-R'>predict_one_vs_rest_TSP(classifier,
                        Data,
                        tolerate_missed_genes = TRUE,
                        weighted_votes = TRUE,
                        classes,
                        verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict_one_vs_rest_TSP_+3A_classifier">classifier</code></td>
<td>

<p>classifier as a OnevsrestScheme_TSP object, generated by train_one_vs_rest_TSP function
</p>
</td></tr>
<tr><td><code id="predict_one_vs_rest_TSP_+3A_data">Data</code></td>
<td>

<p>a matrix, dataframe, ExpressionSet, or data_object generated by ReadData function.
Samples as columns and row as features/genes.
</p>
</td></tr>
<tr><td><code id="predict_one_vs_rest_TSP_+3A_tolerate_missed_genes">tolerate_missed_genes</code></td>
<td>

<p>logical. TRUE means that if a gene in the classifier is missed in the data then this rule will be not considered in the prediction. If tolerate_missed_genes=TRUE then the user should keep an eye on the left rules. In some cases when the missed genes are too many then no enough rules left for a good prediction.
</p>
</td></tr>
<tr><td><code id="predict_one_vs_rest_TSP_+3A_weighted_votes">weighted_votes</code></td>
<td>

<p>logical indicates if the rules will be treated equally or be weighted by their scores. weighted_votes=TRUE is useful to break vote ties between classes.
</p>
</td></tr>
<tr><td><code id="predict_one_vs_rest_TSP_+3A_classes">classes</code></td>
<td>

<p>optional vector with the names of the classes.
This will be used to order the columns of the ouput dataframe. In case not all classes in the classifier is mentioned in the vector, then these classes will be excluded from the prediction.
</p>
</td></tr>
<tr><td><code id="predict_one_vs_rest_TSP_+3A_verbose">verbose</code></td>
<td>

<p>a logical value indicating whether processing messages will be printed or not. Default is TRUE.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>returns dataframe with classes votes, ties, and final prediction
</p>


<h3>Author(s)</h3>

<p>Nour-al-dain Marzouka &lt;nour-al-dain.marzouka at med.lu.se&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'># random data
Data &lt;- matrix(runif(10000), nrow=100, ncol=100,
               dimnames = list(paste0("G",1:100), paste0("S",1:100)))

# labels
L &lt;- sample(x = c("A","B","C"), size = 100, replace = TRUE)

# study/platform
P &lt;- sample(c("P1","P2"), size = 100, replace = TRUE)

object &lt;- ReadData(Data = Data,
             Labels = L,
                   Platform = P)

# not to run
# switchBox package from Bioconductor is needed
# Visit their website or install switchBox package using:
# if(!requireNamespace("switchBox", quietly = TRUE)){
#       if (!requireNamespace('BiocManager', quietly = TRUE)) {
#       install.packages('BiocManager')
#      }
#      BiocManager::install('switchBox')", call. = FALSE)
#  }

# filtered_genes &lt;- filter_genes_TSP(data_object = object,
#                                  filter = "one_vs_rest",
#                                  platform_wise = FALSE,
#                                  featureNo = 10,
#                                  UpDown = TRUE,
#                                  verbose = FALSE)

# training
# classifier &lt;- train_one_vs_rest_TSP(data_object = object,
#                              filtered_genes = filtered_genes,
#                              k_range = 2:50,
#                              include_pivot = FALSE,
#                              one_vs_one_scores = FALSE,
#                              platform_wise_scores = FALSE,
#                              seed = 1234,
#                              verbose = FALSE)

# results &lt;- predict_one_vs_rest_TSP(classifier = classifier,
#                                   Data = object,
#                                   tolerate_missed_genes = TRUE,
#                                   weighted_votes = TRUE,
#                                   verbose = FALSE)

# Confusion Matrix and Statistics on training data
#  caret::confusionMatrix(data = factor(results$max_score, levels = unique(L)),
#                         reference = factor(L, levels = unique(L)),
#                         mode="everything")

# plot_binary_TSP(Data = object, classes=c("A","B","C"),
#                 classifier = classifier,
#                 prediction = results,
#                 title = "Test")
</code></pre>

<hr>
<h2 id='predict_RF'>
Predict sample class based on gene pair-based random forest classifier
</h2><span id='topic+predict_RF'></span>

<h3>Description</h3>

<p><code>predict_RF</code> predicts sample class based on pair-based random forest classifier
</p>


<h3>Usage</h3>

<pre><code class='language-R'>predict_RF(classifier,
           Data,
           impute = FALSE,
           impute_reject = 0.67,
           impute_kNN = 5,
           verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict_RF_+3A_classifier">classifier</code></td>
<td>

<p>classifier as a rule_based_RandomForest object, generated by train_RF function
</p>
</td></tr>
<tr><td><code id="predict_RF_+3A_data">Data</code></td>
<td>

<p>a matrix, dataframe, ExpressionSet, or data_object generated by ReadData function.
Samples as columns and row as features/genes.
</p>
</td></tr>
<tr><td><code id="predict_RF_+3A_impute">impute</code></td>
<td>

<p>logical. To determine if missed genes and NA values should be imputed or not. The non missed rules will be used to detemine the closest samples in the training binary matrix (i.e. which is stored in the classifier object). For each sample, the mode value for nearest samples in the training data will be assigned to the missed rules. Default is FALSE.
</p>
</td></tr>
<tr><td><code id="predict_RF_+3A_impute_reject">impute_reject</code></td>
<td>

<p>a number between 0 and 1 indicating the threshold of the missed rules in the sample. Based on this threshold the sample will be rejected (i.e. skipped if higher than the impute_reject threshold) and the missed rules will not be imputed in this sample. Default is 0.67. NOTE, The results object will not have any results for this sample.
</p>
</td></tr>
<tr><td><code id="predict_RF_+3A_impute_knn">impute_kNN</code></td>
<td>

<p>interger determines the number of the nearest samples in the training data to be used in the imputation. Default is 5. It is not recommended to use large number (i.e. &gt;10).
</p>
</td></tr>
<tr><td><code id="predict_RF_+3A_verbose">verbose</code></td>
<td>

<p>a logical value indicating whether processing messages will be printed or not. Default is TRUE.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>returns predictions object as &quot;ranger.prediction&quot; class from ranger package. If the RF classifier was trained with probability=TRUE then the results will contain the scores for the classes, and to help the user to get clearer outputs predict_RF adds a new slot (i.e. results$predictions_classes) contains a vector with the prediction based on the highest scores in results$predictions. If the RF classifier was trained with probability=FALSE then the results will contain the final class but no scores are provided in results$predictions.
In case a sample was rejected in the imputation process (passed the reject cutoff) then it will not be included in the prediction results. This should be kept in mind in case the user wants to match the input samples with the results for the confusion matrix for example.
To help the user to get clearer outputs predict_RF adds the sample names as names/row names to the factor/matrix in results$predictions.
</p>


<h3>Author(s)</h3>

<p>Nour-al-dain Marzouka &lt;nour-al-dain.marzouka at med.lu.se&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'># generate random data
Data &lt;- matrix(runif(8000), nrow=100, ncol=80,
               dimnames = list(paste0("G",1:100), paste0("S",1:80)))

# generate random labels
L &lt;- sample(x = c("A","B","C","D"), size = 80, replace = TRUE)

# generate random platform labels
P &lt;- sample(c("P1","P2","P3"), size = 80, replace = TRUE)

# create data object
object &lt;- ReadData(Data = Data,
                   Labels = L,
                   Platform = P,
                   verbose = FALSE)

# sort genes
genes_RF &lt;- sort_genes_RF(data_object = object,
                          seed=123456, verbose = FALSE)

# to get an idea of how many genes we will use
# and how many rules will be generated
# summary_genes_RF(sorted_genes_RF = genes_RF,
#                  genes_altogether = c(10,20,50,100,150,200),
#                  genes_one_vs_rest = c(10,20,50,100,150,200))

# creat and sort rules
# rules_RF &lt;- sort_rules_RF(data_object = object,
#                           sorted_genes_RF = genes_RF,
#                           genes_altogether = 100,
#                           genes_one_vs_rest = 100,
#                           seed=123456,
#                           verbose = FALSE)

# parameters &lt;- data.frame(
#   gene_repetition=c(3,2,1),
#   rules_one_vs_rest=0,
#   rules_altogether=c(2,3,10),
#   run_boruta=c(FALSE,"produce_error",FALSE),
#   plot_boruta = FALSE,
#   num.trees=c(100,200,300),
#   stringsAsFactors = FALSE)
# parameters

# Or you can use expand.grid to generate dataframe with all parameter combinations
# parameters &lt;- expand.grid(
#   gene_repetition=c(3,2,1),
#   rules_one_vs_rest=0,
#   rules_altogether=c(2,3,10),
#   num.trees=c(100,500,1000),
#   stringsAsFactors = FALSE)
# parameters


# test &lt;- optimize_RF(data_object = object,
#                     sorted_rules_RF = rules_RF,
#                     test_object = NULL,
#                     overall = c("Accuracy"),
#                     byclass = NULL, verbose = FALSE,
#                     parameters = parameters)
# test
# test$summary[which.max(test$summary$Accuracy),]
#
# # train the final model
# # it is preferred to increase the number of trees and rules in case you have
# # large number of samples and features
# # for quick example, we have small number of trees and rules here
# # based on the optimize_RF results we will select the parameters
# RF_classifier &lt;- train_RF(data_object = object,
#                           gene_repetition = 1,
#                           rules_altogether = 0,
#                           rules_one_vs_rest = 10,
#                           run_boruta = FALSE,
#                           plot_boruta = FALSE,
#                           probability = TRUE,
#                           num.trees = 300,
#                           sorted_rules_RF = rules_RF,
#                           boruta_args = list(),
#                           verbose = TRUE)
#
# # training accuracy
# # get the prediction labels
# # if the classifier trained using probability	= FALSE
# training_pred &lt;- RF_classifier$RF_scheme$RF_classifier$predictions
# if (is.factor(training_pred)) {
#   x &lt;- as.character(training_pred)
# }
#
# # if the classifier trained using probability	= TRUE
# if (is.matrix(training_pred)) {
#   x &lt;- colnames(training_pred)[max.col(training_pred)]
# }
#
# # training accuracy
# caret::confusionMatrix(data =factor(x),
#                 reference = factor(object$data$Labels),
#                 mode = "everything")

# not to run
# visualize the binary rules in training dataset
# plot_binary_RF(Data = object,
#                classifier = RF_classifier,
#                prediction = NULL, as_training = TRUE,
#                show_scores = TRUE,
#                top_anno = "ref",
#                show_predictions = TRUE,
#                title = "Training data")

# not to run
# Extract and plot the proximity matrix from the classifier for the training data
# it takes long time for large data
# proximity_mat &lt;- proximity_matrix_RF(object = object,
#                       classifier = RF_classifier,
#                       plot=TRUE,
#                       return_matrix=TRUE,
#                       title = "Test",
#                       cluster_cols = TRUE)

# not to run
# predict
# test_object # any test data
# results &lt;- predict_RF(classifier = RF_classifier, impute = TRUE,
#                       Data = test_object)
#
# # visualize the binary rules in training dataset
# plot_binary_RF(Data = test_object,
#                classifier = RF_classifier,
#                prediction = results, as_training = FALSE,
#                show_scores = TRUE,
#                top_anno = "ref",
#                show_predictions = TRUE,
#                title = "Test data")</code></pre>

<hr>
<h2 id='print-methods'>Methods for Function <code>print</code> in Package <span class="pkg">multiclassPairs</span></h2><span id='topic+print-methods'></span><span id='topic+print+2CmulticlassPairs_object-method'></span><span id='topic+print+2COnevsrestScheme_genes_SB-method'></span><span id='topic+print+2COnevsrestScheme_SB-method'></span><span id='topic+print+2CRandomForest_sorted_genes-method'></span><span id='topic+print+2CRandomForest_sorted_rules-method'></span><span id='topic+print+2Crule_based_RandomForest-method'></span>

<h3>Description</h3>

<p>Methods for function <code>print</code> in package <span class="pkg">multiclassPairs</span>
</p>


<h3>Methods</h3>


<dl>
<dt><code>signature(x = "multiclassPairs_object")</code></dt><dd></dd>
<dt><code>signature(x = "OnevsrestScheme_genes_SB")</code></dt><dd></dd>
<dt><code>signature(x = "OnevsrestScheme_SB")</code></dt><dd></dd>
<dt><code>signature(x = "RandomForest_sorted_genes")</code></dt><dd></dd>
<dt><code>signature(x = "RandomForest_sorted_rules")</code></dt><dd></dd>
<dt><code>signature(x = "rule_based_RandomForest")</code></dt><dd></dd>
</dl>

<hr>
<h2 id='proximity_matrix_RF'>
Plot binary rule-based heatmaps
</h2><span id='topic+proximity_matrix_RF'></span>

<h3>Description</h3>

<p><code>proximity_matrix_RF</code> Plot clustering heatmaps showing which out-of-bag samples are predicted in the same class and in the same trees during the training process for the rule-based random forest classifier
</p>


<h3>Usage</h3>

<pre><code class='language-R'>proximity_matrix_RF(object,
            classifier,
            plot=TRUE,
            return_matrix=TRUE,
            title         = "",
            top_anno      = c("ref","platform")[1],
            classes       = NULL,
            sam_order     = NULL,
            ref_col       = NULL,
            platform_col  = NULL,
            platforms_ord = NULL,
            show_platform = TRUE,
            cluster_cols  = FALSE,
            legend        = TRUE,
            anno_height   = 0.03,
            margin        = c(0, 5, 0, 5))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="proximity_matrix_RF_+3A_object">object</code></td>
<td>

<p>data_object generated by ReadData function which was used in the training process.
</p>
</td></tr>
<tr><td><code id="proximity_matrix_RF_+3A_classifier">classifier</code></td>
<td>

<p>classifier as a rule_based_RandomForest object, generated by train_RF function
</p>
</td></tr>
<tr><td><code id="proximity_matrix_RF_+3A_plot">plot</code></td>
<td>

<p>logical. To plot the proximity matrix or not. Default is TRUE.
</p>
</td></tr>
<tr><td><code id="proximity_matrix_RF_+3A_return_matrix">return_matrix</code></td>
<td>

<p>logical. To return the proximity matrix or not. Default is TRUE.
</p>
</td></tr>
<tr><td><code id="proximity_matrix_RF_+3A_title">title</code></td>
<td>

<p>Character input as a title for the whole heatmap. Default is &quot;&quot;.
</p>
</td></tr>
<tr><td><code id="proximity_matrix_RF_+3A_top_anno">top_anno</code></td>
<td>

<p>Determine the top annotation level. Samples will be grouped based on the top_anno. Input can be one of two options: &quot;ref&quot;, &quot;platform&quot;. Default is &quot;ref&quot;.
</p>
</td></tr>
<tr><td><code id="proximity_matrix_RF_+3A_classes">classes</code></td>
<td>

<p>Optional vector with the class names. Classes will determine which classes will be plotted and in which order. It is not recommended to use both &quot;classes&quot; and &quot;platforms_ord&quot; arguments together.
</p>
</td></tr>
<tr><td><code id="proximity_matrix_RF_+3A_sam_order">sam_order</code></td>
<td>

<p>Optional vector with the samples order in the heatmap.
</p>
</td></tr>
<tr><td><code id="proximity_matrix_RF_+3A_ref_col">ref_col</code></td>
<td>

<p>optional named vector determines the colors of classes for the reference labels. Default is NULL. Vector names should match with the ref labels.
</p>
</td></tr>
<tr><td><code id="proximity_matrix_RF_+3A_platform_col">platform_col</code></td>
<td>

<p>optional named vector determines the colors of platforms/study labels. Default is NULL. Vector names should match with the platforms/study labels.
</p>
</td></tr>
<tr><td><code id="proximity_matrix_RF_+3A_platforms_ord">platforms_ord</code></td>
<td>

<p>Optional vector with the platform/study names. This will determine which platform/study will be plotted and in which order. This will be used when top_anno=&quot;platform&quot;. It is not recommended to use both &quot;classes&quot; and &quot;platforms_ord&quot; arguments together.
</p>
</td></tr>
<tr><td><code id="proximity_matrix_RF_+3A_show_platform">show_platform</code></td>
<td>

<p>logical. Determines if the platform/study labels will be plotted or not. If the top_anno argument is &quot;platform&quot; then show_platform will be ignored.
</p>
</td></tr>
<tr><td><code id="proximity_matrix_RF_+3A_cluster_cols">cluster_cols</code></td>
<td>

<p>logical. samples will be grouped based on the class then will be Clustered in each class (i.e. not all samples in the cohort). If top_anno is &quot;platform&quot; then the rules from all classes are used to cluster the samples in each platform.
</p>
</td></tr>
<tr><td><code id="proximity_matrix_RF_+3A_legend">legend</code></td>
<td>

<p>logical. Determines if a legend will be plotted under the heatmap.
</p>
</td></tr>
<tr><td><code id="proximity_matrix_RF_+3A_anno_height">anno_height</code></td>
<td>

<p>Determines the height of the annotations. It is recommended not to go out of this range 0.01&lt;height&lt;0.1. Default is 0.03.
</p>
</td></tr>
<tr><td><code id="proximity_matrix_RF_+3A_margin">margin</code></td>
<td>

<p>Determines the margins of the heatmap. Default is c(0, 5, 0, 5).
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>returns the proximity matrix and/or a heatmap plot for the proximity matrix.
</p>


<h3>Author(s)</h3>

<p>Nour-al-dain Marzouka &lt;nour-al-dain.marzouka at med.lu.se&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'># generate random data
Data &lt;- matrix(runif(8000), nrow=100, ncol=80,
               dimnames = list(paste0("G",1:100), paste0("S",1:80)))

# generate random labels
L &lt;- sample(x = c("A","B","C","D"), size = 80, replace = TRUE)

# generate random platform labels
P &lt;- sample(c("P1","P2","P3"), size = 80, replace = TRUE)

# create data object
object &lt;- ReadData(Data = Data,
                   Labels = L,
                   Platform = P,
                   verbose = FALSE)

# sort genes
genes_RF &lt;- sort_genes_RF(data_object = object,
                          seed=123456, verbose = FALSE)

# to get an idea of how many genes we will use
# and how many rules will be generated
# summary_genes_RF(sorted_genes_RF = genes_RF,
#                  genes_altogether = c(10,20,50,100,150,200),
#                  genes_one_vs_rest = c(10,20,50,100,150,200))

# creat and sort rules
# rules_RF &lt;- sort_rules_RF(data_object = object,
#                           sorted_genes_RF = genes_RF,
#                           genes_altogether = 100,
#                           genes_one_vs_rest = 100,
#                           seed=123456,
#                           verbose = FALSE)

# parameters &lt;- data.frame(
#   gene_repetition=c(3,2,1),
#   rules_one_vs_rest=0,
#   rules_altogether=c(2,3,10),
#   run_boruta=c(FALSE,"produce_error",FALSE),
#   plot_boruta = FALSE,
#   num.trees=c(100,200,300),
#   stringsAsFactors = FALSE)
# parameters

# Or you can use expand.grid to generate dataframe with all parameter combinations
# parameters &lt;- expand.grid(
#   gene_repetition=c(3,2,1),
#   rules_one_vs_rest=0,
#   rules_altogether=c(2,3,10),
#   num.trees=c(100,500,1000),
#   stringsAsFactors = FALSE)
# parameters


# test &lt;- optimize_RF(data_object = object,
#                     sorted_rules_RF = rules_RF,
#                     test_object = NULL,
#                     overall = c("Accuracy"),
#                     byclass = NULL, verbose = FALSE,
#                     parameters = parameters)
# test
# test$summary[which.max(test$summary$Accuracy),]
#
# # train the final model
# # it is preferred to increase the number of trees and rules in case you have
# # large number of samples and features
# # for quick example, we have small number of trees and rules here
# # based on the optimize_RF results we will select the parameters
# RF_classifier &lt;- train_RF(data_object = object,
#                           gene_repetition = 1,
#                           rules_altogether = 0,
#                           rules_one_vs_rest = 10,
#                           run_boruta = FALSE,
#                           plot_boruta = FALSE,
#                           probability = TRUE,
#                           num.trees = 300,
#                           sorted_rules_RF = rules_RF,
#                           boruta_args = list(),
#                           verbose = TRUE)
#
# # training accuracy
# # get the prediction labels
# # if the classifier trained using probability	= FALSE
# training_pred &lt;- RF_classifier$RF_scheme$RF_classifier$predictions
# if (is.factor(training_pred)) {
#   x &lt;- as.character(training_pred)
# }
#
# # if the classifier trained using probability	= TRUE
# if (is.matrix(training_pred)) {
#   x &lt;- colnames(training_pred)[max.col(training_pred)]
# }
#
# # training accuracy
# caret::confusionMatrix(data =factor(x),
#                 reference = factor(object$data$Labels),
#                 mode = "everything")

# not to run
# visualize the binary rules in training dataset
# plot_binary_RF(Data = object,
#                classifier = RF_classifier,
#                prediction = NULL, as_training = TRUE,
#                show_scores = TRUE,
#                top_anno = "ref",
#                show_predictions = TRUE,
#                title = "Training data")

# not to run
# Extract and plot the proximity matrix from the classifier for the training data
# it takes long time for large data
# proximity_mat &lt;- proximity_matrix_RF(object = object,
#                       classifier = RF_classifier,
#                       plot=TRUE,
#                       return_matrix=TRUE,
#                       title = "Test",
#                       cluster_cols = TRUE)

# not to run
# predict
# test_object # any test data
# results &lt;- predict_RF(classifier = RF_classifier, impute = TRUE,
#                       Data = test_object)
#
# # visualize the binary rules in training dataset
# plot_binary_RF(Data = test_object,
#                classifier = RF_classifier,
#                prediction = results, as_training = FALSE,
#                show_scores = TRUE,
#                top_anno = "ref",
#                show_predictions = TRUE,
#                title = "Test data")
</code></pre>

<hr>
<h2 id='ReadData'>
Function for preparing data object
</h2><span id='topic+ReadData'></span>

<h3>Description</h3>

<p>ReadData takes data such as matrix, labels, and platform information, and produce data object to be used in the down stream analysis, such as filtering genes.</p>


<h3>Usage</h3>

<pre><code class='language-R'>ReadData(Data, Labels, Platform = NULL, verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ReadData_+3A_data">Data</code></td>
<td>

<p>a dataframe, matrix, or ExpressionSet with values to be used in the down stream analysis. Samples as columns  and rows genes/features as rows. Matrix should has column names and row names. It is recommended to avoid &quot;-&quot; symbol for the feature/gene names.
</p>
</td></tr>
<tr><td><code id="ReadData_+3A_labels">Labels</code></td>
<td>

<p>a vector indicating the classes of the samples.
Should be with the same length of the columns number in data.
This can be a variable name stored in the ExpressionSet if ExpressionSet is used.
</p>
</td></tr>
<tr><td><code id="ReadData_+3A_platform">Platform</code></td>
<td>

<p>Optional, vector with the same length of labels indicating.
This can be a variable name stored in the ExpressionSet if ExpressionSet is used.
</p>
</td></tr>
<tr><td><code id="ReadData_+3A_verbose">verbose</code></td>
<td>

<p>a logical value indicating whether processing messages will be printed or not. Default is TRUE.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data object multiclassPairs_object
</p>
<table>
<tr><td><code>Data</code></td>
<td>

<p>dataframe (gene as rows and samples as columns)
</p>
</td></tr>
<tr><td><code>Labels</code></td>
<td>

<p>a vector containing classes information
</p>
</td></tr>
<tr><td><code>Platform</code></td>
<td>

<p>a vector containing Platform information, or NULL if no input is used
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Nour-al-dain Marzouka &lt;nour-al-dain.marzouka at med.lu.se&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'># example of loading data from matrix
Data &lt;- matrix(runif(10000), nrow=100, ncol=100,
               dimnames = list(paste0("G",1:100), paste0("S",1:100)))

L &lt;- sample(x = c("A","B","C"), size = 100, replace = TRUE)

P &lt;- sample(x = c("P1","P2"), size = 100, replace = TRUE)


table(P,L)

object &lt;- ReadData(Data = Data,
                   Labels = L,
                   Platform = P,
                   verbose = FALSE)
object


# Not to run
# example of loading data from ExpressionSet
# library(leukemiasEset, quietly = TRUE)
# data(leukemiasEset)

# split the data to training and testing
# n &lt;- ncol(leukemiasEset)
# set.seed(1234)
# training_samples &lt;- sample(1:n,size = n*0.6)

# train &lt;- leukemiasEset[1:1000,training_samples]
# test  &lt;- leukemiasEset[1:1000,-training_samples]

# create the data object
# when we use Expressionset we can use the name of the phenotypes variable
# ReadData will automatically extract the phenotype variable and use it as class labels
# the same can be used with the Platform/study labels
# in this example we are not using any platform labels, so leave it NULL
# object &lt;- ReadData(Data = train,
#                   Labels = "LeukemiaType",
#                   Platform = NULL,
#                   verbose = FALSE)
# object
</code></pre>

<hr>
<h2 id='sort_genes_RF'>
Sort genes/features for pair-based random forest classifier downstream steps
</h2><span id='topic+sort_genes_RF'></span>

<h3>Description</h3>

<p><code>sort_genes_RF</code> uses random forest to sort genes/features prior of downstream steps such as gene pairs/rules selection which will involve random forest models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sort_genes_RF(data_object,
             featureNo_altogether,
             featureNo_one_vs_rest,
             rank_data = FALSE,
             platform_wise = FALSE,
             num.trees = 500,
             min.node.size = 1,
             importance = "impurity",
             write.forest = FALSE,
             keep.inbag = FALSE,
             verbose = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sort_genes_RF_+3A_data_object">data_object</code></td>
<td>

<p>data object generated by ReadData function. Object contains the data and labels.
</p>
</td></tr>
<tr><td><code id="sort_genes_RF_+3A_featureno_altogether">featureNo_altogether</code></td>
<td>

<p>an integer. Optional. Indicating specific number of top sorted genes to be returned from one random forest model contains all the labels together. If 0 then this sorting will be skipped. By default, if no number is specified then all available genes will be sorted and returned because user can specify how many top genes will be used in the downstream analysis.
</p>
</td></tr>
<tr><td><code id="sort_genes_RF_+3A_featureno_one_vs_rest">featureNo_one_vs_rest</code></td>
<td>

<p>an integer. Optional. Indicating specific number of top sorted genes to be returned from 'one vs rest' random forest models. This means each class will have a random forest where the samples from the other classes will be labels as 'rest'. If 0 then this sorting will be skipped. By default, if no number is specified then all available genes will be sorted and returned because user can specify how many top genes will be used in the downstream analysis.
</p>
</td></tr>
<tr><td><code id="sort_genes_RF_+3A_rank_data">rank_data</code></td>
<td>

<p>logical indicates if the data should be ranked (features will be ranked inside each sample). Default is FALSE.
</p>
</td></tr>
<tr><td><code id="sort_genes_RF_+3A_platform_wise">platform_wise</code></td>
<td>

<p>logical indicates if the gene importance should be calculated in each platform seperatly then combined based on the lowest importance value (i.e. a gene with low importance in any platform will not be prioritized). Default is FALSE. see details for more description.
</p>
</td></tr>
<tr><td><code id="sort_genes_RF_+3A_num.trees">num.trees</code></td>
<td>

<p>an integer. Number of trees. Default is 500. It is recommended to increase num.trees in case of having large number of features (ranger function argument).
</p>
</td></tr>
<tr><td><code id="sort_genes_RF_+3A_min.node.size">min.node.size</code></td>
<td>

<p>an integer. Minimal node size. Default is 1. (ranger function argument)
</p>
</td></tr>
<tr><td><code id="sort_genes_RF_+3A_importance">importance</code></td>
<td>

<p>Variable importance mode, should be one of 'impurity', 'impurity_corrected', 'permutation'.
Defualt is 'impurity' (ranger function argument)
</p>
</td></tr>
<tr><td><code id="sort_genes_RF_+3A_write.forest">write.forest</code></td>
<td>

<p>Save ranger.forest object, required for prediction. Default is FALSE to reduce memory. (ranger function argument)
</p>
</td></tr>
<tr><td><code id="sort_genes_RF_+3A_keep.inbag">keep.inbag</code></td>
<td>

<p>Save how often observations are in-bag in each tree. Default is FALSE. (ranger function argument)
</p>
</td></tr>
<tr><td><code id="sort_genes_RF_+3A_verbose">verbose</code></td>
<td>

<p>a logical value indicating whether processing messages will be printed or not. Default is TRUE.
</p>
</td></tr>
<tr><td><code id="sort_genes_RF_+3A_...">...</code></td>
<td>

<p>any additional arguments to be passed to ranger function (i.e. random forest function) in ranger package. For example, seed for reproducibility.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For platform-wise option. When platform_wise=TRUE, for example, if data has three platforms (i.e. P1, P2, and P3), and random forest was performed for class 1 (C1) versus rest in each platform seperatly, then C1 will have 3 importance lists contain the genes sorted based on P1-P3, genes will be sorted and ranked in each list (lower rank number means higher importance), the combined final sorting will be determined by the lowest importance level in the lists, it means a gene with (5,5,5) will be prioritized over a gene with (1,1,6). And this is applied on the altogether sorting and one-vs-rest sorting. Other combining methods could be added in the future.
</p>


<h3>Value</h3>

<p>returns RandomForest_sorted_genes object which contains sorted genes based on the importance in each class (one-vs-rest) sorting and based altogether sorting. Also it contains the random forest objects those used in the sorting.
</p>


<h3>Author(s)</h3>

<p>Nour-al-dain Marzouka &lt;nour-al-dain.marzouka at med.lu.se&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'># generate random data
Data &lt;- matrix(runif(8000), nrow=100, ncol=80,
               dimnames = list(paste0("G",1:100), paste0("S",1:80)))

# generate random labels
L &lt;- sample(x = c("A","B","C","D"), size = 80, replace = TRUE)

# generate random platform labels
P &lt;- sample(c("P1","P2","P3"), size = 80, replace = TRUE)

# create data object
object &lt;- ReadData(Data = Data,
                   Labels = L,
                   Platform = P,
                   verbose = FALSE)

# sort genes
genes_RF &lt;- sort_genes_RF(data_object = object,
                          seed=123456, verbose = FALSE)

# to get an idea of how many genes we will use
# and how many rules will be generated
# summary_genes_RF(sorted_genes_RF = genes_RF,
#                  genes_altogether = c(10,20,50,100,150,200),
#                  genes_one_vs_rest = c(10,20,50,100,150,200))

# creat and sort rules
# rules_RF &lt;- sort_rules_RF(data_object = object,
#                           sorted_genes_RF = genes_RF,
#                           genes_altogether = 100,
#                           genes_one_vs_rest = 100,
#                           seed=123456,
#                           verbose = FALSE)

# parameters &lt;- data.frame(
#   gene_repetition=c(3,2,1),
#   rules_one_vs_rest=0,
#   rules_altogether=c(2,3,10),
#   run_boruta=c(FALSE,"produce_error",FALSE),
#   plot_boruta = FALSE,
#   num.trees=c(100,200,300),
#   stringsAsFactors = FALSE)
# parameters

# Or you can use expand.grid to generate dataframe with all parameter combinations
# parameters &lt;- expand.grid(
#   gene_repetition=c(3,2,1),
#   rules_one_vs_rest=0,
#   rules_altogether=c(2,3,10),
#   num.trees=c(100,500,1000),
#   stringsAsFactors = FALSE)
# parameters


# test &lt;- optimize_RF(data_object = object,
#                     sorted_rules_RF = rules_RF,
#                     test_object = NULL,
#                     overall = c("Accuracy"),
#                     byclass = NULL, verbose = FALSE,
#                     parameters = parameters)
# test
# test$summary[which.max(test$summary$Accuracy),]
#
# # train the final model
# # it is preferred to increase the number of trees and rules in case you have
# # large number of samples and features
# # for quick example, we have small number of trees and rules here
# # based on the optimize_RF results we will select the parameters
# RF_classifier &lt;- train_RF(data_object = object,
#                           gene_repetition = 1,
#                           rules_altogether = 0,
#                           rules_one_vs_rest = 10,
#                           run_boruta = FALSE,
#                           plot_boruta = FALSE,
#                           probability = TRUE,
#                           num.trees = 300,
#                           sorted_rules_RF = rules_RF,
#                           boruta_args = list(),
#                           verbose = TRUE)
#
# # training accuracy
# # get the prediction labels
# # if the classifier trained using probability	= FALSE
# training_pred &lt;- RF_classifier$RF_scheme$RF_classifier$predictions
# if (is.factor(training_pred)) {
#   x &lt;- as.character(training_pred)
# }
#
# # if the classifier trained using probability	= TRUE
# if (is.matrix(training_pred)) {
#   x &lt;- colnames(training_pred)[max.col(training_pred)]
# }
#
# # training accuracy
# caret::confusionMatrix(data =factor(x),
#                 reference = factor(object$data$Labels),
#                 mode = "everything")

# not to run
# visualize the binary rules in training dataset
# plot_binary_RF(Data = object,
#                classifier = RF_classifier,
#                prediction = NULL, as_training = TRUE,
#                show_scores = TRUE,
#                top_anno = "ref",
#                show_predictions = TRUE,
#                title = "Training data")

# not to run
# Extract and plot the proximity matrix from the classifier for the training data
# it takes long time for large data
# proximity_mat &lt;- proximity_matrix_RF(object = object,
#                       classifier = RF_classifier,
#                       plot=TRUE,
#                       return_matrix=TRUE,
#                       title = "Test",
#                       cluster_cols = TRUE)

# not to run
# predict
# test_object # any test data
# results &lt;- predict_RF(classifier = RF_classifier, impute = TRUE,
#                       Data = test_object)
#
# # visualize the binary rules in training dataset
# plot_binary_RF(Data = test_object,
#                classifier = RF_classifier,
#                prediction = results, as_training = FALSE,
#                show_scores = TRUE,
#                top_anno = "ref",
#                show_predictions = TRUE,
#                title = "Test data")</code></pre>

<hr>
<h2 id='sort_rules_RF'>
Create and sort feature/gene pairs for pair-based random forest classifier training step
</h2><span id='topic+sort_rules_RF'></span>

<h3>Description</h3>

<p><code>sort_rules_RF uses</code> random forest to create and sort genes/features pairs prior of downstream random forest model training step.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sort_rules_RF(data_object,
             sorted_genes_RF,
             genes_altogether = 50,
             genes_one_vs_rest = 50,
             run_altogether = TRUE,
             run_one_vs_rest = TRUE,
             platform_wise = FALSE,
             num.trees = 500,
             min.node.size = 1,
             importance = "impurity",
             write.forest = FALSE,
             keep.inbag = FALSE,
             verbose = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sort_rules_RF_+3A_data_object">data_object</code></td>
<td>

<p>data object generated by ReadData function. Object contains the data and labels.
</p>
</td></tr>
<tr><td><code id="sort_rules_RF_+3A_sorted_genes_rf">sorted_genes_RF</code></td>
<td>

<p>RandomForest_sorted_genes object created by sort_genes_RF function
</p>
</td></tr>
<tr><td><code id="sort_rules_RF_+3A_genes_altogether">genes_altogether</code></td>
<td>

<p>integer indicates how many top gene/features should be used altogether genes. These genes will be pooled with the selected gene from genes_one_vs_rest to create all the possible rules. Default is 200.</p>
</td></tr>
<tr><td><code id="sort_rules_RF_+3A_genes_one_vs_rest">genes_one_vs_rest</code></td>
<td>

<p>integer indicates how many top gene/features should be used from one-vs-rest genes. These genes will be pooled with the selected gene from genes_altogether to create all the possible rules. Default is 200.</p>
</td></tr>
<tr><td><code id="sort_rules_RF_+3A_run_altogether">run_altogether</code></td>
<td>

<p>logical indicates if altogether RF model should be performed to sort the rules based in their importance in all classes together. Default is TRUE.
</p>
</td></tr>
<tr><td><code id="sort_rules_RF_+3A_run_one_vs_rest">run_one_vs_rest</code></td>
<td>

<p>logical indicates if one_vs_rest RF model for each class should be performed to sort the rules based in their importance in each class. Default is TRUE.
</p>
</td></tr>
<tr><td><code id="sort_rules_RF_+3A_platform_wise">platform_wise</code></td>
<td>

<p>logical indicates if the rules importance should be calculated in each platform seperatly then combined based on the lowest importance value (i.e. a rule with low importance in any platform will not be prioritized). Default is FALSE. see details for more description.
</p>
</td></tr>
<tr><td><code id="sort_rules_RF_+3A_num.trees">num.trees</code></td>
<td>

<p>an integer. Number of trees. Default is 500. It is recommended to increase num.trees in case of having large number of features (ranger function argument).
</p>
</td></tr>
<tr><td><code id="sort_rules_RF_+3A_min.node.size">min.node.size</code></td>
<td>

<p>an integer. Minimal node size. Default is 1. (ranger function argument)
</p>
</td></tr>
<tr><td><code id="sort_rules_RF_+3A_importance">importance</code></td>
<td>

<p>Variable importance mode, should be one of 'impurity', 'impurity_corrected', 'permutation'.
Defualt is 'impurity' (ranger function argument)
</p>
</td></tr>
<tr><td><code id="sort_rules_RF_+3A_write.forest">write.forest</code></td>
<td>

<p>Save ranger.forest object, required for prediction. Default is FALSE to reduce memory. (ranger function argument)
</p>
</td></tr>
<tr><td><code id="sort_rules_RF_+3A_keep.inbag">keep.inbag</code></td>
<td>

<p>Save how often observations are in-bag in each tree. Default is FALSE. (ranger function argument)
</p>
</td></tr>
<tr><td><code id="sort_rules_RF_+3A_verbose">verbose</code></td>
<td>

<p>a logical value indicating whether processing messages will be printed or not. Default is TRUE.
</p>
</td></tr>
<tr><td><code id="sort_rules_RF_+3A_...">...</code></td>
<td>

<p>any additional arguments to be passed to ranger function (i.e. random forest function) in ranger package. For example, seed for reproducibility.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In case of class imbalance rules_one_vs_rest=TRUE is recommended.
</p>
<p>For platform-wise option. When platform_wise=TRUE, for example, if data has three platforms (i.e. P1, P2, and P3), and random forest was performed for class 1 (C1) versus rest in each platform seperatly, then C1 will have 3 importance lists contain the rules sorted based on P1-P3, rules will be sorted and ranked in each list (lower rank number means higher importance), the combined final sorting will be determined by the lowest importance level in the lists, it means a rule with (5,5,5) will be prioritized over a rule with (1,1,6). And this is applied on the altogether sorting and one-vs-rest sorting. Other combining methods could be added in the future.
</p>


<h3>Value</h3>

<p>returns RandomForest_sorted_rules object which contains sorted rules based on the importance in each class (one-vs-rest) sorting and based on altogether sorting. Also it contains the random forest objects those used in the sorting.
</p>


<h3>Author(s)</h3>

<p>Nour-al-dain Marzouka &lt;nour-al-dain.marzouka at med.lu.se&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'># generate random data
Data &lt;- matrix(runif(8000), nrow=100, ncol=80,
               dimnames = list(paste0("G",1:100), paste0("S",1:80)))

# generate random labels
L &lt;- sample(x = c("A","B","C","D"), size = 80, replace = TRUE)

# generate random platform labels
P &lt;- sample(c("P1","P2","P3"), size = 80, replace = TRUE)

# create data object
object &lt;- ReadData(Data = Data,
                   Labels = L,
                   Platform = P,
                   verbose = FALSE)

# sort genes
genes_RF &lt;- sort_genes_RF(data_object = object,
                          seed=123456, verbose = FALSE)

# to get an idea of how many genes we will use
# and how many rules will be generated
# summary_genes_RF(sorted_genes_RF = genes_RF,
#                  genes_altogether = c(10,20,50,100,150,200),
#                  genes_one_vs_rest = c(10,20,50,100,150,200))

# creat and sort rules
# rules_RF &lt;- sort_rules_RF(data_object = object,
#                           sorted_genes_RF = genes_RF,
#                           genes_altogether = 100,
#                           genes_one_vs_rest = 100,
#                           seed=123456,
#                           verbose = FALSE)

# parameters &lt;- data.frame(
#   gene_repetition=c(3,2,1),
#   rules_one_vs_rest=0,
#   rules_altogether=c(2,3,10),
#   run_boruta=c(FALSE,"produce_error",FALSE),
#   plot_boruta = FALSE,
#   num.trees=c(100,200,300),
#   stringsAsFactors = FALSE)
# parameters

# Or you can use expand.grid to generate dataframe with all parameter combinations
# parameters &lt;- expand.grid(
#   gene_repetition=c(3,2,1),
#   rules_one_vs_rest=0,
#   rules_altogether=c(2,3,10),
#   num.trees=c(100,500,1000),
#   stringsAsFactors = FALSE)
# parameters


# test &lt;- optimize_RF(data_object = object,
#                     sorted_rules_RF = rules_RF,
#                     test_object = NULL,
#                     overall = c("Accuracy"),
#                     byclass = NULL, verbose = FALSE,
#                     parameters = parameters)
# test
# test$summary[which.max(test$summary$Accuracy),]
#
# # train the final model
# # it is preferred to increase the number of trees and rules in case you have
# # large number of samples and features
# # for quick example, we have small number of trees and rules here
# # based on the optimize_RF results we will select the parameters
# RF_classifier &lt;- train_RF(data_object = object,
#                           gene_repetition = 1,
#                           rules_altogether = 0,
#                           rules_one_vs_rest = 10,
#                           run_boruta = FALSE,
#                           plot_boruta = FALSE,
#                           probability = TRUE,
#                           num.trees = 300,
#                           sorted_rules_RF = rules_RF,
#                           boruta_args = list(),
#                           verbose = TRUE)
#
# # training accuracy
# # get the prediction labels
# # if the classifier trained using probability	= FALSE
# training_pred &lt;- RF_classifier$RF_scheme$RF_classifier$predictions
# if (is.factor(training_pred)) {
#   x &lt;- as.character(training_pred)
# }
#
# # if the classifier trained using probability	= TRUE
# if (is.matrix(training_pred)) {
#   x &lt;- colnames(training_pred)[max.col(training_pred)]
# }
#
# # training accuracy
# caret::confusionMatrix(data =factor(x),
#                 reference = factor(object$data$Labels),
#                 mode = "everything")

# not to run
# visualize the binary rules in training dataset
# plot_binary_RF(Data = object,
#                classifier = RF_classifier,
#                prediction = NULL, as_training = TRUE,
#                show_scores = TRUE,
#                top_anno = "ref",
#                show_predictions = TRUE,
#                title = "Training data")

# not to run
# Extract and plot the proximity matrix from the classifier for the training data
# it takes long time for large data
# proximity_mat &lt;- proximity_matrix_RF(object = object,
#                       classifier = RF_classifier,
#                       plot=TRUE,
#                       return_matrix=TRUE,
#                       title = "Test",
#                       cluster_cols = TRUE)

# not to run
# predict
# test_object # any test data
# results &lt;- predict_RF(classifier = RF_classifier, impute = TRUE,
#                       Data = test_object)
#
# # visualize the binary rules in training dataset
# plot_binary_RF(Data = test_object,
#                classifier = RF_classifier,
#                prediction = results, as_training = FALSE,
#                show_scores = TRUE,
#                top_anno = "ref",
#                show_predictions = TRUE,
#                title = "Test data")</code></pre>

<hr>
<h2 id='summary_genes_RF'>
Summarize sorted genes to rules
</h2><span id='topic+summary_genes_RF'></span>

<h3>Description</h3>

<p>After sorting genes RF by <code>sort_genes_RF</code> function <code>summary_genes_RF</code> gives an idea of how many genes you need to use to generate specific number of rules in <code>sort_rules_RF</code> function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>summary_genes_RF(sorted_genes_RF,
                 genes_altogether,
                 genes_one_vs_rest)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary_genes_RF_+3A_sorted_genes_rf">sorted_genes_RF</code></td>
<td>

<p>sorted genes object with class <code>RandomForest_sorted_genes</code> generated by <code>sort_genes_RF</code> function</p>
</td></tr>
<tr><td><code id="summary_genes_RF_+3A_genes_altogether">genes_altogether</code></td>
<td>

<p>numeric vector indicating how many genes from altogether slot (i.e. 'all') should be used each time. <code>genes_altogether</code> should be a vector with zero or positive numbers and with the same length of <code>genes_one_vs_rest</code> vector. Each element in this vector will be used with the element with the same index in <code>genes_one_vs_rest</code> vector.</p>
</td></tr>
<tr><td><code id="summary_genes_RF_+3A_genes_one_vs_rest">genes_one_vs_rest</code></td>
<td>

<p>numeric vector indicating how many genes from one_vs_rest slots (i.e. per class) should be used each time. <code>genes_one_vs_rest</code> should be a vector with zero or positive numbers and with the same length of <code>genes_altogether</code> vector. Each element in this vector will be used with the element with the same index in <code>genes_altogether</code> vector.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>summary_genes_RF</code> function helps the user to know which number of genes should be used to get the needed number of rules in <code>sort_rules_RF</code> function. NOTE: without consideration of gene replication in rules, because the rules are not sorted yet. <code>summary_genes_RF</code> workes as follows: take the first element in <code>genes_altogether</code> and <code>genes_one_vs_rest</code>, then bring this number of top genes from altogether slot and one_vs_rest slots (this number of genes will be taken from each class), respectively, from the <code>sorted_genes_RF</code> object. Then pool the extracted genes. Then take the unique genes. Then calculate the number of the possible combinations. Store the number of unique genes and rules in first row in the output dataframe then pick the second element from the <code>genes_altogether</code> and <code>genes_one_vs_rest</code> and repeat the steps again.
</p>


<h3>Value</h3>

<p>returns a dataframe with the used paramerters and the expected number of unique genes and rules. Number of rows of the dataframe equals the length of <code>genes_altogether</code> and <code>genes_one_vs_rest</code>.
</p>


<h3>Author(s)</h3>

<p>Nour-al-dain Marzouka &lt;nour-al-dain.marzouka at med.lu.se&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'># generate random data
Data &lt;- matrix(runif(8000), nrow=100, ncol=80,
               dimnames = list(paste0("G",1:100), paste0("S",1:80)))

# generate random labels
L &lt;- sample(x = c("A","B","C","D"), size = 80, replace = TRUE)

# generate random platform labels
P &lt;- sample(c("P1","P2","P3"), size = 80, replace = TRUE)

# create data object
object &lt;- ReadData(Data = Data,
                   Labels = L,
                   Platform = P,
                   verbose = FALSE)

# sort genes
genes_RF &lt;- sort_genes_RF(data_object = object,
                          seed=123456, verbose = FALSE)

# to get an idea of how many genes we will use
# and how many rules will be generated
# summary_genes_RF(sorted_genes_RF = genes_RF,
#                  genes_altogether = c(10,20,50,100,150,200),
#                  genes_one_vs_rest = c(10,20,50,100,150,200))

# creat and sort rules
# rules_RF &lt;- sort_rules_RF(data_object = object,
#                           sorted_genes_RF = genes_RF,
#                           genes_altogether = 100,
#                           genes_one_vs_rest = 100,
#                           seed=123456,
#                           verbose = FALSE)

# parameters &lt;- data.frame(
#   gene_repetition=c(3,2,1),
#   rules_one_vs_rest=0,
#   rules_altogether=c(2,3,10),
#   run_boruta=c(FALSE,"produce_error",FALSE),
#   plot_boruta = FALSE,
#   num.trees=c(100,200,300),
#   stringsAsFactors = FALSE)
# parameters

# Or you can use expand.grid to generate dataframe with all parameter combinations
# parameters &lt;- expand.grid(
#   gene_repetition=c(3,2,1),
#   rules_one_vs_rest=0,
#   rules_altogether=c(2,3,10),
#   num.trees=c(100,500,1000),
#   stringsAsFactors = FALSE)
# parameters


# test &lt;- optimize_RF(data_object = object,
#                     sorted_rules_RF = rules_RF,
#                     test_object = NULL,
#                     overall = c("Accuracy"),
#                     byclass = NULL, verbose = FALSE,
#                     parameters = parameters)
# test
# test$summary[which.max(test$summary$Accuracy),]
#
# # train the final model
# # it is preferred to increase the number of trees and rules in case you have
# # large number of samples and features
# # for quick example, we have small number of trees and rules here
# # based on the optimize_RF results we will select the parameters
# RF_classifier &lt;- train_RF(data_object = object,
#                           gene_repetition = 1,
#                           rules_altogether = 0,
#                           rules_one_vs_rest = 10,
#                           run_boruta = FALSE,
#                           plot_boruta = FALSE,
#                           probability = TRUE,
#                           num.trees = 300,
#                           sorted_rules_RF = rules_RF,
#                           boruta_args = list(),
#                           verbose = TRUE)
#
# # training accuracy
# # get the prediction labels
# # if the classifier trained using probability	= FALSE
# training_pred &lt;- RF_classifier$RF_scheme$RF_classifier$predictions
# if (is.factor(training_pred)) {
#   x &lt;- as.character(training_pred)
# }
#
# # if the classifier trained using probability	= TRUE
# if (is.matrix(training_pred)) {
#   x &lt;- colnames(training_pred)[max.col(training_pred)]
# }
#
# # training accuracy
# caret::confusionMatrix(data =factor(x),
#                 reference = factor(object$data$Labels),
#                 mode = "everything")

# not to run
# visualize the binary rules in training dataset
# plot_binary_RF(Data = object,
#                classifier = RF_classifier,
#                prediction = NULL, as_training = TRUE,
#                show_scores = TRUE,
#                top_anno = "ref",
#                show_predictions = TRUE,
#                title = "Training data")

# not to run
# Extract and plot the proximity matrix from the classifier for the training data
# it takes long time for large data
# proximity_mat &lt;- proximity_matrix_RF(object = object,
#                       classifier = RF_classifier,
#                       plot=TRUE,
#                       return_matrix=TRUE,
#                       title = "Test",
#                       cluster_cols = TRUE)

# not to run
# predict
# test_object # any test data
# results &lt;- predict_RF(classifier = RF_classifier, impute = TRUE,
#                       Data = test_object)
#
# # visualize the binary rules in training dataset
# plot_binary_RF(Data = test_object,
#                classifier = RF_classifier,
#                prediction = results, as_training = FALSE,
#                show_scores = TRUE,
#                top_anno = "ref",
#                show_predictions = TRUE,
#                title = "Test data")</code></pre>

<hr>
<h2 id='train_one_vs_rest_TSP'>
Build multiclass rule-based classifier as one-vs-rest scheme</h2><span id='topic+train_one_vs_rest_TSP'></span>

<h3>Description</h3>

<p><code>train_one_vs_rest_TSP</code> trains multiclass classifier in a one-vs-rest scheme by combining binary classifiers for each class produced by switchBox package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>train_one_vs_rest_TSP(data_object,
                      filtered_genes,
                      k_range = 10:50,
                      include_pivot = FALSE,
                      one_vs_one_scores = FALSE,
                      platform_wise_scores = FALSE,
                      disjoint = TRUE,
                      seed = NULL,
                      classes,
                      SB_arg = list(),
                      verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="train_one_vs_rest_TSP_+3A_data_object">data_object</code></td>
<td>

<p>data object generated by ReadData function. Object contains the data and labels.
</p>
</td></tr>
<tr><td><code id="train_one_vs_rest_TSP_+3A_filtered_genes">filtered_genes</code></td>
<td>

<p>filtered genes object produced by filter_genes_TSP function</p>
</td></tr>
<tr><td><code id="train_one_vs_rest_TSP_+3A_k_range">k_range</code></td>
<td>

<p>an integer or range represent the candidate number of Top Scoring Pairs (TSPs) in the individual (i.e. binary) classifiers. Default range from 10 to 50.
</p>
</td></tr>
<tr><td><code id="train_one_vs_rest_TSP_+3A_include_pivot">include_pivot</code></td>
<td>

<p>a logical indicating if the filtered genes should also be paired with all features available in the data matrix. Default is FALSE. include_pivot=FALSE means filtered genes will be paired with themselves only.</p>
</td></tr>
<tr><td><code id="train_one_vs_rest_TSP_+3A_one_vs_one_scores">one_vs_one_scores</code></td>
<td>

<p>logical indicating if rules scores for each class should be calculated as a mean of one-vs-one scores instead of one-vs-rest manner. Default is FALSE.
</p>
</td></tr>
<tr><td><code id="train_one_vs_rest_TSP_+3A_platform_wise_scores">platform_wise_scores</code></td>
<td>

<p>logical indicating if rules scores for each class should be calculated in each platform-wise then averaged instead of merging all platforms together. Default is FALSE.
</p>
</td></tr>
<tr><td><code id="train_one_vs_rest_TSP_+3A_disjoint">disjoint</code></td>
<td>

<p>is a logical value indicating whether only disjoint pairs should be considered in the final set of selected pairs; i.e. all features occur only once among the set of TSPs. This is an argument to be passed to the training function <code>SWAP.Train.KTSP</code> from <span class="pkg">switchBox</span> package.</p>
</td></tr>
<tr><td><code id="train_one_vs_rest_TSP_+3A_seed">seed</code></td>
<td>

<p>an integer to set a seed for the training process (for reproducibility).
</p>
</td></tr>
<tr><td><code id="train_one_vs_rest_TSP_+3A_classes">classes</code></td>
<td>

<p>optional vector contains the names of classes in the wanted order.
This means the individual classifiers will be ordered based on this vector. If this vector does not have all class names, then no classifiers will be train for those classes that are not mentioned and the samples from these classes will be removed from the training dataset.
</p>
</td></tr>
<tr><td><code id="train_one_vs_rest_TSP_+3A_sb_arg">SB_arg</code></td>
<td>

<p>list of any additional arguments to be passed to the training function <code>SWAP.Train.KTSP</code> from <span class="pkg">switchBox</span> package
</p>
</td></tr>
<tr><td><code id="train_one_vs_rest_TSP_+3A_verbose">verbose</code></td>
<td>

<p>a logical value indicating whether processing messages will be printed or not. Default is TRUE.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function uses <code>SWAP.Train.KTSP</code> function from <span class="pkg">switchBox</span> where the algorithm (Afsari et al (AOAS, 2014)) chooses the optimal number of rules (i.e. pairs) among the input range.</p>


<h3>Value</h3>

<p>Returns <code>OnevsrestScheme_TSP</code> object which contains one-vs-rest classifiers for the classes. These individual classifiers are top score pairs classifiers.
</p>


<h3>Author(s)</h3>

<p>Nour-al-dain Marzouka &lt;nour-al-dain.marzouka at med.lu.se&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'># random data
Data &lt;- matrix(runif(10000), nrow=100, ncol=100,
               dimnames = list(paste0("G",1:100), paste0("S",1:100)))

# labels
L &lt;- sample(x = c("A","B","C"), size = 100, replace = TRUE)

# study/platform
P &lt;- sample(c("P1","P2"), size = 100, replace = TRUE)

object &lt;- ReadData(Data = Data,
             Labels = L,
                   Platform = P)

# not to run
# switchBox package from Bioconductor is needed
# Visit their website or install switchBox package using:
# if(!requireNamespace("switchBox", quietly = TRUE)){
#       if (!requireNamespace('BiocManager', quietly = TRUE)) {
#       install.packages('BiocManager')
#      }
#      BiocManager::install('switchBox')", call. = FALSE)
#  }

#filtered_genes &lt;- filter_genes_TSP(data_object = object,
#                                  filter = "one_vs_rest",
#                                  platform_wise = FALSE,
#                                  featureNo = 10,
#                                  UpDown = TRUE,
#                                  verbose = FALSE)

# training
# classifier &lt;- train_one_vs_rest_TSP(data_object = object,
#                              filtered_genes = filtered_genes,
#                              k_range = 10:50,
#                              include_pivot = FALSE,
#                              one_vs_one_scores = FALSE,
#                              platform_wise_scores = FALSE,
#                              seed = 1234,
#                              verbose = FALSE)

# results &lt;- predict_one_vs_rest_TSP(classifier = classifier,
#                                   Data = object,
#                                   tolerate_missed_genes = TRUE,
#                                   weighted_votes = TRUE,
#                                   verbose = FALSE)

# Confusion Matrix and Statistics on training data
#  caret::confusionMatrix(data = factor(results$max_score, levels = unique(L)),
#                         reference = factor(L, levels = unique(L)),
#                         mode="everything")

# plot_binary_TSP(Data = object, classes=c("A","B","C"),
#                 classifier = classifier,
#                 prediction = results,
#                 title = "Test")
</code></pre>

<hr>
<h2 id='train_RF'>
Train pair-based random forest model
</h2><span id='topic+train_RF'></span>

<h3>Description</h3>

<p><code>train_RF</code> trains random forest model based on binary gene rules (such as geneA&lt;geneB).
Boruta package is used to remove the unimportant rules and ranger function from ranger package is used for the training.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>train_RF(data_object,
          sorted_rules_RF,
          gene_repetition = 1,
          rules_altogether = 50,
          rules_one_vs_rest = 50,
          run_boruta = FALSE,
          plot_boruta = FALSE,
          boruta_args = list(doTrace = 1),
          num.trees = 500,
          min.node.size = 1,
          importance = "impurity",
          write.forest = TRUE,
          keep.inbag = TRUE,
          probability	= TRUE,
          verbose = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="train_RF_+3A_data_object">data_object</code></td>
<td>

<p>data object generated by ReadData function. Object contains the data and labels.
</p>
</td></tr>
<tr><td><code id="train_RF_+3A_sorted_rules_rf">sorted_rules_RF</code></td>
<td>

<p>RandomForest_sorted_rules object generated by sort_rules_RF function
</p>
</td></tr>
<tr><td><code id="train_RF_+3A_gene_repetition">gene_repetition</code></td>
<td>

<p>interger indicating how many times the gene is allowed to be repeated in the pairs/rules. Default is 1.
</p>
</td></tr>
<tr><td><code id="train_RF_+3A_rules_altogether">rules_altogether</code></td>
<td>

<p>integer indicating how many unique rules to be used from altogether slot in the sorted rules object. Default is 200.
</p>
</td></tr>
<tr><td><code id="train_RF_+3A_rules_one_vs_rest">rules_one_vs_rest</code></td>
<td>

<p>integer indicating how many unique rules to be used from each one_vs_rest slot (class vs rest slots) in the sorted rules object. Default is 200.
</p>
</td></tr>
<tr><td><code id="train_RF_+3A_run_boruta">run_boruta</code></td>
<td>

<p>logical indicates if Boruta algorithm should be run before building the RF model. Boruta will be used to remove the unimportant rules. Default is FALSE.
</p>
</td></tr>
<tr><td><code id="train_RF_+3A_plot_boruta">plot_boruta</code></td>
<td>

<p>logical indicates if Boruta is allowed to plot importance history plots. Default is FALSE.
</p>
</td></tr>
<tr><td><code id="train_RF_+3A_boruta_args">boruta_args</code></td>
<td>

<p>list of argument to be passed to Boruta algorithm. Default for doTrace argument in Boruta is 1.
</p>
</td></tr>
<tr><td><code id="train_RF_+3A_num.trees">num.trees</code></td>
<td>

<p>an integer. Number of trees. Default is 500. It is recommended to increase num.trees in case of having large number of features (ranger function argument).
</p>
</td></tr>
<tr><td><code id="train_RF_+3A_min.node.size">min.node.size</code></td>
<td>

<p>an integer. Minimal node size. Default is 1. (ranger function argument)
</p>
</td></tr>
<tr><td><code id="train_RF_+3A_importance">importance</code></td>
<td>

<p>Variable importance mode, should be one of 'impurity', 'impurity_corrected', 'permutation'.
Defualt is 'impurity' (ranger function argument)
</p>
</td></tr>
<tr><td><code id="train_RF_+3A_write.forest">write.forest</code></td>
<td>

<p>Save ranger.forest object, required for prediction. Default is TRUE. (ranger function argument). Always should be true to return the trained RF model.
</p>
</td></tr>
<tr><td><code id="train_RF_+3A_keep.inbag">keep.inbag</code></td>
<td>

<p>Save how often observations are in-bag in each tree. Default is TRUE. (ranger function argument). Needed for co-clustering heatmaps.
</p>
</td></tr>
<tr><td><code id="train_RF_+3A_probability">probability</code></td>
<td>

<p>Grow a probability forest as in Malley et al. (2012). Default is TRUE. (ranger function argument). Needed to plot probability scores in the binary rules heatmaps. If TRUE, when the classifier is used to predict a sample class the user will get &quot;ranger.prediction&quot; object with a matrix with scores for each class. If FALSE, the classifier will give a &quot;ranger.prediction&quot; object with the predicted class without scores for each class.
</p>
</td></tr>
<tr><td><code id="train_RF_+3A_verbose">verbose</code></td>
<td>

<p>a logical value indicating whether processing messages will be printed or not. Default is TRUE.
</p>
</td></tr>
<tr><td><code id="train_RF_+3A_...">...</code></td>
<td>

<p>any additional arguments to be passed to ranger function (i.e. random forest function) in ranger package. For example, seed for reproducibility. Note, seed argument will be used also for Burota run.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>train_RF function extracts the lists of the sorted rules from altogether and classes slots, then it keep the top rules those fit with the gene_repetition number, this step reduces the number of the rule dramatically. From the left rules, rules_altogether and rules_one_vs_rest determine how many rules will be used. In case rules_altogether and rules_one_vs_rest were larger than the left rules then all the rules will be used. After that these rules will be pooled in one list and fid to Boruta function to remove the unimportant rules. Then random forest will be trained on the important rules.
</p>


<h3>Value</h3>

<p>train_RF returns rule_based_RandomForest object which contains the final RF classifier and the used genes and rules in the final model.
</p>
<p>Boruta results are also included in the object.
</p>
<p>The object also contains TrainingMatrix which is a binary matrix for the rules in the training data, this is used for imputation purposes during the prediction if the sample misses some values.
</p>


<h3>Author(s)</h3>

<p>Nour-al-dain Marzouka &lt;nour-al-dain.marzouka at med.lu.se&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'># generate random data
Data &lt;- matrix(runif(800), nrow=100, ncol=80,
               dimnames = list(paste0("G",1:100), paste0("S",1:80)))

# generate random labels
L &lt;- sample(x = c("A","B","C","D"), size = 80, replace = TRUE)

# generate random platform labels
P &lt;- sample(c("P1","P2","P3"), size = 80, replace = TRUE)

# create data object
object &lt;- ReadData(Data = Data,
                   Labels = L,
                   Platform = P,
                   verbose = FALSE)

# sort genes
genes_RF &lt;- sort_genes_RF(data_object = object,
                          seed=123456, verbose = FALSE)

# to get an idea of how many genes we will use
# and how many rules will be generated
# summary_genes_RF(sorted_genes_RF = genes_RF,
#                  genes_altogether = c(10,20,50,100,150,200),
#                  genes_one_vs_rest = c(10,20,50,100,150,200))

# creat and sort rules
# rules_RF &lt;- sort_rules_RF(data_object = object,
#                           sorted_genes_RF = genes_RF,
#                           genes_altogether = 100,
#                           genes_one_vs_rest = 100,
#                           seed=123456,
#                           verbose = FALSE)

# parameters &lt;- data.frame(
#   gene_repetition=c(3,2,1),
#   rules_one_vs_rest=0,
#   rules_altogether=c(2,3,10),
#   run_boruta=c(FALSE,"produce_error",FALSE),
#   plot_boruta = FALSE,
#   num.trees=c(100,200,300),
#   stringsAsFactors = FALSE)

# parameters

# test &lt;- optimize_RF(data_object = object,
#                     sorted_rules_RF = rules_RF,
#                     test_object = NULL,
#                     overall = c("Accuracy"),
#                     byclass = NULL, verbose = FALSE,
#                     parameters = parameters)
# test
# test$summary[which.max(test$summary$Accuracy),]
#
# # train the final model
# # it is preferred to increase the number of trees and rules in case you have
# # large number of samples and features
# # for quick example, we have small number of trees and rules here
# # based on the optimize_RF results we will select the parameters
# RF_classifier &lt;- train_RF(data_object = object,
#                           gene_repetition = 1,
#                           rules_altogether = 0,
#                           rules_one_vs_rest = 10,
#                           run_boruta = FALSE,
#                           plot_boruta = FALSE,
#                           probability = TRUE,
#                           num.trees = 300,
#                           sorted_rules_RF = rules_RF,
#                           boruta_args = list(),
#                           verbose = TRUE)
#
# # training accuracy
# # get the prediction labels
# # if the classifier trained using probability	= FALSE
# training_pred &lt;- RF_classifier$RF_scheme$RF_classifier$predictions
# if (is.factor(training_pred)) {
#   x &lt;- as.character(training_pred)
# }
#
# # if the classifier trained using probability	= TRUE
# if (is.matrix(training_pred)) {
#   x &lt;- colnames(training_pred)[max.col(training_pred)]
# }
#
# # training accuracy
# caret::confusionMatrix(data =factor(x),
#                 reference = factor(object$data$Labels),
#                 mode = "everything")

# not to run
# visualize the binary rules in training dataset
# plot_binary_RF(Data = object,
#                classifier = RF_classifier,
#                prediction = NULL, as_training = TRUE,
#                show_scores = TRUE,
#                top_anno = "ref",
#                show_predictions = TRUE,
#                title = "Training data")

# not to run
# predict
# test_object # any test data
# results &lt;- predict_RF(classifier = RF_classifier, impute = TRUE,
#                       Data = test_object)
#
# # visualize the binary rules in training dataset
# plot_binary_RF(Data = test_object,
#                classifier = RF_classifier,
#                prediction = results, as_training = FALSE,
#                show_scores = TRUE,
#                top_anno = "ref",
#                show_predictions = TRUE,
#                title = "Test data")
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
