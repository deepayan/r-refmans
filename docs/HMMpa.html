<!DOCTYPE html><html><head><title>Help for package HMMpa</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {HMMpa}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#AIC_HMM'>
<p>AIC and BIC Value for a Discrete Time Hidden Markov Model</p></a></li>
<li><a href='#Baum_Welch_algorithm'>
<p>Estimation Using the Baum-Welch Algorithm</p></a></li>
<li><a href='#cut_off_point_method'>
<p>Cut-Off Point Method for Assigning Physical Activity Patterns</p></a></li>
<li><a href='#dgenpois'>
<p>The Generalized Poisson Distribution</p></a></li>
<li><a href='#direct_numerical_maximization'>
<p>Estimation by Directly Maximizing the log-Likelihood</p></a></li>
<li><a href='#forward_backward_algorithm'>
<p>Calculating Forward and Backward Probabilities and Likelihood</p></a></li>
<li><a href='#HMM_based_method'>
<p>Hidden Markov Method for Predicting Physical Activity Patterns</p></a></li>
<li><a href='#HMM_decoding'>
<p>Algorithm for Decoding Hidden Markov Models (local or global)</p></a></li>
<li><a href='#HMM_simulation'>
<p>Generating Realizations of a Hidden Markov Model</p></a></li>
<li><a href='#HMM_training'>
<p>Training of Hidden Markov Models</p></a></li>
<li><a href='#HMMpa-package'>
<p>Analysing Accelerometer Data Using Hidden Markov Models</p></a></li>
<li><a href='#initial_parameter_training'>
<p>Algorithm to Find Plausible Starting Values for Parameter Estimation</p></a></li>
<li><a href='#local_decoding_algorithm'>
<p>Algorithm for Decoding Hidden Markov Models (local)</p></a></li>
<li><a href='#Viterbi_algorithm'>
<p>Algorithm for Decoding Hidden Markov Models (global)</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Analysing Accelerometer Data Using Hidden Markov Models</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.1</td>
</tr>
<tr>
<td>Author:</td>
<td>Vitali Witowski, Ronja Foraita</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Ronja Foraita &lt;foraita@leibniz-bips.de&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Analysing time-series accelerometer data to quantify length and 
             intensity of physical activity using hidden Markov models. 
             It also contains the traditional cut-off point method.
             Witowski V, Foraita R, Pitsiladis Y, Pigeot I, 
             Wirsik N (2014)&lt;<a href="https://doi.org/10.1371%2Fjournal.pone.0114089">doi:10.1371/journal.pone.0114089</a>&gt;.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10.0)</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2018-04-23 08:53:53 UTC; foraita</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2018-11-15 09:00:03 UTC</td>
</tr>
</table>
<hr>
<h2 id='AIC_HMM'>
AIC and BIC Value for a Discrete Time Hidden Markov Model
</h2><span id='topic+AIC_HMM'></span><span id='topic+BIC_HMM'></span>

<h3>Description</h3>

<p>Computes the Aikaike's information criterion and the Bayesian information criterion for a discrete time hidden Markov model, given a time-series of observations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>AIC_HMM(logL, m, k)
BIC_HMM(size, m, k, logL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="AIC_HMM_+3A_size">size</code></td>
<td>
<p>length of the time-series of observations x (also <code>T</code>).</p>
</td></tr>
<tr><td><code id="AIC_HMM_+3A_m">m</code></td>
<td>
<p>number of states in the Markov chain of the model.</p>
</td></tr>
<tr><td><code id="AIC_HMM_+3A_k">k</code></td>
<td>
<p>single numeric value representing the number of parameters of the underlying  distribution of 
the observation process (e.g. k=2 for the normal distribution (mean and standard deviation)).</p>
</td></tr>
<tr><td><code id="AIC_HMM_+3A_logl">logL</code></td>
<td>
<p>logarithmized likelihood of the model.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For a discrete-time hidden Markov model, AIC and BIC are as follows (MacDonald &amp; Zucchini (2009, Paragraph 6.1 and A.2.3)):
</p>
<p style="text-align: center;"><code class="reqn">\mbox{AIC} = -2 logL + 2p</code>
</p>

<p style="text-align: center;"><code class="reqn">\mbox{BIC} = -2 logL + p \log T</code>
</p>

<p>where <code>T</code> indicates the length/size of the observation time-series and <code>p</code> denotes the number of independent parameters of the model. In case of a HMM as provided by this package, where <code>k</code> and <code>m</code> are defined as in the arguments above, <code>p</code> can be calculated as follows:
</p>
<p style="text-align: center;"><code class="reqn">p = m^2+km-1.</code>
</p>



<h3>Value</h3>

<p>The AIC or BIC value of the fitted hidden Markov model.
</p>


<h3>Author(s)</h3>

<p>Based on MacDonald &amp; Zucchini (2009, Paragraph 6.1 and A.2.3). Implementation by Vitali Witowski (2013). 
</p>


<h3>References</h3>

<p>MacDonald, I. L.,  Zucchini, W. (2009) <em>Hidden Markov Models for Time Series: An Introduction Using R</em>, Boca Raton: Chapman &amp; Hall.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+HMM_training">HMM_training</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
################################################################
### Fictitious observations ####################################
################################################################

x &lt;- c(1,16,19,34,22,6,3,5,6,3,4,1,4,3,5,7,9,8,11,11,
  14,16,13,11,11,10,12,19,23,25,24,23,20,21,22,22,18,7,
  5,3,4,3,2,3,4,5,4,2,1,3,4,5,4,5,3,5,6,4,3,6,4,8,9,12,
  9,14,17,15,25,23,25,35,29,36,34,36,29,41,42,39,40,43,
  37,36,20,20,21,22,23,26,27,28,25,28,24,21,25,21,20,21,
  11,18,19,20,21,13,19,18,20,7,18,8,15,17,16,13,10,4,9,
  7,8,10,9,11,9,11,10,12,12,5,13,4,6,6,13,8,9,10,13,13,
  11,10,5,3,3,4,9,6,8,3,5,3,2,2,1,3,5,11,2,3,5,6,9,8,5,
  2,5,3,4,6,4,8,15,12,16,20,18,23,18,19,24,23,24,21,26,
  36,38,37,39,45,42,41,37,38,38,35,37,35,31,32,30,20,39,
  40,33,32,35,34,36,34,32,33,27,28,25,22,17,18,16,10,9,
  5,12,7,8,8,9,19,21,24,20,23,19,17,18,17,22,11,12,3,9,
  10,4,5,13,3,5,6,3,5,4,2,5,1,2,4,4,3,2,1) 

### Assummptions (probability vector, transition matrix, 
### and distribution parameters)

delta &lt;- c(0.25,0.25,0.25,0.25)

gamma &lt;- 0.7 * diag(length(delta)) + rep(0.3 / length(delta))

distribution_class &lt;- "pois"

distribution_theta &lt;- list(lambda = c(4,9,17,25))


### log-likelihood

logL &lt;- forward_backward_algorithm (x = x, 
                delta = delta, gamma=gamma,  
                distribution_class= distribution_class, 
                distribution_theta=distribution_theta)$logL


### the Poisson distribution has one paramter, hence k=1

AIC_HMM(logL = logL, m = length(delta), k = 1)

BIC_HMM(size = length(x) , logL = logL, m = length(delta), k = 1)

</code></pre>

<hr>
<h2 id='Baum_Welch_algorithm'>
Estimation Using the Baum-Welch Algorithm
</h2><span id='topic+Baum_Welch_algorithm'></span>

<h3>Description</h3>

<p>Estimates the parameters of a (non-stationary) discrete-time hidden Markov model. The Baum-Welch algorithm is a version of the EM (Estimation/Maximization) algorithm.  See MacDonald &amp; Zucchini (2009, Paragraph 4.2) for further details. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Baum_Welch_algorithm(x, m, delta, gamma, distribution_class, 
                     distribution_theta, discr_logL = FALSE, 
                     discr_logL_eps = 0.5,
                     BW_max_iter = 50, BW_limit_accuracy = 0.001,
                     BW_print = TRUE, Mstep_numerical = FALSE, 
                     DNM_limit_accuracy = 0.001, DNM_max_iter = 50, 
                     DNM_print = 2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Baum_Welch_algorithm_+3A_x">x</code></td>
<td>

<p>a vector object containing the time-series of observations that are assumed to be realizations of the (hidden Markov state dependent) observation process of the model.</p>
</td></tr>
<tr><td><code id="Baum_Welch_algorithm_+3A_m">m</code></td>
<td>
<p>a (finite) number of states in the hidden Markov chain.</p>
</td></tr>
<tr><td><code id="Baum_Welch_algorithm_+3A_delta">delta</code></td>
<td>

<p>a vector object containing starting values for the marginal probability distribution of the <code>m</code> 
states of the Markov chain at the time point <code>t=1</code> for the Baum-Welch algorithm.</p>
</td></tr>
<tr><td><code id="Baum_Welch_algorithm_+3A_gamma">gamma</code></td>
<td>
<p>a matrix (<code>ncol=nrow=m</code>) containing starting values for the transition matrix of the hidden Markov chain.</p>
</td></tr>
<tr><td><code id="Baum_Welch_algorithm_+3A_distribution_class">distribution_class</code></td>
<td>

<p>a single character string object with the abbreviated name of the <code>m</code> observation
distributions of the Markov dependent observation process. The following distributions are supported:
Poisson (<code>pois</code>); generalized Poisson (<code>genpois</code>, parameter estimation
via the Baum-Welch algorithm is only supported if the M-step is performed numerically, i.e. if <code>Mstep_numerical = TRUE</code>); 
normal (<code>norm</code>).</p>
</td></tr>
<tr><td><code id="Baum_Welch_algorithm_+3A_distribution_theta">distribution_theta</code></td>
<td>

<p>a list object containing starting values for the parameters of the <code>m</code> observation distributions of the observation process that are dependent on the hidden Markov state.</p>
</td></tr>
<tr><td><code id="Baum_Welch_algorithm_+3A_discr_logl">discr_logL</code></td>
<td>

<p>a logical object indicating whether the discrete log-likelihood should be used (for <code>distribution_class="norm"</code>) for estimating the model specific parameters instead
of the general log-likelihood. See MacDonald &amp; Zucchini (2009, Paragraph 1.2.3) for further details.  Default value is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="Baum_Welch_algorithm_+3A_discr_logl_eps">discr_logL_eps</code></td>
<td>

<p>a single numerical value to approximately determine the
discrete likelihood for a hidden Markov model based on nomal distributions (for <code>"norm"</code>).  Default value is <code>0.5</code>.  See MacDonald &amp; Zucchini (2009, Paragraph 1.2.3) for further details.</p>
</td></tr>
<tr><td><code id="Baum_Welch_algorithm_+3A_bw_max_iter">BW_max_iter</code></td>
<td>

<p>a single numerical value representing the maximum number of iterations in the Baum-Welch algorithm. Default value is <code>50</code>.</p>
</td></tr>
<tr><td><code id="Baum_Welch_algorithm_+3A_bw_limit_accuracy">BW_limit_accuracy</code></td>
<td>

<p>a single numerical value representing the convergence criterion of the 
Baum-Welch algorithm. Default value is <code>0.001</code>.</p>
</td></tr>
<tr><td><code id="Baum_Welch_algorithm_+3A_bw_print">BW_print</code></td>
<td>

<p>a logical object indicating whether the log-likelihood at each iteration-step shall be printed. Default value is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="Baum_Welch_algorithm_+3A_mstep_numerical">Mstep_numerical</code></td>
<td>

<p>a logical object indicating whether the Maximization Step of the Baum-Welch algorithm shall be performed by numerical maximization using the <a href="stats.html#topic+nlm">nlm</a>-function.  Default value is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="Baum_Welch_algorithm_+3A_dnm_limit_accuracy">DNM_limit_accuracy</code></td>
<td>

<p>a single numerical value representing the convergence criterion of the numerical
maximization algorithm using the <a href="stats.html#topic+nlm">nlm</a>-function (used to perform the M-step of 
the Baum-Welch-algorithm). Default value is <code>0.001</code>.</p>
</td></tr>
<tr><td><code id="Baum_Welch_algorithm_+3A_dnm_max_iter">DNM_max_iter</code></td>
<td>

<p>a single numerical value representing the maximum number of iterations
of the numerical maximization using the <a href="stats.html#topic+nlm">nlm</a>-function (used to perform the M-step of the 
Baum-Welch-algorithm). Default value is <code>50</code>.</p>
</td></tr>
<tr><td><code id="Baum_Welch_algorithm_+3A_dnm_print">DNM_print</code></td>
<td>

<p>a single numerical value to determine the level of printing of the <code>nlm</code>-function.  See <code>nlm</code>-function for further informations. The value <code>0</code> suppresses, that no printing will be outputted. Default value is <code>2</code> for full printing.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>Baum_Welch_algorithm</code> returns a list containing the estimated parameters of the hidden Markov model and other components. See MacDonald &amp; Zucchini (2009, Paragraph 4.2) for further details on the calculated objects within this algorithm. 
</p>
<table>
<tr><td><code>x</code></td>
<td>
<p>input time-series of observations.</p>
</td></tr>
<tr><td><code>m</code></td>
<td>
<p>input number of hidden states in the Markov chain.</p>
</td></tr>
<tr><td><code>zeta</code></td>
<td>
<p>a (T,m)-matrix (when T indicates the length/size of the observation time-series and m the number of states of the HMM) containing probabilities (estimates of the conditional expectations of the missing data given the observations and the estimated model specific parameters) calculated by the algorithm. See MacDonald &amp; Zucchini (2009, Paragraph 4.2.2) for further details.</p>
</td></tr>
<tr><td><code>eta</code></td>
<td>
<p>a (T,m,m)-dimensional-array (when T indicates the length of the observation time-series and m the number of states of the HMM) containing probabilities (estimates of the conditional expectations of the missing data given the observations and the estimated model specific parameters) calculated by the algorithm. See MacDonald &amp; Zucchini (2009, Paragraph 4.2.2) for further details.</p>
</td></tr>
<tr><td><code>logL</code></td>
<td>
<p>a numerical value representing the logarithmized likelihood calculated by the 
<code><a href="#topic+forward_backward_algorithm">forward_backward_algorithm</a></code>.</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>number of performed iterations.</p>
</td></tr>
<tr><td><code>BIC</code></td>
<td>
<p>a numerical value representing the Bayesian information criterion for the hidden Markov model 
with estimated parameters.</p>
</td></tr>
<tr><td><code>delta</code></td>
<td>
<p>a vector object containing the estimates for the marginal probability distribution of the <code>m</code> 
states of the Markov chain at time-point point <code>t=1</code>.</p>
</td></tr>
<tr><td><code>gamma</code></td>
<td>
<p>a matrix containing the estimates for the transition matrix of the hidden Markov chain.</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
<p>other input values (as arguments above). In the case that the algorithm stops before the targeted accuracy or the maximum number of iterations has been reached, further values are displayed and the estimates from the last successful iteration step are saved.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>The basic algorithm for a Poisson-HMM is provided by MacDonald &amp; Zucchini (2009, Paragraph 4.2, Paragraph A.2.3).  Extension and implementation by Vitali Witowski (2013).
</p>


<h3>References</h3>

<p>Baum, L., Petrie, T., Soules, G., Weiss, N. (1970). A maximization technique occurring in the statistical analysis of probabilistic functions of markov chains. The annals of mathematical statistics, vol. <b>41</b>(1), 164&ndash;171.
</p>
<p>Dempster, A., Laird, N., Rubin, D. (1977). Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society. Series B (Methodological), vol. <b>39</b>(1), 1&ndash;38.
</p>
<p>MacDonald, I. L.,  Zucchini, W. (2009) <em>Hidden Markov Models for Time Series: An Introduction Using R</em>, Boca Raton: Chapman &amp; Hall.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+HMM_based_method">HMM_based_method</a></code>, 
<code><a href="#topic+HMM_training">HMM_training</a></code>, 
<code><a href="#topic+direct_numerical_maximization">direct_numerical_maximization</a></code>,  
<code><a href="#topic+forward_backward_algorithm">forward_backward_algorithm</a></code>, 
<code><a href="#topic+initial_parameter_training">initial_parameter_training</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
################################################################
### Fictitious observations ####################################
################################################################

x &lt;- c(1,16,19,34,22,6,3,5,6,3,4,1,4,3,5,7,9,8,11,11,
  14,16,13,11,11,10,12,19,23,25,24,23,20,21,22,22,18,7,
  5,3,4,3,2,3,4,5,4,2,1,3,4,5,4,5,3,5,6,4,3,6,4,8,9,12,
  9,14,17,15,25,23,25,35,29,36,34,36,29,41,42,39,40,43,
  37,36,20,20,21,22,23,26,27,28,25,28,24,21,25,21,20,21,
  11,18,19,20,21,13,19,18,20,7,18,8,15,17,16,13,10,4,9,
  7,8,10,9,11,9,11,10,12,12,5,13,4,6,6,13,8,9,10,13,13,
  11,10,5,3,3,4,9,6,8,3,5,3,2,2,1,3,5,11,2,3,5,6,9,8,5,
  2,5,3,4,6,4,8,15,12,16,20,18,23,18,19,24,23,24,21,26,
  36,38,37,39,45,42,41,37,38,38,35,37,35,31,32,30,20,39,
  40,33,32,35,34,36,34,32,33,27,28,25,22,17,18,16,10,9,
  5,12,7,8,8,9,19,21,24,20,23,19,17,18,17,22,11,12,3,9,
  10,4,5,13,3,5,6,3,5,4,2,5,1,2,4,4,3,2,1)  


### Assummptions (number of states, probability vector, 
### transition matrix, and distribution parameters)

m &lt;- 4

delta &lt;- c(0.25,0.25,0.25,0.25)

gamma &lt;- 0.7 * diag(m) + rep(0.3 / m)

distribution_class &lt;- "pois"

distribution_theta &lt;- list(lambda = c(4,9,17,25))


### Estimation of a HMM using the Baum-Welch algorithm

trained_HMM_with_m_hidden_states &lt;- 
    Baum_Welch_algorithm(x = x, 
        m = m, 
        delta = delta, 
        gamma = gamma,
        distribution_class = distribution_class, 
        distribution_theta = distribution_theta)

print(trained_HMM_with_m_hidden_states)

</code></pre>

<hr>
<h2 id='cut_off_point_method'>
Cut-Off Point Method for Assigning Physical Activity Patterns
</h2><span id='topic+cut_off_point_method'></span>

<h3>Description</h3>

<p>This function assigns an activity range to each observation of a time-series, such as for a sequence of impulse counts recorded by an accelerometer.  The activity ranges are defined by thresholds called &ldquo;cut-off points&rdquo;.  Furthermore, bout periods are analysed (see Details for further informations). 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cut_off_point_method (x, cut_points, 
        names_activity_ranges = NA, hidden_PA_levels = NA, 
        bout_lengths = NULL, plotting = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cut_off_point_method_+3A_x">x</code></td>
<td>

<p>a vector object of length <code>T</code>  containing non-negative observations of a time-series, such as a sequence of accelerometer impulse counts.</p>
</td></tr>
<tr><td><code id="cut_off_point_method_+3A_cut_points">cut_points</code></td>
<td>

<p>a vector object containing cut-off points to separate activity ranges.  For instance, the vector c(7,15,23) separates the four activity ranges [0,7);[7,15);[15,23);[23,Inf).</p>
</td></tr>
<tr><td><code id="cut_off_point_method_+3A_names_activity_ranges">names_activity_ranges</code></td>
<td>

<p>an optional character string vector to name the activity ranges induced by the cut-points.  This vector must contain one element more than the vector <code>cut_points</code>.</p>
</td></tr>
<tr><td><code id="cut_off_point_method_+3A_bout_lengths">bout_lengths</code></td>
<td>

<p>a vector object (with even number of elemets) to define the range of the bout intervals (see Details for the definition of bouts).  
For instance, 
</p>
<p><code>bout_lengths=c(1,1,2,2,3,10,11,20,1,20)</code> defines the five bout intervals 
[1,1] (1 count); [2,2] (2 counts); [3,10] (3-10 counts); [11,20] (11-20 counts); [1,20] 
(1-20 counts - overlapping with other bout intervalls is possible). Default value is <code>bout_lengths=NULL</code>. 
</p>
</td></tr>
<tr><td><code id="cut_off_point_method_+3A_hidden_pa_levels">hidden_PA_levels</code></td>
<td>

<p>an optional vector object of length <code>T</code> containing a sequence of the estimated hidden physical activity levels (i.e. means) underlying the time-series of accelerometer counts.  Such a sequence can be extracted by decoding a trained hidden Markov model.  The cut-point method classifies then each count by its level in the hidden Markov chain that generates the physical activity counts, and does not use the observed count value (see <code><a href="#topic+HMM_based_method">HMM_based_method</a></code> for further details).  Default is NA (for the traditional cut-point method).</p>
</td></tr>
<tr><td><code id="cut_off_point_method_+3A_plotting">plotting</code></td>
<td>

<p>a numeric value between <code>0</code> and <code>5</code> (generates different outputs). NA suppresses graphical output. Default value is <code>0</code>.<br />
<code>0</code>: output 1-5 <br />
<code>1</code>: summary of all results <br />
<code>2</code>: time series of activity counts, classified into activity ranges <br />
<code>3</code>: time series of bouts (and, if available, the sequence of the estimated hidden physical activity levels, extracted by decoding a trained HMM, in green colour)<br />
<code>4</code>: barplots of absolute and relative frequencies of time spent in different activity ranges<br />
<code>5</code>: barplots of absolute frequencies of different bout intervals (overall and by activity ranges )</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A bout is defined as a period of time spending a defined intensity of physical activities in a specified physical activity range, without switching to activity intensities in a different activity range.
</p>


<h3>Value</h3>

<p><code> cut_off_point_method </code> returns a list containing the extracted sequence of activity ranges  and plots key figures. 
</p>
<table>
<tr><td><code>activity_ranges</code></td>
<td>
<p>an array object containing the cut-off intervals that indicate the activity ranges.</p>
</td></tr>
<tr><td><code>classification</code></td>
<td>
<p>an integer vector containing the sequence of activity ranges  that were assigned to the observed time-series of accelerometer counts.  If <code>hidden_PA_levels=NA</code>, then <code>classification</code> is the output of the traditional cut-point method, meaning that an activity range has been assigned to each accelerometer count over its observed value actual position.  In case when <code>hidden_PA_levels</code> is available, <code>classification</code> is the output of the extendend cut-point method using hidden Markov models (see <code><a href="#topic+HMM_based_method">HMM_based_method</a></code> for further details). </p>
</td></tr>
<tr><td><code>classification_per_activity_range</code></td>
<td>
<p>a pairlist object containing the classification of the observed counts by the assigned activity range.</p>
</td></tr>
<tr><td><code>freq_acitvity_range</code></td>
<td>
<p>table object containing the absolute frequencies of classifications into activity ranges.</p>
</td></tr>
<tr><td><code>rel_freq_acitvity_range</code></td>
<td>
<p>table object containing the relative frequencies of classifications into activity ranges.</p>
</td></tr>
<tr><td><code>quantity_of_bouts</code></td>
<td>
<p>overall number of bouts.</p>
</td></tr>
<tr><td><code>bout_periods</code></td>
<td>
<p>an array including the bout length assigned to acitiy ranges.</p>
</td></tr>
<tr><td><code>abs_freq_bouts_el</code></td>
<td>
<p>a pairlist object containing the absolute frequency of bout length per epoch length (aggregated).</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Vitali Witowski (2013).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+HMM_based_method">HMM_based_method</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

################################################################
### Fictitious activity counts #################################
################################################################

x &lt;- c(1,16,19,34,22,6,3,5,6,3,4,1,4,3,5,7,9,8,11,11,
  14,16,13,11,11,10,12,19,23,25,24,23,20,21,22,22,18,7,
  5,3,4,3,2,3,4,5,4,2,1,3,4,5,4,5,3,5,6,4,3,6,4,8,9,12,
  9,14,17,15,25,23,25,35,29,36,34,36,29,41,42,39,40,43,
  37,36,20,20,21,22,23,26,27,28,25,28,24,21,25,21,20,21,
  11,18,19,20,21,13,19,18,20,7,18,8,15,17,16,13,10,4,9,
  7,8,10,9,11,9,11,10,12,12,5,13,4,6,6,13,8,9,10,13,13,
  11,10,5,3,3,4,9,6,8,3,5,3,2,2,1,3,5,11,2,3,5,6,9,8,5,
  2,5,3,4,6,4,8,15,12,16,20,18,23,18,19,24,23,24,21,26,
  36,38,37,39,45,42,41,37,38,38,35,37,35,31,32,30,20,39,
  40,33,32,35,34,36,34,32,33,27,28,25,22,17,18,16,10,9,
  5,12,7,8,8,9,19,21,24,20,23,19,17,18,17,22,11,12,3,9,
  10,4,5,13,3,5,6,3,5,4,2,5,1,2,4,4,3,2,1)  


################################################################
### i.) Tradionional cut_point method ##########################
################################################################

### Assigning activity ranges to activity counts using 
### fictitious cut-off points that produce the four activity 
### ranges "sedentary"", "light"", "moderate"", and "vigorous". 

solution_of_traditional_cut_off_point_method &lt;- 
    cut_off_point_method(x = x, 
      cut_points = c(5,15,23), 
      names_activity_ranges = c("SED","LIG","MOD","VIG"), 
      bout_lengths = c(1,1,2,2,3,3,4,4,5,5,6,12,
      13,40,41,265,1,265), 
      plotting = 0)

print(solution_of_traditional_cut_off_point_method)


###############################################################
### ii.) Extension of the tradionional cut_point method #######
###      using HMMs      ######################################
###############################################################

## The following three steps define an extension of the 
## traditional cut-off method by first extracting the hidden 
## physical activity pattern behind the accelerometer counts 
## using a HMM (those three steps are basically combined in 
## the function HMM_based_method, see HMM_based_method for 
## further details and references): 


### Step 1 ##################################################### 

## Train hidden Markov model for different number of 
## states m=2,...,6 and select the model with the most 
## plausible m

m_trained_HMM &lt;- 
    HMM_training(x = x, 
      min_m = 2, 
      max_m = 6, BW_print=FALSE,
      distribution_class = "pois")$trained_HMM_with_selected_m


### Step 2 ##################################################### 

## Decode the trained HMM (by using the 
## Viterbi algorithm (global decoding)) to get the estimated 
## sequence of hidden physical activity levels 
## underlying the the accelerometer counts 

## You have to compute 'm_trained_HMM' first (see Step 1)

global_decoding &lt;- 
    HMM_decoding(x = x, 
      m = m_trained_HMM$m, 
      delta = m_trained_HMM$delta, 
      gamma = m_trained_HMM$gamma, 
      distribution_class = m_trained_HMM$distribution_class, 
      distribution_theta = m_trained_HMM$distribution_theta,
      decoding_method = "global")
        
hidden_PA_levels &lt;- 
    global_decoding$decoding_distr_means


### Step 3 #####################################################

## Assigning activity ranges to activity counts using the 
## information extracted by decoding the HMM for the counts 
## (PA-levels) and fictitious cut-off points that produce 
## four so-called activity ranges:"sedentary", "light", 
## "moderate" and "vigorous":

## You have to compute 'm_trained_HMM' and 
## 'hidden_PA_levels' first (see above)

solution_of_HMM_based_cut_off_point_method &lt;- 
 cut_off_point_method(x = x, 
  hidden_PA_levels = hidden_PA_levels, 
  cut_points = c(5,15,23), 
  names_activity_ranges = c("SED","LIG","MOD","VIG"), 
  bout_lengths = c(1,1,2,2,3,3,4,4,5,5,6,12,13,40,41,265,1,265), 
  plotting=1)

</code></pre>

<hr>
<h2 id='dgenpois'>
The Generalized Poisson Distribution
</h2><span id='topic+dgenpois'></span><span id='topic+pgenpois'></span><span id='topic+rgenpois'></span>

<h3>Description</h3>

<p>Density, distribution function and random generation function for the generalized Poisson distribution. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dgenpois(x, lambda1, lambda2)
pgenpois(q, lambda1, lambda2)
rgenpois(n, lambda1, lambda2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dgenpois_+3A_x">x</code></td>
<td>
<p>a vector object of (non-negative integer) quantiles.</p>
</td></tr>
<tr><td><code id="dgenpois_+3A_q">q</code></td>
<td>
<p>a numeric value.</p>
</td></tr>
<tr><td><code id="dgenpois_+3A_n">n</code></td>
<td>
<p>number of random values to return.</p>
</td></tr>
<tr><td><code id="dgenpois_+3A_lambda1">lambda1</code></td>
<td>
<p>a single numeric value for parameter <code>lambda1</code> with <code class="reqn">lambda1 &gt; 0</code>.</p>
</td></tr>
<tr><td><code id="dgenpois_+3A_lambda2">lambda2</code></td>
<td>
<p>a single numeric value for parameter <code>lambda2</code> with <code class="reqn">0 \le lamdba2 &lt; 1</code>.  When <code>lambda2=0</code>, the generalized Poisson distribution reduces to the Poisson distribution.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The generalized Poisson distribution has the density
</p>
<p style="text-align: center;"><code class="reqn"> p(x) = \lambda_1 (\lambda_1 + \lambda_2 \cdot x)^{x-1} \frac{ \exp(-\lambda_1-\lambda_2 \cdot x) )}{x!}</code>
</p>

<p>for <code class="reqn">x = 0,1,2,\ldots</code>,b
</p>
<p>with <code class="reqn">\mbox{E}(X)=\frac{\lambda_1}{1-\lambda_2}</code> and variance <code class="reqn">\mbox{var}(X)=\frac{\lambda_1}{(1-\lambda_2)^3}</code>.
</p>


<h3>Value</h3>

<p><code>dgenpois</code> gives the density, <code>pgenpois</code> gives the distribution function and <code>rgenpois</code> generates random deviates.
</p>


<h3>Author(s)</h3>

<p>Based on Joe and Zhu (2005). Implementation by  Vitali Witowski (2013).
</p>


<h3>References</h3>

<p>Joe, H., Zhu, R. (2005). Generalized poisson distribution: the property of mixture of poisson and comparison with negative binomial distribution. Biometrical Journal <b>47</b>(2):219&ndash;229. 
</p>


<h3>See Also</h3>

<p><a href="stats.html#topic+Distributions">Distributions</a> for other standard distributions, including <code><a href="stats.html#topic+dpois">dpois</a></code> for the Poisson distribution.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dgenpois(x = seq(0,20), lambda1 = 10, lambda2 = 0.5) 

pgenpois(q = 5, lambda1 = 10, lambda2 = 0.5) 

hist(rgenpois(n = 1000, lambda1 = 10, lambda2 = 0.5) )

</code></pre>

<hr>
<h2 id='direct_numerical_maximization'>
Estimation by Directly Maximizing the log-Likelihood 
</h2><span id='topic+direct_numerical_maximization'></span>

<h3>Description</h3>

<p>Estimates the parameters of a (stationary) discrete-time hidden Markov model by directly maximizing the log-likelihood of the model using the <a href="stats.html#topic+nlm">nlm</a>-function. See MacDonald &amp; Zucchini (2009, Paragraph 3) for further details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>direct_numerical_maximization(x, m, delta, gamma, 
     distribution_class, distribution_theta, 
     DNM_limit_accuracy = 0.001, DNM_max_iter = 50, 
     DNM_print = 2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="direct_numerical_maximization_+3A_x">x</code></td>
<td>

<p>a vector object containing the time-series of observations that are assumed to be realizations of the (hidden Markov state dependent) observation process of the model.</p>
</td></tr>
<tr><td><code id="direct_numerical_maximization_+3A_m">m</code></td>
<td>
<p>a (finite) number of states in the hidden Markov chain.</p>
</td></tr>
<tr><td><code id="direct_numerical_maximization_+3A_delta">delta</code></td>
<td>

<p>a vector object containing starting values for the marginal probability distribution of the <code>m</code> states of the Markov chain at the time point <code>t=1</code>. This implementation of the algorithm uses the stationary distribution as delta.</p>
</td></tr>
<tr><td><code id="direct_numerical_maximization_+3A_gamma">gamma</code></td>
<td>

<p>a matrix (<code>nrow=ncol=m</code>) containing starting values for the transition matrix of the hidden Markov chain.</p>
</td></tr>
<tr><td><code id="direct_numerical_maximization_+3A_distribution_class">distribution_class</code></td>
<td>

<p>a single character string object with the abbreviated name of the <code>m</code> observation distributions of the Markov dependent observation process. The following distributions are supported by this algorithm: Poisson (<code>pois</code>); generalized Poisson (<code>genpois</code>); normal (<code>norm</code>, discrete log-Likelihood not applicable by this algorithm).</p>
</td></tr>
<tr><td><code id="direct_numerical_maximization_+3A_distribution_theta">distribution_theta</code></td>
<td>

<p>a list object containing starting values for the parameters of the <code>m</code> observation distributions that are dependent on the hidden Markov state.</p>
</td></tr>
<tr><td><code id="direct_numerical_maximization_+3A_dnm_limit_accuracy">DNM_limit_accuracy</code></td>
<td>

<p>a single numerical value representing the convergence criterion of the direct numerical maximization algorithm using the <a href="stats.html#topic+nlm">nlm</a>-function. Default value is <code>0.001</code>.</p>
</td></tr>
<tr><td><code id="direct_numerical_maximization_+3A_dnm_max_iter">DNM_max_iter</code></td>
<td>

<p>a single numerical value representing the maximum number of iterations of the direct numerical maximization using the <a href="stats.html#topic+nlm">nlm</a>-function. Default value is <code>50</code>.</p>
</td></tr>
<tr><td><code id="direct_numerical_maximization_+3A_dnm_print">DNM_print</code></td>
<td>

<p>a single numerical value to determine the level of printing of the <code>nlm</code>-function.  See <code>nlm</code>-function for further informations. The value <code>0</code> suppresses, that no printing will be outputted. Default value is <code>2</code> for full printing.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>direct_numerical_maximization </code> returns a list containing the estimated parameters of the hidden Markov model and other components.
</p>
<table>
<tr><td><code>x</code></td>
<td>
<p>input time-series of observations.</p>
</td></tr>
<tr><td><code>m</code></td>
<td>
<p>input number of hidden states in the Markov chain.</p>
</td></tr>
<tr><td><code>logL</code></td>
<td>

<p>a numerical value representing the logarithmized likelihood calculated by the <code><a href="#topic+forward_backward_algorithm">forward_backward_algorithm</a></code>.</p>
</td></tr>
<tr><td><code>AIC</code></td>
<td>

<p>a numerical value representing Akaike's information criterion for the hidden Markov model with estimated parameters.</p>
</td></tr>
<tr><td><code>BIC</code></td>
<td>

<p>a numerical value representing the Bayesian information criterion for the hidden Markov model with estimated parameters.</p>
</td></tr>
<tr><td><code>delta</code></td>
<td>

<p>a vector object containing the estimates for the marginal probability distribution of the <code>m</code> states of the Markov chain at time-point point <code>t=1</code>.</p>
</td></tr>
<tr><td><code>gamma</code></td>
<td>

<p>a matrix containing the estimates for the transition matrix of the hidden Markov chain.</p>
</td></tr>
<tr><td><code>distribution_theta</code></td>
<td>

<p>a list object containing estimates for the parameters of the <code>m</code> observation distributions that are dependent on the hidden Markov state.</p>
</td></tr>
<tr><td><code>distribution_class</code></td>
<td>
<p>input distribution class.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>The basic algorithm of a Poisson-HMM is provided by MacDonald &amp; Zucchini (2009, Paragraph A.1). Extension and implementation by Vitali Witowski (2013).
</p>


<h3>References</h3>

<p>MacDonald, I. L., Zucchini, W. (2009) <em>Hidden Markov Models for Time Series: An Introduction Using R</em>, Boca Raton: Chapman &amp; Hall.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+HMM_based_method">HMM_based_method</a></code>, 
<code><a href="#topic+HMM_training">HMM_training</a></code>, 
<code><a href="#topic+Baum_Welch_algorithm">Baum_Welch_algorithm</a></code>, 
<code><a href="#topic+forward_backward_algorithm">forward_backward_algorithm</a></code>,
</p>
<p><code><a href="#topic+initial_parameter_training">initial_parameter_training</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
################################################################
### Fictitious observations ####################################
################################################################

x &lt;- c(1,16,19,34,22,6,3,5,6,3,4,1,4,3,5,7,9,8,11,11,
  14,16,13,11,11,10,12,19,23,25,24,23,20,21,22,22,18,7,
  5,3,4,3,2,3,4,5,4,2,1,3,4,5,4,5,3,5,6,4,3,6,4,8,9,12,
  9,14,17,15,25,23,25,35,29,36,34,36,29,41,42,39,40,43,
  37,36,20,20,21,22,23,26,27,28,25,28,24,21,25,21,20,21,
  11,18,19,20,21,13,19,18,20,7,18,8,15,17,16,13,10,4,9,
  7,8,10,9,11,9,11,10,12,12,5,13,4,6,6,13,8,9,10,13,13,
  11,10,5,3,3,4,9,6,8,3,5,3,2,2,1,3,5,11,2,3,5,6,9,8,5,
  2,5,3,4,6,4,8,15,12,16,20,18,23,18,19,24,23,24,21,26,
  36,38,37,39,45,42,41,37,38,38,35,37,35,31,32,30,20,39,
  40,33,32,35,34,36,34,32,33,27,28,25,22,17,18,16,10,9,
  5,12,7,8,8,9,19,21,24,20,23,19,17,18,17,22,11,12,3,9,
  10,4,5,13,3,5,6,3,5,4,2,5,1,2,4,4,3,2,1) 


### Assummptions (number of states, probability vector, 
### transition matrix, and distribution parameters)
    m &lt;-4
delta &lt;- c(0.25,0.25,0.25,0.25)
gamma &lt;- 0.7 * diag(m) + rep(0.3 / m)
distribution_class &lt;- "pois"
distribution_theta &lt;- list(lambda = c(4,9,17,25))

### Estimation of a HMM using the method of 
### direct numerical maximization

trained_HMM_with_m_hidden_states &lt;- 
		direct_numerical_maximization(x = x, 
      m = m, 
      delta = delta, 
      gamma = gamma, 
      distribution_class = distribution_class,
      DNM_max_iter=100,
      distribution_theta = distribution_theta)

print(trained_HMM_with_m_hidden_states)

</code></pre>

<hr>
<h2 id='forward_backward_algorithm'>
Calculating Forward and Backward Probabilities and Likelihood
</h2><span id='topic+forward_backward_algorithm'></span>

<h3>Description</h3>

<p>The function calculates the logarithmized forward and backward probabilities and the logarithmized likelihood for a discrete time hidden Markov model, as defined in MacDonald &amp; Zucchini (2009, Paragraph 3.1- Paragraph 3.3 and Paragraph 4.1).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>forward_backward_algorithm(x, delta, gamma, distribution_class, 
distribution_theta, discr_logL=FALSE, discr_logL_eps=0.5) 

### Default for normal distributions 
### (calculate non-discrete likelihood)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="forward_backward_algorithm_+3A_x">x</code></td>
<td>

<p>a vector object containing the time-series of observations that are assumed to be realizations of the   
(hidden Markov state dependent) observation process of the model.</p>
</td></tr>
<tr><td><code id="forward_backward_algorithm_+3A_delta">delta</code></td>
<td>

<p>a vector object containing values for the marginal probability distribution of the <code>m</code> states of the Markov chain at the time point <code>t=1</code>.</p>
</td></tr>
<tr><td><code id="forward_backward_algorithm_+3A_gamma">gamma</code></td>
<td>

<p>a matrix (<code>nrow=ncol=m</code>) containing values for the transition matrix of the hidden Markov chain.</p>
</td></tr>
<tr><td><code id="forward_backward_algorithm_+3A_distribution_class">distribution_class</code></td>
<td>

<p>a single character string object with the abbreviated name of the <code>m</code> observation distributions of the Markov dependent observation process. The following distributions are supported: Poisson (<code>pois</code>); generalized Poisson (<code>genpois</code>); normal (<code>norm</code>); geometric (<code>geom</code>).</p>
</td></tr>
<tr><td><code id="forward_backward_algorithm_+3A_distribution_theta">distribution_theta</code></td>
<td>

<p>a list object containing the parameter values for the <code>m</code> observation distributions of the observation process that are dependent on the hidden Markov state.</p>
</td></tr>
<tr><td><code id="forward_backward_algorithm_+3A_discr_logl">discr_logL</code></td>
<td>

<p>a logical object. It is <code>TRUE</code> if the discrete log-likelihood shall be calculated (for <code>distribution_class="norm"</code> ) instead of the general log-likelihood.  See MacDonald &amp; Zucchini (2009, Paragraph 1.2.3) for further details.  Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="forward_backward_algorithm_+3A_discr_logl_eps">discr_logL_eps</code></td>
<td>

<p>a single numerical value to approximately determine the discrete log-likelihood for a hidden Markov model based on normal distributions (for <code>"norm"</code>). The default value is <code>0.5</code>.  See MacDonald &amp; Zucchini (2009, Paragraph 1.2.3) for further details. </p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>forward_backward_algorithm</code> returns a list containing the logarithmized forward and backward probabilities and the logarithmized likelihood.
</p>
<table>
<tr><td><code>log_alpha</code></td>
<td>
<p>a (T,m)-matrix (when T indicates the length/size of the observation time-series and m the number of states of the HMM) containing the logarithmized forward probabilities.</p>
</td></tr>
<tr><td><code>log_beta</code></td>
<td>
<p>a (T,m)-matrix (when T indicates the length/size of the observation time-series and m the number of states of the HMM) containing the logarithmized backward probabilities.</p>
</td></tr>
<tr><td><code>logL</code></td>
<td>
<p>a single numerical value representing the logarithmized likelihood.</p>
</td></tr>
<tr><td><code>logL_calculation</code></td>
<td>
<p>a single character string object which indicates how <code>logL</code> has been
calculated (see Zucchini (2009) Paragrahp 3.1-3.4, 4.1, A.2.2, A.2.3 for further details).</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>The basic algorithm for a Poisson-HMM is provided by MacDonald &amp; Zucchini (2009, Paragraph A.2.2).  Extension and implementation by Vitali Witowski (2013).
</p>


<h3>References</h3>

<p>MacDonald, I. L., Zucchini, W. (2009) <em>Hidden Markov Models for Time Series: An Introduction Using R</em>, Boca Raton: Chapman &amp; Hall.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+HMM_based_method">HMM_based_method</a></code>,
<code><a href="#topic+HMM_training">HMM_training</a></code>,
<code><a href="#topic+Baum_Welch_algorithm">Baum_Welch_algorithm</a></code>,
<code><a href="#topic+direct_numerical_maximization">direct_numerical_maximization</a></code>,
<code><a href="#topic+initial_parameter_training">initial_parameter_training</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
################################################################
### Fictitious observations ####################################
################################################################

x &lt;- c(1,16,19,34,22,6,3,5,6,3,4,1,4,3,5,7,9,8,11,11,
  14,16,13,11,11,10,12,19,23,25,24,23,20,21,22,22,18,7,
  5,3,4,3,2,3,4,5,4,2,1,3,4,5,4,5,3,5,6,4,3,6,4,8,9,12,
  9,14,17,15,25,23,25,35,29,36,34,36,29,41,42,39,40,43,
  37,36,20,20,21,22,23,26,27,28,25,28,24,21,25,21,20,21,
  11,18,19,20,21,13,19,18,20,7,18,8,15,17,16,13,10,4,9,
  7,8,10,9,11,9,11,10,12,12,5,13,4,6,6,13,8,9,10,13,13,
  11,10,5,3,3,4,9,6,8,3,5,3,2,2,1,3,5,11,2,3,5,6,9,8,5,
  2,5,3,4,6,4,8,15,12,16,20,18,23,18,19,24,23,24,21,26,
  36,38,37,39,45,42,41,37,38,38,35,37,35,31,32,30,20,39,
  40,33,32,35,34,36,34,32,33,27,28,25,22,17,18,16,10,9,
  5,12,7,8,8,9,19,21,24,20,23,19,17,18,17,22,11,12,3,9,
  10,4,5,13,3,5,6,3,5,4,2,5,1,2,4,4,3,2,1) 


### Assummptions (number of states, probability vector, 
### transition matrix, and distribution parameters)

m &lt;-4

delta &lt;- c(0.25,0.25,0.25,0.25)

gamma &lt;- 0.7 * diag(m) + rep(0.3 / m)

distribution_class &lt;- "pois"

distribution_theta &lt;- list(lambda = c(4,9,17,25))


### Calculating logarithmized forward/backward probabilities 
### and logarithmized likelihood

forward_and_backward_probabilities_and_logL &lt;- 
    forward_backward_algorithm (x = x, 
        delta = delta, 
        gamma = gamma, 
        distribution_class = distribution_class, 
        distribution_theta = distribution_theta)

print(forward_and_backward_probabilities_and_logL)

</code></pre>

<hr>
<h2 id='HMM_based_method'>
Hidden Markov Method for Predicting Physical Activity Patterns
</h2><span id='topic+HMM_based_method'></span>

<h3>Description</h3>

<p>This function assigns a physical activity range to each observation of a time-series (such as a sequence of impulse counts recorded by an accelerometer) using hidden Markov models (HMM). The activity ranges are defined by thresholds called cut-off points.  Basically, this function combines <code><a href="#topic+HMM_training">HMM_training</a></code>, <code><a href="#topic+HMM_decoding">HMM_decoding</a></code> and <code><a href="#topic+cut_off_point_method">cut_off_point_method</a></code>. See Details for further information.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>HMM_based_method(x, cut_points, distribution_class, 
                 min_m = 2, max_m = 6, n = 100,
                 max_scaled_x = NA, names_activity_ranges = NA,  
                 discr_logL = FALSE, discr_logL_eps = 0.5, 
                 dynamical_selection = TRUE, training_method = "EM", 
                 Mstep_numerical = FALSE, BW_max_iter = 50, 
                 BW_limit_accuracy = 0.001, BW_print = TRUE,
                 DNM_max_iter = 50, DNM_limit_accuracy = 0.001, 
                 DNM_print = 2, decoding_method = 'global',
                 bout_lengths = NULL, plotting = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="HMM_based_method_+3A_x">x</code></td>
<td>

<p>a vector object of length <code>T</code> containing non-negative observations of a time-series, such as a sequence of accelerometer impulse counts, which are assumed to be realizations of the (hidden Markov state dependent) observation process of a HMM.</p>
</td></tr>
<tr><td><code id="HMM_based_method_+3A_cut_points">cut_points</code></td>
<td>

<p>a vector object containing cut-off points to separate activity ranges.  For instance, the vector <code>c(7,15,23)</code> separates the four activity ranges [0,7), [7,15), [15,23) and [23,Inf).</p>
</td></tr>
<tr><td><code id="HMM_based_method_+3A_distribution_class">distribution_class</code></td>
<td>

<p>a single character string object with the abbreviated name of the <code>m</code> observation distributions of the Markov dependent observation process. The following distributions are supported: Poisson (<code>pois</code>); generalized Poisson (<code>genpois</code>); normal (<code>norm</code>)).</p>
</td></tr>
<tr><td><code id="HMM_based_method_+3A_min_m">min_m</code></td>
<td>
<p>miminum number of hidden states in the hidden Markov chain. Default value is <code>2</code>.</p>
</td></tr>
<tr><td><code id="HMM_based_method_+3A_max_m">max_m</code></td>
<td>
<p>maximum number of hidden states in the hidden Markov chain. Default value is <code>6</code>.</p>
</td></tr>
<tr><td><code id="HMM_based_method_+3A_n">n</code></td>
<td>

<p>a single numerical value specifying the number of samples.  Default value is <code>100</code>.</p>
</td></tr>
<tr><td><code id="HMM_based_method_+3A_max_scaled_x">max_scaled_x</code></td>
<td>

<p>an optional numerical value,  to be used to scale the observations of the time-series <code>x</code> before the hidden Markov model is trained and decoded (see Details). Default value is <code>NA</code>.</p>
</td></tr>
<tr><td><code id="HMM_based_method_+3A_names_activity_ranges">names_activity_ranges</code></td>
<td>

<p>an optional character string vector to name the activity ranges induced by the cut-points. This vector must contain one element more than the vector <code>cut_points</code>.</p>
</td></tr>
<tr><td><code id="HMM_based_method_+3A_discr_logl">discr_logL</code></td>
<td>

<p>a logical object indicating whether the discrete log-likelihood should be used (for <code>"norm"</code>) for estimating the model specific parameters instead
of the general log-likelihood. See MacDonald &amp; Zucchini (2009, Paragraph 1.2.3) for further details.  Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="HMM_based_method_+3A_discr_logl_eps">discr_logL_eps</code></td>
<td>

<p>a single numerical value to approximate the discrete log-likelihood for a hidden Markov model based on nomal distributions (for <code>distribution_class="norm"</code>).  The default value is <code>0.5</code>.</p>
</td></tr>
<tr><td><code id="HMM_based_method_+3A_dynamical_selection">dynamical_selection</code></td>
<td>

<p>a logical value indicating whether the method of dynamical initial parameter selection should be applied (see <code><a href="#topic+HMM_training">HMM_training</a></code> for details).  Default is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="HMM_based_method_+3A_training_method">training_method</code></td>
<td>

<p>a logical value indicating whether the Baum-Welch algorithm (<code>"EM"</code>) or the method of direct numerical maximization (<code>"numerical"</code>) should be applied for estimating the model specific parameters of the HMM. See <code><a href="#topic+Baum_Welch_algorithm">Baum_Welch_algorithm</a></code> and <code><a href="#topic+direct_numerical_maximization">direct_numerical_maximization</a></code> for further details.  Default is 
</p>
<p><code>training_method="EM"</code>.</p>
</td></tr>
<tr><td><code id="HMM_based_method_+3A_mstep_numerical">Mstep_numerical</code></td>
<td>

<p>a logical object indicating whether the Maximization Step of the Baum-Welch algorithm shall be performed by numerical maximization.  Default is FALSE.</p>
</td></tr>
<tr><td><code id="HMM_based_method_+3A_bw_max_iter">BW_max_iter</code></td>
<td>

<p>a single numerical value representing the maximum number of iterations in the Baum-Welch algorithm. Default value is <code>50</code>.
</p>
</td></tr>
<tr><td><code id="HMM_based_method_+3A_bw_limit_accuracy">BW_limit_accuracy</code></td>
<td>

<p>a single numerical value representing the convergence criterion of the 
Baum-Welch algorithm. Default value is <code>0.001</code>.</p>
</td></tr>
<tr><td><code id="HMM_based_method_+3A_bw_print">BW_print</code></td>
<td>

<p>a logical object indicating whether the log-likelihood at each iteration-step shall be printed. Default is <code>TRUE</code>.
</p>
</td></tr>
<tr><td><code id="HMM_based_method_+3A_dnm_max_iter">DNM_max_iter</code></td>
<td>

<p>a single numerical value representing the maximum number of iterations
of the numerical maximization using the nlm-function (used to perform the M-step of the 
Baum-Welch-algorithm). Default value is <code>50</code>.</p>
</td></tr>
<tr><td><code id="HMM_based_method_+3A_dnm_limit_accuracy">DNM_limit_accuracy</code></td>
<td>

<p>a single numerical value representing the convergence criterion of the numerical
maximization algorithm using the <a href="stats.html#topic+nlm">nlm</a> function (used to perform the M-step of 
the Baum-Welch-algorithm). Default value is <code>0.001</code>.  
</p>
</td></tr>
<tr><td><code id="HMM_based_method_+3A_dnm_print">DNM_print</code></td>
<td>

<p>a single numerical value to determine the level of printing of the <code>nlm</code>-function.  See <code>nlm</code>-function for further informations. The value <code>0</code> suppresses, that no printing will be outputted. Default value is <code>2</code> for full printing.</p>
</td></tr>
<tr><td><code id="HMM_based_method_+3A_decoding_method">decoding_method</code></td>
<td>

<p>a string object to choose the applied decoding-method to decode the HMM given the time-series of observations <code>x</code>.  Possible values are <code>"global"</code> (for the use of the <code><a href="#topic+Viterbi_algorithm">Viterbi_algorithm</a></code>) and <code>"local"</code> (for the use of the <code><a href="#topic+local_decoding_algorithm">local_decoding_algorithm</a></code>). Default value is <code>"global"</code>.</p>
</td></tr>
<tr><td><code id="HMM_based_method_+3A_bout_lengths">bout_lengths</code></td>
<td>

<p>a vector object (with even number of elemets) to define the range of the bout intervals (see Details for the definition of bouts).  
For instance, 
</p>
<p><code>bout_lengths=c(1,1,2,2,3,10,11,20,1,20)</code> defines the five bout intervals 
[1,1] (1 count); [2,2] (2 counts); [3,10] (3-10 counts); [11,20] (11-20 counts); [1,20] 
(1-20 counts - overlapping with other bout intervalls is possible). Default value is <code>bout_lengths=NULL</code>. 
</p>
</td></tr>
<tr><td><code id="HMM_based_method_+3A_plotting">plotting</code></td>
<td>

<p>a numeric value between 0 and 5 (generates different outputs). NA suppresses graphical output. Default value is <code>0</code>.<br />
<code>0</code>: output 1-5 <br />
<code>1</code>: summary of all results <br />
<code>2</code>: time series of activity counts, classified into activity ranges  <br />
<code>3</code>: time series of bouts (and, if available, the sequence of the estimated hidden physical activity levels, extracted by decoding a trained HMM, in green colour) <br />
<code>4</code>: barplots of absolute and relative frequencies of time spent in different activity ranges  <br />
<code>5</code>: barplots of relative frequencies of the lenghts of bout intervals (overall and by activity ranges )</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function combines <code><a href="#topic+HMM_training">HMM_training</a></code>, <code><a href="#topic+HMM_decoding">HMM_decoding</a></code> and <code><a href="#topic+cut_off_point_method">cut_off_point_method</a></code> as follows: <br />
</p>
<p><b>Step 1:</b> <code><a href="#topic+HMM_training">HMM_training</a></code> trains the most likely HMM for a given time-series of accelerometer counts. <br />
<b>Step 2:</b> <code><a href="#topic+HMM_decoding">HMM_decoding</a></code> decodes the trained HMM (Step 1) into the most likely sequence of hidden states corresponding to the given time-series of observations (respectively the most likely sequence of physical activity levels corresponding to the time-series of accelerometer counts). <br />
<b>Step 3</b>. <code><a href="#topic+cut_off_point_method">cut_off_point_method</a></code> assigns an activity range to each accelerometer count by its hidden physical activity level (extracted in Step 2).
</p>


<h3>Value</h3>

<p><code>HMM_based_method</code> returns a list containing the output of the trained hidden Markov model, including the selected number of states <code>m</code> (i.e., number of physical activities) and plots key figures.
</p>
<table>
<tr><td><code>trained_HMM_with_selected_m</code></td>
<td>

<p>a list object containing the trained hidden Markov model including the selected number of states <code>m</code> (see <code><a href="#topic+HMM_training">HMM_training</a></code> for further details).</p>
</td></tr>
<tr><td><code>decoding</code></td>
<td>

<p>a list object containing the output of the decoding (see <code><a href="#topic+HMM_decoding">HMM_decoding</a></code> for further details)</p>
</td></tr></table>
<p>.
</p>
<table>
<tr><td><code>extendend_cut_off_point_method</code></td>
<td>

<p>a list object containing the output of the cut-off point method. 
The counts <code> x </code> are classified into the activity ranges by the corresponding sequence of hidden PA-levels, which were decoded by the HMM (see <code><a href="#topic+cut_off_point_method">cut_off_point_method</a></code> for further details).</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The parameter <code> max_scaled_x </code> can be applied to scale the values of the observations. This might prevent the alogrithm from numerical instabilities.  At the end, the results are internaly rescaled to the original scale.  For instance, a value of <code> max_scaled_x=200 </code> shrinks the count values of the complete time-series <code> x </code> to a maximum of 200. Training and decoding of the HMM is carried out using the scaled time-series. <br />
From our experience, especially time-series with observations values <code> &gt;1500</code>, or where <code>T &gt; 1000</code>, show numerical instabilities. We then advice to make use of <code> max_scaled_x </code>.
</p>
<p>The extention of the cut-off point method using a Poisson based HMM has been provided and evaluated successfully on simulated data firstly by Barbara Brachmann in her diploma thesis (see References).
</p>


<h3>Author(s)</h3>

<p>Vitali Witowski (2013). 
</p>


<h3>References</h3>

<p>Brachmann, B. (2011). Hidden-Markov-Modelle fuer Akzelerometerdaten. Diploma Thesis, University Bremen - Bremen Institute for Prevention Research and Social Medicine (BIPS).
</p>
<p>MacDonald, I. L., Zucchini, W. (2009) <em>Hidden Markov Models for Time Series: An Introduction Using R</em>, Boca Raton: Chapman &amp; Hall.
</p>
<p>Witowski, V., Foraita, R., Pitsiladis, Y., Pigeot, I., Wirsik, N. (2014) Using hidden Markov models to improve quantifying physical activity in accelerometer data - A simulation study. PLOS ONE. <b>9</b>(12), e114089. 
http://dx.doi.org/10.1371/journal.pone.0114089</p>


<h3>See Also</h3>

<p><code><a href="#topic+initial_parameter_training">initial_parameter_training</a></code>,
<code><a href="#topic+Baum_Welch_algorithm">Baum_Welch_algorithm</a></code>,
<code><a href="#topic+direct_numerical_maximization">direct_numerical_maximization</a></code>,
<code><a href="#topic+AIC_HMM">AIC_HMM</a></code>,
<code><a href="#topic+BIC_HMM">BIC_HMM</a></code>,
<code><a href="#topic+HMM_training">HMM_training</a></code>,
<code><a href="#topic+Viterbi_algorithm">Viterbi_algorithm</a></code>,
<code><a href="#topic+local_decoding_algorithm">local_decoding_algorithm</a></code>,
<code><a href="#topic+cut_off_point_method">cut_off_point_method</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
################################################################
### Fictitious activity counts #################################
################################################################

x &lt;- 100 * c(1,16,19,34,22,6,3,5,6,3,4,1,4,3,5,7,9,8,11,11,
  14,16,13,11,11,10,12,19,23,25,24,23,20,21,22,22,18,7,
  5,3,4,3,2,3,4,5,4,2,1,3,4,5,4,5,3,5,6,4,3,6,4,8,9,12,
  9,14,17,15,25,23,25,35,29,36,34,36,29,41,42,39,40,43,
  37,36,20,20,21,22,23,26,27,28,25,28,24,21,25,21,20,21,
  11,18,19,20,21,13,19,18,20,7,18,8,15,17,16,13,10,4,9,
  7,8,10,9,11,9,11,10,12,12,5,13,4,6,6,13,8,9,10,13,13,
  11,10,5,3,3,4,9,6,8,3,5,3,2,2,1,3,5,11,2,3,5,6,9,8,5,
  2,5,3,4,6,4,8,15,12,16,20,18,23,18,19,24,23,24,21,26,
  36,38,37,39,45,42,41,37,38,38,35,37,35,31,32,30,20,39,
  40,33,32,35,34,36,34,32,33,27,28,25,22,17,18,16,10,9,
  5,12,7,8,8,9,19,21,24,20,23,19,17,18,17,22,11,12,3,9,
  10,4,5,13,3,5,6,3,5,4,2,5,1,2,4,4,3,2,1)  

	   
### Fictitious cut-off points that produce four so-called 
### activity ranges "sedentary", "light", "moderate", 
### and "vigorous".
cut_points &lt;- 100 * c(7,15,23)
names_activity_ranges &lt;- c("SED","LIG","MOD","VIG")


### Plot fictitious activity counts
plot(x, main = "counts with high values", 
     xlab = "time/epoch", ylab = "counts")
abline(h = cut_points, col = "grey50", lty = "dashed")


################################################################
### Comparing the results of the traditional ################### 
### cut-off point method and the new HMM-based method ##########
################################################################

### Apply the traditional cut-off point method to assign 
### physical activity ranges to each observed count

solution_of_tradtionional_cut_off_point_method &lt;-
   cut_off_point_method(x = x, 
       hidden_PA_levels = NA, 
       cut_points = cut_points, 
       names_activity_ranges = names_activity_ranges, 
       bout_lengths = c(1,1,2,2,3,3,4,4,5,5,6,12, 
       13,40,41,265,1,265), 
	     plotting = 1)

### Apply the HMM-based method to assign physical activity 
### ranges to the hidden physical activity level of each count

solution_of_HMM_based_method &lt;- 
    HMM_based_method(x = x, 
      max_scaled_x = 50, 
      cut_points  =cut_points, 
    	min_m = 2, 
    	max_m = 6, 
    	names_activity_ranges = names_activity_ranges, 
      distribution_class = "pois", 
      training_method = "EM", 
      decoding_method = "global", 
      bout_lengths = c(1,1,2,2,3,3,4,4,5,5,6,12,
      13,40,41,265,1,265), 
      plotting = 1)

		
### Print details of the traditional cut-off point method 
### and the new HMM-based method
print(solution_of_tradtionional_cut_off_point_method)
print(solution_of_HMM_based_method)

</code></pre>

<hr>
<h2 id='HMM_decoding'>
Algorithm for Decoding Hidden Markov Models (local or global)
</h2><span id='topic+HMM_decoding'></span>

<h3>Description</h3>

<p>The function decodes a hidden Markov model into a most likely sequence of hidden states. Furthermore this function provides estimated observation values along the most likely sequence of hidden states. See Details for more information.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>		HMM_decoding(x, m, delta, gamma, distribution_class, 
		     distribution_theta, decoding_method = "global", 
		     discr_logL = FALSE, discr_logL_eps = 0.5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="HMM_decoding_+3A_x">x</code></td>
<td>

<p>a vector object containing the time-series of observations that are assumed to be realizations of the (hidden Markov state dependent) observation process of the model.</p>
</td></tr>
<tr><td><code id="HMM_decoding_+3A_m">m</code></td>
<td>

<p>a (finite) number of states in the hidden Markov chain.</p>
</td></tr>
<tr><td><code id="HMM_decoding_+3A_delta">delta</code></td>
<td>

<p>a vector object containing values for the marginal probability distribution of the <code>m</code> states of the Markov chain at the time point <code>t=1</code>.</p>
</td></tr>
<tr><td><code id="HMM_decoding_+3A_gamma">gamma</code></td>
<td>

<p>a matrix (<code>ncol=nrow=m</code>) containing values for the transition matrix of the hidden Markov chain.</p>
</td></tr>
<tr><td><code id="HMM_decoding_+3A_distribution_class">distribution_class</code></td>
<td>

<p>a single character string object with the abbreviated name of the <code>m</code> observation distributions of the Markov dependent observation process.  The following distributions are supported by this algorithm: Poisson (<code>pois</code>); generalized Poisson (<code>genpois</code>); normal (<code>norm</code>); geometric (<code>geom</code>).</p>
</td></tr>
<tr><td><code id="HMM_decoding_+3A_distribution_theta">distribution_theta</code></td>
<td>

<p>a list object containing the parameter values for the <code>m</code> observation distributions that are dependent on the hidden Markov state.</p>
</td></tr>
<tr><td><code id="HMM_decoding_+3A_decoding_method">decoding_method</code></td>
<td>

<p>a string object to choose the applied decoding-method to decode the HMM given the time-series of observations <code>x</code>.  Possible values are <code>"global"</code> (for the use of the <code>Viterbi_algorithm</code>) and <code>"local"</code> (for the use of the 
</p>
<p><code>local_decoding_algorithm</code>). Default value is <code>"global"</code>.</p>
</td></tr>
<tr><td><code id="HMM_decoding_+3A_discr_logl">discr_logL</code></td>
<td>

<p>a logical object. It is <code>TRUE</code> if the discrete log-likelihood shall be calculated (for <code>distribution_class="norm"</code> instead of the general log-likelihood).  Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="HMM_decoding_+3A_discr_logl_eps">discr_logL_eps</code></td>
<td>

<p>a single numerical value to approximately determine the discrete log-likelihood for a hidden Markov model based on nomal distributions (for <code>"norm"</code>).  The default value is <code>0.5</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>More precisely, the function works as follows:
</p>
<p><b>Step 1:</b>
In a first step, the algorithm decodes a HMM into the most likely sequence of hidden states, given a time-series of observations. The user can choose between a global and a local approch.<br />
If <code>decoding_method="global"</code> is applied, the function calls <code><a href="#topic+Viterbi_algorithm">Viterbi_algorithm</a></code> to determine the sequence of most likely hidden states for all time points simultaneously.<br /> 
If <code>decoding_method="local"</code> is applied, the function calls <code><a href="#topic+local_decoding_algorithm">local_decoding_algorithm</a></code> to determine the most likely hidden state for each time point seperately. 
</p>
<p><b>Step 2:</b>
In a second step, this function links each observation to the mean of the distribution, that corresponds to the decoded state at this point in time. 
</p>


<h3>Value</h3>

<p><code> HMM_decoding </code> returns a list containing the following two components:
</p>
<table>
<tr><td><code>decoding_method</code></td>
<td>

<p>a string object indicating the applied decoding method.
</p>
</td></tr>
<tr><td><code>decoding</code></td>
<td>

<p>a numerical vector containing the most likely sequence of hidden states as decoded by the <code><a href="#topic+Viterbi_algorithm">Viterbi_algorithm</a></code> (if <code>"global"</code> was applied) or by the 
</p>
<p><code><a href="#topic+local_decoding_algorithm">local_decoding_algorithm</a></code> (if <code>"local"</code> was applied).
</p>
</td></tr>
<tr><td><code>decoding_distr_means</code></td>
<td>

<p>a numerical vector of estimated oberservation values along the most likely seuquence of hidden states (see <code>decoding</code> and Step 2).</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Vitali Witowski (2013).
</p>


<h3>References</h3>

<p>MacDonald, I. L.,  Zucchini, W. (2009) <em>Hidden Markov Models for Time Series: An Introduction Using R</em>, Boca Raton: Chapman &amp; Hall.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+local_decoding_algorithm">local_decoding_algorithm</a></code>,		
<code><a href="#topic+Viterbi_algorithm">Viterbi_algorithm</a></code>	
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

################################################################
### i) HMM-training  ###########################################
################################################################

################################################################
### Fictitious observations ####################################
################################################################

x &lt;- c(1,16,19,34,22,6,3,5,6,3,4,1,4,3,5,7,9,8,11,11,
  14,16,13,11,11,10,12,19,23,25,24,23,20,21,22,22,18,7,
  5,3,4,3,2,3,4,5,4,2,1,3,4,5,4,5,3,5,6,4,3,6,4,8,9,12,
  9,14,17,15,25,23,25,35,29,36,34,36,29,41,42,39,40,43,
  37,36,20,20,21,22,23,26,27,28,25,28,24,21,25,21,20,21,
  11,18,19,20,21,13,19,18,20,7,18,8,15,17,16,13,10,4,9,
  7,8,10,9,11,9,11,10,12,12,5,13,4,6,6,13,8,9,10,13,13,
  11,10,5,3,3,4,9,6,8,3,5,3,2,2,1,3,5,11,2,3,5,6,9,8,5,
  2,5,3,4,6,4,8,15,12,16,20,18,23,18,19,24,23,24,21,26,
  36,38,37,39,45,42,41,37,38,38,35,37,35,31,32,30,20,39,
  40,33,32,35,34,36,34,32,33,27,28,25,22,17,18,16,10,9,
  5,12,7,8,8,9,19,21,24,20,23,19,17,18,17,22,11,12,3,9,
  10,4,5,13,3,5,6,3,5,4,2,5,1,2,4,4,3,2,1)  

### Train hidden Markov model for different number of 
### states m=2,...,6 and select the optimal model

m_trained_HMM &lt;- 
    HMM_training(x = x, 
       min_m = 2, 
       max_m = 6, 
       distribution_class="pois")$trained_HMM_with_selected_m
         
                 
################################################################
################################################################
### ii) Global decoding ########################################
################################################################
################################################################

### Decode the trained HMM using the Viterbi algorithm to get 
### the estimated sequence of hidden physical activity levels
global_decoding &lt;- 
    HMM_decoding(x = x, 
        m = m_trained_HMM$m, 
        delta = m_trained_HMM$delta, 
        gamma = m_trained_HMM$gamma, 
        distribution_class = m_trained_HMM$distribution_class, 
        distribution_theta = m_trained_HMM$distribution_theta,
        decoding_method = "global")
          
### Globally most likely sequence of hidden states, 
### i.e. in this case sequence of activity levels
global_decoding$decoding

par(mfrow = c(1,1))
plot(global_decoding$decoding)


### Plot the observed impulse counts and the most likely 
### sequence (green) according to the Viterbi algorithm that 
### generated these observations
plot(x)
lines(global_decoding$decoding_distr_means, col = "green")

################################################################
################################################################ 
### iii) Local decoding ########################################
################################################################ 
################################################################

### Decode the trained HMM using the local decoding algorithm 
### to get the estimated sequence of hidden physical activity 
### levels
local_decoding &lt;- 
   HMM_decoding(x = x, 
   m = m_trained_HMM$m, 
   delta = m_trained_HMM$delta, 
   gamma = m_trained_HMM$gamma, 
   distribution_class = m_trained_HMM$distribution_class, 
   distribution_theta = m_trained_HMM$distribution_theta,
   decoding_method = "local")
        

### Locally most likely sequence of hidden states, 
### i.e. in this case sequence of activity levels
local_decoding$decoding

par(mfrow=c(1,1))
plot(local_decoding$decoding)


### Plot the observed impulse counts and the most likely 
### sequence (green) according to the local decoding algorithm 
### that generated these observations
plot(x)
lines(local_decoding$decoding_distr_means, col = "red")

################################################################
################################################################
### iv) Comparison of global and local decoding ################
################################################################
################################################################

### Comparison of global decoding (green), local decoding (red) 
### and the connection to the closest mean (blue)
print(global_decoding$decoding)  
print(local_decoding$decoding)

### Plot comparison 
par(mfrow = c(2,2))
plot(global_decoding$decoding[seq(230,260)], col = "green", 
  ylab = "global decoding", main = "(zooming)")
  plot(x[seq(230,260)], ylab = "global decoding", 
  main = "(zooming x[seq(230,260)])")
lines(global_decoding$decoding_distr_means[seq(230,260)], 
  col = "green")
plot(local_decoding$decoding[seq(230,260)], col = "red", 
  ylab = "local decoding", main = "(zooming)")
plot(x[seq(230,260)], ylab = "local decoding", 
  main = "(zooming x[seq(230,260)])")
lines(local_decoding$decoding_distr_means[seq(230,260)], 
  col = "red")
par(mfrow = c(1,1))

</code></pre>

<hr>
<h2 id='HMM_simulation'>
Generating Realizations of a Hidden Markov Model
</h2><span id='topic+HMM_simulation'></span>

<h3>Description</h3>

<p>This function generates a sequence of hidden states of a Markov chain and a corresponding parallel sequence of observations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>HMM_simulation(size, m, delta = rep(1 / m, times = m), 
               gamma = 0.8 * diag(m) + rep(0.2 / m, times = m), 
               distribution_class, distribution_theta, 
               obs_range = c(NA, NA), obs_round = FALSE, 
               obs_non_neg = FALSE, plotting = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="HMM_simulation_+3A_size">size</code></td>
<td>
<p>length of the time-series of hidden states and observations (also <code>T</code>).</p>
</td></tr>
<tr><td><code id="HMM_simulation_+3A_m">m</code></td>
<td>
<p>a (finite) number of states in the hidden Markov chain.</p>
</td></tr>
<tr><td><code id="HMM_simulation_+3A_delta">delta</code></td>
<td>

<p>a vector object containing starting values for the marginal probability distribution of the <code>m</code> states of the Markov chain at the time point <code>t=1</code>.  Default is <code>delta=rep(1/m,times=m)</code>. 
</p>
</td></tr>
<tr><td><code id="HMM_simulation_+3A_gamma">gamma</code></td>
<td>

<p>a matrix (<code>nrow=ncol=m</code>) containing starting values for the transition matrix of the hidden Markov chain.  
</p>
<p>Default is <code>gamma=0.8 * diag(m)</code> <code> + rep(0.2 / m, times = m)</code>.  
</p>
</td></tr>
<tr><td><code id="HMM_simulation_+3A_distribution_class">distribution_class</code></td>
<td>

<p>a single character string object with the abbreviated name of the <code>m</code> observation distributions of the Markov dependent observation process.  The following distributions are supported by this algorithm: Poisson (<code>pois</code>); generalized Poisson (<code>genpois</code>); normal (<code>norm</code>, discrete log-Likelihood not applicable by this algorithm); geometric (<code>geom</code>).
</p>
</td></tr>
<tr><td><code id="HMM_simulation_+3A_distribution_theta">distribution_theta</code></td>
<td>

<p>a list object containing starting values for the parameters of the <code>m</code> observation distributions that are dependent on the hidden Markov state.
</p>
</td></tr>
<tr><td><code id="HMM_simulation_+3A_obs_range">obs_range</code></td>
<td>

<p>a vector object specifying the range for the observations to be generated.  For instance, the vector <code>c(0,1500)</code> allows only observations between 0 and 1500 to be generated by the HMM.  Default value is <code>FALSE</code>.  See Notes for further details.
</p>
</td></tr>
<tr><td><code id="HMM_simulation_+3A_obs_round">obs_round</code></td>
<td>

<p>a logical object. <code>TRUE</code> if all generated observations are natural.  Default value is <code>FALSE</code>.  See Notes for further details.
</p>
</td></tr>
<tr><td><code id="HMM_simulation_+3A_obs_non_neg">obs_non_neg</code></td>
<td>

<p>a logical object. <code>TRUE</code>, if non negative observations are generated.  Default value is <code>FALSE</code>.  See Notes for further details.
</p>
</td></tr>
<tr><td><code id="HMM_simulation_+3A_plotting">plotting</code></td>
<td>
<p>a numeric value between 0 and 5 (generates different outputs). NA suppresses graphical output. Default value is <code>0</code>.<br />
<code>0</code>: output 1-5 <br />
<code>1</code>: summary of all results <br />
<code>2</code>: generated time series of states of the hidden Markov chain  <br />
<code>3</code>: means (of  the observation distributions, which depend on the states of the Markov chain) along the time series of states of the hidden Markov chain <br />
<code>4</code>: observations along the time series of states of the hidden Markov chain <br />
<code>5</code>: simulated observations</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function <code> HMM_simulation </code> returns a list containing the following components:
</p>
<table>
<tr><td><code>size</code></td>
<td>
<p>length of the generated time-series of hidden states and observations.</p>
</td></tr>
<tr><td><code>m</code></td>
<td>
<p>input number of states in the hidden Markov chain.</p>
</td></tr>
<tr><td><code>delta</code></td>
<td>
<p>a vector object containing the chosen values for the marginal probability distribution of the <code>m</code> states of the Markov chain at the time point <code>t=1</code>.</p>
</td></tr>
<tr><td><code>gamma</code></td>
<td>
<p>a matrix containing the chosen values for the transition matrix of the hidden Markov chain.</p>
</td></tr>
<tr><td><code>distribution_class</code></td>
<td>
<p>a single character string object with the abbreviated name of the chosen observation distributions of the Markov dependent observation process.</p>
</td></tr>
<tr><td><code>distribution_theta</code></td>
<td>
<p>a list object containing the chosen values for the parameters of the <code>m</code> observation distributions that are dependent on the hidden Markov state.</p>
</td></tr>
<tr><td><code>markov_chain</code></td>
<td>
<p>a vector object containing the generated sequence of states of the hidden Markov chain of the HMM.</p>
</td></tr>
<tr><td><code>means_along_markov_chain</code></td>
<td>
<p>a vector object containing the sequence of means (of the state dependent distributions) corresponding to the generated sequence of states.</p>
</td></tr>
<tr><td><code>observations</code></td>
<td>
<p>a vector object containing the generated sequence of (state dependent) observations of the HMM.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Some notes regarding the default values:
</p>
<p><code>gamma</code>: <br />
The default setting assigns higher probabilities for remaining in a state than changing into another.  
</p>
<p><code>obs_range</code>:<br />
Has to be used with caution. since it manipulates the results of the HMM. If a value for an observation at time <code>t</code> is generated outside the defined range, it will be regenerated as long as it falls into <code>obs_range</code>. It is possible just to define one boundary, e.g. <code>obs_range=c(NA,2000)</code> for observations lower than 2000, or <code>obs_range=c(100,NA)</code> for observations higher than 100. 
</p>
<p><code>obs_round </code>:<br />
Has to be used with caution!  Rounds each generated observation and hence manipulates the results of the HMM (important for the normal distribution based HMM). 
</p>
<p><code>obs_ non_neg</code>:<br />
Has to be used with caution, since it manipulates the results of the HMM. If a negative value for an observation at a time <code>t</code> is generated, it will be re-generated as long as it is non-negative (important for the normal distribution based HMM).
</p>


<h3>Author(s)</h3>

<p>Vitali Witowski (2013).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+AIC_HMM">AIC_HMM</a></code>, 
<code><a href="#topic+BIC_HMM">BIC_HMM</a></code>, 
<code><a href="#topic+HMM_training">HMM_training</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
################################################################
### i.) Generating a HMM with Poisson-distributed data #########
################################################################

Pois_HMM_data &lt;- 
   HMM_simulation(size = 300, 
      m = 5, 
      distribution_class = "pois", 
      distribution_theta = list( lambda=c(10,15,25,35,55)))

print(Pois_HMM_data)

################################################################
### ii.) Generating 6 physical activities with normally ########
###      distributed accelerometer counts using a HMM. #########
################################################################

## Define number of time points (1440 counts equal 6 hours of 
## activity counts assuming an epoch length of 15 seconds).
size &lt;- 1440

## Define 6 possible physical activity ranges
m &lt;- 6

## Start with the lowest possible state 
## (in this case with the lowest physical activity)
delta &lt;- c(1, rep(0, times = (m - 1)))

## Define transition matrix to generate according to a 
## specific activity 
gamma &lt;- 0.935 * diag(m) + rep(0.065 / m, times = m)

## Define parameters 
## (here: means and standard deviations for m=6 normal 
##  distributions that define the distribution in 
##  a phsycial acitivity level)
distribution_theta &lt;- list(mean = c(0,100,400,600,900,1200), 
   sd = rep(x = 200, times = 6))

### Assume for each count an upper boundary of 2000
obs_range &lt;-c(NA,2000)

### Accelerometer counts shall not be negative
obs_non_neg &lt;-TRUE

### Start simulation

accelerometer_data &lt;- 
   HMM_simulation(size = size, 
     m = m, 
     delta = delta, 
     gamma = gamma, 
     distribution_class = "norm", 
     distribution_theta = distribution_theta, 
     obs_range = obs_range, 
     obs_non_neg= obs_non_neg, plotting=0)

print(accelerometer_data)
</code></pre>

<hr>
<h2 id='HMM_training'>
Training of Hidden Markov Models
</h2><span id='topic+HMM_training'></span>

<h3>Description</h3>

<p>Function to estimate the model specific parameters (<code>delta, gamma, distribution_theta</code>) for a hidden Markov model, given a time-series and a user-defined distribution class. Can also be used for model selection (selecting the optimal number of states <code>m</code>). See Details for more information. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>HMM_training (x, distribution_class, min_m = 2, max_m = 6, 
              n = 100, training_method = "EM", discr_logL = FALSE, 
              discr_logL_eps = 0.5, Mstep_numerical = FALSE, 
              dynamical_selection = TRUE, BW_max_iter = 50, 
              BW_limit_accuracy = 0.001, BW_print = TRUE,
              DNM_max_iter = 50, DNM_limit_accuracy = 0.001, 
              DNM_print = 2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="HMM_training_+3A_x">x</code></td>
<td>

<p>a vector object of length <code>T</code> containing observations of a time-series <code>x</code>, which are assumed to be realizations of the (hidden Markov state dependent) observation process of the HMM.</p>
</td></tr>
<tr><td><code id="HMM_training_+3A_distribution_class">distribution_class</code></td>
<td>

<p>a single character string object with the abbreviated name of the $m$ observation distributions of the Markov dependent observation process.  The following distributions are supported:  Poisson (<code>pois</code>); generalized Poisson (<code>genpois</code>, only available for <code>training_method="numerical"</code>); normal (<code>norm</code>)).</p>
</td></tr>
<tr><td><code id="HMM_training_+3A_min_m">min_m</code></td>
<td>
<p>minimum number of hidden states in the hidden Markov chain.  Default value is <code>2</code>.</p>
</td></tr>
<tr><td><code id="HMM_training_+3A_max_m">max_m</code></td>
<td>
<p>maximum number of hidden states in the hidden Markov chain.  Default value is <code>6</code>.</p>
</td></tr>
<tr><td><code id="HMM_training_+3A_n">n</code></td>
<td>

<p>a single numerical value specifying the number of samples to find the best starting values for the training algorithm.  Default value is <code>n=100</code>.</p>
</td></tr>
<tr><td><code id="HMM_training_+3A_training_method">training_method</code></td>
<td>

<p>a logical value indicating whether the Baum-Welch algorithm (<code>"EM"</code>) or the method of direct numerical maximization (<code>"numerical"</code>) should be applied for estimating the model specific parameters. See <code><a href="#topic+Baum_Welch_algorithm">Baum_Welch_algorithm</a></code> and <code><a href="#topic+direct_numerical_maximization">direct_numerical_maximization</a></code> for further details.  
</p>
<p>Default is <code>training_method="EM"</code>.</p>
</td></tr>
<tr><td><code id="HMM_training_+3A_discr_logl">discr_logL</code></td>
<td>

<p>a logical object.  Default is <code>FALSE</code> for the general log-likelihood, <code>TRUE</code> for the discrete log-likelihood (for <code>distribution_class="norm"</code>).
</p>
</td></tr>
<tr><td><code id="HMM_training_+3A_discr_logl_eps">discr_logL_eps</code></td>
<td>

<p>a single numerical value, used to approximate the discrete log-likelihood for a hidden Markov model based on nomal distributions (for <code>"norm"</code>).  The default value is <code>0.5</code>.
</p>
</td></tr>
<tr><td><code id="HMM_training_+3A_mstep_numerical">Mstep_numerical</code></td>
<td>

<p>a logical object indicating whether the Maximization Step of the Baum-Welch algorithm should be performed by numerical maximization. Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="HMM_training_+3A_dynamical_selection">dynamical_selection</code></td>
<td>

<p>a logical value indicating whether the method of dynamical initial parameter selection should be applied (see Details).  Default is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="HMM_training_+3A_bw_max_iter">BW_max_iter</code></td>
<td>

<p>a single numerical value representing the maximum number of iterations in the Baum-Welch algorithm.  Default value is <code>50</code>.</p>
</td></tr>
<tr><td><code id="HMM_training_+3A_bw_limit_accuracy">BW_limit_accuracy</code></td>
<td>

<p>a single numerical value representing the convergence criterion of the Baum-Welch algorithm. Default value is is <code>0.001</code>.</p>
</td></tr>
<tr><td><code id="HMM_training_+3A_bw_print">BW_print</code></td>
<td>

<p>a logical object indicating whether the log-likelihood at each iteration-step shall be printed. Default is <code>TRUE</code>.
</p>
</td></tr>
<tr><td><code id="HMM_training_+3A_dnm_max_iter">DNM_max_iter</code></td>
<td>

<p>a single numerical value representing the maximum number of iterations of the numerical maximization using the nlm-function (used to perform the Maximization Step of the Baum-Welch-algorithm).  Default value is <code>50</code>.</p>
</td></tr>
<tr><td><code id="HMM_training_+3A_dnm_limit_accuracy">DNM_limit_accuracy</code></td>
<td>

<p>a single numerical value representing the convergence criterion of the numerical maximization algorithm using the <a href="stats.html#topic+nlm">nlm</a> function (used to perform the Maximization Step of the Baum-Welch- algorithm).  Default value is <code>0.001</code>.</p>
</td></tr> 
<tr><td><code id="HMM_training_+3A_dnm_print">DNM_print</code></td>
<td>

<p>a single numerical value to determine the level of printing of the <code>nlm</code>-function.  See <code>nlm</code>-function for further informations. The value <code>0</code> suppresses, that no printing will be outputted. Default value is <code>2</code> for full printing.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>More precisely, the function works as follows:
</p>
<p><b>Step 1:</b>
In a first step, the algorithm estimates the model specific parameters for different values of <code>m</code> (indeed for <code>min_m,...,max_m</code>) using either the function <code><a href="#topic+Baum_Welch_algorithm">Baum_Welch_algorithm</a></code> or 
</p>
<p><code><a href="#topic+direct_numerical_maximization">direct_numerical_maximization</a></code>. Therefore, the function first searches for plausible starting values by using the function <code><a href="#topic+initial_parameter_training">initial_parameter_training</a></code>. 
</p>
<p><b>Step 2:</b>
In a second step, this function evaluates the AIC and BIC values for each HMM (built in Step 1) using the functions <code><a href="#topic+AIC_HMM">AIC_HMM</a></code> and <code><a href="#topic+BIC_HMM">BIC_HMM</a></code>. Then, based on that values, this function decides for the most plausible number of states <code>m</code> (respectively for the most appropriate HMM for the given time-series of observations). In case when AIC and BIC claim for a different <code>m</code>, the algorithm decides for the smaller value for <code>m</code> (with the background to have a more simplistic model). 
If the user is intereseted in having a HMM with a fixed number for <code>m</code>, <code>min_m</code> and <code>max_m</code> have to be chosen equally (for instance <code>min_m=4=max_m</code> for a HMM with <code>m=4</code> hidden states).
</p>
<p>To speed up the parameter estimation for each <code class="reqn">m &gt; m_min</code>, the user can choose the method of dynamical initial parameter selection.
</p>
<p>If the method of dynamical intial parameter selection <b>is not applied</b>, the function 
</p>
<p><code><a href="#topic+initial_parameter_training">initial_parameter_training</a></code> will be called to find plausible starting values for each state 
<code class="reqn"> m \in \{min_m, \ldots, max_m\}</code>. <br />
</p>
<p>If the method of dynamical intial parameter selection <b>is applied</b>, then starting parameter values using the function <code><a href="#topic+initial_parameter_training">initial_parameter_training</a></code> will be found only for the first HMM (respectively the HMM with <code>m_min</code> states). The further starting parameter values for the next HMM (with <code>m+1</code> states and so on) are retained from the trained parameter values of the last HMM (with <code>m</code> states and so on). 
</p>


<h3>Value</h3>

<p><code>HMM_training</code> returns a list containing the following components:
</p>
<table>
<tr><td><code>trained_HMM_with_selected_m</code></td>
<td>

<p>a list object containg the key data of the optimal trained HMM (HMM with selected <code>m</code>) &ndash; summarized output of the <code><a href="#topic+Baum_Welch_algorithm">Baum_Welch_algorithm</a></code> or 
</p>
<p><code><a href="#topic+direct_numerical_maximization">direct_numerical_maximization</a></code> algorithm, respectively.</p>
</td></tr>
<tr><td><code>list_of_all_initial_parameters</code></td>
<td>

<p>a list object containing the plausible starting values for all HMMs (one for each state <code>m</code>).</p>
</td></tr>
<tr><td><code>list_of_all_trained_HMMs</code></td>
<td>

<p>a list object containing all trained m-state-HMMs. See <code><a href="#topic+Baum_Welch_algorithm">Baum_Welch_algorithm</a></code> or <code><a href="#topic+direct_numerical_maximization">direct_numerical_maximization</a></code> for <code>training_method="EM"</code> or 
</p>
<p><code>training_method="numerical"</code>, respectively.</p>
</td></tr>
<tr><td><code>list_of_all_logLs_for_each_HMM_with_m_states</code></td>
<td>

<p>a list object containing all logarithmized Likelihoods of each trained HMM.</p>
</td></tr>
<tr><td><code>list_of_all_AICs_for_each_HMM_with_m_states</code></td>
<td>

<p>a list object containing the AIC values of all trained HMMs.</p>
</td></tr>
<tr><td><code>list_of_all_BICs_for_each_HMM_with_m_states</code></td>
<td>

<p>a list object containing the BIC values of all trained HMMs.</p>
</td></tr>
<tr><td><code>model_selection_over_AIC</code></td>
<td>
<p>is logical.  <code>TRUE</code>, if model selection was based on AIC and <code>FALSE</code>, if model selection was based on BIC.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Vitali Witowski (2013).
</p>


<h3>References</h3>

<p>MacDonald, I. L.,  Zucchini, W. (2009) <em>Hidden Markov Models for Time Series: An Introduction Using R</em>, Boca Raton: Chapman &amp; Hall.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+initial_parameter_training">initial_parameter_training</a></code>,
<code><a href="#topic+Baum_Welch_algorithm">Baum_Welch_algorithm</a></code>,
<code><a href="#topic+direct_numerical_maximization">direct_numerical_maximization</a></code>,
<code><a href="#topic+AIC_HMM">AIC_HMM</a></code>,
<code><a href="#topic+BIC_HMM">BIC_HMM</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
################################################################
### Fictitious observations ####################################
################################################################

x &lt;- c(1,16,19,34,22,6,3,5,6,3,4,1,4,3,5,7,9,8,11,11,
  14,16,13,11,11,10,12,19,23,25,24,23,20,21,22,22,18,7,
  5,3,4,3,2,3,4,5,4,2,1,3,4,5,4,5,3,5,6,4,3,6,4,8,9,12,
  9,14,17,15,25,23,25,35,29,36,34,36,29,41,42,39,40,43,
  37,36,20,20,21,22,23,26,27,28,25,28,24,21,25,21,20,21,
  11,18,19,20,21,13,19,18,20,7,18,8,15,17,16,13,10,4,9,
  7,8,10,9,11,9,11,10,12,12,5,13,4,6,6,13,8,9,10,13,13,
  11,10,5,3,3,4,9,6,8,3,5,3,2,2,1,3,5,11,2,3,5,6,9,8,5,
  2,5,3,4,6,4,8,15,12,16,20,18,23,18,19,24,23,24,21,26,
  36,38,37,39,45,42,41,37,38,38,35,37,35,31,32,30,20,39,
  40,33,32,35,34,36,34,32,33,27,28,25,22,17,18,16,10,9,
  5,12,7,8,8,9,19,21,24,20,23,19,17,18,17,22,11,12,3,9,
  10,4,5,13,3,5,6,3,5,4,2,5,1,2,4,4,3,2,1) 


## Train a poisson hidden Markov model using the Baum-Welch 
## algorithm for different number of states m=2,...,6

trained_HMMs &lt;- 
    HMM_training(x = x, 
      distribution_class = "pois", 
      min_m = 2, 
      max_m = 6, 
      training_method = "EM")


## Various output values for the HMM
names(trained_HMMs)

## Print details of the most plausible HMM for the given 
## time-series of observations
print(trained_HMMs$trained_HMM_with_selected_m)

## Print details of all trained HMMs (by this function) 
## for the given time-series of observations
print(trained_HMMs$list_of_all_trained_HMMs)

## Print the BIC-values of all trained HMMs for the given 
## time-series of observations  
print(trained_HMMs$list_of_all_BICs_for_each_HMM_with_m_states)

## Print the logL-values of all trained HMMs for the 
## given time-series of observations  
print(trained_HMMs$list_of_all_logLs_for_each_HMM_with_m_states)

</code></pre>

<hr>
<h2 id='HMMpa-package'>
Analysing Accelerometer Data Using Hidden Markov Models
</h2><span id='topic+HMMpa-package'></span><span id='topic+HMMpa'></span>

<h3>Description</h3>

<p>This package provides functions for analyzing accelerometer outpout data (known as a time-series of (impulse)-counts) to quantify length and intensity of physical activity.
</p>
<p>Usually, so called <em>activity ranges</em> are used to classify an activity as &ldquo;sedentary&rdquo;, &ldquo;moderate&rdquo; and so on. <em>Activity ranges</em> are separated by certain thresholds (<em>cut-off points</em>). The choice of these cut-off points depends on different components like the subjects' age or the type of accelerometer device. 
</p>
<p>Cut-off point values and defined activity ranges are important input values of the following analyzing tools provided by this package:
</p>
<p>1. <strong>Cut-off point method</strong> (<em>assigns an activity range to a count given its total magintude</em>).
This traditional approach assigns an activity range to each count of the time-series independently of each other given its total magnitude.
</p>
<p>2. <strong>HMM-based method</strong> (<em>assigns an activity range to a count given its underlying PA-level</em>).
This approach uses a stochastic model (the hidden Markov model or HMM) to identify the (Markov dependent) time-series of physical activity states underlying the given time-series of accelerometer counts. In contrast to the cut-off point method, this approach assigns activity ranges to the estimated PA-levels corresponding to the hidden activity states, and not directly to the accelerometer counts.
</p>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> HMMpa</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 1.0.1</td>
</tr>
<tr>
 <td style="text-align: left;">
Date: </td><td style="text-align: left;"> 2018-04-20</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL-3 </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p>The new procedure for analyzing accelerometer data can be roughly described as follows:<br />
First, a hidden Markov model (HMM) is trained to estimate the number <code>m</code> of hidden physical activity states and the model specific parameters (<code>delta, gamma, distribution_theta</code>).  Then, a user-sepcified decoding algorithm decodes the trainded HMM to classify each accelerometer count into the <code>m</code> hidden physical activity states.  Finally, the estimated distribution mean values (PA-levels) corresponding to the hidden physical activity states are extracted and the accelerometer counts are assigned by the total magnitudes of their corresponding PA-levels to given physical activity ranges (e.g. &quot;sedentary&quot;, &quot;light&quot;, &quot;moderate&quot; and &quot;vigorous&quot;) by the traditional cut-off point method.
</p>


<h3>Note</h3>

<p>We thank Moritz Hanke for his help in realizing this package.
</p>


<h3>Author(s)</h3>

<p>Vitali Witowski, <br />
Ronja Foraita,
Leibniz Institute for Prevention Research and Epidemiology (BIPS) <br />
</p>
<p>Maintainer: Ronja Foraita &lt;foraita@leibniz-bips.de&gt;
</p>


<h3>References</h3>

<p>Baum, L., Petrie, T., Soules, G., Weiss, N. (1970). A maximization technique occurring in the statistical analysis of probabilistic functions of markov chains. The annals of mathematical statistics, vol. <b>41</b>(1), 164&ndash;171.
</p>
<p>Brachmann, B. (2011). Hidden-Markov-Modelle fuer Akzelerometerdaten. Diploma Thesis, University Bremen - Bremen Institute for Prevention Research and Social Medicine (BIPS).
</p>
<p>Dempster, A., Laird, N., Rubin, D. (1977). Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society. Series B (Methodological), vol. <b>39</b>(1), 1&ndash;38.
</p>
<p>Forney, G.D. (1973). The Viterbi algorithm. Proceeding of the IEE, vol. <b>61</b>(3), 268&ndash;278.
</p>
<p>Joe, H., Zhu, R. (2005). Generalized poisson distribution: the property of mixture of poisson and comparison with negative binomial distribution. Biometrical Journal, vol. <b>47</b>(2), 219&ndash;229. 
</p>
<p>MacDonald, I. L., Zucchini, W. (2009) <em>Hidden Markov Models for Time Series: An Introduction Using R</em>, Boca Raton: Chapman &amp; Hall.
</p>
<p>Viterbi, A.J. (1967). Error Bounds for concolutional codes and an asymptotically optimal decoding algorithm. Information Theory, IEEE Transactions on, vol. <b>13</b>(2), 260&ndash;269.
</p>
<p>Witowski, V., Foraita, R., Pitsiladis, Y., Pigeot, I., Wirsik, N. (2014) Using hidden Markov models to improve quantifying physical activity in accelerometer data - A simulation study. PLOS ONE. <b>9</b>(12), e114089. 
http://dx.doi.org/10.1371/journal.pone.0114089
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
################################################################
###     Example 1  (traditional approach)    ################### 
###     Solution of the cut-off point method ################### 
################################################################

################################################################
### Fictitious activity counts #################################
################################################################

x &lt;- c(1,16,19,34,22,6,3,5,6,3,4,1,4,3,5,7,9,8,11,11,
  14,16,13,11,11,10,12,19,23,25,24,23,20,21,22,22,18,7,
  5,3,4,3,2,3,4,5,4,2,1,3,4,5,4,5,3,5,6,4,3,6,4,8,9,12,
  9,14,17,15,25,23,25,35,29,36,34,36,29,41,42,39,40,43,
  37,36,20,20,21,22,23,26,27,28,25,28,24,21,25,21,20,21,
  11,18,19,20,21,13,19,18,20,7,18,8,15,17,16,13,10,4,9,
  7,8,10,9,11,9,11,10,12,12,5,13,4,6,6,13,8,9,10,13,13,
  11,10,5,3,3,4,9,6,8,3,5,3,2,2,1,3,5,11,2,3,5,6,9,8,5,
  2,5,3,4,6,4,8,15,12,16,20,18,23,18,19,24,23,24,21,26,
  36,38,37,39,45,42,41,37,38,38,35,37,35,31,32,30,20,39,
  40,33,32,35,34,36,34,32,33,27,28,25,22,17,18,16,10,9,
  5,12,7,8,8,9,19,21,24,20,23,19,17,18,17,22,11,12,3,9,
  10,4,5,13,3,5,6,3,5,4,2,5,1,2,4,4,3,2,1)  
    
traditional_cut_off_point_method &lt;- cut_off_point_method(x=x, 
 	cut_points = c(5,15,23), 
 	names_activity_ranges = c("SED","LIG","MOD","VIG"), 
 	bout_lengths = c(1,1,2,4,5,10,11,20,21,60,61,260),
 	plotting = 1)



################################################################ 
################################################################ 
###      Examples 2,3 and 4  (new approach)             ########
###      Solution of the HMM based cut-off point method ######## 
################################################################
###      Demonstrated both in three steps (Example 2)     ###### 
###      and condensed in one function (Examples 3 and 4) ######
################################################################

################################################################
### Example 2) Manually in three steps    ######################
################################################################

################################################################
### Fictitious activity counts #################################
################################################################
                                                                
x &lt;- c(1,16,19,34,22,6,3,5,6,3,4,1,4,3,5,7,9,8,11,11,           
  14,16,13,11,11,10,12,19,23,25,24,23,20,21,22,22,18,7,         
  5,3,4,3,2,3,4,5,4,2,1,3,4,5,4,5,3,5,6,4,3,6,4,8,9,12,         
  9,14,17,15,25,23,25,35,29,36,34,36,29,41,42,39,40,43,         
  37,36,20,20,21,22,23,26,27,28,25,28,24,21,25,21,20,21,        
  11,18,19,20,21,13,19,18,20,7,18,8,15,17,16,13,10,4,9,         
  7,8,10,9,11,9,11,10,12,12,5,13,4,6,6,13,8,9,10,13,13,         
  11,10,5,3,3,4,9,6,8,3,5,3,2,2,1,3,5,11,2,3,5,6,9,8,5,         
  2,5,3,4,6,4,8,15,12,16,20,18,23,18,19,24,23,24,21,26,         
  36,38,37,39,45,42,41,37,38,38,35,37,35,31,32,30,20,39,        
  40,33,32,35,34,36,34,32,33,27,28,25,22,17,18,16,10,9,         
  5,12,7,8,8,9,19,21,24,20,23,19,17,18,17,22,11,12,3,9,         
  10,4,5,13,3,5,6,3,5,4,2,5,1,2,4,4,3,2,1)                      
     
################################################################
## Step 1: Training of a HMM ##################################
##         for the given time-series of counts ################ 
##         ####################################################
##         More precisely: training of a poisson ##############
##         distribution based hidden Markov model for #########
##         number of states m=2,...,6 #########################
##         and selection of the model with the most ########### 
##         plausible m ########################################
################################################################

m_trained_HMM &lt;- HMM_training(x = x, 
 min_m = 2, 
 max_m = 6, 
 distribution_class = "pois")$trained_HMM_with_selected_m  
 	 
###############################################################
## Step 2: Decoding of the trained HMM for the given ##########
##         time-series of accelerometer counts to extract #####
##         hidden PA-levels ###################################
###############################################################
hidden_PA_levels &lt;- HMM_decoding(x = x, 
 	 m = m_trained_HMM$m, 
 	 delta = m_trained_HMM$delta, 
 	 gamma = m_trained_HMM$gamma, 
 	 distribution_class = m_trained_HMM
 	 $distribution_class, 
 	 distribution_theta = m_trained_HMM$
 	 distribution_theta)$decoding_distr_means

############################################################### 
## Step 3: Assigning of user-sepcified activity ranges ########
##         to the accelerometer counts via the total ##########
##         magnitudes of their corresponding ##################
##         hidden PA-levels ###################################
##         ####################################################
##         In this example four activity ranges ###############
##         (named as "sedentary", "light", "moderate" #########
##         and "vigorous" physical activity) are ##############
##         separated by the three cut-points 5, 15 and 23) ####
################################################################
HMM_based_cut_off_point_method &lt;- cut_off_point_method(x = x, 
 	 hidden_PA_levels = hidden_PA_levels, 
 	 cut_points = c(5,15,23), 
 	 names_activity_ranges = c("SED","LIG","MOD","VIG"), 
 	 bout_lengths = c(1,1,2,4,5,10,11,20,21,60,61,260),
 	 plotting=1)      				 




###############################################################
## Example 3) In a single function (Step 1-3 of Example 2  ####
##            combined in one function)                    ####
###############################################################
	
###############################################################
## Fictitious activity counts #################################
###############################################################
                                                                
x &lt;- c(1,16,19,34,22,6,3,5,6,3,4,1,4,3,5,7,9,8,11,11,           
  14,16,13,11,11,10,12,19,23,25,24,23,20,21,22,22,18,7,         
  5,3,4,3,2,3,4,5,4,2,1,3,4,5,4,5,3,5,6,4,3,6,4,8,9,12,         
  9,14,17,15,25,23,25,35,29,36,34,36,29,41,42,39,40,43,         
  37,36,20,20,21,22,23,26,27,28,25,28,24,21,25,21,20,21,        
  11,18,19,20,21,13,19,18,20,7,18,8,15,17,16,13,10,4,9,         
  7,8,10,9,11,9,11,10,12,12,5,13,4,6,6,13,8,9,10,13,13,         
  11,10,5,3,3,4,9,6,8,3,5,3,2,2,1,3,5,11,2,3,5,6,9,8,5,         
  2,5,3,4,6,4,8,15,12,16,20,18,23,18,19,24,23,24,21,26,         
  36,38,37,39,45,42,41,37,38,38,35,37,35,31,32,30,20,39,        
  40,33,32,35,34,36,34,32,33,27,28,25,22,17,18,16,10,9,         
  5,12,7,8,8,9,19,21,24,20,23,19,17,18,17,22,11,12,3,9,         
  10,4,5,13,3,5,6,3,5,4,2,5,1,2,4,4,3,2,1)                      

###############################################################
## Use a (m=4 state) hidden Markov model based on the #########
## generalized poisson distribution to assign an      #########
## activity range to the counts                       #########
###############################################################
## In this example three activity ranges                   ####
## (named as "light", "moderate" and "vigorous" physical   ####
## activity) are separated by the two cut-points 15 and 23 ####
###############################################################

HMM_based_cut_off_point_method &lt;- HMM_based_method(x = x, 
 	 cut_points = c(15,23), 
 	 min_m = 4, 
 	 max_m = 4, 
 	 names_activity_ranges = c("LIG","MOD","VIG"), 
 	 distribution_class = "genpois", 
 	 training_method = "numerical",
 	 DNM_limit_accuracy = 0.05,
 	 DNM_max_iter = 10,
 	 bout_lengths = c(1,1,2,4,5,10,11,20,21,60,61,260),
 	 plotting = 1)
 	 

###############################################################
## Example 4) In a single function (Step 1-3 of Example 2  ####
##            combined in one function)                    ####
##            (large and highly scatterd time-series)      ####
###############################################################
	 
################################################################
### Generate a large time-series of highly scattered counts ####
################################################################

x &lt;- HMM_simulation(
 size = 1500, 
 m = 10,
 gamma = 0.93 * diag(10) + rep(0.07 / 10, times = 10),
 distribution_class = "norm", 
 distribution_theta = list(mean = c(10, 100, 200, 300, 450, 
 600, 700, 900, 1100, 1300, 1500), 
 sd=c(rep(100,times=10))), 
 obs_round=TRUE, 
 obs_non_neg=TRUE,
 plotting=5)$observations

################################################################
### Compare results of the tradional cut-point method ##########
### and the (6-state-normal-)HMM based method ##################
################################################################
		
traditional_cut_off_point_method &lt;- cut_off_point_method(x=x, 
 	 cut_points = c(200,500,1000), 
 	 names_activity_ranges = c("SED","LIG","MOD","VIG"), 
 	 bout_lengths = c(1,1,2,4,5,10,11,20,21,60,61,260),
 	 plotting = 1)

HMM_based_cut_off_point_method &lt;- HMM_based_method(x=x,
	 max_scaled_x = 200, 
 	 cut_points = c(200,500,1000), 
 	 min_m = 6, 
 	 max_m = 6,
 	 BW_limit_accuracy = 0.5, 
 	 BW_max_iter = 10,
 	 names_activity_ranges = c("SED","LIG","MOD","VIG"), 
 	 distribution_class = "norm", 
 	 bout_lengths = c(1,1,2,4,5,10,11,20,21,60,61,260),
 	 plotting = 1)

</code></pre>

<hr>
<h2 id='initial_parameter_training'>
Algorithm to Find Plausible Starting Values for Parameter Estimation
</h2><span id='topic+initial_parameter_training'></span>

<h3>Description</h3>

<p>The function computes plausible starting values for both the Baum-Welch algorithm and the algorithm for directly maximizing the log-Likelihood.  Plausible starting values can potentially diminish problems of (i) numerical instability and (ii) not finding the global optimum.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>initial_parameter_training(x, m, distribution_class, n = 100, 
                           discr_logL = FALSE, discr_logL_eps = 0.5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="initial_parameter_training_+3A_x">x</code></td>
<td>

<p>a vector object containing the time-series of observations that are assumed to be realizations of the (hidden Markov state dependent) observation process of the model.</p>
</td></tr>
<tr><td><code id="initial_parameter_training_+3A_m">m</code></td>
<td>
<p>a (finite) number of states in the hidden Markov chain.</p>
</td></tr>
<tr><td><code id="initial_parameter_training_+3A_distribution_class">distribution_class</code></td>
<td>

<p>a single character string object with the abbreviated name of the <code>m</code> observation distributions of the Markov dependent observation process.  The following distributions are supported by this algorithm: Poisson (<code>pois</code>); generalized Poisson (<code>genpois</code>); normal (<code>norm</code>); geometric (<code>geom</code>).</p>
</td></tr>
<tr><td><code id="initial_parameter_training_+3A_n">n</code></td>
<td>

<p>a single numerical value specifying the number of samples to find the best starting value for the training algorithm.  Default value is <code>100</code>.</p>
</td></tr>
<tr><td><code id="initial_parameter_training_+3A_discr_logl">discr_logL</code></td>
<td>

<p>a logical object.  <code>TRUE</code>, if the discrete log-likelihood shall be calculated (for <code>distribution_class="norm"</code> instead of the general log-likelihood.  Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="initial_parameter_training_+3A_discr_logl_eps">discr_logL_eps</code></td>
<td>

<p>discrete log-likelihood for a hidden Markov model based on nomal distributions (for <code>distribution_class="norm"</code>).  The default value is <code>0.5</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>From our experience, parameter estimation for long time-series of observations (<code>T&gt;1000</code>) or observation values <code>&gt;1500</code> tend to be numerical instable and does not necessarily find a global maximum.  Both problems can eventually be diminished with plausible starting values.  Basically, the idea behind <code>initial_parameter_training</code> is to sample randomly <code>n</code> sets of <code>m</code> observations from the time-series <code>x</code>, as means (<code>E</code>) of the state-dependent distributions. This <code>n</code> samplings of <code>E</code>, therefore induce <code>n</code> sets of parameters (<code>distribution_theta</code>) for the HMM without running a (slow) parameter estimation algorithm. Furthermore, <code>initial_parameter_training</code> calculates the log-Likelihood for all those <code>n</code> sets of parameters.  The set of parameters with the best Likelihood are outputted as plausible starting values.
(Additionally to the <code>n</code> sets of randomly chosen observations as means, the <code>m</code> quantiles of the observations are also checked as plausible means within this algorithm.)
</p>


<h3>Value</h3>

<p><code> initial_parameter_training </code> returns a list containing the following components:
</p>
<table>
<tr><td><code>m</code></td>
<td>
<p>input number of states in the hidden Markov chain.</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>a single numerical value representing the number of parameters of the defined distribution class of the observation process.</p>
</td></tr>
<tr><td><code>logL</code></td>
<td>
<p>logarithmized likelihood of the model evaluated at the HMM with given starting values (<code>delta, gamma, distribution theta</code>) induced by <code>E</code>.</p>
</td></tr>
<tr><td><code>E</code></td>
<td>
<p>randomly choosen means of the observation time-series <code>x</code>, used for the observation distributions, for which the induced parameters 
</p>
<p>(<code>delta, gamma, distribution theta</code>) produce the largest Likelihood.</p>
</td></tr>
<tr><td><code>distribution_theta</code></td>
<td>
<p>a list object containing the plausible starting values for the parameters of the <code>m</code> observation distributions that are dependent on the hidden Markov state.</p>
</td></tr>
<tr><td><code>delta</code></td>
<td>
<p>a vector object containing plausible starting values for the marginal probability distribution of the <code>m</code> states of the Markov chain at the time point <code>t=1</code>.  At the moment:<br /> <code>delta = rep(1/m, times=m)</code>.</p>
</td></tr>
<tr><td><code>gamma</code></td>
<td>
<p>a matrix (<code>nrow=ncol=m</code>) containing the plausible starting values for the transition matrix of the hidden Markov chain.  At the moment:<br /> <code>gamma = 0.8 * diag(m) + rep(0.2/m, times=m)</code>.</p>
</td></tr> 
</table>


<h3>Author(s)</h3>

<p>Vitali Witowski (2013).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Baum_Welch_algorithm">Baum_Welch_algorithm</a></code>
<code><a href="#topic+direct_numerical_maximization">direct_numerical_maximization</a></code>
<code><a href="#topic+HMM_training">HMM_training</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
################################################################
### Fictitious observations ####################################
################################################################

x &lt;- c(1,16,19,34,22,6,3,5,6,3,4,1,4,3,5,7,9,8,11,11,
  14,16,13,11,11,10,12,19,23,25,24,23,20,21,22,22,18,7,
  5,3,4,3,2,3,4,5,4,2,1,3,4,5,4,5,3,5,6,4,3,6,4,8,9,12,
  9,14,17,15,25,23,25,35,29,36,34,36,29,41,42,39,40,43,
  37,36,20,20,21,22,23,26,27,28,25,28,24,21,25,21,20,21,
  11,18,19,20,21,13,19,18,20,7,18,8,15,17,16,13,10,4,9,
  7,8,10,9,11,9,11,10,12,12,5,13,4,6,6,13,8,9,10,13,13,
  11,10,5,3,3,4,9,6,8,3,5,3,2,2,1,3,5,11,2,3,5,6,9,8,5,
  2,5,3,4,6,4,8,15,12,16,20,18,23,18,19,24,23,24,21,26,
  36,38,37,39,45,42,41,37,38,38,35,37,35,31,32,30,20,39,
  40,33,32,35,34,36,34,32,33,27,28,25,22,17,18,16,10,9,
  5,12,7,8,8,9,19,21,24,20,23,19,17,18,17,22,11,12,3,9,
  10,4,5,13,3,5,6,3,5,4,2,5,1,2,4,4,3,2,1) 


### Finding plausibel starting values for the parameter estimation 
### for a generealized-Pois-HMM with m=4 states
m &lt;- 4 


plausible_starting_values &lt;- 
   initial_parameter_training(x = x, 
     m = m, 
     distribution_class = "genpois", 
     n=100)

print(plausible_starting_values)

</code></pre>

<hr>
<h2 id='local_decoding_algorithm'>
Algorithm for Decoding Hidden Markov Models (local)
</h2><span id='topic+local_decoding_algorithm'></span>

<h3>Description</h3>

<p>The function decodes a hidden Markov model into a most likely sequence of hidden states. Different to the <code><a href="#topic+Viterbi_algorithm">Viterbi_algorithm</a></code>, this algorithm determines the most likely hidden state for each time point seperately.</p>


<h3>Usage</h3>

<pre><code class='language-R'>local_decoding_algorithm(x, m, delta, gamma, distribution_class, 
      distribution_theta, discr_logL = FALSE, discr_logL_eps = 0.5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="local_decoding_algorithm_+3A_x">x</code></td>
<td>
<p>a vector object containing the time-series of observations that are assumed to be realizations of the (hidden Markov state dependent) observation process of the model.</p>
</td></tr>
<tr><td><code id="local_decoding_algorithm_+3A_m">m</code></td>
<td>
<p>a (finite) number of states in the hidden Markov chain.</p>
</td></tr>
<tr><td><code id="local_decoding_algorithm_+3A_delta">delta</code></td>
<td>

<p>a vector object containing values for the marginal probability distribution of the <code>m</code> states of the Markov chain at the time point <code>t=1</code>.</p>
</td></tr>
<tr><td><code id="local_decoding_algorithm_+3A_gamma">gamma</code></td>
<td>
<p>a matrix (<code>ncol=nrow=m</code>) containing values for the transition matrix of the hidden Markov chain.</p>
</td></tr>
<tr><td><code id="local_decoding_algorithm_+3A_distribution_class">distribution_class</code></td>
<td>

<p>a single character string object with the abbreviated name of the <code>m</code> observation distributions of the Markov dependent observation process.  The following distributions are supported by this algorithm: Poisson (<code>pois</code>); generalized Poisson (<code>genpois</code>); normal (<code>norm</code>); geometric (<code>geom</code>).</p>
</td></tr>
<tr><td><code id="local_decoding_algorithm_+3A_distribution_theta">distribution_theta</code></td>
<td>

<p>a list object containing the parameter values for the <code>m</code> observation distributions that are dependent on the hidden Markov state.</p>
</td></tr>
<tr><td><code id="local_decoding_algorithm_+3A_discr_logl">discr_logL</code></td>
<td>

<p>a logical object. It is <code>TRUE</code> if the discrete log-likelihood shall be calculated (for <code>distribution_class="norm"</code> instead of the general log-likelihood).  Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="local_decoding_algorithm_+3A_discr_logl_eps">discr_logL_eps</code></td>
<td>

<p>a single numerical value to approximately determine the discrete log-likelihood for a hidden Markov model based on nomal distributions (for <code>"norm"</code>).  The default value is <code>0.5</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code> local_decoding_algorithm </code> returns a list containing the following two components:
</p>
<table>
<tr><td><code>state_probabilities</code></td>
<td>

<p>a (T,m)-matrix (when T indicates the length/size of the observation time-series and m the number of states of the HMM) containing probabilities (conditional probability of a state i=1,...,m at a time point t=1,...,T given all observations x) calculated by the algorithm. See MacDonald &amp; Zucchini (2009, Paragraph 5.3.1) for further details.</p>
</td></tr>
<tr><td><code>decoding</code></td>
<td>

<p>a numerical vector containing the locally most likely sequence of hidden states as decoded by the local_decoding_algorithm.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>The basic algorithm for a Poisson-HMM can be found in MacDonald &amp; Zucchini (2009, Paragraph A.2.6).  Extension and implementation by Vitali Witowski (2013).
</p>


<h3>References</h3>

<p>MacDonald, I. L.,  Zucchini, W. (2009) <em>Hidden Markov Models for Time Series: An Introduction Using R</em>, Boca Raton: Chapman &amp; Hall.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Viterbi_algorithm">Viterbi_algorithm</a></code>,
<code><a href="#topic+HMM_decoding">HMM_decoding</a></code>		
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
################################################################
### Fictitious observations ####################################
################################################################

x &lt;- c(1,16,19,34,22,6,3,5,6,3,4,1,4,3,5,7,9,8,11,11,
  14,16,13,11,11,10,12,19,23,25,24,23,20,21,22,22,18,7,
  5,3,4,3,2,3,4,5,4,2,1,3,4,5,4,5,3,5,6,4,3,6,4,8,9,12,
  9,14,17,15,25,23,25,35,29,36,34,36,29,41,42,39,40,43,
  37,36,20,20,21,22,23,26,27,28,25,28,24,21,25,21,20,21,
  11,18,19,20,21,13,19,18,20,7,18,8,15,17,16,13,10,4,9,
  7,8,10,9,11,9,11,10,12,12,5,13,4,6,6,13,8,9,10,13,13,
  11,10,5,3,3,4,9,6,8,3,5,3,2,2,1,3,5,11,2,3,5,6,9,8,5,
  2,5,3,4,6,4,8,15,12,16,20,18,23,18,19,24,23,24,21,26,
  36,38,37,39,45,42,41,37,38,38,35,37,35,31,32,30,20,39,
  40,33,32,35,34,36,34,32,33,27,28,25,22,17,18,16,10,9,
  5,12,7,8,8,9,19,21,24,20,23,19,17,18,17,22,11,12,3,9,
  10,4,5,13,3,5,6,3,5,4,2,5,1,2,4,4,3,2,1) 

### Train hidden Markov model for m=4

m_trained_HMM &lt;- 
    HMM_training(x = x, 
      min_m = 4, 
      max_m = 4, 
      distribution_class = "pois")$trained_HMM_with_selected_m

### Decode the trained HMM using the local-decoding algorithm 
### to get the locally most likely sequence of hidden states 
### for the time-series of observations
local_decoding &lt;- 
    local_decoding_algorithm(x = x, 
       m = m_trained_HMM$m, 
       delta = m_trained_HMM$delta, 
       gamma = m_trained_HMM$gamma, 
       distribution_class = m_trained_HMM$distribution_class, 
       distribution_theta = m_trained_HMM$distribution_theta)

### Most likely sequence of hidden states
print(local_decoding$decoding)
plot(local_decoding$decoding)

</code></pre>

<hr>
<h2 id='Viterbi_algorithm'>
Algorithm for Decoding Hidden Markov Models (global)
</h2><span id='topic+Viterbi_algorithm'></span>

<h3>Description</h3>

<p>The function decodes a trainded hidden Markov model into a most likely sequence of hidden states. Different to the <code><a href="#topic+local_decoding_algorithm">local_decoding_algorithm</a></code>, this algorithm determines the sequence of most likely hidden states for all time points simultaneously.  See MacDonald &amp; Zucchini (2009, Paragraph 5.3.2) for further details.</p>


<h3>Usage</h3>

<pre><code class='language-R'>Viterbi_algorithm(x, m, delta, gamma, distribution_class, 
   distribution_theta, discr_logL = FALSE, discr_logL_eps = 0.5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Viterbi_algorithm_+3A_x">x</code></td>
<td>
<p>a vector object containing the time-series of observations that are assumed to be realizations of the (hidden Markov state dependent) observation process of the model.</p>
</td></tr>
<tr><td><code id="Viterbi_algorithm_+3A_m">m</code></td>
<td>
<p>a (finite) number of states in the hidden Markov chain.</p>
</td></tr>
<tr><td><code id="Viterbi_algorithm_+3A_delta">delta</code></td>
<td>

<p>a vector object containing values for the marginal probability distribution of the <code>m</code> states of the Markov chain at the time point <code>t=1</code>.</p>
</td></tr>
<tr><td><code id="Viterbi_algorithm_+3A_gamma">gamma</code></td>
<td>
<p>a matrix (<code>ncol=nrow=m</code>) containing values for the transition matrix of the hidden Markov chain.</p>
</td></tr>
<tr><td><code id="Viterbi_algorithm_+3A_distribution_class">distribution_class</code></td>
<td>

<p>a single character string object with the abbreviated name of the <code>m</code> observation distributions of the Markov dependent observation process.  The following distributions are supported by this algorithm: Poisson (<code>pois</code>); generalized Poisson (<code>genpois</code>); normal (<code>norm</code>); geometric (<code>geom</code>).</p>
</td></tr>
<tr><td><code id="Viterbi_algorithm_+3A_distribution_theta">distribution_theta</code></td>
<td>

<p>a list object containing the parameter values for the <code>m</code> observation distributions that are dependent on the hidden Markov state.</p>
</td></tr>
<tr><td><code id="Viterbi_algorithm_+3A_discr_logl">discr_logL</code></td>
<td>

<p>a logical object. It is <code>TRUE</code> if the discrete log-likelihood shall be calculated (for <code>distribution_class="norm"</code> instead of the general log-likelihood).  Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="Viterbi_algorithm_+3A_discr_logl_eps">discr_logL_eps</code></td>
<td>

<p>a single numerical value to approximately determine the discrete log-likelihood for a hidden Markov model based on nomal distributions (for <code>"norm"</code>).  The default value is <code>0.5</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code> Viterbi_algorithm </code> returns a list containing the following two components:
</p>
<table>
<tr><td><code>omega</code></td>
<td>

<p>a (T,m)-matrix (when T indicates the length/size of the observation time-series and m the number of states of the HMM) containing probabilities (maximum probability to generate the first t members (t=1,...,T) of the given time-series x with the HMM and to stop in state i=1,...,m) calculated by the algorithm. See MacDonald &amp; Zucchini (2009, Paragraph 5.3.2) for further details.</p>
</td></tr>
<tr><td><code>decoding</code></td>
<td>

<p>a numerical vector containing the globally most likely sequence of hidden states as decoded by the Viterbi algorithm.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>The basic algorithm for a Poisson-HMM can be found in MacDonald &amp; Zucchini (2009, Paragraph A.2.4).  Extension and implementation by Vitali Witowski (2013).
</p>


<h3>References</h3>

<p>MacDonald, I. L.,  Zucchini, W. (2009) <em>Hidden Markov Models for Time Series: An Introduction Using R</em>, Boca Raton: Chapman &amp; Hall.
</p>
<p>Forney, G.D. (1973). The Viterbi algorithm. Proceeding of the IEE, vol. <b>61</b>(3), 268&ndash;278.
</p>
<p>Viterbi, A.J. (1967). Error Bounds for concolutional codes and an asymptotically optimal decoding algorithm. Information Theory, IEEE Transactions on, vol. <b>13</b>(2), 260&ndash;269.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+local_decoding_algorithm">local_decoding_algorithm</a></code>,		
<code><a href="#topic+HMM_decoding">HMM_decoding</a></code>		
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
################################################################
### Fictitious observations ####################################
################################################################

x &lt;- c(1,16,19,34,22,6,3,5,6,3,4,1,4,3,5,7,9,8,11,11,
  14,16,13,11,11,10,12,19,23,25,24,23,20,21,22,22,18,7,
  5,3,4,3,2,3,4,5,4,2,1,3,4,5,4,5,3,5,6,4,3,6,4,8,9,12,
  9,14,17,15,25,23,25,35,29,36,34,36,29,41,42,39,40,43,
  37,36,20,20,21,22,23,26,27,28,25,28,24,21,25,21,20,21,
  11,18,19,20,21,13,19,18,20,7,18,8,15,17,16,13,10,4,9,
  7,8,10,9,11,9,11,10,12,12,5,13,4,6,6,13,8,9,10,13,13,
  11,10,5,3,3,4,9,6,8,3,5,3,2,2,1,3,5,11,2,3,5,6,9,8,5,
  2,5,3,4,6,4,8,15,12,16,20,18,23,18,19,24,23,24,21,26,
  36,38,37,39,45,42,41,37,38,38,35,37,35,31,32,30,20,39,
  40,33,32,35,34,36,34,32,33,27,28,25,22,17,18,16,10,9,
  5,12,7,8,8,9,19,21,24,20,23,19,17,18,17,22,11,12,3,9,
  10,4,5,13,3,5,6,3,5,4,2,5,1,2,4,4,3,2,1) 


### Train hidden Markov model for m=4

m_trained_HMM &lt;- 
    HMM_training(x = x, 
      min_m = 4, 
      max_m = 4, 
      distribution_class="pois")$trained_HMM_with_selected_m

### Decode the trained HMM using the Viterbi algorithm to get 
### the globally most likely sequence of hidden states for 
### the time-series of observations
global_decoding &lt;- 
    Viterbi_algorithm(x = x, 
      m = m_trained_HMM$m, 
      delta = m_trained_HMM$delta, 
      gamma = m_trained_HMM$gamma, 
      distribution_class = m_trained_HMM$distribution_class, 
      distribution_theta = m_trained_HMM$distribution_theta)

### Most likely sequence of hidden states
print(global_decoding$decoding)
plot(global_decoding$decoding)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
