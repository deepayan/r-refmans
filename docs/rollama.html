<!DOCTYPE html><html><head><title>Help for package rollama</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {rollama}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#rollama-package'><p>rollama: Communicate with 'Ollama'</p></a></li>
<li><a href='#chat_history'><p>Handle conversations</p></a></li>
<li><a href='#check_model_installed'><p>Check if one or several models are installed on the server</p></a></li>
<li><a href='#create_model'><p>Create a model from a Modelfile</p></a></li>
<li><a href='#embed_text'><p>Generate Embeddings</p></a></li>
<li><a href='#list_models'><p>List models that are available locally.</p></a></li>
<li><a href='#ping_ollama'><p>Ping server to see if Ollama is reachable</p></a></li>
<li><a href='#pull_model'><p>Pull, show and delete models</p></a></li>
<li><a href='#query'><p>Chat with a LLM through Ollama</p></a></li>
<li><a href='#rollama-options'><p>rollama Options</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Communicate with 'Ollama'</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.0</td>
</tr>
<tr>
<td>Description:</td>
<td>Wraps the 'Ollama' <a href="https://ollama.com">https://ollama.com</a> API, which can be used to 
    communicate with generative large language models locally.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.1</td>
</tr>
<tr>
<td>Imports:</td>
<td>callr, cli, dplyr, httr2, jsonlite, methods, prettyunits,
purrr, rlang, tibble</td>
</tr>
<tr>
<td>Suggests:</td>
<td>base64enc, covr, knitr, rmarkdown, spelling, testthat (&ge;
3.0.0)</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.1.0)</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>Language:</td>
<td>en-US</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://jbgruber.github.io/rollama/">https://jbgruber.github.io/rollama/</a>,
<a href="https://github.com/JBGruber/rollama">https://github.com/JBGruber/rollama</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-05-01 07:04:33 UTC; johannes</td>
</tr>
<tr>
<td>Author:</td>
<td>Johannes B. Gruber
    <a href="https://orcid.org/0000-0001-9177-1772"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut, cre],
  Maximilian Weber <a href="https://orcid.org/0000-0002-1174-449X"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Johannes B. Gruber &lt;JohannesB.Gruber@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-05-01 07:20:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='rollama-package'>rollama: Communicate with 'Ollama'</h2><span id='topic+rollama'></span><span id='topic+rollama-package'></span>

<h3>Description</h3>

<p><img src="../help/figures/logo.png" style='float: right' alt='logo' width='120' />
</p>
<p>Wraps the 'Ollama' <a href="https://ollama.com">https://ollama.com</a> API, which can be used to communicate with generative large language models locally.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Johannes B. Gruber <a href="mailto:JohannesB.Gruber@gmail.com">JohannesB.Gruber@gmail.com</a> (<a href="https://orcid.org/0000-0001-9177-1772">ORCID</a>)
</p>
<p>Authors:
</p>

<ul>
<li><p> Maximilian Weber (<a href="https://orcid.org/0000-0002-1174-449X">ORCID</a>) [contributor]
</p>
</li></ul>



<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://jbgruber.github.io/rollama/">https://jbgruber.github.io/rollama/</a>
</p>
</li>
<li> <p><a href="https://github.com/JBGruber/rollama">https://github.com/JBGruber/rollama</a>
</p>
</li></ul>


<hr>
<h2 id='chat_history'>Handle conversations</h2><span id='topic+chat_history'></span><span id='topic+new_chat'></span>

<h3>Description</h3>

<p>Shows and deletes (<code>new_chat</code>) the local prompt and response history to start a new conversation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>chat_history()

new_chat()
</code></pre>


<h3>Value</h3>

<p>chat_history: tibble with chat history
</p>
<p>new_chat: Does not return a value
</p>

<hr>
<h2 id='check_model_installed'>Check if one or several models are installed on the server</h2><span id='topic+check_model_installed'></span>

<h3>Description</h3>

<p>Check if one or several models are installed on the server
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_model_installed(model, auto_pull = FALSE, server = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_model_installed_+3A_model">model</code></td>
<td>
<p>names of one or several models as character vector.</p>
</td></tr>
<tr><td><code id="check_model_installed_+3A_auto_pull">auto_pull</code></td>
<td>
<p>if FALSE, the default, asks before downloading models.</p>
</td></tr>
<tr><td><code id="check_model_installed_+3A_server">server</code></td>
<td>
<p>URL to an Ollama server (not the API). Defaults to
&quot;http://localhost:11434&quot;.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>invisible TRUE/FALSE
</p>

<hr>
<h2 id='create_model'>Create a model from a Modelfile</h2><span id='topic+create_model'></span>

<h3>Description</h3>

<p>Create a model from a Modelfile
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_model(model, modelfile, server = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="create_model_+3A_model">model</code></td>
<td>
<p>name of the model to create</p>
</td></tr>
<tr><td><code id="create_model_+3A_modelfile">modelfile</code></td>
<td>
<p>either a path to a model file to be read or the contents of
the model file as a character vector.</p>
</td></tr>
<tr><td><code id="create_model_+3A_server">server</code></td>
<td>
<p>URL to an Ollama server (not the API). Defaults to
&quot;http://localhost:11434&quot;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Custom models are the way to save your system message and model
parameters in a dedicated shareable way. If you use <code>show_model()</code>, you can
look at the configuration of a model in the column modelfile. To get more
information and a list of valid parameters, check out
<a href="https://github.com/ollama/ollama/blob/main/docs/modelfile.md">https://github.com/ollama/ollama/blob/main/docs/modelfile.md</a>. Most
options are also available through the <code>query</code> and <code>chat</code> functions, yet
are not persistent over sessions.
</p>


<h3>Value</h3>

<p>Nothing. Called to create a model on the Ollama server.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>modelfile &lt;- system.file("extdata", "modelfile.txt", package = "rollama")
## Not run: create_model("mario", modelfile)
modelfile &lt;- "FROM llama3\nSYSTEM You are mario from Super Mario Bros."
## Not run: create_model("mario", modelfile)
</code></pre>

<hr>
<h2 id='embed_text'>Generate Embeddings</h2><span id='topic+embed_text'></span>

<h3>Description</h3>

<p>Generate Embeddings
</p>


<h3>Usage</h3>

<pre><code class='language-R'>embed_text(
  text,
  model = NULL,
  server = NULL,
  model_params = NULL,
  verbose = getOption("rollama_verbose", default = interactive())
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="embed_text_+3A_text">text</code></td>
<td>
<p>text vector to generate embeddings for.</p>
</td></tr>
<tr><td><code id="embed_text_+3A_model">model</code></td>
<td>
<p>which model to use. See <a href="https://ollama.com/library">https://ollama.com/library</a> for options.
Default is &quot;llama3&quot;. Set option(rollama_model = &quot;modelname&quot;) to change
default for the current session. See <a href="#topic+pull_model">pull_model</a> for more details.</p>
</td></tr>
<tr><td><code id="embed_text_+3A_server">server</code></td>
<td>
<p>URL to an Ollama server (not the API). Defaults to
&quot;http://localhost:11434&quot;.</p>
</td></tr>
<tr><td><code id="embed_text_+3A_model_params">model_params</code></td>
<td>
<p>a named list of additional model parameters listed in the
<a href="https://github.com/ollama/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values">documentation for the Modelfile</a>.</p>
</td></tr>
<tr><td><code id="embed_text_+3A_verbose">verbose</code></td>
<td>
<p>Whether to print status messages to the Console
(<code>TRUE</code>/<code>FALSE</code>). The default is to have status messages in
interactive sessions. Can be changed with <code>options(rollama_verbose =
  FALSE)</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a tibble with embeddings.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
embed_text(c("Here is an article about llamas...",
             "R is a language and environment for statistical computing and graphics."))

## End(Not run)
</code></pre>

<hr>
<h2 id='list_models'>List models that are available locally.</h2><span id='topic+list_models'></span>

<h3>Description</h3>

<p>List models that are available locally.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>list_models(server = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="list_models_+3A_server">server</code></td>
<td>
<p>URL to an Ollama server (not the API). Defaults to
&quot;http://localhost:11434&quot;.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a tibble of installed models
</p>

<hr>
<h2 id='ping_ollama'>Ping server to see if Ollama is reachable</h2><span id='topic+ping_ollama'></span>

<h3>Description</h3>

<p>Ping server to see if Ollama is reachable
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ping_ollama(server = NULL, silent = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ping_ollama_+3A_server">server</code></td>
<td>
<p>URL to an Ollama server (not the API). Defaults to
&quot;http://localhost:11434&quot;.</p>
</td></tr>
<tr><td><code id="ping_ollama_+3A_silent">silent</code></td>
<td>
<p>suppress warnings and status (only return <code>TRUE</code>/<code>FALSE</code>).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>TRUE if server is running
</p>

<hr>
<h2 id='pull_model'>Pull, show and delete models</h2><span id='topic+pull_model'></span><span id='topic+show_model'></span><span id='topic+delete_model'></span><span id='topic+copy_model'></span>

<h3>Description</h3>

<p>Pull, show and delete models
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pull_model(model = NULL, server = NULL, insecure = FALSE)

show_model(model = NULL, server = NULL)

delete_model(model, server = NULL)

copy_model(model, destination = paste0(model, "-copy"), server = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pull_model_+3A_model">model</code></td>
<td>
<p>name of the model. Defaults to &quot;llama3&quot; when <code>NULL</code> (except in
<code>delete_model</code>).</p>
</td></tr>
<tr><td><code id="pull_model_+3A_server">server</code></td>
<td>
<p>URL to an Ollama server (not the API). Defaults to
&quot;http://localhost:11434&quot;.</p>
</td></tr>
<tr><td><code id="pull_model_+3A_insecure">insecure</code></td>
<td>
<p>allow insecure connections to the library. Only use this if
you are pulling from your own library during development. description</p>
</td></tr>
<tr><td><code id="pull_model_+3A_destination">destination</code></td>
<td>
<p>name of the copied model.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li> <p><code>pull_model()</code>: downloads model
</p>
</li>
<li> <p><code>show_model()</code>: displays information about a local model
</p>
</li>
<li> <p><code>copy_model()</code>: creates a model with another name from an existing model
</p>
</li>
<li> <p><code>delete_model()</code>: deletes local model
</p>
</li></ul>

<p><strong>Model names</strong>: Model names follow a model:tag format, where model can have
an optional namespace such as example/model. Some examples are
orca-mini:3b-q4_1 and llama3:70b. The tag is optional and, if not provided,
will default to latest. The tag is used to identify a specific version.
</p>


<h3>Value</h3>

<p>(invisible) a tibble with information about the model (except in
<code>delete_model</code>)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
model_info &lt;- pull_model("mixtral")
# after you pull, you can get the same information with:
model_info &lt;- show_model("mixtral")

## End(Not run)
</code></pre>

<hr>
<h2 id='query'>Chat with a LLM through Ollama</h2><span id='topic+query'></span><span id='topic+chat'></span>

<h3>Description</h3>

<p>Chat with a LLM through Ollama
</p>


<h3>Usage</h3>

<pre><code class='language-R'>query(
  q,
  model = NULL,
  screen = TRUE,
  server = NULL,
  images = NULL,
  model_params = NULL,
  format = NULL,
  template = NULL
)

chat(
  q,
  model = NULL,
  screen = TRUE,
  server = NULL,
  images = NULL,
  model_params = NULL,
  template = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="query_+3A_q">q</code></td>
<td>
<p>the question as a character string or a conversation object.</p>
</td></tr>
<tr><td><code id="query_+3A_model">model</code></td>
<td>
<p>which model(s) to use. See <a href="https://ollama.com/library">https://ollama.com/library</a> for
options. Default is &quot;llama3&quot;. Set option(rollama_model = &quot;modelname&quot;) to
change default for the current session. See <a href="#topic+pull_model">pull_model</a> for more
details.</p>
</td></tr>
<tr><td><code id="query_+3A_screen">screen</code></td>
<td>
<p>Logical. Should the answer be printed to the screen.</p>
</td></tr>
<tr><td><code id="query_+3A_server">server</code></td>
<td>
<p>URL to an Ollama server (not the API). Defaults to
&quot;http://localhost:11434&quot;.</p>
</td></tr>
<tr><td><code id="query_+3A_images">images</code></td>
<td>
<p>path(s) to images (for multimodal models such as llava).</p>
</td></tr>
<tr><td><code id="query_+3A_model_params">model_params</code></td>
<td>
<p>a named list of additional model parameters listed in the
<a href="https://github.com/ollama/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values">documentation for the Modelfile</a>
such as temperature. Use a seed and set the temperature to zero to get
reproducible results (see examples).</p>
</td></tr>
<tr><td><code id="query_+3A_format">format</code></td>
<td>
<p>the format to return a response in. Currently the only accepted
value is <code>"json"</code>.</p>
</td></tr>
<tr><td><code id="query_+3A_template">template</code></td>
<td>
<p>the prompt template to use (overrides what is defined in the
Modelfile).</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>query</code> sends a single question to the API, without knowledge about
previous questions (only the config message is relevant). <code>chat</code> treats new
messages as part of the same conversation until <a href="#topic+new_chat">new_chat</a> is called.
</p>


<h3>Value</h3>

<p>an httr2 response.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# ask a single question
query("why is the sky blue?")

# hold a conversation
chat("why is the sky blue?")
chat("and how do you know that?")

# save the response to an object and extract the answer
resp &lt;- query(q = "why is the sky blue?")
answer &lt;- resp$message$content

# ask question about images (to a multimodal model)
images &lt;- c("https://avatars.githubusercontent.com/u/23524101?v=4", # remote
            "/path/to/your/image.jpg") # or local images supported
query(q = "describe these images",
      model = "llava",
      images = images)

# set custom options for the model at runtime (rather than in create_model())
query("why is the sky blue?",
      model_params = list(
        num_keep = 5,
        seed = 42,
        num_predict = 100,
        top_k = 20,
        top_p = 0.9,
        tfs_z = 0.5,
        typical_p = 0.7,
        repeat_last_n = 33,
        temperature = 0.8,
        repeat_penalty = 1.2,
        presence_penalty = 1.5,
        frequency_penalty = 1.0,
        mirostat = 1,
        mirostat_tau = 0.8,
        mirostat_eta = 0.6,
        penalize_newline = TRUE,
        stop = c("\n", "user:"),
        numa = FALSE,
        num_ctx = 1024,
        num_batch = 2,
        num_gqa = 1,
        num_gpu = 1,
        main_gpu = 0,
        low_vram = FALSE,
        f16_kv = TRUE,
        vocab_only = FALSE,
        use_mmap = TRUE,
        use_mlock = FALSE,
        embedding_only = FALSE,
        rope_frequency_base = 1.1,
        rope_frequency_scale = 0.8,
        num_thread = 8
      ))

# use a seed and zero temperature to get reproducible results
query("why is the sky blue?", model_params = list(seed = 42, temperature = 0)

# this might be interesting if you want to turn off the GPU and load the
# model into the system memory (slower, but most people have more RAM than
# VRAM, which might be interesting for larger models)
query("why is the sky blue?",
       model_params = list(num_gpu = 0))

# You can use a custom prompt to override what prompt the model receives
query("why is the sky blue?",
      template = "Just say I'm a llama!")

# Asking the same question to multiple models is also supported
query("why is the sky blue?", model = c("llama3", "orca-mini"))

## End(Not run)
</code></pre>

<hr>
<h2 id='rollama-options'>rollama Options</h2><span id='topic+rollama-options'></span>

<h3>Description</h3>

<p>The behaviour of <code>rollama</code> can be controlled through <code>options()</code>. Specifically,
the options below can be set.
</p>


<h3>Details</h3>


<dl>
<dt>rollama_server</dt><dd>
<p>This controls the default server where Ollama is expected to run. It assumes
that you are running Ollama locally in a Docker container.
</p>
<dl>
<dt>default:</dt><dd><p><code>"http://localhost:11434"</code></p>
</dd>
</dl>
</dd>
<dt>rollama_model</dt><dd>
<p>The default model is llama3, which is a good overall option with reasonable
performance and size for most tasks. You can change the model in each
function call or globally with this option.
</p>
<dl>
<dt>default:</dt><dd><p><code>"llama3"</code></p>
</dd>
</dl>
</dd>
<dt>rollama_verbose</dt><dd>
<p>Whether the package tells users what is going on, e.g., showing a spinner
while the models are thinking or showing the download speed while pulling
models. Since this adds some complexity to the code, you might want to
disable it when you get errors (it won't fix the error, but you get a
better error trace).
</p>
<dl>
<dt>default:</dt><dd><p><code>TRUE</code></p>
</dd>
</dl>
</dd>
<dt>rollama_config</dt><dd>
<p>The default configuration or system message. If NULL, the system message
defined in the used model is employed.
</p>
<dl>
<dt>default:</dt><dd><p>None</p>
</dd>
</dl>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>options(rollama_config = "You make answers understandable to a 5 year old")
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
