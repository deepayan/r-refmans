<!DOCTYPE html><html lang="en-US"><head><title>Help for package rollama</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {rollama}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#rollama-package'><p>rollama: Communicate with 'Ollama' to run large language models locally</p></a></li>
<li><a href='#chat_history'><p>Handle conversations</p></a></li>
<li><a href='#check_model_installed'><p>Check if one or several models are installed on the server</p></a></li>
<li><a href='#create_model'><p>Create a model from a Modelfile</p></a></li>
<li><a href='#embed_text'><p>Generate Embeddings</p></a></li>
<li><a href='#list_models'><p>List models that are available locally.</p></a></li>
<li><a href='#make_query'><p>Generate and format queries for a language model</p></a></li>
<li><a href='#ping_ollama'><p>Ping server to see if Ollama is reachable</p></a></li>
<li><a href='#pull_model'><p>Pull, show and delete models</p></a></li>
<li><a href='#query'><p>Chat with a LLM through Ollama</p></a></li>
<li><a href='#rollama-options'><p>rollama Options</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>Communicate with 'Ollama' to Run Large Language Models Locally</td>
</tr>
<tr>
<td>Version:</td>
<td>0.2.0</td>
</tr>
<tr>
<td>Description:</td>
<td>Wraps the 'Ollama' <a href="https://ollama.com">https://ollama.com</a> API, which can be used to 
    communicate with generative large language models locally.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.2</td>
</tr>
<tr>
<td>Imports:</td>
<td>callr, cli, dplyr, httr2, jsonlite, methods, prettyunits,
purrr, rlang, tibble, withr</td>
</tr>
<tr>
<td>Suggests:</td>
<td>base64enc, covr, glue, knitr, rmarkdown, spelling, testthat
(&ge; 3.0.0)</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.1.0)</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>Language:</td>
<td>en-US</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://jbgruber.github.io/rollama/">https://jbgruber.github.io/rollama/</a>,
<a href="https://github.com/JBGruber/rollama">https://github.com/JBGruber/rollama</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/JBGruber/rollama/issues">https://github.com/JBGruber/rollama/issues</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-12-06 15:05:40 UTC; johannes</td>
</tr>
<tr>
<td>Author:</td>
<td>Johannes B. Gruber
    <a href="https://orcid.org/0000-0001-9177-1772"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut, cre],
  Maximilian Weber <a href="https://orcid.org/0000-0002-1174-449X"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Johannes B. Gruber &lt;JohannesB.Gruber@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-12-06 15:20:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='rollama-package'>rollama: Communicate with 'Ollama' to run large language models locally</h2><span id='topic+rollama'></span><span id='topic+rollama-package'></span>

<h3>Description</h3>

<p><img src="../help/figures/logo.png" style='float: right' alt='logo' width='120' />
</p>
<p>Wraps the 'Ollama' <a href="https://ollama.com">https://ollama.com</a> API, which can be used to communicate with generative large language models locally.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Johannes B. Gruber <a href="mailto:JohannesB.Gruber@gmail.com">JohannesB.Gruber@gmail.com</a> (<a href="https://orcid.org/0000-0001-9177-1772">ORCID</a>)
</p>
<p>Authors:
</p>

<ul>
<li><p> Maximilian Weber (<a href="https://orcid.org/0000-0002-1174-449X">ORCID</a>) [contributor]
</p>
</li></ul>



<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://jbgruber.github.io/rollama/">https://jbgruber.github.io/rollama/</a>
</p>
</li>
<li> <p><a href="https://github.com/JBGruber/rollama">https://github.com/JBGruber/rollama</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/JBGruber/rollama/issues">https://github.com/JBGruber/rollama/issues</a>
</p>
</li></ul>


<hr>
<h2 id='chat_history'>Handle conversations</h2><span id='topic+chat_history'></span><span id='topic+new_chat'></span>

<h3>Description</h3>

<p>Shows and deletes (<code>new_chat</code>) the local prompt and response history to start
a new conversation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>chat_history()

new_chat()
</code></pre>


<h3>Value</h3>

<p>chat_history: tibble with chat history
</p>
<p>new_chat: Does not return a value
</p>

<hr>
<h2 id='check_model_installed'>Check if one or several models are installed on the server</h2><span id='topic+check_model_installed'></span>

<h3>Description</h3>

<p>Check if one or several models are installed on the server
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_model_installed(
  model,
  check_only = FALSE,
  auto_pull = FALSE,
  server = getOption("rollama_server", default = "http://localhost:11434")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="check_model_installed_+3A_model">model</code></td>
<td>
<p>names of one or several models as character vector.</p>
</td></tr>
<tr><td><code id="check_model_installed_+3A_check_only">check_only</code></td>
<td>
<p>only return TRUE/FALSE and don't download models.</p>
</td></tr>
<tr><td><code id="check_model_installed_+3A_auto_pull">auto_pull</code></td>
<td>
<p>if FALSE, the default, asks before downloading models.</p>
</td></tr>
<tr><td><code id="check_model_installed_+3A_server">server</code></td>
<td>
<p>URL to one or several Ollama servers (not the API). Defaults to
&quot;http://localhost:11434&quot;.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>invisible TRUE/FALSE
</p>

<hr>
<h2 id='create_model'>Create a model from a Modelfile</h2><span id='topic+create_model'></span>

<h3>Description</h3>

<p>Create a model from a Modelfile
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_model(model, modelfile, server = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="create_model_+3A_model">model</code></td>
<td>
<p>name of the model to create</p>
</td></tr>
<tr><td><code id="create_model_+3A_modelfile">modelfile</code></td>
<td>
<p>either a path to a model file to be read or the contents of
the model file as a character vector.</p>
</td></tr>
<tr><td><code id="create_model_+3A_server">server</code></td>
<td>
<p>URL to one or several Ollama servers (not the API). Defaults to
&quot;http://localhost:11434&quot;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Custom models are the way to save your system message and model
parameters in a dedicated shareable way. If you use <code>show_model()</code>, you can
look at the configuration of a model in the column modelfile. To get more
information and a list of valid parameters, check out
<a href="https://github.com/ollama/ollama/blob/main/docs/modelfile.md">https://github.com/ollama/ollama/blob/main/docs/modelfile.md</a>. Most
options are also available through the <code>query</code> and <code>chat</code> functions, yet
are not persistent over sessions.
</p>


<h3>Value</h3>

<p>Nothing. Called to create a model on the Ollama server.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>modelfile &lt;- system.file("extdata", "modelfile.txt", package = "rollama")
## Not run: create_model("mario", modelfile)
modelfile &lt;- "FROM llama3.1\nSYSTEM You are mario from Super Mario Bros."
## Not run: create_model("mario", modelfile)
</code></pre>

<hr>
<h2 id='embed_text'>Generate Embeddings</h2><span id='topic+embed_text'></span>

<h3>Description</h3>

<p>Generate Embeddings
</p>


<h3>Usage</h3>

<pre><code class='language-R'>embed_text(
  text,
  model = NULL,
  server = NULL,
  model_params = NULL,
  verbose = getOption("rollama_verbose", default = interactive())
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="embed_text_+3A_text">text</code></td>
<td>
<p>text vector to generate embeddings for.</p>
</td></tr>
<tr><td><code id="embed_text_+3A_model">model</code></td>
<td>
<p>which model to use. See <a href="https://ollama.com/library">https://ollama.com/library</a> for
options. Default is &quot;llama3.1&quot;. Set option(rollama_model = &quot;modelname&quot;) to
change default for the current session. See <a href="#topic+pull_model">pull_model</a> for more
details.</p>
</td></tr>
<tr><td><code id="embed_text_+3A_server">server</code></td>
<td>
<p>URL to one or several Ollama servers (not the API). Defaults to
&quot;http://localhost:11434&quot;.</p>
</td></tr>
<tr><td><code id="embed_text_+3A_model_params">model_params</code></td>
<td>
<p>a named list of additional model parameters listed in the
<a href="https://github.com/ollama/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values">documentation for the Modelfile</a>.</p>
</td></tr>
<tr><td><code id="embed_text_+3A_verbose">verbose</code></td>
<td>
<p>Whether to print status messages to the Console
(<code>TRUE</code>/<code>FALSE</code>). The default is to have status messages in
interactive sessions. Can be changed with <code>options(rollama_verbose =
  FALSE)</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a tibble with embeddings.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
embed_text(c(
  "Here is an article about llamas...",
  "R is a language and environment for statistical computing and graphics."))

## End(Not run)
</code></pre>

<hr>
<h2 id='list_models'>List models that are available locally.</h2><span id='topic+list_models'></span>

<h3>Description</h3>

<p>List models that are available locally.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>list_models(server = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="list_models_+3A_server">server</code></td>
<td>
<p>URL to one or several Ollama servers (not the API). Defaults to
&quot;http://localhost:11434&quot;.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a tibble of installed models
</p>

<hr>
<h2 id='make_query'>Generate and format queries for a language model</h2><span id='topic+make_query'></span>

<h3>Description</h3>

<p><code>make_query</code> generates structured input for a language model, including
system prompt, user messages, and optional examples (assistant answers).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>make_query(
  text,
  prompt,
  template = "{prefix}{text}\n{prompt}\n{suffix}",
  system = NULL,
  prefix = NULL,
  suffix = NULL,
  examples = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="make_query_+3A_text">text</code></td>
<td>
<p>A character vector of texts to be annotated.</p>
</td></tr>
<tr><td><code id="make_query_+3A_prompt">prompt</code></td>
<td>
<p>A string defining the main task or question to be passed to the
language model.</p>
</td></tr>
<tr><td><code id="make_query_+3A_template">template</code></td>
<td>
<p>A string template for formatting user queries, containing
placeholders like <code>{text}</code>, <code>{prefix}</code>, and <code>{suffix}</code>.</p>
</td></tr>
<tr><td><code id="make_query_+3A_system">system</code></td>
<td>
<p>An optional string to specify a system prompt.</p>
</td></tr>
<tr><td><code id="make_query_+3A_prefix">prefix</code></td>
<td>
<p>A prefix string to prepend to each user query.</p>
</td></tr>
<tr><td><code id="make_query_+3A_suffix">suffix</code></td>
<td>
<p>A suffix string to append to each user query.</p>
</td></tr>
<tr><td><code id="make_query_+3A_examples">examples</code></td>
<td>
<p>A <code>tibble</code> with columns <code>text</code> and <code>answer</code>, representing
example user messages and corresponding assistant responses.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function supports the inclusion of examples, which are
dynamically added to the structured input. Each example follows the same
format as the primary user query.
</p>


<h3>Value</h3>

<p>A list of tibbles, one for each input <code>text</code>, containing structured
rows for system messages, user messages, and assistant responses.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>template &lt;- "{prefix}{text}\n\n{prompt}{suffix}"
examples &lt;- tibble::tribble(
  ~text, ~answer,
  "This movie was amazing, with great acting and story.", "positive",
  "The film was okay, but not particularly memorable.", "neutral",
  "I found this movie boring and poorly made.", "negative"
)
queries &lt;- make_query(
  text = c("A stunning visual spectacle.", "Predictable but well-acted."),
  prompt = "Classify sentiment as positive, neutral, or negative.",
  template = template,
  system = "Provide a sentiment classification.",
  prefix = "Review: ",
  suffix = " Please classify.",
  examples = examples
)
print(queries)
if (ping_ollama()) { # only run this example when Ollama is running
  query(queries, screen = TRUE, output = "text")
}
</code></pre>

<hr>
<h2 id='ping_ollama'>Ping server to see if Ollama is reachable</h2><span id='topic+ping_ollama'></span>

<h3>Description</h3>

<p>Ping server to see if Ollama is reachable
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ping_ollama(server = NULL, silent = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ping_ollama_+3A_server">server</code></td>
<td>
<p>URL to one or several Ollama servers (not the API). Defaults to
&quot;http://localhost:11434&quot;.</p>
</td></tr>
<tr><td><code id="ping_ollama_+3A_silent">silent</code></td>
<td>
<p>suppress warnings and status (only return <code>TRUE</code>/<code>FALSE</code>).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>TRUE if server is running
</p>

<hr>
<h2 id='pull_model'>Pull, show and delete models</h2><span id='topic+pull_model'></span><span id='topic+show_model'></span><span id='topic+delete_model'></span><span id='topic+copy_model'></span>

<h3>Description</h3>

<p>Pull, show and delete models
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pull_model(
  model = NULL,
  server = NULL,
  insecure = FALSE,
  verbose = getOption("rollama_verbose", default = interactive())
)

show_model(model = NULL, server = NULL)

delete_model(model, server = NULL)

copy_model(model, destination = paste0(model, "-copy"), server = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="pull_model_+3A_model">model</code></td>
<td>
<p>name of the model(s). Defaults to &quot;llama3.1&quot; when <code>NULL</code> (except
in <code>delete_model</code>).</p>
</td></tr>
<tr><td><code id="pull_model_+3A_server">server</code></td>
<td>
<p>URL to one or several Ollama servers (not the API). Defaults to
&quot;http://localhost:11434&quot;.</p>
</td></tr>
<tr><td><code id="pull_model_+3A_insecure">insecure</code></td>
<td>
<p>allow insecure connections to the library. Only use this if
you are pulling from your own library during development.</p>
</td></tr>
<tr><td><code id="pull_model_+3A_verbose">verbose</code></td>
<td>
<p>Whether to print status messages to the Console
(<code>TRUE</code>/<code>FALSE</code>). The default is to have status messages in
interactive sessions. Can be changed with <code>options(rollama_verbose = FALSE)</code>.</p>
</td></tr>
<tr><td><code id="pull_model_+3A_destination">destination</code></td>
<td>
<p>name of the copied model.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li> <p><code>pull_model()</code>: downloads model
</p>
</li>
<li> <p><code>show_model()</code>: displays information about a local model
</p>
</li>
<li> <p><code>copy_model()</code>: creates a model with another name from an existing model
</p>
</li>
<li> <p><code>delete_model()</code>: deletes local model
</p>
</li></ul>

<p><strong>Model names</strong>: Model names follow a model:tag format, where model can have
an optional namespace such as example/model. Some examples are
orca-mini:3b-q4_1 and llama3.1:70b. The tag is optional and, if not provided,
will default to latest. The tag is used to identify a specific version.
</p>


<h3>Value</h3>

<p>(invisible) a tibble with information about the model (except in
<code>delete_model</code>)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# download a model and save information in an object
model_info &lt;- pull_model("mixtral")
# after you pull, you can get the same information with:
model_info &lt;- show_model("mixtral")
# pulling models from Hugging Face Hub is also possible
pull_model("https://huggingface.co/oxyapi/oxy-1-small-GGUF:Q2_K")

## End(Not run)
</code></pre>

<hr>
<h2 id='query'>Chat with a LLM through Ollama</h2><span id='topic+query'></span><span id='topic+chat'></span>

<h3>Description</h3>

<p>Chat with a LLM through Ollama
</p>


<h3>Usage</h3>

<pre><code class='language-R'>query(
  q,
  model = NULL,
  screen = TRUE,
  server = NULL,
  images = NULL,
  model_params = NULL,
  output = c("response", "text", "list", "data.frame", "httr2_response", "httr2_request"),
  format = NULL,
  template = NULL,
  verbose = getOption("rollama_verbose", default = interactive())
)

chat(
  q,
  model = NULL,
  screen = TRUE,
  server = NULL,
  images = NULL,
  model_params = NULL,
  template = NULL,
  verbose = getOption("rollama_verbose", default = interactive())
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="query_+3A_q">q</code></td>
<td>
<p>the question as a character string or a conversation object.</p>
</td></tr>
<tr><td><code id="query_+3A_model">model</code></td>
<td>
<p>which model(s) to use. See <a href="https://ollama.com/library">https://ollama.com/library</a> for
options. Default is &quot;llama3.1&quot;. Set <code>option(rollama_model = "modelname")</code> to
change default for the current session. See <a href="#topic+pull_model">pull_model</a> for more
details.</p>
</td></tr>
<tr><td><code id="query_+3A_screen">screen</code></td>
<td>
<p>Logical. Should the answer be printed to the screen.</p>
</td></tr>
<tr><td><code id="query_+3A_server">server</code></td>
<td>
<p>URL to one or several Ollama servers (not the API). Defaults to
&quot;http://localhost:11434&quot;.</p>
</td></tr>
<tr><td><code id="query_+3A_images">images</code></td>
<td>
<p>path(s) to images (for multimodal models such as llava).</p>
</td></tr>
<tr><td><code id="query_+3A_model_params">model_params</code></td>
<td>
<p>a named list of additional model parameters listed in the
<a href="https://github.com/ollama/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values">documentation for the Modelfile</a>
such as temperature. Use a seed and set the temperature to zero to get
reproducible results (see examples).</p>
</td></tr>
<tr><td><code id="query_+3A_output">output</code></td>
<td>
<p>what the function should return. Possible values are
&quot;response&quot;, &quot;text&quot;, &quot;list&quot;, &quot;data.frame&quot;, &quot;httr2_response&quot; or
&quot;httr2_request&quot; see details.</p>
</td></tr>
<tr><td><code id="query_+3A_format">format</code></td>
<td>
<p>the format to return a response in. Currently the only accepted
value is <code>"json"</code>.</p>
</td></tr>
<tr><td><code id="query_+3A_template">template</code></td>
<td>
<p>the prompt template to use (overrides what is defined in the
Modelfile).</p>
</td></tr>
<tr><td><code id="query_+3A_verbose">verbose</code></td>
<td>
<p>Whether to print status messages to the Console
(<code>TRUE</code>/<code>FALSE</code>). The default is to have status messages in
interactive sessions. Can be changed with <code>options(rollama_verbose = FALSE)</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>query</code> sends a single question to the API, without knowledge about
previous questions (only the config message is relevant). <code>chat</code> treats new
messages as part of the same conversation until <a href="#topic+new_chat">new_chat</a> is called.
</p>
<p>To make the output reproducible, you can set a seed with
<code>options(rollama_seed = 42)</code>. As long as the seed stays the same, the
models will give the same answer, changing the seed leads to a different
answer.
</p>
<p>For the output of <code>query</code>, there are a couple of options:
</p>

<ul>
<li> <p><code>response</code>: the response of the Ollama server
</p>
</li>
<li> <p><code>text</code>: only the answer as a character vector
</p>
</li>
<li> <p><code>data.frame</code>: a data.frame containing model and response
</p>
</li>
<li> <p><code>list</code>: a list containing the prompt to Ollama and the response
</p>
</li>
<li> <p><code>httr2_response</code>: the response of the Ollama server including HTML
headers in the <code>httr2</code> response format
</p>
</li>
<li> <p><code>httr2_request</code>: httr2_request objects in a list, in case you want to run
them with <code><a href="httr2.html#topic+req_perform">httr2::req_perform()</a></code>, <code><a href="httr2.html#topic+req_perform_sequential">httr2::req_perform_sequential()</a></code>, or
<code><a href="httr2.html#topic+req_perform_parallel">httr2::req_perform_parallel()</a></code> yourself.
</p>
</li></ul>



<h3>Value</h3>

<p>list of objects set in output parameter.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
#' # ask a single question
query("why is the sky blue?")

# hold a conversation
chat("why is the sky blue?")
chat("and how do you know that?")

# save the response to an object and extract the answer
resp &lt;- query(q = "why is the sky blue?")
answer &lt;- resp[[1]]$message$content

# or just get the answer directly
answer &lt;- query(q = "why is the sky blue?", output = "text")

# ask question about images (to a multimodal model)
images &lt;- c("https://avatars.githubusercontent.com/u/23524101?v=4", # remote
            "/path/to/your/image.jpg") # or local images supported
query(q = "describe these images",
      model = "llava",
      images = images[1]) # just using the first path as the second is not real

# set custom options for the model at runtime (rather than in create_model())
query("why is the sky blue?",
      model_params = list(
        num_keep = 5,
        seed = 42,
        num_predict = 100,
        top_k = 20,
        top_p = 0.9,
        min_p = 0.0,
        tfs_z = 0.5,
        typical_p = 0.7,
        repeat_last_n = 33,
        temperature = 0.8,
        repeat_penalty = 1.2,
        presence_penalty = 1.5,
        frequency_penalty = 1.0,
        mirostat = 1,
        mirostat_tau = 0.8,
        mirostat_eta = 0.6,
        penalize_newline = TRUE,
        numa = FALSE,
        num_ctx = 1024,
        num_batch = 2,
        num_gpu = 0,
        main_gpu = 0,
        low_vram = FALSE,
        vocab_only = FALSE,
        use_mmap = TRUE,
        use_mlock = FALSE,
        num_thread = 8
      ))

# use a seed to get reproducible results
query("why is the sky blue?", model_params = list(seed = 42))

# to set a seed for the whole session you can use
options(rollama_seed = 42)

# this might be interesting if you want to turn off the GPU and load the
# model into the system memory (slower, but most people have more RAM than
# VRAM, which might be interesting for larger models)
query("why is the sky blue?",
       model_params = list(num_gpu = 0))

# Asking the same question to multiple models is also supported
query("why is the sky blue?", model = c("llama3.1", "orca-mini"))

# And if you have multiple Ollama servers in your network, you can send
# requests to them in parallel
if (ping_ollama(c("http://localhost:11434/",
                  "http://192.168.2.45:11434/"))) { # check if servers are running
  query("why is the sky blue?", model = c("llama3.1", "orca-mini"),
        server = c("http://localhost:11434/",
                   "http://192.168.2.45:11434/"))
}

</code></pre>

<hr>
<h2 id='rollama-options'>rollama Options</h2><span id='topic+rollama-options'></span>

<h3>Description</h3>

<p>The behaviour of <code>rollama</code> can be controlled through
<code>options()</code>. Specifically, the options below can be set.
</p>


<h3>Details</h3>


<dl>
<dt>rollama_server</dt><dd>
<p>This controls the default server where Ollama is expected to run. It assumes
that you are running Ollama locally in a Docker container.
</p>
<dl>
<dt>default:</dt><dd><p><code>"http://localhost:11434"</code></p>
</dd>
</dl>
</dd>
<dt>rollama_model</dt><dd>
<p>The default model is llama3.1, which is a good overall option with reasonable
performance and size for most tasks. You can change the model in each
function call or globally with this option.
</p>
<dl>
<dt>default:</dt><dd><p><code>"llama3.1"</code></p>
</dd>
</dl>
</dd>
<dt>rollama_verbose</dt><dd>
<p>Whether the package tells users what is going on, e.g., showing a spinner
while the models are thinking or showing the download speed while pulling
models. Since this adds some complexity to the code, you might want to
disable it when you get errors (it won't fix the error, but you get a
better error trace).
</p>
<dl>
<dt>default:</dt><dd><p><code>TRUE</code></p>
</dd>
</dl>
</dd>
<dt>rollama_config</dt><dd>
<p>The default configuration or system message. If NULL, the system message
defined in the used model is employed.
</p>
<dl>
<dt>default:</dt><dd><p>None</p>
</dd>
</dl>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>options(rollama_config = "You make answers understandable to a 5 year old")
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
