<!DOCTYPE html><html><head><title>Help for package DDRTree</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {DDRTree}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#DDRTree'><p>Perform DDRTree construction</p></a></li>
<li><a href='#get_major_eigenvalue'><p>Get the top L eigenvalues</p></a></li>
<li><a href='#pca_projection_R'><p>Compute the PCA projection</p></a></li>
<li><a href='#sqdist_R'><p>calculate the square distance between a, b</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Learning Principal Graphs with DDRTree</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.5</td>
</tr>
<tr>
<td>Date:</td>
<td>2017-4-14</td>
</tr>
<tr>
<td>Author:</td>
<td>Xiaojie Qiu, Cole Trapnell, Qi Mao, Li Wang</td>
</tr>
<tr>
<td>Depends:</td>
<td>irlba</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppEigen, BH</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Xiaojie Qiu &lt;xqiu@uw.edu&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Provides an implementation of the framework of reversed graph embedding (RGE) which projects data into a reduced dimensional space while constructs a principal tree which passes through the middle of the data simultaneously. DDRTree shows superiority to alternatives (Wishbone, DPT) for inferring the ordering as well as the intrinsic structure of the single cell genomics data. In general, it could be used to reconstruct the temporal progression as well as bifurcation structure of any datatype. </td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/Artistic-2.0">Artistic License 2.0</a></td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>6.0.1</td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>C++11</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2017-04-14 17:38:16 UTC; xqiu</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2017-04-30 20:54:17 UTC</td>
</tr>
</table>
<hr>
<h2 id='DDRTree'>Perform DDRTree construction</h2><span id='topic+DDRTree'></span><span id='topic+DDRTree'></span><span id='topic+DDRTree-package'></span><span id='topic+DDRTree-package'></span>

<h3>Description</h3>

<p>Perform DDRTree construction
</p>
<p>This is an R and C code implementation of the DDRTree algorithm from Qi Mao, Li Wang et al. <br /> <br />
Qi Mao, Li Wang, Steve Goodison, and Yijun Sun. Dimensionality Reduction via Graph Structure Learning.
The 21st ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD'15), 2015 <br />
<a href="http://dl.acm.org/citation.cfm?id=2783309">http://dl.acm.org/citation.cfm?id=2783309</a> <br /> <br />
to perform dimension reduction and principal graph
learning simultaneously. Please cite this package and KDD'15 paper if you found DDRTree is useful for your research.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>DDRTree(X, dimensions = 2, initial_method = NULL, maxIter = 20,
  sigma = 0.001, lambda = NULL, ncenter = NULL, param.gamma = 10,
  tol = 0.001, verbose = F, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="DDRTree_+3A_x">X</code></td>
<td>
<p>a matrix with <code class="reqn">\mathbf{D \times N}</code> dimension which is needed to perform DDRTree construction</p>
</td></tr>
<tr><td><code id="DDRTree_+3A_dimensions">dimensions</code></td>
<td>
<p>reduced dimension</p>
</td></tr>
<tr><td><code id="DDRTree_+3A_initial_method">initial_method</code></td>
<td>
<p>a function to take the data transpose of X as input and then output the reduced dimension,
row number should not larger than observation and column number should not be larger than variables (like isomap may only
return matrix on valid sample sets). Sample names of returned reduced dimension should be preserved.</p>
</td></tr>
<tr><td><code id="DDRTree_+3A_maxiter">maxIter</code></td>
<td>
<p>maximum iterations</p>
</td></tr>
<tr><td><code id="DDRTree_+3A_sigma">sigma</code></td>
<td>
<p>bandwidth parameter</p>
</td></tr>
<tr><td><code id="DDRTree_+3A_lambda">lambda</code></td>
<td>
<p>regularization parameter for inverse graph embedding</p>
</td></tr>
<tr><td><code id="DDRTree_+3A_ncenter">ncenter</code></td>
<td>
<p>number of nodes allowed in the regularization graph</p>
</td></tr>
<tr><td><code id="DDRTree_+3A_param.gamma">param.gamma</code></td>
<td>
<p>regularization parameter for k-means (the prefix of 'param' is used to avoid name collision with gamma)</p>
</td></tr>
<tr><td><code id="DDRTree_+3A_tol">tol</code></td>
<td>
<p>relative objective difference</p>
</td></tr>
<tr><td><code id="DDRTree_+3A_verbose">verbose</code></td>
<td>
<p>emit extensive debug output</p>
</td></tr>
<tr><td><code id="DDRTree_+3A_...">...</code></td>
<td>
<p>additional arguments passed to DDRTree</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list with W, Z, stree, Y, history
W is the orthogonal set of d (dimensions) linear basis vector
Z is the reduced dimension space
stree is the smooth tree graph embedded in the low dimension space
Y represents latent points as the center of Z
</p>


<h3>Introduction</h3>

<p>The unprecedented increase in big-data causes a huge difficulty in data visualization and downstream analysis.
Conventional dimension reduction approaches (for example, PCA, ICA, Isomap, LLE, etc.) are limited in their ability
to explictly recover the intrinisic structure from the data as well as the discriminative feature representation,
both are important for scientific discovery. The DDRTree algorithm is a new algorithm to perform the following
three tasks in one setting: <br />
<br />
1. Reduce high dimension data into a low dimension space <br />
<br />
2. Recover an explicit smooth graph structure with local geometry only captured by distances of data
points in the low dimension space. <br />
<br />
3. Obtain clustering structures of data points in reduced dimension <br />
</p>


<h3>Dimensionality reduction via graph structure learning</h3>

<p>Reverse graph embedding is previously applied to learn the intrinisic graph structure in the original dimension.
The optimization of graph inference can be represented as:
</p>
<p style="text-align: center;"><code class="reqn">\mathop{min}_{f_g \in \mathcal{F}} \mathop{min}_{\{\mathbf{z}_1, ..., \mathbf{z}_M\}} \sum_{(V_i, V_j) \in
\mathcal{E}} b_{i,j}||f_g(\mathbf{z}_i) - f_g(\mathbf{z}_j)||^2</code>
</p>
<p> where
<code class="reqn">f_g</code> is a function to map the instrinsic data space <code class="reqn">\mathcal{Z} = \{\mathbf{z}_1, ..., \mathbf{z}_M\}</code> back to the input data space (reverse embedding)
<code class="reqn">\mathcal{X} = \{ \mathbf{x}_1, ..., \mathbf{x}_N\}</code>. <code class="reqn">V_i</code> is the the vertex of the instrinsic undirected graph
<code class="reqn">\mathcal{G} = (\mathcal{V}, \mathcal{E})</code>. <code class="reqn">b_{ij}</code> is
the edge weight associates with the edge set <code class="reqn">\mathcal{E}</code>.
In order to learn the intrinsic structure from a reduced dimension, we need also to consider a term which includes
the error during the learning of the instrinsic structure. This strategy is incorporated as the following:
</p>
<p style="text-align: center;"><code class="reqn">\mathop{min}_{\mathcal{G} \in \hat{\mathcal{G}}_b}\mathop{min}_{f_g \in \mathcal{F}} \mathop{min}_{\{\mathbf{z}_1, ...,
\mathbf{z}_M\}} \sum_{i = 1}^N ||\mathbf{x}_i - f_g (\mathbf{z}_i)||^2 + \frac{\lambda}{2} \sum_{(V_i, V_j) \in \mathcal{E}}
b_{i,j}||f_g(\mathbf{z}_i) - f_g(\mathbf{z}_j)||^2</code>
</p>
<p> where <code class="reqn">\lambda</code> is a non-negative parameter which controls the tradeoff between the data
reconstruction error and the reverse graph embedding.
</p>


<h3>Dimensionality reduction via learning a tree</h3>

<p>The general framework for reducing dimension by learning an intrinsic structure in a low dimension requires a feasible set
<code class="reqn">\hat{\mathcal{G}}_b</code> of graph and a mapping function <code class="reqn">f_\mathcal{G}</code>. The algorithm uses minimum spanning tree as the feasible tree
graph structure, which can be solved by Kruskal' algoritm. A linear projection model <code class="reqn">f_g (\mathbf{z}) = \mathbf{Wz}</code> is used as the mapping function.
Those setting results in the following specific form for the previous framework:
</p>
<p style="text-align: center;"><code class="reqn">\mathop{min}_{\mathbf{W}, \mathbf{Z}, \mathbf{B}} \sum_{i = 1}^N ||\mathbf{x}_i  - \mathbf{W}\mathbf{z}_i||^2 + \frac{\lambda}{2} \sum_{i,j}b_{i,j}||\mathbf{W} \mathbf{z}_i - \mathbf{W} \mathbf{z}_j||^2</code>
</p>

<p>where <code class="reqn">\mathbf{W} = [\mathbf{w}_1, ..., \mathbf{w}_d] \in
\mathcal{R}^{D \times d}</code> is an orthogonal set of <code class="reqn">d</code> linear basis vectors. We can group tree graph <code class="reqn">\mathbf{B}</code>, the orthogonal set of linear basis vectors
and projected points in reduced dimension <code class="reqn">\mathbf{W}, \mathbf{Z}</code> as two groups and apply alternative structure optimization to optimize the tree graph.
This method is defined as DRtree (Dimension Reduction tree) as discussed by the authors.
</p>


<h3>Discriminative dimensionality reduction via learning a tree</h3>

<p>In order to avoid the issues where data points scattered into different branches (which leads to lose of cluster information) and to
incorporate the discriminative information,another set of points <code class="reqn">\{\mathbf{y}_k\}_{k = 1}^K</code> as the centers of <code class="reqn">\{\mathbf{z}_i\}^N_{i = 1}</code> can be also introduced.
By so doing, the objective functions of K-means and the DRtree can be simulatenously minimized. The author further proposed a soft partition method
to account for the limits from K-means and proposed the following objective function:
</p>
<p style="text-align: center;"><code class="reqn">\mathop{min}_{\mathbf{W}, \mathbf{Z}, \mathbf{B}, \mathbf{Y}, \mathbf{R}} \sum_{i = 1}^N ||\mathbf{x}_i - \mathbf{W} \mathbf{z}_i||^2 +
\frac{\lambda}{2} \sum_{k, k'}b_{k, k'}||\mathbf{W} \mathbf{y}_k - \mathbf{W} \mathbf{y}_k'||^2 +
\gamma\Big[\sum_{k = 1}^K \sum_{i = 1}^N r_{i, k} ||\mathbf{z}_i - \mathbf{y}_k||^2 + \sigma \Omega (\mathbf{R})\Big]</code>
</p>

<p style="text-align: center;"><code class="reqn">s.t.\ \mathbf{W}^T \mathbf{W} = \mathbf{I}, \mathbf{B} \in \mathcal{B}, \sum_{k = 1}^K r_{i, k} = 1,
r_{i, k} \leq 0, \forall i, \forall k</code>
</p>
<p> where <code class="reqn">\mathbf{R} \in \mathcal{R}^{N \times N}, \Omega(\mathbf{R}) = \sum_{i = 1}^N \sum_{k = 1}^k r_{i, k} log\ r_{i, k}</code> is the negative
entropy regularization which transforms the hard assignments used in K-means into soft assignments and <code class="reqn">\sigma &gt; 0</code> is the regulization parameter.
Alternative structure optimization is again used to solve the above problem by separately optimize each group <code class="reqn">{\mathbf{W}, \mathbf{Z}, \mathbf{Y}}, {\mathbf{B}, \mathbf{R}}</code> until convergence.
</p>


<h3>The actual algorithm of DDRTree</h3>

<p><code class="reqn">1.</code> <code class="reqn">\mathbf{Input}</code>: Data matrix <code class="reqn">\mathbf{X}</code>, parameters <code class="reqn">\lambda, \sigma, \gamma</code> <br />
<code class="reqn">2.</code> Initialize <code class="reqn">\mathbf{Z}</code> by PCA <br />
<code class="reqn">3.</code> <code class="reqn">K = N, \mathbf{Y} = \mathbf{Z}</code> <br />
<code class="reqn">4.</code> <code class="reqn">\mathbf{repeat}</code>: <br />
<code class="reqn">\    5.</code>  <code class="reqn">d_{k,k'} = ||\mathbf{y}_k - \mathbf{y}_{k'}||^2, \forall k, \forall k'</code> <br />
<code class="reqn">\     6.</code>   Obtain <code class="reqn">\mathbf{B}</code> via Kruskal's algorithm <br />
<code class="reqn">\     7.</code>   <code class="reqn">\mathbf{L} = diag(\mathbf{B1}) - \mathbf{B}</code> <br />
<code class="reqn">\     8.</code>   Compute <code class="reqn">\mathbf{R}</code> with each element <br />
<code class="reqn">\     9.</code>   <code class="reqn">\tau = diag(\mathbf{1}^T \mathbf{R})</code> <br />
<code class="reqn">\     10.</code>  <code class="reqn">\mathbf{Q} = \frac{1}{\mathbf{1} + \gamma} \Big[\mathbf{I} + \mathbf{R} (\frac{1 + \gamma}{\gamma}(\frac{\lambda}{\gamma} \mathbf{L} +
    \tau) - \mathbf{R}^T \mathbf{R})^{-1} \mathbf{R}^T\Big]</code> <br />
<code class="reqn">\     11.</code>  <code class="reqn">\mathbf{C} = \mathbf{X Q X}^T</code> <br />
<code class="reqn">\     12.</code>  Perform eigen-decomposition on <code class="reqn">\mathbf{C}</code> such that <code class="reqn">\mathbf{C} = \mathbf{U} \wedge \mathbf{U}^T</code> and <code class="reqn">diag(\wedge)</code> is sorted in a descending order <br />
<code class="reqn">\     13.</code> <code class="reqn">\mathbf{W} = \mathbf{U}(:, 1:d)</code> <br />
<code class="reqn">\     14.</code> <code class="reqn">\mathbf{Z} = \mathbf{W}^T \mathbf{X Q}</code> <br />
<code class="reqn">\     15.</code> <code class="reqn">\mathbf{Y} = \mathbf{Z R} (\frac{\lambda}{\gamma} \mathbf{L} + \tau)^{-1}</code> <br />
<code class="reqn">16.</code> <code class="reqn">\mathbf{Until}</code> Convergence <br />
</p>


<h3>Implementation of DDRTree algorithm</h3>

<p>We implemented the algorithm mostly in Rcpp for the purpose of efficiency. It also has extensive optimization
for sparse input data. This implementation is originally based on the matlab code provided from the author of DDRTree paper.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data('iris')
subset_iris_mat &lt;- as.matrix(t(iris[c(1, 2, 52, 103), 1:4])) #subset the data
#run DDRTree with ncenters equal to species number
DDRTree_res &lt;- DDRTree(subset_iris_mat, dimensions = 2, maxIter = 5, sigma = 1e-2,
lambda = 1, ncenter = 3, param.gamma = 10, tol = 1e-2, verbose = FALSE)
Z &lt;- DDRTree_res$Z #obatain matrix
Y &lt;- DDRTree_res$Y
stree &lt;- DDRTree_res$stree
plot(Z[1, ], Z[2, ], col = iris[c(1, 2, 52, 103), 'Species']) #reduced dimension
legend("center", legend = unique(iris[c(1, 2, 52, 103), 'Species']), cex=0.8,
col=unique(iris[c(1, 2, 52, 103), 'Species']), pch = 1) #legend
title(main="DDRTree reduced dimension", col.main="red", font.main=4)
dev.off()
plot(Y[1, ], Y[2, ], col = 'blue', pch = 17) #center of the Z
title(main="DDRTree smooth principal curves", col.main="red", font.main=4)

#run DDRTree with ncenters equal to species number
DDRTree_res &lt;- DDRTree(subset_iris_mat, dimensions = 2, maxIter = 5, sigma = 1e-3,
lambda = 1, ncenter = NULL,param.gamma = 10, tol = 1e-2, verbose = FALSE)
Z &lt;- DDRTree_res$Z #obatain matrix
Y &lt;- DDRTree_res$Y
stree &lt;- DDRTree_res$stree
plot(Z[1, ], Z[2, ], col = iris[c(1, 2, 52, 103), 'Species']) #reduced dimension
legend("center", legend = unique(iris[c(1, 2, 52, 103), 'Species']), cex=0.8,
col=unique(iris[c(1, 2, 52, 103), 'Species']), pch = 1) #legend
title(main="DDRTree reduced dimension", col.main="red", font.main=4)
dev.off()
plot(Y[1, ], Y[2, ], col = 'blue', pch = 2) #center of the Z
title(main="DDRTree smooth principal graphs", col.main="red", font.main=4)
</code></pre>

<hr>
<h2 id='get_major_eigenvalue'>Get the top L eigenvalues</h2><span id='topic+get_major_eigenvalue'></span>

<h3>Description</h3>

<p>Get the top L eigenvalues
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_major_eigenvalue(C, L)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_major_eigenvalue_+3A_c">C</code></td>
<td>
<p>data matrix used for eigendecomposition</p>
</td></tr>
<tr><td><code id="get_major_eigenvalue_+3A_l">L</code></td>
<td>
<p>number for the top eigenvalues</p>
</td></tr>
</table>

<hr>
<h2 id='pca_projection_R'>Compute the PCA projection</h2><span id='topic+pca_projection_R'></span>

<h3>Description</h3>

<p>Compute the PCA projection
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pca_projection_R(C, L)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pca_projection_R_+3A_c">C</code></td>
<td>
<p>data matrix used for PCA projection</p>
</td></tr>
<tr><td><code id="pca_projection_R_+3A_l">L</code></td>
<td>
<p>number for the top principal components</p>
</td></tr>
</table>

<hr>
<h2 id='sqdist_R'>calculate the square distance between a, b</h2><span id='topic+sqdist_R'></span>

<h3>Description</h3>

<p>calculate the square distance between a, b
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sqdist_R(a, b)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sqdist_R_+3A_a">a</code></td>
<td>
<p>a matrix with <code class="reqn">D \times N</code> dimension</p>
</td></tr>
<tr><td><code id="sqdist_R_+3A_b">b</code></td>
<td>
<p>a matrix with <code class="reqn">D \times N</code> dimension</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a numeric value for the different between a and b
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
