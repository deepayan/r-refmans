<!DOCTYPE html><html><head><title>Help for package Buddle</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {Buddle}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#CheckNonNumeric'><p>Detecting Non-numeric Values.</p></a></li>
<li><a href='#FetchBuddle'><p>Predicting Classification and Regression.</p></a></li>
<li><a href='#GetPrecision'><p>Obtaining Accuracy.</p></a></li>
<li><a href='#MakeConfusionMatrix'><p>Making a Confusion Matrix.</p></a></li>
<li><a href='#mnist_data'><p>Image data of handwritten digits.</p></a></li>
<li><a href='#OneHot2Label'><p>Obtaining Labels</p></a></li>
<li><a href='#Split2TrainTest'><p>Splitting Data into Training and Test Sets.</p></a></li>
<li><a href='#TrainBuddle'><p>Implementing Statistical Classification and Regression.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>A Deep Learning for Statistical Classification and Regression
Analysis with Random Effects</td>
</tr>
<tr>
<td>Version:</td>
<td>2.0.1</td>
</tr>
<tr>
<td>Date:</td>
<td>2020-02-04</td>
</tr>
<tr>
<td>Author:</td>
<td>Jiwoong Kim &lt;jwboys26 at gmail.com&gt;</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Jiwoong Kim &lt;jwboys26@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Statistical classification and regression have been popular among various fields and stayed in the limelight of scientists of those fields. Examples of the fields include clinical trials where the statistical classification of patients is indispensable to predict the clinical courses of diseases. Considering the negative impact of diseases on performing daily tasks, correctly classifying patients based on the clinical information is vital in that we need to identify patients of the high-risk group to develop a severe state and arrange medical treatment for them at an opportune moment. Deep learning - a part of artificial intelligence - has gained much attention, and research on it burgeons during past decades: see, e.g, Kazemi and Mirroshandel (2018) &lt;<a href="https://doi.org/10.1016%2Fj.artmed.2017.12.001">doi:10.1016/j.artmed.2017.12.001</a>&gt;. It is a veritable technique which was originally designed for the classification, and hence, the Buddle package can provide sublime solutions to various challenging classification and regression problems encountered in the clinical trials. The Buddle package is based on the back-propagation algorithm - together with various powerful techniques such as batch normalization and dropout - which performs a multi-layer feed-forward neural network: see Krizhevsky et. al (2017) &lt;<a href="https://doi.org/10.1145%2F3065386">doi:10.1145/3065386</a>&gt;, Schmidhuber (2015) &lt;<a href="https://doi.org/10.1016%2Fj.neunet.2014.09.003">doi:10.1016/j.neunet.2014.09.003</a>&gt; and LeCun et al. (1998) &lt;<a href="https://doi.org/10.1109%2F5.726791">doi:10.1109/5.726791</a>&gt; for more details. This package contains two main functions: TrainBuddle() and FetchBuddle(). TrainBuddle() builds a feed-forward neural network model and trains the model. FetchBuddle() recalls the trained model which is the output of TrainBuddle(), classifies or regresses given data, and make a final prediction for the data.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>LazyData:</td>
<td>TRUE</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp (&ge; 0.12.17), plyr, stats, graphics</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.0.2</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2020-02-11 13:27:23 UTC; Jason</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2020-02-13 09:30:10 UTC</td>
</tr>
</table>
<hr>
<h2 id='CheckNonNumeric'>Detecting Non-numeric Values.</h2><span id='topic+CheckNonNumeric'></span>

<h3>Description</h3>

<p>Check whether or not an input matrix includes any non-numeric values (NA, NULL, &quot;&quot;, character, etc) before being used for training. If any non-numeric values exist, then TrainBuddle() or FetchBuddle() will return non-numeric results.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CheckNonNumeric(X)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CheckNonNumeric_+3A_x">X</code></td>
<td>
<p>an n-by-p matrix.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of (n+1) values where n is the number of non-numeric values. The first element of the list is n, and all other elements are entries of X where non-numeric values occur. For example, when the (1,1)th and the (2,3)th entries of a 5-by-5 matrix X are non-numeric, then the list returned by CheckNonNumeric() will contain 2, (1,1), and (2,3).
</p>


<h3>See Also</h3>

<p>GetPrecision(), FetchBuddle(), MakeConfusionMatrix(), OneHot2Label(), Split2TrainTest(), TrainBuddle()
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
n = 5;
p = 5;
X = matrix(0, n, p)       #### Generate a 5-by-5 matrix which includes two NA's. 
X[1,1] = NA
X[2,3] = NA

lst = CheckNonNumeric(X)

lst

</code></pre>

<hr>
<h2 id='FetchBuddle'>Predicting Classification and Regression.</h2><span id='topic+FetchBuddle'></span>

<h3>Description</h3>

<p>Yield prediction (softmax value or value) for regression and classification for given data based on the results of training.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>FetchBuddle(X, lW, lb, lParam)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="FetchBuddle_+3A_x">X</code></td>
<td>
<p>a matrix of real values which will be used for predicting classification or regression.</p>
</td></tr>
<tr><td><code id="FetchBuddle_+3A_lw">lW</code></td>
<td>
<p>a list of weight matrices obtained after training.</p>
</td></tr>
<tr><td><code id="FetchBuddle_+3A_lb">lb</code></td>
<td>
<p>a list of bias vectors obtained after training.</p>
</td></tr>
<tr><td><code id="FetchBuddle_+3A_lparam">lParam</code></td>
<td>
<p>a list of parameters used for training. It includes: label, hiddenlayer, batch, drop, drop.ratio, lr, init.weight, activation, optim, type, rand.eff, distr, and disp.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of the following values: 
</p>

<dl>
<dt>predicted</dt><dd><p>predicted real values (regression) or softmax values (classification).</p>
</dd>
<dt>One.Hot.Encoding</dt><dd><p>one-hot encoding values of the predicted softmax values for classification. For regression, a zero matrix will be returned. To convert the one-hot encoding values to labels, use OneHot2Label().</p>
</dd>
</dl>



<h3>References</h3>

<p>[1] Geron, A. Hand-On Machine Learning with Scikit-Learn and TensorFlow. Sebastopol: O'Reilly, 2017. Print.
</p>
<p>[2] Han, J., Pei, J, Kamber, M. Data Mining: Concepts and Techniques. New York: Elsevier, 2011. Print.
</p>
<p>[3] Weilman, S. Deep Learning from Scratch. O'Reilly Media, 2019. Print.
</p>


<h3>See Also</h3>

<p>CheckNonNumeric(), GetPrecision(), MakeConfusionMatrix(), OneHot2Label(), Split2TrainTest(), TrainBuddle()
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
### Using mnist data again

data(mnist_data)

X1 = mnist_data$Images       ### X1: 100 x 784 matrix
Y1 = mnist_data$Labels       ### Y1: 100 x 1 vector



############################# Train Buddle

lst = TrainBuddle(Y1, X1, train.ratio=0.6, arrange=TRUE, batch.size=10, total.iter = 100, 
                 hiddenlayer=c(20, 10), batch.norm=TRUE, drop=TRUE, 
                 drop.ratio=0.1, lr=0.1, init.weight=0.1, 
                 activation=c("Relu","SoftPlus"), optim="AdaGrad", 
                 type = "Classification", rand.eff=TRUE, distr = "Logistic", disp=TRUE)

lW = lst[[1]]
lb = lst[[2]]
lParam = lst[[3]]


X2 = matrix(rnorm(20*784,0,1), 20,784)  ## Genderate a 20-by-784 matrix

lst = FetchBuddle(X2, lW, lb, lParam)   ## Pass X2 to FetchBuddle for prediction





</code></pre>

<hr>
<h2 id='GetPrecision'>Obtaining Accuracy.</h2><span id='topic+GetPrecision'></span>

<h3>Description</h3>

<p>Compute measures of accuracy such as precision, recall, and F1 from a given confusion matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>GetPrecision(confusion.matrix)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="GetPrecision_+3A_confusion.matrix">confusion.matrix</code></td>
<td>
<p>a confusion matrix.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An (r+1)-by-3 matrix when the input is an r-by-r confusion matrix.
</p>


<h3>See Also</h3>

<p>CheckNonNumeric(), FetchBuddle(), MakeConfusionMatrix(), OneHot2Label(), Split2TrainTest(), TrainBuddle()
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(iris)

Label = c("setosa", "versicolor", "virginica")

predicted.label = c("setosa", "setosa",    "virginica", "setosa", "versicolor", "versicolor")
true.label      = c("setosa", "virginica", "versicolor","setosa", "versicolor", "virginica")

confusion.matrix = MakeConfusionMatrix(predicted.label, true.label, Label)
precision = GetPrecision(confusion.matrix)

confusion.matrix
precision


</code></pre>

<hr>
<h2 id='MakeConfusionMatrix'>Making a Confusion Matrix.</h2><span id='topic+MakeConfusionMatrix'></span>

<h3>Description</h3>

<p>Create a confusion matrix from two vectors of labels: predicted label obtained from FetchBuddle() as a result of prediction and true label of a test set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MakeConfusionMatrix(predicted.label, true.label, Label)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MakeConfusionMatrix_+3A_predicted.label">predicted.label</code></td>
<td>
<p>a vector of predicted labels.</p>
</td></tr>
<tr><td><code id="MakeConfusionMatrix_+3A_true.label">true.label</code></td>
<td>
<p>a vector of true labels.</p>
</td></tr>
<tr><td><code id="MakeConfusionMatrix_+3A_label">Label</code></td>
<td>
<p>a vector of all possible values or levels which a label can take.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An r-by-r confusion matrix where r is the length of Label.
</p>


<h3>See Also</h3>

<p>CheckNonNumeric(), GetPrecision(), FetchBuddle(), OneHot2Label(), Split2TrainTest(), TrainBuddle()
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

data(iris)

Label = c("setosa", "versicolor", "virginica")

predicted.label = c("setosa", "setosa",    "virginica", "setosa", "versicolor", "versicolor")
true.label      = c("setosa", "virginica", "versicolor","setosa", "versicolor", "virginica")

confusion.matrix = MakeConfusionMatrix(predicted.label, true.label, Label)
precision = GetPrecision(confusion.matrix)

confusion.matrix
precision





</code></pre>

<hr>
<h2 id='mnist_data'>Image data of handwritten digits.</h2><span id='topic+mnist_data'></span>

<h3>Description</h3>

<p>A dataset containing 100 images of handwritten digits.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(mnist_data)
</code></pre>


<h3>Details</h3>

<p>#'@format A list containing a matrix of image data and a vector of labels:
</p>

<dl>
<dt>Images</dt><dd><p>100-by-784 matrix of image data of handwritten digits.</p>
</dd>
<dt>Labels</dt><dd><p>100-by-1 vector of labels of handwritten digits.</p>
</dd>
</dl>



<h3>Source</h3>

<p><a href="http://yann.lecun.com/exdb/mnist/">http://yann.lecun.com/exdb/mnist/</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(mnist_data)

Img_Mat = mnist_data$Images
Img_Label = mnist_data$Labels

digit_data = Img_Mat[1, ]      ### image data (784-by-1 vector) of the first handwritten digit (=5) 
label = Img_Label[1]           ### label of the first handwritten digit (=5)
imgmat = matrix(digit_data, 28, 28)    ### transform the vector of image data to a matrix 
   

</code></pre>

<hr>
<h2 id='OneHot2Label'>Obtaining Labels</h2><span id='topic+OneHot2Label'></span>

<h3>Description</h3>

<p>Convert a one-hot encoding matrix to a vector of labels.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>OneHot2Label(OHE, Label)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="OneHot2Label_+3A_ohe">OHE</code></td>
<td>
<p>an r-by-n one-hot encoding matrix.</p>
</td></tr>
<tr><td><code id="OneHot2Label_+3A_label">Label</code></td>
<td>
<p>an r-by-1 vector of values or levels which a label can take.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An n-by-1 vector of labels.
</p>


<h3>See Also</h3>

<p>CheckNonNumeric(), GetPrecision(), FetchBuddle(), MakeConfusionMatrix(), Split2TrainTest(), TrainBuddle()
</p>

<hr>
<h2 id='Split2TrainTest'>Splitting Data into Training and Test Sets.</h2><span id='topic+Split2TrainTest'></span>

<h3>Description</h3>

<p>Convert data into training and test sets so that the training set contains approximately the specified ratio of all labels.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Split2TrainTest(Y, X, train.ratio)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Split2TrainTest_+3A_y">Y</code></td>
<td>
<p>an n-by-1 vector of responses or labels.</p>
</td></tr>
<tr><td><code id="Split2TrainTest_+3A_x">X</code></td>
<td>
<p>an n-by-p design matrix of predictors.</p>
</td></tr>
<tr><td><code id="Split2TrainTest_+3A_train.ratio">train.ratio</code></td>
<td>
<p>a ratio of the size of the resulting training set to the size of data.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of the following values:
</p>

<dl>
<dt>y.train</dt><dd><p>the training set of Y.</p>
</dd>
<dt>y.test</dt><dd><p>the test set of Y.</p>
</dd>
<dt>x.train</dt><dd><p>the training set of X.</p>
</dd>
<dt>x.test</dt><dd><p>the test set of X.</p>
</dd>
</dl>



<h3>See Also</h3>

<p>CheckNonNumeric(), GetPrecision(), FetchBuddle(), MakeConfusionMatrix(), OneHot2Label(), TrainBuddle()
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(iris)

Label = c("setosa", "versicolor", "virginica")


train.ratio=0.8
Y = iris$Species 
X = cbind( iris$Sepal.Length, iris$Sepal.Width, iris$Petal.Length, iris$Petal.Width)

lst = Split2TrainTest(Y, X, train.ratio)

Ytrain = lst$y.train
Ytest = lst$y.test

length(Ytrain)
length(Ytest)

length(which(Ytrain==Label[1]))
length(which(Ytrain==Label[2]))
length(which(Ytrain==Label[3]))

length(which(Ytest==Label[1]))
length(which(Ytest==Label[2]))
length(which(Ytest==Label[3]))




</code></pre>

<hr>
<h2 id='TrainBuddle'>Implementing Statistical Classification and Regression.</h2><span id='topic+TrainBuddle'></span>

<h3>Description</h3>

<p>Build a multi-layer feed-forward neural network model for statistical classification and regression analysis with random effects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>TrainBuddle(
  formula.string,
  data,
  train.ratio = 0.7,
  arrange = 0,
  batch.size = 10,
  total.iter = 10000,
  hiddenlayer = c(100),
  batch.norm = TRUE,
  drop = TRUE,
  drop.ratio = 0.1,
  lr = 0.1,
  init.weight = 0.1,
  activation = c("Sigmoid"),
  optim = "SGD",
  type = "Classification",
  rand.eff = FALSE,
  distr = "Normal",
  disp = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="TrainBuddle_+3A_formula.string">formula.string</code></td>
<td>
<p>a formula string or a vector of numeric values. When it is a string, it denotes a classification or regression equation, of the form label ~ predictors or response ~ predictors, where predictors are separated by + operator. If it is a numeric vector, it will be a label or a response variable of a classification or regression equation, respectively.</p>
</td></tr>
<tr><td><code id="TrainBuddle_+3A_data">data</code></td>
<td>
<p>a data frame or a design matrix. When formula.string is a string, data should be a data frame which includes the label (or the response) and the predictors expressed in the formula string. When formula.string is a vector, i.e. a vector of labels or responses, data should be an nxp numeric matrix whose columns are predictors for further classification or regression.</p>
</td></tr>
<tr><td><code id="TrainBuddle_+3A_train.ratio">train.ratio</code></td>
<td>
<p>a ratio that is used to split data into training and test sets. When data is an n-by-p matrix, the resulting train data will be a (train.ratio x n)-by-p matrix. The default is 0.7.</p>
</td></tr>
<tr><td><code id="TrainBuddle_+3A_arrange">arrange</code></td>
<td>
<p>a logical value to arrange data for the classification only (automatically set up to FALSE for regression) when splitting data into training and test sets. If it is true, data will be arranged for the resulting training set to contain the specified ratio (train.ratio) of labels of the whole data. See also Split2TrainTest().</p>
</td></tr>
<tr><td><code id="TrainBuddle_+3A_batch.size">batch.size</code></td>
<td>
<p>a batch size used for training during iterations.</p>
</td></tr>
<tr><td><code id="TrainBuddle_+3A_total.iter">total.iter</code></td>
<td>
<p>a number of iterations used for training.</p>
</td></tr>
<tr><td><code id="TrainBuddle_+3A_hiddenlayer">hiddenlayer</code></td>
<td>
<p>a vector of numbers of nodes in hidden layers.</p>
</td></tr>
<tr><td><code id="TrainBuddle_+3A_batch.norm">batch.norm</code></td>
<td>
<p>a logical value to specify whether or not to use the batch normalization option for training. The default is TRUE.</p>
</td></tr>
<tr><td><code id="TrainBuddle_+3A_drop">drop</code></td>
<td>
<p>a logical value to specify whether or not to use the dropout option for training. The default is TRUE.</p>
</td></tr>
<tr><td><code id="TrainBuddle_+3A_drop.ratio">drop.ratio</code></td>
<td>
<p>a ratio for the dropout; used only if drop is TRUE. The default is 0.1.</p>
</td></tr>
<tr><td><code id="TrainBuddle_+3A_lr">lr</code></td>
<td>
<p>a learning rate. The default is 0.1.</p>
</td></tr>
<tr><td><code id="TrainBuddle_+3A_init.weight">init.weight</code></td>
<td>
<p>a weight used to initialize the weight matrix of each layer. The default is 0.1.</p>
</td></tr>
<tr><td><code id="TrainBuddle_+3A_activation">activation</code></td>
<td>
<p>a vector of activation functions used in all hidden layers. For two hidden layers (e.g., hiddenlayer=c(100, 50)), it is a vector of two activation functions, e.g., c(&quot;Sigmoid&quot;, &quot;SoftPlus&quot;). The list of available activation functions includes Sigmoid, Relu, LeakyRelu, TanH, ArcTan, ArcSinH, ElliotSig, SoftPlus, BentIdentity, Sinusoid, Gaussian, Sinc, and Identity. For details of the activation functions, please refer to Wikipedia.</p>
</td></tr>
<tr><td><code id="TrainBuddle_+3A_optim">optim</code></td>
<td>
<p>an optimization method which is used for training. The following methods are available: &quot;SGD&quot;, &quot;Momentum&quot;, &quot;AdaGrad&quot;, &quot;Adam&quot;, &quot;Nesterov&quot;, and &quot;RMSprop.&quot;</p>
</td></tr>
<tr><td><code id="TrainBuddle_+3A_type">type</code></td>
<td>
<p>a statistical model for the analysis: &quot;Classification&quot; or &quot;Regression.&quot;</p>
</td></tr>
<tr><td><code id="TrainBuddle_+3A_rand.eff">rand.eff</code></td>
<td>
<p>a logical value to specify whether or not to add a random effect into classification or regression.</p>
</td></tr>
<tr><td><code id="TrainBuddle_+3A_distr">distr</code></td>
<td>
<p>a distribution of a random effect; used only if rand.eff is TRUE. The following distributions are available: &quot;Normal&quot;, &quot;Exponential&quot;, &quot;Logistic&quot;, and &quot;Cauchy.&quot;</p>
</td></tr>
<tr><td><code id="TrainBuddle_+3A_disp">disp</code></td>
<td>
<p>a logical value which specifies whether or not to display intermediate training results (loss and accuracy) during the iterations.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of the following values: 
</p>

<dl>
<dt>lW</dt><dd><p>a list of n terms of weight matrices where n is equal to the number of hidden layers plus one.</p>
</dd> 
<dt>lb</dt><dd><p>a list of n terms of bias vectors where n is equal to the number of hidden layers plus one.</p>
</dd>
<dt>lParam</dt><dd><p>a list of parameters used for the training process.</p>
</dd>
<dt>train.loss</dt><dd><p>a vector of loss values of the training set obtained during iterations where its length is eqaul to number of epochs.</p>
</dd>
<dt>train.accuracy</dt><dd><p>a vector of accuracy values of the training set obtained during during iterations where its length is eqaul to number of epochs.</p>
</dd>
<dt>test.loss</dt><dd><p>a vector of loss values of the test set obtained during the iterations where its length is eqaul to number of epochs.</p>
</dd>
<dt>test.accuracy</dt><dd><p>a vector of accuracy values of the test set obtained during the iterations where its length is eqaul to number of epochs.</p>
</dd>
<dt>predicted.softmax</dt><dd><p>an r-by-n numeric matrix where r is the number of labels (classification) or 1 (regression), and n is the size of the test set. Its entries are predicted softmax values (classification) or predicted values (regression) of the test sets, obtained by using the weight matrices (lW) and biases (lb).</p>
</dd>
<dt>predicted.encoding</dt><dd><p>an r-by-n numeric matrix which is a result of one-hot encoding of the predicted.softmax; valid for classification only.</p>
</dd>
<dt>confusion.matrix</dt><dd><p>an r-by-r confusion matrix; valid classification only.</p>
</dd>
<dt>precision</dt><dd><p>an (r+1)-by-3 matrix which reports precision, recall, and F1 of each label; valid classification only.</p>
</dd>
</dl>



<h3>References</h3>

<p>[1] Geron, A. Hand-On Machine Learning with Scikit-Learn and TensorFlow. Sebastopol: O'Reilly, 2017. Print.
</p>
<p>[2] Han, J., Pei, J, Kamber, M. Data Mining: Concepts and Techniques. New York: Elsevier, 2011. Print.
</p>
<p>[3] Weilman, S. Deep Learning from Scratch. O'Reilly Media, 2019. Print.
</p>


<h3>See Also</h3>

<p>CheckNonNumeric(), GetPrecision(), FetchBuddle(), MakeConfusionMatrix(), OneHot2Label(), Split2TrainTest()
</p>


<h3>Examples</h3>

<pre><code class='language-R'>####################
# train.ratio = 0.6                    ## 60% of data is used for training
# batch.size = 10     
# total.iter = 100
# hiddenlayer=c(20,10)                ## Use two hidden layers
# arrange=TRUE                         #### Use "arrange" option 
# activations = c("Relu","SoftPlus")   ### Use Relu and SoftPlus 
# optim = "Nesterov"                   ### Use the "Nesterov" method for the optimization.
# type = Classification  
# rand.eff = TRUE                      #### Add some random effect
# distr="Normal"                       #### The random effect is a normal random variable
# disp = TRUE                          #### Display intemeidate results during iterations.


data(iris)

lst = TrainBuddle("Species~Sepal.Width+Petal.Width", iris, train.ratio=0.6, 
         arrange=TRUE, batch.size=10, total.iter = 100, hiddenlayer=c(20, 10), 
         batch.norm=TRUE, drop=TRUE, drop.ratio=0.1, lr=0.1, init.weight=0.1, 
         activation=c("Relu","SoftPlus"), optim="Nesterov", 
         type = "Classification", rand.eff=TRUE, distr = "Normal", disp=TRUE)

lW = lst$lW
lb = lst$lb
lParam = lst$lParam

confusion.matrix = lst$confusion.matrix
precision = lst$precision

confusion.matrix
precision


### Another classification example 
### Using mnist data


data(mnist_data)

Img_Mat = mnist_data$Images
Img_Label = mnist_data$Labels

                             ##### Use 100 images

X = Img_Mat                   ### X: 100 x 784 matrix
Y = Img_Label                 ### Y: 100 x 1 vector

lst = TrainBuddle(Y, X, train.ratio=0.6, arrange=TRUE, batch.size=10, total.iter = 100, 
                 hiddenlayer=c(20, 10), batch.norm=TRUE, drop=TRUE, 
                 drop.ratio=0.1, lr=0.1, init.weight=0.1, 
                 activation=c("Relu","SoftPlus"), optim="AdaGrad", 
                 type = "Classification", rand.eff=TRUE, distr = "Logistic", disp=TRUE)


confusion.matrix = lst$confusion.matrix
precision = lst$precision

confusion.matrix
precision






###############   Regression example  


n=100
p=10
X = matrix(rnorm(n*p, 1, 1), n, p)  ## X is a 100-by-10 design matrix 
b = matrix( rnorm(p, 1, 1), p,1)
e = matrix(rnorm(n, 0, 1), n,1)
Y = X %*% b + e                     ### Y=X b + e
######### train.ratio=0.7
######### batch.size=20
######### arrange=FALSE
######### total.iter = 100
######### hiddenlayer=c(20)
######### activation = c("Identity")
######### "optim" = "Adam"
######### type = "Regression"
######### rand.eff=FALSE

lst = TrainBuddle(Y, X, train.ratio=0.7, arrange=FALSE, batch.size=20, total.iter = 100, 
                 hiddenlayer=c(20), batch.norm=TRUE, drop=TRUE, drop.ratio=0.1, lr=0.1, 
                 init.weight=0.1, activation=c("Identity"), optim="AdaGrad", 
                 type = "Regression", rand.eff=FALSE, disp=TRUE)




</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
