<!DOCTYPE html><html><head><title>Help for package sparseMatEst</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {sparseMatEst}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#genSparseMatrix'><p>Sparse matrix generator</p></a></li>
<li><a href='#sparseCov'><p>Sparse covariance matrix estimator with error control</p></a></li>
<li><a href='#sparseMatEst-package'><p>Sparse Matrix Estimation and Inference</p></a></li>
<li><a href='#sparsePrec'><p>Sparse precision matrix estimator with error control</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Sparse Matrix Estimation and Inference</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.0</td>
</tr>
<tr>
<td>Author:</td>
<td>Adam B Kashlak [aut, cre],
  Xinyu Zhang [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Adam B Kashlak &lt;kashlak@ualberta.ca&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>The 'sparseMatEst' package provides functions for estimating sparse 
  covariance and precision matrices with error control. A false positive
  rate is fixed corresponding to the probability of falsely including a matrix 
  entry in the support of the estimator.  It uses the binary search
  method outlined in Kashlak and Kong (2019) &lt;<a href="https://arxiv.org/abs/1705.02679">arXiv:1705.02679</a>&gt; and in 
  Kashlak (2019) &lt;<a href="https://arxiv.org/abs/1903.10988">arXiv:1903.10988</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>6.1.1</td>
</tr>
<tr>
<td>Imports:</td>
<td>stats, glasso</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2019-05-17 03:30:20 UTC; AdKas</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2019-05-17 08:20:05 UTC</td>
</tr>
</table>
<hr>
<h2 id='genSparseMatrix'>Sparse matrix generator</h2><span id='topic+genSparseMatrix'></span>

<h3>Description</h3>

<p>A way to generate sparse matrices for simulation and testing.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>genSparseMatrix(k, rho, type)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="genSparseMatrix_+3A_k">k</code></td>
<td>
<p>dimension parameter</p>
</td></tr>
<tr><td><code id="genSparseMatrix_+3A_rho">rho</code></td>
<td>
<p>additional parameter</p>
</td></tr>
<tr><td><code id="genSparseMatrix_+3A_type">type</code></td>
<td>
<p>type of matrix to generate</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>genSparseMatrix</code> constructs a sparse matrix to be used for
testing and simulation.  The <code>type</code> argument determines what
type of matrix is produced while <code>k</code> effects the dimension and
<code>rho</code> is an additional parameter to effect the output.
</p>
<p>For <code>type = 'tri'</code>, a (k x k) tridiagonal matrix is returned
with off-diagonal entries equal to <code>rho</code>.
</p>
<p>For <code>type = 'arm'</code>, a (k x k) autoregressive matrix is returned
with off-diagonal entries equal to <code>rho^|i-j|</code>.
</p>
<p>For <code>type = 'band'</code>, a (k x k) banded matrix is returned with
<code>rho</code> bands.
</p>
<p>For <code>type = 'rand'</code>, a (k x k) matrix is returned with
<code>rho</code> in (0,1) randomly selected off-diagonal entries set to
be non-zero.
</p>
<p>For <code>type = 'tree'</code>, the adjacency matrix for a k-deep binary tree
is returned with off-diagonal entries set to <code>rho</code>.  The dimension
of this matrix is k(k+1)/2.
</p>
<p>For <code>type = 'multi'</code>, a (k x k) matrix is returned with <code>rho</code>
off-diagonals set to ones.
</p>
<p>For <code>type = 'block'</code>, a block diagonal matrix is returned with
k blocks of size <code>rho</code>.
</p>


<h3>Value</h3>

<p>a matrix corresponding to the arguments chosen.
</p>


<h3>Author(s)</h3>

<p>Adam B Kashlak <a href="mailto:kashlak@ualberta.ca">kashlak@ualberta.ca</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
  out = list();
  out[[1]]= genSparseMatrix( 20,0.5,"tri" );
  out[[2]]= genSparseMatrix( 20,0.5,"arm" );
  out[[3]]= genSparseMatrix( 20,5,"band" );
  out[[4]]= genSparseMatrix( 20,0.5,"rand" );
  out[[5]]= genSparseMatrix( 7,0.5,"tree" );
  out[[6]]= genSparseMatrix( 20,5,"multi" );
  out[[7]]= genSparseMatrix( 5,4,"block" );

    par(mfrow=c(2,3));
    lab = c("tri","arm","band","rand","tree","multi","block");
    for( i in 2:7  )
      image(out[[i]],main=lab[i]);

</code></pre>

<hr>
<h2 id='sparseCov'>Sparse covariance matrix estimator with error control</h2><span id='topic+sparseCov'></span>

<h3>Description</h3>

<p>Given a data matrix, <code>sparseCov</code> estimates the covariance
matrix for the data under the assumption that the true covariance
matrix is sparse, i.e. that most of the off-diagonal entries are
equal to zero.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sparseCov(dat, alf = 0.5, iter = 10, pnrm = Inf, THRSH = "hard")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sparseCov_+3A_dat">dat</code></td>
<td>
<p>nxk data matrix, n observations, k dimensions</p>
</td></tr>
<tr><td><code id="sparseCov_+3A_alf">alf</code></td>
<td>
<p>false positive rate in [0,1], Default is 0.5</p>
</td></tr>
<tr><td><code id="sparseCov_+3A_iter">iter</code></td>
<td>
<p>number of iterates, Default is 10</p>
</td></tr>
<tr><td><code id="sparseCov_+3A_pnrm">pnrm</code></td>
<td>
<p>norm to use, Default = Inf</p>
</td></tr>
<tr><td><code id="sparseCov_+3A_thrsh">THRSH</code></td>
<td>
<p>Type of thresholding used;
Takes values: hard, soft, adpt, scad.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The algorithm begins with the empirical covariance estimator as
computed by <code>cov</code>.  It then iteratively computes covariance
estimators with false positive rates of <code>alf</code>, <code>alf</code>^2,
and so on until <code>iter</code> estimators have been constructed.
</p>
<p>The norm chosen determines the topology on the space of matrices.
<code>pnorm</code> defaults to <code>Inf</code> being the operator norm
or maximal eigenvalue.  This is theoretically justified to work
in the references.
Other norms could be considered, but their performance is not
a strong.
</p>
<p>Four thresholding methods are implemented.  <code>THRSH</code> defaults
to hard thresholding where small matrix entries are set to zero
while large entries are not affected.  The soft and adpt thresholds
shrink all entries towards zero.  The scad threshold interpolates
between hard and soft thresholding.  More details can be found
in the references.
</p>


<h3>Value</h3>

<p>a list of arrays containing <code>iter+1</code> sparse
covariance matrices corresponding to false positive rates of
<code>1</code>, <code>alf</code>,
<code>alf</code>^2,..., <code>alf</code>^<code>iter</code>.  Each list
corresponds to one type of thresholding chosen by <code>THRSH</code>.
</p>


<h3>Author(s)</h3>

<p>Adam B Kashlak <a href="mailto:kashlak@ualberta.ca">kashlak@ualberta.ca</a>
</p>


<h3>References</h3>

<p>Kashlak, Adam B., and Linglong Kong.
&quot;A concentration inequality based methodology
for sparse covariance estimation.&quot; arXiv
preprint arXiv:1705.02679 (2017).
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Generate four sparse covariance matrix estimators
# with false positive rates of 0.5, 0.25, 0.125,
# and 0.0625
n = 30
k = 50
dat = matrix(rnorm(n*k),n,k)
out = sparseCov( dat, alf=0.5, iter=4, THRSH=c("hard","soft") )
  par(mfcol=c(2,2))
  lab = c(1,0.5,0.5^2,0.5^3,0.5^4);
  for( i in 2:5 )
    image( out$hard[,,i]!=0, main=lab[i] )
  for( i in 2:5 )
    image( log(abs(out$hard[,,i] - out$soft[,,i])), main=lab[i] )
</code></pre>

<hr>
<h2 id='sparseMatEst-package'>Sparse Matrix Estimation and Inference</h2><span id='topic+sparseMatEst'></span><span id='topic+sparseMatEst-package'></span>

<h3>Description</h3>

<p>The <code>sparseMatEst</code> library provides functions for estimating
sparse covariance and precision matrices with error control.
</p>


<h3>Details</h3>

<p>Given a data matrix, this package contains two main functions
used to estimate the covariance matrix and the precision matrix
of the data under the assumption of sparsity, that most
off-diagonal entries are zero.  This is achieved by selecting
a false positive rate corresponding to the probability that
a true zero entry is falsely chosen to be non-zero by the estimator.
</p>
<p>The false positive rate can be treated as an interpretable
penalization parameter.  Setting this to zero will return
a diagonal matrix.  Choosing a false positive rate away from
zero will allow for the estimator to contain some non-zero
off-diagonal entries.
</p>
<p>Future updates coming Fall 2019 include inferential tools
based on these
sparse matrix estimators.  These include a variant of
linear and quadratic discriminant analysis, fitting a
Gaussian mixture assuming sparsity, network clustering
algorithm, and a method to fit a random design linear
regression model.
</p>


<h3>Author(s)</h3>

<p>Adam B Kashlak <a href="mailto:kashlak@ualberta.ca">kashlak@ualberta.ca</a>
</p>


<h3>References</h3>

<p>Kashlak, Adam B., and Linglong Kong.
&quot;A concentration inequality based methodology
for sparse covariance estimation.&quot; arXiv
preprint arXiv:1705.02679 (2017).
</p>
<p>Kashlak, Adam B. &quot;Non-asymptotic error controlled
sparse high dimensional precision matrix estimation.&quot;
arXiv preprint arXiv:1903.10988 (2019).
</p>

<hr>
<h2 id='sparsePrec'>Sparse precision matrix estimator with error control</h2><span id='topic+sparsePrec'></span>

<h3>Description</h3>

<p>Given a data matrix, <code>sparsePrec</code> estimates the precision
matrix for the data under the assumption that the true precision
matrix is sparse, i.e. that most of the off-diagonal entries are
equal to zero.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sparsePrec(dat = 0, prec = 0, alf = 0.5, iter = 10, pnrm = Inf,
  THRSH = "hard", rho = 1, regMeth = "glasso")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sparsePrec_+3A_dat">dat</code></td>
<td>
<p>nxk data matrix, n observations, k dimensions</p>
</td></tr>
<tr><td><code id="sparsePrec_+3A_prec">prec</code></td>
<td>
<p>start with a precision estimator instead of <code>dat</code></p>
</td></tr>
<tr><td><code id="sparsePrec_+3A_alf">alf</code></td>
<td>
<p>false positive rate in [0,1], Default is 0.5</p>
</td></tr>
<tr><td><code id="sparsePrec_+3A_iter">iter</code></td>
<td>
<p>number of iterates, Default is 10</p>
</td></tr>
<tr><td><code id="sparsePrec_+3A_pnrm">pnrm</code></td>
<td>
<p>norm to use, Default = Inf</p>
</td></tr>
<tr><td><code id="sparsePrec_+3A_thrsh">THRSH</code></td>
<td>
<p>Type of thresholding used;
Takes values: hard, soft, adpt, scad.</p>
</td></tr>
<tr><td><code id="sparsePrec_+3A_rho">rho</code></td>
<td>
<p>Penalization parameter for the initial estimator, Default is 1</p>
</td></tr>
<tr><td><code id="sparsePrec_+3A_regmeth">regMeth</code></td>
<td>
<p>Type of initial estimator, Default is 'glasso'</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The algorithm begins with an initial precision matrix estimator
being either <code>regMeth</code> = 'glasso' for debiased graphical lasso
or <code>regMeth</code> = 'ridge' for debiased ridge estimator.
It then iteratively computes covariance
estimators with false positive rates of <code>alf</code>, <code>alf</code>^2,
and so on until <code>iter</code> estimators have been constructed.
</p>
<p>If <code>dat</code> is not included, but <code>prec</code> is, then the
code runs as before but using the argument <code>prec</code> as the
initial precision matrix estimator.  In this case, the <code>regMeth</code>
is not considered.
</p>
<p>The norm chosen determines the topology on the space of matrices.
<code>pnorm</code> defaults to <code>Inf</code> being the operator norm
or maximal eigenvalue.  This is theoretically justified to work
in the references.
Other norms could be considered, but their performance is not
a strong.
</p>
<p>Four thresholding methods are implemented.  <code>THRSH</code> defaults
to hard thresholding where small matrix entries are set to zero
while large entries are not affected.  The soft and adpt thresholds
shrink all entries towards zero.  The scad threshold interpolates
between hard and soft thresholding.  More details can be found
in the references.
</p>


<h3>Value</h3>

<p>a list of arrays containing <code>iter+1</code> sparse
precision matrices corresponding to false positive rates of
<code>1</code>, <code>alf</code>,
<code>alf</code>^2,..., <code>alf</code>^<code>iter</code>.  Each list
corresponds to one type of thresholding chosen by <code>THRSH</code>.
</p>


<h3>Author(s)</h3>

<p>Adam B Kashlak <a href="mailto:kashlak@ualberta.ca">kashlak@ualberta.ca</a>
</p>


<h3>References</h3>

<p>Kashlak, Adam B. &quot;Non-asymptotic error controlled
sparse high dimensional precision matrix estimation.&quot;
arXiv preprint arXiv:1903.10988 (2019).
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Generate four sparse covariance matrix estimators
# with false positive rates of 0.5, 0.25, 0.125,
# and 0.0625
n = 30
k = 50
dat = matrix(rnorm(n*k),n,k)
out = sparsePrec( dat, alf=0.5, iter=4, THRSH=c("hard","soft") )
  par(mfcol=c(2,2))
  lab = c(1,0.5,0.5^2,0.5^3,0.5^4);
  for( i in 2:5 )
    image( out$hard[,,i]!=0, main=lab[i] )
  for( i in 2:5 )
    image( log(abs(out$hard[,,i] - out$soft[,,i])), main=lab[i] )
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
