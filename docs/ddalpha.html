<!DOCTYPE html><html><head><title>Help for package ddalpha</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {ddalpha}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#ddalpha-package'>
<p>Depth-Based Classification and Calculation of Data Depth</p></a></li>
<li><a href='#Cmetric'><p>Fast Computation of the Uniform Metric for Sets of Functional Data</p></a></li>
<li><a href='#compclassf.classify'>
<p>Classify using Functional Componentwise Classifier</p></a></li>
<li><a href='#compclassf.train'>
<p>Functional Componentwise Classifier</p></a></li>
<li><a href='#Custom+20Methods'>
<p>Using Custom Depth Functions and Classifiers</p></a></li>
<li><a href='#dataf'>
<p>Converts data from fdata class to the functional class.</p></a></li>
<li><a href='#dataf.+2A'>
<p>Functional Data Sets</p></a></li>
<li><a href='#dataf.geneexp'>
<p>Gene Expression Profile Data</p></a></li>
<li><a href='#dataf.growth'>
<p>Berkeley Growth Study Data</p></a></li>
<li><a href='#dataf.medflies'>
<p>Relationship of Age Patterns of Fecundity to Mortality for Female Medflies.</p></a></li>
<li><a href='#dataf.population'><p>World Historical Population-by-Country Dataset</p></a></li>
<li><a href='#dataf.population2010'><p>World Historical Population-by-Country Dataset (2010 Revision)</p></a></li>
<li><a href='#dataf.sim.1.CFF07'>
<p>Model 1 from Cuevas et al. (2007)</p></a></li>
<li><a href='#dataf.sim.2.CFF07'>
<p>Model 2 from Cuevas et al. (2007)</p></a></li>
<li><a href='#dataf.tecator'>
<p>Functional Data Set Spectrometric Data (Tecator)</p></a></li>
<li><a href='#dataf2rawfd'><p>Transform a <code>dataf</code> Object to Raw Functional Data</p></a></li>
<li><a href='#ddalpha.classify'>
<p>Classify using DD-Classifier</p></a></li>
<li><a href='#ddalpha.getErrorRateCV'>
<p>Test DD-Classifier</p></a></li>
<li><a href='#ddalpha.getErrorRatePart'>
<p>Test DD-Classifier</p></a></li>
<li><a href='#ddalpha.test'>
<p>Test DD-Classifier</p></a></li>
<li><a href='#ddalpha.train'>
<p>Train DD-Classifier</p></a></li>
<li><a href='#ddalphaf.classify'>
<p>Classify using Functional DD-Classifier</p></a></li>
<li><a href='#ddalphaf.getErrorRateCV'>
<p>Test Functional DD-Classifier</p></a></li>
<li><a href='#ddalphaf.getErrorRatePart'>
<p>Test Functional DD-Classifier</p></a></li>
<li><a href='#ddalphaf.test'>
<p>Test Functional DD-Classifier</p></a></li>
<li><a href='#ddalphaf.train'>
<p>Functional DD-Classifier</p></a></li>
<li><a href='#depth.'>
<p>Calculate Depth</p></a></li>
<li><a href='#depth.betaSkeleton'>
<p>Calculate Beta-Skeleton Depth</p></a></li>
<li><a href='#depth.contours'>
<p>Depth Contours</p></a></li>
<li><a href='#depth.contours.ddalpha'>
<p>Depth Contours</p></a></li>
<li><a href='#depth.graph'>
<p>Depth Graph</p></a></li>
<li><a href='#depth.halfspace'>
<p>Calculate Halfspace Depth</p></a></li>
<li><a href='#depth.L2'>
<p>Calculate L2-Depth</p></a></li>
<li><a href='#depth.Mahalanobis'>
<p>Calculate Mahalanobis Depth</p></a></li>
<li><a href='#depth.potential'>
<p>Calculate Potential of the Data</p></a></li>
<li><a href='#depth.projection'>
<p>Calculate Projection Depth</p></a></li>
<li><a href='#depth.qhpeeling'>
<p>Calculate Convex Hull Peeling Depth</p></a></li>
<li><a href='#depth.sample'><p>Fast Depth Computation for Univariate and Bivariate Random Samples</p></a></li>
<li><a href='#depth.simplicial'>
<p>Calculate Simplicial Depth</p></a></li>
<li><a href='#depth.simplicialVolume'>
<p>Calculate Simplicial Volume Depth</p></a></li>
<li><a href='#depth.space.'>
<p>Calculate Depth Space using the Given Depth</p></a></li>
<li><a href='#depth.space.halfspace'>
<p>Calculate Depth Space using Halfspace Depth</p></a></li>
<li><a href='#depth.space.Mahalanobis'>
<p>Calculate Depth Space using Mahalanobis Depth</p></a></li>
<li><a href='#depth.space.potential'>
<p>Calculate Potential Space</p></a></li>
<li><a href='#depth.space.projection'>
<p>Calculate Depth Space using Projection Depth</p></a></li>
<li><a href='#depth.space.simplicial'>
<p>Calculate Depth Space using Simplicial Depth</p></a></li>
<li><a href='#depth.space.simplicialVolume'>
<p>Calculate Depth Space using Simplicial Volume Depth</p></a></li>
<li><a href='#depth.space.spatial'>
<p>Calculate Depth Space using Spatial Depth</p></a></li>
<li><a href='#depth.space.zonoid'>
<p>Calculate Depth Space using Zonoid Depth</p></a></li>
<li><a href='#depth.spatial'>
<p>Calculate Spatial Depth</p></a></li>
<li><a href='#depth.zonoid'>
<p>Calculate Zonoid Depth</p></a></li>
<li><a href='#depthf.'>
<p>Calculate Functional Depth</p></a></li>
<li><a href='#depthf.ABD'><p>Adjusted Band Depth for Functional Data</p></a></li>
<li><a href='#depthf.BD'><p>Band Depth for Functional Data</p></a></li>
<li><a href='#depthf.fd1'><p>Univariate Integrated and Infimal Depth for Functional Data</p></a></li>
<li><a href='#depthf.fd2'><p>Bivariate Integrated and Infimal Depth for Functional Data</p></a></li>
<li><a href='#depthf.hM'><p>h-Mode Depth for Functional Data</p></a></li>
<li><a href='#depthf.hM2'><p>Bivariate h-Mode Depth for Functional Data Based on the <code class="reqn">L^2</code> Metric</p></a></li>
<li><a href='#depthf.HR'><p>Half-Region Depth for Functional Data</p></a></li>
<li><a href='#depthf.RP1'><p>Univariate Random Projection Depths for Functional Data</p></a></li>
<li><a href='#depthf.RP2'><p>Bivariate Random Projection Depths for Functional Data</p></a></li>
<li><a href='#depthf.simplicialBand'>
<p>Calculate Simplicial Band Depth</p></a></li>
<li><a href='#derivatives.est'><p>Estimation of the First Two Derivatives for Functional Data</p></a></li>
<li><a href='#dknn.classify'>
<p>Depth-Based kNN</p></a></li>
<li><a href='#dknn.classify.trained'>
<p>Depth-Based kNN</p></a></li>
<li><a href='#dknn.train'>
<p>Depth-Based kNN</p></a></li>
<li><a href='#draw.ddplot'>
<p>Draw <em>DD</em>-Plot</p></a></li>
<li><a href='#FKS'><p>Fast Kernel Smoothing</p></a></li>
<li><a href='#getdata'>
<p>Data for Classification</p></a></li>
<li><a href='#infimalRank'><p>Adjusted Ranking of Functional Data Based on the Infimal Depth</p></a></li>
<li><a href='#is.in.convex'>
<p>Check Outsiderness</p></a></li>
<li><a href='#L2metric'><p>Fast Computation of the <code class="reqn">L^2</code> Metric for Sets of Functional Data</p></a></li>
<li><a href='#plot.ddalpha'>
<p>Plots for the &quot;ddalpha&quot; Class</p></a></li>
<li><a href='#plot.ddalphaf'>
<p>Plots for the &quot;ddalphaf&quot; Class</p></a></li>
<li><a href='#plot.functional'>
<p>Plot functions for the Functional Data</p></a></li>
<li><a href='#rawfd2dataf'><p>Transform Raw Functional Data to a <code>dataf</code> Object</p></a></li>
<li><a href='#resetPar'>
<p>Reset Graphical Parameters</p></a></li>
<li><a href='#shape.fd.analysis'><p>Diagnostic Plot for First and Second Order Integrated and Infimal Depths</p></a></li>
<li><a href='#shape.fd.outliers'><p>Functional Depth-Based Shape Outlier Detection</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Depth-Based Classification and Calculation of Data Depth</td>
</tr>
<tr>
<td>Version:</td>
<td>1.3.15</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-01-12</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10), stats, utils, graphics, grDevices, MASS, class,
robustbase, sfsmisc, geometry</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp (&ge; 0.11.0)</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>BH, Rcpp</td>
</tr>
<tr>
<td>Description:</td>
<td>Contains procedures for depth-based supervised learning, which are entirely non-parametric, in particular the DDalpha-procedure (Lange, Mosler and Mozharovskyi, 2014 &lt;<a href="https://doi.org/10.1007%2Fs00362-012-0488-4">doi:10.1007/s00362-012-0488-4</a>&gt;). The training data sample is transformed by a statistical depth function to a compact low-dimensional space, where the final classification is done. It also offers an extension to functional data and routines for calculating certain notions of statistical depth functions. 50 multivariate and 5 functional classification problems are included. (Pokotylo, Mozharovskyi and Dyckerhoff, 2019 &lt;<a href="https://doi.org/10.18637%2Fjss.v091.i05">doi:10.18637/jss.v091.i05</a>&gt;).</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-01-12 14:29:22 UTC; alexis</td>
</tr>
<tr>
<td>Author:</td>
<td>Oleksii Pokotylo [aut, cre],
  Pavlo Mozharovskyi [aut],
  Rainer Dyckerhoff [aut],
  Stanislav Nagy [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Oleksii Pokotylo &lt;alexey.pokotylo@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-01-12 15:00:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='ddalpha-package'>
Depth-Based Classification and Calculation of Data Depth
</h2><span id='topic+ddalpha-package'></span><span id='topic+ddalpha'></span>

<h3>Description</h3>

<p>The package provides many procedures for calculating the depth of points in an empirical distribution for many notions of data depth. 
Further it provides implementations for depth-based classification, for multivariate and functional data.
</p>
<p>The package implements the DD<code class="reqn">\alpha</code>-classifier (Lange, Mosler and Mozharovskyi, 2014), a nonparametric procedure for supervised binary classification with <code class="reqn">q\ge 2</code> classes. In the training step, the sample is first transformed into a <code class="reqn">q</code>-dimensional cube of depth vectors, then a linear separation rule in its polynomial extension is constructed with the <code class="reqn">\alpha</code>-procedure. The classification step involves alternative treatments of 'outsiders'.
</p>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> ddalpha</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 1.3.15</td>
</tr>
<tr>
 <td style="text-align: left;">
Date: </td><td style="text-align: left;"> 2024-01-12</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL-2</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p>Use <code><a href="#topic+ddalpha.train">ddalpha.train</a></code> to train the DD-classifier and <code><a href="#topic+ddalpha.classify">ddalpha.classify</a></code> to classify with it. 
Load sample classification problems using <code><a href="#topic+getdata">getdata</a></code>. The package contains 50 classification problems built of 33 sets of real data.
</p>
<p>The list of the implemented multivariate depths is found in topic <code><a href="#topic+depth.">depth.</a></code>, for functional depths see <code><a href="#topic+depthf.">depthf.</a></code>. The depth representations of the multivariate data are obtained with <code><a href="#topic+depth.space.">depth.space.</a></code>. Functions <code><a href="#topic+depth.contours">depth.contours</a></code> and <code><a href="#topic+depth.contours.ddalpha">depth.contours.ddalpha</a></code> build depth contours, and <code><a href="#topic+depth.graph">depth.graph</a></code> builds depth graphs for two-dimensional data. Function <code><a href="#topic+draw.ddplot">draw.ddplot</a></code> draws DD-plot for the existing DD-classifier, or for pre-calculated depth space.
</p>
<p>The package supports user-defined depths and classifiers, see topic <code><a href="#topic+Custom+20Methods">Custom Methods</a></code>. A pre-calculated DD-plot may also be used as <code>data</code>, see topic <code><a href="#topic+ddalpha.train">ddalpha.train</a></code>.
</p>
<p><code><a href="#topic+is.in.convex">is.in.convex</a></code> shows whether an object is no 'outsider', i.e. can be classified by its depth values. 
Outsiders are alternatively classified by LDA, kNN and maximum Mahalanobis depth as well as by random assignment.
</p>
<p>Use <code><a href="#topic+compclassf.train">compclassf.train</a></code> and <code><a href="#topic+ddalphaf.train">ddalphaf.train</a></code> to train the functional DD-classifiers and <code><a href="#topic+compclassf.classify">compclassf.classify</a></code> <code><a href="#topic+ddalpha.classify">ddalpha.classify</a></code> to classify with them. Load sample functional classification problems with <code><a href="#topic+dataf.+2A">dataf.*</a></code>. The package contains 4 functional data sets and 2 data set generators. The functional data are visualized with <code><a href="#topic+plot.functional">plot.functional</a></code>.
</p>


<h3>Author(s)</h3>

<p>Oleksii Pokotylo, &lt;alexey.pokotylo@gmail.com&gt;
</p>
<p>Pavlo Mozharovskyi, &lt;pavlo.mozharovskyi@telecom-paris.fr&gt;
</p>
<p>Rainer Dyckerhoff, &lt;rainer.dyckerhoff@statistik.uni-koeln.de&gt;
</p>
<p>Stanislav Nagy, &lt;nagy@karlin.mff.cuni.cz&gt;
</p>


<h3>References</h3>

<p>Pokotylo, O., Mozharovskyi, P., Dyckerhoff, R. (2019). Depth and depth-based classification with R-package ddalpha. <em>Journal of Statistical Software</em> <b>91</b> 1&ndash;46.
</p>
<p>Lange, T., Mosler, K., and Mozharovskyi, P. (2014). Fast nonparametric classification based on data depth. <em>Statistical Papers</em> <b>55</b> 49&ndash;69.
</p>
<p>Lange, T., Mosler, K., and Mozharovskyi, P. (2014). DD<code class="reqn">\alpha</code>-classification of asymmetric and fat-tailed data. In: Spiliopoulou, M., Schmidt-Thieme, L., Janning, R. (eds), <em>Data Analysis, Machine Learning and Knowledge Discovery</em>, Springer (Berlin), 71&ndash;78.
</p>
<p>Mosler, K. and Mozharovskyi, P. (2017). Fast DD-classification of functional data. <em>Statistical Papers</em> <b>58</b> 1055&ndash;1089.
</p>
<p>Mozharovskyi, P. (2015). <em>Contributions to Depth-based Classification and Computation of the Tukey Depth</em>. Verlag Dr. Kovac (Hamburg).
</p>
<p>Mozharovskyi, P., Mosler, K., and Lange, T. (2015). Classifying real-world data with the DD<code class="reqn">\alpha</code>-procedure. <em>Advances in Data Analysis and Classification</em> <b>9</b> 287&ndash;314.
</p>
<p>Nagy, S., Gijbels, I. and Hlubinka, D.  (2017). Depth-based recognition of shape outlying functions. <em>Journal of Computational and Graphical Statistics</em>. To appear.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ddalpha.train">ddalpha.train</a></code>, <code><a href="#topic+ddalpha.classify">ddalpha.classify</a></code>, 
</p>
<p><code><a href="#topic+ddalphaf.train">ddalphaf.train</a></code>, <code><a href="#topic+ddalphaf.classify">ddalphaf.classify</a></code>, <code><a href="#topic+compclassf.train">compclassf.train</a></code>, <code><a href="#topic+compclassf.classify">compclassf.classify</a></code>
</p>
<p><code><a href="#topic+depth.">depth.</a></code>, <code><a href="#topic+depthf.">depthf.</a></code>, <code><a href="#topic+depth.space.">depth.space.</a></code>, 
</p>
<p><code><a href="#topic+is.in.convex">is.in.convex</a></code>,
</p>
<p><code><a href="#topic+getdata">getdata</a></code>, <code><a href="#topic+dataf.+2A">dataf.*</a></code>,
</p>
<p><code><a href="#topic+plot.ddalpha">plot.ddalpha</a></code>, <code><a href="#topic+plot.ddalphaf">plot.ddalphaf</a></code>,
<code><a href="#topic+plot.functional">plot.functional</a></code>, <code><a href="#topic+depth.graph">depth.graph</a></code>, <code><a href="#topic+draw.ddplot">draw.ddplot</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Generate a bivariate normal location-shift classification task
# containing 200 training objects and 200 to test with
class1 &lt;- mvrnorm(200, c(0,0), 
                  matrix(c(1,1,1,4), nrow = 2, ncol = 2, byrow = TRUE))
class2 &lt;- mvrnorm(200, c(2,2), 
                  matrix(c(1,1,1,4), nrow = 2, ncol = 2, byrow = TRUE))
trainIndices &lt;- c(1:100)
testIndices &lt;- c(101:200)
propertyVars &lt;- c(1:2)
classVar &lt;- 3
trainData &lt;- rbind(cbind(class1[trainIndices,], rep(1, 100)), 
                   cbind(class2[trainIndices,], rep(2, 100)))
testData &lt;- rbind(cbind(class1[testIndices,], rep(1, 100)), 
                  cbind(class2[testIndices,], rep(2, 100)))
data &lt;- list(train = trainData, test = testData)

# Train the DDalpha-classifier
ddalpha &lt;- ddalpha.train(data$train)

# Classify by means of DDalpha-classifier
classes &lt;- ddalpha.classify(ddalpha, data$test[,propertyVars])
cat("Classification error rate:", 
    sum(unlist(classes) != data$test[,classVar])/200, "\n")

# Calculate zonoid depth of top 10 testing objects w.r.t. 1st class
depths.zonoid &lt;- depth.zonoid(data$test[1:10,propertyVars], 
                              data$train[trainIndices,propertyVars])
cat("Zonoid depths:", depths.zonoid, "\n")

# Calculate the random Tukey depth of top 10 testing objects w.r.t. 1st class
depths.halfspace &lt;- depth.halfspace(data$test[1:10,propertyVars], 
                                        data$train[trainIndices,propertyVars])
cat("Random Tukey depths:", depths.halfspace, "\n")

# Calculate depth space with zonoid depth
dspace.zonoid &lt;- depth.space.zonoid(data$train[,propertyVars], c(100, 100))

# Calculate depth space with the exact Tukey depth
dspace.halfspace &lt;- depth.space.halfspace(data$train[,propertyVars], c(100, 100), exact = TRUE)

# Count outsiders
numOutsiders = sum(rowSums(is.in.convex(data$test[,propertyVars], 
                                data$train[,propertyVars], c(100, 100))) == 0)
cat(numOutsiders, "outsiders found in the testing sample.\n")
</code></pre>

<hr>
<h2 id='Cmetric'>Fast Computation of the Uniform Metric for Sets of Functional Data</h2><span id='topic+Cmetric'></span>

<h3>Description</h3>

<p>Returns the matrix of <code class="reqn">C</code> (uniform) distances between two sets of functional data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Cmetric(A, B)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Cmetric_+3A_a">A</code></td>
<td>
<p>Functions of the first set, represented by a matrix of their functional values of 
size <code>m*d</code>. <code>m</code> stands for the number of functions, <code>d</code>
is the number of the equi-distant points in the domain of the data at which the functional
values of the <code>m</code> functions are evaluated.</p>
</td></tr>
<tr><td><code id="Cmetric_+3A_b">B</code></td>
<td>
<p>Functions of the second set, represented by a matrix of their functional values of 
size <code>n*d</code>. <code>n</code> stands for the number of functions, <code>d</code>
is the number of the equi-distant points in the domain of the data at which the functional
values of the <code>n</code> functions are evaluated. The grid of observation points for the 
functions <code>A</code> and <code>B</code> must be the same.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For two sets of functional data of sizes <code>m</code> and <code>n</code>
represented by matrices of their functional values, 
this function returns the symmetric matrix of size <code>m*n</code> whose entry in the
<code>i</code>-th row and <code>j</code>-th column is the approximated <code class="reqn">C</code> (uniform) distance of the 
<code>i</code>-th function from the first set, and the <code>j</code>-th function from the second set.
This function is utilized in the computation of the h-mode depth.
</p>


<h3>Value</h3>

<p>A symmetric matrix of the distances of the functions of size <code>m*n</code>.
</p>


<h3>Author(s)</h3>

<p>Stanislav Nagy, <a href="mailto:nagy@karlin.mff.cuni.cz">nagy@karlin.mff.cuni.cz</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+depthf.hM">depthf.hM</a></code>
</p>
<p><code><a href="#topic+dataf2rawfd">dataf2rawfd</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>datapop = dataf2rawfd(dataf.population()$dataf,range=c(1950,2015),d=66)
A = datapop[1:20,]
B = datapop[21:50,]
Cmetric(A,B)

</code></pre>

<hr>
<h2 id='compclassf.classify'>
Classify using Functional Componentwise Classifier
</h2><span id='topic+compclassf.classify'></span><span id='topic+predict.compclassf'></span>

<h3>Description</h3>

<p>Classifies data using the functional componentwise classifier.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compclassf.classify(compclassf, objectsf, subset, ...)

## S3 method for class 'compclassf'
predict(object, objectsf, subset, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="compclassf.classify_+3A_compclassf">compclassf</code>, <code id="compclassf.classify_+3A_object">object</code></td>
<td>

<p>Functional componentwise classifier (obtained by <code><a href="#topic+compclassf.train">compclassf.train</a></code>).
</p>
</td></tr>
<tr><td><code id="compclassf.classify_+3A_objectsf">objectsf</code></td>
<td>
<p>list containing lists (functions) of two vectors of equal length, named &quot;args&quot; and &quot;vals&quot;: arguments sorted in ascending order and corresponding them values respectively
</p>
</td></tr>
<tr><td><code id="compclassf.classify_+3A_subset">subset</code></td>
<td>

<p>an optional vector specifying a subset of observations to be classified.
</p>
</td></tr>
<tr><td><code id="compclassf.classify_+3A_...">...</code></td>
<td>

<p>additional parameters, passed to the classifier, selected with parameter <code>classifier.type</code> in <code><a href="#topic+compclassf.train">compclassf.train</a></code>.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List containing class labels.
</p>


<h3>References</h3>

<p>Delaigle, A., Hall, P., and Bathia, N. (2012). Componentwise classification and clustering of functional data. <em>Biometrika</em> <b>99</b> 299&ndash;313.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+compclassf.train">compclassf.train</a></code> to train the functional componentwise classifier.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 
## load the Growth dataset
dataf = dataf.growth()

learn = c(head(dataf$dataf, 49), tail(dataf$dataf, 34))
labels =c(head(dataf$labels, 49), tail(dataf$labels, 34)) 
test = tail(head(dataf$dataf, 59), 10)    # elements 50:59. 5 girls, 5 boys

c = compclassf.train (learn, labels, classifier.type = "ddalpha")

classified = compclassf.classify(c, test)

print(unlist(classified))


## End(Not run)
</code></pre>

<hr>
<h2 id='compclassf.train'>
Functional Componentwise Classifier
</h2><span id='topic+compclassf.train'></span>

<h3>Description</h3>

<p>Trains the functional componentwise classifier
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compclassf.train (dataf, labels, subset,
                  to.equalize = TRUE, 
                  to.reduce = TRUE, 
                  classifier.type = c("ddalpha", "maxdepth", "knnaff", "lda", "qda"), 
                  ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="compclassf.train_+3A_dataf">dataf</code></td>
<td>

<p>list containing lists (functions) of two vectors of equal length, named &quot;args&quot; and &quot;vals&quot;: arguments sorted in ascending order and corresponding them values respectively
</p>
</td></tr>
<tr><td><code id="compclassf.train_+3A_labels">labels</code></td>
<td>

<p>list of output labels of the functional observations
</p>
</td></tr>
<tr><td><code id="compclassf.train_+3A_subset">subset</code></td>
<td>

<p>an optional vector specifying a subset of observations to be used in training the classifier.
</p>
</td></tr>
<tr><td><code id="compclassf.train_+3A_to.equalize">to.equalize</code></td>
<td>

<p>Adjust the data to have equal (the largest) argument interval.
</p>
</td></tr>
<tr><td><code id="compclassf.train_+3A_to.reduce">to.reduce</code></td>
<td>

<p>If the data spans a subspace only, project on it (by PCA).
</p>
</td></tr>
<tr><td><code id="compclassf.train_+3A_classifier.type">classifier.type</code></td>
<td>

<p>the classifier which is used on the transformed space. The default value is 'ddalpha'.
</p>
</td></tr>
<tr><td><code id="compclassf.train_+3A_...">...</code></td>
<td>

<p>additional parameters, passed to the classifier, selected with parameter <code>classifier.type</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The finite-dimensional space is directly constructed from the observed values. 
Delaigle, Hall and Bathia (2012) consider (almost) all sets of discretization points
that have a given cardinality. 
</p>
<p>The usual classifiers are then trained on the constructed finite-dimensional space.
</p>


<h3>Value</h3>

<p>Trained functional componentwise classifier
</p>


<h3>References</h3>

<p>Delaigle, A., Hall, P., and Bathia, N. (2012). Componentwise classification and clustering of functional data. <em>Biometrika</em> <b>99</b> 299&ndash;313.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+compclassf.classify">compclassf.classify</a></code> for classification using functional componentwise classifier,
</p>
<p><code><a href="#topic+ddalphaf.train">ddalphaf.train</a></code> to train the functional DD-classifier,
</p>
<p><code><a href="#topic+dataf.+2A">dataf.*</a></code> for functional data sets included in the package.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 
## load the Growth dataset
dataf = dataf.growth()

learn = c(head(dataf$dataf, 49), tail(dataf$dataf, 34))
labels =c(head(dataf$labels, 49), tail(dataf$labels, 34)) 
test = tail(head(dataf$dataf, 59), 10)    # elements 50:59. 5 girls, 5 boys

c = compclassf.train (learn, labels, classifier.type = "ddalpha")

classified = compclassf.classify(c, test)

print(unlist(classified))


## End(Not run)
</code></pre>

<hr>
<h2 id='Custom+20Methods'>
Using Custom Depth Functions and Classifiers
</h2><span id='topic+Custom+20Methods'></span>

<h3>Description</h3>

<p>To use a custom depth function or classifier one has to implement three functions: parameters validator, learning and calculating functions.
</p>


<h3>Details</h3>

<p><b>To define a depth function:</b>
</p>

<dl>
<dt>.&lt;NAME&gt;_validate</dt><dd>
<p>validates parameters passed to <code><a href="#topic+ddalpha.train">ddalpha.train</a></code> and passes them to the <code>ddalpha</code> object.
</p>

<table>
<tr>
 <td style="text-align: left;">IN:</td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>ddalpha</code> </td><td style="text-align: left;"> the ddalpha object, containing the data and settings (mandatory)</td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>&lt;custom parameters&gt;</code> </td><td style="text-align: left;">  parameters that are passed to the user-defined method</td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>...</code> </td><td style="text-align: left;"> other parameters (mandatory)
    
    </td>
</tr>
<tr>
 <td style="text-align: left;">
    OUT:</td>
</tr>
<tr>
 <td style="text-align: left;"> 
    <code>list()</code> </td><td style="text-align: left;"> list of output parameters, after the validation is finished, these parameters are stored in the <code>ddalpha</code> object
    </td>
</tr>

</table>

</dd>
<dt>.&lt;NAME&gt;_learn</dt><dd>
<p>trains the depth
</p>

<table>
<tr>
 <td style="text-align: left;">IN:</td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>ddalpha</code> </td><td style="text-align: left;"> the ddalpha object, containing the data and settings
    
    </td>
</tr>
<tr>
 <td style="text-align: left;">
  MODIFIES:</td>
</tr>
<tr>
 <td style="text-align: left;">
  <code>ddalpha</code> </td><td style="text-align: left;"> store the calculated statistics in the <code>ddalpha</code> object </td>
</tr>
<tr>
 <td style="text-align: left;">
  depths </td><td style="text-align: left;"> calculate the depths of each pattern, e.g. </td>
</tr>
<tr>
 <td style="text-align: left;"> </td><td style="text-align: left;"> <code>for (i in 1:ddalpha$numPatterns) ddalpha$patterns[[i]]$depths = .&lt;NAME&gt;_depths(ddalpha, ddalpha$patterns[[i]]$points)</code>
  
  </td>
</tr>
<tr>
 <td style="text-align: left;">
  OUT:</td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>ddalpha</code> </td><td style="text-align: left;"> the updated <code>ddalpha</code> object
    </td>
</tr>

</table>

</dd>
<dt>.&lt;NAME&gt;_depths</dt><dd>
<p>calculates the depths
</p>

<table>
<tr>
 <td style="text-align: left;">IN:</td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>ddalpha</code> </td><td style="text-align: left;"> the ddalpha object, containing the data and settings </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>objects</code> </td><td style="text-align: left;"> the objects for which the depths are calculated </td>
</tr>
<tr>
 <td style="text-align: left;">
    </td>
</tr>
<tr>
 <td style="text-align: left;">
    OUT:</td>
</tr>
<tr>
 <td style="text-align: left;">
      <code>depths</code> </td><td style="text-align: left;"> the calculated depths for each object (rows), with respect to each class (cols)
      </td>
</tr>

</table>

</dd>
</dl>

<p>Usage: <code>ddalpha.train(data, depth = "&lt;NAME&gt;", &lt;custom parameters&gt;, ...)</code>
</p>
<p><code style="white-space: pre;">&#8288;
#### Custom depths ####

.MyDepth_validate  &lt;- function(ddalpha, mydepth.parameter = "value", ...){ 
  print("MyDepth validating")
  # validate the parameters
  if (!is.valid(mydepth.parameter)){
    warning("Argument \"mydepth.parameter\" not specified correctly. Default value is used")
    mydepth.parameter = "value"
    # or stop("Argument \"mydepth.parameter\" not specified correctly.")
  }
  
  # the values from the return list will be stored in the ddalpha object
  return (list(mydepthpar = mydepth.parameter))
}

.MyDepth_learn &lt;- function(ddalpha){
  print("MyDepth learning")
  
  #1. Calculate any statistics based on data that .MyDepth_depths needs
  #   and store them to the ddalpha object: 
  ddalpha$mydepth.statistic = "some value"
  
  #2. Calculate depths for each pattern
  for (i in 1:ddalpha$numPatterns){
    ddalpha$patterns[[i]]$depths = .MyDepth_depths(ddalpha, ddalpha$patterns[[i]]$points)
  }
  
  return(ddalpha)
}

.MyDepth_depths &lt;- function(ddalpha, objects){
  print("MyDepth calculating")
  depths &lt;- NULL
  
  # The depth parameters are accessible in the ddalpha object:
  mydepth.parameter = ddalpha$mydepth.parameter
  mydepth.statistic = ddalpha$mydepth.statistic
  
  #calculate the depths of the objects w.r.t. each pattern
  for (i in 1:ddalpha$numPatterns){
    depth_wrt_i = #calculate depths of the objects, as vector
    
    depths &lt;- cbind(depths, depth_wrt_i)
  }
    
  return (depths)
}

ddalpha.train(data, depth = "MyDepth", ...)
&#8288;</code>
</p>
<p><b>To define a classifier:</b>
</p>

<dl>
<dt>.&lt;NAME&gt;_validate</dt><dd>
<p>validates parameters passed to <code><a href="#topic+ddalpha.train">ddalpha.train</a></code> and passes them to the <code>ddalpha</code> object
</p>

<table>
<tr>
 <td style="text-align: left;">IN:</td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>ddalpha</code> </td><td style="text-align: left;"> the ddalpha object, containing the data and settings (mandatory)</td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>&lt;custom parameters&gt;</code> </td><td style="text-align: left;">  parameters that are passed to the user-defined method</td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>...</code> </td><td style="text-align: left;"> other parameters (mandatory)
    </td>
</tr>
<tr>
 <td style="text-align: left;">
    OUT:</td>
</tr>
<tr>
 <td style="text-align: left;"> 
    <code>list()</code> </td><td style="text-align: left;"> list of output parameters, after the validation is finished, these parameters are stored in the <code>ddalpha</code> object. 
In case of a multiclass classifier the validator must return <code>methodSeparatorBinary = F</code> and/or pass <code>aggregation.method = "none"</code> to <code>ddalpha.train</code>
    </td>
</tr>

</table>

</dd>
<dt>.&lt;NAME&gt;_learn</dt><dd>
<p>trains the classifier. Is different for binnary and mylticlass classifiers.
</p>

<table>
<tr>
 <td style="text-align: left;">IN:</td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>ddalpha</code> </td><td style="text-align: left;"> the ddalpha object, containing the data and settings</td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>index1</code> </td><td style="text-align: left;"> (only for binary) index of the first class</td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>index2</code> </td><td style="text-align: left;"> (only for binary) index of the second class</td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>depths1</code> </td><td style="text-align: left;"> (only for binary) depths of the first class w.r.t. all classes</td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>depths2</code> </td><td style="text-align: left;"> (only for binary) depths of the second class w.r.t. all classes</td>
</tr>
<tr>
 <td style="text-align: left;"></td>
</tr>
<tr>
 <td style="text-align: left;">
    </td><td style="text-align: left;"> depths w.r.t. only given classes are received by <code>depths1[,c(index1, index2)]</code></td>
</tr>
<tr>
 <td style="text-align: left;"></td>
</tr>
<tr>
 <td style="text-align: left;">
    </td><td style="text-align: left;"> for the multiclass classifiers the depths are accessible via <code>ddalpha$patterns[[i]]$depths</code>
    </td>
</tr>
<tr>
 <td style="text-align: left;">
    OUT:</td>
</tr>
<tr>
 <td style="text-align: left;"> 
    <code>classifier</code> </td><td style="text-align: left;">  the trained <code>classifier</code> object
    </td>
</tr>

</table>

</dd>
<dt>.&lt;NAME&gt;_classify</dt><dd>
<p>classifies the objects
</p>

<table>
<tr>
 <td style="text-align: left;">IN:</td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>ddalpha</code> </td><td style="text-align: left;"> the ddalpha object, containing the data and global settings </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>classifier</code> </td><td style="text-align: left;"> the previously trained classifier</td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>objects</code> </td><td style="text-align: left;"> the objects (depths) that are classified </td>
</tr>
<tr>
 <td style="text-align: left;">
    </td>
</tr>
<tr>
 <td style="text-align: left;">
    OUT:</td>
</tr>
<tr>
 <td style="text-align: left;">
      <code>result</code> </td><td style="text-align: left;">  a vector with classification results</td>
</tr>
<tr>
 <td style="text-align: left;">
        </td><td style="text-align: left;"> (binary) the objects from class <code>"classifier$index1"</code> get positive values </td>
</tr>
<tr>
 <td style="text-align: left;">
        </td><td style="text-align: left;"> (multiclass) the objects get the numbers of patterns in <code>ddalpha</code>
    </td>
</tr>

</table>

</dd>
</dl>

<p>Usage: <code>ddalpha.train(data, separator = "&lt;NAME&gt;", ...)</code>
</p>
<p><code style="white-space: pre;">&#8288;
#### Custom classifiers ####


.MyClassifier_validate  &lt;- function(ddalpha, my.parameter = "value", ...){
  print("MyClassifier validating")
  # validate the parameters
  ...
  
  # always include methodSeparatorBinary. 
  # TRUE for the binary classifier, FALSE otherwise
  return(list(methodSeparatorBinary = T, 
              my.parameter = my.parameter
              ))
}

# a binary classifier 
# the package takes care of the voting procedures. Just train it as if there are only two classes
.MyClassifier_learn &lt;- function(ddalpha, index1, index2, depths1, depths2){
  print("MyClassifier (binary) learning")
  
  # The parameters are accessible in the ddalpha object:
  my.parameter = ddalpha$my.parameter
  
  #depths w.r.t. only given classes are received by
  depths1[,c(index1, index2)]
  depths2[,c(index1, index2)]
  
  # train the classifier
  classifier &lt;-  ...
  
  #return the needed values in a list, e.g.
  return(list(
    coefficients  = classifier$coefficients,
    ... ))
}

# a multiclass classifier 
.MyClassifier_learn &lt;- function(ddalpha){
  print("MyClassifier (multiclass) learning")
  
  # access the data through the ddalpha object, e.g.
  for (i in 1:ddalpha$numPatterns){
    depth  &lt;- ddalpha$patterns[[i]]$depths
    number &lt;- ddalpha$patterns[[i]]$cardinality
    ...
  }
  
  # train the classifier
  classifier &lt;-  ...
  
  # return the classifier
  return(classifier)
}

# the interface of the classify function is equal for binary and multiclass classifiers
.MyClassifier_classify &lt;- function(ddalpha, classifier, depths){
  print("MyClassifier classifying")
  
  # The global parameters are accessible in the ddalpha object:
  my.parameter = ddalpha$my.parameter
  # The parameters generated by .MyClassifier_learn are accessible in the classifier object:
  classifier$coefficients
  
  # here are the depths w.r.t. the first class
  depths[,classifier$index1]
  # here are the depths w.r.t. the second class
  depths[,classifier$index2]
  
  # fill results in a vector, so that:
  # (binary) the objects from class "classifier$index1" get positive values
  # (multiclass) the objects get the numbers of patterns in ddalpha
  result &lt;- ...
  
  return(result)
}

ddalpha.train(data, separator = "MyClassifier", ...)
&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ddalpha.train">ddalpha.train</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 

#### example:  Euclidean depth ####

#.Euclidean_validate is basically not needed

.Euclidean_learn &lt;- function(ddalpha){
  print("Euclidean depth learning")
  
  #1. Calculate any statistics based on data that .MyDepth_depths needs
  #   and store them to the ddalpha object: 
  for (i in 1:ddalpha$numPatterns){
    ddalpha$patterns[[i]]$center &lt;- colMeans(ddalpha$patterns[[i]]$points)
  }
  
  #2. Calculate depths for each pattern
  for (i in 1:ddalpha$numPatterns){
    ddalpha$patterns[[i]]$depths = .Euclidian_depths(ddalpha, ddalpha$patterns[[i]]$points)
  }
  
  return(ddalpha)
}

.Euclidean_depths &lt;- function(ddalpha, objects){
  print("Euclidean depth calculating")
  depths &lt;- NULL
  
  #calculate the depths of the objects w.r.t. each pattern
  for (i in 1:ddalpha$numPatterns){
    # The depth parameters are accessible in the ddalpha object:
    center = ddalpha$patterns[[i]]$center
    
    depth_wrt_i &lt;- 1/(1 + colSums((t(objects) - center)^2))
    depths &lt;- cbind(depths, depth_wrt_i)
  }
  
  return (depths)
}

#### example:  binary decision tree ####

library(rpart)

.tree_validate  &lt;- function(ddalpha, ...){
  print("tree validating")
  return(list(methodSeparatorBinary = T))
}

# a binary classifier 
# the package takes care of the voting procedures. Just train it as if there are only two classes
.tree_learn &lt;- function(ddalpha, index1, index2, depths1, depths2){
  print("tree learning")
  
  # prepare the data
  data = as.data.frame(cbind( (rbind(depths1, depths2)), 
                              c(rep(1, times = nrow(depths1)), rep(-1, times = nrow(depths1)))))
  names(data) &lt;- paste0("V",seq_len(ncol(data)))
  names(data)[ncol(data)] &lt;- "O"
  # train the classifier
  classifier &lt;- rpart(O~., data = data)
  
  #return the needed values in a list, e.g.
  return(classifier)
}

# the interface of the classify function is equal for binary and multiclass classifiers
.tree_classify &lt;- function(ddalpha, classifier, depths){
  print("tree classifying")
  
  # fill results in a vector, so that the objects from class "classifier$index1" get positive values
  data = as.data.frame(depths)
  names(data) &lt;- paste0("V",seq_len(ncol(data)))
  result &lt;- predict(classifier, as.data.frame(depths), type = "vector")
  
  return(result)
}




#### checking ####

library(ddalpha)
data = getdata("hemophilia")

ddalpha = ddalpha.train(data = data, depth = "Euclidean", separator = "tree")
c = ddalpha.classify(ddalpha, data[,1:2])
errors = sum(unlist(c) != data[,3])/nrow(data)
print(paste("Error rate: ",errors))

# building the depth contours using the custom depth
depth.contours.ddalpha(ddalpha, asp = T, levels = seq(0.5, 1, length.out = 10))


## End(Not run)

</code></pre>

<hr>
<h2 id='dataf'>
Converts data from fdata class to the functional class.
</h2><span id='topic+dataf'></span>

<h3>Description</h3>

<p><span class="pkg">fda.usc</span> contains a handy function <code><a href="fda.usc.html#topic+fdata">fdata</a></code> that converts varios types of functional data to the <code>fdata</code> class. 
To use these data in <code><a href="#topic+ddalphaf.train">ddalphaf.train</a></code> it must first be converted with <code>dataf</code>.
</p>
<p>The function may be used either to convert a fdata object that contains multiple classes, or to convert multiple fdata objects, each of which contains one class.
</p>
<p>Note that <code>fdata$fdata2d = TRUE</code> is not supported.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
dataf(fdatas, labels) 

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dataf_+3A_fdatas">fdatas</code></td>
<td>
 
<p>an <code>fdata</code> object with curves belonging to multiple classes, or a list of <code>fdata</code> objects, each of which contains curves of the same class
</p>
</td></tr>
<tr><td><code id="dataf_+3A_labels">labels</code></td>
<td>

<p>a list of labels of the functional observations.
If <code>fdatas</code> is a single <code>fdata</code> object, the list contains labels for each curve. 
If <code>fdatas</code> is a list of <code>fdata</code> objects, the list labels for each of these <code>fdata</code> objects. 
</p>
</td></tr>
</table>


<h3>Format</h3>

<p>The functional data as a data structure (see <code><a href="#topic+dataf.+2A">dataf.*</a></code>).
</p>

<dl>
<dt><code>dataf</code></dt><dd>
<p>The functional data as a list of objects. Each object is characterized by two coordinates 
</p>

<dl>
<dt><code>args</code></dt><dd><p>The arguments vector</p>
</dd>
<dt><code>vals</code></dt><dd><p>The values vector</p>
</dd>
</dl>

</dd>
<dt><code>labels</code></dt><dd><p>The classes of the objects</p>
</dd>
</dl>



<h3>See Also</h3>

<p><code><a href="#topic+dataf.+2A">dataf.*</a></code> for the functional data format.
</p>
<p><code><a href="#topic+ddalphaf.train">ddalphaf.train</a></code> to train the functional DD<code class="reqn">\alpha</code>-classifier
</p>
<p><code><a href="#topic+compclassf.train">compclassf.train</a></code> to train the functional componentwise classifier
</p>
<p><code><a href="#topic+plot.functional">plot.functional</a></code> for building plots of functional data
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(fda.usc)
data(phoneme)

# 1. convert a fdata object that contains multiple classes.
#    labels are defined for each curve
converted = dataf(phoneme$learn, phoneme$classlearn)
plot.functional(converted)

# 2. convert multiple fdata objects, each of which contains one class
#    the same label is applied to all curves of each fdata object
converted = dataf(list(phoneme$learn, phoneme$test), c("1 red", "2 blue"))
converted$name = "Phoneme learn (red) and test (blue)"
plot.functional(converted)

## End(Not run)
</code></pre>

<hr>
<h2 id='dataf.+2A'>
Functional Data Sets
</h2><span id='topic+dataf.+2A'></span>

<h3>Description</h3>

<p>The functions generate data sets of functional two-dimensional data of two or more classes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'># dataf.[name]()

# load the data set by name
# data(list = "name")

# load the data set by name to a variable
# getdata("name")
</code></pre>


<h3>Format</h3>

<p>The functional data as a data structure.
</p>

<dl>
<dt><code>dataf</code></dt><dd>
<p>The functional data as a list of objects. Each object is characterized by two coordinates 
</p>

<dl>
<dt><code>args</code></dt><dd><p>The arguments vector</p>
</dd>
<dt><code>vals</code></dt><dd><p>The values vector</p>
</dd>
</dl>

</dd>
<dt><code>labels</code></dt><dd><p>The classes of the objects</p>
</dd>
</dl>



<h3>Details</h3>

<p>More details about the datasets in the topics:
</p>
<p><code><a href="#topic+dataf.geneexp">dataf.geneexp</a></code>
</p>
<p><code><a href="#topic+dataf.growth">dataf.growth</a></code>
</p>
<p><code><a href="#topic+dataf.medflies">dataf.medflies</a></code>
</p>
<p><code><a href="#topic+dataf.population">dataf.population</a></code>
</p>
<p><code><a href="#topic+dataf.population2010">dataf.population2010</a></code>
</p>
<p><code><a href="#topic+dataf.tecator">dataf.tecator</a></code>
</p>
<p><code><a href="#topic+dataf.tecator">dataf.tecator</a></code>
</p>
<p>The following datasets provide simulated data:
</p>
<p><code><a href="#topic+dataf.sim.1.CFF07">dataf.sim.1.CFF07</a></code>
</p>
<p><code><a href="#topic+dataf.sim.2.CFF07">dataf.sim.2.CFF07</a></code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plot.functional">plot.functional</a></code> for building plots of functional data
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## load the Growth dataset
dataf = dataf.growth()

## view the classes
unique(dataf$labels)

## access the 5th point of the 2nd object
dataf$dataf[[2]]$args[5]
dataf$dataf[[2]]$vals[5]

## Not run: plot.functional(dataf)
</code></pre>

<hr>
<h2 id='dataf.geneexp'>
Gene Expression Profile Data
</h2><span id='topic+dataf.geneexp'></span><span id='topic+geneexp'></span>

<h3>Description</h3>

<p>A subet of the Drosophila life cycle gene expression data of Arbeitman et al. (2002). The original data set contains 77 gene expression profiles during 58 sequential time points from the embryonic, larval, and pupal periods of the life cycle. The gene expression levels were obtained by a cDNA microarray experiment. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dataf.geneexp()
</code></pre>


<h3>Format</h3>

<p>The functional data as a data structure.
</p>

<dl>
<dt><code>dataf</code></dt><dd>
<p>The functional data as a list of objects. Each object is characterized by two coordinates.
</p>

<dl>
<dt><code>args</code></dt><dd><p><b>Time</b> - a numeric vector of time periods</p>
</dd>
<dt><code>vals</code></dt><dd><p><b>Gene Expression Level</b> - a numeric vector</p>
</dd>
</dl>

</dd>
<dt><code>labels</code></dt><dd> 
<p>Biological classifications identified in Arbeitman et al.(2002) (1 = transient early zygotic genes; 2 = muscle-specific genes; 3 = eye-specific genes. )</p>
</dd>
</dl>



<h3>Source</h3>

<p>Chiou, J.-M. and Li, P.-L. Functional clustering and identifying substructures of longitudinal data, J. R. Statist. Soc. B, Volume 69 (2007), 679-699
</p>
<p>Arbeitman, M.N., Furlong, E.E.M., Imam,F., Johnson, E., Null,B.H., Baker,B.S., Krasnow, M.A., Scott,M.P., Davis,R.W. and White,K.P. (2002) Gene expression during the life cycle of Drosophila melanogaster. Science, 297, 2270-2274.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dataf.+2A">dataf.*</a></code> for other functional data sets
</p>
<p><code><a href="#topic+plot.functional">plot.functional</a></code> for building plots of functional data
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## load the dataset
dataf = dataf.geneexp()

## view the classes
unique(dataf$labels)

## access the 5th point of the 2nd object
dataf$dataf[[2]]$args[5]
dataf$dataf[[2]]$vals[5]

## plot the data
## Not run: 
labels = unlist(dataf$labels)
plot(dataf, 
  xlab="Time", ylab="Gene Expression Level", 
  main=paste0("Gene Expression:  1 red (", sum(labels == 1), "), ", 
            "2 green (", sum(labels == 2), "), ", 
            "3 blue (", sum(labels == 3), ")"),
  colors = c("red", "green", "blue"))

## End(Not run)
</code></pre>

<hr>
<h2 id='dataf.growth'>
Berkeley Growth Study Data
</h2><span id='topic+dataf.growth'></span><span id='topic+growth'></span>

<h3>Description</h3>

<p>The data set contains the heights of 39 boys and 54 girls from age 1 to 18 and the ages at which they were collected.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dataf.growth()
</code></pre>


<h3>Format</h3>

<p>The functional data as a data structure.
</p>

<dl>
<dt><code>dataf</code></dt><dd>
<p>The functional data as a list of objects. Each object is characterized by two coordinates 
</p>

<dl>
<dt><code>args</code></dt><dd><p><b>age</b> - a numeric vector of length 31 giving the ages at which the heights were measured</p>
</dd>
<dt><code>vals</code></dt><dd><p><b>height</b> - a numeric vector of heights in centimeters of 39 boys and 54 girls</p>
</dd>
</dl>

</dd>
<dt><code>labels</code></dt><dd><p>The classes of the objects: boy, girl</p>
</dd>
</dl>



<h3>Details</h3>

<p>The ages are not equally spaced.
</p>


<h3>Source</h3>

<p>Ramsay, James O., and Silverman, Bernard W. (2006), Functional Data Analysis, 2nd ed., Springer, New York.
</p>
<p>Ramsay, James O., and Silverman, Bernard W. (2002), Applied Functional Data Analysis, Springer, New York, ch. 6.
</p>
<p>Tuddenham, R. D., and Snyder, M. M. (1954) &quot;Physical growth of California boys and girls from birth to age 18&quot;, University of California Publications in Child Development, 1, 183-364.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dataf.+2A">dataf.*</a></code> for other functional data sets
</p>
<p><code><a href="#topic+plot.functional">plot.functional</a></code> for building plots of functional data
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## load the Growth dataset
dataf = dataf.growth()

## view the classes
unique(dataf$labels)

## access the 5th point of the 2nd object
dataf$dataf[[2]]$args[5]
dataf$dataf[[2]]$vals[5]

## plot the data
## Not run: 
  labels = unlist(dataf$labels)
  plot(dataf, 
    main = paste("Growth: girls red (", sum(labels == "girl"), "),", 
                      " boys blue (", sum(labels == "boy"), ")", sep=""),
    xlab="Year", ylab="Height, cm",
    colors = c("blue", "red")   # in alphabetical order of class labels   
    )

## End(Not run)
</code></pre>

<hr>
<h2 id='dataf.medflies'>
Relationship of Age Patterns of Fecundity to Mortality for Female Medflies.
</h2><span id='topic+dataf.medflies'></span><span id='topic+medflies'></span>

<h3>Description</h3>

<p>The data set consists of number of eggs laid daily for each of 1000 medflies (Mediterranean fruit flies, Ceratitis capitata) until time of death. Data were obtained in Dr. Carey's laboratory. The main questions are to explore the relationship of age patterns of fecundity to mortality, longevity and lifetime reproduction.
</p>
<p>A basic finding was that individual mortality is associated with the time-dynamics of the egg-laying trajectory. An approximate parametric model of the egg laying process was developed and used in Muller et al. (2001). Non-parametric approaches which extend principal component analysis for curve data to the situation when covariates are present have been developed and discussed in  Chiou, Muller and Wang (2003) and Chiou et al. (2003).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dataf.medflies()
</code></pre>


<h3>Format</h3>

<p>The functional data as a data structure.
</p>

<dl>
<dt><code>dataf</code></dt><dd>
<p>The functional data as a list of objects. Each object is characterized by two coordinates.
</p>

<dl>
<dt><code>args</code></dt><dd><p><b>day</b> - a numeric vector of the days numbers</p>
</dd>
<dt><code>vals</code></dt><dd><p><b>#eggs</b> - a numeric vector of numbers of eggs laid daily</p>
</dd>
</dl>

</dd>
<dt><code>labels</code></dt><dd><p>The classes of the objects: long-lived, short-lived</p>
</dd>
</dl>



<h3>Source</h3>

<p>Carey, J.R., Liedo, P., Muller, H.G., Wang, J.L., Chiou, J.M. (1998). Relationship of age patterns of fecundity to mortality, longevity, and lifetime reproduction in a large cohort of Mediterranean fruit fly females. J. of Gerontology &ndash;Biological Sciences 53, 245-251. 
</p>
<p>Muller, H.G., Carey, J.R., Wu, D., Liedo, P., Vaupel, J.W. (2001). Reproductive potential predicts longevity of female Mediterranean fruit flies. Proceedings of the Royal Society B 268, 445-450. 
</p>
<p>Chiou, J.M., Muller, H.G., Wang, J.L. (2003). Functional quasi-likelihood regression models with smooth random effects. J. Royal Statist. Soc. B65, 405-423. 
</p>
<p>Chiou, J.M., Muller, H.G., Wang, J.L., Carey, J.R. (2003). A functional multiplicative effects model for longitudinal data, with application to reproductive histories of female medflies. Statistica Sinica 13, 1119-1133. 
</p>
<p>Chiou, J.M., Muller, H.G., Wang, J.L. (2004).Functional response models. Statistica Sinica 14,675-693.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dataf.+2A">dataf.*</a></code> for other functional data sets
</p>
<p><code><a href="#topic+plot.functional">plot.functional</a></code> for building plots of functional data
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## load the dataset
dataf = dataf.medflies()

## view the classes
unique(dataf$labels)

## access the 5th point of the 2nd object
dataf$dataf[[2]]$args[5]
dataf$dataf[[2]]$vals[5]

## plot the data
## Not run: 
labels = unlist(dataf$labels)
plot(dataf,
  xlab="Day", ylab="# eggs", 
  main=paste("Medflies (training time):\n short-lived red (", sum(labels == "short-lived"), "),", 
                    " long-lived blue (", sum(labels == "long-lived"), ")", sep=""),
  colors = c("blue", "red")   # in alphabetical order of class labels
  )


## End(Not run)
</code></pre>

<hr>
<h2 id='dataf.population'>World Historical Population-by-Country Dataset</h2><span id='topic+dataf.population'></span><span id='topic+population'></span>

<h3>Description</h3>

<p>Historical world population data by countries.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dataf.population()
</code></pre>


<h3>Format</h3>

<p>The functional data as a data structure.
</p>

<dl>
<dt><code>dataf</code></dt><dd>
<p>The functional data as a list of objects. Each object is characterized by two coordinates 
</p>

<dl>
<dt><code>args</code></dt><dd><p><b>year</b> - a numeric vector of years 1950-2015 (66 years)</p>
</dd>
<dt><code>vals</code></dt><dd><p><b>population</b> - a numeric vector of the estimated total population in thousands in 233 countries and regions</p>
</dd>
</dl>

</dd>
<dt><code>labels</code></dt><dd><p>The geographic region of the country: Africa, Asia, Europe, Latin America, North America, Oceania</p>
</dd>
<dt><code>identifier</code></dt><dd><p>The name of country or region</p>
</dd>
</dl>



<h3>Details</h3>

<p>World population data by a country, area or region as of 1 July 
of the year indicated. Figures are presented in thousands.
</p>


<h3>Source</h3>

<p>United Nations, Department of Economic and Social Affairs,
Population Division,
<a href="https://esa.un.org/unpd/wpp/Download/Standard/Population/">https://esa.un.org/unpd/wpp/Download/Standard/Population/</a>, 
file <code>Total population - Both sexes</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dataf.population2010">dataf.population2010</a></code>
</p>
<p><code><a href="#topic+dataf.+2A">dataf.*</a></code> for other functional data sets
</p>
<p><code><a href="#topic+plot.functional">plot.functional</a></code> for building plots of functional data
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## load the Population dataset
dataf = dataf.population()

## view the classes
unique(dataf$labels)

## access the 5th point of the 2nd object
dataf$dataf[[2]]$args[5]
dataf$dataf[[2]]$vals[5]

## plot the data
## Not run:
labels = unlist(dataf$labels)
plot(dataf,
  main = "World population data",
  xlab="Year", ylab="Population (in thousands)"
  )
## End(Not run)

## compute the integrated and infimal depths of the data curves
## with respect to the same set of curves
depthf.fd1(dataf$dataf, dataf$dataf)
</code></pre>

<hr>
<h2 id='dataf.population2010'>World Historical Population-by-Country Dataset (2010 Revision)</h2><span id='topic+dataf.population2010'></span><span id='topic+population2010'></span>

<h3>Description</h3>

<p>Historical world population data by countries.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dataf.population2010()
</code></pre>


<h3>Format</h3>

<p>The functional data as a data structure.
</p>

<dl>
<dt><code>dataf</code></dt><dd>
<p>The functional data as a list of objects. Each object is characterized by two coordinates 
</p>

<dl>
<dt><code>args</code></dt><dd><p><b>year</b> - a numeric vector of years 1950-2010 (61 years)</p>
</dd>
<dt><code>vals</code></dt><dd><p><b>population</b> - a numeric vector of the estimated total population in thousands in 233 countries and regions</p>
</dd>
</dl>

</dd>
<dt><code>labels</code></dt><dd><p>The geographic region of the country</p>
</dd>
<dt><code>identifier</code></dt><dd><p>The name of country or region</p>
</dd>
</dl>



<h3>Details</h3>

<p>World population data by a country, area or region as of 1 July 
of the year indicated. Figures are presented in thousands.
</p>


<h3>Source</h3>

<p>United Nations, Department of Economic and Social Affairs,
Population Division,
<a href="https://esa.un.org/unpd/wpp/Download/Standard/Population/">https://esa.un.org/unpd/wpp/Download/Standard/Population/</a>, 
file <code>Total population - Both sexes</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dataf.population">dataf.population</a></code>
</p>
<p><code><a href="#topic+dataf.+2A">dataf.*</a></code> for other functional data sets
</p>
<p><code><a href="#topic+plot.functional">plot.functional</a></code> for building plots of functional data
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## load the Population dataset
dataf = dataf.population2010()

## view the classes
unique(dataf$labels)

## access the 5th point of the 2nd object
dataf$dataf[[2]]$args[5]
dataf$dataf[[2]]$vals[5]

## plot the data
## Not run:
labels = unlist(dataf$labels)
plot(dataf,
  main = "World population data",
  xlab="Year", ylab="Population (in thousands)"
  )
## End(Not run)

## compute the integrated and infimal depths of the data curves
## with respect to the same set of curves
depthf.fd1(dataf$dataf, dataf$dataf)
</code></pre>

<hr>
<h2 id='dataf.sim.1.CFF07'>
Model 1 from Cuevas et al. (2007)
</h2><span id='topic+dataf.sim.1.CFF07'></span>

<h3>Description</h3>

<p>Model 1 from Cuevas et al. (2007)
</p>
<p>Processes: <br />
X(t) = m_0(t) + e(t), m_0(t) = 30*(1-t)*t^1.2 <br />
Y(t) = m_1(t) + e(t), m_1(t) = 30*(1-t)^1.2*t <br />
e(t): Gaussian with mean = 0, cov(X(s), X(t)) = 0.2*exp(-abs(s - t)/0.3)<br />
the processes are discretized at <code>numDiscrets</code> equally distant points on [0, 1]. The functions
are smooth and differ in mean only, which makes the classification task rather simple.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dataf.sim.1.CFF07(numTrain = 100, numTest = 50, numDiscrets = 51, plot = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dataf.sim.1.CFF07_+3A_numtrain">numTrain</code></td>
<td>

<p>number of objects in the training sample
</p>
</td></tr>
<tr><td><code id="dataf.sim.1.CFF07_+3A_numtest">numTest</code></td>
<td>

<p>number of objects in the test sample
</p>
</td></tr>
<tr><td><code id="dataf.sim.1.CFF07_+3A_numdiscrets">numDiscrets</code></td>
<td>

<p>number of points for each object
</p>
</td></tr>
<tr><td><code id="dataf.sim.1.CFF07_+3A_plot">plot</code></td>
<td>

<p>if TRUE the training sample is plotted
</p>
</td></tr>
</table>


<h3>Format</h3>

<p>A data strusture containing <code>$learn</code> and <code>$test</code> functional data.
The functional data are given as data structures.
</p>

<dl>
<dt><code>dataf</code></dt><dd>
<p>The functional data as a list of objects. Each object is characterized by two coordinates.
</p>

<dl>
<dt><code>args</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>vals</code></dt><dd><p>a numeric vector</p>
</dd>
</dl>

</dd>
<dt><code>labels</code></dt><dd><p>The classes of the objects: 0 for X(t), 1 for Y(t)</p>
</dd>
</dl>



<h3>Source</h3>

<p>Cuevas, A., Febrero, M. and Fraiman, R. (2007). Robust estimation and classification for functional data via projection-based depth notions. Computational Statistics 22 481-496.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dataf.+2A">dataf.*</a></code> for other functional data sets
</p>
<p><code><a href="#topic+plot.functional">plot.functional</a></code> for building plots of functional data
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## load the dataset
dataf = dataf.sim.1.CFF07(numTrain = 100, numTest = 50, numDiscrets = 51)
learn = dataf$learn
test = dataf$test

## view the classes
unique(learn$labels)

## access the 5th point of the 2nd object
learn$dataf[[2]]$args[5]
learn$dataf[[2]]$vals[5]

## Not run: 
## plot the data
plot(learn)
plot(test)

## or
dataf = dataf.sim.1.CFF07(numTrain = 100, numTest = 50, numDiscrets = 51, plot = TRUE)

## End(Not run)

</code></pre>

<hr>
<h2 id='dataf.sim.2.CFF07'>
Model 2 from Cuevas et al. (2007)
</h2><span id='topic+dataf.sim.2.CFF07'></span>

<h3>Description</h3>

<p>Model 2 from Cuevas et al. (2007)
</p>
<p>Processes: <br />
X(t) = m_0(t) + e(t), m_0(t) = 30*(1-t)*t^2 + 0.5*abs(sin(20*pi*t)) <br />
Y(t) = an 8-knot spline approximation of X <br />
e(t): Gaussian with mean = 0, cov(X(s), X(t)) = 0.2*exp(-abs(s - t)/0.3)<br />
the processes are discretized at <code>numDiscrets</code> equally distant points on [0, 1].
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dataf.sim.2.CFF07(numTrain = 100, numTest = 50, numDiscrets = 51, plot = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dataf.sim.2.CFF07_+3A_numtrain">numTrain</code></td>
<td>

<p>number of objects in the training sample
</p>
</td></tr>
<tr><td><code id="dataf.sim.2.CFF07_+3A_numtest">numTest</code></td>
<td>

<p>number of objects in the test sample
</p>
</td></tr>
<tr><td><code id="dataf.sim.2.CFF07_+3A_numdiscrets">numDiscrets</code></td>
<td>

<p>number of points for each object
</p>
</td></tr>
<tr><td><code id="dataf.sim.2.CFF07_+3A_plot">plot</code></td>
<td>

<p>if TRUE the training sample is plotted
</p>
</td></tr>
</table>


<h3>Format</h3>

<p>A data strusture containing <code>$learn</code> and <code>$test</code> functional data.
The functional data are given as data structures.
</p>

<dl>
<dt><code>dataf</code></dt><dd>
<p>The functional data as a list of objects. Each object is characterized by two coordinates.
</p>

<dl>
<dt><code>args</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>vals</code></dt><dd><p>a numeric vector</p>
</dd>
</dl>

</dd>
<dt><code>labels</code></dt><dd><p>The classes of the objects: 0 for X(t), 1 for Y(t)</p>
</dd>
</dl>



<h3>Source</h3>

<p>Cuevas, A., Febrero, M. and Fraiman, R. (2007). Robust estimation and classification for functional data via projection-based depth notions. Computational Statistics 22 481-496.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dataf.+2A">dataf.*</a></code> for other functional data sets
</p>
<p><code><a href="#topic+plot.functional">plot.functional</a></code> for building plots of functional data
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## load the dataset
dataf = dataf.sim.2.CFF07(numTrain = 100, numTest = 50, numDiscrets = 51)
learn = dataf$learn
test = dataf$test

## view the classes
unique(learn$labels)

## access the 5th point of the 2nd object
learn$dataf[[2]]$args[5]
learn$dataf[[2]]$vals[5]

## Not run: 
## plot the data
plot(learn)
plot(test)

## or
dataf = dataf.sim.2.CFF07(numTrain = 100, numTest = 50, numDiscrets = 51, plot = TRUE)

## End(Not run)

</code></pre>

<hr>
<h2 id='dataf.tecator'>
Functional Data Set Spectrometric Data (Tecator)
</h2><span id='topic+dataf.tecator'></span><span id='topic+tecator'></span>

<h3>Description</h3>

<p>This dataset is a part of the original one which can be found at
<a href="https://www.cmu.edu/dietrich/statistics-datascience/">https://www.cmu.edu/dietrich/statistics-datascience/</a>. For each peace of finely chopped meat we observe one spectrometric curve which
corresponds to the absorbance measured at 100 wavelengths. 
The peaces are split according to Ferraty and Vieu (2006) into two classes: with small (&lt;20) and large fat
content obtained by an analytical chemical processing.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dataf.tecator()
</code></pre>


<h3>Format</h3>

<p>The functional data as a data structure.
</p>

<dl>
<dt><code>dataf</code></dt><dd>
<p>The functional data as a list of objects. Each object is characterized by two coordinates.
</p>

<dl>
<dt><code>args</code></dt><dd><p><b>wavelength</b> - a numeric vector of discretization points from 850 to 1050mm </p>
</dd>
<dt><code>vals</code></dt><dd><p><b>absorbance</b> - a numeric vector of absorbance values</p>
</dd>
</dl>

</dd>
<dt><code>labels</code></dt><dd><p>The classes of the objects: &quot;small&quot; (&lt;20) and &quot;large&quot; fat content</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Febrero-Bande, M and Oviedo de la Fuente, Manuel
</p>


<h3>Source</h3>

<p><a href="https://www.cmu.edu/dietrich/statistics-datascience/">https://www.cmu.edu/dietrich/statistics-datascience/</a>
</p>


<h3>References</h3>

<p>Ferraty, F. and Vieu, P. (2006). <em>Nonparametric functional data analysis: theory and practice</em>. Springer.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dataf.+2A">dataf.*</a></code> for other functional data sets
</p>
<p><code><a href="#topic+plot.functional">plot.functional</a></code> for building plots of functional data
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## load the dataset
dataf = dataf.tecator()

## view the classes
unique(dataf$labels)

## access the 5th point of the 2nd object
dataf$dataf[[2]]$args[5]
dataf$dataf[[2]]$vals[5]

## plot the data
## Not run: 
labels = unlist(dataf$labels)
plot(dataf, 
  xlab="Wavelengths", ylab="Absorbances", 
  main=paste("Tecator: &lt; 20 red (", sum(labels == "small"), "),", 
            " &gt;= 20 blue (", sum(labels == "large"), ")", sep=""),
    colors = c("blue", "red"))

## End(Not run)
</code></pre>

<hr>
<h2 id='dataf2rawfd'>Transform a <code>dataf</code> Object to Raw Functional Data</h2><span id='topic+dataf2rawfd'></span>

<h3>Description</h3>

<p>From a (possibly multivariate) functional data object <code>dataf</code> constructs an array of the functional values
evaluated at an equi-distant grid of points.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dataf2rawfd(dataf, range = NULL, d = 101)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dataf2rawfd_+3A_dataf">dataf</code></td>
<td>
<p>Functions to be transformed, represented by a (possibly multivariate) <code>dataf</code> object of their arguments
and functional values. <code>m</code> stands for the number of functions. The grid of observation points for the 
functions in <code>dataf</code> may not be the same.</p>
</td></tr>
<tr><td><code id="dataf2rawfd_+3A_range">range</code></td>
<td>
<p>The common range of the domain where the functions <code>dataf</code> are observed.
Vector of length 2 with the left and the right end of the interval. Must contain all arguments given in 
<code>dataf</code>. If the range is not provided, the smallest interval in which all the arguments from the data functions
are contained is chosen as the domain.</p>
</td></tr>
<tr><td><code id="dataf2rawfd_+3A_d">d</code></td>
<td>
<p>Grid size to which all the functional data are transformed. All functional observations are 
transformed into vectors of their functional values of length <code>d</code>
corresponding to equi-spaced points in the domain given by the interval <code>range</code>. Functional values in these
points are reconstructed using linear interpolation, and extrapolation, see Nagy et al. (2016).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>If the functional data are univariate (scalar-valued), a matrix of size <code>m*d</code> is given, with each row
corresponding to one function. If the functional data are <code>k</code>-variate with k&gt;1, an array of size <code>m*d*k</code>
of the functional values is given.
</p>


<h3>Author(s)</h3>

<p>Stanislav Nagy, <a href="mailto:nagy@karlin.mff.cuni.cz">nagy@karlin.mff.cuni.cz</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rawfd2dataf">rawfd2dataf</a></code>
</p>
<p><code><a href="#topic+depthf.fd1">depthf.fd1</a></code>
</p>
<p><code><a href="#topic+depthf.fd2">depthf.fd2</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## transform a matrix into a functional data set and back
n = 5
d = 21
X = matrix(rnorm(n*d),ncol=d)
R = rawfd2dataf(X,range=c(0,1))
R2 = dataf2rawfd(R,range=c(0,1),d=d)
all.equal(X,R2)

## transform a functional dataset into a raw matrix of functional values
dataf = dataf.population()$dataf
dataf2rawfd(dataf,range=c(1950,2015),d=66)

## transform an array into a multivariate functional data set and back
k = 3
X = array(rnorm(n*d*k),dim=c(n,d,k))
R = rawfd2dataf(X,range=c(-1,1))
dataf2rawfd(R,range=c(-1,1),d=50)

</code></pre>

<hr>
<h2 id='ddalpha.classify'>
Classify using DD-Classifier
</h2><span id='topic+ddalpha.classify'></span><span id='topic+predict.ddalpha'></span>

<h3>Description</h3>

<p>Classifies data using the DD-classifier and a specified outsider treatment.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ddalpha.classify(ddalpha, objects, subset, outsider.method = NULL, use.convex = NULL)

## S3 method for class 'ddalpha'
predict(object, objects, subset, outsider.method = NULL, use.convex = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ddalpha.classify_+3A_ddalpha">ddalpha</code>, <code id="ddalpha.classify_+3A_object">object</code></td>
<td>

<p>DD<code class="reqn">\alpha</code>-classifier (obtained by <code><a href="#topic+ddalpha.train">ddalpha.train</a></code>).
</p>
</td></tr>
<tr><td><code id="ddalpha.classify_+3A_objects">objects</code></td>
<td>

<p>Matrix containing objects to be classified; each row is one <code class="reqn">d</code>-dimensional object.
</p>
</td></tr>
<tr><td><code id="ddalpha.classify_+3A_subset">subset</code></td>
<td>

<p>an optional vector specifying a subset of observations to be classified.
</p>
</td></tr>
<tr><td><code id="ddalpha.classify_+3A_outsider.method">outsider.method</code></td>
<td>

<p>Character string, name of a treatment to be used for outsiders; one of those trained by <code><a href="#topic+ddalpha.train">ddalpha.train</a></code>. If the treatment was specified using the argument <code>outsider.methods</code> then use the name of the method.
</p>
</td></tr>
<tr><td><code id="ddalpha.classify_+3A_use.convex">use.convex</code></td>
<td>

<p>Logical variable indicating whether outsiders should be determined as the points not contained in any of the convex hulls of the classes from the training sample (<code>TRUE</code>) or those having zero depth w.r.t. each class from the training sample (<code>FALSE</code>). For <code>depth =</code> <code>"zonoid"</code> both values give the same result. If <code>NULL</code> the value specified in DD<code class="reqn">\alpha</code>-classifier (in <code><a href="#topic+ddalpha.train">ddalpha.train</a></code>) is used.
</p>
</td></tr>
<tr><td><code id="ddalpha.classify_+3A_...">...</code></td>
<td>

<p>additional parameters are ignored
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Only one outsider treatment can be specified.
</p>
<p>See Lange, Mosler and Mozharovskyi (2014) for details and additional information.
</p>


<h3>Value</h3>

<p>List containing class labels, or character string &quot;Ignored&quot; for the outsiders if &quot;Ignore&quot; was specified as the outsider treating method.
</p>


<h3>References</h3>

<p>Dyckerhoff, R., Koshevoy, G., and Mosler, K. (1996). Zonoid data depth: theory and computation. In: Prat A. (ed), <em>COMPSTAT 1996. Proceedings in computational statistics</em>, Physica-Verlag (Heidelberg), 235&ndash;240.
</p>
<p>Lange, T., Mosler, K., and Mozharovskyi, P. (2014). Fast nonparametric classification based on data depth. <em>Statistical Papers</em> <b>55</b> 49&ndash;69.
</p>
<p>Li, J., Cuesta-Albertos, J.A., and Liu, R.Y. (2012). DD-classifier: Nonparametric classification procedure based on DD-plot. <em>Journal of the American Statistical Association</em> <b>107</b> 737&ndash;753.
</p>
<p>Mozharovskyi, P. (2015). <em>Contributions to Depth-based Classification and Computation of the Tukey Depth</em>. Verlag Dr. Kovac (Hamburg).
</p>
<p>Mozharovskyi, P., Mosler, K., and Lange, T. (2015). Classifying real-world data with the DD<code class="reqn">\alpha</code>-procedure. <em>Advances in Data Analysis and Classification</em> <b>9</b> 287&ndash;314.
</p>
<p>Vasil'ev, V.I. (2003). The reduction principle in problems of revealing regularities I. <em>Cybernetics and Systems Analysis</em> <b>39</b> 686&ndash;694.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ddalpha.train">ddalpha.train</a></code> to train the DD-classifier.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Generate a bivariate normal location-shift classification task
# containing 200 training objects and 200 to test with
class1 &lt;- mvrnorm(200, c(0,0), 
                  matrix(c(1,1,1,4), nrow = 2, ncol = 2, byrow = TRUE))
class2 &lt;- mvrnorm(200, c(2,2), 
                  matrix(c(1,1,1,4), nrow = 2, ncol = 2, byrow = TRUE))
trainIndices &lt;- c(1:100)
testIndices &lt;- c(101:200)
propertyVars &lt;- c(1:2)
classVar &lt;- 3
trainData &lt;- rbind(cbind(class1[trainIndices,], rep(1, 100)), 
                   cbind(class2[trainIndices,], rep(2, 100)))
testData &lt;- rbind(cbind(class1[testIndices,], rep(1, 100)), 
                  cbind(class2[testIndices,], rep(2, 100)))
data &lt;- list(train = trainData, test = testData)

# Train the DDalpha-Classifier (zonoid depth, maximum Mahalanobis depth 
# classifier with defaults as outsider treatment)
ddalpha &lt;- ddalpha.train(data$train, 
                         depth = "zonoid", 
                         outsider.methods = "depth.Mahalanobis")
# Get the classification error rate
classes &lt;- ddalpha.classify(data$test[,propertyVars], ddalpha, 
                            outsider.method = "depth.Mahalanobis")
cat("Classification error rate: ", 
    sum(unlist(classes) != data$test[,classVar])/200, ".\n", sep="")
</code></pre>

<hr>
<h2 id='ddalpha.getErrorRateCV'>
Test DD-Classifier
</h2><span id='topic+ddalpha.getErrorRateCV'></span>

<h3>Description</h3>

<p>Performs a cross-validation procedure over the given data. 
On each step every <code>numchunks</code> observation is removed from the data, the DD-classifier is trained on these data and tested on the removed observations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ddalpha.getErrorRateCV (data, numchunks = 10,  ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ddalpha.getErrorRateCV_+3A_data">data</code></td>
<td>

<p>Matrix containing training sample where each of <code class="reqn">n</code> rows is one object of the training sample where first <code class="reqn">d</code> entries are inputs and the last entry is output (class label).
</p>
</td></tr>
<tr><td><code id="ddalpha.getErrorRateCV_+3A_numchunks">numchunks</code></td>
<td>

<p>number of subsets of testing data. Equals to the number of times the classifier is trained.
</p>
</td></tr>
<tr><td><code id="ddalpha.getErrorRateCV_+3A_...">...</code></td>
<td>

<p>additional parameters passed to <code><a href="#topic+ddalpha.train">ddalpha.train</a></code>
</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>errors</code></td>
<td>

<p>the part of incorrectly classified data
</p>
</td></tr>
<tr><td><code>time</code></td>
<td>

<p>the mean training time
</p>
</td></tr>
<tr><td><code>time_sd</code></td>
<td>

<p>the standard deviation of training time
</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+ddalpha.train">ddalpha.train</a></code> to train the DD<code class="reqn">\alpha</code>-classifier, 
<code><a href="#topic+ddalpha.classify">ddalpha.classify</a></code> for classification using DD<code class="reqn">\alpha</code>-classifier, 
<code><a href="#topic+ddalpha.test">ddalpha.test</a></code> to test the DD-classifier on particular learning and testing data,
<code><a href="#topic+ddalpha.getErrorRatePart">ddalpha.getErrorRatePart</a></code> to perform a benchmark study of the DD-classifier on particular data.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Generate a bivariate normal location-shift classification task
# containing 200 training objects and 200 to test with
class1 &lt;- mvrnorm(150, c(0,0), 
                  matrix(c(1,1,1,4), nrow = 2, ncol = 2, byrow = TRUE))
class2 &lt;- mvrnorm(150, c(2,2), 
                  matrix(c(1,1,1,4), nrow = 2, ncol = 2, byrow = TRUE))
propertyVars &lt;- c(1:2)
classVar &lt;- 3
data &lt;- rbind(cbind(class1, rep(1, 150)), cbind(class2, rep(2, 150)))

# Train 1st DDalpha-classifier (default settings) 
# and get the classification error rate
stat &lt;- ddalpha.getErrorRateCV(data, numchunks = 5)
cat("1. Classification error rate (defaults): ", 
    stat$error, ".\n", sep = "")

# Train 2nd DDalpha-classifier (zonoid depth, maximum Mahalanobis 
# depth classifier with defaults as outsider treatment) 
# and get the classification error rate
stat2 &lt;- ddalpha.getErrorRateCV(data, depth = "zonoid", 
                          outsider.methods = "depth.Mahalanobis")
cat("2. Classification error rate (depth.Mahalanobis): ", 
    stat2$error, ".\n", sep = "")


</code></pre>

<hr>
<h2 id='ddalpha.getErrorRatePart'>
Test DD-Classifier
</h2><span id='topic+ddalpha.getErrorRatePart'></span>

<h3>Description</h3>

<p>Performs a benchmark procedure by partitioning the given data. 
On each of <code>times</code> steps <code>size</code> observations are removed from the data, the DD-classifier is trained on these data and tested on the removed observations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ddalpha.getErrorRatePart(data, size = 0.3, times = 10,  ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ddalpha.getErrorRatePart_+3A_data">data</code></td>
<td>

<p>Matrix containing training sample where each of <code class="reqn">n</code> rows is one object of the training sample where first <code class="reqn">d</code> entries are inputs and the last entry is output (class label).
</p>
</td></tr>
<tr><td><code id="ddalpha.getErrorRatePart_+3A_size">size</code></td>
<td>

<p>the excluded sequences size. Either an integer between <code class="reqn">1</code> and <code class="reqn">n</code>, or a fraction of data between <code class="reqn">0</code> and <code class="reqn">1</code>.
</p>
</td></tr>
<tr><td><code id="ddalpha.getErrorRatePart_+3A_times">times</code></td>
<td>

<p>the number of times the classifier is trained.
</p>
</td></tr>
<tr><td><code id="ddalpha.getErrorRatePart_+3A_...">...</code></td>
<td>

<p>additional parameters passed to <code><a href="#topic+ddalpha.train">ddalpha.train</a></code>
</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>errors</code></td>
<td>

<p>the part of incorrectly classified data (mean)
</p>
</td></tr>
<tr><td><code>errors_sd</code></td>
<td>

<p>the standard deviation of errors
</p>
</td></tr>
<tr><td><code>errors_vec</code></td>
<td>

<p>vector of errors
</p>
</td></tr>
<tr><td><code>time</code></td>
<td>

<p>the mean training time
</p>
</td></tr>
<tr><td><code>time_sd</code></td>
<td>

<p>the standard deviation of training time
</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+ddalpha.train">ddalpha.train</a></code> to train the DD<code class="reqn">\alpha</code>-classifier, 
<code><a href="#topic+ddalpha.classify">ddalpha.classify</a></code> for classification using DD<code class="reqn">\alpha</code>-classifier, 
<code><a href="#topic+ddalpha.test">ddalpha.test</a></code> to test the DD-classifier on particular learning and testing data,
<code><a href="#topic+ddalpha.getErrorRateCV">ddalpha.getErrorRateCV</a></code> to get error rate of the DD-classifier on particular data.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Generate a bivariate normal location-shift classification task
# containing 200 objects
class1 &lt;- mvrnorm(100, c(0,0), 
                  matrix(c(1,1,1,4), nrow = 2, ncol = 2, byrow = TRUE))
class2 &lt;- mvrnorm(100, c(2,2), 
                  matrix(c(1,1,1,4), nrow = 2, ncol = 2, byrow = TRUE))
propertyVars &lt;- c(1:2)
classVar &lt;- 3
data &lt;- rbind(cbind(class1, rep(1, 100)), cbind(class2, rep(2, 100)))

# Train 1st DDalpha-classifier (default settings) 
# and get the classification error rate
stat &lt;- ddalpha.getErrorRatePart(data, size = 10, times = 10)
cat("1. Classification error rate (defaults): ", 
    stat$error, ".\n", sep = "")

# Train 2nd DDalpha-classifier (zonoid depth, maximum Mahalanobis 
# depth classifier with defaults as outsider treatment) 
# and get the classification error rate
stat2 &lt;- ddalpha.getErrorRatePart(data, depth = "zonoid", 
                                outsider.methods = "depth.Mahalanobis", size = 0.2, times = 10)
cat("2. Classification error rate (depth.Mahalanobis): ", 
    stat2$error, ".\n", sep = "")



</code></pre>

<hr>
<h2 id='ddalpha.test'>
Test DD-Classifier
</h2><span id='topic+ddalpha.test'></span>

<h3>Description</h3>

<p>Trains DD-classifier on the learning sequence of the data and tests it on the testing sequence.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ddalpha.test(learn, test, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ddalpha.test_+3A_learn">learn</code></td>
<td>

<p>the learning sequence of the data. Matrix containing training sample where each of <code class="reqn">n</code> rows is one object of the training sample where first <code class="reqn">d</code> entries are inputs and the last entry is output (class label).
</p>
</td></tr>
<tr><td><code id="ddalpha.test_+3A_test">test</code></td>
<td>

<p>the testing sequence. Has the same format as <code>learn</code>
</p>
</td></tr>
<tr><td><code id="ddalpha.test_+3A_...">...</code></td>
<td>

<p>additional parameters passed to <code><a href="#topic+ddalpha.train">ddalpha.train</a></code>
</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>error</code></td>
<td>

<p>the part of incorrectly classified data
</p>
</td></tr>
<tr><td><code>correct</code></td>
<td>

<p>the number of correctly classified objects
</p>
</td></tr>
<tr><td><code>incorrect</code></td>
<td>

<p>the number of incorrectly classified objects
</p>
</td></tr>
<tr><td><code>total</code></td>
<td>

<p>the number of classified objects
</p>
</td></tr>
<tr><td><code>ignored</code></td>
<td>

<p>the number of ignored objects (outside the convex hull of the learning data)
</p>
</td></tr>
<tr><td><code>n</code></td>
<td>

<p>the number of objects in the testing sequence
</p>
</td></tr>
<tr><td><code>time</code></td>
<td>

<p>training time
</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+ddalpha.train">ddalpha.train</a></code> to train the DD-classifier, 
<code><a href="#topic+ddalpha.classify">ddalpha.classify</a></code> for classification using DD-classifier, 
<code><a href="#topic+ddalpha.getErrorRateCV">ddalpha.getErrorRateCV</a></code> and <code><a href="#topic+ddalpha.getErrorRatePart">ddalpha.getErrorRatePart</a></code> to get error rate of the DD-classifier on particular data.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Generate a bivariate normal location-shift classification task
# containing 200 training objects and 200 to test with
class1 &lt;- mvrnorm(200, c(0,0), 
                  matrix(c(1,1,1,4), nrow = 2, ncol = 2, byrow = TRUE))
class2 &lt;- mvrnorm(200, c(2,2), 
                  matrix(c(1,1,1,4), nrow = 2, ncol = 2, byrow = TRUE))
trainIndices &lt;- c(1:100)
testIndices &lt;- c(101:200)
propertyVars &lt;- c(1:2)
classVar &lt;- 3
trainData &lt;- rbind(cbind(class1[trainIndices,], rep(1, 100)), 
                   cbind(class2[trainIndices,], rep(2, 100)))
testData &lt;- rbind(cbind(class1[testIndices,], rep(1, 100)), 
                  cbind(class2[testIndices,], rep(2, 100)))
data &lt;- list(train = trainData, test = testData)

# Train 1st DDalpha-classifier (default settings) 
# and get the classification error rate
stat &lt;- ddalpha.test(data$train, data$test)
cat("1. Classification error rate (defaults): ", 
    stat$error, ".\n", sep = "")

# Train 2nd DDalpha-classifier (zonoid depth, maximum Mahalanobis 
# depth classifier with defaults as outsider treatment) 
# and get the classification error rate
stat2 &lt;- ddalpha.test(data$train, data$test, depth = "zonoid", 
                          outsider.methods = "depth.Mahalanobis")
cat("2. Classification error rate (depth.Mahalanobis): ", 
    stat2$error, ".\n", sep = "")

</code></pre>

<hr>
<h2 id='ddalpha.train'>
Train DD-Classifier
</h2><span id='topic+ddalpha.train'></span><span id='topic+alpha'></span><span id='topic+polynomial'></span><span id='topic+knnlm'></span><span id='topic+maxD'></span><span id='topic+outsiders'></span>

<h3>Description</h3>

<p>Trains the DD-classifier using a training sample according to given parameters. 
The DD-classifier is a non-parametric procedure that first transforms the training sample into the depth space calculating the depth of each point w.r.t each class (dimension of this space equals the number of classes in the training sample), and then constructs a separating rule in this depth space. 
If in the classification phase an object does not belong to the convex hull of at least one class (we mention such an object as an 'outsider'), it is mapped into the origin of the depth space and hence cannot be classified in the depth space. For these objects, after 'outsiderness' has been assured, an outsider treatment, i.e. a classification procedure functioning outside convex hulls of the classes is applied; it has to be trained too.
</p>
<p>The current realization of the DD-classifier allows for several alternative outsider treatments; they involve different traditional classification methods, see 'Details' and 'Arguments' for parameters needed. 
</p>
<p>The function allows for classification with <code class="reqn">q\ge 2</code> classes, see <code>aggregation.method</code> in 'Arguments'.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ddalpha.train(formula, data, subset,
              depth = "halfspace", 
              separator = "alpha", 
              outsider.methods = "LDA", 
              outsider.settings = NULL, 
              aggregation.method = "majority",
              pretransform = NULL,
              use.convex = FALSE,     
              seed = 0,
              ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ddalpha.train_+3A_formula">formula</code></td>
<td>

<p>an object of class &ldquo;formula&rdquo; (or one that can be coerced to that class): a symbolic description of the model. If not found in <code>data</code>, the variables of the model are taken from environment.
</p>
</td></tr>
<tr><td><code id="ddalpha.train_+3A_data">data</code></td>
<td>

<p>Matrix or data.frame containing training sample where each of <code class="reqn">n</code> rows is one object of the training sample where first <code class="reqn">d</code> entries are inputs and the last entry is output (class label).
</p>
<p>A pre-calculated DD-plot may be used as <code>data</code> with <code>depth="ddplot"</code>.
</p>
</td></tr>
<tr><td><code id="ddalpha.train_+3A_subset">subset</code></td>
<td>

<p>an optional vector specifying a subset of observations to be used in training the classifier.
</p>
</td></tr>
<tr><td><code id="ddalpha.train_+3A_depth">depth</code></td>
<td>

<p>Character string determining which depth notion to use; the default value is <code>"halfspace"</code>. The list of the supported depths is given in section <em><b><code>Depths</code></b></em>. To use a custom depth, see topic <code><a href="#topic+Custom+20Methods">Custom Methods</a></code>. To use an outsider treatment only set <code>depth = NULL</code>.
</p>
</td></tr>
<tr><td><code id="ddalpha.train_+3A_separator">separator</code></td>
<td>

<p>The method used for separation on the DD-plot; can be <code>"alpha"</code> (the default), <code>"polynomial"</code>, <code>"knnlm"</code> or <code>"maxD"</code>. See section <em><b><code>Separators</code></b></em> for the description of the separators and additional parameters. To use a custom separator, see topic <code><a href="#topic+Custom+20Methods">Custom Methods</a></code>.
</p>
</td></tr>
<tr><td><code id="ddalpha.train_+3A_outsider.methods">outsider.methods</code></td>
<td>

<p>Vector of character strings each being a name of a basic outsider method for eventual classification; possible names are: <code>"LDA"</code> (the default), <code>"QDA"</code>, <code>"kNN"</code>, <code>"kNNAff"</code>, <code>"depth.Mahalanobis"</code>, <code>"RandProp"</code>, <code>"RandEqual"</code> and <code>"Ignore"</code>. Each method can be specified only once, replications are ignored. By specifying treatments in such a way only a basic treatment method can be chosen (by the name), and the default settings for each of the methods are applied, see 'Details'.
</p>
</td></tr>
<tr><td><code id="ddalpha.train_+3A_outsider.settings">outsider.settings</code></td>
<td>

<p>List containing outsider treatments each described by a list of parameters including a name, see 'Details' and 'Examples'. Each method can be used multiply with (not necessarily) different parameters, just the name should be unique, entries with the repeating names are ignored.
</p>
</td></tr>
<tr><td><code id="ddalpha.train_+3A_aggregation.method">aggregation.method</code></td>
<td>

<p>Character string determining which method to apply to aggregate binary classification results during multiclass classification; can be <code>"majority"</code> (the default) or <code>"sequent"</code>. If <code>"majority"</code>, <code class="reqn">q(q-1)/2</code> (with <code class="reqn">q</code> being the number of classes in the training sample) binary classifiers are trained, the classification results are aggregated using the majority voting, where classes with larger proportions in the training sample (eventually with the earlier entries in the <code>data</code>) are preferred when tied. If <code>"sequent"</code>, <code class="reqn">q</code> binary 'one against all'-classifiers are trained and ties during the classification are resolved as before.
</p>
</td></tr>
<tr><td><code id="ddalpha.train_+3A_pretransform">pretransform</code></td>
<td>

<p>indicates if the data has to be scaled before the learning procedure.
If the used depth method is affine-invariant and pretransform doesn't influence the result, the data won't be transformed (the parameter is ignored).
</p>

<dl>
<dt>NULL</dt><dd>
<p>applies no transformation to the data
</p>
</dd>
<dt>&quot;1Mom&quot;, &quot;1MCD&quot;</dt><dd>
<p>the data is transformed with the common covariance matrix of the whole data
</p>
</dd>
<dt>&quot;NMom&quot;, &quot;NMCD&quot;</dt><dd>
<p>the data is transformed w.r.t. each class using its covariance martix. The depths w.r.t. each class are calculated using the transformed data.
</p>
</dd>
</dl>
<p>for the values <code>"1MCD", "NMCD"</code> <code><a href="robustbase.html#topic+covMcd">covMcd</a></code> is used to calculate the covariance matrix, and the parameter <code>mah.parMcd</code> is used.

</p>
</td></tr>
<tr><td><code id="ddalpha.train_+3A_use.convex">use.convex</code></td>
<td>

<p>Logical variable indicating whether outsiders should be determined exactly, i.e. as the points not contained in any of the convex hulls of the classes from the training sample (<code>TRUE</code>), or those having zero depth w.r.t. each class from the training sample (<code>FALSE</code>). For <code>depth =</code> <code>"zonoid"</code> both values give the same result.
</p>
</td></tr>
<tr><td><code id="ddalpha.train_+3A_seed">seed</code></td>
<td>

<p>the random seed. The default value <code>seed=0</code> makes no changes.
</p>
</td></tr>
<tr><td><code id="ddalpha.train_+3A_...">...</code></td>
<td>

<p>The parameters for the depth calculating and separation methods.
</p>
</td></tr>
</table>


<h3>Details</h3>



<h4>Depths</h4>

<p>For <code>depth="ddplot"</code> the pre-calculated DD-plot shall be passed as <code>data</code>.
</p>
<p>To use a custom depth, see topic <code><a href="#topic+Custom+20Methods">Custom Methods</a></code>.
</p>
<p>To use an outsider treatment only set <code>depth = NULL</code>.
</p>
<p>The following depths are supported:
</p>
<p><code><a href="#topic+depth.halfspace">depth.halfspace</a></code> for calculation of the Tukey depth.
</p>
<p><code><a href="#topic+depth.Mahalanobis">depth.Mahalanobis</a></code> for calculation of Mahalanobis depth.
</p>
<p><code><a href="#topic+depth.projection">depth.projection</a></code> for calculation of projection depth.
</p>
<p><code><a href="#topic+depth.simplicial">depth.simplicial</a></code> for calculation of simplicial depth.
</p>
<p><code><a href="#topic+depth.simplicialVolume">depth.simplicialVolume</a></code> for calculation of simplicial volume depth.
</p>
<p><code><a href="#topic+depth.spatial">depth.spatial</a></code> for calculation of spatial depth.
</p>
<p><code><a href="#topic+depth.zonoid">depth.zonoid</a></code> for calculation of zonoid depth.
</p>
<p>The additional parameters are described in the corresponding topics.
</p>



<h4>Separators</h4>

<p>The separators classify data on the 2-dimensional space of a DD-plot built using the depths.
</p>
<p>To use a custom separator, see topic <code><a href="#topic+Custom+20Methods">Custom Methods</a></code>.
</p>


<h5>alpha</h5>

<p>Trains the DD<code class="reqn">\alpha</code>-classifier (Lange, Mosler and Mozharovskyi, 2014; Mozharovskyi, Mosler and Lange, 2015). The DD<code class="reqn">\alpha</code>-classifier constructs a linear separating rule in the polynomial extension of the depth space with the <code class="reqn">\alpha</code>-procedure (Vasil'ev, 2003); maximum degree of the polynomial products is determined via cross-validation (in the depth space).
</p>
<p>The additional parameters:
</p>

<dl>
<dt>max.degree</dt><dd>
<p>Maximum of the range of degrees of the polynomial depth space extension over which the <code class="reqn">\alpha</code>-procedure is to be cross-validated; can be 1, 2 or 3 (default).
</p>
</dd>
<dt>num.chunks</dt><dd>
<p>Number of chunks to split data into when cross-validating the <code class="reqn">\alpha</code>-procedure; should be <code class="reqn">&gt;0</code>, and smaller than the total number of points in the two smallest classes when <code>aggregation.method =</code> <code>"majority"</code> and smaller than the total number of points in the training sample when <code>aggregation.method =</code> <code>"sequent"</code>. The default value is 10.
</p>
</dd>
</dl>




<h5>polynomial</h5>

<p>Trains the polynomial DD-classifier (Li, Cuesta-Albertos and Liu, 2012). The DD-classifier constructs a polynomial separating rule in the depth space; the degree of the polynomial is determined via cross-validation (in the depth space).
</p>
<p>The additional parameters:
</p>

<dl>
<dt>max.degree</dt><dd>
<p>Maximum of the range of degrees of the polynomial over which the separator is to be cross-validated; can be in [1:10], the default value is 3.
</p>
</dd>
<dt>num.chunks</dt><dd>
<p>Number of chunks to split data into when cross-validating the separator; should be <code class="reqn">&gt;0</code>, and smaller than the total number of points in the two smallest classes when <code>aggregation.method =</code> <code>"majority"</code> and smaller than the total number of points in the training sample when <code>aggregation.method =</code> <code>"sequent"</code>. The default value is 10.
</p>
</dd>
</dl>




<h5>knnlm</h5>

<p>Trains the <code>k</code>-nearest neighbours classifier in the depth space.
</p>
<p>The additional parameters:
</p>

<dl>
<dt>knnrange</dt><dd>
<p>The maximal number of neighbours for kNN separation. The value is bounded by <code class="reqn">2</code> and <code class="reqn">n/2</code>.
</p>
<p><code>NULL</code> for the default value <code class="reqn">10*(n^{1/q})+1</code>, where <code class="reqn">n</code> is the number of objects, <code class="reqn">q</code> is the number of classes. 
</p>
<p><code>"MAX"</code> for the maximum value <code class="reqn">n/2</code>
</p>
</dd>
</dl>




<h5>maxD</h5>

<p>The <code>maximum depth</code> separator classifies an object to the class that provides it the largest depth value.
</p>




<h4>Outsider treatment</h4>

<p>An outsider treatment is a supplementary classifier for data that lie outside the convex hulls of all <code class="reqn">q</code> training classes.
Available methods are: Linear Discriminant Analysis (referred to as &quot;LDA&quot;), see <code><a href="MASS.html#topic+lda">lda</a></code>; <code class="reqn">k</code>-Nearest-Neighbor Classifier (&quot;kNN&quot;), see <code><a href="class.html#topic+knn">knn</a></code>, <code><a href="class.html#topic+knn.cv">knn.cv</a></code>; Affine-Invariant kNN (&quot;kNNAff&quot;), an affine-invariant version of the kNN, suited only for binary classification (some aggregation is used with multiple classes) and not accounting for ties (at all), but very fast by that; Maximum Mahalanobis Depth Classifier (&quot;depth.Mahalanobis&quot;), the outsider is referred to a class w.r.t. which it has the highest depth value scaled by (approximated) priors; Proportional Randomization (&quot;RandProp&quot;), the outsider is referred to a class randomly with probability equal to it (approximated) prior; Equal Randomization (&quot;RandEqual&quot;), the outsider is referred to a class randomly, chances for each class are equal; Ignoring (&quot;Ignore&quot;), the outsider is not classified, the string &quot;Ignored&quot; is returned instead.
</p>
<p>An outsider treatment is specified by a list containing a name and parameters:
</p>
<p><code>name</code> is a character string, name of the outsider treatment to be freely specified; should be unique; is obligatory.
</p>
<p><code>method</code> is a character string, name of the method to use, can be <code>"LDA"</code>, <code>"kNN"</code>, <code>"kNNAff"</code>, <code>"depth.Mahalanobis"</code>, <code>"RandProp"</code>, <code>"RandEqual"</code> and <code>"Ignore"</code>; is obligatory.
</p>
<p><code>priors</code> is a numerical vector specifying prior probabilities of classes; class portions in the training sample are used by the default. <code>priors</code> is used in methods &quot;LDA&quot;, &quot;depth.Mahalanobis&quot; and &quot;RandProp&quot;.
</p>
<p><code>knn.k</code> is the number of the nearest neighbors taken into account; can be between <code class="reqn">1</code> and the number of points in the training sample. Set to <code class="reqn">-1</code> (the default) to be determined by the leave-one-out cross-validation. <code>knn.k</code> is used in method &quot;kNN&quot;.
</p>
<p><code>knn.range</code> is the upper bound on the range over which the leave-one-out cross-validation is performed (the lower bound is <code class="reqn">1</code>); can be between <code class="reqn">2</code> and the number of points in the training sample <code class="reqn">-1</code>. Set to <code class="reqn">-1</code> (the default) to be calculated automatically accounting for number of points and dimension. <code>knn.range</code> is used in method &quot;kNN&quot;.
</p>
<p><code>knnAff.methodAggregation</code> is a character string specifying the aggregation technique for method &quot;kNNAff&quot;; works in the same way as the function argument <code>aggregation.method</code>. <code>knnAff.methodAggregation</code> is used in method &quot;kNNAff&quot;.
</p>
<p><code>knnAff.k</code> is the number of the nearest neighbors taken into account; should be at least <code class="reqn">1</code> and up to the number of points in the training sample when <code>knnAff.methodAggregation =</code> <code>"sequent"</code>, and up to the total number of points in the training sample when <code>knnAff.methodAggregation =</code> <code>"majority"</code>. Set to <code class="reqn">-1</code> (the default) to be determined by the leave-one-out cross-validation. <code>knnAff.k</code> is used in method &quot;kNNAff&quot;.
</p>
<p><code>knnAff.range</code> is the upper bound on the range over which the leave-one-out cross-validation is performed (the lower bound is <code class="reqn">1</code>); should be <code class="reqn">&gt;1</code> and smaller than the total number of points in the two smallest classes when <code>knnAff.methodAggregation =</code> <code>"majority"</code>, and <code class="reqn">&gt;1</code> and smaller than the total number of points in the training sample when <code>knnAff.methodAggregation =</code> <code>"sequent"</code>. Set to <code class="reqn">-1</code> to be calculated automatically accounting for number of points and dimension. <code>knnAff.range</code> is used in method &quot;kNNAff&quot;.
</p>
<p><code>mah.estimate</code> is a character string specifying which estimates to use when calculating the Mahalanobis depth; can be <code>"moment"</code> or <code>"MCD"</code>, determining whether traditional moment or Minimum Covariance Determinant (MCD) (see <code><a href="robustbase.html#topic+covMcd">covMcd</a></code>) estimates for mean and covariance are used. <code>mah.estimate</code> is used in method &quot;depth.Mahalanobis&quot;.
</p>
<p><code>mcd.alpha</code> is the value of the argument <code>alpha</code> for the function <code><a href="robustbase.html#topic+covMcd">covMcd</a></code>; is used in method &quot;depth.Mahalanobis&quot; when <code>mah.estimate =</code> <code>"MCD"</code>.
</p>



<h3>Value</h3>

<p>Trained DD<code class="reqn">\alpha</code>-classifier containing following - rather informative - fields:
</p>
<table>
<tr><td><code>num.points</code></td>
<td>
<p>Total number of points in the training sample.</p>
</td></tr>
<tr><td><code>dimension</code></td>
<td>
<p>Dimension of the original space.</p>
</td></tr>
<tr><td><code>depth</code></td>
<td>
<p>Character string determining which depth notion to use.</p>
</td></tr>
<tr><td><code>methodAggregation</code></td>
<td>
<p>Character string determining which method to apply to aggregate binary classification results.</p>
</td></tr>
<tr><td><code>num.chunks</code></td>
<td>
<p>Number of chunks data has been split into when cross-validating the <code class="reqn">\alpha</code>-procedure.</p>
</td></tr>
<tr><td><code>num.directions</code></td>
<td>
<p>Number of directions used for approximating the Tukey depth (when it is used).</p>
</td></tr>
<tr><td><code>use.convex</code></td>
<td>
<p>Logical variable indicating whether outsiders should be determined exactly when classifying.</p>
</td></tr>
<tr><td><code>max.degree</code></td>
<td>
<p>Maximum of the range of degrees of the polynomial depth space extension over which the <code class="reqn">\alpha</code>-procedure has been cross-validated.</p>
</td></tr>
<tr><td><code>patterns</code></td>
<td>
<p>Classes of the training sample.</p>
</td></tr>
<tr><td><code>num.classifiers</code></td>
<td>
<p>Number of binary classifiers trained.</p>
</td></tr>
<tr><td><code>outsider.methods</code></td>
<td>
<p>Treatments to be used to classify outsiders.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Dyckerhoff, R., Koshevoy, G., and Mosler, K. (1996). Zonoid data depth: theory and computation. In: Prat A. (ed), <em>COMPSTAT 1996. Proceedings in computational statistics</em>, Physica-Verlag (Heidelberg), 235&ndash;240.
</p>
<p>Lange, T., Mosler, K., and Mozharovskyi, P. (2014). Fast nonparametric classification based on data depth. <em>Statistical Papers</em> <b>55</b> 49&ndash;69.
</p>
<p>Li, J., Cuesta-Albertos, J.A., and Liu, R.Y. (2012). DD-classifier: Nonparametric classification procedure based on DD-plot. <em>Journal of the American Statistical Association</em> <b>107</b> 737&ndash;753.
</p>
<p>Mozharovskyi, P. (2015). <em>Contributions to Depth-based Classification and Computation of the Tukey Depth</em>. Verlag Dr. Kovac (Hamburg).
</p>
<p>Mozharovskyi, P., Mosler, K., and Lange, T. (2015). Classifying real-world data with the DD<code class="reqn">\alpha</code>-procedure. <em>Advances in Data Analysis and Classification</em> <b>9</b> 287&ndash;314.
</p>
<p>Vasil'ev, V.I. (2003). The reduction principle in problems of revealing regularities I. <em>Cybernetics and Systems Analysis</em> <b>39</b> 686&ndash;694.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ddalpha.classify">ddalpha.classify</a></code> for classification using DD-classifier, 
<code><a href="#topic+depth.">depth.</a></code> for calculation of depths, 
<code><a href="#topic+depth.space.">depth.space.</a></code> for calculation of depth spaces, 
<code><a href="#topic+is.in.convex">is.in.convex</a></code> to check whether a point is not an outsider.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Generate a bivariate normal location-shift classification task
# containing 200 training objects and 200 to test with
class1 &lt;- mvrnorm(200, c(0,0), 
                  matrix(c(1,1,1,4), nrow = 2, ncol = 2, byrow = TRUE))
class2 &lt;- mvrnorm(200, c(2,2), 
                  matrix(c(1,1,1,4), nrow = 2, ncol = 2, byrow = TRUE))
trainIndices &lt;- c(1:100)
testIndices &lt;- c(101:200)
propertyVars &lt;- c(1:2)
classVar &lt;- 3
trainData &lt;- rbind(cbind(class1[trainIndices,], rep(1, 100)), 
                   cbind(class2[trainIndices,], rep(2, 100)))
testData &lt;- rbind(cbind(class1[testIndices,], rep(1, 100)), 
                  cbind(class2[testIndices,], rep(2, 100)))
data &lt;- list(train = trainData, test = testData)

# Train 1st DDalpha-classifier (default settings) 
# and get the classification error rate
ddalpha1 &lt;- ddalpha.train(data$train)
classes1 &lt;- ddalpha.classify(ddalpha1, data$test[,propertyVars])
cat("1. Classification error rate (defaults): ", 
    sum(unlist(classes1) != data$test[,classVar])/200, ".\n", sep = "")

# Train 2nd DDalpha-classifier (zonoid depth, maximum Mahalanobis 
# depth classifier with defaults as outsider treatment) 
# and get the classification error rate
ddalpha2 &lt;- ddalpha.train(data$train, depth = "zonoid", 
                          outsider.methods = "depth.Mahalanobis")
classes2 &lt;- ddalpha.classify(ddalpha2, data$test[,propertyVars], 
                               outsider.method = "depth.Mahalanobis")
cat("2. Classification error rate (depth.Mahalanobis): ", 
    sum(unlist(classes2) != data$test[,classVar])/200, ".\n", sep = "")

# Train 3rd DDalpha-classifier (100 random directions for the Tukey depth, 
# adjusted maximum Mahalanobis depth classifier 
# and equal randomization as outsider treatments) 
# and get the classification error rates
treatments &lt;- list(list(name = "mahd1", method = "depth.Mahalanobis", 
                        mah.estimate = "MCD", mcd.alpha = 0.75, priors = c(1, 1)/2), 
                   list(name = "rand1", method = "RandEqual"))
ddalpha3 &lt;- ddalpha.train(data$train, outsider.settings = treatments, 
                          num.direction = 100)
classes31 &lt;- ddalpha.classify(ddalpha3, data$test[,propertyVars], 
                              outsider.method = "mahd1")
classes32 &lt;- ddalpha.classify(ddalpha3, data$test[,propertyVars], 
                              outsider.method = "rand1")
cat("3. Classification error rate (by treatments):\n")
cat("   Error (mahd1): ", 
    sum(unlist(classes31) != data$test[,classVar])/200, ".\n", sep = "")
cat("   Error (rand1): ", 
    sum(unlist(classes32) != data$test[,classVar])/200, ".\n", sep = "")
    
# Train using some weird formula
ddalpha = ddalpha.train(
    I(mpg &gt;= 19.2) ~ log(disp) + I(disp^2) + disp + I(disp * drat),
    data = mtcars, subset = (carb!=1), 
    depth = "Mahalanobis", separator = "alpha")
print(ddalpha) # make sure that the resulting table is what you wanted
CC = ddalpha.classify(ddalpha, mtcars)
sum((mtcars$mpg&gt;=19.2)!= unlist(CC))/nrow(mtcars) # error rate
    
#Use the pre-calculated DD-plot
data = cbind(rbind(mvrnorm(n = 50, mu = c(0,0), Sigma = diag(2)),
                   mvrnorm(n = 50, mu = c(5,10), Sigma = diag(2)),
                   mvrnorm(n = 50, mu = c(10,0), Sigma = diag(2))),
             rep(c(1,2,3), each = 50))
plot(data[,1:2], col = (data[,3]+1))

ddplot = depth.space.Mahalanobis(data = data[,1:2], cardinalities = c(50,50,50))
ddplot = cbind(ddplot, data[,3])
ddalphaD = ddalpha.train(data = ddplot, depth = "ddplot", separator = "alpha")
c = ddalpha.classify(ddalphaD, ddplot[,1:3])
errors = sum(unlist(c) != data[,3])/nrow(data)
print(paste("Error rate: ",errors))

ddalpha = ddalpha.train(data = data, depth = "Mahalanobis", separator = "alpha")
c = ddalpha.classify(ddalpha, data[,1:2])
errors = sum(unlist(c) != data[,3])/nrow(data)
print(paste("Error rate: ",errors))
</code></pre>

<hr>
<h2 id='ddalphaf.classify'>
Classify using Functional DD-Classifier
</h2><span id='topic+ddalphaf.classify'></span><span id='topic+predict.ddalphaf'></span>

<h3>Description</h3>

<p>Classifies data using the functional DD-classifier.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ddalphaf.classify(ddalphaf, objectsf, subset, ...)

## S3 method for class 'ddalphaf'
predict(object, objectsf, subset, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ddalphaf.classify_+3A_ddalphaf">ddalphaf</code>, <code id="ddalphaf.classify_+3A_object">object</code></td>
<td>

<p>Functional DD-classifier (obtained by <code><a href="#topic+ddalphaf.train">ddalphaf.train</a></code>).
</p>
</td></tr>
<tr><td><code id="ddalphaf.classify_+3A_objectsf">objectsf</code></td>
<td>
<p>list containing lists (functions) of two vectors of equal length, named &quot;args&quot; and &quot;vals&quot;: arguments sorted in ascending order and corresponding them values respectively
</p>
</td></tr>
<tr><td><code id="ddalphaf.classify_+3A_subset">subset</code></td>
<td>

<p>an optional vector specifying a subset of observations to be classified.
</p>
</td></tr>
<tr><td><code id="ddalphaf.classify_+3A_...">...</code></td>
<td>

<p>additional parameters, passed to the classifier, selected with parameter <code>classifier.type</code> in <code><a href="#topic+ddalphaf.train">ddalphaf.train</a></code>.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List containing class labels.
</p>


<h3>References</h3>

<p>Mosler, K. and Mozharovskyi, P. (2017). Fast DD-classification of functional data. <em>Statistical Papers</em> <b>58</b> 1055&ndash;1089.
</p>
<p>Mozharovskyi, P. (2015). <em>Contributions to Depth-based Classification and Computation of the Tukey Depth</em>. Verlag Dr. Kovac (Hamburg).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ddalphaf.train">ddalphaf.train</a></code> to train the functional DD<code class="reqn">\alpha</code>-classifier.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 
## load the Growth dataset
dataf = dataf.growth()

learn = c(head(dataf$dataf, 49), tail(dataf$dataf, 34))
labels= c(head(dataf$labels, 49), tail(dataf$labels, 34)) 
test  = tail(head(dataf$dataf, 59), 10)    # elements 50:59. 5 girls, 5 boys

c = ddalphaf.train (learn, labels, classifier.type = "ddalpha")

classified = ddalphaf.classify(c, test)

print(unlist(classified))


## End(Not run)
</code></pre>

<hr>
<h2 id='ddalphaf.getErrorRateCV'>
Test Functional DD-Classifier
</h2><span id='topic+ddalphaf.getErrorRateCV'></span>

<h3>Description</h3>

<p>Performs a cross-validation procedure over the given data. 
On each step every <code>numchunks</code> observation is removed from the data, the functional DD-classifier is trained on these data and tested on the removed observations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ddalphaf.getErrorRateCV (dataf, labels, numchunks = 10, disc.type = c("LS", "comp"),  ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ddalphaf.getErrorRateCV_+3A_dataf">dataf</code></td>
<td>

<p>list containing lists (functions) of two vectors of equal length, named &quot;args&quot; and &quot;vals&quot;: arguments sorted in ascending order and corresponding them values respectively
</p>
</td></tr>
<tr><td><code id="ddalphaf.getErrorRateCV_+3A_labels">labels</code></td>
<td>

<p>list of output labels of the functional observations
</p>
</td></tr>
<tr><td><code id="ddalphaf.getErrorRateCV_+3A_numchunks">numchunks</code></td>
<td>

<p>number of subsets of testing data. Equals to the number of times the classifier is trained.
</p>
</td></tr>
<tr><td><code id="ddalphaf.getErrorRateCV_+3A_disc.type">disc.type</code></td>
<td>

<p>type of the used discretization scheme. &quot;LS&quot; for <code><a href="#topic+ddalphaf.train">ddalphaf.train</a></code>, &quot;comp&quot; for  for <code><a href="#topic+compclassf.train">compclassf.train</a></code>
</p>
</td></tr>
<tr><td><code id="ddalphaf.getErrorRateCV_+3A_...">...</code></td>
<td>

<p>additional parameters passed to <code><a href="#topic+ddalphaf.train">ddalphaf.train</a></code>
</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>errors</code></td>
<td>

<p>the part of incorrectly classified data
</p>
</td></tr>
<tr><td><code>time</code></td>
<td>

<p>the mean training time
</p>
</td></tr>
<tr><td><code>time_sd</code></td>
<td>

<p>the standard deviation of training time
</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+ddalphaf.train">ddalphaf.train</a></code> to train the functional DD<code class="reqn">\alpha</code>-classifier, 
<code><a href="#topic+ddalphaf.classify">ddalphaf.classify</a></code> for classification using functional DD<code class="reqn">\alpha</code>-classifier, 
<code><a href="#topic+ddalphaf.test">ddalphaf.test</a></code> to test the functional DD-classifier on particular learning and testing data,
<code><a href="#topic+ddalphaf.getErrorRatePart">ddalphaf.getErrorRatePart</a></code> to perform a benchmark study of the functional DD-classifier on particular data.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># load the fdata
df = dataf.growth()

stat &lt;- ddalphaf.getErrorRateCV(dataf = df$dataf, labels = df$labels, 
                                numchunks = 5,
                                adc.args = list(instance = "avr", 
                                                numFcn = 2, 
                                                numDer = 2))
                                                
cat("Classification error rate: ", stat$errors, ".\n", sep = "")

    
</code></pre>

<hr>
<h2 id='ddalphaf.getErrorRatePart'>
Test Functional DD-Classifier
</h2><span id='topic+ddalphaf.getErrorRatePart'></span>

<h3>Description</h3>

<p>Performs a benchmark procedure by partitioning the given data. 
On each of <code>times</code> steps <code>size</code> observations are removed from the data, the functional DD-classifier is trained on these data and tested on the removed observations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ddalphaf.getErrorRatePart(dataf, labels, size = 0.3, times = 10, 
                          disc.type = c("LS", "comp"),  ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ddalphaf.getErrorRatePart_+3A_dataf">dataf</code></td>
<td>

<p>list containing lists (functions) of two vectors of equal length, named &quot;args&quot; and &quot;vals&quot;: arguments sorted in ascending order and corresponding them values respectively
</p>
</td></tr>
<tr><td><code id="ddalphaf.getErrorRatePart_+3A_labels">labels</code></td>
<td>

<p>list of output labels of the functional observations
</p>
</td></tr>
<tr><td><code id="ddalphaf.getErrorRatePart_+3A_size">size</code></td>
<td>

<p>the excluded sequences size. Either an integer between <code class="reqn">1</code> and <code class="reqn">n</code>, or a fraction of data between <code class="reqn">0</code> and <code class="reqn">1</code>.
</p>
</td></tr>
<tr><td><code id="ddalphaf.getErrorRatePart_+3A_times">times</code></td>
<td>

<p>the number of times the classifier is trained.
</p>
</td></tr>
<tr><td><code id="ddalphaf.getErrorRatePart_+3A_disc.type">disc.type</code></td>
<td>

<p>type of the used discretization scheme. &quot;LS&quot; for <code><a href="#topic+ddalphaf.train">ddalphaf.train</a></code>, &quot;comp&quot; for  for <code><a href="#topic+compclassf.train">compclassf.train</a></code>
</p>
</td></tr>
<tr><td><code id="ddalphaf.getErrorRatePart_+3A_...">...</code></td>
<td>

<p>additional parameters passed to <code><a href="#topic+ddalphaf.train">ddalphaf.train</a></code>
</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>errors</code></td>
<td>

<p>the part of incorrectly classified data (mean)
</p>
</td></tr>
<tr><td><code>errors_sd</code></td>
<td>

<p>the standard deviation of errors
</p>
</td></tr>
<tr><td><code>errors_vec</code></td>
<td>

<p>vector of errors
</p>
</td></tr>
<tr><td><code>time</code></td>
<td>

<p>the mean training time
</p>
</td></tr>
<tr><td><code>time_sd</code></td>
<td>

<p>the standard deviation of training time
</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+ddalphaf.train">ddalphaf.train</a></code> to train the functional DD<code class="reqn">\alpha</code>-classifier, 
<code><a href="#topic+ddalphaf.classify">ddalphaf.classify</a></code> for classification using functional DD<code class="reqn">\alpha</code>-classifier, 
<code><a href="#topic+ddalphaf.test">ddalphaf.test</a></code> to test the functional DD-classifier on particular learning and testing data,
<code><a href="#topic+ddalphaf.getErrorRateCV">ddalphaf.getErrorRateCV</a></code> to get error rate of the functional DD-classifier on particular data.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># load the fdata
df = dataf.growth()

stat &lt;- ddalphaf.getErrorRatePart(dataf = df$dataf, labels = df$labels, 
                          size = 0.3, times = 5,
                          adc.args = list(instance = "avr", 
                                         numFcn = 2, 
                                         numDer = 2))

cat("Classification error rate: ", stat$errors, ".\n", sep = "")


</code></pre>

<hr>
<h2 id='ddalphaf.test'>
Test Functional DD-Classifier
</h2><span id='topic+ddalphaf.test'></span>

<h3>Description</h3>

<p>Trains functional DD-classifier on the learning sequence of the data and tests it on the testing sequence.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ddalphaf.test(learn, learnlabels, test, testlabels, disc.type = c("LS", "comp"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ddalphaf.test_+3A_learn">learn</code></td>
<td>

<p>list containing lists (functions) of two vectors of equal length, named &quot;args&quot; and &quot;vals&quot;: arguments sorted in ascending order and corresponding them values respectively
</p>
</td></tr>
<tr><td><code id="ddalphaf.test_+3A_learnlabels">learnlabels</code></td>
<td>

<p>list of output labels of the functional observations
</p>
</td></tr>
<tr><td><code id="ddalphaf.test_+3A_test">test</code></td>
<td>

<p>the testing sequence. Has the same format as <code>learn</code>
</p>
</td></tr>
<tr><td><code id="ddalphaf.test_+3A_disc.type">disc.type</code></td>
<td>

<p>type of the used discretization scheme. &quot;LS&quot; for <code><a href="#topic+ddalphaf.train">ddalphaf.train</a></code>, &quot;comp&quot; for  for <code><a href="#topic+compclassf.train">compclassf.train</a></code>
</p>
</td></tr>
<tr><td><code id="ddalphaf.test_+3A_testlabels">testlabels</code></td>
<td>

<p>list of output labels of the functinal observations
</p>
</td></tr>
<tr><td><code id="ddalphaf.test_+3A_...">...</code></td>
<td>

<p>additional parameters passed to <code><a href="#topic+ddalphaf.train">ddalphaf.train</a></code>
</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>error</code></td>
<td>

<p>the part of incorrectly classified data
</p>
</td></tr>
<tr><td><code>correct</code></td>
<td>

<p>the number of correctly classified objects
</p>
</td></tr>
<tr><td><code>incorrect</code></td>
<td>

<p>the number of incorrectly classified objects
</p>
</td></tr>
<tr><td><code>total</code></td>
<td>

<p>the number of classified objects
</p>
</td></tr>
<tr><td><code>ignored</code></td>
<td>

<p>the number of ignored objects (outside the convex hull of the learning data)
</p>
</td></tr>
<tr><td><code>n</code></td>
<td>

<p>the number of objects in the testing sequence
</p>
</td></tr>
<tr><td><code>time</code></td>
<td>

<p>training time
</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+ddalphaf.train">ddalphaf.train</a></code> to train the functional DD<code class="reqn">\alpha</code>-classifier, 
<code><a href="#topic+ddalphaf.classify">ddalphaf.classify</a></code> for classification using functonal DD<code class="reqn">\alpha</code>-classifier, 
<code><a href="#topic+ddalphaf.getErrorRateCV">ddalphaf.getErrorRateCV</a></code> and <code><a href="#topic+ddalphaf.getErrorRatePart">ddalphaf.getErrorRatePart</a></code> to get error rate of the functional DD-classifier on particular data.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# load the fdata
df = dataf.growth()

samp = c(35:70)

ddalphaf.test(learn = df$dataf[-samp], learnlabels = df$labels[-samp], 
              test =  df$dataf[samp],  testlabels =  df$labels[samp], 
              adc.args = list(instance = "avr", 
                              numFcn = 2, 
                              numDer = 2))

</code></pre>

<hr>
<h2 id='ddalphaf.train'>
Functional DD-Classifier
</h2><span id='topic+ddalphaf.train'></span>

<h3>Description</h3>

<p>Trains the functional DD-classifier
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ddalphaf.train(dataf, labels, subset, 
                adc.args = list(instance = "avr", 
                               numFcn = -1, 
                               numDer = -1), 
                classifier.type = c("ddalpha", "maxdepth", "knnaff", "lda", "qda"), 
                cv.complete = FALSE, 
                maxNumIntervals = min(25, ceiling(length(dataf[[1]]$args)/2)),
                seed = 0,
                ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ddalphaf.train_+3A_dataf">dataf</code></td>
<td>

<p>list containing lists (functions) of two vectors of equal length, named &quot;args&quot; and &quot;vals&quot;: arguments sorted in ascending order and corresponding them values respectively
</p>
</td></tr>
<tr><td><code id="ddalphaf.train_+3A_labels">labels</code></td>
<td>

<p>list of output labels of the functional observations
</p>
</td></tr>
<tr><td><code id="ddalphaf.train_+3A_subset">subset</code></td>
<td>

<p>an optional vector specifying a subset of observations to be used in training the classifier.
</p>
</td></tr>
<tr><td><code id="ddalphaf.train_+3A_adc.args">adc.args</code></td>
<td>

<p>Represents a function sample as a multidimensional (dimension=<code>"numFcn"+"numDer"</code>) 
one averaging (<code>instance = "avr"</code>) or evaluating (<code>instance = "val"</code>) for that each function and it derivative on <code>"numFcn"</code> 
(resp. <code>"numDer"</code>) equal nonoverlapping covering intervals
</p>
<p>First two named <code>"args"</code> and <code>"vals"</code> are arguments sorted in 
ascending order and having same bounds for all functions and 
corresponding them values respectively
</p>

<dl>
<dt>instance</dt><dd>
<p>type of discretizing the functions: <br />
&quot;avr&quot; - by averaging over intervals of the same length <br />
&quot;val&quot; - by taking values on equally-spaced grid
</p>
</dd>
<dt>numFcn</dt><dd>
<p>number of function intervals
</p>
</dd>  
<dt>numDer</dt><dd>
<p>number of first-derivative intervals 
</p>
</dd>
</dl>

<p>Set <code>numFcn</code> and <code>numDer</code> to -1 to apply cross-validation.
</p>
<p>Set  <code>adc.args</code> to a list of &quot;adc.args&quot; objects to cross-validate only over these values.
</p>
</td></tr>
<tr><td><code id="ddalphaf.train_+3A_classifier.type">classifier.type</code></td>
<td>

<p>the classifier which is used on the transformed space. The default value is 'ddalpha'.
</p>
</td></tr>
<tr><td><code id="ddalphaf.train_+3A_cv.complete">cv.complete</code></td>
<td>

<p>T: apply complete cross-validation<br />
F: restrict cross-validation by Vapnik-Chervonenkis bound
</p>
</td></tr>
<tr><td><code id="ddalphaf.train_+3A_maxnumintervals">maxNumIntervals</code></td>
<td>

<p>maximal number of intervals for cross-validation ( max(numFcn + numDer) = maxNumIntervals )
</p>
</td></tr>
<tr><td><code id="ddalphaf.train_+3A_seed">seed</code></td>
<td>

<p>the random seed. The default value <code>seed=0</code> makes no changes.
</p>
</td></tr>
<tr><td><code id="ddalphaf.train_+3A_...">...</code></td>
<td>

<p>additional parameters, passed to the classifier, selected with parameter <code>classifier.type</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The functional DD-classifier is fast nonparametric procedure for classifying functional data. It consists of
a two-step transformation of the original data plus a classifier operating on a low-dimensional
hypercube. The functional data are first mapped into a finite-dimensional location-slope space
and then transformed by a multivariate depth function into the DD-plot, which is a subset of
the unit hypercube. This transformation yields a new notion of depth for functional data. Three
alternative depth functions are employed for this, as well as two rules for the final classification. 
The resulting classifier is cross-validated over a small range of parameters only,
which is restricted by a Vapnik-Cervonenkis bound. The entire methodology does not involve
smoothing techniques, is completely nonparametric and allows to achieve Bayes optimality under
standard distributional settings. It is robust and efficiently computable.
</p>


<h3>Value</h3>

<p>Trained functional DD-classifier
</p>


<h3>References</h3>

<p>Mosler, K. and Mozharovskyi, P. (2017). Fast DD-classification of functional data. <em>Statistical Papers</em> <b>58</b> 1055&ndash;1089.
</p>
<p>Mozharovskyi, P. (2015). <em>Contributions to Depth-based Classification and Computation of the Tukey Depth</em>. Verlag Dr. Kovac (Hamburg).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ddalphaf.classify">ddalphaf.classify</a></code> for classification using functional DD<code class="reqn">\alpha</code>-classifier,
<code><a href="#topic+compclassf.train">compclassf.train</a></code> to train the functional componentwise classifier,
<code><a href="#topic+dataf.+2A">dataf.*</a></code> for functional data sets included in the package.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 

## load the Growth dataset
dataf = dataf.growth()

learn = c(head(dataf$dataf, 49), tail(dataf$dataf, 34))
labels= c(head(dataf$labels, 49), tail(dataf$labels, 34)) 
test  = tail(head(dataf$dataf, 59), 10)    # elements 50:59. 5 girls, 5 boys

#cross-validate over the whole variants up to dimension 3
c1 = ddalphaf.train (learn, labels, classifier.type = "ddalpha", maxNumIntervals = 3)

classified1 = ddalphaf.classify(c1, test)

print(unlist(classified1))
print(c1$adc.args) 

# cross-validate over these two variants
c2 = ddalphaf.train (learn, labels, classifier.type = "ddalpha", 
                     adc.args = list(
                       list(instance = "avr", 
                            numFcn = 1, 
                            numDer = 2),
                       list(instance = "avr", 
                            numFcn = 0, 
                            numDer = 2)))

classified2 = ddalphaf.classify(c2, test)

print(unlist(classified2))
print(c2$adc.args) 


## End(Not run)
</code></pre>

<hr>
<h2 id='depth.'>
Calculate Depth
</h2><span id='topic+depth.'></span>

<h3>Description</h3>

<p>Calculates the depth of points w.r.t. a multivariate data set.
</p>
<p>The detailed descriptions are found in the corresponding topics.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>depth.(x, data, notion, ...)

## beta-skeleton depth
# depth.betaSkeleton(x, data, beta = 2, distance = "Lp", Lp.p = 2, 
#                   mah.estimate = "moment", mah.parMcd = 0.75)

## Tukey depth
# depth.halfspace(x, data, exact, method, num.directions = 1000, seed = 0)

## L2-depth
# depth.L2(x, data, mah.estimate = "moment", mah.parMcd = 0.75)

## Mahalanobis depth
# depth.Mahalanobis(x, data, mah.estimate = "moment", mah.parMcd = 0.75)

## projection depth
# depth.projection(x, data, method = "random", num.directions = 1000)

## simplicial depth
# depth.simplicial(x, data, exact = F, k = 0.05, seed = 0)

## simplicial volume depth
# depth.simplicialVolume(x, data, exact = F, k = 0.05, seed = 0)

## spatial depth
# depth.spatial(x, data)

## zonoid depth
# depth.zonoid(x, data)

## potential
# depth.potential (x, data, pretransform = "1Mom", 
#            kernel = "GKernel", kernel.bandwidth = NULL, mah.parMcd = 0.75)

## convex hull peeling depth
# depth.qhpeeling(x, data)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="depth._+3A_x">x</code></td>
<td>

<p>Matrix of objects (numerical vector as one object) whose depth is to be calculated; each row contains a <code class="reqn">d</code>-variate point. Should have the same dimension as <code>data</code>.
</p>
</td></tr>
<tr><td><code id="depth._+3A_data">data</code></td>
<td>

<p>Matrix of data where each row contains a <code class="reqn">d</code>-variate point, w.r.t. which the depth is to be calculated.
</p>
</td></tr>
<tr><td><code id="depth._+3A_notion">notion</code></td>
<td>

<p>The name of the depth notion (shall also work with a user-defined depth function named <code>"depth.&lt;name&gt;"</code>).
</p>
</td></tr>
<tr><td><code id="depth._+3A_...">...</code></td>
<td>

<p>Additional parameters passed to the depth functions.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Numerical vector of depths, one for each row in <code>x</code>; or one depth value if <code>x</code> is a numerical vector.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+depth.betaSkeleton">depth.betaSkeleton</a></code>
</p>
<p><code><a href="#topic+depth.halfspace">depth.halfspace</a></code>
</p>
<p><code><a href="#topic+depth.L2">depth.L2</a></code>
</p>
<p><code><a href="#topic+depth.Mahalanobis">depth.Mahalanobis</a></code>
</p>
<p><code><a href="#topic+depth.projection">depth.projection</a></code>
</p>
<p><code><a href="#topic+depth.simplicial">depth.simplicial</a></code>
</p>
<p><code><a href="#topic+depth.simplicialVolume">depth.simplicialVolume</a></code>
</p>
<p><code><a href="#topic+depth.spatial">depth.spatial</a></code>
</p>
<p><code><a href="#topic+depth.zonoid">depth.zonoid</a></code>
</p>
<p><code><a href="#topic+depth.potential">depth.potential</a></code>
</p>
<p><code><a href="#topic+depth.qhpeeling">depth.qhpeeling</a></code>
</p>
<p><code><a href="#topic+depth.graph">depth.graph</a></code> for building the depth surfaces of the two dimensional data.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># 5-dimensional normal distribution
data &lt;- mvrnorm(1000, rep(0, 5), 
                matrix(c(1, 0, 0, 0, 0, 
                         0, 2, 0, 0, 0, 
                         0, 0, 3, 0, 0, 
                         0, 0, 0, 2, 0, 
                         0, 0, 0, 0, 1),
                nrow = 5))
x &lt;- mvrnorm(10, rep(1, 5), 
             matrix(c(1, 0, 0, 0, 0, 
                      0, 1, 0, 0, 0, 
                      0, 0, 1, 0, 0, 
                      0, 0, 0, 1, 0, 
                      0, 0, 0, 0, 1),
             nrow = 5))
                
depths &lt;- depth.(x, data, notion = "zonoid")
cat("Depths: ", depths, "\n")
</code></pre>

<hr>
<h2 id='depth.betaSkeleton'>
Calculate Beta-Skeleton Depth
</h2><span id='topic+depth.betaSkeleton'></span>

<h3>Description</h3>

<p>Calculates the beta-skeleton depth of points w.r.t. a multivariate data set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>depth.betaSkeleton(x, data, beta = 2, distance = "Lp", Lp.p = 2, 
                   mah.estimate = "moment", mah.parMcd = 0.75)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="depth.betaSkeleton_+3A_x">x</code></td>
<td>
<p>Matrix of objects (numerical vector as one object) whose depth is to be calculated; each row contains a <code class="reqn">d</code>-variate point. Should have the same dimension as <code>data</code>.
</p>
</td></tr>
<tr><td><code id="depth.betaSkeleton_+3A_data">data</code></td>
<td>
<p>Matrix of data where each row contains a <code class="reqn">d</code>-variate point, w.r.t. which the depth is to be calculated.
</p>
</td></tr>
<tr><td><code id="depth.betaSkeleton_+3A_beta">beta</code></td>
<td>
<p>The paremeter defining the positionning of the balls' centers, see Yang and Modarres (2017) for details. By default (together with other arguments) equals <code>2</code>, which corresponds to the lens depth, see Liu and Modarres (2011).
</p>
</td></tr>
<tr><td><code id="depth.betaSkeleton_+3A_distance">distance</code></td>
<td>
<p>A character string defining the distance to be used for determining inclusion of a point into the lens (influence region), see Yang and Modarres (2017) for details. Possibilities are <code>"Lp"</code> for the Lp-metric (default) or <code>"Mahalanobis"</code> for the Mahalanobis distance adjustment.
</p>
</td></tr>
<tr><td><code id="depth.betaSkeleton_+3A_lp.p">Lp.p</code></td>
<td>
<p>A non-negative number defining the distance's power equal <code>2</code> by default (Euclidean distance); is used only when <code>distance = "Lp"</code>.
</p>
</td></tr>
<tr><td><code id="depth.betaSkeleton_+3A_mah.estimate">mah.estimate</code></td>
<td>
<p>A character string specifying which estimates to use when calculating sample covariance matrix; can be <code>"none"</code>, <code>"moment"</code> or <code>"MCD"</code>, determining whether traditional moment or Minimum Covariance Determinant (MCD) (see <code><a href="robustbase.html#topic+covMcd">covMcd</a></code>) estimates for mean and covariance are used. By default <code>"moment"</code> is used. Is used only when <code>distance = "Mahalanobis"</code>.
</p>
</td></tr>
<tr><td><code id="depth.betaSkeleton_+3A_mah.parmcd">mah.parMcd</code></td>
<td>
<p>The value of the argument <code>alpha</code> for the function <code><a href="robustbase.html#topic+covMcd">covMcd</a></code>; is used when <code>distance = "Mahalanobis"</code> and <code>mah.estimate = "MCD"</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Calculates the beta-skeleton depth, see Yang and Modarres (2017). Its particular case, lens depth, see Liu and Modarres (2011), is obtained when <code>beta = 2</code>, <code>distance = "Lp"</code> and <code>Lp.p = 2</code> (default settings). For tne example of the lens depth, the depth of an observation <code>x</code> is calculated as the portion of lens containing <code>x</code>, with lens being an intersection of two closed balls centered at two sample's points each having radius equal to the distance between these two points.
</p>


<h3>Value</h3>

<p>Numerical vector of depths, one for each row in <code>x</code>; or one depth value if <code>x</code> is a numerical vector.
</p>


<h3>References</h3>

<p>Liu, Z. and Modarres, R. (2011). Lens data depth and median. <em>Journal of Nonparametric Statistics</em> <b>23</b>(4) 1063&ndash;1074.
</p>
<p>Yang, M. and Modarres, R. (2017). <code class="reqn">\beta</code>-skeleton depth functions and medians. <em>Commmunications in Statistics - Theory and Methods</em> to appear.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+depth.halfspace">depth.halfspace</a></code> for calculation of the Tukey depth.
</p>
<p><code><a href="#topic+depth.Mahalanobis">depth.Mahalanobis</a></code> for calculation of Mahalanobis depth.
</p>
<p><code><a href="#topic+depth.projection">depth.projection</a></code> for calculation of projection depth.
</p>
<p><code><a href="#topic+depth.simplicial">depth.simplicial</a></code> for calculation of simplicial depth.
</p>
<p><code><a href="#topic+depth.simplicialVolume">depth.simplicialVolume</a></code> for calculation of simplicial volume depth.
</p>
<p><code><a href="#topic+depth.spatial">depth.spatial</a></code> for calculation of spatial depth.
</p>
<p><code><a href="#topic+depth.zonoid">depth.zonoid</a></code> for calculation of zonoid depth.
</p>
<p><code><a href="#topic+depth.potential">depth.potential</a></code> for calculation of data potential.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># 5-dimensional normal distribution
data &lt;- mvrnorm(1000, rep(0, 5), 
                matrix(c(1, 0, 0, 0, 0, 
                         0, 2, 0, 0, 0, 
                         0, 0, 3, 0, 0, 
                         0, 0, 0, 2, 0, 
                         0, 0, 0, 0, 1),
                nrow = 5))
x &lt;- mvrnorm(10, rep(1, 5), 
             matrix(c(1, 0, 0, 0, 0, 
                      0, 1, 0, 0, 0, 
                      0, 0, 1, 0, 0, 
                      0, 0, 0, 1, 0, 
                      0, 0, 0, 0, 1),
             nrow = 5))
                
depths &lt;- depth.betaSkeleton(x, data)
cat("Depths:", depths, "\n")
</code></pre>

<hr>
<h2 id='depth.contours'>
Depth Contours
</h2><span id='topic+depth.contours'></span>

<h3>Description</h3>

<p>Builds the data depth contours for 2-dimensional data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>depth.contours(data, depth, 
              main = "", xlab="", ylab = "", 
              drawplot = T, frequency=100, levels = 10,
              col = "red",
              ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="depth.contours_+3A_data">data</code></td>
<td>

<p>2-dimensional numeric data frame or matrix
</p>
</td></tr>
<tr><td><code id="depth.contours_+3A_depth">depth</code></td>
<td>

<p>the name of the depth function. The list of the supported depths and described in the topic <code><a href="#topic+depth.">depth.</a></code>.
</p>
</td></tr>
<tr><td><code id="depth.contours_+3A_main">main</code></td>
<td>

<p>an overall title for the plot: see <code><a href="graphics.html#topic+title">title</a></code>
</p>
</td></tr>
<tr><td><code id="depth.contours_+3A_xlab">xlab</code>, <code id="depth.contours_+3A_ylab">ylab</code></td>
<td>

<p>labels of the axes
</p>
</td></tr>
<tr><td><code id="depth.contours_+3A_drawplot">drawplot</code></td>
<td>

<p>if set to false, the contours are built on the existing plot. 
</p>
</td></tr>
<tr><td><code id="depth.contours_+3A_frequency">frequency</code></td>
<td>

<p>number of points on each direction, x and y. Impacts the smoothness of the contours.
</p>
</td></tr>
<tr><td><code id="depth.contours_+3A_levels">levels</code></td>
<td>

<p>numeric vector of levels at which to draw contour lines. 
If the vector contains only ONE element, the levels are generated automatically as <code>seq(0, max(depth), length.out = levels)</code>.
</p>
</td></tr>
<tr><td><code id="depth.contours_+3A_col">col</code></td>
<td>

<p>color, used to draw points and contours
</p>
</td></tr>
<tr><td><code id="depth.contours_+3A_...">...</code></td>
<td>

<p>additional parameters passed to the depth functions and to <code><a href="base.html#topic+plot">plot</a></code>
</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+depth.">depth.</a></code>, 
<code><a href="#topic+depth.contours.ddalpha">depth.contours.ddalpha</a></code>, 
<code><a href="#topic+depth.graph">depth.graph</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 

par(mfrow = c(2,2))
data(hemophilia)

depth.contours(hemophilia[,1:2], depth = "none", main = "data")

for (depth in c("zonoid", "Mahalanobis", "projection", "spatial")){
  depth.contours(hemophilia[,1:2], depth = depth, main = depth)
}

for (depth in c("halfspace", "simplicial", "simplicialVolume")){
  depth.contours(hemophilia[,1:2], depth = depth, main = depth, exact = T)
}



## End(Not run)
</code></pre>

<hr>
<h2 id='depth.contours.ddalpha'>
Depth Contours
</h2><span id='topic+depth.contours.ddalpha'></span>

<h3>Description</h3>

<p>Builds the data depth contours for multiclass 2-dimensional data using the trained classifier.
Also accessible from <code><a href="#topic+plot.ddalpha">plot.ddalpha</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>depth.contours.ddalpha(ddalpha,
              main = "", xlab="", ylab = "", 
              drawplot = T, frequency=100, levels = 10, drawsep = T, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="depth.contours.ddalpha_+3A_ddalpha">ddalpha</code></td>
<td>

<p>DD<code class="reqn">\alpha</code>-classifier (obtained by <code><a href="#topic+ddalpha.train">ddalpha.train</a></code>).
</p>
</td></tr>
<tr><td><code id="depth.contours.ddalpha_+3A_main">main</code></td>
<td>

<p>an overall title for the plot: see <code><a href="graphics.html#topic+title">title</a></code>
</p>
</td></tr>
<tr><td><code id="depth.contours.ddalpha_+3A_xlab">xlab</code>, <code id="depth.contours.ddalpha_+3A_ylab">ylab</code></td>
<td>

<p>labels of the axes
</p>
</td></tr>
<tr><td><code id="depth.contours.ddalpha_+3A_drawplot">drawplot</code></td>
<td>

<p>if set to false, the contours are built on the existing plot. 
</p>
</td></tr>
<tr><td><code id="depth.contours.ddalpha_+3A_frequency">frequency</code></td>
<td>

<p>number of points on each direction, x and y. Impacts the smoothness of the contours.
</p>
</td></tr>
<tr><td><code id="depth.contours.ddalpha_+3A_levels">levels</code></td>
<td>

<p>numeric vector of levels at which to draw contour lines. 
If the vector contains only ONE element, the levels are generated automatically as <code>seq(0, max(depth), length.out = levels)</code>.
</p>
</td></tr>
<tr><td><code id="depth.contours.ddalpha_+3A_drawsep">drawsep</code></td>
<td>

<p>draws the separation on the DD-plot (currently for 2 classes and not for knn)
</p>
</td></tr>
<tr><td><code id="depth.contours.ddalpha_+3A_...">...</code></td>
<td>

<p>additional parameters passed to the depth functions and to <code><a href="base.html#topic+plot">plot</a></code>
</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+depth.">depth.</a></code>, 
<code><a href="#topic+depth.contours">depth.contours</a></code>, 
<code><a href="#topic+depth.graph">depth.graph</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 

par(mfrow = c(2,2))
data(hemophilia)

ddalpha = ddalpha.train(hemophilia, depth = "none")
depth.contours.ddalpha(ddalpha, main = "data")

for (depth in c("zonoid", "Mahalanobis", "projection", "spatial")){
  ddalpha = ddalpha.train(hemophilia, depth = depth)
  depth.contours.ddalpha(ddalpha, main = depth)
}

for (depth in c("halfspace", "simplicial", "simplicialVolume")){
  ddalpha = ddalpha.train(hemophilia, depth = depth, exact = T)
  depth.contours.ddalpha(ddalpha, main = depth)
}



## End(Not run)
</code></pre>

<hr>
<h2 id='depth.graph'>
Depth Graph
</h2><span id='topic+depth.graph'></span>

<h3>Description</h3>

<p>Builds the data depth graphs for 2-dimensional data. The graph is built using <code><a href="graphics.html#topic+persp">persp</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>depth.graph(data, 
  depth_f = c("halfspace", "Mahalanobis", "projection", "simplicial", 
              "simplicialVolume", "spatial", "zonoid", "none"), 
  apoint = NULL, 
  main = depth_f,  
  xlim = c(min(data[, 1]), max(data[, 1])), 
  ylim = c(min(data[, 2]), max(data[, 2])), 
  zlim = c(0, max(z)), 
  xnum = 250, 
  ynum = 250, 
  theta=15, phi=60,
  bold = F,
  ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="depth.graph_+3A_data">data</code></td>
<td>

<p>2-dimensional numeric data frame or matrix
</p>
</td></tr>
<tr><td><code id="depth.graph_+3A_depth_f">depth_f</code></td>
<td>

<p>the name of the depth function. The list of the supported depths and described in the topic <code><a href="#topic+depth.">depth.</a></code>.
</p>
</td></tr>
<tr><td><code id="depth.graph_+3A_apoint">apoint</code></td>
<td>

<p>a 2-dimensional point which is shown in black color.
</p>
</td></tr>
<tr><td><code id="depth.graph_+3A_main">main</code></td>
<td>

<p>an overall title for the plot: see <code><a href="graphics.html#topic+title">title</a></code>
</p>
</td></tr>
<tr><td><code id="depth.graph_+3A_xlim">xlim</code>, <code id="depth.graph_+3A_ylim">ylim</code>, <code id="depth.graph_+3A_zlim">zlim</code></td>
<td>

<p>numeric vectors of length 2, giving the x, y and z coordinates ranges: see <code><a href="graphics.html#topic+plot.window">plot.window</a></code>
</p>
</td></tr>
<tr><td><code id="depth.graph_+3A_xnum">xnum</code>, <code id="depth.graph_+3A_ynum">ynum</code></td>
<td>

<p>number of points on each direction, x and y. Impacts the smoothness of the surface. 
</p>
</td></tr>
<tr><td><code id="depth.graph_+3A_theta">theta</code>, <code id="depth.graph_+3A_phi">phi</code></td>
<td>

<p>rotation angles
</p>
</td></tr>
<tr><td><code id="depth.graph_+3A_bold">bold</code></td>
<td>

<p>draws bold points
</p>
</td></tr>
<tr><td><code id="depth.graph_+3A_...">...</code></td>
<td>

<p>additional parameters passed to <code><a href="graphics.html#topic+persp">persp</a></code>
</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+depth.">depth.</a></code>
</p>
<p><code><a href="graphics.html#topic+persp">persp</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 

par(mfrow = c(2,3), mar = c(0,0,0,0), mai = c(0,0,0.2,0))
data(hemophilia)
depth.graph(hemophilia, "none", xnum = 100, ynum = 100)
depth.graph(hemophilia, "Mahalanobis", xnum = 100, ynum = 100)
depth.graph(hemophilia, "halfspace", xnum = 100, ynum = 100)
depth.graph(hemophilia, "projection", xnum = 100, ynum = 100)
depth.graph(hemophilia, "zonoid", xnum = 100, ynum = 100)
depth.graph(hemophilia, "spatial", xnum = 100, ynum = 100)


## End(Not run)
</code></pre>

<hr>
<h2 id='depth.halfspace'>
Calculate Halfspace Depth
</h2><span id='topic+depth.halfspace'></span>

<h3>Description</h3>

<p>Calculates the exact or random Tukey (=halfspace, location) depth  (Tukey, 1975) of points w.r.t. a multivariate data set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>depth.halfspace(x, data, exact, method, num.directions = 1000, seed = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="depth.halfspace_+3A_x">x</code></td>
<td>

<p>Matrix of objects (numerical vector as one object) whose depth is to be calculated; each row contains a <code class="reqn">d</code>-variate point. Should have the same dimension as <code>data</code>.
</p>
</td></tr>
<tr><td><code id="depth.halfspace_+3A_data">data</code></td>
<td>

<p>Matrix of data where each row contains a <code class="reqn">d</code>-variate point, w.r.t. which the depth is to be calculated.
</p>
</td></tr>
<tr><td><code id="depth.halfspace_+3A_exact">exact</code></td>
<td>

<p>The type of the used method. The default is <code>exact=F</code>, which leads to approximate computation of the Tukey depth. For <code>exact=F</code>, <code>method="Sunif.1D"</code> is used by default. If <code>exact=T</code>, the Tukey depth is computed exactly, with <code>method="recursive"</code> by default.</p>
</td></tr>
<tr><td><code id="depth.halfspace_+3A_method">method</code></td>
<td>

<p>For <code>exact=F</code>, if <code>method="Sunif.1D"</code> (by default), the Tukey depth is computed approximately by being minimized over univariate projections (see Details below).
</p>
<p>For <code>exact=T</code>, the Tukey depth is calculated as the minimum over all combinations of <code class="reqn">k</code> points from <code>data</code> (see Details below). In this case parameter <code>method</code> specifies <code class="reqn">k</code>, with possible values <code class="reqn">1</code> for <code>method="recursive"</code> (by default), <code class="reqn">d-2</code> for <code>method="plane"</code>, <code class="reqn">d-1</code> for <code>method="line"</code>.
</p>
<p>The name of the method may be given as well as just parameter <code>exact</code>, in which case the default method will be used.
</p>
</td></tr>
<tr><td><code id="depth.halfspace_+3A_num.directions">num.directions</code></td>
<td>

<p>Number of random directions to be generated (for <code>method="Sunif.1D"</code>). The algorithmic complexity is linear in the number of observations in <code>data</code>, given the number of directions.
</p>
</td></tr>
<tr><td><code id="depth.halfspace_+3A_seed">seed</code></td>
<td>

<p>The random seed. The default value <code>seed=0</code> makes no changes (for <code>method="Sunif.1D"</code>).
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For <code>exact=F</code>, if <code>method="Sunif.1D"</code>, the Tukey depth is computed approximately using the random Tukey depth method proposed by Cuesta-Albertos and Nieto-Reyes (2008). Here the depth is determined as the minimum univariate Tukey depth of the - on lines in several directions - projected data. The directions are distributed uniformly on the <code class="reqn">(d-1)</code>-sphere; the same direction set is used for all points.
</p>
<p>For <code>exact=T</code>, the Tukey depth is computed exactly as the minimum of the sum of the depths in two orthogonal complementary affine subspaces, which dimensions add to <code class="reqn">d</code>: one of the subspaces (combinatorial) is the <code class="reqn">k</code>-dimensional hyperplane through (a point from) <code>x</code> and <code class="reqn">k</code> points from <code>data</code>, another one is its orthogonal complement (see Dyckerhoff and Mozharovskyi, 2016 for the detailed description of the algorithmic framework). The algorithm then minimizes the depth over all combinations of <code class="reqn">k</code> points, in which the depth in the orthogonal complements is computed using an exact algorithm. In this case, parameter <code>method</code> specifies the dimensionality <code class="reqn">k</code> of the combinatorial space. The implemented (reasonable) algorithms (and corresponding names) are: <code class="reqn">k=1</code> (or <code>method="recursive"</code>), <code class="reqn">k=d-2</code> (or <code>method="plane"</code>), and <code class="reqn">k=d-1</code> (or <code>method="line"</code>).
</p>


<h3>Value</h3>

<p>Numerical vector of depths, one for each row in <code>x</code>; or one depth value if <code>x</code> is a numerical vector.
</p>


<h3>References</h3>

<p>Cuesta-Albertos, J.A. and Nieto-Reyes, A. (2008). The random Tukey depth. <em>Computational Statistics and Data Analysis</em> <b>52</b> 4979&ndash;4988.
</p>
<p>Dyckerhoff, R. and Mozharovskyi, P. (2016). Exact computation of the halfspace depth. <em>Computational Statistics and Data Analysis</em> <b>98</b> 19&ndash;30.
</p>
<p>Rousseeuw, P.J. and Ruts, I. (1996). Algorithm AS 307: Bivariate location depth. <em>Journal of the Royal Statistical Society. Seriec C (Applied Statistics)</em> <b>45</b> 516&ndash;526.
</p>
<p>Tukey, J.W. (1974). Mathematics and the picturing of data. In: <em>Proceeding of the International Congress of Mathematicians</em>, Vancouver, 523&ndash;531.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+depth.Mahalanobis">depth.Mahalanobis</a></code> for calculation of Mahalanobis depth.
</p>
<p><code><a href="#topic+depth.projection">depth.projection</a></code> for calculation of projection depth.
</p>
<p><code><a href="#topic+depth.simplicial">depth.simplicial</a></code> for calculation of simplicial depth.
</p>
<p><code><a href="#topic+depth.simplicialVolume">depth.simplicialVolume</a></code> for calculation of simplicial volume depth.
</p>
<p><code><a href="#topic+depth.spatial">depth.spatial</a></code> for calculation of spatial depth.
</p>
<p><code><a href="#topic+depth.zonoid">depth.zonoid</a></code> for calculation of zonoid depth.
</p>
<p><code><a href="#topic+depth.potential">depth.potential</a></code> for calculation of data potential.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># 3-dimensional normal distribution
data &lt;- mvrnorm(200, rep(0, 3), 
                matrix(c(1, 0, 0,
                         0, 2, 0, 
                         0, 0, 1),
                nrow = 3))
x &lt;- mvrnorm(10, rep(1, 3), 
             matrix(c(1, 0, 0,
                      0, 1, 0, 
                      0, 0, 1),
             nrow = 3))
              
# default - random Tukey depth
depths &lt;- depth.halfspace(x, data)
cat("Depths: ", depths, "\n")

# default exact method - "recursive"
depths &lt;- depth.halfspace(x, data, exact = TRUE)
cat("Depths: ", depths, "\n")

# method "line"
depths &lt;- depth.halfspace(x, data, method = "line")
cat("Depths: ", depths, "\n")
</code></pre>

<hr>
<h2 id='depth.L2'>
Calculate L2-Depth
</h2><span id='topic+depth.L2'></span>

<h3>Description</h3>

<p>Calculates the L2-depth of points w.r.t. a multivariate data set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>depth.L2(x, data, mah.estimate = "moment", mah.parMcd = 0.75)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="depth.L2_+3A_x">x</code></td>
<td>

<p>Matrix of objects (numerical vector as one object) whose depth is to be calculated; each row contains a <code class="reqn">d</code>-variate point. Should have the same dimension as <code>data</code>.
</p>
</td></tr>
<tr><td><code id="depth.L2_+3A_data">data</code></td>
<td>

<p>Matrix of data where each row contains a <code class="reqn">d</code>-variate point, w.r.t. which the depth is to be calculated.
</p>
</td></tr>
<tr><td><code id="depth.L2_+3A_mah.estimate">mah.estimate</code></td>
<td>
<p> is a character string specifying which estimates to use when calculating sample covariance matrix; can be <code>"none"</code>, <code>"moment"</code> or <code>"MCD"</code>, determining whether traditional moment or Minimum Covariance Determinant (MCD) (see <code><a href="robustbase.html#topic+covMcd">covMcd</a></code>) estimates for mean and covariance are used. By default <code>"moment"</code> is used. With <code>"none"</code> the non-affine invariant version of the L2-depth is calculated
</p>
</td></tr>
<tr><td><code id="depth.L2_+3A_mah.parmcd">mah.parMcd</code></td>
<td>

<p>is the value of the argument <code>alpha</code> for the function <code><a href="robustbase.html#topic+covMcd">covMcd</a></code>; is used when <code>mah.estimate =</code> <code>"MCD"</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Calculates L2-depth (Mosler, 2013). L2-depth is based on the oultyingness distance calculated as the average L2-distance from (a row of) <code>x</code> to each point in <code>data</code>.
</p>


<h3>Value</h3>

<p>Numerical vector of depths, one for each row in <code>x</code>; or one depth value if <code>x</code> is a numerical vector.
</p>


<h3>References</h3>

<p>Mosler, K. (2013). Depth statistics. In: Becker, C., Fried, R. and Kuhnt, S. (eds), <em>Robustness and Complex Data Structures: Festschrift in Honour of Ursula Gather</em>, Springer-Verlag (Berlin, Heidelberg), 17&ndash;34.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+depth.halfspace">depth.halfspace</a></code> for calculation of the Tukey depth.
</p>
<p><code><a href="#topic+depth.Mahalanobis">depth.Mahalanobis</a></code> for calculation of Mahalanobis depth.
</p>
<p><code><a href="#topic+depth.projection">depth.projection</a></code> for calculation of projection depth.
</p>
<p><code><a href="#topic+depth.qhpeeling">depth.qhpeeling</a></code> for calculation of convex hull peeling depth.
</p>
<p><code><a href="#topic+depth.simplicial">depth.simplicial</a></code> for calculation of simplicial depth.
</p>
<p><code><a href="#topic+depth.simplicialVolume">depth.simplicialVolume</a></code> for calculation of simplicial volume depth.
</p>
<p><code><a href="#topic+depth.spatial">depth.spatial</a></code> for calculation of spatial depth.
</p>
<p><code><a href="#topic+depth.potential">depth.potential</a></code> for calculation of data potential.
</p>
<p><code><a href="#topic+depth.zonoid">depth.zonoid</a></code> for calculation of zonoid depth.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># 5-dimensional normal distribution
data &lt;- mvrnorm(1000, rep(0, 5), 
                matrix(c(1, 0, 0, 0, 0, 
                         0, 2, 0, 0, 0, 
                         0, 0, 3, 0, 0, 
                         0, 0, 0, 2, 0, 
                         0, 0, 0, 0, 1),
                nrow = 5))
x &lt;- mvrnorm(10, rep(1, 5), 
             matrix(c(1, 0, 0, 0, 0, 
                      0, 1, 0, 0, 0, 
                      0, 0, 1, 0, 0, 
                      0, 0, 0, 1, 0, 
                      0, 0, 0, 0, 1),
             nrow = 5))
                
depths &lt;- depth.spatial(x, data)
cat("Depths:", depths, "\n")
</code></pre>

<hr>
<h2 id='depth.Mahalanobis'>
Calculate Mahalanobis Depth
</h2><span id='topic+depth.Mahalanobis'></span>

<h3>Description</h3>

<p>Calculates the Mahalanobis depth of points w.r.t. a multivariate data set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>depth.Mahalanobis(x, data, mah.estimate = "moment", mah.parMcd = 0.75)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="depth.Mahalanobis_+3A_x">x</code></td>
<td>

<p>Matrix of objects (numerical vector as one object) whose depth is to be calculated; each row contains a <code class="reqn">d</code>-variate point. Should have the same dimension as <code>data</code>.
</p>
</td></tr>
<tr><td><code id="depth.Mahalanobis_+3A_data">data</code></td>
<td>

<p>Matrix of data where each row contains a <code class="reqn">d</code>-variate point, w.r.t. which the depth is to be calculated.
</p>
</td></tr>
<tr><td><code id="depth.Mahalanobis_+3A_mah.estimate">mah.estimate</code></td>
<td>
<p> is a character string specifying which estimates to use when calculating the Mahalanobis depth; can be <code>"moment"</code> or <code>"MCD"</code>, determining whether traditional moment or Minimum Covariance Determinant (MCD) (see <code><a href="robustbase.html#topic+covMcd">covMcd</a></code>) estimates for mean and covariance are used. By default <code>"moment"</code> is used.
</p>
</td></tr>
<tr><td><code id="depth.Mahalanobis_+3A_mah.parmcd">mah.parMcd</code></td>
<td>

<p>is the value of the argument <code>alpha</code> for the function <code><a href="robustbase.html#topic+covMcd">covMcd</a></code>; is used when <code>mah.estimate =</code> <code>"MCD"</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Calculates Mahalanobis depth. Mahalanobis depth is based on an outlyingness measure (Zuo &amp; Serfling, 2000), <em>viz.</em> the Mahalanobis distance between the given point and the center of the data (Mahalanobis, 1936).
</p>
<p><em>Moment estimates</em> may be used i.e. traditional <em>mean</em> and <em>covariance matrix</em>, the corresponding depth may be sensitive to
outliers. A more robust depth is obtained with <em>minimum volume ellipsoid</em> (MVE) or <em>minimum
covariance determinant</em> (MCD) estimators, see Rousseeuw &amp; Leroy (1987) and Lopuhaa &amp;
Rousseeuw (1991).
</p>


<h3>Value</h3>

<p>Numerical vector of depths, one for each row in <code>x</code>; or one depth value if <code>x</code> is a numerical vector.
</p>


<h3>References</h3>

<p>Mahalanobis, P. (1936). On the generalized distance in statistics. <em>Proceedings of the National
Academy India</em> <b>12</b> 49&ndash;55.
</p>
<p>Liu, R.Y. (1992). Data depth and multivariate rank tests. In: Dodge, Y. (ed.), <em>L1-Statistics and Related Methods</em>, North-Holland (Amsterdam), 279&ndash;294.
</p>
<p>Lopuhaa, H.P. and Rousseeuw, P.J. (1991). Breakdown points of affine equivariant estimators of multivariate location and covariance matrices. <em>The Annals of Statistics</em> <b>19</b> 229&ndash;248.
</p>
<p>Rousseeuw, P.J. and Leroy, A.M. (1987). Robust Regression and Outlier Detection. John Wiley &amp; Sons (New York).
</p>
<p>Zuo, Y.J. and Serfling, R. (2000). General notions of statistical depth function. <em>The Annals of Statistics</em> <b>28</b> 461&ndash;482.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+depth.halfspace">depth.halfspace</a></code> for calculation of the Tukey depth.
</p>
<p><code><a href="#topic+depth.projection">depth.projection</a></code> for calculation of projection depth.
</p>
<p><code><a href="#topic+depth.simplicial">depth.simplicial</a></code> for calculation of simplicial depth.
</p>
<p><code><a href="#topic+depth.simplicialVolume">depth.simplicialVolume</a></code> for calculation of simplicial volume depth.
</p>
<p><code><a href="#topic+depth.spatial">depth.spatial</a></code> for calculation of spatial depth.
</p>
<p><code><a href="#topic+depth.zonoid">depth.zonoid</a></code> for calculation of zonoid depth.
</p>
<p><code><a href="#topic+depth.potential">depth.potential</a></code> for calculation of data potential.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># 5-dimensional normal distribution
data &lt;- mvrnorm(1000, rep(0, 5), 
                matrix(c(1, 0, 0, 0, 0, 
                         0, 2, 0, 0, 0, 
                         0, 0, 3, 0, 0, 
                         0, 0, 0, 2, 0, 
                         0, 0, 0, 0, 1),
                nrow = 5))
x &lt;- mvrnorm(10, rep(1, 5), 
             matrix(c(1, 0, 0, 0, 0, 
                      0, 1, 0, 0, 0, 
                      0, 0, 1, 0, 0, 
                      0, 0, 0, 1, 0, 
                      0, 0, 0, 0, 1),
             nrow = 5))
                
depths &lt;- depth.Mahalanobis(x, data)
cat("Depths moment: ", depths, "\n")
depths &lt;- depth.Mahalanobis(x, data, mah.estimate = "MCD", mah.parMcd = 0.75)
cat("Depths MCD: ", depths, "\n")
</code></pre>

<hr>
<h2 id='depth.potential'>
Calculate Potential of the Data
</h2><span id='topic+depth.potential'></span>

<h3>Description</h3>

<p>Calculate the potential of the points w.r.t. a multivariate data set. The potential is the kernel-estimated density multiplied by the prior probability of a class. Different from the data depths, a density estimate measures at a given point how much mass is located around it.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>depth.potential (x, data, pretransform = "1Mom", 
                kernel = "GKernel", kernel.bandwidth = NULL, mah.parMcd = 0.75)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="depth.potential_+3A_x">x</code></td>
<td>

<p>Matrix of objects (numerical vector as one object) whose depth is to be calculated; each row contains a <code class="reqn">d</code>-variate point. Should have the same dimension as <code>data</code>.
</p>
</td></tr>
<tr><td><code id="depth.potential_+3A_data">data</code></td>
<td>

<p>Matrix of data where each row contains a <code class="reqn">d</code>-variate point, w.r.t. which the depth is to be calculated.
</p>
</td></tr>
<tr><td><code id="depth.potential_+3A_pretransform">pretransform</code></td>
<td>

<p>The method of data scaling. 
</p>
<p><code>NULL</code> to use the original data, 
</p>
<p><code>1Mom</code> or <code>NMom</code> for scaling using data moments, 
</p>
<p><code>1MCD</code> or <code>NMCD</code> for scaling using robust data moments (Minimum Covariance Determinant (MCD) ).
</p>
</td></tr>
<tr><td><code id="depth.potential_+3A_kernel">kernel</code></td>
<td>

<p><code>"EDKernel"</code> for the kernel of type 1/(1+kernel.bandwidth*EuclidianDistance2(x, y)), 
</p>
<p><code>"GKernel"</code> [default and recommended] for the simple Gaussian kernel, 
</p>
<p><code>"EKernel"</code> exponential kernel: exp(-kernel.bandwidth*EuclidianDistance(x, y)), 
</p>
<p><code>"VarGKernel"</code> variable Gaussian kernel, where <code>kernel.bandwidth</code> is proportional to the <code>depth.zonoid</code> of a point.
</p>
</td></tr>
<tr><td><code id="depth.potential_+3A_kernel.bandwidth">kernel.bandwidth</code></td>
<td>

<p>the single bandwidth parameter of the kernel. If <code>NULL</code> - the Scott's rule of thumb is used.
</p>
</td></tr>
<tr><td><code id="depth.potential_+3A_mah.parmcd">mah.parMcd</code></td>
<td>

<p>is the value of the argument <code>alpha</code> for the function <code><a href="robustbase.html#topic+covMcd">covMcd</a></code>; is used when <code>pretransform = "*MCD"</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The potential is the kernel-estimated density multiplied by the prior probability of a class.
The kernel bandwidth matrix is decomposed into two parts, one of which describes the form of the data, and the other the width of the kernel. Then the first part is used to transform the data using the moments, while the second is employed as a parameter of the kernel and tuned to achieve the best separation.
For details see Pokotylo and Mosler (2015).
</p>


<h3>Value</h3>

<p>Numerical vector of potentials, one for each row in <code>x</code>; or one potential value if <code>x</code> is a numerical vector.
</p>


<h3>References</h3>

<p>Aizerman, M.A., Braverman, E.M., and Rozonoer, L.I. (1970). <em>The Method of Potential Functions in the Theory of Machine Learning</em>. Nauka (Moscow).
</p>
<p>Pokotylo, O. and Mosler, K. (2015). Classification with the pot-pot plot. <em>Mimeo</em>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+depth.halfspace">depth.halfspace</a></code> for calculation of the Tukey depth.
</p>
<p><code><a href="#topic+depth.Mahalanobis">depth.Mahalanobis</a></code> for calculation of Mahalanobis depth.
</p>
<p><code><a href="#topic+depth.projection">depth.projection</a></code> for calculation of projection depth.
</p>
<p><code><a href="#topic+depth.simplicial">depth.simplicial</a></code> for calculation of simplicial depth.
</p>
<p><code><a href="#topic+depth.simplicialVolume">depth.simplicialVolume</a></code> for calculation of simplicial volume depth.
</p>
<p><code><a href="#topic+depth.spatial">depth.spatial</a></code> for calculation of spatial depth.
</p>
<p><code><a href="#topic+depth.zonoid">depth.zonoid</a></code> for calculation of zonoid depth.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># 3-dimensional normal distribution
data &lt;- mvrnorm(200, rep(0, 3), 
                matrix(c(1, 0, 0,
                         0, 2, 0, 
                         0, 0, 1),
                       nrow = 3))
x &lt;- mvrnorm(10, rep(1, 3), 
             matrix(c(1, 0, 0,
                      0, 1, 0, 
                      0, 0, 1),
                    nrow = 3))

# potential with rule of thumb bandwidth
pot &lt;- depth.potential(x, data)
cat("Potentials: ", pot, "\n")

# potential with bandwidth = 0.1
pot &lt;- depth.potential(x, data, kernel.bandwidth = 0.1)
cat("Potentials: ", pot, "\n")

# potential with robust MCD scaling
pot &lt;- depth.potential(x, data, kernel.bandwidth = 0.1, 
                      pretransform = "NMCD", mah.parMcd = 0.6)
cat("Potentials: ", pot, "\n")
</code></pre>

<hr>
<h2 id='depth.projection'>
Calculate Projection Depth
</h2><span id='topic+depth.projection'></span>

<h3>Description</h3>

<p>Calculates the projection depth of points w.r.t. a multivariate data set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  depth.projection(x, data, method = "random", num.directions = 1000, seed = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="depth.projection_+3A_x">x</code></td>
<td>

<p>Matrix of objects (numerical vector as one object) whose depth is to be calculated; each row contains a <code class="reqn">d</code>-variate point. Should have the same dimension as <code>data</code>.
</p>
</td></tr>
<tr><td><code id="depth.projection_+3A_data">data</code></td>
<td>

<p>Matrix of data where each row contains a <code class="reqn">d</code>-variate point, w.r.t. which the depth is to be calculated.
</p>
</td></tr>
<tr><td><code id="depth.projection_+3A_method">method</code></td>
<td>

<p>to be used in calculations.
</p>
<p><code>"random"</code> Here the depth is determined as the minimum univariate depth of the data projected on lines in several directions. The directions are distributed uniformly on the <code class="reqn">(d-1)</code>-sphere; the same direction set is used for all points.
</p>
<p><code>"linearize"</code> The Nelder-Mead method for function minimization, taken from Olsson, Journal of Quality Technology, 1974, 6, 56.
</p>
</td></tr>
<tr><td><code id="depth.projection_+3A_num.directions">num.directions</code></td>
<td>

<p>Number of random directions to be generated for <code>method = "random"</code>. With the growth of n the complexity grows linearly for the same number of directions. 
</p>
</td></tr>
<tr><td><code id="depth.projection_+3A_seed">seed</code></td>
<td>

<p>the random seed. The default value <code>seed=0</code> makes no changes.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Calculates projection depth. Projection depth, similar to Mahalanobis depth, is based on a measure of outlyingness,
used by Stahel (1981) and Donoho (1982), and has been first formulated by Liu (1992). The
worst case outlyingness is obtained by maximizing an outlyingness measure over all univariate
projections. In practice most
often <em>median</em>, and <em>median absolute deviation from the median </em>(MAD), are used as they are robust measures.
</p>


<h3>Value</h3>

<p>Numerical vector of depths, one for each row in <code>x</code>; or one depth value if <code>x</code> is a numerical vector.
</p>


<h3>Author(s)</h3>

<p>R-codes for the &quot;linearize&quot; method were written by Subhajit Dutta.
</p>


<h3>References</h3>

<p>Donoho, D.L. (1982). <em>Breakdown properties of multivariate location estimators</em>. Ph.D. qualifying paper. Department of Statistics, Harvard University.
</p>
<p>Liu, R.Y. (1992). Data depth and multivariate rank tests. In: Dodge, Y. (ed.), L1-Statistics and Related Methods, North-Holland (Amsterdam), 279&ndash;294.
</p>
<p>Liu, X. and Zuo, Y. (2014). Computing projection depth and its associated estimators. <em>Statistics and Computing</em> <b>24</b> 51&ndash;63.
</p>
<p>Stahel, W.A. (1981). <em>Robust estimation: infinitesimal optimality and covariance matrix estimators</em>. Ph.D. thesis (in German). Eidgenossische Technische Hochschule Zurich.
</p>
<p>Zuo, Y.J. and Lai, S.Y. (2011). Exact computation of bivariate projection depth and the Stahel-Donoho estimator. <em>Computational Statistics and Data Analysis</em> <b>55</b> 1173&ndash;1179.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+depth.halfspace">depth.halfspace</a></code> for calculation of the Tukey depth.
</p>
<p><code><a href="#topic+depth.Mahalanobis">depth.Mahalanobis</a></code> for calculation of Mahalanobis depth.
</p>
<p><code><a href="#topic+depth.simplicial">depth.simplicial</a></code> for calculation of simplicial depth.
</p>
<p><code><a href="#topic+depth.simplicialVolume">depth.simplicialVolume</a></code> for calculation of simplicial volume depth.
</p>
<p><code><a href="#topic+depth.spatial">depth.spatial</a></code> for calculation of spatial depth.
</p>
<p><code><a href="#topic+depth.zonoid">depth.zonoid</a></code> for calculation of zonoid depth.
</p>
<p><code><a href="#topic+depth.potential">depth.potential</a></code> for calculation of data potential.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  # 5-dimensional normal distribution
  data &lt;- mvrnorm(100, rep(0, 5), 
                  matrix(c(1, 0, 0, 0, 0, 
                           0, 2, 0, 0, 0, 
                           0, 0, 3, 0, 0, 
                           0, 0, 0, 2, 0, 
                           0, 0, 0, 0, 1),
                         nrow = 5))
  x &lt;- mvrnorm(10, rep(1, 5), 
               matrix(c(1, 0, 0, 0, 0, 
                        0, 1, 0, 0, 0, 
                        0, 0, 1, 0, 0, 
                        0, 0, 0, 1, 0, 
                        0, 0, 0, 0, 1),
                      nrow = 5))
  
  depths &lt;- depth.projection(x, data, method = "random", num.directions = 1000)
  cat("Depths random: ", depths, "\n")
  depths &lt;- depth.projection(x, data, method = "linearize")
  cat("Depths linearize: ", depths, "\n")
</code></pre>

<hr>
<h2 id='depth.qhpeeling'>
Calculate Convex Hull Peeling Depth
</h2><span id='topic+depth.qhpeeling'></span>

<h3>Description</h3>

<p>Calculates the convex hull peeling depth of points w.r.t. a multivariate data set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>depth.qhpeeling(x, data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="depth.qhpeeling_+3A_x">x</code></td>
<td>

<p>Matrix of objects (numerical vector as one object) whose depth is to be calculated; each row contains a <code class="reqn">d</code>-variate point. Should have the same dimension as <code>data</code>.
</p>
</td></tr>
<tr><td><code id="depth.qhpeeling_+3A_data">data</code></td>
<td>

<p>Matrix of data where each row contains a <code class="reqn">d</code>-variate point, w.r.t. which the depth is to be calculated.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Calculates the convex hull peeling depth (Eddy, 1982; see also Cascos, 2009).
</p>


<h3>Value</h3>

<p>Numerical vector of depths, one for each row in <code>x</code>; or one depth value if <code>x</code> is a numerical vector. Each depth value equals the number of the convex hulls to be peeled from <code>data</code> so that (the corresponding row of) <code>x</code> is not contained in the convex hull of the rest of the data; the depths are normalized by the number of points in <code>data</code>.
</p>


<h3>References</h3>

<p>Eddy, W.F. (1982). Convex hull peeling. In: Caussinus, H., Ettinger, P. and Tomassone, R. (eds), <em>COMPSTAT 1982. Proceedings in computational statistics</em>, Physica-Verlag (Vienna), 42&ndash;47.
</p>
<p>Cascos, I. (2009). Data depth: multivariate statistics and geometry. In: Kendall, W.S. and Molchanov, I. (eds) <em>New Perspectives in Stochastic Geometry</em>, Clarendon/Oxford University Press (Oxford).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+depth.halfspace">depth.halfspace</a></code> for calculation of the Tukey depth.
</p>
<p><code><a href="#topic+depth.L2">depth.L2</a></code> for calculation of L2-depth.
</p>
<p><code><a href="#topic+depth.Mahalanobis">depth.Mahalanobis</a></code> for calculation of Mahalanobis depth.
</p>
<p><code><a href="#topic+depth.projection">depth.projection</a></code> for calculation of projection depth.
</p>
<p><code><a href="#topic+depth.simplicial">depth.simplicial</a></code> for calculation of simplicial depth.
</p>
<p><code><a href="#topic+depth.simplicialVolume">depth.simplicialVolume</a></code> for calculation of simplicial volume depth.
</p>
<p><code><a href="#topic+depth.spatial">depth.spatial</a></code> for calculation of spatial depth.
</p>
<p><code><a href="#topic+depth.potential">depth.potential</a></code> for calculation of data potential.
</p>
<p><code><a href="#topic+depth.zonoid">depth.zonoid</a></code> for calculation of zonoid depth.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Mixture of 3-variate normal distributions
data &lt;- mvrnorm(25, rep(0, 3), diag(3))
x &lt;- rbind(mvrnorm(10, rep(1, 3), diag(3)), data)
depths &lt;- depth.qhpeeling(x, data)
cat("Depths:", depths, "\n")
</code></pre>

<hr>
<h2 id='depth.sample'>Fast Depth Computation for Univariate and Bivariate Random Samples</h2><span id='topic+depth.sample'></span>

<h3>Description</h3>

<p>Faster implementation of the halfspace and the simplicial depth. Computes the depth 
of a whole random sample of a univariate or a bivariate data in one run.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>depth.sample(A, B)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="depth.sample_+3A_a">A</code></td>
<td>
<p>Univariate or bivariate points whose depth is computed, represented by a matrix of 
size <code>m*2</code>. <code>m</code> stands for the number of points, <code>d</code> is 1 for univariate and 2 
for bivariate data.</p>
</td></tr>
<tr><td><code id="depth.sample_+3A_b">B</code></td>
<td>
<p>Random sample points with respect to which the depth of <code>A</code> is computed. 
<code>B</code> is represented by a matrix of size <code>n*2</code>, where <code>n</code> is the sample size.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function returns vectors of sample halfspace and simplicial depth values.
</p>


<h3>Value</h3>

<p>Vector of length <code>m</code> of depth halfspace depth values is returned.
</p>


<h3>Author(s)</h3>

<p>Stanislav Nagy, <a href="mailto:nagy@karlin.mff.cuni.cz">nagy@karlin.mff.cuni.cz</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+depth.halfspace">depth.halfspace</a></code>
</p>
<p><code><a href="#topic+depth.simplicial">depth.simplicial</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n = 100
m = 150
A = matrix(rnorm(2*n),ncol=2)
B = matrix(rnorm(2*m),ncol=2)
depth.sample(A,B)
system.time(D1&lt;-depth.halfspace(A,B))
system.time(D2&lt;-depth.sample(A,B))
max(D1-D2$Half)

A = rnorm(100)
B = rnorm(150)
depth.sample(A,B)
# depth.halfspace(matrix(A,ncol=1),matrix(B,ncol=1))

</code></pre>

<hr>
<h2 id='depth.simplicial'>
Calculate Simplicial Depth
</h2><span id='topic+depth.simplicial'></span>

<h3>Description</h3>

<p>Calculates the simplicial depth of points w.r.t. a multivariate data set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>depth.simplicial(x, data, exact = F, k = 0.05, seed = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="depth.simplicial_+3A_x">x</code></td>
<td>

<p>Matrix of objects (numerical vector as one object) whose depth is to be calculated; each row contains a <code class="reqn">d</code>-variate point. Should have the same dimension as <code>data</code>.
</p>
</td></tr>
<tr><td><code id="depth.simplicial_+3A_data">data</code></td>
<td>

<p>Matrix of data where each row contains a <code class="reqn">d</code>-variate point, w.r.t. which the depth is to be calculated.
</p>
</td></tr>
<tr><td><code id="depth.simplicial_+3A_exact">exact</code></td>
<td>

<p><code>exact=F</code> (by default) implies the approximative algorithm, considering <code>k</code> simplices, <code>exact=T</code> implies the exact algorithm.
</p>
</td></tr>
<tr><td><code id="depth.simplicial_+3A_k">k</code></td>
<td>

<p>Number (<code class="reqn">k&gt;1</code>) or portion (if <code class="reqn">0&lt;k&lt;1</code>) of simplices that are considered if <code>exact=F</code>. If <code class="reqn">k&gt;1</code>, then the algorithmic complexity is polynomial in <code class="reqn">d</code> but is independent of the number of observations in <code>data</code>, given <code class="reqn">k</code>. If <code class="reqn">0&lt;k&lt;1</code>, then the algorithmic complexity is exponential in the number of observations in <code>data</code>, but the calculation precision stays approximately the same.
</p>
</td></tr>
<tr><td><code id="depth.simplicial_+3A_seed">seed</code></td>
<td>

<p>the random seed. The default value <code>seed=0</code> makes no changes.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Calculates simplicial depth. Simplicial depth is counted as a probability that a point lies in a simplex, built on <code class="reqn">d+1</code> data points.
</p>


<h3>Value</h3>

<p>Numerical vector of depths, one for each row in <code>x</code>; or one depth value if <code>x</code> is a numerical vector.
</p>


<h3>References</h3>

<p>Chaudhuri, P. (1996). On a geometric notion of quantiles for multivariate data. <em>Journal of the American Statistical Association</em> <b>91</b> 862&ndash;872.
</p>
<p>Liu, R. Y. (1990). On a notion of data depth based on random simplices. <em>The Annals of Statistics</em> <b>18</b> 405&ndash;414.
</p>
<p>Rousseeuw, P.J. and Ruts, I. (1996). Algorithm AS 307: Bivariate location depth. <em>Journal of the Royal Statistical Society. Seriec C (Applied Statistics)</em> <b>45</b> 516&ndash;526.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+depth.halfspace">depth.halfspace</a></code> for calculation of the Tukey depth.
</p>
<p><code><a href="#topic+depth.Mahalanobis">depth.Mahalanobis</a></code> for calculation of Mahalanobis depth.
</p>
<p><code><a href="#topic+depth.projection">depth.projection</a></code> for calculation of projection depth.
</p>
<p><code><a href="#topic+depth.simplicialVolume">depth.simplicialVolume</a></code> for calculation of simplicial volume depth.
</p>
<p><code><a href="#topic+depth.spatial">depth.spatial</a></code> for calculation of spatial depth.
</p>
<p><code><a href="#topic+depth.zonoid">depth.zonoid</a></code> for calculation of zonoid depth.
</p>
<p><code><a href="#topic+depth.potential">depth.potential</a></code> for calculation of data potential.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># 3-dimensional normal distribution
data &lt;- mvrnorm(20, rep(0, 3), 
                matrix(c(1, 0, 0,
                         0, 2, 0,
                         0, 0, 1),
                       nrow = 3))
x &lt;- mvrnorm(10, rep(1, 3), 
             matrix(c(1, 0, 0,
                      0, 1, 0,
                      0, 0, 1),
                    nrow = 3))

#exact
depths &lt;- depth.simplicial(x, data, exact = TRUE)
cat("Depths: ", depths, "\n")

#approximative
depths &lt;- depth.simplicial(x, data, exact = FALSE, k = 0.2)
cat("Depths: ", depths, "\n")
</code></pre>

<hr>
<h2 id='depth.simplicialVolume'>
Calculate Simplicial Volume Depth
</h2><span id='topic+depth.simplicialVolume'></span>

<h3>Description</h3>

<p>Calculates the simpicial volume depth of points w.r.t. a multivariate data set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>depth.simplicialVolume(x, data, exact = F, k = 0.05, mah.estimate = "moment", 
                       mah.parMcd = 0.75, seed = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="depth.simplicialVolume_+3A_x">x</code></td>
<td>

<p>Matrix of objects (numerical vector as one object) whose depth is to be calculated; each row contains a <code class="reqn">d</code>-variate point. Should have the same dimension as <code>data</code>.
</p>
</td></tr>
<tr><td><code id="depth.simplicialVolume_+3A_data">data</code></td>
<td>

<p>Matrix of data where each row contains a <code class="reqn">d</code>-variate point, w.r.t. which the depth is to be calculated.
</p>
</td></tr>
<tr><td><code id="depth.simplicialVolume_+3A_exact">exact</code></td>
<td>

<p><code>exact=F</code> (by default) implies the approximative algorithm, considering <code>k</code> simplices, <code>exact=T</code> implies the exact algorithm.
</p>
</td></tr>
<tr><td><code id="depth.simplicialVolume_+3A_k">k</code></td>
<td>

<p>Number (<code class="reqn">k&gt;1</code>) or portion (if <code class="reqn">0&lt;k&lt;1</code>) of simplices that are considered if <code>exact=F</code>. If <code class="reqn">k&gt;1</code>, then the algorithmic complexity is polynomial in <code class="reqn">d</code> but is independent of the number of observations in <code>data</code>, given <code class="reqn">k</code>. If <code class="reqn">0&lt;k&lt;1</code>, then the algorithmic complexity is exponential in the number of observations in <code>data</code>, but the calculation precision stays approximately the same.
</p>
</td></tr>
<tr><td><code id="depth.simplicialVolume_+3A_mah.estimate">mah.estimate</code></td>
<td>

<p>A character string specifying affine-invariance adjustment; can be <code>"none"</code>, <code>"moment"</code> or <code>"MCD"</code>, determining whether no affine-invariance adjustemt or moment or Minimum Covariance Determinant (MCD) (see <code><a href="robustbase.html#topic+covMcd">covMcd</a></code>) estimates of the covariance are used. By default <code>"moment"</code> is used.
</p>
</td></tr>
<tr><td><code id="depth.simplicialVolume_+3A_mah.parmcd">mah.parMcd</code></td>
<td>

<p>The value of the argument <code>alpha</code> for the function <code><a href="robustbase.html#topic+covMcd">covMcd</a></code>; is used when <code>mah.estimate =</code> <code>"MCD"</code>.
</p>
</td></tr>
<tr><td><code id="depth.simplicialVolume_+3A_seed">seed</code></td>
<td>

<p>The random seed. The default value <code>seed=0</code> makes no changes.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Calculates Oja depth (also: Simplicial volume depth). 
At first the Oja outlyingness function <code>O(x,data)</code> is calculated as the average of the volumes of simplices built on <code class="reqn">d</code> data points and the measurement point <code>x</code> (Oja, 1983).
</p>
<p>Zuo and Serfling (2000) proposed Oja depth based on the Oja outlyingness function as <code>1/(1 + O(x,data)/S)</code>, where S is a square root of the determinant of <code>cov(data)</code>, which makes the depth function affine-invariant.
</p>


<h3>Value</h3>

<p>Numerical vector of depths, one for each row in <code>x</code>; or one depth value if <code>x</code> is a numerical vector.
</p>


<h3>References</h3>

<p>Oja, H. (1983). Descriptive statistics for multivariate distributions. <em>Statistics &amp; Probability Letters</em> <b>1</b> 327&ndash;332.
</p>
<p>Zuo, Y.J. and Serfling, R. (2000). General notions of statistical depth function. <em>The Annals of Statistics</em> <b>28</b> 461&ndash;482.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+depth.halfspace">depth.halfspace</a></code> for calculation of the Tukey depth.
</p>
<p><code><a href="#topic+depth.Mahalanobis">depth.Mahalanobis</a></code> for calculation of Mahalanobis depth.
</p>
<p><code><a href="#topic+depth.projection">depth.projection</a></code> for calculation of projection depth.
</p>
<p><code><a href="#topic+depth.simplicial">depth.simplicial</a></code> for calculation of simplicial depth.
</p>
<p><code><a href="#topic+depth.spatial">depth.spatial</a></code> for calculation of spatial depth.
</p>
<p><code><a href="#topic+depth.zonoid">depth.zonoid</a></code> for calculation of zonoid depth.
</p>
<p><code><a href="#topic+depth.potential">depth.potential</a></code> for calculation of data potential.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># 3-dimensional normal distribution
data &lt;- mvrnorm(20, rep(0, 3), 
                matrix(c(1, 0, 0,
                         0, 2, 0,
                         0, 0, 1),
                       nrow = 3))
x &lt;- mvrnorm(10, rep(1, 3), 
             matrix(c(1, 0, 0,
                      0, 1, 0,
                      0, 0, 1),
                    nrow = 3))

#exact
depths &lt;- depth.simplicialVolume(x, data, exact = TRUE)
cat("Depths: ", depths, "\n")

#approximative
depths &lt;- depth.simplicialVolume(x, data, exact = FALSE, k = 0.2)
cat("Depths: ", depths, "\n")
</code></pre>

<hr>
<h2 id='depth.space.'>
Calculate Depth Space using the Given Depth
</h2><span id='topic+depth.space.'></span>

<h3>Description</h3>

<p>Calculates the representation of the training classes in depth space.
</p>
<p>The detailed descriptions are found in the corresponding topics.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>depth.space.(data, cardinalities, notion, ...)

## Mahalanobis depth
# depth.space.Mahalanobis(data, cardinalities, mah.estimate = "moment", mah.parMcd = 0.75)

## projection depth
# depth.space.projection(data, cardinalities, method = "random", num.directions = 1000)

## Tukey depth
# depth.space.halfspace(data, cardinalities, exact, alg, num.directions = 1000)

## spatial depth
# depth.space.spatial(data, cardinalities)

## zonoid depth
# depth.space.zonoid(data, cardinalities)

# Potential
# depth.space.potential(data, cardinalities, pretransform = "NMom", 
#            kernel = "GKernel", kernel.bandwidth = NULL, mah.parMcd = 0.75)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="depth.space._+3A_data">data</code></td>
<td>

<p>Matrix containing training sample where each row is a <code class="reqn">d</code>-dimensional object, and objects of each class are kept together so that the matrix can be thought of as containing blocks of objects representing classes.
</p>
</td></tr>
<tr><td><code id="depth.space._+3A_cardinalities">cardinalities</code></td>
<td>

<p>Numerical vector of cardinalities of each class in <code>data</code>, each entry corresponds to one class.
</p>
</td></tr>
<tr><td><code id="depth.space._+3A_notion">notion</code></td>
<td>

<p>The name of the depth notion (shall also work with <code><a href="#topic+Custom+20Methods">Custom Methods</a></code>).
</p>
</td></tr>
<tr><td><code id="depth.space._+3A_...">...</code></td>
<td>

<p>Additional parameters passed to the depth functions.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Matrix of objects, each object (row) is represented via its depths (columns) w.r.t. each of the classes of the training sample; order of the classes in columns corresponds to the one in the argument <code>cardinalities</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+depth.space.Mahalanobis">depth.space.Mahalanobis</a></code>
</p>
<p><code><a href="#topic+depth.space.projection">depth.space.projection</a></code>
</p>
<p><code><a href="#topic+depth.space.halfspace">depth.space.halfspace</a></code>
</p>
<p><code><a href="#topic+depth.space.spatial">depth.space.spatial</a></code>
</p>
<p><code><a href="#topic+depth.space.zonoid">depth.space.zonoid</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Generate a bivariate normal location-shift classification task
# containing 20 training objects
class1 &lt;- mvrnorm(10, c(0,0), 
                  matrix(c(1,1,1,4), nrow = 2, ncol = 2, byrow = TRUE))
class2 &lt;- mvrnorm(10, c(2,2), 
                  matrix(c(1,1,1,4), nrow = 2, ncol = 2, byrow = TRUE))
data &lt;- rbind(class1, class2)
# Get depth space using zonoid depth
depth.space.(data, c(10, 10), notion = "zonoid")
</code></pre>

<hr>
<h2 id='depth.space.halfspace'>
Calculate Depth Space using Halfspace Depth
</h2><span id='topic+depth.space.halfspace'></span>

<h3>Description</h3>

<p>Calculates the representation of the training classes in depth space using the halfspace depth.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>depth.space.halfspace(data, cardinalities, exact, method, num.directions = 1000, seed = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="depth.space.halfspace_+3A_data">data</code></td>
<td>

<p>Matrix containing training sample where each row is a <code class="reqn">d</code>-dimensional object, and objects of each class are kept together so that the matrix can be thought of as containing blocks of objects representing classes.
</p>
</td></tr>
<tr><td><code id="depth.space.halfspace_+3A_cardinalities">cardinalities</code></td>
<td>

<p>Numerical vector of cardinalities of each class in <code>data</code>, each entry corresponds to one class.
</p>
</td></tr>
<tr><td><code id="depth.space.halfspace_+3A_exact">exact</code></td>
<td>

<p>The type of the used method. The default is <code>exact=F</code>, which leads to approximate computation of the halfspace depth. For <code>exact=F</code>, <code>method="Sunif.1D"</code> is used by default. If <code>exact=T</code>, the halfspace depth is computed exactly, with <code>method="recursive"</code> by default.</p>
</td></tr>
<tr><td><code id="depth.space.halfspace_+3A_method">method</code></td>
<td>

<p>For <code>exact=F</code>, if <code>method="Sunif.1D"</code> (by default), the halfspace depth is computed approximately by being minimized over univariate projections (see details).
</p>
<p>For <code>exact=T</code>, the halfspace depth is calculated as the minimum over all combinations of <code class="reqn">k</code> points from <code>data</code> (see details). In this case parameter <code>method</code> specifies <code class="reqn">k</code>, with possible values <code class="reqn">1</code> for <code>method="recursive"</code> (by default), <code class="reqn">d-2</code> for <code>method="plane"</code>, <code class="reqn">d-1</code> for <code>method="line"</code>.
</p>
<p>The name of the method may be given as well as just parameter <code>exact</code>, in which case the default method will be used.
</p>
</td></tr>
<tr><td><code id="depth.space.halfspace_+3A_num.directions">num.directions</code></td>
<td>

<p>Number of random directions to be generated. As the same direction set is used for all observations, the algorithmic complexity of calculating the depth of each single point in <code>data</code> is logarithmic in the number of observations in <code>data</code>, given the number of directions, see Mozharovskyi et al. (2015), Section 2.3 for discussion.
</p>
</td></tr>
<tr><td><code id="depth.space.halfspace_+3A_seed">seed</code></td>
<td>

<p>The random seed. The default value <code>seed=0</code> makes no changes.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The depth representation is calculated in the same way as in <code><a href="#topic+depth.halfspace">depth.halfspace</a></code>, see References below for more information and details.
</p>


<h3>Value</h3>

<p>Matrix of objects, each object (row) is represented via its depths (columns) w.r.t. each of the classes of the training sample; order of the classes in columns corresponds to the one in the argument <code>cardinalities</code>.
</p>


<h3>References</h3>

<p>Cuesta-Albertos, J.A. and Nieto-Reyes, A. (2008). The random Tukey depth. <em>Computational Statistics and Data Analysis</em> <b>52</b> 4979&ndash;4988.
</p>
<p>Dyckerhoff, R. and Mozharovskyi, P. (2016). Exact computation of the halfspace depth. <em>Computational Statistics and Data Analysis</em> <b>98</b> 19&ndash;30.
</p>
<p>Mozharovskyi, P., Mosler, K., and Lange, T. (2015). Classifying real-world data with the DD<code class="reqn">\alpha</code>-procedure. <em>Advances in Data Analysis and Classification</em> <b>9</b> 287&ndash;314.
</p>
<p>Rousseeuw, P.J. and Ruts, I. (1996). Algorithm AS 307: Bivariate location depth. <em>Journal of the Royal Statistical Society. Series C (Applied Statistics)</em> <b>45</b> 516&ndash;526.
</p>
<p>Tukey, J.W. (1974). Mathematics and the picturing of data. In: <em>Proceeding of the International Congress of Mathematicians</em>, Vancouver, 523&ndash;531.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ddalpha.train">ddalpha.train</a></code> and <code><a href="#topic+ddalpha.classify">ddalpha.classify</a></code> for application, <code><a href="#topic+depth.halfspace">depth.halfspace</a></code> for calculation of the Tukey depth.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Generate a bivariate normal location-shift classification task
# containing 20 training objects
class1 &lt;- mvrnorm(10, c(0,0), 
                  matrix(c(1,1,1,4), nrow = 2, ncol = 2, byrow = TRUE))
class2 &lt;- mvrnorm(10, c(1,1), 
                  matrix(c(1,1,1,4), nrow = 2, ncol = 2, byrow = TRUE))
data &lt;- rbind(class1, class2)
plot(data, col = c(rep(1,10), rep(2,10)))
# Get depth space using the random Tukey depth
dhA = depth.space.halfspace(data, c(10, 10))
(dhA)

# Get depth space using default exact method - "recursive"
dhE = depth.space.halfspace(data, c(10, 10), exact = TRUE)
(dhE)

data &lt;- getdata("hemophilia")
cardinalities = c(sum(data$gr == "normal"), sum(data$gr == "carrier"))
depth.space.halfspace(data[,1:2], cardinalities)
</code></pre>

<hr>
<h2 id='depth.space.Mahalanobis'>
Calculate Depth Space using Mahalanobis Depth
</h2><span id='topic+depth.space.Mahalanobis'></span>

<h3>Description</h3>

<p>Calculates the representation of the training classes in depth space using Mahalanobis depth.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>depth.space.Mahalanobis(data, cardinalities, mah.estimate = "moment", mah.parMcd = 0.75)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="depth.space.Mahalanobis_+3A_data">data</code></td>
<td>

<p>Matrix containing training sample where each row is a <code class="reqn">d</code>-dimensional object, and objects of each class are kept together so that the matrix can be thought of as containing blocks of objects representing classes.
</p>
</td></tr>
<tr><td><code id="depth.space.Mahalanobis_+3A_cardinalities">cardinalities</code></td>
<td>

<p>Numerical vector of cardinalities of each class in <code>data</code>, each entry corresponds to one class.
</p>
</td></tr>
<tr><td><code id="depth.space.Mahalanobis_+3A_mah.estimate">mah.estimate</code></td>
<td>
<p> is a character string specifying which estimates to use when calculating the Mahalanobis depth; can be <code>"moment"</code> or <code>"MCD"</code>, determining whether traditional moment or Minimum Covariance Determinant (MCD) (see <code><a href="robustbase.html#topic+covMcd">covMcd</a></code>) estimates for mean and covariance are used. By default <code>"moment"</code> is used.
</p>
</td></tr>
<tr><td><code id="depth.space.Mahalanobis_+3A_mah.parmcd">mah.parMcd</code></td>
<td>

<p>is the value of the argument <code>alpha</code> for the function <code><a href="robustbase.html#topic+covMcd">covMcd</a></code>; is used when <code>mah.estimate =</code> <code>"MCD"</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The depth representation is calculated in the same way as in <code><a href="#topic+depth.Mahalanobis">depth.Mahalanobis</a></code>, see 'References' for more information and details.
</p>


<h3>Value</h3>

<p>Matrix of objects, each object (row) is represented via its depths (columns) w.r.t. each of the classes of the training sample; order of the classes in columns corresponds to the one in the argument <code>cardinalities</code>.
</p>


<h3>References</h3>

<p>Mahalanobis, P. (1936). On the generalized distance in statistics. <em>Proceedings of the National
Academy India</em> <b>12</b> 49&ndash;55.
</p>
<p>Liu, R.Y. (1992). Data depth and multivariate rank tests. In: Dodge, Y. (ed.), <em>L1-Statistics and Related Methods</em>, North-Holland (Amsterdam), 279&ndash;294.
</p>
<p>Lopuhaa, H.P. and Rousseeuw, P.J. (1991). Breakdown points of affine equivariant estimators of multivariate location and covariance matrices. <em>The Annals of Statistics</em> <b>19</b> 229&ndash;248.
</p>
<p>Rousseeuw, P.J. and Leroy, A.M. (1987). Robust Regression and Outlier Detection. John Wiley &amp; Sons (New York).
</p>
<p>Zuo, Y.J. and Serfling, R. (2000). General notions of statistical depth function. <em>The Annals of Statistics</em> <b>28</b> 461&ndash;482.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ddalpha.train">ddalpha.train</a></code> and <code><a href="#topic+ddalpha.classify">ddalpha.classify</a></code> for application, <code><a href="#topic+depth.Mahalanobis">depth.Mahalanobis</a></code> for calculation of Mahalanobis depth.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Generate a bivariate normal location-shift classification task
# containing 20 training objects
class1 &lt;- mvrnorm(10, c(0,0), 
                  matrix(c(1,1,1,4), nrow = 2, ncol = 2, byrow = TRUE))
class2 &lt;- mvrnorm(10, c(2,2), 
                  matrix(c(1,1,1,4), nrow = 2, ncol = 2, byrow = TRUE))
data &lt;- rbind(class1, class2)
# Get depth space using Mahalanobis depth
depth.space.Mahalanobis(data, c(10, 10))
depth.space.Mahalanobis(data, c(10, 10), mah.estimate = "MCD", mah.parMcd = 0.75)

data &lt;- getdata("hemophilia")
cardinalities = c(sum(data$gr == "normal"), sum(data$gr == "carrier"))
depth.space.Mahalanobis(data[,1:2], cardinalities)
</code></pre>

<hr>
<h2 id='depth.space.potential'>
Calculate Potential Space
</h2><span id='topic+depth.space.potential'></span>

<h3>Description</h3>

<p>Calculates the representation of the training classes in potential space.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>depth.space.potential(data, cardinalities, pretransform = "NMom", 
            kernel = "GKernel", kernel.bandwidth = NULL, mah.parMcd = 0.75)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="depth.space.potential_+3A_data">data</code></td>
<td>

<p>Matrix containing training sample where each row is a <code class="reqn">d</code>-dimensional object, and objects of each class are kept together so that the matrix can be thought of as containing blocks of objects representing classes.
</p>
</td></tr>
<tr><td><code id="depth.space.potential_+3A_cardinalities">cardinalities</code></td>
<td>

<p>Numerical vector of cardinalities of each class in <code>data</code>, each entry corresponds to one class.
</p>
</td></tr>
<tr><td><code id="depth.space.potential_+3A_pretransform">pretransform</code></td>
<td>

<p>The method of data scaling. 
</p>
<p><code>NULL</code> to use the original data, 
</p>
<p>The data may be scaled jointly or separately:
</p>
<p><code>1Mom</code> or <code>1MCD</code> for joint scaling of the classes, 
</p>
<p><code>NMom</code> or <code>NMCD</code> for separate scaling of the classes.
</p>
<p>You may use traditional moments or Minimum Covariance Determinant (MCD) estimates for mean and covariance:
</p>
<p><code>1Mom</code> or <code>NMom</code> for scaling using traditional data moments, 
</p>
<p><code>1MCD</code> or <code>NMCD</code> for scaling using robust MCD data moments.
</p>
</td></tr>
<tr><td><code id="depth.space.potential_+3A_kernel">kernel</code></td>
<td>

<p><code>"EDKernel"</code> for the kernel of type 1/(1+kernel.bandwidth*EuclidianDistance2(x, y)), 
</p>
<p><code>"GKernel"</code> [default and recommended] for the simple Gaussian kernel, 
</p>
<p><code>"EKernel"</code> exponential kernel: exp(-kernel.bandwidth*EuclidianDistance(x, y)), 
</p>
<p><code>"VarGKernel"</code> variable Gaussian kernel, where <code>kernel.bandwidth</code> is proportional to the <code>depth.zonoid</code> of a point.
</p>
</td></tr>
<tr><td><code id="depth.space.potential_+3A_kernel.bandwidth">kernel.bandwidth</code></td>
<td>

<p>the bandwidth parameter of the kernel. If <code>NULL</code> - the Scott's rule of thumb is used.
May be a single value for all classes, or a vector of values for each of the classes.
</p>
</td></tr>
<tr><td><code id="depth.space.potential_+3A_mah.parmcd">mah.parMcd</code></td>
<td>

<p>is the value of the argument <code>alpha</code> for the function <code><a href="robustbase.html#topic+covMcd">covMcd</a></code>; is used when <code>pretransform = "*MCD"</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The potential representation is calculated in the same way as in <code><a href="#topic+depth.potential">depth.potential</a></code>, see References below for more information and details.
</p>


<h3>Value</h3>

<p>Matrix of objects, each object (row) is represented via its potentials (columns) w.r.t. each of the classes of the training sample; order of the classes in columns corresponds to the one in the argument <code>cardinalities</code>.
</p>


<h3>References</h3>

<p>Aizerman, M.A., Braverman, E.M., and Rozonoer, L.I. (1970). <em>The Method of Potential Functions in the Theory of Machine Learning</em>. Nauka (Moscow).
</p>
<p>Pokotylo, O. and Mosler, K. (2015). Classification with the pot-pot plot. <em>Mimeo</em>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ddalpha.train">ddalpha.train</a></code> and <code><a href="#topic+ddalpha.classify">ddalpha.classify</a></code> for application, <code><a href="#topic+depth.potential">depth.potential</a></code> for calculation of the potential.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Generate a bivariate normal location-shift classification task
# containing 20 training objects
class1 &lt;- mvrnorm(50, c(0,0), 
                  matrix(c(1,1,1,4), nrow = 2, ncol = 2, byrow = TRUE))
class2 &lt;- mvrnorm(50, c(1,1), 
                  matrix(c(1,1,1,4), nrow = 2, ncol = 2, byrow = TRUE))
data &lt;- rbind(class1, class2)
plot(data, col = c(rep(1,50), rep(2,50)))
# potential with rule of thumb bandwidth
ds = depth.space.potential(data, c(50, 50))
# draw.ddplot(depth.space = ds, cardinalities = c(50, 50))

# potential with bandwidth = 0.5 and joint scaling
ds = depth.space.potential(data, c(50, 50), kernel.bandwidth = 0.5,
                           pretransform = "1Mom")
# draw.ddplot(depth.space = ds, cardinalities = c(50, 50))

# potential with bandwidth = 0.5 and separate scaling
ds = depth.space.potential(data, c(50, 50), kernel.bandwidth = 0.5, 
                           pretransform = "NahMom") # or without pretransform
# draw.ddplot(depth.space = ds, cardinalities = c(50, 50))

data &lt;- getdata("hemophilia")
cardinalities = c(sum(data$gr == "normal"), sum(data$gr == "carrier"))
ds = depth.space.potential(data[,1:2], cardinalities)
# draw.ddplot(depth.space = ds, cardinalities = cardinalities)

</code></pre>

<hr>
<h2 id='depth.space.projection'>
Calculate Depth Space using Projection Depth
</h2><span id='topic+depth.space.projection'></span>

<h3>Description</h3>

<p>Calculates the representation of the training classes in depth space using projection depth.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>depth.space.projection(data, cardinalities, 
                       method = "random", num.directions = 1000, seed = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="depth.space.projection_+3A_data">data</code></td>
<td>

<p>Matrix containing training sample where each row is a <code class="reqn">d</code>-dimensional object, and objects of each class are kept together so that the matrix can be thought of as containing blocks of objects representing classes.
</p>
</td></tr>
<tr><td><code id="depth.space.projection_+3A_cardinalities">cardinalities</code></td>
<td>

<p>Numerical vector of cardinalities of each class in <code>data</code>, each entry corresponds to one class.
</p>
</td></tr>
<tr><td><code id="depth.space.projection_+3A_method">method</code></td>
<td>

<p>to be used in calculations.
</p>
<p><code>"random"</code> Here the depth is determined as the minimum univariate depth of the data projected on lines in several directions. The directions are distributed uniformly on the <code class="reqn">(d-1)</code>-sphere; the same direction set is used for all points.
</p>
<p><code>"linearize"</code> The Nelder-Mead method for function minimization, taken from Olsson, Journal of Quality Technology, 1974, 6, 56. R-codes of this function were written by Subhajit Dutta.
</p>
</td></tr>
<tr><td><code id="depth.space.projection_+3A_num.directions">num.directions</code></td>
<td>

<p>Number of random directions to be generated for <code>method = "random"</code>. With the growth of n the complexity grows linearly for the same number of directions. 
</p>
</td></tr>
<tr><td><code id="depth.space.projection_+3A_seed">seed</code></td>
<td>

<p>the random seed. The default value <code>seed=0</code> makes no changes.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The depth representation is calculated in the same way as in <code><a href="#topic+depth.projection">depth.projection</a></code>, see 'References' for more information and details.
</p>


<h3>Value</h3>

<p>Matrix of objects, each object (row) is represented via its depths (columns) w.r.t. each of the classes of the training sample; order of the classes in columns corresponds to the one in the argument <code>cardinalities</code>.
</p>


<h3>References</h3>

<p>Donoho, D.L. (1982). <em>Breakdown properties of multivariate location estimators</em>. Ph.D. qualifying paper. Department of Statistics, Harvard University.
</p>
<p>Liu, R.Y. (1992). Data depth and multivariate rank tests. In: Dodge, Y. (ed.), L1-Statistics and Related Methods, North-Holland (Amsterdam), 279&ndash;294.
</p>
<p>Liu, X. and Zuo, Y. (2014). Computing projection depth and its associated estimators. <em>Statistics and Computing</em> <b>24</b> 51&ndash;63.
</p>
<p>Stahel, W.A. (1981). <em>Robust estimation: infinitesimal optimality and covariance matrix estimators</em>. Ph.D. thesis (in German). Eidgenossische Technische Hochschule Zurich.
</p>
<p>Zuo, Y.J. and Lai, S.Y. (2011). Exact computation of bivariate projection depth and the Stahel-Donoho estimator. <em>Computational Statistics and Data Analysis</em> <b>55</b> 1173&ndash;1179.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ddalpha.train">ddalpha.train</a></code> and <code><a href="#topic+ddalpha.classify">ddalpha.classify</a></code> for application, <code><a href="#topic+depth.projection">depth.projection</a></code> for calculation of projection depth.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Generate a bivariate normal location-shift classification task
# containing 20 training objects
class1 &lt;- mvrnorm(10, c(0,0), 
                  matrix(c(1,1,1,4), nrow = 2, ncol = 2, byrow = TRUE))
class2 &lt;- mvrnorm(10, c(2,2), 
                  matrix(c(1,1,1,4), nrow = 2, ncol = 2, byrow = TRUE))
data &lt;- rbind(class1, class2)
# Get depth space using projection depth
depth.space.projection(data, c(10, 10), method = "random", num.directions = 1000)
depth.space.projection(data, c(10, 10), method = "linearize")

data &lt;- getdata("hemophilia")
cardinalities = c(sum(data$gr == "normal"), sum(data$gr == "carrier"))
depth.space.projection(data[,1:2], cardinalities)
</code></pre>

<hr>
<h2 id='depth.space.simplicial'>
Calculate Depth Space using Simplicial Depth
</h2><span id='topic+depth.space.simplicial'></span>

<h3>Description</h3>

<p>Calculates the representation of the training classes in depth space using simplicial depth.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>depth.space.simplicial(data, cardinalities, exact = F, k = 0.05, seed = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="depth.space.simplicial_+3A_data">data</code></td>
<td>

<p>Matrix containing training sample where each row is a <code class="reqn">d</code>-dimensional object, and objects of each class are kept together so that the matrix can be thought of as containing blocks of objects representing classes.
</p>
</td></tr>
<tr><td><code id="depth.space.simplicial_+3A_cardinalities">cardinalities</code></td>
<td>

<p>Numerical vector of cardinalities of each class in <code>data</code>, each entry corresponds to one class.
</p>
</td></tr>
<tr><td><code id="depth.space.simplicial_+3A_exact">exact</code></td>
<td>

<p><code>exact=F</code> (by default) implies the approximative algorithm, considering <code>k</code> simplices, <code>exact=T</code> implies the exact algorithm.
</p>
</td></tr>
<tr><td><code id="depth.space.simplicial_+3A_k">k</code></td>
<td>

<p>Number (<code class="reqn">k&gt;1</code>) or portion (if <code class="reqn">0&lt;k&lt;1</code>) of simplices that are considered if <code>exact=F</code>. If <code class="reqn">k&gt;1</code>, then the algorithmic complexity is polynomial in <code class="reqn">d</code> but is independent of the number of observations in <code>data</code>, given <code class="reqn">k</code>. If <code class="reqn">0&lt;k&lt;1</code>, then the algorithmic complexity is exponential in the number of observations in <code>data</code>, but the calculation precision stays approximately the same.
</p>
</td></tr>
<tr><td><code id="depth.space.simplicial_+3A_seed">seed</code></td>
<td>

<p>The random seed. The default value <code>seed=0</code> makes no changes.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The depth representation is calculated in the same way as in <code><a href="#topic+depth.simplicial">depth.simplicial</a></code>, see 'References' for more information and details.
</p>


<h3>Value</h3>

<p>Matrix of objects, each object (row) is represented via its depths (columns) w.r.t. each of the classes of the training sample; order of the classes in columns corresponds to the one in the argument <code>cardinalities</code>.
</p>


<h3>References</h3>

<p>Chaudhuri, P. (1996). On a geometric notion of quantiles for multivariate data. <em>Journal of the American Statistical Association</em> <b>91</b> 862&ndash;872.
</p>
<p>Liu, R. Y. (1990). On a notion of data depth based on random simplices. <em>The Annals of Statistics</em> <b>18</b> 405&ndash;414.
</p>
<p>Rousseeuw, P.J. and Ruts, I. (1996). Algorithm AS 307: Bivariate location depth. <em>Journal of the Royal Statistical Society. Seriec C (Applied Statistics)</em> <b>45</b> 516&ndash;526.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ddalpha.train">ddalpha.train</a></code> and <code><a href="#topic+ddalpha.classify">ddalpha.classify</a></code> for application, <code><a href="#topic+depth.simplicial">depth.simplicial</a></code> for calculation of simplicial depth.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Generate a bivariate normal location-shift classification task
# containing 20 training objects
class1 &lt;- mvrnorm(10, c(0,0), 
                  matrix(c(1,1,1,4), nrow = 2, ncol = 2, byrow = TRUE))
class2 &lt;- mvrnorm(10, c(1,1), 
                  matrix(c(1,1,1,4), nrow = 2, ncol = 2, byrow = TRUE))
data &lt;- rbind(class1, class2)
# Get depth space using simplicial depth
depth.space.simplicial(data, c(10, 10))

data &lt;- getdata("hemophilia")
cardinalities = c(sum(data$gr == "normal"), sum(data$gr == "carrier"))
depth.space.simplicial(data[,1:2], cardinalities)
</code></pre>

<hr>
<h2 id='depth.space.simplicialVolume'>
Calculate Depth Space using Simplicial Volume Depth
</h2><span id='topic+depth.space.simplicialVolume'></span>

<h3>Description</h3>

<p>Calculates the representation of the training classes in depth space using simplicial volume depth.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>depth.space.simplicialVolume(data, cardinalities, exact = F, k = 0.05, 
                             mah.estimate = "moment", mah.parMcd = 0.75, seed = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="depth.space.simplicialVolume_+3A_data">data</code></td>
<td>

<p>Matrix containing training sample where each row is a <code class="reqn">d</code>-dimensional object, and objects of each class are kept together so that the matrix can be thought of as containing blocks of objects representing classes.
</p>
</td></tr>
<tr><td><code id="depth.space.simplicialVolume_+3A_cardinalities">cardinalities</code></td>
<td>

<p>Numerical vector of cardinalities of each class in <code>data</code>, each entry corresponds to one class.
</p>
</td></tr>
<tr><td><code id="depth.space.simplicialVolume_+3A_exact">exact</code></td>
<td>

<p><code>exact=F</code> (by default) implies the approximative algorithm, considering <code>k</code> simplices, <code>exact=T</code> implies the exact algorithm.
</p>
</td></tr>
<tr><td><code id="depth.space.simplicialVolume_+3A_k">k</code></td>
<td>

<p>Number (<code class="reqn">k&gt;1</code>) or portion (if <code class="reqn">0&lt;k&lt;1</code>) of simplices that are considered if <code>exact=F</code>. If <code class="reqn">k&gt;1</code>, then the algorithmic complexity is polynomial in <code class="reqn">d</code> but is independent of the number of observations in <code>data</code>, given <code class="reqn">k</code>. If <code class="reqn">0&lt;k&lt;1</code>, then the algorithmic complexity is exponential in the number of observations in <code>data</code>, but the calculation precision stays approximately the same.
</p>
</td></tr>
<tr><td><code id="depth.space.simplicialVolume_+3A_mah.estimate">mah.estimate</code></td>
<td>

<p>A character string specifying affine-invariance adjustment; can be <code>"none"</code>, <code>"moment"</code> or <code>"MCD"</code>, determining whether no affine-invariance adjustemt or moment or Minimum Covariance Determinant (MCD) (see <code><a href="robustbase.html#topic+covMcd">covMcd</a></code>) estimates of the covariance are used. By default <code>"moment"</code> is used.
</p>
</td></tr>
<tr><td><code id="depth.space.simplicialVolume_+3A_mah.parmcd">mah.parMcd</code></td>
<td>

<p>The value of the argument <code>alpha</code> for the function <code><a href="robustbase.html#topic+covMcd">covMcd</a></code>; is used when <code>mah.estimate =</code> <code>"MCD"</code>.
</p>
</td></tr>
<tr><td><code id="depth.space.simplicialVolume_+3A_seed">seed</code></td>
<td>

<p>The random seed. The default value <code>seed=0</code> makes no changes.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The depth representation is calculated in the same way as in <code><a href="#topic+depth.simplicialVolume">depth.simplicialVolume</a></code>, see References below for more information and details.
</p>


<h3>Value</h3>

<p>Matrix of objects, each object (row) is represented via its depths (columns) w.r.t. each of the classes of the training sample; order of the classes in columns corresponds to the one in the argument <code>cardinalities</code>.
</p>


<h3>References</h3>

<p>Oja, H. (1983). Descriptive statistics for multivariate distributions. <em>Statistics &amp; Probability Letters</em> <b>1</b> 327&ndash;332.
</p>
<p>Zuo, Y.J. and Serfling, R. (2000). General notions of statistical depth function. <em>The Annals of Statistics</em> <b>28</b> 461&ndash;482.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ddalpha.train">ddalpha.train</a></code> and <code><a href="#topic+ddalpha.classify">ddalpha.classify</a></code> for application, <code><a href="#topic+depth.simplicialVolume">depth.simplicialVolume</a></code> for calculation of simplicial depth.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Generate a bivariate normal location-shift classification task
# containing 20 training objects
class1 &lt;- mvrnorm(10, c(0,0), 
                  matrix(c(1,1,1,4), nrow = 2, ncol = 2, byrow = TRUE))
class2 &lt;- mvrnorm(10, c(2,2), 
                  matrix(c(1,1,1,4), nrow = 2, ncol = 2, byrow = TRUE))
data &lt;- rbind(class1, class2)
# Get depth space using Oja depth
depth.space.simplicialVolume(data, c(10, 10))

data &lt;- getdata("hemophilia")
cardinalities = c(sum(data$gr == "normal"), sum(data$gr == "carrier"))
depth.space.simplicialVolume(data[,1:2], cardinalities)
</code></pre>

<hr>
<h2 id='depth.space.spatial'>
Calculate Depth Space using Spatial Depth
</h2><span id='topic+depth.space.spatial'></span>

<h3>Description</h3>

<p>Calculates the representation of the training classes in depth space using spatial depth.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>depth.space.spatial(data, cardinalities, mah.estimate = "moment", mah.parMcd = 0.75)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="depth.space.spatial_+3A_data">data</code></td>
<td>

<p>Matrix containing training sample where each row is a <code class="reqn">d</code>-dimensional object, and objects of each class are kept together so that the matrix can be thought of as containing blocks of objects representing classes.
</p>
</td></tr>
<tr><td><code id="depth.space.spatial_+3A_cardinalities">cardinalities</code></td>
<td>

<p>Numerical vector of cardinalities of each class in <code>data</code>, each entry corresponds to one class.
</p>
</td></tr>
<tr><td><code id="depth.space.spatial_+3A_mah.estimate">mah.estimate</code></td>
<td>
<p> is a character string specifying which estimates to use when calculating sample covariance matrix; can be <code>"none"</code>, <code>"moment"</code> or <code>"MCD"</code>, determining whether traditional moment or Minimum Covariance Determinant (MCD) (see <code><a href="robustbase.html#topic+covMcd">covMcd</a></code>) estimates for mean and covariance are used. By default <code>"moment"</code> is used. With <code>"none"</code> the non-affine invariant version of Spatial depth is calculated
</p>
</td></tr>
<tr><td><code id="depth.space.spatial_+3A_mah.parmcd">mah.parMcd</code></td>
<td>

<p>is the value of the argument <code>alpha</code> for the function <code><a href="robustbase.html#topic+covMcd">covMcd</a></code>; is used when <code>mah.estimate =</code> <code>"MCD"</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The depth representation is calculated in the same way as in <code><a href="#topic+depth.spatial">depth.spatial</a></code>, see 'References' for more information and details.
</p>


<h3>Value</h3>

<p>Matrix of objects, each object (row) is represented via its depths (columns) w.r.t. each of the classes of the training sample; order of the classes in columns corresponds to the one in the argument <code>cardinalities</code>.
</p>


<h3>References</h3>

<p>Chaudhuri, P. (1996). On a geometric notion of quantiles for multivariate data. <em>Journal of the Americal Statistical Association</em> <b>91</b> 862&ndash;872.
</p>
<p>Koltchinskii, V.I. (1997). M-estimation, convexity and quantiles. <em>The Annals of Statistics</em> <b>25</b> 435&ndash;477.
</p>
<p>Serfling, R. (2006). Depth functions in nonparametric multivariate inference. In: Liu, R., Serfling, R., Souvaine, D. (eds.), <em>Data Depth: Robust Multivariate Analysis, Computational Geometry and Applications</em>, American Mathematical Society, 1&ndash;16.
</p>
<p>Vardi, Y. and Zhang, C.H. (2000). The multivariate L1-median and associated data depth. <em>Proceedings of the National Academy of Sciences, U.S.A.</em> <b>97</b> 1423&ndash;1426.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ddalpha.train">ddalpha.train</a></code> and <code><a href="#topic+ddalpha.classify">ddalpha.classify</a></code> for application, <code><a href="#topic+depth.spatial">depth.spatial</a></code> for calculation of spatial depth.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Generate a bivariate normal location-shift classification task
# containing 20 training objects
class1 &lt;- mvrnorm(10, c(0,0), 
                  matrix(c(1,1,1,4), nrow = 2, ncol = 2, byrow = TRUE))
class2 &lt;- mvrnorm(10, c(2,2), 
                  matrix(c(1,1,1,4), nrow = 2, ncol = 2, byrow = TRUE))
data &lt;- rbind(class1, class2)
# Get depth space using spatial depth
depth.space.spatial(data, c(10, 10))

data &lt;- getdata("hemophilia")
cardinalities = c(sum(data$gr == "normal"), sum(data$gr == "carrier"))
depth.space.spatial(data[,1:2], cardinalities)
</code></pre>

<hr>
<h2 id='depth.space.zonoid'>
Calculate Depth Space using Zonoid Depth
</h2><span id='topic+depth.space.zonoid'></span>

<h3>Description</h3>

<p>Calculates the representation of the training classes in depth space using zonoid depth.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>depth.space.zonoid(data, cardinalities, seed = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="depth.space.zonoid_+3A_data">data</code></td>
<td>

<p>Matrix containing training sample where each row is a <code class="reqn">d</code>-dimensional object, and objects of each class are kept together so that the matrix can be thought of as containing blocks of objects representing classes.
</p>
</td></tr>
<tr><td><code id="depth.space.zonoid_+3A_cardinalities">cardinalities</code></td>
<td>

<p>Numerical vector of cardinalities of each class in <code>data</code>, each entry corresponds to one class.
</p>
</td></tr>
<tr><td><code id="depth.space.zonoid_+3A_seed">seed</code></td>
<td>

<p>the random seed. The default value <code>seed=0</code> makes no changes.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The depth representation is calculated in the same way as in <code><a href="#topic+depth.zonoid">depth.zonoid</a></code>, see 'References' for more information and details.
</p>


<h3>Value</h3>

<p>Matrix of objects, each object (row) is represented via its depths (columns) w.r.t. each of the classes of the training sample; order of the classes in columns corresponds to the one in the argument <code>cardinalities</code>.
</p>


<h3>References</h3>

<p>Dyckerhoff, R., Koshevoy, G., and Mosler, K. (1996). Zonoid data depth: theory and computation. In: Prat A. (ed), <em>COMPSTAT 1996. Proceedings in computational statistics</em>, Physica-Verlag (Heidelberg), 235&ndash;240.
</p>
<p>Koshevoy, G. and Mosler, K. (1997). Zonoid trimming for multivariate distributions <em>Annals of Statistics</em> <b>25</b> 1998&ndash;2017.
</p>
<p>Mosler, K. (2002). <em>Multivariate dispersion, central regions and depth: the lift zonoid approach</em> Springer (New York).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ddalpha.train">ddalpha.train</a></code> and <code><a href="#topic+ddalpha.classify">ddalpha.classify</a></code> for application, <code><a href="#topic+depth.zonoid">depth.zonoid</a></code> for calculation of zonoid depth.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Generate a bivariate normal location-shift classification task
# containing 20 training objects
class1 &lt;- mvrnorm(10, c(0,0), 
                  matrix(c(1,1,1,4), nrow = 2, ncol = 2, byrow = TRUE))
class2 &lt;- mvrnorm(10, c(2,2), 
                  matrix(c(1,1,1,4), nrow = 2, ncol = 2, byrow = TRUE))
data &lt;- rbind(class1, class2)
# Get depth space using zonoid depth
depth.space.zonoid(data, c(10, 10))

data &lt;- getdata("hemophilia")
cardinalities = c(sum(data$gr == "normal"), sum(data$gr == "carrier"))
depth.space.zonoid(data[,1:2], cardinalities)

</code></pre>

<hr>
<h2 id='depth.spatial'>
Calculate Spatial Depth
</h2><span id='topic+depth.spatial'></span>

<h3>Description</h3>

<p>Calculates the spatial depth of points w.r.t. a multivariate data set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>depth.spatial(x, data, mah.estimate = "moment", mah.parMcd = 0.75)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="depth.spatial_+3A_x">x</code></td>
<td>

<p>Matrix of objects (numerical vector as one object) whose depth is to be calculated; each row contains a <code class="reqn">d</code>-variate point. Should have the same dimension as <code>data</code>.
</p>
</td></tr>
<tr><td><code id="depth.spatial_+3A_data">data</code></td>
<td>

<p>Matrix of data where each row contains a <code class="reqn">d</code>-variate point, w.r.t. which the depth is to be calculated.
</p>
</td></tr>
<tr><td><code id="depth.spatial_+3A_mah.estimate">mah.estimate</code></td>
<td>
<p> is a character string specifying which estimates to use when calculating sample covariance matrix; can be <code>"none"</code>, <code>"moment"</code> or <code>"MCD"</code>, determining whether traditional moment or Minimum Covariance Determinant (MCD) (see <code><a href="robustbase.html#topic+covMcd">covMcd</a></code>) estimates for mean and covariance are used. By default <code>"moment"</code> is used. With <code>"none"</code> the non-affine invariant version of Spatial depth is calculated
</p>
</td></tr>
<tr><td><code id="depth.spatial_+3A_mah.parmcd">mah.parMcd</code></td>
<td>

<p>is the value of the argument <code>alpha</code> for the function <code><a href="robustbase.html#topic+covMcd">covMcd</a></code>; is used when <code>mah.estimate =</code> <code>"MCD"</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Calculates spatial depth. Spatial depth (also L1-depth) is a distance-based depth exploiting the idea of spatial quantiles of Chaudhuri (1996) and Koltchinskii (1997), formulated by Vardi &amp; Zhang (2000) and Serfling (2002).
</p>


<h3>Value</h3>

<p>Numerical vector of depths, one for each row in <code>x</code>; or one depth value if <code>x</code> is a numerical vector.
</p>


<h3>References</h3>

<p>Chaudhuri, P. (1996). On a geometric notion of quantiles for multivariate data. <em>Journal of the Americal Statistical Association</em> <b>91</b> 862&ndash;872.
</p>
<p>Koltchinskii, V.I. (1997). M-estimation, convexity and quantiles. <em>The Annals of Statistics</em> <b>25</b> 435&ndash;477.
</p>
<p>Serfling, R. (2006). Depth functions in nonparametric multivariate inference. In: Liu, R., Serfling, R., Souvaine, D. (eds.), <em>Data Depth: Robust Multivariate Analysis, Computational Geometry and Applications</em>, American Mathematical Society, 1&ndash;16.
</p>
<p>Vardi, Y. and Zhang, C.H. (2000). The multivariate L1-median and associated data depth. <em>Proceedings of the National Academy of Sciences, U.S.A.</em> <b>97</b> 1423&ndash;1426.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+depth.halfspace">depth.halfspace</a></code> for calculation of the Tukey depth.
</p>
<p><code><a href="#topic+depth.Mahalanobis">depth.Mahalanobis</a></code> for calculation of Mahalanobis depth.
</p>
<p><code><a href="#topic+depth.projection">depth.projection</a></code> for calculation of projection depth.
</p>
<p><code><a href="#topic+depth.simplicial">depth.simplicial</a></code> for calculation of simplicial depth.
</p>
<p><code><a href="#topic+depth.simplicialVolume">depth.simplicialVolume</a></code> for calculation of simplicial volume depth.
</p>
<p><code><a href="#topic+depth.zonoid">depth.zonoid</a></code> for calculation of zonoid depth.
</p>
<p><code><a href="#topic+depth.potential">depth.potential</a></code> for calculation of data potential.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># 5-dimensional normal distribution
data &lt;- mvrnorm(1000, rep(0, 5), 
                matrix(c(1, 0, 0, 0, 0, 
                         0, 2, 0, 0, 0, 
                         0, 0, 3, 0, 0, 
                         0, 0, 0, 2, 0, 
                         0, 0, 0, 0, 1),
                nrow = 5))
x &lt;- mvrnorm(10, rep(1, 5), 
             matrix(c(1, 0, 0, 0, 0, 
                      0, 1, 0, 0, 0, 
                      0, 0, 1, 0, 0, 
                      0, 0, 0, 1, 0, 
                      0, 0, 0, 0, 1),
             nrow = 5))
                
depths &lt;- depth.spatial(x, data)
cat("Depths: ", depths, "\n")
</code></pre>

<hr>
<h2 id='depth.zonoid'>
Calculate Zonoid Depth
</h2><span id='topic+depth.zonoid'></span>

<h3>Description</h3>

<p>Calculates the zonoid depth of points w.r.t. a multivariate data set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>depth.zonoid(x, data, seed = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="depth.zonoid_+3A_x">x</code></td>
<td>

<p>Matrix of objects (numerical vector as one object) whose depth is to be calculated; each row contains a <code class="reqn">d</code>-variate point. Should have the same dimension as <code>data</code>.
</p>
</td></tr>
<tr><td><code id="depth.zonoid_+3A_data">data</code></td>
<td>

<p>Matrix of data where each row contains a <code class="reqn">d</code>-variate point, w.r.t. which the depth is to be calculated.
</p>
</td></tr>
<tr><td><code id="depth.zonoid_+3A_seed">seed</code></td>
<td>

<p>the random seed. The default value <code>seed=0</code> makes no changes.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Calculates zonoid depth (Koshevoy and Mosler, 1997; Mosler, 2002) exactly based on the algorithm of Dyckerhoff, Koshevoy and Mosler (1996), implemented in C++ (and provided) by Rainer Dyckerhoff.
</p>


<h3>Value</h3>

<p>Numerical vector of depths, one for each row in <code>x</code>; or one depth value if <code>x</code> is a numerical vector.
</p>


<h3>References</h3>

<p>Dyckerhoff, R., Koshevoy, G., and Mosler, K. (1996). Zonoid data depth: theory and computation. In: Prat A. (ed), <em>COMPSTAT 1996. Proceedings in computational statistics</em>, Physica-Verlag (Heidelberg), 235&ndash;240.
</p>
<p>Koshevoy, G. and Mosler, K. (1997). Zonoid trimming for multivariate distributions <em>Annals of Statistics</em> <b>25</b> 1998&ndash;2017.
</p>
<p>Mosler, K. (2002). <em>Multivariate dispersion, central regions and depth: the lift zonoid approach</em> Springer (New York).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+depth.halfspace">depth.halfspace</a></code> for calculation of the Tukey depth.
</p>
<p><code><a href="#topic+depth.Mahalanobis">depth.Mahalanobis</a></code> for calculation of Mahalanobis depth.
</p>
<p><code><a href="#topic+depth.projection">depth.projection</a></code> for calculation of projection depth.
</p>
<p><code><a href="#topic+depth.simplicial">depth.simplicial</a></code> for calculation of simplicial depth.
</p>
<p><code><a href="#topic+depth.simplicialVolume">depth.simplicialVolume</a></code> for calculation of simplicial volume depth.
</p>
<p><code><a href="#topic+depth.spatial">depth.spatial</a></code> for calculation of spatial depth.
</p>
<p><code><a href="#topic+depth.potential">depth.potential</a></code> for calculation of data potential.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># 5-dimensional normal distribution
data &lt;- mvrnorm(1000, rep(0, 5), 
                matrix(c(1, 0, 0, 0, 0, 
                         0, 2, 0, 0, 0, 
                         0, 0, 3, 0, 0, 
                         0, 0, 0, 2, 0, 
                         0, 0, 0, 0, 1),
                nrow = 5))
x &lt;- mvrnorm(10, rep(1, 5), 
             matrix(c(1, 0, 0, 0, 0, 
                      0, 1, 0, 0, 0, 
                      0, 0, 1, 0, 0, 
                      0, 0, 0, 1, 0, 
                      0, 0, 0, 0, 1),
             nrow = 5))
                
depths &lt;- depth.zonoid(x, data)
cat("Depths: ", depths, "\n")
</code></pre>

<hr>
<h2 id='depthf.'>
Calculate Functional Depth
</h2><span id='topic+depthf.'></span>

<h3>Description</h3>

<p>Calculates the depth of functions w.r.t. a functional data set.
</p>
<p>The detailed descriptions are found in the corresponding topics.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>depthf.(datafA, datafB, notion, ...)

## Adjusted band depth
# depthf.ABD(datafA, datafB, range = NULL, d = 101, norm = c("C", "L2"), 
# J = 2, K = 1)

## Band depth
# depthf.BD(datafA, datafB, range = NULL, d = 101)

## Univariate integrated and infimal depth
# depthf.fd1(datafA, datafB, range = NULL, d = 101, order = 1, approx = 0)

## Bivariate integrated and infimal depth
# depthf.fd2(datafA, datafB, range = NULL, d = 101)

## h-mode depth
# depthf.hM(datafA, datafB, range = NULL, d = 101, norm = c("C", "L2"),
#  q = 0.2)

## Bivariate h-mode depth
# depthf.hM2(datafA, datafB, range = NULL, d = 101, q = 0.2)

## Half-region depth
# depthf.HR(datafA, datafB, range = NULL, d = 101)

## Univariate random projection depths
# depthf.RP1(datafA, datafB, range = NULL, d = 101, nproj = 50, nproj2 = 5)

# Bivariate random projection depths
# depthf.RP2(datafA, datafB, range = NULL, d = 101, nproj = 51)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="depthf._+3A_datafa">datafA</code></td>
<td>

<p>Functions whose depth is computed, represented by a <code>dataf</code> object of their arguments
and functional values.
</p>
</td></tr>
<tr><td><code id="depthf._+3A_datafb">datafB</code></td>
<td>

<p>Random sample functions with respect to which the depth of <code>datafA</code> is computed. 
<code>datafB</code> is represented by a <code>dataf</code> object of their arguments
and functional values.
</p>
</td></tr>
<tr><td><code id="depthf._+3A_notion">notion</code></td>
<td>

<p>The name of the depth notion (shall also work with a user-defined depth function named <code>"depthf.&lt;name&gt;"</code>).
</p>
</td></tr>
<tr><td><code id="depthf._+3A_...">...</code></td>
<td>

<p>Additional parameters passed to the depth functions.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Numerical vector of depths, one for each function in <code>datafA</code>; or one depth value if <code>datafA</code> is a single function.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+depthf.ABD">depthf.ABD</a></code>
</p>
<p><code><a href="#topic+depthf.BD">depthf.BD</a></code>
</p>
<p><code><a href="#topic+depthf.fd1">depthf.fd1</a></code>
</p>
<p><code><a href="#topic+depthf.fd2">depthf.fd2</a></code>
</p>
<p><code><a href="#topic+depthf.hM">depthf.hM</a></code>
</p>
<p><code><a href="#topic+depthf.hM2">depthf.hM2</a></code>
</p>
<p><code><a href="#topic+depthf.HR">depthf.HR</a></code>
</p>
<p><code><a href="#topic+depthf.RP1">depthf.RP1</a></code>
</p>
<p><code><a href="#topic+depthf.RP2">depthf.RP2</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># real data example
datafA = dataf.population()$dataf[1:20]
datafB = dataf.population()$dataf[21:50]
                
depthf.(datafA, datafB, notion = "HR")

dataf2A = derivatives.est(datafA,deriv=c(0,1))
dataf2B = derivatives.est(datafB,deriv=c(0,1))

depthf.(dataf2A, dataf2B, notion = "fd2")
</code></pre>

<hr>
<h2 id='depthf.ABD'>Adjusted Band Depth for Functional Data</h2><span id='topic+depthf.ABD'></span>

<h3>Description</h3>

<p>The adjusted band depth 
of functional real-valued data based on either the
<code class="reqn">C</code> (uniform) norm, or on the <code class="reqn">L^2</code> norm of functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>depthf.ABD(datafA, datafB, range = NULL, d = 101, norm = c("C", "L2"),
  J = 2, K = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="depthf.ABD_+3A_datafa">datafA</code></td>
<td>
<p>Functions whose depth is computed, represented by a <code>dataf</code> object of their arguments
and functional values. <code>m</code> stands for the number of functions.</p>
</td></tr>
<tr><td><code id="depthf.ABD_+3A_datafb">datafB</code></td>
<td>
<p>Random sample functions with respect to which the depth of <code>datafA</code> is computed. 
<code>datafB</code> is represented by a <code>dataf</code> object of their arguments
and functional values. <code>n</code> is the sample size. The grid of observation points for the 
functions <code>datafA</code> and <code>datafB</code> may not be the same.</p>
</td></tr>
<tr><td><code id="depthf.ABD_+3A_range">range</code></td>
<td>
<p>The common range of the domain where the functions <code>datafA</code> and <code>datafB</code> are observed.
Vector of length 2 with the left and the right end of the interval. Must contain all arguments given in 
<code>datafA</code> and <code>datafB</code>.</p>
</td></tr>
<tr><td><code id="depthf.ABD_+3A_d">d</code></td>
<td>
<p>Grid size to which all the functional data are transformed. For depth computation, 
all functional observations are first transformed into vectors of their functional values of length <code>d</code>
corresponding to equi-spaced points in the domain given by the interval <code>range</code>. Functional values in these
points are reconstructed using linear interpolation, and extrapolation, see Nagy et al. (2016).</p>
</td></tr>
<tr><td><code id="depthf.ABD_+3A_norm">norm</code></td>
<td>
<p>The norm used for the computation of the depth. Two possible 
choices are implemented: <code>C</code> for the uniform norm of continuous functions, 
and <code>L2</code> for the <code class="reqn">L^2</code> norm of integrable functions.</p>
</td></tr>
<tr><td><code id="depthf.ABD_+3A_j">J</code></td>
<td>
<p>The order of the adjusted band depth, that is the maximal number of functions
taken in a band. Acceptable values are <code>2</code>, <code>3</code>,... By default this value is set to <code>2</code>. 
Note that this is NOT the order as
defined in the order-extended version of adjusted band depths in Nagy et al. (2016), used
for the detection of shape outlying curves.</p>
</td></tr>
<tr><td><code id="depthf.ABD_+3A_k">K</code></td>
<td>
<p>Number of sub-samples of the functions from <code>B</code> taken to speed up the
computation. By default, sub-sampling is not performed. Values of <code>K</code> larger than <code>1</code>
result in an approximation of the adjusted band depth.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function returns the vector of the sample adjusted band depth values. The kernel 
used in the evaluation is the function <code class="reqn">K(u) = exp(-u)</code>.
</p>


<h3>Value</h3>

<p>A vectors of length <code>m</code> of the adjusted band depths.
</p>


<h3>Author(s)</h3>

<p>Stanislav Nagy, <a href="mailto:nagy@karlin.mff.cuni.cz">nagy@karlin.mff.cuni.cz</a>
</p>


<h3>References</h3>

<p>Gijbels, I., Nagy, S. (2015).
Consistency of non-integrated depths for functional data.
<em>Journal of Multivariate Analysis</em> <b>140</b>, 259&ndash;282.
</p>
<p>Nagy, S., Gijbels, I. and Hlubinka, D. (2016). 
Weak convergence of discretely observed functional data with applications. 
<em>Journal of Multivariate Analysis</em>, <b>146</b>, 46&ndash;62.
</p>
<p>Nagy, S., Gijbels, I. and Hlubinka, D.  (2017).
Depth-based recognition of shape outlying functions. 
<em>Journal of Computational and Graphical Statistics</em>, <b>26</b> (4), 883&ndash;893.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+depthf.BD">depthf.BD</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>datafA = dataf.population()$dataf[1:20]
datafB = dataf.population()$dataf[21:50]
depthf.ABD(datafA,datafB)
depthf.ABD(datafA,datafB,norm="L2")

</code></pre>

<hr>
<h2 id='depthf.BD'>Band Depth for Functional Data</h2><span id='topic+depthf.BD'></span>

<h3>Description</h3>

<p>The (unadjusted) band depth 
for functional real-valued data of order <code>J=2</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>depthf.BD(datafA, datafB, range = NULL, d = 101)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="depthf.BD_+3A_datafa">datafA</code></td>
<td>
<p>Functions whose depth is computed, represented by a <code>dataf</code> object of their arguments
and functional values. <code>m</code> stands for the number of functions.</p>
</td></tr>
<tr><td><code id="depthf.BD_+3A_datafb">datafB</code></td>
<td>
<p>Random sample functions with respect to which the depth of <code>datafA</code> is computed. 
<code>datafB</code> is represented by a <code>dataf</code> object of their arguments
and functional values. <code>n</code> is the sample size. The grid of observation points for the 
functions <code>datafA</code> and <code>datafB</code> may not be the same.</p>
</td></tr>
<tr><td><code id="depthf.BD_+3A_range">range</code></td>
<td>
<p>The common range of the domain where the functions <code>datafA</code> and <code>datafB</code> are observed.
Vector of length 2 with the left and the right end of the interval. Must contain all arguments given in 
<code>datafA</code> and <code>datafB</code>.</p>
</td></tr>
<tr><td><code id="depthf.BD_+3A_d">d</code></td>
<td>
<p>Grid size to which all the functional data are transformed. For depth computation, 
all functional observations are first transformed into vectors of their functional values of length <code>d</code>
corresponding to equi-spaced points in the domain given by the interval <code>range</code>. Functional values in these
points are reconstructed using linear interpolation, and extrapolation.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function returns the vector of the sample (unadjusted) band depth values.
</p>


<h3>Value</h3>

<p>A vector of length <code>m</code> of the band depth values.
</p>


<h3>Author(s)</h3>

<p>Stanislav Nagy, <a href="mailto:nagy@karlin.mff.cuni.cz">nagy@karlin.mff.cuni.cz</a>
</p>


<h3>References</h3>

<p>Lopez-Pintado, S. and Romo, J. (2009), On the concept of depth for functional data,
<em>J. Amer. Statist. Assoc.</em> <b>104</b> (486), 718 - 734.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+depthf.ABD">depthf.ABD</a></code>, <code><a href="#topic+depthf.fd1">depthf.fd1</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>datafA = dataf.population()$dataf[1:20]
datafB = dataf.population()$dataf[21:50]
depthf.BD(datafA,datafB)

</code></pre>

<hr>
<h2 id='depthf.fd1'>Univariate Integrated and Infimal Depth for Functional Data</h2><span id='topic+depthf.fd1'></span>

<h3>Description</h3>

<p>Usual, and order extended integrated and infimal depths for real-valued functional data based on the
halfspace and simplicial depth.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>depthf.fd1(datafA, datafB, range = NULL, d = 101, order = 1, approx = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="depthf.fd1_+3A_datafa">datafA</code></td>
<td>
<p>Functions whose depth is computed, represented by a <code>dataf</code> object of their arguments
and functional values. <code>m</code> stands for the number of functions.</p>
</td></tr>
<tr><td><code id="depthf.fd1_+3A_datafb">datafB</code></td>
<td>
<p>Random sample functions with respect to which the depth of <code>datafA</code> is computed. 
<code>datafB</code> is represented by a <code>dataf</code> object of their arguments
and functional values. <code>n</code> is the sample size. The grid of observation points for the 
functions <code>datafA</code> and <code>datafB</code> may not be the same.</p>
</td></tr>
<tr><td><code id="depthf.fd1_+3A_range">range</code></td>
<td>
<p>The common range of the domain where the functions <code>datafA</code> and <code>datafB</code> are observed.
Vector of length 2 with the left and the right end of the interval. Must contain all arguments given in 
<code>datafA</code> and <code>datafB</code>.</p>
</td></tr>
<tr><td><code id="depthf.fd1_+3A_d">d</code></td>
<td>
<p>Grid size to which all the functional data are transformed. For depth computation, 
all functional observations are first transformed into vectors of their functional values of length <code>d</code>
corresponding to equi-spaced points in the domain given by the interval <code>range</code>. Functional values in these
points are reconstructed using linear interpolation, and extrapolation, see Nagy et al. (2016).</p>
</td></tr>
<tr><td><code id="depthf.fd1_+3A_order">order</code></td>
<td>
<p>The order of the order extended integrated and infimal depths.
By default, this is set to <code>1</code>, meaning that the usual univariate depths of 
the functional values are computed. For <code>order=2</code> or <code>3</code>, the second
and the third order extended integrated and infimal depths are computed, 
respectively.</p>
</td></tr>
<tr><td><code id="depthf.fd1_+3A_approx">approx</code></td>
<td>
<p>Number of approximations used in the computation of the order extended depth
for <code>order</code> greater than <code>1</code>. For <code>order=2</code>, the default
value is set to <code>0</code>, meaning that the depth is computed at all possible <code>d^order</code>
combinations of the points in the domain. For <code>order=3</code>, 
the default value is set to <code>101</code>. When <code>approx</code> is a positive integer, <code>approx</code>
points are randomly sampled in <code>[0,1]^order</code> and at these points the <code>order</code>-variate depths of the
corresponding functional values are computed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function returns vectors of sample integrated and infimal depth values.
</p>


<h3>Value</h3>

<p>Four vectors of length <code>m</code> of depth values are returned:
</p>

<ul>
<li> <p><code>Simpl_FD</code> the integrated depth based on the simplicial depth,
</p>
</li>
<li> <p><code>Half_FD</code> the integrated depth based on the halfspace depth,
</p>
</li>
<li> <p><code>Simpl_ID</code> the infimal depth based on the simplicial depth,
</p>
</li>
<li> <p><code>Half_ID</code> the infimal depth based on the halfspace depth.
</p>
</li></ul>

<p>In addition, two vectors of length <code>m</code> of the relative area of smallest depth values is returned:
</p>

<ul>
<li> <p><code>Simpl_IA</code> the proportions of points at which the depth <code>Simpl_ID</code> was attained,
</p>
</li>
<li> <p><code>Half_IA</code> the proportions of points at which the depth <code>Half_ID</code> was attained.
</p>
</li></ul>

<p>The values <code>Simpl_IA</code> and <code>Half_IA</code> are always in the interval [0,1]. 
They introduce ranking also among functions having the same
infimal depth value - if two functions have the same infimal depth, the one with larger infimal area
<code>IA</code> is said to be less central. 	
For <code>order=2</code> and <code>m=1</code>, two additional matrices of pointwise depths are also returned:
</p>

<ul>
<li> <p><code>PSD</code> the matrix of size <code>d*d</code> containing the computed 
pointwise bivariate simplicial depths used for the computation of <code>Simpl_FD</code> and <code>Simpl_ID</code>,
</p>
</li>
<li> <p><code>PHD</code> the matrix of size <code>d*d</code> containing the computed 
pointwise bivariate halfspace depths used for the computation of <code>Half_FD</code> and <code>Half_ID</code>.
</p>
</li></ul>

<p>For <code>order=3</code>, only <code>Half_FD</code> and <code>Half_ID</code> are provided.
</p>


<h3>Author(s)</h3>

<p>Stanislav Nagy, <a href="mailto:nagy@karlin.mff.cuni.cz">nagy@karlin.mff.cuni.cz</a>
</p>


<h3>References</h3>

<p>Nagy, S., Gijbels, I. and Hlubinka, D. (2016). 
Weak convergence of discretely observed functional data with applications. 
<em>Journal of Multivariate Analysis</em>, <b>146</b>, 46&ndash;62.
</p>
<p>Nagy, S., Gijbels, I. and Hlubinka, D.  (2017).
Depth-based recognition of shape outlying functions. 
<em>Journal of Computational and Graphical Statistics</em>, <b>26</b> (4), 883&ndash;893.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+depthf.fd2">depthf.fd2</a></code>, <code><a href="#topic+infimalRank">infimalRank</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>datafA = dataf.population()$dataf[1:20]
datafB = dataf.population()$dataf[21:50]
depthf.fd1(datafA,datafB)
depthf.fd1(datafA,datafB,order=2)
depthf.fd1(datafA,datafB,order=3,approx=51)

</code></pre>

<hr>
<h2 id='depthf.fd2'>Bivariate Integrated and Infimal Depth for Functional Data</h2><span id='topic+depthf.fd2'></span>

<h3>Description</h3>

<p>Integrated and infimal depths 
of functional bivariate data (that is, data of the form <code class="reqn">X:[a,b] \to R^2</code>,
or <code class="reqn">X:[a,b] \to R</code> and the derivative of <code class="reqn">X</code>) based on the
bivariate halfspace and simplicial depths.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>depthf.fd2(datafA, datafB, range = NULL, d = 101)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="depthf.fd2_+3A_datafa">datafA</code></td>
<td>
<p>Bivariate functions whose depth is computed, represented by a multivariate <code>dataf</code> object of 
their arguments (vector), and a matrix with two columns of the corresponding bivariate functional values. 
<code>m</code> stands for the number of functions.</p>
</td></tr>
<tr><td><code id="depthf.fd2_+3A_datafb">datafB</code></td>
<td>
<p>Bivariate random sample functions with respect to which the depth of <code>datafA</code> is computed. 
<code>datafB</code> is represented by a multivariate <code>dataf</code> object of their arguments
(vector), and a matrix with two columns of the corresponding bivariate functional values.
<code>n</code> is the sample size. The grid of observation points for the 
functions <code>datafA</code> and <code>datafB</code> may not be the same.</p>
</td></tr>
<tr><td><code id="depthf.fd2_+3A_range">range</code></td>
<td>
<p>The common range of the domain where the functions <code>datafA</code> and <code>datafB</code> are observed.
Vector of length 2 with the left and the right end of the interval. Must contain all arguments given in 
<code>datafA</code> and <code>datafB</code>.</p>
</td></tr>
<tr><td><code id="depthf.fd2_+3A_d">d</code></td>
<td>
<p>Grid size to which all the functional data are transformed. For depth computation, 
all functional observations are first transformed into vectors of their functional values of length <code>d</code>
corresponding to equi-spaced points in the domain given by the interval <code>range</code>. Functional values in these
points are reconstructed using linear interpolation, and extrapolation.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function returns the vectors of sample integrated and infimal depth values.
</p>


<h3>Value</h3>

<p>Four vectors of length <code>m</code> are returned:
</p>

<ul>
<li> <p><code>Simpl_FD</code> the integrated depth based on the bivariate simplicial depth,
</p>
</li>
<li> <p><code>Half_FD</code> the integrated depth based on the bivariate halfspace depth,
</p>
</li>
<li> <p><code>Simpl_ID</code> the infimal depth based on the bivariate simplicial depth,
</p>
</li>
<li> <p><code>Half_ID</code> the infimal depth based on the bivariate halfspace depth.
</p>
</li></ul>

<p>In addition, two vectors of length <code>m</code> of the relative area of smallest depth values is returned:
</p>

<ul>
<li> <p><code>Simpl_IA</code> the proportions of points at which the depth <code>Simpl_ID</code> was attained,
</p>
</li>
<li> <p><code>Half_IA</code> the proportions of points at which the depth <code>Half_ID</code> was attained.
</p>
</li></ul>

<p>The values <code>Simpl_IA</code> and <code>Half_IA</code> are always in the interval [0,1]. 
They introduce ranking also among functions having the same
infimal depth value - if two functions have the same infimal depth, the one with larger infimal area
<code>IA</code> is said to be less central.
</p>


<h3>Author(s)</h3>

<p>Stanislav Nagy, <a href="mailto:nagy@karlin.mff.cuni.cz">nagy@karlin.mff.cuni.cz</a>
</p>


<h3>References</h3>

<p>Hlubinka, D., Gijbels, I., Omelka, M. and Nagy, S. (2015). 
Integrated data depth for smooth functions and its application in supervised classification. 
<em>Computational Statistics</em>, <b>30</b> (4), 1011&ndash;1031.
</p>
<p>Nagy, S., Gijbels, I. and Hlubinka, D.  (2017).
Depth-based recognition of shape outlying functions. 
<em>Journal of Computational and Graphical Statistics</em>, <b>26</b> (4), 883&ndash;893.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+depthf.fd1">depthf.fd1</a></code>, <code><a href="#topic+infimalRank">infimalRank</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>datafA = dataf.population()$dataf[1:20]
datafB = dataf.population()$dataf[21:50]

dataf2A = derivatives.est(datafA,deriv=c(0,1))
dataf2B = derivatives.est(datafB,deriv=c(0,1))
depthf.fd2(dataf2A,dataf2B)

</code></pre>

<hr>
<h2 id='depthf.hM'>h-Mode Depth for Functional Data</h2><span id='topic+depthf.hM'></span>

<h3>Description</h3>

<p>The h-mode depth of functional real-valued data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>depthf.hM(datafA, datafB, range = NULL, d = 101, norm = c("C", "L2"),
  q = 0.2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="depthf.hM_+3A_datafa">datafA</code></td>
<td>
<p>Functions whose depth is computed, represented by a <code>dataf</code> object of their arguments
and functional values. <code>m</code> stands for the number of functions.</p>
</td></tr>
<tr><td><code id="depthf.hM_+3A_datafb">datafB</code></td>
<td>
<p>Random sample functions with respect to which the depth of <code>datafA</code> is computed. 
<code>datafB</code> is represented by a <code>dataf</code> object of their arguments
and functional values. <code>n</code> is the sample size. The grid of observation points for the 
functions <code>datafA</code> and <code>datafB</code> may not be the same.</p>
</td></tr>
<tr><td><code id="depthf.hM_+3A_range">range</code></td>
<td>
<p>The common range of the domain where the functions <code>datafA</code> and <code>datafB</code> are observed.
Vector of length 2 with the left and the right end of the interval. Must contain all arguments given in 
<code>datafA</code> and <code>datafB</code>.</p>
</td></tr>
<tr><td><code id="depthf.hM_+3A_d">d</code></td>
<td>
<p>Grid size to which all the functional data are transformed. For depth computation, 
all functional observations are first transformed into vectors of their functional values of length <code>d</code>
corresponding to equi-spaced points in the domain given by the interval <code>range</code>. Functional values in these
points are reconstructed using linear interpolation, and extrapolation.</p>
</td></tr>
<tr><td><code id="depthf.hM_+3A_norm">norm</code></td>
<td>
<p>The norm used for the computation of the depth. Two possible 
choices are implemented: <code>C</code> for the uniform norm of continuous functions, 
and <code>L2</code> for the <code class="reqn">L^2</code> norm of integrable functions.</p>
</td></tr>
<tr><td><code id="depthf.hM_+3A_q">q</code></td>
<td>
<p>The quantile used to determine the value of the bandwidth <code class="reqn">h</code>
in the computation of the h-mode depth. <code class="reqn">h</code> is taken as the <code>q</code>-quantile of
all non-zero distances between the functions <code>B</code>. By default, this value is set
to <code>q=0.2</code>, in accordance with the choice of Cuevas et al. (2007).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function returns the vectors of the sample h-mode depth values. The kernel 
used in the evaluation is the standard Gaussian kernel, the bandwidth value is chosen
as a quantile of the non-zero distances between the random sample curves.
</p>


<h3>Value</h3>

<p>A vector of length <code>m</code> of the h-mode depth values.
</p>


<h3>Author(s)</h3>

<p>Stanislav Nagy, <a href="mailto:nagy@karlin.mff.cuni.cz">nagy@karlin.mff.cuni.cz</a>
</p>


<h3>References</h3>

<p>Cuevas, A., Febrero, M. and Fraiman, R.  (2007).
Robust estimation and classification for functional data via projection-based depth notions. 
<em>Computational Statistics</em> <b>22</b> (3), 481&ndash;496.
</p>
<p>Nagy, S., Gijbels, I. and Hlubinka, D. (2016). 
Weak convergence of discretely observed functional data with applications. 
<em>Journal of Multivariate Analysis</em>, <b>146</b>, 46&ndash;62.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>datafA = dataf.population()$dataf[1:20]
datafB = dataf.population()$dataf[21:50]
depthf.hM(datafA,datafB)
depthf.hM(datafA,datafB,norm="L2")
</code></pre>

<hr>
<h2 id='depthf.hM2'>Bivariate h-Mode Depth for Functional Data Based on the <code class="reqn">L^2</code> Metric</h2><span id='topic+depthf.hM2'></span>

<h3>Description</h3>

<p>The h-mode depth 
of functional bivariate data (that is, data of the form <code class="reqn">X:[a,b] \to R^2</code>,
or <code class="reqn">X:[a,b] \to R</code> and the derivative of <code class="reqn">X</code>) based on the
<code class="reqn">L^2</code> metric of functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>depthf.hM2(datafA, datafB, range = NULL, d = 101, q = 0.2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="depthf.hM2_+3A_datafa">datafA</code></td>
<td>
<p>Bivariate functions whose depth is computed, represented by a multivariate <code>dataf</code> object of 
their arguments (vector), and a matrix with two columns of the corresponding bivariate functional values. 
<code>m</code> stands for the number of functions.</p>
</td></tr>
<tr><td><code id="depthf.hM2_+3A_datafb">datafB</code></td>
<td>
<p>Bivariate random sample functions with respect to which the depth of <code>datafA</code> is computed. 
<code>datafB</code> is represented by a multivariate <code>dataf</code> object of their arguments
(vector), and a matrix with two columns of the corresponding bivariate functional values.
<code>n</code> is the sample size. The grid of observation points for the 
functions <code>datafA</code> and <code>datafB</code> may not be the same.</p>
</td></tr>
<tr><td><code id="depthf.hM2_+3A_range">range</code></td>
<td>
<p>The common range of the domain where the functions <code>datafA</code> and <code>datafB</code> are observed.
Vector of length 2 with the left and the right end of the interval. Must contain all arguments given in 
<code>datafA</code> and <code>datafB</code>.</p>
</td></tr>
<tr><td><code id="depthf.hM2_+3A_d">d</code></td>
<td>
<p>Grid size to which all the functional data are transformed. For depth computation, 
all functional observations are first transformed into vectors of their functional values of length <code>d</code>
corresponding to equi-spaced points in the domain given by the interval <code>range</code>. Functional values in these
points are reconstructed using linear interpolation, and extrapolation.</p>
</td></tr>
<tr><td><code id="depthf.hM2_+3A_q">q</code></td>
<td>
<p>The quantile used to determine the value of the bandwidth <code class="reqn">h</code>
in the computation of the h-mode depth. <code class="reqn">h</code> is taken as the <code>q</code>-quantile of
all non-zero distances between the functions <code>B</code>. By default, this value is set
to <code>q=0.2</code>, in accordance with the choice of Cuevas et al. (2007).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function returns the vectors of sample h-mode depth values. The kernel 
used in the evaluation is the standard Gaussian kernel, the bandwidth value is chosen
as a quantile of the non-zero distances between the random sample curves.
</p>


<h3>Value</h3>

<p>Three vectors of length <code>m</code> of h-mode depth values are returned:
</p>

<ul>
<li> <p><code>hM</code> the unscaled h-mode depth,
</p>
</li>
<li> <p><code>hM_norm</code> the h-mode depth <code>hM</code> linearly transformed so that its range is [0,1],
</p>
</li>
<li> <p><code>hM_norm2</code> the h-mode depth <code>FD</code> linearly transformed by a transformation such that 
the range of the h-mode depth of <code>B</code> with respect to <code>B</code> is [0,1]. This depth may give negative values.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Stanislav Nagy, <a href="mailto:nagy@karlin.mff.cuni.cz">nagy@karlin.mff.cuni.cz</a>
</p>


<h3>References</h3>

<p>Cuevas, A., Febrero, M. and Fraiman, R.  (2007). 
Robust estimation and classification for functional data via projection-based depth notions. 
<em>Computational Statistics</em> <b>22</b> (3), 481&ndash;496.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+depthf.hM">depthf.hM</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>datafA = dataf.population()$dataf[1:20]
datafB = dataf.population()$dataf[21:50]

datafA2 = derivatives.est(datafA,deriv=c(0,1))
datafB2 = derivatives.est(datafB,deriv=c(0,1))

depthf.hM2(datafA2,datafB2)

depthf.hM2(datafA2,datafB2)$hM
# depthf.hM2(cbind(A2[,,1],A2[,,2]),cbind(B2[,,1],B2[,,2]))$hM
# the two expressions above should give the same result

</code></pre>

<hr>
<h2 id='depthf.HR'>Half-Region Depth for Functional Data</h2><span id='topic+depthf.HR'></span>

<h3>Description</h3>

<p>The half-region depth 
for functional real-valued data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>depthf.HR(datafA, datafB, range = NULL, d = 101)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="depthf.HR_+3A_datafa">datafA</code></td>
<td>
<p>Functions whose depth is computed, represented by a <code>dataf</code> object of their arguments
and functional values. <code>m</code> stands for the number of functions.</p>
</td></tr>
<tr><td><code id="depthf.HR_+3A_datafb">datafB</code></td>
<td>
<p>Random sample functions with respect to which the depth of <code>datafA</code> is computed. 
<code>datafB</code> is represented by a <code>dataf</code> object of their arguments
and functional values. <code>n</code> is the sample size. The grid of observation points for the 
functions <code>datafA</code> and <code>datafB</code> may not be the same.</p>
</td></tr>
<tr><td><code id="depthf.HR_+3A_range">range</code></td>
<td>
<p>The common range of the domain where the functions <code>datafA</code> and <code>datafB</code> are observed.
Vector of length 2 with the left and the right end of the interval. Must contain all arguments given in 
<code>datafA</code> and <code>datafB</code>.</p>
</td></tr>
<tr><td><code id="depthf.HR_+3A_d">d</code></td>
<td>
<p>Grid size to which all the functional data are transformed. For depth computation, 
all functional observations are first transformed into vectors of their functional values of length <code>d</code>
corresponding to equi-spaced points in the domain given by the interval <code>range</code>. Functional values in these
points are reconstructed using linear interpolation, and extrapolation.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function returns the vector of the sample half-region depth values.
</p>


<h3>Value</h3>

<p>A vector of length <code>m</code> of the half-region depth values.
</p>


<h3>Author(s)</h3>

<p>Stanislav Nagy, <a href="mailto:nagy@karlin.mff.cuni.cz">nagy@karlin.mff.cuni.cz</a>
</p>


<h3>References</h3>

<p>Lopez-Pintado, S. and Romo, J. (2011).
A half-region depth for functional data.
<em>Computational Statistics &amp; Data Analysis</em> <b>55</b> (4), 1679&ndash;1695.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>datafA = dataf.population()$dataf[1:20]
datafB = dataf.population()$dataf[21:50]
depthf.HR(datafA,datafB)
</code></pre>

<hr>
<h2 id='depthf.RP1'>Univariate Random Projection Depths for Functional Data</h2><span id='topic+depthf.RP1'></span>

<h3>Description</h3>

<p>Random projection depth and random functional depth for functional data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>depthf.RP1(datafA, datafB, range = NULL, d = 101, nproj = 50, nproj2 = 5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="depthf.RP1_+3A_datafa">datafA</code></td>
<td>
<p>Functions whose depth is computed, represented by a <code>dataf</code> object of their arguments
and functional values. <code>m</code> stands for the number of functions.</p>
</td></tr>
<tr><td><code id="depthf.RP1_+3A_datafb">datafB</code></td>
<td>
<p>Random sample functions with respect to which the depth of <code>datafA</code> is computed. 
<code>datafB</code> is represented by a <code>dataf</code> object of their arguments
and functional values. <code>n</code> is the sample size. The grid of observation points for the 
functions <code>datafA</code> and <code>datafB</code> may not be the same.</p>
</td></tr>
<tr><td><code id="depthf.RP1_+3A_range">range</code></td>
<td>
<p>The common range of the domain where the functions <code>datafA</code> and <code>datafB</code> are observed.
Vector of length 2 with the left and the right end of the interval. Must contain all arguments given in 
<code>datafA</code> and <code>datafB</code>.</p>
</td></tr>
<tr><td><code id="depthf.RP1_+3A_d">d</code></td>
<td>
<p>Grid size to which all the functional data are transformed. For depth computation, 
all functional observations are first transformed into vectors of their functional values of length <code>d</code>
corresponding to equi-spaced points in the domain given by the interval <code>range</code>. Functional values in these
points are reconstructed using linear interpolation, and extrapolation.</p>
</td></tr>
<tr><td><code id="depthf.RP1_+3A_nproj">nproj</code></td>
<td>
<p>Number of projections taken in the computation of the random projection depth. By default taken
to be <code>51</code>.</p>
</td></tr>
<tr><td><code id="depthf.RP1_+3A_nproj2">nproj2</code></td>
<td>
<p>Number of projections taken in the computation of the random functional depth. By default taken
to be <code>5</code>. <code>nproj2</code> should be much smaller than <code>d</code>, the dimensionality of the discretized 
functional data.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function returns the vectors of sample random projection, and random functional depth values. 
The random projection depth described in Cuevas et al. (2007) is based on the average univariate depth
of one-dimensional projections of functional data. The projections are taken randomly as a sample of standard
normal <code>d</code>-dimensional random variables, where <code>d</code> stands for the dimensionality of the discretized
functional data. 
</p>
<p>The random functional depth (also called random Tukey depth, or random halfspace depth) is described in
Cuesta-Albertos and Nieto-Reyes (2008). The functional data are projected into the real line in random 
directions as for the random projection depths. Afterwards, an approximation of the halfspace (Tukey) depth
based on this limited number of univariate projections is assessed.
</p>


<h3>Value</h3>

<p>Three vectors of depth values of length <code>m</code> are returned:
</p>

<ul>
<li> <p><code>Simpl_FD</code> the random projection depth based on the univariate simplicial depth,
</p>
</li>
<li> <p><code>Half_FD</code> the random projection depth based on the univariate halfspace depth,
</p>
</li>
<li> <p><code>RHalf_FD</code> the random halfspace depth.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Stanislav Nagy, <a href="mailto:nagy@karlin.mff.cuni.cz">nagy@karlin.mff.cuni.cz</a>
</p>


<h3>References</h3>

<p>Cuevas, A., Febrero, M. and Fraiman, R.  (2007).
Robust estimation and classification for functional data via projection-based depth notions, 
<em>Computational Statistics</em> <b>22</b> (3), 481&ndash;496.
</p>
<p>Cuesta-Albertos, J.A. and Nieto-Reyes, A. (2008).
The random Tukey depth. 
<em>Computational Statistics &amp; Data Analysis</em> <b>52</b> (11), 4979&ndash;4988.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+depthf.RP2">depthf.RP2</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>datafA = dataf.population()$dataf[1:20]
datafB = dataf.population()$dataf[21:50]

depthf.RP1(datafA,datafB)

</code></pre>

<hr>
<h2 id='depthf.RP2'>Bivariate Random Projection Depths for Functional Data</h2><span id='topic+depthf.RP2'></span>

<h3>Description</h3>

<p>Double random projection depths of functional bivariate data (that is, data of the form <code class="reqn">X:[a,b] \to R^2</code>,
or <code class="reqn">X:[a,b] \to R</code> and the derivative of <code class="reqn">X</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>depthf.RP2(datafA, datafB, range = NULL, d = 101, nproj = 51)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="depthf.RP2_+3A_datafa">datafA</code></td>
<td>
<p>Bivariate functions whose depth is computed, represented by a multivariate <code>dataf</code> object of 
their arguments (vector), and a matrix with two columns of the corresponding bivariate functional values. 
<code>m</code> stands for the number of functions.</p>
</td></tr>
<tr><td><code id="depthf.RP2_+3A_datafb">datafB</code></td>
<td>
<p>Bivariate random sample functions with respect to which the depth of <code>datafA</code> is computed. 
<code>datafB</code> is represented by a multivariate <code>dataf</code> object of their arguments
(vector), and a matrix with two columns of the corresponding bivariate functional values.
<code>n</code> is the sample size. The grid of observation points for the 
functions <code>datafA</code> and <code>datafB</code> may not be the same.</p>
</td></tr>
<tr><td><code id="depthf.RP2_+3A_range">range</code></td>
<td>
<p>The common range of the domain where the functions <code>datafA</code> and <code>datafB</code> are observed.
Vector of length 2 with the left and the right end of the interval. Must contain all arguments given in 
<code>datafA</code> and <code>datafB</code>.</p>
</td></tr>
<tr><td><code id="depthf.RP2_+3A_d">d</code></td>
<td>
<p>Grid size to which all the functional data are transformed. For depth computation, 
all functional observations are first transformed into vectors of their functional values of length <code>d</code>
corresponding to equi-spaced points in the domain given by the interval <code>range</code>. Functional values in these
points are reconstructed using linear interpolation, and extrapolation.</p>
</td></tr>
<tr><td><code id="depthf.RP2_+3A_nproj">nproj</code></td>
<td>
<p>Number of projections taken in the computation of the double random projection depth. By default taken
to be <code>51</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function returns the vectors of sample double random projection depth values. 
The double random projection depths are described in Cuevas et al. (2007). They are of two types: RP2 type, and
RPD type. Both types of depths are based on bivariate projections of the bivariate functional data. 
These projections are taken randomly as a sample of standard
normal <code>d</code>-dimensional random variables, where <code>d</code> stands for the dimensionality of the internally 
represented discretized
functional data. For RP2 type depths, the average bivariate depth of the projected quantities is assessed.
For RPD type depths, further univariate projections of these bivariate projected quantities are evaluated, and
based on these final univariate quantities, the average univariate depth is computed.
</p>


<h3>Value</h3>

<p>Five vectors of length <code>m</code> are returned:
</p>

<ul>
<li> <p><code>Simpl_FD</code> the double random projection depth RP2 based on the bivariate simplicial depth,
</p>
</li>
<li> <p><code>Half_FD</code> the double random projection depth RP2 based on the bivariate halfspace depth,
</p>
</li>
<li> <p><code>hM_FD</code> the double random projection depth RP2 based on the bivariate h-mode depth,
</p>
</li>
<li> <p><code>Simpl_DD</code> the double random projection depth RPD based on the univariate simplicial depth,
</p>
</li>
<li> <p><code>Half_DD</code> the random projection depth RPD based on the univariate halfspace depth,
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Stanislav Nagy, <a href="mailto:nagy@karlin.mff.cuni.cz">nagy@karlin.mff.cuni.cz</a>
</p>


<h3>References</h3>

<p>Cuevas, A., Febrero, M. and Fraiman, R.  (2007).
Robust estimation and classification for functional data via projection-based depth notions.
<em>Computational Statistics</em> <b>22</b> (3), 481&ndash;496.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+depthf.RP1">depthf.RP1</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>datafA = dataf.population()$dataf[1:20]
datafB = dataf.population()$dataf[21:50]

dataf2A = derivatives.est(datafA,deriv=c(0,1))
dataf2B = derivatives.est(datafB,deriv=c(0,1))
depthf.RP2(dataf2A,dataf2B)


</code></pre>

<hr>
<h2 id='depthf.simplicialBand'>
Calculate Simplicial Band Depth
</h2><span id='topic+depthf.simplicialBand'></span>

<h3>Description</h3>

<p>Calculate the simplicial band depth defined by Lopez-Pintado, Sun, Lin, Genton (2014).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>depthf.simplicialBand(objectsf, dataf, modified = TRUE, J = NULL, 
                      range = NULL, d = 101)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="depthf.simplicialBand_+3A_objectsf">objectsf</code></td>
<td>

<p>Functoins for which the depth should be computed; a list containing lists (functions) of two vectors of equal length, named <code>args</code> and <code>vals</code>: arguments sorted in ascending order and corresponding them values respectively.
</p>
</td></tr>
<tr><td><code id="depthf.simplicialBand_+3A_dataf">dataf</code></td>
<td>

<p>Data sample of functoins w.r.t. which the depth should be computed; structure as for <code>objectsf</code>.
</p>
</td></tr>
<tr><td><code id="depthf.simplicialBand_+3A_modified">modified</code></td>
<td>

<p>Whether modified simplicial band depth should be computed; logical, <code>TRUE</code> by default.
</p>
</td></tr>
<tr><td><code id="depthf.simplicialBand_+3A_j">J</code></td>
<td>

<p>How many functions to consider in each tuple of the U-statistics; integer, <code>d+1</code> by default.
</p>
</td></tr>
<tr><td><code id="depthf.simplicialBand_+3A_range">range</code></td>
<td>

<p>The common range of the domain where the functions of objectsf and dataf are observed. Vector of length <code>2</code> with the left and the right end of the interval. Must contain all arguments given in objectsf and dataf.
</p>
</td></tr>
<tr><td><code id="depthf.simplicialBand_+3A_d">d</code></td>
<td>

<p>Grid size to which all the functional data are transformed. For depth computation, all functional observations are first transformed into vectors of their functional values of length d corresponding to equi-spaced points in the domain given by the interval range. Functional values in these points are reconstructed using linear interpolation, and extrapolation.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of depths of each of <code>objectsf</code> w.r.t. <code>dataf</code>.
</p>


<h3>References</h3>

<p>Lopez-Pintado, Sun, Lin, Genton (2014). Simplicial band depth for multivariate data. <em>Advances in Data Analysis and Classification</em> <b>8</b>(3) 321&ndash;338.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+depthf.BD">depthf.BD</a></code>, <code><a href="#topic+depthf.ABD">depthf.ABD</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>datafA = dataf.population()$dataf[1:20]
datafB = dataf.population()$dataf[21:50]

dataf2A = derivatives.est(datafA,deriv=c(0,1))
dataf2B = derivatives.est(datafB,deriv=c(0,1))
depthf.simplicialBand(dataf2A,dataf2B)
</code></pre>

<hr>
<h2 id='derivatives.est'>Estimation of the First Two Derivatives for Functional Data</h2><span id='topic+derivatives.est'></span>

<h3>Description</h3>

<p>Returns the estimated values of derivatives of functional data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>derivatives.est(dataf, range = NULL, d = 101, spar = NULL, deriv = c(0,1))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="derivatives.est_+3A_dataf">dataf</code></td>
<td>
<p>Functional dataset, represented by a <code>dataf</code> object of their arguments
and functional values. <code>m</code> stands for the number of functions.</p>
</td></tr>
<tr><td><code id="derivatives.est_+3A_range">range</code></td>
<td>
<p>The common range of the domain where the functions <code>dataf</code> are observed.
Vector of length 2 with the left and the right end of the interval. Must contain all arguments given in 
<code>dataf</code>.</p>
</td></tr>
<tr><td><code id="derivatives.est_+3A_d">d</code></td>
<td>
<p>Grid size to which all the functional data are transformed. For computation, 
all functional observations are first transformed into vectors of their functional values of length <code>d</code>
corresponding to equi-spaced points in the domain given by the interval <code>range</code>. Functional values in these
points are reconstructed using linear interpolation, and extrapolation.</p>
</td></tr>
<tr><td><code id="derivatives.est_+3A_spar">spar</code></td>
<td>
<p>If provided, this parameter is passed to functions <code>D1ss</code> and <code>D2ss</code> from package <code>sfsmisc</code>
as the value of the smoothing spline parameter in order to numerically approximate
the derivatives of <code>dataf</code>.</p>
</td></tr>
<tr><td><code id="derivatives.est_+3A_deriv">deriv</code></td>
<td>
<p>A vector composed of <code>0</code>, <code>1</code>, and <code>2</code> of the demanded 
functional values / derivatives of the functions in the rows of <code>dataf</code>.
<code>0</code> stands for the functional values, <code>1</code> for the first derivatives, 
<code>2</code> for the second derivatives.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If the input <code>dataf</code> is a functional random sample of size <code>m</code>, 
the function returns a <code>dataf</code> object of <code>nd</code>-dimensional functional data, where 
in the elements of the vector-valued functional data represent the estimated values of the 
derivatives of <code>dataf</code>. All derivatives are evaluated at an equi-distant grid of <code>d</code>
points in the domain given by <code>range</code>. <code>nd</code> here stands for <code>1</code>, <code>2</code> or <code>3</code>, 
depending on how many derivatives of <code>dataf</code> are
requested to be computed. For the estimation, functions <code>D1ss</code> and <code>D2ss</code> from the package
<code>sfsmisc</code> are utilized.
</p>


<h3>Value</h3>

<p>A multivariate <code>dataf</code> object of the functional values and / or the derivatives of <code>dataf</code>. 
The dimensionality of the vector-valued functional data is <code>nd</code>. The arguments of the data are all equal to 
an equi-distant grid of <code>d</code> points in the domain given by <code>range</code>. <code>nd</code> is the demanded number 
of derivatives at the output, i.e. the length of the vector <code>deriv</code>.
</p>


<h3>Author(s)</h3>

<p>Stanislav Nagy, <a href="mailto:nagy@karlin.mff.cuni.cz">nagy@karlin.mff.cuni.cz</a>
</p>


<h3>See Also</h3>

<p><code><a href="sfsmisc.html#topic+D2ss">D1ss</a></code> in package <span class="pkg">sfsmisc</span>
</p>
<p><code><a href="sfsmisc.html#topic+D2ss">D2ss</a></code> in package <span class="pkg">sfsmisc</span>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dataf = dataf.population()$dataf
derivatives.est(dataf,deriv=c(0,1,2))
</code></pre>

<hr>
<h2 id='dknn.classify'>
Depth-Based kNN
</h2><span id='topic+dknn.classify'></span>

<h3>Description</h3>

<p>The implementation of the affine-invariant depth-based kNN of Paindaveine and Van Bever (2015). 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dknn.classify(objects, data, k, depth = "halfspace", seed = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dknn.classify_+3A_objects">objects</code></td>
<td>

<p>Matrix containing objects to be classified; each row is one <code class="reqn">d</code>-dimensional object.
</p>
</td></tr>
<tr><td><code id="dknn.classify_+3A_data">data</code></td>
<td>

<p>Matrix containing training sample where each of <code class="reqn">n</code> rows is one object of the training sample where first <code class="reqn">d</code> entries are inputs and the last entry is output (class label).
</p>
</td></tr>
<tr><td><code id="dknn.classify_+3A_k">k</code></td>
<td>

<p>the number of neighbours
</p>
</td></tr>
<tr><td><code id="dknn.classify_+3A_depth">depth</code></td>
<td>

<p>Character string determining which depth notion to use; the default value is <code>"halfspace"</code>. 
Currently the method supports the following depths: &quot;halfspace&quot;, &quot;Mahalanobis&quot;, &quot;simplicial&quot;.
</p>
</td></tr>
<tr><td><code id="dknn.classify_+3A_seed">seed</code></td>
<td>

<p>the random seed. The default value <code>seed=0</code> makes no changes.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List containing class labels, or character string &quot;Ignored&quot; for the outsiders if &quot;Ignore&quot; was specified as the outsider treating method.
</p>


<h3>References</h3>

<p>Paindaveine, D. and Van Bever, G. (2015). Nonparametrically consistent depth-based classifiers. <em>Bernoulli</em> <b>21</b> 62&ndash;82.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dknn.train">dknn.train</a></code> to train the Dknn-classifier.
</p>
<p><code><a href="#topic+dknn.classify.trained">dknn.classify.trained</a></code> to classify with the Dknn-classifier.
</p>
<p><code><a href="#topic+ddalpha.train">ddalpha.train</a></code> to train the DD<code class="reqn">\alpha</code>-classifier.
</p>
<p><code><a href="#topic+ddalpha.getErrorRateCV">ddalpha.getErrorRateCV</a></code> and <code><a href="#topic+ddalpha.getErrorRatePart">ddalpha.getErrorRatePart</a></code> to get error rate of the Dknn-classifier on particular data (set <code>separator = "Dknn"</code>).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Generate a bivariate normal location-shift classification task
# containing 200 training objects and 200 to test with
class1 &lt;- mvrnorm(200, c(0,0), 
                  matrix(c(1,1,1,4), nrow = 2, ncol = 2, byrow = TRUE))
class2 &lt;- mvrnorm(200, c(2,2), 
                  matrix(c(1,1,1,4), nrow = 2, ncol = 2, byrow = TRUE))
trainIndices &lt;- c(1:100)
testIndices &lt;- c(101:200)
propertyVars &lt;- c(1:2)
classVar &lt;- 3
trainData &lt;- rbind(cbind(class1[trainIndices,], rep(1, 100)), 
                   cbind(class2[trainIndices,], rep(2, 100)))
testData &lt;- rbind(cbind(class1[testIndices,], rep(1, 100)), 
                  cbind(class2[testIndices,], rep(2, 100)))
data &lt;- list(train = trainData, test = testData)

# Train the classifier
# and get the classification error rate
cls &lt;- dknn.train(data$train, kMax = 20, depth = "Mahalanobis")
cls$k
classes1 &lt;- dknn.classify.trained(data$test[,propertyVars], cls)
cat("Classification error rate: ", 
    sum(unlist(classes1) != data$test[,classVar])/200)

# Classify the new data based on the old ones in one step
classes2 &lt;- dknn.classify(data$test[,propertyVars], data$train, k = cls$k, depth = "Mahalanobis")
cat("Classification error rate: ", 
    sum(unlist(classes2) != data$test[,classVar])/200)

</code></pre>

<hr>
<h2 id='dknn.classify.trained'>
Depth-Based kNN
</h2><span id='topic+dknn.classify.trained'></span>

<h3>Description</h3>

<p>The implementation of the affine-invariant depth-based kNN of Paindaveine and Van Bever (2015). 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dknn.classify.trained(objects, dknn)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dknn.classify.trained_+3A_objects">objects</code></td>
<td>

<p>Matrix containing objects to be classified; each row is one <code class="reqn">d</code>-dimensional object.
</p>
</td></tr>
<tr><td><code id="dknn.classify.trained_+3A_dknn">dknn</code></td>
<td>

<p>Dknn-classifier (obtained by <code><a href="#topic+dknn.train">dknn.train</a></code>).
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List containing class labels, or character string &quot;Ignored&quot; for the outsiders if &quot;Ignore&quot; was specified as the outsider treating method.
</p>


<h3>References</h3>

<p>Paindaveine, D. and Van Bever, G. (2015). Nonparametrically consistent depth-based classifiers. <em>Bernoulli</em> <b>21</b> 62&ndash;82.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dknn.train">dknn.train</a></code> to train the Dknn-classifier.
</p>
<p><code><a href="#topic+dknn.classify">dknn.classify</a></code> to classify with the Dknn-classifier.
</p>
<p><code><a href="#topic+ddalpha.train">ddalpha.train</a></code> to train the DD<code class="reqn">\alpha</code>-classifier.
</p>
<p><code><a href="#topic+ddalpha.getErrorRateCV">ddalpha.getErrorRateCV</a></code> and <code><a href="#topic+ddalpha.getErrorRatePart">ddalpha.getErrorRatePart</a></code> to get error rate of the Dknn-classifier on particular data (set <code>separator = "Dknn"</code>).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Generate a bivariate normal location-shift classification task
# containing 200 training objects and 200 to test with
class1 &lt;- mvrnorm(200, c(0,0), 
                  matrix(c(1,1,1,4), nrow = 2, ncol = 2, byrow = TRUE))
class2 &lt;- mvrnorm(200, c(2,2), 
                  matrix(c(1,1,1,4), nrow = 2, ncol = 2, byrow = TRUE))
trainIndices &lt;- c(1:100)
testIndices &lt;- c(101:200)
propertyVars &lt;- c(1:2)
classVar &lt;- 3
trainData &lt;- rbind(cbind(class1[trainIndices,], rep(1, 100)), 
                   cbind(class2[trainIndices,], rep(2, 100)))
testData &lt;- rbind(cbind(class1[testIndices,], rep(1, 100)), 
                  cbind(class2[testIndices,], rep(2, 100)))
data &lt;- list(train = trainData, test = testData)

# Train the classifier
# and get the classification error rate
cls &lt;- dknn.train(data$train, kMax = 20, depth = "Mahalanobis")
cls$k
classes1 &lt;- dknn.classify.trained(data$test[,propertyVars], cls)
cat("Classification error rate: ", 
    sum(unlist(classes1) != data$test[,classVar])/200)

# Classify the new data based on the old ones in one step
classes2 &lt;- dknn.classify(data$test[,propertyVars], data$train, k = cls$k, depth = "Mahalanobis")
cat("Classification error rate: ", 
    sum(unlist(classes2) != data$test[,classVar])/200)

</code></pre>

<hr>
<h2 id='dknn.train'>
Depth-Based kNN
</h2><span id='topic+dknn.train'></span>

<h3>Description</h3>

<p>The implementation of the affine-invariant depht-based kNN of Paindaveine and Van Bever (2015). 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dknn.train(data, kMax = -1, depth = "halfspace", seed = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dknn.train_+3A_data">data</code></td>
<td>

<p>Matrix containing training sample where each of <code class="reqn">n</code> rows is one object of the training sample where first <code class="reqn">d</code> entries are inputs and the last entry is output (class label).
</p>
</td></tr>
<tr><td><code id="dknn.train_+3A_kmax">kMax</code></td>
<td>

<p>the maximal value for the number of neighbours. If the value is set to -1, the default value is calculated as n/2, but at least 2, at most n-1.
</p>
</td></tr>
<tr><td><code id="dknn.train_+3A_depth">depth</code></td>
<td>

<p>Character string determining which depth notion to use; the default value is <code>"halfspace"</code>. 
Currently the method supports the following depths: &quot;halfspace&quot;, &quot;Mahalanobis&quot;, &quot;simplicial&quot;.
</p>
</td></tr>
<tr><td><code id="dknn.train_+3A_seed">seed</code></td>
<td>

<p>the random seed. The default value <code>seed=0</code> makes no changes.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The returned object contains technical information for classification, including the found optimal value <code>k</code>.
</p>


<h3>References</h3>

<p>Paindaveine, D. and Van Bever, G. (2015). Nonparametrically consistent depth-based classifiers. <em>Bernoulli</em> <b>21</b> 62&ndash;82.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dknn.classify">dknn.classify</a></code> and <code><a href="#topic+dknn.classify.trained">dknn.classify.trained</a></code> to classify with the Dknn-classifier.
</p>
<p><code><a href="#topic+ddalpha.train">ddalpha.train</a></code> to train the DD<code class="reqn">\alpha</code>-classifier.
</p>
<p><code><a href="#topic+ddalpha.getErrorRateCV">ddalpha.getErrorRateCV</a></code> and <code><a href="#topic+ddalpha.getErrorRatePart">ddalpha.getErrorRatePart</a></code> to get error rate of the Dknn-classifier on particular data (set <code>separator = "Dknn"</code>).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Generate a bivariate normal location-shift classification task
# containing 200 training objects and 200 to test with
class1 &lt;- mvrnorm(200, c(0,0), 
                  matrix(c(1,1,1,4), nrow = 2, ncol = 2, byrow = TRUE))
class2 &lt;- mvrnorm(200, c(2,2), 
                  matrix(c(1,1,1,4), nrow = 2, ncol = 2, byrow = TRUE))
trainIndices &lt;- c(1:100)
testIndices &lt;- c(101:200)
propertyVars &lt;- c(1:2)
classVar &lt;- 3
trainData &lt;- rbind(cbind(class1[trainIndices,], rep(1, 100)), 
                   cbind(class2[trainIndices,], rep(2, 100)))
testData &lt;- rbind(cbind(class1[testIndices,], rep(1, 100)), 
                  cbind(class2[testIndices,], rep(2, 100)))
data &lt;- list(train = trainData, test = testData)

# Train the classifier
# and get the classification error rate
cls &lt;- dknn.train(data$train, kMax = 20, depth = "Mahalanobis")
cls$k
classes1 &lt;- dknn.classify.trained(data$test[,propertyVars], cls)
cat("Classification error rate: ", 
    sum(unlist(classes1) != data$test[,classVar])/200)

# Classify the new data based on the old ones in one step
classes2 &lt;- dknn.classify(data$test[,propertyVars], data$train, k = cls$k, depth = "Mahalanobis")
cat("Classification error rate: ", 
    sum(unlist(classes2) != data$test[,classVar])/200)

</code></pre>

<hr>
<h2 id='draw.ddplot'>
Draw <em>DD</em>-Plot
</h2><span id='topic+draw.ddplot'></span>

<h3>Description</h3>

<p>The function draws the <em>DD</em>-plot either of the existing DD<code class="reqn">\alpha</code>-classifier of the depth space.
Also accessible from <code><a href="#topic+plot.ddalpha">plot.ddalpha</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>draw.ddplot(ddalpha, depth.space, cardinalities, 
            main = "DD plot", xlab = "C1", ylab = "C2", xlim, ylim,
            classes = c(1, 2), colors = c("red", "blue", "green"), drawsep = T)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="draw.ddplot_+3A_ddalpha">ddalpha</code></td>
<td>

<p>DD<code class="reqn">\alpha</code>-classifier (obtained by <code><a href="#topic+ddalpha.train">ddalpha.train</a></code>).
</p>
</td></tr>
<tr><td><code id="draw.ddplot_+3A_depth.space">depth.space</code></td>
<td>

<p>The ready depth space obtained by <code><a href="#topic+depth.space.">depth.space.</a></code></p>
</td></tr>
<tr><td><code id="draw.ddplot_+3A_cardinalities">cardinalities</code></td>
<td>

<p>Numerical vector of cardinalities of each class in <code>data</code>, each entry corresponds to one class.
</p>
</td></tr>
<tr><td><code id="draw.ddplot_+3A_main">main</code></td>
<td>

<p>an overall title for the plot: see <code><a href="graphics.html#topic+title">title</a></code>
</p>
</td></tr>
<tr><td><code id="draw.ddplot_+3A_xlab">xlab</code>, <code id="draw.ddplot_+3A_ylab">ylab</code></td>
<td>

<p>class labels
</p>
</td></tr>
<tr><td><code id="draw.ddplot_+3A_xlim">xlim</code>, <code id="draw.ddplot_+3A_ylim">ylim</code></td>
<td>

<p>range of axis
</p>
</td></tr>
<tr><td><code id="draw.ddplot_+3A_classes">classes</code></td>
<td>

<p>vector of numbers of two classes used for depth calculation
</p>
</td></tr>
<tr><td><code id="draw.ddplot_+3A_colors">colors</code></td>
<td>

<p>vector of the classes' colors
</p>
</td></tr>
<tr><td><code id="draw.ddplot_+3A_drawsep">drawsep</code></td>
<td>

<p>draws the separation on the DD-plot (currently for 2 classes and not for knn)
</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+ddalpha.train">ddalpha.train</a></code>
</p>
<p><code><a href="#topic+depth.space.">depth.space.</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  data = getdata("kidney")
  
  #1. using the existing ddalpha classifier
  ddalpha = ddalpha.train(data, depth = "spatial")
  draw.ddplot(ddalpha, main = "DD-plot")
  
  #2. using depth.space.
  # Sort the data w.r.t. classes
  data = rbind(data[data$C == 1,], data[data$C == 2,])
  cardinalities = c(sum(data$C == 1), sum(data$C == 2))
  
  dspace = depth.space.spatial(data[,-6], cardinalities = cardinalities)
  draw.ddplot(depth.space = dspace, cardinalities = cardinalities, 
              main = "DD-plot", xlab = 1, ylab = 2)
</code></pre>

<hr>
<h2 id='FKS'>Fast Kernel Smoothing</h2><span id='topic+FKS'></span>

<h3>Description</h3>

<p>Produces a kernel smoothed version of a function based on
the vectors given in the input. Bandwidth is selected using cross-validation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>FKS(dataf, Tout, kernel = c("uniform", "triangular", "Epanechnikov",
  "biweight", "triweight", "Gaussian"), m = 51, K = 20)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="FKS_+3A_dataf">dataf</code></td>
<td>
<p>A set of functional data given by a <code>dataf</code> object that are to be smoothed.</p>
</td></tr>
<tr><td><code id="FKS_+3A_tout">Tout</code></td>
<td>
<p>vector of values in the domain of the functions at which the
resulting smoothed function is evaluated</p>
</td></tr>
<tr><td><code id="FKS_+3A_kernel">kernel</code></td>
<td>
<p>Kernel used for smoothing. Admissible values are <code>uniform</code>, 
<code>triangular</code>, <code>Epanechnikov</code>, <code>biweight</code>, <code>triweight</code> and <code>Gaussian</code>.
By default, <code>uniform</code> is used.</p>
</td></tr>
<tr><td><code id="FKS_+3A_m">m</code></td>
<td>
<p>Number of points in the grid for choosing the cross-validated bandwidth.</p>
</td></tr>
<tr><td><code id="FKS_+3A_k">K</code></td>
<td>
<p>Performs <code>K</code>-fold cross-validation based on randomly shuffled data.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A vector of the same length as <code>Tout</code>
corresponding to the values of the
function produced using kernel smoothing, is provided. Bandwidth is selected using the
<code>K</code>-fold cross-validation of randomly shuffled input values.
</p>


<h3>Value</h3>

<p>A <code>dataf</code> object corresponding to <code>Tout</code> of smoothed functional values.
</p>


<h3>Author(s)</h3>

<p>Stanislav Nagy, <a href="mailto:nagy@karlin.mff.cuni.cz">nagy@karlin.mff.cuni.cz</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>d = 10
T = sort(runif(d))
X = T^2+ rnorm(d,sd=.1)
Tout = seq(0,1,length=101)

plot(T,X)
dataf = list(list(args=T,vals=X))
data.sm = FKS(dataf,Tout,kernel="Epan")
lines(data.sm[[1]]$args,data.sm[[1]]$vals,col=2)

datafs = structure(list(dataf=dataf,labels=1:length(dataf)),class="functional")
plot(datafs)
points(T,X)
data.sms = structure(list(dataf=data.sm,labels=1:length(data.sm)),class="functional")
plot(data.sms)

n = 6
dataf = list()
for(i in 1:n) dataf[[i]] = list(args = T&lt;-sort(runif(d)), vals = T^2 + rnorm(d,sd=.1))
data.sm = FKS(dataf,Tout,kernel="triweight")
data.sms = structure(list(dataf=data.sm,labels=1:length(data.sm)),class="functional")
plot(data.sms)
</code></pre>

<hr>
<h2 id='getdata'>
Data for Classification
</h2><span id='topic+getdata'></span><span id='topic+data'></span><span id='topic+baby'></span><span id='topic+banknoten'></span><span id='topic+biomed'></span><span id='topic+bloodtransfusion'></span><span id='topic+breast_cancer_wisconsin'></span><span id='topic+bupa'></span><span id='topic+chemdiab_1vs2'></span><span id='topic+chemdiab_1vs3'></span><span id='topic+chemdiab_2vs3'></span><span id='topic+cloud'></span><span id='topic+crabB_MvsF'></span><span id='topic+crabF_BvsO'></span><span id='topic+crabM_BvsO'></span><span id='topic+crabO_MvsF'></span><span id='topic+crab_BvsO'></span><span id='topic+crab_MvsF'></span><span id='topic+cricket_CvsP'></span><span id='topic+diabetes'></span><span id='topic+ecoli_cpvsim'></span><span id='topic+ecoli_cpvspp'></span><span id='topic+ecoli_imvspp'></span><span id='topic+gemsen_MvsF'></span><span id='topic+glass'></span><span id='topic+groessen_MvsF'></span><span id='topic+haberman'></span><span id='topic+heart'></span><span id='topic+hemophilia'></span><span id='topic+indian_liver_patient_1vs2'></span><span id='topic+indian_liver_patient_FvsM'></span><span id='topic+iris_setosavsversicolor'></span><span id='topic+iris_setosavsvirginica'></span><span id='topic+iris_versicolorvsvirginica'></span><span id='topic+irish_ed_MvsF'></span><span id='topic+kidney'></span><span id='topic+pima'></span><span id='topic+plasma_retinol_MvsF'></span><span id='topic+segmentation'></span><span id='topic+socmob_IvsNI'></span><span id='topic+socmob_WvsB'></span><span id='topic+tae'></span><span id='topic+tennis_MvsF'></span><span id='topic+tips_DvsN'></span><span id='topic+tips_MvsF'></span><span id='topic+uscrime_SvsN'></span><span id='topic+vertebral_column'></span><span id='topic+veteran_lung_cancer'></span><span id='topic+vowel_MvsF'></span><span id='topic+wine_1vs2'></span><span id='topic+wine_1vs3'></span><span id='topic+wine_2vs3'></span>

<h3>Description</h3>

<p>50 multivariate data sets for binary classification. For more details refer <a href="https://wisostat.uni-koeln.de/de/forschung/software-und-daten/data-for-classification/">https://wisostat.uni-koeln.de/de/forschung/software-und-daten/data-for-classification/</a>
</p>
<p>The <code>getdata</code> function gets the data set from the package, and returns it. The dataset itself does not appear in the global environment and the existing variables with the same name remain unchanged.
</p>


<h3>Usage</h3>

<pre><code class='language-R'># load the data set
# data(name)

# load the data set by name
# data(list = "name")

# load the data set by name to a variable
# getdata("name")

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="getdata_+3A_name">name</code></td>
<td>

<p>the data set name. 
</p>
</td></tr>
</table>


<h3>Format</h3>

<p>A data frame with <code>n</code> observations on the <code>d</code> variables. The last <code>d+1</code> column is the class label.
</p>

<dl>
<dt><code>x[,1:d]</code></dt><dd><p>numeric values</p>
</dd>
<dt><code>x[,d+1]</code></dt><dd><p>the numeric class label (0 or 1) or (1 or 2)</p>
</dd>
</dl>



<h3>Details</h3>

<p>The package contains data sets used in the joint project of the University of Cologne and the Hochschule Merseburg &quot;Classifying real-world data with the DDalpha-procedure&quot;. Comprehensive description of the methodology, and experimental settings and results of the study are presented in the work:
</p>
<p>Mozharovskyi, P., Mosler, K., and Lange, T. (2015). Classifying real-world data with the DD<code class="reqn">\alpha</code>-procedure. <em>Advances in Data Analysis and Classification</em> <b>9</b> 287&ndash;314.
</p>
<p>For a more complete explanation of the technique and further experiments see:
Lange, T., Mosler, K., and Mozharovskyi, P. (2014). Fast nonparametric classification based on data depth. <em>Statistical Papers</em> <b>55</b> 49&ndash;69.
</p>
<p>50 binary classification tasks have been obtained from partitioning 33 freely accessible data sets. Multiclass problems were reasonably split into binary classification problems, some of the data set were slightly processed by removing objects or attributes and selecting prevailing classes. Each data set is provided with a (short) description and brief descriptive statistics. The name reflects the origination of the data. A letter after the name is a property filter, letters (also their combinations) in brackets separated by &quot;vs&quot; are the classes opposed. The letters (combinations or words) stand for labels of classes (names of properties) and are intuitive. Each description contains a link to the original data.
</p>
<p>The data have been collected as open source data in January 2013. Owners of the package decline any responsibility regarding their correctness or consequences of their usage. If you publish material based on these data, please quote the original source. Special requests regarding citations are found on data set's web page.
</p>


<h3>Note</h3>

<p>List of the datasets:
</p>
<p>baby<br />
banknoten<br />
biomed<br />
bloodtransfusion<br />
breast_cancer_wisconsin<br />
bupa<br />
chemdiab_1vs2<br />
chemdiab_1vs3<br />
chemdiab_2vs3<br />
cloud<br />
crabB_MvsF<br />
crabF_BvsO<br />
crabM_BvsO<br />
crabO_MvsF<br />
crab_BvsO<br />
crab_MvsF<br />
cricket_CvsP<br />
diabetes<br />
ecoli_cpvsim<br />
ecoli_cpvspp<br />
ecoli_imvspp<br />
gemsen_MvsF<br />
glass<br />
groessen_MvsF<br />
haberman<br />
heart<br />
hemophilia<br />
indian_liver_patient_1vs2<br />
indian_liver_patient_FvsM<br />
iris_setosavsversicolor<br />
iris_setosavsvirginica<br />
iris_versicolorvsvirginica<br />
irish_ed_MvsF<br />
kidney<br />
pima<br />
plasma_retinol_MvsF<br />
segmentation<br />
socmob_IvsNI<br />
socmob_WvsB<br />
tae<br />
tennis_MvsF<br />
tips_DvsN<br />
tips_MvsF<br />
uscrime_SvsN<br />
vertebral_column<br />
veteran_lung_cancer<br />
vowel_MvsF<br />
wine_1vs2<br />
wine_1vs3<br />
wine_2vs3<br />
</p>
<p>Also functional data sets can be loaded:
</p>
<p>geneexp<br />
growth<br />
medflies<br />
population<br />
population2010<br />
tecator
</p>


<h3>References</h3>

<p>Lange, T., Mosler, K., and Mozharovskyi, P. (2014). Fast nonparametric classification based on data depth. <em>Statistical Papers</em> <b>55</b> 49&ndash;69.
</p>
<p>Mozharovskyi, P., Mosler, K., and Lange, T. (2015). Classifying real-world data with the DD<code class="reqn">\alpha</code>-procedure. <em>Advances in Data Analysis and Classification</em> <b>9</b> 287&ndash;314.
</p>
<p>The general list of sources consists of:
</p>
<p>UCI Machine Learning Repository <a href="https://archive.ics.uci.edu/ml/">https://archive.ics.uci.edu/ml/</a><br />
R-packages <a href="https://CRAN.R-project.org/">https://CRAN.R-project.org/</a> <br />
<a href="https://www.cmu.edu/dietrich/statistics-datascience/">https://www.cmu.edu/dietrich/statistics-datascience/</a> <br />
<a href="https://stat.ethz.ch/Teaching/Datasets/">https://stat.ethz.ch/Teaching/Datasets/</a> <br />
<a href="https://www.stats.ox.ac.uk/pub/PRNN/">https://www.stats.ox.ac.uk/pub/PRNN/</a> 
</p>


<h3>See Also</h3>

<p><code><a href="utils.html#topic+data">utils:data</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># load a dataset using data()
data(hemophilia)
data(list = "hemophilia")
rm(hemophilia)

# load data set using getdata()
hemophilia = "This is some existing object called 'hemophilia'. It remains unchanged"
d = getdata("hemophilia")
head(d)
print(hemophilia)

#get the list of all data sets
names = data(package = "ddalpha")$results[,3]

</code></pre>

<hr>
<h2 id='infimalRank'>Adjusted Ranking of Functional Data Based on the Infimal Depth</h2><span id='topic+infimalRank'></span>

<h3>Description</h3>

<p>Returns a vector of adjusted depth-based ranks for infimal depth for functional data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>infimalRank(ID, IA, ties.method = "max")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="infimalRank_+3A_id">ID</code></td>
<td>
<p>The vector of infimal depths of the curves of length <code>n</code>.</p>
</td></tr>
<tr><td><code id="infimalRank_+3A_ia">IA</code></td>
<td>
<p>The vector of the infimal areas corresponding to the infimal depths from <code>ID</code>
of length <code>n</code>.</p>
</td></tr>
<tr><td><code id="infimalRank_+3A_ties.method">ties.method</code></td>
<td>
<p>Parameter for breaking ties in infimal area index. By default <code>max</code>, see 
<code>rank</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Infimal depths for functional data tend to give to many functional observations the same 
value of depth. Using this function, the data whose depth is the same is ranked according
to the infimal area indicator. This indicator is provided in functions <code>depthf.fd1</code> along
the value of the infimal depth.
</p>


<h3>Value</h3>

<p>A vector of length <code>n</code>. Low depth values mean high ranks, i.e. potential outlyingness. 
If some of the infimal depths are identical, the ranking of these functions is made according to the 
values of the infimal area. There, higher infimal area index means higher rank, i.e. non-centrality.
</p>


<h3>Author(s)</h3>

<p>Stanislav Nagy, <a href="mailto:nagy@karlin.mff.cuni.cz">nagy@karlin.mff.cuni.cz</a>
</p>


<h3>References</h3>

<p>Nagy, S., Gijbels, I. and Hlubinka, D.  (2017).
Depth-based recognition of shape outlying functions. 
<em>Journal of Computational and Graphical Statistics</em>, <b>26</b> (4), 883&ndash;893.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>datafA = dataf.population()$dataf[1:20]
datafB = dataf.population()$dataf[21:50]
D = depthf.fd1(datafA,datafB)
infimalRank(D$Half_ID,D$Half_IA) 

ID = c(0,1,0,0,0,1,1)
IA = c(2,3,1,0,2,4,1)
infimalRank(ID,IA)
</code></pre>

<hr>
<h2 id='is.in.convex'>
Check Outsiderness
</h2><span id='topic+is.in.convex'></span>

<h3>Description</h3>

<p>Checks the belonging to at least one of class convex hulls of the training sample.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.in.convex(x, data, cardinalities, seed = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="is.in.convex_+3A_x">x</code></td>
<td>

<p>Matrix of objects (numerical vector as one object) whose belonging to convex hulls is to be checked; each row contains a <code class="reqn">d</code>-variate point. Should have the same dimension as <code>data</code>.
</p>
</td></tr>
<tr><td><code id="is.in.convex_+3A_data">data</code></td>
<td>

<p>Matrix containing training sample where each row is a <code class="reqn">d</code>-dimensional object, and objects of each class are kept together so that the matrix can be thought of as containing blocks of objects, representing classes.
</p>
</td></tr>
<tr><td><code id="is.in.convex_+3A_cardinalities">cardinalities</code></td>
<td>

<p>Numerical vector of cardinalities of each class in <code>data</code>, each entry corresponds to one class.
</p>
</td></tr>
<tr><td><code id="is.in.convex_+3A_seed">seed</code></td>
<td>

<p>the random seed. The default value <code>seed=0</code> makes no changes.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Checks are conducted w.r.t. each separate class in <code>data</code> using the simplex algorithm, taken from the C++ implementation of the zonoid depth calculation  by Rainer Dyckerhoff.
</p>


<h3>Value</h3>

<p>Matrix of <code>number of objects</code> rows and <code>number of classes</code> columns, containing <code>1</code> if an object belongs to the convex hull of the corresponding class, and <code>0</code> otherwise.
</p>


<h3>Author(s)</h3>

<p>Implementation of the simplex algorithm is taken from the algorithm for computation of zonoid depth (Dyckerhoff, Koshevoy and Mosler, 1996) that has been implemented in C++ by Rainer Dyckerhoff.
</p>


<h3>References</h3>

<p>Dyckerhoff, R., Koshevoy, G., and Mosler, K. (1996). Zonoid data depth: theory and computation. In: Prat A. (ed), <em>COMPSTAT 1996. Proceedings in computational statistics</em>, Physica-Verlag (Heidelberg), 235&ndash;240.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ddalpha.train">ddalpha.train</a></code> and <code><a href="#topic+ddalpha.classify">ddalpha.classify</a></code> for application.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Generate a bivariate normal location-shift classification task
# containing 400 training objects and 1000 to test with
class1 &lt;- mvrnorm(700, c(0,0), 
                  matrix(c(1,1,1,4), nrow = 2, ncol = 2, byrow = TRUE))
class2 &lt;- mvrnorm(700, c(2,2), 
                  matrix(c(1,1,1,4), nrow = 2, ncol = 2, byrow = TRUE))
trainIndices &lt;- c(1:200)
testIndices &lt;- c(201:700)
propertyVars &lt;- c(1:2)
classVar &lt;- 3
trainData &lt;- rbind(cbind(class1[trainIndices,], rep(1, 200)), 
                   cbind(class2[trainIndices,], rep(2, 200)))
testData &lt;- rbind(cbind(class1[testIndices,], rep(1, 500)), 
                  cbind(class2[testIndices,], rep(2, 500)))
data &lt;- list(train = trainData, test = testData)

# Count outsiders
numOutsiders = sum(rowSums(is.in.convex(data$test[,propertyVars], 
                                data$train[,propertyVars], c(200, 200))) == 0)
cat(numOutsiders, "outsiders found in the testing sample.\n")
</code></pre>

<hr>
<h2 id='L2metric'>Fast Computation of the <code class="reqn">L^2</code> Metric for Sets of Functional Data</h2><span id='topic+L2metric'></span>

<h3>Description</h3>

<p>Returns the matrix of <code class="reqn">L^2</code> distances between two sets of functional data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>L2metric(A, B)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="L2metric_+3A_a">A</code></td>
<td>
<p>Functions of the first set, represented by a matrix of their functional values of 
size <code>m*d</code>. <code>m</code> stands for the number of functions, <code>d</code>
is the number of the equi-distant points {1,...,d} in the domain of the data [1,d] at which the functional
values of the <code>m</code> functions are evaluated.</p>
</td></tr>
<tr><td><code id="L2metric_+3A_b">B</code></td>
<td>
<p>Functions of the second set, represented by a matrix of their functional values of 
size <code>n*d</code>. <code>n</code> stands for the number of functions, <code>d</code>
is the number of the equi-distant points {1,...,d} in the domain of the data [1,d] at which the functional
values of the <code>n</code> functions are evaluated. The grid of observation points for the 
functions <code>A</code> and <code>B</code> must be the same.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For two sets of functional data of sizes <code>m</code> and <code>n</code>
represented by matrices of their functional values on the common domain {1,...,d}, 
this function returns the symmetric matrix of size <code>m*n</code> whose entry in the
<code>i</code>-th row and <code>j</code>-th column is the approximated <code class="reqn">L^2</code> distance of the 
<code>i</code>-th function from the first set, and the <code>j</code>-th function from the second set.
This function is utilized in the computation of the h-mode depth.
</p>


<h3>Value</h3>

<p>A symmetric matrix of the distances of the functions of size <code>m*n</code>.
</p>


<h3>Author(s)</h3>

<p>Stanislav Nagy, <a href="mailto:nagy@karlin.mff.cuni.cz">nagy@karlin.mff.cuni.cz</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+depthf.hM">depthf.hM</a></code>
</p>
<p><code><a href="#topic+dataf2rawfd">dataf2rawfd</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>datapop = dataf2rawfd(dataf.population()$dataf,range=c(1950,2015),d=66)
A = datapop[1:20,]
B = datapop[21:50,]
L2metric(A,B)

</code></pre>

<hr>
<h2 id='plot.ddalpha'>
Plots for the &quot;ddalpha&quot; Class
</h2><span id='topic+plot.ddalpha'></span>

<h3>Description</h3>

<p><code><a href="#topic+depth.contours.ddalpha">depth.contours.ddalpha</a></code> &ndash; builds the data depth contours for multiclass 2-dimensional data using the trained classifier.
<code><a href="#topic+draw.ddplot">draw.ddplot</a></code> &ndash; draws the <em>DD</em>-plot of the existing DD<code class="reqn">\alpha</code>-classifier.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ddalpha'
plot(x, type = c("ddplot", "depth.contours"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.ddalpha_+3A_x">x</code></td>
<td>

<p>DD<code class="reqn">\alpha</code>-classifier (obtained by <code><a href="#topic+ddalpha.train">ddalpha.train</a></code>).
</p>
</td></tr>
<tr><td><code id="plot.ddalpha_+3A_type">type</code></td>
<td>

<p>type of the plot for <code><a href="#topic+draw.ddplot">draw.ddplot</a></code> or <code><a href="#topic+depth.contours.ddalpha">depth.contours.ddalpha</a></code>
</p>
</td></tr>
<tr><td><code id="plot.ddalpha_+3A_...">...</code></td>
<td>

<p>additional parameters passed to the depth functions and to <code><a href="base.html#topic+plot">plot</a></code>
</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+depth.">depth.</a></code>
</p>
<p><code><a href="#topic+depth.contours">depth.contours</a></code>
</p>
<p><code><a href="#topic+depth.graph">depth.graph</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 


par(mfrow = c(2,2))
data(hemophilia)

ddalpha = ddalpha.train(hemophilia, depth = "none")
plot(ddalpha, type = "depth.contours", main = "data")
plot(ddalpha, type = "ddplot", main = "data", drawsep = F)

for (depth in c("zonoid", "Mahalanobis", "projection", "spatial")){
  ddalpha = ddalpha.train(hemophilia, depth = depth)
  plot(ddalpha, type = "depth.contours", main = depth, drawsep = T)
  plot(ddalpha, type = "ddplot", main = depth)
}


## End(Not run)
</code></pre>

<hr>
<h2 id='plot.ddalphaf'>
Plots for the &quot;ddalphaf&quot; Class
</h2><span id='topic+plot.ddalphaf'></span>

<h3>Description</h3>

<p><code><a href="#topic+plot.functional">plot.functional</a></code> &ndash; plots the functional data used by classifier
</p>
<p><code><a href="#topic+depth.contours.ddalpha">depth.contours.ddalpha</a></code> &ndash; builds the data depth contours for multiclass 2-dimensional data using the trained classifier.
<code><a href="#topic+draw.ddplot">draw.ddplot</a></code> &ndash; draws the <em>DD</em>-plot of the existing DD<code class="reqn">\alpha</code>-classifier.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ddalphaf'
plot(x, type = c("functional.data", "ddplot", "depth.contours"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.ddalphaf_+3A_x">x</code></td>
<td>

<p>functional DD<code class="reqn">\alpha</code>-classifier (obtained by <code><a href="#topic+ddalphaf.train">ddalphaf.train</a></code>).
</p>
</td></tr>
<tr><td><code id="plot.ddalphaf_+3A_type">type</code></td>
<td>

<p>type of the plot for <code><a href="#topic+plot.functional">plot.functional</a></code>, <code><a href="#topic+draw.ddplot">draw.ddplot</a></code> or <code><a href="#topic+depth.contours.ddalpha">depth.contours.ddalpha</a></code>
</p>
</td></tr>
<tr><td><code id="plot.ddalphaf_+3A_...">...</code></td>
<td>

<p>additional parameters passed to the depth functions and to <code><a href="base.html#topic+plot">plot</a></code>
</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+depth.">depth.</a></code>
</p>
<p><code><a href="#topic+depth.contours">depth.contours</a></code>
</p>
<p><code><a href="#topic+depth.graph">depth.graph</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 

dataf = dataf.growth()
ddalphaf = ddalphaf.train (dataf$dataf, dataf$labels, 
                            classifier.type = "ddalpha", maxNumIntervals = 2)

# plot the functional data
plot(ddalphaf)

# plot depth contours and separation in the transformed space 
# (possible only if maxNumIntervals = 2)
plot(ddalphaf, type = "depth.contours")

# plot the DD-plot
plot(ddalphaf, type = "ddplot")


## End(Not run)
</code></pre>

<hr>
<h2 id='plot.functional'>
Plot functions for the Functional Data
</h2><span id='topic+plot.functional'></span><span id='topic+lines.functional'></span><span id='topic+points.functional'></span>

<h3>Description</h3>

<p>Plots the functional data given in the form which is described in the topic <code><a href="#topic+dataf.+2A">dataf.*</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'functional'
plot(x, 
      main = "Functional data", xlab = "args", ylab = "vals", 
      colors = c("red", "blue", "green", "black", "orange", "pink"), ...)
      
## S3 method for class 'functional'
lines(x, 
      colors = c("red", "blue", "green", "black", "orange", "pink"), ...)
      
## S3 method for class 'functional'
points(x, 
      colors = c("red", "blue", "green", "black", "orange", "pink"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.functional_+3A_x">x</code></td>
<td>

<p>The functional data as in the topic <code><a href="#topic+dataf.+2A">dataf.*</a></code>. 
Note, that the in order to use s3 methods the data must be of class &quot;functional&quot;. 
</p>
</td></tr>
<tr><td><code id="plot.functional_+3A_main">main</code></td>
<td>

<p>an overall title for the plot: see <code><a href="graphics.html#topic+title">title</a></code>
</p>
</td></tr>
<tr><td><code id="plot.functional_+3A_xlab">xlab</code></td>
<td>

<p>a title for the x axis: see <code><a href="graphics.html#topic+title">title</a></code>
</p>
</td></tr>
<tr><td><code id="plot.functional_+3A_ylab">ylab</code></td>
<td>

<p>a title for the y axis: see <code><a href="graphics.html#topic+title">title</a></code>
</p>
</td></tr>
<tr><td><code id="plot.functional_+3A_colors">colors</code></td>
<td>

<p>the colors for the classes of the data. The colors are applied to the classes sorted in alphabetical order. Use the same set of classes to ensure that the same colours are selected in <code>lines</code> and <code>points</code> as in <code>plot</code> (do not remove entire classes). 
</p>
</td></tr>
<tr><td><code id="plot.functional_+3A_...">...</code></td>
<td>

<p>additional parameters
</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+dataf.+2A">dataf.*</a></code> for functional data description
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 
  ## load the Growth dataset
  dataf = dataf.growth()
  
  labels = unlist(dataf$labels)
  plot(dataf, 
       main = paste("Growth: girls red (", sum(labels == "girl"), "),", 
                    " boys blue (", sum(labels == "boy"), ")", sep=""),
       xlab="Year", ylab="Height, cm",
       colors = c("blue", "red")   # in alphabetical order of class labels   
  )
  
  # plot an observation as a line
  observation = structure(list(dataf = list(dataf$dataf[[1]])), class = "functional")
  lines(observation, colors = "green", lwd = 3)
  
  # plot hight at the age of 14 
  indexAge14 = which(observation$dataf[[1]]$args == 14)
  hightAge14 = observation$dataf[[1]]$vals[indexAge14]
  atAge14 = structure(list(
                      dataf = list(dataf = list(args = 14, vals = hightAge14))
                      ), class = "functional")
  points(atAge14, colors = "yellow", pch = 18)

## End(Not run)

</code></pre>

<hr>
<h2 id='rawfd2dataf'>Transform Raw Functional Data to a <code>dataf</code> Object</h2><span id='topic+rawfd2dataf'></span>

<h3>Description</h3>

<p>Constructs a (possibly multivariate) functional data object given by an array of its functional values
evaluated at an equi-distant grid of points, and transforms it into a <code>dataf</code> object more suitable 
for work in the <code>ddalpha</code> package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rawfd2dataf(X, range)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rawfd2dataf_+3A_x">X</code></td>
<td>
<p>Either a matrix of size <code>n*d</code>, or an array of dimension <code>n*d*k</code> of functional values. Here <code>n</code>
stands for the number of functions, <code>d</code> is the number of equi-distant points in the domain where the functional
values are evaluated, and if applicable, <code>k</code> is the dimensionality of the (vector-valued) functional data.</p>
</td></tr>
<tr><td><code id="rawfd2dataf_+3A_range">range</code></td>
<td>
<p>A vector of size two that represents the endpoints of the common domain of all functions <code>X</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A (possibly multivariate) <code>dataf</code> object corresponding to the functional data <code>X</code> evaluated at an
equi-distant grid of points.
</p>


<h3>Author(s)</h3>

<p>Stanislav Nagy, <a href="mailto:nagy@karlin.mff.cuni.cz">nagy@karlin.mff.cuni.cz</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dataf2rawfd">dataf2rawfd</a></code>
</p>
<p><code><a href="#topic+depthf.fd1">depthf.fd1</a></code>
</p>
<p><code><a href="#topic+depthf.fd2">depthf.fd2</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## transform a matrix into a functional data set
n = 5
d = 21
X = matrix(rnorm(n*d),ncol=d)
rawfd2dataf(X,range=c(0,1))

## transform an array into a multivariate functional data set
k = 3
X = array(rnorm(n*d*k),dim=c(n,d,k))
rawfd2dataf(X,range=c(-1,1))

</code></pre>

<hr>
<h2 id='resetPar'>
Reset Graphical Parameters
</h2><span id='topic+resetPar'></span>

<h3>Description</h3>

<p>The function returns the default graphical parameters for <code><a href="graphics.html#topic+par">par</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>resetPar()
</code></pre>


<h3>Details</h3>

<p>The returned parameters are used as input parameters for <code><a href="graphics.html#topic+par">par</a></code>.
</p>


<h3>Value</h3>

<p>The list of graphical parameters described in the 'Graphical Parameters' section of <code><a href="graphics.html#topic+par">par</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
par(mfrow = c(1,2), mar = c(2,2,2,2))
plot(sin, -pi, 2*pi)
plot(cos, -pi, 2*pi)

par(resetPar())
plot(sin, -pi, 2*pi)
plot(cos, -pi, 2*pi)

</code></pre>

<hr>
<h2 id='shape.fd.analysis'>Diagnostic Plot for First and Second Order Integrated and Infimal Depths</h2><span id='topic+shape.fd.analysis'></span>

<h3>Description</h3>

<p>Produce the diagnostic plot based on the fist or second order extended integrated / infimal depths.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>shape.fd.analysis(datafA, datafB, range = NULL, d = 101, order = 1,
  method = c("halfspace", "simplicial"), approx = 0, title = "",
  nfun = 10, plot = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="shape.fd.analysis_+3A_datafa">datafA</code></td>
<td>
<p>A single function whose depth is computed, represented by a 
<code>dataf</code> object of arguments and functional values.</p>
</td></tr>
<tr><td><code id="shape.fd.analysis_+3A_datafb">datafB</code></td>
<td>
<p>Functional dataset with respect to which the depth of <code>datafA</code> is computed. 
<code>datafB</code> is represented by a <code>dataf</code> object of arguments and functional values. 
<code>n</code> stands for the number of functions. The grid of observation points for the 
functions in <code>datafA</code> and <code>datafB</code> may not be the same.</p>
</td></tr>
<tr><td><code id="shape.fd.analysis_+3A_range">range</code></td>
<td>
<p>The common range of the domain where the functions <code>datafA</code> and <code>datafB</code> are observed.
Vector of length 2 with the left and the right end of the interval. Must contain all arguments given in 
<code>datafA</code> and <code>datafB</code>.</p>
</td></tr>
<tr><td><code id="shape.fd.analysis_+3A_d">d</code></td>
<td>
<p>Grid size to which all the functional data are transformed. For depth computation, 
all functional observations are first transformed into vectors of their functional values of length <code>d</code>
corresponding to equi-spaced points in the domain given by the interval <code>range</code>. Functional values in these
points are reconstructed using linear interpolation, and extrapolation.</p>
</td></tr>
<tr><td><code id="shape.fd.analysis_+3A_order">order</code></td>
<td>
<p>The order of the depth to be used in the plot, for <code>order=1</code> produces
the plot of univariate marginal depth of <code>A</code> and <code>nfun</code> functions from <code>B</code> 
over the domain of the functions. For <code>order=2</code> produces the bivariate contour plot 
of the bivariate depths of <code>A</code> at couples of points from the domain.</p>
</td></tr>
<tr><td><code id="shape.fd.analysis_+3A_method">method</code></td>
<td>
<p>The depth that is used in the diagnostic plot. possible values are <code>halfspace</code> for 
the halfspace depth, or <code>simplicial</code> for the simplicial depth.</p>
</td></tr>
<tr><td><code id="shape.fd.analysis_+3A_approx">approx</code></td>
<td>
<p>For <code>order=2</code>, the number of approximations used in the computation of the order extended depth. By default
this is set to <code>0</code>, meaning that the depth is computed at all possible <code>d^2</code>
combinations of the points in the domain. When set to a positive integer, <code>approx</code>
bivariate points are randomly sampled in unit square, and at these points the bivariate depths of the
corresponding functional values are computed.</p>
</td></tr>
<tr><td><code id="shape.fd.analysis_+3A_title">title</code></td>
<td>
<p>The title of the diagnostic plot.</p>
</td></tr>
<tr><td><code id="shape.fd.analysis_+3A_nfun">nfun</code></td>
<td>
<p>For <code>order=1</code>, the number of functions from <code>B</code> whose coordinate-wise
univariate depths of functional values should be displayed with the depth of <code>A</code>.
The depth of <code>A</code> is displayed in solid red line, the depths of the functions from <code>B</code>
in dashed black.</p>
</td></tr>
<tr><td><code id="shape.fd.analysis_+3A_plot">plot</code></td>
<td>
<p>Logical: should the function by plotted?</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Plots a diagnostic plot of pointwise univariate (or bivariate) depths for all possible points (or couples of points) from the domain of the 
functional data. From such a plot it is possible to infer into the first order (or second order) properties of a single function <em>x</em> with respect 
to the given set of functional data. For <code>order=1</code>, the integral of the displayed function is the integrated depth of <em>x</em>, 
the smallest value of the function is the infimal depth of <em>x</em>. 
For <code>order=2</code>, the bivariate integral of the displayed surface gives the second order extended 
integrated depth of <em>x</em>, the infimum of this bivariate function gives the second order infimal depth of <em>x</em>. 
For details see Nagy et al. (2016) and <code><a href="#topic+depthf.fd1">depthf.fd1</a></code>.
</p>


<h3>Value</h3>

<p>For <code>order=1</code> two depth values, and two vectors of pointwise depths:
</p>

<ul>
<li> <p><code>Simpl_FD</code> the first order integrated depth based on the simplicial depth,
</p>
</li>
<li> <p><code>Half_FD</code> the first order integrated depth based on the halfspace depth,
</p>
</li>
<li> <p><code>Simpl_ID</code> the first order infimal depth based on the simplicial depth,
</p>
</li>
<li> <p><code>Half_ID</code> the first order infimal depth based on the halfspace depth,
</p>
</li>
<li> <p><code>PSD</code> the vector of length <code>d</code> containing the computed 
pointwise univariate simplicial depths used for the computation of <code>Simpl_FD</code> and <code>Simpl_ID</code>,
</p>
</li>
<li> <p><code>PHD</code> the vector of length <code>d</code> containing the computed 
pointwise univariate halfspace depths used for the computation of <code>Half_FD</code> and <code>Half_ID</code>.
</p>
</li></ul>

<p>In addition, the first order integrated / infimal depth diagnostic plot of the function <code>A</code> with respect to
the random sample given by the functions corresponding to the rows of the matrix <code>B</code> is produced.
</p>
<p>For <code>order=2</code> four depth values, and two matrices of pointwise depths:
</p>

<ul>
<li> <p><code>Simpl_FD</code> the second order integrated depth based on the simplicial depth,
</p>
</li>
<li> <p><code>Half_FD</code> the second order integrated depth based on the halfspace depth,
</p>
</li>
<li> <p><code>Simpl_ID</code> the second order infimal depth based on the simplicial depth,
</p>
</li>
<li> <p><code>Half_ID</code> the second order infimal depth based on the halfspace depth,
</p>
</li>
<li> <p><code>PSD</code> the matrix of size <code>d*d</code> containing the computed 
pointwise bivariate simplicial depths used for the computation of <code>Simpl_FD</code> and <code>Simpl_ID</code>,
</p>
</li>
<li> <p><code>PHD</code> the matrix of size <code>d*d</code> containing the computed 
pointwise bivariate halfspace depths used for the computation of <code>Half_FD</code> and <code>Half_ID</code>.
</p>
</li></ul>

<p>In addition, the second order integrated / infimal depth diagnostic plot of the function <code>A</code> with respect to
the random sample given by the functions corresponding to the rows of the matrix <code>B</code> is produced.
</p>


<h3>Author(s)</h3>

<p>Stanislav Nagy, <a href="mailto:nagy@karlin.mff.cuni.cz">nagy@karlin.mff.cuni.cz</a>
</p>


<h3>References</h3>

<p>Nagy, S., Gijbels, I. and Hlubinka, D.  (2017).
Depth-based recognition of shape outlying functions. 
<em>Journal of Computational and Graphical Statistics</em>, <b>26</b> (4), 883&ndash;893.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+depthf.fd1">depthf.fd1</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>datafA = dataf.population()$dataf[1]
dataf = dataf.population()$dataf[2:20]
shape.fd.analysis(datafA,dataf,order=1)
shape.fd.analysis(datafA,dataf,order=2,approx=0)

</code></pre>

<hr>
<h2 id='shape.fd.outliers'>Functional Depth-Based Shape Outlier Detection</h2><span id='topic+shape.fd.outliers'></span>

<h3>Description</h3>

<p>Detects functional outliers of first three orders, based on the order extended integrated depth for functional data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>shape.fd.outliers(dataf, range = NULL, d = 101, q = 0.05,
  method = c("halfspace", "simplicial"), approx = 100, print = FALSE,
  plotpairs = FALSE, max.order = 3, exclude.out = TRUE,
  output = c("matrix", "list"), identifiers = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="shape.fd.outliers_+3A_dataf">dataf</code></td>
<td>
<p>Functional dataset, represented by a <code>dataf</code> object of their arguments
and functional values. <code>n</code> stands for the number of functions.</p>
</td></tr>
<tr><td><code id="shape.fd.outliers_+3A_range">range</code></td>
<td>
<p>The common range of the domain where the fucntions <code>dataf</code> are observed.
Vector of length 2 with the left and the right end of the interval. Must contain all arguments given in 
<code>dataf</code>.</p>
</td></tr>
<tr><td><code id="shape.fd.outliers_+3A_d">d</code></td>
<td>
<p>Grid size to which all the functional data are transformed. For depth computation, 
all functional observations are first transformed into vectors of their functional values of length <code>d</code>
corresponding to equi-spaced points in the domain given by the interval <code>range</code>. Functional values in these
points are reconstructed using linear interpolation, and extrapolation.</p>
</td></tr>
<tr><td><code id="shape.fd.outliers_+3A_q">q</code></td>
<td>
<p>The quantile presenting a threshold for the first order outlier detection. Functions with first order integrated depth
smaller than the <code>q</code> quantile of this sample of depths are flagged as potential outliers. If set to <code>NULL</code>, the
the outliers are detected from the first order integrated depth after the log-transformation, as for higher order outliers.</p>
</td></tr>
<tr><td><code id="shape.fd.outliers_+3A_method">method</code></td>
<td>
<p>The depth that is used in the diagnostic plot. possible values are <code>halfspace</code> for 
the halfspace depth, or <code>simplicial</code> for the simplicial depth.</p>
</td></tr>
<tr><td><code id="shape.fd.outliers_+3A_approx">approx</code></td>
<td>
<p>For the computation of the third order integrated depth,
the number of approximations used in the computation of the order extended depth. By default
this is set to <code>100</code>, meaning that <code>100</code>
trivariate points are randomly sampled in unit cube, and at these points the trivariate depths of the
corresponding functional values. May be set to <code>0</code> to compute the depth at all possible <code>d^3</code>
combinations of the points in the domain. This choice may result in very slow computation, see also <code><a href="#topic+depthf.fd1">depthf.fd1</a></code>.</p>
</td></tr>
<tr><td><code id="shape.fd.outliers_+3A_print">print</code></td>
<td>
<p>If the rows of <code>X</code> are named, <code>print=TRUE</code> enables a graphical output when the names of the outlying curves
are displayed.</p>
</td></tr>
<tr><td><code id="shape.fd.outliers_+3A_plotpairs">plotpairs</code></td>
<td>
<p>If set to <code>TRUE</code>, the scatter plot of the computed depths for orders <code>1</code>, <code>2</code> and <code>3</code> is
is displayed. Here, the depths corresponding to the flagged outliers are plotted in colour.</p>
</td></tr>
<tr><td><code id="shape.fd.outliers_+3A_max.order">max.order</code></td>
<td>
<p>Maximal order of shape outlyingness to be computed, can be set to <code>1</code>, <code>2</code>, or <code>3</code>.</p>
</td></tr>
<tr><td><code id="shape.fd.outliers_+3A_exclude.out">exclude.out</code></td>
<td>
<p>Logical variable; exclude the detected lower order outliers in the flagging process? By default <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="shape.fd.outliers_+3A_output">output</code></td>
<td>
<p>Output method, can be set to <code>matrix</code> for a matrix with logical entries (<code>TRUE</code> for outliers), or <code>list</code> for 
a list of outliers.</p>
</td></tr>
<tr><td><code id="shape.fd.outliers_+3A_identifiers">identifiers</code></td>
<td>
<p>A vector of names for the data observations. Facilitates identification of outlying functions.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Using the procedure described in Nagy et al. (2016), the function uses the order extended integrated depths for functions, 
see <code><a href="#topic+depthf.fd1">depthf.fd1</a></code> and <code><a href="#topic+shape.fd.analysis">shape.fd.analysis</a></code>, to perform informal functional shape outlier detection. 
Outliers of the first order (horizontal shift outliers) are found as the functions with <code>q</code> % of smallest (first order)
integrated depth values. Second and third order outliers (shape outliers) are found using the extension of the boxplot method
for depths as described in the paper Nagy et al. (2016).
</p>


<h3>Value</h3>

<p>A matrix of logical values of size <code>n*4</code>, where <code>n</code> is the sample size. In the first three rows indicators of outlyingness
of the corresponding functions for orders <code>1</code>, <code>2</code> and <code>3</code> are given, in the fourth row the indicator of outlyingness
with respect to the comparison of the first, and third order depths is given. That is, the fist row corresponds to the first order outliers, 
the second row to the second order outliers, and the last two rows formally to the third order outliers. Please consult Nagy et al. (2016)
to interpret the notion of shape outlyingness.
</p>


<h3>Author(s)</h3>

<p>Stanislav Nagy, <a href="mailto:nagy@karlin.mff.cuni.cz">nagy@karlin.mff.cuni.cz</a>
</p>


<h3>References</h3>

<p>Nagy, S., Gijbels, I. and Hlubinka, D.  (2017).
Depth-based recognition of shape outlying functions. 
<em>Journal of Computational and Graphical Statistics</em>, <b>26</b> (4), 883&ndash;893.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+depthf.fd1">depthf.fd1</a></code>, <code><a href="#topic+shape.fd.analysis">shape.fd.analysis</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n = 30
dataf = dataf.population()$dataf[1:n]
shape.fd.outliers(dataf,print=TRUE,plotpairs=TRUE,
identifiers=unlist(dataf.population()$identifier)[1:n])

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
