<!DOCTYPE html><html><head><title>Help for package ML2Pvae</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {ML2Pvae}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#.onLoad'><p>Display a message upon loading package</p></a></li>
<li><a href='#build_hidden_encoder'><p>Build the encoder for a VAE</p></a></li>
<li><a href='#build_vae_correlated'><p>Build a VAE that fits to a normal, full covariance N(m,S) latent distribution</p></a></li>
<li><a href='#build_vae_independent'><p>Build a VAE that fits to a standard N(0,I) latent distribution with independent latent traits</p></a></li>
<li><a href='#correlation_matrix'><p>Simulated latent abilities correlation matrix</p></a></li>
<li><a href='#diff_true'><p>Simulated difficulty parameters</p></a></li>
<li><a href='#disc_true'><p>Simulated discrimination parameters</p></a></li>
<li><a href='#get_ability_parameter_estimates'><p>Feed forward response sets through the encoder, which outputs student ability estimates</p></a></li>
<li><a href='#get_item_parameter_estimates'><p>Get trainable variables from the decoder, which serve as item parameter estimates.</p></a></li>
<li><a href='#ML2Pvae'><p>ML2Pvae: A package for creating a VAE whose decoder recovers the parameters of the ML2P model.</p>
The encoder can be used to predict the latent skills based on assessment scores.</a></li>
<li><a href='#q_1pl_constraint'><p>A custom kernel constraint function that forces nonzero weights to be equal to one, so the VAE will estimate the 1-parameter logistic model. Nonzero weights are determined by the Q matrix.</p></a></li>
<li><a href='#q_constraint'><p>A custom kernel constraint function that restricts weights between the learned distribution and output. Nonzero weights are determined by the Q matrix.</p></a></li>
<li><a href='#q_matrix'><p>Simulated Q-matrix</p></a></li>
<li><a href='#responses'><p>Response data</p></a></li>
<li><a href='#sampling_correlated'><p>A reparameterization in order to sample from the learned multivariate normal distribution of the VAE</p></a></li>
<li><a href='#sampling_independent'><p>A reparameterization in order to sample from the learned standard normal distribution of the VAE</p></a></li>
<li><a href='#theta_true'><p>Simulated ability parameters</p></a></li>
<li><a href='#train_model'><p>Trains a VAE or autoencoder model. This acts as a wrapper for keras::fit().</p></a></li>
<li><a href='#vae_loss_correlated'><p>A custom loss function for a VAE learning a multivariate normal distribution with a full covariance matrix</p></a></li>
<li><a href='#vae_loss_independent'><p>A custom loss function for a VAE learning a standard normal distribution</p></a></li>
<li><a href='#validate_inputs'><p>Give error messages for invalid inputs in exported functions.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Variational Autoencoder Models for IRT Parameter Estimation</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.0.1</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Geoffrey Converse &lt;converseg@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Based on the work of Curi, Converse, Hajewski, and Oliveira (2019) &lt;<a href="https://doi.org/10.1109%2FIJCNN.2019.8852333">doi:10.1109/IJCNN.2019.8852333</a>&gt;. This package provides easy-to-use functions which create a variational autoencoder (VAE) to be used for parameter estimation in Item Response Theory (IRT) - namely the Multidimensional Logistic 2-Parameter (ML2P) model. To use a neural network as such, nontrivial modifications to the architecture must be made, such as restricting the nonzero weights in the decoder according to some binary matrix Q. The functions in this package allow for straight-forward construction, training, and evaluation so that minimal knowledge of 'tensorflow' or 'keras' is required. </td>
</tr>
<tr>
<td>Note:</td>
<td>The developer version of 'keras' should be used, rather than the
CRAN version. The latter will cause tests to fail on an initial
run, but work on subsequent tries. To avoid this, use
devtools::install_github("rstudio/keras"). The user also must
have an installation of 'Python 3'.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>Imports:</td>
<td>keras (&ge; 2.3.0), reticulate (&ge; 1.0), tensorflow (&ge; 2.2.0),
tfprobability (&ge; 0.11.0)</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.1</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown, testthat, R.rsp</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>R.rsp</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.6)</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://converseg.github.io">https://converseg.github.io</a></td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>TensorFlow (https://www.tensorflow.org), Keras
(https://keras.io), TensorFlow Probability
(https://www.tensorflow.org/probability)</td>
</tr>
<tr>
<td>Config/reticulate:</td>
<td>list( packages = list( list(package = "keras", pip =
TRUE), list(package = "tensorflow", pip = TRUE), list(package =
"tensorflow-probability", pip = TRUE) ) )</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-05-23 07:38:42 UTC; hornik</td>
</tr>
<tr>
<td>Author:</td>
<td>Geoffrey Converse [aut, cre, cph],
  Suely Oliveira [ctb, ths],
  Mariana Curi [ctb]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-05-23 08:02:16 UTC</td>
</tr>
</table>
<hr>
<h2 id='.onLoad'>Display a message upon loading package</h2><span id='topic+.onLoad'></span>

<h3>Description</h3>

<p>Display a message upon loading package
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.onLoad(libnam, pkgname)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".onLoad_+3A_libnam">libnam</code></td>
<td>
<p>the library name</p>
</td></tr>
<tr><td><code id=".onLoad_+3A_pkgname">pkgname</code></td>
<td>
<p>the package name</p>
</td></tr>
</table>

<hr>
<h2 id='build_hidden_encoder'>Build the encoder for a VAE</h2><span id='topic+build_hidden_encoder'></span>

<h3>Description</h3>

<p>Build the encoder for a VAE
</p>


<h3>Usage</h3>

<pre><code class='language-R'>build_hidden_encoder(
  input_size,
  layers,
  activations = rep("sigmoid", length(layers))
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="build_hidden_encoder_+3A_input_size">input_size</code></td>
<td>
<p>an integer representing the number of items</p>
</td></tr>
<tr><td><code id="build_hidden_encoder_+3A_layers">layers</code></td>
<td>
<p>a list of integers giving the size of each hidden layer</p>
</td></tr>
<tr><td><code id="build_hidden_encoder_+3A_activations">activations</code></td>
<td>
<p>a list of strings, the same length as layers</p>
</td></tr>
</table>


<h3>Value</h3>

<p>two tensors: the input layer to the VAE and the last hidden layer of the encoder
</p>

<hr>
<h2 id='build_vae_correlated'>Build a VAE that fits to a normal, full covariance N(m,S) latent distribution</h2><span id='topic+build_vae_correlated'></span>

<h3>Description</h3>

<p>Build a VAE that fits to a normal, full covariance N(m,S) latent distribution
</p>


<h3>Usage</h3>

<pre><code class='language-R'>build_vae_correlated(
  num_items,
  num_skills,
  Q_matrix,
  mean_vector = rep(0, num_skills),
  covariance_matrix = diag(num_skills),
  model_type = 2,
  enc_hid_arch = c(ceiling((num_items + num_skills)/2)),
  hid_enc_activations = rep("sigmoid", length(enc_hid_arch)),
  output_activation = "sigmoid",
  kl_weight = 1,
  learning_rate = 0.001
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="build_vae_correlated_+3A_num_items">num_items</code></td>
<td>
<p>an integer giving the number of items on the assessment; also the number of nodes in the input/output layers of the VAE</p>
</td></tr>
<tr><td><code id="build_vae_correlated_+3A_num_skills">num_skills</code></td>
<td>
<p>an integer giving the number of skills being evaluated; also the dimensionality of the distribution learned by the VAE</p>
</td></tr>
<tr><td><code id="build_vae_correlated_+3A_q_matrix">Q_matrix</code></td>
<td>
<p>a binary, <code>num_skills</code> by <code>num_items</code> matrix relating the assessment items with skills</p>
</td></tr>
<tr><td><code id="build_vae_correlated_+3A_mean_vector">mean_vector</code></td>
<td>
<p>a vector of length <code>num_skills</code> specifying the mean of each latent trait; the default of <code>rep(0, num_skills)</code> should almost always be used</p>
</td></tr>
<tr><td><code id="build_vae_correlated_+3A_covariance_matrix">covariance_matrix</code></td>
<td>
<p>a symmetric, positive definite, <code>num_skills</code> by <code>num_skills</code> matrix giving the covariance of the latent traits</p>
</td></tr>
<tr><td><code id="build_vae_correlated_+3A_model_type">model_type</code></td>
<td>
<p>either 1 or 2, specifying a 1 parameter (1PL) or 2 parameter (2PL) model; if 1PL, then all decoder weights are fixed to be equal to one</p>
</td></tr>
<tr><td><code id="build_vae_correlated_+3A_enc_hid_arch">enc_hid_arch</code></td>
<td>
<p>a vector detailing the size of hidden layers in the encoder; the number of hidden layers is determined by the length of this vector</p>
</td></tr>
<tr><td><code id="build_vae_correlated_+3A_hid_enc_activations">hid_enc_activations</code></td>
<td>
<p>a vector specifying the activation function in each hidden layer in the encoder; must be the same length as <code>enc_hid_arch</code></p>
</td></tr>
<tr><td><code id="build_vae_correlated_+3A_output_activation">output_activation</code></td>
<td>
<p>a string specifying the activation function in the output of the decoder; the ML2P model always used 'sigmoid'</p>
</td></tr>
<tr><td><code id="build_vae_correlated_+3A_kl_weight">kl_weight</code></td>
<td>
<p>an optional weight for the KL divergence term in the loss function</p>
</td></tr>
<tr><td><code id="build_vae_correlated_+3A_learning_rate">learning_rate</code></td>
<td>
<p>an optional parameter for the adam optimizer</p>
</td></tr>
</table>


<h3>Value</h3>

<p>returns three keras models: the encoder, decoder, and vae
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
Q &lt;- matrix(c(1,0,1,1,0,1,1,0), nrow = 2, ncol = 4)
cov &lt;- matrix(c(.7,.3,.3,1), nrow = 2, ncol = 2)
models &lt;- build_vae_correlated(4, 2, Q,
          mean_vector = c(-0.5, 0), covariance_matrix = cov,
          enc_hid_arch = c(6, 3), hid_enc_activation = c('sigmoid', 'relu'),
          output_activation = 'tanh',
          kl_weight = 0.1)
vae &lt;- models[[3]]

</code></pre>

<hr>
<h2 id='build_vae_independent'>Build a VAE that fits to a standard N(0,I) latent distribution with independent latent traits</h2><span id='topic+build_vae_independent'></span>

<h3>Description</h3>

<p>Build a VAE that fits to a standard N(0,I) latent distribution with independent latent traits
</p>


<h3>Usage</h3>

<pre><code class='language-R'>build_vae_independent(
  num_items,
  num_skills,
  Q_matrix,
  model_type = 2,
  enc_hid_arch = c(ceiling((num_items + num_skills)/2)),
  hid_enc_activations = rep("sigmoid", length(enc_hid_arch)),
  output_activation = "sigmoid",
  kl_weight = 1,
  learning_rate = 0.001
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="build_vae_independent_+3A_num_items">num_items</code></td>
<td>
<p>an integer giving the number of items on the assessment; also the number of nodes in the input/output layers of the VAE</p>
</td></tr>
<tr><td><code id="build_vae_independent_+3A_num_skills">num_skills</code></td>
<td>
<p>an integer giving the number of skills being evaluated; also the dimensionality of the distribution learned by the VAE</p>
</td></tr>
<tr><td><code id="build_vae_independent_+3A_q_matrix">Q_matrix</code></td>
<td>
<p>a binary, <code>num_skills</code> by <code>num_items</code> matrix relating the assessment items with skills</p>
</td></tr>
<tr><td><code id="build_vae_independent_+3A_model_type">model_type</code></td>
<td>
<p>either 1 or 2, specifying a 1 parameter (1PL) or 2 parameter (2PL) model; if 1PL, then all decoder weights are fixed to be equal to one</p>
</td></tr>
<tr><td><code id="build_vae_independent_+3A_enc_hid_arch">enc_hid_arch</code></td>
<td>
<p>a vector detailing the size of hidden layers in the encoder; the number of hidden layers is determined by the length of this vector</p>
</td></tr>
<tr><td><code id="build_vae_independent_+3A_hid_enc_activations">hid_enc_activations</code></td>
<td>
<p>a vector specifying the activation function in each hidden layer in the encoder; must be the same length as <code>enc_hid_arch</code></p>
</td></tr>
<tr><td><code id="build_vae_independent_+3A_output_activation">output_activation</code></td>
<td>
<p>a string specifying the activation function in the output of the decoder; the ML2P model always uses 'sigmoid'</p>
</td></tr>
<tr><td><code id="build_vae_independent_+3A_kl_weight">kl_weight</code></td>
<td>
<p>an optional weight for the KL divergence term in the loss function</p>
</td></tr>
<tr><td><code id="build_vae_independent_+3A_learning_rate">learning_rate</code></td>
<td>
<p>an optional parameter for the adam optimizer</p>
</td></tr>
</table>


<h3>Value</h3>

<p>returns three keras models: the encoder, decoder, and vae.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
Q &lt;- matrix(c(1,0,1,1,0,1,1,0), nrow = 2, ncol = 4)
models &lt;- build_vae_independent(4, 2, Q,
          enc_hid_arch = c(6, 3), hid_enc_activation = c('sigmoid', 'relu'),
          output_activation = 'tanh', kl_weight = 0.1)
models &lt;- build_vae_independent(4, 2, Q)
vae &lt;- models[[3]]

</code></pre>

<hr>
<h2 id='correlation_matrix'>Simulated latent abilities correlation matrix</h2><span id='topic+correlation_matrix'></span>

<h3>Description</h3>

<p>A symmetric positive definite matrix detailing the 
correlations among three latent traits.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>correlation_matrix
</code></pre>


<h3>Format</h3>

<p>A data frame with 3 rows and 3 columns
</p>


<h3>Source</h3>

<p>Generated using the python package SciPy
</p>

<hr>
<h2 id='diff_true'>Simulated difficulty parameters</h2><span id='topic+diff_true'></span>

<h3>Description</h3>

<p>Difficulty parameters for an exam with 30 items.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>diff_true
</code></pre>


<h3>Format</h3>

<p>A data frame with 30 rows and one column.
Each entry corresponds to the true value of a particular difficulty parameter.
</p>


<h3>Source</h3>

<p>Each entry is sampled uniformly from <code>[-3,3]</code>.
</p>

<hr>
<h2 id='disc_true'>Simulated discrimination parameters</h2><span id='topic+disc_true'></span>

<h3>Description</h3>

<p>Difficulty parameters for an exam of 30 items assessing 3 latent abilities.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>disc_true
</code></pre>


<h3>Format</h3>

<p>A data frame with 3 rows and 30 columns. Entry <code>[k,i]</code> represents the discrimination
parameter between item <code>i</code> and ability <code>k</code>.
</p>


<h3>Source</h3>

<p>Each entry is sampled uniformly from <code>[0.25,1.75]</code>.
If an entry in <code>q_matrix.rda</code> is 0, then so is the corresponding entry in <code>disc_true.rda</code>.
</p>

<hr>
<h2 id='get_ability_parameter_estimates'>Feed forward response sets through the encoder, which outputs student ability estimates</h2><span id='topic+get_ability_parameter_estimates'></span>

<h3>Description</h3>

<p>Feed forward response sets through the encoder, which outputs student ability estimates
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_ability_parameter_estimates(encoder, responses)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_ability_parameter_estimates_+3A_encoder">encoder</code></td>
<td>
<p>a trained keras model; should be the encoder returned from either <code>build_vae_independent()</code> or <code>build_vae_correlated</code></p>
</td></tr>
<tr><td><code id="get_ability_parameter_estimates_+3A_responses">responses</code></td>
<td>
<p>a <code>num_students</code> by <code>num_items</code> matrix of binary responses, as used in training</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list where the first entry contains student ability estimates and the second entry holds the variance (or covariance matrix) of those estimates
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data &lt;- matrix(c(1,1,0,0,1,0,1,1,0,1,1,0), nrow = 3, ncol = 4)
Q &lt;- matrix(c(1,0,1,1,0,1,1,0), nrow = 2, ncol = 4)
models &lt;- build_vae_independent(4, 2, Q, model_type = 2)
encoder &lt;- models[[1]]
ability_parameter_estimates_variances &lt;- get_ability_parameter_estimates(encoder, data)
student_ability_est &lt;- ability_parameter_estimates_variances[[1]]

</code></pre>

<hr>
<h2 id='get_item_parameter_estimates'>Get trainable variables from the decoder, which serve as item parameter estimates.</h2><span id='topic+get_item_parameter_estimates'></span>

<h3>Description</h3>

<p>Get trainable variables from the decoder, which serve as item parameter estimates.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_item_parameter_estimates(decoder, model_type = 2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_item_parameter_estimates_+3A_decoder">decoder</code></td>
<td>
<p>a trained keras model; can either be the decoder or vae returned from <code>build_vae_independent()</code> or <code>build_vae_correlated</code></p>
</td></tr>
<tr><td><code id="get_item_parameter_estimates_+3A_model_type">model_type</code></td>
<td>
<p>either 1 or 2, specifying a 1 parameter (1PL) or 2 parameter (2PL) model; if 1PL, then only the difficulty parameter estimates (output layer bias) will be returned; if 2PL, then the discrimination parameter estimates (output layer weights) will also be returned</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list which contains item parameter estimates; the length of this list is equal to model_type - the first entry in the list holds the difficulty parameter estimates, and the second entry (if 2PL) contains discrimination parameter estimates
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
Q &lt;- matrix(c(1,0,1,1,0,1,1,0), nrow = 2, ncol = 4)
models &lt;- build_vae_independent(4, 2, Q, model_type = 2)
decoder &lt;- models[[2]]
item_parameter_estimates &lt;- get_item_parameter_estimates(decoder, model_type = 2)
difficulty_est &lt;- item_parameter_estimates[[1]]
discrimination_est &lt;- item_parameter_estimates[[2]]

</code></pre>

<hr>
<h2 id='ML2Pvae'>ML2Pvae: A package for creating a VAE whose decoder recovers the parameters of the ML2P model.
The encoder can be used to predict the latent skills based on assessment scores.</h2><span id='topic+ML2Pvae'></span>

<h3>Description</h3>

<p>The ML2Pvae package includes functions which build a VAE with the desired architecture, and
fits the latent skills to either a standard normal (independent) distrubution,
or a multivariate normal distribution with a full covariance matrix. Based on the work 
&quot;Interpretable Variational Autoencdoers for Cognitive Models&quot;
by Curi, M., Converse, G., Hajewski, J., and Oliveira, S. 
Found in International Joint Conference on Neural Networks, 2019.
</p>

<hr>
<h2 id='q_1pl_constraint'>A custom kernel constraint function that forces nonzero weights to be equal to one, so the VAE will estimate the 1-parameter logistic model. Nonzero weights are determined by the Q matrix.</h2><span id='topic+q_1pl_constraint'></span>

<h3>Description</h3>

<p>A custom kernel constraint function that forces nonzero weights to be equal to one, so the VAE will estimate the 1-parameter logistic model. Nonzero weights are determined by the Q matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>q_1pl_constraint(Q)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="q_1pl_constraint_+3A_q">Q</code></td>
<td>
<p>a binary matrix of size <code>num_skills</code> by <code>num_items</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>returns a function whose parameters match keras kernel constraint format
</p>

<hr>
<h2 id='q_constraint'>A custom kernel constraint function that restricts weights between the learned distribution and output. Nonzero weights are determined by the Q matrix.</h2><span id='topic+q_constraint'></span>

<h3>Description</h3>

<p>A custom kernel constraint function that restricts weights between the learned distribution and output. Nonzero weights are determined by the Q matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>q_constraint(Q)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="q_constraint_+3A_q">Q</code></td>
<td>
<p>a binary matrix of size <code>num_skills</code> by <code>num_items</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>returns a function whose parameters match keras kernel constraint format
</p>

<hr>
<h2 id='q_matrix'>Simulated Q-matrix</h2><span id='topic+q_matrix'></span>

<h3>Description</h3>

<p>The Q-matrix determines the relation between items and abilities.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>q_matrix
</code></pre>


<h3>Format</h3>

<p>A data frame with 3 rows and 30 columns. If entry <code>[k,i] = 1</code>,
then item <code>i</code> requires skill <code>k</code>.
</p>


<h3>Source</h3>

<p>Generated by sampling each entry from <code>Bernoulli(0.35)</code>, but ensures
each item assess at least one latent ability
</p>

<hr>
<h2 id='responses'>Response data</h2><span id='topic+responses'></span>

<h3>Description</h3>

<p>Simulated response sets for 5000 students on an exam with 30 items.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>responses
</code></pre>


<h3>Format</h3>

<p>A data frame with 30 columns and 5000 rows.
Entry <code>[j,i]</code> is 1 if student <code>j</code> answers item <code>i</code> correctly, and 0 otherwise.
</p>


<h3>Source</h3>

<p>Generated by sampling from the probability of student success
on a given item according to the ML2P model. Model parameters can be found in
<code>diff_true.rda</code>, <code>disc_true.rda</code>, and <code>theta_true.rda</code>.
</p>

<hr>
<h2 id='sampling_correlated'>A reparameterization in order to sample from the learned multivariate normal distribution of the VAE</h2><span id='topic+sampling_correlated'></span>

<h3>Description</h3>

<p>A reparameterization in order to sample from the learned multivariate normal distribution of the VAE
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sampling_correlated(arg)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sampling_correlated_+3A_arg">arg</code></td>
<td>
<p>a layer of tensors representing the mean and log cholesky transform of the covariance matrix</p>
</td></tr>
</table>

<hr>
<h2 id='sampling_independent'>A reparameterization in order to sample from the learned standard normal distribution of the VAE</h2><span id='topic+sampling_independent'></span>

<h3>Description</h3>

<p>A reparameterization in order to sample from the learned standard normal distribution of the VAE
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sampling_independent(arg)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sampling_independent_+3A_arg">arg</code></td>
<td>
<p>a layer of tensors representing the mean and variance</p>
</td></tr>
</table>

<hr>
<h2 id='theta_true'>Simulated ability parameters</h2><span id='topic+theta_true'></span>

<h3>Description</h3>

<p>Three correlated ability parameters for 5000 students.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>theta_true
</code></pre>


<h3>Format</h3>

<p>A data frame with 5000 rows and 3 columns. Each row represents a particular
student's three latent abilities.
</p>


<h3>Source</h3>

<p>Generated by sampling from a 3-dimensional multivariate Gaussian distribution
with mean 0 and covariance matrix <code>correlation_matrix.rda</code>.
</p>

<hr>
<h2 id='train_model'>Trains a VAE or autoencoder model. This acts as a wrapper for keras::fit().</h2><span id='topic+train_model'></span>

<h3>Description</h3>

<p>Trains a VAE or autoencoder model. This acts as a wrapper for keras::fit().
</p>


<h3>Usage</h3>

<pre><code class='language-R'>train_model(
  model,
  train_data,
  num_epochs = 10,
  batch_size = 1,
  validation_split = 0.15,
  shuffle = FALSE,
  verbose = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="train_model_+3A_model">model</code></td>
<td>
<p>the keras model to be trained; this should be the vae returned from <code>build_vae_independent()</code> or <code>build_vae_correlated</code></p>
</td></tr>
<tr><td><code id="train_model_+3A_train_data">train_data</code></td>
<td>
<p>training data; this should be a binary <code>num_students</code> by <code>num_items</code> matrix of student responses to an assessment</p>
</td></tr>
<tr><td><code id="train_model_+3A_num_epochs">num_epochs</code></td>
<td>
<p>number of epochs to train for</p>
</td></tr>
<tr><td><code id="train_model_+3A_batch_size">batch_size</code></td>
<td>
<p>batch size for mini-batch stochastic gradient descent; default is 1, detailing pure SGD; if a larger batch size is used (e.g. 32), then a larger number of epochs should be set (e.g. 50)</p>
</td></tr>
<tr><td><code id="train_model_+3A_validation_split">validation_split</code></td>
<td>
<p>split percentage to use as validation data</p>
</td></tr>
<tr><td><code id="train_model_+3A_shuffle">shuffle</code></td>
<td>
<p>whether or not to shuffle data</p>
</td></tr>
<tr><td><code id="train_model_+3A_verbose">verbose</code></td>
<td>
<p>verbosity levels; 0 = silent; 1 = progress bar and epoch message; 2 = epoch message</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list containing training history; this holds the loss from each epoch which can be plotted
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data &lt;- matrix(c(1,1,0,0,1,0,1,1,0,1,1,0), nrow = 3, ncol = 4)
Q &lt;- matrix(c(1,0,1,1,0,1,1,0), nrow = 2, ncol = 4)
models &lt;- build_vae_independent(4, 2, Q)
vae &lt;- models[[3]]
history &lt;- train_model(vae, data, num_epochs = 3, validation_split = 0, verbose = 0)
plot(history)

</code></pre>

<hr>
<h2 id='vae_loss_correlated'>A custom loss function for a VAE learning a multivariate normal distribution with a full covariance matrix</h2><span id='topic+vae_loss_correlated'></span>

<h3>Description</h3>

<p>A custom loss function for a VAE learning a multivariate normal distribution with a full covariance matrix
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vae_loss_correlated(
  encoder,
  inv_skill_cov,
  det_skill_cov,
  skill_mean,
  kl_weight,
  rec_dim
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vae_loss_correlated_+3A_encoder">encoder</code></td>
<td>
<p>the encoder model of the VAE, used to obtain z_mean and z_log_cholesky from inputs</p>
</td></tr>
<tr><td><code id="vae_loss_correlated_+3A_inv_skill_cov">inv_skill_cov</code></td>
<td>
<p>a constant tensor matrix of the inverse of the covariance matrix being learned</p>
</td></tr>
<tr><td><code id="vae_loss_correlated_+3A_det_skill_cov">det_skill_cov</code></td>
<td>
<p>a constant tensor scalar representing the determinant of the covariance matrix being learned</p>
</td></tr>
<tr><td><code id="vae_loss_correlated_+3A_skill_mean">skill_mean</code></td>
<td>
<p>a constant tensor vector representing the means of the latent skills being learned</p>
</td></tr>
<tr><td><code id="vae_loss_correlated_+3A_kl_weight">kl_weight</code></td>
<td>
<p>weight for the KL divergence term</p>
</td></tr>
<tr><td><code id="vae_loss_correlated_+3A_rec_dim">rec_dim</code></td>
<td>
<p>the number of nodes in the input/output of the VAE</p>
</td></tr>
</table>


<h3>Value</h3>

<p>returns a function whose parameters match keras loss format
</p>

<hr>
<h2 id='vae_loss_independent'>A custom loss function for a VAE learning a standard normal distribution</h2><span id='topic+vae_loss_independent'></span>

<h3>Description</h3>

<p>A custom loss function for a VAE learning a standard normal distribution
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vae_loss_independent(encoder, kl_weight, rec_dim)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vae_loss_independent_+3A_encoder">encoder</code></td>
<td>
<p>the encoder model of the VAE, used to obtain z_mean and z_log_var from inputs</p>
</td></tr>
<tr><td><code id="vae_loss_independent_+3A_kl_weight">kl_weight</code></td>
<td>
<p>weight for the KL divergence term</p>
</td></tr>
<tr><td><code id="vae_loss_independent_+3A_rec_dim">rec_dim</code></td>
<td>
<p>the number of nodes in the input/output of the VAE</p>
</td></tr>
</table>


<h3>Value</h3>

<p>returns a function whose parameters match keras loss format
</p>

<hr>
<h2 id='validate_inputs'>Give error messages for invalid inputs in exported functions.</h2><span id='topic+validate_inputs'></span>

<h3>Description</h3>

<p>Give error messages for invalid inputs in exported functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>validate_inputs(
  num_items,
  num_skills,
  Q_matrix,
  model_type = 2,
  mean_vector = rep(0, num_skills),
  covariance_matrix = diag(num_skills),
  enc_hid_arch = c(ceiling((num_items + num_skills)/2)),
  hid_enc_activations = rep("sigmoid", length(enc_hid_arch)),
  output_activation = "sigmoid",
  kl_weight = 1,
  learning_rate = 0.001
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="validate_inputs_+3A_num_items">num_items</code></td>
<td>
<p>the number of items on the assessment; also the number of nodes in the input/output layers of the VAE</p>
</td></tr>
<tr><td><code id="validate_inputs_+3A_num_skills">num_skills</code></td>
<td>
<p>the number of skills being evaluated; also the size of the distribution learned by the VAE</p>
</td></tr>
<tr><td><code id="validate_inputs_+3A_q_matrix">Q_matrix</code></td>
<td>
<p>a binary, <code>num_skills</code> by <code>num_items</code> matrix relating the assessment items with skills</p>
</td></tr>
<tr><td><code id="validate_inputs_+3A_model_type">model_type</code></td>
<td>
<p>either 1 or 2, specifying a 1 parameter (1PL) or 2 parameter (2PL) model</p>
</td></tr>
<tr><td><code id="validate_inputs_+3A_mean_vector">mean_vector</code></td>
<td>
<p>a vector of length <code>num_skills</code> specifying the mean of each latent trait</p>
</td></tr>
<tr><td><code id="validate_inputs_+3A_covariance_matrix">covariance_matrix</code></td>
<td>
<p>a symmetric, positive definite, <code>num_skills</code> by <code>num_skills</code>, matrix giving the covariance of the latent traits</p>
</td></tr>
<tr><td><code id="validate_inputs_+3A_enc_hid_arch">enc_hid_arch</code></td>
<td>
<p>a vector detailing the number an size of hidden layers in the encoder</p>
</td></tr>
<tr><td><code id="validate_inputs_+3A_hid_enc_activations">hid_enc_activations</code></td>
<td>
<p>a vector specifying the activation function in each hidden layer in the encoder; must be the same length as <code>enc_hid_arch</code></p>
</td></tr>
<tr><td><code id="validate_inputs_+3A_output_activation">output_activation</code></td>
<td>
<p>a string specifying the activation function in the output of the decoder; the ML2P model alsways used 'sigmoid'</p>
</td></tr>
<tr><td><code id="validate_inputs_+3A_kl_weight">kl_weight</code></td>
<td>
<p>an optional weight for the KL divergence term in the loss function</p>
</td></tr>
<tr><td><code id="validate_inputs_+3A_learning_rate">learning_rate</code></td>
<td>
<p>an optional parameter for the adam optimizer</p>
</td></tr>
</table>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
