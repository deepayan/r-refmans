<!DOCTYPE html><html><head><title>Help for package kernelshap</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {kernelshap}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#is.kernelshap'><p>Check for kernelshap</p></a></li>
<li><a href='#is.permshap'><p>Check for permshap</p></a></li>
<li><a href='#kernelshap'><p>Kernel SHAP</p></a></li>
<li><a href='#permshap'><p>Permutation SHAP</p></a></li>
<li><a href='#print.kernelshap'><p>Prints &quot;kernelshap&quot; Object</p></a></li>
<li><a href='#print.permshap'><p>Prints &quot;permshap&quot; Object</p></a></li>
<li><a href='#summary.kernelshap'><p>Summarizes &quot;kernelshap&quot; Object</p></a></li>
<li><a href='#summary.permshap'><p>Summarizes &quot;permshap&quot; Object</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Kernel SHAP</td>
</tr>
<tr>
<td>Version:</td>
<td>0.4.1</td>
</tr>
<tr>
<td>Description:</td>
<td>Efficient implementation of Kernel SHAP, see Lundberg and Lee
    (2017), and Covert and Lee (2021)
    <a href="http://proceedings.mlr.press/v130/covert21a">http://proceedings.mlr.press/v130/covert21a</a>.  Furthermore, for up to
    14 features, exact permutation SHAP values can be calculated.  The
    package plays well together with meta-learning packages like
    'tidymodels', 'caret' or 'mlr3'.  Visualizations can be done using the
    R package 'shapviz'.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.2.0)</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Imports:</td>
<td>foreach, stats, utils</td>
</tr>
<tr>
<td>Suggests:</td>
<td>doFuture, testthat (&ge; 3.0.0)</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/ModelOriented/kernelshap">https://github.com/ModelOriented/kernelshap</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/ModelOriented/kernelshap/issues">https://github.com/ModelOriented/kernelshap/issues</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-12-03 14:00:05 UTC; Michael</td>
</tr>
<tr>
<td>Author:</td>
<td>Michael Mayer [aut, cre],
  David Watson [aut],
  Przemyslaw Biecek <a href="https://orcid.org/0000-0001-8423-1823"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Michael Mayer &lt;mayermichael79@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-12-03 14:20:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='is.kernelshap'>Check for kernelshap</h2><span id='topic+is.kernelshap'></span>

<h3>Description</h3>

<p>Is object of class &quot;kernelshap&quot;?
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.kernelshap(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="is.kernelshap_+3A_object">object</code></td>
<td>
<p>An R object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>TRUE</code> if <code>object</code> is of class &quot;kernelshap&quot;, and <code>FALSE</code> otherwise.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+kernelshap">kernelshap()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>fit &lt;- lm(Sepal.Length ~ ., data = iris)
s &lt;- kernelshap(fit, iris[1:2, -1], bg_X = iris[, -1])
is.kernelshap(s)
is.kernelshap("a")
</code></pre>

<hr>
<h2 id='is.permshap'>Check for permshap</h2><span id='topic+is.permshap'></span>

<h3>Description</h3>

<p>Is object of class &quot;permshap&quot;?
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.permshap(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="is.permshap_+3A_object">object</code></td>
<td>
<p>An R object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>TRUE</code> if <code>object</code> is of class &quot;permshap&quot;, and <code>FALSE</code> otherwise.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+kernelshap">kernelshap()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>fit &lt;- lm(Sepal.Length ~ ., data = iris)
s &lt;- permshap(fit, iris[1:2, -1], bg_X = iris[, -1])
is.permshap(s)
is.permshap("a")
</code></pre>

<hr>
<h2 id='kernelshap'>Kernel SHAP</h2><span id='topic+kernelshap'></span><span id='topic+kernelshap.default'></span><span id='topic+kernelshap.ranger'></span><span id='topic+kernelshap.Learner'></span>

<h3>Description</h3>

<p>Efficient implementation of Kernel SHAP, see Lundberg and Lee (2017), and
Covert and Lee (2021), abbreviated by CL21.
For up to <code class="reqn">p=8</code> features, the resulting Kernel SHAP values are exact regarding
the selected background data. For larger <code class="reqn">p</code>, an almost exact
hybrid algorithm involving iterative sampling is used, see Details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kernelshap(object, ...)

## Default S3 method:
kernelshap(
  object,
  X,
  bg_X,
  pred_fun = stats::predict,
  feature_names = colnames(X),
  bg_w = NULL,
  exact = length(feature_names) &lt;= 8L,
  hybrid_degree = 1L + length(feature_names) %in% 4:16,
  paired_sampling = TRUE,
  m = 2L * length(feature_names) * (1L + 3L * (hybrid_degree == 0L)),
  tol = 0.005,
  max_iter = 100L,
  parallel = FALSE,
  parallel_args = NULL,
  verbose = TRUE,
  ...
)

## S3 method for class 'ranger'
kernelshap(
  object,
  X,
  bg_X,
  pred_fun = function(m, X, ...) stats::predict(m, X, ...)$predictions,
  feature_names = colnames(X),
  bg_w = NULL,
  exact = length(feature_names) &lt;= 8L,
  hybrid_degree = 1L + length(feature_names) %in% 4:16,
  paired_sampling = TRUE,
  m = 2L * length(feature_names) * (1L + 3L * (hybrid_degree == 0L)),
  tol = 0.005,
  max_iter = 100L,
  parallel = FALSE,
  parallel_args = NULL,
  verbose = TRUE,
  ...
)

## S3 method for class 'Learner'
kernelshap(
  object,
  X,
  bg_X,
  pred_fun = NULL,
  feature_names = colnames(X),
  bg_w = NULL,
  exact = length(feature_names) &lt;= 8L,
  hybrid_degree = 1L + length(feature_names) %in% 4:16,
  paired_sampling = TRUE,
  m = 2L * length(feature_names) * (1L + 3L * (hybrid_degree == 0L)),
  tol = 0.005,
  max_iter = 100L,
  parallel = FALSE,
  parallel_args = NULL,
  verbose = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kernelshap_+3A_object">object</code></td>
<td>
<p>Fitted model object.</p>
</td></tr>
<tr><td><code id="kernelshap_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code>pred_fun(object, X, ...)</code>.</p>
</td></tr>
<tr><td><code id="kernelshap_+3A_x">X</code></td>
<td>
<p><code class="reqn">(n \times p)</code> matrix or <code>data.frame</code> with rows to be explained.
The columns should only represent model features, not the response
(but see <code>feature_names</code> on how to overrule this).</p>
</td></tr>
<tr><td><code id="kernelshap_+3A_bg_x">bg_X</code></td>
<td>
<p>Background data used to integrate out &quot;switched off&quot; features,
often a subset of the training data (typically 50 to 500 rows)
It should contain the same columns as <code>X</code>.
In cases with a natural &quot;off&quot; value (like MNIST digits),
this can also be a single row with all values set to the off value.</p>
</td></tr>
<tr><td><code id="kernelshap_+3A_pred_fun">pred_fun</code></td>
<td>
<p>Prediction function of the form <code style="white-space: pre;">&#8288;function(object, X, ...)&#8288;</code>,
providing <code class="reqn">K \ge 1</code> predictions per row. Its first argument
represents the model <code>object</code>, its second argument a data structure like <code>X</code>.
Additional (named) arguments are passed via <code>...</code>.
The default, <code><a href="stats.html#topic+predict">stats::predict()</a></code>, will work in most cases.</p>
</td></tr>
<tr><td><code id="kernelshap_+3A_feature_names">feature_names</code></td>
<td>
<p>Optional vector of column names in <code>X</code> used to calculate
SHAP values. By default, this equals <code>colnames(X)</code>. Not supported if <code>X</code>
is a matrix.</p>
</td></tr>
<tr><td><code id="kernelshap_+3A_bg_w">bg_w</code></td>
<td>
<p>Optional vector of case weights for each row of <code>bg_X</code>.</p>
</td></tr>
<tr><td><code id="kernelshap_+3A_exact">exact</code></td>
<td>
<p>If <code>TRUE</code>, the algorithm will produce exact Kernel SHAP values
with respect to the background data. In this case, the arguments <code>hybrid_degree</code>,
<code>m</code>, <code>paired_sampling</code>, <code>tol</code>, and <code>max_iter</code> are ignored.
The default is <code>TRUE</code> up to eight features, and <code>FALSE</code> otherwise.</p>
</td></tr>
<tr><td><code id="kernelshap_+3A_hybrid_degree">hybrid_degree</code></td>
<td>
<p>Integer controlling the exactness of the hybrid strategy. For
<code class="reqn">4 \le p \le 16</code>, the default is 2, otherwise it is 1.
Ignored if <code>exact = TRUE</code>.
</p>

<ul>
<li> <p><code>0</code>: Pure sampling strategy not involving any exact part. It is strictly
worse than the hybrid strategy and should therefore only be used for
studying properties of the Kernel SHAP algorithm.
</p>
</li>
<li> <p><code>1</code>: Uses all <code class="reqn">2p</code> on-off vectors <code class="reqn">z</code> with <code class="reqn">\sum z \in \{1, p-1\}</code>
for the exact part, which covers at least 75% of the mass of the Kernel weight
distribution. The remaining mass is covered by random sampling.
</p>
</li>
<li> <p><code>2</code>: Uses all <code class="reqn">p(p+1)</code> on-off vectors <code class="reqn">z</code> with
<code class="reqn">\sum z \in \{1, 2, p-2, p-1\}</code>. This covers at least 92% of the mass of the
Kernel weight distribution. The remaining mass is covered by sampling.
Convergence usually happens in the minimal possible number of iterations of two.
</p>
</li>
<li> <p><code>k&gt;2</code>: Uses all on-off vectors with
<code class="reqn">\sum z \in \{1, \dots, k, p-k, \dots, p-1\}</code>.
</p>
</li></ul>
</td></tr>
<tr><td><code id="kernelshap_+3A_paired_sampling">paired_sampling</code></td>
<td>
<p>Logical flag indicating whether to do the sampling in a paired
manner. This means that with every on-off vector <code class="reqn">z</code>, also <code class="reqn">1-z</code> is
considered. CL21 shows its superiority compared to standard sampling, therefore the
default (<code>TRUE</code>) should usually not be changed except for studying properties
of Kernel SHAP algorithms. Ignored if <code>exact = TRUE</code>.</p>
</td></tr>
<tr><td><code id="kernelshap_+3A_m">m</code></td>
<td>
<p>Even number of on-off vectors sampled during one iteration.
The default is <code class="reqn">2p</code>, except when <code>hybrid_degree == 0</code>.
Then it is set to <code class="reqn">8p</code>. Ignored if <code>exact = TRUE</code>.</p>
</td></tr>
<tr><td><code id="kernelshap_+3A_tol">tol</code></td>
<td>
<p>Tolerance determining when to stop. Following CL21, the algorithm keeps
iterating until <code class="reqn">\textrm{max}(\sigma_n)/(\textrm{max}(\beta_n) - \textrm{min}(\beta_n)) &lt; \textrm{tol}</code>,
where the <code class="reqn">\beta_n</code> are the SHAP values of a given observation,
and <code class="reqn">\sigma_n</code> their standard errors.
For multidimensional predictions, the criterion must be satisfied for each
dimension separately. The stopping criterion uses the fact that standard errors
and SHAP values are all on the same scale. Ignored if <code>exact = TRUE</code>.</p>
</td></tr>
<tr><td><code id="kernelshap_+3A_max_iter">max_iter</code></td>
<td>
<p>If the stopping criterion (see <code>tol</code>) is not reached after
<code>max_iter</code> iterations, the algorithm stops. Ignored if <code>exact = TRUE</code>.</p>
</td></tr>
<tr><td><code id="kernelshap_+3A_parallel">parallel</code></td>
<td>
<p>If <code>TRUE</code>, use parallel <code><a href="foreach.html#topic+foreach">foreach::foreach()</a></code> to loop over rows
to be explained. Must register backend beforehand, e.g., via 'doFuture' package,
see README for an example. Parallelization automatically disables the progress bar.</p>
</td></tr>
<tr><td><code id="kernelshap_+3A_parallel_args">parallel_args</code></td>
<td>
<p>Named list of arguments passed to <code><a href="foreach.html#topic+foreach">foreach::foreach()</a></code>.
Ideally, this is <code>NULL</code> (default). Only relevant if <code>parallel = TRUE</code>.
Example on Windows: if <code>object</code> is a GAM fitted with package 'mgcv',
then one might need to set <code>parallel_args = list(.packages = "mgcv")</code>.</p>
</td></tr>
<tr><td><code id="kernelshap_+3A_verbose">verbose</code></td>
<td>
<p>Set to <code>FALSE</code> to suppress messages and the progress bar.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Pure iterative Kernel SHAP sampling as in Covert and Lee (2021) works like this:
</p>

<ol>
<li><p> A binary &quot;on-off&quot; vector <code class="reqn">z</code> is drawn from <code class="reqn">\{0, 1\}^p</code>
such that its sum follows the SHAP Kernel weight distribution
(normalized to the range <code class="reqn">\{1, \dots, p-1\}</code>).
</p>
</li>
<li><p> For each <code class="reqn">j</code> with <code class="reqn">z_j = 1</code>, the <code class="reqn">j</code>-th column of the
original background data is replaced by the corresponding feature value <code class="reqn">x_j</code>
of the observation to be explained.
</p>
</li>
<li><p> The average prediction <code class="reqn">v_z</code> on the data of Step 2 is calculated, and the
average prediction <code class="reqn">v_0</code> on the background data is subtracted.
</p>
</li>
<li><p> Steps 1 to 3 are repeated <code class="reqn">m</code> times. This produces a binary <code class="reqn">m \times p</code>
matrix <code class="reqn">Z</code> (each row equals one of the <code class="reqn">z</code>) and a vector <code class="reqn">v</code> of
shifted predictions.
</p>
</li>
<li> <p><code class="reqn">v</code> is regressed onto <code class="reqn">Z</code> under the constraint that the sum of the
coefficients equals <code class="reqn">v_1 - v_0</code>, where <code class="reqn">v_1</code> is the prediction of the
observation to be explained. The resulting coefficients are the Kernel SHAP values.
</p>
</li></ol>

<p>This is repeated multiple times until convergence, see CL21 for details.
</p>
<p>A drawback of this strategy is that many (at least 75%) of the <code class="reqn">z</code> vectors will
have <code class="reqn">\sum z \in \{1, p-1\}</code>, producing many duplicates. Similarly, at least 92%
of the mass will be used for the <code class="reqn">p(p+1)</code> possible vectors with
<code class="reqn">\sum z \in \{1, 2, p-2, p-1\}</code>.
This inefficiency can be fixed by a hybrid strategy, combining exact calculations
with sampling.
</p>
<p>The hybrid algorithm has two steps:
</p>

<ol>
<li><p> Step 1 (exact part): There are <code class="reqn">2p</code> different on-off vectors <code class="reqn">z</code> with
<code class="reqn">\sum z \in \{1, p-1\}</code>, covering a large proportion of the Kernel SHAP
distribution. The degree 1 hybrid will list those vectors and use them according
to their weights in the upcoming calculations. Depending on <code class="reqn">p</code>, we can also go
a step further to a degree 2 hybrid by adding all <code class="reqn">p(p-1)</code> vectors with
<code class="reqn">\sum z \in \{2, p-2\}</code> to the process etc. The necessary predictions are
obtained along with other calculations similar to those described in CL21.
</p>
</li>
<li><p> Step 2 (sampling part): The remaining weight is filled by sampling vectors z
according to Kernel SHAP weights renormalized to the values not yet covered by Step 1.
Together with the results from Step 1 - correctly weighted - this now forms a
complete iteration as in CL21. The difference is that most mass is covered by exact
calculations. Afterwards, the algorithm iterates until convergence.
The output of Step 1 is reused in every iteration, leading to an extremely
efficient strategy.
</p>
</li></ol>

<p>If <code class="reqn">p</code> is sufficiently small, all possible <code class="reqn">2^p-2</code> on-off vectors <code class="reqn">z</code> can be
evaluated. In this case, no sampling is required and the algorithm returns exact
Kernel SHAP values with respect to the given background data.
Since <code><a href="#topic+kernelshap">kernelshap()</a></code> calculates predictions on data with <code class="reqn">MN</code> rows
(<code class="reqn">N</code> is the background data size and <code class="reqn">M</code> the number of <code class="reqn">z</code> vectors), <code class="reqn">p</code>
should not be much higher than 10 for exact calculations.
For similar reasons, degree 2 hybrids should not use <code class="reqn">p</code> much larger than 40.
</p>


<h3>Value</h3>

<p>An object of class &quot;kernelshap&quot; with the following components:
</p>

<ul>
<li> <p><code>S</code>: <code class="reqn">(n \times p)</code> matrix with SHAP values or, if the model output has
dimension <code class="reqn">K &gt; 1</code>, a list of <code class="reqn">K</code> such matrices.
</p>
</li>
<li> <p><code>X</code>: Same as input argument <code>X</code>.
</p>
</li>
<li> <p><code>baseline</code>: Vector of length K representing the average prediction on the
background data.
</p>
</li>
<li> <p><code>SE</code>: Standard errors corresponding to <code>S</code> (and organized like <code>S</code>).
</p>
</li>
<li> <p><code>n_iter</code>: Integer vector of length n providing the number of iterations
per row of <code>X</code>.
</p>
</li>
<li> <p><code>converged</code>: Logical vector of length n indicating convergence per row of <code>X</code>.
</p>
</li>
<li> <p><code>m</code>: Integer providing the effective number of sampled on-off vectors used
per iteration.
</p>
</li>
<li> <p><code>m_exact</code>: Integer providing the effective number of exact on-off vectors used
per iteration.
</p>
</li>
<li> <p><code>prop_exact</code>: Proportion of the Kernel SHAP weight distribution covered by
exact calculations.
</p>
</li>
<li> <p><code>exact</code>: Logical flag indicating whether calculations are exact or not.
</p>
</li>
<li> <p><code>txt</code>: Summary text.
</p>
</li>
<li> <p><code>predictions</code>: <code class="reqn">(n \times K)</code> matrix with predictions of <code>X</code>.
</p>
</li></ul>



<h3>Methods (by class)</h3>


<ul>
<li> <p><code>kernelshap(default)</code>: Default Kernel SHAP method.
</p>
</li>
<li> <p><code>kernelshap(ranger)</code>: Kernel SHAP method for &quot;ranger&quot; models, see Readme for an example.
</p>
</li>
<li> <p><code>kernelshap(Learner)</code>: Kernel SHAP method for &quot;mlr3&quot; models, see Readme for an example.
</p>
</li></ul>


<h3>References</h3>


<ol>
<li><p> Scott M. Lundberg and Su-In Lee. A unified approach to interpreting model
predictions. Proceedings of the 31st International Conference on Neural
Information Processing Systems, 2017.
</p>
</li>
<li><p> Ian Covert and Su-In Lee. Improving KernelSHAP: Practical Shapley Value
Estimation Using Linear Regression. Proceedings of The 24th International
Conference on Artificial Intelligence and Statistics, PMLR 130:3457-3465, 2021.
</p>
</li></ol>



<h3>Examples</h3>

<pre><code class='language-R'># MODEL ONE: Linear regression
fit &lt;- lm(Sepal.Length ~ ., data = iris)

# Select rows to explain (only feature columns)
X_explain &lt;- iris[1:2, -1]

# Select small background dataset (could use all rows here because iris is small) 
set.seed(1)
bg_X &lt;- iris[sample(nrow(iris), 100), ]

# Calculate SHAP values
s &lt;- kernelshap(fit, X_explain, bg_X = bg_X)
s

# MODEL TWO: Multi-response linear regression
fit &lt;- lm(as.matrix(iris[, 1:2]) ~ Petal.Length + Petal.Width + Species, data = iris)
s &lt;- kernelshap(fit, iris[1:4, 3:5], bg_X = bg_X)
summary(s)

# Non-feature columns can be dropped via 'feature_names'
s &lt;- kernelshap(
  fit, 
  iris[1:4, ],
  bg_X = bg_X, 
  feature_names = c("Petal.Length", "Petal.Width", "Species")
)
s
</code></pre>

<hr>
<h2 id='permshap'>Permutation SHAP</h2><span id='topic+permshap'></span><span id='topic+permshap.default'></span><span id='topic+permshap.ranger'></span><span id='topic+permshap.Learner'></span>

<h3>Description</h3>

<p>Exact permutation SHAP algorithm with respect to a background dataset,
see Strumbelj and Kononenko. The function works for up to 14 features.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>permshap(object, ...)

## Default S3 method:
permshap(
  object,
  X,
  bg_X,
  pred_fun = stats::predict,
  feature_names = colnames(X),
  bg_w = NULL,
  parallel = FALSE,
  parallel_args = NULL,
  verbose = TRUE,
  ...
)

## S3 method for class 'ranger'
permshap(
  object,
  X,
  bg_X,
  pred_fun = function(m, X, ...) stats::predict(m, X, ...)$predictions,
  feature_names = colnames(X),
  bg_w = NULL,
  parallel = FALSE,
  parallel_args = NULL,
  verbose = TRUE,
  ...
)

## S3 method for class 'Learner'
permshap(
  object,
  X,
  bg_X,
  pred_fun = NULL,
  feature_names = colnames(X),
  bg_w = NULL,
  parallel = FALSE,
  parallel_args = NULL,
  verbose = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="permshap_+3A_object">object</code></td>
<td>
<p>Fitted model object.</p>
</td></tr>
<tr><td><code id="permshap_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code>pred_fun(object, X, ...)</code>.</p>
</td></tr>
<tr><td><code id="permshap_+3A_x">X</code></td>
<td>
<p><code class="reqn">(n \times p)</code> matrix or <code>data.frame</code> with rows to be explained.
The columns should only represent model features, not the response
(but see <code>feature_names</code> on how to overrule this).</p>
</td></tr>
<tr><td><code id="permshap_+3A_bg_x">bg_X</code></td>
<td>
<p>Background data used to integrate out &quot;switched off&quot; features,
often a subset of the training data (typically 50 to 500 rows)
It should contain the same columns as <code>X</code>.
In cases with a natural &quot;off&quot; value (like MNIST digits),
this can also be a single row with all values set to the off value.</p>
</td></tr>
<tr><td><code id="permshap_+3A_pred_fun">pred_fun</code></td>
<td>
<p>Prediction function of the form <code style="white-space: pre;">&#8288;function(object, X, ...)&#8288;</code>,
providing <code class="reqn">K \ge 1</code> predictions per row. Its first argument
represents the model <code>object</code>, its second argument a data structure like <code>X</code>.
Additional (named) arguments are passed via <code>...</code>.
The default, <code><a href="stats.html#topic+predict">stats::predict()</a></code>, will work in most cases.</p>
</td></tr>
<tr><td><code id="permshap_+3A_feature_names">feature_names</code></td>
<td>
<p>Optional vector of column names in <code>X</code> used to calculate
SHAP values. By default, this equals <code>colnames(X)</code>. Not supported if <code>X</code>
is a matrix.</p>
</td></tr>
<tr><td><code id="permshap_+3A_bg_w">bg_w</code></td>
<td>
<p>Optional vector of case weights for each row of <code>bg_X</code>.</p>
</td></tr>
<tr><td><code id="permshap_+3A_parallel">parallel</code></td>
<td>
<p>If <code>TRUE</code>, use parallel <code><a href="foreach.html#topic+foreach">foreach::foreach()</a></code> to loop over rows
to be explained. Must register backend beforehand, e.g., via 'doFuture' package,
see README for an example. Parallelization automatically disables the progress bar.</p>
</td></tr>
<tr><td><code id="permshap_+3A_parallel_args">parallel_args</code></td>
<td>
<p>Named list of arguments passed to <code><a href="foreach.html#topic+foreach">foreach::foreach()</a></code>.
Ideally, this is <code>NULL</code> (default). Only relevant if <code>parallel = TRUE</code>.
Example on Windows: if <code>object</code> is a GAM fitted with package 'mgcv',
then one might need to set <code>parallel_args = list(.packages = "mgcv")</code>.</p>
</td></tr>
<tr><td><code id="permshap_+3A_verbose">verbose</code></td>
<td>
<p>Set to <code>FALSE</code> to suppress messages and the progress bar.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class &quot;permshap&quot; with the following components:
</p>

<ul>
<li> <p><code>S</code>: <code class="reqn">(n \times p)</code> matrix with SHAP values or, if the model output has
dimension <code class="reqn">K &gt; 1</code>, a list of <code class="reqn">K</code> such matrices.
</p>
</li>
<li> <p><code>X</code>: Same as input argument <code>X</code>.
</p>
</li>
<li> <p><code>baseline</code>: Vector of length K representing the average prediction on the
background data.
</p>
</li>
<li> <p><code>m_exact</code>: Integer providing the effective number of exact on-off vectors used.
</p>
</li>
<li> <p><code>exact</code>: Logical flag indicating whether calculations are exact or not
(currently <code>TRUE</code>).
</p>
</li>
<li> <p><code>txt</code>: Summary text.
</p>
</li>
<li> <p><code>predictions</code>: <code class="reqn">(n \times K)</code> matrix with predictions of <code>X</code>.
</p>
</li></ul>



<h3>Methods (by class)</h3>


<ul>
<li> <p><code>permshap(default)</code>: Default permutation SHAP method.
</p>
</li>
<li> <p><code>permshap(ranger)</code>: Permutation SHAP method for &quot;ranger&quot; models, see Readme for an example.
</p>
</li>
<li> <p><code>permshap(Learner)</code>: Permutation SHAP method for &quot;mlr3&quot; models, see Readme for an example.
</p>
</li></ul>


<h3>References</h3>


<ol>
<li><p> Erik Strumbelj and Igor Kononenko. Explaining prediction models and individual
predictions with feature contributions. Knowledge and Information Systems 41, 2014.
</p>
</li></ol>



<h3>Examples</h3>

<pre><code class='language-R'># MODEL ONE: Linear regression
fit &lt;- lm(Sepal.Length ~ ., data = iris)

# Select rows to explain (only feature columns)
X_explain &lt;- iris[1:2, -1]

# Select small background dataset (could use all rows here because iris is small)
set.seed(1)
bg_X &lt;- iris[sample(nrow(iris), 100), ]

# Calculate SHAP values
s &lt;- permshap(fit, X_explain, bg_X = bg_X)
s

# MODEL TWO: Multi-response linear regression
fit &lt;- lm(as.matrix(iris[, 1:2]) ~ Petal.Length + Petal.Width + Species, data = iris)
s &lt;- permshap(fit, iris[1:4, 3:5], bg_X = bg_X)
s

# Non-feature columns can be dropped via 'feature_names'
s &lt;- permshap(
  fit,
  iris[1:4, ],
  bg_X = bg_X,
  feature_names = c("Petal.Length", "Petal.Width", "Species")
)
s
</code></pre>

<hr>
<h2 id='print.kernelshap'>Prints &quot;kernelshap&quot; Object</h2><span id='topic+print.kernelshap'></span>

<h3>Description</h3>

<p>Prints &quot;kernelshap&quot; Object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'kernelshap'
print(x, n = 2L, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.kernelshap_+3A_x">x</code></td>
<td>
<p>An object of class &quot;kernelshap&quot;.</p>
</td></tr>
<tr><td><code id="print.kernelshap_+3A_n">n</code></td>
<td>
<p>Maximum number of rows of SHAP values to print.</p>
</td></tr>
<tr><td><code id="print.kernelshap_+3A_...">...</code></td>
<td>
<p>Further arguments passed from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Invisibly, the input is returned.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+kernelshap">kernelshap()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>fit &lt;- lm(Sepal.Length ~ ., data = iris)
s &lt;- kernelshap(fit, iris[1:3, -1], bg_X = iris[, -1])
s
</code></pre>

<hr>
<h2 id='print.permshap'>Prints &quot;permshap&quot; Object</h2><span id='topic+print.permshap'></span>

<h3>Description</h3>

<p>Prints &quot;permshap&quot; Object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'permshap'
print(x, n = 2L, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.permshap_+3A_x">x</code></td>
<td>
<p>An object of class &quot;permshap&quot;.</p>
</td></tr>
<tr><td><code id="print.permshap_+3A_n">n</code></td>
<td>
<p>Maximum number of rows of SHAP values to print.</p>
</td></tr>
<tr><td><code id="print.permshap_+3A_...">...</code></td>
<td>
<p>Further arguments passed from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Invisibly, the input is returned.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+permshap">permshap()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>fit &lt;- lm(Sepal.Length ~ ., data = iris)
s &lt;- permshap(fit, iris[1:3, -1], bg_X = iris[, -1])
s
</code></pre>

<hr>
<h2 id='summary.kernelshap'>Summarizes &quot;kernelshap&quot; Object</h2><span id='topic+summary.kernelshap'></span>

<h3>Description</h3>

<p>Summarizes &quot;kernelshap&quot; Object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'kernelshap'
summary(object, compact = FALSE, n = 2L, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.kernelshap_+3A_object">object</code></td>
<td>
<p>An object of class &quot;kernelshap&quot;.</p>
</td></tr>
<tr><td><code id="summary.kernelshap_+3A_compact">compact</code></td>
<td>
<p>Set to <code>TRUE</code> for a more compact summary.</p>
</td></tr>
<tr><td><code id="summary.kernelshap_+3A_n">n</code></td>
<td>
<p>Maximum number of rows of SHAP values etc. to print.</p>
</td></tr>
<tr><td><code id="summary.kernelshap_+3A_...">...</code></td>
<td>
<p>Further arguments passed from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Invisibly, the input is returned.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+kernelshap">kernelshap()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>fit &lt;- lm(Sepal.Length ~ ., data = iris)
s &lt;- kernelshap(fit, iris[1:3, -1], bg_X = iris[, -1])
summary(s)
</code></pre>

<hr>
<h2 id='summary.permshap'>Summarizes &quot;permshap&quot; Object</h2><span id='topic+summary.permshap'></span>

<h3>Description</h3>

<p>Summarizes &quot;permshap&quot; Object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'permshap'
summary(object, compact = FALSE, n = 2L, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.permshap_+3A_object">object</code></td>
<td>
<p>An object of class &quot;permshap&quot;.</p>
</td></tr>
<tr><td><code id="summary.permshap_+3A_compact">compact</code></td>
<td>
<p>Set to <code>TRUE</code> for a more compact summary.</p>
</td></tr>
<tr><td><code id="summary.permshap_+3A_n">n</code></td>
<td>
<p>Maximum number of rows of SHAP values etc. to print.</p>
</td></tr>
<tr><td><code id="summary.permshap_+3A_...">...</code></td>
<td>
<p>Further arguments passed from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Invisibly, the input is returned.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+permshap">permshap()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>fit &lt;- lm(Sepal.Length ~ ., data = iris)
s &lt;- permshap(fit, iris[1:3, -1], bg_X = iris[, -1])
summary(s)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
