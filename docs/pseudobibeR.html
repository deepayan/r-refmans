<!DOCTYPE html><html lang="en"><head><title>Help for package pseudobibeR</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {pseudobibeR}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#biber'><p>Extract Biber features from a document parsed and annotated by spacyr or udpipe</p></a></li>
<li><a href='#dict'><p>Dictionaries defining text features</p></a></li>
<li><a href='#normalize_counts'><p>Normalize to counts per 1,000 tokens</p></a></li>
<li><a href='#replace_nas'><p>Replace all NAs with 0</p></a></li>
<li><a href='#udpipe_samples'><p>Samples of parsed text</p></a></li>
<li><a href='#word_lists'><p>Lists of words defining text features</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>Aggregate Counts of Linguistic Features</td>
</tr>
<tr>
<td>Version:</td>
<td>1.2</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-11-18</td>
</tr>
<tr>
<td>Description:</td>
<td>Calculates the lexicogrammatical and functional features described
  by Biber (1985) &lt;<a href="https://doi.org/10.1515%2Fling.1985.23.2.337">doi:10.1515/ling.1985.23.2.337</a>&gt; and widely used for
  text-type, register, and genre classification tasks.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.2</td>
</tr>
<tr>
<td>LazyData:</td>
<td>TRUE</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>dplyr, purrr, quanteda, quanteda.textstats, rlang, stringr,
tibble, magrittr</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat (&ge; 3.0.0), udpipe</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-11-18 19:20:40 UTC; alexreinhart</td>
</tr>
<tr>
<td>Author:</td>
<td>David Brown <a href="https://orcid.org/0000-0001-7745-6354"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut,
    cre],
  Alex Reinhart <a href="https://orcid.org/0000-0002-6658-514X"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>David Brown &lt;dwb2@andrew.cmu.edu&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-11-19 12:20:06 UTC</td>
</tr>
</table>
<hr>
<h2 id='biber'>Extract Biber features from a document parsed and annotated by spacyr or udpipe</h2><span id='topic+biber'></span><span id='topic+biber.spacyr_parsed'></span><span id='topic+biber.udpipe_connlu'></span>

<h3>Description</h3>

<p>Takes data that has been part-of-speech tagged and dependency parsed and
extracts counts of features that have been used in Douglas Biber's research
since the late 1980s.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>biber(
  tokens,
  measure = c("MATTR", "TTR", "CTTR", "MSTTR", "none"),
  normalize = TRUE
)

## S3 method for class 'spacyr_parsed'
biber(
  tokens,
  measure = c("MATTR", "TTR", "CTTR", "MSTTR", "none"),
  normalize = TRUE
)

## S3 method for class 'udpipe_connlu'
biber(
  tokens,
  measure = c("MATTR", "TTR", "CTTR", "MSTTR", "none"),
  normalize = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="biber_+3A_tokens">tokens</code></td>
<td>
<p>A dataset of tokens created by <code>spacyr::spacy_parse()</code> or
<code>udpipe::udpipe_annotate()</code></p>
</td></tr>
<tr><td><code id="biber_+3A_measure">measure</code></td>
<td>
<p>Measure to use for type-token ratio. Passed to
<code>quanteda.textstats::textstat_lexdiv()</code> to calculate the statistic. Can be
the Moving Average Type-Token Ratio (MATTR), ordinary Type-Token Ratio
(TTR), corrected TTR (CTTR), Mean Segmental Type-Token Ratio (MSTTR), or
<code>"none"</code> to skip calculating a type-token ratio. If a statistic is chosen
but there are fewer than 200 token in the smallest document, the TTR is
used instead.</p>
</td></tr>
<tr><td><code id="biber_+3A_normalize">normalize</code></td>
<td>
<p>If <code>TRUE</code>, count features are normalized to the rate per
1,000 tokens.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Refer to <code>spacyr::spacy_parse()</code> or <code>udpipe::udpipe_annotate()</code> for details
on parsing texts. These must be configured to do part-of-speech and
dependency parsing. For <code>spacyr::spacy_parse()</code>, use the <code>dependency = TRUE</code>,
<code>tag = TRUE</code>, and <code>pos = TRUE</code> arguments; for <code>udpipe::udpipe_annotate()</code>,
set the <code>tagger</code> and <code>parser</code> arguments to <code>"default"</code>.
</p>
<p>Feature extraction relies on a dictionary (included as <code><a href="#topic+dict">dict</a></code>) and word
lists (<code><a href="#topic+word_lists">word_lists</a></code>) to match specific features; see their documentation
and values for details on the exact patterns and words matched by each. The
function identifies other features based on local cues, which are
approximations. Because they rely on probabilistic taggers provided by spaCy
or udpipe, the accuracy of the resulting counts are dependent on the accuracy
of those models. Thus, texts with irregular spellings, non-normative
punctuation, etc. will likely produce unreliable outputs, unless taggers are
tuned specifically for those purposes.
</p>
<p>The following features are detected. Square brackets in example sentences
indicate the location of the feature.
</p>


<h4>Tense and aspect markers</h4>


<dl>
<dt>f_01_past_tense</dt><dd><p>Verbs in the past tense.</p>
</dd>
<dt>f_02_perfect_aspect</dt><dd><p>Verbs in the perfect aspect, indicated by &quot;have&quot; as an auxiliary verb (e.g. <em>I [have] written this sentence.</em>)&quot;</p>
</dd>
<dt>f_03_present_tense</dt><dd><p>Verbs in the present tense.</p>
</dd>
</dl>




<h4>Place and time adverbials</h4>


<dl>
<dt>f_04_place_adverbials</dt><dd><p>Place adverbials (e.g., <em>above</em>, <em>beside</em>, <em>outdoors</em>; see list in <code>dict$f_04_place_adverbials</code>)</p>
</dd>
<dt>f_05_time_adverbials</dt><dd><p>Time adverbials (e.g., <em>early</em>, <em>instantly</em>, <em>soon</em>; see <code>dict$f_05_time_adverbials</code>)</p>
</dd>
</dl>




<h4>Pronouns and pro-verbs</h4>


<dl>
<dt>f_06_first_person_pronouns</dt><dd><p>First-person pronouns; see <code>dict$f_06_first_person_pronouns</code></p>
</dd>
<dt>f_07_second_person_pronouns</dt><dd><p>Second-person pronouns; see <code>dict$f_07_second_person_pronouns</code></p>
</dd>
<dt>f_08_third_person_pronouns</dt><dd><p>Third-person personal pronouns (excluding <em>it</em>); see <code>dict$f_08_third_person_pronouns</code></p>
</dd>
<dt>f_09_pronoun_it</dt><dd><p>Pronoun <em>it</em>, <em>its</em>, or <em>itself</em></p>
</dd>
<dt>f_10_demonstrative_pronoun</dt><dd><p>Pronouns being used to replace a noun (e.g. <em>[That] is an example sentence.</em>)</p>
</dd>
<dt>f_11_indefinite_pronouns</dt><dd><p>Indefinite pronouns (e.g., <em>anybody</em>, <em>nothing</em>, <em>someone</em>; see <code>dict$f_11_indefinite_pronouns</code>)</p>
</dd>
<dt>f_12_proverb_do</dt><dd><p>Pro-verb <em>do</em></p>
</dd>
</dl>




<h4>Questions</h4>


<dl>
<dt>f_13_wh_question</dt><dd><p>Direct <em>wh-</em> questions (e.g., <em>When are you leaving?</em>)</p>
</dd>
</dl>




<h4>Nominal forms</h4>


<dl>
<dt>f_14_nominalizations</dt><dd><p>Nominalizations (nouns ending in <em>-tion</em>, <em>-ment</em>, <em>-ness</em>, <em>-ity</em>, e.g. <em>adjustment</em>, <em>abandonment</em>)</p>
</dd>
<dt>f_15_gerunds</dt><dd><p>Gerunds (participial forms functioning as nouns)</p>
</dd>
<dt>f_16_other_nouns</dt><dd><p>Total other nouns</p>
</dd>
</dl>




<h4>Passives</h4>


<dl>
<dt>f_17_agentless_passives</dt><dd><p>Agentless passives (e.g., <em>The task [was done].</em>)</p>
</dd>
<dt>f_18_by_passives</dt><dd><p><em>by-</em> passives (e.g., <em>The task [was done by Steve].</em>)</p>
</dd>
</dl>




<h4>Stative forms</h4>


<dl>
<dt>f_19_be_main_verb</dt><dd><p><em>be</em> as main verb</p>
</dd>
<dt>f_20_existential_there</dt><dd><p>Existential <em>there</em> (e.g., <em>[There] is a feature in this sentence.</em>)</p>
</dd>
</dl>




<h4>Subordination features</h4>


<dl>
<dt>f_21_that_verb_comp</dt><dd><p><em>that</em> verb complements (e.g., <em>I said [that he went].</em>)</p>
</dd>
<dt>f_22_that_adj_comp</dt><dd><p><em>that</em> adjective complements (e.g., <em>I'm glad [that you like it].</em>)</p>
</dd>
<dt>f_23_wh_clause</dt><dd><p><em>wh-</em> clauses (e.g., <em>I believed [what he told me].</em>)</p>
</dd>
<dt>f_24_infinitives</dt><dd><p>Infinitives</p>
</dd>
<dt>f_25_present_participle</dt><dd><p>Present participial adverbial clauses (e.g., <em>[Stuffing his mouth with cookies], Joe ran out the door.</em>)</p>
</dd>
<dt>f_26_past_participle</dt><dd><p>Past participial adverbial clauses (e.g., <em>[Built in a single week], the house would stand for fifty years.</em>)</p>
</dd>
<dt>f_27_past_participle_whiz</dt><dd><p>Past participial postnominal (reduced relative) clauses (e.g., <em>the solution [produced by this process]</em>)</p>
</dd>
<dt>f_28_present_participle_whiz</dt><dd><p>Present participial postnominal (reduced relative) clauses (e.g., <em>the event [causing this decline]</em>)</p>
</dd>
<dt>f_29_that_subj</dt><dd><p><em>that</em> relative clauses on subject position (e.g., <em>the dog [that bit me]</em>)</p>
</dd>
<dt>f_30_that_obj</dt><dd><p><em>that</em> relative clauses on object position (e.g., <em>the dog [that I saw]</em>)</p>
</dd>
<dt>f_31_wh_subj</dt><dd><p><em>wh-</em> relatives on subject position (e.g., <em>the man [who likes popcorn]</em>)</p>
</dd>
<dt>f_32_wh_obj</dt><dd><p><em>wh-</em> relatives on object position (e.g., <em>the man [who Sally likes]</em>)</p>
</dd>
<dt>f_33_pied_piping</dt><dd><p>Pied-piping relative clauses (e.g., <em>the manner [in which he was told]</em>)</p>
</dd>
<dt>f_34_sentence_relatives</dt><dd><p>Sentence relatives (e.g., <em>Bob likes fried mangoes, [which is the most disgusting thing I've ever heard of].</em>)</p>
</dd>
<dt>f_35_because</dt><dd><p>Causative adverbial subordinator (<em>because</em>)</p>
</dd>
<dt>f_36_though</dt><dd><p>Concessive adverbial subordinators (<em>although</em>, <em>though</em>)</p>
</dd>
<dt>f_37_if</dt><dd><p>Conditional adverbial subordinators (<em>if</em>, <em>unless</em>)</p>
</dd>
<dt>f_38_other_adv_sub</dt><dd><p>Other adverbial subordinators (e.g., <em>since</em>, <em>while</em>, <em>whereas</em>)</p>
</dd>
</dl>




<h4>Prepositional phrases, adjectives, and adverbs</h4>


<dl>
<dt>f_39_prepositions</dt><dd><p>Total prepositional phrases</p>
</dd>
<dt>f_40_adj_attr</dt><dd><p>Attributive adjectives (e.g., <em>the [big] horse</em>)</p>
</dd>
<dt>f_41_adj_pred</dt><dd><p>Predicative adjectives (e.g., <em>The horse is [big].</em>)</p>
</dd>
<dt>f_42_adverbs</dt><dd><p>Total adverbs</p>
</dd>
</dl>




<h4>Lexical specificity</h4>


<dl>
<dt>f_43_type_token</dt><dd><p>Type-token ratio (including punctuation), using the statistic chosen in <code>measure</code>, or TTR if there are fewer than 200 tokens in the smallest document.</p>
</dd>
<dt>f_44_mean_word_length</dt><dd><p>Average word length (across tokens, excluding punctuation)</p>
</dd>
</dl>




<h4>Lexical classes</h4>


<dl>
<dt>f_45_conjuncts</dt><dd><p>Conjuncts (e.g., <em>consequently</em>, <em>furthermore</em>, <em>however</em>; see <code>dict$f_45_conjuncts</code>)</p>
</dd>
<dt>f_46_downtoners</dt><dd><p>Downtoners (e.g., <em>barely</em>, <em>nearly</em>, <em>slightly</em>; see <code>dict$f_46_downtoners</code>)</p>
</dd>
<dt>f_47_hedges</dt><dd><p>Hedges (e.g., <em>at about</em>, <em>something like</em>, <em>almost</em>; see <code>dict$f_47_hedges</code>)</p>
</dd>
<dt>f_48_amplifiers</dt><dd><p>Amplifiers (e.g., <em>absolutely</em>, <em>extremely</em>, <em>perfectly</em>; see <code>dict$f_48_amplifiers</code>)</p>
</dd>
<dt>f_49_emphatics</dt><dd><p>Emphatics (e.g., <em>a lot</em>, <em>for sure</em>, <em>really</em>; see <code>dict$f_49_emphatics</code>)</p>
</dd>
<dt>f_50_discourse_particles</dt><dd><p>Discourse particles (e.g., sentence-initial <em>well</em>, <em>now</em>, <em>anyway</em>; see <code>dict$f_50_discourse_particles</code>)</p>
</dd>
<dt>f_51_demonstratives</dt><dd><p>Demonstratives (<em>that</em>, <em>this</em>, <em>these</em>, or <em>those</em> used as determiners, e.g. <em>[That] is the feature</em>)</p>
</dd>
</dl>




<h4>Modals</h4>


<dl>
<dt>f_52_modal_possibility</dt><dd><p>Possibility modals (<em>can</em>, <em>may</em>, <em>might</em>, <em>could</em>)</p>
</dd>
<dt>f_53_modal_necessity</dt><dd><p>Necessity modals (<em>ought</em>, <em>should</em>, <em>must</em>)</p>
</dd>
<dt>f_54_modal_predictive</dt><dd><p>Predictive modals (<em>will</em>, <em>would</em>, <em>shall</em>)</p>
</dd>
</dl>




<h4>Specialized verb classes</h4>


<dl>
<dt>f_55_verb_public</dt><dd><p>Public verbs (e.g., <em>assert</em>, <em>declare</em>, <em>mention</em>; see <code>dict$f_55_verb_public</code>)</p>
</dd>
<dt>f_56_verb_private</dt><dd><p>Private verbs (e.g., <em>assume</em>, <em>believe</em>, <em>doubt</em>, <em>know</em>; see <code>dict$f_56_verb_private</code>)</p>
</dd>
<dt>f_57_verb_suasive</dt><dd><p>Suasive verbs (e.g., <em>command</em>, <em>insist</em>, <em>propose</em>; see <code>dict$f_57_verb_suasive</code>)</p>
</dd>
<dt>f_58_verb_seem</dt><dd><p><em>seem</em> and <em>appear</em></p>
</dd>
</dl>




<h4>Reduced forms and dispreferred structures</h4>


<dl>
<dt>f_59_contractions</dt><dd><p>Contractions</p>
</dd>
<dt>f_60_that_deletion</dt><dd><p>Subordinator <em>that</em> deletion (e.g., <em>I think [he went].</em>)</p>
</dd>
<dt>f_61_stranded_preposition</dt><dd><p>Stranded prepositions (e.g., <em>the candidate that I was thinking [of]</em>)</p>
</dd>
<dt>f_62_split_infinitive</dt><dd><p>Split infinitives (e.g., <em>He wants [to convincingly prove] that ...</em>)</p>
</dd>
<dt>f_63_split_auxiliary</dt><dd><p>Split auxiliaries (e.g., <em>They [were apparently shown] to ...</em>)</p>
</dd>
</dl>




<h4>Co-ordination</h4>


<dl>
<dt>f_64_phrasal_coordination</dt><dd><p>Phrasal co-ordination (N and N; Adj and Adj; V and V; Adv and Adv)</p>
</dd>
<dt>f_65_clausal_coordination</dt><dd><p>Independent clause co-ordination (clause-initial <em>and</em>)</p>
</dd>
</dl>




<h4>Negation</h4>


<dl>
<dt>f_66_neg_synthetic</dt><dd><p>Synthetic negation (e.g., <em>No answer is good enough for Jones.</em>)</p>
</dd>
<dt>f_67_neg_analytic</dt><dd><p>Analytic negation (e.g., <em>That isn't good enough.</em>)</p>
</dd>
</dl>




<h3>Value</h3>

<p>A <code>data.frame</code> of features containing one row per document and one
column per feature. If <code>normalize</code> is <code>TRUE</code>, count features are normalized
to the rate per 1,000 tokens.
</p>


<h3>References</h3>

<p>Biber, Douglas (1985). &quot;Investigating macroscopic textual
variation through multifeature/multidimensional analyses.&quot; <em>Linguistics</em>
23(2), 337-360. <a href="https://doi.org/10.1515/ling.1985.23.2.337">doi:10.1515/ling.1985.23.2.337</a>
</p>
<p>Biber, Douglas (1988). <em>Variation across Speech and Writing</em>.
Cambridge University Press.
</p>
<p>Biber, Douglas (1995). <em>Dimensions of Register Variation: A Cross-Linguistic
Comparison.</em> Cambridge University Press.
</p>
<p>Covington, M. A., &amp; McFall, J. D. (2010). Cutting the Gordian Knot: The
Moving-Average Type–Token Ratio (MATTR). <em>Journal of Quantitative
Linguistics</em>, 17(2), 94–100. <a href="https://doi.org/10.1080/09296171003643098">doi:10.1080/09296171003643098</a>
</p>


<h3>See Also</h3>

<p><a href="#topic+dict">dict</a>, <a href="#topic+word_lists">word_lists</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Parse the example documents provided with the package
biber(udpipe_samples)

biber(spacy_samples)
</code></pre>

<hr>
<h2 id='dict'>Dictionaries defining text features</h2><span id='topic+dict'></span>

<h3>Description</h3>

<p>For Biber features defined by matching text against dictionaries of word
patterns (such as third-person pronouns or conjunctions), or features that
can be found by matching patterns against text, this gives the dictionary of
patterns for each feature. These are primarily used internally by <code>biber()</code>,
but are exported so users can examine the feature definitions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dict
</code></pre>


<h3>Format</h3>

<p>A named list with one entry per feature. The name is the feature
name, such as <code>f_33_pied_piping</code>; values give a list of terms or patterns.
Patterns are matched to spaCy tokens using <code>quanteda::tokens_lookup()</code>
using the <code>glob</code> valuetype.
</p>

<hr>
<h2 id='normalize_counts'>Normalize to counts per 1,000 tokens</h2><span id='topic+normalize_counts'></span>

<h3>Description</h3>

<p>Normalize to counts per 1,000 tokens
</p>


<h3>Usage</h3>

<pre><code class='language-R'>normalize_counts(counts)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="normalize_counts_+3A_counts">counts</code></td>
<td>
<p>Data frame with numeric columns for counts of token, with one
row per document. Must include a <code>tot_counts</code> column with the total number
of tokens per document.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>counts</code> data frame with counts normalized to rate per 1,000 tokens,
and <code>tot_counts</code> column removed
</p>

<hr>
<h2 id='replace_nas'>Replace all NAs with 0</h2><span id='topic+replace_nas'></span>

<h3>Description</h3>

<p>Replace all NAs with 0
</p>


<h3>Usage</h3>

<pre><code class='language-R'>replace_nas(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="replace_nas_+3A_x">x</code></td>
<td>
<p>Vector potentially containing NAs</p>
</td></tr>
</table>

<hr>
<h2 id='udpipe_samples'>Samples of parsed text</h2><span id='topic+udpipe_samples'></span><span id='topic+spacy_samples'></span>

<h3>Description</h3>

<p>Examples of spaCy and udpipe tagging output from excerpts of several
public-domain texts. Can be passed to <code>biber()</code> to see examples of its
feature detection.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>udpipe_samples

spacy_samples
</code></pre>


<h3>Format</h3>

<p>An object of class <code>udpipe_connlu</code> of length 3.
</p>
<p>An object of class <code>spacyr_parsed</code> (inherits from <code>data.frame</code>) with 1346 rows and 9 columns.
</p>


<h3>Details</h3>

<p>Texts consist of early paragraphs from several public-domain books
distributed by Project Gutenberg <a href="https://gutenberg.org">https://gutenberg.org</a>. Document IDs are
the Project Gutenberg book numbers.
</p>
<p>See <code>udpipe::udpipe_annotate()</code> and <code>spacyr::spacy_parse()</code> for
further details on the data format produced by each package.
</p>

<hr>
<h2 id='word_lists'>Lists of words defining text features</h2><span id='topic+word_lists'></span>

<h3>Description</h3>

<p>For Biber features defined by matching texts against certain exact words,
rather than patterns, this list defines the exact words defining the
features. These lists are primarily used internally by <code>biber()</code>, but are
exported so users can examine the feature definitions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>word_lists
</code></pre>


<h3>Format</h3>

<p>A named list with one entry per word list. Each entry is a vector of
words.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
