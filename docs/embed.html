<!DOCTYPE html><html lang="en"><head><title>Help for package embed</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {embed}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#embed-package'><p>embed: Extra Recipes for Encoding Predictors</p></a></li>
<li><a href='#add_woe'><p>Add WoE in a data frame</p></a></li>
<li><a href='#dictionary'><p>Weight of evidence dictionary</p></a></li>
<li><a href='#reexports'><p>Objects exported from other packages</p></a></li>
<li><a href='#required_pkgs.step_collapse_cart'><p>S3 methods for tracking which additional packages are needed for steps.</p></a></li>
<li><a href='#solubility'><p>Compound solubility data</p></a></li>
<li><a href='#step_collapse_cart'><p>Supervised Collapsing of Factor Levels</p></a></li>
<li><a href='#step_collapse_stringdist'><p>collapse factor levels using stringdist</p></a></li>
<li><a href='#step_discretize_cart'><p>Discretize numeric variables with CART</p></a></li>
<li><a href='#step_discretize_xgb'><p>Discretize numeric variables with XgBoost</p></a></li>
<li><a href='#step_embed'><p>Encoding Factors into Multiple Columns</p></a></li>
<li><a href='#step_feature_hash'><p>Dummy Variables Creation via Feature Hashing</p></a></li>
<li><a href='#step_lencode_bayes'><p>Supervised Factor Conversions into Linear Functions using Bayesian Likelihood</p>
Encodings</a></li>
<li><a href='#step_lencode_glm'><p>Supervised Factor Conversions into Linear Functions using Likelihood</p>
Encodings</a></li>
<li><a href='#step_lencode_mixed'><p>Supervised Factor Conversions into Linear Functions using Bayesian Likelihood</p>
Encodings</a></li>
<li><a href='#step_pca_sparse'><p>Sparse PCA Signal Extraction</p></a></li>
<li><a href='#step_pca_sparse_bayes'><p>Sparse Bayesian PCA Signal Extraction</p></a></li>
<li><a href='#step_pca_truncated'><p>Truncated PCA Signal Extraction</p></a></li>
<li><a href='#step_umap'><p>Supervised and unsupervised uniform manifold approximation and projection</p>
(UMAP)</a></li>
<li><a href='#step_woe'><p>Weight of evidence transformation</p></a></li>
<li><a href='#tunable.step_discretize_cart'><p>tunable methods for embed</p></a></li>
<li><a href='#woe_table'><p>Crosstable with woe between a binary outcome and a predictor variable.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>Extra Recipes for Encoding Predictors</td>
</tr>
<tr>
<td>Version:</td>
<td>1.1.5</td>
</tr>
<tr>
<td>Description:</td>
<td>Predictors can be converted to one or more numeric
    representations using a variety of methods. Effect encodings using
    simple generalized linear models &lt;<a href="https://doi.org/10.48550%2FarXiv.1611.09477">doi:10.48550/arXiv.1611.09477</a>&gt; or 
    nonlinear models &lt;<a href="https://doi.org/10.48550%2FarXiv.1604.06737">doi:10.48550/arXiv.1604.06737</a>&gt; can be used. There 
    are also functions for dimension reduction and other approaches.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://embed.tidymodels.org">https://embed.tidymodels.org</a>, <a href="https://github.com/tidymodels/embed">https://github.com/tidymodels/embed</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/tidymodels/embed/issues">https://github.com/tidymodels/embed/issues</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.6), recipes (&ge; 1.1.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>cli, glue, dplyr (&ge; 1.1.0), generics (&ge; 0.1.0), lifecycle,
purrr, rlang (&ge; 1.1.0), rsample, stats, tibble, tidyr, utils,
uwot, withr, vctrs</td>
</tr>
<tr>
<td>Suggests:</td>
<td>covr, dials (&ge; 1.2.0), ggplot2, hardhat, irlba, keras,
knitr, lme4, modeldata, rmarkdown, rpart, rstanarm, stringdist,
tensorflow, testthat (&ge; 3.0.0), VBsparsePCA, xgboost</td>
</tr>
<tr>
<td>ByteCompile:</td>
<td>true</td>
</tr>
<tr>
<td>Config/Needs/website:</td>
<td>tidymodels, ggiraph, tidyverse/tidytemplate,
reticulate</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.2</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2025-01-23 00:00:09 UTC; emilhvitfeldt</td>
</tr>
<tr>
<td>Author:</td>
<td>Emil Hvitfeldt <a href="https://orcid.org/0000-0002-0679-1945"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre],
  Max Kuhn <a href="https://orcid.org/0000-0003-2402-136X"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut],
  Posit Software, PBC [cph, fnd]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Emil Hvitfeldt &lt;emil.hvitfeldt@posit.co&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2025-01-23 00:10:06 UTC</td>
</tr>
</table>
<hr>
<h2 id='embed-package'>embed: Extra Recipes for Encoding Predictors</h2><span id='topic+embed'></span><span id='topic+embed-package'></span>

<h3>Description</h3>

<p><img src="../help/figures/logo.png" style='float: right' alt='logo' width='120' />
</p>
<p>Predictors can be converted to one or more numeric representations using a variety of methods. Effect encodings using simple generalized linear models <a href="https://arxiv.org/abs/1611.09477">arXiv:1611.09477</a> or nonlinear models <a href="https://arxiv.org/abs/1604.06737">arXiv:1604.06737</a> can be used. There are also functions for dimension reduction and other approaches.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Emil Hvitfeldt <a href="mailto:emil.hvitfeldt@posit.co">emil.hvitfeldt@posit.co</a> (<a href="https://orcid.org/0000-0002-0679-1945">ORCID</a>)
</p>
<p>Authors:
</p>

<ul>
<li><p> Max Kuhn <a href="mailto:max@posit.co">max@posit.co</a> (<a href="https://orcid.org/0000-0003-2402-136X">ORCID</a>)
</p>
</li></ul>

<p>Other contributors:
</p>

<ul>
<li><p> Posit Software, PBC [copyright holder, funder]
</p>
</li></ul>



<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://embed.tidymodels.org">https://embed.tidymodels.org</a>
</p>
</li>
<li> <p><a href="https://github.com/tidymodels/embed">https://github.com/tidymodels/embed</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/tidymodels/embed/issues">https://github.com/tidymodels/embed/issues</a>
</p>
</li></ul>


<hr>
<h2 id='add_woe'>Add WoE in a data frame</h2><span id='topic+add_woe'></span>

<h3>Description</h3>

<p>A tidyverse friendly way to plug WoE versions of a set of predictor variables
against a given binary outcome.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>add_woe(.data, outcome, ..., dictionary = NULL, prefix = "woe")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="add_woe_+3A_.data">.data</code></td>
<td>
<p>A tbl. The data.frame to plug the new woe version columns.</p>
</td></tr>
<tr><td><code id="add_woe_+3A_outcome">outcome</code></td>
<td>
<p>The bare name of the outcome variable.</p>
</td></tr>
<tr><td><code id="add_woe_+3A_...">...</code></td>
<td>
<p>Bare names of predictor variables, passed as you would pass
variables to <code>dplyr::select()</code>. This means that you can use all the
helpers like <code>starts_with()</code> and <code>matches()</code>.</p>
</td></tr>
<tr><td><code id="add_woe_+3A_dictionary">dictionary</code></td>
<td>
<p>A tbl. If NULL the function will build a dictionary with
those variables passed to <code>...</code>. You can pass a custom dictionary too,
see <code><a href="#topic+dictionary">dictionary()</a></code> for details.</p>
</td></tr>
<tr><td><code id="add_woe_+3A_prefix">prefix</code></td>
<td>
<p>A character string that will be the prefix to the resulting new
variables.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>You can pass a custom dictionary to <code><a href="#topic+add_woe">add_woe()</a></code>. It must have the exactly the
same structure of the output of <code><a href="#topic+dictionary">dictionary()</a></code>. One easy way to do this is to
tweak a output returned from it.
</p>


<h3>Value</h3>

<p>A tibble with the original columns of .data plus the woe columns
wanted.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
mtcars %&gt;% add_woe("am", cyl, gear:carb)
</code></pre>

<hr>
<h2 id='dictionary'>Weight of evidence dictionary</h2><span id='topic+dictionary'></span>

<h3>Description</h3>

<p>Builds the woe dictionary of a set of predictor variables upon a given binary
outcome. Convenient to make a woe version of the given set of predictor
variables and also to allow one to tweak some woe values by hand.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dictionary(.data, outcome, ..., Laplace = 1e-06)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="dictionary_+3A_.data">.data</code></td>
<td>
<p>A tbl. The data.frame where the variables come from.</p>
</td></tr>
<tr><td><code id="dictionary_+3A_outcome">outcome</code></td>
<td>
<p>The bare name of the outcome variable with exactly 2 distinct
values.</p>
</td></tr>
<tr><td><code id="dictionary_+3A_...">...</code></td>
<td>
<p>bare names of predictor variables or selectors accepted by
<code>dplyr::select()</code>.</p>
</td></tr>
<tr><td><code id="dictionary_+3A_laplace">Laplace</code></td>
<td>
<p>Default to 1e-6. The <code>pseudocount</code> parameter of the Laplace
Smoothing estimator. Value to avoid -Inf/Inf from predictor category with
only one outcome class. Set to 0 to allow Inf/-Inf.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>You can pass a custom dictionary to <code>step_woe()</code>. It must have the
exactly the same structure of the output of <code><a href="#topic+dictionary">dictionary()</a></code>. One easy way to
do this is by tweaking an output returned from it.
</p>


<h3>Value</h3>

<p>a tibble with summaries and woe for every given predictor variable
stacked up.
</p>


<h3>References</h3>

<p>Kullback, S. (1959). <em>Information Theory and Statistics.</em> Wiley, New York.
</p>
<p>Hastie, T., Tibshirani, R. and Friedman, J. (1986). <em>Elements of Statistical
Learning</em>, Second Edition, Springer, 2009.
</p>
<p>Good, I. J. (1985), &quot;Weight of evidence: A brief survey&quot;, <em>Bayesian
Statistics</em>, 2, pp.249-270.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
mtcars %&gt;% dictionary("am", cyl, gear:carb)
</code></pre>

<hr>
<h2 id='reexports'>Objects exported from other packages</h2><span id='topic+reexports'></span><span id='topic+tidy'></span><span id='topic+required_pkgs'></span><span id='topic+tunable'></span>

<h3>Description</h3>

<p>These objects are imported from other packages. Follow the links
below to see their documentation.
</p>

<dl>
<dt>generics</dt><dd><p><code><a href="generics.html#topic+required_pkgs">required_pkgs</a></code>, <code><a href="generics.html#topic+tidy">tidy</a></code>, <code><a href="generics.html#topic+tunable">tunable</a></code></p>
</dd>
</dl>

<hr>
<h2 id='required_pkgs.step_collapse_cart'>S3 methods for tracking which additional packages are needed for steps.</h2><span id='topic+required_pkgs.step_collapse_cart'></span><span id='topic+required_pkgs.step_collapse_stringdist'></span><span id='topic+required_pkgs.step_discretize_cart'></span><span id='topic+required_pkgs.step_discretize_xgb'></span><span id='topic+required_pkgs.step_embed'></span><span id='topic+required_pkgs.step_feature_hash'></span><span id='topic+required_pkgs.step_lencode_bayes'></span><span id='topic+required_pkgs.step_lencode_glm'></span><span id='topic+required_pkgs.step_lencode_mixed'></span><span id='topic+required_pkgs.step_pca_sparse'></span><span id='topic+required_pkgs.step_pca_sparse_bayes'></span><span id='topic+required_pkgs.step_pca_truncated'></span><span id='topic+required_pkgs.step_umap'></span><span id='topic+required_pkgs.step_woe'></span>

<h3>Description</h3>

<p>Recipe-adjacent packages always list themselves as a required package so that
the steps can function properly within parallel processing schemes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'step_collapse_cart'
required_pkgs(x, ...)

## S3 method for class 'step_collapse_stringdist'
required_pkgs(x, ...)

## S3 method for class 'step_discretize_cart'
required_pkgs(x, ...)

## S3 method for class 'step_discretize_xgb'
required_pkgs(x, ...)

## S3 method for class 'step_embed'
required_pkgs(x, ...)

## S3 method for class 'step_feature_hash'
required_pkgs(x, ...)

## S3 method for class 'step_lencode_bayes'
required_pkgs(x, ...)

## S3 method for class 'step_lencode_glm'
required_pkgs(x, ...)

## S3 method for class 'step_lencode_mixed'
required_pkgs(x, ...)

## S3 method for class 'step_pca_sparse'
required_pkgs(x, ...)

## S3 method for class 'step_pca_sparse_bayes'
required_pkgs(x, ...)

## S3 method for class 'step_pca_truncated'
required_pkgs(x, ...)

## S3 method for class 'step_umap'
required_pkgs(x, ...)

## S3 method for class 'step_woe'
required_pkgs(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="required_pkgs.step_collapse_cart_+3A_x">x</code></td>
<td>
<p>A recipe step</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A character vector
</p>

<hr>
<h2 id='solubility'>Compound solubility data</h2><span id='topic+solubility'></span>

<h3>Description</h3>

<p>Compound solubility data
</p>


<h3>Details</h3>

<p>Tetko et al. (2001) and Huuskonen (2000) investigated a set of compounds with
corresponding experimental solubility values using complex sets of
descriptors. They used linear regression and neural network models to
estimate the relationship between chemical structure and solubility. For our
analyses, we will use 1267 compounds and a set of more understandable
descriptors that fall into one of three groups: 208 binary &quot;fingerprints&quot;
that indicate the presence or absence of a particular chemical sub-structure,
16 count descriptors (such as the number of bonds or the number of Bromine
atoms) and 4 continuous descriptors (such as molecular weight or surface
area).
</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>solubility</code></td>
<td>
<p>a data frame</p>
</td></tr>
</table>


<h3>Source</h3>

<p>Tetko, I., Tanchuk, V., Kasheva, T., and Villa, A. (2001). Estimation of
aqueous solubility of chemical compounds using E-state indices. <em>Journal of
Chemical Information and Computer Sciences</em>, 41(6), 1488-1493.
</p>
<p>Huuskonen, J. (2000). Estimation of aqueous solubility for a diverse set of
organic compounds based on molecular topology. <em>Journal of Chemical
Information and Computer Sciences</em>, 40(3), 773-777.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(solubility)
str(solubility)
</code></pre>

<hr>
<h2 id='step_collapse_cart'>Supervised Collapsing of Factor Levels</h2><span id='topic+step_collapse_cart'></span><span id='topic+tidy.step_collapse_cart'></span>

<h3>Description</h3>

<p><code>step_collapse_cart()</code> creates a <em>specification</em> of a recipe step that can
collapse factor levels into a smaller set using a supervised tree.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>step_collapse_cart(
  recipe,
  ...,
  role = NA,
  trained = FALSE,
  outcome = NULL,
  cost_complexity = 1e-04,
  min_n = 5,
  results = NULL,
  skip = FALSE,
  id = rand_id("step_collapse_cart")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="step_collapse_cart_+3A_recipe">recipe</code></td>
<td>
<p>A recipe object. The step will be added to the sequence of
operations for this recipe.</p>
</td></tr>
<tr><td><code id="step_collapse_cart_+3A_...">...</code></td>
<td>
<p>One or more selector functions to choose which variables are
affected by the step. See <a href="recipes.html#topic+selections">recipes::selections</a> for more details. For the <code>tidy</code>
method, these are not currently used.</p>
</td></tr>
<tr><td><code id="step_collapse_cart_+3A_role">role</code></td>
<td>
<p>Not used by this step since no new variables are created.</p>
</td></tr>
<tr><td><code id="step_collapse_cart_+3A_trained">trained</code></td>
<td>
<p>A logical to indicate if the quantities for preprocessing have
been estimated.</p>
</td></tr>
<tr><td><code id="step_collapse_cart_+3A_outcome">outcome</code></td>
<td>
<p>A call to <code>vars</code> to specify which variable is used as the
outcome to train CART models in order to pool factor levels.</p>
</td></tr>
<tr><td><code id="step_collapse_cart_+3A_cost_complexity">cost_complexity</code></td>
<td>
<p>A non-negative value that regulates the complexity of
the tree when pruning occurs. Values near 0.1 usually correspond to a tree
with a single splits. Values of zero correspond to unpruned tree.</p>
</td></tr>
<tr><td><code id="step_collapse_cart_+3A_min_n">min_n</code></td>
<td>
<p>An integer for how many data points are required to make further
splits during the tree growing process. Larger values correspond to less
complex trees.</p>
</td></tr>
<tr><td><code id="step_collapse_cart_+3A_results">results</code></td>
<td>
<p>A list of results to convert to new factor levels.</p>
</td></tr>
<tr><td><code id="step_collapse_cart_+3A_skip">skip</code></td>
<td>
<p>A logical. Should the step be skipped when the recipe is baked by
<a href="recipes.html#topic+bake">recipes::bake</a>? While all operations are baked when <a href="recipes.html#topic+prep">recipes::prep</a> is run, some
operations may not be able to be conducted on new data (e.g. processing the
outcome variable(s)). Care should be taken when using <code>skip = TRUE</code> as it
may affect the computations for subsequent operations</p>
</td></tr>
<tr><td><code id="step_collapse_cart_+3A_id">id</code></td>
<td>
<p>A character string that is unique to this step to identify it.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This step uses a CART tree (classification or regression) to group the
existing factor levels into a potentially smaller set. It changes the levels
in the factor predictor (and the <code>tidy()</code> method can be used to understand
the translation).
</p>
<p>There are a few different ways that the step will not be able to collapse
levels. If the model fails or, if the results have each level being in its
own split, the original factor levels are retained. There are also cases
where there is &quot;no admissible split&quot; which means that the model could not
find any signal in the data.
</p>


<h3>Value</h3>

<p>An updated recipe step.
</p>


<h3>Tidying</h3>

<p>When you <code><a href="recipes.html#topic+tidy.recipe">tidy()</a></code> this step, a tibble is returned with
columns <code>terms</code>, <code>old</code>, <code>new</code>, and <code>id</code>:
</p>

<dl>
<dt>terms</dt><dd><p>character, the selectors or variables selected</p>
</dd>
<dt>old</dt><dd><p>character, the old levels</p>
</dd>
<dt>new</dt><dd><p>character, the new levels</p>
</dd>
<dt>id</dt><dd><p>character, id of this step</p>
</dd>
</dl>



<h3>Case weights</h3>

<p>The underlying operation does not allow for case weights.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(ames, package = "modeldata")
ames$Sale_Price &lt;- log10(ames$Sale_Price)

rec &lt;-
  recipe(Sale_Price ~ ., data = ames) %&gt;%
  step_collapse_cart(
    Sale_Type, Garage_Type, Neighborhood,
    outcome = vars(Sale_Price)
  ) %&gt;%
  prep()
tidy(rec, number = 1)

</code></pre>

<hr>
<h2 id='step_collapse_stringdist'>collapse factor levels using stringdist</h2><span id='topic+step_collapse_stringdist'></span><span id='topic+tidy.step_collapse_stringdist'></span>

<h3>Description</h3>

<p><code>step_collapse_stringdist()</code> creates a <em>specification</em> of a recipe step that
will collapse factor levels that have a low stringdist between them.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>step_collapse_stringdist(
  recipe,
  ...,
  role = NA,
  trained = FALSE,
  distance = NULL,
  method = "osa",
  options = list(),
  results = NULL,
  columns = NULL,
  skip = FALSE,
  id = rand_id("collapse_stringdist")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="step_collapse_stringdist_+3A_recipe">recipe</code></td>
<td>
<p>A recipe object. The step will be added to the
sequence of operations for this recipe.</p>
</td></tr>
<tr><td><code id="step_collapse_stringdist_+3A_...">...</code></td>
<td>
<p>One or more selector functions to choose which variables are
affected by the step. See <a href="recipes.html#topic+selections">recipes::selections</a> for more details.  For the <code>tidy</code>
method, these are not currently used.</p>
</td></tr>
<tr><td><code id="step_collapse_stringdist_+3A_role">role</code></td>
<td>
<p>Not used by this step since no new variables are created.</p>
</td></tr>
<tr><td><code id="step_collapse_stringdist_+3A_trained">trained</code></td>
<td>
<p>A logical to indicate if the quantities for
preprocessing have been estimated.</p>
</td></tr>
<tr><td><code id="step_collapse_stringdist_+3A_distance">distance</code></td>
<td>
<p>Integer, value to determine which strings should be collapsed
with which. The value is being used inclusive, so <code>2</code> will collapse levels
that have a string distance between them of 2 or lower.</p>
</td></tr>
<tr><td><code id="step_collapse_stringdist_+3A_method">method</code></td>
<td>
<p>Character, method for distance calculation. The default is
<code>"osa"</code>, see <a href="stringdist.html#topic+stringdist-metrics">stringdist::stringdist-metrics</a>.</p>
</td></tr>
<tr><td><code id="step_collapse_stringdist_+3A_options">options</code></td>
<td>
<p>List, other arguments passed to
<code><a href="stringdist.html#topic+stringdist">stringdist::stringdistmatrix()</a></code> such as <code>weight</code>, <code>q</code>, <code>p</code>, and <code>bt</code>, that
are used for different values of <code>method</code>.</p>
</td></tr>
<tr><td><code id="step_collapse_stringdist_+3A_results">results</code></td>
<td>
<p>A list denoting the way the labels should be collapses is
stored here once this preprocessing step has be trained by <a href="recipes.html#topic+prep">recipes::prep</a>.</p>
</td></tr>
<tr><td><code id="step_collapse_stringdist_+3A_columns">columns</code></td>
<td>
<p>A character string of variable names that will be populated
(eventually) by the <code>terms</code> argument.</p>
</td></tr>
<tr><td><code id="step_collapse_stringdist_+3A_skip">skip</code></td>
<td>
<p>A logical. Should the step be skipped when the
recipe is baked by <code><a href="recipes.html#topic+bake">bake()</a></code>? While all operations are baked
when <code><a href="recipes.html#topic+prep">prep()</a></code> is run, some operations may not be able to be
conducted on new data (e.g. processing the outcome variable(s)).
Care should be taken when using <code>skip = TRUE</code> as it may affect
the computations for subsequent operations.</p>
</td></tr>
<tr><td><code id="step_collapse_stringdist_+3A_id">id</code></td>
<td>
<p>A character string that is unique to this step to identify it.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An updated version of <code>recipe</code> with the new step added to the
sequence of existing steps (if any). For the <code>tidy</code> method, a tibble with
columns <code>terms</code> (the columns that will be affected) and <code>base</code>.
</p>


<h3>Tidying</h3>

<p>When you <code><a href="recipes.html#topic+tidy.recipe">tidy()</a></code> this step, a tibble is returned with
columns <code>terms</code>, <code>from</code>, <code>to</code>, and <code>id</code>:
</p>

<dl>
<dt>terms</dt><dd><p>character, the selectors or variables selected</p>
</dd>
<dt>from</dt><dd><p>character, the old levels</p>
</dd>
<dt>too</dt><dd><p>character, the new levels</p>
</dd>
<dt>id</dt><dd><p>character, id of this step</p>
</dd>
</dl>



<h3>Case weights</h3>

<p>The underlying operation does not allow for case weights.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(recipes)
library(tibble)
data0 &lt;- tibble(
  x1 = c("a", "b", "d", "e", "sfgsfgsd", "hjhgfgjgr"),
  x2 = c("ak", "b", "djj", "e", "hjhgfgjgr", "hjhgfgjgr")
)

rec &lt;- recipe(~., data = data0) %&gt;%
  step_collapse_stringdist(all_predictors(), distance = 1) %&gt;%
  prep()

rec %&gt;%
  bake(new_data = NULL)

tidy(rec, 1)

rec &lt;- recipe(~., data = data0) %&gt;%
  step_collapse_stringdist(all_predictors(), distance = 2) %&gt;%
  prep()

rec %&gt;%
  bake(new_data = NULL)

tidy(rec, 1)

</code></pre>

<hr>
<h2 id='step_discretize_cart'>Discretize numeric variables with CART</h2><span id='topic+step_discretize_cart'></span><span id='topic+tidy.step_discretize_cart'></span>

<h3>Description</h3>

<p><code>step_discretize_cart()</code> creates a <em>specification</em> of a recipe step that will
discretize numeric data (e.g. integers or doubles) into bins in a supervised
way using a CART model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>step_discretize_cart(
  recipe,
  ...,
  role = NA,
  trained = FALSE,
  outcome = NULL,
  cost_complexity = 0.01,
  tree_depth = 10,
  min_n = 20,
  rules = NULL,
  skip = FALSE,
  id = rand_id("discretize_cart")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="step_discretize_cart_+3A_recipe">recipe</code></td>
<td>
<p>A recipe object. The step will be added to the sequence of
operations for this recipe.</p>
</td></tr>
<tr><td><code id="step_discretize_cart_+3A_...">...</code></td>
<td>
<p>One or more selector functions to choose which variables are
affected by the step. See <a href="recipes.html#topic+selections">recipes::selections</a> for more details.</p>
</td></tr>
<tr><td><code id="step_discretize_cart_+3A_role">role</code></td>
<td>
<p>Defaults to <code>"predictor"</code>.</p>
</td></tr>
<tr><td><code id="step_discretize_cart_+3A_trained">trained</code></td>
<td>
<p>A logical to indicate if the quantities for preprocessing have
been estimated.</p>
</td></tr>
<tr><td><code id="step_discretize_cart_+3A_outcome">outcome</code></td>
<td>
<p>A call to <code>vars</code> to specify which variable is used as the
outcome to train CART models in order to discretize explanatory variables.</p>
</td></tr>
<tr><td><code id="step_discretize_cart_+3A_cost_complexity">cost_complexity</code></td>
<td>
<p>The regularization parameter. Any split that does not
decrease the overall lack of fit by a factor of <code>cost_complexity</code> is not
attempted. Corresponds to <code>cp</code> in <code><a href="rpart.html#topic+rpart">rpart::rpart()</a></code>. Defaults to 0.01.</p>
</td></tr>
<tr><td><code id="step_discretize_cart_+3A_tree_depth">tree_depth</code></td>
<td>
<p>The <em>maximum</em> depth in the final tree. Corresponds to
<code>maxdepth</code> in  <code><a href="rpart.html#topic+rpart">rpart::rpart()</a></code>. Defaults to 10.</p>
</td></tr>
<tr><td><code id="step_discretize_cart_+3A_min_n">min_n</code></td>
<td>
<p>The number of data points in a node required to continue
splitting. Corresponds to <code>minsplit</code> in  <code><a href="rpart.html#topic+rpart">rpart::rpart()</a></code>. Defaults to 20.</p>
</td></tr>
<tr><td><code id="step_discretize_cart_+3A_rules">rules</code></td>
<td>
<p>The splitting rules of the best CART tree to retain for each
variable. If length zero, splitting could not be used on that column.</p>
</td></tr>
<tr><td><code id="step_discretize_cart_+3A_skip">skip</code></td>
<td>
<p>A logical. Should the step be skipped when the recipe is baked by
<code><a href="recipes.html#topic+bake">recipes::bake()</a></code>? While all operations are baked when <code><a href="recipes.html#topic+prep">recipes::prep()</a></code> is
run, some operations may not be able to be conducted on new data (e.g.
processing the outcome variable(s)). Care should be taken when using <code>skip = TRUE</code> as it may affect the computations for subsequent operations</p>
</td></tr>
<tr><td><code id="step_discretize_cart_+3A_id">id</code></td>
<td>
<p>A character string that is unique to this step to identify it.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>step_discretize_cart()</code> creates non-uniform bins from numerical variables by
utilizing the information about the outcome variable and applying a CART
model.
</p>
<p>The best selection of buckets for each variable is selected using the
standard cost-complexity pruning of CART, which makes this discretization
method resistant to overfitting.
</p>
<p>This step requires the <span class="pkg">rpart</span> package. If not installed, the step will
stop with a note about installing the package.
</p>
<p>Note that the original data will be replaced with the new bins.
</p>


<h3>Value</h3>

<p>An updated version of <code>recipe</code> with the new step added to the
sequence of any existing operations.
</p>


<h3>Tidying</h3>

<p>When you <code><a href="recipes.html#topic+tidy.recipe">tidy()</a></code> this step, a tibble is returned with
columns <code>terms</code>, <code>value</code>, and <code>id</code>:
</p>

<dl>
<dt>terms</dt><dd><p>character, the selectors or variables selected</p>
</dd>
<dt>value</dt><dd><p>numeric, location of the splits</p>
</dd>
<dt>id</dt><dd><p>character, id of this step</p>
</dd>
</dl>



<h3>Tuning Parameters</h3>

<p>This step has 3 tuning parameters:
</p>

<ul>
<li> <p><code>cost_complexity</code>: Cost-Complexity Parameter (type: double, default: 0.01)
</p>
</li>
<li> <p><code>tree_depth</code>: Tree Depth (type: integer, default: 10)
</p>
</li>
<li> <p><code>min_n</code>: Minimal Node Size (type: integer, default: 20)
</p>
</li></ul>



<h3>Case weights</h3>

<p>This step performs an supervised operation that can utilize case weights.
To use them, see the documentation in <a href="recipes.html#topic+case_weights">recipes::case_weights</a> and the examples on
<code>tidymodels.org</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+step_discretize_xgb">step_discretize_xgb()</a></code>, <code><a href="recipes.html#topic+recipe">recipes::recipe()</a></code>,
<code><a href="recipes.html#topic+prep">recipes::prep()</a></code>, <code><a href="recipes.html#topic+bake">recipes::bake()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(modeldata)
data(ad_data)
library(rsample)

split &lt;- initial_split(ad_data, strata = "Class")

ad_data_tr &lt;- training(split)
ad_data_te &lt;- testing(split)

cart_rec &lt;-
  recipe(Class ~ ., data = ad_data_tr) %&gt;%
  step_discretize_cart(
    tau, age, p_tau, Ab_42,
    outcome = "Class", id = "cart splits"
  )

cart_rec &lt;- prep(cart_rec, training = ad_data_tr)

# The splits:
tidy(cart_rec, id = "cart splits")

bake(cart_rec, ad_data_te, tau)

</code></pre>

<hr>
<h2 id='step_discretize_xgb'>Discretize numeric variables with XgBoost</h2><span id='topic+step_discretize_xgb'></span><span id='topic+tidy.step_discretize_xgb'></span>

<h3>Description</h3>

<p><code>step_discretize_xgb()</code> creates a <em>specification</em> of a recipe step that will
discretize numeric data (e.g. integers or doubles) into bins in a supervised
way using an XgBoost model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>step_discretize_xgb(
  recipe,
  ...,
  role = NA,
  trained = FALSE,
  outcome = NULL,
  sample_val = 0.2,
  learn_rate = 0.3,
  num_breaks = 10,
  tree_depth = 1,
  min_n = 5,
  rules = NULL,
  skip = FALSE,
  id = rand_id("discretize_xgb")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="step_discretize_xgb_+3A_recipe">recipe</code></td>
<td>
<p>A recipe object. The step will be added to the sequence of
operations for this recipe.</p>
</td></tr>
<tr><td><code id="step_discretize_xgb_+3A_...">...</code></td>
<td>
<p>One or more selector functions to choose which variables are
affected by the step. See <a href="recipes.html#topic+selections">recipes::selections</a> for more details.</p>
</td></tr>
<tr><td><code id="step_discretize_xgb_+3A_role">role</code></td>
<td>
<p>Defaults to <code>"predictor"</code>.</p>
</td></tr>
<tr><td><code id="step_discretize_xgb_+3A_trained">trained</code></td>
<td>
<p>A logical to indicate if the quantities for preprocessing have
been estimated.</p>
</td></tr>
<tr><td><code id="step_discretize_xgb_+3A_outcome">outcome</code></td>
<td>
<p>A call to <code>vars</code> to specify which variable is used as the
outcome to train XgBoost models in order to discretize explanatory
variables.</p>
</td></tr>
<tr><td><code id="step_discretize_xgb_+3A_sample_val">sample_val</code></td>
<td>
<p>Share of data used for validation (with early stopping) of
the learned splits (the rest is used for training). Defaults to 0.20.</p>
</td></tr>
<tr><td><code id="step_discretize_xgb_+3A_learn_rate">learn_rate</code></td>
<td>
<p>The rate at which the boosting algorithm adapts from
iteration-to-iteration. Corresponds to <code>eta</code> in the <span class="pkg">xgboost</span> package.
Defaults to 0.3.</p>
</td></tr>
<tr><td><code id="step_discretize_xgb_+3A_num_breaks">num_breaks</code></td>
<td>
<p>The <em>maximum</em> number of discrete bins to bucket continuous
features. Corresponds to <code>max_bin</code> in the <span class="pkg">xgboost</span> package. Defaults
to 10.</p>
</td></tr>
<tr><td><code id="step_discretize_xgb_+3A_tree_depth">tree_depth</code></td>
<td>
<p>The maximum depth of the tree (i.e. number of splits).
Corresponds to <code>max_depth</code> in the <span class="pkg">xgboost</span> package. Defaults to 1.</p>
</td></tr>
<tr><td><code id="step_discretize_xgb_+3A_min_n">min_n</code></td>
<td>
<p>The minimum number of instances needed to be in each node.
Corresponds to <code>min_child_weight</code> in the <span class="pkg">xgboost</span> package. Defaults to
5.</p>
</td></tr>
<tr><td><code id="step_discretize_xgb_+3A_rules">rules</code></td>
<td>
<p>The splitting rules of the best XgBoost tree to retain for each
variable.</p>
</td></tr>
<tr><td><code id="step_discretize_xgb_+3A_skip">skip</code></td>
<td>
<p>A logical. Should the step be skipped when the recipe is baked by
<code><a href="recipes.html#topic+bake">recipes::bake()</a></code>? While all operations are baked when <code><a href="recipes.html#topic+prep">recipes::prep()</a></code> is
run, some operations may not be able to be conducted on new data (e.g.
processing the outcome variable(s)). Care should be taken when using <code>skip = TRUE</code> as it may affect the computations for subsequent operations</p>
</td></tr>
<tr><td><code id="step_discretize_xgb_+3A_id">id</code></td>
<td>
<p>A character string that is unique to this step to identify it.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>step_discretize_xgb()</code> creates non-uniform bins from numerical variables by
utilizing the information about the outcome variable and applying the xgboost
model. It is advised to impute missing values before this step. This step is
intended to be used particularly with linear models because thanks to
creating non-uniform bins it becomes easier to learn non-linear patterns from
the data.
</p>
<p>The best selection of buckets for each variable is selected using an internal
early stopping scheme implemented in the <span class="pkg">xgboost</span> package, which makes
this discretization method prone to overfitting.
</p>
<p>The pre-defined values of the underlying xgboost learns good and reasonably
complex results. However, if one wishes to tune them the recommended path
would be to first start with changing the value of <code>num_breaks</code> to e.g.: 20
or 30. If that doesn't give satisfactory results one could experiment with
modifying the <code>tree_depth</code> or <code>min_n</code> parameters. Note that it is not
recommended to tune <code>learn_rate</code> simultaneously with other parameters.
</p>
<p>This step requires the <span class="pkg">xgboost</span> package. If not installed, the step will
stop with a note about installing the package.
</p>
<p>Note that the original data will be replaced with the new bins.
</p>


<h3>Value</h3>

<p>An updated version of <code>recipe</code> with the new step added to the
sequence of any existing operations.
</p>


<h3>Tidying</h3>

<p>When you <code><a href="recipes.html#topic+tidy.recipe">tidy()</a></code> this step, a tibble is returned with
columns <code>terms</code>, <code>value</code>, and <code>id</code>:
</p>

<dl>
<dt>terms</dt><dd><p>character, the selectors or variables selected</p>
</dd>
<dt>value</dt><dd><p>numeric, location of the splits</p>
</dd>
<dt>id</dt><dd><p>character, id of this step</p>
</dd>
</dl>



<h3>Tuning Parameters</h3>

<p>This step has 5 tuning parameters:
</p>

<ul>
<li> <p><code>sample_val</code>: Proportion of data for validation (type: double, default: 0.2)
</p>
</li>
<li> <p><code>learn_rate</code>: Learning Rate (type: double, default: 0.3)
</p>
</li>
<li> <p><code>num_breaks</code>: Number of Cut Points (type: integer, default: 10)
</p>
</li>
<li> <p><code>tree_depth</code>: Tree Depth (type: integer, default: 1)
</p>
</li>
<li> <p><code>min_n</code>: Minimal Node Size (type: integer, default: 5)
</p>
</li></ul>



<h3>Case weights</h3>

<p>This step performs an supervised operation that can utilize case weights.
To use them, see the documentation in <a href="recipes.html#topic+case_weights">recipes::case_weights</a> and the examples on
<code>tidymodels.org</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+step_discretize_cart">step_discretize_cart()</a></code>, <code><a href="recipes.html#topic+recipe">recipes::recipe()</a></code>,
<code><a href="recipes.html#topic+prep">recipes::prep()</a></code>, <code><a href="recipes.html#topic+bake">recipes::bake()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(rsample)
library(recipes)
data(credit_data, package = "modeldata")

set.seed(1234)
split &lt;- initial_split(credit_data[1:1000, ], strata = "Status")

credit_data_tr &lt;- training(split)
credit_data_te &lt;- testing(split)

xgb_rec &lt;-
  recipe(Status ~ Income + Assets, data = credit_data_tr) %&gt;%
  step_impute_median(Income, Assets) %&gt;%
  step_discretize_xgb(Income, Assets, outcome = "Status")

xgb_rec &lt;- prep(xgb_rec, training = credit_data_tr)

bake(xgb_rec, credit_data_te, Assets)

</code></pre>

<hr>
<h2 id='step_embed'>Encoding Factors into Multiple Columns</h2><span id='topic+step_embed'></span><span id='topic+tidy.step_embed'></span><span id='topic+embed_control'></span>

<h3>Description</h3>

<p><code>step_embed()</code> creates a <em>specification</em> of a recipe step that will convert a
nominal (i.e. factor) predictor into a set of scores derived from a
tensorflow model via a word-embedding model. <code>embed_control</code> is a simple
wrapper for setting default options.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>step_embed(
  recipe,
  ...,
  role = "predictor",
  trained = FALSE,
  outcome = NULL,
  predictors = NULL,
  num_terms = 2,
  hidden_units = 0,
  options = embed_control(),
  mapping = NULL,
  history = NULL,
  keep_original_cols = FALSE,
  skip = FALSE,
  id = rand_id("embed")
)

embed_control(
  loss = "mse",
  metrics = NULL,
  optimizer = "sgd",
  epochs = 20,
  validation_split = 0,
  batch_size = 32,
  verbose = 0,
  callbacks = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="step_embed_+3A_recipe">recipe</code></td>
<td>
<p>A recipe object. The step will be added to the sequence of
operations for this recipe.</p>
</td></tr>
<tr><td><code id="step_embed_+3A_...">...</code></td>
<td>
<p>One or more selector functions to choose variables. For
<code>step_embed</code>, this indicates the variables to be encoded into a numeric
format. See <code><a href="recipes.html#topic+selections">recipes::selections()</a></code> for more details. For the <code>tidy</code>
method, these are not currently used.</p>
</td></tr>
<tr><td><code id="step_embed_+3A_role">role</code></td>
<td>
<p>For model terms created by this step, what analysis role should
they be assigned?. By default, the function assumes that the embedding
variables created will be used as predictors in a model.</p>
</td></tr>
<tr><td><code id="step_embed_+3A_trained">trained</code></td>
<td>
<p>A logical to indicate if the quantities for preprocessing have
been estimated.</p>
</td></tr>
<tr><td><code id="step_embed_+3A_outcome">outcome</code></td>
<td>
<p>A call to <code>vars</code> to specify which variable is used as the
outcome in the neural network.</p>
</td></tr>
<tr><td><code id="step_embed_+3A_predictors">predictors</code></td>
<td>
<p>An optional call to <code>vars</code> to specify any variables to be
added as additional predictors in the neural network. These variables
should be numeric and perhaps centered and scaled.</p>
</td></tr>
<tr><td><code id="step_embed_+3A_num_terms">num_terms</code></td>
<td>
<p>An integer for the number of resulting variables.</p>
</td></tr>
<tr><td><code id="step_embed_+3A_hidden_units">hidden_units</code></td>
<td>
<p>An integer for the number of hidden units in a dense ReLu
layer between the embedding and output later. Use a value of zero for no
intermediate layer (see Details below).</p>
</td></tr>
<tr><td><code id="step_embed_+3A_options">options</code></td>
<td>
<p>A list of options for the model fitting process.</p>
</td></tr>
<tr><td><code id="step_embed_+3A_mapping">mapping</code></td>
<td>
<p>A list of tibble results that define the encoding. This is
<code>NULL</code> until the step is trained by <code><a href="recipes.html#topic+prep">recipes::prep()</a></code>.</p>
</td></tr>
<tr><td><code id="step_embed_+3A_history">history</code></td>
<td>
<p>A tibble with the convergence statistics for each term. This
is <code>NULL</code> until the step is trained by <code><a href="recipes.html#topic+prep">recipes::prep()</a></code>.</p>
</td></tr>
<tr><td><code id="step_embed_+3A_keep_original_cols">keep_original_cols</code></td>
<td>
<p>A logical to keep the original variables in the
output. Defaults to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="step_embed_+3A_skip">skip</code></td>
<td>
<p>A logical. Should the step be skipped when the recipe is baked by
<code><a href="recipes.html#topic+bake">recipes::bake()</a></code>? While all operations are baked when <code><a href="recipes.html#topic+prep">recipes::prep()</a></code> is
run, some operations may not be able to be conducted on new data (e.g.
processing the outcome variable(s)). Care should be taken when using <code>skip = TRUE</code> as it may affect the computations for subsequent operations.</p>
</td></tr>
<tr><td><code id="step_embed_+3A_id">id</code></td>
<td>
<p>A character string that is unique to this step to identify it.</p>
</td></tr>
<tr><td><code id="step_embed_+3A_optimizer">optimizer</code>, <code id="step_embed_+3A_loss">loss</code>, <code id="step_embed_+3A_metrics">metrics</code></td>
<td>
<p>Arguments to pass to keras::compile()</p>
</td></tr>
<tr><td><code id="step_embed_+3A_epochs">epochs</code>, <code id="step_embed_+3A_validation_split">validation_split</code>, <code id="step_embed_+3A_batch_size">batch_size</code>, <code id="step_embed_+3A_verbose">verbose</code>, <code id="step_embed_+3A_callbacks">callbacks</code></td>
<td>
<p>Arguments to pass
to keras::fit()</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Factor levels are initially assigned at random to the new variables and these
variables are used in a neural network to optimize both the allocation of
levels to new columns as well as estimating a model to predict the outcome.
See Section 6.1.2 of Francois and Allaire (2018) for more details.
</p>
<p>The new variables are mapped to the specific levels seen at the time of model
training and an extra instance of the variables are used for new levels of
the factor.
</p>
<p>One model is created for each call to <code>step_embed</code>. All terms given to the
step are estimated and encoded in the same model which would also contain
predictors give in <code>predictors</code> (if any).
</p>
<p>When the outcome is numeric, a linear activation function is used in the last
layer while softmax is used for factor outcomes (with any number of levels).
</p>
<p>For example, the <code>keras</code> code for a numeric outcome, one categorical
predictor, and no hidden units used here would be
</p>
<div class="sourceCode"><pre>  keras_model_sequential() %&gt;%
  layer_embedding(
    input_dim = num_factor_levels_x + 1,
    output_dim = num_terms,
    input_length = 1
  ) %&gt;%
  layer_flatten() %&gt;%
  layer_dense(units = 1, activation = 'linear')
</pre></div>
<p>If a factor outcome is used and hidden units were requested, the code would
be
</p>
<div class="sourceCode"><pre>  keras_model_sequential() %&gt;%
  layer_embedding(
    input_dim = num_factor_levels_x + 1,
    output_dim = num_terms,
    input_length = 1
   ) %&gt;%
  layer_flatten() %&gt;%
  layer_dense(units = hidden_units, activation = "relu") %&gt;%
  layer_dense(units = num_factor_levels_y, activation = 'softmax')
</pre></div>
<p>Other variables specified by <code>predictors</code> are added as an additional dense
layer after <code>layer_flatten</code> and before the hidden layer.
</p>
<p>Also note that it may be difficult to obtain reproducible results using this
step due to the nature of Tensorflow (see link in References).
</p>
<p>tensorflow models cannot be run in parallel within the same session (via
<code>foreach</code> or <code>futures</code>) or the <code>parallel</code> package. If using a recipes with
this step with <code>caret</code>, avoid parallel processing.
</p>


<h3>Value</h3>

<p>An updated version of <code>recipe</code> with the new step added to the
sequence of existing steps (if any). For the <code>tidy</code> method, a tibble with
columns <code>terms</code> (the selectors or variables for encoding), <code>level</code> (the
factor levels), and several columns containing <code>embed</code> in the name.
</p>


<h3>Tidying</h3>

<p>When you <code><a href="recipes.html#topic+tidy.recipe">tidy()</a></code> this step, a tibble is returned with
a number of columns with embedding information, and columns <code>terms</code>,
<code>levels</code>, and <code>id</code>:
</p>

<dl>
<dt>terms</dt><dd><p>character, the selectors or variables selected</p>
</dd>
<dt>levels</dt><dd><p>character, levels in variable</p>
</dd>
<dt>id</dt><dd><p>character, id of this step</p>
</dd>
</dl>



<h3>Tuning Parameters</h3>

<p>This step has 2 tuning parameters:
</p>

<ul>
<li> <p><code>num_terms</code>: # Model Terms (type: integer, default: 2)
</p>
</li>
<li> <p><code>hidden_units</code>: # Hidden Units (type: integer, default: 0)
</p>
</li></ul>



<h3>Case weights</h3>

<p>The underlying operation does not allow for case weights.
</p>


<h3>References</h3>

<p>Francois C and Allaire JJ (2018) <em>Deep Learning with R</em>, Manning
</p>
<p>&quot;Concatenate Embeddings for Categorical Variables with Keras&quot;
<a href="https://flovv.github.io/Embeddings_with_keras_part2/">https://flovv.github.io/Embeddings_with_keras_part2/</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(grants, package = "modeldata")

set.seed(1)
grants_other &lt;- sample_n(grants_other, 500)

rec &lt;- recipe(class ~ num_ci + sponsor_code, data = grants_other) %&gt;%
  step_embed(sponsor_code,
    outcome = vars(class),
    options = embed_control(epochs = 10)
  )

</code></pre>

<hr>
<h2 id='step_feature_hash'>Dummy Variables Creation via Feature Hashing</h2><span id='topic+step_feature_hash'></span><span id='topic+tidy.step_feature_hash'></span>

<h3>Description</h3>

<p><a href="https://lifecycle.r-lib.org/articles/stages.html#soft-deprecated"><img src="../help/figures/lifecycle-soft-deprecated.svg" alt='[Soft-deprecated]' /></a>
</p>
<p><code>step_feature_hash()</code> is being deprecated in favor of
<code><a href="textrecipes.html#topic+step_dummy_hash">textrecipes::step_dummy_hash()</a></code>. This function creates a <em>specification</em>
of a recipe step that will convert nominal data (e.g. character or factors)
into one or more numeric binary columns using the levels of the original
data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>step_feature_hash(
  recipe,
  ...,
  role = "predictor",
  trained = FALSE,
  num_hash = 2^6,
  preserve = deprecated(),
  columns = NULL,
  keep_original_cols = FALSE,
  skip = FALSE,
  id = rand_id("feature_hash")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="step_feature_hash_+3A_recipe">recipe</code></td>
<td>
<p>A recipe object. The step will be added to the
sequence of operations for this recipe.</p>
</td></tr>
<tr><td><code id="step_feature_hash_+3A_...">...</code></td>
<td>
<p>One or more selector functions to choose variables
for this step. See <code><a href="recipes.html#topic+selections">selections()</a></code> for more details.</p>
</td></tr>
<tr><td><code id="step_feature_hash_+3A_role">role</code></td>
<td>
<p>For model terms created by this step, what analysis role should
they be assigned? By default, the new columns created by this step from
the original variables will be used as <em>predictors</em> in a model.</p>
</td></tr>
<tr><td><code id="step_feature_hash_+3A_trained">trained</code></td>
<td>
<p>A logical to indicate if the quantities for
preprocessing have been estimated.</p>
</td></tr>
<tr><td><code id="step_feature_hash_+3A_num_hash">num_hash</code></td>
<td>
<p>The number of resulting dummy variable columns.</p>
</td></tr>
<tr><td><code id="step_feature_hash_+3A_preserve">preserve</code></td>
<td>
<p>Use <code>keep_original_cols</code> instead to specify whether the
selected column(s) should be retained in addition to the new dummy
variables.</p>
</td></tr>
<tr><td><code id="step_feature_hash_+3A_columns">columns</code></td>
<td>
<p>A character vector for the selected columns. This is <code>NULL</code>
until the step is trained by <code><a href="recipes.html#topic+prep">recipes::prep()</a></code>.</p>
</td></tr>
<tr><td><code id="step_feature_hash_+3A_keep_original_cols">keep_original_cols</code></td>
<td>
<p>A logical to keep the original variables in the
output. Defaults to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="step_feature_hash_+3A_skip">skip</code></td>
<td>
<p>A logical. Should the step be skipped when the
recipe is baked by <code><a href="recipes.html#topic+bake">bake()</a></code>? While all operations are baked
when <code><a href="recipes.html#topic+prep">prep()</a></code> is run, some operations may not be able to be
conducted on new data (e.g. processing the outcome variable(s)).
Care should be taken when using <code>skip = TRUE</code> as it may affect
the computations for subsequent operations.</p>
</td></tr>
<tr><td><code id="step_feature_hash_+3A_id">id</code></td>
<td>
<p>A character string that is unique to this step to identify it.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>step_feature_hash()</code> will create a set of binary dummy variables from a
factor or character variable. The values themselves are used to determine
which row that the dummy variable should be assigned (as opposed to having a
specific column that the value will map to).
</p>
<p>Since this method does not rely on a pre-determined assignment of levels to
columns, new factor levels can be added to the selected columns without
issue. Missing values result in missing values for all of the hashed columns.
</p>
<p>Note that the assignment of the levels to the hashing columns does not try to
maximize the allocation. It is likely that multiple levels of the column will
map to the same hashed columns (even with small data sets). Similarly, it is
likely that some columns will have all zeros. A zero-variance filter (via
<code><a href="recipes.html#topic+step_zv">recipes::step_zv()</a></code>) is recommended for any recipe that uses hashed columns.
</p>


<h3>Value</h3>

<p>An updated version of <code>recipe</code> with the new step added to the
sequence of any existing operations.
</p>


<h3>Tidying</h3>

<p>When you <code><a href="recipes.html#topic+tidy.recipe">tidy()</a></code> this step, a tibble is returned with
columns <code>terms</code> and <code>id</code>:
</p>

<dl>
<dt>terms</dt><dd><p>character, the selectors or variables selected</p>
</dd>
<dt>id</dt><dd><p>character, id of this step</p>
</dd>
</dl>



<h3>Case weights</h3>

<p>The underlying operation does not allow for case weights.
</p>


<h3>References</h3>

<p>Weinberger, K, A Dasgupta, J Langford, A Smola, and J Attenberg. 2009.
&quot;Feature Hashing for Large Scale Multitask Learning.&quot; In Proceedings of the
26th Annual International Conference on Machine Learning, 1113–20. ACM.
</p>
<p>Kuhn and Johnson (2020) <em>Feature Engineering and Selection: A Practical
Approach for Predictive Models</em>. CRC/Chapman Hall
<a href="https://bookdown.org/max/FES/encoding-predictors-with-many-categories.html">https://bookdown.org/max/FES/encoding-predictors-with-many-categories.html</a>
</p>


<h3>See Also</h3>

<p><code><a href="recipes.html#topic+step_dummy">recipes::step_dummy()</a></code>, <code><a href="recipes.html#topic+step_zv">recipes::step_zv()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(grants, package = "modeldata")
rec &lt;-
  recipe(class ~ sponsor_code, data = grants_other) %&gt;%
  step_feature_hash(
    sponsor_code,
    num_hash = 2^6, keep_original_cols = TRUE
  ) %&gt;%
  prep()

# How many of the 298 locations ended up in each hash column?
results &lt;-
  bake(rec, new_data = NULL, starts_with("sponsor_code")) %&gt;%
  distinct()

apply(results %&gt;% select(-sponsor_code), 2, sum) %&gt;% table()

</code></pre>

<hr>
<h2 id='step_lencode_bayes'>Supervised Factor Conversions into Linear Functions using Bayesian Likelihood
Encodings</h2><span id='topic+step_lencode_bayes'></span><span id='topic+tidy.step_lencode_bayes'></span>

<h3>Description</h3>

<p><code>step_lencode_bayes()</code> creates a <em>specification</em> of a recipe step that will
convert a nominal (i.e. factor) predictor into a single set of scores derived
from a generalized linear model estimated using Bayesian analysis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>step_lencode_bayes(
  recipe,
  ...,
  role = NA,
  trained = FALSE,
  outcome = NULL,
  options = list(seed = sample.int(10^5, 1)),
  verbose = FALSE,
  mapping = NULL,
  skip = FALSE,
  id = rand_id("lencode_bayes")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="step_lencode_bayes_+3A_recipe">recipe</code></td>
<td>
<p>A recipe object. The step will be added to the sequence of
operations for this recipe.</p>
</td></tr>
<tr><td><code id="step_lencode_bayes_+3A_...">...</code></td>
<td>
<p>One or more selector functions to choose variables. For
<code>step_lencode_bayes</code>, this indicates the variables to be encoded into a
numeric format. See <code><a href="recipes.html#topic+selections">recipes::selections()</a></code> for more details. For the
<code>tidy</code> method, these are not currently used.</p>
</td></tr>
<tr><td><code id="step_lencode_bayes_+3A_role">role</code></td>
<td>
<p>Not used by this step since no new variables are created.</p>
</td></tr>
<tr><td><code id="step_lencode_bayes_+3A_trained">trained</code></td>
<td>
<p>A logical to indicate if the quantities for preprocessing have
been estimated.</p>
</td></tr>
<tr><td><code id="step_lencode_bayes_+3A_outcome">outcome</code></td>
<td>
<p>A call to <code>vars</code> to specify which variable is used as the
outcome in the generalized linear model. Only numeric and two-level factors
are currently supported.</p>
</td></tr>
<tr><td><code id="step_lencode_bayes_+3A_options">options</code></td>
<td>
<p>A list of options to pass to <code><a href="rstanarm.html#topic+stan_glmer">rstanarm::stan_glmer()</a></code>.</p>
</td></tr>
<tr><td><code id="step_lencode_bayes_+3A_verbose">verbose</code></td>
<td>
<p>A logical to control the default printing by
<code><a href="rstanarm.html#topic+stan_glmer">rstanarm::stan_glmer()</a></code>.</p>
</td></tr>
<tr><td><code id="step_lencode_bayes_+3A_mapping">mapping</code></td>
<td>
<p>A list of tibble results that define the encoding. This is
<code>NULL</code> until the step is trained by <code><a href="recipes.html#topic+prep">recipes::prep()</a></code>.</p>
</td></tr>
<tr><td><code id="step_lencode_bayes_+3A_skip">skip</code></td>
<td>
<p>A logical. Should the step be skipped when the recipe is baked by
<code><a href="recipes.html#topic+bake">recipes::bake()</a></code>? While all operations are baked when <code><a href="recipes.html#topic+prep">recipes::prep()</a></code> is
run, some operations may not be able to be conducted on new data (e.g.
processing the outcome variable(s)). Care should be taken when using <code>skip = TRUE</code> as it may affect the computations for subsequent operations</p>
</td></tr>
<tr><td><code id="step_lencode_bayes_+3A_id">id</code></td>
<td>
<p>A character string that is unique to this step to identify it.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For each factor predictor, a generalized linear model is fit to the outcome
and the coefficients are returned as the encoding. These coefficients are on
the linear predictor scale so, for factor outcomes, they are in log-odds
units. The coefficients are created using a no intercept model and, when two
factor outcomes are used, the log-odds reflect the event of interest being
the <em>first</em> level of the factor.
</p>
<p>For novel levels, a slightly timmed average of the coefficients is returned.
</p>
<p>A hierarchical generalized linear model is fit using <code><a href="rstanarm.html#topic+stan_glmer">rstanarm::stan_glmer()</a></code>
and no intercept via
</p>
<div class="sourceCode"><pre>  stan_glmer(outcome ~ (1 | predictor), data = data, ...)
</pre></div>
<p>where the <code>...</code> include the <code>family</code> argument (automatically set by the step,
unless passed in by <code>options</code>) as well as any arguments given to the
<code>options</code> argument to the step. Relevant options include <code>chains</code>, <code>iter</code>,
<code>cores</code>, and arguments for the priors (see the links in the References
below). <code>prior_intercept</code> is the argument that has the most effect on the
amount of shrinkage.
</p>


<h3>Value</h3>

<p>An updated version of <code>recipe</code> with the new step added to the
sequence of existing steps (if any). For the <code>tidy</code> method, a tibble with
columns <code>terms</code> (the selectors or variables for encoding), <code>level</code> (the
factor levels), and <code>value</code> (the encodings).
</p>


<h3>Tidying</h3>

<p>When you <code><a href="recipes.html#topic+tidy.recipe">tidy()</a></code> this step, a tibble is returned with
columns <code>level</code>, <code>value</code>, <code>terms</code>, and <code>id</code>:
</p>

<dl>
<dt>level</dt><dd><p>character, the factor levels</p>
</dd>
<dt>value</dt><dd><p>numeric, the encoding</p>
</dd>
<dt>terms</dt><dd><p>character, the selectors or variables selected</p>
</dd>
<dt>id</dt><dd><p>character, id of this step</p>
</dd>
</dl>



<h3>Case weights</h3>

<p>This step performs an supervised operation that can utilize case weights.
To use them, see the documentation in <a href="recipes.html#topic+case_weights">recipes::case_weights</a> and the examples on
<code>tidymodels.org</code>.
</p>


<h3>References</h3>

<p>Micci-Barreca D (2001) &quot;A preprocessing scheme for high-cardinality
categorical attributes in classification and prediction problems,&quot; ACM SIGKDD
Explorations Newsletter, 3(1), 27-32.
</p>
<p>Zumel N and Mount J (2017) &quot;vtreat: a data.frame Processor for Predictive
Modeling,&quot; arXiv:1611.09477
</p>
<p>&quot;Hierarchical Partial Pooling for Repeated Binary Trials&quot;
<a href="https://CRAN.R-project.org/package=rstanarm/vignettes/pooling.html">https://CRAN.R-project.org/package=rstanarm/vignettes/pooling.html</a>
</p>
<p>&quot;Prior Distributions for <code>rstanarm</code> Models&quot;
<a href="http://mc-stan.org/rstanarm/reference/priors.html">http://mc-stan.org/rstanarm/reference/priors.html</a>
</p>
<p>&quot;Estimating Generalized (Non-)Linear Models with Group-Specific Terms with
<code>rstanarm</code>&quot; <a href="http://mc-stan.org/rstanarm/articles/glmer.html">http://mc-stan.org/rstanarm/articles/glmer.html</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(recipes)
library(dplyr)
library(modeldata)

data(grants)

set.seed(1)
grants_other &lt;- sample_n(grants_other, 500)

reencoded &lt;- recipe(class ~ sponsor_code, data = grants_other) %&gt;%
  step_lencode_bayes(sponsor_code, outcome = vars(class))


</code></pre>

<hr>
<h2 id='step_lencode_glm'>Supervised Factor Conversions into Linear Functions using Likelihood
Encodings</h2><span id='topic+step_lencode_glm'></span><span id='topic+tidy.step_lencode_glm'></span>

<h3>Description</h3>

<p><code>step_lencode_glm()</code> creates a <em>specification</em> of a recipe step that will
convert a nominal (i.e. factor) predictor into a single set of scores derived
from a generalized linear model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>step_lencode_glm(
  recipe,
  ...,
  role = NA,
  trained = FALSE,
  outcome = NULL,
  mapping = NULL,
  skip = FALSE,
  id = rand_id("lencode_glm")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="step_lencode_glm_+3A_recipe">recipe</code></td>
<td>
<p>A recipe object. The step will be added to the sequence of
operations for this recipe.</p>
</td></tr>
<tr><td><code id="step_lencode_glm_+3A_...">...</code></td>
<td>
<p>One or more selector functions to choose variables. For
<code>step_lencode_glm</code>, this indicates the variables to be encoded into a
numeric format. See <code><a href="recipes.html#topic+selections">recipes::selections()</a></code> for more details. For the
<code>tidy</code> method, these are not currently used.</p>
</td></tr>
<tr><td><code id="step_lencode_glm_+3A_role">role</code></td>
<td>
<p>Not used by this step since no new variables are created.</p>
</td></tr>
<tr><td><code id="step_lencode_glm_+3A_trained">trained</code></td>
<td>
<p>A logical to indicate if the quantities for preprocessing have
been estimated.</p>
</td></tr>
<tr><td><code id="step_lencode_glm_+3A_outcome">outcome</code></td>
<td>
<p>A call to <code>vars</code> to specify which variable is used as the
outcome in the generalized linear model. Only numeric and two-level factors
are currently supported.</p>
</td></tr>
<tr><td><code id="step_lencode_glm_+3A_mapping">mapping</code></td>
<td>
<p>A list of tibble results that define the encoding. This is
<code>NULL</code> until the step is trained by <code><a href="recipes.html#topic+prep">recipes::prep()</a></code>.</p>
</td></tr>
<tr><td><code id="step_lencode_glm_+3A_skip">skip</code></td>
<td>
<p>A logical. Should the step be skipped when the recipe is baked by
<code><a href="recipes.html#topic+bake">recipes::bake()</a></code>? While all operations are baked when <code><a href="recipes.html#topic+prep">recipes::prep()</a></code> is
run, some operations may not be able to be conducted on new data (e.g.
processing the outcome variable(s)). Care should be taken when using <code>skip = TRUE</code> as it may affect the computations for subsequent operations</p>
</td></tr>
<tr><td><code id="step_lencode_glm_+3A_id">id</code></td>
<td>
<p>A character string that is unique to this step to identify it.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For each factor predictor, a generalized linear model is fit to the outcome
and the coefficients are returned as the encoding. These coefficients are on
the linear predictor scale so, for factor outcomes, they are in log-odds
units. The coefficients are created using a no intercept model and, when two
factor outcomes are used, the log-odds reflect the event of interest being
the <em>first</em> level of the factor.
</p>
<p>For novel levels, a slightly timmed average of the coefficients is returned.
</p>


<h3>Value</h3>

<p>An updated version of <code>recipe</code> with the new step added to the
sequence of existing steps (if any). For the <code>tidy</code> method, a tibble with
columns <code>terms</code> (the selectors or variables for encoding), <code>level</code> (the
factor levels), and <code>value</code> (the encodings).
</p>


<h3>Tidying</h3>

<p>When you <code><a href="recipes.html#topic+tidy.recipe">tidy()</a></code> this step, a tibble is returned with
columns <code>level</code>, <code>value</code>, <code>terms</code>, and <code>id</code>:
</p>

<dl>
<dt>level</dt><dd><p>character, the factor levels</p>
</dd>
<dt>value</dt><dd><p>numeric, the encoding</p>
</dd>
<dt>terms</dt><dd><p>character, the selectors or variables selected</p>
</dd>
<dt>id</dt><dd><p>character, id of this step</p>
</dd>
</dl>



<h3>Case weights</h3>

<p>This step performs an supervised operation that can utilize case weights.
To use them, see the documentation in <a href="recipes.html#topic+case_weights">recipes::case_weights</a> and the examples on
<code>tidymodels.org</code>.
</p>


<h3>References</h3>

<p>Micci-Barreca D (2001) &quot;A preprocessing scheme for high-cardinality
categorical attributes in classification and prediction problems,&quot; ACM SIGKDD
Explorations Newsletter, 3(1), 27-32.
</p>
<p>Zumel N and Mount J (2017) &quot;vtreat: a data.frame Processor for Predictive
Modeling,&quot; arXiv:1611.09477
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(recipes)
library(dplyr)
library(modeldata)

data(grants)

set.seed(1)
grants_other &lt;- sample_n(grants_other, 500)

reencoded &lt;- recipe(class ~ sponsor_code, data = grants_other) %&gt;%
  step_lencode_glm(sponsor_code, outcome = vars(class))


</code></pre>

<hr>
<h2 id='step_lencode_mixed'>Supervised Factor Conversions into Linear Functions using Bayesian Likelihood
Encodings</h2><span id='topic+step_lencode_mixed'></span><span id='topic+tidy.step_lencode_mixed'></span>

<h3>Description</h3>

<p><code>step_lencode_mixed()</code> creates a <em>specification</em> of a recipe step that will
convert a nominal (i.e. factor) predictor into a single set of scores derived
from a generalized linear mixed model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>step_lencode_mixed(
  recipe,
  ...,
  role = NA,
  trained = FALSE,
  outcome = NULL,
  options = list(verbose = 0),
  mapping = NULL,
  skip = FALSE,
  id = rand_id("lencode_mixed")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="step_lencode_mixed_+3A_recipe">recipe</code></td>
<td>
<p>A recipe object. The step will be added to the sequence of
operations for this recipe.</p>
</td></tr>
<tr><td><code id="step_lencode_mixed_+3A_...">...</code></td>
<td>
<p>One or more selector functions to choose variables. For
<code>step_lencode_mixed</code>, this indicates the variables to be encoded into a
numeric format. See <code><a href="recipes.html#topic+selections">recipes::selections()</a></code> for more details. For the
<code>tidy</code> method, these are not currently used.</p>
</td></tr>
<tr><td><code id="step_lencode_mixed_+3A_role">role</code></td>
<td>
<p>Not used by this step since no new variables are created.</p>
</td></tr>
<tr><td><code id="step_lencode_mixed_+3A_trained">trained</code></td>
<td>
<p>A logical to indicate if the quantities for preprocessing have
been estimated.</p>
</td></tr>
<tr><td><code id="step_lencode_mixed_+3A_outcome">outcome</code></td>
<td>
<p>A call to <code>vars</code> to specify which variable is used as the
outcome in the generalized linear model. Only numeric and two-level factors
are currently supported.</p>
</td></tr>
<tr><td><code id="step_lencode_mixed_+3A_options">options</code></td>
<td>
<p>A list of options to pass to <code><a href="lme4.html#topic+lmer">lme4::lmer()</a></code> or
<code><a href="lme4.html#topic+glmer">lme4::glmer()</a></code>.</p>
</td></tr>
<tr><td><code id="step_lencode_mixed_+3A_mapping">mapping</code></td>
<td>
<p>A list of tibble results that define the encoding. This is
<code>NULL</code> until the step is trained by <code><a href="recipes.html#topic+prep">recipes::prep()</a></code>.</p>
</td></tr>
<tr><td><code id="step_lencode_mixed_+3A_skip">skip</code></td>
<td>
<p>A logical. Should the step be skipped when the recipe is baked by
<code><a href="recipes.html#topic+bake">recipes::bake()</a></code>? While all operations are baked when <code><a href="recipes.html#topic+prep">recipes::prep()</a></code> is
run, some operations may not be able to be conducted on new data (e.g.
processing the outcome variable(s)). Care should be taken when using <code>skip = TRUE</code> as it may affect the computations for subsequent operations</p>
</td></tr>
<tr><td><code id="step_lencode_mixed_+3A_id">id</code></td>
<td>
<p>A character string that is unique to this step to identify it.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For each factor predictor, a generalized linear model is fit to the outcome
and the coefficients are returned as the encoding. These coefficients are on
the linear predictor scale so, for factor outcomes, they are in log-odds
units. The coefficients are created using a no intercept model and, when two
factor outcomes are used, the log-odds reflect the event of interest being
the <em>first</em> level of the factor.
</p>
<p>For novel levels, a slightly timmed average of the coefficients is returned.
</p>
<p>A hierarchical generalized linear model is fit using <code><a href="lme4.html#topic+lmer">lme4::lmer()</a></code> or
<code><a href="lme4.html#topic+glmer">lme4::glmer()</a></code>, depending on the nature of the outcome, and no intercept via
</p>
<div class="sourceCode"><pre>  lmer(outcome ~ 1 + (1 | predictor), data = data, ...)
</pre></div>
<p>where the <code>...</code> include the <code>family</code> argument (automatically set by the step)
as well as any arguments given to the <code>options</code> argument to the step.
Relevant options include <code>control</code> and others.
</p>


<h3>Value</h3>

<p>An updated version of <code>recipe</code> with the new step added to the
sequence of existing steps (if any). For the <code>tidy</code> method, a tibble with
columns <code>terms</code> (the selectors or variables for encoding), <code>level</code> (the
factor levels), and <code>value</code> (the encodings).
</p>


<h3>Tidying</h3>

<p>When you <code><a href="recipes.html#topic+tidy.recipe">tidy()</a></code> this step, a tibble is returned with
columns <code>level</code>, <code>value</code>, <code>terms</code>, and <code>id</code>:
</p>

<dl>
<dt>level</dt><dd><p>character, the factor levels</p>
</dd>
<dt>value</dt><dd><p>numeric, the encoding</p>
</dd>
<dt>terms</dt><dd><p>character, the selectors or variables selected</p>
</dd>
<dt>id</dt><dd><p>character, id of this step</p>
</dd>
</dl>



<h3>Case weights</h3>

<p>This step performs an supervised operation that can utilize case weights.
To use them, see the documentation in <a href="recipes.html#topic+case_weights">recipes::case_weights</a> and the examples on
<code>tidymodels.org</code>.
</p>


<h3>References</h3>

<p>Micci-Barreca D (2001) &quot;A preprocessing scheme for high-cardinality
categorical attributes in classification and prediction problems,&quot; ACM SIGKDD
Explorations Newsletter, 3(1), 27-32.
</p>
<p>Zumel N and Mount J (2017) &quot;vtreat: a data.frame Processor for Predictive
Modeling,&quot; arXiv:1611.09477
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(recipes)
library(dplyr)
library(modeldata)

data(grants)

set.seed(1)
grants_other &lt;- sample_n(grants_other, 500)

reencoded &lt;- recipe(class ~ sponsor_code, data = grants_other) %&gt;%
  step_lencode_mixed(sponsor_code, outcome = vars(class))


</code></pre>

<hr>
<h2 id='step_pca_sparse'>Sparse PCA Signal Extraction</h2><span id='topic+step_pca_sparse'></span><span id='topic+tidy.step_pca_sparse'></span>

<h3>Description</h3>

<p><code>step_pca_sparse()</code> creates a <em>specification</em> of a recipe step that will
convert numeric data into one or more principal components that can have some
zero coefficients.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>step_pca_sparse(
  recipe,
  ...,
  role = "predictor",
  trained = FALSE,
  num_comp = 5,
  predictor_prop = 1,
  options = list(),
  res = NULL,
  prefix = "PC",
  keep_original_cols = FALSE,
  skip = FALSE,
  id = rand_id("pca_sparse")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="step_pca_sparse_+3A_recipe">recipe</code></td>
<td>
<p>A recipe object. The step will be added to the sequence of
operations for this recipe.</p>
</td></tr>
<tr><td><code id="step_pca_sparse_+3A_...">...</code></td>
<td>
<p>One or more selector functions to choose which variables will be
used to compute the components. See <a href="recipes.html#topic+selections">recipes::selections</a> for more details. For
the <code>tidy</code> method, these are not currently used.</p>
</td></tr>
<tr><td><code id="step_pca_sparse_+3A_role">role</code></td>
<td>
<p>For model terms created by this step, what analysis role should
they be assigned? By default, the function assumes that the new principal
component columns created by the original variables will be used as
predictors in a model.</p>
</td></tr>
<tr><td><code id="step_pca_sparse_+3A_trained">trained</code></td>
<td>
<p>A logical to indicate if the quantities for preprocessing have
been estimated.</p>
</td></tr>
<tr><td><code id="step_pca_sparse_+3A_num_comp">num_comp</code></td>
<td>
<p>The number of components to retain as new predictors.
If <code>num_comp</code> is greater than the number of columns or the number of
possible components, a smaller value will be used. If <code>num_comp = 0</code>
is set then no transformation is done and selected variables will
stay unchanged, regardless of the value of <code>keep_original_cols</code>.</p>
</td></tr>
<tr><td><code id="step_pca_sparse_+3A_predictor_prop">predictor_prop</code></td>
<td>
<p>The maximum number of original predictors that can have
non-zero coefficients for each PCA component (via regularization).</p>
</td></tr>
<tr><td><code id="step_pca_sparse_+3A_options">options</code></td>
<td>
<p>A list of options to the default method for <code><a href="irlba.html#topic+ssvd">irlba::ssvd()</a></code>.</p>
</td></tr>
<tr><td><code id="step_pca_sparse_+3A_res">res</code></td>
<td>
<p>The rotation matrix once this preprocessing step has be trained by
<a href="recipes.html#topic+prep">recipes::prep</a>.</p>
</td></tr>
<tr><td><code id="step_pca_sparse_+3A_prefix">prefix</code></td>
<td>
<p>A character string that will be the prefix to the resulting new
variables. See notes below.</p>
</td></tr>
<tr><td><code id="step_pca_sparse_+3A_keep_original_cols">keep_original_cols</code></td>
<td>
<p>A logical to keep the original variables in the
output. Defaults to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="step_pca_sparse_+3A_skip">skip</code></td>
<td>
<p>A logical. Should the step be skipped when the recipe is baked by
<code><a href="recipes.html#topic+bake">recipes::bake()</a></code>? While all operations are baked when <code><a href="recipes.html#topic+prep">recipes::prep()</a></code> is
run, some operations may not be able to be conducted on new data (e.g.
processing the outcome variable(s)). Care should be taken when using <code>skip = TRUE</code> as it may affect the computations for subsequent operations</p>
</td></tr>
<tr><td><code id="step_pca_sparse_+3A_id">id</code></td>
<td>
<p>A character string that is unique to this step to identify it.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>irlba</code> package is required for this step. If it is not installed, the
user will be prompted to do so when the step is defined. The <code><a href="irlba.html#topic+ssvd">irlba::ssvd()</a></code>
function is used to encourage sparsity; that documentation has details about
this method.
</p>
<p>The argument <code>num_comp</code> controls the number of components that will be retained
(the original variables that are used to derive the components are removed from
the data). The new components will have names that begin with <code>prefix</code> and a
sequence of numbers. The variable names are padded with zeros. For example, if
<code>num_comp &lt; 10</code>, their names will be <code>PC1</code> - <code>PC9</code>. If <code>num_comp = 101</code>,
the names would be <code>PC1</code> - <code>PC101</code>.
</p>


<h3>Value</h3>

<p>An updated version of <code>recipe</code> with the new step added to the
sequence of existing steps (if any). For the <code>tidy</code> method, a tibble with
columns <code>terms</code> (the selectors or variables selected), <code>value</code> (the
loading), and <code>component</code>.
</p>


<h3>Tidying</h3>

<p>When you <code><a href="recipes.html#topic+tidy.recipe">tidy()</a></code> this step, a tibble is returned with
columns <code>terms</code>, <code>value</code>, <code>component</code>, and <code>id</code>:
</p>

<dl>
<dt>terms</dt><dd><p>character, the selectors or variables selected</p>
</dd>
<dt>value</dt><dd><p>numeric, variable loading</p>
</dd>
<dt>component</dt><dd><p>character, principle component</p>
</dd>
<dt>id</dt><dd><p>character, id of this step</p>
</dd>
</dl>



<h3>Tuning Parameters</h3>

<p>This step has 2 tuning parameters:
</p>

<ul>
<li> <p><code>num_comp</code>: # Components (type: integer, default: 5)
</p>
</li>
<li> <p><code>predictor_prop</code>: Proportion of Predictors (type: double, default: 1)
</p>
</li></ul>



<h3>Case weights</h3>

<p>The underlying operation does not allow for case weights.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+step_pca_sparse_bayes">step_pca_sparse_bayes()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(recipes)
library(ggplot2)

data(ad_data, package = "modeldata")

ad_rec &lt;-
  recipe(Class ~ ., data = ad_data) %&gt;%
  step_zv(all_predictors()) %&gt;%
  step_YeoJohnson(all_numeric_predictors()) %&gt;%
  step_normalize(all_numeric_predictors()) %&gt;%
  step_pca_sparse(
    all_numeric_predictors(),
    predictor_prop = 0.75,
    num_comp = 3,
    id = "sparse pca"
  ) %&gt;%
  prep()

tidy(ad_rec, id = "sparse pca") %&gt;%
  mutate(value = ifelse(value == 0, NA, value)) %&gt;%
  ggplot(aes(x = component, y = terms, fill = value)) +
  geom_tile() +
  scale_fill_gradient2() +
  theme(axis.text.y = element_blank())

</code></pre>

<hr>
<h2 id='step_pca_sparse_bayes'>Sparse Bayesian PCA Signal Extraction</h2><span id='topic+step_pca_sparse_bayes'></span><span id='topic+tidy.step_pca_sparse_bayes'></span>

<h3>Description</h3>

<p><code>step_pca_sparse_bayes()</code> creates a <em>specification</em> of a recipe step that
will convert numeric data into one or more principal components that can have
some zero coefficients.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>step_pca_sparse_bayes(
  recipe,
  ...,
  role = "predictor",
  trained = FALSE,
  num_comp = 5,
  prior_slab_dispersion = 1,
  prior_mixture_threshold = 0.1,
  options = list(),
  res = NULL,
  prefix = "PC",
  keep_original_cols = FALSE,
  skip = FALSE,
  id = rand_id("pca_sparse_bayes")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="step_pca_sparse_bayes_+3A_recipe">recipe</code></td>
<td>
<p>A recipe object. The step will be added to the sequence of
operations for this recipe.</p>
</td></tr>
<tr><td><code id="step_pca_sparse_bayes_+3A_...">...</code></td>
<td>
<p>One or more selector functions to choose which variables will be
used to compute the components. See <a href="recipes.html#topic+selections">recipes::selections</a> for more details. For
the <code>tidy</code> method, these are not currently used.</p>
</td></tr>
<tr><td><code id="step_pca_sparse_bayes_+3A_role">role</code></td>
<td>
<p>For model terms created by this step, what analysis role should
they be assigned? By default, the function assumes that the new principal
component columns created by the original variables will be used as
predictors in a model.</p>
</td></tr>
<tr><td><code id="step_pca_sparse_bayes_+3A_trained">trained</code></td>
<td>
<p>A logical to indicate if the quantities for preprocessing have
been estimated.</p>
</td></tr>
<tr><td><code id="step_pca_sparse_bayes_+3A_num_comp">num_comp</code></td>
<td>
<p>The number of components to retain as new predictors.
If <code>num_comp</code> is greater than the number of columns or the number of
possible components, a smaller value will be used. If <code>num_comp = 0</code>
is set then no transformation is done and selected variables will
stay unchanged, regardless of the value of <code>keep_original_cols</code>.</p>
</td></tr>
<tr><td><code id="step_pca_sparse_bayes_+3A_prior_slab_dispersion">prior_slab_dispersion</code></td>
<td>
<p>This value is proportional to the dispersion (or
scale) parameter for the slab portion of the prior. Smaller values result
in an increase in zero coefficients.</p>
</td></tr>
<tr><td><code id="step_pca_sparse_bayes_+3A_prior_mixture_threshold">prior_mixture_threshold</code></td>
<td>
<p>The parameter that defines the trade-off
between the spike and slab components of the prior. Increasing this
parameter increases the number of zero coefficients.</p>
</td></tr>
<tr><td><code id="step_pca_sparse_bayes_+3A_options">options</code></td>
<td>
<p>A list of options to the default method for
<code><a href="VBsparsePCA.html#topic+VBsparsePCA">VBsparsePCA::VBsparsePCA()</a></code>.</p>
</td></tr>
<tr><td><code id="step_pca_sparse_bayes_+3A_res">res</code></td>
<td>
<p>The rotation matrix once this preprocessing step has been trained
by <a href="recipes.html#topic+prep">recipes::prep</a>.</p>
</td></tr>
<tr><td><code id="step_pca_sparse_bayes_+3A_prefix">prefix</code></td>
<td>
<p>A character string that will be the prefix to the resulting new
variables. See notes below.</p>
</td></tr>
<tr><td><code id="step_pca_sparse_bayes_+3A_keep_original_cols">keep_original_cols</code></td>
<td>
<p>A logical to keep the original variables in the
output. Defaults to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="step_pca_sparse_bayes_+3A_skip">skip</code></td>
<td>
<p>A logical. Should the step be skipped when the recipe is baked by
<code><a href="recipes.html#topic+bake">recipes::bake()</a></code>? While all operations are baked when <code><a href="recipes.html#topic+prep">recipes::prep()</a></code> is
run, some operations may not be able to be conducted on new data (e.g.
processing the outcome variable(s)). Care should be taken when using <code>skip = TRUE</code> as it may affect the computations for subsequent operations</p>
</td></tr>
<tr><td><code id="step_pca_sparse_bayes_+3A_id">id</code></td>
<td>
<p>A character string that is unique to this step to identify it.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>VBsparsePCA</code> package is required for this step. If it is not installed,
the user will be prompted to do so when the step is defined.
</p>
<p>A spike-and-slab prior is a mixture of two priors. One (the &quot;spike&quot;) has all
of its mass at zero and represents a variable that has no contribution to the
PCA coefficients. The other prior is a broader distribution that reflects the
coefficient distribution of variables that do affect the PCA analysis. This
is the &quot;slab&quot;. The narrower the slab, the more likely that a coefficient will
be zero (or are regularized to be closer to zero). The mixture of these two
priors is governed by a mixing parameter, which itself has a prior
distribution and a hyper-parameter prior.
</p>
<p>PCA coefficients and their resulting scores are unique only up to the sign.
This step will attempt to make the sign of the components more consistent
from run-to-run. However, the sparsity constraint may interfere with this
goal.
</p>
<p>The argument <code>num_comp</code> controls the number of components that will be retained
(the original variables that are used to derive the components are removed from
the data). The new components will have names that begin with <code>prefix</code> and a
sequence of numbers. The variable names are padded with zeros. For example, if
<code>num_comp &lt; 10</code>, their names will be <code>PC1</code> - <code>PC9</code>. If <code>num_comp = 101</code>,
the names would be <code>PC1</code> - <code>PC101</code>.
</p>


<h3>Value</h3>

<p>An updated version of <code>recipe</code> with the new step added to the
sequence of existing steps (if any). For the <code>tidy</code> method, a tibble with
columns <code>terms</code> (the selectors or variables selected), <code>value</code> (the
loading), and <code>component</code>.
</p>


<h3>Tidying</h3>

<p>When you <code><a href="recipes.html#topic+tidy.recipe">tidy()</a></code> this step, a tibble is returned with
columns <code>terms</code>, <code>value</code>, <code>component</code>, and <code>id</code>:
</p>

<dl>
<dt>terms</dt><dd><p>character, the selectors or variables selected</p>
</dd>
<dt>value</dt><dd><p>numeric, variable loading</p>
</dd>
<dt>component</dt><dd><p>character, principle component</p>
</dd>
<dt>id</dt><dd><p>character, id of this step</p>
</dd>
</dl>



<h3>Tuning Parameters</h3>

<p>This step has 3 tuning parameters:
</p>

<ul>
<li> <p><code>num_comp</code>: # Components (type: integer, default: 5)
</p>
</li>
<li> <p><code>prior_slab_dispersion</code>: Dispersion of Slab Prior (type: double, default: 1)
</p>
</li>
<li> <p><code>prior_mixture_threshold</code>: Threshold for Mixture Prior (type: double, default: 0.1)
</p>
</li></ul>



<h3>Case weights</h3>

<p>The underlying operation does not allow for case weights.
</p>


<h3>References</h3>

<p>Ning, B. (2021). Spike and slab Bayesian sparse principal
component analysis. arXiv:2102.00305.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+step_pca_sparse">step_pca_sparse()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(recipes)
library(ggplot2)

data(ad_data, package = "modeldata")

ad_rec &lt;-
  recipe(Class ~ ., data = ad_data) %&gt;%
  step_zv(all_predictors()) %&gt;%
  step_YeoJohnson(all_numeric_predictors()) %&gt;%
  step_normalize(all_numeric_predictors()) %&gt;%
  step_pca_sparse_bayes(
    all_numeric_predictors(),
    prior_mixture_threshold = 0.95,
    prior_slab_dispersion = 0.05,
    num_comp = 3,
    id = "sparse bayesian pca"
  ) %&gt;%
  prep()

tidy(ad_rec, id = "sparse bayesian pca") %&gt;%
  mutate(value = ifelse(value == 0, NA, value)) %&gt;%
  ggplot(aes(x = component, y = terms, fill = value)) +
  geom_tile() +
  scale_fill_gradient2() +
  theme(axis.text.y = element_blank())

</code></pre>

<hr>
<h2 id='step_pca_truncated'>Truncated PCA Signal Extraction</h2><span id='topic+step_pca_truncated'></span><span id='topic+tidy.step_pca_truncated'></span>

<h3>Description</h3>

<p><code>step_pca_truncated()</code> creates a <em>specification</em> of a recipe step that will
convert numeric data into one or more principal components. It is truncated
as it only calculates the number of components it is asked instead of all of
them as is done in <code><a href="recipes.html#topic+step_pca">recipes::step_pca()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>step_pca_truncated(
  recipe,
  ...,
  role = "predictor",
  trained = FALSE,
  num_comp = 5,
  options = list(),
  res = NULL,
  columns = NULL,
  prefix = "PC",
  keep_original_cols = FALSE,
  skip = FALSE,
  id = rand_id("pca_truncated")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="step_pca_truncated_+3A_recipe">recipe</code></td>
<td>
<p>A recipe object. The step will be added to the
sequence of operations for this recipe.</p>
</td></tr>
<tr><td><code id="step_pca_truncated_+3A_...">...</code></td>
<td>
<p>One or more selector functions to choose variables
for this step. See <code><a href="recipes.html#topic+selections">selections()</a></code> for more details.</p>
</td></tr>
<tr><td><code id="step_pca_truncated_+3A_role">role</code></td>
<td>
<p>For model terms created by this step, what analysis role should
they be assigned? By default, the new columns created by this step from
the original variables will be used as <em>predictors</em> in a model.</p>
</td></tr>
<tr><td><code id="step_pca_truncated_+3A_trained">trained</code></td>
<td>
<p>A logical to indicate if the quantities for
preprocessing have been estimated.</p>
</td></tr>
<tr><td><code id="step_pca_truncated_+3A_num_comp">num_comp</code></td>
<td>
<p>The number of components to retain as new predictors.
If <code>num_comp</code> is greater than the number of columns or the number of
possible components, a smaller value will be used. If <code>num_comp = 0</code>
is set then no transformation is done and selected variables will
stay unchanged, regardless of the value of <code>keep_original_cols</code>.</p>
</td></tr>
<tr><td><code id="step_pca_truncated_+3A_options">options</code></td>
<td>
<p>A list of options to the default method for
<code><a href="irlba.html#topic+prcomp_irlba">irlba::prcomp_irlba()</a></code>. Argument defaults are set to <code>retx = FALSE</code>,
<code>center = FALSE</code>, <code>scale. = FALSE</code>, and <code>tol = NULL</code>. <strong>Note</strong> that the
argument <code>x</code> should not be passed here (or at all).</p>
</td></tr>
<tr><td><code id="step_pca_truncated_+3A_res">res</code></td>
<td>
<p>The <code><a href="irlba.html#topic+prcomp_irlba">irlba::prcomp_irlba()</a></code> object is stored here once this
preprocessing step has be trained by <a href="recipes.html#topic+prep">recipes::prep</a>.</p>
</td></tr>
<tr><td><code id="step_pca_truncated_+3A_columns">columns</code></td>
<td>
<p>A character string of the selected variable names. This field
is a placeholder and will be populated once <code><a href="recipes.html#topic+prep">prep()</a></code> is used.</p>
</td></tr>
<tr><td><code id="step_pca_truncated_+3A_prefix">prefix</code></td>
<td>
<p>A character string for the prefix of the resulting new
variables. See notes below.</p>
</td></tr>
<tr><td><code id="step_pca_truncated_+3A_keep_original_cols">keep_original_cols</code></td>
<td>
<p>A logical to keep the original variables in the
output. Defaults to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="step_pca_truncated_+3A_skip">skip</code></td>
<td>
<p>A logical. Should the step be skipped when the
recipe is baked by <code><a href="recipes.html#topic+bake">bake()</a></code>? While all operations are baked
when <code><a href="recipes.html#topic+prep">prep()</a></code> is run, some operations may not be able to be
conducted on new data (e.g. processing the outcome variable(s)).
Care should be taken when using <code>skip = TRUE</code> as it may affect
the computations for subsequent operations.</p>
</td></tr>
<tr><td><code id="step_pca_truncated_+3A_id">id</code></td>
<td>
<p>A character string that is unique to this step to identify it.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Principal component analysis (PCA) is a transformation of a group of
variables that produces a new set of artificial features or components. These
components are designed to capture the maximum amount of information (i.e.
variance) in the original variables. Also, the components are statistically
independent from one another. This means that they can be used to combat
large inter-variables correlations in a data set.
</p>
<p>It is advisable to standardize the variables prior to running PCA. Here, each
variable will be centered and scaled prior to the PCA calculation. This can
be changed using the <code>options</code> argument or by using <code><a href="recipes.html#topic+step_center">recipes::step_center()</a></code> and
<code><a href="recipes.html#topic+step_scale">recipes::step_scale()</a></code>.
</p>
<p>The argument <code>num_comp</code> controls the number of components that will be retained
(the original variables that are used to derive the components are removed from
the data). The new components will have names that begin with <code>prefix</code> and a
sequence of numbers. The variable names are padded with zeros. For example, if
<code>num_comp &lt; 10</code>, their names will be <code>PC1</code> - <code>PC9</code>. If <code>num_comp = 101</code>,
the names would be <code>PC1</code> - <code>PC101</code>.
</p>


<h3>Value</h3>

<p>An updated version of <code>recipe</code> with the new step added to the
sequence of any existing operations.
</p>


<h3>Tidying</h3>

<p>When you <code><a href="recipes.html#topic+tidy.recipe">tidy()</a></code> this step two things can happen depending
the <code>type</code> argument. If <code>type = "coef"</code> a tibble returned with 4 columns
<code>terms</code>, <code>value</code>, <code>component</code> , and <code>id</code>:
</p>

<dl>
<dt>terms</dt><dd><p>character, the selectors or variables selected</p>
</dd>
<dt>value</dt><dd><p>numeric, variable loading</p>
</dd>
<dt>component</dt><dd><p>character, principle component</p>
</dd>
<dt>id</dt><dd><p>character, id of this step</p>
</dd>
</dl>

<p>If <code>type = "variance"</code> a tibble returned with 4 columns <code>terms</code>, <code>value</code>,
<code>component</code> , and <code>id</code>:
</p>

<dl>
<dt>terms</dt><dd><p>character, type of variance</p>
</dd>
<dt>value</dt><dd><p>numeric, value of the variance</p>
</dd>
<dt>component</dt><dd><p>integer, principle component</p>
</dd>
<dt>id</dt><dd><p>character, id of this step</p>
</dd>
</dl>



<h3>Tuning Parameters</h3>

<p>This step has 1 tuning parameters:
</p>

<ul>
<li> <p><code>num_comp</code>: # Components (type: integer, default: 5)
</p>
</li></ul>



<h3>Case weights</h3>

<p>This step performs an unsupervised operation that can utilize case weights.
As a result, case weights are only used with frequency weights. For more
information, see the documentation in <a href="recipes.html#topic+case_weights">recipes::case_weights</a> and the examples on
<code>tidymodels.org</code>.
</p>


<h3>References</h3>

<p>Jolliffe, I. T. (2010). <em>Principal Component Analysis</em>. Springer.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>rec &lt;- recipe(~., data = mtcars)
pca_trans &lt;- rec %&gt;%
  step_normalize(all_numeric()) %&gt;%
  step_pca_truncated(all_numeric(), num_comp = 2)
pca_estimates &lt;- prep(pca_trans, training = mtcars)
pca_data &lt;- bake(pca_estimates, mtcars)

rng &lt;- extendrange(c(pca_data$PC1, pca_data$PC2))
plot(pca_data$PC1, pca_data$PC2,
  xlim = rng, ylim = rng
)

tidy(pca_trans, number = 2)
tidy(pca_estimates, number = 2)
</code></pre>

<hr>
<h2 id='step_umap'>Supervised and unsupervised uniform manifold approximation and projection
(UMAP)</h2><span id='topic+step_umap'></span><span id='topic+tidy.step_umap'></span>

<h3>Description</h3>

<p><code>step_umap()</code> creates a <em>specification</em> of a recipe step that will project a
set of features into a smaller space.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>step_umap(
  recipe,
  ...,
  role = "predictor",
  trained = FALSE,
  outcome = NULL,
  neighbors = 15,
  num_comp = 2,
  min_dist = 0.01,
  metric = "euclidean",
  learn_rate = 1,
  epochs = NULL,
  initial = "spectral",
  target_weight = 0.5,
  options = list(verbose = FALSE, n_threads = 1),
  seed = sample(10^5, 2),
  prefix = "UMAP",
  keep_original_cols = FALSE,
  retain = deprecated(),
  object = NULL,
  skip = FALSE,
  id = rand_id("umap")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="step_umap_+3A_recipe">recipe</code></td>
<td>
<p>A recipe object. The step will be added to the
sequence of operations for this recipe.</p>
</td></tr>
<tr><td><code id="step_umap_+3A_...">...</code></td>
<td>
<p>One or more selector functions to choose variables
for this step. See <code><a href="recipes.html#topic+selections">selections()</a></code> for more details.</p>
</td></tr>
<tr><td><code id="step_umap_+3A_role">role</code></td>
<td>
<p>For model terms created by this step, what analysis role should
they be assigned? By default, the new columns created by this step from
the original variables will be used as <em>predictors</em> in a model.</p>
</td></tr>
<tr><td><code id="step_umap_+3A_trained">trained</code></td>
<td>
<p>A logical to indicate if the quantities for
preprocessing have been estimated.</p>
</td></tr>
<tr><td><code id="step_umap_+3A_outcome">outcome</code></td>
<td>
<p>A call to <code>vars</code> to specify which variable is used as the
outcome in the encoding process (if any).</p>
</td></tr>
<tr><td><code id="step_umap_+3A_neighbors">neighbors</code></td>
<td>
<p>An integer for the number of nearest neighbors used to
construct the target simplicial set. If <code>neighbors</code> is greater than the
number of data points, the smaller value is used.</p>
</td></tr>
<tr><td><code id="step_umap_+3A_num_comp">num_comp</code></td>
<td>
<p>An integer for the number of UMAP components. If <code>num_comp</code>
is greater than the number of selected columns minus one, the smaller value
is used.</p>
</td></tr>
<tr><td><code id="step_umap_+3A_min_dist">min_dist</code></td>
<td>
<p>The effective minimum distance between embedded points.</p>
</td></tr>
<tr><td><code id="step_umap_+3A_metric">metric</code></td>
<td>
<p>Character, type of distance metric to use to find nearest
neighbors. See <code><a href="uwot.html#topic+umap">uwot::umap()</a></code> for more details. Default to <code>"euclidean"</code>.</p>
</td></tr>
<tr><td><code id="step_umap_+3A_learn_rate">learn_rate</code></td>
<td>
<p>Positive number of the learning rate for the optimization
process.</p>
</td></tr>
<tr><td><code id="step_umap_+3A_epochs">epochs</code></td>
<td>
<p>Number of iterations for the neighbor optimization. See
<code><a href="uwot.html#topic+umap">uwot::umap()</a></code> for more details.</p>
</td></tr>
<tr><td><code id="step_umap_+3A_initial">initial</code></td>
<td>
<p>Character, Type of initialization for the coordinates. Can be
one of <code>"spectral"</code>, <code>"normlaplacian"</code>, <code>"random"</code>, <code>"lvrandom"</code>,
<code>"laplacian"</code>, <code>"pca"</code>, <code>"spca"</code>, <code>"agspectral"</code>, or a matrix of initial
coordinates. See <code><a href="uwot.html#topic+umap">uwot::umap()</a></code> for more details. Default to <code>"spectral"</code>.</p>
</td></tr>
<tr><td><code id="step_umap_+3A_target_weight">target_weight</code></td>
<td>
<p>Weighting factor between data topology and target
topology. A value of 0.0 weights entirely on data, a value of 1.0 weights
entirely on target. The default of 0.5 balances the weighting equally
between data and target.</p>
</td></tr>
<tr><td><code id="step_umap_+3A_options">options</code></td>
<td>
<p>A list of options to pass to <code><a href="uwot.html#topic+umap">uwot::umap()</a></code>. The arguments
<code>X</code>, <code>n_neighbors</code>, <code>n_components</code>, <code>min_dist</code>, <code>n_epochs</code>, <code>ret_model</code>,
and <code>learning_rate</code> should not be passed here. By default, <code>verbose</code> and
<code>n_threads</code> are set.</p>
</td></tr>
<tr><td><code id="step_umap_+3A_seed">seed</code></td>
<td>
<p>Two integers to control the random numbers used by the numerical
methods. The default pulls from the main session's stream of numbers and
will give reproducible results if the seed is set prior to calling <a href="recipes.html#topic+prep">recipes::prep</a>
or <a href="recipes.html#topic+bake">recipes::bake</a>.</p>
</td></tr>
<tr><td><code id="step_umap_+3A_prefix">prefix</code></td>
<td>
<p>A character string for the prefix of the resulting new
variables. See notes below.</p>
</td></tr>
<tr><td><code id="step_umap_+3A_keep_original_cols">keep_original_cols</code></td>
<td>
<p>A logical to keep the original variables in the
output. Defaults to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="step_umap_+3A_retain">retain</code></td>
<td>
<p>Use <code>keep_original_cols</code> instead to specify whether the
original predictors should be retained along with the new embedding
variables.</p>
</td></tr>
<tr><td><code id="step_umap_+3A_object">object</code></td>
<td>
<p>An object that defines the encoding. This is <code>NULL</code> until the
step is trained by <code><a href="recipes.html#topic+prep">recipes::prep()</a></code>.</p>
</td></tr>
<tr><td><code id="step_umap_+3A_skip">skip</code></td>
<td>
<p>A logical. Should the step be skipped when the
recipe is baked by <code><a href="recipes.html#topic+bake">bake()</a></code>? While all operations are baked
when <code><a href="recipes.html#topic+prep">prep()</a></code> is run, some operations may not be able to be
conducted on new data (e.g. processing the outcome variable(s)).
Care should be taken when using <code>skip = TRUE</code> as it may affect
the computations for subsequent operations.</p>
</td></tr>
<tr><td><code id="step_umap_+3A_id">id</code></td>
<td>
<p>A character string that is unique to this step to identify it.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>UMAP, short for Uniform Manifold Approximation and Projection, is a nonlinear
dimension reduction technique that finds local, low-dimensional
representations of the data. It can be run unsupervised or supervised with
different types of outcome data (e.g. numeric, factor, etc).
</p>
<p>The argument <code>num_comp</code> controls the number of components that will be retained
(the original variables that are used to derive the components are removed from
the data). The new components will have names that begin with <code>prefix</code> and a
sequence of numbers. The variable names are padded with zeros. For example, if
<code>num_comp &lt; 10</code>, their names will be <code>UMAP1</code> - <code>UMAP9</code>. If <code>num_comp = 101</code>,
the names would be <code>UMAP1</code> - <code>UMAP101</code>.
</p>


<h3>Value</h3>

<p>An updated version of <code>recipe</code> with the new step added to the
sequence of any existing operations.
</p>


<h3>Tidying</h3>

<p>When you <code><a href="recipes.html#topic+tidy.recipe">tidy()</a></code> this step, a tibble is returned with
columns <code>terms</code> and <code>id</code>:
</p>

<dl>
<dt>terms</dt><dd><p>character, the selectors or variables selected</p>
</dd>
<dt>id</dt><dd><p>character, id of this step</p>
</dd>
</dl>



<h3>Tuning Parameters</h3>

<p>This step has 7 tuning parameters:
</p>

<ul>
<li> <p><code>num_comp</code>: # Components (type: integer, default: 2)
</p>
</li>
<li> <p><code>neighbors</code>: # Nearest Neighbors (type: integer, default: 15)
</p>
</li>
<li> <p><code>min_dist</code>: Min Distance between Points (type: double, default: 0.01)
</p>
</li>
<li> <p><code>learn_rate</code>: Learning Rate (type: double, default: 1)
</p>
</li>
<li> <p><code>epochs</code>: # Epochs (type: integer, default: NULL)
</p>
</li>
<li> <p><code>initial</code>: UMAP Initialization (type: character, default: spectral)
</p>
</li>
<li> <p><code>target_weight</code>: Proportion Supervised (type: double, default: 0.5)
</p>
</li></ul>



<h3>Case weights</h3>

<p>The underlying operation does not allow for case weights.
</p>


<h3>Saving prepped recipe object</h3>

<p>This recipe step may require native serialization when saving for
use in another R session. To learn more about serialization for
prepped recipes, see the <a href="https://rstudio.github.io/bundle/">bundle</a> package.
</p>


<h3>References</h3>

<p>McInnes, L., &amp; Healy, J. (2018). UMAP: Uniform Manifold
Approximation and Projection for Dimension Reduction.
<a href="https://arxiv.org/abs/1802.03426">https://arxiv.org/abs/1802.03426</a>.
</p>
<p>&quot;How UMAP Works&quot;
<a href="https://umap-learn.readthedocs.io/en/latest/how_umap_works.html">https://umap-learn.readthedocs.io/en/latest/how_umap_works.html</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(recipes)
library(ggplot2)

split &lt;- seq.int(1, 150, by = 9)
tr &lt;- iris[-split, ]
te &lt;- iris[split, ]

set.seed(11)
supervised &lt;-
  recipe(Species ~ ., data = tr) %&gt;%
  step_center(all_predictors()) %&gt;%
  step_scale(all_predictors()) %&gt;%
  step_umap(all_predictors(), outcome = vars(Species), num_comp = 2) %&gt;%
  prep(training = tr)

theme_set(theme_bw())

bake(supervised, new_data = te, Species, starts_with("umap")) %&gt;%
  ggplot(aes(x = UMAP1, y = UMAP2, col = Species)) +
  geom_point(alpha = .5)

</code></pre>

<hr>
<h2 id='step_woe'>Weight of evidence transformation</h2><span id='topic+step_woe'></span><span id='topic+tidy.step_woe'></span>

<h3>Description</h3>

<p><code>step_woe()</code> creates a <em>specification</em> of a recipe step that will transform
nominal data into its numerical transformation based on weights of evidence
against a binary outcome.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>step_woe(
  recipe,
  ...,
  role = "predictor",
  outcome,
  trained = FALSE,
  dictionary = NULL,
  Laplace = 1e-06,
  prefix = "woe",
  keep_original_cols = FALSE,
  skip = FALSE,
  id = rand_id("woe")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="step_woe_+3A_recipe">recipe</code></td>
<td>
<p>A recipe object. The step will be added to the sequence of
operations for this recipe.</p>
</td></tr>
<tr><td><code id="step_woe_+3A_...">...</code></td>
<td>
<p>One or more selector functions to choose which variables will be
used to compute the components. See <a href="recipes.html#topic+selections">recipes::selections</a> for more details. For
the <code>tidy</code> method, these are not currently used.</p>
</td></tr>
<tr><td><code id="step_woe_+3A_role">role</code></td>
<td>
<p>For model terms created by this step, what analysis role should
they be assigned?. By default, the function assumes that the new woe
components columns created by the original variables will be used as
predictors in a model.</p>
</td></tr>
<tr><td><code id="step_woe_+3A_outcome">outcome</code></td>
<td>
<p>The bare name of the binary outcome encased in <code>vars()</code>.</p>
</td></tr>
<tr><td><code id="step_woe_+3A_trained">trained</code></td>
<td>
<p>A logical to indicate if the quantities for preprocessing have
been estimated.</p>
</td></tr>
<tr><td><code id="step_woe_+3A_dictionary">dictionary</code></td>
<td>
<p>A tbl. A map of levels and woe values. It must have the
same layout than the output returned from <code><a href="#topic+dictionary">dictionary()</a></code>. If <code>NULL</code> the
function will build a dictionary with those variables passed to <code>...</code>.
See <code><a href="#topic+dictionary">dictionary()</a></code> for details.</p>
</td></tr>
<tr><td><code id="step_woe_+3A_laplace">Laplace</code></td>
<td>
<p>The Laplace smoothing parameter. A value usually applied to
avoid -Inf/Inf from predictor category with only one outcome class. Set to
0 to allow Inf/-Inf. The default is 1e-6. Also known as 'pseudocount'
parameter of the Laplace smoothing technique.</p>
</td></tr>
<tr><td><code id="step_woe_+3A_prefix">prefix</code></td>
<td>
<p>A character string that will be the prefix to the resulting new
variables. See notes below.</p>
</td></tr>
<tr><td><code id="step_woe_+3A_keep_original_cols">keep_original_cols</code></td>
<td>
<p>A logical to keep the original variables in the
output. Defaults to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="step_woe_+3A_skip">skip</code></td>
<td>
<p>A logical. Should the step be skipped when the recipe is baked by
<code><a href="recipes.html#topic+bake">recipes::bake()</a></code>? While all operations are baked when <code><a href="recipes.html#topic+prep">recipes::prep()</a></code> is
run, some operations may not be able to be conducted on new data (e.g.
processing the outcome variable(s)). Care should be taken when using <code>skip = TRUE</code> as it may affect the computations for subsequent operations</p>
</td></tr>
<tr><td><code id="step_woe_+3A_id">id</code></td>
<td>
<p>A character string that is unique to this step to identify it.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>WoE is a transformation of a group of variables that produces a new set of
features. The formula is
</p>
<p style="text-align: center;"><code class="reqn">woe_c = log((P(X = c|Y = 1))/(P(X = c|Y = 0)))</code>
</p>

<p>where <code class="reqn">c</code> goes from 1 to <code class="reqn">C</code> levels of a given nominal predictor
variable <code class="reqn">X</code>.
</p>
<p>These components are designed to transform nominal variables into numerical
ones with the property that the order and magnitude reflects the association
with a binary outcome.  To apply it on numerical predictors, it is advisable
to discretize the variables prior to running WoE. Here, each variable will be
binarized to have woe associated later. This can achieved by using
<code><a href="recipes.html#topic+step_discretize">recipes::step_discretize()</a></code>.
</p>
<p>The argument <code>Laplace</code> is an small quantity added to the proportions of 1's
and 0's with the goal to avoid log(p/0) or log(0/p) results. The numerical
woe versions will have names that begin with <code>woe_</code> followed by the
respective original name of the variables. See Good (1985).
</p>
<p>One can pass a custom <code>dictionary</code> tibble to <code>step_woe()</code>. It must have
the same structure of the output from <code>dictionary()</code> (see examples). If
not provided it will be created automatically. The role of this tibble is to
store the map between the levels of nominal predictor to its woe values. You
may want to tweak this object with the goal to fix the orders between the
levels of one given predictor. One easy way to do this is by tweaking an
output returned from <code>dictionary()</code>.
</p>


<h3>Value</h3>

<p>An updated version of <code>recipe</code> with the new step added to the
sequence of existing steps (if any). For the <code>tidy</code> method, a tibble with
the woe dictionary used to map categories with woe values.
</p>


<h3>Tidying</h3>

<p>When you <code><a href="recipes.html#topic+tidy.recipe">tidy()</a></code> this step, a tibble with columns <code>terms</code>
(the selectors or variables selected), <code>value</code>, <code>n_tot</code>, <code>n_bad</code>, <code>n_good</code>,
<code>p_bad</code>, <code>p_good</code>, <code>woe</code> and <code>outcome</code> is returned.. See <code><a href="#topic+dictionary">dictionary()</a></code> for
more information.
</p>
<p>When you <code><a href="recipes.html#topic+tidy.recipe">tidy()</a></code> this step, a tibble is returned with
columns <code>terms</code> <code>value</code>, <code>n_tot</code>, <code>n_bad</code>, <code>n_good</code>, <code>p_bad</code>, <code>p_good</code>, <code>woe</code>
and <code>outcome</code> and <code>id</code>:
</p>

<dl>
<dt>terms</dt><dd><p>character, the selectors or variables selected</p>
</dd>
<dt>value</dt><dd><p>character, level of the outcome</p>
</dd>
<dt>n_tot</dt><dd><p>integer, total number</p>
</dd>
<dt>n_bad</dt><dd><p>integer, number of bad examples</p>
</dd>
<dt>n_good</dt><dd><p>integer, number of good examples</p>
</dd>
<dt>p_bad</dt><dd><p>numeric, p of bad examples</p>
</dd>
<dt>p_good</dt><dd><p>numeric, p of good examples</p>
</dd>
<dt>woe</dt><dd><p>numeric, weight of evidence</p>
</dd>
<dt>outcome</dt><dd><p>character, name of outcome variable</p>
</dd>
<dt>id</dt><dd><p>character, id of this step</p>
</dd>
</dl>



<h3>Tuning Parameters</h3>

<p>This step has 1 tuning parameters:
</p>

<ul>
<li> <p><code>Laplace</code>: Laplace Correction (type: double, default: 1e-06)
</p>
</li></ul>



<h3>Case weights</h3>

<p>The underlying operation does not allow for case weights.
</p>


<h3>References</h3>

<p>Kullback, S. (1959). <em>Information Theory and Statistics.</em> Wiley, New York.
</p>
<p>Hastie, T., Tibshirani, R. and Friedman, J. (1986). <em>Elements of Statistical
Learning</em>, Second Edition, Springer, 2009.
</p>
<p>Good, I. J. (1985), &quot;Weight of evidence: A brief survey&quot;, <em>Bayesian
Statistics</em>, 2, pp.249-270.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(modeldata)
data("credit_data")

set.seed(111)
in_training &lt;- sample(1:nrow(credit_data), 2000)

credit_tr &lt;- credit_data[in_training, ]
credit_te &lt;- credit_data[-in_training, ]

rec &lt;- recipe(Status ~ ., data = credit_tr) %&gt;%
  step_woe(Job, Home, outcome = vars(Status))

woe_models &lt;- prep(rec, training = credit_tr)

# the encoding:
bake(woe_models, new_data = credit_te %&gt;% slice(1:5), starts_with("woe"))
# the original data
credit_te %&gt;%
  slice(1:5) %&gt;%
  dplyr::select(Job, Home)
# the details:
tidy(woe_models, number = 1)

# Example of custom dictionary + tweaking
# custom dictionary
woe_dict_custom &lt;- credit_tr %&gt;% dictionary(Job, Home, outcome = "Status")
woe_dict_custom[4, "woe"] &lt;- 1.23 # tweak

# passing custom dict to step_woe()
rec_custom &lt;- recipe(Status ~ ., data = credit_tr) %&gt;%
  step_woe(
    Job, Home,
    outcome = vars(Status), dictionary = woe_dict_custom
  ) %&gt;%
  prep()

rec_custom_baked &lt;- bake(rec_custom, new_data = credit_te)
rec_custom_baked %&gt;%
  dplyr::filter(woe_Job == 1.23) %&gt;%
  head()

</code></pre>

<hr>
<h2 id='tunable.step_discretize_cart'>tunable methods for embed</h2><span id='topic+tunable.step_discretize_cart'></span><span id='topic+tunable.step_discretize_xgb'></span><span id='topic+tunable.step_embed'></span><span id='topic+tunable.step_pca_sparse'></span><span id='topic+tunable.step_pca_sparse_bayes'></span><span id='topic+tunable_embed'></span><span id='topic+tunable.step_umap'></span><span id='topic+tunable.step_woe'></span>

<h3>Description</h3>

<p>These functions define what parameters <em>can</em> be tuned for specific steps.
They also define the recommended objects from the <code>dials</code> package that can be
used to generate new parameter values and other characteristics.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'step_discretize_cart'
tunable(x, ...)

## S3 method for class 'step_discretize_xgb'
tunable(x, ...)

## S3 method for class 'step_embed'
tunable(x, ...)

## S3 method for class 'step_pca_sparse'
tunable(x, ...)

## S3 method for class 'step_pca_sparse_bayes'
tunable(x, ...)

## S3 method for class 'step_umap'
tunable(x, ...)

## S3 method for class 'step_woe'
tunable(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tunable.step_discretize_cart_+3A_x">x</code></td>
<td>
<p>A recipe step object</p>
</td></tr>
<tr><td><code id="tunable.step_discretize_cart_+3A_...">...</code></td>
<td>
<p>Not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A tibble object.
</p>

<hr>
<h2 id='woe_table'>Crosstable with woe between a binary outcome and a predictor variable.</h2><span id='topic+woe_table'></span>

<h3>Description</h3>

<p>Calculates some summaries and the WoE (Weight of Evidence) between a binary
outcome and a given predictor variable. Used to biuld the dictionary.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>woe_table(predictor, outcome, Laplace = 1e-06, call = rlang::caller_env(0))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="woe_table_+3A_predictor">predictor</code></td>
<td>
<p>A atomic vector, usualy with few distinct values.</p>
</td></tr>
<tr><td><code id="woe_table_+3A_outcome">outcome</code></td>
<td>
<p>The dependent variable. A atomic vector with exactly 2
distinct values.</p>
</td></tr>
<tr><td><code id="woe_table_+3A_laplace">Laplace</code></td>
<td>
<p>The <code>pseudocount</code> parameter of the Laplace Smoothing
estimator. Default to 1e-6. Value to avoid -Inf/Inf from predictor category
with only one outcome class. Set to 0 to allow Inf/-Inf.</p>
</td></tr>
<tr><td><code id="woe_table_+3A_call">call</code></td>
<td>
<p>The execution environment of a currently running function, e.g.
<code>caller_env()</code>. The function will be mentioned in error messages as the
source of the error. See the call argument of <code><a href="rlang.html#topic+abort">rlang::abort()</a></code> for more
information.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a tibble with counts, proportions and woe. Warning: woe can possibly
be -Inf. Use 'Laplace' arg to avoid that.
</p>


<h3>References</h3>

<p>Kullback, S. (1959). <em>Information Theory and Statistics.</em> Wiley, New York.
</p>
<p>Hastie, T., Tibshirani, R. and Friedman, J. (1986). <em>Elements of Statistical
Learning</em>, Second Edition, Springer, 2009.
</p>
<p>Good, I. J. (1985), &quot;Weight of evidence: A brief survey&quot;, <em>Bayesian
Statistics</em>, 2, pp.249-270.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
