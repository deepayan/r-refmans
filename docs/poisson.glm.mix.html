<!DOCTYPE html><html><head><title>Help for package poisson.glm.mix</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {poisson.glm.mix}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#bjkmodel'>
<p>EM algorithm for the <code class="reqn">\beta_{jk}</code> (m=1) Poisson GLM mixture.</p></a></li>
<li><a href='#bjmodel'>
<p>EM algorithm for the <code class="reqn">\beta_{j}</code> (m=2) Poisson GLM mixture.</p></a></li>
<li><a href='#bkmodel'>
<p>EM algorithm for the <code class="reqn">\beta_{k}</code> (m=3) Poisson GLM mixture.</p></a></li>
<li><a href='#init1.1.jk.j'>
<p>1st step of Initialization 1 for the <code class="reqn">\beta_{jk}</code> (<code class="reqn">m=1</code>) or <code class="reqn">\beta_{j}</code> (<code class="reqn">m=2</code>) parameterization.</p></a></li>
<li><a href='#init1.2.jk.j'>
<p>2nd step of Initialization 1 for the <code class="reqn">\beta_{jk}</code> (<code class="reqn">m=1</code>) or <code class="reqn">\beta_{j}</code> (<code class="reqn">m=2</code>) parameterization.</p></a></li>
<li><a href='#init1.k'>
<p>Initialization 1 for the <code class="reqn">\beta_{k}</code> parameterization (<code class="reqn">m=3</code>).</p></a></li>
<li><a href='#init2.jk.j'>
<p>Initialization 2 for the <code class="reqn">\beta_{jk}</code> (<code class="reqn">m=1</code>) or <code class="reqn">\beta_{j}</code> (<code class="reqn">m=2</code>) parameterization.</p></a></li>
<li><a href='#init2.k'>
<p>Initialization 2 for the  <code class="reqn">\beta_k</code> parameterization (<code class="reqn">m=3</code>).</p></a></li>
<li><a href='#mylogLikePoisMix'><p>Function to compute the loglikelihood of the mixture.</p></a></li>
<li><a href='#pois.glm.mix'>
<p>Main call function of the package.</p></a></li>
<li><a href='#poisson.glm.mix'><p>Estimation of high dimensional Poisson GLMs via EM algorithm.</p></a></li>
<li><a href='#sim.data'><p>Simulated data set of 500 observations</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Fit High Dimensional Mixtures of Poisson GLMs</td>
</tr>
<tr>
<td>Version:</td>
<td>1.4</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-08-19</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Panagiotis Papastamoulis &lt;papapast@yahoo.gr&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Mixtures of Poisson Generalized Linear Models for high dimensional count data clustering. The (multivariate) responses can be partitioned into set of blocks. Three different parameterizations of the linear predictor are considered. The models are estimated according to the EM algorithm with an efficient initialization scheme &lt;<a href="https://doi.org/10.1016%2Fj.csda.2014.07.005">doi:10.1016/j.csda.2014.07.005</a>&gt;. </td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-08-19 07:36:19 UTC; panos</td>
</tr>
<tr>
<td>Author:</td>
<td>Panagiotis Papastamoulis
    <a href="https://orcid.org/0000-0001-9468-7613"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut, cre]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-08-19 08:10:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='bjkmodel'>
EM algorithm for the <code class="reqn">\beta_{jk}</code> (m=1) Poisson GLM mixture.  
</h2><span id='topic+bjkmodel'></span>

<h3>Description</h3>

<p>This function applies EM algorithm for estimating a <code class="reqn">K</code>-component mixture of Poisson GLM's, using parameterization <code class="reqn">m=1</code>, that is the <code class="reqn">\beta_{jk}</code> model. Initialization can be done using two different intialization schemes. The first one is a two-step small EM procedure. The second  one is  a random splitting small EM procedure based on results of a mixture with less components. Output of the function is the updates of the parameters at each iteration of the EM algorithm, the estimate of <code class="reqn">\gamma</code>, the estimated clusters and conditional probabilities of the observations, as well as the values of the BIC, ICL and loglikelihood of the model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bjkmodel(reference, response, L, m, K, nr, maxnr, m1, m2, t1, t2, 
         msplit, tsplit, prev.z, prev.clust, start.type, 
         prev.alpha, prev.beta)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bjkmodel_+3A_reference">reference</code></td>
<td>

<p>a numeric array of dimension <code class="reqn">n\times V</code> containing the <code class="reqn">V</code> covariates for each of the <code class="reqn">n</code> observations.
</p>
</td></tr>
<tr><td><code id="bjkmodel_+3A_response">response</code></td>
<td>
<p>a numeric array of count data with dimension <code class="reqn">n\times d</code> containing the <code class="reqn">d</code> response variables for each of the <code class="reqn">n</code> observations.
</p>
</td></tr>
<tr><td><code id="bjkmodel_+3A_l">L</code></td>
<td>
<p>numeric vector of positive integers containing the partition of the <code class="reqn">d</code> response variables into <code class="reqn">J\leq d</code> blocks, with <code class="reqn">\sum_{j=1}^{J}L_j=d</code>.
</p>
</td></tr>
<tr><td><code id="bjkmodel_+3A_m">m</code></td>
<td>
<p>positive integer denoting the maximum number of EM iterations.
</p>
</td></tr>
<tr><td><code id="bjkmodel_+3A_k">K</code></td>
<td>
<p>positive integer denoting the number of mixture components.
</p>
</td></tr>
<tr><td><code id="bjkmodel_+3A_nr">nr</code></td>
<td>
<p>negative number denoting the tolerance for the convergence of the Newton Raphson iterations.
</p>
</td></tr>
<tr><td><code id="bjkmodel_+3A_maxnr">maxnr</code></td>
<td>
<p>positive integer denoting the maximum number of Newton Raphson iterations.
</p>
</td></tr>
<tr><td><code id="bjkmodel_+3A_m1">m1</code></td>
<td>
<p>positive integer denoting the number of iterations for each call of the 1st small EM iterations used by Initialization 1 (<code>init1.1.jk.j</code>).
</p>
</td></tr>
<tr><td><code id="bjkmodel_+3A_m2">m2</code></td>
<td>
<p>positive integer denoting the number of iterations for each call of the 2nd small EM iterations used by Initialization 1 (<code>init1.2.jk.j</code>).
</p>
</td></tr>
<tr><td><code id="bjkmodel_+3A_t1">t1</code></td>
<td>
<p>positive integer denoting the number of different runs of the 1st small EM used by Initialization 1 (<code>init1.1.jk.j</code>).
</p>
</td></tr>
<tr><td><code id="bjkmodel_+3A_t2">t2</code></td>
<td>
<p>positive integer denoting the number of different runs of the 2nd small EM used by Initialization 1 (<code>init1.2.jk.j</code>).
</p>
</td></tr>
<tr><td><code id="bjkmodel_+3A_msplit">msplit</code></td>
<td>
<p>positive integer denoting the number of different runs for each call of the splitting small EM used by Initialization 2 (<code>init2.jk.j</code>).
</p>
</td></tr>
<tr><td><code id="bjkmodel_+3A_tsplit">tsplit</code></td>
<td>
<p>positive integer denoting the number of different runs for each call of the splitting small EM used by Initialization 2 (<code>init2.jk.j</code>).
</p>
</td></tr>
<tr><td><code id="bjkmodel_+3A_prev.z">prev.z</code></td>
<td>
<p>numeric array of dimension <code class="reqn">n\times(K-1)</code> containing the estimates of the posterior probabilities according to the previous run of EM. This is used when Initialization 2 is adopted.
</p>
</td></tr>
<tr><td><code id="bjkmodel_+3A_prev.clust">prev.clust</code></td>
<td>
<p>numeric vector of length <code class="reqn">n</code> containing the estimated clusters according to the MAP rule obtained by the previous run of EM. This is used when Initialization 2 is adopted.
</p>
</td></tr>
<tr><td><code id="bjkmodel_+3A_start.type">start.type</code></td>
<td>
<p>binary variable (1 or 2) indicating the type of initialization (1 for initialization 1 and 2 for initialization 2).
</p>
</td></tr>
<tr><td><code id="bjkmodel_+3A_prev.alpha">prev.alpha</code></td>
<td>
<p>numeric array of dimension <code class="reqn">J\times (K-1)</code> containing the matrix of the ML estimates of the regression constants <code class="reqn">\alpha_{jk}</code>, <code class="reqn">j=1,\ldots,J</code>, <code class="reqn">k=1,\ldots,K-1</code>, based on the previous run of EM algorithm. This is used in case of Initialization 2.
</p>
</td></tr>
<tr><td><code id="bjkmodel_+3A_prev.beta">prev.beta</code></td>
<td>
<p>numeric array of dimension <code class="reqn">J\times (K-1)\times T</code> containing the matrix of the ML estimates of the regression coefficients <code class="reqn">\beta_{jk\tau}</code>, <code class="reqn">j=1,\ldots,J</code>, <code class="reqn">k=1,\ldots,K-1</code>, <code class="reqn">\tau=1,\ldots,T</code>, based on the previous run of EM algorithm. This is used in case of Initialization 2.
</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>alpha</code></td>
<td>
<p>numeric array of dimension <code class="reqn">t_{EM}\times J \times K</code>, containing the updates of regression constants <code class="reqn">\alpha_{jk}^{t})</code>, <code class="reqn">j=1,\ldots,J</code>, <code class="reqn">k=1,\ldots,K</code>, for each iteration <code class="reqn">t=1,2,\ldots,t_{EM}</code> of the EM algorithm.</p>
</td></tr>
<tr><td><code>beta</code></td>
<td>
<p>numeric array of dimension <code class="reqn">t_{EM}\times J \times K \times T</code> containing the updates of regression coefficients <code class="reqn">\beta_{jk\tau}^{t})</code>, <code class="reqn">j=1,\ldots,J</code>, <code class="reqn">k=1,\ldots,K</code>, <code class="reqn">\tau=1,\ldots,T</code>, for each iteration <code class="reqn">t=1,2,\ldots,t_{EM}</code> of the EM algorithm.</p>
</td></tr>
<tr><td><code>gamma</code></td>
<td>
<p>numeric array of dimension <code class="reqn">J \times \max(L)</code> containing the MLE of <code class="reqn">\gamma_{j\ell}</code>, <code class="reqn">j=1,\ldots,J</code>, <code class="reqn">\ell=1,\ldots,L_j</code>.</p>
</td></tr>
<tr><td><code>psim</code></td>
<td>
<p>numeric array of dimension <code class="reqn">t_{EM}\times K</code> containing the updates of mixture weights <code class="reqn">\pi_{k}^{t})</code>, <code class="reqn">k=1,\ldots,K</code>, for each iteration <code class="reqn">t=1,2,\ldots,t_{EM}</code> of the EM algorithm.</p>
</td></tr>
<tr><td><code>clust</code></td>
<td>
<p>numeric vector of length <code class="reqn">n</code> containing the estimated cluster for each observation according to the MAP rule.</p>
</td></tr>
<tr><td><code>z</code></td>
<td>
<p>numeric array of dimension <code class="reqn">n\times K</code> containing the estimated conditional probabilities <code class="reqn">\tau_{ik}</code>, <code class="reqn">i=1,\ldots,n</code>, <code class="reqn">k=,\ldots,K</code>, according to the last iteration of the EM algorithm.</p>
</td></tr>
<tr><td><code>bic</code></td>
<td>
<p>numeric, the value of the BIC.</p>
</td></tr>
<tr><td><code>icl</code></td>
<td>
<p>numeric, the value of the ICL.</p>
</td></tr>
<tr><td><code>ll</code></td>
<td>
<p>numeric, the value of the loglikelihood, computed according to the <code>mylogLikePoisMix</code> function.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Panagiotis Papastamoulis
</p>


<h3>See Also</h3>

<p><code><a href="#topic+init1.1.jk.j">init1.1.jk.j</a></code>, <code><a href="#topic+init1.2.jk.j">init1.2.jk.j</a></code>, <code><a href="#topic+init2.jk.j">init2.jk.j</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>############################################################
#1.            Example with Initialization 1               #
############################################################


## load a simulated dataset according to the b_jk model
## number of observations: 500
## design: L=(3,2,1)
data("simulated_data_15_components_bjk")
x &lt;- sim.data[,1]
x &lt;- array(x,dim=c(length(x),1))
y &lt;- sim.data[,-1]
## use Initialization 1 with 2 components
## the number of different 1st small runs equals t1=3, 
##	each one consisting of m1 = 5 iterations
## the number of different 2nd small runs equals t2=3, 
##	each one consisting of m2 = 5 iterations
## the maximum number of EM iterations is set to m = 1000.
nc &lt;- 2
run &lt;- bjkmodel(reference=x, response=y, L=c(3,2,1), m=1000, K=nc, nr=-10*log(10), 
                maxnr=10, m1=5, m2=5, t1=3, t2=3, msplit, tsplit, prev.z, 
                prev.clust, start.type=1, prev.alpha, prev.beta) 
## retrieve the iteration that the small em converged:
tem &lt;- length(run$psim)/nc
## print the estimate of regression constants alpha.
run$alpha[tem,,]
## print the estimate of regression coefficients beta.
beta &lt;- run$beta[tem,,,]
## print the estimate of gamma.
run$gamma
## print the estimate of mixture weights.
run$psim[tem,]
## frequency table of the resulting clustering of the 
##		500 observations among the 2 components.
table(run$clust)
## print the value of the ICL criterion
run$icl
## print the value of the BIC
run$bic
## print the value of the loglikelihood
run$ll


############################################################
#2.            Example with Initialization 2               #
############################################################

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Given the estimates of Example 1, estimate a 3-component mixture using   ~
# Initialization 2. The number of different runs is set to $tsplit=2$ with ~
# each one of them using $msplit=5$ em iterations.                         ~
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
run.previous&lt;-run
## number of conditions
q &lt;- 3
## number of covariates
tau &lt;- 1
## number of components
nc &lt;- 3
## estimated conditional probabilities for K=2
z &lt;- run.previous$z
## number of iteration that the previous EM converged
ml &lt;- length(run.previous$psim)/(nc - 1) 	
## estimates of alpha when K=2
alpha &lt;- array(run.previous$alpha[ml, , ], dim = c(q, nc - 1)) 
## estimates of beta when K=2
beta &lt;- array(run.previous$beta[ml, , , ], dim = c(q, nc - 1, tau))
clust &lt;- run.previous$clust ##(estimated clusters when K=2)

run &lt;- bjkmodel(reference=x, response=y, L=c(3,2,1), m=1000, K=3, nr=-10*log(10), 
                maxnr=10, m1, m2, t1, t2, msplit=5, tsplit=2, prev.z=z, 
                prev.clust=clust, start.type=2, prev.alpha=alpha, prev.beta=beta)

# retrieve the iteration that EM converged 
tem &lt;- length(run$psim)/nc
# estimates of the mixture weights
run$psim[tem,]
# estimates of the regression constants alpha_{jk}, j = 1,2,3, k=1,..,3
run$alpha[tem,,]
# estimates of the regression coefficients beta_{jk\tau}, j = 1,2,3, k=1,..,3, \tau=1
run$beta[tem,,,]



# note: useR should specify larger values for Kmax, m1, m2, t1, t2, msplit and 
#	tsplit for a complete analysis.




</code></pre>

<hr>
<h2 id='bjmodel'>
EM algorithm for the <code class="reqn">\beta_{j}</code> (m=2) Poisson GLM mixture.
</h2><span id='topic+bjmodel'></span>

<h3>Description</h3>

<p>This function applies  EM algorithm for estimating a <code class="reqn">K</code>-component mixture of Poisson GLM's, using parameterization <code class="reqn">m=2</code>, that is the <code class="reqn">\beta_{j}</code> model. Initialization can be done using two different intialization schemes. The first one is a two-step small EM procedure. The second  one is  a random splitting small EM procedure based on results of a mixture with less components. Output of the function is the updates of the parameters at each iteration of the EM algorithm, the estimate of <code class="reqn">\gamma</code>, the estimated clusters and conditional probabilities of the observations, as well as the values of the BIC, ICL and loglikelihood of the model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bjmodel(reference, response, L, m, K, nr, maxnr, m1, m2, t1, t2, 
        msplit, tsplit, prev.z, prev.clust, start.type, 
        prev.alpha, prev.beta)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bjmodel_+3A_reference">reference</code></td>
<td>
<p>a numeric array of dimension <code class="reqn">n\times V</code> containing the <code class="reqn">V</code> covariates for each of the <code class="reqn">n</code> observations.
</p>
</td></tr>
<tr><td><code id="bjmodel_+3A_response">response</code></td>
<td>
<p>a numeric array of count data with dimension <code class="reqn">n\times d</code> containing the <code class="reqn">d</code> response variables for each of the <code class="reqn">n</code> observations.
</p>
</td></tr>
<tr><td><code id="bjmodel_+3A_l">L</code></td>
<td>
<p>numeric vector of positive integers containing the partition of the <code class="reqn">d</code> response variables into <code class="reqn">J\leq d</code> blocks, with <code class="reqn">\sum_{j=1}^{J}L_j=d</code>.
</p>
</td></tr>
<tr><td><code id="bjmodel_+3A_m">m</code></td>
<td>
<p>positive integer denoting the maximum number of EM iterations.
</p>
</td></tr>
<tr><td><code id="bjmodel_+3A_k">K</code></td>
<td>
<p>positive integer denoting the number of mixture components.
</p>
</td></tr>
<tr><td><code id="bjmodel_+3A_nr">nr</code></td>
<td>
<p>negative number denoting the tolerance for the convergence of the Newton Raphson iterations.
</p>
</td></tr>
<tr><td><code id="bjmodel_+3A_maxnr">maxnr</code></td>
<td>
<p>positive integer denoting the maximum number of Newton Raphson iterations.
</p>
</td></tr>
<tr><td><code id="bjmodel_+3A_m1">m1</code></td>
<td>
<p>positive integer denoting the number of iterations for each call of the 1st small EM iterations used by Initialization 1 (<code>init1.1.jk.j</code>).
</p>
</td></tr>
<tr><td><code id="bjmodel_+3A_m2">m2</code></td>
<td>
<p>positive integer denoting the number of iterations for each call of the 2nd small EM iterations used by Initialization 1 (<code>init1.2.jk.j</code>).
</p>
</td></tr>
<tr><td><code id="bjmodel_+3A_t1">t1</code></td>
<td>
<p>positive integer denoting the number of different runs of the 1st small EM used by Initialization 1 (<code>init1.1.jk.j</code>).
</p>
</td></tr>
<tr><td><code id="bjmodel_+3A_t2">t2</code></td>
<td>
<p>positive integer denoting the number of different runs of the 2nd small EM used by Initialization 1 (<code>init1.2.jk.j</code>).
</p>
</td></tr>
<tr><td><code id="bjmodel_+3A_msplit">msplit</code></td>
<td>
<p>positive integer denoting the number of different runs for each call of the splitting small EM used by Initialization 2 (<code>init2.jk.j</code>).
</p>
</td></tr>
<tr><td><code id="bjmodel_+3A_tsplit">tsplit</code></td>
<td>
<p>positive integer denoting the number of different runs for each call of the splitting small EM used by Initialization 2 (<code>init2.jk.j</code>).
</p>
</td></tr>
<tr><td><code id="bjmodel_+3A_prev.z">prev.z</code></td>
<td>
<p>numeric array of dimension <code class="reqn">n\times(K-1)</code> containing the estimates of the posterior probabilities according to the previous run of EM. This is used when Initialization 2 is adopted.
</p>
</td></tr>
<tr><td><code id="bjmodel_+3A_prev.clust">prev.clust</code></td>
<td>
<p>numeric vector of length <code class="reqn">n</code> containing the estimated clusters according to the MAP rule obtained by the previous run of EM. This is used when Initialization 2 is adopted.
</p>
</td></tr>
<tr><td><code id="bjmodel_+3A_start.type">start.type</code></td>
<td>
<p>binary variable (1 or 2) indicating the type of initialization (1 for initialization 1 and 2 for initialization 2).
</p>
</td></tr>
<tr><td><code id="bjmodel_+3A_prev.alpha">prev.alpha</code></td>
<td>
<p>numeric array of dimension <code class="reqn">J\times (K-1)</code> containing the matrix of the ML estimates of the regression constants <code class="reqn">\alpha_{jk}</code>, <code class="reqn">j=1,\ldots,J</code>, <code class="reqn">k=1,\ldots,K-1</code>, based on the previous run of EM algorithm. This is used in case of Initialization 2.
</p>
</td></tr>
<tr><td><code id="bjmodel_+3A_prev.beta">prev.beta</code></td>
<td>
<p>numeric array of dimension <code class="reqn">J\times T</code> containing the matrix of the ML estimates of the regression coefficients <code class="reqn">\beta_{j\tau}</code>,<code class="reqn">j=1,\ldots,J</code>, <code class="reqn">\tau=1,\ldots,T</code>, based on the previous run of EM algorithm. This is used in case of Initialization 2.
</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>alpha</code></td>
<td>
<p>numeric array of dimension <code class="reqn">t_{EM}\times J \times K</code> containing the updates of regression constants <code class="reqn">\alpha_{jk}^{(t)}</code>, <code class="reqn">j=1,\ldots,J</code>, <code class="reqn">k=1,\ldots,K</code>, for each iteration <code class="reqn">t=1,2,\ldots,t_{EM}</code> of the EM algorithm.</p>
</td></tr>
<tr><td><code>beta</code></td>
<td>
<p>numeric array of dimension <code class="reqn">t_{EM}\times J \times T</code> containing the updates of regression coefficients <code class="reqn">\beta_{j\tau}^{(t)}</code>, <code class="reqn">j=1,\ldots,J</code>, <code class="reqn">\tau=1,\ldots,T</code>, for each iteration <code class="reqn">t=1,2,\ldots,t_{EM}</code> of the EM algorithm.</p>
</td></tr>
<tr><td><code>gamma</code></td>
<td>
<p>numeric array of dimension <code class="reqn">J \times \max(L)</code> containing the MLE of <code class="reqn">\gamma_{j\ell}</code>, <code class="reqn">j=1,\ldots,J</code>, <code class="reqn">\ell=1,\ldots,L_j</code>.</p>
</td></tr>
<tr><td><code>psim</code></td>
<td>
<p>numeric array of dimension <code class="reqn">t_{EM}\times K</code> containing the updates of mixture weights <code class="reqn">\pi_{k}^{(t)}</code>, <code class="reqn">k=1,\ldots,K</code>, for each iteration <code class="reqn">t=1,2,\ldots,t_{EM}</code> of the EM algorithm.</p>
</td></tr>
<tr><td><code>clust</code></td>
<td>
<p>numeric vector of length <code class="reqn">n</code> containing the estimated cluster for each observation according to the MAP rule.</p>
</td></tr>
<tr><td><code>z</code></td>
<td>
<p>numeric array of length <code class="reqn">n\times K</code> containing the estimated conditional probabilities <code class="reqn">\tau_{ik}</code>, <code class="reqn">i=1,\ldots,n</code>, <code class="reqn">k=,\ldots,K</code>, according to the last iteration of the EM algorithm.</p>
</td></tr>
<tr><td><code>bic</code></td>
<td>
<p>numeric, the value of the BIC.</p>
</td></tr>
<tr><td><code>icl</code></td>
<td>
<p>numeric, the value of the ICL.</p>
</td></tr>
<tr><td><code>ll</code></td>
<td>
<p>numeric, the value of the loglikelihood, computed according to the <code>mylogLikePoisMix</code> function.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Panagiotis Papastamoulis
</p>


<h3>See Also</h3>

<p><code><a href="#topic+init1.1.jk.j">init1.1.jk.j</a></code>, <code><a href="#topic+init1.2.jk.j">init1.2.jk.j</a></code>, <code><a href="#topic+init2.jk.j">init2.jk.j</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>############################################################
#1.            Example with Initialization 1               #
############################################################


## load a simulated dataset according to the b_jk model
## number of observations: 500
## design: L=(3,2,1)
data("simulated_data_15_components_bjk")
x &lt;- sim.data[,1]
x &lt;- array(x,dim=c(length(x),1))
y &lt;- sim.data[,-1]
## use Initialization 1 with 2 components
## the number of different 1st small runs equals t1=2, 
##	each one consisting of m1 = 5 iterations
## the number of different 2nd small runs equals t2=5, 
##	each one consisting of m2 = 10 iterations
## the maximum number of EM iterations is set to m = 1000.
nc &lt;- 2
run &lt;- bjmodel(reference=x, response=y, L=c(3,2,1), m=1000, K=nc, nr=-10*log(10), 
                maxnr=10, m1=5, m2=10, t1=2, t2=5, msplit, tsplit, prev.z, 
                prev.clust, start.type=1, prev.alpha, prev.beta) 
## retrieve the iteration that the small em converged:
tem &lt;- length(run$psim)/nc
## print the estimate of regression constants alpha.
run$alpha[tem,,]
## print the estimate of regression coefficients beta.
beta &lt;- run$beta[tem,,]
## print the estimate of gamma.
run$gamma
## print the estimate of mixture weights.
run$psim[tem,]
## frequency table of the resulting clustering of the 
##		500 observations among the 2 components.
table(run$clust)
## print the value of the ICL criterion
run$icl
## print the value of the BIC
run$bic
## print the value of the loglikelihood
run$ll


############################################################
#2.            Example with Initialization 2               #
############################################################

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Given the estimates of Example 1, estimate a 11-component mixture using  ~
# Initialization 2. The number of different runs is set to $tsplit=2$ with ~
# each one of them using $msplit=5$ em iterations.                         ~
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
run.previous&lt;-run
## number of conditions
q &lt;- 3
## number of covariates
tau &lt;- 1
## number of components
nc &lt;- 3
## estimated conditional probabilities for K=10
z &lt;- run.previous$z
## number of iteration that the previous EM converged
ml &lt;- length(run.previous$psim)/(nc - 1) 	
## estimates of alpha when K=2
alpha &lt;- array(run.previous$alpha[ml, , ], dim = c(q, nc - 1)) 
## estimates of beta when K=2
beta &lt;- array(run.previous$beta[ml, , ], dim = c(q, tau))
clust &lt;- run.previous$clust ##(estimated clusters when K=2)

run &lt;- bjmodel(reference=x, response=y, L=c(3,2,1), m=1000, K=3, nr=-10*log(10), 
                maxnr=10, m1, m2, t1, t2, msplit=5, tsplit=2, prev.z=z, 
                prev.clust=clust, start.type=2, prev.alpha=alpha, prev.beta=beta)

# retrieve the iteration that EM converged 
tem &lt;- length(run$psim)/nc
# estimates of the mixture weights
run$psim[tem,]
# estimates of the regression constants alpha_{jk}, j = 1,2,3, k=1,..,3
run$alpha[tem,,]
# estimates of the regression coefficients beta_{j\tau}, j = 1,2,3, \tau=1
run$beta[tem,,]


# note: useR should specify larger values for Kmax, m1, m2, t1, t2, msplit
#	 and tsplit for a complete analysis.


</code></pre>

<hr>
<h2 id='bkmodel'>
EM algorithm for the <code class="reqn">\beta_{k}</code> (m=3) Poisson GLM mixture.
</h2><span id='topic+bkmodel'></span>

<h3>Description</h3>

<p>This function applies EM algorithm for estimating a <code class="reqn">K</code>-component mixture of Poisson GLM's, using parameterization <code class="reqn">m=3</code>, that is the <code class="reqn">\beta_{k}</code> model. Initialization can be done using two different intialization schemes. The first one is a one-step small EM procedure. The second  one is  a random splitting small EM procedure based on results of a mixture with less components. Output of the function is the updates of the parameters at each iteration of the EM algorithm, the estimate of <code class="reqn">\gamma</code>, the estimated clusters and conditional probabilities of the observations, as well as the values of the BIC, ICL and loglikelihood of the model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bkmodel(reference, response, L, m, K, nr, maxnr, t2, m2, 
        prev.z, prev.clust, start.type, prev.alpha, prev.beta)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bkmodel_+3A_reference">reference</code></td>
<td>

<p>a numeric array of dimension <code class="reqn">n\times V</code> containing the <code class="reqn">V</code> covariates for each of the <code class="reqn">n</code> observations.
</p>
</td></tr>
<tr><td><code id="bkmodel_+3A_response">response</code></td>
<td>

<p>a numeric array of count data with dimension <code class="reqn">n\times d</code> containing the <code class="reqn">d</code> response variables for each of the <code class="reqn">n</code> observations.
</p>
</td></tr>
<tr><td><code id="bkmodel_+3A_l">L</code></td>
<td>
<p>numeric vector of positive integers containing the partition of the <code class="reqn">d</code> response variables into <code class="reqn">J\leq d</code> blocks, with <code class="reqn">\sum_{j=1}^{J}L_j=d</code>.
</p>
</td></tr>
<tr><td><code id="bkmodel_+3A_m">m</code></td>
<td>
<p>positive integer denoting the maximum number of EM iterations.
</p>
</td></tr>
<tr><td><code id="bkmodel_+3A_k">K</code></td>
<td>
<p>positive integer denoting the number of mixture components.
</p>
</td></tr>
<tr><td><code id="bkmodel_+3A_nr">nr</code></td>
<td>
<p>negative number denoting the tolerance for the convergence of the Newton Raphson iterations.
</p>
</td></tr>
<tr><td><code id="bkmodel_+3A_maxnr">maxnr</code></td>
<td>
<p>positive integer denoting the maximum number of Newton Raphson iterations.
</p>
</td></tr>
<tr><td><code id="bkmodel_+3A_t2">t2</code></td>
<td>
<p>positive integer denoting the number of different runs of the small EM used by Initialization 1 (<code>init1.k</code>).
</p>
</td></tr>
<tr><td><code id="bkmodel_+3A_m2">m2</code></td>
<td>
<p>positive integer denoting the number of iterations for each call of the small EM iterations used by Initialization 1 (<code>init1.k</code>).
</p>
</td></tr>
<tr><td><code id="bkmodel_+3A_prev.z">prev.z</code></td>
<td>
<p>numeric array of dimension <code class="reqn">n\times(K-1)</code> containing the estimates of the posterior probabilities according to the previous run of EM. This is used when Initialization 2 is adopted.
</p>
</td></tr>
<tr><td><code id="bkmodel_+3A_prev.clust">prev.clust</code></td>
<td>
<p>numeric vector of length <code class="reqn">n</code> containing the estimated clusters according to the MAP rule obtained by the previous run of EM. This is used when Initialization 2 is adopted.
</p>
</td></tr>
<tr><td><code id="bkmodel_+3A_start.type">start.type</code></td>
<td>
<p>binary variable (1 or 2) indicating the type of initialization (1 for initialization 1 and 2 for initialization 2).
</p>
</td></tr>
<tr><td><code id="bkmodel_+3A_prev.alpha">prev.alpha</code></td>
<td>
<p>numeric array of dimension <code class="reqn">J\times (K-1)</code> containing the matrix of the ML estimates of the regression constants <code class="reqn">\alpha_{jk}</code>, <code class="reqn">j=1,\ldots,J</code>, <code class="reqn">k=1,\ldots,K-1</code>, based on the previous run of EM algorithm. This is used in case of Initialization 2.
</p>
</td></tr>
<tr><td><code id="bkmodel_+3A_prev.beta">prev.beta</code></td>
<td>
<p>numeric array of dimension <code class="reqn">(K-1)\times T</code> containing the matrix of the ML estimates of the regression coefficients <code class="reqn">\beta_{k\tau}</code>, <code class="reqn">k=1,\ldots,K-1</code>, <code class="reqn">\tau=1,\ldots,T</code>, based on the previous run of EM algorithm. This is used in case of Initialization 2.
</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>alpha</code></td>
<td>
<p>numeric array of dimension <code class="reqn">t_{EM}\times J \times K</code> containing the updates of regression constants <code class="reqn">\alpha_{jk}^{(t)})</code>, <code class="reqn">j=1,\ldots,J</code>, <code class="reqn">k=1,\ldots,K</code>, for each iteration <code class="reqn">t=1,2,\ldots,t_{EM}</code> of the EM algorithm.</p>
</td></tr>
<tr><td><code>beta</code></td>
<td>
<p>numeric array of dimension <code class="reqn">t_{EM}\times K \times T</code> containing the updates of regression coefficients <code class="reqn">\beta_{k\tau}^{(t)})</code>, <code class="reqn">k=1,\ldots,K</code>, <code class="reqn">\tau=1,\ldots,T</code>, for each iteration <code class="reqn">t=1,2,\ldots,t_{EM}</code> of the EM algorithm.</p>
</td></tr>
<tr><td><code>gamma</code></td>
<td>
<p>numeric array of dimension <code class="reqn">J \times \max(L)</code> containing the MLE of <code class="reqn">\gamma_{j\ell}</code>, <code class="reqn">j=1,\ldots,J</code>, <code class="reqn">\ell=1,\ldots,L_j</code>.</p>
</td></tr>
<tr><td><code>psim</code></td>
<td>
<p>numeric array of dimension <code class="reqn">t_{EM}\times K</code> containing the updates of mixture weights <code class="reqn">\pi_{k}^{(t)})</code>, <code class="reqn">k=1,\ldots,K</code>, for each iteration <code class="reqn">t=1,2,\ldots,t_{EM}</code> of the EM algorithm.</p>
</td></tr>
<tr><td><code>clust</code></td>
<td>
<p>numeric vector of length <code class="reqn">n</code> containing the estimated cluster for each observation according to the MAP rule.</p>
</td></tr>
<tr><td><code>z</code></td>
<td>
<p>numeric array of length <code class="reqn">n\times K</code> containing the estimated conditional probabilities <code class="reqn">\tau_{ik}</code>, <code class="reqn">i=1,\ldots,n</code>, <code class="reqn">k=,\ldots,K</code>, according to the last iteration of the EM algorithm.</p>
</td></tr>
<tr><td><code>bic</code></td>
<td>
<p>numeric, the value of the BIC.</p>
</td></tr>
<tr><td><code>icl</code></td>
<td>
<p>numeric, the value of the ICL.</p>
</td></tr>
<tr><td><code>ll</code></td>
<td>
<p>numeric, the value of the loglikelihood, computed according to the <code>mylogLikePoisMix</code> function.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Panagiotis Papastamoulis
</p>


<h3>See Also</h3>

<p><code><a href="#topic+init1.k">init1.k</a></code>, <code><a href="#topic+init2.k">init2.k</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>############################################################
#1.            Example with Initialization 1               #
############################################################


## load a simulated dataset according to the b_jk model
## number of observations: 500
## design: L=(3,2,1)
data("simulated_data_15_components_bjk")
x &lt;- sim.data[,1]
x &lt;- array(x,dim=c(length(x),1))
y &lt;- sim.data[,-1]
## use Initialization 1 with 2 components
## the number of different small runs equals t2=5, 
##	each one consisting of m1 = 5 iterations
## the maximum number of EM iterations is set to m = 1000.
nc &lt;- 2
run &lt;- bkmodel(reference=x, response=y, L=c(3,2,1), m=1000, K=nc, nr=-10*log(10), 
               maxnr=10, t2=5, m2=5, prev.z, prev.clust, start.type=1, 
               prev.alpha, prev.beta)
## retrieve the iteration that the small em converged:
tem &lt;- length(run$psim)/nc
## print the estimate of regression constants alpha.
run$alpha[tem,,]
## print the estimate of regression coefficients beta.
beta &lt;- run$beta[tem,,]
## print the estimate of gamma.
run$gamma
## print the estimate of mixture weights.
run$psim[tem,]
## frequency table of the resulting clustering of the 
##		500 observations among the 2 components.
table(run$clust)
## print the value of the ICL criterion
run$icl
## print the value of the BIC
run$bic
## print the value of the loglikelihood
run$ll


############################################################
#2.            Example with Initialization 2               #
############################################################

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Given the estimates of Example 1, estimate a 3-component mixture using   ~
# Initialization 2. The number of different runs is set to $t2=2$ with     ~
# each one of them using $m2=5$ em iterations.                             ~
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
run.previous&lt;-run
## number of conditions
q &lt;- 3
## number of covariates
tau &lt;- 1
## number of components
nc &lt;- 3
## estimated conditional probabilities for K=10
z &lt;- run.previous$z
## number of iteration that the previous EM converged
ml &lt;- length(run.previous$psim)/(nc - 1) 	
## estimates of alpha when K=2
alpha &lt;- array(run.previous$alpha[ml, , ], dim = c(q, nc - 1)) 
## estimates of beta when K=2
beta &lt;- array(run.previous$beta[ml, , ], dim = c(nc - 1, tau))
clust &lt;- run.previous$clust ##(estimated clusters when K=2)


run &lt;- bkmodel(reference=x, response=y, L=c(3,2,1), m=1000, K=nc, nr=-10*log(10), 
               maxnr=10, t2=2, m2=5, prev.z=z, prev.clust=clust, start.type=2, 
               prev.alpha=alpha, prev.beta=beta)

# retrieve the iteration that EM converged 
tem &lt;- length(run$psim)/nc
# estimates of the mixture weights
run$psim[tem,]
# estimates of the regression constants alpha_{jk}, j = 1,2,3, k=1,..,11
run$alpha[tem,,]
# estimates of the regression coefficients beta_{k\tau}, k = 1,..,11, \tau=1
run$beta[tem,,]

# note: useR should specify larger values for Kmax, m1, m2, t1, t2 
#	for a complete analysis.



</code></pre>

<hr>
<h2 id='init1.1.jk.j'>
1st step of Initialization 1 for the <code class="reqn">\beta_{jk}</code> (<code class="reqn">m=1</code>) or <code class="reqn">\beta_{j}</code> (<code class="reqn">m=2</code>) parameterization.
</h2><span id='topic+init1.1.jk.j'></span>

<h3>Description</h3>

<p>This function is the first step of the two-step small initialization procedure (Initialization 1), used for the parameterizations <code class="reqn">m=1</code> (<code class="reqn">\beta_{jk}</code>) or <code class="reqn">m=2</code> (<code class="reqn">\beta_{j}</code>). For each condition <code class="reqn">j=1,\ldots,J</code>, a small EM is run in order to find some good starting values for the <code class="reqn">K</code>-component mixtures: <code class="reqn">\sum_{k=1}^{K}p_j\prod_{\ell=1}^{L_j}f(y_{ij\ell})</code>, independently for each <code class="reqn">j=1,\ldots,J</code>. These values are used in order to initialize the second step (<code>init1.2.jk.j</code>) of the small EM algorithm for fitting the overall mixture <code class="reqn">\sum_{k=1}^{K}\pi_j\prod_{j=1}^{J}\prod_{\ell=1}^{L_j}f(y_{ij\ell})</code>.</p>


<h3>Usage</h3>

<pre><code class='language-R'>init1.1.jk.j(reference, response, L, K, t1, model, m1,mnr)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="init1.1.jk.j_+3A_reference">reference</code></td>
<td>
<p>a numeric array of dimension <code class="reqn">n\times V</code> containing the <code class="reqn">V</code> covariates for each of the <code class="reqn">n</code> observations.
</p>
</td></tr>
<tr><td><code id="init1.1.jk.j_+3A_response">response</code></td>
<td>
<p>a numeric array of count data with dimension <code class="reqn">n\times d</code> containing the <code class="reqn">d</code> response variables for each of the <code class="reqn">n</code> observations.
</p>
</td></tr>
<tr><td><code id="init1.1.jk.j_+3A_l">L</code></td>
<td>
<p>numeric vector of positive integers containing the partition of the <code class="reqn">d</code> response variables into <code class="reqn">J\leq d</code> blocks, with <code class="reqn">\sum_{j=1}^{J}L_j=d</code>.
</p>
</td></tr>
<tr><td><code id="init1.1.jk.j_+3A_k">K</code></td>
<td>
<p>positive integer denoting the number of mixture components.
</p>
</td></tr>
<tr><td><code id="init1.1.jk.j_+3A_t1">t1</code></td>
<td>
<p>positive integer denoting the number of different runs.
</p>
</td></tr>
<tr><td><code id="init1.1.jk.j_+3A_model">model</code></td>
<td>
<p>binary variable denoting the parameterization of the model: 1 for <code class="reqn">\beta_{jk}</code> and 2 for <code class="reqn">\beta_{j}</code> parameterization.
</p>
</td></tr>
<tr><td><code id="init1.1.jk.j_+3A_m1">m1</code></td>
<td>
<p>positive integer denoting the number of iterations for each run.
</p>
</td></tr>
<tr><td><code id="init1.1.jk.j_+3A_mnr">mnr</code></td>
<td>
<p>positive integer denoting the maximum number of Newton-Raphson iterations.
</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>alpha</code></td>
<td>
<p>numeric array of dimension <code class="reqn">J \times K</code> containing the selected values <code class="reqn">\alpha_{jk}^{(0)}</code>, <code class="reqn">j=1,\ldots,J</code>, <code class="reqn">k=1,\ldots,K</code> that will be used to initialize the second step of the small EM.</p>
</td></tr>
<tr><td><code>beta</code></td>
<td>
<p>numeric array of dimension <code class="reqn">J \times K \times T</code> (if <code>model = 1</code>) or <code class="reqn">J \times T</code> (if <code>model = 2</code>) containing the selected values of <code class="reqn">\beta_{jk\tau}^{(t)}</code> (or <code class="reqn">\beta_{j\tau}^{(t)}</code>), <code class="reqn">j=1,\ldots,J</code>, <code class="reqn">k=1,\ldots,K</code>, <code class="reqn">\tau=1,\ldots,T</code>, that will be used to initialize the second step of the small EM.</p>
</td></tr>
<tr><td><code>psim</code></td>
<td>
<p>numeric vector of length <code class="reqn">K</code>.</p>
</td></tr>
<tr><td><code>ll</code></td>
<td>
<p>numeric, the value of the loglikelihood, computed according to the <code>mylogLikePoisMix</code> function.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Panagiotis Papastamoulis
</p>


<h3>See Also</h3>

<p><code><a href="#topic+init1.2.jk.j">init1.2.jk.j</a></code>, <code><a href="#topic+bjkmodel">bjkmodel</a></code>, <code><a href="#topic+bjmodel">bjmodel</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>############################################################
#1.            Example with beta_jk (m=1) model            #
############################################################
## load a simulated dataset according to the b_jk model
## number of observations: 500
## design: L=(3,2,1)
data("simulated_data_15_components_bjk")
x &lt;- sim.data[,1]
x &lt;- array(x,dim=c(length(x),1))
y &lt;- sim.data[,-1]
## initialize the component specific parameters 
##                   for a 2 component mixture
start1 &lt;- init1.1.jk.j(reference=x, response=y, L=c(3,2,1), 
                       K=2, t1=3, model=1, m1=5,mnr = 5)
summary(start1)

############################################################
#2.            Example with beta_j (m=2) model             #
############################################################

start1 &lt;- init1.1.jk.j(reference=x, response=y, L=c(3,2,1), 
                       K=2, t1=3, model=2, m1=5,mnr = 5)
summary(start1)

</code></pre>

<hr>
<h2 id='init1.2.jk.j'>
2nd step of Initialization 1 for the <code class="reqn">\beta_{jk}</code> (<code class="reqn">m=1</code>) or <code class="reqn">\beta_{j}</code> (<code class="reqn">m=2</code>) parameterization.
</h2><span id='topic+init1.2.jk.j'></span>

<h3>Description</h3>

<p>This function is the second step of the two-step small initialization procedure (Initialization 1), used for parameterizations <code class="reqn">m=1</code> or <code class="reqn">m=2</code>. At first, <code>init1.1.jk.j</code> is called for each condition <code class="reqn">j=1,\ldots,J</code>. The values obtained from the first step are used for initializing the second step of the small EM algorithm for fitting the overall mixture <code class="reqn">\sum_{k=1}^{K}\pi_j\prod_{j=1}^{J}\prod_{\ell=1}^{L_j}f(y_{ij\ell})</code>. The selected values from the second step are the ones that initialize the EM algorithm (<code>bjkmodel</code> or <code>bjmodel</code>), when <code class="reqn">K=K_{min}</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>init1.2.jk.j(reference, response, L, K, m1, m2, t1, t2, model,mnr)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="init1.2.jk.j_+3A_reference">reference</code></td>
<td>
<p>a numeric array of dimension <code class="reqn">n\times V</code> containing the <code class="reqn">V</code> covariates for each of the <code class="reqn">n</code> observations.
</p>
</td></tr>
<tr><td><code id="init1.2.jk.j_+3A_response">response</code></td>
<td>
<p>a numeric array of count data with dimension <code class="reqn">n\times d</code> containing the <code class="reqn">d</code> response variables for each of the <code class="reqn">n</code> observations.
</p>
</td></tr>
<tr><td><code id="init1.2.jk.j_+3A_l">L</code></td>
<td>
<p>numeric vector of positive integers containing the partition of the <code class="reqn">d</code> response variables into <code class="reqn">J\leq d</code> blocks, with <code class="reqn">\sum_{j=1}^{J}L_j=d</code>.
</p>
</td></tr>
<tr><td><code id="init1.2.jk.j_+3A_k">K</code></td>
<td>
<p>positive integer denoting the number of mixture components.
</p>
</td></tr>
<tr><td><code id="init1.2.jk.j_+3A_m1">m1</code></td>
<td>
<p>positive integer denoting the number of iterations for each run of <code>init1.1.jk.j</code>.
</p>
</td></tr>
<tr><td><code id="init1.2.jk.j_+3A_m2">m2</code></td>
<td>
<p>positive integer denoting the number of iterations for each run of <code>init1.2.jk.j</code>.
</p>
</td></tr>
<tr><td><code id="init1.2.jk.j_+3A_t1">t1</code></td>
<td>
<p>positive integer denoting the number of different runs of <code>init1.1.jk.j</code>.
</p>
</td></tr>
<tr><td><code id="init1.2.jk.j_+3A_t2">t2</code></td>
<td>
<p>positive integer denoting the number of different runs of <code>init1.2.jk.j</code>.
</p>
</td></tr>
<tr><td><code id="init1.2.jk.j_+3A_model">model</code></td>
<td>
<p>binary variable denoting the parameterization of the model: 1 for <code class="reqn">\beta_{jk}</code> and 2 for <code class="reqn">\beta_{j}</code> parameterization.
</p>
</td></tr>
<tr><td><code id="init1.2.jk.j_+3A_mnr">mnr</code></td>
<td>
<p>positive integer denoting the maximum number of Newton-Raphson iterations.
</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>alpha</code></td>
<td>
<p>numeric array of dimension <code class="reqn">J \times K</code> containing the selected values <code class="reqn">\alpha_{jk}^{(0)}</code>, <code class="reqn">j=1,\ldots,J</code>, <code class="reqn">k=1,\ldots,K</code> that will be used to initialize main EM.</p>
</td></tr>
<tr><td><code>beta</code></td>
<td>
<p>numeric array of dimension <code class="reqn">J \times K \times T</code> (if <code>model = 1</code>) or <code class="reqn">J \times T</code> (if <code>model = 2</code>) containing the selected values of <code class="reqn">\beta_{jk\tau}^{(0)}</code> (or <code class="reqn">\beta_{j\tau}^{(t)}</code>), <code class="reqn">j=1,\ldots,J</code>, <code class="reqn">k=1,\ldots,K</code>, <code class="reqn">\tau=1,\ldots,T</code>, that will be used to initialize the main EM.</p>
</td></tr>
<tr><td><code>psim</code></td>
<td>
<p>numeric vector of length <code class="reqn">K</code> containing the weights that will initialize the main EM.</p>
</td></tr>
<tr><td><code>ll</code></td>
<td>
<p>numeric, the value of the loglikelihood, computed according to the <code>mylogLikePoisMix</code> function.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Panagiotis Papastamoulis
</p>


<h3>See Also</h3>

<p><code><a href="#topic+init1.1.jk.j">init1.1.jk.j</a></code>, <code><a href="#topic+bjkmodel">bjkmodel</a></code>, <code><a href="#topic+bjmodel">bjmodel</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>############################################################
#1.            Example with beta_jk (m=1) model            #
############################################################
## load a simulated dataset according to the b_jk model
## number of observations: 500
## design: L=(3,2,1)
data("simulated_data_15_components_bjk")
x &lt;- sim.data[,1]
x &lt;- array(x,dim=c(length(x),1))
y &lt;- sim.data[,-1]
## initialize the parameters for a 2 component mixture
## the number of the overall small runs are t2 = 2
## each one consisting of m2 = 2 iterations of the EM.
## the number of the small runs for the first step small EM
## is t1 = 2, each one consisting of m1 = 2 iterations.
start2 &lt;- init1.2.jk.j(reference=x, response=y, L=c(3,2,1), 
                       K=2, m1=2, m2=2, t1=2, t2=2, model=1,mnr = 3)
summary(start2)

############################################################
#2.            Example with beta_j (m=2) model             #
############################################################

## initialize the parameters for a 2 component mixture
## the number of the overall small runs are t2 = 3
## each one consisting of m2 = 2 iterations of the EM.
## the number of the small runs for the first step small EM
## is t1 = 2, each one consisting of m1 = 2 iterations.
start2 &lt;- init1.2.jk.j(reference=x, response=y, L=c(3,2,1), 
                       K=2, m1=2, m2=2, t1=2, t2=3, model=2,mnr = 5)
summary(start2)



</code></pre>

<hr>
<h2 id='init1.k'>
Initialization 1 for the <code class="reqn">\beta_{k}</code> parameterization (<code class="reqn">m=3</code>).
</h2><span id='topic+init1.k'></span>

<h3>Description</h3>

<p>This function is the small initialization procedure (Initialization 1) for parameterization <code class="reqn">m=3</code>. The selected values are the ones that initialize the EM algorithm <code>bkmodel</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>init1.k(reference, response, L, K, t2, m2,mnr)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="init1.k_+3A_reference">reference</code></td>
<td>
<p>a numeric array of dimension <code class="reqn">n\times V</code> containing the <code class="reqn">V</code> covariates for each of the <code class="reqn">n</code> observations.
</p>
</td></tr>
<tr><td><code id="init1.k_+3A_response">response</code></td>
<td>
<p>a numeric array of count data with dimension <code class="reqn">n\times d</code> containing the <code class="reqn">d</code> response variables for each of the <code class="reqn">n</code> observations.
</p>
</td></tr>
<tr><td><code id="init1.k_+3A_l">L</code></td>
<td>
<p>numeric vector of positive integers containing the partition of the <code class="reqn">d</code> response variables into <code class="reqn">J\leq d</code> blocks, with <code class="reqn">\sum_{j=1}^{J}L_j=d</code>.
</p>
</td></tr>
<tr><td><code id="init1.k_+3A_k">K</code></td>
<td>
<p>positive integer denoting the number of mixture components.
</p>
</td></tr>
<tr><td><code id="init1.k_+3A_t2">t2</code></td>
<td>
<p>positive integer denoting the number of different runs.
</p>
</td></tr>
<tr><td><code id="init1.k_+3A_m2">m2</code></td>
<td>
<p>positive integer denoting the number of iterations for each run.
</p>
</td></tr>
<tr><td><code id="init1.k_+3A_mnr">mnr</code></td>
<td>
<p>positive integer denoting the maximum number of Newton-Raphson iterations.
</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>alpha</code></td>
<td>
<p>numeric array of dimension <code class="reqn">J \times K</code> containing the selected values <code class="reqn">\alpha_{jk}^{(0)}</code>, <code class="reqn">j=1,\ldots,J</code>, <code class="reqn">k=1,\ldots,K</code> that will be used to initialize main EM.</p>
</td></tr>
<tr><td><code>beta</code></td>
<td>
<p>numeric array of dimension <code class="reqn">K \times T</code> containing the selected values of <code class="reqn">\beta_{k\tau}^{(0)}</code>, <code class="reqn">k=1,\ldots,K</code>, <code class="reqn">\tau=1,\ldots,T</code>, that will be used to initialize the main EM.</p>
</td></tr>
<tr><td><code>psim</code></td>
<td>
<p>numeric vector of length <code class="reqn">K</code> containing the weights that will initialize the main EM.</p>
</td></tr>
<tr><td><code>ll</code></td>
<td>
<p>numeric, the value of the loglikelihood, computed according to the <code>mylogLikePoisMix</code> function.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Panagiotis Papastamoulis
</p>


<h3>See Also</h3>

<p><code><a href="#topic+bkmodel">bkmodel</a></code>, <code><a href="#topic+init2.k">init2.k</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## load a simulated dataset according to the b_jk model
## number of observations: 500
## design: L=(3,2,1)
data("simulated_data_15_components_bjk")
x &lt;- sim.data[,1]
x &lt;- array(x,dim=c(length(x),1))
y &lt;- sim.data[,-1]
## initialize the parameters for a 2 component mixture
## the number of the small runs are t2 = 3
## each one consisting of m2 = 5 iterations of the EM.
start1 &lt;- init1.k(reference=x, response=y, L=c(3,2,1), 
                       K=2, m2=5, t2=3,mnr = 5)
summary(start1)


</code></pre>

<hr>
<h2 id='init2.jk.j'>
Initialization 2 for the <code class="reqn">\beta_{jk}</code> (<code class="reqn">m=1</code>) or <code class="reqn">\beta_{j}</code> (<code class="reqn">m=2</code>) parameterization.
</h2><span id='topic+init2.jk.j'></span>

<h3>Description</h3>

<p>This function applies a random splitting small EM initialization scheme (Initialization 2), for parameterizations <code class="reqn">m=1</code> or 2. It can be implemented only in case where a previous run of the EM algorithm is available (with respect to the same parameterization). The initialization scheme proposes random splits of the existing clusters, increasing the number of mixture components by one. Then an EM is ran for (<code>msplit</code>) iterations and the procedure is repeated for <code>tsplit</code> times. The best values in terms of observed loglikelihood  are chosen to initialize the main EM algorithm (<code>bjkmodel</code> or <code>bjmodel</code>).  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>init2.jk.j(reference, response, L, K, tsplit, model, msplit, 
           previousz, previousclust, previous.alpha, previous.beta,mnr)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="init2.jk.j_+3A_reference">reference</code></td>
<td>
<p>a numeric array of dimension <code class="reqn">n\times V</code> containing the <code class="reqn">V</code> covariates for each of the <code class="reqn">n</code> observations.
</p>
</td></tr>
<tr><td><code id="init2.jk.j_+3A_response">response</code></td>
<td>
<p>a numeric array of count data with dimension <code class="reqn">n\times d</code> containing the <code class="reqn">d</code> response variables for each of the <code class="reqn">n</code> observations.
</p>
</td></tr>
<tr><td><code id="init2.jk.j_+3A_l">L</code></td>
<td>
<p>numeric vector of positive integers containing the partition of the <code class="reqn">d</code> response variables into <code class="reqn">J\leq d</code> blocks, with <code class="reqn">\sum_{j=1}^{J}L_j=d</code>.
</p>
</td></tr>
<tr><td><code id="init2.jk.j_+3A_k">K</code></td>
<td>
<p>positive integer denoting the number of mixture components.
</p>
</td></tr>
<tr><td><code id="init2.jk.j_+3A_tsplit">tsplit</code></td>
<td>
<p>positive integer denoting the number of different runs.
</p>
</td></tr>
<tr><td><code id="init2.jk.j_+3A_model">model</code></td>
<td>
<p>binary variable denoting the parameterization of the model: 1 for <code class="reqn">\beta_{jk}</code> and 2 for <code class="reqn">\beta_{j}</code> parameterization.
</p>
</td></tr>
<tr><td><code id="init2.jk.j_+3A_msplit">msplit</code></td>
<td>
<p>positive integer denoting the number of iterations for each run.
</p>
</td></tr>
<tr><td><code id="init2.jk.j_+3A_previousz">previousz</code></td>
<td>
<p>numeric array of dimension <code class="reqn">n\times(K-1)</code> containing the estimates of the posterior probabilities according to the previous run of EM. 
</p>
</td></tr>
<tr><td><code id="init2.jk.j_+3A_previousclust">previousclust</code></td>
<td>
<p>numeric vector of length $n$ containing the estimated clusters according to the MAP rule obtained by the previous run of EM.
</p>
</td></tr>
<tr><td><code id="init2.jk.j_+3A_previous.alpha">previous.alpha</code></td>
<td>
<p>numeric array of dimension <code class="reqn">J\times (K-1)</code> containing the matrix of the ML estimates of the regression constants <code class="reqn">\alpha_{jk}</code>, <code class="reqn">j=1,\ldots,J</code>, <code class="reqn">k=1,\ldots,K-1</code>, based on the previous run of EM algorithm.
</p>
</td></tr>
<tr><td><code id="init2.jk.j_+3A_previous.beta">previous.beta</code></td>
<td>
<p>numeric array of dimension <code class="reqn">J\times (K-1)\times T</code> (if <code>model = 1</code>) or <code class="reqn">J\times T</code> (if <code>model = 2</code>) containing the matrix of the ML estimates of the regression coefficients <code class="reqn">\beta_{jk\tau}</code> or <code class="reqn">\beta_{j\tau}</code>, <code class="reqn">j=1,\ldots,J</code>, <code class="reqn">k=1,\ldots,K-1</code>, <code class="reqn">\tau=1,\ldots,T</code>, based on the previous run of EM algorithm. 
</p>
</td></tr>
<tr><td><code id="init2.jk.j_+3A_mnr">mnr</code></td>
<td>
<p>positive integer denoting the maximum number of Newton-Raphson iterations.
</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>alpha</code></td>
<td>
<p>numeric array of dimension <code class="reqn">J \times K</code> containing the selected values <code class="reqn">\alpha_{jk}^{0})</code>, <code class="reqn">j=1,\ldots,J</code>, <code class="reqn">k=1,\ldots,K</code> that will be used to initialize main EM (<code>bjkmodel</code> or <code>bjmodel</code>).</p>
</td></tr>
<tr><td><code>beta</code></td>
<td>
<p>numeric array of dimension <code class="reqn">J \times K \times T</code> (if <code>model = 1</code>) or <code class="reqn">J \times T</code> (if <code>model = 2</code>) containing the selected values of <code class="reqn">\beta_{jk\tau}^{0})</code> (or <code class="reqn">\beta_{j\tau}^{t})</code>), <code class="reqn">j=1,\ldots,J</code>, <code class="reqn">k=1,\ldots,K</code>, <code class="reqn">\tau=1,\ldots,T</code>, that will be used to initialize the main EM.</p>
</td></tr>
<tr><td><code>psim</code></td>
<td>
<p>numeric vector of length <code class="reqn">K</code> containing the weights that will initialize the main EM.</p>
</td></tr>
<tr><td><code>ll</code></td>
<td>
<p>numeric, the value of the loglikelihood, computed according to the <code>mylogLikePoisMix</code> function.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>In case that an exhaustive search is desired instead of a random selection of the splitted components, use <code>tsplit = -1</code>.
</p>


<h3>Author(s)</h3>

<p>Panagiotis Papastamoulis
</p>


<h3>See Also</h3>

<p><code><a href="#topic+init1.1.jk.j">init1.1.jk.j</a></code>, <code><a href="#topic+init1.2.jk.j">init1.2.jk.j</a></code>, <code><a href="#topic+bjkmodel">bjkmodel</a></code>, <code><a href="#topic+bjmodel">bjmodel</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

data("simulated_data_15_components_bjk")
x &lt;- sim.data[,1]
x &lt;- array(x,dim=c(length(x),1))
y &lt;- sim.data[,-1]

# At first a 2 component mixture is fitted using parameterization $m=1$.
run.previous&lt;-bjkmodel(reference=x, response=y, L=c(3,2,1), m=100, K=2, 
                       nr=-10*log(10), maxnr=5, m1=2, m2=2, t1=1, t2=2, 
                       msplit, tsplit, prev.z, prev.clust, start.type=1, 
                       prev.alpha, prev.beta)
## Then the estimated clusters and parameters are used to initialize a 
##   3 component mixture using Initialization 2. The number of different
##   runs is set to $tsplit=3$ with each one of them using msplit = 2 
##   em iterations. 
q &lt;- 3
tau &lt;- 1
nc &lt;- 3
z &lt;- run.previous$z
ml &lt;- length(run.previous$psim)/(nc - 1)
alpha &lt;- array(run.previous$alpha[ml, , ], dim = c(q, nc - 1))
beta &lt;- array(run.previous$beta[ml, , , ], dim = c(q, nc - 1, tau))
clust &lt;- run.previous$clust
run&lt;-init2.jk.j(reference=x, response=y, L=c(3,2,1), K=nc, tsplit=2, 
                model=1, msplit=2, previousz=z, previousclust=clust,
                previous.alpha=alpha, previous.beta=beta,mnr = 5)
# note: useR should specify larger values for msplit and tsplit for a complete analysis.
</code></pre>

<hr>
<h2 id='init2.k'>
Initialization 2 for the  <code class="reqn">\beta_k</code> parameterization (<code class="reqn">m=3</code>).
</h2><span id='topic+init2.k'></span>

<h3>Description</h3>

<p>This function applies a random splitting small EM initialization scheme (Initialization 2), for parameterization <code class="reqn">m=3</code>. It can be implemented only in case where a previous run of the EM algorithm is available (with respect to the same parameterization). The initialization scheme proposes random splits of the existing clusters, increasing the number of mixture components by one. Then EM is ran for (<code>m2</code>) iterations, and the procedure is repeated for <code>t2</code> times. The best values in terms of observed loglikelihood are chosen in order to initialize the main EM algorithm (<code>bkmodel</code>), when <code class="reqn">K&gt;K_{min}</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>init2.k(reference, response, L, K, t2, m2, previousz, previousclust, 
        previous.alpha, previous.beta,mnr)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="init2.k_+3A_reference">reference</code></td>
<td>
<p>a numeric array of dimension <code class="reqn">n\times V</code> containing the <code class="reqn">V</code> covariates for each of the <code class="reqn">n</code> observations.
</p>
</td></tr>
<tr><td><code id="init2.k_+3A_response">response</code></td>
<td>
<p>a numeric array of count data with dimension <code class="reqn">n\times d</code> containing the <code class="reqn">d</code> response variables for each of the <code class="reqn">n</code> observations.
</p>
</td></tr>
<tr><td><code id="init2.k_+3A_l">L</code></td>
<td>
<p>numeric vector of positive integers containing the partition of the <code class="reqn">d</code> response variables into <code class="reqn">J\leq d</code> blocks, with <code class="reqn">\sum_{j=1}^{J}L_j=d</code>.
</p>
</td></tr>
<tr><td><code id="init2.k_+3A_k">K</code></td>
<td>
<p>positive integer denoting the number of mixture components.
</p>
</td></tr>
<tr><td><code id="init2.k_+3A_t2">t2</code></td>
<td>
<p>positive integer denoting the number of different runs.
</p>
</td></tr>
<tr><td><code id="init2.k_+3A_m2">m2</code></td>
<td>
<p>positive integer denoting the number of iterations for each run.
</p>
</td></tr>
<tr><td><code id="init2.k_+3A_previousz">previousz</code></td>
<td>
<p>numeric array of dimension <code class="reqn">n\times(K-1)</code> containing the estimates of the posterior probabilities according to the previous run of EM. 
</p>
</td></tr>
<tr><td><code id="init2.k_+3A_previousclust">previousclust</code></td>
<td>
<p>numeric vector of length <code class="reqn">n</code> containing the estimated clusters according to the MAP rule obtained by the previous run of EM.
</p>
</td></tr>
<tr><td><code id="init2.k_+3A_previous.alpha">previous.alpha</code></td>
<td>
<p>numeric array of dimension <code class="reqn">J\times (K-1)</code> containing the matrix of the ML estimates of the regression constants <code class="reqn">\alpha_{jk}</code>, <code class="reqn">j=1,\ldots,J</code>, <code class="reqn">k=1,\ldots,K-1</code>, based on the previous run of EM algorithm.
</p>
</td></tr>
<tr><td><code id="init2.k_+3A_previous.beta">previous.beta</code></td>
<td>
<p>numeric array of dimension <code class="reqn">(K-1)\times T</code> containing the matrix of the ML estimates of the regression coefficients <code class="reqn">\beta_{k\tau}</code>, <code class="reqn">k=1,\ldots,K-1</code>, <code class="reqn">\tau=1,\ldots,T</code>, based on the previous run of EM algorithm.
</p>
</td></tr>
<tr><td><code id="init2.k_+3A_mnr">mnr</code></td>
<td>
<p>positive integer denoting the maximum number of Newton-Raphson iterations.
</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>alpha</code></td>
<td>
<p>numeric array of dimension <code class="reqn">J \times K</code> containing the selected values <code class="reqn">\alpha_{jk}^{(0)}</code>, <code class="reqn">j=1,\ldots,J</code>, <code class="reqn">k=1,\ldots,K</code> that will be used to initialize main EM.</p>
</td></tr>
<tr><td><code>beta</code></td>
<td>
<p>numeric array of dimension <code class="reqn">K \times T</code> containing the selected values of <code class="reqn">\beta_{k\tau}^{(0)}</code>, <code class="reqn">k=1,\ldots,K</code>, <code class="reqn">\tau=1,\ldots,T</code>, that will be used to initialize the main EM.</p>
</td></tr>
<tr><td><code>psim</code></td>
<td>
<p>numeric vector of length <code class="reqn">K</code> containing the weights that will initialize the main EM.</p>
</td></tr>
<tr><td><code>ll</code></td>
<td>
<p>numeric, the value of the loglikelihood, computed according to the <code>mylogLikePoisMix</code> function.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>In case that an exhaustive search is desired instead of a random selection of the splitted components, use<code>t2 = -1</code>.
</p>


<h3>Author(s)</h3>

<p>Panagiotis Papastamoulis
</p>


<h3>See Also</h3>

<p><code><a href="#topic+init1.k">init1.k</a></code>, <code><a href="#topic+bkmodel">bkmodel</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># this is to be used as an example with the simulated data

data("simulated_data_15_components_bjk")
x &lt;- sim.data[,1]
x &lt;- array(x,dim=c(length(x),1))
y &lt;- sim.data[,-1]

# At first a 2 component mixture is fitted using parameterization $m=1$.
run.previous&lt;-bkmodel(reference=x, response=y, L=c(3,2,1), m=100, K=2, 
                      nr=-10*log(10), maxnr=5, m2=3, t2=3, prev.z, 
                      prev.clust, start.type=1, prev.alpha, prev.beta)
## Then the estimated clusters and parameters are used to initialize a 
##  3 component mixture using Initialization 2. The number of different 
##  runs is set to tsplit=3 with each one of them using msplit = 5
##  em iterations. 
q &lt;- 3
tau &lt;- 1
nc &lt;- 3
z &lt;- run.previous$z
ml &lt;- length(run.previous$psim)/(nc - 1)
alpha &lt;- array(run.previous$alpha[ml, , ], dim = c(q, nc - 1))
beta &lt;- array(run.previous$beta[ml, , ], dim = c(nc - 1, tau))
clust &lt;- run.previous$clust
run&lt;-init2.k(reference=x, response=y, L=c(3,2,1), K=nc, t2=3, m2=5, previousz=z, 
             previousclust=clust, previous.alpha=alpha, previous.beta=beta,mnr = 5)
summary(run)
# note: useR should specify larger values for m2, t2 for a complete analysis.
</code></pre>

<hr>
<h2 id='mylogLikePoisMix'>Function to compute the loglikelihood of the mixture.
</h2><span id='topic+mylogLikePoisMix'></span>

<h3>Description</h3>

<p>This function computes the observed loglikelihood given the means and the mixing proportions of each component. Instead of computing <code class="reqn">L_{i}=\log\sum_{k=1}^{K}g_{ik}</code>, <code class="reqn">i=1,\ldots,n</code>, where <code class="reqn">g_{ik}:=\pi_{k}\prod_{j=1}^{J}\prod_{\ell=1}^{L_j}f(y_{ij\ell}|\mu_{ijlk;m})</code>, <code class="reqn">h_{ik}:=\log g_{ik}</code> are computed for all <code class="reqn"> i </code>. Let <code class="reqn">h_{i}^{*}=\max\{h_{ik},k=1,\ldots,K\}</code>, then  <code class="reqn">L_{i}=h_{i}^{*}+\log\sum_{k=1}^{K}\exp(h_{ik}-h^{*}_{i})</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mylogLikePoisMix(y, mean, pi)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mylogLikePoisMix_+3A_y">y</code></td>
<td>
<p>a numeric array of count data with dimension <code class="reqn">n\times d</code>.
</p>
</td></tr>
<tr><td><code id="mylogLikePoisMix_+3A_mean">mean</code></td>
<td>
<p>a list of length K (the number of mixture components) of positive data. Each list element is a matrix with dimension <code class="reqn">n\times d</code> containing <code class="reqn">d</code> Poisson means for each of the <code class="reqn">n</code> observations.
</p>
</td></tr>
<tr><td><code id="mylogLikePoisMix_+3A_pi">pi</code></td>
<td>
<p>a numeric vector of length K (the number of mixture components) containing the mixture weights.
</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>ll</code></td>
<td>
<p>the value of the loglikelihood.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Panagiotis Papastamoulis
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## This example computes the loglikelihood of a K = 10 component 
##      Poisson GLM mixture. The number of response variables is
##      d = 6, while the sample size equals to n = 5000. They are
##      stored in the array sim.data[,-1]. The number of covariates
##      equals 1 (corresponding to sim.data[,1]). We will use a 
##      random generation of the regression coefficients alpha and 
##      beta, in order to show that the loglikelihood can be computed 
##      without computational errors even in cases where the parameters
##      are quite ''bad'' for the data.   

data("simulated_data_15_components_bjk_full")
K &lt;- 10
d &lt;- 6
n &lt;- dim(sim.data)[1]
condmean=vector("list",length=K)
weights&lt;-rep(1,K)/K
ar&lt;-array(data=NA,dim=c(n,d))
for (k in 1:K){
for (i in 1:d){
ar[,i]&lt;-runif(n)+(1+0.1*(runif(n)-1))*sim.data[,1]}
condmean[[k]]&lt;-ar}
mylogLikePoisMix(sim.data[,-1],condmean,weights)

</code></pre>

<hr>
<h2 id='pois.glm.mix'>
Main call function of the package.
</h2><span id='topic+pois.glm.mix'></span>

<h3>Description</h3>

<p>This function is the main function of the package. User has only to call it by specifying the data (<code class="reqn">x</code> and <code class="reqn">y</code>), the vector <code class="reqn">L</code>, the parameterization (<code class="reqn">m\in \{1,2,3\}</code>), the desirable range for the number of components, the type of initialization and the number of EM runs and iterations for the  small-EM strategy. When <code class="reqn">K=K_{min}</code>, EM algorithm is initialized according to Initialization scheme 1 (the functions <code>init1.1.jk.j</code>, <code>init1.2.jk.j</code>, <code>init1.k</code>). For consecutive run (<code class="reqn">K&gt;K_{min}</code>), EM algorithm is initialized using Initialization 2 (the functions <code>init2.jk.j</code> or <code>init2.k</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pois.glm.mix(reference, response, L, m, max.iter, Kmin, Kmax, 
             m1, m2, t1, t2, msplit, tsplit,mnr)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pois.glm.mix_+3A_reference">reference</code></td>
<td>
<p>a numeric array of dimension <code class="reqn">n\times V</code> containing the <code class="reqn">V</code> covariates for each of the <code class="reqn">n</code> observations.
</p>
</td></tr>
<tr><td><code id="pois.glm.mix_+3A_response">response</code></td>
<td>
<p>a numeric array of count data with dimension <code class="reqn">n\times d</code> containing the <code class="reqn">d</code> response variables for each of the <code class="reqn">n</code> observations.
</p>
</td></tr>
<tr><td><code id="pois.glm.mix_+3A_l">L</code></td>
<td>
<p>numeric vector of positive integers containing the partition of the <code class="reqn">d</code> response variables into <code class="reqn">J\leq d</code> blocks, with <code class="reqn">\sum_{j=1}^{J}L_j=d</code>.
</p>
</td></tr>
<tr><td><code id="pois.glm.mix_+3A_m">m</code></td>
<td>
<p>variable denoting the parameterization of the model: 1 for <code class="reqn">\beta_{jk}</code> , 2 for <code class="reqn">\beta_{j}</code> and 3 for <code class="reqn">\beta_{k}</code> parameterization.
</p>
</td></tr>
<tr><td><code id="pois.glm.mix_+3A_max.iter">max.iter</code></td>
<td>
<p>positive integer denoting the maximum number of EM iterations.
</p>
</td></tr>
<tr><td><code id="pois.glm.mix_+3A_kmin">Kmin</code></td>
<td>
<p>the minimum number of mixture components.
</p>
</td></tr>
<tr><td><code id="pois.glm.mix_+3A_kmax">Kmax</code></td>
<td>
<p>the maximum number of mixture components.
</p>
</td></tr>
<tr><td><code id="pois.glm.mix_+3A_m1">m1</code></td>
<td>
<p>positive integer denoting the number of iterations for each call of the 1st small EM iterations used by Initialization 1 (<code>init1.1.jk.j</code>). Leave blank in case of parameterization <code class="reqn">m=3</code>.
</p>
</td></tr>
<tr><td><code id="pois.glm.mix_+3A_m2">m2</code></td>
<td>
<p>positive integer denoting the number of iterations for each call of the overall small EM iterations used by Initialization 1 (<code>init1.2.jk.j</code> or <code>init1.k</code>). 
</p>
</td></tr>
<tr><td><code id="pois.glm.mix_+3A_t1">t1</code></td>
<td>
<p>positive integer denoting the number of different runs of the 1st small EM used by Initialization 1 (<code>init1.1.jk.j</code>). Leave blank in case of parameterization <code class="reqn">m=3</code>.
</p>
</td></tr>
<tr><td><code id="pois.glm.mix_+3A_t2">t2</code></td>
<td>
<p>positive integer denoting the number of different runs of the overall small EM used by Initialization 1 (<code>init1.2.jk.j</code> or <code>init1.k</code>).
</p>
</td></tr>
<tr><td><code id="pois.glm.mix_+3A_msplit">msplit</code></td>
<td>
<p>positive integer denoting the number of different runs for each call of the splitting small EM used by Initialization 2 (<code>init2.jk.j</code> or <code>init2.k</code>).
</p>
</td></tr>
<tr><td><code id="pois.glm.mix_+3A_tsplit">tsplit</code></td>
<td>
<p>positive integer denoting the number of different runs for each call of the splitting small EM used by Initialization 2 (<code>init2.jk.j</code> or <code>init2.k</code>).
</p>
</td></tr>
<tr><td><code id="pois.glm.mix_+3A_mnr">mnr</code></td>
<td>
<p>positive integer denoting the maximum number of Newton-Raphson iterations.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The output of the function is a list of lists. During the run of the function <code>pois.glm.mix</code> two <code>R</code> graphic devices are opened: The first one contains the graph of the information criteria (BIC and ICL). In the second graphe, the resulting fitted clusters per condition are plotted until the ICL criterion no longer selects a better model. Notice that in this graph the <code class="reqn">L_j</code> replicates of condition <code class="reqn">j=1,\ldots,J</code> are summed. 
</p>
<p>The EM algorithm is run until the increase to the loglikelihood of the mixture model is less than <code class="reqn">10^{-6}</code>. The Newton - Raphson iterations at the Maximization step of EM algorithm are repeated until the square Euclidean norm of the gradient vector of the component specific parameters is less than <code class="reqn">10^{-10}</code>. 
</p>


<h3>Value</h3>

<table>
<tr><td><code>information.criteria</code></td>
<td>
<p>numeric array of dimension <code class="reqn">(Kmax-Kmin + 1)\times 3</code> containing the values of BIC, ICL and loglikelihood for each <code class="reqn">K</code>. The latter is computed according to the function <code>mylogLikePoisMix</code>.</p>
</td></tr>
<tr><td><code>runs</code></td>
<td>
<p>A list containing the output for the estimated mixture for each <code class="reqn">K</code>. The output is the same as in the functions <code>bjkmodel</code>, <code>bjmodel</code> and <code>bkmodel</code>.</p>
</td></tr>
<tr><td><code>sel.mod.icl</code></td>
<td>
<p>The selected number of mixture components according to the ICL criterion.</p>
</td></tr>
<tr><td><code>sel.mod.bic</code></td>
<td>
<p>The selected number of mixture components according to the BIC.</p>
</td></tr>
<tr><td><code>est.sel.mod.icl</code></td>
<td>
<p>The final estimates for the selected number of mixture components according to the ICL criterion. It is a list containing <code class="reqn">\widehat{\pi}_k</code>, <code class="reqn">\widehat{\alpha}_{jk}</code>, <code class="reqn">\widehat{\beta}_{jkv}</code>, <code class="reqn">\widehat{\gamma}_{j\ell}</code>, <code class="reqn">\widehat{c}_{i}</code>, <code class="reqn">\widehat{\tau}_{ik}</code>, <code class="reqn">i=1,\ldots,n</code>, <code class="reqn">j=1,\ldots,J</code>, <code class="reqn">\ell=1,\ldots,L_j</code>, <code class="reqn">k=1,\ldots,\widehat{K}_{icl}</code>, <code class="reqn">v=1,\ldots,V</code>, while <code class="reqn">\widehat{c}_i</code> denotes the estimated cluster of observation <code class="reqn">i</code>, according to the MAP rule.</p>
</td></tr> 
<tr><td><code>est.sel.mod.bic</code></td>
<td>
<p>The final estimates for the selected number of mixture components according to the BIC. It is a list containing <code class="reqn">\widehat{\pi}_k</code>, <code class="reqn">\widehat{\alpha}_{jk}</code>, <code class="reqn">\widehat{\beta}_{jkv}</code>, <code class="reqn">\widehat{\gamma}_{j\ell}</code>, <code class="reqn">\widehat{c}_{i}</code>, <code class="reqn">\widehat{\tau}_{ik}</code>, <code class="reqn">i=1,\ldots,n</code>, <code class="reqn">j=1,\ldots,J</code>, <code class="reqn">\ell=1,\ldots,L_j</code>, <code class="reqn">k=1,\ldots,\widehat{K}_{bic}</code>, <code class="reqn">v=1,\ldots,V</code>.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>In case that an exhaustive search is desired instead of a random selection of the splitted components, use <code>tsplit = -1</code>.
</p>


<h3>Author(s)</h3>

<p>Panagiotis Papastamoulis
</p>


<h3>See Also</h3>

<p><code><a href="#topic+bjkmodel">bjkmodel</a></code>, <code><a href="#topic+bjmodel">bjmodel</a></code>, <code><a href="#topic+bkmodel">bkmodel</a></code>, <code><a href="#topic+init1.1.jk.j">init1.1.jk.j</a></code>, <code><a href="#topic+init1.2.jk.j">init1.2.jk.j</a></code>, <code><a href="#topic+init1.k">init1.k</a></code>, <code><a href="#topic+init2.jk.j">init2.jk.j</a></code>, <code><a href="#topic+init2.k">init2.k</a></code>, <code><a href="#topic+mylogLikePoisMix">mylogLikePoisMix</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## load a small dataset of 500 observations
data("simulated_data_15_components_bjk")
## in this example there is V = 1 covariates (x)
##   and d = 6 response variables (y). The design is
##   L = (3,2,1).
V &lt;- 1
x &lt;- array(sim.data[,1],dim=c(dim(sim.data)[1],V))
y &lt;- sim.data[,-1]

## We will run the algorithm using parameterization
##   m = 1 and the number of components in the set
##   {2,3,4}.

rr&lt;-pois.glm.mix(reference=x, response=y, L=c(3,2,1), m=1, 
                  max.iter=1000, Kmin=2, Kmax= 4, 
                  m1=3, m2=3, t1=3, t2=3, msplit=3, tsplit=3,mnr = 5)

# note: useR should specify larger values for Kmax, m1, m2, 
#	t1, t2, msplit and tsplit for a complete analysis.


# retrieve the selected models according to BIC or ICL
rr$sel.mod.icl
rr$sel.mod.bic
# retrieve the estimates according to ICL
# alpha
rr$est.sel.mod.icl$alpha
# beta
rr$est.sel.mod.icl$beta
# gamma
rr$est.sel.mod.icl$gamma
# pi
rr$est.sel.mod.icl$pi
# frequency table with estimated clusters
table(rr$est.sel.mod.icl$clust)
# histogram of the maximum conditional probabilities
hist(apply(rr$est.sel.mod.icl$tau,1,max),30)

##(the full data of 5000 observations can be loaded using 
##     data("simulated_data_15_components_bjk_full")
</code></pre>

<hr>
<h2 id='poisson.glm.mix'>Estimation of high dimensional Poisson GLMs via EM algorithm.</h2><span id='topic+poisson.glm.mix'></span><span id='topic+poisson.glm.mix-package'></span>

<h3>Description</h3>

<p>This package can be used to cluster high dimensional count data under the presence of covariates. A mixture of Poisson Generalized Linear models (GLM's) is proposed. Conditionally to the covariates, Poisson multivariate distribution describing each cluster is a product of independent Poisson distributions. Different parameterizations for the slopes are proposed. Case of partioning the response variables into a set of replicates is considered. Poisson GLM mixture is estimated via Expectation Maximization (EM) algorithm with Newton-Raphson steps. An efficient initialization of EM algorithm is proposed to improve parameter estimation. It is a splitting scheme which is combined with a Small EM strategy. The user is referred to the function <code><a href="#topic+pois.glm.mix">pois.glm.mix</a></code> for an automatic evaluation of the proposed methodology.
</p>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> poisson.glm.mix</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 1.4</td>
</tr>
<tr>
 <td style="text-align: left;">
Date: </td><td style="text-align: left;"> 2023-08-19</td>
</tr>

</table>

<p>Assume that the observed data can be written as <code class="reqn">y = (y_{1},\ldots,y_{n})</code> where <code class="reqn">y_i=\{y_{ij\ell};j = 1, \ldots,J,\ell = 1, \ldots,L_{j}\}</code>, <code class="reqn">y_i\in Z_+^{d}</code>, <code class="reqn">i = 1,\ldots,n</code>, with <code class="reqn">d = \sum_{j=1}^{J}L_{j}</code> and <code class="reqn">L_j \geq 1</code>, <code class="reqn">j=1,\ldots,J</code>. Index <code class="reqn">i</code> denotes the observation, while the vector <code class="reqn">L=(L_1,\ldots,L_J)</code> defines a partition of the <code class="reqn">d</code> variables into <code class="reqn">J</code> blocks: the first block consists of the first <code class="reqn">L_1</code> variables, the second block consists of the next <code class="reqn">L_2</code> variables and so on. We will refer to <code class="reqn">j</code> and <code class="reqn">\ell</code> using the terms &ldquo;condition&rdquo; and &ldquo;replicate&rdquo;, respectively. In addition to <code class="reqn">y</code>, consider that a vector of <code class="reqn">V</code> covariates is observed, denoted by <code class="reqn">x_{i} := \{x_{iv};v=1,\ldots,V\}</code>, for all <code class="reqn">i = 1, \ldots,n</code>.  Assume now that conditional to <code class="reqn">x_{i}</code>, a model indicator <code class="reqn">m</code> taking values in the  discrete set <code class="reqn">\{1,2,3\}</code> and a positive integer <code class="reqn">K</code>, the response <code class="reqn">y_{i}</code>, is a realization of the corresponding random vector </p>
<p style="text-align: center;"><code class="reqn">Y_{i}|x_{i}, m\sim \sum_{k = 1}^{K}\pi_{k}\prod_{j=1}^{J}\prod_{\ell=1}^{L_{j}}\mathcal P(\mu_{ij\ell k;m})</code>
</p>
<p> where <code class="reqn">\mathcal P</code> denotes the Poisson distribution. The following parameterizations for the Poisson means <code class="reqn">\mu_{ij\ell k;m}</code> are considered: If <code class="reqn">m=1</code> (the &ldquo;<code class="reqn">\beta_{jk}</code>&rdquo; parameterization), then </p>
<p style="text-align: center;"><code class="reqn">\mu_{ij\ell k;m}:=\alpha_{jk}+\gamma_{j\ell}+\sum_{v=1}^{V}\beta_{jkv}x_i.</code>
</p>
<p> If <code class="reqn">m=2</code> (the &ldquo;<code class="reqn">\beta_{j}</code>&rdquo; parameterization), then </p>
<p style="text-align: center;"><code class="reqn">\mu_{ij\ell k;m}:=\alpha_{jk}+\gamma_{j\ell}+\sum_{v=1}^{V}\beta_{jv}x_i.</code>
</p>
<p> If <code class="reqn">m=3</code> (the &ldquo;<code class="reqn">\beta_{k}</code>&rdquo; parameterization), then </p>
<p style="text-align: center;"><code class="reqn">\mu_{ij\ell k;m}:=\alpha_{jk}+\gamma_{j\ell}+\sum_{v=1}^{V}\beta_{kv}x_i.</code>
</p>
<p> For identifiability purposes assume that <code class="reqn">\sum_{\ell=1}^{L_j}\gamma_{j\ell}=0</code>, <code class="reqn">j=1,\ldots,J</code>.</p>


<h3>Author(s)</h3>

<p>Papastamoulis Panagiotis
Maintainer: Papastamoulis Panagiotis &lt;papapast@yahoo.gr&gt;
</p>


<h3>References</h3>

<p>Papastamoulis, P., Martin-Magniette, M. L., &amp; Maugis-Rabusseau, C. (2016). On the estimation of mixtures of Poisson regression models with large number of components. Computational Statistics &amp; Data Analysis, 93, 97-106.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## load a small dataset of 500 observations
data("simulated_data_15_components_bjk")
## in this example there is V = 1 covariates (x)
##   and d = 6 response variables (y). The design is
##   L = (3,2,1).
V &lt;- 1
x &lt;- array(sim.data[,1],dim=c(dim(sim.data)[1],V))
y &lt;- sim.data[,-1]

## We will run the algorithm using parameterization
##   m = 1 and the number of components in the set
##   {2,3,4}.

rr&lt;-pois.glm.mix(reference=x, response=y, L=c(3,2,1), m=1, 
                  max.iter=1000, Kmin=2, Kmax= 4, 
                  m1=3, m2=3, t1=3, t2=3, msplit=4, tsplit=3,mnr = 5)

# note: useR should specify larger values for Kmax, m1, m2, t1,
#	 t2, msplit and tsplit for a complete analysis.

# retrieve the selected models according to BIC or ICL
rr$sel.mod.icl
rr$sel.mod.bic
# retrieve the estimates according to ICL
# alpha
rr$est.sel.mod.icl$alpha
# beta
rr$est.sel.mod.icl$beta
# gamma
rr$est.sel.mod.icl$gamma
# pi
rr$est.sel.mod.icl$pi
# frequency table with estimated clusters
table(rr$est.sel.mod.icl$clust)
# histogram of the maximum conditional probabilities
hist(apply(rr$est.sel.mod.icl$tau,1,max),30)

##(the full data of 5000 observations can be loaded using 
##     data("simulated_data_15_components_bjk_full")

</code></pre>

<hr>
<h2 id='sim.data'>Simulated data set of 500 observations</h2><span id='topic+sim.data'></span>

<h3>Description</h3>

<p>This is a small dataset of 500 observations according to the <code class="reqn">\beta_{jk}</code> parameterization. The number of reference variables is 1 and correspond to the values in the first column of the array <code>sim.data</code>. The number of response variables is 6 and correspond to the values in the last 6 column of the array <code>sim.data</code>. There are <code class="reqn">J=3</code> conditions, while the number of replicates per condition is <code class="reqn">L=(3,2,1)</code>.</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(simulated_data_15_components_bjk)</code></pre>


<h3>Format</h3>

<p>A numeric array (sim.data) containing <code class="reqn">500\times 7</code> observations.</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
