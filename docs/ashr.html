<!DOCTYPE html><html><head><title>Help for package ashr</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {ashr}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#ash'><p>Adaptive Shrinkage</p></a></li>
<li><a href='#ash_pois'><p>Performs adaptive shrinkage on Poisson data</p></a></li>
<li><a href='#ashci'><p>Credible Interval Computation for the ash object</p></a></li>
<li><a href='#ashr'><p>ashr</p></a></li>
<li><a href='#calc_loglik'><p>Compute loglikelihood for data from ash fit</p></a></li>
<li><a href='#calc_logLR'><p>Compute loglikelihood ratio for data from ash fit</p></a></li>
<li><a href='#calc_mixmean'><p>Generic function of calculating the overall mean of the mixture</p></a></li>
<li><a href='#calc_mixsd'><p>Generic function of calculating the overall standard deviation of</p>
the mixture</a></li>
<li><a href='#calc_null_loglik'><p>Compute loglikelihood for data under null that all beta are 0</p></a></li>
<li><a href='#calc_null_vloglik'><p>Compute vector of loglikelihood for data under null that all</p>
beta are 0</a></li>
<li><a href='#calc_vloglik'><p>Compute vector of loglikelihood for data from ash fit</p></a></li>
<li><a href='#calc_vlogLR'><p>Compute vector of loglikelihood ratio for data from ash fit</p></a></li>
<li><a href='#cdf_conv'><p>cdf_conv</p></a></li>
<li><a href='#cdf_post'><p>cdf_post</p></a></li>
<li><a href='#cdf.ash'><p>cdf method for ash object</p></a></li>
<li><a href='#comp_cdf'><p>Generic function of computing the cdf for each component</p></a></li>
<li><a href='#comp_cdf_conv'><p>comp_cdf_conv</p></a></li>
<li><a href='#comp_cdf_conv.normalmix'><p>comp_cdf_conv.normalmix</p></a></li>
<li><a href='#comp_cdf_conv.unimix'><p>cdf of convolution of each component of a unif mixture</p></a></li>
<li><a href='#comp_cdf_post'><p>comp_cdf_post</p></a></li>
<li><a href='#comp_dens'><p>Generic function of calculating the component densities of the</p>
mixture</a></li>
<li><a href='#comp_dens_conv'><p>comp_dens_conv</p></a></li>
<li><a href='#comp_dens_conv.normalmix'><p>comp_dens_conv.normalmix</p></a></li>
<li><a href='#comp_dens_conv.unimix'><p>density of convolution of each component of a unif mixture</p></a></li>
<li><a href='#comp_mean'><p>Generic function of calculating the first moment of components of</p>
the mixture</a></li>
<li><a href='#comp_mean.normalmix'><p>comp_mean.normalmix</p></a></li>
<li><a href='#comp_mean.tnormalmix'><p>comp_mean.tnormalmix</p></a></li>
<li><a href='#comp_mean2'><p>Generic function of calculating the second moment of components of</p>
the mixture</a></li>
<li><a href='#comp_postmean'><p>comp_postmean</p></a></li>
<li><a href='#comp_postmean2'><p>comp_postmean2</p></a></li>
<li><a href='#comp_postprob'><p>comp_postprob</p></a></li>
<li><a href='#comp_postsd'><p>comp_postsd</p></a></li>
<li><a href='#comp_sd'><p>Generic function to extract the standard deviations of</p>
components of the mixture</a></li>
<li><a href='#comp_sd.normalmix'><p>comp_sd.normalmix</p></a></li>
<li><a href='#comp_sd.tnormalmix'><p>comp_sd.normalmix</p></a></li>
<li><a href='#compute_lfsr'><p>Function to compute the local false sign rate</p></a></li>
<li><a href='#cxxMixSquarem'><p>Brief description of function.</p></a></li>
<li><a href='#dens'><p>Find density at y, a generic function</p></a></li>
<li><a href='#dens_conv'><p>dens_conv</p></a></li>
<li><a href='#dlogf'><p>The log-F distribution</p></a></li>
<li><a href='#estimate_mixprop'><p>Estimate mixture proportions of a mixture g given noisy (error-prone) data from that mixture.</p></a></li>
<li><a href='#gen_etruncFUN'><p>gen_etruncFUN</p></a></li>
<li><a href='#get_density'><p>Density method for ash object</p></a></li>
<li><a href='#get_lfsr'><p>Return lfsr from an ash object</p></a></li>
<li><a href='#get_post_sample'><p>Sample from posterior</p></a></li>
<li><a href='#igmix'><p>Constructor for igmix class</p></a></li>
<li><a href='#lik_binom'><p>Likelihood object for Binomial error distribution</p></a></li>
<li><a href='#lik_logF'><p>Likelihood object for logF error distribution</p></a></li>
<li><a href='#lik_normal'><p>Likelihood object for normal error distribution</p></a></li>
<li><a href='#lik_normalmix'><p>Likelihood object for normal mixture error distribution</p></a></li>
<li><a href='#lik_pois'><p>Likelihood object for Poisson error distribution</p></a></li>
<li><a href='#lik_t'><p>Likelihood object for t error distribution</p></a></li>
<li><a href='#log_comp_dens_conv'><p>log_comp_dens_conv</p></a></li>
<li><a href='#log_comp_dens_conv.normalmix'><p>log_comp_dens_conv.normalmix</p></a></li>
<li><a href='#log_comp_dens_conv.unimix'><p>log density of convolution of each component of a unif mixture</p></a></li>
<li><a href='#loglik_conv'><p>loglik_conv</p></a></li>
<li><a href='#loglik_conv.default'><p>loglik_conv.default</p></a></li>
<li><a href='#mixcdf'><p>mixcdf</p></a></li>
<li><a href='#mixcdf.default'><p>mixcdf.default</p></a></li>
<li><a href='#mixEM'><p>Estimate mixture proportions of a mixture model by EM algorithm</p></a></li>
<li><a href='#mixIP'><p>Estimate mixture proportions of a mixture model by Interior Point method</p></a></li>
<li><a href='#mixmean2'><p>Generic function of calculating the overall second moment of the</p>
mixture</a></li>
<li><a href='#mixprop'><p>Generic function of extracting the mixture proportions</p></a></li>
<li><a href='#mixSQP'><p>Estimate mixture proportions of a mixture model using</p>
mix-SQP algorithm.</a></li>
<li><a href='#mixVBEM'><p>Estimate posterior distribution on mixture proportions of a mixture model by a Variational Bayes EM algorithm</p></a></li>
<li><a href='#my_e2truncbeta'><p>second moment of truncated Beta distribution</p></a></li>
<li><a href='#my_e2truncgamma'><p>second moment of truncated gamma distribution</p></a></li>
<li><a href='#my_e2truncnorm'><p>Expected Squared Value of Truncated Normal</p></a></li>
<li><a href='#my_e2trunct'><p>my_e2trunct</p></a></li>
<li><a href='#my_etruncbeta'><p>mean of truncated Beta distribution</p></a></li>
<li><a href='#my_etruncgamma'><p>mean of truncated gamma distribution</p></a></li>
<li><a href='#my_etrunclogf'><p>my_etrunclogf</p></a></li>
<li><a href='#my_etruncnorm'><p>Expected Value of Truncated Normal</p></a></li>
<li><a href='#my_etrunct'><p>my_etrunct</p></a></li>
<li><a href='#my_vtruncnorm'><p>Variance of Truncated Normal</p></a></li>
<li><a href='#ncomp'><p>ncomp</p></a></li>
<li><a href='#ncomp.default'><p>ncomp.default</p></a></li>
<li><a href='#normalmix'><p>Constructor for normalmix class</p></a></li>
<li><a href='#pcdf_post'><p>pcdf_post</p></a></li>
<li><a href='#plogf'><p>The log-F distribution</p></a></li>
<li><a href='#plot_diagnostic'><p>Diagnostic plots for ash object</p></a></li>
<li><a href='#plot.ash'><p>Plot method for ash object</p></a></li>
<li><a href='#pm_on_zero'><p>Generic function to extract which components of mixture are point mass on 0</p></a></li>
<li><a href='#post_sample'><p>post_sample</p></a></li>
<li><a href='#post_sample.normalmix'><p>post_sample.normalmix</p></a></li>
<li><a href='#post_sample.unimix'><p>post_sample.unimix</p></a></li>
<li><a href='#posterior_dist'><p>Compute Posterior</p></a></li>
<li><a href='#postmean'><p>postmean</p></a></li>
<li><a href='#postmean2'><p>postmean2</p></a></li>
<li><a href='#postsd'><p>postsd</p></a></li>
<li><a href='#print.ash'><p>Print method for ash object</p></a></li>
<li><a href='#prune'><p>prune</p></a></li>
<li><a href='#qval.from.lfdr'><p>Function to compute q values from local false discovery rates</p></a></li>
<li><a href='#set_data'><p>Takes raw data and sets up data object for use by ash</p></a></li>
<li><a href='#summary.ash'><p>Summary method for ash object</p></a></li>
<li><a href='#tnormalmix'><p>Constructor for tnormalmix class</p></a></li>
<li><a href='#unimix'><p>Constructor for unimix class</p></a></li>
<li><a href='#vcdf_post'><p>vcdf_post</p></a></li>
<li><a href='#w_mixEM'><p>Estimate mixture proportions of a mixture model by EM algorithm (weighted version)</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Peter Carbonetto &lt;pcarbo@uchicago.edu&gt;</td>
</tr>
<tr>
<td>Version:</td>
<td>2.2-63</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-08-21</td>
</tr>
<tr>
<td>Title:</td>
<td>Methods for Adaptive Shrinkage, using Empirical Bayes</td>
</tr>
<tr>
<td>Description:</td>
<td>The R package 'ashr' implements an Empirical Bayes
    approach for large-scale hypothesis testing and false discovery
    rate (FDR) estimation based on the methods proposed in
    M. Stephens, 2016, "False discovery rates: a new deal",
    &lt;<a href="https://doi.org/10.1093%2Fbiostatistics%2Fkxw041">doi:10.1093/biostatistics/kxw041</a>&gt;. These methods can be applied
    whenever two sets of summary statistics&mdash;estimated effects and
    standard errors&mdash;are available, just as 'qvalue' can be applied
    to previously computed p-values. Two main interfaces are
    provided: ash(), which is more user-friendly; and ash.workhorse(),
    which has more options and is geared toward advanced users. The
    ash() and ash.workhorse() also provides a flexible modeling
    interface that can accommodate a variety of likelihoods (e.g.,
    normal, Poisson) and mixture priors (e.g., uniform, normal).</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.1.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>Matrix, stats, graphics, Rcpp (&ge; 0.10.5), truncnorm, mixsqp,
SQUAREM, etrunct, invgamma</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat, knitr, rmarkdown, ggplot2, REBayes</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/stephens999/ashr">https://github.com/stephens999/ashr</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/stephens999/ashr/issues">https://github.com/stephens999/ashr/issues</a></td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.2</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-08-21 18:44:14 UTC; pcarbo</td>
</tr>
<tr>
<td>Author:</td>
<td>Matthew Stephens [aut],
  Peter Carbonetto [aut, cre],
  Chaoxing Dai [ctb],
  David Gerard [aut],
  Mengyin Lu [aut],
  Lei Sun [aut],
  Jason Willwerscheid [aut],
  Nan Xiao [aut],
  Mazon Zeng [ctb]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-08-21 23:50:03 UTC</td>
</tr>
</table>
<hr>
<h2 id='ash'>Adaptive Shrinkage</h2><span id='topic+ash'></span><span id='topic+ash.workhorse'></span>

<h3>Description</h3>

<p>Implements Empirical Bayes shrinkage and false discovery rate
methods based on unimodal prior distributions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ash(
  betahat,
  sebetahat,
  mixcompdist = c("uniform", "halfuniform", "normal", "+uniform", "-uniform",
    "halfnormal"),
  df = NULL,
  ...
)

ash.workhorse(
  betahat,
  sebetahat,
  method = c("fdr", "shrink"),
  mixcompdist = c("uniform", "halfuniform", "normal", "+uniform", "-uniform",
    "halfnormal"),
  optmethod = c("mixSQP", "mixIP", "cxxMixSquarem", "mixEM", "mixVBEM", "w_mixEM"),
  df = NULL,
  nullweight = 10,
  pointmass = TRUE,
  prior = c("nullbiased", "uniform", "unit"),
  mixsd = NULL,
  gridmult = sqrt(2),
  outputlevel = 2,
  g = NULL,
  fixg = FALSE,
  mode = 0,
  alpha = 0,
  grange = c(-Inf, Inf),
  control = list(),
  lik = NULL,
  weights = NULL,
  pi_thresh = 1e-10
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ash_+3A_betahat">betahat</code></td>
<td>
<p>a p vector of estimates</p>
</td></tr>
<tr><td><code id="ash_+3A_sebetahat">sebetahat</code></td>
<td>
<p>a p vector of corresponding standard errors</p>
</td></tr>
<tr><td><code id="ash_+3A_mixcompdist">mixcompdist</code></td>
<td>
<p>distribution of components in mixture used to represent the family G.
Depending on the choice of mixture component, the family G becomes more or less flexible.
Options are:<br />
</p>

<dl>
<dt>uniform</dt><dd><p>G is (approximately) any symmetric unimodal distribution</p>
</dd>
<dt>normal</dt><dd><p>G is (approximately) any scale mixture of normals</p>
</dd>
<dt>halfuniform</dt><dd><p>G is (approximately) any unimodal distribution</p>
</dd>
<dt>+uniform</dt><dd><p>G is (approximately) any unimodal distribution with support constrained to be greater than the mode.</p>
</dd>
<dt>-uniform</dt><dd><p>G is (approximately) any unimodal distribution with support constrained to be less than the mode.</p>
</dd>
<dt>halfnormal</dt><dd><p>G is (approximately) any scale mixture of truncated normals where the normals are truncated at the mode</p>
</dd>
</dl>

<p>If you are happy to assume a symmetric distribution for effects, you can use
&quot;uniform&quot; or &quot;normal&quot;. If you believe your effects
may be asymmetric, use &quot;halfuniform&quot; or &quot;halfnormal&quot;. If you want
to allow only positive/negative effects use &quot;+uniform&quot;/&quot;-uniform&quot;.
The use of &quot;normal&quot; and &quot;halfnormal&quot; is permitted only if df=NULL.</p>
</td></tr>
<tr><td><code id="ash_+3A_df">df</code></td>
<td>
<p>appropriate degrees of freedom for (t) distribution of
(betahat-beta)/sebetahat; default is NULL which is actually treated as
infinity (Gaussian)</p>
</td></tr>
<tr><td><code id="ash_+3A_...">...</code></td>
<td>
<p>Further arguments of function <code>ash</code> to be passed to
<code><a href="#topic+ash.workhorse">ash.workhorse</a></code>.</p>
</td></tr>
<tr><td><code id="ash_+3A_method">method</code></td>
<td>
<p>specifies how ash is to be run. Can be &quot;shrinkage&quot;
(if main aim is shrinkage) or &quot;fdr&quot; (if main aim is to assess false discovery rate
or false sign rate (fsr)). This is simply a convenient way to specify certain
combinations of parameters: &quot;shrinkage&quot; sets pointmass=FALSE and
prior=&quot;uniform&quot;; &quot;fdr&quot; sets pointmass=TRUE and prior=&quot;nullbiased&quot;.</p>
</td></tr>
<tr><td><code id="ash_+3A_optmethod">optmethod</code></td>
<td>
<p>specifies the function implementing an
optimization method.</p>
</td></tr>
<tr><td><code id="ash_+3A_nullweight">nullweight</code></td>
<td>
<p>scalar, the weight put on the prior under
&quot;nullbiased&quot; specification, see <code>prior</code></p>
</td></tr>
<tr><td><code id="ash_+3A_pointmass">pointmass</code></td>
<td>
<p>Logical, indicating whether to use a point mass at
zero as one of components for a mixture distribution.</p>
</td></tr>
<tr><td><code id="ash_+3A_prior">prior</code></td>
<td>
<p>string, or numeric vector indicating Dirichlet prior
on mixture proportions: &ldquo;nullbiased&rdquo;,
<code>c(nullweight,1,...,1)</code>, puts more weight on first component;
&ldquo;uniform&rdquo; is <code>c(1,1...,1)</code>; &ldquo;unit&rdquo; is
(1/K,...,1/K), for <code>optmethod = mixVBEM</code> version only.</p>
</td></tr>
<tr><td><code id="ash_+3A_mixsd">mixsd</code></td>
<td>
<p>Vector of standard deviations for underlying mixture components.</p>
</td></tr>
<tr><td><code id="ash_+3A_gridmult">gridmult</code></td>
<td>
<p>the multiplier by which the default grid values for
mixsd differ by one another. (Smaller values produce finer grids.)</p>
</td></tr>
<tr><td><code id="ash_+3A_outputlevel">outputlevel</code></td>
<td>
<p>Determines amount of output. There are several
numeric options: 0 = just fitted g; 1 = also PosteriorMean and
PosteriorSD; 2 = everything usually needed; 3 = also include results
of mixture fitting procedure (including matrix of log-likelihoods
used to fit mixture). 4 and 5 are reserved for outputting additional
data required by the (in-development) flashr package. The user can
also specify the output they require in detail (see Examples).</p>
</td></tr>
<tr><td><code id="ash_+3A_g">g</code></td>
<td>
<p>The prior distribution for beta. Usually this is unspecified (NULL) and
estimated from the data. However, it can be used in conjuction with fixg=TRUE
to specify the g to use (e.g. useful in simulations to do computations with the &quot;true&quot; g).
Or, if g is specified but fixg=FALSE, the g specifies the initial value of g used before optimization,
(which also implicitly specifies mixcompdist).</p>
</td></tr>
<tr><td><code id="ash_+3A_fixg">fixg</code></td>
<td>
<p>If TRUE, don't estimate g but use the specified g -
useful for computations under the &quot;true&quot; g in simulations.</p>
</td></tr>
<tr><td><code id="ash_+3A_mode">mode</code></td>
<td>
<p>either numeric (indicating mode of g) or string
&quot;estimate&quot;, to indicate mode should be estimated, or a two
dimension numeric vector to indicate the interval to be searched
for the mode.</p>
</td></tr>
<tr><td><code id="ash_+3A_alpha">alpha</code></td>
<td>
<p>Numeric value of alpha parameter in the model.</p>
</td></tr>
<tr><td><code id="ash_+3A_grange">grange</code></td>
<td>
<p>Two dimension numeric vector indicating the left and
right limit of g. Default is c(-Inf, Inf).</p>
</td></tr>
<tr><td><code id="ash_+3A_control">control</code></td>
<td>
<p>A list of control parameters passed to optmethod.</p>
</td></tr>
<tr><td><code id="ash_+3A_lik">lik</code></td>
<td>
<p>Contains details of the likelihood used; for general
ash. Currently, the following choices are allowed: normal (see
function lik_normal(); binomial likelihood (see function
lik_binom); likelihood based on logF error distribution (see
function lik_logF); mixture of normals likelihood (see function
lik_normalmix); and Poisson likelihood (see function lik_pois).</p>
</td></tr>
<tr><td><code id="ash_+3A_weights">weights</code></td>
<td>
<p>a vector of weights for observations; use with
optmethod = &quot;w_mixEM&quot;; this is currently beta-functionality.</p>
</td></tr>
<tr><td><code id="ash_+3A_pi_thresh">pi_thresh</code></td>
<td>
<p>a threshold below which to prune out mixture
components before computing summaries (speeds up computation since
empirically many components are usually assigned negligible
weight). The current implementation still returns the full fitted
distribution; this only affects the posterior summaries.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The ash function provides a number of ways to perform Empirical Bayes shrinkage
estimation and false discovery rate estimation. The main assumption is that
the underlying distribution of effects is unimodal. Novice users are recommended
to start with the examples provided below.
</p>
<p>In the simplest case the inputs to ash are a vector of estimates (betahat)
and their corresponding standard errors (sebetahat), and degrees of freedom (df).
The method assumes that for some (unknown) &quot;true&quot; vector of effects beta, the statistic
(betahat[j]-beta[j])/sebetahat[j] has a $t$ distribution on $df$ degrees of freedom.
(The default of df=NULL assumes a normal distribution instead of a t.)
</p>
<p>By default the method estimates the vector beta under the assumption that beta ~ g for a distribution
g in G, where G is some unimodal family of distributions to be specified (see parameter <code>mixcompdist</code>).
By default is to assume the mode is 0, and this is suitable for settings where you are interested in testing which beta[j]
are non-zero. To estimate the mode see parameter <code>mode</code>.
</p>
<p>As is standard in empirical Bayes methods, the fitting proceeds in two stages:
i) estimate g by maximizing a (possibly penalized) likelihood;
ii) compute the posterior distribution for each beta[j] | betahat[j],sebetahat[j]
using the estimated g as the prior distribution.
</p>
<p>A more general case allows that beta[j]/sebetahat[j]^alpha | sebetahat[j] ~ g.
</p>


<h3>Value</h3>

<p>ash returns an object of <code><a href="base.html#topic+class">class</a></code> &quot;ash&quot;, a
list with some or all of the following elements (determined by
outputlevel) <br />
</p>
<table>
<tr><td><code>fitted_g</code></td>
<td>
<p>fitted mixture</p>
</td></tr>
<tr><td><code>loglik</code></td>
<td>
<p>log P(D|fitted_g)</p>
</td></tr>
<tr><td><code>logLR</code></td>
<td>
<p>log[P(D|fitted_g)/P(D|beta==0)]</p>
</td></tr>
<tr><td><code>result</code></td>
<td>
<p>A dataframe whose columns are:</p>
</td></tr>
</table>

<dl>
<dt>NegativeProb</dt><dd><p>A vector of posterior probability that beta is
negative.</p>
</dd>
<dt>PositiveProb</dt><dd><p>A vector of posterior probability that beta is
positive.</p>
</dd>
<dt>lfsr</dt><dd><p>A vector of estimated local false sign rate.</p>
</dd>
<dt>lfdr</dt><dd><p>A vector of estimated local false discovery rate.</p>
</dd>
<dt>qvalue</dt><dd><p>A vector of q values.</p>
</dd>
<dt>svalue</dt><dd><p>A vector of s values.</p>
</dd>
<dt>PosteriorMean</dt><dd><p>A vector consisting the posterior mean of beta
from the mixture.</p>
</dd>
<dt>PosteriorSD</dt><dd><p>A vector consisting the corresponding posterior
standard deviation.</p>
</dd>
</dl>

<table>
<tr><td><code>call</code></td>
<td>
<p>a call in which all of the specified arguments are
specified by their full names</p>
</td></tr>
<tr><td><code>data</code></td>
<td>
<p>a list containing details of the data and models
used (mostly for internal use)</p>
</td></tr>
<tr><td><code>fit_details</code></td>
<td>
<p>a list containing results of mixture optimization,
and matrix of component log-likelihoods used in this optimization</p>
</td></tr>
</table>


<h3>Functions</h3>


<ul>
<li> <p><code>ash.workhorse</code>: Adaptive Shrinkage with full set of options.
</p>
</li></ul>


<h3>See Also</h3>

<p><code><a href="#topic+ashci">ashci</a></code> for computation of credible intervals
after getting the ash object return by <code>ash()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
beta = c(rep(0,100),rnorm(100))
sebetahat = abs(rnorm(200,0,1))
betahat = rnorm(200,beta,sebetahat)
beta.ash = ash(betahat, sebetahat)
names(beta.ash)
head(beta.ash$result) # the main dataframe of results
head(get_pm(beta.ash)) # get_pm returns posterior mean
head(get_lfsr(beta.ash)) # get_lfsr returns the local false sign rate
graphics::plot(betahat,get_pm(beta.ash),xlim=c(-4,4),ylim=c(-4,4))

## Not run: 
# Why is this example included here? -Peter
CIMatrix=ashci(beta.ash,level=0.95)
print(CIMatrix)

## End(Not run)

# Illustrating the non-zero mode feature.
betahat=betahat+5
beta.ash = ash(betahat, sebetahat)
graphics::plot(betahat,get_pm(beta.ash))
betan.ash=ash(betahat, sebetahat,mode=5)
graphics::plot(betahat,get_pm(betan.ash))
summary(betan.ash)

# Running ash with different error models
beta.ash1 = ash(betahat, sebetahat, lik = lik_normal())
beta.ash2 = ash(betahat, sebetahat, lik = lik_t(df=4))

e = rnorm(100)+log(rf(100,df1=10,df2=10)) # simulated data with log(F) error
e.ash = ash(e,1,lik=lik_logF(df1=10,df2=10))

# Specifying the output
beta.ash = ash(betahat, sebetahat, output = c("fitted_g","logLR","lfsr"))

#Running ash with a pre-specified g, rather than estimating it
beta = c(rep(0,100),rnorm(100))
sebetahat = abs(rnorm(200,0,1))
betahat = rnorm(200,beta,sebetahat)
true_g = normalmix(c(0.5,0.5),c(0,0),c(0,1)) # define true g
## Passing this g into ash causes it to i) take the sd and the means
## for each component from this g, and ii) initialize pi to the value
## from this g.
beta.ash = ash(betahat, sebetahat,g=true_g,fixg=TRUE)

# running with weights
beta.ash = ash(betahat, sebetahat, optmethod="w_mixEM",
               weights = c(rep(0.5,100),rep(1,100)))

# Different algorithms can be used to compute maximum-likelihood
# estimates of the mixture weights. Here, we illustrate use of the
# EM algorithm and the (default) SQP algorithm.
set.seed(1)
betahat  &lt;- c(8.115,9.027,9.289,10.097,9.463)
sebeta   &lt;- c(0.6157,0.4129,0.3197,0.3920,0.5496)
fit.em   &lt;- ash(betahat,sebeta,mixcompdist = "normal",optmethod = "mixEM")
fit.sqp  &lt;- ash(betahat,sebeta,mixcompdist = "normal",optmethod = "mixSQP")
range(fit.em$fitted$pi - fit.sqp$fitted$pi)
</code></pre>

<hr>
<h2 id='ash_pois'>Performs adaptive shrinkage on Poisson data</h2><span id='topic+ash_pois'></span>

<h3>Description</h3>

<p>Uses Empirical Bayes to fit the model </p>
<p style="text-align: center;"><code class="reqn">y_j | \lambda_j ~ Poi(c_j \lambda_j)</code>
</p>
<p> with </p>
<p style="text-align: center;"><code class="reqn">h(lambda_j) ~ g()</code>
</p>

<p>where <code class="reqn">h</code> is a specified link function (either &quot;identity&quot; or &quot;log&quot; are permitted).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ash_pois(y, scale = 1, link = c("identity", "log"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ash_pois_+3A_y">y</code></td>
<td>
<p>vector of Poisson observations.</p>
</td></tr>
<tr><td><code id="ash_pois_+3A_scale">scale</code></td>
<td>
<p>vector of scale factors for Poisson observations: the model is <code class="reqn">y[j]~Pois(scale[j]*lambda[j])</code>.</p>
</td></tr>
<tr><td><code id="ash_pois_+3A_link">link</code></td>
<td>
<p>string, either &quot;identity&quot; or &quot;log&quot;, indicating the link function.</p>
</td></tr>
<tr><td><code id="ash_pois_+3A_...">...</code></td>
<td>
<p>other parameters to be passed to ash</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The model is fit in two stages: i) estimate <code class="reqn">g</code> by maximum likelihood (over the set of symmetric
unimodal distributions) to give estimate <code class="reqn">\hat{g}</code>; 
ii) Compute posterior distributions for <code class="reqn">\lambda_j</code> given <code class="reqn">y_j,\hat{g}</code>.
Note that the link function <code class="reqn">h</code> affects the prior assumptions (because, e.g., assuming a unimodal prior on <code class="reqn">\lambda</code> is
different from assuming unimodal on <code class="reqn">\log\lambda</code>), but posterior quantities are always computed for the
for <code class="reqn">\lambda</code> and *not* <code class="reqn">h(\lambda)</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>   beta = c(rep(0,50),rexp(50))
   y = rpois(100,beta) # simulate Poisson observations
   y.ash = ash_pois(y,scale=1)
</code></pre>

<hr>
<h2 id='ashci'>Credible Interval Computation for the ash object</h2><span id='topic+ashci'></span>

<h3>Description</h3>

<p>Given the ash object returned by the main function ash,
this function computes a posterior credible interval (CI) for each observation. The ash object
must include a data component to use this function (which it does by default).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ashci(
  a,
  level = 0.95,
  betaindex,
  lfsr_threshold = 1,
  tol = 0.001,
  trace = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ashci_+3A_a">a</code></td>
<td>
<p>the fitted ash object</p>
</td></tr>
<tr><td><code id="ashci_+3A_level">level</code></td>
<td>
<p>the level for the credible interval, (default=0.95)</p>
</td></tr>
<tr><td><code id="ashci_+3A_betaindex">betaindex</code></td>
<td>
<p>a vector consisting of locations of betahat where
you would like to compute the credible interval</p>
</td></tr>
<tr><td><code id="ashci_+3A_lfsr_threshold">lfsr_threshold</code></td>
<td>
<p>a scalar, if specified then computes CIs only for observations
more significant than that threshold.</p>
</td></tr>
<tr><td><code id="ashci_+3A_tol">tol</code></td>
<td>
<p>passed to uniroot; indicates desired accuracy.</p>
</td></tr>
<tr><td><code id="ashci_+3A_trace">trace</code></td>
<td>
<p>a logical variable denoting whether some of the
intermediate results of iterations should be displayed to the
user. Default is FALSE.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Uses uniroot to find credible interval, one at a time for each observation. 
The computation cost is linear in number of observations.
</p>


<h3>Value</h3>

<p>A matrix, with 2 columns, ith row giving CI for ith observation
</p>


<h3>Examples</h3>

<pre><code class='language-R'>beta = c(rep(0,20),rnorm(20))
sebetahat = abs(rnorm(40,0,1))
betahat = rnorm(40,beta,sebetahat)
beta.ash = ash(betahat, sebetahat)

CImatrix=ashci(beta.ash,level=0.95)

CImatrix1=ashci(beta.ash,level=0.95,betaindex=c(1,2,5))
CImatrix2=ashci(beta.ash,level=0.95,lfsr_threshold=0.1)
</code></pre>

<hr>
<h2 id='ashr'>ashr</h2><span id='topic+ashr'></span><span id='topic+ashr-package'></span>

<h3>Description</h3>

<p>The main function in the ashr package is <code><a href="#topic+ash">ash</a></code>, which should be examined for more details. For simplicity only the most commonly-used options are documented under <code><a href="#topic+ash">ash</a></code>. For expert or interested users the documentation for function <code><a href="#topic+ash.workhorse">ash.workhorse</a></code> provides documentation on all implemented options.
</p>

<hr>
<h2 id='calc_loglik'>Compute loglikelihood for data from ash fit</h2><span id='topic+calc_loglik'></span>

<h3>Description</h3>

<p>Return the log-likelihood of the data for a given g() prior
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calc_loglik(g, data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="calc_loglik_+3A_g">g</code></td>
<td>
<p>the fitted g, or an ash object containing g</p>
</td></tr>
<tr><td><code id="calc_loglik_+3A_data">data</code></td>
<td>
<p>a data object, see set_data</p>
</td></tr>
</table>

<hr>
<h2 id='calc_logLR'>Compute loglikelihood ratio for data from ash fit</h2><span id='topic+calc_logLR'></span>

<h3>Description</h3>

<p>Return the log-likelihood ratio of the data for a given g() prior
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calc_logLR(g, data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="calc_logLR_+3A_g">g</code></td>
<td>
<p>the fitted g, or an ash object containing g</p>
</td></tr>
<tr><td><code id="calc_logLR_+3A_data">data</code></td>
<td>
<p>a data object, see set_data</p>
</td></tr>
</table>

<hr>
<h2 id='calc_mixmean'>Generic function of calculating the overall mean of the mixture</h2><span id='topic+calc_mixmean'></span>

<h3>Description</h3>

<p>Generic function of calculating the overall mean of the mixture
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calc_mixmean(m)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="calc_mixmean_+3A_m">m</code></td>
<td>
<p>a mixture of k components generated by normalmix() or
unimix() or igmix()</p>
</td></tr>
</table>


<h3>Value</h3>

<p>it returns scalar, the mean of the mixture distribution.
</p>

<hr>
<h2 id='calc_mixsd'>Generic function of calculating the overall standard deviation of
the mixture</h2><span id='topic+calc_mixsd'></span>

<h3>Description</h3>

<p>Generic function of calculating the overall standard deviation of
the mixture
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calc_mixsd(m)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="calc_mixsd_+3A_m">m</code></td>
<td>
<p>a mixture of k components generated by normalmix() or
unimix() or igmix()</p>
</td></tr>
</table>


<h3>Value</h3>

<p>it returns scalar
</p>

<hr>
<h2 id='calc_null_loglik'>Compute loglikelihood for data under null that all beta are 0</h2><span id='topic+calc_null_loglik'></span>

<h3>Description</h3>

<p>Return the log-likelihood of the data betahat, with
standard errors betahatsd, under the null that beta==0
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calc_null_loglik(data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="calc_null_loglik_+3A_data">data</code></td>
<td>
<p>a data object; see set_data</p>
</td></tr>
</table>

<hr>
<h2 id='calc_null_vloglik'>Compute vector of loglikelihood for data under null that all
beta are 0</h2><span id='topic+calc_null_vloglik'></span>

<h3>Description</h3>

<p>Return the vector of log-likelihoods of the data points under the null
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calc_null_vloglik(data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="calc_null_vloglik_+3A_data">data</code></td>
<td>
<p>a data object; see set_data</p>
</td></tr>
</table>

<hr>
<h2 id='calc_vloglik'>Compute vector of loglikelihood for data from ash fit</h2><span id='topic+calc_vloglik'></span>

<h3>Description</h3>

<p>Return the vector of log-likelihoods of the data
betahat, with standard errors betahatsd, for a given g() prior
on beta, or an ash object containing that
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calc_vloglik(g, data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="calc_vloglik_+3A_g">g</code></td>
<td>
<p>the fitted g, or an ash object containing g</p>
</td></tr>
<tr><td><code id="calc_vloglik_+3A_data">data</code></td>
<td>
<p>a data object, see set_data</p>
</td></tr>
</table>

<hr>
<h2 id='calc_vlogLR'>Compute vector of loglikelihood ratio for data from ash fit</h2><span id='topic+calc_vlogLR'></span>

<h3>Description</h3>

<p>Return the vector of log-likelihood ratios of the data
betahat, with standard errors betahatsd, for a given g() prior
on beta, or an ash object containing that, vs the null that g()
is point mass on 0
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calc_vlogLR(g, data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="calc_vlogLR_+3A_g">g</code></td>
<td>
<p>the fitted g, or an ash object containing g</p>
</td></tr>
<tr><td><code id="calc_vlogLR_+3A_data">data</code></td>
<td>
<p>a data object, see set_data</p>
</td></tr>
</table>

<hr>
<h2 id='cdf_conv'>cdf_conv</h2><span id='topic+cdf_conv'></span>

<h3>Description</h3>

<p>compute cdf of mixture m convoluted with error distribution
either normal of sd (s) or student t with df v at locations x
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cdf_conv(m, data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cdf_conv_+3A_m">m</code></td>
<td>
<p>mixture distribution with k components</p>
</td></tr>
<tr><td><code id="cdf_conv_+3A_data">data</code></td>
<td>
<p>details depend on the model</p>
</td></tr>
</table>

<hr>
<h2 id='cdf_post'>cdf_post</h2><span id='topic+cdf_post'></span>

<h3>Description</h3>

<p>evaluate cdf of posterior distribution of beta at c. m
is the prior on beta, a mixture; c is location of evaluation
assumption is betahat | beta ~ t_v(beta,sebetahat)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cdf_post(m, c, data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cdf_post_+3A_m">m</code></td>
<td>
<p>mixture distribution with k components</p>
</td></tr>
<tr><td><code id="cdf_post_+3A_c">c</code></td>
<td>
<p>a scalar</p>
</td></tr>
<tr><td><code id="cdf_post_+3A_data">data</code></td>
<td>
<p>details depend on model</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an n vector containing the cdf for beta_i at c
</p>


<h3>Examples</h3>

<pre><code class='language-R'>beta = rnorm(100,0,1)
betahat= beta+rnorm(100,0,1)
sebetahat=rep(1,100)
ash.beta = ash(betahat,1,mixcompdist="normal")
cdf0 = cdf_post(ash.beta$fitted_g,0,set_data(betahat,sebetahat))
graphics::plot(cdf0,1-get_pp(ash.beta))
</code></pre>

<hr>
<h2 id='cdf.ash'>cdf method for ash object</h2><span id='topic+cdf.ash'></span>

<h3>Description</h3>

<p>Computed the cdf of the underlying fitted distribution
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cdf.ash(a, x, lower.tail = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cdf.ash_+3A_a">a</code></td>
<td>
<p>the fitted ash object</p>
</td></tr>
<tr><td><code id="cdf.ash_+3A_x">x</code></td>
<td>
<p>the vector of locations at which cdf is to be computed</p>
</td></tr>
<tr><td><code id="cdf.ash_+3A_lower.tail">lower.tail</code></td>
<td>
<p>(default=TRUE) whether to compute the lower or upper tail</p>
</td></tr>
</table>


<h3>Details</h3>

<p>None
</p>

<hr>
<h2 id='comp_cdf'>Generic function of computing the cdf for each component</h2><span id='topic+comp_cdf'></span>

<h3>Description</h3>

<p>Generic function of computing the cdf for each component
</p>


<h3>Usage</h3>

<pre><code class='language-R'>comp_cdf(m, y, lower.tail = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="comp_cdf_+3A_m">m</code></td>
<td>
<p>a mixture (eg of type normalmix or unimix)</p>
</td></tr>
<tr><td><code id="comp_cdf_+3A_y">y</code></td>
<td>
<p>locations at which cdf to be computed</p>
</td></tr>
<tr><td><code id="comp_cdf_+3A_lower.tail">lower.tail</code></td>
<td>
<p>boolean indicating whether to report lower tail</p>
</td></tr>
</table>


<h3>Value</h3>

<p>it returns a vector of probabilities, with length equals to
number of components in m
</p>

<hr>
<h2 id='comp_cdf_conv'>comp_cdf_conv</h2><span id='topic+comp_cdf_conv'></span>

<h3>Description</h3>

<p>compute the cdf of data for each component of mixture when convolved with error distribution
</p>


<h3>Usage</h3>

<pre><code class='language-R'>comp_cdf_conv(m, data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="comp_cdf_conv_+3A_m">m</code></td>
<td>
<p>mixture distribution with k components</p>
</td></tr>
<tr><td><code id="comp_cdf_conv_+3A_data">data</code></td>
<td>
<p>details depend on the model</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a k by n matrix of cdfs
</p>

<hr>
<h2 id='comp_cdf_conv.normalmix'>comp_cdf_conv.normalmix</h2><span id='topic+comp_cdf_conv.normalmix'></span>

<h3>Description</h3>

<p>returns cdf of convolution of each component of a
normal mixture with N(0,s^2) at x. Note that
convolution of two normals is normal, so it works that way
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'normalmix'
comp_cdf_conv(m, data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="comp_cdf_conv.normalmix_+3A_m">m</code></td>
<td>
<p>mixture distribution with k components</p>
</td></tr>
<tr><td><code id="comp_cdf_conv.normalmix_+3A_data">data</code></td>
<td>
<p>a list with components x and s to be interpreted as a normally-distributed observation and its standard error</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a k by n matrix
</p>

<hr>
<h2 id='comp_cdf_conv.unimix'>cdf of convolution of each component of a unif mixture</h2><span id='topic+comp_cdf_conv.unimix'></span>

<h3>Description</h3>

<p>cdf of convolution of each component of a unif mixture
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'unimix'
comp_cdf_conv(m, data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="comp_cdf_conv.unimix_+3A_m">m</code></td>
<td>
<p>a mixture of class unimix</p>
</td></tr>
<tr><td><code id="comp_cdf_conv.unimix_+3A_data">data</code></td>
<td>
<p>see set_data()</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a k by n matrix
</p>

<hr>
<h2 id='comp_cdf_post'>comp_cdf_post</h2><span id='topic+comp_cdf_post'></span>

<h3>Description</h3>

<p>evaluate cdf of posterior distribution of beta at c. m
is the prior on beta, a mixture; c is location of evaluation
assumption is betahat | beta ~ t_v(beta,sebetahat)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>comp_cdf_post(m, c, data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="comp_cdf_post_+3A_m">m</code></td>
<td>
<p>mixture distribution with k components</p>
</td></tr>
<tr><td><code id="comp_cdf_post_+3A_c">c</code></td>
<td>
<p>a scalar</p>
</td></tr>
<tr><td><code id="comp_cdf_post_+3A_data">data</code></td>
<td>
<p>details depend on model</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a k by n matrix
</p>


<h3>Examples</h3>

<pre><code class='language-R'>beta = rnorm(100,0,1)
betahat= beta+rnorm(100,0,1)
sebetahat=rep(1,100)
ash.beta = ash(betahat,1,mixcompdist="normal")
comp_cdf_post(get_fitted_g(ash.beta),0,data=set_data(beta,sebetahat))
</code></pre>

<hr>
<h2 id='comp_dens'>Generic function of calculating the component densities of the
mixture</h2><span id='topic+comp_dens'></span>

<h3>Description</h3>

<p>Generic function of calculating the component densities of the
mixture
</p>


<h3>Usage</h3>

<pre><code class='language-R'>comp_dens(m, y, log = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="comp_dens_+3A_m">m</code></td>
<td>
<p>mixture of k components generated by normalmix() or
unimix() or igmix()</p>
</td></tr>
<tr><td><code id="comp_dens_+3A_y">y</code></td>
<td>
<p>is an n-vector of location</p>
</td></tr>
<tr><td><code id="comp_dens_+3A_log">log</code></td>
<td>
<p>whether to use log-scale on densities</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A k by n matrix of densities
</p>

<hr>
<h2 id='comp_dens_conv'>comp_dens_conv</h2><span id='topic+comp_dens_conv'></span>

<h3>Description</h3>

<p>compute the density of data for each component of mixture when convolved with error distribution
</p>


<h3>Usage</h3>

<pre><code class='language-R'>comp_dens_conv(m, data, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="comp_dens_conv_+3A_m">m</code></td>
<td>
<p>mixture distribution with k components</p>
</td></tr>
<tr><td><code id="comp_dens_conv_+3A_data">data</code></td>
<td>
<p>details depend on the model</p>
</td></tr>
<tr><td><code id="comp_dens_conv_+3A_...">...</code></td>
<td>
<p>other arguments</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a k by n matrix of densities
</p>

<hr>
<h2 id='comp_dens_conv.normalmix'>comp_dens_conv.normalmix</h2><span id='topic+comp_dens_conv.normalmix'></span>

<h3>Description</h3>

<p>returns density of convolution of each component of a
normal mixture with N(0,s^2) at x. Note that
convolution of two normals is normal, so it works that way
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'normalmix'
comp_dens_conv(m, data, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="comp_dens_conv.normalmix_+3A_m">m</code></td>
<td>
<p>mixture distribution with k components</p>
</td></tr>
<tr><td><code id="comp_dens_conv.normalmix_+3A_data">data</code></td>
<td>
<p>a list with components x and s to be interpreted as a normally-distributed observation and its standard error</p>
</td></tr>
<tr><td><code id="comp_dens_conv.normalmix_+3A_...">...</code></td>
<td>
<p>other arguments (unused)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a k by n matrix
</p>

<hr>
<h2 id='comp_dens_conv.unimix'>density of convolution of each component of a unif mixture</h2><span id='topic+comp_dens_conv.unimix'></span>

<h3>Description</h3>

<p>density of convolution of each component of a unif mixture
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'unimix'
comp_dens_conv(m, data, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="comp_dens_conv.unimix_+3A_m">m</code></td>
<td>
<p>a mixture of class unimix</p>
</td></tr>
<tr><td><code id="comp_dens_conv.unimix_+3A_data">data</code></td>
<td>
<p>see set_data()</p>
</td></tr>
<tr><td><code id="comp_dens_conv.unimix_+3A_...">...</code></td>
<td>
<p>other arguments (unused)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a k by n matrix
</p>

<hr>
<h2 id='comp_mean'>Generic function of calculating the first moment of components of
the mixture</h2><span id='topic+comp_mean'></span>

<h3>Description</h3>

<p>Generic function of calculating the first moment of components of
the mixture
</p>


<h3>Usage</h3>

<pre><code class='language-R'>comp_mean(m)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="comp_mean_+3A_m">m</code></td>
<td>
<p>a mixture of k components generated by normalmix() or
unimix() or igmix()</p>
</td></tr>
</table>


<h3>Value</h3>

<p>it returns a vector of means.
</p>

<hr>
<h2 id='comp_mean.normalmix'>comp_mean.normalmix</h2><span id='topic+comp_mean.normalmix'></span>

<h3>Description</h3>

<p>returns mean of the normal mixture
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'normalmix'
comp_mean(m)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="comp_mean.normalmix_+3A_m">m</code></td>
<td>
<p>a normal mixture distribution with k components</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a vector of length k
</p>

<hr>
<h2 id='comp_mean.tnormalmix'>comp_mean.tnormalmix</h2><span id='topic+comp_mean.tnormalmix'></span>

<h3>Description</h3>

<p>Returns mean of the truncated-normal mixture.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'tnormalmix'
comp_mean(m)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="comp_mean.tnormalmix_+3A_m">m</code></td>
<td>
<p>A truncated normal mixture distribution with k components.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of length k.
</p>

<hr>
<h2 id='comp_mean2'>Generic function of calculating the second moment of components of
the mixture</h2><span id='topic+comp_mean2'></span>

<h3>Description</h3>

<p>Generic function of calculating the second moment of components of
the mixture
</p>


<h3>Usage</h3>

<pre><code class='language-R'>comp_mean2(m)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="comp_mean2_+3A_m">m</code></td>
<td>
<p>a mixture of k components generated by normalmix() or
unimix() or igmix()</p>
</td></tr>
</table>


<h3>Value</h3>

<p>it returns a vector of second moments.
</p>

<hr>
<h2 id='comp_postmean'>comp_postmean</h2><span id='topic+comp_postmean'></span>

<h3>Description</h3>

<p>output posterior mean for beta for each component of
prior mixture m,given data
</p>


<h3>Usage</h3>

<pre><code class='language-R'>comp_postmean(m, data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="comp_postmean_+3A_m">m</code></td>
<td>
<p>mixture distribution with k components</p>
</td></tr>
<tr><td><code id="comp_postmean_+3A_data">data</code></td>
<td>
<p>details depend on the model</p>
</td></tr>
</table>

<hr>
<h2 id='comp_postmean2'>comp_postmean2</h2><span id='topic+comp_postmean2'></span>

<h3>Description</h3>

<p>output posterior mean-squared value given prior
mixture m and data
</p>


<h3>Usage</h3>

<pre><code class='language-R'>comp_postmean2(m, data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="comp_postmean2_+3A_m">m</code></td>
<td>
<p>mixture distribution with k components</p>
</td></tr>
<tr><td><code id="comp_postmean2_+3A_data">data</code></td>
<td>
<p>details depend on the model</p>
</td></tr>
</table>

<hr>
<h2 id='comp_postprob'>comp_postprob</h2><span id='topic+comp_postprob'></span>

<h3>Description</h3>

<p>compute the posterior prob that each observation came
from each component of the mixture m,output a k by n vector of
probabilities computed by weighting the component densities by
pi and then normalizing
</p>


<h3>Usage</h3>

<pre><code class='language-R'>comp_postprob(m, data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="comp_postprob_+3A_m">m</code></td>
<td>
<p>mixture distribution with k components</p>
</td></tr>
<tr><td><code id="comp_postprob_+3A_data">data</code></td>
<td>
<p>details depend on the model</p>
</td></tr>
</table>

<hr>
<h2 id='comp_postsd'>comp_postsd</h2><span id='topic+comp_postsd'></span>

<h3>Description</h3>

<p>output posterior sd for beta for each component of
prior mixture m,given data
</p>


<h3>Usage</h3>

<pre><code class='language-R'>comp_postsd(m, data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="comp_postsd_+3A_m">m</code></td>
<td>
<p>mixture distribution with k components</p>
</td></tr>
<tr><td><code id="comp_postsd_+3A_data">data</code></td>
<td>
<p>details depend on the model</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>beta = rnorm(100,0,1)
betahat= beta+rnorm(100,0,1)
ash.beta = ash(betahat,1,mixcompdist="normal")
data= set_data(betahat,rep(1,100))
comp_postmean(get_fitted_g(ash.beta),data)
comp_postsd(get_fitted_g(ash.beta),data)
comp_postprob(get_fitted_g(ash.beta),data)
</code></pre>

<hr>
<h2 id='comp_sd'>Generic function to extract the standard deviations of
components of the mixture</h2><span id='topic+comp_sd'></span>

<h3>Description</h3>

<p>Generic function to extract the standard deviations of
components of the mixture
</p>


<h3>Usage</h3>

<pre><code class='language-R'>comp_sd(m)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="comp_sd_+3A_m">m</code></td>
<td>
<p>a mixture of k components generated by normalmix() or
unimix() or igmix()</p>
</td></tr>
</table>


<h3>Value</h3>

<p>it returns a vector of standard deviations
</p>

<hr>
<h2 id='comp_sd.normalmix'>comp_sd.normalmix</h2><span id='topic+comp_sd.normalmix'></span>

<h3>Description</h3>

<p>returns sds of the normal mixture
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'normalmix'
comp_sd(m)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="comp_sd.normalmix_+3A_m">m</code></td>
<td>
<p>a normal mixture distribution with k components</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a vector of length k
</p>

<hr>
<h2 id='comp_sd.tnormalmix'>comp_sd.normalmix</h2><span id='topic+comp_sd.tnormalmix'></span>

<h3>Description</h3>

<p>Returns standard deviations of the truncated normal mixture.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'tnormalmix'
comp_sd(m)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="comp_sd.tnormalmix_+3A_m">m</code></td>
<td>
<p>A truncated normal mixture distribution with k components.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of length k.
</p>

<hr>
<h2 id='compute_lfsr'>Function to compute the local false sign rate</h2><span id='topic+compute_lfsr'></span>

<h3>Description</h3>

<p>Function to compute the local false sign rate
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compute_lfsr(NegativeProb, ZeroProb)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="compute_lfsr_+3A_negativeprob">NegativeProb</code></td>
<td>
<p>A vector of posterior probability that beta is
negative.</p>
</td></tr>
<tr><td><code id="compute_lfsr_+3A_zeroprob">ZeroProb</code></td>
<td>
<p>A vector of posterior probability that beta is
zero.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The local false sign rate.
</p>

<hr>
<h2 id='cxxMixSquarem'>Brief description of function.</h2><span id='topic+cxxMixSquarem'></span>

<h3>Description</h3>

<p>Explain here what this function does.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cxxMixSquarem(matrix_lik, prior, pi_init, control)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cxxMixSquarem_+3A_matrix_lik">matrix_lik</code></td>
<td>
<p>Description of argument goes here.</p>
</td></tr>
<tr><td><code id="cxxMixSquarem_+3A_prior">prior</code></td>
<td>
<p>Description of argument goes here.</p>
</td></tr>
<tr><td><code id="cxxMixSquarem_+3A_pi_init">pi_init</code></td>
<td>
<p>Description of argument goes shere.</p>
</td></tr>
<tr><td><code id="cxxMixSquarem_+3A_control">control</code></td>
<td>
<p>Description of argument goes here.</p>
</td></tr>
</table>

<hr>
<h2 id='dens'>Find density at y, a generic function</h2><span id='topic+dens'></span>

<h3>Description</h3>

<p>Find density at y, a generic function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dens(x, y)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dens_+3A_x">x</code></td>
<td>
<p>A mixture of k components generated by
<code><a href="#topic+normalmix">normalmix</a></code> or <code><a href="#topic+unimix">unimix</a></code>.</p>
</td></tr>
<tr><td><code id="dens_+3A_y">y</code></td>
<td>
<p>An n-vector of the location.</p>
</td></tr>
</table>

<hr>
<h2 id='dens_conv'>dens_conv</h2><span id='topic+dens_conv'></span>

<h3>Description</h3>

<p>compute density of mixture m convoluted with normal of
sd (s) or student t with df v at locations x
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dens_conv(m, data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dens_conv_+3A_m">m</code></td>
<td>
<p>mixture distribution with k components</p>
</td></tr>
<tr><td><code id="dens_conv_+3A_data">data</code></td>
<td>
<p>details depend on the model</p>
</td></tr>
</table>

<hr>
<h2 id='dlogf'>The log-F distribution</h2><span id='topic+dlogf'></span>

<h3>Description</h3>

<p>Density function for the log-F distribution with <code>df1</code> and <code>df2</code>
degrees of freedom (and optional non-centrality parameter <code>ncp</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dlogf(x, df1, df2, ncp, log = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dlogf_+3A_x">x</code></td>
<td>
<p>vector of quantiles</p>
</td></tr>
<tr><td><code id="dlogf_+3A_df1">df1</code></td>
<td>
<p>degrees of freedom</p>
</td></tr>
<tr><td><code id="dlogf_+3A_df2">df2</code></td>
<td>
<p>degrees of freedom</p>
</td></tr>
<tr><td><code id="dlogf_+3A_ncp">ncp</code></td>
<td>
<p>non-centrality parameter. If omitted the central F is assumed.</p>
</td></tr>
<tr><td><code id="dlogf_+3A_log">log</code></td>
<td>
<p>logical; if TRUE, probabilities p are given as log(p).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The density function.
</p>

<hr>
<h2 id='estimate_mixprop'>Estimate mixture proportions of a mixture g given noisy (error-prone) data from that mixture.</h2><span id='topic+estimate_mixprop'></span>

<h3>Description</h3>

<p>Estimate mixture proportions of a mixture g given noisy (error-prone) data from that mixture.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>estimate_mixprop(
  data,
  g,
  prior,
  optmethod = c("mixSQP", "mixEM", "mixVBEM", "cxxMixSquarem", "mixIP", "w_mixEM"),
  control,
  weights = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="estimate_mixprop_+3A_data">data</code></td>
<td>
<p>list to be passed to log_comp_dens_conv; details depend on model</p>
</td></tr>
<tr><td><code id="estimate_mixprop_+3A_g">g</code></td>
<td>
<p>an object representing a mixture distribution (eg normalmix for mixture of normals;
unimix for mixture of uniforms). The component parameters of g (eg the means and variances) specify the
components whose mixture proportions are to be estimated. The mixture proportions of g are the parameters to be estimated;
the values passed in may be used to initialize the optimization (depending on the optmethod used)</p>
</td></tr>
<tr><td><code id="estimate_mixprop_+3A_prior">prior</code></td>
<td>
<p>numeric vector indicating parameters of &quot;Dirichlet prior&quot;
on mixture proportions</p>
</td></tr>
<tr><td><code id="estimate_mixprop_+3A_optmethod">optmethod</code></td>
<td>
<p>name of function to use to do optimization</p>
</td></tr>
<tr><td><code id="estimate_mixprop_+3A_control">control</code></td>
<td>
<p>list of control parameters to be passed to optmethod,
typically affecting things like convergence tolerance</p>
</td></tr>
<tr><td><code id="estimate_mixprop_+3A_weights">weights</code></td>
<td>
<p>vector of weights (for use with w_mixEM; in beta)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is used by the ash function. Most users won't need to call this directly, but is
exported for use by some other related packages.
</p>


<h3>Value</h3>

<p>list, including the final loglikelihood, the null loglikelihood,
an n by k likelihood matrix with (j,k)th element equal to <code class="reqn">f_k(x_j)</code>,
the fit
and results of optmethod
</p>

<hr>
<h2 id='gen_etruncFUN'>gen_etruncFUN</h2><span id='topic+gen_etruncFUN'></span>

<h3>Description</h3>

<p>Produce function to compute expectation of truncated 
error distribution from log cdf and log pdf (using numerical integration)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gen_etruncFUN(lcdfFUN, lpdfFUN)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gen_etruncFUN_+3A_lcdffun">lcdfFUN</code></td>
<td>
<p>the log cdfFUN of the error distribution</p>
</td></tr>
<tr><td><code id="gen_etruncFUN_+3A_lpdffun">lpdfFUN</code></td>
<td>
<p>the log pdfFUN of the error distribution</p>
</td></tr>
</table>

<hr>
<h2 id='get_density'>Density method for ash object</h2><span id='topic+get_density'></span>

<h3>Description</h3>

<p>Return the density of the underlying fitted distribution
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_density(a, x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_density_+3A_a">a</code></td>
<td>
<p>the fitted ash object</p>
</td></tr>
<tr><td><code id="get_density_+3A_x">x</code></td>
<td>
<p>the vector of locations at which density is to be computed</p>
</td></tr>
</table>


<h3>Details</h3>

<p>None
</p>

<hr>
<h2 id='get_lfsr'>Return lfsr from an ash object</h2><span id='topic+get_lfsr'></span><span id='topic+get_lfdr'></span><span id='topic+get_svalue'></span><span id='topic+get_qvalue'></span><span id='topic+get_pm'></span><span id='topic+get_psd'></span><span id='topic+get_pp'></span><span id='topic+get_np'></span><span id='topic+get_loglik'></span><span id='topic+get_logLR'></span><span id='topic+get_fitted_g'></span><span id='topic+get_pi0'></span>

<h3>Description</h3>

<p>These functions simply return elements of an ash object, generally without doing any calculations.
(So if the value was not computed during the original call to ash, eg because of how outputlevel was set in the call,
then NULL will be returned.)
Accessing elements in this way
rather than directly from the ash object will help ensure compatability moving forward
(e.g. if the internal structure of the ash object changes during software development.)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_lfsr(x)

get_lfdr(a)

get_svalue(a)

get_qvalue(a)

get_pm(a)

get_psd(a)

get_pp(a)

get_np(a)

get_loglik(a)

get_logLR(a)

get_fitted_g(a)

get_pi0(a)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_lfsr_+3A_x">x</code></td>
<td>
<p>an ash fit (e.g. from running ash)</p>
</td></tr>
<tr><td><code id="get_lfsr_+3A_a">a</code></td>
<td>
<p>an ash fit (e.g. from running ash)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a vector (ash) of local false sign rates
</p>


<h3>Functions</h3>


<ul>
<li> <p><code>get_lfsr</code>: local false sign rate
</p>
</li>
<li> <p><code>get_lfdr</code>: local false discovery rate
</p>
</li>
<li> <p><code>get_svalue</code>: svalue
</p>
</li>
<li> <p><code>get_qvalue</code>: qvalue
</p>
</li>
<li> <p><code>get_pm</code>: posterior mean
</p>
</li>
<li> <p><code>get_psd</code>: posterior standard deviation
</p>
</li>
<li> <p><code>get_pp</code>: positive probability
</p>
</li>
<li> <p><code>get_np</code>: negative probability
</p>
</li>
<li> <p><code>get_loglik</code>: log-likelihood
</p>
</li>
<li> <p><code>get_logLR</code>: log-likelihood ratio
</p>
</li>
<li> <p><code>get_fitted_g</code>: fitted g mixture
</p>
</li>
<li> <p><code>get_pi0</code>: pi0, the proportion of nulls
</p>
</li></ul>

<hr>
<h2 id='get_post_sample'>Sample from posterior</h2><span id='topic+get_post_sample'></span>

<h3>Description</h3>

<p>Returns random samples from the posterior distribution for each
observation in an ash object. A matrix is returned, with columns corresponding
to observations and rows corresponding to samples.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_post_sample(a, nsamp)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_post_sample_+3A_a">a</code></td>
<td>
<p>the fitted ash object</p>
</td></tr>
<tr><td><code id="get_post_sample_+3A_nsamp">nsamp</code></td>
<td>
<p>number of samples to return (for each observation)</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>beta = rnorm(100,0,1)
betahat= beta+rnorm(100,0,1)
ash.beta = ash(betahat,1,mixcompdist="normal")
post.beta = get_post_sample(ash.beta,1000)
</code></pre>

<hr>
<h2 id='igmix'>Constructor for igmix class</h2><span id='topic+igmix'></span>

<h3>Description</h3>

<p>Creates an object of class igmix (finite mixture of
univariate inverse-gammas)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>igmix(pi, alpha, beta)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="igmix_+3A_pi">pi</code></td>
<td>
<p>vector of mixture proportions</p>
</td></tr>
<tr><td><code id="igmix_+3A_alpha">alpha</code></td>
<td>
<p>vector of shape parameters</p>
</td></tr>
<tr><td><code id="igmix_+3A_beta">beta</code></td>
<td>
<p>vector of rate parameters</p>
</td></tr>
</table>


<h3>Details</h3>

<p>None
</p>


<h3>Value</h3>

<p>an object of class igmix
</p>


<h3>Examples</h3>

<pre><code class='language-R'>igmix(c(0.5,0.5),c(1,1),c(1,2))

</code></pre>

<hr>
<h2 id='lik_binom'>Likelihood object for Binomial error distribution</h2><span id='topic+lik_binom'></span>

<h3>Description</h3>

<p>Creates a likelihood object for ash for use with Binomial error distribution
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lik_binom(y, n, link = c("identity", "logit"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lik_binom_+3A_y">y</code></td>
<td>
<p>Binomial observations</p>
</td></tr>
<tr><td><code id="lik_binom_+3A_n">n</code></td>
<td>
<p>Binomial number of trials</p>
</td></tr>
<tr><td><code id="lik_binom_+3A_link">link</code></td>
<td>
<p>Link function. The &quot;identity&quot; link directly puts unimodal prior on Binomial success
probabilities p, and &quot;logit&quot; link puts unimodal prior on logit(p).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Suppose we have Binomial observations <code>y</code> where <code class="reqn">y_i\sim Bin(n_i,p_i)</code>. 
We either put an unimodal prior g on the success probabilities <code class="reqn">p_i\sim g</code> (by specifying 
<code>link="identity"</code>) or on the logit success probabilities <code class="reqn">logit(p_i)\sim g</code> 
(by specifying <code>link="logit"</code>). Either way, ASH with this Binomial likelihood function 
will compute the posterior mean of the success probabilities <code class="reqn">p_i</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>   p = rbeta(100,2,2) # prior mode: 0.5
   n = rpois(100,10)
   y = rbinom(100,n,p) # simulate Binomial observations
   ash(rep(0,length(y)),1,lik=lik_binom(y,n))
</code></pre>

<hr>
<h2 id='lik_logF'>Likelihood object for logF error distribution</h2><span id='topic+lik_logF'></span>

<h3>Description</h3>

<p>Creates a likelihood object for ash for use with logF error distribution
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lik_logF(df1, df2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lik_logF_+3A_df1">df1</code></td>
<td>
<p>first degree of freedom parameter of F distribution</p>
</td></tr>
<tr><td><code id="lik_logF_+3A_df2">df2</code></td>
<td>
<p>second degree of freedom parameter of F distribution</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>   e = rnorm(100) + log(rf(100,df1=10,df2=10)) # simulate some data with log(F) error
   ash(e,1,lik=lik_logF(df1=10,df2=10))
</code></pre>

<hr>
<h2 id='lik_normal'>Likelihood object for normal error distribution</h2><span id='topic+lik_normal'></span>

<h3>Description</h3>

<p>Creates a likelihood object for ash for use with normal error distribution
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lik_normal()
</code></pre>


<h3>Examples</h3>

<pre><code class='language-R'>   z = rnorm(100) + rnorm(100) # simulate some data with normal error
   ash(z,1,lik=lik_normal())
</code></pre>

<hr>
<h2 id='lik_normalmix'>Likelihood object for normal mixture error distribution</h2><span id='topic+lik_normalmix'></span>

<h3>Description</h3>

<p>Creates a likelihood object for ash for use with normal mixture error distribution
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lik_normalmix(pilik, sdlik)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lik_normalmix_+3A_pilik">pilik</code></td>
<td>
<p>a k vector of mixture proportions (k is the number of mixture components), 
or an n*k matrix that the j'th row the is mixture proportions for betahat_j</p>
</td></tr>
<tr><td><code id="lik_normalmix_+3A_sdlik">sdlik</code></td>
<td>
<p>a k vector of component-wise standard deviations, 
or an n*k matrix that the j'th row the is component-wise standard deviations for betahat_j</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>   e = rnorm(100,0,0.8) 
   e[seq(1,100,by=2)] = rnorm(50,0,1.5) # generate e~0.5*N(0,0.8^2)+0.5*N(0,1.5^2)
   betahat = rnorm(100)+e
   ash(betahat, 1, lik=lik_normalmix(c(0.5,0.5),c(0.8,1.5)))
</code></pre>

<hr>
<h2 id='lik_pois'>Likelihood object for Poisson error distribution</h2><span id='topic+lik_pois'></span>

<h3>Description</h3>

<p>Creates a likelihood object for ash for use with Poisson error distribution
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lik_pois(y, scale = 1, link = c("identity", "log"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lik_pois_+3A_y">y</code></td>
<td>
<p>Poisson observations.</p>
</td></tr>
<tr><td><code id="lik_pois_+3A_scale">scale</code></td>
<td>
<p>Scale factor for Poisson observations: y~Pois(scale*lambda).</p>
</td></tr>
<tr><td><code id="lik_pois_+3A_link">link</code></td>
<td>
<p>Link function. The &quot;identity&quot; link directly puts unimodal prior on Poisson
intensities lambda, and &quot;log&quot; link puts unimodal prior on log(lambda).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Suppose we have Poisson observations <code>y</code> where <code class="reqn">y_i\sim Poisson(c_i\lambda_i)</code>. 
We either put an unimodal prior g on the (scaled) intensities <code class="reqn">\lambda_i\sim g</code> 
(by specifying <code>link="identity"</code>) or on the log intensities 
<code class="reqn">log(\lambda_i)\sim g</code> (by specifying <code>link="log"</code>). Either way, 
ASH with this Poisson likelihood function will compute the posterior mean of the 
intensities <code class="reqn">\lambda_i</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>   beta = c(rnorm(100,50,5)) # prior mode: 50
   y = rpois(100,beta) # simulate Poisson observations
   ash(rep(0,length(y)),1,lik=lik_pois(y))

</code></pre>

<hr>
<h2 id='lik_t'>Likelihood object for t error distribution</h2><span id='topic+lik_t'></span>

<h3>Description</h3>

<p>Creates a likelihood object for ash for use with t error distribution
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lik_t(df)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lik_t_+3A_df">df</code></td>
<td>
<p>degree of freedom parameter of t distribution</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>   z = rnorm(100) + rt(100,df=4) # simulate some data with t error
   ash(z,1,lik=lik_t(df=4))
</code></pre>

<hr>
<h2 id='log_comp_dens_conv'>log_comp_dens_conv</h2><span id='topic+log_comp_dens_conv'></span>

<h3>Description</h3>

<p>compute the log density of the components of the
mixture m when convoluted with a normal with standard deviation
s or a scaled (se) student.t with df v, the density is
evaluated at x
</p>


<h3>Usage</h3>

<pre><code class='language-R'>log_comp_dens_conv(m, data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="log_comp_dens_conv_+3A_m">m</code></td>
<td>
<p>mixture distribution with k components</p>
</td></tr>
<tr><td><code id="log_comp_dens_conv_+3A_data">data</code></td>
<td>
<p>details depend on the model</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a k by n matrix of log densities
</p>

<hr>
<h2 id='log_comp_dens_conv.normalmix'>log_comp_dens_conv.normalmix</h2><span id='topic+log_comp_dens_conv.normalmix'></span>

<h3>Description</h3>

<p>returns log-density of convolution of each component
of a normal mixture with N(0,s^2) or s*t(v) at x. Note that
convolution of two normals is normal, so it works that way
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'normalmix'
log_comp_dens_conv(m, data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="log_comp_dens_conv.normalmix_+3A_m">m</code></td>
<td>
<p>mixture distribution with k components</p>
</td></tr>
<tr><td><code id="log_comp_dens_conv.normalmix_+3A_data">data</code></td>
<td>
<p>a list with components x and s to be interpreted as a normally-distributed observation and its standard error</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a k by n matrix
</p>

<hr>
<h2 id='log_comp_dens_conv.unimix'>log density of convolution of each component of a unif mixture</h2><span id='topic+log_comp_dens_conv.unimix'></span>

<h3>Description</h3>

<p>log density of convolution of each component of a unif mixture
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'unimix'
log_comp_dens_conv(m, data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="log_comp_dens_conv.unimix_+3A_m">m</code></td>
<td>
<p>a mixture of class unimix</p>
</td></tr>
<tr><td><code id="log_comp_dens_conv.unimix_+3A_data">data</code></td>
<td>
<p>see set_data()</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a k by n matrix of densities
</p>

<hr>
<h2 id='loglik_conv'>loglik_conv</h2><span id='topic+loglik_conv'></span>

<h3>Description</h3>

<p>find log likelihood of data using convolution of mixture with error distribution
</p>


<h3>Usage</h3>

<pre><code class='language-R'>loglik_conv(m, data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="loglik_conv_+3A_m">m</code></td>
<td>
<p>mixture distribution with k components</p>
</td></tr>
<tr><td><code id="loglik_conv_+3A_data">data</code></td>
<td>
<p>details depend on the model</p>
</td></tr>
</table>

<hr>
<h2 id='loglik_conv.default'>loglik_conv.default</h2><span id='topic+loglik_conv.default'></span>

<h3>Description</h3>

<p>The default version of <code><a href="#topic+loglik_conv">loglik_conv</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## Default S3 method:
loglik_conv(m, data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="loglik_conv.default_+3A_m">m</code></td>
<td>
<p>mixture distribution with k components</p>
</td></tr>
<tr><td><code id="loglik_conv.default_+3A_data">data</code></td>
<td>
<p>data whose details depend on model</p>
</td></tr>
</table>

<hr>
<h2 id='mixcdf'>mixcdf</h2><span id='topic+mixcdf'></span>

<h3>Description</h3>

<p>Returns cdf for a mixture (generic function)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mixcdf(x, y, lower.tail = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mixcdf_+3A_x">x</code></td>
<td>
<p>a mixture (eg of type normalmix or unimix)</p>
</td></tr>
<tr><td><code id="mixcdf_+3A_y">y</code></td>
<td>
<p>locations at which cdf to be computed</p>
</td></tr>
<tr><td><code id="mixcdf_+3A_lower.tail">lower.tail</code></td>
<td>
<p>boolean indicating whether to report lower tail</p>
</td></tr>
</table>


<h3>Details</h3>

<p>None
</p>


<h3>Value</h3>

<p>an object of class normalmix
</p>


<h3>Examples</h3>

<pre><code class='language-R'>mixcdf(normalmix(c(0.5,0.5),c(0,0),c(1,2)),seq(-4,4,length=100))

</code></pre>

<hr>
<h2 id='mixcdf.default'>mixcdf.default</h2><span id='topic+mixcdf.default'></span>

<h3>Description</h3>

<p>The default version of <code><a href="#topic+mixcdf">mixcdf</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## Default S3 method:
mixcdf(x, y, lower.tail = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mixcdf.default_+3A_x">x</code></td>
<td>
<p>a mixture (eg of type normalmix or unimix)</p>
</td></tr>
<tr><td><code id="mixcdf.default_+3A_y">y</code></td>
<td>
<p>locations at which cdf to be computed</p>
</td></tr>
<tr><td><code id="mixcdf.default_+3A_lower.tail">lower.tail</code></td>
<td>
<p>boolean indicating whether to report lower tail</p>
</td></tr>
</table>

<hr>
<h2 id='mixEM'>Estimate mixture proportions of a mixture model by EM algorithm</h2><span id='topic+mixEM'></span>

<h3>Description</h3>

<p>Given the individual component likelihoods for a mixture model, estimates the mixture proportions by an EM algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mixEM(matrix_lik, prior, pi_init = NULL, control = list())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mixEM_+3A_matrix_lik">matrix_lik</code></td>
<td>
<p>a n by k matrix with (j,k)th element equal to <code class="reqn">f_k(x_j)</code>.</p>
</td></tr>
<tr><td><code id="mixEM_+3A_prior">prior</code></td>
<td>
<p>a k vector of the parameters of the Dirichlet prior on <code class="reqn">\pi</code>. Recommended to be rep(1,k)</p>
</td></tr>
<tr><td><code id="mixEM_+3A_pi_init">pi_init</code></td>
<td>
<p>the initial value of <code class="reqn">\pi</code> to use. If not specified defaults to (1/k,...,1/k).</p>
</td></tr>
<tr><td><code id="mixEM_+3A_control">control</code></td>
<td>
<p>A list of control parameters for the SQUAREM algorithm, default value is set to be control.default=list(K = 1, method=3, square=TRUE, step.min0=1, step.max0=1, mstep=4, kr=1, objfn.inc=1,tol=1.e-07, maxiter=5000, trace=FALSE).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Fits a k component mixture model </p>
<p style="text-align: center;"><code class="reqn">f(x|\pi)= \sum_k \pi_k f_k(x)</code>
</p>
<p> to independent
and identically distributed data <code class="reqn">x_1,\dots,x_n</code>. 
Estimates mixture proportions <code class="reqn">\pi</code> by maximum likelihood, or by maximum a posteriori (MAP) estimation for a Dirichlet prior on <code class="reqn">\pi</code> 
(if a prior is specified).  Uses the SQUAREM package to accelerate convergence of EM. Used by the ash main function; there is no need for a user to call this 
function separately, but it is exported for convenience.
</p>


<h3>Value</h3>

<p>A list, including the estimates (pihat), the log likelihood for each interation (B)
and a flag to indicate convergence
</p>

<hr>
<h2 id='mixIP'>Estimate mixture proportions of a mixture model by Interior Point method</h2><span id='topic+mixIP'></span>

<h3>Description</h3>

<p>Given the individual component likelihoods for a mixture model, estimates the mixture proportions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mixIP(matrix_lik, prior, pi_init = NULL, control = list(), weights = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mixIP_+3A_matrix_lik">matrix_lik</code></td>
<td>
<p>a n by k matrix with (j,k)th element equal to <code class="reqn">f_k(x_j)</code>.</p>
</td></tr>
<tr><td><code id="mixIP_+3A_prior">prior</code></td>
<td>
<p>a k vector of the parameters of the Dirichlet prior on <code class="reqn">\pi</code>. Recommended to be rep(1,k)</p>
</td></tr>
<tr><td><code id="mixIP_+3A_pi_init">pi_init</code></td>
<td>
<p>the initial value of <code class="reqn">\pi</code> to use. If not specified defaults to (1/k,...,1/k).</p>
</td></tr>
<tr><td><code id="mixIP_+3A_control">control</code></td>
<td>
<p>A list of control parameters to be passed to REBayes::KWDual</p>
</td></tr>
<tr><td><code id="mixIP_+3A_weights">weights</code></td>
<td>
<p>weights to be assigned to the observations (an n vector)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Optimizes </p>
<p style="text-align: center;"><code class="reqn">L(pi)= sum_j w_j log(sum_k pi_k f_{jk}) + h(pi)</code>
</p>
 
<p>subject to pi_k non-negative and sum_k pi_k = 1. Here </p>
<p style="text-align: center;"><code class="reqn">h(pi)</code>
</p>
<p> is
a penalty function h(pi) = sum_k (prior_k-1) log pi_k.
Calls REBayes::KWDual in the REBayes package, which is in turn a wrapper to the mosek 
convex optimization software. So REBayes must be installed to use this. 
Used by the ash main function; there is no need for a user to call this 
function separately, but it is exported for convenience.
</p>


<h3>Value</h3>

<p>A list, including the estimates (pihat), the log likelihood for each interation (B)
and a flag to indicate convergence
</p>

<hr>
<h2 id='mixmean2'>Generic function of calculating the overall second moment of the
mixture</h2><span id='topic+mixmean2'></span>

<h3>Description</h3>

<p>Generic function of calculating the overall second moment of the
mixture
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mixmean2(m)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mixmean2_+3A_m">m</code></td>
<td>
<p>a mixture of k components generated by normalmix() or
unimix() or igmix()</p>
</td></tr>
</table>


<h3>Value</h3>

<p>it returns scalar
</p>

<hr>
<h2 id='mixprop'>Generic function of extracting the mixture proportions</h2><span id='topic+mixprop'></span>

<h3>Description</h3>

<p>Generic function of extracting the mixture proportions
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mixprop(m)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mixprop_+3A_m">m</code></td>
<td>
<p>a mixture of k components generated by normalmix() or
unimix() or igmix()</p>
</td></tr>
</table>


<h3>Value</h3>

<p>it returns a vector of component probabilities, summing up
to 1.
</p>

<hr>
<h2 id='mixSQP'>Estimate mixture proportions of a mixture model using
mix-SQP algorithm.</h2><span id='topic+mixSQP'></span>

<h3>Description</h3>

<p>Estimate mixture proportions of a mixture model using
mix-SQP algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mixSQP(matrix_lik, prior, pi_init = NULL, control = list(), weights = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mixSQP_+3A_matrix_lik">matrix_lik</code></td>
<td>
<p>A matrix containing the conditional likelihood
values, possibly normalized.</p>
</td></tr>
<tr><td><code id="mixSQP_+3A_prior">prior</code></td>
<td>
<p>A vector of the parameters of the Dirichlet prior on
the mixture weights.</p>
</td></tr>
<tr><td><code id="mixSQP_+3A_pi_init">pi_init</code></td>
<td>
<p>The initial estimate of the mixture weights.</p>
</td></tr>
<tr><td><code id="mixSQP_+3A_control">control</code></td>
<td>
<p>A list of settings for the mix-SQP optimization
algorithm; see <code><a href="mixsqp.html#topic+mixsqp">mixsqp</a></code> for details.</p>
</td></tr>
<tr><td><code id="mixSQP_+3A_weights">weights</code></td>
<td>
<p>The weights to be assigned to the observations. Must
be a vector of length equal the number of rows of <code>matrix_lik</code>.
If <code>weights = NULL</code>, all observations are assigned the same
weight.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list object including the estimates (<code>pihat</code>) and a
flag (<code>control</code>) indicating convergence success or failure.
</p>

<hr>
<h2 id='mixVBEM'>Estimate posterior distribution on mixture proportions of a mixture model by a Variational Bayes EM algorithm</h2><span id='topic+mixVBEM'></span>

<h3>Description</h3>

<p>Given the individual component likelihoods for a mixture model, estimates the posterior on 
the mixture proportions by an VBEM algorithm. Used by the ash main function; there is no need for a user to call this 
function separately, but it is exported for convenience.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mixVBEM(matrix_lik, prior, pi_init = NULL, control = list())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mixVBEM_+3A_matrix_lik">matrix_lik</code></td>
<td>
<p>a n by k matrix with (j,k)th element equal to <code class="reqn">f_k(x_j)</code>.</p>
</td></tr>
<tr><td><code id="mixVBEM_+3A_prior">prior</code></td>
<td>
<p>a k vector of the parameters of the Dirichlet prior on <code class="reqn">\pi</code>. Recommended to be rep(1,k)</p>
</td></tr>
<tr><td><code id="mixVBEM_+3A_pi_init">pi_init</code></td>
<td>
<p>the initial value of the posterior parameters. If not specified defaults to the prior parameters.</p>
</td></tr>
<tr><td><code id="mixVBEM_+3A_control">control</code></td>
<td>
<p>A list of control parameters for the SQUAREM algorithm, default value is set to be   control.default=list(K = 1, method=3, square=TRUE, step.min0=1, step.max0=1, mstep=4, kr=1, objfn.inc=1,tol=1.e-07, maxiter=5000, trace=FALSE).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Fits a k component mixture model </p>
<p style="text-align: center;"><code class="reqn">f(x|\pi) = \sum_k \pi_k f_k(x)</code>
</p>
<p> to independent
and identically distributed data <code class="reqn">x_1,\dots,x_n</code>. 
Estimates posterior on mixture proportions <code class="reqn">\pi</code> by Variational Bayes, 
with a Dirichlet prior on <code class="reqn">\pi</code>. 
Algorithm adapted from Bishop (2009), Pattern Recognition and Machine Learning, Chapter 10.
</p>


<h3>Value</h3>

<p>A list, whose components include point estimates (pihat), 
the parameters of the fitted posterior on <code class="reqn">\pi</code> (pipost),
the bound on the log likelihood for each iteration (B)
and a flag to indicate convergence (converged).
</p>

<hr>
<h2 id='my_e2truncbeta'>second moment of truncated Beta distribution</h2><span id='topic+my_e2truncbeta'></span>

<h3>Description</h3>

<p>Compute second moment of the truncated Beta.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>my_e2truncbeta(a, b, alpha, beta)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="my_e2truncbeta_+3A_a">a</code></td>
<td>
<p>left limit of distribution</p>
</td></tr>
<tr><td><code id="my_e2truncbeta_+3A_b">b</code></td>
<td>
<p>right limit of distribution</p>
</td></tr>
<tr><td><code id="my_e2truncbeta_+3A_alpha">alpha</code>, <code id="my_e2truncbeta_+3A_beta">beta</code></td>
<td>
<p>shape parameters of Beta distribution</p>
</td></tr>
</table>

<hr>
<h2 id='my_e2truncgamma'>second moment of truncated gamma distribution</h2><span id='topic+my_e2truncgamma'></span>

<h3>Description</h3>

<p>Compute second moment of the truncated gamma.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>my_e2truncgamma(a, b, shape, rate)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="my_e2truncgamma_+3A_a">a</code></td>
<td>
<p>left limit of distribution</p>
</td></tr>
<tr><td><code id="my_e2truncgamma_+3A_b">b</code></td>
<td>
<p>right limit of distribution</p>
</td></tr>
<tr><td><code id="my_e2truncgamma_+3A_shape">shape</code></td>
<td>
<p>shape of gamma distribution</p>
</td></tr>
<tr><td><code id="my_e2truncgamma_+3A_rate">rate</code></td>
<td>
<p>rate of gamma distribution</p>
</td></tr>
</table>

<hr>
<h2 id='my_e2truncnorm'>Expected Squared Value of Truncated Normal</h2><span id='topic+my_e2truncnorm'></span>

<h3>Description</h3>

<p>Computes the expected squared values of truncated normal 
distributions with parameters <code>a</code>, <code>b</code>, <code>mean</code>, and 
<code>sd</code>. Arguments can be scalars, vectors, or matrices. Arguments of 
shorter length will be recycled according to the usual recycling rules, 
but <code>a</code> and <code>b</code> must have the same length. Missing values are 
accepted for all arguments.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>my_e2truncnorm(a, b, mean = 0, sd = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="my_e2truncnorm_+3A_a">a</code></td>
<td>
<p>The lower limit for the support of the truncated normal. Can be
<code>-Inf</code>.</p>
</td></tr>
<tr><td><code id="my_e2truncnorm_+3A_b">b</code></td>
<td>
<p>The upper limit for the support. Can be <code>Inf</code>. <code>a</code> and 
<code>b</code> must have the same length, and each element of <code>a</code> should 
be less than or equal to the corresponding element of <code>b</code>.</p>
</td></tr>
<tr><td><code id="my_e2truncnorm_+3A_mean">mean</code></td>
<td>
<p>The mean of the untruncated normal.</p>
</td></tr>
<tr><td><code id="my_e2truncnorm_+3A_sd">sd</code></td>
<td>
<p>The standard deviation of the untruncated normal. Standard
deviations of zero are interpreted as numerically (rather than exactly)
zero, so that the square of the untruncated mean is returned if it lies 
within <code>[a, b]</code> and the square of the nearer of <code>a</code> and 
<code>b</code> is returned otherwise.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The expected squared values of truncated normal
distributions with parameters <code>a</code>, <code>b</code>, <code>mean</code>, and
<code>sd</code>. If any of the arguments is a matrix, then a matrix will
be returned.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+my_etruncnorm">my_etruncnorm</a></code>, <code><a href="#topic+my_vtruncnorm">my_vtruncnorm</a></code>
</p>

<hr>
<h2 id='my_e2trunct'>my_e2trunct</h2><span id='topic+my_e2trunct'></span>

<h3>Description</h3>

<p>Compute second moment of the truncated t. Uses results from O'Hagan, Biometrika, 1973
</p>


<h3>Usage</h3>

<pre><code class='language-R'>my_e2trunct(a, b, df)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="my_e2trunct_+3A_a">a</code></td>
<td>
<p>left limit of distribution</p>
</td></tr>
<tr><td><code id="my_e2trunct_+3A_b">b</code></td>
<td>
<p>right limit of distribution</p>
</td></tr>
<tr><td><code id="my_e2trunct_+3A_df">df</code></td>
<td>
<p>degree of freedom of error distribution</p>
</td></tr>
</table>

<hr>
<h2 id='my_etruncbeta'>mean of truncated Beta distribution</h2><span id='topic+my_etruncbeta'></span>

<h3>Description</h3>

<p>Compute mean of the truncated Beta.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>my_etruncbeta(a, b, alpha, beta)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="my_etruncbeta_+3A_a">a</code></td>
<td>
<p>left limit of distribution</p>
</td></tr>
<tr><td><code id="my_etruncbeta_+3A_b">b</code></td>
<td>
<p>right limit of distribution</p>
</td></tr>
<tr><td><code id="my_etruncbeta_+3A_alpha">alpha</code>, <code id="my_etruncbeta_+3A_beta">beta</code></td>
<td>
<p>shape parameters of Beta distribution</p>
</td></tr>
</table>

<hr>
<h2 id='my_etruncgamma'>mean of truncated gamma distribution</h2><span id='topic+my_etruncgamma'></span>

<h3>Description</h3>

<p>Compute mean of the truncated gamma.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>my_etruncgamma(a, b, shape, rate)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="my_etruncgamma_+3A_a">a</code></td>
<td>
<p>left limit of distribution</p>
</td></tr>
<tr><td><code id="my_etruncgamma_+3A_b">b</code></td>
<td>
<p>right limit of distribution</p>
</td></tr>
<tr><td><code id="my_etruncgamma_+3A_shape">shape</code></td>
<td>
<p>shape of gamma distribution</p>
</td></tr>
<tr><td><code id="my_etruncgamma_+3A_rate">rate</code></td>
<td>
<p>rate of gamma distribution</p>
</td></tr>
</table>

<hr>
<h2 id='my_etrunclogf'>my_etrunclogf</h2><span id='topic+my_etrunclogf'></span>

<h3>Description</h3>

<p>Compute expectation of truncated log-F distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>my_etrunclogf(a, b, df1, df2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="my_etrunclogf_+3A_a">a</code></td>
<td>
<p>Left limit of distribution.</p>
</td></tr>
<tr><td><code id="my_etrunclogf_+3A_b">b</code></td>
<td>
<p>Right limit of distribution.</p>
</td></tr>
<tr><td><code id="my_etrunclogf_+3A_df1">df1</code>, <code id="my_etrunclogf_+3A_df2">df2</code></td>
<td>
<p>degrees of freedom</p>
</td></tr>
</table>

<hr>
<h2 id='my_etruncnorm'>Expected Value of Truncated Normal</h2><span id='topic+my_etruncnorm'></span>

<h3>Description</h3>

<p>Computes the means of truncated normal distributions with
parameters <code>a</code>, <code>b</code>, <code>mean</code>, and <code>sd</code>. Arguments
can be scalars, vectors, or matrices. Arguments of shorter length will
be recycled according to the usual recycling rules, but <code>a</code> and 
<code>b</code> must have the same length. Missing values are accepted for all 
arguments.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>my_etruncnorm(a, b, mean = 0, sd = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="my_etruncnorm_+3A_a">a</code></td>
<td>
<p>The lower limit for the support of the truncated normal. Can be
<code>-Inf</code>.</p>
</td></tr>
<tr><td><code id="my_etruncnorm_+3A_b">b</code></td>
<td>
<p>The upper limit for the support. Can be <code>Inf</code>. <code>a</code> and 
<code>b</code> must have the same length, and each element of <code>a</code> should 
be less than or equal to the corresponding element of <code>b</code>.</p>
</td></tr>
<tr><td><code id="my_etruncnorm_+3A_mean">mean</code></td>
<td>
<p>The mean of the untruncated normal.</p>
</td></tr>
<tr><td><code id="my_etruncnorm_+3A_sd">sd</code></td>
<td>
<p>The standard deviation of the untruncated normal. Standard
deviations of zero are interpreted as numerically (rather than exactly)
zero, so that the untruncated mean is returned if it lies within 
<code>[a, b]</code> and the nearer of <code>a</code> and <code>b</code> is returned
otherwise.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The expected values of truncated normal distributions with
parameters <code>a</code>, <code>b</code>, <code>mean</code>, and <code>sd</code>. If any of the
arguments is a matrix, then a matrix will be returned.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+my_e2truncnorm">my_e2truncnorm</a></code>, <code><a href="#topic+my_vtruncnorm">my_vtruncnorm</a></code>
</p>

<hr>
<h2 id='my_etrunct'>my_etrunct</h2><span id='topic+my_etrunct'></span>

<h3>Description</h3>

<p>Compute second moment of the truncated t. Uses results from O'Hagan, Biometrika, 1973
</p>


<h3>Usage</h3>

<pre><code class='language-R'>my_etrunct(a, b, df)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="my_etrunct_+3A_a">a</code></td>
<td>
<p>left limit of distribution</p>
</td></tr>
<tr><td><code id="my_etrunct_+3A_b">b</code></td>
<td>
<p>right limit of distribution</p>
</td></tr>
<tr><td><code id="my_etrunct_+3A_df">df</code></td>
<td>
<p>degree of freedom of error distribution</p>
</td></tr>
</table>

<hr>
<h2 id='my_vtruncnorm'>Variance of Truncated Normal</h2><span id='topic+my_vtruncnorm'></span>

<h3>Description</h3>

<p>Computes the variance of truncated normal distributions with
parameters <code>a</code>, <code>b</code>, <code>mean</code>, and <code>sd</code>. Arguments can 
be scalars, vectors, or matrices. Arguments of shorter length will be 
recycled according to the usual recycling rules, but <code>a</code> and <code>b</code>
must have the same length. Missing values are accepted for all arguments.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>my_vtruncnorm(a, b, mean = 0, sd = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="my_vtruncnorm_+3A_a">a</code></td>
<td>
<p>The lower limit for the support of the truncated normal. Can be
<code>-Inf</code>.</p>
</td></tr>
<tr><td><code id="my_vtruncnorm_+3A_b">b</code></td>
<td>
<p>The upper limit for the support. Can be <code>Inf</code>. <code>a</code> and 
<code>b</code> must have the same length, and each element of <code>a</code> should 
be less than or equal to the corresponding element of <code>b</code>.</p>
</td></tr>
<tr><td><code id="my_vtruncnorm_+3A_mean">mean</code></td>
<td>
<p>The mean of the untruncated normal.</p>
</td></tr>
<tr><td><code id="my_vtruncnorm_+3A_sd">sd</code></td>
<td>
<p>The standard deviation of the untruncated normal.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The variance of truncated normal distributions with parameters 
<code>a</code>, <code>b</code>, <code>mean</code>, and <code>sd</code>. If any of the arguments 
is a matrix, then a matrix will be returned.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+my_etruncnorm">my_etruncnorm</a></code>, <code><a href="#topic+my_e2truncnorm">my_e2truncnorm</a></code>
</p>

<hr>
<h2 id='ncomp'>ncomp</h2><span id='topic+ncomp'></span>

<h3>Description</h3>

<p>ncomp
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ncomp(m)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ncomp_+3A_m">m</code></td>
<td>
<p>a mixture of k components generated by normalmix() or
unimix() or igmix()</p>
</td></tr>
</table>

<hr>
<h2 id='ncomp.default'>ncomp.default</h2><span id='topic+ncomp.default'></span>

<h3>Description</h3>

<p>The default version of <code><a href="#topic+ncomp">ncomp</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## Default S3 method:
ncomp(m)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ncomp.default_+3A_m">m</code></td>
<td>
<p>a mixture of k components generated by normalmix() or
unimix() or igmix()</p>
</td></tr>
</table>

<hr>
<h2 id='normalmix'>Constructor for normalmix class</h2><span id='topic+normalmix'></span>

<h3>Description</h3>

<p>Creates an object of class normalmix (finite mixture
of univariate normals)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>normalmix(pi, mean, sd)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="normalmix_+3A_pi">pi</code></td>
<td>
<p>vector of mixture proportions</p>
</td></tr>
<tr><td><code id="normalmix_+3A_mean">mean</code></td>
<td>
<p>vector of means</p>
</td></tr>
<tr><td><code id="normalmix_+3A_sd">sd</code></td>
<td>
<p>vector of standard deviations</p>
</td></tr>
</table>


<h3>Details</h3>

<p>None
</p>


<h3>Value</h3>

<p>an object of class normalmix
</p>


<h3>Examples</h3>

<pre><code class='language-R'>normalmix(c(0.5,0.5),c(0,0),c(1,2))

</code></pre>

<hr>
<h2 id='pcdf_post'>pcdf_post</h2><span id='topic+pcdf_post'></span>

<h3>Description</h3>

<p>&ldquo;parallel&quot; vector version of <code><a href="#topic+cdf_post">cdf_post</a></code> where c is a vector, of same length as betahat and sebetahat
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pcdf_post(m, c, data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pcdf_post_+3A_m">m</code></td>
<td>
<p>mixture distribution with k components</p>
</td></tr>
<tr><td><code id="pcdf_post_+3A_c">c</code></td>
<td>
<p>a numeric vector with n elements</p>
</td></tr>
<tr><td><code id="pcdf_post_+3A_data">data</code></td>
<td>
<p>depends on context</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an n vector, whose ith element is the cdf for beta_i at c_i
</p>


<h3>Examples</h3>

<pre><code class='language-R'>beta = rnorm(100,0,1)
betahat= beta+rnorm(100,0,1)
sebetahat=rep(1,100)
ash.beta = ash(betahat,1,mixcompdist="normal")
c = pcdf_post(get_fitted_g(ash.beta),beta,set_data(betahat,sebetahat))
</code></pre>

<hr>
<h2 id='plogf'>The log-F distribution</h2><span id='topic+plogf'></span>

<h3>Description</h3>

<p>Distribution function for the log-F distribution with <code>df1</code> and <code>df2</code>
degrees of freedom (and optional non-centrality parameter <code>ncp</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plogf(q, df1, df2, ncp, lower.tail = TRUE, log.p = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plogf_+3A_q">q</code></td>
<td>
<p>vector of quantiles</p>
</td></tr>
<tr><td><code id="plogf_+3A_df1">df1</code>, <code id="plogf_+3A_df2">df2</code></td>
<td>
<p>degrees of freedom</p>
</td></tr>
<tr><td><code id="plogf_+3A_ncp">ncp</code></td>
<td>
<p>non-centrality parameter. If omitted the central F is assumed.</p>
</td></tr>
<tr><td><code id="plogf_+3A_lower.tail">lower.tail</code></td>
<td>
<p>logical; if TRUE (default), probabilities are P[X &lt;= x], otherwise, P[X &gt; x].</p>
</td></tr>
<tr><td><code id="plogf_+3A_log.p">log.p</code></td>
<td>
<p>logical; if TRUE, probabilities p are given as log(p).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The distribution function.
</p>

<hr>
<h2 id='plot_diagnostic'>Diagnostic plots for ash object</h2><span id='topic+plot_diagnostic'></span>

<h3>Description</h3>

<p>Generate several plots to diagnose the fitness of ASH on the data
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_diagnostic(
  x,
  plot.it = TRUE,
  sebetahat.tol = 0.001,
  plot.hist,
  xmin,
  xmax,
  breaks = "Sturges",
  alpha = 0.01,
  pch = 19,
  cex = 0.25
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_diagnostic_+3A_x">x</code></td>
<td>
<p>the fitted ash object</p>
</td></tr>
<tr><td><code id="plot_diagnostic_+3A_plot.it">plot.it</code></td>
<td>
<p>logical. whether to plot the diagnostic result</p>
</td></tr>
<tr><td><code id="plot_diagnostic_+3A_sebetahat.tol">sebetahat.tol</code></td>
<td>
<p>tolerance to test the equality of betahat</p>
</td></tr>
<tr><td><code id="plot_diagnostic_+3A_plot.hist">plot.hist</code></td>
<td>
<p>logical. whether to plot the histogram of betahat when sebetahat is not constant</p>
</td></tr>
<tr><td><code id="plot_diagnostic_+3A_xmin">xmin</code>, <code id="plot_diagnostic_+3A_xmax">xmax</code></td>
<td>
<p>range of the histogram of betahat to be plotted</p>
</td></tr>
<tr><td><code id="plot_diagnostic_+3A_breaks">breaks</code></td>
<td>
<p>histograms parameter (see <code><a href="graphics.html#topic+hist">hist</a></code>)</p>
</td></tr>
<tr><td><code id="plot_diagnostic_+3A_alpha">alpha</code></td>
<td>
<p>error level for the de-trended diagnostic plot</p>
</td></tr>
<tr><td><code id="plot_diagnostic_+3A_pch">pch</code>, <code id="plot_diagnostic_+3A_cex">cex</code></td>
<td>
<p>plot parameters for dots</p>
</td></tr>
</table>


<h3>Details</h3>

<p>None.
</p>

<hr>
<h2 id='plot.ash'>Plot method for ash object</h2><span id='topic+plot.ash'></span>

<h3>Description</h3>

<p>Plot the cdf of the underlying fitted distribution
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ash'
plot(x, ..., xmin, xmax)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.ash_+3A_x">x</code></td>
<td>
<p>the fitted ash object</p>
</td></tr>
<tr><td><code id="plot.ash_+3A_...">...</code></td>
<td>
<p>Arguments to be passed to methods,such as graphical parameters (see <code><a href="graphics.html#topic+plot">plot</a></code>)</p>
</td></tr>
<tr><td><code id="plot.ash_+3A_xmin">xmin</code></td>
<td>
<p>xlim lower range, default is the lowest value of betahat</p>
</td></tr>
<tr><td><code id="plot.ash_+3A_xmax">xmax</code></td>
<td>
<p>xlim upper range, default is the highest value of betahat</p>
</td></tr>
</table>


<h3>Details</h3>

<p>None
</p>

<hr>
<h2 id='pm_on_zero'>Generic function to extract which components of mixture are point mass on 0</h2><span id='topic+pm_on_zero'></span>

<h3>Description</h3>

<p>Generic function to extract which components of mixture are point mass on 0
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pm_on_zero(m)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pm_on_zero_+3A_m">m</code></td>
<td>
<p>a mixture of k components generated by normalmix() or unimix() or igmix()</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a boolean vector indicating which components are point mass on 0
</p>

<hr>
<h2 id='post_sample'>post_sample</h2><span id='topic+post_sample'></span>

<h3>Description</h3>

<p>returns random samples from the posterior, given a prior distribution
m and n observed datapoints.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>post_sample(m, data, nsamp)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="post_sample_+3A_m">m</code></td>
<td>
<p>prior distribution (eg of type normalmix)</p>
</td></tr>
<tr><td><code id="post_sample_+3A_data">data</code></td>
<td>
<p>a list with components x and s, each vectors of length n, to be interpreted as a 
normally-distributed observations and corresponding standard errors</p>
</td></tr>
<tr><td><code id="post_sample_+3A_nsamp">nsamp</code></td>
<td>
<p>number of random samples to return for each observation</p>
</td></tr>
</table>


<h3>Details</h3>

<p>exported, but mostly users will want to use 'get_post_sample'
</p>


<h3>Value</h3>

<p>an nsamp by n matrix
</p>

<hr>
<h2 id='post_sample.normalmix'>post_sample.normalmix</h2><span id='topic+post_sample.normalmix'></span>

<h3>Description</h3>

<p>returns random samples from the posterior, given a
prior distribution m and n observed datapoints.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'normalmix'
post_sample(m, data, nsamp)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="post_sample.normalmix_+3A_m">m</code></td>
<td>
<p>mixture distribution with k components</p>
</td></tr>
<tr><td><code id="post_sample.normalmix_+3A_data">data</code></td>
<td>
<p>a list with components x and s to be interpreted as a 
normally-distributed observation and its standard error</p>
</td></tr>
<tr><td><code id="post_sample.normalmix_+3A_nsamp">nsamp</code></td>
<td>
<p>number of samples to return for each observation</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a nsamp by n matrix
</p>

<hr>
<h2 id='post_sample.unimix'>post_sample.unimix</h2><span id='topic+post_sample.unimix'></span>

<h3>Description</h3>

<p>returns random samples from the posterior, given a
prior distribution m and n observed datapoints.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'unimix'
post_sample(m, data, nsamp)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="post_sample.unimix_+3A_m">m</code></td>
<td>
<p>mixture distribution with k components</p>
</td></tr>
<tr><td><code id="post_sample.unimix_+3A_data">data</code></td>
<td>
<p>a list with components x and s to be interpreted as a 
normally-distributed observation and its standard error</p>
</td></tr>
<tr><td><code id="post_sample.unimix_+3A_nsamp">nsamp</code></td>
<td>
<p>number of samples to return for each observation</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a nsamp by n matrix
</p>

<hr>
<h2 id='posterior_dist'>Compute Posterior</h2><span id='topic+posterior_dist'></span>

<h3>Description</h3>

<p>Return the posterior on beta given a prior (g) that is
a mixture of normals (class normalmix) and observation
<code class="reqn">betahat ~ N(beta,sebetahat)</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>posterior_dist(g, betahat, sebetahat)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="posterior_dist_+3A_g">g</code></td>
<td>
<p>a normalmix with components indicating the prior; works
only if g has means 0</p>
</td></tr>
<tr><td><code id="posterior_dist_+3A_betahat">betahat</code></td>
<td>
<p>(n vector of observations)</p>
</td></tr>
<tr><td><code id="posterior_dist_+3A_sebetahat">sebetahat</code></td>
<td>
<p>(n vector of standard errors/deviations of
observations)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This can be used to obt
</p>


<h3>Value</h3>

<p>A list, (pi1,mu1,sigma1) whose components are each k by n matrices
where k is number of mixture components in g, n is number of observations in betahat
</p>

<hr>
<h2 id='postmean'>postmean</h2><span id='topic+postmean'></span>

<h3>Description</h3>

<p>postmean
</p>


<h3>Usage</h3>

<pre><code class='language-R'>postmean(m, data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="postmean_+3A_m">m</code></td>
<td>
<p>mixture distribution with k components</p>
</td></tr>
<tr><td><code id="postmean_+3A_data">data</code></td>
<td>
<p>details depend on the model</p>
</td></tr>
</table>

<hr>
<h2 id='postmean2'>postmean2</h2><span id='topic+postmean2'></span>

<h3>Description</h3>

<p>output posterior mean-squared value given prior
mixture m and data
</p>


<h3>Usage</h3>

<pre><code class='language-R'>postmean2(m, data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="postmean2_+3A_m">m</code></td>
<td>
<p>mixture distribution with k components</p>
</td></tr>
<tr><td><code id="postmean2_+3A_data">data</code></td>
<td>
<p>details depend on the model</p>
</td></tr>
</table>

<hr>
<h2 id='postsd'>postsd</h2><span id='topic+postsd'></span>

<h3>Description</h3>

<p>output posterior sd given prior mixture m and data
</p>


<h3>Usage</h3>

<pre><code class='language-R'>postsd(m, data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="postsd_+3A_m">m</code></td>
<td>
<p>mixture distribution with k components</p>
</td></tr>
<tr><td><code id="postsd_+3A_data">data</code></td>
<td>
<p>details depend on the model</p>
</td></tr>
</table>

<hr>
<h2 id='print.ash'>Print method for ash object</h2><span id='topic+print.ash'></span>

<h3>Description</h3>

<p>Print the fitted distribution of beta values in the EB
hierarchical model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ash'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.ash_+3A_x">x</code></td>
<td>
<p>the fitted ash object</p>
</td></tr>
<tr><td><code id="print.ash_+3A_...">...</code></td>
<td>
<p>not used, included for consistency as an S3
generic/method.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>None
</p>

<hr>
<h2 id='prune'>prune</h2><span id='topic+prune'></span>

<h3>Description</h3>

<p>prunes out mixture components with low weight
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prune(m, thresh = 1e-10)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prune_+3A_m">m</code></td>
<td>
<p>What is this argument?</p>
</td></tr>
<tr><td><code id="prune_+3A_thresh">thresh</code></td>
<td>
<p>the threshold below which components are removed</p>
</td></tr>
</table>

<hr>
<h2 id='qval.from.lfdr'>Function to compute q values from local false discovery rates</h2><span id='topic+qval.from.lfdr'></span>

<h3>Description</h3>

<p>Computes q values from a vector of local fdr estimates
</p>


<h3>Usage</h3>

<pre><code class='language-R'>qval.from.lfdr(lfdr)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="qval.from.lfdr_+3A_lfdr">lfdr</code></td>
<td>
<p>a vector of local fdr estimates</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The q value for a given lfdr is an estimate of the (tail)
False Discovery Rate for all findings with a smaller lfdr, and
is found by the average of the lfdr for all more significant
findings. See Storey (2003), Annals of Statistics, for
definition of q value.
</p>


<h3>Value</h3>

<p>vector of q values
</p>

<hr>
<h2 id='set_data'>Takes raw data and sets up data object for use by ash</h2><span id='topic+set_data'></span>

<h3>Description</h3>

<p>Takes raw data and sets up data object for use by ash
</p>


<h3>Usage</h3>

<pre><code class='language-R'>set_data(betahat, sebetahat, lik = NULL, alpha = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="set_data_+3A_betahat">betahat</code></td>
<td>
<p>vector of betahats</p>
</td></tr>
<tr><td><code id="set_data_+3A_sebetahat">sebetahat</code></td>
<td>
<p>vector of standard errors</p>
</td></tr>
<tr><td><code id="set_data_+3A_lik">lik</code></td>
<td>
<p>a likelihood (see e.g., lik_normal())</p>
</td></tr>
<tr><td><code id="set_data_+3A_alpha">alpha</code></td>
<td>
<p>specifies value of alpha to use (model is for betahat/sebetahat^alpha | sebetahat)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The data object stores both the data, and details of the model to be used for the data.
For example, in the generalized version of ash the cdf and pdf of the likelihood are
stored here.
</p>


<h3>Value</h3>

<p>data object (list)
</p>

<hr>
<h2 id='summary.ash'>Summary method for ash object</h2><span id='topic+summary.ash'></span>

<h3>Description</h3>

<p>Print summary of fitted ash object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ash'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.ash_+3A_object">object</code></td>
<td>
<p>the fitted ash object</p>
</td></tr>
<tr><td><code id="summary.ash_+3A_...">...</code></td>
<td>
<p>not used, included for consistency as an S3
generic/method.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code><a href="base.html#topic+summary">summary</a></code> prints the fitted mixture, the
fitted log likelihood with 10 digits and a flag to indicate
convergence
</p>

<hr>
<h2 id='tnormalmix'>Constructor for tnormalmix class</h2><span id='topic+tnormalmix'></span>

<h3>Description</h3>

<p>Creates an object of class tnormalmix (finite mixture
of truncated univariate normals).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tnormalmix(pi, mean, sd, a, b)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tnormalmix_+3A_pi">pi</code></td>
<td>
<p>Cector of mixture proportions (length k say).</p>
</td></tr>
<tr><td><code id="tnormalmix_+3A_mean">mean</code></td>
<td>
<p>Vector of means (length k).</p>
</td></tr>
<tr><td><code id="tnormalmix_+3A_sd">sd</code></td>
<td>
<p>Vector of standard deviations (length k).</p>
</td></tr>
<tr><td><code id="tnormalmix_+3A_a">a</code></td>
<td>
<p>Vector of left truncation points of each component (length k).</p>
</td></tr>
<tr><td><code id="tnormalmix_+3A_b">b</code></td>
<td>
<p>Cector of right truncation points of each component (length k).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class &ldquo;tnormalmix&rdquo;.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tnormalmix(c(0.5,0.5),c(0,0),c(1,2),c(-10,0),c(0,10))

</code></pre>

<hr>
<h2 id='unimix'>Constructor for unimix class</h2><span id='topic+unimix'></span>

<h3>Description</h3>

<p>Creates an object of class unimix (finite mixture of
univariate uniforms)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>unimix(pi, a, b)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="unimix_+3A_pi">pi</code></td>
<td>
<p>vector of mixture proportions</p>
</td></tr>
<tr><td><code id="unimix_+3A_a">a</code></td>
<td>
<p>vector of left hand ends of uniforms</p>
</td></tr>
<tr><td><code id="unimix_+3A_b">b</code></td>
<td>
<p>vector of right hand ends of uniforms</p>
</td></tr>
</table>


<h3>Details</h3>

<p>None
</p>


<h3>Value</h3>

<p>an object of class unimix
</p>


<h3>Examples</h3>

<pre><code class='language-R'>unimix(c(0.5,0.5),c(0,0),c(1,2))
</code></pre>

<hr>
<h2 id='vcdf_post'>vcdf_post</h2><span id='topic+vcdf_post'></span>

<h3>Description</h3>

<p>vectorized version of <code><a href="#topic+cdf_post">cdf_post</a></code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vcdf_post(m, c, data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vcdf_post_+3A_m">m</code></td>
<td>
<p>mixture distribution with k components</p>
</td></tr>
<tr><td><code id="vcdf_post_+3A_c">c</code></td>
<td>
<p>a numeric vector</p>
</td></tr>
<tr><td><code id="vcdf_post_+3A_data">data</code></td>
<td>
<p>depends on context</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an n vector containing the cdf for beta_i at c
</p>


<h3>Examples</h3>

<pre><code class='language-R'>beta = rnorm(100,0,1)
betahat= beta+rnorm(100,0,1)
sebetahat=rep(1,100)
ash.beta = ash(betahat,1,mixcompdist="normal")
c = vcdf_post(get_fitted_g(ash.beta),seq(-5,5,length=1000),data = set_data(betahat,sebetahat))
</code></pre>

<hr>
<h2 id='w_mixEM'>Estimate mixture proportions of a mixture model by EM algorithm (weighted version)</h2><span id='topic+w_mixEM'></span>

<h3>Description</h3>

<p>Given the individual component likelihoods for a mixture model, and a set of weights, estimates the mixture proportions by an EM algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>w_mixEM(matrix_lik, prior, pi_init = NULL, weights = NULL, control = list())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="w_mixEM_+3A_matrix_lik">matrix_lik</code></td>
<td>
<p>a n by k matrix with (j,k)th element equal to <code class="reqn">f_k(x_j)</code>.</p>
</td></tr>
<tr><td><code id="w_mixEM_+3A_prior">prior</code></td>
<td>
<p>a k vector of the parameters of the Dirichlet prior on <code class="reqn">\pi</code>. Recommended to be rep(1,k)</p>
</td></tr>
<tr><td><code id="w_mixEM_+3A_pi_init">pi_init</code></td>
<td>
<p>the initial value of <code class="reqn">\pi</code> to use. If not specified defaults to (1/k,...,1/k).</p>
</td></tr>
<tr><td><code id="w_mixEM_+3A_weights">weights</code></td>
<td>
<p>an n vector of weights</p>
</td></tr>
<tr><td><code id="w_mixEM_+3A_control">control</code></td>
<td>
<p>A list of control parameters for the SQUAREM algorithm, default value is set to be control.default=list(K = 1, method=3, square=TRUE, step.min0=1, step.max0=1, mstep=4, kr=1, objfn.inc=1,tol=1.e-07, maxiter=5000, trace=FALSE).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Fits a k component mixture model </p>
<p style="text-align: center;"><code class="reqn">f(x|\pi)= \sum_k \pi_k f_k(x)</code>
</p>
<p> to independent
and identically distributed data <code class="reqn">x_1,\dots,x_n</code> with weights <code class="reqn">w_1,\dots,w_n</code>.
Estimates mixture proportions <code class="reqn">\pi</code> by maximum likelihood, or by maximum a posteriori (MAP) estimation for a Dirichlet prior on <code class="reqn">\pi</code> 
(if a prior is specified).  Here the log-likelihood for the weighted data is defined as <code class="reqn">l(\pi) = \sum_j w_j log f(x_j | \pi)</code>. Uses the SQUAREM package to accelerate convergence of EM. Used by the ash main function; there is no need for a user to call this 
function separately, but it is exported for convenience.
</p>


<h3>Value</h3>

<p>A list, including the estimates (pihat), the log likelihood for each interation (B)
and a flag to indicate convergence
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
