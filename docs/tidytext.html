<!DOCTYPE html><html><head><title>Help for package tidytext</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {tidytext}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#tidytext-package'><p>tidytext: Text Mining using 'dplyr', 'ggplot2', and Other Tidy Tools</p></a></li>
<li><a href='#bind_tf_idf'><p>Bind the term frequency and inverse document frequency of a tidy text</p>
dataset to the dataset</a></li>
<li><a href='#cast_sparse'><p>Create a sparse matrix from row names, column names, and values</p>
in a table.</a></li>
<li><a href='#cast_tdm'><p>Casting a data frame to</p>
a DocumentTermMatrix, TermDocumentMatrix, or dfm</a></li>
<li><a href='#corpus_tidiers'><p>Tidiers for a corpus object from the quanteda package</p></a></li>
<li><a href='#dictionary_tidiers'><p>Tidy dictionary objects from the quanteda package</p></a></li>
<li><a href='#get_sentiments'><p>Get a tidy data frame of a single sentiment lexicon</p></a></li>
<li><a href='#get_stopwords'><p>Get a tidy data frame of a single stopword lexicon</p></a></li>
<li><a href='#lda_tidiers'><p>Tidiers for LDA and CTM objects from the topicmodels package</p></a></li>
<li><a href='#mallet_tidiers'><p>Tidiers for Latent Dirichlet Allocation models from the mallet package</p></a></li>
<li><a href='#nma_words'><p>English negators, modals, and adverbs</p></a></li>
<li><a href='#parts_of_speech'><p>Parts of speech for English words from the Moby Project</p></a></li>
<li><a href='#reexports'><p>Objects exported from other packages</p></a></li>
<li><a href='#reorder_within'><p>Reorder an x or y axis within facets</p></a></li>
<li><a href='#sentiments'><p>Sentiment lexicon from Bing Liu and collaborators</p></a></li>
<li><a href='#stm_tidiers'><p>Tidiers for Structural Topic Models from the stm package</p></a></li>
<li><a href='#stop_words'><p>Various lexicons for English stop words</p></a></li>
<li><a href='#tdm_tidiers'><p>Tidy DocumentTermMatrix, TermDocumentMatrix, and related objects</p>
from the tm package</a></li>
<li><a href='#tidy_triplet'><p>Utility function to tidy a simple triplet matrix</p></a></li>
<li><a href='#tidy.Corpus'><p>Tidy a Corpus object from the tm package</p></a></li>
<li><a href='#unnest_characters'><p>Wrapper around unnest_tokens for characters and character shingles</p></a></li>
<li><a href='#unnest_ngrams'><p>Wrapper around unnest_tokens for n-grams</p></a></li>
<li><a href='#unnest_ptb'><p>Wrapper around unnest_tokens for Penn Treebank Tokenizer</p></a></li>
<li><a href='#unnest_regex'><p>Wrapper around unnest_tokens for regular expressions</p></a></li>
<li><a href='#unnest_sentences'><p>Wrapper around unnest_tokens for sentences, lines, and paragraphs</p></a></li>
<li><a href='#unnest_tokens'><p>Split a column into tokens</p></a></li>
<li><a href='#unnest_tweets'><p>Wrapper around unnest_tokens for tweets</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Text Mining using 'dplyr', 'ggplot2', and Other Tidy Tools</td>
</tr>
<tr>
<td>Version:</td>
<td>0.4.1</td>
</tr>
<tr>
<td>Description:</td>
<td>Using tidy data principles can make many text mining tasks
    easier, more effective, and consistent with tools already in wide use.
    Much of the infrastructure needed for text mining with tidy data
    frames already exists in packages like 'dplyr', 'broom', 'tidyr', and
    'ggplot2'. In this package, we provide functions and supporting data
    sets to allow conversion of text to and from tidy formats, and to
    switch seamlessly between tidy tools and existing text mining
    packages.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/juliasilge/tidytext">https://github.com/juliasilge/tidytext</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/juliasilge/tidytext/issues">https://github.com/juliasilge/tidytext/issues</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10)</td>
</tr>
<tr>
<td>Imports:</td>
<td>cli, dplyr, generics, janeaustenr, lifecycle, Matrix, methods,
purrr (&ge; 0.1.1), rlang (&ge; 0.4.10), stringr, tibble,
tokenizers, vctrs</td>
</tr>
<tr>
<td>Suggests:</td>
<td>broom, covr, data.table, ggplot2, hunspell, knitr, mallet,
NLP, quanteda, readr, reshape2, rmarkdown, scales, stm,
stopwords, testthat (&ge; 2.1.0), textdata, tidyr, tm,
topicmodels, vdiffr, wordcloud</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Config/Needs/website:</td>
<td>ropensci/gutenbergr</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>TRUE</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.2</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-01-07 23:06:42 UTC; juliasilge</td>
</tr>
<tr>
<td>Author:</td>
<td>Gabriela De Queiroz [ctb],
  Colin Fay <a href="https://orcid.org/0000-0001-7343-1846"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [ctb],
  Emil Hvitfeldt [ctb],
  Os Keyes <a href="https://orcid.org/0000-0001-5196-609X"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [ctb],
  Kanishka Misra [ctb],
  Tim Mastny [ctb],
  Jeff Erickson [ctb],
  David Robinson [aut],
  Julia Silge <a href="https://orcid.org/0000-0002-3671-836X"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut,
    cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Julia Silge &lt;julia.silge@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-01-07 23:30:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='tidytext-package'>tidytext: Text Mining using 'dplyr', 'ggplot2', and Other Tidy Tools</h2><span id='topic+tidytext'></span><span id='topic+tidytext-package'></span>

<h3>Description</h3>

<p><img src="../help/figures/logo.png" style='float: right' alt='logo' width='120' />
</p>
<p>Using tidy data principles can make many text mining tasks easier, more effective, and consistent with tools already in wide use. Much of the infrastructure needed for text mining with tidy data frames already exists in packages like 'dplyr', 'broom', 'tidyr', and 'ggplot2'. In this package, we provide functions and supporting data sets to allow conversion of text to and from tidy formats, and to switch seamlessly between tidy tools and existing text mining packages.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Julia Silge <a href="mailto:julia.silge@gmail.com">julia.silge@gmail.com</a> (<a href="https://orcid.org/0000-0002-3671-836X">ORCID</a>)
</p>
<p>Authors:
</p>

<ul>
<li><p> David Robinson <a href="mailto:admiral.david@gmail.com">admiral.david@gmail.com</a>
</p>
</li></ul>

<p>Other contributors:
</p>

<ul>
<li><p> Gabriela De Queiroz <a href="mailto:gabidequeiroz@gmail.com">gabidequeiroz@gmail.com</a> [contributor]
</p>
</li>
<li><p> Colin Fay <a href="mailto:contact@colinfay.me">contact@colinfay.me</a> (<a href="https://orcid.org/0000-0001-7343-1846">ORCID</a>) [contributor]
</p>
</li>
<li><p> Emil Hvitfeldt <a href="mailto:emilhhvitfeldt@gmail.com">emilhhvitfeldt@gmail.com</a> [contributor]
</p>
</li>
<li><p> Os Keyes <a href="mailto:ironholds@gmail.com">ironholds@gmail.com</a> (<a href="https://orcid.org/0000-0001-5196-609X">ORCID</a>) [contributor]
</p>
</li>
<li><p> Kanishka Misra <a href="mailto:kmisra@purdue.edu">kmisra@purdue.edu</a> [contributor]
</p>
</li>
<li><p> Tim Mastny <a href="mailto:tim.mastny@gmail.com">tim.mastny@gmail.com</a> [contributor]
</p>
</li>
<li><p> Jeff Erickson <a href="mailto:jeff@erick.so">jeff@erick.so</a> [contributor]
</p>
</li></ul>



<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://github.com/juliasilge/tidytext">https://github.com/juliasilge/tidytext</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/juliasilge/tidytext/issues">https://github.com/juliasilge/tidytext/issues</a>
</p>
</li></ul>


<hr>
<h2 id='bind_tf_idf'>Bind the term frequency and inverse document frequency of a tidy text
dataset to the dataset</h2><span id='topic+bind_tf_idf'></span>

<h3>Description</h3>

<p>Calculate and bind the term frequency and inverse document frequency of a
tidy text dataset, along with the product, tf-idf, to the dataset. Each of
these values are added as columns. This function supports non-standard
evaluation through the tidyeval framework.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bind_tf_idf(tbl, term, document, n)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bind_tf_idf_+3A_tbl">tbl</code></td>
<td>
<p>A tidy text dataset with one-row-per-term-per-document</p>
</td></tr>
<tr><td><code id="bind_tf_idf_+3A_term">term</code></td>
<td>
<p>Column containing terms as string or symbol</p>
</td></tr>
<tr><td><code id="bind_tf_idf_+3A_document">document</code></td>
<td>
<p>Column containing document IDs as string or symbol</p>
</td></tr>
<tr><td><code id="bind_tf_idf_+3A_n">n</code></td>
<td>
<p>Column containing document-term counts as string or symbol</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The arguments <code>term</code>, <code>document</code>, and <code>n</code>
are passed by expression and support <a href="rlang.html#topic+topic-inject">quasiquotation</a>;
you can unquote strings and symbols.
</p>
<p>If the dataset is grouped, the groups are ignored but are
retained.
</p>
<p>The dataset must have exactly one row per document-term combination
for this to work.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(dplyr)
library(janeaustenr)

book_words &lt;- austen_books() %&gt;%
  unnest_tokens(word, text) %&gt;%
  count(book, word, sort = TRUE)

book_words

# find the words most distinctive to each document
book_words %&gt;%
  bind_tf_idf(word, book, n) %&gt;%
  arrange(desc(tf_idf))

</code></pre>

<hr>
<h2 id='cast_sparse'>Create a sparse matrix from row names, column names, and values
in a table.</h2><span id='topic+cast_sparse'></span>

<h3>Description</h3>

<p>This function supports non-standard evaluation through the tidyeval framework.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cast_sparse(data, row, column, value, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cast_sparse_+3A_data">data</code></td>
<td>
<p>A tbl</p>
</td></tr>
<tr><td><code id="cast_sparse_+3A_row">row</code></td>
<td>
<p>Column name to use as row names in sparse matrix, as string or symbol</p>
</td></tr>
<tr><td><code id="cast_sparse_+3A_column">column</code></td>
<td>
<p>Column name to use as column names in sparse matrix, as string or symbol</p>
</td></tr>
<tr><td><code id="cast_sparse_+3A_value">value</code></td>
<td>
<p>Column name to use as sparse matrix values (default 1) as string or symbol</p>
</td></tr>
<tr><td><code id="cast_sparse_+3A_...">...</code></td>
<td>
<p>Extra arguments to pass on to <code><a href="Matrix.html#topic+sparseMatrix">sparseMatrix()</a></code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that cast_sparse ignores groups in a grouped tbl_df. The arguments
<code>row</code>, <code>column</code>, and <code>value</code> are passed by expression and support
<a href="rlang.html#topic+topic-inject">quasiquotation</a>; you can unquote strings and symbols.
</p>


<h3>Value</h3>

<p>A sparse Matrix object, with one row for each unique value in
the <code>row</code> column, one column for each unique value in the <code>column</code>
column, and with as many non-zero values as there are rows in <code>data</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
dat &lt;- data.frame(a = c("row1", "row1", "row2", "row2", "row2"),
                  b = c("col1", "col2", "col1", "col3", "col4"),
                  val = 1:5)

cast_sparse(dat, a, b)

cast_sparse(dat, a, b, val)

</code></pre>

<hr>
<h2 id='cast_tdm'>Casting a data frame to
a DocumentTermMatrix, TermDocumentMatrix, or dfm</h2><span id='topic+cast_tdm'></span><span id='topic+cast_dtm'></span><span id='topic+cast_dfm'></span>

<h3>Description</h3>

<p>This turns a &quot;tidy&quot; one-term-per-document-per-row data frame into a
DocumentTermMatrix or TermDocumentMatrix from the tm package, or a
dfm from the quanteda package. These functions support non-standard
evaluation through the tidyeval framework. Groups are ignored.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cast_tdm(data, term, document, value, weighting = tm::weightTf, ...)

cast_dtm(data, document, term, value, weighting = tm::weightTf, ...)

cast_dfm(data, document, term, value, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cast_tdm_+3A_data">data</code></td>
<td>
<p>Table with one-term-per-document-per-row</p>
</td></tr>
<tr><td><code id="cast_tdm_+3A_term">term</code></td>
<td>
<p>Column containing terms as string or symbol</p>
</td></tr>
<tr><td><code id="cast_tdm_+3A_document">document</code></td>
<td>
<p>Column containing document IDs as string or symbol</p>
</td></tr>
<tr><td><code id="cast_tdm_+3A_value">value</code></td>
<td>
<p>Column containing values as string or symbol</p>
</td></tr>
<tr><td><code id="cast_tdm_+3A_weighting">weighting</code></td>
<td>
<p>The weighting function for the DTM/TDM
(default is term-frequency, effectively unweighted)</p>
</td></tr>
<tr><td><code id="cast_tdm_+3A_...">...</code></td>
<td>
<p>Extra arguments passed on to
<code><a href="Matrix.html#topic+sparseMatrix">sparseMatrix()</a></code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>The arguments <code>term</code>, <code>document</code>, and <code>value</code>
are passed by expression and support <a href="rlang.html#topic+topic-inject">quasiquotation</a>;
you can unquote strings and symbols.
</p>

<hr>
<h2 id='corpus_tidiers'>Tidiers for a corpus object from the quanteda package</h2><span id='topic+corpus_tidiers'></span><span id='topic+tidy.corpus'></span><span id='topic+glance.corpus'></span>

<h3>Description</h3>

<p>Tidy a corpus object from the quanteda package. <code>tidy</code> returns a
tbl_df with one-row-per-document, with a <code>text</code> column containing
the document's text, and one column for each document-level metadata.
<code>glance</code> returns a one-row tbl_df with corpus-level metadata,
such as source and created. For Corpus objects from the tm package,
see <code><a href="#topic+tidy.Corpus">tidy.Corpus()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'corpus'
tidy(x, ...)

## S3 method for class 'corpus'
glance(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="corpus_tidiers_+3A_x">x</code></td>
<td>
<p>A Corpus object, such as a VCorpus or PCorpus</p>
</td></tr>
<tr><td><code id="corpus_tidiers_+3A_...">...</code></td>
<td>
<p>Extra arguments, not used</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For the most part, the <code>tidy</code> output is equivalent to the
&quot;documents&quot; data frame in the corpus object, except that it is converted
to a tbl_df, and <code>texts</code> column is renamed to <code>text</code>
to be consistent with other uses in tidytext.
</p>
<p>Similarly, the <code>glance</code> output is simply the &quot;metadata&quot; object,
with NULL fields removed and turned into a one-row tbl_df.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
if (requireNamespace("quanteda", quietly = TRUE)) {
 data("data_corpus_inaugural", package = "quanteda")

 data_corpus_inaugural

 tidy(data_corpus_inaugural)
}

</code></pre>

<hr>
<h2 id='dictionary_tidiers'>Tidy dictionary objects from the quanteda package</h2><span id='topic+dictionary_tidiers'></span><span id='topic+tidy.dictionary2'></span>

<h3>Description</h3>

<p>Tidy dictionary objects from the quanteda package
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'dictionary2'
tidy(x, regex = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dictionary_tidiers_+3A_x">x</code></td>
<td>
<p>A dictionary object</p>
</td></tr>
<tr><td><code id="dictionary_tidiers_+3A_regex">regex</code></td>
<td>
<p>Whether to turn dictionary items from a glob to a regex</p>
</td></tr>
<tr><td><code id="dictionary_tidiers_+3A_...">...</code></td>
<td>
<p>Extra arguments, not used</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame with two columns: category and word.
</p>

<hr>
<h2 id='get_sentiments'>Get a tidy data frame of a single sentiment lexicon</h2><span id='topic+get_sentiments'></span>

<h3>Description</h3>

<p>Get specific sentiment lexicons in a tidy format, with one row per word,
in a form that can be joined with a one-word-per-row dataset.
The <code>"bing"</code> option comes from the included <code><a href="#topic+sentiments">sentiments()</a></code>
data frame, and others call the relevant function in the <span class="pkg">textdata</span>
package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_sentiments(lexicon = c("bing", "afinn", "loughran", "nrc"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_sentiments_+3A_lexicon">lexicon</code></td>
<td>
<p>The sentiment lexicon to retrieve;
either &quot;afinn&quot;, &quot;bing&quot;, &quot;nrc&quot;, or &quot;loughran&quot;</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A tbl_df with a <code>word</code> column, and either a <code>sentiment</code>
column (if <code>lexicon</code> is not &quot;afinn&quot;) or a numeric <code>value</code> column
(if <code>lexicon</code> is &quot;afinn&quot;).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(dplyr)

get_sentiments("bing")

## Not run: 
get_sentiments("afinn")
get_sentiments("nrc")

## End(Not run)

</code></pre>

<hr>
<h2 id='get_stopwords'>Get a tidy data frame of a single stopword lexicon</h2><span id='topic+get_stopwords'></span>

<h3>Description</h3>

<p>Get a specific stop word lexicon via the <span class="pkg">stopwords</span> package's
<a href="stopwords.html#topic+stopwords">stopwords</a> function, in a tidy format with one word per row.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_stopwords(language = "en", source = "snowball")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_stopwords_+3A_language">language</code></td>
<td>
<p>The language of the stopword lexicon specified as a
two-letter ISO code, such as <code>"es"</code>, <code>"de"</code>, or <code>"fr"</code>.
Default is <code>"en"</code> for English. Use
<a href="stopwords.html#topic+stopwords_getlanguages">stopwords_getlanguages</a> from <span class="pkg">stopwords</span> to see available
languages.</p>
</td></tr>
<tr><td><code id="get_stopwords_+3A_source">source</code></td>
<td>
<p>The source of the stopword lexicon specified. Default is
<code>"snowball"</code>. Use <a href="stopwords.html#topic+stopwords_getsources">stopwords_getsources</a> from
<span class="pkg">stopwords</span> to see available sources.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A tibble with two columns, <code>word</code> and <code>lexicon</code>. The
parameter <code>lexicon</code> is &quot;quanteda&quot; in this case.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

library(dplyr)
get_stopwords()
get_stopwords(source = "smart")
get_stopwords("es", "snowball")
get_stopwords("ru", "snowball")

</code></pre>

<hr>
<h2 id='lda_tidiers'>Tidiers for LDA and CTM objects from the topicmodels package</h2><span id='topic+lda_tidiers'></span><span id='topic+tidy.LDA'></span><span id='topic+tidy.CTM'></span><span id='topic+augment.LDA'></span><span id='topic+augment.CTM'></span><span id='topic+glance.LDA'></span><span id='topic+glance.CTM'></span>

<h3>Description</h3>

<p>Tidy the results of a Latent Dirichlet Allocation or Correlated Topic Model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'LDA'
tidy(x, matrix = c("beta", "gamma"), log = FALSE, ...)

## S3 method for class 'CTM'
tidy(x, matrix = c("beta", "gamma"), log = FALSE, ...)

## S3 method for class 'LDA'
augment(x, data, ...)

## S3 method for class 'CTM'
augment(x, data, ...)

## S3 method for class 'LDA'
glance(x, ...)

## S3 method for class 'CTM'
glance(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lda_tidiers_+3A_x">x</code></td>
<td>
<p>An LDA or CTM (or LDA_VEM/CTA_VEM) object from the topicmodels package</p>
</td></tr>
<tr><td><code id="lda_tidiers_+3A_matrix">matrix</code></td>
<td>
<p>Whether to tidy the beta (per-term-per-topic, default)
or gamma (per-document-per-topic) matrix</p>
</td></tr>
<tr><td><code id="lda_tidiers_+3A_log">log</code></td>
<td>
<p>Whether beta/gamma should be on a log scale, default FALSE</p>
</td></tr>
<tr><td><code id="lda_tidiers_+3A_...">...</code></td>
<td>
<p>Extra arguments, not used</p>
</td></tr>
<tr><td><code id="lda_tidiers_+3A_data">data</code></td>
<td>
<p>For <code>augment</code>, the data given to the LDA or CTM function, either
as a DocumentTermMatrix or as a tidied table with &quot;document&quot; and &quot;term&quot;
columns</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>tidy</code> returns a tidied version of either the beta or gamma matrix.
</p>
<p>If <code>matrix == "beta"</code> (default), returns a table with one row per topic and term,
with columns
</p>

<dl>
<dt>topic</dt><dd><p>Topic, as an integer</p>
</dd>
<dt>term</dt><dd><p>Term</p>
</dd>
<dt>beta</dt><dd><p>Probability of a term generated from a topic according to
the multinomial model</p>
</dd>
</dl>

<p>If <code>matrix == "gamma"</code>, returns a table with one row per topic and document,
with columns
</p>

<dl>
<dt>topic</dt><dd><p>Topic, as an integer</p>
</dd>
<dt>document</dt><dd><p>Document name or ID</p>
</dd>
<dt>gamma</dt><dd><p>Probability of topic given document</p>
</dd>
</dl>

<p><code>augment</code> returns a table with one row per original
document-term pair, such as is returned by <a href="#topic+tdm_tidiers">tdm_tidiers</a>:
</p>

<dl>
<dt>document</dt><dd><p>Name of document (if present), or index</p>
</dd>
<dt>term</dt><dd><p>Term</p>
</dd>
<dt>.topic</dt><dd><p>Topic assignment</p>
</dd>
</dl>

<p>If the <code>data</code> argument is provided, any columns in the original
data are included, combined based on the <code>document</code> and <code>term</code>
columns.
</p>
<p><code>glance</code> always returns a one-row table, with columns
</p>

<dl>
<dt>iter</dt><dd><p>Number of iterations used</p>
</dd>
<dt>terms</dt><dd><p>Number of terms in the model</p>
</dd>
<dt>alpha</dt><dd><p>If an LDA_VEM, the parameter of the Dirichlet distribution
for topics over documents</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>
if (requireNamespace("topicmodels", quietly = TRUE)) {
  set.seed(2016)
  library(dplyr)
  library(topicmodels)

  data("AssociatedPress", package = "topicmodels")
  ap &lt;- AssociatedPress[1:100, ]
  lda &lt;- LDA(ap, control = list(alpha = 0.1), k = 4)

  # get term distribution within each topic
  td_lda &lt;- tidy(lda)
  td_lda

  library(ggplot2)

  # visualize the top terms within each topic
  td_lda_filtered &lt;- td_lda %&gt;%
    filter(beta &gt; .004) %&gt;%
    mutate(term = reorder(term, beta))

  ggplot(td_lda_filtered, aes(term, beta)) +
    geom_bar(stat = "identity") +
    facet_wrap(~ topic, scales = "free") +
    theme(axis.text.x = element_text(angle = 90, size = 15))

  # get classification of each document
  td_lda_docs &lt;- tidy(lda, matrix = "gamma")
  td_lda_docs

  doc_classes &lt;- td_lda_docs %&gt;%
    group_by(document) %&gt;%
    top_n(1) %&gt;%
    ungroup()

  doc_classes

  # which were we most uncertain about?
  doc_classes %&gt;%
    arrange(gamma)
}

</code></pre>

<hr>
<h2 id='mallet_tidiers'>Tidiers for Latent Dirichlet Allocation models from the mallet package</h2><span id='topic+mallet_tidiers'></span><span id='topic+tidy.jobjRef'></span><span id='topic+augment.jobjRef'></span>

<h3>Description</h3>

<p>Tidy LDA models fit by the mallet package, which wraps the Mallet topic
modeling package in Java. The arguments and return values
are similar to <code><a href="#topic+lda_tidiers">lda_tidiers()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'jobjRef'
tidy(
  x,
  matrix = c("beta", "gamma"),
  log = FALSE,
  normalized = TRUE,
  smoothed = TRUE,
  ...
)

## S3 method for class 'jobjRef'
augment(x, data, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mallet_tidiers_+3A_x">x</code></td>
<td>
<p>A jobjRef object, of type RTopicModel, such as created
by <code><a href="mallet.html#topic+MalletLDA">mallet::MalletLDA()</a></code>.</p>
</td></tr>
<tr><td><code id="mallet_tidiers_+3A_matrix">matrix</code></td>
<td>
<p>Whether to tidy the beta (per-term-per-topic, default)
or gamma (per-document-per-topic) matrix.</p>
</td></tr>
<tr><td><code id="mallet_tidiers_+3A_log">log</code></td>
<td>
<p>Whether beta/gamma should be on a log scale, default FALSE</p>
</td></tr>
<tr><td><code id="mallet_tidiers_+3A_normalized">normalized</code></td>
<td>
<p>If true (default), normalize so that each
document or word sums to one across the topics. If false, values will
be integers representing the actual number of word-topic or document-topic
assignments.</p>
</td></tr>
<tr><td><code id="mallet_tidiers_+3A_smoothed">smoothed</code></td>
<td>
<p>If true (default), add the smoothing parameter to each
to avoid any values being zero. This smoothing parameter is initialized
as <code>alpha.sum</code> in <code><a href="mallet.html#topic+MalletLDA">mallet::MalletLDA()</a></code>.</p>
</td></tr>
<tr><td><code id="mallet_tidiers_+3A_...">...</code></td>
<td>
<p>Extra arguments, not used</p>
</td></tr>
<tr><td><code id="mallet_tidiers_+3A_data">data</code></td>
<td>
<p>For <code>augment</code>, the data given to the LDA function, either
as a DocumentTermMatrix or as a tidied table with &quot;document&quot; and &quot;term&quot;
columns.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that the LDA models from <code><a href="mallet.html#topic+MalletLDA">mallet::MalletLDA()</a></code>
are technically a special case of S4 objects with class <code>jobjRef</code>.
These are thus implemented as <code>jobjRef</code> tidiers, with a check for
whether the <code>toString</code> output is as expected.
</p>


<h3>Value</h3>

<p><code>augment</code> must be provided a data argument containing
one row per original document-term pair, such as is returned by
<a href="#topic+tdm_tidiers">tdm_tidiers</a>, containing columns <code>document</code> and <code>term</code>.
It returns that same data with an additional column
<code>.topic</code> with the topic assignment for that document-term combination.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+lda_tidiers">lda_tidiers()</a></code>, <code><a href="mallet.html#topic+mallet.doc.topics">mallet::mallet.doc.topics()</a></code>,
<code><a href="mallet.html#topic+mallet.topic.words">mallet::mallet.topic.words()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 
library(mallet)
library(dplyr)

data("AssociatedPress", package = "topicmodels")
td &lt;- tidy(AssociatedPress)

# mallet needs a file with stop words
tmp &lt;- tempfile()
writeLines(stop_words$word, tmp)

# two vectors: one with document IDs, one with text
docs &lt;- td %&gt;%
  group_by(document = as.character(document)) %&gt;%
  summarize(text = paste(rep(term, count), collapse = " "))

docs &lt;- mallet.import(docs$document, docs$text, tmp)

# create and run a topic model
topic_model &lt;- MalletLDA(num.topics = 4)
topic_model$loadDocuments(docs)
topic_model$train(20)

# tidy the word-topic combinations
td_beta &lt;- tidy(topic_model)
td_beta

# Examine the four topics
td_beta %&gt;%
  group_by(topic) %&gt;%
  top_n(8, beta) %&gt;%
  ungroup() %&gt;%
  mutate(term = reorder(term, beta)) %&gt;%
  ggplot(aes(term, beta)) +
  geom_col() +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()

# find the assignments of each word in each document
assignments &lt;- augment(topic_model, td)
assignments

## End(Not run)

</code></pre>

<hr>
<h2 id='nma_words'>English negators, modals, and adverbs</h2><span id='topic+nma_words'></span>

<h3>Description</h3>

<p>English negators, modals, and adverbs, as a data frame. A few of these
entries are two-word phrases instead of single words.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nma_words
</code></pre>


<h3>Format</h3>

<p>A data frame with 44 rows and 2 variables:
</p>

<dl>
<dt>word</dt><dd><p>An English word or bigram</p>
</dd>
<dt>modifier</dt><dd><p>The modifier type for <code>word</code>, either &quot;negator&quot;,
&quot;modal&quot;, or &quot;adverb&quot;</p>
</dd>
</dl>



<h3>Source</h3>

<p><a href="http://saifmohammad.com/WebPages/SCL.html#NMA">http://saifmohammad.com/WebPages/SCL.html#NMA</a>
</p>

<hr>
<h2 id='parts_of_speech'>Parts of speech for English words from the Moby Project</h2><span id='topic+parts_of_speech'></span>

<h3>Description</h3>

<p>Parts of speech for English words from the Moby Project by Grady Ward.
Words with non-ASCII characters and items with a space have been removed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>parts_of_speech
</code></pre>


<h3>Format</h3>

<p>A data frame with 205,985 rows and 2 variables:
</p>

<dl>
<dt>word</dt><dd><p>An English word</p>
</dd>
<dt>pos</dt><dd><p>The part of speech of the word. One of 13 options, such as
&quot;Noun&quot;, &quot;Adverb&quot;, &quot;Adjective&quot;</p>
</dd>
</dl>



<h3>Details</h3>

<p>Another dataset of English parts of speech, available only for
non-commercial use, is available as part of SUBTLEXus at
<a href="https://www.ugent.be/pp/experimentele-psychologie/en/research/documents/subtlexus/">https://www.ugent.be/pp/experimentele-psychologie/en/research/documents/subtlexus/</a>.
</p>


<h3>Source</h3>

<p><a href="https://archive.org/details/mobypartofspeech03203gut">https://archive.org/details/mobypartofspeech03203gut</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(dplyr)

parts_of_speech

parts_of_speech %&gt;%
  count(pos, sort = TRUE)

</code></pre>

<hr>
<h2 id='reexports'>Objects exported from other packages</h2><span id='topic+reexports'></span><span id='topic+glance'></span><span id='topic+augment'></span><span id='topic+tidy'></span>

<h3>Description</h3>

<p>These objects are imported from other packages. Follow the links
below to see their documentation.
</p>

<dl>
<dt>generics</dt><dd><p><code><a href="generics.html#topic+augment">augment</a></code>, <code><a href="generics.html#topic+augment">augment</a></code>, <code><a href="generics.html#topic+glance">glance</a></code>, <code><a href="generics.html#topic+tidy">tidy</a></code></p>
</dd>
</dl>

<hr>
<h2 id='reorder_within'>Reorder an x or y axis within facets</h2><span id='topic+reorder_within'></span><span id='topic+scale_x_reordered'></span><span id='topic+scale_y_reordered'></span><span id='topic+reorder_func'></span>

<h3>Description</h3>

<p>Reorder a column before plotting with faceting, such that the values are
ordered within each facet. This requires two functions: <code>reorder_within</code>
applied to the column, then either <code>scale_x_reordered</code> or
<code>scale_y_reordered</code> added to the plot.
This is implemented as a bit of a hack: it appends ___ and then the facet
at the end of each string.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>reorder_within(x, by, within, fun = mean, sep = "___", ...)

scale_x_reordered(..., labels = reorder_func, sep = deprecated())

scale_y_reordered(..., labels = reorder_func, sep = deprecated())

reorder_func(x, sep = "___")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="reorder_within_+3A_x">x</code></td>
<td>
<p>Vector to reorder.</p>
</td></tr>
<tr><td><code id="reorder_within_+3A_by">by</code></td>
<td>
<p>Vector of the same length, to use for reordering.</p>
</td></tr>
<tr><td><code id="reorder_within_+3A_within">within</code></td>
<td>
<p>Vector or list of vectors of the same length that will later
be used for faceting. A list of vectors will be used to facet within multiple
variables.</p>
</td></tr>
<tr><td><code id="reorder_within_+3A_fun">fun</code></td>
<td>
<p>Function to perform within each subset to determine the resulting
ordering. By default, mean.</p>
</td></tr>
<tr><td><code id="reorder_within_+3A_sep">sep</code></td>
<td>
<p>Separator to distinguish <code>by</code> and <code>within</code>. You may want to set this
manually if ___ can exist within one of your labels.</p>
</td></tr>
<tr><td><code id="reorder_within_+3A_...">...</code></td>
<td>
<p>In <code>reorder_within</code> arguments passed on to
<code><a href="stats.html#topic+reorder">reorder()</a></code>. In the scale functions, extra arguments passed on to
<code><a href="ggplot2.html#topic+scale_discrete">ggplot2::scale_x_discrete()</a></code> or <code><a href="ggplot2.html#topic+scale_discrete">ggplot2::scale_y_discrete()</a></code>.</p>
</td></tr>
<tr><td><code id="reorder_within_+3A_labels">labels</code></td>
<td>
<p>Function to transform the labels of
<code><a href="ggplot2.html#topic+scale_discrete">ggplot2::scale_x_discrete()</a></code>, by default <code>reorder_func</code>.</p>
</td></tr>
</table>


<h3>Source</h3>

<p>&quot;Ordering categories within ggplot2 Facets&quot; by Tyler Rinker:
<a href="https://trinkerrstuff.wordpress.com/2016/12/23/ordering-categories-within-ggplot2-facets/">https://trinkerrstuff.wordpress.com/2016/12/23/ordering-categories-within-ggplot2-facets/</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

library(tidyr)
library(ggplot2)

iris_gathered &lt;- gather(iris, metric, value, -Species)

# reordering doesn't work within each facet (see Sepal.Width):
ggplot(iris_gathered, aes(reorder(Species, value), value)) +
  geom_boxplot() +
  facet_wrap(~ metric)

# reorder_within and scale_x_reordered work.
# (Note that you need to set scales = "free_x" in the facet)
ggplot(iris_gathered, aes(reorder_within(Species, value, metric), value)) +
  geom_boxplot() +
  scale_x_reordered() +
  facet_wrap(~ metric, scales = "free_x")

# to reorder within multiple variables, set within to the list of
# facet variables.
ggplot(mtcars, aes(reorder_within(carb, mpg, list(vs, am)), mpg)) +
  geom_boxplot() +
  scale_x_reordered() +
  facet_wrap(vs ~ am, scales = "free_x")

</code></pre>

<hr>
<h2 id='sentiments'>Sentiment lexicon from Bing Liu and collaborators</h2><span id='topic+sentiments'></span>

<h3>Description</h3>

<p>Lexicon for opinion and sentiment analysis in a tidy data frame. This
dataset is included in this package with permission of the creators, and
may be used in research, commercial, etc. contexts with attribution, using
either the paper or URL below.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sentiments
</code></pre>


<h3>Format</h3>

<p>A data frame with 6,786 rows and 2 variables:
</p>

<dl>
<dt>word</dt><dd><p>An English word</p>
</dd>
<dt>sentiment</dt><dd><p>A sentiment for that word, either positive or negative.</p>
</dd>
</dl>



<h3>Details</h3>

<p>This lexicon was first published in:
</p>
<p>Minqing Hu and Bing Liu, &ldquo;Mining and summarizing customer reviews.&rdquo;,
Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery
&amp; Data Mining (KDD-2004), Seattle, Washington, USA, Aug 22-25, 2004.
</p>
<p>Words with non-ASCII characters were removed.
</p>


<h3>Source</h3>

<p><a href="https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html">https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html</a>
</p>

<hr>
<h2 id='stm_tidiers'>Tidiers for Structural Topic Models from the stm package</h2><span id='topic+stm_tidiers'></span><span id='topic+tidy.STM'></span><span id='topic+tidy.estimateEffect'></span><span id='topic+glance.estimateEffect'></span><span id='topic+augment.STM'></span><span id='topic+glance.STM'></span>

<h3>Description</h3>

<p>Tidy topic models fit by the stm package. The arguments and return values
are similar to <code><a href="#topic+lda_tidiers">lda_tidiers()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'STM'
tidy(
  x,
  matrix = c("beta", "gamma", "theta", "frex", "lift"),
  log = FALSE,
  document_names = NULL,
  ...
)

## S3 method for class 'estimateEffect'
tidy(x, ...)

## S3 method for class 'estimateEffect'
glance(x, ...)

## S3 method for class 'STM'
augment(x, data, ...)

## S3 method for class 'STM'
glance(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stm_tidiers_+3A_x">x</code></td>
<td>
<p>An STM fitted model object from either <code><a href="stm.html#topic+stm">stm::stm()</a></code> or
<code><a href="stm.html#topic+estimateEffect">stm::estimateEffect()</a></code></p>
</td></tr>
<tr><td><code id="stm_tidiers_+3A_matrix">matrix</code></td>
<td>
<p>Which matrix to tidy:
</p>

<ul>
<li><p> the beta matrix (per-term-per-topic, default)
</p>
</li>
<li><p> the gamma/theta matrix (per-document-per-topic); the stm package calls
this the theta matrix, but other topic modeling packages call this gamma
</p>
</li>
<li><p> the FREX matrix, for words with high frequency and exclusivity
</p>
</li>
<li><p> the lift matrix, for words with high lift
</p>
</li></ul>
</td></tr>
<tr><td><code id="stm_tidiers_+3A_log">log</code></td>
<td>
<p>Whether beta/gamma/theta should be on a log scale, default FALSE</p>
</td></tr>
<tr><td><code id="stm_tidiers_+3A_document_names">document_names</code></td>
<td>
<p>Optional vector of document names for use with
per-document-per-topic tidying</p>
</td></tr>
<tr><td><code id="stm_tidiers_+3A_...">...</code></td>
<td>
<p>Extra arguments for tidying, such as <code>w</code> as used in
<code><a href="stm.html#topic+calcfrex">stm::calcfrex()</a></code></p>
</td></tr>
<tr><td><code id="stm_tidiers_+3A_data">data</code></td>
<td>
<p>For <code>augment</code>, the data given to the stm function, either
as a <code>dfm</code> from quanteda or as a tidied table with &quot;document&quot; and
&quot;term&quot; columns</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>tidy</code> returns a tidied version of either the beta, gamma, FREX, or
lift matrix if called on an object from <code><a href="stm.html#topic+stm">stm::stm()</a></code>, or a tidied version of
the estimated regressions if called on an object from <code><a href="stm.html#topic+estimateEffect">stm::estimateEffect()</a></code>.
</p>
<p><code>glance</code> returns a tibble with exactly one row of model summaries.
</p>
<p><code>augment</code> must be provided a data argument, either a
<code>dfm</code> from quanteda or a table containing one row per original
document-term pair, such as is returned by <a href="#topic+tdm_tidiers">tdm_tidiers</a>, containing
columns <code>document</code> and <code>term</code>. It returns that same data with an additional
column <code>.topic</code> with the topic assignment for that document-term combination.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+lda_tidiers">lda_tidiers()</a></code>, <code><a href="stm.html#topic+calcfrex">stm::calcfrex()</a></code>, <code><a href="stm.html#topic+calclift">stm::calclift()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(dplyr)
library(ggplot2)
library(stm)
library(janeaustenr)

austen_sparse &lt;- austen_books() %&gt;%
    unnest_tokens(word, text) %&gt;%
    anti_join(stop_words) %&gt;%
    count(book, word) %&gt;%
    cast_sparse(book, word, n)
topic_model &lt;- stm(austen_sparse, K = 12, verbose = FALSE)

# tidy the word-topic combinations
td_beta &lt;- tidy(topic_model)
td_beta

# Examine the topics
td_beta %&gt;%
    group_by(topic) %&gt;%
    slice_max(beta, n = 10) %&gt;%
    ungroup() %&gt;%
    ggplot(aes(beta, term)) +
    geom_col() +
    facet_wrap(~ topic, scales = "free")

# high FREX words per topic
tidy(topic_model, matrix = "frex")

# high lift words per topic
tidy(topic_model, matrix = "lift")

# tidy the document-topic combinations, with optional document names
td_gamma &lt;- tidy(topic_model, matrix = "gamma",
                 document_names = rownames(austen_sparse))
td_gamma

# using stm's gardarianFit, we can tidy the result of a model
# estimated with covariates
effects &lt;- estimateEffect(1:3 ~ treatment, gadarianFit, gadarian)
glance(effects)
td_estimate &lt;- tidy(effects)
td_estimate

</code></pre>

<hr>
<h2 id='stop_words'>Various lexicons for English stop words</h2><span id='topic+stop_words'></span>

<h3>Description</h3>

<p>English stop words from three lexicons, as a data frame.
The snowball and SMART sets are pulled from the tm package. Note
that words with non-ASCII characters have been removed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stop_words
</code></pre>


<h3>Format</h3>

<p>A data frame with 1149 rows and 2 variables:
</p>

<dl>
<dt>word</dt><dd><p>An English word</p>
</dd>
<dt>lexicon</dt><dd><p>The source of the stop word. Either &quot;onix&quot;, &quot;SMART&quot;, or &quot;snowball&quot;</p>
</dd>
</dl>



<h3>Source</h3>


<ul>
<li> <p><a href="http://www.lextek.com/manuals/onix/stopwords1.html">http://www.lextek.com/manuals/onix/stopwords1.html</a>
</p>
</li>
<li> <p><a href="https://www.jmlr.org/papers/volume5/lewis04a/lewis04a.pdf">https://www.jmlr.org/papers/volume5/lewis04a/lewis04a.pdf</a>
</p>
</li>
<li> <p><a href="http://snowball.tartarus.org/algorithms/english/stop.txt">http://snowball.tartarus.org/algorithms/english/stop.txt</a>
</p>
</li></ul>


<hr>
<h2 id='tdm_tidiers'>Tidy DocumentTermMatrix, TermDocumentMatrix, and related objects
from the tm package</h2><span id='topic+tdm_tidiers'></span><span id='topic+tidy.DocumentTermMatrix'></span><span id='topic+tidy.TermDocumentMatrix'></span><span id='topic+tidy.dfm'></span><span id='topic+tidy.dfmSparse'></span><span id='topic+tidy.simple_triplet_matrix'></span>

<h3>Description</h3>

<p>Tidy a DocumentTermMatrix or TermDocumentMatrix into
a three-column data frame: <code>term{}</code>, and value (with
zeros missing), with one-row-per-term-per-document.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'DocumentTermMatrix'
tidy(x, ...)

## S3 method for class 'TermDocumentMatrix'
tidy(x, ...)

## S3 method for class 'dfm'
tidy(x, ...)

## S3 method for class 'dfmSparse'
tidy(x, ...)

## S3 method for class 'simple_triplet_matrix'
tidy(x, row_names = NULL, col_names = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tdm_tidiers_+3A_x">x</code></td>
<td>
<p>A DocumentTermMatrix or TermDocumentMatrix object</p>
</td></tr>
<tr><td><code id="tdm_tidiers_+3A_...">...</code></td>
<td>
<p>Extra arguments, not used</p>
</td></tr>
<tr><td><code id="tdm_tidiers_+3A_row_names">row_names</code></td>
<td>
<p>Specify row names</p>
</td></tr>
<tr><td><code id="tdm_tidiers_+3A_col_names">col_names</code></td>
<td>
<p>Specify column names</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
if (requireNamespace("topicmodels", quietly = TRUE)) {
  data("AssociatedPress", package = "topicmodels")
  AssociatedPress

  tidy(AssociatedPress)
}

</code></pre>

<hr>
<h2 id='tidy_triplet'>Utility function to tidy a simple triplet matrix</h2><span id='topic+tidy_triplet'></span>

<h3>Description</h3>

<p>Utility function to tidy a simple triplet matrix
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tidy_triplet(x, triplets, row_names = NULL, col_names = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tidy_triplet_+3A_x">x</code></td>
<td>
<p>Object with rownames and colnames</p>
</td></tr>
<tr><td><code id="tidy_triplet_+3A_triplets">triplets</code></td>
<td>
<p>A data frame or list of i, j, x</p>
</td></tr>
<tr><td><code id="tidy_triplet_+3A_row_names">row_names</code></td>
<td>
<p>rownames, if not gotten from rownames(x)</p>
</td></tr>
<tr><td><code id="tidy_triplet_+3A_col_names">col_names</code></td>
<td>
<p>colnames, if not gotten from colnames(x)</p>
</td></tr>
</table>

<hr>
<h2 id='tidy.Corpus'>Tidy a Corpus object from the tm package</h2><span id='topic+tidy.Corpus'></span>

<h3>Description</h3>

<p>Tidy a Corpus object from the tm package. Returns a data frame
with one-row-per-document, with a <code>text</code> column containing
the document's text, and one column for each local (per-document)
metadata tag. For corpus objects from the quanteda package,
see <code><a href="#topic+tidy.corpus">tidy.corpus()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'Corpus'
tidy(x, collapse = "\n", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tidy.Corpus_+3A_x">x</code></td>
<td>
<p>A Corpus object, such as a VCorpus or PCorpus</p>
</td></tr>
<tr><td><code id="tidy.Corpus_+3A_collapse">collapse</code></td>
<td>
<p>A string that should be used to
collapse text within each corpus (if a document has multiple lines).
Give NULL to not collapse strings, in which case a corpus
will end up as a list column if there are multi-line documents.</p>
</td></tr>
<tr><td><code id="tidy.Corpus_+3A_...">...</code></td>
<td>
<p>Extra arguments, not used</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
library(dplyr)   # displaying tbl_dfs

if (requireNamespace("tm", quietly = TRUE)) {
  library(tm)
  #' # tm package examples
  txt &lt;- system.file("texts", "txt", package = "tm")
  ovid &lt;- VCorpus(DirSource(txt, encoding = "UTF-8"),
                  readerControl = list(language = "lat"))

  ovid
  tidy(ovid)

  # choose different options for collapsing text within each
  # document
  tidy(ovid, collapse = "")$text
  tidy(ovid, collapse = NULL)$text

  # another example from Reuters articles
  reut21578 &lt;- system.file("texts", "crude", package = "tm")
  reuters &lt;- VCorpus(DirSource(reut21578),
                     readerControl = list(reader = readReut21578XMLasPlain))
  reuters

  tidy(reuters)
}

</code></pre>

<hr>
<h2 id='unnest_characters'>Wrapper around unnest_tokens for characters and character shingles</h2><span id='topic+unnest_characters'></span><span id='topic+unnest_character_shingles'></span>

<h3>Description</h3>

<p>These functions are a wrapper around <code>unnest_tokens( token = "characters" )</code>
and <code>unnest_tokens( token = "character_shingles" )</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>unnest_characters(
  tbl,
  output,
  input,
  strip_non_alphanum = TRUE,
  format = c("text", "man", "latex", "html", "xml"),
  to_lower = TRUE,
  drop = TRUE,
  collapse = NULL,
  ...
)

unnest_character_shingles(
  tbl,
  output,
  input,
  n = 3L,
  n_min = n,
  strip_non_alphanum = TRUE,
  format = c("text", "man", "latex", "html", "xml"),
  to_lower = TRUE,
  drop = TRUE,
  collapse = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="unnest_characters_+3A_tbl">tbl</code></td>
<td>
<p>A data frame</p>
</td></tr>
<tr><td><code id="unnest_characters_+3A_output">output</code></td>
<td>
<p>Output column to be created as string or symbol.</p>
</td></tr>
<tr><td><code id="unnest_characters_+3A_input">input</code></td>
<td>
<p>Input column that gets split as string or symbol.
</p>
<p>The output/input arguments are passed by expression and support
<a href="rlang.html#topic+topic-inject">quasiquotation</a>; you can unquote strings and symbols.</p>
</td></tr>
<tr><td><code id="unnest_characters_+3A_strip_non_alphanum">strip_non_alphanum</code></td>
<td>
<p>Should punctuation and white space be stripped?</p>
</td></tr>
<tr><td><code id="unnest_characters_+3A_format">format</code></td>
<td>
<p>Either &quot;text&quot;, &quot;man&quot;, &quot;latex&quot;, &quot;html&quot;, or &quot;xml&quot;. When the
format is &quot;text&quot;, this function uses the tokenizers package. If not &quot;text&quot;,
this uses the hunspell tokenizer, and can tokenize only by &quot;word&quot;.</p>
</td></tr>
<tr><td><code id="unnest_characters_+3A_to_lower">to_lower</code></td>
<td>
<p>Whether to convert tokens to lowercase.</p>
</td></tr>
<tr><td><code id="unnest_characters_+3A_drop">drop</code></td>
<td>
<p>Whether original input column should get dropped. Ignored
if the original input and new output column have the same name.</p>
</td></tr>
<tr><td><code id="unnest_characters_+3A_collapse">collapse</code></td>
<td>
<p>A character vector of variables to collapse text across,
or <code>NULL</code>.
</p>
<p>For tokens like n-grams or sentences, text can be collapsed across rows
within variables specified by <code>collapse</code> before tokenization. At tidytext
0.2.7, the default behavior for <code>collapse = NULL</code> changed to be more
consistent. The new behavior is that text is <em>not</em> collapsed for <code>NULL</code>.
</p>
<p>Grouping data specifies variables to collapse across in the same way as
<code>collapse</code> but you <strong>cannot</strong> use both the <code>collapse</code> argument and
grouped data. Collapsing applies mostly to <code>token</code> options of &quot;ngrams&quot;,
&quot;skip_ngrams&quot;, &quot;sentences&quot;, &quot;lines&quot;, &quot;paragraphs&quot;, or &quot;regex&quot;.</p>
</td></tr>
<tr><td><code id="unnest_characters_+3A_...">...</code></td>
<td>
<p>Extra arguments passed on to <a href="tokenizers.html#topic+tokenizers">tokenizers</a></p>
</td></tr>
<tr><td><code id="unnest_characters_+3A_n">n</code></td>
<td>
<p>The number of characters in each shingle. This must be an integer
greater than or equal to 1.</p>
</td></tr>
<tr><td><code id="unnest_characters_+3A_n_min">n_min</code></td>
<td>
<p>This must be an integer greater than or equal to 1, and less
than or equal to <code>n</code>.</p>
</td></tr>
</table>


<h3>See Also</h3>


<ul>
<li> <p><code><a href="#topic+unnest_tokens">unnest_tokens()</a></code>
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>library(dplyr)
library(janeaustenr)

d &lt;- tibble(txt = prideprejudice)

d %&gt;%
  unnest_characters(word, txt)

d %&gt;%
  unnest_character_shingles(word, txt, n = 3)

</code></pre>

<hr>
<h2 id='unnest_ngrams'>Wrapper around unnest_tokens for n-grams</h2><span id='topic+unnest_ngrams'></span><span id='topic+unnest_skip_ngrams'></span>

<h3>Description</h3>

<p>These functions are wrappers around <code>unnest_tokens( token = "ngrams" )</code>
and <code>unnest_tokens( token = "skip_ngrams" )</code> .
</p>


<h3>Usage</h3>

<pre><code class='language-R'>unnest_ngrams(
  tbl,
  output,
  input,
  n = 3L,
  n_min = n,
  ngram_delim = " ",
  format = c("text", "man", "latex", "html", "xml"),
  to_lower = TRUE,
  drop = TRUE,
  collapse = NULL,
  ...
)

unnest_skip_ngrams(
  tbl,
  output,
  input,
  n_min = 1,
  n = 3,
  k = 1,
  format = c("text", "man", "latex", "html", "xml"),
  to_lower = TRUE,
  drop = TRUE,
  collapse = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="unnest_ngrams_+3A_tbl">tbl</code></td>
<td>
<p>A data frame</p>
</td></tr>
<tr><td><code id="unnest_ngrams_+3A_output">output</code></td>
<td>
<p>Output column to be created as string or symbol.</p>
</td></tr>
<tr><td><code id="unnest_ngrams_+3A_input">input</code></td>
<td>
<p>Input column that gets split as string or symbol.
</p>
<p>The output/input arguments are passed by expression and support
<a href="rlang.html#topic+topic-inject">quasiquotation</a>; you can unquote strings and symbols.</p>
</td></tr>
<tr><td><code id="unnest_ngrams_+3A_n">n</code></td>
<td>
<p>The number of words in the n-gram. This must be an integer greater
than or equal to 1.</p>
</td></tr>
<tr><td><code id="unnest_ngrams_+3A_n_min">n_min</code></td>
<td>
<p>The minimum number of words in the n-gram. This must be an
integer greater than or equal to 1, and less than or equal to <code>n</code>.</p>
</td></tr>
<tr><td><code id="unnest_ngrams_+3A_ngram_delim">ngram_delim</code></td>
<td>
<p>The separator between words in an n-gram.</p>
</td></tr>
<tr><td><code id="unnest_ngrams_+3A_format">format</code></td>
<td>
<p>Either &quot;text&quot;, &quot;man&quot;, &quot;latex&quot;, &quot;html&quot;, or &quot;xml&quot;. When the
format is &quot;text&quot;, this function uses the tokenizers package. If not &quot;text&quot;,
this uses the hunspell tokenizer, and can tokenize only by &quot;word&quot;.</p>
</td></tr>
<tr><td><code id="unnest_ngrams_+3A_to_lower">to_lower</code></td>
<td>
<p>Whether to convert tokens to lowercase.</p>
</td></tr>
<tr><td><code id="unnest_ngrams_+3A_drop">drop</code></td>
<td>
<p>Whether original input column should get dropped. Ignored
if the original input and new output column have the same name.</p>
</td></tr>
<tr><td><code id="unnest_ngrams_+3A_collapse">collapse</code></td>
<td>
<p>A character vector of variables to collapse text across,
or <code>NULL</code>.
</p>
<p>For tokens like n-grams or sentences, text can be collapsed across rows
within variables specified by <code>collapse</code> before tokenization. At tidytext
0.2.7, the default behavior for <code>collapse = NULL</code> changed to be more
consistent. The new behavior is that text is <em>not</em> collapsed for <code>NULL</code>.
</p>
<p>Grouping data specifies variables to collapse across in the same way as
<code>collapse</code> but you <strong>cannot</strong> use both the <code>collapse</code> argument and
grouped data. Collapsing applies mostly to <code>token</code> options of &quot;ngrams&quot;,
&quot;skip_ngrams&quot;, &quot;sentences&quot;, &quot;lines&quot;, &quot;paragraphs&quot;, or &quot;regex&quot;.</p>
</td></tr>
<tr><td><code id="unnest_ngrams_+3A_...">...</code></td>
<td>
<p>Extra arguments passed on to <a href="tokenizers.html#topic+tokenizers">tokenizers</a></p>
</td></tr>
<tr><td><code id="unnest_ngrams_+3A_k">k</code></td>
<td>
<p>For the skip n-gram tokenizer, the maximum skip distance between
words. The function will compute all skip n-grams between <code>0</code> and
<code>k</code>.</p>
</td></tr>
</table>


<h3>See Also</h3>


<ul>
<li> <p><code><a href="#topic+unnest_tokens">unnest_tokens()</a></code>
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>library(dplyr)
library(janeaustenr)

d &lt;- tibble(txt = prideprejudice)

d %&gt;%
  unnest_ngrams(word, txt, n = 2)

d %&gt;%
  unnest_skip_ngrams(word, txt, n = 3, k = 1)

</code></pre>

<hr>
<h2 id='unnest_ptb'>Wrapper around unnest_tokens for Penn Treebank Tokenizer</h2><span id='topic+unnest_ptb'></span>

<h3>Description</h3>

<p>This function is a wrapper around <code>unnest_tokens( token = "ptb" )</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>unnest_ptb(
  tbl,
  output,
  input,
  format = c("text", "man", "latex", "html", "xml"),
  to_lower = TRUE,
  drop = TRUE,
  collapse = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="unnest_ptb_+3A_tbl">tbl</code></td>
<td>
<p>A data frame</p>
</td></tr>
<tr><td><code id="unnest_ptb_+3A_output">output</code></td>
<td>
<p>Output column to be created as string or symbol.</p>
</td></tr>
<tr><td><code id="unnest_ptb_+3A_input">input</code></td>
<td>
<p>Input column that gets split as string or symbol.
</p>
<p>The output/input arguments are passed by expression and support
<a href="rlang.html#topic+topic-inject">quasiquotation</a>; you can unquote strings and symbols.</p>
</td></tr>
<tr><td><code id="unnest_ptb_+3A_format">format</code></td>
<td>
<p>Either &quot;text&quot;, &quot;man&quot;, &quot;latex&quot;, &quot;html&quot;, or &quot;xml&quot;. When the
format is &quot;text&quot;, this function uses the tokenizers package. If not &quot;text&quot;,
this uses the hunspell tokenizer, and can tokenize only by &quot;word&quot;.</p>
</td></tr>
<tr><td><code id="unnest_ptb_+3A_to_lower">to_lower</code></td>
<td>
<p>Whether to convert tokens to lowercase.</p>
</td></tr>
<tr><td><code id="unnest_ptb_+3A_drop">drop</code></td>
<td>
<p>Whether original input column should get dropped. Ignored
if the original input and new output column have the same name.</p>
</td></tr>
<tr><td><code id="unnest_ptb_+3A_collapse">collapse</code></td>
<td>
<p>A character vector of variables to collapse text across,
or <code>NULL</code>.
</p>
<p>For tokens like n-grams or sentences, text can be collapsed across rows
within variables specified by <code>collapse</code> before tokenization. At tidytext
0.2.7, the default behavior for <code>collapse = NULL</code> changed to be more
consistent. The new behavior is that text is <em>not</em> collapsed for <code>NULL</code>.
</p>
<p>Grouping data specifies variables to collapse across in the same way as
<code>collapse</code> but you <strong>cannot</strong> use both the <code>collapse</code> argument and
grouped data. Collapsing applies mostly to <code>token</code> options of &quot;ngrams&quot;,
&quot;skip_ngrams&quot;, &quot;sentences&quot;, &quot;lines&quot;, &quot;paragraphs&quot;, or &quot;regex&quot;.</p>
</td></tr>
<tr><td><code id="unnest_ptb_+3A_...">...</code></td>
<td>
<p>Extra arguments passed on to <a href="tokenizers.html#topic+tokenizers">tokenizers</a></p>
</td></tr>
</table>


<h3>See Also</h3>


<ul>
<li> <p><code><a href="#topic+unnest_tokens">unnest_tokens()</a></code>
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>library(dplyr)
library(janeaustenr)

d &lt;- tibble(txt = prideprejudice)

d %&gt;%
  unnest_ptb(word, txt)

</code></pre>

<hr>
<h2 id='unnest_regex'>Wrapper around unnest_tokens for regular expressions</h2><span id='topic+unnest_regex'></span>

<h3>Description</h3>

<p>This function is a wrapper around <code>unnest_tokens( token = "regex" )</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>unnest_regex(
  tbl,
  output,
  input,
  pattern = "\\s+",
  format = c("text", "man", "latex", "html", "xml"),
  to_lower = TRUE,
  drop = TRUE,
  collapse = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="unnest_regex_+3A_tbl">tbl</code></td>
<td>
<p>A data frame</p>
</td></tr>
<tr><td><code id="unnest_regex_+3A_output">output</code></td>
<td>
<p>Output column to be created as string or symbol.</p>
</td></tr>
<tr><td><code id="unnest_regex_+3A_input">input</code></td>
<td>
<p>Input column that gets split as string or symbol.
</p>
<p>The output/input arguments are passed by expression and support
<a href="rlang.html#topic+topic-inject">quasiquotation</a>; you can unquote strings and symbols.</p>
</td></tr>
<tr><td><code id="unnest_regex_+3A_pattern">pattern</code></td>
<td>
<p>A regular expression that defines the split.</p>
</td></tr>
<tr><td><code id="unnest_regex_+3A_format">format</code></td>
<td>
<p>Either &quot;text&quot;, &quot;man&quot;, &quot;latex&quot;, &quot;html&quot;, or &quot;xml&quot;. When the
format is &quot;text&quot;, this function uses the tokenizers package. If not &quot;text&quot;,
this uses the hunspell tokenizer, and can tokenize only by &quot;word&quot;.</p>
</td></tr>
<tr><td><code id="unnest_regex_+3A_to_lower">to_lower</code></td>
<td>
<p>Whether to convert tokens to lowercase.</p>
</td></tr>
<tr><td><code id="unnest_regex_+3A_drop">drop</code></td>
<td>
<p>Whether original input column should get dropped. Ignored
if the original input and new output column have the same name.</p>
</td></tr>
<tr><td><code id="unnest_regex_+3A_collapse">collapse</code></td>
<td>
<p>A character vector of variables to collapse text across,
or <code>NULL</code>.
</p>
<p>For tokens like n-grams or sentences, text can be collapsed across rows
within variables specified by <code>collapse</code> before tokenization. At tidytext
0.2.7, the default behavior for <code>collapse = NULL</code> changed to be more
consistent. The new behavior is that text is <em>not</em> collapsed for <code>NULL</code>.
</p>
<p>Grouping data specifies variables to collapse across in the same way as
<code>collapse</code> but you <strong>cannot</strong> use both the <code>collapse</code> argument and
grouped data. Collapsing applies mostly to <code>token</code> options of &quot;ngrams&quot;,
&quot;skip_ngrams&quot;, &quot;sentences&quot;, &quot;lines&quot;, &quot;paragraphs&quot;, or &quot;regex&quot;.</p>
</td></tr>
<tr><td><code id="unnest_regex_+3A_...">...</code></td>
<td>
<p>Extra arguments passed on to <a href="tokenizers.html#topic+tokenizers">tokenizers</a></p>
</td></tr>
</table>


<h3>See Also</h3>


<ul>
<li> <p><code><a href="#topic+unnest_tokens">unnest_tokens()</a></code>
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>library(dplyr)
library(janeaustenr)

d &lt;- tibble(txt = prideprejudice)

d %&gt;%
  unnest_regex(word, txt, pattern = "Chapter [\\\\d]")

</code></pre>

<hr>
<h2 id='unnest_sentences'>Wrapper around unnest_tokens for sentences, lines, and paragraphs</h2><span id='topic+unnest_sentences'></span><span id='topic+unnest_lines'></span><span id='topic+unnest_paragraphs'></span>

<h3>Description</h3>

<p>These functions are wrappers around <code>unnest_tokens( token = "sentences" )</code>
<code>unnest_tokens( token = "lines" )</code> and <code>unnest_tokens( token = "paragraphs" )</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>unnest_sentences(
  tbl,
  output,
  input,
  strip_punct = FALSE,
  format = c("text", "man", "latex", "html", "xml"),
  to_lower = TRUE,
  drop = TRUE,
  collapse = NULL,
  ...
)

unnest_lines(
  tbl,
  output,
  input,
  format = c("text", "man", "latex", "html", "xml"),
  to_lower = TRUE,
  drop = TRUE,
  collapse = NULL,
  ...
)

unnest_paragraphs(
  tbl,
  output,
  input,
  paragraph_break = "\n\n",
  format = c("text", "man", "latex", "html", "xml"),
  to_lower = TRUE,
  drop = TRUE,
  collapse = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="unnest_sentences_+3A_tbl">tbl</code></td>
<td>
<p>A data frame</p>
</td></tr>
<tr><td><code id="unnest_sentences_+3A_output">output</code></td>
<td>
<p>Output column to be created as string or symbol.</p>
</td></tr>
<tr><td><code id="unnest_sentences_+3A_input">input</code></td>
<td>
<p>Input column that gets split as string or symbol.
</p>
<p>The output/input arguments are passed by expression and support
<a href="rlang.html#topic+topic-inject">quasiquotation</a>; you can unquote strings and symbols.</p>
</td></tr>
<tr><td><code id="unnest_sentences_+3A_strip_punct">strip_punct</code></td>
<td>
<p>Should punctuation be stripped?</p>
</td></tr>
<tr><td><code id="unnest_sentences_+3A_format">format</code></td>
<td>
<p>Either &quot;text&quot;, &quot;man&quot;, &quot;latex&quot;, &quot;html&quot;, or &quot;xml&quot;. When the
format is &quot;text&quot;, this function uses the tokenizers package. If not &quot;text&quot;,
this uses the hunspell tokenizer, and can tokenize only by &quot;word&quot;.</p>
</td></tr>
<tr><td><code id="unnest_sentences_+3A_to_lower">to_lower</code></td>
<td>
<p>Whether to convert tokens to lowercase.</p>
</td></tr>
<tr><td><code id="unnest_sentences_+3A_drop">drop</code></td>
<td>
<p>Whether original input column should get dropped. Ignored
if the original input and new output column have the same name.</p>
</td></tr>
<tr><td><code id="unnest_sentences_+3A_collapse">collapse</code></td>
<td>
<p>A character vector of variables to collapse text across,
or <code>NULL</code>.
</p>
<p>For tokens like n-grams or sentences, text can be collapsed across rows
within variables specified by <code>collapse</code> before tokenization. At tidytext
0.2.7, the default behavior for <code>collapse = NULL</code> changed to be more
consistent. The new behavior is that text is <em>not</em> collapsed for <code>NULL</code>.
</p>
<p>Grouping data specifies variables to collapse across in the same way as
<code>collapse</code> but you <strong>cannot</strong> use both the <code>collapse</code> argument and
grouped data. Collapsing applies mostly to <code>token</code> options of &quot;ngrams&quot;,
&quot;skip_ngrams&quot;, &quot;sentences&quot;, &quot;lines&quot;, &quot;paragraphs&quot;, or &quot;regex&quot;.</p>
</td></tr>
<tr><td><code id="unnest_sentences_+3A_...">...</code></td>
<td>
<p>Extra arguments passed on to <a href="tokenizers.html#topic+tokenizers">tokenizers</a></p>
</td></tr>
<tr><td><code id="unnest_sentences_+3A_paragraph_break">paragraph_break</code></td>
<td>
<p>A string identifying the boundary between two
paragraphs.</p>
</td></tr>
</table>


<h3>See Also</h3>


<ul>
<li> <p><code><a href="#topic+unnest_tokens">unnest_tokens()</a></code>
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>library(dplyr)
library(janeaustenr)

d &lt;- tibble(txt = prideprejudice)

d %&gt;%
  unnest_sentences(word, txt)

</code></pre>

<hr>
<h2 id='unnest_tokens'>Split a column into tokens</h2><span id='topic+unnest_tokens'></span>

<h3>Description</h3>

<p>Split a column into tokens, flattening the table into one-token-per-row.
This function supports non-standard evaluation through the tidyeval framework.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>unnest_tokens(
  tbl,
  output,
  input,
  token = "words",
  format = c("text", "man", "latex", "html", "xml"),
  to_lower = TRUE,
  drop = TRUE,
  collapse = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="unnest_tokens_+3A_tbl">tbl</code></td>
<td>
<p>A data frame</p>
</td></tr>
<tr><td><code id="unnest_tokens_+3A_output">output</code></td>
<td>
<p>Output column to be created as string or symbol.</p>
</td></tr>
<tr><td><code id="unnest_tokens_+3A_input">input</code></td>
<td>
<p>Input column that gets split as string or symbol.
</p>
<p>The output/input arguments are passed by expression and support
<a href="rlang.html#topic+topic-inject">quasiquotation</a>; you can unquote strings and symbols.</p>
</td></tr>
<tr><td><code id="unnest_tokens_+3A_token">token</code></td>
<td>
<p>Unit for tokenizing, or a custom tokenizing function. Built-in
options are &quot;words&quot; (default), &quot;characters&quot;, &quot;character_shingles&quot;, &quot;ngrams&quot;,
&quot;skip_ngrams&quot;, &quot;sentences&quot;, &quot;lines&quot;, &quot;paragraphs&quot;, &quot;regex&quot;, and
&quot;ptb&quot; (Penn Treebank). If a function, should take a character vector and
return a list of character vectors of the same length.</p>
</td></tr>
<tr><td><code id="unnest_tokens_+3A_format">format</code></td>
<td>
<p>Either &quot;text&quot;, &quot;man&quot;, &quot;latex&quot;, &quot;html&quot;, or &quot;xml&quot;. When the
format is &quot;text&quot;, this function uses the tokenizers package. If not &quot;text&quot;,
this uses the hunspell tokenizer, and can tokenize only by &quot;word&quot;.</p>
</td></tr>
<tr><td><code id="unnest_tokens_+3A_to_lower">to_lower</code></td>
<td>
<p>Whether to convert tokens to lowercase.</p>
</td></tr>
<tr><td><code id="unnest_tokens_+3A_drop">drop</code></td>
<td>
<p>Whether original input column should get dropped. Ignored
if the original input and new output column have the same name.</p>
</td></tr>
<tr><td><code id="unnest_tokens_+3A_collapse">collapse</code></td>
<td>
<p>A character vector of variables to collapse text across,
or <code>NULL</code>.
</p>
<p>For tokens like n-grams or sentences, text can be collapsed across rows
within variables specified by <code>collapse</code> before tokenization. At tidytext
0.2.7, the default behavior for <code>collapse = NULL</code> changed to be more
consistent. The new behavior is that text is <em>not</em> collapsed for <code>NULL</code>.
</p>
<p>Grouping data specifies variables to collapse across in the same way as
<code>collapse</code> but you <strong>cannot</strong> use both the <code>collapse</code> argument and
grouped data. Collapsing applies mostly to <code>token</code> options of &quot;ngrams&quot;,
&quot;skip_ngrams&quot;, &quot;sentences&quot;, &quot;lines&quot;, &quot;paragraphs&quot;, or &quot;regex&quot;.</p>
</td></tr>
<tr><td><code id="unnest_tokens_+3A_...">...</code></td>
<td>
<p>Extra arguments passed on to <a href="tokenizers.html#topic+tokenizers">tokenizers</a>, such
as <code>strip_punct</code> for &quot;words&quot;, <code>n</code> and <code>k</code> for &quot;ngrams&quot; and &quot;skip_ngrams&quot;,
and <code>pattern</code> for &quot;regex&quot;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If format is anything other than &quot;text&quot;, this uses the
<code><a href="hunspell.html#topic+hunspell">hunspell::hunspell_parse()</a></code> tokenizer instead of the tokenizers package.
This does not yet have support for tokenizing by any unit other than words.
</p>
<p>Support for <code>token = "tweets"</code> was removed in tidytext 0.4.0 because of
changes in upstream dependencies.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

library(dplyr)
library(janeaustenr)

d &lt;- tibble(txt = prideprejudice)
d

d %&gt;%
  unnest_tokens(word, txt)

d %&gt;%
  unnest_tokens(sentence, txt, token = "sentences")

d %&gt;%
  unnest_tokens(ngram, txt, token = "ngrams", n = 2)

d %&gt;%
  unnest_tokens(chapter, txt, token = "regex", pattern = "Chapter [\\\\d]")

d %&gt;%
  unnest_tokens(shingle, txt, token = "character_shingles", n = 4)

# custom function
d %&gt;%
  unnest_tokens(word, txt, token = stringr::str_split, pattern = " ")

# tokenize HTML
h &lt;- tibble(row = 1:2,
                text = c("&lt;h1&gt;Text &lt;b&gt;is&lt;/b&gt;", "&lt;a href='example.com'&gt;here&lt;/a&gt;"))

h %&gt;%
  unnest_tokens(word, text, format = "html")

</code></pre>

<hr>
<h2 id='unnest_tweets'>Wrapper around unnest_tokens for tweets</h2><span id='topic+unnest_tweets'></span>

<h3>Description</h3>

<p><a href="https://lifecycle.r-lib.org/articles/stages.html#deprecated"><img src="../help/figures/lifecycle-deprecated.svg" alt='[Deprecated]' /></a>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>unnest_tweets(tbl, output, input, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="unnest_tweets_+3A_tbl">tbl</code></td>
<td>
<p>A data frame</p>
</td></tr>
<tr><td><code id="unnest_tweets_+3A_output">output</code></td>
<td>
<p>Output column to be created as string or symbol.</p>
</td></tr>
<tr><td><code id="unnest_tweets_+3A_input">input</code></td>
<td>
<p>Input column that gets split as string or symbol.
</p>
<p>The output/input arguments are passed by expression and support
<a href="rlang.html#topic+topic-inject">quasiquotation</a>; you can unquote strings and symbols.</p>
</td></tr>
<tr><td><code id="unnest_tweets_+3A_...">...</code></td>
<td>
<p>Extra arguments passed on to <a href="tokenizers.html#topic+tokenizers">tokenizers</a></p>
</td></tr>
</table>


<h3>See Also</h3>


<ul>
<li> <p><code><a href="#topic+unnest_tokens">unnest_tokens()</a></code>
</p>
</li></ul>


</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
