<!DOCTYPE html><html><head><title>Help for package mtlgmm</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {mtlgmm}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#alignment'><p>Align the initializations.</p></a></li>
<li><a href='#alignment_swap'><p>Complete the alignment of initializations based on the output of function <code>alignment_swap</code>.</p></a></li>
<li><a href='#data_generation'><p>Generate data for simulations.</p></a></li>
<li><a href='#estimation_error'><p>Caluclate the estimation error of GMM parameters under the MTL setting (the worst performance among all tasks).</p></a></li>
<li><a href='#initialize'><p>Initialize the estimators of GMM parameters on each task.</p></a></li>
<li><a href='#misclustering_error'><p>Calculate the misclustering error given the predicted cluster labels.</p></a></li>
<li><a href='#mtlgmm'><p>Fit binary Gaussian mixture models (GMMs) on multiple data sets under a multi-task learning (MTL) setting.</p></a></li>
<li><a href='#predict_gmm'><p>Clustering new observations based on fitted GMM estimators.</p></a></li>
<li><a href='#tlgmm'><p>Fit the binary Gaussian mixture model (GMM) on target data set by leveraging multiple source data sets under a transfer learning (TL) setting.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Unsupervised Multi-Task and Transfer Learning on Gaussian
Mixture Models</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.0</td>
</tr>
<tr>
<td>Description:</td>
<td>Unsupervised learning has been widely used in many real-world applications. One of the simplest and most important unsupervised learning models is the Gaussian mixture model (GMM). In this work, we study the multi-task learning problem on GMMs, which aims to leverage potentially similar GMM parameter structures among tasks to obtain improved learning performance compared to single-task learning. We propose a multi-task GMM learning procedure based on the Expectation-Maximization (EM) algorithm that not only can effectively utilize unknown similarity between related tasks but is also robust against a fraction of outlier tasks from arbitrary sources. The proposed procedure is shown to achieve minimax optimal rate of convergence for both parameter estimation error and the excess mis-clustering error, in a wide range of regimes. Moreover, we generalize our approach to tackle the problem of transfer learning for GMMs, where similar theoretical results are derived. Finally, we demonstrate the effectiveness of our methods through simulations and a real data analysis. To the best of our knowledge, this is the first work studying multi-task and transfer learning on GMMs with theoretical guarantees. This package implements the algorithms proposed in Tian, Y., Weng, H., &amp; Feng, Y. (2022) &lt;<a href="https://doi.org/10.48550/arXiv.2209.15224">doi:10.48550/arXiv.2209.15224</a>&gt;.</td>
</tr>
<tr>
<td>Imports:</td>
<td>doParallel, foreach, caret, mclust, stats</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.2</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-10-30 02:29:48 UTC; yetian</td>
</tr>
<tr>
<td>Author:</td>
<td>Ye Tian [aut, cre],
  Haolei Weng [aut],
  Yang Feng [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Ye Tian &lt;ye.t@columbia.edu&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-10-31 14:17:37 UTC</td>
</tr>
</table>
<hr>
<h2 id='alignment'>Align the initializations.</h2><span id='topic+alignment'></span>

<h3>Description</h3>

<p>Align the initializations. This function implements the two alignment algorithms (Algorithms 2 and 3) in Tian, Y., Weng, H., &amp; Feng, Y. (2022). This function is mainly for people to align the single-task initializations manually. The alignment procedure has been automatically implemented in function <code>mtlgmm</code> and <code>tlgmm</code>. So there is no need to call this function when fitting MTL-GMM or TL-GMM.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>alignment(mu1, mu2, method = c("exhaustive", "greedy"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="alignment_+3A_mu1">mu1</code></td>
<td>
<p>the initializations for mu1 of all tasks. Should be a matrix of which each column is a mu1 estimate of a task.</p>
</td></tr>
<tr><td><code id="alignment_+3A_mu2">mu2</code></td>
<td>
<p>the initializations for mu2 of all tasks. Should be a matrix of which each column is a mu2 estimate of a task.</p>
</td></tr>
<tr><td><code id="alignment_+3A_method">method</code></td>
<td>
<p>alignment method. Can be either &quot;exhaustive&quot; (Algorithm 2 in Tian, Y., Weng, H., &amp; Feng, Y. (2022)) or &quot;greedy&quot; (Algorithm 3 in Tian, Y., Weng, H., &amp; Feng, Y. (2022)). Default: &quot;exhaustive&quot;</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the index of two clusters to become well-aligned, i.e. the &quot;r_k&quot; in Section 2.4.2 of Tian, Y., Weng, H., &amp; Feng, Y. (2022). The output can be passed to function <code><a href="#topic+alignment_swap">alignment_swap</a></code> to obtain the well-aligned intializations.
</p>


<h3>Note</h3>

<p>For examples, see part &quot;fit signle-task GMMs&quot; of examples in function <code><a href="#topic+mtlgmm">mtlgmm</a></code>.
</p>


<h3>References</h3>

<p>Tian, Y., Weng, H., &amp; Feng, Y. (2022). Unsupervised Multi-task and Transfer Learning on Gaussian Mixture Models. arXiv preprint arXiv:2209.15224.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mtlgmm">mtlgmm</a></code>, <code><a href="#topic+tlgmm">tlgmm</a></code>, <code><a href="#topic+predict_gmm">predict_gmm</a></code>, <code><a href="#topic+data_generation">data_generation</a></code>, <code><a href="#topic+initialize">initialize</a></code>, <code><a href="#topic+alignment_swap">alignment_swap</a></code>, <code><a href="#topic+estimation_error">estimation_error</a></code>, <code><a href="#topic+misclustering_error">misclustering_error</a></code>.
</p>

<hr>
<h2 id='alignment_swap'>Complete the alignment of initializations based on the output of function <code><a href="#topic+alignment_swap">alignment_swap</a></code>.</h2><span id='topic+alignment_swap'></span>

<h3>Description</h3>

<p>Complete the alignment of initializations based on the output of function <code><a href="#topic+alignment_swap">alignment_swap</a></code>. This function is mainly for people to align the single-task initializations manually. The alignment procedure has been automatically implemented in function <code>mtlgmm</code> and <code>tlgmm</code>. So there is no need to call this function when fitting MTL-GMM or TL-GMM.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>alignment_swap(L1, L2, initial_value_list)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="alignment_swap_+3A_l1">L1</code></td>
<td>
<p>the component &quot;L1&quot; of the output from function <code><a href="#topic+alignment_swap">alignment_swap</a></code></p>
</td></tr>
<tr><td><code id="alignment_swap_+3A_l2">L2</code></td>
<td>
<p>the component &quot;L2&quot; of the output from function <code><a href="#topic+alignment_swap">alignment_swap</a></code></p>
</td></tr>
<tr><td><code id="alignment_swap_+3A_initial_value_list">initial_value_list</code></td>
<td>
<p>the output from function <code><a href="#topic+initialize">initialize</a></code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with the following components (well-aligned).
</p>
<table>
<tr><td><code>w</code></td>
<td>
<p>the estimate of mixture proportion in GMMs for each task. Will be a vector.</p>
</td></tr>
<tr><td><code>mu1</code></td>
<td>
<p>the estimate of Gaussian mean in the first cluster of GMMs for each task. Will be a matrix, where each column represents the estimate for a task.</p>
</td></tr>
<tr><td><code>mu2</code></td>
<td>
<p>the estimate of Gaussian mean in the second cluster of GMMs for each task. Will be a matrix, where each column represents the estimate for a task.</p>
</td></tr>
<tr><td><code>beta</code></td>
<td>
<p>the estimate of the discriminant coefficient for each task. Will be a matrix, where each column represents the estimate for a task.</p>
</td></tr>
<tr><td><code>Sigma</code></td>
<td>
<p>the estimate of the common covariance matrix for each task. Will be a list, where each component represents the estimate for a task.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>For examples, see part &quot;fit signle-task GMMs&quot; of examples in function <code><a href="#topic+mtlgmm">mtlgmm</a></code>.
</p>


<h3>References</h3>

<p>Tian, Y., Weng, H., &amp; Feng, Y. (2022). Unsupervised Multi-task and Transfer Learning on Gaussian Mixture Models. arXiv preprint arXiv:2209.15224.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mtlgmm">mtlgmm</a></code>, <code><a href="#topic+tlgmm">tlgmm</a></code>, <code><a href="#topic+predict_gmm">predict_gmm</a></code>, <code><a href="#topic+data_generation">data_generation</a></code>, <code><a href="#topic+initialize">initialize</a></code>, <code><a href="#topic+alignment">alignment</a></code>, <code><a href="#topic+estimation_error">estimation_error</a></code>, <code><a href="#topic+misclustering_error">misclustering_error</a></code>.
</p>

<hr>
<h2 id='data_generation'>Generate data for simulations.</h2><span id='topic+data_generation'></span>

<h3>Description</h3>

<p>Generate data for simulations. All models used in Tian, Y., Weng, H., &amp; Feng, Y. (2022)) are implemented.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data_generation(
  K = 10,
  outlier_K = 1,
  simulation_no = c("MTL-1", "MTL-2"),
  h_w = 0.1,
  h_mu = 1,
  n = 50
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="data_generation_+3A_k">K</code></td>
<td>
<p>the number of tasks (data sets). Default: 10</p>
</td></tr>
<tr><td><code id="data_generation_+3A_outlier_k">outlier_K</code></td>
<td>
<p>the number of outlier tasks. Default: 1</p>
</td></tr>
<tr><td><code id="data_generation_+3A_simulation_no">simulation_no</code></td>
<td>
<p>simulation number in Tian, Y., Weng, H., &amp; Feng, Y. (2022)). Can be &quot;MTL-1&quot;, &quot;MTL-2&quot;. Default = &quot;MTL-1&quot;.</p>
</td></tr>
<tr><td><code id="data_generation_+3A_h_w">h_w</code></td>
<td>
<p>the value of h_w. Default: 0.1</p>
</td></tr>
<tr><td><code id="data_generation_+3A_h_mu">h_mu</code></td>
<td>
<p>the value of h_mu. Default: 1</p>
</td></tr>
<tr><td><code id="data_generation_+3A_n">n</code></td>
<td>
<p>the sample size of each task. Can be either an positive integer or a vector of length <code>K</code>. If it is an integer, then the sample size of all tasks will be the same and equal to <code>n</code>. If it is a vector, then the k-th number will be the sample size of the k-th task. Default: 50.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list of two sub-lists &quot;data&quot; and &quot;parameter&quot;. List &quot;data&quot; contains a list of design matrices <code>x</code>, a list of hidden labels <code>y</code>, and a vector of outlier task indices <code>outlier_index</code>. List &quot;parameter&quot; contains a vector <code>w</code> of mixture proportions, a matrix <code>mu1</code> of which each column is the GMM mean of the first cluster of each task, a matrix <code>mu2</code> of which each column is the GMM mean of the second cluster of each task, a matrix <code>beta</code> of which each column is the discriminant coefficient in each task, a list <code>Sigma</code> of covariance matrices for each task.
</p>


<h3>References</h3>

<p>Tian, Y., Weng, H., &amp; Feng, Y. (2022). Unsupervised Multi-task and Transfer Learning on Gaussian Mixture Models. arXiv preprint arXiv:2209.15224.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mtlgmm">mtlgmm</a></code>, <code><a href="#topic+tlgmm">tlgmm</a></code>, <code><a href="#topic+predict_gmm">predict_gmm</a></code>, <code><a href="#topic+initialize">initialize</a></code>, <code><a href="#topic+alignment">alignment</a></code>, <code><a href="#topic+alignment_swap">alignment_swap</a></code>, <code><a href="#topic+estimation_error">estimation_error</a></code>, <code><a href="#topic+misclustering_error">misclustering_error</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data_list &lt;- data_generation(K = 5, outlier_K = 1, simulation_no = "MTL-1", h_w = 0.1,
h_mu = 1, n = 50)
</code></pre>

<hr>
<h2 id='estimation_error'>Caluclate the estimation error of GMM parameters under the MTL setting (the worst performance among all tasks).</h2><span id='topic+estimation_error'></span>

<h3>Description</h3>

<p>Caluclate the estimation error of GMM parameters under the MTL setting (the worst performance among all tasks). Euclidean norms are used.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>estimation_error(
  estimated_value,
  true_value,
  parameter = c("w", "mu", "beta", "Sigma")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="estimation_error_+3A_estimated_value">estimated_value</code></td>
<td>
<p>estimate of GMM parameters. The form of input depends on the parameter <code>parameter</code>.</p>
</td></tr>
<tr><td><code id="estimation_error_+3A_true_value">true_value</code></td>
<td>
<p>true values of GMM parameters. The form of input depends on the parameter <code>parameter</code>.</p>
</td></tr>
<tr><td><code id="estimation_error_+3A_parameter">parameter</code></td>
<td>
<p>which parameter to calculate the estimation error for. Can be &quot;w&quot;, &quot;mu&quot;, &quot;beta&quot;, or &quot;Sigma&quot;.
</p>

<ul>
<li><p> w: the Gaussian mixture proportions. Both <code>estimated_value</code> and <code>true_value</code> require an input of a K-dimensional vector, where K is the number of tasks. Each element in the vector is an &quot;w&quot; (estimate or true value) for each task.
</p>
</li>
<li><p> mu: Gaussian mean parameters. Both <code>estimated_value</code> and <code>true_value</code> require an input of a list of two p-by-K matrices, where p is the dimension of Gaussian distribution and K is the number of tasks. Each column of the matrix is a &quot;mu1&quot; or &quot;mu2&quot; (estimate or true value) for each task.
</p>
</li>
<li><p> beta: discriminant coefficients. Both <code>estimated_value</code> and <code>true_value</code> require an input of a p-by-K matrix, where p is the dimension of Gaussian distribution and K is the number of tasks. Each column of the matrix is a &quot;beta&quot; (estimate or true value) for each task.
</p>
</li>
<li><p> Sigma: Gaussian covariance matrices. Both <code>estimated_value</code> and <code>true_value</code> require an input of a list of K p-by-p matrices, where p is the dimension of Gaussian distribution and K is the number of tasks. Each matrix in the list is a &quot;Sigma&quot; (estimate or true value) for each task.
</p>
</li></ul>
</td></tr>
</table>


<h3>Value</h3>

<p>the largest estimation error among all tasks.
</p>


<h3>Note</h3>

<p>For examples, see examples in function <code><a href="#topic+mtlgmm">mtlgmm</a></code>.
</p>


<h3>References</h3>

<p>Tian, Y., Weng, H., &amp; Feng, Y. (2022). Unsupervised Multi-task and Transfer Learning on Gaussian Mixture Models. arXiv preprint arXiv:2209.15224.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mtlgmm">mtlgmm</a></code>, <code><a href="#topic+tlgmm">tlgmm</a></code>, <code><a href="#topic+predict_gmm">predict_gmm</a></code>, <code><a href="#topic+data_generation">data_generation</a></code>, <code><a href="#topic+initialize">initialize</a></code>, <code><a href="#topic+alignment">alignment</a></code>, <code><a href="#topic+alignment_swap">alignment_swap</a></code>, <code><a href="#topic+misclustering_error">misclustering_error</a></code>.
</p>

<hr>
<h2 id='initialize'>Initialize the estimators of GMM parameters on each task.</h2><span id='topic+initialize'></span>

<h3>Description</h3>

<p>Initialize the estimators of GMM parameters on each task.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>initialize(x, method = c("kmeans", "EM"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="initialize_+3A_x">x</code></td>
<td>
<p>design matrices from multiple data sets. Should be a list, of which each component is a <code>matrix</code> or <code>data.frame</code> object, representing the design matrix from each task.</p>
</td></tr>
<tr><td><code id="initialize_+3A_method">method</code></td>
<td>
<p>initialization method. This indicates the method to initialize the estimates of GMM parameters for each data set. Can be either &quot;EM&quot; or &quot;kmeans&quot;. Default: &quot;EM&quot;.
</p>

<ul>
<li><p> EM: the initial estimates of GMM parameters will be generated from the single-task EM algorithm. Will call <code><a href="mclust.html#topic+Mclust">Mclust</a></code> function in <code>mclust</code> package.
</p>
</li>
<li><p> kmeans: the initial estimates of GMM parameters will be generated from the single-task k-means algorithm. Will call <code><a href="stats.html#topic+kmeans">kmeans</a></code> function in <code>stats</code> package.
</p>
</li></ul>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with the following components.
</p>
<table>
<tr><td><code>w</code></td>
<td>
<p>the estimate of mixture proportion in GMMs for each task. Will be a vector.</p>
</td></tr>
<tr><td><code>mu1</code></td>
<td>
<p>the estimate of Gaussian mean in the first cluster of GMMs for each task. Will be a matrix, where each column represents the estimate for a task.</p>
</td></tr>
<tr><td><code>mu2</code></td>
<td>
<p>the estimate of Gaussian mean in the second cluster of GMMs for each task. Will be a matrix, where each column represents the estimate for a task.</p>
</td></tr>
<tr><td><code>beta</code></td>
<td>
<p>the estimate of the discriminant coefficient for each task. Will be a matrix, where each column represents the estimate for a task.</p>
</td></tr>
<tr><td><code>Sigma</code></td>
<td>
<p>the estimate of the common covariance matrix for each task. Will be a list, where each component represents the estimate for a task.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+mtlgmm">mtlgmm</a></code>, <code><a href="#topic+tlgmm">tlgmm</a></code>, <code><a href="#topic+predict_gmm">predict_gmm</a></code>, <code><a href="#topic+data_generation">data_generation</a></code>, <code><a href="#topic+alignment">alignment</a></code>, <code><a href="#topic+alignment_swap">alignment_swap</a></code>, <code><a href="#topic+estimation_error">estimation_error</a></code>, <code><a href="#topic+misclustering_error">misclustering_error</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(0, kind = "L'Ecuyer-CMRG")
## Consider a 5-task multi-task learning problem in the setting "MTL-1"
data_list &lt;- data_generation(K = 5, outlier_K = 1, simulation_no = "MTL-1", h_w = 0.1,
h_mu = 1, n = 50)  # generate the data
fit &lt;- mtlgmm(x = data_list$data$x, C1_w = 0.05, C1_mu = 0.2, C1_beta = 0.2,
C2_w = 0.05, C2_mu = 0.2, C2_beta = 0.2, kappa = 1/3, initial_method = "EM",
trim = 0.1, lambda_choice = "fixed", step_size = "lipschitz")

## Initialize the estimators of GMM parameters on each task.
fitted_values_EM &lt;- initialize(data_list$data$x,
"EM")  # initilize the estimates by single-task EM algorithm
fitted_values_kmeans &lt;- initialize(data_list$data$x,
"EM")  # initilize the estimates by single-task k-means
</code></pre>

<hr>
<h2 id='misclustering_error'>Calculate the misclustering error given the predicted cluster labels.</h2><span id='topic+misclustering_error'></span>

<h3>Description</h3>

<p>Calculate the misclustering error given the predicted cluster labels.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>misclustering_error(y_pred, y_test, type = c("max", "all", "avg"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="misclustering_error_+3A_y_pred">y_pred</code></td>
<td>
<p>predicted cluster labels</p>
</td></tr>
<tr><td><code id="misclustering_error_+3A_y_test">y_test</code></td>
<td>
<p>true cluster labels</p>
</td></tr>
<tr><td><code id="misclustering_error_+3A_type">type</code></td>
<td>
<p>which type of the misclustering error rate to return. Can be either &quot;max&quot;, &quot;all&quot;, or &quot;avg&quot;. Default: &quot;max&quot;.
</p>

<ul>
<li><p> max: maximum of misclustering error rates on all tasks
</p>
</li>
<li><p> all: a vector of misclustering error rates on each tasks
</p>
</li>
<li><p> avg: average of misclustering error rates on all tasks
</p>
</li></ul>
</td></tr>
</table>


<h3>Value</h3>

<p>Depends on <code>type</code>.
</p>


<h3>References</h3>

<p>Tian, Y., Weng, H., &amp; Feng, Y. (2022). Unsupervised Multi-task and Transfer Learning on Gaussian Mixture Models. arXiv preprint arXiv:2209.15224.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mtlgmm">mtlgmm</a></code>, <code><a href="#topic+tlgmm">tlgmm</a></code>, <code><a href="#topic+data_generation">data_generation</a></code>, <code><a href="#topic+predict_gmm">predict_gmm</a></code>, <code><a href="#topic+initialize">initialize</a></code>, <code><a href="#topic+alignment">alignment</a></code>, <code><a href="#topic+alignment_swap">alignment_swap</a></code>, <code><a href="#topic+estimation_error">estimation_error</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(23, kind = "L'Ecuyer-CMRG")
## Consider a 5-task multi-task learning problem in the setting "MTL-1"
data_list &lt;- data_generation(K = 5, outlier_K = 1, simulation_no = "MTL-1", h_w = 0.1,
h_mu = 1, n = 100)  # generate the data
x_train &lt;- sapply(1:length(data_list$data$x), function(k){
  data_list$data$x[[k]][1:50,]
}, simplify = FALSE)
x_test &lt;- sapply(1:length(data_list$data$x), function(k){
  data_list$data$x[[k]][-(1:50),]
}, simplify = FALSE)
y_test &lt;- sapply(1:length(data_list$data$x), function(k){
  data_list$data$y[[k]][-(1:50)]
}, simplify = FALSE)

fit &lt;- mtlgmm(x = x_train, C1_w = 0.05, C1_mu = 0.2, C1_beta = 0.2,
C2_w = 0.05, C2_mu = 0.2, C2_beta = 0.2, kappa = 1/3, initial_method = "EM",
trim = 0.1, lambda_choice = "fixed", step_size = "lipschitz")

y_pred &lt;- sapply(1:length(data_list$data$x), function(i){
predict_gmm(w = fit$w[i], mu1 = fit$mu1[, i], mu2 = fit$mu2[, i],
beta = fit$beta[, i], newx = x_test[[i]])
}, simplify = FALSE)
misclustering_error(y_pred[-data_list$data$outlier_index],
y_test[-data_list$data$outlier_index], type = "max")
</code></pre>

<hr>
<h2 id='mtlgmm'>Fit binary Gaussian mixture models (GMMs) on multiple data sets under a multi-task learning (MTL) setting.</h2><span id='topic+mtlgmm'></span>

<h3>Description</h3>

<p>it binary Gaussian mixture models (GMMs) on multiple data sets under a multi-task learning (MTL) setting. This function implements the modified EM algorithm (Altorithm 1) proposed in Tian, Y., Weng, H., &amp; Feng, Y. (2022).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mtlgmm(
  x,
  step_size = c("lipschitz", "fixed"),
  eta_w = 0.1,
  eta_mu = 0.1,
  eta_beta = 0.1,
  lambda_choice = c("cv", "fixed"),
  cv_nfolds = 5,
  cv_upper = 5,
  cv_lower = 0.01,
  cv_length = 5,
  C1_w = 0.05,
  C1_mu = 0.2,
  C1_beta = 0.2,
  C2_w = 0.05,
  C2_mu = 0.2,
  C2_beta = 0.2,
  kappa = 1/3,
  tol = 1e-05,
  initial_method = c("EM", "kmeans"),
  alignment_method = ifelse(length(x) &lt;= 10, "exhaustive", "greedy"),
  trim = 0.1,
  iter_max = 1000,
  iter_max_prox = 100,
  ncores = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mtlgmm_+3A_x">x</code></td>
<td>
<p>design matrices from multiple data sets. Should be a list, of which each component is a <code>matrix</code> or <code>data.frame</code> object, representing the design matrix from each task.</p>
</td></tr>
<tr><td><code id="mtlgmm_+3A_step_size">step_size</code></td>
<td>
<p>step size choice in proximal gradient method to solve each optimization problem in the revised EM algorithm (Algorithm 1 in Tian, Y., Weng, H., &amp; Feng, Y. (2022)), which can be either &quot;lipschitz&quot; or &quot;fixed&quot;. Default = &quot;lipschitz&quot;.
</p>

<ul>
<li><p> lipschitz: <code>eta_w</code>, <code>eta_mu</code> and <code>eta_beta</code> will be chosen by the Lipschitz property of the gradient of objective function (without the penalty part). See Section 4.2 of Parikh, N., &amp; Boyd, S. (2014).
</p>
</li>
<li><p> fixed: <code>eta_w</code>, <code>eta_mu</code> and <code>eta_beta</code> need to be specified
</p>
</li></ul>
</td></tr>
<tr><td><code id="mtlgmm_+3A_eta_w">eta_w</code></td>
<td>
<p>step size in the proximal gradient method to learn w (Step 3 of Algorithm 1 in Tian, Y., Weng, H., &amp; Feng, Y. (2022)). Default: 0.1. Only used when <code>step_size</code> = &quot;fixed&quot;.</p>
</td></tr>
<tr><td><code id="mtlgmm_+3A_eta_mu">eta_mu</code></td>
<td>
<p>step size in the proximal gradient method to learn mu (Steps 4 and 5 of Algorithm 1 in Tian, Y., Weng, H., &amp; Feng, Y. (2022)). Default: 0.1. Only used when <code>step_size</code> = &quot;fixed&quot;.</p>
</td></tr>
<tr><td><code id="mtlgmm_+3A_eta_beta">eta_beta</code></td>
<td>
<p>step size in the proximal gradient method to learn beta (Step 9 of Algorithm 1 in Tian, Y., Weng, H., &amp; Feng, Y. (2022)). Default: 0.1. Only used when <code>step_size</code> = &quot;fixed&quot;.</p>
</td></tr>
<tr><td><code id="mtlgmm_+3A_lambda_choice">lambda_choice</code></td>
<td>
<p>the choice of constants in the penalty parameter used in the optimization problems. See Algorithm 1 of Tian, Y., Weng, H., &amp; Feng, Y. (2022), which can be either &quot;fixed&quot; or &quot;cv&quot;. Default: &quot;cv&quot;.
</p>

<ul>
<li><p> cv: <code>cv_nfolds</code>, <code>cv_upper</code>, and <code>cv_length</code> need to be specified. Then the C1 and C2 parameters will be chosen in all combinations in <code>exp(seq(log(cv_lower/10), log(cv_upper/10), length.out = cv_length))</code> via cross-validation. Note that this is a two-dimensional cv process, because we set <code>C1_w</code> = <code>C2_w</code>, <code>C1_mu</code> = <code>C1_beta</code> = <code>C2_mu</code> = <code>C2_beta</code> to reduce the computational cost.
</p>
</li>
<li><p> fixed: <code>C1_w</code>, <code>C1_mu</code>, <code>C1_beta</code>, <code>C2_w</code>, <code>C2_mu</code>, and <code>C2_beta</code> need to be specified. See equations (7)-(12) in Tian, Y., Weng, H., &amp; Feng, Y. (2022).
</p>
</li></ul>
</td></tr>
<tr><td><code id="mtlgmm_+3A_cv_nfolds">cv_nfolds</code></td>
<td>
<p>the number of cross-validation folds. Default: 5</p>
</td></tr>
<tr><td><code id="mtlgmm_+3A_cv_upper">cv_upper</code></td>
<td>
<p>the upper bound of <code>lambda</code> values used in cross-validation. Default: 5</p>
</td></tr>
<tr><td><code id="mtlgmm_+3A_cv_lower">cv_lower</code></td>
<td>
<p>the lower bound of <code>lambda</code> values used in cross-validation. Default: 0.01</p>
</td></tr>
<tr><td><code id="mtlgmm_+3A_cv_length">cv_length</code></td>
<td>
<p>the number of <code>lambda</code> values considered in cross-validation. Default: 5</p>
</td></tr>
<tr><td><code id="mtlgmm_+3A_c1_w">C1_w</code></td>
<td>
<p>the initial value of C1_w. See equations (7) in Tian, Y., Weng, H., &amp; Feng, Y. (2022). Default: 0.05</p>
</td></tr>
<tr><td><code id="mtlgmm_+3A_c1_mu">C1_mu</code></td>
<td>
<p>the initial value of C1_mu. See equations (8) in Tian, Y., Weng, H., &amp; Feng, Y. (2022). Default: 0.2</p>
</td></tr>
<tr><td><code id="mtlgmm_+3A_c1_beta">C1_beta</code></td>
<td>
<p>the initial value of C1_beta. See equations (9) in Tian, Y., Weng, H., &amp; Feng, Y. (2022). Default: 0.2</p>
</td></tr>
<tr><td><code id="mtlgmm_+3A_c2_w">C2_w</code></td>
<td>
<p>the initial value of C2_w. See equations (10) in Tian, Y., Weng, H., &amp; Feng, Y. (2022). Default: 0.05</p>
</td></tr>
<tr><td><code id="mtlgmm_+3A_c2_mu">C2_mu</code></td>
<td>
<p>the initial value of C2_mu. See equations (11) in Tian, Y., Weng, H., &amp; Feng, Y. (2022). Default: 0.2</p>
</td></tr>
<tr><td><code id="mtlgmm_+3A_c2_beta">C2_beta</code></td>
<td>
<p>the initial value of C2_beta. See equations (12) in Tian, Y., Weng, H., &amp; Feng, Y. (2022). Default: 0.2</p>
</td></tr>
<tr><td><code id="mtlgmm_+3A_kappa">kappa</code></td>
<td>
<p>the decaying rate used in equation (7)-(12) in Tian, Y., Weng, H., &amp; Feng, Y. (2022). Default: 1/3</p>
</td></tr>
<tr><td><code id="mtlgmm_+3A_tol">tol</code></td>
<td>
<p>maximum tolerance in all optimization problems. If the difference between last update and the current update is less than this value, the iterations of optimization will stop. Default: 1e-05</p>
</td></tr>
<tr><td><code id="mtlgmm_+3A_initial_method">initial_method</code></td>
<td>
<p>initialization method. This indicates the method to initialize the estimates of GMM parameters for each data set. Can be either &quot;EM&quot; or &quot;kmeans&quot;. Default: &quot;EM&quot;.
</p>

<ul>
<li><p> EM: the initial estimates of GMM parameters will be generated from the single-task EM algorithm. Will call <code><a href="mclust.html#topic+Mclust">Mclust</a></code> function in <code>mclust</code> package.
</p>
</li>
<li><p> kmeans: the initial estimates of GMM parameters will be generated from the single-task k-means algorithm. Will call <code><a href="stats.html#topic+kmeans">kmeans</a></code> function in <code>stats</code> package.
</p>
</li></ul>
</td></tr>
<tr><td><code id="mtlgmm_+3A_alignment_method">alignment_method</code></td>
<td>
<p>the alignment algorithm to use. See Section 2.4 of Tian, Y., Weng, H., &amp; Feng, Y. (2022). Can either be &quot;exhaustive&quot; or &quot;greedy&quot;. Default: when <code>length(x)</code> &lt;= 10, &quot;exhaustive&quot; will be used, otherwise &quot;greedy&quot; will be used.
</p>

<ul>
<li><p> exhaustive: exhaustive search algorithm (Algorithm 2 in Tian, Y., Weng, H., &amp; Feng, Y. (2022)) will be used.
</p>
</li>
<li><p> greedy: greey label swapping algorithm (Algorithm 3 in Tian, Y., Weng, H., &amp; Feng, Y. (2022)) will be used.
</p>
</li></ul>
</td></tr>
<tr><td><code id="mtlgmm_+3A_trim">trim</code></td>
<td>
<p>the proportion of trimmed data sets in the cross-validation procedure of choosing tuning parameters. Setting it to a non-zero small value can help avoid the impact of outlier tasks on the choice of tuning parameters. Default: 0.1</p>
</td></tr>
<tr><td><code id="mtlgmm_+3A_iter_max">iter_max</code></td>
<td>
<p>the maximum iteration number of the revised EM algorithm (i.e. the parameter T in Algorithm 1 in Tian, Y., Weng, H., &amp; Feng, Y. (2022)). Default: 1000</p>
</td></tr>
<tr><td><code id="mtlgmm_+3A_iter_max_prox">iter_max_prox</code></td>
<td>
<p>the maximum iteration number of the proximal gradient method. Default: 100</p>
</td></tr>
<tr><td><code id="mtlgmm_+3A_ncores">ncores</code></td>
<td>
<p>the number of cores to use. Parallel computing is strongly suggested, specially when <code>lambda_choice</code> = &quot;cv&quot;. Default: 1</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with the following components.
</p>
<table>
<tr><td><code>w</code></td>
<td>
<p>the estimate of mixture proportion in GMMs for each task. Will be a vector.</p>
</td></tr>
<tr><td><code>mu1</code></td>
<td>
<p>the estimate of Gaussian mean in the first cluster of GMMs for each task. Will be a matrix, where each column represents the estimate for a task.</p>
</td></tr>
<tr><td><code>mu2</code></td>
<td>
<p>the estimate of Gaussian mean in the second cluster of GMMs for each task. Will be a matrix, where each column represents the estimate for a task.</p>
</td></tr>
<tr><td><code>beta</code></td>
<td>
<p>the estimate of the discriminant coefficient for each task. Will be a matrix, where each column represents the estimate for a task.</p>
</td></tr>
<tr><td><code>Sigma</code></td>
<td>
<p>the estimate of the common covariance matrix for each task. Will be a list, where each component represents the estimate for a task.</p>
</td></tr>
<tr><td><code>w_bar</code></td>
<td>
<p>the center estimate of w. Numeric. See Algorithm 1 in Tian, Y., Weng, H., &amp; Feng, Y. (2022). </p>
</td></tr>
<tr><td><code>mu1_bar</code></td>
<td>
<p>the center estimate of mu1. Will be a vector. See Algorithm 1 in Tian, Y., Weng, H., &amp; Feng, Y. (2022).</p>
</td></tr>
<tr><td><code>mu2_bar</code></td>
<td>
<p>the center estimate of mu2. Will be a vector. See Algorithm 1 in Tian, Y., Weng, H., &amp; Feng, Y. (2022).</p>
</td></tr>
<tr><td><code>beta_bar</code></td>
<td>
<p>the center estimate of beta. Will be a vector. See Algorithm 1 in Tian, Y., Weng, H., &amp; Feng, Y. (2022).</p>
</td></tr>
<tr><td><code>C1_w</code></td>
<td>
<p>the initial value of C1_w.</p>
</td></tr>
<tr><td><code>C1_mu</code></td>
<td>
<p>the initial value of C1_mu.</p>
</td></tr>
<tr><td><code>C1_beta</code></td>
<td>
<p>the initial value of C1_beta.</p>
</td></tr>
<tr><td><code>C2_w</code></td>
<td>
<p>the initial value of C2_w.</p>
</td></tr>
<tr><td><code>C2_mu</code></td>
<td>
<p>the initial value of C2_mu.</p>
</td></tr>
<tr><td><code>C2_beta</code></td>
<td>
<p>the initial value of C2_beta.</p>
</td></tr>
<tr><td><code>initial_mu1</code></td>
<td>
<p>the well-aligned initial estimate of mu1 of different tasks. Useful for the alignment problem in transfer learning. See Section 3.4 in Tian, Y., Weng, H., &amp; Feng, Y. (2022).</p>
</td></tr>
<tr><td><code>initial_mu2</code></td>
<td>
<p>the well-aligned initial estimate of mu2 of different tasks. Useful for the alignment problem in transfer learning. See Section 3.4 in Tian, Y., Weng, H., &amp; Feng, Y. (2022).</p>
</td></tr>
</table>


<h3>References</h3>

<p>Tian, Y., Weng, H., &amp; Feng, Y. (2022). Unsupervised Multi-task and Transfer Learning on Gaussian Mixture Models. arXiv preprint arXiv:2209.15224.
</p>
<p>Parikh, N., &amp; Boyd, S. (2014). Proximal algorithms. Foundations and trends in Optimization, 1(3), 127-239.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+tlgmm">tlgmm</a></code>, <code><a href="#topic+predict_gmm">predict_gmm</a></code>, <code><a href="#topic+data_generation">data_generation</a></code>, <code><a href="#topic+initialize">initialize</a></code>, <code><a href="#topic+alignment">alignment</a></code>, <code><a href="#topic+alignment_swap">alignment_swap</a></code>, <code><a href="#topic+estimation_error">estimation_error</a></code>, <code><a href="#topic+misclustering_error">misclustering_error</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(0, kind = "L'Ecuyer-CMRG")
library(mclust)
## Consider a 5-task multi-task learning problem in the setting "MTL-1"
data_list &lt;- data_generation(K = 5, outlier_K = 1, simulation_no = "MTL-1",
h_w = 0.1, h_mu = 1, n = 50)  # generate the data
fit &lt;- mtlgmm(x = data_list$data$x, C1_w = 0.05, C1_mu = 0.2, C1_beta = 0.2,
C2_w = 0.05, C2_mu = 0.2, C2_beta = 0.2, kappa = 1/3, initial_method = "EM",
trim = 0.1, lambda_choice = "fixed", step_size = "lipschitz")


## compare the performance with that of single-task estimators
# fit single-task GMMs
fitted_values &lt;- initialize(data_list$data$x, "EM")  # initilize the estimates
L &lt;- alignment(fitted_values$mu1, fitted_values$mu2,
method = "exhaustive")  # call the alignment algorithm
fitted_values &lt;- alignment_swap(L$L1, L$L2,
initial_value_list = fitted_values)  # obtain the well-aligned initial estimates

# fit a pooled GMM
x.comb &lt;- Reduce("rbind", data_list$data$x)
fit_pooled &lt;- Mclust(x.comb, G = 2, modelNames = "EEE")
fitted_values_pooled &lt;- list(w = NULL, mu1 = NULL, mu2 = NULL, beta = NULL, Sigma = NULL)
fitted_values_pooled$w &lt;- rep(fit_pooled$parameters$pro[1], length(data_list$data$x))
fitted_values_pooled$mu1 &lt;- matrix(rep(fit_pooled$parameters$mean[,1],
length(data_list$data$x)), ncol = length(data_list$data$x))
fitted_values_pooled$mu2 &lt;- matrix(rep(fit_pooled$parameters$mean[,2],
length(data_list$data$x)), ncol = length(data_list$data$x))
fitted_values_pooled$Sigma &lt;- sapply(1:length(data_list$data$x), function(k){
  fit_pooled$parameters$variance$Sigma
}, simplify = FALSE)
fitted_values_pooled$beta &lt;- sapply(1:length(data_list$data$x), function(k){
  solve(fit_pooled$parameters$variance$Sigma) %*%
  (fit_pooled$parameters$mean[,1] - fit_pooled$parameters$mean[,2])
})
error &lt;- matrix(nrow = 3, ncol = 4, dimnames = list(c("Single-task-GMM","Pooled-GMM","MTL-GMM"),
c("w", "mu", "beta", "Sigma")))
error["Single-task-GMM", "w"] &lt;- estimation_error(
fitted_values$w[-data_list$data$outlier_index],
data_list$parameter$w[-data_list$data$outlier_index], "w")
error["Pooled-GMM", "w"] &lt;- estimation_error(
fitted_values_pooled$w[-data_list$data$outlier_index],
data_list$parameter$w[-data_list$data$outlier_index], "w")
error["MTL-GMM", "w"] &lt;- estimation_error(
fit$w[-data_list$data$outlier_index],
data_list$parameter$w[-data_list$data$outlier_index], "w")

error["Single-task-GMM", "mu"] &lt;- estimation_error(
list(fitted_values$mu1[, -data_list$data$outlier_index],
fitted_values$mu2[, -data_list$data$outlier_index]),
list(data_list$parameter$mu1[, -data_list$data$outlier_index],
data_list$parameter$mu2[, -data_list$data$outlier_index]), "mu")
error["Pooled-GMM", "mu"] &lt;- estimation_error(list(
fitted_values_pooled$mu1[, -data_list$data$outlier_index],
fitted_values_pooled$mu2[, -data_list$data$outlier_index]),
list(data_list$parameter$mu1[, -data_list$data$outlier_index],
data_list$parameter$mu2[, -data_list$data$outlier_index]), "mu")
error["MTL-GMM", "mu"] &lt;- estimation_error(list(
fit$mu1[, -data_list$data$outlier_index],
fit$mu2[, -data_list$data$outlier_index]),
list(data_list$parameter$mu1[, -data_list$data$outlier_index],
data_list$parameter$mu2[, -data_list$data$outlier_index]), "mu")

error["Single-task-GMM", "beta"]  &lt;- estimation_error(
fitted_values$beta[, -data_list$data$outlier_index],
data_list$parameter$beta[, -data_list$data$outlier_index], "beta")
error["Pooled-GMM", "beta"] &lt;- estimation_error(
fitted_values_pooled$beta[, -data_list$data$outlier_index],
data_list$parameter$beta[, -data_list$data$outlier_index], "beta")
error["MTL-GMM", "beta"] &lt;- estimation_error(
fit$beta[, -data_list$data$outlier_index],
data_list$parameter$beta[, -data_list$data$outlier_index], "beta")

error["Single-task-GMM", "Sigma"] &lt;- estimation_error(
fitted_values$Sigma[-data_list$data$outlier_index],
data_list$parameter$Sigma[-data_list$data$outlier_index], "Sigma")
error["Pooled-GMM", "Sigma"] &lt;- estimation_error(
fitted_values_pooled$Sigma[-data_list$data$outlier_index],
data_list$parameter$Sigma[-data_list$data$outlier_index], "Sigma")
error["MTL-GMM", "Sigma"] &lt;- estimation_error(
fit$Sigma[-data_list$data$outlier_index],
data_list$parameter$Sigma[-data_list$data$outlier_index], "Sigma")

error


# use cross-validation to choose the tuning parameters
# warning: can be quite slow, large "ncores" input is suggested!!
fit &lt;- mtlgmm(x = data_list$data$x, kappa = 1/3, initial_method = "EM", ncores = 2, cv_length = 5,
trim = 0.1, cv_upper = 2, cv_lower = 0.01, lambda = "cv", step_size = "lipschitz")


</code></pre>

<hr>
<h2 id='predict_gmm'>Clustering new observations based on fitted GMM estimators.</h2><span id='topic+predict_gmm'></span>

<h3>Description</h3>

<p>Clustering new observations based on fitted GMM estimators, which is an empirical version of Bayes classifier. See equation (13) in Tian, Y., Weng, H., &amp; Feng, Y. (2022).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>predict_gmm(w, mu1, mu2, beta, newx)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict_gmm_+3A_w">w</code></td>
<td>
<p>the estimate of mixture proportion in the GMM. Numeric.</p>
</td></tr>
<tr><td><code id="predict_gmm_+3A_mu1">mu1</code></td>
<td>
<p>the estimate of Gaussian mean of the first cluster in the GMM. Should be a vector.</p>
</td></tr>
<tr><td><code id="predict_gmm_+3A_mu2">mu2</code></td>
<td>
<p>the estimate of Gaussian mean of the first cluster in the GMM. Should be a vector.</p>
</td></tr>
<tr><td><code id="predict_gmm_+3A_beta">beta</code></td>
<td>
<p>the estimate of the discriminant coefficient for the GMM. Should be a vector.</p>
</td></tr>
<tr><td><code id="predict_gmm_+3A_newx">newx</code></td>
<td>
<p>design matrix of new observations. Should be a matrix.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of predicted labels of new observations.
</p>


<h3>References</h3>

<p>Tian, Y., Weng, H., &amp; Feng, Y. (2022). Unsupervised Multi-task and Transfer Learning on Gaussian Mixture Models. arXiv preprint arXiv:2209.15224.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mtlgmm">mtlgmm</a></code>, <code><a href="#topic+tlgmm">tlgmm</a></code>, <code><a href="#topic+data_generation">data_generation</a></code>, <code><a href="#topic+initialize">initialize</a></code>, <code><a href="#topic+alignment">alignment</a></code>, <code><a href="#topic+alignment_swap">alignment_swap</a></code>, <code><a href="#topic+estimation_error">estimation_error</a></code>, <code><a href="#topic+misclustering_error">misclustering_error</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(23, kind = "L'Ecuyer-CMRG")
## Consider a 5-task multi-task learning problem in the setting "MTL-1"
data_list &lt;- data_generation(K = 5, outlier_K = 1, simulation_no = "MTL-1", h_w = 0.1,
h_mu = 1, n = 50)  # generate the data
x_train &lt;- sapply(1:length(data_list$data$x), function(k){
  data_list$data$x[[k]][1:50,]
}, simplify = FALSE)
x_test &lt;- sapply(1:length(data_list$data$x), function(k){
  data_list$data$x[[k]][-(1:50),]
}, simplify = FALSE)
y_test &lt;- sapply(1:length(data_list$data$x), function(k){
  data_list$data$y[[k]][-(1:50)]
}, simplify = FALSE)

fit &lt;- mtlgmm(x = x_train, C1_w = 0.05, C1_mu = 0.2, C1_beta = 0.2,
C2_w = 0.05, C2_mu = 0.2, C2_beta = 0.2, kappa = 1/3, initial_method = "EM",
trim = 0.1, lambda_choice = "fixed", step_size = "lipschitz")

y_pred &lt;- sapply(1:length(data_list$data$x), function(i){
predict_gmm(w = fit$w[i], mu1 = fit$mu1[, i], mu2 = fit$mu2[, i],
beta = fit$beta[, i], newx = x_test[[i]])
}, simplify = FALSE)
misclustering_error(y_pred[-data_list$data$outlier_index],
y_test[-data_list$data$outlier_index], type = "max")
</code></pre>

<hr>
<h2 id='tlgmm'>Fit the binary Gaussian mixture model (GMM) on target data set by leveraging multiple source data sets under a transfer learning (TL) setting.</h2><span id='topic+tlgmm'></span>

<h3>Description</h3>

<p>Fit the binary Gaussian mixture model (GMM) on target data set by leveraging multiple source data sets under a transfer learning (TL) setting. This function implements the modified EM algorithm (Altorithm 4) proposed in Tian, Y., Weng, H., &amp; Feng, Y. (2022).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tlgmm(
  x,
  fitted_bar,
  step_size = c("lipschitz", "fixed"),
  eta_w = 0.1,
  eta_mu = 0.1,
  eta_beta = 0.1,
  lambda_choice = c("fixed", "cv"),
  cv_nfolds = 5,
  cv_upper = 2,
  cv_lower = 0.01,
  cv_length = 5,
  C1_w = 0.05,
  C1_mu = 0.2,
  C1_beta = 0.2,
  C2_w = 0.05,
  C2_mu = 0.2,
  C2_beta = 0.2,
  kappa0 = 1/3,
  tol = 1e-05,
  initial_method = c("kmeans", "EM"),
  iter_max = 1000,
  iter_max_prox = 100,
  ncores = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tlgmm_+3A_x">x</code></td>
<td>
<p>design matrix of the target data set. Should be a <code>matrix</code> or <code>data.frame</code> object.</p>
</td></tr>
<tr><td><code id="tlgmm_+3A_fitted_bar">fitted_bar</code></td>
<td>
<p>the output from <code>mtlgmm</code> function.</p>
</td></tr>
<tr><td><code id="tlgmm_+3A_step_size">step_size</code></td>
<td>
<p>step size choice in proximal gradient method to solve each optimization problem in the revised EM algorithm (Algorithm 1 in Tian, Y., Weng, H., &amp; Feng, Y. (2022)), which can be either &quot;lipschitz&quot; or &quot;fixed&quot;. Default = &quot;lipschitz&quot;.
</p>

<ul>
<li><p> lipschitz: <code>eta_w</code>, <code>eta_mu</code> and <code>eta_beta</code> will be chosen by the Lipschitz property of the gradient of objective function (without the penalty part). See Section 4.2 of Parikh, N., &amp; Boyd, S. (2014).
</p>
</li>
<li><p> fixed: <code>eta_w</code>, <code>eta_mu</code> and <code>eta_beta</code> need to be specified
</p>
</li></ul>
</td></tr>
<tr><td><code id="tlgmm_+3A_eta_w">eta_w</code></td>
<td>
<p>step size in the proximal gradient method to learn w (Step 3 of Algorithm 4 in Tian, Y., Weng, H., &amp; Feng, Y. (2022)). Default: 0.1. Only used when <code>step_size</code> = &quot;fixed&quot;.</p>
</td></tr>
<tr><td><code id="tlgmm_+3A_eta_mu">eta_mu</code></td>
<td>
<p>step size in the proximal gradient method to learn mu (Steps 4 and 5 of Algorithm 4 in Tian, Y., Weng, H., &amp; Feng, Y. (2022)). Default: 0.1. Only used when <code>step_size</code> = &quot;fixed&quot;.</p>
</td></tr>
<tr><td><code id="tlgmm_+3A_eta_beta">eta_beta</code></td>
<td>
<p>step size in the proximal gradient method to learn beta (Step 7 of Algorithm 4 in Tian, Y., Weng, H., &amp; Feng, Y. (2022)). Default: 0.1. Only used when <code>step_size</code> = &quot;fixed&quot;.</p>
</td></tr>
<tr><td><code id="tlgmm_+3A_lambda_choice">lambda_choice</code></td>
<td>
<p>the choice of constants in the penalty parameter used in the optimization problems. See Algorithm 4 of Tian, Y., Weng, H., &amp; Feng, Y. (2022), which can be either &quot;fixed&quot; or &quot;cv&quot;. Default = &quot;cv&quot;.
</p>

<ul>
<li><p> cv: <code>cv_nfolds</code>, <code>cv_upper</code>, and <code>cv_length</code> need to be specified. Then the C1 and C2 parameters will be chosen in all combinations in <code>exp(seq(log(cv_lower/10), log(cv_upper/10), length.out = cv_length))</code> via cross-validation. Note that this is a two-dimensional cv process, because we set <code>C1_w</code> = <code>C2_w</code>, <code>C1_mu</code> = <code>C1_beta</code> = <code>C2_mu</code> = <code>C2_beta</code> to reduce the computational cost.
</p>
</li>
<li><p> fixed: <code>C1_w</code>, <code>C1_mu</code>, <code>C1_beta</code>, <code>C2_w</code>, <code>C2_mu</code>, and <code>C2_beta</code> need to be specified. See equations (19)-(24) in Tian, Y., Weng, H., &amp; Feng, Y. (2022).
</p>
</li></ul>
</td></tr>
<tr><td><code id="tlgmm_+3A_cv_nfolds">cv_nfolds</code></td>
<td>
<p>the number of cross-validation folds. Default: 5</p>
</td></tr>
<tr><td><code id="tlgmm_+3A_cv_upper">cv_upper</code></td>
<td>
<p>the upper bound of <code>lambda</code> values used in cross-validation. Default: 5</p>
</td></tr>
<tr><td><code id="tlgmm_+3A_cv_lower">cv_lower</code></td>
<td>
<p>the lower bound of <code>lambda</code> values used in cross-validation. Default: 0.01</p>
</td></tr>
<tr><td><code id="tlgmm_+3A_cv_length">cv_length</code></td>
<td>
<p>the number of <code>lambda</code> values considered in cross-validation. Default: 5</p>
</td></tr>
<tr><td><code id="tlgmm_+3A_c1_w">C1_w</code></td>
<td>
<p>the initial value of C1_w. See equations (19) in Tian, Y., Weng, H., &amp; Feng, Y. (2022). Default: 0.05</p>
</td></tr>
<tr><td><code id="tlgmm_+3A_c1_mu">C1_mu</code></td>
<td>
<p>the initial value of C1_mu. See equations (20) in Tian, Y., Weng, H., &amp; Feng, Y. (2022). Default: 0.2</p>
</td></tr>
<tr><td><code id="tlgmm_+3A_c1_beta">C1_beta</code></td>
<td>
<p>the initial value of C1_beta. See equations (21) in Tian, Y., Weng, H., &amp; Feng, Y. (2022). Default: 0.2</p>
</td></tr>
<tr><td><code id="tlgmm_+3A_c2_w">C2_w</code></td>
<td>
<p>the initial value of C2_w. See equations (22) in Tian, Y., Weng, H., &amp; Feng, Y. (2022). Default: 0.05</p>
</td></tr>
<tr><td><code id="tlgmm_+3A_c2_mu">C2_mu</code></td>
<td>
<p>the initial value of C2_mu. See equations (23) in Tian, Y., Weng, H., &amp; Feng, Y. (2022). Default: 0.2</p>
</td></tr>
<tr><td><code id="tlgmm_+3A_c2_beta">C2_beta</code></td>
<td>
<p>the initial value of C2_beta. See equations (24) in Tian, Y., Weng, H., &amp; Feng, Y. (2022). Default: 0.2</p>
</td></tr>
<tr><td><code id="tlgmm_+3A_kappa0">kappa0</code></td>
<td>
<p>the decaying rate used in equation (19)-(24) in Tian, Y., Weng, H., &amp; Feng, Y. (2022). Default: 1/3</p>
</td></tr>
<tr><td><code id="tlgmm_+3A_tol">tol</code></td>
<td>
<p>maximum tolerance in all optimization problems. If the difference between last update and the current update is less than this value, the iterations of optimization will stop. Default: 1e-05</p>
</td></tr>
<tr><td><code id="tlgmm_+3A_initial_method">initial_method</code></td>
<td>
<p>initialization method. This indicates the method to initialize the estimates of GMM parameters for each data set. Can be either &quot;kmeans&quot; or &quot;EM&quot;.
</p>

<ul>
<li><p> kmeans: the initial estimates of GMM parameters will be generated from the single-task k-means algorithm. Will call <code><a href="stats.html#topic+kmeans">kmeans</a></code> function in <code>stats</code> package.
</p>
</li>
<li><p> EM: the initial estimates of GMM parameters will be generated from the single-task EM algorithm. Will call <code><a href="mclust.html#topic+Mclust">Mclust</a></code> function in <code>mclust</code> package.
</p>
</li></ul>
</td></tr>
<tr><td><code id="tlgmm_+3A_iter_max">iter_max</code></td>
<td>
<p>the maximum iteration number of the revised EM algorithm (i.e. the parameter T in Algorithm 1 in Tian, Y., Weng, H., &amp; Feng, Y. (2022)). Default: 1000</p>
</td></tr>
<tr><td><code id="tlgmm_+3A_iter_max_prox">iter_max_prox</code></td>
<td>
<p>the maximum iteration number of the proximal gradient method. Default: 100</p>
</td></tr>
<tr><td><code id="tlgmm_+3A_ncores">ncores</code></td>
<td>
<p>the number of cores to use. Parallel computing is strongly suggested, specially when <code>lambda_choice</code> = &quot;cv&quot;. Default: 1</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with the following components.
</p>
<table>
<tr><td><code>w</code></td>
<td>
<p>the estimate of mixture proportion in GMMs for the target task. Will be a vector.</p>
</td></tr>
<tr><td><code>mu1</code></td>
<td>
<p>the estimate of Gaussian mean in the first cluster of GMMs for the target task. Will be a matrix, where each column represents the estimate for a task.</p>
</td></tr>
<tr><td><code>mu2</code></td>
<td>
<p>the estimate of Gaussian mean in the second cluster of GMMs for the target task. Will be a matrix, where each column represents the estimate for a task.</p>
</td></tr>
<tr><td><code>beta</code></td>
<td>
<p>the estimate of the discriminant coefficient for the target task. Will be a matrix, where each column represents the estimate for a task.</p>
</td></tr>
<tr><td><code>Sigma</code></td>
<td>
<p>the estimate of the common covariance matrix for the target task. Will be a list, where each component represents the estimate for a task.</p>
</td></tr>
<tr><td><code>C1_w</code></td>
<td>
<p>the initial value of C1_w.</p>
</td></tr>
<tr><td><code>C1_mu</code></td>
<td>
<p>the initial value of C1_mu.</p>
</td></tr>
<tr><td><code>C1_beta</code></td>
<td>
<p>the initial value of C1_beta.</p>
</td></tr>
<tr><td><code>C2_w</code></td>
<td>
<p>the initial value of C2_w.</p>
</td></tr>
<tr><td><code>C2_mu</code></td>
<td>
<p>the initial value of C2_mu.</p>
</td></tr>
<tr><td><code>C2_beta</code></td>
<td>
<p>the initial value of C2_beta.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Tian, Y., Weng, H., &amp; Feng, Y. (2022). Unsupervised Multi-task and Transfer Learning on Gaussian Mixture Models. arXiv preprint arXiv:2209.15224.
</p>
<p>Parikh, N., &amp; Boyd, S. (2014). Proximal algorithms. Foundations and trends in Optimization, 1(3), 127-239.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mtlgmm">mtlgmm</a></code>, <code><a href="#topic+predict_gmm">predict_gmm</a></code>, <code><a href="#topic+data_generation">data_generation</a></code>, <code><a href="#topic+initialize">initialize</a></code>, <code><a href="#topic+alignment">alignment</a></code>, <code><a href="#topic+alignment_swap">alignment_swap</a></code>, <code><a href="#topic+estimation_error">estimation_error</a></code>, <code><a href="#topic+misclustering_error">misclustering_error</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(0, kind = "L'Ecuyer-CMRG")
## Consider a transfer learning problem with 3 source tasks and 1 target task in the setting "MTL-1"
data_list_source &lt;- data_generation(K = 3, outlier_K = 0, simulation_no = "MTL-1", h_w = 0,
h_mu = 0, n = 50)  # generate the source data
data_target &lt;- data_generation(K = 1, outlier_K = 0, simulation_no = "MTL-1", h_w = 0.1,
h_mu = 1, n = 50)  # generate the target data
fit_mtl &lt;- mtlgmm(x = data_list_source$data$x, C1_w = 0.05, C1_mu = 0.2, C1_beta = 0.2,
C2_w = 0.05, C2_mu = 0.2, C2_beta = 0.2, kappa = 1/3, initial_method = "EM",
trim = 0.1, lambda_choice = "fixed", step_size = "lipschitz")

fit_tl &lt;- tlgmm(x = data_target$data$x[[1]], fitted_bar = fit_mtl, C1_w = 0.05,
C1_mu = 0.2, C1_beta = 0.2, C2_w = 0.05, C2_mu = 0.2, C2_beta = 0.2, kappa0 = 1/3,
initial_method = "EM", ncores = 1, lambda_choice = "fixed", step_size = "lipschitz")


# use cross-validation to choose the tuning parameters
# warning: can be quite slow, large "ncores" input is suggested!!
fit_tl &lt;- tlgmm(x = data_target$data$x[[1]], fitted_bar = fit_mtl, kappa0 = 1/3,
initial_method = "EM", ncores = 2, lambda_choice = "cv", step_size = "lipschitz")

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
