<!DOCTYPE html><html><head><title>Help for package tok</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {tok}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#tok-package'><p>tok: Fast Text Tokenization</p></a></li>
<li><a href='#decoder_byte_level'><p>Byte level decoder</p></a></li>
<li><a href='#encoding'><p>Encoding</p></a></li>
<li><a href='#model_bpe'><p>BPE model</p></a></li>
<li><a href='#model_unigram'><p>An implementation of the Unigram algorithm</p></a></li>
<li><a href='#model_wordpiece'><p>An implementation of the WordPiece algorithm</p></a></li>
<li><a href='#normalizer_nfc'><p>NFC normalizer</p></a></li>
<li><a href='#normalizer_nfkc'><p>NFKC normalizer</p></a></li>
<li><a href='#pre_tokenizer'><p>Generic class for tokenizers</p></a></li>
<li><a href='#pre_tokenizer_byte_level'><p>Byte level pre tokenizer</p></a></li>
<li><a href='#pre_tokenizer_whitespace'><p>This pre-tokenizer simply splits using the following regex: <code style="white-space: pre;">&#8288;\w+|[^\w\s]+&#8288;</code></p></a></li>
<li><a href='#processor_byte_level'><p>Byte Level post processor</p></a></li>
<li><a href='#tok_decoder'><p>Generic class for decoders</p></a></li>
<li><a href='#tok_model'><p>Generic class for tokenization models</p></a></li>
<li><a href='#tok_normalizer'><p>Generic class for normalizers</p></a></li>
<li><a href='#tok_processor'><p>Generic class for processors</p></a></li>
<li><a href='#tok_trainer'><p>Generic training class</p></a></li>
<li><a href='#tokenizer'><p>Tokenizer</p></a></li>
<li><a href='#trainer_bpe'><p>BPE trainer</p></a></li>
<li><a href='#trainer_unigram'><p>Unigram tokenizer trainer</p></a></li>
<li><a href='#trainer_wordpiece'><p>WordPiece tokenizer trainer</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Fast Text Tokenization</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.3</td>
</tr>
<tr>
<td>Description:</td>
<td>
  Interfaces with the 'Hugging Face' tokenizers library to provide implementations
  of today's most used tokenizers such as the 'Byte-Pair Encoding' algorithm 
  <a href="https://huggingface.co/docs/tokenizers/index">https://huggingface.co/docs/tokenizers/index</a>. It's extremely fast for both 
  training new vocabularies and tokenizing texts.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>Rust tool chain w/ cargo, libclang/llvm-config</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.1</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.2.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>R6, cli</td>
</tr>
<tr>
<td>Suggests:</td>
<td>rmarkdown, testthat (&ge; 3.0.0), hfhub (&ge; 0.1.1), withr</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/mlverse/tok">https://github.com/mlverse/tok</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/mlverse/tok/issues">https://github.com/mlverse/tok/issues</a></td>
</tr>
<tr>
<td>Config/rextendr/version:</td>
<td>0.3.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-07-05 19:01:35 UTC; dfalbel</td>
</tr>
<tr>
<td>Author:</td>
<td>Daniel Falbel [aut, cre],
  Posit [cph]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Daniel Falbel &lt;daniel@posit.co&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-07-06 13:40:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='tok-package'>tok: Fast Text Tokenization</h2><span id='topic+tok'></span><span id='topic+tok-package'></span>

<h3>Description</h3>

<p>Interfaces with the 'Hugging Face' tokenizers library to provide implementations of today's most used tokenizers such as the 'Byte-Pair Encoding' algorithm <a href="https://huggingface.co/docs/tokenizers/index">https://huggingface.co/docs/tokenizers/index</a>. It's extremely fast for both training new vocabularies and tokenizing texts.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Daniel Falbel <a href="mailto:daniel@posit.co">daniel@posit.co</a>
</p>
<p>Other contributors:
</p>

<ul>
<li><p>  Posit [copyright holder]
</p>
</li></ul>



<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://github.com/mlverse/tok">https://github.com/mlverse/tok</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/mlverse/tok/issues">https://github.com/mlverse/tok/issues</a>
</p>
</li></ul>


<hr>
<h2 id='decoder_byte_level'>Byte level decoder</h2><span id='topic+decoder_byte_level'></span>

<h3>Description</h3>

<p>Byte level decoder
</p>
<p>Byte level decoder
</p>


<h3>Details</h3>

<p>This decoder is to be used with the <a href="#topic+pre_tokenizer_byte_level">pre_tokenizer_byte_level</a>.
</p>


<h3>Super class</h3>

<p><code><a href="#topic+tok_decoder">tok::tok_decoder</a></code> -&gt; <code>tok_decoder_byte_level</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-tok_decoder_byte_level-new"><code>decoder_byte_level$new()</code></a>
</p>
</li>
<li> <p><a href="#method-tok_decoder_byte_level-clone"><code>decoder_byte_level$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-tok_decoder_byte_level-new"></a>



<h4>Method <code>new()</code></h4>

<p>Initializes a byte level decoder
</p>


<h5>Usage</h5>

<div class="r"><pre>decoder_byte_level$new()</pre></div>


<hr>
<a id="method-tok_decoder_byte_level-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>decoder_byte_level$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>See Also</h3>

<p>Other decoders: 
<code><a href="#topic+tok_decoder">tok_decoder</a></code>
</p>

<hr>
<h2 id='encoding'>Encoding</h2><span id='topic+encoding'></span>

<h3>Description</h3>

<p>Represents the output of a <a href="#topic+tokenizer">tokenizer</a>.
</p>


<h3>Value</h3>

<p>An encoding object containing encoding information such as attention masks
and token ids.
</p>


<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>.encoding</code></dt><dd><p>The underlying implementation pointer.</p>
</dd>
</dl>

</div>


<h3>Active bindings</h3>

<div class="r6-active-bindings">

<dl>
<dt><code>ids</code></dt><dd><p>The IDs are the main input to a Language Model. They are the
token indices, the numerical representations that a LM understands.</p>
</dd>
<dt><code>attention_mask</code></dt><dd><p>The attention mask used as input for transformers models.</p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-tok_encoding-new"><code>encoding$new()</code></a>
</p>
</li>
<li> <p><a href="#method-tok_encoding-clone"><code>encoding$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-tok_encoding-new"></a>



<h4>Method <code>new()</code></h4>

<p>Initializes an encoding object (Not to use directly)
</p>


<h5>Usage</h5>

<div class="r"><pre>encoding$new(encoding)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>encoding</code></dt><dd><p>an encoding implementation object</p>
</dd>
</dl>

</div>


<hr>
<a id="method-tok_encoding-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>encoding$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>Examples</h3>

<pre><code class='language-R'>withr::with_envvar(c(HUGGINGFACE_HUB_CACHE = tempdir()), {
try({
tok &lt;- tokenizer$from_pretrained("gpt2")
encoding &lt;- tok$encode("Hello world")
encoding
})
})
</code></pre>

<hr>
<h2 id='model_bpe'>BPE model</h2><span id='topic+model_bpe'></span>

<h3>Description</h3>

<p>BPE model
</p>
<p>BPE model
</p>


<h3>Super class</h3>

<p><code><a href="#topic+tok_model">tok::tok_model</a></code> -&gt; <code>tok_model_bpe</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-tok_model_bpe-new"><code>model_bpe$new()</code></a>
</p>
</li>
<li> <p><a href="#method-tok_model_bpe-clone"><code>model_bpe$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-tok_model_bpe-new"></a>



<h4>Method <code>new()</code></h4>

<p>Initializes a BPE model
An implementation of the BPE (Byte-Pair Encoding) algorithm
</p>


<h5>Usage</h5>

<div class="r"><pre>model_bpe$new(
  vocab = NULL,
  merges = NULL,
  cache_capacity = NULL,
  dropout = NULL,
  unk_token = NULL,
  continuing_subword_prefix = NULL,
  end_of_word_suffix = NULL,
  fuse_unk = NULL,
  byte_fallback = FALSE
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>vocab</code></dt><dd><p>A named integer vector of string keys and their corresponding ids. Default: <code>NULL</code></p>
</dd>
<dt><code>merges</code></dt><dd><p>A list of pairs of tokens (<code style="white-space: pre;">&#8288;[character, character]&#8288;</code>). Default: <code>NULL</code>.</p>
</dd>
<dt><code>cache_capacity</code></dt><dd><p>The number of words that the BPE cache can contain.
The cache speeds up the process by storing merge operation results. Default: <code>NULL.</code></p>
</dd>
<dt><code>dropout</code></dt><dd><p>A float between 0 and 1 representing the BPE dropout to use. Default: <code>NULL</code></p>
</dd>
<dt><code>unk_token</code></dt><dd><p>The unknown token to be used by the model. Default: 'NULL&ldquo;'.</p>
</dd>
<dt><code>continuing_subword_prefix</code></dt><dd><p>The prefix to attach to subword units that don’t
represent the beginning of a word. Default: <code>NULL</code></p>
</dd>
<dt><code>end_of_word_suffix</code></dt><dd><p>The suffix to attach to subword units that represent
the end of a word. Default: <code>NULL</code></p>
</dd>
<dt><code>fuse_unk</code></dt><dd><p>Whether to fuse any subsequent unknown tokens into a single one. Default: <code>NULL</code>.</p>
</dd>
<dt><code>byte_fallback</code></dt><dd><p>Whether to use the spm byte-fallback trick. Default: <code>FALSE</code>.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-tok_model_bpe-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>model_bpe$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>See Also</h3>

<p>Other model: 
<code><a href="#topic+model_unigram">model_unigram</a></code>,
<code><a href="#topic+model_wordpiece">model_wordpiece</a></code>,
<code><a href="#topic+tok_model">tok_model</a></code>
</p>

<hr>
<h2 id='model_unigram'>An implementation of the Unigram algorithm</h2><span id='topic+model_unigram'></span>

<h3>Description</h3>

<p>An implementation of the Unigram algorithm
</p>
<p>An implementation of the Unigram algorithm
</p>


<h3>Super class</h3>

<p><code><a href="#topic+tok_model">tok::tok_model</a></code> -&gt; <code>tok_model_unigram</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-tok_model_unigram-new"><code>model_unigram$new()</code></a>
</p>
</li>
<li> <p><a href="#method-tok_model_unigram-clone"><code>model_unigram$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-tok_model_unigram-new"></a>



<h4>Method <code>new()</code></h4>

<p>Constructor for Unigram Model
</p>


<h5>Usage</h5>

<div class="r"><pre>model_unigram$new(vocab = NULL, unk_id = NULL, byte_fallback = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>vocab</code></dt><dd><p>A dictionary of string keys and their corresponding relative score.
Default: <code>NULL</code>.</p>
</dd>
<dt><code>unk_id</code></dt><dd><p>The unknown token id to be used by the model.
Default: <code>NULL</code>.</p>
</dd>
<dt><code>byte_fallback</code></dt><dd><p>Whether to use byte-fallback trick. Default: <code>FALSE</code>.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-tok_model_unigram-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>model_unigram$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>See Also</h3>

<p>Other model: 
<code><a href="#topic+model_bpe">model_bpe</a></code>,
<code><a href="#topic+model_wordpiece">model_wordpiece</a></code>,
<code><a href="#topic+tok_model">tok_model</a></code>
</p>

<hr>
<h2 id='model_wordpiece'>An implementation of the WordPiece algorithm</h2><span id='topic+model_wordpiece'></span>

<h3>Description</h3>

<p>An implementation of the WordPiece algorithm
</p>
<p>An implementation of the WordPiece algorithm
</p>


<h3>Super class</h3>

<p><code><a href="#topic+tok_model">tok::tok_model</a></code> -&gt; <code>tok_model_wordpiece</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-tok_model_wordpiece-new"><code>model_wordpiece$new()</code></a>
</p>
</li>
<li> <p><a href="#method-tok_model_wordpiece-clone"><code>model_wordpiece$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-tok_model_wordpiece-new"></a>



<h4>Method <code>new()</code></h4>

<p>Constructor for the wordpiece tokenizer
</p>


<h5>Usage</h5>

<div class="r"><pre>model_wordpiece$new(
  vocab = NULL,
  unk_token = NULL,
  max_input_chars_per_word = NULL
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>vocab</code></dt><dd><p>A dictionary of string keys and their corresponding ids.
Default: <code>NULL</code>.</p>
</dd>
<dt><code>unk_token</code></dt><dd><p>The unknown token to be used by the model.
Default: <code>NULL</code>.</p>
</dd>
<dt><code>max_input_chars_per_word</code></dt><dd><p>The maximum number of characters to allow in a single word.
Default: <code>NULL</code>.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-tok_model_wordpiece-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>model_wordpiece$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>See Also</h3>

<p>Other model: 
<code><a href="#topic+model_bpe">model_bpe</a></code>,
<code><a href="#topic+model_unigram">model_unigram</a></code>,
<code><a href="#topic+tok_model">tok_model</a></code>
</p>

<hr>
<h2 id='normalizer_nfc'>NFC normalizer</h2><span id='topic+normalizer_nfc'></span>

<h3>Description</h3>

<p>NFC normalizer
</p>
<p>NFC normalizer
</p>


<h3>Super class</h3>

<p><code><a href="#topic+tok_normalizer">tok::tok_normalizer</a></code> -&gt; <code>tok_normalizer_nfc</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-tok_normalizer_nfc-new"><code>normalizer_nfc$new()</code></a>
</p>
</li>
<li> <p><a href="#method-tok_normalizer_nfc-clone"><code>normalizer_nfc$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-tok_normalizer_nfc-new"></a>



<h4>Method <code>new()</code></h4>

<p>Initializes the NFC normalizer
</p>


<h5>Usage</h5>

<div class="r"><pre>normalizer_nfc$new()</pre></div>


<hr>
<a id="method-tok_normalizer_nfc-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>normalizer_nfc$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>See Also</h3>

<p>Other normalizers: 
<code><a href="#topic+normalizer_nfkc">normalizer_nfkc</a></code>,
<code><a href="#topic+tok_normalizer">tok_normalizer</a></code>
</p>

<hr>
<h2 id='normalizer_nfkc'>NFKC normalizer</h2><span id='topic+normalizer_nfkc'></span>

<h3>Description</h3>

<p>NFKC normalizer
</p>
<p>NFKC normalizer
</p>


<h3>Super class</h3>

<p><code><a href="#topic+tok_normalizer">tok::tok_normalizer</a></code> -&gt; <code>tok_normalizer_nfc</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-tok_normalizer_nfc-new"><code>normalizer_nfkc$new()</code></a>
</p>
</li>
<li> <p><a href="#method-tok_normalizer_nfc-clone"><code>normalizer_nfkc$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-tok_normalizer_nfc-new"></a>



<h4>Method <code>new()</code></h4>

<p>Initializes the NFKC normalizer
</p>


<h5>Usage</h5>

<div class="r"><pre>normalizer_nfkc$new()</pre></div>


<hr>
<a id="method-tok_normalizer_nfc-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>normalizer_nfkc$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>See Also</h3>

<p>Other normalizers: 
<code><a href="#topic+normalizer_nfc">normalizer_nfc</a></code>,
<code><a href="#topic+tok_normalizer">tok_normalizer</a></code>
</p>

<hr>
<h2 id='pre_tokenizer'>Generic class for tokenizers</h2><span id='topic+pre_tokenizer'></span>

<h3>Description</h3>

<p>Generic class for tokenizers
</p>
<p>Generic class for tokenizers
</p>


<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>.pre_tokenizer</code></dt><dd><p>Internal pointer to tokenizer object</p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-tok_pre_tokenizer-new"><code>pre_tokenizer$new()</code></a>
</p>
</li>
<li> <p><a href="#method-tok_pre_tokenizer-clone"><code>pre_tokenizer$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-tok_pre_tokenizer-new"></a>



<h4>Method <code>new()</code></h4>

<p>Initializes a tokenizer
</p>


<h5>Usage</h5>

<div class="r"><pre>pre_tokenizer$new(pre_tokenizer)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>pre_tokenizer</code></dt><dd><p>a raw pointer to a tokenizer</p>
</dd>
</dl>

</div>


<hr>
<a id="method-tok_pre_tokenizer-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>pre_tokenizer$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>See Also</h3>

<p>Other pre_tokenizer: 
<code><a href="#topic+pre_tokenizer_byte_level">pre_tokenizer_byte_level</a></code>,
<code><a href="#topic+pre_tokenizer_whitespace">pre_tokenizer_whitespace</a></code>
</p>

<hr>
<h2 id='pre_tokenizer_byte_level'>Byte level pre tokenizer</h2><span id='topic+pre_tokenizer_byte_level'></span>

<h3>Description</h3>

<p>Byte level pre tokenizer
</p>
<p>Byte level pre tokenizer
</p>


<h3>Details</h3>

<p>This pre-tokenizer takes care of replacing all bytes of the given string with
a corresponding representation, as well as splitting into words.
</p>


<h3>Super class</h3>

<p><code>tok::tok_pre_tokenizer</code> -&gt; <code>tok_pre_tokenizer_whitespace</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-tok_pre_tokenizer_whitespace-new"><code>pre_tokenizer_byte_level$new()</code></a>
</p>
</li>
<li> <p><a href="#method-tok_pre_tokenizer_whitespace-clone"><code>pre_tokenizer_byte_level$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-tok_pre_tokenizer_whitespace-new"></a>



<h4>Method <code>new()</code></h4>

<p>Initializes the bytelevel tokenizer
</p>


<h5>Usage</h5>

<div class="r"><pre>pre_tokenizer_byte_level$new(add_prefix_space = TRUE, use_regex = TRUE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>add_prefix_space</code></dt><dd><p>Whether to add a space to the first word</p>
</dd>
<dt><code>use_regex</code></dt><dd><p>Set this to False to prevent this pre_tokenizer from using
the GPT2 specific regexp for spliting on whitespace.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-tok_pre_tokenizer_whitespace-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>pre_tokenizer_byte_level$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>See Also</h3>

<p>Other pre_tokenizer: 
<code><a href="#topic+pre_tokenizer">pre_tokenizer</a></code>,
<code><a href="#topic+pre_tokenizer_whitespace">pre_tokenizer_whitespace</a></code>
</p>

<hr>
<h2 id='pre_tokenizer_whitespace'>This pre-tokenizer simply splits using the following regex: <code style="white-space: pre;">&#8288;\w+|[^\w\s]+&#8288;</code></h2><span id='topic+pre_tokenizer_whitespace'></span>

<h3>Description</h3>

<p>This pre-tokenizer simply splits using the following regex: <code style="white-space: pre;">&#8288;\w+|[^\w\s]+&#8288;</code>
</p>
<p>This pre-tokenizer simply splits using the following regex: <code style="white-space: pre;">&#8288;\w+|[^\w\s]+&#8288;</code>
</p>


<h3>Super class</h3>

<p><code>tok::tok_pre_tokenizer</code> -&gt; <code>tok_pre_tokenizer_whitespace</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-tok_pre_tokenizer_whitespace-new"><code>pre_tokenizer_whitespace$new()</code></a>
</p>
</li>
<li> <p><a href="#method-tok_pre_tokenizer_whitespace-clone"><code>pre_tokenizer_whitespace$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-tok_pre_tokenizer_whitespace-new"></a>



<h4>Method <code>new()</code></h4>

<p>Initializes the whistespace tokenizer
</p>


<h5>Usage</h5>

<div class="r"><pre>pre_tokenizer_whitespace$new()</pre></div>


<hr>
<a id="method-tok_pre_tokenizer_whitespace-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>pre_tokenizer_whitespace$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>See Also</h3>

<p>Other pre_tokenizer: 
<code><a href="#topic+pre_tokenizer">pre_tokenizer</a></code>,
<code><a href="#topic+pre_tokenizer_byte_level">pre_tokenizer_byte_level</a></code>
</p>

<hr>
<h2 id='processor_byte_level'>Byte Level post processor</h2><span id='topic+processor_byte_level'></span>

<h3>Description</h3>

<p>Byte Level post processor
</p>
<p>Byte Level post processor
</p>


<h3>Details</h3>

<p>This post-processor takes care of trimming the offsets.
By default, the ByteLevel BPE might include whitespaces in the produced tokens.
If you don’t want the offsets to include these whitespaces, then this
PostProcessor must be used.
</p>


<h3>Super class</h3>

<p><code><a href="#topic+tok_processor">tok::tok_processor</a></code> -&gt; <code>tok_processor_byte_level</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-tok_processor_byte_level-new"><code>processor_byte_level$new()</code></a>
</p>
</li>
<li> <p><a href="#method-tok_processor_byte_level-clone"><code>processor_byte_level$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-tok_processor_byte_level-new"></a>



<h4>Method <code>new()</code></h4>

<p>Initializes the byte level post processor
</p>


<h5>Usage</h5>

<div class="r"><pre>processor_byte_level$new(trim_offsets = TRUE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>trim_offsets</code></dt><dd><p>Whether to trim the whitespaces from the produced offsets.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-tok_processor_byte_level-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>processor_byte_level$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>See Also</h3>

<p>Other processors: 
<code><a href="#topic+tok_processor">tok_processor</a></code>
</p>

<hr>
<h2 id='tok_decoder'>Generic class for decoders</h2><span id='topic+tok_decoder'></span>

<h3>Description</h3>

<p>Generic class for decoders
</p>
<p>Generic class for decoders
</p>


<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>.decoder</code></dt><dd><p>The raw pointer to the decoder</p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-tok_decoder-new"><code>tok_decoder$new()</code></a>
</p>
</li>
<li> <p><a href="#method-tok_decoder-clone"><code>tok_decoder$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-tok_decoder-new"></a>



<h4>Method <code>new()</code></h4>

<p>Initializes a decoder
</p>


<h5>Usage</h5>

<div class="r"><pre>tok_decoder$new(decoder)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>decoder</code></dt><dd><p>a raw decoder pointer</p>
</dd>
</dl>

</div>


<hr>
<a id="method-tok_decoder-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>tok_decoder$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>See Also</h3>

<p>Other decoders: 
<code><a href="#topic+decoder_byte_level">decoder_byte_level</a></code>
</p>

<hr>
<h2 id='tok_model'>Generic class for tokenization models</h2><span id='topic+tok_model'></span>

<h3>Description</h3>

<p>Generic class for tokenization models
</p>
<p>Generic class for tokenization models
</p>


<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>.model</code></dt><dd><p>stores the pointer to the model. internal</p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-tok_model-new"><code>tok_model$new()</code></a>
</p>
</li>
<li> <p><a href="#method-tok_model-clone"><code>tok_model$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-tok_model-new"></a>



<h4>Method <code>new()</code></h4>

<p>Initializes a genric abstract tokenizer model
</p>


<h5>Usage</h5>

<div class="r"><pre>tok_model$new(model)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>model</code></dt><dd><p>Pointer to a tokenization model</p>
</dd>
</dl>

</div>


<hr>
<a id="method-tok_model-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>tok_model$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>See Also</h3>

<p>Other model: 
<code><a href="#topic+model_bpe">model_bpe</a></code>,
<code><a href="#topic+model_unigram">model_unigram</a></code>,
<code><a href="#topic+model_wordpiece">model_wordpiece</a></code>
</p>

<hr>
<h2 id='tok_normalizer'>Generic class for normalizers</h2><span id='topic+tok_normalizer'></span>

<h3>Description</h3>

<p>Generic class for normalizers
</p>
<p>Generic class for normalizers
</p>


<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>.normalizer</code></dt><dd><p>Internal pointer to normalizer object</p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-tok_normalizer-new"><code>tok_normalizer$new()</code></a>
</p>
</li>
<li> <p><a href="#method-tok_normalizer-clone"><code>tok_normalizer$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-tok_normalizer-new"></a>



<h4>Method <code>new()</code></h4>

<p>Initializes a tokenizer
</p>


<h5>Usage</h5>

<div class="r"><pre>tok_normalizer$new(normalizer)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>normalizer</code></dt><dd><p>a raw pointer to a tokenizer</p>
</dd>
</dl>

</div>


<hr>
<a id="method-tok_normalizer-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>tok_normalizer$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>See Also</h3>

<p>Other normalizers: 
<code><a href="#topic+normalizer_nfc">normalizer_nfc</a></code>,
<code><a href="#topic+normalizer_nfkc">normalizer_nfkc</a></code>
</p>

<hr>
<h2 id='tok_processor'>Generic class for processors</h2><span id='topic+tok_processor'></span>

<h3>Description</h3>

<p>Generic class for processors
</p>
<p>Generic class for processors
</p>


<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>.processor</code></dt><dd><p>Internal pointer to processor object</p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-tok_processor-new"><code>tok_processor$new()</code></a>
</p>
</li>
<li> <p><a href="#method-tok_processor-clone"><code>tok_processor$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-tok_processor-new"></a>



<h4>Method <code>new()</code></h4>

<p>Initializes a tokenizer
</p>


<h5>Usage</h5>

<div class="r"><pre>tok_processor$new(processor)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>processor</code></dt><dd><p>a raw pointer to a processor</p>
</dd>
</dl>

</div>


<hr>
<a id="method-tok_processor-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>tok_processor$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>See Also</h3>

<p>Other processors: 
<code><a href="#topic+processor_byte_level">processor_byte_level</a></code>
</p>

<hr>
<h2 id='tok_trainer'>Generic training class</h2><span id='topic+tok_trainer'></span>

<h3>Description</h3>

<p>Generic training class
</p>
<p>Generic training class
</p>


<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>.trainer</code></dt><dd><p>a pointer to a raw trainer</p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-tok_trainer-new"><code>tok_trainer$new()</code></a>
</p>
</li>
<li> <p><a href="#method-tok_trainer-clone"><code>tok_trainer$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-tok_trainer-new"></a>



<h4>Method <code>new()</code></h4>

<p>Initializes a generic trainer from a raw trainer
</p>


<h5>Usage</h5>

<div class="r"><pre>tok_trainer$new(trainer)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>trainer</code></dt><dd><p>raw trainer (internal)</p>
</dd>
</dl>

</div>


<hr>
<a id="method-tok_trainer-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>tok_trainer$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>See Also</h3>

<p>Other trainer: 
<code><a href="#topic+trainer_bpe">trainer_bpe</a></code>,
<code><a href="#topic+trainer_unigram">trainer_unigram</a></code>,
<code><a href="#topic+trainer_wordpiece">trainer_wordpiece</a></code>
</p>

<hr>
<h2 id='tokenizer'>Tokenizer</h2><span id='topic+tokenizer'></span>

<h3>Description</h3>

<p>A Tokenizer works as a pipeline. It processes some raw text as input and outputs
an <a href="#topic+encoding">encoding</a>.
</p>


<h3>Value</h3>

<p>A tokenizer that can be used for encoding character strings or decoding
integers.
</p>


<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>.tokenizer</code></dt><dd><p>(unsafe usage) Lower level pointer to tokenizer</p>
</dd>
</dl>

</div>


<h3>Active bindings</h3>

<div class="r6-active-bindings">

<dl>
<dt><code>pre_tokenizer</code></dt><dd><p>instance of the pre-tokenizer</p>
</dd>
<dt><code>normalizer</code></dt><dd><p>Gets the normalizer instance</p>
</dd>
<dt><code>post_processor</code></dt><dd><p>Gets the post processor used by tokenizer</p>
</dd>
<dt><code>decoder</code></dt><dd><p>Gets and sets the decoder</p>
</dd>
<dt><code>padding</code></dt><dd><p>Gets padding configuration</p>
</dd>
<dt><code>truncation</code></dt><dd><p>Gets truncation configuration</p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-tok_tokenizer-new"><code>tokenizer$new()</code></a>
</p>
</li>
<li> <p><a href="#method-tok_tokenizer-encode"><code>tokenizer$encode()</code></a>
</p>
</li>
<li> <p><a href="#method-tok_tokenizer-decode"><code>tokenizer$decode()</code></a>
</p>
</li>
<li> <p><a href="#method-tok_tokenizer-encode_batch"><code>tokenizer$encode_batch()</code></a>
</p>
</li>
<li> <p><a href="#method-tok_tokenizer-decode_batch"><code>tokenizer$decode_batch()</code></a>
</p>
</li>
<li> <p><a href="#method-tok_tokenizer-from_file"><code>tokenizer$from_file()</code></a>
</p>
</li>
<li> <p><a href="#method-tok_tokenizer-from_pretrained"><code>tokenizer$from_pretrained()</code></a>
</p>
</li>
<li> <p><a href="#method-tok_tokenizer-train"><code>tokenizer$train()</code></a>
</p>
</li>
<li> <p><a href="#method-tok_tokenizer-train_from_memory"><code>tokenizer$train_from_memory()</code></a>
</p>
</li>
<li> <p><a href="#method-tok_tokenizer-save"><code>tokenizer$save()</code></a>
</p>
</li>
<li> <p><a href="#method-tok_tokenizer-enable_padding"><code>tokenizer$enable_padding()</code></a>
</p>
</li>
<li> <p><a href="#method-tok_tokenizer-no_padding"><code>tokenizer$no_padding()</code></a>
</p>
</li>
<li> <p><a href="#method-tok_tokenizer-enable_truncation"><code>tokenizer$enable_truncation()</code></a>
</p>
</li>
<li> <p><a href="#method-tok_tokenizer-no_truncation"><code>tokenizer$no_truncation()</code></a>
</p>
</li>
<li> <p><a href="#method-tok_tokenizer-get_vocab_size"><code>tokenizer$get_vocab_size()</code></a>
</p>
</li>
<li> <p><a href="#method-tok_tokenizer-clone"><code>tokenizer$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-tok_tokenizer-new"></a>



<h4>Method <code>new()</code></h4>

<p>Initializes a tokenizer
</p>


<h5>Usage</h5>

<div class="r"><pre>tokenizer$new(tokenizer)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>tokenizer</code></dt><dd><p>Will be cloned to initialize a new tokenizer</p>
</dd>
</dl>

</div>


<hr>
<a id="method-tok_tokenizer-encode"></a>



<h4>Method <code>encode()</code></h4>

<p>Encode the given sequence and pair. This method can process raw text sequences
as well as already pre-tokenized sequences.
</p>


<h5>Usage</h5>

<div class="r"><pre>tokenizer$encode(
  sequence,
  pair = NULL,
  is_pretokenized = FALSE,
  add_special_tokens = TRUE
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>sequence</code></dt><dd><p>The main input sequence we want to encode. This sequence can
be either raw text or pre-tokenized, according to the is_pretokenized argument</p>
</dd>
<dt><code>pair</code></dt><dd><p>An optional input sequence. The expected format is the same
that for sequence.</p>
</dd>
<dt><code>is_pretokenized</code></dt><dd><p>Whether the input is already pre-tokenized</p>
</dd>
<dt><code>add_special_tokens</code></dt><dd><p>Whether to add the special tokens</p>
</dd>
</dl>

</div>


<hr>
<a id="method-tok_tokenizer-decode"></a>



<h4>Method <code>decode()</code></h4>

<p>Decode the given list of ids back to a string
</p>


<h5>Usage</h5>

<div class="r"><pre>tokenizer$decode(ids, skip_special_tokens = TRUE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>ids</code></dt><dd><p>The list of ids that we want to decode</p>
</dd>
<dt><code>skip_special_tokens</code></dt><dd><p>Whether the special tokens should be removed from the decoded string</p>
</dd>
</dl>

</div>


<hr>
<a id="method-tok_tokenizer-encode_batch"></a>



<h4>Method <code>encode_batch()</code></h4>

<p>Encodes a batch of sequences. Returns a list of <a href="#topic+encoding">encoding</a>s.
</p>


<h5>Usage</h5>

<div class="r"><pre>tokenizer$encode_batch(
  input,
  is_pretokenized = FALSE,
  add_special_tokens = TRUE
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>input</code></dt><dd><p>A list of single sequences or pair sequences to encode. Each
sequence can be either raw text or pre-tokenized, according to the is_pretokenized
argument.</p>
</dd>
<dt><code>is_pretokenized</code></dt><dd><p>Whether the input is already pre-tokenized</p>
</dd>
<dt><code>add_special_tokens</code></dt><dd><p>Whether to add the special tokens</p>
</dd>
</dl>

</div>


<hr>
<a id="method-tok_tokenizer-decode_batch"></a>



<h4>Method <code>decode_batch()</code></h4>

<p>Decode a batch of ids back to their corresponding string
</p>


<h5>Usage</h5>

<div class="r"><pre>tokenizer$decode_batch(sequences, skip_special_tokens = TRUE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>sequences</code></dt><dd><p>The batch of sequences we want to decode</p>
</dd>
<dt><code>skip_special_tokens</code></dt><dd><p>Whether the special tokens should be removed from the decoded strings</p>
</dd>
</dl>

</div>


<hr>
<a id="method-tok_tokenizer-from_file"></a>



<h4>Method <code>from_file()</code></h4>

<p>Creates a tokenizer from the path of a serialized tokenizer.
This is a static method and should be called instead of <code style="white-space: pre;">&#8288;$new&#8288;</code> when initializing
the tokenizer.
</p>


<h5>Usage</h5>

<div class="r"><pre>tokenizer$from_file(path)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>path</code></dt><dd><p>Path to tokenizer.json file</p>
</dd>
</dl>

</div>


<hr>
<a id="method-tok_tokenizer-from_pretrained"></a>



<h4>Method <code>from_pretrained()</code></h4>

<p>Instantiate a new Tokenizer from an existing file on the Hugging Face Hub.
</p>


<h5>Usage</h5>

<div class="r"><pre>tokenizer$from_pretrained(identifier, revision = "main", auth_token = NULL)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>identifier</code></dt><dd><p>The identifier of a Model on the Hugging Face Hub, that
contains a tokenizer.json file</p>
</dd>
<dt><code>revision</code></dt><dd><p>A branch or commit id</p>
</dd>
<dt><code>auth_token</code></dt><dd><p>An optional auth token used to access private repositories
on the Hugging Face Hub</p>
</dd>
</dl>

</div>


<hr>
<a id="method-tok_tokenizer-train"></a>



<h4>Method <code>train()</code></h4>

<p>Train the Tokenizer using the given files.
Reads the files line by line, while keeping all the whitespace, even new lines.
</p>


<h5>Usage</h5>

<div class="r"><pre>tokenizer$train(files, trainer)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>files</code></dt><dd><p>character vector of file paths.</p>
</dd>
<dt><code>trainer</code></dt><dd><p>an instance of a trainer object, specific to that tokenizer type.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-tok_tokenizer-train_from_memory"></a>



<h4>Method <code>train_from_memory()</code></h4>

<p>Train the tokenizer on a chracter vector of texts
</p>


<h5>Usage</h5>

<div class="r"><pre>tokenizer$train_from_memory(texts, trainer)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>texts</code></dt><dd><p>a character vector of texts.</p>
</dd>
<dt><code>trainer</code></dt><dd><p>an instance of a trainer object, specific to that tokenizer type.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-tok_tokenizer-save"></a>



<h4>Method <code>save()</code></h4>

<p>Saves the tokenizer to a json file
</p>


<h5>Usage</h5>

<div class="r"><pre>tokenizer$save(path, pretty = TRUE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>path</code></dt><dd><p>A path to a file in which to save the serialized tokenizer.</p>
</dd>
<dt><code>pretty</code></dt><dd><p>Whether the JSON file should be pretty formatted.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-tok_tokenizer-enable_padding"></a>



<h4>Method <code>enable_padding()</code></h4>

<p>Enables padding for the tokenizer
</p>


<h5>Usage</h5>

<div class="r"><pre>tokenizer$enable_padding(
  direction = "right",
  pad_id = 0L,
  pad_type_id = 0L,
  pad_token = "[PAD]",
  length = NULL,
  pad_to_multiple_of = NULL
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>direction</code></dt><dd><p>(str, optional, defaults to right) — The direction in which
to pad. Can be either <code>'right'</code> or <code>'left'</code></p>
</dd>
<dt><code>pad_id</code></dt><dd><p>(int, defaults to 0) — The id to be used when padding</p>
</dd>
<dt><code>pad_type_id</code></dt><dd><p>(int, defaults to 0) — The type id to be used when padding</p>
</dd>
<dt><code>pad_token</code></dt><dd><p>(str, defaults to <code>'[PAD]'</code>) — The pad token to be used when padding</p>
</dd>
<dt><code>length</code></dt><dd><p>(int, optional) — If specified, the length at which to pad. If not
specified we pad using the size of the longest sequence in a batch.</p>
</dd>
<dt><code>pad_to_multiple_of</code></dt><dd><p>(int, optional) — If specified, the padding length should
always snap to the next multiple of the given value. For example if we were
going to pad with a length of 250 but <code>pad_to_multiple_of=8</code> then we will
pad to 256.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-tok_tokenizer-no_padding"></a>



<h4>Method <code>no_padding()</code></h4>

<p>Disables padding
</p>


<h5>Usage</h5>

<div class="r"><pre>tokenizer$no_padding()</pre></div>


<hr>
<a id="method-tok_tokenizer-enable_truncation"></a>



<h4>Method <code>enable_truncation()</code></h4>

<p>Enables truncation on the tokenizer
</p>


<h5>Usage</h5>

<div class="r"><pre>tokenizer$enable_truncation(
  max_length,
  stride = 0,
  strategy = "longest_first",
  direction = "right"
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>max_length</code></dt><dd><p>The maximum length at which to truncate.</p>
</dd>
<dt><code>stride</code></dt><dd><p>The length of the previous first sequence to be included
in the overflowing sequence. Default: <code>0</code>.</p>
</dd>
<dt><code>strategy</code></dt><dd><p>The strategy used for truncation. Can be one of:
&quot;longest_first&quot;, &quot;only_first&quot;, or &quot;only_second&quot;. Default: &quot;longest_first&quot;.</p>
</dd>
<dt><code>direction</code></dt><dd><p>The truncation direction. Default: &quot;right&quot;.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-tok_tokenizer-no_truncation"></a>



<h4>Method <code>no_truncation()</code></h4>

<p>Disables truncation
</p>


<h5>Usage</h5>

<div class="r"><pre>tokenizer$no_truncation()</pre></div>


<hr>
<a id="method-tok_tokenizer-get_vocab_size"></a>



<h4>Method <code>get_vocab_size()</code></h4>

<p>Gets the vocabulary size
</p>


<h5>Usage</h5>

<div class="r"><pre>tokenizer$get_vocab_size(with_added_tokens = TRUE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>with_added_tokens</code></dt><dd><p>Wether to count added tokens</p>
</dd>
</dl>

</div>


<hr>
<a id="method-tok_tokenizer-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>tokenizer$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>Examples</h3>

<pre><code class='language-R'>withr::with_envvar(c(HUGGINGFACE_HUB_CACHE = tempdir()), {
try({
tok &lt;- tokenizer$from_pretrained("gpt2")
tok$encode("Hello world")$ids
})
})

</code></pre>

<hr>
<h2 id='trainer_bpe'>BPE trainer</h2><span id='topic+trainer_bpe'></span>

<h3>Description</h3>

<p>BPE trainer
</p>
<p>BPE trainer
</p>


<h3>Super class</h3>

<p><code><a href="#topic+tok_trainer">tok::tok_trainer</a></code> -&gt; <code>tok_trainer_bpe</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-tok_trainer_bpe-new"><code>trainer_bpe$new()</code></a>
</p>
</li>
<li> <p><a href="#method-tok_trainer_bpe-clone"><code>trainer_bpe$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-tok_trainer_bpe-new"></a>



<h4>Method <code>new()</code></h4>

<p>Constrcutor for the BPE trainer
</p>


<h5>Usage</h5>

<div class="r"><pre>trainer_bpe$new(
  vocab_size = NULL,
  min_frequency = NULL,
  show_progress = NULL,
  special_tokens = NULL,
  limit_alphabet = NULL,
  initial_alphabet = NULL,
  continuing_subword_prefix = NULL,
  end_of_word_suffix = NULL,
  max_token_length = NULL
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>vocab_size</code></dt><dd><p>The size of the final vocabulary, including all tokens and alphabet.
Default: <code>NULL</code>.</p>
</dd>
<dt><code>min_frequency</code></dt><dd><p>The minimum frequency a pair should have in order to be merged.
Default: <code>NULL</code>.</p>
</dd>
<dt><code>show_progress</code></dt><dd><p>Whether to show progress bars while training. Default: <code>TRUE</code>.</p>
</dd>
<dt><code>special_tokens</code></dt><dd><p>A list of special tokens the model should be aware of.
Default: <code>NULL</code>.</p>
</dd>
<dt><code>limit_alphabet</code></dt><dd><p>The maximum number of different characters to keep in the alphabet.
Default: <code>NULL</code>.</p>
</dd>
<dt><code>initial_alphabet</code></dt><dd><p>A list of characters to include in the initial alphabet,
even if not seen in the training dataset. Default: <code>NULL</code>.</p>
</dd>
<dt><code>continuing_subword_prefix</code></dt><dd><p>A prefix to be used for every subword that is not a beginning-of-word.
Default: <code>NULL</code>.</p>
</dd>
<dt><code>end_of_word_suffix</code></dt><dd><p>A suffix to be used for every subword that is an end-of-word.
Default: <code>NULL</code>.</p>
</dd>
<dt><code>max_token_length</code></dt><dd><p>Prevents creating tokens longer than the specified size.
Default: <code>NULL</code>.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-tok_trainer_bpe-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>trainer_bpe$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>See Also</h3>

<p>Other trainer: 
<code><a href="#topic+tok_trainer">tok_trainer</a></code>,
<code><a href="#topic+trainer_unigram">trainer_unigram</a></code>,
<code><a href="#topic+trainer_wordpiece">trainer_wordpiece</a></code>
</p>

<hr>
<h2 id='trainer_unigram'>Unigram tokenizer trainer</h2><span id='topic+trainer_unigram'></span>

<h3>Description</h3>

<p>Unigram tokenizer trainer
</p>
<p>Unigram tokenizer trainer
</p>


<h3>Super class</h3>

<p><code><a href="#topic+tok_trainer">tok::tok_trainer</a></code> -&gt; <code>tok_trainer_unigram</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-tok_trainer_unigram-new"><code>trainer_unigram$new()</code></a>
</p>
</li>
<li> <p><a href="#method-tok_trainer_unigram-clone"><code>trainer_unigram$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-tok_trainer_unigram-new"></a>



<h4>Method <code>new()</code></h4>

<p>Constructor for the Unigram tokenizer
</p>


<h5>Usage</h5>

<div class="r"><pre>trainer_unigram$new(
  vocab_size = 8000,
  show_progress = TRUE,
  special_tokens = NULL,
  shrinking_factor = 0.75,
  unk_token = NULL,
  max_piece_length = 16,
  n_sub_iterations = 2
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>vocab_size</code></dt><dd><p>The size of the final vocabulary, including all tokens and alphabet.</p>
</dd>
<dt><code>show_progress</code></dt><dd><p>Whether to show progress bars while training.</p>
</dd>
<dt><code>special_tokens</code></dt><dd><p>A list of special tokens the model should be aware of.</p>
</dd>
<dt><code>shrinking_factor</code></dt><dd><p>The shrinking factor used at each step of training
to prune the vocabulary.</p>
</dd>
<dt><code>unk_token</code></dt><dd><p>The token used for out-of-vocabulary tokens.</p>
</dd>
<dt><code>max_piece_length</code></dt><dd><p>The maximum length of a given token.</p>
</dd>
<dt><code>n_sub_iterations</code></dt><dd><p>The number of iterations of the EM algorithm to perform
before pruning the vocabulary.</p>
</dd>
<dt><code>initial_alphabet</code></dt><dd><p>A list of characters to include in the initial alphabet,
even if not seen in the training dataset. If the strings contain more than
one character, only the first one is kept.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-tok_trainer_unigram-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>trainer_unigram$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>See Also</h3>

<p>Other trainer: 
<code><a href="#topic+tok_trainer">tok_trainer</a></code>,
<code><a href="#topic+trainer_bpe">trainer_bpe</a></code>,
<code><a href="#topic+trainer_wordpiece">trainer_wordpiece</a></code>
</p>

<hr>
<h2 id='trainer_wordpiece'>WordPiece tokenizer trainer</h2><span id='topic+trainer_wordpiece'></span>

<h3>Description</h3>

<p>WordPiece tokenizer trainer
</p>
<p>WordPiece tokenizer trainer
</p>


<h3>Super class</h3>

<p><code><a href="#topic+tok_trainer">tok::tok_trainer</a></code> -&gt; <code>tok_trainer_wordpiece</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-tok_trainer_wordpiece-new"><code>trainer_wordpiece$new()</code></a>
</p>
</li>
<li> <p><a href="#method-tok_trainer_wordpiece-clone"><code>trainer_wordpiece$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-tok_trainer_wordpiece-new"></a>



<h4>Method <code>new()</code></h4>

<p>Constructor for the WordPiece tokenizer trainer
</p>


<h5>Usage</h5>

<div class="r"><pre>trainer_wordpiece$new(
  vocab_size = 30000,
  min_frequency = 0,
  show_progress = FALSE,
  special_tokens = NULL,
  limit_alphabet = NULL,
  initial_alphabet = NULL,
  continuing_subword_prefix = "##",
  end_of_word_suffix = NULL
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>vocab_size</code></dt><dd><p>The size of the final vocabulary, including all tokens and alphabet.
Default: <code>NULL</code>.</p>
</dd>
<dt><code>min_frequency</code></dt><dd><p>The minimum frequency a pair should have in order to be merged.
Default: <code>NULL</code>.</p>
</dd>
<dt><code>show_progress</code></dt><dd><p>Whether to show progress bars while training. Default: <code>TRUE</code>.</p>
</dd>
<dt><code>special_tokens</code></dt><dd><p>A list of special tokens the model should be aware of.
Default: <code>NULL</code>.</p>
</dd>
<dt><code>limit_alphabet</code></dt><dd><p>The maximum number of different characters to keep in the alphabet.
Default: <code>NULL</code>.</p>
</dd>
<dt><code>initial_alphabet</code></dt><dd><p>A list of characters to include in the initial alphabet,
even if not seen in the training dataset. If the strings contain more than
one character, only the first one is kept. Default: <code>NULL</code>.</p>
</dd>
<dt><code>continuing_subword_prefix</code></dt><dd><p>A prefix to be used for every subword that is not a beginning-of-word.
Default: <code>NULL</code>.</p>
</dd>
<dt><code>end_of_word_suffix</code></dt><dd><p>A suffix to be used for every subword that is an end-of-word.
Default: <code>NULL</code>.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-tok_trainer_wordpiece-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>trainer_wordpiece$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>See Also</h3>

<p>Other trainer: 
<code><a href="#topic+tok_trainer">tok_trainer</a></code>,
<code><a href="#topic+trainer_bpe">trainer_bpe</a></code>,
<code><a href="#topic+trainer_unigram">trainer_unigram</a></code>
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
