<!DOCTYPE html><html><head><title>Help for package tok</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {tok}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#encoding'><p>Encoding</p></a></li>
<li><a href='#tokenizer'><p>Tokenizer</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Fast Text Tokenization</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.1</td>
</tr>
<tr>
<td>Description:</td>
<td>
  Interfaces with the 'Hugging Face' tokenizers library to provide implementations
  of today's most used tokenizers such as the 'Byte-Pair Encoding' algorithm 
  <a href="https://huggingface.co/docs/tokenizers/index">https://huggingface.co/docs/tokenizers/index</a>. It's extremely fast for both 
  training new vocabularies and tokenizing texts.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>Rust tool chain w/ cargo, libclang/llvm-config</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.2.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>R6, cli</td>
</tr>
<tr>
<td>Suggests:</td>
<td>rmarkdown, testthat (&ge; 3.0.0), hfhub, withr</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/mlverse/tok">https://github.com/mlverse/tok</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/mlverse/tok/issues">https://github.com/mlverse/tok/issues</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-08-16 16:36:36 UTC; dfalbel</td>
</tr>
<tr>
<td>Author:</td>
<td>Daniel Falbel [aut, cre],
  Posit [cph]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Daniel Falbel &lt;daniel@posit.co&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-08-17 23:30:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='encoding'>Encoding</h2><span id='topic+encoding'></span>

<h3>Description</h3>

<p>Represents the output of a <a href="#topic+tokenizer">tokenizer</a>.
</p>


<h3>Value</h3>

<p>An encoding object containing encoding information such as attention masks
and token ids.
</p>


<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>.encoding</code></dt><dd><p>The underlying implementation pointer.</p>
</dd>
</dl>

</div>


<h3>Active bindings</h3>

<div class="r6-active-bindings">

<dl>
<dt><code>ids</code></dt><dd><p>The IDs are the main input to a Language Model. They are the
token indices, the numerical representations that a LM understands.</p>
</dd>
<dt><code>attention_mask</code></dt><dd><p>The attention mask used as input for transformers models.</p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-tok_encoding-new"><code>encoding$new()</code></a>
</p>
</li>
<li> <p><a href="#method-tok_encoding-clone"><code>encoding$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-tok_encoding-new"></a>



<h4>Method <code>new()</code></h4>

<p>Initializes an encoding object (Not to use directly)
</p>


<h5>Usage</h5>

<div class="r"><pre>encoding$new(encoding)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>encoding</code></dt><dd><p>an encoding implementation object</p>
</dd>
</dl>

</div>


<hr>
<a id="method-tok_encoding-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>encoding$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>Examples</h3>

<pre><code class='language-R'>withr::with_envvar(c(HUGGINGFACE_HUB_CACHE = tempdir()), {
try({
tok &lt;- tokenizer$from_pretrained("gpt2")
encoding &lt;- tok$encode("Hello world")
encoding
})
})
</code></pre>

<hr>
<h2 id='tokenizer'>Tokenizer</h2><span id='topic+tokenizer'></span>

<h3>Description</h3>

<p>A Tokenizer works as a pipeline. It processes some raw text as input and outputs
an <a href="#topic+encoding">encoding</a>.
</p>


<h3>Value</h3>

<p>A tokenizer that can be used for encoding character strings or decoding
integers.
</p>


<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>.tokenizer</code></dt><dd><p>(unsafe usage) Lower level pointer to tokenizer</p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-tok_tokenizer-new"><code>tokenizer$new()</code></a>
</p>
</li>
<li> <p><a href="#method-tok_tokenizer-encode"><code>tokenizer$encode()</code></a>
</p>
</li>
<li> <p><a href="#method-tok_tokenizer-decode"><code>tokenizer$decode()</code></a>
</p>
</li>
<li> <p><a href="#method-tok_tokenizer-encode_batch"><code>tokenizer$encode_batch()</code></a>
</p>
</li>
<li> <p><a href="#method-tok_tokenizer-decode_batch"><code>tokenizer$decode_batch()</code></a>
</p>
</li>
<li> <p><a href="#method-tok_tokenizer-from_file"><code>tokenizer$from_file()</code></a>
</p>
</li>
<li> <p><a href="#method-tok_tokenizer-from_pretrained"><code>tokenizer$from_pretrained()</code></a>
</p>
</li>
<li> <p><a href="#method-tok_tokenizer-clone"><code>tokenizer$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-tok_tokenizer-new"></a>



<h4>Method <code>new()</code></h4>

<p>Initializes a tokenizer
</p>


<h5>Usage</h5>

<div class="r"><pre>tokenizer$new(tokenizer)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>tokenizer</code></dt><dd><p>Will be cloned to initialize a new tokenizer</p>
</dd>
</dl>

</div>


<hr>
<a id="method-tok_tokenizer-encode"></a>



<h4>Method <code>encode()</code></h4>

<p>Encode the given sequence and pair. This method can process raw text sequences
as well as already pre-tokenized sequences.
</p>


<h5>Usage</h5>

<div class="r"><pre>tokenizer$encode(
  sequence,
  pair = NULL,
  is_pretokenized = FALSE,
  add_special_tokens = TRUE
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>sequence</code></dt><dd><p>The main input sequence we want to encode. This sequence can
be either raw text or pre-tokenized, according to the is_pretokenized argument</p>
</dd>
<dt><code>pair</code></dt><dd><p>An optional input sequence. The expected format is the same
that for sequence.</p>
</dd>
<dt><code>is_pretokenized</code></dt><dd><p>Whether the input is already pre-tokenized</p>
</dd>
<dt><code>add_special_tokens</code></dt><dd><p>Whether to add the special tokens</p>
</dd>
</dl>

</div>


<hr>
<a id="method-tok_tokenizer-decode"></a>



<h4>Method <code>decode()</code></h4>

<p>Decode the given list of ids back to a string
</p>


<h5>Usage</h5>

<div class="r"><pre>tokenizer$decode(ids, skip_special_tokens = TRUE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>ids</code></dt><dd><p>The list of ids that we want to decode</p>
</dd>
<dt><code>skip_special_tokens</code></dt><dd><p>Whether the special tokens should be removed from the decoded string</p>
</dd>
</dl>

</div>


<hr>
<a id="method-tok_tokenizer-encode_batch"></a>



<h4>Method <code>encode_batch()</code></h4>

<p>Encodes a batch of sequences. Returns a list of <a href="#topic+encoding">encoding</a>s.
</p>


<h5>Usage</h5>

<div class="r"><pre>tokenizer$encode_batch(
  input,
  is_pretokenized = FALSE,
  add_special_tokens = TRUE
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>input</code></dt><dd><p>A list of single sequences or pair sequences to encode. Each
sequence can be either raw text or pre-tokenized, according to the is_pretokenized
argument.</p>
</dd>
<dt><code>is_pretokenized</code></dt><dd><p>Whether the input is already pre-tokenized</p>
</dd>
<dt><code>add_special_tokens</code></dt><dd><p>Whether to add the special tokens</p>
</dd>
</dl>

</div>


<hr>
<a id="method-tok_tokenizer-decode_batch"></a>



<h4>Method <code>decode_batch()</code></h4>

<p>Decode a batch of ids back to their corresponding string
</p>


<h5>Usage</h5>

<div class="r"><pre>tokenizer$decode_batch(sequences, skip_special_tokens = TRUE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>sequences</code></dt><dd><p>The batch of sequences we want to decode</p>
</dd>
<dt><code>skip_special_tokens</code></dt><dd><p>Whether the special tokens should be removed from the decoded strings</p>
</dd>
</dl>

</div>


<hr>
<a id="method-tok_tokenizer-from_file"></a>



<h4>Method <code>from_file()</code></h4>

<p>Creates a tokenizer from the path of a serialized tokenizer.
This is a static method and should be called instead of <code style="white-space: pre;">&#8288;$new&#8288;</code> when initializing
the tokenizer.
</p>


<h5>Usage</h5>

<div class="r"><pre>tokenizer$from_file(path)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>path</code></dt><dd><p>Path to tokenizer.json file</p>
</dd>
</dl>

</div>


<hr>
<a id="method-tok_tokenizer-from_pretrained"></a>



<h4>Method <code>from_pretrained()</code></h4>

<p>Instantiate a new Tokenizer from an existing file on the Hugging Face Hub.
</p>


<h5>Usage</h5>

<div class="r"><pre>tokenizer$from_pretrained(identifier, revision = "main", auth_token = NULL)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>identifier</code></dt><dd><p>The identifier of a Model on the Hugging Face Hub, that
contains a tokenizer.json file</p>
</dd>
<dt><code>revision</code></dt><dd><p>A branch or commit id</p>
</dd>
<dt><code>auth_token</code></dt><dd><p>An optional auth token used to access private repositories
on the Hugging Face Hub</p>
</dd>
</dl>

</div>


<hr>
<a id="method-tok_tokenizer-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>tokenizer$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>Examples</h3>

<pre><code class='language-R'>withr::with_envvar(c(HUGGINGFACE_HUB_CACHE = tempdir()), {
try({
tok &lt;- tokenizer$from_pretrained("gpt2")
tok$encode("Hello world")$ids
})
})

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
