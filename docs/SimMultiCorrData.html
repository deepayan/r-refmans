<!DOCTYPE html><html><head><title>Help for package SimMultiCorrData</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {SimMultiCorrData}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#calc_final_corr'><p>Calculate Final Correlation Matrix</p></a></li>
<li><a href='#calc_fisherk'><p>Find Standardized Cumulants of Data based on Fisher's k-statistics</p></a></li>
<li><a href='#calc_lower_skurt'><p>Find Lower Boundary of Standardized Kurtosis for Polynomial Transformation</p></a></li>
<li><a href='#calc_moments'><p>Find Standardized Cumulants of Data by Method of Moments</p></a></li>
<li><a href='#calc_theory'><p>Find Theoretical Standardized Cumulants for Continuous Distributions</p></a></li>
<li><a href='#cdf_prob'><p>Calculate Theoretical Cumulative Probability for Continuous Variables</p></a></li>
<li><a href='#chat_nb'><p>Calculate Upper Frechet-Hoeffding Correlation Bound: Negative Binomial - Normal Variables</p></a></li>
<li><a href='#chat_pois'><p>Calculate Upper Frechet-Hoeffding Correlation Bound: Poisson - Normal Variables</p></a></li>
<li><a href='#denom_corr_cat'><p>Calculate Denominator Used in Intercorrelations Involving Ordinal Variables</p></a></li>
<li><a href='#error_loop'><p>Error Loop to Correct Final Correlation of Simulated Variables</p></a></li>
<li><a href='#error_vars'><p>Generate Variables for Error Loop</p></a></li>
<li><a href='#find_constants'><p>Find Power Method Transformation Constants</p></a></li>
<li><a href='#findintercorr'><p>Calculate Intermediate MVN Correlation for Ordinal, Continuous, Poisson, or Negative Binomial Variables: Correlation Method 1</p></a></li>
<li><a href='#findintercorr_cat_nb'><p>Calculate Intermediate MVN Correlation for Ordinal - Negative Binomial Variables: Correlation Method 1</p></a></li>
<li><a href='#findintercorr_cat_pois'><p>Calculate Intermediate MVN Correlation for Ordinal - Poisson Variables: Correlation Method 1</p></a></li>
<li><a href='#findintercorr_cont'><p>Calculate Intermediate MVN Correlation for Continuous Variables Generated by Polynomial Transformation</p></a></li>
<li><a href='#findintercorr_cont_cat'><p>Calculate Intermediate MVN Correlation for Continuous - Ordinal Variables</p></a></li>
<li><a href='#findintercorr_cont_nb'><p>Calculate Intermediate MVN Correlation for Continuous - Negative Binomial Variables: Correlation Method 1</p></a></li>
<li><a href='#findintercorr_cont_nb2'><p>Calculate Intermediate MVN Correlation for Continuous - Negative Binomial Variables: Correlation Method 2</p></a></li>
<li><a href='#findintercorr_cont_pois'><p>Calculate Intermediate MVN Correlation for Continuous - Poisson Variables: Correlation Method 1</p></a></li>
<li><a href='#findintercorr_cont_pois2'><p>Calculate Intermediate MVN Correlation for Continuous - Poisson Variables: Correlation Method 2</p></a></li>
<li><a href='#findintercorr_nb'><p>Calculate Intermediate MVN Correlation for Negative Binomial Variables: Correlation Method 1</p></a></li>
<li><a href='#findintercorr_pois'><p>Calculate Intermediate MVN Correlation for Poisson Variables: Correlation Method 1</p></a></li>
<li><a href='#findintercorr_pois_nb'><p>Calculate Intermediate MVN Correlation for Poisson - Negative Binomial Variables: Correlation Method 1</p></a></li>
<li><a href='#findintercorr2'><p>Calculate Intermediate MVN Correlation for Ordinal, Continuous, Poisson, or Negative Binomial Variables: Correlation Method 2</p></a></li>
<li><a href='#fleish'><p>Fleishman's Third-Order Polynomial Transformation Equations</p></a></li>
<li><a href='#fleish_Hessian'><p>Fleishman's Third-Order Transformation Hessian Calculation for Lower Boundary of Standardized Kurtosis in Asymmetric Distributions</p></a></li>
<li><a href='#fleish_skurt_check'><p>Fleishman's Third-Order Transformation Lagrangean Constraints for Lower Boundary of Standardized Kurtosis in Asymmetric Distributions</p></a></li>
<li><a href='#H_params'><p>Parameters for Examples of Constants Calculated by Headrick's Fifth-Order Polynomial Transformation</p></a></li>
<li><a href='#Headrick.dist'><p>Examples of Constants Calculated by Headrick's Fifth-Order Polynomial Transformation</p></a></li>
<li><a href='#intercorr_fleish'><p>Fleishman's Third-Order Polynomial Transformation Intermediate Correlation Equations</p></a></li>
<li><a href='#intercorr_poly'><p>Headrick's Fifth-Order Polynomial Transformation Intermediate Correlation Equations</p></a></li>
<li><a href='#max_count_support'><p>Calculate Maximum Support Value for Count Variables: Correlation Method 2</p></a></li>
<li><a href='#nonnormvar1'><p>Generation of One Non-Normal Continuous Variable Using the Power Method</p></a></li>
<li><a href='#ordnorm'><p>Calculate Intermediate MVN Correlation to Generate Variables Treated as Ordinal</p></a></li>
<li><a href='#pdf_check'><p>Check Polynomial Transformation Constants for Valid Power Method PDF</p></a></li>
<li><a href='#plot_cdf'><p>Plot Theoretical Power Method Cumulative Distribution Function for Continuous Variables</p></a></li>
<li><a href='#plot_pdf_ext'><p>Plot Theoretical Power Method Probability Density Function and Target PDF of External Data for Continuous Variables</p></a></li>
<li><a href='#plot_pdf_theory'><p>Plot Theoretical Power Method Probability Density Function and Target PDF by Distribution Name or Function for Continuous Variables</p></a></li>
<li><a href='#plot_sim_cdf'><p>Plot Simulated (Empirical) Cumulative Distribution Function for Continuous, Ordinal, or Count Variables</p></a></li>
<li><a href='#plot_sim_ext'><p>Plot Simulated Data and Target External Data for Continuous or Count Variables</p></a></li>
<li><a href='#plot_sim_pdf_ext'><p>Plot Simulated Probability Density Function and Target PDF of External Data for Continuous or Count Variables</p></a></li>
<li><a href='#plot_sim_pdf_theory'><p>Plot Simulated Probability Density Function and Target PDF by Distribution Name or Function for Continuous or Count Variables</p></a></li>
<li><a href='#plot_sim_theory'><p>Plot Simulated Data and Target Distribution Data by Name or Function for Continuous or Count Variables</p></a></li>
<li><a href='#poly'><p>Headrick's Fifth-Order Polynomial Transformation Equations</p></a></li>
<li><a href='#poly_skurt_check'><p>Headrick's Fifth-Order Transformation Lagrangean Constraints for Lower Boundary of Standardized Kurtosis</p></a></li>
<li><a href='#power_norm_corr'><p>Calculate Power Method Correlation</p></a></li>
<li><a href='#rcorrvar'><p>Generation of Correlated Ordinal, Continuous, Poisson, and/or Negative Binomial Variables: Correlation Method 1</p></a></li>
<li><a href='#rcorrvar2'><p>Generation of Correlated Ordinal, Continuous, Poisson, and/or Negative Binomial Variables: Correlation Method 2</p></a></li>
<li><a href='#separate_rho'><p>Separate Target Correlation Matrix by Variable Type</p></a></li>
<li><a href='#sim_cdf_prob'><p>Calculate Simulated (Empirical) Cumulative Probability</p></a></li>
<li><a href='#SimMultiCorrData'><p>Simulation of Correlated Data with Multiple Variable Types</p></a></li>
<li><a href='#stats_pdf'><p>Calculate Theoretical Statistics for a Valid Power Method PDF</p></a></li>
<li><a href='#valid_corr'><p>Determine Correlation Bounds for Ordinal, Continuous, Poisson, and/or Negative Binomial Variables: Correlation Method 1</p></a></li>
<li><a href='#valid_corr2'><p>Determine Correlation Bounds for Ordinal, Continuous, Poisson, and/or Negative Binomial Variables: Correlation Method 2</p></a></li>
<li><a href='#var_cat'><p>Calculate Variance of Binary or Ordinal Variable</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Simulation of Correlated Data with Multiple Variable Types</td>
</tr>
<tr>
<td>Version:</td>
<td>0.2.2</td>
</tr>
<tr>
<td>Author:</td>
<td>Allison Cynthia Fialkowski</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Allison Cynthia Fialkowski &lt;allijazz@uab.edu&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Generate continuous (normal or non-normal), binary, ordinal, and count (Poisson or Negative 
    Binomial) variables with a specified correlation matrix.  It can also produce a single continuous 
    variable.  This package can be used to simulate data sets that mimic real-world situations (i.e. 
    clinical or genetic data sets, plasmodes).  All variables are generated from standard normal 
    variables with an imposed intermediate correlation matrix.  Continuous variables are simulated 
    by specifying mean, variance, skewness, standardized kurtosis, and fifth and sixth standardized 
    cumulants using either Fleishman's third-order (&lt;<a href="https://doi.org/10.1007%2FBF02293811">doi:10.1007/BF02293811</a>&gt;) or Headrick's 
    fifth-order (&lt;<a href="https://doi.org/10.1016%2FS0167-9473%2802%2900072-5">doi:10.1016/S0167-9473(02)00072-5</a>&gt;) polynomial transformation.  Binary and 
    ordinal variables are simulated using a modification of the ordsample() function from 'GenOrd'.  
    Count variables are simulated using the inverse cdf method.  There are two simulation pathways 
    which differ primarily according to the calculation of the intermediate correlation matrix.  In 
    Correlation Method 1, the intercorrelations involving count variables are determined using a 
    simulation based, logarithmic correlation correction (adapting Yahav and Shmueli's 2012 method, 
    &lt;<a href="https://doi.org/10.1002%2Fasmb.901">doi:10.1002/asmb.901</a>&gt;).  In Correlation Method 2, the count variables are treated as ordinal 
    (adapting Barbiero and Ferrari's 2015 modification of GenOrd, &lt;<a href="https://doi.org/10.1002%2Fasmb.2072">doi:10.1002/asmb.2072</a>&gt;).  
    There is an optional error loop that corrects the final correlation matrix to be within a 
    user-specified precision value of the target matrix.  The package also includes functions to 
    calculate standardized cumulants for theoretical distributions or from real data sets, check 
    if a target correlation matrix is within the possible correlation bounds (given the distributions 
    of the simulated variables), summarize results (numerically or graphically), to verify valid power 
    method pdfs, and to calculate lower standardized kurtosis bounds.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.3.0)</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>Imports:</td>
<td>BB, nleqslv, GenOrd, psych, Matrix, VGAM, triangle, ggplot2,
grid, stats, utils</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>6.0.1</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown, printr, testthat</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/AFialkowski/SimMultiCorrData">https://github.com/AFialkowski/SimMultiCorrData</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2018-06-28 17:07:14 UTC; Allison</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2018-06-28 17:37:55 UTC</td>
</tr>
</table>
<hr>
<h2 id='calc_final_corr'>Calculate Final Correlation Matrix</h2><span id='topic+calc_final_corr'></span>

<h3>Description</h3>

<p>This function calculates the final correlation matrix based on simulated variable type (ordinal, continuous, Poisson, and/or
Negative Binomial).  The function is used in <code><a href="#topic+rcorrvar">rcorrvar</a></code> and
<code><a href="#topic+rcorrvar2">rcorrvar2</a></code>.  This would not ordinarily be called directly by the user.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calc_final_corr(k_cat, k_cont, k_pois, k_nb, Y_cat, Yb, Y_pois, Y_nb)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="calc_final_corr_+3A_k_cat">k_cat</code></td>
<td>
<p>the number of ordinal (r &gt;= 2 categories) variables</p>
</td></tr>
<tr><td><code id="calc_final_corr_+3A_k_cont">k_cont</code></td>
<td>
<p>the number of continuous variables</p>
</td></tr>
<tr><td><code id="calc_final_corr_+3A_k_pois">k_pois</code></td>
<td>
<p>the number of Poisson variables</p>
</td></tr>
<tr><td><code id="calc_final_corr_+3A_k_nb">k_nb</code></td>
<td>
<p>the number of Negative Binomial variables</p>
</td></tr>
<tr><td><code id="calc_final_corr_+3A_y_cat">Y_cat</code></td>
<td>
<p>the ordinal (r &gt;= 2 categories) variables</p>
</td></tr>
<tr><td><code id="calc_final_corr_+3A_yb">Yb</code></td>
<td>
<p>the continuous variables</p>
</td></tr>
<tr><td><code id="calc_final_corr_+3A_y_pois">Y_pois</code></td>
<td>
<p>the Poisson variables</p>
</td></tr>
<tr><td><code id="calc_final_corr_+3A_y_nb">Y_nb</code></td>
<td>
<p>the Negative Binomial variables</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a correlation matrix
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rcorrvar">rcorrvar</a></code>, <code><a href="#topic+rcorrvar2">rcorrvar2</a></code>
</p>

<hr>
<h2 id='calc_fisherk'>Find Standardized Cumulants of Data based on Fisher's k-statistics</h2><span id='topic+calc_fisherk'></span>

<h3>Description</h3>

<p>This function uses Fisher's k-statistics to calculate the mean, standard deviation, skewness,
standardized kurtosis, and standardized fifth and sixth cumulants given a vector of data.  The result can be used
as input to <code><a href="#topic+find_constants">find_constants</a></code> or for data simulation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calc_fisherk(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="calc_fisherk_+3A_x">x</code></td>
<td>
<p>a vector of data</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of the mean, standard deviation, skewness, standardized kurtosis, and standardized fifth and sixth cumulants
</p>


<h3>References</h3>

<p>Fisher RA (1928). Moments and Product Moments of Sampling Distributions. Proc. London Math. Soc. 30, 199-238. doi: <a href="http://doi.org/10.1112/plms/s2-30.1.199">10.1112/plms/s2-30.1.199</a>.
</p>
<p>Headrick TC, Sheng Y, &amp; Hodis FA (2007). Numerical Computing and Graphics for the Power Method Transformation Using
Mathematica. Journal of Statistical Software, 19(3), 1 - 17. doi: <a href="http://doi.org/10.18637/jss.v019.i03">10.18637/jss.v019.i03</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+calc_theory">calc_theory</a></code>, <code><a href="#topic+calc_moments">calc_moments</a></code>,
<code><a href="#topic+find_constants">find_constants</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- rgamma(n = 10000, 10, 10)
calc_fisherk(x)
</code></pre>

<hr>
<h2 id='calc_lower_skurt'>Find Lower Boundary of Standardized Kurtosis for Polynomial Transformation</h2><span id='topic+calc_lower_skurt'></span>

<h3>Description</h3>

<p>This function calculates the lower boundary of standardized kurtosis for Fleishman's Third-Order (<code>method</code> = &quot;Fleishman&quot;,
doi: <a href="http://doi.org/10.1007/BF02293811">10.1007/BF02293811</a>) or Headrick's Fifth-Order (<code>method</code> = &quot;Polynomial&quot;, doi: <a href="http://doi.org/10.1016/S0167-9473(02)00072-5">10.1016/S0167-9473(02)00072-5</a>), given values of skewness and standardized fifth and sixth
cumulants.  It uses <code><a href="nleqslv.html#topic+nleqslv">nleqslv</a></code> to search for solutions to the multi-constraint Lagrangean expression in either
<code><a href="#topic+fleish_skurt_check">fleish_skurt_check</a></code> or <code><a href="#topic+poly_skurt_check">poly_skurt_check</a></code>.  When Headrick's method
is used (<code>method</code> = &quot;Polynomial&quot;), if no solutions converge and a vector of sixth cumulant correction values (<code>Six</code>) is
provided, the smallest value is found that yields solutions.  Otherwise, the function stops with an error.
</p>
<p>Each set of constants is checked for a positive correlation with the underlying normal variable
(using <code><a href="#topic+power_norm_corr">power_norm_corr</a></code>) and a valid power method pdf (using <code><a href="#topic+pdf_check">pdf_check</a></code>).
If the correlation is &lt;= 0, the signs of c1 and c3 are reversed (for <code>method</code> = &quot;Fleishman&quot;),
or c1, c3, and c5 (for <code>method</code> = &quot;Polynomial&quot;).  It will return a kurtosis value with constants that yield in invalid pdf
if no other solutions can be found (<code>valid.pdf</code> = &quot;FALSE&quot;).  If a vector of kurtosis correction values (<code>Skurt</code>) is provided, the function
finds the smallest value that produces a kurtosis with constants that yield a valid pdf.  If valid pdf constants
still can not be found, the original invalid pdf constants (calculated without a correction) will be provided.  If no solutions
can be found, an error is given and the function stops.  Please note that this function can take
considerable computation time, depending on the number of starting values (n) and lengths of kurtosis (<code>Skurt</code>) and sixth cumulant
(<code>Six</code>) correction vectors.  Different seeds should be tested to see if a lower boundary can be found.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calc_lower_skurt(method = c("Fleishman", "Polynomial"), skews = NULL,
  fifths = NULL, sixths = NULL, Skurt = NULL, Six = NULL,
  xstart = NULL, seed = 104, n = 50)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="calc_lower_skurt_+3A_method">method</code></td>
<td>
<p>the method used to find the constants.  &quot;Fleishman&quot; uses a third-order polynomial transformation and
requires only a skewness input.  &quot;Polynomial&quot; uses Headrick's fifth-order transformation and requires skewness plus standardized
fifth and sixth cumulants.</p>
</td></tr>
<tr><td><code id="calc_lower_skurt_+3A_skews">skews</code></td>
<td>
<p>the skewness value</p>
</td></tr>
<tr><td><code id="calc_lower_skurt_+3A_fifths">fifths</code></td>
<td>
<p>the standardized fifth cumulant (if <code>method</code> = &quot;Fleishman&quot;, keep NULL)</p>
</td></tr>
<tr><td><code id="calc_lower_skurt_+3A_sixths">sixths</code></td>
<td>
<p>the standardized sixth cumulant (if <code>method</code> = &quot;Fleishman&quot;, keep NULL)</p>
</td></tr>
<tr><td><code id="calc_lower_skurt_+3A_skurt">Skurt</code></td>
<td>
<p>a vector of correction values to add to the lower kurtosis boundary if the constants yield an invalid pdf,
ex: <code>Skurt</code> = seq(0.1, 10, by = 0.1)</p>
</td></tr>
<tr><td><code id="calc_lower_skurt_+3A_six">Six</code></td>
<td>
<p>a vector of correction values to add to the sixth cumulant if no solutions converged,
ex: <code>Six</code> = seq(0.05, 2, by = 0.05)</p>
</td></tr>
<tr><td><code id="calc_lower_skurt_+3A_xstart">xstart</code></td>
<td>
<p>initial value for root-solving algorithm (see <code><a href="nleqslv.html#topic+nleqslv">nleqslv</a></code>).  If user specified,
must be input as a matrix. If NULL generates n sets of random starting values from
uniform distributions.</p>
</td></tr>
<tr><td><code id="calc_lower_skurt_+3A_seed">seed</code></td>
<td>
<p>the seed value for random starting value generation (default = 104)</p>
</td></tr>
<tr><td><code id="calc_lower_skurt_+3A_n">n</code></td>
<td>
<p>the number of initial starting values to use (default = 50).  More starting values require more calculation time.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with components:
</p>
<p><code>Min</code>    a data.frame containing the skewness, fifth and sixth standardized cumulants (if <code>method</code> = &quot;Polynomial&quot;), constants,
a valid.pdf column indicating whether or not the constants generate a valid power method pdf, and the minimum value of standardized
kurtosis (&quot;skurtosis&quot;)
</p>
<p><code>C</code>    a data.frame of valid power method pdf solutions, containing the skewness, fifth and sixth standardized cumulants
(if <code>method</code> = &quot;Polynomial&quot;), constants, a valid.pdf column indicating TRUE, and all values of standardized kurtosis (&quot;skurtosis&quot;).
If the Lagrangean equations yielded valid pdf solutions, this will also include the lambda values, and for <code>method</code> = &quot;Fleishman&quot;, the
Hessian determinant and a minimum column indicating TRUE if the solutions give a minimum kurtosis.  If the Lagrangean equations
yielded invalid pdf solutions, this data.frame contains constants calculated from <code><a href="#topic+find_constants">find_constants</a></code>
using the kurtosis correction.
</p>
<p><code>Invalid.C</code>    if the Lagrangean equations yielded invalid pdf solutions, a data.frame containing the skewness, fifth and sixth
standardized cumulants (if <code>method</code> = &quot;Polynomial&quot;), constants, lambda values, a valid.pdf column indicating FALSE, and all values
of standardized kurtosis (&quot;skurtosis&quot;).  If <code>method</code> = &quot;Fleishman&quot;, also the
Hessian determinant and a minimum column indicating TRUE if the solutions give a minimum kurtosis.
</p>
<p><code>Time</code>    the total calculation time in minutes
</p>
<p><code>start</code>    a matrix of starting values used in root-solver
</p>
<p><code>SixCorr1</code>    if Six is specified, the sixth cumulant correction required to achieve converged solutions
</p>
<p><code>SkurtCorr1</code>    if Skurt is specified, the kurtosis correction required to achieve a valid power method pdf (or the maximum value
attempted if no valid pdf solutions could be found)
</p>


<h3>Notes on Fleishman Method</h3>

<p>The Fleishman method <em>can not generate valid power method distributions</em> with a ratio of <code class="reqn">skew^2/skurtosis &gt; 9/14</code>, where skurtosis is kurtosis - 3.
This prevents the method from being used for any of the <em>Chi-squared distributions</em>, which have a constant ratio of
<code class="reqn">skew^2/skurtosis = 2/3</code>.
</p>
<p><b>Symmetric Distributions:</b> All symmetric distributions (which have skew = 0) possess the same lower kurtosis boundary.  This is solved for
using <code><a href="stats.html#topic+optimize">optimize</a></code> and the equations in Headrick &amp; Sawilowsky (2002, doi: <a href="http://doi.org/10.3102/10769986025004417">10.3102/10769986025004417</a>).
The result will always be: c0 = 0, c1 = 1.341159,
c2 = 0, c3 = -0.1314796, and minimum standardized kurtosis = -1.151323.  Note that this set of constants does NOT generate a valid
power method pdf.  If a <code>Skurt</code> vector of kurtosis correction values is provided, the function will find the smallest addition that yields a
valid pdf.  This value is 1.16, giving a lower kurtosis boundary of 0.008676821.
</p>
<p><b>Asymmetric Distributions:</b> Due to the square roots involved in the calculation of the lower kurtosis boundary (see Headrick &amp; Sawilowsky, 2002),
this function uses the absolute value of the skewness.  If the true skewness is less than zero, the signs on the constants c0 and c2 are
switched after calculations (which changes skewness from positive to negative without affecting kurtosis).
</p>
<p><b>Verification of Minimum Kurtosis:</b> Since differentiability is a local property, it is possible to obtain a local, instead of a global, minimum.
For the Fleishman method, Headrick &amp; Sawilowsky (2002) explain that since the equation for kurtosis is not &quot;quasiconvex on the domain
consisting only of the nonnegative orthant,&quot; second-order conditions must be verified.  The solutions for
lambda, c1, and c3 generate a global kurtosis minimum if and only if the determinant of a bordered Hessian is less than zero.  Therefore,
this function first obtains the solutions to the Lagrangean expression in <code><a href="#topic+fleish_skurt_check">fleish_skurt_check</a></code> for a given
skewness value.  These are used to calculate the standardized kurtosis, the constants c1 and c3, and the Hessian determinant
(using <code><a href="#topic+fleish_Hessian">fleish_Hessian</a></code>).  If this determinant is less than zero, the kurtosis is indicated as a minimum.
The constants c0, c1, c2, and c3 are checked to see if they yield a continuous variable with a positive correlation with the generating
standard normal variable (using <code><a href="#topic+power_norm_corr">power_norm_corr</a></code>).  If not, the signs of c1 and c3 are switched.
The final set of constants is checked to see if they generate a valid power method pdf (using <code><a href="#topic+pdf_check">pdf_check</a></code>).
If a <code>Skurt</code> vector of kurtosis correction values is provided, the function will find the smallest value that yields a
valid pdf.
</p>


<h3>Notes on Headrick's Method</h3>

<p>The <em>sixth cumulant correction vector</em> (<code>Six</code>) may be used in order to aid in obtaining solutions which converge.  The calculation methods
are the same for symmetric or asymmetric distributions, and for positive or negative skew.
</p>
<p><b>Verification of Minimum Kurtosis:</b> For the fifth-order approximation, Headrick (2002, doi: <a href="http://doi.org/10.1016/S0167-9473(02)00072-5">10.1016/S0167-9473(02)00072-5</a>) states
&quot;it is assumed that the hypersurface of the objective function [for the kurtosis
equation] has the appropriate (quasiconvex) configuration.&quot;  This assumption alleviates the need to check second-order conditions.
Headrick discusses steps he took to verify the kurtosis solution was in fact a minimum, including: 1) substituting the constant solutions
back into the 1st four Lagrangean constraints to ensure the results are zero, 2) substituting the skewness, kurtosis solution, and
standardized fifth and sixth cumulants back into the fifth-order equations to ensure the same constants are produced
(i.e. using <code><a href="#topic+find_constants">find_constants</a></code>), and 3) searching for values below the kurtosis solution that solve the
Lagrangean equation.  This function ensures steps 1 and 2 by the nature of the root-solving algorithm of <code><a href="nleqslv.html#topic+nleqslv">nleqslv</a></code>.
Using a sufficiently large n (and, if necessary, executing the function for different seeds) makes step 3 unnecessary.
</p>


<h3>Reasons for Function Errors</h3>

<p>The most likely cause for function errors is that no solutions to <code><a href="#topic+fleish_skurt_check">fleish_skurt_check</a></code> or
<code><a href="#topic+poly_skurt_check">poly_skurt_check</a></code> converged.  If this happens,
the simulation will stop.  Possible solutions include: a) increasing the number of initial starting values (<code>n</code>),
b) using a different seed, or c) specifying a <code>Six</code> vector of sixth cumulant correction values (for <code>method</code> = &quot;Polynomial&quot;).
If the standardized cumulants are obtained from <code>calc_theory</code>, the user may need to use rounded values as inputs (i.e.
<code>skews = round(skews, 8)</code>).  Due to the nature of the integration involved in <code>calc_theory</code>, the results are
approximations.  Greater accuracy can be achieved by increasing the number of subdivisions (<code>sub</code>) used in the integration
process.  For example, in order to ensure that skew is exactly 0 for symmetric distributions.
</p>


<h3>References</h3>

<p>Fleishman AI (1978). A Method for Simulating Non-normal Distributions. Psychometrika, 43, 521-532. doi: <a href="http://doi.org/10.1007/BF02293811">10.1007/BF02293811</a>.
</p>
<p>Hasselman B (2018). nleqslv: Solve Systems of Nonlinear Equations. R package version 3.3.2.
<a href="https://CRAN.R-project.org/package=nleqslv">https://CRAN.R-project.org/package=nleqslv</a>
</p>
<p>Headrick TC (2002). Fast Fifth-order Polynomial Transforms for Generating Univariate and Multivariate
Non-normal Distributions. Computational Statistics &amp; Data Analysis, 40(4):685-711. doi: <a href="http://doi.org/10.1016/S0167-9473(02)00072-5">10.1016/S0167-9473(02)00072-5</a>.
(<a href="http://www.sciencedirect.com/science/article/pii/S0167947302000725">ScienceDirect</a>)
</p>
<p>Headrick TC (2004). On Polynomial Transformations for Simulating Multivariate Nonnormal Distributions.
Journal of Modern Applied Statistical Methods, 3(1), 65-71. doi: <a href="http://doi.org/10.22237/jmasm/1083370080">10.22237/jmasm/1083370080</a>.
</p>
<p>Headrick TC, Kowalchuk RK (2007). The Power Method Transformation: Its Probability Density Function, Distribution
Function, and Its Further Use for Fitting Data. Journal of Statistical Computation and Simulation, 77, 229-249. doi: <a href="http://doi.org/10.1080/10629360600605065">10.1080/10629360600605065</a>.
</p>
<p>Headrick TC, Sawilowsky SS (1999). Simulating Correlated Non-normal Distributions: Extending the Fleishman Power
Method. Psychometrika, 64, 25-35. doi: <a href="http://doi.org/10.1007/BF02294317">10.1007/BF02294317</a>.
</p>
<p>Headrick TC, Sawilowsky SS (2002). Weighted Simplex Procedures for Determining Boundary Points and Constants for the
Univariate and Multivariate Power Methods. Journal of Educational and Behavioral Statistics, 25, 417-436. doi: <a href="http://doi.org/10.3102/10769986025004417">10.3102/10769986025004417</a>.
</p>
<p>Headrick TC, Sheng Y, &amp; Hodis FA (2007). Numerical Computing and Graphics for the Power Method Transformation Using
Mathematica. Journal of Statistical Software, 19(3), 1 - 17. doi: <a href="http://doi.org/10.18637/jss.v019.i03">10.18637/jss.v019.i03</a>.
</p>


<h3>See Also</h3>

<p><code><a href="nleqslv.html#topic+nleqslv">nleqslv</a></code>, <code><a href="#topic+fleish_skurt_check">fleish_skurt_check</a></code>,
<code><a href="#topic+fleish_Hessian">fleish_Hessian</a></code>, <code><a href="#topic+poly_skurt_check">poly_skurt_check</a></code>,
<code><a href="#topic+power_norm_corr">power_norm_corr</a></code>, <code><a href="#topic+pdf_check">pdf_check</a></code>,
<code><a href="#topic+find_constants">find_constants</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Normal distribution with Fleishman transformation
calc_lower_skurt("Fleishman", 0, 0, 0)

## Not run: 

# This example takes considerable computation time.

# Reproduce Headrick's Table 2 (2002, p.698): note the seed here is 104.
# If you use seed = 1234, you get higher Headrick kurtosis values for V7 and V9.
# This shows the importance of trying different seeds.

options(scipen = 999)

V1 &lt;- c(0, 0, 28.5)
V2 &lt;- c(0.24, -1, 11)
V3 &lt;- c(0.48, -2, 6.25)
V4 &lt;- c(0.72, -2.5, 2.5)
V5 &lt;- c(0.96, -2.25, -0.25)
V6 &lt;- c(1.20, -1.20, -3.08)
V7 &lt;- c(1.44, 0.40, 6)
V8 &lt;- c(1.68, 2.38, 6)
V9 &lt;- c(1.92, 11, 195)
V10 &lt;- c(2.16, 10, 37)
V11 &lt;- c(2.40, 15, 200)

G &lt;- as.data.frame(rbind(V1, V2, V3, V4, V5, V6, V7, V8, V9, V10, V11))
colnames(G) &lt;- c("g1", "g3", "g4")

# kurtosis correction vector (used in case of invalid power method pdf constants)
Skurt &lt;- seq(0.01, 2, 0.01)

# sixth cumulant correction vector (used in case of no converged solutions for
# method = "Polynomial")
Six &lt;- seq(0.1, 10, 0.1)

# Fleishman's Third-order transformation
F_lower &lt;- list()
for (i in 1:nrow(G)) {
  F_lower[[i]] &lt;- calc_lower_skurt("Fleishman", G[i, 1], Skurt = Skurt,
                                   seed = 104)
}

# Headrick's Fifth-order transformation
H_lower &lt;- list()
for (i in 1:nrow(G)) {
  H_lower[[i]] &lt;- calc_lower_skurt("Polynomial", G[i, 1], G[i, 2], G[i, 3],
                                   Skurt = Skurt, Six = Six, seed = 104)
}

# Approximate boundary from PoisBinOrdNonNor
PBON_lower &lt;- G$g1^2 - 2

# Compare results:
# Note: 1) the lower Headrick kurtosis boundary for V4 is slightly lower than the
#          value found by Headrick (-0.480129), and
#       2) the approximate lower kurtosis boundaries used in PoisBinOrdNonNor are
#          much lower than the actual Fleishman boundaries, indicating that the
#          guideline is not accurate.
Lower &lt;- matrix(1, nrow = nrow(G), ncol = 12)
colnames(Lower) &lt;- c("skew", "fifth", "sixth", "H_valid.skurt",
                     "F_valid.skurt", "H_invalid.skurt", "F_invalid.skurt",
                     "PBON_skurt", "H_skurt_corr", "F_skurt_corr",
                     "H_time", "F_time")

for (i in 1:nrow(G)) {
  Lower[i, 1:3] &lt;- as.numeric(G[i, 1:3])
  Lower[i, 4] &lt;- ifelse(H_lower[[i]]$Min[1, "valid.pdf"] == "TRUE",
                        H_lower[[i]]$Min[1, "skurtosis"], NA)
  Lower[i, 5] &lt;- ifelse(F_lower[[i]]$Min[1, "valid.pdf"] == "TRUE",
                        F_lower[[i]]$Min[1, "skurtosis"], NA)
  Lower[i, 6] &lt;- min(H_lower[[i]]$Invalid.C[, "skurtosis"])
  Lower[i, 7] &lt;- min(F_lower[[i]]$Invalid.C[, "skurtosis"])
  Lower[i, 8:12] &lt;- c(PBON_lower[i], H_lower[[i]]$SkurtCorr1,
                      F_lower[[i]]$SkurtCorr1,
                      H_lower[[i]]$Time, F_lower[[i]]$Time)
}
Lower &lt;- as.data.frame(Lower)
print(Lower[, 1:8], digits = 4)

#    skew fifth  sixth H_valid.skurt F_valid.skurt H_invalid.skurt F_invalid.skurt PBON_skurt
# 1  0.00  0.00  28.50       -1.0551      0.008677         -1.3851         -1.1513    -2.0000
# 2  0.24 -1.00  11.00       -0.8600      0.096715         -1.2100         -1.0533    -1.9424
# 3  0.48 -2.00   6.25       -0.5766      0.367177         -0.9266         -0.7728    -1.7696
# 4  0.72 -2.50   2.50       -0.1319      0.808779         -0.4819         -0.3212    -1.4816
# 5  0.96 -2.25  -0.25        0.4934      1.443567          0.1334          0.3036    -1.0784
# 6  1.20 -1.20  -3.08        1.2575      2.256908          0.9075          1.1069    -0.5600
# 7  1.44  0.40   6.00            NA      3.264374          1.7758          2.0944     0.0736
# 8  1.68  2.38   6.00            NA      4.452011          2.7624          3.2720     0.8224
# 9  1.92 11.00 195.00        5.7229      5.837442          4.1729          4.6474     1.6864
# 10 2.16 10.00  37.00            NA      7.411697          5.1993          6.2317     2.6656
# 11 2.40 15.00 200.00            NA      9.182819          6.6066          8.0428     3.7600

Lower[, 9:12]

#    H_skurt_corr F_skurt_corr H_time F_time
# 1          0.33         1.16  1.757  8.227
# 2          0.35         1.15  1.566  8.164
# 3          0.35         1.14  1.630  6.321
# 4          0.35         1.13  1.537  5.568
# 5          0.36         1.14  1.558  5.540
# 6          0.35         1.15  1.602  6.619
# 7          2.00         1.17  9.088  8.835
# 8          2.00         1.18  9.425 11.103
# 9          1.55         1.19  6.776 14.364
# 10         2.00         1.18 11.174 15.382
# 11         2.00         1.14 10.567 18.184

# The 1st 3 columns give the skewness and standardized fifth and sixth cumulants.
# "H_valid.skurt" gives the lower kurtosis boundary that produces a valid power method pdf
#     using Headrick's approximation, with the kurtosis addition given in the "H_skurt_corr"
#     column if necessary.
# "F_valid.skurt" gives the lower kurtosis boundary that produces a valid power method pdf
#     using Fleishman's approximation, with the kurtosis addition given in the "F_skurt_corr"
#     column if necessary.
# "H_invalid.skurt" gives the lower kurtosis boundary that produces an invalid power method
#     pdf using Headrick's approximation, without the use of a kurtosis correction.
# "F_valid.skurt" gives the lower kurtosis boundary that produces an invalid power method
#     pdf using Fleishman's approximation, without the use of a kurtosis correction.
# "PBON_skurt" gives the lower kurtosis boundary approximation used in the PoisBinOrdNonNor
#     package.
# "H_time" gives the computation time (minutes) for Headrick's method.
# "F_time" gives the computation time (minutes) for Fleishman's method.


## End(Not run)
</code></pre>

<hr>
<h2 id='calc_moments'>Find Standardized Cumulants of Data by Method of Moments</h2><span id='topic+calc_moments'></span>

<h3>Description</h3>

<p>This function uses the method of moments to calculate the mean, standard deviation, skewness,
standardized kurtosis, and standardized fifth and sixth cumulants given a vector of data.  The result can be used
as input to <code><a href="#topic+find_constants">find_constants</a></code> or for data simulation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calc_moments(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="calc_moments_+3A_x">x</code></td>
<td>
<p>a vector of data</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of the mean, standard deviation, skewness, standardized kurtosis, and standardized fifth and sixth cumulants
</p>


<h3>References</h3>

<p>Headrick TC (2002). Fast Fifth-order Polynomial Transforms for Generating Univariate and Multivariate
Non-normal Distributions. Computational Statistics &amp; Data Analysis, 40(4):685-711. doi: <a href="http://doi.org/10.1016/S0167-9473(02)00072-5">10.1016/S0167-9473(02)00072-5</a>.
(<a href="http://www.sciencedirect.com/science/article/pii/S0167947302000725">ScienceDirect</a>)
</p>
<p>Headrick TC, Kowalchuk RK (2007). The Power Method Transformation: Its Probability Density Function, Distribution
Function, and Its Further Use for Fitting Data. Journal of Statistical Computation and Simulation, 77, 229-249. doi: <a href="http://doi.org/10.1080/10629360600605065">10.1080/10629360600605065</a>.
</p>
<p>Headrick TC, Sheng Y, &amp; Hodis FA (2007). Numerical Computing and Graphics for the Power Method Transformation Using
Mathematica. Journal of Statistical Software, 19(3), 1 - 17. doi: <a href="http://doi.org/10.18637/jss.v019.i03">10.18637/jss.v019.i03</a>.
</p>
<p>Kendall M &amp; Stuart A (1977). The Advanced Theory of Statistics, 4th Edition. Macmillan, New York.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+calc_fisherk">calc_fisherk</a></code>, <code><a href="#topic+calc_theory">calc_theory</a></code>,
<code><a href="#topic+find_constants">find_constants</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- rgamma(n = 10000, 10, 10)
calc_moments(x)
</code></pre>

<hr>
<h2 id='calc_theory'>Find Theoretical Standardized Cumulants for Continuous Distributions</h2><span id='topic+calc_theory'></span>

<h3>Description</h3>

<p>This function calculates the theoretical mean, standard deviation, skewness,
standardized kurtosis, and standardized fifth and sixth cumulants given either a Distribution name (plus up to 4
parameters) or a pdf (with specified lower and upper support bounds).  The result can be used as input to
<code><a href="#topic+find_constants">find_constants</a></code> or for data simulation.
</p>
<p><b>Note:</b> Due to the nature of the integration involved in calculating the standardized cumulants, the results are
approximations.  Greater accuracy can be achieved by increasing the number of subdivisions (<code>sub</code>) used in the integration
process.  However, the user may need to round the cumulants (i.e. using <code>round(x, 8)</code>) before using them in other functions
(i.e. <code>find_constants</code>, <code>calc_lower_skurt</code>, <code>nonnormvar1</code>, <code>rcorrvar</code>, <code>rcorrvar2</code>) in order to achieve
the desired results.  For example, in order to ensure that skew is exactly 0 for symmetric distributions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calc_theory(Dist = c("Benini", "Beta", "Beta-Normal", "Birnbaum-Saunders",
  "Chisq", "Dagum", "Exponential", "Exp-Geometric", "Exp-Logarithmic",
  "Exp-Poisson", "F", "Fisk", "Frechet", "Gamma", "Gaussian", "Gompertz",
  "Gumbel", "Kumaraswamy", "Laplace", "Lindley", "Logistic", "Loggamma",
  "Lognormal", "Lomax", "Makeham", "Maxwell", "Nakagami", "Paralogistic",
  "Pareto", "Perks", "Rayleigh", "Rice", "Singh-Maddala", "Skewnormal", "t",
  "Topp-Leone", "Triangular", "Uniform", "Weibull"), params = NULL,
  fx = NULL, lower = NULL, upper = NULL, sub = 1000)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="calc_theory_+3A_dist">Dist</code></td>
<td>
<p>name of the distribution. The possible values are: &quot;Benini&quot;, &quot;Beta&quot;, &quot;Beta-Normal&quot;, &quot;Birnbaum-Saunders&quot;, &quot;Chisq&quot;,
&quot;Exponential&quot;, &quot;Exp-Geometric&quot;, &quot;Exp-Logarithmic&quot;, &quot;Exp-Poisson&quot;, &quot;F&quot;, &quot;Fisk&quot;, &quot;Frechet&quot;, &quot;Gamma&quot;, &quot;Gaussian&quot;, &quot;Gompertz&quot;,
&quot;Gumbel&quot;, &quot;Kumaraswamy&quot;, &quot;Laplace&quot;, &quot;Lindley&quot;, &quot;Logistic&quot;, &quot;Loggamma&quot;, &quot;Lognormal&quot;, &quot;Lomax&quot;, &quot;Makeham&quot;, &quot;Maxwell&quot;,
&quot;Nakagami&quot;, &quot;Paralogistic&quot;, &quot;Pareto&quot;, &quot;Perks&quot;, &quot;Rayleigh&quot;, &quot;Rice&quot;, &quot;Singh-Maddala&quot;, &quot;Skewnormal&quot;, &quot;t&quot;, &quot;Topp-Leone&quot;, &quot;Triangular&quot;,
&quot;Uniform&quot;, &quot;Weibull&quot;.
Please refer to the documentation for each package (either <code><a href="stats.html#topic+stats-package">stats-package</a></code>, <code><a href="VGAM.html#topic+VGAM-package">VGAM-package</a></code>, or
<code><a href="triangle.html#topic+triangle">triangle</a></code>) for information on appropriate parameter inputs.</p>
</td></tr>
<tr><td><code id="calc_theory_+3A_params">params</code></td>
<td>
<p>a vector of parameters (up to 4) for the desired distribution (keep NULL if <code>fx</code> supplied instead)</p>
</td></tr>
<tr><td><code id="calc_theory_+3A_fx">fx</code></td>
<td>
<p>a pdf input as a function of x only, i.e. fx &lt;- function(x) 0.5*(x-1)^2; must return a scalar
(keep NULL if Dist supplied instead)</p>
</td></tr>
<tr><td><code id="calc_theory_+3A_lower">lower</code></td>
<td>
<p>the lower support bound for a supplied fx, else keep NULL</p>
</td></tr>
<tr><td><code id="calc_theory_+3A_upper">upper</code></td>
<td>
<p>the upper support bound for a supplied fx, else keep NULL</p>
</td></tr>
<tr><td><code id="calc_theory_+3A_sub">sub</code></td>
<td>
<p>the number of subdivisions to use in the integration; if no result, try increasing sub (requires longer
computation time)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of the mean, standard deviation, skewness, standardized kurtosis, and standardized fifth and sixth cumulants
</p>


<h3>References</h3>

<p>Headrick TC (2002). Fast Fifth-order Polynomial Transforms for Generating Univariate and Multivariate
Non-normal Distributions. Computational Statistics &amp; Data Analysis, 40(4):685-711. doi: <a href="http://doi.org/10.1016/S0167-9473(02)00072-5">10.1016/S0167-9473(02)00072-5</a>.
(<a href="http://www.sciencedirect.com/science/article/pii/S0167947302000725">ScienceDirect</a>)
</p>
<p>Headrick TC, Kowalchuk RK (2007). The Power Method Transformation: Its Probability Density Function, Distribution
Function, and Its Further Use for Fitting Data. Journal of Statistical Computation and Simulation, 77, 229-249. doi: <a href="http://doi.org/10.1080/10629360600605065">10.1080/10629360600605065</a>.
</p>
<p>Headrick TC, Sheng Y, &amp; Hodis FA (2007). Numerical Computing and Graphics for the Power Method Transformation Using
Mathematica. Journal of Statistical Software, 19(3), 1 - 17. doi: <a href="http://doi.org/10.18637/jss.v019.i03">10.18637/jss.v019.i03</a>
</p>
<p>Thomas W. Yee (2018). VGAM: Vector Generalized Linear and Additive Models. R package version 1.0-5.
<a href="https://CRAN.R-project.org/package=VGAM">https://CRAN.R-project.org/package=VGAM</a>
</p>
<p>Rob Carnell (2017). triangle: Provides the Standard Distribution Functions for the Triangle Distribution. R package
version 0.11. <a href="https://CRAN.R-project.org/package=triangle">https://CRAN.R-project.org/package=triangle</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+calc_fisherk">calc_fisherk</a></code>, <code><a href="#topic+calc_moments">calc_moments</a></code>,
<code><a href="#topic+find_constants">find_constants</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>options(scipen = 999)

# Pareto Distribution: params = c(alpha = scale, theta = shape)
calc_theory(Dist = "Pareto", params = c(1, 10))

# Generalized Rayleigh Distribution: params = c(alpha = scale, mu/sqrt(pi/2) = shape)
calc_theory(Dist = "Rayleigh", params = c(0.5, 1))

# Laplace Distribution: params = c(location, scale)
calc_theory(Dist = "Laplace", params = c(0, 1))

# Triangle Distribution: params = c(a, b)
calc_theory(Dist = "Triangular", params = c(0, 1))
</code></pre>

<hr>
<h2 id='cdf_prob'>Calculate Theoretical Cumulative Probability for Continuous Variables</h2><span id='topic+cdf_prob'></span>

<h3>Description</h3>

<p>This function calculates a cumulative probability using the theoretical power method cdf
<code class="reqn">F_p(Z)(p(z)) = F_p(Z)(p(z), F_Z(z))</code> up to <code class="reqn">sigma * y + mu = delta</code>, where <code class="reqn">y = p(z)</code>, after using
<code><a href="#topic+pdf_check">pdf_check</a></code>.  If the given constants do not produce a valid power method pdf, a warning is given.
The formulas were obtained from Headrick &amp; Kowalchuk (2007, doi: <a href="http://doi.org/10.1080/10629360600605065">10.1080/10629360600605065</a>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cdf_prob(c, method = c("Fleishman", "Polynomial"), delta = 0.5, mu = 0,
  sigma = 1, lower = -1000000, upper = 1000000)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cdf_prob_+3A_c">c</code></td>
<td>
<p>a vector of constants c0, c1, c2, c3 (if <code>method</code> = &quot;Fleishman&quot;) or c0, c1, c2, c3, c4, c5 (if <code>method</code> =
&quot;Polynomial&quot;), like that returned by <code><a href="#topic+find_constants">find_constants</a></code></p>
</td></tr>
<tr><td><code id="cdf_prob_+3A_method">method</code></td>
<td>
<p>the method used to find the constants.  &quot;Fleishman&quot; uses a third-order polynomial transformation and
&quot;Polynomial&quot; uses Headrick's fifth-order transformation.</p>
</td></tr>
<tr><td><code id="cdf_prob_+3A_delta">delta</code></td>
<td>
<p>the value <code class="reqn">sigma * y + mu</code>, where <code class="reqn">y = p(z)</code>, at which to evaluate the cumulative probability</p>
</td></tr>
<tr><td><code id="cdf_prob_+3A_mu">mu</code></td>
<td>
<p>mean for the continuous variable</p>
</td></tr>
<tr><td><code id="cdf_prob_+3A_sigma">sigma</code></td>
<td>
<p>standard deviation for the continuous variable</p>
</td></tr>
<tr><td><code id="cdf_prob_+3A_lower">lower</code></td>
<td>
<p>lower bound for integration of the standard normal variable Z that generates the continuous variable</p>
</td></tr>
<tr><td><code id="cdf_prob_+3A_upper">upper</code></td>
<td>
<p>upper bound for integration</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with components:
</p>
<p><code>cumulative probability</code> the theoretical cumulative probability up to delta
</p>
<p><code>roots</code> the roots z that make <code class="reqn">sigma * p(z) + mu = delta</code>
</p>


<h3>References</h3>

<p>Fleishman AI (1978). A Method for Simulating Non-normal Distributions. Psychometrika, 43, 521-532. doi: <a href="http://doi.org/10.1007/BF02293811">10.1007/BF02293811</a>.
</p>
<p>Headrick TC (2002). Fast Fifth-order Polynomial Transforms for Generating Univariate and Multivariate
Non-normal Distributions. Computational Statistics &amp; Data Analysis, 40(4):685-711. doi: <a href="http://doi.org/10.1016/S0167-9473(02)00072-5">10.1016/S0167-9473(02)00072-5</a>.
(<a href="http://www.sciencedirect.com/science/article/pii/S0167947302000725">ScienceDirect</a>)
</p>
<p>Headrick TC (2004). On Polynomial Transformations for Simulating Multivariate Nonnormal Distributions.
Journal of Modern Applied Statistical Methods, 3(1), 65-71. doi: <a href="http://doi.org/10.22237/jmasm/1083370080">10.22237/jmasm/1083370080</a>.
</p>
<p>Headrick TC, Kowalchuk RK (2007). The Power Method Transformation: Its Probability Density Function, Distribution
Function, and Its Further Use for Fitting Data. Journal of Statistical Computation and Simulation, 77, 229-249. doi: <a href="http://doi.org/10.1080/10629360600605065">10.1080/10629360600605065</a>.
</p>
<p>Headrick TC, Sawilowsky SS (1999). Simulating Correlated Non-normal Distributions: Extending the Fleishman Power
Method. Psychometrika, 64, 25-35. doi: <a href="http://doi.org/10.1007/BF02294317">10.1007/BF02294317</a>.
</p>
<p>Headrick TC, Sheng Y, &amp; Hodis FA (2007). Numerical Computing and Graphics for the Power Method Transformation Using
Mathematica. Journal of Statistical Software, 19(3), 1 - 17. doi: <a href="http://doi.org/10.18637/jss.v019.i03">10.18637/jss.v019.i03</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+find_constants">find_constants</a></code>, <code><a href="#topic+pdf_check">pdf_check</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Normal distribution with Headrick's fifth-order PMT:
cdf_prob(c = c(0, 1, 0, 0, 0, 0), "Polynomial", delta = qnorm(0.05))

## Not run: 
# Beta(a = 4, b = 2) Distribution:
con &lt;- find_constants(method = "Polynomial", skews = -0.467707, skurts = -0.375,
                      fifths = 1.403122, sixths = -0.426136)$constants
cdf_prob(c = con, method = "Polynomial", delta = 0.5)

## End(Not run)
</code></pre>

<hr>
<h2 id='chat_nb'>Calculate Upper Frechet-Hoeffding Correlation Bound: Negative Binomial - Normal Variables</h2><span id='topic+chat_nb'></span>

<h3>Description</h3>

<p>This function calculates the upper Frechet-Hoeffding bound on the correlation between a Negative Binomial variable
and the normal variable used to generate it.  It is used in <code><a href="#topic+findintercorr_cat_nb">findintercorr_cat_nb</a></code>
and <code><a href="#topic+findintercorr_cont_nb">findintercorr_cont_nb</a></code> in calculating the intermediate MVN correlations.  This extends
the method of Amatya &amp; Demirtas (2015, doi: <a href="http://doi.org/10.1080/00949655.2014.953534">10.1080/00949655.2014.953534</a>) to Negative Binomial variables.  This function would not ordinarily be called directly by the user.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>chat_nb(size, prob = NULL, mu = NULL, n_unif = 10000, seed = 1234)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="chat_nb_+3A_size">size</code></td>
<td>
<p>a vector of size parameters for the Negative Binomial variables (see <code><a href="stats.html#topic+NegBinomial">NegBinomial</a></code>)</p>
</td></tr>
<tr><td><code id="chat_nb_+3A_prob">prob</code></td>
<td>
<p>a vector of success probability parameters</p>
</td></tr>
<tr><td><code id="chat_nb_+3A_mu">mu</code></td>
<td>
<p>a vector of mean parameters (*Note: either <code>prob</code> or <code>mu</code> should be supplied for all Negative Binomial variables,
not a mixture; default = NULL)</p>
</td></tr>
<tr><td><code id="chat_nb_+3A_n_unif">n_unif</code></td>
<td>
<p>the number of uniform random numbers to generate in calculating the bound (default = 10000)</p>
</td></tr>
<tr><td><code id="chat_nb_+3A_seed">seed</code></td>
<td>
<p>the seed used in random number generation (default = 1234)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A scalar equal to the correlation upper bound.
</p>


<h3>References</h3>

<p>Please see references for <code><a href="#topic+chat_pois">chat_pois</a></code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+findintercorr_cat_nb">findintercorr_cat_nb</a></code>, <code><a href="#topic+findintercorr_cont_nb">findintercorr_cont_nb</a></code>,
<code><a href="#topic+findintercorr">findintercorr</a></code>
</p>

<hr>
<h2 id='chat_pois'>Calculate Upper Frechet-Hoeffding Correlation Bound: Poisson - Normal Variables</h2><span id='topic+chat_pois'></span>

<h3>Description</h3>

<p>This function calculates the upper Frechet-Hoeffding bound on the correlation between a Poisson variable
and the normal variable used to generate it.  It is used in <code><a href="#topic+findintercorr_cat_pois">findintercorr_cat_pois</a></code>
and <code><a href="#topic+findintercorr_cont_pois">findintercorr_cont_pois</a></code> in calculating the intermediate MVN correlations.  This uses
the method of Amatya &amp; Demirtas (2015, doi: <a href="http://doi.org/10.1080/00949655.2014.953534">10.1080/00949655.2014.953534</a>).  This function would not ordinarily be called directly by the user.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>chat_pois(lam, n_unif = 10000, seed = 1234)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="chat_pois_+3A_lam">lam</code></td>
<td>
<p>a vector of lambda (&gt; 0) constants for the Poisson variables (see <code><a href="stats.html#topic+Poisson">Poisson</a></code>)</p>
</td></tr>
<tr><td><code id="chat_pois_+3A_n_unif">n_unif</code></td>
<td>
<p>the number of uniform random numbers to generate in calculating the bound (default = 10000)</p>
</td></tr>
<tr><td><code id="chat_pois_+3A_seed">seed</code></td>
<td>
<p>the seed used in random number generation (default = 1234)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A scalar equal to the correlation upper bound.
</p>


<h3>References</h3>

<p>Amatya A &amp; Demirtas H (2015). Simultaneous generation of multivariate mixed data with Poisson and normal marginals.
Journal of Statistical Computation and Simulation, 85(15): 3129-39. doi: <a href="http://doi.org/10.1080/00949655.2014.953534">10.1080/00949655.2014.953534</a>.
</p>
<p>Demirtas H &amp; Hedeker D (2011). A practical way for computing approximate lower and upper correlation bounds.
American Statistician, 65(2): 104-109. doi: <a href="http://doi.org/10.1198/tast.2011.10090">10.1198/tast.2011.10090</a>.
</p>
<p>Frechet M.  Sur les tableaux de correlation dont les marges sont donnees.  Ann. l'Univ. Lyon SectA.  1951;14:53-77.
</p>
<p>Hoeffding W. Scale-invariant correlation theory. In: Fisher NI, Sen PK, editors. The collected works of Wassily Hoeffding.
New York: Springer-Verlag; 1994. p. 57-107.
</p>
<p>Yahav I &amp; Shmueli G (2012). On Generating Multivariate Poisson Data in Management Science Applications. Applied Stochastic
Models in Business and Industry, 28(1): 91-102. doi: <a href="http://doi.org/10.1002/asmb.901">10.1002/asmb.901</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+findintercorr_cat_pois">findintercorr_cat_pois</a></code>, <code><a href="#topic+findintercorr_cont_pois">findintercorr_cont_pois</a></code>,
<code><a href="#topic+findintercorr">findintercorr</a></code>
</p>

<hr>
<h2 id='denom_corr_cat'>Calculate Denominator Used in Intercorrelations Involving Ordinal Variables</h2><span id='topic+denom_corr_cat'></span>

<h3>Description</h3>

<p>This function calculates part of the the denominator used to find intercorrelations involving ordinal variables
or variables that are treated as ordinal (i.e. count variables in the method used in
<code><a href="#topic+rcorrvar2">rcorrvar2</a></code>).  It uses the formula given by Olsson et al. (1982, doi: <a href="http://doi.org/10.1007/BF02294164">10.1007/BF02294164</a>) in
describing polyserial and point-polyserial correlations.  For an ordinal variable with r &gt;= 2 categories, the value is given by:
</p>
<p style="text-align: center;"><code class="reqn">\sum_{j = 1}^{r-1} \phi(\tau_{j})*(y_{j+1} - y_{j}),</code>
</p>
<p> where
</p>
<p style="text-align: center;"><code class="reqn">\phi(\tau) = (2\pi)^{-1/2} * exp(-0.5 * \tau^2).</code>
</p>
<p>  Here, <code class="reqn">y_{j}</code> is the j-th support
value and <code class="reqn">\tau_{j}</code> is <code class="reqn">\Phi^{-1}(\sum_{i=1}^{j} Pr(Y = y_{i}))</code>.  This function would not ordinarily be called directly by the user.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>denom_corr_cat(marginal, support)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="denom_corr_cat_+3A_marginal">marginal</code></td>
<td>
<p>a vector of cumulative probabilities defining the marginal distribution of the variable;
if the variable can take r values, the vector will contain r - 1 probabilities (the r-th is assumed to be 1)</p>
</td></tr>
<tr><td><code id="denom_corr_cat_+3A_support">support</code></td>
<td>
<p>a vector of containing the ordered support values</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A scalar
</p>


<h3>References</h3>

<p>Olsson U, Drasgow F, &amp; Dorans NJ (1982). The Polyserial Correlation Coefficient. Psychometrika, 47(3): 337-47.
doi: <a href="http://doi.org/10.1007/BF02294164">10.1007/BF02294164</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ordnorm">ordnorm</a></code>, <code><a href="#topic+rcorrvar">rcorrvar</a></code>,
<code><a href="#topic+rcorrvar2">rcorrvar2</a></code>, <code><a href="#topic+findintercorr_cont_cat">findintercorr_cont_cat</a></code>,
<code><a href="#topic+findintercorr_cont_pois2">findintercorr_cont_pois2</a></code>, <br />
<code><a href="#topic+findintercorr_cont_nb2">findintercorr_cont_nb2</a></code>
</p>

<hr>
<h2 id='error_loop'>Error Loop to Correct Final Correlation of Simulated Variables</h2><span id='topic+error_loop'></span>

<h3>Description</h3>

<p>This function corrects the final correlation of simulated variables to be within a precision value (<code>epsilon</code>) of the
target correlation.  It updates the pairwise intermediate MVN correlation iteratively in a loop until either the maximum error
is less than epsilon or the number of iterations exceeds the maximum number set by the user (<code>maxit</code>).  It uses
<code><a href="#topic+error_vars">error_vars</a></code> to simulate all variables and calculate the correlation of all variables in each
iteration.  This function would not ordinarily be called directly by the user.  The function is a
modification of  Barbiero &amp; Ferrari's <code><a href="GenOrd.html#topic+ordcont">ordcont</a></code> function in <code><a href="GenOrd.html#topic+GenOrd-package">GenOrd-package</a></code>.
The <code><a href="GenOrd.html#topic+ordcont">ordcont</a></code> has been modified in the following ways:
</p>
<p>1) It works for continuous, ordinal (r &gt;= 2 categories), and count variables.
</p>
<p>2) The initial correlation check has been removed because this intermediate correlation
Sigma from <code><a href="#topic+rcorrvar">rcorrvar</a></code> or <code><a href="#topic+rcorrvar2">rcorrvar2</a></code> has already been
checked for positive-definiteness and used to generate variables.
</p>
<p>3) Eigenvalue decomposition is done on <code>Sigma</code> to impose the correct interemdiate correlations on the normal variables.
If <code>Sigma</code> is not positive-definite, the negative eigen values are replaced with 0.
</p>
<p>4) The final positive-definite check has been removed.
</p>
<p>5) The intermediate correlation update function was changed to accommodate more situations.
</p>
<p>6) A final &quot;fail-safe&quot; check was added at the end of the iteration loop where if the absolute
error between the final and target pairwise correlation is still &gt; 0.1, the intermediate correlation is set
equal to the target correlation (if <code>extra_correct</code> = &quot;TRUE&quot;).
</p>
<p>7) Allowing specifications for the sample size and the seed for reproducibility.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>error_loop(k_cat, k_cont, k_pois, k_nb, Y_cat, Y, Yb, Y_pois, Y_nb, marginal,
  support, method, means, vars, constants, lam, size, prob, mu, n, seed,
  epsilon, maxit, rho0, Sigma, rho_calc, extra_correct)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="error_loop_+3A_k_cat">k_cat</code></td>
<td>
<p>the number of ordinal (r &gt;= 2 categories) variables</p>
</td></tr>
<tr><td><code id="error_loop_+3A_k_cont">k_cont</code></td>
<td>
<p>the number of continuous variables</p>
</td></tr>
<tr><td><code id="error_loop_+3A_k_pois">k_pois</code></td>
<td>
<p>the number of Poisson variables</p>
</td></tr>
<tr><td><code id="error_loop_+3A_k_nb">k_nb</code></td>
<td>
<p>the number of Negative Binomial variables</p>
</td></tr>
<tr><td><code id="error_loop_+3A_y_cat">Y_cat</code></td>
<td>
<p>the ordinal variables generated from <code><a href="#topic+rcorrvar">rcorrvar</a></code> or <code><a href="#topic+rcorrvar2">rcorrvar2</a></code></p>
</td></tr>
<tr><td><code id="error_loop_+3A_y">Y</code></td>
<td>
<p>the continuous (mean 0, variance 1) variables</p>
</td></tr>
<tr><td><code id="error_loop_+3A_yb">Yb</code></td>
<td>
<p>the continuous variables with desired mean and variance</p>
</td></tr>
<tr><td><code id="error_loop_+3A_y_pois">Y_pois</code></td>
<td>
<p>the Poisson variables</p>
</td></tr>
<tr><td><code id="error_loop_+3A_y_nb">Y_nb</code></td>
<td>
<p>the Negative Binomial variables</p>
</td></tr>
<tr><td><code id="error_loop_+3A_marginal">marginal</code></td>
<td>
<p>a list of length equal <code>k_cat</code>; the i-th element is a vector of the cumulative
probabilities defining the marginal distribution of the i-th variable;
if the variable can take r values, the vector will contain r - 1 probabilities (the r-th is assumed to be 1)</p>
</td></tr>
<tr><td><code id="error_loop_+3A_support">support</code></td>
<td>
<p>a list of length equal <code>k_cat</code>; the i-th element is a vector of containing the r
ordered support values; if not provided, the default is for the i-th element to be the vector 1, ..., r</p>
</td></tr>
<tr><td><code id="error_loop_+3A_method">method</code></td>
<td>
<p>the method used to generate the continuous variables.  &quot;Fleishman&quot; uses a third-order polynomial transformation
and &quot;Polynomial&quot; uses Headrick's fifth-order transformation.</p>
</td></tr>
<tr><td><code id="error_loop_+3A_means">means</code></td>
<td>
<p>a vector of means for the continuous variables</p>
</td></tr>
<tr><td><code id="error_loop_+3A_vars">vars</code></td>
<td>
<p>a vector of variances</p>
</td></tr>
<tr><td><code id="error_loop_+3A_constants">constants</code></td>
<td>
<p>a matrix with <code>k_cont</code> rows, each a vector of constants c0, c1, c2, c3 (if <code>method</code> = &quot;Fleishman&quot;) or
c0, c1, c2, c3, c4, c5 (if <code>method</code> = &quot;Polynomial&quot;), like that returned by
<code><a href="#topic+find_constants">find_constants</a></code></p>
</td></tr>
<tr><td><code id="error_loop_+3A_lam">lam</code></td>
<td>
<p>a vector of lambda (&gt; 0) constants for the Poisson variables (see <code><a href="stats.html#topic+Poisson">Poisson</a></code>)</p>
</td></tr>
<tr><td><code id="error_loop_+3A_size">size</code></td>
<td>
<p>a vector of size parameters for the Negative Binomial variables (see <code><a href="stats.html#topic+NegBinomial">NegBinomial</a></code>)</p>
</td></tr>
<tr><td><code id="error_loop_+3A_prob">prob</code></td>
<td>
<p>a vector of success probability parameters</p>
</td></tr>
<tr><td><code id="error_loop_+3A_mu">mu</code></td>
<td>
<p>a vector of mean parameters (*Note: either <code>prob</code> or <code>mu</code> should be supplied for all Negative Binomial variables,
not a mixture)</p>
</td></tr>
<tr><td><code id="error_loop_+3A_n">n</code></td>
<td>
<p>the sample size</p>
</td></tr>
<tr><td><code id="error_loop_+3A_seed">seed</code></td>
<td>
<p>the seed value for random number generation</p>
</td></tr>
<tr><td><code id="error_loop_+3A_epsilon">epsilon</code></td>
<td>
<p>the maximum acceptable error between the final and target correlation matrices;
smaller epsilons take more time</p>
</td></tr>
<tr><td><code id="error_loop_+3A_maxit">maxit</code></td>
<td>
<p>the maximum number of iterations to use to find the intermediate correlation; the
correction loop stops when either the iteration number passes <code>maxit</code> or <code>epsilon</code> is reached</p>
</td></tr>
<tr><td><code id="error_loop_+3A_rho0">rho0</code></td>
<td>
<p>the target correlation matrix</p>
</td></tr>
<tr><td><code id="error_loop_+3A_sigma">Sigma</code></td>
<td>
<p>the intermediate correlation matrix previously used in <code><a href="#topic+rcorrvar">rcorrvar</a></code>
or <code><a href="#topic+rcorrvar2">rcorrvar2</a></code></p>
</td></tr>
<tr><td><code id="error_loop_+3A_rho_calc">rho_calc</code></td>
<td>
<p>the final correlation matrix calculated in <code><a href="#topic+rcorrvar">rcorrvar</a></code>
or <code><a href="#topic+rcorrvar2">rcorrvar2</a></code></p>
</td></tr>
<tr><td><code id="error_loop_+3A_extra_correct">extra_correct</code></td>
<td>
<p>if &quot;TRUE&quot;, a final &quot;fail-safe&quot; check is used at the end of the iteration loop where if the absolute
error between the final and target pairwise correlation is still &gt; 0.1, the intermediate correlation is set
equal to the target correlation</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with the following components:
</p>
<p><code>Sigma</code> the intermediate MVN correlation matrix resulting from the error loop
</p>
<p><code>rho_calc</code> the calculated final correlation matrix generated from Sigma
</p>
<p><code>Y_cat</code> the ordinal variables
</p>
<p><code>Y</code> the continuous (mean 0, variance 1) variables
</p>
<p><code>Yb</code> the continuous variables with desired mean and variance
</p>
<p><code>Y_pois</code> the Poisson variables
</p>
<p><code>Y_nb</code> the Negative Binomial variables
</p>
<p><code>niter</code> a matrix containing the number of iterations required for each variable pair
</p>


<h3>References</h3>

<p>Barbiero A, Ferrari PA (2015). GenOrd: Simulation of Discrete Random Variables with Given
Correlation Matrix and Marginal Distributions. R package version 1.4.0. <a href="https://CRAN.R-project.org/package=GenOrd">https://CRAN.R-project.org/package=GenOrd</a>
</p>
<p>Ferrari PA, Barbiero A (2012). Simulating ordinal data. Multivariate Behavioral Research, 47(4): 566-589.
doi: <a href="http://doi.org/10.1080/00273171.2012.692630">10.1080/00273171.2012.692630</a>.
</p>
<p>Fleishman AI (1978). A Method for Simulating Non-normal Distributions. Psychometrika, 43, 521-532. doi: <a href="http://doi.org/10.1007/BF02293811">10.1007/BF02293811</a>.
</p>
<p>Headrick TC (2002). Fast Fifth-order Polynomial Transforms for Generating Univariate and Multivariate
Non-normal Distributions. Computational Statistics &amp; Data Analysis, 40(4):685-711. doi: <a href="http://doi.org/10.1016/S0167-9473(02)00072-5">10.1016/S0167-9473(02)00072-5</a>.
(<a href="http://www.sciencedirect.com/science/article/pii/S0167947302000725">ScienceDirect</a>)
</p>
<p>Headrick TC (2004). On Polynomial Transformations for Simulating Multivariate Nonnormal Distributions.
Journal of Modern Applied Statistical Methods, 3(1), 65-71. doi: <a href="http://doi.org/10.22237/jmasm/1083370080">10.22237/jmasm/1083370080</a>.
</p>
<p>Headrick TC, Kowalchuk RK (2007). The Power Method Transformation: Its Probability Density Function, Distribution
Function, and Its Further Use for Fitting Data. Journal of Statistical Computation and Simulation, 77, 229-249. doi: <a href="http://doi.org/10.1080/10629360600605065">10.1080/10629360600605065</a>.
</p>
<p>Headrick TC, Sawilowsky SS (1999). Simulating Correlated Non-normal Distributions: Extending the Fleishman Power
Method. Psychometrika, 64, 25-35. doi: <a href="http://doi.org/10.1007/BF02294317">10.1007/BF02294317</a>.
</p>
<p>Headrick TC, Sheng Y, &amp; Hodis FA (2007). Numerical Computing and Graphics for the Power Method Transformation Using
Mathematica. Journal of Statistical Software, 19(3), 1 - 17. doi: <a href="http://doi.org/10.18637/jss.v019.i03">10.18637/jss.v019.i03</a>.
</p>
<p>Higham N (2002). Computing the nearest correlation matrix - a problem from finance; IMA Journal of Numerical Analysis 22: 329-343.
</p>


<h3>See Also</h3>

<p><code><a href="GenOrd.html#topic+ordcont">ordcont</a></code>, <code><a href="#topic+rcorrvar">rcorrvar</a></code>, <code><a href="#topic+rcorrvar2">rcorrvar2</a></code>,
<code><a href="#topic+findintercorr">findintercorr</a></code>, <code><a href="#topic+findintercorr2">findintercorr2</a></code>
</p>

<hr>
<h2 id='error_vars'>Generate Variables for Error Loop</h2><span id='topic+error_vars'></span>

<h3>Description</h3>

<p>This function simulates the continuous, ordinal (r &gt;= 2 categories), Poisson, or Negative Binomial variables
used in <code><a href="#topic+error_loop">error_loop</a></code>.  It is called in each iteration, regenerates all variables, and calculates the
resulting correlation matrix.
This function would not ordinarily be called directly by the user.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>error_vars(marginal, support, method, means, vars, constants, lam, size, prob,
  mu, Sigma, rho_calc, q, r, k_cat, k_cont, k_pois, k_nb, Y_cat, Y, Yb, Y_pois,
  Y_nb, n, seed)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="error_vars_+3A_marginal">marginal</code></td>
<td>
<p>a list of length equal <code>k_cat</code>; the i-th element is a vector of the cumulative
probabilities defining the marginal distribution of the i-th variable;
if the variable can take r values, the vector will contain r - 1 probabilities (the r-th is assumed to be 1)</p>
</td></tr>
<tr><td><code id="error_vars_+3A_support">support</code></td>
<td>
<p>a list of length equal <code>k_cat</code>; the i-th element is a vector of containing the r
ordered support values; if not provided, the default is for the i-th element to be the vector 1, ..., r</p>
</td></tr>
<tr><td><code id="error_vars_+3A_method">method</code></td>
<td>
<p>the method used to generate the continuous variables.  &quot;Fleishman&quot; uses a third-order polynomial transformation
and &quot;Polynomial&quot; uses Headrick's fifth-order transformation.</p>
</td></tr>
<tr><td><code id="error_vars_+3A_means">means</code></td>
<td>
<p>a vector of means for the continuous variables</p>
</td></tr>
<tr><td><code id="error_vars_+3A_vars">vars</code></td>
<td>
<p>a vector of variances</p>
</td></tr>
<tr><td><code id="error_vars_+3A_constants">constants</code></td>
<td>
<p>a matrix with <code>k_cont</code> rows, each a vector of constants c0, c1, c2, c3 (if <code>method</code> = &quot;Fleishman&quot;) or
c0, c1, c2, c3, c4, c5 (if <code>method</code> = &quot;Polynomial&quot;), like that returned by
<code><a href="#topic+find_constants">find_constants</a></code></p>
</td></tr>
<tr><td><code id="error_vars_+3A_lam">lam</code></td>
<td>
<p>a vector of lambda (&gt; 0) constants for the Poisson variables (see <code><a href="stats.html#topic+Poisson">Poisson</a></code>)</p>
</td></tr>
<tr><td><code id="error_vars_+3A_size">size</code></td>
<td>
<p>a vector of size parameters for the Negative Binomial variables (see <code><a href="stats.html#topic+NegBinomial">NegBinomial</a></code>)</p>
</td></tr>
<tr><td><code id="error_vars_+3A_prob">prob</code></td>
<td>
<p>a vector of success probability parameters</p>
</td></tr>
<tr><td><code id="error_vars_+3A_mu">mu</code></td>
<td>
<p>a vector of mean parameters (*Note: either <code>prob</code> or <code>mu</code> should be supplied for all Negative Binomial variables,
not a mixture)</p>
</td></tr>
<tr><td><code id="error_vars_+3A_sigma">Sigma</code></td>
<td>
<p>the 2 x 2 intermediate correlation matrix generated by <code><a href="#topic+error_loop">error_loop</a></code></p>
</td></tr>
<tr><td><code id="error_vars_+3A_rho_calc">rho_calc</code></td>
<td>
<p>the 2 x 2 final correlation matrix calculated in <code><a href="#topic+error_loop">error_loop</a></code></p>
</td></tr>
<tr><td><code id="error_vars_+3A_q">q</code></td>
<td>
<p>the row index of the 1st variable</p>
</td></tr>
<tr><td><code id="error_vars_+3A_r">r</code></td>
<td>
<p>the column index of the 2nd variable</p>
</td></tr>
<tr><td><code id="error_vars_+3A_k_cat">k_cat</code></td>
<td>
<p>the number of ordinal (r &gt;= 2 categories) variables</p>
</td></tr>
<tr><td><code id="error_vars_+3A_k_cont">k_cont</code></td>
<td>
<p>the number of continuous variables</p>
</td></tr>
<tr><td><code id="error_vars_+3A_k_pois">k_pois</code></td>
<td>
<p>the number of Poisson variables</p>
</td></tr>
<tr><td><code id="error_vars_+3A_k_nb">k_nb</code></td>
<td>
<p>the number of Negative Binomial variables</p>
</td></tr>
<tr><td><code id="error_vars_+3A_y_cat">Y_cat</code></td>
<td>
<p>the ordinal variables generated from <code><a href="#topic+error_loop">error_loop</a></code></p>
</td></tr>
<tr><td><code id="error_vars_+3A_y">Y</code></td>
<td>
<p>the continuous (mean 0, variance 1) variables</p>
</td></tr>
<tr><td><code id="error_vars_+3A_yb">Yb</code></td>
<td>
<p>the continuous variables with desired mean and variance</p>
</td></tr>
<tr><td><code id="error_vars_+3A_y_pois">Y_pois</code></td>
<td>
<p>the Poisson variables</p>
</td></tr>
<tr><td><code id="error_vars_+3A_y_nb">Y_nb</code></td>
<td>
<p>the Negative Binomial variables</p>
</td></tr>
<tr><td><code id="error_vars_+3A_n">n</code></td>
<td>
<p>the sample size</p>
</td></tr>
<tr><td><code id="error_vars_+3A_seed">seed</code></td>
<td>
<p>the seed value for random number generation</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with the following components:
</p>
<p><code>Sigma</code> the intermediate MVN correlation matrix
</p>
<p><code>rho_calc</code> the calculated final correlation matrix generated from Sigma
</p>
<p><code>Y_cat</code> the ordinal variables
</p>
<p><code>Y</code> the continuous (mean 0, variance 1) variables
</p>
<p><code>Yb</code> the continuous variables with desired mean and variance
</p>
<p><code>Y_pois</code> the Poisson variables
</p>
<p><code>Y_nb</code> the Negative Binomial variables
</p>


<h3>References</h3>

<p>Please see references for <code><a href="#topic+error_loop">error_loop</a></code>.
</p>


<h3>See Also</h3>

<p><code><a href="GenOrd.html#topic+ordcont">ordcont</a></code>, <code><a href="#topic+rcorrvar">rcorrvar</a></code>, <code><a href="#topic+rcorrvar2">rcorrvar2</a></code>,
<code><a href="#topic+error_loop">error_loop</a></code>
</p>

<hr>
<h2 id='find_constants'>Find Power Method Transformation Constants</h2><span id='topic+find_constants'></span>

<h3>Description</h3>

<p>This function calculates Fleishman's third or Headrick's fifth-order constants necessary to transform a standard normal
random variable into a continuous variable with the specified skewness, standardized kurtosis, and standardized
fifth and sixth cumulants.  It uses <code><a href="BB.html#topic+multiStart">multiStart</a></code> to find solutions to <code><a href="#topic+fleish">fleish</a></code> or
<code><a href="nleqslv.html#topic+nleqslv">nleqslv</a></code> for <code><a href="#topic+poly">poly</a></code>. Multiple starting values are used to ensure the correct
solution is found.  If not user-specified and <code>method</code> = &quot;Polynomial&quot;, the cumulant values are checked to see if they fall in
Headrick's Table 1 (2002, p.691-2, doi: <a href="http://doi.org/10.1016/S0167-9473(02)00072-5">10.1016/S0167-9473(02)00072-5</a>) of common distributions (see <code><a href="#topic+Headrick.dist">Headrick.dist</a></code>).
If so, his solutions are used as starting values.  Otherwise, a set of <code>n</code> values randomly generated from uniform distributions is used to
determine the power method constants.
</p>
<p>Each set of constants is checked for a positive correlation with the underlying normal variable (using
<code><a href="#topic+power_norm_corr">power_norm_corr</a></code>)
and a valid power method pdf (using <code><a href="#topic+pdf_check">pdf_check</a></code>).  If the correlation is &lt;= 0, the signs of c1 and c3 are
reversed (for <code>method</code> = &quot;Fleishman&quot;), or c1, c3, and c5 (for <code>method</code> = &quot;Polynomial&quot;).  These sign changes have no effect on the cumulants
of the resulting distribution.  If only invalid pdf constants are found and a vector of sixth cumulant correction values (<code>Six</code>) is provided,
each is checked for valid pdf constants.  The smallest correction that generates a valid power method pdf is used.  If valid pdf constants
still can not be found, the original invalid pdf constants (calculated without a sixth cumulant correction) will be provided if they exist.
If not, the invalid pdf constants calculated with the sixth cumulant correction will be provided.  If no solutions
can be found, an error is given and the result is NULL.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>find_constants(method = c("Fleishman", "Polynomial"), skews = NULL,
  skurts = NULL, fifths = NULL, sixths = NULL, Six = NULL,
  cstart = NULL, n = 25, seed = 1234)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="find_constants_+3A_method">method</code></td>
<td>
<p>the method used to find the constants.  &quot;Fleishman&quot; uses a third-order polynomial transformation and
requires skewness and standardized kurtosis inputs.  &quot;Polynomial&quot; uses Headrick's fifth-order
transformation and requires all four standardized cumulants.</p>
</td></tr>
<tr><td><code id="find_constants_+3A_skews">skews</code></td>
<td>
<p>the skewness value</p>
</td></tr>
<tr><td><code id="find_constants_+3A_skurts">skurts</code></td>
<td>
<p>the standardized kurtosis value (kurtosis - 3, so that normal variables have a value of 0)</p>
</td></tr>
<tr><td><code id="find_constants_+3A_fifths">fifths</code></td>
<td>
<p>the standardized fifth cumulant (if <code>method</code> = &quot;Fleishman&quot;, keep NULL)</p>
</td></tr>
<tr><td><code id="find_constants_+3A_sixths">sixths</code></td>
<td>
<p>the standardized sixth cumulant (if <code>method</code> = &quot;Fleishman&quot;, keep NULL)</p>
</td></tr>
<tr><td><code id="find_constants_+3A_six">Six</code></td>
<td>
<p>a vector of correction values to add to the sixth cumulant if no valid pdf constants are found,
ex: <code>Six = seq(1.5, 2,by = 0.05)</code>; longer vectors take more computation time</p>
</td></tr>
<tr><td><code id="find_constants_+3A_cstart">cstart</code></td>
<td>
<p>initial value for root-solving algorithm (see <code><a href="BB.html#topic+multiStart">multiStart</a></code> for <code>method</code> = &quot;Fleishman&quot;
or <code><a href="nleqslv.html#topic+nleqslv">nleqslv</a></code> for <code>method</code> = &quot;Polynomial&quot;).  If user-specified,
must be input as a matrix. If NULL and all 4 standardized cumulants (rounded to 3 digits) are within
0.01 of those in Headrick's common distribution table (see <code><a href="#topic+Headrick.dist">Headrick.dist</a></code>
data), uses his constants as starting values; else, generates <code>n</code> sets of random starting values from
uniform distributions.</p>
</td></tr>
<tr><td><code id="find_constants_+3A_n">n</code></td>
<td>
<p>the number of initial starting values to use with root-solver.  More starting values
require more calculation time (default = 25).</p>
</td></tr>
<tr><td><code id="find_constants_+3A_seed">seed</code></td>
<td>
<p>the seed value for random starting value generation (default = 1234)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with components:
</p>
<p><code>constants</code> a vector of valid or invalid power method solutions, c(&quot;c0&quot;,&quot;c1&quot;,&quot;c2&quot;,&quot;c3&quot;) for <code>method</code> = &quot;Fleishman&quot; or
c(&quot;c0&quot;,&quot;c1&quot;,&quot;c2&quot;,&quot;c3&quot;,&quot;c4,&quot;c5&quot;) for <code>method</code> = &quot;Polynomial&quot;
</p>
<p><code>valid</code> &quot;TRUE&quot; if the constants produce a valid power method pdf, else &quot;FALSE&quot;
</p>
<p><code>SixCorr1</code> if <code>Six</code> is specified, the sixth cumulant correction required to achieve a valid pdf
</p>


<h3>Reasons for Function Errors</h3>

<p>1) The most likely cause for function errors is that no solutions to <code><a href="#topic+fleish">fleish</a></code> or
<code><a href="#topic+poly">poly</a></code> converged when using <code><a href="#topic+find_constants">find_constants</a></code>.  If this happens,
the simulation will stop.  Possible solutions include: a) increasing the number of initial starting values (<code>n</code>),
b) using a different seed, or c) specifying a <code>Six</code> vector of sixth cumulant correction values (for <code>method</code> = &quot;Polynomial&quot;).
If the standardized cumulants are obtained from <code>calc_theory</code>, the user may need to use rounded values as inputs (i.e.
<code>skews = round(skews, 8)</code>).  Due to the nature of the integration involved in <code>calc_theory</code>, the results are
approximations.  Greater accuracy can be achieved by increasing the number of subdivisions (<code>sub</code>) used in the integration
process.  For example, in order to ensure that skew is exactly 0 for symmetric distributions.
</p>
<p>2) In addition, the kurtosis may be outside the region of possible values.  There is an associated lower boundary for kurtosis associated
with a given skew (for Fleishman's method) or skew and fifth and sixth cumulants (for Headrick's method).  Use
<code><a href="#topic+calc_lower_skurt">calc_lower_skurt</a></code> to determine the boundary for a given set of cumulants.
</p>


<h3>References</h3>

<p>Fleishman AI (1978). A Method for Simulating Non-normal Distributions. Psychometrika, 43, 521-532. doi: <a href="http://doi.org/10.1007/BF02293811">10.1007/BF02293811</a>.
</p>
<p>Hasselman B (2018). nleqslv: Solve Systems of Nonlinear Equations. R package version 3.3.2.
<a href="https://CRAN.R-project.org/package=nleqslv">https://CRAN.R-project.org/package=nleqslv</a>
</p>
<p>Headrick TC (2002). Fast Fifth-order Polynomial Transforms for Generating Univariate and Multivariate
Non-normal Distributions. Computational Statistics &amp; Data Analysis, 40(4):685-711. doi: <a href="http://doi.org/10.1016/S0167-9473(02)00072-5">10.1016/S0167-9473(02)00072-5</a>.
(<a href="http://www.sciencedirect.com/science/article/pii/S0167947302000725">ScienceDirect</a>)
</p>
<p>Headrick TC (2004). On Polynomial Transformations for Simulating Multivariate Nonnormal Distributions.
Journal of Modern Applied Statistical Methods, 3(1), 65-71. doi: <a href="http://doi.org/10.22237/jmasm/1083370080">10.22237/jmasm/1083370080</a>.
</p>
<p>Headrick TC, Kowalchuk RK (2007). The Power Method Transformation: Its Probability Density Function, Distribution
Function, and Its Further Use for Fitting Data. Journal of Statistical Computation and Simulation, 77, 229-249. doi: <a href="http://doi.org/10.1080/10629360600605065">10.1080/10629360600605065</a>.
</p>
<p>Headrick TC, Sawilowsky SS (1999). Simulating Correlated Non-normal Distributions: Extending the Fleishman Power
Method. Psychometrika, 64, 25-35. doi: <a href="http://doi.org/10.1007/BF02294317">10.1007/BF02294317</a>.
</p>
<p>Headrick TC, Sheng Y, &amp; Hodis FA (2007). Numerical Computing and Graphics for the Power Method Transformation Using
Mathematica. Journal of Statistical Software, 19(3), 1 - 17. doi: <a href="http://doi.org/10.18637/jss.v019.i03">10.18637/jss.v019.i03</a>.
</p>
<p>Varadhan R, Gilbert P (2009). BB: An R Package for Solving a Large System of Nonlinear Equations and for
Optimizing a High-Dimensional Nonlinear Objective Function, J. Statistical Software, 32:4,
<a href="http://www.jstatsoft.org/v32/i04/">http://www.jstatsoft.org/v32/i04/</a>
</p>


<h3>See Also</h3>

<p><code><a href="BB.html#topic+multiStart">multiStart</a></code>, <code><a href="nleqslv.html#topic+nleqslv">nleqslv</a></code>,
<code><a href="#topic+fleish">fleish</a></code>, <code><a href="#topic+poly">poly</a></code>,
<code><a href="#topic+power_norm_corr">power_norm_corr</a></code>, <code><a href="#topic+pdf_check">pdf_check</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Exponential Distribution
find_constants("Fleishman", 2, 6)

## Not run: 
# Compute third-order power method constants.

options(scipen = 999) # turn off scientific notation

# Laplace Distribution
find_constants("Fleishman", 0, 3)

# Compute fifth-order power method constants.

# Logistic Distribution
find_constants(method = "Polynomial", skews = 0, skurts = 6/5, fifths = 0,
               sixths = 48/7)

# with correction to sixth cumulant
find_constants(method = "Polynomial", skews = 0, skurts = 6/5, fifths = 0,
               sixths = 48/7, Six = seq(1.7, 2, by = 0.01))


## End(Not run)
</code></pre>

<hr>
<h2 id='findintercorr'>Calculate Intermediate MVN Correlation for Ordinal, Continuous, Poisson, or Negative Binomial Variables: Correlation Method 1</h2><span id='topic+findintercorr'></span>

<h3>Description</h3>

<p>This function calculates a <code>k x k</code> intermediate matrix of correlations, where <code>k = k_cat + k_cont + k_pois + k_nb</code>,
to be used in simulating variables with <code><a href="#topic+rcorrvar">rcorrvar</a></code>.  The ordering of the variables must be
ordinal, continuous, Poisson, and Negative Binomial (note that it is possible for <code>k_cat</code>, <code>k_cont</code>, <code>k_pois</code>,
and/or <code>k_nb</code> to be 0).
The function first checks that the target correlation matrix <code>rho</code> is positive-definite and the marginal distributions for the
ordinal variables are cumulative probabilities with r - 1 values (for r categories).  There is a warning given at the end of simulation
if the calculated intermediate correlation matrix <code>Sigma</code> is not positive-definite.  This function is called by the simulation function
<code><a href="#topic+rcorrvar">rcorrvar</a></code>, and would only be used separately if the user wants to find the intermediate correlation matrix
only.  The simulation functions also return the intermediate correlation matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>findintercorr(n, k_cont = 0, k_cat = 0, k_pois = 0, k_nb = 0,
  method = c("Fleishman", "Polynomial"), constants, marginal = list(),
  support = list(), nrand = 100000, lam = NULL, size = NULL,
  prob = NULL, mu = NULL, rho = NULL, seed = 1234, epsilon = 0.001,
  maxit = 1000)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="findintercorr_+3A_n">n</code></td>
<td>
<p>the sample size (i.e. the length of each simulated variable)</p>
</td></tr>
<tr><td><code id="findintercorr_+3A_k_cont">k_cont</code></td>
<td>
<p>the number of continuous variables (default = 0)</p>
</td></tr>
<tr><td><code id="findintercorr_+3A_k_cat">k_cat</code></td>
<td>
<p>the number of ordinal (r &gt;= 2 categories) variables (default = 0)</p>
</td></tr>
<tr><td><code id="findintercorr_+3A_k_pois">k_pois</code></td>
<td>
<p>the number of Poisson variables (default = 0)</p>
</td></tr>
<tr><td><code id="findintercorr_+3A_k_nb">k_nb</code></td>
<td>
<p>the number of Negative Binomial variables (default = 0)</p>
</td></tr>
<tr><td><code id="findintercorr_+3A_method">method</code></td>
<td>
<p>the method used to generate the <code>k_cont</code> continuous variables.  &quot;Fleishman&quot; uses a third-order polynomial transformation
and &quot;Polynomial&quot; uses Headrick's fifth-order transformation.</p>
</td></tr>
<tr><td><code id="findintercorr_+3A_constants">constants</code></td>
<td>
<p>a matrix with <code>k_cont</code> rows, each a vector of constants c0, c1, c2, c3 (if <code>method</code> = &quot;Fleishman&quot;) or
c0, c1, c2, c3, c4, c5 (if <code>method</code> = &quot;Polynomial&quot;) like that returned by
<code><a href="#topic+find_constants">find_constants</a></code></p>
</td></tr>
<tr><td><code id="findintercorr_+3A_marginal">marginal</code></td>
<td>
<p>a list of length equal to <code>k_cat</code>; the i-th element is a vector of the cumulative
probabilities defining the marginal distribution of the i-th variable;
if the variable can take r values, the vector will contain r - 1 probabilities (the r-th is assumed to be 1; default = list())</p>
</td></tr>
<tr><td><code id="findintercorr_+3A_support">support</code></td>
<td>
<p>a list of length equal to <code>k_cat</code>; the i-th element is a vector of containing the r
ordered support values; if not provided (i.e. <code>support</code> = list()), the default is for the i-th element to be the vector 1, ..., r</p>
</td></tr>
<tr><td><code id="findintercorr_+3A_nrand">nrand</code></td>
<td>
<p>the number of random numbers to generate in calculating the bound (default = 10000)</p>
</td></tr>
<tr><td><code id="findintercorr_+3A_lam">lam</code></td>
<td>
<p>a vector of lambda (&gt; 0) constants for the Poisson variables (see <code><a href="stats.html#topic+Poisson">Poisson</a></code>)</p>
</td></tr>
<tr><td><code id="findintercorr_+3A_size">size</code></td>
<td>
<p>a vector of size parameters for the Negative Binomial variables (see <code><a href="stats.html#topic+NegBinomial">NegBinomial</a></code>)</p>
</td></tr>
<tr><td><code id="findintercorr_+3A_prob">prob</code></td>
<td>
<p>a vector of success probability parameters</p>
</td></tr>
<tr><td><code id="findintercorr_+3A_mu">mu</code></td>
<td>
<p>a vector of mean parameters (*Note: either <code>prob</code> or <code>mu</code> should be supplied for all Negative Binomial variables,
not a mixture; default = NULL)</p>
</td></tr>
<tr><td><code id="findintercorr_+3A_rho">rho</code></td>
<td>
<p>the target correlation matrix (<em>must be ordered ordinal, continuous, Poisson, Negative Binomial</em>; default = NULL)</p>
</td></tr>
<tr><td><code id="findintercorr_+3A_seed">seed</code></td>
<td>
<p>the seed value for random number generation (default = 1234)</p>
</td></tr>
<tr><td><code id="findintercorr_+3A_epsilon">epsilon</code></td>
<td>
<p>the maximum acceptable error between the final and target correlation matrices (default = 0.001)
in the calculation of ordinal intermediate correlations with <code><a href="#topic+ordnorm">ordnorm</a></code></p>
</td></tr>
<tr><td><code id="findintercorr_+3A_maxit">maxit</code></td>
<td>
<p>the maximum number of iterations to use (default = 1000) in the calculation of ordinal
intermediate correlations with <code><a href="#topic+ordnorm">ordnorm</a></code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>the intermediate MVN correlation matrix
</p>


<h3>Overview of Correlation Method 1</h3>

<p>The intermediate correlations used in correlation method 1 are more simulation based than those in correlation method 2, which means that accuracy
increases with sample size and the number of repetitions.  In addition, specifying the seed allows for reproducibility.  In
addition, method 1 differs from method 2 in the following ways:
</p>
<p>1) The intermediate correlation for <b>count variables</b> is based on the method of Yahav &amp; Shmueli (2012, doi: <a href="http://doi.org/10.1002/asmb.901">10.1002/asmb.901</a>), which uses a
simulation based, logarithmic transformation of the target correlation.  This method becomes less accurate as the variable mean
gets closer to zero.
</p>
<p>2) The <b>ordinal - count variable</b> correlations are based on an extension of the method of Amatya &amp; Demirtas (2015,
doi: <a href="http://doi.org/10.1080/00949655.2014.953534">10.1080/00949655.2014.953534</a>), in which
the correlation correction factor is the product of the upper Frechet-Hoeffding bound on the correlation between the count
variable and the normal variable used to generate it and a simulated upper bound on the correlation between an ordinal variable
and the normal variable used to generate it (see Demirtas &amp; Hedeker, 2011, doi: <a href="http://doi.org/10.1198/tast.2011.10090">10.1198/tast.2011.10090</a>).
</p>
<p>3) The <b>continuous - count variable</b> correlations are based on an extension of the methods of Amatya &amp; Demirtas (2015) and
Demirtas et al. (2012, doi: <a href="http://doi.org/10.1002/sim.5362">10.1002/sim.5362</a>), in which the correlation correction factor is the product of the upper Frechet-Hoeffding bound
on the correlation between the count variable and the normal variable used to generate it and the power method correlation
between the continuous variable and the normal variable used to generate it (see Headrick &amp; Kowalchuk, 2007, doi: <a href="http://doi.org/10.1080/10629360600605065">10.1080/10629360600605065</a>).  The
intermediate correlations are the ratio of the target correlations to the correction factor.
</p>
<p>The processes used to find the intermediate correlations for each variable type are described below.  Please see the
corresponding function help page for more information:
</p>


<h3>Ordinal Variables</h3>

<p>Correlations are computed pairwise.  If both variables are binary, the method of Demirtas et al. (2012, doi: <a href="http://doi.org/10.1002/sim.5362">10.1002/sim.5362</a>) is used to find the
tetrachoric correlation (code adapted from <code><a href="BinNonNor.html#topic+Tetra.Corr.BB">Tetra.Corr.BB</a></code>).  This method is based on Emrich and Piedmonte's
(1991, doi: <a href="http://doi.org/10.1080/00031305.1991.10475828">10.1080/00031305.1991.10475828</a>) work, in which the joint binary distribution is determined from the third and higher moments of a multivariate normal
distribution: Let <code class="reqn">Y_{1}</code> and <code class="reqn">Y_{2}</code> be binary variables with <code class="reqn">E[Y_{1}] = Pr(Y_{1} = 1) = p_{1}</code>,
<code class="reqn">E[Y_{2}] = Pr(Y_{2} = 1) = p_{2}</code>, and correlation <code class="reqn">\rho_{y1y2}</code>.  Let <code class="reqn">\Phi[x_{1}, x_{2}, \rho_{x1x2}]</code> be the
standard bivariate normal cumulative distribution function, given by:
</p>
<p style="text-align: center;"><code class="reqn">\Phi[x_{1}, x_{2}, \rho_{x1x2}] = \int_{-\infty}^{x_{1}} \int_{-\infty}^{x_{2}} f(z_{1}, z_{2}, \rho_{x1x2}) dz_{1} dz_{2}</code>
</p>

<p>where
</p>
<p style="text-align: center;"><code class="reqn">f(z_{1}, z_{2}, \rho_{x1x2}) = [2\pi\sqrt{1 - \rho_{x1x2}^2}]^{-1} * exp[-0.5(z_{1}^2 - 2\rho_{x1x2}z_{1}z_{2} + z_{2}^2)/(1 - \rho_{x1x2}^2)]</code>
</p>

<p>Then solving the equation
</p>
<p style="text-align: center;"><code class="reqn">\Phi[z(p_{1}), z(p_{2}), \rho_{x1x2}] = \rho_{y1y2}\sqrt{p_{1}(1 - p_{1})p_{2}(1 - p_{2})} + p_{1}p_{2}</code>
</p>

<p>for <code class="reqn">\rho_{x1x2}</code> gives the intermediate correlation of the standard normal variables needed to generate binary variables with
correlation <code class="reqn">\rho_{y1y2}</code>.  Here <code class="reqn">z(p)</code> indicates the <code class="reqn">pth</code> quantile of the standard normal distribution.
</p>
<p>Otherwise, <code><a href="#topic+ordnorm">ordnorm</a></code> is called for each pair.  If the resulting
intermediate matrix is not positive-definite, there is a warning given because it may not be possible to find a MVN correlation
matrix that will produce the desired marginal distributions after discretization.  Any problems with positive-definiteness are
corrected later.
</p>


<h3>Continuous Variables</h3>

<p>Correlations are computed pairwise.  <code><a href="#topic+findintercorr_cont">findintercorr_cont</a></code> is called for each pair.
</p>


<h3>Poisson Variables</h3>

<p><code><a href="#topic+findintercorr_pois">findintercorr_pois</a></code> is called to calculate the intermediate MVN correlation for all Poisson variables.
</p>


<h3>Negative Binomial Variables</h3>

<p><code><a href="#topic+findintercorr_nb">findintercorr_nb</a></code> is called to calculate the intermediate MVN correlation for all Negative
Binomial variables.
</p>


<h3>Continuous - Ordinal Pairs</h3>

<p><code><a href="#topic+findintercorr_cont_cat">findintercorr_cont_cat</a></code> is called to calculate the intermediate MVN correlation for all
Continuous and Ordinal combinations.
</p>


<h3>Ordinal - Poisson Pairs</h3>

<p><code><a href="#topic+findintercorr_cat_pois">findintercorr_cat_pois</a></code> is called to calculate the intermediate MVN correlation for all
Ordinal and Poisson combinations.
</p>


<h3>Ordinal - Negative Binomial Pairs</h3>

<p><code><a href="#topic+findintercorr_cat_nb">findintercorr_cat_nb</a></code> is called to calculate the intermediate MVN correlation for all
Ordinal and Negative Binomial combinations.
</p>


<h3>Continuous - Poisson Pairs</h3>

<p><code><a href="#topic+findintercorr_cont_pois">findintercorr_cont_pois</a></code> is called to calculate the intermediate MVN correlation for all
Continuous and Poisson combinations.
</p>


<h3>Continuous - Negative Binomial Pairs</h3>

<p><code><a href="#topic+findintercorr_cont_nb">findintercorr_cont_nb</a></code> is called to calculate the intermediate MVN correlation for all
Continuous and Negative Binomial combinations.
</p>


<h3>Poisson - Negative Binomial Pairs</h3>

<p><code><a href="#topic+findintercorr_pois_nb">findintercorr_pois_nb</a></code> is called to calculate the intermediate MVN correlation for all
Poisson and Negative Binomial combinations.
</p>


<h3>References</h3>

<p>Please see <code><a href="#topic+rcorrvar">rcorrvar</a></code> for additional references.
</p>
<p>Emrich LJ &amp; Piedmonte MR (1991). A Method for Generating High-Dimensional Multivariate Binary Variables. The American Statistician, 45(4): 302-4.
doi: <a href="http://doi.org/10.1080/00031305.1991.10475828">10.1080/00031305.1991.10475828</a>.
</p>
<p>Inan G &amp; Demirtas H (2016). BinNonNor: Data Generation with Binary and Continuous Non-Normal Components.
R package version 1.3. <a href="https://CRAN.R-project.org/package=BinNonNor">https://CRAN.R-project.org/package=BinNonNor</a>
</p>
<p>Vale CD &amp; Maurelli VA (1983). Simulating Multivariate Nonnormal Distributions. Psychometrika, 48, 465-471. doi: <a href="http://doi.org/10.1007/BF02293687">10.1007/BF02293687</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+find_constants">find_constants</a></code>, <code><a href="#topic+rcorrvar">rcorrvar</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

# Binary, Ordinal, Continuous, Poisson, and Negative Binomial Variables

options(scipen = 999)
seed &lt;- 1234
n &lt;- 10000

# Continuous Distributions: Normal, t (df = 10), Chisq (df = 4),
# Beta (a = 4, b = 2), Gamma (a = 4, b = 4)
Dist &lt;- c("Gaussian", "t", "Chisq", "Beta", "Gamma")

# calculate standardized cumulants
# those for the normal and t distributions are rounded to ensure the
# correct values (i.e. skew = 0)

M1 &lt;- round(calc_theory(Dist = "Gaussian", params = c(0, 1)), 8)
M2 &lt;- round(calc_theory(Dist = "t", params = 10), 8)
M3 &lt;- calc_theory(Dist = "Chisq", params = 4)
M4 &lt;- calc_theory(Dist = "Beta", params = c(4, 2))
M5 &lt;- calc_theory(Dist = "Gamma", params = c(4, 4))
M &lt;- cbind(M1, M2, M3, M4, M5)
M &lt;- round(M[-c(1:2),], digits = 6)
colnames(M) &lt;- Dist
rownames(M) &lt;- c("skew", "skurtosis", "fifth", "sixth")
means &lt;- rep(0, length(Dist))
vars &lt;- rep(1, length(Dist))

# calculate constants
con &lt;- matrix(1, nrow = ncol(M), ncol = 6)
for (i in 1:ncol(M)) {
 con[i, ] &lt;- find_constants(method = "Polynomial", skews = M[1, i],
                            skurts = M[2, i], fifths = M[3, i],
                            sixths = M[4, i])
}

# Binary and Ordinal Distributions
marginal &lt;- list(0.3, 0.4, c(0.1, 0.5), c(0.3, 0.6, 0.9),
                 c(0.2, 0.4, 0.7, 0.8))
support &lt;- list()

# Poisson Distributions
lam &lt;- c(1, 5, 10)

# Negative Binomial Distributions
size &lt;- c(3, 6)
prob &lt;- c(0.2, 0.8)

ncat &lt;- length(marginal)
ncont &lt;- ncol(M)
npois &lt;- length(lam)
nnb &lt;- length(size)

# Create correlation matrix from a uniform distribution (-0.8, 0.8)
set.seed(seed)
Rey &lt;- diag(1, nrow = (ncat + ncont + npois + nnb))
for (i in 1:nrow(Rey)) {
  for (j in 1:ncol(Rey)) {
    if (i &gt; j) Rey[i, j] &lt;- runif(1, -0.8, 0.8)
    Rey[j, i] &lt;- Rey[i, j]
  }
}

# Test for positive-definiteness
library(Matrix)
if(min(eigen(Rey, symmetric = TRUE)$values) &lt; 0) {
  Rey &lt;- as.matrix(nearPD(Rey, corr = T, keepDiag = T)$mat)
}

# Make sure Rey is within upper and lower correlation limits
valid &lt;- valid_corr(k_cat = ncat, k_cont = ncont, k_pois = npois,
                    k_nb = nnb, method = "Polynomial", means = means,
                    vars = vars, skews = M[1, ], skurts = M[2, ],
                    fifths = M[3, ], sixths = M[4, ], marginal = marginal,
                    lam = lam, size = size, prob = prob, rho = Rey,
                    seed = seed)

# Find intermediate correlation
Sigma1 &lt;- findintercorr(n = n, k_cont = ncont, k_cat = ncat, k_pois = npois,
                        k_nb = nnb, method = "Polynomial", constants = con,
                        marginal = marginal, lam = lam, size = size,
                        prob = prob, rho = Rey, seed = seed)
Sigma1


## End(Not run)
</code></pre>

<hr>
<h2 id='findintercorr_cat_nb'>Calculate Intermediate MVN Correlation for Ordinal - Negative Binomial Variables: Correlation Method 1</h2><span id='topic+findintercorr_cat_nb'></span>

<h3>Description</h3>

<p>This function calculates a <code>k_cat x k_nb</code> intermediate matrix of correlations for the k_cat ordinal (r &gt;=
2 categories) and k_nb Negative Binomial variables. It extends the method of Amatya &amp; Demirtas (2015, doi: <a href="http://doi.org/10.1080/00949655.2014.953534">10.1080/00949655.2014.953534</a>)
to ordinal - Negative Binomial pairs.  Here, the intermediate correlation between Z1 and Z2 (where Z1 is the standard normal variable
discretized to produce an ordinal variable Y1, and Z2 is the standard normal variable used to generate a Negative Binomial
variable via the inverse cdf method) is calculated by dividing the target correlation by a correction factor.  The
correction factor is the product of the upper Frechet-Hoeffding bound on the correlation between a Negative Binomial variable
and the normal variable used to generate it (see <code><a href="#topic+chat_nb">chat_nb</a></code>) and a simulated GSC upper bound on
the correlation between an ordinal variable and the normal variable used to generate it (see Demirtas &amp; Hedeker, 2011,
doi: <a href="http://doi.org/10.1198/tast.2011.10090">10.1198/tast.2011.10090</a>).
The function is used in <code><a href="#topic+findintercorr">findintercorr</a></code> and <code><a href="#topic+rcorrvar">rcorrvar</a></code>.
This function would not ordinarily be called by the user.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>findintercorr_cat_nb(rho_cat_nb, marginal, size, prob, mu = NULL,
  nrand = 100000, seed = 1234)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="findintercorr_cat_nb_+3A_rho_cat_nb">rho_cat_nb</code></td>
<td>
<p>a <code>k_cat x k_nb</code> matrix of target correlations among ordinal and Negative Binomial variables</p>
</td></tr>
<tr><td><code id="findintercorr_cat_nb_+3A_marginal">marginal</code></td>
<td>
<p>a list of length equal to <code>k_cat</code>; the i-th element is a vector of the cumulative
probabilities defining the marginal distribution of the i-th variable;
if the variable can take r values, the vector will contain r - 1 probabilities (the r-th is assumed to be 1)</p>
</td></tr>
<tr><td><code id="findintercorr_cat_nb_+3A_size">size</code></td>
<td>
<p>a vector of size parameters for the Negative Binomial variables (see <code><a href="stats.html#topic+NegBinomial">NegBinomial</a></code>)</p>
</td></tr>
<tr><td><code id="findintercorr_cat_nb_+3A_prob">prob</code></td>
<td>
<p>a vector of success probability parameters</p>
</td></tr>
<tr><td><code id="findintercorr_cat_nb_+3A_mu">mu</code></td>
<td>
<p>a vector of mean parameters (*Note: either <code>prob</code> or <code>mu</code> should be supplied for all Negative Binomial variables,
not a mixture; default = NULL)</p>
</td></tr>
<tr><td><code id="findintercorr_cat_nb_+3A_nrand">nrand</code></td>
<td>
<p>the number of random numbers to generate in calculating the bound (default = 10000)</p>
</td></tr>
<tr><td><code id="findintercorr_cat_nb_+3A_seed">seed</code></td>
<td>
<p>the seed used in random number generation (default = 1234)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>k_cat x k_nb</code> matrix whose rows represent the <code>k_cat</code> ordinal variables and columns represent the
<code>k_nb</code> Negative Binomial variables
</p>


<h3>References</h3>

<p>Please see references for <code><a href="#topic+findintercorr_cat_pois">findintercorr_cat_pois</a></code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+chat_nb">chat_nb</a></code>,
<code><a href="#topic+findintercorr">findintercorr</a></code>, <code><a href="#topic+rcorrvar">rcorrvar</a></code>
</p>

<hr>
<h2 id='findintercorr_cat_pois'>Calculate Intermediate MVN Correlation for Ordinal - Poisson Variables: Correlation Method 1</h2><span id='topic+findintercorr_cat_pois'></span>

<h3>Description</h3>

<p>This function calculates a <code>k_cat x k_pois</code> intermediate matrix of correlations for the <code>k_cat</code> ordinal (r &gt;=
2 categories) and <code>k_pois</code> Poisson variables. It extends the method of Amatya &amp; Demirtas (2015, doi: <a href="http://doi.org/10.1080/00949655.2014.953534">10.1080/00949655.2014.953534</a>)
to ordinal - Poisson pairs.
Here, the intermediate correlation between Z1 and Z2 (where Z1 is the standard normal variable discretized to produce an
ordinal variable Y1, and Z2 is the standard normal variable used to generate a Poisson variable via the inverse cdf method) is
calculated by dividing the target correlation by a correction factor.  The correction factor is the product of the
upper Frechet-Hoeffding bound on the correlation between a Poisson variable and the normal variable used to generate it
(see <code><a href="#topic+chat_pois">chat_pois</a></code>) and a simulated GSC upper bound on the correlation between an ordinal variable
and the normal variable used to generate it (see Demirtas &amp; Hedeker, 2011, doi: <a href="http://doi.org/10.1198/tast.2011.10090">10.1198/tast.2011.10090</a>).  The function is used in
<code><a href="#topic+findintercorr">findintercorr</a></code> and <code><a href="#topic+rcorrvar">rcorrvar</a></code>.
This function would not ordinarily be called by the user.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>findintercorr_cat_pois(rho_cat_pois, marginal, lam, nrand = 100000,
  seed = 1234)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="findintercorr_cat_pois_+3A_rho_cat_pois">rho_cat_pois</code></td>
<td>
<p>a <code>k_cat x k_pois</code> matrix of target correlations among ordinal and Poisson variables</p>
</td></tr>
<tr><td><code id="findintercorr_cat_pois_+3A_marginal">marginal</code></td>
<td>
<p>a list of length equal to <code>k_cat</code>; the i-th element is a vector of the cumulative
probabilities defining the marginal distribution of the i-th variable;
if the variable can take r values, the vector will contain r - 1 probabilities (the r-th is assumed to be 1)</p>
</td></tr>
<tr><td><code id="findintercorr_cat_pois_+3A_lam">lam</code></td>
<td>
<p>a vector of lambda (&gt; 0) constants for the Poisson variables (see <code><a href="stats.html#topic+Poisson">Poisson</a></code>)</p>
</td></tr>
<tr><td><code id="findintercorr_cat_pois_+3A_nrand">nrand</code></td>
<td>
<p>the number of random numbers to generate in calculating the bound (default = 10000)</p>
</td></tr>
<tr><td><code id="findintercorr_cat_pois_+3A_seed">seed</code></td>
<td>
<p>the seed used in random number generation (default = 1234)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a k_cat x k_pois matrix whose rows represent the k_cat ordinal variables and columns represent the k_pois Poisson variables
</p>


<h3>References</h3>

<p>Amatya A &amp; Demirtas H (2015). Simultaneous generation of multivariate mixed data with Poisson and normal marginals.
Journal of Statistical Computation and Simulation, 85(15): 3129-39. doi: <a href="http://doi.org/10.1080/00949655.2014.953534">10.1080/00949655.2014.953534</a>.
</p>
<p>Demirtas H &amp; Hedeker D (2011). A practical way for computing approximate lower and upper correlation bounds.
American Statistician, 65(2): 104-109. doi: <a href="http://doi.org/10.1198/tast.2011.10090">10.1198/tast.2011.10090</a>.
</p>
<p>Frechet M.  Sur les tableaux de correlation dont les marges sont donnees.  Ann. l'Univ. Lyon SectA.  1951;14:53-77.
</p>
<p>Hoeffding W. Scale-invariant correlation theory. In: Fisher NI, Sen PK, editors. The collected works of Wassily Hoeffding.
New York: Springer-Verlag; 1994. p. 57-107.
</p>
<p>Yahav I &amp; Shmueli G (2012). On Generating Multivariate Poisson Data in Management Science Applications. Applied Stochastic
Models in Business and Industry, 28(1): 91-102. doi: <a href="http://doi.org/10.1002/asmb.901">10.1002/asmb.901</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+chat_pois">chat_pois</a></code>,
<code><a href="#topic+findintercorr">findintercorr</a></code>, <code><a href="#topic+rcorrvar">rcorrvar</a></code>
</p>

<hr>
<h2 id='findintercorr_cont'>Calculate Intermediate MVN Correlation for Continuous Variables Generated by Polynomial Transformation</h2><span id='topic+findintercorr_cont'></span>

<h3>Description</h3>

<p>This function finds the roots to the equations in <code><a href="#topic+intercorr_fleish">intercorr_fleish</a></code> or
<code><a href="#topic+intercorr_poly">intercorr_poly</a></code> using <code><a href="nleqslv.html#topic+nleqslv">nleqslv</a></code>.  It is used in
<code><a href="#topic+findintercorr">findintercorr</a></code> and
<code><a href="#topic+findintercorr2">findintercorr2</a></code> to find the intermediate correlation for standard normal random variables
which are used in Fleishman's Third-Order (doi: <a href="http://doi.org/10.1007/BF02293811">10.1007/BF02293811</a>) or Headrick's Fifth-Order
(doi: <a href="http://doi.org/10.1016/S0167-9473(02)00072-5">10.1016/S0167-9473(02)00072-5</a>) Polynomial Transformation.  It works for two or three
variables in the case of <code>method</code> = &quot;Fleishman&quot;, or two, three, or four variables in the case of <code>method</code> = &quot;Polynomial&quot;.
Otherwise, Headrick &amp; Sawilowsky (1999, doi: <a href="http://doi.org/10.1007/BF02294317">10.1007/BF02294317</a>) recommend using the technique of Vale &amp; Maurelli (1983,
doi: <a href="http://doi.org/10.1007/BF02293687">10.1007/BF02293687</a>), in which
the intermediate correlations are found pairwise and then eigen value decomposition is used on the intermediate
correlation matrix.  This function would not ordinarily be called by the user.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>findintercorr_cont(method = c("Fleishman", "Polynomial"), constants, rho_cont)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="findintercorr_cont_+3A_method">method</code></td>
<td>
<p>the method used to generate the continuous variables.  &quot;Fleishman&quot; uses Fleishman's third-order polynomial transformation
and &quot;Polynomial&quot; uses Headrick's fifth-order transformation.</p>
</td></tr>
<tr><td><code id="findintercorr_cont_+3A_constants">constants</code></td>
<td>
<p>a matrix with either 2, 3, or 4 rows, each a vector of constants c0, c1, c2, c3 (if <code>method</code> = &quot;Fleishman&quot;) or
c0, c1, c2, c3, c4, c5 (if <code>method</code> = &quot;Polynomial&quot;), like that returned by
<code><a href="#topic+find_constants">find_constants</a></code></p>
</td></tr>
<tr><td><code id="findintercorr_cont_+3A_rho_cont">rho_cont</code></td>
<td>
<p>a matrix of target correlations among continuous variables; if <code>nrow(rho_cont) = 1</code>, it represents a pairwise
correlation; if <code>nrow(rho_cont) = 2, 3, or 4</code>, it represents a correlation matrix between two, three, or four variables</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list containing the results from <code><a href="nleqslv.html#topic+nleqslv">nleqslv</a></code>
</p>


<h3>References</h3>

<p>Fleishman AI (1978). A Method for Simulating Non-normal Distributions. Psychometrika, 43, 521-532. doi: <a href="http://doi.org/10.1007/BF02293811">10.1007/BF02293811</a>.
</p>
<p>Hasselman B (2018). nleqslv: Solve Systems of Nonlinear Equations. R package version 3.3.2.
<a href="https://CRAN.R-project.org/package=nleqslv">https://CRAN.R-project.org/package=nleqslv</a>
</p>
<p>Headrick TC (2002). Fast Fifth-order Polynomial Transforms for Generating Univariate and Multivariate
Non-normal Distributions. Computational Statistics &amp; Data Analysis, 40(4):685-711. doi: <a href="http://doi.org/10.1016/S0167-9473(02)00072-5">10.1016/S0167-9473(02)00072-5</a>.
(<a href="http://www.sciencedirect.com/science/article/pii/S0167947302000725">ScienceDirect</a>)
</p>
<p>Headrick TC (2004). On Polynomial Transformations for Simulating Multivariate Nonnormal Distributions.
Journal of Modern Applied Statistical Methods, 3(1), 65-71. doi: <a href="http://doi.org/10.22237/jmasm/1083370080">10.22237/jmasm/1083370080</a>.
</p>
<p>Headrick TC, Kowalchuk RK (2007). The Power Method Transformation: Its Probability Density Function, Distribution
Function, and Its Further Use for Fitting Data. Journal of Statistical Computation and Simulation, 77, 229-249. doi: <a href="http://doi.org/10.1080/10629360600605065">10.1080/10629360600605065</a>.
</p>
<p>Headrick TC, Sawilowsky SS (1999). Simulating Correlated Non-normal Distributions: Extending the Fleishman Power
Method. Psychometrika, 64, 25-35. doi: <a href="http://doi.org/10.1007/BF02294317">10.1007/BF02294317</a>.
</p>
<p>Headrick TC, Sheng Y, &amp; Hodis FA (2007). Numerical Computing and Graphics for the Power Method Transformation Using
Mathematica. Journal of Statistical Software, 19(3), 1 - 17. doi: <a href="http://doi.org/10.18637/jss.v019.i03">10.18637/jss.v019.i03</a>.
</p>
<p>Vale CD &amp; Maurelli VA (1983). Simulating Multivariate Nonnormal Distributions. Psychometrika, 48, 465-471. doi: <a href="http://doi.org/10.1007/BF02293687">10.1007/BF02293687</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+poly">poly</a></code>, <code><a href="#topic+fleish">fleish</a></code>, <code><a href="#topic+power_norm_corr">power_norm_corr</a></code>,
<code><a href="#topic+pdf_check">pdf_check</a></code>, <code><a href="#topic+find_constants">find_constants</a></code>,
<code><a href="#topic+intercorr_fleish">intercorr_fleish</a></code>, <br />
<code><a href="#topic+intercorr_poly">intercorr_poly</a></code>, <code><a href="nleqslv.html#topic+nleqslv">nleqslv</a></code>
</p>

<hr>
<h2 id='findintercorr_cont_cat'>Calculate Intermediate MVN Correlation for Continuous - Ordinal Variables</h2><span id='topic+findintercorr_cont_cat'></span>

<h3>Description</h3>

<p>This function calculates a <code>k_cont x k_cat</code> intermediate matrix of correlations for the <code>k_cont</code> continuous and
<code>k_cat</code> ordinal (r &gt;= 2 categories) variables. It extends the method of Demirtas et al. (2012, doi: <a href="http://doi.org/10.1198/tast.2011.10090">10.1198/tast.2011.10090</a>)
in simulating binary and non-normal data using the Fleishman transformation by:
</p>
<p>1) allowing the continuous variables to be generated via Fleishman's third-order or Headrick's fifth-order transformation, and
</p>
<p>2) allowing for ordinal variables with more than 2 categories.
</p>
<p>Here, the intermediate correlation between Z1 and Z2 (where Z1 is the standard normal variable transformed using Headrick's
fifth-order or Fleishman's third-order method to produce a continuous variable Y1, and Z2 is the standard normal variable
discretized to produce an ordinal variable Y2) is calculated by dividing the target correlation by a correction factor.  The
correction factor is the product of the point-polyserial correlation between Y2 and Z2 (described in Olsson et al., 1982,
doi: <a href="http://doi.org/10.1007/BF02294164">10.1007/BF02294164</a>)
and the power method correlation (described in Headrick &amp; Kowalchuk, 2007, doi: <a href="http://doi.org/10.1080/10629360600605065">10.1080/10629360600605065</a>) between Y1 and Z1.
The point-polyserial correlation is given by:
</p>
<p style="text-align: center;"><code class="reqn">\rho_{y2,z2} = (1/\sigma_{y2})*\sum_{j = 1}^{r-1} \phi(\tau_{j})(y2_{j+1} - y2_{j})</code>
</p>
<p> where
</p>
<p style="text-align: center;"><code class="reqn">\phi(\tau) = (2\pi)^{-1/2}*exp(-\tau^2/2)</code>
</p>
<p>  Here, <code class="reqn">y_{j}</code> is the j-th support
value and <code class="reqn">\tau_{j}</code> is <code class="reqn">\Phi^{-1}(\sum_{i=1}^{j} Pr(Y = y_{i}))</code>.  The power method correlation is given by:
</p>
<p style="text-align: center;"><code class="reqn">\rho_{y1,z1} = c1 + 3c3 + 15c5</code>
</p>
<p> where c5 = 0 if <code>method</code> = &quot;Fleishman&quot;.  The function is used in
<code><a href="#topic+findintercorr">findintercorr</a></code> and
<code><a href="#topic+findintercorr2">findintercorr2</a></code>.  This function would not ordinarily be called by the user.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>findintercorr_cont_cat(method = c("Fleishman", "Polynomial"), constants,
  rho_cont_cat, marginal, support)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="findintercorr_cont_cat_+3A_method">method</code></td>
<td>
<p>the method used to generate the k_cont continuous variables.  &quot;Fleishman&quot; uses a third-order polynomial transformation
and &quot;Polynomial&quot; uses Headrick's fifth-order transformation.</p>
</td></tr>
<tr><td><code id="findintercorr_cont_cat_+3A_constants">constants</code></td>
<td>
<p>a matrix with <code>k_cont</code> rows, each a vector of constants c0, c1, c2, c3 (if <code>method</code> = &quot;Fleishman&quot;) or
c0, c1, c2, c3, c4, c5 (if <code>method</code> = &quot;Polynomial&quot;), like that returned by
<code><a href="#topic+find_constants">find_constants</a></code></p>
</td></tr>
<tr><td><code id="findintercorr_cont_cat_+3A_rho_cont_cat">rho_cont_cat</code></td>
<td>
<p>a <code>k_cont x k_cat</code> matrix of target correlations among continuous and ordinal variables</p>
</td></tr>
<tr><td><code id="findintercorr_cont_cat_+3A_marginal">marginal</code></td>
<td>
<p>a list of length equal to <code>k_cat</code>; the i-th element is a vector of the cumulative
probabilities defining the marginal distribution of the i-th variable;
if the variable can take r values, the vector will contain r - 1 probabilities (the r-th is assumed to be 1)</p>
</td></tr>
<tr><td><code id="findintercorr_cont_cat_+3A_support">support</code></td>
<td>
<p>a list of length equal to <code>k_cat</code>; the i-th element is a vector of containing the r
ordered support values</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>k_cont x k_cat</code> matrix whose rows represent the <code>k_cont</code> continuous variables and columns represent
the <code>k_cat</code> ordinal variables
</p>


<h3>References</h3>

<p>Demirtas H, Hedeker D, &amp; Mermelstein RJ (2012). Simulation of massive public health data by power polynomials.
Statistics in Medicine, 31(27): 3337-3346. doi: <a href="http://doi.org/10.1002/sim.5362">10.1002/sim.5362</a>.
</p>
<p>Fleishman AI (1978). A Method for Simulating Non-normal Distributions. Psychometrika, 43, 521-532. doi: <a href="http://doi.org/10.1007/BF02293811">10.1007/BF02293811</a>.
</p>
<p>Headrick TC (2002). Fast Fifth-order Polynomial Transforms for Generating Univariate and Multivariate
Non-normal Distributions. Computational Statistics &amp; Data Analysis, 40(4):685-711. doi: <a href="http://doi.org/10.1016/S0167-9473(02)00072-5">10.1016/S0167-9473(02)00072-5</a>.
(<a href="http://www.sciencedirect.com/science/article/pii/S0167947302000725">ScienceDirect</a>)
</p>
<p>Headrick TC (2004). On Polynomial Transformations for Simulating Multivariate Nonnormal Distributions.
Journal of Modern Applied Statistical Methods, 3(1), 65-71. doi: <a href="http://doi.org/10.22237/jmasm/1083370080">10.22237/jmasm/1083370080</a>.
</p>
<p>Headrick TC, Kowalchuk RK (2007). The Power Method Transformation: Its Probability Density Function, Distribution
Function, and Its Further Use for Fitting Data. Journal of Statistical Computation and Simulation, 77, 229-249. doi: <a href="http://doi.org/10.1080/10629360600605065">10.1080/10629360600605065</a>.
</p>
<p>Headrick TC, Sawilowsky SS (1999). Simulating Correlated Non-normal Distributions: Extending the Fleishman Power
Method. Psychometrika, 64, 25-35. doi: <a href="http://doi.org/10.1007/BF02294317">10.1007/BF02294317</a>.
</p>
<p>Headrick TC, Sheng Y, &amp; Hodis FA (2007). Numerical Computing and Graphics for the Power Method Transformation Using
Mathematica. Journal of Statistical Software, 19(3), 1 - 17. doi: <a href="http://doi.org/10.18637/jss.v019.i03">10.18637/jss.v019.i03</a>.
</p>
<p>Olsson U, Drasgow F, &amp; Dorans NJ (1982). The Polyserial Correlation Coefficient. Psychometrika, 47(3): 337-47.
doi: <a href="http://doi.org/10.1007/BF02294164">10.1007/BF02294164</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+power_norm_corr">power_norm_corr</a></code>, <code><a href="#topic+find_constants">find_constants</a></code>,
<code><a href="#topic+findintercorr">findintercorr</a></code>, <code><a href="#topic+findintercorr2">findintercorr2</a></code>
</p>

<hr>
<h2 id='findintercorr_cont_nb'>Calculate Intermediate MVN Correlation for Continuous - Negative Binomial Variables: Correlation Method 1</h2><span id='topic+findintercorr_cont_nb'></span>

<h3>Description</h3>

<p>This function calculates a <code>k_cont x k_nb</code> intermediate matrix of correlations for the <code>k_cont</code> continuous and
<code>k_nb</code> Negative Binomial variables. It extends the method of Amatya &amp; Demirtas (2015, doi: <a href="http://doi.org/10.1080/00949655.2014.953534">10.1080/00949655.2014.953534</a>) to
continuous variables generated using
Headrick's fifth-order polynomial transformation and Negative Binomial variables.  Here, the intermediate correlation
between Z1 and Z2 (where Z1 is the standard normal variable transformed using Headrick's fifth-order or Fleishman's
third-order method to produce a continuous variable Y1, and Z2 is the standard normal variable used to generate a
Negative Binomial variable via the inverse cdf method) is calculated by dividing the target correlation by a correction factor.
The correction factor is the product of the upper Frechet-Hoeffding bound on the correlation between a Negative Binomial
variable and the normal variable used to generate it (see <code><a href="#topic+chat_nb">chat_nb</a></code>) and the power method
correlation (described in Headrick &amp; Kowalchuk, 2007, doi: <a href="http://doi.org/10.1080/10629360600605065">10.1080/10629360600605065</a>) between Y1 and Z1.  The function is used in
<code><a href="#topic+findintercorr">findintercorr</a></code> and <code><a href="#topic+rcorrvar">rcorrvar</a></code>.
This function would not ordinarily be called by the user.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>findintercorr_cont_nb(method, constants, rho_cont_nb, size, prob, mu = NULL,
  nrand = 100000, seed = 1234)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="findintercorr_cont_nb_+3A_method">method</code></td>
<td>
<p>the method used to generate the <code>k_cont</code> continuous variables.  &quot;Fleishman&quot; uses a third-order polynomial transformation
and &quot;Polynomial&quot; uses Headrick's fifth-order transformation.</p>
</td></tr>
<tr><td><code id="findintercorr_cont_nb_+3A_constants">constants</code></td>
<td>
<p>a matrix with <code>k_cont</code> rows, each a vector of constants c0, c1, c2, c3 (if <code>method</code> = &quot;Fleishman&quot;) or
c0, c1, c2, c3, c4, c5 (if <code>method</code> = &quot;Polynomial&quot;), like that returned by <code><a href="#topic+find_constants">find_constants</a></code></p>
</td></tr>
<tr><td><code id="findintercorr_cont_nb_+3A_rho_cont_nb">rho_cont_nb</code></td>
<td>
<p>a <code>k_cont x k_nb</code> matrix of target correlations among continuous and Negative Binomial variables</p>
</td></tr>
<tr><td><code id="findintercorr_cont_nb_+3A_size">size</code></td>
<td>
<p>a vector of size parameters for the Negative Binomial variables (see <code><a href="stats.html#topic+NegBinomial">NegBinomial</a></code>)</p>
</td></tr>
<tr><td><code id="findintercorr_cont_nb_+3A_prob">prob</code></td>
<td>
<p>a vector of success probability parameters</p>
</td></tr>
<tr><td><code id="findintercorr_cont_nb_+3A_mu">mu</code></td>
<td>
<p>a vector of mean parameters (*Note: either <code>prob</code> or <code>mu</code> should be supplied for all Negative Binomial variables,
not a mixture; default = NULL)</p>
</td></tr>
<tr><td><code id="findintercorr_cont_nb_+3A_nrand">nrand</code></td>
<td>
<p>the number of random numbers to generate in calculating the bound (default = 10000)</p>
</td></tr>
<tr><td><code id="findintercorr_cont_nb_+3A_seed">seed</code></td>
<td>
<p>the seed used in random number generation (default = 1234)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>k_cont x k_nb</code> matrix whose rows represent the <code>k_cont</code> continuous variables and columns represent the
<code>k_nb</code> Negative Binomial variables
</p>


<h3>References</h3>

<p>Please see references for <code><a href="#topic+findintercorr_cont_pois">findintercorr_cont_pois</a></code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+chat_nb">chat_nb</a></code>, <code><a href="#topic+power_norm_corr">power_norm_corr</a></code>,
<code><a href="#topic+find_constants">find_constants</a></code>,
<code><a href="#topic+findintercorr">findintercorr</a></code>, <code><a href="#topic+rcorrvar">rcorrvar</a></code>
</p>

<hr>
<h2 id='findintercorr_cont_nb2'>Calculate Intermediate MVN Correlation for Continuous - Negative Binomial Variables: Correlation Method 2</h2><span id='topic+findintercorr_cont_nb2'></span>

<h3>Description</h3>

<p>This function calculates a <code>k_cont x k_nb</code> intermediate matrix of correlations for the <code>k_cont</code> continuous and
<code>k_nb</code> Negative Binomial variables. It extends the methods of Demirtas et al. (2012, doi: <a href="http://doi.org/10.1002/sim.5362">10.1002/sim.5362</a>) and
Barbiero &amp; Ferrari (2015, doi: <a href="http://doi.org/10.1002/asmb.2072">10.1002/asmb.2072</a>) by:
</p>
<p>1) including non-normal continuous and count (Poisson and Negative Binomial) variables
</p>
<p>2) allowing the continuous variables to be generated via Fleishman's third-order or Headrick's fifth-order transformation, and
</p>
<p>3) since the count variables are treated as ordinal, using the point-polyserial and polyserial correlations to calculate the
intermediate correlations (similar to <code><a href="#topic+findintercorr_cont_cat">findintercorr_cont_cat</a></code>).
</p>
<p>Here, the intermediate correlation between Z1 and Z2 (where Z1
is the standard normal variable transformed using Headrick's fifth-order or Fleishman's third-order method to produce a
continuous variable Y1, and Z2 is the standard normal variable used to generate a Negative Binomial variable via the inverse
cdf method) is calculated by dividing the target correlation by a correction factor.  The correction factor is the product of the
point-polyserial correlation between Y2 and Z2 (described in Olsson et al., 1982, doi: <a href="http://doi.org/10.1007/BF02294164">10.1007/BF02294164</a>)
and the power method correlation (described in Headrick &amp; Kowalchuk, 2007, doi: <a href="http://doi.org/10.1080/10629360600605065">10.1080/10629360600605065</a>) between Y1 and Z1.
After the maximum support value has been found using
<code><a href="#topic+max_count_support">max_count_support</a></code>, the point-polyserial correlation is given by:
</p>
<p style="text-align: center;"><code class="reqn">\rho_{y2,z2} = (1/\sigma_{y2})\sum_{j = 1}^{r-1} \phi(\tau_{j})(y2_{j+1} - y2_{j})</code>
</p>
<p> where
</p>
<p style="text-align: center;"><code class="reqn">\phi(\tau) = (2\pi)^{-1/2}*exp(-\tau^2/2)</code>
</p>
<p>  Here, <code class="reqn">y_{j}</code> is the j-th support
value and <code class="reqn">\tau_{j}</code> is <code class="reqn">\Phi^{-1}(\sum_{i=1}^{j} Pr(Y = y_{i}))</code>.  The power method correlation is given by:
</p>
<p style="text-align: center;"><code class="reqn">\rho_{y1,z1} = c1 + 3c3 + 15c5</code>
</p>
<p>, where c5 = 0 if <code>method</code> = &quot;Fleishman&quot;.  The function is used in
<code><a href="#topic+findintercorr2">findintercorr2</a></code> and <code><a href="#topic+rcorrvar2">rcorrvar2</a></code>.
This function would not ordinarily be called by the user.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>findintercorr_cont_nb2(method, constants, rho_cont_nb, nb_marg, nb_support)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="findintercorr_cont_nb2_+3A_method">method</code></td>
<td>
<p>the method used to generate the <code>k_cont</code> continuous variables.  &quot;Fleishman&quot; uses Fleishman's third-order polynomial transformation
and &quot;Polynomial&quot; uses Headrick's fifth-order transformation.</p>
</td></tr>
<tr><td><code id="findintercorr_cont_nb2_+3A_constants">constants</code></td>
<td>
<p>a matrix with <code>k_cont</code> rows, each a vector of constants c0, c1, c2, c3 (if <code>method</code> = &quot;Fleishman&quot;) or
c0, c1, c2, c3, c4, c5 (if <code>method</code> = &quot;Polynomial&quot;), like that returned by <code><a href="#topic+find_constants">find_constants</a></code></p>
</td></tr>
<tr><td><code id="findintercorr_cont_nb2_+3A_rho_cont_nb">rho_cont_nb</code></td>
<td>
<p>a <code>k_cont x k_nb</code> matrix of target correlations among continuous and Negative Binomial variables</p>
</td></tr>
<tr><td><code id="findintercorr_cont_nb2_+3A_nb_marg">nb_marg</code></td>
<td>
<p>a list of length equal to <code>k_nb</code>; the i-th element is a vector of the cumulative
probabilities defining the marginal distribution of the i-th variable;
if the variable can take r values, the vector will contain r - 1 probabilities (the r-th is assumed to be 1);
this is created within <code><a href="#topic+findintercorr2">findintercorr2</a></code> and <code><a href="#topic+rcorrvar2">rcorrvar2</a></code></p>
</td></tr>
<tr><td><code id="findintercorr_cont_nb2_+3A_nb_support">nb_support</code></td>
<td>
<p>a list of length equal to <code>k_nb</code>; the i-th element is a vector of containing the r
ordered support values, with a minimum of 0 and maximum determined via <code><a href="#topic+max_count_support">max_count_support</a></code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>k_cont x k_nb</code> matrix whose rows represent the <code>k_cont</code> continuous variables and columns represent the
<code>k_nb</code> Negative Binomial variables
</p>


<h3>References</h3>

<p>Please see additional references in <code><a href="#topic+findintercorr_cont_cat">findintercorr_cont_cat</a></code>.
</p>
<p>Barbiero A &amp; Ferrari PA (2015). Simulation of correlated Poisson variables. Applied Stochastic Models in
Business and Industry, 31: 669-80. doi: <a href="http://doi.org/10.1002/asmb.2072">10.1002/asmb.2072</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+find_constants">find_constants</a></code>, <code><a href="#topic+power_norm_corr">power_norm_corr</a></code>,
<code><a href="#topic+findintercorr2">findintercorr2</a></code>, <code><a href="#topic+rcorrvar2">rcorrvar2</a></code>
</p>

<hr>
<h2 id='findintercorr_cont_pois'>Calculate Intermediate MVN Correlation for Continuous - Poisson Variables: Correlation Method 1</h2><span id='topic+findintercorr_cont_pois'></span>

<h3>Description</h3>

<p>This function calculates a <code>k_cont x k_pois</code> intermediate matrix of correlations for the <code>k_cont</code> continuous and
<code>k_pois</code> Poisson variables. It extends the method of Amatya &amp; Demirtas (2015, doi: <a href="http://doi.org/10.1080/00949655.2014.953534">10.1080/00949655.2014.953534</a>) to continuous
variables generated using Headrick's fifth-order polynomial transformation.  Here, the intermediate correlation between Z1 and Z2 (where Z1 is
the standard normal variable transformed using Headrick's fifth-order or Fleishman's third-order method to produce a
continuous variable Y1, and Z2 is the standard normal variable used to generate a Poisson variable via the inverse cdf method) is
calculated by dividing the target correlation by a correction factor.  The correction factor is the product of the
upper Frechet-Hoeffding bound on the correlation between a Poisson variable and the normal variable used to generate it
(see <code><a href="#topic+chat_pois">chat_pois</a></code>) and the power method correlation (described in Headrick &amp; Kowalchuk, 2007,
doi: <a href="http://doi.org/10.1080/10629360600605065">10.1080/10629360600605065</a>) between Y1 and Z1.  The function is used in <code><a href="#topic+findintercorr">findintercorr</a></code> and
<code><a href="#topic+rcorrvar">rcorrvar</a></code>.  This function would not ordinarily be called by the user.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>findintercorr_cont_pois(method, constants, rho_cont_pois, lam, nrand = 100000,
  seed = 1234)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="findintercorr_cont_pois_+3A_method">method</code></td>
<td>
<p>the method used to generate the <code>k_cont</code> continuous variables.  &quot;Fleishman&quot; uses a third-order polynomial transformation
and &quot;Polynomial&quot; uses Headrick's fifth-order transformation.</p>
</td></tr>
<tr><td><code id="findintercorr_cont_pois_+3A_constants">constants</code></td>
<td>
<p>a matrix with <code>k_cont</code> rows, each a vector of constants c0, c1, c2, c3 (if <code>method</code> = &quot;Fleishman&quot;) or
c0, c1, c2, c3, c4, c5 (if <code>method</code> = &quot;Polynomial&quot;), like that returned by <code><a href="#topic+find_constants">find_constants</a></code></p>
</td></tr>
<tr><td><code id="findintercorr_cont_pois_+3A_rho_cont_pois">rho_cont_pois</code></td>
<td>
<p>a <code>k_cont x k_pois</code> matrix of target correlations among continuous and Poisson variables</p>
</td></tr>
<tr><td><code id="findintercorr_cont_pois_+3A_lam">lam</code></td>
<td>
<p>a vector of lambda (&gt; 0) constants for the Poisson variables (see <code><a href="stats.html#topic+Poisson">Poisson</a></code>)</p>
</td></tr>
<tr><td><code id="findintercorr_cont_pois_+3A_nrand">nrand</code></td>
<td>
<p>the number of random numbers to generate in calculating the bound (default = 10000)</p>
</td></tr>
<tr><td><code id="findintercorr_cont_pois_+3A_seed">seed</code></td>
<td>
<p>the seed used in random number generation (default = 1234)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>k_cont x k_pois</code> matrix whose rows represent the <code>k_cont</code> continuous variables and columns represent the
<code>k_pois</code> Poisson variables
</p>


<h3>References</h3>

<p>Amatya A &amp; Demirtas H (2015). Simultaneous generation of multivariate mixed data with Poisson and normal marginals.
Journal of Statistical Computation and Simulation, 85(15): 3129-39. doi: <a href="http://doi.org/10.1080/00949655.2014.953534">10.1080/00949655.2014.953534</a>.
</p>
<p>Demirtas H &amp; Hedeker D (2011). A practical way for computing approximate lower and upper correlation bounds.
American Statistician, 65(2): 104-109. doi: <a href="http://doi.org/10.1198/tast.2011.10090">10.1198/tast.2011.10090</a>.
</p>
<p>Fleishman AI (1978). A Method for Simulating Non-normal Distributions. Psychometrika, 43, 521-532. doi: <a href="http://doi.org/10.1007/BF02293811">10.1007/BF02293811</a>.
</p>
<p>Frechet M.  Sur les tableaux de correlation dont les marges sont donnees.  Ann. l'Univ. Lyon SectA.  1951;14:53-77.
</p>
<p>Headrick TC (2002). Fast Fifth-order Polynomial Transforms for Generating Univariate and Multivariate
Non-normal Distributions. Computational Statistics &amp; Data Analysis, 40(4):685-711. doi: <a href="http://doi.org/10.1016/S0167-9473(02)00072-5">10.1016/S0167-9473(02)00072-5</a>.
(<a href="http://www.sciencedirect.com/science/article/pii/S0167947302000725">ScienceDirect</a>)
</p>
<p>Headrick TC (2004). On Polynomial Transformations for Simulating Multivariate Nonnormal Distributions.
Journal of Modern Applied Statistical Methods, 3(1), 65-71. doi: <a href="http://doi.org/10.22237/jmasm/1083370080">10.22237/jmasm/1083370080</a>.
</p>
<p>Headrick TC, Kowalchuk RK (2007). The Power Method Transformation: Its Probability Density Function, Distribution
Function, and Its Further Use for Fitting Data. Journal of Statistical Computation and Simulation, 77, 229-249. doi: <a href="http://doi.org/10.1080/10629360600605065">10.1080/10629360600605065</a>.
</p>
<p>Headrick TC, Sawilowsky SS (1999). Simulating Correlated Non-normal Distributions: Extending the Fleishman Power
Method. Psychometrika, 64, 25-35. doi: <a href="http://doi.org/10.1007/BF02294317">10.1007/BF02294317</a>.
</p>
<p>Headrick TC, Sheng Y, &amp; Hodis FA (2007). Numerical Computing and Graphics for the Power Method Transformation Using
Mathematica. Journal of Statistical Software, 19(3), 1 - 17. doi: <a href="http://doi.org/10.18637/jss.v019.i03">10.18637/jss.v019.i03</a>.
</p>
<p>Hoeffding W. Scale-invariant correlation theory. In: Fisher NI, Sen PK, editors. The collected works of Wassily Hoeffding.
New York: Springer-Verlag; 1994. p. 57-107.
</p>
<p>Yahav I &amp; Shmueli G (2012). On Generating Multivariate Poisson Data in Management Science Applications. Applied Stochastic
Models in Business and Industry, 28(1): 91-102. doi: <a href="http://doi.org/10.1002/asmb.901">10.1002/asmb.901</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+chat_pois">chat_pois</a></code>, <code><a href="#topic+power_norm_corr">power_norm_corr</a></code>,
<code><a href="#topic+find_constants">find_constants</a></code>,
<code><a href="#topic+findintercorr">findintercorr</a></code>, <code><a href="#topic+rcorrvar">rcorrvar</a></code>
</p>

<hr>
<h2 id='findintercorr_cont_pois2'>Calculate Intermediate MVN Correlation for Continuous - Poisson Variables: Correlation Method 2</h2><span id='topic+findintercorr_cont_pois2'></span>

<h3>Description</h3>

<p>This function calculates a <code>k_cont x k_pois</code> intermediate matrix of correlations for the <code>k_cont</code> continuous and
<code>k_pois</code> Poisson variables. It extends the methods of Demirtas et al. (2012, doi: <a href="http://doi.org/10.1002/sim.5362">10.1002/sim.5362</a>) and
Barbiero &amp; Ferrari (2015, doi: <a href="http://doi.org/10.1002/asmb.2072">10.1002/asmb.2072</a>) by:
</p>
<p>1) including non-normal continuous and count variables
</p>
<p>2) allowing the continuous variables to be generated via Fleishman's third-order or Headrick's fifth-order transformation, and
</p>
<p>3) since the count variables are treated as ordinal, using the point-polyserial and polyserial correlations to calculate the
intermediate correlations (similar to <code><a href="#topic+findintercorr_cont_cat">findintercorr_cont_cat</a></code>).
</p>
<p>Here, the intermediate correlation between Z1 and Z2 (where Z1
is the standard normal variable transformed using Headrick's fifth-order or Fleishman's third-order method to produce a
continuous variable Y1, and Z2 is the standard normal variable used to generate a Poisson variable via the inverse cdf method)
is calculated by dividing the target correlation by a correction factor.  The correction factor is the product of the
point-polyserial correlation between Y2 and Z2 (described in Olsson et al., 1982, doi: <a href="http://doi.org/10.1007/BF02294164">10.1007/BF02294164</a>)
and the power method correlation (described in Headrick &amp; Kowalchuk, 2007, doi: <a href="http://doi.org/10.1080/10629360600605065">10.1080/10629360600605065</a>) between Y1 and Z1.
After the maximum support value has been found using
<code><a href="#topic+max_count_support">max_count_support</a></code>, the point-polyserial correlation is given by:
</p>
<p style="text-align: center;"><code class="reqn">\rho_{y2,z2} = (1/\sigma_{y2})\sum_{j = 1}^{r-1} \phi(\tau_{j})(y2_{j+1} - y2_{j})</code>
</p>
<p> where
</p>
<p style="text-align: center;"><code class="reqn">\phi(\tau) = (2\pi)^{-1/2}*exp(-\tau^2/2)</code>
</p>
<p>  Here, <code class="reqn">y_{j}</code> is the j-th support
value and <code class="reqn">\tau_{j}</code> is <code class="reqn">\Phi^{-1}(\sum_{i=1}^{j} Pr(Y = y_{i}))</code>.  The power method correlation is given by:
</p>
<p style="text-align: center;"><code class="reqn">\rho_{y1,z1} = c1 + 3c3 + 15c5</code>
</p>
<p>, where c5 = 0 if <code>method</code> = &quot;Fleishman&quot;.  The function is used in
<code><a href="#topic+findintercorr2">findintercorr2</a></code> and <code><a href="#topic+rcorrvar2">rcorrvar2</a></code>.
This function would not ordinarily be called by the user.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>findintercorr_cont_pois2(method, constants, rho_cont_pois, pois_marg,
  pois_support)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="findintercorr_cont_pois2_+3A_method">method</code></td>
<td>
<p>the method used to generate the <code>k_cont</code> continuous variables.  &quot;Fleishman&quot; uses Fleishman's third-order polynomial transformation
and &quot;Polynomial&quot; uses Headrick's fifth-order transformation.</p>
</td></tr>
<tr><td><code id="findintercorr_cont_pois2_+3A_constants">constants</code></td>
<td>
<p>a matrix with <code>k_cont</code> rows, each a vector of constants c0, c1, c2, c3 (if <code>method</code> = &quot;Fleishman&quot;) or
c0, c1, c2, c3, c4, c5 (if <code>method</code> = &quot;Polynomial&quot;), like that returned by <code><a href="#topic+find_constants">find_constants</a></code></p>
</td></tr>
<tr><td><code id="findintercorr_cont_pois2_+3A_rho_cont_pois">rho_cont_pois</code></td>
<td>
<p>a <code>k_cont x k_pois</code> matrix of target correlations among continuous and Poisson variables</p>
</td></tr>
<tr><td><code id="findintercorr_cont_pois2_+3A_pois_marg">pois_marg</code></td>
<td>
<p>a list of length equal to <code>k_pois</code>; the i-th element is a vector of the cumulative
probabilities defining the marginal distribution of the i-th variable;
if the variable can take r values, the vector will contain r - 1 probabilities (the r-th is assumed to be 1);
this is created within <code><a href="#topic+findintercorr2">findintercorr2</a></code> and <code><a href="#topic+rcorrvar2">rcorrvar2</a></code></p>
</td></tr>
<tr><td><code id="findintercorr_cont_pois2_+3A_pois_support">pois_support</code></td>
<td>
<p>a list of length equal to <code>k_pois</code>; the i-th element is a vector of containing the r
ordered support values, with a minimum of 0 and maximum determined via <code><a href="#topic+max_count_support">max_count_support</a></code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>k_cont x k_pois</code> matrix whose rows represent the <code>k_cont</code> continuous variables and columns represent the
<code>k_pois</code> Poisson variables
</p>


<h3>References</h3>

<p>Please see additional references in <code><a href="#topic+findintercorr_cont_cat">findintercorr_cont_cat</a></code>.
</p>
<p>Barbiero A &amp; Ferrari PA (2015). Simulation of correlated Poisson variables. Applied Stochastic Models in
Business and Industry, 31: 669-80. doi: <a href="http://doi.org/10.1002/asmb.2072">10.1002/asmb.2072</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+find_constants">find_constants</a></code>, <code><a href="#topic+power_norm_corr">power_norm_corr</a></code>,
<code><a href="#topic+findintercorr2">findintercorr2</a></code>, <code><a href="#topic+rcorrvar2">rcorrvar2</a></code>
</p>

<hr>
<h2 id='findintercorr_nb'>Calculate Intermediate MVN Correlation for Negative Binomial Variables: Correlation Method 1</h2><span id='topic+findintercorr_nb'></span>

<h3>Description</h3>

<p>This function calculates a <code>k_nb x k_nb</code> intermediate matrix of correlations for the Negative Binomial variables by
extending the method of Yahav &amp; Shmueli (2012, doi: <a href="http://doi.org/10.1002/asmb.901">10.1002/asmb.901</a>). The intermediate correlation between Z1 and Z2 (the
standard normal variables used to generate the Negative Binomial variables Y1 and Y2 via the inverse cdf method) is
calculated using a logarithmic transformation of the target correlation.  First, the upper and lower Frechet-Hoeffding bounds
(mincor, maxcor) on <code class="reqn">\rho_{y1,y2}</code> are simulated.  Then the intermediate correlation is found as follows:
</p>
<p style="text-align: center;"><code class="reqn">\rho_{z1,z2} = (1/b) * log((\rho_{y1,y2} - c)/a)</code>
</p>
<p>, where <code class="reqn">a = -(maxcor * mincor)/(maxcor + mincor)</code>,
<code class="reqn">b = log((maxcor + a)/a)</code>, and <code class="reqn">c = -a</code>.  The function adapts code from Amatya &amp; Demirtas' (2016) package
<code><a href="PoisNor.html#topic+PoisNor-package">PoisNor-package</a></code> by:
</p>
<p>1) allowing specifications for the number of random variates and the seed for reproducibility
</p>
<p>2) providing the following checks: if <code class="reqn">\rho_{z1,z2}</code> &gt;= 1, <code class="reqn">\rho_{z1,z2}</code> is set to 0.99; if <code class="reqn">\rho_{z1,z2}</code> &lt;= -1,
<code class="reqn">\rho_{z1,z2}</code> is set to -0.99
</p>
<p>3) simulating Negative Binomial variables.
</p>
<p>The function is used in <code><a href="#topic+findintercorr">findintercorr</a></code> and <code><a href="#topic+rcorrvar">rcorrvar</a></code>.
This function would not ordinarily be called by the user.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>findintercorr_nb(rho_nb, size, prob, mu = NULL, nrand = 100000,
  seed = 1234)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="findintercorr_nb_+3A_rho_nb">rho_nb</code></td>
<td>
<p>a <code>k_nb x k_nb</code> matrix of target correlations</p>
</td></tr>
<tr><td><code id="findintercorr_nb_+3A_size">size</code></td>
<td>
<p>a vector of size parameters for the Negative Binomial variables (see <code><a href="stats.html#topic+NegBinomial">NegBinomial</a></code>)</p>
</td></tr>
<tr><td><code id="findintercorr_nb_+3A_prob">prob</code></td>
<td>
<p>a vector of success probability parameters</p>
</td></tr>
<tr><td><code id="findintercorr_nb_+3A_mu">mu</code></td>
<td>
<p>a vector of mean parameters (*Note: either <code>prob</code> or <code>mu</code> should be supplied for all Negative Binomial variables,
not a mixture; default = NULL)</p>
</td></tr>
<tr><td><code id="findintercorr_nb_+3A_nrand">nrand</code></td>
<td>
<p>the number of random numbers to generate in calculating the bound (default = 10000)</p>
</td></tr>
<tr><td><code id="findintercorr_nb_+3A_seed">seed</code></td>
<td>
<p>the seed used in random number generation (default = 1234)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the <code>k_nb x k_nb</code> intermediate correlation matrix for the Negative Binomial variables
</p>


<h3>References</h3>

<p>Please see references for <code><a href="#topic+findintercorr_pois">findintercorr_pois</a></code>.
</p>


<h3>See Also</h3>

<p><code><a href="PoisNor.html#topic+PoisNor-package">PoisNor-package</a></code>, <code><a href="#topic+findintercorr_pois">findintercorr_pois</a></code>,
<code><a href="#topic+findintercorr_pois_nb">findintercorr_pois_nb</a></code>,
<code><a href="#topic+findintercorr">findintercorr</a></code>, <code><a href="#topic+rcorrvar">rcorrvar</a></code>
</p>

<hr>
<h2 id='findintercorr_pois'>Calculate Intermediate MVN Correlation for Poisson Variables: Correlation Method 1</h2><span id='topic+findintercorr_pois'></span>

<h3>Description</h3>

<p>This function calculates a <code>k_pois x k_pois</code> intermediate matrix of correlations for the
Poisson variables using the method of Yahav &amp; Shmueli (2012, doi: <a href="http://doi.org/10.1002/asmb.901">10.1002/asmb.901</a>). The intermediate correlation between Z1 and Z2 (the
standard normal variables used to generate the Poisson variables Y1 and Y2 via the inverse cdf method) is
calculated using a logarithmic transformation of the target correlation.  First, the upper and lower Frechet-Hoeffding bounds
(mincor, maxcor) <code class="reqn">\rho_{y1,y2}</code> are simulated.  Then the intermediate correlation is found as follows:
</p>
<p style="text-align: center;"><code class="reqn">\rho_{z1,z2} = (1/b) * log((\rho_{y1,y2} - c)/a)</code>
</p>
<p>, where <code class="reqn">a = -(maxcor * mincor)/(maxcor + mincor)</code>,
<code class="reqn">b = log((maxcor + a)/a)</code>, and <code class="reqn">c = -a</code>.  The function adapts code from Amatya &amp; Demirtas' (2016) package
<code><a href="PoisNor.html#topic+PoisNor-package">PoisNor-package</a></code> by:
</p>
<p>1) allowing specifications for the number of random variates and the seed for reproducibility
</p>
<p>2) providing the following checks: if <code class="reqn">\rho_{z1,z2}</code> &gt;= 1, <code class="reqn">\rho_{z1,z2}</code> is set to 0.99; if <code class="reqn">\rho_{z1,z2}</code> &lt;= -1,
<code class="reqn">\rho_{z1,z2}</code> is set to -0.99.
</p>
<p>The function is used in <code><a href="#topic+findintercorr">findintercorr</a></code> and <code><a href="#topic+rcorrvar">rcorrvar</a></code>.
This function would not ordinarily be called by the user.
</p>
<p>Note: The method used here is also used in the packages <code><a href="PoisBinOrdNor.html#topic+PoisBinOrdNor-package">PoisBinOrdNor-package</a></code> and
<code><a href="PoisBinOrdNonNor.html#topic+PoisBinOrdNonNor-package">PoisBinOrdNonNor-package</a></code> by Demirtas et al. (2017), but without my modifications.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>findintercorr_pois(rho_pois, lam, nrand = 100000, seed = 1234)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="findintercorr_pois_+3A_rho_pois">rho_pois</code></td>
<td>
<p>a <code>k_pois x k_pois</code> matrix of target correlations</p>
</td></tr>
<tr><td><code id="findintercorr_pois_+3A_lam">lam</code></td>
<td>
<p>a vector of lambda (&gt; 0) constants for the Poisson variables (see <code><a href="stats.html#topic+Poisson">Poisson</a></code>)</p>
</td></tr>
<tr><td><code id="findintercorr_pois_+3A_nrand">nrand</code></td>
<td>
<p>the number of random numbers to generate in calculating the bound (default = 10000)</p>
</td></tr>
<tr><td><code id="findintercorr_pois_+3A_seed">seed</code></td>
<td>
<p>the seed used in random number generation (default = 1234)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the <code>k_pois x k_pois</code> intermediate correlation matrix for the Poisson variables
</p>


<h3>References</h3>

<p>Amatya A &amp; Demirtas H (2015). Simultaneous generation of multivariate mixed data with Poisson and normal marginals.
Journal of Statistical Computation and Simulation, 85(15): 3129-39. doi: <a href="http://doi.org/10.1080/00949655.2014.953534">10.1080/00949655.2014.953534</a>.
</p>
<p>Amatya A &amp; Demirtas H (2016). PoisNor: Simultaneous Generation of Multivariate Data with Poisson and Normal Marginals.
R package version 1.1. <a href="https://CRAN.R-project.org/package=PoisNor">https://CRAN.R-project.org/package=PoisNor</a>
</p>
<p>Demirtas H &amp; Hedeker D (2011). A practical way for computing approximate lower and upper correlation bounds.
American Statistician, 65(2): 104-109.
</p>
<p>Demirtas H, Hu Y, &amp; Allozi R (2017). PoisBinOrdNor: Data Generation with Poisson, Binary, Ordinal and Normal
Components. R package version 1.4. <a href="https://CRAN.R-project.org/package=PoisBinOrdNor">https://CRAN.R-project.org/package=PoisBinOrdNor</a>
</p>
<p>Demirtas H, Nordgren R, &amp; Allozi R (2017). PoisBinOrdNonNor: Generation of Up to Four Different
Types of Variables. R package version 1.3. <a href="https://CRAN.R-project.org/package=PoisBinOrdNonNor">https://CRAN.R-project.org/package=PoisBinOrdNonNor</a>
</p>
<p>Frechet M.  Sur les tableaux de correlation dont les marges sont donnees.  Ann. l'Univ. Lyon SectA.  1951;14:53-77.
</p>
<p>Hoeffding W. Scale-invariant correlation theory. In: Fisher NI, Sen PK, editors. The collected works of Wassily Hoeffding.
New York: Springer-Verlag; 1994. p. 57-107.
</p>
<p>Yahav I &amp; Shmueli G (2012). On Generating Multivariate Poisson Data in Management Science Applications. Applied Stochastic
Models in Business and Industry, 28(1): 91-102. doi: <a href="http://doi.org/10.1002/asmb.901">10.1002/asmb.901</a>.
</p>


<h3>See Also</h3>

<p><code><a href="PoisNor.html#topic+PoisNor-package">PoisNor-package</a></code>, <code><a href="#topic+findintercorr_nb">findintercorr_nb</a></code>,
<code><a href="#topic+findintercorr_pois_nb">findintercorr_pois_nb</a></code>,
<code><a href="#topic+findintercorr">findintercorr</a></code>, <code><a href="#topic+rcorrvar">rcorrvar</a></code>
</p>

<hr>
<h2 id='findintercorr_pois_nb'>Calculate Intermediate MVN Correlation for Poisson - Negative Binomial Variables: Correlation Method 1</h2><span id='topic+findintercorr_pois_nb'></span>

<h3>Description</h3>

<p>This function calculates a <code>k_pois x k_nb</code> intermediate matrix of correlations for the
Poisson and Negative Binomial variables by extending the method of Yahav &amp; Shmueli (2012, doi: <a href="http://doi.org/10.1002/asmb.901">10.1002/asmb.901</a>). The intermediate correlation
between Z1 and Z2 (the standard normal variables used to generate the Poisson and Negative Binomial variables Y1 and Y2
via the inverse cdf method) is calculated using a logarithmic transformation of the target correlation.
First, the upper and lower Frechet-Hoeffding bounds (mincor, maxcor) on <code class="reqn">\rho_{y1,y2}</code> are simulated.
Then the intermediate correlation is found as follows:
</p>
<p style="text-align: center;"><code class="reqn">\rho_{z1,z2} = (1/b) * log((\rho_{y1,y2} - c)/a)</code>
</p>
<p>, where <code class="reqn">a = -(maxcor * mincor)/(maxcor + mincor)</code>,
<code class="reqn">b = log((maxcor + a)/a)</code>, and <code class="reqn">c = -a</code>.  The function adapts code from Amatya &amp; Demirtas' (2016) package
<code><a href="PoisNor.html#topic+PoisNor-package">PoisNor-package</a></code> by:
</p>
<p>1) allowing specifications for the number of random variates and the seed for reproducibility
</p>
<p>2) providing the following checks: if <code class="reqn">\rho_{z1,z2}</code> &gt;= 1, <code class="reqn">\rho_{z1,z2}</code> is set to 0.99; if <code class="reqn">\rho_{z1,z2}</code> &lt;= -1,
<code class="reqn">\rho_{z1,z2}</code> is set to -0.99
</p>
<p>3) simulating Negative Binomial variables.
The function is used in <code><a href="#topic+findintercorr">findintercorr</a></code> and <code><a href="#topic+rcorrvar">rcorrvar</a></code>.
This function would not ordinarily be called by the user.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>findintercorr_pois_nb(rho_pois_nb, lam, size, prob, mu = NULL,
  nrand = 100000, seed = 1234)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="findintercorr_pois_nb_+3A_rho_pois_nb">rho_pois_nb</code></td>
<td>
<p>a <code>k_pois x k_nb</code> matrix of target correlations</p>
</td></tr>
<tr><td><code id="findintercorr_pois_nb_+3A_lam">lam</code></td>
<td>
<p>a vector of lambda (&gt; 0) constants for the Poisson variables (see <code><a href="stats.html#topic+Poisson">Poisson</a></code>)</p>
</td></tr>
<tr><td><code id="findintercorr_pois_nb_+3A_size">size</code></td>
<td>
<p>a vector of size parameters for the Negative Binomial variables (see <code><a href="stats.html#topic+NegBinomial">NegBinomial</a></code>)</p>
</td></tr>
<tr><td><code id="findintercorr_pois_nb_+3A_prob">prob</code></td>
<td>
<p>a vector of success probability parameters</p>
</td></tr>
<tr><td><code id="findintercorr_pois_nb_+3A_mu">mu</code></td>
<td>
<p>a vector of mean parameters (*Note: either <code>prob</code> or <code>mu</code> should be supplied for all Negative Binomial variables,
not a mixture; default = NULL)</p>
</td></tr>
<tr><td><code id="findintercorr_pois_nb_+3A_nrand">nrand</code></td>
<td>
<p>the number of random numbers to generate in calculating the bound (default = 10000)</p>
</td></tr>
<tr><td><code id="findintercorr_pois_nb_+3A_seed">seed</code></td>
<td>
<p>the seed used in random number generation (default = 1234)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the <code>k_pois x k_nb</code> intermediate correlation matrix whose rows represent the <code>k_pois</code> Poisson variables and
columns represent the <code>k_nb</code> Negative Binomial variables
</p>


<h3>References</h3>

<p>Please see references for <code><a href="#topic+findintercorr_pois">findintercorr_pois</a></code>.
</p>


<h3>See Also</h3>

<p><code><a href="PoisNor.html#topic+PoisNor-package">PoisNor-package</a></code>, <code><a href="#topic+findintercorr_pois">findintercorr_pois</a></code>,
<code><a href="#topic+findintercorr_nb">findintercorr_nb</a></code>,
<code><a href="#topic+findintercorr">findintercorr</a></code>, <code><a href="#topic+rcorrvar">rcorrvar</a></code>
</p>

<hr>
<h2 id='findintercorr2'>Calculate Intermediate MVN Correlation for Ordinal, Continuous, Poisson, or Negative Binomial Variables: Correlation Method 2</h2><span id='topic+findintercorr2'></span>

<h3>Description</h3>

<p>This function calculates a <code>k x k</code> intermediate matrix of correlations, where <code>k = k_cat + k_cont + k_pois + k_nb</code>,
to be used in simulating variables with <code><a href="#topic+rcorrvar2">rcorrvar2</a></code>.  The ordering of the variables must be
ordinal, continuous, Poisson, and Negative Binomial (note that it is possible for <code>k_cat</code>, <code>k_cont</code>, <code>k_pois</code>,
and/or <code>k_nb</code> to be 0).
The function first checks that the target correlation matrix <code>rho</code> is positive-definite and the marginal distributions for the
ordinal variables are cumulative probabilities with r - 1 values (for r categories).  There is a warning given at the end of simulation
if the calculated intermediate correlation matrix <code>Sigma</code> is not positive-definite.  This function is called by the simulation function
<code><a href="#topic+rcorrvar2">rcorrvar2</a></code>, and would only be used separately if the user wants to find the intermediate correlation matrix
only.  The simulation functions also return the intermediate correlation matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>findintercorr2(n, k_cont = 0, k_cat = 0, k_pois = 0, k_nb = 0,
  method = c("Fleishman", "Polynomial"), constants, marginal = list(),
  support = list(), lam = NULL, size = NULL, prob = NULL, mu = NULL,
  pois_eps = NULL, nb_eps = NULL, rho = NULL, epsilon = 0.001,
  maxit = 1000)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="findintercorr2_+3A_n">n</code></td>
<td>
<p>the sample size (i.e. the length of each simulated variable)</p>
</td></tr>
<tr><td><code id="findintercorr2_+3A_k_cont">k_cont</code></td>
<td>
<p>the number of continuous variables (default = 0)</p>
</td></tr>
<tr><td><code id="findintercorr2_+3A_k_cat">k_cat</code></td>
<td>
<p>the number of ordinal (r &gt;= 2 categories) variables (default = 0)</p>
</td></tr>
<tr><td><code id="findintercorr2_+3A_k_pois">k_pois</code></td>
<td>
<p>the number of Poisson variables (default = 0)</p>
</td></tr>
<tr><td><code id="findintercorr2_+3A_k_nb">k_nb</code></td>
<td>
<p>the number of Negative Binomial variables (default = 0)</p>
</td></tr>
<tr><td><code id="findintercorr2_+3A_method">method</code></td>
<td>
<p>the method used to generate the <code>k_cont</code> continuous variables.  &quot;Fleishman&quot; uses a third-order polynomial transformation
and &quot;Polynomial&quot; uses Headrick's fifth-order transformation.</p>
</td></tr>
<tr><td><code id="findintercorr2_+3A_constants">constants</code></td>
<td>
<p>a matrix with <code>k_cont</code> rows, each a vector of constants c0, c1, c2, c3 (if <code>method</code> = &quot;Fleishman&quot;) or
c0, c1, c2, c3, c4, c5 (if <code>method</code> = &quot;Polynomial&quot;) like that returned by
<code><a href="#topic+find_constants">find_constants</a></code></p>
</td></tr>
<tr><td><code id="findintercorr2_+3A_marginal">marginal</code></td>
<td>
<p>a list of length equal to <code>k_cat</code>; the i-th element is a vector of the cumulative
probabilities defining the marginal distribution of the i-th variable;
if the variable can take r values, the vector will contain r - 1 probabilities (the r-th is assumed to be 1; default = list())</p>
</td></tr>
<tr><td><code id="findintercorr2_+3A_support">support</code></td>
<td>
<p>a list of length equal to <code>k_cat</code>; the i-th element is a vector of containing the r
ordered support values; if not provided (i.e. <code>support</code> = list()), the default is for the i-th element to be the vector 1, ..., r</p>
</td></tr>
<tr><td><code id="findintercorr2_+3A_lam">lam</code></td>
<td>
<p>a vector of lambda (&gt; 0) constants for the Poisson variables (see <code><a href="stats.html#topic+Poisson">Poisson</a></code>)</p>
</td></tr>
<tr><td><code id="findintercorr2_+3A_size">size</code></td>
<td>
<p>a vector of size parameters for the Negative Binomial variables (see <code><a href="stats.html#topic+NegBinomial">NegBinomial</a></code>)</p>
</td></tr>
<tr><td><code id="findintercorr2_+3A_prob">prob</code></td>
<td>
<p>a vector of success probability parameters</p>
</td></tr>
<tr><td><code id="findintercorr2_+3A_mu">mu</code></td>
<td>
<p>a vector of mean parameters (*Note: either <code>prob</code> or <code>mu</code> should be supplied for all Negative Binomial variables,
not a mixture; default = NULL)</p>
</td></tr>
<tr><td><code id="findintercorr2_+3A_pois_eps">pois_eps</code></td>
<td>
<p>a vector of length <code>k_pois</code> containing the truncation values (i.e. = rep(0.0001, k_pois); default = NULL)</p>
</td></tr>
<tr><td><code id="findintercorr2_+3A_nb_eps">nb_eps</code></td>
<td>
<p>a vector of length <code>k_nb</code> containing the truncation values (i.e. = rep(0.0001, k_nb); default = NULL)</p>
</td></tr>
<tr><td><code id="findintercorr2_+3A_rho">rho</code></td>
<td>
<p>the target correlation matrix (<em>must be ordered ordinal, continuous, Poisson, Negative Binomial</em>; default = NULL)</p>
</td></tr>
<tr><td><code id="findintercorr2_+3A_epsilon">epsilon</code></td>
<td>
<p>the maximum acceptable error between the final and target correlation matrices (default = 0.001)
in the calculation of ordinal intermediate correlations with <code><a href="#topic+ordnorm">ordnorm</a></code></p>
</td></tr>
<tr><td><code id="findintercorr2_+3A_maxit">maxit</code></td>
<td>
<p>the maximum number of iterations to use (default = 1000) in the calculation of ordinal
intermediate correlations with <code><a href="#topic+ordnorm">ordnorm</a></code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>the intermediate MVN correlation matrix
</p>


<h3>Overview of Correlation Method 2</h3>

<p>The intermediate correlations used in correlation method 2 are less simulation based than those in correlation method 1, and no seed is needed.
Their calculations involve greater utilization of correction loops which make iterative adjustments until a maximum error
has been reached (if possible).  In addition, method 2 differs from method 1 in the following ways:
</p>
<p>1) The intermediate correlations involving <b>count variables</b> are based on the methods of Barbiero &amp; Ferrari (2012,
doi: <a href="http://doi.org/10.1080/00273171.2012.692630">10.1080/00273171.2012.692630</a>, 2015, doi: <a href="http://doi.org/10.1002/asmb.2072">10.1002/asmb.2072</a>).
The Poisson or Negative Binomial support is made finite by removing a small user-specified value (i.e. 1e-06) from the total
cumulative probability.  This truncation factor may differ for each count variable.  The count variables are subsequently
treated as ordinal and intermediate correlations are calculated using the correction loop of
<code><a href="#topic+ordnorm">ordnorm</a></code>.
</p>
<p>2) The <b>continuous - count variable</b> correlations are based on an extension of the method of Demirtas et al. (2012,
doi: <a href="http://doi.org/10.1002/sim.5362">10.1002/sim.5362</a>), and the count
variables are treated as ordinal.  The correction factor is the product of the power method correlation between the
continuous variable and the normal variable used to generate it (see Headrick &amp; Kowalchuk, 2007, doi: <a href="http://doi.org/10.1080/10629360600605065">10.1080/10629360600605065</a>)
and the point-polyserial correlation between the ordinalized count variable and the normal variable used to generate it (see Olsson et al., 1982,
doi: <a href="http://doi.org/10.1007/BF02294164">10.1007/BF02294164</a>).
The intermediate correlations are the ratio of the target correlations to the correction factor.
</p>
<p>The processes used to find the intermediate correlations for each variable type are described below.  Please see the
corresponding function help page for more information:
</p>


<h3>Ordinal Variables</h3>

<p>Correlations are computed pairwise.  If both variables are binary, the method of Demirtas et al. (2012, doi: <a href="http://doi.org/10.1002/sim.5362">10.1002/sim.5362</a>) is used to find the
tetrachoric correlation (code adapted from <code><a href="BinNonNor.html#topic+Tetra.Corr.BB">Tetra.Corr.BB</a></code>).  This method is based on Emrich and Piedmonte's
(1991, doi: <a href="http://doi.org/10.1080/00031305.1991.10475828">10.1080/00031305.1991.10475828</a>) work, in which the joint binary distribution is determined from the third and higher moments of a multivariate normal
distribution: Let <code class="reqn">Y_{1}</code> and <code class="reqn">Y_{2}</code> be binary variables with <code class="reqn">E[Y_{1}] = Pr(Y_{1} = 1) = p_{1}</code>,
<code class="reqn">E[Y_{2}] = Pr(Y_{2} = 1) = p_{2}</code>, and correlation <code class="reqn">\rho_{y1y2}</code>.  Let <code class="reqn">\Phi[x_{1}, x_{2}, \rho_{x1x2}]</code> be the
standard bivariate normal cumulative distribution function, given by:
</p>
<p style="text-align: center;"><code class="reqn">\Phi[x_{1}, x_{2}, \rho_{x1x2}] = \int_{-\infty}^{x_{1}} \int_{-\infty}^{x_{2}} f(z_{1}, z_{2}, \rho_{x1x2}) dz_{1} dz_{2}</code>
</p>

<p>where
</p>
<p style="text-align: center;"><code class="reqn">f(z_{1}, z_{2}, \rho_{x1x2}) = [2\pi\sqrt{1 - \rho_{x1x2}^2}]^{-1} * exp[-0.5(z_{1}^2 - 2\rho_{x1x2}z_{1}z_{2} + z_{2}^2)/(1 - \rho_{x1x2}^2)]</code>
</p>

<p>Then solving the equation
</p>
<p style="text-align: center;"><code class="reqn">\Phi[z(p_{1}), z(p_{2}), \rho_{x1x2}] = \rho_{y1y2}\sqrt{p_{1}(1 - p_{1})p_{2}(1 - p_{2})} + p_{1}p_{2}</code>
</p>

<p>for <code class="reqn">\rho_{x1x2}</code> gives the intermediate correlation of the standard normal variables needed to generate binary variables with
correlation <code class="reqn">\rho_{y1y2}</code>.  Here <code class="reqn">z(p)</code> indicates the <code class="reqn">pth</code> quantile of the standard normal distribution.
</p>
<p>Otherwise, <code><a href="#topic+ordnorm">ordnorm</a></code> is called for each pair.  If the resulting
intermediate matrix is not positive-definite, there is a warning given because it may not be possible to find a MVN correlation
matrix that will produce the desired marginal distributions after discretization.  Any problems with positive-definiteness are
corrected later.
</p>


<h3>Continuous Variables</h3>

<p>Correlations are computed pairwise.  <code><a href="#topic+findintercorr_cont">findintercorr_cont</a></code> is called for each pair.
</p>


<h3>Poisson Variables</h3>

<p><code><a href="#topic+max_count_support">max_count_support</a></code> is used to find the maximum support value given the vector pois_eps
of truncation values.  This is used to create a Poisson marginal list consisting of cumulative probabilities for each
variable (like that for the ordinal variables).  Then <code><a href="#topic+ordnorm">ordnorm</a></code> is called to calculate the
intermediate MVN correlation for all Poisson variables.
</p>


<h3>Negative Binomial Variables</h3>

<p><code><a href="#topic+max_count_support">max_count_support</a></code> is used to find the maximum support value given the vector nb_eps
of truncation values.  This is used to create a Negative Binomial marginal list consisting of cumulative probabilities for each
variable (like that for the ordinal variables).  Then <code><a href="#topic+ordnorm">ordnorm</a></code> is called to calculate the
intermediate MVN correlation for all  Negative Binomial variables.
</p>


<h3>Continuous - Ordinal Pairs</h3>

<p><code><a href="#topic+findintercorr_cont_cat">findintercorr_cont_cat</a></code> is called to calculate the intermediate MVN correlation for all
Continuous and Ordinal combinations.
</p>


<h3>Ordinal - Poisson Pairs</h3>

<p>The Poisson marginal list is appended to the ordinal marginal list (similarly for the support lists).  Then
<code><a href="#topic+ordnorm">ordnorm</a></code> is called to calculate the intermediate MVN correlation for all
Ordinal and Poisson combinations.
</p>


<h3>Ordinal - Negative Binomial Pairs</h3>

<p>The Negative Binomial marginal list is appended to the ordinal marginal list (similarly for the support lists).  Then
<code><a href="#topic+ordnorm">ordnorm</a></code> is called to calculate the intermediate MVN correlation for all
Ordinal and Negative Binomial combinations.
</p>


<h3>Continuous - Poisson Pairs</h3>

<p><code><a href="#topic+findintercorr_cont_pois2">findintercorr_cont_pois2</a></code> is called to calculate the intermediate MVN correlation for all
Continuous and Poisson combinations.
</p>


<h3>Continuous - Negative Binomial Pairs</h3>

<p><code><a href="#topic+findintercorr_cont_nb2">findintercorr_cont_nb2</a></code> is called to calculate the intermediate MVN correlation for all
Continuous and Negative Binomial combinations.
</p>


<h3>Poisson - Negative Binomial Pairs</h3>

<p>The Negative Binomial marginal list is appended to the Poisson marginal list (similarly for the support lists).  Then
<code><a href="#topic+ordnorm">ordnorm</a></code> is called to calculate the intermediate MVN correlation for all
Poisson and Negative Binomial combinations.
</p>


<h3>References</h3>

<p>Please see <code><a href="#topic+rcorrvar2">rcorrvar2</a></code> for additional references.
</p>
<p>Emrich LJ &amp; Piedmonte MR (1991). A Method for Generating High-Dimensional Multivariate Binary Variables. The American Statistician, 45(4): 302-4.
doi: <a href="http://doi.org/10.1080/00031305.1991.10475828">10.1080/00031305.1991.10475828</a>.
</p>
<p>Inan G &amp; Demirtas H (2016). BinNonNor: Data Generation with Binary and Continuous Non-Normal Components.
R package version 1.3. <a href="https://CRAN.R-project.org/package=BinNonNor">https://CRAN.R-project.org/package=BinNonNor</a>
</p>
<p>Vale CD &amp; Maurelli VA (1983). Simulating Multivariate Nonnormal Distributions. Psychometrika, 48, 465-471. doi: <a href="http://doi.org/10.1007/BF02293687">10.1007/BF02293687</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+find_constants">find_constants</a></code>, <code><a href="#topic+rcorrvar2">rcorrvar2</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

# Binary, Ordinal, Continuous, Poisson, and Negative Binomial Variables

options(scipen = 999)
seed &lt;- 1234
n &lt;- 10000

# Continuous Distributions: Normal, t (df = 10), Chisq (df = 4),
# Beta (a = 4, b = 2), Gamma (a = 4, b = 4)
Dist &lt;- c("Gaussian", "t", "Chisq", "Beta", "Gamma")

# calculate standardized cumulants
# those for the normal and t distributions are rounded to ensure the
# correct values (i.e. skew = 0)

M1 &lt;- round(calc_theory(Dist = "Gaussian", params = c(0, 1)), 8)
M2 &lt;- round(calc_theory(Dist = "t", params = 10), 8)
M3 &lt;- calc_theory(Dist = "Chisq", params = 4)
M4 &lt;- calc_theory(Dist = "Beta", params = c(4, 2))
M5 &lt;- calc_theory(Dist = "Gamma", params = c(4, 4))
M &lt;- cbind(M1, M2, M3, M4, M5)
M &lt;- round(M[-c(1:2),], digits = 6)
colnames(M) &lt;- Dist
rownames(M) &lt;- c("skew", "skurtosis", "fifth", "sixth")
means &lt;- rep(0, length(Dist))
vars &lt;- rep(1, length(Dist))

# calculate constants
con &lt;- matrix(1, nrow = ncol(M), ncol = 6)
for (i in 1:ncol(M)) {
 con[i, ] &lt;- find_constants(method = "Polynomial", skews = M[1, i],
                            skurts = M[2, i], fifths = M[3, i],
                            sixths = M[4, i])
}

# Binary and Ordinal Distributions
marginal &lt;- list(0.3, 0.4, c(0.1, 0.5), c(0.3, 0.6, 0.9),
                 c(0.2, 0.4, 0.7, 0.8))
support &lt;- list()

# Poisson Distributions
lam &lt;- c(1, 5, 10)

# Negative Binomial Distributions
size &lt;- c(3, 6)
prob &lt;- c(0.2, 0.8)

ncat &lt;- length(marginal)
ncont &lt;- ncol(M)
npois &lt;- length(lam)
nnb &lt;- length(size)

# Create correlation matrix from a uniform distribution (-0.8, 0.8)
set.seed(seed)
Rey &lt;- diag(1, nrow = (ncat + ncont + npois + nnb))
for (i in 1:nrow(Rey)) {
  for (j in 1:ncol(Rey)) {
    if (i &gt; j) Rey[i, j] &lt;- runif(1, -0.8, 0.8)
    Rey[j, i] &lt;- Rey[i, j]
  }
}

# Test for positive-definiteness
library(Matrix)
if(min(eigen(Rey, symmetric = TRUE)$values) &lt; 0) {
  Rey &lt;- as.matrix(nearPD(Rey, corr = T, keepDiag = T)$mat)
}

# Make sure Rey is within upper and lower correlation limits
valid &lt;- valid_corr2(k_cat = ncat, k_cont = ncont, k_pois = npois,
                     k_nb = nnb, method = "Polynomial", means = means,
                     vars = vars, skews = M[1, ], skurts = M[2, ],
                     fifths = M[3, ], sixths = M[4, ],
                     marginal = marginal, lam = lam,
                     pois_eps = rep(0.0001, npois),
                     size = size, prob = prob,
                     nb_eps = rep(0.0001, nnb),
                     rho = Rey, seed = seed)

# Find intermediate correlation
Sigma2 &lt;- findintercorr2(n = n, k_cont = ncont, k_cat = ncat,
                         k_pois = npois, k_nb = nnb,
                         method = "Polynomial", constants = con,
                         marginal = marginal, lam = lam, size = size,
                         prob = prob, pois_eps = rep(0.0001, npois),
                         nb_eps = rep(0.0001, nnb), rho = Rey)
Sigma2


## End(Not run)
</code></pre>

<hr>
<h2 id='fleish'>Fleishman's Third-Order Polynomial Transformation Equations</h2><span id='topic+fleish'></span>

<h3>Description</h3>

<p>This function contains Fleishman's third-order polynomial transformation equations (doi: <a href="http://doi.org/10.1007/BF02293811">10.1007/BF02293811</a>).  It is used in
<code><a href="#topic+find_constants">find_constants</a></code> to find the constants c1, c2, and c3 (c0 = -c2) that satisfy the
equations given skewness and standardized kurtosis values.  It can be used to verify a set of constants satisfy
the equations.  Note that there exist solutions that yield invalid power method pdfs (see
<code><a href="#topic+power_norm_corr">power_norm_corr</a></code>, <code><a href="#topic+pdf_check">pdf_check</a></code>).
This function would not ordinarily be called by the user.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fleish(c, a)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fleish_+3A_c">c</code></td>
<td>
<p>a vector of constants c1, c2, c3; note that <code><a href="#topic+find_constants">find_constants</a></code> returns c0, c1, c2, c3</p>
</td></tr>
<tr><td><code id="fleish_+3A_a">a</code></td>
<td>
<p>a vector c(skewness, standardized kurtosis)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list of length 3; if the constants satisfy the equations, returns 0 for all list elements
</p>


<h3>References</h3>

<p>Fleishman AI (1978). A Method for Simulating Non-normal Distributions. Psychometrika, 43, 521-532. doi: <a href="http://doi.org/10.1007/BF02293811">10.1007/BF02293811</a>.
</p>
<p>Headrick TC, Sawilowsky SS (1999). Simulating Correlated Non-normal Distributions: Extending the Fleishman Power
Method. Psychometrika, 64, 25-35. doi: <a href="http://doi.org/10.1007/BF02294317">10.1007/BF02294317</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+poly">poly</a></code>, <code><a href="#topic+power_norm_corr">power_norm_corr</a></code>,
<code><a href="#topic+pdf_check">pdf_check</a></code>, <code><a href="#topic+find_constants">find_constants</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Laplace Distribution
fleish(c = c(0.782356, 0, 0.067905), a = c(0, 3))
</code></pre>

<hr>
<h2 id='fleish_Hessian'>Fleishman's Third-Order Transformation Hessian Calculation for Lower Boundary of Standardized Kurtosis in Asymmetric Distributions</h2><span id='topic+fleish_Hessian'></span>

<h3>Description</h3>

<p>This function gives the second-order conditions necessary to verify that a kurtosis is a global minimum.  A kurtosis solution from
<code><a href="#topic+fleish_skurt_check">fleish_skurt_check</a></code> is a global minimum if and only if the determinant of the bordered Hessian, <code class="reqn">H</code>, is
less than zero (see Headrick &amp; Sawilowsky, 2002, doi: <a href="http://doi.org/10.3102/10769986025004417">10.3102/10769986025004417</a>), where
</p>
<p style="text-align: center;"><code class="reqn">|\bar{H}| = matrix(c(0, dg(c1, c3)/dc1, dg(c1, c3)/dc3,</code>
</p>

<p style="text-align: center;"><code class="reqn">dg(c1, c3)/dc1, d^2 F(c1, c3, \lambda)/dc1^2, d^2 F(c1, c3, \lambda)/(dc3 dc1),</code>
</p>

<p style="text-align: center;"><code class="reqn">dg(c1, c3)/dc3, d^2 F(c1, c3, \lambda)/(dc1 dc3), d^2 F(c1, c3, \lambda)/dc3^2), 3, 3, byrow = TRUE)</code>
</p>

<p>Here, <code class="reqn">F(c1, c3, \lambda) = f(c1, c3) + \lambda * [\gamma_{1} - g(c1, c3)]</code> is the Fleishman Transformation Lagrangean expression
(see <code><a href="#topic+fleish_skurt_check">fleish_skurt_check</a></code>).  Headrick &amp; Sawilowsky (2002) gave equations for the second-order derivatives
<code class="reqn">d^2 F/dc1^2</code>, <code class="reqn">d^2 F/dc3^2</code>, and <code class="reqn">d^2 F/(dc1 dc3)</code>.  These were verified and <code class="reqn">dg/dc1</code> and <code class="reqn">dg/dc3</code> were calculated
using <code>D</code> (see <code><a href="stats.html#topic+deriv">deriv</a></code>).  This function would not ordinarily be called by the user.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fleish_Hessian(c)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fleish_Hessian_+3A_c">c</code></td>
<td>
<p>a vector of constants c1, c3, lambda</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with components:
</p>
<p><code>Hessian</code> the Hessian matrix H
</p>
<p><code>H_det</code> the determinant of H
</p>


<h3>References</h3>

<p>Please see references for <code><a href="#topic+fleish_skurt_check">fleish_skurt_check</a></code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fleish_skurt_check">fleish_skurt_check</a></code>, <code><a href="#topic+calc_lower_skurt">calc_lower_skurt</a></code>
</p>

<hr>
<h2 id='fleish_skurt_check'>Fleishman's Third-Order Transformation Lagrangean Constraints for Lower Boundary of Standardized Kurtosis in Asymmetric Distributions</h2><span id='topic+fleish_skurt_check'></span>

<h3>Description</h3>

<p>This function gives the first-order conditions of the Fleishman Transformation Lagrangean expression
<code class="reqn">F(c1, c3, \lambda) = f(c1, c3) + \lambda * [\gamma_{1} - g(c1, c3)]</code> used to find the lower kurtosis boundary for a given non-zero skewness
in <code><a href="#topic+calc_lower_skurt">calc_lower_skurt</a></code> (see Headrick &amp; Sawilowsky, 2002, doi: <a href="http://doi.org/10.3102/10769986025004417">10.3102/10769986025004417</a>).  Here, <code class="reqn">f(c1, c3)</code> is the equation for
standardized kurtosis expressed in terms of c1 and c3 only,
<code class="reqn">\lambda</code> is the Lagrangean multiplier, <code class="reqn">\gamma_{1}</code> is skewness, and <code class="reqn">g(c1, c3)</code> is the equation for skewness expressed
in terms of c1 and c3 only.  It should be noted that these equations are for <code class="reqn">\gamma_{1} &gt; 0</code>.  Negative skew values are handled within
<code><a href="#topic+calc_lower_skurt">calc_lower_skurt</a></code>.  Headrick &amp; Sawilowsky (2002) gave equations for the first-order derivatives <code class="reqn">dF/dc1</code>
and <code class="reqn">dF/dc3</code>.  These were verified and <code class="reqn">dF/d\lambda</code> was calculated using <code>D</code> (see <code><a href="stats.html#topic+deriv">deriv</a></code>).  The second-order conditions to
verify that the kurtosis is a global minimum are in <code><a href="#topic+fleish_Hessian">fleish_Hessian</a></code>.
This function would not ordinarily be called by the user.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fleish_skurt_check(c, a)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fleish_skurt_check_+3A_c">c</code></td>
<td>
<p>a vector of constants c1, c3, lambda</p>
</td></tr>
<tr><td><code id="fleish_skurt_check_+3A_a">a</code></td>
<td>
<p>skew value</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with components:
</p>
<p><code class="reqn">dF(c1, c3, \lambda)/d\lambda = \gamma_{1} - g(c1, c3)</code>
</p>
<p><code class="reqn">dF(c1, c3, \lambda)/dc1 = df(c1, c3)/dc1  - \lambda * dg(c1, c3)/dc1</code>
</p>
<p><code class="reqn">dF(c1, c3, \lambda)/dc3 = df(c1, c3)/dc3  - \lambda * dg(c1, c3)/dc3</code>
</p>
<p>If the suppled values for c and skew satisfy the Lagrangean expression, it will return 0 for each component.
</p>


<h3>References</h3>

<p>Fleishman AI (1978). A Method for Simulating Non-normal Distributions. Psychometrika, 43, 521-532. doi: <a href="http://doi.org/10.1007/BF02293811">10.1007/BF02293811</a>.
</p>
<p>Headrick TC, Sawilowsky SS (2002). Weighted Simplex Procedures for Determining Boundary Points and Constants for the
Univariate and Multivariate Power Methods. Journal of Educational and Behavioral Statistics, 25, 417-436. doi: <a href="http://doi.org/10.3102/10769986025004417">10.3102/10769986025004417</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fleish_Hessian">fleish_Hessian</a></code>, <code><a href="#topic+calc_lower_skurt">calc_lower_skurt</a></code>
</p>

<hr>
<h2 id='H_params'>Parameters for Examples of Constants Calculated by Headrick's Fifth-Order Polynomial Transformation</h2><span id='topic+H_params'></span>

<h3>Description</h3>

<p>These are the parameters for <code><a href="#topic+Headrick.dist">Headrick.dist</a></code>, which contains selected symmetrical and
asymmetrical theoretical densities with their associated values
of skewness (gamma1), standardized kurtosis (gamma2), and standardized fifth (gamma3) and
sixth (gamma4) cumulants.  Constants were calculated by Headrick using his fifth-order
polynomial transformation and given in his Table 1 (2002, p. 691-2, doi: <a href="http://doi.org/10.1016/S0167-9473(02)00072-5">10.1016/S0167-9473(02)00072-5</a>).
Note that the standardized cumulants for the Gamma(10, 10)
distribution do not arise from using <code class="reqn">\alpha = 10,\ \beta = 10</code>.  Therefore, either there is a typo in the table or
Headrick used a different parameterization.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(H_params)
</code></pre>


<h3>Format</h3>

<p>An object of class <code>"data.frame"</code>; Colnames are distribution names as inputs for
<code><a href="#topic+calc_theory">calc_theory</a></code>; rownames are param1, param2.</p>


<h3>References</h3>

<p>Headrick TC (2002). Fast Fifth-order Polynomial Transforms for Generating Univariate and Multivariate
Non-normal Distributions. Computational Statistics &amp; Data Analysis, 40(4):685-711. doi: <a href="http://doi.org/10.1016/S0167-9473(02)00072-5">10.1016/S0167-9473(02)00072-5</a>.
(<a href="http://www.sciencedirect.com/science/article/pii/S0167947302000725">ScienceDirect</a>)
</p>

<hr>
<h2 id='Headrick.dist'>Examples of Constants Calculated by Headrick's Fifth-Order Polynomial Transformation</h2><span id='topic+Headrick.dist'></span>

<h3>Description</h3>

<p>Selected symmetrical and asymmetrical theoretical densities with their associated values
of skewness (gamma1), standardized kurtosis (gamma2), and standardized fifth (gamma3) and
sixth (gamma4) cumulants.  Constants were calculated by Headrick using his fifth-order
polynomial transformation and given in his Table 1 (2002, p. 691-2, doi: <a href="http://doi.org/10.1016/S0167-9473(02)00072-5">10.1016/S0167-9473(02)00072-5</a>).  Note that the standardized cumulants for the Gamma(10, 10)
distribution do not arise from using <code class="reqn">\alpha = 10,\ \beta = 10</code>.  Therefore, either there is a typo in the table or
Headrick used a different parameterization.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(Headrick.dist)
</code></pre>


<h3>Format</h3>

<p>An object of class <code>"data.frame"</code>; Colnames are distribution names; rownames are
standardized cumulant names followed by c0, ..., c5.</p>


<h3>References</h3>

<p>Headrick TC (2002). Fast Fifth-order Polynomial Transforms for Generating Univariate and Multivariate
Non-normal Distributions. Computational Statistics &amp; Data Analysis, 40(4):685-711. doi: <a href="http://doi.org/10.1016/S0167-9473(02)00072-5">10.1016/S0167-9473(02)00072-5</a>.
(<a href="http://www.sciencedirect.com/science/article/pii/S0167947302000725">ScienceDirect</a>)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>z &lt;- rnorm(10000)
g &lt;- Headrick.dist$Gamma_a10b10[-c(1:4)]
gamma_a10b10 &lt;- g[1] + g[2] * z + g[3] * z^2 + g[4] * z^3 + g[5] * z^4 +
                g[6] * z^5
summary(gamma_a10b10)
</code></pre>

<hr>
<h2 id='intercorr_fleish'>Fleishman's Third-Order Polynomial Transformation Intermediate Correlation Equations</h2><span id='topic+intercorr_fleish'></span>

<h3>Description</h3>

<p>This function contains Fleishman's third-order polynomial transformation intermediate correlation
equations (Headrick &amp; Sawilowsky, 1999, doi: <a href="http://doi.org/10.1007/BF02294317">10.1007/BF02294317</a>).  It is used in <code><a href="#topic+findintercorr">findintercorr</a></code> and <code><a href="#topic+findintercorr2">findintercorr2</a></code>
to find the intermediate correlation for standard normal random variables which are used in the Fleishman
polynomial transformation.  It can be used to verify a set of constants and an intermediate correlation satisfy
the equations for the desired post-transformation correlation.  It works for two or three variables.  Headrick &amp;
Sawilowsky recommended using the technique of Vale &amp; Maurelli (1983,
doi: <a href="http://doi.org/10.1007/BF02293687">10.1007/BF02293687</a>), in the case of more than 3 variables, in which
the intermediate correlations are found pairwise and then eigen value decomposition is used on the correlation matrix.
Note that there exist solutions that yield invalid power
method pdfs (see <code><a href="#topic+power_norm_corr">power_norm_corr</a></code>, <code><a href="#topic+pdf_check">pdf_check</a></code>).
This function would not ordinarily be called by the user.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>intercorr_fleish(r, c, a)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="intercorr_fleish_+3A_r">r</code></td>
<td>
<p>either a scalar, in which case it represents pairwise intermediate correlation between standard normal variables,
or a vector of 3 values, in which case:
</p>
<p style="text-align: center;"><code class="reqn">r[1]*r[2] = \rho_{z1,z2},\ r[1]*r[3] = \rho_{z1,z3},\ r[2]*r[3] = \rho_{z2,z3}</code>
</p>
</td></tr>
<tr><td><code id="intercorr_fleish_+3A_c">c</code></td>
<td>
<p>a matrix with either 2 or 3 rows, each a vector of constants c0, c1, c2, c3, like that returned by
<code><a href="#topic+find_constants">find_constants</a></code></p>
</td></tr>
<tr><td><code id="intercorr_fleish_+3A_a">a</code></td>
<td>
<p>a matrix of target correlations among continuous variables; if <code>nrow(a) = 1</code>, it represents a pairwise
correlation; if <code>nrow(a) = 2 or 3</code>, it represents a correlation matrix between two or three variables</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list of length 1 for pairwise correlations or length 3 for three variables;
if the inputs satisfy the equations, returns 0 for all list elements
</p>


<h3>References</h3>

<p>Please see references for <code><a href="#topic+findintercorr_cont">findintercorr_cont</a></code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fleish">fleish</a></code>, <code><a href="#topic+power_norm_corr">power_norm_corr</a></code>,
<code><a href="#topic+pdf_check">pdf_check</a></code>, <code><a href="#topic+find_constants">find_constants</a></code>
</p>

<hr>
<h2 id='intercorr_poly'>Headrick's Fifth-Order Polynomial Transformation Intermediate Correlation Equations</h2><span id='topic+intercorr_poly'></span>

<h3>Description</h3>

<p>This function contains Headrick's fifth-order polynomial transformation intermediate correlation
equations (2002, doi: <a href="http://doi.org/10.1016/S0167-9473(02)00072-5">10.1016/S0167-9473(02)00072-5</a>).  It is used in <code><a href="#topic+findintercorr">findintercorr</a></code> and
<code><a href="#topic+findintercorr2">findintercorr2</a></code>
to find the intermediate correlation for standard normal random variables which are used in the Headrick
polynomial transformation.  It can be used to verify a set of constants and an intermediate correlation satisfy
the equations for the desired post-transformation correlation.  It works for two, three, or four variables.  Headrick recommended
using the technique of Vale &amp; Maurelli (1983,
doi: <a href="http://doi.org/10.1007/BF02293687">10.1007/BF02293687</a>), in the case of more than 4 variables, in which
the intermediate correlations are found pairwise and then eigen value decomposition is used on the correlation matrix.
Note that there exist solutions that yield invalid power
method pdfs (see <code><a href="#topic+power_norm_corr">power_norm_corr</a></code>, <code><a href="#topic+pdf_check">pdf_check</a></code>).
This function would not ordinarily be called by the user.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>intercorr_poly(r, c, a)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="intercorr_poly_+3A_r">r</code></td>
<td>
<p>either a scalar, in which case it represents pairwise intermediate correlation between standard normal variables,
or a vector of 3 values, in which case:
</p>
<p style="text-align: center;"><code class="reqn">r[1]*r[2] = \rho_{z1,z2},\ r[1]*r[3] = \rho_{z1,z3},\ r[2]*r[3] = \rho_{z2,z3}</code>
</p>

<p>or a vector of 4 values, in which case:
</p>
<p style="text-align: center;"><code class="reqn">r0 = r[5]*r[6],\ r0*r[1]*r[2] = \rho_{z1,z2},\ r0*r[1]*r[3] = \rho_{z1,z3}</code>
</p>

<p style="text-align: center;"><code class="reqn">r0*r[2]*r[3] = \rho_{z2,z3},\ r0*r[1]*r[4] = \rho_{z1,z4},\ r0*r[2]*r[4] = \rho_{z2,z4},</code>
</p>

<p style="text-align: center;"><code class="reqn">r0*r[3]*r[4] = \rho_{z3,z4}</code>
</p>
</td></tr>
<tr><td><code id="intercorr_poly_+3A_c">c</code></td>
<td>
<p>a matrix with either 2, 3, or 4 rows, each a vector of constants c0, c1, c2, c3, like that returned by
<code><a href="#topic+find_constants">find_constants</a></code></p>
</td></tr>
<tr><td><code id="intercorr_poly_+3A_a">a</code></td>
<td>
<p>a matrix of target correlations among continuous variables; if <code>nrow(a) = 1</code>, it represents a pairwise
correlation; if <code>nrow(a) = 2, 3, or 4</code>, it represents a correlation matrix between two, three, or four variables</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list of length 1 for pairwise correlations, length 3 for three variables, or length 6 for four variables;
if the inputs satisfy the equations, returns 0 for all list elements
</p>


<h3>References</h3>

<p>Please see references for <code><a href="#topic+findintercorr_cont">findintercorr_cont</a></code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+poly">poly</a></code>, <code><a href="#topic+power_norm_corr">power_norm_corr</a></code>,
<code><a href="#topic+pdf_check">pdf_check</a></code>, <code><a href="#topic+find_constants">find_constants</a></code>
</p>

<hr>
<h2 id='max_count_support'>Calculate Maximum Support Value for Count Variables: Correlation Method 2</h2><span id='topic+max_count_support'></span>

<h3>Description</h3>

<p>This function calculates the maximum support value for count variables by extending the method of Barbiero &amp;
Ferrari (2015, doi: <a href="http://doi.org/10.1002/asmb.2072">10.1002/asmb.2072</a>) to include Negative Binomial variables.  In order for count variables to be treated as ordinal in the
calculation of the intermediate MVN correlation matrix, their infinite support must be truncated (made finite).  This is
done by setting the total cumulative probability equal to 1 - a small user-specified value (<code>pois_eps</code> or <code>nb_eps</code>.  The
maximum support value equals the inverse cdf applied to this result.  The values pois_eps and nb_eps may differ for each variable.
The function is used in <code><a href="#topic+findintercorr2">findintercorr2</a></code> and <code><a href="#topic+rcorrvar2">rcorrvar2</a></code>.
This function would not ordinarily be called by the user.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>max_count_support(k_pois, k_nb, lam, pois_eps = NULL, size, prob, mu = NULL,
  nb_eps = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="max_count_support_+3A_k_pois">k_pois</code></td>
<td>
<p>the number of Poisson variables</p>
</td></tr>
<tr><td><code id="max_count_support_+3A_k_nb">k_nb</code></td>
<td>
<p>the number of Negative Binomial variables</p>
</td></tr>
<tr><td><code id="max_count_support_+3A_lam">lam</code></td>
<td>
<p>a vector of lambda (&gt; 0) constants for the Poisson variables (see <code><a href="stats.html#topic+Poisson">Poisson</a></code>)</p>
</td></tr>
<tr><td><code id="max_count_support_+3A_pois_eps">pois_eps</code></td>
<td>
<p>a vector of length k_pois containing the truncation values (i.e. = rep(0.0001, k_pois); default = NULL)</p>
</td></tr>
<tr><td><code id="max_count_support_+3A_size">size</code></td>
<td>
<p>a vector of size parameters for the Negative Binomial variables (see <code><a href="stats.html#topic+NegBinomial">NegBinomial</a></code>)</p>
</td></tr>
<tr><td><code id="max_count_support_+3A_prob">prob</code></td>
<td>
<p>a vector of success probability parameters</p>
</td></tr>
<tr><td><code id="max_count_support_+3A_mu">mu</code></td>
<td>
<p>a vector of mean parameters (*Note: either <code>prob</code> or <code>mu</code> should be supplied for all Negative Binomial variables,
not a mixture; default = NULL)</p>
</td></tr>
<tr><td><code id="max_count_support_+3A_nb_eps">nb_eps</code></td>
<td>
<p>a vector of length <code>k_nb</code> containing the truncation values (i.e. = rep(0.0001, k_nb); default = NULL)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a data.frame with <code>k_pois + k_nb</code> rows; the column names are:
</p>
<p><code>Distribution</code> Poisson or Negative Binomial
</p>
<p><code>Number</code> the variable index
</p>
<p><code>Max</code> the maximum support value
</p>


<h3>References</h3>

<p>Barbiero A &amp; Ferrari PA (2015). Simulation of correlated Poisson variables. Applied Stochastic Models in
Business and Industry, 31: 669-80. doi: <a href="http://doi.org/10.1002/asmb.2072">10.1002/asmb.2072</a>.
</p>
<p>Ferrari PA, Barbiero A (2012). Simulating ordinal data, Multivariate Behavioral Research, 47(4): 566-589. doi: <a href="http://doi.org/10.1080/00273171.2012.692630">10.1080/00273171.2012.692630</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+findintercorr2">findintercorr2</a></code>, <code><a href="#topic+rcorrvar2">rcorrvar2</a></code>
</p>

<hr>
<h2 id='nonnormvar1'>Generation of One Non-Normal Continuous Variable Using the Power Method</h2><span id='topic+nonnormvar1'></span>

<h3>Description</h3>

<p>This function simulates one non-normal continuous variable using either Fleishman's Third-Order (<code>method</code> = &quot;Fleishman&quot;,
doi: <a href="http://doi.org/10.1007/BF02293811">10.1007/BF02293811</a>) or Headrick's Fifth-Order (<code>method</code> = &quot;Polynomial&quot;, doi: <a href="http://doi.org/10.1016/S0167-9473(02)00072-5">10.1016/S0167-9473(02)00072-5</a>) Polynomial
Transformation.  If only one variable is desired and that variable is continuous, this function should be used.  The power method
transformation is a computationally efficient algorithm that simulates continuous distributions through the method of moments.
It works by matching standardized cumulants &ndash; the first four (mean, variance, skew, and standardized kurtosis) for Fleishman's
method, or the first six (mean, variance, skew, standardized kurtosis, and standardized fifth and sixth cumulants) for Headrick's
method.  The transformation is expressed as follows:
</p>
<p><code class="reqn">Y = c_{0} + c_{1} * Z + c_{2} * Z^2 + c_{3} * Z^3 + c_{4} * Z^4 + c_{5} * Z^5</code>,
</p>
<p>where <code class="reqn">Z ~ N(0,1)</code>, and <code class="reqn">c_{4}</code> and <code class="reqn">c_{5}</code> both equal <code class="reqn">0</code> for Fleishman's method.  The real constants are calculated by
<code><a href="#topic+find_constants">find_constants</a></code>.  All variables are simulated with mean <code class="reqn">0</code> and variance <code class="reqn">1</code>, and then
transformed to the specified mean and variance at the end.
</p>
<p>The required parameters for simulating continuous variables include: mean, variance, skewness, standardized kurtosis (kurtosis - 3), and
standardized fifth and sixth cumulants (for <code>method</code> = &quot;Polynomial&quot;).  If the goal is to simulate a theoretical distribution
(i.e. Gamma, Beta, Logistic, etc.), these values can be obtained using <code><a href="#topic+calc_theory">calc_theory</a></code>.  If the goal is to
mimic an empirical data set, these values can be found using <code><a href="#topic+calc_moments">calc_moments</a></code> (using the method of moments) or
<code><a href="#topic+calc_fisherk">calc_fisherk</a></code> (using Fisher's k-statistics).  If the standardized cumulants
are obtained from <code>calc_theory</code>, the user may need to use rounded values as inputs (i.e. <code>skews = round(skews, 8)</code>).  Due to the nature
of the integration involved in <code>calc_theory</code>, the results are approximations.  Greater accuracy can be achieved by increasing the number of
subdivisions (<code>sub</code>) used in the integration process.  For example, in order to ensure that skew is exactly 0 for symmetric distributions.
</p>
<p>For some sets of cumulants, it is either not possible to find power method constants or the
calculated constants do not generate valid power method pdfs.  In these situations, adding a value to the sixth cumulant may
provide solutions (see <code><a href="#topic+find_constants">find_constants</a></code>).  If simulation results indicate that a continuous variable
does not generate a valid pdf, the user can try <code><a href="#topic+find_constants">find_constants</a></code> with various sixth cumulant correction
vectors to determine if a valid pdf can be found.
</p>
<p>Headrick &amp; Kowalchuk (2007, doi: <a href="http://doi.org/10.1080/10629360600605065">10.1080/10629360600605065</a>) outlined a general
method for comparing a simulated distribution <code class="reqn">Y</code> to a given theoretical distribution <code class="reqn">Y^*</code>.  These steps can be found in
the example and the <b>Comparison of Simulated Distribution to Theoretical Distribution or Empirical Data</b> vignette.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nonnormvar1(method = c("Fleishman", "Polynomial"), means = 0, vars = 1,
  skews = 0, skurts = 0, fifths = 0, sixths = 0, Six = NULL,
  cstart = NULL, n = 10000, seed = 1234)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nonnormvar1_+3A_method">method</code></td>
<td>
<p>the method used to generate the continuous variable.  &quot;Fleishman&quot; uses Fleishman's third-order polynomial transformation
and &quot;Polynomial&quot; uses Headrick's fifth-order transformation.</p>
</td></tr>
<tr><td><code id="nonnormvar1_+3A_means">means</code></td>
<td>
<p>mean for the continuous variable (default = 0)</p>
</td></tr>
<tr><td><code id="nonnormvar1_+3A_vars">vars</code></td>
<td>
<p>variance (default = 1)</p>
</td></tr>
<tr><td><code id="nonnormvar1_+3A_skews">skews</code></td>
<td>
<p>skewness value (default = 0)</p>
</td></tr>
<tr><td><code id="nonnormvar1_+3A_skurts">skurts</code></td>
<td>
<p>standardized kurtosis (kurtosis - 3, so that normal variables have a value of 0; default = 0)</p>
</td></tr>
<tr><td><code id="nonnormvar1_+3A_fifths">fifths</code></td>
<td>
<p>standardized fifth cumulant (not necessary for <code>method</code> = &quot;Fleishman&quot;; default = 0)</p>
</td></tr>
<tr><td><code id="nonnormvar1_+3A_sixths">sixths</code></td>
<td>
<p>standardized sixth cumulant (not necessary for <code>method</code> = &quot;Fleishman&quot;; default = 0)</p>
</td></tr>
<tr><td><code id="nonnormvar1_+3A_six">Six</code></td>
<td>
<p>a vector of correction values to add to the sixth cumulant if no valid pdf constants are found,
ex: <code>Six = seq(0.01, 2, by = 0.01)</code>; if no correction is desired, set <code>Six = NULL</code> (default)</p>
</td></tr>
<tr><td><code id="nonnormvar1_+3A_cstart">cstart</code></td>
<td>
<p>initial values for root-solving algorithm (see <code><a href="BB.html#topic+multiStart">multiStart</a></code> for <code>method</code> = &quot;Fleishman&quot;
or <code><a href="nleqslv.html#topic+nleqslv">nleqslv</a></code> for <code>method</code> = &quot;Polynomial&quot;).  If user specified,
must be input as a matrix. If NULL and all 4 standardized cumulants (rounded to 3 digits) are within
0.01 of those in Headrick's common distribution table (see <code><a href="#topic+Headrick.dist">Headrick.dist</a></code>
data), uses his constants as starting values; else, generates n sets of random starting values from
uniform distributions.</p>
</td></tr>
<tr><td><code id="nonnormvar1_+3A_n">n</code></td>
<td>
<p>the sample size (i.e. the length of the simulated variable; default = 10000)</p>
</td></tr>
<tr><td><code id="nonnormvar1_+3A_seed">seed</code></td>
<td>
<p>the seed value for random number generation (default = 1234)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with the following components:
</p>
<p><code>constants</code> a data.frame of the constants
</p>
<p><code>continuous_variable</code> a data.frame of the generated continuous variable
</p>
<p><code>summary_continuous</code> a data.frame containing a summary of the variable
</p>
<p><code>summary_targetcont</code> a data.frame containing a summary of the target variable
</p>
<p><code>sixth_correction</code> the sixth cumulant correction value
</p>
<p><code>valid.pdf</code> &quot;TRUE&quot; if constants generate a valid pdf, else &quot;FALSE&quot;
</p>
<p><code>Constants_Time</code> the time in minutes required to calculate the constants
</p>
<p><code>Simulation_Time</code> the total simulation time in minutes
</p>


<h3>Choice of Fleishman's third-order or Headrick's fifth-order method</h3>

<p>Using the fifth-order approximation allows additional control over the fifth and sixth moments of the generated distribution, improving
accuracy.  In addition, the range of feasible standardized kurtosis values, given skew and standardized fifth (<code class="reqn">\gamma_{3}</code>) and sixth
(<code class="reqn">\gamma_{4}</code>) cumulants, is larger than with Fleishman's method (see <code><a href="#topic+calc_lower_skurt">calc_lower_skurt</a></code>).
For example, the Fleishman method can not be used to generate a
non-normal distribution with a ratio of <code class="reqn">\gamma_{3}^2/\gamma_{4} &gt; 9/14</code> (see Headrick &amp; Kowalchuk, 2007).  This eliminates the
Chi-squared family of distributions, which has a constant ratio of <code class="reqn">\gamma_{3}^2/\gamma_{4} = 2/3</code>.  However, if the fifth and
sixth cumulants do not exist, the Fleishman approximation should be used.
</p>


<h3>Overview of Simulation Process</h3>

<p>1) The constants are calculated for the continuous variable using <code><a href="#topic+find_constants">find_constants</a></code>.  If no
solutions are found that generate a valid power method pdf, the function will return constants that produce an invalid pdf
(or a stop error if no solutions can be found).  Possible solutions include: 1) changing the seed, or 2) using a <code>Six</code> vector
of sixth cumulant correction values (if <code>method</code> = &quot;Polynomial&quot;).  Errors regarding constant
calculation are the most probable cause of function failure.
</p>
<p>2) An intermediate standard normal variate X of length n is generated.
</p>
<p>3) Summary statistics are calculated.
</p>


<h3>Reasons for Function Errors</h3>

<p>1) The most likely cause for function errors is that no solutions to <code><a href="#topic+fleish">fleish</a></code> or
<code><a href="#topic+poly">poly</a></code> converged when using <code><a href="#topic+find_constants">find_constants</a></code>.  If this happens,
the simulation will stop.  It may help to first use <code><a href="#topic+find_constants">find_constants</a></code> for each continuous variable to
determine if a vector of sixth cumulant correction values is needed.  The solutions can be used as starting values (see <code>cstart</code> below).
If the standardized cumulants are obtained from <code>calc_theory</code>, the user may need to use rounded values as inputs (i.e.
<code>skews = round(skews, 8)</code>).
</p>
<p>2) In addition, the kurtosis may be outside the region of possible values.  There is an associated lower boundary for kurtosis associated
with a given skew (for Fleishman's method) or skew and fifth and sixth cumulants (for Headrick's method).  Use
<code><a href="#topic+calc_lower_skurt">calc_lower_skurt</a></code> to determine the boundary for a given set of cumulants.
</p>


<h3>References</h3>

<p>Fleishman AI (1978). A Method for Simulating Non-normal Distributions. Psychometrika, 43, 521-532. doi: <a href="http://doi.org/10.1007/BF02293811">10.1007/BF02293811</a>.
</p>
<p>Headrick TC (2002). Fast Fifth-order Polynomial Transforms for Generating Univariate and Multivariate
Non-normal Distributions. Computational Statistics &amp; Data Analysis, 40(4):685-711. doi: <a href="http://doi.org/10.1016/S0167-9473(02)00072-5">10.1016/S0167-9473(02)00072-5</a>.
(<a href="http://www.sciencedirect.com/science/article/pii/S0167947302000725">ScienceDirect</a>)
</p>
<p>Headrick TC (2004). On Polynomial Transformations for Simulating Multivariate Nonnormal Distributions.
Journal of Modern Applied Statistical Methods, 3(1), 65-71. doi: <a href="http://doi.org/10.22237/jmasm/1083370080">10.22237/jmasm/1083370080</a>.
</p>
<p>Headrick TC, Kowalchuk RK (2007). The Power Method Transformation: Its Probability Density Function, Distribution
Function, and Its Further Use for Fitting Data. Journal of Statistical Computation and Simulation, 77, 229-249. doi: <a href="http://doi.org/10.1080/10629360600605065">10.1080/10629360600605065</a>.
</p>
<p>Headrick TC, Sawilowsky SS (1999). Simulating Correlated Non-normal Distributions: Extending the Fleishman Power
Method. Psychometrika, 64, 25-35. doi: <a href="http://doi.org/10.1007/BF02294317">10.1007/BF02294317</a>.
</p>
<p>Headrick TC, Sheng Y, &amp; Hodis FA (2007). Numerical Computing and Graphics for the Power Method Transformation Using
Mathematica. Journal of Statistical Software, 19(3), 1 - 17. doi: <a href="http://doi.org/10.18637/jss.v019.i03">10.18637/jss.v019.i03</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+find_constants">find_constants</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Normal distribution with Headrick's fifth-order PMT:
N &lt;- nonnormvar1("Polynomial", 0, 1, 0, 0, 0, 0)

## Not run: 
# Use Headrick &amp; Kowalchuk's (2007) steps to compare a simulated exponential
# (mean = 2) variable to the theoretical exponential(mean = 2) density:

# 1) Obtain the standardized cumulants:
stcums &lt;- calc_theory(Dist = "Exponential", params = 0.5) # rate = 1/mean

# 2) Simulate the variable:
H_exp &lt;- nonnormvar1("Polynomial", means = 2, vars = 2, skews = stcums[3],
                    skurts = stcums[4], fifths = stcums[5],
                    sixths = stcums[6], n = 10000, seed = 1234)

H_exp$constants
#           c0        c1       c2         c3          c4           c5
# 1 -0.3077396 0.8005605 0.318764 0.03350012 -0.00367481 0.0001587076

# 3) Determine whether the constants produce a valid power method pdf:

H_exp$valid.pdf
# [1] "TRUE"

# 4) Select a critical value:

# Let alpha = 0.05.
y_star &lt;- qexp(1 - 0.05, rate = 0.5) # note that rate = 1/mean
y_star
# [1] 5.991465

# 5) Solve m_{2}^{1/2} * p(z') + m_{1} - y* = 0 for z', where m_{1} and
# m_{2} are the 1st and 2nd moments of Y*:

# The exponential(2) distribution has a mean and standard deviation equal
# to 2.
# Solve 2 * p(z') + 2 - y_star = 0 for z'
# p(z') = c0 + c1 * z' + c2 * z'^2 + c3 * z'^3 + c4 * z'^4 + c5 * z'^5

f_exp &lt;- function(z, c, y) {
  return(2 * (c[1] + c[2] * z + c[3] * z^2 + c[4] * z^3 + c[5] * z^4 +
              c[6] * z^5) + 2 - y)
}

z_prime &lt;- uniroot(f_exp, interval = c(-1e06, 1e06),
                   c = as.numeric(H_exp$constants), y = y_star)$root
z_prime
# [1] 1.644926

# 6) Calculate 1 - Phi(z'), the corresponding probability for the
# approximation Y to Y* (i.e. 1 - Phi(z') = 0.05), and compare to target
# value alpha:

1 - pnorm(z_prime)
# [1] 0.04999249

# 7) Plot a parametric graph of Y* and Y:

plot_sim_pdf_theory(sim_y = as.numeric(H_exp$continuous_variable[, 1]),
                    Dist = "Exponential", params = 0.5)

# Note we can also plot the empirical cdf and show the cumulative
# probability up to y_star:

plot_sim_cdf(sim_y = as.numeric(H_exp$continuous_variable[, 1]),
             calc_cprob = TRUE, delta = y_star)


## End(Not run)
</code></pre>

<hr>
<h2 id='ordnorm'>Calculate Intermediate MVN Correlation to Generate Variables Treated as Ordinal</h2><span id='topic+ordnorm'></span>

<h3>Description</h3>

<p>This function calculates the intermediate MVN correlation needed to generate a variable described by
a discrete marginal distribution and associated finite support.  This includes ordinal (r &gt;= 2 categories) variables
or variables that are treated as ordinal (i.e. count variables in the Barbiero &amp; Ferrari, 2015 method used in
<code><a href="#topic+rcorrvar2">rcorrvar2</a></code>, doi: <a href="http://doi.org/10.1002/asmb.2072">10.1002/asmb.2072</a>).  The function is a modification of  Barbiero &amp; Ferrari's
<code><a href="GenOrd.html#topic+ordcont">ordcont</a></code>
function in <code><a href="GenOrd.html#topic+GenOrd-package">GenOrd-package</a></code>.  It works by setting the intermediate MVN correlation equal to the target
correlation and updating each intermediate pairwise correlation until the final pairwise correlation is within epsilon of the
target correlation or the maximum number of iterations has been reached.  This function uses <code><a href="GenOrd.html#topic+contord">contord</a></code>
to calculate the ordinal correlation obtained from discretizing the normal variables generated from the intermediate
correlation matrix.  The <code><a href="GenOrd.html#topic+ordcont">ordcont</a></code> has been modified in the following ways:
</p>
<p>1) the initial correlation check has been removed because it is assumed the user has done this before simulation using
<code><a href="#topic+valid_corr">valid_corr</a></code> or <code><a href="#topic+valid_corr2">valid_corr2</a></code>
</p>
<p>2) the final positive-definite check has been removed
</p>
<p>3) the intermediate correlation update function was changed to accomodate more situations, and
</p>
<p>4) a final &quot;fail-safe&quot; check was added at the end of the iteration loop where if the absolute
error between the final and target pairwise correlation is still &gt; 0.1, the intermediate correlation is set
equal to the target correlation.
</p>
<p>This function would not ordinarily be called by the user.  Note that this will return a matrix that is NOT positive-definite
because this is corrected for in the
simulation functions <code><a href="#topic+rcorrvar">rcorrvar</a></code> and <code><a href="#topic+rcorrvar2">rcorrvar2</a></code>
using the method of Higham (2002) and the <code><a href="Matrix.html#topic+nearPD">nearPD</a></code> function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ordnorm(marginal, rho, support = list(), epsilon = 0.001, maxit = 1000)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ordnorm_+3A_marginal">marginal</code></td>
<td>
<p>a list of length equal to the number of variables; the i-th element is a vector of the cumulative
probabilities defining the marginal distribution of the i-th variable;
if the variable can take r values, the vector will contain r - 1 probabilities (the r-th is assumed to be 1)</p>
</td></tr>
<tr><td><code id="ordnorm_+3A_rho">rho</code></td>
<td>
<p>the target correlation matrix</p>
</td></tr>
<tr><td><code id="ordnorm_+3A_support">support</code></td>
<td>
<p>a list of length equal to the number of variables; the i-th element is a vector of containing the r
ordered support values; if not provided (i.e. support = list()), the default is for the i-th element to be the vector 1, ..., r</p>
</td></tr>
<tr><td><code id="ordnorm_+3A_epsilon">epsilon</code></td>
<td>
<p>the maximum acceptable error between the final and target correlation matrices (default = 0.001);
smaller epsilons take more time</p>
</td></tr>
<tr><td><code id="ordnorm_+3A_maxit">maxit</code></td>
<td>
<p>the maximum number of iterations to use (default = 1000) to find the intermediate correlation; the
correction loop stops when either the iteration number passes maxit or epsilon is reached</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with the following components:
</p>
<p><code>SigmaC</code> the intermediate MVN correlation matrix
</p>
<p><code>rho0</code> the calculated final correlation matrix generated from <code>SigmaC</code>
</p>
<p><code>rho</code> the target final correlation matrix
</p>
<p><code>niter</code> a matrix containing the number of iterations required for each variable pair
</p>
<p><code>maxerr</code> the maximum final error between the final and target correlation matrices
</p>


<h3>References</h3>

<p>Barbiero A, Ferrari PA (2015). Simulation of correlated Poisson variables. Applied Stochastic Models
in Business and Industry, 31: 669-80. doi: <a href="http://doi.org/10.1002/asmb.2072">10.1002/asmb.2072</a>.
</p>
<p>Barbiero A, Ferrari PA (2015). GenOrd: Simulation of Discrete Random Variables with Given
Correlation Matrix and Marginal Distributions. R package version 1.4.0.
<a href="https://CRAN.R-project.org/package=GenOrd">https://CRAN.R-project.org/package=GenOrd</a>
</p>
<p>Ferrari PA, Barbiero A (2012). Simulating ordinal data, Multivariate Behavioral Research, 47(4): 566-589. doi: <a href="http://doi.org/10.1080/00273171.2012.692630">10.1080/00273171.2012.692630</a>.
</p>


<h3>See Also</h3>

<p><code><a href="GenOrd.html#topic+ordcont">ordcont</a></code>, <code><a href="#topic+rcorrvar">rcorrvar</a></code>, <code><a href="#topic+rcorrvar2">rcorrvar2</a></code>,
<code><a href="#topic+findintercorr">findintercorr</a></code>, <code><a href="#topic+findintercorr2">findintercorr2</a></code>
</p>

<hr>
<h2 id='pdf_check'>Check Polynomial Transformation Constants for Valid Power Method PDF</h2><span id='topic+pdf_check'></span>

<h3>Description</h3>

<p>This function determines if a given set of constants, calculated using Fleishman's Third-Order (<code>method</code> = &quot;Fleishman&quot;,
doi: <a href="http://doi.org/10.1007/BF02293811">10.1007/BF02293811</a>) or Headrick's Fifth-Order (<code>method</code> = &quot;Polynomial&quot;, doi: <a href="http://doi.org/10.1016/S0167-9473(02)00072-5">10.1016/S0167-9473(02)00072-5</a>) Polynomial
Transformation, yields a valid pdf.  This requires 1) the correlation between the
resulting continuous variable and the underlying standard normal variable (see
<code><a href="#topic+power_norm_corr">power_norm_corr</a></code>) is &gt; 0, and 2) the constants satisfy certain constraints (see Headrick &amp; Kowalchuk, 2007,
doi: <a href="http://doi.org/10.1080/10629360600605065">10.1080/10629360600605065</a>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pdf_check(c, method)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pdf_check_+3A_c">c</code></td>
<td>
<p>a vector of constants c0, c1, c2, c3 (if <code>method</code> = &quot;Fleishman&quot;) or c0, c1, c2, c3, c4, c5 (if <code>method</code> =
&quot;Polynomial&quot;), like that returned by <code><a href="#topic+find_constants">find_constants</a></code></p>
</td></tr>
<tr><td><code id="pdf_check_+3A_method">method</code></td>
<td>
<p>the method used to find the constants.  &quot;Fleishman&quot; uses a third-order polynomial transformation and
&quot;Polynomial&quot; uses Headrick's fifth-order transformation.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with components:
</p>
<p><code>rho_pZ</code> the correlation between the continuous variable and the underlying standard normal variable
</p>
<p><code>valid.pdf</code> &quot;TRUE&quot; if the constants produce a valid power method pdf, else &quot;FALSE&quot;
</p>


<h3>References</h3>

<p>Fleishman AI (1978). A Method for Simulating Non-normal Distributions. Psychometrika, 43, 521-532. doi: <a href="http://doi.org/10.1007/BF02293811">10.1007/BF02293811</a>.
</p>
<p>Headrick TC (2002). Fast Fifth-order Polynomial Transforms for Generating Univariate and Multivariate
Non-normal Distributions. Computational Statistics &amp; Data Analysis, 40(4):685-711. doi: <a href="http://doi.org/10.1016/S0167-9473(02)00072-5">10.1016/S0167-9473(02)00072-5</a>.
(<a href="http://www.sciencedirect.com/science/article/pii/S0167947302000725">ScienceDirect</a>)
</p>
<p>Headrick TC (2004). On Polynomial Transformations for Simulating Multivariate Nonnormal Distributions.
Journal of Modern Applied Statistical Methods, 3(1), 65-71. doi: <a href="http://doi.org/10.22237/jmasm/1083370080">10.22237/jmasm/1083370080</a>.
</p>
<p>Headrick TC, Kowalchuk RK (2007). The Power Method Transformation: Its Probability Density Function, Distribution
Function, and Its Further Use for Fitting Data. Journal of Statistical Computation and Simulation, 77, 229-249. doi: <a href="http://doi.org/10.1080/10629360600605065">10.1080/10629360600605065</a>.
</p>
<p>Headrick TC, Sawilowsky SS (1999). Simulating Correlated Non-normal Distributions: Extending the Fleishman Power
Method. Psychometrika, 64, 25-35. doi: <a href="http://doi.org/10.1007/BF02294317">10.1007/BF02294317</a>.
</p>
<p>Headrick TC, Sheng Y, &amp; Hodis FA (2007). Numerical Computing and Graphics for the Power Method Transformation Using
Mathematica. Journal of Statistical Software, 19(3), 1 - 17. doi: <a href="http://doi.org/10.18637/jss.v019.i03">10.18637/jss.v019.i03</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fleish">fleish</a></code>, <code><a href="#topic+poly">poly</a></code>,
<code><a href="#topic+power_norm_corr">power_norm_corr</a></code>, <code><a href="#topic+find_constants">find_constants</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Normal distribution
pdf_check(c(0, 1, 0, 0, 0, 0), "Polynomial")

## Not run: 
# Chi-squared (df = 1) Distribution (invalid power method pdf)
con &lt;- find_constants(method = "Polynomial", skews = sqrt(8), skurts = 12,
                      fifths = 48*sqrt(2), sixths = 480)$constants
pdf_check(c = con, method = "Polynomial")

# Beta (a = 4, b = 2) Distribution (valid power method pdf)
con &lt;- find_constants(method = "Polynomial", skews = -0.467707,
                      skurts = -0.375, fifths = 1.403122,
                      sixths = -0.426136)$constants
pdf_check(c = con, method = "Polynomial")


## End(Not run)
</code></pre>

<hr>
<h2 id='plot_cdf'>Plot Theoretical Power Method Cumulative Distribution Function for Continuous Variables</h2><span id='topic+plot_cdf'></span>

<h3>Description</h3>

<p>This plots the theoretical power method cumulative distribution function:
</p>
<p style="text-align: center;"><code class="reqn">F_p(Z)(p(z)) = F_p(Z)(p(z), F_Z(z)),</code>
</p>
<p> as given in Headrick &amp; Kowalchuk (2007, doi: <a href="http://doi.org/10.1080/10629360600605065">10.1080/10629360600605065</a>).
It is a parametric plot with <code class="reqn">sigma * y + mu</code>, where <code class="reqn">y = p(z)</code>, on the x-axis and <code class="reqn">F_Z(z)</code> on the y-axis,
where <code class="reqn">z</code> is vector of <code class="reqn">n</code> random standard normal numbers (generated with a seed set by user).  Given a vector of polynomial
transformation constants, the function generates <code class="reqn">sigma * y + mu</code> and calculates the theoretical cumulative probabilities
using <code class="reqn">F_p(Z)(p(z), F_Z(z))</code>.  If <code>calc_cprob</code> = TRUE, the cumulative probability up to <code class="reqn">delta = sigma * y + mu</code> is
calculated (see <code><a href="#topic+cdf_prob">cdf_prob</a></code>) and the region on the plot is filled with a dashed horizontal
line drawn at <code class="reqn">F_p(Z)(delta)</code>.  The cumulative probability is stated on top of the line.  It returns a <code><a href="ggplot2.html#topic+ggplot2-package">ggplot2-package</a></code> object so
the user can modify as necessary.  The graph parameters (i.e. <code>title</code>, <code>color</code>, <code>fill</code>, <code>hline</code>) are
<code><a href="ggplot2.html#topic+ggplot2-package">ggplot2-package</a></code> parameters.  It works for valid or invalid power method pdfs.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_cdf(c = NULL, method = c("Fleishman", "Polynomial"), mu = 0,
  sigma = 1, title = "Cumulative Distribution Function", ylower = NULL,
  yupper = NULL, calc_cprob = FALSE, delta = 5, color = "dark blue",
  fill = "blue", hline = "dark green", n = 10000, seed = 1234,
  text.size = 11, title.text.size = 15, axis.text.size = 10,
  axis.title.size = 13, lower = -1000000, upper = 1000000)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_cdf_+3A_c">c</code></td>
<td>
<p>a vector of constants c0, c1, c2, c3 (if <code>method</code> = &quot;Fleishman&quot;) or c0, c1, c2, c3, c4, c5 (if <code>method</code> =
&quot;Polynomial&quot;), like that returned by <code><a href="#topic+find_constants">find_constants</a></code></p>
</td></tr>
<tr><td><code id="plot_cdf_+3A_method">method</code></td>
<td>
<p>the method used to generate the continuous variable <code class="reqn">y = p(z)</code>.  &quot;Fleishman&quot; uses Fleishman's third-order polynomial
transformation and &quot;Polynomial&quot; uses Headrick's fifth-order transformation.</p>
</td></tr>
<tr><td><code id="plot_cdf_+3A_mu">mu</code></td>
<td>
<p>mean for the continuous variable (default = 0)</p>
</td></tr>
<tr><td><code id="plot_cdf_+3A_sigma">sigma</code></td>
<td>
<p>standard deviation for the continuous variable (default = 1)</p>
</td></tr>
<tr><td><code id="plot_cdf_+3A_title">title</code></td>
<td>
<p>the title for the graph (default = &quot;Cumulative Distribution Function&quot;)</p>
</td></tr>
<tr><td><code id="plot_cdf_+3A_ylower">ylower</code></td>
<td>
<p>the lower y value to use in the plot (default = NULL, uses minimum simulated y value)</p>
</td></tr>
<tr><td><code id="plot_cdf_+3A_yupper">yupper</code></td>
<td>
<p>the upper y value (default = NULL, uses maximum simulated y value)</p>
</td></tr>
<tr><td><code id="plot_cdf_+3A_calc_cprob">calc_cprob</code></td>
<td>
<p>if TRUE (default = FALSE), <code><a href="#topic+cdf_prob">cdf_prob</a></code> is used to find the cumulative probability
up to <code class="reqn">delta = sigma * y + mu</code> and the region on the plot is filled with a dashed horizontal line drawn at <code class="reqn">F_p(Z)(delta)</code></p>
</td></tr>
<tr><td><code id="plot_cdf_+3A_delta">delta</code></td>
<td>
<p>the value <code class="reqn">sigma * y + mu</code>, where <code class="reqn">y = p(z)</code>, at which to evaluate the cumulative probability</p>
</td></tr>
<tr><td><code id="plot_cdf_+3A_color">color</code></td>
<td>
<p>the line color for the cdf (default = &quot;dark blue&quot;)</p>
</td></tr>
<tr><td><code id="plot_cdf_+3A_fill">fill</code></td>
<td>
<p>the fill color if <code>calc_cprob</code> = TRUE (default = &quot;blue)</p>
</td></tr>
<tr><td><code id="plot_cdf_+3A_hline">hline</code></td>
<td>
<p>the dashed horizontal line color drawn at delta if <code>calc_cprob</code> = TRUE (default = &quot;dark green&quot;)</p>
</td></tr>
<tr><td><code id="plot_cdf_+3A_n">n</code></td>
<td>
<p>the number of random standard normal numbers to use in generating <code class="reqn">y = p(z)</code> (default = 10000)</p>
</td></tr>
<tr><td><code id="plot_cdf_+3A_seed">seed</code></td>
<td>
<p>the seed value for random number generation (default = 1234)</p>
</td></tr>
<tr><td><code id="plot_cdf_+3A_text.size">text.size</code></td>
<td>
<p>the size of the text displaying the cumulative probability up to <code>delta</code> if <code>calc_cprob</code> = TRUE</p>
</td></tr>
<tr><td><code id="plot_cdf_+3A_title.text.size">title.text.size</code></td>
<td>
<p>the size of the plot title</p>
</td></tr>
<tr><td><code id="plot_cdf_+3A_axis.text.size">axis.text.size</code></td>
<td>
<p>the size of the axes text (tick labels)</p>
</td></tr>
<tr><td><code id="plot_cdf_+3A_axis.title.size">axis.title.size</code></td>
<td>
<p>the size of the axes titles</p>
</td></tr>
<tr><td><code id="plot_cdf_+3A_lower">lower</code></td>
<td>
<p>lower bound for <code>cdf_prob</code></p>
</td></tr>
<tr><td><code id="plot_cdf_+3A_upper">upper</code></td>
<td>
<p>upper bound for <code>cdf_prob</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code><a href="ggplot2.html#topic+ggplot2-package">ggplot2-package</a></code> object.
</p>


<h3>References</h3>

<p>Fleishman AI (1978). A Method for Simulating Non-normal Distributions. Psychometrika, 43, 521-532. doi: <a href="http://doi.org/10.1007/BF02293811">10.1007/BF02293811</a>.
</p>
<p>Headrick TC (2002). Fast Fifth-order Polynomial Transforms for Generating Univariate and Multivariate
Non-normal Distributions. Computational Statistics &amp; Data Analysis, 40(4):685-711. doi: <a href="http://doi.org/10.1016/S0167-9473(02)00072-5">10.1016/S0167-9473(02)00072-5</a>.
(<a href="http://www.sciencedirect.com/science/article/pii/S0167947302000725">ScienceDirect</a>)
</p>
<p>Headrick TC (2004). On Polynomial Transformations for Simulating Multivariate Nonnormal Distributions.
Journal of Modern Applied Statistical Methods, 3(1), 65-71. doi: <a href="http://doi.org/10.22237/jmasm/1083370080">10.22237/jmasm/1083370080</a>.
</p>
<p>Headrick TC, Kowalchuk RK (2007). The Power Method Transformation: Its Probability Density Function, Distribution
Function, and Its Further Use for Fitting Data. Journal of Statistical Computation and Simulation, 77, 229-249. doi: <a href="http://doi.org/10.1080/10629360600605065">10.1080/10629360600605065</a>.
</p>
<p>Headrick TC, Sawilowsky SS (1999). Simulating Correlated Non-normal Distributions: Extending the Fleishman Power
Method. Psychometrika, 64, 25-35. doi: <a href="http://doi.org/10.1007/BF02294317">10.1007/BF02294317</a>.
</p>
<p>Headrick TC, Sheng Y, &amp; Hodis FA (2007). Numerical Computing and Graphics for the Power Method Transformation Using
Mathematica. Journal of Statistical Software, 19(3), 1 - 17. doi: <a href="http://doi.org/10.18637/jss.v019.i03">10.18637/jss.v019.i03</a>.
</p>
<p>Wickham H. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York, 2009.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+find_constants">find_constants</a></code>, <code><a href="#topic+cdf_prob">cdf_prob</a></code>,
<code><a href="ggplot2.html#topic+ggplot2-package">ggplot2-package</a></code>, <code><a href="ggplot2.html#topic+geom_path">geom_path</a></code>, <code><a href="ggplot2.html#topic+geom_abline">geom_abline</a></code>,
<code><a href="ggplot2.html#topic+geom_ribbon">geom_ribbon</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Logistic Distribution: mean = 0, sigma = 1

# Find standardized cumulants
stcum &lt;- calc_theory(Dist = "Logistic", params = c(0, 1))

# Find constants without the sixth cumulant correction
# (invalid power method pdf)
con1 &lt;- find_constants(method = "Polynomial", skews = stcum[3],
                      skurts = stcum[4], fifths = stcum[5],
                      sixths = stcum[6], n = 25, seed = 1234)

# Plot cdf with cumulative probability calculated up to delta = 5
plot_cdf(c = con1$constants, method = "Polynomial",
         title = "Invalid Logistic CDF", calc_cprob = TRUE, delta = 5)

# Find constants with the sixth cumulant correction
# (valid power method pdf)
con2 &lt;- find_constants(method = "Polynomial", skews = stcum[3],
                      skurts = stcum[4], fifths = stcum[5],
                      sixths = stcum[6], Six = seq(1.5, 2, 0.05))

# Plot cdf with cumulative probability calculated up to delta = 5
plot_cdf(c = con2$constants, method = "Polynomial",
         title = "Valid Logistic CDF", calc_cprob = TRUE, delta = 5)

## End(Not run)

</code></pre>

<hr>
<h2 id='plot_pdf_ext'>Plot Theoretical Power Method Probability Density Function and Target PDF of External Data for Continuous Variables</h2><span id='topic+plot_pdf_ext'></span>

<h3>Description</h3>

<p>This plots the theoretical power method probability density function: </p>
<p style="text-align: center;"><code class="reqn">f_p(Z)(p(z)) = f_p(Z)(p(z), f_Z(z)/p'(z)),</code>
</p>
<p> as given in
Headrick &amp; Kowalchuk (2007, doi: <a href="http://doi.org/10.1080/10629360600605065">10.1080/10629360600605065</a>), and target
pdf.  It is a parametric plot with <code class="reqn">sigma * y + mu</code>, where <code class="reqn">y = p(z)</code>, on the x-axis and
<code class="reqn">f_Z(z)/p'(z)</code> on the y-axis, where <code class="reqn">z</code> is vector of <code class="reqn">n</code> random standard normal numbers (generated with a seed set
by user; length equal to length of external data vector).  <code>sigma</code> is the standard deviation and <code>mu</code> is the mean of the external data set.
Given a vector of polynomial transformation constants, the function generates <code class="reqn">sigma * y + mu</code> and calculates the theoretical
probabilities using <code class="reqn">f_p(Z)(p(z), f_Z(z)/p'(z))</code>.  The target distribution is also plotted given a vector
of external data.  This external data is required.  The <code class="reqn">y</code> values are centered and scaled to have the same mean and standard
deviation as the external data.  If the user wants to only plot the power method pdf,
<code><a href="#topic+plot_pdf_theory">plot_pdf_theory</a></code> should be used instead with <code>overlay = FALSE</code>.  It returns a <code><a href="ggplot2.html#topic+ggplot2-package">ggplot2-package</a></code> object so the user can modify as necessary.  The graph parameters (i.e. <code>title</code>,
<code>power_color</code>, <code>target_color</code>, <code>nbins</code>) are <code><a href="ggplot2.html#topic+ggplot2-package">ggplot2-package</a></code> parameters.  It works for valid or invalid power method pdfs.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_pdf_ext(c = NULL, method = c("Fleishman", "Polynomial"),
  title = "Probability Density Function", ylower = NULL, yupper = NULL,
  power_color = "dark blue", ext_y = NULL, target_color = "dark green",
  target_lty = 2, seed = 1234, legend.position = c(0.975, 0.9),
  legend.justification = c(1, 1), legend.text.size = 10,
  title.text.size = 15, axis.text.size = 10, axis.title.size = 13)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_pdf_ext_+3A_c">c</code></td>
<td>
<p>a vector of constants c0, c1, c2, c3 (if <code>method</code> = &quot;Fleishman&quot;) or c0, c1, c2, c3, c4, c5 (if <code>method</code> =
&quot;Polynomial&quot;), like that returned by <code><a href="#topic+find_constants">find_constants</a></code></p>
</td></tr>
<tr><td><code id="plot_pdf_ext_+3A_method">method</code></td>
<td>
<p>the method used to generate the continuous variable y = p(z).  &quot;Fleishman&quot; uses Fleishman's third-order polynomial
transformation and &quot;Polynomial&quot; uses Headrick's fifth-order transformation.</p>
</td></tr>
<tr><td><code id="plot_pdf_ext_+3A_title">title</code></td>
<td>
<p>the title for the graph (default = &quot;Probability Density Function&quot;)</p>
</td></tr>
<tr><td><code id="plot_pdf_ext_+3A_ylower">ylower</code></td>
<td>
<p>the lower y value to use in the plot (default = NULL, uses minimum simulated y value)</p>
</td></tr>
<tr><td><code id="plot_pdf_ext_+3A_yupper">yupper</code></td>
<td>
<p>the upper y value (default = NULL, uses maximum simulated y value)</p>
</td></tr>
<tr><td><code id="plot_pdf_ext_+3A_power_color">power_color</code></td>
<td>
<p>the line color for the power method pdf (default = &quot;dark blue&quot;)</p>
</td></tr>
<tr><td><code id="plot_pdf_ext_+3A_ext_y">ext_y</code></td>
<td>
<p>a vector of external data (required)</p>
</td></tr>
<tr><td><code id="plot_pdf_ext_+3A_target_color">target_color</code></td>
<td>
<p>the histogram color for the target pdf (default = &quot;dark green&quot;)</p>
</td></tr>
<tr><td><code id="plot_pdf_ext_+3A_target_lty">target_lty</code></td>
<td>
<p>the line type for the target pdf (default = 2, dashed line)</p>
</td></tr>
<tr><td><code id="plot_pdf_ext_+3A_seed">seed</code></td>
<td>
<p>the seed value for random number generation (default = 1234)</p>
</td></tr>
<tr><td><code id="plot_pdf_ext_+3A_legend.position">legend.position</code></td>
<td>
<p>the position of the legend</p>
</td></tr>
<tr><td><code id="plot_pdf_ext_+3A_legend.justification">legend.justification</code></td>
<td>
<p>the justification of the legend</p>
</td></tr>
<tr><td><code id="plot_pdf_ext_+3A_legend.text.size">legend.text.size</code></td>
<td>
<p>the size of the legend labels</p>
</td></tr>
<tr><td><code id="plot_pdf_ext_+3A_title.text.size">title.text.size</code></td>
<td>
<p>the size of the plot title</p>
</td></tr>
<tr><td><code id="plot_pdf_ext_+3A_axis.text.size">axis.text.size</code></td>
<td>
<p>the size of the axes text (tick labels)</p>
</td></tr>
<tr><td><code id="plot_pdf_ext_+3A_axis.title.size">axis.title.size</code></td>
<td>
<p>the size of the axes titles</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code><a href="ggplot2.html#topic+ggplot2-package">ggplot2-package</a></code> object.
</p>


<h3>References</h3>

<p>Please see the references for <code><a href="#topic+plot_cdf">plot_cdf</a></code>.
</p>
<p>Wickham H. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York, 2009.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+find_constants">find_constants</a></code>, <code><a href="#topic+calc_theory">calc_theory</a></code>,
<code><a href="ggplot2.html#topic+ggplot2-package">ggplot2-package</a></code>, <code><a href="ggplot2.html#topic+geom_path">geom_path</a></code>, <code><a href="ggplot2.html#topic+geom_density">geom_density</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Logistic Distribution

seed = 1234

# Simulate "external" data set
set.seed(seed)
ext_y &lt;- rlogis(10000)

# Find standardized cumulants
stcum &lt;- calc_theory(Dist = "Logistic", params = c(0, 1))

# Find constants without the sixth cumulant correction
# (invalid power method pdf)
con1 &lt;- find_constants(method = "Polynomial", skews = stcum[3],
                      skurts = stcum[4], fifths = stcum[5],
                      sixths = stcum[6])

# Plot invalid power method pdf with external data
plot_pdf_ext(c = con1$constants, method = "Polynomial",
             title = "Invalid Logistic PDF", ext_y = ext_y,
             seed = seed)

# Find constants with the sixth cumulant correction
# (valid power method pdf)
con2 &lt;- find_constants(method = "Polynomial", skews = stcum[3],
                      skurts = stcum[4], fifths = stcum[5],
                      sixths = stcum[6], Six = seq(1.5, 2, 0.05))

# Plot invalid power method pdf with external data
plot_pdf_ext(c = con2$constants, method = "Polynomial",
             title = "Valid Logistic PDF", ext_y = ext_y,
             seed = seed)

## End(Not run)

</code></pre>

<hr>
<h2 id='plot_pdf_theory'>Plot Theoretical Power Method Probability Density Function and Target PDF by Distribution Name or Function for Continuous Variables</h2><span id='topic+plot_pdf_theory'></span>

<h3>Description</h3>

<p>This plots the theoretical power method probability density function: </p>
<p style="text-align: center;"><code class="reqn">f_p(Z)(p(z)) = f_p(Z)(p(z), f_Z(z)/p'(z)),</code>
</p>
<p> as given
in Headrick &amp; Kowalchuk (2007, doi: <a href="http://doi.org/10.1080/10629360600605065">10.1080/10629360600605065</a>), and target
pdf (if overlay = TRUE).  It is a parametric plot with <code class="reqn">sigma * y + mu</code>, where <code class="reqn">y = p(z)</code>, on the x-axis and
<code class="reqn">f_Z(z)/p'(z)</code> on the y-axis, where <code class="reqn">z</code> is vector of <code class="reqn">n</code> random standard normal numbers (generated with a seed set by
user).  Given a vector of polynomial
transformation constants, the function generates <code class="reqn">sigma * y + mu</code> and calculates the theoretical probabilities
using <code class="reqn">f_p(Z)(p(z), f_Z(z)/p'(z))</code>.  If <code>overlay</code> = TRUE, the target distribution is also plotted given either a
distribution name (plus up to 4 parameters) or a pdf function <code class="reqn">fx</code>.  If a target distribution is specified, <code class="reqn">y</code> is
scaled and then transformed so that it has the same mean and variance as the target distribution.
It returns a <code><a href="ggplot2.html#topic+ggplot2-package">ggplot2-package</a></code> object so the user can modify as necessary.  The graph parameters
(i.e. <code>title</code>, <code>power_color</code>, <code>target_color</code>, <code>target_lty</code>) are <code><a href="ggplot2.html#topic+ggplot2-package">ggplot2-package</a></code> parameters.
It works for valid or invalid power method pdfs.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_pdf_theory(c = NULL, method = c("Fleishman", "Polynomial"), mu = 0,
  sigma = 1, title = "Probability Density Function", ylower = NULL,
  yupper = NULL, power_color = "dark blue", overlay = TRUE,
  target_color = "dark green", target_lty = 2, Dist = c("Benini", "Beta",
  "Beta-Normal", "Birnbaum-Saunders", "Chisq", "Dagum", "Exponential",
  "Exp-Geometric", "Exp-Logarithmic", "Exp-Poisson", "F", "Fisk", "Frechet",
  "Gamma", "Gaussian", "Gompertz", "Gumbel", "Kumaraswamy", "Laplace",
  "Lindley", "Logistic", "Loggamma", "Lognormal", "Lomax", "Makeham", "Maxwell",
  "Nakagami", "Paralogistic", "Pareto", "Perks", "Rayleigh", "Rice",
  "Singh-Maddala", "Skewnormal", "t", "Topp-Leone", "Triangular", "Uniform",
  "Weibull"), params = NULL, fx = NULL, lower = NULL, upper = NULL,
  n = 100, seed = 1234, legend.position = c(0.975, 0.9),
  legend.justification = c(1, 1), legend.text.size = 10,
  title.text.size = 15, axis.text.size = 10, axis.title.size = 13)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_pdf_theory_+3A_c">c</code></td>
<td>
<p>a vector of constants c0, c1, c2, c3 (if <code>method</code> = &quot;Fleishman&quot;) or c0, c1, c2, c3, c4, c5 (if <code>method</code> =
&quot;Polynomial&quot;), like that returned by <code><a href="#topic+find_constants">find_constants</a></code></p>
</td></tr>
<tr><td><code id="plot_pdf_theory_+3A_method">method</code></td>
<td>
<p>the method used to generate the continuous variable <code class="reqn">y = p(z)</code>.  &quot;Fleishman&quot; uses Fleishman's third-order polynomial
transformation and &quot;Polynomial&quot; uses Headrick's fifth-order transformation.</p>
</td></tr>
<tr><td><code id="plot_pdf_theory_+3A_mu">mu</code></td>
<td>
<p>the desired mean for the continuous variable (used if <code>overlay = FALSE</code>, otherwise variable centered to have the
same mean as the target distribution)</p>
</td></tr>
<tr><td><code id="plot_pdf_theory_+3A_sigma">sigma</code></td>
<td>
<p>the desired standard deviation for the continuous variable (used if <code>overlay = FALSE</code>, otherwise variable scaled
to have the same standard deviation as the target distribution)</p>
</td></tr>
<tr><td><code id="plot_pdf_theory_+3A_title">title</code></td>
<td>
<p>the title for the graph (default = &quot;Probability Density Function&quot;)</p>
</td></tr>
<tr><td><code id="plot_pdf_theory_+3A_ylower">ylower</code></td>
<td>
<p>the lower y value to use in the plot (default = NULL, uses minimum simulated y value)</p>
</td></tr>
<tr><td><code id="plot_pdf_theory_+3A_yupper">yupper</code></td>
<td>
<p>the upper y value (default = NULL, uses maximum simulated y value)</p>
</td></tr>
<tr><td><code id="plot_pdf_theory_+3A_power_color">power_color</code></td>
<td>
<p>the line color for the power method pdf (default = &quot;dark blue)</p>
</td></tr>
<tr><td><code id="plot_pdf_theory_+3A_overlay">overlay</code></td>
<td>
<p>if TRUE (default), the target distribution is also plotted given either a distribution name (and parameters)
or pdf function fx (with bounds = ylower, yupper)</p>
</td></tr>
<tr><td><code id="plot_pdf_theory_+3A_target_color">target_color</code></td>
<td>
<p>the line color for the target pdf (default = &quot;dark green&quot;)</p>
</td></tr>
<tr><td><code id="plot_pdf_theory_+3A_target_lty">target_lty</code></td>
<td>
<p>the line type for the target pdf (default = 2, dashed line)</p>
</td></tr>
<tr><td><code id="plot_pdf_theory_+3A_dist">Dist</code></td>
<td>
<p>name of the distribution. The possible values are: &quot;Benini&quot;, &quot;Beta&quot;, &quot;Beta-Normal&quot;, &quot;Birnbaum-Saunders&quot;, &quot;Chisq&quot;,
&quot;Exponential&quot;, &quot;Exp-Geometric&quot;, &quot;Exp-Logarithmic&quot;, &quot;Exp-Poisson&quot;, &quot;F&quot;, &quot;Fisk&quot;, &quot;Frechet&quot;, &quot;Gamma&quot;, &quot;Gaussian&quot;, &quot;Gompertz&quot;,
&quot;Gumbel&quot;, &quot;Kumaraswamy&quot;, &quot;Laplace&quot;, &quot;Lindley&quot;, &quot;Logistic&quot;, &quot;Loggamma&quot;, &quot;Lognormal&quot;, &quot;Lomax&quot;, &quot;Makeham&quot;, &quot;Maxwell&quot;,
&quot;Nakagami&quot;, &quot;Paralogistic&quot;, &quot;Pareto&quot;, &quot;Perks&quot;, &quot;Rayleigh&quot;, &quot;Rice&quot;, &quot;Singh-Maddala&quot;, &quot;Skewnormal&quot;, &quot;t&quot;, &quot;Topp-Leone&quot;, &quot;Triangular&quot;,
&quot;Uniform&quot;, &quot;Weibull&quot;.
Please refer to the documentation for each package (either <code><a href="stats.html#topic+stats-package">stats-package</a></code>, <code><a href="VGAM.html#topic+VGAM-package">VGAM-package</a></code>, or
<code><a href="triangle.html#topic+triangle">triangle</a></code>) for information on appropriate parameter inputs.</p>
</td></tr>
<tr><td><code id="plot_pdf_theory_+3A_params">params</code></td>
<td>
<p>a vector of parameters (up to 4) for the desired distribution (keep NULL if <code>fx</code> supplied instead)</p>
</td></tr>
<tr><td><code id="plot_pdf_theory_+3A_fx">fx</code></td>
<td>
<p>a pdf input as a function of x only, i.e. fx &lt;- function(x) 0.5*(x-1)^2; must return a scalar
(keep NULL if <code>Dist</code> supplied instead)</p>
</td></tr>
<tr><td><code id="plot_pdf_theory_+3A_lower">lower</code></td>
<td>
<p>the lower support bound for <code>fx</code></p>
</td></tr>
<tr><td><code id="plot_pdf_theory_+3A_upper">upper</code></td>
<td>
<p>the upper support bound for <code>fx</code></p>
</td></tr>
<tr><td><code id="plot_pdf_theory_+3A_n">n</code></td>
<td>
<p>the number of random standard normal numbers to use in generating <code class="reqn">y = p(z)</code> (default = 100)</p>
</td></tr>
<tr><td><code id="plot_pdf_theory_+3A_seed">seed</code></td>
<td>
<p>the seed value for random number generation (default = 1234)</p>
</td></tr>
<tr><td><code id="plot_pdf_theory_+3A_legend.position">legend.position</code></td>
<td>
<p>the position of the legend</p>
</td></tr>
<tr><td><code id="plot_pdf_theory_+3A_legend.justification">legend.justification</code></td>
<td>
<p>the justification of the legend</p>
</td></tr>
<tr><td><code id="plot_pdf_theory_+3A_legend.text.size">legend.text.size</code></td>
<td>
<p>the size of the legend labels</p>
</td></tr>
<tr><td><code id="plot_pdf_theory_+3A_title.text.size">title.text.size</code></td>
<td>
<p>the size of the plot title</p>
</td></tr>
<tr><td><code id="plot_pdf_theory_+3A_axis.text.size">axis.text.size</code></td>
<td>
<p>the size of the axes text (tick labels)</p>
</td></tr>
<tr><td><code id="plot_pdf_theory_+3A_axis.title.size">axis.title.size</code></td>
<td>
<p>the size of the axes titles</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code><a href="ggplot2.html#topic+ggplot2-package">ggplot2-package</a></code> object.
</p>


<h3>References</h3>

<p>Please see the references for <code><a href="#topic+plot_cdf">plot_cdf</a></code>.
</p>
<p>Wickham H. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York, 2009.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+find_constants">find_constants</a></code>, <code><a href="#topic+calc_theory">calc_theory</a></code>,
<code><a href="ggplot2.html#topic+ggplot2-package">ggplot2-package</a></code>, <code><a href="ggplot2.html#topic+geom_path">geom_path</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Logistic Distribution

# Find standardized cumulants
stcum &lt;- calc_theory(Dist = "Logistic", params = c(0, 1))

# Find constants without the sixth cumulant correction
# (invalid power method pdf)
con1 &lt;- find_constants(method = "Polynomial", skews = stcum[3],
                      skurts = stcum[4], fifths = stcum[5],
                      sixths = stcum[6])

# Plot invalid power method pdf with theoretical pdf overlayed
plot_pdf_theory(c = con1$constants, method = "Polynomial",
         title = "Invalid Logistic PDF", overlay = TRUE,
         Dist = "Logistic", params = c(0, 1))

# Find constants with the sixth cumulant correction
# (valid power method pdf)
con2 &lt;- find_constants(method = "Polynomial", skews = stcum[3],
                      skurts = stcum[4], fifths = stcum[5],
                      sixths = stcum[6], Six = seq(1.5, 2, 0.05))

# Plot valid power method pdf with theoretical pdf overlayed
plot_pdf_theory(c = con2$constants, method = "Polynomial",
         title = "Valid Logistic PDF", overlay = TRUE,
         Dist = "Logistic", params = c(0, 1))

## End(Not run)

</code></pre>

<hr>
<h2 id='plot_sim_cdf'>Plot Simulated (Empirical) Cumulative Distribution Function for Continuous, Ordinal, or Count Variables</h2><span id='topic+plot_sim_cdf'></span>

<h3>Description</h3>

<p>This plots the cumulative distribution function of simulated continuous, ordinal, or count data using the empirical cdf
<code class="reqn">Fn</code> (see <code><a href="ggplot2.html#topic+stat_ecdf">stat_ecdf</a></code>).
<code class="reqn">Fn</code> is a step function with jumps <code class="reqn">i/n</code> at observation values, where <code class="reqn">i</code> is the number of tied observations at that
value. Missing values are
ignored.  For observations <code class="reqn">y = (y1, y2, ..., yn)</code>, <code class="reqn">Fn</code> is the fraction of observations less or equal to <code class="reqn">t</code>, i.e.,
<code class="reqn">Fn(t) = sum[yi &lt;= t]/n</code>.  If <code>calc_cprob</code> = TRUE and the variable is <em>continuous</em>, the cumulative probability up to
<code class="reqn">y = delta</code> is calculated (see <code><a href="#topic+sim_cdf_prob">sim_cdf_prob</a></code>) and the region on the plot is filled with a
dashed horizontal line drawn at Fn(delta).  The cumulative probability is stated on top of the line.
This fill option does not work for ordinal or count variables.  The function returns a
<code><a href="ggplot2.html#topic+ggplot2-package">ggplot2-package</a></code> object so the user can modify as necessary.
The graph parameters (i.e. <code>title</code>, <code>color</code>, <code>fill</code>, <code>hline</code>) are <code><a href="ggplot2.html#topic+ggplot2-package">ggplot2-package</a></code> parameters.
It works for valid or invalid power method pdfs.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_sim_cdf(sim_y, title = "Empirical Cumulative Distribution Function",
  ylower = NULL, yupper = NULL, calc_cprob = FALSE, delta = 5,
  color = "dark blue", fill = "blue", hline = "dark green",
  text.size = 11, title.text.size = 15, axis.text.size = 10,
  axis.title.size = 13)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_sim_cdf_+3A_sim_y">sim_y</code></td>
<td>
<p>a vector of simulated data</p>
</td></tr>
<tr><td><code id="plot_sim_cdf_+3A_title">title</code></td>
<td>
<p>the title for the graph (default = &quot;Empirical Cumulative Distribution Function&quot;)</p>
</td></tr>
<tr><td><code id="plot_sim_cdf_+3A_ylower">ylower</code></td>
<td>
<p>the lower y value to use in the plot (default = NULL, uses minimum simulated y value)</p>
</td></tr>
<tr><td><code id="plot_sim_cdf_+3A_yupper">yupper</code></td>
<td>
<p>the upper y value (default = NULL, uses maximum simulated y value)</p>
</td></tr>
<tr><td><code id="plot_sim_cdf_+3A_calc_cprob">calc_cprob</code></td>
<td>
<p>if TRUE (default = FALSE) and <code>sim_y</code> is continuous, <code><a href="#topic+sim_cdf_prob">sim_cdf_prob</a></code> is used to find the empirical cumulative probability
up to y = delta and the region on the plot is filled with a dashed horizontal line drawn at <code class="reqn">Fn(delta)</code></p>
</td></tr>
<tr><td><code id="plot_sim_cdf_+3A_delta">delta</code></td>
<td>
<p>the value y at which to evaluate the cumulative probability (default = 5)</p>
</td></tr>
<tr><td><code id="plot_sim_cdf_+3A_color">color</code></td>
<td>
<p>the line color for the cdf (default = &quot;dark blue&quot;)</p>
</td></tr>
<tr><td><code id="plot_sim_cdf_+3A_fill">fill</code></td>
<td>
<p>the fill color if <code>calc_cprob</code> = TRUE (default = &quot;blue)</p>
</td></tr>
<tr><td><code id="plot_sim_cdf_+3A_hline">hline</code></td>
<td>
<p>the dashed horizontal line color drawn at <code>delta</code> if <code>calc_cprob</code> = TRUE (default = &quot;dark green&quot;)</p>
</td></tr>
<tr><td><code id="plot_sim_cdf_+3A_text.size">text.size</code></td>
<td>
<p>the size of the text displaying the cumulative probability up to <code>delta</code> if <code>calc_cprob</code> = TRUE</p>
</td></tr>
<tr><td><code id="plot_sim_cdf_+3A_title.text.size">title.text.size</code></td>
<td>
<p>the size of the plot title</p>
</td></tr>
<tr><td><code id="plot_sim_cdf_+3A_axis.text.size">axis.text.size</code></td>
<td>
<p>the size of the axes text (tick labels)</p>
</td></tr>
<tr><td><code id="plot_sim_cdf_+3A_axis.title.size">axis.title.size</code></td>
<td>
<p>the size of the axes titles</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code><a href="ggplot2.html#topic+ggplot2-package">ggplot2-package</a></code> object.
</p>


<h3>References</h3>

<p>Please see the references for <code><a href="#topic+plot_cdf">plot_cdf</a></code>.
</p>
<p>Wickham H. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York, 2009.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+ecdf">ecdf</a></code>, <code><a href="#topic+sim_cdf_prob">sim_cdf_prob</a></code>, <code><a href="ggplot2.html#topic+ggplot2-package">ggplot2-package</a></code>,
<code><a href="ggplot2.html#topic+stat_ecdf">stat_ecdf</a></code>, <code><a href="ggplot2.html#topic+geom_abline">geom_abline</a></code>, <code><a href="ggplot2.html#topic+geom_ribbon">geom_ribbon</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Logistic Distribution: mean = 0, variance = 1
seed = 1234

# Find standardized cumulants
stcum &lt;- calc_theory(Dist = "Logistic", params = c(0, 1))

# Simulate without the sixth cumulant correction
# (invalid power method pdf)
Logvar1 &lt;- nonnormvar1(method = "Polynomial", means = 0, vars = 1,
                      skews = stcum[3], skurts = stcum[4],
                      fifths = stcum[5], sixths = stcum[6], seed = seed)

# Plot cdf with cumulative probability calculated up to delta = 5
plot_sim_cdf(sim_y = Logvar1$continuous_variable,
             title = "Invalid Logistic Empirical CDF",
             calc_cprob = TRUE, delta = 5)

# Simulate with the sixth cumulant correction
# (valid power method pdf)
Logvar2 &lt;- nonnormvar1(method = "Polynomial", means = 0, vars = 1,
                      skews = stcum[3], skurts = stcum[4],
                      fifths = stcum[5], sixths = stcum[6],
                      Six = seq(1.5, 2, 0.05), seed = seed)

# Plot cdf with cumulative probability calculated up to delta = 5
plot_sim_cdf(sim_y = Logvar2$continuous_variable,
             title = "Valid Logistic Empirical CDF",
             calc_cprob = TRUE, delta = 5)

# Simulate one binary and one ordinal variable (4 categories) with
# correlation 0.3
Ordvars = rcorrvar(k_cat = 2, marginal = list(0.4, c(0.2, 0.5, 0.7)),
                   rho = matrix(c(1, 0.3, 0.3, 1), 2, 2), seed = seed)

# Plot cdf of 2nd variable
plot_sim_cdf(Ordvars$ordinal_variables[, 2])


## End(Not run)

</code></pre>

<hr>
<h2 id='plot_sim_ext'>Plot Simulated Data and Target External Data for Continuous or Count Variables</h2><span id='topic+plot_sim_ext'></span>

<h3>Description</h3>

<p>This plots simulated continuous or count data and overlays external data, both as histograms.
The external data is a required input.  The simulated data is centered and scaled to have the same mean and variance as the external
data set.  If the user wants to only plot simulated data, <code><a href="#topic+plot_sim_theory">plot_sim_theory</a></code> should be used instead
with <code>overlay = FALSE</code>.
It returns a <code><a href="ggplot2.html#topic+ggplot2-package">ggplot2-package</a></code> object so the user can modify as necessary.
The graph parameters (i.e. <code>title</code>, <code>power_color</code>, <code>target_color</code>, <code>nbins</code>) are
<code><a href="ggplot2.html#topic+ggplot2-package">ggplot2-package</a></code> parameters.  It works for valid or invalid power method pdfs.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_sim_ext(sim_y, title = "Simulated Data Values", ylower = NULL,
  yupper = NULL, power_color = "dark blue", ext_y = NULL,
  target_color = "dark green", nbins = 100, legend.position = c(0.975,
  0.9), legend.justification = c(1, 1), legend.text.size = 10,
  title.text.size = 15, axis.text.size = 10, axis.title.size = 13)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_sim_ext_+3A_sim_y">sim_y</code></td>
<td>
<p>a vector of simulated data</p>
</td></tr>
<tr><td><code id="plot_sim_ext_+3A_title">title</code></td>
<td>
<p>the title for the graph (default = &quot;Simulated Data Values&quot;)</p>
</td></tr>
<tr><td><code id="plot_sim_ext_+3A_ylower">ylower</code></td>
<td>
<p>the lower y value to use in the plot (default = NULL, uses minimum simulated y value)</p>
</td></tr>
<tr><td><code id="plot_sim_ext_+3A_yupper">yupper</code></td>
<td>
<p>the upper y value (default = NULL, uses maximum simulated y value)</p>
</td></tr>
<tr><td><code id="plot_sim_ext_+3A_power_color">power_color</code></td>
<td>
<p>the histogram fill color for the simulated variable (default = &quot;dark blue&quot;)</p>
</td></tr>
<tr><td><code id="plot_sim_ext_+3A_ext_y">ext_y</code></td>
<td>
<p>a vector of external data (required)</p>
</td></tr>
<tr><td><code id="plot_sim_ext_+3A_target_color">target_color</code></td>
<td>
<p>the histogram fill color for the target data (default = &quot;dark green&quot;)</p>
</td></tr>
<tr><td><code id="plot_sim_ext_+3A_nbins">nbins</code></td>
<td>
<p>the number of bins to use in generating the histograms (default = 100)</p>
</td></tr>
<tr><td><code id="plot_sim_ext_+3A_legend.position">legend.position</code></td>
<td>
<p>the position of the legend</p>
</td></tr>
<tr><td><code id="plot_sim_ext_+3A_legend.justification">legend.justification</code></td>
<td>
<p>the justification of the legend</p>
</td></tr>
<tr><td><code id="plot_sim_ext_+3A_legend.text.size">legend.text.size</code></td>
<td>
<p>the size of the legend labels</p>
</td></tr>
<tr><td><code id="plot_sim_ext_+3A_title.text.size">title.text.size</code></td>
<td>
<p>the size of the plot title</p>
</td></tr>
<tr><td><code id="plot_sim_ext_+3A_axis.text.size">axis.text.size</code></td>
<td>
<p>the size of the axes text (tick labels)</p>
</td></tr>
<tr><td><code id="plot_sim_ext_+3A_axis.title.size">axis.title.size</code></td>
<td>
<p>the size of the axes titles</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code><a href="ggplot2.html#topic+ggplot2-package">ggplot2-package</a></code> object.
</p>


<h3>References</h3>

<p>Please see the references for <code><a href="#topic+plot_cdf">plot_cdf</a></code>.
</p>
<p>Wickham H. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York, 2009.
</p>


<h3>See Also</h3>

<p><code><a href="ggplot2.html#topic+ggplot2-package">ggplot2-package</a></code>, <code><a href="ggplot2.html#topic+geom_histogram">geom_histogram</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Logistic Distribution: mean = 0, variance = 1

seed = 1234

# Simulate "external" data set
set.seed(seed)
ext_y &lt;- rlogis(10000)

# Find standardized cumulants
stcum &lt;- calc_theory(Dist = "Logistic", params = c(0, 1))

# Simulate without the sixth cumulant correction
# (invalid power method pdf)
Logvar1 &lt;- nonnormvar1(method = "Polynomial", means = 0, vars = 1,
                      skews = stcum[3], skurts = stcum[4],
                      fifths = stcum[5], sixths = stcum[6],
                      n = 10000, seed = seed)

# Plot simulated variable and external data
plot_sim_ext(sim_y = Logvar1$continuous_variable,
             title = "Invalid Logistic Simulated Data Values",
             ext_y = ext_y)

# Simulate with the sixth cumulant correction
# (valid power method pdf)
Logvar2 &lt;- nonnormvar1(method = "Polynomial", means = 0, vars = 1,
                      skews = stcum[3], skurts = stcum[4],
                      fifths = stcum[5], sixths = stcum[6],
                      Six = seq(1.5, 2, 0.05), n = 10000, seed = seed)

# Plot simulated variable and external data
plot_sim_ext(sim_y = Logvar2$continuous_variable,
             title = "Valid Logistic Simulated Data Values",
             ext_y = ext_y)

# Simulate 2 Poisson distributions (means = 10, 15) and correlation 0.3
# using Method 1
Pvars &lt;- rcorrvar(k_pois = 2, lam = c(10, 15),
                  rho = matrix(c(1, 0.3, 0.3, 1), 2, 2), seed = seed)

# Simulate "external" data set
set.seed(seed)
ext_y &lt;- rpois(10000, 10)

# Plot 1st simulated variable and external data
plot_sim_ext(sim_y = Pvars$Poisson_variable[, 1], ext_y = ext_y)


## End(Not run)

</code></pre>

<hr>
<h2 id='plot_sim_pdf_ext'>Plot Simulated Probability Density Function and Target PDF of External Data for Continuous or Count Variables</h2><span id='topic+plot_sim_pdf_ext'></span>

<h3>Description</h3>

<p>This plots the pdf of simulated continuous or count data and overlays the target pdf computed from the
given external data vector.  The external data is a required input.  The simulated data is centered and scaled to have the same
mean and variance as the external data set.  If the user wants to only plot simulated data,
<code><a href="#topic+plot_sim_theory">plot_sim_theory</a></code> should be used instead (with <code>overlay = FALSE</code>).
It returns a <code><a href="ggplot2.html#topic+ggplot2-package">ggplot2-package</a></code> object so the user can modify as necessary.  The graph parameters
(i.e. <code>title</code>, <code>power_color</code>, <code>target_color</code>, <code>target_lty</code>) are <code><a href="ggplot2.html#topic+ggplot2-package">ggplot2-package</a></code> parameters.
It works for valid or invalid power method pdfs.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_sim_pdf_ext(sim_y, title = "Simulated Probability Density Function",
  ylower = NULL, yupper = NULL, power_color = "dark blue", ext_y = NULL,
  target_color = "dark green", target_lty = 2, legend.position = c(0.975,
  0.9), legend.justification = c(1, 1), legend.text.size = 10,
  title.text.size = 15, axis.text.size = 10, axis.title.size = 13)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_sim_pdf_ext_+3A_sim_y">sim_y</code></td>
<td>
<p>a vector of simulated data</p>
</td></tr>
<tr><td><code id="plot_sim_pdf_ext_+3A_title">title</code></td>
<td>
<p>the title for the graph (default = &quot;Simulated Probability Density Function&quot;)</p>
</td></tr>
<tr><td><code id="plot_sim_pdf_ext_+3A_ylower">ylower</code></td>
<td>
<p>the lower y value to use in the plot (default = NULL, uses minimum simulated y value)</p>
</td></tr>
<tr><td><code id="plot_sim_pdf_ext_+3A_yupper">yupper</code></td>
<td>
<p>the upper y value (default = NULL, uses maximum simulated y value)</p>
</td></tr>
<tr><td><code id="plot_sim_pdf_ext_+3A_power_color">power_color</code></td>
<td>
<p>the histogram color for the simulated variable (default = &quot;dark blue&quot;)</p>
</td></tr>
<tr><td><code id="plot_sim_pdf_ext_+3A_ext_y">ext_y</code></td>
<td>
<p>a vector of external data (required)</p>
</td></tr>
<tr><td><code id="plot_sim_pdf_ext_+3A_target_color">target_color</code></td>
<td>
<p>the histogram color for the target pdf (default = &quot;dark green&quot;)</p>
</td></tr>
<tr><td><code id="plot_sim_pdf_ext_+3A_target_lty">target_lty</code></td>
<td>
<p>the line type for the target pdf (default = 2, dashed line)</p>
</td></tr>
<tr><td><code id="plot_sim_pdf_ext_+3A_legend.position">legend.position</code></td>
<td>
<p>the position of the legend</p>
</td></tr>
<tr><td><code id="plot_sim_pdf_ext_+3A_legend.justification">legend.justification</code></td>
<td>
<p>the justification of the legend</p>
</td></tr>
<tr><td><code id="plot_sim_pdf_ext_+3A_legend.text.size">legend.text.size</code></td>
<td>
<p>the size of the legend labels</p>
</td></tr>
<tr><td><code id="plot_sim_pdf_ext_+3A_title.text.size">title.text.size</code></td>
<td>
<p>the size of the plot title</p>
</td></tr>
<tr><td><code id="plot_sim_pdf_ext_+3A_axis.text.size">axis.text.size</code></td>
<td>
<p>the size of the axes text (tick labels)</p>
</td></tr>
<tr><td><code id="plot_sim_pdf_ext_+3A_axis.title.size">axis.title.size</code></td>
<td>
<p>the size of the axes titles</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code><a href="ggplot2.html#topic+ggplot2-package">ggplot2-package</a></code> object.
</p>


<h3>References</h3>

<p>Please see the references for <code><a href="#topic+plot_cdf">plot_cdf</a></code>.
</p>
<p>Wickham H. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York, 2009.
</p>


<h3>See Also</h3>

<p><code><a href="ggplot2.html#topic+ggplot2-package">ggplot2-package</a></code>, <code><a href="ggplot2.html#topic+geom_density">geom_density</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Logistic Distribution: mean = 0, variance = 1

seed = 1234

# Simulate "external" data set
set.seed(seed)
ext_y &lt;- rlogis(10000)

# Find standardized cumulants
stcum &lt;- calc_theory(Dist = "Logistic", params = c(0, 1))

# Simulate without the sixth cumulant correction
# (invalid power method pdf)
Logvar1 &lt;- nonnormvar1(method = "Polynomial", means = 0, vars = 1,
                      skews = stcum[3], skurts = stcum[4],
                      fifths = stcum[5], sixths = stcum[6],
                      n = 10000, seed = seed)

# Plot pdfs of simulated variable (invalid) and external data
plot_sim_pdf_ext(sim_y = Logvar1$continuous_variable,
                 title = "Invalid Logistic Simulated PDF", ext_y = ext_y)

# Simulate with the sixth cumulant correction
# (valid power method pdf)
Logvar2 &lt;- nonnormvar1(method = "Polynomial", means = 0, vars = 1,
                      skews = stcum[3], skurts = stcum[4],
                      fifths = stcum[5], sixths = stcum[6],
                      Six = seq(1.5, 2, 0.05), n = 10000, seed = 1234)

# Plot pdfs of simulated variable (valid) and external data
plot_sim_pdf_ext(sim_y = Logvar2$continuous_variable,
                 title = "Valid Logistic Simulated PDF", ext_y = ext_y)

# Simulate 2 Poisson distributions (means = 10, 15) and correlation 0.3
# using Method 1
Pvars &lt;- rcorrvar(k_pois = 2, lam = c(10, 15),
                  rho = matrix(c(1, 0.3, 0.3, 1), 2, 2), seed = seed)

# Simulate "external" data set
set.seed(seed)
ext_y &lt;- rpois(10000, 10)

# Plot pdfs of 1st simulated variable and external data
plot_sim_pdf_ext(sim_y = Pvars$Poisson_variable[, 1], ext_y = ext_y)


## End(Not run)

</code></pre>

<hr>
<h2 id='plot_sim_pdf_theory'>Plot Simulated Probability Density Function and Target PDF by Distribution Name or Function for Continuous or Count Variables</h2><span id='topic+plot_sim_pdf_theory'></span>

<h3>Description</h3>

<p>This plots the pdf of simulated continuous or count data and overlays the target pdf (if <code>overlay</code> = TRUE),
which is specified by distribution name (plus up to 4 parameters) or pdf function <code>fx</code> (plus support bounds).
If a continuous target distribution is provided (<code>cont_var = TRUE</code>), the simulated data <code class="reqn">y</code> is
scaled and then transformed (i.e. <code class="reqn">y = sigma * scale(y) + mu</code>) so that it has the same mean (<code class="reqn">mu</code>) and variance (<code class="reqn">sigma^2</code>) as the
target distribution.  If the variable is Negative Binomial, the parameters must be size and success probability (not mu).
The function returns a <code><a href="ggplot2.html#topic+ggplot2-package">ggplot2-package</a></code> object so the user can modify as necessary.  The graph parameters (i.e. <code>title</code>,
<code>power_color</code>, <code>target_color</code>, <code>target_lty</code>) are <code><a href="ggplot2.html#topic+ggplot2-package">ggplot2-package</a></code> parameters.  It works for valid or invalid power method pdfs.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_sim_pdf_theory(sim_y, title = "Simulated Probability Density Function",
  ylower = NULL, yupper = NULL, power_color = "dark blue",
  overlay = TRUE, cont_var = TRUE, target_color = "dark green",
  target_lty = 2, Dist = c("Benini", "Beta", "Beta-Normal",
  "Birnbaum-Saunders", "Chisq", "Dagum", "Exponential", "Exp-Geometric",
  "Exp-Logarithmic", "Exp-Poisson", "F", "Fisk", "Frechet", "Gamma", "Gaussian",
  "Gompertz", "Gumbel", "Kumaraswamy", "Laplace", "Lindley", "Logistic",
  "Loggamma", "Lognormal", "Lomax", "Makeham", "Maxwell", "Nakagami",
  "Paralogistic", "Pareto", "Perks", "Rayleigh", "Rice", "Singh-Maddala",
  "Skewnormal", "t", "Topp-Leone", "Triangular", "Uniform", "Weibull",
  "Poisson", "Negative_Binomial"), params = NULL, fx = NULL, lower = NULL,
  upper = NULL, legend.position = c(0.975, 0.9),
  legend.justification = c(1, 1), legend.text.size = 10,
  title.text.size = 15, axis.text.size = 10, axis.title.size = 13)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_sim_pdf_theory_+3A_sim_y">sim_y</code></td>
<td>
<p>a vector of simulated data</p>
</td></tr>
<tr><td><code id="plot_sim_pdf_theory_+3A_title">title</code></td>
<td>
<p>the title for the graph (default = &quot;Simulated Probability Density Function&quot;)</p>
</td></tr>
<tr><td><code id="plot_sim_pdf_theory_+3A_ylower">ylower</code></td>
<td>
<p>the lower y value to use in the plot (default = NULL, uses minimum simulated y value)</p>
</td></tr>
<tr><td><code id="plot_sim_pdf_theory_+3A_yupper">yupper</code></td>
<td>
<p>the upper y value (default = NULL, uses maximum simulated y value)</p>
</td></tr>
<tr><td><code id="plot_sim_pdf_theory_+3A_power_color">power_color</code></td>
<td>
<p>the line color for the simulated variable</p>
</td></tr>
<tr><td><code id="plot_sim_pdf_theory_+3A_overlay">overlay</code></td>
<td>
<p>if TRUE (default), the target distribution is also plotted given either a distribution name (and parameters)
or pdf function fx (with bounds = ylower, yupper)</p>
</td></tr>
<tr><td><code id="plot_sim_pdf_theory_+3A_cont_var">cont_var</code></td>
<td>
<p>TRUE (default) for continuous variables, FALSE for count variables</p>
</td></tr>
<tr><td><code id="plot_sim_pdf_theory_+3A_target_color">target_color</code></td>
<td>
<p>the line color for the target pdf</p>
</td></tr>
<tr><td><code id="plot_sim_pdf_theory_+3A_target_lty">target_lty</code></td>
<td>
<p>the line type for the target pdf (default = 2, dashed line)</p>
</td></tr>
<tr><td><code id="plot_sim_pdf_theory_+3A_dist">Dist</code></td>
<td>
<p>name of the distribution. The possible values are: &quot;Benini&quot;, &quot;Beta&quot;, &quot;Beta-Normal&quot;, &quot;Birnbaum-Saunders&quot;, &quot;Chisq&quot;,
&quot;Exponential&quot;, &quot;Exp-Geometric&quot;, &quot;Exp-Logarithmic&quot;, &quot;Exp-Poisson&quot;, &quot;F&quot;, &quot;Fisk&quot;, &quot;Frechet&quot;, &quot;Gamma&quot;, &quot;Gaussian&quot;, &quot;Gompertz&quot;,
&quot;Gumbel&quot;, &quot;Kumaraswamy&quot;, &quot;Laplace&quot;, &quot;Lindley&quot;, &quot;Logistic&quot;, &quot;Loggamma&quot;, &quot;Lognormal&quot;, &quot;Lomax&quot;, &quot;Makeham&quot;, &quot;Maxwell&quot;,
&quot;Nakagami&quot;, &quot;Paralogistic&quot;, &quot;Pareto&quot;, &quot;Perks&quot;, &quot;Rayleigh&quot;, &quot;Rice&quot;, &quot;Singh-Maddala&quot;, &quot;Skewnormal&quot;, &quot;t&quot;, &quot;Topp-Leone&quot;, &quot;Triangular&quot;,
&quot;Uniform&quot;, &quot;Weibull&quot;, &quot;Poisson&quot;, and &quot;Negative_Binomial&quot;.
Please refer to the documentation for each package (either <code><a href="stats.html#topic+stats-package">stats-package</a></code>, <code><a href="VGAM.html#topic+VGAM-package">VGAM-package</a></code>, or
<code><a href="triangle.html#topic+triangle">triangle</a></code>) for information on appropriate parameter inputs.</p>
</td></tr>
<tr><td><code id="plot_sim_pdf_theory_+3A_params">params</code></td>
<td>
<p>a vector of parameters (up to 4) for the desired distribution (keep NULL if <code>fx</code> supplied instead)</p>
</td></tr>
<tr><td><code id="plot_sim_pdf_theory_+3A_fx">fx</code></td>
<td>
<p>a pdf input as a function of x only, i.e. fx &lt;- function(x) 0.5*(x-1)^2; must return a scalar
(keep NULL if <code>Dist</code> supplied instead)</p>
</td></tr>
<tr><td><code id="plot_sim_pdf_theory_+3A_lower">lower</code></td>
<td>
<p>the lower support bound for <code>fx</code></p>
</td></tr>
<tr><td><code id="plot_sim_pdf_theory_+3A_upper">upper</code></td>
<td>
<p>the upper support bound for <code>fx</code></p>
</td></tr>
<tr><td><code id="plot_sim_pdf_theory_+3A_legend.position">legend.position</code></td>
<td>
<p>the position of the legend</p>
</td></tr>
<tr><td><code id="plot_sim_pdf_theory_+3A_legend.justification">legend.justification</code></td>
<td>
<p>the justification of the legend</p>
</td></tr>
<tr><td><code id="plot_sim_pdf_theory_+3A_legend.text.size">legend.text.size</code></td>
<td>
<p>the size of the legend labels</p>
</td></tr>
<tr><td><code id="plot_sim_pdf_theory_+3A_title.text.size">title.text.size</code></td>
<td>
<p>the size of the plot title</p>
</td></tr>
<tr><td><code id="plot_sim_pdf_theory_+3A_axis.text.size">axis.text.size</code></td>
<td>
<p>the size of the axes text (tick labels)</p>
</td></tr>
<tr><td><code id="plot_sim_pdf_theory_+3A_axis.title.size">axis.title.size</code></td>
<td>
<p>the size of the axes titles</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code><a href="ggplot2.html#topic+ggplot2-package">ggplot2-package</a></code> object.
</p>


<h3>References</h3>

<p>Please see the references for <code><a href="#topic+plot_cdf">plot_cdf</a></code>.
</p>
<p>Wickham H. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York, 2009.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+calc_theory">calc_theory</a></code>,
<code><a href="ggplot2.html#topic+ggplot2-package">ggplot2-package</a></code>, <code><a href="ggplot2.html#topic+geom_path">geom_path</a></code>, <code><a href="ggplot2.html#topic+geom_density">geom_density</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Logistic Distribution: mean = 0, variance = 1
seed = 1234

# Find standardized cumulants
stcum &lt;- calc_theory(Dist = "Logistic", params = c(0, 1))

# Simulate without the sixth cumulant correction
# (invalid power method pdf)
Logvar1 &lt;- nonnormvar1(method = "Polynomial", means = 0, vars = 1,
                       skews = stcum[3], skurts = stcum[4],
                       fifths = stcum[5], sixths = stcum[6],
                       n = 10000, seed = seed)

# Plot pdfs of simulated variable (invalid) and theoretical distribution
plot_sim_pdf_theory(sim_y = Logvar1$continuous_variable,
                    title = "Invalid Logistic Simulated PDF",
                    overlay = TRUE, Dist = "Logistic", params = c(0, 1))

# Simulate with the sixth cumulant correction
# (valid power method pdf)
Logvar2 &lt;- nonnormvar1(method = "Polynomial", means = 0, vars = 1,
                       skews = stcum[3], skurts = stcum[4],
                       fifths = stcum[5], sixths = stcum[6],
                       Six = seq(1.5, 2, 0.05), n = 10000, seed = seed)

# Plot pdfs of simulated variable (invalid) and theoretical distribution
plot_sim_pdf_theory(sim_y = Logvar2$continuous_variable,
                    title = "Valid Logistic Simulated PDF",
                    overlay = TRUE, Dist = "Logistic", params = c(0, 1))

# Simulate 2 Negative Binomial distributions and correlation 0.3
# using Method 1
NBvars &lt;- rcorrvar(k_nb = 2, size = c(10, 15), prob = c(0.4, 0.3),
                  rho = matrix(c(1, 0.3, 0.3, 1), 2, 2), seed = seed)

# Plot pdfs of 1st simulated variable and theoretical distribution
plot_sim_pdf_theory(sim_y = NBvars$Neg_Bin_variable[, 1], overlay = TRUE,
                    cont_var = FALSE, Dist = "Negative_Binomial",
                    params = c(10, 0.4))


## End(Not run)

</code></pre>

<hr>
<h2 id='plot_sim_theory'>Plot Simulated Data and Target Distribution Data by Name or Function for Continuous or Count Variables</h2><span id='topic+plot_sim_theory'></span>

<h3>Description</h3>

<p>This plots simulated continuous or count data and overlays data (if <code>overlay</code> = TRUE) generated from the target
distribution, which is specified by name (plus up to 4 parameters) or pdf function <code>fx</code> (plus support bounds).
Due to the integration involved in evaluating the cdf using <code>fx</code>, only continuous <code>fx</code> may be supplied.  Both are plotted
as histograms.  If a continuous target distribution is specified (<code>cont_var = TRUE</code>), the simulated data <code class="reqn">y</code> is
scaled and then transformed (i.e. <code class="reqn">y = sigma * scale(y) + mu</code>) so that it has the same mean (<code class="reqn">mu</code>) and variance (<code class="reqn">sigma^2</code>) as the
target distribution.  If the variable is Negative Binomial, the parameters must be size and success probability (not mu).
It returns a <code><a href="ggplot2.html#topic+ggplot2-package">ggplot2-package</a></code> object so the user can modify as necessary.
The graph parameters (i.e. <code>title</code>, <code>power_color</code>, <code>target_color</code>,
<code>target_lty</code>) are <code><a href="ggplot2.html#topic+ggplot2-package">ggplot2-package</a></code> parameters.  It works for valid or invalid power method pdfs.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_sim_theory(sim_y, title = "Simulated Data Values", ylower = NULL,
  yupper = NULL, power_color = "dark blue", overlay = TRUE,
  cont_var = TRUE, target_color = "dark green", nbins = 100,
  Dist = c("Benini", "Beta", "Beta-Normal", "Birnbaum-Saunders", "Chisq",
  "Dagum", "Exponential", "Exp-Geometric", "Exp-Logarithmic", "Exp-Poisson",
  "F", "Fisk", "Frechet", "Gamma", "Gaussian", "Gompertz", "Gumbel",
  "Kumaraswamy", "Laplace", "Lindley", "Logistic", "Loggamma", "Lognormal",
  "Lomax", "Makeham", "Maxwell", "Nakagami", "Paralogistic", "Pareto", "Perks",
  "Rayleigh", "Rice", "Singh-Maddala", "Skewnormal", "t", "Topp-Leone",
  "Triangular", "Uniform", "Weibull", "Poisson", "Negative_Binomial"),
  params = NULL, fx = NULL, lower = NULL, upper = NULL, seed = 1234,
  sub = 1000, legend.position = c(0.975, 0.9), legend.justification = c(1,
  1), legend.text.size = 10, title.text.size = 15, axis.text.size = 10,
  axis.title.size = 13)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_sim_theory_+3A_sim_y">sim_y</code></td>
<td>
<p>a vector of simulated data</p>
</td></tr>
<tr><td><code id="plot_sim_theory_+3A_title">title</code></td>
<td>
<p>the title for the graph (default = &quot;Simulated Data Values&quot;)</p>
</td></tr>
<tr><td><code id="plot_sim_theory_+3A_ylower">ylower</code></td>
<td>
<p>the lower y value to use in the plot (default = NULL, uses minimum simulated y value)</p>
</td></tr>
<tr><td><code id="plot_sim_theory_+3A_yupper">yupper</code></td>
<td>
<p>the upper y value (default = NULL, uses maximum simulated y value)</p>
</td></tr>
<tr><td><code id="plot_sim_theory_+3A_power_color">power_color</code></td>
<td>
<p>the histogram fill color for the simulated variable (default = &quot;dark blue&quot;)</p>
</td></tr>
<tr><td><code id="plot_sim_theory_+3A_overlay">overlay</code></td>
<td>
<p>if TRUE (default), the target distribution is also plotted given either a distribution name (and parameters)
or pdf function fx (with support bounds = lower, upper)</p>
</td></tr>
<tr><td><code id="plot_sim_theory_+3A_cont_var">cont_var</code></td>
<td>
<p>TRUE (default) for continuous variables, FALSE for count variables</p>
</td></tr>
<tr><td><code id="plot_sim_theory_+3A_target_color">target_color</code></td>
<td>
<p>the histogram fill color for the target distribution (default = &quot;dark green&quot;)</p>
</td></tr>
<tr><td><code id="plot_sim_theory_+3A_nbins">nbins</code></td>
<td>
<p>the number of bins to use when creating the histograms (default = 100)</p>
</td></tr>
<tr><td><code id="plot_sim_theory_+3A_dist">Dist</code></td>
<td>
<p>name of the distribution. The possible values are: &quot;Benini&quot;, &quot;Beta&quot;, &quot;Beta-Normal&quot;, &quot;Birnbaum-Saunders&quot;, &quot;Chisq&quot;,
&quot;Exponential&quot;, &quot;Exp-Geometric&quot;, &quot;Exp-Logarithmic&quot;, &quot;Exp-Poisson&quot;, &quot;F&quot;, &quot;Fisk&quot;, &quot;Frechet&quot;, &quot;Gamma&quot;, &quot;Gaussian&quot;, &quot;Gompertz&quot;,
&quot;Gumbel&quot;, &quot;Kumaraswamy&quot;, &quot;Laplace&quot;, &quot;Lindley&quot;, &quot;Logistic&quot;, &quot;Loggamma&quot;, &quot;Lognormal&quot;, &quot;Lomax&quot;, &quot;Makeham&quot;, &quot;Maxwell&quot;,
&quot;Nakagami&quot;, &quot;Paralogistic&quot;, &quot;Pareto&quot;, &quot;Perks&quot;, &quot;Rayleigh&quot;, &quot;Rice&quot;, &quot;Singh-Maddala&quot;, &quot;Skewnormal&quot;, &quot;t&quot;, &quot;Topp-Leone&quot;, &quot;Triangular&quot;,
&quot;Uniform&quot;, &quot;Weibull&quot;, &quot;Poisson&quot;, and &quot;Negative_Binomial&quot;.
Please refer to the documentation for each package (either <code><a href="stats.html#topic+stats-package">stats-package</a></code>, <code><a href="VGAM.html#topic+VGAM-package">VGAM-package</a></code>, or
<code><a href="triangle.html#topic+triangle">triangle</a></code>) for information on appropriate parameter inputs.</p>
</td></tr>
<tr><td><code id="plot_sim_theory_+3A_params">params</code></td>
<td>
<p>a vector of parameters (up to 4) for the desired distribution (keep NULL if <code>fx</code> supplied instead)</p>
</td></tr>
<tr><td><code id="plot_sim_theory_+3A_fx">fx</code></td>
<td>
<p>a pdf input as a function of x only, i.e. fx &lt;- function(x) 0.5*(x-1)^2; must return a scalar
(keep NULL if <code>Dist</code> supplied instead)</p>
</td></tr>
<tr><td><code id="plot_sim_theory_+3A_lower">lower</code></td>
<td>
<p>the lower support bound for a supplied fx, else keep NULL (note: if an error is thrown from <code>uniroot</code>,
try a slightly higher lower bound; i.e., 0.0001 instead of 0)</p>
</td></tr>
<tr><td><code id="plot_sim_theory_+3A_upper">upper</code></td>
<td>
<p>the upper support bound for a supplied fx, else keep NULL (note: if an error is thrown from <code>uniroot</code>,
try a lower upper bound; i.e., 100000 instead of Inf)</p>
</td></tr>
<tr><td><code id="plot_sim_theory_+3A_seed">seed</code></td>
<td>
<p>the seed value for random number generation (default = 1234)</p>
</td></tr>
<tr><td><code id="plot_sim_theory_+3A_sub">sub</code></td>
<td>
<p>the number of subdivisions to use in the integration to calculate the cdf from fx; if no result, try increasing
sub (requires longer computation time; default = 1000)</p>
</td></tr>
<tr><td><code id="plot_sim_theory_+3A_legend.position">legend.position</code></td>
<td>
<p>the position of the legend</p>
</td></tr>
<tr><td><code id="plot_sim_theory_+3A_legend.justification">legend.justification</code></td>
<td>
<p>the justification of the legend</p>
</td></tr>
<tr><td><code id="plot_sim_theory_+3A_legend.text.size">legend.text.size</code></td>
<td>
<p>the size of the legend labels</p>
</td></tr>
<tr><td><code id="plot_sim_theory_+3A_title.text.size">title.text.size</code></td>
<td>
<p>the size of the plot title</p>
</td></tr>
<tr><td><code id="plot_sim_theory_+3A_axis.text.size">axis.text.size</code></td>
<td>
<p>the size of the axes text (tick labels)</p>
</td></tr>
<tr><td><code id="plot_sim_theory_+3A_axis.title.size">axis.title.size</code></td>
<td>
<p>the size of the axes titles</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code><a href="ggplot2.html#topic+ggplot2-package">ggplot2-package</a></code> object.
</p>


<h3>References</h3>

<p>Please see the references for <code><a href="#topic+plot_cdf">plot_cdf</a></code>.
</p>
<p>Wickham H. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York, 2009.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+calc_theory">calc_theory</a></code>,
<code><a href="ggplot2.html#topic+ggplot2-package">ggplot2-package</a></code>, <code><a href="ggplot2.html#topic+geom_histogram">geom_histogram</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Logistic Distribution: mean = 0, variance = 1
seed = 1234

# Find standardized cumulants
stcum &lt;- calc_theory(Dist = "Logistic", params = c(0, 1))

# Simulate without the sixth cumulant correction
# (invalid power method pdf)
Logvar1 &lt;- nonnormvar1(method = "Polynomial", means = 0, vars = 1,
                       skews = stcum[3], skurts = stcum[4],
                       fifths = stcum[5], sixths = stcum[6],
                       n = 10000, seed = seed)

# Plot simulated variable (invalid) and data from theoretical distribution
plot_sim_theory(sim_y = Logvar1$continuous_variable,
                title = "Invalid Logistic Simulated Data Values",
                overlay = TRUE, Dist = "Logistic", params = c(0, 1),
                seed = seed)

# Simulate with the sixth cumulant correction
# (valid power method pdf)
Logvar2 &lt;- nonnormvar1(method = "Polynomial", means = 0, vars = 1,
                       skews = stcum[3], skurts = stcum[4],
                       fifths = stcum[5], sixths = stcum[6],
                       Six = seq(1.5, 2, 0.05), n = 10000, seed = seed)

# Plot simulated variable (valid) and data from theoretical distribution
plot_sim_theory(sim_y = Logvar2$continuous_variable,
                title = "Valid Logistic Simulated Data Values",
                overlay = TRUE, Dist = "Logistic", params = c(0, 1),
                seed = seed)

# Simulate 2 Negative Binomial distributions and correlation 0.3
# using Method 1
NBvars &lt;- rcorrvar(k_nb = 2, size = c(10, 15), prob = c(0.4, 0.3),
                   rho = matrix(c(1, 0.3, 0.3, 1), 2, 2), seed = seed)

# Plot pdfs of 1st simulated variable and theoretical distribution
plot_sim_theory(sim_y = NBvars$Neg_Bin_variable[, 1], overlay = TRUE,
                cont_var = FALSE, Dist = "Negative_Binomial",
                params = c(10, 0.4))


## End(Not run)

</code></pre>

<hr>
<h2 id='poly'>Headrick's Fifth-Order Polynomial Transformation Equations</h2><span id='topic+poly'></span>

<h3>Description</h3>

<p>This function contains Headrick's fifth-order polynomial transformation equations
(2002, doi: <a href="http://doi.org/10.1016/S0167-9473(02)00072-5">10.1016/S0167-9473(02)00072-5</a>).  It is used in
<code><a href="#topic+find_constants">find_constants</a></code> to find the constants c1, c2, c3, c4, and c5 (<code class="reqn">c0 = -c2 - 3 * c4</code>)
that satisfy the equations given skewness, standardized kurtosis, and standardized fifth and sixth cumulant values.
It can be used to verify a set of constants satisfy the equations.  Note that there exist solutions that yield
invalid power method pdfs (see <code><a href="#topic+power_norm_corr">power_norm_corr</a></code>,
<code><a href="#topic+pdf_check">pdf_check</a></code>).  This function would not ordinarily be called by the user.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>poly(c, a)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="poly_+3A_c">c</code></td>
<td>
<p>a vector of constants c1, c2, c3, c4, c5; note that <code><a href="#topic+find_constants">find_constants</a></code> returns
c0, c1, c2, c3, c4, c5</p>
</td></tr>
<tr><td><code id="poly_+3A_a">a</code></td>
<td>
<p>a vector c(skewness, standardized kurtosis, standardized fifth cumulant, standardized sixth cumulant)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list of length 5; if the constants satisfy the equations, returns 0 for all list elements
</p>


<h3>References</h3>

<p>Headrick TC (2002). Fast Fifth-order Polynomial Transforms for Generating Univariate and Multivariate
Non-normal Distributions. Computational Statistics &amp; Data Analysis, 40(4):685-711. doi: <a href="http://doi.org/10.1016/S0167-9473(02)00072-5">10.1016/S0167-9473(02)00072-5</a>.
(<a href="http://www.sciencedirect.com/science/article/pii/S0167947302000725">ScienceDirect</a>)
</p>
<p>Headrick TC (2004). On Polynomial Transformations for Simulating Multivariate Nonnormal Distributions.
Journal of Modern Applied Statistical Methods, 3(1), 65-71. doi: <a href="http://doi.org/10.22237/jmasm/1083370080">10.22237/jmasm/1083370080</a>.
</p>
<p>Headrick TC, Kowalchuk RK (2007). The Power Method Transformation: Its Probability Density Function, Distribution
Function, and Its Further Use for Fitting Data. Journal of Statistical Computation and Simulation, 77, 229-249. doi: <a href="http://doi.org/10.1080/10629360600605065">10.1080/10629360600605065</a>.
</p>
<p>Headrick TC, Sheng Y, &amp; Hodis FA (2007). Numerical Computing and Graphics for the Power Method Transformation Using
Mathematica. Journal of Statistical Software, 19(3), 1 - 17. doi: <a href="http://doi.org/10.18637/jss.v019.i03">10.18637/jss.v019.i03</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fleish">fleish</a></code>, <code><a href="#topic+power_norm_corr">power_norm_corr</a></code>,
<code><a href="#topic+pdf_check">pdf_check</a></code>, <code><a href="#topic+find_constants">find_constants</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Laplace Distribution
poly(c = c(0.727709, 0, 0.096303, 0, -0.002232), a = c(0, 3, 0, 30))
</code></pre>

<hr>
<h2 id='poly_skurt_check'>Headrick's Fifth-Order Transformation Lagrangean Constraints for Lower Boundary of Standardized Kurtosis</h2><span id='topic+poly_skurt_check'></span>

<h3>Description</h3>

<p>This function gives the first-order conditions of the multi-constraint Lagrangean expression
</p>
<p style="text-align: center;"><code class="reqn">F(c1, ..., c5, \lambda_{1}, ..., \lambda_{4}) = f(c1, ..., c5) + \lambda_{1} * [1 - g(c1, ..., c5)]</code>
</p>

<p style="text-align: center;"><code class="reqn">+ \lambda_{2} * [\gamma_{1} - h(c1, ..., c5)] + \lambda_{3} * [\gamma_{3} - i(c1, ..., c5)]</code>
</p>

<p style="text-align: center;"><code class="reqn">+ \lambda_{4} * [\gamma_{4} - j(c1, ..., c5)]</code>
</p>

<p>used to find the lower kurtosis boundary for a given skewness and standardized fifth and sixth cumulants
in <code><a href="#topic+calc_lower_skurt">calc_lower_skurt</a></code>.  The partial derivatives are described in Headrick (2002,
doi: <a href="http://doi.org/10.1016/S0167-9473(02)00072-5">10.1016/S0167-9473(02)00072-5</a>), but he does not provide
the actual equations. The equations used here were found with <code>D</code> (see <code><a href="stats.html#topic+deriv">deriv</a></code>).
Here, <code class="reqn">\lambda_{1}, ..., \lambda_{4}</code> are the Lagrangean multipliers, <code class="reqn">\gamma_{1}, \gamma_{3}, \gamma_{4}</code> are the user-specified
values of skewness, fifth cumulant, and sixth cumulant, and <code class="reqn">f, g, h, i, j</code> are the equations for standardized kurtosis, variance,
fifth cumulant, and sixth cumulant expressed in terms of the constants.  This function would not ordinarily be called by the user.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>poly_skurt_check(c, a)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="poly_skurt_check_+3A_c">c</code></td>
<td>
<p>a vector of constants c1, ..., c5, lambda1, ..., lambda4</p>
</td></tr>
<tr><td><code id="poly_skurt_check_+3A_a">a</code></td>
<td>
<p>a vector of skew, fifth standardized cumulant, sixth standardized cumulant</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with components:
</p>
<p><code class="reqn">dF/d\lambda_{1} = 1 - g(c1, ..., c5)</code>
</p>
<p><code class="reqn">dF/d\lambda_{2} = \gamma_{1} - h(c1, ..., c5)</code>
</p>
<p><code class="reqn">dF/d\lambda_{3} = \gamma_{3} - i(c1, ..., c5)</code>
</p>
<p><code class="reqn">dF/d\lambda_{4} = \gamma_{4} - j(c1, ..., c5)</code>
</p>
<p><code class="reqn">dF/dc1 = df/dc1 - \lambda_{1} * dg/dc1 - \lambda_{2} * dh/dc1 - \lambda_{3} * di/dc1 - \lambda_{4} * dj/dc1</code>
</p>
<p><code class="reqn">dF/dc2 = df/dc2 - \lambda_{1} * dg/dc2 - \lambda_{2} * dh/dc2 - \lambda_{3} * di/dc2 - \lambda_{4} * dj/dc2</code>
</p>
<p><code class="reqn">dF/dc3 = df/dc3 - \lambda_{1} * dg/dc3 - \lambda_{2} * dh/dc3 - \lambda_{3} * di/dc3 - \lambda_{4} * dj/dc3</code>
</p>
<p><code class="reqn">dF/dc4 = df/dc4 - \lambda_{1} * dg/dc4 - \lambda_{2} * dh/dc4 - \lambda_{3} * di/dc4 - \lambda_{4} * dj/dc4</code>
</p>
<p><code class="reqn">dF/dc5 = df/dc5 - \lambda_{1} * dg/dc5 - \lambda_{2} * dh/dc5 - \lambda_{3} * di/dc5 - \lambda_{4} * dj/dc5</code>
</p>
<p>If the suppled values for <code>c</code> and <code>a</code> satisfy the Lagrangean expression, it will return 0 for each component.
</p>


<h3>References</h3>

<p>Headrick TC (2002). Fast Fifth-order Polynomial Transforms for Generating Univariate and Multivariate
Non-normal Distributions. Computational Statistics &amp; Data Analysis, 40(4):685-711. doi: <a href="http://doi.org/10.1016/S0167-9473(02)00072-5">10.1016/S0167-9473(02)00072-5</a>.
(<a href="http://www.sciencedirect.com/science/article/pii/S0167947302000725">ScienceDirect</a>)
</p>
<p>Headrick TC (2004). On Polynomial Transformations for Simulating Multivariate Nonnormal Distributions.
Journal of Modern Applied Statistical Methods, 3(1), 65-71. doi: <a href="http://doi.org/10.22237/jmasm/1083370080">10.22237/jmasm/1083370080</a>.
</p>
<p>Headrick TC, Kowalchuk RK (2007). The Power Method Transformation: Its Probability Density Function, Distribution
Function, and Its Further Use for Fitting Data. Journal of Statistical Computation and Simulation, 77, 229-249. doi: <a href="http://doi.org/10.1080/10629360600605065">10.1080/10629360600605065</a>.
</p>
<p>Headrick TC, Sheng Y, &amp; Hodis FA (2007). Numerical Computing and Graphics for the Power Method Transformation Using
Mathematica. Journal of Statistical Software, 19(3), 1 - 17. doi: <a href="http://doi.org/10.18637/jss.v019.i03">10.18637/jss.v019.i03</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+calc_lower_skurt">calc_lower_skurt</a></code>
</p>

<hr>
<h2 id='power_norm_corr'>Calculate Power Method Correlation</h2><span id='topic+power_norm_corr'></span>

<h3>Description</h3>

<p>This function calculates the correlation between a continuous variable, Y1, generated using a third or fifth-
order polynomial transformation and the generating standard normal variable, Z1.  The power method correlation
(described in Headrick &amp; Kowalchuk, 2007, doi: <a href="http://doi.org/10.1080/10629360600605065">10.1080/10629360600605065</a>) is given by:
<code class="reqn">\rho_{y1,z1} = c1 + 3*c3 + 15* c5</code>, where c5 = 0 if <code>method</code> = &quot;Fleishman&quot;.  A value &lt;= 0 indicates an invalid
pdf and the signs of c1 and c3 should be reversed, which could still yield an invalid pdf.  All constants should
be checked using <code><a href="#topic+pdf_check">pdf_check</a></code> to see if they generate a valid pdf.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>power_norm_corr(c, method)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="power_norm_corr_+3A_c">c</code></td>
<td>
<p>a vector of constants c0, c1, c2, c3 (if <code>method</code> = &quot;Fleishman&quot;) or c0, c1, c2, c3, c4, c5 (if <code>method</code> =
&quot;Polynomial&quot;), like that returned by <code><a href="#topic+find_constants">find_constants</a></code></p>
</td></tr>
<tr><td><code id="power_norm_corr_+3A_method">method</code></td>
<td>
<p>the method used to find the constants.  &quot;Fleishman&quot; uses a third-order polynomial transformation and
&quot;Polynomial&quot; uses Headrick's fifth-order transformation.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A scalar equal to the correlation.
</p>


<h3>References</h3>

<p>Please see references for <code><a href="#topic+pdf_check">pdf_check</a></code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fleish">fleish</a></code>, <code><a href="#topic+poly">poly</a></code>,
<code><a href="#topic+find_constants">find_constants</a></code>, <code><a href="#topic+pdf_check">pdf_check</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Beta(a = 4, b = 2) Distribution
power_norm_corr(c = c(0.108304, 1.104252, -0.123347, -0.045284, 0.005014,
                      0.001285),
                method = "Polynomial")

# Switch signs on c1, c3, and c5 to get negative correlation (invalid pdf):
power_norm_corr(c = c(0.108304, -1.104252, -0.123347, 0.045284, 0.005014,
                      -0.001285),
                method = "Polynomial")

</code></pre>

<hr>
<h2 id='rcorrvar'>Generation of Correlated Ordinal, Continuous, Poisson, and/or Negative Binomial Variables: Correlation Method 1</h2><span id='topic+rcorrvar'></span>

<h3>Description</h3>

<p>This function simulates <code>k_cat</code> ordinal, <code>k_cont</code> continuous, <code>k_pois</code> Poisson, and/or <code>k_nb</code>
Negative Binomial variables with
a specified correlation matrix <code>rho</code>.  The variables are generated from multivariate normal variables with intermediate correlation
matrix <code>Sigma</code>, calculated by <code><a href="#topic+findintercorr">findintercorr</a></code>, and then transformed.  The <em>ordering</em> of the
variables in <code>rho</code> must be <em>ordinal</em> (r &gt;= 2 categories), <em>continuous</em>, <em>Poisson</em>, and <em>Negative Binomial</em>
(note that it is possible for <code>k_cat</code>, <code>k_cont</code>, <code>k_pois</code>, and/or <code>k_nb</code> to be 0).  The vignette
<b>Overall Workflow for Data Simulation</b> provides a detailed example discussing the step-by-step simulation process and comparing
correlation methods 1 and 2.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rcorrvar(n = 10000, k_cont = 0, k_cat = 0, k_pois = 0, k_nb = 0,
  method = c("Fleishman", "Polynomial"), means = NULL, vars = NULL,
  skews = NULL, skurts = NULL, fifths = NULL, sixths = NULL,
  Six = list(), marginal = list(), support = list(), nrand = 100000,
  lam = NULL, size = NULL, prob = NULL, mu = NULL, Sigma = NULL,
  rho = NULL, cstart = NULL, seed = 1234, errorloop = FALSE,
  epsilon = 0.001, maxit = 1000, extra_correct = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rcorrvar_+3A_n">n</code></td>
<td>
<p>the sample size (i.e. the length of each simulated variable; default = 10000)</p>
</td></tr>
<tr><td><code id="rcorrvar_+3A_k_cont">k_cont</code></td>
<td>
<p>the number of continuous variables (default = 0)</p>
</td></tr>
<tr><td><code id="rcorrvar_+3A_k_cat">k_cat</code></td>
<td>
<p>the number of ordinal (r &gt;= 2 categories) variables (default = 0)</p>
</td></tr>
<tr><td><code id="rcorrvar_+3A_k_pois">k_pois</code></td>
<td>
<p>the number of Poisson variables (default = 0)</p>
</td></tr>
<tr><td><code id="rcorrvar_+3A_k_nb">k_nb</code></td>
<td>
<p>the number of Negative Binomial variables (default = 0)</p>
</td></tr>
<tr><td><code id="rcorrvar_+3A_method">method</code></td>
<td>
<p>the method used to generate the k_cont continuous variables.  &quot;Fleishman&quot; uses Fleishman's third-order polynomial transformation
and &quot;Polynomial&quot; uses Headrick's fifth-order transformation.</p>
</td></tr>
<tr><td><code id="rcorrvar_+3A_means">means</code></td>
<td>
<p>a vector of means for the k_cont continuous variables (i.e. = rep(0, k_cont))</p>
</td></tr>
<tr><td><code id="rcorrvar_+3A_vars">vars</code></td>
<td>
<p>a vector of variances (i.e. = rep(1, k_cont))</p>
</td></tr>
<tr><td><code id="rcorrvar_+3A_skews">skews</code></td>
<td>
<p>a vector of skewness values (i.e. = rep(0, k_cont))</p>
</td></tr>
<tr><td><code id="rcorrvar_+3A_skurts">skurts</code></td>
<td>
<p>a vector of standardized kurtoses (kurtosis - 3, so that normal variables have a value of 0; i.e. = rep(0, k_cont))</p>
</td></tr>
<tr><td><code id="rcorrvar_+3A_fifths">fifths</code></td>
<td>
<p>a vector of standardized fifth cumulants (not necessary for <code>method</code> = &quot;Fleishman&quot;; i.e. = rep(0, k_cont))</p>
</td></tr>
<tr><td><code id="rcorrvar_+3A_sixths">sixths</code></td>
<td>
<p>a vector of standardized sixth cumulants (not necessary for <code>method</code> = &quot;Fleishman&quot;; i.e. = rep(0, k_cont))</p>
</td></tr>
<tr><td><code id="rcorrvar_+3A_six">Six</code></td>
<td>
<p>a list of vectors of correction values to add to the sixth cumulants if no valid pdf constants are found,
ex: <code>Six = list(seq(0.01, 2,by = 0.01), seq(1, 10,by = 0.5))</code>; if no correction is desired for variable Y_i, set set the i-th list
component equal to NULL</p>
</td></tr>
<tr><td><code id="rcorrvar_+3A_marginal">marginal</code></td>
<td>
<p>a list of length equal to <code>k_cat</code>; the i-th element is a vector of the cumulative
probabilities defining the marginal distribution of the i-th variable;
if the variable can take r values, the vector will contain r - 1 probabilities (the r-th is assumed to be 1; default = list());
for binary variables, these should be input the same as for ordinal variables with more than 2 categories (i.e. the user-specified
probability is the probability of the 1st category, which has the smaller support value)</p>
</td></tr>
<tr><td><code id="rcorrvar_+3A_support">support</code></td>
<td>
<p>a list of length equal to <code>k_cat</code>; the i-th element is a vector containing the r ordered support values;
if not provided (i.e. <code>support = list()</code>), the default is for the i-th element to be the
vector 1, ..., r</p>
</td></tr>
<tr><td><code id="rcorrvar_+3A_nrand">nrand</code></td>
<td>
<p>the number of random numbers to generate in calculating intermediate correlations (default = 10000)</p>
</td></tr>
<tr><td><code id="rcorrvar_+3A_lam">lam</code></td>
<td>
<p>a vector of lambda (&gt; 0) constants for the Poisson variables (see <code><a href="stats.html#topic+Poisson">Poisson</a></code>)</p>
</td></tr>
<tr><td><code id="rcorrvar_+3A_size">size</code></td>
<td>
<p>a vector of size parameters for the Negative Binomial variables (see <code><a href="stats.html#topic+NegBinomial">NegBinomial</a></code>)</p>
</td></tr>
<tr><td><code id="rcorrvar_+3A_prob">prob</code></td>
<td>
<p>a vector of success probability parameters</p>
</td></tr>
<tr><td><code id="rcorrvar_+3A_mu">mu</code></td>
<td>
<p>a vector of mean parameters (*Note: either <code>prob</code> or <code>mu</code> should be supplied for all Negative Binomial variables,
not a mixture; default = NULL)</p>
</td></tr>
<tr><td><code id="rcorrvar_+3A_sigma">Sigma</code></td>
<td>
<p>an intermediate correlation matrix to use if the user wants to provide one (default = NULL)</p>
</td></tr>
<tr><td><code id="rcorrvar_+3A_rho">rho</code></td>
<td>
<p>the target correlation matrix (<em>must be ordered ordinal, continuous, Poisson, Negative Binomial</em>; default = NULL)</p>
</td></tr>
<tr><td><code id="rcorrvar_+3A_cstart">cstart</code></td>
<td>
<p>a list containing initial values for root-solving algorithm used in <code><a href="#topic+find_constants">find_constants</a></code>
(see <code><a href="BB.html#topic+multiStart">multiStart</a></code> for <code>method</code> = &quot;Fleishman&quot; or <code><a href="nleqslv.html#topic+nleqslv">nleqslv</a></code> for <code>method</code> = &quot;Polynomial&quot;).
If user specified, each list element must be input as a matrix. If no starting values are specified for a given continuous
variable, that list element should be NULL.  If NULL and all 4 standardized cumulants (rounded to 3 digits) are within
0.01 of those in Headrick's common distribution table (see <code><a href="#topic+Headrick.dist">Headrick.dist</a></code>
data), uses his constants as starting values; else, generates n sets of random starting values from
uniform distributions.</p>
</td></tr>
<tr><td><code id="rcorrvar_+3A_seed">seed</code></td>
<td>
<p>the seed value for random number generation (default = 1234)</p>
</td></tr>
<tr><td><code id="rcorrvar_+3A_errorloop">errorloop</code></td>
<td>
<p>if TRUE, uses <code><a href="#topic+error_loop">error_loop</a></code> to attempt to correct the final correlation
(default = FALSE)</p>
</td></tr>
<tr><td><code id="rcorrvar_+3A_epsilon">epsilon</code></td>
<td>
<p>the maximum acceptable error between the final and target correlation matrices (default = 0.001)
in the calculation of ordinal intermediate correlations with <code><a href="#topic+ordnorm">ordnorm</a></code> or in the error loop</p>
</td></tr>
<tr><td><code id="rcorrvar_+3A_maxit">maxit</code></td>
<td>
<p>the maximum number of iterations to use (default = 1000) in the calculation of ordinal
intermediate correlations with <code><a href="#topic+ordnorm">ordnorm</a></code> or in the error loop</p>
</td></tr>
<tr><td><code id="rcorrvar_+3A_extra_correct">extra_correct</code></td>
<td>
<p>if TRUE, within each variable pair, if the maximum correlation error is still greater than 0.1, the intermediate
correlation is set equal to the target correlation (with the assumption that the calculated final correlation will be
less than 0.1 away from the target)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list whose components vary based on the type of simulated variables.  Simulated variables are returned as data.frames:
</p>
<p>If <b>ordinal variables</b> are produced:
</p>
<p><code>ordinal_variables</code> the generated ordinal variables,
</p>
<p><code>summary_ordinal</code> a list, where the i-th element contains a data.frame with column 1 = target cumulative probabilities and
column 2 = simulated cumulative probabilities for ordinal variable Y_i
</p>
<p>If <b>continuous variables</b> are produced:
</p>
<p><code>constants</code> a data.frame of the constants,
</p>
<p><code>continuous_variables</code> the generated continuous variables,
</p>
<p><code>summary_continuous</code> a data.frame containing a summary of each variable,
</p>
<p><code>summary_targetcont</code> a data.frame containing a summary of the target variables,
</p>
<p><code>sixth_correction</code> a vector of sixth cumulant correction values,
</p>
<p><code>valid.pdf</code> a vector where the i-th element is &quot;TRUE&quot; if the constants for the i-th continuous variable generate a valid pdf,
else &quot;FALSE&quot;
</p>
<p>If <b>Poisson variables</b> are produced:
</p>
<p><code>Poisson_variables</code> the generated Poisson variables,
</p>
<p><code>summary_Poisson</code> a data.frame containing a summary of each variable
</p>
<p>If <b>Negative Binomial variables</b> are produced:
</p>
<p><code>Neg_Bin_variables</code> the generated Negative Binomial variables,
</p>
<p><code>summary_Neg_Bin</code> a data.frame containing a summary of each variable
</p>
<p>Additionally, the following elements:
</p>
<p><code>correlations</code> the final correlation matrix,
</p>
<p><code>Sigma1</code> the intermediate correlation before the error loop,
</p>
<p><code>Sigma2</code> the intermediate correlation matrix after the error loop,
</p>
<p><code>Constants_Time</code> the time in minutes required to calculate the constants,
</p>
<p><code>Intercorrelation_Time</code> the time in minutes required to calculate the intermediate correlation matrix,
</p>
<p><code>Error_Loop_Time</code> the time in minutes required to use the error loop,
</p>
<p><code>Simulation_Time</code> the total simulation time in minutes,
</p>
<p><code>niter</code> a matrix of the number of iterations used for each variable in the error loop,
</p>
<p><code>maxerr</code> the maximum final correlation error (from the target rho).
</p>
<p>If a particular element is not required, the result is NULL for that element.
</p>


<h3>Variable Types and Required Inputs</h3>

<p>1) <b>Continuous Variables:</b> Continuous variables are simulated using either Fleishman's third-order (<code>method</code> = &quot;Fleishman&quot;,
doi: <a href="http://doi.org/10.1007/BF02293811">10.1007/BF02293811</a>) or Headrick's fifth-order (<code>method</code> = &quot;Polynomial&quot;, doi: <a href="http://doi.org/10.1016/S0167-9473(02)00072-5">10.1016/S0167-9473(02)00072-5</a>) power method
transformation.  This is a computationally efficient algorithm that simulates continuous distributions through the method of moments.
It works by matching standardized cumulants &ndash; the first four (mean, variance, skew, and standardized kurtosis) for Fleishman's method, or
the first six (mean, variance, skew, standardized kurtosis, and standardized fifth and sixth cumulants) for Headrick's method.  The
transformation is expressed as follows:
</p>
<p><code class="reqn">Y = c_{0} + c_{1} * Z + c_{2} * Z^2 + c_{3} * Z^3 + c_{4} * Z^4 + c_{5} * Z^5</code>,
</p>
<p>where <code class="reqn">Z ~ N(0,1)</code>, and <code class="reqn">c_{4}</code> and <code class="reqn">c_{5}</code> both equal <code class="reqn">0</code> for Fleishman's method.  The real constants are calculated by
<code><a href="#topic+find_constants">find_constants</a></code>.  All variables are simulated with mean <code class="reqn">0</code> and variance <code class="reqn">1</code>, and then transformed
to the specified mean and variance at the end.
</p>
<p>The required parameters for simulating continuous variables include: mean, variance, skewness, standardized kurtosis (kurtosis - 3), and
standardized fifth and sixth cumulants (for <code>method</code> = &quot;Polynomial&quot;).  If the goal is to simulate a theoretical distribution
(i.e. Gamma, Beta, Logistic, etc.), these values can be obtained using <code><a href="#topic+calc_theory">calc_theory</a></code>.  If the goal is to
mimic an empirical data set, these values can be found using <code><a href="#topic+calc_moments">calc_moments</a></code> (using the method of moments) or
<code><a href="#topic+calc_fisherk">calc_fisherk</a></code> (using Fisher's k-statistics).  If the standardized cumulants
are obtained from <code>calc_theory</code>, the user may need to use rounded values as inputs (i.e. <code>skews = round(skews, 8)</code>).  Due to the nature
of the integration involved in <code>calc_theory</code>, the results are approximations.  Greater accuracy can be achieved by increasing the number of
subdivisions (<code>sub</code>) used in the integration process.  For example, in order to ensure that skew is exactly 0 for symmetric distributions.
</p>
<p>For some sets of cumulants, it is either not possible to find
power method constants or the calculated constants do not generate valid power method pdfs.  In these situations, adding a value to the
sixth cumulant may provide solutions (see <code><a href="#topic+find_constants">find_constants</a></code>).  When using Headrick's fifth-order approximation,
if simulation results indicate that a
continuous variable does not generate a valid pdf, the user can try <code><a href="#topic+find_constants">find_constants</a></code> with various sixth
cumulant correction vectors to determine if a valid pdf can be found.
</p>
<p>2) <b>Binary and Ordinal Variables:</b> Ordinal variables (<code class="reqn">r \ge 2</code> categories) are generated by discretizing the standard normal variables
at quantiles.  These quantiles are determined by evaluating the inverse standard normal cdf at the cumulative probabilities defined by each
variable's marginal distribution.  The required inputs for ordinal variables are the cumulative marginal probabilities and support values
(if desired).  The probabilities should be combined into a list of length equal to the number of ordinal variables.  The <code class="reqn">i^{th}</code> element
is a vector of the cumulative probabilities defining the marginal distribution of the <code class="reqn">i^{th}</code> variable.  If the variable can take
<code class="reqn">r</code> values, the vector will contain <code class="reqn">r - 1</code> probabilities (the <code class="reqn">r^{th}</code> is assumed to be <code class="reqn">1</code>).
</p>
<p><em>Note for binary variables:</em> the user-suppled probability should be the probability of the <code class="reqn">1^{st}</code> (lower) support value.  This would
ordinarily be considered the probability of <em>failure</em> (<code class="reqn">q</code>), while the probability of the <code class="reqn">2^{nd}</code> (upper) support value would be
considered the probability of <em>success</em> (<code class="reqn">p = 1 - q</code>).  The support values should be combined into a separate list.  The <code class="reqn">i^{th}</code>
element is a vector containing the <code class="reqn">r</code> ordered support values.
</p>
<p>3) <b>Count Variables:</b> Count variables are generated using the inverse cdf method.  The cumulative distribution function of a standard
normal variable has a uniform distribution.  The appropriate quantile function <code class="reqn">F_{Y}^{-1}</code> is applied to this uniform variable with the
designated parameters to generate the count variable: <code class="reqn">Y = F_{y}^{-1}(\Phi(Z))</code>.  For Poisson variables, the lambda (mean) value should be
given.  For Negative Binomial variables, the size (target number of successes) and either the success probability or the mean should be given.
The Negative Binomial variable represents the number of failures which occur in a sequence of Bernoulli trials before the target number of
successes is achieved.
</p>
<p>More details regarding the variable types can be found in the <b>Variable Types</b> vignette.
</p>


<h3>Overview of Correlation Method 1</h3>

<p>The intermediate correlations used in correlation method 1 are more simulation based than those in method 2, which means that accuracy
increases with sample size and the number of repetitions.  In addition, specifying the seed allows for reproducibility.  In
addition, method 1 differs from method 2 in the following ways:
</p>
<p>1) The intermediate correlation for <b>count variables</b> is based on the method of Yahav &amp; Shmueli (2012, doi: <a href="http://doi.org/10.1002/asmb.901">10.1002/asmb.901</a>), which
uses a simulation based, logarithmic transformation of the target correlation.  This method becomes less accurate as the variable mean
gets closer to zero.
</p>
<p>2) The <b>ordinal - count variable</b> correlations are based on an extension of the method of Amatya &amp; Demirtas (2015,
doi: <a href="http://doi.org/10.1080/00949655.2014.953534">10.1080/00949655.2014.953534</a>), in which
the correlation correction factor is the product of the upper Frechet-Hoeffding bound on the correlation between the count
variable and the normal variable used to generate it and a simulated upper bound on the correlation between an ordinal variable
and the normal variable used to generate it (see Demirtas &amp; Hedeker, 2011, doi: <a href="http://doi.org/10.1198/tast.2011.10090">10.1198/tast.2011.10090</a>).
</p>
<p>3) The <b>continuous - count variable</b> correlations are based on an extension of the methods of Amatya &amp; Demirtas (2015) and
Demirtas et al. (2012, doi: <a href="http://doi.org/10.1002/sim.5362">10.1002/sim.5362</a>), in which the correlation correction factor is the product of the upper Frechet-Hoeffding bound
on the correlation between the count variable and the normal variable used to generate it and the power method correlation
between the continuous variable and the normal variable used to generate it (see Headrick &amp; Kowalchuk, 2007, doi: <a href="http://doi.org/10.1080/10629360600605065">10.1080/10629360600605065</a>).
The intermediate correlations are the ratio of the target correlations to the correction factor.
</p>
<p>Please see the <b>Comparison of Method 1 and Method 2</b> vignette for more information and an step-by-step overview of the
simulation process.
</p>


<h3>Choice of Fleishman's third-order or Headrick's fifth-order method</h3>

<p>Using the fifth-order approximation allows additional control over the fifth and sixth moments of the generated distribution, improving
accuracy.  In addition, the range of feasible standardized kurtosis values, given skew and standardized fifth (<code class="reqn">\gamma_{3}</code>) and sixth
(<code class="reqn">\gamma_{4}</code>) cumulants, is larger than with Fleishman's method (see <code><a href="#topic+calc_lower_skurt">calc_lower_skurt</a></code>).
For example, the Fleishman method can not be used to generate a
non-normal distribution with a ratio of <code class="reqn">\gamma_{3}^2/\gamma_{4} &gt; 9/14</code> (see Headrick &amp; Kowalchuk, 2007).
This eliminates the
Chi-squared family of distributions, which has a constant ratio of <code class="reqn">\gamma_{3}^2/\gamma_{4} = 2/3</code>.  However, if the fifth and
sixth cumulants do not exist, the Fleishman approximation should be used.
</p>


<h3>Reasons for Function Errors</h3>

<p>1) The most likely cause for function errors is that no solutions to <code><a href="#topic+fleish">fleish</a></code> or
<code><a href="#topic+poly">poly</a></code> converged when using <code><a href="#topic+find_constants">find_constants</a></code>.  If this happens,
the simulation will stop.  It may help to first use <code><a href="#topic+find_constants">find_constants</a></code> for each continuous variable to
determine if a vector of sixth cumulant correction values is needed.  The solutions can be used as starting values (see <code>cstart</code> below).
If the standardized cumulants are obtained from <code>calc_theory</code>, the user may need to use rounded values as inputs (i.e.
<code>skews = round(skews, 8)</code>).
</p>
<p>2) In addition, the kurtosis may be outside the region of possible values.  There is an associated lower boundary for kurtosis associated
with a given skew (for Fleishman's method) or skew and fifth and sixth cumulants (for Headrick's method).  Use
<code><a href="#topic+calc_lower_skurt">calc_lower_skurt</a></code> to determine the boundary for a given set of cumulants.
</p>
<p>3) As mentioned above, the feasibility of the final correlation matrix rho, given the
distribution parameters, should be checked first using <code><a href="#topic+valid_corr">valid_corr</a></code>.  This function either checks
if a given <code>rho</code> is plausible or returns the lower and upper final correlation limits.  It should be noted that even if a target
correlation matrix is within the &quot;plausible range,&quot; it still may not be possible to achieve the desired matrix.  This happens most
frequently when generating ordinal variables (r &gt;= 2 categories).  The error loop frequently fixes these problems.
</p>


<h3>References</h3>

<p>Amatya A &amp; Demirtas H (2015). Simultaneous generation of multivariate mixed data with Poisson and normal marginals.
Journal of Statistical Computation and Simulation, 85(15): 3129-39. doi: <a href="http://doi.org/10.1080/00949655.2014.953534">10.1080/00949655.2014.953534</a>.
</p>
<p>Barbiero A, Ferrari PA (2015). GenOrd: Simulation of Discrete Random Variables with Given
Correlation Matrix and Marginal Distributions. R package version 1.4.0. <a href="https://CRAN.R-project.org/package=GenOrd">https://CRAN.R-project.org/package=GenOrd</a>
</p>
<p>Demirtas H &amp; Hedeker D (2011). A practical way for computing approximate lower and upper correlation bounds.
American Statistician, 65(2): 104-109. doi: <a href="http://doi.org/10.1198/tast.2011.10090">10.1198/tast.2011.10090</a>.
</p>
<p>Demirtas H, Hedeker D, &amp; Mermelstein RJ (2012). Simulation of massive public health data by power polynomials.
Statistics in Medicine, 31(27): 3337-3346. doi: <a href="http://doi.org/10.1002/sim.5362">10.1002/sim.5362</a>.
</p>
<p>Ferrari PA, Barbiero A (2012). Simulating ordinal data. Multivariate Behavioral Research, 47(4): 566-589.
doi: <a href="http://doi.org/10.1080/00273171.2012.692630">10.1080/00273171.2012.692630</a>.
</p>
<p>Fleishman AI (1978). A Method for Simulating Non-normal Distributions. Psychometrika, 43, 521-532. doi: <a href="http://doi.org/10.1007/BF02293811">10.1007/BF02293811</a>.
</p>
<p>Frechet M.  Sur les tableaux de correlation dont les marges sont donnees.  Ann. l'Univ. Lyon SectA.  1951;14:53-77.
</p>
<p>Hasselman B (2018). nleqslv: Solve Systems of Nonlinear Equations. R package version 3.3.2.
<a href="https://CRAN.R-project.org/package=nleqslv">https://CRAN.R-project.org/package=nleqslv</a>
</p>
<p>Headrick TC (2002). Fast Fifth-order Polynomial Transforms for Generating Univariate and Multivariate
Non-normal Distributions. Computational Statistics &amp; Data Analysis, 40(4):685-711. doi: <a href="http://doi.org/10.1016/S0167-9473(02)00072-5">10.1016/S0167-9473(02)00072-5</a>.
(<a href="http://www.sciencedirect.com/science/article/pii/S0167947302000725">ScienceDirect</a>)
</p>
<p>Headrick TC (2004). On Polynomial Transformations for Simulating Multivariate Nonnormal Distributions.
Journal of Modern Applied Statistical Methods, 3(1), 65-71. doi: <a href="http://doi.org/10.22237/jmasm/1083370080">10.22237/jmasm/1083370080</a>.
</p>
<p>Headrick TC, Kowalchuk RK (2007). The Power Method Transformation: Its Probability Density Function, Distribution
Function, and Its Further Use for Fitting Data. Journal of Statistical Computation and Simulation, 77, 229-249. doi: <a href="http://doi.org/10.1080/10629360600605065">10.1080/10629360600605065</a>.
</p>
<p>Headrick TC, Sawilowsky SS (1999). Simulating Correlated Non-normal Distributions: Extending the Fleishman Power
Method. Psychometrika, 64, 25-35. doi: <a href="http://doi.org/10.1007/BF02294317">10.1007/BF02294317</a>.
</p>
<p>Headrick TC, Sheng Y, &amp; Hodis FA (2007). Numerical Computing and Graphics for the Power Method Transformation Using
Mathematica. Journal of Statistical Software, 19(3), 1 - 17. doi: <a href="http://doi.org/10.18637/jss.v019.i03">10.18637/jss.v019.i03</a>.
</p>
<p>Higham N (2002). Computing the nearest correlation matrix - a problem from finance; IMA Journal of Numerical Analysis 22: 329-343.
</p>
<p>Hoeffding W. Scale-invariant correlation theory. In: Fisher NI, Sen PK, editors. The collected works of Wassily Hoeffding.
New York: Springer-Verlag; 1994. p. 57-107.
</p>
<p>Olsson U, Drasgow F, &amp; Dorans NJ (1982). The Polyserial Correlation Coefficient. Psychometrika, 47(3): 337-47.
doi: <a href="http://doi.org/10.1007/BF02294164">10.1007/BF02294164</a>.
</p>
<p>Vale CD &amp; Maurelli VA (1983). Simulating Multivariate Nonnormal Distributions. Psychometrika, 48, 465-471. doi: <a href="http://doi.org/10.1007/BF02293687">10.1007/BF02293687</a>.
</p>
<p>Varadhan R, Gilbert P (2009). BB: An R Package for Solving a Large System of Nonlinear Equations and for
Optimizing a High-Dimensional Nonlinear Objective Function, J. Statistical Software, 32(4). doi: <a href="http://doi.org/10.18637/jss.v032.i04">10.18637/jss.v032.i04</a>.
<a href="http://www.jstatsoft.org/v32/i04/">http://www.jstatsoft.org/v32/i04/</a>
</p>
<p>Yahav I &amp; Shmueli G (2012). On Generating Multivariate Poisson Data in Management Science Applications. Applied Stochastic
Models in Business and Industry, 28(1): 91-102. doi: <a href="http://doi.org/10.1002/asmb.901">10.1002/asmb.901</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+find_constants">find_constants</a></code>, <code><a href="#topic+findintercorr">findintercorr</a></code>,
<code><a href="BB.html#topic+multiStart">multiStart</a></code>, <code><a href="nleqslv.html#topic+nleqslv">nleqslv</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>Sim1 &lt;- rcorrvar(n = 1000, k_cat = 1, k_cont = 1, method = "Polynomial",
  means = 0, vars = 1, skews = 0, skurts = 0, fifths = 0, sixths = 0,
  marginal = list(c(1/3, 2/3)), support = list(0:2),
  rho = matrix(c(1, 0.4, 0.4, 1), 2, 2))

## Not run: 

# Binary, Ordinal, Continuous, Poisson, and Negative Binomial Variables

options(scipen = 999)
seed &lt;- 1234
n &lt;- 10000

Dist &lt;- c("Logistic", "Weibull")
Params &lt;- list(c(0, 1), c(3, 5))
Stcum1 &lt;- calc_theory(Dist[1], Params[[1]])
Stcum2 &lt;- calc_theory(Dist[2], Params[[2]])
Stcum &lt;- rbind(Stcum1, Stcum2)
rownames(Stcum) &lt;- Dist
colnames(Stcum) &lt;- c("mean", "sd", "skew", "skurtosis", "fifth", "sixth")
Stcum
Six &lt;- list(seq(1.7, 1.8, 0.01), seq(0.10, 0.25, 0.01))
marginal &lt;- list(0.3)
lam &lt;- 0.5
size &lt;- 2
prob &lt;- 0.75

Rey &lt;- matrix(0.4, 5, 5)
diag(Rey) &lt;- 1

# Make sure Rey is within upper and lower correlation limits
valid &lt;- valid_corr(k_cat = 1, k_cont = 2, k_pois = 1, k_nb = 1,
                    method = "Polynomial", means = Stcum[, 1],
                    vars = Stcum[, 2]^2, skews = Stcum[, 3],
                    skurts = Stcum[, 4], fifths = Stcum[, 5],
                    sixths = Stcum[, 6], Six = Six, marginal = marginal,
                    lam = lam, size = size, prob = prob, rho = Rey,
                    seed = seed)

# Simulate variables without error loop
Sim1 &lt;- rcorrvar(n = n, k_cat = 1, k_cont = 2, k_pois = 1, k_nb = 1,
                 method = "Polynomial", means = Stcum[, 1],
                 vars = Stcum[, 2]^2, skews = Stcum[, 3],
                 skurts = Stcum[, 4], fifths = Stcum[, 5],
                 sixths = Stcum[, 6], Six = Six, marginal = marginal,
                 lam = lam, size = size, prob = prob, rho = Rey,
                 seed = seed)
names(Sim1)

# Look at the maximum correlation error
Sim1$maxerr

Sim1_error = round(Sim1$correlations - Rey, 6)

# interquartile-range of correlation errors
quantile(as.numeric(Sim1_error), 0.25)
quantile(as.numeric(Sim1_error), 0.75)

# Simulate variables with error loop
Sim1_EL &lt;- rcorrvar(n = n, k_cat = 1, k_cont = 2,
                    k_pois = 1, k_nb = 1, method = "Polynomial",
                    means = Stcum[, 1], vars = Stcum[, 2]^2,
                    skews = Stcum[, 3], skurts = Stcum[, 4],
                    fifths = Stcum[, 5], sixths = Stcum[, 6],
                    Six = Six, marginal = marginal, lam = lam,
                    size = size, prob = prob, rho = Rey,
                    seed = seed, errorloop = TRUE)
# Look at the maximum correlation error
Sim1_EL$maxerr

EL_error = round(Sim1_EL$correlations - Rey, 6)

# interquartile-range of correlation errors
quantile(as.numeric(EL_error), 0.25)
quantile(as.numeric(EL_error), 0.75)

# Look at results
# Ordinal variables
Sim1_EL$summary_ordinal

# Continuous variables
round(Sim1_EL$constants, 6)
round(Sim1_EL$summary_continuous, 6)
round(Sim1_EL$summary_targetcont, 6)
Sim1_EL$valid.pdf

# Count variables
Sim1_EL$summary_Poisson
Sim1_EL$summary_Neg_Bin

# Generate Plots

# Logistic (1st continuous variable)
# 1) Simulated Data CDF (find cumulative probability up to y = 0.5)
plot_sim_cdf(Sim1_EL$continuous_variables[, 1], calc_cprob = TRUE,
             delta = 0.5)

# 2) Simulated Data and Target Distribution PDFs
plot_sim_pdf_theory(Sim1_EL$continuous_variables[, 1], Dist = "Logistic",
                    params = c(0, 1))

# 3) Simulated Data and Target Distribution
plot_sim_theory(Sim1_EL$continuous_variables[, 1], Dist = "Logistic",
                params = c(0, 1))


## End(Not run)
</code></pre>

<hr>
<h2 id='rcorrvar2'>Generation of Correlated Ordinal, Continuous, Poisson, and/or Negative Binomial Variables: Correlation Method 2</h2><span id='topic+rcorrvar2'></span>

<h3>Description</h3>

<p>This function simulates <code>k_cat</code> ordinal, <code>k_cont</code> continuous, <code>k_pois</code> Poisson, and/or <code>k_nb</code>
Negative Binomial variables with
a specified correlation matrix <code>rho</code>.  The variables are generated from multivariate normal variables with intermediate correlation
matrix <code>Sigma</code>, calculated by <code><a href="#topic+findintercorr2">findintercorr2</a></code>, and then transformed.  The <em>ordering</em> of the
variables in <code>rho</code> must be <em>ordinal</em> (r &gt;= 2 categories), <em>continuous</em>, <em>Poisson</em>, and <em>Negative Binomial</em>
(note that it is possible for <code>k_cat</code>, <code>k_cont</code>, <code>k_pois</code>, and/or <code>k_nb</code> to be 0).  The vignette
<b>Overall Workflow for Data Simulation</b> provides a detailed example discussing the step-by-step simulation process and comparing
methods 1 and 2.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rcorrvar2(n = 10000, k_cont = 0, k_cat = 0, k_pois = 0, k_nb = 0,
  method = c("Fleishman", "Polynomial"), means = NULL, vars = NULL,
  skews = NULL, skurts = NULL, fifths = NULL, sixths = NULL,
  Six = list(), marginal = list(), support = list(), lam = NULL,
  pois_eps = rep(0.0001, 2), size = NULL, prob = NULL, mu = NULL,
  nb_eps = rep(0.0001, 2), Sigma = NULL, rho = NULL, cstart = NULL,
  seed = 1234, errorloop = FALSE, epsilon = 0.001, maxit = 1000,
  extra_correct = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rcorrvar2_+3A_n">n</code></td>
<td>
<p>the sample size (i.e. the length of each simulated variable; default = 10000)</p>
</td></tr>
<tr><td><code id="rcorrvar2_+3A_k_cont">k_cont</code></td>
<td>
<p>the number of continuous variables (default = 0)</p>
</td></tr>
<tr><td><code id="rcorrvar2_+3A_k_cat">k_cat</code></td>
<td>
<p>the number of ordinal (r &gt;= 2 categories) variables (default = 0)</p>
</td></tr>
<tr><td><code id="rcorrvar2_+3A_k_pois">k_pois</code></td>
<td>
<p>the number of Poisson variables (default = 0)</p>
</td></tr>
<tr><td><code id="rcorrvar2_+3A_k_nb">k_nb</code></td>
<td>
<p>the number of Negative Binomial variables (default = 0)</p>
</td></tr>
<tr><td><code id="rcorrvar2_+3A_method">method</code></td>
<td>
<p>the method used to generate the k_cont continuous variables.  &quot;Fleishman&quot; uses Fleishman's third-order polynomial transformation
and &quot;Polynomial&quot; uses Headrick's fifth-order transformation.</p>
</td></tr>
<tr><td><code id="rcorrvar2_+3A_means">means</code></td>
<td>
<p>a vector of means for the k_cont continuous variables (i.e. = rep(0, k_cont))</p>
</td></tr>
<tr><td><code id="rcorrvar2_+3A_vars">vars</code></td>
<td>
<p>a vector of variances (i.e. = rep(1, k_cont))</p>
</td></tr>
<tr><td><code id="rcorrvar2_+3A_skews">skews</code></td>
<td>
<p>a vector of skewness values (i.e. = rep(0, k_cont))</p>
</td></tr>
<tr><td><code id="rcorrvar2_+3A_skurts">skurts</code></td>
<td>
<p>a vector of standardized kurtoses (kurtosis - 3, so that normal variables have a value of 0; i.e. = rep(0, k_cont))</p>
</td></tr>
<tr><td><code id="rcorrvar2_+3A_fifths">fifths</code></td>
<td>
<p>a vector of standardized fifth cumulants (not necessary for <code>method</code> = &quot;Fleishman&quot;; i.e. = rep(0, k_cont))</p>
</td></tr>
<tr><td><code id="rcorrvar2_+3A_sixths">sixths</code></td>
<td>
<p>a vector of standardized sixth cumulants (not necessary for <code>method</code> = &quot;Fleishman&quot;; i.e. = rep(0, k_cont))</p>
</td></tr>
<tr><td><code id="rcorrvar2_+3A_six">Six</code></td>
<td>
<p>a list of vectors of correction values to add to the sixth cumulants if no valid pdf constants are found,
ex: <code>Six = list(seq(0.01, 2,by = 0.01), seq(1, 10,by = 0.5))</code>; if no correction is desired for variable Y_i, set set the i-th list
component equal to NULL</p>
</td></tr>
<tr><td><code id="rcorrvar2_+3A_marginal">marginal</code></td>
<td>
<p>a list of length equal to <code>k_cat</code>; the i-th element is a vector of the cumulative
probabilities defining the marginal distribution of the i-th variable;
if the variable can take r values, the vector will contain r - 1 probabilities (the r-th is assumed to be 1; default = list());
for binary variables, these should be input the same as for ordinal variables with more than 2 categories (i.e. the user-specified
probability is the probability of the 1st category, which has the smaller support value)</p>
</td></tr>
<tr><td><code id="rcorrvar2_+3A_support">support</code></td>
<td>
<p>a list of length equal to <code>k_cat</code>; the i-th element is a vector containing the r ordered support values;
if not provided (i.e. <code>support = list()</code>), the default is for the i-th element to be the
vector 1, ..., r</p>
</td></tr>
<tr><td><code id="rcorrvar2_+3A_lam">lam</code></td>
<td>
<p>a vector of lambda (&gt; 0) constants for the Poisson variables (see <code><a href="stats.html#topic+Poisson">Poisson</a></code>)</p>
</td></tr>
<tr><td><code id="rcorrvar2_+3A_pois_eps">pois_eps</code></td>
<td>
<p>a vector of length k_pois containing the truncation values (default = rep(0.0001, 2))</p>
</td></tr>
<tr><td><code id="rcorrvar2_+3A_size">size</code></td>
<td>
<p>a vector of size parameters for the Negative Binomial variables (see <code><a href="stats.html#topic+NegBinomial">NegBinomial</a></code>)</p>
</td></tr>
<tr><td><code id="rcorrvar2_+3A_prob">prob</code></td>
<td>
<p>a vector of success probability parameters</p>
</td></tr>
<tr><td><code id="rcorrvar2_+3A_mu">mu</code></td>
<td>
<p>a vector of mean parameters (*Note: either <code>prob</code> or <code>mu</code> should be supplied for all Negative Binomial variables,
not a mixture; default = NULL)</p>
</td></tr>
<tr><td><code id="rcorrvar2_+3A_nb_eps">nb_eps</code></td>
<td>
<p>a vector of length k_nb containing the truncation values (default = rep(0.0001, 2))</p>
</td></tr>
<tr><td><code id="rcorrvar2_+3A_sigma">Sigma</code></td>
<td>
<p>an intermediate correlation matrix to use if the user wants to provide one (default = NULL)</p>
</td></tr>
<tr><td><code id="rcorrvar2_+3A_rho">rho</code></td>
<td>
<p>the target correlation matrix (<em>must be ordered ordinal, continuous, Poisson, Negative Binomial</em>; default = NULL)</p>
</td></tr>
<tr><td><code id="rcorrvar2_+3A_cstart">cstart</code></td>
<td>
<p>a list containing initial values for root-solving algorithm used in <code><a href="#topic+find_constants">find_constants</a></code>
(see <code><a href="BB.html#topic+multiStart">multiStart</a></code> for <code>method</code> = &quot;Fleishman&quot; or <code><a href="nleqslv.html#topic+nleqslv">nleqslv</a></code> for <code>method</code> = &quot;Polynomial&quot;).
If user specified, each list element must be input as a matrix. If no starting values are specified for a given continuous
variable, that list element should be NULL.  If NULL and all 4 standardized cumulants (rounded to 3 digits) are within
0.01 of those in Headrick's common distribution table (see <code><a href="#topic+Headrick.dist">Headrick.dist</a></code>
data), uses his constants as starting values; else, generates n sets of random starting values from
uniform distributions.</p>
</td></tr>
<tr><td><code id="rcorrvar2_+3A_seed">seed</code></td>
<td>
<p>the seed value for random number generation (default = 1234)</p>
</td></tr>
<tr><td><code id="rcorrvar2_+3A_errorloop">errorloop</code></td>
<td>
<p>if TRUE, uses <code><a href="#topic+error_loop">error_loop</a></code> to attempt to correct the final correlation
(default = FALSE)</p>
</td></tr>
<tr><td><code id="rcorrvar2_+3A_epsilon">epsilon</code></td>
<td>
<p>the maximum acceptable error between the final and target correlation matrices (default = 0.001)
in the calculation of ordinal intermediate correlations with <code><a href="#topic+ordnorm">ordnorm</a></code> or in the error loop</p>
</td></tr>
<tr><td><code id="rcorrvar2_+3A_maxit">maxit</code></td>
<td>
<p>the maximum number of iterations to use (default = 1000) in the calculation of ordinal
intermediate correlations with <code><a href="#topic+ordnorm">ordnorm</a></code> or in the error loop</p>
</td></tr>
<tr><td><code id="rcorrvar2_+3A_extra_correct">extra_correct</code></td>
<td>
<p>if TRUE, within each variable pair, if the maximum correlation error is still greater than 0.1, the intermediate
correlation is set equal to the target correlation (with the assumption that the calculated final correlation will be
less than 0.1 away from the target)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list whose components vary based on the type of simulated variables.  Simulated variables are returned as data.frames:
</p>
<p>If <b>ordinal variables</b> are produced:
</p>
<p><code>ordinal_variables</code> the generated ordinal variables,
</p>
<p><code>summary_ordinal</code> a list, where the i-th element contains a data.frame with column 1 = target cumulative probabilities and
column 2 = simulated cumulative probabilities for ordinal variable Y_i
</p>
<p>If <b>continuous variables</b> are produced:
</p>
<p><code>constants</code> a data.frame of the constants,
</p>
<p><code>continuous_variables</code> the generated continuous variables,
</p>
<p><code>summary_continuous</code> a data.frame containing a summary of each variable,
</p>
<p><code>summary_targetcont</code> a data.frame containing a summary of the target variables,
</p>
<p><code>sixth_correction</code> a vector of sixth cumulant correction values,
</p>
<p><code>valid.pdf</code> a vector where the i-th element is &quot;TRUE&quot; if the constants for the i-th continuous variable generate a valid pdf,
else &quot;FALSE&quot;
</p>
<p>If <b>Poisson variables</b> are produced:
</p>
<p><code>Poisson_variables</code> the generated Poisson variables,
</p>
<p><code>summary_Poisson</code> a data.frame containing a summary of each variable
</p>
<p>If <b>Negative Binomial variables</b> are produced:
</p>
<p><code>Neg_Bin_variables</code> the generated Negative Binomial variables,
</p>
<p><code>summary_Neg_Bin</code> a data.frame containing a summary of each variable
</p>
<p>Additionally, the following elements:
</p>
<p><code>correlations</code> the final correlation matrix,
</p>
<p><code>Sigma1</code> the intermediate correlation before the error loop,
</p>
<p><code>Sigma2</code> the intermediate correlation matrix after the error loop,
</p>
<p><code>Constants_Time</code> the time in minutes required to calculate the constants,
</p>
<p><code>Intercorrelation_Time</code> the time in minutes required to calculate the intermediate correlation matrix,
</p>
<p><code>Error_Loop_Time</code> the time in minutes required to use the error loop,
</p>
<p><code>Simulation_Time</code> the total simulation time in minutes,
</p>
<p><code>niter</code> a matrix of the number of iterations used for each variable in the error loop,
</p>
<p><code>maxerr</code> the maximum final correlation error (from the target rho).
</p>
<p>If a particular element is not required, the result is NULL for that element.
</p>


<h3>Variable Types and Required Inputs</h3>

<p>1) <b>Continuous Variables:</b> Continuous variables are simulated using either Fleishman's third-order (<code>method</code> = &quot;Fleishman&quot;,
doi: <a href="http://doi.org/10.1007/BF02293811">10.1007/BF02293811</a>) or Headrick's fifth-order (<code>method</code> = &quot;Polynomial&quot;, doi: <a href="http://doi.org/10.1016/S0167-9473(02)00072-5">10.1016/S0167-9473(02)00072-5</a>) power method
transformation.  This is a computationally efficient algorithm that simulates continuous distributions through the method of moments.
It works by matching standardized cumulants &ndash; the first four (mean, variance, skew, and standardized kurtosis) for Fleishman's method, or
the first six (mean, variance, skew, standardized kurtosis, and standardized fifth and sixth cumulants) for Headrick's method.  The
transformation is expressed as follows:
</p>
<p><code class="reqn">Y = c_{0} + c_{1} * Z + c_{2} * Z^2 + c_{3} * Z^3 + c_{4} * Z^4 + c_{5} * Z^5</code>,
</p>
<p>where <code class="reqn">Z ~ N(0,1)</code>, and <code class="reqn">c_{4}</code> and <code class="reqn">c_{5}</code> both equal <code class="reqn">0</code> for Fleishman's method.  The real constants are calculated by
<code><a href="#topic+find_constants">find_constants</a></code>.  All variables are simulated with mean <code class="reqn">0</code> and variance <code class="reqn">1</code>, and then transformed
to the specified mean and variance at the end.
</p>
<p>The required parameters for simulating continuous variables include: mean, variance, skewness, standardized kurtosis (kurtosis - 3), and
standardized fifth and sixth cumulants (for <code>method</code> = &quot;Polynomial&quot;).  If the goal is to simulate a theoretical distribution
(i.e. Gamma, Beta, Logistic, etc.), these values can be obtained using <code><a href="#topic+calc_theory">calc_theory</a></code>.  If the goal is to
mimic an empirical data set, these values can be found using <code><a href="#topic+calc_moments">calc_moments</a></code> (using the method of moments) or
<code><a href="#topic+calc_fisherk">calc_fisherk</a></code> (using Fisher's k-statistics).  If the standardized cumulants
are obtained from <code>calc_theory</code>, the user may need to use rounded values as inputs (i.e. <code>skews = round(skews, 8)</code>).  Due to the nature
of the integration involved in <code>calc_theory</code>, the results are approximations.  Greater accuracy can be achieved by increasing the number of
subdivisions (<code>sub</code>) used in the integration process.  For example, in order to ensure that skew is exactly 0 for symmetric distributions.
</p>
<p>For some sets of cumulants, it is either not possible to find power method constants or the calculated constants do not generate valid power
method pdfs.  In these situations, adding a value to the
sixth cumulant may provide solutions (see <code><a href="#topic+find_constants">find_constants</a></code>).  When using Headrick's fifth-order approximation,
if simulation results indicate that a
continuous variable does not generate a valid pdf, the user can try <code><a href="#topic+find_constants">find_constants</a></code> with various sixth
cumulant correction vectors to determine if a valid pdf can be found.
</p>
<p>2) <b>Binary and Ordinal Variables:</b> Ordinal variables (<code class="reqn">r \ge 2</code> categories) are generated by discretizing the standard normal variables
at quantiles.  These quantiles are determined by evaluating the inverse standard normal cdf at the cumulative probabilities defined by each
variable's marginal distribution.  The required inputs for ordinal variables are the cumulative marginal probabilities and support values
(if desired).  The probabilities should be combined into a list of length equal to the number of ordinal variables.  The <code class="reqn">i^{th}</code> element
is a vector of the cumulative probabilities defining the marginal distribution of the <code class="reqn">i^{th}</code> variable.  If the variable can take
<code class="reqn">r</code> values, the vector will contain <code class="reqn">r - 1</code> probabilities (the <code class="reqn">r^{th}</code> is assumed to be <code class="reqn">1</code>).
</p>
<p><em>Note for binary variables:</em> the user-suppled probability should be the probability of the <code class="reqn">1^{st}</code> (lower) support value.  This would
ordinarily be considered the probability of <em>failure</em> (<code class="reqn">q</code>), while the probability of the <code class="reqn">2^{nd}</code> (upper) support value would be
considered the probability of <em>success</em> (<code class="reqn">p = 1 - q</code>).  The support values should be combined into a separate list.  The <code class="reqn">i^{th}</code>
element is a vector containing the <code class="reqn">r</code> ordered support values.
</p>
<p>3) <b>Count Variables:</b> Count variables are generated using the inverse cdf method.  The cumulative distribution function of a standard
normal variable has a uniform distribution.  The appropriate quantile function <code class="reqn">F_{Y}^{-1}</code> is applied to this uniform variable with the
designated parameters to generate the count variable: <code class="reqn">Y = F_{y}^{-1}(\Phi(Z))</code>.  For Poisson variables, the lambda (mean) value should be
given.  For Negative Binomial variables, the size (target number of successes) and either the success probability or the mean should be given.
The Negative Binomial variable represents the number of failures which occur in a sequence of Bernoulli trials before the target number of
successes is achieved.  In addition, a vector of total cumulative probability truncation values should be provided (one for Poisson and one for
Negative Binomial).  These values represent the amount of probability removed from the range of the cdf's <code class="reqn">F_{Y}</code> when creating finite
supports.  The value may vary by variable, but a good default value is 0.0001 (suggested by Barbiero &amp; Ferrari, 2015, doi: <a href="http://doi.org/10.1002/asmb.2072">10.1002/asmb.2072</a>).
</p>
<p>More details regarding the variable types can be found in the <b>Variable Types</b> vignette.
</p>


<h3>Overview of Correlation Method 2</h3>

<p>The intermediate correlations used in correlation method 2 are less simulation based than those in correlation method 1, and no seed is needed.
Their calculations involve greater utilization of correction loops which make iterative adjustments until a maximum error
has been reached (if possible).  In addition, method 2 differs from method 1 in the following ways:
</p>
<p>1) The intermediate correlations involving <b>count variables</b> are based on the methods of Barbiero &amp; Ferrari (2012,
doi: <a href="http://doi.org/10.1080/00273171.2012.692630">10.1080/00273171.2012.692630</a>, 2015, doi: <a href="http://doi.org/10.1002/asmb.2072">10.1002/asmb.2072</a>).
The Poisson or Negative Binomial support is made finite by removing a small user-specified value (i.e. 1e-06) from the total
cumulative probability.  This truncation factor may differ for each count variable.  The count variables are subsequently
treated as ordinal and intermediate correlations are calculated using the correction loop of
<code><a href="#topic+ordnorm">ordnorm</a></code>.
</p>
<p>2) The <b>continuous - count variable</b> correlations are based on an extension of the method of Demirtas et al. (2012,
doi: <a href="http://doi.org/10.1002/sim.5362">10.1002/sim.5362</a>), and the count
variables are treated as ordinal.  The correction factor is the product of the power method correlation between the
continuous variable and the normal variable used to generate it (see Headrick &amp; Kowalchuk, 2007, doi: <a href="http://doi.org/10.1080/10629360600605065">10.1080/10629360600605065</a>)
and the point-polyserial correlation between the ordinalized count variable and the normal variable used to generate it (see Olsson et al., 1982,
doi: <a href="http://doi.org/10.1007/BF02294164">10.1007/BF02294164</a>).
The intermediate correlations are the ratio of the target correlations to the correction factor.
</p>
<p>Please see the <b>Comparison of Method 1 and Method 2</b> vignette for more information and an step-by-step overview of the
simulation process.
</p>


<h3>Choice of Fleishman's third-order or Headrick's fifth-order method</h3>

<p>Using the fifth-order approximation allows additional control over the fifth and sixth moments of the generated distribution, improving
accuracy.  In addition, the range of feasible standardized kurtosis values, given skew and standardized fifth (<code class="reqn">\gamma_{3}</code>) and sixth
(<code class="reqn">\gamma_{4}</code>) cumulants, is larger than with Fleishman's method (see <code><a href="#topic+calc_lower_skurt">calc_lower_skurt</a></code>).
For example, the Fleishman method can not be used to generate a
non-normal distribution with a ratio of <code class="reqn">\gamma_{3}^2/\gamma_{4} &gt; 9/14</code> (see Headrick &amp; Kowalchuk, 2007).
This eliminates the
Chi-squared family of distributions, which has a constant ratio of <code class="reqn">\gamma_{3}^2/\gamma_{4} = 2/3</code>.  However, if the fifth and
sixth cumulants do not exist, the Fleishman approximation should be used.
</p>


<h3>Reasons for Function Errors</h3>

<p>1) The most likely cause for function errors is that no solutions to <code><a href="#topic+fleish">fleish</a></code> or
<code><a href="#topic+poly">poly</a></code> converged when using <code><a href="#topic+find_constants">find_constants</a></code>.  If this happens,
the simulation will stop.  It may help to first use <code><a href="#topic+find_constants">find_constants</a></code> for each continuous variable to
determine if a vector of sixth cumulant correction values is needed.  The solutions can be used as starting values (see <code>cstart</code> below).
If the standardized cumulants are obtained from <code>calc_theory</code>, the user may need to use rounded values as inputs (i.e.
<code>skews = round(skews, 8)</code>).
</p>
<p>2) In addition, the kurtosis may be outside the region of possible values.  There is an associated lower boundary for kurtosis associated
with a given skew (for Fleishman's method) or skew and fifth and sixth cumulants (for Headrick's method).  Use
<code><a href="#topic+calc_lower_skurt">calc_lower_skurt</a></code> to determine the boundary for a given set of cumulants.
</p>
<p>3) As mentioned above, the feasibility of the final correlation matrix rho, given the
distribution parameters, should be checked first using <code><a href="#topic+valid_corr2">valid_corr2</a></code>.  This function either checks
if a given <code>rho</code> is plausible or returns the lower and upper final correlation limits.  It should be noted that even if a target
correlation matrix is within the &quot;plausible range,&quot; it still may not be possible to achieve the desired matrix.  This happens most
frequently when generating ordinal variables (r &gt;= 2 categories).  The error loop frequently fixes these problems.
</p>


<h3>References</h3>

<p>Barbiero A &amp; Ferrari PA (2015). Simulation of correlated Poisson variables. Applied Stochastic Models in
Business and Industry, 31: 669-80. doi: <a href="http://doi.org/10.1002/asmb.2072">10.1002/asmb.2072</a>.
</p>
<p>Barbiero A, Ferrari PA (2015). GenOrd: Simulation of Discrete Random Variables with Given
Correlation Matrix and Marginal Distributions. R package version 1.4.0. <a href="https://CRAN.R-project.org/package=GenOrd">https://CRAN.R-project.org/package=GenOrd</a>
</p>
<p>Demirtas H &amp; Hedeker D (2011). A practical way for computing approximate lower and upper correlation bounds.
American Statistician, 65(2): 104-109. doi: <a href="http://doi.org/10.1198/tast.2011.10090">10.1198/tast.2011.10090</a>.
</p>
<p>Demirtas H, Hedeker D, &amp; Mermelstein RJ (2012). Simulation of massive public health data by power polynomials.
Statistics in Medicine, 31(27): 3337-3346. doi: <a href="http://doi.org/10.1002/sim.5362">10.1002/sim.5362</a>.
</p>
<p>Ferrari PA, Barbiero A (2012). Simulating ordinal data. Multivariate Behavioral Research, 47(4): 566-589.
doi: <a href="http://doi.org/10.1080/00273171.2012.692630">10.1080/00273171.2012.692630</a>.
</p>
<p>Fleishman AI (1978). A Method for Simulating Non-normal Distributions. Psychometrika, 43, 521-532. doi: <a href="http://doi.org/10.1007/BF02293811">10.1007/BF02293811</a>.
</p>
<p>Frechet M.  Sur les tableaux de correlation dont les marges sont donnees.  Ann. l'Univ. Lyon SectA.  1951;14:53-77.
</p>
<p>Hasselman B (2018). nleqslv: Solve Systems of Nonlinear Equations. R package version 3.3.2.
<a href="https://CRAN.R-project.org/package=nleqslv">https://CRAN.R-project.org/package=nleqslv</a>
</p>
<p>Headrick TC (2002). Fast Fifth-order Polynomial Transforms for Generating Univariate and Multivariate
Non-normal Distributions. Computational Statistics &amp; Data Analysis, 40(4):685-711. doi: <a href="http://doi.org/10.1016/S0167-9473(02)00072-5">10.1016/S0167-9473(02)00072-5</a>.
(<a href="http://www.sciencedirect.com/science/article/pii/S0167947302000725">ScienceDirect</a>)
</p>
<p>Headrick TC (2004). On Polynomial Transformations for Simulating Multivariate Nonnormal Distributions.
Journal of Modern Applied Statistical Methods, 3(1), 65-71. doi: <a href="http://doi.org/10.22237/jmasm/1083370080">10.22237/jmasm/1083370080</a>.
</p>
<p>Headrick TC, Kowalchuk RK (2007). The Power Method Transformation: Its Probability Density Function, Distribution
Function, and Its Further Use for Fitting Data. Journal of Statistical Computation and Simulation, 77, 229-249. doi: <a href="http://doi.org/10.1080/10629360600605065">10.1080/10629360600605065</a>.
</p>
<p>Headrick TC, Sawilowsky SS (1999). Simulating Correlated Non-normal Distributions: Extending the Fleishman Power
Method. Psychometrika, 64, 25-35. doi: <a href="http://doi.org/10.1007/BF02294317">10.1007/BF02294317</a>.
</p>
<p>Headrick TC, Sheng Y, &amp; Hodis FA (2007). Numerical Computing and Graphics for the Power Method Transformation Using
Mathematica. Journal of Statistical Software, 19(3), 1 - 17. doi: <a href="http://doi.org/10.18637/jss.v019.i03">10.18637/jss.v019.i03</a>.
</p>
<p>Higham N (2002). Computing the nearest correlation matrix - a problem from finance; IMA Journal of Numerical Analysis 22: 329-343.
</p>
<p>Hoeffding W. Scale-invariant correlation theory. In: Fisher NI, Sen PK, editors. The collected works of Wassily Hoeffding.
New York: Springer-Verlag; 1994. p. 57-107.
</p>
<p>Olsson U, Drasgow F, &amp; Dorans NJ (1982). The Polyserial Correlation Coefficient. Psychometrika, 47(3): 337-47.
doi: <a href="http://doi.org/10.1007/BF02294164">10.1007/BF02294164</a>.
</p>
<p>Vale CD &amp; Maurelli VA (1983). Simulating Multivariate Nonnormal Distributions. Psychometrika, 48, 465-471. doi: <a href="http://doi.org/10.1007/BF02293687">10.1007/BF02293687</a>.
</p>
<p>Varadhan R, Gilbert P (2009). BB: An R Package for Solving a Large System of Nonlinear Equations and for
Optimizing a High-Dimensional Nonlinear Objective Function, J. Statistical Software, 32(4). doi: <a href="http://doi.org/10.18637/jss.v032.i04">10.18637/jss.v032.i04</a>.
<a href="http://www.jstatsoft.org/v32/i04/">http://www.jstatsoft.org/v32/i04/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+find_constants">find_constants</a></code>, <code><a href="#topic+findintercorr2">findintercorr2</a></code>,
<code><a href="BB.html#topic+multiStart">multiStart</a></code>, <code><a href="nleqslv.html#topic+nleqslv">nleqslv</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>Sim1 &lt;- rcorrvar2(n = 1000, k_cat = 1, k_cont = 1, method = "Polynomial",
  means = 0, vars = 1, skews = 0, skurts = 0, fifths = 0, sixths = 0,
  marginal = list(c(1/3, 2/3)), support = list(0:2),
  rho = matrix(c(1, 0.4, 0.4, 1), 2, 2))

## Not run: 

# Binary, Ordinal, Continuous, Poisson, and Negative Binomial Variables

options(scipen = 999)
seed &lt;- 1234
n &lt;- 10000

Dist &lt;- c("Logistic", "Weibull")
Params &lt;- list(c(0, 1), c(3, 5))
Stcum1 &lt;- calc_theory(Dist[1], Params[[1]])
Stcum2 &lt;- calc_theory(Dist[2], Params[[2]])
Stcum &lt;- rbind(Stcum1, Stcum2)
rownames(Stcum) &lt;- Dist
colnames(Stcum) &lt;- c("mean", "sd", "skew", "skurtosis", "fifth", "sixth")
Stcum
Six &lt;- list(seq(1.7, 1.8, 0.01), seq(0.10, 0.25, 0.01))
marginal &lt;- list(0.3)
lam &lt;- 0.5
pois_eps &lt;- 0.0001
size &lt;- 2
prob &lt;- 0.75
nb_eps &lt;- 0.0001

Rey &lt;- matrix(0.4, 5, 5)
diag(Rey) &lt;- 1

# Make sure Rey is within upper and lower correlation limits
valid2 &lt;- valid_corr2(k_cat = 1, k_cont = 2, k_pois = 1, k_nb = 1,
                      method = "Polynomial", means = Stcum[, 1],
                      vars = Stcum[, 2]^2, skews = Stcum[, 3],
                      skurts = Stcum[, 4], fifths = Stcum[, 5],
                      sixths = Stcum[, 6], Six = Six, marginal = marginal,
                      lam = lam, pois_eps = pois_eps, size = size,
                      prob = prob, nb_eps = nb_eps, rho = Rey,
                      seed = seed)

# Simulate variables without error loop
Sim2 &lt;- rcorrvar2(n = n, k_cat = 1, k_cont = 2, k_pois = 1, k_nb = 1,
                  method = "Polynomial", means = Stcum[, 1],
                  vars = Stcum[, 2]^2, skews = Stcum[, 3],
                  skurts = Stcum[, 4], fifths = Stcum[, 5],
                  sixths = Stcum[, 6], Six = Six, marginal = marginal,
                  lam = lam, pois_eps = pois_eps, size = size,
                  prob = prob, nb_eps = nb_eps, rho = Rey,
                  seed = seed)
names(Sim2)

# Look at the maximum correlation error
Sim2$maxerr

Sim2_error = round(Sim2$correlations - Rey, 6)

# interquartile-range of correlation errors
quantile(as.numeric(Sim2_error), 0.25)
quantile(as.numeric(Sim2_error), 0.75)

# Simulate variables with error loop
Sim2_EL &lt;- rcorrvar2(n = n, k_cat = 1, k_cont = 2, k_pois = 1, k_nb = 1,
                     method = "Polynomial", means = Stcum[, 1],
                     vars = Stcum[, 2]^2, skews = Stcum[, 3],
                     skurts = Stcum[, 4], fifths = Stcum[, 5],
                     sixths = Stcum[, 6], Six = Six, marginal = marginal,
                     lam = lam, pois_eps = pois_eps, size = size,
                     prob = prob, nb_eps = nb_eps, rho = Rey,
                     seed = seed, errorloop = TRUE)
# Look at the maximum correlation error
Sim2_EL$maxerr

EL_error = round(Sim2_EL$correlations - Rey, 6)

# interquartile-range of correlation errors
quantile(as.numeric(EL_error), 0.25)
quantile(as.numeric(EL_error), 0.75)

# Look at results
# Ordinal variables
Sim2_EL$summary_ordinal

# Continuous variables
round(Sim2_EL$constants, 6)
round(Sim2_EL$summary_continuous, 6)
round(Sim2_EL$summary_targetcont, 6)
Sim2_EL$valid.pdf

# Count variables
Sim2_EL$summary_Poisson
Sim2_EL$summary_Neg_Bin

# Generate Plots

# Logistic (1st continuous variable)
# 1) Simulated Data CDF (find cumulative probability up to y = 0.5)
plot_sim_cdf(Sim2_EL$continuous_variables[, 1], calc_cprob = TRUE,
             delta = 0.5)

# 2) Simulated Data and Target Distribution PDFs
plot_sim_pdf_theory(Sim2_EL$continuous_variables[, 1], Dist = "Logistic",
                    params = c(0, 1))

# 3) Simulated Data and Target Distribution
plot_sim_theory(Sim2_EL$continuous_variables[, 1], Dist = "Logistic",
                params = c(0, 1))


## End(Not run)
</code></pre>

<hr>
<h2 id='separate_rho'>Separate Target Correlation Matrix by Variable Type</h2><span id='topic+separate_rho'></span>

<h3>Description</h3>

<p>This function separates the target correlation matrix rho by variable type (ordinal, continuous, Poisson, and/or
Negative Binomial).  The function is used in <code><a href="#topic+findintercorr">findintercorr</a></code>,
<code><a href="#topic+rcorrvar">rcorrvar</a></code>, <code><a href="#topic+findintercorr2">findintercorr2</a></code>, and
<code><a href="#topic+rcorrvar2">rcorrvar2</a></code>.  This would not ordinarily be called directly by the user.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>separate_rho(k_cat, k_cont, k_pois, k_nb, rho)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="separate_rho_+3A_k_cat">k_cat</code></td>
<td>
<p>the number of ordinal (r &gt;= 2 categories) variables</p>
</td></tr>
<tr><td><code id="separate_rho_+3A_k_cont">k_cont</code></td>
<td>
<p>the number of continuous variables</p>
</td></tr>
<tr><td><code id="separate_rho_+3A_k_pois">k_pois</code></td>
<td>
<p>the number of Poisson variables</p>
</td></tr>
<tr><td><code id="separate_rho_+3A_k_nb">k_nb</code></td>
<td>
<p>the number of Negative Binomial variables</p>
</td></tr>
<tr><td><code id="separate_rho_+3A_rho">rho</code></td>
<td>
<p>the target correlation matrix</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list containing the target correlation matrix components by variable combination
</p>


<h3>See Also</h3>

<p><code><a href="#topic+findintercorr">findintercorr</a></code>, <code><a href="#topic+rcorrvar">rcorrvar</a></code>,
<code><a href="#topic+findintercorr2">findintercorr2</a></code>, <code><a href="#topic+rcorrvar2">rcorrvar2</a></code>
</p>

<hr>
<h2 id='sim_cdf_prob'>Calculate Simulated (Empirical) Cumulative Probability</h2><span id='topic+sim_cdf_prob'></span>

<h3>Description</h3>

<p>This function calculates a cumulative probability using simulated data and
Martin Maechler's <code><a href="stats.html#topic+ecdf">ecdf</a></code> function.  <code class="reqn">Fn</code> is a step function with jumps <code class="reqn">i/n</code> at observation
values, where <code class="reqn">i</code> is the number of tied observations at that value. Missing values are ignored. For
observations <code class="reqn">y = (y1, y2, ..., yn)</code>, <code class="reqn">Fn</code> is the fraction of observations less or equal to <code class="reqn">t</code>, i.e.,
<code class="reqn">Fn(t) = sum[yi &lt;= t]/n</code>.  This works for continuous, ordinal, or count variables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sim_cdf_prob(sim_y, delta = 0.5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sim_cdf_prob_+3A_sim_y">sim_y</code></td>
<td>
<p>a vector of simulated data</p>
</td></tr>
<tr><td><code id="sim_cdf_prob_+3A_delta">delta</code></td>
<td>
<p>the value y at which to evaluate the cumulative probability</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with components:
</p>
<p><code>cumulative_prob</code> the empirical cumulative probability up to delta
</p>
<p><code>Fn</code> the empirical distribution function
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+ecdf">ecdf</a></code>, <code><a href="#topic+plot_sim_cdf">plot_sim_cdf</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Beta(a = 4, b = 2) Distribution:
x &lt;- rbeta(10000, 4, 2)
sim_cdf_prob(x, delta = 0.5)

</code></pre>

<hr>
<h2 id='SimMultiCorrData'>Simulation of Correlated Data with Multiple Variable Types</h2><span id='topic+SimMultiCorrData'></span><span id='topic+SimMultiCorrData-package'></span>

<h3>Description</h3>

<p><span class="pkg">SimMultiCorrData</span> generates continuous (normal or non-normal), binary, ordinal, and count (Poisson or Negative Binomial) variables
with a specified correlation matrix.  It can also produce a single continuous variable.  This package can be used to simulate data sets that mimic
real-world situations (i.e. clinical data sets, plasmodes, as in Vaughan et al., 2009, doi: <a href="http://doi.org/10.1016/j.csda.2008.02.032">10.1016/j.csda.2008.02.032</a>).  All variables are generated from standard normal variables
with an imposed intermediate correlation matrix.  Continuous variables are simulated by specifying mean, variance, skewness, standardized kurtosis,
and fifth and sixth standardized cumulants using either Fleishman's Third-Order (doi: <a href="http://doi.org/10.1007/BF02293811">10.1007/BF02293811</a>) or Headrick's Fifth-Order
(doi: <a href="http://doi.org/10.1016/S0167-9473(02)00072-5">10.1016/S0167-9473(02)00072-5</a>) Polynomial Transformation.  Binary and
ordinal variables are simulated using a modification of <code><a href="GenOrd.html#topic+GenOrd-package">GenOrd-package</a></code>'s <code><a href="GenOrd.html#topic+ordsample">ordsample</a></code> function.
Count variables are simulated using the inverse cdf method.  There are two simulation pathways which differ primarily according to the calculation
of the intermediate correlation matrix.  In <b>Correlation Method 1</b>, the intercorrelations involving count variables are determined using a simulation based,
logarithmic correlation correction (adapting Yahav and Shmueli's 2012 method, doi: <a href="http://doi.org/10.1002/asmb.901">10.1002/asmb.901</a>).  In <b>Correlation Method 2</b>, the count variables are treated as ordinal
(adapting Barbiero and Ferrari's 2015 modification of <code><a href="GenOrd.html#topic+GenOrd-package">GenOrd-package</a></code>, doi: <a href="http://doi.org/10.1002/asmb.2072">10.1002/asmb.2072</a>).  There is an optional error loop that corrects the
final correlation matrix to be within a user-specified precision value. The package also
includes functions to calculate standardized cumulants for theoretical distributions or from real data sets, check if a target correlation
matrix is within the possible correlation bounds (given the distributions of the simulated variables), summarize results,
numerically or graphically, to verify valid power method pdfs, and to calculate lower standardized kurtosis bounds.
</p>


<h3>Vignettes</h3>

<p>There are several vignettes which accompany this package that may help the user understand the simulation and analysis methods.
</p>
<p>1) <b>Benefits of SimMultiCorrData and Comparison to Other Packages</b> describes some of the ways <span class="pkg">SimMultiCorrData</span> improves
upon other simulation packages.
</p>
<p>2) <b>Variable Types</b> describes the different types of variables that can be simulated in <span class="pkg">SimMultiCorrData</span>.
</p>
<p>3) <b>Function by Topic</b> describes each function, separated by topic.
</p>
<p>4) <b>Comparison of Correlation Method 1 and Correlation Method 2</b> describes the two simulation pathways that can be followed.
</p>
<p>5) <b>Overview of Error Loop</b> details the algorithm involved in the optional error loop that improves the accuracy of the
simulated variables' correlation matrix.
</p>
<p>6) <b>Overall Workflow for Data Simulation</b> gives a step-by-step guideline to follow with an example containing continuous
(normal and non-normal), binary, ordinal, Poisson, and Negative Binomial variables.  It also demonstrates the use of the
standardized cumulant calculation function, correlation check functions, the lower kurtosis boundary function, and the plotting functions.
</p>
<p>7) <b>Comparison of Simulation Distribution to Theoretical Distribution or Empirical Data</b> gives a step-by-step guideline for
comparing a simulated univariate continuous distribution to the target distribution with an example.
</p>
<p>8) <b>Using the Sixth Cumulant Correction to Find Valid Power Method Pdfs</b> demonstrates how to use the sixth cumulant correction
to generate a valid power method pdf and the effects this has on the resulting distribution.
</p>


<h3>Functions</h3>

<p>This package contains 3 <em>simulation</em> functions:
</p>
<p><code><a href="#topic+nonnormvar1">nonnormvar1</a></code>, <code><a href="#topic+rcorrvar">rcorrvar</a></code>, and <code><a href="#topic+rcorrvar2">rcorrvar2</a></code>
</p>
<p>8 data description (<em>summary</em>) functions:
</p>
<p><code><a href="#topic+calc_fisherk">calc_fisherk</a></code>, <code><a href="#topic+calc_moments">calc_moments</a></code>, <code><a href="#topic+calc_theory">calc_theory</a></code>,
<code><a href="#topic+cdf_prob">cdf_prob</a></code>, <code><a href="#topic+power_norm_corr">power_norm_corr</a></code>, <br />
<code><a href="#topic+pdf_check">pdf_check</a></code>, <code><a href="#topic+sim_cdf_prob">sim_cdf_prob</a></code>, <code><a href="#topic+stats_pdf">stats_pdf</a></code>
</p>
<p>8 <em>graphing</em> functions:
</p>
<p><code><a href="#topic+plot_cdf">plot_cdf</a></code>, <code><a href="#topic+plot_pdf_ext">plot_pdf_ext</a></code>, <code><a href="#topic+plot_pdf_theory">plot_pdf_theory</a></code>,
<code><a href="#topic+plot_sim_cdf">plot_sim_cdf</a></code>, <code><a href="#topic+plot_sim_ext">plot_sim_ext</a></code>, <br />
<code><a href="#topic+plot_sim_pdf_ext">plot_sim_pdf_ext</a></code>,
<code><a href="#topic+plot_sim_pdf_theory">plot_sim_pdf_theory</a></code>, <code><a href="#topic+plot_sim_theory">plot_sim_theory</a></code>
</p>
<p>5 <em>support</em> functions:
</p>
<p><code><a href="#topic+calc_lower_skurt">calc_lower_skurt</a></code>, <code><a href="#topic+find_constants">find_constants</a></code>,
<code><a href="#topic+pdf_check">pdf_check</a></code>, <code><a href="#topic+valid_corr">valid_corr</a></code>, <code><a href="#topic+valid_corr2">valid_corr2</a></code>
</p>
<p>and 30 <em>auxiliary</em> functions (should not normally be called by the user, but are called by other functions):
</p>
<p><code><a href="#topic+calc_final_corr">calc_final_corr</a></code>, <code><a href="#topic+chat_nb">chat_nb</a></code>, <code><a href="#topic+chat_pois">chat_pois</a></code>,
<code><a href="#topic+denom_corr_cat">denom_corr_cat</a></code>, <code><a href="#topic+error_loop">error_loop</a></code>, <code><a href="#topic+error_vars">error_vars</a></code>, <br />
<code><a href="#topic+findintercorr">findintercorr</a></code>, <code><a href="#topic+findintercorr2">findintercorr2</a></code>,
<code><a href="#topic+findintercorr_cat_nb">findintercorr_cat_nb</a></code>, <code><a href="#topic+findintercorr_cat_pois">findintercorr_cat_pois</a></code>, <br />
<code><a href="#topic+findintercorr_cont">findintercorr_cont</a></code>,
<code><a href="#topic+findintercorr_cont_cat">findintercorr_cont_cat</a></code>,
<code><a href="#topic+findintercorr_cont_nb">findintercorr_cont_nb</a></code>, <br />
<code><a href="#topic+findintercorr_cont_nb2">findintercorr_cont_nb2</a></code>, <code><a href="#topic+findintercorr_cont_pois">findintercorr_cont_pois</a></code>,
<code><a href="#topic+findintercorr_cont_pois2">findintercorr_cont_pois2</a></code>, <br />
<code><a href="#topic+findintercorr_nb">findintercorr_nb</a></code>, <code><a href="#topic+findintercorr_pois">findintercorr_pois</a></code>,
<code><a href="#topic+findintercorr_pois_nb">findintercorr_pois_nb</a></code>, <code><a href="#topic+fleish">fleish</a></code>, <br />
<code><a href="#topic+fleish_Hessian">fleish_Hessian</a></code>,
<code><a href="#topic+fleish_skurt_check">fleish_skurt_check</a></code>, <code><a href="#topic+intercorr_fleish">intercorr_fleish</a></code>,
<code><a href="#topic+intercorr_poly">intercorr_poly</a></code>, <br />
<code><a href="#topic+max_count_support">max_count_support</a></code>, <code><a href="#topic+ordnorm">ordnorm</a></code>,
<code><a href="#topic+poly">poly</a></code>, <code><a href="#topic+poly_skurt_check">poly_skurt_check</a></code>, <code><a href="#topic+separate_rho">separate_rho</a></code>, <br />
<code><a href="#topic+var_cat">var_cat</a></code>
</p>


<h3>References</h3>

<p>Amatya A &amp; Demirtas H (2015). Simultaneous generation of multivariate mixed data with Poisson and normal marginals.
Journal of Statistical Computation and Simulation, 85(15): 3129-39. doi: <a href="http://doi.org/10.1080/00949655.2014.953534">10.1080/00949655.2014.953534</a>.
</p>
<p>Amatya A &amp; Demirtas H (2016). MultiOrd: Generation of Multivariate Ordinal Variates. R package version 2.2.
<a href="https://CRAN.R-project.org/package=MultiOrd">https://CRAN.R-project.org/package=MultiOrd</a>
</p>
<p>Barbiero A &amp; Ferrari PA (2015). Simulation of correlated Poisson variables. Applied Stochastic Models in
Business and Industry, 31: 669-80. doi: <a href="http://doi.org/10.1002/asmb.2072">10.1002/asmb.2072</a>.
</p>
<p>Barbiero A, Ferrari PA (2015). GenOrd: Simulation of Discrete Random Variables with Given
Correlation Matrix and Marginal Distributions. R package version 1.4.0. <a href="https://CRAN.R-project.org/package=GenOrd">https://CRAN.R-project.org/package=GenOrd</a>
</p>
<p>Demirtas H (2006). A method for multivariate ordinal data generation given marginal distributions and correlations. Journal of Statistical
Computation and Simulation, 76(11): 1017-1025. doi: <a href="http://doi.org/10.1080/10629360600569246">10.1080/10629360600569246</a>.
</p>
<p>Demirtas H (2014). Joint Generation of Binary and Nonnormal Continuous Data. Biometrics &amp; Biostatistics, S12.
</p>
<p>Demirtas H &amp; Hedeker D (2011). A practical way for computing approximate lower and upper correlation bounds.
American Statistician, 65(2): 104-109. doi: <a href="http://doi.org/10.1198/tast.2011.10090">10.1198/tast.2011.10090</a>.
</p>
<p>Demirtas H, Hedeker D, &amp; Mermelstein RJ (2012). Simulation of massive public health data by power polynomials.
Statistics in Medicine, 31(27): 3337-3346. doi: <a href="http://doi.org/10.1002/sim.5362">10.1002/sim.5362</a>.
</p>
<p>Demirtas H, Nordgren R, &amp; Allozi R (2017). PoisBinOrdNonNor: Generation of Up to Four Different Types of Variables. R package version 1.3.
<a href="https://CRAN.R-project.org/package=PoisBinOrdNonNor">https://CRAN.R-project.org/package=PoisBinOrdNonNor</a>
</p>
<p>Ferrari PA, Barbiero A (2012). Simulating ordinal data. Multivariate Behavioral Research, 47(4): 566-589.
doi: <a href="http://doi.org/10.1080/00273171.2012.692630">10.1080/00273171.2012.692630</a>.
</p>
<p>Fleishman AI (1978). A Method for Simulating Non-normal Distributions. Psychometrika, 43, 521-532. doi: <a href="http://doi.org/10.1007/BF02293811">10.1007/BF02293811</a>.
</p>
<p>Frechet M.  Sur les tableaux de correlation dont les marges sont donnees.  Ann. l'Univ. Lyon SectA.  1951;14:53-77.
</p>
<p>Hasselman B (2018). nleqslv: Solve Systems of Nonlinear Equations. R package version 3.3.2.
<a href="https://CRAN.R-project.org/package=nleqslv">https://CRAN.R-project.org/package=nleqslv</a>
</p>
<p>Headrick TC (2002). Fast Fifth-order Polynomial Transforms for Generating Univariate and Multivariate
Non-normal Distributions. Computational Statistics &amp; Data Analysis, 40(4):685-711. doi: <a href="http://doi.org/10.1016/S0167-9473(02)00072-5">10.1016/S0167-9473(02)00072-5</a>.
(<a href="http://www.sciencedirect.com/science/article/pii/S0167947302000725">ScienceDirect</a>)
</p>
<p>Headrick TC (2004). On Polynomial Transformations for Simulating Multivariate Nonnormal Distributions.
Journal of Modern Applied Statistical Methods, 3(1), 65-71. doi: <a href="http://doi.org/10.22237/jmasm/1083370080">10.22237/jmasm/1083370080</a>.
</p>
<p>Headrick TC, Kowalchuk RK (2007). The Power Method Transformation: Its Probability Density Function, Distribution
Function, and Its Further Use for Fitting Data. Journal of Statistical Computation and Simulation, 77, 229-249. doi: <a href="http://doi.org/10.1080/10629360600605065">10.1080/10629360600605065</a>.
</p>
<p>Headrick TC, Sawilowsky SS (1999). Simulating Correlated Non-normal Distributions: Extending the Fleishman Power
Method. Psychometrika, 64, 25-35. doi: <a href="http://doi.org/10.1007/BF02294317">10.1007/BF02294317</a>.
</p>
<p>Headrick TC, Sawilowsky SS (2002). Weighted Simplex Procedures for Determining Boundary Points and Constants for the
Univariate and Multivariate Power Methods. Journal of Educational and Behavioral Statistics, 25, 417-436. doi: <a href="http://doi.org/10.3102/10769986025004417">10.3102/10769986025004417</a>.
</p>
<p>Headrick TC, Sheng Y, &amp; Hodis FA (2007). Numerical Computing and Graphics for the Power Method Transformation Using
Mathematica. Journal of Statistical Software, 19(3), 1 - 17. doi: <a href="http://doi.org/10.18637/jss.v019.i03">10.18637/jss.v019.i03</a>.
</p>
<p>Higham N (2002). Computing the nearest correlation matrix - a problem from finance; IMA Journal of Numerical Analysis 22: 329-343.
</p>
<p>Hoeffding W. Scale-invariant correlation theory. In: Fisher NI, Sen PK, editors. The collected works of Wassily Hoeffding.
New York: Springer-Verlag; 1994. p. 57-107.
</p>
<p>Kaiser S, Traeger D, &amp; Leisch F (2011). Generating Correlated Ordinal Random Values.  Technical Report Number 94, Department of Statistics,
University of Munich. <a href="https://epub.ub.uni-muenchen.de/12157/1/kaiser-tr-94-ordinal.pdf">https://epub.ub.uni-muenchen.de/12157/1/kaiser-tr-94-ordinal.pdf</a>
</p>
<p>Leisch F, Kaiser AWS, &amp; Hornik K (2010). orddata: Generation of Artificial Ordinal and Binary Data. R package version 0.1/r4.
</p>
<p>Olsson U, Drasgow F, &amp; Dorans NJ (1982). The Polyserial Correlation Coefficient. Psychometrika, 47(3): 337-47.
doi: <a href="http://doi.org/10.1007/BF02294164">10.1007/BF02294164</a>.
</p>
<p>Vale CD &amp; Maurelli VA (1983). Simulating Multivariate Nonnormal Distributions. Psychometrika, 48, 465-471. doi: <a href="http://doi.org/10.1007/BF02293687">10.1007/BF02293687</a>.
</p>
<p>Varadhan R, Gilbert P (2009). BB: An R Package for Solving a Large System of Nonlinear Equations and for
Optimizing a High-Dimensional Nonlinear Objective Function, J. Statistical Software, 32(4). doi: <a href="http://doi.org/10.18637/jss.v032.i04">10.18637/jss.v032.i04</a>.
<a href="http://www.jstatsoft.org/v32/i04/">http://www.jstatsoft.org/v32/i04/</a>
</p>
<p>Vaughan LK, Divers J, Padilla M, Redden DT, Tiwari HK, Pomp D, Allison DB (2009). The use of plasmodes as a supplement to simulations:
A simple example evaluating individual admixture estimation methodologies. Comput Stat Data Anal, 53(5):1755-66.
doi: <a href="http://doi.org/10.1016/j.csda.2008.02.032">10.1016/j.csda.2008.02.032</a>.
</p>
<p>Yahav I &amp; Shmueli G (2012). On Generating Multivariate Poisson Data in Management Science Applications. Applied Stochastic
Models in Business and Industry, 28(1): 91-102. doi: <a href="http://doi.org/10.1002/asmb.901">10.1002/asmb.901</a>.
</p>


<h3>See Also</h3>

<p>Useful link: <a href="https://github.com/AFialkowski/SimMultiCorrData">https://github.com/AFialkowski/SimMultiCorrData</a>
</p>

<hr>
<h2 id='stats_pdf'>Calculate Theoretical Statistics for a Valid Power Method PDF</h2><span id='topic+stats_pdf'></span>

<h3>Description</h3>

<p>This function calculates the 100*<code>alpha</code> percent symmetric trimmed mean (0 &lt; <code>alpha</code> &lt; 0.50), median,
mode, and maximum height of a valid power method pdf, after using <code><a href="#topic+pdf_check">pdf_check</a></code>.  It will stop with
an error if the pdf is invalid.  The equations are those from Headrick &amp; Kowalchuk (2007, doi: <a href="http://doi.org/10.1080/10629360600605065">10.1080/10629360600605065</a>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stats_pdf(c, method = c("Fleishman", "Polynomial"), alpha = 0.025, mu = 0,
  sigma = 1, lower = -10, upper = 10, sub = 1000)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stats_pdf_+3A_c">c</code></td>
<td>
<p>a vector of constants c0, c1, c2, c3 (if <code>method</code> = &quot;Fleishman&quot;) or c0, c1, c2, c3, c4, c5 (if <code>method</code> =
&quot;Polynomial&quot;), like that returned by <code><a href="#topic+find_constants">find_constants</a></code></p>
</td></tr>
<tr><td><code id="stats_pdf_+3A_method">method</code></td>
<td>
<p>the method used to find the constants.  &quot;Fleishman&quot; uses Fleishman's third-order polynomial transformation and
&quot;Polynomial&quot; uses Headrick's fifth-order transformation.</p>
</td></tr>
<tr><td><code id="stats_pdf_+3A_alpha">alpha</code></td>
<td>
<p>proportion to be trimmed from the lower and upper ends of the power method pdf (default = 0.025)</p>
</td></tr>
<tr><td><code id="stats_pdf_+3A_mu">mu</code></td>
<td>
<p>mean for the continuous variable (default = 0)</p>
</td></tr>
<tr><td><code id="stats_pdf_+3A_sigma">sigma</code></td>
<td>
<p>standard deviation for the continuous variable (default = 1)</p>
</td></tr>
<tr><td><code id="stats_pdf_+3A_lower">lower</code></td>
<td>
<p>lower bound for integration of the standard normal variable Z that generates the continuous variable (default = -10)</p>
</td></tr>
<tr><td><code id="stats_pdf_+3A_upper">upper</code></td>
<td>
<p>upper bound for integration (default = 10)</p>
</td></tr>
<tr><td><code id="stats_pdf_+3A_sub">sub</code></td>
<td>
<p>the number of subdivisions to use in the integration; if no result, try increasing sub (requires longer
computation time; default = 1000)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector with components:
</p>
<p><code>trimmed_mean</code> the trimmed mean value
</p>
<p><code>median</code> the median value
</p>
<p><code>mode</code> the mode value
</p>
<p><code>max_height</code> the maximum pdf height
</p>


<h3>References</h3>

<p>Please see references for <code><a href="#topic+pdf_check">pdf_check</a></code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+find_constants">find_constants</a></code>, <code><a href="#topic+pdf_check">pdf_check</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>stats_pdf(c = c(0, 1, 0, 0, 0, 0), method = "Polynomial", alpha = 0.025)

## Not run: 
# Beta(a = 4, b = 2) Distribution:
con &lt;- find_constants(method = "Polynomial", skews = -0.467707,
                      skurts = -0.375, fifths = 1.403122,
                      sixths = -0.426136)$constants
stats_pdf(c = con, method = "Polynomial", alpha = 0.025)

## End(Not run)
</code></pre>

<hr>
<h2 id='valid_corr'>Determine Correlation Bounds for Ordinal, Continuous, Poisson, and/or Negative Binomial Variables: Correlation Method 1</h2><span id='topic+valid_corr'></span>

<h3>Description</h3>

<p>This function calculates the lower and upper correlation bounds for the given distributions and
checks if a given target correlation matrix <code>rho</code> is within the bounds.  It should be used before simulation with
<code><a href="#topic+rcorrvar">rcorrvar</a></code>.  However, even if all pairwise correlations fall within the bounds, it is still possible
that the desired correlation matrix is not feasible.  This is particularly true when ordinal variables (r &gt;= 2 categories) are
generated or negative correlations are desired.  Therefore, this function should be used as a general check to eliminate pairwise correlations that are obviously
not reproducible.  It will help prevent errors when executing the simulation.
</p>
<p>Note: Some pieces of the function code have been adapted from Demirtas, Hu, &amp; Allozi's (2017) <code><a href="PoisBinOrdNor.html#topic+validation_specs">validation_specs</a></code>.
This function (<code><a href="#topic+valid_corr">valid_corr</a></code>) extends the methods to:
</p>
<p>1) non-normal continuous variables generated by Fleishman's third-order or Headrick's fifth-order polynomial transformation method, and
</p>
<p>2) Negative Binomial variables (including all pairwise correlations involving them).
</p>
<p>Please see the <b>Comparison of Method 1 and Method 2</b> vignette for more information regarding method 1.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>valid_corr(k_cat = 0, k_cont = 0, k_pois = 0, k_nb = 0,
  method = c("Fleishman", "Polynomial"), means = NULL, vars = NULL,
  skews = NULL, skurts = NULL, fifths = NULL, sixths = NULL,
  Six = list(), marginal = list(), lam = NULL, size = NULL,
  prob = NULL, mu = NULL, rho = NULL, n = 100000, seed = 1234)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="valid_corr_+3A_k_cat">k_cat</code></td>
<td>
<p>the number of ordinal (r &gt;= 2 categories) variables (default = 0)</p>
</td></tr>
<tr><td><code id="valid_corr_+3A_k_cont">k_cont</code></td>
<td>
<p>the number of continuous variables (default = 0)</p>
</td></tr>
<tr><td><code id="valid_corr_+3A_k_pois">k_pois</code></td>
<td>
<p>the number of Poisson variables (default = 0)</p>
</td></tr>
<tr><td><code id="valid_corr_+3A_k_nb">k_nb</code></td>
<td>
<p>the number of Negative Binomial variables (default = 0)</p>
</td></tr>
<tr><td><code id="valid_corr_+3A_method">method</code></td>
<td>
<p>the method used to generate the k_cont continuous variables.  &quot;Fleishman&quot; uses a third-order polynomial transformation
and &quot;Polynomial&quot; uses Headrick's fifth-order transformation.</p>
</td></tr>
<tr><td><code id="valid_corr_+3A_means">means</code></td>
<td>
<p>a vector of means for the k_cont continuous variables (i.e. = rep(0, k_cont))</p>
</td></tr>
<tr><td><code id="valid_corr_+3A_vars">vars</code></td>
<td>
<p>a vector of variances (i.e. = rep(1, k_cont))</p>
</td></tr>
<tr><td><code id="valid_corr_+3A_skews">skews</code></td>
<td>
<p>a vector of skewness values (i.e. = rep(0, k_cont))</p>
</td></tr>
<tr><td><code id="valid_corr_+3A_skurts">skurts</code></td>
<td>
<p>a vector of standardized kurtoses (kurtosis - 3, so that normal variables have a value of 0; i.e. = rep(0, k_cont))</p>
</td></tr>
<tr><td><code id="valid_corr_+3A_fifths">fifths</code></td>
<td>
<p>a vector of standardized fifth cumulants (not necessary for <code>method</code> = &quot;Fleishman&quot;; i.e. = rep(0, k_cont))</p>
</td></tr>
<tr><td><code id="valid_corr_+3A_sixths">sixths</code></td>
<td>
<p>a vector of standardized sixth cumulants (not necessary for <code>method</code> = &quot;Fleishman&quot;; i.e. = rep(0, k_cont))</p>
</td></tr>
<tr><td><code id="valid_corr_+3A_six">Six</code></td>
<td>
<p>a list of vectors of correction values to add to the sixth cumulants if no valid pdf constants are found,
ex: <code>Six = list(seq(0.01, 2,by = 0.01), seq(1, 10,by = 0.5))</code>; if no correction is desired for variable Y_i, set the i-th list
component equal to NULL</p>
</td></tr>
<tr><td><code id="valid_corr_+3A_marginal">marginal</code></td>
<td>
<p>a list of length equal to <code>k_cat</code>; the i-th element is a vector of the cumulative
probabilities defining the marginal distribution of the i-th variable;
if the variable can take r values, the vector will contain r - 1 probabilities (the r-th is assumed to be 1; default = list())</p>
</td></tr>
<tr><td><code id="valid_corr_+3A_lam">lam</code></td>
<td>
<p>a vector of lambda (&gt; 0) constants for the Poisson variables (see <code><a href="stats.html#topic+Poisson">Poisson</a></code>)</p>
</td></tr>
<tr><td><code id="valid_corr_+3A_size">size</code></td>
<td>
<p>a vector of size parameters for the Negative Binomial variables (see <code><a href="stats.html#topic+NegBinomial">NegBinomial</a></code>)</p>
</td></tr>
<tr><td><code id="valid_corr_+3A_prob">prob</code></td>
<td>
<p>a vector of success probability parameters</p>
</td></tr>
<tr><td><code id="valid_corr_+3A_mu">mu</code></td>
<td>
<p>a vector of mean parameters (*Note: either <code>prob</code> or <code>mu</code> should be supplied for all Negative Binomial variables,
not a mixture; default = NULL)</p>
</td></tr>
<tr><td><code id="valid_corr_+3A_rho">rho</code></td>
<td>
<p>the target correlation matrix (<em>must be ordered ordinal, continuous, Poisson, Negative Binomial</em>; default = NULL)</p>
</td></tr>
<tr><td><code id="valid_corr_+3A_n">n</code></td>
<td>
<p>the sample size (i.e. the length of each simulated variable; default = 100000)</p>
</td></tr>
<tr><td><code id="valid_corr_+3A_seed">seed</code></td>
<td>
<p>the seed value for random number generation (default = 1234)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with components:
</p>
<p><code>L_rho</code> the lower correlation bound
</p>
<p><code>U_rho</code> the upper correlation bound
</p>
<p>If continuous variables are desired, additional components are:
</p>
<p><code>constants</code> the calculated constants
</p>
<p><code>sixth_correction</code> a vector of the sixth cumulant correction values
</p>
<p><code>valid.pdf</code> a vector with i-th component equal to &quot;TRUE&quot; if variable Y_i has a valid power method pdf, else &quot;FALSE&quot;
</p>
<p>If a target correlation matrix rho is provided, each pairwise correlation is checked to see if it is within the lower and upper
bounds.  If the correlation is outside the bounds, the indices of the variable pair are given.
</p>


<h3>Reasons for Function Errors</h3>

<p>1) The most likely cause for function errors is that no solutions to <code><a href="#topic+fleish">fleish</a></code> or
<code><a href="#topic+poly">poly</a></code> converged when using <code><a href="#topic+find_constants">find_constants</a></code>.  If this happens,
the simulation will stop.  It may help to first use <code><a href="#topic+find_constants">find_constants</a></code> for each continuous variable to
determine if a vector of sixth cumulant correction values is needed.  If the standardized cumulants are obtained from <code>calc_theory</code>,
the user may need to use rounded values as inputs (i.e.
<code>skews = round(skews, 8)</code>).  Due to the nature of the integration involved in <code>calc_theory</code>, the results are
approximations.  Greater accuracy can be achieved by increasing the number of subdivisions (<code>sub</code>) used in the integration
process.  For example, in order to ensure that skew is exactly 0 for symmetric distributions.
</p>
<p>2) In addition, the kurtosis may be outside the region of possible values.  There is an associated lower boundary for kurtosis associated
with a given skew (for Fleishman's method) or skew and fifth and sixth cumulants (for Headrick's method).  Use
<code><a href="#topic+calc_lower_skurt">calc_lower_skurt</a></code> to determine the boundary for a given set of cumulants.
</p>


<h3>The Generate, Sort, and Correlate (GSC, Demirtas &amp; Hedeker, 2011, doi: <a href="http://doi.org/10.1198/tast.2011.10090">10.1198/tast.2011.10090</a>) Algorithm</h3>

<p>The GSC algorithm is a flexible method for determining empirical correlation bounds when the theoretical bounds are unknown.
The steps are as follows:
</p>
<p>1) Generate independent random samples from the desired distributions using a large number of observations (i.e. N = 100,000).
</p>
<p>2) Lower Bound: Sort the two variables in opposite directions (i.e., one increasing and one decreasing) and find the sample correlation.
</p>
<p>3) Upper Bound: Sort the two variables in the same direction and find the sample correlation.
</p>
<p>Demirtas &amp; Hedeker showed that the empirical bounds computed from the GSC method are similar to the theoretical bounds (when they are known).
</p>


<h3>The Frechet-Hoeffding Correlation Bounds</h3>

<p>Suppose two random variables <code class="reqn">Y_{i}</code> and <code class="reqn">Y_{j}</code> have cumulative distribution functions given by <code class="reqn">F_{i}</code> and <code class="reqn">F_{j}</code>.
Let U be a uniform(0,1) random variable, i.e. representing the distribution of the standard normal cdf. Then Hoeffing (1940) and
Frechet (1951) showed that bounds for the correlation between <code class="reqn">Y_{i}</code> and <code class="reqn">Y_{j}</code> are given by
</p>
<p style="text-align: center;"><code class="reqn">(corr(F_{i}^{-1}(U), F_{j}^{-1}(1-U)), corr(F_{i}^{-1}(U), F_{j}^{-1}(U)))</code>
</p>

<p>The processes used to find the correlation bounds for each variable type are described below:
</p>


<h3>Ordinal Variables</h3>

<p>Binary pairs: The correlation bounds are determined as in Demirtas et al. (2012, doi: <a href="http://doi.org/10.1002/sim.5362">10.1002/sim.5362</a>), who used the method of Emrich &amp;
Piedmonte (1991, doi: <a href="http://doi.org/10.1080/00031305.1991.10475828">10.1080/00031305.1991.10475828</a>).  The joint distribution is determined by &quot;borrowing&quot; the moments of a multivariate normal
distribution.  For two binary variables <code class="reqn">Y_{i}</code> and <code class="reqn">Y_{j}</code>, with success probabilities <code class="reqn">p_{i}</code> and <code class="reqn">p_{j}</code>, the lower
correlation bound is given by
</p>
<p style="text-align: center;"><code class="reqn">max(-\sqrt{(p_{i}p_{j})/(q_{i}q_{j})},\ -\sqrt{(q_{i}q_{j})/(p_{i}p_{j})})</code>
</p>

<p>and the upper bound by
</p>
<p style="text-align: center;"><code class="reqn">min(\sqrt{(p_{i}q_{j})/(q_{i}p_{j})},\ \sqrt{(q_{i}p_{j})/(p_{i}q_{j})})</code>
</p>

<p>Here, <code class="reqn">q_{i} = 1 - p_{i}</code> and <code class="reqn">q_{j} = 1 - p_{j}</code>.
</p>
<p>Binary-Ordinal or Ordinal-Ordinal pairs: Randomly generated variables with the given marginal distributions are used in the
GSC algorithm to find the correlation bounds.
</p>


<h3>Continuous Variables</h3>

<p>Continuous variables are randomly generated using constants from <code><a href="#topic+find_constants">find_constants</a></code> and a vector of sixth
cumulant correction values (if provided.)  The GSC algorithm is used to find the lower and upper bounds.
</p>


<h3>Poisson Variables</h3>

<p>Poisson variables with the given means (lam) are randomly generated using the inverse cdf method.  The Frechet-Hoeffding bounds
are used for the correlation bounds.
</p>


<h3>Negative Binomial Variables</h3>

<p>Negative Binomial variables with the given sizes and success probabilities (prob) or means (mu) are randomly generated using the
inverse cdf method.  The Frechet-Hoeffding bounds are used for the correlation bounds.
</p>


<h3>Continuous - Ordinal Pairs</h3>

<p>Randomly generated ordinal variables with the given marginal distributions and the previously generated continuous variables are used in the
GSC algorithm to find the correlation bounds.
</p>


<h3>Ordinal - Poisson Pairs</h3>

<p>Randomly generated ordinal variables with the given marginal distributions and randomly generated Poisson variables with the given
means (lam) are used in the GSC algorithm to find the correlation bounds.
</p>


<h3>Ordinal - Negative Binomial Pairs</h3>

<p>Randomly generated ordinal variables with the given marginal distributions and randomly generated Negative Binomial variables with
the given sizes and success probabilities (prob) or means (mu) are used in the GSC algorithm to find the correlation bounds.
</p>


<h3>Continuous - Poisson Pairs</h3>

<p>The previously generated continuous variables and randomly generated Poisson variables with the given
means (lam) are used in the GSC algorithm to find the correlation bounds.
</p>


<h3>Continuous - Negative Binomial Pairs</h3>

<p>The previously generated continuous variables and randomly generated Negative Binomial variables with
the given sizes and success probabilities (prob) or means (mu) are used in the GSC algorithm to find the correlation bounds.
</p>


<h3>Poisson - Negative Binomial Pairs</h3>

<p>Poisson variables with the given means (lam) and Negative Binomial variables with the given sizes and success probabilities (prob)
or means (mu) are randomly generated using the inverse cdf method.  The Frechet-Hoeffding bounds
are used for the correlation bounds.
</p>


<h3>References</h3>

<p>Please see <code><a href="#topic+rcorrvar">rcorrvar</a></code> for additional references.
</p>
<p>Demirtas H &amp; Hedeker D (2011). A practical way for computing approximate lower and upper correlation bounds.
American Statistician, 65(2): 104-109. doi: <a href="http://doi.org/10.1198/tast.2011.10090">10.1198/tast.2011.10090</a>.
</p>
<p>Demirtas H, Hedeker D, &amp; Mermelstein RJ (2012). Simulation of massive public health data by power polynomials.
Statistics in Medicine, 31(27): 3337-3346. doi: <a href="http://doi.org/10.1002/sim.5362">10.1002/sim.5362</a>.
</p>
<p>Emrich LJ &amp; Piedmonte MR (1991). A Method for Generating High-Dimensional Multivariate Binary Variables. The American Statistician, 45(4): 302-4.
doi: <a href="http://doi.org/10.1080/00031305.1991.10475828">10.1080/00031305.1991.10475828</a>.
</p>
<p>Frechet M.  Sur les tableaux de correlation dont les marges sont donnees.  Ann. l'Univ. Lyon SectA.  1951;14:53-77.
</p>
<p>Hoeffding W. Scale-invariant correlation theory. In: Fisher NI, Sen PK, editors. The collected works of Wassily Hoeffding.
New York: Springer-Verlag; 1994. p. 57-107.
</p>
<p>Hakan Demirtas, Yiran Hu and Rawan Allozi (2017). PoisBinOrdNor: Data Generation with Poisson, Binary, Ordinal and Normal Components.
R package version 1.4. <a href="https://CRAN.R-project.org/package=PoisBinOrdNor">https://CRAN.R-project.org/package=PoisBinOrdNor</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+find_constants">find_constants</a></code>, <code><a href="#topic+rcorrvar">rcorrvar</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>valid_corr(n = 1000, k_cat = 1, k_cont = 1, method = "Polynomial",
  means = 0, vars = 1, skews = 0, skurts = 0, fifths = 0, sixths = 0,
  marginal = list(c(1/3, 2/3)), rho = matrix(c(1, 0.4, 0.4, 1), 2, 2))

## Not run: 

# Binary, Ordinal, Continuous, Poisson, and Negative Binomial Variables

options(scipen = 999)
seed &lt;- 1234
n &lt;- 10000

# Continuous Distributions: Normal, t (df = 10), Chisq (df = 4),
#                           Beta (a = 4, b = 2), Gamma (a = 4, b = 4)
Dist &lt;- c("Gaussian", "t", "Chisq", "Beta", "Gamma")

# calculate standardized cumulants
# those for the normal and t distributions are rounded to ensure the
# correct values (i.e. skew = 0)

M1 &lt;- round(calc_theory(Dist = "Gaussian", params = c(0, 1)), 8)
M2 &lt;- round(calc_theory(Dist = "t", params = 10), 8)
M3 &lt;- calc_theory(Dist = "Chisq", params = 4)
M4 &lt;- calc_theory(Dist = "Beta", params = c(4, 2))
M5 &lt;- calc_theory(Dist = "Gamma", params = c(4, 4))
M &lt;- cbind(M1, M2, M3, M4, M5)
M &lt;- round(M[-c(1:2),], digits = 6)
colnames(M) &lt;- Dist
rownames(M) &lt;- c("skew", "skurtosis", "fifth", "sixth")
means &lt;- rep(0, length(Dist))
vars &lt;- rep(1, length(Dist))

# Binary and Ordinal Distributions
marginal &lt;- list(0.3, 0.4, c(0.1, 0.5), c(0.3, 0.6, 0.9),
                 c(0.2, 0.4, 0.7, 0.8))
support &lt;- list()

# Poisson Distributions
lam &lt;- c(1, 5, 10)

# Negative Binomial Distributions
size &lt;- c(3, 6)
prob &lt;- c(0.2, 0.8)

ncat &lt;- length(marginal)
ncont &lt;- ncol(M)
npois &lt;- length(lam)
nnb &lt;- length(size)

# Create correlation matrix from a uniform distribution (-0.8, 0.8)
set.seed(seed)
Rey &lt;- diag(1, nrow = (ncat + ncont + npois + nnb))
for (i in 1:nrow(Rey)) {
  for (j in 1:ncol(Rey)) {
    if (i &gt; j) Rey[i, j] &lt;- runif(1, -0.8, 0.8)
    Rey[j, i] &lt;- Rey[i, j]
  }
}

# Test for positive-definiteness
library(Matrix)
if(min(eigen(Rey, symmetric = TRUE)$values) &lt; 0) {
  Rey &lt;- as.matrix(nearPD(Rey, corr = T, keepDiag = T)$mat)
}

# Make sure Rey is within upper and lower correlation limits
valid &lt;- valid_corr(k_cat = ncat, k_cont = ncont, k_pois = npois,
                    k_nb = nnb, method = "Polynomial", means = means,
                    vars = vars, skews = M[1, ], skurts = M[2, ],
                    fifths = M[3, ], sixths = M[4, ], marginal = marginal,
                    lam = lam, size = size, prob = prob, rho = Rey,
                    seed = seed)

## End(Not run)
</code></pre>

<hr>
<h2 id='valid_corr2'>Determine Correlation Bounds for Ordinal, Continuous, Poisson, and/or Negative Binomial Variables: Correlation Method 2</h2><span id='topic+valid_corr2'></span>

<h3>Description</h3>

<p>This function calculates the lower and upper correlation bounds for the given distributions and
checks if a given target correlation matrix rho is within the bounds.  It should be used before simulation with
<code><a href="#topic+rcorrvar2">rcorrvar2</a></code>.  However, even if all pairwise correlations fall within the bounds, it is still possible
that the desired correlation matrix is not feasible.  This is particularly true when ordinal variables (r &gt;= 2 categories) are
generated or negative correlations are desired.  Therefore, this function should be used as a general check to eliminate pairwise correlations that are obviously
not reproducible.  It will help prevent errors when executing the simulation.
</p>
<p>Note: Some pieces of the function code have been adapted from Demirtas, Hu, &amp; Allozi's (2017) <code><a href="PoisBinOrdNor.html#topic+validation_specs">validation_specs</a></code>.
This function (<code><a href="#topic+valid_corr2">valid_corr2</a></code>) extends the methods to:
</p>
<p>1) non-normal continuous variables generated by Fleishman's third-order or Headrick's fifth-order polynomial transformation method,
</p>
<p>2) Negative Binomial variables (including all pairwise correlations involving them), and
</p>
<p>3) Count variables are treated as ordinal when calculating the bounds since that is the intermediate correlation calculation method.
</p>
<p>Please see the <b>Comparison of Method 1 and Method 2</b> vignette for more information regarding method 2.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>valid_corr2(k_cat = 0, k_cont = 0, k_pois = 0, k_nb = 0,
  method = c("Fleishman", "Polynomial"), means = NULL, vars = NULL,
  skews = NULL, skurts = NULL, fifths = NULL, sixths = NULL,
  Six = list(), marginal = list(), lam = NULL, pois_eps = NULL,
  size = NULL, prob = NULL, mu = NULL, nb_eps = NULL, rho = NULL,
  n = 100000, seed = 1234)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="valid_corr2_+3A_k_cat">k_cat</code></td>
<td>
<p>the number of ordinal (r &gt;= 2 categories) variables (default = 0)</p>
</td></tr>
<tr><td><code id="valid_corr2_+3A_k_cont">k_cont</code></td>
<td>
<p>the number of continuous variables (default = 0)</p>
</td></tr>
<tr><td><code id="valid_corr2_+3A_k_pois">k_pois</code></td>
<td>
<p>the number of Poisson variables (default = 0)</p>
</td></tr>
<tr><td><code id="valid_corr2_+3A_k_nb">k_nb</code></td>
<td>
<p>the number of Negative Binomial variables (default = 0)</p>
</td></tr>
<tr><td><code id="valid_corr2_+3A_method">method</code></td>
<td>
<p>the method used to generate the k_cont continuous variables.  &quot;Fleishman&quot; uses a third-order polynomial transformation
and &quot;Polynomial&quot; uses Headrick's fifth-order transformation.</p>
</td></tr>
<tr><td><code id="valid_corr2_+3A_means">means</code></td>
<td>
<p>a vector of means for the k_cont continuous variables (i.e. = rep(0, k_cont))</p>
</td></tr>
<tr><td><code id="valid_corr2_+3A_vars">vars</code></td>
<td>
<p>a vector of variances (i.e. = rep(1, k_cont))</p>
</td></tr>
<tr><td><code id="valid_corr2_+3A_skews">skews</code></td>
<td>
<p>a vector of skewness values (i.e. = rep(0, k_cont))</p>
</td></tr>
<tr><td><code id="valid_corr2_+3A_skurts">skurts</code></td>
<td>
<p>a vector of standardized kurtoses (kurtosis - 3, so that normal variables have a value of 0; i.e. = rep(0, k_cont))</p>
</td></tr>
<tr><td><code id="valid_corr2_+3A_fifths">fifths</code></td>
<td>
<p>a vector of standardized fifth cumulants (not necessary for <code>method</code> = &quot;Fleishman&quot;; i.e. = rep(0, k_cont))</p>
</td></tr>
<tr><td><code id="valid_corr2_+3A_sixths">sixths</code></td>
<td>
<p>a vector of standardized sixth cumulants (not necessary for <code>method</code> = &quot;Fleishman&quot;; i.e. = rep(0, k_cont))</p>
</td></tr>
<tr><td><code id="valid_corr2_+3A_six">Six</code></td>
<td>
<p>a list of vectors of correction values to add to the sixth cumulants if no valid pdf constants are found,
ex: <code>Six = list(seq(0.01, 2,by = 0.01), seq(1, 10,by = 0.5))</code>; if no correction is desired for variable Y_i, set the i-th list
component equal to NULL</p>
</td></tr>
<tr><td><code id="valid_corr2_+3A_marginal">marginal</code></td>
<td>
<p>a list of length equal to <code>k_cat</code>; the i-th element is a vector of the cumulative
probabilities defining the marginal distribution of the i-th variable;
if the variable can take r values, the vector will contain r - 1 probabilities (the r-th is assumed to be 1; default = list())</p>
</td></tr>
<tr><td><code id="valid_corr2_+3A_lam">lam</code></td>
<td>
<p>a vector of lambda (&gt; 0) constants for the Poisson variables (see <code><a href="stats.html#topic+Poisson">Poisson</a></code>)</p>
</td></tr>
<tr><td><code id="valid_corr2_+3A_pois_eps">pois_eps</code></td>
<td>
<p>a vector of length <code>k_pois</code> containing the truncation values (i.e. = rep(0.0001, k_pois); default = NULL)</p>
</td></tr>
<tr><td><code id="valid_corr2_+3A_size">size</code></td>
<td>
<p>a vector of size parameters for the Negative Binomial variables (see <code><a href="stats.html#topic+NegBinomial">NegBinomial</a></code>)</p>
</td></tr>
<tr><td><code id="valid_corr2_+3A_prob">prob</code></td>
<td>
<p>a vector of success probability parameters</p>
</td></tr>
<tr><td><code id="valid_corr2_+3A_mu">mu</code></td>
<td>
<p>a vector of mean parameters (*Note: either <code>prob</code> or <code>mu</code> should be supplied for all Negative Binomial variables,
not a mixture; default = NULL)</p>
</td></tr>
<tr><td><code id="valid_corr2_+3A_nb_eps">nb_eps</code></td>
<td>
<p>a vector of length <code>k_nb</code> containing the truncation values (i.e. = rep(0.0001, k_nb); default = NULL)</p>
</td></tr>
<tr><td><code id="valid_corr2_+3A_rho">rho</code></td>
<td>
<p>the target correlation matrix (<em>must be ordered ordinal, continuous, Poisson, Negative Binomial</em>; default = NULL)</p>
</td></tr>
<tr><td><code id="valid_corr2_+3A_n">n</code></td>
<td>
<p>the sample size (i.e. the length of each simulated variable; default = 100000)</p>
</td></tr>
<tr><td><code id="valid_corr2_+3A_seed">seed</code></td>
<td>
<p>the seed value for random number generation (default = 1234)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with components:
</p>
<p><code>L_rho</code> the lower correlation bound
</p>
<p><code>U_rho</code> the upper correlation bound
</p>
<p>If continuous variables are desired, additional components are:
</p>
<p><code>constants</code> the calculated constants
</p>
<p><code>sixth_correction</code> a vector of the sixth cumulant correction values
</p>
<p><code>valid.pdf</code> a vector with i-th component equal to &quot;TRUE&quot; if variable Y_i has a valid power method pdf, else &quot;FALSE&quot;
</p>
<p>If a target correlation matrix rho is provided, each pairwise correlation is checked to see if it is within the lower and upper
bounds.  If the correlation is outside the bounds, the indices of the variable pair are given.
</p>


<h3>Reasons for Function Errors</h3>

<p>1) The most likely cause for function errors is that no solutions to <code><a href="#topic+fleish">fleish</a></code> or
<code><a href="#topic+poly">poly</a></code> converged when using <code><a href="#topic+find_constants">find_constants</a></code>.  If this happens,
the simulation will stop.  It may help to first use <code><a href="#topic+find_constants">find_constants</a></code> for each continuous variable to
determine if a vector of sixth cumulant correction values is needed.  If the standardized cumulants are obtained from <code>calc_theory</code>,
the user may need to use rounded values as inputs (i.e.
<code>skews = round(skews, 8)</code>).  Due to the nature of the integration involved in <code>calc_theory</code>, the results are
approximations.  Greater accuracy can be achieved by increasing the number of subdivisions (<code>sub</code>) used in the integration
process.  For example, in order to ensure that skew is exactly 0 for symmetric distributions.
</p>
<p>2) In addition, the kurtosis may be outside the region of possible values.  There is an associated lower boundary for kurtosis associated
with a given skew (for Fleishman's method) or skew and fifth and sixth cumulants (for Headrick's method).  Use
<code><a href="#topic+calc_lower_skurt">calc_lower_skurt</a></code> to determine the boundary for a given set of cumulants.
</p>


<h3>The Generate, Sort, and Correlate (GSC, Demirtas &amp; Hedeker, 2011, doi: <a href="http://doi.org/10.1198/tast.2011.10090">10.1198/tast.2011.10090</a>) Algorithm</h3>

<p>The GSC algorithm is a flexible method for determining empirical correlation bounds when the theoretical bounds are unknown.
The steps are as follows:
</p>
<p>1) Generate independent random samples from the desired distributions using a large number of observations (i.e. N = 100,000).
</p>
<p>2) Lower Bound: Sort the two variables in opposite directions (i.e., one increasing and one decreasing) and find the sample correlation.
</p>
<p>3) Upper Bound: Sort the two variables in the same direction and find the sample correlation.
</p>
<p>Demirtas &amp; Hedeker showed that the empirical bounds computed from the GSC method are similar to the theoretical bounds (when they are known).
</p>
<p>The processes used to find the correlation bounds for each variable type are described below:
</p>


<h3>Ordinal Variables</h3>

<p>Binary pairs: The correlation bounds are determined as in Demirtas et al. (2012, doi: <a href="http://doi.org/10.1002/sim.5362">10.1002/sim.5362</a>), who used the method of Emrich &amp;
Piedmonte (1991, doi: <a href="http://doi.org/10.1080/00031305.1991.10475828">10.1080/00031305.1991.10475828</a>).  The joint distribution is determined by &quot;borrowing&quot; the moments of a multivariate normal
distribution.  For two binary variables <code class="reqn">Y_{i}</code> and <code class="reqn">Y_{j}</code>, with success probabilities <code class="reqn">p_{i}</code> and <code class="reqn">p_{j}</code>, the lower
correlation bound is given by
</p>
<p style="text-align: center;"><code class="reqn">max(-\sqrt{(p_{i}p_{j})/(q_{i}q_{j})},\ -\sqrt{(q_{i}q_{j})/(p_{i}p_{j})})</code>
</p>

<p>and the upper bound by
</p>
<p style="text-align: center;"><code class="reqn">min(\sqrt{(p_{i}q_{j})/(q_{i}p_{j})},\ \sqrt{(q_{i}p_{j})/(p_{i}q_{j})})</code>
</p>

<p>Here, <code class="reqn">q_{i} = 1 - p_{i}</code> and <code class="reqn">q_{j} = 1 - p_{j}</code>.
</p>
<p>Binary-Ordinal or Ordinal-Ordinal pairs: Randomly generated variables with the given marginal distributions are used in the
GSC algorithm to find the correlation bounds.
</p>


<h3>Continuous Variables</h3>

<p>Continuous variables are randomly generated using constants from <code><a href="#topic+find_constants">find_constants</a></code> and a vector of sixth
cumulant correction values (if provided.)  The GSC algorithm is used to find the lower and upper bounds.
</p>


<h3>Poisson Variables</h3>

<p>The maximum support values, given the vector of cumulative probability truncation values (pois_eps) and vector of means (lam), are calculated using
<code><a href="#topic+max_count_support">max_count_support</a></code>.  The finite supports are used to determine marginal distributions for each Poisson variable.
Randomly generated variables with the given marginal distributions are used in the GSC algorithm to find the correlation bounds.
</p>


<h3>Negative Binomial Variables</h3>

<p>The maximum support values, given the vector of cumulative probability truncation values (nb_eps) and vectors of
sizes and success probabilities (prob) or means (mu), are calculated using <code><a href="#topic+max_count_support">max_count_support</a></code>.
The finite supports are used to determine marginal distributions for each Negative Binomial variable.
Randomly generated variables with the given marginal distributions are used in the GSC algorithm to find the correlation bounds.
</p>


<h3>Continuous - Ordinal Pairs</h3>

<p>Randomly generated ordinal variables with the given marginal distributions and the previously generated continuous variables are used in the
GSC algorithm to find the correlation bounds.
</p>


<h3>Ordinal - Poisson Pairs</h3>

<p>Randomly generated ordinal and Poisson variables with the given marginal distributions are used in the GSC algorithm to find
the correlation bounds.
</p>


<h3>Ordinal - Negative Binomial Pairs</h3>

<p>Randomly generated ordinal and Negative Binomial variables with the given marginal distributions are used in the GSC algorithm to find
the correlation bounds.
</p>


<h3>Continuous - Poisson Pairs</h3>

<p>The previously generated continuous variables and randomly generated Poisson variables with the given marginal distributions are used
in the GSC algorithm to find the correlation bounds.
</p>


<h3>Continuous - Negative Binomial Pairs</h3>

<p>The previously generated continuous variables and randomly generated Negative Binomial variables with the given marginal distributions are used
in the GSC algorithm to find the correlation bounds.
</p>


<h3>Poisson - Negative Binomial Pairs</h3>

<p>Randomly generated variables with the given marginal distributions are used in the GSC algorithm to find the correlation bounds.
</p>


<h3>References</h3>

<p>Please see <code><a href="#topic+rcorrvar2">rcorrvar2</a></code> for additional references.
</p>
<p>Demirtas H &amp; Hedeker D (2011). A practical way for computing approximate lower and upper correlation bounds.
American Statistician, 65(2): 104-109. doi: <a href="http://doi.org/10.1198/tast.2011.10090">10.1198/tast.2011.10090</a>.
</p>
<p>Demirtas H, Hedeker D, &amp; Mermelstein RJ (2012). Simulation of massive public health data by power polynomials.
Statistics in Medicine, 31(27): 3337-3346. doi: <a href="http://doi.org/10.1002/sim.5362">10.1002/sim.5362</a>.
</p>
<p>Emrich LJ &amp; Piedmonte MR (1991). A Method for Generating High-Dimensional Multivariate Binary Variables. The American Statistician, 45(4): 302-4.
doi: <a href="http://doi.org/10.1080/00031305.1991.10475828">10.1080/00031305.1991.10475828</a>.
</p>
<p>Frechet M.  Sur les tableaux de correlation dont les marges sont donnees.  Ann. l'Univ. Lyon SectA.  1951;14:53-77.
</p>
<p>Hoeffding W. Scale-invariant correlation theory. In: Fisher NI, Sen PK, editors. The collected works of Wassily Hoeffding.
New York: Springer-Verlag; 1994. p. 57-107.
</p>
<p>Hakan Demirtas, Yiran Hu and Rawan Allozi (2017). PoisBinOrdNor: Data Generation with Poisson, Binary, Ordinal and Normal Components.
R package version 1.4. <a href="https://CRAN.R-project.org/package=PoisBinOrdNor">https://CRAN.R-project.org/package=PoisBinOrdNor</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+find_constants">find_constants</a></code>, <code><a href="#topic+rcorrvar2">rcorrvar2</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>valid_corr2(n = 1000, k_cat = 1, k_cont = 1, method = "Polynomial",
  means = 0, vars = 1, skews = 0, skurts = 0, fifths = 0, sixths = 0,
  marginal = list(c(1/3, 2/3)), rho = matrix(c(1, 0.4, 0.4, 1), 2, 2))

## Not run: 

# Binary, Ordinal, Continuous, Poisson, and Negative Binomial Variables

options(scipen = 999)
seed &lt;- 1234
n &lt;- 10000

# Continuous Distributions: Normal, t (df = 10), Chisq (df = 4),
#                           Beta (a = 4, b = 2), Gamma (a = 4, b = 4)
Dist &lt;- c("Gaussian", "t", "Chisq", "Beta", "Gamma")

# calculate standardized cumulants
# those for the normal and t distributions are rounded to ensure the
# correct values (i.e. skew = 0)

M1 &lt;- round(calc_theory(Dist = "Gaussian", params = c(0, 1)), 8)
M2 &lt;- round(calc_theory(Dist = "t", params = 10), 8)
M3 &lt;- calc_theory(Dist = "Chisq", params = 4)
M4 &lt;- calc_theory(Dist = "Beta", params = c(4, 2))
M5 &lt;- calc_theory(Dist = "Gamma", params = c(4, 4))
M &lt;- cbind(M1, M2, M3, M4, M5)
M &lt;- round(M[-c(1:2),], digits = 6)
colnames(M) &lt;- Dist
rownames(M) &lt;- c("skew", "skurtosis", "fifth", "sixth")
means &lt;- rep(0, length(Dist))
vars &lt;- rep(1, length(Dist))

# Binary and Ordinal Distributions
marginal &lt;- list(0.3, 0.4, c(0.1, 0.5), c(0.3, 0.6, 0.9),
                 c(0.2, 0.4, 0.7, 0.8))
support &lt;- list()

# Poisson Distributions
lam &lt;- c(1, 5, 10)

# Negative Binomial Distributions
size &lt;- c(3, 6)
prob &lt;- c(0.2, 0.8)

ncat &lt;- length(marginal)
ncont &lt;- ncol(M)
npois &lt;- length(lam)
nnb &lt;- length(size)

# Create correlation matrix from a uniform distribution (-0.8, 0.8)
set.seed(seed)
Rey &lt;- diag(1, nrow = (ncat + ncont + npois + nnb))
for (i in 1:nrow(Rey)) {
  for (j in 1:ncol(Rey)) {
    if (i &gt; j) Rey[i, j] &lt;- runif(1, -0.8, 0.8)
    Rey[j, i] &lt;- Rey[i, j]
  }
}

# Test for positive-definiteness
library(Matrix)
if(min(eigen(Rey, symmetric = TRUE)$values) &lt; 0) {
  Rey &lt;- as.matrix(nearPD(Rey, corr = T, keepDiag = T)$mat)
}

# Make sure Rey is within upper and lower correlation limits
valid &lt;- valid_corr2(k_cat = ncat, k_cont = ncont, k_pois = npois,
                     k_nb = nnb, method = "Polynomial", means = means,
                     vars = vars, skews = M[1, ], skurts = M[2, ],
                     fifths = M[3, ], sixths = M[4, ], marginal = marginal,
                     lam = lam, pois_eps = rep(0.0001, npois),
                     size = size, prob = prob, nb_eps = rep(0.0001, nnb),
                     rho = Rey, seed = seed)


## End(Not run)
</code></pre>

<hr>
<h2 id='var_cat'>Calculate Variance of Binary or Ordinal Variable</h2><span id='topic+var_cat'></span>

<h3>Description</h3>

<p>This function calculates the variance of a binary or ordinal (r &gt; 2 categories) variable.  It uses the
formula given by Olsson et al. (1982, doi: <a href="http://doi.org/10.1007/BF02294164">10.1007/BF02294164</a>) in describing polyserial and point-polyserial correlations.  The
function is used to find intercorrelations involving ordinal variables or variables that are treated as ordinal
(i.e. count variables in the method used in <code><a href="#topic+rcorrvar2">rcorrvar2</a></code>).
For an ordinal variable with r &gt;= 2 categories, the variance is given by:
</p>
<p style="text-align: center;"><code class="reqn">\sum_{j=1}^{r} {y_{j}}^{2}*p_{j} - {(\sum_{j=1}^{r} y_{j}*p_{j})}^{2}</code>
</p>
<p>.  Here, <code class="reqn">y_{j}</code> is the j-th support
value and <code class="reqn">p_{j}</code> is <code class="reqn">Pr(Y = y_{j})</code>.  This function would not ordinarily be called by the user.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>var_cat(marginal, support)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="var_cat_+3A_marginal">marginal</code></td>
<td>
<p>a vector of cumulative probabilities defining the marginal distribution of the variable;
if the variable can take r values, the vector will contain r - 1 probabilities (the r-th is assumed to be 1)</p>
</td></tr>
<tr><td><code id="var_cat_+3A_support">support</code></td>
<td>
<p>a vector of containing the ordered support values</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A scalar equal to the variance
</p>


<h3>References</h3>

<p>Olsson U, Drasgow F, &amp; Dorans NJ (1982). The Polyserial Correlation Coefficient. Psychometrika, 47(3): 337-47.
doi: <a href="http://doi.org/10.1007/BF02294164">10.1007/BF02294164</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ordnorm">ordnorm</a></code>, <code><a href="#topic+rcorrvar">rcorrvar</a></code>,
<code><a href="#topic+rcorrvar2">rcorrvar2</a></code>, <code><a href="#topic+findintercorr_cont_cat">findintercorr_cont_cat</a></code>,
<code><a href="#topic+findintercorr_cont_pois2">findintercorr_cont_pois2</a></code>, <br /> <code><a href="#topic+findintercorr_cont_nb2">findintercorr_cont_nb2</a></code>
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
